[
    {
        "id": 1,
        "title": "Dapple: A pipelined data parallel approach for training large models",
        "author": [
            "S. Fan",
            "Y. Rong",
            "C. Meng",
            "Z. Cao",
            "S. Wang",
            "Z. Zheng",
            "C. Wu",
            "G. Long",
            "J. Yang",
            "L. Xia"
        ],
        "year": "2021",
        "doi": null,
        "in_text_citation": "[1]",
        "sentence": "An alternative is to develop new parallelization strategies that balance memory usage and communication, as explored in strategies like those in [1]\u2013[3].",
        "abstract": "It is a challenging task to train large DNN models on sophisticated GPU\nplatforms with diversified interconnect capabilities. Recently, pipelined\ntraining has been proposed as an effective approach for improving device\nutilization. However, there are still several tricky issues to address:\nimproving computing efficiency while ensuring convergence, and reducing memory\nusage without incurring additional computing costs. We propose DAPPLE, a\nsynchronous training framework which combines data parallelism and pipeline\nparallelism for large DNN models. It features a novel parallelization strategy\nplanner to solve the partition and placement problems, and explores the optimal\nhybrid strategy of data and pipeline parallelism. We also propose a new runtime\nscheduling algorithm to reduce device memory usage, which is orthogonal to\nre-computation approach and does not come at the expense of training\nthroughput. Experiments show that DAPPLE planner consistently outperforms\nstrategies generated by PipeDream's planner by up to 3.23x under synchronous\ntraining scenarios, and DAPPLE runtime outperforms GPipe by 1.6x speedup of\ntraining throughput and reduces the memory consumption of 12% at the same time.",
        "full_text": "DAPPLE: A Pipelined Data Parallel Approach for\nTraining Large Models\nShiqing Fan1, Yi Rong1, Chen Meng1, Zongyan Cao1, Siyu Wang1, Zhen Zheng1,\nChuan Wu2, Guoping Long1, Jun Yang1, Lixue Xia1, Lansong Diao1, Xiaoyong Liu1, and Wei Lin1\n1Alibaba Group, China\n2The University of Hong Kong, China\n{shiqing.fsq, rongyi.ry, mc119496, zongyan.cao, siyu.wsy, james.zz}@alibaba-inc.com\ncwu@cs.hku.hk, {guopinglong.lgp, muzhuo.yj, lixue.xlx, lansong.dls, xiaoyong.liu, weilin.lw}@alibaba-inc.com\nAbstract\u2014It is a challenging task to train large DNN models\non sophisticated GPU platforms with diversi\ufb01ed interconnect\ncapabilities. Recently, pipelined training has been proposed as\nan effective approach for improving device utilization. However,\nthere are still several tricky issues to address: improving comput-\ning ef\ufb01ciency while ensuring convergence, and reducing memory\nusage without incurring additional computing costs. We propose\nDAPPLE, a synchronous training framework which combines\ndata parallelism and pipeline parallelism for large DNN models.\nIt features a novel parallelization strategy planner to solve the\npartition and placement problems, and explores the optimal hy-\nbrid strategies of data and pipeline parallelism. We also propose\na new runtime scheduling algorithm to reduce device memory\nusage, which is orthogonal to re-computation approach and does\nnot come at the expense of training throughput. Experiments\nshow that DAPPLE planner consistently outperforms strategies\ngenerated by PipeDreams planner by up to 3.23\u00d7 speedup\nunder synchronous training scenarios, and DAPPLE runtime\noutperforms GPipe by 1.6\u00d7 speedup of training throughput and\nsaves 12% of memory consumption at the same time.\nIndex Terms\u2014deep learning, data parallelism, pipeline paral-\nlelism, hybrid parallelism\nI. INTRODUCTION\nThe arti\ufb01cial intelligence research community has a long\nhistory of harnessing computing power to achieve signi\ufb01cant\nbreakthroughs [1]. For deep learning, a trend has been increas-\ning the model scale up to the limit of modern AI hardware.\nMany state-of-the-art DNN models (e.g., NLP [2], Internet\nscale E-commerce search/recommendation systems [3], [4])\nhave billions of parameters, demanding tens to hundreds of\nGBs of device memory for training. A critical challenge is\nhow to train such large DNN models on hardware accelerators,\nsuch as GPUs, with diversi\ufb01ed interconnect capabilities.\nA common approach is sychronous data parallel (DP) train-\ning. Multiple workers each performs complete model compu-\ntation and synchronizes gradients periodically to ensure proper\nmodel convergence. DP is simple to implement and friendly\nin terms of load balance, but the gradients sychronization\noverhead can be a major factor preventing linear scalability.\nWhile the performance issue can be alleviated by optimizations\nsuch as local gradients accumulation [5]\u2013[7] or computation\nand communication overlap techniques [8], [9], aggressive\nDP typically requires large training batch sizes, which makes\nmodel tuning harder. More importantly, DP is not feasible once\nthe parameter scale of the model exceeds the memory limit of\na single device.\nRecently, pipeline parallelism [10]\u2013[12] has been proposed\nas a promising approach for training large DNN models.\nThe idea is to partition model layers into multiple groups\n(stages) and place them on a set of inter-connected devices.\nDuring training, each input batch is further divided into\nmultiple micro-batches, which are scheduled to run over\nmultiple devices in a pipelined manner. Prior research on\npipeline training generally falls into two categories. One is\non optimizing pipeline parallelism for synchronous training\n[10], [11]. This approach requires necessary gradients syn-\nchronizations between adjacent training iterations to ensure\nconvergence. At runtime, it schedules as many concurrent pipe\nstages as possible in order to maximize device utilization.\nIn practice, this scheduling policy can incur notable peak\nmemory consumption. To remedy this issue, re-computation\n[13] can be introduced to trade redundant computation costs\nfor reduced memory usage. Re-computation works by check-\npointing nodes in the computation graph de\ufb01ned by user\nmodel, and re-computing the parts of the graph in between\nthose nodes during backpropagation. The other category is\nasynchronous(async) pipeline training [12]. This manner in-\nserts mini-batches into pipeline continuously and discards the\noriginal sync operations to achieve maximum throughput.\nAlthough these efforts have made good contributions to\nadvance pipelined training techniques, they have some se-\nrious limitations. While PipeDream [14] made progress in\nimproving the time-to-accuracy for some benchmarks with\nasync pipeline parallelism, async training is not a common\npractice in important industry application domains due to\nconvergence concerns. This is re\ufb02ected in a characterization\nstudy [15] of widely diversi\ufb01ed and fast evolving workloads in\nindustry scale clusters. In addition, the async approach requires\nthe storage of multiple versions of model parameters. This,\nwhile friendly for increasing parallelism, further exacerbates\nthe already critical memory consumption issue. As for syn-\nchronous training, current approach [10] still requires notable\nmemory consumption, because no backward processing(BW)\ncan be scheduled until the forward processing(FW) of all\nmicro-batches is \ufb01nished. The intermediate results of some\nmicro-batches in FW need to be stored in the memory (for\narXiv:2007.01045v1  [cs.DC]  2 Jul 2020\ncorresponding BW\u2019s usage later) while the devices are busy\nwith FW of some other micro-batches. GPipe [10] proposes\nto discard some intermediate results to free the memory and\nre-computes them during BW when needed. But this may\nintroduce additional \u223c20% re-computation overhead [16].\nIn this paper, we propose DAPPLE, a distributed training\nscheme which combines pipeline parallelism and data par-\nallelism to ensure both training convergence and memory\nef\ufb01ciency. DAPPLE adopts synchronous training to guarantee\nconvergence, while avoiding the storage of multiple versions\nof parameters in async approach. Speci\ufb01cally, we address\ntwo design challenges. The \ufb01rst challenge is how to deter-\nmine an optimal parallelization strategy given model structure\nand hardware con\ufb01gurations. The optimal strategy refers to\nthe one where the execution time of a single global step\ntheoretically reaches the minimum for given resources. The\ntarget optimization space includes DP, pipelined parallelism,\nand hybrid approaches combining both. Current state-of-the-\nart pipeline partitioning algorithm [12] is not able to be\napplied to synchronous training effectively. Some other work\n[10], [16] relies on empirical and manual optimizations, and\nstill lacks consideration of some parallelism dimensions. We\nintroduce a synchronous pipeline planner, which generates\noptimal parallelization strategies automatically by minimizing\nexecution time of training iterations. Our planner combines\npipeline and data parallelism (via stage-level replication) to-\ngether while partitioning layers into multiple stages. Besides\npipeline planning, for those models that can \ufb01t into a single\ndevice and with high computation/communication ratio, the\nplanner is also capable of producing DP strategies directly\nfor runtime execution. Furthermore, the planner takes both\nmemory usage and interconnect topology constraints into\nconsideration when assigning pipeline stages to devices. The\nassignment of computation tasks to devices is critical for\ndistributed training performance in hardware environments\nwith complex interconnect con\ufb01gurations. In DAPPLE, three\ntopology-aware device assignment mechanisms are de\ufb01ned\nand incorporated into the pipeline partitioning algorithm.\nThe second challenge is how to schedule pipeline stage\ncomputations, in order to achieve a balance among paral-\nlelism, memory consumption and execution ef\ufb01ciency. We\nintroduce DAPPLE schedule, a novel pipeline stage scheduling\nalgorithm which achieves decent execution ef\ufb01ciency with\nreasonably low peak memory consumption. A key feature of\nour algorithm is to schedule forward and backward stages in\na deterministic and interleaved manner to release the memory\nof each pipeline task as early as possible.\nWe evaluate DAPPLE on three representative application\ndomains, namely image classi\ufb01cation, machine translation and\nlanguage modeling. For all benchmarks, experiments show that\nour planner can consistently produce optimal hybrid paral-\nlelization strategies combining data and pipeline parallelism\non three typical GPU hardware environments in industry, i.e.\nhierarchical NVLink + Ethernet, 25 Gbps and 10 Gbps Ether-\nnet interconnects. Besides large models, DAPPLE also works\nwell for medium scale models with relatively large weights yet\nsmall activations (i.e. VGG-19). Performance results show that\n(1) DAPPLE can \ufb01nd optimal hybrid parallelization strategy\noutperforming the best DP baseline up to 2.32\u00d7 speedup; (2)\nDAPPLE planner consistently outperforms strategies gener-\nated by PipeDreams planner by up to 3.23\u00d7 speedup under\nsynchronous training scenarios, and (3) DAPPLE runtime\noutperforms GPipe by 1.6\u00d7 speedup of training throughput\nand reduces the memory consumption of 12% at the same\ntime.\nThe contributions of DAPPLE are summarized as follows:\n1) We systematically explore hybrid of data and pipeline\nparallelism with a pipeline stage partition algorithm for\nsynchronous training, incorporating a topology-aware\ndevice assignment mechanism given model graphs and\nhardware con\ufb01gurations. This facilitates large model\ntraining and reduces communication overhead of sync\ntraining, which is friendly for model convergence.\n2) We feature a novel parallelization strategy DAPPLE\nplanner to solve the partition and placement prob-\nlems and explore the optimal hybrid strategies of data\nand pipeline parallelism, which consistently outperforms\nSOTA planner\u2019s strategies under synchronous training\nscenarios.\n3) We eliminate the need of storing multiple versions of pa-\nrameters, DAPPLE introduces a pipeline task scheduling\napproach to further reduce memory consumption. This\nmethod is orthogonal to re-computation approach and\ndoes not come at the expense of training throughput.\nExperiments show that DAPPLE can further save about\n20% of device memory on the basis of enabling re-\ncomputation optimization.\n4) We provide a DAPPLE runtime which realizes ef\ufb01cient\npipelined model training with above techniques without\ncompromising model convergence accuracy.\nII. MOTIVATION AND DAPPLE OVERVIEW\nWe consider pipelines training only if DP optimizations [5],\n[6], [8], [17]\u2013[20] are unable to achieve satisfactory ef\ufb01ciency,\nor the model size is too large to \ufb01t in a device with a minimum\nrequired batch size. In this section, we summarize key design\nissues in synchronous training with parallelism and motivate\nour work.\nA. Synchronous Pipeline Training Ef\ufb01ciency\nPipeline training introduces explicit data dependencies be-\ntween consecutive stages (devices). A common approach to\nkeep all stages busy is to split the training batch into multiple\nmicro-batches [10]. These micro-batches are scheduled in\nthe pipelined manner to be executed on different devices\nconcurrently.Note that activation communication (comm) over-\nhead matters in synchronous training. Here we incorporate\ncomm as a special pipeline stage for our analysis. We de\ufb01ne\npipeline ef\ufb01ciency as average GPU utilization of all devices\nin the pipeline. The pipeline ef\ufb01ciency is\n1\n1+P [10], where\nP = (1+\u03b1)\u02d9(S\u22121)\nM\n. S, M and \u03b1 are number of stages, number of\nequal micro-batches and communication-to-computation ratio,\nrespectively. Given a pipeline partitioning scheme (namely\n\ufb01xed S and \u03b1), the larger the number of micro-batches M,\nthe higher the pipeline ef\ufb01ciency. Similarly, the smaller S, the\nhigher the ef\ufb01ciency for \ufb01xed \u03b1 and M,.\nThere are efforts to improve synchronous pipelines with\nmicro-batch scheduling [10], which suffer from two issues.\n(1) Extra memory consumption. State-of-the-art approach\ninjects all micro-batches consecutively into the \ufb01rst stage of\nFW. Then the computed activations are the input to BW.\nExecution states (i.e. activations) have to be kept in memory\nfor all micro-batches until their corresponding BW operations\nstart. Therefore, while more injected micro-batches may imply\nhigher ef\ufb01ciency, the memory limit throttles the number of\nmicro-batches allowed.\n(2) Redundant computations. State-of-the-art approach may\nadopt activation re-computation to reduce peak memory con-\nsumption [13], i.e. discarding some activations during the FW\nphase, and recomputing them in the BW phase when needed.\nHowever, redundant computations come with extra costs. It is\nreported that re-computation can consume approximately 20%\nmore execution time [16].\nB. Pipeline Planning\nTo maximize resource utilization and training throughput\nfor pipelined training, it is crucial to \ufb01nd a good strategy for\npartitioning stages and mapping stages to devices. We refer to\nthe stage partitioning and device mapping problem as pipeline\nplanning problem. Current state-of-the-art planning algorithm\n[14] may suffer from the following issues.\nFirst, it does not consider synchronous pipeline training.\nSynchronous pipeline is very important as convergence is\nthe prerequisite of training. Compared against async pipeline\ntraining, an additional step is needed for sync pipeline training\nat the end of all micro-batches to synchronize parameter\nupdates. In more generic case where a stage may be replicated\non multiple devices, there exists additional gradients syn-\nchronizations (AllReduce) overheads before parameter updates.\nCurrent pipeline planner does not take such overhead into\nconsideration and thus can not accurately model the end-to-\nend pipeline training time.\nSecond, previous approach does not consider the impact of\nthe number of stages S, which is important for synchronous\nplanning. As discussed in the previous subsection, with \ufb01xed\nmicro-batches M and comm overhead ratio \u03b1, the fewer the\nnumber of stages(S) , the higher the pipeline ef\ufb01ciency.\nFinally, uneven stage partitions have not been well studied\nin existing works. We show in Section IV that uneven stage\npartitions can sometimes produce better training performance.\nC. The DAPPLE Approach\nWe propose the DAPPLE framework to address aforemen-\ntioned scheduling and planning challenges with synchronous\ntraining. Fig. 1 shows high-level work\ufb02ow of DAPPLE. It\nfeatures a pro\ufb01ler, a planner and a runtime system. Overall,\nDAPPLE pro\ufb01ler takes a user\u2019s DNN model as input, and\npro\ufb01les execution time, activation sizes and parameter sizes\n2. DAPPLE\nPlanner\nUser Input\n1. DAPPLE\nPro\ufb01ler\nPer-layer Statistics:\n- Compute Times\n- Activation Sizes\n- Parameter Sizes\nDistributed\nTraining\n3. DAPPLE\nRuntime\nHardware\nCon\ufb01gurations\nHybrid Parallel Model\nModel\nGlobal Batch Size\nFig. 1: DAPPLE framework overview.\nTABLE I: The traf\ufb01c volume.\nBenchmark\nActivation Size at\nthe Partition Boundaries\nGradient Size\nGNMT-16\n26MB\n1.1GB\nBERT-48\n8.8MB\n2.8GB\nXLNET-36\n4.2MB\n2.1GB\nAmoebaNet-36\n11.2MB\n3.7GB\nVGG-19\n6MB\n550MB\nfor each layer. Taking pro\ufb01ling results as input, DAPPLE\nplanner generates an optimized (hybrid) parallelization plan on\na given global batch size. In terms of the execution time, both\nDAPPLE pro\ufb01ler and planner are of\ufb02ine and can be completed\nwithin a few seconds for all our benckmark models (Table\nII). Finally DAPPLE runtime takes the planner\u2019s results, and\ntransforms the original model graph into a pipelined parallel\ngraph. At this stage, global batch size is further split into\nmultiple micro-batches and then been scheduled for execution\nby DAPPLE runtime.\nMore speci\ufb01cally, the planner aims at minimizing the end-\nto-end execution time of one training iteration. This module\nis responsible for stage partition, stage replication and device\nassignment and generates optimal parallelization plans. In\nparticular, the device assignment process is aware of the\nhardware topology, as shown in Fig. 1.\nWe also explore the mapping of a single stage onto multiple\ndevices. With the replication of pipeline stages on multiple\ndevices, DAPPLE processes training with the hybrid of data\nand pipeline parallelism. In practice, this hybrid strategy can\nexploit hierarchical interconnects effectively. Fig. 2 gives an\nexample where a model is partitioned into two stages and\neach stage is replicated on four devices within the same\nserver(NVLink connections within server), while inter-stage\ncommunication goes over the Ethernet. This mapping exploits\nworkload characteristics (Table I) by leveraging the high-\nspeed NVLink for heavy gradients sync, while using the slow\nEthernet bandwidth for small activations communication. We\ndiscuss details of our planner in Section IV.\nFinally, the DAPPLE runtime involves a carefully designed\nscheduling algorithm. Unlike existing pipeline scheduling al-\ngorithms [21], DAPPLE scheduler (Section III) signi\ufb01cantly\nGPU0\nGPU1\nGPU2\nGPU3\nGPU0\nGPU1\nGPU2\nGPU3\nNVLink\nEthernet\nPipeline Parallelism\nMachine0 (Stage0)\nMachine1 (Stage1)\nData Parallelism\nNVLink\nNVLink\nNVLink\nNVLink\nNVLink\nFig. 2: Device mapping on hierarchical interconnects.\nreduces the need for re-computation, retains a reasonable level\nof memory consumption, while saturates the pipeline with\nenough micro-batches to keep all devices busy.\nIII. DAPPLE SCHEDULE\nA. Limitations of GPipe Schedule\nTo improve pipeline training ef\ufb01ciency, GPipe [10] proposes\nto split global batch into multiple micro-batches and injects\nthem into the pipeline concurrently (Fig. 3 (a)). However,\nthis scheduling pattern alone is not memory-friendly and will\nnot scale well with large batch. The activations produced\nby forward tasks have to be kept for all micro-batches until\ncorresponding backward tasks start, thus leads to the mem-\nory demand to be proportional (O(M)) to the number of\nconcurrently scheduled micro-batches (M). GPipe adopts re-\ncomputation to save memory while brings approximately 20%\nextra computation. In DAPPLE, we propose early backward\nscheduling to reduce memory consumptions while achieving\ngood pipeline training ef\ufb01ciency(Fig. 3 (b)).\nB. Early backward scheduling\nThe main idea is to schedule backward tasks(BW) earlier\nand hence free the memory used for storing activations pro-\nduced by corresponding forward tasks(FW). Fig. 3(b) shows\nDAPPLE\u2019s scheduling mechanism, compared to GPipe in Fig.\n3 (a). Firstly, instead of injecting all M micro-batches at\nonce, we propose to inject K micro-batches (K < M) at\nthe beginning to release memory pressure while retaining high\npipeline ef\ufb01ciency. Secondly, we schedule one FW of a micro-\nbatch followed by one BW strictly to guarantee that BW\ncan be scheduled earlier. Fig. 3 (c) shows how the memory\nconsumptions change over time in GPipe and DAPPLE. At\nthe beginning, the memory usage in DAPPLE increases with\ntime and is the same as GPipe\u2019s until K micro-batches are\ninjected, then it reaches the maximum due to the early BW\nscheduling. Speci\ufb01cally, with strictly controlling the execution\norder of FW and BW, the occupied memory for activations\nproduced by the FW of a micro-batch will be freed after\nthe corresponding BW so that it can be reused by the next\ninjected micro-batch. In comparison, GPipe\u2019s peak memory\nconsumptions increases continuously and has no opportunity\nfor early release. Moreover, DAPPLE does not sacri\ufb01ce in\npipeline training ef\ufb01ciency. Actually, DAPPLE introduces the\n6\nForward\nBackward\nGPU0\nGPU1\nGPU2\nGPU0\nGPU1\nGPU2\n0\n0\n0\n1\n1\n1\n2\n2\n3\n2\n3\n3\n3\n3\n3\n2\n2\n2\n1\n1\n1\n0\n0\n0\n4\n5\n5\n6\n6\n4\n5\n4\n6\n6\n6\n5\n5\n4\n4\n4\n5\nPeak Memory\nTime\n2\n0\n0\n0\n1\n1\n1\n2\n3\n3\n2\n3\n3\n3\n3\n2\n2\n2\n1\n1\n1\n0\n0\n0\n4\n4\n4\n5\n5\n5\n6\n6\n6\n4\n4\n4\n5\n5\n5\n6\n6\n6\n(c) Memory consumption of GPU0  from (a) and (b) \n(a) GPipe\n(b) DAPPLE\n(b) DAPPLE\n(a) GPipe\nFig. 3: The different scheduling between GPipe(a) and DAP-\nPLE(b) and their memory consumptions.\n1\n0\n1\n0\n1\n2\n3\n0\n0\n4\nStage 4\nAllReduce\nStage 0\nStage 1\nForward\nBackward\n2\n0\n1\n2\n0\n1\n2\n0\n0\n1\n1\n1\n0\n0\n0\n0\n3\n3\n3\n2\n2\n2\n2\n3\n3\n3\n3\n1\n1\n1\n4\n4\n4\n2\n2\n2\n5\n1\n6\n2\n7\n3\n5\n5\n5\n3\n3\n3\n6\n6\n6\n4\n4\n4\n7\n7\n7\n4\n4\n4\n4\n5\n5\n5\n5\n6\n6\n6\n6\n7\n7\n7\n7\n5\n5\n5\n6\n6\n6\n7\n7\n7\n4\n5\n6\n7\nStage 2\nAllReduce\nAllReduce\nNetwork Transmission\nStage 2\nTw (Warmup Phase)\nTs\u00a0(Steady Phase)\nTe\u00a0(Ending Phase)\nStage 3\nStage 4\nBubble\nFig. 4: DAPPLE pipeline example.\nexact same bubble time as GPipe when given the same stage\npartition, micro-batches and device mapping. We will present\nthe details in section V-C.\nNote that the combination of early backward scheduling and\nre-combination allows further exploitation in memory usage.\nWe present performance comparisons of DAPPLE and GPipe\nin section VI-D.\nIV. DAPPLE PLANNER\nDAPPLE Planner generates an optimal hybrid parallelism\nexecution plan given pro\ufb01ling results of DAPPLE pro\ufb01ler,\nhardware con\ufb01gurations and a global training batch size.\nA. The Optimization Objective\nFor synchronous training, we use the execution time of a\nsingle global batch as our performance metric, which we call\npipeline latency. The optimization objective is to minimize\npipeline latency L with the consideration of all solution spaces\nof data/pipeline parallelism.\nIn synchronous pipelined training, computations and cross-\nstage communication of all stages usually form a trapezoid,\nbut not diamond formed by normal pipelines without backward\nphase. Fig.4 shows a pipelined training example with well\ndesigned task scheduling arrangement. We use blue blocks\nfor forward computation, and green ones for backwards, with\nnumbers in them being the corresponding micro-batch index.\nNetwork communications are arranged as individual stages.\nGray blocks are bubbles.\nWe denote the stage with the least bubble overhead as pivot\nstage, which will be the dominant factor in calculating pipeline\nlatency L. Let its stage id be Q. We will discuss about how\nto choose pivot stage later.\nA pipeline training iteration consists of warmup phase,\nsteady phase and ending phase, as shown in Fig. 4 in which\npivot stage is the last stage. Pivot stage dominates steady\nphase. We call the execution period from the start to pivot\nstage\u2019s \ufb01rst micro-batch as warmup phase in a pipeline iter-\nation, the period from pivot stage\u2019s last micro-batch to the\nend as ending phase. Pipeline latency is the sum of these\nthree phases. The optimization objective for estimating L is\nas follows:\nTw =\nQ\nX\ns=0\nFs\nTs = (M \u22121) \u00d7 (FQ + BQ)\nTe =\nS\u22121\nmax\ns=0 (AR(Ps, gs) +\n(\n\u2212Ps\na=Q Ba\ns \u2265Q\nPQ\na=s Ba\ns < Q)\n(1)\nL = Tw + Ts + Te\n(2)\nTw denotes the execution time of warmup phase, which is\nthe sum of forward execution time of stages till Q for one\nmicro-batch. Ts denotes the steady phase, which includes both\nforward and backward time of stage Q for all micro-batches\nexcept for the one contributing to warmup and ending phase.\nTe corresponds to the ending phase. Te includes allreduce\noverhead and thus considers stages both before and after Q.\nNote that some stages before Q may contribute to Te with\nallreduce cost. M, S, Fs and Bs denote the total number of\nmicro-batches, the number of stages (computation stages +\nnetwork stages), forward and backward computation time of\nstage s, respectively. AR(Ps, gs) represents the gradients syn-\nchronization (AllReduce) time for stage s, with its parameter\nset Ps on the device set gs.\nNote we consider inter-stage communication as an indepen-\ndent stage alongside the computation stages. The AllReduce\ntime AR(Ps, gs) is always 0 for communication stages. More-\nover, we de\ufb01ne Fs and Bs for a communication stages as its\nfollowing forward and backward communication time.\nIn practice, synchronous pipelines in some cases include\nbubbles in stage Q, which may contribute a small fraction\nof additional delay to time. This objective does not model\nthose internal bubbles, and thus is an approximation to the\ntrue pipeline latency. But it works practically very well for all\nour benchmarks (Section VI).\nB. Device Assignment\nDevice assignment affects communication ef\ufb01ciency and\ncomputing resource utilization. Previous work [12] uses hi-\nerarchical planning and works well for asynchronous training.\nHowever, it lacks consideration of synchronous pipeline train-\ning, in which the latency of the whole pipeline, rather than of a\nsingle stage, matters to overall performance. It cannot be used\nOccupied GPUi\nReturned GPUi after policy\nAvailable GPUi\n1\n0\n3\n2\n5\n4\n7\n6\n9\n8\n11\n10\n13\n12\n15\n14\n1\n0\n3\n2\n5\n4\n7\n6\n9\n8\n11\n10\n13\n12\n15\n14\n17\n16\n19\n18\n21\n20\n23\n22\n1\n0\n3\n2\n5\n4\n7\n6\n9\n8\n11\n10\n13\n12\n15\n14\n17\n16\n19\n18\n21\n20\n23\n22\n(c) Append First based on (a)\n(b) Fresh First based on (a)\n(d) Scatter First based on (a)\nM2\nM1\nM0\ni\ni\ni\n(a) Original device assignment state\n17\n16\n19\n18\n21\n20\n23\n22\n1\n0\n3\n2\n5\n4\n7\n6\n9\n8\n11\n10\n13\n12\n15\n14\n17\n16\n19\n18\n21\n20\n23\n22\nM2\nM1\nM0\nMk: the kth machine\nFig. 5: Device assignment examples: applying for 6 devices\nusing three different strategies respectively from (a).\nto ef\ufb01ciently estimate the whole pipeline latency. Meanwhile,\nit does not allow stages to be placed on arbitrary devices.\nOur approach essentially allows a speci\ufb01c stage to be mapped\nto any set of devices, and therefore is able to handle more\nplacement cases, at a reasonable searching cost.\nInstead of enumerating all possibilities of placement plans\nusing brute force, we designed three policies (Fig. 5), and\nexplore their compositions to form the \ufb01nal placement plan.\na) Fresh First: allocate GPUs from a fresh machine. It\ntends to put tasks within a stage onto the same machine,\nwhich can leverage high-speed NVLink [22] for intra-stage\ncommunication. A problem of this policy is that, it can cause\nfragmentation if the stage cannot fully occupy the machine.\nb) Append First: allocate from machines that already\nhave GPUs occupied. It helps to reduce fragmentation. It also\nlargely implies that the stage is likely to be within the same\nmachine.\nc) Scatter First: try to use available GPUs equally from\nall used machines, or use GPUs equally from all machines\nif they are all fresh. It is suitable for those stages that have\nnegligible weights compared to activation sizes (less intra-\nstage communication). This policy could also serve as an in-\ntermediate state to allocate GPU with minimal fragmentation.\nThe overall device placement policies reduce search space\neffectively down to less than O(2S), while retaining room for\npotential performance gain.\nC. Planning Algorithm\nOur planning algorithm use Dynamic Programming to \ufb01nd\nthe optimal partition, replication and placement strategy, so\nthat the pipeline latency L is minimized. Here we \ufb01rst present\nhow to update the pivot stage ID Q along the planning process,\nand then the formulation of our algorithm\n1) Determining The Pivot Stage Q: It is vital to select a\nproper pivot stage Q for the estimation of L. The insight is to\n\ufb01nd the stage with minimum bubbles, which dominates steady\nphase. We use a heuristic to determine Q (formula 3).\nQ = arg\n0\nmax\ns=S\u22121 max\n\u0010\nT Q\nst +\nQ\u22121\nX\ns\u2032=s+1\n(Fs\u2032 + Bs\u2032), T s\nst\n\u0011\n(3)\nThe initial Q is set to S \u22121. DAPPLE updates Q iteratively\nfrom stage S \u22121 to stage 0 according to formula 3. T j\nst =\n(M \u22121) \u00d7 (Fj + Bj) means the duration of steady phase,\nPlanned layers\nNew stage s'1\nm GPUs\nm' GPUs\n0\nj\nj'\nN-1\nNew stage s'2\nlayer id:\nCurrently, we get:\nTPL(j, m, g)\nNext step, we get:\nTPL(j', m+m', g+g')\n(G - m - m') GPUs\nFig. 6: Planning process for j\u2019.\nwithout bubbles, suppose pivot stage is j. For a stage s <\nQ, if T s\nst is larger than the sum of T Q\nst and corresponding\nforward/backward costs between s and current Q, it means\nthe steady phase will have less bubbles if pivot stage is set as\ns other than current Q. Q will then be updated to s.\n2) Algorithm\nFormulation:\nWe\nde\ufb01ne\nthe\nestimated\npipeline latency TP L(j, m, g) as the subproblem, for which\nwe have planned the strategy for the \ufb01rst j layers using m\nGPUs (with device id set g). The unplanned layers forms the\nlast stage and replicates on the other (G \u2212m) GPUs. Our\nobjective is to solve for TP L(N, G, G), G = {0, 1, ..., G \u22121}.\nN, G and G denote the number of layers, number of GPUs\nand GPU set, respectively. Formula 4 describes the algorithm.\nTP L(N, G, G) =\nmin\n1\u2264j<N\nmin\n1\u2264m<G\nmin\ng\u2208D(G,m) TP L(j, m, g) (4)\nFig. 6 describes the iterative planning process. Suppose we\nhave already planned for the \ufb01rst j (0 \u2264j < N) layers\nand have the estimation TP L(j, m, g) as pipeline latency. The\nlayers after j forms a stage s\u2032. Meanwhile, we get the optimal\nQ for current strategy along with the cost of FQ and BQ\nfor stage Q. Next step, we try to add one more partition in\nstage s\u2032, supposing after layer j\u2032 (j < j\u2032 \u2264N), and split s\u2032\ninto two new stages s\u2032\n1 and s\u2032\n2. We assign m\u2032 GPUs for s\u2032\n1 and\n(G\u2212m\u2212m\u2032) GPUs for s\u2032\n2, and estimate TP L(j\u2032, m+m\u2032, g+g\u2032)\naccording to formula 5. Note DAPPLE enumerates the three\nstrategies in section IV-B for device placement of stage s\u2032\n1.\nTP L(j\u2032, m + m\u2032, g + g\u2032) = L\n(5)\nHere, L is the same with that in formula 2. The key for\nthe estimation of L in formula 5 is to \ufb01nd Q of subproblem\nTP L(j\u2032, m+m\u2032, g+g\u2032). In the sample in Fig. 6, we get Qj for\nTP L(j, m, g). We apply formula 3 to get Qj\u2032 for TP L(j, m +\nm\u2032, g + g\u2032) with the help of Qj: if Qj is not s\u2032, we do not\nneed to iterate all stages before j, but use Qj for all stages\nbefore layer j instead in the iterative process.\nAlong the above process, we record the current best split,\nreplication and placement for each point in our solution space\nusing memorized search.\nD. Contributions over previous work\nPrevious works on pipeline planning includes PipeDream\n[12] (for asynchronous training) and torchgpipe [23], a com-\nmunity implementation of GPipe [10] which uses \u201cBlock Par-\ntitioning of Sequences\u201d [24]. Both aim to balance the workload\nacross all GPUs. While this idea works good in PipeDream\u2019s\nasynchronous scenarios and gives reasonable solutions under\nForward\nBackward\nGPU0\nGPU1\n0\n0\n0\nGPU0\nGPU1\n1\n0\n0\n1\n0\n1\n1\n1\n0\n0\n1\n1\n1\nFig. 7: Uneven pipeline minimum example.\nGPipe\u2019s synchronous pipeline for its micro-batch arrangement,\nwe found that our micro-batch arrangement could achieve\neven higher performance by 1) intentionally preferring slightly\nuneven partitioning with fewer stages, and 2) exploring a\nbroader solution space of device assignment. The following\nsections highlight our contributions of planning for hybrid\nparallelism. The resulting strategies and performance gain on\nreal-world models will be demonstrated in Section VI-F.\n1) Uneven Pipeline Partitioning with Fewer Stages: In syn-\nchronous Pipeline Parallelism scenarios, we found two insights\nthat could provide an additional performance improvements.\nThe \ufb01rst one is to partition the model into as few stages\nas possible to minimize the bubble overhead under the same\nnumber of micro-batches. This conclusion is also mentioned\nin GPipe. The second one is that partitioning the model in a\nslightly uneven way yields much higher performance than a\nperfectly even split, like the example in Fig. 7.\n2) Versatile Device Placement: DAPPLE device assign-\nment strategy covers a broad solution space for stage place-\nment, and is a strict superset of PipeDream\u2019s hierarchical re-\ncursive partitioning approach. This allows us to handle various\nreal world models. For example, for models that have layers\nwith huge activations compared to their weights, DAPPLE\nallows such a layer to be replicated across multiple machines\n(Scatter First) to utilize high-speed NVLink for activation\ncommunication and low-speed Ethernet for AllReduce.\nV. DAPPLE RUNTIME\nA. Overview\nWe design and implement DAPPLE runtime in Tensor\ufb02ow\n[25] (TF) 1.12, which employs a graph based execution\nparadigm. As common practices, TF takes a precise and\ncomplete computation graph (DAG), schedules and executes\ngraph nodes respecting all data/control dependencies.\nDAPPLE runtime takes a user model and its planning results\nas input, transforms the model graph into a pipelined parallel\ngraph and executes on multiple distributed devices. It \ufb01rst\nbuilds forward/backward graphs separately for each pipe stage.\nThen additional split/concat nodes are introduced between ad-\njacent stages for activation communication. Finally, it builds a\nsubgraph to perform weights update for synchronous training.\nThis step is replication aware, meaning it will generate dif-\nferent graphs with or without device replication of stages. We\nleverage control dependences in TF to enforce extra execution\norders among forward/backward stage computations. Section\nV-B presents how to build basic TF graph units for a single\nmicro-batch. Section V-C discusses how to chain multiple\nsuch units using control dependencies to facilitate DAPPLE\nexecution.\n(a) Split replicated stage across multiple devices by dividing micro batch size \n(b) Each device of replicated stage consumes the whole micro batch size\nand resulting in more bubbles\nGPU2\nGPU1\nGPU0\nStage0\nStage1\n0\n0\n1\n1\n2\n2\n3\n3\n4\n4\n1\n3\n1\n3\n0\n2\n0\n4\n2\n4\nGPU2\nGPU1\nGPU0\nStage0\nStage1\nForward\nBackward\nIdle\n0\n0\n1\n1\n2\n2\n3\n3\n4\n4\n0\n1\n0\n2\n1\n3\n2\n4\n3\n4\n0\n1\n0\n2\n1\n3\n2\n4\n3\n4\nFig. 8: Ef\ufb01ciency of two stage replication approaches. Stage\n0 consumes twice as much time as stage 1 for a micro-batch.\nB. Building Micro-batch Units\n1) Forward/Backward Stages: In order to enforce execution\norders with control dependencies between stages, we need to\nbuild forward and backward graphs stage by stage to deal with\nthe boundary output tensors such as activations.\nSpeci\ufb01cally, we \ufb01rst construct the forward graph of each\nstage in sequence and record the boundary tensors. No back-\nward graphs should be built until all forward graphs are ready.\nSecond, backward graphs will be built in reverse order for each\nstage.\n2) Cross Stage Communication: DAPPLE replicates some\nstages such that the number of nodes running a stage can\nbe different between adjacent stages, and the communication\npatterns between them are different from straight pipeline\ndesign. We introduce special split-concat operations between\nthese stages.\nFig. 8(a) shows the replication in DAPPLE for a 2-stage\npipeline, whose \ufb01rst stage consumes twice as much time as\nthe second stage for a micro-batch and thus is replicated on\ntwo devices. For the \ufb01rst stage, we split the micro-batch further\ninto 2 even slices, and assign each to a device. An alternative\napproach [12] (Fig. 8(b)) is not to split, but to schedule an\nentire micro-batch to two devices in round robin manner.\nHowever, the second approach has lower pipeline ef\ufb01ciency\ndue to tail effect [26]. Though the second approach does not\ninvolve extra split-concat operations, the overhead of tail effect\nis larger than split-concat in practice. We hence use the \ufb01rst\napproach with large enough micro-batch size setting to ensure\ndevice ef\ufb01ciency.\nThe split-concat operations include one-to-many, many-to-\none and many-to-many communication. We need split for\none-to-many(Fig. 9(b)), that is, splitting the micro-batch into\neven slices and sending each slice to a device in the next\nstage. We need concat for many-to-one(Fig. 9(c)), where all\nslices should be concatenated from the previous stage and fed\ninto the device in the next stage. For many-to-many(Fig. 9(d))\nwe need both split and concat of micro-batch slices to\nconnect adjacent stages. If two adjacent stages have the same\nreplication count, no split-concat is needed.\n3) Synchronous Weights Update: Weights updating in DAP-\nPLE is different with naive training as there are multiple\nmicro-batches injected concurrently. Meanwhile, the replica-\ntion makes weights updating more complex. As is shown in\n0\n1\n1\n0\n(a) No replication\n0\n1\n1\n1\n1\n0\nS\nC\n1\n0\n0\n0\n0\n1\nS\nC\n1\n0\n0\n0\n0\n1\nS\n1\n1\n1\n1\nC\nS\nS\nC\nC\n(b) One to many\n(c) Many to one\n(d) Many to many\nS Split Node C Concat Node\nForward Pass\nBackward Pass\nFig. 9: Split-Concat for Cross Stage Communication\nGA\nApply\nApply\nAllReduce\nReplica 0\nreduced\ngradients\nreduced\ngradients\nReplica n\n......\naccumulated\ngradients\ncompute next micro batch\nGA\naccumulated\ngradients\ngradients\nFW\nBW\nBW\nFW\ngradients\ncompute next micro batch\nFig. 10: Weights update. GA means gradient accumulation [5].\nFig. 10, each device produces and accumulates gradients for\nall micro-batches. There is an AllReduce operation to synchro-\nnize gradients among all replicas, if exists. A normal Apply\noperation updates weights with averaged gradients eventually.\nC. Micro-batch Unit Scheduling\nThe early backward scheduling strikes a trade-off between\nmicro-batch level parallelism and peak memory consumption:\nfeeding more micro-batches into pipeline at once implies\nhigher parallelism, but may lead to more memory usage.\nDAPPLE scheduler enforces special execution orders be-\ntween micro-batches to reduce memory usage. For the \ufb01rst\nstage, we suppose K micro-batches are scheduled concurrently\nat the beginning for forward computation. Speci\ufb01cally, Ki is\nthe number of scheduled micro-batches at the beginning for\nstage i. The overall execution follows a round robin order with\ninterleaving FW and BW.\nWe realize the scheduler with control dependency edges in\nTF. Fig. 11 shows how up to three micro-batches are connected\nvia control dependencies to implement the schedule for a two\nstage pipeline. Control dependency is not necessary when there\nis only one micro-batch (Fig. 11(a)). With two micro-batches\n(Fig. 11(b)), two control edges are introduced. The control\nedge between B0 and F1 in stage 1 is to form the round\nrobin order of FW and BW. The early execution of B0 helps\nto free memory of F0 and B0 in stage 1, which can be\nreused in following tasks. The edge between F0 and F1 in\nF0\nGPU0\nForward Pass\nBackward Pass\nB0\nF0\nB0\nF0\nB0\nF0\nB0\nF1\nF1\nB1\nB1\nF0\nB0\nF0\nB0\nF1\nF1\nB1\nB1\nF2\nF2\nB2\nB2\nGPU1\n(a) One micro-batch\n(b) Two micro-batches\n(c) Three micro-batches\nData Dependency\nControl Dependency\nGPU0\nGPU1\nGPU0\nGPU1\nFig. 11: Micro-batches scheduling. The solid blue and dotted\nred arrows denote data and control dependencies, respectively.\nstage 0 is to enforce the order that micro-batch 1 is strictly\nexecuted after micro-batch 0, thus the backward of micro-\nbatch 0 can be executed earlier and its corresponding memory\ncan be freed earlier. In practice, F0 and F1 are typically large\nchunks of computations. The lack of parallelism between F0\nand F1 does not affect the gain of reducing memory usage.\nThe situation with three micro-batches (Fig. 11(c)) is the same.\nAn appropriate Ki is essential as it indicates the peak mem-\nory consumption for stage i. There are two primary factors for\ndeciding Ki: memory demand for one micro-batch execution,\nand the ratio between cross stage communication latency and\nthe average FW/BW computation time (referred as activation\ncommunication ratio, ACR in short). The former determines\nhow many forward batches can be scheduled concurrently\nat most. We de\ufb01ne the maximum number of micro-batches\nsupported by the device memory as D; Lower ACR means\nless warm up forward batches Ki (smaller Ki) are enough to\nsaturate the pipeline. While notable ACR means larger Ki is\nnecessary.\nWe implement two policies to set Ki in practice. Policy A\n(PA): Ki = min(S \u2212i, D). PA works well when ACR is\nsmall, i.e. the impact of cross stage communication overhead\nis negligible. Policy B (PB): Ki = min(2 \u2217(S \u2212i) \u22121, D).\nHere we schedule twice the number of forward micro-batches\nthan PA. The underlying intuition is that in some workloads,\nthe cross stage communication overhead is comparable with\nforward/backward computations and thus more micro-batches\nis needed to saturate the pipeline.\nVI. EVALUATION\nA. Experimental Setup\nBenchmarks. Table II summarizes all the six representative\nDNN models that we use as benchmarks in this section. The\ndatasets applied for the three tasks are WMT16 En-De [27],\nSQuAD2.0 [28] and ImageNet [29], respectively.\nHardware Con\ufb01gurations. Table III summarizes three com-\nmon hardware environments for DNN training in our exper-\niments, where hierarchical and \ufb02at interconnections are both\ncovered. In general, hierarchical interconnection is popular in\nTABLE II: Benchmark models.\nTask\nModel\n# of\nParams\n(cbch Size,\nMemory Cost)\nTranslation\nGNMT-16 [30]\n291M\n(64, 3.9GB)\nLanguage\nModel\nBERT-48 [31]\n640M\n(2, 11.4GB)\nXLNet-36 [32]\n500M\n(1, 12GB)\nImage\nClassi\ufb01cation\nResNet-50 [33]\n24.5M\n(128, 1GB)\nVGG-19 [34]\n137M\n(32, 5.6GB)\nAmoebaNet-36 [35]\n933M\n(1, 20GB)\nTABLE III: Hardware con\ufb01gurations.\nCon\ufb01g\nGPU(s) per\nserver(Ns)\nIntra-server\nconnnections\nInter-server\nconnections\nA\n8x V100\nNVLink\n25 Gbps\nB\n1x V100\nN/A\n25 Gbps\nC\n1x V100\nN/A\n10 Gbps\nindustry GPU data centers. We also consider \ufb02at Ethernet net-\nworks interconnections because NVLink may not be available\nand GPU resources are highly fragmented in some real-world\nproduction clusters. Speci\ufb01cally, Con\ufb01g-A (hierarchical) has\nservers each with 8 V100 interconnected with NVLink, and a\n25Gbps Ethernet interface. Con\ufb01g-B (\ufb02at) has servers each\nwith only one V100 (no NVLink) and a 25Gbps Ethernet\ninterface. Con\ufb01g-C (\ufb02at) is the same with Con\ufb01g-B except\nwith only 10 Gbps Ethernet equipped. The V100 GPU has\n16 GB of device memory. All servers run 64-bits CentOS 7.2\nwith CUDA 9.0, cuDNN v7.3 and NCCL 2.4.2 [36].\nBatch Size and Training Setup. The batch sizes of of\ufb02ine\npro\ufb01ling for the benchmarks are shown in the last column\nof Table II (ch size. As for AmoebaNet-36, it reaches OOM\neven if batch size = 1 on a single V100. Thus we extend\nto two V100s where batch size = 1 just works. We use\nlarge enough global batch size for each benchmark to ensure\nhigh utilization on each device. All global batch sizes we use\nare consistent with common practices of the ML community.\nWe train GNMT-16, BERT-48 and XLNet-36 using the Adam\noptimizer [37] with initial learning rate of 0.0001, 0.0001, 0.01\nand 0.00003 respectively. For VGG19, we use SGD with an\ninitial learning rate of 0.1. For AmoebaNet, we use RMSProp\n[38] optimizer with an initial learning rate of 2.56. We use fp32\nfor training in all our experiments. Note that all the pipeline\nlatency optimizations proposed in this paper give equivalent\ngradients for training when keeping global batch size \ufb01xed\nand thus convergence is safely preserved.\nB. Planning Results\nTable V summarizes DAPPLE planning results of \ufb01ve mod-\nels in the three hardware environments, where the total number\nof available devices are all \ufb01xed at 16. The \ufb01rst column also\ngives the global batch size (GBS) correspondingly.\nWe use three notations to explain the output plans. (1) A\nplan of P : Q indicates a two stage pipeline, with the \ufb01rst\nstage and the second stages replicated on P and Q devices,\n0\n4\n8\n12\n16\n0\n1024\n2048\n3072\n4096\nTraining Speedup\nGlobal BatchSize\nDP No Overlap\nDP+Normal Overlap\nBest Hybrid Speedup\n(a) VGG19 on con\ufb01g A\n0\n4\n8\n12\n16\n0\n1024\n2048\n3072\n4096\nTraining Speedup\nGlobal BatchSize\nDP No Overlap\nDP+Normal Overlap\nBest Hybrid Speedup\n(b) VGG19 on con\ufb01g B\n0\n4\n8\n12\n16\n0\n1024\n2048\n3072\n4096\nTraining Speedup\nGlobal BatchSize\nDP No Overlap\nDP+Normal Overlap\nBest Hybrid Speedup\n(c) VGG19 on con\ufb01g C\n0\n4\n8\n12\n16\n0\n1024\n2048\n3072\n4096\nTraining Speedup\nGlobal BatchSize\nDP No Overlap\nDP+Normal Overlap\nBest Hybrid Speedup\n(d) GNMT-16 on con\ufb01g A\n0\n4\n8\n12\n16\n0\n1024\n2048\n3072\n4096\nTraining Speedup\nGlobal BatchSize\nDP No Overlap\nDP+Normal Overlap\nBest Hybrid Speedup\n(e) GNMT-16 on con\ufb01g B\n0\n4\n8\n12\n16\n0\n1024\n2048\n3072\n4096\nTraining Speedup\nGlobal BatchSize\nDP No Overlap\nDP+Normal Overlap\nBest Hybrid Speedup\n(f) GNMT-16 on con\ufb01g C\n0\n4\n8\n12\n16\n0\n64\n128\n192\n256\nTraining Speedup\nGlobal BatchSize\nDP No Overlap\nDP+Normal Overlap\nBest Hybrid Speedup\n(g) BERT-48 on con\ufb01g A\n0\n4\n8\n12\n16\n0\n64\n128\n192\n256\nTraining Speedup\nGlobal BatchSize\nDP No Overlap\nDP+Normal Overlap\nBest Hybrid Speedup\n(h) BERT-48 on con\ufb01g B\n0\n4\n8\n12\n16\n0\n64\n128\n192\n256\nTraining Speedup\nGlobal BatchSize\nDP No Overlap\nDP+Normal Overlap\nBest Hybrid Speedup\n(i) BERT-48 on con\ufb01g C\n0\n4\n8\n12\n16\n0\n64\n128\n192\n256\nTraining Speedup\nGlobal BatchSize\nDP No Overlap\nDP+Normal Overlap\nBest Hybrid Speedup\n(j) XLNet-36 on con\ufb01g A\n0\n4\n8\n12\n16\n0\n64\n128\n192\n256\nTraining Speedup\nGlobal BatchSize\nDP No Overlap\nDP+Normal Overlap\nBest Hybrid Speedup\n(k) XLNet-36 on con\ufb01g B\n0\n4\n8\n12\n16\n0\n64\n128\n192\n256\nTraining Speedup\nGlobal BatchSize\nDP No Overlap\nDP+Normal Overlap\nBest Hybrid Speedup\n(l) XLNet-36 on con\ufb01g C\n0\n4\n8\n12\n16\n0\n256\n512\n768\n1024\nTraining Speedup\nGlobal BatchSize\nDP No Overlap\nDP+Normal Overlap\nBest Hybrid Speedup\n(m) AmoebaNet-36 on con\ufb01g A\n0\n4\n8\n12\n16\n0\n256\n512\n768\n1024\nTraining Speedup\nGlobal BatchSize\nDP No Overlap\nDP+Normal Overlap\nBest Hybrid Speedup\n(n) AmoebaNet-36 on con\ufb01g B\n0\n4\n8\n12\n16\n0\n256\n512\n768\n1024\nTraining Speedup\nGlobal BatchSize\nDP No Overlap\nDP+Normal Overlap\nBest Hybrid Speedup\n(o) AmoebaNet-36 on con\ufb01g C\nFig. 12: Speedups on con\ufb01gurations with hierarchical/\ufb02at interconnects.\nTABLE IV: Normalized training throughput speedup of\nscheduling policies PB compared to PA.\nModel\nBert-48\nXLNet-36\nVGG-19\nGNMT-16\nSpeedup\n1.0\n1.02\n1.1\n1.31\nTABLE V: DAPPLE planning results.\nModel\n(GBS)\n#Servers \u00d7 Ns\nOutput\nPlan\nSplit\nPosition\nACR\nResNet-50\n(2048)\n2 \u00d7 8 (A)\nDP\n-\n-\n16 \u00d7 1 (B)\nDP\n-\n-\n16 \u00d7 1 (C)\nDP\n-\n-\nVGG-19\n(2048)\n2 \u00d7 8 (A)\nDP\n-\n-\n16 \u00d7 1 (B)\nDP\n-\n-\n16 \u00d7 1 (C)\n15 : 1\n13 : 6\n0.40\nGNMT-16\n(1024)\n2 \u00d7 8 (A)\n8 : 8\n9 : 7\n0.10\n16 \u00d7 1 (B)\n8 : 8\n9 : 7\n0.10\n16 \u00d7 1 (C)\nStraight\n-\n3.75\nBERT-48\n(64)\n2 \u00d7 8 (A)\n8 : 8\n23 : 25\n0.06\n16 \u00d7 1 (B)\nStraight\n-\n0.50\n16 \u00d7 1 (C)\nStraight\n-\n1.25\nXLNet-36\n(128)\n2 \u00d7 8 (A)\n8 : 8\n18 : 18\n0.03\n16 \u00d7 1 (B)\n8 : 8\n18 : 18\n0.03\n16 \u00d7 1 (C)\nStraight\n-\n0.67\nAmoebaNet-36\n(128)\n2 \u00d7 8 (A)\n8 : 8\n24 : 12\n0.18\n16 \u00d7 1 (B)\n11 : 5\n27 : 9\n0.14\n16 \u00d7 1 (C)\n11 : 5\n27 : 9\n0.35\nrespectively. For example, when P = 8 and Q = 8, we\nput each stage on one server, and replicate each stage on\nall 8 devices within the server(con\ufb01g-A). Besides, for plans\nwhere P > 8 or Q > 8 (e.g., 15 : 1) where some stages\nare replicated across servers, it will most likely be chosen\nfor con\ufb01gurations with \ufb02at interconnections such as Con\ufb01g-B\nor Con\ufb01g-C, since for Con\ufb01g-A replicating one stage across\nservers incurs additional inter-server communication overhead.\n(2) A straight plan denotes pipelines with no replication. (3) A\nDP plan means the optimal strategy is data-parallel. We treat\nDP and straight as special cases of general DAPPLE plans.\nThe Split Position column of Table V shows the stage\npartition point of each model for the corresponding pipeline\nplan. The ACR column of the table shows the averaged ratio\nof cross-stage communication latency (i.e. communication\nof both activations in FW and gradients in BW) and stage\ncomputation time.\nIn the case of single server of con\ufb01g A, there is no relative\nlow-speed inter-server connection, the intra-server bandwidth\nis fast enough (up to 130GB/s) to easily handle the magnitude\n(up to 3.7GB) of gradients communication of all benchmark\nmodels, and we \ufb01nd all models prefer DP plan for this case.\nResNet-50. The best plan is consistently DP for all three\nhardware con\ufb01gurations. This is not surprising due to its rel-\natively small model size (100MB) yet large computation den-\nsity. Even with low speed interconnects con\ufb01g C, DP with no-\ntably gradients accumulation and computation/communication\noverlap outperforms the pipelined approach.\nVGG-19. Best plans in con\ufb01g A and B are also DP (Fig.\n12 (a) and(b)), due to the moderate model size (548MB),\nrelatively fast interconnects (25 Gbps), and the overlapping in\nDP. The weights and computation distributions of VGG19 are\nalso considered overlapping-friendly, since most of the weights\nare towards the end of the model while computations are at\nthe beginning, allowing gradients aggregation to be overlapped\nduring that computation-heavy phase. In the case of low speed\ninterconnects (con\ufb01g C), a 15 : 1 pipelined outperforms DP\n(Fig. 12 (c). This is because most parameters in VGG-19\nagglomerate in the last fully connected layer. A 15 : 1 two-\nstage pipeline thus avoids most of the overheads of gradients\nsynchronization due to replication (note we do not replicate the\nsecond stage). In this case gradients synchronization overheads\noutweigh bene\ufb01ts of DP with overlap.\nGNMT-16/BERT-48/XLNet-36. All three models have uni-\nform layer structures, i.e., each layer has roughly the same\nscale of computations and parameters. And the parameter\nscales of these models vary from 1.2 GB up to 2.6 GB(Table\nII). In con\ufb01g-A where all three models achieve low ACR\nvalues (0.10, 0.06 and 0.03, respectively, as shown in Table\nV), a two stage 8 : 8 pipeline works best. Unlike VGG-19, the\nthree models\u2019 layers are relatively uniformly distributed, thus\na symmetric, evenly partitioning is more ef\ufb01cient. In con\ufb01g\nC, a straight pipeline works best for all three models. In this\ncon\ufb01g, all devices have approximately the same workload.\nMore importantly, no replication eliminates gradients sync\noverheads for relatively large models (1.2-2.6 GB) on a slow\nnetwork (10 Gbps). The three models behave differently in\ncon\ufb01g B. BERT-48 prefers straight pipeline in con\ufb01g B, while\nGNMT-16 and XLNet-36 keep the same plan results as shown\nin con\ufb01g A. This is because for \ufb01xed 16 devices, 16 and 48\nuniform layers are more friendly for even partition compared\nto 36 layers for \ufb02at interconnections.\nAmoebaNet-36. For AmoebaNet-36, DP is not available\ndue to device memory limit. AmoebaNet-36 has more complex\nnetwork patterns than other models we evaluated, and larger\nACR in con\ufb01g A as well. Thus, more successive forward\nmicro-batches are needed to saturate the pipeline. For all\nthree con\ufb01gs, two-stage pipeline (8 : 8, 11 : 5 and 11 : 5,\nrespectively) works best.\nC. Performance Analysis\nIn this work, we measure training speed-up as the ratio\nbetween the time executing all micro-batches sequentially on\na single device and the time executing all micro-batches in\nparallel by all devices, with the same global batch size.\nFig. 12 shows training speed-ups for all models except\nResNet-50 on con\ufb01g A, B and C. For ResNet-50, the planning\nresults are obvious and we simply present it in Table V.\nFor the other models, we compare training speed-ups of\nthree different implementations: (1) Best Hybrid Speedup,\nperformance of the best hybrid plan of pipeline and data\nparallelism returned by DAPPLE planner; (2) DP No Overlap,\nperformance of DP with gradients accumulation but without\ncomputation/communication overlap; (3) DP Overlap, perfor-\nmance of DP with both gradients accumulation and intra-\niteration computation/communication overlap between back-\nward computation and gradients communication [20].\nOverall analysis across these \ufb01ve models from Fig. 12, for\n\ufb01xed GBS = 128, we can \ufb01nd that the hybrid approaches\nfrom DAPPLE outperform the DP approach with best intra-\nbatch overlapping with averaged 1.71X/1.37/1.79X speedup\nfor con\ufb01g-A, con\ufb01g-B and con\ufb01g-C, respectively. Specially,\nthis speedup is up to 2.32X for GNMT-16 on con\ufb01g-C.\nSpeci\ufb01c analysis for each model is given below.\nVGG-19. For VGG-19, about 70% of model weights (about\n400 MB) are in the last fully connected (fc) layer, while\nthe activation size between any two adjacent layers gradually\ndecreases from the \ufb01rst convolution layer to the last fc layer,\nvarying dramatically from 384 MB to 3 MB for batch size\n32. Thus, the split between VGG-19\u2019s convolutional layers\nand fully-connected layers leads to very small activation\n(3MB), and only replicating all the convolutional layers other\nthan fully-connected layers greatly reduces communication\noverhead in case of relatively slow interconnects (Fig. 12 (c)).\nGNMT-16. GNMT-16 prefers a two-stage pipeline on hi-\nerarchical network (con\ufb01g A) and \ufb02at network with relative\nhigh-speed connection (con\ufb01g B). And the corresponding spit\nposition is 9 : 7 but not 8 : 8, this is because the per-\nlayer workloads of encoder layer and decoder of GNMT are\nunbalanced (approximately 1 : 1.45), thus the split position\nof DAPPLE plan shifts one layer up into decoder for pursuit\nof better system load-balance. For low speed interconnection\nenvironments (con\ufb01g C), straight pipeline ranks \ufb01rst when\nGBS = 1024. Each device is assigned exactly one LSTM\nlayers of GNMT, and the GBS is large enough to \ufb01ll the\n16-stage pipeline.\nBERT-48/XLNet-36. The best DAPPLE plan outperforms\nall DP variants for both models (Fig. 12 (g) to (l)) in all\ncon\ufb01gurations. Compared to XLNet, the memory requirement\nfor BERT is much smaller and thus allows more micro-batches\non a single device. More computation per-step implies more\nbackward computation time can be leveraged for overlapping\ncommunication overhead. As for con\ufb01g B and C, the slower\nthe network is(from 25 Gbps to 10 Gbps), the higher the\nadvantage of our approach has over DP variants. This is\nbecause the cross stage communication for both models is\nnegligible with respect to gradients communication and the\npipelined approach is more tolerant of slow network than DP.\nAmoebaNet-36. The DAPPLE plan works best in all three\ncon\ufb01gurations when GBS is \ufb01xed to 128. Unlike BERT-48 and\nXLNet-36, AmoebaNet has non uniform distributions of per\nlayer parameters and computation density. The last third part\nof the model holds 73% of all parameters, and the per-layer\ncomputation time gradually increases for large layer id and the\noverall maximum increase is within 40%. As DAPPLE planner\nseeks for load-balanced staging scheme while considering the\nallreduce overhead across replicated stages, the split positions\nof pipelined approach for AmoebaNet-36 will obviously tilt\nto larger layer ID for better system ef\ufb01ciency. Take con\ufb01g\nA as an example, a two-stage pipeline is chosen and each\nstage is replicated over a separate server with 8 devices each.\nTABLE VI: DAPPLE vs. GPipe on BERT-48 with 2-stage\npipeline when keeping micro-batch size \ufb01xed to 2 on Con\ufb01g-\nB. RC is short for re-computation.\nCon\ufb01g\n# of micro\nbatch (M)\nThroughput\n(samples/sec)\nAverage Peak\nMemory (GB)\nGPipe\n2\n5.10\n12.1\n3\n\u2013\nOOM\nGPipe + RC\n2\n4.00\n9.9\n5\n5.53\n13.2\n8\n\u2013\nOOM\nDAPPLE\n2\n5.10\n10.6\n8\n7.60\n10.6\n16\n8.18\n10.6\nDAPPLE + RC\n2\n4.24\n8.5\n8\n6.23\n8.5\n16\n6.77\n8.5\nFor this case a total of 36 normal cells layers are divided\ninto 2 parts, namely 24 : 12, where the per-stage computation\ntime ratio and allreduce time ratio of stage0 and stage1 is\n1.57 : 1 and 1 : 2.74, respectively. For lower bandwidth\nnetwork con\ufb01gurations (con\ufb01g B and C), the split position\nkeeps shrinking to larger layer ID, because the allreduce comm\noverheads turn out to be the dominant factor with the absent\nof high-speed NVLink bandwidth.\nD. Scheduling Policy\nAs discussed in Section V-C, the number of successive\nforward micro-batches (Ki for stage i) scheduled in the warm\nup phase is an important factor to pipeline ef\ufb01ciency. We\nimplement two policies, PA and PB, referring to smaller and\nlarger Ki numbers, respectively. Table IV shows the nor-\nmalized speedups for four benchmark models on hierarchical\ninterconnects(con\ufb01g A), where all models\u2019 stage partition and\nreplication schemes are consistent with the planning results of\n2 servers of con\ufb01g A as shown in Table V.\nFor VGG-19 and GNMT-16 (as well as AmoebaNet-36,\nwhich is not given in this \ufb01gure yet), where the ACR ratio\nis relative high (0.16, 0.10, 0.18, respectively), there exists\nnotable performance difference between these two policies\n(10%, 31% improvement from PA to PB, respectively). Hence\nwe choose a larger Ki to maximize pipeline ef\ufb01ciency. For the\nother models (BERT-48, XLNet-36), whose ACRs are very\nsmall (0.06, 0.03, respectively), the cross stage communication\noverhead is negligible compared to intra-stage computation\ntime, leading to little performance difference. In this case, we\nprefer a smaller Ki to conserve memory consumption.\nE. Comparison with GPipe\nTable VI shows the performance comparisons with GPipe.\nWe focus on the throughput and peak memory usage on BERT-\n48 with a 2-stage pipeline on Con\ufb01g-B. To align with GPipe,\nwe adopt the same re-computation strategy which stores acti-\nvations only at the partition boundaries during forward. Note\nthat all the pipeline latency optimizations in DAPPLE give\nequivalent gradients for training when keeping global batch\nTABLE VII: Strategy Comparison between DAPPLE and\nPipeDream, in the form of (start layer, end layer)@[GPU IDs].\nModel (GBS)\nDAPPLE\nPipeDream\nVGG19 (1024)\n(0, 16) @ [G0 - G13]\n(17, 25) @ [G14, G15]\n(0, 11) @ [G0 - G7]\n(11, 17) @ [G8 - G13]\n(17, 19) @ G14\n(19, 25) @ G15\nAmoebaNet-36 (128)\n(0, 30) @ [G0 - G7]\n(31, 43) @ [G8 - G15]\nstraight\nBERT Large (128)\n(0, 13) @ [G0 - G7]\n(14, 26) @ [G8 - G15]\n(0, 4) @ [G0, G1]\n(4, 13) @ [G2 - G7]\n(13, 16) @ [G8, G9]\n(16, 19) @ [G10, G11]\n(19, 22) @ [G12, G13]\n(22, 26) @ [G14, G15]\nXLNet-36 (128)\n(0, 22) @ [G0 - G7]\n(23, 41) @ [G8 - G15]\nstraight\nsize \ufb01xed and thus convergence is safely preserved and will\nnot be further analysed here.\nWhen applying re-computation, both DAPPLE and GPipe\nsave about 19% averaged peak memory at the expense of 20%\non throughput when keeping M = 2 \ufb01xed.\nWhen both without re-computation, DAPPLE gets 1.6\u00d7\nhigher throughput with M = 16, and consumes 0.88\u00d7 aver-\naged peak memory compared to GPipe, which only supports\nup to 2 micro-batches. The speedup is mainly because higher\nM leads to lower proportion of bubbles. Note DAPPLE\nallows more micro-batches as the peak memory requirement\nis independent of M due to early backward scheduling.\nThe combination of DAPPLE scheduler and re-computation\nallows a further exploitation in memory usage. Compared\nwith baseline GPipe (without re-computation), DAPPLE + RC\nachieves 0.70\u00d7 memory consumption when M = 16, which\nallows us to handle larger micro-batch size or larger model.\nF. Comparison with PipeDream\nWe compare the results of our planner with those of\nPipeDream\u2019s under the synchronous training scenarios. We\nuse the same con\ufb01gurations for both planners (e.g. same\ndevice topology, same interconnect and same pro\ufb01ling data),\nand evaluate both planners with DAPPLE Runtime. Table\nVII shows the strategy results under a two-machine cluster\nof con\ufb01g-A. Fig. 13 shows the performance results for the\nstrategies running in both 2 \u00d7 8 and 4 \u00d7 8 con\ufb01gurations.\nAs shown in Fig. 13, in terms of speedup relative to to\ndata parallelism, our strategies consistently outperform those\ngenerated by PipeDream\u2019s planner by up to 3.23\u00d7 speedup\nunder synchronous training scenarios, thanks to the advantages\ndetailed in Section IV-D.\nG. Strong Scaling\nFig. 14 shows training speed-ups for four models. The\nnumber of GPUs ranges from 2 to 16. We use \ufb01xed but\ndifferent global batch size for each model and apply con\ufb01g\nA. For AmoebaNet-36 when GBS\n= 256 (Fig. 14(d)),\nboth DP approaches achieve NaN as the model size is too\n23.8\n24.4\n12.7\n16.3\n15.7\n19.2\n7.4\n2.2\n14.9\n14.5\n11.6\n9.6\n8.6\n11.5\n6.3\n3.0\nSpeedup\n0.0\n10.0\n20.0\n30.0\nXLNet-36\nBERT-Large\nAmoebaNet-36\nVGG-19\nDAPPLE 4x8\nDAPPLE w/ PipeDream Strategy 4x8\nDAPPLE 2x8\nDAPPLE w/ PipeDream Strategy 2x8\nFig. 13: Performance comparison with PipeDream.\nTABLE VIII: Maximum model size of BERT supported by\nDAPPLE + re-computation on V100 (16GB each) on con\ufb01g-\nA. BERT-L: BERT model with L encoder layers. Each model\nparameter needs 16 bytes since we applied Adam optimizer.\nCon\ufb01g\nBERT-L\n# of Model\nParams\nTotal Model\nParams Mem\nAvg. GPU\nUtil\nNative-1\n48\n640M\n10.2GB\n93%\nPipeline-2\n106\n1.4B\n21.9GB\n89%\nPipeline-4\n215\n2.7B\n43.8GB\n89%\nPipeline-8\n428\n5.5B\n88.2GB\n87%\nlarge to \ufb01t memory of single V100. For all these four\nmodels, we observe better scalability of DAPPLE over DP\nvariants. Scalability weakens on all DP variants when the\nnumber of GPUs increases from 8 to 10, where gradients\nsynchronization performance drops substantially due to slow\nEthernet bandwidth. The performance of DAPPLE approach\nscales smoothly due to the rather small magnitude of cross-\nstage activations as compared with weights(Table I), which\nis insensitive to the relatively low-speed inter-server commu-\nnications(25Gbps). In general, for hierarchical interconnects,\nthe lower the cross-machine bandwidth, the more obvious the\nadvantage DAPPLE approach as compared with DP.\nH. Weak Scaling\nTable VIII shows the maximum model size that DAPPLE\nsupports under reasonable input size with re-computation.\nWe scale the model by varying the numbers of layers. We\nare able to scale BERT to 5.5B on 8 V100s with NVLink.\nThere is a slight reduction in average GPU utilization due\nto more bubbles introduced by longer pipeline. In this case,\nthe maximum model size scales linearly due to the balanced\ndistribution of model params over encoder layers in BERT.\nVII. RELATED WORK\nLarge DNN models are increasingly computational in-\ntensive. It is a common practice to parallelize training by\nleveraging multiple GPUs [39]\u2013[42]. Data parallelism, model\nparallelism and pipeline parallelism are common approaches\nfor distributed training of DNN models.\nData Parallelism [43] . Some prior studies [9], [44]\u2013\n[48] focus on reducing the communication overheads for\n0\n4\n8\n12\n16\n2\n4\n6\n8\n10\n12\n14\n16\nTraining Speedup\nNumber of GPUs\nDP No Overlap\nDP+Normal Overlap\nBest Hybrid Speedup\nStraight Pipeline\n(a) GNMT-16(GBS = 2048)\n0\n4\n8\n12\n16\n2\n4\n6\n8\n10\n12\n14\n16\nTraining Speedup\nNumber of GPUs\nDP No Overlap\nDP+Normal Overlap\nBest Hybrid Speedup\n(b) BERT-48(GBS = 128)\n0\n4\n8\n12\n16\n2\n4\n6\n8\n10\n12\n14\n16\nTraining Speedup\nNumber of GPUs\nDP No Overlap\nDP+Normal Overlap\nBest Hybrid Speedup\n(c) XLNet-36(GBS = 128)\n0\n4\n8\n12\n16\n2\n4\n6\n8\n10\n12\n14\n16\nTraining Speedup\nNumber of GPUs\nDP No Overlap\nDP+Normal Overlap\nBest Hybrid Speedup\n(d) AmoebaNet-36(GBS = 256)\nFig. 14: Speedup with \ufb01xed GBS in con\ufb01g-A.\ndata parallelism. As a commonly used performance optimiza-\ntion method, gradients accumulation [5], [6], [49] offers an\neffective approach to reduce communication-to-computation\nratio. Another complementary approach is computation and\ncommunication overlap, with promising results reported in\nsome CNN benchmarks [8], [20].\nModel Parallelism. Model Parallelism [50] partitions DNN\nmodels among GPUs to mitigate communication overhead and\nmemory bottlenecks for distributed training [10], [14], [39],\n[40], [51]\u2013[54]. This paper focuses on model partition between\nlayers, namely, pipeline parallelism.\nPipeline parallelism. Pipeline Parallelism [10], [11], [14],\n[21], [55] has been recently proposed to train DNN in a\npipelined manner. GPipe [10] explores synchronous pipeline\napproach to train large models with limited GPU memory.\nPipeDream [14] explores the hybrid approach of data and\npipeline parallelism for asynchronous training. [53], [55], [56]\nmake further optimization based on PipeDream. Pal et al.\n[40] evaluated the hybrid approach without thorough study.\nSome researchers have been seeking for the optimal placement\nstrategy to assign operations in a DNN to different devices\n[57]\u2013[59] to further improve system ef\ufb01ciency.\nVIII. CONCLUSION\nIn this paper, we propose DAPPLE framework for pipelined\ntraining of large DNN models. DAPPLE addresses the need for\nsynchronous pipelined training and advances current state-of-\nthe-art by novel pipeline planning and micro-batch scheduling\napproaches. On one hand, DAPPLE planner module deter-\nmines an optimal parallelization strategy given model structure\nand hardware con\ufb01gurations. It considers pipeline partition,\nreplication and placement, and generates a high-performance\nhybrid data/pipeline parallel strategy. On the other hand,\nDAPPLE scheduler module is capable of simultaneously\nachieving optimal training ef\ufb01ciency and moderate memory\nconsumption, without storing multiple versions of parameters\nand getting rid of the strong demand of re-computation which\nhurts system ef\ufb01ciency at the same time. Experiments show\nthat DAPPLE planner consistently outperforms strategies gen-\nerated by PipeDreams planner by up to 3.23\u00d7 speedup under\nsynchronous training scenarios, and DAPPLE scheduler out-\nperforms GPipe by 1.6\u00d7 speedup of training throughput and\nsaves 12% of memory consumption at the same time.\nREFERENCES\n[1] R. Sutton, The Bitter Lesson, 2019, http://www.incompleteideas.net/\nIncIdeas/BitterLesson.html.\n[2] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,\nY. Zhou, W. Li, and J. P. Liu, \u201cExploring the limits of transfer learning\nwith a uni\ufb01ed text-to-text transformer,\u201d https://arxiv.org/abs/1910.10683,\n2019.\n[3] J. Wang, P. Huang, H. Zhao, Z. Zhang, B. Zhao, and D. L. Lee,\n\u201cBillion-scale commodity embedding for e-commerce recommendation\nin alibaba,\u201d in Proceedings of the 24th ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining.\nACM, 2018,\npp. 839\u2013848.\n[4] P. Covington, J. Adams, and E. Sargin, \u201cDeep neural networks for\nyoutube recommendations,\u201d in Proceedings of the 10th ACM Conference\non Recommender Systems, ACM, New York, NY, USA.\nACM, 2016.\n[5] Gradients\nAccumulation-Tensor\ufb02ow,\n2019,\nhttps://github.com/\ntensor\ufb02ow/tensor\ufb02ow/pull/32576.\n[6] Gradients\nAccumulation-PyTorch,\n2019,\nhttps://gist.github.com/\nthomwolf/ac7a7da6b1888c2eeac8ac8b9b05d3d3.\n[7] A New Lightweight, Modular, and Scalable Deep Learning Framework,\n2016, https://caffe2.ai/.\n[8] A. Jayarajan, J. Wei, G. Gibson, A. Fedorova, and G. Pekhimenko,\n\u201cPriority-based parameter propagation for distributed dnn training,\u201d\narXiv preprint arXiv:1905.03960, 2019.\n[9] A. Sergeev and M. Del Balso, \u201cHorovod: fast and easy distributed deep\nlearning in tensor\ufb02ow,\u201d arXiv preprint arXiv:1802.05799, 2018.\n[10] Y. Huang, Y. Cheng, A. Bapna, O. Firat, D. Chen, M. Chen, H. Lee,\nJ. Ngiam, Q. V. Le, Y. Wu et al., \u201cGpipe: Ef\ufb01cient training of giant\nneural networks using pipeline parallelism,\u201d in Advances in Neural\nInformation Processing Systems, 2019, pp. 103\u2013112.\n[11] J. Zhan and J. Zhang, \u201cPipe-torch: Pipeline-based distributed deep\nlearning in a gpu cluster with heterogeneous networking,\u201d in 2019\nSeventh International Conference on Advanced Cloud and Big Data\n(CBD).\nIEEE, 2019, pp. 55\u201360.\n[12] D. Narayanan, A. Harlap, A. Phanishayee, V. Seshadri, N. R. Devanur,\nG. R. Ganger, P. B. Gibbons, and M. Zaharia, \u201cPipedream: generalized\npipeline parallelism for dnn training,\u201d in Proceedings of the 27th ACM\nSymposium on Operating Systems Principles.\nACM, 2019, pp. 1\u201315.\n[13] T. Chen, B. Xu, C. Zhang, and C. Guestrin, \u201cTraining deep nets with\nsublinear memory cost,\u201d arXiv preprint arXiv:1604.06174, 2016.\n[14] A. Harlap, D. Narayanan, A. Phanishayee, V. Seshadri, N. Devanur,\nG. Ganger, and P. Gibbons, \u201cPipedream: Fast and ef\ufb01cient pipeline\nparallel dnn training,\u201d arXiv preprint arXiv:1806.03377, 2018.\n[15] M. Wang, C. Meng, G. Long, C. Wu, J. Yang, W. Lin, and Y. Jia,\n\u201cCharacterizing deep learning training workloads on alibaba-pai,\u201d arXiv\npreprint arXiv:1910.05930, 2019.\n[16] GpipeTalk, 2019, https://www.youtube.com/watch?v=9s2cum25Kkc.\n[17] P. Goyal, P. Doll\u00b4ar, R. Girshick, P. Noordhuis, L. Wesolowski, A. Kyrola,\nA. Tulloch, Y. Jia, and K. He, \u201cAccurate, large minibatch sgd: Training\nimagenet in 1 hour,\u201d arXiv preprint arXiv:1706.02677, 2017.\n[18] G. Long, J. Yang, K. Zhu, and W. Lin, \u201cFusionstitching: Deep\nfusion and code generation for tensor\ufb02ow computations on gpus,\u201d\nhttps://arxiv.org/abs/1811.05213, 2018.\n[19] G. Long, J. Yang, and W. Lin, \u201cFusionstitching: Boosting execu-\ntion ef\ufb01ciency of memory intensive computations for dl workloads,\u201d\nhttps://arxiv.org/abs/1911.11576, 2019.\n[20] H. Zhang, Z. Zheng, S. Xu, W. Dai, Q. Ho, X. Liang, Z. Hu,\nJ. Wei, P. Xi, and E. P. Xing, \u201cPoseidon: An ef\ufb01cient communication\narchitecture for distributed deep learning on GPU clusters,\u201d in 2017\nUSENIX Annual Technical Conference (USENIX ATC 17).\nSanta\nClara, CA: USENIX Association, Jul. 2017, pp. 181\u2013193. [Online].\nAvailable:\nhttps://www.usenix.org/conference/atc17/technical-sessions/\npresentation/zhang\n[21] B. Yang, J. Zhang, J. Li, C. R\u00b4e, C. R. Aberger, and C. De Sa,\n\u201cPipemare: Asynchronous pipeline parallel dnn training,\u201d arXiv preprint\narXiv:1910.05124, 2019.\n[22] NVLink, 2019, https://www.nvidia.com/en-us/data-center/nvlink/.\n[23] H. Lee, M. Jeong, C. Kim, S. Lim, I. Kim, W. Baek, and B. Yoon,\n\u201ctorchgpipe, A GPipe implementation in PyTorch,\u201d https://github.com/\nkakaobrain/torchgpipe, 2019.\n[24] I. B\u00b4ar\u00b4any and V. S. Grinberg, \u201cBlock partitions of sequences,\u201d Israel\nJournal of Mathematics, vol. 206, no. 1, pp. 155\u2013164, 2015.\n[25] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S.\nCorrado, A. Davis, J. Dean, M. Devin, S. Ghemawat, I. Goodfellow,\nA. Harp, G. Irving, M. Isard, Y. Jia, R. Jozefowicz, L. Kaiser,\nM. Kudlur, J. Levenberg, D. Man\u00b4e, R. Monga, S. Moore, D. Murray,\nC. Olah, M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar,\nP. Tucker, V. Vanhoucke, V. Vasudevan, F. Vi\u00b4egas, O. Vinyals,\nP. Warden, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng,\n\u201cTensorFlow: Large-scale machine learning on heterogeneous systems,\u201d\n2015, software available from tensor\ufb02ow.org. [Online]. Available:\nhttp://tensor\ufb02ow.org/\n[26] J.\nDemouth,\n\u201cCuda\npro\ntip:\nMinimize\nthe\ntail\nef-\nfect,\u201d\n2015.\n[Online].\nAvailable:\nhttps://devblogs.nvidia.com/\ncuda-pro-tip-minimize-the-tail-effect/\n[27] R. Sennrich, B. Haddow, and A. Birch, \u201cEdinburgh neural machine\ntranslation systems for wmt 16,\u201d arXiv preprint arXiv:1606.02891, 2016.\n[28] P. Rajpurkar, R. Jia, and P. Liang, \u201cKnow what you don\u2019t know:\nUnanswerable questions for squad,\u201d arXiv preprint arXiv:1806.03822,\n2018.\n[29] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,\nZ. Huang, A. Karpathy, A. Khosla, M. Bernstein et al., \u201cImagenet large\nscale visual recognition challenge,\u201d International journal of computer\nvision, vol. 115, no. 3, pp. 211\u2013252, 2015.\n[30] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey,\nM. Krikun, Y. Cao, Q. Gao, K. Macherey et al., \u201cGoogle\u2019s neural\nmachine translation system: Bridging the gap between human and\nmachine translation,\u201d arXiv preprint arXiv:1609.08144, 2016.\n[31] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBert: Pre-training\nof deep bidirectional transformers for language understanding,\u201d arXiv\npreprint arXiv:1810.04805, 2018.\n[32] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. Salakhutdinov, and Q. V. Le,\n\u201cXlnet: Generalized autoregressive pretraining for language understand-\ning,\u201d arXiv preprint arXiv:1906.08237, 2019.\n[33] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning for image\nrecognition,\u201d in Proceedings of the IEEE conference on computer vision\nand pattern recognition, 2016, pp. 770\u2013778.\n[34] K. Simonyan and A. Zisserman, \u201cVery deep convolutional networks for\nlarge-scale image recognition,\u201d arXiv preprint arXiv:1409.1556, 2014.\n[35] S. A. R. Shah, W. Wu, Q. Lu, L. Zhang, S. Sasidharan, P. DeMar,\nC. Guok, J. Macauley, E. Pouyoul, J. Kim et al., \u201cAmoebanet: An sdn-\nenabled network service for big data science,\u201d Journal of Network and\nComputer Applications, vol. 119, pp. 70\u201382, 2018.\n[36] NCCL, 2019, https://developer.nvidia.com/nccl.\n[37] D. P. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d\narXiv preprint arXiv:1412.6980, 2014.\n[38] T. Tieleman and G. Hinton, \u201cLecture 6.5-rmsprop: Divide the gradient\nby a running average of its recent magnitude,\u201d COURSERA: Neural\nNetworks for Machine Learning 4, 2012.\n[39] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, M. Mao, M. Ran-\nzato, A. Senior, P. Tucker, K. Yang et al., \u201cLarge scale distributed deep\nnetworks,\u201d in Advances in neural information processing systems, 2012,\npp. 1223\u20131231.\n[40] S. Pal, E. Ebrahimi, A. Zul\ufb01qar, Y. Fu, V. Zhang, S. Migacz, D. Nellans,\nand P. Gupta, \u201cOptimizing multi-gpu parallelization strategies for deep\nlearning training,\u201d IEEE Micro, vol. 39, no. 5, pp. 91\u2013101, 2019.\n[41] Z. Jia, S. Lin, C. R. Qi, and A. Aiken, \u201cExploring the hidden dimension\nin accelerating convolutional neural networks,\u201d 2018.\n[42] J. Geng, D. Li, and S. Wang, \u201cHorizontal or vertical?: A hybrid approach\nto large-scale distributed machine learning,\u201d in Proceedings of the 10th\nWorkshop on Scienti\ufb01c Cloud Computing.\nACM, 2019, pp. 1\u20134.\n[43] A. Krizhevsky, \u201cOne weird trick for parallelizing convolutional neural\nnetworks,\u201d arXiv preprint arXiv:1404.5997, 2014.\n[44] Baidu-allreduce,\n2018,\nhttps://github.com/baidu-research/\nbaidu-allreduce.\n[45] X. Jia, S. Song, W. He, Y. Wang, H. Rong, F. Zhou, L. Xie, Z. Guo,\nY. Yang, L. Yu et al., \u201cHighly scalable deep learning training system\nwith mixed-precision: Training imagenet in four minutes,\u201d arXiv preprint\narXiv:1807.11205, 2018.\n[46] N. Arivazhagan, A. Bapna, O. Firat, D. Lepikhin, M. Johnson,\nM. Krikun, M. X. Chen, Y. Cao, G. Foster, C. Cherry et al., \u201cMas-\nsively multilingual neural machine translation in the wild: Findings and\nchallenges,\u201d arXiv preprint arXiv:1907.05019, 2019.\n[47] Byteps, A high performance and generic framework for distributed DNN\ntraining, 2019, https://github.com/bytedance/byteps.\n[48] Y. You, J. Hseu, C. Ying, J. Demmel, K. Keutzer, and C.-J. Hsieh,\n\u201cLarge-batch training for lstm and beyond,\u201d in Proceedings of the\nInternational Conference for High Performance Computing, Networking,\nStorage and Analysis, 2019, pp. 1\u201316.\n[49] T. D. Le, T. Sekiyama, Y. Negishi, H. Imai, and K. Kawachiya,\n\u201cInvolving cpus into multi-gpu deep learning,\u201d in Proceedings of the\n2018 ACM/SPEC International Conference on Performance Engineer-\ning.\nACM, 2018, pp. 56\u201367.\n[50] Z. Jia, M. Zaharia, and A. Aiken, \u201cBeyond data and model parallelism\nfor deep neural networks,\u201d arXiv preprint arXiv:1807.05358, 2018.\n[51] Z. Huo, B. Gu, Q. Yang, and H. Huang, \u201cDecoupled parallel backprop-\nagation with convergence guarantee,\u201d arXiv preprint arXiv:1804.10574,\n2018.\n[52] J. Geng, D. Li, and S. Wang, \u201cRima: An rdma-accelerated model-\nparallelized solution to large-scale matrix factorization,\u201d in 2019 IEEE\n35th International Conference on Data Engineering (ICDE).\nIEEE,\n2019, pp. 100\u2013111.\n[53] C.-C. Chen, C.-L. Yang, and H.-Y. Cheng, \u201cEf\ufb01cient and robust parallel\ndnn training through model parallelism on multi-gpu platform,\u201d arXiv\npreprint arXiv:1809.02839, 2018.\n[54] N. Dryden, N. Maruyama, T. Moon, T. Benson, M. Snir, and B. Van Es-\nsen, \u201cChannel and \ufb01lter parallelism for large-scale cnn training,\u201d in\nProceedings of the International Conference for High Performance\nComputing, Networking, Storage and Analysis, 2019, pp. 1\u201320.\n[55] J. Geng, D. Li, and S. Wang, \u201cElasticpipe: An ef\ufb01cient and dynamic\nmodel-parallel solution to dnn training,\u201d in Proceedings of the 10th\nWorkshop on Scienti\ufb01c Cloud Computing.\nACM, 2019, pp. 5\u20139.\n[56] L. Guan, W. Yin, D. Li, and X. Lu, \u201cXpipe: Ef\ufb01cient pipeline model par-\nallelism for multi-gpu dnn training,\u201d arXiv preprint arXiv:1911.04610,\n2019.\n[57] A. Mirhoseini, H. Pham, Q. V. Le, B. Steiner, R. Larsen, Y. Zhou,\nN. Kumar, M. Norouzi, S. Bengio, and J. Dean, \u201cDevice placement\noptimization with reinforcement learning,\u201d in Proceedings of the 34th\nInternational Conference on Machine Learning-Volume 70. JMLR. org,\n2017, pp. 2430\u20132439.\n[58] J. Gu, M. Chowdhury, K. G. Shin, Y. Zhu, M. Jeon, J. Qian, H. Liu,\nand C. Guo, \u201cTiresias: A {GPU} cluster manager for distributed deep\nlearning,\u201d in 16th {USENIX} Symposium on Networked Systems Design\nand Implementation ({NSDI} 19), 2019, pp. 485\u2013500.\n[59] M. Wang, C.-c. Huang, and J. Li, \u201cSupporting very large models using\nautomatic data\ufb02ow graph partitioning,\u201d in Proceedings of the Fourteenth\nEuroSys Conference 2019, 2019, pp. 1\u201317.\n",
        "context": "brid strategies of data and pipeline parallelism. We also propose\na new runtime scheduling algorithm to reduce device memory\nusage, which is orthogonal to re-computation approach and does\nnot come at the expense of training throughput. Experiments\nData Parallelism [43] . Some prior studies [9], [44]\u2013\n[48] focus on reducing the communication overheads for\n0\n4\n8\n12\n16\n2\n4\n6\n8\n10\n12\n14\n16\nTraining Speedup\nNumber of GPUs\nDP No Overlap\nDP+Normal Overlap\nBest Hybrid Speedup\nStraight Pipeline\nmodels among GPUs to mitigate communication overhead and\nmemory bottlenecks for distributed training [10], [14], [39],\n[40], [51]\u2013[54]. This paper focuses on model partition between\nlayers, namely, pipeline parallelism."
    },
    {
        "id": 2,
        "title": "Beyond data and model parallelism for deep neural networks",
        "author": [
            "Z. Jia",
            "M. Zaharia",
            "A. Aiken"
        ],
        "year": "2019",
        "doi": null,
        "in_text_citation": "[2]",
        "sentence": "An alternative is to develop new parallelization strategies that balance memory usage and communication, as explored in strategies like those in [1]\u2013[3].",
        "abstract": "The computational requirements for training deep neural networks (DNNs) have\ngrown to the point that it is now standard practice to parallelize training.\nExisting deep learning systems commonly use data or model parallelism, but\nunfortunately, these strategies often result in suboptimal parallelization\nperformance.\n  In this paper, we define a more comprehensive search space of parallelization\nstrategies for DNNs called SOAP, which includes strategies to parallelize a DNN\nin the Sample, Operation, Attribute, and Parameter dimensions. We also propose\nFlexFlow, a deep learning framework that uses guided randomized search of the\nSOAP space to find a fast parallelization strategy for a specific parallel\nmachine. To accelerate this search, FlexFlow introduces a novel execution\nsimulator that can accurately predict a parallelization strategy's performance\nand is three orders of magnitude faster than prior approaches that have to\nexecute each strategy. We evaluate FlexFlow with six real-world DNN benchmarks\non two GPU clusters and show that FlexFlow can increase training throughput by\nup to 3.8x over state-of-the-art approaches, even when including its search\ntime, and also improves scalability.",
        "full_text": "Beyond Data and Model Parallelism for Deep Neural Networks\nZhihao Jia\nMatei Zaharia\nStanford University\nAlex Aiken\nAbstract\nThe computational requirements for training deep neu-\nral networks (DNNs) have grown to the point that it is\nnow standard practice to parallelize training. Existing\ndeep learning systems commonly use data or model par-\nallelism, but unfortunately, these strategies often result in\nsuboptimal parallelization performance.\nIn this paper, we de\ufb01ne a more comprehensive search\nspace of parallelization strategies for DNNs called SOAP,\nwhich includes strategies to parallelize a DNN in the\nSample, Operation, Attribute, and Parameter dimensions.\nWe also propose FlexFlow, a deep learning framework\nthat uses guided randomized search of the SOAP space\nto \ufb01nd a fast parallelization strategy for a speci\ufb01c parallel\nmachine. To accelerate this search, FlexFlow introduces\na novel execution simulator that can accurately predict a\nparallelization strategy\u2019s performance and is three orders\nof magnitude faster than prior approaches that have to\nexecute each strategy. We evaluate FlexFlow with six\nreal-world DNN benchmarks on two GPU clusters and\nshow that FlexFlow can increase training throughput by\nup to 3.8\u00d7 over state-of-the-art approaches, even when\nincluding its search time, and also improves scalability.\n1\nIntroduction\nOver the past few years, deep neural networks (DNNs)\nhave driven advances in many practical problems, such as\nimage classi\ufb01cation [28, 38], speech recognition [20, 8],\nmachine translation [42, 9], and game playing [37]. Be-\ncause sophisticated DNN models [23, 40] and larger train-\ning datasets [16, 11] have increased the computational\nrequirements to train DNN models, it is now standard\npractice to parallelize training across distributed hetero-\ngeneous clusters [7, 15].\nAlthough DNN applications and the clusters used to\nparallelize them are increasingly complex, the strate-\ngies used by today\u2019s deep learning systems (e.g., Ten-\nsorFlow [7], PyTorch [6], Caffe2 [2], MXNet [12]) to\nparallelize training remain simple. The most common\nparallelization technique is data parallelism [28], which\nplaces a replica of the entire neural network on each de-\nvice, so that each device processes a subset of the training\ndata and synchronizes network parameters in different\nreplicas at the end of an iteration. Data parallelism is ef\ufb01-\ncient for compute-intensive DNN operations with a few\ntrainable parameters (e.g., convolution) but achieves sub-\noptimal parallelization performance for operations with a\nlarge number of parameters (e.g., matrix-multiplication).\nAnother common parallelization strategy is model par-\nallelism [15], which assigns disjoint subsets of a neural\nnetwork each to a dedicated device. Model parallelism\neliminates parameter synchronization between devices\nbut requires data transfers between operations and disal-\nlows parallelism within an operation.\nPrevious work [27, 42] has proposed expert-designed\nstrategies that manually optimize parallelization based on\nhuman experts\u2019 domain knowledge and intuitions. For\nexample, [27] uses data parallelism for convolutional\nand pooling layers and switches to model parallelism for\nfully-connected layers to accelerate training convolutional\nneural networks. Expert-designed strategies achieve im-\nproved performance compared to data and model paral-\nlelism but still result in suboptimal behaviors. Section 8\nshows that we are able to \ufb01nd parallelization strategies\nthat are up to 2.3\u00d7 faster than expert-designed strategies.\nIn addition to these manually designed parallelization\nstrategies, recent work has proposed automated frame-\nworks [33, 25] for \ufb01nding ef\ufb01cient parallelization strate-\ngies in a limited search space.\nFor example, REIN-\nFORCE [33] uses a reinforcement learning model to learn\nef\ufb01cient operation assignments for model parallelism\nby running diverse strategies on real devices. As an-\nother example, OptCNN [25] is designed for parallelizing\nDNNs with linear computation graphs (e.g., AlexNet [28],\nVGG [38]) and automatically \ufb01nds strategies that exploit\nparallelism within each DNN operation. Existing auto-\nmated frameworks only explore either parallelism across\ndifferent operations (e.g., REINFORCE) or parallelism\nwithin a single operation (e.g., OptCNN) and therefore\nmiss faster strategies that use parallelism in both dimen-\nsions. We show that exploring a broader search space\ndiscovers parallelization strategies 1.2-3.8\u00d7 faster than\nexisting automated frameworks (see Section 8).\nIn this paper, we present FlexFlow, a deep learning\nframework that automatically \ufb01nds fast parallelization\nstrategies over a signi\ufb01cantly broader search space than\nprevious systems. To formalize the problem, we \ufb01rst de-\n\ufb01ne the SOAP (Sample-Operation-Attribute-Parameter)\nsearch space of parallelization strategies for DNNs. The\noperation dimension describes how different operations\nin a DNN are parallelized. In addition, for a single DNN\noperation, the sample and parameter dimensions indi-\ncate how training samples and model parameters are dis-\narXiv:1807.05358v1  [cs.DC]  14 Jul 2018\ntributed across devices. Finally, the attribute dimension\nde\ufb01nes how different attributes within a sample are par-\ntitioned. Compared to existing systems that parallelize\nDNNs in a subset of SOAP dimensions, FlexFlow con-\nsiders parallelizing DNNs in all these dimensions and\ntherefore de\ufb01nes a more comprehensive search space that\nincludes existing approaches as special cases.\nA key challenge with the much larger SOAP search\nspace is effectively evaluating candidate parallelization\nstrategies to \ufb01nd an ef\ufb01cient one. Prior work such as RE-\nINFORCE [33] relies on executing each parallelization\nstrategy on the hardware for one iteration to measure its\nexecution time. Unfortunately, this approach becomes\nprohibitively expensive with the multiple orders of mag-\nnitude larger SOAP search space.\nTo address this problem, FlexFlow introduces a novel\nexecution simulator that is accurate for predicting the per-\nformance of a parallelization strategy and is three orders\nof magnitude faster than pro\ufb01ling real executions. The\nchallenge in designing the simulator is how to accurately\nestimate the execution time of different DNN operators\n(e.g., convolution and matrix multiplication), which scale\nnon-linearly in a hardware-dependent way with the data.\nThe FlexFlow simulator relies on the following two facts:\n(1) many DNN models use a small number of distinct\noperators (e.g., a neural machine translation model [42]\nwith hundreds of operators only uses four distinct opera-\ntors); and (2) the execution time of each DNN operator\nis typically low-variance and largely independent of the\ncontents of the input data.\nThe FlexFlow simulator measures the execution time\nof an operation once for each input size and uses the mea-\nsured time to predict all operations with the same type,\nwhich only takes tens of milliseconds. These estimates\nare then used to predict the performance of a wide variety\nof parallelization strategies. In addition, the execution\nsimulator uses a delta simulation algorithm that simu-\nlates a new strategy using incremental updates to previous\nsimulations. Compared to existing approaches [33, 32]\nthat measure the performance from real executions, our\napproach has two advantages. First, the FlexFlow simula-\ntor is much faster. As a comparison, REINFORCE [33]\nrequires 12-27 hours to \ufb01nd an ef\ufb01cient operation as-\nsignment for model parallelism on 4 GPUs, while the\nFlexFlow simulator enables exploring a more comprehen-\nsive search space and \ufb01nding better parallelization strate-\ngies (with 3.4-3.8x higher throughput than REINFORCE)\nin 14-40 seconds. Furthermore, REINFORCE uses 160\ncompute nodes (with 4 GPUs on each node) to \ufb01nd an\nef\ufb01cient strategy in tens of hours, while our experiments\nuse only a single compute node for the simulator.\nThe execution simulator also achieves high accuracy\nfor predicting parallelization performance. We evaluate\nthe simulator with six real-world DNNs on two differ-\nent GPU clusters and show that, for all the measured\nexecutions, the relative difference between the real and\nsimulated execution time is less than 30%. Most impor-\ntantly for the search, we test different strategies for a given\nDNN application and show that their simulated execution\ntime preserves real execution time ordering.\nUsing the execution simulator as an oracle, the\nFlexFlow execution optimizer uses a general Markov\nChain Monte Carlo (MCMC) search algorithm (other\nsearch strategies could also be used) to explore the SOAP\nsearch space and iteratively propose candidate strategies\nbased on the simulated performance of previous candi-\ndates. When the search procedure is \ufb01nished, the execu-\ntion optimizer returns the best strategy it has discovered.\nWe evaluate FlexFlow on a variety of real-world DNN\nbenchmarks including image classi\ufb01cation [28, 22, 40],\ntext classi\ufb01cation [26], language modeling [43], and neu-\nral machine translation [42]. Compared to data/model\nparallelism and expert-designed parallelization strate-\ngies [27, 42], FlexFlow increases training throughput\nby up to 3.3\u00d7, reduces communication costs by up to\n5\u00d7, and achieves signi\ufb01cantly better scaling. In addition,\nFlexFlow also outperforms the strategies found by REIN-\nFORCE by 3.4-3.8\u00d7 on the same hardware con\ufb01guration\nevaluated in REINFORCE, and outperforms OptCNN by\n1.2-1.6\u00d7, by supporting a broader search space.\nTo summarize, our contributions are:\n\u2022 We de\ufb01ne the SOAP search space for parallelizing\nDNN applications, which includes strategies that par-\nallelize in any combination of the sample, operation,\nattribute, and parameter dimensions.\n\u2022 We show that under reasonable assumptions it is\npossible to reliably predict the execution time of\nparallelized DNNs using a simulator that is three\norders of magnitude faster than actually running the\nDNNs directly on the hardware.\n\u2022 We describe FlexFlow, a deep learning framework\nthat can search for and execute strategies from the\nentire SOAP space to accelerate DNN training.\n\u2022 We show that FlexFlow can increase training\nthroughput by up to 3.8\u00d7 over state-of-the-art paral-\nlelization approaches while improving scalability.\n2\nRelated Work\nData and model parallelism have been widely used by\nexisting deep learning systems (e.g., TensorFlow [7],\nCaffe2 [2], and PyTorch [6]) to distribute the training\nprocess across devices. Data parallelism [28] keeps a\n2\nParallelization \nApproach \nParallelism \nDimensions \nHybrid \nParallelism \nSupported \nDNNs \nData Parallelism \nS \nall \nModel Parallelism \nO, P \nall \nExpert-Designed [27, 42] S, O, P \nall \nREINFORCE \nO \nall \nOptCNN \nS, A, P \n\u2713 \nlinear \nFlexFlow \nS, O, A, P \n\u2713 \nall \nFigure 1: The parallelism dimensions explored by dif-\nferent approaches. S, O, A, and P indicate parallelism\nin the Sample, Operation, Attribute, and Parameter di-\nmensions (see Section 4). Hybrid parallelism shows if an\napproach supports parallelizing an operation in a combi-\nnation of the sample, attribute, and parameter dimensions\n(see FIgure 3). OptCNN is designed for DNNs with linear\ncomputation graphs.\ncopy of an entire DNN on each device, which is inef\ufb01-\ncient for operations with a large number of parameters\n(e.g., densely-connected layers) and becomes a scalability\nbottleneck in large scale distributed training. Model paral-\nlelism [9, 15] splits a DNN into disjoint subsets and trains\neach subset on a dedicated device, which reduces commu-\nnication costs for synchronizing network parameters in a\nDNN but exposes limited parallelism.\nExpert-designed parallelization strategies manually\noptimize parallelization for speci\ufb01c DNNs by using ex-\nperts\u2019 domain knowledge and experience. For example,\n[27] introduces \u201cone weird trick\u201d that uses data paral-\nlelism for convolutional and pooling layers and switches\nto model parallelism for densely-connected layers to ac-\ncelerate convolutional neural networks. To parallelize\nrecurrent neural networks, [42] uses data parallelism\nthat replicates the entire DNN on each compute node\nand switches to model parallelism for intra-node paral-\nlelization. Although these expert-designed parallelization\nstrategies achieve performance improvement over data\nand model parallelism, they are suboptimal. We use these\nexpert-designed strategies as baselines in our experiments\nand show that FlexFlow can further improve training per-\nformance by up to 2.3\u00d7.\nAutomated frameworks have been proposed for \ufb01nd-\ning ef\ufb01cient parallelization strategies in a limited search\nspace. REINFORCE [33] uses reinforcement learning\nto \ufb01nd ef\ufb01cient device placement for model parallelism.\nOptCNN [25] is designed for parallelizing DNNs with lin-\near computation graphs and automatically \ufb01nds ef\ufb01cient\nstrategies that exploit parallelism within an operation.\nFigure 1 summarizes the parallelism dimensions ex-\nplored by existing approaches. Data parallelism uses\nthe sample dimension to parallelize the training process,\nwhile model parallelism exploits the parameter and op-\neration dimensions. Expert-designed strategies [27, 42]\nexploit parallelism in the sample or parameter dimension\nto parallelize an operation but do not support hybrid par-\nallelism that uses a combination of the sample, attribute,\nand parameter dimensions to parallelize an operation (see\nFigure 3). Compared to these manually designed strate-\ngies, FlexFlow considers more sophisticated, and often\nmore ef\ufb01cient, strategies to parallelize a single opera-\ntion. In addition, compared to existing automated frame-\nworks [33, 25], FlexFlow explores a signi\ufb01cantly broader\nsearch space and is able to \ufb01nd strategies that are up to\n3.8\u00d7 faster.\nGraph-based cluster schedulers. Previous work [24,\n18] has proposed cluster schedulers that schedule cluster-\nwide tasks by using graph-based algorithms. For example,\nQuincy [24] maps task scheduling to a \ufb02ow network and\nuses a min-cost max-\ufb02ow (MCMF) algorithm to \ufb01nd ef\ufb01-\ncient task placement. Firmament [18] generalizes Quincy\nby employing multiple MCMF optimization algorithms\nto reduce task placement latencies. Existing graph-based\nschedulers optimize task placement by assuming a \ufb01xed\ntask graph. However, FlexFlow solves a different problem\nthat requires jointly optimizing how to partition an op-\neration into tasks by exploiting parallelism in the SOAP\ndimensions and how to assign tasks to devices.\n3\nOverview\nIn this section, we compare the FlexFlow programming\ninterface with other frameworks in Section 3.1, provide a\ngeneral overview of FlexFlow in Section 3.2, and discuss\nthe limitations of our approach in Section 3.3.\n3.1\nProgramming Interface\nSimilar to existing deep learning systems [7, 6, 2],\nFlexFlow uses an operator graph G to describe all op-\nerations and state in a DNN. Each node oi \u2208G is an oper-\nation (e.g., matrix multiplication, convolution, etc.), and\neach edge (oi, oj) \u2208G is a tensor (i.e., a n-dimensional\narray) that is an output of oi and an input of oj.\nAs far as we know, most deep learning systems (e.g.,\nTensorFlow [7], PyTorch [6], and Caffe2 [2]) use data par-\nallelism as the default parallelization strategy and support\nmodel parallelism as an alternative by allowing users to\nmanually specify the device placement for each operation.\nIn contrast, FlexFlow takes a device topology D =\n(DN, DE) describing all available hardware devices and\ntheir interconnections, as shown in Figure 2. Each node\ndi \u2208DN represents a device (e.g., a CPU or a GPU), and\neach edge (di, dj) \u2208DE is a hardware connection (e.g.,\na NVLink, a PCI-e, or a network link) between device di\nand dj. The edges are labeled with the bandwidth and\n3\nMCMC \nSearch Alg. \nDistributed Runtime \nBest Found Strategy \nCandidate \nStrategy \nSimulated \nPerformance \nExecution Optimizer \n \nExecution \nSimulator \nOperator Graph \nDevice Topology \nGPU \nGPU \nCPU \nNetwork \nGPU \nGPU \nCPU \nConv \nConv \nConcat \nMatMul \nFigure 2: FlexFlow overview.\nlatency of the connection.\nFlexFlow automatically \ufb01nds a parallelization strategy\nfor an operator graph and a device topology. Compared\nto existing frameworks, FlexFlow has two advantages:\nProgrammability. For DNN applications with com-\nplex operator graphs running on clusters with deep device\ntopologies, it is dif\ufb01cult for application developers, even\ndomain experts, to manually design ef\ufb01cient operation\nassignments. FlexFlow takes the responsibility for \ufb01nd-\ning ef\ufb01cient parallelization strategies and provides a more\nproductive programming interface.\nPortability.\nA parallelization strategy \ufb01ne-tuned\nfor one cluster may behave poorly on other clusters.\nFlexFlow\u2019s search method automatically selects an ef-\n\ufb01cient strategy for each hardware con\ufb01guration, without\nrequiring application changes.\n3.2\nFlexFlow Architecture\nThe main components of FlexFlow are shown in Figure 2.\nThe FlexFlowexecution optimizer takes an operator graph\nand a device topology as inputs and automatically gen-\nerates an ef\ufb01cient parallelization strategy. The optimizer\nuses a MCMC search algorithm to explore the space of\npossible parallelization strategies and iteratively proposes\ncandidate strategies that are evaluated by a execution sim-\nulator. The execution simulator uses a delta simulation\nalgorithm that simulates a new strategy using incremental\nupdates to previous simulations. The simulated execution\ntime guides the search in generating future candidates.\nWhen the search time budget is exhausted, the execu-\ntion optimizer sends the best discovered strategy to a\ndistributed runtime for parallelizing the actual executions.\n3.3\nLimitations\nThe main limitation of our approach is that the execution\nsimulator assumes the execution time of each operation is\npredictable and independent of the contents of input ten-\nTable 1: Parallelizable dimensions for different opera-\ntions. The sample and channel dimension index different\nsamples and neurons in a tensor, respectively. For 1D and\n2D images, the length and the combination of height and\nwidth dimensions specify a position in an image.\nOperation\nParallelizable Dimensions\n(S)ample\n(A)ttribute\n(P)arameter\n1D pooling\nsample\nlength, channel\n1D convolution\nsample\nlength\nchannel\n2D convolution\nsample\nheight, width\nchannel\nMatrix multiplication\nsample\nchannel\nSample \nSample \nSample \nSample \nLength \nChannel \nChannel \nLength \nChannel \nLength \nChannel \nLength \nData Parallelism \n(S) \nModel Parallelism \n(P) \nHybrid Parallelism \n(S, P) \nHybrid Parallelism \n(S, A, P) \nFigure 3: Example parallelization con\ufb01gurations for 1D\nconvolution. Dashed lines show partitioning the tensor.\nsors, as we discuss in Section 5. Therefore, our approach\nmay not be applicable to applications whose execution\ntime is data dependent. However, for the DNN applica-\ntions that are the subject of study here, which are based\non dense matrix operations, execution time is highly pre-\ndictable and independent of the contents of the matrices.\n4\nThe SOAP Search Space\nThis section introduces the SOAP search space of par-\nallelization strategies for DNNs. To parallelize a DNN\noperation across devices, we require each device to com-\npute a disjoint subset of the operation\u2019s output tensors.\nTherefore, we model the parallelization of an operation\noi by de\ufb01ning how the output tensor of oi is partitioned.\nFor an operation oi, we de\ufb01ne its parallelizable dimen-\nsions Pi as the set of all divisible dimensions in its output\ntensor. Pi always includes a sample dimension. For all\nother dimensions in Pi, we call it a parameter dimension\nif partitioning over that dimension requires splitting the\nmodel parameters and call it an attribute dimension other-\nwise. Table 1 shows the parallelizable dimensions of some\nexample operations. Finally, we also consider parallelism\nacross differ operations in the operation dimension.\nA parallelization con\ufb01guration ci of an operation oi\nde\ufb01nes how the operation is parallelized across multiple\ndevices. Figure 3 shows some example con\ufb01gurations for\nparallelizing a 1D convolution operation in a single di-\nmension as well as combinations of multiple dimensions.\nFor each parallelizable dimension in Pi, ci includes a\npositive integer that is the degree of parallelism in that\ndimension. |ci| is the product of the parallelism degrees\nfor all parallelizable dimensions of ci. We use equal size\n4\nSample (S) \nt1:1 (GPU1) \nt1:3 (GPU3) \nt1:2 (GPU2) \nt1:4 (GPU4) \nConfig c1: \n \n deg(Sample) = 2 \ndeg(Channelout) = 2 \nt1:1 = GPU1 \nt1:2 = GPU2 \nt1:3 = GPU3 \nt1:4 = GPU4 \n \nSample (S) \nt1:1  t1:3  \nChannelout (P) \nY (output) \nX (input) \nW (input) \nt1:2  t1:4  \nt1:1 \nt1:3  t1:4 \nt1:2 \nt1:3 \nt1:4 \nt1:1  t1:2 \nChannelin (P) \nChannelout (P) \nChannelin (P) \nFigure 4: An example parallelization con\ufb01guration for a\nmatrix multiplication operation.\npartitions in each dimension to guarantee well-balanced\nworkload distributions. A parallelization con\ufb01guration\nci partitions the operation oi into |ci| independent tasks,\ndenoted as ti:1, ..., ti:|ci|, meanwhile ci also includes the\ndevice assignment for each task ti:k (1 \u2264k \u2264|ci|). Given\nthe output tensor of a task and its operation type, we can\ninfer the necessary input tensors to execute each task.\nFigure 4 shows an example parallelization con\ufb01gura-\ntion for a matrix multiplication operation (i.e., Y = WX).\nThe operation is partitioned into four independent tasks\nassigned to dedicated GPU devices. The input and output\ntensors of the tasks are shown in the \ufb01gure.\nA parallelization strategy S describes one possible par-\nallelization of an application. S includes a parallelization\ncon\ufb01guration ci for each operation oi, and each oi\u2019s con-\n\ufb01guration can be chosen independently from among all\npossible con\ufb01gurations for oi.\n5\nExecution Simulator\nIn this section, we describe the execution simulator, which\ntakes an operator graph G, a device topology D, and a\nparallelization strategy S as inputs and predicts the ex-\necution time to run G on D using strategy S. FlexFlow\nsimulates the execution process instead of measuring the\nelapsed time from real executions for two reasons. First,\nprocessing one iteration of a DNN application can take\nseconds even on modern GPUs [19, 7]. The simulator\nruns up to three orders of magnitude faster than real ex-\necutions and allows the execution optimizer to explore\nmany more candidates in a given time budget. Second, the\nexecution simulator requires fewer computation resources.\nA large-scale execution on thousands of devices can be\nsimulated on a single node.\nThe simulator depends on the following assumptions:\nA1. The execution time of each task is predictable with\nlow variance and is independent of the contents of\ninput tensors.\nA2. For each connection (di, dj) between device di and\ndj with bandwidth b, transferring a tensor of size s\nfrom di to dj takes s/b time (i.e., the communication\nbandwidth can be fully-utilized).\nA3. Each device processes the assigned tasks with a\nFIFO (\ufb01rst-in-\ufb01rst-out) scheduling policy. This is\nthe policy used by modern devices such as GPUs.\nA4. The runtime has negligible overhead. A device be-\ngins processing a task as soon as its input tensors are\navailable and the device has \ufb01nished previous tasks.\nTo simulate an execution, the simulator \ufb01rst builds a\ntask graph, which includes all tasks derived from oper-\nations and dependencies between tasks, and then runs a\nsimulation algorithm to generate an execution timeline.\nSection 5.1 describes task graph construction. Section 5.2\nintroduces a full simulation algorithm that builds time-\nlines from scratch. Finally, Section 5.3 introduces an\nalternative delta simulation algorithm that generates a\nnew timeline using incremental updates to a previous one.\n5.1\nTask Graph\nA task graph models dependencies between individual\ntasks derived from operations and can also represent task\nexecution timelines on individual devices. To unify the\nabstraction, we treat each hardware connection between\ndevices as a communication device, and each data transfer\nas a communication task. Note that devices and hardware\nconnections are modeled as separate devices. This allows\ncomputation (i.e., normal tasks) and communication (i.e.,\ncommunication tasks) to be overlapped if possible.\nGiven an operator graph G, a device topology D, and a\nparallelization strategy S, we use the following steps to\nconstruct a task graph T = (TN, TE), where each node\nt \u2208TN is a task (i.e., a normal task or a communication\ntask) and each edge (ti, tj) \u2208TE is a dependency that\ntask tj cannot start until task ti is completed. Note that the\nedges in the task graph are simply ordering constraints\u2014\nthe edges do not indicate data \ufb02ow, as all data \ufb02ow is\nincluded in the task graph as communication tasks.\n1. For each operation oi \u2208G with parallelization con-\n\ufb01guration ci, we add tasks ti:1, ..., ti:|ci| into TN.\n2. For each tensor (oi, oj) \u2208G, which is an output\nof operation oi and an input of oj, we compute the\noutput sub-tensors written by tasks ti:ki (1 \u2264ki \u2264\n|ci|) and the input sub-tensors read by tasks tj:kj\n(1 \u2264kj \u2264|cj|). For every task pair ti:ki and tj:kj\nwith shared tensors, if two tasks are assigned to the\nsame device, we add an edge (ti:ki, tj:kj) into TE,\nindicating a dependency between the two tasks, and\nno communication task is needed. If ti:ki and tj:kj\n5\no1 \no2 \no3 \no4 \no5 \no6 \nEmbedding Layer \nRecurrent Layer \nLinear Layer \nConfig c1, c2:  \n# batch = 2 \n# channel = 1 \nti:k = GPU0 \n \nConfig c5, c6: \n# batch = 1 \n# channel = 1 \nti:k = GPU2 \nConfig c3, c4:  \n# batch = 2 \n# channel = 1 \nti:k = GPU1 \n \n(a) An example parallelization strategy.\nt1:1 \nt1:2 \nt2:1 \nt2:2 \nt3:1 \nt5:1 \nt3:2 \nt4:1 \nt6:1 \nt4:2 \ntc \ntc \ntc \ntc \ntc \ntc \ntc \ntc \nexe: 2 \nGPU0 \nexe: 1 \nGPU1 \nexe: 3 \nGPU2 \nexe: 1 \nXfer 0\u21921 \nexe: 1 \nXfer 1\u21922 \n(b) The corresponding task graph.\nr: 0 \ns: 0 \nr: 0 \ns: 2 \nr: 0 \ns: 4 \nr: 0 \ns: 6 \nr: 3 \ns: 3 \nr: 7 \ns: 7 \nr: 5 \ns: 5 \nr: 7 \ns: 7 \nr: 11 \ns: 11 \nr: 9 \ns: 9 \nr: 2 \ns: 2 \nr: 4 \ns: 4 \nr: 6 \ns: 6 \nr: 8 \ns: 8 \nr: 4 \ns: 4 \nr: 6 \ns: 6 \nr: 8 \ns: 8 \nr: 10 \ns: 10 \nexe: 2 \nGPU0 \nexe: 1 \nGPU1 \nexe: 3 \nGPU2 \nexe: 1 \nXfer 0\u21921 \nexe: 1 \nXfer 1\u21922 \n(c) The task graph after the\nfull simulation algorithm.\nr: 0 \ns: 0 \nr: 0 \ns: 2 \nr: 0 \ns: 4 \nr: 0 \ns: 6 \nr: 5 \ns: 5 \nr: 7 \ns: 7 \nr: 9 \ns: 9 \nr: 9 \ns: 9 \nr: 11 \ns: 12 \nr: 2 \ns: 2 \nr: 4 \ns: 4 \nr: 6 \ns: 6 \nr: 8 \ns: 8 \nr: 7 \ns: 7 \nr: 10 \ns: 10 \nr: 8 \ns: 9 \nexe: 2 \nGPU0 \nexe: 2 \nGPU1 \nexe: 3 \nGPU2 \nexe: 1 \nXfer 0\u21921 \nexe: 2 \nXfer 1\u21922 \n(d) The task graph after the\ndelta simulation algorithm.\nFigure 5: Simulating an example parallelization strategy. The tasks\u2019 exeTime and device are shown on the top\nof each column. In Figure 5c and 5d, the word \u201cr\u201d and \u201cs\u201d indicate the readyTime and startTime of each task,\nrespectively, and the dashed edges represents the nextTask.\nTable 2: Properties for each task in the task graph.\nProperty\nDescription\nProperties set in graph construction\nexeTime\nThe elapsed time to execute the task.\ndevice\nThe assigned device of the task.\nI(t)\n{tin|(tin, t) \u2208TE}\nO(t)\n{tout|(t, tout) \u2208TE}\nProperties set in simulation\nreadyTime\nThe time when the task is ready to run.\nstartTime\nThe time when the task starts to run.\nendTime\nThe time when the task is completed.\npreTask\nThe previous task performed on device.\nnextTask\nThe next task performed on device.\nInternal properties used by the full simulation algorithm\nstate\nCurrent state of the task, which is one of\nNOTREADY, READY, and COMPLETE.\nwith shared tensors are assigned to different devices,\nwe add a communication task tc to TN and two edges\n(ti:ki, tc) and (tc, tj:kj) to TE. The new task tc is\nassigned to the communication device between the\ndevices that perform ti:ki and tj:kj.\nFigure 5a shows an example parallelization strategy for\na standard 3-layer recurrent neural network consisting of\nan embedding layer, a recurrent layer, and a linear layer.\nThe parallelization strategy represents commonly used\nmodel parallelism that assigns operations in each layer to\na dedicated GPU. Figure 5b shows the corresponding task\ngraph. Each square and hexagon indicate a normal task\nand a communication task, respectively, and each directed\nedge represents a dependency between tasks.\nTable 2 lists the properties for each task in the task\ngraph. The exeTime property is set during the graph\nconstruction. For a normal task derived from an operation,\nits exeTime is the time to execute the task on the given\ndevice and is estimated by running the task multiple times\non the device and measuring the average execution time\n(assumption A1). A task\u2019s exeTime is cached, and all\nfuture tasks with the same operation type and output size\nwill use the cached value without rerunning the task. For a\ncommunication task, its exeTime is the time to transfer\na tensor (of size s) between devices with bandwidth b and\nAlgorithm 1 Full Simulation Algorithm.\n1: Input: An operator graph G, a device topology D, and a paralleliza-\ntion strategy S.\n2: T = BUILDTASKGRAPH(G, D, S)\n3: readyQueue = {} // a priority queue sorted by readyTime\n4: for t \u2208TN do\n5:\nt.state = NOTREADY\n6:\nif I(t) = {} then\n7:\nt.state = READY\n8:\nreadyQueue.enqueue(t)\n9: while readyQueue \u0338= {} do\n10:\nTask t = readyQueue.dequeue()\n11:\nDevice d = t.device\n12:\nt.state = COMPLETE\n13:\nt.startTime = max{t.readyTime, d.last.endTime}\n14:\nt.endTime = t.startTime + t.exeTime\n15:\nd.last = t\n16:\nfor n \u2208O(t) do\n17:\nn.readyTime = max{n.readyTime, t.endTime}\n18:\nif all tasks in I(n) are COMPLETE then\n19:\nn.state = READY\n20:\nreadyQueue.enqueue(n)\n21: return max{t.endTime | t \u2208TN}\nis estimated as s/b (assumption A2).\nIn addition to the exeTime property, FlexFlow also\nsets the device, I(t), and O(t) (de\ufb01ned in Table 2)\nduring graph construction. Other properties in Table 2\nremain unset and must be \ufb01lled in by the simulation.\n5.2\nFull Simulation Algorithm\nWe now describes a full simulation algorithm that we will\nuse as a baseline for comparisons with our delta simula-\ntion algorithm. Algorithm 1 shows the pseudocode. It\n\ufb01rst builds a task graph using the method described in\nSection 5.1 and then sets the properties for each task using\na variant of Dijkstra\u2019s shortest-path algorithm [14]. Tasks\nare enqueued into a global priority queue when ready (i.e.,\nall predecessor tasks are completed) and are dequeued in\nincreasing order by their readyTime. Therefore, when\na task t is dequeued, all tasks with an earlier readyTime\nhave been scheduled, and we can set the properties for\n6\nAlgorithm 2 Delta Simulation Algorithm.\n1: Input: An operator graph G, a device topology D, an original task\ngraph T , and a new con\ufb01guration c\u2032\ni for operation oi.\n2: updateQueue = {} // a priority queue sorted by readyTime\n3: /*UPDATETASKGRAPH returns the updated task graph and a list\nof tasks with new readyTime*/\n4: T , L = UPDATETASKGRAPH(T , G, D, ci, c\u2032\ni)\n5: updateQueue.enqueue(L)\n6: while updateQueue \u0338= {} do\n7:\nTask t = updateQueue.dequeue()\n8:\nt.startTime = max{t.readyTime, t.preTask.endTime}\n9:\nt.endTime = t.startTime + t.exeTime\n10:\nfor n \u2208O(t) do\n11:\nif UPDATETASK(n) then\n12:\nupdateQueue.push(n)\n13:\nif UPDATETASK(t.nextTask) then\n14:\nupdateQueue.push(t.nextTask)\n15: return max{t.endTime | t \u2208TN}\n16:\n17: function UPDATETASK(t)\n18:\nt.readyTime = max{p.endTime | p \u2208I(t)}\n19:\n/*Swap t with other tasks on the device to maintain FIFO.*/\n20:\nt.startTime = max{t.readyTime, t.preTask.endTime}\n21:\nif t\u2019s readyTime or startTime is changed then\n22:\nreturn True\n23:\nelse\n24:\nreturn False\ntask t while maintaining the FIFO scheduling order (as-\nsumption A3). Figure 5c shows the execution timeline of\nthe example parallelization strategy.\n5.3\nDelta Simulation Algorithm\nFlexFlow uses a MCMC search algorithm that proposes a\nnew parallelization strategy by changing the paralleliza-\ntion con\ufb01guration of a single operation in the previous\nstrategy (see Section 6.2). As a result, in the common case,\nmost of the execution timeline does not change from one\nsimulated strategy to the next. Based on this observation,\nwe introduce a delta simulation algorithm that starts from\na previous task graph and only re-simulates tasks involved\nin the portion of the execution timeline that changes, an\noptimization that dramatically speeds up the simulator,\nespecially for strategies for large distributed machines.\nThe full and delta simulation algorithms always produce\nthe same timeline for a given task graph.\nAlgorithm 2 shows the pseudocode for the delta simu-\nlation algorithm. It \ufb01rst updates tasks and dependencies\nin the task graph and enqueues all modi\ufb01ed tasks into a\nglobal priority queue (line 4-5). Similar to the Bellman-\nFord shortest-path algorithm [14], the delta simulation\nalgorithm iteratively dequeues updated tasks and propa-\ngates the updates to subsequent tasks (line 6-14).\nFor the example in Figure 5, consider a new paralleliza-\ntion strategy derived from the original strategy (Figure 5a)\nby only reducing the parallelism of operation o3 to 1 (i.e.,\n|c3| = 1). Figure 5d shows the task graph for the new\nparallelization strategy, which can be generated from the\noriginal task graph (in Figure 5c) by updating the simula-\ntion properties of tasks in the grey area.\n6\nExecution Optimizer\nThis section describes the execution optimizer that takes\nan operator graph and a device topology as inputs and\nautomatically \ufb01nds an ef\ufb01cient parallelization strategy.\nUsing the simulator as an oracle, FlexFlow transforms the\nparallelization optimization problem into a cost minimiza-\ntion problem, namely minimizing the predicted execution\ntime. The primary advantage of this approach is that it\navoids explicitly encoding the trade-offs between interde-\npendent optimizations (e.g., reducing data transfers v.s.\nbalancing workload distributions) and simply focuses on\nminimizing the application\u2019s overall execution time.\nFinding the optimal parallelization strategy is NP-hard,\nby an easy reduction from minimum makespan [29]. In\naddition, as described in Section 4, the number of possi-\nble strategies is exponential to the number of operations\nin the operator graph, which makes it intractable to ex-\nhaustively enumerate the search space. To \ufb01nd a low-cost\nstrategy, FlexFlow uses a cost minimization search pro-\ncedure to heuristically explore the space and returns the\nbest strategy discovered.\n6.1\nMCMC Sampling\nThis section brie\ufb02y introduces the MCMC sampling\nmethod used by the execution optimizer. MCMC sam-\npling is a technique for obtaining samples from a proba-\nbility distribution so that higher probability samples are\nvisited proportionately more often than low probability\nsamples. A common method (described in [17]) to trans-\nform a cost function cost(\u00b7) into a probability distribution\nis the following, where \u03b2 is a constant that can be chosen:\np(S) \u221dexp\n\u0000\u2212\u03b2 \u00b7 cost(S)\n\u0001\n(1)\nMCMC works by starting at any point in the search\nspace (a random point, or perhaps a well-known start-\ning point) and then generating a sequence of points with\nthe guarantee that in the limit the set of points visited\napproaches the distribution given by p(\u00b7). In our setting,\nwe begin with some parallelization strategy S0 and then\ngenerate a sequence of strategies S0, S1, . . ..\nWe use the Metropolis-Hastings algorithm [21] for gen-\nerating Markov chains, which maintains a current strategy\nS and proposes a modi\ufb01ed strategy S\u2217from a proposal\ndistribution q(S|S\u2217). If the proposal is accepted, S\u2217be-\ncomes the new current strategy, otherwise another strategy\nbased on S is proposed. This process is repeated indef-\ninitely (e.g., until a time budget is exhausted). If the\n7\nTable 3: Details of the DNNs and datasets used in evaluation.\nDNN\nDescription\nDataset\nReported Acc.\nOur Acc.\nConvolutional Neural Networks (CNNs)\nAlexNet [28]\nA 12-layer CNN\nSynthetic data\n-\n-\nInception-v3 [40]\nA 102-layer CNN with Inception modules [39]\nImageNet [36]\n78.0%a\n78.0%a\nResNet-101 [22]\nA 101-layer residual CNN with shortcut connections\nImageNet [36]\n76.4%a\n76.5%a\nRecurrent Neural Networks (RNNs)\nRNNTC [26]\n4 recurrent layers followed by a softmax layer\nMovie Reviews [1]\n79.8%\n80.3%\nRNNLM [43]\n2 recurrent layers followed by a softmax layer\nPenn Treebank [31]\n78.4b\n76.1b\nNMT [42]\n4 recurrent layers followed by an attention and a softmax layer\nWMT English-German [3]\n19.67c\n19.85c\na top-1 accuracy for single crop on the validation dataset (higher is better).\nb word-level test perplexities on the Peen Treebank dataset (lower is better).\nc BLEU scores [34] on the test dataset (higher is better).\nproposal distribution is symmetric, q(S|S\u2217) = q(S\u2217|S),\nthe acceptance criteria of a new strategy is the following:\n\u03b1(S \u2192S\u2217) = min\n\u00001, p(S\u2217)/p(S)\n\u0001\n= min\n\u0010\n1, exp\n\u0000\u03b2 \u00b7 (cost(S) \u2212cost(S\u2217)\n\u0001\u0011\n(2)\nThe acceptance criteria has several important proper-\nties. If S\u2217has a lower cost than S, then S\u2217is always\naccepted. If S\u2217has a higher cost than S, then S\u2217may\nstill be accepted with a probability that decreases as a\nfunction of the difference between cost(S) and cost(S\u2217).\nIntuitively, MCMC tends to behave as a greedy search al-\ngorithm, preferring to move towards lower cost whenever\nthat is readily available, but can also escape local minima.\n6.2\nSearch Algorithm\nOur method for generating proposals is simple: an oper-\nation in the current parallelization strategy is selected at\nrandom, and its parallelization con\ufb01guration is replaced\nby a random con\ufb01guration. Our de\ufb01nition of the pro-\nposal distribution q(\u00b7) satis\ufb01es the symmetry property,\nq(S|S\u2217) = q(S\u2217|S), since, for any operation, its con\ufb01gu-\nrations are selected with the same probability.\nWe uses existing strategies (e.g., data parallelism,\nexpert-designed strategies) as well as randomly generated\nstrategies as the initial candidates for the search algorithm.\nFor each initial strategy, the search algorithm iteratively\nproposes new candidates until one of the following two\ncriteria is satis\ufb01ed: (1) the search time budget for current\ninitial strategy is exhausted; or (2) the search procedure\ncannot further improve the best discovered strategy for\nhalf of the search time.\n7\nFlexFlow Runtime\nWe found that existing deep learning systems (e.g., Ten-\nsorFlow [7], PyTorch [6], Caffe2 [2], and MXNet [12])\nonly support parallelizing an operation in the batch di-\nmension through data parallelism, and it is non-trivial to\nparallelize an operation in other dimensions or combina-\ntions of several dimensions in these systems. In addition,\nwe are not aware of any existing system that supports\nparallelization at the granularity of individual operations.\nTo support parallelizing DNN models using any strat-\negy de\ufb01ned in our parallelization space (see Section 4),\nwe implemented the FlexFlow distributed runtime in Le-\ngion [10], a high-performance parallel runtime for dis-\ntributed heterogeneous architectures, and use cuDNN [13]\nand cuBLAS [4] as the underlying libraries for processing\nDNN operations. We use the Legion high-dimensional\npartitioning interface [41] to support parallelizing an oper-\nation in any combination of the parallelizable dimensions\nand use Legion\u2019s \ufb01ne-grain control mechanism to control\nparallelization at the granularity of each operation.\nThe key difference between the FlexFlow runtime and\nexisting systems is that FlexFlow supports parallelizing\nan operation in any combination of the parallelizable di-\nmensions and controls parallelization at the granularity of\nindividual operations.\n8\nEvaluation\nThis section evaluates the performance of FlexFlow on\nsix real-world DNN benchmarks and two GPU clusters.\nSection 8.1 describes the experimental setup for the eval-\nuation. Section 8.2 compares FlexFlow with state-of-the-\nart parallelization approaches. Section 8.3 evaluates the\naccuracy and ef\ufb01ciency of the execution simulator. Sec-\ntions 8.4 and 8.5 evaluate the quality of the best strategies\ndiscovered by the execution optimizer and discuss two of\nthe best discovered strategies.\n8.1\nExperimental Setup\nTable 3 summarizes the DNNs used in our experiments.\nAlexNet, Inception-v3, and ResNet-101 are three CNNs\nthat achieved the best accuracy in the ILSVRC compe-\ntitions [35]. For AlexNet, the per-iteration training time\nis smaller than the time to load training data from disk.\nWe follow the suggestions in [5] and use synthetic data\nto benchmark the performance of AlexNet. For all other\nexperiments, the training data is loaded from disk in the\ntraining procedure.\nRNNTC, RNNLM and NMT are sequence-to-sequence\nRNN models for text classi\ufb01cation, language model-\ning, and neural machine translation, respectively. RN-\nNTC uses four LSTM layers with a hidden size of 1024.\n8\nP100 \nP100 \nP100 \nP100 \nCPUs \nNetwork \nP100 \nP100 \nP100 \nP100 \nCPUs \n100 GB/s \n(a) The P100 Cluster (4 nodes).\nK80 \nK80 \nK80 \nK80 \nCPUs \nNetwork \nK80 \nK80 \nK80 \nK80 \nCPUs \n56 GB/s \n(b) The K80 Cluster (16 nodes).\nFigure 6: Architectures of the GPU clusters used in the\nexperiments. An arrow line indicates a NVLink connec-\ntion. A solid line is a PCI-e connection. Dashed lines are\nIn\ufb01niband connections across different nodes.\nRNNLM uses two LSTM layers with a hidden size of\n2048. Both RNN models include a softmax linear after\nthe last LSTM layer. NMT includes an encoder and a\ndecoder, both of which consist of 2 LSTM layers with\na hidden size of 1024. To improve model accuracy, we\nalso use an attention layer [9] on top of the last decoder\nLSTM layer. Figure 14 illustrates the structure of the\nNMT model. For all three RNN models, we set the num-\nber of unrolling steps for each recurrent layer to 40.\nWe follow prior work [28, 40, 22, 26, 43, 42] to con-\nstruct operator graphs and set hyperparameters (e.g., learn-\ning rates, weight decays). We use synchronous training\nand a batch size of 64 for all DNN benchmarks, except\nfor AlexNet, which uses a batch size of 256.\nTo evaluate the performance of FlexFlow with different\ndevice topologies, we performed the experiments on two\nGPU clusters, as shown in Figure 6. The \ufb01rst cluster con-\ntains 4 compute nodes, each of which is equipped with\ntwo Intel 10-core E5-2600 CPUs, 256GB main memory,\nand four NVIDIA Tesla P100 GPUs. GPUs on the same\nnode are connected by NVLink, and nodes are connected\nover 100GB/s EDR In\ufb01niband. The second cluster con-\nsists of 16 nodes, each of which is equipped with two Intel\n10-core E5-2680 GPUs, 256GB main memory, and four\nNVIDIA Tesla K80 GPUs. Adjacent GPUs are connected\nby a separate PCI-e switch, and all GPUs are connected to\nCPUs through a shared PCI-e switch. Compute nodes in\nthe cluster are connected over 56 GB/s EDR In\ufb01niband.\nUnless otherwise stated, we set 30 minutes as the time\nbudget for the execution optimizer and use data paral-\nlelism and a randomly generated parallelization strategy\nas the initial candidates for the search algorithm. As\nshown in Section 8.3.2, the search procedure terminates\nin a few minutes for most executions.\n8.2\nParallelization Performance\n8.2.1\nPer-iteration Performance\nWe compare the per-iteration training performance of\nFlexFlow with the following baselines. Data parallelism is\ncommonly used in existing deep learning systems [7, 2, 6].\n1(1)\n2(1)\n4(1)\n8(2) 16(4) 32(8)64(16)\n0\n500\n1000\n1500\n2000\n2500 AlexNet (batch size = 256)\n1(1)\n2(1)\n4(1)\n8(2) 16(4) 32(8)64(16)\n0\n50\n100\n150\n200Inception_v3 (batch size = 64)\n1(1)\n2(1)\n4(1)\n8(2) 16(4) 32(8)64(16)\n0\n50\n100\n150\n200ResNet-101 (batch size = 64)\n1(1)\n2(1)\n4(1)\n8(2) 16(4) 32(8)64(16)\n0\n100\n200\n300\n400\n500\n600\nRNNTC (batch size = 64)\n1(1)\n2(1)\n4(1)\n8(2) 16(4) 32(8)64(16)\n0\n50\n100\n150\n200\n250\n300\n350\n400\nRNNLM (batch size = 64)\n1(1)\n2(1)\n4(1)\n8(2) 16(4) 32(8)64(16)\n0\n50\n100\n150\n200\n250\n300\n350\n400\nNMT (batch size = 64)\nNum. Samples/second/GPU\nNum. Devices\nData Parallelism (P100)\nExpert-designed Strategy (P100)\nFlexFlow (P100)\nData Parallelism (K80)\nExpert-designed Strategy (K80)\nFlexFlow (K80)\nFigure 7: Per-iteration training performance on six DNN\nbenchmarks. Numbers in parenthesis are the number of\ncompute nodes used in the experiments. The dash lines\nshow the ideal training throughput.\nTo control for implementation differences, we ran data\nparallelism experiments in TensorFlow r1.7, PyTorch\nv0.3, and our implementation and compared the perfor-\nmance numbers. Compared to TensorFlow and PyTorch,\nFlexFlow achieves the same or better performance num-\nbers on all six DNN benchmarks, and therefore we report\nthe data parallelism performance achieved by FlexFlow\nin the experiments.\nExpert-designed strategies optimize parallelization\nbased on domain experts\u2019 knowledge and experience.\nFor CNNs, [27] uses data parallelism for parallelizing\nconvolutional and pooling layers and switches to model\nparallelism for densely-connected layers. For RNNs, [42]\nuses data parallelism that replicates the entire operator\ngraph on each compute node and uses model parallelism\nthat assign operations with the same depth to the same\nGPU on each node. These expert-designed strategies\nare used as a baseline in our experiments. Model par-\nallelism only exposes limited parallelism by itself, and\nwe compare against model parallelism as a part of these\nexpert-designed strategies.\nFigure 7 shows the per-iteration training performance\non all six DNN benchmarks. For ResNet-101, FlexFlow\n\ufb01nds strategies similar to data parallelism (except using\nmodel parallelism on a single node for the last fully-\nconnected layer) and therefore achieves similar paral-\nlelization performance.\nFor other DNN benchmarks,\n9\nData\nParallelism\nExpert\nDesigned\nFlexFlow\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nPer-iteration Execution Time\n1.9\n2.6\n1.1\n(a) Per-iteration\nexecution time.\nData\nParallelism\nExpert\nDesigned\nFlexFlow\n0\n10\n20\n30\n40\n50\n60\n70\nTotal Data Transfers Per Iteration (GB)\n65.8\n24.2\n12.1\n(b) Overall data trans-\nfers per iteration.\nData\nParallelism\nExpert\nDesigned\nFlexFlow\n0\n5\n10\n15\n20\n25\n30\n35\n40\nTotal Task Computation Time\nPer Iteration (seconds)\n35.7\n28.2\n28.7\n(c) Overall task compu-\ntation time per iteration.\nFigure 8: Parallelization performance for the NMT model\non 64 K80 GPUs (16 nodes). FlexFlow reduces per-\niteration execution time by 1.7-2.4\u00d7 and data transfers by\n2-5.5\u00d7 compared to other approaches. FlexFlow achieves\nsimilar overall task computation time as expert-designed\nstrategy, which is 20% fewer than data parallelism.\nFlexFlow \ufb01nds more ef\ufb01cient strategies than the base-\nlines and achieves 1.3-3.3\u00d7 speedup. Note that FlexFlow\nperforms the same operations as data parallelism and\nexpert-designed strategies, and the performance improve-\nment is achieved by using faster parallelization strategies.\nWe found that the parallelization strategies discovered by\nFlexFlow have two advantages over data parallelism and\nexpert-designed strategies.\nReducing overall communication costs. Similar to\nexisting deep learning systems, the FlexFlow distributed\nruntime supports overlapping data transfers with compu-\ntation to hide communication overheads. However, as\nwe scale the number of devices, the communication over-\nheads increase, but the computation time used to hide\ncommunication remains constant. Therefore, reducing\noverall communication costs is bene\ufb01cial for large-scale\ndistributed training. Figure 8b shows that, to parallelize\nthe NMT model on 64 K80 GPUs (16 nodes), FlexFlow\nreduces the per-iteration data transfers by 2-5.5\u00d7 com-\npared to other parallelization approaches.\nReducing overall task computation time. Data par-\nallelism always parallelizes an operation in the batch di-\nmension. However, as reported in [25], parallelizing an\noperation through different dimensions can result in dif-\nferent task computation time. For the matrix multipli-\ncation operation in the NMT model, parallelizing it in\nthe channel dimension reduces the operation\u2019s overall\ncomputation time by 38% compared to parallelizing the\noperation in the batch dimension. Figure 8c shows that\nFlexFlow reduces the overall task computation time by\n20% compared to data parallelism for the NMT model.\nThe expert-designed strategy achieves slightly better total\ntask computation time than FlexFlow. However, this is\nachieved by using model parallelism on each node, which\ndisables any parallelism within each operation and results\nin imbalanced workloads. As a result, the expert-designed\nstrategy achieves even worse execution performance than\n0\n5\n10\n15\n20\nTraining Time (hours)\n0\n2\n4\n6\n8\n10\nAverage Training Loss\nTensorFlow\nFlexFlow\nFigure 9: Training curves of Inception-v3 in different sys-\ntems. The model is trained on 16 P100 GPUs (4 nodes).\nInception_v3\nNMT\n0\n50\n100\n150\n200\n250\n300\n350\n400\nTraining Throughput (per second)\nREINFORCE\nFlexFlow\n(a) REINFORCE\nInception_v3 RNNTC\nRNNLM\nNMT\n0\n1000\n2000\n3000\n4000\n5000\n6000\n7000\n8000\nTraining Throughput (per second)\nOptCNN\nFlexFlow\n(b) OptCNN\nFigure 10: Comparison among the parallelization strate-\ngies found by different automated frameworks.\ndata parallelism (see Figure 8a). FlexFlow reduces the\noverall task computation time while enabling parallelism\nwithin an operation and maintaining load balance.\n8.2.2\nEnd-to-end Performance\nFlexFlow performs the same computation as other deep\nlearning systems for a DNN model and therefore achieves\nthe same model accuracy. Table 3 veri\ufb01es that FlexFlow\nachieves the state-of-the-art accuracies on the DNN bench-\nmarks used in the experiments.\nIn this experiment, we compare the end-to-end train-\ning performance between FlexFlow and TensorFlow on\nInception-v3. We train Inception-v3 on the ImageNet\ndataset until the model reaches the single-crop top-1 accu-\nracy of 72% on the validation set. The training processes\nin both frameworks use stochastic gradient decent (SGD)\nwith a learning rate of 0.045 and a weight decay of 0.0001.\nFigure 9 illustrates the training curves of the two systems\non Inception-v3 and show that FlexFlow reduces the end-\nto-end training time by 38% compared to TensorFlow.\n8.2.3\nAutomated Parallelization Optimizer\nWe compare against two automated frameworks that \ufb01nd\nparallelization strategies in a limited search space.\nREINFORCE [33] uses reinforcement learning to\nlearn device placement for model parallelism. We are\nnot aware of any publicly available implementation of\nREINFORCE, so we compare against the learned device\nplacement for Inception-v3 and NMT, as reported in [33].\nFigure 10a compares the training throughput of the\nstrategies found by FlexFlow and REINFORCE for four\nK80 GPUs on a single node. The parallelization strategies\n10\nTable 4: The end-to-end search time with different simulation algorithms (seconds).\nNum.\nAlexNet\nResNet\nInception\nRNNTC\nRNNLM\nNMT\nGPUs\nFull\nDelta\nSpeedup\nFull\nDelta\nSpeedup\nFull\nDelta\nSpeedup\nFull\nDelta\nSpeedup\nFull\nDelta\nSpeedup\nFull\nDelta\nSpeedup\n4\n0.11\n0.04\n2.9\u00d7\n1.4\n0.4\n3.2\u00d7\n14\n4.1\n3.4\u00d7\n16\n7.5\n2.2\u00d7\n21\n9.2\n2.3\u00d7\n40\n16\n2.5\u00d7\n8\n0.40\n0.13\n3.0\u00d7\n4.5\n1.4\n3.2\u00d7\n66\n17\n3.9\u00d7\n91\n39\n2.3\u00d7\n76\n31\n2.5\u00d7\n178\n65\n2.7\u00d7\n16\n1.4\n0.48\n2.9\u00d7\n22\n7.3\n3.1\u00d7\n388\n77\n5.0\u00d7\n404\n170\n2.4\u00d7\n327\n121\n2.7\u00d7\n998\n328\n3.0\u00d7\n32\n5.3\n1.8\n3.0\u00d7\n107\n33\n3.2\u00d7\n1746\n298\n5.9\u00d7\n1358\n516\n2.6\u00d7\n1102\n342\n3.2\u00d7\n2698\n701\n3.8\u00d7\n64\n18\n5.9\n3.0\u00d7\n515\n158\n3.3\u00d7\n8817\n1278\n6.9\u00d7\n4404\n1489\n3.0\u00d7\n3406\n969\n3.6\u00d7\n8982\n2190\n4.1\u00d7\n0.4\n1\n2\n4\n8\nSimulated Execution Time (seconds)\n0.4\n1\n2\n4\n8\nReal Execution Time (seconds)\n4 x P100 (1 node)\n16 x P100 (4 nodes)\n4 x K80 (1 node)\n16 x K80 (4 nodes)\n(a) Inception-v3\n0.1\n0.2\n0.5\n1\n2\n4\nSimulated Execution Time (seconds)\n0.1\n0.2\n0.5\n1\n2\n4\nReal Execution Time (seconds)\n4 x P100 (1 node)\n16 x P100 (4 nodes)\n4 x K80 (1 node)\n16 x K80 (4 nodes)\n(b) NMT\nFigure 11: Comparison between the simulated and actual\nexecution time for different DNNs and device topologies.\nfound by FlexFlow achieve 3.4 - 3.8\u00d7 speedup compared\nto REINFORCE. We attribute the performance improve-\nment to the larger search space explored by FlexFlow.\nBesides improving training performance, FlexFlow has\ntwo additional advantages over REINFORCE. First, RE-\nINFORCE requires executing each strategy in the hard-\nware environment to get reward signals and takes 12-27\nhours to \ufb01nd the best placement [33], while the FlexFlow\nexecution optimizer \ufb01nds ef\ufb01cient parallelization strate-\ngies for these executions in 14-40 seconds. Second, REIN-\nFORCE uses up to 160 compute nodes (with 4 GPUs on\neach node) to \ufb01nd the placement in time, while FlexFlow\nuses a single compute node to run the execution optimizer.\nOptCNN [25] optimizes parallelization for DNNs with\nlinear operator graphs. OptCNN assumes that different\noperations in an operator graph cannot be performed in\nparallel and estimates a DNN\u2019s execution time as the sum\nof the operations\u2019 computation time and synchronization\ntime and the tensors\u2019 data transfer time. This assumption\nallows OptCNN to use a dynamic programming algorithm\nto \ufb01nd an ef\ufb01cient parallelization strategy.\nWe compare the strategies found by FlexFlow and\nOptCNN for different DNNs on 16 P100 GPUs. The\nframeworks found the same parallelization strategies for\nAlexNet and ResNet with linear operator graphs and\nfound different strategies for the other DNNs as shown\nin Figure 10b. For these DNNs with non-linear operator\ngraphs, FlexFlow achieves 1.2-1.6\u00d7 speedup compared\nto OptCNN by using parallelization strategies that exploit\nparallelism across different operations. We show two\nexamples in Section 8.5.\n8.3\nExecution Simulator\nWe evaluate the performance of the simulator using two\nmetrics: simulator accuracy and simulator execution time.\n0\n5\n10\n15\nElapsed Time (minutes)\n100\n150\n200\n250\n300\nExpected Runtime of Best Found \nParallelization Strategy (milliseconds)\nFull Simulation\nDelta Simulation\nFigure 12: Search performance with the full and delta\nsimulation algorithms for the NMT model on 16 P100\nGPUs (4 nodes).\n8.3.1\nSimulator Accuracy\nIn this experiment, we compare the estimated execution\ntime predicted by the execution simulator with the real\nexecution time measured by actual executions. Figure 11\nshows the results for different DNNs and different avail-\nable devices. The dashed lines indicate a relative differ-\nence of 0% and 30%, respectively, which encompasses\nthe variance between actual and predicted execution time.\nIn addition, for different parallelization strategies with\nthe same operator graph and device topology (i.e., points\nof the same shape in the \ufb01gure), their simulated execu-\ntion time preserves actual execution time ordering, which\nshows that simulated execution time is an appropriate\nmetric to evaluate the performance of different strategies.\n8.3.2\nSimulator Execution Time\nFigure 12 shows the search performance with different\nsimulation algorithms for \ufb01nding a strategy for the NMT\nmodel on 16 P100 GPUs on 4 nodes. The full and delta\nsimulation algorithms terminate in 16 and 6 minutes,\nrespectively. If the allowed time budget is less than 8\nminutes, the full simulation algorithm will \ufb01nd a worse\nstrategy than the delta simulation algorithm.\nWe compare the end-to-end search time of the execu-\ntion optimizer with different simulation algorithms. For a\ngiven DNN model and device topology, we measure the\naverage execution time of the optimizer using 10 random\ninitial strategies. The results are shown in Table 4. The\ndelta simulation algorithm is 2.2-6.9\u00d7 faster than the full\nsimulation algorithm. Moreover, the speedup over the full\nsimulation algorithm increases as we scale the number of\ndevices.\n11\nFigure 13: The best strategy for parallelizing the Inception-v3 model on 4 P100 GPUs. For each operation, the vertical\nand horizontal dimensions indicate parallelism in the batch and channel dimension, respectively. Each GPU is denoted\nby a color. This strategy reduces the per-iteration execution time by 12% compared to data parallelism.\n. . .  \n. . .  \n. . .  \n. . .  \n. . .  \n. . .  \n. . .  \n. . .  \nSoftmax \nAttention \nDecoder \nLSTM2 \nDecoder \nLSTM1 \nDecoder \nEmbed \nEncoder  \nLSTM2 \nEncoder \nLSTM1 \nEncoder \nEmbed \nFigure 14: The best strategy for parallelizing the NMT\nmodel on 4 P100 GPUs. For each operation, the verti-\ncal and horizontal dimensions indicate parallelism in the\nbatch and channel dimension, respectively. Each grey box\ndenotes a layer, whose operations share the same network\nparameters. Each GPU is denoted by a color.\n8.4\nSearch Algorithm\nThis section evaluates the quality of the best paralleliza-\ntion strategies discovered by the search algorithm.\nFirst, we compare the best discovered strategies with\nthe global optimal strategies for small executions. To\nobtain a search space of reasonable size, we limit the\nnumber of devices to 4 and consider the following two\nDNNs. LeNet [30] is a 6-layer CNN for image classi\ufb01-\ncation. The second DNN is a variant of RNNLM where\nthe number of unrolling steps for each recurrent layer is\nrestricted to 2. The search space for both DNNs contains\napproximately 1011 strategies. We use depth-\ufb01rst search\nto explore the search space and use A\u2217[14] to prune the\nsearch space. Finding the optimal strategies for LeNet\nand RNNLM took 0.8 and 18 hours, respectively. For\nboth DNNs, FlexFlow \ufb01nds the global optimal strategy.\nSecond, we test if the search algorithm returns at least a\nlocally optimal strategy in larger search spaces by compar-\ning the best discovered strategy with all of its neighbors.\nFor this experiment, we consider all six DNNs on 2, 4,\nand 8 devices, where the number of neighbors remains\nsmall enough to exhaustively enumerate them all. All the\nstrategies returned by FlexFlow were locally optimal.\n8.5\nCase Studies\nWe discuss the best strategies discovered by FlexFlow and\nhow they improve parallelization performance.\nInception-v3. Figure 13 shows the best discovered\nstrategy for parallelizing Inception-v3 on four P100 GPUs\non a single node, which exploits intra-operation paral-\nlelism for operations on the critical path and uses a combi-\nnation of intra- and inter-operation parallelism for opera-\ntions on different branches. This results in a well-balanced\nworkload and reduces data transfers for parameter syn-\nchronization. Compared to data parallelism, this strategy\nreduces the parameter synchronization costs by 75% and\nthe per-iteration execution time by 12%.\nFor parallelizing the same Inception-v3 model on four\nK80 GPUs with asymmetric connections between GPUs\n(see Figure 6b), we observe that the best discovered strat-\negy tends to parallelize operations on adjacent GPUs with\na direct connection to reduce the communication costs.\nNMT. Figure 14 shows the best discovered strategy for\nparallelizing NMT on four P100 GPUs, which uses vari-\nous strategies for parallelizing different layers. We brie\ufb02y\ndiscuss the insights from this strategy. First, for a layer\nwith a large number of network parameters and little com-\nputation (e.g., the embed layer), it is bene\ufb01cial to perform\nthe computation on a small number of GPU devices to re-\nduce parameter synchronization costs. Second, for a layer\nwith a large number of network parameters and a heavy\ncomputation workload (e.g., the softmax layer), FlexFlow\nuses parallelism in the channel dimension and assigns the\ncomputation for a subset of channels to each task. This\nallows each device to use a subset of the network parame-\nters, which reduces parameter synchronization costs while\nmaintaining load balance. Third, for multiple recurrent\nlayers (e.g., the LSTM and attention layers), FlexFlow\nuses concurrency among different layers as well as par-\nallelism within each operation to cooperatively reduce\nparameter synchronization costs while balancing load.\n9\nConclusion\nThis paper presents FlexFlow, a deep learning system\nthat automatically \ufb01nds ef\ufb01cient parallelization strategies\nfor DNN applications. FlexFlow uses a guided random-\nized search procedure to explore the space of possible\nstrategies and includes an execution simulator that is an\nef\ufb01cient and accurate predictor of DNN performance. We\nevaluate FlexFlow with six real-world DNN benchmarks\non two GPU clusters and show FlexFlow signi\ufb01cantly\noutperforms state-of-the-art parallelization approaches.\n12\nReferences\n[1] Movie\nreview\ndata.\nhttps://www.\ncs.cornell.edu/people/pabo/\nmovie-review-data/, 2005.\n[2] A New Lightweight, Modular, and Scalable Deep\nLearning Framework.\nhttps://caffe2.ai,\n2016.\n[3] Conference on machine translation. http://www.\nstatmt.org/wmt16, 2016.\n[4] Dense Linear Algebra on GPUs.\nhttps://\ndeveloper.nvidia.com/cublas, 2016.\n[5] TensorFlow\nBenchmarks.\nhttps:\n//www.tensorflow.org/performance/\nbenchmarks, 2017.\n[6] Tensors and Dynamic neural networks in Python\nwith strong GPU acceleration.\nhttps://\npytorch.org, 2017.\n[7] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis,\nJ. Dean, M. Devin, S. Ghemawat, G. Irving, M. Is-\nard, M. Kudlur, J. Levenberg, R. Monga, S. Moore,\nD. G. Murray, B. Steiner, P. Tucker, V. Vasudevan,\nP. Warden, M. Wicke, Y. Yu, and X. Zheng. Tensor-\n\ufb02ow: A system for large-scale machine learning. In\nProceedings of the 12th USENIX Conference on Op-\nerating Systems Design and Implementation, OSDI,\n2016.\n[8] D. Amodei, S. Ananthanarayanan, R. Anubhai,\nJ. Bai, E. Battenberg, C. Case, J. Casper, B. Catan-\nzaro, Q. Cheng, G. Chen, J. Chen, J. Chen, Z. Chen,\nM. Chrzanowski, A. Coates, G. Diamos, K. Ding,\nN. Du, E. Elsen, J. Engel, W. Fang, L. Fan,\nC. Fougner, L. Gao, C. Gong, A. Hannun, T. Han,\nL. V. Johannes, B. Jiang, C. Ju, B. Jun, P. LeGres-\nley, L. Lin, J. Liu, Y. Liu, W. Li, X. Li, D. Ma,\nS. Narang, A. Ng, S. Ozair, Y. Peng, R. Prenger,\nS. Qian, Z. Quan, J. Raiman, V. Rao, S. Satheesh,\nD. Seetapun, S. Sengupta, K. Srinet, A. Sriram,\nH. Tang, L. Tang, C. Wang, J. Wang, K. Wang,\nY. Wang, Z. Wang, Z. Wang, S. Wu, L. Wei, B. Xiao,\nW. Xie, Y. Xie, D. Yogatama, B. Yuan, J. Zhan, and\nZ. Zhu. Deep speech 2: End-to-end speech recog-\nnition in english and mandarin. In Proceedings of\nthe 33rd International Conference on International\nConference on Machine Learning, ICML\u201916.\n[9] D. Bahdanau, K. Cho, and Y. Bengio. Neural ma-\nchine translation by jointly learning to align and\ntranslate. CoRR, abs/1409.0473, 2014.\n[10] M. Bauer, S. Treichler, E. Slaughter, and A. Aiken.\nLegion: Expressing locality and independence with\nlogical regions. In Proceedings of the International\nConference on High Performance Computing, Net-\nworking, Storage and Analysis, 2012.\n[11] C. Chelba, T. Mikolov, M. Schuster, Q. Ge,\nT. Brants, and P. Koehn. One billion word bench-\nmark for measuring progress in statistical language\nmodeling. CoRR, abs/1312.3005, 2013.\n[12] T. Chen, M. Li, Y. Li, M. Lin, N. Wang, M. Wang,\nT. Xiao, B. Xu, C. Zhang, and Z. Zhang. MXNet:\nA \ufb02exible and ef\ufb01cient machine learning library\nfor heterogeneous distributed systems.\nCoRR,\nabs/1512.01274, 2015.\n[13] S. Chetlur, C. Woolley, P. Vandermersch, J. Co-\nhen, J. Tran, B. Catanzaro, and E. Shelhamer.\ncudnn: Ef\ufb01cient primitives for deep learning. CoRR,\nabs/1410.0759, 2014.\n[14] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and\nC. Stein. Introduction to Algorithms, Third Edition.\nThe MIT Press, 3rd edition, 2009.\n[15] J. Dean, G. S. Corrado, R. Monga, K. Chen,\nM. Devin, Q. V. Le, M. Z. Mao, M. Ranzato, A. Se-\nnior, P. Tucker, K. Yang, and A. Y. Ng. Large scale\ndistributed deep networks. In NIPS, 2012.\n[16] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and\nL. Fei-Fei. ImageNet: A large-scale hierarchical\nimage database. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition,\nCVPR, 2009.\n[17] W. R. Gilks, S. Richardson, and D. Spiegelhalter.\nMarkov chain Monte Carlo in practice. CRC press,\n1995.\n[18] I. Gog, M. Schwarzkopf, A. Gleave, R. N. M. Wat-\nson, and S. Hand. Firmament: Fast, centralized\ncluster scheduling at scale. In 12th USENIX Sympo-\nsium on Operating Systems Design and Implementa-\ntion (OSDI 16), pages 99\u2013115, Savannah, GA, 2016.\nUSENIX Association.\n[19] P. Goyal, P. Doll\u00b4ar, R. B. Girshick, P. Noordhuis,\nL. Wesolowski, A. Kyrola, A. Tulloch, Y. Jia, and\nK. He. Accurate, large minibatch SGD: training\nimagenet in 1 hour. CoRR, abs/1706.02677, 2017.\n[20] A. Graves and N. Jaitly. Towards end-to-end speech\nrecognition with recurrent neural networks.\nIn\n13\nProceedings of the 31st International Conference\non International Conference on Machine Learning,\nICML\u201914, 2014.\n[21] W. K. Hastings.\nMonte carlo sampling meth-\nods using markov chains and their applications.\nBiometrika, 57(1):97\u2013109, 1970.\n[22] K. He, X. Zhang, S. Ren, and J. Sun. Deep resid-\nual learning for image recognition. In Proceedings\nof the IEEE Conference on Computer Vision and\nPattern Recognition, CVPR, 2016.\n[23] G. Huang,\nZ. Liu,\nand K. Q. Weinberger.\nDensely connected convolutional networks. CoRR,\nabs/1608.06993, 2016.\n[24] M. Isard, V. Prabhakaran, J. Currey, U. Wieder,\nK. Talwar, and A. Goldberg. Quincy: Fair schedul-\ning for distributed computing clusters. In Proceed-\nings of the ACM SIGOPS 22nd Symposium on Oper-\nating Systems Principles, SOSP \u201909, pages 261\u2013276.\nACM, 2009.\n[25] Z. Jia, S. Lin, C. R. Qi, and A. Aiken. Exploring\nhidden dimensions in parallelizing convolutional\nneural networks. CoRR, abs/1802.04924, 2018.\n[26] Y. Kim. Convolutional neural networks for sentence\nclassi\ufb01cation. CoRR, abs/1408.5882, 2014.\n[27] A. Krizhevsky. One weird trick for parallelizing con-\nvolutional neural networks. CoRR, abs/1404.5997,\n2014.\n[28] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Ima-\ngeNet classi\ufb01cation with deep convolutional neural\nnetworks. In Proceedings of the 25th International\nConference on Neural Information Processing Sys-\ntems, NIPS, 2012.\n[29] S. Lam and R. Sethi. Worst case analysis of two\nscheduling algorithms. SIAM Journal on Computing,\n6, 1977.\n[30] Y. LeCun. LeNet-5, convolutional neural networks.\nURL: http://yann. lecun. com/exdb/lenet, 2015.\n[31] M. P. Marcus, M. A. Marcinkiewicz, and B. San-\ntorini. Building a large annotated corpus of english:\nThe penn treebank. Comput. Linguist., 19.\n[32] A. Mirhoseini, A. Goldie, H. Pham, B. Steiner, Q. V.\nLe, and J. Dean. A hierarchical model for device\nplacement. In International Conference on Learning\nRepresentations, 2018.\n[33] A. Mirhoseini, H. Pham, Q. V. Le, B. Steiner,\nR. Larsen, Y. Zhou, N. Kumar, M. Norouzi, S. Ben-\ngio, and J. Dean. Device placement optimization\nwith reinforcement learning. 2017.\n[34] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu.\nBleu: A method for automatic evaluation of ma-\nchine translation. In Proceedings of the 40th Annual\nMeeting on Association for Computational Linguis-\ntics, ACL \u201902, 2002.\n[35] O. Russakovsky, J. Deng, H. Su, J. Krause,\nS. Satheesh, S. Ma, Z. Huang, A. Karpathy,\nA. Khosla, M. Bernstein, A. C. Berg, and L. Fei-\nFei. ImageNet Large Scale Visual Recognition Chal-\nlenge. International Journal of Computer Vision\n(IJCV), 115(3):211\u2013252, 2015.\n[36] O. Russakovsky, J. Deng, H. Su, J. Krause,\nS. Satheesh, S. Ma, Z. Huang, A. Karpathy,\nA. Khosla, M. Bernstein, et al. Imagenet large scale\nvisual recognition challenge. International Journal\nof Computer Vision, 2015.\n[37] D. Silver, A. Huang, C. J. Maddison, A. Guez,\nL. Sifre, G. Van Den Driessche, J. Schrittwieser,\nI. Antonoglou, V. Panneershelvam, M. Lanctot, et al.\nMastering the game of go with deep neural networks\nand tree search. Nature, 529:484\u2013489, 2016.\n[38] K. Simonyan and A. Zisserman. Very deep convo-\nlutional networks for large-scale image recognition.\nCoRR, abs/1409.1556, 2014.\n[39] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. E. Reed,\nD. Anguelov, D. Erhan, V. Vanhoucke, and A. Ra-\nbinovich. Going deeper with convolutions. CoRR,\nabs/1409.4842, 2014.\n[40] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and\nZ. Wojna. Rethinking the inception architecture for\ncomputer vision. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition,\n2016.\n[41] S. Treichler, M. Bauer, R. Sharma, E. Slaughter,\nand A. Aiken. Dependent partitioning. In Proceed-\nings of the 2016 ACM SIGPLAN International Con-\nference on Object-Oriented Programming, Systems,\nLanguages, and Applications, OOPSLA\u2019 16. ACM,\n2016.\n[42] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi,\nW. Macherey, M. Krikun, Y. Cao, Q. Gao,\nK. Macherey, J. Klingner, A. Shah, M. Johnson,\n14\nX. Liu, L. Kaiser, S. Gouws, Y. Kato, T. Kudo,\nH. Kazawa, K. Stevens, G. Kurian, N. Patil,\nW. Wang, C. Young, J. Smith, J. Riesa, A. Rudnick,\nO. Vinyals, G. Corrado, M. Hughes, and J. Dean.\nGoogle\u2019s neural machine translation system: Bridg-\ning the gap between human and machine translation.\nCoRR, abs/1609.08144, 2016.\n[43] W. Zaremba, I. Sutskever, and O. Vinyals.\nRe-\ncurrent neural network regularization.\nCoRR,\nabs/1409.2329, 2014.\n15\n",
        "context": "miss faster strategies that use parallelism in both dimen-\nsions. We show that exploring a broader search space\ndiscovers parallelization strategies 1.2-3.8\u00d7 faster than\nexisting automated frameworks (see Section 8).\nnow standard practice to parallelize training. Existing\ndeep learning systems commonly use data or model par-\nallelism, but unfortunately, these strategies often result in\nsuboptimal parallelization performance.\nstrategies to \ufb01nd an ef\ufb01cient one. Prior work such as RE-\nINFORCE [33] relies on executing each parallelization\nstrategy on the hardware for one iteration to measure its\nexecution time. Unfortunately, this approach becomes"
    },
    {
        "id": 3,
        "title": "Pytorch fsdp: experiences on scaling fully sharded data parallel",
        "author": [
            "Y. Zhao",
            "A. Gu",
            "R. Varma",
            "L. Luo",
            "C.-C. Huang",
            "M. Xu",
            "L. Wright",
            "H. Shojanazeri",
            "M. Ott",
            "S. Shleifer"
        ],
        "year": "2023",
        "doi": "10.48550/arXiv.2304.11277",
        "in_text_citation": "[3]",
        "sentence": "An alternative is to develop new parallelization strategies that balance memory usage and communication, as explored in strategies like those in [1]\u2013[3].",
        "abstract": "It is widely acknowledged that large models have the potential to deliver\nsuperior performance across a broad range of domains. Despite the remarkable\nprogress made in the field of machine learning systems research, which has\nenabled the development and exploration of large models, such abilities remain\nconfined to a small group of advanced users and industry leaders, resulting in\nan implicit technical barrier for the wider community to access and leverage\nthese technologies. In this paper, we introduce PyTorch Fully Sharded Data\nParallel (FSDP) as an industry-grade solution for large model training. FSDP\nhas been closely co-designed with several key PyTorch core components including\nTensor implementation, dispatcher system, and CUDA memory caching allocator, to\nprovide non-intrusive user experiences and high training efficiency.\nAdditionally, FSDP natively incorporates a range of techniques and settings to\noptimize resource utilization across a variety of hardware configurations. The\nexperimental results demonstrate that FSDP is capable of achieving comparable\nperformance to Distributed Data Parallel while providing support for\nsignificantly larger models with near-linear scalability in terms of TFLOPS.",
        "full_text": "PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel\nYanli Zhao\nMeta AI\nyanlizhao@meta.com\nAndrew Gu\nMeta AI\nandgu@meta.com\nRohan Varma\nMeta AI\nrvarm1@meta.com\nLiang Luo\nMeta AI\nliangluo@meta.com\nChien-Chin Huang\nMeta AI\nchienchin@meta.com\nMin Xu\nMeta AI\nm1n@meta.com\nLess Wright\nMeta AI\nless@meta.com\nHamid Shojanazeri\nMeta AI\nhamidnazeri@meta.com\nMyle Ott\nMeta AI\nmyleott@gmail.com\nSam Shleifer\nMeta AI\nsshleifer@gmail.com\nAlban Desmaison\nMeta AI\nalbandes@meta.com\nCan Balioglu\nMeta AI\nbalioglu@meta.com\nPritam Damania\nMeta AI\npritam.damania@gmail.com\nBernard Nguyen\nMeta AI\nbernardn@meta.com\nGeeta Chauhan\nMeta AI\ngchauhan@meta.com\nYuchen Hao\nMeta AI\nhaoyc@meta.com\nAjit Mathews\nMeta AI\namath@meta.com\nShen Li\nMeta AI\nshenli@meta.com\nABSTRACT\nIt is widely acknowledged that large models have the potential\nto deliver superior performance across a broad range of domains.\nDespite the remarkable progress made in the field of machine learn-\ning systems research, which has enabled the development and\nexploration of large models, such abilities remain confined to a\nsmall group of advanced users and industry leaders, resulting in an\nimplicit technical barrier for the wider community to access and\nleverage these technologies. In this paper, we introduce PyTorch\nFully Sharded Data Parallel (FSDP) as an industry-grade solution\nfor large model training. FSDP has been closely co-designed with\nseveral key PyTorch core components including Tensor implemen-\ntation, dispatcher system, and CUDA memory caching allocator, to\nprovide non-intrusive user experiences and high training efficiency.\nAdditionally, FSDP natively incorporates a range of techniques and\nsettings to optimize resource utilization across a variety of hardware\nconfigurations. The experimental results demonstrate that FSDP is\ncapable of achieving comparable performance to Distributed Data\nParallel while providing support for significantly larger models\nwith near-linear scalability in terms of TFLOPS.\nPVLDB Reference Format:\nYanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min\nXu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, Alban\nDesmaison, Can Balioglu, Pritam Damania, Bernard Nguyen, Geeta\nChauhan, Yuchen Hao, Ajit Mathews, and Shen Li. PyTorch FSDP:\nExperiences on Scaling Fully Sharded Data Parallel. PVLDB, 16(12):\n3848-3860, 2023.\ndoi:10.14778/3611540.3611569\nThis work is licensed under the Creative Commons BY-NC-ND 4.0 International\nLicense. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of\nthis license. For any use beyond those covered by this license, obtain permission by\nemailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights\nlicensed to the VLDB Endowment.\nProceedings of the VLDB Endowment, Vol. 16, No. 12 ISSN 2150-8097.\nPVLDB Artifact Availability:\nThe source code, data, and/or other artifacts have been made available\nat https://github.com/pytorch/pytorch/blob/main/torch/distributed/fsdp/\nfully_sharded_data_parallel.py/.\n1\nINTRODUCTION\nThe magnitude of neural network models is growing at an unprece-\ndented rate, facilitating breakthroughs across a wide spectrum\nof domains. Upon inception, the 175-billion-parameter GPT-3 [3]\nmodel set a new record for almost all Natural Language Process-\ning tasks. The product applications constructed on top of GPT\nmodels [23] have quickly demonstrated their potential to revolu-\ntionize the entire industry. Modern large scale recommendation\nmodels [19, 33] can reach beyond 1 trillion parameters, replete\nwith rapidly growing dense layer components. These models power\napplications that serve multi-billions of users every day. As large\nneural networks continue to push the limits of science and technol-\nogy, an industry-grade tool to simplify the training of such models\nwith high efficiency would help expedite the progress.\nIn recent years, the community has introduced and investigated\nnumerous advanced methodologies to enlarge neural network mod-\nels. Pipeline parallelism [6, 8, 11, 15, 20] partitions a model instance\ninto stages and distributes stages across multiple devices, where\nactivations and gradients are communicated across stage bound-\naries. Tensor parallelism [9, 21, 31, 32] shards model parameters,\nconducts partial computation on individual devices and communi-\ncates activations at required layer boundaries. Zero-Redundancy\nparallelism [27, 28, 30] shards parameters as well but communicates\nparameters on-demand to recover their unsharded form and exe-\ncutes the model as if it were replicated on every device. The afore-\nmentioned techniques have served as the fundamental building\ndoi:10.14778/3611540.3611569\narXiv:2304.11277v2  [cs.DC]  12 Sep 2023\nblocks to enable the training of large neural networks across vari-\nous applications. Nevertheless, two challenges still persist. Firstly,\nsome of these methods are tightly integrated with specific model\narchitectures, which hinder them from being utilized as a generic\nsolution for training large models. Secondly, some of these tech-\nniques are built on top of rapidly-evolving internal interfaces of\nunderlying machine learning frameworks, which become vulnera-\nble to changes in framework implementations. Therefore, it is more\nrobust and efficient to have a native solution co-designed with the\ncore functionalities of machine learning frameworks. Additionally,\nconstructing such a solution in a composable and customizable man-\nner could potentially facilitate the community\u2019s future innovations\nas well.\nThis paper presents PyTorch [24] Fully Sharded Data Parallel\n(FSDP), which enables the training of large-scale models by shard-\ning model parameters. The FSDP algorithm is motivated by the\nZeroRedundancyOptimizer [27, 28] technique from DeepSpeed but\nwith a revised design and implementation that is aligned with the\nother components of PyTorch. FSDP breaks down a model instance\ninto smaller units and then flattens and shards all of the parameters\nwithin each unit. The sharded parameters are communicated and\nrecovered on-demand before computations, and then they are im-\nmediately discarded afterwards. This approach ensures that FSDP\nonly needs to materialize parameters from one unit at a time, which\nsignificantly reduces peak memory consumption. The design and\nimplementation of FSDP faces the following challenges.\n\u2022 User Experience is critical for achieving broad adoption.\nWhen working on prior PyTorch distributed training fea-\ntures such as DistributeDataParallel (DDP) [14], we observed\nthat aligning the user experience of distributed training\nwith that of local training can significantly lower the learn-\ning barrier. Techniques like DDP require the model to be\nreplicated on every device, which implies that the entire\nmodel can be constructed on the target device. However,\nalthough FSDP can easily adopt DDP\u2019s API design, large\nmodels might not fit into one GPU device and therefore\ncannot even be initialized efficiently.\n\u2022 Hardware Heterogeneity often exists in modern GPU\nclusters, whereby interconnects are partitioned into high-\nbandwidth islands within each machine and low-bandwidth\nmesh across machines. Additionally, there may be further\nhierarchical structures at the rack or pod levels. Conse-\nquently, the design of FSDP must accommodate such het-\nerogeneity and optimize accordingly.\n\u2022 Resource Utilization is usually tightly linked with capi-\ntal and operational expenditures, especially for companies\nthat depend on large GPU clusters to power their mission-\ncritical systems. To ensure that GPU devices remain fully\nutilized during distributed training, it is essential to mini-\nmize downtime caused by non-computational operations.\n\u2022 Memory Planning plays a crucial role in large model\ntraining. PyTorch makes GPU memory block allocation\nefficient and transparent through caching. Frequent mem-\nory defragmentations can significantly slow down training,\nwhich becomes particularly acute when working with large\nmodels. In such scenarios, practitioners typically seek to\nsaturate GPU memory as much as possible to accommo-\ndate the largest batches or models. However, operating near\nGPU memory capacity significantly increases the chance\nto trigger defragmentations.\nFSDP tackles the aforementioned challenges through a variety\nof techniques. Firstly, to improve user experience, FSDP introduces\ndeferred initialization that allows users to create a model instance\non a dummy device and record operations invoked during initializa-\ntion. Then, the model can be initialized and sharded unit by unit by\nreplaying the recorded operations on a real GPU device. With this\ntechnique, FSDP can provide similar user experiences as local train-\ning, while effectively scaling large models. Secondly, FSDP offers\nconfigurable sharding strategies that can be customized to match\nthe physical interconnect topology of the cluster to handle hard-\nware heterogeneity. Thirdly, although parameter sharding design\ninevitably inserts communications, which might block computa-\ntions and introduces bubbles during execution, FSDP can squeeze\nout bubbles using an abundant set of tools to aggressively overlap\ncommunication with computation through operation reordering\nand parameter prefetching. Lastly, FSDP optimizes memory usage\nby prudently restricting the amount of blocks allocated for inflight\nunsharded parameters and suspending CPU execution if necessary.\nWe evaluated the performance of FSDP on various models includ-\ning popular language models and recommendation system models,\nutilizing up to 512 80GB A100 GPUs. The experiments showed that\nFSDP can achieve similar performance to that of DDP on small\nmodels. Beyond that FDSP can facilitate significantly larger models\nwith near-linear scalability in terms of TFLOPS. FSDP is currently\na beta feature as of PyTorch 2.0 release, and has been battle-tested\nby both industrial and research applications.\nTo simplify presentation, the rest of this paper uses FSDP to\nrefer to the techniques in general and FullyShardedDataParallel to\ndenote the Python implementation. The remainder of the paper\nis organized as follows. Section 2 introduces background on some\npopular distributed training techniques. Section 3 and Section 4\nelaborate system design and implementation details. Evaluations\nare presented in Section 5. Section 6 surveys related work, and\nSection 7 discusses topics related to FSDP but falls outside of FSDP\ncore. Finally, Section 8 concludes the paper.\n2\nBACKGROUND\nPyTorch [24] has emerged as a fundamental cornerstone for a\nplethora of machine learning endeavors. PyTorch stores values\nin Tensor objects, which are versatile n-dimensional arrays featur-\ning a rich set of data manipulation operations. Every Tensor object\nhas an associated storage that is allocated on a specific device.\nWhen Tensors only represent simple transformations such as reshape\nand split, they can share the same underlying storage. Each Module\ndescribes a transformation from input to output values, and its\nbehavior during the forward pass is specified by its forward member\nfunction. Such a module may feature Tensor objects as parameters,\nwith the Linear module being an example that contains both weight\nand bias parameters. During the forward pass, the Linear module\napplies these parameters to the input to produce the output by\nmeans of multiplication and addition operations, respectively.\nAs both the data size and model complexity continue to escalate\nat a staggering pace, the need for an industry-grade distributed\ntraining framework becomes increasingly imperative for applica-\ntions built on top of PyTorch. This section elucidates the trajectory\nof PyTorch\u2019s distributed training capabilities.\n2.1\nModel Replication\nModel replication approaches are designed to tackle high-volume\ndatasets by scaling out and distributing computations across multi-\nple devices. DistributedDataParallel (DDP) [14] is the first end-to-end\ndistributed training feature in PyTorch that falls into this category.\nDDP\u2019s adoption has been extensive, spanning both the academic\nand industrial domains.\nDDP maintains a model replica on each device and synchronizes\ngradients through collective AllReduce operations in the backward\npass, thereby ensuring model consistency across replicas during\ntraining. To expedite training, DDP overlaps gradient communica-\ntion with backward computation, facilitating concurrent workload\nexecutions on diverse resources. However, one conspicuous limi-\ntation is that DDP requires all model parameters, gradients, and\noptimizer states to fit in the memory of one GPU device. Conse-\nquently, DDP is inadequate for supporting large models, which are\ncritical for cutting-edge machine learning breakthroughs. For exam-\nple, when training models with more than one billion parameters\nusing a 40GB GPU device, DDP will likely encounter out-of-memory\nerrors on each device.\n2.2\nModel Partitioning\nAs the size of models grow, they may no longer fit in a single GPU\ndevice. In such cases, a viable solution is to partition the model into\nsmaller components and distribute them across multiple devices.\nBoth pipeline parallelism [8] and Tensor RPC [25] are along this\ndirection. Pipeline parallelism involves breaking a sequence of lay-\ners into stages and feeding inputs to different stages in a pipelined\nfashion to optimize resource utilization. On the other hand, Tensor\nRPC provides a lower-level toolkit that enables arbitrary computa-\ntions to be executed on remote devices. While both techniques are\ncapable of scaling large models across multiple devices, they either\nlimit the model to a sequence of stages or require modifications to\nthe model authoring code to insert remote computations, which\ncan pose a significant obstacle to users\u2019 adoption. Moreover, many\nindustrial training infrastructures only support the single-program\nmulti-data paradigm, which necessitates a simpler entry point to\nhandle large models.\n2.3\nModel Sharding\nIn addition to partitioning, sharding the parameters of a model can\nalso help reduce its memory footprint and support models with sizes\nbeyond the memory capacity of a single GPU device. After sharding\nmodels, each rank only holds a shard of the model parameters,\nwhich prevents it from performing the same computations as local\ntraining. To guarantee correctness, the training process needs to\nemploy one or both of the following techniques:\n\u2022 Perform computations with parameter shards and commu-\nnicate activations accordingly. With this approach, ranks\nnever need to fully materialize any parameter. However,\nlayer0\nlayer0\nlayer1\nlayer2\nlayer3\nlayer4\nlayer5\nWrap\n&\nShard\nFSDP Unit0\nFSDP Unit1\nFSDP Unit2\nlayer1\nlayer2\nlayer3\nlayer4\nlayer5\nlayer1\nlayer2\ngather full \nparams\nfree peer \nshards\nForward\nBackward\nExec\nlayer1\nlayer2\nfree peer \nshards\ngather full \nparams\nsynchronize \ngradients\nFigure 1: FSDP Algorithm Overview\neach communication will appear in the critical path as it is\ninserted between two consecutive and dependent compu-\ntation operations. As a result, this communication cannot\neasily overlap with computations, unless non-dependent\ncomputations or computations from other iterations can be\nre-ordered to overlap with communication.\n\u2022 Perform the same computation as local training by com-\nmunicating parameter on-demand before computations.\nSince parameter communications do not have any data de-\npendency on preceding computations, they can overlap\nwith the preceding computations performed in the same\nforward or backward pass. However, this approach requires\nthat the on-demand communicated parameters could be\nfully materialized and could fit in the memory of a single\nGPU device.\nFSDP falls into the second category of communicating parame-\nters. Based on our observations and experiments, this approach is\nsufficient to support the vast majority of large model applications\ntoday and in the near future. It is worth noting that if the require-\nment of fully materializing each parameter unit on GPU becomes a\nblocker, we can further combine both techniques to support such\nuse cases.\n3\nSYSTEM DESIGN\nFully Sharded Data Parallel (FSDP) is capable of scaling to accom-\nmodate large models that may not fit in a single GPU device by\nsharding the dense parameters. More specifically, FSDP decom-\nposes the model instance into smaller units and handles each unit\nindependently. During forward and backward computation, FSDP\nonly materializes unsharded parameters and gradients of one unit\nat a time, and otherwise, it keeps parameters and gradients sharded.\nThroughout the training loop, the optimizer states are kept sharded.\nThe memory requirements for FSDP are proportional to the size\nof the sharded model plus the size of the largest fully-materialized\nFSDP unit.\nFigure 1 demonstrates the overall workflow using a simple six\nlayer model. Suppose FSDP decomposes the model into three parts,\nnamely, [layer0, layer3], [layer1, layer2], and [layer4, layer5]. The\ndecomposition behavior can be controlled by user-defined func-\ntions. FSDP then wraps each of these three parts into one FSDP unit\nand shards parameters accordingly. To ensure correctness, FSDP\nneeds to recover the unsharded parameters before corresponding\ncomputations. Let us consider FSDP unit1 that contains [layer1,\nlayer2] to explain this process. Before forward computation enters\nlayer1, FSDP collects the unsharded parameters for layer1 and layer2\nby gathering shards from other peer ranks. With the unsharded\nparameters, FSDP runs the local computation of those layers and\nthen frees the peer shards it just collected to reduce memory foot-\nprint. Therefore, during the entire forward pass, FSDP only needs\nto fully materialize one unit at a time, while all other units can\nstay sharded. Similarly, during the backward computation, FSDP\nunit1 recovers the unsharded parameters for layer1 and layer2 be-\nfore backward reaches layer2. When the autograd engine finishes\nthe backward computation of these two layers, FSDP frees the peer\nshards and launches ReduceScatter to reduce and shard gradients.\nHence, after backward computation, each rank only keeps a shard\nof both parameters and gradients.\nFSDP offers a wide spectrum of optimizations and knobs to\naccount for diverse model structures and hardware capabilities. The\nremainder of this section delves further into the intricacies of model\ninitialization, sharding strategies, communication optimizations,\nand memory management, which are all critical components of\nFSDP\u2019s underlying design.\n3.1\nModel Initialization\nBefore the advent of FSDP, PyTorch mandated the full materializa-\ntion of the entire model instance on one device. Although users can\nallocate different sub-modules to different devices, this would re-\nquire modifying the model source code, which may not be feasible,\nparticularly if model authors and application developers belong to\ndifferent parties. To facilitate a smooth transition from local to dis-\ntributed training, FSDP must effectively aid in the materialization\nand initialization of a massive model, which poses two challenges:\n\u2022 How to create a model instance without materializing any\ntensor storage, postponing initialization until a storage on\na concrete device is attached to the tensor.\n\u2022 How to ensure accurate initialization of model parame-\nters in line with the user\u2019s implementation, even when the\nmodel is too large to fit on a single GPU.\nTo overcome the first challenge, we have introduced a mecha-\nnism called deferred initialization, which involves the allocation of\nmodel parameter tensors on a simulated or \"fake\" device. During\nthis process, all initialization operations performed on the tensor\nare recorded. Subsequently, when the tensor is moved from the\n\"fake\" device to a GPU device, all recorded operations are auto-\nmatically replayed. By adopting this technique, users can generate\na model instance from any third-party library without allocating\nany GPU memory blocks, while still accurately capturing their\nparameter initialization implementations.\nAs illustrated in Figure 1, once the FSDP has wrapped the model,\nit is evenly distributed across all GPUs, with each device holding\nonly one shard in its memory. Therefore, in order to address the\nsecond challenge, each rank should ideally only materialize and\ninitialize the shard that it owns. However, this is not always practi-\ncal, since we cannot predict what initialization logic the user will\nimplement in the model init method. The initialization logic may\nrely on having a unsharded parameter on the device, which makes\nit impossible to shard the initialization. Consequently, FSDP must\nAll-Gather Base\n(Even)\nAll-Gather\n(Even)\nAll-Gather\n(1 Numel\nUneven)\nAll-Gather\n(1e6 Numel\nUneven)\n18\n19\n20\n21\n22\n23\n24\nTime per All-Gather (ms)\n(a) Uneven Input Sizes\n1073M536M\n268M\n134M 67M\n33M\n16M 8M\n4M\nNumel per All-Gather\n100\n150\n200\n250\n300\n350\nTotal Communication Time (ms)\n(b) Reducing Input Size\nFigure 2: Communication Efficiency vs. Input Size\nprepare the unsharded parameters before executing Tensor initializa-\ntion operations and simultaneously reduce the memory footprint.\nGiven that sharding initialization is unsafe, FSDP applies the same\napproach as how it handles model forward and backward passes,\ni.e., initialize one FSDP unit at a time and shard the unit before mov-\ning on to the next one. When combined with deferred initialization,\nFSDP traverses the fake device model instance to decompose it into\nFSDP units, moves one unit to a GPU device at a time, and replays\nthe recorded initialization operations for tensors in that FSDP unit.\n3.2\nSharding Strategies\nThe sharding strategy is an important element in FSDP that plays\na significant role in determining the memory footprint and com-\nmunication overhead. FSDP offers a variety of sharding strategies,\nranging from fully replicated to fully sharded. To generalize these\nsharding strategies, we introduce the sharding factor \ud835\udc39as the num-\nber of ranks over which parameters are sharded. By setting the\nsharding factor to 1, FSDP fully replicates the model and simplifies\nto vanilla data parallelism that uses AllReduce for gradient reduction.\nBy setting the sharding factor equal to the number of devices (i.e.,\nglobal world size\ud835\udc4a), FSDP fully shards the model, with each device\nonly holding 1\n\ud835\udc4aof the model. Hybrid sharding occurs when the\nsharding factor ranges between 1 and \ud835\udc4a. The remainder of this\nsection focuses on full sharding and hybrid sharding since the full\nreplication strategy is similar to the existing DDP [14].\n3.2.1\nFull Sharding.\nThe full sharding strategy leads to the lowest memory footprint\nbut incurs the most communication overhead, for example, full\nsharding has 1.5x communication overhead and volume over DDP\nif using bandwidth optimal ring algorithm. Therefore, FSDP must\ncarefully organize communications to maximize its efficiency under\nthis strategy.\nWe conducted two sets of experiments to understand the im-\npact of input size on collective communication efficiency. Results\nare shown in Figure 2, which helped identify two ingredients for\nefficiencies:\n(1) Even Input Size: The Nvidia NCCL [22] library offers effi-\ncient collective implementations for all-gather and reduce-\nscatter that require even input tensor sizes across ranks.\n1\n0\n7\n8\n9\n15\n...\n...\nSharding Group\nFull Sharding\n(F = 16)\nLocal Shard\n1\n8\nFlatParameter\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\ni\nRank i\nweight\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n0\n0\n0\n9\nbias\n7\n12\n13\n14\nPadding\nFigure 3: Full Sharding Across 16 GPUs\n(2) Larger Input Size: For fixed communication volume, batch-\ning data and issuing fewer collectives improves perfor-\nmance by avoiding the collectives\u2019 launch overhead and\nincreasing network bandwidth utilization.\nFor (1), NCCL\u2019s AllGather API requires even input tensor size\nand writes outputs into one single tensor. PyTorch\u2019s ProcessGroup\nwraps the NCCL API and enhances it by supporting uneven input\ntensor sizes across ranks and allowing users to provide a list of\noutput tensors. The flexibility comes with an efficiency trade-off,\nas shown in Figure 2 (a). We use All-Gather Base to denote NCCL\u2019s\nAllGather behavior, and All-Gather to denote the one that takes a list\nof tensors as outputs. The latter incurs additional copies between\nthe individual output tensors and the consolidated single large\noutput tensor before and after the communication. Moreover, for\nuneven inputs, ProcessGroup mimics AllGather\u2019s behavior using group\nBroadcast, which is slower than All-Gather Base. In the experiments,\nwe created artificial unevenness by moving 1 element and 1\ud835\udc526\nelements from rank 1 to rank 0 respectively. The results show that\nthe All-Gather Base with even input size achieved highest efficiency.\nFor (2), Figure 2 (b) fixes the total communication to be 230 \u2248\n1B FP32 elements and varies the size per All-Gather, i.e., smaller\nAllGather size means more AllGather invocations. Once the AllGather\nsize decreases below 33M elements, the total communication time\nbegins increasing rapidly.\nThus, to deliver highly efficient communications, FSDP orga-\nnizes all parameters within one FSDP unit into a large FlatParameter,\nwhere the FlatParameter coalesces the communications of its indi-\nvidual parameters and also evenly shards them across ranks. More\nspecifically, the FlatParameter is a 1D tensor constructed by concate-\nnating \ud835\udc5dflattened original parameters and padding on the right\nto achieve a size divisible by the sharding factor. To shard the\nFlatParameter, FSDP divides it into equal-sized chunks, where the\nnumber of chunks equals the sharding factor, and assigns one chunk\nper rank. The FlatParameter\u2019s gradient inherits the same unsharded\nand sharded shapes from the FlatParameter, and the FlatParameter\nand its gradient own the underlying storage of the original parame-\nters and their gradients, respectively. Figure 3 depicts one example,\nwhere we use one FSDP unit to shard a 4 \u00d7 3 nn.Linear layer across\n16 GPUs. In this case, every GPU only holds one element from the\nFlatParameter with the last rank holding the padded value.\n0\n1\n0\n7\n8\n9\n15\n...\n...\nSharding Group\nReplication Group\nHybrid Sharding (F = 8)\n0\n0\n1\nLocal Shard\nFlatParameter\n1\n1\n7\n2\n3\n4\n5\n6\n7\n7\ni\nRank i\nFigure 4: Hybrid Sharding on 16 GPUs: GPUs are configured\ninto 2 sharding groups and 8 replication groups\nThis flatten-concat-chunk algorithm permits each original pa-\nrameter to have arbitrary shape while minimizing the required\npadding (to be at most \ud835\udc39\u22121), reflecting its generality. Moreover,\nunder this algorithm, the sharded and unsharded FlatParameter and\nits gradient have the exact data layout expected by AllGather and\nReduceScatter, respectively. This enables calling the collectives with-\nout any additional copies for either the input or output tensors.\nMore formally, suppose for a model with \u03a8 number of elements,\nFSDP constructs \ud835\udc41FlatParameters with numels \ud835\udf131, . . . ,\ud835\udf13\ud835\udc41, where\n\u2211\ufe01\ud835\udc41\n\ud835\udc56=1\ud835\udf13= \u03a8. For sharding factor \ud835\udc39, the peak parameter memory\ncontribution is in \ud835\udc42(\u2211\ufe01\ud835\udc41\n\ud835\udc56=1\n\ud835\udf13\ud835\udc56\n\ud835\udc39+ max\ud835\udc41\n\ud835\udc56=1\ud835\udf13\ud835\udc56) because FSDP always\nkeeps each local sharded FlatParameter with size \ud835\udf13\ud835\udc56\n\ud835\udc39in GPU memory\nand must materialize each unsharded FlatParameter with size \ud835\udf13\ud835\udc56one\nby one during forward and backward. Since the first \u2211\ufe01\ud835\udc41\n\ud835\udc56=1\ud835\udf13\ud835\udc56= \u03a8\nis fixed, the peak parameter memory contribution is determined by\nmax\ud835\udc41\n\ud835\udc56=1\ud835\udf13\ud835\udc56. At the same time, the number of collectives per iteration\nis in \ud835\udc42(\ud835\udc41). This evidences FSDP\u2019s memory-throughput trade-off:\nFiner-grained FlatParameter construction decreases peak memory\nbut may decrease throughput by requiring more collectives. Users\ncan control this trade-off by specifying how to wrap sub-modules\ninto FSDP units.\n3.2.2\nHybrid Sharding.\nWe refer to the strategy when the sharding factor is greater than\n1 but less than \ud835\udc4aas hybrid sharding, as it combines both sharding\nand replication. For global world size \ud835\udc4aand sharding factor \ud835\udc39,\nthe parameters are sharded within each group \ud835\udc461, . . . ,\ud835\udc46\ud835\udc4a/\ud835\udc39and\nare replicated within each complementary group \ud835\udc451, . . . , \ud835\udc45\ud835\udc39, where\neach \ud835\udc46\ud835\udc56, \ud835\udc45\ud835\udc57\u2286{1, . . . ,\ud835\udc4a} gives the ranks in the sharded or replicated\ngroup, respectively.\nFor gradient reduction, the single reduce-scatter over all ranks\nbecomes a reduce-scatter within each of the sharded groups fol-\nlowed by an all-reduce within each of the replicated groups to\nreduce the sharded gradients. The equivalence follows from the\ndecomposition\n\ud835\udc4a\n\u2211\ufe02\n\ud835\udc5f=1\n\ud835\udc54\ud835\udc5f=\n\ud835\udc4a/\ud835\udc39\n\u2211\ufe02\n\ud835\udc56=1\n\u2211\ufe02\n\ud835\udc5f\u2208\ud835\udc46\ud835\udc56\n\ud835\udc54\ud835\udc5f,\n(1)\nwhere \ud835\udc54\ud835\udc5frepresents the gradient on rank \ud835\udc5f.\nHybrid sharding can take advantage of datacenter locality for\naccelerated training and can reduce cross host traffic to avoid as\nmuch contention in the oversubscribed environment as possible. At\nthe same time, it provides a graduating trade-off between memory\nsaving and throughput degradation, which is particularly helpful\nfor models whose required memory footprint when trained with\nfull replication is just slightly above the device capacity and do not\nwant full sharding. Figure 4 shows one example.\nSpecifically, datacenters typically adopt a fat-tree network topol-\nogy [16] with over-subscription, leading to abundant locality to\nexploit and a well-motivated reason to reduce cross-host traffic [17].\nHybrid sharding can provide a natural mechanism to map the de-\nvice mesh into the datacenter layout to exploit such locality. For\nexample, consider a cluster as a group of \ud835\udc4aaccelerators grouped\ninto hosts of of \ud835\udc3aaccelerators each (where the communication\namong accelerators on the same host is much faster than the com-\nmunication across hosts), we can set \ud835\udc39= \ud835\udc4a\n\ud835\udc3ato limit the AllGather\n(and ReduceScatter) operations within the same host, while creating\na replication group for accelerators with the same local rank across\nhosts. For an \ud835\udc40-sized model, we can then compute the total cross-\nhost traffic per GPU in the hybrid setup to be 2\ud835\udc40\ud835\udc4a\u22121\n\ud835\udc3a\ud835\udc4a, a drastic\nreduction compared to full replication\u2019s 2\ud835\udc40\ud835\udc4a\u22121\n\ud835\udc4a\nand full sharding\u2019s\n3\ud835\udc40\ud835\udc4a\u22121\n\ud835\udc4a. Additionally, since the AllReduce collectives used in hybrid\nsharding operates at a smaller world size, they empirically achieve\na better performance than invoking collectives at the global scale\n(in the case of full replication and full sharding), due to straggler\neffects and larger network interference.\nAnother important design motivation for hybrid sharding is the\nneeds from medium-sized models. These models are large enough\nto cause out of memory issues when trained with full replication\nbut are not large enough to fully utilize accelerator memory when\nused with full sharding, leading to both runtime overhead and\nmemory waste. The hybrid sharding strategy creates a much richer\nmemory-throughput trade-off space by simply adjusting \ud835\udc39.\n3.2.3\nAutograd.\nFSDP\u2019s FlatParameter must inter-operate with PyTorch\u2019s autograd\nengine to ensure (1) correct gradient propagation and (2) timely gra-\ndient reduction. For (1), recall that the FlatParameter and its gradient\nown the underlying storage of the original parameters and their gra-\ndients, respectively. To achieve this, before forward computation,\nFSDP sets the original parameters to be views into their unsharded\nFlatParameter using autograd-visible torch.split() and torch.view()\ncalls. Then, the autograd engine naturally allocates the unsharded\nFlatParameter gradient and writes each original parameter\u2019s gradi-\nent to the appropriate offset as defined by torch.split()\u2019s backward\nfunction. For (2), FSDP registers a gradient hook that only runs once\nthe FlatParameter\u2019s gradient is finalized. The hook represents the\npost-backward logic and includes the gradient reduction. Notably,\nFSDP\u2019s approach builds on top of PyTorch\u2019s autograd engine in-\nstead of hacking around it. As a result, FSDP automatically handles\nunconventional cases such as when not all parameters are used in\nthe forward or when there are multiple forwards before a backward.\nFSDP Unit i\n0\nAG0\nFWD1\n1 1\n2 2\nFWD2\nCPU\nGPU Comp.\nStream\nGPU Comm.\nStream\n2\nBWD2\nForward\nBackward\nRS2\nAG2\n1\n2\nFWD0\nFWD0\nAG2\nAG1\n0\n0\n2\n2\n2\nBWD1\nAG1\n1\n0\n0\n0\n1\nRS1\nRS0\n1\n0\n1\nParameter Free\nAll-Gather (AG)\nReduce-Scatter (RS)\nForward Comp. (FWD)\nBackward Comp. (BWD)\ni\nBWD0\nBWD0\nFigure 5: Overlap Communication and Computation\n3.3\nCommunication Optimizations\nThe FSDP framework incorporates a range of native communi-\ncation optimization techniques. This section unveils four major\nones: overlapping, backward pre-fetching, forward pre-fetching,\nand accumulation.\n3.3.1\nOverlapping Communication and Computation.\nThe PyTorch c10d library has a ProcessGroup abstraction that repre-\nsents a group of processes that can run collectives together. For the\nNCCL backend, the ProcessGroupNCCL implementation has an internal\nNCCL stream per device, where the separate internal stream is for\nasynchronous execution with the current stream, which is typi-\ncally the default stream running computation. Those asynchronous\ncollectives return Work objects, where calling Work.wait() blocks the\nCPU thread until the collective finishes. For general correctness,\nProcessGroupNCCL synchronizes the internal stream with the current\nstream before running the collective. DistributedDataParallel lever-\nages the async-collective-and-wait() approach to overlap the gradi-\nent All-Reduces with backward computation. However, in contrast\nto DDP\u2019s backward where the AllReduce proceeds the computation\nwith which to overlap, FSDP\u2019s forward issues the AllGather following\nthe computation with which to overlap since in eager execution,\nFSDP cannot know which FlatParameter to AllGather next to reorder\nit before the computation. This difference in kernel-issue order\nmakes following the async-collective-and-wait() approach infea-\nsible for FSDP. Namely, since ProcessGroupNCCL synchronizes with\nthe current (default) stream, the All-Gather will not run until the\ncomputation with which to overlap finishes. To address this, FSDP\nuses a separate CUDA stream to issue the AllGathers, bypassing the\nfalse dependency on preceding computation in the default stream\nand allowing each AllGather to overlap. As a result, FSDP\u2019s collective\nsynchronization operates on streams, not simply Work objects. Fig-\nure 5 illustrates one example. Note that the backward pass excludes\nthe AG0 All-Gather because FSDP intentionally keeps the outermost\nFSDP unit\u2019s parameters in memory to avoid redundantly freeing at\nthe end of forward and then re-All-Gathering to begin backward.\n3.3.2\nBackward Prefetching.\nFSDP enforces a single CUDA device per rank and uses a single\nprocess group for both AllGather and ReduceScatter, which means\nthat its collectives run sequentially in the process group\u2019s internal\nNCCL stream. In the backward pass, FSDP issues the ReduceScatter\nfor the current FlatParameter and then the AllGather for the next\nFlatParameter. Hence, the single NCCL stream forces the ReduceScatter\nto block the next AllGather, which in turn blocks the next gradient\ncomputation and may become exposed on the critical path.\nTo avoid two consecutive exposed communication calls in the\nbackward pass, FSDP backward prefetching issues the next AllGather\nbefore the current ReduceScatter. However, as mentioned before, a\nchallenge for eager execution is knowing which FlatParameter to\nAllGather next. FSDP resolved this challenge by recording the reverse\nforward execution order of modules as the proxy of their backward\nexecution order. Moreover, the forward order is freshly recorded\neach iteration, meaning that the backward prefetching is compatible\nwith dynamism across iterations.\n3.3.3\nForward Prefetching.\nFor some workloads with relatively slow CPU execution, the\nCPU thread may not be able to issue the next forward AllGather early\nenough to efficiently fill the NCCL stream. If the model follows a\nstatic computational graph across iterations, then FSDP can assume\nthe forward execution order of modules from the previous itera-\ntion and prefetch the next AllGather explicitly in the forward pass.\nThis forward prefetching issues the next AllGather before forward\ncomputation of current FSDP unit.\n3.3.4\nGradient Accumulation.\nFSDP offers two variations of gradient accumulation: with and\nwithout communication. With communication, FSDP still reduces\ngradients across ranks, and each rank saves the sharded gradi-\nents. Simply running multiple iterations without clearing gradients\nachieves this. Without communication, FSDP does not reduce gradi-\nents across ranks, and each rank saves the unsharded gradients. This\nlatter variation trades off increased memory usage with decreased\ncommunication, which can increase end-to-end throughput.\n3.4\nMemory Management\nPyTorch uses a CUDA caching allocator as a middle layer to serve\nGPU allocation and free requests for PyTorch programs. In order to\neffectively manage memory, FSDP uses a rate limiter to take into\naccount the memory impact of the caching allocator on programs\nthat use several CUDA streams and run fast CPU threads.\n3.4.1\nHow Does PyTorch Caching Allocator Affect Memory.\nThe caching allocator avoids frequent calls to cudaMalloc and\ncudaFree, where the latter incurs a costly device synchronization.\nSpecifically, the caching allocator requests CUDA memory blocks\nand internally determines how to split and reuse the blocks without\nreturning them to CUDA with the goal being to reach a steady state\nwithout further calls to cudaMalloc and cudaFree.\nThe caching allocator runs from the CPU thread, meaning that it\nmust decide which caching allocator block to use for an allocation\nwhen the CPU thread processes the allocation request. It cannot\nwait until the GPU kernel needing the allocation actually runs,\nwhich may be much later.\nFor a single stream, the caching allocator can directly reuse mem-\nory blocks by the stream\u2019s sequential ordering semantics. However,\nfor separate producer and consumer streams, there are no inter-\nstream ordering guarantees, and the caching allocator cannot be\ncertain that a block is safe to reuse until the last GPU kernel depend-\ning on that memory finishes running. Hence, if the CPU thread\nruns far ahead of the GPU execution, then the caching allocator\ncannot reuse blocks for the producer stream with pending GPU\nkernels from the consumer stream.\nFurthermore, caching allocator blocks are allocated per stream\nand cannot be reused for a different stream, this over-allocates\nblocks to the producer stream that could otherwise be used for\nthe consumer stream (e.g. for activations). The GPU itself may\nhave enough memory to serve a new allocation in the consumer\nstream, but the overallocation to the producer stream may lead\nto the caching allocator failing to serve it. This forces a blocking\nsequence of cudaFrees to reset the caching allocator memory state\ncalled a cudaMalloc retry that greatly degrades training throughput.\n3.4.2\nRate Limiter.\nFSDP allocates the AllGather destination tensor representing the\nunsharded FlatParameter in a producer stream, and the forward and\nbackward computations using the AllGathered parameters run in\na consumer stream (typically the default stream). For a fast CPU\nthread, there may be pending GPU computation kernels when the\ncaching allocator must serve the next AllGather, leading to no block\nreuse. Even after the blocks are not active in the AllGather producer\nstream, these reserved blocks can not serve default computation\nstream\u2019s allocation requests, and thus may force blocking cudaFrees\nand cudaMallocs.\nFSDP offers a rate limiter that intentionally blocks the CPU\nthread to ensure proper caching allocator block reuse. It allows at\nmost two inflight AllGathers, which is the minimum amount to still\nachieve communication and computation overlap.\n4\nIMPLEMENTATION\nThis section delves into the intricacies of FSDP implementation,\nwhich although do not alter the FSDP core algorithm, are crucial to\nunderstand before adopting FSDP.\nUsers can access FSDP through two APIs, FullyShardedDataParallel\nmodel wrapper and fully_shard module annotator. The former wraps\nthe entire model and replaces sub-modules with corresponding\nFSDP units. In contrast, the latter installs FSDP logic as nn.Module\nforward and backward hooks, preserving both model structures\nand parameter fully-qualified names.\n4.1\nInitialization\nSection 3.2.1 described FSDP\u2019s solution to efficiently initialize large\nmodels, which works well when sub-module initializations are self-\ncontained. In a rare situation where one sub-module\u2019s initialization\ndepends on a parameter from the different sub-module, the on-\ndemand materialization and record-replay approach might break\nif the parameter belongs to a different FSDP unit, because the un-\nsharded version of that parameter could have been discarded to\nreduce memory footprint. Therefore, besides the advanced deferred\ninitialization, FSDP offers two more options:\n\u2022 Initialize unsharded model on GPU. The memory re-\nquirement for model initialization may be smaller than that\nfor training since training also involves gradients, activa-\ntions, and optimizer states. Consequently, if the training\nstep cannot be performed on a single GPU device, users\nmight still be able to initialize the entire model on a GPU\nand pass it to FSDP. Then, optimizers should be instanti-\nated after FSDP shards the model, to reduce the memory\nfootprint and align with the sharded gradients produced by\nFSDP.\n\u2022 Initialize unsharded model on CPU. If the size of the\nunsharded model surpasses the capacity of GPU memory\nand can only be accommodated in CPU memory, it becomes\nimpracticable to move the unsharded model entirely to the\nGPU before handing it over to FSDP for parameter shard-\ning. To overcome this challenge, FSDP adopts a streaming\napproach, where the model is migrated to the GPU unit by\nunit. Upon arrival to the GPU, the parameters of each unit\nare immediately sharded, which in turn reduces the mem-\nory overhead before processing the next unit. This approach\nremains viable even when there are cross-submodule depen-\ndencies during initialization, given that all parameters of\nthe entire unsharded model are present in the CPU memory.\nNote that both approaches above are subject to their own limi-\ntations. The first method entails the entire model fitting within a\nsingle GPU device and thus becomes infeasible for larger models.\nThe second method, on the other hand, can handle larger models\nsince the CPU has considerably larger memory. However, this ap-\nproach may experience substantial slowdowns in comparison to\ndeferred initialization due to the limited memory bandwidth and\nparallelization capabilities of the CPU. In light of these observations,\nusers may still prefer deferred initialization, even when dealing\nwith models of the size range encompassed by the previous two\nmethods.\nTo delimit the scope of each FSDP unit, users may choose to em-\nploy the FullyShardedDataParallel wrapper by intrusively applying\nit to sub-modules in model source code, or alternatively, provide a\ncustom function to the auto_wrap_policy argument upon instantia-\ntion. Selecting the optimal wrapping approach typically requires\nsome experiments and measurements.\n4.2\nFlat Parameters\nThe FlatParameter class inherits from nn.Parameter and behaves like\nan nn.Parameter. FSDP implements an accompanying FlatParamHandle\nclass that is responsible for managing individual FlatParameter in-\nstances. The frontend, either FullyShardedDataParallel or fully_shard,\ninterfaces with the FlatParameters only through FlatParamHandle.\nOne FlatParameter accommodates storage for all parameter ten-\nsors within one FSDP unit. The boundary of the FSDP unit controls\nthe timing for AllGather and ReduceScatter, which has a direct im-\npact on overall FSDP performance. In the ideal case, FSDP unit\nboundaries should align with model execution order.\nFSDP has access to the model\u2019s static nn.Module structure at con-\nstruction time. Fortunately, although this structure does not guaran-\ntee to faithfully represent model execution order, model authors con-\nventionally translate layers and broader blocks to nested nn.Module\ndefinitions that may naturally have the desired parameter locality.\nFSDP can leverage that structure to choose the FlatParameter con-\nstruction. Indeed, FSDP supports annotating nn.Modules and follows\na simple rule: All parameters in the annotated nn.Module are assigned\nto one FlatParameter, excluding those parameters already assigned.\nThis rule lends itself naturally to nested annotation, where blocks\nare annotated, forming well-sized FlatParameters, and any residual\nparameters are assigned to their parent.\nAnother approach we explored is using the execution order\nand reconstructing FlatParameters dynamically. This approach starts\nwith an initial small FlatParameter construction, runs a possibly in-\nefficient first iteration while observing the execution order, and\nreconstructs the FlatParameters by coalescing the existing small\nFlatParameters according to the observed order.\n4.3\nRuntime\nFSDP augments a local model instance by incorporating commu-\nnication operations to reduce gradients and gather parameters.\nTimely initiation of these operations is of paramount importance\nfor ensuring both correctness and efficiency. Starting communi-\ncation too soon would cause the parameters or gradients with\npending updates to be consumed, while initiating communication\ntoo late would result in wasting network bandwidth and delay in\nsubsequent computations.\nTo insert communication-related code to the model forward pass,\nthe FullyShardedDataParallel nn.Module wrapper overrides nn.Module\u2019s\nforward() method to install pre-forward and post-forward logic,\nwhereas the functional fully_shard implements them by registering\nnn.Module hooks through methods such as register_forward_pre_hook()\nand register_forward_hook(). It is more challenging to capture appro-\npriate signals from the backward pass, as PyTorch automatically\nand transparently handles the backward pass. Fortunately, the auto-\ngrad engine exposes a variety of hooks that enable the installation\nof custom logic with precise granularity.\n\u2022 Hooks on Tensor through register_hook() allows to run cus-\ntom function when the gradient of a Tensor is generated.\nThis can help anchor FSDP logic to an activation\u2019s gradient\ncomputation in the backward pass. FSDP registers this type\nof hook to the forward output tensor of every FSDP unit\nto insert communications before backward pass enters that\nFSDP unit.\n\u2022 Hooks on backward() through queue_callback() run right be-\nfore exiting the current autograd GraphTask, which is usually\nthe end of overall backward pass. FSDP relies on this hook\nto wait for pending communications so that the subsequent\noptimizer step will not consume gradients too early.\n\u2022 Hooks on AccumulateGrad autograd function fires when the\ngradient of a parameter has finished accumulation in the\ncurrent backward pass. FSDP attaches this type of hook to\neach FlatParameter\u2019s AccumulateGrad function to immediately\nlaunch ReduceScatter when gradients are ready. Note that\nthe Tensor hook mentioned above can potentially achieve\nthe same behavior, but might incur unnecessary delay as it\nneeds to wait for gradient computations for input activa-\ntions as well.\nThe aforementioned methodologies collectively integrate the\nFSDP algorithm with the PyTorch nn.Module and autograd engine in\na non-intrusive and efficient manner.\n4.4\nNative Mixed Precision\nFSDP offers a versatile native mixed precision mechanism. In terms\nof parameter management, it adheres to the standard mixed preci-\nsion technique, which maintains both low and full precision copies\nof parameters [18]. Forward and backward computation use the low\nprecision, and the optimizer step uses full precision. FSDP permits\nuser-specified precisions for parameters, gradient reduction, and\nnon-trainable buffers, each independently if desired.\nFor \u03a8 number of parameter elements (torch.numel), \ud835\udc3elow bytes\nper low precision element, and \ud835\udc3efull bytes per full precision element,\nthis approach to mixed precision normally increases the memory\noverhead from \ud835\udc3efull\u03a8 to (\ud835\udc3elow + \ud835\udc3efull)\u03a8 due to maintaining both\nprecision copies. However, FSDP can sidestep the problem given\nour design to always keep each local sharded FlatParameter in GPU\nmemory and only dynamically allocate the unsharded FlatParameter.\nFor \ud835\udc41FlatParameters with numels given by \ud835\udf131, . . . ,\ud835\udf13\ud835\udc41, the param-\neter peak memory contribution for FSDP actually decreases from\n\ud835\udc3efull\n\ud835\udc39\n\u2211\ufe01\ud835\udc41\n\ud835\udc56=1\ud835\udf13\ud835\udc56+\ud835\udc3efull max\ud835\udc41\n\ud835\udc56=1\ud835\udf13\ud835\udc56to \ud835\udc3efull\n\ud835\udc39\n\u2211\ufe01\ud835\udc41\n\ud835\udc56=1\ud835\udf13\ud835\udc56+\ud835\udc3elow max\ud835\udc41\n\ud835\udc56=1\ud835\udf13\ud835\udc56bytes.\nIn other words, FSDP directly reduces the second \ud835\udc3efull max\ud835\udc41\n\ud835\udc56=1\ud835\udf13\ud835\udc56\nterm to \ud835\udc3elow max\ud835\udc41\n\ud835\udc56=1\ud835\udf13\ud835\udc56.\nIn contrast to torch.amp.autocast that performs just-in-time casts\nat the operator level, FSDP\u2019s native mixed precision only incurs a\nfull-to-low-precision cast per FlatParameter in its pre-forward and,\nif resharding after forward, its pre-backward. Moreover, FSDP\u2019s\nmixed precision permits running all collectives in the low precision,\nwhich saves communication volume.\nUsers most commonly choose FP16 or BF16 as the low precision\nand FP32 as the full precision. FP16\u2019s smaller dynamic range com-\npared that of FP32 exposes FP16 to greater risk of numeric underflow\nand overflow. The standard solution includes a gradient scaler [1]\nthat scales gradients to a safe magnitude. However, since FSDP\nshards gradients across ranks, a normal local gradient scaler im-\nplementation breaks mathematical equivalence, and instead, FSDP\nprovides its own sharded gradient scaler.\n5\nEVALUATION\nWe conducted an empirical evaluation of FSDP on large language\nmodels and recommendation system models and compared the re-\nsults with those of DDP. Experiment specifications are described in\nSection 5.1. Then, we organize experiments into three categories.\nSection 5.2 focuses on how well FSDP handles different sizes of\nmodels. Then, Section 5.3 discusses the impact of throttling commu-\nnications. Finally, Section 5.4 demonstrate FSDP\u2019s ability to scale\nto gigantic models.\n5.1\nExperiment Setup\nIn these experiments, we conducted evaluations on the Hugging-\nFace T5-11B transformer [26], minGPT-175B transformer [3], and\nDHEN recommendation model [33]. The recommendation models\nconsist of 768B sparse parameters and 550M dense parameters, the\nsparse parameter tensors were sharded using the first approach\nmentioned in Section 2.3, which communicates activations instead\nof parameters, while the dense parameters were trained using FSDP\non 8 to 512 A100 80GB GPUs interconnected by a 2Tb/s RoCE net-\nwork. The objective was to assess the capability and scalability of\nFSDP in training large-scale models. Additionally, we employed\nT5-611M, T5-2B and T5-11B transformers to evaluate the perfor-\nmance of various sharding strategies, communication efficiency\nof prefetching, and communication throttling using rate limiter.\nMetrics employed in these experiments included TFLOPS per GPU,\nlatency per batch, peak memory allocated, peak memory active,\nand peak memory reserved.\n5.2\nModel Scale\nIn this section, we investigate the performance of FSDP when deal-\ning with models of different sizes, spanning from 611M to 175B,\nand make a comparison with DDP [14].\nThe experimental results on T5 models are displayed in Fig-\nure 6 (a). The performance of FSDP and DDP is similar when evalu-\nating 611M and 2.28B models. However, DDP encounters an out-of-\nmemory error when attempting to wrap models larger than 2.28B.\nIn contrast, FSDP can effortlessly accommodate the 11B model and\nachieve significantly higher TFLOPS by turning on BF16. These ex-\nperiments illustrate that practitioners can utilize FSDP for both\nsmall and large models, and seamlessly transition across different\nmodel configurations.\nThen, we conduct additional experiments to measure the accel-\neration attained through backward pre-fetching. This time we use\na larger GPT-175B model, where communication overhead is more\nprominent. Results are presented in Figure 6 (b), where pre-fetching\nleads to approximately 18% speedup, and this TFLOPS gain per-\nsists across different GPU cluster sizes. Therefore, for subsequent\nexperiments, we always turn-on backward pre-fetching.\n5.3\nThrottle Communications\nIn the subsequent analysis, we investigate the implications of throt-\ntling FSDP communications. As expounded in Section 3.4, launching\nAllGather too aggressively can lead to unnecessarily high memory\nfootprint, as the CPU thread needs to allocate CUDA memory blocks\nwhen the communication kernel is added into the CUDA stream.\nThis predicament may sometimes result in significant performance\nproblems when the CPU thread runs too fast in comparison to\nCUDA streams. To gauge its efficacy in varying scenarios, we apply\nrate limiting to three different types of models and applied the\nmaximum feasible batch size in each experiment.\n\u2022 RegNet [29]: model size 9B, and batch size 48 for 2 nodes\nand 72 for 4 nodes.\n\u2022 T5 [26]: model size 11B, and batch size 2.\n\u2022 DeepViT [36]: model size 8B, and batch size 105 for 2 nodes\nand 120 for 4 nodes.\nExperiment results are plotted in Figure 6 (c). One notable ob-\nservation is that the rate limiter\u2019s effectiveness is not consistent,\nas it does not attain any speedups in the RegNet experiments,\nand even impedes the DeepViT ones. This behavior is expected\nsince throttling the communications can only boost training if the\nfast CPU thread aggressively allocates GPU memory blocks and\ncauses defragmentations. If it is difficult to identify with certainty\nfrom latency measurements or profiled traces, CUDA malloc retry\ncan serve as a helpful indicator, which can be obtained from the\nnum_alloc_retries key in the torch.cuda.memory_stats() dictionary.\nThe experiments conducted with T5 models have demonstrated\nthat the rate limiter technique can greatly benefit training efficiency,\n611M\n2.28B\n11.3B\nModel Size (Numel)\n0\n20\n40\n60\n80\n100\n120\n140\nTFLOPS/GPU\n15.18\n27.40\n148.48\n15.28\n27.70\n145.81\n14.61\n25.76\n14.65\n26.04\nFull Sharding\nHybrid Sharding\nFull Replication\nDDP\n(a) Model Scale\n128\n256\n512\nNumber of GPUs\n140\n145\n150\n155\n160\n165\n170\n175\nTFLOPS/GPU\nBackward Prefetching\nNo Prefetching\n(b) GPT-175B Backward Prefetch\nRegNet\n(2 Ms)\nRegNet\n(4 Ms)\nT5\n(2 Ms)\nT5\n(4 Ms)\nDeepViT\n(2 Ms)\nDeepViT\n(4 Ms)\n0\n5\n10\n15\n20\nMedian Latency per Batch (s)\n14.81\n21.70\n8.36\n5.02\n18.78\n22.79\n14.80\n21.81\n18.61\n15.33\n18.00\n21.64\nLimit\nNo Limit\n(c) Rate Limiter (Ms = Machines)\nFigure 6: Model Scale and Training Efficiency\nyielding up to 5X speedups. However, for DeepViT models, intro-\nducing communication throttling can result in an additional 5%\noverhead. This is due to the fact that delaying the AllGather com-\nmunication can potentially block subsequent model computations\nthat rely on the AllGathered parameters, especially in cases where\ncommunication is the dominant factor. Therefore, before enabling\nrate limiting, practioners should verify whether defragmentation\nhas taken place during training.\n5.4\nEfficient Training for Large Models\nTo evaluate capability of using FSDP for large models, we ran three\ntypes of models using Full Sharding with prefetching and rate lim-\niter turned on. Activation checkpointing and BF16 mixed precision\nare also applied in these experiments. Adam optimizer is used to\nreflect a production workload setup and to incur the costly two\noptimizer states per parameter.\n\u2022 DHEN large recommendation model [33]: model size -\n768B sparse parameters and 550M dense parameters, and\nbatch size 1024.\n\u2022 minGPT transformer [10]: model size 175B, vocab size\n50000, block size 2048, batch size 1 and 2 for 128, 192, 256,\n384 and 512 GPUs.\n\u2022 HuggingFace T5 transformer [26]: model size 11B, se-\nquence length 512, batch size 8 and 16 for 8, 16, 32, 64, 128,\n256, 512 GPUs.\nIn the DHEN experiments, we further combine sharding strate-\ngies with two different configurations:\n\u2022 RAF: reshard-after-forward frees AllGathered shards from\nother GPUs after forward pass and unshards them again\nbefore backward computation. This reduces peak memory\nconsumption at the cost of higher communication overhead.\n\u2022 NRAF: no-reshard-after-forward is the opposite where the\nunsharded model parameters stay in GPU memory after\nforward pass until backward computations finish, which\ntrades higher memory footprint for lower communication\noverhead.\nThe experimental results in Figure 7 (a) and Figure 8 (a) indicate\nthat FSDP is capable of accommodating DHEN models on a large\nGPU cluster. It was observed that Full Sharding with RAF yields the\nsmallest memory footprint but with a corresponding trade-off of re-\nduced QPS. Conversely, Hybrid Sharding with NRAF demonstrated\nthe opposite behavior, as it has employs both a smaller sharding\ngroup and skips one reshard. When adding more GPUs to in the\ncluster, the peak memory usage consistently decreases as a result\nof a decrease in the size of each rank\u2019s model shard.\nWith the 175B model, the experiments achieved more than 173\nand 186 TFLOPS per GPU with batch size equal to 1 and 2 respec-\ntively as shown in Figure 7 (b). This is equivalent to approximately\n55% and 60% of GPU hardware utilization, given that the A100\u2019s\npeak is 312 TFLOPS using the BF16 tensor core. Furthermore, the\nmodel demonstrated linear scalability from 128 GPUs to 512 GPUs,\nin terms of TFLOPS, which affirms the efficacy of FSDP in handling\nlarge models with expensive computations or high-speed network\ninterconnections. Notably, with 128 GPUs, setting the batch size to\n2 resulted in a considerably lower per-GPU TFLOPs in comparison\nto other scenarios. This was due to CUDA memory defragmentation\nduring the backward pass. The backward pass contributed 85.56%\nof the iteration latency for the 128 GPU batch size equals 2 case,\nwhile a normal backward pass only accounted for about 67% in\nthese experiments. Using 128 GPUs is more likely to trigger de-\nfragmentation, as each GPU needs to accommodate a larger model\nshard. Figure 8 confirms this explanation, where the PyTorch CUDA\ncaching allocator depletes all 80GB of the CUDA memory as shown\non the top left corner.\nFinally, for T5-11B models as shown in Figure 8 (c), all exper-\niments are executed comfortably below GPU memory capacity,\nwhere defragmentations are unlikely to happen. Nevertheless, as\nthe number of GPUs increases from 8 to 512, a 7% regression in\nper-GPU TFLOPS is still evident as illustrated in Figure 7 (c). This\nsuggests that communications begin to outweigh computations on\nlarge clusters, and a near-perfect overlap between communication\nand computation is no longer attainable.\n6\nRELATED WORK\nThe DDP [14] model wrapper, which is based on the model replica-\ntion design, was an initial distributed training feature introduced\nin PyTorch [24]. Although it can handle large datasets, it cannot ac-\ncommodate the ever-increasing model sizes that are now prevalent\nin the field.\n32\n64\n128\n256\n512\nNumber of 80 GB A100s\n2800\n3000\n3200\n3400\n3600\n3800\n4000\n4200\n4400\nP90 QPS\nFull Sharding (RAF)\nFull Sharding (NRAF)\nHybrid Sharding (RAF)\nHybrid Sharding (NRAF)\n(a) DHEN QPS\n128\n192\n256\n320\n384\n448\n512\nNumber of 80 GB A100s\n100\n120\n140\n160\n180\nTFLOPS / GPU\nB = 1\nB = 2\n(b) GPT-175B TFLOPS\n8\n16\n32\n64\n128\n256\n512\nNumber of 80 GB A100s\n142\n144\n146\n148\n150\n152\n154\nTFLOPS / GPU\nB = 8\nB = 16\n(c) T5-11B TFLOPS\nFigure 7: Training Throughput: To conform with DHEN convention, we use sample/ GPU/second (QPS) for DHEN.\n32\n64\n128\n256\n512\nNumber of 80 GB A100s\n30\n40\n50\n60\n70\nPeak Memory (GB)\nFull Sharding (RAF)\nFull Sharding (NRAF)\nHybrid Sharding (RAF)\nHybrid Sharding (NRAF)\n(a) DHEN\n150\n200\n250\n300\n350\n400\n450\n500\nNumber of 80 GB A100s\n0\n10\n20\n30\n40\n50\n60\n70\n80\nPeak Memory (GB)\nAlloc (B = 1)\nActive (B = 1)\nReserved (B = 1)\nAlloc (B = 2)\nActive (B = 2)\nReserved (B = 2)\n(b) GPT-175B\n8\n16\n32\n64\n128\n256\n512\nNumber of 80 GB A100s\n0\n10\n20\n30\n40\n50\n60\nPeak Memory (GB)\nAlloc (B = 8)\nActive (B = 8)\nReserved (B = 8)\nAlloc (B = 16)\nActive (B = 16)\nReserved (B = 16)\n(c) T5-11B\nFigure 8: Memory Footprint\nZeRO [27, 28] and cross-replica sharding [30] inspired the FSDP\ndesign, but FSDP is intrinsically different. Prior work employs\nmodel partitioning or per-parameter sharding to distribute param-\neter tensors, and rely on Broadcast and Gather collective communi-\ncation primitives to synchronize values. Although this design can\nachieve the same functionality, it could lead to uneven workload\ndistribution across GPU devices, which hampers the efficiency of\nsynchronized distributed training. Additionally, since this approach\nmodifies the internals of the machine learning framework, such as\ntensor storage and memory management, it might no longer work\nwhen the internal implementation is updated or new features are\nintroduced. Therefore, a native solution that is co-designed with the\ncore components of the framework would provide a more robust\nand consistent user experience.\nMiCS [34] and FSDP differ in gradient communication strategies.\nMiCS uses a global AllReduce followed by sharding within each par-\ntition group, whereas FSDP employs AllGather and ReduceScatter. As\na result, each rank in MiCS must hold the entire model gradients,\nleading to higher memory usage than FSDP\u2019s approach of sharding\na single layer. While both MiCS and FSDP use a hybrid commu-\nnication strategy to improve efficiency at scale, FSDP\u2019s approach\nschedules AllGather within a flexibly-sized sharded group, poten-\ntially resulting in lower runtime latency than the two-hop AllGather\nutilized by MiCS. This reduced latency is crucial as the AllGather\noperation is critical to execution, and limiting the world size and\nparticipants of AllGather to accelerators within a group with good\nlocality can result in lower latency and higher throughput.\nPipeline parallelism [5, 8] involves partitioning model parame-\nters and their activations across multiple devices through the divi-\nsion of models into pipeline stages. However, pipeline parallelism\nrequires model changes and meticulous tuning for microbatch sizes,\nnumber of stages and partitions, as well as intricate scheduling\nprocedures to optimize performance by squeezing out bubbles.\nAdditionally, specific attention is given to high profile architec-\ntures such as transformers. For example, sequence parallelism [13]\nreduces activation memory in conjunction with tensor parallelism;\nPipetransformer [6] designed a dynamic 2D parallelism that allows\nchanging the dimensions of pipeline and data parallelism on the fly,\ndepending on learning signals. These methods are highly effective\nbut can be difficult to generalize as they either rely on the specific\nimplementation or the model\u2019s layered structure.\nMany existing solutions combine data parallelism with other par-\nallelisms to achieve speedup. For example, Megatron [21] demon-\nstrated highly efficient deep transformer training on large clusters\nusing 3D (data, tensor and pipeline) parallelism. Further, compiler-\nbased techniques such as Alpa [35], GSPMD [31], and FlexFlow [9]\nleverage profiling, performance modeling, user annotations and\nsearch to find the best configuration across the parallelism space of\ndata, tensor and pipeline for a given cluster. In all cases, FSDP pro-\nvides the benefit of being a drop-in replacement for data parallelism\nthat reduces data redundancy along the data parallel axis.\nOrthogonal memory-saving techniques include gradient com-\npression [2], mixed-precision training [7], tensor rematerializa-\ntion [12] and CPU-offloading [4], but they could have implications\non model accuracy and incur overhead in (un)compression, quanti-\nzation, recomputation, and host-to-device copies, respectively.\n7\nDISCUSSION\nThis section discusses how FSDP can be combined with other par-\nallelism paradigms and known limitations when adopting FSDP.\n7.1\nFSDP Interoperability\nFurther increasing scalability and efficiency of distributed training\nrequires combining FSDP with other paradigms. This section briefly\nhighlights how the FSDP design enables mixing and matching with\nother types of parallelisms.\n7.1.1\nPipeline Parallelism.\nPipeline parallel can be functionally integrated with FSDP by\nemploying FSDP to wrap each individual pipeline stage. However,\nas pipeline parallel divides input mini-batches into smaller micro-\nbatches, the default full sharding strategy in FSDP would have to\nunshard model parameters for every micro-batch. Consequently,\ncombining these approaches with default FSDP configurations may\nlead to significant communication overhead. Fortunately, FSDP\noffers alternative sharding strategies that can keep parameters\nunsharded after the forward pass, avoiding unnecessary AllGather\ncommunications per micro-batch. Admittedly, this requires stor-\ning parameters of an entire pipeline stage on the GPU device, but\nFSDP can still reduce memory usage as it still shards gradients and\noptimizer states.\n7.1.2\nTensor Parallelism.\nIn contrast to FSDP, tensor parallel keeps parameters sharded\nduring computation, which is necessary if any sub-module is too\nlarge to fit in GPU memory. Presently, PyTorch provides a prototype\nfeature called parallelize_module that can be combined with FSDP\nto construct 2D parallelism. It works by organizing devices into\na 2D mesh where PyTorch\u2019s distributed tensor DTensor manages\ntensor parallelism on one dimension and FSDP applies sharded\ndata parallelism on the other dimension. These two dimensions\ncommunicate activations and parameters, respectively. We usually\nkeep the tensor-parallel communications, which block subsequent\ncomputation, intra-node to leverage the higher network bandwidth,\nand allow the FSDP communications operate on the other mesh\ndimension inter-node.\n7.2\nLimitations\nDuring our work with production and research applications, we\nhave encountered certain limitations associated with FSDP. This sec-\ntion aims to discuss two tricky caveats that are not readily apparent\nand pose significant challenges when it comes to troubleshooting.\n7.2.1\nMathematical Equivalence.\nFSDP cannot ensure that it always achieves the same mathe-\nmatical equivalence as local training, especially with respect to\nthe optimizer computation. This stems from the fact that the op-\ntimizer step operates on the sharded parameters, whose data lay-\nout is a function of FSDP\u2019s FlatParameter sharding algorithm that\ndoes not respect individual parameter boundaries. As a result, any\noptimizer computation that depends on an original parameter\u2019s\nunsharded value (e.g. vector norm), its tensor structure (e.g. approx-\nimate second-order optimizers), or require global states over all\nparameters will become invalid. Addressing this requires uneven\nsharding, padding, or extra communication, all of which hurt perfor-\nmance. Co-designing such optimizer computations with sharding\nis an open research question.\n7.2.2\nShared Parameters.\nFor shared parameters, FSDP must ensure to not flatten them into\nmultiple FlatParameters and to ensure that they are unsharded prop-\nerly when needed for all usages. If handled incorrectly, PyTorch may\nraise an error regarding missing tensor storage or size mismatch,\nwhich can happen when an FSDP unit attempts to use a shared pa-\nrameter that has already been resharded by a preceding FSDP unit.\nThe current recommendation is to construct FSDP units such that\nthe shared parameter belongs to the lowest-common-ancestor unit\nto ensure that the shared parameter is unsharded throughout all\nusages. This may require some inspection of the model structure to\ndo correctly and may undesirably keep the FlatParameter unsharded\nfor a large interval, so we are investigating approaches to improve\nshared parameter handling.\n8\nCONCLUSION\nThis manuscript elucidates the underlying rationale, design philos-\nophy, and implementation of FullyShardedDataParallel as of PyTorch\n2.0 release. FSDP attains usability and efficiency through a set of ad-\nvanced techniques, including deferred initialization, flexible shard-\ning strategies, communication overlapping and prefetching, and\nrate limiting communication collectives. All of these techniques are\nclosely co-designed with other key PyTorch components to ensure\nthe solution is sound and robust. Evaluations show that FSDP can\nfacilitate large language and recommendation models with near\nlinear scalability.\nACKNOWLEDGMENTS\nWe are grateful to the PyTorch community and PyTorch FSDP users\nfor their feedback and contributions.\nREFERENCES\n[1] 2023. torch.amp Gradient Scaling. https://pytorch.org/docs/2.0/amp.html#\ngradient-scaling.\n[2] Youhui Bai, Cheng Li, Quan Zhou, Jun Yi, Ping Gong, Feng Yan, Ruichuan Chen,\nand Yinlong Xu. 2021. Gradient compression supercharged high-performance\ndata parallel dnn training. In Proceedings of the ACM SIGOPS 28th Symposium on\nOperating Systems Principles. 359\u2013375.\n[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot learners. Advances in neural\ninformation processing systems 33 (2020), 1877\u20131901.\n[4] Jiarui Fang, Zilin Zhu, Shenggui Li, Hui Su, Yang Yu, Jie Zhou, and Yang You. 2022.\nParallel Training of Pre-Trained Models via Chunk-Based Dynamic Memory\nManagement. IEEE Transactions on Parallel and Distributed Systems 34, 1 (2022),\n304\u2013315.\n[5] Aaron Harlap, Deepak Narayanan, Amar Phanishayee, Vivek Seshadri, Nikhil\nDevanur, Greg Ganger, and Phil Gibbons. 2018. Pipedream: Fast and efficient\npipeline parallel dnn training. arXiv preprint arXiv:1806.03377 (2018).\n[6] Chaoyang He, Shen Li, Mahdi Soltanolkotabi, and Salman Avestimehr. 2021.\nPipetransformer: Automated elastic pipelining for distributed training of trans-\nformers. arXiv preprint arXiv:2102.03161 (2021).\n[7] Xin He, Jianhua Sun, Hao Chen, and Dong Li. 2022. Campo: Cost-Aware Per-\nformance Optimization for Mixed-Precision Neural Network Training. In 2022\nUSENIX Annual Technical Conference (USENIX ATC 22). USENIX Association,\nCarlsbad, CA, 505\u2013518. https://www.usenix.org/conference/atc22/presentation/\nhe\n[8] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia\nChen, HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, et al. 2019.\nGpipe: Efficient training of giant neural networks using pipeline parallelism.\nAdvances in neural information processing systems 32 (2019).\n[9] Zhihao Jia, Matei Zaharia, and Alex Aiken. 2018. Beyond Data and Model\nParallelism for Deep Neural Networks. https://doi.org/10.48550/ARXIV.1807.\n05358\n[10] Andrej Karpathy. 2020.\nMinGPT Transformer model.\nhttps://github.com/\nkarpathy/minGPT.\n[11] Chiheon Kim, Heungsub Lee, Myungryong Jeong, Woonhyuk Baek, Boogeon\nYoon, Ildoo Kim, Sungbin Lim, and Sungwoong Kim. 2020. torchgpipe: On-the-fly\npipeline parallelism for training giant models. arXiv preprint arXiv:2004.09910\n(2020).\n[12] Marisa Kirisame, Steven Lyubomirsky, Altan Haan, Jennifer Brennan, Mike\nHe, Jared Roesch, Tianqi Chen, and Zachary Tatlock. 2020. Dynamic Tensor\nRematerialization. https://doi.org/10.48550/ARXIV.2006.09616\n[13] Vijay Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael An-\ndersch, Mohammad Shoeybi, and Bryan Catanzaro. 2022. Reducing activation\nrecomputation in large transformer models. arXiv preprint arXiv:2205.05198\n(2022).\n[14] Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis, Teng Li,\nAdam Paszke, Jeff Smith, Brian Vaughan, Pritam Damania, et al. 2020. Pytorch\ndistributed: Experiences on accelerating data parallel training. arXiv preprint\narXiv:2006.15704 (2020).\n[15] Zhuohan Li, Siyuan Zhuang, Shiyuan Guo, Danyang Zhuo, Hao Zhang, Dawn\nSong, and Ion Stoica. 2021. Terapipe: Token-level pipeline parallelism for training\nlarge-scale language models. In International Conference on Machine Learning.\nPMLR, 6543\u20136552.\n[16] Ming Liu, Liang Luo, Jacob Nelson, Luis Ceze, Arvind Krishnamurthy, and\nKishore Atreya. 2017. Incbricks: Toward in-network computation with an in-\nnetwork cache. In Proceedings of the Twenty-Second International Conference\non Architectural Support for Programming Languages and Operating Systems.\n795\u2013809.\n[17] Liang Luo, Peter West, Jacob Nelson, Arvind Krishnamurthy, and Luis Ceze. 2020.\nPlink: Discovering and exploiting locality for accelerated distributed training on\nthe public cloud. Proceedings of Machine Learning and Systems 2 (2020), 82\u201397.\n[18] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich\nElsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh\nVenkatesh, and Hao Wu. 2017. Mixed Precision Training.\nhttps://doi.org/10.\n48550/ARXIV.1710.03740\n[19] Dheevatsa Mudigere, Yuchen Hao, Jianyu Huang, Andrew Tulloch, Srinivas\nSridharan, Xing Liu, Mustafa Ozdal, Jade Nie, Jongsoo Park, Liang Luo, et al.\n2021. High-performance, distributed training of large-scale deep learning rec-\nommendation models. arXiv preprint arXiv:2104.05158 (2021).\n[20] Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil R\nDevanur, Gregory R Ganger, Phillip B Gibbons, and Matei Zaharia. 2019.\nPipeDream: Generalized pipeline parallelism for DNN training. In Proceedings of\nthe 27th ACM Symposium on Operating Systems Principles. 1\u201315.\n[21] Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley,\nMostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti,\nJulie Bernauer, Bryan Catanzaro, et al. 2021. Efficient large-scale language model\ntraining on gpu clusters using megatron-lm. In Proceedings of the International\nConference for High Performance Computing, Networking, Storage and Analysis.\n1\u201315.\n[22] NVIDIA. 2023. The NVIDIA Collective Communication Library (NCCL). https:\n//developer.nvidia.com/nccl.\n[23] OpenAI. 2023. ChatGPT. https://chat.openai.com/.\n[24] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gre-\ngory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga,\nAlban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison,\nAlykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and\nSoumith Chintala. 2019. PyTorch: An Imperative Style, High-Performance Deep\nLearning Library. In Advances in Neural Information Processing Systems 32. Cur-\nran Associates, Inc., 8024\u20138035. http://papers.nips.cc/paper/9015-pytorch-an-\nimperative-style-high-performance-deep-learning-library.pdf\n[25] Team PyTorch. 2023. DISTRIBUTED RPC FRAMEWORK. https://pytorch.org/\ndocs/stable/rpc.html.\n[26] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\nMichael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text transformer. The Journal of\nMachine Learning Research 21, 1 (2020), 5485\u20135551.\n[27] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020. Zero:\nMemory optimizations toward training trillion parameter models. In SC20: Inter-\nnational Conference for High Performance Computing, Networking, Storage and\nAnalysis. IEEE, 1\u201316.\n[28] Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase,\nShuangyan Yang, Minjia Zhang, Dong Li, and Yuxiong He. 2021. ZeRO-Offload:\nDemocratizing Billion-Scale Model Training.. In USENIX Annual Technical Con-\nference. 551\u2013564.\n[29] Nick Schneider, Florian Piewak, Christoph Stiller, and Uwe Franke. 2017. Reg-\nNet: Multimodal sensor registration using deep neural networks. In 2017 IEEE\nintelligent vehicles symposium (IV). IEEE, 1803\u20131810.\n[30] Yuanzhong Xu, HyoukJoong Lee, Dehao Chen, Hongjun Choi, Blake Hechtman,\nand Shibo Wang. 2020. Automatic cross-replica sharding of weight update in\ndata-parallel training. arXiv preprint arXiv:2004.13336 (2020).\n[31] Yuanzhong Xu, HyoukJoong Lee, Dehao Chen, Blake Hechtman, Yanping Huang,\nRahul Joshi, Maxim Krikun, Dmitry Lepikhin, Andy Ly, Marcello Maggioni, et al.\n2021. GSPMD: general and scalable parallelization for ML computation graphs.\narXiv preprint arXiv:2105.04663 (2021).\n[32] Jinhui Yuan, Xinqi Li, Cheng Cheng, Juncheng Liu, Ran Guo, Shenghang Cai, Chi\nYao, Fei Yang, Xiaodong Yi, Chuan Wu, et al. 2021. Oneflow: Redesign the dis-\ntributed deep learning framework from scratch. arXiv preprint arXiv:2110.15032\n(2021).\n[33] Buyun Zhang, Liang Luo, Xi Liu, Jay Li, Zeliang Chen, Weilin Zhang, Xiaohan\nWei, Yuchen Hao, Michael Tsang, Wenjun Wang, Yang Liu, Huayu Li, Yasmine\nBadr, Jongsoo Park, Jiyan Yang, Dheevatsa Mudigere, and Ellie Wen. 2022. DHEN:\nA Deep and Hierarchical Ensemble Network for Large-Scale Click-Through Rate\nPrediction. https://doi.org/10.48550/ARXIV.2203.11014\n[34] Zhen Zhang, Shuai Zheng, Yida Wang, Justin Chiu, George Karypis, Trishul\nChilimbi, Mu Li, and Xin Jin. 2022. MiCS: near-linear scaling for training gigantic\nmodel on public cloud. arXiv preprint arXiv:2205.00119 (2022).\n[35] Lianmin Zheng, Zhuohan Li, Hao Zhang, Yonghao Zhuang, Zhifeng Chen, Yan-\nping Huang, Yida Wang, Yuanzhong Xu, Danyang Zhuo, Eric P Xing, et al.\n2022. Alpa: Automating Inter-and {Intra-Operator} Parallelism for Distributed\nDeep Learning. In 16th USENIX Symposium on Operating Systems Design and\nImplementation (OSDI 22). 559\u2013578.\n[36] Daquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xiaochen Lian, Zihang Jiang,\nQibin Hou, and Jiashi Feng. 2021. Deepvit: Towards deeper vision transformer.\narXiv preprint arXiv:2103.11886 (2021).\n",
        "context": "that reduces data redundancy along the data parallel axis.\nOrthogonal memory-saving techniques include gradient com-\npression [2], mixed-precision training [7], tensor rematerializa-\ntion [12] and CPU-offloading [4], but they could have implications\nMany existing solutions combine data parallelism with other par-\nallelisms to achieve speedup. For example, Megatron [21] demon-\nstrated highly efficient deep transformer training on large clusters\nwith high efficiency would help expedite the progress.\nIn recent years, the community has introduced and investigated\nnumerous advanced methodologies to enlarge neural network mod-\nels. Pipeline parallelism [6, 8, 11, 15, 20] partitions a model instance"
    },
    {
        "id": 4,
        "title": "A survey of quantization methods for efficient neural network inference",
        "author": [
            "A. Gholami",
            "S. Kim",
            "Z. Dong",
            "Z. Yao",
            "M. W. Mahoney",
            "K. Keutzer"
        ],
        "year": "2022",
        "doi": null,
        "in_text_citation": "[4]",
        "sentence": "Another optimization strategy involves quantization [4]\u2013[6], which aims to minimize the memory footprint and computational requirements without significantly impacting accuracy.",
        "abstract": "As soon as abstract mathematical computations were adapted to computation on\ndigital computers, the problem of efficient representation, manipulation, and\ncommunication of the numerical values in those computations arose. Strongly\nrelated to the problem of numerical representation is the problem of\nquantization: in what manner should a set of continuous real-valued numbers be\ndistributed over a fixed discrete set of numbers to minimize the number of bits\nrequired and also to maximize the accuracy of the attendant computations? This\nperennial problem of quantization is particularly relevant whenever memory\nand/or computational resources are severely restricted, and it has come to the\nforefront in recent years due to the remarkable performance of Neural Network\nmodels in computer vision, natural language processing, and related areas.\nMoving from floating-point representations to low-precision fixed integer\nvalues represented in four bits or less holds the potential to reduce the\nmemory footprint and latency by a factor of 16x; and, in fact, reductions of 4x\nto 8x are often realized in practice in these applications. Thus, it is not\nsurprising that quantization has emerged recently as an important and very\nactive sub-area of research in the efficient implementation of computations\nassociated with Neural Networks. In this article, we survey approaches to the\nproblem of quantizing the numerical values in deep Neural Network computations,\ncovering the advantages/disadvantages of current methods. With this survey and\nits organization, we hope to have presented a useful snapshot of the current\nresearch in quantization for Neural Networks and to have given an intelligent\norganization to ease the evaluation of future research in this area.",
        "full_text": "A Survey of Quantization Methods for Ef\ufb01cient\nNeural Network Inference\nAmir Gholami\u2217, Sehoon Kim\u2217, Zhen Dong\u2217, Zhewei Yao\u2217, Michael W. Mahoney, Kurt Keutzer\nUniversity of California, Berkeley\n{amirgh, sehoonkim, zhendong, zheweiy, mahoneymw, keutzer}@berkeley.edu\nAbstract\u2014As soon as abstract mathematical computa-\ntions were adapted to computation on digital computers,\nthe problem of ef\ufb01cient representation, manipulation, and\ncommunication of the numerical values in those computa-\ntions arose. Strongly related to the problem of numerical\nrepresentation is the problem of quantization: in what\nmanner should a set of continuous real-valued numbers be\ndistributed over a \ufb01xed discrete set of numbers to minimize\nthe number of bits required and also to maximize the\naccuracy of the attendant computations? This perennial\nproblem of quantization is particularly relevant whenever\nmemory and/or computational resources are severely re-\nstricted, and it has come to the forefront in recent years due\nto the remarkable performance of Neural Network models\nin computer vision, natural language processing, and re-\nlated areas. Moving from \ufb02oating-point representations to\nlow-precision \ufb01xed integer values represented in four bits\nor less holds the potential to reduce the memory footprint\nand latency by a factor of 16x; and, in fact, reductions of\n4x to 8x are often realized in practice in these applications.\nThus, it is not surprising that quantization has emerged\nrecently as an important and very active sub-area of\nresearch in the ef\ufb01cient implementation of computations\nassociated with Neural Networks. In this article, we survey\napproaches to the problem of quantizing the numerical\nvalues in deep Neural Network computations, covering the\nadvantages/disadvantages of current methods. With this\nsurvey and its organization, we hope to have presented a\nuseful snapshot of the current research in quantization\nfor Neural Networks and to have given an intelligent\norganization to ease the evaluation of future research in\nthis area.\nI. INTRODUCTION\nOver the past decade, we have observed signi\ufb01cant\nimprovements in the accuracy of Neural Networks (NNs)\nfor a wide range of problems, often achieved by highly\nover-parameterized models. While the accuracy of these\nover-parameterized (and thus very large) NN models has\nsigni\ufb01cantly increased, the sheer size of these models\n\u2217Equal contribution.\nmeans that it is not possible to deploy them for many\nresource-constrained applications. This creates a problem\nfor realizing pervasive deep learning, which requires\nreal-time inference, with low energy consumption and\nhigh accuracy, in resource-constrained environments. This\npervasive deep learning is expected to have a signi\ufb01cant\nimpact on a wide range of applications such as real-time\nintelligent healthcare monitoring, autonomous driving,\naudio analytics, and speech recognition.\nAchieving ef\ufb01cient, real-time NNs with optimal ac-\ncuracy requires rethinking the design, training, and\ndeployment of NN models [71]. There is a large body of\nliterature that has focused on addressing these issues by\nmaking NN models more ef\ufb01cient (in terms of latency,\nmemory footprint, and energy consumption, etc.), while\nstill providing optimal accuracy/generalization trade-offs.\nThese efforts can be broadly categorized as follows.\na) Designing ef\ufb01cient NN model architectures:\nOne line of work has focused on optimizing the NN model\narchitecture in terms of its micro-architecture [101, 111,\n127, 167, 168, 212, 253, 280] (e.g., kernel types such as\ndepth-wise convolution or low-rank factorization) as well\nas its macro-architecture [100, 101, 104, 110, 214, 233]\n(e.g., module types such as residual, or inception). The\nclassical techniques here mostly found new architecture\nmodules using manual search, which is not scalable. As\nsuch, a new line of work is to design Automated machine\nlearning (AutoML) and Neural Architecture Search (NAS)\nmethods. These aim to \ufb01nd in an automated way the right\nNN architecture, under given constraints of model size,\ndepth, and/or width [161, 194, 232, 245, 252, 291]. We\nrefer interested reader to [54] for a recent survey of NAS\nmethods.\nb) Co-designing NN architecture and hardware\ntogether: Another recent line of work has been to adapt\n(and co-design) the NN architecture for a particular target\nhardware platform. The importance of this is because the\noverhead of a NN component (in terms of latency and\nenergy) is hardware-dependent. For example, hardware\narXiv:2103.13630v3  [cs.CV]  21 Jun 2021\nwith a dedicated cache hierarchy can execute bandwidth\nbound operations much more ef\ufb01ciently than hardware\nwithout such cache hierarchy. Similar to NN architecture\ndesign, initial approaches at architecture-hardware co-\ndesign were manual, where an expert would adapt/change\nthe NN architecture [70], followed by using automated\nAutoML and/or NAS techniques [22, 23, 100, 252].\nc) Pruning: Another approach to reducing the\nmemory footprint and computational cost of NNs is to\napply pruning. In pruning, neurons with small saliency\n(sensitivity) are removed, resulting in a sparse computa-\ntional graph. Here, neurons with small saliency are those\nwhose removal minimally affects the model output/loss\nfunction. Pruning methods can be broadly categorized\ninto unstructured pruning [49, 86, 139, 143, 191, 257],\nand structured pruning [91, 106, 156, 166, 274, 275, 279].\nWith unstructured pruning, one removes neurons with\nwith small saliency, wherever they occur. With this\napproach, one can perform aggressive pruning, removing\nmost of the NN parameters, with very little impact on\nthe generalization performance of the model. However,\nthis approach leads to sparse matrix operations, which\nare known to be hard to accelerate, and which are\ntypically memory-bound [21, 66]. On the other hand,\nwith structured pruning, a group of parameters (e.g.,\nentire convolutional \ufb01lters) is removed. This has the\neffect of changing the input and output shapes of layers\nand weight matrices, thus still permitting dense matrix\noperations. However, aggressive structured pruning often\nleads to signi\ufb01cant accuracy degradation. Training and\ninference with high levels of pruning/sparsity, while\nmaintaining state-of-the-art performance, has remained\nan open problem [16]. We refer the interested reader\nto [66, 96, 134] for a thorough survey of related work\nin pruning/sparsity.\nd) Knowledge distillation: Model distillation [3, 95,\n150, 177, 195, 207, 269, 270] involves training a large\nmodel and then using it as a teacher to train a more com-\npact model. Instead of using \u201chard\u201d class labels during\nthe training of the student model, the key idea of model\ndistillation is to leverage the \u201csoft\u201d probabilities produced\nby the teacher, as these probabilities can contain more\ninformation about the input. Despite the large body of\nwork on distillation, a major challenge here is to achieve a\nhigh compression ratio with distillation alone. Compared\nto quantization and pruning, which can maintain the\nperformance with \u22654\u00d7 compression (with INT8 and\nlower precision), knowledge distillation methods tend to\nhave non-negligible accuracy degradation with aggressive\ncompression. However, the combination of knowledge\ndistillation with prior methods (i.e., quantization and\npruning) has shown great success [195].\ne) Quantization: Finally, quantization is an ap-\nproach that has shown great and consistent success in\nboth training and inference of NN models. While the\nproblems of numerical representation and quantization\nare as old as digital computing, Neural Nets offer unique\nopportunities for improvement. While this survey on\nquantization is mostly focused on inference, we should\nemphasize that an important success of quantization has\nbeen in NN training [10, 35, 57, 130, 247]. In particular,\nthe breakthroughs of half-precision and mixed-precision\ntraining [41, 72, 79, 175] have been the main drivers that\nhave enabled an order of magnitude higher throughput in\nAI accelerators. However, it has proven very dif\ufb01cult to\ngo below half-precision without signi\ufb01cant tuning, and\nmost of the recent quantization research has focused on\ninference. This quantization for inference is the focus of\nthis article.\nf) Quantization and Neuroscience: Loosely related\nto (and for some a motivation for) NN quantization\nis work in neuroscience that suggests that the human\nbrain stores information in a discrete/quantized form,\nrather than in a continuous form [171, 236, 240]. A\npopular rationale for this idea is that information stored\nin continuous form will inevitably get corrupted by noise\n(which is always present in the physical environment,\nincluding our brains, and which can be induced by\nthermal, sensory, external, synaptic noise, etc.) [27, 58].\nHowever, discrete signal representations can be more\nrobust to such low-level noise. Other reasons, including\nthe higher generalization power of discrete representa-\ntions [128, 138, 242] and their higher ef\ufb01ciency under\nlimited resources [241], have also been proposed. We\nrefer the reader to [228] for a thorough review of related\nwork in neuroscience literature.\nThe goal of this work is to introduce current methods\nand concepts used in quantization and to discuss the\ncurrent challenges and opportunities in this line of\nresearch. In doing so, we have tried to discuss most\nrelevant work. It is not possible to discuss every work in\na \ufb01eld as large as NN quantization in the page limit of a\nshort survey; and there is no doubt that we have missed\nsome relevant papers. We apologize in advance both to\nthe readers and the authors of papers that we may have\nneglected.\nIn terms of the structure of this survey, we will \ufb01rst\nprovide a brief history of quantization in Section II,\nand then we will introduce basic concepts underlying\nquantization in Section III. These basic concepts are\n2\nshared with most of the quantization algorithms, and\nthey are necessary for understanding and deploying\nexisting methods. Then we discuss more advanced topics\nin Section IV. These mostly involve recent state-of-the-art\nmethods, especially for low/mixed-precision quantization.\nThen we discuss the implications of quantization in\nhardware accelerators in Section V, with a special focus\non edge processors. Finally, we provide a summary and\nconclusions in Section VII.\nII. GENERAL HISTORY OF QUANTIZATION\nGray and Neuhoff have written a very nice survey of the\nhistory of quantization up to 1998 [76]. The article is an\nexcellent one and merits reading in its entirety; however,\nfor the reader\u2019s convenience we will brie\ufb02y summarize\nsome of the key points here. Quantization, as a method\nto map from input values in a large (often continuous) set\nto output values in a small (often \ufb01nite) set, has a long\nhistory. Rounding and truncation are typical examples.\nQuantization is related to the foundations of the calculus,\nand related methods can be seen in the early 1800s\n(as well as much earlier), e.g., in early work on least-\nsquares and related techniques for large-scale (by the\nstandards of the early 1800s) data analysis [225]. An\nearly work on quantization dates back to 1867, where\ndiscretization was used to approximate the calculation\nof integrals [206]; and, subsequently, in 1897, when\nShappard investigated the impact of rounding errors on\nthe integration result [220]. More recently, quantization\nhas been important in digital signal processing, as the\nprocess of representing a signal in digital form ordinarily\ninvolves rounding, as well as in numerical analysis\nand the implementation of numerical algorithms, where\ncomputations on real-valued numbers are implemented\nwith \ufb01nite-precision arithmetic.\nIt was not until 1948, around the advent of the digital\ncomputer, when Shannon wrote his seminal paper on the\nmathematical theory of communication [215], that the\neffect of quantization and its use in coding theory were\nformally presented. In particular, Shannon argued in his\nlossless coding theory that using the same number of\nbits is wasteful, when events of interest have a non-\nuniform probability. He argued that a more optimal\napproach would be to vary the number of bits based on the\nprobability of an event, a concept that is now known as\nvariable-rate quantization. Huffman coding in particular\nis motivated by this [109]. In subsequent work in\n1959 [216], Shannon introduced distortion-rate functions\n(which provide a lower bound on the signal distortion\nafter coding) as well as the notion of vector quantization\n(also brie\ufb02y discussed in Section IV-F). This concept was\nextended and became practical in [53, 55, 67, 208] for real\ncommunication applications. Other important historical\nresearch on quantization in signal processing in that time\nperiod includes [188], which introduced the Pulse Code\nModulation (PCM) concept (a pulsing method proposed\nto approximate/represent/encode sampled analog signals),\nas well as the classical result of high resolution quanti-\nzation [14]. We refer the interested reader to [76] for a\ndetailed discussion of these issues.\nQuantization appears in a slightly different way in\nalgorithms that use numerical approximation for problems\ninvolving continuous mathematical quantities, an area that\nalso has a long history, but that also received renewed\ninterest with the advent of the digital computer. In\nnumerical analysis, an important notion was (and still is)\nthat of a well-posed problem\u2014roughly, a problem is well-\nposed if: a solution exists; that solution is unique; and\nthat solution depends continuously on the input data in\nsome reasonable topology. Such problems are sometimes\ncalled well-conditioned problems. It turned out that, even\nwhen working with a given well-conditioned problem,\ncertain algorithms that solve that problem \u201cexactly\u201d in\nsome idealized sense perform very poorly in the presence\nof \u201cnoise\u201d introduced by the peculiarities of roundoff\nand truncation errors. These roundoff errors have to do\nwith representing real numbers with only \ufb01nitely-many\nbits\u2014a quantization speci\ufb01ed, e.g., by the IEEE \ufb02oating\npoint standard; and truncation errors arise since only a\n\ufb01nite number of iterations of an iterative algorithm can\nactually be performed. The latter are important even in\n\u201cexact arithmetic,\u201d since most problems of continuous\nmathematics cannot even in principle be solved by a\n\ufb01nite sequence of elementary operations; but the former\nhave to do with quantization. These issues led to the\nnotion of the numerical stability of an algorithm. Let us\nview a numerical algorithm as a function f attempting\nto map the input data x to the \u201ctrue\u201d solution y; but\ndue to roundoff and truncation errors, the output of the\nalgorithm is actually some other y\u2217. In this case, the\nforward error of the algorithm is \u2206y = y\u2217\u2212y; and the\nbackward error of the algorithm is the smallest \u2206x such\nthat f(x + \u2206x) = y\u2217. Thus, the forward error tells us\nthe difference between the exact or true answer and what\nwas output by the algorithm; and the backward error\ntells us what input data the algorithm we ran actually\nsolved exactly. The forward error and backward error for\nan algorithm are related by the condition number of the\nproblem. We refer the interested reader to [237] for a\ndetailed discussion of these issues.\n3\nA. Quantization in Neural Nets\nNo doubt thousands of papers have been written on\nthese topics, and one might wonder: how is recent work\non NN quantization different from these earlier works?\nCertainly, many of the recently proposed \u201cnovel algo-\nrithms\u201d have strong connections with (and in some cases\nare essentially rediscoveries of) past work in the literature.\nHowever, NNs bring unique challenges and opportunities\nto the problem of quantization. First, inference and\ntraining of Neural Nets are both computationally intensive.\nSo, the ef\ufb01cient representation of numerical values is\nparticularly important. Second, most current Neural Net\nmodels are heavily over-parameterized, so there is ample\nopportunity for reducing bit precision without impacting\naccuracy. However, one very important difference is\nthat NNs are very robust to aggressive quantization and\nextreme discretization. The new degree of freedom here\nhas to do with the number of parameters involved, i.e.,\nthat we are working with over-parameterized models. This\nhas direct implications for whether we are solving well-\nposed problems, whether we are interested in forward\nerror or backward error, etc. In the NN applications\ndriving recent developments in quantization, there is not\na single well-posed or well-conditioned problem that\nis being solved. Instead, one is interested in some sort\nof forward error metric (based on classi\ufb01cation quality,\nperplexity, etc.), but due to the over-parameterization,\nthere are many very different models that exactly or\napproximately optimize this metric. Thus, it is possible\nto have high error/distance between a quantized model\nand the original non-quantized model, while still attaining\nvery good generalization performance. This added degree\nof freedom was not present in many of the classical\nresearch, which mostly focused on \ufb01nding compression\nmethods that would not change the signal too much,\nor with numerical methods in which there was strong\ncontrol on the difference between the \u201cexact\u201d versus\nthe \u201cdiscretized\u201d computation. This observation that has\nbeen the main driver for researching novel techniques for\nNN quantization. Finally,the layered structure of Neural\nNet models offers an additional dimension to explore.\nDifferent layers in a Neural Net have different impact on\nthe loss function, and this motivates a mixed-precision\napproach to quantization.\nIII. BASIC CONCEPTS OF QUANTIZATION\nIn this section, we \ufb01rst brie\ufb02y introduce common\nnotations and the problem setup in Section III-A, and\nthen we describe the basic quantization concepts and\nmethods in Section III-B-III-F. Afterwards, we discuss the\n\ud835\udc5f\n\ud835\udc44\n\ud835\udc5f\n\ud835\udc44\nFigure 1: Comparison between uniform quantization\n(left) and non-uniform quantization (right). Real values in\nthe continuous domain r are mapped into discrete, lower\nprecision values in the quantized domain Q, which are\nmarked with the orange bullets. Note that the distances\nbetween the quantized values (quantization levels) are\nthe same in uniform quantization, whereas they can vary\nin non-uniform quantization.\ndifferent \ufb01ne-tuning methods in Section III-G, followed\nby stochastic quantization in Section III-H.\nA. Problem Setup and Notations\nAssume that the NN has L layers with learnable pa-\nrameters, denoted as {W1, W2, ..., WL}, with \u03b8 denoting\nthe combination of all such parameters. Without loss of\ngenerality, we focus on the supervised learning problem,\nwhere the nominal goal is to optimize the following\nempirical risk minimization function:\nL(\u03b8) = 1\nN\nN\nX\ni=1\nl(xi, yi; \u03b8),\n(1)\nwhere (x, y) is the input data and the corresponding label,\nl(x, y; \u03b8) is the loss function (e.g., Mean Squared Error\nor Cross Entropy loss), and N is the total number of data\npoints. Let us also denote the input hidden activations of\nthe ith layer as hi, and the corresponding output hidden\nactivation as ai. We assume that we have the trained\nmodel parameters \u03b8, stored in \ufb02oating point precision. In\nquantization, the goal is to reduce the precision of both\nthe parameters (\u03b8), as well as the intermediate activation\nmaps (i.e., hi, ai) to low-precision, with minimal impact\non the generalization power/accuracy of the model. To\ndo this, we need to de\ufb01ne a quantization operator that\nmaps a \ufb02oating point value to a quantized one, which is\ndescribed next.\nB. Uniform Quantization\nWe need \ufb01rst to de\ufb01ne a function that can quantize\nNN weights and activations to a \ufb01nite set of values. This\n4\n0\n0\n\ud835\udefc= \u22121\n\ud835\udefd= 1\n\ud835\udc5f\n\ud835\udc44\n\u2212127\n127\n\u2212128\n0\n0\n\ud835\udefc= \u22120.5\n\ud835\udefd= 1.5\n\ud835\udc5f\n\ud835\udc44\n127\n\u2212\ud835\udc4d\n\ud835\udc46\ud835\udc4d\nFigure 2: Illustration of symmetric quantization and asymmetric quantization. Symmetric quantization with restricted\nrange maps real values to [-127, 127], and full range maps to [-128, 127] for 8-bit quantization.\nfunction takes real values in \ufb02oating point, and it maps\nthem to a lower precision range, as illustrated in Figure 1.\nA popular choice for a quantization function is as follows:\nQ(r) = Int\n\u0000r/S\n\u0001\n\u2212Z,\n(2)\nwhere Q is the quantization operator, r is a real valued\ninput (activation or weight), S is a real valued scaling\nfactor, and Z is an integer zero point. Furthermore,\nthe Int function maps a real value to an integer value\nthrough a rounding operation (e.g., round to nearest and\ntruncation). In essence, this function is a mapping from\nreal values r to some integer values. This method of\nquantization is also known as uniform quantization, as\nthe resulting quantized values (aka quantization levels)\nare uniformly spaced (Figure 1, left). There are also non-\nuniform quantization methods whose quantized values\nare not necessarily uniformly spaced (Figure 1, right),\nand these methods will be discussed in more detail in\nSection III-F. It is possible to recover real values r from\nthe quantized values Q(r) through an operation that is\noften referred to as dequantization:\n\u02dcr = S(Q(r) + Z).\n(3)\nNote that the recovered real values \u02dcr will not exactly\nmatch r due to the rounding operation.\nC. Symmetric and Asymmetric Quantization\nOne important factor in uniform quantization is the\nchoice of the scaling factor S in Eq. 2. This scaling factor\nessentially divides a given range of real values r into a\nnumber of partitions (as discussed in [113, 133]):\nS = \u03b2 \u2212\u03b1\n2b \u22121,\n(4)\nwhere [\u03b1, \u03b2] denotes the clipping range, a bounded range\nthat we are clipping the real values with, and b is\nthe quantization bit width. Therefore, in order for the\nscaling factor to be de\ufb01ned, the clipping range [\u03b1, \u03b2]\nshould \ufb01rst be determined. The process of choosing\nthe clipping range is often referred to as calibration.\nA straightforward choice is to use the min/max of\nthe signal for the clipping range, i.e., \u03b1 = rmin, and\n\u03b2 = rmax. This approach is an asymmetric quantization\nscheme, since the clipping range is not necessarily\nsymmetric with respect to the origin, i.e., \u2212\u03b1 \u0338= \u03b2,\nas illustrated in Figure 2 (Right). It is also possible\nto use a symmetric quantization scheme by choosing a\nsymmetric clipping range of \u03b1 = \u2212\u03b2. A popular choice\nis to choose these based on the min/max values of the\nsignal: \u2212\u03b1 = \u03b2 = max(|rmax|, |rmin|). Asymmetric\nquantization often results in a tighter clipping range as\ncompared to symmetric quantization. This is especially\nimportant when the target weights or activations are\nimbalanced, e.g., the activation after ReLU that always\nhas non-negative values. Using symmetric quantization,\nhowever, simpli\ufb01es the quantization function in Eq. 2 by\nreplacing the zero point with Z = 0:\nQ(r) = Int\n\u0010 r\nS\n\u0011\n.\n(5)\nHere, there are two choices for the scaling factor. In \u201cfull\nrange\u201d symmetric quantization S is chosen as 2max(|r|)\n2n\u22121\n(with \ufb02oor rounding mode), to use the full INT8 range\nof [-128,127]. However, in \u201crestricted range\u201d S is chosen\nas\nmax(|r|)\n2n\u22121\u22121 , which only uses the range of [-127,127].\nAs expected, the full range approach is more accurate.\nSymmetric quantization is widely adopted in practice\nfor quantizing weights because zeroing out the zero\npoint can lead to reduction in computational cost during\ninference [255], and also makes the implementation\nmore straightforward. However, note that for activation\nthe cross terms occupying due to the offset in the\nasymmetric activations are a static data independent term\n5\n!\n\u00d7\n!\n!\"#$%&'\n!\"#$%&'()\n!\"#$%&*\n!\"#$%&)\n!\"#$%&'(\n!\"#$%&')\n!\n!\"#$%&'*\n!\"#$%&'+\n!\n!\n!\"#$%&'($\n)*\"+,'-\",'.+\n/0\"++$1&'($\n)*\"+,'-\",'.+\n!\"#$\"#%&!\"\n'($\"#%&#\nFigure 3: Illustration of different quantization granularities. In layerwise quantization, the same clipping range\nis applied to all the \ufb01lters that belong to the same layer. This can result in bad quantization resolution for the\nchannels that have narrow distributions (e.g., Filter 1 in the \ufb01gure). One can achieve better quantization resolution\nusing channelwise quantization that dedicates different clipping ranges to different channels.\nand can be absorbed in the bias (or used to initialize the\naccumulator) [15].\nUsing the min/max of the signal for both symmetric\nand asymmetric quantization is a popular method. How-\never, this approach is susceptible to outlier data in the\nactivations. These could unnecessarily increase the range\nand, as a result, reduce the resolution of quantization.\nOne approach to address this is to use percentile instead\nof min/max of the signal [172]. That is to say, instead of\nthe largest/smallest value, the i-th largest/smallest values\nare used as \u03b2/\u03b1. Another approach is to select \u03b1 and\n\u03b2 to minimize KL divergence (i.e., information loss)\nbetween the real values and the quantized values [176].\nWe refer the interested readers to [255] where the different\ncalibration methods are evaluated on various models.\nSummary (Symmetric vs Asymmetric Quantiza-\ntion). Symmetric quantization partitions the clipping\nusing a symmetric range. This has the advantage of easier\nimplementation, as it leads to Z = 0 in Eq. 2. However,\nit is sub-optimal for cases where the range could be\nskewed and not symmetric. For such cases, asymmetric\nquantization is preferred.\nD. Range Calibration Algorithms: Static vs Dynamic\nQuantization\nSo far, we discussed different calibration methods for\ndetermining the clipping range of [\u03b1, \u03b2]. Another impor-\ntant differentiator of quantization methods is when the\nclipping range is determined. This range can be computed\nstatically for weights, as in most cases the parameters\nare \ufb01xed during inference. However, the activation maps\ndiffer for each input sample (x in Eq. 1). As such, there\nare two approaches to quantizing activations: dynamic\nquantization, and static quantization.\nIn dynamic quantization, this range is dynamically\ncalculated for each activation map during runtime. This\napproach requires real-time computation of the signal\nstatistics (min, max, percentile, etc.) which can have a\nvery high overhead. However, dynamic quantization often\nresults in higher accuracy as the signal range is exactly\ncalculated for each input.\nAnother quantization approach is static quantization,\nin which the clipping range is pre-calculated and static\nduring inference. This approach does not add any com-\nputational overhead, but it typically results in lower\naccuracy as compared to dynamic quantization. One\npopular method for the pre-calculation is to run a\n6\nseries of calibration inputs to compute the typical range\nof activations [113, 267]. Multiple different metrics\nhave been proposed to \ufb01nd the best range, including\nminimizing Mean Squared Error (MSE) between original\nunquantized weight distribution and the corresponding\nquantized values [40, 221, 229, 281]. One could also\nconsider using other metrics such as entropy [189],\nalthough MSE is the most common method used. Another\napproach is to learn/impose this clipping range during\nNN training [36, 146, 276, 287]. Notable work here are\nLQNets [276], PACT [36], LSQ [56], and LSQ+ [15]\nwhich jointly optimizes the clipping range and the weights\nin NN during training.\nSummary (Dynamic vs Static Quantization). Dy-\nnamic quantization dynamically computes the clipping\nrange of each activation and often achieves the highest\naccuracy. However, calculating the range of a signal\ndynamically is very expensive, and as such, practitioners\nmost often use static quantization where the clipping\nrange is \ufb01xed for all inputs.\nE. Quantization Granularity\nIn most computer vision tasks, the activation input to\na layer is convolved with many different convolutional\n\ufb01lters, as illustrated in Figure 3. Each of these convo-\nlutional \ufb01lters can have a different range of values. As\nsuch, one differentiator for quantization methods is the\ngranularity of how the clipping range [\u03b1, \u03b2] is calculated\nfor the weights. We categorized them as follows.\na) Layerwise Quantization: In this approach, the\nclipping range is determined by considering all of the\nweights in convolutional \ufb01lters of a layer [133], as shown\nin the third column of Figure 3. Here one examines the\nstatistics of the entire parameters in that layer (e.g., min,\nmax, percentile, etc.), and then uses the same clipping\nrange for all the convolutional \ufb01lters. While this approach\nis very simple to implement, it often results in sub-optimal\naccuracy, as the range of each convolutional \ufb01lter can\nbe vary a lot. For example, a convolutional kernel that\nhas relatively narrower range of parameters may lose its\nquantization resolution due to another kernel in the same\nlayer with a wider range.\nb) Groupwise Quantization: One could group mul-\ntiple different channels inside a layer to calculate the clip-\nping range (of either activations or convolution kernels).\nThis could be helpful for cases where the distribution\nof the parameters across a single convolution/activation\nvaries a lot. For instance, this approach was found\nuseful in Q-BERT [219] for quantizing Transformer [243]\nmodels that consist of fully-connected attention layers.\nHowever, this approach inevitably comes with the extra\ncost of accounting for different scaling factors.\nc) Channelwise Quantization: A popular choice\nof the clipping range is to use a \ufb01xed value for each\nconvolutional \ufb01lter, independent of other channels [105,\n113, 133, 222, 276, 285], as shown in the last column\nof Figure 3. That is to say, each channel is assigned a\ndedicated scaling factor. This ensures a better quantization\nresolution and often results in higher accuracy.\nd) Sub-channelwise Quantization: The previous\napproach could be taken to the extreme, where the\nclipping range is determined with respect to any groups\nof parameters in a convolution or fully-connected layer.\nHowever, this approach could add considerable overhead,\nsince the different scaling factors need to be taken into\naccount when processing a single convolution or full-\nconnected layer. Therefore, groupwise quantization could\nestablish a good compromise between the quantization\nresolution and the computation overhead.\nSummary (Quantization Granularity). Channelwise\nquantization is currently the standard method used for\nquantizing convolutional kernels. It enables the practi-\ntioner to adjust the clipping range for each individual ker-\nnel with negligible overhead. In contrast, sub-channelwise\nquantization may result in signi\ufb01cant overhead and is not\ncurrently the standard choice (we also refer interested\nreader to [68] for tradeoffs associated with these design\nchoices).\nF. Non-Uniform Quantization\nSome work in the literature has also explored non-\nuniform quantization [25, 38, 62, 74, 79, 99, 118, 125,\n153, 159, 179, 189, 190, 238, 248, 256, 264, 266, 276,\n284], where quantization steps as well as quantization\nlevels are allowed to be non-uniformly spaced. The formal\nde\ufb01nition of non-uniform quantization is shown in Eq. 6,\nwhere Xi represents the discrete quantization levels and\n\u2206i the quantization steps (thresholds):\nQ(r) = Xi, if r \u2208[\u2206i, \u2206i+1).\n(6)\nSpeci\ufb01cally, when the value of a real number r falls in\nbetween the quantization step \u2206i and \u2206i+1, quantizer\nQ projects it to the corresponding quantization level Xi.\nNote that neither Xi\u2019s nor \u2206i\u2019s are uniformly spaced.\nNon-uniform quantization may achieve higher accuracy\nfor a \ufb01xed bit-width, because one could better capture the\ndistributions by focusing more on important value regions\nor \ufb01nding appropriate dynamic ranges. For instance, many\nnon-uniform quantization methods have been designed for\nbell-shaped distributions of the weights and activations\n7\nPre-trained model\nPre-trained model\nTraining data\nQuantization\nRetraining / Finetuning\nQuantized model\nCalibration data\nCalibration\nQuantized model\nQuantization\nFigure 4: Comparison between Quantization-Aware Training (QAT, Left) and Post-Training Quantization (PTQ,\nRight). In QAT, a pre-trained model is quantized and then \ufb01netuned using training data to adjust parameters and\nrecover accuracy degradation. In PTQ, a pre-trained model is calibrated using calibration data (e.g., a small subset\nof training data) to compute the clipping ranges and the scaling factors. Then, the model is quantized based on the\ncalibration result. Note that the calibration process is often conducted in parallel with the \ufb01netuning process for\nQAT.\nthat often involve long tails [12, 25, 61, 115, 147, 179].\nA typical rule-based non-uniform quantization is to\nuse a logarithmic distribution [179, 283], where the\nquantization steps and levels increase exponentially\ninstead of linearly. Another popular branch is binary-\ncode-based quantization [78, 107, 118, 258, 276] where\na real-number vector r \u2208Rn is quantized into m binary\nvectors by representing r \u2248Pm\ni=1 \u03b1ibi, with the scaling\nfactors \u03b1i \u2208R and the binary vectors bi \u2208{\u22121, +1}n.\nSince there is no closed-form solution for minimizing\nthe error between r and Pm\ni=1 \u03b1ibi, previous research\nrelies on heuristic solutions. To further improve the\nquantizer, more recent work [78, 234, 258] formulates\nnon-uniform quantization as an optimization problem.\nAs shown in Eq. 7, the quantization steps/levels in the\nquantizer Q are adjusted to minimize the difference\nbetween the original tensor and the quantized counterpart.\nmin\nQ \u2225Q(r) \u2212r\u22252\n(7)\nFurthermore, the quantizer itself can also be jointly\ntrained with the model parameters. These methods are\nreferred to as learnable quantizers, and the quantization\nsteps/levels are generally trained with iterative optimiza-\ntion [258, 276] or gradient descent [125, 158, 264].\nIn addition to rule-based and optimization-based non-\nuniform quantization, clustering can also be bene\ufb01cial to\nalleviate the information loss due to quantization. Some\nworks [74, 256] use k-means on different tensors to\ndetermine the quantization steps and levels, while other\nwork [38] applies a Hessian-weighted k-means clustering\non weights to minimize the performance loss. Further\ndiscussion can be found in Section IV-F.\nSummary (Uniform vs Non-uniform Quantization).\nGenerally, non-uniform quantization enables us to better\ncapture the signal information, by assigning bits and\ndiscreitizing the range of parameters non-uniformly.\nHowever, non-uniform quantization schemes are typically\ndif\ufb01cult to deploy ef\ufb01ciently on general computation\nhardware, e.g., GPU and CPU. As such, the uniform\nquantization is currently the de-facto method due to its\nsimplicity and its ef\ufb01cient mapping to hardware.\nG. Fine-tuning Methods\nIt is often necessary to adjust the parameters in the NN\nafter quantization. This can either be performed by re-\ntraining the model, a process that is called Quantization-\nAware Training (QAT), or done without re-training,\na process that is often referred to as Post-Training\nQuantization (PTQ). A schematic comparison between\nthese two approaches is illustrated in Figure 4, and further\ndiscussed below (we refer interested reader to [183] for\nmore detailed discussion on this topic).\n1) Quantization-Aware Training:\nGiven a trained\nmodel, quantization may introduce a perturbation to the\ntrained model parameters, and this can push the model\naway from the point to which it had converged when it\nwas trained with \ufb02oating point precision. It is possible to\naddress this by re-training the NN model with quantized\nparameters so that the model can converge to a point with\nbetter loss. One popular approach is to use Quantization-\nAware Training (QAT), in which the usual forward\nand backward pass are performed on the quantized\nmodel in \ufb02oating point, but the model parameters are\nquantized after each gradient update (similar to projected\ngradient descent). In particular, it is important to do\n8\nFigure 5: Illustration of Quantization-Aware Training procedure, including the use of Straight Through Estimator\n(STE).\nthis projection after the weight update is performed\nin \ufb02oating point precision. Performing the backward\npass with \ufb02oating point is important, as accumulating\nthe gradients in quantized precision can result in zero-\ngradient or gradients that have high error, especially in\nlow-precision [42, 80, 81, 107, 159, 186, 204, 231].\nAn important subtlety in backpropagation is how the\nthe non-differentiable quantization operator (Eq. 2) is\ntreated. Without any approximation, the gradient of this\noperator is zero almost everywhere, since the rounding\noperation in Eq. 2 is a piece-wise \ufb02at operator. A\npopular approach to address this is to approximate\nthe gradient of this operator by the so-called Straight\nThrough Estimator (STE) [13]. STE essentially ignores\nthe rounding operation and approximates it with an\nidentity function, as illustrated in Figure 5.\nDespite the coarse approximation of STE, it often\nworks well in practice, except for ultra low-precision quan-\ntization such as binary quantization [8]. The work of [271]\nprovides a theoretical justi\ufb01cation for this phenomena,\nand it \ufb01nds that the coarse gradient approximation of STE\ncan in expectation correlate with population gradient (for\na proper choice of STE). From a historical perspective,\nwe should note that the original idea of STE can be\ntraced back to the seminal work of [209, 210], where an\nidentity operator was used to approximate gradient from\nthe binary neurons.\nWhile STE is the mainstream approach [226, 289],\nother approaches have also been explored in the lit-\nerature [2, 25, 31, 59, 144, 164]. We should \ufb01rst\nmention that [13] also proposes a stochastic neuron\napproach as an alternative to STE (this is brie\ufb02y discussed\nin Section III-H). Other approaches using combinatorial\noptimization [65], target propagation [140], or Gumbel-\nsoftmax [116] have also been proposed. Another different\nclass of alternative methods tries to use regularization\noperators to enforce the weight to be quantized. This\nremoves the need to use the non-differentiable quanti-\nzation operator in Eq. 2. These are often referred to\nas Non-STE methods [4, 8, 39, 99, 144, 184, 283].\nRecent research in this area includes ProxQuant [8]\nwhich removes the rounding operation in the quantization\nformula Eq. 2, and instead uses the so-called W-shape,\nnon-smooth regularization function to enforce the weights\nto quantized values. Other notable research includes\nusing pulse training to approximate the derivative of\ndiscontinuous points [45], or replacing the quantized\nweights with an af\ufb01ne combination of \ufb02oating point and\nquantized parameters [165]. The recent work of [181]\nalso suggests AdaRound, which is an adaptive rounding\nmethod as an alternative to round-to-nearest method.\nDespite interesting works in this area, these methods\noften require a lot of tuning and so far STE approach is\nthe most commonly used method.\nIn addition to adjusting model parameters, some prior\nwork found it effective to learn quantization parameters\nduring QAT as well. PACT [36] learns the clipping\nranges of activations under uniform quantization, while\nQIT [125] also learns quantization steps and levels as an\nextension to a non-uniform quantization setting. LSQ [56]\nintroduces a new gradient estimate to learn scaling factors\nfor non-negative activations (e.g., ReLU) during QAT, and\nLSQ+ [15] further extends this idea to general activation\nfunctions such as swish [202] and h-swish [100] that\n9\nproduce negative values.\nSummary (QAT). QAT has been shown to work\ndespite the coarse approximation of STE. However, the\nmain disadvantage of QAT is the computational cost of\nre-training the NN model. This re-training may need\nto be performed for several hundred epochs to recover\naccuracy, especially for low-bit precision quantization. If\na quantized model is going to be deployed for an extended\nperiod, and if ef\ufb01ciency and accuracy are especially\nimportant, then this investment in re-training is likely\nto be worth it. However, this is not always the case, as\nsome models have a relatively short lifetime. Next, we\nnext discuss an alternative approach that does not have\nthis overhead.\n2) Post-Training Quantization: An alternative to the\nexpensive QAT method is Post-Training Quantization\n(PTQ) which performs the quantization and the adjust-\nments of the weights, without any \ufb01ne-tuning [11, 24, 40,\n60, 61, 68, 69, 89, 108, 142, 148, 174, 182, 223, 281].\nAs such, the overhead of PTQ is very low and often\nnegligible. Unlike QAT, which requires a suf\ufb01cient\namount of training data for retraining, PTQ has an\nadditional advantage that it can be applied in situations\nwhere data is limited or unlabeled. However, this often\ncomes at the cost of lower accuracy as compared to QAT,\nespecially for low-precision quantization.\nFor this reason, multiple approaches have been pro-\nposed to mitigate the accuracy degradation of PTQ. For\nexample, [11, 63] observe inherent bias in the mean and\nvariance of the weight values following their quantization\nand propose bias correction methods; and [174, 182]\nshow that equalizing the weight ranges (and implicitly\nactivation ranges) between different layers or channels\ncan reduce quantization errors. ACIQ [11] analytically\ncomputes the optimal clipping range and the channel-wise\nbitwidth setting for PTQ. Although ACIQ can achieve\nlow accuracy degradation, the channel-wise activation\nquantization used in ACIQ is hard to ef\ufb01ciently deploy on\nhardware. In order to address this, the OMSE method [40]\nremoves channel-wise quantization on activation and\nproposes to conduct PTQ by optimizing the L2 distance\nbetween the quantized tensor and the corresponding\n\ufb02oating point tensor. Furthermore, to better alleviate\nthe adverse impact of outliers on PTQ, an outlier\nchannel splitting (OCS) method is proposed in [281]\nwhich duplicates and halves the channels containing\noutlier values. Another notable work is AdaRound [181]\nwhich shows that the naive round-to-nearest method for\nquantization can counter-intuitively results in sub-optimal\nsolutions, and it proposes an adaptive rounding method\nthat better reduces the loss. While AdaRound restricts\nthe changes of the quantized weights to be within \u00b11\nfrom their full-precision counterparts, AdaQuant [108]\nproposes a more general method that allows the quantized\nweights to change as needed. PTQ schemes can be taken\nto the extreme, where neither training nor testing data\nare utilized during quantization (aka zero-shot scenarios),\nwhich is discussed next.\nSummary (PTQ). In PTQ, all the weights and acti-\nvations quantization parameters are determined without\nany re-training of the NN model. As such, PTQ is a very\nfast method for quantizing NN models. However, this\noften comes at the cost of lower accuracy as compared\nto QAT.\n3) Zero-shot Quantization: As discussed so far, in\norder to achieve minimal accuracy degradation after\nquantization, we need access to the entire of a fraction\nof training data. First, we need to know the range of\nactivations so that we can clip the values and determine\nthe proper scaling factors (which is usually referred to as\ncalibration in the literature). Second, quantized models\noften require \ufb01ne-tuning to adjust the model parameters\nand recover the accuracy degradation. In many cases,\nhowever, access to the original training data is not possible\nduring the quantization procedure. This is because the\ntraining dataset is either too large to be distributed,\nproprietary (e.g., Google\u2019s JFT-300M), or sensitive due to\nsecurity or privacy concerns (e.g., medical data). Several\ndifferent methods have been proposed to address this\nchallenge, which we refer to as zero-shot quantization\n(ZSQ). Inspired by\n[182], here we \ufb01rst describe two\ndifferent levels of zero-shot quantization:\n\u2022 Level 1: No data and no \ufb01netuning (ZSQ + PTQ).\n\u2022 Level 2: No data but requires \ufb01netuning (ZSQ +\nQAT).\nLevel 1 allows faster and easier quantization without\nany \ufb01netuning. Finetuning is in general time-consuming\nand often requires additional hyperparamenter search.\nHowever, Level 2 usually results in higher accuracy,\nas \ufb01netuning helps the quantized model to recover\nthe accuracy degradation, particularly in ultra-low bit\nprecision settings [85]. The work of [182] uses a Level\n1 approach that relies on equalizing the weight ranges\nand correcting bias errors to make a given NN model\nmore amenable to quantization without any data or\n\ufb01netuning. However, as this method is based on the scale-\nequivariance property of (piece-wise) linear activation\nfunctions, it can be sub-optimal for NNs with non-linear\nactivations, such as BERT [46] with GELU [94] activation\nor MobileNetV3 [100] with swish activation [203].\n10\nMultiplication (FP32)\nFP32 Weight\nFP32 Activation \nFP32\nAccumulation (FP32)\nFP32 Activation \nDequantize\nMultiplication (FP32)\nINT4 Weight\nINT4 Activation \nFP32\nFP32\nAccumulation (FP32)\nRequantize\nFP32\nINT4 Activation \nMultiplication (INT4)\nINT4 Weight\nINT4 Activation \nINT4\nAccumulation (INT32)\nINT4 Activation \nRequantize\nINT32\nFigure 6: Comparison between full-precision inference (Left), inference with simulated quantization (Middle), and\ninference with integer-only quantization (Right).\nA popular branch of research in ZSQ is to generate\nsynthetic data similar to the real data from which the\ntarget pre-trained model is trained. The synthetic data is\nthen used for calibrating and/or \ufb01netuning the quantized\nmodel. An early work in this area [28] exploits Generative\nAdversarial Networks (GANs) [75] for synthetic data\ngeneration. Using the pre-trained model as a discriminator,\nit trains the generator so that its outputs can be well\nclassi\ufb01ed by the discriminator. Then, using the synthetic\ndata samples collected from the generator, the quantized\nmodel can be \ufb01netuned with knowledge distillation from\nthe full-precision counterpart (see Section IV-D for more\ndetails). However, this method fails to capture the internal\nstatistics (e.g., distributions of the intermediate layer\nactivations) of the real data, as it is generated only using\nthe \ufb01nal outputs of the model. Synthetic data which\ndoes not take the internal statistics into account may\nnot properly represent the real data distribution [85]. To\naddress this, a number of subsequent efforts use the statis-\ntics stored in Batch Normalization (BatchNorm) [112],\ni.e., channel-wise mean and variance, to generate more\nrealistic synthetic data. In particular, [85] generates data\nby directly minimizing the KL divergence of the internal\nstatistics, and it uses the synthetic data to calibrate and\n\ufb01netune the quantized models. Furthermore, ZeroQ [24]\nshows that the synthetic data can be used for sensitivity\nmeasurement as well as calibration, thereby enabling\nmixed-precision post-training quantization without any\naccess to the training/validation data. ZeroQ also extends\nZSQ to the object detection tasks, as it does not rely\non the output labels when generating data. Both [85]\nand [24] set the input images as trainable parameters\nand directly perform backpropagation on them until their\ninternal statistics become similar to those of the real data.\nTo take a step further, recent research [37, 90, 259] \ufb01nds\nit effective to train and exploit generative models that\ncan better capture the real data distribution and generate\nmore realistic synthetic data.\nSummary (ZSQ). Zero Shot (aka data free) quan-\ntization performs the entire quantization without any\naccess to the training/validation data. This is particularly\nimportant for Machine Learning as a Service (MLaaS)\nproviders who want to accelerate the deployment of a\ncustomer\u2019s workload, without the need to access their\ndataset. Moreover, this is important for cases where\nsecurity or privacy concerns may limit access to the\ntraining data.\nH. Stochastic Quantization\nDuring inference, the quantization scheme is usually\ndeterministic. However, this is not the only possibility,\nand some works have explored stochastic quantization for\nquantization aware training as well as reduced precision\ntraining [13, 79]. The high level intuition has been that the\nstochastic quantization may allow a NN to explore more,\nas compared to deterministic quantization. One popular\nsupporting argument has been that small weight updates\nmay not lead to any weight change, as the rounding\noperation may always return the same weights. However,\nenabling a stochastic rounding may provide the NN an\nopportunity to escape, thereby updating its parameters.\nMore formally, stochastic quantization maps the \ufb02oat-\ning number up or down with a probability associated\n11\nFP32\nFP16\nINT8\nINT4\nData Type\n102\n103\nOperators (Tops)\nTitan RTX\nA100\nOperation:\nEnergy(pJ):\n8b Add\n0.03\n16b Add\n0.05\n32b Add\n0.1\n16b FP Add\n0.4\n32b FP Add\n0.9\n8b Mult\n0.2\n32b Mult\n3.1\n16b FP Mult\n1.1\n32b FP Mult\n3.7\n32b SRAM Read (8kb)5.0\n32b DRAM Read\n640\nArea(\u03bcm\ud835\udfd0):\n36\n67\n137\n1360\n4184\n282\n3495\n1640\n7700\nN/A\nN/A\nRelative Energy Cost\nRelative Area Cost\n1\n10\n100\n1000 10000\n1\n10\n100\n1000\nFigure 7: (Left) Comparison between peak throughput for different bit-precision logic on Titan RTX and A100\nGPU. (Right) Comparison of the corresponding energy cost and relative area cost for different precision for\n45nm technology [97]. As one can see, lower precision provides exponentially better energy ef\ufb01ciency and higher\nthroughput.\nto the magnitude of the weight update. For instance,\nin [29, 79], the Int operator in Eq. 2 is de\ufb01ned as\nInt(x) =\n(\n\u230ax\u230b\nwith probability \u2308x\u2309\u2212x,\n\u2308x\u2309\nwith probability x \u2212\u230ax\u230b.\n(8)\nHowever, this de\ufb01nition cannot be used for binary\nquantization. Hence, [42] extends this to\nBinary(x) =\n(\n\u22121\nwith probability 1 \u2212\u03c3(x),\n+1\nwith probability \u03c3(x),\n(9)\nwhere Binary is a function to binarize the real value x,\nand \u03c3(\u00b7) is the sigmoid function.\nRecently, another stochastic quantization method is\nintroduced in QuantNoise [59]. QuantNoise quantizes a\ndifferent random subset of weights during each forward\npass and trains the model with unbiased gradients. This\nallows lower-bit precision quantization without signi\ufb01cant\naccuracy drop in many computer vision and natural\nlanguage processing models. However, a major challenge\nwith stochastic quantization methods is the overhead of\ncreating random numbers for every single weight update,\nand as such they are not yet adopted widely in practice.\nIV. ADVANCED CONCEPTS: QUANTIZATION BELOW 8\nBITS\nIn this section, we will discuss more advanced topics\nin quantization which are mostly used for sub-INT8\nquantization. We will \ufb01rst discuss simulated quantiza-\ntion and its difference with integer-only quantization\nin Section IV-A. Afterward, we will discuss different\nmethods for mixed-precision quantization in Section IV-B,\nfollowed by hardware-aware quantization in Section IV-C.\nThen we will describe how distillation can be used to\nboost the quantization accuracy in Section IV-D, and then\nwe will discuss extremely low bit precision quantization\nin Section IV-E. Finally, we will brie\ufb02y describe the\ndifferent methods for vector quantization in Section IV-F.\nA. Simulated and Integer-only Quantization\nThere are two common approaches to deploy a quan-\ntized NN model, simulated quantization (aka fake quan-\ntization) and integer-only quantization (aka \ufb01xed-point\nquantization). In simulated quantization, the quantized\nmodel parameters are stored in low-precision, but the\noperations (e.g. matrix multiplications and convolutions)\nare carried out with \ufb02oating point arithmetic. Therefore,\nthe quantized parameters need to be dequantized before\nthe \ufb02oating point operations as schematically shown\nin Figure 6 (Middle). As such, one cannot fully bene\ufb01t\nfrom fast and ef\ufb01cient low-precision logic with simulated\nquantization. However, in integer-only quantization, all\nthe operations are performed using low-precision integer\narithmetic [113, 132, 154, 193, 267], as illustrated\nin Figure 6 (Right). This permits the entire inference\nto be carried out with ef\ufb01cient integer arithmetic, without\nany \ufb02oating point dequantization of any parameters or\nactivations.\nIn general, performing the inference in full-precision\nwith \ufb02oating point arithmetic may help the \ufb01nal quantiza-\ntion accuracy, but this comes at the cost of not being able\nto bene\ufb01t from the low-precision logic. Low-precision\nlogic has multiple bene\ufb01ts over the full-precision coun-\nterpart in terms of latency, power consumption, and\narea ef\ufb01ciency. As shown in Figure 7 (left), many\n12\n64\nconv1\n6464\nconv2/3\n+\n6464\nconv4/5\n+\n128 128\nconv6/7\n+\n128 128\nconv8/9\n+\n. . .\n. . .\n+\n512\n512\nconv16/17\n+\n4 Bits\n8 Bits\n4 Bits\n8 Bits\n4 Bits\n8 Bits\n4 Bits\n8 Bits\n4 Bits\n8 Bits\n4 Bits\n8 Bits\n4 Bits\n8 Bits\nFC&softmax\nSensitivity: Flat vs. Sharp Local Minima\n\u22120.4\n\u22120.2 0\n0.2\n0.4\n\u22120.4\n\u22120.2\n0\n0.2\n0.4\n\u22122\n\u22121\n0\n1\n\u270f1\n\u270f2\nLoss(Log)\nBalance the \nTrade-off\nInference Latency\nINT8\nINT4\n\u22120.4\n\u22120.2 0\n0.2\n0.4\n\u22120.4\n\u22120.2\n0\n0.2\n0.4\n0\n0.5\n1\n\u270f1\n\u270f2\nLoss(Log)\n17th Block \u03bb0 = 0.7\nFigure 8: Illustration of mixed-precision quantization. In mixed-precision quantization the goal is to keep sensitive\nand ef\ufb01cient layers in higher precision, and only apply low-precision quantization to insensitive and inef\ufb01cient\nlayers. The ef\ufb01ciency metric is hardware dependant, and it could be latency or energy consumption.\nhardware processors, including NVIDIA V100 and Titan\nRTX, support fast processing of low-precision arithmetic\nthat can boost the inference throughput and latency.\nMoreover, as illustrated in Figure 7 (right) for a 45nm\ntechnology [97], low-precision logic is signi\ufb01cantly more\nef\ufb01cient in terms of energy and area. For example,\nperforming INT8 addition is 30\u00d7 more energy ef\ufb01cient\nand 116\u00d7 more area ef\ufb01cient as compared to FP32\naddition [97].\nNotable integer-only quantization works include [154],\nwhich fuses Batch Normalization into the previous\nconvolution layer, and [113], which proposes an integer-\nonly computation method for residual networks with\nbatch normalization. However, both methods are limited\nto ReLU activation. The recent work of [132] addresses\nthis limitation by approximating GELU [94], Softmax,\nand Layer Normalization [6] with integer arithmetic\nand further extends integer-only quantization to Trans-\nformer [243] architectures.\nDyadic quantization is another class of integer-only\nquantization, where all the scaling is performed with\ndyadic numbers, which are rational numbers with integer\nvalues in their numerator and a power of 2 in the\ndenominator [267]. This results in a computational graph\nthat only requires integer addition, multiplication, bit\nshifting, but no integer division. Importantly, in this\napproach, all the additions (e.g. residual connections)\nare enforced to have the same dyadic scale, which can\nmake the addition logic simpler with higher ef\ufb01ciency.\nSummary (Simulated vs Integer-only Quantiza-\ntion). In general integer-only and dyadic quantization\nare more desirable as compared to simulated/fake quanti-\nzation. This is because integer-only uses lower precision\nlogic for the arithmetic, whereas simulated quantization\nuses \ufb02oating point logic to perform the operations.\nHowever, this does not mean that fake quantization is\nnever useful. In fact, fake quantization methods can\nbe bene\ufb01cial for problems that are bandwidth-bound\nrather than compute-bound, such as in recommendation\nsystems [185]. For these tasks, the bottleneck is the\nmemory footprint and the cost of loading parameters\nfrom memory. Therefore, performing fake quantization\ncan be acceptable for these cases.\nB. Mixed-Precision Quantization\nIt is easy to see that the hardware performance im-\nproves as we use lower precision quantization. However,\nuniformly quantizing a model to ultra low-precision can\ncause signi\ufb01cant accuracy degradation. It is possible to\naddress this with mixed-precision quantization [51, 82,\n102, 162, 187, 199, 211, 239, 246, 249, 263, 282, 286].\n13\nIn this approach, each layer is quantized with different\nbit precision, as illustrated in Figure 8. One challenge\nwith this approach is that the search space for choosing\nthis bit setting is exponential in the number of layers.\nDifferent approaches have been proposed to address this\nhuge search space.\nSelecting this mixed-precision for each layer is essen-\ntially a searching problem, and many different methods\nhave been proposed for it. The recent work of [246]\nproposed a reinforcement learning (RL) based method to\ndetermine automatically the quantization policy, and the\nauthors used a hardware simulator to take the hardware\naccelerator\u2019s feedback in the RL agent feedback. The\npaper [254] formulated the mixed-precision con\ufb01guration\nsearching problem as a Neural Architecture Search (NAS)\nproblem and used the Differentiable NAS (DNAS) method\nto ef\ufb01ciently explore the search space. One disadvantage\nof these exploration-based methods [246, 254] is that they\noften require large computational resources, and their\nperformance is typically sensitive to hyperparameters and\neven initialization.\nAnother class of mixed-precision methods uses periodic\nfunction regularization to train mixed-precision models\nby automatically distinguishing different layers and\ntheir varying importance with respect to accuracy while\nlearning their respective bitwidths [184].\nDifferent than these exploration and regularization-\nbased approaches, HAWQ [51] introduces an automatic\nway to \ufb01nd the mixed-precision settings based on second-\norder sensitivity of the model. It was theoretically shown\nthat the trace of the second-order operator (i.e., the\nHessian) can be used to measure the sensitivity of a\nlayer to quantization [50], similar to results for pruning\nin the seminal work of Optimal Brain Damage [139].\nIn HAWQv2, this method was extended to mixed-\nprecision activation quantization [50], and was shown to\nbe more than 100x faster than RL based mixed-precision\nmethods [246]. Recently, in HAWQv3, an integer-only,\nhardware-aware quantization was introduced [267] that\nproposed a fast Integer Linear Programming method to\n\ufb01nd the optimal bit precision for a given application-\nspeci\ufb01c constraint (e.g., model size or latency). This work\nalso addressed the common question about hardware\nef\ufb01ciency of mixed-precision quantization by directly\ndeploying them on T4 GPUs, showing up to 50% speed\nup with mixed-precision (INT4/INT8) quantization as\ncompared to INT8 quantization.\nSummary (Mixed-precision Quantization). Mixed-\nprecision quantization has proved to be an effective and\nhardware-ef\ufb01cient method for low-precision quantization\nof different NN models. In this approach, the layers of a\nNN are grouped into sensitive/insensitive to quantization,\nand higher/lower bits are used for each layer. As such,\none can minimize accuracy degradation and still bene\ufb01t\nfrom reduced memory footprint and faster speed up with\nlow precision quantization. Recent work [267] has also\nshown that this approach is hardware-ef\ufb01cient as mixed-\nprecision is only used across operations/layers.\nC. Hardware Aware Quantization\nOne of the goals of quantization is to improve the\ninference latency. However, not all hardware provide\nthe same speed up after a certain layer/operation is\nquantized. In fact, the bene\ufb01ts from quantization is\nhardware-dependant, with many factors such as on-chip\nmemory, bandwidth, and cache hierarchy affecting the\nquantization speed up.\nIt is important to consider this fact for achieving\noptimal bene\ufb01ts through hardware-aware quantization [87,\n91, 246, 250, 254, 256, 265, 267]. In particular, the\nwork [246] uses a reinforcement learning agent to\ndetermine the hardware-aware mixed-precision setting\nfor quantization, based on a look-up table of latency\nwith respect to different layers with different bitwidth.\nHowever, this approach uses simulated hardware latency.\nTo address this the recent work of [267] directly deploys\nquantized operations in hardware, and measures the\nactual deployment latency of each layer for different\nquantization bit precisions.\nD. Distillation-Assisted Quantization\nAn interesting line of work in quantization is to\nincorporate model distillation to boost quantization accu-\nracy [126, 177, 195, 267]. Model distillation [3, 95, 150,\n177, 195, 207, 268, 270, 289] is a method in which a\nlarge model with higher accuracy is used as a teacher to\nhelp the training of a compact student model. During the\ntraining of the student model, instead of using just the\nground-truth class labels, model distillation proposes to\nleverage the soft probabilities produced by the teacher,\nwhich may contain more information of the input. That is\nthe overall loss function incorporates both the student loss\nand the distillation loss, which is typically formulated as\nfollows:\nL = \u03b1H(y, \u03c3(zs)) + \u03b2H(\u03c3(zt, T), \u03c3(zs, T))\n(10)\nIn Eq. 10, \u03b1 and \u03b2 are weighting coef\ufb01cients to tune the\namount of loss from the student model and the distillation\nloss, y is the ground-truth class label, H is the cross-\nentropy loss function, zs/zt are logits generated by the\n14\nstudent/teacher model, \u03c3 is the softmax function, and T\nis its temperature de\ufb01ned as follows:\npi =\nexp zi\nT\nP\nj exp zj\nT\n(11)\nPrevious methods of knowledge distillation focus on\nexploring different knowledge sources. [95, 150, 192] use\nlogits (the soft probabilities) as the source of knowledge,\nwhile [3, 207, 269] try to leverage the knowledge\nfrom intermediate layers. The choices of teacher models\nare also well studied, where [235, 273] use multiple\nteacher models to jointly supervise the student model,\nwhile [43, 277] apply self-distillation without an extra\nteacher model.\nE. Extreme Quantization\nBinarization, where the quantized values are con-\nstrained to a 1-bit representation, thereby drastically\nreducing the memory requirement by 32\u00d7, is the most\nextreme quantization method. Besides the memory ad-\nvantages, binary (1-bit) and ternary (2-bit) operations can\noften be computed ef\ufb01ciently with bit-wise arithmetic and\ncan achieve signi\ufb01cant acceleration over higher precisions,\nsuch as FP32 and INT8. For instance, the peak binary\narithmetic on NVIDIA V100 GPUs is 8x higher than\nINT8. However, a naive binarization method would lead\nto signi\ufb01cant accuracy degradation. As such, there is a\nlarge body of work that has proposed different solutions\nto address this [18, 25, 47, 52, 77, 78, 83, 92, 93, 120,\n122, 124, 129, 131, 135, 141, 149, 155, 160, 196, 198,\n205, 217, 249, 251, 260, 262, 288, 290].\nAn important work here is BinaryConnect [42] which\nconstrains the weights to either +1 or -1. In this approach,\nthe weights are kept as real values and are only binarized\nduring the forward and backward passes to simulate the\nbinarization effect. During the forward pass, the real-\nvalue weights are converted into +1 or -1 based on the\nsign function. Then the network can be trained using\nthe standard training method with STE to propagate the\ngradients through the non-differentiable sign function. Bi-\nnarized NN [107] (BNN) extends this idea by binarizing\nthe activations as well as the weights. Jointly binarizing\nweights and activations has the additional bene\ufb01t of\nimproved latency, since the costly \ufb02oating-point matrix\nmultiplications can be replaced with lightweight XNOR\noperations followed by bit-counting. Another interesting\nwork is Binary Weight Network (BWN) and XNOR-\nNet proposed in [45], which achieve higher accuracy by\nincorporating a scaling factor to the weights and using\n+\u03b1 or -\u03b1 instead of +1 or -1. Here, \u03b1 is the scaling factor\nchosen to minimize the distance between the real-valued\nweights and the resulting binarized weights. In other\nwords, a real-valued weight matrix W can be formulated\nas W \u2248\u03b1B, where B is a binary weight matrix that\nsatis\ufb01es the following optimization problem:\n\u03b1, B = argmin\u2225W \u2212\u03b1B\u22252.\n(12)\nFurthermore, inspired by the observation that many\nlearned weights are close to zero, there have been\nattempts to ternarize network by constraining the\nweights/activations with ternary values, e.g., +1, 0 and\n-1, thereby explicitly permitting the quantized values to\nbe zero [145, 159]. Ternarization also drastically reduces\nthe inference latency by eliminating the costly matrix\nmultiplications as binarization does. Later, Ternary-Binary\nNetwork (TBN) [244] shows that combining binary\nnetwork weights and ternary activations can achieve an\noptimal tradeoff between the accuracy and computational\nef\ufb01ciency.\nSince the naive binarization and ternarization methods\ngenerally result in severe accuracy degradation, especially\nfor complex tasks such as ImageNet classi\ufb01cation, a\nnumber of solutions have been proposed to reduce the\naccuracy degradation in extreme quantization. The work\nof [197] broadly categorizes these solutions into three\nbranches. Here, we brie\ufb02y discuss each branch, and we\nrefer the interested readers to [197] for more details.\na) Quantization Error Minimization:\nThe \ufb01rst\nbranch of solutions aims to minimize the quantization\nerror, i.e., the gap between the real values and the\nquantized values [19, 34, 62, 103, 151, 158, 164, 169,\n178, 218, 248]. Instead of using a single binary matrix\nto represent real-value weights/activations, HORQ [151]\nand ABC-Net [158] use a linear combination of multiple\nbinary matrices, i.e., W \u2248\u03b11B1 + \u00b7 \u00b7 \u00b7 + \u03b1MBM, to\nreduce the quantization error. Inspired by the fact that\nbinarizing the activations reduces their representational\ncapability for the succeeding convolution block, [178]\nand [34] show that binarization of wider networks (i.e.,\nnetworks with larger number of \ufb01lters) can achieve a\ngood trade-off between the accuracy and the model size.\nb) Improved Loss function: Another branch of\nworks focuses on the choice of loss function [48, 98,\n99, 251, 284]. Important works here are loss-aware\nbinarization and ternarization [98, 99] that directly min-\nimize the loss with respect to the binarized/ternatized\nweights. This is different from other approaches that\nonly approximate the weights and do not consider the\n\ufb01nal loss. Knowledge distillation from full-precision\nteacher models has also been shown as a promising\n15\nmethod to recover the accuracy degradation after bina-\nrization/ternarization [33, 177, 195, 260].\nc) Improved Training Method: Another interesting\nbranch of work aims for better training methods for\nbinary/ternary models [5, 20, 44, 73, 160, 164, 285, 288].\nA number of efforts point out the limitation of STE\nin backpropagating gradients through the sign function:\nSTE only propagate the gradients for the weights and/or\nactivations that are in the range of [-1, 1]. To address this,\nBNN+ [44] introduces a continuous approximation for\nthe derivative of the sign function, while [198, 261, 272]\nreplace the sign function with smooth, differentiable\nfunctions that gradually sharpens and approaches the sign\nfunction. Bi-Real Net [164] introduces identity shortcuts\nconnecting activations to activations in consecutive blocks,\nthrough which 32-bit activations can be propagated. While\nmost research focuses on reducing the inference time\nlatency, DoReFa-Net [285] quantizes the gradients in\naddition to the weights and activations, in order to\naccelerate the training as well.\nExtreme quantization has been successful in drastically\nreducing the inference/training latency as well as the\nmodel size for many CNN models on computer vision\ntasks. Recently, there have been attempts to extend this\nidea to Natural Language Processing (NLP) tasks [7, 119,\n121, 278]. Considering the prohibitive model size and\ninference latency of state-of-the-art NLP models (e.g.,\nBERT [46], RoBERTa [163], and the GPT family [17,\n200, 201]) that are pre-trained on a large amount of\nunlabeled data, extreme quantization is emerging as a\npowerful tool for bringing NLP inference tasks to the\nedge.\nSummary (Extreme Quantization). Extreme low-\nbit precision quantization is a very promising line of\nresearch. However, existing methods often incur high\naccuracy degradation as compared to baseline, unless very\nextensive tuning and hyperparameter search is performed.\nBut this accuracy degradation may be acceptable for less\ncritical applications.\nF. Vector Quantization\nAs discussed in Section II, quantization has not been\ninvented in machine learning, but has been widely studied\nin the past century in information theory, and particularly\nin digital signal processing \ufb01eld as a compression\ntool. However, the main difference between quantization\nmethods for machine learning is that fundamentally we\nare not interested to compress the signal with minimum\nchange/error as compared to the original signal. Instead,\nthe goal is to \ufb01nd a reduced-precision representation\nthat results in as small loss as possible. As such, it is\ncompletely acceptable if the quantized weights/activations\nare far away from the non-quantized ones.\nHaving said that, there are a lot of interesting ideas\nin the classical quantization methods in DSP that have\nbeen applied to NN quantization, and in particular vector\nquantization [9]. In particular, the work of [1, 30, 74,\n84, 117, 170, 180, 189, 256] clusters the weights into\ndifferent groups and use the centroid of each group as\nquantized values during inference. As shown in Eq. 13,\ni is the index of weights in a tensor, c1, ..., ck are\nthe k centroids found by the clustering, and cj is the\ncorresponding centroid to wi. After clustering, weight wi\nwill have a cluster index j related to cj in the codebook\n(look-up table).\nmin\nc1,...,ck\nX\ni\n\u2225wi \u2212cj\u22252\n(13)\nIt has been found that using a k-means clustering is\nsuf\ufb01cient to reduce the model size up to 8\u00d7 without\nsigni\ufb01cant accuracy degradation [74]. In addition to that,\njointly applying k-means based vector quantization with\npruning and Huffman coding can further reduce the model\nsize [84].\nProduct quantization [74, 227, 256] is an extension of\nvector quantization, where the weight matrix is divided\ninto submatrices and vector quantization is applied to each\nsubmatrix. Besides basic product quantization method,\nmore \ufb01ne-grained usage of clustering can further improve\nthe accuracy. For example, in [74] the residuals after\nk-means product quantization are further recursively\nquantized. And in [189], the authors apply more clusters\nfor more important quantization ranges to better preserve\nthe information.\nV. QUANTIZATION AND HARDWARE PROCESSORS\nWe have said that quantization not only reduces the\nmodel size, but it also enables faster speed and requires\nless power, in particular for hardware that has low-\nprecision logic. As such, quantization has been particu-\nlarly crucial for edge deployment in IoT and mobile\napplications. Edge devices often have tight resource\nconstraints including compute, memory, and importantly\npower budget. These are often too costly to meet for many\ndeep NN models. In addition, many edge processors do\nnot have any support \ufb02oating point operations, especially\nin micro-controllers.\nHere, we brie\ufb02y discuss different hardware platforms\nin the context of quantization. ARM Cortex-M is a group\nof 32-bit RISC ARM processor cores that are designed\n16\nQualcomm Wear 4100+\nLattice CrossLink-NX-40\nGreenWaves GAP9\nSynaptics AS-371\nKneron KL720\nKneron KL720\nQualcomm XR2\nSnapDragon 888\nMythic M1108\nMobileEye Q5\nFlexLogix Infer X1\nTesla FSD\nFigure 9: Throughput comparison of different commercial edge processors for NN inference at the edge.\nfor low-cost and power-ef\ufb01cient embedded devices. For\ninstance, the STM32 family are the microcontrollers\nbased on the ARM Cortex-M cores that are also used\nfor NN inference at the edge. Because some of the\nARM Cortex-M cores do not include dedicated \ufb02oating-\npoint units, the models should \ufb01rst be quantized before\ndeployment. CMSIS-NN [136] is a library from ARM\nthat helps quantizing and deploying NN models onto the\nARM Cortex-M cores. Speci\ufb01cally, the library leverages\n\ufb01xed-point quantization [113, 154, 267] with power-of-\ntwo scaling factors so that quantization and dequantization\nprocesses can be carried out ef\ufb01ciently with bit shifting\noperations. GAP-8 [64], a RISC-V SoC (System on Chip)\nfor edge inference with a dedicated CNN accelerator, is\nanother example of an edge processor that only supports\ninteger arithmetic. While programmable general-purpose\nprocessors are widely adopted due to their \ufb02exibility,\nGoogle Edge TPU, a purpose-built ASIC chip, is another\nemerging solution for running inference at the edge.\nUnlike Cloud TPUs that run in Google data centers with\na large amount of computing resources, the Edge TPU is\ndesigned for small and low-power devices, and thereby\nit only supports 8-bit arithmetic. NN models must be\nquantized using either quantization-aware training or post-\ntraining quantization of TensorFlow.\nFigure 9 plots the throughput of different commercial\nedge processors that are widely used for NN inference\nat the edge. In the past few years, there has been a\nsigni\ufb01cant improvement in the computing power of the\nedge processors, and this allows deployment and inference\nof costly NN models that were previously available only\non servers. Quantization, combined with ef\ufb01cient low-\nprecision logic and dedicated deep learning accelerators,\nhas been one important driving force for the evolution\nof such edge processors.\nWhile quantization is an indispensable technique for\na lot of edge processors, it can also bring a remarkable\nimprovement for non-edge processors, e.g., to meet Ser-\nvice Level Agreement (SLA) requirements such as 99th\npercentile latency. A good example is provided by the\nrecent NVIDIA Turing GPUs, and in particular T4 GPUs,\nwhich include the Turing Tensor Cores. Tensor Cores\nare specialized execution units designed for ef\ufb01cient low-\nprecision matrix multiplications.\nVI. FUTURE DIRECTIONS FOR RESEARCH IN\nQUANTIZATION\nHere, we brie\ufb02y discuss several high level challenges\nand opportunities for future research in quantization. This\nis broken down into quantization software, hardware and\nNN architecture co-design, coupled compression methods,\nand quantized training.\nQuantization Software: With current methods, it is\nstraightforward to quantize and deploy different NN\n17\nmodels to INT8, without losing accuracy. There are\nseveral software packages that can be used to deploy\nINT8 quantized models (e.g., Nvidia\u2019s TensorRT, TVM,\netc.), each with good documentation. Furthermore, the\nimplementations are also quite optimal and one can\neasily observe speed up with quantization. However, the\nsoftware for lower bit-precision quantization is not widely\navailable, and sometimes it is non-existent. For instance,\nNvidia\u2019s TensorRT does not currently support sub-INT8\nquantization. Moreover, support for INT4 quantization\nwas only recently added to TVM [267]. Recent work has\nshown that low precision and mixed-precision quantiza-\ntion with INT4/INT8 works in practice [51, 82, 102, 108,\n187, 199, 211, 239, 246, 246, 249, 263, 267, 286]. Thus,\ndeveloping ef\ufb01cient software APIs for lower precision\nquantization will have an important impact.\nHardware and NN Architecture Co-Design: As dis-\ncussed above, an important difference between classical\nwork in low-precision quantization and the recent work in\nmachine learning is the fact that NN parameters may have\nvery different quantized values but may still generalize\nsimilarly well. For example, with quantization-aware\ntraining, we might converge to a different solution, far\naway from the original solution with single precision\nparameters, but still get good accuracy. One can take\nadvantage of this degree of freedom and also adapt the\nNN architecture as it is being quantized. For instance,\nthe recent work of [34] shows that changing the width of\nthe NN architecture could reduce/remove generalization\ngap after quantization. One line of future work is to\nadapt jointly other architecture parameters, such as depth\nor individual kernels, as the model is being quantized.\nAnother line of future work is to extend this co-design\nto hardware architecture. This may be particularly useful\nfor FPGA deployment, as one can explore many different\npossible hardware con\ufb01gurations (such as different micro-\narchitectures of multiply-accumulate elements), and then\ncouple this with the NN architecture and quantization\nco-design.\nCoupled Compression Methods: As discussed above,\nquantization is only one of the methods for ef\ufb01cient\ndeployment of NNs. Other methods include ef\ufb01cient\nNN architecture design, co-design of hardware and\nNN architecture, pruning, and knowledge distillation.\nQuantization can be coupled with these other approaches.\nHowever, there is currently very little work exploring\nwhat are the optimal combinations of these methods. For\ninstance, pruning and quantization can be applied together\nto a model to reduce its overhead [87, 152], and it is\nimportant to understand the best combination of struc-\ntured/unstructured pruning and quantization. Similarly,\nanother future direction is to study the coupling between\nthese methods and other approaches described above.\nQuantized Training: Perhaps the most important use\nof quantization has been to accelerate NN training with\nhalf-precision [41, 72, 79, 175]. This has enabled the use\nof much faster and more power-ef\ufb01cient reduced-precision\nlogic for training. However, it has been very dif\ufb01cult\nto push this further down to INT8 precision training.\nWhile several interesting works exist in this area [10, 26,\n123, 137, 173], the proposed methods often require a lot\nof hyperparameter tuning, or they only work for a few\nNN models on relatively easy learning tasks. The basic\nproblem is that, with INT8 precision, the training can\nbecome unstable and diverge. Addressing this challenge\ncan have a high impact on several applications, especially\nfor training at the edge.\nVII. SUMMARY AND CONCLUSIONS\nAs soon as abstract mathematical computations were\nadapted to computation on digital computers, the problem\nof ef\ufb01cient representation, manipulation, and communi-\ncation of the numerical values in those computations\narose. Strongly related to the problem of numerical\nrepresentation is the problem of quantization: in what\nmanner should a set of continuous real-valued numbers\nbe distributed over a \ufb01xed discrete set of numbers\nto minimize the number of bits required and also to\nmaximize the accuracy of the attendant computations?\nWhile these problems are as old as computer science,\nthese problems are especially relevant to the design\nof ef\ufb01cient NN models. There are several reasons for\nthis. First, NNs are computationally intensive. So, the\nef\ufb01cient representation of numerical values is particularly\nimportant. Second, most current NN models are heavily\nover-parameterized. So, there is ample opportunity for\nreducing the bit precision without impacting accuracy.\nThird, the layered structure of NN models offers an\nadditional dimension to explore. Thus, different layers in\nthe NN have different impact on the loss function, and this\nmotivates interesting approaches such mixed-precision\nquantization.\nMoving from \ufb02oating-point representations to low-\nprecision \ufb01xed integer values represented in eight/four\nbits or less holds the potential to reduce the memory\nfootprint and latency. [157] shows that INT8 inference of\npopular computer vision models, including ResNet50 [88],\nVGG-19 [224], and inceptionV3 [230] using TVM [32]\nquantization library, can achieve 3.89\u00d7, 3.32\u00d7, and\n5.02\u00d7 speedup on NVIDIA GTX 1080, respectively.\n18\n[213] further shows that INT4 inference of ResNet50\ncould bring an additional 50-60% speedup on NVIDIA T4\nand RTX, compared to its INT8 counterpart, emphasizing\nthe importance of using lower-bit precision to maxi-\nmize ef\ufb01ciency. Recently, [267] leverages mix-precision\nquantization to achieve 23% speedup for ResNet50, as\ncompared to INT8 inference without accuracy degrada-\ntion, and [132] extends INT8-only inference to BERT\nmodel to enable up to 4.0\u00d7 faster inference than FP32.\nWhile the aforementioned works focus on acceleration\non GPUs, [114] also obtained 2.35\u00d7 and 1.40\u00d7 latency\nspeedup on Intel Cascade Lake CPU and Raspberry Pi4\n(which are both non-GPU architectures), respectively,\nthrough INT8 quantization of various computer vision\nmodels. As a result, as our bibliography attests, the\nproblem of quantization in NN models has been a highly\nactive research area.\nIn this work, we have tried to bring some conceptual\nstructure to these very diverse efforts. We began with\na discussion of topics common to many applications of\nquantization, such as uniform, non-uniform, symmetric,\nasymmetric, static, and dynamic quantization. We then\nconsidered quantization issues that are more unique to\nthe quantization of NNs. These include layerwise, group-\nwise, channelwise, and sub-channelwise quantization. We\nfurther considered the inter-relationship between training\nand quantization, and we discussed the advantages and\ndisadvantages of quantization-aware training as compared\nto post-training quantization. Further nuancing the discus-\nsion of the relationship between quantization and training\nis the issue of the availability of data. The extreme case\nof this is one in which the data used in training are,\ndue to a variety of sensible reasons such as privacy, no\nlonger available. This motivates the problem of zero-shot\nquantization.\nAs we are particularly concerned about ef\ufb01cient NNs\ntargeted for edge-deployment, we considered problems\nthat are unique to this environment. These include\nquantization techniques that result in parameters rep-\nresented by fewer than 8 bits, perhaps as low as binary\nvalues. We also considered the problem of integer-only\nquantization, which enables the deployment of NNs on\nlow-end microprocessors which often lack \ufb02oating-point\nunits.\nWith this survey and its organization, we hope to have\npresented a useful snapshot of the current research in\nquantization for Neural Networks and to have given an\nintelligent organization to ease the evaluation of future\nresearch in this area.\nACKNOWLEDGMENTS\nThe UC Berkeley team also acknowledges gracious\nsupport from Samsung (in particular Joseph Hassoun),\nIntel corporation, Intel VLAB team, Google TRC team,\nand Google Brain (in particular Prof. David Patterson,\nDr. Ed Chi, and Jing Li). Amir Gholami was supported\nthrough through funding from Samsung SAIT. Our\nconclusions do not necessarily re\ufb02ect the position or\nthe policy of our sponsors, and no of\ufb01cial endorsement\nshould be inferred.\nREFERENCES\n[1] Eirikur Agustsson, Fabian Mentzer, Michael\nTschannen, Lukas Cavigelli, Radu Timofte, Luca\nBenini, and Luc Van Gool. Soft-to-hard vector\nquantization for end-to-end learning compressible\nrepresentations. arXiv preprint arXiv:1704.00648,\n2017.\n[2] Eirikur Agustsson and Lucas Theis. Universally\nquantized neural compression. Advances in neural\ninformation processing systems, 2020.\n[3] Sungsoo Ahn, Shell Xu Hu, Andreas Damianou,\nNeil D Lawrence, and Zhenwen Dai. Variational\ninformation distillation for knowledge transfer.\nIn Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages\n9163\u20139171, 2019.\n[4] Milad Alizadeh, Arash Behboodi, Mart van Baalen,\nChristos Louizos, Tijmen Blankevoort, and Max\nWelling. Gradient l1 regularization for quantization\nrobustness.\narXiv preprint arXiv:2002.07520,\n2020.\n[5] Milad\nAlizadeh,\nJavier\nFern\u00e1ndez-Marqu\u00e9s,\nNicholas D Lane, and Yarin Gal. An empirical\nstudy of binary neural networks\u2019 optimisation.\nIn\nInternational\nConference\non\nLearning\nRepresentations, 2018.\n[6] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E\nHinton.\nLayer normalization.\narXiv preprint\narXiv:1607.06450, 2016.\n[7] Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang,\nJing Jin, Xin Jiang, Qun Liu, Michael Lyu, and\nIrwin King. Binarybert: Pushing the limit of bert\nquantization.\narXiv preprint arXiv:2012.15701,\n2020.\n[8] Yu Bai, Yu-Xiang Wang, and Edo Liberty. Prox-\nquant: Quantized neural networks via proximal\noperators. arXiv preprint arXiv:1810.00861, 2018.\n[9] Dana Harry Ballard. An introduction to natural\ncomputation. MIT press, 1999.\n19\n[10] Ron Banner, Itay Hubara, Elad Hoffer, and Daniel\nSoudry. Scalable methods for 8-bit training of\nneural networks. Advances in neural information\nprocessing systems, 2018.\n[11] Ron Banner, Yury Nahshan, Elad Hoffer, and\nDaniel Soudry. Post-training 4-bit quantization of\nconvolution networks for rapid-deployment. arXiv\npreprint arXiv:1810.05723, 2018.\n[12] Chaim Baskin, Eli Schwartz, Evgenii Zheltonozh-\nskii, Natan Liss, Raja Giryes, Alex M Bronstein,\nand Avi Mendelson. Uniq: Uniform noise injection\nfor non-uniform quantization of neural networks.\narXiv preprint arXiv:1804.10969, 2018.\n[13] Yoshua Bengio, Nicholas L\u00e9onard, and Aaron\nCourville.\nEstimating or propagating gradients\nthrough stochastic neurons for conditional compu-\ntation. arXiv preprint arXiv:1308.3432, 2013.\n[14] William Ralph Bennett. Spectra of quantized sig-\nnals. The Bell System Technical Journal, 27(3):446\u2013\n472, 1948.\n[15] Yash Bhalgat, Jinwon Lee, Markus Nagel, Tijmen\nBlankevoort, and Nojun Kwak.\nLsq+: Improv-\ning low-bit quantization through learnable offsets\nand better initialization. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and\nPattern Recognition Workshops, pages 696\u2013697,\n2020.\n[16] Davis\nBlalock,\nJose\nJavier\nGonzalez\nOrtiz,\nJonathan Frankle, and John Guttag. What is the\nstate of neural network pruning? arXiv preprint\narXiv:2003.03033, 2020.\n[17] Tom B Brown, Benjamin Mann, Nick Ryder,\nMelanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, et al. Language models are few-\nshot learners. arXiv preprint arXiv:2005.14165,\n2020.\n[18] Adrian Bulat, Brais Martinez, and Georgios Tz-\nimiropoulos. High-capacity expert binary networks.\nInternational Conference on Learning Representa-\ntions, 2021.\n[19] Adrian Bulat and Georgios Tzimiropoulos. Xnor-\nnet++: Improved binary neural networks. arXiv\npreprint arXiv:1909.13863, 2019.\n[20] Adrian Bulat, Georgios Tzimiropoulos, Jean Kos-\nsai\ufb01, and Maja Pantic. Improved training of binary\nnetworks for human pose estimation and image\nrecognition.\narXiv preprint arXiv:1904.05868,\n2019.\n[21] Aydin Buluc and John R Gilbert. Challenges and\nadvances in parallel sparse matrix-matrix multipli-\ncation. In 2008 37th International Conference on\nParallel Processing, pages 503\u2013510. IEEE, 2008.\n[22] Han Cai, Chuang Gan, Tianzhe Wang, Zhekai\nZhang, and Song Han. Once-for-all: Train one\nnetwork and specialize it for ef\ufb01cient deployment.\narXiv preprint arXiv:1908.09791, 2019.\n[23] Han Cai, Ligeng Zhu, and Song Han. Proxylessnas:\nDirect neural architecture search on target task and\nhardware. arXiv preprint arXiv:1812.00332, 2018.\n[24] Yaohui Cai, Zhewei Yao, Zhen Dong, Amir Gho-\nlami, Michael W Mahoney, and Kurt Keutzer.\nZeroq: A novel zero shot quantization framework.\nIn Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages\n13169\u201313178, 2020.\n[25] Zhaowei Cai, Xiaodong He, Jian Sun, and Nuno\nVasconcelos. Deep learning with low precision by\nhalf-wave gaussian quantization. In Proceedings\nof the IEEE Conference on Computer Vision and\nPattern Recognition, pages 5918\u20135926, 2017.\n[26] L\u00e9opold Cambier, Anahita Bhiwandiwalla, Ting\nGong, Mehran Nekuii, Oguz H Elibol, and Hanlin\nTang. Shifted and squeezed 8-bit \ufb02oating point\nformat for low-precision training of deep neural\nnetworks. arXiv preprint arXiv:2001.05674, 2020.\n[27] Rishidev Chaudhuri and Ila Fiete.\nComputa-\ntional principles of memory. Nature neuroscience,\n19(3):394, 2016.\n[28] Hanting Chen, Yunhe Wang, Chang Xu, Zhaohui\nYang, Chuanjian Liu, Boxin Shi, Chunjing Xu,\nChao Xu, and Qi Tian.\nData-free learning of\nstudent networks. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision,\npages 3514\u20133522, 2019.\n[29] Jianfei Chen, Yu Gai, Zhewei Yao, Michael W\nMahoney, and Joseph E Gonzalez. A statistical\nframework for low-bitwidth training of deep neural\nnetworks. arXiv preprint arXiv:2010.14298, 2020.\n[30] Kuilin Chen and Chi-Guhn Lee.\nIncremental\nfew-shot learning via vector quantization in deep\nembedded space. In International Conference on\nLearning Representations, 2021.\n[31] Shangyu Chen, Wenya Wang, and Sinno Jialin\nPan. Metaquant: Learning to quantize by learn-\ning to penetrate non-differentiable quantization.\nIn H. Wallach, H. Larochelle, A. Beygelzimer,\nF. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors,\nAdvances in Neural Information Processing Sys-\ntems, volume 32. Curran Associates, Inc., 2019.\n20\n[32] Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lian-\nmin Zheng, Eddie Yan, Haichen Shen, Meghan\nCowan, Leyuan Wang, Yuwei Hu, Luis Ceze,\net al. TVM: An automated end-to-end optimizing\ncompiler for deep learning. In 13th {USENIX}\nSymposium on Operating Systems Design and Im-\nplementation ({OSDI} 18), pages 578\u2013594, 2018.\n[33] Xiuyi Chen, Guangcan Liu, Jing Shi, Jiaming Xu,\nand Bo Xu. Distilled binary neural network for\nmonaural speech separation. In 2018 International\nJoint Conference on Neural Networks (IJCNN),\npages 1\u20138. IEEE, 2018.\n[34] Ting-Wu Chin, Pierce I-Jen Chuang, Vikas Chan-\ndra, and Diana Marculescu. One weight bitwidth\nto rule them all. Proceedings of the European\nConference on Computer Vision (ECCV), 2020.\n[35] Brian Chmiel, Liad Ben-Uri, Moran Shkolnik, Elad\nHoffer, Ron Banner, and Daniel Soudry. Neural\ngradients are near-lognormal: improved quantized\nand sparse training. In International Conference\non Learning Representations, 2021.\n[36] Jungwook Choi, Zhuo Wang, Swagath Venkatara-\nmani, Pierce I-Jen Chuang, Vijayalakshmi Srini-\nvasan, and Kailash Gopalakrishnan. Pact: Param-\neterized clipping activation for quantized neural\nnetworks. arXiv preprint arXiv:1805.06085, 2018.\n[37] Yoojin Choi, Jihwan Choi, Mostafa El-Khamy, and\nJungwon Lee. Data-free network quantization with\nadversarial knowledge distillation. In Proceedings\nof the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition Workshops, pages 710\u2013\n711, 2020.\n[38] Yoojin Choi, Mostafa El-Khamy, and Jungwon Lee.\nTowards the limit of network quantization. arXiv\npreprint arXiv:1612.01543, 2016.\n[39] Yoojin Choi, Mostafa El-Khamy, and Jungwon\nLee.\nLearning low precision deep neural net-\nworks through regularization.\narXiv preprint\narXiv:1809.00095, 2, 2018.\n[40] Yoni Choukroun, Eli Kravchik, Fan Yang, and\nPavel Kisilev. Low-bit quantization of neural net-\nworks for ef\ufb01cient inference. In ICCV Workshops,\npages 3009\u20133018, 2019.\n[41] Matthieu Courbariaux, Yoshua Bengio, and Jean-\nPierre David.\nTraining deep neural networks\nwith low precision multiplications. arXiv preprint\narXiv:1412.7024, 2014.\n[42] Matthieu Courbariaux, Yoshua Bengio, and Jean-\nPierre David. BinaryConnect: Training deep neural\nnetworks with binary weights during propagations.\nIn Advances in neural information processing\nsystems, pages 3123\u20133131, 2015.\n[43] Elliot J Crowley, Gavin Gray, and Amos J Storkey.\nMoonshine: Distilling with cheap convolutions. In\nNeurIPS, pages 2893\u20132903, 2018.\n[44] Sajad Darabi, Mouloud Belbahri, Matthieu Cour-\nbariaux, and Vahid Partovi Nia. Bnn+: Improved\nbinary network training. 2018.\n[45] Lei Deng, Peng Jiao, Jing Pei, Zhenzhi Wu,\nand Guoqi Li. Gxnor-net: Training deep neural\nnetworks with ternary weights and activations\nwithout full-precision memory under a uni\ufb01ed dis-\ncretization framework. Neural Networks, 100:49\u2013\n58, 2018.\n[46] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. Bert: Pre-training of deep bidi-\nrectional transformers for language understanding.\narXiv preprint arXiv:1810.04805, 2018.\n[47] James Diffenderfer and Bhavya Kailkhura. Multi-\nprize lottery ticket hypothesis: Finding accurate\nbinary neural networks by pruning a randomly\nweighted network. In International Conference on\nLearning Representations, 2021.\n[48] Ruizhou Ding, Ting-Wu Chin, Zeye Liu, and\nDiana Marculescu.\nRegularizing activation dis-\ntribution for training binarized deep networks.\nIn Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages\n11408\u201311417, 2019.\n[49] Xin Dong, Shangyu Chen, and Sinno Jialin Pan.\nLearning to prune deep neural networks via\nlayer-wise optimal brain surgeon. arXiv preprint\narXiv:1705.07565, 2017.\n[50] Zhen Dong, Zhewei Yao, Daiyaan Arfeen, Amir\nGholami, Michael W. Mahoney, and Kurt Keutzer.\nHAWQ-V2: Hessian aware trace-weighted quan-\ntization of neural networks. Advances in neural\ninformation processing systems, 2020.\n[51] Zhen\nDong,\nZhewei\nYao,\nAmir\nGholami,\nMichael W Mahoney, and Kurt Keutzer. Hawq:\nHessian aware quantization of neural networks\nwith mixed-precision.\nIn Proceedings of the\nIEEE/CVF International Conference on Computer\nVision, pages 293\u2013302, 2019.\n[52] Yueqi Duan, Jiwen Lu, Ziwei Wang, Jianjiang\nFeng, and Jie Zhou. Learning deep binary descrip-\ntor with multi-quantization. In Proceedings of the\nIEEE conference on computer vision and pattern\nrecognition, pages 1183\u20131192, 2017.\n[53] JG Dunn. The performance of a class of n dimen-\n21\nsional quantizers for a gaussian source. In Proc.\nColumbia Symp. Signal Transmission Processing,\npages 76\u201381, 1965.\n[54] Thomas Elsken, Jan Hendrik Metzen, Frank Hutter,\net al. Neural architecture search: A survey. J. Mach.\nLearn. Res., 20(55):1\u201321, 2019.\n[55] William H Equitz. A new vector quantization clus-\ntering algorithm. IEEE transactions on acoustics,\nspeech, and signal processing, 37(10):1568\u20131575,\n1989.\n[56] Steven K Esser, Jeffrey L McKinstry, Deepika\nBablani, Rathinakumar Appuswamy, and Dharmen-\ndra S Modha. Learned step size quantization. arXiv\npreprint arXiv:1902.08153, 2019.\n[57] Fartash Faghri, Iman Tabrizian, Ilia Markov, Dan\nAlistarh, Daniel Roy, and Ali Ramezani-Kebrya.\nAdaptive gradient quantization for data-parallel\nsgd. Advances in neural information processing\nsystems, 2020.\n[58] A Aldo Faisal, Luc PJ Selen, and Daniel M\nWolpert. Noise in the nervous system. Nature\nreviews neuroscience, 9(4):292\u2013303, 2008.\n[59] Angela Fan, Pierre Stock, Benjamin Graham,\nEdouard Grave, R\u00e9mi Gribonval, Herv\u00e9 J\u00e9gou, and\nArmand Joulin. Training with quantization noise\nfor extreme model compression. arXiv e-prints,\npages arXiv\u20132004, 2020.\n[60] Jun Fang, Ali Sha\ufb01ee, Hamzah Abdel-Aziz, David\nThorsley, Georgios Georgiadis, and Joseph Has-\nsoun.\nNear-lossless post-training quantization\nof deep neural networks via a piecewise linear\napproximation. arXiv preprint arXiv:2002.00104,\n2020.\n[61] Jun Fang, Ali Sha\ufb01ee, Hamzah Abdel-Aziz, David\nThorsley, Georgios Georgiadis, and Joseph H Has-\nsoun. Post-training piecewise linear quantization\nfor deep neural networks. In European Conference\non Computer Vision, pages 69\u201386. Springer, 2020.\n[62] Julian Faraone, Nicholas Fraser, Michaela Blott,\nand Philip HW Leong. Syq: Learning symmetric\nquantization for ef\ufb01cient deep neural networks. In\nProceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 4300\u20134309,\n2018.\n[63] Alexander Finkelstein, Uri Almog, and Mark\nGrobman. Fighting quantization bias with bias.\narXiv preprint arXiv:1906.03193, 2019.\n[64] Eric Flamand, Davide Rossi, Francesco Conti, Igor\nLoi, Antonio Pullini, Florent Rotenberg, and Luca\nBenini. Gap-8: A risc-v soc for ai at the edge of the\niot. In 2018 IEEE 29th International Conference\non Application-speci\ufb01c Systems, Architectures and\nProcessors (ASAP), pages 1\u20134. IEEE, 2018.\n[65] Abram L Friesen and Pedro Domingos. Deep learn-\ning as a mixed convex-combinatorial optimization\nproblem. arXiv preprint arXiv:1710.11573, 2017.\n[66] Trevor Gale, Erich Elsen, and Sara Hooker. The\nstate of sparsity in deep neural networks. arXiv\npreprint arXiv:1902.09574, 2019.\n[67] AE Gamal, L Hemachandra, Itzhak Shperling, and\nV Wei. Using simulated annealing to design good\ncodes. IEEE Transactions on Information Theory,\n33(1):116\u2013123, 1987.\n[68] Sahaj Garg, Anirudh Jain, Joe Lou, and Mitchell\nNahmias.\nConfounding\ntradeoffs\nfor\nneu-\nral\nnetwork\nquantization.\narXiv\npreprint\narXiv:2102.06366, 2021.\n[69] Sahaj Garg, Joe Lou, Anirudh Jain, and Mitchell\nNahmias. Dynamic precision analog computing for\nneural networks. arXiv preprint arXiv:2102.06365,\n2021.\n[70] Amir Gholami, Kiseok Kwon, Bichen Wu, Zizheng\nTai, Xiangyu Yue, Peter Jin, Sicheng Zhao, and\nKurt Keutzer.\nSqueezeNext: Hardware-aware\nneural network design. Workshop paper in CVPR,\n2018.\n[71] Amir Gholami, Michael W Mahoney, and Kurt\nKeutzer. An integrated approach to neural network\ndesign, training, and inference. Univ. California,\nBerkeley, Berkeley, CA, USA, Tech. Rep, 2020.\n[72] Boris Ginsburg, Sergei Nikolaev, Ahmad Kiswani,\nHao Wu, Amir Gholaminejad, Slawomir Kierat,\nMichael Houston, and Alex Fit-Florea. Tensor pro-\ncessing using low precision format, December 28\n2017. US Patent App. 15/624,577.\n[73] Ruihao Gong, Xianglong Liu, Shenghu Jiang,\nTianxiang Li, Peng Hu, Jiazhen Lin, Fengwei Yu,\nand Junjie Yan. Differentiable soft quantization:\nBridging full-precision and low-bit neural networks.\nIn Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 4852\u20134861,\n2019.\n[74] Yunchao Gong, Liu Liu, Ming Yang, and Lubomir\nBourdev.\nCompressing deep convolutional net-\nworks using vector quantization. arXiv preprint\narXiv:1412.6115, 2014.\n[75] Ian J Goodfellow, Jean Pouget-Abadie, Mehdi\nMirza, Bing Xu, David Warde-Farley, Sherjil\nOzair, Aaron Courville, and Yoshua Bengio. Gen-\nerative adversarial networks.\narXiv preprint\n22\narXiv:1406.2661, 2014.\n[76] Robert M. Gray and David L. Neuhoff. Quanti-\nzation. IEEE transactions on information theory,\n44(6):2325\u20132383, 1998.\n[77] Nianhui Guo, Joseph Bethge, Haojin Yang, Kai\nZhong, Xuefei Ning, Christoph Meinel, and\nYu Wang. Boolnet: Minimizing the energy con-\nsumption of binary neural networks. arXiv preprint\narXiv:2106.06991, 2021.\n[78] Yiwen Guo, Anbang Yao, Hao Zhao, and Yurong\nChen.\nNetwork sketching: Exploiting binary\nstructure in deep cnns.\nIn Proceedings of the\nIEEE Conference on Computer Vision and Pattern\nRecognition, pages 5955\u20135963, 2017.\n[79] Suyog Gupta, Ankur Agrawal, Kailash Gopalakr-\nishnan, and Pritish Narayanan.\nDeep learning\nwith limited numerical precision. In International\nconference on machine learning, pages 1737\u20131746.\nPMLR, 2015.\n[80] Philipp Gysel, Mohammad Motamedi, and So-\nheil Ghiasi.\nHardware-oriented approximation\nof convolutional neural networks. arXiv preprint\narXiv:1604.03168, 2016.\n[81] Philipp Gysel, Jon Pimentel, Mohammad Mo-\ntamedi, and Soheil Ghiasi. Ristretto: A framework\nfor empirical study of resource-ef\ufb01cient inference\nin convolutional neural networks. IEEE transac-\ntions on neural networks and learning systems,\n29(11):5784\u20135789, 2018.\n[82] Hai Victor Habi, Roy H Jennings, and Arnon\nNetzer.\nHmq: Hardware friendly mixed preci-\nsion quantization block for cnns. arXiv preprint\narXiv:2007.09952, 2020.\n[83] Kai Han, Yunhe Wang, Yixing Xu, Chunjing Xu,\nEnhua Wu, and Chang Xu. Training binary neural\nnetworks through learning with noisy supervision.\nIn International Conference on Machine Learning,\npages 4017\u20134026. PMLR, 2020.\n[84] Song Han, Huizi Mao, and William J Dally. Deep\ncompression: Compressing deep neural networks\nwith pruning, trained quantization and huffman\ncoding. arXiv preprint arXiv:1510.00149, 2015.\n[85] Matan Haroush, Itay Hubara, Elad Hoffer, and\nDaniel Soudry. The knowledge within: Methods\nfor data-free model compression. In Proceedings\nof the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 8494\u20138502, 2020.\n[86] Babak Hassibi and David G Stork. Second order\nderivatives for network pruning: Optimal brain\nsurgeon. Morgan Kaufmann, 1993.\n[87] Benjamin Hawks, Javier Duarte, Nicholas J Fraser,\nAlessandro Pappalardo, Nhan Tran, and Yaman\nUmuroglu. Ps and qs: Quantization-aware pruning\nfor ef\ufb01cient low latency neural network inference.\narXiv preprint arXiv:2102.11289, 2021.\n[88] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and\nJian Sun.\nDeep residual learning for image\nrecognition. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pages\n770\u2013778, 2016.\n[89] Xiangyu He and Jian Cheng. Learning compression\nfrom limited unlabeled data. In Proceedings of the\nEuropean Conference on Computer Vision (ECCV),\npages 752\u2013769, 2018.\n[90] Xiangyu He, Qinghao Hu, Peisong Wang, and Jian\nCheng. Generative zero-shot network quantization.\narXiv preprint arXiv:2101.08430, 2021.\n[91] Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-\nJia Li, and Song Han. Amc: Automl for model\ncompression and acceleration on mobile devices.\nIn Proceedings of the European Conference on\nComputer Vision (ECCV), pages 784\u2013800, 2018.\n[92] Zhezhi He and Deliang Fan.\nSimultaneously\noptimizing weight and quantizer of ternary neural\nnetwork using truncated gaussian approximation.\nIn Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages\n11438\u201311446, 2019.\n[93] Koen Helwegen, James Widdicombe, Lukas Geiger,\nZechun Liu, Kwang-Ting Cheng, and Roeland\nNusselder. Latent weights do not exist: Rethinking\nbinarized neural network optimization. Advances\nin neural information processing systems, 2019.\n[94] Dan Hendrycks and Kevin Gimpel.\nGaussian\nerror linear units (GELUs).\narXiv preprint\narXiv:1606.08415, 2016.\n[95] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531, 2015.\n[96] Torsten Hoe\ufb02er, Dan Alistarh, Tal Ben-Nun, Nikoli\nDryden, and Alexandra Peste. Sparsity in deep\nlearning: Pruning and growth for ef\ufb01cient inference\nand training in neural networks. arXiv preprint\narXiv:2102.00554, 2021.\n[97] Mark Horowitz. 1.1 computing\u2019s energy problem\n(and what we can do about it). In 2014 IEEE In-\nternational Solid-State Circuits Conference Digest\nof Technical Papers (ISSCC), pages 10\u201314. IEEE,\n2014.\n[98] Lu Hou and James T Kwok. Loss-aware weight\n23\nquantization of deep networks.\narXiv preprint\narXiv:1802.08635, 2018.\n[99] Lu Hou, Quanming Yao, and James T Kwok.\nLoss-aware binarization of deep networks. arXiv\npreprint arXiv:1611.01600, 2016.\n[100] Andrew Howard, Mark Sandler, Grace Chu, Liang-\nChieh Chen, Bo Chen, Mingxing Tan, Weijun\nWang, Yukun Zhu, Ruoming Pang, Vijay Va-\nsudevan, et al. Searching for MobilenetV3. In\nProceedings of the IEEE International Conference\non Computer Vision, pages 1314\u20131324, 2019.\n[101] Andrew G Howard, Menglong Zhu, Bo Chen,\nDmitry Kalenichenko, Weijun Wang, Tobias\nWeyand, Marco Andreetto, and Hartwig Adam.\nMobileNets: Ef\ufb01cient convolutional neural net-\nworks for mobile vision applications.\narXiv\npreprint arXiv:1704.04861, 2017.\n[102] Peng Hu, Xi Peng, Hongyuan Zhu, Mohamed\nM Sabry Aly, and Jie Lin.\nOpq: Compress-\ning deep neural networks with one-shot pruning-\nquantization. 2021.\n[103] Qinghao Hu, Peisong Wang, and Jian Cheng.\nFrom hashing to cnns: Training binary weight\nnetworks via hashing. In Proceedings of the AAAI\nConference on Arti\ufb01cial Intelligence, volume 32,\n2018.\n[104] Gao Huang, Zhuang Liu, Laurens Van Der Maaten,\nand Kilian Q Weinberger.\nDensely connected\nconvolutional networks.\nIn Proceedings of the\nIEEE conference on computer vision and pattern\nrecognition, pages 4700\u20134708, 2017.\n[105] Qijing Huang, Dequan Wang, Zhen Dong, Yizhao\nGao, Yaohui Cai, Tian Li, Bichen Wu, Kurt\nKeutzer, and John Wawrzynek. Codenet: Ef\ufb01cient\ndeployment of input-adaptive object detection\non embedded fpgas. In The 2021 ACM/SIGDA\nInternational Symposium on Field-Programmable\nGate Arrays, pages 206\u2013216, 2021.\n[106] Zehao Huang and Naiyan Wang.\nData-driven\nsparse structure selection for deep neural networks.\nIn Proceedings of the European conference on\ncomputer vision (ECCV), pages 304\u2013320, 2018.\n[107] Itay Hubara, Matthieu Courbariaux, Daniel Soudry,\nRan El-Yaniv, and Yoshua Bengio.\nBinarized\nneural networks. In Advances in neural information\nprocessing systems, pages 4107\u20134115, 2016.\n[108] Itay Hubara, Yury Nahshan, Yair Hanani, Ron\nBanner, and Daniel Soudry.\nImproving post\ntraining neural quantization: Layer-wise calibra-\ntion and integer programming.\narXiv preprint\narXiv:2006.10518, 2020.\n[109] David A Huffman. A method for the construction\nof minimum-redundancy codes. Proceedings of\nthe IRE, 40(9):1098\u20131101, 1952.\n[110] Forrest N Iandola, Song Han, Matthew W\nMoskewicz, Khalid Ashraf, William J Dally, and\nKurt Keutzer. SqueezeNet: Alexnet-level accuracy\nwith 50x fewer parameters and< 0.5 mb model\nsize. arXiv preprint arXiv:1602.07360, 2016.\n[111] Yani Ioannou, Duncan Robertson, Roberto Cipolla,\nand Antonio Criminisi. Deep roots: Improving\ncnn ef\ufb01ciency with hierarchical \ufb01lter groups. In\nProceedings of the IEEE conference on computer\nvision and pattern recognition, pages 1231\u20131240,\n2017.\n[112] Sergey Ioffe and Christian Szegedy. Batch nor-\nmalization: Accelerating deep network training by\nreducing internal covariate shift. In International\nconference on machine learning, pages 448\u2013456.\nPMLR, 2015.\n[113] Benoit Jacob, Skirmantas Kligys, Bo Chen, Men-\nglong Zhu, Matthew Tang, Andrew Howard,\nHartwig Adam, and Dmitry Kalenichenko. Quanti-\nzation and training of neural networks for ef\ufb01cient\ninteger-arithmetic-only inference. In Proceedings\nof the IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2018.\n[114] Animesh Jain, Shoubhik Bhattacharya, Masahiro\nMasuda, Vin Sharma, and Yida Wang. Ef\ufb01cient ex-\necution of quantized deep learning models: A com-\npiler approach. arXiv preprint arXiv:2006.10226,\n2020.\n[115] Shubham Jain, Swagath Venkataramani, Vijay-\nalakshmi Srinivasan, Jungwook Choi, Kailash\nGopalakrishnan, and Leland Chang.\nBiscaled-\ndnn: Quantizing long-tailed datastructures with two\nscale factors for deep neural networks. In 2019\n56th ACM/IEEE Design Automation Conference\n(DAC), pages 1\u20136. IEEE, 2019.\n[116] Eric Jang, Shixiang Gu, and Ben Poole. Categorical\nreparameterization with gumbel-softmax. arXiv\npreprint arXiv:1611.01144, 2016.\n[117] Herve Jegou, Matthijs Douze, and Cordelia Schmid.\nProduct quantization for nearest neighbor search.\nIEEE transactions on pattern analysis and machine\nintelligence, 33(1):117\u2013128, 2010.\n[118] Yongkweon Jeon, Baeseong Park, Se Jung Kwon,\nByeongwook Kim, Jeongin Yun, and Dongsoo Lee.\nBiqgemm: matrix multiplication with lookup table\nfor binary-coding-based quantized dnns.\narXiv\n24\npreprint arXiv:2005.09904, 2020.\n[119] Tianchu Ji, Shraddhan Jain, Michael Ferdman,\nPeter Milder, H Andrew Schwartz, and Niranjan\nBalasubramanian. On the distribution, sparsity, and\ninference-time quantization of attention values in\ntransformers. arXiv preprint arXiv:2106.01335,\n2021.\n[120] Kai Jia and Martin Rinard. Ef\ufb01cient exact veri\ufb01-\ncation of binarized neural networks. Advances in\nneural information processing systems, 2020.\n[121] Jing Jin, Cai Liang, Tiancheng Wu, Liqin Zou,\nand Zhiliang Gan. Kdlsq-bert: A quantized bert\ncombining knowledge distillation with learned step\nsize quantization. arXiv preprint arXiv:2101.05938,\n2021.\n[122] Qing Jin, Linjie Yang, and Zhenyu Liao. Adabits:\nNeural network quantization with adaptive bit-\nwidths. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition,\npages 2146\u20132156, 2020.\n[123] Jeff Johnson. Rethinking \ufb02oating point for deep\nlearning. arXiv preprint arXiv:1811.01721, 2018.\n[124] Felix Juefei-Xu, Vishnu Naresh Boddeti, and Mar-\nios Savvides. Local binary convolutional neural\nnetworks. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pages\n19\u201328, 2017.\n[125] Sangil Jung, Changyong Son, Seohyung Lee, Jin-\nwoo Son, Jae-Joon Han, Youngjun Kwak, Sung Ju\nHwang, and Changkyu Choi. Learning to quantize\ndeep networks by optimizing quantization intervals\nwith task loss. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recog-\nnition, pages 4350\u20134359, 2019.\n[126] Prad Kadambi, Karthikeyan Natesan Ramamurthy,\nand Visar Berisha. Comparing \ufb01sher information\nregularization with distillation for dnn quantization.\nAdvances in neural information processing systems,\n2020.\n[127] PP Kanjilal, PK Dey, and DN Banerjee. Reduced-\nsize neural networks through singular value decom-\nposition and subset selection. Electronics Letters,\n29(17):1516\u20131518, 1993.\n[128] Mel Win Khaw, Luminita Stevens, and Michael\nWoodford.\nDiscrete adjustment to a changing\nenvironment: Experimental evidence. Journal of\nMonetary Economics, 91:88\u2013103, 2017.\n[129] Hyungjun Kim, Kyungsu Kim, Jinseok Kim, and\nJae-Joon Kim.\nBinaryduo: Reducing gradient\nmismatch in binary activation network by coupling\nbinary activations. International Conference on\nLearning Representations, 2020.\n[130] Jangho Kim, KiYoon Yoo, and Nojun Kwak.\nPosition-based scaled gradient for model quan-\ntization and sparse training. Advances in neural\ninformation processing systems, 2020.\n[131] Minje Kim and Paris Smaragdis. Bitwise neural\nnetworks. arXiv preprint arXiv:1601.06071, 2016.\n[132] Sehoon\nKim,\nAmir\nGholami,\nZhewei\nYao,\nMichael W Mahoney, and Kurt Keutzer. I-bert:\nInteger-only bert quantization.\narXiv preprint\narXiv:2101.01321, 2021.\n[133] Raghuraman Krishnamoorthi.\nQuantizing deep\nconvolutional networks for ef\ufb01cient inference: A\nwhitepaper.\narXiv preprint arXiv:1806.08342,\n2018.\n[134] Andrey Kuzmin, Markus Nagel, Saurabh Pitre,\nSandeep Pendyam, Tijmen Blankevoort, and Max\nWelling. Taxonomy and evaluation of structured\ncompression of convolutional neural networks.\narXiv preprint arXiv:1912.09802, 2019.\n[135] Se Jung Kwon, Dongsoo Lee, Byeongwook Kim,\nParichay Kapoor, Baeseong Park, and Gu-Yeon\nWei. Structured compression by weight encryption\nfor unstructured pruning and quantization.\nIn\nProceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages\n1909\u20131918, 2020.\n[136] Liangzhen Lai, Naveen Suda, and Vikas Chan-\ndra.\nCMSIS-NN: Ef\ufb01cient neural network ker-\nnels for arm cortex-m cpus.\narXiv preprint\narXiv:1801.06601, 2018.\n[137] Hamed F Langroudi, Zachariah Carmichael, David\nPastuch, and Dhireesha Kudithipudi.\nCheetah:\nMixed low-precision hardware & software co-\ndesign framework for dnns on the edge. arXiv\npreprint arXiv:1908.02386, 2019.\n[138] Kenneth W Latimer, Jacob L Yates, Miriam LR\nMeister, Alexander C Huk, and Jonathan W Pillow.\nSingle-trial spike trains in parietal cortex reveal\ndiscrete steps during decision-making. Science,\n349(6244):184\u2013187, 2015.\n[139] Yann LeCun, John S Denker, and Sara A Solla.\nOptimal brain damage.\nIn Advances in neural\ninformation processing systems, pages 598\u2013605,\n1990.\n[140] Dong-Hyun Lee, Saizheng Zhang, Asja Fischer,\nand Yoshua Bengio. Difference target propagation.\nIn Joint european conference on machine learning\nand knowledge discovery in databases, pages 498\u2013\n25\n515. Springer, 2015.\n[141] Dongsoo Lee, Se Jung Kwon, Byeongwook Kim,\nYongkweon Jeon, Baeseong Park, and Jeongin Yun.\nFlexor: Trainable fractional quantization. Advances\nin neural information processing systems, 2020.\n[142] Jun Haeng Lee, Sangwon Ha, Saerom Choi, Won-\nJo Lee, and Seungwon Lee.\nQuantization for\nrapid deployment of deep neural networks. arXiv\npreprint arXiv:1810.05488, 2018.\n[143] Namhoon Lee, Thalaiyasingam Ajanthan, and\nPhilip HS Torr. Snip: Single-shot network pruning\nbased on connection sensitivity. arXiv preprint\narXiv:1810.02340, 2018.\n[144] Cong Leng, Zesheng Dou, Hao Li, Shenghuo Zhu,\nand Rong Jin. Extremely low bit neural network:\nSqueeze the last bit out with admm. In Proceedings\nof the AAAI Conference on Arti\ufb01cial Intelligence,\nvolume 32, 2018.\n[145] Fengfu Li, Bo Zhang, and Bin Liu. Ternary weight\nnetworks. arXiv preprint arXiv:1605.04711, 2016.\n[146] Rundong Li, Yan Wang, Feng Liang, Hongwei\nQin, Junjie Yan, and Rui Fan. Fully quantized\nnetwork for object detection. In Proceedings of\nthe IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2019.\n[147] Yuhang Li, Xin Dong, and Wei Wang.\nAddi-\ntive powers-of-two quantization: An ef\ufb01cient non-\nuniform discretization for neural networks. arXiv\npreprint arXiv:1909.13144, 2019.\n[148] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang,\nPeng Hu, Qi Zhang, Fengwei Yu, Wei Wang, and\nShi Gu. Brecq: Pushing the limit of post-training\nquantization by block reconstruction. International\nConference on Learning Representations, 2021.\n[149] Yuhang Li, Ruihao Gong, Fengwei Yu, Xin Dong,\nand Xianglong Liu. Dms: Differentiable dimension\nsearch for binary neural networks. International\nConference on Learning Representations, 2020.\n[150] Yuncheng Li, Jianchao Yang, Yale Song, Lian-\ngliang Cao, Jiebo Luo, and Li-Jia Li. Learning\nfrom noisy labels with distillation. In Proceedings\nof the IEEE International Conference on Computer\nVision, pages 1910\u20131918, 2017.\n[151] Zefan Li, Bingbing Ni, Wenjun Zhang, Xiaokang\nYang, and Wen Gao.\nPerformance guaranteed\nnetwork acceleration via high-order residual quan-\ntization. In Proceedings of the IEEE international\nconference on computer vision, pages 2584\u20132592,\n2017.\n[152] Tailin Liang, John Glossner, Lei Wang, and Shaobo\nShi.\nPruning and quantization for deep neural\nnetwork acceleration: A survey. arXiv preprint\narXiv:2101.09671, 2021.\n[153] Zhenyu Liao, Romain Couillet, and Michael W\nMahoney.\nSparse quantized spectral clustering.\nInternational Conference on Learning Representa-\ntions, 2021.\n[154] Darryl Lin, Sachin Talathi, and Sreekanth Anna-\npureddy. Fixed point quantization of deep con-\nvolutional networks. In International conference\non machine learning, pages 2849\u20132858. PMLR,\n2016.\n[155] Mingbao Lin, Rongrong Ji, Zihan Xu, Baochang\nZhang, Yan Wang, Yongjian Wu, Feiyue Huang,\nand Chia-Wen Lin. Rotated binary neural network.\nAdvances in neural information processing systems,\n2020.\n[156] Shaohui Lin, Rongrong Ji, Yuchao Li, Yongjian\nWu, Feiyue Huang, and Baochang Zhang. Acceler-\nating convolutional networks via global & dynamic\n\ufb01lter pruning. In IJCAI, pages 2425\u20132432, 2018.\n[157] Wuwei\nLin.\nAutomating\noptimization\nof\nquantized\ndeep\nlearning\nmodels\non\ncuda:\nhttps://tvm.apache.org/2019/04/29/opt-cuda-\nquantized, 2019.\n[158] Xiaofan Lin, Cong Zhao, and Wei Pan. Towards ac-\ncurate binary convolutional neural network. arXiv\npreprint arXiv:1711.11294, 2017.\n[159] Zhouhan Lin, Matthieu Courbariaux, Roland\nMemisevic, and Yoshua Bengio.\nNeural net-\nworks with few multiplications. arXiv preprint\narXiv:1510.03009, 2015.\n[160] Chunlei Liu, Wenrui Ding, Xin Xia, Baochang\nZhang, Jiaxin Gu, Jianzhuang Liu, Rongrong Ji,\nand David Doermann.\nCirculant binary convo-\nlutional networks: Enhancing the performance\nof 1-bit dcnns with circulant back propagation.\nIn Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages\n2691\u20132699, 2019.\n[161] Hanxiao Liu, Karen Simonyan, and Yiming Yang.\nDarts: Differentiable architecture search. arXiv\npreprint arXiv:1806.09055, 2018.\n[162] Hongyang Liu, Sara Elkerdawy, Nilanjan Ray, and\nMostafa Elhoushi. Layer importance estimation\nwith imprinting for neural network quantization.\nIn Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages\n2408\u20132417, 2021.\n[163] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,\n26\nMandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov.\nRoBERTa: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692, 2019.\n[164] Zechun Liu, Baoyuan Wu, Wenhan Luo, Xin Yang,\nWei Liu, and Kwang-Ting Cheng.\nBi-real net:\nEnhancing the performance of 1-bit cnns with\nimproved representational capability and advanced\ntraining algorithm. In Proceedings of the European\nconference on computer vision (ECCV), pages 722\u2013\n737, 2018.\n[165] Zhi-Gang Liu and Matthew Mattina. Learning low-\nprecision neural networks without straight-through\nestimator (STE). arXiv preprint arXiv:1903.01061,\n2019.\n[166] Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. Thinet:\nA \ufb01lter level pruning method for deep neural\nnetwork compression. In Proceedings of the IEEE\ninternational conference on computer vision, pages\n5058\u20135066, 2017.\n[167] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and\nJian Sun. Shuf\ufb02enet V2: Practical guidelines for\nef\ufb01cient cnn architecture design. In Proceedings\nof the European Conference on Computer Vision\n(ECCV), pages 116\u2013131, 2018.\n[168] Franck Mamalet and Christophe Garcia. Simpli-\nfying convnets for fast learning. In International\nConference on Arti\ufb01cial Neural Networks, pages\n58\u201365. Springer, 2012.\n[169] Brais Martinez, Jing Yang, Adrian Bulat, and\nGeorgios Tzimiropoulos. Training binary neural\nnetworks with real-to-binary convolutions. arXiv\npreprint arXiv:2003.11535, 2020.\n[170] Julieta Martinez, Shobhit Zakhmi, Holger H Hoos,\nand James J Little. Lsq++: Lower running time\nand higher recall in multi-codebook quantization.\nIn Proceedings of the European Conference on\nComputer Vision (ECCV), pages 491\u2013506, 2018.\n[171] Warren S McCulloch and Walter Pitts. A logical\ncalculus of the ideas immanent in nervous activity.\nThe bulletin of mathematical biophysics, 5(4):115\u2013\n133, 1943.\n[172] Jeffrey L McKinstry, Steven K Esser, Rathinaku-\nmar Appuswamy, Deepika Bablani, John V Arthur,\nIzzet B Yildiz, and Dharmendra S Modha. Discov-\nering low-precision networks close to full-precision\nnetworks for ef\ufb01cient embedded inference. arXiv\npreprint arXiv:1809.04191, 2018.\n[173] Naveen Mellempudi, Sudarshan Srinivasan, Di-\npankar Das, and Bharat Kaul. Mixed precision\ntraining with 8-bit \ufb02oating point. arXiv preprint\narXiv:1905.12334, 2019.\n[174] Eldad Meller, Alexander Finkelstein, Uri Almog,\nand Mark Grobman. Same, same but different: Re-\ncovering neural network quantization error through\nweight factorization. In International Conference\non Machine Learning, pages 4486\u20134495. PMLR,\n2019.\n[175] Paulius Micikevicius, Sharan Narang, Jonah Alben,\nGregory Diamos, Erich Elsen, David Garcia, Boris\nGinsburg, Michael Houston, Oleksii Kuchaiev,\nGanesh Venkatesh, et al. Mixed precision training.\narXiv preprint arXiv:1710.03740, 2017.\n[176] Szymon Migacz.\nNvidia 8-bit inference with\ntensorrt. GPU Technology Conference, 2017.\n[177] Asit Mishra and Debbie Marr. Apprentice: Us-\ning knowledge distillation techniques to improve\nlow-precision network accuracy. arXiv preprint\narXiv:1711.05852, 2017.\n[178] Asit Mishra, Eriko Nurvitadhi, Jeffrey J Cook,\nand Debbie Marr. Wrpn: Wide reduced-precision\nnetworks. arXiv preprint arXiv:1709.01134, 2017.\n[179] Daisuke Miyashita, Edward H Lee, and Boris\nMurmann. Convolutional neural networks using\nlogarithmic data representation.\narXiv preprint\narXiv:1603.01025, 2016.\n[180] Lopamudra Mukherjee, Sathya N Ravi, Jiming\nPeng, and Vikas Singh. A biresolution spectral\nframework for product quantization. In Proceed-\nings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 3329\u20133338, 2018.\n[181] Markus Nagel, Rana Ali Amjad, Mart Van Baalen,\nChristos Louizos, and Tijmen Blankevoort. Up or\ndown? adaptive rounding for post-training quanti-\nzation. In International Conference on Machine\nLearning, pages 7197\u20137206. PMLR, 2020.\n[182] Markus\nNagel,\nMart\nvan\nBaalen,\nTijmen\nBlankevoort, and Max Welling. Data-free quanti-\nzation through weight equalization and bias correc-\ntion. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 1325\u20131334,\n2019.\n[183] Markus Nagel, Marios Fournarakis, Rana Ali Am-\njad, Yelysei Bondarenko, Mart van Baalen, and Tij-\nmen Blankevoort. A white paper on neural network\nquantization.\narXiv preprint arXiv:2106.08295,\n2021.\n[184] Maxim Naumov, Utku Diril, Jongsoo Park, Ben-\njamin Ray, Jedrzej Jablonski, and Andrew Tul-\nloch. On periodic functions as regularizers for\n27\nquantization of neural networks. arXiv preprint\narXiv:1811.09862, 2018.\n[185] Maxim\nNaumov,\nDheevatsa\nMudigere,\nHao-\nJun Michael Shi, Jianyu Huang, Narayanan Sun-\ndaraman, Jongsoo Park, Xiaodong Wang, Udit\nGupta, Carole-Jean Wu, Alisson G Azzolini, et al.\nDeep learning recommendation model for person-\nalization and recommendation systems.\narXiv\npreprint arXiv:1906.00091, 2019.\n[186] Renkun Ni, Hong-min Chu, Oscar Casta\u00f1eda,\nPing-yeh Chiang, Christoph Studer, and Tom\nGoldstein. Wrapnet: Neural net inference with\nultra-low-resolution arithmetic.\narXiv preprint\narXiv:2007.13242, 2020.\n[187] Lin Ning, Guoyang Chen, Weifeng Zhang, and\nXipeng Shen. Simple augmentation goes a long\nway: {ADRL} for {dnn} quantization. In Inter-\nnational Conference on Learning Representations,\n2021.\n[188] BM Oliver, JR Pierce, and Claude E Shannon.\nThe philosophy of pcm. Proceedings of the IRE,\n36(11):1324\u20131331, 1948.\n[189] Eunhyeok Park, Junwhan Ahn, and Sungjoo Yoo.\nWeighted-entropy-based quantization for deep neu-\nral networks. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition,\npages 5456\u20135464, 2017.\n[190] Eunhyeok Park, Sungjoo Yoo, and Peter Vajda.\nValue-aware quantization for training and inference\nof neural networks. In Proceedings of the European\nConference on Computer Vision (ECCV), pages\n580\u2013595, 2018.\n[191] Sejun Park, Jaeho Lee, Sangwoo Mo, and Jin-\nwoo Shin. Lookahead: a far-sighted alternative\nof magnitude-based pruning.\narXiv preprint\narXiv:2002.04809, 2020.\n[192] Wonpyo Park, Dongju Kim, Yan Lu, and Minsu\nCho. Relational knowledge distillation. In Proceed-\nings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 3967\u20133976,\n2019.\n[193] Peng Peng, Mingyu You, Weisheng Xu, and\nJiaxin Li.\nFully integer-based quantization for\nmobile convolutional neural network inference.\nNeurocomputing, 432:194\u2013205, 2021.\n[194] Hieu Pham, Melody Guan, Barret Zoph, Quoc\nLe, and Jeff Dean. Ef\ufb01cient neural architecture\nsearch via parameters sharing. In International\nConference on Machine Learning, pages 4095\u2013\n4104. PMLR, 2018.\n[195] Antonio Polino, Razvan Pascanu, and Dan Alistarh.\nModel compression via distillation and quantiza-\ntion. arXiv preprint arXiv:1802.05668, 2018.\n[196] Haotong Qin, Zhongang Cai, Mingyuan Zhang,\nYifu Ding, Haiyu Zhao, Shuai Yi, Xianglong Liu,\nand Hao Su. Bipointnet: Binary neural network for\npoint clouds. International Conference on Learning\nRepresentations, 2021.\n[197] Haotong Qin, Ruihao Gong, Xianglong Liu, Xiao\nBai, Jingkuan Song, and Nicu Sebe.\nBinary\nneural networks: A survey. Pattern Recognition,\n105:107281, 2020.\n[198] Haotong Qin, Ruihao Gong, Xianglong Liu,\nMingzhu Shen, Ziran Wei, Fengwei Yu, and\nJingkuan Song. Forward and backward information\nretention for accurate binary neural networks.\nIn Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages\n2250\u20132259, 2020.\n[199] Zhongnan Qu, Zimu Zhou, Yun Cheng, and Lothar\nThiele.\nAdaptive loss-aware quantization for\nmulti-bit networks. In IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR),\nJune 2020.\n[200] Alec Radford, Karthik Narasimhan, Tim Salimans,\nand Ilya Sutskever. Improving language under-\nstanding by generative pre-training, 2018.\n[201] Alec Radford, Jeffrey Wu, Rewon Child, David\nLuan, Dario Amodei, and Ilya Sutskever. Lan-\nguage models are unsupervised multitask learners.\nOpenAI blog, 1(8):9, 2019.\n[202] Prajit Ramachandran, Barret Zoph, and Quoc V Le.\nSearching for activation functions. arXiv preprint\narXiv:1710.05941, 2017.\n[203] Prajit Ramachandran, Barret Zoph, and Quoc V\nLe. Swish: a self-gated activation function. arXiv\npreprint arXiv:1710.05941, 7:1, 2017.\n[204] Mohammad Rastegari, Vicente Ordonez, Joseph\nRedmon, and Ali Farhadi.\nXnor-net: Imagenet\nclassi\ufb01cation using binary convolutional neural\nnetworks. In European conference on computer\nvision, pages 525\u2013542. Springer, 2016.\n[205] Ryan Razani, Gregoire Morin, Eyyub Sari, and\nVahid Partovi Nia. Adaptive binary-ternary quanti-\nzation. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition,\npages 4613\u20134618, 2021.\n[206] Bernhard Riemann.\nUeber die Darstellbarkeit\neiner Function durch eine trigonometrische Reihe,\nvolume 13. Dieterich, 1867.\n28\n[207] Adriana Romero, Nicolas Ballas, Samira Ebrahimi\nKahou, Antoine Chassang, Carlo Gatta, and Yoshua\nBengio. Fitnets: Hints for thin deep nets. arXiv\npreprint arXiv:1412.6550, 2014.\n[208] Kenneth Rose, Eitan Gurewitz, and Geoffrey Fox.\nA deterministic annealing approach to clustering.\nPattern Recognition Letters, 11(9):589\u2013594, 1990.\n[209] Frank Rosenblatt. The perceptron, a perceiving\nand recognizing automaton Project Para. Cornell\nAeronautical Laboratory, 1957.\n[210] Frank Rosenblatt. Principles of neurodynamics.\nperceptrons and the theory of brain mechanisms.\nTechnical report, Cornell Aeronautical Lab Inc\nBuffalo NY, 1961.\n[211] Manuele Rusci, Marco Fariselli, Alessandro Capo-\ntondi, and Luca Benini. Leveraging automated\nmixed-low-precision quantization for tiny edge\nmicrocontrollers. In IoT Streams for Data-Driven\nPredictive Maintenance and IoT, Edge, and Mobile\nfor Embedded Machine Learning, pages 296\u2013308.\nSpringer, 2020.\n[212] Tara N Sainath, Brian Kingsbury, Vikas Sindhwani,\nEbru Arisoy, and Bhuvana Ramabhadran. Low-\nrank matrix factorization for deep neural network\ntraining with high-dimensional output targets. In\n2013 IEEE international conference on acoustics,\nspeech and signal processing, pages 6655\u20136659.\nIEEE, 2013.\n[213] Dave Salvator, Hao Wu, Milind Kulkarni, and\nNiall Emmart.\nInt4 precision for ai infer-\nence: https://developer.nvidia.com/blog/int4-for-ai-\ninference/, 2019.\n[214] Mark Sandler, Andrew Howard, Menglong Zhu,\nAndrey Zhmoginov, and Liang-Chieh Chen. Mo-\nbilenetV2: Inverted residuals and linear bottlenecks.\nIn Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, pages 4510\u2013\n4520, 2018.\n[215] Claude E Shannon.\nA mathematical theory of\ncommunication. The Bell system technical journal,\n27(3):379\u2013423, 1948.\n[216] Claude E Shannon. Coding theorems for a discrete\nsource with a \ufb01delity criterion. IRE Nat. Conv. Rec,\n4(142-163):1, 1959.\n[217] Alexander Shekhovtsov, Viktor Yanush, and Boris\nFlach. Path sample-analytic gradient estimators\nfor stochastic binary networks. Advances in neural\ninformation processing systems, 2020.\n[218] Mingzhu Shen, Xianglong Liu, Ruihao Gong,\nand Kai Han. Balanced binary neural networks\nwith gated residual. In ICASSP 2020-2020 IEEE\nInternational Conference on Acoustics, Speech and\nSignal Processing (ICASSP), pages 4197\u20134201.\nIEEE, 2020.\n[219] Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma,\nZhewei Yao, Amir Gholami, Michael W Mahoney,\nand Kurt Keutzer. Q-BERT: Hessian based ultra\nlow precision quantization of bert. In AAAI, pages\n8815\u20138821, 2020.\n[220] William Fleetwood Sheppard. On the calculation\nof the most probable values of frequency-constants,\nfor data arranged according to equidistant division\nof a scale. Proceedings of the London Mathemati-\ncal Society, 1(1):353\u2013380, 1897.\n[221] Sungho Shin, Kyuyeon Hwang, and Wonyong\nSung. Fixed-point performance analysis of recur-\nrent neural networks. In 2016 IEEE International\nConference on Acoustics, Speech and Signal Pro-\ncessing (ICASSP), pages 976\u2013980. IEEE, 2016.\n[222] Moran Shkolnik, Brian Chmiel, Ron Banner, Gil\nShomron, Yuri Nahshan, Alex Bronstein, and\nUri Weiser. Robust quantization: One model to\nrule them all.\nAdvances in neural information\nprocessing systems, 2020.\n[223] Gil Shomron, Freddy Gabbay, Samer Kurzum,\nand Uri Weiser.\nPost-training sparsity-aware\nquantization.\narXiv preprint arXiv:2105.11010,\n2021.\n[224] K. Simonyan and A. Zisserman.\nVery deep\nconvolutional networks for large-scale image recog-\nnition. In International Conference on Learning\nRepresentations, 2015.\n[225] S. M. Stigler.\nThe History of Statistics: The\nMeasurement of Uncertainty before 1900. Harvard\nUniversity Press, Cambridge, 1986.\n[226] Pierre Stock, Angela Fan, Benjamin Graham,\nEdouard Grave, R\u00e9mi Gribonval, Herve Jegou, and\nArmand Joulin. Training with quantization noise\nfor extreme model compression. In International\nConference on Learning Representations, 2021.\n[227] Pierre Stock, Armand Joulin, R\u00e9mi Gribonval,\nBenjamin Graham, and Herv\u00e9 J\u00e9gou. And the bit\ngoes down: Revisiting the quantization of neural\nnetworks. arXiv preprint arXiv:1907.05686, 2019.\n[228] John Z Sun, Grace I Wang, Vivek K Goyal, and\nLav R Varshney.\nA framework for bayesian\noptimality of psychophysical laws.\nJournal of\nMathematical Psychology, 56(6):495\u2013501, 2012.\n[229] Wonyong Sung, Sungho Shin, and Kyuyeon\nHwang. Resiliency of deep neural networks under\n29\nquantization.\narXiv preprint arXiv:1511.06488,\n2015.\n[230] Christian Szegedy, Vincent Vanhoucke, Sergey\nIoffe, Jon Shlens, and Zbigniew Wojna. Rethinking\nthe Inception architecture for computer vision. In\nProceedings of the IEEE conference on computer\nvision and pattern recognition, pages 2818\u20132826,\n2016.\n[231] Shyam A Tailor, Javier Fernandez-Marques, and\nNicholas D Lane.\nDegree-quant: Quantization-\naware training for graph neural networks. Inter-\nnational Conference on Learning Representations,\n2021.\n[232] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay\nVasudevan, Mark Sandler, Andrew Howard, and\nQuoc V Le.\nMnasnet: Platform-aware neural\narchitecture search for mobile. In Proceedings\nof the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 2820\u20132828, 2019.\n[233] Mingxing Tan and Quoc V Le.\nEf\ufb01cientNet:\nRethinking model scaling for convolutional neural\nnetworks. arXiv preprint arXiv:1905.11946, 2019.\n[234] Wei Tang, Gang Hua, and Liang Wang. How to\ntrain a compact binary neural network with high\naccuracy? In Proceedings of the AAAI Conference\non Arti\ufb01cial Intelligence, volume 31, 2017.\n[235] Antti Tarvainen and Harri Valpola. Mean teachers\nare better role models: Weight-averaged consis-\ntency targets improve semi-supervised deep learn-\ning results. arXiv preprint arXiv:1703.01780, 2017.\n[236] James Tee and Desmond P Taylor. Is information\nin the brain represented in continuous or discrete\nform? IEEE Transactions on Molecular, Biological\nand Multi-Scale Communications, 6(3):199\u2013209,\n2020.\n[237] L.N. Trefethen and D. Bau III. Numerical Linear\nAlgebra. SIAM, Philadelphia, 1997.\n[238] Frederick Tung and Greg Mori. Clip-q: Deep net-\nwork compression learning by in-parallel pruning-\nquantization. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition,\npages 7873\u20137882, 2018.\n[239] Mart van Baalen, Christos Louizos, Markus Nagel,\nRana Ali Amjad, Ying Wang, Tijmen Blankevoort,\nand Max Welling. Bayesian bits: Unifying quanti-\nzation and pruning. Advances in neural information\nprocessing systems, 2020.\n[240] Ru\ufb01n VanRullen and Christof Koch. Is percep-\ntion discrete or continuous? Trends in cognitive\nsciences, 7(5):207\u2013213, 2003.\n[241] Lav R Varshney, Per Jesper Sj\u00f6str\u00f6m, and Dmitri B\nChklovskii. Optimal information storage in noisy\nsynapses under resource constraints.\nNeuron,\n52(3):409\u2013423, 2006.\n[242] Lav R Varshney and Kush R Varshney. Decision\nmaking with quantized priors leads to discrimina-\ntion. Proceedings of the IEEE, 105(2):241\u2013255,\n2016.\n[243] Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is\nall you need. In Advances in neural information\nprocessing systems, pages 5998\u20136008, 2017.\n[244] Diwen Wan, Fumin Shen, Li Liu, Fan Zhu, Jie\nQin, Ling Shao, and Heng Tao Shen. Tbn: Con-\nvolutional neural network with ternary inputs and\nbinary weights. In Proceedings of the European\nConference on Computer Vision (ECCV), pages\n315\u2013332, 2018.\n[245] Dilin Wang, Meng Li, Chengyue Gong, and Vikas\nChandra. Attentivenas: Improving neural architec-\nture search via attentive sampling. arXiv preprint\narXiv:2011.09011, 2020.\n[246] Kuan Wang, Zhijian Liu, Yujun Lin, Ji Lin, and\nSong Han. HAQ: Hardware-aware automated quan-\ntization. In Proceedings of the IEEE conference\non computer vision and pattern recognition, 2019.\n[247] Naigang Wang, Jungwook Choi, Daniel Brand,\nChia-Yu Chen, and Kailash Gopalakrishnan. Train-\ning deep neural networks with 8-bit \ufb02oating point\nnumbers. Advances in neural information process-\ning systems, 2018.\n[248] Peisong Wang, Qinghao Hu, Yifan Zhang, Chunjie\nZhang, Yang Liu, and Jian Cheng.\nTwo-step\nquantization for low-bit neural networks.\nIn\nProceedings of the IEEE Conference on computer\nvision and pattern recognition, pages 4376\u20134384,\n2018.\n[249] Tianzhe Wang, Kuan Wang, Han Cai, Ji Lin,\nZhijian Liu, Hanrui Wang, Yujun Lin, and Song\nHan. Apq: Joint search for network architecture,\npruning and quantization policy. In Proceedings\nof the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 2078\u20132087, 2020.\n[250] Ying Wang, Yadong Lu, and Tijmen Blankevoort.\nDifferentiable joint pruning and quantization for\nhardware ef\ufb01ciency. In European Conference on\nComputer Vision, pages 259\u2013277. Springer, 2020.\n[251] Ziwei Wang, Jiwen Lu, Chenxin Tao, Jie Zhou,\nand Qi Tian. Learning channel-wise interactions\n30\nfor binary convolutional neural networks.\nIn\nProceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages\n568\u2013577, 2019.\n[252] Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yang-\nhan Wang, Fei Sun, Yiming Wu, Yuandong Tian,\nPeter Vajda, Yangqing Jia, and Kurt Keutzer.\nFBNet: Hardware-aware ef\ufb01cient convnet design\nvia differentiable neural architecture search. In\nProceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 10734\u2013\n10742, 2019.\n[253] Bichen Wu, Alvin Wan, Xiangyu Yue, Peter Jin,\nSicheng Zhao, Noah Golmant, Amir Gholaminejad,\nJoseph Gonzalez, and Kurt Keutzer. Shift: A zero\n\ufb02op, zero parameter alternative to spatial convolu-\ntions. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, pages\n9127\u20139135, 2018.\n[254] Bichen Wu, Yanghan Wang, Peizhao Zhang, Yuan-\ndong Tian, Peter Vajda, and Kurt Keutzer. Mixed\nprecision quantization of convnets via differen-\ntiable neural architecture search. arXiv preprint\narXiv:1812.00090, 2018.\n[255] Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail\nIsaev, and Paulius Micikevicius.\nInteger quan-\ntization for deep learning inference: Princi-\nples and empirical evaluation.\narXiv preprint\narXiv:2004.09602, 2020.\n[256] Jiaxiang Wu, Cong Leng, Yuhang Wang, Qinghao\nHu, and Jian Cheng.\nQuantized convolutional\nneural networks for mobile devices. In Proceedings\nof the IEEE Conference on Computer Vision and\nPattern Recognition, pages 4820\u20134828, 2016.\n[257] Xia Xiao, Zigeng Wang, and Sanguthevar Ra-\njasekaran. Autoprune: Automatic network pruning\nby regularizing auxiliary parameters. In Advances\nin Neural Information Processing Systems, pages\n13681\u201313691, 2019.\n[258] Chen Xu, Jianqiang Yao, Zhouchen Lin, Wenwu\nOu, Yuanbin Cao, Zhirong Wang, and Hong-\nbin Zha.\nAlternating multi-bit quantization\nfor recurrent neural networks.\narXiv preprint\narXiv:1802.00150, 2018.\n[259] Shoukai Xu, Haokun Li, Bohan Zhuang, Jing Liu,\nJiezhang Cao, Chuangrun Liang, and Mingkui Tan.\nGenerative low-bitwidth data free quantization. In\nEuropean Conference on Computer Vision, pages\n1\u201317. Springer, 2020.\n[260] Yinghao Xu, Xin Dong, Yudian Li, and Hao\nSu.\nA main/subsidiary network framework for\nsimplifying binary neural networks. In Proceedings\nof the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 7154\u20137162, 2019.\n[261] Zhe Xu and Ray CC Cheung. Accurate and com-\npact convolutional neural networks with trained\nbinarization.\narXiv preprint arXiv:1909.11366,\n2019.\n[262] Haichuan Yang, Shupeng Gui, Yuhao Zhu, and\nJi Liu. Automatic neural network compression by\nsparsity-quantization joint learning: A constrained\noptimization-based approach. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 2178\u20132188, 2020.\n[263] Huanrui Yang, Lin Duan, Yiran Chen, and Hai\nLi. Bsq: Exploring bit-level sparsity for mixed-\nprecision neural network quantization.\narXiv\npreprint arXiv:2102.10462, 2021.\n[264] Jiwei Yang, Xu Shen, Jun Xing, Xinmei Tian,\nHouqiang Li, Bing Deng, Jianqiang Huang, and\nXian-sheng Hua.\nQuantization networks.\nIn\nProceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages\n7308\u20137316, 2019.\n[265] Tien-Ju Yang, Andrew Howard, Bo Chen, Xiao\nZhang, Alec Go, Mark Sandler, Vivienne Sze,\nand Hartwig Adam.\nNetadapt: Platform-aware\nneural network adaptation for mobile applications.\nIn Proceedings of the European Conference on\nComputer Vision (ECCV), pages 285\u2013300, 2018.\n[266] Zhaohui Yang, Yunhe Wang, Kai Han, Chun-\njing Xu, Chao Xu, Dacheng Tao, and Chang\nXu. Searching for low-bit weights in quantized\nneural networks. Advances in neural information\nprocessing systems, 2020.\n[267] Zhewei Yao, Zhen Dong, Zhangcheng Zheng,\nAmir Gholami, Jiali Yu, Eric Tan, Leyuan Wang,\nQijing Huang, Yida Wang, Michael W Mahoney,\net al. Hawqv3: Dyadic neural network quantization.\narXiv preprint arXiv:2011.10680, 2020.\n[268] Jianming Ye, Shiliang Zhang, and Jingdong Wang.\nDistillation guided residual learning for binary\nconvolutional neural networks.\narXiv preprint\narXiv:2007.05223, 2020.\n[269] Junho Yim, Donggyu Joo, Jihoon Bae, and Junmo\nKim.\nA gift from knowledge distillation: Fast\noptimization, network minimization and transfer\nlearning. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition,\npages 4133\u20134141, 2017.\n31\n[270] Hongxu Yin, Pavlo Molchanov, Jose M Alvarez,\nZhizhong Li, Arun Mallya, Derek Hoiem, Niraj K\nJha, and Jan Kautz. Dreaming to distill: Data-\nfree knowledge transfer via deepinversion.\nIn\nProceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages\n8715\u20138724, 2020.\n[271] Penghang Yin, Jiancheng Lyu, Shuai Zhang, Stan-\nley Osher, Yingyong Qi, and Jack Xin.\nUn-\nderstanding straight-through estimator in training\nactivation quantized neural nets. arXiv preprint\narXiv:1903.05662, 2019.\n[272] Penghang Yin, Shuai Zhang, Jiancheng Lyu, Stan-\nley Osher, Yingyong Qi, and Jack Xin. Blended\ncoarse gradient descent for full quantization of\ndeep neural networks. Research in the Mathemati-\ncal Sciences, 6(1):14, 2019.\n[273] Shan You, Chang Xu, Chao Xu, and Dacheng Tao.\nLearning from multiple teacher networks. In Pro-\nceedings of the 23rd ACM SIGKDD International\nConference on Knowledge Discovery and Data\nMining, pages 1285\u20131294, 2017.\n[274] Ruichi Yu, Ang Li, Chun-Fu Chen, Jui-Hsin Lai,\nVlad I Morariu, Xintong Han, Mingfei Gao, Ching-\nYung Lin, and Larry S Davis.\nNisp: Pruning\nnetworks using neuron importance score propa-\ngation. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, pages\n9194\u20139203, 2018.\n[275] Shixing Yu, Zhewei Yao, Amir Gholami, Zhen\nDong, Michael W Mahoney, and Kurt Keutzer.\nHessian-aware pruning and optimal neural implant.\narXiv preprint arXiv:2101.08940, 2021.\n[276] Dongqing Zhang, Jiaolong Yang, Dongqiangzi Ye,\nand Gang Hua.\nLq-nets: Learned quantization\nfor highly accurate and compact deep neural\nnetworks. In European conference on computer\nvision (ECCV), 2018.\n[277] Linfeng Zhang, Jiebo Song, Anni Gao, Jingwei\nChen, Chenglong Bao, and Kaisheng Ma.\nBe\nyour own teacher: Improve the performance of\nconvolutional neural networks via self distillation.\nIn Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 3713\u20133722,\n2019.\n[278] Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang,\nXiao Chen, Xin Jiang, and Qun Liu. Ternarybert:\nDistillation-aware ultra-low bit bert. arXiv preprint\narXiv:2009.12812, 2020.\n[279] Chenglong Zhao, Bingbing Ni, Jian Zhang, Qiwei\nZhao, Wenjun Zhang, and Qi Tian. Variational\nconvolutional neural network pruning. In Proceed-\nings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 2780\u20132789, 2019.\n[280] Qibin Zhao, Masashi Sugiyama, Longhao Yuan,\nand Andrzej Cichocki. Learning ef\ufb01cient tensor\nrepresentations with ring-structured networks. In\nICASSP 2019-2019 IEEE International Confer-\nence on Acoustics, Speech and Signal Processing\n(ICASSP), pages 8608\u20138612. IEEE, 2019.\n[281] Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Christo-\npher De Sa, and Zhiru Zhang. Improving neural\nnetwork quantization without retraining using out-\nlier channel splitting.\nProceedings of Machine\nLearning Research, 2019.\n[282] Sijie Zhao, Tao Yue, and Xuemei Hu. Distribution-\naware adaptive multi-bit quantization. In Proceed-\nings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 9281\u20139290,\n2021.\n[283] Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, and\nYurong Chen. Incremental network quantization:\nTowards lossless cnns with low-precision weights.\narXiv preprint arXiv:1702.03044, 2017.\n[284] Aojun Zhou, Anbang Yao, Kuan Wang, and Yurong\nChen. Explicit loss-error-aware quantization for\nlow-bit deep neural networks. In Proceedings of the\nIEEE conference on computer vision and pattern\nrecognition, pages 9426\u20139435, 2018.\n[285] Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu\nZhou, He Wen, and Yuheng Zou.\nDorefa-net:\nTraining low bitwidth convolutional neural net-\nworks with low bitwidth gradients. arXiv preprint\narXiv:1606.06160, 2016.\n[286] Yiren Zhou, Seyed-Mohsen Moosavi-Dezfooli,\nNgai-Man Cheung, and Pascal Frossard. Adaptive\nquantization for deep neural network.\narXiv\npreprint arXiv:1712.01048, 2017.\n[287] Chenzhuo Zhu, Song Han, Huizi Mao, and\nWilliam J Dally.\nTrained ternary quantization.\narXiv preprint arXiv:1612.01064, 2016.\n[288] Shilin Zhu, Xin Dong, and Hao Su.\nBinary\nensemble neural network: More bits per network\nor more networks per bit? In Proceedings of the\nIEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 4923\u20134932, 2019.\n[289] Bohan Zhuang, Chunhua Shen, Mingkui Tan,\nLingqiao Liu, and Ian Reid. Towards effective\nlow-bitwidth convolutional neural networks. In\nProceedings of the IEEE conference on computer\n32\nvision and pattern recognition, pages 7920\u20137928,\n2018.\n[290] Bohan Zhuang, Chunhua Shen, Mingkui Tan,\nLingqiao Liu, and Ian Reid.\nStructured binary\nneural networks for accurate image classi\ufb01cation\nand semantic segmentation. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 413\u2013422, 2019.\n[291] Barret Zoph and Quoc V Le. Neural architecture\nsearch with reinforcement learning. arXiv preprint\narXiv:1611.01578, 2016.\n33\n",
        "context": "one can minimize accuracy degradation and still bene\ufb01t\nfrom reduced memory footprint and faster speed up with\nlow precision quantization. Recent work [267] has also\nshown that this approach is hardware-ef\ufb01cient as mixed-\n4x to 8x are often realized in practice in these applications.\nThus, it is not surprising that quantization has emerged\nrecently as an important and very active sub-area of\nresearch in the ef\ufb01cient implementation of computations\nduring inference. This approach does not add any com-\nputational overhead, but it typically results in lower\naccuracy as compared to dynamic quantization. One\npopular method for the pre-calculation is to run a\n6"
    },
    {
        "id": 5,
        "title": "Hawq: Hessian aware quantization of neural networks with mixed-precision",
        "author": [
            "Z. Dong",
            "Z. Yao",
            "A. Gholami",
            "M. W. Mahoney",
            "K. Keutzer"
        ],
        "year": "2019",
        "doi": null,
        "in_text_citation": "[5]",
        "sentence": "Another optimization strategy involves quantization [4]\u2013[6], which aims to minimize the memory footprint and computational requirements without significantly impacting accuracy.",
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "context": null
    },
    {
        "id": 6,
        "title": "Noisyquant: Noisy bias-enhanced post-training activation quantization for vision transformers",
        "author": [
            "Y. Liu",
            "H. Yang",
            "Z. Dong",
            "K. Keutzer",
            "L. Du",
            "S. Zhang"
        ],
        "year": "2023",
        "doi": null,
        "in_text_citation": "[6]",
        "sentence": "Another optimization strategy involves quantization [4]\u2013[6], which aims to minimize the memory footprint and computational requirements without significantly impacting accuracy.",
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "context": null
    },
    {
        "id": 7,
        "title": "Wide & deep learning for recommender systems",
        "author": [
            "H.-T. Cheng",
            "L. Koc",
            "J. Harmsen",
            "T. Shaked",
            "T. Chandra",
            "H. Aradhye",
            "G. Anderson",
            "G. Corrado",
            "W. Chai",
            "M. Ispir"
        ],
        "year": "2016",
        "doi": null,
        "in_text_citation": "[7]",
        "sentence": "Embedding tables, which map sparse categorical features to dense vectors [7], [8], are often prime targets for quantization due to their large sizes.",
        "abstract": "Generalized linear models with nonlinear feature transformations are widely\nused for large-scale regression and classification problems with sparse inputs.\nMemorization of feature interactions through a wide set of cross-product\nfeature transformations are effective and interpretable, while generalization\nrequires more feature engineering effort. With less feature engineering, deep\nneural networks can generalize better to unseen feature combinations through\nlow-dimensional dense embeddings learned for the sparse features. However, deep\nneural networks with embeddings can over-generalize and recommend less relevant\nitems when the user-item interactions are sparse and high-rank. In this paper,\nwe present Wide & Deep learning---jointly trained wide linear models and deep\nneural networks---to combine the benefits of memorization and generalization\nfor recommender systems. We productionized and evaluated the system on Google\nPlay, a commercial mobile app store with over one billion active users and over\none million apps. Online experiment results show that Wide & Deep significantly\nincreased app acquisitions compared with wide-only and deep-only models. We\nhave also open-sourced our implementation in TensorFlow.",
        "full_text": "Wide & Deep Learning for Recommender Systems\nHeng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra,\nHrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, Rohan Anil,\nZakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, Hemal Shah\nGoogle Inc.\n\u2217\nABSTRACT\nGeneralized linear models with nonlinear feature transfor-\nmations are widely used for large-scale regression and clas-\nsi\ufb01cation problems with sparse inputs. Memorization of fea-\nture interactions through a wide set of cross-product feature\ntransformations are e\ufb00ective and interpretable, while gener-\nalization requires more feature engineering e\ufb00ort. With less\nfeature engineering, deep neural networks can generalize bet-\nter to unseen feature combinations through low-dimensional\ndense embeddings learned for the sparse features. However,\ndeep neural networks with embeddings can over-generalize\nand recommend less relevant items when the user-item inter-\nactions are sparse and high-rank. In this paper, we present\nWide & Deep learning\u2014jointly trained wide linear models\nand deep neural networks\u2014to combine the bene\ufb01ts of mem-\norization and generalization for recommender systems. We\nproductionized and evaluated the system on Google Play,\na commercial mobile app store with over one billion active\nusers and over one million apps. Online experiment results\nshow that Wide & Deep signi\ufb01cantly increased app acquisi-\ntions compared with wide-only and deep-only models. We\nhave also open-sourced our implementation in TensorFlow.\nCCS Concepts\n\u2022Computing methodologies \u2192Machine learning; Neu-\nral networks; Supervised learning; \u2022Information systems\n\u2192Recommender systems;\nKeywords\nWide & Deep Learning, Recommender Systems.\n1.\nINTRODUCTION\nA recommender system can be viewed as a search ranking\nsystem, where the input query is a set of user and contextual\ninformation, and the output is a ranked list of items. Given\na query, the recommendation task is to \ufb01nd the relevant\nitems in a database and then rank the items based on certain\nobjectives, such as clicks or purchases.\nOne challenge in recommender systems, similar to the gen-\neral search ranking problem, is to achieve both memorization\nand generalization. Memorization can be loosely de\ufb01ned as\nlearning the frequent co-occurrence of items or features and\nexploiting the correlation available in the historical data.\nGeneralization, on the other hand, is based on transitivity\nof correlation and explores new feature combinations that\n\u2217Corresponding author: hengtze@google.com\nhave never or rarely occurred in the past.\nRecommenda-\ntions based on memorization are usually more topical and\ndirectly relevant to the items on which users have already\nperformed actions. Compared with memorization, general-\nization tends to improve the diversity of the recommended\nitems. In this paper, we focus on the apps recommendation\nproblem for the Google Play store, but the approach should\napply to generic recommender systems.\nFor massive-scale online recommendation and ranking sys-\ntems in an industrial setting, generalized linear models such\nas logistic regression are widely used because they are sim-\nple, scalable and interpretable. The models are often trained\non binarized sparse features with one-hot encoding. E.g., the\nbinary feature \u201cuser_installed_app=netflix\u201d has value 1\nif the user installed Net\ufb02ix. Memorization can be achieved\ne\ufb00ectively using cross-product transformations over sparse\nfeatures, such as AND(user_installed_app=netflix, impres-\nsion_app=pandora\u201d), whose value is 1 if the user installed\nNet\ufb02ix and then is later shown Pandora. This explains how\nthe co-occurrence of a feature pair correlates with the target\nlabel. Generalization can be added by using features that are\nless granular, such as AND(user_installed_category=video,\nimpression_category=music), but manual feature engineer-\ning is often required. One limitation of cross-product trans-\nformations is that they do not generalize to query-item fea-\nture pairs that have not appeared in the training data.\nEmbedding-based models, such as factorization machines\n[5] or deep neural networks, can generalize to previously un-\nseen query-item feature pairs by learning a low-dimensional\ndense embedding vector for each query and item feature,\nwith less burden of feature engineering. However, it is dif-\n\ufb01cult to learn e\ufb00ective low-dimensional representations for\nqueries and items when the underlying query-item matrix is\nsparse and high-rank, such as users with speci\ufb01c preferences\nor niche items with a narrow appeal. In such cases, there\nshould be no interactions between most query-item pairs,\nbut dense embeddings will lead to nonzero predictions for all\nquery-item pairs, and thus can over-generalize and make less\nrelevant recommendations. On the other hand, linear mod-\nels with cross-product feature transformations can memorize\nthese \u201cexception rules\u201d with much fewer parameters.\nIn this paper, we present the Wide & Deep learning frame-\nwork to achieve both memorization and generalization in one\nmodel, by jointly training a linear model component and a\nneural network component as shown in Figure 1.\nThe main contributions of the paper include:\n\u2022 The Wide & Deep learning framework for jointly train-\ning feed-forward neural networks with embeddings and\narXiv:1606.07792v1  [cs.LG]  24 Jun 2016\nWide Models\nDeep Models\nWide & Deep Models\nHidden Layers\nSparse Features\nOutput Units\nDense\nEmbeddings\nFigure 1: The spectrum of Wide & Deep models.\nlinear model with feature transformations for generic\nrecommender systems with sparse inputs.\n\u2022 The implementation and evaluation of the Wide &\nDeep recommender system productionized on Google\nPlay, a mobile app store with over one billion active\nusers and over one million apps.\n\u2022 We have open-sourced our implementation along with\na high-level API in TensorFlow1.\nWhile the idea is simple, we show that the Wide & Deep\nframework signi\ufb01cantly improves the app acquisition rate\non the mobile app store, while satisfying the training and\nserving speed requirements.\n2.\nRECOMMENDER SYSTEM OVERVIEW\nAn overview of the app recommender system is shown\nin Figure 2. A query, which can include various user and\ncontextual features, is generated when a user visits the app\nstore. The recommender system returns a list of apps (also\nreferred to as impressions) on which users can perform cer-\ntain actions such as clicks or purchases. These user actions,\nalong with the queries and impressions, are recorded in the\nlogs as the training data for the learner.\nSince there are over a million apps in the database, it is\nintractable to exhaustively score every app for every query\nwithin the serving latency requirements (often O(10) mil-\nliseconds). Therefore, the \ufb01rst step upon receiving a query\nis retrieval. The retrieval system returns a short list of items\nthat best match the query using various signals, usually a\ncombination of machine-learned models and human-de\ufb01ned\nrules. After reducing the candidate pool, the ranking sys-\ntem ranks all items by their scores. The scores are usually\nP(y|x), the probability of a user action label y given the\nfeatures x, including user features (e.g., country, language,\ndemographics), contextual features (e.g., device, hour of the\nday, day of the week), and impression features (e.g., app age,\nhistorical statistics of an app). In this paper, we focus on the\nranking model using the Wide & Deep learning framework.\n3.\nWIDE & DEEP LEARNING\n3.1\nThe Wide Component\nThe wide component is a generalized linear model of the\nform y = wT x + b, as illustrated in Figure 1 (left). y is the\nprediction, x = [x1, x2, ..., xd] is a vector of d features, w =\n[w1, w2, ..., wd] are the model parameters and b is the bias.\nThe feature set includes raw input features and transformed\n1See Wide & Deep Tutorial on http://tensorflow.org.\nItem 1\nItem 2\nItem 3\n...\nDatabase\nQuery\nItems\nLearner\nModel\nRanked\nO(10) items\nLogs\nUser Actions\nRetrieval\nO(100) items\nRanking\nRecommendation System\nAll items\nFigure 2: Overview of the recommender system.\nfeatures. One of the most important transformations is the\ncross-product transformation, which is de\ufb01ned as:\n\u03c6k(x) =\nd\nY\ni=1\nxcki\ni\ncki \u2208{0, 1}\n(1)\nwhere cki is a boolean variable that is 1 if the i-th fea-\nture is part of the k-th transformation \u03c6k, and 0 otherwise.\nFor binary features, a cross-product transformation (e.g.,\n\u201cAND(gender=female, language=en)\u201d) is 1 if and only if the\nconstituent features (\u201cgender=female\u201d and \u201clanguage=en\u201d)\nare all 1, and 0 otherwise. This captures the interactions\nbetween the binary features, and adds nonlinearity to the\ngeneralized linear model.\n3.2\nThe Deep Component\nThe deep component is a feed-forward neural network, as\nshown in Figure 1 (right). For categorical features, the orig-\ninal inputs are feature strings (e.g., \u201clanguage=en\u201d). Each\nof these sparse, high-dimensional categorical features are\n\ufb01rst converted into a low-dimensional and dense real-valued\nvector, often referred to as an embedding vector. The di-\nmensionality of the embeddings are usually on the order of\nO(10) to O(100). The embedding vectors are initialized ran-\ndomly and then the values are trained to minimize the \ufb01nal\nloss function during model training. These low-dimensional\ndense embedding vectors are then fed into the hidden layers\nof a neural network in the forward pass. Speci\ufb01cally, each\nhidden layer performs the following computation:\na(l+1) = f(W (l)a(l) + b(l))\n(2)\nwhere l is the layer number and f is the activation function,\noften recti\ufb01ed linear units (ReLUs). a(l), b(l), and W (l) are\nthe activations, bias, and model weights at l-th layer.\n3.3\nJoint Training of Wide & Deep Model\nThe wide component and deep component are combined\nusing a weighted sum of their output log odds as the pre-\nUser Data\nApp \nImpression \nData\nTraining Data\nGeneration\nVocabulary \nGenerator\nModel \nTrainer\nModel Verifier\nModel Servers\nModel Serving\nApps \nRecommendation \nEngine\nPrevious Models\n     Data Generation\nModel Training\nFigure 3: Apps recommendation pipeline overview.\ndiction, which is then fed to one common logistic loss func-\ntion for joint training. Note that there is a distinction be-\ntween joint training and ensemble.\nIn an ensemble, indi-\nvidual models are trained separately without knowing each\nother, and their predictions are combined only at inference\ntime but not at training time. In contrast, joint training\noptimizes all parameters simultaneously by taking both the\nwide and deep part as well as the weights of their sum into\naccount at training time. There are implications on model\nsize too: For an ensemble, since the training is disjoint, each\nindividual model size usually needs to be larger (e.g., with\nmore features and transformations) to achieve reasonable\naccuracy for an ensemble to work. In comparison, for joint\ntraining the wide part only needs to complement the weak-\nnesses of the deep part with a small number of cross-product\nfeature transformations, rather than a full-size wide model.\nJoint training of a Wide & Deep Model is done by back-\npropagating the gradients from the output to both the wide\nand deep part of the model simultaneously using mini-batch\nstochastic optimization. In the experiments, we used Follow-\nthe-regularized-leader (FTRL) algorithm [3] with L1 regu-\nlarization as the optimizer for the wide part of the model,\nand AdaGrad [1] for the deep part.\nThe combined model is illustrated in Figure 1 (center).\nFor a logistic regression problem, the model\u2019s prediction is:\nP(Y = 1|x) = \u03c3(wT\nwide[x, \u03c6(x)] + wT\ndeepa(lf ) + b)\n(3)\nwhere Y is the binary class label, \u03c3(\u00b7) is the sigmoid func-\ntion, \u03c6(x) are the cross product transformations of the orig-\ninal features x, and b is the bias term. wwide is the vector of\nall wide model weights, and wdeep are the weights applied\non the \ufb01nal activations a(lf ).\n4.\nSYSTEM IMPLEMENTATION\nThe implementation of the apps recommendation pipeline\nconsists of three stages: data generation, model training,\nand model serving as shown in Figure 3.\n4.1\nData Generation\nIn this stage, user and app impression data within a period\nof time are used to generate training data. Each example\ncorresponds to one impression. The label is app acquisition:\n1 if the impressed app was installed, and 0 otherwise.\nVocabularies, which are tables mapping categorical fea-\nture strings to integer IDs, are also generated in this stage.\nThe system computes the ID space for all the string features\nthat occurred more than a minimum number of times. Con-\ntinuous real-valued features are normalized to [0, 1] by map-\nping a feature value x to its cumulative distribution function\nP(X \u2264x), divided into nq quantiles. The normalized value\nis\ni\u22121\nnq\u22121 for values in the i-th quantiles. Quantile boundaries\nReLU (1024)\nLogistic Loss\nEmbeddings\nReLU (512)\nReLU (256)\nUser Installed \nApp\nImpression \nApp\nUser \nDemographics\nDevice \nClass\n...\nAge\n#App \nInstalls\n#Engagement \nsessions\n...\nCross Product \nTransformation\nEmbeddings\nEmbeddings\nEmbeddings\nConcatenated Embeddings (~1200 dimensions)\nContinuous Features\nCategorical Features\nFigure 4: Wide & Deep model structure for apps\nrecommendation.\nare computed during data generation.\n4.2\nModel Training\nThe model structure we used in the experiment is shown in\nFigure 4. During training, our input layer takes in training\ndata and vocabularies and generate sparse and dense fea-\ntures together with a label. The wide component consists\nof the cross-product transformation of user installed apps\nand impression apps. For the deep part of the model, A 32-\ndimensional embedding vector is learned for each categorical\nfeature. We concatenate all the embeddings together with\nthe dense features, resulting in a dense vector of approxi-\nmately 1200 dimensions. The concatenated vector is then\nfed into 3 ReLU layers, and \ufb01nally the logistic output unit.\nThe Wide & Deep models are trained on over 500 billion\nexamples.\nEvery time a new set of training data arrives,\nthe model needs to be re-trained. However, retraining from\nscratch every time is computationally expensive and delays\nthe time from data arrival to serving an updated model.\nTo tackle this challenge, we implemented a warm-starting\nsystem which initializes a new model with the embeddings\nand the linear model weights from the previous model.\nBefore loading the models into the model servers, a dry\nrun of the model is done to make sure that it does not cause\nproblems in serving live tra\ufb03c. We empirically validate the\nmodel quality against the previous model as a sanity check.\n4.3\nModel Serving\nOnce the model is trained and veri\ufb01ed, we load it into the\nmodel servers. For each request, the servers receive a set\nof app candidates from the app retrieval system and user\nfeatures to score each app. Then, the apps are ranked from\nthe highest scores to the lowest, and we show the apps to\nthe users in this order. The scores are calculated by running\na forward inference pass over the Wide & Deep model.\nIn order to serve each request on the order of 10 ms, we\noptimized the performance using multithreading parallelism\nby running smaller batches in parallel, instead of scoring all\ncandidate apps in a single batch inference step.\n5.\nEXPERIMENT RESULTS\nTo evaluate the e\ufb00ectiveness of Wide & Deep learning in\na real-world recommender system, we ran live experiments\nand evaluated the system in a couple of aspects: app acqui-\nsitions and serving performance.\n5.1\nApp Acquisitions\nWe conducted live online experiments in an A/B test-\ning framework for 3 weeks. For the control group, 1% of\nTable 1: O\ufb04ine & online metrics of di\ufb00erent models.\nOnline Acquisition Gain is relative to the control.\nModel\nO\ufb04ine AUC\nOnline Acquisition Gain\nWide (control)\n0.726\n0%\nDeep\n0.722\n+2.9%\nWide & Deep\n0.728\n+3.9%\nusers were randomly selected and presented with recom-\nmendations generated by the previous version of ranking\nmodel, which is a highly-optimized wide-only logistic regres-\nsion model with rich cross-product feature transformations.\nFor the experiment group, 1% of users were presented with\nrecommendations generated by the Wide & Deep model,\ntrained with the same set of features. As shown in Table 1,\nWide & Deep model improved the app acquisition rate on\nthe main landing page of the app store by +3.9% relative to\nthe control group (statistically signi\ufb01cant). The results were\nalso compared with another 1% group using only the deep\npart of the model with the same features and neural network\nstructure, and the Wide & Deep mode had +1% gain on top\nof the deep-only model (statistically signi\ufb01cant).\nBesides online experiments, we also show the Area Under\nReceiver Operator Characteristic Curve (AUC) on a holdout\nset o\ufb04ine. While Wide & Deep has a slightly higher o\ufb04ine\nAUC, the impact is more signi\ufb01cant on online tra\ufb03c. One\npossible reason is that the impressions and labels in o\ufb04ine\ndata sets are \ufb01xed, whereas the online system can generate\nnew exploratory recommendations by blending generaliza-\ntion with memorization, and learn from new user responses.\n5.2\nServing Performance\nServing with high throughput and low latency is challeng-\ning with the high level of tra\ufb03c faced by our commercial\nmobile app store. At peak tra\ufb03c, our recommender servers\nscore over 10 million apps per second. With single threading,\nscoring all candidates in a single batch takes 31 ms. We im-\nplemented multithreading and split each batch into smaller\nsizes, which signi\ufb01cantly reduced the client-side latency to\n14 ms (including serving overhead) as shown in Table 2.\n6.\nRELATED WORK\nThe idea of combining wide linear models with cross-\nproduct feature transformations and deep neural networks\nwith dense embeddings is inspired by previous work, such as\nfactorization machines [5] which add generalization to linear\nmodels by factorizing the interactions between two variables\nas a dot product between two low-dimensional embedding\nvectors. In this paper, we expanded the model capacity by\nlearning highly nonlinear interactions between embeddings\nvia neural networks instead of dot products.\nIn language models, joint training of recurrent neural net-\nworks (RNNs) and maximum entropy models with n-gram\nfeatures has been proposed to signi\ufb01cantly reduce the RNN\ncomplexity (e.g., hidden layer sizes) by learning direct weights\nbetween inputs and outputs [4]. In computer vision, deep\nresidual learning [2] has been used to reduce the di\ufb03culty of\ntraining deeper models and improve accuracy with shortcut\nconnections which skip one or more layers. Joint training of\nneural networks with graphical models has also been applied\nto human pose estimation from images [6]. In this work we\nexplored the joint training of feed-forward neural networks\nTable 2: Serving latency vs. batch size and threads.\nBatch size\nNumber of Threads\nServing Latency (ms)\n200\n1\n31\n100\n2\n17\n50\n4\n14\nand linear models, with direct connections between sparse\nfeatures and the output unit, for generic recommendation\nand ranking problems with sparse input data.\nIn the recommender systems literature, collaborative deep\nlearning has been explored by coupling deep learning for\ncontent information and collaborative \ufb01ltering (CF) for the\nratings matrix [7]. There has also been previous work on\nmobile app recommender systems, such as AppJoy which\nused CF on users\u2019 app usage records [8]. Di\ufb00erent from the\nCF-based or content-based approaches in the previous work,\nwe jointly train Wide & Deep models on user and impression\ndata for app recommender systems.\n7.\nCONCLUSION\nMemorization and generalization are both important for\nrecommender systems. Wide linear models can e\ufb00ectively\nmemorize sparse feature interactions using cross-product fea-\nture transformations, while deep neural networks can gener-\nalize to previously unseen feature interactions through low-\ndimensional embeddings. We presented the Wide & Deep\nlearning framework to combine the strengths of both types\nof model. We productionized and evaluated the framework\non the recommender system of Google Play, a massive-scale\ncommercial app store.\nOnline experiment results showed\nthat the Wide & Deep model led to signi\ufb01cant improvement\non app acquisitions over wide-only and deep-only models.\n8.\nREFERENCES\n[1] J. Duchi, E. Hazan, and Y. Singer. Adaptive\nsubgradient methods for online learning and stochastic\noptimization. Journal of Machine Learning Research,\n12:2121\u20132159, July 2011.\n[2] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual\nlearning for image recognition. Proc. IEEE Conference\non Computer Vision and Pattern Recognition, 2016.\n[3] H. B. McMahan. Follow-the-regularized-leader and\nmirror descent: Equivalence theorems and l1\nregularization. In Proc. AISTATS, 2011.\n[4] T. Mikolov, A. Deoras, D. Povey, L. Burget, and J. H.\nCernocky. Strategies for training large scale neural\nnetwork language models. In IEEE Automatic Speech\nRecognition & Understanding Workshop, 2011.\n[5] S. Rendle. Factorization machines with libFM. ACM\nTrans. Intell. Syst. Technol., 3(3):57:1\u201357:22, May 2012.\n[6] J. J. Tompson, A. Jain, Y. LeCun, and C. Bregler. Joint\ntraining of a convolutional network and a graphical\nmodel for human pose estimation. In Z. Ghahramani,\nM. Welling, C. Cortes, N. D. Lawrence, and K. Q.\nWeinberger, editors, NIPS, pages 1799\u20131807. 2014.\n[7] H. Wang, N. Wang, and D.-Y. Yeung. Collaborative\ndeep learning for recommender systems. In Proc. KDD,\npages 1235\u20131244, 2015.\n[8] B. Yan and G. Chen. AppJoy: Personalized mobile\napplication discovery. In MobiSys, pages 113\u2013126, 2011.\n",
        "context": "[5] or deep neural networks, can generalize to previously un-\nseen query-item feature pairs by learning a low-dimensional\ndense embedding vector for each query and item feature,\nwith less burden of feature engineering. However, it is dif-\nshown in Figure 1 (right). For categorical features, the orig-\ninal inputs are feature strings (e.g., \u201clanguage=en\u201d). Each\nof these sparse, high-dimensional categorical features are\n\ufb01rst converted into a low-dimensional and dense real-valued\nfeature engineering, deep neural networks can generalize bet-\nter to unseen feature combinations through low-dimensional\ndense embeddings learned for the sparse features. However,\ndeep neural networks with embeddings can over-generalize"
    },
    {
        "id": 8,
        "title": "Analysis of quantization on mlp-based vision models",
        "author": [
            "L. Zhao",
            "Z. Dong",
            "K. Keutzer"
        ],
        "year": "2022",
        "doi": "10.48550/arXiv.2209.06383",
        "in_text_citation": "[8]",
        "sentence": "Embedding tables, which map sparse categorical features to dense vectors [7], [8], are often prime targets for quantization due to their large sizes.",
        "abstract": "",
        "full_text": "",
        "context": null
    },
    {
        "id": 9,
        "title": "Deep learning recommendation model for personalization and recommendation systems",
        "author": [
            "M. Naumov",
            "D. Mudigere",
            "H.-J. M. Shi",
            "J. Huang",
            "N. Sundaraman",
            "J. Park",
            "X. Wang",
            "U. Gupta",
            "C.-J. Wu",
            "A. G. Azzolini"
        ],
        "year": "2019",
        "doi": "10.48550/arXiv.1906.00091",
        "in_text_citation": "[9]",
        "sentence": "By quantizing the embeddings to lower bit-width representations in DLRMs [9], such as 4-bit [10], [11], or performing tensor train decomposition [12], memory usage can be significantly reduced, making it more feasible to train and inference.",
        "abstract": "With the advent of deep learning, neural network-based recommendation models\nhave emerged as an important tool for tackling personalization and\nrecommendation tasks. These networks differ significantly from other deep\nlearning networks due to their need to handle categorical features and are not\nwell studied or understood. In this paper, we develop a state-of-the-art deep\nlearning recommendation model (DLRM) and provide its implementation in both\nPyTorch and Caffe2 frameworks. In addition, we design a specialized\nparallelization scheme utilizing model parallelism on the embedding tables to\nmitigate memory constraints while exploiting data parallelism to scale-out\ncompute from the fully-connected layers. We compare DLRM against existing\nrecommendation models and characterize its performance on the Big Basin AI\nplatform, demonstrating its usefulness as a benchmark for future algorithmic\nexperimentation and system co-design.",
        "full_text": "Deep Learning Recommendation Model for\nPersonalization and Recommendation Systems\nMaxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael Shi\u2217, Jianyu Huang,\nNarayanan Sundaraman, Jongsoo Park, Xiaodong Wang, Udit Gupta\u2020, Carole-Jean Wu,\nAlisson G. Azzolini, Dmytro Dzhulgakov, Andrey Mallevich, Ilia Cherniavskii, Yinghai Lu,\nRaghuraman Krishnamoorthi, Ansha Yu, Volodymyr Kondratenko, Stephanie Pereira,\nXianjie Chen, Wenlin Chen, Vijay Rao, Bill Jia, Liang Xiong and Misha Smelyanskiy\nFacebook, 1 Hacker Way, Menlo Park, CA 94065\n{mnaumov,dheevatsa}@fb.com\nAbstract\nWith the advent of deep learning, neural network-based recommendation models\nhave emerged as an important tool for tackling personalization and recommendation\ntasks. These networks differ signi\ufb01cantly from other deep learning networks due\nto their need to handle categorical features and are not well studied or understood.\nIn this paper, we develop a state-of-the-art deep learning recommendation model\n(DLRM) and provide its implementation in both PyTorch and Caffe2 frameworks.\nIn addition, we design a specialized parallelization scheme utilizing model paral-\nlelism on the embedding tables to mitigate memory constraints while exploiting\ndata parallelism to scale-out compute from the fully-connected layers. We compare\nDLRM against existing recommendation models and characterize its performance\non the Big Basin AI platform, demonstrating its usefulness as a benchmark for\nfuture algorithmic experimentation and system co-design.\n1\nIntroduction\nPersonalization and recommendation systems are currently deployed for a variety of tasks at large\ninternet companies, including ad click-through rate (CTR) prediction and rankings. Although these\nmethods have had long histories, these approaches have only recently embraced neural networks.\nTwo primary perspectives contributed towards the architectural design of deep learning models for\npersonalization and recommendation.\nThe \ufb01rst comes from the view of recommendation systems. These systems initially employed content\n\ufb01ltering where a set of experts classi\ufb01ed products into categories, while users selected their preferred\ncategories and were matched based on their preferences [22]. The \ufb01eld subsequently evolved to use\ncollaborative \ufb01ltering, where recommendations are based on past user behaviors, such as prior ratings\ngiven to products. Neighborhood methods [21] that provide recommendations by grouping users and\nproducts together and latent factor methods that characterize users and products by certain implicit\nfactors via matrix factorization techniques [9, 17] were later deployed with success.\nThe second view comes from predictive analytics, which relies on statistical models to classify or\npredict the probability of events based on the given data [5]. Predictive models shifted from using\nsimple models such as linear and logistic regression [26] to models that incorporate deep networks.\nIn order to process categorical data, these models adopted the use of embeddings, which transform\nthe one- and multi-hot vectors into dense representations in an abstract space [20]. This abstract\nspace may be interpreted as the space of the latent factors found by recommendation systems.\n\u2217Northwestern University, \u2020Harvard University, work done while at Facebook.\nPreprint. Under review.\narXiv:1906.00091v1  [cs.IR]  31 May 2019\nIn this paper, we introduce a personalization model that was conceived by the union of the two\nperspectives described above. The model uses embeddings to process sparse features that represent\ncategorical data and a multilayer perceptron (MLP) to process dense features, then interacts these\nfeatures explicitly using the statistical techniques proposed in [24]. Finally, it \ufb01nds the event\nprobability by post-processing the interactions with another MLP. We refer to this model as a deep\nlearning recommendation model (DLRM); see Fig. 1. A PyTorch and Caffe2 implementation of this\nmodel will be released for testing and experimentation with the publication of this manuscript.\n2\nModel Design and Architecture\nIn this section, we will describe the design of DLRM. We will begin with the high level components\nof the network and explain how and why they have been assembled together in a particular way, with\nimplications for future model design, then characterize the low level operators and primitives that\nmake up the model, with implications for future hardware and system design.\n2.1\nComponents of DLRM\nFigure 1: A deep learning recommendation model\nThe high-level components of the DLRM can be\nmore easily understood by reviewing early mod-\nels. We will avoid the full scienti\ufb01c literature\nreview and focus instead on the four techniques\nused in early models that can be interpreted as\nsalient high-level components of the DLRM.\n2.1.1\nEmbeddings\nIn order to handle categorical data, embeddings\nmap each category to a dense representation in\nan abstract space. In particular, each embedding\nlookup may be interpreted as using a one-hot\nvector ei (with the i-th position being 1 while\nothers are 0, where index i corresponds to i-th\ncategory) to obtain the corresponding row vector\nof the embedding table W \u2208Rm\u00d7d as follows\nwT\ni = eT\ni W.\n(1)\nIn more complex scenarios, an embedding can also represent a weighted combination of multiple\nitems, with a multi-hot vector of weights aT = [0, ..., ai1, ..., aik, ..., 0], with elements ai \u0338= 0 for\ni = i1, ..., ik and 0 everywhere else, where i1, ..., ik index the corresponding items. Note that a\nmini-batch of t embedding lookups can hence be written as\nS = AT W\n(2)\nwhere sparse matrix A = [a1, ..., at] [20].\nDLRMs will utilize embedding tables for mapping categorical features to dense representations.\nHowever, even after these embeddings are meaningfully devised, how are they to be exploited to\nproduce accurate predictions? To answer this, we return to latent factor methods.\n2.1.2\nMatrix Factorization\nRecall that in the typical formulation of the recommendation problem, we are given a set S of users\nthat have rated some products. We would like to represent the i-th product by a vector wi \u2208Rd for\ni = 1, ..., n and j-th user by a vector vj \u2208Rd for j = 1, ..., m to \ufb01nd all the ratings, where n and m\ndenote the total number of products and users, respectively. More rigorously, the set S consists of\ntuples (i, j) indexing when the i-th product has been rated by the j-th user.\nThe matrix factorization approach solves this problem by minimizing\nmin\nX\n(i,j)\u2208S\nrij \u2212wT\ni vj\n(3)\n2\nwhere rij \u2208R is the rating of the i-th product by the j-th user for i = 1, ..., m and j = 1, ..., n.\nThen, letting W T = [w1, ..., wm] and V T = [v1, ..., vn], we may approximate the full matrix of\nratings R = [rij] as the matrix product R \u2248WV T . Note that W and V may be interpreted as two\nembedding tables, where each row represents a user/product in a latent factor space2 [17]. The dot\nproduct of these embedding vectors yields a meaningful prediction of the subsequent rating, a key\nobservation to the design of factorization machines and DLRM.\n2.1.3\nFactorization Machine\nIn classi\ufb01cation problems, we want to de\ufb01ne a prediction function \u03c6 : Rn \u2192T from an input\ndatapoint x \u2208Rn to a target label y \u2208T. As an example, we can predict the click-through rate by\nde\ufb01ning T = {+1, \u22121} with +1 denoting the presence of a click and \u22121 as the absence of a click.\nFactorization machines (FM) incorporate second-order interactions into a linear model with categori-\ncal data by de\ufb01ning a model of the form\n\u02c6y = b + wT x + xT upper(V V T )x\n(4)\nwhere V \u2208Rn\u00d7d, w \u2208Rn, and b \u2208R are the parameters with d \u226an, and upper selects the strictly\nupper triangular part of the matrix [24].\nFMs are notably distinct from support vector machines (SVMs) with polynomial kernels [4] because\nthey factorize the second-order interaction matrix into its latent factors (or embedding vectors) as\nin matrix factorization, which more effectively handles sparse data. This signi\ufb01cantly reduces the\ncomplexity of the second-order interactions by only capturing interactions between pairs of distinct\nembedding vectors, yielding linear computational complexity.\n2.1.4\nMultilayer Perceptrons\nSimultaneously, much recent success in machine learning has been due to the rise of deep learning.\nThe most fundamental model of these is the multilayer perceptron (MLP), a prediction function\ncomposed of an interleaving sequence of fully connected (FC) layers and an activation function\n\u03c3 : R \u2192R applied componentwise as shown below\n\u02c6y = Wk\u03c3(Wk\u22121\u03c3(...\u03c3(W1x + b1)...) + bk\u22121) + bk\n(5)\nwhere weight matrix Wl \u2208Rnl\u00d7nl\u22121, bias bl \u2208Rnl for layer l = 1, ..., k.\nThese methods have been used to capture more complex interactions. It has been shown, for example,\nthat given enough parameters, MLPs with suf\ufb01cient depth and width can \ufb01t data to arbitrary precision\n[1]. Variations of these methods have been widely used in various applications including computer\nvision and natural language processing. One speci\ufb01c case, Neural Collaborative Filtering (NCF)\n[15, 25] used as part of the MLPerf benchmark [19], uses an MLP rather than dot product to compute\ninteractions between embeddings in matrix factorization.\n2.2\nDLRM Architecture\nSo far, we have described different models used in recommendation systems and predictive analytics.\nLet us now combine their intuitions to build a state-of-the-art personalization model.\nLet the users and products be described by many continuous and categorical features. To process\nthe categorical features, each categorical feature will be represented by an embedding vector of the\nsame dimension, generalizing the concept of latent factors used in matrix factorization (3). To handle\nthe continuous features, the continuous features will be transformed by an MLP (which we call the\nbottom or dense MLP) which will yield a dense representation of the same length as the embedding\nvectors (5).\nWe will compute second-order interaction of different features explicitly, following the intuition for\nhandling sparse data provided in FMs (4), optionally passing them through MLPs. This is done by\ntaking the dot product between all pairs of embedding vectors and processed dense features. These\ndot products are concatenated with the original processed dense features and post-processed with\nanother MLP (the top or output MLP) (5), and fed into a sigmoid function to give a probability.\n2This problem is different from low-rank approximation, which can be solved by SVD [11], because not all entries of matrix R are known.\n3\nWe refer to the resulting model as DLRM, shown in Fig. 1. We show some of the operators used in\nDLRM in PyTorch [23] and Caffe2 [8] frameworks in Table 1.\nEmbedding\nMLP\nInteractions\nLoss\nPyTorch\nnn.EmbeddingBag\nnn.Linear/addmm\nmatmul/bmm\nnn.CrossEntropyLoss\nCaffe2\nSparseLengthSum\nFC\nBatchMatMul\nCrossEntropy\nTable 1: DLRM operators by framework\n2.3\nComparison with Prior Models\nMany deep learning-based recommendation models [3, 13, 27, 18, 28, 29] use similar underlying\nideas to generate higher-order terms to handle sparse features. Wide and Deep, Deep and Cross,\nDeepFM, and xDeepFM networks, for example, design specialized networks to systematically\nconstruct higher-order interactions. These networks then sum the results from both their specialized\nmodel and an MLP, passing this through a linear layer and sigmoid activation to yield a \ufb01nal\nprobability. DLRM speci\ufb01cally interacts embeddings in a structured way that mimics factorization\nmachines to signi\ufb01cantly reduce the dimensionality of the model by only considering cross-terms\nproduced by the dot-product between pairs of embeddings in the \ufb01nal MLP. We argue that higher-\norder interactions beyond second-order found in other networks may not necessarily be worth the\nadditional computational/memory cost.\nA key difference between DLRM and other networks is in how these networks treat embedded feature\nvectors and their cross-terms. In particular, DLRM (and xDeepFM [18]) interpret each feature vector\nas a single unit representing a single category, whereas networks like Deep and Cross treat each\nelement in the feature vector as a new unit that should yield different cross-terms. Hence, Deep and\nCross networks will produce cross-terms not only between elements from different feature vectors\nas in DLRM via the dot product, but also produce cross-terms between elements within the same\nfeature vector, resulting in higher dimensionality.\n3\nParallelism\nModern personalization and recommendation systems require large and complex models to capitalize\non vast amounts of data. DLRMs particularly contain a very large number of parameters, up to\nmultiple orders of magnitude more than other common deep learning models like convolutional\nneural networks (CNN), transformer and recurrent networks (RNN), and generative networks (GAN).\nThis results in training times up to several weeks or more. Hence, it is important to parallelize these\nmodels ef\ufb01ciently in order to solve these problems at practical scales.\nAs described in the previous section, DLRMs process both categorical features (with embeddings)\nand continuous features (with the bottom MLP) in a coupled manner. Embeddings contribute the\nmajority of the parameters, with several tables each requiring in excess of multiple GBs of memory,\nmaking DLRM memory-capacity and bandwidth intensive. The size of the embeddings makes it\nprohibitive to use data parallelism since it requires replicating large embeddings on every device. In\nmany cases, this memory constraint necessitates the distribution of the model across multiple devices\nto be able satisfy memory capacity requirements.\nOn the other hand, the MLP parameters are smaller in memory but translate into sizeable amounts of\ncompute. Hence, data-parallelism is preferred for MLPs since this enables concurrent processing\nof the samples on different devices and only requires communication when accumulating updates.\nOur parallelized DLRM will use a combination of model parallelism for the embeddings and data\nparallelism for the MLPs to mitigate the memory bottleneck produced by the embeddings while\nparallelizing the forward and backward propagations over the MLPs. Combined model and data\nparallelism is a unique requirement of DLRM as a result of its architecture and large model sizes.\nSuch combined parallelism is not supported in either Caffe2 or PyTorch (as well as other popular\ndeep learning frameworks), therefore we design a custom implementation. We plan to provide its\ndetailed performance study in forthcoming work.\nIn our setup, the top MLP and the interaction operator require access to part of the mini-batch from\nthe bottom MLP and all of the embeddings. Since model parallelism has been used to distribute the\nembeddings across devices, this requires a personalized all-to-all communication [12]. At the end of\nthe embedding lookup, each device has a vector for the embedding tables resident on those devices\nfor all the samples in the mini-batch, which needs to be split along the mini-batch dimension and\n4\nFigure 2: Butter\ufb02y shuf\ufb02e for the all-to-all (personalized) communication\ncommunicated to the appropriate devices, as shown in Fig. 2. Neither PyTorch nor Caffe2 provide\nnative support for model parallelism; therefore, we have implemented it by explicitly mapping the\nembedding operators (nn.EmbeddingBag for PyTorch, SparseLengthSum for Caffe2) to different\ndevices. Then personalized all-to-all communication is implemented using the butter\ufb02y shuf\ufb02e\noperator, which appropriately slices the resulting embedding vectors and transfers them to the target\ndevices. In the current version, these transfers are explicit copies, but we intend to further optimize\nthis using the available communication primitives (such as all-gather and send-recv).\nWe note that for the data parallel MLPs, the parameter updates in the backward pass are accu-\nmulated with an allreduce3 and applied to the replicated parameters on each device [12] in a\nsynchronous fashion, ensuring the updated parameters on each device are consistent before every\niteration. In PyTorch, data parallelism is enabled through the nn.DistributedDataParallel and\nnn.DataParallel modules that replicate the model on each device and insert allreduce with the\nnecessary dependencies. In Caffe2, we manually insert allreduce before the gradient update.\n4\nData\nIn order to measure the accuracy of the model, test its overall performance, and characterize the\nindividual operators, we need to create or obtain a data set for our implementation. Our current\nimplementation of the model supplies three types of data sets: random, synthetic and public data sets.\nThe former two data sets are useful in experimenting with the model from the systems perspective.\nIn particular, it permits us to exercise different hardware properties and bottlenecks by generating\ndata on the \ufb02y while removing dependencies on data storage systems. The latter allows us to perform\nexperiments on real data and measure the accuracy of the model.\n4.1\nRandom\nRecall that DLRM accepts continuous and categorical features as inputs. The former can be modeled\nby generating a vector of random numbers using either a uniform or normal (Gaussian) distributions\nwith the numpy.random package rand or randn calls with default parameters. Then a mini-batch\nof inputs can be obtained by generating a matrix where each row corresponds to an element in the\nmini-batch.\nTo generate categorical features, we need to determine how many non-zero elements we would like\nhave in a given multi-hot vector. The benchmark allows this number to be either \ufb01xed or random\nwithin a range4 [1, k]. Then, we generate the corresponding number of integer indices, within a\nrange [1, m], where m is the number of rows in the embedding W in (2). Finally, in order to create a\nmini-batch of lookups, we concatenate the above indices and delineate each individual lookup with\nlengths (SparseLengthsSum) or offsets (nn.EmbeddingBag)5.\n3Optimized implementations for the allreduce op. include Nvidia\u2019s NCCL [16] and Facebook\u2019s gloo [7].\n4see options --num-indices-per-lookup=k and --num-indices-per-lookup-fixed\n5For instance, in order to represent three embedding lookups, with indices {0, 2}, {0, 1, 5} and {3} we use\nlengths/offsets\n=\n{2, 3, 1}/{0, 2, 5}\nindices\n=\n{0, 2, 0, 1, 5, 3}\nNote that this format resembles Compressed-Sparse Row (CSR) often used for sparse matrices in linear algebra.\n5\n4.2\nSynthetic\nThere are many reasons to support custom generation of indices corresponding to categorical features.\nFor instance, if our application uses a particular data set, but we would not like to share it for privacy\npurposes, then we may choose to express the categorical features through distributions. This could\npotentially serve as an alternative to the privacy preserving techniques used in applications such as\nfederated learning [2, 10]. Also, if we would like to exercise system components, such as studying\nmemory behavior, we may want to capture fundamental locality of accesses of original trace within\nsynthetic trace.\nLet us now illustrate how we can use a synthetic data set. Assume that we have a trace of indices\nthat correspond to embedding lookups for a single categorical feature (and repeat the process for all\nfeatures). We can record the unique accesses and frequency of distances between repeated accesses\nin this trace (Alg. 1) and then generate a synthetic trace (Alg. 2) as proposed in [14].\nAlgorithm 1 Pro\ufb01le (Original) Trace\n1: Let tr be input sequence, s stack of distances, u list of unique accesses and p probability\ndistribution\n2: Let s.position_from_the_top return d = 0 if the index is not found, and d > 0 otherwise.\n3: for i=0; i<length(tr); i++ do\n4:\na = tr[i]\n5:\nd = s.position_from_the_top(a)\n6:\nif d == 0 then\n7:\nu.append(a)\n8:\nelse\n9:\ns.remove_from_the_top_at_position(d)\n10:\nend if\n11:\np[d] += 1.0/length(tr)\n12:\ns.push_to_the_top(a)\n13: end for\nAlgorithm 2 Generate (Synthetic) Trace\n1: Let u be input list of unique accesses and p probability distribution of distances, while tr output\ntrace.\n2: for s=0, i=0; i<length; i++ do\n3:\nd = p.sample_from_distribution_with_support(0,s)\n4:\nif d == 0 then\n5:\na = u.remove_from_front()\n6:\ns++\n7:\nelse\n8:\na = u.remove_from_the_back_at_position(d)\n9:\nend if\n10:\nu.append(a)\n11:\ntr[i] = a\n12: end for\nNote that we can only generate a stack distance up to s number of unique accesses we have seen so\nfar, therefore s is used to control the support of the distribution p in Alg. 2. Given a \ufb01xed number of\nunique accesses, the longer input trace will result in lower probability being assigned to them in Alg.\n1, which will lead to longer time to achieve full distribution support in Alg. 2. In order to address\nthis problem, we increase the probability for the unique accesses up to a minimum threshold and\nadjust support to remove unique accesses from it once all have been seen. A visual comparison of\nprobability distribution p based on original and synthetic traces is shown in Fig. 3. In our experiments\noriginal and adjusted synthetic traces produce similar cache hit/miss rates.\nAlg. 1 and 2 were designed for more accurate cache simulations, but they illustrate a general idea of\nhow probability distributions can be used to generate synthetic traces with desired properties.\n6\n(a) original\n(b) synthetic trace\n(c) adjusted synthetic trace\nFigure 3: Probability distribution p based on a sample trace tr = random.uniform(1,100,100K)\n4.3\nPublic\nFew public data sets are available for recommendation and personalization systems. The Criteo AI\nLabs Ad Kaggle6 and Terabyte7 data sets are open-sourced data sets consisting of click logs for\nad CTR prediction. Each data set contains 13 continuous and 26 categorical features. Typically\nthe continuous features are pre-processed with a simple log transform log(1 + x). The categorical\nfeature are mapped to its corresponding embedding index, with unlabeled categorical features or\nlabels mapped to 0 or NULL.\nThe Criteo Ad Kaggle data set contains approximately 45 million samples over 7 days. In experiments,\ntypically the 7th day is split into a validation and test set while the \ufb01rst 6 days are used as the training\nset. The Criteo Ad Terabyte data set is sampled over 24 days, where the 24th day is split into\na validation and test set and the \ufb01rst 23 days is used as a training set. Note that there are an\napproximately equal number of samples from each day.\n5\nExperiments\nFigure 4: Big Basin AI platform\nLet us now illustrate the performance and accuracy of DLRM.\nThe model is implemented in PyTorch and Caffe2 frameworks\nand is available on GitHub8. It uses fp32 \ufb02oating point and\nint32(Caffe2)/int64(PyTorch) types for model parameters\nand indices, respectively. The experiments are performed on\nthe Big Basin platform with Dual Socket Intel Xeon 6138 CPU\n@ 2.00GHz and eight Nvidia Tesla V100 16GB GPUs, publicly\navailable through the Open Compute Project9, shown in Fig. 4.\n5.1\nModel Accuracy on Public Data Sets\nWe evaluate the accuracy of the model on Criteo Ad Kaggle data set and compare the performance of\nDLRM against a Deep and Cross network (DCN) as-is without extensive tuning [27]. We compare\nwith DCN because it is one of the few models that has comprehensive results on the same data set.\nNotice that in this case the models are sized to accommodate the number of features present in the\ndata set. In particular, DLRM consists of both a bottom MLP for processing dense features consisting\nof three hidden layers with 512, 256 and 64 nodes, respectively, and a top MLP consisting of two\nhidden layers with 512 and 256 nodes. On the other hand DCN consists of six cross layers and a\ndeep network with 512 and 256 nodes. An embedding dimension of 16 is used. Note that this yields\na DLRM and DCN both with approximately 540M parameters.\nWe plot both the training (solid) and validation (dashed) accuracies over a full single epoch of training\nfor both models with SGD and Adagrad optimizers [6]. No regularization is used. In this experiment,\nDLRM obtains slightly higher training and validation accuracy, as shown in Fig. 5. We emphasize\nthat this is without extensive tuning of model hyperparameters.\n6https://www.kaggle.com/c/criteo-display-ad-challenge\n7https://labs.criteo.com/2013/12/download-terabyte-click-logs/\n8https://github.com/facebookresearch/dlrm\n9https://www.opencompute.org\n7\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nIterations\n1e5\n0.75\n0.76\n0.77\n0.78\n0.79\nAccuracy\nKaggle DLRM\nDLRM\nDCN\n(a) SGD\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nIterations\n1e5\n0.770\n0.775\n0.780\n0.785\n0.790\n0.795\nAccuracy\nKaggle DLRM\nDLRM\nDCN\n(b) Adagrad\nFigure 5: Comparison of training (solid) and validation (dashed) accuracies of DLRM and DCN\n5.2\nModel Performance on a Single Socket/Device\nTo pro\ufb01le the performance of our model on a single socket device, we consider a sample model with\n8 categorical features and 512 continuous features. Each categorical feature is processed through\nan embedding table with 1M vectors, with vector dimension 64, while the continuous features are\nassembled into a vector of dimension 512. Let the bottom MLP have two layers, while the top\nMLP has four layers. We pro\ufb01le this model on a data set with 2048K randomly generated samples\norganized into 1K mini-batches10.\n(a) Caffe2\n(b) PyTorch\nFigure 6: Pro\ufb01ling of a sample DLRM on a single socket/device\nThis model implementation in Caffe2 runs in around 256 seconds on the CPU and 62 seconds on the\nGPU, with pro\ufb01ling of individual operators shown in Fig. 6. As expected, the majority of time is\nspent performing embedding lookups and fully connected layers. On the CPU, fully connected layers\ntake a signi\ufb01cant portion of the computation, while on the GPU they are almost negligible.\n6\nConclusion\nIn this paper, we have proposed and open-sourced a novel deep learning-based recommendation\nmodel that exploits categorical data. Although recommendation and personalization systems still\ndrive much practical success of deep learning within industry today, these networks continue to\nreceive little attention in the academic community. By providing a detailed description of a state-of-\nthe-art recommendation system and its open-source implementation, we hope to draw attention to the\nunique challenges that this class of networks present in an accessible way for the purpose of further\nalgorithmic experimentation, modeling, system co-design, and benchmarking.\n10 For instance, this con\ufb01guration can be achieved with the following command line arguments\n--arch-embedding-size=1000000-1000000-1000000-1000000-1000000-1000000-1000000-1000000\n--arch-sparse-feature-size=64 --arch-mlp-bot=512-512-64 --arch-mlp-top=1024-1024-1024-1\n--data-generation=random --mini-batch-size=2048 --num-batches=1000 --num-indices-per-lookup=100 [--use-gpu]\n[--enable-profiling]\n8\nAcknowledgments\nThe authors would like to acknowledge AI Systems Co-Design, Caffe2, PyTorch and AML team\nmembers for their help in reviewing this document.\nReferences\n[1] Christopher M. Bishop. Neural Networks for Pattern Recognition. The Oxford University Press, 1st edition,\n1995.\n[2] Keith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex Ingerman, Vladimir Ivanov,\nChlo\u00e9 Kiddon, Jakub Kone\u02c7cn\u00fd, Stefano Mazzocchi, Brendan McMahan, Timon Van Overveldt, David\nPetrou, Daniel Ramage, and Jason Roselander. Towards federated learning at scale: System design. In\nProc. 2nd Conference on Systems and Machine Learning (SysML), 2019.\n[3] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen\nAnderson, Greg Corrado, Wei Chai, Mustafa Ispir, Rohan Anil, Zakaria Haque, Lichan Hong, Vihan Jain,\nXiaobing Liu, and Hemal Shah. Wide & deep learning for recommender systems. In Proc. 1st Workshop\non Deep Learning for Recommender Systems, pages 7\u201310, 2016.\n[4] Corinna Cortes and Vladimir N. Vapnik. Support-vector networks. Machine Learning, 2:273\u2013297, 1995.\n[5] Luc Devroye, Laszlo Gyor\ufb01, and Gabor Lugosi. A Probabilistic Theory of Pattern Recognition. New York,\nSpringer-Verlag, 1996.\n[6] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and\nstochastic optimization. Journal of Machine Learning Research, 12:2121\u20132159, 2011.\n[7] Facebook. Collective communications library with various primitives for multi-machine training (gloo),\nhttps://github.com/facebookincubator/gloo.\n[8] Facebook. Caffe2, https://caffe2.ai, 2016.\n[9] Evgeny Frolov and Ivan Oseledets. Tensor methods and recommender systems. Wiley Interdisciplinary\nReviews: Data Mining and Knowledge Discovery, 7(3):e1201, 2017.\n[10] Craig Gentry. A fully homomorphic encryption scheme. PhD thesis, Stanford University, 2009.\n[11] Gene H. Golub and Charles F. Van Loan. Matrix Computations. The John Hopkins University Press, 3rd\nedition, 1996.\n[12] Ananth Grama, Vipin Kumar, Anshul Gupta, and George Karypis. Introduction to parallel computing.\nPearson Education, 2003.\n[13] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. DeepFM: a factorization-\nmachine based neural network for CTR prediction. arXiv preprint arXiv:1703.04247, 2017.\n[14] Rahman Hassan, Antony Harris, Nigel Topham, and Aris Efthymiou. Synthetic trace-driven simulation\nof cache memory. In Proc. 21st International Conference on Advanced Information Networking and\nApplications Workshops (AINAW\u201907), 2007.\n[15] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. Neural collaborative\n\ufb01ltering. In Proc. 26th Int. Conf. World Wide Web, pages 173\u2013182, 2017.\n[16] Sylvain Jeaugey. Nccl 2.0, 2017.\n[17] Yehuda Koren, Robert Bell, and Chris Volinsky. Matrix factorization techniques for recommender systems.\nComputer, (8):30\u201337, 2009.\n[18] Jianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, Zhongxia Chen, Xing Xie, and Guangzhong Sun. xDeepFM:\nCombining explicit and implicit feature interactions for recommender systems. In Proc. of the 24th ACM\nSIGKDD International Conference on Knowledge Discovery & Data Mining, pages 1754\u20131763. ACM,\n2018.\n[19] MLPerf. https://mlperf.org/.\n[20] Maxim Naumov. On the dimensionality of embeddings for sparse features and data. In arXiv preprint\narXiv:1901.02103, 2019.\n[21] Xia Ning, Christian Desrosiers, and George Karypis. A comprehensive survey of neighborhood-based\nrecommendation methods. In Recommender Systems Handbook, 2015.\n[22] Pandora. Music genome project https://www.pandora.com/about/mgp.\n[23] Adam Paszke, Sam Gross, Soumith Chintala, and Gregory Chanan. PyTorch: Tensors and dynamic neural\nnetworks in python with strong GPU acceleration https://pytorch.org/, 2017.\n[24] Steffen Rendle. Factorization machines. In Proc. 2010 IEEE International Conference on Data Mining,\npages 995\u20131000, 2010.\n9\n[25] Suvash Sedhain, Aditya Krishna Menon, Scott Sanner, and Lexing Xie. Autorec: Autoencoders meet\ncollaborative \ufb01ltering. In Proc. 24th Int. Conf. World Wide Web, pages 111\u2013112, 2015.\n[26] Strother H. Walker and David B. Duncan. Estimation of the probability of an event as a function of several\nindependent variables. Biometrika, 54:167\u2013178, 1967.\n[27] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. Deep & cross network for ad click predictions. In\nProc. ADKDD, page 12, 2017.\n[28] Guorui Zhou, Na Mou, Ying Fan, Qi Pi, Weijie Bian, Chang Zhou, Xiaoqiang Zhu, and Kun Gai. Deep\ninterest evolution network for click-through rate prediction. arXiv preprint arXiv:1809.03672, 2018.\n[29] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li,\nand Kun Gai. Deep interest network for click-through rate prediction. In Proc. of the 24th ACM SIGKDD\nInternational Conference on Knowledge Discovery & Data Mining, pages 1059\u20131068. ACM, 2018.\n10\n",
        "context": "embedding vectors, yielding linear computational complexity.\n2.1.4\nMultilayer Perceptrons\nSimultaneously, much recent success in machine learning has been due to the rise of deep learning.\nsalient high-level components of the DLRM.\n2.1.1\nEmbeddings\nIn order to handle categorical data, embeddings\nmap each category to a dense representation in\nan abstract space. In particular, each embedding\nlookup may be interpreted as using a one-hot\nproduct of these embedding vectors yields a meaningful prediction of the subsequent rating, a key\nobservation to the design of factorization machines and DLRM.\n2.1.3\nFactorization Machine"
    },
    {
        "id": 10,
        "title": "Post-training 4-bit quantization on embedding tables",
        "author": [
            "H. Guan",
            "A. Malevich",
            "J. Yang",
            "J. Park",
            "H. Yuen"
        ],
        "year": "2019",
        "doi": "10.48550/arXiv.1911.02079",
        "in_text_citation": "[10]",
        "sentence": "By quantizing the embeddings to lower bit-width representations in DLRMs [9], such as 4-bit [10], [11], or performing tensor train decomposition [12], memory usage can be significantly reduced, making it more feasible to train and inference.",
        "abstract": "",
        "full_text": "",
        "context": null
    },
    {
        "id": 11,
        "title": "Dqrm: Deep quantized recommendation models",
        "author": [
            "Y. Zhou",
            "Z. Dong",
            "E. Chan",
            "D. Kalamkar",
            "D. Marculescu",
            "K. Keutzer"
        ],
        "year": "2024",
        "doi": "10.48550/arXiv.2410.20046",
        "in_text_citation": "[11]",
        "sentence": "By quantizing the embeddings to lower bit-width representations in DLRMs [9], such as 4-bit [10], [11], or performing tensor train decomposition [12], memory usage can be significantly reduced, making it more feasible to train and inference.",
        "abstract": "Large-scale recommendation models are currently the dominant workload for\nmany large Internet companies. These recommenders are characterized by massive\nembedding tables that are sparsely accessed by the index for user and item\nfeatures. The size of these 1TB+ tables imposes a severe memory bottleneck for\nthe training and inference of recommendation models. In this work, we propose a\nnovel recommendation framework that is small, powerful, and efficient to run\nand train, based on the state-of-the-art Deep Learning Recommendation Model\n(DLRM). The proposed framework makes inference more efficient on the cloud\nservers, explores the possibility of deploying powerful recommenders on smaller\nedge devices, and optimizes the workload of the communication overhead in\ndistributed training under the data parallelism settings. Specifically, we show\nthat quantization-aware training (QAT) can impose a strong regularization\neffect to mitigate the severe overfitting issues suffered by DLRMs.\nConsequently, we achieved INT4 quantization of DLRM models without any accuracy\ndrop. We further propose two techniques that improve and accelerate the\nconventional QAT workload specifically for the embedding tables in the\nrecommendation models. Furthermore, to achieve efficient training, we quantize\nthe gradients of the embedding tables into INT8 on top of the well-supported\nspecified sparsification. We show that combining gradient sparsification and\nquantization together significantly reduces the amount of communication.\nBriefly, DQRM models with INT4 can achieve 79.07% accuracy on Kaggle with 0.27\nGB model size, and 81.21% accuracy on the Terabyte dataset with 1.57 GB, which\neven outperform FP32 DLRMs that have much larger model sizes (2.16 GB on Kaggle\nand 12.58 on Terabyte).",
        "full_text": "DQRM: Deep Quantized Recommendation Models\nYang Zhou1, Zhen Dong2, Ellick Chan3, Dhiraj Kalamkar3, Diana Marculescu5, and Kurt\nKeutzer2\n1Carnegie Mellon University, yangzho6@andrew.cmu.edu\n2University of California, Berkeley, {zhendong, keutzer}@berkeley.edu\n3Intel, {ellick.chan, dhiraj.d.kalamkar}@intel.com\n5University of Texas at Austin, dianam@utexas.edu\nAbstract\nLarge-scale recommendation models are currently the dominant workload for many large In-\nternet companies. These recommenders are characterized by massive embedding tables that are\nsparsely accessed by the index for user and item features. The size of these 1TB+ tables imposes\na severe memory bottleneck for the training and inference of recommendation models. In this\nwork, we propose a novel recommendation framework that is small, powerful, and efficient to run\nand train, based on the state-of-the-art Deep Learning Recommendation Model (DLRM). The\nproposed framework makes inference more efficient on the cloud servers, explores the possibility\nof deploying powerful recommenders on smaller edge devices, and optimizes the workload of the\ncommunication overhead in distributed training under the data parallelism settings. Specifically,\nwe show that quantization-aware training (QAT) can impose a strong regularization effect to\nmitigate the severe overfitting issues suffered by DLRMs. Consequently, we achieved INT4 quan-\ntization of DLRM models without any accuracy drop. We further propose two techniques that\nimprove and accelerate the conventional QAT workload specifically for the embedding tables in\nthe recommendation models. Furthermore, to achieve efficient training, we quantize the gradi-\nents of the embedding tables into INT8 on top of the well-supported specified sparsification. We\nshow that combining gradient sparsification and quantization together significantly reduces the\namount of communication. Briefly, DQRM models with INT4 can achieve 79.07% accuracy on\nKaggle with 0.27 GB model size, and 81.21% accuracy on the Terabyte dataset with 1.57 GB,\nwhich even outperform FP32 DLRMs that have much larger model sizes (2.16 GB on Kaggle and\n12.58 on Terabyte). We open-sourced our implementation in DQRM code.\nKeywords - Quantization, Recommendation Systems, Efficient Distributed Training, Data Parallelism\n1\nIntroduction\nWith the widespread adoption of Internet services, personalization becomes a critical function of the services\nprovided by the current Internet giants.\nEvery user\u2019s unique taste and preferences need to be catered\nto. With billions of internet users today, the need for recommendation models is more crucial than ever.\nAccording to [Gupta et al.(2019)], over 79% of the entire Meta\u2019s cloud ML inference cycles are spent on the\ninference of various sizes of recommendation models. By Amdahl\u2019s law, a slight boost in recommendation\nmodel efficiency can yield a massive performance boost. On the other hand, if part of the inference workload\ncan be migrated to edge devices, Internet service providers can save precious cloud resources, while the users\ncan have less of their personal data sent to the cloud environment, which strengthens their personal data\nprivacy. To achieve less costly recommenders, in this work, we propose a deep quantized recommendation\nmodel (DQRM) framework, where the models are both more efficient on the cloud environment, and are\npossible to fit on edge devices. Moreover, to address the necessity for periodic retraining in recommenders,\nthe proposed framework is optimized for model training in the cloud-distributed environment.\n1\narXiv:2410.20046v1  [cs.IR]  26 Oct 2024\nBot\nMLP\nEMB\n#1\nEMB\n#2\nEMB\n#26\n\u2026\nDense Features\nSparse Features\nInteraction Layer \nTop MLP\nOutput\n(a)\nBreakdown of Single Node Training Latency \n(b)\n(c) \nLocal Gradient Quantization (8-bit)\nLocal Accumulation in High Precision\n\u2026\nLocal Gradient Sparsification (Specified)\nGlobal Gradient Allreduce\nLocal Optimizer Step\nNode 1\nNode 2\nNode n\nFigure 1: (a) shows the state-of-the-art large-scale recommendation model architecture. The model contains\ntwo types of layers: Embedding tables and MLP layers.\n(b) Our framework builds on top of specified\nsparsity and adds quantization to achieve additional gradient compression ratio. (c) shows a breakdown of\nthe single-machine training time running DQRM in INT4; the majority of the training time on the GPU\nplatform (left) is spent on finding the quantization scales, and even more so on the CPU node (right).\nSpecifically, designing deep learning-based recommendation models is challenging, because of the necessity\nto both process dense and sparse inputs. Successful previous works [Naumov et al.(2019), Wang et al.(2017)]\nutilize massive embedding tables, each corresponding to a sparse feature category. Although embedding ta-\nbles are proven to learn sparse features well, they are usually massive, and easily take up many GB, and\nin some cases even TB in DRAM memory. The massive model size makes memory the prime bottleneck\nin both training and inference.\nThis severe problem motivates a huge effort in shrinking recommenda-\ntion model size [Ginart et al.(2021), Shi et al.(2020), Kang et al.(2020), Ko et al.(2021), Guan et al.(2019),\nDeng et al.(2021)]. In our work, we base our framework on the Deep Learning Recommendation Model\n(DLRM) [Naumov et al.(2019)], which is the state-of-the-art for Click-Through-Rate (CTR) ( [Zhu et al.(2020)])\nprediction (as in Figure 1 (a)), and we apply quantization and sparsification to enhance its efficiency.\nBased on experiments and analyses, we find that DLRM models suffer from severe overfitting issues, with\ntesting accuracy dropping steeply after the first one or two epochs of training. We propose to heavily quantize\nthe DLRM model into ultra-low INT4 precision, and we show that QAT can greatly reduce the memory and\ncomputational costs while imposing a strong regularization effect to benefit DLRM training. As a result, we\nachieve comparable or even better testing accuracy than the full-precision models. Besides, our experiments\nindicate that conventional QAT can be highly inefficient in recommendation model quantization. To deal\nwith this issue, we propose two techniques that reduce additional memory copies of large embedding tables\nduring QAT and avoid the costly traversal of large tensors in memory to compute quantization scales. Our\nproposed methods significantly reduce time costs.\nBesides the model size and inference costs, the training of recommendation models is also crucial in\nreal-world applications. Recommenders are usually trained in highly distributed cloud environments, where\nthe training time is dominated by heavy inter and intra-node communications overhead. In this work, we\npropose to apply communication sparsification and quantization jointly to shrink the workload. We show that\nusing well-supported specified sparsification of gradients can compress the communication by three orders of\nmagnitude. On top of that, we further quantize the communication from the massive embedding tables into\nINT8, shrinking the remaining size of communication by one-half during training, with a negligible accuracy\ndegradation on the large Criteo Terabyte dataset. We show the backpropagation of the proposed framework\nin Figure 1 (b). Our contributions are summarized as follows:\n\u2022 Section 3, we propose to apply ultra-low precision quantization to alleviate the overfitting of DLRMs\nmeanwhile achieving smaller model sizes;\n\u2022 Section 3.1 and 3.2, we introduce two novel techniques to improve the speed of QAT on recommendation\n2\nsystems;\n\u2022 Section 3.3, we enable more efficient training by jointly applying sparsification and quantization on\ncommunications;\n\u2022 Section 4, our DQRM models with INT4 achieve an 8\u00d7 reduction in model size, while obtaining\n79.071% accuracy (0.148% higher than DLRM FP32 baseline) on the Kaggle dataset, and 81.210%\naccuracy (0.045% higher than DLRM FP32 baseline) on the Terabyte dataset.\n2\nPrevious Works\nCompressing Large-scale Recommendation Models - DLRM [Naumov et al.(2019)] is a typical and\nhighly popular click-through-rate recommendation model designed and vastly deployed by Meta.\nMoti-\nvated by its importance, many previous works ( [Ginart et al.(2021)], [Shi et al.(2020)], [Yin et al.(2021)],\n[Desai et al.(2021)],\n[Wu et al.(2020)]) focus on compressing DLRM\u2019s model size. Since over 99% of the\nDLRM model size is occupied by the embedding tables, previous efforts focus on shrinking the embedding\ntables without lowering weight precision. Our work focuses on neural network quantization, which is orthog-\nonal and complementary to these works.\nPreviously, quantization [Gholami et al.(2021)] has been extensively studied on CNNs [Choi et al.(2018),\nEsser et al.(2019), Ni et al.(2020), Tailor et al.(2020), Yao et al.(2021), Xiao et al.(2023)] and Transform-\ners [Shen et al.(2019), Fan et al.(2019), Zadeh et al.(2020), Liu et al.(2023), Kim et al.(2023)] and success-\nfully applied to the Matrix Factorization and Neural Collaborative Filtering models [Kang et al.(2020),\nKo et al.(2021)]. Recently, some works have extended quantization to DLRM models but mainly focus on\nPost Training Quantization (PTQ).\n[Guan et al.(2019)] uses codebook quantization to quantize the em-\nbedding tables of DLRM models into INT4, while\n[Deng et al.(2021)] further quantizes the whole model\ninto 4-bit. Both works revealed that PTQ introduces accuracy degradation, which motivates other quanti-\nzation methods like Quantization-aware Training (QAT) to improve. Besides, low-precision training (LPT)\nof DLRM\n[Zhang et al.(2018), Xu et al.(2021), Li et al.(2022)] received attention.\n[Zhang et al.(2018)]\nand [Xu et al.(2021)] train CTR models using quantized FP16 weights, while [Li et al.(2022)] trains DCN\n[Wang et al.(2017)] using INT8 weights during low-precision training and compress models into 2-bit and\n4-bit. Different from the LPT works that achieve savings in training memory usage by allowing accuracy\ndrop, our work explores QAT on DLRM models and aims for ultra-low precision during inference without\naccuracy loss, while making QAT much more efficient for large-scale recommendation models specifically.\nEfficient Training of Recommendation Models - Training of large-scale CTR models is distributed\nin the real world, and communication bottlenecks the training time.\nMany previous works compress\nDLRM communication loads to speedup training time losslessly ( [Pumma and Vishnu(2021)]) or lossily\n( [Gupta et al.(2021), Yang et al.(2020)]) for a higher compression ratio. [Gupta et al.(2021)] improves con-\nventional Top-k sparsification on communication during DLRM\u2019s hybrid parallelism.\n[Yang et al.(2020)]\nquantizes the communication to ultra-low precision with tolerable accuracy drop through a novel error com-\npensation scheme. Previously, [Renggli et al.(2018)] combines sparsification and quantization on distributed\ncommunication for CNN and ASR models. However, the merit of combining the two is not generalizable to\ndifferent model architectures. In this work, we look into these techniques\u2019 effects on different parts of DQRM\nunder the DP environment and combine specified sparsification and quantization together to further benefit\nDQRM training.\n3\nMethodology\n3.1\nReducing Memory of Unused Weights\nDLRM-style recommenders have over 99% of their model size occupied by embedding tables, unlike other pop-\nular architectures like CNNs or transformers. Because of these giant embedding tables, previous works [Ko et al.(2021),\nGinart et al.(2021), Erdogan et al.(2024)] have shown that training large-scale recommendation models is\n3\nLarge \nEmbedding \nTables In FP32\nPer table \nMin, Max\nQuantize\n+\nOutput of the \nembedding table \nScale\nCopy of the embedding table \nScale\nWhole table Min, Max\nQuantize\nCopy of quantized rows needed \n+\nOutput of the \nembedding table \nIndexing \ninto the \ntable first\n(a)\n(b)\n\ud835\udc46= \ud835\udefd\u2212\ud835\udefc\n2\ud835\udc4f\u22121\n\ud835\udc44\ud835\udc5f= \ud835\udc3c\ud835\udc5b\ud835\udc61(\ud835\udc5f\n\ud835\udc46)\nFigure 2: (a) Conventional QAT method, where the entire set of weights of the embeddings are copied and\nquantized. As embedding accesses are very sparse, this method is wasteful as it processes unused entries and\nworsens the memory bottleneck in QAT. (b) Our method to avoid the massive copy is by first performing\nthe functional part of each table and then performing copying and quantization. In this way, only part of\nthe weights that are actually used is copied and quantized, utilizing the memory more efficiently (Figure\nbest viewed in color).\nmemory-bound instead of compute-bound. The memory bottleneck problem for DLRMs is further exacer-\nbated if Quantization-aware training (QAT) is naively applied.\nAs shown in Figure\n2 (a), in naive QAT, the entire weight tensor of convolutional or MLP layers is\nquantized since it is used in the subsequent computation. This requires two copies of the tensors to be\nstored: a low-precision and a full-precision shadow copy. However, as shown in Figure 2 (b), since not all\nembedding weights contribute to subsequent computation, copying the entire embedding table in memory is\nunnecessary during quantization.\nTherefore, our solution is to run the embedding layer first and retrieve only the necessary sparse embed-\nding vectors to quantize, so only used embedding vectors have low-precision copies stored. This approach\nensures that only active embedding table entries are copied throughout the iteration, which is usually more\nthan three magnitudes smaller than the total number of embedding vectors in the tables on average depending\non the training batch size.\n3.2\nPeriodic Update to Find Quantization Scale\nNaive QAT poses more challenges to DLRM quantization. During the quantization process, the first step is\nusually finding the quantization scale. Given the large embedding table size, this step is extremely costly as\nthe program needs to traverse through an enormous chunk of contiguous memory. We run a breakdown of\nQAT training time shown in Figure 1 (c) under the single-node training setting for both GPU (training on\nKaggle dataset) and CPU (training on Terabyte dataset). 1\nFor reference, without quantization, backward\npropagation workload dominates total single-node GPU training time: 5.8 ms out of 8.5 ms total. When QAT\nis applied, backprop (the yellow portion) no longer dominates, as forward propagation becomes significantly\nlonger (indicated by the rest of the pillars). The primary reason for this increase in time is the operation\nof finding the scale (shown by the blue portion). Finding the scale of large embedding tables occupies more\nthan one-third of the entire training time (left pillar) for GPU on Kaggle and dominates (over 97%) the\nentire training time for training on single-node CPU on Terabyte (right pillar). The problem of large tensor\ntraversal magnifies specifically on CPUs. 2\nWe found that periodic updates of the quantization scale effectively eliminate the latency of founding\nscales. These updates amortize the step\u2019s overhead over hundreds or thousands of iterations without hurting\nmodel convergence. We provide empirical results in Section 4 and in-depth ablation studies in Appendix\nA.1 to show that periodic updates can even boost model accuracy.\n1More experiment details can be found in Appendix B.2.\n2We also run single-node GPU training on Terabyte and find scale operation barely surpasses 50%.\n4\nTable 1: DLRM model architecture\nconfigurations\nModel\nKaggle\nTerabyte\nSpecifications\n# Embed Tables\n26\n26\nMax # Row in Tables\n10131227\n9994101\nEmbed Feature Size\n16\n64\nBot MLP Arch\n13-512-256-64-16 13-512-256-64\nTop MLP Arch\n512-256-1\n512-512-256-1\nTable 2: DLRM Embedding Tables\nQuantization, accuracies evaluated on the\nKaggle Dataset\nWeight\nTesting\nbitwidth\nAccuracy\nROC AUC\nFP32\n78.923%\n0.8047\nINT16\n78.928% (+0.005%)\n0.8046 (-0.0001)\nINT8\n78.985% (+0.062%)\n0.8054 (+0.0007)\nINT4\n79.092% (+0.169%) 0.8073 (+0.0026)\n3.3\nDistributed Training with Gradient Quantization with only MLP Error\nCompensation\nWe package the entire modified quantized model for DLRM into the Deep Quantized Recommendation Model\n(DQRM). To make DQRM more efficient and competitive in training time compared with normal training,\nwe further optimize the communication workload that occurs in every iteration.\nWe break down multi-node distributed data parallelism training of DQRM on both GPUs and CPUs\nin Appendix B.2. We found that although PyTorch\u2019s built-in specified sparsity greatly reduces all-reduce\ncommunication message load, all-reduce gradient communication after every iteration still dominates the\ntraining time. The observation motivates us to compress communication further. We explore quantizing\nall gradients into fixed-point INT8 format on top of the applied significantly specified sparsity. However,\nnaively doing so is challenging. Ablation studies presented in Appendix A.2 show that naively quantizing the\ngradient into INT8 hurts model convergence significantly. Although [Yang et al.(2020)]\u2019s error compensation\nscheme can prevent accuracy degradation, naively performing error compensation to all parts of the weights\ncan also be highly inefficient due to the enormous error buffer needed for the large embedding tables, as\nadding large error buffers will greatly increase the model size stored in memory during training.\nWe identify surprisingly that gradients in MLP layers are more sensitive to quantization than those\nin embedding tables while embedding tables are much more robust when specified gradients are heavily\nquantized, as shown empirically in Appendix A.2. Therefore, we choose to only compensate the MLP layers\nfor gradient quantization. We achieve reasonable accuracy degradation while compressing the communication\nmessage size by roughly 4\u00d7 on top of the already heavy and well-supported gradient sparsification added to\nembedding tables. Detailed experimental settings, results, and evaluations are presented in Section 4.3.\n4\nExperimental Results\nIn this section, we present results evaluating DQRM on two popular datasets for CTR: the Criteo Kaggle\nDisplay Advertising Challenge Dataset (shortened below as the Kaggle dataset) and the Criteo Terabyte\nDataset (shortened as the Terabyte dataset). We followed [Naumov et al.(2019)]\u2019s mlperf DLRM optimal\nconfigurations for each dataset as summarized in Table 2.\n4.1\nQuantization of Embedding Tables\nEmbedding tables occupy 99% of DLRM, which motivates heavy model compression of embedding tables.\nWe used one Nvidia A5000 GPU to study the effect of quantization on the embedding tables.\nDiffer-\nent bit widths for embedding table quantization are used: INT16, INT8, INT4.\nUnlike previous works\n[Guan et al.(2019), Deng et al.(2021)] that utilize a row-wise scheme for embedding table quantization, we\nquantize the embedding table with a per-table scheme, as it reduces the number of FP32 scales stored and\nretrieved every iteration.\nFigure 3(a) illustrates testing accuracy curves for various quantization bit widths. DLRM single precision\n(blue) overfits after the second epoch, causing a steep accuracy drop, which is a common issue in large-scale\nCTR models as recently explored in [Zhang et al.(2022)]. INT16 (orange) and INT8 (grey) follow similar\n5\nFigure 3: (a) shows the effect of using different QAT bit widths on quantizing embedding tables in DLRM for\nfive epochs of training (epochs are separated by the black dashed lines in all figures). QAT in uniform 4-bit\novercomes the severe overfitting suffered by the original DLRM training and leads to significantly higher\ntesting accuracy over five epochs of training. (b) shows the comparison between DQRM 4-bit compared to\nnormal training on the Terabyte dataset; DQRM, with a significantly smaller model size, achieves on-par test\naccuracy as DLRM FP32 model by better overcoming the overfitting problem. (c) shows that the training\nloss (orange curve) for normal training starts decreasing drastically in the third epoch, right where the\noverfitting occurs. In (d), the training loss curve for 4-bit DQRM decreases stably throughout five epochs\nof training.\noverfitting patterns. However, INT4 (green) converges slower but overcomes overfitting, increasing accuracy\nover five epochs. In supplementary materials, we empirically show that INT4 convergence lasts up to 16\nepochs.\nWe also found that INT2 prevents overfitting like INT4, although it incurs a greater accuracy\ndegradation.\nWe compare the performance after embedding table quantization in Table 2. Uniform INT4 quantization\noutperforms the original model in testing accuracy by roughly 0.15% and ROC AUC score by 0.0047 and\nachieving 8\u00d7 reduction in embedding table size. The accuracy improvement is significant in the Kaggle\ndataset. We studied the weight distribution of the training of FP32 weights, INT8, and INT4. We found\nthat the range of distribution for the single-precision weights constantly shifted outwards, while the INT8\nweights closely followed this trend. In contrast, INT4 only loosely captures the trend, changing the entire\nweight distribution much slower compared to higher precision weights. We hypothesize that such observation\nis linked to DQRM INT4 strong regularization ability. Details are in Appendix A.5.\n4.2\nQuantization of the Whole Model\nDLRM models have two components: Embedding tables and MLP layers. Compared with the embedding\ntable, we observe that MLP layers are more sensitive to quantization, aligning with\n[Deng et al.(2021),\nZhao et al.(2022)]. Channel-wise quantization of MLP layers performs better, as it has hardly any accuracy\ndrop from the single-precision MLP, while INT4 matrix-wise MLP quantization badly converges. Additional\nablation studies are presented in Appendix A.3.\nWe evaluate DQRM INT4 on both the Kaggle and Terabyte datasets. We used the same experiment\nplatforms for the Kaggle model as in Section 4.1. However, to meet the demanding resource required by the\nTerabyte models, we use the Intel Academic Compute Environment (ACE) CPU clusters and specifically\nIntel(R) Xeon(R) Platinum 8280 CPUs for model training and inference.\nThe quantized models are all\ntrained in DQRM for five epochs till convergence.\nFigure 3 (b), (c), and (d) display training loss and testing accuracy curves for the Terabyte dataset. In\n(b), the original model testing accuracy (blue) overfits at the 2nd epoch, while DQRM INT4 (orange) steadily\nrises and avoids overfitting. Comparing DLRM (c) and DQRM INT4 (d), the latter demonstrates better\nregularization, with a consistent decrease in loss (orange curve in (d)) and a plateau in testing accuracy\n(blue curve in (d)). In contrast, DLRM\u2019s accuracy (blue curve in (c)) and training loss (orange curve in (c))\ncrash after the second epoch.\nWe report single-node training and testing performance in Table 3. We compare DQRM INT4 against\n6\nTable 3: DQRM 4-bit quantization results evaluated on Kaggle and Criteo datasets\n(a) 4-bit quantization for DLRM on Kaggle\nQuantization\nModel\nModel Size\nTraining Loss\nTraining\nTesting\nSettings\nBit Width\ntime/it\nAccuracy\nROC AUC\nBaseline\nFP32\n2.161 GB\n0.304\n7 ms\n78.923%\n0.8047\nVanilla PTQ\nINT4\n0.270 GB\n-\n-\n76.571%\n0.7675\nPACT* [Choi et al.(2018)]\nINT4\n0.270 GB\nCannot Converge\n(MLP in FP32)\n0.271 GB\n0.303\n69 ms\n78.858%\n0.8040\nLSQ [Esser et al.(2019)]\nINT4\n0.270 GB\n0.350\n25 ms\n78.972%\n0.8051\n(MLP in FP32)\n0.271 GB\n0.352\n21 ms\n78.987%\n0.8059\nHAWQ [Dong et al.(2019)]\nINT4\n0.270 GB\n0.437\n31 ms\n79.040%\n0.8064\n(MLP in FP32)\n0.271 GB\n0.436\n27 ms\n79.070%\n0.8075\nDQRM (Ours)\nINT4\n0.270 GB\n0.437\n22 ms\n79.071%\n0.8073\n(MLP in FP32)\n0.271 GB\n0.436\n20 ms\n79.092%\n0.8073\n(b) 4-bit quantization for DLRM on Terabyte\nQuantization\nModel\nModel\nTraining Loss Training time/it\nTesting\nSettings\nBit Width\nSize\nAccuracy ROC AUC\nBaseline\nFP32\n12.575 GB\n0.347071\n19 ms\n81.165%\n0.8004\nVanilla PTQ\nINT4\n1.572 GB\n-\n-\n78.681%\n0.7283\nPACT* [Esser et al.(2019)]\nINT4\n1.572 GB\nCannot Finish >1000 ms/it\nHAWQ [Dong et al.(2019)]\nINT4\n1.572 GB\nCannot Finish >1000 ms/it\nLSQ [Esser et al.(2019)]\nINT4\n1.572 GB\n0.350\n42 ms\n81.134%\n0.7996\n(MLP in FP32)\n1.572 GB\n0.356\n42 ms\n81.127%\n0.7998\nDQRM (Ours)\nINT4\n1.572 GB\n0.409774\n29 ms\n81.210%\n0.8015\n(MLP in FP32)\n1.572 GB\n0.412\n29 ms\n81.200%\n0.8010\nPTQ and prior popular QAT works [Esser et al.(2019), Choi et al.(2018), Dong et al.(2019)] that achieve\nstrong INT4 results on CNN. Unfortunately,\n[Deng et al.(2021)] does not open-source their INT4 PTQ\nimplementation, so we apply vanilla PTQ using our quantization method with channel-wise quantization for\nMLP. Also, we implemented [Esser et al.(2019), Choi et al.(2018)] ourselves on DLRM. Table 3 (a) shows\nthe experimental results on the Kaggle dataset using GPU. Using an update period of 200, DQRM achieves\nboth a higher testing accuracy (79.071%) and a higher testing ROC AUC score (0.8073) than DLRM in\nFP32. On the Terabyte dataset in (b), by using an update period of 1000, DQRM INT4 achieves slightly\nhigher than FP32 baseline test accuracy (0.45%) and testing ROC AUC (0.0011). The vanilla PTQ incurs\na significant accuracy drop in both datasets.\nInterestingly, prior QAT works exhibit their inefficiencies.\nPACT [Choi et al.(2018)] and LSQ [Esser et al.(2019)] do not avoid overfitting completely like DQRM and,\nthus, achieve lower testing accuracy on both datasets. HAWQ [Dong et al.(2019)] cannot effectively eliminate\nunnecessary memory traversal, making it consistently slower than DQRM in training time and even failing\nto finish on Terabyte.\n4.3\nAdding Gradient Quantization on Specified Sparsity\nOur experiments are based on synchronous data parallelism (DP). However, we argue that the gradient com-\npression techniques presented are directly applicable to hybrid parallelism settings as in [Naumov et al.(2019)].\nMoreover, the only difference between the data parallelism and the DLRM\u2019s hybrid parallelism is the all2all\nround of communication, and it can also be directly benefited from DQRM INT with message load shrunk\nby 8\u00d7 directly. Due to PyTorch DistributedDataParallel library limitations, we customized and open-\nsourced our own Data Parallel library implementing gradient quantization before allreduce. Appendix B.1\n7\nTable 4: Communications compression for Distributed Data Parallelism training among four nodes or GPUs,\nthe baseline used here has forward prop weights in 4-Bit\nModel\nTraining\nCommunication\nCommunication\nLatency Training\nTesting\nSettings\nPlatforms\nCompression settings\nOverhead per iter per iter\nLoss\nACC\nROC AUC\nKaggle\n4X\ngrad uncompressed\n2.161 GB\n>1000 ms\n0.436\n78.897%1\n0.8035\nNvidia A5000\n+ EMB gradient sparse2\n2.010 MB\n61 ms\n0.436\n78.897%\n0.8035\nGPUs\n+ INT8 grad Quantization\n0.509 MB\n110 ms3\n0.442\n78.840%\n0.8023\nTerabyte\n2X (2 processes)\ngrad uncompressed\n12.575 GB\n>1000 ms\n0.412\n81.156%\n0.7997\nIntel(R) Xeon(R)\n+ EMB grad sparse\n6.756 MB\n210 ms\n0.412\n81.156%\n0.7997\nPlatinum 8280 CPU\n+ INT8 grad Quantization\n1.732 MB\n225 ms\n0.414\n81.035%\n0.7960\naData parallelism consistently lowers the test accuracy in all settings compared with single-node training.\nbSparsification used is specified sparsity which is a lossless compression for embedding tables so the testing accuracy is\nexactly the same as uncompressed case.\ncPyTorch sparse tensor allreduce library doesn\u2019t support low-precision arithmetic, without further system-level effort in\nlow-precision optimization, the latency per iteration increases purely from the quantization overhead per iteration.\ndetails the implementation. Our approach supports multi-GPU and CPU node platforms, with gradient\ncompression experimental results in Table 4.\nFor experiments on the Kaggle dataset, we utilized four NVIDIA A5000 GPUs. For the Terabyte dataset,\nwe ran on two Intel(R) Xeon(R) Platinum 8280 CPU nodes. Naively, because of the giant model size, when\nthe gradient is completely uncompressed, the overhead of gradient communication is up to 2.1 GB per\niteration. PyTorch has built-in support to exploit specified sparsity in the EmbeddingBag modules which is\nvery powerful in the allreduce in DP and can compress 2.1 GB to 2 MB. Moreover, using specified sparsity\nis lossless and has no impact on model training performance.\nSurprisingly, after the specified sparsity is applied, the MLP gradients become significant among the\ntotal gradient left. After careful profiling, they occupy around 95% and 45% in size among the remaining\ngradients for training in DP on Kaggle and Terabyte respectively. We further add error compensation to the\nMLP layers to reduce accuracy degradation. PyTorch doesn\u2019t support low-precision sparse allreduce well.\nWith limited backend support and significant communication overhead caused by sparse tensor coalescing,\nachieving speedup from communication reduction remains difficult. With in-depth system-level optimization\nfor low-precision sparse allreduce and gradient quantization workload, the training time cost reduction of\ndistributed DQRM can be realistically fulfilled, but such an endeavor is outside the scope of our current work.\nNevertheless, we showed empirically that adding gradient quantization only introduces a trivial decrease\nof 0.057% in the testing accuracy and 0.0012 in the ROC AUC score despite a significant reduction in\ncommunication size for the Kaggle dataset.\nSimilarly, gradient compression of the embedding tables introduces insignificant accuracy loss with\nroughly 0.1% for testing accuracy and less than 0.004 for testing ROC AUC. We detail more evaluations of\nMLP gradient sensitivity for quantization and the effect of error compensation in Appendix A.2.\n5\nConclusion\nIn this work, we propose a systematic quantization framework DQRM for large-scale recommendation models.\nSpecifically, we discover that the DLRM model suffers severely from the overfitting problem. We show that\nultra-low precision quantization can help overcome the strong overfitting and better utilize the training\ndataset, which eventually leads to higher test accuracy. We observe that conventional QAT is troublesome\nin training large-scale recommendation models and we propose two techniques that significantly alleviate\nthe issue. Besides, to further optimize DQRM under the distributed environment, we combine specified\nsparsification and quantization together to compress communications. Our framework is intensively evaluated\non the published dataset Kaggle and Terabyte, where we outperform the full-precision DLRM baselines while\nachieving an 8\u00d7 reduction of model size.\n8\nReferences\n[Choi et al.(2018)] Jungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen Chuang, Vijayalak-\nshmi Srinivasan, and Kailash Gopalakrishnan. 2018. Pact: Parameterized clipping activation for quan-\ntized neural networks. arXiv preprint arXiv:1805.06085 (2018).\n[Deng et al.(2021)] Zhaoxia Deng, Jongsoo Park, Ping Tak Peter Tang, Haixin Liu, Jie Yang, Hector Yuen,\nJianyu Huang, Daya Khudia, Xiaohan Wei, Ellie Wen, et al. 2021. Low-precision hardware architectures\nmeet recommendation model inference at scale. IEEE Micro 41, 5 (2021), 93\u2013100.\n[Desai et al.(2021)] Aditya Desai, Li Chou, and Anshumali Shrivastava. 2021. Random Offset Block Em-\nbedding Array (ROBE) for CriteoTB Benchmark MLPerf DLRM Model: 1000 times Compression and\n3.1 times Faster Inference. arXiv preprint arXiv:2108.02191 (2021).\n[Dong et al.(2020)] Zhen Dong, Zhewei Yao, Daiyaan Arfeen, Amir Gholami, Michael W Mahoney, and Kurt\nKeutzer. 2020. Hawq-v2: Hessian aware trace-weighted quantization of neural networks. Advances in\nneural information processing systems 33 (2020), 18518\u201318529.\n[Dong et al.(2019)] Zhen Dong, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. 2019.\nHawq:\nHessian aware quantization of neural networks with mixed-precision. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision. 293\u2013302.\n[Erdogan et al.(2024)] Lutfi Eren Erdogan, Vijay Anand Raghava Kanakagiri, Kurt Keutzer, and Zhen\nDong. 2024. Stochastic Communication Avoidance for Recommendation Systems. (2024).\n[Esser et al.(2019)] Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and\nDharmendra S Modha. 2019. Learned step size quantization. arXiv preprint arXiv:1902.08153 (2019).\n[Fan et al.(2019)] Angela Fan, Edouard Grave, and Armand Joulin. 2019. Reducing transformer depth on\ndemand with structured dropout. arXiv preprint arXiv:1909.11556 (2019).\n[Gholami et al.(2021)] Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W. Mahoney, and Kurt\nKeutzer. 2021. A Survey of Quantization Methods for Efficient Neural Network Inference.\nhttps:\n//doi.org/10.48550/ARXIV.2103.13630\n[Ginart et al.(2021)] Antonio A Ginart, Maxim Naumov, Dheevatsa Mudigere, Jiyan Yang, and James Zou.\n2021. Mixed dimension embeddings with application to memory-efficient recommendation systems. In\n2021 IEEE International Symposium on Information Theory (ISIT). IEEE, 2786\u20132791.\n[Guan et al.(2019)] Hui Guan, Andrey Malevich, Jiyan Yang, Jongsoo Park, and Hector Yuen. 2019. Post-\ntraining 4-bit quantization on embedding tables. arXiv preprint arXiv:1911.02079 (2019).\n[Gupta et al.(2019)] Udit Gupta, Xiaodong Wang, Maxim Naumov, Carole-Jean Wu, Brandon Reagen,\nDavid Brooks, Bradford Cottel, Kim M. Hazelwood, Bill Jia, Hsien-Hsin S. Lee, Andrey Malevich,\nDheevatsa Mudigere, Mikhail Smelyanskiy, Liang Xiong, and Xuan Zhang. 2019. The Architectural\nImplications of Facebook\u2019s DNN-based Personalized Recommendation. CoRR abs/1906.03109 (2019).\nhttps://arxiv.org/abs/1906.03109\n[Gupta et al.(2021)] Vipul Gupta, Dhruv Choudhary, Peter Tang, Xiaohan Wei, Xing Wang, Yuzhen Huang,\nArun Kejariwal, Kannan Ramchandran, and Michael W Mahoney. 2021. Training recommender systems\nat scale: Communication-efficient model and data parallelism. In Proceedings of the 27th ACM SIGKDD\nConference on Knowledge Discovery & Data Mining. 2928\u20132936.\n[Kang et al.(2020)] Wang-Cheng Kang, Derek Zhiyuan Cheng, Ting Chen, Xinyang Yi, Dong Lin, Lichan\nHong, and Ed H Chi. 2020. Learning multi-granular quantized embeddings for large-vocab categorical\nfeatures in recommender systems. In Companion Proceedings of the Web Conference 2020. 562\u2013566.\n[Kim et al.(2023)] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen,\nMichael W Mahoney, and Kurt Keutzer. 2023.\nSqueezellm: Dense-and-sparse quantization.\narXiv\npreprint arXiv:2306.07629 (2023).\n9\n[Ko et al.(2021)] Yunyong Ko, Jae-Seo Yu, Hong-Kyun Bae, Yongjun Park, Dongwon Lee, and Sang-Wook\nKim. 2021. MASCOT: A Quantization Framework for Efficient Matrix Factorization in Recommender\nSystems. In 2021 IEEE International Conference on Data Mining (ICDM). IEEE, 290\u2013299.\n[Li et al.(2022)] Shiwei Li, Huifeng Guo, Lu Hou, Wei Zhang, Xing Tang, Ruiming Tang, Rui Zhang, and\nRuixuan Li. 2022. Adaptive Low-Precision Training for Embeddings in Click-Through Rate Prediction.\narXiv preprint arXiv:2212.05735 (2022).\n[Li et al.(2023)] Zhikai Li, Xiaoxuan Liu, Banghua Zhu, Zhen Dong, Qingyi Gu, and Kurt Keutzer.\n2023.\nQFT: Quantized Full-parameter Tuning of LLMs with Affordable Resources.\narXiv preprint\narXiv:2310.07147 (2023).\n[Liu et al.(2023)] Yijiang Liu, Huanrui Yang, Zhen Dong, Kurt Keutzer, Li Du, and Shanghang Zhang.\n2023. Noisyquant: Noisy bias-enhanced post-training activation quantization for vision transformers. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 20321\u201320330.\n[Naumov et al.(2019)] Maxim Naumov,\nDheevatsa Mudigere,\nHao-Jun Michael Shi,\nJianyu Huang,\nNarayanan Sundaraman, Jongsoo Park, Xiaodong Wang, Udit Gupta, Carole-Jean Wu, Alisson G.\nAzzolini, Dmytro Dzhulgakov, Andrey Mallevich, Ilia Cherniavskii, Yinghai Lu, Raghuraman Kr-\nishnamoorthi, Ansha Yu, Volodymyr Kondratenko, Stephanie Pereira, Xianjie Chen, Wenlin Chen,\nVijay Rao, Bill Jia, Liang Xiong, and Misha Smelyanskiy. 2019.\nDeep Learning Recommendation\nModel for Personalization and Recommendation Systems.\nCoRR abs/1906.00091 (2019).\nhttps:\n//arxiv.org/abs/1906.00091\n[Ni et al.(2020)] Renkun Ni, Hong-min Chu, Oscar Casta\u02dcneda, Ping-yeh Chiang, Christoph Studer, and\nTom Goldstein. 2020. WrapNet: Neural Net Inference with Ultra-Low-Resolution Arithmetic.\nhttps:\n//doi.org/10.48550/ARXIV.2007.13242\n[Pumma and Vishnu(2021)] Sarunya Pumma and Abhinav Vishnu. 2021. Semantic-Aware Lossless Data\nCompression for Deep Learning Recommendation Model (DLRM). In 2021 IEEE/ACM Workshop on\nMachine Learning in High Performance Computing Environments (MLHPC). IEEE, 1\u20138.\n[Renggli et al.(2018)] Cedric Renggli, Saleh Ashkboos, Mehdi Aghagolzadeh, Dan Alistarh, and Torsten\nHoefler. 2018. SparCML: High-Performance Sparse Communication for Machine Learning.\nhttps:\n//doi.org/10.48550/ARXIV.1802.08021\n[Shang et al.(2023)] Yuzhang Shang, Zhihang Yuan, Qiang Wu, and Zhen Dong. 2023. Pb-llm: Partially\nbinarized large language models. arXiv preprint arXiv:2310.00034 (2023).\n[Shen et al.(2019)] Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W.\nMahoney, and Kurt Keutzer. 2019. Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT.\n(2019). https://doi.org/10.48550/ARXIV.1909.05840\n[Shi et al.(2020)] Hao-Jun Michael Shi, Dheevatsa Mudigere, Maxim Naumov, and Jiyan Yang. 2020. Com-\npositional embeddings using complementary partitions for memory-efficient recommendation systems.\nIn Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data\nMining. 165\u2013175.\n[Tailor et al.(2020)] Shyam A. Tailor, Javier Fernandez-Marques, and Nicholas D. Lane. 2020. Degree-Quant:\nQuantization-Aware Training for Graph Neural Networks. https://doi.org/10.48550/ARXIV.2008.\n05000\n[Wang et al.(2017)] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & cross network for\nad click predictions. In Proceedings of the ADKDD\u201917. 1\u20137.\n[Wu et al.(2020)] Xiaorui Wu, Hong Xu, Honglin Zhang, Huaming Chen, and Jian Wang. 2020.\nSaec:\nsimilarity-aware embedding compression in recommendation systems. In Proceedings of the 11th ACM\nSIGOPS Asia-Pacific Workshop on Systems. 82\u201389.\n10\n[Xiao et al.(2023)] Lirui Xiao, Huanrui Yang, Zhen Dong, Kurt Keutzer, Li Du, and Shanghang Zhang.\n2023. Csq: Growing mixed-precision quantization scheme with bi-level continuous sparsification. In\n2023 60th ACM/IEEE Design Automation Conference (DAC). IEEE, 1\u20136.\n[Xu et al.(2021)] Zhiqiang Xu, Dong Li, Weijie Zhao, Xing Shen, Tianbo Huang, Xiaoyun Li, and Ping Li.\n2021. Agile and accurate CTR prediction model training for massive-scale online advertising systems.\nIn Proceedings of the 2021 international conference on management of data. 2404\u20132409.\n[Yang et al.(2020)] Jie Amy Yang, Jongsoo Park, Srinivas Sridharan, and Ping Tak Peter Tang. 2020. Train-\ning deep learning recommendation model with quantized collective communications. In Conference on\nKnowledge Discovery and Data Mining (KDD).\n[Yao et al.(2021)] Zhewei Yao, Zhen Dong, Zhangcheng Zheng, Amir Gholami, Jiali Yu, Eric Tan, Leyuan\nWang, Qijing Huang, Yida Wang, Michael Mahoney, et al. 2021. Hawq-v3: Dyadic neural network\nquantization. In International Conference on Machine Learning. PMLR, 11875\u201311886.\n[Yin et al.(2021)] Chunxing Yin, Bilge Acun, Carole-Jean Wu, and Xing Liu. 2021. Tt-rec: Tensor train\ncompression for deep learning recommendation models. Proceedings of Machine Learning and Systems\n3 (2021), 448\u2013462.\n[Zadeh et al.(2020)] Ali Hadi Zadeh, Isak Edo, Omar Mohamed Awad, and Andreas Moshovos. 2020. GOBO:\nQuantizing Attention-Based NLP Models for Low Latency and Energy Efficient Inference. In 2020\n53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO). IEEE.\nhttps:\n//doi.org/10.1109/micro50266.2020.00071\n[Zhang et al.(2018)] Jian Zhang, Jiyan Yang, and Hector Yuen. 2018. Training with low-precision embedding\ntables. In Systems for Machine Learning Workshop at NeurIPS, Vol. 2018.\n[Zhang et al.(2023)] Yifan Zhang, Zhen Dong, Huanrui Yang, Ming Lu, Cheng-Ching Tseng, Yuan Du, Kurt\nKeutzer, Li Du, and Shanghang Zhang. 2023. QD-BEV: quantization-aware view-guided distillation for\nmulti-view 3D object detection. In Proceedings of the IEEE/CVF International Conference on Computer\nVision. 3825\u20133835.\n[Zhang et al.(2022)] Zhao-Yu Zhang, Xiang-Rong Sheng, Yujing Zhang, Biye Jiang, Shuguang Han, Hongbo\nDeng, and Bo Zheng. 2022. Towards understanding the overfitting phenomenon of deep click-through\nrate prediction models. arXiv preprint arXiv:2209.06053 (2022).\n[Zhao et al.(2022)] Lingran Zhao, Zhen Dong, and Kurt Keutzer. 2022. Analysis of quantization on mlp-\nbased vision models. arXiv preprint arXiv:2209.06383 (2022).\n[Zhou et al.(2016)] Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. 2016.\nDorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients. arXiv\npreprint arXiv:1606.06160 (2016).\n[Zhu et al.(2020)] Jieming Zhu, Jinyang Liu, Shuai Yang, Qi Zhang, and Xiuqiang He. 2020. Fuxictr: An\nopen benchmark for click-through rate prediction. arXiv preprint arXiv:2009.05794 (2020).\n11\nAppendix\nA\nAblation Studies\nIn the ablation studies, we present extended studies on the parameters of the DQRM framework, including\ndifferent update period\u2019s impact on DQRM in Section A.1, different gradient quantization bit width\u2019s effect\non model convergence in Section A.2, different sensitivity to quantization among layers in DQRM in Section\nA.3, and the effect of QAT without retraining on DQRM. Then, we evaluate DQRM INT4 with the large\nnumber of epochs in training in Section A.4.\nA.1\nPeriodic update\nAs shown in the methodology section, the sheer size of embedding tables renders table traversal a bottleneck\nduring the forward pass. We propose to utilize periodic updates of the quantization scale of each embedding\ntable during quantization in order to amortize the huge latency of table lookup into multiple iterations.\nAmong the options, we found that the optimal period is different for training settings with different model\nsizes and batch sizes. The experimental results are presented in Table 5.\nTable 5: Evaluation of Periodic Update on Kaggle and Terabyte Datasets\nModel\nSettings Period\nLatency\nTesting\nper iter Accuracy ROC AUC\nKaggle\n1\n31 ms\n79.040%\n0.8064\n200\n22 ms\n79.071%\n0.8073\n500\n22 ms\n79.034%\n0.8067\nTerabyte\n1\n>1200 ms\n-\n-\n200\n58 ms\n81.159%\n0.7998\n500\n51 ms\n81.193%\n0.8009\n1000\n46 ms\n81.210%\n0.8015\nFor experiments running on the Kaggle dataset, we run on one Nvidia A5000 GPU. We again trained the\nmodel for 5 epochs. During DQRM, all weights are quantized into INT4. Training time per iteration decreases\nsignificantly, from over 31 ms to 22 ms when the quantization scale is updated once every 200 iterations\ncompared with every iteration. Also, along with decreased single-node training time, model convergence\neven slightly improved. We also test the update period of 500, and we found diminishing gains in training\ntime reduction and a slight decrease in testing performance.\nSimilar experiments are conducted on a single Intel Xeon platinum 8280 CPU on Terabyte. We found\nthat with the current ML framework, CPUs have much greater difficulties in traversing large embedding\ntables. A single iteration of training of the model under the Terabyte setting takes more than 1.2 seconds\nto finish, thus making it impractical to complete. When the quantization scale is periodically updated once\nevery 200 iterations, the training time per iteration dramatically improves to 57 ms per iteration. Due to the\ndifference in model size and training mini-batch size, we found that using a longer period of 500 iterations\nbrings further speedup in training time per iteration, dropping to 34 ms, while even slightly benefiting the\ntraining accuracy (0.03% compared with 200 iterations).\nWe hypothesize that the reason for the slight\naccuracy boost is that, QAT inherently has an unstably growing quantization scale, and periodic update of\nthe quantization scale helps stabilize training, which further helps the model converge.\nA.2\nDifferent Gradient Quantization Bit Width\nIn this Section, we present our findings related to the quantization of the gradients during distributed DP\ntraining for DQRM. All experiments presented in this Section assume that specified sparsity is enabled.\nFirstly, we study the effect of naive quantization of gradients on the model convergence. The experiments\nare run on a single NVIDIA M40 GPU under a simulated DP environment under the Kaggle dataset. More\n12\nFigure 4: (a) Testing accuracy of naively quantizing gradients into different bit widths.\nNaive gradient\nquantization leads to a significant accuracy drop. (b) MLP gradients are more sensitive to quantization. If\nonly quantizing the embedding table gradients into INT16, it will lead to less drop in accuracy compared to\nonly quantizing MLP gradients into INT16.\ndetails of the experiment platform setup can be found in B.1. Quantizing all the gradients naively introduces\nhuge accuracy loss and largely hurts the model convergence. In Figure 5 (a), the blue curve signifies the\nbaseline: FP32 gradients.\nThe orange curve is for INT16 gradients, while the green curve is for INT8\nquantization gradients. Both the INT16 and the INT8 quantization force the training to plateau earlier and\nconverge to a much lower testing accuracy.\nFollowing our first experiment and setup, we investigate quantizing just embedding table gradients or just\nMLP layers gradient but not both Figure 5 (b). We show that only quantizing embedding table gradients\nallows the model to continuously converge throughout the five epochs while quantizing the MLP layers\nsignificantly slows down the model convergence. Therefore, we decided to add Error Compensation only\nto MLP layers\u2019 gradients, as MLP layers\u2019 gradients are more sensitive to quantization. More experiments\nare presented in Table\n6 on the similar 4 Nvidia A5000 GPUs with our custom framework. We showed\nthat quantizing Embedding Table gradient into INT8 doesn\u2019t hurt test accuracy much, and MLP error\ncompensation largely prevents the large accuracy degradation from MLP gradient quantization.\nTable 6: Evaluation of MLP Quantization and Error Compensation\nSettings\nTesting Accuracy Testing ROC AUC\nbaseline\n78.897%\n0.8035\nOnly Embedding gradient in INT8\n78.858%\n0.8023\nEmbedding and MLP gradient in INT8\n78.608%\n0.7974\nEmbedding and MLP gradient in INT8 + MLP EC\n78.840%\n0.8023\nA.3\nQuantization of Different Part of Models\nIn DQRM training, we observe that different layers exhibit different sensitivities to quantization. To study\nthis, we utilize four NVIDIA M40 GPUs and conduct experiments on the Kaggle dataset under the dis-\ntributed DP settings. The experiment results are presented in Table 7. Compared with quantizing embed-\nding tables, MLP layers quantization cannot benefit from the diminishing overfitting effect. Instead, when\nquantizing MLP layers in the matrix-wise fashion into INT4 and quantizing activation during QAT, the\n13\nTable 7: Quantization Evaluation of Each Part of the Model\nSettings\nTesting Accuracy Testing ROC AUC\nBaseline\n78.718%\n0.8001\n+ Embedding Tables in 4-bit\n78.936%\n0.8040\n+ MLP in 4-bit matrix-wise\n78.830%\n0.8022\n+ MLP in 4-bit channelwise\n78.897%\n0.8035\n+ MLP in 8-bit channelwise\n78.950%\n0.8045\nDQRM framework fails to converge. When just quantizing the MLP layers in the matrix-wise fashion and\nwithout quantizing the activations, DQRM gives more than 0.1% testing accuracy drop, which is a lot for the\nKaggle dataset. In contrast, we analyze MLP quantization in a channel-wise fashion, specifically finding a\nscale for every row inside the linear layer weights. MLP channel-wise quantization outperforms matrix-wise,\nbringing the overall accuracy loss for the entire INT4 model to be under 0.04% and 0.0005 in testing ROC\nAUC compared to a single precision unquantized MLP model. We also provide the performance of quantizing\nthe MLP layers in INT8 channel-wise, it is on par with the single precision MLP in test accuracy and test\nROC AUC score.\nA.4\nQAT without Pre-training\nTable 8: Evaluation of QAT from scratch on Recommendation Models\nSettings\nTesting Accuracy Testing ROC AUC\nOne epoch pretraining + Four epochs of QAT\n78.926%\n0.8039\nFive epochs of QAT without pretraining\n78.936%\n0.8040\nPreviously, QAT has been used as a fine-tuning technique for the quantization of CNN [Dong et al.(2020),\nZhang et al.(2023)] and transformer models [Shang et al.(2023), Li et al.(2023)]. Usually, with pretraining\non the single-precision bit-width, the trained weights can reduce accuracy loss when fine-tuning on low-\nprecision settings. We examine such paradigms on the DLRM models. We train DLRM in QAT but only\nquantize the embedding tables into INT4 under the Kaggle dataset using four NVIDIA M40 GPUs under the\ndistributed DP settings. We compare DLRM with one epoch of pretraining in the single precision followed\nby four epochs of INT4 QAT with DLRM with INT4 QAT from scratch.\nWe plot the testing accuracy versus iteration curves in Figure 5 (a). The experiment results are presented\nin Table 8. In the diagram, the vertical dashed lines signify the boundary of each epoch. The blue curve\nis for QAT with pretraining, while the orange curve is without.\nAfter the first epoch, we can see that\nas the model transitions from single-precision to INT4 quantized data type, the blue curve drops slightly,\nwhich is expected. Further, in the mid-third epoch, QAT with pretraining (blue) reaches its peak and then\ndrops quickly afterward. QAT from scratch eventually has a slight testing accuracy edge over QAT with\npretraining. Also, in Figure\n5 (b), we plot the training loss across five epochs for two different settings.\nQAT with pretraining (blue) has training loss constantly below QAT from scratch. From here, we argue that\nQAT with pretraining on DLRM models does speed up the model convergence, taking less number of epochs\nto reach its peak. However, it suffers from earlier overfitting compared with QAT from scratch. Under our\nsettings, we observe that QAT from scratch slightly gains in testing accuracy of 0.01% with 0.0001 of testing\nROC AUC score.\nA.5\nHow Quantization Affects Weight Distribution\nTo provide more intuition of why INT4 quantization can provide stronger resilience toward overfitting. We\nlook into the weight distribution shift of normal unquantized training, the QAT in INT8 training, and the\nDQRM in INT4. We select Table 6 among the 26 tables from the DLRM Kaggle setting because Table 6 is\na medium size embedding table. We found that large tables usually have the majority of weights distributed\n14\n(a)\n(b)\nFigure 5: (a) Testing accuracy over the 5 epochs for 1 epoch of pretraining before 4 epochs of QAT and 5\nepochs of QAT without pretraining. Pretraining leads to faster overfitting, and QAT without pretraining\navoids overfitting and achieves better testing accuracy from 5 epochs of training. (b) Training loss over the\n5 epochs. Pretraining before QAT leads to a faster decrease in training loss in DLRM compared with QAT\nwithout pretraining.\naround 0. However, we found that the medium size embedding tables have a similar trend to large embedding\ntables. The model gradually shifted outwards from a uniform distribution probability mass function into a\nbell-shaped curve. The large embedding table has a trend similar to that, but still, after a few epochs, the\nmajority of weight still gathers around 0. In comparison, mid-sized embedding tables display a more obvious\nweight distribution shift, so we select table 6, a mid-sized embedding table among 26 tables.\nAcross the 5 epochs, we sampled four sections in time: before training, at the end of the second, the\nthird, and the fifth epoch. These four sections are aligned on the row. The contrast between rows is for the\ncontrast between different quantization bit-widths of the Quantization-aware training. The weight gradually\nmoves from a uniform initialization into a bell-shaped curve. Figure 6 contrasts quantized weights (orange,\nforeground) and unquantized weights (blue background) distribution. INT8 traces the unquantized weight\ndistribution much closer than INT4 quantization.\nThe general trend of weight distribution shift during normal DLRM training is to gradually shift outward,\nwhich can be noted by the contrast of the blue histogram between (b) the weight distribution after the second\nepoch and (d) after 5 epochs. We observed that the quantized weights (orange) exhibit a similar outward-\nshifting trend. DQRM INT4 quantized weights (orange) in (b) and (d) seemingly trace the unquantized\nweight poorly compared with INT8 weights in (c), having the majority of the weight distributed near the\norigin while having some portion of the weights located symmetrically around 0. In comparison, (c) shows\nINT8 can roughly cover the entire weight distribution neatly. We also observe that the FP32 copy of weights\nunder the quantization-aware training regardless of bitwidth are highly similar in distribution compared with\nnormal training. As shown in section 4.1, INT8 although converges slightly higher than the unquantized\nmodel, still follows a similar overfitting trend. We hypothesized that the INT4 quantization resilience towards\noverfitting comes from its large alienation to normal embedding table weight distribution shift. Also, we\nobserve that the INT4 model has the majority of weights close to zero while performing well, which provokes\nthe thought of whether high sparsity in large embedding tables exists and can be better utilized to tackle\noverfitting. We leave the question to explore later in the work.\n15\nEpoch 0 iter 0 (INT4) \nEpoch 1 iter 51200 (INT4) \nEpoch 2 iter 51200 (INT4)\nEpoch 4 iter 51200 (INT4)\nEpoch 0 iter 0 (INT8) \nEpoch 1 iter 51200 (INT8) \nEpoch 2 iter 51200 (INT8)\nEpoch 4 iter 51200 (INT8)\nFigure 6: Contrast between INT4 and INT8 quantization-aware training weight distribution shift. Orange\npillars are for the quantized weight distribution under QAT with low precision, while blue is for the unquan-\ntized weight under normal training. Figure 6 follows unquantized weight distribution plots from Figure 7\nand plots these blue pillars on the background of each subplot. The orange pillars in the foreground signify\nthe quantized weight distribution shift. The first row is for the distribution shift of the INT4 quantization\n(with four sampled sections: before training, after the second, the third, and the fifth epoch), while the\nsecond row is for the distribution shift of the INT8 quantization.\nA.6\nTraining DQRM for a Large Number of Epochs\nIn our main body, we observed that DQRM INT4 prevents overfitting through five training epochs. For\nreference, the baseline DLRM in FP32 overfits after one or two epochs. To what extent will DQRM INT4\nprevent overfits? Are five epochs enough? To what extent the testing accuracy will plateau? Motivated by\nthese questions, we run DQRM INT4 for an extended 20 epochs till it eventually overfits. We present the\ntesting accuracy curve in Figure 8 (a) contrasting DLRM in FP32 (orange curve) and DQRM INT4 (blue\ncurve). Below we also summarize the specific testing performance in Table 9. Training convergence reaches\nits peak at the 11th epoch. After that, it plateaus for 2 to 3 epochs and then goes down. Our experiment\nshows that 5 epochs aren\u2019t enough to make DQRM INT4 fully converged. However, still, considering spending\ntwice the amount of time for a slight accuracy advantage, we leave our results here for the user to decide\nwhich scenario they are willing to go for.\nTable 9: Test results by number of epochs\n# Epochs\nTest Accuracy\nTest ROC AUC\n1\n78.581%\n0.7967\n5\n78.949%\n0.8050\n10\n79.092%\n0.8073\nPeak (epoch 11)\n79.092%\n0.8076\n15\n78.743%\n0.8009\n20\n78.219%\n0.7499\n16\nFigure 7: The unquantized weight distribution shift across five epochs. The unquantized model starts overfit\nafter the first epoch\nA.7\nPrior QAT Techniques on DLRM\nIn this section, we try to address why different prior QAT methods underachieved in the DLRM model\nsettings. In the main body, we also compare DQRM against HAWQ. Since DQRM without all the tricks\nadded additionally was built on top of the mere quantization scheme of HAWQ. Therefore, HAWQ directly\nfaces the overwhelmingly significant memory usage and long training time for the Terabyte dataset to con-\nverge. What we want to discuss here is more focused on other QAT techniques such as PACT and LSQ.\nQuantization is key in building a mapping between quantized integer values and their original real values.\nPACT introduces the learned clipping range during activation quantization, while the weight quantization\nis completely reliant on DoReFa quantization. DoReFa uses the tanh function extensively, but empirically\nthe mapping is not comparable to uniform mapping implemented by the vast majority of quantization works\nthat follow, such as LSQ or HAWQ. Shown in Figure 8 (b), it converges the fastest among all techniques and\nsuffers from overfitting severely. LSQ on the other hand learns its clipping range during weight quantization.\nHowever, it is not as competitive as DQRM which uses static min max clipping range on both datasets. A\npotential explanation is that in A.5 of the supplemental materials, we present the weight distribution shift of\nthe DLRM model during training. The range of values is spreading out constantly throughout, unfortunately\nfor LSQ, the optimal clipping range might happen to be a moving objective, which hurts its performance.\nDQRM\u2019s static min-max clipping range might benefit from its stronger ability to adapt.\nB\nExperiment Platforms\nB.1\nDistributed Environment with Gradient Quantization\nIn the PyTorch DistributedDataParallelism library, during backward propagation, the error tensor\u2019s back-\nward() function handles both backpropagating gradients to local parameters together with the allreduce of\ngradient tensor across nodes. Therefore, after the powerful single line of code, gradients behind each pa-\nrameter tensor in the distributed data parallelism model have their corresponding gradient ready. However,\nthe quantization of the gradient has to happen between the local backpropagation and the global allreduce.\nTherefore, the distributed data parallelism package is not very handy.\nTo implement gradient quantization, we implement a stand-alone framework that performs the majority\nof distributed data parallelism workflow. To implement gradient quantization, we summarized our implemen-\ntation in Figure 9. We start by partitioning the training batch into pieces to be used by different machines.\nAfter forward propagation, the local devices are asked to backpropagate the corresponding gradient locally.\nThen, the following sequence of steps is performed for all parameters in the model. The quantization scale\nof parameter tensors is computed locally at first. The quantization scale is completely determined by the\n17\n(a) \n(b) \nFigure 8: Figure (a) shows the observations when running DQRM INT (blue) for 20 epochs versus running\nDLRM in FP32 for 5 epochs; Figure (b) compares DQRM, LSQ, and PACT;\nquantization bit-width and local tensor distribution so can be performed locally. Then, the quantization\nscale is first allreduced to be unified. Then, using the same scale, gradient tensors on each device are quan-\ntized locally. The integer gradients are then allreduced. After two allreduce steps, each device then has the\nsame quantization scale and the same quantized gradients. The local step update of parameters then can be\ndequantized. The quantization bit width used is INT8 and unified across all parameters. When it comes to\nsparse gradients from embedding tables, we only quantize the values part of the sparse coo tensor format\nand leave the indices and other fields unchanged.\nOne caveat of the entire flow of our customized gradient communication is that it can only be used for\nallreduce paradigms that perform the reduced operation at the destined machine, such as the Ring allreduce.\nHowever, for recursive doubling allreduce, where the reduced operation is collectively performed by many\nmachines, our framework is not applicable. The main problem is at each reduced operation, where a sum\nis taken by two low-precision operations. To continue propagating the gradients under the low-precision\nbit-width, an additional rounding should occur, which may potentially lose the gradient precision further.\nUnfortunately, PyTorch allreduce operations use recursive-doubling allreduce, throughout our presented ex-\nperiment, we use FP32 tensors to contain quantized INT8 integer gradients to avoid accumulation and\nrounding issues from the impact of allreduce.\nB.2\nAdditional Analysis on DLRM Distributed Training\nFirstly, we provide more details on the experiment setup for Figure 1(c). The experiments are run on both\na single GPU and a single CPU node. We train the model under the Kaggle dataset settings on a single\nNvidia A5000 GPU, as shown in the left pillar. We also run experiments of the Terabyte dataset on a single\nIntel(R) Xeon(R) Platinum 8280 CPU node. We found that such a problem significantly magnifies on the\nCPUs. Finding the quantization scale for large DLRM models under the Terabyte dataset settings can take\nmore than 1000 ms per iteration of QAT while occupying the majority of the training time. The Terabyte\ndataset has embedding tables that are ten times larger than those on the Kaggle dataset.\nIn Figure 10, we profile the total time spent during the entire training process when using the PyTorch\n18\n\u2026\nThe models are \nshared weights \nDifferent \ntraining data \nGradient\nGradient\nGradient\nGradient\ns\ns\ns\ns\n1. Allreduce\nthe scale \ns\nInteger\nInteger\nInteger\nInteger\n2. Use the shared \nscale to quantize \nInteger\nGradient scale for Quantization \nQuantized gradient \n3. Allreduce the \ninteger gradients \nMachine#1\nMachine#2\nMachine#3\nMachine#4\nFigure 9: Illustration of the Customized Data Parallelism Framework that Supports Gradient Quantization\nFigure 10: A breakdown of training time on distributed data parallelism environment on multiple GPUs and\nCPU nodes\nbuilt-in specified sparsity on Embedding tables. During the profile, we train DQRM Kaggle across four\nNvidia A5000 GPUs and DQRM Terabyte across two Intel Xeon Platinum 8280 CPUs with an update\nperiod set to 1000 to eliminate the difficulties of finding the scale. Under the pure Data Parallelism settings,\ngradient communication (blue) is a severe bottleneck for both GPU and CPU platforms. Notice that the\nweight update (orange) portion of CPU platforms is significant, because the vanilla PyTorch library we used\nis highly inefficient in sparse tensor operations, and the inefficiency in arithmetic will be largely reduced by\nthe recently published IPEX1 package from Intel. Besides the significant weight update (orange) portion\nwhich can be optimized relatively easily, we argue that the gradient communication (blue) is still a severe\nbottleneck on the CPU platform.\nB.3\nSimulated Data Parallelism Environment\nFor some experiments presented in the main content, we use a simulated environment to run Data Paral-\nlelism on single device to imitate the multiple device environment. Here we provide more details of our\nimplementation. In our implementation, we add an addition gradient buffer for every parameter tensor. As\n1IPEX official GitHub repo: https://github.com/intel/intel-extension-for-pytorch\n19\nAlgorithm 1: Simulated Data Parallelism on single machine\nbuffer clean \u2190False;\nfor j, batch in enumerate(train loader) do\nZ \u2190model forward(training input);\nE \u2190loss criterion(Z, label);\nclear gradient(model);\nif buffer clean then\ngrad buffer zero();\nbuffer clean \u2190False;\nend\nE.backward() ;\n/* local backward propagation */\ngrad buffer update();\nif j % simulated nodes == 0 then\nweight buffer update();\nbuffer clean \u2190True;\nend\nend\nsummarized in Algorithm 1, the batch size is splited similar to one distributed machine. In the second if\nstatement, the weight is actually updated by the gradient every simulated node iterations. Between updates,\ntraining still backpropagate gradients from relevant iteration and are hold inside the parameter buffers. Af-\nter weight being updated by the buffer, the buffer is cleared prior to new gradient being backpropagated.\nOne key difference between simulated and the real distributed environment lays in its allreduce mechanism.\nShown in Section B.1, if gradients are quantized, two allreduce processes occur in one iteration: the first\ncommunicates quantization scale, while the second one communicates quantized gradient tensors. However,\ntwo allreduce processes are difficult to implement in the simulated environment. Therefore, we reuse the\nquantization scale of the first iteration throughout simulated machine number of iterations in our implemen-\ntation, which can potentially hurts the gradient quantization precision. However, through our experiments,\nwe didn\u2019t observe this case.\nB.4\nSimulated Framework Evaluation\nTable 10: Multi-node Experiment Results with 8-bit gradients, loss evaluated on the Terabyte Dataset\n#Node\nTraining Loss\nTesting\nDrop\nAcc Drop AUC Drop\n2\n-0.002447\n0.092\n0.0025\n4\n-0.00273\n0.114\n0.0036\n8\n-0.00395\n0.059\n0.0053\nWe also evaluate the effect of different node counts on gradient quantization. The result is listed in Table\n10. Currently, different node counts are simulated on the single CPU node. Across three different node\ncounts, 2, 4, and 8, the drop in training loss, testing accuracy, and ROC AUC score is consistent and small.\nC\nDemo Recommender on the Phone\nWe include a demo of DQRM exported to an Android phone. From Figure 11, the tested DQRM model size\nis 405.65 MB. As a reference, the DLRM Kaggle model size is 2.16 GB. The model size is not strictly 8\u00d7\ncompression because of the following two reasons: 1) Embedding tables can be quantized into INT4, but\nthe embedding vectors have to be bit-packed together into INT8 format to fully benefit from the INT4 low\n20\nFigure 11: A screenshot of the quantized model on a high-end Android Phone\nprecision. However, bitpacking on PyTorch is tricky and PyTorch 4-bit packing is not fully optimized in\nterms of performance. 2) PyTorch doesn\u2019t support MLP layers to be quantized into bitwidth below INT8.\nTherefore, MLP layers, although quantized into INT4, still need to be stored as INT8 numbers. Still, we\nbelieve this is a first step towards deploying large recommendation models to edge devices so as to alleviate\nthe heavy cloud AI inference pressure.\n21\n",
        "context": "Ko et al.(2021)]. Recently, some works have extended quantization to DLRM models but mainly focus on\nPost Training Quantization (PTQ).\n[Guan et al.(2019)] uses codebook quantization to quantize the em-\nbedding tables of DLRM models into INT4, while\nDLRM model size is occupied by the embedding tables, previous efforts focus on shrinking the embedding\ntables without lowering weight precision. Our work focuses on neural network quantization, which is orthog-\nonal and complementary to these works.\n5\nFigure 3: (a) shows the effect of using different QAT bit widths on quantizing embedding tables in DLRM for\nfive epochs of training (epochs are separated by the black dashed lines in all figures). QAT in uniform 4-bit"
    },
    {
        "id": 12,
        "title": "Tt-rec: Tensor train compression for deep learning recommendation models",
        "author": [
            "C. Yin",
            "B. Acun",
            "C.-J. Wu",
            "X. Liu"
        ],
        "year": "2021",
        "doi": null,
        "in_text_citation": "[12]",
        "sentence": "By quantizing the embeddings to lower bit-width representations in DLRMs [9], such as 4-bit [10], [11], or performing tensor train decomposition [12], memory usage can be significantly reduced, making it more feasible to train and inference.",
        "abstract": "The memory capacity of embedding tables in deep learning recommendation\nmodels (DLRMs) is increasing dramatically from tens of GBs to TBs across the\nindustry. Given the fast growth in DLRMs, novel solutions are urgently needed,\nin order to enable fast and efficient DLRM innovations. At the same time, this\nmust be done without having to exponentially increase infrastructure capacity\ndemands. In this paper, we demonstrate the promising potential of Tensor Train\ndecomposition for DLRMs (TT-Rec), an important yet under-investigated context.\nWe design and implement optimized kernels (TT-EmbeddingBag) to evaluate the\nproposed TT-Rec design. TT-EmbeddingBag is 3 times faster than the SOTA TT\nimplementation. The performance of TT-Rec is further optimized with the batched\nmatrix multiplication and caching strategies for embedding vector lookup\noperations. In addition, we present mathematically and empirically the effect\nof weight initialization distribution on DLRM accuracy and propose to\ninitialize the tensor cores of TT-Rec following the sampled Gaussian\ndistribution. We evaluate TT-Rec across three important design space dimensions\n-- memory capacity, accuracy, and timing performance -- by training MLPerf-DLRM\nwith Criteo's Kaggle and Terabyte data sets. TT-Rec achieves 117 times and 112\ntimes model size compression, for Kaggle and Terabyte, respectively. This\nimpressive model size reduction can come with no accuracy nor training time\noverhead as compared to the uncompressed baseline.",
        "full_text": "TT-REC: TENSOR TRAIN COMPRESSION\nFOR DEEP LEARNING RECOMMENDATION MODEL EMBEDDINGS\nChunxing Yin 1 2 Bilge Acun 2 Xing Liu 2 Carole-Jean Wu 2\n1Georgia Institute of Technology, 2Facebook AI\nABSTRACT\nThe memory capacity of embedding tables in deep learning recommendation models (DLRMs) is increasing\ndramatically from tens of GBs to TBs across the industry. Given the fast growth in DLRMs, novel solutions\nare urgently needed, in order to enable fast and ef\ufb01cient DLRM innovations. At the same time, this must be\ndone without having to exponentially increase infrastructure capacity demands. In this paper, we demonstrate the\npromising potential of Tensor Train decomposition for DLRMs (TT-Rec), an important yet under-investigated\ncontext. We design and implement optimized kernels (TT-EmbeddingBag) to evaluate the proposed TT-Rec\ndesign. TT-EmbeddingBag is 3\u00d7 faster than the SOTA TT implementation. The performance of TT-Rec is\nfurther optimized with the batched matrix multiplication and caching strategies for embedding vector lookup\noperations. In addition, we present mathematically and empirically the effect of weight initialization distribution on\nDLRM accuracy and propose to initialize the tensor cores of TT-Rec following the sampled Gaussian distribution.\nWe evaluate TT-Rec across three important design space dimensions\u2014memory capacity, accuracy, and timing\nperformance\u2014by training MLPerf-DLRM with Criteo\u2019s Kaggle and Terabyte data sets. TT-Rec achieves 117\u00d7\nand 112\u00d7 model size compression, for Kaggle and Terabyte, respectively. This impressive model size reduction\ncan come with no accuracy nor training time overhead as compared to the uncompressed baseline.\nOur code is available on Github at facebookresearch/FBTT-Embedding.\n1\nINTRODUCTION\nDeep neural networks (DNNs) are witnessing an unprece-\ndented growth in all dimensions: data, model complexity,\nand the cost of infrastructure required for model training\nand deployment. For instance, at Facebook, the amount of\ndata used in machine learning tripled in one year (2019\u2013\n20), which led to an eight-fold increase in the amount of\ncomputation required for training (Hazelwood, 2020). Simi-\nlarly, the number of parameters in state-of-the-art language\nmodels have increased exponentially, currently at over 175\nbillion parameters in OpenAI\u2019s GPT-3 (Brown et al., 2020).\nIn response, there is considerable interest to design domain-\nspeci\ufb01c accelerators and at-scale infrastructures (Alibaba,\n2019; Amazon, 2019; Chung et al., 2018; Fowers et al.,\n2018; Hazelwood et al., 2018; Jouppi et al., 2017; Lee &\nRao, 2019; Mattson et al., 2019; NVIDIA, 2020a; Ovtcharov\net al., 2015; Reddi et al., 2019). But to achieve signi\ufb01cant\nreductions in the cost and capacity, we still need to dis-\ncover orders-of-magnitude reductions in the infrastructure\ndemand while maintaining or even outperforming SOTA\nmodel accuracy.\nIn this work, we consider a new algorithmic approach to\ncope with the large memory requirement of DNNs, focus-\ning on the critical use-case of embedding tables in deep\nlearning-based recommendation models (DLRMs). These\nmodels represent one of the most resource-demanding deep\nlearning workloads, consuming more than 50% of training\nand 80% of the total AI inference cycles at Facebook\u2019s data\ncenters (Gupta et al., 2019; Naumov et al., 2020). From\nsystems perspective, the large embedding tables that con-\ntribute to more than 99% of the total recommendation model\ncapacity are the Amdahl\u2019s bottleneck for optimization. Our\napproach uses tensorization to address the large memory\ncapacity demand of embedding tables in a DLRM.\nAt a high level, tensorization replaces layers of a neu-\nral network with an approximate and structured low-rank\nform (Novikov et al., 2015). This form is parametric \u2013 its\n\u201cshape\u201d determines the design trade-off between storage ca-\npacity, execution time, and model accuracy. Furthermore,\ntensorized representation can be \ufb01ne-tuned with respect to\nthe architecture of a given hardware platform. Figure 1\nillustrates the design space with respective to the tunable\nparameters, such as rank of the tensorization method, dimen-\nsions of the embedding and number of tables to compress.\nData points on the Pareto frontier (black curve) represent\nthe optimal settings that maximize the recommendation\nmodel accuracy (y-axis) given a corresponding memory\narXiv:2101.11714v1  [cs.LG]  25 Jan 2021\nTT-Rec: Tensor Train Compression for Deep Learning Recommendation Models\nFigure 1. The design space demonstrates the potential of DLRM\naccuracy and model size tradeoff with respect to the tunable param-\neters, e.g., ranks of the tensorization method (colors), dimensions\nof embedding (shapes), and number of compressed embedding ta-\nbles (brightness). The data points that fall onto the Pareto Frontier\n(the black curve) represent optimal settings that maximize DLRM\naccuracy (the y-axis) given a memory size (the x-axis).\nsize (x-axis). The parameters of these optimal data points\nvary, depending on the model\u2019s characteristics (embedding\ndimensions), the tensorization setting (i.e., tensor ranks),\nand the memory capacity of the underlying training system.\nGiven the large con\ufb01guration space, parameters need to be\ncarefully studied in order to achieve highest possible model\nquality given a target model size.\nWe design a Tensor-Train compression technique for deep\nlearning Recommendation models, called TT-Rec. The core\nidea is to replace large embedding tables in a DLRM with\na sequence of matrix products. This method is analogous\nto techniques that use lookup tables to trade-off memory\nstorage and bandwidth with computation. TT-Rec suits well\nfor accelerators like GPUs, which have a relatively higher\ncompute-to-memory (FLOPs-per-Byte) ratio and limited\nmemory capacity. Since the tensor representation is a \u201clossy\u201d\ncompression scheme, to compensate for accuracy loss we\npropose a new way to initialize the element distribution of\nthe tensor form. Furthermore, to mitigate increases in train-\ning time when the tensor form must be decompressed, we\nintroduce a cache structure that exploits the unique sparse\nfeature distribution in DLRMs, in which we store the most\naccessed embedding vectors in the uncompressed format.\nSince these cached embedding vectors are learned without\ncompression, using this cache design can also help recover-\ning model accuracy. Thus, TT-Rec uses a hybrid approach\nto learn features and deliver on-par model accuracy while\nrequiring orders-of-magnitude less memory capacity.\nWe show signi\ufb01cant compression ratios and improved train-\ning time performance at the same time with a judicious\ndesign and parameterization of the tensor-train compression\ntechnique. The compute-to-memory ratio of the underly-\ning hardware can also be taken into account when param-\neterizing the proposed technique. While prior works have\ndemonstrated tensor-train compression techniques for em-\nbedding layers in language models (Hrinchuk et al., 2020),\nthis paper is the \ufb01rst to explore and customize tensor-train\ncompression techniques for DLRMs, with a particular focus\non minimizing the signi\ufb01cant memory capacity requirement\nof the embedding layers (over tens to hundreds of GBs, or\nover 99% of the total model size).\nThe main contributions of this paper are as follows:\n\u2022 This work applies tensor-train compression in a new ap-\nplication context, compressing the embedding layers of\nDLRMs.\n\u2022 Our in-depth design space characterization shows the im-\nportance of choosing the right number of embedding ta-\nbles to compress and the dimension of the compressed\ntensors. In particular, we quantify the potential trade-off\nbetween memory requirements and accuracy.\n\u2022 To recover accuracy loss, we propose to use a sampled\nGaussian distribution for the weight initialization of the\ntensor cores. Furthermore, to accelerate the training per-\nformance of TT-Rec, we introduce a separate cache struc-\nture to store frequently-accessed embedding vectors in the\nuncompressed format, which we show empirically helps\naccuracy improvement.\n\u2022 We demonstrate the promise of TT-Rec: on Terabyte, TT-\nRec achieves higher model accuracy (0.19% to 0.42%\nover the baseline) while reducing the total memory re-\nquirement of the embedding tables by 22\u00d7 to 112\u00d7 with\na small amount of 10% training time increase on average,\nand similarly for Kaggle.\n\u2022 TT-Rec offers a \ufb02exible design space between memory\ncapacity, training time and model accuracy. It is an ef-\nfective approach especially for online recommendation\ntraining. The orders-of-magnitude lower memory require-\nment with TT-Rec also unlocks a range of modern AI\ntraining accelerators for DLRM training.\n2\nBACKGROUND\nDeep learning recommendation models.\nFigure 2 de-\npicts the generalized model architecture for DLRMs. The\nshaded structure (with the orthogonal green stripes) repre-\nsents the baseline while the other shaded structure (with\nthe solid yellow box) depicts the proposed TT-Rec design.\nThere are two primary components: the Multi Layer Per-\nceptron (MLP) layer modules and the Embedding Tables\n(EMBs). The MLP layers are used to process continuous fea-\ntures, such as user age, while the EMBs are used to process\ncategorical features by encoding sparse, high-dimensional\ninputs into dense, vector representation. The encoded vec-\ntors, usually of length 16 to 128 in industry-scale recom-\nmender systems, are processed by an interaction operation\nfollowed by a top MLP layer.\nTT-Rec: Tensor Train Compression for Deep Learning Recommendation Models\nMLPbottom\nDense Features\nSparse Features\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\n0\n0\n0\n1\n1\n1\n1\nTT-Rec\nTT-EMB1\nTT-Reccache\nTT-EMB2\nTT-Reccache\nTT-EMB3\nTT-Reccache\nEmbedding Aggregation\nEMBN\nEMBN-1\n\u2026\n\u2026\nFeature Interaction\nMLPtop\nPrediction\nBaseline\nEMB1\nEMB2\nEMBN\nEMBN-1\nEMB3\n\u2026\nFigure 2. Generalized model architecture for DLRMs. The shaded\nstructure (with orthogonal green stripes) is the baseline DLRM\nand the yellow box to its right represents the alternative proposed\nTT-Rec design that would replace the baseline in the stack.\nThe sparse embedding tables pose infrastructure challenges\nfrom both the perspectives of storage and bandwidth require-\nment. There are tens of millions of rows in an embedding\ntable, where the number of rows are growing exponentially\nfor future recommendation models, resulting in memory\nrequirement into the TB scale (Zhao et al., 2020). Further-\nmore, the EMB lookup operation gathers multiple embed-\nding vectors simultaneously across the tables, making the\nexecution memory bandwidth bound. To address the ever-\nincreasing memory capacity and bandwidth challenges, this\nwork examines a fundamentally different approach\u2014instead\nof gathering dense embedding vectors from EMBs that store\ninformation in the latent space representation, we seek meth-\nods to replace large EMBs with a sequence of small matrix\nproducts using tensor train decomposition.\nTensor-train decompositions.\nSimilar to matrix de-\ncomposition, such as Singular Value Decomposition\n(SVD) (Xue et al., 2013) and Principal Component Analysis\n(PCA) (Wold et al., 1987), Tensor-Train decomposition is\nan approach to matrix decomposition by decomposing ten-\nsor representation of multidimensional data into product of\nsmaller tensors. Tensor-Train (TT) decomposition is a sim-\nple and robust method for model compression (Oseledets,\n2011) and has been studied extensively for deep learning ap-\nplication domains, such as computer vision (Yang et al.,\n2017) and natural language understanding (Rusu et al.,\n2020). However, such method has not been investigated\nfor the deep learning recommendation space; thus, its poten-\ntial remains unknown. Here, we describe the fundamental\nprinciple of TT-decomposition and its application to recom-\nmendation embedding.\nAssume A \u2208RI1\u00d7I2\u00d7...Id is a d\u2212dimensional tensor, where\nIk is the size of dimension k. A can be decomposed as\nA(i1, i2, . . . , id) = G1(:, i1, :)G2(:, i2, :) . . . Gd(:, id, :),\n(1)\nwhere Gk \u2208RRk\u22121\u00d7Ik\u00d7Rk, and R0 = Rk = 1 to keep\nproduct of the sequence of tensors a scalar. The sequence\n{Rk}d\nk=0 is referred as to TT-ranks, and each 3-dimension\ntensor Gk is called a TT-core.\nThe TT decomposition can also be generalized to com-\npress a matrix W \u2208RM\u00d7N. We assume that M and N\ncan be factorized into sequences of integers, i.e., M =\nQd\ni=1 mk, and N = Qd\ni=1 nk. Correspondingly, we re-\nshape the matrix W as a 2d-dimensional tensor W \u2208\nR(m1\u00d7n1)\u00d7(m2\u00d7n2)\u00d7\u00b7\u00b7\u00b7\u00d7(md\u00d7nd), where\nW((i1, j1), (i2, j2), . . . , (id, jd))\n= G1(:, i1, j1, :)G2(:, i2, j2, :) . . . Gd(:, id, jd, :)\n(2)\nand each 4-d tensor Gk \u2208RRk\u22121\u00d7mk\u00d7nk\u00d7Rk, R0 = Rd =\n1. Let R, m, and n be the maximal rk, mk and nk respec-\ntively for k = 1, . . . , d. TT format reduces the space for\nstoring the matrix from O(MN) to O(dR2 max(m, n)2).\nTable 2 in Section 6.6 illustrates the detailed embedding ta-\nble sizes and their corresponding compressed dimensions.\n3\nPROPOSED DESIGN OF TT-REC\nIn this section, we present the proposed design, called TT-\nRec. TT-Rec customizes the TT-decomposition method to\ncompress embedding tables in deep learning recommenda-\ntion models (\u00a7 3.1). An overview of our design, where the\nlarge embedding tables are replaced with TT-Rec, is shown\nin Figure 2. In order to compensate the accuracy loss from\nreplacing embedding vectors with vector-matrix multipli-\ncations, we introduce a new way to initialize the element\ndistribution for the TT-cores (\u00a7 3.2). This is an important\nstep, since the distribution of the weights resulting from\nmultiplication of the TT-cores are skewed from TT-cores\u2019\ninitial distribution due to the product operation.\n3.1\nCustomizing TT-decomposition for Embedding\nTable Compression\nEach embedding lookup can be interpreted as a one-hot vec-\ntor matrix multiplication wT\ni = eT\ni W, where ei is a vector\nwith i-th position to be 1, and 0 anywhere else. In more com-\nplex scenarios, an embedding lookup represents a weighted\ncombination of multiple items w = P\nk iT\nk W. We compress\nthe embedding table W as in Equation (2), and hence em-\nbedding lookup operation of row i = Pd\ni=1 ik\nQd\nj=i+1 Ij\nis be represented the following:\nwi = G1(i1, :, :)G2(:, i2, :, :) . . . Gd(:, id, :)\n(3)\nLet w(k)\ni\n\u2208R\nQk\u22121\n1\nni\u00d7nkRk be the partial product of the\n\ufb01rst k TT-cores in Equation 3. The tensor multiplication\nTT-Rec: Tensor Train Compression for Deep Learning Recommendation Models\n0E+0\n4E+6\n7E+6\n1E+7\n-0.001\n0.000\n0.001\nSampled N(0,1)\nN(0,1/3n)\nUniform(-1/sqrt(n), 1/sqrt(n))\nFigure 3. The probability density function (PDF) of product of\nthree independent and identically distributed (i.i.d.) random vari-\nables (left) from Uniform(0,1) and from N(0,1); and from (right)\nsampled Gaussian distribution (red), comparing to N(0, 1) (grey)\nand uniform distribution.\nof w(k)\ni\nGk+1(:, ik, :, :) can be unfolded and formulated as a\nmatrix-matrix multiplication where w(k)\ni\n\u2208R\nQk\n1 ni\u00d7Rk and\nGk+1(:, ik, :, :) \u2208RRk\u00d7nk+1Rk+1.\nIn uncompressed models, storing and updating an embed-\nding table of size M \u00d7N requires for O(MN) space, while\nin TT-Rec, we propose to only learn the gradient of the loss\nfunction L with respect to the TT-cores through backward\npropagation (Equation 5).\n\u2202L\n\u2202Gk(:, ik, :) =\n(\nwT\ni\n\u2202L\n\u2202y , if k = d\n(w(k)\ni\n)T\n\u2202L\n\u2202w(k+1)\ni\n, if 1 < k < d\n(4)\n\u2202L\n\u2202w(k)\ni\n=\n( \u2202L\n\u2202y GT\nd (:, id, :), , if k = d\n\u2202L\n\u2202w(k+1)\ni\nGT\nk (:, ik, :, :), if 1 < k < d\n(5)\nNot all sparse features are equally important is a key ob-\nservation used in the design of TT-Rec. Data samples in\nindustry-scale recommendation use cases often follow a\nPower or Zip\ufb01an distribution (Wu et al., 2020). A small\nsubset of sparse features capture a signi\ufb01cant portion of\ntraining samples that index to the set of the features in the\nembedding tables. This observation is particularly impor-\ntant to reduce the data movement overhead when GPUs are\nemployed as AI training accelerators.\nOne of the performance optimization potential unlocked by\nTT-Rec is that the collection of DLRMs that require mem-\nory capacities larger than that of training accelerators can\nnow be accelerated with the accelerators. In addition to op-\ntimizing TT-Rec\u2019s performance using GPUs, we introduce\na caching scheme to retain the frequently-accessed embed-\nding vectors/rows in the EMBs. The cache enables TT-Rec\nto exploit the aforementioned temporal locality by storing\nmost frequently-accessed embedding rows in the uncom-\npressed format. By doing so, TT-Rec minimizes the need\nof computation. The detail of the performance optimization\nimplementation are described in detail later in Section 4.2.\n3.2\nWeight Initialization\nReplacing an embedding vector lookup with a sequence\nof tensor computation to approximate the original vector\nmay introduce an accuracy loss in TT-Rec. To compensate\nthe accuracy loss, we introduce a new way to initialize the\nweights of the TT-cores. Typically, larger TT-ranks provide\nlower compression ratios while achieving model accuracy\ncloser to that of the baseline. However, when increasing the\nTT-rank value in TT-Rec, we \ufb01nd that the corresponding ac-\ncuracy improvements saturate quickly despite the decreasing\ncompression ratios, as we show later in detail in Section 6.6.\nThis led us to investigate the initialization behavior of the\nuncompressed baseline and TT-Rec\u2014the initial distribution\nof the TT-cores can signi\ufb01cantly in\ufb02uence the model quality.\nSince the TT decomposition approximates the full tensor,\nwe hope to \ufb01nd the best con\ufb01guration of the uncompressed\nmodel and have TT-Rec approximate the same con\ufb01guration.\nFor the uncompressed DLRMs, uniform random distribu-\ntion usually outperforms normal distribution. For validation,\nwe initialize DLRMs with different forms of Gaussian dis-\ntribution. Then, we determine the correlation between the\nmodel accuracy and the distance between the Gaussian and\nuniform distribution.\nTo approximate a uniform distribution on [a, b] by a Gaus-\nsian distribution N(\u00b5, \u03c32), we want to minimize the KL-\ndivergence\nDKL(P|Q) = \u2212\nZ \u221e\n\u2212\u221e\nP(x) ln P(x)\nQ(x)dx\nwhere P(x) =\n1\nb\u2212a if x \u2208[a, b], otherwise 0; and Q(x) =\n1\n\u221a\n2\u03c0\u03c32 e\n(x\u2212\u00b5)2\n2\u03c32 . Minimizing the KL-divergence for a given\nuniform distribution using the \ufb01rst-order approach results in\n\u00b5 = a + b\n2\n,\n\u03c32 = (b \u2212a)2\n12\n.\nTherefore, to best approximate the uniform distribution\nused in the DLRM (Uniform( \u22121\n\u221an,\n1\n\u221an)), we should adopt\nN(0, 1\n3n) for initialization. Table 1 shows the accuracy of\nthe DLRMs that are initialized with various Gaussian forms,\nwhere accuracy gap is proportional to the KL-divergence\nbetween the Gaussian and uniform distribution.\nFor TT-Rec, we want the product of TT cores to either\napproximate Uniform( \u22121\n\u221an,\n1\n\u221an) or N(0, 1\n3n) during initial-\nization. Initializing the TT-cores is challenging since the\ndistribution of product of random variables is non-trivial. In\npractice, both uniform and normal distributions can be used\nto reasonably initialize TT-cores (Hrinchuk et al., 2020).\nHowever, the TT product of these distributions do not serve\nas an appropriate approximation to uniform distribution as\nFigure 3 (left) shows.\nTT-Rec: Tensor Train Compression for Deep Learning Recommendation Models\nDistribution\nKL-divergence\nAccuracy\nuniform(( \u22121\n\u221an,\n1\n\u221an))\n0\n79.263 %\nN(0, 1)\nc \u2212\n1\n6n\n78.123 %\nN(0, 1/2)\nc \u2212\n1\n3n + 0.34\n78.371 %\nN(0, 1/8)\nc \u2212\n4\n3n + 1.4\n78.823 %\nN(0, 1/3n)\n\u22120.17\n79.256 %\nN(0, 1/9n2)\n1\n2 ln 18\n\u03c0n \u22121.5n\n79.220 %\nTable 1. The accuracy of uncompressed DLRM with embed-\nding tables initialized from various Gaussian distributions, c =\n0.5 ln\n2\n\u03c0n < \u22127, compared with uniform distribution.\nEmbedding Lookup\nWarm up LFU \ncounter\nTrain TT-Rec\nUncompress \nfrequent rows \nfrom TT; \nUpdate cache\nCache Lookup(IC)\nTT Lookup(IT)\nPartition \nLookup Index\n I = IC \u222a IT  \nLoss\nPartition \nLookup Index\n I = IC \u222a IT  \nEmbedding Update\nCache Update(IC)\nTT Update( IT )\nForward propagation\nBackward propagation\nStage 1\nStage 3\nStage 2\nFigure 4. Multi-stage training process with caching.\nTo better approximate the best Gaussian distribution shown\nin Table 1, we reduce the amount of values close to zero\nin each core by sampling the Gaussian distribution. Our\nsampling method is shown in Algorithm 3 in Appendix A.\nWe validate the product of the TT-cores with Algorithm 3\nin Appendix A. In Figure 3-(right), we compare the product\nwith N(0, 1/3n), which serves as the best approximation\nto Uniform(\u22121/\u221an, 1/\u221an). We will show in Section 6.2\nthat our algorithm achieves the highest accuracy in training.\n4\nPERFORMANCE OPTIMIZATIONS\nReplacing embedding vectors with TT-core multiplications\ntrades off memory with computation. Depending on the em-\nbedding vector lookup patterns and the underlying system\narchitectures, training time overhead from the computations\ncan vary. In order to mitigate the associated performance\noverhead, we introduce a cache structure as part of TT-Rec\nto leverage the observation \u2013 a small subset of features (rows\nin embedding tables) constitute most of the embedding ta-\nble accesses. Thus, here, we describe the execution \ufb02ow\nfor deep learning recommendation model training: forward\npropagation and backward propagation implementations in\nSection 4.1. Then, we introduce TT-Rec\u2019s cache design to\nimprove the training time performance in Section 4.2.\n4.1\nTT-Rec Implementation\nThe embedding layer takes row indices as input in Com-\npressed Sparse Row (CSR) format. The input is translated\ninto multiple embedding bags, in which the embedding vec-\ntors belonging to an embedding bag are pooled together\nby summation or average. Let indices[] be an integer ar-\nray of length n, where indices[i] < Index Range for\n0 \u2264i < n, and offsets[] be an integer array of length\nm \u2264n to specify the starting index in indices[] of each\nembedding bag. Speci\ufb01cally, to compute for jth embed-\nding bag, the algorithm will summarize or average the rows\n{W(indices[k], :)} for offsets[j] \u2264k < offsets[j + 1].\nIn TT-Rec, this is computed as a sequence of small matrix-\nmatrix products.\noutputi =\noffset[i+1]\nX\nk=offset[i]\n\u03b1indices[k]windices[k]\n(6)\n=\noffset[i+1]\nX\nk=offset[i]\n\u03b1indices[k]G1(ik\n1, :, :) . . . Gd(:, ik\nd, :)\n(7)\n, where ik\nj indexes the slice in Gj that is used for computing\nvector W(indices[k], :), and \u03b1i is the per sample weight\nassociated with each embedding vector.\nTo obtain a high-performance implementation of TT-\nEmbeddingBag, we use a batched GEMM implementation\nfrom cuBLAS (NVIDIA, 2020). The algorithm computes\nfor a batch of B embedding vectors and stores the inter-\nmediate results in tri, 0 \u2264i \u2264d \u22122. Given a batch of\nqueried embedding vectors, the algorithm reduces the batch\nto embedding bags in parallel. Algorithm 1 of Appendix A\nshows the pseudocode of the batched embedding kernel for\nthe 3-dimensional TT decomposition. This algorithm can\nbe generalized to any arbitrary TT dimension by extending\nlines 5-10 to set up the pointers to intermediate TT results.\nThe backward propagation algorithm for the 3-dimensional\nTT compression is illustrated by Algorithm 2 of Appendix A.\nBased on the chain rule as described in Equation 5, we com-\npute the gradient w.r.t. the embedding bags, and accumulate\nthe gradients into each TT-core.\n4.2\nA Least-Frequently-Used Cache for TT-Rec\nWhile TT-Rec reduces the memory requirement of embed-\nding tables, this method introduces latency through addi-\ntional computations from (1) reconstructing values of the\nqueried embedding rows from the TT format in the forward\npropagation, (2) determining the gradient for each TT-core\nin the backward propagation, and (3) recomputing the inter-\nmediate GEMM results for the gradient computation.\nRecomputation of the intermediate results, in Algorithm 2\nTT-Rec: Tensor Train Compression for Deep Learning Recommendation Models\nEmb. Table Dimensions\nTT-Core Shapes\n# of TT Parameters\nMemory Reduction\n# Rows\nEmb. Dim.\nG1\nG2\nG3\nR = 16\nR = 32\nR = 64\nR = 16\nR = 32\nR = 64\n10131227\n16\n(1, 200, 2, R)\n(R, 220, 2, R)\n(R, 250, 4, 1)\n135040\n495360\n1891840\n1200\n327\n86\n8351593\n16\n(1, 200, 2, R)\n(R, 200, 2, R)\n(R, 209, 4, 1)\n122176\n449152\n1717504\n1094\n297\n78\n7046547\n16\n(1, 200, 2, R)\n(R, 200, 2, R)\n(R, 200, 4, 1)\n121600\n448000\n1715200\n927\n252\n66\n5461306\n16\n(1, 166, 2, R)\n(R, 175, 2, R)\n(R, 188, 4, 1)\n106944\n393088\n1502976\n817\n222\n58\n2202608\n16\n(1, 125, 2, R)\n(R, 130, 2, R)\n(R, 136, 4, 1)\n79264\n291648\n1115776\n445\n121\n32\n286181\n16\n(1, 53, 2, R)\n(R, 72, 2, R)\n(R, 75, 4, 1)\n43360\n160448\n615808\n106\n28\n7\n142572\n16\n(1, 50, 2, R)\n(R, 52, 2, R)\n(R, 55, 4, 1)\n31744\n116736\n446464\n72\n19\n5\nTable 2. The original dimensions of Kaggle\u2019s 7 largest embedding tables in DLRM and their respective TT decomposition parameters.\n(line 3) of Appendix A, can be eliminated by storing ten-\nsors from the forward pass. This reduces the latency but\ncomes with slightly increased memory footprint and mem-\nory allocation time. The dominating source of performance\noptimization potential, however, comes from the aforemen-\ntioned distribution of sparse features in the training samples.\nThe row access frequency follows the Power Law distribu-\ntion in the largest embedding tables, i.e. a few embedding\nvectors are recurrently accessed throughout training.\nTo consider this unique characteristics of recommendation\ndata samples, we design and implement a software cache\nstructure to store the frequently-used embedding vectors on\nthe GPU. The cache stores an uncompressed copy of the\nfrequently accessed embedding vectors. As Section 6.5 later\nshows, we perform performance sensitive analysis over a\nwide range of cache sizes and empirically determine that\ndevoting 0.01% of the respective embedding table as TT-Rec\ncache is suf\ufb01cient from both model accuracy\u2019s and training\ntime\u2019s perspectives.\nGiven the sequence of embedding bags, the indices are\n\ufb01rst partitioned into two different groups: cached indices\nand tt indices. The cached embedding rows are directly\nfetched from the cache whereas the embedding vectors from\nthe tt indices group are computed, following the forward\npropagation operations described in Algorithm 2. Then,\nduring the backward propagation, the cached, uncompressed\nvectors can be simply updated with W \u2032 = W + \u2202L\n\u2202W , while\nthe non-cached vectors are updated to TT-cores as described\nin Algorithm 1. In this way, the weights of the two index\nsets are learned separately by the cache and the TT cores.\nIn order to offset the cache population overheads, TT-\nRec adopts a semi-dynamic cache, where the frequently-\naccessed embedding vectors are loaded into cache only ev-\nery 100s to 1000s of iterations, initialized from TT cores.\nIn order to track the frequencies of the all the existing in-\ndices, an open addressing hash table is used. On the other\nhand, the learned weights in the cache would be discarded\nwhen an eviction happens. In practice, this strategy does\nnot affect training accuracy as the evicted cache lines are\nnot accessed frequently and therefore contribute less to the\noverall model. We chose this strategy as decomposing the\nevicted vectors and updating the decomposed parameters\nwith the existing TT cores are equivalent to dynamically\ntracking TT decomposition for a streaming matrix, which is\na challenging algebraic problem itself.\nFigure 4 summarizes the multi-stage training process. The\nmodel training starts with the TT embedding tables only.\nThe \ufb01rst few iterations (e.g. 10% training samples) are\nused to warm up the cache state. The most frequently ac-\ncessed embedding vectors will be stored in the cache as\nuncompressed. Depending on the phase behavior, one might\nconsider updating the cache and repeat the warm up pro-\ncess periodically. Based on our empirical observation, the\nset of the most-frequently-accessed vectors are stable over\nwindow, indicating little periodic warm-up need.\n5\nEXPERIMENTAL SETUP\nDeep Learning Recommendation Model Parameters\nand Data Sets:\nWe implement the proposed TT-Rec de-\nsign over the open-source MLPerf reference implementation\nof the DLRM recommendation model architecture (MLPerf-\nDLRM) (Mattson et al., 2019; 2020; MLPerf, 2020; Nau-\nmov et al., 2019). We train TT-Rec with the Criteo Kaggle\nDisplay Advertising Challenge Dataset (Kaggle)1 and Criteo\nTerabyte Click Logs (Terabyte)2. In both the datasets, each\ndata sample consists of 13 numerical features and 26 cat-\negorical features, and a binary label. Kaggle contains 7\ndays of ads click data, whereas Terabyte contains 24 days of\nclick data (4.3 billion records). The 26 categorical features\nare interpreted into 26 embedding tables in TT-Rec, where\neach row in the embedding table corresponds to an element\nin that category. Figure 5-Baseline bars show the size and\ncomposition the embedding tables in the two datasets. Ta-\nble 2 summarizes the dimensions of the 7 largest embedding\ntables when training MLPerf-DLRM with Kaggle and the\nrespective TT decomposition parameters.\nTo study the effectiveness of TT-Rec, we adopt the same hy-\n1https://labs.criteo.com/2014/02/\nkaggle-display-advertising-challenge-dataset/\n2https://labs.criteo.com/2013/12/\ndownload-terabyte-click-logs/\nTT-Rec: Tensor Train Compression for Deep Learning Recommendation Models\n0\n2\n4\n6\n8\n10\n12\n14\nBaseline 3 TT-\nEmb.\n5 TT-\nEmb.\n7 TT-\nEmb.\nBaseline 3 TT-\nEmb.\n5 TT-\nEmb.\n7 TT-\nEmb.\nKaggle\nTerabyte\nMemory Requirement (GB)\n4x \n48.2x\n117.3x\n2.5x\n21.8x\n95.5x\nTT-Rec Compression Ratio\nTT-Rec Compression Ratio\nFigure 5. TT-Rec shows signi\ufb01cant model size reduction when\ncompressing different number of embeddings for Kaggle in green,\nand Terabyte in purple labels (TT-rank of 32).\nperparamters as speci\ufb01ed in the MLPerf-DLRM reference\nimplementation, including the embedding dimensions, MLP\ndimensions, learning rate, and batch size. Both datasets are\ntrained with the SGD optimizer. Note, as the dimension\nof the embedding increases from 64 to 512, the total mem-\nory requirement is over 96 GB, exceeding the latest GPU\nmemory capacity. This is when TT-Rec shines. The uncom-\npressed baseline has to run on CPUs or multiple GPUs via\nmodel parallelism (which requires extra all-to-all communi-\ncation overheads) while TT-Rec enables recommendation\ntraining on GPUs with data parallelism.\nFor accuracy evaluation, we report the test accuracy (%)\nas well as the BCE loss. We train TT-Rec for a single\nepoch using all the data samples in Kaggle. For Terabyte,\nwe downsize the negative training samples by 0.875, as\nspeci\ufb01ed by the MLPerf-DLRM benchmark.\nAll the evaluation results are obtained by training Kaggle\ndataset on an NVIDIA Tesla V100-SXM2 GPU with an Intel\nXeon E5-2698 CPU. Terabyte dataset results are obtained\non the same system but using eight CPUs with a single GPU\ndue to its larger system memory requirement.\n6\nEXPERIMENTAL RESULTS\nIn this section, we present the evaluation results for the pro-\nposed TT-Rec design (\u00a75). We evaluate TT-Rec across the\nthree important dimensions of DLRMs: memory capacity\nreduction (\u00a76.1), model quality (\u00a76.2), and training time\nperformance (\u00a76.3). Then, \u00a76.4, \u00a76.5 and \u00a76.6 provide a\ndeeper performance analysis for the TT-Embedding kernel\nimplementation, the TT-Rec cache design, and embedding-\ndominated DLRMs, respectively.\nTT-Rec:\nOverall, TT-Rec demonstrates to be an effective\ntechnique. For Kaggle, TT-Rec reduces the overall model\nsize requirement by 117\u00d7 from 2.16 GB to 18.36 MB. The\nsigni\ufb01cant capacity reduction comes with a relatively small\namount of training time increase by 14.3% while maintain-\ning the accuracy as the baseline. Similarly, for Terabyte, the\noverall model size requirement is reduced by 112\u00d7 from\n12.57 to 0.11 GB. This capacity reduction also comes with a\nrelatively small amount of training time increase by 13.9%.\nFinally, the model quality experiences negligible degrada-\ntion. For example, for both Kaggle and Terabyte, using\nTT-Rec to train the \ufb01ve largest embedding tables in the TT-\nEmb. format using TT-rank of 32 leads to model accuracy\nloss within 0.03%.\n6.1\nMemory Capacity Requirement with TT-Rec\nTT-Rec achieves signi\ufb01cant compression ratios for the em-\nbedding tables of Terabyte and Kaggle, by as much as 327\u00d7\nand by an average of 181\u00d7 (with TT-rank of 32). In the\nuncompressed baseline, the 7 largest tables constitutes 99%\nof the model. For Kaggle, with TT-Rec, the memory re-\nquirement of the 7 embedding tables is reduced from 2.16\nGB to only 18 MB, leading to 112\u00d7 model size reduction.\nFigure 5 compares the memory capacity requirement be-\ntween the baseline and TT-Rec (x-axis) across the 3, 5, and\n7 largest embedding tables (y-axis). As illustrated in Fig-\nure 5, the model size requirement also becomes signi\ufb01cantly\nlower when TT-Rec trains the less number of the large em-\nbedding tables in the TT-Emb. format \u2013 for TT-Emb. of 5\nand 3, the overall model size is reduced by 48\u00d7 and 4\u00d7,\nrespectively. For Terabyte, TT-Rec achieves 2.6, 21.8, and\n95.5\u00d7 model size reduction for TT-Emb. of 3, 5 and 7,\nrespectively. This impressive memory capacity reduction\nunlocks industry-scale multi-GB/TB DLRMs that cannot be\npreviously trained using commodity training accelerators,\nsuch as GPUs (40 GB of HBM2 in the latest NVIDIA A100\nGPUs (NVIDIA, 2020b) or TPUs 16-32 GB of HBM (Chao\n& Saeta, 2019)), to enjoy signi\ufb01cantly higher throughput\nperformance in state-of-the-art training accelerators.\n6.2\nModel Accuracy with TT-Rec\nTo achieve accuracy-neutral while still enjoying TT-Rec\u2019s\nmemory reduction bene\ufb01t, Figure 6(a) and (b) compare the\nvalidation accuracy of the uncompressed baseline with that\nof TT-Rec for Kaggle and Terabyte, respectively. Figure 6(a)\nshows that, when TT-Rec trains the largest 3, 5, and 7 em-\nbedding tables in the TT-Emb. format (x-axis), the optimal\nTT-rank to achieve a nearly accuracy-neural result varies,\nwith the optimal rank of 8, 32, and 64, respectively.\nInterestingly, for Terabyte, Figure 6(b) shows that TT-Rec\nachieves higher validation accuracies (y-axis) across the\nboard. As expected, with more embedding tables trained in\nthe TT-Emb. format, TT-Rec brings signi\ufb01cantly higher\nmodel size reduction at the expense of model accuracy\ndegradation. Increasing the number of the large embedding\ntables trained in the TT-Emb. format from 3 to 7 improves\nTT-Rec: Tensor Train Compression for Deep Learning Recommendation Models\n78\n78.1\n78.2\n78.3\n78.4\n78.5\n78.6\n78.7\n78.8\n78.9\n79\n8\n16\n32\n64\nAccuracy (%)\nTT-Rank\nUniform\nGaussian\nSampled Gaussian\nBaseline\n80\n80.1\n80.2\n80.3\n80.4\n80.5\n80.6\n80.7\n80.8\n80.9\n81\n3 TT-Emb.\n5 TT-Emb.\n7 TT-Emb.\nAccuracy (%)\nTT-Rank 8\nTT-Trank 16\nTT-Rank 32\nTT-Rank 64\nBaseline\n78\n78.1\n78.2\n78.3\n78.4\n78.5\n78.6\n78.7\n78.8\n78.9\n79\n3 TT-Emb.\n5 TT-Emb.\n7 TT-Emb.\nAccuracy (%)\nTT-Rank 8\nTT-Trank 16\nTT-Rank 32\nTT-Rank 64\nBaseline\n(a) Kaggle\n(b) Terabyte\n(c) Kaggle Distributions\nFigure 6. (a) and (b) Validation accuracy of TT-Rec with various tables compressed and TT ranks training with Kaggle and Terabyte\nrespectively. (c) Validation accuracy of TT-Rec using different initialization techniques on Kaggle.\n0.95\n1.05\n1.15\n1.25\n3 TT-\nEmb.\n5 TT-\nEmb.\n7 TT-\nEmb.\n3 TT-\nEmb.\n5 TT-\nEmb.\n7 TT-\nEmb.\nKaggle\nTerabyte\nTraining Time Normalized to \nBaseline(Lower is Better)\nTT-Rank = 8\nTT-Rank = 16\nTT-Rank = 32\nTT-Rank = 64\n1.30\n1.53\nFigure 7. TT-Rec training time comparison across TT-ranks and\nthe setting of TT-Emb. Baseline takes 12.14ms/iter on Kaggle, and\n12.64ms/iter on Terabyte.\nthe model size reduction from 2.6 to 95.5\u00d7 while the vali-\ndation accuracy degrades from 80.975% to 80.682%. Note,\neven though the validation accuracy is lowered, the model\naccuracy (TT-Emb. of 7) still outperforms the uncompressed\nbaseline of 80.45%.\nUsing larger TT-ranks produces more accurate models at\nthe expense of lower compression ratios. We notice that,\nalthough mathematically larger TT-ranks should produce\nmore accurate approximations to the full tensor, increasing\nthe rank does not always compensate the loss of accuracy.\nWe believe that such accuracy loss is caused by the weight\ninitialization distribution, as we describe next.\nFigure 6(c) presents the TT-Rec accuracy results using the\ndifferent weight initialization strategies, described in \u00a73.2.\nRecall, the model accuracy difference between TT-Rec and\nthe baseline strongly correlates with the distance between\nthe distribution of the full matrix generated by the set of\nthe tensor cores and the uniform distribution. Thus, the\nsampled Gaussian distribution is expected to outperform the\nother distributions as the full matrix generated by TT-Rec\napproximate N(0, 1\n3n) the best. The accuracy results in\nFigure 6(c) verify this expectation empirically.\nEmbedding Table Size\nCost [GFLOPs or GB]\n1E-4\n1E-3\n1E-2\n1E-1\n1E+0\n1E+1\n1E+2\n0.0E+0\n2.5E+6\n5.0E+6\n7.5E+6\n1.0E+7\nT3nsor Comp\nTT-Rec Comp\nT3nsor Mem\nTT-Rec Mem\nFigure 8. System resource requirement of T3nsor and TT-Rec.\n6.3\nTraining Time Performance of TT-Rec\nTo have a full picture of TT-Rec\u2019s potential, we present the\ntraining time performance of TT-Rec next. Figure 7 depicts\nthe normalized training time of TT-Rec (y-axis), using TT-\nranks of [8, 16, 32, 64] across the TT-Emb. settings of [3, 5,\n7] (x-axis). Higher model size reduction ratios come with\nhigher training time overheads. Increasing the number of\nthe large embedding tables trained in the TT-Emb. format\nfrom 3 to 7 reduces the model sizes by 46.5 and 37.4\u00d7\nfor Kaggle and Terabyte, respectively, while the training\ntime using the optimal TT-rank increases by 12.5% and\n11.8%, respectively. Depending on the importance of the\nthree axes\u2014memory capacity requirement, model quality\nand training time performance for DLRM training\u2014TT-\nRec offers a \ufb02exible design space that can be navigated\naccording to the desired optimization goal.\n6.4\nTT-Embedding Kernel Implementation Ef\ufb01ciency\nTo quantify the ef\ufb01ciency of our TT-Rec implementation,\nwe compare the performance of the TT-Embedding kernel\nwith the baseline PyTorch EmbeddingBag operator (Paszke\net al., 2019) and the state-of-the-art TT embedding library,\ncalled T3nsor (Hrinchuk et al., 2020), for word embedding\nof NLP use cases. Figure 8 shows that our TT-Embedding\nimplementation is order-of-magnitude more ef\ufb01cient than\nthat of T3nsor, from the perspectives of compute (repre-\nTT-Rec: Tensor Train Compression for Deep Learning Recommendation Models\nTraining Progress [%]\n% Difference\n0.5%\n1.0%\n5.0%\n10.0%\n50.0%\n100.0\n20\n40\n60\n80\n100\nTB EMB1\nTB EMB2\nTB EMB3\nKG EMB1\nKG EMB2\nKG EMB3\nFigure 9. The set of frequently-accessed embedding rows over time\nstabilize at around 5% and 50% of the training run for Terabyte\nand Kaggle, respectively.\nsented by circle) and memory (represented by square) re-\nquirement, over the number of rows in embedding table\n(x-axis). T3nsor decompresses embedding tables on the\n\ufb02y; thus, it requires the same amount of memory footprint\nduring training as that of the PyTorch Embedding Bag op-\nerator. Our implementation achieves a memory footprint\nreduction of #EmbRows\nBatchSize , yielding roughly 10,000\u00d7 lower\nmemory footprint requirement as compared to T3nsor and\nthe PyTorch implementation. The overall training time of\nTT-Rec is on par with that of the baseline using the PyTorch\nEmbedding Bag operator (Figure 7; TT-Emb. of 3; Kaggle),\nand is 2.4\u00d7 faster than T3nsor on average.\n6.5\nAnalysis for Feature Reuse Potential and TT-Rec\nCache Performance\nAs described in \u00a74.2, we introduce a cache structure in TT-\nRec to capture the small subset of embedding rows with\nmuch higher degree of locality. Based on the available\nmemory capacity on the training system, TT-Rec caches\nan uncompressed version of frequently-accessed rows to\nreduce the computation need.\nTo illustrate the feature reuse potential in the context of\nTT-Rec, Figure 9 depicts the percentage of changes in the\nset of the most-frequently-accessed 10k embedding rows\nover the training run, for the three largest embedding tables\n(EMB1, EMB2, and EMB3). We count the cumulative row\naccess frequencies every 3% of training progress and mea-\nsure the difference between each consecutive points (y-axis\nof Figure 9 in the log-scale). This difference is indicative\nof training phase stability: the lower the difference is, the\nmore stable the set of frequently-accessed rows is.\nFigure 10(a) shows the impact of warm-up iterations on the\nTT-Rec training time and model accuracy. During the warm-\nup period, the cache structure is being \ufb01lled up with most-\nfrequently-accessed embedding vectors, using the aforemen-\ntioned LFU replacement. Thus, the longer the warm-up\nperiod is, the better the TT-Rec cache captures the most-\nfrequently-accessed embedding vectors and the higher the\ncache hit rate is for the remaining training iterations.\nFor Kaggle, as the warm-up iterations increase from 10% to\n30% of the total training iterations, the total training time\nincreases by 33%. This is because the training time speedup\nfrom the slightly higher cache hit rate (from the warm-up\nperiod) is not suf\ufb01cient to compensate the warm-up time\noverhead. As warm-up continues to increase beyond 30%\nof the training, we start seeing the training time overhead\nto decrease. This happens as the hit rate improves with\nmore warm-up iterations. In contrast, with Terabyte (the\nlarge industry-scale data set), TT-Rec cache can effectively\nand consistently reduce the model training time across the\ndifferent warm-up iterations. It improves the end-to-end\nmodel training time by up-to 19% with negligible accuracy\nimpact. Overall, with the cache support, TT-Rec receives\nadditional accuracy improvement of 0.09% on Kaggle and\n0.02% on Terabyte.\nAnother important design parameter for TT-Rec is the size\nof the LFU cache. Figure 10(b) shows the corresponding\nmodel training time and accuracy over the cache sizes rang-\ning from 0.01% to 10% of the respective embedding tables.\nFor Kaggle and Terabyte, devoting 0.01% worth of the em-\nbedding table memory requirement is suf\ufb01cient.\n6.6\nTT-Rec Performance with Large Pooling Factors\nTo understand Rec-TT training performance for other cat-\negories of DLRMs that are embedding-dominated (Gupta\net al., 2019; Gupta et al., 2020; Ke et al., 2020), we develop\na suite of microbenchmarks to synthetically generate em-\nbedding vectors with a polling factor (P) of 10 and 100. The\npooling factor is de\ufb01ned as the average number of embed-\nding lookups required per training sample. The larger P, the\nmore embedding vectors are looked up and gathered, and\nthe embedding operations are more expensive. Both Kaggle\nand Terabyte correspond to P = 1.\nFigure 11 compares the performance of the non-cached TT-\nRec kernel with PyTorch EmbeddingBag for P =[1, 10, 100]\nacross various TT-ranks. As seen in Figure 11, the perfor-\nmance per training sample is better as P increases. This\nis because as the number of embedding lookups increase,\nthe overhead of EmbeddingBag is amortized, in both the\nbaseline and TT-Rec cases. Furthermore, the performance\ngap between the non-cached TT-Rec kernel and Embedding-\nBag increases as P increases. This is because there exists\nhigher reuses of embedding vectors when P is larger and\nEmbeddingBag bene\ufb01ts from such reuse.\nTo reduce the execution time of TT-Rec as P increases, Fig-\nure 12 compares the performance of TT-Rec with caching\nenabled with EmbeddingBag. To quantify the timing per-\nformance per training sample, we synthetically generate\ndata samples to control the cache hit rate. As expected, as\nthe cache hit rate increases (the x-axis), the performance of\nTT-Rec improves and eventually outperforms the baseline\nTT-Rec: Tensor Train Compression for Deep Learning Recommendation Models\nWarmup Iterations [% of Total Iterations]\nNormalized Execution Time\nAccuracy Difference [%]\n0.50\n0.75\n1.00\n1.25\n1.50\n-0.025\n0.000\n0.025\n0.050\n0.075\n0.100\n10\n20\n30\n40\n50\nKaggle\nTerabyte\nKaggle-Accuracy\nTerabyte-Accuracy\nCache Size [% of Emb. Table Rows]\nNormalized Execution Time\nAccuracy Difference [%]\n0.50\n0.75\n1.00\n1.25\n1.50\n-0.025\n0.000\n0.025\n0.050\n0.075\n0.100\n10\n1\n0.5\n0.1\n0.05\n0.01\nKaggle\nTerabyte\nKaggle-Accuracy\nTerabyte-Accuracy\nFigure 10. The impact of warm-up iterations on the TT-Rec training time and model accuracy (left). The model training time and accuracy\nover the cache sizes with respect to the embedding table (right).\n0\n0.5\n1\n1.5\n2\n2.5\nP = 1\nP = 10\nP = 100\nTime / #-Embedding \nLookups (ns)\nBaseline\nTT-Rank = 8\nTT-Rank = 16\nTT-Rank = 32\nFigure 11. Performance of TT-Rec for MLPerf-DLRM (P of 1)\nand embedding-dominated DLRMs (P of 10 and 100).\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.1\n0.3\n0.5\n0.7\n0.9\n1.1\nTime / #-Embedding \nLookup (us)\nCache Hite Rate\nTT-Rec (P=10)\nBaseline (P=10)\nTT-Rec (P=100)\nBaseline (P=100)\nFigure 12. Performance comparison for TT-Rec\u2019s EmbeddingBag\nkernel and the baseline.\nEmbeddingBag when the cache hit rate reaches 90%.\n7\nRELATED WORK\nGeneral Model Compression Techniques: Among the\nmany compression techniques for training, the commonly-\nused ones include magnitude pruning (Zhu & Gupta, 2017),\nvariational dropout (Kingma & Welling, 2013), and l0 reg-\nularization (Louizos et al., 2017). Other efforts propose to\nimpose structured sparsity in model weights upfront (Child\net al., 2019; Gray et al., 2017). Such approaches can sig-\nni\ufb01cantly reduce both training and inference cost, but have\nnot been proven as effective solutions for deep learning rec-\nommendation models. Furthermore, while these methods\nare generally applicable, the prior works are fundamentally\ndifferent from our proposed TT-decomposition approach.\nEmbedding Table Compression Techniques: For embed-\nding, the seminal work by Weinberger et al. examines fea-\nture hashing, that allows multiple elements to map to the\nsame embedding vector; thus, it reduces the embedding\nspace (Weinberger et al., 2009). However, hash collisions\nyield signi\ufb01cant accuracy losses. For example, Zhao et al.\nobserved an intolerable degree of accuracy loss if hashing\nwere applied to TB-scale recommendation models (Zhao\net al., 2020). Guan et al. propose a 4-bit quantization\nscheme to compress pre-trained models for inference (Guan\net al., 2019). This design is feasible for recommendation in-\nference although, quantization for training is more challeng-\ning and often comes with accuracy tradeoff. Other works\nalso explore the potential of low-rank approximation on the\nembedding tables (Ghaemmaghami et al., 2020; Hrinchuk\net al., 2020) but experience critical accuracy degradation.\nAnd none provide a computationally-ef\ufb01cient interface for\nindustry-scale DLRMs.\nTensorization: Tensor methods have been extensively stud-\nied for compressing DNNs. One of the most common\nmethod is the Tucker factorization (Cohen et al., 2016),\nwhich can generate high-quality DNNs when compressing\nfully-connected layers. Tensor Train (TT) and Tensor Ring\n(TR) decomposition techniques have been recently studied\nin the context of DNNs (Hawkins & Zhang, 2019; Wang\net al., 2018). But previous work has explored the accu-\nracy trade-off for fully-connected and convolution layers\nonly. In particular, TT decomposition offers a structural\nway to compress DNNs and thus is capable of preserving\nthe DNN weights. TR can preserve the weights with moder-\nately lower compression ratios than that of TT (Wang et al.,\n2018). Despite interest in TT-based methods, to the best of\nour knowledge, ours is the \ufb01rst to consider them in the con-\ntext of DLRMs. In our analysis, we comprehensively study\nthe design space of how memory size reduction, model qual-\nity, and training time overheads trade-off. Finally, we also\npresent an ef\ufb01cient implementation for TT-Rec, which we\nwill release as open source upon the acceptance of this work.\nTT-Rec: Tensor Train Compression for Deep Learning Recommendation Models\n8\nCONCLUSION\nThe spirit of TT-Rec is to use principled, parameterized al-\ngorithmic methods to help control the explosive demands on\ncomputational infrastructure. This strategy complements in-\nnovations in the infrastructure itself. TT-Rec speci\ufb01cally at-\ntacks the considerable memory requirements of embedding\nlayers of modern recommendation models, whose memory\nrequirements in industrial applications at scale can require\nhundreds of GBs to TBs of memory. TT-Rec replaces oth-\nerwise large embedding tables with a sequence of matrix\nproducts, reducing the total model memory capacity re-\nquirement by 112 times with only a small amount of 13.9%\ntraining time overhead while maintaining the same model\naccuracy as the baseline. Such signi\ufb01cant memory capacity\nreductions can be achieved with a relatively small increase\nin training time through clever caching strategies, making\nonline recommendation model training more practical.\nREFERENCES\nAlibaba. Alibaba unveils ai chip to enhance cloud comput-\ning power. https://www.alibabagroup.com/\nen/news/article?news=p190925, 2019.\nAmazon.\nAWS Inferentia:\nHigh performance ma-\nchine\nlearning\ninference\nchip,\ncustom\ndesigned\nby\nAWS.\nhttps://aws.amazon.com/\nmachine-learning/inferentia/, 2019.\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,\nJ., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., et al. Language models are few-shot learners.\narXiv preprint arXiv:2005.14165, 2020.\nChao,\nC. and Saeta,\nB.\nCloud TPU: Codesign-\ning architecture and infrastructure,\n2019.\nURL\nhttps://www.hotchips.org/hc31/HC31_\nT3_Cloud_TPU_Codesign.pdf.\nChild, R., Gray, S., Radford, A., and Sutskever, I. Gen-\nerating long sequences with sparse transformers. arXiv\npreprint arXiv:1904.10509, 2019.\nChung, E., Fowers, J., Ovtcharov, K., Papamichael, M.,\nCaul\ufb01eld, A., Massengill, T., Liu, M., Ghandi, M., Lo, D.,\nReinhardt, S., Alkalay, S., Angepat, H., Chiou, D., Forin,\nA., Burger, D., Woods, L., Weisz, G., Haselman, M., and\nZhang, D. Serving dnns in real time at datacenter scale\nwith project brainwave. IEEE Micro, 38:8\u201320, March\n2018.\nCohen, N., Sharir, O., and Shashua, A. On the expressive\npower of deep learning: A tensor analysis. In Conference\non learning theory, pp. 698\u2013728, 2016.\nFowers, J., Ovtcharov, K., Papamichael, M., Massen-\ngill, T., Liu, M., Lo, D., Alkalay, S., Haselman, M.,\nAdams, L., Ghandi, M., Heil, S., Patel, P., Sapek,\nA., Weisz, G., Woods, L., Lanka, S., Reinhardt, S.,\nCaul\ufb01eld, A., Chung, E., and Burger, D.\nA con\ufb01g-\nurable cloud-scale dnn processor for real-time ai. In\nProceedings of the 45th International Symposium on\nComputer Architecture, 2018. ACM, June 2018.\nGhaemmaghami, B., Deng, Z., Cho, B., Orshansky, L.,\nSingh, A. K., Erez, M., and Orshansky, M. Training\nwith multi-layer embeddings for model reduction. arXiv\npreprint arXiv:2006.05623, 2020.\nGray, S., Radford, A., and Kingma, D. P. Gpu kernels for\nblock-sparse weights. arXiv preprint arXiv:1711.09224,\n3, 2017.\nGuan, H., Malevich, A., Yang, J., Park, J., and Yuen, H. Post-\ntraining 4-bit quantization on embedding tables, 2019.\nGupta, U., Wu, C.-J., Wang, X., Naumov, M., Reagen, B.,\nBrooks, D., Cottel, B., Hazelwood, K., Jia, B., Lee, H.-\nH. S., Malevich, A., Mudigere, D., Smelyanskiy, M.,\nXiong, L., and Zhang, X. The Architectural Implications\nof Facebook\u2019s DNN-based Personalized Recommenda-\ntion. 2019. URL http://arxiv.org/abs/1906.\n03109.\nGupta, U., Hsia, S., Saraph, V., Wang, X., Reagen, B.,\nWei, G., Lee, H. S., Brooks, D., and Wu, C.\nDeep-\nrecsys: A system for optimizing end-to-end at-scale\nneural recommendation inference. In 2020 ACM/IEEE\n47th Annual International Symposium on Computer\nArchitecture (ISCA), pp. 982\u2013995, 2020.\nHawkins, C. and Zhang, Z. Bayesian Tensorized Neural\nNetworks with Automatic Rank Selection. 2019. URL\nhttp://arxiv.org/abs/1905.10478.\nHazelwood, K. Deep learning: It\u2019s not all about recognizing\ncats and dogs, 2020. URL https://databricks.\ncom/speaker/kim-hazelwood.\nHazelwood, K., Bird, S., Brooks, D., Chintala, S., Diril,\nU., Dzhulgakov, D., Fawzy, M., Jia, B., Jia, Y., Kalro,\nA., Law, J., Lee, K., Lu, J., Noordhuis, P., Smelyan-\nskiy, M., Xiong, L., and Wang, X. Applied machine\nlearning at facebook: A datacenter infrastructure perspec-\ntive. In 2018 IEEE International Symposium on High\nPerformance Computer Architecture (HPCA), pp. 620\u2013\n629, Feb 2018. doi: 10.1109/HPCA.2018.00059.\nHrinchuk, O., Khrulkov, V., Mirvakhabova, L., Orlova, E.,\nand Oseledets, I. Tensorized embedding layers for ef\ufb01-\ncient model compression, 2020.\nTT-Rec: Tensor Train Compression for Deep Learning Recommendation Models\nJouppi, N. P., Young, C., Patil, N., Patterson, D., Agrawal,\nG., Bajwa, R., Bates, S., Bhatia, S., Boden, N., Borchers,\nA., Boyle, R., Cantin, P., Chao, C., Clark, C., Coriell, J.,\nDaley, M., Dau, M., Dean, J., Gelb, B., Ghaemmaghami,\nT. V., Gottipati, R., Gulland, W., Hagmann, R., Ho, C. R.,\nHogberg, D., Hu, J., Hundt, R., Hurt, D., Ibarz, J., Jaffey,\nA., Jaworski, A., Kaplan, A., Khaitan, H., Killebrew, D.,\nKoch, A., Kumar, N., Lacy, S., Laudon, J., Law, J., Le,\nD., Leary, C., Liu, Z., Lucke, K., Lundin, A., MacKean,\nG., Maggiore, A., Mahony, M., Miller, K., Nagarajan, R.,\nNarayanaswami, R., Ni, R., Nix, K., Norrie, T., Omernick,\nM., Penukonda, N., Phelps, A., Ross, J., Ross, M., Salek,\nA., Samadiani, E., Severn, C., Sizikov, G., Snelham, M.,\nSouter, J., Steinberg, D., Swing, A., Tan, M., Thorson,\nG., Tian, B., Toma, H., Tuttle, E., Vasudevan, V., Walter,\nR., Wang, W., Wilcox, E., and Yoon, D. H. In-datacenter\nperformance analysis of a tensor processing unit.\nIn\n2017 ACM/IEEE 44th Annual International Symposium\non Computer Architecture (ISCA), pp. 1\u201312, 2017.\nKe, L., Gupta, U., Cho, B. Y., Brooks, D., Chandra, V., Diril,\nU., Firoozshahian, A., Hazelwood, K., Jia, B., Lee, H. S.,\nLi, M., Maher, B., Mudigere, D., Naumov, M., Schatz, M.,\nSmelyanskiy, M., Wang, X., Reagen, B., Wu, C., Hemp-\nstead, M., and Zhang, X. Recnmp: Accelerating person-\nalized recommendation with near-memory processing. In\n2020 ACM/IEEE 47th Annual International Symposium\non Computer Architecture (ISCA), pp. 790\u2013803, 2020.\nKingma, D. P. and Welling, M. Auto-encoding variational\nbayes. arXiv preprint arXiv:1312.6114, 2013.\nLee,\nK.\nand\nRao,\nV.\nAccelerating\nface-\nbook\u2019s\ninfrastructure\nwith\napplication-speci\ufb01c\nhardware.\nhttps://engineering.\nfb.com/data-center-engineering/\naccelerating-infrastructure/,\nMarch\n2019.\nLouizos, C., Welling, M., and Kingma, D. P. Learning\nsparse neural networks through l 0 regularization. arXiv\npreprint arXiv:1712.01312, 2017.\nMattson, P., Cheng, C., Coleman, C., Diamos, G., Micike-\nvicius, P., Patterson, D., Tang, H., Wei, G.-Y., Bailis, P.,\nBittorf, V., Brooks, D., Chen, D., Dutta, D., Gupta, U.,\nHazelwood, K., Hock, A., Huang, X., Jia, B., Kang, D.,\nKanter, D., Kumar, N., Liao, J., Narayanan, D., Oguntebi,\nT., Pekhimenko, G., Pentecost, L., Reddi, V. J., Robie,\nT., John, T. S., Wu, C.-J., Xu, L., Young, C., and Za-\nharia, M. Mlperf training benchmark. arXiv preprint\narXiv:1910.01500, 2019.\nMattson, P., Reddi, V. J., Cheng, C., Coleman, C., Di-\namos, G., Kanter, D., Micikevicius, P., Patterson, D.,\nSchmuelling, G., Tang, H., Wei, G.-w., and Wu, C.-J.\nMlperf: An industry standard benchmark suite for ma-\nchine learning performance. IEEE Micro, 40(2):8\u201316,\n2020.\nMLPerf. MLPerf Training. https://mlperf.org/\ntraining-overview/#overview, 2020.\nNaumov, M., Mudigere, D., Shi, H.-J. M., Huang, J., Sun-\ndaraman, N., Park, J., Wang, X., Gupta, U., Wu, C.-J.,\nAzzolini, A. G., Dzhulgakov, D., Mallevich, A., Cher-\nniavskii, I., Lu, Y., Krishnamoorthi, R., Yu, A., Kon-\ndratenko, V., Pereira, S., Chen, X., Chen, W., Rao,\nV., Jia, B., Xiong, L., and Smelyanskiy, M.\nDeep\nLearning Recommendation Model for Personalization\nand Recommendation Systems.\n2019.\nURL http:\n//arxiv.org/abs/1906.00091.\nNaumov, M., Kim, J., Mudigere, D., Sridharan, S., Wang,\nX., Zhao, W., Yilmaz, S., Kim, C., Yuen, H., Ozdal, M.,\nNair, K., Gao, I., Su, B.-Y., Yang, J., and Smelyanskiy, M.\nDeep learning training in facebook data centers: Design\nof scale-up and scale-out systems, 2020.\nNovikov, A., Podoprikhin, D., Osokin, A., and Vetrov,\nD. Tensorizing neural networks. Advances in Neural\nInformation Processing Systems, 2015-Janua:442\u2013450,\n2015. ISSN 10495258.\nNVIDIA.\n2.8.13.\ncublasgemmbatchedex,\ncublas\n::\nCuda\ntoolkit\ndocumentation,\n2020.\nURL\nhttps://docs.nvidia.com/cuda/cublas/\nindex.html#cublas-GemmBatchedEx.\nNVIDIA. NVIDIA DGX A100: the universal system for AI\ninfrastructure, 2020a. URL https://www.nvidia.\ncom/en-us/data-center/dgx-a100/.\nNVIDIA. NVIDIA A100 Tensor Core GPU, 2020b. URL\nhttps://www.nvidia.com/content/dam/\nen-zz/Solutions/Data-Center/a100/pdf/\nnvidia-a100-datasheet.pdf.\nOseledets, I. V. Tensor-train decomposition. 2011 Society\nfor Industrial and Applied Mathematics, 45(4):1600\u2013\n1621, 2011.\nOvtcharov, K., Ruwase, O., Kim, J.-Y., Fowers, J., Strauss,\nK., and Chung, E. Toward accelerating deep learning\nat scale using specialized hardware in the datacenter.\nIn Proceedings of the 27th IEEE HotChips Symposium\non High-Performance Chips (HotChips 2015). IEEE, Au-\ngust 2015.\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury,\nJ., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N.,\nAntiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito,\nZ., Raison, M., Tejani, A., Chilamkurthy, S., Steiner,\nB., Fang, L., Bai, J., and Chintala, S.\nPytorch: An\nTT-Rec: Tensor Train Compression for Deep Learning Recommendation Models\nimperative\nstyle,\nhigh-performance\ndeep\nlearning\nlibrary. In Wallach, H., Larochelle, H., Beygelzimer,\nA., d'Alch\u00b4e-Buc, F., Fox, E., and Garnett, R. (eds.),\nAdvances in Neural Information Processing Systems\n32, pp. 8024\u20138035. Curran Associates, Inc., 2019.\nURL\nhttp://papers.neurips.cc/paper/\n9015-pytorch-an-imperative-style-high-performance-deep-learning-library.\npdf.\nReddi, V. J., Cheng, C., Kanter, D., Mattson, P.,\nSchmuelling, G., Wu, C.-J., Anderson, B., Breughe, M.,\nCharlebois, M., Chou, W., Chukka, R., Coleman, C.,\nDavis, S., Deng, P., Diamos, G., Duke, J., Fick, D., Gard-\nner, J. S., Hubara, I., Idgunji, S., Jablin, T. B., Jiao, J.,\nJohn, T. S., Kanwar, P., Lee, D., Liao, J., Lokhmotov, A.,\nMassa, F., Meng, P., Micikevicius, P., Osborne, C., Pekhi-\nmenko, G., Rajan, A. T. R., Sequeira, D., Sirasao, A.,\nSun, F., Tang, H., Thomson, M., Wei, F., Wu, E., Xu, L.,\nYamada, K., Yu, B., Yuan, G., Zhong, A., Zhang, P., and\nZhou, Y. Mlperf inference benchmark. arXiv preprint\narXiv:1911.02549, 2019.\nRusu, A. A., Rao, D., Sygnowski, J., Vinyals, O., Pascanu,\nR., Osindero, S., and Hadsell, R. Tensorized Embedding\nLayers for Ef\ufb01cient Model Compression. pp. 1\u201315, 2020.\nWang, W., Sun, Y., Eriksson, B., Wang, W., and Aggarwal,\nV. Wide Compression: Tensor Ring Nets. Proceedings\nof the IEEE Computer Society Conference on Computer\nVision and Pattern Recognition, pp. 9329\u20139338, 2018.\nISSN 10636919. doi: 10.1109/CVPR.2018.00972.\nWeinberger, K., Dasgupta, A., Langford, J., Smola, A., and\nAttenberg, J. Feature hashing for large scale multitask\nlearning. In Proceedings of the 26th annual international\nconference on machine learning, pp. 1113\u20131120, 2009.\nWold, S., Esbensen, K., and Geladi, P. Principal compo-\nnent analysis. Chemometrics and intelligent laboratory\nsystems, 2(1-3):37\u201352, 1987.\nWu, C.-J., Burke, R., Chi, E. H., Konstan, J., McAuley,\nJ., Raimond, Y., and Zhang, H. Developing a recom-\nmendation benchmark for mlperf training and inference,\n2020.\nXue, J., Li, J., and Gong, Y. Restructuring of deep neural\nnetwork acoustic models with singular value decomposi-\ntion. In Interspeech, pp. 2365\u20132369, 2013.\nYang, Y., Krompass, D., and Tresp, V. Tensor-train re-\ncurrent neural networks for video classi\ufb01cation. 34th\nInternational Conference on Machine Learning, ICML\n2017, 8:5929\u20135938, 2017.\nZhao, W., Xie, D., Jia, R., Qian, Y., Ding, R., Sun, M., and\nLi, P. Distributed hierarchical gpu parameter server for\nmassive scale deep learning ads systems. arXiv preprint\narXiv:2003.05622, 2020.\nZhu, M. and Gupta, S. To prune, or not to prune: exploring\nthe ef\ufb01cacy of pruning for model compression. arXiv\npreprint arXiv:1710.01878, 2017.\nTT-Rec: Tensor Train Compression for Deep Learning Recommendation Models\nAPPENDIX\nA\nALGORITHM DETAILS\nAlgorithm 1 and Algorithm 2 below shows the details of the forward and back-propagation algorithm for TT-Rec embedding\ntables. Algorithm 3 shows how the TT-cores are initialized using our proposed Sampled Gaussian method.\nAlgorithm 1: Forward prop. of TT-Embedding\n1 while S idx < offsets[m] do\n2\nE idx = min(offsets[S idx + B], offsets[m])\n3\nfor k = S idx to E idx do\n4\nidx[j][k] = ik\nj in Eqn( 7)\n5\na[k] = &G1[idx[1][k]][0]\n6\nb[k] = &G0[idx[0][k]][0]\n7\nc[k] = &tr0[k][0]\n8\na[k + B] = &G2[idx[2][k]][0]\n9\nb[k + B] = &tr0[k][0]\n10\nc[k + B] = &tr1[k][0]\n11\nend\n12\nfor j = 0 to d-2 do\n13\n\u25b7Batched GEMM kernel calls\n14\nc[jB : (j + 1)B] = a[jB : (j + 1)B] \u2217b[jB : (j + 1)B]\n15\nend\n16\n\u25b7Reduce embedding rows to output\n17\noutput[S idx : E idx] = Poffsets[i+1]\nj=offsets[i] c[j]\n18\nS idx = E idx\n19 end\nTT-Rec: Tensor Train Compression for Deep Learning Recommendation Models\nAlgorithm 2: Backward prop. of TT-Rec Embeddings\n1 while S idx < offsets[m] do\n2\nE idx = min(offsets[S idx + B], offsets[m])\n3\nRecompute for tri\u2019s as in Algorithm 1\n4\nfor k = S idx to E idx do\n5\nidx[j][k] = ik\nj in Eqn( 7)\n6\na0[k] = &G0[idx[0][k]][0]\n7\nb0[k] = &tr0[k][0]\n8\nc0[k] = &tr G1[k][0]\n9\na1[k] = &tr0[k][0]\n10\nb1[k] = &G1[idx[1][k]][0]\n11\nc1[k] = &tr G0[k][0]\n12\na0[k + B] = &tr0[k][0]\n13\nb0[k + B] = &dx\n14\nc0[k + B] = &tr G2[k][0]\n15\na1[k + B] = &dx\n16\nb1[k + B] = &G2[idx[2][k]][0]\n17\nc1[k + B] = &tr0[k][0]\n18\nend\n19\nfor j = d-2 to 0 do\n20\n\u25b7Batched GEMM calls to compute \u2202Gj\n21\nc0[jB : (j + 1)B] = a0[jB : (j + 1)B] \u2217b0[jB : (j + 1)B]\n22\n\u25b7Batched GEMM calls to compute \u2202x\n23\nc1[jB : (j + 1)B] = a1[jB : (j + 1)B] \u2217b1[jB : (j + 1)B]\n24\n\u2202Gj[idx[k]]+ = tr Gj[k]\n25\nend\n26\nstart idx = end idx\n27 end\nAlgorithm 3: Sampled Gaussian initialization.\n1 for d = 0 to tt-dim do\n2\nGd = random.normal(0,1)\n3\nfor each entry Gd(i, j, k, l) in Gd do\n4\nwhile Gd(i, j, k, l) \u22642 do\n5\nGd(i, j, k, l) = random.normal(0,1)\n6\nGd/ = (\np\n1/3n)1/d\n",
        "context": "three axes\u2014memory capacity requirement, model quality\nand training time performance for DLRM training\u2014TT-\nRec offers a \ufb02exible design space that can be navigated\naccording to the desired optimization goal.\n6.4\nTT-Embedding Kernel Implementation Ef\ufb01ciency\nerwise large embedding tables with a sequence of matrix\nproducts, reducing the total model memory capacity re-\nquirement by 112 times with only a small amount of 13.9%\ntraining time overhead while maintaining the same model\nDLRMs.\n\u2022 Our in-depth design space characterization shows the im-\nportance of choosing the right number of embedding ta-\nbles to compress and the dimension of the compressed\ntensors. In particular, we quantify the potential trade-off"
    },
    {
        "id": 13,
        "title": "Embedding compression in recommender systems: A survey",
        "author": [
            "S. Li",
            "H. Guo",
            "X. Tang",
            "R. Tang",
            "L. Hou",
            "R. Li",
            "R. Zhang"
        ],
        "year": "2024",
        "doi": null,
        "in_text_citation": "[13]",
        "sentence": "These methods primarily focus on reducing the embedding size [13], while another solution is to modify the training process to save memory.",
        "abstract": "To alleviate the problem of information explosion, recommender systems are\nwidely deployed to provide personalized information filtering services.\nUsually, embedding tables are employed in recommender systems to transform\nhigh-dimensional sparse one-hot vectors into dense real-valued embeddings.\nHowever, the embedding tables are huge and account for most of the parameters\nin industrial-scale recommender systems. In order to reduce memory costs and\nimprove efficiency, various approaches are proposed to compress the embedding\ntables. In this survey, we provide a comprehensive review of embedding\ncompression approaches in recommender systems. We first introduce deep learning\nrecommendation models and the basic concept of embedding compression in\nrecommender systems. Subsequently, we systematically organize existing\napproaches into three categories, namely low-precision, mixed-dimension, and\nweight-sharing, respectively. Lastly, we summarize the survey with some general\nsuggestions and provide future prospects for this field.",
        "full_text": "130\nEmbedding Compression in Recommender Systems: A Survey\nSHIWEI LI\u2217, Huazhong University of Science and Technology, China\nHUIFENG GUO\u2217, Huawei Noah\u2019s Ark Lab, China\nXING TANG\u2020, Tencent, China\nRUIMING TANG and LU HOU, Huawei Noah\u2019s Ark Lab, China\nRUIXUAN LI\u2021, Huazhong University of Science and Technology, China\nRUI ZHANG\u2021, ruizhang.info, China\nTo alleviate the problem of information explosion, recommender systems are widely deployed to provide\npersonalized information filtering services. Usually, embedding tables are employed in recommender systems\nto transform high-dimensional sparse one-hot vectors into dense real-valued embeddings. However, the\nembedding tables are huge and account for most of the parameters in industrial-scale recommender systems.\nIn order to reduce memory costs and improve efficiency, various approaches are proposed to compress the\nembedding tables. In this survey, we provide a comprehensive review of embedding compression approaches\nin recommender systems. We first introduce deep learning recommendation models and the basic concept\nof embedding compression in recommender systems. Subsequently, we systematically organize existing\napproaches into three categories, namely low-precision, mixed-dimension, and weight-sharing, respectively.\nLastly, we summarize the survey with some general suggestions and provide future prospects for this field.\nCCS Concepts: \u2022 Information systems \u2192Recommender systems.\nAdditional Key Words and Phrases: recommender systems; embedding tables; model compression; survey\nACM Reference Format:\nShiwei Li, Huifeng Guo, Xing Tang, Ruiming Tang, Lu Hou, Ruixuan Li, and Rui Zhang. 2024. Embedding\nCompression in Recommender Systems: A Survey. ACM Comput. Surv. 56, 5, Article 130 (January 2024),\n21 pages. https://doi.org/10.1145/3637841\n1\nINTRODUCTION\nTo alleviate the problem of information explosion, recommender systems [55, 61] are extensively\ndeployed to provide personalized information filtering services, including online shopping [80],\nadvertising systems [46] and so on. Meanwhile, deep learning techniques have shown impressive\ncapabilities in capturing user preferences for candidate items. Thereupon, both the industry and\nresearch communities have proposed a variety of deep learning recommendation models (DLRMs)\n\u2217Shiwei Li and Huifeng Guo contributed equally to this research.\n\u2020This work was done when Xing Tang worked at Huawei Noah\u2019s Ark Lab.\n\u2021Ruixuan Li and Rui Zhang are the corresponding authors.\nAuthors\u2019 addresses: Shiwei Li, lishiwei@hust.edu.cn, Huazhong University of Science and Technology, Wuhan, China,\n430074; Huifeng Guo, huifeng.guo@huawei.com, Huawei Noah\u2019s Ark Lab, Shenzhen, China, 518129; Xing Tang, xing.tang@\nhotmail.com, Tencent, Shenzhen, China, 518054; Ruiming Tang, tangruiming@huawei.com; Lu Hou, houlu3@huawei.com,\nHuawei Noah\u2019s Ark Lab, Shenzhen, China, 518129; Ruixuan Li, rxli@hust.edu.cn, Huazhong University of Science and\nTechnology, Wuhan, China, 430074; Rui Zhang, rayteam@yeah.net, ruizhang.info, China.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and\nthe full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.\nAbstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\nprior specific permission and/or a fee. Request permissions from permissions@acm.org.\n\u00a9 2024 Association for Computing Machinery.\n0360-0300/2024/1-ART130 $15.00\nhttps://doi.org/10.1145/3637841\nACM Comput. Surv., Vol. 56, No. 5, Article 130. Publication date: January 2024.\narXiv:2408.02304v1  [cs.IR]  5 Aug 2024\n130:2\nShiwei Li and Huifeng Guo, et al.\nto enhance the performance of recommender systems, such as Wide & Deep [6] in Google Play,\nDIN [80] in Alibaba and DeepFM [18] in Huawei.\n1.1\nDeep Learning Recommendation Models\nRecommender systems are utilized for a diverse range of tasks, such as candidate item matching [30],\nclick-through rate (CTR) prediction [51, 52], and conversion rate (CVR) prediction [44]. For each of\nthese tasks, the employed DLRMs have undergone meticulous design processes to ensure optimal\nperformance. However, without loss of generality, most DLRMs follow the embedding table and\nneural network paradigm [18, 45, 57, 58], despite the specific design of the neural network component\nmay vary across different model architectures.\nFig. 1. The embedding table and neural network paradigm of deep learning recommendation models (DLRMs).\nNote that the neural network component may vary in different model architectures, here we only present the\nneural network with the classic dual tower architecture as an example.\nAs illustrated in Figure 1, the embedding table is responsible for converting input rows into\ndense embedding vectors. It is worth noting that the input rows of DLRMs typically consist of\ncategorical features, which are encoded as high-dimensional one-hot vectors. Each category feature\nwill be referred to as feature for short, and all features under the same category form a feature\nfield. Generally, each feature is associated with a unique embedding stored in the embedding table\nE \u2208R\ud835\udc5b\u00d7\ud835\udc51, where \ud835\udc5bdenotes the total number of features and \ud835\udc51denotes the embedding dimension.\nOn the other hand, the neural network is primarily engaged in interacting, processing, and\nanalyzing feature embeddings, along with making predictions. Recent studies [18, 29, 36, 40, 65, 70]\nhave consistently focused on optimizing the feature extraction capabilities of the neural networks.\nFor example, [18, 58] utilize product operators to model the feature interactions between different\nfeature fields. [36, 40] employ convolutions on embeddings to capture feature interactions of\narbitrary order. [65] introduces an additional attention network to assign varying importance to\ndifferent feature interactions. Additionally, [29, 70] automatically search for suitable interaction\nfunctions using AutoML techniques [21]. In this manuscript, we do not delve into the detailed\ndesign of the neural network component. Instead, we recommend referring to the works [61, 74, 75]\nfor a comprehensive understanding of the neural networks used in DLRMs.\nDespite incorporating various intricate designs, the neural network usually entails relatively\nshallow layers and a limited number of model parameters. In contrast, the embedding table occupies\nthe vast majority of model parameters. Especially in industrial-scale recommender systems, where\nthere are billions or even trillions of categorical features, the embedding table may take hundreds of\nACM Comput. Surv., Vol. 56, No. 5, Article 130. Publication date: January 2024.\nEmbedding Compression in Recommender Systems: A Survey\n130:3\nGB or even TB to hold [17]. For example, the size of embedding tables in Baidu\u2019s advertising systems\nreaches 10 TB [66]. As the scale of recommender systems perpetually expands, the continuous\ngrowth in the number of features will bring greater storage overhead.\n1.2\nEmbedding Compression in Recommender Systems\nIn addition to increasing storage overhead, larger embedding tables will also result in higher latency\nduring table lookup 1, which will reduce the efficiency of model training and inference. Therefore,\nto deploy the DLRMs with large embedding tables in real production environment efficiently and\neconomically, it is necessary to compress their embedding tables.\nHowever, embedding compression in DLRMs differs significantly from model compression in\nother fields, such as Computer Vision (CV) [8] and Natural Language Processing (NLP) [19]. These\ndifferences primarily manifest in three aspects: model architectures, properties of input data, and\nmodel size. Firstly, vision models and language models are usually very deep neural networks\nstacked by fully-connected layers, convolutional layers, or transformers. Consequently, compression\nmethods designed for these models focus on compressing the aforementioned modules rather than\nembedding tables. In contrast, DLRMs are typically shallow models, with the majority of parameters\nconcentrated in the embedding tables. Secondly, the input data of vision models and language models\nare usually images and texts, inherently containing abundant visual and semantic information that\ncan be leveraged for model compression. For example, [4] use the semantic information as a prior\nknowledge to compress the word embeddings, while [20] exploit the similarity between feature\nmaps derived from image inputs to compress convolution kernels. However, in recommender\nsystems, there is generally limited visual or semantic information available. Fortunately, DLRMs\npossess unique properties in the input data that can facilitate embedding compression. Specifically,\ncategorical features are organized in feature fields and often follow a highly skewed long-tail\ndistribution, with varying numbers of features in different fields. We can compress embedding\ntables based on feature frequency and field size. Thirdly, embedding tables of DLRMs are usually\nhundreds or even thousands of times larger than vision models or language models [53], which\npresents a more challenging and necessary task for compression.\nRecently, embedding compression has gained increasing attention in recommender systems,\nleading to the development and application of various embedding compression techniques for\nDLRMs. However, there is currently no comprehensive survey summarizing the methods em-\nployed for embedding compression. Therefore, the primary objective of this paper is to review\nand summarize representative research in this field. The embedding table can be regarded as a\nmatrix with three dimensions, that are the precision of weights, the dimension of embeddings,\nand the number of embeddings. To this end, we summary the embedding compression methods\ninto three categories according to the dimensions they compress, as illustrated in Figure 2. Firstly,\nlow-precision methods reduce the memory of each weight by decreasing its bit width. According\nto the size of bit width and its corresponding advantages, we further divide the low-precision meth-\nods into binarization and quantization. Secondly, mixed-dimension methods reduce the memory\nof specific embeddings by decreasing their dimensions and using mixed-dimension embeddings.\nAccording to the techniques of determining the embedding dimension for different features, we\ncategorize the mixed-dimension methods into rule-based approaches, NAS-based approaches, and\npruning. Thirdly, weight-sharing methods reduce the actual parameters of the embedding table by\nsharing weights among different embeddings. Considering that the number of features is given by\nthe dataset, a solution to reduce the number of embeddings is to reuse embeddings among features.\nFurthermore, we generalize the sharing to the weight level and define the weight-sharing methods\n1The process of retrieving an embedding from the embedding table based on the input feature or index.\nACM Comput. Surv., Vol. 56, No. 5, Article 130. Publication date: January 2024.\n130:4\nShiwei Li and Huifeng Guo, et al.\nEmbedding\nCompression\nLow-precision\nMixed-dimension\nWeight-sharing\nBinarization:\n[42], [77], DCF [73], DCMF [33], DPR [76], DFM [37]\nCIGAR [28], HashGNN [54], L2Q-GCN [5]\nQuantization:\n[16], [69], [66], ALPT [31]\nRule-based Approaches:\nMDE [15], CpRec [53]\nNAS-based Approaches:\nNIS [25], ESAPN [39], AutoIAS [59],\nAutoEmb [78], AutoDim [79], RULE [3], OptEmbed [43]\nPruning:\nDeepLight [10], DNIS [7], AMTL [68], PEP [41], SSEDS [48]\nHashing:\nQR [50], MEmCom [47], BCH [67], FDH [72], LMA [12], ROBE [11]\nVector Quantization:\nSaec [62], MGQE [26], xLightFM [24], LightRec [34], LISA [63]\nDecomposition:\nMLET [14], ANT [35], DHE [27], TT-Rec[71], LLRec [56], [64]\nFig. 2. Summary of representative studies on embedding compression in recommender systems.\nas generating embeddings with shared weights. According to the way embeddings are generated,\nwe categorize the mixed-dimension methods into hashing, vector quantization, and decomposition.\nWe will introduce the three primary categories in Sections 2, 3 and 4, respectively.\nNote that embeddings are fed into the neural network as representations of categorical features\nand form the foundations of DLRMs. Therefore, when compressing embeddings, it may affect\nthe model performance on many aspects, including model accuracy, inference efficiency, training\nefficiency, and training memory usage. We will discuss the pros and cons of different methods\nregarding these metrics at the end of each section. In Section 5, the survey is summarized, providing\ngeneral suggestions for different scenarios and discussing future prospects for this field.\n2\nLOW-PRECISION\nAs we all know, embedding weights are typically stored in the format of FP32 2 which occupies 32\nbits. To reduce the storage of each weight, low-precision approaches are developed to represent a\nweight with fewer bits. In particular, according to the bit width of weights, low-precision approaches\ncan be further divided into binarization and quantization.\n2.1\nBinarization\nBinarization is to compress a full-precision weight into a binary code that only occupy 1 bit. It\nis widely used in the embedding-based similarity search of recommender systems [28, 54], since\nthe binary embeddings have two distinct advantages compared to the full-precision ones: (1) less\nmemory or disk cost for storing embeddings; (2) higher inference efficiency as the similarity (i.e.,\ninner product) between binary embeddings can be calculated more efficiently through the Hamming\ndistance, which has been proved in [73].\n2The short of single-precision floating-point format.\nACM Comput. Surv., Vol. 56, No. 5, Article 130. Publication date: January 2024.\nEmbedding Compression in Recommender Systems: A Survey\n130:5\n[42, 77] pioneered to obtain binary embeddings in a two-stage (i.e, post-training) manner. Specif-\nically, they first learn a full-precision embedding table while ignoring the binary constraints, and\nthen perform binarization (e.g., \ud835\udc60\ud835\udc56\ud835\udc54\ud835\udc5b(\ud835\udc65)) on the full-precision embeddings to get binary embeddings.\nHowever, the binarization procedure is not in the training process and thus cannot be optimized\nby minimizing the training objective, which will bring large irreparable errors and fail to meet an\nacceptable accuracy. To reduce accuracy degradation, subsequent works have focused on end-to-end\napproaches to learn the binary embeddings during training.\nAs shown in Figure 3, recent works typically learn binary embeddings following two optimization\nparadigms, namely direct optimization and indirect optimization. As Figure 3(a) shows, in the\ndirect optimization of binarization, the binary embeddings are maintained as part of the model\nparameters and will be optimized directly by the training loss. For example, to improve the efficiency\nof Collaborative Filtering (CF), DCF [73] learns a binary embedding table B \u2208{\u00b11}\ud835\udc5b\u00d7\ud835\udc51. To maximize\nthe information encoded in each binary embedding, DCF further adds a balance-uncorrelation\nconstraint to B (i.e., B\ud835\udc471 = 0, B\ud835\udc47B = \ud835\udc5bI), where I is an identity matrix. However, it is NP-hard to\noptimize the binary embeddings with such constraint. To resolve this problem, DCF also maintains\na full-precision embedding table E \u2208R\ud835\udc5b\u00d7\ud835\udc51with the same balance-uncorrelation constraint. The\nconstraint of B is then replaced by adding the mean-square-error (MSE) of (B \u2212E) to the objective\nfunction. During training, DCF will update B and E alternatively through different optimization\nalgorithms. Specifically, B is updated by Discrete Coordinate Descent (DCD) and E is updated with\nthe aid of Singular Value Decomposition (SVD). This optimization paradigm has been widely used\nto learn binary embeddings in recommender systems, such as DPR [76], DCMF [33], DFM [37].\nDPR changes the objective function of DCF (i.e., rating prediction) to personalized items ranking.\nDCMF and DFM extends this binarization paradigm to Content-aware Collaborative Filtering [32]\nand Factorization Machine (FM) [49], respectively.\nFig. 3. End-to-end optimization paradigms of learning binary embeddings. \ud835\udc35(yellow) and \ud835\udc38(green) represent\nthe binary and full-precision weights, respectively. \ud835\udc3f\ud835\udc5c\ud835\udc60\ud835\udc60(red) is the training loss where \u03a8(\u00b7) is the objective\nfunction and MSE(\u00b7) is the mean-square-error function.\nAs shown in Figure 3(b), another paradigm is the indirect optimization, where the binary em-\nbeddings B are generated from full-precision embeddings E on the fly and will be optimized\nindirectly by optimizing E. However, it is infeasible to optimize E by the standard gradient descent\nas the gradients of the binary operations (e.g., \ud835\udc60\ud835\udc56\ud835\udc54\ud835\udc5b(\ud835\udc65)) are constantly zero. To solve this problem,\nCIGAR [28] replaces \ud835\udc60\ud835\udc56\ud835\udc54\ud835\udc5b(\ud835\udc65) with the scaled tanh function \ud835\udc61\ud835\udc4e\ud835\udc5b\u210e(\ud835\udefc\ud835\udc65) as lim\ud835\udefc\u2192\u221e\ud835\udc61\ud835\udc4e\ud835\udc5b\u210e(\ud835\udefc\ud835\udc65) = \ud835\udc60\ud835\udc56\ud835\udc54\ud835\udc5b(\ud835\udc65)\nand \ud835\udc61\ud835\udc4e\ud835\udc5b\u210e(\ud835\udefc\ud835\udc65) has better differential property. In the early stages of training, a smaller value of\n\ud835\udefcis utilized to yield superior representations, and as the training progresses, its value gradually\nincreases to approximate \ud835\udc60\ud835\udc56\ud835\udc54\ud835\udc5b(). Another way to solve the non-propagable gradient is straight-\nthrough-estimator (STE) [9], which treats some operations as identity maps during backpropagation.\nHashGNN [54] employs the STE variant of \ud835\udc60\ud835\udc56\ud835\udc54\ud835\udc5b() and thus updating E with the gradients of B.\nACM Comput. Surv., Vol. 56, No. 5, Article 130. Publication date: January 2024.\n130:6\nShiwei Li and Huifeng Guo, et al.\nHowever, the huge gap between B and E will cause an imprecise update for E. To solve this issue,\nHashGNN further develops a dropout-based binarization. Specifically, \u02c6E = (1 \u2212P) \u2299E + P \u2299B\nwill be fed into the following networks, where P \u2208{0, 1}\ud835\udc5b\u00d7\ud835\udc51and \u2299is the element-wise product.\nEach element in P is a Bernoulli random value with probability \ud835\udc5d. During backpropagation, only\nthe embeddings that are binarized will be updated through STE and the rest will be updated by\nstandard gradient descent. To ensure convergence, HashGNN adopts a small value for \ud835\udc5din the initial\ntraining phase, gradually increasing it as the training progresses. Similarly, L2Q-GCN [5] uses STE\nto optimize the full-precision embeddings, while introducing a positive scaling factor \ud835\udc60= \ud835\udc5a\ud835\udc52\ud835\udc4e\ud835\udc5b(|\ud835\udc86|)\nfor each binary embedding to enhance its presentation capability, where \ud835\udc86is the full-precision\nembedding. The comparison of the above three methods is summarized in Algorithm 1.\nAlgorithm 1: Comparison between CIGAR [28], HashGNN [54] and L2Q-GCN [5].\nInput: a full-precision embedding \ud835\udc86.\nOutput: the output embedding \u02c6\ud835\udc86.\n// \u02c6\ud835\udc86will be fed into following networks.\nFunc CIGAR(\ud835\udc86):\n\u02c6\ud835\udc86= \ud835\udc61\ud835\udc4e\ud835\udc5b\u210e(\ud835\udefc\u00b7 \ud835\udc86)\n// \ud835\udefcwill increase as training progresses.\nFunc HashGNN(\ud835\udc86):\n\ud835\udc83= \ud835\udc60\ud835\udc56\ud835\udc54\ud835\udc5b_\ud835\udc60\ud835\udc61\ud835\udc52(\ud835\udc86)\n// \ud835\udc60\ud835\udc56\ud835\udc54\ud835\udc5b_\ud835\udc60\ud835\udc61\ud835\udc52() is the STE variant of \ud835\udc60\ud835\udc56\ud835\udc54\ud835\udc5b().\n\ud835\udc91:= {0, 1}\ud835\udc51\n// sample from Bernoulli distribution with probability \ud835\udc5d.\n\u02c6\ud835\udc86= (1 \u2212\ud835\udc91) \u2299\ud835\udc86+ \ud835\udc91\u2299\ud835\udc83\n// \ud835\udc5dwill increase as training progresses.\nFunc L2Q-GCN(\ud835\udc86):\n\ud835\udc83= \ud835\udc60\ud835\udc56\ud835\udc54\ud835\udc5b_\ud835\udc60\ud835\udc61\ud835\udc52(\ud835\udc86)\n\u02c6\ud835\udc86= \ud835\udc5a\ud835\udc52\ud835\udc4e\ud835\udc5b(|\ud835\udc86|) \u00b7 \ud835\udc83\n2.2\nQuantization\nAlthough binarization has better efficiency and less memory cost at the inference stage, it may\nlead to a significant drop of accuracy, which is not acceptable in several scenarios. As Cheng et\nal. [6] claim, even 0.1% decrease of the prediction accuracy may result in large decline in revenue.\nTo trade off the memory cost and the prediction accuracy, quantization is used to represent each\nweight with a multi-bit integer.\nQuantization is the mapping of a 32-bit full-precision weight to an element in the set of quantized\nvalues S = {\ud835\udc5e0,\ud835\udc5e1, ...,\ud835\udc5e\ud835\udc58}, where \ud835\udc58= 2\ud835\udc60\u22121 and \ud835\udc60is the bit width. The most commonly used\nquantization function is uniform quantization, where the quantized values are uniformly distributed.\nSpecifically, the step size \u0394 = \ud835\udc5e\ud835\udc56\u2212\ud835\udc5e\ud835\udc56\u22121 remains the same for any \ud835\udc56\u2208[1,\ud835\udc58]. Let \ud835\udc64be a value clipped\ninto the range [\ud835\udc5e0,\ud835\udc5e\ud835\udc58], we can quantize it into an integer as \u02c6\ud835\udc64= \ud835\udc5f\ud835\udc51((\ud835\udc64\u2212\ud835\udc5e0)/\u25b3), where \ud835\udc5f\ud835\udc51(\ud835\udc65)\nrounds \ud835\udc65to an adjacent integer. The integer \u02c6\ud835\udc64will be de-quantized into a floating-point value\n( \u02c6\ud835\udc64\u00d7 \u25b3+ \ud835\udc5e0) when used. Existing work on embedding quantization either performs post-training\nquantization or trains a quantized embedding table from scratch.\nGuan et al. [16] studies post-training quantization (PTQ) on the embedding tables and proposes\na uniform and a non-uniform quantization algorithm. Specifically, in the uniform quantization,\nthey maintain a quantization range for each embedding and find the best quantization range by a\ngreedy search algorithm. In the non-uniform quantization, they divide similar embeddings into\ngroups and apply k-means clustering on the weights to produce a codebook for each group. The\nweights in each group will be mapped to the index of a value in the corresponding codebook. These\ntwo algorithms improve the accuracy of PTQ, however, they still suffer from accuracy degradation.\nACM Comput. Surv., Vol. 56, No. 5, Article 130. Publication date: January 2024.\nEmbedding Compression in Recommender Systems: A Survey\n130:7\nFig. 4. Training frameworks of quantization. \ud835\udc3f\ud835\udc5c\ud835\udc60\ud835\udc60(red) is the training loss. \ud835\udc40(purple) and \ud835\udc38(green) represent\nthe integer and full-precision weights, respectively.\nTo further reduce accuracy degradation, recent works [66, 69] learn quantized weights from\nscratch. Unlike the well-known quantization-aware training (QAT) [2, 13], [66, 69] use another\nquantization training framework to exploit the sparsity of the input data, which we term low-\nprecision training (LPT). As Figure 4(a) shows, QAT quantizes the full-precision weights in the\nforward pass and updates the full-precision weights with the gradients estimated by STE. As\nFigure 4(b) shows, in LPT, the weights are stored in the format of integers at training, thereby\ncompressing the training memory. The model takes the de-quantized weights as input and will\nquantize the weights back into integers after the backward propagation. Since the input one-hot\nvectors of DLRMs are highly sparse, only extremely small part of the embeddings will be de-\nquantized into floating-point values, whose memory is negligible. Xu et al. [66] uses 16-bit LPT on\nthe embedding table without sacrificing accuracy. To enhance the compression capability of LPT,\nYang et al. [69] proposes a mixed-precision scheme where most embeddings are stored in the format\nof integers, only the most recently or frequently used embeddings are stored in a full-precision\ncache. With a small cache, they achieve lossless compression with 8-bit or even 4-bit quantization.\nLi et al. [31] proposes an adaptive low-precision training scheme to learn the quantization step size\nfor better model accuracy.\n2.3\nDiscussion\nLow-precision is a simple yet effective way for embedding compression. At the inference stage,\nbinarization can reduce the memory usage by 32\u00d7 and accelerate the inference through Hamming\ndistance. However, the binary embeddings usually cause severe accuracy degradation and need to\nbe trained with the guidance of full-precision embeddings, which requires more memory usage and\ncomputing resources at training. In contrast, quantization has a limited compression capability but\ncan achieve a comparable accuracy as the full-precision embeddings. Besides, recent quantization\napproaches for embedding tables can also compress the memory usage at the training stage and\nimprove the training efficiency by reducing the communication traffic.\n3\nMIXED-DIMENSION\nEmbedding tables usually assign a uniform dimension to all the embeddings in a heuristic way, and\nit turns out to be suboptimal in both prediction accuracy and memory usage [25]. As confirmed\nin [78], a low dimensional embedding is good at handling less frequent features where a high\ndimensional embedding cannot be well trained. Therefore, to boost the model performance, it is\nimportant to assign a appropriate dimension to each feature and use mixed-dimension embeddings.\nExisting methods can obtain mixed-dimension embeddings in a structured or an unstructured\nmanner. As shown in Figure 5, structured approaches divide the embeddings into groups each of\nACM Comput. Surv., Vol. 56, No. 5, Article 130. Publication date: January 2024.\n130:8\nShiwei Li and Huifeng Guo, et al.\nFig. 5. Structures of mixed-dimension embeddings.\nwhich has a unique dimension, while unstructured approaches learn a sparse embedding table\nwhere the embeddings have various dimensions. However these mixed-dimension embeddings are\nnot friendly for the operations (e.g., inner product) which require the embeddings of the same length.\nTherefore, the mixed-dimension embeddings need to be transformed into a uniform dimension\nbefore feeding into the following networks. Such transformation is usually achieved by linear\nprojection or simply zero padding. Apart from the difference of the embedding structures, existing\nmethods also differ greatly in the way of generating mixed-dimension embeddings. In this section,\nwe will introduce three kinds of mixed-dimension approaches named rule-based approaches,\nNAS-based approaches and pruning, respectively.\n3.1\nRule-based Approaches\nIt is a common understanding that the features with higher frequencies are more informative and\nthe fields with more features occupy more memory. Thus, the embedding dimension can be set with\na heuristic rule based on the feature frequency and the field size. To deploy the item embeddings\ninto resource-constraint devices, CpRec [53] divides the items into several groups by frequency\nand assigns a predefined dimension to each group according to the frequencies of owned features.\nSimilarly, MDE [15] assigns each feature field with a unique dimension according to the number\nof features included in this field. Specifically, let \ud835\udc8f\u2208R\ud835\udc5adenotes the number of features in all\n\ud835\udc5afeature fields and \ud835\udc91= 1/\ud835\udc8f, then the embedding dimension of \ud835\udc56-th field would be \u00af\ud835\udc51\ud835\udc91\ud835\udc8a\ud835\udefc/||\ud835\udc91||\ud835\udefc\n\u221e,\nwhere \u00af\ud835\udc51is the base dimension and \ud835\udefc\u2208[0, 1] denotes the temperature. These rule-based approaches\nare simple yet effective in reducing the memory usage and alleviating the overfitting problems,\nhowever, they suffer from suboptimal performance as the heuristic rules can not be optimized by\nthe ultimate goal of minimizing the training objective.\n3.2\nNAS-based Approaches\nNAS was originally proposed to search for the optimal neural network architectures [82]. Recently,\nit has also been adopted in searching embedding dimensions for different features. Unlike the\nrule-based approaches where the dimension is set based on a priori, it is now learned. Generally,\nthere are three components in the NAS-based approaches: (1) search space: relaxing the large\noptimization space of embedding dimensions with heuristic assumptions; (2) controller: usually a\nneural network or learnable parameters, selecting candidate dimension from the search space in a\nhard or soft manner; (3) updating algorithm: updating the controller with reinforcement learning\n(RL) algorithms or differential architecture search (DARTS) [38] techniques and so on.\nWe first introduce the approaches of NIS [25], ESAPN [39] and AutoIAS [59], which adopt a\npolicy network as the controller and update the controller with RL algorithms. NIS [25] and ESAPN\n[39] are designed to learn embedding dimensions of users and items. In NIS, the authors relax the\nACM Comput. Surv., Vol. 56, No. 5, Article 130. Publication date: January 2024.\nEmbedding Compression in Recommender Systems: A Survey\n130:9\noriginal search space \ud835\udc5b\u00d7 \ud835\udc51to \ud835\udc3a\u00d7 \ud835\udc35(\ud835\udc3a< \ud835\udc5band \ud835\udc35< \ud835\udc51) by dividing the features into \ud835\udc3agroups and\ncutting the embedding dimension of each group into \ud835\udc35blocks. Then, they use the controller to\nselect several blocks and generate the final embeddings. In ESAPN, the authors predefine the search\nspace as a set of candidate embedding dimensions D = {\ud835\udc511,\ud835\udc512, ...,\ud835\udc51\ud835\udc58}, where \ud835\udc511 < \ud835\udc512 < ... < \ud835\udc51\ud835\udc58.\nInspired by the fact that the frequencies of features are monotonically increasing in the data stream,\nthey decide to increase or keep the current embedding dimension instead of selecting one in D.\nThe decision is made by the controller based on the feature frequency and the current embedding\ndimension. Different from NIS and ESAPN, AutoIAS searches not only the embedding dimensions\nof feature fields but also the following neural network architectures. The authors design a search\nspace for each model component (e.g., the search space of embedding dimensions is similar as\nD in ESAPN.). To boost the training efficiency, they maintain a supernet at training and use the\ncontroller to generate sub-architectures by inheriting parameters from the supernet.\nThe RL-based approaches described above perform a hard selection by selecting only one\nembedding dimension for each feature or field at a time. Instead, inspired by DARTS, AutoEmb [78]\nand AutoDim [79] make a soft selection by weighted summing over the embeddings of the candidate\ndimensions in D = {\ud835\udc511,\ud835\udc512, ...,\ud835\udc51\ud835\udc58}. Let \ud835\udf4e\u2208[0, 1]\ud835\udc58denote the vector composed of the weighting\ncoefficients. AutoEmb searches for the embedding dimensions of individual features, while AutoDim\nsearches for the embedding dimensions of the feature fields. In AutoEmb, the controller is a neural\nnetwork and generates \ud835\udf4ebased on the feature frequency. While AutoDim directly assigns each\nfield with a learnable vector \ud835\udf4e, and it further approximates the hard selection by performing\ngumbel-softmax [22] on \ud835\udf4e. At training, the controller in AutoEmb and the learnable vectors in\nAutoDim are optimized through DARTS techniques. After training, the corresponding dimension\nof the largest weight in \ud835\udf4eis selected and the model will be retrained for a better accuracy.\nConsidering that the training process of the controller is quite time-consuming, recent works [3,\n43] search for the optimal embedding dimensions after training the models, without using any\ncontroller. They first sample some structures from the search space and then explore the entire search\nspace by using evolutionary search strategies on the sampled structures. Specifically, RULE [3]\ncuts the embedding table into \ud835\udc3a\u00d7 \ud835\udc35blocks similar as NIS and adds a diversity regularization\nto the blocks in the same group for maximizing expressiveness. After training, RULE selects the\nmost suitable embedding blocks under a memory budget (i.e., the maximum number of blocks).\nOptEmbed [43] trains a supernet while removing non-informative embeddings. After training, it\nthen assigns each field with a binary mask \ud835\udc8e\u2208{0, 1}\ud835\udc51to obtain mixed-dimension embeddings,\nwhere \ud835\udc51is the original dimension. The block selections in RULE and the masks in OptEmbed are\ndetermined and evolved by the search strategies.\n3.3\nPruning\nInstead of shortening the length of embeddings, pruning can obtain a sparse embedding table and\nthus get mixed-dimension embeddings. For instance, DeepLight [10] prunes the embedding table\nin a certain proportion. During training, it prunes and retrains the embedding table alternatively\nso that the mistakenly pruned weights can grow back. In addition, DeepLight will increase the\npruning proportion gradually as training proceeds.\nAnother way to prune the embeddings is to train the embeddings with learnable masks. Specifi-\ncally, an embedding \ud835\udc86is pruned as \u02c6\ud835\udc86= \ud835\udc8e\u2299\ud835\udc86for the forward pass, where \ud835\udc8eis the mask and \u2299is\nthe element-wise product. DNIS [7] divides features into groups by frequency and assigns each\ngroup with a learnable mask \ud835\udc8e\u2208[0, 1]\ud835\udc51. AMTL [68] develops a network to generate a binary\nmask \ud835\udc8e\u2208{0, 1}\ud835\udc51for each feature based on its frequency. Similarly, PEP [41] generates a binary\nmask \ud835\udc8e\u2208{0, 1}\ud835\udc51for each feature as \ud835\udc8e= I(|\ud835\udc86| > \ud835\udc54(\ud835\udc60)), where I(\u00b7) is the indicator function and\nACM Comput. Surv., Vol. 56, No. 5, Article 130. Publication date: January 2024.\n130:10\nShiwei Li and Huifeng Guo, et al.\n\ud835\udc54(\ud835\udc60) (e.g., \ud835\udc60\ud835\udc56\ud835\udc54\ud835\udc5a\ud835\udc5c\ud835\udc56\ud835\udc51(\ud835\udc60)) serves as a learnable threshold. Specially, in PEP, the embeddings should\nminus \ud835\udc54(\ud835\udc60) before being pruned by the masks (i.e. \u02c6\ud835\udc86= \ud835\udc8e\u2299(\ud835\udc86\u2212\ud835\udc54(\ud835\udc60))). At training, the network\nin AMTL and the learnable threshold in PEP are optimized together with the model parameters\nby gradient descent, while the learnable masks in DNIS are optimized by DARTS. After training,\nAMTL and PEP preserve \u02c6\ud835\udc86as the final embeddings, while DNIS need pruning \u02c6\ud835\udc86with a threshold\nas the redundant weights in \u02c6\ud835\udc86are not exact zero. The differences between the above methods in\ngenerating masks are summarized in Algorithm 2.\nAlgorithm 2: Comparison between DNIS [7], AMTL [68] and PEP [41].\nInput: the full-precision embedding \ud835\udc86and feature frequency \ud835\udc53.\nOutput: the pruned embedding \u02c6\ud835\udc86. // \u02c6\ud835\udc52will be fed into following networks.\nFunc DNIS(\ud835\udc86):\n\ud835\udc8e:= [0, 1]\ud835\udc51\n// \ud835\udc8eis a learnable mask.\n\u02c6\ud835\udc86= \ud835\udc8e\u2299\ud835\udc86\n// \ud835\udc8eis shared by features with similar frequency.\nFunc AMTL(\ud835\udc86):\n\ud835\udc8e:= \ud835\udc4e\ud835\udc5a\ud835\udc61\ud835\udc59(\ud835\udc53) // \ud835\udc8eis generated by a network \ud835\udc4e\ud835\udc5a\ud835\udc61\ud835\udc59() with the frequency \ud835\udc53.\n\u02c6\ud835\udc86= \ud835\udc8e\u2299\ud835\udc86\nFunc PEP(\ud835\udc86):\n\ud835\udc8e= I(|\ud835\udc86| > \ud835\udc60\ud835\udc56\ud835\udc54\ud835\udc5a\ud835\udc5c\ud835\udc56\ud835\udc51(\ud835\udc60))\n// \ud835\udc60\ud835\udc56\ud835\udc54\ud835\udc5a\ud835\udc5c\ud835\udc56\ud835\udc51(\ud835\udc60) serves as a learnable threshold.\n\u02c6\ud835\udc86= \ud835\udc8e\u2299(\ud835\udc86\u2212\ud835\udc60\ud835\udc56\ud835\udc54\ud835\udc5a\ud835\udc5c\ud835\udc56\ud835\udc51(\ud835\udc60))\nTo get rid of the extra training process of optimizing the masks, SSEDS [48] develop a single-shot\npruning algorithm to prune on the pretrained models. For a pretrained model, SSEDS will prune the\ncolumns of the embedding matrix for each field and produce structured embeddings. After training,\nSSEDS assigns each feature field with a mask and form a mask matrix M = {1} \u02c6\ud835\udc5b\u00d7\ud835\udc51, where \u02c6\ud835\udc5bis the\nnumber of fields and \ud835\udc51is the original embedding dimension. Instead of learning M in the training\nprocess, SSEDS use \ud835\udc54\ud835\udc56\ud835\udc57= \ud835\udf15\ud835\udc53(M, E)/\ud835\udf15M\ud835\udc56\ud835\udc57to identify the importance of \ud835\udc57-th dimension in \ud835\udc56-th field,\nwhere \ud835\udc54\ud835\udc56\ud835\udc57is the gradient of the loss function with respect to M\ud835\udc56\ud835\udc57. Specifically, a larger magnitude of\n|\ud835\udc54\ud835\udc56\ud835\udc57| means that the corresponding dimension has a greater impact on the loss function. Note that\nall |\ud835\udc54\ud835\udc56\ud835\udc57| can be computed efficiently via only one forward-backward pass. Given a memory budget,\nSSEDS calculates a saliency score for each dimension as \ud835\udc60\ud835\udc56\ud835\udc57= |\ud835\udc54\ud835\udc56\ud835\udc57|/\u00cd \u02c6\ud835\udc5b\n\ud835\udc56=0\n\u00cd\ud835\udc51\n\ud835\udc57=0 |\ud835\udc54\ud835\udc56\ud835\udc57| and prunes the\ndimensions with the lowest saliency scores.\n3.4\nDiscussion\nMixed-dimension approaches can alleviate the overfitting problems and obtain better accuracy, but\nusually have worse efficiency at the training and the inference stage. At inference, the structured\napproaches usually suffer from extra computing cost due to the linear transformation and the\nunstructured approaches store the sparse embedding table using sparse matrix storage, which will\ncost extra effort to access. At training, NAS-based approaches require extremely long time for\nsearching and pruning usually needs to retrain the pruned models for better accuracy which doubles\nthe training time. In contrast, rule-based approaches have little influence on the efficiency and can\nsave memory also at the training stage. However, they cannot achieve the optimal accuracy.\n4\nWEIGHT-SHARING\nLow-precision approaches reduce the number of bits in a weight and mixed-dimension approaches\nreduce the number of weights in an embedding. Unlike them, weight-sharing approaches share\nweights among the embedding table, thereby reducing the actual number of parameters within it.\nACM Comput. Surv., Vol. 56, No. 5, Article 130. Publication date: January 2024.\nEmbedding Compression in Recommender Systems: A Survey\n130:11\nExisting weight-sharing approaches usually generate embeddings with several shared vectors.\nIn this section, we relax the definition of weight-sharing and formulate a weight-sharing par-\nadigm based on existing approaches. Specifically, the embedding generation is formulated as\n\ud835\udc86= \u00d0/\u00cd\ud835\udc60\n\ud835\udc56=1 \ud835\udc70\ud835\udc56\u00d7 T\ud835\udc56, where \u00d0/\u00cd denotes concatenation or summation, T\ud835\udc56is a matrix composed of\nshared vectors and \ud835\udc70\ud835\udc56is an index vector for selecting shared vectors in T\ud835\udc56. For ease of expression,\nwe refer to the shared vectors as meta-embeddings and the matrices of shared vectors as meta-\ntables. According to the principle of constructing the index vectors, we introduce three kinds of\nweight-sharing methods named hashing, vector quantization, and decomposition, respectively.\n4.1\nHashing\nHashing methods generate the index vectors by processing the original feature id with hash\nfunctions. For instance, the naive hashing method [60] compresses the embedding table with\na simple hash function (e.g., the reminder function). Specifically, given the original size of the\nembedding table \ud835\udc5b\u00d7 \ud835\udc51, each feature has an embedding \ud835\udc86= \ud835\udc70\u00d7 T, where T \u2208R\ud835\udc5a\u00d7\ud835\udc51(\ud835\udc5a< \ud835\udc5b) and\n\ud835\udc70= one-hot(id %\ud835\udc5a) \u2208{0, 1}\ud835\udc5a. Note that \ud835\udc70\u00d7 T is actually achieved by table look-up when \ud835\udc70is a\none-hot vector. However, [60] naively maps multiple features to the same embedding. The collisions\nbetween features will result in loss of information and drop of accuracy.\nAlgorithm 3: Comparison between QR [50], MEmCom [47], BCH [67] and FDH [72].\nInput: the feature id (id \u2264\ud835\udc5b).\nOutput: the generated embedding \u02c6\ud835\udc86.\n// \u02c6\ud835\udc52will be fed into following networks.\nFunc QR(id):\n\ud835\udc701 = one-hot(id %\ud835\udc5a), \ud835\udc702 = one-hot(id | \ud835\udc5a)\n// \ud835\udc5ais a predefined parameter.\n\u02c6\ud835\udc86= \ud835\udc701 \u00d7 T1 + \ud835\udc702 \u00d7 T2\n// T1 \u2208R\ud835\udc5a\u00d7\ud835\udc51and T2 \u2208R(\ud835\udc5b|\ud835\udc5a)\u00d7\ud835\udc51.\nFunc MEmCom(id):\n\ud835\udc701 = one-hot(id %\ud835\udc5a), \ud835\udc702 = one-hot(id)\n\u02c6\ud835\udc86= (\ud835\udc701 \u00d7 T1) \u00b7 (\ud835\udc702 \u00d7 T2)\n// T1 \u2208R\ud835\udc5a\u00d7\ud835\udc51and T2 \u2208R\ud835\udc5b\u00d71.\nFunc BCH(id):\ndivide the bits within id into \ud835\udc60sub-ids {id1, ..., id\ud835\udc60}.\n\u02c6\ud835\udc86= \u00cd\ud835\udc60\n\ud835\udc56=1 one-hot(id\ud835\udc56) \u00d7 T\n// T is a shared meta-table.\nFunc FDH(id):\nif feature id is frequent then\n\u02c6\ud835\udc86:= R\ud835\udc51\n// frequent features have unique embeddings.\nelse\n\u02c6\ud835\udc86= QR(id)\nTo reduce the collisions, existing hashing methods use multiple hash functions to process the\nfeature id. They usually maintain multiple meta-tables (T1, ..., T\ud835\udc60) and generate multiple index\nvectors as \ud835\udc70\ud835\udc56= one-hot(hash\ud835\udc56(id)), where \ud835\udc56\u2208[1,\ud835\udc60] and {hash\ud835\udc56}\ud835\udc60\n\ud835\udc56=1 is a group of hash functions.\nFor example, QR [50] maintain two meta-tables and use the quotient function and the reminder\nfunction to generate two index vectors. Similarly, MEmCom [47] also maintain two meta-tables\n(T1 \u2208R\ud835\udc5a\u00d7\ud835\udc51, T2 \u2208R\ud835\udc5b\u00d71) and generate two index vectors as \ud835\udc701 = one-hot(id %\ud835\udc5a), \ud835\udc702 = one-hot(id).\nTo better distinguish the features, MEmCom multiplies two meta-embeddings as the final embedding.\nFurther, Yan et al. [67] use Binary Code based Hash (BCH) functions to process the feature id at\nbit level. It divides the 64 bits of a feature id into \ud835\udc60groups and restructures them into \ud835\udc60sub-ids\n(id1, ..., id\ud835\udc60). Each sub-id corresponds to an index vector (i.e., \ud835\udc70\ud835\udc56= one-hot(id\ud835\udc56), \ud835\udc56\u2208[1,\ud835\udc60]) and\nobtains a meta-embedding. Additionally, to enhance the compression capability, BCH keeps one\nACM Comput. Surv., Vol. 56, No. 5, Article 130. Publication date: January 2024.\n130:12\nShiwei Li and Huifeng Guo, et al.\nsingle meta-table and shares it among all T\ud835\udc56,\ud835\udc56\u2208[1,\ud835\udc60]. Although the hashing methods described\nabove are efficient in memory reduction, they still suffer from accuracy degradation and extra\ncomputing cost of the hash functions. To alleviate these problems, Zhang et al. [72] develop a\nFrequency-based Double Hashing (FDH) method, which only uses hashing on the features with\nlow frequencies. In this way, fewer features need to be processed by the hash function. With a little\nextra storage for the most frequent features, FDH not only improves the prediction accuracy but\nalso the inference efficiency. The difference between the above methods in generating embeddings\nis reflected in Algorithm 3.\nInstead of generating embeddings with meta-embeddings, LMA [12] and ROBE [11] use hash\nfunctions to map each weight in the embedding table into a shared memory \ud835\udc40. For a weight \ud835\udc64\ud835\udc56,\ud835\udc57in\nthe embedding table, they take both \ud835\udc56and \ud835\udc57as the input of hash functions. LMA utilizes locality\nsensitive hashing (LSH) to map the weights of each embedding to \ud835\udc40randomly. ROBE organizes \ud835\udc40\nas a circular array and divides the flattened embedding table (i.e. concatenate all rows) into blocks\nof size \ud835\udc4d. The head of each block is mapped to \ud835\udc40randomly and the following weights in the block\nwill be mapped to the position next to the head.\n4.2\nVector Quantization\nHashing methods typically get the index vector by processing the feature id with hash functions,\nwhich fail to capture the similarity between features themselves [26]. To capture the similarity,\nvector quantization (VQ) constructs the index vectors through approximated nearest neighbor\nsearch (ANNS). Specifically, for a feature with an original embedding of \ud835\udc86, VQ gets its index vector\nas \ud835\udc70= one-hot(arg max\ud835\udc58sim(\ud835\udc86, T\ud835\udc58)) \u2208{0, 1}\ud835\udc5a, where T\ud835\udc58is the \ud835\udc58-th meta-embedding in the meta-\ntable T \u2208R\ud835\udc5a\u00d7\ud835\udc51and sim(\u00b7) is a similarity function (e.g., Euclidean distance). In other words, VQ\ntakes the original embedding as input and quantizes it into its most similar meta-embedding. Note\nthat the meta-table and the meta-embedding are commonly referred to codebook and codeword in\nrecent literature on VQ. Here we use meta-table and meta-embedding for consistency.\nSaec [62] generates a meta-table T by clustering the most frequent embeddings of a pretrained\nmodel and then quantizes each original embedding into a meta-embedding in T. However, assigning\nthe same meta-embedding to different features (i.e., collisions) still results in drop of accuracy,\neven though the features have some similarity. In addition, Saec cannot optimize the meta-table\ntogether with the original embeddings, which also results in suboptimal accuracy. To alleviate\nthe collisions, subsequent works adopt product quantization (PQ) [23] and additive quantization\n(AQ) [1] to quantize an embedding into multiple meta-embeddings. To optimize the meta-table\ntogether with the original embeddings, researchers usually quantize the original embeddings into\nmeta-embeddings during training and use the meta-embeddings as the input of the following\nnetwork, where the original embeddings will be optimized through STE [9].\nPQ considers an embedding as a concatenation of several segments (i.e., \ud835\udc86= \u00d0\ud835\udc60\n\ud835\udc56=1 \ud835\udc86\ud835\udc56). Each\nsegment \ud835\udc86\ud835\udc56corresponds to a meta-table T\ud835\udc56. At training, an embedding \ud835\udc86is quantized as \u00d0\ud835\udc60\n\ud835\udc56=1 \ud835\udc70\ud835\udc56\u00d7 T\ud835\udc56,\nwhere \ud835\udc70\ud835\udc56= one-hot(arg max\ud835\udc58sim(\ud835\udc86\ud835\udc56, T\ud835\udc56\n\ud835\udc58)). In other words, PQ quantizes each segment into its most\nsimilar meta-embedding in the corresponding meta-table. After training, the original embeddings are\ndiscarded and only the meta-tables are preserved. Since the selection of a meta-embedding in a meta-\ntable can be compactly encoded by log \ud835\udc41bits, where \ud835\udc41is the size of the meta-table, an embedding\ncan now be stored by \ud835\udc60log \ud835\udc41bits with the help of the meta-tables. Further, MGQE [26] takes the\nfeature frequency into consideration when using PQ. Specifically, it divides the embeddings of items\ninto\ud835\udc5agroups in ascending order of frequency as G = {E1, E2, ..., E\ud835\udc5a} and defines N = {\ud835\udc5b1,\ud835\udc5b2, ...,\ud835\udc5b\ud835\udc5a}\nwhere \ud835\udc5b1 < \ud835\udc5b2 < ... < \ud835\udc5b\ud835\udc5a. The embeddings in \ud835\udc56-th group can only be quantized into the first \ud835\udc5b\ud835\udc56\nmeta-embeddings in each meta-table. Similarly, xLightFM [24] performs PQ in each feature field.\nACM Comput. Surv., Vol. 56, No. 5, Article 130. Publication date: January 2024.\nEmbedding Compression in Recommender Systems: A Survey\n130:13\nConsidering that the feature fields have various size, xLightFM searches for the optimal size (i.e.,\nthe number of meta-embeddings) of the meta-tables for each field. The search process is achieved\nby the DARTS algorithm which is similar as the embedding dimension search in Section 3.2.\nSimilar to PQ, AQ considers an embedding as a summation of \ud835\udc60vectors: \ud835\udc86= \u00cd\ud835\udc60\n\ud835\udc56=1 \ud835\udc86\ud835\udc56. AQ generates\nits quantized embeddings by \u00cd\ud835\udc60\n\ud835\udc56=1 \ud835\udc70\ud835\udc56\u00d7 T\ud835\udc56, \ud835\udc70\ud835\udc56= one-hot(arg max\ud835\udc58sim(\ud835\udc86\u2212\u00cd\ud835\udc56\u22121\n\ud835\udc63=1 T\ud835\udc63\n\ud835\udc58\ud835\udc63, T\ud835\udc56\n\ud835\udc58)) where \ud835\udc58\ud835\udc63\nis the index of the selected meta-embedding in the \ud835\udc63-th meta-table. Specifically, the first meta-table\ntakes the embedding \ud835\udc86as input, and outputs its nearest meta-embedding T1\n\ud835\udc581, the second meta-\ntable then quantizes the residual part (\ud835\udc86\u2212T1\n\ud835\udc581) into T2\n\ud835\udc582 and so on. The final output embedding\n\u02c6\ud835\udc86= \u00cd\ud835\udc60\n\ud835\udc56=1 T\ud835\udc56\n\ud835\udc58\ud835\udc56. LightRec [34] adopts AQ to compress the item embeddings and uses a pretrained\nmodel as a teacher to train the meta-tables effectively. LISA [63] utilizes AQ to compress the DLRMs\nwhere self-attention is performed for sequence processing. Note that there is a mass of inner product\nbetween embeddings in self-attention which suffer from extremely expensive computing costs. To\nalleviates this problem, LISA pre-calculates the inner product between meta-embeddings in the\nsame meta-table and stores the results in a small table after training. Then, the inner product of\nembeddings in self-attention can be calculated by summing the inner product of meta-embeddings\nwhich can accelerate the inference significantly.\n4.3\nDecomposition\nHashing and vector quantization use one-hot index vectors to perform a hard selection (i.e., selecting\nonly one meta-embedding) in the meta-table and alleviate the collisions between features by\nmaintaining multiply meta-tables. On the contrary, decomposition approaches make a soft selection\nby summing over all the meta-embeddings in a meta-table T \u2208R\ud835\udc5a\u00d7\ud835\udc51with a real-valued index vector\n\ud835\udc70\u2208R\ud835\udc5a. Due to the wide representation space of the real-valued index vectors, one meta-table is\nsufficient to resolve the collisions between features. Each feature will have a unique index vector\nstored in the index matrix I\ud835\udc40\u2208R\ud835\udc5b\u00d7\ud835\udc5awhen formulating the decomposition as E = I\ud835\udc40\u00d7 T.\nMLET [14] factorizes the embedding table E \u2208R\ud835\udc5b\u00d7\ud835\udc51in terms of I\ud835\udc40\u2208R\ud835\udc5b\u00d7\ud835\udc5aand T \u2208\ud835\udc45\ud835\udc5a\u00d7\ud835\udc51.\nDifferent from the low-rank decomposition where \ud835\udc5a< \ud835\udc51, MLET decomposes the embedding\ntable into larger matrices (i.e., \ud835\udc5a> \ud835\udc51) at training to ensure a larger optimization space. After\ntraining, MLET generates the embedding table as E = I\ud835\udc40\u00d7 T for memory reduction and fast\nretrieval. ANT [35] adopt a better initialization for T and imposes a sparse constraint on I\ud835\udc40.\nSpecifically, ANT initializes the meta-table T by clustering the embeddings of a pretrained model.\nIn addition, to reduce redundancy, ANT use an \u21131 penalty on I\ud835\udc40and constrain its domain to be\nnon-negative. Instead of learning and storing the index vectors at training, DHE [27] develop\na hash encoder H : N \u2192R\ud835\udc5ato map each feature id into an index vector \ud835\udc70\u2208R\ud835\udc5aon the fly.\nSpecifically, H (\ud835\udc65) = [\u210e1(\ud835\udc65),\u210e2(\ud835\udc65), ...,\u210e\ud835\udc5a(\ud835\udc65)], where {\u210e\ud835\udc56}\ud835\udc5a\n\ud835\udc56=1 is a group of hash functions. With\nthe hash encoder, DHE can eliminate the storage and optimization of I\ud835\udc40. Moreover, considering\nthat the index vectors are deterministic and cannot be optimized, DHE further decomposes the\nmeta-table \ud835\udc47into a multi-layer neural network to enhance its expressive ability.\nDifferent from the above naive decomposition, [56, 64, 71] use tensor train decomposition (TTD)\nto decompose the embedding tables. As shown in Figure 6, the embedding table E \u2208R\ud835\udc5b\u00d7\ud835\udc51will\nfirst be reshaped into E \u2208R(\ud835\udc5b1\ud835\udc511)\u00d7(\ud835\udc5b2\ud835\udc512)\u00d7...\u00d7(\ud835\udc5b\ud835\udc60\ud835\udc51\ud835\udc60), where \ud835\udc5b= \u00ce\ud835\udc60\n\ud835\udc56=1 \ud835\udc5b\ud835\udc56and \ud835\udc51= \u00ce\ud835\udc60\n\ud835\udc56=1 \ud835\udc51\ud835\udc56. Then, E will\nbe decomposed as E = G1 \u00d7 G2 \u00d7 ... \u00d7 G\ud835\udc60, where G\ud835\udc56\u2208R\ud835\udc5f\ud835\udc56\u22121\u00d7\ud835\udc5b\ud835\udc56\ud835\udc51\ud835\udc56\u00d7\ud835\udc5f\ud835\udc56. {G\ud835\udc56}\ud835\udc60\n\ud835\udc56=1 are called TT-cores\nand {\ud835\udc5f\ud835\udc56}\ud835\udc60\n\ud835\udc56=0 are called TT-ranks, in particular \ud835\udc5f0 = \ud835\udc5f\ud835\udc60= 1. TT-Rec [71] is the first to use TTD on\nthe embedding tables of DLRMs. It implements optimized kernels of TTD for embedding tables.\nLLRec [56] uses TTD on the embedding tables while maintaining the prediction accuracy by\nknowledge distillation. To enhance the compression capability of TTD, [64] further develop semi-\ntensor product based tensor train decomposition (STTD). Semi-tensor product is a generalization of\nACM Comput. Surv., Vol. 56, No. 5, Article 130. Publication date: January 2024.\n130:14\nShiwei Li and Huifeng Guo, et al.\nFig. 6. Example of TTD and STTD, where \ud835\udc5f3 = 1, \ud835\udc5f1 = \ud835\udc5f2 = \ud835\udc58= 2, \ud835\udc5f0 is 1 in TTD and is 2 in STTD.\nmatrix product. Specifically, given \ud835\udc82\u2208R1\u00d7\ud835\udc5b\ud835\udc5dand \ud835\udc83\u2208R\ud835\udc5d, \ud835\udc82can be cut into \ud835\udc5dblocks {\ud835\udc82\ud835\udc56\u2208R1\u00d7\ud835\udc5b}\ud835\udc5d\n\ud835\udc56=1\nand \ud835\udc82\u22c9\ud835\udc83= \u00cd\ud835\udc5d\n\ud835\udc56=1 \ud835\udc82\ud835\udc56\u00d7 \ud835\udc83\ud835\udc56\u2208R1\u00d7\ud835\udc5b, where \u22c9is the left semi-tensor product. For matrices A \u2208R\u210e\u00d7\ud835\udc5b\ud835\udc5d\nand B \u2208R\ud835\udc5d\u00d7\ud835\udc5e, A \u22c9B \u2208R\u210e\u00d7\ud835\udc5b\ud835\udc5econtains \u210e\u00d7 \ud835\udc5eblocks and each block is the semi-tensor product\nbetween a row of A and a column of B. [64] replaces the conventional matrix tensor product of\nTTD with the left semi-tensor product. As Figure 6 shows, in STTD, E = \u02c6G1 \u22c9\u02c6G2 \u22c9... \u22c9\u02c6G\ud835\udc60, where\n\u02c6G\ud835\udc56\u2208R\n\ud835\udc5f\ud835\udc56\u22121\n\ud835\udc58\n\u00d7 \ud835\udc5b\ud835\udc56\ud835\udc51\ud835\udc56\n\ud835\udc58\n\u00d7\ud835\udc5f\ud835\udc56,\ud835\udc5f0 = \ud835\udc58and \ud835\udc5f\ud835\udc60= 1. In addition, [64] uses self-supervised knowledge distillation\nto to reduce accuracy loss from compression.\n4.4\nDiscussion\nWeight sharing approaches usually make remarkable reduction to the memory usage. However,\nthey suffer from low efficiency at training due to extra computing cost for generating embeddings,\nespecially the nearest neighbor search in vector quantization and the matrix multiplication in\ndecomposition approaches. The extra computing cost will also slow down the inference speed except\nin vector quantization where we can store the results of inner product between meta-embeddings\nto accelerate the inference. Nevertheless, vector quantization maintains the original embeddings\nduring training which requires extra memory usage. Moreover, these methods usually cannot\nimprove the prediction accuracy, especially hashing usually causes severe drop of accuracy.\n5\nSUMMARY\nEmbedding tables usually constitute a large portion of model parameters in DLRMs, which need\nto be compressed for efficient and economical deployment. As recommender systems continue to\ngrow in scale, embedding compression has attracted more and more attention. In this survey, we\nprovide a comprehensive review of the embedding compression methods in recommender systems,\naccompanied by a systematic and rational organization of existing studies.\nThe embedding table can be conceptualized as a matrix with three dimensions, namely the\nprecision of weights, the dimension of embeddings, and the number of embeddings. Consequently,\nwe classify embedding compression methods into three primary categories according to the dimen-\nsions they compress, which are low-precision, mixed-dimension, and weight-sharing, respectively.\nLow-precision methods reduce the memory of each weight by decreasing its bit width, including\nbinarization and quantization. Mixed-dimension methods reduce the memory of specific embed-\ndings by decreasing their dimensions, including rule-based approaches, NAS-based approaches and\npruning. Weight-sharing methods reduce the actual parameters of the embedding table by sharing\nweights among different embeddings, including hashing, vector quantization and decomposition.\n5.1\nGeneral Suggestions\nIn the above sections, we have discussed the pros and cons of different compression methods in\ndetail. However, there are no golden criteria to measure which one is the best. How to choose\nACM Comput. Surv., Vol. 56, No. 5, Article 130. Publication date: January 2024.\nEmbedding Compression in Recommender Systems: A Survey\n130:15\na proper compression method depends greatly on the application scenarios and requirements.\nTherefore, we offer some general suggestions for the common requirements on the key metrics\ndiscussed in Section 1.2, namely model accuracy, inference efficiency, training efficiency, and\ntraining memory usage, respectively.\n\u2022 Model accuracy. In scenarios that demand high model accuracy, any accuracy degradation\ncaused by compression is deemed unacceptable. In such cases, mixed-dimension methods are\nrecommended as they have been reported to remove redundant parameters and avoid model\noverfitting. With an appropriate compression ratio, mixed-dimension methods can effectively\ncompress embeddings while maintaining or even improving accuracy. Furthermore, accuracy\ncan also be preserved when compressing embeddings by quantization with a higher bit width.\nFor instance, using a 16-bit representation has been proven to be sufficient for achieving accurate\nresults. On the contrary, for scenarios that do not require high prediction accuracy, quantization\nwith lower bit width or even binarization (1-bit) can be employed to achieve stronger compression.\n\u2022 Inference efficiency. In scenarios such as online inference, model inference efficiency is of\nparamount importance. Generally speaking, most embedding compression methods will not have\na great negative impact on the inference speed. However, in several decomposition methods\nwhere the embedding table is decomposed into multiple small matrices, the process of recovering\nembeddings may introduce significant inference latency and should be avoided in this context.\nTo improve inference efficiency while compressing embeddings, vector quantization is suggested,\nas the feature interaction (e.g., inner-product) of embeddings can be pre-calculated to accelerate\nthe inference process. Additionally, binarization is also worth considering when there is no\nhigh requirement on model accuracy. The calculation of feature interactions between binary\nembeddings is faster compared to that between full-precision embeddings.\n\u2022 Training efficiency. In scenarios where the models are supposed to be updated in a timely man-\nner, training efficiency becomes a critical factor. However, it is unfortunate that most embedding\ncompression methods do not contribute to improving the training efficiency. In fact, some of\nthem may significantly reduce training efficiency, particularly NAS-based approaches, pruning,\nvector quantization, and decomposition. Specifically, NAS-based approaches involve complex\ncalculations to search for optimal embedding dimensions, which can be computationally intensive\nand time-consuming. Pruning often necessitates retraining to achieve higher accuracy, resulting\nin additional training overhead. Vector quantization also involves cumbersome calculations for\nnearest neighbor searches. Decomposition may require multiple matrix multiplications to recover\nand retrieve embeddings. Therefore, in scenarios that prioritize training efficiency and timely\nmodel updates, these methods are not recommended.\n\u2022 Training memory usage. In scenarios where computing devices have limited memory, it is\ndesirable to compress the training memory usage of embeddings or, at the very least, avoid\nincreasing it. In such cases, we suggest using rule-based approaches, hashing, or decomposition,\nas they can compress the embedding table before training. Besides, the low-precision training of\nquantization is also worth considering, as the embeddings are stored in the format of integers\nduring training. On the contrary, NAS-based approaches and vector quantization are not recom-\nmended in this context. They often require storing a significant number of intermediate results\nto guide the training process, which will consume more memory.\n5.2\nFuture Prospects\nEmbedding compression in recommender systems has witnessed rapid development and notable\nachievements, although there are still several challenging issues that require attention. We identify\nseveral potential directions for further research in this field.\nACM Comput. Surv., Vol. 56, No. 5, Article 130. Publication date: January 2024.\n130:16\nShiwei Li and Huifeng Guo, et al.\n\u2022 Low-precision. The key problem faced by low-precision methods is the severe accuracy degrada-\ntion at extremely lower bit widths. In view of the extensive and advanced research on quantization\nand binarization in the deep learning community, we can refer to related techniques to alleviate\nthe accuracy loss when compressing embeddings, which is quite challenging and valuable.\n\u2022 Mixed-dimension. In recent advanced mixed-dimension methods, there is a need to enhance\nthe training efficiency of NAS-based approaches and pruning. To address this, we recommend\ndesigning lighter NAS frameworks that can efficiently search for the optimal embedding dimen-\nsion. On the other hand, finding solutions to avoid retraining pruned models is also crucial for\nenhancing training efficiency. Furthermore, while numerous studies have demonstrated the sig-\nnificant impact of the embedding dimension on model accuracy, there is still a lack of theoretical\nunderstanding regarding how the embedding dimension precisely affects model accuracy. Having\na solid theoretical basis would be invaluable in guiding the optimal selection of embedding\ndimensions, enabling more efficient and effective model training.\n\u2022 Weight-sharing. To approach the limit of weight sharing methods, we believe that an intriguing\ndirection to explore is the use of embedding generation networks. Considering the powerful\nrepresentation capabilities of neural networks, we may learn a powerful neural network to\ngenerate embeddings, instead of directly learning and maintaining the embeddings themselves.\n\u2022 Hybrid approaches. Since the methods within the three primary categories compress the\nembeddings from different dimensions and enjoy different advantages, we expect future research\nto establish a unified method for compressing multiple dimensions, or develop hybrid approaches\ncombining these techniques. By integrating the strengths of different compression methods, it is\npossible to create more powerful and comprehensive compression algorithms.\n\u2022 Open benchmarks. This review offers a thorough discussion of embedding compression meth-\nods. However, we did not undertake an experimental comparison across these methods. On one\nhand, distinct methods are applied to different tasks in recommender systems, each of which has\nunique accuracy metrics. For example, in click-through rate (CTR) prediction, the commonly\nused metric is the Area Under the Curve (AUC); whereas for rating prediction, Root Mean Square\nError (RMSE) and Mean Absolute Error (MAE) are typically employed; for Top-N recommen-\ndations, Mean Average Precision (MAP) and Normalized Discounted Cumulative Gain (NDCG)\nare commonly utilized as accuracy metrics. On the other hand, a majority of research relies on\nproprietary datasets without sharing open-source code, presenting obstacles to reproducibility\nand comparative analyses. Nonetheless, the implementation of these methods is not inherently\ncomplex. Given the focus on the embedding tables, a solution involves the definition of a new\nembedding module during implementation, coupled with the rewriting of the lookup operation\nfor the embedding vector. Therefore, it is necessary to establish a foundational benchmark to\nevaluate the effectiveness of distinct methods across a spectrum of tasks, like BARS [81], a\nbenchmark designed for recommendations. We posit that this would substantially expedite the\napplication and advancement of this field.\nACKNOWLEDGMENTS\nThis work is supported in part by National Natural Science Foundation of China under grants\n62376103, 62302184, 62206102, and Science and Technology Support Program of Hubei Province\nunder grant 2022BAA046.\nREFERENCES\n[1] Artem Babenko and Victor S. Lempitsky. 2014. Additive Quantization for Extreme Vector Compression. In 2014\nIEEE Conference on Computer Vision and Pattern Recognition, CVPR 2014, Columbus, OH, USA, June 23-28, 2014. IEEE\nComputer Society, Columbus, OH, USA, 931\u2013938.\nACM Comput. Surv., Vol. 56, No. 5, Article 130. Publication date: January 2024.\nEmbedding Compression in Recommender Systems: A Survey\n130:17\n[2] Yash Bhalgat, Jinwon Lee, Markus Nagel, Tijmen Blankevoort, and Nojun Kwak. 2020. LSQ+: Improving low-bit\nquantization through learnable offsets and better initialization. In 2020 IEEE/CVF Conference on Computer Vision and\nPattern Recognition, CVPR Workshops 2020, Seattle, WA, USA, June 14-19, 2020. Computer Vision Foundation / IEEE,\n2978\u20132985.\n[3] Tong Chen, Hongzhi Yin, Yujia Zheng, Zi Huang, Yang Wang, and Meng Wang. 2021. Learning Elastic Embeddings for\nCustomizing On-Device Recommenders. In KDD \u201921: The 27th ACM SIGKDD Conference on Knowledge Discovery and\nData Mining, Virtual Event, Singapore, August 14-18, 2021. ACM, 138\u2013147.\n[4] Yunchuan Chen, Lili Mou, Yan Xu, Ge Li, and Zhi Jin. 2016. Compressing Neural Language Models by Sparse Word\nRepresentations. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016,\nAugust 7-12, 2016, Berlin, Germany, Volume 1: Long Papers. The Association for Computer Linguistics.\n[5] Yankai Chen, Yifei Zhang, Yingxue Zhang, Huifeng Guo, Jingjie Li, Ruiming Tang, Xiuqiang He, and Irwin King. 2021.\nTowards Low-loss 1-bit Quantization of User-item Representations for Top-K Recommendation. CoRR abs/2112.01944\n(2021).\n[6] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg\nCorrado, Wei Chai, Mustafa Ispir, Rohan Anil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, and Hemal\nShah. 2016. Wide & Deep Learning for Recommender Systems. In Proceedings of the 1st Workshop on Deep Learning for\nRecommender Systems, DLRS@RecSys 2016, Boston, MA, USA, September 15, 2016. ACM, 7\u201310.\n[7] Weiyu Cheng, Yanyan Shen, and Linpeng Huang. 2020. Differentiable Neural Input Search for Recommender Systems.\nCoRR abs/2006.04466 (2020).\n[8] Tejalal Choudhary, Vipul Kumar Mishra, Anurag Goswami, and Jagannathan Sarangapani. 2020. A comprehensive\nsurvey on model compression and acceleration. Artif. Intell. Rev. 53, 7 (2020), 5113\u20135155.\n[9] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. 2015. BinaryConnect: Training Deep Neural Networks\nwith binary weights during propagations. In Advances in Neural Information Processing Systems 28: Annual Conference\non Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada. 3123\u20133131.\n[10] Wei Deng, Junwei Pan, Tian Zhou, Deguang Kong, Aaron Flores, and Guang Lin. 2021. DeepLight: Deep Lightweight\nFeature Interactions for Accelerating CTR Predictions in Ad Serving. In WSDM \u201921, The Fourteenth ACM International\nConference on Web Search and Data Mining, Virtual Event, Israel, March 8-12, 2021. ACM, 922\u2013930.\n[11] Aditya Desai, Li Chou, and Anshumali Shrivastava. 2022. Random Offset Block Embedding (ROBE) for compressed\nembedding tables in deep learning recommendation systems. Proceedings of Machine Learning and Systems 4 (2022),\n762\u2013778.\n[12] Aditya Desai, Yanzhou Pan, Kuangyuan Sun, et al. 2021. Semantically Constrained Memory Allocation (SCMA) for\nEmbedding in Efficient Recommendation Systems. CoRR abs/2103.06124 (2021).\n[13] Steven K. Esser, Jeffrey L. McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dharmendra S. Modha. 2020.\nLearned Step Size quantization. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net.\n[14] Benjamin Ghaemmaghami, Zihao Deng, Benjamin Y. Cho, et al. 2020. Training with Multi-Layer Embeddings for\nModel Reduction. CoRR abs/2006.05623 (2020).\n[15] Antonio A. Ginart, Maxim Naumov, Dheevatsa Mudigere, Jiyan Yang, and James Zou. 2021. Mixed Dimension\nEmbeddings with Application to Memory-Efficient Recommendation Systems. In IEEE International Symposium on\nInformation Theory, ISIT 2021, Melbourne, Australia, July 12-20, 2021. IEEE, 2786\u20132791.\n[16] Hui Guan, Andrey Malevich, Jiyan Yang, Jongsoo Park, and Hector Yuen. 2019. Post-Training 4-bit Quantization on\nEmbedding Tables. CoRR abs/1911.02079 (2019).\n[17] Huifeng Guo, Wei Guo, Yong Gao, Ruiming Tang, Xiuqiang He, and Wenzhi Liu. 2021. ScaleFreeCTR: MixCache-based\nDistributed Training System for CTR Models with Huge Embedding Table. In SIGIR \u201921: The 44th International ACM\nSIGIR Conference on Research and Development in Information Retrieval, Virtual Event, Canada, July 11-15, 2021. ACM,\n1269\u20131278.\n[18] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: A Factorization-Machine\nbased Neural Network for CTR Prediction. In Proceedings of the Twenty-Sixth International Joint Conference on Artificial\nIntelligence, IJCAI 2017, Melbourne, Australia, August 19-25, 2017. ijcai.org, 1725\u20131731.\n[19] Manish Gupta and Puneet Agrawal. 2022. Compression of Deep Learning Models for Text: A Survey. ACM Trans.\nKnowl. Discov. Data 16, 4 (2022), 61:1\u201361:55.\n[20] Kai Han, Yunhe Wang, Qi Tian, Jianyuan Guo, Chunjing Xu, and Chang Xu. 2020. GhostNet: More Features From\nCheap Operations. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA,\nUSA, June 13-19, 2020. 1577\u20131586.\n[21] Xin He, Kaiyong Zhao, and Xiaowen Chu. 2021. AutoML: A Survey of the State-of-the-Art. Knowledge-Based Systems\n212 (2021), 106622.\nACM Comput. Surv., Vol. 56, No. 5, Article 130. Publication date: January 2024.\n130:18\nShiwei Li and Huifeng Guo, et al.\n[22] Eric Jang, Shixiang Gu, and Ben Poole. 2017. Categorical Reparameterization with Gumbel-Softmax. In 5th International\nConference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings.\nOpenReview.net.\n[23] Herv\u00e9 J\u00e9gou, Matthijs Douze, and Cordelia Schmid. 2011. Product Quantization for Nearest Neighbor Search. IEEE\nTrans. Pattern Anal. Mach. Intell. 33, 1 (2011), 117\u2013128.\n[24] Gangwei Jiang, Hao Wang, Jin Chen, Haoyu Wang, Defu Lian, and Enhong Chen. 2021. xLightFM: Extremely Memory-\nEfficient Factorization Machine. In SIGIR \u201921: The 44th International ACM SIGIR Conference on Research and Development\nin Information Retrieval, Virtual Event, Canada, July 11-15, 2021. ACM, 337\u2013346.\n[25] Manas R. Joglekar, Cong Li, Mei Chen, Taibai Xu, Xiaoming Wang, Jay K. Adams, Pranav Khaitan, Jiahui Liu, and\nQuoc V. Le. 2020. Neural Input Search for Large Scale Recommendation Models. In KDD \u201920: The 26th ACM SIGKDD\nConference on Knowledge Discovery and Data Mining, Virtual Event, CA, USA, August 23-27, 2020. ACM, 2387\u20132397.\n[26] Wang-Cheng Kang, Derek Zhiyuan Cheng, Ting Chen, Xinyang Yi, Dong Lin, Lichan Hong, and Ed H. Chi. 2020.\nLearning Multi-granular Quantized Embeddings for Large-Vocab Categorical Features in Recommender Systems. In\nCompanion of The 2020 Web Conference 2020, Taipei, Taiwan, April 20-24, 2020. ACM / IW3C2, 562\u2013566.\n[27] Wang-Cheng Kang, Derek Zhiyuan Cheng, Tiansheng Yao, Xinyang Yi, Ting Chen, Lichan Hong, and Ed H. Chi.\n2021. Learning to Embed Categorical Features without Embedding Tables for Recommendation. In KDD \u201921: The 27th\nACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, Singapore, August 14-18, 2021. ACM,\n840\u2013850.\n[28] Wang-Cheng Kang and Julian John McAuley. 2019. Candidate Generation with Binary Codes for Large-Scale Top-N\nRecommendation. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management,\nCIKM 2019, Beijing, China, November 3-7, 2019. ACM, 1523\u20131532.\n[29] Farhan Khawar, Xu Hang, Ruiming Tang, Bin Liu, Zhenguo Li, and Xiuqiang He. 2020. AutoFeature: Searching for\nFeature Interactions and Their Architectures for Click-through Rate Prediction. In CIKM \u201920: The 29th ACM International\nConference on Information and Knowledge Management, Virtual Event, Ireland, October 19-23, 2020. ACM, 625\u2013634.\n[30] Houyi Li, Zhihong Chen, Chenliang Li, Rong Xiao, Hongbo Deng, Peng Zhang, Yongchao Liu, and Haihong Tang.\n2021. Path-based Deep Network for Candidate Item Matching in Recommenders. In SIGIR \u201921: The 44th International\nACM SIGIR Conference on Research and Development in Information Retrieval, Virtual Event, Canada, July 11-15, 2021.\nACM, 1493\u20131502.\n[31] Shiwei Li, Huifeng Guo, Lu Hou, Wei Zhang, Xing Tang, Ruiming Tang, Rui Zhang, and Ruixuan Li. 2023. Adaptive\nLow-Precision Training for Embeddings in Click-Through Rate Prediction. In Thirty-Seventh AAAI Conference on\nArtificial Intelligence, AAAI 2023, Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence, IAAI 2023,\nThirteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2023, Washington, DC, USA, February\n7-14, 2023. AAAI Press, 4435\u20134443.\n[32] Defu Lian, Yong Ge, Fuzheng Zhang, Nicholas Jing Yuan, Xing Xie, Tao Zhou, and Yong Rui. 2015. Content-Aware\nCollaborative Filtering for Location Recommendation Based on Human Mobility Data. In 2015 IEEE International\nConference on Data Mining, ICDM 2015, Atlantic City, NJ, USA, November 14-17, 2015. IEEE Computer Society, 261\u2013270.\n[33] Defu Lian, Rui Liu, Yong Ge, Kai Zheng, Xing Xie, and Longbing Cao. 2017. Discrete Content-aware Matrix Factorization.\nIn Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, NS,\nCanada, August 13 - 17, 2017. ACM, 325\u2013334.\n[34] Defu Lian, Haoyu Wang, Zheng Liu, Jianxun Lian, Enhong Chen, and Xing Xie. 2020. LightRec: A Memory and\nSearch-Efficient Recommender System. In WWW \u201920: The Web Conference 2020, Taipei, Taiwan, April 20-24, 2020. ACM\n/ IW3C2, 695\u2013705.\n[35] Paul Pu Liang, Manzil Zaheer, Yuan Wang, and Amr Ahmed. 2021. Anchor & Transform: Learning Sparse Embeddings\nfor Large Vocabularies. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria,\nMay 3-7, 2021. OpenReview.net.\n[36] Bin Liu, Ruiming Tang, Yingzhi Chen, Jinkai Yu, Huifeng Guo, and Yuzhou Zhang. 2019. Feature Generation by\nConvolutional Neural Network for Click-Through Rate Prediction. In The World Wide Web Conference, WWW 2019,\nSan Francisco, CA, USA, May 13-17, 2019. ACM, 1119\u20131129.\n[37] Han Liu, Xiangnan He, Fuli Feng, Liqiang Nie, Rui Liu, and Hanwang Zhang. 2018. Discrete Factorization Machines for\nFast Feature-based Recommendation. In Proceedings of the Twenty-Seventh International Joint Conference on Artificial\nIntelligence, IJCAI 2018, July 13-19, 2018, Stockholm, Sweden. ijcai.org, 3449\u20133455.\n[38] Hanxiao Liu, Karen Simonyan, and Yiming Yang. 2019. DARTS: Differentiable Architecture Search. In 7th International\nConference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.\n[39] Haochen Liu, Xiangyu Zhao, Chong Wang, Xiaobing Liu, and Jiliang Tang. 2020. Automated Embedding Size Search in\nDeep Recommender Systems. In Proceedings of the 43rd International ACM SIGIR conference on research and development\nin Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020. ACM, 2307\u20132316.\nACM Comput. Surv., Vol. 56, No. 5, Article 130. Publication date: January 2024.\nEmbedding Compression in Recommender Systems: A Survey\n130:19\n[40] Qiang Liu, Feng Yu, Shu Wu, and Liang Wang. 2015. A Convolutional Click Prediction Model. In Proceedings of the\n24th ACM International Conference on Information and Knowledge Management, CIKM 2015, Melbourne, VIC, Australia,\nOctober 19 - 23, 2015. ACM, 1743\u20131746.\n[41] Siyi Liu, Chen Gao, Yihong Chen, Depeng Jin, and Yong Li. 2021. Learnable Embedding sizes for Recommender\nSystems. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.\nOpenReview.net.\n[42] Xianglong Liu, Junfeng He, Cheng Deng, and Bo Lang. 2014. Collaborative Hashing. In 2014 IEEE Conference on\nComputer Vision and Pattern Recognition, CVPR 2014, Columbus, OH, USA, June 23-28, 2014. IEEE Computer Society,\n2147\u20132154.\n[43] Fuyuan Lyu, Xing Tang, Hong Zhu, Huifeng Guo, Yingxue Zhang, Ruiming Tang, and Xue Liu. 2022. OptEmbed:\nLearning Optimal Embedding Table for Click-through Rate Prediction. In Proceedings of the 31st ACM International\nConference on Information & Knowledge Management, Atlanta, GA, USA, October 17-21, 2022. ACM, 1399\u20131409.\n[44] Xiao Ma, Liqin Zhao, Guan Huang, Zhi Wang, Zelin Hu, Xiaoqiang Zhu, and Kun Gai. 2018. Entire Space Multi-\nTask Model: An Effective Approach for Estimating Post-Click Conversion Rate. In The 41st International ACM SIGIR\nConference on Research & Development in Information Retrieval, SIGIR 2018, Ann Arbor, MI, USA, July 08-12, 2018, Kevyn\nCollins-Thompson, Qiaozhu Mei, Brian D. Davison, Yiqun Liu, and Emine Yilmaz (Eds.). ACM, 1137\u20131140.\n[45] Kelong Mao, Jieming Zhu, Jinpeng Wang, Quanyu Dai, Zhenhua Dong, Xi Xiao, and Xiuqiang He. 2021. SimpleX:\nA Simple and Strong Baseline for Collaborative Filtering. In CIKM \u201921: The 30th ACM International Conference on\nInformation and Knowledge Management, Virtual Event, Queensland, Australia, November 1 - 5, 2021. ACM, 1243\u20131252.\n[46] H. Brendan McMahan, Gary Holt, David Sculley, Michael Young, Dietmar Ebner, Julian Grady, Lan Nie, Todd Phillips,\nEugene Davydov, Daniel Golovin, Sharat Chikkerur, Dan Liu, Martin Wattenberg, Arnar Mar Hrafnkelsson, Tom\nBoulos, and Jeremy Kubica. 2013. Ad click prediction: a view from the trenches. In The 19th ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining, KDD 2013, Chicago, IL, USA, August 11-14, 2013. ACM, 1222\u20131230.\n[47] Niketan Pansare, Jay Katukuri, Aditya Arora, Frank Cipollone, Riyaaz Shaik, Noyan Tokgozoglu, and Chandru\nVenkataraman. 2022. Learning Compressed Embeddings for On-Device Inference. In Proceedings of Machine Learning\nand Systems 2022, MLSys 2022, Santa Clara, CA, USA, August 29 - September 1, 2022. mlsys.org.\n[48] Liang Qu, Yonghong Ye, Ningzhi Tang, Lixin Zhang, Yuhui Shi, and Hongzhi Yin. 2022. Single-shot Embedding\nDimension Search in Recommender System. In SIGIR \u201922: The 45th International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval, Madrid, Spain, July 11 - 15, 2022. ACM, 513\u2013522.\n[49] Steffen Rendle. 2010. Factorization Machines. In ICDM 2010, The 10th IEEE International Conference on Data Mining,\nSydney, Australia, 14-17 December 2010. IEEE Computer Society, 995\u20131000.\n[50] Hao-Jun Michael Shi, Dheevatsa Mudigere, Maxim Naumov, and Jiyan Yang. 2020. Compositional Embeddings Using\nComplementary Partitions for Memory-Efficient Recommendation Systems. In KDD \u201920: The 26th ACM SIGKDD\nConference on Knowledge Discovery and Data Mining, Virtual Event, CA, USA, August 23-27, 2020. ACM, 165\u2013175.\n[51] Yixin Su, Rui Zhang, Sarah M. Erfani, and Zhenghua Xu. 2021. Detecting Beneficial Feature Interactions for Rec-\nommender Systems. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference\non Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in\nArtificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021. AAAI Press, 4357\u20134365.\n[52] Yixin Su, Yunxiang Zhao, Sarah M. Erfani, Junhao Gan, and Rui Zhang. 2022. Detecting Arbitrary Order Beneficial\nFeature Interactions for Recommender Systems. In KDD \u201922: The 28th ACM SIGKDD Conference on Knowledge Discovery\nand Data Mining, Washington, DC, USA, August 14 - 18, 2022. ACM, 1676\u20131686.\n[53] Yang Sun, Fajie Yuan, Min Yang, Guoao Wei, Zhou Zhao, and Duo Liu. 2020. A Generic Network Compression\nFramework for Sequential Recommender Systems. In Proceedings of the 43rd International ACM SIGIR conference on\nresearch and development in Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020. ACM, 1299\u20131308.\n[54] Qiaoyu Tan, Ninghao Liu, Xing Zhao, Hongxia Yang, Jingren Zhou, and Xia Hu. 2020. Learning to Hash with Graph\nNeural Networks for Recommender Systems. In WWW \u201920: The Web Conference 2020, Taipei, Taiwan, April 20-24, 2020.\nACM / IW3C2, 1988\u20131998.\n[55] Jinpeng Wang, Ziyun Zeng, Yunxiao Wang, Yuting Wang, Xingyu Lu, Tianxiang Li, Jun Yuan, Rui Zhang, Hai-Tao Zheng,\nand Shu-Tao Xia. 2023. MISSRec: Pre-training and Transferring Multi-modal Interest-aware Sequence Representation\nfor Recommendation. In Proceedings of the 31st ACM International Conference on Multimedia, MM 2023, Ottawa, ON,\nCanada, 29 October 2023- 3 November 2023. ACM, 6548\u20136557.\n[56] Qinyong Wang, Hongzhi Yin, Tong Chen, Zi Huang, Hao Wang, Yanchang Zhao, and Nguyen Quoc Viet Hung. 2020.\nNext Point-of-Interest Recommendation on Resource-Constrained Mobile Devices. In WWW \u201920: The Web Conference\n2020, Taipei, Taiwan, April 20-24, 2020. ACM / IW3C2, 906\u2013916.\n[57] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & Cross Network for Ad Click Predictions. In\nProceedings of the ADKDD\u201917, Halifax, NS, Canada, August 13 - 17, 2017. ACM, 12:1\u201312:7.\nACM Comput. Surv., Vol. 56, No. 5, Article 130. Publication date: January 2024.\n130:20\nShiwei Li and Huifeng Guo, et al.\n[58] Ruoxi Wang, Rakesh Shivanna, Derek Zhiyuan Cheng, Sagar Jain, Dong Lin, Lichan Hong, and Ed H. Chi. 2021. DCN\nV2: Improved Deep & Cross Network and Practical Lessons for Web-scale Learning to Rank Systems. In WWW \u201921:\nThe Web Conference 2021, Virtual Event / Ljubljana, Slovenia, April 19-23, 2021. ACM / IW3C2, 1785\u20131797.\n[59] Zhikun Wei, Xin Wang, and Wenwu Zhu. 2021. AutoIAS: Automatic Integrated Architecture Searcher for Click-Trough\nRate Prediction. In CIKM \u201921: The 30th ACM International Conference on Information and Knowledge Management,\nVirtual Event, Queensland, Australia, November 1 - 5, 2021. ACM, 2101\u20132110.\n[60] Kilian Q. Weinberger, Anirban Dasgupta, John Langford, Alexander J. Smola, and Josh Attenberg. 2009. Feature hashing\nfor large scale multitask learning. In Proceedings of the 26th Annual International Conference on Machine Learning,\nICML 2009, Montreal, Quebec, Canada, June 14-18, 2009 (ACM International Conference Proceeding Series, Vol. 382). ACM,\n1113\u20131120.\n[61] Le Wu, Xiangnan He, Xiang Wang, Kun Zhang, and Meng Wang. 2023. A Survey on Accuracy-Oriented Neural\nRecommendation: From Collaborative Filtering to Information-Rich Recommendation. IEEE Trans. Knowl. Data Eng.\n35, 5 (2023), 4425\u20134445.\n[62] Xiaorui Wu, Hong Xu, Honglin Zhang, Huaming Chen, and Jian Wang. 2020. Saec: similarity-aware embedding\ncompression in recommendation systems. In APSys \u201920: 11th ACM SIGOPS Asia-Pacific Workshop on Systems, Tsukuba,\nJapan, August 24-25, 2020. ACM, 82\u201389.\n[63] Yongji Wu, Defu Lian, Neil Zhenqiang Gong, Lu Yin, Mingyang Yin, Jingren Zhou, and Hongxia Yang. 2021. Linear-\nTime Self Attention with Codeword Histogram for Efficient Recommendation. In WWW \u201921: The Web Conference 2021,\nVirtual Event / Ljubljana, Slovenia, April 19-23, 2021. ACM / IW3C2, 1262\u20131273.\n[64] Xin Xia, Hongzhi Yin, Junliang Yu, Qinyong Wang, Guandong Xu, and Quoc Viet Hung Nguyen. 2022. On-Device\nNext-Item Recommendation with Self-Supervised Knowledge Distillation. In SIGIR \u201922: The 45th International ACM\nSIGIR Conference on Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022. ACM, 546\u2013555.\n[65] Jun Xiao, Hao Ye, Xiangnan He, Hanwang Zhang, Fei Wu, and Tat-Seng Chua. 2017. Attentional Factorization Machines:\nLearning the Weight of Feature Interactions via Attention Networks. In Proceedings of the Twenty-Sixth International\nJoint Conference on Artificial Intelligence, IJCAI 2017, Melbourne, Australia, August 19-25, 2017. ijcai.org, 3119\u20133125.\n[66] Zhiqiang Xu, Dong Li, Weijie Zhao, Xing Shen, Tianbo Huang, Xiaoyun Li, and Ping Li. 2021. Agile and Accurate CTR\nPrediction Model Training for Massive-Scale Online Advertising Systems. In SIGMOD \u201921: International Conference on\nManagement of Data, Virtual Event, China, June 20-25, 2021. ACM, 2404\u20132409.\n[67] Bencheng Yan, Pengjie Wang, Jinquan Liu, Wei Lin, Kuang-Chih Lee, Jian Xu, and Bo Zheng. 2021. Binary Code based\nHash Embedding for Web-scale Applications. In CIKM \u201921: The 30th ACM International Conference on Information and\nKnowledge Management, Virtual Event, Queensland, Australia, November 1 - 5, 2021. ACM, 3563\u20133567.\n[68] Bencheng Yan, Pengjie Wang, Kai Zhang, Wei Lin, Kuang-Chih Lee, Jian Xu, and Bo Zheng. 2021. Learning Effective\nand Efficient Embedding via an Adaptively-Masked Twins-based Layer. In CIKM \u201921: The 30th ACM International\nConference on Information and Knowledge Management, Virtual Event, Queensland, Australia, November 1 - 5, 2021.\nACM, 3568\u20133572.\n[69] Jie Amy Yang, Jianyu Huang, Jongsoo Park, Ping Tak Peter Tang, and Andrew Tulloch. 2020. Mixed-Precision\nEmbedding Using a Cache. CoRR abs/2010.11305 (2020).\n[70] Quanming Yao, Xiangning Chen, James T. Kwok, Yong Li, and Cho-Jui Hsieh. 2020. Efficient Neural Interaction\nFunction Search for Collaborative Filtering. In WWW \u201920: The Web Conference 2020, Taipei, Taiwan, April 20-24, 2020.\nACM / IW3C2, 1660\u20131670.\n[71] Chunxing Yin, Bilge Acun, Carole-Jean Wu, and Xing Liu. 2021. TT-Rec: Tensor Train Compression for Deep Learning\nRecommendation Models. In Proceedings of Machine Learning and Systems 2021, MLSys 2021, virtual, April 5-9, 2021.\nmlsys.org.\n[72] Caojin Zhang, Yicun Liu, Yuanpu Xie, Sofia Ira Ktena, Alykhan Tejani, Akshay Gupta, Pranay Kumar Myana, Deepak\nDilipkumar, Suvadip Paul, Ikuhiro Ihara, Prasang Upadhyaya, Ferenc Huszar, and Wenzhe Shi. 2020. Model Size\nReduction Using Frequency Based Double Hashing for Recommender Systems. In RecSys 2020: Fourteenth ACM\nConference on Recommender Systems, Virtual Event, Brazil, September 22-26, 2020. ACM, 521\u2013526.\n[73] Hanwang Zhang, Fumin Shen, Wei Liu, Xiangnan He, Huanbo Luan, and Tat-Seng Chua. 2016. Discrete Collaborative\nFiltering. In Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information\nRetrieval, SIGIR 2016, Pisa, Italy, July 17-21, 2016. ACM, 325\u2013334.\n[74] Shuai Zhang, Lina Yao, Aixin Sun, and Yi Tay. 2019. Deep Learning Based Recommender System: A Survey and New\nPerspectives. ACM Comput. Surv. 52, 1 (2019), 5:1\u20135:38.\n[75] Weinan Zhang, Jiarui Qin, Wei Guo, Ruiming Tang, and Xiuqiang He. 2021. Deep Learning for Click-Through Rate\nEstimation. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual\nEvent / Montreal, Canada, 19-27 August 2021. ijcai.org, 4695\u20134703.\n[76] Yan Zhang, Defu Lian, and Guowu Yang. 2017. Discrete Personalized Ranking for Fast Collaborative Filtering from\nImplicit Feedback. In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, February 4-9, 2017, San\nACM Comput. Surv., Vol. 56, No. 5, Article 130. Publication date: January 2024.\nEmbedding Compression in Recommender Systems: A Survey\n130:21\nFrancisco, California, USA. AAAI Press, 1669\u20131675.\n[77] Zhiwei Zhang, Qifan Wang, Lingyun Ruan, and Luo Si. 2014. Preference preserving hashing for efficient recommenda-\ntion. In The 37th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR \u201914,\nGold Coast , QLD, Australia - July 06 - 11, 2014. ACM, 183\u2013192.\n[78] Xiangyu Zhao, Haochen Liu, Wenqi Fan, Hui Liu, Jiliang Tang, Chong Wang, Ming Chen, Xudong Zheng, Xiaobing\nLiu, and Xiwang Yang. 2021. AutoEmb: Automated Embedding Dimensionality Search in Streaming Recommendations.\n(2021), 896\u2013905.\n[79] Xiangyu Zhao, Haochen Liu, Hui Liu, Jiliang Tang, Weiwei Guo, Jun Shi, Sida Wang, Huiji Gao, and Bo Long. 2020.\nMemory-efficient Embedding for Recommendations. CoRR abs/2006.14827 (2020).\n[80] Guorui Zhou, Xiaoqiang Zhu, Chengru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai.\n2018. Deep Interest Network for Click-Through Rate Prediction. In Proceedings of the 24th ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining, KDD 2018, London, UK, August 19-23, 2018. ACM, 1059\u20131068.\n[81] Jieming Zhu, Quanyu Dai, Liangcai Su, Rong Ma, Jinyang Liu, Guohao Cai, Xi Xiao, and Rui Zhang. 2022. BARS:\nTowards Open Benchmarking for Recommender Systems. In SIGIR \u201922: The 45th International ACM SIGIR Conference on\nResearch and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022. ACM, 2912\u20132923.\n[82] Barret Zoph and Quoc V. Le. 2017. Neural Architecture Search with Reinforcement Learning. In 5th International\nConference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings.\nOpenReview.net.\nACM Comput. Surv., Vol. 56, No. 5, Article 130. Publication date: January 2024.\n",
        "context": "desirable to compress the training memory usage of embeddings or, at the very least, avoid\nincreasing it. In such cases, we suggest using rule-based approaches, hashing, or decomposition,\n4.4\nDiscussion\nWeight sharing approaches usually make remarkable reduction to the memory usage. However,\nthey suffer from low efficiency at training due to extra computing cost for generating embeddings,\nby minimizing the training objective, which will bring large irreparable errors and fail to meet an\nacceptable accuracy. To reduce accuracy degradation, subsequent works have focused on end-to-end\napproaches to learn the binary embeddings during training."
    },
    {
        "id": 14,
        "title": "Reversible vision transformers",
        "author": [
            "K. Mangalam",
            "H. Fan",
            "Y. Li",
            "C.-Y. Wu",
            "B. Xiong",
            "C. Feichtenhofer",
            "J. Malik"
        ],
        "year": "2022",
        "doi": null,
        "in_text_citation": "[14]",
        "sentence": "Examples of this approach include reversible networks [14], which change the model\u2019s structure, and techniques like Checkmate [15], which alter the model\u2019s execution pattern by adding additional operations to backpropagation to decrease the number of intermediate values that need to be stored in memory.",
        "abstract": "We present Reversible Vision Transformers, a memory efficient architecture\ndesign for visual recognition. By decoupling the GPU memory requirement from\nthe depth of the model, Reversible Vision Transformers enable scaling up\narchitectures with efficient memory usage. We adapt two popular models, namely\nVision Transformer and Multiscale Vision Transformers, to reversible variants\nand benchmark extensively across both model sizes and tasks of image\nclassification, object detection and video classification. Reversible Vision\nTransformers achieve a reduced memory footprint of up to 15.5x at roughly\nidentical model complexity, parameters and accuracy, demonstrating the promise\nof reversible vision transformers as an efficient backbone for hardware\nresource limited training regimes. Finally, we find that the additional\ncomputational burden of recomputing activations is more than overcome for\ndeeper models, where throughput can increase up to 2.3x over their\nnon-reversible counterparts. Full code and trained models are available at\nhttps://github.com/facebookresearch/slowfast. A simpler, easy to understand and\nmodify version is also available at https://github.com/karttikeya/minREV",
        "full_text": "Reversible Vision Transformers\nKarttikeya Mangalam*, 2\nHaoqi Fan1\nYanghao Li1\nChao-Yuan Wu1\nBo Xiong 1\nChristoph Feichtenhofer *, 1\nJitendra Malik 1, 2\n1Meta AI, FAIR\n2UC Berkeley\nAbstract\nWe present Reversible Vision Transformers, a memory\nef\ufb01cient architecture design for visual recognition. By de-\ncoupling the GPU memory requirement from the depth of\nthe model, Reversible Vision Transformers enable scal-\ning up architectures with ef\ufb01cient memory usage.\nWe\nadapt two popular models, namely Vision Transformer and\nMultiscale Vision Transformers, to reversible variants and\nbenchmark extensively across both model sizes and tasks\nof image classi\ufb01cation, object detection and video clas-\nsi\ufb01cation.\nReversible Vision Transformers achieve a re-\nduced memory footprint of up to 15.5\u00d7 at roughly iden-\ntical model complexity, parameters and accuracy, demon-\nstrating the promise of reversible vision transformers as\nan ef\ufb01cient backbone for hardware resource limited train-\ning regimes. Finally, we \ufb01nd that the additional compu-\ntational burden of recomputing activations is more than\novercome for deeper models, where throughput can in-\ncrease up to 2.3\u00d7 over their non-reversible counterparts.\nFull code and trained models are available at https:\n//github.com/facebookresearch/slowfast.\nA simpler, easy to understand and modify version is also\navailable at https://github.com/karttikeya/\nminREV.\n1. Introduction\nThe deep learning revolution in computer vision has\nrested on the bedrock of high performance hardware ac-\ncelerators. Fueled by special purpose AI accelerators, the\ncompute requirements for state-of-the-art models are grow-\ning exponentially. However, compute is only half the story.\nThe other, and often overlooked half, is memory bandwidth\nbottleneck, which has been dif\ufb01cult to proportionally scale\nas compared to peak accelerator FLOPs [54]. In particular,\nthe peak accelerator FLOPs have been increasing at a rate\nof \u223c3.1\u00d7 every 2 years [21,62]. However, peak bandwidth\nonly scales at a rate of \u223c1.4\u00d7 every 2 years. This disparity\nis exacerbated in transformers, which have been doubling\n*Equal technical contribution.\n10\n20\n30\n40\n50\n60\nModel Complexity (GFLOPs)\n101\n102\nGPU Memory (MB/img)\n15.5x\n4.5x\nSmall (79.9)\nSmall (79.9)\nBase (81.8)\nBase (81.8)\nLarge (81.4)\nLarge (81.5)\nMViT\nRev-MViT\nViT\nRev-ViT\nRegNetY\nResNet\nFigure 1. Reversible Vision Transformers are more memory-\nef\ufb01cient, yet powerful reversible counterparts of state-of-the-\nart Vision Transformer (ViT) [15] and Multiscale Vision Trans-\nformer (MViT) [18] architectures with varying model complex-\nity. Numbers in parentheses denote top-1 ImageNet performance.\nResNet [28] and RegNet [58] are only shown for reference. For\ndetailed discussion please refer to \u00a74.1.\nin required compute roughly every three months for the past\nthree years, resulting in a so-called memory wall [21] where\nboth the overall model performance as well as the training\nspeed have become tightly memory-bound [34].\nAs such, for bandwidth bound models, trading compute\nfor memory through re-computation could actually be more\nef\ufb01cient than using work-optimal algorithms [70,71]. In the\ncase of training neural network models, this can be achieved\nby re-computing activations instead of storing and then\nloading them from DRAM [31]. Besides training speed,\nscaling vision transformers in depth naturally hits the GPU\nmemory capacity, especially in memory starved regimes\nsuch as video recognition where state-of-the-art models are\noften limited to batch size 1 due to high memory footprint\nof intermediate activations.\nWe propose Reversible Vision Transformers, a family of\nexpressive visual recognition architectures with very favor-\nable activation memory footprints (Figure 1) compared to\ntheir non-reversible variants. By trading-off GPU activation\ncaching with ef\ufb01cient on-the-\ufb02y activation re-computation,\nreversible vision transformers effectively decouple the acti-\nvation memory growth from the depth of the model.\n1\narXiv:2302.04869v1  [cs.CV]  9 Feb 2023\nWhile the natural language processing community has\nperformed some early exploration of reversible transform-\ners for machine translation [38], these techniques focus on\nlonger sequence lengths rather than depth.\nOur experiments show that a straightforward adaptation\nof vision transformers to reversible architectures fails to\nscale for deeper models because of training convergence in-\nstabilities due to internal sub-block residual connections.\nIn this work, we recon\ufb01gure the residual paths in Vision\nTransformers (ViT) [15] and Multiscale Vision Transform-\ners (MViT) [18] to overcome this issue. We further \ufb01nd\nthat reversible structures have stronger inherent regulariza-\ntion and therefore, we use a lighter augmentation recipe (re-\npeated augmentation, augmentation magnitude and stochas-\ntic depth) and lateral connections between residual blocks.\nWe benchmark extensively across image recognition\ntasks such as image classi\ufb01cation and object detection\nas well as video classi\ufb01cation, across all of which, re-\nversible vision transformers have competitive performance\nto their non-reversible counterparts suffering negligible to\nno performance decay. Moreover, reversible models have\nextremely favorable per-image memory footprint, saving\n15.5\u00d7 on the ViT-Large model and 4.5\u00d7 on the MViT-\nLarge model with reversible training.\nIn summary, our contributions are three-fold.\n(i)\nWe\npropose\nReversible\nVision\nTransformer\n(Rev-ViT) and Reversible Multiscale Vision Transformers\n(Rev-MViT), memory ef\ufb01cient reversible adaptations of\nstate-of-the-art visual recognition backbones.\n(ii) We observe reversible transformers to have a stronger\ninherent regularization than vanilla networks.\nHence,\nwe develop new training recipes by adapting the original\nrecipes with different repeated augmentations, augmenta-\ntion magnitudes and drop path rate to match the perfor-\nmance of their non-reversible counterparts.\n(iii) We benchmark our models across several tasks: im-\nage classi\ufb01cation, object detection and action recognition,\nacross accuracy, memory, maximum training batch size\nand model complexity. In particular, at matched complex-\nity (FLOPs/parameters) and \ufb01nal accuracy, Rev-ViT-B and\nRev-ViT-L train with per image memory footprints that are\n8.2\u00d7 and 15.5\u00d7 lighter than ViT-B and ViT-L respectively.\nFurther, we show how deep reversible networks can achieve\nup to 2-4\u00d7 throughput than their vanilla counterparts.\n2. Related Work\nTransformers\nare a popular network structure that were\n\ufb01rst proposed for natural language applications [68] and\nnow are widely used in all areas of deep learning such\nas Reinforcement Learning [7], Speech [41], Music [32],\nmulti-modal learning [35] and recently, in traditional vision\ntasks [15] as well. Since their introduction, Vision Trans-\nformers have experienced enthusiastic adoption and have\nbeen applied to several visual recognition tasks [15,63,64]\nusing priors such as multi-scale feature hierarchies [18,24,\n49,69,76] and local structure modelling [9,14,49]. Further,\nvision transformers have also been generalized for action\nrecognition and detection in videos [1,3,18,49,52,53].\nHowever, a crucial problem with scaling up transformer\nmodels is the growth of required GPU memory with depth.\nThis linear growth in memory is prohibitive to the devel-\nopment of very deep models since the batch size needs to\nbe reduced considerably to be able to accommodate storing\nthe intermediate activations on GPU. This problem is ex-\nacerbated in video models which process very large input\ntensors and are often trained with batch size 1 even for shal-\nlower depths. A potential systems-level solution to scale\nup conventional transformer architectures is model paral-\nlelism [10] that puts different parts of the model on dif-\nferent GPUs. However in practice, it is quite slow and re-\nquires special high bandwith network infrastructure because\nof huge across device traf\ufb01c.\nIn this work, we use Vision Transformers [15] and Multi-\nscale Vision Transformers [18] as our base models and pro-\npose their reversible transformer version that decouple the\nmemory requirement from depth of the model. This facil-\nitates saving GPU memory and allows training with much\nhigher batch size, and consequently, to preserve or even in-\ncrease training throughput of deep non-reversible models.\nReversible Architectures\nare a family of neural network\narchitectures that are based on the NICE [12,13] reversible\ntransformation model which are the precursors of the mod-\nern day generative \ufb02ow based image generation architec-\ntures [30, 37]. Based on the NICE invertible transforma-\ntions, Gomez et al. [22] propose a Reversible ResNet ar-\nchitecture that employs the reversible transformation [12]\nfor memory-ef\ufb01cient image classi\ufb01cation in ResNets [27].\nAn interesting line of work builds upon the Reversible\nResNets ideas proposing better reversible CNN models us-\ning ODE characterizations [6, 39, 59], momentum [39, 59],\nlayer-wise inversion [25], fourier transform based inver-\nsion [20] and \ufb01xed point iteration based inversion [2, 60].\nReversible CNNs have been applied to several traditional\nimage tasks such as compression [46], reconstruction [43],\nretrieval [42], and denoising [33, 47] as well as to com-\npressed sensing [61], compact resolution [75], image to im-\nage translation [67], remote sensing [56], medical image\nsegmentation [55, 74] and MRI reconstruction [57]. Re-\nversible transformation have also been adapted to other net-\nworks such as RNNs [51], Unet [4, 16], Masked Convo-\nlutional Networks [60] and 1000-layer deep Graph Neural\nNetworks [40]. Some early attempts have also been made\nto adapt the reversible transformation to the NLP domain,\ninitiated by Kiatev et al. [38] and built upon in [78,79] for\nmachine translation.\n2\nHowever, word-level input partitioning contains much\nricher semantic content than patch level image partition-\ning and NLP transformers tend to be shallower in depth but\nwider in channel dimension. For example, Kiatev et al. [38]\nfocus on expanding on the input sequence dimension rather\nthan model depth and with no benchmarking on maximum\nbatch-size, peak GPU memory and training throughput.\nOur experiments show that a na\u00a8\u0131ve adaption of reversible\nvision transformers performs poorly for deeper (\u22658 blocks)\nmodels. This work is the \ufb01rst to propose Reversible Vision\nTransformers, adapt it to two state-of-the-art transformer\nnetworks, namely, ViT and MViT. Furthermore, this work\nis the \ufb01rst use of a reversible backbone for object detec-\ntion and video classi\ufb01cation, which tends to be one the most\nmemory starved domains of visual recognition.\n3. Approach\nWe \ufb01rst present a brief overview of the reversible trans-\nformation (\u00a73.1.1) and its bene\ufb01ts in neural network train-\ning (\u00a73.1.3). We then present our proposed Reversible Vi-\nsion Transformer (\u00a73.2) its two residual stream structure\n(\u00a73.2.1 and associated constraints (\u00a73.1.3. This is followed\nby our proposed Reversible Multiscale Vision Transformer\narchitecture (\u00a73.3) and its sub-blocks (\u00a73.3.2 and \u00a73.3.1)\nthat allow end-to-end reversible training.\n3.1. Reversible Block Structure\nThe reversible transformer is composed of a stack of re-\nversible blocks that follow the structure of the reversible\ntransformation to allow analytic invertibility of outputs.\n3.1.1\nReversible Transformation\nConsider a transformation T1 that transforms an input ten-\nsor I partitioned into two d dimensional tensors, [I1; I2]\ninto the output tensor O also similarly partitioned into\ntensors, [O1; O2] with an arbitrary differentiable function\nF(\u00b7) : Rd \u2192Rd as follows:\nI =\n\u0014I1\nI2\n\u0015\n\u2212\u2192\nT1\n\u0014O1\nO2\n\u0015\n=\n\u0014\nI1\nI2 + F(I1)\n\u0015\n= O\nNote that the above transformation T1 allows an inverse\ntransformation T \u2032\n1 such that T \u2032\n1 \u25e6T1 is an identity transform.\nAlso, consider an analogous transposed transformation T2\nusing the function G(\u00b7) : Rd \u2192Rd as follows:\nI =\n\u0014I1\nI2\n\u0015\n\u2212\u2192\nT2\n\u0014O1\nO2\n\u0015\n=\n\u0014I1 + G(I2)\nI2\n\u0015\n= O\nSimilar to T1, T2 also allows an inverse transform T \u2032\n2. Now\nconsider the composition T = T2 \u25e6T1 that transforms both\nthe partitions of the input vector I and is obtained as,\nI =\n\u0014I1\nI2\n\u0015\n\u2212\u2192\nT\n\u0014O1\nO2\n\u0015\n=\n\u0014I1 + G(I2 + F(I1))\nI2 + F(I1)\n\u0015\n= O (1)\nNaturally, T affords the inverse transform T \u2032 = T \u2032\n1\u25e6T \u2032\n2 that\nfollows T \u2032(T(I)) = I. Note that the inverse transform T \u2032\nqueries the functions F and G exactly once and hence has\nthe same computational cost as the forward transform T.\n3.1.2\nVanilla networks require caching activations\nConsider the back-propagation mechanism. Given a com-\nputation graph node, M, its children nodes {Nj}, and the\ngradients of the children node with respect to \ufb01nal loss\nn\ndL\ndNj\no\n, the back-propagation algorithm uses the chain rule\nto calculate the gradient with respect to M as,\ndL\ndM =\nX\nNj\n\u0012 \u2202fj\n\u2202M\n\u0013T dL\ndNj\nwhere fj denotes the function computing node Nj from its\nparents, M being one of them. The jacobian \u2202fj\n\u2202M, requires\ncalculating the partial gradient of the fj output with respect\nto the current node M.\nNow consider the simplest possible neural network layer\nf(X) = W T X, where X is an intermediate activation in-\nside the network. Applying the above described backprop-\nagation algorithm to compute the derivative with respect to\nparent nodes, and using the output Y as the sole child node,\nNj, we get,\ndL\ndW =\n\u0012 dL\ndY\n\u0013\nXT\ndL\ndX = W dL\ndY\nThus, because of the function jacobian, the backpropaga-\ntion algorithm requires intermediate activations during the\nforward pass to be available in the backward pass to com-\npute the gradients with respect to the weights.\nTypically, this is achieved by caching the intermediate\nactivations on GPU memory for use in the backward pass.\nThis allows fast gradient computation at the cost of extra\nmemory. Further, the sequential nature of the network re-\nquires the activations for all the layers to be cached in be-\nfore the loss gradients are calculated and the cached mem-\nory is freed. This dependence signi\ufb01cantly affects the peak\nmemory usage which thus becomes linearly dependent on\nthe network depth D.\n3.1.3\nLearning without caching activations\nAs noted in \u00a73.1.1, an input transformed with the reversible\ntransformation T allows recalculating the input from the\noutput of the transformation. Hence, a network composed\nof such reversible transformations does not need to store\nintermediate activations since they can be recomputed eas-\nily in the backward pass from the output. However the re-\nversible transformation T places an important constraint on\nthe property of the learnt function.\n3\nLinear\nLinear  \nLinear  \nLayerNorm\nLayerNorm\nLinear  \nSoftmax\nMLP\nAttention\nBlock\nMLP\nBlock\n O1\n O2\n I2\n I1\n(a) Rev-ViT Block\n I2\n I1\nQconv \nKconv \nVconv \nLinear \nLinear  \nLinear  \n O1 = O2\nLinear   \nLinear\nFusion Block\nLayerNorm\nLayerNorm\nMLP\nSoftmax\n Pooling\nAttention Block\nMLP Block\nMLP Block\n(b) Stage-Transition Rev-MViT Block\nKconv\nVconv\nLinear   \nLinear  \nLinear  \nLinear  \n I1\n I2\nLayerNorm\nLayerNorm\nMLP\nSoftmax\n O1\n O2\nMLP Block\nPooling\nAttention Block\n(c) Stage-Preserving Rev-MViT Block\nFigure 2. Reversible ViT is a two-residual-stream architecture composed of a stack of Reversible ViT blocks (a) that transforms the inputs\nI1 and I2 with the ViT design [15], but in our reversible fashion. Reversible MViT is a two-residual-stream architecture as well, made\nup of a stack of two type of blocks \u2013 (b) The stage-transition blocks that act as coupling between the residual streams as well as perform\nchannel upsampling and resolution downsampling and (c) the stage-preserving blocks that form the majority of the computational graph\nand propagate information preserving input feature dimension.\nEquidimensional Constraint. As mentioned in \u00a73.1.1, the\nfunctions F and G need to be equidimensional in input and\noutput spaces. Hence, the feature dimensions need to re-\nmain constant under T. While this constraint is an obstruc-\ntion for other vision architectures such as ResNets [27] that\nrequire a change of feature dimensions, it is easily satis\ufb01ed\nin the Vision Transformer architecture [15] which maintains\na constant feature dimension throughout the layers.\n3.2. Reversible Vision Transformers\n3.2.1\nAdapting ViT to Two-Residual-Streams\nFig. 2a shows the reversible transformation T adapted to the\nVision Transformer architecture [15]. The input consists of\ntwo partitioned tensors I1 and I2 that are transformed as per\nthe equation 3.1.1 maintaining reversibility. This leads to a\ntwo-residual-stream architecture where each of the inputs\nI1 and I2 maintain their own residual streams while mixing\ninformation with each other using functions F and G. Fol-\nlowing ViT [15], we use the Multi-head attention and the\nMLP sublocks as functions F and G respectively.\n3.2.2\nBoundary Conditions\nAs the ViT architecture only uses a single residual stream,\nthe architecture needs to be modi\ufb01ed to support the two-\nresidual-stream design (\u00a73.2.1). We propose the following:\n1. Initiation. We keep the stem intact and send the patchi-\n\ufb01cation output activations to I1 and I2. Note that this de-\nsign choice is different from [23] which proposes to split in\nhalves along the channel dimensions.\n2. Termination. The two residual paths need to be fused\nbefore the \ufb01nal classi\ufb01er head to preserve information. We\npropose to layer-normalize the inputs \ufb01rst, followed by con-\ncatenation, to reduce the fusion computational overhead.\n3.2.3\nRecon\ufb01guring Residual Connections\nResidual connections play a key role for signal propagation\nin deep networks [27]. The reversible transform T itself\nalso depends crucially on the residual connections between\nthe two streams to maintain reversibility. Interestingly, we\nobserve a key relationship between the residual connections\nand signal propagation in Reversible Vision Transformer.\nNote that while it is common practice for neural net-\nwork blocks to be wrapped around a residual connection\nfor better gradient \ufb02ow [27], there is no such connection\nfor either the I1 or I2 inputs. Speci\ufb01cally, internal residual\nconnections around the MLP and attention sub-blocks for\nboth the I1 and I2 streams are absent. Instead, the resid-\nual connections for each residual stream \ufb02ows through the\nother stream, operating through the inherent skip connec-\ntion present in the reversible transformation T (\u00a73.1.1). We\n\ufb01nd these internal skip connections detrimental to training\nconvergence for deeper models while bringing no additional\ngain for shallower models and choose to omit them entirely\nfor reversible vision transformer blocks.\n4\n3.3. Reversible Multiscale Vision Transformers\nThe recently proposed MViT architecture develops a fea-\nture hierarchy inside the model by downsampling the visual\nresolution and upsampling the channel dimension. It ob-\ntains state-of-the-art results on both image and video clas-\nsi\ufb01cation benchmarks. To showcase the \ufb02exibility of the\nreversible design, we adapt the MViT model to Reversible\nMultiscale Vision Transformers. We propose to compose\nthe Reversible MViT architecture in the same structure as\nthe MViT model but using two different layers \u2013 the Stage\nTransition and the Stage-Preserving blocks.\n3.3.1\nStage-Transition Block\nFigure 2b depicts the architecture of the proposed stage-\ntransition block. The stage-transition block closely follows\nthe design of the resolution upsampling blocks in MViT\n[18] with the following crucial modi\ufb01cations:\nLateral Connections.\nThe residual streams I1 and I2 are\nfused with lateral connections at the start of the stage-\ntransition block. This allows ef\ufb01cient computation of the\nresolution downsampling and feature upsampling without\nrepeat computation in each stream separately.\nFeature Upsampling.\nMViT performs feature upsam-\npling in the last MLP block before the resolution upsam-\npling block. We propose to move the channel upsampling\nstage inside the pooling attention sub-block of the stage-\ntransition block. Speci\ufb01cally, we propose to upsample the\nQuery, Key and Value vectors in the linear layer following\nthe pooling channel-wise convolutional layers (Figure 2b\nand 2c). This was the dual bene\ufb01t of (A) allowing all fea-\nture dimension changes to take place in sync inside the same\nblock and allowing other blocks to keep feature dimensions\nintact, a virtue of reversible architectures (\u00a73.1.3) and (B)\nsaving additional computation from being used in the prior\nMLP and pooling layers. We follow the same boundary\nconditions at the stage-transition blocks as in the reversible\nvision transformer architecture (\u00a73.2.2).\n3.3.2\nStage-Preserving Block\nFigure 2c shows the reversible transformation T (\u00a73.1.1)\nadapted to the Multiscale Vision Transformer architec-\nture [18].\nThe design closely resembles that of the re-\nversible vision transformer block (Figure 2a) with the ad-\ndition of multi-head pooling attention [18]. Note that even\nthough the attention uses pooling on key and value ten-\nsors, thereby changing the sequence length, the output di-\nmensions are still preserved. Hence, the stage-preserving\nblock still follows the equidimensional constraint (\u00a73.1.3)\nand hence can be made fully reversible and learnt without\ncaching activations.\nSince each stage-transition block changes the spatiotem-\nporal resolution, they occur only a limited number of times\nin the entire MViT network. In other words, the majority\nof the computation as well as memory usage is performed\nwithin the stage-preserving blocks and is fully reversible.\nWe follow the same residual connection circuit (\u00a73.2.3) as\nin Reversible Vision Transformer blocks for both the stage-\ntransition and the stage-preserving blocks.\n4. Results\nDatasets.\nWe benchmark both the Reversible Vision\nTransformer and the Reversible Multiscale Vision Trans-\nformer architectures extensively across image classi\ufb01cation\n(ImageNet [11]), video classi\ufb01cation (Kinetics 400 [36] &\nKinetics 600 [5]) and object detection (MS-COCO [45]).\nAcross all the benchmarks, we observe signi\ufb01cant memory\nsavings by using the reversible architecture with negligible\nto no accuracy change. All presented results and ablations\nare trained from random initialization, except for COCO\nwhere we initialize from ImageNet weights.\n4.1. Image Classi\ufb01cation\nSettings.\nWe benchmark our proposed models on im-\nage classi\ufb01cation on the ImageNet-1K dataset [11] with\n\u223c1.28M images among 1000 classes. We follow training\nrecipes [17] for both ViT [15] and MViT [18] models with\ncertain crucial adaptions (\u00a76). All models are trained from\nrandom initialization without EMA for 300 epochs except\nfor ViT-L and Rev-ViT-L which follow a 200 epoch train-\ning recipe. Training details are in Supplementary.\nResults.\nTable 1 shows the results for Reversible Vision\nand Reversible Multiscale Vision Transformers across dif-\nferent models and FLOP regimes. We benchmark all the\nmodels on a single 16 GB V100 GPU under 224 \u00d7 224 im-\nage size and otherwise identical conditions. The maximum\nbatch size is obtained as the highest number of images in\na batch than can train without running out of GPU mem-\nory. The memory per image is measured as the peak GPU\nmemory each image occupies during training.\nWe note that Reversible Vision Transformers match the\nFLOP and parameter speci\ufb01cations of their non-reversible\ncounterparts exactly owing to the parsimonious design of\nthe reversible vision transformer block (\u00a73.2).\nThe Re-\nversible Multiscale Vision Transformer has slightly higher\nFLOPs due to the stage-transition (\u00a73.3.1) stages while\nstill being very GPU memory ef\ufb01cient owing to the stage-\npreserving (\u00a73.3.2) stages.\n5\nmodel\nAcc Memory\n(MB/img)\nMaxiumum\nBatch Size GFLOPs Param (M)\nResNet-101 [29]\n76.4 118.7\n112\n7.6\n45\nResNet-152 [29]\n77.0 165.2\n79\n11.3\n60\nRegNetY-4GF [58]\n80.0 101.1\n136\n4.0\n21\nRegNetY-12GF [58] 80.3 175.2\n75\n12.1\n51.8\nRegNetY-32GF [58] 80.9 250.2\n46\n32.3\n32.3\nSwin-T [48]\n81.3 -\n-\n4.5\n29\nViT-S [63]\n79.9 66.5\n207\n4.6\n22\nRev-ViT-S\n79.9 8.8 \u21937.5\u00d7\n1232 \u21915.9\u00d7\n4.6\n22\nViT-B [63]\n81.8 129.7\n95\n17.6\n87\nRev-ViT-B\n81.8 17.0 \u21937.6\u00d7\n602 \u21916.3\u00d7\n17.6\n87\nRegNetY-8GF [58]\n81.7 147.2\n91\n8.0\n39\nCSWin-T [14]\n82.7 -\n-\n4.3\n23\nSwin-S [48]\n83.0 -\n-\n8.7\n50\nViT-L\n81.5 349.3\n26\n61.6\n305\nRev-ViT-L\n81.4 22.6 \u219315.5\u00d7 341 \u219113.1\u00d7\n61.6\n305\nMViT-B-16 [18]\n82.8 153.6\n89\n7.8\n37\nRev-MViT-B-16\n82.5 66.8 \u21932.3\u00d7\n157 \u21911.8\u00d7\n8.7\n39\nTable 1. Comparison to prior work on ImageNet-1K classi\ufb01-\ncation. All memory and maximum batch size are on 224\u00d7224\ninput resolution on a 16G V100 GPU. Rev-ViT and Rev-MViT\nmatch performance across different FLOP regimes at a fraction of\nthe per-input GPU memory cost.\nIncreasing memory savings with depth.\nIn Table 1,\nwe observe that our Rev-ViT matches the performance of\nvanilla ViT to very close \ufb01delity across all model vari-\nants (Small, Base and Large) and FLOP regimes. Since\nthe memory used per image is linearly dependent on the\ndepth of the model for vanilla networks (\u00a73.1.2), the mem-\nory gains of the reversible model increases as the network\nscales in depth. Notably, while the Reversible ViT-S al-\nready enjoys an impressive memory saving of about 86.8%\n(equivalent to a 7.6\u00d7 reduction) with respect to the vanilla\nViT-S model, the gain increases further to 15.5\u00d7 or, about\n93% memory savings for the Reversible ViT-L model.\nEquivalently, the saved memory can be used to increase\nthe training batch size where we observe a similar trend as\nwell. While reversible ViT-S model achieves a 6.1\u00d7 in-\ncrease in batch size on the ViT-S model, the effect is more\nfor ViT-L model where the maximum batch size increases\nby 14.3\u00d7 jumping from a small 24 image per batch to 344\nimages. This is a very favorable trend, since it is indeed the\ndeeper models that hit the GPU memory wall [21].\nFurther, hierarchical vision transformers such as MViT\nalso enjoy a memory saving of about 52.1% without suf-\nfering any signi\ufb01cant drop in performance. The memory\nsavings in Rev-MViT are smaller compared to the ViT vari-\nants because of the stage-transition blocks in hierarchical\nmodels (\u00a73.3.1) that require storing the input activations due\nto the non-reversible nature of pooling attention stemming\nfrom the feature dimension change [18].\nmodel\ntop-1 Mem\n(GB)\nMax\nBS\nGFLOPs\u00d7\nviews\nParam\nTwo-Stream I3D [5]\n71.6\n-\n-\n216 \u00d7 NA\n25.0\nR(2+1)D [66]\n72.0\n-\n-\n152\u00d7115\n63.6\nTwo-Stream R(2+1)D [66] 73.9\n-\n-\n304 \u00d7 115\n127.2\nOct-I3D + NL [8]\n75.7\n-\n-\n28.9\u00d73\u00d710\n33.6\nip-CSN-152 [65]\n77.8\n-\n-\n109\u00d73\u00d710\n32.8\nSlowFast 4\u00d716, R50 [19]\n75.6\n-\n-\n36.1 \u00d7 30\n34.4\nSlowFast 8\u00d78, R101 [19]\n77.9\n-\n-\n106 \u00d7 30\n53.7\nSlowFast 8\u00d78 +NL [19]\n78.7\n-\n-\n116\u00d73\u00d710\n59.9\nViT-B-VTN-IN-1K [52]\n75.6\n-\n-\n4218\u00d71\u00d71\n114.0\nViT-B-VTN-IN-21K [52]\n78.6\n-\n-\n4218\u00d71\u00d71\n114.0\nMViT-B-16 , 16\u00d74\n78.4 1.27\n10\n70.5\u00d71\u00d75\n36.6\nRev-MViT-B-16, 16\u00d74\n78.5 0.64\n20\n64\u00d71\u00d75\n34.9\nTable 2. Comparison to prior work on Kinetics-400 video clas-\nsi\ufb01cation. Single view inference cost is reported along with used\nnumber of views (FLOPs\u00d7viewspace\u00d7viewtime). Memory (Mem)\nreported in Gigabytes per input clip. Maximum Batch Size (Max\nBS) measured as the maximum possible single GPU batch size.\nAll measurements are performed on a single 16G V100 GPU.\nmodel\ntop-1 Mem\n(GB)\nMax\nBS\nGFLOPs\u00d7\nviews\nParam\nSlowFast 16\u00d78 +NL [19]\n81.8\n-\n-\n234\u00d73\u00d710\n59.9\nX3D-XL\n81.9\n-\n-\n48.4\u00d73\u00d710\n11.0\nViT-B-TimeSformer-IN-21K [3] 82.4\n-\n-\n1703\u00d73\u00d71\n121.4\nViT-L-ViViT-IN-21K [1]\n83.0\n-\n-\n3992\u00d73\u00d74\n310.8\nMViT-B-16, 16\u00d74\n81.3\n-\n-\n70.3\u00d71\u00d75\n36.6\nMViT-B-16, 32\u00d73\n83.4\n-\n-\n170\u00d71\u00d75\n36.8\nMViT-B-24, 32\u00d73\n83.8 4.40\n2\n236\u00d71\u00d75\n52.9\nRev-MViT-B-24, 32\u00d73\n83.7 1.64\n7\n223\u00d71\u00d75\n51.8\nTable 3. Comparison to prior work on Kinetics-600 video clas-\nsi\ufb01cation. Results under same settings as Kinetics-400 in Table\n2.\n4.2. Video Classi\ufb01cation\nSettings.\nWe also benchmark Rev-MViT-B models on\naction classi\ufb01cation on Kinetics-400 [36] and Kinetics-\n600 [5] datasets. All the models are trained from scratch\nwith training recipes adapted from [18].\nResults.\nTable 2 and 3 present the results on action recog-\nnition task on the Kinetics-400 [36] and Kinetics-600 [5]\nrespectively.\nFor action recognition, we benchmark our\nadapted Reversible MViT model and report top-1 and top-5\nperformance for both datasets. Similar to the image classi-\n\ufb01cation benchmark, we observe that the Reversible MViT\nmodels closely match the accuracy for their non reversible\ncounterparts at a fraction of the memory cost.\nIncreasing video model batch sizes.\nWe note that our\nadapted Reversible MViT forms a very competitive video\nrecognition model. Speci\ufb01cally, for both Kinetics-400 (Ta-\nble 2) and Kinetics-600 (Table 3) the reversible models\n6\nModel\nAPbox APmask Memory(GB) GFLOPs Param (M)\nRes50 [28]\n41.0\n37.1\n-\n260\n44\nRes101 [28]\n42.8\n38.5\n-\n336\n63\nX101-64 [73]\n44.4\n39.7\n-\n493\n101\nPVT-L [69]\n44.5\n40.7\n-\n364\n81\nMViT-B\n48.2\n43.9\n18.9\n668\n57\nRev-MViT-B\n48.0\n43.5\n10.9\n683\n58\nTable 4. Comparison on MS-COCO object detection. Rev-\nMViT achieves competitive performance to MViT across all met-\nrics at 1.7\u00d7 lower memory footprint.\nmatch the overall accuracy very closely at only 51.5% and\n37.2% of the memory cost respectively.\nThis allows a batch size increase of 2\u00d7 on the 16 layer,\n70.5 GFLOPs Kinetics-400 MViT-B-16 model and of 3.5\u00d7\non the 24 layer, 236 GFLOPs Kinetics-600 MViT-B-24\nmodel, a very bene\ufb01cial result for large video models which\nare often severely memory limited and trained with very\nsmall batch sizes (Table 3).\nMoreover, due to the more\nef\ufb01cient design of stage-transition blocks in Rev-MViT\n(\u00a73.3.1), i.e. bringing the dimension upsampling operation\ninside the pooling attention instead of being performed in\nthe prior MLP stage [18], the Rev-MViT are also slightly\nmore parameter and FLOP ef\ufb01cient on both Kinetics.\n4.3. Object Detection\nWe benchmark the proposed Rev-ViT-B and Rev-MViT-\nB models on object detection on MS-COCO [45] as well.\nAll the models are trained on 118K training images and\nevaluated on the 5K validation images. We take the ViT-B\nand MViT-B backbones pre-trained on IN and use the stan-\ndard Mask R-CNN [26] as the detection framework. All\nmodels are trained with a standard 3\u00d7 schedule (36 epochs).\nFor MViT, we integrate the multi-scale backbone with the\nfeature pyramid network [44]. Referring to Table 4 we ob-\nserve that, the Rev-MViT-B model closely matches the AP\nperformance on MViT-B at only 54.8% of the memory cost.\n4.4. Ablations\nStronger\nInherent\nRegularization.\nAcross\ndifferent\nmodels and datasets, we \ufb01nd that at the same FLOP and pa-\nrameter speci\ufb01cations, the reversible models tend to have\nstronger inherent regularization than their non-reversible\ncounterparts. Hence, training recipes for reversible vision\ntransformers have lighter repeated augmentations, smaller\naugmentation magnitudes and consequently, higher weight\ndecay. Table 5 shows the effects of these recipe changes\non Rev-ViT-B. We also observe similar effects on other\nRev-ViT and Rev-MViT models where a modi\ufb01ed training\nrecipe with lighter augmentations and higher weight decay\nplay a crucial role in matching performance.\nLateral Fusion Strategies.\nThe stage-transition blocks\nemploy residual stream fusion blocks for mixing informa-\ntion between I1 and I2 (\u00a73.3.1). We explore several fusion\nTraining Improvement\nTrain Acc Top-1 ImageNet Acc\nNa\u00a8\u0131ve Rev-ViT-B\n15.3\n12.1\n+ Re-con\ufb01guring residual streams\n82.1\n77.2\n+ Repeated Augmentation\n84.9\n80.6\n+ Lighter Augmentation magnitude\n93.2\n81.0\n+ Stronger Stochastic Depth\n92.0\n81.4\n+ Higher weight decay\n91.0\n81.8\nRev-ViT-B\n91.0\n81.8\nTable 5. Rev-ViT-B Training Recipe. We observe that reversible\ntransformers tend to have a stronger inherent regularization and\nrequire a lighter augmented training recipe for peak performance.\nStage-Transition\nFusion\nTermination Fusion\nTrain Acc Top-1 Acc\nMax\nNorm \u2192Concat\n78.1\n81.7\nConcat\nNorm \u2192Concat\n79.1\n82.0\n2\u00d7-MLP\nNorm \u21922\u00d7-MLP\n80.2\n81.8\n2\u00d7-MLP + 0.2 dp\nNorm \u21922\u00d7-MLP \u21920.5dp\n77.1\n81.2\n2\u00d7-MLP\nNorm \u21921-layer\n53.6\n82.1\n2\u00d7-MLP\nNorm \u21921-layer \u21920.2dp\n64.0\n82.4\nNorm \u21922\u00d7-MLP Norm \u2192Concat\n79.4\n82.3\nNorm \u21922\u00d7-MLP Norm \u21921-layer \u2192\n0.2dp \u2192Norm\n78.3\n82.3\n4\u00d7-MLP\nNorm \u2192Concat\n80.4\n82.3\n2\u00d7-MLP\nConcat \u2192Norm\n80.5\n82.2\n2\u00d7-MLP\nNorm \u2192Concat\n80.1\n82.5\nTable 6. Lateral Fusion Strategies. Residual streams I1 and I2\nare fused in state-transition blocks (\u00a73.3.1) as well as on termina-\ntion (\u00a73.2.2) before the network head. We \ufb01nd fusion strategy to\nplay a key role for ReV-MViT performance. Rev-MViT-B uses a\n2-layer MLP with 2\u00d7 hidden dimensions in stage-transition blocks\n(gray). Please see Section 4.4 for details.\nstrategies in Table 6 using a combination of: (A) n\u00d7-MLP:\nTwo layer perceptrons with n times the hidden dimension\nand GELU activations. (B) 0.n dp: n \u00d7 10 percent dropout\non output activations. (C) Simple operators such as channel-\nwise maximum of I1 and I2 activations, and channel-wise\nconcatenation of tensor.\nLateral connections in stage-transition stages allows ef-\nfective information mixing between the residual streams\nand hence increases network capacity. For example, com-\npared to concatenation, 2\u00d7-MLP increases to training accu-\nracy by 1% and also the top-1 performance by 0.5%. How-\never an even heavier strategy, such as 4\u00d7-MLP widens the\ngeneralization gap and promotes over-\ufb01tting. Note that the\ntraining accuracy is often lower than the top-1 performance\nbecause of training data augmentations.\nRe-con\ufb01guring residual connections.\nAs discussed in\n\u00a73.2.3, the reversible vision transformer design removes the\nskip connections that are commonly used inside the Atten-\ntion and MLP blocks (Figure 2). Speci\ufb01cally, for all of the\nreversible blocks in Rev-MViT and Rev-ViT, the inputs I1\nand I2 do not have residual connections that allow resid-\nual signal propagation by directly skipping their respective\nfunctions (MLP for I2 and Attention for I1). Instead their\n7\n2\n4\n8\n12\nNumber of Transformer Blocks\n10\n20\n30\n40\n50\n60\n70\n80\nImageNet Top-1 Acc\nOurs (without caching)\nOurs (with caching)\nWith internal residuals\n(a) Activation caching and internal residuals.\n2.1x\n2.3x\n(b) Training throughput vs. Model Depth\n12\n16\n32\n36\n42\nNumber of Transformer Blocks\n100\n101\n102\nMax Training Batch Size\nB\nL\nH\nB\nL\nH\nBatch Size = 1\nRev-ViT-L\nRev-ViT-H\nRev-ViT\nViT-L\nViT-H\nViT\n(c) Reversible training and maximum batch size.\nFigure 3. Ablation Experiments. (a): We observe that (1) Learning without activation caching does not hurt reversible accuracy for Rev-\nViT-B of varying depths and (2) Internal residual connections train well for shallow models but the training diverges for deeper models.\n(b): Rev-MViT has higher throughput for higher input resolution and deeper MViT models increasing up to 2.3\u00d7 at 224 resolution for 80\nlayers. (c): We benchmark the maximum batch size for Rev-ViT Base (B), Large (L) and Huge (H) and their non-reversible counterparts.\nresiduals are performed via the other residual stream oper-\nating through the reversible transform T (\u00a73.1.1).\nWe ablate this design choice for the ViT architecture in\nFigure 3a. We vary the model depth without changing any\nother model dimensions and observe the performance of\nthe two reversible variants i.e., with and without internal\nskip connections. We note that while the na\u00a8\u0131ve version with\ninternal skip connections trains stably for shallow models,\nfor deeper models the accuracy drops signi\ufb01cantly. On the\nother hand, Rev-MViT scales well with depth, just as accu-\nrate as the other variant at shallower depths but signi\ufb01cantly\nbetter with deeper models.\nEffect of learning without caching activations.\nFig-\nure 3a also compares the image classi\ufb01cation performance\nof the Rev-ViT-B architecture trained with and without\ncaching activations. This allows us to disentangle the ef-\nfect of the proposed residual con\ufb01gurations necessary for\nreversible vision transformer from any artefacts that might\nresult from learning without caching activations. However,\nfor all depths we notice the Rev-ViT-B performance trained\nwithout caching activations to closely track the performance\nof the same architecture trained with caching. The slight\ndifference at depth 12 results might stem from the training\nrecipe being adapted for the actual Rev-ViT-B architecture\ntrained without activations.\nModel size and Input Resolution vs. Throughput.\nFig-\nure 3b shows the training throughput comparisons for dif-\nferent models sizes at 224 and 320 input resolutions. We\nnote that while for smaller models such as, MViT-B with a\ndepth 12 layers, Rev-MViT-B has a slightly smaller training\nthroughput (98.5 vs. 86.0), the additional re-computation\nburden of intermediate activations is easily overcome at\nboth higher resolution training as well as for deeper models.\nIn particular, at 224 resolution, the 24-layer and 48-layer\nRev-MViT models have similar throughput as the MViT\nmodels increasing upto to 2.1\u00d7 higher throughput at 384\nresolution and 2.3\u00d7 higher throughput at 384 resolution for\nthe 80 layer depth models. Further, the rate of memory in-\ncrease for deeper models is much lower for reversible vari-\nants than vanilla networks, allowing scaling to much deeper\nmodels without any additional training infrastructure bur-\nden or memory requirement like with gradient checkpoint-\ning or model parallelism.\nMaximum batch-size.\nWe benchmark the maximum pos-\nsible batch size for Rev-ViT Base (B), Large (L) and Huge\n(H) and their non-reversible counterparts in Fig.3c. We ex-\ntrapolate the trend (denoted by\n) to larger models by\nscaling ViT-L and ViT-H in depth (keeping other model di-\nmensions constant) and benchmark the maximum batch size\nfor their reversible counterparts.\nModel size vs. GPU memory footprint.\nFigure 1 plots\nthe GPU Memory footprint for both Rev-ViT and Rev-\nMViT family of models as well as for several other prior\nnetworks such as MViT [18], ViT [15], ResNets and Reg-\nNetY [58]. We note that at \ufb01xed GFLOPs, reversible vari-\nants are extremely memory ef\ufb01cient going upto 4.5\u00d7 for\nMViT and 15.5\u00d7 for ViT surpassing prior convolutional\nvariants by orders of magnitude.\n5. Conclusion\nWe present Reversible Vision Transformers, memory-\nef\ufb01cient reversible architectural adaptations of ViT and\nMViT models. We benchmark across several tasks, such as\nimage classi\ufb01cation, object detection and video classi\ufb01ca-\ntion and across several metrics, such as model complexity,\nthroughput, accuracy and memory usage. Given any speci-\n\ufb01cation, our Rev-ViT and Rev-MViT match the accuracy of\nnon-reversible variants at a tiny fraction of the memory cost\nwhile maintaining similar training throughput for smaller\nmodels and up to 2.3\u00d7 higher throughput for larger mod-\nels. Speci\ufb01cally, we observe that the Rev-ViT and the Rev-\n8\nMViT models achieve upto 15.5\u00d7 and 4.5\u00d7 lighter memory\nfootprint than ViT and MViT models respectively.\nFollow-Up Work\nChen et al. [77] apply the proposed Reversible Vision\nTransformer architecture design for Reversible Swin Trans-\nformers and apply the model for memory-ef\ufb01cient temporal\naction localization. Temporal action localization involves\ndetecting precise temporal frame positions for the start and\nend boundaries of an action and hence needs to be per-\nformed at on a densely sampled video. Dense frame sam-\npling causes GPU memory overheads during training that\nprohibit \ufb01netuning the backbone end-to-end on the tempo-\nral action localization task (TAL). Reversible backbone al-\nleviate the memory overhead and allow ef\ufb01cient end-to-end\nTAL training, thereby providing signi\ufb01cant localization per-\nformance boost.\nConcurrently, [80] proposes a training procedure for\nreversible transformers that allows speeding up training\nwhile ensuring exact replication of the original computa-\ntion. In particular, [80] proposes to stagger the activation\nre-computation one transformer block ahead of the gradient\ncomputation using the recomputed activations of the previ-\nous block. This allows the activations to be available for\ngradient calculation of the next block, as soon as the previ-\nous block \ufb01nishes. Hence, the gradient calculation step does\nnot need to wait for activation recomputation, effectively\nhiding the latency of the burden of re-computation behind\nlatency of graident calculations, thus effectively speeding\nup training. This requires maintaining separate cuda work-\nstreams that process the above two steps asynchronously.\nDepending on the hardware and computation size, there can\nbe signi\ufb01cant speedups from such operator parallelization.\nAcknowledgements\nThe authors would like to thank Harshayu Girase for\nhelp with benchmarking models, Amir Gholami, Ajay Jain\nand Nikita Kiatev for helpful research discussions and ref-\nerence suggestions, Ajay Jain, Matthew Tancik and Hang\nGao for writing discussions and Shubh Gupta, Suzie Petryk,\nHang Gao, Abhinav Agarwal, Medhini Narasimhan and\nAmur Ghosh for proofreading the manuscript.\nAppendix\nA. Architecture Details\nReversible Vision Transformers\nTable 7 shows the ar-\nchitectures for all the Reversible Vision Transformer Mod-\nels. All models closely follow the original ViT architec-\ntures [15] in matched performance, parameters, FLOPs and\nmuch lower memory footprint (Table 1). Output sizes de-\nnote the tensor shapes of the two residual streams at the\nend of each reversible Vision Transformer block. Note that\neven though the intermediate activations are twice the non-\nreversible variant, the actual memory needed is much lower\nbecause of memory reuse in reversible training. Further, the\nFLOPs are matched since each layer is performed only one\nof the two streams.\nReversible Multiscale Vision Transformers\nTable 8\nshows the architecture for the Rev-MViT-B model for im-\nage classi\ufb01cation. The backbone is made-up of two stages\n\u2013 Stage-transition blocks that increase the channel capacity\nand down-sample the resolution and the reversible Stage-\npreserving blocks that perform the majority of computation\nwithout changing feature dimensions. Similar to Rev-ViT,\nthe output sizes of both the streams are denoted. Fusion\nblocks operate on Y1 and Y2 together, hence operate with\ncomputationally light operations (Table 6).\nB. Training Settings\nImageNet.\nTable 9 shows the training recipes for ViT-L\nand Rev-ViT-L models presented in Table 1. Note that ViT-\nL is quite heavy with 61.6 GFLOPs and hence we adopt a\nshorter 200 epochs recipe for faster experiment cycle for\ndeveloping Rev-ViT-L. Smaller ViT models \u2013 ViT-S and\nViT-B \u2013 are trained according to the Data ef\ufb01cient trans-\nformers [63] and are all trained for 300 epochs. Hence, the\naccuracy difference between ViT-L which achieves 81.5%\nwhile ViT-B achieves 81.8% overall. MViT-B model fol-\nlows the 300 epochs recipe as well proposed in [18].\nKinetics-400 & Kinetics-600.\nWe follow the recipes pro-\nposed in [18] to train the Rev-MViT-B architecture (Table 8)\nfollowing crucial modi\ufb01cations shown in Table 5.\nMS-COCO.\nFor object detection experiments, we adopt\nthe Mask R-CNN [26] object detection framework in Detec-\ntron2 [72]. We follow the same training settings from [48],\nAdamW optimizer [50] (\u03b21, \u03b22 = 0.9, 0.999, base learning\nrate 1.6e\u22124 for base size of 64, and weight decay of 0.1),\nand 3x schedule (36 epochs). The drop path rate is set as\n0.4. We use PyTorch\u2019s automatic mixed precision during\ntraining.\nReferences\n[1] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen\nSun, Mario Lu\u02c7ci\u00b4c, and Cordelia Schmid. Vivit: A video vi-\nsion transformer. In Proc. ICCV, 2021. 2, 6\n[2] Jens Behrmann, Will Grathwohl, Ricky TQ Chen, David Du-\nvenaud, and J\u00a8orn-Henrik Jacobsen. Invertible residual net-\nworks. In International Conference on Machine Learning,\npages 573\u2013582. PMLR, 2019. 2\n9\nstage\noperators\noutput sizes\ndata\n224\u00d7224\npatch\n1\u00d716\u00d716, 384\n384\u00d714\u00d714\nstride 1\u00d716\u00d716\nrev\n\u0014\nF : MHA(384)\nG : MLP(1536)\n\u0015\n\u00d712\n\u0014\nY1 : 384\u00d714\u00d714\nY2 : 384\u00d714\u00d714\n\u0015\n(a)\nRev-ViT-S\nwith\n4.6G\nFLOPs,\n22M\nparam,\n8.8MB/img memory, and 79.9% top-1 accuracy.\nstage\noperators\noutput sizes\ndata\n224\u00d7224\npatch\n1\u00d716\u00d716, 768\n768\u00d714\u00d714\nstride 1\u00d716\u00d716\nrev\n\u0014\nF : MHA(768)\nG : MLP(3072)\n\u0015\n\u00d712\n\u0014\nY1 : 768\u00d714\u00d714\nY2 : 768\u00d714\u00d714\n\u0015\n(b)\nRev-ViT-B\nwith\n17.6G\nFLOPs,\n87M\nparam,\n17MB/img memory, and 81.8% top-1 accuracy.\nstage\noperators\noutput sizes\ndata\n224\u00d7224\npatch\n1\u00d716\u00d716, 1024\n1024\u00d714\u00d714\nstride 1\u00d716\u00d716\nrev\n\u0014\nF : MHA(1024)\nG : MLP(4096)\n\u0015\n\u00d724\n\u0014\nY1 : 1024\u00d714\u00d714\nY2 : 1024\u00d714\u00d714\n\u0015\n(c)\nRev-ViT-L\nwith\n61.6G\nFLOPs,\n305M\nparam,\n22.6MB/img memory, and 81.4% top-1 accuracy.\nTable 7. Reversible Vision Transformer Architectures: Rev-\nViT are reversible adaption of ViT with exactly matched FLOPs,\nparameters and accuracy under identical conditions but with much\nlower GPU memory footprints.\nstage\noperators\noutput sizes\ndata\n224\u00d7224\ncubi\ufb01cation\n7\u00d77, 96\n96\u00d756\u00d756\nstride 4\u00d74\nStage-Preserving\n\u0014\nF : MHPA(96)\nG : MLP(384)\n\u0015\n\u00d71\n\u0014\nY1 : 96\u00d756\u00d756\nY2 : 96\u00d756\u00d756\n\u0015\nStage-Transition\n\uf8ee\n\uf8f0\nFUSION(192)\nMHPA(192)\nMLP(768)\n\uf8f9\n\uf8fb\u00d71\n192\u00d728\u00d728\nStage-Preserving\n\u0014\nF : MHPA(192)\nG : MLP(768)\n\u0015\n\u00d71\n\u0014\nY1 : 192\u00d728\u00d728\nY2 : 192\u00d728\u00d728\n\u0015\nStage-Transition\n\uf8ee\n\uf8f0\nFUSION(384)\nMHPA(384)\nMLP(1536)\n\uf8f9\n\uf8fb\u00d71\n384\u00d714\u00d714\nStage-Preserving\n\u0014\nF : MHPA(384)\nG : MLP(1536)\n\u0015\n\u00d710\n\u0014\nY1 : 384\u00d714\u00d714\nY2 : 384\u00d714\u00d714\n\u0015\nStage-Transition\n\uf8ee\n\uf8f0\nFUSION(768)\nMHPA(768)\nMLP(3072)\n\uf8f9\n\uf8fb\u00d71\n768\u00d77\u00d77\nStage-Preserving\n\u0014\nF : MHPA(768)\nG : MLP(3072)\n\u0015\n\u00d71\n\u0014\nY1 : 768\u00d77\u00d77\nY2 : 768\u00d77\u00d77\n\u0015\nTable 8.\nRev-MViT-B with 8.7G FLOPs,\n39M param,\n66.8MB/img memory, and 82.5% top-1 accuracy is reversible\nadaption of MViT-B architecture [15].\n[3] Gedas Bertasius, Heng Wang, and Lorenzo Torresani.\nIs\nspace-time attention all you need for video understanding?\nTraining Hyperparameter\nViT-B\nRev-ViT-B\nLearning Rate\n1e-4\n7e-5\nRandom augment Repeats (N)\n1\n2\nRandom augment Magnitude (M)\n9\n7\nOptimizer Momentum\n(0.9, 0.95) (0.9, 0.999)\nWeight Decay\n0.3\n0.3\nBatch Size\n4096\n4096\nEpochs\n200\n200\nLabel Smoothing\n0.1\n0.1\nDrop Path Rate\n0.2\n0.2\nMixup\n0.8\n0.8\nCutmix\n1.0\n1.0\nTable 9. Training Recipe for ViT-L and Rev-ViT-L\nIn Proc. ICCV, 2021. 2, 6\n[4] Robin Br\u00a8ugger,\nChristian F Baumgartner,\nand Ender\nKonukoglu. A partially reversible u-net for memory-ef\ufb01cient\nvolumetric image segmentation. In International conference\non medical image computing and computer-assisted inter-\nvention, pages 429\u2013437. Springer, 2019. 2\n[5] Joao Carreira and Andrew Zisserman.\nQuo vadis, action\nrecognition? a new model and the kinetics dataset. In Proc.\nCVPR, 2017. 5, 6\n[6] Bo Chang, Lili Meng, Eldad Haber, Lars Ruthotto, David\nBegert, and Elliot Holtham. Reversible architectures for ar-\nbitrarily deep residual neural networks. In Proceedings of\nthe AAAI Conference on Arti\ufb01cial Intelligence, volume 32,\n2018. 2\n[7] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee,\nAditya Grover, Michael Laskin, Pieter Abbeel, Aravind\nSrinivas, and Igor Mordatch. Decision transformer: Rein-\nforcement learning via sequence modeling. arXiv preprint\narXiv:2106.01345, 2021. 2\n[8] Yunpeng Chen, Haoqi Fan, Bing Xu, Zhicheng Yan, Yan-\nnis Kalantidis, Marcus Rohrbach, Shuicheng Yan, and Jiashi\nFeng. Drop an octave: Reducing spatial redundancy in con-\nvolutional neural networks with octave convolution. In Proc.\nCVPR, 2019. 6\n[9] Zhengsu Chen, Lingxi Xie, Jianwei Niu, Xuefeng Liu,\nLonghui Wei, and Qi Tian. Visformer: The vision-friendly\ntransformer. In Proc. ICCV, 2021. 2\n[10] Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen,\nMatthieu Devin, Mark Mao, Marc\u2019aurelio Ranzato, Andrew\nSenior, Paul Tucker, Ke Yang, et al. Large scale distributed\ndeep networks. Advances in neural information processing\nsystems, 25:1223\u20131231, 2012. 2\n[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In Proc. CVPR, pages 248\u2013255. Ieee, 2009. 5\n[12] Laurent Dinh, David Krueger, and Yoshua Bengio. Nice:\nNon-linear independent components estimation.\narXiv\npreprint arXiv:1410.8516, 2014. 2\n[13] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Ben-\ngio.\nDensity estimation using real nvp.\narXiv preprint\narXiv:1605.08803, 2016. 2\n[14] Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming\nZhang, Nenghai Yu, Lu Yuan, Dong Chen, and Bain-\n10\ning Guo.\nCswin transformer:\nA general vision trans-\nformer backbone with cross-shaped windows. arXiv preprint\narXiv:2107.00652, 2021. 2, 6\n[15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. In Proc. ICLR, 2021.\n1, 2, 4, 5, 8, 9, 10\n[16] Christian\nEtmann,\nRihuan\nKe,\nand\nCarola-Bibiane\nSch\u00a8onlieb.\niunets: Fully invertible u-nets with learnable\nup-and downsampling.\narXiv preprint arXiv:2005.05220,\n2020. 2\n[17] Haoqi Fan, Yanghao Li, Bo Xiong, Wan-Yen Lo, and\nChristoph Feichtenhofer. Pyslowfast. https://github.\ncom/facebookresearch/slowfast, 2020. 5\n[18] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li,\nZhicheng Yan, Jitendra Malik, and Christoph Feichtenhofer.\nMultiscale vision transformers. In Proc. ICCV, 2021. 1, 2,\n5, 6, 7, 8, 9\n[19] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and\nKaiming He. SlowFast networks for video recognition. In\nProc. ICCV, 2019. 6\n[20] Marc Finzi,\nPavel Izmailov,\nWesley Maddox,\nPolina\nKirichenko, and Andrew Gordon Wilson. Invertible convo-\nlutional networks. In Workshop on Invertible Neural Nets\nand Normalizing Flows, International Conference on Ma-\nchine Learning, 2019. 2\n[21] Amir Gholami, Zhewei Yao, Kim Sehoon, Michael W. Ma-\nhoney, and Kurt Keutzer. Ai and memory wall. RiseLab\nMedium Post, 2021. 1, 6\n[22] Aidan N Gomez, Mengye Ren, Raquel Urtasun, and Roger B\nGrosse. The reversible residual network: Backpropagation\nwithout storing activations. In Proceedings of the 31st Inter-\nnational Conference on Neural Information Processing Sys-\ntems, pages 2211\u20132221, 2017. 2\n[23] Aidan N Gomez, Mengye Ren, Raquel Urtasun, and Roger B\nGrosse. The reversible residual network: Backpropagation\nwithout storing activations. In Proceedings of the 31st Inter-\nnational Conference on Neural Information Processing Sys-\ntems, pages 2211\u20132221, 2017. 4\n[24] Benjamin Graham, Alaaeldin El-Nouby, Hugo Touvron,\nPierre Stock, Armand Joulin, Herve Jegou, and Matthijs\nDouze. LeViT: A vision transformer in ConvNet\u2019s clothing\nfor faster inference. In Proc. ICCV, 2021. 2\n[25] Tristan Hascoet, Quentin Febvre, Weihao Zhuang, Yasuo\nAriki, and Tetsuya Takiguchi. Layer-wise invertibility for ex-\ntreme memory cost reduction of cnn training. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion Workshops, pages 0\u20130, 2019. 2\n[26] Kaiming He, Georgia Gkioxari, Piotr Doll\u00b4ar, and Ross Gir-\nshick. Mask R-CNN. In Proc. ICCV, 2017. 7, 9\n[27] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDelving deep into recti\ufb01ers: Surpassing human-level perfor-\nmance on imagenet classi\ufb01cation. In Proc. CVPR, 2015. 2,\n4\n[28] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition.\nIn Proc.\nCVPR, 2016. 1, 7\n[29] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nIdentity mappings in deep residual networks. In Proc. ECCV,\n2016. 6\n[30] Jonathan Ho, Xi Chen, Aravind Srinivas, Yan Duan, and\nPieter Abbeel. Flow++: Improving \ufb02ow-based generative\nmodels with variational dequantization and architecture de-\nsign.\nIn International Conference on Machine Learning,\npages 2722\u20132730. PMLR, 2019. 2\n[31] Mark Horowitz. 1.1 computing\u2019s energy problem (and what\nwe can do about it). In 2014 IEEE International Solid-State\nCircuits Conference Digest of Technical Papers (ISSCC),\npages 10\u201314. IEEE, 2014. 1\n[32] Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit,\nNoam Shazeer, Ian Simon, Curtis Hawthorne, Andrew M\nDai, Matthew D Hoffman, Monica Dinculescu, and Douglas\nEck. Music transformer. In ICLR, 2019. 2\n[33] Jun-Jie Huang and Pier Luigi Dragotti. Winnet: Wavelet-\ninspired invertible network for image denoising.\narXiv\npreprint arXiv:2109.06381, 2021. 2\n[34] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li,\nand Torsten Hoe\ufb02er.\nData movement is all you need:\nA case study on optimizing transformers.\narXiv preprint\narXiv:2007.00072, 2020. 1\n[35] Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zis-\nserman, Oriol Vinyals, and Joao Carreira. Perceiver: Gen-\neral perception with iterative attention.\narXiv preprint\narXiv:2103.03206, 2021. 2\n[36] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,\nChloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola,\nTim Green, Trevor Back, Paul Natsev, et al. The kinetics\nhuman action video dataset. arXiv:1705.06950, 2017. 5, 6\n[37] Diederik P Kingma and Prafulla Dhariwal. Glow: Gener-\native \ufb02ow with invertible 1x1 convolutions. arXiv preprint\narXiv:1807.03039, 2018. 2\n[38] Nikita Kitaev,\n\u0141ukasz Kaiser,\nand Anselm Levskaya.\nReformer:\nThe ef\ufb01cient transformer.\narXiv preprint\narXiv:2001.04451. 2, 3\n[39] Duo Li and Shang-Hua Gao.\nm-revnet:\nDeep re-\nversible neural networks with momentum.\narXiv preprint\narXiv:2108.05862, 2021. 2\n[40] Guohao Li, Matthias M\u00a8uller, Bernard Ghanem, and Vladlen\nKoltun. Training graph neural networks with 1000 layers.\narXiv preprint arXiv:2106.07476, 2021. 2\n[41] Naihan Li, Shujie Liu, Yanqing Liu, Sheng Zhao, and Ming\nLiu. Neural speech synthesis with transformer network. In\nProceedings of the AAAI Conference on Arti\ufb01cial Intelli-\ngence, volume 33, pages 6706\u20136713, 2019. 2\n[42] Shanshan Li,\nQiang Cai,\nZhuangzi Li,\nHaisheng Li,\nNaiguang Zhang, and Jian Cao. Attention-aware invertible\nhashing network. In International Conference on Image and\nGraphics, pages 409\u2013420. Springer, 2019. 2\n[43] Shaohui Li, Ziyang Zheng, Wenrui Dai, Junni Zou, and\nHongkai Xiong.\nRev-ae:\nA learned frame set for im-\nage reconstruction.\nIn ICASSP 2020-2020 IEEE Interna-\n11\ntional Conference on Acoustics, Speech and Signal Process-\ning (ICASSP), pages 1823\u20131827. IEEE, 2020. 2\n[44] Tsung-Yi Lin, Piotr Doll\u00b4ar, Ross Girshick, Kaiming He,\nBharath Hariharan, and Serge Belongie.\nFeature pyra-\nmid networks for object detection.\nIn Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 2117\u20132125, 2017. 7\n[45] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft COCO: Common objects in context. In\nProc. ECCV, 2014. 5, 7\n[46] Kang Liu, Dong Liu, Li Li, Ning Yan, and Houqiang\nLi.\nSemantics-to-signal scalable image compression with\nlearned revertible representations. International Journal of\nComputer Vision, pages 1\u201317, 2021. 2\n[47] Yang Liu, Zhenyue Qin, Saeed Anwar, Pan Ji, Dongwoo\nKim, Sabrina Caldwell, and Tom Gedeon. Invertible denois-\ning network: A light solution for real noise removal. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 13365\u201313374, 2021. 2\n[48] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\nHierarchical vision transformer using shifted windows. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 10012\u201310022, 2021. 6, 9\n[49] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\nHierarchical vision transformer using shifted windows. In\nProc. CVPR, 2022. 2\n[50] Ilya Loshchilov and Frank Hutter. Fixing weight decay reg-\nularization in adam. 2018. 9\n[51] Matthew MacKay, Paul Vicol, Jimmy Ba, and Roger\nGrosse. Reversible recurrent neural networks. arXiv preprint\narXiv:1810.10999, 2018. 2\n[52] Daniel Neimark, Omri Bar, Maya Zohar, and Dotan As-\nselmann.\nVideo transformer network.\narXiv preprint\narXiv:2102.00719, 2021. 2, 6\n[53] Mandela Patrick, Dylan Campbell, Yuki M Asano, Is-\nhan Misra Florian Metze, Christoph Feichtenhofer, Andrea\nVedaldi, Jo Henriques, et al. Keeping your eye on the ball:\nTrajectory attention in video transformers. NIPS, 2021. 2\n[54] David A Patterson. Latency lags bandwith. Communications\nof the ACM, 47(10):71\u201375, 2004. 1\n[55] Mihir Pendse, Vithursan Thangarasa, Vitaliy Chiley, Ryan\nHolmdahl, Joel Hestness, and Dennis DeCoste. Memory ef\ufb01-\ncient 3d u-net with reversible mobile inverted bottlenecks for\nbrain tumor segmentation. In International MICCAI Brain-\nlesion Workshop, pages 388\u2013397. Springer, 2020. 2\n[56] Bas Peters, Eldad Haber, and Keegan Lensink.\nFully re-\nversible neural networks for large-scale surface and sub-\nsurface characterization via remote sensing. arXiv preprint\narXiv:2003.07474, 2020. 2\n[57] Patrick Putzky and Max Welling. Invert to learn to invert. Ad-\nvances in Neural Information Processing Systems, 32:446\u2013\n456, 2019. 2\n[58] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick,\nKaiming He, and Piotr Doll\u00b4ar. Designing network design\nspaces. In Proc. CVPR, June 2020. 1, 6, 8\n[59] Michael E Sander, Pierre Ablin, Mathieu Blondel, and\nGabriel Peyr\u00b4e. Momentum residual neural networks. arXiv\npreprint arXiv:2102.07870, 2021. 2\n[60] Yang Song, Chenlin Meng, and Stefano Ermon. Mintnet:\nBuilding invertible neural networks with masked convolu-\ntions. arXiv preprint arXiv:1907.07945, 2019. 2\n[61] Bingfeng Sun and Jian Zhang. Invertible image compressive\nsensing. In Chinese Conference on Pattern Recognition and\nComputer Vision (PRCV), pages 548\u2013560. Springer, 2021. 2\n[62] Yifan Sun, Nicolas Bohm Agostini, Shi Dong, and David\nKaeli. Summarizing cpu and gpu design trends with product\ndata. arXiv preprint arXiv:1911.11313, 2019. 1\n[63] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herve Jegou. Training\ndata-ef\ufb01cient image transformers & distillation through at-\ntention. In icml, 2021. 2, 6, 9\n[64] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles,\nGabriel Synnaeve, and Herv\u00b4e J\u00b4egou. Going deeper with im-\nage transformers. In Proc. ICCV, 2021. 2\n[65] Du Tran, Heng Wang, Lorenzo Torresani, and Matt Feis-\nzli.\nVideo classi\ufb01cation with channel-separated convolu-\ntional networks. In Proc. ICCV, 2019. 6\n[66] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann\nLeCun, and Manohar Paluri. A closer look at spatiotemporal\nconvolutions for action recognition. In Proc. CVPR, 2018. 6\n[67] Tycho FA van der Ouderaa and Daniel E Worrall. Reversible\ngans for memory-ef\ufb01cient image-to-image translation.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 4720\u20134728, 2019. 2\n[68] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Il-\nlia Polosukhin. Attention is all you need. arXiv preprint\narXiv:1706.03762, 2017. 2\n[69] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\nSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyra-\nmid vision transformer: A versatile backbone for dense pre-\ndiction without convolutions. In Proc. ICCV, 2021. 2, 7\n[70] Samuel Williams, Andrew Waterman, and David Patterson.\nRoo\ufb02ine: an insightful visual performance model for multi-\ncore architectures. Communications of the ACM, 52(4):65\u2013\n76, 2009. 1\n[71] Samuel Webb Williams. Auto-tuning performance on mul-\nticore computers. University of California, Berkeley, 2008.\n1\n[72] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen\nLo, and Ross Girshick. Detectron2. https://github.\ncom/facebookresearch/detectron2, 2019. 9\n[73] Saining Xie, Ross Girshick, Piotr Doll\u00b4ar, Zhuowen Tu, and\nKaiming He. Aggregated residual transformations for deep\nneural networks. In Proc. CVPR, 2017. 7\n[74] Kashu Yamazaki, Vidhiwar Singh Rathour, and T Le. Invert-\nible residual network with regularization for effective med-\nical image segmentation. arXiv preprint arXiv:2103.09042,\n2021. 2\n[75] Jieming Yang, Hongwei Ge, Jinlong Yang, and Yubing\nTong. Image compact-resolution and reconstruction using re-\nversible network. IET Image Processing, 14(16):4376\u20134384,\n2020. 2\n12\n[76] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,\nFrancis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-\nto-token vit: Training vision transformers from scratch on\nimagenet. arXiv preprint arXiv:2101.11986, 2021. 2\n[77] Chen Zhao, Shuming Liu, Karttikeya Mangalam, and\nBernard Ghanem. Re\u02c6 2tal: Rewiring pretrained video back-\nbones for reversible temporal action localization.\narXiv\npreprint arXiv:2211.14053, 2022. 9\n[78] Yuekai Zhao, Shuchang Zhou, and Zhihua Zhang. Multi-\nsplit reversible transformers can enhance neural machine\ntranslation.\nIn Proceedings of the 16th Conference of the\nEuropean Chapter of the Association for Computational Lin-\nguistics: Main Volume, pages 244\u2013254, 2021. 2\n[79] Zaixiang Zheng, Hao Zhou, Shujian Huang, Jiajun Chen,\nJingjing Xu, and Lei Li.\nDuplex sequence-to-sequence\nlearning for reversible machine translation. arXiv preprint\narXiv:2105.03458, 2021. 2\n[80] Tyler Zhu.\nSpeeding up reversible vision transformers.\nhttp://bit.ly/3J6Q0Cb, 2022. 9\n13\n",
        "context": "Reversible Architectures\nare a family of neural network\narchitectures that are based on the NICE [12,13] reversible\ntransformation model which are the precursors of the mod-\nern day generative \ufb02ow based image generation architec-\nworks such as RNNs [51], Unet [4, 16], Masked Convo-\nlutional Networks [60] and 1000-layer deep Graph Neural\nNetworks [40]. Some early attempts have also been made\nto adapt the reversible transformation to the NLP domain,\nFurther, we show how deep reversible networks can achieve\nup to 2-4\u00d7 throughput than their vanilla counterparts.\n2. Related Work\nTransformers\nare a popular network structure that were\n\ufb01rst proposed for natural language applications [68] and"
    },
    {
        "id": 15,
        "title": "Checkmate: Breaking the memory wall with optimal tensor rematerialization",
        "author": [
            "P. Jain",
            "A. Jain",
            "A. Nrusimha",
            "A. Gholami",
            "P. Abbeel",
            "J. E. Gonzalez",
            "I. Stoica",
            "K. Keutzer"
        ],
        "year": "2020",
        "doi": null,
        "in_text_citation": "[15]",
        "sentence": "Examples of this approach include reversible networks [14], which change the model\u2019s structure, and techniques like Checkmate [15], which alter the model\u2019s execution pattern by adding additional operations to backpropagation to decrease the number of intermediate values that need to be stored in memory.",
        "abstract": "We formalize the problem of trading-off DNN training time and memory\nrequirements as the tensor rematerialization optimization problem, a\ngeneralization of prior checkpointing strategies. We introduce Checkmate, a\nsystem that solves for optimal rematerialization schedules in reasonable times\n(under an hour) using off-the-shelf MILP solvers or near-optimal schedules with\nan approximation algorithm, then uses these schedules to accelerate millions of\ntraining iterations. Our method scales to complex, realistic architectures and\nis hardware-aware through the use of accelerator-specific, profile-based cost\nmodels. In addition to reducing training cost, Checkmate enables real-world\nnetworks to be trained with up to 5.1x larger input sizes. Checkmate is an\nopen-source project, available at https://github.com/parasj/checkmate.",
        "full_text": "CHECKMATE: BREAKING THE MEMORY WALL\nWITH OPTIMAL TENSOR REMATERIALIZATION\nParas Jain * 1 Ajay Jain * 1 Aniruddha Nrusimha 1\nAmir Gholami 1 Pieter Abbeel 1 Kurt Keutzer 1 Ion Stoica 1 Joseph E. Gonzalez 1\nABSTRACT\nWe formalize the problem of trading-off DNN training time and memory requirements as the tensor remateri-\nalization optimization problem, a generalization of prior checkpointing strategies. We introduce Checkmate, a\nsystem that solves for optimal rematerialization schedules in reasonable times (under an hour) using off-the-shelf\nMILP solvers or near-optimal schedules with an approximation algorithm, then uses these schedules to accelerate\nmillions of training iterations. Our method scales to complex, realistic architectures and is hardware-aware\nthrough the use of accelerator-speci\ufb01c, pro\ufb01le-based cost models. In addition to reducing training cost, Checkmate\nenables real-world networks to be trained with up to 5.1\u00d7 larger input sizes. Checkmate is an open-source project,\navailable at https://github.com/parasj/checkmate.\n1\nINTRODUCTION\nDeep learning training workloads demand large amounts\nof high bandwidth memory. Researchers are pushing the\nmemory capacity limits of hardware accelerators such as\nGPUs by training neural networks on high-resolution im-\nages (Dong et al., 2016; Kim et al., 2016; Tai et al., 2017),\n3D point-clouds (Chen et al., 2017; Yang et al., 2018), and\nlong natural language sequences (Vaswani et al., 2017; De-\nvlin et al., 2018; Child et al., 2019). In these applications,\ntraining memory usage is dominated by the intermediate\nactivation tensors needed for backpropagation (Figure 3).\nThe limited availability of high bandwidth on-device mem-\nory creates a memory wall that sti\ufb02es exploration of novel\narchitectures. Across applications, authors of state-of-the-\nart models cite memory as a limiting factor in deep neural\nnetwork (DNN) design (Krizhevsky et al., 2012; He et al.,\n2016; Chen et al., 2016a; Gomez et al., 2017; Pohlen et al.,\n2017; Child et al., 2019; Liu et al., 2019; Dai et al., 2019).\nAs there is insuf\ufb01cient RAM to cache all activation tensors\nfor backpropagation, some select tensors can be discarded\nduring forward evaluation. When a discarded tensor is nec-\nessary as a dependency for gradient calculation, the tensor\ncan be rematerialized. As illustrated in Figure 1, rematerial-\nizing values allows a large DNN to \ufb01t within memory at the\nexpense of additional computation.\n*Equal contribution 1Department of EECS, UC Berkeley.\nCorrespondence to: Paras Jain <parasj@berkeley.edu>.\nProceedings of the 3 rd Conference on Machine Learning and\nSystems, Austin, TX, USA, 2020. Copyright 2020 by the author(s).\nTime\n0\n10\n20\n30\nRAM used (GB)\nRetain all\nactivations\nRematerialize\nactivations\nFigure 1. This 32-layer deep neural network requires 30GB of\nmemory during training in order to cache forward pass activations\nfor the backward pass. Freeing certain activations early and rema-\nterializing them later reduces memory requirements by 21GB at\nthe cost of a modest runtime increase. Rematerialized layers are\ndenoted as shaded blue regions. We present Checkmate, a system\nto rematerialize large neural networks optimally. Checkmate is\nhardware-aware, memory-aware and supports arbitrary DAGs.\nGriewank & Walther (2000) and Chen et al. (2016b) present\nheuristics for rematerialization when the forward pass forms\na linear graph, or path graph. They refer to the problem as\ncheckpointing. However, their approaches cannot be applied\ngenerally to nonlinear DNN structures such as residual con-\nnections, and rely on the strong assumption that all nodes in\nthe graph have the same cost. Prior work also assumes that\ngradients may never be rematerialized. These assumptions\nlimit the ef\ufb01ciency and generality of prior approaches.\nOur work formalizes tensor rematerialization as a con-\nstrained optimization problem. Using off-the-shelf numeri-\ncal solvers, we are able to discover optimal rematerializa-\narXiv:1910.02653v3  [cs.LG]  14 May 2020\nCheckmate: Breaking the Memory Wall with Optimal Tensor Rematerialization\nLP construction\nand optimization\n(minutes)\nRebuild\nstatic graph with\nrematerialization\nStatic reverse \nmode auto-\ndifferentiation\nUser specified \narchitecture\nTraining loop\n(days)\nHardware\ncost model\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\ng ,\nFREEt,i,k = \u00ac\n0\nB\nB\n@\u00acRt,k _ St+1,i\n_\nj2USERS[i]\nj>k\nRt,j\n1\nC\nC\nA\n=\n0\n@1 \u2212Rt,k + St+1,i +\nX\nj2USERS[i],j>k\nRt,j = 0\n1\nA\n, (num_hazards(t, i, k) = 0)\n(6)\nwhere num_hazards(t, i, k) is introduced simply for nota-\ntional convenience. Relation (6) is implemented with linear\ncast-to-boolean constraints, where \uf8ffis the maximum value\nnum_hazards(t, i, k) can assume,\nFREEt,i,k 2 {0, 1}\n(7a)\n1 \u2212FREEt,i,k \uf8ffnum_hazards(t, i, k)\n(7b)\n\uf8ff(1 \u2212FREEt,i,k) \u2265num_hazards(t, i, k)\n(7c)\nThe complete memory constrained ILP follows in (8), with\nO(|V ||E|) variables and constraints.\narg min\nR, S, U, FREE\nn\nX\nt=1\nt\nX\ni=1\nCiRt,i\n(1a)\nsubject to\n(1b), (1c), (1d), (1e),\n(2), (3), (7a), (7b), (7c),\nUt,k \uf8ffMbudget\n(8)\n4.5\nConstraints implied by optimality\nProblem 8 can be simpli\ufb01ed by removing constraints im-\nplied by optimality of a solution. In (2), all values with\nSt,i = 1 are allocated space, even if they are unused. If\nsuch a value is unused, the checkpoint is spurious and the\nsolver can set St,i = 0 to reduce memory usage if needed.\nFurther, FREEt,k,k = 1 only if operation k is spuriously\nevaluated with no uses of the result. Hence, the solver can\nset Rt,k = 0 to reduce cost. When solving the MILP, we\neliminate |V |2 variables FREEt,k,k, assumed to be 0, by\nonly summing over i 2 DEPS[k] in (4). Note that the elim-\ninated variables can be computed inexpensively from R and\nS after solving.\n4.6\nGenerating an execution plan\nGiven a feasible solution to (8), (R, S, FREE), we generate\na concrete execution plan that evaluates the computation\nfor k\n1 to |V | do\nif Rt,k then\n// Materialize vk\nemit %r = allocate vk\nemit compute vk, %r\nREGS[k] = r\nr = r + 1\nend if\n// Free vk and dependencies\nfor i 2 DEPS[k] [ {k} do\nif FREEt,i,k then\nemit deallocate %REGS[i]\nend if\nend for\nend for\nend for\ngraph with bounded memory usage. This execution plan,\nor schedule, is constructed via a row major scan of the so-\nlution matrices, detailed in Algorithm 1.\nA\nconcrete\nexecution\nplan\nis\na\nprogram\nconsist-\ning\nof\nk\nstatements\nP\n=\n(s1, . . . , sk),\nwhere\nsi 2 {allocate, compute, deallocate}.\nState-\nment %r = allocate v de\ufb01nes a virtual register for\nthe result of the operation corresponding to v, used to\ntrack memory usage during execution.\nSuch a register\nmust be allocated for v before an instance of statement\ncompute v, %r in the plan, which invokes the opera-\ntion and generates an output value which is tracked by the\nregister %r. Finally, statement deallocate %r deletes\nthe virtual register, marks the output value for garbage col-\nlection, and updates the tracked memory usage.\nThe execution plan generated by Algorithm 1 is further op-\ntimized by moving deallocations earlier in the plan if possi-\nble. For example, spurious checkpoints that are unused in a\nstage can be deallocated at the start of the stage rather than\nduring the stage. Note that this code motion is unnecessary\nas the solver guarantees that the unoptimized schedule will\nnot exceed the desired memory budget.\n4.7\nGenerating static computation graph\nFor implementation, the concrete execution plan can either\nbe interpreted, or encoded as a static computation graph.\nIn this work, we generate a static graph G0 = (V 0, E0)\nfrom the plan, which is executed by a numerical machine\nlearning framework. See Section 6.2 for implementation\nFigure 2. Overview of the Checkmate system.\ntion strategies for arbitrary deep neural networks in Ten-\nsorFlow with non-uniform computation and memory costs.\nWe demonstrate that optimal rematerialization allows larger\nbatch sizes and substantially reduced memory usage with\nminimal computational overhead across a range of image\nclassi\ufb01cation and semantic segmentation architectures. As a\nconsequence, our approach allows researchers to easily ex-\nplore larger models, at larger batch sizes, on more complex\nsignals with minimal computation overhead.\nIn particular, the contributions of this work include:\n\u2022 a formalization of the rematerialization problem as a\nmixed integer linear program with a substantially more\n\ufb02exible search space than prior work, in Section 4.7.\n\u2022 a fast approximation algorithm based on two-phase\ndeterministic LP rounding, in Section 5.\n\u2022 Checkmate, a system implemented in TensorFlow that\nenables training models with up to 5.1\u00d7 larger input\nsizes than prior art at minimal overhead.\n2\nMOTIVATION\nWhile inference optimizations are well studied (Jain et al.,\n2019), training workloads have received less attention.\nMemory consumption during training consists of (a) in-\ntermediate features, or activations, whose size depends on\ninput dimensions and (b) parameters and their gradients\nwhose size depends on weight dimensions. Given that in-\nputs are often several order of magnitude larger than kernels,\nmost memory is used by features, demonstrated in Figure 3.\nFrameworks such as TensorFlow (Abadi et al., 2016) and\nPyTorch (Paszke et al., 2017; 2019) store all activations\nduring the forward pass. Gradients are backpropagated from\nthe loss node, and each activation is freed after its gradient\nhas been calculated. In Figure 1, we compare this memory\nintensive policy and a rematerialization strategy for a real\nneural network. Memory usage is signi\ufb01cantly reduced\nby deallocating some activations in the forward pass and\nrecomputing them in the backward pass. Our goal is \ufb01t an\nAlexNet, 2012\nVGG19, 2014\nInception v3, 2015\nResNet-152, 2015\nDenseNet-201, 2016\nResNeXt-101, 2016\nFCN8s, 2017\nTransformer, 2017\nRoBERTa, 2018\nBigGAN, 2018\n0GB\n5GB\n10GB\n15GB\nFeatures\nWorkspace memory\nParameter gradients\nParameters\nTotal memory consumed\nGPU memory limit\nFigure 3. Memory consumed by activations far outweigh parame-\nters for popular model architectures. Moreover, advances in GPU\nDRAM capacity are quickly utilized by researchers; the dashed\nline notes the memory limit of the GPU used to train each model.\narbitrary network within our memory budget while incurring\nthe minimal additional runtime penalty from recomputation.\nMost prior work assumes networks have linear graphs. For\nexample, Chen et al. (2016b) divides the computation into\n\u221an segments, each with \u221an nodes. Each segment endpoint\nis stored during the forward pass. During the backward pass,\nsegments are recomputed in reverse order at O(n) cost.\nLinear graph assumptions limit applicability of prior work.\nFor example, while the popular ResNet50 (He et al., 2016)\ncan be linearized by treating each residual block as a single\nnode, this leads to inef\ufb01cient solutions. For networks with\nlonger skip connections, e.g., U-Net (Ronneberger et al.,\n2015), grouping nodes oversimpli\ufb01es the graph.\nPrior work also assumes all layers are equally expensive\nto recompute. In the VGG19 architecture (Simonyan &\nZisserman, 2014), the largest layer is six orders of magnitude\nmore expensive than the smallest layer.\nOur work makes few assumptions on neural network graphs.\nWe explore a solution space that allows for (a) arbitrary\ngraphs with several inputs and outputs for each node, (b)\nvariable memory costs across layers and (c) variable com-\nputation costs for each layer (such as FLOPs or pro\ufb01led\nruntimes). We constrain solutions to simply be correct (a\nnode\u2019s dependencies must be materialized before it can be\nevaluated) and within the RAM budget (at any point during\nexecution, resident tensors must \ufb01t into RAM).\nCheckmate: Breaking the Memory Wall with Optimal Tensor Rematerialization\nSubject to these constraints, we \ufb01nd solutions that minimize\nthe amount of time it takes to perform a single training\niteration. We project schedules into space and time, allowing\nus to cast the objective as a linear expression. This problem\ncan then be solved using off-the-shelf mixed integer linear\nprogram solvers such as GLPK or COIN-OR Branch-and-\nCut (Forrest et al., 2019). An optimal solution to the MILP\nwill minimize the amount of additional compute cost within\nthe memory budget.\n3\nRELATED WORK\nWe categorize related work as checkpointing, reversible net-\nworks, distributed computation, and activation compression.\nCheckpointing and rematerialization Chen et al. (2016b)\npropose a heuristic for checkpointing idealized unit-cost lin-\near n-layer graphs with O(\u221an) memory usage. Griewank\n& Walther (2000) checkpoint similar linear unit-cost graphs\nwith O(log n) memory usage and prove optimality for lin-\near graphs with unit per-node cost and memory. In practice,\nDNN layers vary signi\ufb01cantly in memory usage and com-\nputational cost (Sze et al., 2017), so these heuristics are\nnot optimal in practice. Chen et al. (2016b) also develop\na greedy algorithm that checkpoints layers of a network in\nroughly memory equal segments, with a hyperparameter b\nfor the size of such segments. Still, neither procedure is cost-\naware nor deallocates checkpoints when possible. Gruslys\net al. (2016) develop a dynamic programming algorithm for\ncheckpoint selection in unrolled recurrent neural network\ntraining, exploiting their linear forward graphs. Feng &\nHuang (2018) provide a dynamic program to select check-\npoints that partition branching networks, but ignore layer\ncosts and memory usage. Siskind & Pearlmutter (2018a) de-\nvelop a divide-and-conquer strategy in programs. Beaumont\net al. (2019) use dynamic programming for checkpoint se-\nlection in a speci\ufb01c architecture with joining sub-networks.\nIntermediate value recomputation is also common in reg-\nister allocation. Compiler backends lower an intermediate\nrepresentation of code to an architecture-speci\ufb01c executable\nbinary. During lowering, an abstract static single assign-\nment (SSA) graph of values and operations (Rosen et al.,\n1988; Cytron et al., 1991) is concretized by mapping values\nto a \ufb01nite number of registers. If insuf\ufb01cient registers are\navailable for an SSA form computation graph, values are\nspilled to main memory by storing and later loading the\nvalue. Register allocation has been formulated as graph\ncoloring problem (Chaitin et al., 1981), integer program\n(Goodwin & Wilken, 1996; Lozano et al., 2018), and net-\nwork \ufb02ow (Koes & Goldstein, 2006).\nRegister allocators may recompute constants and values\nwith register-resident dependencies if the cost of doing so\nis less than the cost of a spill (Chaitin et al., 1981; Briggs\net al., 1992; Punjani, 2004). While similar to our setup,\nregister rematerialization is limited to exceptional values\nthat can be recomputed in a single instruction with depen-\ndencies already in registers. For example, memory offset\ncomputations can be cheaply recomputed, and loads of con-\nstants can be statically resolved. In contrast, Checkmate can\nrecompute entire subgraphs of the program\u2019s data-\ufb02ow.\nDuring the evaluation of a single kernel, GPUs spill per-\nthread registers to a thread-local region of global memory\n(i.e. local memory) (Micikevicius, 2011; NVIDIA, 2017).\nNN training executes DAGs of kernels and stores intermedi-\nate values in shared global memory. This produces a high\nrange of value sizes, from 4 byte \ufb02oats to gigabyte tensors,\nwhereas CPU and GPU registers range from 1 to 64 bytes.\nOur problem of interkernel memory scheduling thus differs\nin scale from the classical problem of register allocation\nwithin a kernel or program. Rematerialization is more ap-\npropriate than copying values out of core as the cost of\nspilling values from global GPU memory to main memory\n(RAM) is substantial (Micikevicius, 2011; Jain et al., 2018),\nthough possible (Meng et al., 2017).\nReversible Networks Gomez et al. (2017) propose a re-\nversible (approximately invertible) residual DNN architec-\nture, where intermediate temporary values can be recom-\nputed from values derived later in the standard forward\ncomputation. Reversibility enables recomputation during\nthe backward pass. Bulo et al. (2018) replace only ReLU\nand batch normalization layers with invertible variants and\nreduce memory usage up to 50%. We \ufb01nd rematerialization\nenables greater savings and a wider range of budgets, but\nreversibility is a promising complementary approach.\nDistributed computation Orthogonal approaches to ad-\ndress the limited memory problem are distributed-memory\ncomputations and gradient accumulation. However, model\nparallelism requires access to additional expensive com-\npute accelerators, fast networks, and non-trivial partition-\ning of model state to balance communication and compu-\ntation (Gholami et al., 2018; Jia et al., 2018b; McCandlish\net al.). Gradient accumulation enables larger batch sizes by\ncomputing the gradients with multiple sub-batches across a\nmini-batch. However, gradient accumulation can degrade\nperformance as batch normalization performs poorly on\nsmall batch sizes (Wu & He, 2018; Ioffe & Szegedy, 2015).\nActivation compression In some DNN applications, it is\npossible to process compressed representations with mini-\nmal accuracy loss. Gueguen et al. (2018) classify discrete\ncosine transforms of JPEG images rather than raw images.\nJain et al. (2018) quantize activations, cutting memory usage\nin half. Compression reduces memory usage by a constant\nfactor, but reduces accuracy. Our approach is mathemati-\ncally equivalent to rematerialization-free training and incurs\nno accuracy penalty.\nCheckmate: Breaking the Memory Wall with Optimal Tensor Rematerialization\nMETHOD\nDESCRIPTION\nGENERAL\nGRAPHS\nCOST\nAWARE\nMEMORY\nAWARE\nCheckpoint all (Ideal)\nNo rematerialization. Default in deep learning frameworks.\n\u221a\n\u00d7\n\u00d7\nGriewank et al. log n\nGriewank & Walther (2000) REVOLVE procedure\n\u00d7\n\u00d7\n\u00d7\nChen et al. \u221an\nChen et al. (2016b) checkpointing heuristic\n\u00d7\n\u00d7\n\u00d7\nChen et al. greedy\nChen et al. (2016b), with search over parameter b\n\u00d7\n\u00d7\n\u223c\nAP \u221an\nChen et al. \u221an on articulation points + optimal R solve\n\u223c\n\u00d7\n\u00d7\nAP greedy\nChen et al. greedy on articulation points + optimal R solve\n\u223c\n\u00d7\n\u223c\nLinearized \u221an\nChen et al. \u221an on topological sort + optimal R solve\n\u221a\n\u00d7\n\u00d7\nLinearized greedy\nChen et al. greedy on topological sort + optimal R solve\n\u221a\n\u00d7\n\u223c\nCheckmate ILP\nOur ILP as formulated in Section 4\n\u221a\n\u221a\n\u221a\nCheckmate approx.\nOur LP rounding approximation algorithm (Section 5)\n\u221a\n\u221a\n\u221a\nTable 1. Rematerialization baselines and our extensions to make them applicable to non-linear architectures\n4\nOPTIMAL REMATERIALIZATION\nIn this section, we develop an optimal solver that schedules\ncomputation and garbage collection during the evaluation\nof general data-\ufb02ow graphs including those used in neu-\nral network training. Our proposed scheduler minimizes\ncomputation or execution time while guaranteeing that the\nschedule will not exceed device memory limitations. The\nrematerialization problem is formulated as a mixed integer\nlinear program (MILP) that can be solved with standard\ncommercial or open-source solvers.\n4.1\nProblem de\ufb01nition\nA computation or data-\ufb02ow graph G = (V, E) is a directed\nacyclic graph with n nodes V = {v1, . . . , vn} that represent\noperations yielding values (e.g. tensors). Edges represent\ndependencies between operators, such as layer inputs in a\nneural network. Nodes are numbered according to a topo-\nlogical order, such that operation vj may only depend on\nthe results of operations vi<j.\nEach operator\u2019s output takes Mv memory to store and costs\nCv to compute from its inputs. We wish to \ufb01nd the terminal\nnode vn with peak memory consumption under a memory\nbudget, Mbudget, and minimum total cost of computation.\n4.2\nRepresenting a schedule\nWe represent a schedule as a series of nodes being saved\nor (re)computed. We unroll the execution of the network\ninto T stages and only allow a node to be computed once\nper stage. St,i \u2208{0, 1} indicates that the result of operation\ni should be retained in memory at stage t \u22121 until stage t.\nWe also de\ufb01ne Rt,i \u2208{0, 1} be a binary variable re\ufb02ecting\nwhether operation i is recomputed at time step t.\nOur representation generalizes checkpointing (Griewank\n& Walther, 2000; Chen et al., 2016b; Gruslys et al., 2016;\nSiskind & Pearlmutter, 2018b; Feng & Huang, 2018), as\nvalues can be retained and deallocated many times, but\ncomes at the cost of O(Tn) decision variables.\nTo trade-off the number of decision variables and schedule\n\ufb02exibility, we limit T to T = n. This allows for O(n2)\noperations and constant memory in linear graphs.\n4.3\nScheduling with ample memory\nFirst, consider neural network evaluation on a processor\nwith ample memory. Even without a memory constraint,\nour solver must ensure that checkpointed and computed op-\nerations have dependencies resident in memory. Minimizing\nthe total cost of computation across stages with dependency\nconstraints yields objective (1a):\narg min\nR, S\nn\nX\nt=1\nt\nX\ni=1\nCiRt,i\n(1a)\nsubject to\nRt,j \u2264Rt,i + St,i\n\u2200t \u2200(vi, vj) \u2208E, (1b)\nSt,i \u2264Rt\u22121,i + St\u22121,i\n\u2200t \u22652 \u2200i,\n(1c)\nP\ni S1,i = 0,\n(1d)\nP\nt Rt,n \u22651,\n(1e)\nRt,i, St,i \u2208{0, 1}\n\u2200t \u2200i\n(1f)\nConstraints ensure feasibility and completion. Constraint\n(1b) and (1c) ensure that an operation is computed in stage t\nonly if all dependencies are available. To cover the edge case\nof the \ufb01rst stage, constraint (1d) speci\ufb01es that no values are\ninitially in memory. Finally, covering constraint (1e) ensures\nthat the last node in the topological order is computed at\nsome point in the schedule so that training progresses.\n4.4\nConstraining memory utilization\nTo constrain memory usage, we introduce memory account-\ning variables Ut,k \u2208R+ into the ILP. Let Ut,k denote the\nmemory used just after computing node vk in stage t. Ut,k\nis de\ufb01ned recursively in terms of auxiliary binary variables\nFREEt,i,k for (vi, vk) \u2208E, which speci\ufb01es whether node\nCheckmate: Breaking the Memory Wall with Optimal Tensor Rematerialization\n{\nva\nvb\n\u2026\nvc\n\u2026\nDeps[vk]\nvk\nGC\nDeps[vk]\nUt,k\nUt,k+1\n\u2026\nB\nvk+1\ncompute\ncompute\nFigure 4. Dependencies of vk can only be garbage collected after it is evaluated. Ut,k measures the memory used after evaluating vk and\nbefore deallocating its dependencies. vb and vc may be deallocated during garbage collection, but va may not due to a forward edge.\nvi may be deallocated in stage t after evaluating node vk.\nWe assume that (1) network inputs and parameters are al-\nways resident in memory and (2) enough space is allocated\nfor gradients of the loss with respect to parameters.1 Pa-\nrameter gradients are typically small, the same size as the\nparameters themselves. Additionally, at the beginning of\na stage, all checkpointed values are resident in memory.\nHence, we initialize the recurrence,\nUt,0 = Minput + 2Mparam\n|\n{z\n}\nConstant overhead\n+\nn\nX\ni=1\nMiSt,i\n| {z }\nCheckpoints\n(2)\nSuppose Ut,k bytes of memory are in use after evaluating\nvk. Before evaluating vk+1, vk and dependencies (parents)\nof vk may be deallocated if there are no future uses. Then,\nan output tensor for the result of vk+1 is allocated, consum-\ning memory Mk+1. The timeline is depicted in Figure 4,\nyielding recurrence (3):\nUt,k+1 = Ut,k \u2212mem freedt(vk) + Rt,k+1Mk+1,\n(3)\nwhere mem freedt(vk) is the amount of memory freed by\ndeallocating vk and its parents at stage t. Let\nDEPS[k] = {i : (vi, vk) \u2208E}, and\nUSERS[i] = {j : (vi, vj) \u2208E}\ndenote parents and children of a node, respectively. Then,\nin terms of auxiliary variable FREEt,i,k, for (vi, vk) \u2208E,\nmem freedt(vk) = P\ni\u2208DEPS[k]\n\u222a{k}\nMi \u2217FREEt,i,k, and\n(4)\nFREEt,i,k = Rt,k \u2217(1 \u2212St+1,i)\n|\n{z\n}\nNot checkpoint\nY\nj\u2208USERS[i]\nj>k\n(1 \u2212Rt,j)\n|\n{z\n}\nNot dep.\n(5)\nThe second factor in (5) ensures that Mi bytes are freed only\nif vi is not checkpointed for the next stage. The \ufb01nal factors\nensure that FREEt,i,k = 0 if any child of vi is computed in\n1While gradients can be deleted after updating parameters, we\nreserve constant space since many parameter optimizers such as\nSGD with momentum maintain gradient statistics.\nthe stage, since then vi needs to be retained for later use.\nMultiplying by Rt,k in (5) ensures that values are only freed\nat most once per stage according to Theorem 4.1,\nTheorem 4.1 (No double deallocation). If (5) holds for all\n(vi, vk) \u2208E, then P\nk\u2208USERS[i] FREEt,i,k \u22641 \u2200t, i.\nProof. Assume for the sake of contradiction that \u2203k1, k2 \u2208\nUSERS[i] such that FREEt,i,k1 = FREEt,i,k2 = 1. By the\n\ufb01rst factor in (5), we must have Rt,k1 = Rt,k2 = 1. Assume\nwithout loss of generality that k2 > k1. By the \ufb01nal factor\nin (5), we have FREEt,i,k1 \u22641 \u2212Rt,k2 = 0, which is a\ncontradiction.\n4.5\nLinear reformulation of memory constraint\nWhile the recurrence (2-3) de\ufb01ning U is linear, the right\nhand size of (5) is a polynomial. To express FREE in our\nILP, it must be de\ufb01ned via linear constraints. We rely on\nLemma 4.1 and 4.2 to reformulate (5) into a tractable form.\nLemma 4.1 (Linear Reformulation of Binary Polynomial).\nIf x1, . . . , xn \u2208{0, 1}, then\nn\nY\ni=1\nxi =\n(\n1\nPn\ni=1(1 \u2212xi) = 0\n0\notherwise\nProof. If all x1, . . . , xn = 1, then Pn\ni=1(1 \u2212xi) = 0 and\nwe have \u03a0n\ni=1xi = 1. If otherwise any xj = 0, then we\nhave \u03a0n\ni=1xi = 0, as desired. This can also be seen as an\napplication of De Morgan\u2019s laws for boolean arithmetic.\nLemma 4.2 (Linear Reformulation of Indicator Constraints).\nGiven 0 \u2264y \u2264\u03ba where y is integral and \u03ba is a constant\nupper bound on y, then\nx =\n(\n1\ny = 0\n0\notherwise\nif and only if x \u2208{0, 1} and (1 \u2212x) \u2264y \u2264\u03ba(1 \u2212x).\nProof. For the forward direction, \ufb01rst note that by con-\nstruction, x \u2208{0, 1}.\nIf y = 0 and x = 1, then\nCheckmate: Breaking the Memory Wall with Optimal Tensor Rematerialization\n(1 \u2212x) = 0 \u2264y \u22640 = \u03ba(1 \u2212x). Similarly, if y \u22651\nand x = 0, then 1 \u2264y \u2264\u03ba, which is true since 0 \u2264y \u2264\u03ba\nand y is integral. The converse holds similarly.\nTo reformulate Constraint 5, let num hazards(t, i, k) be the\nnumber of zero factors on the RHS of the constraint. This is\na linear function of the decision variables,\nnum hazards(t, i, k) = (1\u2212Rt,k)+St+1,i+\nX\nj\u2208USERS[i]\nj>k\nRt,j\nApplying Lemma 4.1 to the polynomial constraint, we have,\nFREEt,i,k =\n(\n1\nnum hazards(t, i, k) = 0\n0\notherwise\n(6)\nBy Lemma 4.2,\nif \u03ba is the maximum value that\nnum hazards(t, i, k) can assume, the following constraints\nare equivalent to (6),\nFREEt,i,k \u2208{0, 1}\n(7a)\n1 \u2212FREEt,i,k \u2264num hazards(t, i, k)\n(7b)\n\u03ba(1 \u2212FREEt,i,k) \u2265num hazards(t, i, k)\n(7c)\n4.6\nTractability via frontier-advancing stages\nFixing the execution order of nodes in the graph can im-\nprove the running time of the algorithm. In eager-execution\nframeworks such as PyTorch, the order is given by user\ncode and operations are executed serially. Separating order-\ning and allocation is common in compiler design, and both\nLLVM (Lattner, 2002) and GCC (Olesen, 2011) have sepa-\nrate instruction scheduling and register allocation passes.\nAny topological order of the nodes is a possible execution\norder. Given a topological order, such as the one intro-\nduced in Section 4.1, we partition the schedule into frontier-\nadvancing stages such that node vi is evaluated for the \ufb01rst\ntime in stage i. We replace constraints (1d, 1e) that ensure\nthe last node is computed with stricter constraints (8a-8c),\nRi,i = 1 \u2200i (frontier-advancing partitions)\n(8a)\nP\ni\u2265t St,i = 0 (lower tri., no initial checkpoints)\n(8b)\nP\ni>t Rt,i = 0 (lower triangular)\n(8c)\nThis reduces the feasible set, constraining the search space\nand improving running time. For an 8 layer (n = 17)\nlinear graph neural network with unit Ci, Mi at a memory\nbudget of 4, Gurobi optimizes the unpartitioned MILP in\n9.4 hours and the partitioned MILP in 0.23 seconds to the\nsame objective. In Appendix A, we analyze the integrality\ngap of both forms of the problem to understand the speedup.\n4.7\nComplete Integer Linear Program formulation\nThe complete memory constrained MILP follows in (9),\nwith O(|V ||E|) variables and constraints.\narg min\nR, S, U, FREE\nn\nX\nt=1\nt\nX\ni=1\nCiRt,i\nsubject to\n(1b), (1c), (1f), (2), (3),\n(7a), (7b), (7c), (8a), (8b), (8c),\nUt,k \u2264Mbudget\n(9)\n4.8\nConstraints implied by optimality\nProblem 9 can be simpli\ufb01ed by removing constraints im-\nplied by optimality of a solution. FREEt,k,k = 1 only if\noperation k is spuriously evaluated with no uses of the re-\nsult. Hence, the solver can set Rt,k = 0 to reduce cost.\nWe eliminate |V |2 variables FREEt,k,k, assumed to be 0,\nby modifying (4) to only sum over i \u2208DEPS[k]. These\nvariables can be computed inexpensively after solving.\n4.9\nGenerating an execution plan\nGiven a feasible solution to (9), (R, S, U, FREE), Algo-\nrithm 1 generates an execution plan via a row major scan\nof R and S with deallocations determined by FREE. An\nexecution plan is a program P = (s1, . . . , sk) with k state-\nments. When statement %r = compute v is interpreted,\noperation v is evaluated. The symbol %r denotes a vir-\ntual register used to track the resulting value. Statement\ndeallocate %r marks the value tracked by virtual reg-\nister %r for garbage collection.\nThe execution plan generated by Algorithm 1 is further\noptimized by moving deallocations earlier in the plan when\npossible. Spurious checkpoints that are unused in a stage can\nbe deallocated at the start of the stage rather than during the\nstage. Still, this code motion is unnecessary for feasibility\nas the solver guarantees that the unoptimized schedule will\nnot exceed the desired memory budget.\nThe execution plan can either be interpreted during training,\nor encoded as a static computation graph. In this work, we\ngenerate a static graph G\u2032 = (V \u2032, E\u2032) from the plan, which\nis executed by a numerical machine learning framework.\nSee Section 6.2 for implementation details.\n4.10\nCost model\nTo estimate the runtime of a training iteration under a re-\nmaterialization plan, we apply an additive cost model (1a),\nincurring cost Ci when node vi is evaluated. Costs are de-\ntermined prior to MILP construction by pro\ufb01ling network\nlayers on target hardware with random inputs across a range\nof batch sizes and input shapes, and exclude static graph\nconstruction and input generation time. As neural network\nCheckmate: Breaking the Memory Wall with Optimal Tensor Rematerialization\nAlgorithm 1 Generate execution plan\nInput: graph G = (V, E), feasible (R, S, FREE)\nOutput: execution plan P = (s1, . . . , sk)\nInitialize REGS[1 . . . |V |] = \u22121, r = 0, P = ().\nfor t = 1 to |V | do\nfor k = 1 to |V | do\nif Rt,k then\n// Materialize vk\nadd %r = compute vk to P\nREGS[k] = r\nr = r + 1\nend if\n// Free vk and dependencies\nfor i \u2208DEPS[k] \u222a{k} do\nif FREEt,i,k then\nadd deallocate %REGS[i] to P\nend if\nend for\nend for\nend for\nreturn P\noperations consist of dense numerical kernels such as matrix\nmultiplication, these runtimes are low variance and largely\nindependent of the speci\ufb01c input data (Jia et al., 2018a; Si-\nvathanu et al., 2019). However, forward pass time per batch\nitem decreases with increasing batch size due to improved\ndata parallelism (Canziani et al., 2016), so it is important to\ncompute costs with appropriate input dimensions.\nThe memory consumption of each value in the data-\ufb02ow\ngraph is computed statically as input and output sizes are\nknown. Values are dense, multi-dimensional tensors stored\nat 4 byte \ufb02oating point precision. The computed consump-\ntion Mi is used to construct memory constraints (2-3).\n5\nAPPROXIMATION\nMany of our benchmark problem instances are tractable to\nsolve using off-the-shelf integer linear program solvers, with\npractical solve times ranging from seconds to an hour. ILP\nresults in this paper are obtained with a 1 hour time limit\non a computer with at least 24 cores. Relative to training\ntime, e.g. 21 days for the BERT model (Devlin et al., 2018),\nsolving the ILP adds less than a percent of runtime overhead.\nWhile COTS solvers such as COIN-OR (Forrest et al., 2019)\nleverage methods like branch-and-bound to aggressively\nprune the decision space, they can take superpolynomial\ntime in the worst-case and solving ILPs is NP-hard in gen-\neral. In the worst-case, for neural network architectures with\nhundreds of layers, it is not feasible to solve the remateri-\nalization problem via our ILP. An instance of the VGG16\narchitecture (Simonyan & Zisserman, 2014) takes seconds\nto solve. For DenseNet161 (Huang et al., 2017), no feasible\nsolution was found within one day.\nFor many classical NP-hard problems, approximation al-\ngorithms give solutions close to optimal with polynomial\nruntime. We review a linear program that produces frac-\ntional solutions in polynomial time in Section 5.1. Using\nthe fractional solutions, we present a two-phase rounding\nalgorithm in Section 5.2 that rounds a subset of the decision\nvariables, then \ufb01nds a minimum cost, feasible setting of the\nremaining variables to \ufb01nd near-optimal integral solutions.\n5.1\nRelaxing integrality constraints\nBy relaxing integrality constraints (1f), the problem be-\ncomes trivial to solve as it is a linear program over continu-\nous variables. It is well known that an LP is solvable in poly-\nnomial time via Karmarkar\u2019s algorithm (Karmarkar, 1984)\nor barrier methods (Nesterov & Nemirovskii, 1994). With\nrelaxation R, S, FREE \u2208[0, 1], the objective (1a) de\ufb01nes a\nlower-bound for the cost of the optimal integral solution.\nRounding is a common approach to \ufb01nd approximate inte-\ngral solutions given the result of an LP relaxation. For exam-\nple, one can achieve a 3\n4-approximation for MAX SAT (Yan-\nnakakis, 1994) via a simple combination of randomized\nrounding (Pr\n\u0002\nxint\ni = 1\n\u0003\n= x\u2217\ni ) and deterministic rounding\n(xint\ni = 1 if x\u2217\ni \u2265p, where commonly p = 0.5).\nWe attempt to round the fractional solution R\u2217, S\u2217using\nthese two strategies, and then apply Algorithm 1 to Rint, Sint.\nHowever, direct application of deterministic rounding re-\nturns infeasible results: the rounded solution violates con-\nstraints. Randomized rounding may show more promise as a\nsingle relaxed solution can be used to sample many integral\nsolutions, some of which are hopefully feasible. Unfortu-\nnately, using randomized rounding with the LP relaxation\nfor VGG16 at a 4\u00d7 smaller budget than default, we could\nnot \ufb01nd a single feasible solution out of 50,000 samples.\n5.2\nA two-phase rounding strategy\nTo \ufb01nd feasible solutions, we introduce two-phase rounding,\ndetailed in Algorithm 2. Two-phase rounding is applicable\nwhen a subset of variables can be solved in polynomial time\ngiven the remaining variables. Our approximation algorithm\nonly rounds the checkpoint matrix S\u2217. Given S\u2217, we solve\nfor the conditionally optimal binary computation matrix Rint\nby setting as few values to 1 as possible. Algorithm 2 begins\nwith an all-zero matrix Rint = 0, then iteratively corrects\nviolated correctness constraints.\nNote that during any of the above steps, once we set some\nRint\ni,j = 1, the variable is never changed. Algorithm 2 cor-\nrects constraints in a particular order so that constraints that\nare satis\ufb01ed will continue to be satis\ufb01ed as other violated\nconstraints are corrected. The matrix Rint generated by this\nCheckmate: Breaking the Memory Wall with Optimal Tensor Rematerialization\nAlgorithm 2 Two-phase rounding\nInput: Fractional checkpoint matrix S\u2217from LP\nOutput: Binary Sint, Rint, FREE\nRound S\u2217deterministically: Sint\nt,i \u21901[S\u2217\nt,i > 0.5]\nRint \u2190In thereby satisfying (8a)\nwhile \u2203t \u22652, i \u2208[n] such that Sint\nt,i > Rint\nt\u22121,i + Sint\nt\u22121,i\ni.e. (1c) violated do\nCompute vi to materialize checkpoint: Rint\nt\u22121,i \u21901\nend while\nwhile \u2203t \u22651, (i, j) \u2208E such that Rint\nt,j > Rint\nt,i + Sint\nt,i\ni.e. (1b) violated do\nCompute vi as temporary for dependency: Rint\nt,i \u21901\nend while\nEvaluate FREE by simulating execution\nreturn Sint, Rint, FREE\nrounding scheme will be optimal up to the choice of Sint as\nevery entry in Rint is set to 1 if and only if it is necessary\nto satisfy a constraint. In implementation, we detect and\ncorrect violations of (1b) in reverse topological order for\neach stage, scanning Rint, Sint matrices from right to left.\n5.3\nMemory budget feasibility\nSince we approximate S by rounding the fractional solu-\ntion, Sint, Rint can be infeasible by the budget constraint\nUt,k \u2264Mbudget. While the fractional solution may come\nunder the budget and two-phase rounding preserves correct-\nness constraints, the rounding procedure makes no attempt\nto maintain budget feasibility. Therefore, we leave an al-\nlowance on the total memory budget constraint (Ut,k \u2264\n(1 \u2212\u03f5)Mbudget). We empirically \ufb01nd \u03f5 = 0.1 to work well.\n6\nEVALUATION\nIn this section, we investigate the impact of tensor remate-\nrialization on the cost and memory usage of DNN training.\nWe study the following experimental questions: (1) What\nis the trade-off between memory usage and computational\noverhead when using rematerialization? (2) Are large in-\nputs practical with rematerialization? and (3) How well can\nwe approximate the optimal rematerialization policy?\nWe compare our proposed solver against baseline heuristics\non representative image classi\ufb01cation and high resolution\nsemantic segmentation models including VGG16, VGG19,\nResNet50, MobileNet, U-Net and FCN with VGG layers,\nand SegNet. As prior work is largely limited to linear graphs,\nwe propose novel extensions where necessary for compar-\nison. Results show that optimal rematerialization allows\nsigni\ufb01cantly lower computational overhead than baselines\nat all memory budgets, and lower memory usage than previ-\nously possible. As a consequence, optimal rematerialization\nallows training with larger input sizes than previously possi-\nble, up to 5.1\u00d7 higher batch sizes on the same accelerator.\nFinally, we \ufb01nd that our two-phase rounding approximation\nalgorithm \ufb01nds near-optimal solutions in polynomial time.\n6.1\nBaselines and generalizations\nTable 1 summarizes baseline rematerialization strategies.\nThe nominal evaluation strategy stores all features generated\nduring the forward pass for use during the backward pass\u2014\nthis is the default in frameworks such as TensorFlow. Hence,\nevery layer is computed once. We refer to this baseline as\nCheckpoint all, an ideal approach given ample memory.\nOn the linear graph architectures, such as VGG16 and Mo-\nbileNet (v1), we directly apply prior work from Griewank &\nWalther (2000) and Chen et al. (2016b), baselines referred to\nas Griewank and Walther log n, Chen et al. \u221an and Chen et\nal. greedy. To build a tradeoff curve for computation versus\nmemory budget, we search over the segment size hyperpa-\nrameter b in the greedy strategy. However, these baselines\ncannot be used for modern architectures with residual con-\nnections. For a fair comparison, we extend the \u221an and\ngreedy algorithms to apply to general computation graphs\n(e.g. ResNet50 and U-Net).\nChen et al. (2016b) suggests manually annotating good\ncheckpointing candidates in a computation graph. For the\n\ufb01rst extensions, denoted by AP \u221an and AP greedy, we\nautomatically identify articulation points, or cut vertices,\nvertices that disconnect the forward pass DAG, and use these\nas candidates. The heuristics then select a subset of these\ncandidates, and we work backwards from the checkpoints\nto identify which nodes require recomputation.\nStill, some networks have few articulation points, including\nU-Net. We also extend heuristics by treating the original\ngraph as a linear network, with nodes connected in topolog-\nical order, again backing out the minimal recomputations\nfrom the selected checkpoints. These extensions are referred\nto as Linearized \u221an and Linearized greedy.\nSections B.1 and B.2 provide more details on our gener-\nalizations. Note that all proposed generalizations exactly\nreproduce the original heuristics on linear networks.\n6.2\nEvaluation setup\nCheckmate is implemented in Tensor\ufb02ow 2.0 (Abadi et al.,\n2016), accepting user-de\ufb01ned models expressed via the high-\nlevel Keras interface. We extract the forward and backward\ncomputation graph, then construct and solve optimization\nproblem (9) with the Gurobi mathematical programming\nlibrary as an integer linear program. Finally, Checkmate\ntranslates solutions into execution plans and constructs a\nnew static training graph. Together, these components form\nthe Checkmate system, illustrated in Figure 2.\nCheckmate: Breaking the Memory Wall with Optimal Tensor Rematerialization\n14\n16\n18\n20\n1.0\n1.1\n1.2\n1.3\n1.4\n1.5\n10\n20\n30\n40\n1.0\n1.1\n1.2\n1.3\n1.4\n1.5\n10\n20\n30\n1.0\n1.1\n1.2\n1.3\n1.4\nOverhead (x)\nBudget (GB)\nChen et al.\nChen et al. greedy\nCheckmate (proposed)\nCheckpoint all (ideal)\nVGG16 (256)\nMobileNet (512)\nU-Net (32)\n224x224\n224x224\n416x608\nGriewank & Walther\n**\n*\n**\n*\n*\n** AP adaptation\nLinearized\nadaptation\nFigure 5. Computational overhead versus memory budget for (a) VGG16 image classi\ufb01cation NN (Simonyan & Zisserman, 2014), (b)\nMobileNet image classi\ufb01cation NN, and (c) the U-Net semantic segmentation NN (Ronneberger et al., 2015). Overhead is with respect to\nthe best possible strategy without a memory restriction based on a pro\ufb01le-based cost model of a single NVIDIA V100 GPU. For U-Net (c),\nat the 16 GB V100 memory budget, we achieve a 1.20\u00d7 speedup over the best baseline\u2014linearized greedy\u2014and a 1.38\u00d7 speedup over\nthe next best\u2014linearized \u221an. Takeaway: our model- and hardware-aware solver produces in-budget solutions with the lowest overhead\non linear networks (a-b), and dramatically lowers memory consumption and overhead on complex architectures (c).\nTo accelerate problem construction, decision variables R\nand S are expressed as lower triangular matrices, as are\naccounting variables U. FREE is represented as a |V | \u00d7 |E|\nmatrix. Except for our maximum batch size experiments,\nsolutions are generated with a user-con\ufb01gurable time limit\nof 3600 seconds, though the majority of problems solve\nwithin minutes. Problems with exceptionally large batch\nsizes or heavily constrained memory budgets may reach\nthis time limit while the solver attempts to prove that the\nproblem is infeasible. The cost of a solution is measured\nwith a pro\ufb01le-based cost model (Section 4.10) and compared\nto the ideal, unachievable cost with no recomputation.\nThe feasible set of our optimal ILP formulation is a su-\nperset of baseline heuristics. We implement baselines as\na static policy for the decision variable S and then solve\nfor the lowest-cost recomputation schedule using a similar\nprocedure to that described in Algorithm 2.\n6.3\nWhat is the trade-off between memory usage and\ncomputational overhead?\nFigure 5 compares remateralization strategies on VGG-16,\nMobileNet, and U-Net. The y-axis shows the computational\noverhead of checkpointing in terms of time as compared to\nbaseline. The time is computed by pro\ufb01ling each individual\nlayer of the network. The x-axis shows the total memory\nbudget required to run each model with the speci\ufb01ed batch\nsize, computed for single precision training. Except for the\n\u221an heuristics, each rematerialization algorithm has a knob\nto trade-off the amount of recomputation and memory usage,\nwhere a smaller memory budget leads to higher overhead.\nTakeaways: For all three DNNs, Checkmate produces\nclearly faster execution plans as compared to algorithms\nproposed by Chen et al. (2016b) and Griewank & Walther\n(2000) \u2013 over 1.2\u00d7 faster than the next best on U-Net at\nthe NVIDIA V100 memory budget. Our framework allows\ntraining a U-Net at a batch size of 32 images per GPU with\nless than 10% higher overhead. This would require 23 GB\nof memory without rematerialization, or with the original\nbaselines without our generalizations.\n6.4\nAre large inputs practical with rematerialization?\nThe maximum batch size enabled by different rematerializa-\ntion strategies is shown in Figure 6. The y-axis shows the\ntheoretical maximum batch size we could feasibly train with\nbounded compute cost. This is calculated by enforcing that\nthe total cost must be less than the cost of performing just\none additional forward pass. That is, in Figure 6 the cost is\nat most an additional forward pass higher, if the speci\ufb01ed\nbatch size would have \ufb01t in GPU memory. To \ufb01nd Check-\nmate\u2019s maximum batch size, we reformulate Problem (9) to\nmaximize a batch size variable B \u2208N subject to modi\ufb01ed\nmemory constraints that use B \u2217Mi in place of Mi and\nsubject to an additional cost constraint,\nn\nX\nt=1\nt\nX\ni=1\nCiRt,i \u22642\nX\nvi\u2208Gfwd\nCi +\nX\nvi\u2208Gbwd\nCi.\n(10)\nThe modi\ufb01ed integer program has quadratic constraints, and\nis dif\ufb01cult to solve. We set a time limit of one day for the\nexperiment, but Gurobi may be unable to reach optimality\nCheckmate: Breaking the Memory Wall with Optimal Tensor Rematerialization\n16\n29\n21\n167\n98\n215\n18\n51\n33\n197\n116\n452\n35\n51\n43\n266\n199\n640\n61\n60\n62\n289\n225\n1105\n0x\n1x\n2x\n3x\n4x\n5x\nU-Net\nFCN8\nSegNet\nVGG19\nResNet50 MobileNet\nNormalized batch size\nCheckpoint all\nAP \u221an\nLin. greedy\nCheckmate (ours)\nFigure 6. Maximum batch size possible on a single NVIDIA V100\nGPU when using different generalized rematerialization strategies\nwith at most a single extra forward pass. We enable increasing\nbatch size by up to 5.1\u00d7 over the current practice of caching\nall activations (on MobileNet), and up to 1.73\u00d7 over the best\ncheckpointing scheme (on U-Net).\nwithin that limit. Figure 6 then provides a lower bound on\nthe maximum batch size that Checkmate can achieve.\nFor fair comparison on the non-linear graphs used in U-\nNet, FCN, and ResNet, we use the AP \u221an and linearized\ngreedy baseline generalizations described in Section 6.1.\nFor the baselines, we iterate over batch sizes, \ufb01nd candidate\nsolutions (multiple candidates for linearized greedy), and\n\ufb01lter out the solutions that cost more than an additional\nforward pass or that would exceed the 16GB memory budget.\nThe iteration stops when no solutions are available.\nCosts are measured in FLOPs, determined statically. U-\nNet, FCN8 and SegNet semantic segmentation networks\nuse a resolution of 416 \u00d7 608, and classi\ufb01cation networks\nResNet50, VGG19 and MobileNet use resolution 224\u00d7224.\nTakeaways: We can theoretically increase the batch size\nof U-Net to 61 at a high resolution, an unprecedented re-\nsult. For many tasks such as semantic segmentation, where\nU-Net is commonly used, it is not possible to use batch\nsizes greater than 16, depending on resolution. This is sub-\noptimal for batch normalization layers, and being able to\nincrease the batch size by 3.8\u00d7 (61 vs 16 at this resolution)\nis quite signi\ufb01cant. Orthogonal approaches to achieve this\ninclude model parallelism and distributed memory batch\nnormalization which can be signi\ufb01cantly more dif\ufb01cult to\nimplement and have high communication costs.\nFurthermore, for MobileNet, Checkmate allows a batch\nsize of 1105 which is 1.73\u00d7 higher than the best baseline\nsolution, a greedy heuristic, and 5.1\u00d7 common practice,\ncheckpointing all activations. The same schedules can also\nbe used to increase image resolution rather than batch size.\nAP\n\u221an\nAP\ngreedy\nGriewank\nlog n\nTwo-phase\nLP rounding\nMobileNet\n1.14\u00d7\n1.07\u00d7\n7.07\u00d7\n1.06\u00d7\nVGG16\n1.28\u00d7\n1.06\u00d7\n1.44\u00d7\n1.01\u00d7\nVGG19\n1.54\u00d7\n1.39\u00d7\n1.75\u00d7\n1.00\u00d7\nU-Net\n1.27\u00d7\n1.23\u00d7\n-\n1.03\u00d7\nResNet50\n1.20\u00d7\n1.25\u00d7\n-\n1.05\u00d7\nTable 2. Approximation ratios for baseline heuristics and our LP\nrounding strategy.\nResults are given as the geometric mean\nspeedup of the optimal ILP across feasible budgets.\n6.5\nHow well can we approximate the optimal\nrematerialization policy?\nTo understand how well our LP rounding strategy (Sec-\ntion 5) approximates the ILP, we measure the ratio\nCOSTapprox/COSTopt, i.e. the speedup of the optimal sched-\nule, in FLOPs. As in Section 6.3, we solve each strategy at a\nrange of memory budgets, then compute the geometric mean\nof the ratio across budgets. The aggregated ratio is used\nbecause some budgets are feasible via the ILP but not via\nthe approximations. Table 2 shows results. The two-phase\ndeterministic rounding approach has approximation factors\nclose to optimal, at most 1.06\u00d7 for all tested architectures.\n7\nCONCLUSION\nOne of the main challenges when training large neural net-\nworks is the limited capacity of high-bandwidth memory\non accelerators such as GPUs and TPUs. This has created\na memory wall that limits the size of the models that can\nbe trained. The bottleneck for state-of-the-art model de-\nvelopment is now memory rather than data and compute\navailability, and we expect this trend to worsen in the future.\nTo address this challenge, we proposed a novel rematerial-\nization algorithm which allows large models to be trained\nwith limited available memory. Our method does not make\nthe strong assumptions required in prior work, supporting\ngeneral non-linear computation graphs such as residual net-\nworks and capturing the impact of non-uniform memory\nusage and computation cost throughout the graph with a\nhardware-aware, pro\ufb01le-guided cost model. We presented\nan ILP formulation for the problem, implemented the Check-\nmate system for optimal rematerialization in TensorFlow,\nand tested the proposed system on a range of neural network\nmodels. In evaluation, we \ufb01nd that optimal rematerializa-\ntion has minimal computational overhead at a wide range of\nmemory budgets and showed that Checkmate enables prac-\ntitioners to train high-resolution models with signi\ufb01cantly\nlarger batch sizes. Finally, a novel two-phase rounding\nstrategy closely approximates the optimal solver.\nCheckmate: Breaking the Memory Wall with Optimal Tensor Rematerialization\nACKNOWLEDGEMENTS\nWe would like to thank Barna Saha and Laurent El Ghaoui\nfor guidance on approximation, Mong H. Ng for help in\nevaluation, and the paper and artifact reviewers for helpful\nsuggestions. In addition to NSF CISE Expeditions Award\nCCF-1730628 and ONR PECASE N000141612723, this\nwork was supported by gifts from Alibaba, Amazon Web\nServices, Ant Financial, CapitalOne, Ericsson, Facebook,\nFuturewei, Google, Intel, Microsoft, NVIDIA, Scotiabank,\nSplunk and VMware. This work was also supported by the\nNSF GRFP under Grant No. DGE-1752814. Any opinions,\n\ufb01ndings, and conclusions or recommendations expressed in\nthis material are those of the author(s) and do not necessarily\nre\ufb02ect the views of the NSF.\nREFERENCES\nAbadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z.,\nCitro, C., Corrado, G. S., Davis, A., Dean, J., Devin, M.,\nGhemawat, S., Goodfellow, I., Harp, A., Irving, G., Isard,\nM., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M., Lev-\nenberg, J., Mane, D., Monga, R., Moore, S., Murray, D.,\nOlah, C., Schuster, M., Shlens, J., Steiner, B., Sutskever,\nI., Talwar, K., Tucker, P., Vanhoucke, V., Vasudevan,\nV., Viegas, F., Vinyals, O., Warden, P., Wattenberg, M.,\nWicke, M., Yu, Y., and Zheng, X. TensorFlow: Large-\nScale Machine Learning on Heterogeneous Distributed\nSystems. March 2016.\nBeaumont, O., Herrmann, J., Pallez, G., and Shilova, A.\nOptimal memory-aware backpropagation of deep join\nnetworks. Research Report RR-9273, Inria, May 2019.\nBriggs, P., Cooper, K. D., and Torczon, L. Rematerialization.\nIn Proceedings of the ACM SIGPLAN 1992 Conference\non Programming Language Design and Implementation,\nPLDI \u201992, pp. 311\u2013321, New York, NY, USA, 1992.\nBrock, A., Donahue, J., and Simonyan, K. Large scale GAN\ntraining for high \ufb01delity natural image synthesis. arXiv\npreprint arXiv:1809.11096, 2018.\nBulo, S. R., Porzi, L., and Kontschieder, P. In-place Ac-\ntivated BatchNorm for Memory-Optimized Training of\nDNNs. In 2018 IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 5639\u20135647. IEEE,\nJune 2018.\nCanziani, A., Paszke, A., and Culurciello, E. An Analysis of\nDeep Neural Network Models for Practical Applications.\nMay 2016. arXiv: 1605.07678.\nChaitin, G. J., Auslander, M. A., Chandra, A. K., Cocke, J.,\nHopkins, M. E., and Markstein, P. W. Register allocation\nvia coloring. Computer Languages, 6(1):47\u201357, January\n1981.\nChen, L.-C., Papandreou, G., Kokkinos, I., Murphy, K., and\nYuille, A. L. DeepLab: Semantic Image Segmentation\nwith Deep Convolutional Nets, Atrous Convolution, and\nFully Connected CRFs. June 2016a. arXiv: 1606.00915.\nChen, T., Xu, B., Zhang, C., and Guestrin, C. Training Deep\nNets with Sublinear Memory Cost. April 2016b. arXiv:\n1604.06174.\nChen, X., Ma, H., Wan, J., Li, B., and Xia, T. Multi-view 3D\nObject Detection Network for Autonomous Driving. In\n2017 IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pp. 6526\u20136534. IEEE, 2017.\nChild, R., Gray, S., Radford, A., and Sutskever, I. Gener-\nating Long Sequences with Sparse Transformers. April\n2019. arXiv: 1904.10509.\nCytron, R., Ferrante, J., Rosen, B. K., Wegman, M. N.,\nand Zadeck, F. K. Ef\ufb01ciently Computing Static Single\nAssignment Form and the Control Dependence Graph.\nACM Trans. Program. Lang. Syst., 13(4):451\u2013490, Octo-\nber 1991.\nDai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q. V., and\nSalakhutdinov, R. Transformer-XL: Attentive Language\nModels Beyond a Fixed-Length Context. January 2019.\narXiv: 1901.02860.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT:\nPre-training of Deep Bidirectional Transformers for Lan-\nguage Understanding. October 2018. arXiv: 1810.04805.\nDong, C., Loy, C. C., He, K., and Tang, X. Image super-\nresolution using deep convolutional networks.\nIEEE\nTransactions on Pattern Analysis and Machine Intelli-\ngence, 38(2):295\u2013307, Feb 2016.\nFeng, J. and Huang, D. Cutting Down Training Memory by\nRe-fowarding. July 2018.\nForrest, J. J., Vigerske, S., Ralphs, T., Santos, H. G., Hafer,\nL., Kristjansson, B., Fasano, J., Straver, E., Lubin, M.,\nrlougee, jpgoncal1, Gassmann, H. I., and Saltzman, M.\nCOIN-OR Branch-and-Cut solver, June 2019.\nGholami, A., Azad, A., Jin, P., Keutzer, K., and Buluc, A. In-\ntegrated model, batch, and domain parallelism in training\nneural networks. In Proceedings of the 30th on Sympo-\nsium on Parallelism in Algorithms and Architectures, pp.\n77\u201386. ACM, 2018.\nGLPK. GNU Project - Free Software Foundation (FSF).\nGomez, A. N., Ren, M., Urtasun, R., and Grosse, R. B. The\nReversible Residual Network: Backpropagation Without\nStoring Activations. In Guyon, I., Luxburg, U. V., Ben-\ngio, S., Wallach, H., Fergus, R., Vishwanathan, S., and\nCheckmate: Breaking the Memory Wall with Optimal Tensor Rematerialization\nGarnett, R. (eds.), Advances in Neural Information Pro-\ncessing Systems 30, pp. 2214\u20132224. Curran Associates,\nInc., 2017.\nGoodwin, D. W. and Wilken, K. D. Optimal and Near-\noptimal Global Register Allocation Using 01 Integer Pro-\ngramming. Software: Practice and Experience, 26(8):\n929\u2013965, 1996.\nGriewank, A. and Walther, A. Algorithm 799: revolve: an\nimplementation of checkpointing for the reverse or ad-\njoint mode of computational differentiation. ACM Trans-\nactions on Mathematical Software, 26(1):19\u201345, March\n2000.\nGruslys, A., Munos, R., Danihelka, I., Lanctot, M.,\nand Graves, A.\nMemory-ef\ufb01cient Backpropagation\nThrough Time. In Proceedings of the 30th International\nConference on Neural Information Processing Systems,\nNIPS\u201916, pp. 4132\u20134140, USA, June 2016. Curran Asso-\nciates Inc.\nGueguen, L., Sergeev, A., Kadlec, B., Liu, R., and Yosin-\nski, J. Faster Neural Networks Straight from JPEG. In\nBengio, S., Wallach, H., Larochelle, H., Grauman, K.,\nCesa-Bianchi, N., and Garnett, R. (eds.), Advances in\nNeural Information Processing Systems 31, pp. 3933\u2013\n3944. Curran Associates, Inc., 2018.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-\ning for image recognition. In Proceedings of the IEEE\nconference on computer vision and pattern recognition,\npp. 770\u2013778, 2016.\nHolder, L. Graph Algorithms: Applications, 2008.\nHuang, G., Liu, Z., Van Der Maaten, L., and Weinberger,\nK. Q. Densely connected convolutional networks. In\nProceedings of the IEEE conference on computer vision\nand pattern recognition, pp. 4700\u20134708, 2017.\nIoffe, S. and Szegedy, C. Batch Normalization: Accelerating\nDeep Network Training by Reducing Internal Covariate\nShift. International Conference on Machine Learning,\nFebruary 2015.\nJain, A., Phanishayee, A., Mars, J., Tang, L., and Pekhi-\nmenko, G. Gist: Ef\ufb01cient Data Encoding for Deep Neu-\nral Network Training. In Proceedings of the 45th An-\nnual International Symposium on Computer Architecture,\nISCA \u201918, pp. 776\u2013789, Piscataway, NJ, USA, 2018.\nIEEE Press.\nJain, P., Mo, X., Jain, A., Tumanov, A., Gonzalez, J. E., and\nStoica, I. The ooo vliw jit compiler for gpu inference.\narXiv preprint arXiv:1901.10008, 2019.\nJia, Z., Lin, S., Qi, C. R., and Aiken, A. Exploring Hidden\nDimensions in Accelerating Convolutional Neural Net-\nworks. In International Conference on Machine Learning,\npp. 2274\u20132283, July 2018a.\nJia, Z., Zaharia, M., and Aiken, A. Beyond Data and Model\nParallelism for Deep Neural Networks. SysML Confer-\nence, pp. 13, Feb. 2018b.\nKarmarkar, N. A new polynomial-time algorithm for linear\nprogramming. In Proceedings of the sixteenth annual\nACM symposium on Theory of computing, pp. 302\u2013311.\nACM, 1984.\nKim, J., Lee, J. K., and Lee, K. M. Accurate image super-\nresolution using very deep convolutional networks. In\n2016 IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pp. 1646\u20131654, June 2016. doi:\n10.1109/CVPR.2016.182.\nKoes, D. R. and Goldstein, S. C. A Global Progressive\nRegister Allocator. In Proceedings of the 27th ACM\nSIGPLAN Conference on Programming Language Design\nand Implementation, PLDI \u201906, pp. 204\u2013215, New York,\nNY, USA, 2006. ACM. event-place: Ottawa, Ontario,\nCanada.\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. ImageNet\nClassi\ufb01cation with Deep Convolutional Neural Networks.\nIn Pereira, F., Burges, C. J. C., Bottou, L., and Weinberger,\nK. Q. (eds.), Advances in Neural Information Process-\ning Systems 25, pp. 1097\u20131105. Curran Associates, Inc.,\n2012.\nLattner, C. LLVM: An Infrastructure for Multi-Stage Op-\ntimization. Master\u2019s thesis, Computer Science Dept.,\nUniversity of Illinois at Urbana-Champaign, Urbana, IL,\nDecember 2002.\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,\nLevy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V.\nRoBERTa: A Robustly Optimized BERT Pretraining Ap-\nproach. July 2019. arXiv: 1907.11692.\nLong, J., Shelhamer, E., and Darrell, T. Fully convolutional\nnetworks for semantic segmentation. In Proceedings\nof the IEEE conference on computer vision and pattern\nrecognition, pp. 3431\u20133440, 2015.\nLozano, R. C., Carlsson, M., Blindell, G. H., and Schulte,\nC. Combinatorial Register Allocation and Instruction\nScheduling. April 2018. arXiv: 1804.02452.\nMcCandlish, S., Kaplan, J., Amodei, D., and Team, O. D.\nAn Empirical Model of Large-Batch Training. arXiv:\n1812.06162.\nCheckmate: Breaking the Memory Wall with Optimal Tensor Rematerialization\nMeng, C., Sun, M., Yang, J., Qiu, M., and Gu, Y. Train-\ning Deeper Models by GPU Memory Optimization on\nTensorFlow. pp. 8, December 2017.\nMicikevicius, P. Local Memory and Register Spilling, 2011.\nNakata, I. On Compiling Algorithms for Arithmetic Ex-\npressions. Commun. ACM, 10(8):492\u2013494, August 1967.\nISSN 0001-0782. doi: 10.1145/363534.363549.\nNesterov, Y. and Nemirovskii, A. Interior-point polynomial\nalgorithms in convex programming, volume 13. Siam,\n1994.\nNVIDIA. NVIDIA Tesla V100 GPU Architecture, 2017.\nOlesen, J. S. Register Allocation in LLVM 3.0, November\n2011.\nPaszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E.,\nDeVito, Z., Lin, Z., Desmaison, A., Antiga, L., and Lerer,\nA. Automatic differentiation in PyTorch. In NIPS 2017\nAutodiff Workshop, 2017.\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,\nChanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga,\nL., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Rai-\nson, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang,\nL., Bai, J., and Chintala, S. Pytorch: An imperative\nstyle, high-performance deep learning library. In Wal-\nlach, H., Larochelle, H., Beygelzimer, A., d\u2019 Alch\u00b4e-Buc,\nF., Fox, E., and Garnett, R. (eds.), Advances in Neural In-\nformation Processing Systems 32, pp. 8024\u20138035. Curran\nAssociates, Inc., 2019.\nPohlen, T., Hermans, A., Mathias, M., and Leibe, B. Full-\nresolution residual networks for semantic segmentation\nin street scenes. In Computer Vision and Pattern Recog-\nnition (CVPR), 2017 IEEE Conference on, 2017.\nPunjani, M. Register Rematerialization in GCC. In GCC\nDevelopers Summit, volume 2004. Citeseer, 2004.\nRonneberger, O., Fischer, P., and Brox, T. U-Net: Convolu-\ntional Networks for Biomedical Image Segmentation. In\nNavab, N., Hornegger, J., Wells, W. M., and Frangi, A. F.\n(eds.), Medical Image Computing and Computer-Assisted\nIntervention MICCAI 2015, Lecture Notes in Computer\nScience, pp. 234\u2013241. Springer International Publishing,\n2015. ISBN 978-3-319-24574-4.\nRosen, B. K., Wegman, M. N., and Zadeck, F. K. Global\nValue Numbers and Redundant Computations. In Pro-\nceedings of the 15th ACM SIGPLAN-SIGACT Symposium\non Principles of Programming Languages, POPL \u201988, pp.\n12\u201327, New York, NY, USA, 1988. ACM.\nSethi, R. Complete Register Allocation Problems. pp. 14,\nApril 1973.\nSimonyan, K. and Zisserman, A. Very Deep Convolutional\nNetworks for Large-Scale Image Recognition. September\n2014. arXiv: 1409.1556.\nSiskind, J. M. and Pearlmutter, B. A. Divide-and-conquer\ncheckpointing for arbitrary programs with no user annota-\ntion. Optimization Methods and Software, 33(4-6):1288\u2013\n1330, 2018a. doi: 10.1080/10556788.2018.1459621.\nSiskind, J. M. and Pearlmutter, B. A. Divide-and-Conquer\nCheckpointing for Arbitrary Programs with No User An-\nnotation. Optimization Methods and Software, 33(4-6):\n1288\u20131330, November 2018b.\nSivathanu, M., Chugh, T., Singapuram, S. S., and Zhou, L.\nAstra: Exploiting Predictability to Optimize Deep Learn-\ning. In Proceedings of the Twenty-Fourth International\nConference on Architectural Support for Programming\nLanguages and Operating Systems - ASPLOS \u201919, pp.\n909\u2013923, Providence, RI, USA, 2019. ACM Press.\nSze, V., Chen, Y.-H., Yang, T.-J., and Emer, J. S. Ef\ufb01cient\nprocessing of deep neural networks: A tutorial and survey.\nProceedings of the IEEE, 105(12):2295\u20132329, 2017.\nSzegedy, C., Wei Liu, Yangqing Jia, Sermanet, P., Reed,\nS., Anguelov, D., Erhan, D., Vanhoucke, V., and Ra-\nbinovich, A.\nGoing deeper with convolutions.\nIn\n2015 IEEE Conference on Computer Vision and Pat-\ntern Recognition (CVPR), pp. 1\u20139, June 2015.\ndoi:\n10.1109/CVPR.2015.7298594.\nTai, Y., Yang, J., and Liu, X. Image super-resolution via\ndeep recursive residual network. In 2017 IEEE Con-\nference on Computer Vision and Pattern Recognition\n(CVPR), pp. 2790\u20132798, July 2017. doi: 10.1109/CVPR.\n2017.298.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention\nis All you Need. In Guyon, I., Luxburg, U. V., Bengio, S.,\nWallach, H., Fergus, R., Vishwanathan, S., and Garnett, R.\n(eds.), Advances in Neural Information Processing Sys-\ntems 30, pp. 5998\u20136008. Curran Associates, Inc., 2017.\nWu, Y. and He, K. Group Normalization. pp. 3\u201319, 2018.\nXie, S., Girshick, R., Doll\u00b4ar, P., Tu, Z., and He, K. Aggre-\ngated residual transformations for deep neural networks.\nIn Proceedings of the IEEE conference on computer vi-\nsion and pattern recognition, pp. 1492\u20131500, 2017.\nYang, B., Liang, M., and Urtasun, R. HDNET: Exploiting\nHD Maps for 3D Object Detection. pp. 10, 2018.\nYannakakis, M. On the approximation of maximum satis\ufb01a-\nbility. Journal of Algorithms, 17(3):475\u2013502, 1994.\nCheckmate: Breaking the Memory Wall with Optimal Tensor Rematerialization\nLayer\nStage\nLayer\nLayer\nStage\nStage\nTensorFlow 2.0\n167 batch size\nChen et al. (2016)\n197 batch size\nCheckmate\n289 batch size\nFigure 7. We visualize schedules (R matrix) for VGG19. The R matrix denotes when each layer in a neural network is evaluated.\nRows refer to execution stage while columns refer to a particular layer. We make no distinction between forward and backward nodes.\nTensorFlow 2.0 can train VGG19 on a V100 with a batch size of 167. By applying the heuristic from Chen et al. (2016b), a V100 can\nsustain training at a batch size od 197. However, our proposed method in Checkmate sustains training at a batch size of 289, representing\na 73% improvement. The integer linear program took 10 seconds to solve to optimality.\nA\nINTEGRALITY GAP\nTo understand why the partitioned variant of the MILP (Sec-\ntion 4.2) is faster to solve via branch-and-bound, we can\nmeasure the integrality gap for particular problem instances.\nThe integrality gap is the maximum ratio between the opti-\nmal value of the ILP and its relaxation, de\ufb01ned as follows:\nIG = max\nI\nCOSTint\nCOSTfrac\n,\nwhere COSTint and COSTfrac are the optimal value the ILP\nand that of its relaxation, respectively. I describes a problem\ninstance,\nI = (G, C, M, Mbudget).\nAs our ILP is a minimization problem, COSTint\n\u2265\nCOSTfrac for all I, and IG \u22651. While it is not possible\nto measure the ratio between the ILP and LP solutions for\nall problem instances, the ratio for any particular problem\ninstance gives a lower bound on the integrality gap.\nFor the 8-layer linear neural network graph discussed in\nSection 4.2, frontier-advancement reduces the integrality\ngap from 21.56 to 1.18, i.e. the LP relaxation is signi\ufb01cantly\ntighter. In branch-and-bound algorithms for ILP optimiz-\ntion, a subset of feasible solutions can be pruned if the LP\nrelaxation over the subset yields an objective higher than\nthe best integer solution found thus far. With a tight LP\nrelaxation, this condition for pruning is often met, so fewer\nsolutions need to be enumerated.\nB\nGENERALIZATIONS OF PRIOR WORK\nB.1\nAP \u221an and AP greedy\nWe identify Articulation Points (AP) in the undirected form\nof the forward pass data-\ufb02ow graph as candidates for check-\npointing. APs are vertices that increase the number of con-\nnected components (i.e. disconnect) the graph if removed,\nand can be identi\ufb01ed in time O(V + E) via a modi\ufb01ed DFS\ntraversal (Holder, 2008). An articulation point va is a good\ncandidate for checkpointing as subsequent vertices in the\ntopological order have no dependencies on vertices before\nva in the order. DNN computation graphs are connected, so\neach intermediate tensor can be reconstructed from a single\narticulation point earlier in the topological order, or the in-\nput if there is no such AP. APs include the input and output\nnodes of residual blocks in ResNet, but not vertices inside\nblocks. We apply Chen\u2019s heuristics to checkpoint a subset of\nthese candidates, then solve for the optimal recomputation\nplan R to restore correctness. Solving for R ensures that a\nnode\u2019s dependencies are resident prior to evaluation.\nWe could \ufb01nd R by solving the optimization problem (9)\nwith additional constraints on S that encode the heuristi-\ncally selected checkpoints. However, as S is given, the\noptimization is solvable in O(|V ||E|) via a graph traversal\nper row of R that \ufb01lls in entries when a needed value is not\nin memory by the same process described in Section 5.2.\nB.2\nLinearized \u221an and Linearized greedy\nThe forward graph of the DNN Gfwd = (Vfwd, Efwd) can\nbe treated as a linear graph Glin = (Vfwd, Elin) with edges\nCheckmate: Breaking the Memory Wall with Optimal Tensor Rematerialization\nDeterministic rounding\nILP\nCheckpoint all\nRandomized rounding\n16\n17\n18\n19\n20\n15.4\n15.6\n15.8\n15\n16\n165\n170\n175\n180\n185\nGPU time (ms)\nVGG16\nMobileNet\nActivation memory usage (GB)\nFigure 8. Comparison of the two-phase LP rounding approxima-\ntion with randomized rounding of S\u2217and deterministic rounding\nof S\u2217on different models. We compare memory usage and compu-\ntational cost (objective), in milliseconds according to pro\ufb01le-based\ncost model. The average of the randomized rounding costs is\nshown as a dotted line.\nconnecting consecutive vertices in a topological order:\nElin = {(v1, v2), (v2, v3), . . . , (vL\u22121, vL)}\nWhile Glin does not properly encode data dependencies, it\nis a linear graph that baselines can analyze. To extend a\nbaseline, we apply it to Glin, generate checkpoint matrix S\nfrom the resulting checkpoint set, and \ufb01nd the optimal R as\nwith the AP baselines.\nC\nHARDNESS OF REMATERIALIZATION\nSethi (1973) reduced 3-SAT to a decision problem based on\nregister allocation in straight line programs, with no recom-\nputation permitted. Such programs can be represented by\nresult-rooted Directed Ayclic Graphs (DAGs), with nodes\ncorresponding to operations and edges labeled by values.\nIn Sethi\u2019s graphs, the desired results are the roots of the\nDAG. If a program has no common subexpressions, i.e. the\ngraph forms a tree, optimal allocation is possible via a lin-\near time tree traversal (Nakata, 1967). However, Sethi\u2019s\nreduction shows a register allocation decision problem in\nthe general case\u2014whether a result-rooted DAG can be com-\nputed with fewer than k registers without recomputation\u2014is\nNP-complete.\nThe decision problem characterizes computation of a DAG\nas a sequence of four possible moves of stones, or registers,\non the nodes of the graph, analogous to statements discussed\nin Section 4.9. The valid moves are to (1) place a register at\na leaf, computing it, or (2) pick up a register from a node.\nAlso, if there are registers at all children of a node x, then\nit is valid to (3) place a register at x, computing it, or (4)\nmove a stone to x from one of the children of x, computing\nx. The register allocation problem reduces to the following\nno-overhead rematerialization decision problem (RP-DEC):\nDe\ufb01nition C.1. (RP-DEC): Given result-terminated data-\n\ufb02ow DAG G = (V, E) corresponding to a program, with\nunit cost to compute each node and unit memory for the\nresults of each node, does there exist an execution plan that\nevaluates the leaf (terminal) node t \u2208V with maximum\nmemory usage b at cost at most |V |?\nRP-DEC is decidable by solving the memory-constrained\nform of Problem 1 with suf\ufb01cient stages, then checking if\nthe returned execution plan has cost at most |V |. RP-DEC\nclosely resembles Sethi\u2019s decision problem, differing only\nin subtleties. The register allocation DAG is rooted at the\ndesired result t whereas a data-\ufb02ow graph terminates at the\nresult. Second, register-based computations can be in place,\ne.g. a summation a + b may be written to the same location\nas either of the operands. In neural network computation\ngraphs, we cannot perform all computations in place, so we\ndid not make this assumption. To reduce Sethi\u2019s decision\nproblem to RP-DEC, given result-rooted DAG G, construct\nresult-terminated G\u2032 by reversing all edges. Then, if Sethi\u2019s\ninstance allows for at most k registers, allow for a memory\nbudget of b = k + 1 bytes: one byte to temporarily write\noutputs of operations that would have been written in place.\nDespite hardness of register allocation, Goodwin & Wilken\n(1996) observe that a 0-1 integer program for optimal alloca-\ntion under an instruction schedule has empirical complexity\nO(n2.5), polynomial in the number of constraints. Similarly,\nSection 6 shows that the frontier-advancing, constrained op-\ntimization problem (9) is tractable for many networks.\nD\nCOMPARISON OF APPROXIMATIONS\nIn Section 5, we discussed an approximation strategy based\non rounding the LP relaxation, evaluated with deterministic\nrounding in Section 6.5. Figure 8 compares schedules pro-\nduced by our proposed two-phase rounding strategy when\nthe S\u2217matrix from the LP relaxation is rounded with a ran-\ndomized and a deterministic approach. While two-phase\nrandomized rounding of S\u2217offers a range of feasible so-\nlutions, two-phase deterministic rounding produces consis-\ntently lower cost schedules. While appropriate for VGG16,\nfor MobileNet, our budget allowance \u03f5 = 0.1 is overly\nconservative as schedules use less memory than the 16 GB\nbudget. A search procedure over \u03f5 \u2208[0, 1] could be used to\nproduce more ef\ufb01cient schedules.\n",
        "context": "Reversible Networks Gomez et al. (2017) propose a re-\nversible (approximately invertible) residual DNN architec-\nture, where intermediate temporary values can be recom-\nputed from values derived later in the standard forward\nand tested the proposed system on a range of neural network\nmodels. In evaluation, we \ufb01nd that optimal rematerializa-\ntion has minimal computational overhead at a wide range of\nmemory budgets and showed that Checkmate enables prac-\nmillions of training iterations. Our method scales to complex, realistic architectures and is hardware-aware\nthrough the use of accelerator-speci\ufb01c, pro\ufb01le-based cost models. In addition to reducing training cost, Checkmate"
    },
    {
        "id": 16,
        "title": "Pb-llm: Partially binarized large language models",
        "author": [
            "Y. Shang",
            "Z. Yuan",
            "Q. Wu",
            "Z. Dong"
        ],
        "year": "2023",
        "doi": "10.48550/arXiv.2310.00034",
        "in_text_citation": "[16]",
        "sentence": "Recent algorithms have significantly increased the scale of NLP and CV models by reducing the memory demands per GPU [16]\u2013[18], allowing the use of accelerators for extremely large models.",
        "abstract": "This paper explores network binarization, a radical form of quantization,\ncompressing model weights to a single bit, specifically for Large Language\nModels (LLMs) compression. Due to previous binarization methods collapsing\nLLMs, we propose a novel approach, Partially-Binarized LLM (PB-LLM), which can\nachieve extreme low-bit quantization while maintaining the linguistic reasoning\ncapacity of quantized LLMs. Specifically, our exploration first uncovers the\nineffectiveness of naive applications of existing binarization algorithms and\nhighlights the imperative role of salient weights in achieving low-bit\nquantization. Thus, PB-LLM filters a small ratio of salient weights during\nbinarization, allocating them to higher-bit storage, i.e.,\npartially-binarization. PB-LLM is extended to recover the capacities of\nquantized LMMs, by analyzing from the perspective of post-training quantization\n(PTQ) and quantization-aware training (QAT). Under PTQ, combining the concepts\nfrom GPTQ, we reconstruct the binarized weight matrix guided by the Hessian\nmatrix and successfully recover the reasoning capacity of PB-LLM in low-bit.\nUnder QAT, we freeze the salient weights during training, explore the\nderivation of optimal scaling factors crucial for minimizing the quantization\nerror, and propose a scaling mechanism based on this derived scaling strategy\nfor residual binarized weights. Those explorations and the developed\nmethodologies significantly contribute to rejuvenating the performance of\nlow-bit quantized LLMs and present substantial advancements in the field of\nnetwork binarization for LLMs.The code is available at\nhttps://github.com/hahnyuan/BinaryLLM.",
        "full_text": "PB-LLM: PARTIALLY BINARIZED LARGE LANGUAGE\nMODELS\nYuzhang Shang\u2217\nIllinois Institute of Technology\nZhihang Yuan\u2217\nHuomo AI\nQiang Wu\nHuomo AI\nZhen Dong\nUC Berkeley\nABSTRACT\nThis paper explores network binarization, a radical form of quantization, compress-\ning model weights to a single bit, specifically for Large Language Models (LLMs)\ncompression. Due to previous binarization methods collapsing LLMs, we propose\na novel approach, Partially-Binarized LLM (PB-LLM), which can achieve extreme\nlow-bit quantization while maintaining the linguistic reasoning capacity of quan-\ntized LLMs. Specifically, our exploration first uncovers the ineffectiveness of na\u00a8\u0131ve\napplications of existing binarization algorithms and highlights the imperative role\nof salient weights in achieving low-bit quantization. Thus, PB-LLM filters a small\nratio of salient weights during binarization, allocating them to higher-bit storage,\ni.e., partially-binarization. PB-LLM is extended to recover the capacities of quan-\ntized LMMs, by analyzing from the perspective of post-training quantization (PTQ)\nand quantization-aware training (QAT). Under PTQ, combining the concepts from\nGPTQ, we reconstruct the binarized weight matrix guided by the Hessian matrix\nand successfully recover the reasoning capacity of PB-LLM in low-bit. Under QAT,\nwe freeze the salient weights during training, explore the derivation of optimal\nscaling factors crucial for minimizing the quantization error, and propose a scaling\nmechanism based on this derived scaling strategy for residual binarized weights.\nThose explorations and the developed methodologies significantly contribute to\nrejuvenating the performance of low-bit quantized LLMs and present substantial\nadvancements in the field of network binarization for LLMs. The code is available\nat PB-LLM.\n1\nINTRODUCTION\nRecently, large language models (LLMs) have gained significant traction in artificial intelligence. It\ncan be attributed to the success of models such as ChatGPT [Brown et al., 2020, Ouyang et al., 2022].\nFollowing its lead, other LLMs such as OPT [Zhang et al., 2022], BLOOM [Scao et al., 2022], and\nLLaMA [Touvron et al., 2023] have emerged, proving that an increase in model size typically results\nin enhanced capabilities. As a result, models with tens to hundreds of billions of parameters have\nbecome the norm. However, their vast size poses considerable deployment challenges on memory-\nconstrained devices. A model such as the LLAMA-65B (with 65 billion parameters) requires at least\n130GB of memory for inference - a number that often exceeds the capacity of a single GPU or server.\nMany methods have been proposed to reduce the memory consumption of LLMs [Zhu et al., 2023].\nThose methods can be categorized into weight quantization [Dettmers et al., 2022], network prun-\ning [Frantar and Alistarh, 2023], and low-rank factorization [Zhang et al., 2023]. Among these\ncompression paradigms, weight quantization is particularly prominent and widely adopted for LLMs.\nSince it preserves the original model architecture and leverages well-trained LLMs\u2019 full-precision\ncheckpoints, the compression process is greatly simplified [Zhu et al., 2023]. However, state-of-the-art\nLLM quantization methods show a marked decline in quality beyond 4 bits [Liu et al., 2023a].\nMore aggressive compression methods are required to push the LLM quantization into the lower bit\nrange. The network binarization technique stands out, reducing the bit-width of weights to just one\nbit [Helwegen et al., 2019, Rusci et al., 2020, Qin et al., 2020a; 2023]. The binarized models take\nlittle storage and memory, and accelerate the inference by efficient bitwise operations. Compared\n\u2217Equal contribution.\n1\narXiv:2310.00034v2  [cs.LG]  7 Nov 2023\nActivation\nPartially-\nBinarized\nWeight FC \ud835\udc78\ud835\udc78\nPartially-\nBinarized\nWeight FC \ud835\udc72\ud835\udc72\nPartially-\nBinarized\nWeight FC \ud835\udc7d\ud835\udc7d\nKV cache\nKV cache\nActivation\nPartially-\nBinarized\nWeight FC\nActivation\nPartially-\nBinarized\nWeight FC-1\nActivation\nPartially-\nBinarized\nWeight FC-1\nActivation\nMulti-Head \nSelf-Attention\nFeed-\nForward\nNetwork\n+0.7 -0.3 +0.1\n-0.3 -0.3 +0.9\n-3.7 -0.4 +0.6\n+0.1\n+2.9\n+0.6\n+0.1\n+0.2\n+0.6\n\u22121\n2\n+\n1\n3\n\u22121\n2\n-3.7\n+2.9\n\u22121\n3\n\u22121\n3\n+\n1\n3\n+\n1\n3\n+\n1\n3\n+\n1\n2\n+\n1\n2\n+\n1\n3\n+\n1\n3\n+\n1\n3\nPartially   Binarize\nColumn    Scaling\n-1\n+1\n+1\n-1\n-1\n+1\n-3.7\n-1\n+1\n+1\n+2.9\n+1\n+1\n-1\n-1\nPartially-Binarized\nWeight Matrix\n(a) One basic block of the Partially-Binarized LLM.\nFP \nOPT1.3B\nRandom\nGuess\n(b) Performance on BoolQ.\nFigure 1: (a) We introduce Partially-Binarized Large Language Model (PB-LLM), where a small subset of\nthe weights of the LLM are frozen and preserved with higher bit precision, while the remaining weights are\nbinarized utilizing an optimal scaling factor strategy; (b) By using PB-LLM, an extremely low-bit LLM can\nbe acquired efficiently (i.e., quantization-aware training converges quickly) while maintaining its language\nreasoning capabilities.\nto other aggressive compression technologies like high-sparsity pruning, network binarization has\npotent topological generics, as it only applies to parameters. Binarization is widely studied in\nacademic research as a standalone compression technique, rather than simply a 1-bit specialization of\nquantization. Some SoTA binarization algorithms have even achieved full-precision performance on\nlarge-scale tasks, e.g., ReActNet [Liu et al., 2020a] for ImageNet classification [Deng et al., 2009].\nIt is theoretically possible to significantly lower the LLM quantization if we generalize the idea of\nbinarizing the weights of LLMs.\nIn this paper, we explore network binarization specifically for LLM quantization and propose Partially-\nbinarized LLMs (abbreviated as PB-LLM). This methodology aims to achieve extreme quantization\nto the lowest possible bit, while maintaining the language reasoning capacity inherent in LLMs. The\nexplorations indicate that simple adaptations of existing binarization algorithms do not work well\nfor LLM quantization. As a result of this realization, attention is directed towards the salient-weight\nproperty of LLM quantization. In order to achieve the desired extreme low-bit quantization, salient\nweights must be fully exploited. We investigate the salient weights in aspects of their detection criteria\nand granularity, as well as the storage costs. Then, we propose the partially binarized matrix, storing\nthe salient weights in higher bits. After establishing the foundation of PB-LLM, the exploration\nextends to regain the lost reasoning capacity of the quantized LLMs, under the frameworks of post-\ntraining quantization (PTQ) and quantization-aware training (QAT). In the view of PTQ, inspired\nby the concepts of GPTQ [Frantar et al., 2022], we reconstruct the PB-LLM matrix guided by the\nHessian matrix and successfully recover the reasoning capacity of PB-LLM in low-bit. In the view of\nQAT, salient weights are frozen throughout the binarization process for efficient training. In addition,\nfrom the perspective of quantization error minimization, we explore how binarized LLMs should be\nscaled based on the ideal scaling factor. We scale the binarized weight based on the derived scaling\nstrategy shown in Fig. 1a. Low-bit quantized LLMs can significantly improve their performance with\nsuch explorations. Benefited from explorations of PTQ and QAT, PB-LLM can efficiently obtain\nan extremely low-bit LLM with comparable reasoning capacity (see Fig. 1b). The methodologies\napplied and the insights gained within this study stand to contribute substantially to the advancement\nof knowledge and development in the field of network binarization for LLMs.\n2\nRELATED WORK\n2.1\nNETWORK BINARIZATION.\nBinarization uses the sign function to binarize weights and activations to \u00b11. To eliminate the\nvanishing gradient issue caused by the sign function in the binarization, the straight-through estimator\n2\n(STE) [Bengio et al., 2013] is utilized for the network backpropagation. Based on this archetype,\ncopious studies contribute to improving the performance of BNNs. Binarization techniques can\nbe broadly classified into three categories: the enhancement of training objectives, the reduction\nof gradient mismatch, and the minimization of quantization errors [Qin et al., 2020b; 2023, Yuan\nand Agaian, 2023]. To illustrate: Gradient Mismatch: Liu et al. [2020b] introduce double residual\nconnections paired with full-precision downsampling layers. This approach addresses the gradient\nvanishing problem that arises due to binarization. Training Objectives: Martinez et al. [2020]\nfocus on optimizing the loss function during training. They suggest aligning the spatial attention\nmaps derived from both binary and real-valued convolutions. Quantization Error Minimization:\nRastegari et al. [2016] identify that the disparity in quantization between full-precision and binarized\nweights can impede the representational abilities of BNNs. As a solution, they introduce a scaling\nfactor\u2014determined by the L1 norm\u2014for both weights and activation functions.\nWhile binarization has proven successful in computer vision, its exploration in natural language\nprocessing remains limited. Existing methods [Bai et al., 2020, Qin et al., 2022, Liu et al., 2022;\n2023b] primarily target smaller language models (e.g., BERT-base [Devlin et al., 2018] with 110M\nparameters) potentially hindering their generalization to larger ones (e.g., LLAMA-7B [Touvron\net al., 2023] with 7B parameters). We investigate binarization for LLMs comprehensively in this\npaper and propose PB-LLM, which is an attempt to compress LLMs using binarization.\n2.2\nLARGE LANGUAGE MODEL QUANTIZATION.\nQuantization, a prominent method in model compression, addresses the storage and computational\noverhead of deep learning models. Recent research efforts successfully apply quantization to compress\nLarge Language Models (LLMs), including Quantization-Aware Training (QAT) and Post-Training\nQuantization (PTQ).\nIn the domain of QAT, innovative strategies like LLM-QAT [Liu et al., 2023a] address challenges in\nacquiring training data for LLMs by leveraging pre-trained models for data-free distillation. Addi-\ntionally, techniques such as QLORA [Dettmers et al., 2023a] focus on parameter-efficient fine-tuning\n(PEFT), expediting model compression and inference acceleration. In PTQ, approaches range from\nquantizing only the weights of LLMs to jointly quantizing both weights and activations. Methods like\nGPTQ [Frantar et al., 2022] and QuIP [Chee et al., 2023] optimize matrix multiplications and propose\nnovel layer-wise quantization techniques achieving high compression rates. SqueezeLLM [Kim et al.,\n2023] and SpQR [Dettmers et al., 2023b] identify weights that lead to particularly large quantization\nerrors and subsequently storing them with higher precision to mitigate the accuracy degradation\ncaused by weight quantization. AWQ [Lin et al., 2023] and OWQ [Lee et al., 2023] contend that when\nquantizing weights, it is crucial to account for the impact of activation outliers on weights. Norm\nTweaking [Li et al., 2023] addresses the issue of activation value deviation by training LayerNorm.\nFor activation quantization, ZeroQuant [Yao et al., 2022] proposes a fine-grained quantization method\nthat can be applied to both weights and activations. Methods like SmoothQuant [Xiao et al., 2022]\nand Outlier Suppression [Wei et al., 2022; 2023] shift the quantization challenge from activations\nto weights by proposing a mathematically equivalent per-channel scaling transformation. Omni-\nQuant [Shao et al., 2023] further enhances performance by training the quantization parameters.\nRPTQ [Yuan et al., 2023] proposed proposes performance improvement through grouped quantization\nafter clustering similar channels. In this paper, our primary focus lies in the binarization of weights\nexclusively, employing both PTQ and QAT methodologies.\n3\nPARTIALLY BINARIZING LARGE LANGUAGE MODELS (PB-LLM)\nIn this section, we elaborate on the methodology of Partially Binarizing Large Language Models,\nnamed PB-LLM. To begin, a review of the foundational framework of binarized neural networks is\npresented, showcasing its applicability and limitation to LLM quantization. Subsequently, a novel\nformat for the quantized matrix is formulated, specifically tailored for the binarization of LLMs.\nTaking advantage of the proposed partially-binarized weight matrix, we delve into its potential in\nthe realms of post-training quantization and training-aware training for LLMs, to break the trade-off\nbetween bit-width and performance. It is crucial to note that, due to constraints in computational\nresources, the methodology exploration predominantly utilizes OPT-1.3B [Zhang et al., 2022] to\nperform the majority of experiments. Given the space constraints, this section primarily focuses on key\n3\naspects of the methodology. For detailed discussions, exact result values, and specific implementation\ndetails in codes, readers are referred to the supplemental materials.\n3.1\nPRELIMINARY: NETWORK BINARIZATION\nTo begin with, we briefly review the general concept of network binarization and binarized neural\nnetworks (BNNs) in [Courbariaux et al., 2016, Hubara et al., 2016]. As most optimizable quantized\nstructures of LLMs are linear layers (see Fig. 1a) in LLMs, we use a one-layer Perceptron to\nshow the training and inference processes of the BNN. The one-layer neural network is defined as\nf(x) = (W)(a), where a \u2208Rdi is the input activation and W : Rdi 7\u2212\u2192Rdo stands for the weight\nmatrix, with di and do representing the sizes of the input and output of the layer, respectively.\nThe goal of network binarization is to represent floating-point (FP) weights, denoted as WF , and/or\nFP activations aF as 1-bit (i.e., ., \u00b11) values [Qin et al., 2020b]. Networks utilizing this representation\nare referred to as BNNs. BNNs diverge from FP neural networks in their forward operations and in\nthe approximation of backward gradients. In the forward propagation, the sign function is used for\nbinarizing FP values of weights:\nForward: sign(x) =\n\u001a\n+1\nx \u22650\n\u22121\nx < 0.\n(1)\nSpecifically, in the training process of binarized network, the BNN maintains FP latent weights WF\nfor gradient updates, and the updated weight matrix WF is binarized into the binary weight matrix\nWB via the binarize function sign(\u00b7), i.e. WB = sign(WF ). Then the intermediate activation\nmap (full-precision) of this layer is produced by AF,o = WBAF,i. For inference efficiency, BNNs\nwith 1-bit weights significantly reduce the memory cost of inference. Theoretically, BNNs can\nbinarize both weights and activations to 1-bit, providing a 32x compression in memory cost and a\n64x acceleration in inference speed, by replacing FP multiplications in conventional floating-point\nnetworks with Xnor-Bitcount operations. However, recent studies highlight that the weights of LLMs\nas the main contributor to memory overhead [Kim et al., 2023], and thus we primarily aim to curtail\nmemory costs. Therefore, in this pivotal exploration of binarized LLMs, our attention is specifically\ncentered on weight binarization, foregoing the simultaneous binarization of weights and activations.\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nARC-Easy\nWinoGrande\nHellaSwag\nPIQA\nBoolQ\n Random\n FP\n BNN\n XNOR\n Bi-Real\n ReCU\n FDA\nARC-Challenge\nOBQA\nMean\nFigure 2: We implement five renowned bi-\nnarization methods on LLMs and assess the\nresultant binarized LLMs across seven zero-\nshot common sense reasoning tasks. Ran-\ndom represents the hypothetical worst base-\nline, indicating random guesses, while FP\nstands as the optimal baseline, representing\nfull-precision OPT-1.3B. The exact values\ncorresponding to this radar graph are detailed\nin the Appendix.\nIn the backward propagation, the main challenge is\nthat the pervasive sign functions are theoretically non-\ndifferentiable, and thus extremely destroy the gradient\nchain in the backward propagation. To address this prob-\nlem, researchers widely exploit the straight-through esti-\nmator (STE) [Bengio et al., 2013] to numerically approx-\nimate the derivative of the whole BNN [Qin et al., 2020b],\ni.e.,\nBackward:\n\u2202L\n\u2202x =\n\u001a\n\u2202L\n\u2202sign(x)\n|x| \u22641\n0\n|x| > 1,\n(2)\nwhich makes the optimization of BNN accessible.\nWe first investigate the possibility of implementing bina-\nrization to LLM quantization. Specifically, following\nthe binarization benchmark in BiBench [Qin et al., 2023],\nwe generalize some representative binarization methods\ninto LLM quantization scenarios. BNN [Hubara et al.,\n2016], XNOR [Rastegari et al., 2016], Bi-Real [Liu et al.,\n2020b], ReCU [Xu et al., 2021a] and FDA [Xu et al.,\n2021b] are re-implemented to quantize LLMs, particularly\nto OPT [Zhang et al., 2022]. Training details are illustrated\nin the Sec. 4. The results evaluated on seven zero-shot\ncommon sense reasoning tasks are shown in Fig. 2. We can see that the LLMs binarized via the exist-\ning popular binarization algorithms perform worse than random guesses, showing that the existing\nbinarization methods are not suitable for LLM binarization.\n4\n3.2\nPARTIALLY BINARIZED WEIGHT MATRIX\nIn the low-bit quantization of Transformers, a significant challenge is managing the salient weights,\nas they can unnecessarily extend the quantization range [Kovaleva et al., 2021]. Several outlier-aware\nquantization methods have been explored to tackle this issue [Dettmers et al., 2022, Wei et al., 2022,\nKim et al., 2023, Lin et al., 2023]. Notably, SqueezeLLM [Kim et al., 2023] provides a generalized\nmethodology for handling outliers in weight values during 4-bit LLM post-training quantization.\nConcurrently, AWQ [Lin et al., 2023] demonstrates that preserving only 1% of significant weights\ncan benefit 4-bit LLM quantization. Motivated by existing research, this study also seeks to optimize\nthe treatment of salient weights while binarizing most of weights. We present Partially-Binarized\nLLMs (PB-LLM), a method involving the selective binarization of the LLMs\u2019 weight matrix, wherein\na minor fraction of weights is kept in high bits for enhanced language capacity.\n3.2.1\nSALIENT WEIGHT: CRITERIA, GRANULARITY, AND COST\nBeyond the most straightforward method of choosing salient weights\u2014selecting based on magnitude\nelement-wise\u2014we conduct a thorough investigation into salient weight detection from two perspec-\ntives: criteria and granularity. For criteria, we compare Magnitude- and Hessian-based methods, and\nfor granularity, we explore both element-wise and column-wise approaches. In addition, we discuss\nthe cost of storing matrix weights in a mixed-precision manner.\nCriteria: Magnitude vs. Hessian. Beyond the identification of salient weights through magnitude,\nalternative criteria have also been examined. The Hessian metric emerges as a crucial factor in LLM\nquantization, as elucidated in [Dong et al., 2019, Frantar et al., 2022, Frantar and Alistarh, 2023],\nparticularly in relation to post-training quantization for LLMs (details regarding the Hessian criteria\nfor PTQ can be found in Sec. 3.3). However, we observe that the selection of salient weights, whether\nby magnitude or Hessian, does not significantly impact the efficacy of PTQ. Consequently, magnitude\nis elected as the preferred criterion for the identification of salient weights in both PTQ and QAT,\nprimarily due to its simplicity and efficacy in distinguishing critical weight components.\n0\n50\n100\n150\n200\n250\n0\n50\n100\n150\n200\n250\nOutliers Matrix Visualization\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 3: Distribution\nof 5% salient weight.\nGranularity: Element-wise vs. Column-wise. Our investigations reveal\nthat adopting a column-wise approach for selecting salient weights has the\npotential to impair the performance of binarization. Visualization of the salient\nweights\u2019 distribution within the matrix, as depicted in Fig. 3 (where the white\ndots represent the filtered salient weights), disclosed a random and uniform\nscattering of these weights. Given the absence of any discernable column-wise\npattern in the distribution of salient weights, a column-wise filtration method\nis deemed unsuitable. This scattered and uniform distribution necessitates an\nelement-wise approach for effective filtration in the binarization process.\nSalient Weight Storing Cost. The additional overhead for storing the salient weights is acceptable.\nThe overall bit number, Nbit must adhere to the following condition:\nNbit \u2264\nfor binary weights\nz\n}|\n{\n1 \u2217rbinary +\nfor salient weights\nz\n}|\n{\nNsalient\u2212bit \u2217(1 \u2212rbinary) +\nfor index storing, could be optimized\nz\n}|\n{\n1\n,\n(3)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRatio of Binarized Weights (rbinary)\n2\n3\n4\n5\n6\n7\n8\n9\nOverall Bit Number (Nbit)\nNoutlier\nbit = 8\nNbit\n9\nNbit\n2\nFigure 4: Variation in overall bit\nnumber Nbit with the ratio of\nthe salient weights rbinary, where\nsalient weights are stored in 8-bit.\nHere, rbinary denotes the ratio of the binarized weights, Nsalient\u2212bit\nrepresents the number of bits allocated for storing salient weights\n(e.g., 8 bits), and the additional 1 bit is allocated for using the\nbitmap mechanism [Chan and Ioannidis, 1998] for index saving. It\u2019s\nimportant to note that employing bitmap for index storage is not\nthe most efficient method and can be optimized further using sparse\nmatrix storage methods such as Compressed Sparse Row (CSR) or\nCompressed Sparse Column (CSC) [Bor\u02c7stnik et al., 2014]; hence the\nuse of \u2264instead of = in Eq. 3. Given this research\u2019s emphasis on\nthe theoretical aspects of binarization for LLM quantization, we do\nnot delve into saving the cost of storing the index. The relationship\nbetween the ratio of salient weights and the overall bit number is\nillustrated in Fig. 4, depicting that a lower ratio corresponds to a\nreduced overall bit number. For example, retaining 10% of weights in 8 bits and binarizing the\nremaining 90% equates to, at most, a 2.7-bit quantization.\n5\nTable 1: Perplexity of C4 on OPT-1.3B quantized with RTN (without GPTQ) and PB-GPTQ. Magnitude criteria\nor Hessian criteria is used for detecting salient weights.\nSalient Fraction\n50%\n20%\n10%\n5%\nRTN Magnitude\n24.5675\n5892.0898\n4889.0385\n8023.1132\nRTN Hessian\n20.2512\n2109.8522\n7508.7788\n6173.1611\nPB-GPTQ Magnitude\n18.3674\n46.4093\n895.0322\n2880.6157\nPB-GPTQ Hessian\n17.7567\n42.1157\n165.6767\n528.4877\nPB-GPTQ Magnitude g=128\n18.0293\n57.2164\n1230.8537\n2662.7114\nPB-GPTQ Hessian g=128\n17.6000\n45.9811\n157.8825\n646.3616\n3.3\nPOST-TRAINING QUANTIZATION FOR PB-LLMS\nAfter defining the partially-binarized matrix format, the next step is to recover the performance\n(i.e., the reasoning capacity in the literature of LLMs) of the quantized PB-LLM. In this section,\nwe explore the weight binarization with post-training quantization (PTQ) methods. PTQ methods\nhold a prominent position in the realm of quantization techniques for LLMs due to their ease of\nimplementation. They enable direct quantization of pre-trained LLMs without the need for a training\ndataset and additional training overhead. Therefore, we first explore the weight binarization within\nthe PTQ framework.\nGPTQ [Frantar et al., 2022] is the most efficient and effective method for weight quantization [Zhu\net al., 2023], capable of quantizing LLMs to 4-bit or even 2-bit. Therefore, we generalize the idea\nof GPTQ to the partial-binarization setting. Specifically, GPTQ quantizes the weights in LLM\nlayer-by-layer to minimize the layer-wise quantization error:\narg min\n\u02c6\nW\n||WX \u2212\u02c6\nWX||2\n2\n(4)\nGPTQ quantizes a weight wq to \u02c6\nwq, calculates the compensation \u03b4\u2212q for remaining weights w\u2212q,\nand then applies the compensation factor to the remaining weights:\n\u03b4\u2212q = wq \u2212\u02c6\nwq\n[H\u22121]qq\n\u00b7 (H\u22121):,q,\nw\u2212q := w\u2212q + \u03b4\u2212q,\n(5)\nwhere the H is the Hessian matrix of the layer-wise quantization error with respect to the weights\nand wq is the q-th value in flattened weight matrix W. In GPTQ, weights are quantized iteratively\nand the remaining weights are updated until all weights have been quantized.\nWe propose to use GPTQ to iteratively bianrize the un-salient weights and quantize the salient weights\nto higher bit, and then apply the compensation to the remaining weights. Specifically, we first detect\nthe salient weights Wsal and un-salient (to-be-binarized) weights Wunsal in the weight matrix\nW = Wsal + Wunsal. Drawing inspiration from SparseGPT [Frantar and Alistarh, 2023], we\ncalculate the saliency metric, represented as vi = w2\ni /[H\u22121]2\nii, for the purpose of detecting salient\nweights using Hessian criterion. The un-salient weights will be binarized to \u02c6\nWunsal, and the salient\nweights will be quantized to higher bit \u02c6\nWsal. We use asymmetric per-channel quantization for both\nsalient and un-salient weights. For un-salient weight, we use the per-channel mean as zero point and\ncalculate the optimal scaling factor \u03b1 for the un-salient weights using the method in Sec. 3.4.2. We\nuse MinMax metric to calibrate the scaling factor and zero point for salient weights.\nIn the quantization process, we iteratively quantize the columns in the weight matrix W. For each\ncolumn, we binarize the un-salient weights and quantize the salient weights, and then calculate\nthe compensation for remaining weights, and then apply the compensation factor to the remaining\ncolumns of weights. This process is repeated until all the weights are quantized. The proposed\nmethod is denoted as PB-GPTQ. We also explore the fine-grained PB-GPTQ, which quantizes the\nweights in a group-wise manner. Specifically, the weight matrix is split into several groups, each\ngroup contains g columns. In each group, we detect the salient weights and un-salient weights, and\nthen calibrate to set the scaling factor and zero point using the weights in this group.\nThe results are listed in Tab. 1. PB-GPTQ is significantly better than RTN. We note that the Hessian-\nbased PB-GPTQ exhibits a superior performance compared to the Magnitude criterion PB-GPTQ.\nThe group-wise PB-GPTQ performs better or worse than the non-group-wise PB-GPTQ, but the\ndifference is not significant. Our analysis suggests that the disparity in scaling factors is not the\nprimary determinant of binarization performance; hence, the introduction of group-wise methodology\ndoes not yield an enhancement in binarization performance. Subsequently, our next endeavor will\ninvolve the application of QAT to reduce the error introduced by weight binarization.\n6\n3.4\nQUANTIZATION-AWARE TRAINING FOR PB-LLMS\nIn order to further enhance the reasoning capacity of the Partially-Binarized Large Language Models\n(PB-LLM), we extend our exploration by employing Quantization-aware Training (QAT) to meticu-\nlously train the quantized models. Because LLM training is difficult, we desire that PB-LLM training\ncould be as efficient as possible. To realize efficient training for PB-LLM, we propose the Salient\nWeights Frozen and Optimal Scaling Factor for Binary Weights, targeting the salient weights and\nbinarized weights, respectively.\n3.4.1\nSALIENT WEIGHTS FROZEN\n0\n2000\n4000\n6000\n8000\n10000\nStep\n3\n4\n5\n6\n7\n8\n9\nLoss\nLoss (w.o. Outlier Frozen)\nLoss (w. 2% Outlier Frozen)\nFigure 5: Training Loss Curves:\nWhen only 2% of weights are re-\ntained in their un-binarized state,\nthe training loss converges more\nswiftly.\nTo leverage the value of pre-trained weights, we propose freezing\nthe salient weights, determined by weight magnitude, prior to the\nweight binarization process. As illustrated in Fig. 1a, we initially\nfilter out a number of weights from a pre-trained weight matrix\u2014e.g.,\n2% by magnitude\u2014at the beginning of quantization-aware training,\nmaintaining their fixed state throughout the training process. Exami-\nnation of training efficiency (refer to Fig.5) suggests that these salient\nweights play a crucial role in LLM capacity. Maintaining the high\nbit representation of certain weights, thereby freezing them, aids\nin the training of quantized LLMs and reduces their optimization\ndifficulty.\n3.4.2\nOPTIMAL SCALING FACTOR FOR BINARY WEIGHTS.\nAWQ [Lin et al., 2023] enhances the weight-only quantization method for LLMs by optimizing scaling\nfactors to mitigate the quantization error of quantized weights. Specifically, AWQ demonstrates\nthat searching for empirically optimal scaling factors proves to be an effective strategy for reducing\nquantization errors and recovering the performance of the quantized models. Fortunately, in the\ncontext of LLM binarization, we have a better choice for scaling the binarized weights. There\u2019s no\nneed to search for optimal scaling factors as they can be analytically derived. Specifically, we apply\na column-wise scaling factor to binarized weights to reduce the binarization error, i.e., enforcing\nwF = \u03b1 \u00afwB. The optimal values of scaling factor \u03b1 for the \u00afwB \u2208{\u22121, 1} can be calculated by\nminimizing the L2 error:\n\u03b1\u22c6= arg min\n\u03b1\u2208R+ J (\u03b1), in which J (\u03b1) = \u2225wF \u2212\u03b1 \u00afwB\u22252\n2\n(6)\nFollowing XNOR-Net [Rastegari et al., 2016], by expanding the below equation, we have\nJ (\u03b1) = \u03b12 \u00afwT\nB \u00afwB \u22122\u03b1wT\nF \u00afwB + wT\nF wF\n(7)\nMethod\n0\n50\n100\n150\n200\n250\n300\n350\nPPL on C4\n367.3791\n322.6059\n141.7981\n109.5769\n47.8226\n16.0707\nMethod\nXNOR\nBNN\nPB-LLM  w.o. Outlier Frozen\nPB-LLM  w.o. Optimal Scaling\nPB-LLM\nFP-OPT1.3B\nFigure 6: Perplexity (PPL) on C4:\nWhen 50% of the weights are main-\ntained in their un-binarized state\n(equivalent to around 5-bit quantiza-\ntion), the untrained PB-LLM does not\nexperience a total loss of reasoning\ncapabilities.\nFor the vector with wF \u2208Rn we follow the traditional methods\nof binarizing weights [Hubara et al., 2016] by taking the sign of\nreal-valued weights:\n\u00afwi\nB = sign(wi\nF ) =\n\u001a\n+1,\nwi\nF \u22650;\n\u22121,\nwi\nF < 0.\n(8)\nIn that case, \u00afwT\nB \u00afwB = nwF , where nwF is number of elements\nin wF , and \u03b1\u2217can be solved as:\n\u03b1\u2217= wT\nF \u00afwB\nnwF\n= \u2225wF \u22251\nnwF\n.\n(9)\nA counterintuitive outcome emerges from the incorporation of\nsalient-frozen and optimal-scaling mechanisms: directly deploy-\ning those two mechanisms to pre-trained LLM even without any\nretraining or fine-tuning, still results in commendable perfor-\nmance. For instance, applying these techniques to OPT-1.3B with 50% salient weights (see Fig. 6)\nreveals that the partially-binarized OPT-1.3B retains a small amount of language capacity, corrob-\norating the importance of a small number of salient weights in LLM quantization. Consequently,\n7\n100\n101\n102\n103\n104\nEpoch\n0.40\n0.45\n0.50\n0.55\n0.60\nAccuracy\n30%\u00a0Salient\u00a0PB\u00adLLM\n4\u00adbit\u00a0LLM\u00adQAT\n(a) BoolQ [Clark et al., 2019]\n100\n101\n102\n103\n104\nEpoch\n0.52\n0.54\n0.56\n0.58\n0.60\n0.62\n0.64\n0.66\n0.68\nAccuracy\n30%\u00a0Salient\u00a0PB\u00adLLM\n4\u00adbit\u00a0LLM\u00adQAT\n(b) PIQA [Bisk et al., 2020]\n100\n101\n102\n103\n104\nEpoch\n0.225\n0.250\n0.275\n0.300\n0.325\n0.350\n0.375\n0.400\nAccuracy\n30%\u00a0Salient\u00a0PB\u00adLLM\n4\u00adbit\u00a0LLM\u00adQAT\n(c) HellaSwag [Zellers et al., 2019]\n100\n101\n102\n103\n104\nEpoch\n0.50\n0.51\n0.52\n0.53\n0.54\n0.55\n0.56\n0.57\nAccuracy\n30%\u00a0Salient\u00a0PB\u00adLLM\n4\u00adbit\u00a0LLM\u00adQAT\n(d) WinoGrande [Sakaguchi et al., 2021]\n100\n101\n102\n103\n104\nEpoch\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\nAccuracy\n30%\u00a0Salient\u00a0PB\u00adLLM\n4\u00adbit\u00a0LLM\u00adQAT\n(e) ARC-E [Clark et al., 2018]\n100\n101\n102\n103\n104\nEpoch\n0.16\n0.18\n0.20\n0.22\n0.24\n0.26\nAccuracy\n30%\u00a0Salient\u00a0PB\u00adLLM\n4\u00adbit\u00a0LLM\u00adQAT\n(f) ARC-C [Clark et al., 2018]\n100\n101\n102\n103\n104\nEpoch\n0.16\n0.17\n0.18\n0.19\n0.20\n0.21\nAccuracy\n30%\u00a0Salient\u00a0PB\u00adLLM\n4\u00adbit\u00a0LLM\u00adQAT\n(g) OBQA [Mihaylov et al., 2018]\n100\n101\n102\n103\n104\nEpoch\n0.32\n0.34\n0.36\n0.38\n0.40\n0.42\n0.44\nAccuracy\n30%\u00a0Salient\u00a0PB\u00adLLM\n4\u00adbit\u00a0LLM\u00adQAT\n(h) Average on Seven Tasks\n100\n101\n102\n103\n104\nEpoch\n0.40\n0.45\n0.50\n0.55\n0.60\nAccuracy\n5%\u00a0Salient\u00a0PB\u00adLLM\n3\u00adbit\u00a0LLM\u00adQAT\n(i) BoolQ [Clark et al., 2019]\n100\n101\n102\n103\n104\nEpoch\n0.500\n0.525\n0.550\n0.575\n0.600\n0.625\n0.650\n0.675\nAccuracy\n5%\u00a0Salient\u00a0PB\u00adLLM\n3\u00adbit\u00a0LLM\u00adQAT\n(j) PIQA [Bisk et al., 2020]\n100\n101\n102\n103\n104\nEpoch\n0.22\n0.24\n0.26\n0.28\n0.30\n0.32\n0.34\n0.36\nAccuracy\n5%\u00a0Salient\u00a0PB\u00adLLM\n3\u00adbit\u00a0LLM\u00adQAT\n(k) HellaSwag [Zellers et al., 2019]\n100\n101\n102\n103\n104\nEpoch\n0.40\n0.42\n0.44\n0.46\n0.48\n0.50\n0.52\n0.54\nAccuracy\n5%\u00a0Salient\u00a0PB\u00adLLM\n3\u00adbit\u00a0LLM\u00adQAT\n(l) WinoGrande [Sakaguchi et al., 2021]\n100\n101\n102\n103\n104\nEpoch\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\nAccuracy\n5%\u00a0Salient\u00a0PB\u00adLLM\n3\u00adbit\u00a0LLM\u00adQAT\n(m) ARC-E [Clark et al., 2018]\n100\n101\n102\n103\n104\nEpoch\n0.16\n0.17\n0.18\n0.19\n0.20\n0.21\n0.22\n0.23\nAccuracy\n5%\u00a0Salient\u00a0PB\u00adLLM\n3\u00adbit\u00a0LLM\u00adQAT\n(n) ARC-C [Clark et al., 2018]\n100\n101\n102\n103\n104\nEpoch\n0.16\n0.18\n0.20\n0.22\n0.24\nAccuracy\n5%\u00a0Salient\u00a0PB\u00adLLM\n3\u00adbit\u00a0LLM\u00adQAT\n(o) OBQA [Mihaylov et al., 2018]\n100\n101\n102\n103\n104\nEpoch\n0.28\n0.30\n0.32\n0.34\n0.36\n0.38\n0.40\n0.42\nAccuracy\n5%\u00a0Salient\u00a0PB\u00adLLM\n3\u00adbit\u00a0LLM\u00adQAT\n(p) Average on Seven Tasks\nFigure 7: QAT training results with 30% salient weights PB-LLM (upper two lines): As fine-tuning epochs\nincrease, quantized models swiftly regain their reasoning capacities, demonstrating the resilience and adaptability\nof PB-LLMin sustaining cognitive functionalities within models, despite substantial quantization; QAT training\nresults with 5% salient weights PB-LLM (bottom two lines): Existing LLM QAT methods exhibit an absolute\nfailure when subjected to extremely-low bit conditions. In contrast, PB-LLMtriumphs in restoring the reasoning\ncapacities of low-bit quantized LLMs. This underlines the efficacy of PB-LLM in balancing quantization and\nperformance, preserving the essential reasoning abilities of LLMs even under rigorous bit reduction.\nimplementing just these two techniques\u2014Outlier Frozen and Optimal Scaling Factor for Binary\nWeights\u2014on pre-trained LLMs serves as an efficient starting point for training PB-LLM.\nBoth of the above-proposed mechanisms are very effective when used during quantization-aware\ntraining of PB-LLM. The consequential outcomes are delineated in Figs.7a-7p. Observations from the\npresented results elucidate that optimizing using the partially-binarized quantization format is notably\nmore straightforward compared to single-bit quantization. This empirical evidence corroborates the\ndiscussion regarding the rapid convergence property found in Sec.3.4.1, highlighting the efficacy\nand adaptability of our proposed methodology in optimizing LLMs within the constraints of partial\nbinarization. From the perspective of QAT, PB-LLM emerges as more efficient in training compared\nto existing LLM QAT methods. For instance, while models like LLM-QAT [Liu et al., 2023a]\nnecessitate up to 100K iterations for adequate training, PB-LLM remarkably achieves recovery of the\nperformance of quantized LLMs in merely around 1-10K iterations. This substantial reduction in\nrequired iterations represents a leap in training efficiency, streamlining the path to achieving optimal\nperformance in quantized LLMs with significantly reduced computational effort.\n4\nEXPERIMENTS\nBesides the exploration with OPT-1.3B in Sec. 3, we assess the effectiveness of PB-LLM by conduct-\ning experiments on LLaMA-7B [Touvron et al., 2023] and presenting results on various tasks.\n8\nTable 2: Zero-shot performance on Common Sense Reasoning tasks within a 4-bit setting. Reported results of\nprevious works are documented in their papers. PB-LLM 30% denotes the preservation of 30% salient weights,\nand PB-LLM 10% implies the preservation of 10% salient weights.\nMethod\nBoolQ\nPIQA\nHellaSwag\nWinoGrande\nARC-E\nARC-C\nOBQA\nAvg\nFP LLaMA-7B\n76.8\n79.3\n76.1\n70.0\n73.0\n48.0\n57.6\n68.7\nRTN\n71.2\n77.3\n72.7\n66.9\n68.8\n46.4\n52.8\n65.2\nSmoothQuant\n67.7\n76.0\n69.4\n66.7\n66.9\n43.0\n50.6\n63.0\nLLM-QAT\n75.5\n78.3\n74.0\n69.0\n70.0\n45.0\n55.4\n66.6\nPB-GPTQ 10%\n62.3\n55.9\n27.7\n49.3\n29.3\n20.1\n10.6\n36.5\nPB-GPTQ 30%\n73.5\n74.9\n47.5\n64.9\n61.3\n32.4\n25.2\n54.2\nPB-LLM 10%\n68.9\n67.8\n68.1\n67.4\n58.7\n42.9\n50.6\n60.6\nPB-LLM 30%\n75.7\n78.0\n74.3\n69.7\n69.0\n45.6\n55.8\n66.9\n4.1\nEXPERIMENTAL SETUP\nDataset. In this study, the PB-LLM is trained using the RedPajama-simple-1B dataset, as the dataset\nfor LLaMa training is not openly accessible. This dataset, RedPajama-1T, is structured to closely\nresemble the LLaMa paper and serves as a transparent, open-source alternative to LLM training\ndataset. It amalgamates data from diverse sources including Commoncrawl, C4, GitHub, Wikipedia,\nGutenberg Books3, ArXiv, and Stackexchange. RedPajama-simple-1B, representing a 0.1% subset\nof RedPajama-1T, is substantially smaller than the typical datasets used for training other LLMs,\nmaking it a convenient choice for our experiments.\nTraining Details. In the training process of our quantized network, we commence with a pre-\ntrained model for initialization. The optimization of the model is facilitated through the AdamW\noptimizer [Loshchilov and Hutter, 2017], applied with zero weight decay. We assign a batch size of 1\nto each GPU and implement a learning rate of 2e-5, adhering to a cosine learning rate decay strategy.\nWe only fine-tune our PB-LLM for 10K iterations.\nEvaluated Tasks. To eliminate the variance of evaluated performance, we evaluate the binarized\nLLMs on seven zero-shot common sense reasoning tasks, i.e., BoolQ [Clark et al., 2019], PIQA [Bisk\net al., 2020], HellaSwag [Zellers et al., 2019], WinoGrande [Sakaguchi et al., 2021], ARC-Easy,\nARC-Challenge [Clark et al., 2018], OBQA [Mihaylov et al., 2018]. We also along eavulated the\nquantized moelds\u2019 perplexity scores on WikiText2 [Merity et al., 2016] and C4 [Raffel et al., 2020].\n4.2\nRESULTS ON LLAMA\nTable 3: Perplexity of C4, wikitext2 and PTB on\nLLaMA-7b quantized with PTQ methods.\nC4\nWIKI\nPTB\nFP\n7.3435\n5.6770\n41.1509\nGPTQ 4b\n8.6977\n8.1368\n57.9951\nSparseGPT 50%\n15.5949\n12.829483\n505.1396\nPB-GPTQ 50%\n8.1466\n6.3089\n54.8674\nPB-GPTQ 20%\n20.6057\n17.1929\n280.4353\nPB-GPTQ 10%\n72.1115\n85.7838\n708.4120\nPB-GPTQ 5%\n401.6475\n619.1054\n1687.1815\nExperiments were conducted on LLaMA-7B. The\nresults of employing PB-GPTQ and PB-LLM are\nillustrated in Tabs. 2 and 3. When employing PTQ,\nPB-GPTQ exhibited commendable performance,\nparticularly when the salient weight exceeded 30%.\nNevertheless, a noteworthy decline in the perfor-\nmance of the quantized network was observed when\nthe salient weight was reduced to 10%. On the other\nhand, employing QAT resulted in a notable improve-\nment in the performance. A comparison within a 4-bit quantization setting between PB-LLM 30%\nand LLM-QAT in Tab. 2 reveals superior performance by our method. It is notable that PB-LLM is\nonly fine-tuned for 10K iterations, whereas LLM-QAT underwent 100K iterations of training, show-\ning its fast convergence property (refer to Sec. 3.2). The results under PB-LLM 10% represent the\noutcomes of PB-LLM where 10% of salient weights are preserved. This demonstrates the potential\nfor advancing LLM quantization towards a fully 1-bit state.\n5\nCONCLUSION\nIn conclusion, this work is the first to implement network binarization for LLM quantification, intro-\nducing the novel Partially-binarized LLM (PB-LLM) methodology. This approach is meticulously\ndesigned to maintain linguistic reasoning capabilities of LLMs, even under extreme low-bit quantiza-\ntion. The research unearthed the significant role of salient weights in achieving extreme quantization\nand proposed innovative strategies like optimal scaling for effective binarization. This framework\nis extended to recover the capacities of quantized LMMs, by analyzing from the perspective of\npost-training quantization (PTQ) and quantization-aware training (QAT). The methodology is a\nsignificant stride in the realm of network binarization for LLMs.\n9\nREFERENCES\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:1877\u20131901, 2020.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with\nhuman feedback. Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan,\nMona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv\npreprint arXiv:2205.01068, 2022.\nTeven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u00b4c, Daniel Hesslow, Roman Castagn\u00b4e,\nAlexandra Sasha Luccioni, Franc\u00b8ois Yvon, Matthias Gall\u00b4e, et al. Bloom: A 176b-parameter open-access\nmultilingual language model. arXiv preprint arXiv:2211.05100, 2022.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix,\nBaptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation\nlanguage models. arXiv preprint arXiv:2302.13971, 2023.\nXunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping Wang. A survey on model compression for large language\nmodels. arXiv preprint arXiv:2308.07633, 2023.\nTim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for\ntransformers at scale. arXiv preprint arXiv:2208.07339, 2022.\nElias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot.\nICML, 2023.\nMingyang Zhang, Chunhua Shen, Zhen Yang, Linlin Ou, Xinyi Yu, Bohan Zhuang, et al. Pruning meets low-rank\nparameter-efficient fine-tuning. arXiv preprint arXiv:2305.18403, 2023.\nZechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi,\nRaghuraman Krishnamoorthi, and Vikas Chandra. Llm-qat: Data-free quantization aware training for large\nlanguage models. arXiv preprint arXiv:2305.17888, 2023a.\nKoen Helwegen, James Widdicombe, Lukas Geiger, Zechun Liu, Kwang-Ting Cheng, and Roeland Nusselder.\nLatent weights do not exist: Rethinking binarized neural network optimization. Advances in neural information\nprocessing systems, 2019.\nManuele Rusci, Alessandro Capotondi, and Luca Benini. Memory-driven mixed low precision quantization for\nenabling deep network inference on microcontrollers. MLSys, 2020.\nHaotong Qin, Ruihao Gong, Xianglong Liu, Mingzhu Shen, Ziran Wei, Fengwei Yu, and Jingkuan Song.\nForward and backward information retention for accurate binary neural networks. In CVPR, 2020a.\nHaotong Qin, Mingyuan Zhang, Yifu Ding, Aoyu Li, Zhongang Cai, Ziwei Liu, Fisher Yu, and Xianglong Liu.\nBibench: Benchmarking and analyzing network binarization. ICML, 2023.\nZechun Liu, Zhiqiang Shen, Marios Savvides, and Kwang-Ting Cheng. Reactnet: Towards precise binary neural\nnetwork with generalized activation functions. In ECCV, 2020a.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical\nimage database. In CVPR, 2009.\nElias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for\ngenerative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.\nYoshua Bengio, Nicholas L\u00b4eonard, and Aaron Courville. Estimating or propagating gradients through stochastic\nneurons for conditional computation. arXiv:1308.3432, 2013.\nHaotong Qin, Ruihao Gong, Xianglong Liu, Xiao Bai, Jingkuan Song, and Nicu Sebe. Binary neural networks:\nA survey. Pattern Recognition, 105:107281, 2020b.\nChunyu Yuan and Sos S Agaian. A comprehensive review of binary neural network. Artificial Intelligence\nReview, pages 1\u201365, 2023.\n10\nZechun Liu, Wenhan Luo, Baoyuan Wu, Xin Yang, Wei Liu, and Kwang-Ting Cheng. Bi-real net: Binarizing\ndeep network towards real-network performance. IJCV, 2020b.\nBrais Martinez, Jing Yang, Adrian Bulat, and Georgios Tzimiropoulos. Training binary neural networks with\nreal-to-binary convolutions. arXiv preprint arXiv:2003.11535, 2020.\nMohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet classification\nusing binary convolutional neural networks. In ECCV, 2016.\nHaoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin, Xin Jiang, Qun Liu, Michael Lyu, and Irwin King.\nBinarybert: Pushing the limit of bert quantization. arXiv preprint arXiv:2012.15701, 2020.\nHaotong Qin, Yifu Ding, Mingyuan Zhang, Qinghua Yan, Aishan Liu, Qingqing Dang, Ziwei Liu, and Xianglong\nLiu. Bibert: Accurate fully binarized bert. arXiv preprint arXiv:2203.06390, 2022.\nZechun Liu, Barlas Oguz, Aasish Pappu, Lin Xiao, Scott Yih, Meng Li, Raghuraman Krishnamoorthi, and Yashar\nMehdad. Bit: Robustly binarized multi-distilled transformer. Advances in neural information processing\nsystems, 35:14303\u201314316, 2022.\nZechun Liu, Barlas Oguz, Aasish Pappu, Yangyang Shi, and Raghuraman Krishnamoorthi. Binary and ternary\nnatural language generation. ACL, 2023b.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized\nllms. arXiv preprint arXiv:2305.14314, 2023a.\nJerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher De Sa. Quip: 2-bit quantization of large language\nmodels with guarantees. arXiv preprint arXiv:2307.13304, 2023.\nSehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W Mahoney, and\nKurt Keutzer. Squeezellm: Dense-and-sparse quantization. arXiv preprint arXiv:2306.07629, 2023.\nTim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashkboos,\nAlexander Borzunov, Torsten Hoefler, and Dan Alistarh. Spqr: A sparse-quantized representation for\nnear-lossless llm weight compression. arXiv preprint arXiv:2306.03078, 2023b.\nJi Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-aware weight\nquantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023.\nChanghun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim, and Eunhyeok Park. Owq: Lessons learned from\nactivation outliers for weight quantization in large language models. arXiv preprint arXiv:2306.02272, 2023.\nLiang Li, Qingyuan Li, Bo Zhang, and Xiangxiang Chu. Norm tweaking: High-performance low-bit quantization\nof large language models. arXiv preprint arXiv:2309.02784, 2023.\nZhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. Ze-\nroquant: Efficient and affordable post-training quantization for large-scale transformers. arXiv preprint\narXiv:2206.01861, 2022.\nGuangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient\npost-training quantization for large language models. arXiv preprint arXiv:2211.10438, 2022.\nXiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang, Qi Zhang, Fengwei Yu, and\nXianglong Liu. Outlier suppression: Pushing the limit of low-bit transformer language models. NeurIPS,\n2022.\nXiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang Guo, and Xianglong Liu.\nOutlier suppression+: Accurate quantization of large language models by equivalent and optimal shifting and\nscaling. arXiv preprint arXiv:2304.09145, 2023.\nWenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao,\nYu Qiao, and Ping Luo. Omniquant: Omnidirectionally calibrated quantization for large language models.\nCoRR, abs/2308.13137, 2023.\nZhihang Yuan, Lin Niu, Jiawei Liu, Wenyu Liu, Xinggang Wang, Yuzhang Shang, Guangyu Sun, Qiang Wu,\nJiaxiang Wu, and Bingzhe Wu. Rptq: Reorder-based post-training quantization for large language models.\narXiv preprint arXiv:2304.01089, 2023.\n11\nMatthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural networks\nwith binary weights during propagations. In NeurIPS, 2016.\nItay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized neural\nnetworks. In NeurIPS, 2016.\nZihan Xu, Mingbao Lin, Jianzhuang Liu, Jie Chen, Ling Shao, Yue Gao, Yonghong Tian, and Rongrong Ji.\nRecu: Reviving the dead weights in binary neural networks. In ICCV, 2021a.\nYixing Xu, Kai Han, Chang Xu, Yehui Tang, Chunjing Xu, and Yunhe Wang. Learning frequency domain\napproximation for binary neural networks. In NeurIPS, 2021b.\nOlga Kovaleva, Saurabh Kulshreshtha, Anna Rogers, and Anna Rumshisky. Bert busters: Outlier dimensions\nthat disrupt transformers. arXiv preprint arXiv:2105.06990, 2021.\nZhen Dong, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Hawq: Hessian aware\nquantization of neural networks with mixed-precision. In ICCV, 2019.\nChee-Yong Chan and Yannis E Ioannidis. Bitmap index design and evaluation. In SIGMOD, 1998.\nUrban Bor\u02c7stnik, Joost VandeVondele, Val\u00b4ery Weber, and J\u00a8urg Hutter. Sparse matrix multiplication: The\ndistributed block-compressed sparse row library. Parallel Computing, 2014.\nChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova.\nBoolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044,\n2019.\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in\nnatural language. In AAAI, 2020.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really\nfinish your sentence? arXiv preprint arXiv:1905.07830, 2019.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial\nwinograd schema challenge at scale. Communications of the ACM, 2021.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint\narXiv:1803.05457, 2018.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a\nnew dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018.\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101,\n2017.\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv\npreprint arXiv:1609.07843, 2016.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei\nLi, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. JMLR,\n2020.\n12\nA\nSUPPLEMENTAL MATERIALS\nA.1\nEXISITING BINARIZATION METHODS ON LLM QUANTIZATION\nMethod\nBoolQ\nPIQA\nHellaSwag\nWinoGrande\nARC-Easy\nARC-Challenge\nOBQA\nMean\nRandom Performance\n0.5\n0.5\n0.25\n0.5\n0.25\n0.25\n0.25\n0.36\nFP\n0.595\n0.63\n0.415\n0.595\n0.54\n0.22\n0.25\n0.46\nBNN\n0.38\n0.545\n0.235\n0.46\n0.195\n0.165\n0.15\n0.30\nXNOR\n0.37\n0.525\n0.265\n0.49\n0.195\n0.165\n0.16\n0.31\nBi-Real\n0.395\n0.5\n0.25\n0.505\n0.235\n0.185\n0.165\n0.32\nReCU\n0.39\n0.515\n0.24\n0.51\n0.255\n0.185\n0.175\n0.32\nFDA\n0.39\n0.485\n0.265\n0.49\n0.265\n0.19\n0.17\n0.32\nTable 4: Table corresponds to Figure 2 in the main paper: We implement five renowned binarization\nmethods on LLMs and assess the resultant binarized LLMs across seven zero-shot common sense\nreasoning tasks.\nWe first investigate the possibility of implementing binarization to LLM quantization. Specifically,\nfollowing the binarization benchmark in BiBench [Qin et al., 2023], we generalize some representative\nbinarization methods into LLM quantization scenarios. BNN [Hubara et al., 2016], XNOR [Rastegari\net al., 2016], Bi-Real [Liu et al., 2020b], ReCU [Xu et al., 2021a] and FDA [Xu et al., 2021b] are\nre-implemented to quantize LLMs, particularly to OPT [Zhang et al., 2022]. Training details are\nillustrated in the Sec. 4. The results evaluated on seven zero-shot common sense reasoning tasks are\nshown in the above table. We can see that the LLMs binarized via the existing popular binarization\nalgorithms perform worse than random guesses, showing that the existing binarization methods are\nnot suitable for LLM binarization.\n13\n",
        "context": "130GB of memory for inference - a number that often exceeds the capacity of a single GPU or server.\nMany methods have been proposed to reduce the memory consumption of LLMs [Zhu et al., 2023].\nLLaMA [Touvron et al., 2023] have emerged, proving that an increase in model size typically results\nin enhanced capabilities. As a result, models with tens to hundreds of billions of parameters have\n2023b] primarily target smaller language models (e.g., BERT-base [Devlin et al., 2018] with 110M\nparameters) potentially hindering their generalization to larger ones (e.g., LLAMA-7B [Touvron"
    },
    {
        "id": 17,
        "title": "Qft: Quantized full-parameter tuning of llms with affordable resources",
        "author": [
            "Z. Li",
            "X. Liu",
            "B. Zhu",
            "Z. Dong",
            "Q. Gu",
            "K. Keutzer"
        ],
        "year": "2023",
        "doi": "10.48550/arXiv.2310.07147",
        "in_text_citation": "[17]",
        "sentence": "Recent algorithms have significantly increased the scale of NLP and CV models by reducing the memory demands per GPU [16]\u2013[18], allowing the use of accelerators for extremely large models.",
        "abstract": "",
        "full_text": "",
        "context": null
    },
    {
        "id": 18,
        "title": "Megatron-lm: Training multi-billion parameter language models using model parallelism",
        "author": [
            "M. Shoeybi",
            "M. Patwary",
            "R. Puri",
            "P. LeGresley",
            "J. Casper",
            "B. Catanzaro"
        ],
        "year": "2019",
        "doi": "10.48550/arXiv.1909.08053",
        "in_text_citation": "[18]",
        "sentence": "Recent algorithms have significantly increased the scale of NLP and CV models by reducing the memory demands per GPU [16]\u2013[18], allowing the use of accelerators for extremely large models.",
        "abstract": "",
        "full_text": "",
        "context": null
    },
    {
        "id": 19,
        "title": "Bagpipe: Accelerating deep recommendation model training",
        "author": [
            "S. Agarwal",
            "C. Yan",
            "Z. Zhang",
            "S. Venkataraman"
        ],
        "year": "2023",
        "doi": null,
        "in_text_citation": "[19]",
        "sentence": "These embeddings can often be terabytes large, but as observed in practice, they are not accessed uniformly at random. In real datasets, the access pattern of these embeddings varies, generally with a small portion of embeddings being accessed far more frequently than others [19].",
        "abstract": "Deep learning based recommendation models (DLRM) are widely used in several\nbusiness critical applications. Training such recommendation models efficiently\nis challenging because they contain billions of embedding-based parameters,\nleading to significant overheads from embedding access. By profiling existing\nsystems for DLRM training, we observe that around 75\\% of the iteration time is\nspent on embedding access and model synchronization. Our key insight in this\npaper is that embedding access has a specific structure which can be used to\naccelerate training. We observe that embedding accesses are heavily skewed,\nwith around 1\\% of embeddings representing more than 92\\% of total accesses.\nFurther, we observe that during offline training we can lookahead at future\nbatches to determine exactly which embeddings will be needed at what iteration\nin the future. Based on these insights, we develop Bagpipe, a system for\ntraining deep recommendation models that uses caching and prefetching to\noverlap remote embedding accesses with the computation. We design an Oracle\nCacher, a new component that uses a lookahead algorithm to generate optimal\ncache update decisions while providing strong consistency guarantees against\nstaleness. We also design a logically replicated, physically partitioned cache\nand show that our design can reduce synchronization overheads in a distributed\nsetting. Finally, we propose a disaggregated system architecture and show that\nour design can enable low-overhead fault tolerance. Our experiments using three\ndatasets and four models show that Bagpipe provides a speed up of up to 5.6x\ncompared to state of the art baselines, while providing the same convergence\nand reproducibility guarantees as synchronous training.",
        "full_text": "Bagpipe: Accelerating Deep Recommendation Model\nTraining\nSaurabh Agarwal\nUniversity of Wisconsin-Madison\nChengpo Yan\nUniversity of Wisconsin-Madison\nZiyi Zhang\nUniversity of Chicago\nShivaram Venkataraman\nUniversity of Wisconsin-Madison\nAbstract\nDeep learning based recommendation models (DLRM) are\nwidely used in several business critical applications. Training\nsuch recommendation models efficiently is challenging be-\ncause they contain billions of embedding-based parameters,\nleading to significant overheads from embedding access. By\nprofiling existing systems for DLRM training, we observe\nthat around 75% of the iteration time is spent on embed-\nding access and model synchronization. Our key insight in\nthis paper is that embedding access has a specific structure\nwhich can be used to accelerate training. We observe that\nembedding accesses are heavily skewed, with around 1% of\nembeddings representing more than 92% of total accesses.\nFurther, we also observe that during offline training we can\nlookahead at future batches to determine which embeddings\nwill be needed at what iteration in the future. Based on these\ninsights, we develop Bagpipe, a system for training deep\nrecommendation models that uses caching and prefetching\nto overlap remote embedding accesses with the computation.\nWe design an Oracle Cacher, a new component that uses a\nlookahead algorithm to generate optimal cache update deci-\nsions while providing strong consistency guarantees against\nstaleness. We also design a logically replicated, physically\npartitioned cache and show that our design can reduce syn-\nchronization overheads in a distributed setting. Finally, we\npropose a disaggregated system architecture and show that\nour design can enable low-overhead fault tolerance. Our ex-\nperiments using three datasets and four models show that\nBagpipe provides a speed up of up to 5.6x compared to state\nof the art baselines, while providing the same convergence\nand reproducibility guarantees as synchronous training.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies\nare not made or distributed for profit or commercial advantage and that\ncopies bear this notice and the full citation on the first page. Copyrights\nfor components of this work owned by others than the author(s) must\nbe honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific\npermission and/or a fee. Request permissions from permissions@acm.org.\nSOSP \u201923, October 23\u201326, 2023, Koblenz, Germany\n\u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed\nto ACM.\nACM ISBN 979-8-4007-0229-7/23/10...$15.00\nhttps://doi.org/10.1145/3600006.3613142\nCCS Concepts: \u2022 Computing methodologies \u2192Distributed\ncomputing methodologies; Machine learning;\nKeywords: Distributed Training, Recommendation Models\nACM Reference Format:\nSaurabh Agarwal, Chengpo Yan, Ziyi Zhang, and Shivaram Venkatara-\nman. 2023. Bagpipe: Accelerating Deep Recommendation Model\nTraining. In ACM SIGOPS 29th Symposium on Operating Systems\nPrinciples (SOSP \u201923), October 23\u201326, 2023, Koblenz, Germany. ACM,\nKoblenz, Germany, 16 pages. https://doi.org/10.1145/3600006.3613142\n1\nIntroduction\nRecommendation models are widely deployed in enterprises\nto personalize and improve user experience. Applications\nthat use recommendations range from personalized web\nsearch results [59] to friend recommendations in social net-\nworks [12, 44] and product recommendations in e-commerce\n[40]. With growing data sizes [2] and the use of the rec-\nommendation in increasingly sophisticated tasks [22, 37],\nrecent trends have seen the adoption of new deep learning\nbased recommendation models [9, 55]. Deep learning based\nrecommendation models have become one of the largest ML\nworkloads with companies like Meta reporting that more\nthan 50% of all ML training cycles [2] and more than 70% of\ninference cycles [17] are being used for deep learning based\nrecommendation (DLRM) models.\nThe structure of recommendation models differs signif-\nicantly from other popular deep neural networks. Recom-\nmendation models contain two types of model parameters:\nembedding tables that store vector representation of categor-\nical features and neural networks that consist of multi-layer\nperceptrons (MLP) for numeric features (Figure 1). If we\nconsider a click-through-rate (CTR) prediction model, given\na batch of training examples (user clicks), in the forward\npass we first look up the relevant embeddings for categorical\nfeatures of the input. We note that the embedding lookup is\nsparse; for instance, if the user location is New York, we only\nneed to fetch the embeddings corresponding to that location\nfrom the embedding table which contains embeddings for\nall the locations. The numerical features are processed by\na dense MLP, and the representations are then combined to\nform a prediction as shown in Figure 1. Similar to existing\nDNNs, the embeddings and the MLP parameters are updated\nin the backward pass based on the gradients.\narXiv:2202.12429v4  [cs.DC]  1 Nov 2023\nTop Neural\nNetwork\nEmbedding\nTable 1\nEmbedding\nTable 2\nNumerical\n\u00a0Feature 1\u00a0\nNumerical\n\u00a0Feature M\u00a0\nCategorical \nFeature 1\nCategorical \nFeature N\nPairwise Interaction\nConcatenate\nBottom\nNeural\nNetwork\nPrediction\nFigure 1. Architecture of a recommenda-\ntion model: Model parameters include top,\nbottom NNs and embedding tables.\nForward\nBackward+MLP\nGetEmb\nEmbedding Sync\nTorchRec\nBagpipe\n0\n20\n40\n60\n80\n100\n120\n140\nTime (ms)\n(a) DLRM\nTorchRec\nBagpipe\n0\n200\n400\n600\n800\nTime (ms)\n(b) DeepFM\nFigure 2. Training Time Breakdown: Average time spent in various stages of training\nwhen using 8 p3.2xlarge instances with TorchRec [47] (left) and Bagpipe (right) on DLRM\nand DeepFM models (Table 2). For large models like DeepFM, we observe that TorchRec\nspends 75% of each iteration on embedding access, while Bagpipe can bring it down to 10%.\nThe unique structure of recommendation models intro-\nduces a new design challenge which is not handled by ex-\nisting DL training systems [29, 39, 52]. The challenge arises\nfrom the extremely large, memory-intensive embedding ta-\nbles. In production systems it is common for size of embed-\nding tables to be in terabytes [17, 72] and Table 1 shows\nthe embedding table sizes for three open-source datasets.\nSince embedding tables will not fit in the memory of a sin-\ngle worker, a hybrid parallelization approach [2, 47, 51] is\ncurrently used to train these models. With hybrid paralleliza-\ntion, dense MLP layers are replicated across GPUs on each\nmachine while embedding tables are partitioned and stored\non the CPU memory of workers, leading to a design where\nmodel parallelism is used for embeddings, and data paral-\nlelism is used for dense layers.\nDespite using hybrid parallelism, existing approaches to\nDLRM training suffer from embedding access bottlenecks\nas each iteration of training requires remote access of em-\nbeddings. In Figure 2, profiling TorchRec [47] on an 8 GPU\ncluster with the Criteo dataset (details in \u00a75) shows that on\neach iteration, up to 75% of the time is spent in embedding\nfetch and write back operations. Reducing the embedding\naccess overhead forms the primary motivation of our work.\nTo reduce embedding access overhead we observe that the\nembedding accesses have a specific structure which can be\nleveraged to accelerate training. Our analysis across three\ndatasets indicates significant skew in embedding accesses;\ne.g., with the Kaggle (Criteo) dataset, we observe that 90%\nof accesses are for just 0.1% of the embeddings. While this\nmight indicate that we can just cache popular embeddings\nin GPU memory [4], we find that to be insufficient as each\ntraining batch also requires a number of embeddings that\nare not present in the cache. When caching 0.1% of the most\npopular embeddings for the same Criteo datasets with a\nbatch size of 16,384, only 15% of the total unique embedding\naccess are served from the cache(\u00a72.3).\nGiven the above embedding access patterns, to alleviate\noverheads, our key insight is that when performing offline\ntraining of ML models we can lookahead beyond the current\nbatch and observe the data access of future batches. Thus, we\ncan revisit classic perfect-caching algorithms [8] to prefetch\nembeddings based on when they will be used and cache\nembeddings in GPU memory if they will be reused in the\nnear future. Unlike prior systems, we propose jointly using\nprefetching and caching to overlap embedding fetches for\nfuture batches with compute of the current batch, effectively\nhiding the latency of embedding access.\nHowever, there are additional challenges in extending this\nlookahead-based design to a distributed setting. To ensure\nembeddings are not stale, we need to perform cache synchro-\nnization across workers which leads to additional commu-\nnication overheads. We propose a new logically replicated,\nphysically partitioned (LRPP) cache design (\u00a73.3) to minimize\nthe cache synchronization time; having a logically replicated\ncache minimizes the overhead of tracking state separately\nfor each trainer, while having a physically partitioned design\nhelps in reducing the number of bytes transferred across\ntrainers. We also enhance this design using Critical Path\nAnalysis (CPA) [70] to only synchronize necessary embed-\ndings on the critical path and delay the rest, thereby overlap-\nping part of cache synchronization with forward/backward\ncompute of future iterations. In combination, we find that\nour techniques can reduce communication overheads by\n65%\u201370%.\nWe build the above techniques in Bagpipe, a system for\nlarge-scale distributed training of recommendation models.\nCentral to our design is an Oracle Cacher, a new service\nthat looks beyond the current batch, to determine which em-\nbeddings to pre-fetch and/or cache. The caching decisions\nmade by Oracle Cacher are realized on each training worker,\nand trainers overlap prefetching and cache synchronization\nwith model training. We show that this design enables in-\ndependent scaling of various training components based on\nworkload requirements and also minimizes the time required\nto recover from failures (\u00a73.4).\nTo evaluate Bagpipe we use three datasets, Criteo Kag-\ngle [34], Avazu [27], and Criteo Terabyte [35], and four pop-\nular models, DLRM [53], Wide&Deep [9], DeepFM [16] and\nD&C [65]. We scale up our training to model sizes of 4.4\nbillion parameters, (similar to models used in MLperf) us-\ning up to 32 GPUs. Overall we find that Bagpipe can im-\nprove iteration time by 3.7\u00d7 compared to TorchRec [47]\nand 2.3\u00d7 compared to asynchronous training in HET [48].\nWe also show that Bagpipe\u2019s design enables fault tolerance\nwith low overhead and can recover from a trainer failure\n13\u00d7 faster compared to FB-Research\u2019s system [55]. Finally,\nunlike prior work [4, 48], our optimizations in Bagpipe main-\ntain consistent access to embeddings. Thus, we maintain the\nsame statistical efficiency as synchronous training, and our\ntime-per-iteration improvements directly translate to time-\nto-accuracy speedups.\nBy using lookahead to fetch parts of the model (embed-\ndings) out-of order and LRPP distributed caches, Bagpipe is\nthe first distributed recommendation model training system\nthat: (i) alleviates embedding access overhead by up to 92%\nusing both prefetching and caching, (ii) transparently accel-\nerates training while maintaining the same guarantees as\nsynchronous training and (iii) reduces network overheads\nand thus enables an efficient, dis-aggregated deployment\nwhere we can independently scale memory intensive (em-\nbedding servers) and compute intensive (trainers) workers\nbased on requirements.\n2\nBackground & Motivation\nWe first provide background on recommendation model\ntraining and then motivate the need for a new system to\noptimize data movement.\n2.1\nDeep Learning Recommendation Models\nRecommendation models power widely used large-scale in-\nternet services. Recently deep learning is being used to im-\nprove the accuracy of recommendations [9, 28, 53]. All deep\nrecommendation models consist of two types of components\n(i) a memory-intensive embedding layer that stores a map-\nping between the categorical features and their numerical\nrepresentations (ii) a compute-intensive neural network-\nbased modeling layer which models interactions between\nnumerical features and vector representations of categorical\nfeatures. Across models, the structure of embedding tables\ntypically remains the same. The number of rows (elements\nin the table) usually depends on the dataset, i.e., the num-\nber of categories, while machine learning engineers vary\nthe dimension of embedding i.e., the size of the vector to\nrepresent a categorical feature. Common dimensions of em-\nbeddings are 16, 32, 48, and 64 but sometimes can be as\nlarge as 384 [51]. However, the rows in embedding tables\nvary widely and can be as small as 3 elements or as big as\na few billion elements depending on the dataset [17, 72].\nNeural network layers have more diversity, and the type\nof neural network usually depends on the modeling task\nat hand. DLRM [53], a recommendation model popular at\nMeta uses fully connected layers for both bottom and top\nTable 1. Dataset and their embedding tables\nTraining Input\nEmbedding Tables\nDatasets\nDatapoints\nCategorical\nFeatures\nNum\nFeatures\nNum\nEmb\nEmbedding\nDimension\nTable\nSize\nKaggle Criteo\n39.2 Million\n26\n13\n33.76 Million\n48\n6 GB\nAvazu\n40.4 Million\n21\n1\n9.4 Million\n48\n1.7 GB\nTerabyte\n4.37 Billion\n26\n13\n882.77 Million\n16\n157 GB\nTable 2. Model Descriptions: For DLRM, W&D, D&C the num-\nbers indicate the structure of the different Fully Connected (FC)\nlayers. For DeepFM, Linear Features represent a linear layer that is\nused to store feature interactions. For all the models, we used the\nstandard architectures as suggested by original authors.\nModel\nArchitecture of Dense Parameters\nNumber of\nDense Parameters\nDLRM [53]\nFC - 13-512- 256-64-48\nFC - 1024-1024-1024-256-128-1\n2962289\nW&D [9]\nFC - 13-256-256-256\n136673\nD&C [65]\nFC - 1024-512-256-64-48\nFC - 1024-512-256-1\n2718609\nDeepFM [16]\nLinear Features - 33762577-1\nFC - 1248-64-64-64\n33851283\nneural networks. While, DeepFM [16] uses a factorization\nmodule that learns up to two-order feature interactions be-\ntween sparse and dense features (Table 2 summarizes other\nmodels we consider in this paper). The forward pass of train-\ning involves looking up embedding vectors corresponding\nto the data items. All deep learning based recommendation\nmodels [10, 64, 66\u201368] use this step to handle categorical fea-\ntures such as location, product type, gender, etc. Our focus is\nto reduce data access overheads that arise from performing\nembedding lookups and thus speed up the training of all\nrecommendation models which use embedding tables.\n2.2\nTraining Recommendation models\nNext, we discuss the state-of-the-art systems used for train-\ning recommendation models.\nOffline Training vs Online Training. Recommendation\nmodels are trained in both online and offline mode. Offline\ntraining involves training the model on large amounts of\nhistorical data with emphasis on throughput. Alternatively,\nonline training is performed only on the recently acquired\ndata to account for latest user preferences and is latency\nsensitive. To boost performance and prevent catastrophic\nforgetting [14, 32], researchers actively perform offline train-\ning, even for models in production. According to a study by\nMeta [2] offline training is responsible for more than 50% cy-\ncles of all ML model training cycles. This shows that offline\ntraining of recommendation models is an important work-\nload. In this work our primary focus is on offline training of\nrecommendation models.\nTraining Setup. Recommendation models are extremely\nlarge and are currently among the largest ML models used\nin enterprises. Meta recently released a 12 Trillion parame-\nter recommendation model [51]; in comparison GPT-3 has\n175 Billion parameters. However, embedding tables with\nsparse access patterns account for more than 99% of pa-\nrameters. The combination of extremely large model sizes\nwith the sparse access pattern introduces several new chal-\nlenges in distributed training. Figure 1 shows a schematic of\na deep learning based recommendation model. In a typical\nDLRM training setup, dense neural network (NN) parameters\nare replicated and stored on the GPUs and trained in data-\nparallel fashion, where gradients are synchronized using the\nall-reduce communication collective. However, embedding\ntables are extremely large to hold in the GPU memory and\nare usually partitioned.\nExisting Systems. Several systems have been designed\nto perform offline recommendation model training due to\nit\u2019s popularity. Training systems like TorchRec [47], FB-\nResearch\u2019s DLRM [55] and HugeCTR [26] partition the em-\nbedding table across different GPUs and train them in a\nmodel-parallel fashion. Embeddings are fetched using all-to-\nall collective [50]. While, TorchRec tries to overlap embedding-\nrelated operations, like remote embedding reads and write-\nbacks, with the compute-intensive portion of the neural\nnetwork, the amount of embedding data that needs to be\nfetched still adds significant overhead during training. Fig-\nure 2 shows a breakdown of the time taken for one iteration\nof training when using TorchRec [47]. We observe that when\nusing 8 training machines (AWS p3.2xlarge instances), the\noverheads when compared to an ideal baseline that does\nnot perform any embedding lookups, is around 70% for the\nDLRM [53] and 75% for DeepFM [16].\nBeyond spending a majority of time in embedding lookups,\nexisting systems also couple storage and compute resources,\ne.g., if the embedding tables for a model are extremely large\nbut the compute requirements are small, one still has to use\na large number of GPU machines to store the embedding\ntables. This often leads to sub-optimal use of resources.\nTo alleviate the embedding access overhead and improve\nresource utilization, FAE [4] performed an analysis of em-\nbedding accesses and observed a similar skew in embedding\naccess (\u00a72.3) patterns. However, FAE uses a reordering ap-\nproach by dividing examples into hot and cold batches based\non their embedding accesses, this impacts the statistical effi-\nciency as training continuously with hot batches changes the\norder of the training examples and can affect convergence [4].\nFurther, it is not always possible to create batches that only\naccess cached embeddings, because some models [37, 42] use\nfeatures like Unique User ID (UUID) or Session ID [60, 62, 63]\nthat are unlikely to be repeated and thus requiring at least\none cache miss per example.\nSeveral other prior works [2, 18, 25, 43, 48] have proposed\nusing asynchronous training to reduce embedding access\noverhead. With asynchronous training, embedding fetches\ncan happen in the background, e.g., in HET trainers can use\nembeddings that are stale up to a certain number of iterations.\nIf embeddings are stale beyond the bound HET synchronizes\n10\u22122\n10\u22121\n100\n101\n102\nPercent of most frequent embeddings\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nCDF of total accesses\nCDF of access across datasets\nCriteo Terabyte\nCriteo Kaggle\nAvazu\nFigure 3. CDF of embedding accesses: The embedding access\npattern is heavily skewed, with just the top 0.1% of the embeddings\nresponsible for more than 90% of total accesses.\nthose embeddings with the embedding server before a train-\ning iteration. However, similar to other ML models, recent\nworks [25, 51] have observed that asynchronous training can\nlead to degradation in accuracy for recommendation models.\nAccuracy degradation is unacceptable to large enterprises as\nit often directly leads to a loss in revenue [51]. Asynchronous\ntraining is also avoided due to the lack of reproducibility,\nwhich is necessary to reason about and compare different\nmodel versions. Therefore, in this work we focus on design-\ning a system for synchronous distributed training.\n2.3\nEmbedding Access Patterns\nNext, we describe some observations from analyzing embed-\nding access patterns in recommendation model training.\nSkew in embedding accesses. When profiling embedding\naccesses across three datasets (details in Table 1), similar\nto prior work [4], we find that the embedding access pat-\ntern shows a large degree of skew. As shown in Figure 3 we\nobserve that almost 90% of embedding accesses come from\njust 0.1% of the embeddings. This indicates that caching hot\nembeddings can reduce the number of embedding lookups\nsignificantly. Thus, when a training example needs a popular\nembedding, it can access it from a cache. However, we also\nobserve that the set of popular embeddings can change over\ntime. We measured the change in popularity over time across\nthree datasets (Criteo, Alibaba, and Avazu) by, first choos-\ning a fixed fraction of the most popular embeddings on the\nfirst day (e.g., top 0.05% most popular embeddings), and then\nmeasuring the percentage of accesses consisting of those em-\nbeddings in later days. We observed that such a scheme leads\nto a degrading cache hit ratio, e.g., in the case of the Avazu,\nif we chose the top 0.05% of the most popular embeddings\nto be cached, the cache hit rate changes from 91% on day 1\nto 82% on day 9, showing that static caching approaches [4]\nwill require regular updates for good performance. The pres-\nence of similar skew in web scale data is common [3]. Prior\nwork [38] also highlights that, at web scale, users create small\ncommunities and are more likely to strongly interact with\nmembers/items within those communities and very sparsely\nwith items beyond their community. Therefore, we believe\nCriteo Kaggle\nAvazu\nCriteo Terabyte\nAlibaba\n256\n1024\n4096\n16384\n256\n1024\n4096\n16384\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\n0.0\n0.6\n0.8\n0.4\n0.2\n0.0\n0.8\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\n0.70\n0.65\n0.60\n0.55\n0.50\n256\n1024\n4096\n16384\n256\n1024\n4096\n16384\nHit Rate\nHit Rate\nHit Rate\nHit Rate\nBatch Size\nBatch Size\nBatch Size\nBatch Size\nFigure 4. Distribution of unique embeddings accessed from the cache: The hit rate of the cache decreases with increase in batch size.\nThe blue line represents the expected hit rate when storing the most frequently accessed 0.1% embeddings.\nthat most recommendation models trained on click stream\ndata will have a similar skew as observed in Figure 3.\nLong-tail of accesses limits benefits from caching.\nThe analysis presented in previous paragraphs is for a sce-\nnario where embeddings are accessed for one example after\nanother (i.e., batch size of 1). However, during recommenda-\ntion model training it is common to use a large batch size\n(e.g., 16,384 [51]). When embeddings are fetched for a batch,\nfetching only the unique embeddings within a batch is suffi-\ncient. Therefore, when calculating cache hit rates we should\nonly account for the number of unique accesses.\nWe next calculate the effectiveness of only caching the\npopular embeddings when using large batch sizes. Figure 4\nshows the distribution of unique embeddings fetched from\nthe cache as we increase the batch size. Most notably, we see\nthat as batch size increases the ratio of embeddings fetched\nfrom the cache to the total number of unique embeddings\n(accounting for duplicates across examples in a batch) needed\nin a batch keeps decreasing drastically, e.g., for a batch size of\n16,384 (standard batch size used in MLPerf [46]), only around\n10% of the total unique embeddings required are fetched from\nthe cache. Quantitatively, for the Criteo dataset, we observed\nthat a batch of 16,384 needs 425,984 embeddings. Within this\nbatch there are only around 65,000 unique embeddings (or\nunique categorical features); out of these 65,000 embeddings,\nthe cache only has a 10% hit rate.\n2.4\nDesign Goals\nBased on the above discussion we have three primary design\ngoals: (i) High throughput training of deep learning based\nrecommendation models by reducing the embedding access\noverhead, (ii) Provide the same guarantees as synchronous\ntraining to improve reproducibility and maintain training\naccuracy, and (iii) Design a disaggregated architecture where\nmemory and compute can be independently scaled.\nWe approach our design based on the workload patterns\nobserved and aim to improve performance by speeding up\naccess to both frequently used and long-tail of embeddings.\nWe design Bagpipe, a framework that caches hot embed-\ndings and performs out-of-order prefetching for the long-tail\nembeddings. We describe our design next.\nData Processors - 1\nData Processors - N\nData Storage service\nOracle\nCacher -1\nEmbedding Server - Partition 1\nEmbedding Server - Partition N\nTrainer 1\nTrainer N\nDense NN\nParameters\nOracle\nCacher-N\nLocal\nCaches\nFigure 5. Bagpipe setup: All the components of Bagpipe can be\nindividually scaled. The dashed arrows signify async RPCs while\nsolid ones signify sync RPCs.\n3\nBagpipe Design\nWe begin by providing an overview of our design.\n3.1\nDesign Overview\nBagpipe consists of four components that collectively per-\nform training as shown in Figure 5. Each iteration of training\nbegins with sampling a batch of examples, the DataProcessors\npre-process the examples and send them to the Oracle Cacher.\nBased on the examples, Oracle Cacher runs a lookahead algo-\nrithm to determine embeddings to prefetch and to cache, and\ndispatches this information to the Trainers. The trainers\ntypically run on GPU machines and perform gradient com-\nputation. Trainers hold the dense parameters of the model\nand Bagpipe\u2019s cache in the GPU memory. Also, trainers\nfetch necessary embeddings from the EmbeddingServers.\nEmbedding servers hold the embedding tables of the rec-\nommendation model. This design introduces the following\ncontributions:\n\u2022 Bagpipe utilizes both caching and prefetching to reduce\nembedding access overhead. Given an offline training\nregime, we introduce the concept of lookahead, where\nwe can look beyond the current batch and decide which\nelements to cache and prefetch (\u00a73.2).\n\u2022 We extend our scheme to the distributed setting and intro-\nduce a logically replicated, physically partitioned cache\ndesign (\u00a73.3) to minimize communication overheads. To\nfurther reduce synchronization overheads we use CPA, to\nselectively synchronize parts of the cache that are imme-\ndiately needed on the critical path while synchronizing\nthe rest in the background.\n\u2022 Finally, we discuss how Bagpipe\u2019s dis-aggregated design\ncan help improve efficiency, by scaling components de-\npending on the properties of the dataset and the model,\nand enable low-overhead fault tolerance (\u00a73.4).\nInput:LookAheadValue\n1 \ud835\udc35\ud835\udc4e\ud835\udc61\ud835\udc50\u210e\ud835\udc44\ud835\udc62\ud835\udc52\ud835\udc62\ud835\udc52= \ud835\udc44\ud835\udc62\ud835\udc52\ud835\udc62\ud835\udc52();\n2 \ud835\udc3f\ud835\udc4e\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61\ud835\udc47\ud835\udc5f\ud835\udc4e\ud835\udc50\ud835\udc58\ud835\udc52\ud835\udc5f= \ud835\udc37\ud835\udc56\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc4e\ud835\udc5f\ud835\udc66();\n3 \ud835\udc3c\ud835\udc5b\ud835\udc36\ud835\udc4e\ud835\udc50\u210e\ud835\udc52= \ud835\udc46\ud835\udc52\ud835\udc61();\n4 while BatchQueue.\ud835\udc60\ud835\udc56\ud835\udc67\ud835\udc52() > 0 do\n5\nwhile \u210e\ud835\udc4e\ud835\udc60\ud835\udc41\ud835\udc52\ud835\udc65\ud835\udc61\ud835\udc35\ud835\udc4e\ud835\udc61\ud835\udc50\u210e() and\n\ud835\udc35\ud835\udc4e\ud835\udc61\ud835\udc50\u210e\ud835\udc44\ud835\udc62\ud835\udc52\ud835\udc62\ud835\udc52.\ud835\udc60\ud835\udc56\ud835\udc67\ud835\udc52() < \ud835\udc3f\ud835\udc5c\ud835\udc5c\ud835\udc58\ud835\udc34\u210e\ud835\udc52\ud835\udc4e\ud835\udc51\ud835\udc49\ud835\udc4e\ud835\udc59\ud835\udc62\ud835\udc52do\n6\n\ud835\udc35\ud835\udc4e\ud835\udc61\ud835\udc50\u210e= \ud835\udc54\ud835\udc52\ud835\udc61\ud835\udc41\ud835\udc52\ud835\udc65\ud835\udc61\ud835\udc35\ud835\udc4e\ud835\udc61\ud835\udc50\u210e();\n7\n\ud835\udc3c\ud835\udc61\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc41\ud835\udc62\ud835\udc5a\ud835\udc4f\ud835\udc52\ud835\udc5f= \ud835\udc35\ud835\udc4e\ud835\udc61\ud835\udc50\u210e.\ud835\udc3c\ud835\udc61\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc41\ud835\udc62\ud835\udc5a;\n8\nfor EmbID \u2208\ud835\udc35\ud835\udc4e\ud835\udc61\ud835\udc50\u210e.\ud835\udc48\ud835\udc5b\ud835\udc56\ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc38\ud835\udc5a\ud835\udc4f\ud835\udc52\ud835\udc51\ud835\udc51\ud835\udc56\ud835\udc5b\ud835\udc54\ud835\udc60() do\n9\n\ud835\udc3f\ud835\udc4e\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61\ud835\udc47\ud835\udc5f\ud835\udc4e\ud835\udc50\ud835\udc58\ud835\udc52\ud835\udc5f[EmbID] = \ud835\udc3c\ud835\udc61\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc41\ud835\udc62\ud835\udc5a\ud835\udc4f\ud835\udc52\ud835\udc5f;\n10\n\ud835\udc35\ud835\udc4e\ud835\udc61\ud835\udc50\u210e\ud835\udc44\ud835\udc62\ud835\udc52\ud835\udc62\ud835\udc52.\ud835\udc5d\ud835\udc62\ud835\udc60\u210e\ud835\udc35\ud835\udc4e\ud835\udc50\ud835\udc58(\ud835\udc35\ud835\udc4e\ud835\udc61\ud835\udc50\u210e);\n11\nTTLUpdateRequests = {};\n12\n\ud835\udc36\ud835\udc4e\ud835\udc50\u210e\ud835\udc52\ud835\udc39\ud835\udc52\ud835\udc61\ud835\udc50\u210e\ud835\udc45\ud835\udc52\ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc60\ud835\udc61\ud835\udc60= {};\n13\n\ud835\udc36\ud835\udc62\ud835\udc5f\ud835\udc5f\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc35\ud835\udc4e\ud835\udc61\ud835\udc50\u210e= \ud835\udc35\ud835\udc4e\ud835\udc61\ud835\udc50\u210e\ud835\udc44\ud835\udc62\ud835\udc52\ud835\udc62\ud835\udc52.\ud835\udc5d\ud835\udc5c\ud835\udc5d\ud835\udc39\ud835\udc5f\ud835\udc5c\ud835\udc5b\ud835\udc61();\n14\nfor EmbID \u2208\ud835\udc36\ud835\udc62\ud835\udc5f\ud835\udc5f\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc35\ud835\udc4e\ud835\udc61\ud835\udc50\u210e.\ud835\udc48\ud835\udc5b\ud835\udc56\ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc38\ud835\udc5a\ud835\udc4f\ud835\udc52\ud835\udc51\ud835\udc51\ud835\udc56\ud835\udc5b\ud835\udc54\ud835\udc60() do\n15\nTTL = \ud835\udc3f\ud835\udc4e\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61\ud835\udc47\ud835\udc5f\ud835\udc4e\ud835\udc50\ud835\udc58\ud835\udc52\ud835\udc5f[EmbID];\n16\nTTLUpdateRequests.\ud835\udc4e\ud835\udc5d\ud835\udc5d\ud835\udc52\ud835\udc5b\ud835\udc51((EmbID, TTL));\n17\nif EmbID is not in \ud835\udc3c\ud835\udc5b\ud835\udc36\ud835\udc4e\ud835\udc50\u210e\ud835\udc52then\n18\n\ud835\udc36\ud835\udc4e\ud835\udc50\u210e\ud835\udc52\ud835\udc39\ud835\udc52\ud835\udc61\ud835\udc50\u210e\ud835\udc45\ud835\udc52\ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc60\ud835\udc61\ud835\udc60.\ud835\udc4e\ud835\udc5d\ud835\udc5d\ud835\udc52\ud835\udc5b\ud835\udc51(EmbID);\n19\n\ud835\udc3c\ud835\udc5b\ud835\udc36\ud835\udc4e\ud835\udc50\u210e\ud835\udc52.\ud835\udc56\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc5f\ud835\udc61(EmbID);\n20\nif TTL = \ud835\udc36\ud835\udc62\ud835\udc5f\ud835\udc5f\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc35\ud835\udc4e\ud835\udc61\ud835\udc50\u210e.\ud835\udc5b\ud835\udc62\ud835\udc5a\ud835\udc4f\ud835\udc52\ud835\udc5fthen\n21\n\ud835\udc3c\ud835\udc5b\ud835\udc36\ud835\udc4e\ud835\udc50\u210e\ud835\udc52.\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc60\ud835\udc52(EmbID);\n22\n\ud835\udc3f\ud835\udc4e\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61\ud835\udc47\ud835\udc5f\ud835\udc4e\ud835\udc50\ud835\udc58\ud835\udc52\ud835\udc5f.\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc60\ud835\udc52(EmbID);\n23\n\ud835\udc46\ud835\udc52\ud835\udc5b\ud835\udc51\ud835\udc47\ud835\udc5c\ud835\udc47\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc5f\ud835\udc60(TTLUpdateRequests);\n24\n\ud835\udc46\ud835\udc52\ud835\udc5b\ud835\udc51\ud835\udc47\ud835\udc5c\ud835\udc47\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc5f\ud835\udc60(\ud835\udc36\ud835\udc4e\ud835\udc50\u210e\ud835\udc52\ud835\udc39\ud835\udc52\ud835\udc61\ud835\udc50\u210e\ud835\udc45\ud835\udc52\ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc60\ud835\udc61\ud835\udc60);\nAlgorithm 1: Lookahead Algorithm\n3.2\nCaching and Prefetching in Bagpipe\nIn Bagpipe, we introduce the idea of each trainer having a lo-\ncal cache. When designing a system with caching, we need to\ndesign a cache insertion policy (what to cache?) and a cache\neviction policy (what to evict?) so as to maximize the hit rate.\nHowever, offline batch training of machine learning jobs like\nrecommendation model training has additional structure: in\noffline batch training future batches and their contents are\npredictable, i.e., in context of recommendation models we\ncan look beyond the current batch and infer which embed-\ndings will be accessed in future batches. This insight helps us\ncreate a perfect or oracular cache. To utilize this insight, we\ndesign a lookahead algorithm. For ease of explanation, the\ndiscussion in this section assumes there is only one trainer\n(only one cache). We extend this to the distributed setting in\n\u00a73.3.\nLookahead Algorithm. To decide what to cache and what\nto evict we develop a low overhead (benchmark in \u00a75.3)\nlookahead algorithm (Algorithm 1) which also ensures con-\nsistent access to embeddings. We denote the lookahead value\n(\u2112), as the number of batches beyond the current batch,\nwhich will be analyzed to determine what to cache, e.g., if\nthe current batch is \ud835\udc65, we consider embedding accesses in\nbatches from \ud835\udc65to \ud835\udc65+\u2112, to determine which elements in\nbatch \ud835\udc65should be cached. The lookahead algorithm takes\nthree inputs: the current batch, future batches (next \u2112num-\nber of batches), and current state of the cache on the trainer.\nThe lookahead algorithm outputs two pieces of informa-\ntion. First, for the current batch, it generates the list of em-\nbeddings that will not be found in the cache on the trainer\u2019s\nGPUs. This allows Bagpipe to prefetch these embeddings\nout of order before the current batch is used for training.\nPrefetching allows Bagpipe to hide the data access latency\nfor the long tail of embeddings that are not frequently ac-\ncessed. Second, the lookahead algorithm determines which\nembeddings from the current batch will be used in future\nbatches, and the last iteration they will be accessed in the cur-\nrent lookahead window. Any embeddings from the current\nbatch that will be used by future batches in the lookahead\nwindow will be marked for caching, so they can be accessed\nfrom the GPU memory in the future. The last iteration an\nembedding is used within the lookahead range and serves as\ntime-to-live (TTL) for the embedding in the cache.\nNext we describe an example of how lookahead algorithm\nprocesses batches (Figure 6), with lookahead value (\u2112) as 2:\n\u2022 Batch 1 Embedding 3 and 9 are in the batch. For both\nembeddings we launch prefetches. However, embedding 3,\nis accessed again, and the last occurrence in the window\nis at Batch 2, so we cache it with the TTL set to 2.\n\u2022 Batch 2 Embedding 3 is in the cache so we do not send\na prefetch request. But the last occurrence for 3 in the\nwindow is now in Batch 3. Therefore, a TTL update is sent\nfor 3. We will prefetch 4 since it is not in the cache.\n\u2022 Batch 3 We prefetch embedding 6 and cache it with a TTL\nof 4 since it will be reused in Batch 4. At this point in our\nlookahead window Embedding 3 has no future occurrence\nso it will be evicted after batch 3. However, if embedding\n3 was being used by batch 4, we would have kept it in the\ncache and sent a TTL update with eviction batch as 4.\n\u2022 Batch 4 We prefetch 1 since it is not in the cache. We do\nnot send any TTL updates for 6 as it is absent in future\nbatches and will be evicted after this batch.\nConsistency with the Lookahead algorithm. Our con-\nsistency goal is to avoid staleness and ensure that trainers\ndo not prefetch an embedding from the embedding servers\nwhile it has updates that have not yet been written back.\nDespite pre-fetching embeddings out of order, our formula-\ntion of what to cache and what to prefetch (Algorithm 1),\nprovides an extremely important guarantee that allows us to\nmaintain consistency and match the execution of synchro-\nnous training. When the trainer is processing batch number\n\ud835\udc65, an embedding used by the batch will either be available in\nthe cache with it\u2019s most recent value or no preceding batch\nin the lookahead range (any batch number in [\ud835\udc65\u2212\u2112,\ud835\udc65))\nwould have updated that specific embedding. That is, if an\nembedding was needed by a batch in batch number in range\n[\ud835\udc65\u2212\u2112,\ud835\udc65) it will be in the cache; if an embedding is not\nin the cache it means no batch in range of [\ud835\udc65\u2212\u2112,\ud835\udc65) has\nupdated it. Therefore, as long as the prefetch request for\nbatch \ud835\udc65is issued after updates from training batch number\n3\n9\n4\n3\nBatch 1\nBatch 2\nEMB ID\nTTL\u00a0\nOracle\nCacher\n3, 9\nPrefetch\u00a0\n(3,2)\n(Cache EMB ID, TTL Val)\n3\n2\n4\n3\nBatch 2\nOracle\nCacher\n4\nPrefetch\u00a0\n(3,3)\n3\n3\n3\n6\nBatch 3\n3\n6\nBatch 3\nOracle\nCacher\n6\nPrefetch\u00a0\n(6,4)\n6\n1\nBatch 4\nBatch 1\nBatch 2\nBatch 3\n6\n1\nBatch 4\nOracle\nCacher\n1\nPrefetch\u00a0\n6\n9\n7\nBatch 5\n4\nEMB ID\nTTL\u00a0\nEMB ID\nTTL\u00a0\n3\n3\n6\n4\nEMB ID\nTTL\u00a0\nBatch 4\nDynamic\nCache\nDynamic\nCache\nDynamic\nCache\nDynamic\nCache\n(Cache EMB ID, TTL Val)\n(Cache EMB ID, TTL Val)\nFigure 6. Lookahead Algorithm: The above figure shows an illustration at different batch steps of how the lookahead algorithm functions.\nIn the above example, the lookahead value is 2 and the batch size is also 2.\n\ud835\udc65\u2212\u2112have been written back, we can guarantee that we\nwill not see stale embeddings.\n3.3\nDistributed Cache Design in Bagpipe\nFirst we discuss the requirements for the distributed cache\ndesign and our goals. Next, we discuss the design space for\ncache design and finally we compare these designs both\nquantitatively and qualitatively.\nDistributed Cache Requirements. When extending the\ncaching scheme described above to a distributed setting,\nBagpipe can provide consistency as long as the following\ntwo requirements are satisfied (i) Each trainer sends prefetch\nrequests for batch number \ud835\udc65only when cache eviction and\nupdates have been performed by all the trainers on \ud835\udc65\u2212\n\u2112batch. (ii) Each trainer\u2019s cache should contain the latest\nvalue of the embedding. The first requirement is a direct\nextension of our prior discussion and can be satisfied by\nsynchronizing the iteration number that each trainer has\nprocessed. The second condition, however, creates additional\ncommunication overheads and we next discuss the design\nspace and techniques to reduce these overheads.\nGoals of distributed cache design. The primary objective\nof our distributed cache design is to minimize the time spent\non cache synchronization on the critical path. Thus our objec-\ntive includes, accounting for the number of bytes transferred\n(bandwidth) and connection overheads (latency) [61].\nNext, we explore the distributed cache design space and\ndiscuss synchronization costs with each design.\nReplicated Cache. In a replicated cache, each trainer will\npre-fetch all the embeddings which are required by the whole\nbatch (not just a worker\u2019s partition of the batch). After per-\nforming the backward pass we synchronize all the elements\nwhich have updated gradients across all the workers, such\nthat embeddings in the caches are synchronized at the end\nof each iteration using all-reduce. This trivially ensures that\neach trainer\u2019s cache has the latest version of the embedding.\nA replicated design results in high bandwidth cost due to\nsynchronization of all the elements across all trainers even if\nthe element\u2019s updated value would not be required in future\nby other trainers, i.e., it will be evicted from the cache. How-\never, there is very small control (latency) overhead because\nall elements are synchronized.\nPartitioned Cache. A partitioned cache is on the other\nend of the design spectrum where each trainer is assigned\nan exclusive portion of the cache. Before the forward pass\nof training, each trainer fetches the embeddings not avail-\nable locally from their peer trainers. Post backward pass,\neach trainer writes back the gradients to the respective peer\ntrainer which has ownership of the embedding. These steps\nare required to ensure we always use the latest version of the\nembedding. Unlike the replicated cache, where all the em-\nbeddings are synchronized irrespective of whether a trainer\nneeds it, in the case of a partitioned cache, trainers only\nfetch and write back the embeddings they utilize. Further,\npartitioned caches are more space efficient as there is only\none copy of each embedding in the distributed cache.\nThe number of bytes communicated when using a parti-\ntioned cache depends on how batches are partitioned across\ntrainers. To study the scenario where batch partitioning is\ncommunication aware, i.e., batches are partitioned so as to\nminimize bytes communicated across trainers, we formulate\na mixed integer linear program (MILP). Given the cache state\non all trainers, the MILP computes a partitioning of examples\nwhich minimizes the amount of inter-node communication.\nGiven a batch of examples (\ud835\udc4f) and \ud835\udc5dtrainers, we introduce\n\ud835\udc4f\u00d7 \ud835\udc5dvariables in our MILP. Each variable is denoted by \ud835\udc65\ud835\udc56,\ud835\udc57\nwhere if \ud835\udc65\ud835\udc56,\ud835\udc57= 1, then example \ud835\udc56will be assigned to trainer\n\ud835\udc57. We then compute a cost matrix \ud835\udc36, where given the cache\nstate, \ud835\udc36\ud835\udc56,\ud835\udc57represents the cost of inter-node communication\nthat will be required to fetch embeddings for example \ud835\udc56to\nlocation \ud835\udc57. Our objective is to minimize the amount of inter-\nnode communication. We formulate it using our variables\nand cost matrix as:\nMinimize\n\u2211\ufe01\n\ud835\udc56\u2208\ud835\udc3c\n\u2211\ufe01\n\ud835\udc57\u2208\ud835\udc3d\n\ud835\udc36\ud835\udc56,\ud835\udc57\u00b7 \ud835\udc65\ud835\udc56,\ud835\udc57\nWhere \ud835\udc3cand \ud835\udc3drepresent the set of examples and trainers\nrespectively. Further, we include two constraints to ensure\nthat the solution is feasible and avoids load imbalance:\n(i) Each example must be placed on one trainer and all\nexamples need to be placed on at least one trainer node.\n\u2200\ud835\udc56\u2208\ud835\udc3c\n\u00cd\n\ud835\udc57\u2208\ud835\udc3d\ud835\udc65\ud835\udc56,\ud835\udc57= 1. (ii) We add another constraint to\nmake sure the batch is equally distributed across machines\nto prevent load imbalance. The optimization problem can be\nsolved using existing MILP solvers like Gurobi [20].\nHowever, using communication aware partitioned caches\nhas two disadvantages: first solving the MILP takes around\n2.36s on a 16-core machine making it infeasible when itera-\ntion times are around 100ms (Figure 2). Secondly, sync time\ndoes not solely depend on bytes communicated, as overheads\nReplicated\nPartitioned\n Random\nPartitioned \nCommunication Aware\nLRPP\n0\n50\n100\n150\n200\n250\n300\nTime Per Iteration (ms)\nFigure 7. Comparing cache\ndesigns:We observe that LRPP\nprovides\nbest\nperformance\namong all other cache options.\nDLRM\nDeepFM\n0\n10\n20\n30\n40\n50\n60\nCache Sync Time (ms)\nImmediate Synchronization\nDelayed Synchronization\nFigure 8. Effect of Delayed\nSynchronization:Delayed\nSync can reduce time for cache\nsynchronization by up to 44%.\nfrom maintaining data-structures and establishing connec-\ntions also play a role. With partitioned caches we would\nneed to introduce additional data structures to keep track of\nembedding locations and establish multiple connections.\nLogically Replicate Physically Partitioned Cache. Ide-\nally, we would like to design a cache that does not perform\nunnecessary synchronization of embeddings but does not\nintroduce additional overheads due to state tracking. To\nachieve this goal, we propose using Logically Replicated,\nPhysically Partitioned (LRPP) caches. By logically replicated\nwe mean that from the view of Oracle Cacher all caches\nhave all data and are fully replicated but by being physi-\ncally partitioned, the trainers decide which elements need\nsynchronization and which elements can be evicted without\nsynchronization. The primary insight behind our idea comes\nfrom the observation that for the Criteo dataset, around 25%\nof the embeddings are used by only one of the examples in\nbatch. Therefore, these embeddings are updated at only one\ntrainer before being evicted. Thus, fetching or synchronizing\nthem across all trainers is a waste of network bandwidth.\nWe design a new protocol that modifies the replicated cache\nbased on this insight.\nWith LRPP caches, the Oracle Cacher marks embeddings\nwhich are only used by a single trainer. Given this metadata,\nthese embeddings are only fetched by the trainer which\nneeds them and are ignored by other trainers. After the\nforward and backward pass completes, the trainers skip syn-\nchronization for these embeddings and use all-reduce to\nsynchronize the other embeddings. In the background, the\ntrainer which made the only update to the marked embed-\nding evicts it back to the Embedding Server. This optimiza-\ntion is able to reduce the volume of embeddings prefetched\nand synchronize with very minimal control logic. LRPP can\nbe further extended with more fine-grained partitioning,\ni.e., we can synchronize embeddings updated by two work-\ners using a separate communication group containing just\nthose workers. However, further fine-grained partitioning\nwill create additional control logic, which in turn would add\nadditional latency and thus yield diminishing returns.\nThere are parallels between design of LRPP to [71] a con-\ncurrent work which only caches elements which are going to\nbe utilized more than once with a FIFO eviction policy. This\nis analogous to LRPP only synchronizing elements which\nare going to be used in future. We plan to study extensions\nof LRPP in future work.\nComparing cache design choices. For Kaggle critieo dataset\nwith batch size 16,384 and 8 trainer machines (p3.2xlarge)\nwe observe that Replicated Cache communicates around\n65K embeddings per iteration, while Communication Aware-\nPartitioned Cache communicates 21K embeddings per itera-\ntion and LRPP communicates around 48K embeddings. Fur-\nther, we implement all these in Bagpipe and evaluate them\nin terms of per-iteration training time using the same setup.\nTo consider the best case scenario for partitioned caches, we\nignore the time taken by the Gurboi solver. In Figure 7, we\nobserve that LRPP outperforms replicated by 22.8% and com-\nmunication aware partitioned by 59.8%. Our analysis shows\nthat despite synchronizing fewer embeddings partitioned\ncaches do not perform well due to hotspots and additional\ncontrol logic. Since some embeddings are accessed extremely\nfrequently, the trainers that own those embeddings become\na bottleneck. Further, in partitioned caches, the overhead of\nperforming multiple collective communication calls, creating\nmemory buffers for each collective communication call and\ntracking which peer to access embeddings from, leads to an\nadditional overhead of 80-90ms for a batch size of 16K with 8\ntrainers. Therefore, we configure Bagpipe to use LRPP cache\nsynchronization scheme due to it\u2019s superior performance.\nDelayed Synchronization. To further optimize the LRPP\nprotocol we use Critical Path Analysis [70]. In this scenario,\nCPA implies that as long as the embeddings are synchronized\nbefore being critically required it can suffice. However, di-\nrectly using CPA in context of embedding synchronizations\nfor recommendation models will lead to network contention\nwith other competing synchronizations. Therefore, we intro-\nduce delayed synchronization, where we only synchronize\nthe embeddings which will be required in the next iteration\non the critical path. The embeddings which are not needed\nimmediately are synchronized in the background. To, avoid\nnetwork contention due to background synchronization we\nensure that all background synchronizations are completed\nbefore we launch other critical path synchronizations for\nfuture iterations. On Kaggle Criteo dataset with 8 trainers\nand batch size 16K, we see that only 22.7K embeddings out\nof 48K embeddings (47.3%) need to be synchronized on the\ncritical path, the rest can be overlapped with the forward\npass of the next iteration. For two models on Criteo dataset,\nFigure 8 shows that delayed synchronization can further\nreduce cache synchronization time by up to 44% (in addition\nto LRPP) by overlapping synchronization with forward pass.\nWe also observe that LRPP and delayed synchornization\ncan together reduce bytes communicated on critical path by\naround 70%.\n3.4\nDisaggregated Design and Fault Tolerance\nExisting recommendation model training systems [47, 51, 55]\ncouple storage and compute resources, i.e., it is not possible\nto scale the number of embedding table partitions without\nincreasing the number of trainers. This affects fault tolerance\nand resource utilization. For fault tolerance, given the ex-\ntremely large embedding table sizes, checkpointing a trainer\ncan take several minutes [13] during which the compute\nresources stay idle. The lack of disaggregation also leads to\npoor resource utilization [5, 11], e.g., when embedding tables\nare extremely large but the dense neural network parameters\nare small, an optimal configuration would be to use more\nservers for embedding tables but have fewer trainers. Thus,\nwe design a disaggregated architecture for Bagpipe (Figure 5)\nwith four major components: (i) Data Processors (ii) Oracle\nCacher (iii) Distributed Trainer (iv) Embedding Servers.\nData Processor. Data processors read and batch training\ndata which is resource intensive. Similar to prior designs [73]\nwe offload data processing to reduce trainer overheads. Data\nprocessors are stateless and can be restarted on failure.\nOracle Cacher. Oracle Cacher is a centralized service that\ninspects all the training batches using the lookahead al-\ngorithm (Algorithm 1). Oracle Cacher decides which ele-\nments to prefetch for the current batch and the TTL for\neviction of elements being cached. Oracle Cacher sends the\ntraining data as well as the embedding ids that need to be\ncached/prefetched using async RPC calls to the trainers. Or-\nacle Cacher is designed such that all the necessary internal\nstate is also present on the trainers. Therefore, whenever\nOracle Cacher has to be restarted we only need to fetch\nthe last iteration number processed by the trainers and the\nembedding IDs present on them.\nTrainer. Trainers hold the dense neural network portion\nof the recommendation model and the LRPP cache in the\nGPU memory. The trainers perform forward and backward\npasses in a synchronous fashion. Trainers also: (i) prefetch\nthe embeddings based on requests sent by Oracle Cacher (ii)\nperform cache maintenance including addition and eviction\nof embeddings. When a trainer fails, Oracle Cacher makes\nan RPC call to ask existing trainers to checkpoint their state\n(model parameters and cache contents) and then copies this\nstate to the newly started trainer. Each of the trainers then\ndiscard their gradients and Oracle Cacher starts from the\nprevious iteration. With delayed synchronization and LRPP\nenabled we might loose updates of at most one iteration,\nwhich is unlikely to affect model convergence [58].\nEmbedding Server. Embedding servers store all the embed-\nding tables and act as a sharded parameter server, handling\nthe prefetch and update requests from the trainers. We use\nthe techniques presented in prior work [13] to checkpoint\nembedding servers periodically.\n3.5\nDiscussion\nNext, we discuss some benefits and limitations of our design.\nGeneralizing across skew patterns. Unlike prior work [4],\nBagpipe\u2019s optimizations are resistant to embedding access\nskew changes (evaluated in \u00a75.3). This is because Bagpipe\ndoes not just rely on caching of a fixed set of hot embeddings,\nit speeds up access to cold embeddings using pre-fetching.\nSo if there exists datasets that do not display a high degree\nof skew, Bagpipe will still outperform prior work.\nApplicability in online training. Bagpipe\u2019s optimizations\nare applicable to offline setup, as it relies on the ability to look\nat future batches to build a cache. In case of online training\nexamples arrive sporadically thus restricting lookahead.\nScalability of Oracle Cacher. The overhead of Oracle\nCacher is extremely small even for extremely large batch\nsizes and lookahead values. In \u00a75.3 we find that Oracle Cacher,\neven for large batch size of 131K, can dispatch 3.27 Million\nsamples per second. Further, Oracle Cacher only needs to be\nfaster than the time taken by trainers for the forward and\nbackward pass. However, if required, Oracle Cacher can be\npartitioned to increase scalability for datasets with a large\nnumber of embedding tables. To split the work done by Ora-\ncle Cacher, we can partition the embedding tables such that\neach partition of the Oracle Cacher can work on a different\nembedding table. For instance, if there are 1000 categorical\nfeatures and we launch 10 Oracle Cacher; for each example,\neach Oracle Cacher generates caching decisions for their\nsubset of 100 categorical features.\n4\nImplementation\nBagpipe is implemented in around 5000 lines of Python.\nAsync RPC\u2019s are used to communicate across different com-\nponents. For synchronization of dense parameters and caches\nwe use collective communication primitives present in NCCL\n[1]. Bagpipe is completely integrated with PyTorch and exist-\ning model training code can use it with 4 to 5 lines of changes.\nAPI details will be present in our open source version.\nOverlapping cache management with training. We per-\nform, all cache management operations in a separate thread\nthus not affecting the training process. Our caching data\nstructure can operate completely lock free, because in our\nOracle Cacher\u2019s lookahead formulation, we guarantee that\nthe training thread and cache maintenance thread will oper-\nate on completely separate indices of the cache. This ensures\nthat cache management has minimal overhead on training.\nAutomatically Calculating Lookahead. Bagpipe uses\ntwo configuration parameters: max cache size and lookahead\nvalue (\u2112). Providing the max cache size is mandatory, it\ncan be determined by computing amount of free memory\navailable after allocating space for the dense neural network\nparameters. \u2112can be automatically calculated if it is missing.\nTo calculate \u2112, at startup Bagpipe keeps prefetching until\nit detects the cache is full. On detecting that the cache is\nfull, Bagpipe selects the number of batches prefetched so\nfar as the \u2112. Further, Bagpipe can also handle scenarios\nwhere the configuration variables are incompatible. Since\nOracle Cacher always has a consistent view of the cache, if\nit observes that the cache is going to be full it can reduce the\n\u2112. We perform a sensitivity analysis on \u2112in \u00a75.3.\n5\nEvaluation\nWe evaluate Bagpipe by measuring improvements in per\niteration time against four baselines, observing a speedup\nof 2.1\u00d7 to 5.6\u00d7 for the DLRM model. Further, we vary the\nrecommendation model architecture and compare Bagpipe\nagainst the best-performing baseline with four different mod-\nels and observe a speedup of up to 3.7\u00d7. We also analyze the\nperformance of Bagpipe on different hardware and datasets\nand evaluate other aspects of Bagpipe like fault tolerance\n(\u00a75.2) and sensitivity to configuration parameters (\u00a75.3).\nBaseline Systems. To compare Bagpipe we use four open\nsource baselines discussed in \u00a72.2. We compare Bagpipe with\nFAE [4], FB-Research\u2019s training system [55], TorchRec [47]\nand HET [48]. We discuss additional details of these systems\nwhen comparing them with Bagpipe in \u00a75.1.\nModels and Datasets. We use four different recommenda-\ntion models, Facebook\u2019s DLRM [53], Google\u2019s Wide&Deep [9],\nDeep&Cross Networks [65], and Huawei\u2019s DeepFM [16]. Ta-\nble 2 describes the models used to evaluate Bagpipe. The\nmodels differ markedly in terms of the dense parameters,\ne.g., the largest model has 33.8 Million parameters while\nthe smallest one only has 136K parameters. For datasets,\nwe use the Kaggle Criteo [34], Avazu [27] and Criteo Ter-\nabyte dataset [35] (largest publicly available dataset). Table 1\ndescribes the embedding table size for each dataset.\nCluster Setup. We run all our experiments on Amazon\nWeb Services(AWS). For trainers, we use p3.2xlarge instances\nwhile Embedding Server and Oracle Cacher run on a c5.18xlarge\ninstance each. Each p3.2xlarge instance contains a Nvidia\nV100 GPU, 8 CPU cores and 64 GB of memory with inter-\nnode bandwidth of up to 10 Gbps. Each c5.18xlarge has 72\nCPU cores and 144 GB of memory. For Bagpipe we launched\ndataloaders on the same c5.18xlarge as Oracle Cacher since\nthe machine had ample compute. To study the performance\nof Bagpipe in a setting with different amounts of compute\nand bandwidth we also run some experiments on g5.8xlarge\nwhere each machine has an Nvidia A10G GPU,32 CPU cores\nwith inter-node bandwidth of 25 Gbps.\nBagpipe Configuration. Unless otherwise stated, for all\nour experiments we set the cache size to enable lookahead of\nup to 200 batches. We study the sensitivity of these parame-\nters and their effect on throughput in \u00a75.3. We run all our\nexperiments for 2000 iterations, which roughly translates to\n1 epoch of Criteo Kaggle Dataset with batch size of 16,384.\nMetrics. For all our experiments we plot average per-iteration\ntime with error bars representing standard deviation. This\ndirectly translates to the time taken to train a fixed number\nof epochs. As Bagpipe guarantees consistent access to em-\nbeddings, the accuracy after each iteration exactly matches\nother synchronous training baselines (validated in \u00a75.1).\n5.1\nComparing Bagpipe\nWe first evaluate Bagpipe by comparing it against a number\nof existing systems and study how our benefits change as\nwe vary the models, datasets, and hardware.\nComparing Bagpipe with existing systems. In Figure 9,\nwe compare Bagpipe with four existing systems, FAE [4], FB-\nResearch training system [55], TorchRec [47] and HET [48].\nWe use Criteo Kaggle dataset with batch size 16,384 (a com-\nmon batch size among MLPerf [46] entries) and two popular\nrecommendation models DLRM [53] and W&D [9].\nFAE performs pre-processing on training data to classify\nembeddings as either hot or cold. To evaluate the best case\nscenario for FAE, we do not account for the additional time\nFAE spends in partitioning batches and deciding the place-\nment of embeddings. As shown in Figure 9, Bagpipe achieves\n3.4\u00d7 speedups for the DLRM model and 3.7\u00d7 speedups for\nW&D. As discussed in \u00a72.2, during hot batch training FAE\nhas similar cache synchronization overheads as Bagpipe,\nbut when it switches to cold batches, it suffers additional\nembedding access overheads due to no prefetching.\nNext, we compare Bagpipe with open source FB-Research\ntraining system [55], built over PyTorch and Caffe-2, is de-\nsigned for DLRM models [53] but can be easily modified to\nsupport other embedding-based deep recommendation mod-\nels like W&D [9]. Bagpipe provides 5.6\u00d7 and 4.2\u00d7 speedups\nover FB-Research training system. FB-Research system is\nslow due to spending almost 60% of the time on data loading,\nwhich has also been observed by prior works [56, 73] as well,\nleading to worse throughput compared to Bagpipe which\noffloads data-preprocessing to remote machines.\nWhen compared against TorchRec, a recent open source\nsystem built over PyTorch [41] and FBGEMM [57] to facili-\ntate training of recommendation of models, we observe that\nBagpipe is around 2.1\u00d7 faster for DLRM models and around\n1.3\u00d7 faster for W&D models. Unlike Bagpipe, TorchRec\ndoes not perform any caching or pre-fetching, and therefore\nfetches and writes back a large number of embeddings on\nthe critical path. Bagpipe reduces and overlaps the amount\nof embedding-related communication on the critical path.\nTo compare with HET [48], a system that performs bounded\nasynchronous training as described in \u00a72.2, we use the author-\nprovided code implemented in C++ with Python bindings\nto evaluate HET and set the asynchrony bound to 100, as\nsuggested by the authors for maximum speedup. We find\nthat Bagpipe is around 2.3\u00d7 faster than HET for DLRM and\n1.6\u00d7 faster for W&D. We observe that, despite performing\nasynchronous training, HET needs to fetch embeddings that\nare not available in the local cache from the parameter server\non the critical path. With increase in batch size the number\nof cache misses increases as well, due to the long tail of\naccesses (discussed in \u00a72.3). We also verify that our perfor-\nmance closely matches with those reported in the paper [48].\nBagpipe\nFAE\nFB-Research TorchRec\nHET\n0\n50\n100\n150\n200\n250\n300\n350\n400\nPer Iteration Time (ms)\nDLRM\nW&D\nFigure 9. Compare Bagpipe with Ex-\nisting Systems: We compare per iter-\nation time of Bagpipe against existing\nFAE [4], FB-Research training system [55],\nTorchRec [47] and HET [48]. Bagpipe pro-\nvides speedups betwen 1.2\u00d7 and 5.6\u00d7.\nDLRM\nW&D\nD&C\nDeepFM\n0\n200\n400\n600\n800\n1000\nPer Iteration Time (ms)\nBagpipe\nTorchRec\nFigure 10. Compare Bagpipe with\ndifferent models: We compare Bagpipe\nand TorchRec on four different models,\nDLRM [53], W&D [9], D&C [65], and\nDeepFM [16]. We observe speedups be-\ntween 1.2\u00d7 and 3.7\u00d7.\np3.2xlarge\ng5.8xlarge\n0\n200\n400\n600\n800\n1000\nPer Iteration Time (ms)\nDLRM-Bagpipe\nDLRM-TorchRec\nDeepFM-Bagpipe\nDeepFM-TorchRec\nFigure 11. Compare Bagpipe on dif-\nferent Hardware: Speedup provided by\nBagpipe over TorchRec on p3.2xlarge de-\ncreases from 3.7\u00d7 to 2.5\u00d7 on g5.8xlarge\n(high bandwidth) depicting that TorchRec\nis more constrained by bandwidth.\nCriteo Kaggle\nAvazu\nCriteo Terabyte\n0\n25\n50\n75\n100\n125\n150\n175\nPer Iteration Time (ms)\nBagpipe\nTorchRec\nFigure 12. Compare with Different\nDatasets: Bagpipe consistently provides\nspeedups between 1.9\u00d7 to 2.4\u00d7 across\ndatasets.\n0\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\n0.55\n0.60\n0.65\n0.70\n0.75\nBagpipe\nTorchRec\nFigure 13. Loss convergence for Bag-\npipe and TorchRec: Convergence of\nBagpipe and TorchRec is very similar,\nwith slight differences due to random ini-\ntialization.\n0\n200\n400\n600\n800\nTime(s)\n0\n20000\n40000\n60000\n80000\n100000\nThroughput (Examples/sec)\nNew Machine is available\nTrainer shut down signal\nThroughput Bagpipe\nThroughput FB-Research\nFigure 14. Recovery from trainer fail-\nure: Bagpipe requires less than 60 seconds\nto recover from a trainer failure compared\nto 13 minutes for FB-Research System.\nForward-Bagpipe\nForward-Ideal\nBackward+MLPsync-Bagpipe\nBackward+MLPsync-Ideal\nGet-Embedding Bagpipe\nGet-Embedding Ideal\nCache Synchronization\nBagpipe\nIdeal\n0\n10\n20\n30\n40\n50\n60\nPer Iteration Time (ms)\n(a) DLRM: p3.2xlarge\nBagpipe\nIdeal\n0\n50\n100\n150\n200\n250\nPer Iteration Time (ms)\n(b) DeepFM: p3.2xlarge\nBagpipe\nIdeal\n0\n5\n10\n15\n20\n25\n30\nPer Iteration Time (ms)\n(c) DLRM: g5.8xlarge\nBagpipe\nIdeal\n0\n20\n40\n60\n80\n100\n120\nPer Iteration Time (ms)\n(d) DeepFM: g5.8xlarge\nFigure 15. Comparing with Ideal: Comparing Bagpipe with an ideal system which has no overhead for embedding fetch, we observe\nthat system comes within 10% of time per iteration for large models where there is potential to overlap embedding accesses.\n4-trainer\nBS-8192\n8-trainer\nBS-16384\n16-trainer\nBS-32768\n32-trainer\nBS-65536\n0\n50\n100\n150\n200\nPer Iteration Time (ms)\n(a) Increasing Trainers\n16384\n32768\n65536\n131072\nBatch Size\n0\n25\n50\n75\n100\n125\n150\n175\n200\nPer Iteration Time (ms)\n(b) Increasing Batch Size\nFigure 16. Scalability of Bagpipe: (left) we increase the number\nof trainers such that batch size per machine is constant; Bagpipe\nprovides sublinear scalability due to increasing communication\nbottlenecks. (right) Increasing batch size with 8 trainers results in\nbetter throughput as we are able to better overlap communication.\nAs the speedup of Bagpipe varies across models, we per-\nform a detailed investigation to understand this. We observe\nTorchRec to be the best performing baseline as it efficiently\noverlaps different parts of the training pipeline and make bet-\nter use of network bandwidth using the all2all primitive\nfor embedding fetches. Thus, for the next set of experiments\nwe compare Bagpipe with TorchRec.\nComparing Bagpipe on other models. In addition to\nW&D and DLRM, we also train the Deep&Cross Network\n(D&C) [65] and DeepFM models [16] with Bagpipe and\nTorchRec. D&C models contain an additional Cross Net-\nwork component, which performs explicit feature crossing\nof sparse features to learn predictive features of bounded\ndegree without manual feature engineering. DeepFM intro-\nduces a factorization module that learns up to 2-order fea-\nture interactions between sparse and dense features. Details\nof these models are available in Table 2. In Figure 10 we\nobserve that performance gains provided by Bagpipe over\nTorchRec depends on the size and computation require-\nments of the dense portion of recommendation models, e.g.,\nfor W&D which only has around 131,000 dense parame-\nters we observe that Bagpipe provides only a 1.2\u00d7 speedup,\nwhile for DeepFM which has 33.8 million parameters Bag-\npipe provides a speedup of over 3.7\u00d7. We believe that this\nis due to the pipelining mechanism present in TorchRec\nwhere the authors overlap the embedding write-back with\nthe synchronization of the dense model. As the model size\nincreases, the bandwidth requirement for synchronization\nalso increases and the synchronization of dense model and\nembedding write-backs ends up competing for the same\nset of network resources. Meanwhile, Bagpipe significantly\nreduces the amount of embedding synchronization due to\ncaching. Further, delayed synchronization allows Bagpipe\noverlap forward pass of the next iteration. Thus, our analysis\nindicates that TorchRec, unlike Bagpipe, is heavily bottle-\nnecked by the network bandwidth available. To verify this,\nwe next run experiments on a different hardware.\nComparing Bagpipe on different hardware. To under-\nstand the performance of Bagpipe on different hardware se-\ntups, especially in terms of network bandwidth, we evaluate\nBagpipe on g5.8xlarge (A10G GPU and 25 Gbps bandwidth)\ninstances. In terms of compute, A10G performs similar to\nV100, but the bandwidth on g5.8xlarge is 25 Gbps compared\nto 10 Gbps on p3.2xlarge. We use the same hyper-parameters\nand model configurations as in previous sections. Figure 11\nshows a comparison of per-iteration time between DLRM and\nDeepFM, for both Bagpipe and TorchRec. We observe that\nBagpipe with DLRM model on g5.8xlarge trainers is around\n1.9\u00d7 faster than \ud835\udc5d3.2\ud835\udc65\ud835\udc59\ud835\udc4e\ud835\udc5f\ud835\udc54\ud835\udc52trainers. On other hand, the\nDLRM model with TorchRec on g5.8xlarge trainers is around\n2.4\u00d7 faster than p3.2xlarge trainers. Similarly for DeepFM,\ntime for TorchRec reduces by 2.4\u00d7 (1015ms to 414ms) when\nwe switch from p3.2xlarge instances to g5.8xlarge. This con-\nfirms our hypothesis that for larger models TorchRec is\nbounded by bandwidth, while Bagpipe, because of caching\nand efficient pipelining of communication, makes better use\nof network resources. However, it is unclear yet, what frac-\ntion of the iteration time in Bagpipe is spent on network-\nbound embedding access and to understand this, we next\ncompare Bagpipe to an ideal system which has no overhead\nof embedding accesses.\nComparing Bagpipe with an ideal system. Comparing\nBagpipe to an ideal system will show how far Bagpipe is\nfrom completely alleviating embedding access overheads. To\ncreate such an ideal system, we prefetch all necessary embed-\ndings to the GPU memory before starting training and switch\noff prefetch, cache sync, and cache eviction modules of Bag-\npipe. In Figure 15, we perform this comparison for DLRM and\nDeepFM models on both p3.2xlarge and g5.8xlarge. DLRM\nmodel on p3.2xlarge instance on ideal system takes around\n30ms while, Bagpipe takes around 56ms. DLRM model on\ng5.8xlarge on the ideal system takes around 19ms while Bag-\npipe takes around 30ms. This shows that at lower bandwidths\nfor DLRM, there are periods in the pipeline when model train-\ning is blocked on embedding operations. We also study the\nsame effect with DeepFM, a larger model that provides more\nopportunities for Bagpipe to overlap embedding-related op-\nerations. For DeepFM we observe that on p3.2xlarge instances\nideal takes around 236ms while Bagpipe takes around 253ms\n(overhead 17ms). On the high bandwidth g5.8xlarge instance\n30\n40\n50\n60\n70\n80\n90 100\nNumber of Features\n0\n50\n100\n150\n200\nPer Iteration Time (ms)\nOracle Cacher\nTrainer\nBatch Size-16384, Lookahead Value-200\n(a) Categorical Features\n16384\n32768\n65536\n131072\nBatch Size\n0\n25\n50\n75\n100\n125\n150\n175\n200\nPer Iteration Time (ms)\nOracle Cacher\nTrainer\nLookahead Value-200, Num Features-26\n(b) Batch Size\nFigure 17. Latency of Oracle Cacher: We observe that overall\nOracle Cacher scales very well, it increases sub-linearly with the\nincrease in the number of features and batch size. However, training\ntime will always hide the latency of Oracle Cacher.\nideal system takes around 116ms while Bagpipe takes around\n128ms (overhead of 12ms). These results indicate that Bag-\npipe gets within 10% of an ideal system with deeper models\nand has almost constant overhead for providing embeddings\n(around 12 to 20 ms) even at lower bandwidths.\nComparison on Different Datasets. We also analyse per-\nformance of Bagpipe on Avazu [27] and Criteo Terabyte [35]\n(largest publicly available dataset). Using eight p3.2xlarge\ninstances as trainers, in Figure 12 we see that compared to\nTorchRec on DLRM model, Bagpipe is 1.9\u00d7 to 2.4\u00d7 faster.\nThis shows that irrespective of the dataset Bagpipe provides\na significant speedup over the best-performing baseline.\nConvergence Comparison. Since Bagpipe ensures that em-\nbedding reads are not stale, it should have the same conver-\ngence properties as synchronous training using TorchRec.\nWe verify this in Figure 13 where we see that Bagpipe\u2019s con-\nvergence is very close to TorchRec with minor differences\narising from random initialization. For HET we observed\nthat the convergence depends on the model complexity; for\nDLRM, HET\u2019s open source code [23] did not converge to\nthe same loss as TorchRec, while we observed similar con-\nvergence as TorchRec for W&D, a smaller model (We have\nreported this issue to the HET authors). Overall, we see that\nBagpipe retains the convergence of synchronous training\nwhile providing per-iteration speedups.\n5.2\nScalability and Fault Tolerance\nScalability. In Figure 16a, we scale batch size and number\nof machines (up to 32 GPUs and batch size of 65,536). With\nincrease in batch size the number of embeddings to fetch and\nsynchronize increases. Despite increase in communication,\nBagpipe scales around 1.4\u00d7 for 2\u00d7 increase in resources and\nwork (320K samples/sec for 16 trainers vs 446K samples/sec\nfor 32 trainers). In Figure 16b we scale just batch size, we ob-\nserve that batch size 65,536 takes a very similar time as batch\nsize 131,072. Because with a higher batch size Bagpipe is\nable to overlap a bigger proportion of cache synchronization\nwith the longer forward pass.\nFault Tolerance. In Figure 14 we observe that trainer in Bag-\npipe recover in less than a minute, compared to FB-research\nsystem which is close to 13 minutes. For the FB-Research sys-\ntem we make a best-case assumption that the framework can\nTable 3. Effect of increasing \u2112: With increase in \u2112the cache\nsize required increases but improves throughput till \u2112of 100.\nLookahead\nCache Size (MB)\nAvg Time per Iteration (ms)\n5\n39.6\n535.6\n10\n66.7\n289.3\n50\n235.2\n85.7\n100\n410.5\n67.3\n200\n720.6\n65.1\n300\n1003.3\n65.6\ncheckpoint the iteration just before failure, to avoid check-\npointing at every iteration. Even in this case, FB-Research\nsystem takes around 13 minutes to recover since the amount\nof state on each trainer includes a large shard of the embed-\ndings. Meanwhile, trainers in Bagpipe are able to recover in\nless than a minute and do not require checkpointing at every\niteration (\u00a73.4). Other systems do not discuss fault tolerance.\n5.3\nSensitivity Analysis of Bagpipe\nNext, we study the performance of Bagpipe with different\nconfigurations and also micro-benchmark components of\nBagpipe. Unless stated otherwise, we use the same setup\ndescribed \u00a75, with a batch size of 16,384 on Criteo Kaggle\ndataset.\nOverhead of Oracle Cacher. In Figure 17a, 17b we ob-\nserve Oracle Cacher\u2019s overhead increases sub-linearly with\nincrease in categorical features and batch size. However, the\ntime per iteration is still significantly higher than the time\ntaken to perform lookahead by Oracle Cacher. Since Oracle\nCacher is overlaped with training, it only becomes a bottle-\nneck if it\u2019s time exceeds that of trainer. Overall, we find that\nOracle Cacher can almost dispatch 3.27 Million examples per\nsecond. We find that this is sufficient to power the most opti-\nmized systems reported in prior work (e.g., 8 ZionEx nodes\n(128 A100 GPUs) processing up to 1.6 Million samples per\nsecond [51]). We benchmarked Oracle Cacher for other pa-\nrameters like different \u2112and observe constant throughput,\ni.e., complexity of Oracle Cacher does not depend on \u2112.\nEffect of \u2112. In Table 3 we study how cache size required and\nthroughout changes for different \u2112. As \u2112increases, cache\nsize required increases sub-linearly. This sub-linear behavior\ndue to reuse of embeddings found in the previous batches\nduring lookahead process. We also observe that throughput\nbenefits from increasing \u2112start plateauing beyond 100.\nThis is because as the lookahead value goes over 200, we are\nkeeping all the popular elements in the cache, and increasing\n\u2112at this point does not affect communication much.\nEffect of Access Pattern Skew. Unlike some prior sys-\ntems [4], Bagpipe is designed to handle skew pattern changes.\nTo study performance of Bagpipe when the skew pattern\nchanges, we create an artificial dataset similar to Criteo Kag-\ngle dataset with the same number of features and samples\nbut with different skew patterns. We choose top 1% of em-\nbeddings and then create an exponential function such that\ncumulative probability of sampling from top 1% embeddings\nis equivalent to the chosen skew, e.g., top 1% of embeddings\nare responsible for 40% accesses. The remaining embeddings\n90%\n40%\n20%\n10%\n1%\n% of access by top 1% embeddings\n0\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nPer Iteration Time (ms)\n60.9\n61.8\n63.1\n63.7\n69.7\nBagpipe\nFAE\nFigure 18. Effect of change in skew: Comparing when 1% of\nembeddings perform 90% of embedding accesses to just 1% of em-\nbedding access (no skew). Unlike FAE, Bagpipe\u2019s time only increases\nfrom 60.9ms to 69.7ms showing resistance to change in skew.\n1\n2\n3\n4\n5\nZipf Parameter\n0\n100\n200\n300\n400\n500\n600\n700\n800\nPer Iteration Time (ms)\n64.4\n64.1\n63.4\n61.4\n61.1\nBagpipe\nFAE\nFigure 19. Effect of change in skew using Zipf Distribution:\nVarying the \ud835\udefcparameter in Zipf distribution; a higher \ud835\udefcindicates\nhigher skew. Even with drastic increase in the skew, the time taken\nby Bagpipe remains almost constant.\nare sampled uniformly such that they lead to the remaining\n60% of accesses. In Figure 18 we study how the iteration\ntime changes as the embedding reuse of top 1% embeddings\nchanges between 90% and 1%; e.g., the 40% bar reflects the\nruntime when 1% of embeddings are reused 40% of the time.\nWe observe that due to optimizations present in Bagpipe\nlike pre-fetching, LRPP and delayed synchronization even\nwhen the degree of skew changes from 90% skew to no skew,\nBagpipe\u2019s per iteration time changes at most by 13%. On the\nother hand, FAE [4], which relies only on caching degrades\nby 7.2\u00d7. Next we vary the skew of embedding accesses using\nthe popular Zipf [31] distribution. The \ud835\udefcparameter in Zipf\ndistribution determines the skew, with a higher \ud835\udefcdenoting\nhigher skew. In Figure 19 we observe that even with a large\nchange in skew (varying Zipf\u2019s parameter between 1 and\n5), Bagpipe\u2019s throughput does not vary significantly. This\nshows Bagpipe is resistant to changes in skew.\n6\nRelated Work\nMultiple systems have been developed for training large\nDNN\u2019s. Systems like PipeDream [52] and Gpipe [24] intro-\nduce pipeline parallelism to enable efficient model paral-\nlel training. These systems are designed for dense training,\nwhile recommendation models have sparse access patterns,\nalso requiring a hybrid data and model-parallel setup which\nis not supported in these systems.\nPrior domain specific systems like Marius [49] and P3 [15]\nfor Graph Neural Network training, Oort [36] and FedML [21]\nfor federated learning do not support offline recommenda-\ntion model training.\nPipeswitch [6] and SwitchFlow [69] build mechanisms\nfor fast preemption to schedule training and inference jobs\nto improve resource utilization. [6] relies on the idea that\nmodel weights have a deterministic access pattern (a simple\nexample for PipeSwitch is that Layer 1 will be accessed before\nLayer 2), and uses this to start computation before moving\nthe full model to the GPU. Unlike Bagpipe, Pipeswitch does\nnot handle cases where the model weights (embeddings) are\naccessed dynamically depending on the training data.\nFor offline training of recommendation models several\nprior systems like FAE [4], TorchRec [47], HET [48] have\nbeen designed. We have compared to these systems in the \u00a75\nand discussed how Bagpipe is different. Prior work [2, 45]\nhas also proposed system designs to enable disaggregated\ntraining of recommendation models. Unlike Bagpipe, prior\nsystems (e.g., Monolith [45]) suffer from embedding access\noverheads. There have been other systems like cDLRM [7]\nand ScratchPipe [33] which have used the idea of looka-\nhead to provide access to embeddings. However, these sys-\ntems are primarily designed for single node training. Scaling\nthese systems to multi-node setting is non-trivial [33] or\nwill impose a restriction on users to use specific optimiz-\ners like [7]. Systems like cDLRM [7] only allow embedding\naveraging rather than gradient averaging, which restricts\nusers from using optimizers like Adam [30] and SGD with\nMomentum [54] thereby potentially harming convergence\nand changing the underlying training algorithm. Other ap-\nproaches like TTRec [72] and [19] improve performance\nusing approximations like tensor compression and gradient\ncompression, which unlike Bagpipe, change the training\nalgorithm and can lead to accuracy loss.\n7\nConclusion\nWe presented Bagpipe, a new system that can accelerate\nthe training of deep learning based recommendation mod-\nels. Our gains are derived from better resource utilization\nand by overlapping computation with data movement. Our\ndisaggregated architecture also allows independent scaling\nof resources and better fault tolerance, while retaining syn-\nchronous training semantics. Our experiments show that\nBagpipe provides an end-to-end speedup of up to 5.6\u00d7 over\nstate-of-the-art baselines.\nAcknowledgements We would like to thank Carole-Jean\nWu for introducing us to deep learning recommendation\nmodels and the challenges in scaling their training. We would\nalso like to thank Yibo Zhu for early discussion about our\nwork. Finally, we would like to thank our shepherd, Junfeng\nYang, the anonymous SOSP reviewers, Bilge Acun-Uyan and\nMuhammad Adnan for their invaluable feedback that helped\nin making this work better. This research was supported in\npart by NSF-CAREER grant CNS-2237306.\nReferences\n[1] Massively scale your deep learning training with nccl 2.4. https://bit.\nly/341nGfs. Accessed: August 31, 2023.\n[2] Bilge Acun, Matthew Murphy, Xiaodong Wang, Jade Nie, Carole-Jean\nWu, and Kim Hazelwood. Understanding training efficiency of deep\nlearning recommendation models at scale. In 2021 IEEE International\nSymposium on High-Performance Computer Architecture (HPCA), pages\n802\u2013814. IEEE, 2021.\n[3] Lada A Adamic and Bernardo A Huberman. Power-law distribution\nof the world wide web. science, 287(5461):2115\u20132115, 2000.\n[4] Muhammad Adnan, Yassaman Ebrahimzadeh Maboud, Divya Mahajan,\nand Prashant J. Nair. Accelerating recommendation system training\nby leveraging popular choices. Proc. VLDB Endow., 15(1):127\u2013140, sep\n2021.\n[5] Michael Armbrust, Ali Ghodsi, Reynold Xin, and Matei Zaharia. Lake-\nhouse: a new generation of open platforms that unify data warehousing\nand advanced analytics. In Proceedings of CIDR, 2021.\n[6] Zhihao Bai, Zhen Zhang, Yibo Zhu, and Xin Jin. Pipeswitch: Fast\npipelined context switching for deep learning applications. In Proceed-\nings of the 14th USENIX Conference on Operating Systems Design and\nImplementation, pages 499\u2013514, 2020.\n[7] Keshav Balasubramanian, Abdulla Alshabanah, Joshua D Choe, and\nMurali Annavaram. cdlrm: Look ahead caching for scalable training\nof recommendation models. In Proceedings of the 15th ACM Conference\non Recommender Systems, pages 263\u2013272, 2021.\n[8] Laszlo A. Belady. A study of replacement algorithms for a virtual-\nstorage computer. IBM Systems journal, 5(2):78\u2013101, 1966.\n[9] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar\nChandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai,\nMustafa Ispir, et al. Wide & deep learning for recommender systems.\nIn Proceedings of the 1st workshop on deep learning for recommender\nsystems, pages 7\u201310, 2016.\n[10] Paul Covington, Jay Adams, and Emre Sargin. Deep neural networks\nfor youtube recommendations. In Proceedings of the 10th ACM confer-\nence on recommender systems, pages 191\u2013198, 2016.\n[11] Benoit Dageville, Thierry Cruanes, Marcin Zukowski, Vadim Antonov,\nArtin Avanes, Jon Bock, Jonathan Claybaugh, Daniel Engovatov, Mar-\ntin Hentschel, Jiansheng Huang, et al. The snowflake elastic data\nwarehouse. In Proceedings of the 2016 International Conference on\nManagement of Data, pages 215\u2013226, 2016.\n[12] Shuiguang Deng, Longtao Huang, Guandong Xu, Xindong Wu, and\nZhaohui Wu. On deep learning for trust-aware recommendations in\nsocial networks. IEEE transactions on neural networks and learning\nsystems, 28(5):1164\u20131177, 2016.\n[13] Assaf Eisenman, Kiran Kumar Matam, Steven Ingram, Dheevatsa\nMudigere, Raghuraman Krishnamoorthi, Krishnakumar Nair, Misha\nSmelyanskiy, and Murali Annavaram. {Check-N-Run}: a checkpoint-\ning system for training deep learning recommendation models. In 19th\nUSENIX Symposium on Networked Systems Design and Implementation\n(NSDI 22), pages 929\u2013943, 2022.\n[14] Robert M French. Catastrophic forgetting in connectionist networks.\nTrends in cognitive sciences, 3(4):128\u2013135, 1999.\n[15] Swapnil Gandhi and Anand Padmanabha Iyer. P3: Distributed deep\ngraph learning at scale. In 15th USENIX Conference on Operating\nSystems Design and Implementation (OSDI) 21, pages 551\u2013568, 2021.\n[16] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang\nHe. Deepfm: A factorization-machine based neural network for ctr\nprediction. In IJCAI, 2017.\n[17] Udit Gupta, Samuel Hsia, Vikram Saraph, Xiaodong Wang, Brandon\nReagen, Gu-Yeon Wei, Hsien-Hsin S Lee, David Brooks, and Carole-\nJean Wu. Deeprecsys: A system for optimizing end-to-end at-scale\nneural recommendation inference. In 2020 ACM/IEEE 47th Annual\nInternational Symposium on Computer Architecture (ISCA), pages 982\u2013\n995. IEEE, 2020.\n[18] Udit Gupta, Carole-Jean Wu, Xiaodong Wang, Maxim Naumov, Bran-\ndon Reagen, David Brooks, Bradford Cottel, Kim Hazelwood, Mark\nHempstead, Bill Jia, et al. The architectural implications of facebook\u2019s\ndnn-based personalized recommendation. In 2020 IEEE International\nSymposium on High Performance Computer Architecture (HPCA), pages\n488\u2013501. IEEE, 2020.\n[19] Vipul Gupta, Dhruv Choudhary, Peter Tang, Xiaohan Wei, Xing Wang,\nYuzhen Huang, Arun Kejariwal, Kannan Ramchandran, and Michael W\nMahoney. Training recommender systems at scale: Communication-\nefficient model and data parallelism. In Proceedings of the 27th ACM\nSIGKDD Conference on Knowledge Discovery & Data Mining, pages\n2928\u20132936, 2021.\n[20] Gurobi optimization. https://www.gurobi.com/.\n[21] Chaoyang He, Songze Li, Jinhyun So, Mi Zhang, Hongyi Wang, Xi-\naoyang Wang, Praneeth Vepakomma, Abhishek Singh, Hang Qiu,\nLi Shen, Peilin Zhao, Yan Kang, Yang Liu, Ramesh Raskar, Qiang Yang,\nMurali Annavaram, and Salman Avestimehr. Fedml: A research library\nand benchmark for federated machine learning. Advances in Neural\nInformation Processing Systems, Best Paper Award at Federate Learning\nWorkshop, 2020.\n[22] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and\nTat-Seng Chua. Neural collaborative filtering. In Proceedings of the\n26th international conference on world wide web, pages 173\u2013182, 2017.\n[23] Source code for het. https://github.com/Hsword/Hetu/tree/ef1959.\n[24] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao\nChen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui\nWu, et al. Gpipe: Efficient training of giant neural networks using\npipeline parallelism. Advances in neural information processing systems,\n32, 2019.\n[25] Yuzhen Huang, Xiaohan Wei, Xing Wang, Jiyan Yang, Bor-Yiing Su,\nShivam Bharuka, Dhruv Choudhary, Zewei Jiang, Hai Zheng, and\nJack Langman. Hierarchical training: Scaling deep recommendation\nmodels on large cpu clusters. In Proceedings of the 27th ACM SIGKDD\nConference on Knowledge Discovery & Data Mining, KDD \u201921, page\n3050\u20133058, New York, NY, USA, 2021. Association for Computing\nMachinery.\n[26] NVIDIA Merlin HugeCTR Framework. https://developer.nvidia.com/\nnvidia-merlin/hugectr.\n[27] Avazu Inc. Avazu click logs. https://www.kaggle.com/c/avazu-ctr-\nprediction/data, 2013. Accessed: August 31, 2023.\n[28] Tigran Ishkhanov, Maxim Naumov, Xianjie Chen, Yan Zhu, Yuan\nZhong, Alisson Gusatti Azzolini, Chonglin Sun, Frank Jiang, Andrey\nMalevich, and Liang Xiong. Time-based sequence model for personal-\nization and recommendation systems. arXiv preprint arXiv:2008.11922,\n2020.\n[29] Zhihao Jia, Matei Zaharia, and Alex Aiken. Beyond data and model\nparallelism for deep neural networks. Proceedings of Machine Learning\nand Systems, 1:1\u201313, 2019.\n[30] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic\noptimization. arXiv preprint arXiv:1412.6980, 2014.\n[31] George Kingsley Zipf. Selected studies of the principle of relative fre-\nquency in language. Harvard university press, 1932.\n[32] Dharshan Kumaran, Demis Hassabis, and James L McClelland. What\nlearning systems do intelligent agents need? complementary learning\nsystems theory updated. Trends in cognitive sciences, 20(7):512\u2013534,\n2016.\n[33] Youngeun Kwon and Minsoo Rhu. Training personalized recommen-\ndation systems from (gpu) scratch: look forward not backwards. In\nProceedings of the 49th Annual International Symposium on Computer\nArchitecture, pages 860\u2013873, 2022.\n[34] Criteo Labs. Criteo kaggle logs. https://labs.criteo.com/2014/02/kaggle-\ndisplay-advertising-challenge-dataset/, 2013. Accessed: August 31,\n2023.\n[35] Criteo Labs. Terabyte click logs. https://labs.criteo.com/2014/02/\nkaggle-display-advertising-challenge-dataset/, 2013. Accessed: Au-\ngust 31, 2023.\n[36] Fan Lai, Xiangfeng Zhu, Harsha V Madhyastha, and Mosharaf Chowd-\nhury. Oort: Efficient federated learning via guided participant selection.\nIn 15th USENIX Conference on Operating Systems Design and Imple-\nmentation (OSDI) 21, pages 19\u201335, 2021.\n[37] Joonseok Lee, Sami Abu-El-Haija, Balakrishnan Varadarajan, and Apos-\ntol Natsev. Collaborative deep metric learning for video understanding.\nIn Proceedings of the 24th ACM SIGKDD International Conference on\nKnowledge Discovery & Data Mining, pages 481\u2013490, 2018.\n[38] Jure Leskovec, Kevin J Lang, Anirban Dasgupta, and Michael W Ma-\nhoney. Community structure in large networks: Natural cluster sizes\nand the absence of large well-defined clusters. Internet Mathematics,\n6(1):29\u2013123, 2009.\n[39] Mu Li, David G Andersen, Jun Woo Park, Alexander J Smola, Amr\nAhmed, Vanja Josifovski, James Long, Eugene J Shekita, and Bor-Yiing\nSu. Scaling distributed machine learning with the parameter server.\nIn 11th USENIX Symposium on Operating Systems Design and Imple-\nmentation (OSDI) 14), pages 583\u2013598, 2014.\n[40] Pengcheng Li, Runze Li, Qing Da, An-Xiang Zeng, and Lijun Zhang. Im-\nproving multi-scenario learning to rank in e-commerce by exploiting\ntask relationships in the label space. In Proceedings of the 29th ACM\nInternational Conference on Information & Knowledge Management,\npages 2605\u20132612, 2020.\n[41] Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis,\nTeng Li, Adam Paszke, Jeff Smith, Brian Vaughan, Pritam Damania,\net al. Pytorch distributed: Experiences on accelerating data parallel\ntraining. arXiv preprint arXiv:2006.15704, 2020.\n[42] Zhi Li, Hongke Zhao, Qi Liu, Zhenya Huang, Tao Mei, and Enhong\nChen. Learning from history and present: Next-item recommendation\nvia discriminatively exploiting user behaviors. In Proceedings of the\n24th ACM SIGKDD International Conference on Knowledge Discovery &\nData Mining, pages 1734\u20131743, 2018.\n[43] Xiangru Lian, Binhang Yuan, Xuefeng Zhu, Yulong Wang, Yongjun He,\nHonghuan Wu, Lei Sun, Haodong Lyu, Chengjun Liu, Xing Dong, et al.\nPersia: A hybrid system scaling deep learning based recommenders\nup to 100 trillion parameters. arXiv preprint arXiv:2111.05897, 2021.\n[44] David C Liu, Stephanie Rogers, Raymond Shiau, Dmitry Kislyuk,\nKevin C Ma, Zhigang Zhong, Jenny Liu, and Yushi Jing. Related\npins at pinterest: The evolution of a real-world recommender system.\nIn Proceedings of the 26th international conference on world wide web\ncompanion, pages 583\u2013592, 2017.\n[45] Zhuoran Liu, Leqi Zou, Xuan Zou, Caihua Wang, Biao Zhang, Da Tang,\nBolin Zhu, Yijie Zhu, Peng Wu, Ke Wang, et al. Monolith: real time\nrecommendation system with collisionless embedding table. arXiv\npreprint arXiv:2209.07663, 2022.\n[46] Peter Mattson, Christine Cheng, Gregory Diamos, Cody Coleman,\nPaulius Micikevicius, David Patterson, Hanlin Tang, Gu-Yeon Wei, Pe-\nter Bailis, Victor Bittorf, et al. Mlperf training benchmark. Proceedings\nof Machine Learning and Systems, 2:336\u2013349, 2020.\n[47] Meta. Torchrec. https://github.com/pytorch/torchrec/, 2022. Accessed:\nAugust 21, 2022.\n[48] Xupeng Miao, Hailin Zhang, Yining Shi, Xiaonan Nie, Zhi Yang,\nYangyu Tao, and Bin Cui. Het: Scaling out huge embedding model\ntraining via cache-enabled distributed framework. Proc. VLDB Endow.,\n15(2):312\u2013320, 2022.\n[49] Jason Mohoney, Roger Waleffe, Henry Xu, Theodoros Rekatsinas, and\nShivaram Venkataraman. Marius: Learning massive graph embeddings\non a single machine. In 15th USENIX Symposium on Operating Systems\nDesign and Implementation (OSDI) 21, 2021.\n[50] Mpi all to all. https://www.rookiehpc.com/mpi/docs/mpi_alltoall.php,\n2019. Accessed: August 31, 2023.\n[51] Dheevatsa Mudigere, Yuchen Hao, Jianyu Huang, Zhihao Jia, Andrew\nTulloch, Srinivas Sridharan, Xing Liu, Mustafa Ozdal, Jade Nie, Jong-\nsoo Park, et al. Software-hardware co-design for fast and scalable\ntraining of deep learning recommendation models. arXiv preprint\narXiv:2104.05158, 2021.\n[52] Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri,\nNikhil R Devanur, Gregory R Ganger, Phillip B Gibbons, and Matei\nZaharia. Pipedream: generalized pipeline parallelism for dnn train-\ning. In Proceedings of the 27th ACM Symposium on Operating Systems\nPrinciples, pages 1\u201315, 2019.\n[53] Maxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael Shi, Jianyu\nHuang, Narayanan Sundaraman, Jongsoo Park, Xiaodong Wang, Udit\nGupta, Carole-Jean Wu, Alisson G Azzolini, et al. Deep learning recom-\nmendation model for personalization and recommendation systems.\narXiv preprint arXiv:1906.00091, 2019.\n[54] Yurii Nesterov et al. Lectures on convex optimization, volume 137.\nSpringer, 2018.\n[55] Facebook Research. Facebook research dlrm. https://github.com/\nfacebookresearch/dlrm, 2019. Accessed: August 31, 2023.\n[56] Facebook Research. Facebook research dlrm. https://github.com/\nfacebookresearch/dlrm/issues/206, 2019. Accessed: December 10, 2021.\n[57] Facebook Research. Fbgemm. http://github.com/facebookresearch/\nfbgemm, 2019. Accessed: August 31, 2023.\n[58] Sebastian U Stich. Local sgd converges fast and communicates little.\narXiv preprint arXiv:1805.09767, 2018.\n[59] Yuhan Su, Zhongming Jin, Ying Chen, Xinghai Sun, Yaming Yang,\nFangzheng Qiao, Fen Xia, and Wei Xu. Improving click-through rate\nprediction accuracy in online advertising by transfer learning. In\nProceedings of the International Conference on Web Intelligence, pages\n1018\u20131025, 2017.\n[60] Yong Kiam Tan, Xinxing Xu, and Yong Liu. Improved recurrent neural\nnetworks for session-based recommendations. In Proceedings of the\n1st workshop on deep learning for recommender systems, pages 17\u201322,\n2016.\n[61] Rajeev Thakur, Rolf Rabenseifner, and William Gropp. Optimization\nof collective communication operations in mpich. The International\nJournal of High Performance Computing Applications, 19(1):49\u201366, 2005.\n[62] Trinh Xuan Tuan and Tu Minh Phuong. 3d convolutional networks for\nsession-based recommendation with content features. In Proceedings\nof the eleventh ACM conference on recommender systems, pages 138\u2013146,\n2017.\n[63] Bart\u0142omiej Twardowski. Modelling contextual information in session-\naware recommender systems with neural networks. In Proceedings\nof the 10th ACM Conference on Recommender Systems, pages 273\u2013276,\n2016.\n[64] A\u00e4ron Van Den Oord, Sander Dieleman, and Benjamin Schrauwen.\nDeep content-based music recommendation. In Neural Information Pro-\ncessing Systems Conference (NIPS 2013), volume 26. Neural Information\nProcessing Systems Foundation (NIPS), 2013.\n[65] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. Deep & cross\nnetwork for ad click predictions. In Proceedings of the ADKDD\u201917,\nADKDD\u201917, Halifax, NS, Canada, 2017. Association for Computing\nMachinery.\n[66] Suhang Wang, Yilin Wang, Jiliang Tang, Kai Shu, Suhas Ranganath,\nand Huan Liu. What your images reveal: Exploiting visual contents\nfor point-of-interest recommendation. In Proceedings of the 26th inter-\nnational conference on world wide web, pages 391\u2013400, 2017.\n[67] Xinxi Wang and Ye Wang. Improving content-based and hybrid music\nrecommendation using deep learning. In Proceedings of the 22nd ACM\ninternational conference on Multimedia, pages 627\u2013636, 2014.\n[68] Jiqing Wen, James She, Xiaopeng Li, and Hui Mao. Visual background\nrecommendation for dance performances using deep matrix factoriza-\ntion. ACM Transactions on Multimedia Computing, Communications,\nand Applications (TOMM), 14(1):1\u201319, 2018.\n[69] Xiaofeng Wu, Jia Rao, Wei Chen, Hang Huang, Chris Ding, and Heng\nHuang. Switchflow: preemptive multitasking for deep learning. In\nProceedings of the 22nd International Middleware Conference, pages\n146\u2013158, 2021.\n[70] C-Q Yang and Barton P Miller. Critical path analysis for the execution\nof parallel and distributed programs. In The 8th International Confer-\nence on Distributed, pages 366\u2013367. IEEE Computer Society, 1988.\n[71] Juncheng Yang, Yazhuo Zhang, Ziyue Qiu, Yao Yue, and Rashmi\nVinayak. Fifo queues are all you need for cache eviction. In Pro-\nceedings of the 29th Symposium on Operating Systems Principles, pages\n130\u2013149, 2023.\n[72] Chunxing Yin, Bilge Acun, Carole-Jean Wu, and Xing Liu. Tt-rec:\nTensor train compression for deep learning recommendation models.\nIn A. Smola, A. Dimakis, and I. Stoica, editors, Proceedings of Machine\nLearning and Systems, volume 3, pages 448\u2013462, 2021.\n[73] Mark Zhao, Niket Agarwal, Aarti Basant, Bugra Gedik, Satadru Pan,\nMustafa Ozdal, Rakesh Komuravelli, Jerry Pan, Tianshu Bao, Haowei\nLu, et al. Understanding and co-designing the data ingestion pipeline\nfor industry-scale recsys training. arXiv preprint arXiv:2108.09373,\n2021.\n",
        "context": "accesses across three datasets (details in Table 1), similar\nto prior work [4], we find that the embedding access pat-\ntern shows a large degree of skew. As shown in Figure 3 we\nobserve that almost 90% of embedding accesses come from\nthis paper is that embedding access has a specific structure\nwhich can be used to accelerate training. We observe that\nembedding accesses are heavily skewed, with around 1% of\nembeddings representing more than 92% of total accesses.\nleveraged to accelerate training. Our analysis across three\ndatasets indicates significant skew in embedding accesses;\ne.g., with the Kaggle (Criteo) dataset, we observe that 90%\nof accesses are for just 0.1% of the embeddings. While this"
    },
    {
        "id": 20,
        "title": "Distributed hierarchical gpu parameter server for massive scale deep learning ads systems",
        "author": [
            "W. Zhao",
            "D. Xie",
            "R. Jia",
            "Y. Qian",
            "R. Ding",
            "M. Sun",
            "P. Li"
        ],
        "year": "2020",
        "doi": null,
        "in_text_citation": "[20]",
        "sentence": "Existing methods [20] have explored the usage of distributed communication to decrease the communication cost, but the theoretical bounds of communication efficiency has not been analyzed in previous work.",
        "abstract": "Neural networks of ads systems usually take input from multiple resources,\ne.g., query-ad relevance, ad features and user portraits. These inputs are\nencoded into one-hot or multi-hot binary features, with typically only a tiny\nfraction of nonzero feature values per example. Deep learning models in online\nadvertising industries can have terabyte-scale parameters that do not fit in\nthe GPU memory nor the CPU main memory on a computing node. For example, a\nsponsored online advertising system can contain more than $10^{11}$ sparse\nfeatures, making the neural network a massive model with around 10 TB\nparameters. In this paper, we introduce a distributed GPU hierarchical\nparameter server for massive scale deep learning ads systems. We propose a\nhierarchical workflow that utilizes GPU High-Bandwidth Memory, CPU main memory\nand SSD as 3-layer hierarchical storage. All the neural network training\ncomputations are contained in GPUs. Extensive experiments on real-world data\nconfirm the effectiveness and the scalability of the proposed system. A 4-node\nhierarchical GPU parameter server can train a model more than 2X faster than a\n150-node in-memory distributed parameter server in an MPI cluster. In addition,\nthe price-performance ratio of our proposed system is 4-9 times better than an\nMPI-cluster solution.",
        "full_text": "DISTRIBUTED HIERARCHICAL GPU PARAMETER SERVER FOR\nMASSIVE SCALE DEEP LEARNING ADS SYSTEMS\nWeijie Zhao1, Deping Xie2, Ronglai Jia2, Yulei Qian2, Ruiquan Ding3, Mingming Sun1, Ping Li1\n1 Cognitive Computing Lab, Baidu Research\n2 Baidu Search Ads (Phoenix Nest), Baidu Inc.\n3 Sys. & Basic Infra., Baidu Inc.\n{weijiezhao, xiedeping01, jiaronglai, qianyulei, dingruiquan, sunmingming01, liping11}@baidu.com\nABSTRACT\nNeural networks of ads systems usually take input from multiple resources, e.g., query-ad relevance, ad features\nand user portraits. These inputs are encoded into one-hot or multi-hot binary features, with typically only a tiny\nfraction of nonzero feature values per example. Deep learning models in online advertising industries can have\nterabyte-scale parameters that do not \ufb01t in the GPU memory nor the CPU main memory on a computing node.\nFor example, a sponsored online advertising system can contain more than 1011 sparse features, making the\nneural network a massive model with around 10 TB parameters. In this paper, we introduce a distributed GPU\nhierarchical parameter server for massive scale deep learning ads systems. We propose a hierarchical work\ufb02ow\nthat utilizes GPU High-Bandwidth Memory, CPU main memory and SSD as 3-layer hierarchical storage. All the\nneural network training computations are contained in GPUs. Extensive experiments on real-world data con\ufb01rm\nthe effectiveness and the scalability of the proposed system. A 4-node hierarchical GPU parameter server can\ntrain a model more than 2X faster than a 150-node in-memory distributed parameter server in an MPI cluster. In\naddition, the price-performance ratio of our proposed system is 4-9 times better than an MPI-cluster solution.\n1\nINTRODUCTION\nBaidu Search Ads (a.k.a. \u201cPhoenix Nest\u201d) has been success-\nfully using ultra-high dimensional input data and ultra-large-\nscale deep neural networks for training CTR (Click-Through\nRate) models since 2013 (Fan et al., 2019). Sponsored on-\nline advertising produces many billions of dollar revenues\nfor online ad publishers such as Baidu, Bing, and Google.\nThe task of CTR prediction (Broder, 2002; Fain & Pedersen,\n2006; Fan et al., 2019; Zhao et al., 2019) plays a key role to\ndetermine the best ad spaces allocation because it directly\nin\ufb02uences user experience and ads pro\ufb01tability. CTR pre-\ndiction takes input from multiple resources\u2013e.g., query-ad\nrelevance, ad features, and user portraits\u2013then estimates the\nprobability that a user clicks on a given ad. These inputs\nare typically one-hot/multi-hot binary features with only a\ntiny fraction of nonzero feature values per example. The\nhigh-dimensional sparse input data \ufb01rst go through an em-\nbedding layer to get a low-dimensional embedding; then\nthe embedded results are connected by fully-connected lay-\ners. A real-world sponsored online advertising system can\ncontain more than 1011 sparse features, making the neural\nnetwork a massive model with around 10 TB parameters.\nFigure 1 provides an illustration of CTR prediction network.\nProceedings of the 3 rd MLSys Conference, Austin, TX, USA,\n2020. Copyright 2020 by the author(s).\nSparse input\nSparse \nparameters\nDense \nparameters\nOutput\n(~10 TB)\n(< 1 GB)\nEmbedding\nlayer\nFully-connected\nlayers\nFigure 1. A visual illustration of CTR prediction network. The in-\nput data are sparse\u2014only a tiny proportion of features are non-zero.\nThe parameters of hatched neurons are referenced by the input.\n1.1\nCTR Prediction Based on MPI-Cluster Solution\nSince 2013, the standard practice in Baidu Search Ads has\nbeen using an MPI-based solution for training massive-\nscale deep learning CTR prediction. Before that, Baidu had\nadopted a distributed logistic regression (LR) CTR model\nand distributed parameter server around 2010. The MPI\nsolution partitions the model parameters across comput-\ning nodes (e.g., 150 nodes). Each node obtains a batch of\ntraining data streamed from a distributed \ufb01le system. The\nnode pulls the required parameters from other nodes and\narXiv:2003.05622v1  [cs.DC]  12 Mar 2020\nDistributed Hierarchical GPU Parameter Server for Massive Scale Deep Learning Ads Systems\ncomputes the gradients. The gradients are pushed to the\nnodes that maintain the corresponding parameters through\nMPI communications. Baidu\u2019s MPI solution and distributed\nparameter server was also summarized in (Li et al., 2014)\n(while the \ufb01rst author in that paper was with Baidu Inc.).\nWe should mention that this paper focuses on the CTR mod-\nels in the \ufb01nal stage of Baidu\u2019s ads system. In the earlier\nstage of the pipeline, Baidu has been extensively using re-\ncent advancements on approximate near neighbor search\n(ANNS) and maximum inner product search (MIPS) (Fan\net al., 2019; Zhou et al., 2019; Zhao et al., 2020; Tan et al.,\n2020) to improve the quality of ads recalls, which is a sepa-\nrate important task in the full pipeline of the ads system.\n1.2\nDrawbacks of MPI-Cluster Solution\nCTR model researchers and practitioners train the massive-\nscale model rapidly to test and verify their new ideas. How-\never, it is impractical to reserve a large-scale computing\ncluster for everyone\u2014the hardware maintenance and the\nenergy cost of a large-scale computing cluster is pricey. Be-\nsides, further scaling up the training speed is challenging\u2014\nin this large-scale distributed setting, communication and\nsynchronization cost between computing nodes limits the\ntraining performance. We seek to explore other directions to\noptimize the training system for massive-scale ads models.\n1.3\nGPU Parameter Server for CTR Prediction\nRecently, GPUs attract broad attention as a cost-ef\ufb01cient\nmachine learning accelerator. However, it is not straight-\nforward to directly employ GPUs on the CTR prediction\nproblem due to the limited GPU memory and excessive\nCPU-GPU data communications. This amount of parame-\nters exceeds the amount of overall volume of GPU High-\nBandwidth Memory (HBM) of any single computing node,\nand any practical computing cluster. GPUs are considered\ninef\ufb01cient to train the model that does not \ufb01t in the GPU\nmemory (Chilimbi et al., 2014). Prior efforts (Cui et al.,\n2016; Zhao et al., 2019) cache the parameters in the CPU\nmain memory and still require time-consuming data trans-\nferring between CPU and GPU to update parameters.\nWe notice that in the massive-scale ads system, although the\nentire model is huge, the number of non-zero features per\nexample is small. For example, in Figure 1, the referenced\nparameters for the given input are hatched. The sparse\ninput has 2 non-zeros: the 1st and the 6th features. For the\nparameters in the embedding layer, only a subset of them\nis used and will be updated for this sparse input (we call\nthem \u201csparse parameters\u201d for convenience). Following that,\nthe dense parameters in the fully-connected layers are all\nreferenced. The CTR prediction neural network usually\nhas at most a few million dense parameters\u2014the memory\nfootprint of dense parameters is small (around a few hundred\nmegabytes). The working parameters\u2013the parameters that\nare referenced in the current batch input\u2013can \ufb01t in the GPU\nmemory. It allows us to distribute the working parameters\nin the GPU memory and accelerate the training with GPUs.\nIn this paper, we propose a hierarchical parameter server\nthat builds a distributed hash table across multiple GPUs\nand performs direct inter-GPU communications to eliminate\nthe CPU-GPU data transferring overhead. Moreover, the\nhierarchical design on top of SSDs with an in-memory cache\nalso enables us to train a large-scale out-of-main-memory\ndeep neural network on distributed GPU environments.\nChallenges & approaches. There are three major chal-\nlenges in the design of the hierarchical parameter server. 1)\nThe \ufb01rst challenge is to construct an ef\ufb01cient distributed\nGPU hash table to store the working parameters in multiple\nGPUs in multiple computing nodes. We design intra-node\nGPU communications and collective communication strate-\ngies among multiple nodes to synchronize the parameters\nacross all GPUs. Remote direct memory access protocols\nare employed to enable direct peer-to-peer GPU communi-\ncations without involving CPUs. 2) The second challenge\nis to devise a data transferring and caching mechanism to\nkeep working parameters in memory. Data transferring and\nSSD I/Os bandwidth are relatively slow compared with the\ndeep neural network training on GPUs. Without a care-\nfully designed architecture, excessive network and disk I/Os\ncan be a dominant performance slowdown. We propose\na 4-stage pipeline that overlaps inter-node network com-\nmunications, SSD I/Os, CPU/GPU data transferring and\nGPU computations. All stages are optimized to match the\ninevitable GPU computations\u2014the pipeline hides the I/O\nlatency. 3) The third challenge is to effectively organize the\nmaterialized parameters on SSDs. The I/O granularity of\nSSDs is a block, while the training parameters we load is in\nkey-value granularity. This mismatch incurs I/O ampli\ufb01ca-\ntion (Lu et al., 2017)\u2014unnecessary data are read from SSDs\nto load required parameters. In contrast, reading and writ-\ning contiguous addresses on SSDs are faster than random\naccesses\u2014larger block size yields better disk I/O bandwidth.\nWe present a \ufb01le-level parameter management strategy to\nwrite updates in batches as new \ufb01les. A \ufb01le compaction\nthread runs in the background to regularly merge \ufb01les that\ncontain a large proportion of stale parameters into new \ufb01les.\nContributions. The technical contributions we make in this\npaper can be summarized as follows:\n\u2022 We present a hashing method (OP+OSRP) to reduce CTR\nmodels. We observe that combining OP+OSRP with\nDNN-based models can replace the original LR-based\nmodels. However, for DNN-based CTR models, the hash-\ning method will hurt the prediction accuracy to the extent\nthat the revenue would be noticeably affected (Section 2).\n\u2022 We introduce the architecture of a distributed hierarchi-\nDistributed Hierarchical GPU Parameter Server for Massive Scale Deep Learning Ads Systems\ncal GPU parameter server (Section 3). The hierarchical\ndesign employs the main memory and SSDs to store the\nout-of-GPU-memory and out-of-main-memory parame-\nters when training massive-scale neural networks. As\nfar as we know, it is the \ufb01rst distributed GPU parameter\nserver for the terabyte-scale sparse input deep learning.\n\u2022 We propose a novel HBM parameter server that keeps\nthe working parameters in distributed GPU HBMs. The\nHBM parameter server enables GPU worker threads to\nef\ufb01ciently fetch/update neural network parameters and\nto directly synchronize parameters across multiple GPU\ncards from multiple computing nodes (Section 4).\n\u2022 We show a CPU main memory parameter server that\nprefetches and buffers the parameters of the future train-\ning batches. The memory parameter server hides the data\nloading and parameter transfer latency (Section 5).\n\u2022 We present an SSD parameter server that materializes the\nout-of-main-memory parameters in \ufb01les. Parameter up-\ndates are written in batches onto SSDs as new \ufb01les to fully\nutilize the I/O bandwidth. Regular compaction operations\nare performed to bound the disk usage (Section 6).\n\u2022 We perform an extensive set of experiments on 5 real CTR\nprediction models/datasets and compare the results with\nan MPI cluster training framework in the production envi-\nronment (Section 7). The 4-node distributed hierarchical\nGPU parameter server is 1.8-4.8X faster than the MPI\nsolution. The price-performance ratio of our proposed\nsystem is 4-9 times better than the MPI solution.\n2\nPRIOR EFFORT: HASHING METHODS\nFOR REDUCING CTR MODELS\nIt is natural to ask why one could not simply use a good\nhashing method to reduce the model size. In this section, we\nreport a brief summary of our prior efforts back in 2015 for\ndeveloping hashing algorithms to reduce the size of CTR\nmodels, with the hope that hashing would not hurt accuracy.\nHashing algorithms are popular these days (Weinberger\net al., 2009; Li et al., 2011). We had spent serious efforts to\ndevelop effective hashing methods suitable for CTR models.\nThe results are both encouraging and discouraging. Here\nwe would like to make a comment that there is a differ-\nence between academia research and industrial practice. In\nacademia research, we often take it as a good result if, for\nexample, we are able to achieve a 10-fold model reduction\nby losing only 1% of accuracy. For commercial search, how-\never, we often cannot afford to lose an accuracy of 0.1%\nsince that would result in a noticeable decrease in revenue.\nOne permutation + one sign random projection. In the\ncourse of this research, we had tried many versions of hash-\ning. We eventually recommend the method called \u201cone\npermutation + one sign random projection (OP+OSRP)\u201d.\nAs the name suggests, our idea was inspired by several well-\nknown hashing methods, e.g., (Goemans & Williamson,\n1995; Charikar, 2002; Charikar et al., 2004; Cormode &\nMuthukrishnan, 2005; Weinberger et al., 2009; Li et al.,\n2011; 2012; Shrivastava & Li, 2014; Li et al., 2019).\nWe assume the dimensionality of the binary training data is\np. OP+OSRP consists of the following key steps:\n1. Permute the p columns (only once). This can be very\nef\ufb01ciently done via standard 2U or 4U hashing.\n2. Break the permuted p columns uniformly into k bins.\n3. Apply independent random projection in each bin, for\nexample, in the \ufb01rst bin, we let z = Pp/k\ni=1 xiri, where\nwe sample ri \u2208{\u22121, +1} w. p. 1/2.\n4. Store the sign of z and expand it into a vector of length 2:\n[0 1] if z >0, [1 0] if z < 0 , [0 0] if z = 0. The hashed\ndata will be binary in 2k dimensions. This step is differ-\nent from prior research. This way, we still obtain binary\nfeatures so that we do not need to modify the training\nalgorithm (which is coded ef\ufb01ciently for binary data).\nOP+OSRP is very ef\ufb01cient (essentially by touching each\nnonzero entry once) and can naturally handle sparse data\n(i.e., many bins will be empty since k cannot be small).\nExperimental results. This set of experiments was con-\nducted in 2015 using 3 months of sponsored ads click data\nfrom web search and 3 months of data from image search.\nAs one would expect, web search brings in majority of the\nrevenue. Image search is fun and important but its revenue\nis only a tiny fraction of the revenue from web search. Un-\nderstandably, lots of computing/storage resources have been\nallocated for building CTR models for web search.\nTable 1. OP+OSRP for Image Search Sponsored Ads Data\n# Nonzero Weights\nTest AUC\nBaseline LR\n31,949,213,205\n0.7112\nBaseline DNN\n0.7470\nHash+DNN (k = 234)\n6,439,972,994\n0.7407\nHash+DNN (k = 223)\n3,903,844,565\n0.7388\nHash+DNN (k = 222)\n2,275,442,496\n0.7370\nHash+DNN (k = 231)\n1,284,025,453\n0.7339\nHash+DNN (k = 230)\n707,983,366\n0.7310\nHash+DNN (k = 229)\n383,499,175\n0.7278\nHash+DNN (k = 228)\n203,864,439\n0.7245\nHash+DNN (k = 227)\n106,824,123\n0.7208\nHash+DNN (k = 226)\n55,363,771\n0.7175\nHash+DNN (k = 225)\n28,479,330\n0.7132\nHash+DNN (k = 224)\n14,617,324\n0.7113\nTable 1 summarizes the experimental result for applying\nOP+OSRP on image search ads data. There are several\nimportant observations from the results:\n\u2022 Compared to the baseline LR model, the DNN model\nvery substantially improves the AUC. This is basically the\nDistributed Hierarchical GPU Parameter Server for Massive Scale Deep Learning Ads Systems\njusti\ufb01cation of adopting DNN models for CTR prediction.\n\u2022 Hashing reduces the accuracy. Even with k = 234, the\ntest AUC is dropped by 0.7%.\n\u2022 Hash+DNN is a good combination for replacing LR.\nCompared to the original baseline LR model, we can re-\nduce the number of nonzero weights from 31B to merely\n14.6M without affecting the accuracy.\nTable 2 summarizes the experiments on web search ads data.\nThe trend is essentially similar to Table 1. The main differ-\nence is that we cannot really propose to use Hash+DNN for\nweb search ads CTR models, because that would reduce the\naccuracy of current DNN-based models and consequently\nwould affect the revenue for the company.\nTable 2. OP+OSRP for Web Search Sponsored Ads Data\n# Nonzero Weights\nTest AUC\nBaseline LR\n199,359,034,971\n0.7458\nBaseline DNN\n0.7670\nHash+DNN (k = 232)\n3,005,012,154\n0.7556\nHash+DNN (k = 231)\n1,599,247,184\n0.7547\nHash+DNN (k = 230)\n838,120,432\n0.7538\nHash+DNN (k = 229)\n433,267,303\n0.7528\nHash+DNN (k = 228)\n222,780,993\n0.7515\nHash+DNN (k = 227)\n114,222,607\n0.7501\nHash+DNN (k = 226)\n58,517,936\n0.7487\nHash+DNN (k = 224)\n15,410,799\n0.7453\nHash+DNN (k = 222)\n4,125,016\n0.7408\nSummary. This section summarizes our effort on develop-\ning effective hashing methods for ads CTR models. The\nwork was done in 2015 and we had never attempted to pub-\nlish the paper. The proposed algorithm, OP+OSRP, actually\nstill has some novelty to date, although it obviously com-\nbines several previously known ideas. The experiments are\nexciting in a way because it shows that one can use a single\nmachine to store the DNN model and can still achieve a\nnoticeable increase in AUC compared to the original (large)\nLR model. However, for the main ads CTR model used in\nweb search which brings in the majority of the revenue, we\nobserve that the test accuracy is always dropped as soon\nas we try to hash the input data. This is not acceptable in\nthe current business model because even a 0.1% decrease in\nAUC would result in a noticeable decrease in revenue.\nTherefore, this report helps explain why we introduce the\ndistributed hierarchical GPU parameter server in this paper\nto train the massive scale CTR models, in a lossless fashion.\n3\nDISTRIBUTED HIERARCHICAL\nPARAMETER SERVER OVERVIEW\nIn this section, we present the distributed hierarchical param-\neter server overview and describe its main modules from\na high-level view. Figure 2 illustrates the proposed hier-\narchical parameter server architecture. It contains three\nmajor components: HBM-PS, MEM-PS and SSD-PS.\nWorkers\npull/push\nParameter shards\nGPU3\nInter-GPU\ncommunications\nData shards\nGPU1\nGPU4\nGPU2\nHDFS\nMemory\nLocal \nparameters\nData shards\nSSD\nBatch load/dump\nMaterialized\nparameters\nLocal pull/push & Data transfer\nSSD-PS\nMEM-PS\nHBM-PS\nRemote \npull/push\nRDMA remote \nsynchronization\nFigure 2. Hierarchical parameter server architecture.\nWork\ufb02ow. Algorithm 1 depicts the distributed hierarchi-\ncal parameter server training work\ufb02ow. The training data\nbatches are streamed into the main memory through a net-\nwork \ufb01le system, e.g., HDFS (line 2). Our distributed train-\ning framework falls in the data-parallel paradigm (Li et al.,\n2014; Cui et al., 2014; 2016; Luo et al., 2018). Each node\nis responsible to process its own training batches\u2014different\nnodes receive different training data from HDFS. Then, each\nnode identi\ufb01es the union of the referenced parameters in\nthe current received batch and pulls these parameters from\nthe local MEM-PS/SSD-PS (line 3) and the remote MEM-\nPS (line 4). The local MEM-PS loads the local parameters\nstored on local SSD-PS into the memory and requests other\nnodes for the remote parameters through the network. Af-\nter all the referenced parameters are loaded in the memory,\nthese parameters are partitioned and transferred to the HBM-\nPS in GPUs. In order to effectively utilize the limited GPU\nmemory, the parameters are partitioned in a non-overlapped\nfashion\u2014one parameter is stored only in one GPU. When a\nworker thread in a GPU requires the parameter on another\nGPU, it directly fetches the parameter from the remote GPU\nand pushes the updates back to the remote GPU through\nhigh-speed inter-GPU hardware connection NVLink (Foley\n& Danskin, 2017). In addition, the data batch is sharded\ninto multiple mini-batches and sent to each GPU worker\nthread (line 5-10). Many recent machine learning system\nstudies (Ho et al., 2013; Chilimbi et al., 2014; Cui et al.,\n2016; Alistarh et al., 2018) suggest that the parameter stale-\nness shared among workers in data-parallel systems leads to\nslower convergence. In our proposed system, a mini-batch\ncontains thousands of examples. One GPU worker thread\nis responsible to process a few thousand mini-batches. An\nDistributed Hierarchical GPU Parameter Server for Massive Scale Deep Learning Ads Systems\nAlgorithm 1 Distributed Hierarchical Parameter Server\nTraining Work\ufb02ow.\n1. while not converged do\n2.\nbatch \u2190get_batch_from_HDFS()\n3.\nworking\u2190pull_local_MEM-PS_and_SSD-\nPS(batch)\n4.\nworking \u2190working \u222apull_remote_MEM-PS(batch)\n5.\nminibatches \u2190shard_batch(batch,#GPU,#minibatch)\n6.\npartitions \u2190map_parameters_to_GPUs(working,#GPU)\n7.\nfor i \u21901 to #GPU do\n8.\ntransfer_to_GPU(i,minibatchesi)\n9.\ninsert_into_hashtable(i,partitionsi)\n10.\nend for\n11.\nfor j \u21901 to #minibatch do\n12.\npull_HBM-PS_kernel(minibatchj)\n13.\n\u2206j \u2190train_mini-batch_kernel(j)\n14.\npush_parameters_updates_back_kernel(\u2206j)\n15.\nend for\n16.\n\u2206\u2190pull_updates_HBM-PS()\n17.\nparameters_to_dump \u2190update_local_cache()\n18.\npush_local_SSD-PS(parameters_to_dump)\n19. end while\ninter-GPU parameter synchronization is performed after\neach mini-batch is processed to eliminate the parameter stal-\neness. Each training worker pulls the required parameters\nof its corresponding mini-batch from the HBM-PS (line 12),\nperforms forward and backward propagation to update the\nparameters (line 13), and interacts with the HBM-PS to up-\ndate the referenced parameters on other GPUs (line 14). The\nMEM-PS collects the updates from the HBM-PS (line 16)\nand dumps infrequently used parameters to the SSD-PS\nwhen the MEM-PS does not have suf\ufb01cient memory (line 17-\n18). An example for Algorithm 1 is shown in Appendix A.\nHBM-PS, MEM-PS, and SSD-PS communicate in a hier-\narchical storage fashion. The upper-level module acts as a\nhigh-speed cache of the lower-level module.\nHBM-PS. HBM-PS is distributed in the High-Bandwidth\nMemory (HBM) across multiple GPUs. Comparing with\nthe conventional distributed parameter servers, workers in\nGPUs can directly request and update parameters in the\nHBM without transferring data between GPU memory and\nCPU memory. Thus the training throughput is signi\ufb01cantly\nincreased. However, the HBM capacity is limited and pricey.\nIn order to \ufb01t all the terabyte-scale parameters into HBMs,\nwe have to maintain a distributed computing cluster with\nhundreds of GPUs\u2014it is not only expensive but also inef\ufb01-\ncient, because excessive inter-GPU and inter-node commu-\nnications are required to request and update the parameters.\nTherefore, our proposed hierarchical architecture leverages\nmemory and SSD to store the massive model parameters.\nMEM-PS. The MEM-PS pulls the referenced parame-\nters from remote nodes to prepare the data for the train-\ning in GPUs. After the GPU workers complete the for-\nward/backward propagation, the MEM-PS retrieves the up-\ndates from the GPUs, applies the changes, and materializes\nthe updated parameters in the SSDs. The MEM-PS also\ncaches the frequently used parameters to reduce SSD I/Os.\nSSD-PS. We build the SSD-PS to store the materialized pa-\nrameters and provide ef\ufb01cient parameter loading and updat-\ning. Contiguous addresses readings and writings on SSDs\nhave better I/O bandwidth than random disk accesses. Thus,\nthe SSD-PS organizes the parameters in \ufb01les. The updated\nparameters in the memory are written to SSDs in batches as\nnew \ufb01les. A \ufb01le compaction thread runs in the background\nto regularly merge \ufb01les that contain a large proportion of\nstale parameters into new \ufb01les.\n4-stage pipeline. The training work\ufb02ow majorly involves\n4 time-consuming tasks: data transferring, parameter parti-\ntioning, materialized data loading/dumping and neural net-\nwork training. The 4 tasks correspond to independent hard-\nware resources: network, CPU, SSD and GPU, respectively.\nWe build a 4-stage pipeline to hide the latency of those tasks\nby maintaining a prefetch queue for each stage. A worker\nthread is created for each stage\u2014it extracts jobs from the\nprefetch queue and feeds the corresponding hardware re-\nsource. After that, the worker thread pushes the processed\nresults into the prefetch queue of the next stage. Especially,\nthe worker thread stalls when the prefetch queue of the next\nstage is full\u2014the next stage has already obtained too many\nunprocessed jobs. The capacity of the prefetch queue is pre-\nset according to the execution time of each stage. A detailed\nexplanation is shown in Appendix B.\n4\nHBM-PS\nThe HBM-PS stores the working parameters in GPUs and\nprovides ef\ufb01cient accesses. In this section, we introduce the\nmulti-GPU distributed hash table and GPU communications.\nAdditional details of HBM-PS are described in Appendix C.\n4.1\nMulti-GPU Distributed Hash Table\nMulti-GPU distributed hash table manages the GPU HBMs\non the same node and provides a uni\ufb01ed interface to the GPU\nworker threads. All referenced parameters of the current\ntraining batch are partitioned and inserted into the local\nhash table of each GPU. Operations such as insert, get and\naccumulate are provided to interact with the hash table.\nLocal GPU hash table.\nEach GPU maintains its\nown local hash table in the HBM. In this paper, we\nadopts the concurrent_unordered_map of cuDF li-\nbrary (https://github.com/rapidsai/cudf) as\nthe hash table. The hash table implementation uses the\nopen-addressing method to resolve hash collision. In order\nDistributed Hierarchical GPU Parameter Server for Massive Scale Deep Learning Ads Systems\nto avoid the inef\ufb01cient dynamic memory allocation in GPUs,\nwe \ufb01x the hash table capacity when we construct the hash\ntable\u2014we know the number of parameters (key-value pairs)\nto be stored on each GPU. For parallel hash table updates,\nthe data integrity is guaranteed by GPU atomic operations.\nAlgorithm 2 Distributed GPU hash table accumulate.\nInput: A collections of key-value pairs: (keys, vals).\n1. switch_to_GPU(get_resource_owner(keys, vals))\n2. partitioned \u2190parallel_partition(keys, vals)\n3. for i \u21901 to #GPU do\n4.\nif partitionedi \u0338= \u2205then\n5.\nasync_send(i, partitionedi)\n6.\nend if\n7. end for\n8. wait_send_destination()\n9. for i \u21901 to #GPU do\n10.\nswitch_to_GPU(i)\n11.\nhash_tablesi.async_accum(partitionedi)\n12. end for\nOperations. The multi-GPU distributed hash table provides\noperations such as insert, get and accumulate. Here\nwe introduce the accumulate operation that updates the\nparameters during neural network training in detail. The\ninsert and get operations implement the pull and push\ninterfaces to interact with the HBM-PS. Their work\ufb02ows\nare similar to the accumulate\u2014their implementation details\nare omitted due to the page limit.\nThe accumulate operation takes a collection of key-value\npairs as input and accumulates the values onto the param-\neters referenced by the corresponding keys. After each\nback propagation is \ufb01nished in the neural network training,\naccumulate operations are called to update the parameters.\nAlgorithm 2 illustrates the accumulate operation work\ufb02ow.\nThe key-value pairs are passed in a zero-copy fashion\u2014\nthese pairs are represented as the addresses of two arrays\u2013\nkeys and vals\u2013in the GPU HBM. We \ufb01rst switch to the GPU\nthat owns the memory of the input key-value pairs (line 1).\nThen we parallelly partition them according to the partition\npolicy. The partitioned results for each GPU are stored at\npartitioned (line 2). After that, we send the partitioned key-\nvalue pairs to their corresponding GPUs (line 3-7). The send\noperation is executed asynchronously through NVLink\u2014we\ncan send data to other GPUs simultaneously. Finally, we\niterate over all GPUs and apply the actual accumulation on\nthe local hash table of each GPU asynchronously (line 9-12).\n4.2\nGPU Communication\nSince each node processes its own input training batch, these\nbatches may share some working parameters\u2014the shared\nparameters are referenced and updated by multiple nodes.\nThe multi-GPU distributed hash table stores the working\nparameters across all GPUs in the same node. We have to\nsynchronize the parameters on different nodes to guarantee\nthe model convergence (Ho et al., 2013; Chilimbi et al.,\n2014; Cui et al., 2016; Alistarh et al., 2018).\nPhysically, GPU communication is performed through\nRemote Direct Memory Access (RDMA) (Potluri et al.,\n2013) hardware.\nRDMA enables zero-copy network\ncommunication\u2014it allows the network card to transfer data\nfrom a device memory directly to another device memory\nwithout copying data between the device memory and the\ndata buffers in the operating system. Our RDMA hardware\ndesign eliminates the involvement of the CPU and memory.\nLogically, the inter-node GPU parameter synchronization\nrequires an all-reduce communication\u2014each GPU needs to\nreceive all parameter updates from other GPUs and then\nperforms a reduction to accumulate these updates. Ap-\npendix C.3 illustrates the implementation details.\n5\nMEM-PS\nThe MEM-PS identi\ufb01es the referenced parameters from the\ninput and communicates with the local SSD-PS and remote\nMEM-PS to gather the required parameters. Meanwhile, a\nparameter cache is maintained to reduce SSD I/Os. More\nimplementation details are in Appendix D.\nPrepare parameters. As mentioned in the distributed ar-\nchitecture design (Figure 2), each node only maintains a\nshard of parameters\u2014parameters are partitioned according\nto a pre-de\ufb01ned parameter-to-node mapping. We leverage\nthe modulo hashing as the partition scheme in a similar\nmanner to the GPU parameter partition policy discussed in\nSection 4.1. The MEM-PS partitions the referenced parame-\nters according to their keys. For the remote parameters that\nbelong to a remote node, the MEM-PS pulls them from other\nMEM-PS through the network. For the local parameters,\nthe MEM-PS reads the SSDs to fetch the parameters. In\norder to avoid excessive SSD I/Os, an in-memory cache is\nmaintained to store the recently/frequently used parameters\nand to buffer the working parameters.\nUpdate parameters. The MEM-PS pulls the updated pa-\nrameters from the HBM-PS after its GPUs complete the\ncurrent batch and applies these changes in the memory (we\npin the working parameters in the memory). Note that we\ndo not need to push the changes of the remote parameters to\nother MEM-PS, because the remote parameters are synchro-\nnized across GPUs in HBM-PS\u2014the remote node MEM-PS\ncan pull the updates from its own GPUs.\n6\nSSD-PS\nThe SSD-PS aims to maintain the materialized out-of-main-\nmemory parameters ef\ufb01ciently on SSDs.\nDistributed Hierarchical GPU Parameter Server for Massive Scale Deep Learning Ads Systems\nParameter \ufb01les. We organize the model parameters in\nthe \ufb01le-level granularity\u2014each \ufb01le contains a collection\nof parameters. A parameter-to-\ufb01le mapping is maintained\nin the main memory. A \ufb01le descriptor occupies much less\nmemory than the parameter value\u2014a \ufb01le descriptor can be\nrepresented as couples of bytes while a parameter value\nconsists of multiple numeric attributes. In addition, a node\nonly contains a shard of the parameters. Thus, we can\nassume the mapping can \ufb01t in the memory of a single node.\nParameter I/O. We consider a parameter \ufb01le as an SSD\nI/O unit. The SSD-PS gathers requested parameter keys and\nreads an entire parameter \ufb01le when it contains requested\nparameters. On the other hand, parameters evicted from the\nHBM-PS cache are dumped onto SSDs. It is impractical\nto locate these parameters and perform in-place updates\ninside the original \ufb01le because it randomly writes the disk.\nInstead, our SSD-PS chunks these updated parameters into\n\ufb01les and writes them as new \ufb01les on SSDs\u2014data are sequen-\ntially written onto the disk. After the \ufb01les are written, we\nupdate the parameter-to-\ufb01le mapping of these parameters.\nThe older versions of the parameters stored in the previous\n\ufb01les become stale\u2014these older values will not be used since\nthe mapping is updated. The SSD usage hikes as we keep\ncreating \ufb01les on SSDs. A \ufb01le compaction operation is per-\nformed regularly to reduce the disk usage\u2014many old \ufb01les\ncontaining a large proportion of stale values can be merged\ninto new \ufb01les. Appendix E presents the implementation of\nparameter loading/dumping and \ufb01le compaction operations.\n7\nEXPERIMENTAL EVALUATION\nThe objective of the experimental evaluation is to inves-\ntigate the overall performance\u2013as well as the impact of\noptimizations\u2013of the proposed system. Speci\ufb01cally, the\nexperiments are targeted to answer the following questions:\n\u2022 How does the proposed hierarchical GPU parameter\nserver compare with the MPI cluster solution?\n\u2022 How is the execution time of each training stage?\n\u2022 How does the time distribute in each component?\n\u2022 How does the proposed system scale?\nSystem. We execute the distributed hierarchical GPU pa-\nrameter server experiments on 4 GPU computing nodes.\nEach node has 8 cutting-edge 32 GB HBM GPUs, server-\ngrade CPUs with 48 cores (96 threads), \u223c1 TB of memory,\n\u223c20 TB RAID-0 NVMe SSDs and a 100 Gb RDMA net-\nwork adaptor. The nodes in the MPI cluster for the baseline\ncomparison are maintained in the same data center. CPUs\nin the MPI cluster have similar performance speci\ufb01cations\nas the ones in the GPU computing nodes. All nodes are\ninter-connected through a high-speed Ethernet switch. The\nhardware and maintenance cost of 1 GPU node roughly\nequals to the cost of 10 CPU-only MPI nodes.\nTable 3. Model speci\ufb01cations.\n#Non-zeros\n#Sparse\n#Dense\nSize (GB)\nMPI\nA\n100\n8 \u00d7 109\n7 \u00d7 105\n300\n100\nB\n100\n2 \u00d7 1010\n2 \u00d7 104\n600\n80\nC\n500\n6 \u00d7 1010\n2 \u00d7 106\n2,000\n75\nD\n500\n1 \u00d7 1011\n4 \u00d7 106\n6,000\n150\nE\n500\n2 \u00d7 1011\n7 \u00d7 106\n10,000\n128\nModels. We use 5 CTR prediction models in real-world\nonline sponsor advertising applications to investigate the\neffectiveness of our proposed system. Table 3 illustrates the\nspeci\ufb01cation of these models. The number of sparse param-\neters of each model varies from 8 \u00d7 109 (model A) to 1011\n(model E). The number of dense parameters is 4 \u22125 orders\nof magnitude less than the number of sparse parameters for\nall models. The size of the smallest model is 300 GB, while\nthe largest model has 10 TB parameters. The MPI column\nshows the number of CPU-only nodes we used to train each\nmodel in the MPI cluster for real-world ads products. Due\nto different training speed requirements for these products,\nthe number of MPI cluster nodes varies from 75 to 150.\nData. We collect user click history logs from our search\nengine as the training dataset. The datasets are chunked\ninto batches\u2014each batch contains \u223c4 \u00d7 106 examples. The\ntrained CTR models are evaluated over the production envi-\nronment of our search engine in an A/B testing manner.\n7.1\nComparison with MPI Solution\nWe evaluate the performance of the proposed system on the\ntraining execution time and the prediction accuracy. Our\nproposed system runs on 4 GPU computing nodes. We\ntake an MPI cluster training framework in the production\nenvironment as the baseline.The baseline solution employs\na distributed parameter server that shards and keeps the\nmassive parameters in the memory across all the nodes.\nTraining time. Figure 3(a) depicts the total training ex-\necution time of the 4-node hierarchical parameter server\n(HPS-4) and the MPI training solution (MPI-cluster). HPS-\n4 outperforms MPI-cluster in all 5 models. The \ufb01rst row of\nTable 4 shows the speedup over the MPI solution. The\nHPS-4 is 1.8-4.8X faster than the MPI solution.\nNote\nthat the hardware and maintenance cost of 1 GPU node\nis roughly equivalent to 10 CPU-only nodes in the MPI\ncluster. The 4-GPU-node setting is much cheaper than the\n75-150 nodes in the MPI cluster. The second row of the\ntable illustrates the cost-normalized speedup of our pro-\nposed system. The cost-normalized speedup is computed\nas: speedup/4/10 \u00d7 #MPI. The price-performance ratio of\nTable 4. Training speedup over the MPI-cluster solution.\nA\nB\nC\nD\nE\nSpeedup over MPI-cluster\n1.8\n2.7\n4.8\n2.2\n2.6\nCost-normalized speedup\n4.4\n5.4\n9.0\n8.4\n8.3\nDistributed Hierarchical GPU Parameter Server for Massive Scale Deep Learning Ads Systems\nA (300GB) B (600GB)\nC (2TB)\nD (6TB)\nE (10TB)\n0.0E+0\n5.0E+4\n1.0E+5\n1.5E+5\n2.0E+5\nMPI-cluster\nHPS-4\nModels\n#Examples trained/sec\nMPI-100\nMPI-80\nMPI-75\nMPI-150\nMPI-128\n(a) Total execution time\nA (300GB) B (600GB) C (2TB)\nD (6TB)\nE (10TB)\n99.94%\n99.96%\n99.98%\n100.00%\n100.02%\n100.04%\n100.06%\n100.08%\nModels\nRelative AUC\n(b) AUC accuracy\nA (300GB) B (600GB)\nC (2TB)\nD (6TB)\nE (10TB)\n0\n50\n100\n150\n200\n250\nRead examples\nPull/push\nTrain DNN\nModels\nExecution time (sec)\n(c) Execution time distribution\nFigure 3. The performance (execution time and accuracy of 4-node hierarchical parameter server (HPS-4).\nour proposed system is 4.4-9.0X better than the MPI cluster.\nAccuracy. We take Area Under the Curve (Huang & Ling,\n2005) (AUC) as the quality measure of our trained models.\nFigure 3(b) shows the relative AUC of HPS-4 (relative to\nthe MPI-cluster method). The AUC of the MPI-cluster\nsolution is 100%. The AUCs of both trained models are\ntested online for 1 day in real online ads products. Since the\nCTR prediction accuracy is crucial to the revenue, we have\nto ensure all our optimizations are loss-less. For Model C,\nthe relative AUC loss of HPS-4 is less than 0.01%. For other\n4 models, HPS-4 has even slightly better accuracy than the\nMPI-128. Our hierarchical parameter server runs on fewer\nnodes\u2014fewer stale parameters are used than the case on the\nMPI cluster. Therefore, it is possible to have a better AUC\nthan the MPI solution. Overall, the relative differences of\nall 5 models are within 0.1%. We can conclude that our\nhierarchical parameter server training is loss-less.\n7.2\nTime distribution\nThe time distribution of each abstract execution stage is\npresented in Figure 3(c).\nAs illustrated in the training\nwork\ufb02ow (Algorithm 1), Read examples corresponds\nto the operation that reads and extracts the data from HDFS;\nPull/push is the execution time of gathering and updat-\ning the working parameters in the MEM-PS and SSD-PS;\nand Train DNN is the execution time of the inter-GPU\ncommunication and the deep neural network training in the\nHBM-PS. Since these stages are paralleled in the pipeline,\nthe overall execution time for each batch is dominated by\nthe slowest stage. For the two small models (Model A and\nB), the Read examples stage that relies on the HDFS\nthroughput becomes the bottleneck. Because the models are\nsmall and \ufb01t in the memory, the sparse parameter pulling\nand pushing operations are faster than the HDFS I/Os. When\nthe number of sparse parameters grows, the Pull/push\noperations catch up with the example reading (Model C)\nand become the dominant stage in Model D and E.\n7.3\nHBM-PS\nThe execution time distribution of HBM-PS operations is\nshown in Figure 4(a). The pull/push operations of HBM-\nPS depend on the number of non-zero values of training\ninputs. While the training operation (forward/backward\npropagation) correlates to the number of dense parameters\nof the deep model. The execution time of these operations\nshows the same trend in the model speci\ufb01cation (Table 3).\nModel A and Model B have around 100 non-zero input\nfeatures per example, the pull/push HBM-PS operations are\nfaster than the ones of Model C, D and E. Model E has\nthe largest number of dense parameters. Thus its training\noperation costs most of the execution time in HBM-PS.\n7.4\nMEM-PS\nLocal/remote parameters. Figure 4(b) illustrates the exe-\ncution time of pulling local/remote parameters in the MEM-\nPS for Model E over 1, 2 and 4 GPU nodes. The results of\nother models show a similar trend. The remote parameter\npulling operation is not applicable when we only have 1\nGPU node\u2014all the parameters are stored in the same node.\nWhen we deploy the proposed system in distributed envi-\nronments with 2 and 4 nodes, the local and remote pulling\noperations are paralleled. The overall execution time is de-\ntermined by the slower operation. We can observe that the\noverall time of pulling MEM-PS parameters does not hike\nmuch when inter-node communications are involved.\nCache performance. The cache hit rate for Model E is\nillustrated in Figure 4(c). In the beginning of the training,\nthe cache hit rate is low because we start from a cold cache\u2014\nno data are cached. We can observe that the cache hit rate\nincreases steeply during the training of the \ufb01rst 10 batches.\nAfter 40 training batches, the cache hit rate goes to 46% and\nbecomes stable in the following training\u2014the frequently\nvisited parameters are captured by our cache policy.\n7.5\nSSD-PS\nFigure 5(a) presents the SSD I/O time for Model E. The SSD\ndisk usage threshold is reached and the parameter \ufb01le com-\npaction operation is involved from the 54th batch\u2014the SSD\nI/O time hikes. The regular \ufb01le merging in the compaction\noperation causes the I/O performance \ufb02uctuation.\nDistributed Hierarchical GPU Parameter Server for Massive Scale Deep Learning Ads Systems\nA (300GB) B (600GB)\nC (2TB)\nD (6TB)\nE (10TB)\n0\n20\n40\n60\n80\n100\nPush-HBM-PS\nTraining\nPull-HBM-PS\nModels\nExecution time (sec)\n(a) Time distribution in HBM-PS\n1\n2\n4\n0\n50\n100\n150\n200\n250\nPull-local\nPull-remote\n#Nodes\nExecution time (sec)\nN/A\n(b) Time distribution in MEM-PS\n0\n20\n40\n60\n80\n100\n120\n140\n0%\n10%\n20%\n30%\n40%\n50%\n#Batch\nCache hit rate\n(c) Cache hit rate\nFigure 4. The time distribution in HBM-PS/MEM-PS and the cache hit rate on Model E.\n0\n20\n40\n60\n80\n100\n120\n140\n0\n50\n100\n150\n200\n250\n#Batch\nSSD I/O time (sec)\n(a) SSD-PS I/O time\n1\n2\n3\n4\n0E+0\n3E+4\n6E+4\n9E+4\nreal\nideal\n#Nodes\n#Examples trained/sec\n(b) Speedup\nFigure 5. The SSD-PS I/O time and speedup on Model E.\n7.6\nScalability\nThe training throughput (number of examples trained per\nsecond) speedup is depicted in Figure 5(b) over 1, 2, and 4\nnodes. The dashed line represents the ideal linear speedup.\nThe reason for the (slightly) sub-linear speedup is that more\nnetwork communications are incurred when we use more\nnodes. A speedup of 3.57 out of 4 is obtained for 4 nodes.\n7.7\nDiscussion\nBased on the results shown above, we can answer the\nquestions driving the experimental evaluation. The 4-node\ndistributed hierarchical GPU parameter server is 1.8-4.8X\nfaster than the MPI solution in the production environment.\nThe cost of 4 GPU nodes is much less than the cost of main-\ntaining 75-150 CPU nodes in the MPI cluster. After normal-\nizing the execution time by the hardware and maintenance\ncost, the price-performance ratio of our proposed system is\n4.4-9.0X better than the MPI solution. Besides the training\nspeed, the relative accuracy difference to the MPI solution\nis within 0.1%\u2014the training in our proposed system is loss-\nless. The proposed pipeline parallels all training stages.\nThe overall execution time for each batch is dominated by\nthe HDFS when the model is small. When the number of\nsparse parameters grows to 6 and 10 TB, the sparse param-\neter pulling and pushing operations become the dominant\nfactor. The execution time in the HBM-PS depends on the\nmodel speci\ufb01cations. A larger number of non-zero values\nof training inputs yields longer execution time for pulling\nand pushing parameters in the HBM-PS. The training for-\nward and backward propagation time depend on the size of\nfully-connected layers in the deep model. The execution\ntime of pulling working parameters in the MEM-PS is al-\nmost independent of the number of nodes. Although the\nlocal data required to load from SSDs are reduced when we\nhave multiple nodes, more parameter requests from remote\nnodes have to be processed. In expectation, the average\nnumber of parameters to load from SSDs stays the same\nin multi-node cases. The SSD-PS performance has slight\n\ufb02uctuations when the parameter \ufb01le compaction operation\ninvolves in. Our proposed system scales well in distributed\nenvironments\u2014a training throughput speedup of 3.57 out\nof 4 (the ideal speedup) is obtained for 4 nodes.\n8\nRELATED WORK\nIn this section, we discuss relevant work from CTR predic-\ntion models and parameter servers. Additional related work\nabout in-memory cache management systems and key-value\nstores on SSDs are discussed in Appendix F.\nCTR prediction models. The large scale logistic regression\nmodel with careful feature engineering used to dominate the\nCTR prediction strategies (Edelman et al., 2007; Graepel\net al., 2010). Recently, deep neural networks with embed-\nding layers are applied in the CTR prediction problem and\nobtain signi\ufb01cant improvements. Generally, the sparse input\nfeatures are converted into dense vectors through embedding\nlayers, and then feed into neural components to expose the\ncorrelation/interaction among them. The differences among\nthese models lie in the neural components above the embed-\nding layer. For example, Deep Crossing (Shan et al., 2016),\nWide&Deep Learning (Cheng et al., 2016), YouTube Rec-\nommendation CTR model (Covington et al., 2016) and Deep\nInterest Network (DIN) (Zhou et al., 2018) design special\nfully-connected layers for corresponding tasks to capture the\nlatent interactions among features; Product-based Neural\nNetwork (PNN) (Qu et al., 2016) employs a product layer\nto capture high-order feature interactions; DeepFM (Guo\net al., 2017) and xDeepFM (Lian et al., 2018) use factoriza-\ntion machines (FM) to model both low-order and high-order\nDistributed Hierarchical GPU Parameter Server for Massive Scale Deep Learning Ads Systems\nfeature correlations. Through this approach, the correlations\nbetween sparse features are automatically exploited.\nThe successes of these systems have proven that deep learn-\ning with sparse input is an effective approach for real-world\ncommercial advertisement and recommendation systems.\nHowever, when the scale of the problem becomes extremely\nhuge, for example, with 1011 dimensional sparse data and\ntrillions of samples, the architecture of these models must\nbe carefully designed to cope with the challenges mentioned\nin above sections. This paper is our attempts to solving\nthese challenges using an SSD based, three-layer distributed\nGPU training architecture. We believe our solution greatly\nenhances the feasibility of the methodology of deep learning\nwith sparse input in much larger scale problems.\nParameter servers. One important research direction is\nthe parallel mechanism among the computational nodes\nand server nodes. One such topic is the synchronization\npattern of local updates of parameters, where three typical\ntypes of synchronization patterns are proposed: 1) Bulk\nSynchronous Parallel (BSP) (Valiant, 1990), which strictly\nsynchronizes all local updates from all computing nodes\nin each iteration, and thus the learning procedure acts as\nthe same logic as that of learning in a single machine. 2)\nStale Synchronous Parallel (SSP) (Ho et al., 2013), which\nallows the fastest computing node to run a certain num-\nber of iterations ahead of other nodes. 3) Asynchronous\nParallel (ASP), which does not need any synchronization\namong the updates of computing nodes. Instead of using\nthe same parallel strategy throughout the training procedure,\nFlexPS (Huang et al., 2018) proposed a \ufb02exible parallelism\ncontrol for multi-stage learning algorithms such as SVRG.\nAnother direction is to design ef\ufb01cient parameter server\narchitecture for different hardware/network situations. Com-\nmunication is a major bottleneck in parallel training. Posei-\ndon (Zhang et al., 2017) develops two strategies wait-free\nback-propagation and hybrid communication to exploit the\nindependency between parameters of layered model struc-\ntures in deep neural networks and conquer communication\nbottleneck; while Gaia (Hsieh et al., 2017) even considered\nthe problem of running learning program on geo-distributed\nclusters that communicate over WANs. Another challenge\nis the big model size for GPU platforms. When the model\nbecomes bigger, the limited GPU memory cannot hold the\nentire model. GeePS (Cui et al., 2016) builds a two-layer\narchitecture to cope with the problems in this situation, in-\ncluding data movement overheads, GPU stalls, and limited\nGPU memory, where the Memory layer holds the entire\nmodel in memory, and the GPU layer fetch the required\npart of the model from the memory layer, do the compu-\ntation and rewrite the updates to the memory layer. Our\nproposed system constructs a distributed hash table across\nGPU HBMs that enables direct GPU peer-to-peer communi-\ncations. It reduces the excessive CPU-GPU data movement\nand synchronization overhead in GeePS.\nHowever, when the model size becomes even larger, the\nparameters cannot even be stored in the CPU main memory\nof the cluster. Our solution to the challenge is a three-layer\narchitecture that consists of SSD-PS, MEM-PS layer, and\nHBM-PS. The new architecture results in new challenges\nsuch as GPU I/O problems and we develop strategies to\noptimize the performance. Our architecture can ef\ufb01ciently\ntrain a 10 TB model, which greatly enhances the capability\nof GPU clusters to cope with massive scale models.\n9\nCONCLUSIONS\nSince 2013, Baidu Search Ads (a.k.a. \u201cPhoenix Nest\u201d)\nhas been successfully using ultra-high dimensional input\ndata and ultra-large-scale deep neural networks for training\nCTR prediction models. In this paper, we introduce the\narchitecture of a distributed hierarchical GPU parameter\nserver for massive scale deep learning ads systems. As\ndiscussed in Section 2, model hashing techniques such as\n\u201cone-permutation + one sign random projection\u201d are not fully\napplicable to the accuracy-crucial ads industry applications.\nThe deep learning model parameter size can become huge\nand cannot \ufb01t in the CPU/GPU memory. The proposed\nhierarchical design employs the main memory and SSDs\nto, respectively, store the out-of-GPU-memory and out-of-\nmain-memory parameters of massive scale neural networks.\nWe perform an extensive set of experiments on 5 CTR pre-\ndiction models in real-world online sponsor advertising ap-\nplications. The results con\ufb01rm the effectiveness and the\nscalability of the proposed system. The 4-node distributed\nhierarchical GPU parameter server is 1.8-4.8X faster than\nthe MPI solution in the production environment. For exam-\nple, a 4-node hierarchical GPU parameter server can train a\nmodel more than 2X faster than a 150-node in-memory dis-\ntributed parameter server in an MPI cluster on Model D. The\ncost of 4 GPU nodes is much less than the cost of maintain-\ning an MPI cluster of 75-150 CPU nodes. After normalizing\nthe execution time by the hardware cost and maintenance\nexpenditure, the price-performance ratio of this proposed\nsystem is 4.4-9.0X better than the previous MPI solution.\nThe system described in this paper is being integrated with\nthe PaddlePaddle deep learning platform (https://www.\npaddlepaddle.org.cn) to become the \u201cPaddleBox\u201d.\nACKNOWLEDGEMENT\nThe system described in this paper represents the substantial\neffort in the past years involving several major product teams\nat Baidu Inc. We could not include all the names we should\nacknowledge but only a few: Xuewu Jiao, Quanxiang Jia,\nLian Zhao, Lin Liu, Jiajun Zhang, Yue Wang, Anlong Qi.\nDistributed Hierarchical GPU Parameter Server for Massive Scale Deep Learning Ads Systems\nREFERENCES\nMarc Abrams, Charles R. Standridge, Ghaleb Abdulla,\nStephen M. Williams, and Edward A. Fox.\nCaching\nproxies: Limitations and potentials. World Wide Web\nJournal, 1(1), 1996.\nDan Alistarh, Christopher De Sa, and Nikola Konstantinov.\nThe convergence of stochastic gradient descent in asyn-\nchronous shared memory. In Proceedings of the 2018\nACM Symposium on Principles of Distributed Computing\n(PODC), pp. 169\u2013178, New York, NY, 2018.\nAshok Anand, Chitra Muthukrishnan, Steven Kappes,\nAditya Akella, and Suman Nath. Cheap and large cams\nfor high performance data-intensive networked systems.\nIn Proceedings of the 7th USENIX Symposium on Net-\nworked Systems Design and Implementation (NSDI), pp.\n433\u2013448, San Jose, CA, 2010.\nDavid G. Andersen, Jason Franklin, Michael Kaminsky,\nAmar Phanishayee, Lawrence Tan, and Vijay Vasudevan.\nFawn: A fast array of wimpy nodes. In Proceedings of the\n22nd ACM Symposium on Operating Systems Principles\n(SOSP), pp. 1\u201314, Big Sky, MT, 2009.\nAndrei Broder. A taxonomy of web search. SIGIR Forum,\n36(2):3\u201310, 2002.\nPei Cao and Sandy Irani. Cost-aware WWW proxy caching\nalgorithms. In 1st USENIX Symposium on Internet Tech-\nnologies and Systems (USITS), Monterey, CA, 1997.\nMoses Charikar, Kevin Chen, and Martin Farach-Colton.\nFinding frequent items in data streams. Theor. Comput.\nSci., 312(1):3\u201315, 2004.\nMoses S. Charikar. Similarity estimation techniques from\nrounding algorithms. In Proceedings on 34th Annual\nACM Symposium on Theory of Computing (STOC), pp.\n380\u2013388, Montreal, Canada, 2002.\nHeng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal\nShaked, Tushar Chandra, Hrishi Aradhye, Glen Ander-\nson, Greg Corrado, Wei Chai, Mustafa Ispir, Rohan Anil,\nZakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu,\nand Hemal Shah. Wide & deep learning for recommender\nsystems. In Proceedings of the 1st Workshop on Deep\nLearning for Recommender Systems, (DLRS@RecSys),\npp. 7\u201310, Boston, MA, 2016.\nTrishul M. Chilimbi, Yutaka Suzue, Johnson Apacible, and\nKarthik Kalyanaraman. Project adam: Building an ef\ufb01-\ncient and scalable deep learning training system. In 11th\nUSENIX Symposium on Operating Systems Design and\nImplementation (OSDI), pp. 571\u2013582, Broom\ufb01eld, CO,\n2014.\nHong-Tai Chou and David J DeWitt. An evaluation of buffer\nmanagement strategies for relational database systems.\nAlgorithmica, 1(1-4):311\u2013336, 1986.\nGraham Cormode and S. Muthukrishnan. An improved data\nstream summary: the count-min sketch and its applica-\ntions. Journal of Algorithm, 55(1):58\u201375, 2005.\nPaul Covington, Jay Adams, and Emre Sargin. Deep neural\nnetworks for youtube recommendations. In Proceedings\nof the 10th ACM Conference on Recommender Systems\n(RecSys), pp. 191\u2013198, Boston, MA, 2016.\nHenggang Cui, James Cipar, Qirong Ho, Jin Kyu Kim, Se-\nunghak Lee, Abhimanu Kumar, Jinliang Wei, Wei Dai,\nGregory R. Ganger, Phillip B. Gibbons, Garth A. Gibson,\nand Eric P. Xing. Exploiting bounded staleness to speed\nup big data analytics. In 2014 USENIX Annual Technical\nConference (USENIX ATC), pp. 37\u201348, Philadelphia, PA,\n2014.\nHenggang Cui, Hao Zhang, Gregory R. Ganger, Phillip B.\nGibbons, and Eric P. Xing. Geeps: Scalable deep learning\non distributed GPUs with a GPU-specialized parameter\nserver. In Proceedings of the Eleventh European Con-\nference on Computer Systems (EuroSys), pp. 4:1\u20134:16,\nLondon, UK, 2016.\nShaul Dar, Michael J. Franklin, Bj\u00f6rn \u00de\u00f3r J\u00f3nsson, Divesh\nSrivastava, and Michael Tan.\nSemantic data caching\nand replacement. In Proceedings of 22th International\nConference on Very Large Data Bases (VLDB), pp. 330\u2013\n341, Mumbai (Bombay), India, 1996.\nBiplob K. Debnath, Sudipta Sengupta, and Jin Li. Flash-\nstore:\nHigh throughput persistent key-value store.\nPVLDB, 3(2):1414\u20131425, 2010.\nBiplob K. Debnath, Sudipta Sengupta, and Jin Li. Skimpys-\ntash: Ram space skimpy key-value store on \ufb02ash-based\nstorage. In Proceedings of the ACM SIGMOD Interna-\ntional Conference on Management of Data (SIGMOD),\npp. 25\u201336, Athens, Greece, 2011.\nBenjamin Edelman, Michael Ostrovsky, and Michael\nSchwarz. Internet advertising and the generalized second-\nprice auction: Selling billions of dollars worth of key-\nwords. American economic review, 97(1):242\u2013259, 2007.\nAssaf Eisenman, Maxim Naumov, Darryl Gardner, Misha\nSmelyanskiy, Sergey Pupyrev, Kim Hazelwood, Asaf\nCidon, and Sachin Katti. Bandana: Using non-volatile\nmemory for storing deep learning models. arXiv preprint\narXiv:1811.05922, 2018.\nDaniel C. Fain and Jan O. Pedersen. Sponsored search:\nA brief history. Bulletin of the American Society for\nInformation Science and Technology, 32(2):12\u201313, 2006.\nDistributed Hierarchical GPU Parameter Server for Massive Scale Deep Learning Ads Systems\nMiao Fan, Jiacheng Guo, Shuai Zhu, Shuo Miao, Mingming\nSun, and Ping Li. MOBIUS: towards the next generation\nof query-ad matching in baidu\u2019s sponsored search. In\nProceedings of the 25th ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining,\n(KDD), pp. 2509\u20132517, Anchorage, AK, 2019.\nDenis Foley and John Danskin. Ultra-performance pascal\nGPU and nvlink interconnect. IEEE Micro, 37(2):7\u201317,\n2017.\nMichel X. Goemans and David P. Williamson. Improved\napproximation algorithms for maximum cut and satis\ufb01a-\nbility problems using semide\ufb01nite programming. Journal\nof ACM, 42(6):1115\u20131145, 1995.\nThore Graepel, Joaquin Qui\u00f1onero Candela, Thomas\nBorchert, and Ralf Herbrich. Web-scale bayesian click-\nthrough rate prediction for sponsored search advertising\nin microsoft\u2019s bing search engine. In Proceedings of\nthe 27th International Conference on Machine Learning\n(ICML), pp. 13\u201320, Haifa, Israel, 2010.\nHuifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and\nXiuqiang He. Deepfm: A factorization-machine based\nneural network for ctr prediction. In Proceedings of the\nTwenty-Sixth International Joint Conference on Arti\ufb01cial\nIntelligence (IJCAI), pp. 1725\u20131731, Melbourne, Aus-\ntralia, 2017.\nQirong Ho, James Cipar, Henggang Cui, Seunghak Lee,\nJin Kyu Kim, Phillip B. Gibbons, Garth A. Gibson, Gre-\ngory R. Ganger, and Eric P. Xing. More effective dis-\ntributed ml via a stale synchronous parallel parameter\nserver. In Advances in Neural Information Processing\nSystems (NIPS), pp. 1223\u20131231, Lake Tahoe, NV, 2013.\nKevin Hsieh, Aaron Harlap, Nandita Vijaykumar, Dimitris\nKonomis, Gregory R. Ganger, Phillip B. Gibbons, and\nOnur Mutlu. Gaia: Geo-distributed machine learning\napproaching lan speeds. In 14th USENIX Symposium on\nNetworked Systems Design and Implementation (NSDI),\npp. 629\u2013647, Boston, MA, 2017.\nJin Huang and Charles X Ling. Using auc and accuracy in\nevaluating learning algorithms. IEEE Transactions on\nKnowledge and Data Engineering (TKDE), 17(3):299\u2013\n310, 2005.\nYuzhen Huang, Tatiana Jin, Yidi Wu, Zhenkun Cai, Xiao\nYan, Fan Yang, Jinfeng Li, Yuying Guo, and James Cheng.\nFlexps: Flexible parallelism control in parameter server\narchitecture. PVLDB, 11(5):566\u2013579, 2018.\nTim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and\nNeoklis Polyzotis. The case for learned index structures.\nIn Proceedings of the 2018 International Conference on\nManagement of Data (SIGMOD), pp. 489\u2013504, Houston,\nTX, 2018.\nDonghee Lee, Jongmoo Choi, Jong-Hun Kim, Sam H Noh,\nSang Lyul Min, Yookun Cho, and Chong Sang Kim. Lrfu:\nA spectrum of policies that subsumes the least recently\nused and least frequently used policies. IEEE Transac-\ntions on Computers, 50(12):1352\u20131361, 2001.\nMu Li, David G. Andersen, Jun Woo Park, Alexander J.\nSmola, Amr Ahmed, Vanja Josifovski, James Long, Eu-\ngene J. Shekita, and Bor-Yiing Su. Scaling distributed\nmachine learning with the parameter server.\nIn 11th\nUSENIX Symposium on Operating Systems Design and\nImplementation (OSDI), pp. 583\u2013598, Broom\ufb01eld, CO,\n2014.\nPing Li, Anshumali Shrivastava, Joshua Moore, and\nArnd Christian K\u00f6nig. Hashing algorithms for large-scale\nlearning. In Advances in Neural Information Processing\nSystems (NIPS), pp. 2672\u20132680, Granada, Spain, 2011.\nPing Li, Art B Owen, and Cun-Hui Zhang. One permutation\nhashing. In Advances in Neural Information Processing\nSystems (NIPS), pp. 3122\u20133130, Lake Tahoe, NV, 2012.\nPing Li, Xiaoyun Li, and Cun-Hui Zhang. Re-randomized\ndensi\ufb01cation for one permutation hashing and bin-wise\nconsistent weighted sampling. In Advances in Neural\nInformation Processing Systems (NeurIPS), Vancouver,\nCanada, 2019.\nJianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, Zhongxia\nChen, Xing Xie, and Guangzhong Sun. xdeepfm: Com-\nbining explicit and implicit feature interactions for rec-\nommender systems. In Proceedings of the 24th ACM\nSIGKDD International Conference on Knowledge Dis-\ncovery & Data Mining (KDD), pp. 1754\u20131763, London,\nUK, 2018.\nHyeontaek Lim, Bin Fan, David G. Andersen, and Michael\nKaminsky. Silt: A memory-ef\ufb01cient, high-performance\nkey-value store. In Proceedings of the 23rd ACM Sympo-\nsium on Operating Systems Principles (SOSP), pp. 1\u201313,\nCascais, Portugal, 2011.\nLanyue Lu, Thanumalayan Sankaranarayana Pillai, Hari-\nharan Gopalakrishnan, Andrea C Arpaci-Dusseau, and\nRemzi H Arpaci-Dusseau. Wisckey: Separating keys\nfrom values in ssd-conscious storage. ACM Transactions\non Storage (TOS), 13(1):5, 2017.\nLiang Luo, Jacob Nelson, Luis Ceze, Amar Phanishayee,\nand Arvind Krishnamurthy. Parameter hub: a rack-scale\nparameter server for distributed deep neural network train-\ning. In Proceedings of the ACM Symposium on Cloud\nComputing (SoCC), pp. 41\u201354, Carlsbad, CA, 2018.\nDistributed Hierarchical GPU Parameter Server for Massive Scale Deep Learning Ads Systems\nShamkant Navathe et al. Vertical partitioning algorithms for\ndatabase design. ACM Transactions on Database Systems\n(TODS), 9(4):680\u2013710, 1984.\nElizabeth J. O\u2019Neil, Patrick E. O\u2019Neil, and Gerhard Weikum.\nThe lru-k page replacement algorithm for database disk\nbuffering. In Proceedings of the 1993 ACM SIGMOD\nInternational Conference on Management of Data (SIG-\nMOD), pp. 297\u2013306, Washington, DC, 1993.\nSreeram Potluri, Khaled Hamidouche, Akshay Venkatesh,\nDevendar Bureddy, and Dhabaleswar K. Panda. Ef\ufb01cient\ninter-node mpi communication using gpudirect RDMA\nfor in\ufb01niband clusters with nvidia GPUs. In 42nd Inter-\nnational Conference on Parallel Processing (ICPP), pp.\n80\u201389, Lyon, France, 2013.\nYanru Qu, Han Cai, Kan Ren, Weinan Zhang, Yong Yu,\nYing Wen, and Jun Wang. Product-based neural networks\nfor user response prediction. In IEEE 16th International\nConference on Data Mining (ICDM), pp. 1149\u20131154,\nBarcelona, Spain, 2016.\nYing Shan, T. Ryan Hoens, Jian Jiao, Haijing Wang, Dong\nYu, and J. C. Mao. Deep crossing: Web-scale model-\ning without manually crafted combinatorial features. In\nProceedings of the 22nd ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining\n(KDD), pp. 255\u2013262, San Francisco, CA, 2016.\nAnshumali Shrivastava and Ping Li. Improved densi\ufb01cation\nof one permutation hashing. In Proceedings of the Thirti-\neth Conference on Uncertainty in Arti\ufb01cial Intelligence\n(UAI), pp. 732\u2013741, Quebec City, Canada, 2014.\nLeonid B. Sokolinsky. Lfu-k: An effective buffer manage-\nment replacement algorithm. In 9th International Con-\nference on Database Systems for Advances Applications\n(DASFAA), pp. 670\u2013681, Jeju Island, Korea, 2004.\nShulong Tan, Zhixin Zhou, Zhaozhuo Xu, and Ping Li. Fast\nitem ranking under neural network based measures. In\nProceedings of the 13th ACM International Conference\non Web Search and Data Mining (WSDM), Huston, TX,\n2020.\nLeslie G. Valiant. A bridging model for parallel computation.\nCommun. ACM, 33(8):103\u2013111, 1990.\nKilian Weinberger, Anirban Dasgupta, John Langford, Alex\nSmola, and Josh Attenberg. Feature hashing for large\nscale multitask learning.\nIn Proceedings of the 26th\nAnnual International Conference on Machine Learning\n(ICML), pp. 1113\u20131120, Montreal, Canada, 2009.\nRoland P. Wooster and Marc Abrams. Proxy caching that\nestimates page load delays. Computer Networks, 29(8-\n13):977\u2013986, 1997.\nHao Zhang, Zeyu Zheng, Shizhen Xu, Wei Dai, Qirong Ho,\nXiaodan Liang, Zhiting Hu, Jinliang Wei, Pengtao Xie,\nand Eric P. Xing. Poseidon: An ef\ufb01cient communication\narchitecture for distributed deep learning on GPU clusters.\nIn 2017 USENIX Annual Technical Conference (USENIX\nATC), pp. 181\u2013193, Santa Clara, CA, 2017.\nWeijie Zhao, Yu Cheng, and Florin Rusu. Vertical partition-\ning for query processing over raw data. In Proceedings of\nthe 27th International Conference on Scienti\ufb01c and Statis-\ntical Database Management, (SSDBM), pp. 15:1\u201315:12,\nLa Jolla, CA, 2015.\nWeijie Zhao, Jingyuan Zhang, Deping Xie, Yulei Qian,\nRonglai Jia, and Ping Li. AIBox: CTR prediction model\ntraining on a single node. In Proceedings of the 28th ACM\nInternational Conference on Information and Knowledge\nManagement (CIKM), pp. 319\u2013328, Beijing, China, 2019.\nWeijie Zhao, Shulong Tan, and Ping Li. Song: Approximate\nnearest neighbor search on gpu. In 35th IEEE Interna-\ntional Conference on Data Engineering (ICDE), Dallas,\nTX, 2020.\nGuorui Zhou, Xiaoqiang Zhu, Chengru Song, Ying Fan,\nHan Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and\nKun Gai. Deep interest network for click-through rate\nprediction. In Proceedings of the 24th ACM SIGKDD\nInternational Conference on Knowledge Discovery &\nData Mining (KDD), pp. 1059\u20131068, London, UK, 2018.\nZhixin Zhou, Shulong Tan, Zhaozhuo Xu, and Ping Li.\nM\u00f6bius transformation for fast inner product search on\ngraph. In Advances in Neural Information Processing\nSystems (NeurIPS), pp. 8216\u20138227, Vancouver, Canada,\n2019.\nDistributed Hierarchical GPU Parameter Server for Massive Scale Deep Learning Ads Systems\n50, 56, 61\n61, 87\n4, 61\n5, 56\n11, 87\n98\nmini-batch1\nmini-batch2\nworking: 4, 5, 11, 50, 53, 56, 61, 87, 98\nNode1:\n1, 3, 5, \u2026, 97, 99\nNode2:\n2, 4, 6, \u2026, 98, 100\n5, 11, 53, 61, 87\n4, 50, 56, 98 \nMEM:\npull local\nMEM-PS/SSD-PS\npull remote\nMEM-PS\nGPU1: 4, 5, 11, 50 GPU2: 53, 56, 61, 87, 98 \npartition parameters\n11, 87\n98\nGPU1\nmini-batch1\n11\n           87, 98\npull local HBM-PS\npull remote HBM-PS\nforward/backward \npropagation\n87\n4, 53\nmini-batch3\nmini-batch4\n:\nWorker1\n \n \nSSD:\n \nHBM:\nFigure 6. An example for Algorithm 1.\nA\nHIERARCHICAL PARAMETER SERVER\nWORKFLOW EXAMPLE\nExample. Figure 6 depicts an example for the training work-\n\ufb02ow (Algorithm 1). Consider now we are at node1. An input\nbatch is streamed from HDFS and is divided into 4 mini-\nbatches. The working parameters of the current batch are:\n4, 5, 11, 50, 53, 56, 61, 87, 98. Parameters are sharded and\nstored on the SSDs of each node. We have 2 nodes and shard\nthe parameters in a round-robin method in this example\u2014\nnode1 stores the parameters with odd keys while node2\nstores the ones with even keys. Here we have 100 total pa-\nrameters in this example\u2014there are 1011 parameters in real-\nworld large-scale deep learning models. 5, 11, 53, 61, 87 are\nstored on the local node\u2014node1. We pull these parame-\nters from the local MEM-PS (for the cached parameters)\nand the local SSD-PS. For the parameters that stored on\nother nodes\u20144, 50, 56, 98, we pull these parameters from\nthe MEM-PS on node2 through the network. The MEM-\nPS on node2 interacts with its memory cache and its local\nSSD-PS to load the requested parameters. Now all the work-\ning parameters are retrieved and are stored in the memory\nof node1. Here we have 2 GPUs on node1. The working\nparameters are partitioned and transferred to GPU HBMs.\nIn this example, GPU1 obtains the parameters whose keys\nare less than or equal to 50\u20144, 5, 11, 50, and GPU2 takes\n53, 56, 61, 87, 98. The partition strategy can be any hashing\nfunction that maps a parameter key to a GPU id. Consider\nthe worker1 of GPU1 is responsible to process mini-batch1.\nworker1 is required to load 11, 87 and 98. Among them,\n11 is stored in the HBM of the local GPU\u2014GPU1. 87 and\n98 are pulled from GPU2. Since the GPUs are connected\nwith high-speed interconnections\u2014NVLink, the inter-GPU\ndata transfer has low-latency and high-bandwidth. After the\nparameters are ready in the working memory of worker1,\nwe can perform the neural network forward and backward\npropagation operations to update the parameters. All the\nupdated parameters are synchronized among all GPUs on all\nnodes after each mini-batch is \ufb01nished. When all the mini-\nbatches are \ufb01nished, the MEM-PS on each node pulls back\nthe updated parameters and materializes them onto SSDs.\nB\n4-STAGE PIPELINE EXAMPLE\nNetwork\nCPU\nSSD\nGPU\nNetwork\nCPU\nSSD\nGPU\nNetwork\nCPU\nSSD\nGPU\nNetwork\nCPU\nSSD\nNetwork\nCPU\nSSD\nGPU\nNetwork\nCPU\nNetwork\nCPU\nSSD\nGPU\nNetwork\nPull/push\nMEM-PS\nPartition\nparameters\nLoad/dump\nparameters Training\nTimeline\nFigure 7. The 4-stage pipeline.\nFigure 7 is an illustration of the 4-stage pipeline. For ex-\nample, when the GPUs are busy training the model, our\n4-stage pipeline enables the proposed system to prepare\nthe referenced parameters of the next training batch at the\nsame time: the HBM-PS pulls remote parameters from other\nnodes and the SSD-PS loads local parameters for the next\nbatch simultaneously. After the training of the current batch\nis \ufb01nished, all the required parameters of the next batch are\nready to use in the GPU HBM\u2014GPUs are able to train the\nnext batch immediately.\nC\nHBM-PS IMPLEMENTATION\nC.1\nMulti-GPU Distributed Hash Table\nPartition policy. A partition policy that maps a parame-\nter key to a GPU id is required to partition the parameters.\nA simple modulo hash function yields a balanced parti-\ntioning in general cases, because the features of the input\ntraining data are usually distributed randomly. The modulo\nhash function can be computed ef\ufb01ciently with constant\nmemory space. As a trade-off of the memory footprint,\nthe disadvantage of the simple hash partition policy is that\nwe need to pull parameters from other GPUs if the param-\neters referenced in a mini-batch are not stored in the lo-\ncal parameter partition. One possible improvement is to\ngroup parameters with high co-occurrence together (Eisen-\nman et al., 2018), for example, pre-train a learned hash\nDistributed Hierarchical GPU Parameter Server for Massive Scale Deep Learning Ads Systems\nGPU\nHBM\nPCIe bus\nCPU\nMEM\nNetwork Card\nGPU\nHBM\nPCIe bus\nCPU\nMEM\nNetwork Card\nSender\nReceiver\nRoCE\nBaseline\nRDMA\nFigure 8. Inter-node RDMA communication.\nfunction (Kraska et al., 2018) to maximize the parameter\nco-occurrence. It is another research axis\u2013vertical partition-\ning (Navathe et al., 1984; Zhao et al., 2015) that is beyond\nthe scope of this paper. Generally, no perfect balanced parti-\ntion solution exists for random inputs. Although the number\nof pulled parameters is reduced, we still have to pull param-\neters from almost all other GPUs even with an optimized\npartition policy. Besides, transferring a large batch of data\ncan better utilize the NVLink bandwidth\u2014the disadvantage\nof the simple hash function partition policy is reduced.\nC.2\nGPU RDMA Communication\nGPU Communication mechanism. The inter-node GPU\ncommunication mechanism is depicted in Figure 8. Two\nnodes are shown in the \ufb01gure\u2014the left node is the sender\nand the right one is the receiver. For each node, the CPU,\nGPU and network card are connected with a PCIe bus.\nWe \ufb01rst examine the baseline method before we introduce\nour RDMA solution. In the \ufb01gure, the data \ufb02ow of the base-\nline method is represented as the dashed line starting from\nthe sender HBM to the receiver HBM. The CPU calls the\nGPU driver to copy the data from GPU HBM into the CPU\nmemory. Then, the CPU reads the data in the memory and\ntransmits the data through the network. The transmitted\ndata are stored in the memory of the receiver node. Finally,\nthe receiver CPU transfers the in-memory data to the GPU\nHBM. In this baseline method, the CPU memory is uti-\nlized as a buffer to store the communication data\u2014it incurs\nunnecessary data copies and CPU consumptions.\nOur RDMA hardware design eliminates the involvement\nof the CPU and memory. Its data \ufb02ow is represented as\na solid line in the \ufb01gure. Remote Direct Memory Access\n(RDMA) (Potluri et al., 2013) enables zero-copy network\ncommunication\u2014it allows the network card to transfer data\nfrom a device memory directly to another device memory\nwithout copying data between the device memory and the\ndata buffers in the operating system. The RDMA data trans-\nfer demands no CPU consumption or context switches. The\nnetwork protocol\u2013RDMA over Converged Ethernet (RoCE)\u2013\nis employed to allow RDMA over the Ethernet network. The\nsender GPU driver1 directly streams the data in HBM to the\nnetwork, while the receiver network card directly stores the\ncollected data into the GPU HBM.\nGPU1\nGPU1\nGPU1\nGPU1\nNode1\nNode2\nNode3\nNode4\n\u2026 ...\n\u2026 ...\n\u2026 ...\n\u2026 ...\n1\n1\n2\n2\nGPU8\nGPU8\nGPU8\nGPU8\n1\n1\n2\n2\n3\n3\n3\n3\nFigure 9. All-reduce communication.\nC.3\nInter-Node GPU Communication\nAll-reduce communication. The parameter synchroniza-\ntion requires an all-reduce communication\u2014each GPU\nneeds to receive all parameter updates from other GPUs\nand then performs a reduction to accumulate these updates.\nFigure 9 presents an example communication work\ufb02ow. 4\nnodes are shown in this example. Each node contains 8\nGPUs. Initially, the GPUs on Node1 exchange their parame-\nter updates with their corresponding GPUs on Node2 (step\n1 )\u2014i.e., the ith GPU on Node1 communicates with the ith\nGPU on Node2. Meanwhile, the GPUs with the same id on\nNode3 and Node4 share their data with each other. Then,\nthe GPUs on Node1 perform the communication with the\nones on Node3 (step 2 ). Likewise, the GPUs on Node2\nand Node4 perform the same pattern communication in par-\nallel. After these two steps, each GPU on each node has\ncollected all the parameter updates of its corresponding\n1https://docs.nvidia.com/cuda/\ngpudirect-rdma/index.html\nDistributed Hierarchical GPU Parameter Server for Massive Scale Deep Learning Ads Systems\nGPUs on other nodes. An intra-node GPU tree all-reduce\ncommunication2 is executed to share the data across all 8\nGPUs on the same node (step 3 ). Most of the communi-\ncations are paralleled\u2014log2 #nodes non-parallel inter-node\nand log2 #GPUs intra-node all-reduce communications are\nrequired to synchronize the parameters across all nodes.\nC.4\nDense Parameters\nAs we discussed in the CTR prediction neural network ex-\nample (Figure 1), besides the large-scale sparse parameters,\nthere are a small number of dense parameters for the fully-\nconnected layers. For any sparse input, all the dense parame-\nters are referenced and updated. Therefore, we can pin these\ndense parameters in the HBM of all GPUs at the beginning\nof the training for better training performance. In extreme\ncases\u2014we have insuf\ufb01cient HBM memory to replicate the\ndense parameters, we can shard the dense parameters as the\nsparse parameters and distribute them across all GPU HBMs.\nThe dense parameters are also synchronized as the sparse\nones in the HBM-PS after each mini-batch is processed.\nD\nMEM-PS IMPLEMENTATION\nCache policy. We target to cache the most recently and the\nmost frequently used parameters in the memory to reduce\nSSD I/Os. In this paper, we leverage a cache eviction policy\nthat combines two cache methods\u2014Least Recently Used\n(LRU) (O\u2019Neil et al., 1993) and Least Frequently Used\n(LFU) (Sokolinsky, 2004). Whenever we visit a parameter,\nwe add it into an LRU cache. For the evicted parameters\nfrom the LRU cache, we insert them into an LFU cache. The\nevicted parameters from the LFU cache are collected\u2014we\nhave to \ufb02ush them into SSDs before releasing their memory.\nTo guarantee the data integrity of our pipeline, we pin the\nworking parameters of the current batch and the pre-fetched\nparameters of next iterations in the LRU cache\u2014they cannot\nbe evicted from the memory until their batch is completed.\nE\nSSD-PS IMPLEMENTATION\nLoad parameters. The SSD-PS gathers requested param-\neter keys from the MEM-PS and looks up the parameter-\nto-\ufb01le mapping to locate the parameter \ufb01les to read. We\nhave to read an entire parameter \ufb01le when it contains re-\nquested parameters\u2014a larger \ufb01le causes more unnecessary\nparameter readings. This is a trade-off between the SSD\nI/O bandwidth and the unnecessary parameter reading\u2014a\nsmall-size \ufb01le cannot fully utilize the SSD I/O bandwidth.\nWe tune the \ufb01le size to obtain the optimal performance. Fig-\nure 10(a) depicts an example of parameter \ufb01les on SSDs. In\n2https://docs.nvidia.com/deeplearning/\nsdk/nccl-developer-guide/docs/usage/\noperations.html#allreduce\nthe example, each parameter \ufb01le can store 3 parameters.\nDump parameters. Parameters evicted from the HBM-PS\ncache are required to be dumped onto SSDs. It is impracti-\ncal to locate these parameters and perform in-place updates\ninside the original \ufb01le\u2014it poorly utilizes the SSD I/O band-\nwidth because it requires us to randomly write the disk.\nInstead, our SSD-PS chunks these updated parameters into\n\ufb01les and writes them as new \ufb01les on SSDs\u2014data are sequen-\ntially written onto the disk. After the \ufb01les are written, we\nupdate the parameter-to-\ufb01le mapping of these parameters.\nThe older versions of the parameters stored in the previous\n\ufb01les become stale\u2014these older values will not be used since\nthe mapping is updated. In Figure 10(b), we present an ex-\nample for dumping parameters\u20141, 2, 4, 6, 8, 9 are updated\nand dumped to SSD-PS. We chunked them into two \ufb01les\nand write these two \ufb01les onto the SSD\u2014\ufb01le4 and \ufb01le5. The\nunderlined values\u2013the values of the updated parameters in\nthe old \ufb01les\u2013are stale.\nFile compaction. The SSD usage hikes as we keep cre-\nating new \ufb01les on SSDs. A \ufb01le compaction operation is\nperformed regularly to reduce the disk usage\u2014many old\n\ufb01les containing a large proportion of stale values can be\nmerged into new \ufb01les. We adopt the leveled compaction\nalgorithm of LevelDB 3 to create a lightweight \ufb01le merging\nstrategy. A worker thread runs in the background to check\nthe disk usage. When the usage reaches a pre-set thresh-\nold, the SSD-PS scans the old parameter \ufb01les, collects the\nnon-stale parameters, merges them into new \ufb01les, and erases\nthe old \ufb01les. The parameter-to-\ufb01le mapping of the merged\nparameters is also updated in the \ufb01le compaction operation.\nFigure 10(c) illustrates the \ufb01le compaction effects. Before\nthe compaction (Figure 10(b)), the stale values in \ufb01le1, \ufb01le2\nand \ufb01le3 occupy more than a half of the \ufb01le capacity. We\nscan these \ufb01les, merge the non-stale values into a new \ufb01le\n(\ufb01le6), and erase these \ufb01les (\ufb01le1 and \ufb01le2). The compaction\noperation may merge a large number of \ufb01les on SSDs. In\norder to reduce the excessive merging, we set a threshold\nto limit the number of merged \ufb01les\u2014we only merge \ufb01les\nthat contain more than 50% stale parameters. By employing\nthis threshold, we can limit the total SSD space usage\u2014the\nsize of all parameter \ufb01les will not exceed 2 times (1/50%)\nof the original non-stale parameter size. Note that we do\nnot need to read the entire \ufb01le to obtain the proportion of\nstale parameters\u2014a counter that counts the number of stale\nparameters is maintained as an auxiliary attribute for each\n\ufb01le. When we update the parameter-to-\ufb01le mapping, we\naccumulate the counter of the old \ufb01le it previously maps to.\nF\nADDITIONAL RELATED WORK\nIn-memory cache management. Many caching policies\nhave been developed for storage systems, such as the LRU-\n3https://github.com/google/leveldb\nDistributed Hierarchical GPU Parameter Server for Massive Scale Deep Learning Ads Systems\n1, 2, 3\n4, 5, 6\n7, 8, 9\n1\n2\n3\n4\n5\n6\n7\n8\n9\n1\n1\n1\n2\n2\n2\n3\n3\n3\n1, 2, 3\n4, 5, 6\n7, 8, 9\n1, 2, 4\n1\n2\n3\n4\n5\n6\n7\n8\n9\n4\n4\n1\n4\n2\n5\n3\n5\n5\n3, 5, 7\n1\n2\n3\n4\n5\n6\n7\n8\n9\n7\n7\n5\n5\n2\n2\n3\n6\n5\nfil e1\nfil e2\nfil e3\nfil e4\nfil e5\nfil e4\nfil e5\nfil e6\nparameter id\nSSD fil es\n6, 8, 9\nfil e1\nfil e2\nfil e3\n1, 2, 4\n6, 8, 9\n(a)\n(b)\n(c)\nfil e id\nFigure 10. SSD-PS examples: (a) parameter-to-\ufb01le mapping and parameter \ufb01les; (b) 1, 2, 4, 8, 9 are updated; (c) a compaction operation.\nK (O\u2019Neil et al., 1993), DBMIN (Chou & DeWitt, 1986),\nLRFU(Lee et al., 2001), and Semantic Caching (Dar et al.,\n1996). These algorithms evict cache according to a com-\nbined weight of recently used time-stamp and frequency.\nIn the web context, there is extensive work developed for\nvariable-size objects. Some of the most well-known algo-\nrithms in this space are Lowest-Latency-First(Wooster &\nAbrams, 1997), LRU-Threshold (Abrams et al., 1996), and\nGreedy-Dual-Size(Cao & Irani, 1997). Unlike our caching\nproblem, the parameter we tackle with has a \ufb01xed size and a\nclear access pattern in our CTR prediction model training\u2014\nsome parameters are frequently referenced. It is effective\nto keep those \u201chot parameters\u201d in the cache by applying an\nLFU eviction policy. While our additional LRU linked list\nmaintains the parameters referenced in the current pass to\naccelerate the hash table probing.\nKey-value store for SSDs. There is a signi\ufb01cant amount\nof work on key-value stores for SSD devices. The major\ndesigns (Andersen et al., 2009; Lim et al., 2011) follow\nthe paradigm that maintains an in-memory hash table and\nconstructs an append-only LSM-tree-like data structure on\nthe SSD for updates. FlashStore (Debnath et al., 2010)\noptimize the hash function for the in-memory index to com-\npact key memory footprints. SkimpyStash (Debnath et al.,\n2011) moves the key-value pointers in the hash table onto\nthe SSD. BufferHash (Anand et al., 2010) builds multiple\nhash tables with Bloom \ufb01lters for hash table selection. Wis-\ncKey (Lu et al., 2017) separates keys and values to minimize\nread/write ampli\ufb01cations. Our SSD-PS design follows the\nmainstream paradigm, while it is specialized for our training\nproblem. We do not need to confront the challenges to store\ngeneral keys and values. The keys are the index of parame-\nters that distributes uniformly. It is unnecessary to employ\nany sophisticated hashing functions. Also, the values have\na known \ufb01xed length, the serialized bucket on SSD exactly\n\ufb01ts in an SSD block\u2014I/O ampli\ufb01cation is minimized.\n",
        "context": "munication is a major bottleneck in parallel training. Posei-\ndon (Zhang et al., 2017) develops two strategies wait-free\nback-propagation and hybrid communication to exploit the\nindependency between parameters of layered model struc-\nenergy cost of a large-scale computing cluster is pricey. Be-\nsides, further scaling up the training speed is challenging\u2014\nin this large-scale distributed setting, communication and\nsynchronization cost between computing nodes limits the\ntures in deep neural networks and conquer communication\nbottleneck; while Gaia (Hsieh et al., 2017) even considered\nthe problem of running learning program on geo-distributed\nclusters that communicate over WANs. Another challenge"
    },
    {
        "id": 21,
        "title": "Time-based sequence model for personalization and recommendation systems",
        "author": [
            "T. Ishkhanov",
            "M. Naumov",
            "X. Chen",
            "Y. Zhu",
            "Y. Zhong",
            "A. G. Azzolini",
            "C. Sun",
            "F. Jiang",
            "A. Malevich",
            "L. Xiong"
        ],
        "year": "2020",
        "doi": "10.48550/arXiv.2008.11922",
        "in_text_citation": "[21]",
        "sentence": "Since our method and our theoretical framework is applicable to any arbitrary distributed system that uses lookup tables, we expect the same performance gains if we have used TBSM [21] on the Alibaba UBA dataset.",
        "abstract": "",
        "full_text": "",
        "context": null
    }
]