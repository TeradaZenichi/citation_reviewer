[
    {
        "title": "Fast high-dimensional filtering using the permutohedral lattice",
        "author": [
            "Adams",
            "Andrew",
            "Baek",
            "Jongmin",
            "Davis",
            "Myers Abraham"
        ],
        "venue": "In Computer Graphics Forum,",
        "citeRegEx": "Adams et al\\.,? \\Q2010\\E",
        "shortCiteRegEx": "Adams et al\\.",
        "year": 2010,
        "abstract": "",
        "full_text": "",
        "sentence": " Mean-field Inference for this form of pairwise terms can be efficiently implemented with high-dimensional filtering (Adams et al., 2010). Mean-field Inference for this form of pairwise terms can be efficiently implemented with high-dimensional filtering (Adams et al., 2010). Our modifications are very simple: firstly, we adapt the multi-resolution architecture outlined in the previous section to semantic segmentation. Using multi-resolution processing with tied-weights and performing late score fusion yielded substantially better results than using a single-resolution network: as shown in Table. 3 when combining the multi-scale network\u2019s output with DenseCRF inference, performance increases from 72.7 (single-scale counterpart of Chen et al. (2015)) or 73. Mean-field Inference for this form of pairwise terms can be efficiently implemented with high-dimensional filtering (Adams et al., 2010). Our modifications are very simple: firstly, we adapt the multi-resolution architecture outlined in the previous section to semantic segmentation. Using multi-resolution processing with tied-weights and performing late score fusion yielded substantially better results than using a single-resolution network: as shown in Table. 3 when combining the multi-scale network\u2019s output with DenseCRF inference, performance increases from 72.7 (single-scale counterpart of Chen et al. (2015)) or 73.9 (skip-layer multi-scale counterpart of Chen et al. (2015)) to 74. Mean-field Inference for this form of pairwise terms can be efficiently implemented with high-dimensional filtering (Adams et al., 2010). Our modifications are very simple: firstly, we adapt the multi-resolution architecture outlined in the previous section to semantic segmentation. Using multi-resolution processing with tied-weights and performing late score fusion yielded substantially better results than using a single-resolution network: as shown in Table. 3 when combining the multi-scale network\u2019s output with DenseCRF inference, performance increases from 72.7 (single-scale counterpart of Chen et al. (2015)) or 73.9 (skip-layer multi-scale counterpart of Chen et al. (2015)) to 74.8 (our multi-scale) in mean accuracy. Secondly, we integrate the boundary information extracted by our detector into the DenseCRF by using the eigenvectors computed by normalized Cuts to augment the RGB color features of Eq. 12, thereby conveying boundary-based proximity into DenseCRF inference. In particular we augment the dimensionality of Ii in Eq. 12 from 3 to 6, by concatenating the 3 eigenvectors delivered by NCuts with the RGB values. We observe that introducing the Normalized Cut eigenvectors into DenseCRF inference yields a clear improvement over an already high-performing system (from 74.8 to 75.4), while a small additional improvement was obtained we performing graph-cut inference with pairwise terms that depend on the boundary strength (from 75.4 to 75.7). Further improvements can be anticipated though an end-to-end training using the recursive CNN framework of Zheng et al. (2015) as in the currently leading works - we will explore this in future work.",
        "context": null
    },
    {
        "title": "Contour detection and hierarchical image segmentation",
        "author": [
            "Arbelaez",
            "Pablo",
            "Maire",
            "Michael",
            "Fowlkes",
            "Charless",
            "Malik",
            "Jitendra"
        ],
        "venue": null,
        "citeRegEx": "Arbelaez et al\\.,? \\Q2011\\E",
        "shortCiteRegEx": "Arbelaez et al\\.",
        "year": 2011,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " As detailed in Arbelaez et al. (2011) we can \u2018benchmark\u2019 humans against each other, by comparing every annotator to the \u2018committee\u2019 formed by the rest: if a user provides details that no committee member has provided these count as false positives, while if a user misses details provided by a committee member, these count as misses. (Dollar et al., 2006; Arbelaez et al., 2011; Ren, 2008; Kokkinos, 2010a; Ren & Bo, 2012; Doll\u00e1r & Zitnick, 2015), we use machine learning to optimize the performance of our boundary detector. , 2006; Arbelaez et al., 2011; Ren, 2008; Kokkinos, 2010a; Ren & Bo, 2012; Doll\u00e1r & Zitnick, 2015), we use machine learning to optimize the performance of our boundary detector. Recent works (Bertasius et al., 2015; Kivinen et al., 2014; Hwang & Liu, 2015) have shown hat DCNNs yield substantial improvements over flat classifiers; the Holistic Edge Detection approach of Xie & Tu (2015) recently achieved dramatic improvements over the previous state-of-the-art, from an F-measure of 0. in Doll\u00e1r & Zitnick (2015); Arbelaez et al. (2011). Even though the authors of HED use information from multiple scales by fusing the outputs of many layers, multi-resolution boundary detection can still help. To capture such information we use the Normalized Cuts (NCuts) technique of Shi & Malik (1997); Arbelaez et al. (2011). We treat the image as a weighted graph, where nodes corresponding to pixels and weights correspond to low-level affinity between pixels measured in terms of the Intervening Contour cue (Shi & Malik, 1997), where the contours are now estimated by our boundary detector. These embeddings can be used for boundary detection in terms of their directional derivatives, in order to provide some \u2018global\u2019 evidence for the presence of a boundary, known as the \u2018spectral probability of boundary\u2019 cue (Arbelaez et al., 2011). These embeddings can be used for boundary detection in terms of their directional derivatives, in order to provide some \u2018global\u2019 evidence for the presence of a boundary, known as the \u2018spectral probability of boundary\u2019 cue (Arbelaez et al., 2011). This further improves the performance of our detector, yielding an F-measure of 0.813, which is substantially better than our earlier performance of 0.807, and humans, who operate at 0.803. Due to time constraints we have used a very simple fusion scheme (addition of the posterior with SpectralPB) - we anticipate that adding a few processing layers can further improve performance. We summarize the impact of the different steps described above in Fig. 6 - starting from a baseline (that performs slightly worse than the HED system of Xie & Tu (2015) we have introduced a series of changes that resulted in super-human boundary detection performance. These embeddings can be used for boundary detection in terms of their directional derivatives, in order to provide some \u2018global\u2019 evidence for the presence of a boundary, known as the \u2018spectral probability of boundary\u2019 cue (Arbelaez et al., 2011). This further improves the performance of our detector, yielding an F-measure of 0.813, which is substantially better than our earlier performance of 0.807, and humans, who operate at 0.803. Due to time constraints we have used a very simple fusion scheme (addition of the posterior with SpectralPB) - we anticipate that adding a few processing layers can further improve performance. We summarize the impact of the different steps described above in Fig. 6 - starting from a baseline (that performs slightly worse than the HED system of Xie & Tu (2015) we have introduced a series of changes that resulted in super-human boundary detection performance. When compared to the current state-of-the-art method of Xie & Tu (2015) our method clearly dominates in terms of all typical performance measures, as shown in Table 2. Method ODS OIS AP gPb-owt-ucm (Arbelaez et al., 2011) 0.",
        "context": null
    },
    {
        "title": "Laplacian eigenmaps and spectral techniques for embedding and clustering",
        "author": [
            "Belkin",
            "Mikhail",
            "Niyogi",
            "Partha"
        ],
        "venue": "In NIPS,",
        "citeRegEx": "Belkin et al\\.,? \\Q2001\\E",
        "shortCiteRegEx": "Belkin et al\\.",
        "year": 2001,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Deepedge: A multi-scale bifurcated deep network for top-down contour detection",
        "author": [
            "Bertasius",
            "Gedas",
            "Shi",
            "Jianbo",
            "Torresani",
            "Lorenzo"
        ],
        "venue": "In Proc. CVPR,",
        "citeRegEx": "Bertasius et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "Bertasius et al\\.",
        "year": 2015,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Recent works (Bertasius et al., 2015; Kivinen et al., 2014; Hwang & Liu, 2015) have shown hat DCNNs yield substantial improvements over flat classifiers; the Holistic Edge Detection approach of Xie & Tu (2015) recently achieved dramatic improvements over the previous state-of-the-art, from an F-measure of 0. 784 DeepEdge (Bertasius et al., 2015) 0.",
        "context": null
    },
    {
        "title": "Visual Reconstruction",
        "author": [
            "Blake",
            "Andrew",
            "Zisserman"
        ],
        "venue": null,
        "citeRegEx": "Blake et al\\.,? \\Q1987\\E",
        "shortCiteRegEx": "Blake et al\\.",
        "year": 1987,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Efficient, high-quality image contour detection",
        "author": [
            "Catanzaro",
            "Bryan C",
            "Su",
            "Bor-Yiing",
            "Sundaram",
            "Narayanan",
            "Lee",
            "Yunsup",
            "Murphy",
            "Mark",
            "Keutzer",
            "Kurt"
        ],
        "venue": "In Proc. ICCV,",
        "citeRegEx": "Catanzaro et al\\.,? \\Q2009\\E",
        "shortCiteRegEx": "Catanzaro et al\\.",
        "year": 2009,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " (2005), we found it simpler to harness the computational power of GPUs and integrate the Damascene system of (Catanzaro et al., 2009) with the Caffe deep learning framework; when integrated with our boundary detector Damascene yields 8 eigenvectors for a 577\u00d7 865 image in less that 0. Cour et al. (2005), we found it simpler to harness the computational power of GPUs and integrate the Damascene system of (Catanzaro et al.",
        "context": null
    },
    {
        "title": "Semantic image segmentation with deep convolutional nets and fully connected crfs",
        "author": [
            "Chen",
            "Liang-Chieh",
            "Papandreou",
            "George",
            "Kokkinos",
            "Iasonas",
            "Murphy",
            "Kevin",
            "Yuille",
            "Alan L"
        ],
        "venue": "In ICLR,",
        "citeRegEx": "Chen et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "Chen et al\\.",
        "year": 2015,
        "abstract": "Deep Convolutional Neural Networks (DCNNs) have recently shown state of the\nart performance in high level vision tasks, such as image classification and\nobject detection. This work brings together methods from DCNNs and\nprobabilistic graphical models for addressing the task of pixel-level\nclassification (also called \"semantic image segmentation\"). We show that\nresponses at the final layer of DCNNs are not sufficiently localized for\naccurate object segmentation. This is due to the very invariance properties\nthat make DCNNs good for high level tasks. We overcome this poor localization\nproperty of deep networks by combining the responses at the final DCNN layer\nwith a fully connected Conditional Random Field (CRF). Qualitatively, our\n\"DeepLab\" system is able to localize segment boundaries at a level of accuracy\nwhich is beyond previous methods. Quantitatively, our method sets the new\nstate-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching\n71.6% IOU accuracy in the test set. We show how these results can be obtained\nefficiently: Careful network re-purposing and a novel application of the 'hole'\nalgorithm from the wavelet community allow dense computation of neural net\nresponses at 8 frames per second on a modern GPU.",
        "full_text": "Published as a conference paper at ICLR 2015\nSEMANTIC IMAGE SEGMENTATION WITH DEEP CON-\nVOLUTIONAL NETS AND FULLY CONNECTED CRFS\nLiang-Chieh Chen\nUniv. of California, Los Angeles\nlcchen@cs.ucla.edu\nGeorge Papandreou \u2217\nGoogle Inc.\ngpapan@google.com\nIasonas Kokkinos\nCentraleSup\u00b4elec and INRIA\niasonas.kokkinos@ecp.fr\nKevin Murphy\nGoogle Inc.\nkpmurphy@google.com\nAlan L. Yuille\nUniv. of California, Los Angeles\nyuille@stat.ucla.edu\nABSTRACT\nDeep Convolutional Neural Networks (DCNNs) have recently shown state of the\nart performance in high level vision tasks, such as image classi\ufb01cation and ob-\nject detection. This work brings together methods from DCNNs and probabilistic\ngraphical models for addressing the task of pixel-level classi\ufb01cation (also called\n\u201dsemantic image segmentation\u201d). We show that responses at the \ufb01nal layer of\nDCNNs are not suf\ufb01ciently localized for accurate object segmentation. This is\ndue to the very invariance properties that make DCNNs good for high level tasks.\nWe overcome this poor localization property of deep networks by combining the\nresponses at the \ufb01nal DCNN layer with a fully connected Conditional Random\nField (CRF). Qualitatively, our \u201cDeepLab\u201d system is able to localize segment\nboundaries at a level of accuracy which is beyond previous methods. Quantita-\ntively, our method sets the new state-of-art at the PASCAL VOC-2012 semantic\nimage segmentation task, reaching 71.6% IOU accuracy in the test set. We show\nhow these results can be obtained ef\ufb01ciently: Careful network re-purposing and a\nnovel application of the \u2019hole\u2019 algorithm from the wavelet community allow dense\ncomputation of neural net responses at 8 frames per second on a modern GPU.\n1\nINTRODUCTION\nDeep Convolutional Neural Networks (DCNNs) had been the method of choice for document recog-\nnition since LeCun et al. (1998), but have only recently become the mainstream of high-level vision\nresearch. Over the past two years DCNNs have pushed the performance of computer vision sys-\ntems to soaring heights on a broad array of high-level problems, including image classi\ufb01cation\n(Krizhevsky et al., 2013; Sermanet et al., 2013; Simonyan & Zisserman, 2014; Szegedy et al., 2014;\n\u2217Work initiated when G.P. was with the Toyota Technological Institute at Chicago. The \ufb01rst two authors\ncontributed equally to this work.\n1\narXiv:1412.7062v4  [cs.CV]  7 Jun 2016\nPublished as a conference paper at ICLR 2015\nPapandreou et al., 2014), object detection (Girshick et al., 2014), \ufb01ne-grained categorization (Zhang\net al., 2014), among others. A common theme in these works is that DCNNs trained in an end-to-end\nmanner deliver strikingly better results than systems relying on carefully engineered representations,\nsuch as SIFT or HOG features. This success can be partially attributed to the built-in invariance of\nDCNNs to local image transformations, which underpins their ability to learn hierarchical abstrac-\ntions of data (Zeiler & Fergus, 2014). While this invariance is clearly desirable for high-level vision\ntasks, it can hamper low-level tasks, such as pose estimation (Chen & Yuille, 2014; Tompson et al.,\n2014) and semantic segmentation - where we want precise localization, rather than abstraction of\nspatial details.\nThere are two technical hurdles in the application of DCNNs to image labeling tasks: signal down-\nsampling, and spatial \u2018insensitivity\u2019 (invariance). The \ufb01rst problem relates to the reduction of signal\nresolution incurred by the repeated combination of max-pooling and downsampling (\u2018striding\u2019) per-\nformed at every layer of standard DCNNs (Krizhevsky et al., 2013; Simonyan & Zisserman, 2014;\nSzegedy et al., 2014). Instead, as in Papandreou et al. (2014), we employ the \u2018atrous\u2019 (with holes)\nalgorithm originally developed for ef\ufb01ciently computing the undecimated discrete wavelet transform\n(Mallat, 1999). This allows ef\ufb01cient dense computation of DCNN responses in a scheme substan-\ntially simpler than earlier solutions to this problem (Giusti et al., 2013; Sermanet et al., 2013).\nThe second problem relates to the fact that obtaining object-centric decisions from a classi\ufb01er re-\nquires invariance to spatial transformations, inherently limiting the spatial accuracy of the DCNN\nmodel. We boost our model\u2019s ability to capture \ufb01ne details by employing a fully-connected Condi-\ntional Random Field (CRF). Conditional Random Fields have been broadly used in semantic seg-\nmentation to combine class scores computed by multi-way classi\ufb01ers with the low-level information\ncaptured by the local interactions of pixels and edges (Rother et al., 2004; Shotton et al., 2009) or\nsuperpixels (Lucchi et al., 2011). Even though works of increased sophistication have been proposed\nto model the hierarchical dependency (He et al., 2004; Ladicky et al., 2009; Lempitsky et al., 2011)\nand/or high-order dependencies of segments (Delong et al., 2012; Gonfaus et al., 2010; Kohli et al.,\n2009; Chen et al., 2013; Wang et al., 2015), we use the fully connected pairwise CRF proposed by\nKr\u00a8ahenb\u00a8uhl & Koltun (2011) for its ef\ufb01cient computation, and ability to capture \ufb01ne edge details\nwhile also catering for long range dependencies. That model was shown in Kr\u00a8ahenb\u00a8uhl & Koltun\n(2011) to largely improve the performance of a boosting-based pixel-level classi\ufb01er, and in our work\nwe demonstrate that it leads to state-of-the-art results when coupled with a DCNN-based pixel-level\nclassi\ufb01er.\nThe three main advantages of our \u201cDeepLab\u201d system are (i) speed: by virtue of the \u2018atrous\u2019 algo-\nrithm, our dense DCNN operates at 8 fps, while Mean Field Inference for the fully-connected CRF\nrequires 0.5 second, (ii) accuracy: we obtain state-of-the-art results on the PASCAL semantic seg-\nmentation challenge, outperforming the second-best approach of Mostajabi et al. (2014) by a margin\nof 7.2% and (iii) simplicity: our system is composed of a cascade of two fairly well-established mod-\nules, DCNNs and CRFs.\n2\nRELATED WORK\nOur system works directly on the pixel representation, similarly to Long et al. (2014). This is in con-\ntrast to the two-stage approaches that are now most common in semantic segmentation with DCNNs:\nsuch techniques typically use a cascade of bottom-up image segmentation and DCNN-based region\nclassi\ufb01cation, which makes the system commit to potential errors of the front-end segmentation sys-\ntem. For instance, the bounding box proposals and masked regions delivered by (Arbel\u00b4aez et al.,\n2014; Uijlings et al., 2013) are used in Girshick et al. (2014) and (Hariharan et al., 2014b) as inputs\nto a DCNN to introduce shape information into the classi\ufb01cation process. Similarly, the authors of\nMostajabi et al. (2014) rely on a superpixel representation. A celebrated non-DCNN precursor to\nthese works is the second order pooling method of (Carreira et al., 2012) which also assigns labels\nto the regions proposals delivered by (Carreira & Sminchisescu, 2012). Understanding the perils\nof committing to a single segmentation, the authors of Cogswell et al. (2014) build on (Yadollah-\npour et al., 2013) to explore a diverse set of CRF-based segmentation proposals, computed also by\n(Carreira & Sminchisescu, 2012). These segmentation proposals are then re-ranked according to a\nDCNN trained in particular for this reranking task. Even though this approach explicitly tries to\nhandle the temperamental nature of a front-end segmentation algorithm, there is still no explicit ex-\n2\nPublished as a conference paper at ICLR 2015\nploitation of the DCNN scores in the CRF-based segmentation algorithm: the DCNN is only applied\npost-hoc, while it would make sense to directly try to use its results during segmentation.\nMoving towards works that lie closer to our approach, several other researchers have considered\nthe use of convolutionally computed DCNN features for dense image labeling. Among the \ufb01rst\nhave been Farabet et al. (2013) who apply DCNNs at multiple image resolutions and then employ a\nsegmentation tree to smooth the prediction results; more recently, Hariharan et al. (2014a) propose to\nconcatenate the computed inter-mediate feature maps within the DCNNs for pixel classi\ufb01cation, and\nDai et al. (2014) propose to pool the inter-mediate feature maps by region proposals. Even though\nthese works still employ segmentation algorithms that are decoupled from the DCNN classi\ufb01er\u2019s\nresults, we believe it is advantageous that segmentation is only used at a later stage, avoiding the\ncommitment to premature decisions.\nMore recently, the segmentation-free techniques of (Long et al., 2014; Eigen & Fergus, 2014) di-\nrectly apply DCNNs to the whole image in a sliding window fashion, replacing the last fully con-\nnected layers of a DCNN by convolutional layers. In order to deal with the spatial localization\nissues outlined in the beginning of the introduction, Long et al. (2014) upsample and concatenate\nthe scores from inter-mediate feature maps, while Eigen & Fergus (2014) re\ufb01ne the prediction result\nfrom coarse to \ufb01ne by propagating the coarse results to another DCNN.\nThe main difference between our model and other state-of-the-art models is the combination of\npixel-level CRFs and DCNN-based \u2018unary terms\u2019. Focusing on the closest works in this direction,\nCogswell et al. (2014) use CRFs as a proposal mechanism for a DCNN-based reranking system,\nwhile Farabet et al. (2013) treat superpixels as nodes for a local pairwise CRF and use graph-cuts for\ndiscrete inference; as such their results can be limited by errors in superpixel computations, while ig-\nnoring long-range superpixel dependencies. Our approach instead treats every pixel as a CRF node,\nexploits long-range dependencies, and uses CRF inference to directly optimize a DCNN-driven cost\nfunction. We note that mean \ufb01eld had been extensively studied for traditional image segmenta-\ntion/edge detection tasks, e.g., (Geiger & Girosi, 1991; Geiger & Yuille, 1991; Kokkinos et al.,\n2008), but recently Kr\u00a8ahenb\u00a8uhl & Koltun (2011) showed that the inference can be very ef\ufb01cient for\nfully connected CRF and particularly effective in the context of semantic segmentation.\nAfter the \ufb01rst version of our manuscript was made publicly available, it came to our attention that\ntwo other groups have independently and concurrently pursued a very similar direction, combining\nDCNNs and densely connected CRFs (Bell et al., 2014; Zheng et al., 2015). There are several\ndifferences in technical aspects of the respective models. Bell et al. (2014) focus on the problem\nof material classi\ufb01cation, while Zheng et al. (2015) unroll the CRF mean-\ufb01eld inference steps to\nconvert the whole system into an end-to-end trainable feed-forward network.\nWe have updated our proposed \u201cDeepLab\u201d system with much improved methods and results in our\nlatest work (Chen et al., 2016). We refer the interested reader to the paper for details.\n3\nCONVOLUTIONAL NEURAL NETWORKS FOR DENSE IMAGE LABELING\nHerein we describe how we have re-purposed and \ufb01netuned the publicly available Imagenet-\npretrained state-of-art 16-layer classi\ufb01cation network of (Simonyan & Zisserman, 2014) (VGG-16)\ninto an ef\ufb01cient and effective dense feature extractor for our dense semantic image segmentation\nsystem.\n3.1\nEFFICIENT DENSE SLIDING WINDOW FEATURE EXTRACTION WITH THE HOLE\nALGORITHM\nDense spatial score evaluation is instrumental in the success of our dense CNN feature extractor. As\na \ufb01rst step to implement this, we convert the fully-connected layers of VGG-16 into convolutional\nones and run the network in a convolutional fashion on the image at its original resolution. However\nthis is not enough as it yields very sparsely computed detection scores (with a stride of 32 pixels). To\ncompute scores more densely at our target stride of 8 pixels, we develop a variation of the method\npreviously employed by Giusti et al. (2013); Sermanet et al. (2013). We skip subsampling after\nthe last two max-pooling layers in the network of Simonyan & Zisserman (2014) and modify the\nconvolutional \ufb01lters in the layers that follow them by introducing zeros to increase their length (2\u00d7in\n3\nPublished as a conference paper at ICLR 2015\nInput stride\nOutput stride\nFigure 1: Illustration of the hole algorithm in 1-D, when kernel size = 3, input stride = 2, and\noutput stride = 1.\nthe last three convolutional layers and 4\u00d7in the \ufb01rst fully connected layer). We can implement this\nmore ef\ufb01ciently by keeping the \ufb01lters intact and instead sparsely sample the feature maps on which\nthey are applied on using an input stride of 2 or 4 pixels, respectively. This approach, illustrated\nin Fig. 1 is known as the \u2018hole algorithm\u2019 (\u2018atrous algorithm\u2019) and has been developed before for\nef\ufb01cient computation of the undecimated wavelet transform (Mallat, 1999). We have implemented\nthis within the Caffe framework (Jia et al., 2014) by adding to the im2col function (it converts multi-\nchannel feature maps to vectorized patches) the option to sparsely sample the underlying feature\nmap. This approach is generally applicable and allows us to ef\ufb01ciently compute dense CNN feature\nmaps at any target subsampling rate without introducing any approximations.\nWe \ufb01netune the model weights of the Imagenet-pretrained VGG-16 network to adapt it to the image\nclassi\ufb01cation task in a straightforward fashion, following the procedure of Long et al. (2014). We\nreplace the 1000-way Imagenet classi\ufb01er in the last layer of VGG-16 with a 21-way one. Our\nloss function is the sum of cross-entropy terms for each spatial position in the CNN output map\n(subsampled by 8 compared to the original image). All positions and labels are equally weighted in\nthe overall loss function. Our targets are the ground truth labels (subsampled by 8). We optimize the\nobjective function with respect to the weights at all network layers by the standard SGD procedure\nof Krizhevsky et al. (2013).\nDuring testing, we need class score maps at the original image resolution. As illustrated in Figure 2\nand further elaborated in Section 4.1, the class score maps (corresponding to log-probabilities) are\nquite smooth, which allows us to use simple bilinear interpolation to increase their resolution by a\nfactor of 8 at a negligible computational cost. Note that the method of Long et al. (2014) does not\nuse the hole algorithm and produces very coarse scores (subsampled by a factor of 32) at the CNN\noutput. This forced them to use learned upsampling layers, signi\ufb01cantly increasing the complexity\nand training time of their system: Fine-tuning our network on PASCAL VOC 2012 takes about 10\nhours, while they report a training time of several days (both timings on a modern GPU).\n3.2\nCONTROLLING THE RECEPTIVE FIELD SIZE AND ACCELERATING DENSE\nCOMPUTATION WITH CONVOLUTIONAL NETS\nAnother key ingredient in re-purposing our network for dense score computation is explicitly con-\ntrolling the network\u2019s receptive \ufb01eld size. Most recent DCNN-based image recognition methods\nrely on networks pre-trained on the Imagenet large-scale classi\ufb01cation task. These networks typi-\ncally have large receptive \ufb01eld size: in the case of the VGG-16 net we consider, its receptive \ufb01eld\nis 224\u00d7224 (with zero-padding) and 404\u00d7404 pixels if the net is applied convolutionally. After\nconverting the network to a fully convolutional one, the \ufb01rst fully connected layer has 4,096 \ufb01l-\nters of large 7\u00d77 spatial size and becomes the computational bottleneck in our dense score map\ncomputation.\nWe have addressed this practical problem by spatially subsampling (by simple decimation) the \ufb01rst\nFC layer to 4\u00d74 (or 3\u00d73) spatial size. This has reduced the receptive \ufb01eld of the network down to\n128\u00d7128 (with zero-padding) or 308\u00d7308 (in convolutional mode) and has reduced computation time\nfor the \ufb01rst FC layer by 2 \u22123 times. Using our Caffe-based implementation and a Titan GPU, the\nresulting VGG-derived network is very ef\ufb01cient: Given a 306\u00d7306 input image, it produces 39\u00d739\n4\nPublished as a conference paper at ICLR 2015\ndense raw feature scores at the top of the network at a rate of about 8 frames/sec during testing. The\nspeed during training is 3 frames/sec. We have also successfully experimented with reducing the\nnumber of channels at the fully connected layers from 4,096 down to 1,024, considerably further\ndecreasing computation time and memory footprint without sacri\ufb01cing performance, as detailed in\nSection 5. Using smaller networks such as Krizhevsky et al. (2013) could allow video-rate test-time\ndense feature computation even on light-weight GPUs.\n4\nDETAILED BOUNDARY RECOVERY: FULLY-CONNECTED CONDITIONAL\nRANDOM FIELDS AND MULTI-SCALE PREDICTION\n4.1\nDEEP CONVOLUTIONAL NETWORKS AND THE LOCALIZATION CHALLENGE\nAs illustrated in Figure 2, DCNN score maps can reliably predict the presence and rough position\nof objects in an image but are less well suited for pin-pointing their exact outline. There is a natural\ntrade-off between classi\ufb01cation accuracy and localization accuracy with convolutional networks:\nDeeper models with multiple max-pooling layers have proven most successful in classi\ufb01cation tasks,\nhowever their increased invariance and large receptive \ufb01elds make the problem of inferring position\nfrom the scores at their top output levels more challenging.\nRecent work has pursued two directions to address this localization challenge. The \ufb01rst approach is\nto harness information from multiple layers in the convolutional network in order to better estimate\nthe object boundaries (Long et al., 2014; Eigen & Fergus, 2014). The second approach is to employ\na super-pixel representation, essentially delegating the localization task to a low-level segmentation\nmethod. This route is followed by the very successful recent method of Mostajabi et al. (2014).\nIn Section 4.2, we pursue a novel alternative direction based on coupling the recognition capacity\nof DCNNs and the \ufb01ne-grained localization accuracy of fully connected CRFs and show that it is\nremarkably successful in addressing the localization challenge, producing accurate semantic seg-\nmentation results and recovering object boundaries at a level of detail that is well beyond the reach\nof existing methods.\n4.2\nFULLY-CONNECTED CONDITIONAL RANDOM FIELDS FOR ACCURATE LOCALIZATION\nImage/G.T.\nDCNN output\nCRF Iteration 1\nCRF Iteration 2\nCRF Iteration 10\nFigure 2: Score map (input before softmax function) and belief map (output of softmax function) for\nAeroplane. We show the score (1st row) and belief (2nd row) maps after each mean \ufb01eld iteration.\nThe output of last DCNN layer is used as input to the mean \ufb01eld inference. Best viewed in color.\nTraditionally, conditional random \ufb01elds (CRFs) have been employed to smooth noisy segmentation\nmaps (Rother et al., 2004; Kohli et al., 2009). Typically these models contain energy terms that\ncouple neighboring nodes, favoring same-label assignments to spatially proximal pixels. Qualita-\ntively, the primary function of these short-range CRFs has been to clean up the spurious predictions\nof weak classi\ufb01ers built on top of local hand-engineered features.\nCompared to these weaker classi\ufb01ers, modern DCNN architectures such as the one we use in this\nwork produce score maps and semantic label predictions which are qualitatively different. As illus-\ntrated in Figure 2, the score maps are typically quite smooth and produce homogeneous classi\ufb01cation\nresults. In this regime, using short-range CRFs can be detrimental, as our goal should be to recover\ndetailed local structure rather than further smooth it. Using contrast-sensitive potentials (Rother\n5\nPublished as a conference paper at ICLR 2015\nDeep \nConvolutional \nNeural \nNetwork\nInput\nAeroplane\nCoarse Score map\nBi-linear Interpolation\nFully Connected CRF\nFinal Output\nFigure 3: Model Illustration. The coarse score map from Deep Convolutional Neural Network (with\nfully convolutional layers) is upsampled by bi-linear interpolation. A fully connected CRF is applied\nto re\ufb01ne the segmentation result. Best viewed in color.\net al., 2004) in conjunction to local-range CRFs can potentially improve localization but still miss\nthin-structures and typically requires solving an expensive discrete optimization problem.\nTo overcome these limitations of short-range CRFs, we integrate into our system the fully connected\nCRF model of Kr\u00a8ahenb\u00a8uhl & Koltun (2011). The model employs the energy function\nE(x) =\nX\ni\n\u03b8i(xi) +\nX\nij\n\u03b8ij(xi, xj)\n(1)\nwhere x is the label assignment for pixels. We use as unary potential \u03b8i(xi) = \u2212log P(xi), where\nP(xi) is the label assignment probability at pixel i as computed by DCNN. The pairwise potential\nis \u03b8ij(xi, xj) = \u00b5(xi, xj) PK\nm=1 wm \u00b7 km(f i, f j), where \u00b5(xi, xj) = 1 if xi \u0338= xj, and zero\notherwise (i.e., Potts Model). There is one pairwise term for each pair of pixels i and j in the image\nno matter how far from each other they lie, i.e. the model\u2019s factor graph is fully connected. Each km\nis the Gaussian kernel depends on features (denoted as f) extracted for pixel i and j and is weighted\nby parameter wm. We adopt bilateral position and color terms, speci\ufb01cally, the kernels are\nw1 exp\n\u0010\n\u2212||pi \u2212pj||2\n2\u03c32\u03b1\n\u2212||Ii \u2212Ij||2\n2\u03c32\n\u03b2\n\u0011\n+ w2 exp\n\u0010\n\u2212||pi \u2212pj||2\n2\u03c32\u03b3\n\u0011\n(2)\nwhere the \ufb01rst kernel depends on both pixel positions (denoted as p) and pixel color intensities\n(denoted as I), and the second kernel only depends on pixel positions. The hyper parameters \u03c3\u03b1, \u03c3\u03b2\nand \u03c3\u03b3 control the \u201cscale\u201d of the Gaussian kernels.\nCrucially, this model is amenable to ef\ufb01cient approximate probabilistic inference (Kr\u00a8ahenb\u00a8uhl &\nKoltun, 2011). The message passing updates under a fully decomposable mean \ufb01eld approxima-\ntion b(x) = Q\ni bi(xi) can be expressed as convolutions with a Gaussian kernel in feature space.\nHigh-dimensional \ufb01ltering algorithms (Adams et al., 2010) signi\ufb01cantly speed-up this computation\nresulting in an algorithm that is very fast in practice, less that 0.5 sec on average for Pascal VOC\nimages using the publicly available implementation of (Kr\u00a8ahenb\u00a8uhl & Koltun, 2011).\n4.3\nMULTI-SCALE PREDICTION\nFollowing the promising recent results of (Hariharan et al., 2014a; Long et al., 2014) we have also\nexplored a multi-scale prediction method to increase the boundary localization accuracy. Specif-\nically, we attach to the input image and the output of each of the \ufb01rst four max pooling layers a\ntwo-layer MLP (\ufb01rst layer: 128 3x3 convolutional \ufb01lters, second layer: 128 1x1 convolutional \ufb01l-\nters) whose feature map is concatenated to the main network\u2019s last layer feature map. The aggregate\nfeature map fed into the softmax layer is thus enhanced by 5 * 128 = 640 channels. We only adjust\nthe newly added weights, keeping the other network parameters to the values learned by the method\nof Section 3. As discussed in the experimental section, introducing these extra direct connections\nfrom \ufb01ne-resolution layers improves localization performance, yet the effect is not as dramatic as\nthe one obtained with the fully-connected CRF.\n6\nPublished as a conference paper at ICLR 2015\nMethod\nmean IOU (%)\nDeepLab\n59.80\nDeepLab-CRF\n63.74\nDeepLab-MSc\n61.30\nDeepLab-MSc-CRF\n65.21\nDeepLab-7x7\n64.38\nDeepLab-CRF-7x7\n67.64\nDeepLab-LargeFOV\n62.25\nDeepLab-CRF-LargeFOV\n67.64\nDeepLab-MSc-LargeFOV\n64.21\nDeepLab-MSc-CRF-LargeFOV\n68.70\nMethod\nmean IOU (%)\nMSRA-CFM\n61.8\nFCN-8s\n62.2\nTTI-Zoomout-16\n64.4\nDeepLab-CRF\n66.4\nDeepLab-MSc-CRF\n67.1\nDeepLab-CRF-7x7\n70.3\nDeepLab-CRF-LargeFOV\n70.3\nDeepLab-MSc-CRF-LargeFOV\n71.6\n(a)\n(b)\nTable 1: (a) Performance of our proposed models on the PASCAL VOC 2012 \u2018val\u2019 set (with training\nin the augmented \u2018train\u2019 set). The best performance is achieved by exploiting both multi-scale\nfeatures and large \ufb01eld-of-view. (b) Performance of our proposed models (with training in the\naugmented \u2018trainval\u2019 set) compared to other state-of-art methods on the PASCAL VOC 2012 \u2018test\u2019\nset.\n5\nEXPERIMENTAL EVALUATION\nDataset\nWe test our DeepLab model on the PASCAL VOC 2012 segmentation benchmark (Ev-\neringham et al., 2014), consisting of 20 foreground object classes and one background class. The\noriginal dataset contains 1, 464, 1, 449, and 1, 456 images for training, validation, and testing, re-\nspectively. The dataset is augmented by the extra annotations provided by Hariharan et al. (2011),\nresulting in 10, 582 training images. The performance is measured in terms of pixel intersection-\nover-union (IOU) averaged across the 21 classes.\nTraining\nWe adopt the simplest form of piecewise training, decoupling the DCNN and CRF train-\ning stages, assuming the unary terms provided by the DCNN are \ufb01xed during CRF training.\nFor DCNN training we employ the VGG-16 network which has been pre-trained on ImageNet. We\n\ufb01ne-tuned the VGG-16 network on the VOC 21-way pixel-classi\ufb01cation task by stochastic gradient\ndescent on the cross-entropy loss function, as described in Section 3.1. We use a mini-batch of 20\nimages and initial learning rate of 0.001 (0.01 for the \ufb01nal classi\ufb01er layer), multiplying the learning\nrate by 0.1 at every 2000 iterations. We use momentum of 0.9 and a weight decay of 0.0005.\nAfter the DCNN has been \ufb01ne-tuned, we cross-validate the parameters of the fully connected CRF\nmodel in Eq. (2) along the lines of Kr\u00a8ahenb\u00a8uhl & Koltun (2011). We use the default values of\nw2 = 3 and \u03c3\u03b3 = 3 and we search for the best values of w1, \u03c3\u03b1, and \u03c3\u03b2 by cross-validation on a\nsmall subset of the validation set (we use 100 images). We employ coarse-to-\ufb01ne search scheme.\nSpeci\ufb01cally, the initial search range of the parameters are w1 \u2208[5, 10], \u03c3\u03b1 \u2208[50 : 10 : 100] and\n\u03c3\u03b2 \u2208[3 : 1 : 10] (MATLAB notation), and then we re\ufb01ne the search step sizes around the \ufb01rst\nround\u2019s best values. We \ufb01x the number of mean \ufb01eld iterations to 10 for all reported experiments.\nEvaluation on Validation set\nWe conduct the majority of our evaluations on the PASCAL \u2018val\u2019\nset, training our model on the augmented PASCAL \u2018train\u2019 set. As shown in Tab. 1 (a), incorporating\nthe fully connected CRF to our model (denoted by DeepLab-CRF) yields a substantial performance\nboost, about 4% improvement over DeepLab. We note that the work of Kr\u00a8ahenb\u00a8uhl & Koltun\n(2011) improved the 27.6% result of TextonBoost (Shotton et al., 2009) to 29.1%, which makes the\nimprovement we report here (from 59.8% to 63.7%) all the more impressive.\nTurning to qualitative results, we provide visual comparisons between DeepLab and DeepLab-CRF\nin Fig. 7. Employing a fully connected CRF signi\ufb01cantly improves the results, allowing the model\nto accurately capture intricate object boundaries.\nMulti-Scale features\nWe also exploit the features from the intermediate layers, similar to Hariha-\nran et al. (2014a); Long et al. (2014). As shown in Tab. 1 (a), adding the multi-scale features to our\n7\nPublished as a conference paper at ICLR 2015\nMethod\nkernel size\ninput stride\nreceptive \ufb01eld\n# parameters\nmean IOU (%)\nTraining speed (img/sec)\nDeepLab-CRF-7x7\n7\u00d77\n4\n224\n134.3M\n67.64\n1.44\nDeepLab-CRF\n4\u00d74\n4\n128\n65.1M\n63.74\n2.90\nDeepLab-CRF-4x4\n4\u00d74\n8\n224\n65.1M\n67.14\n2.90\nDeepLab-CRF-LargeFOV\n3\u00d73\n12\n224\n20.5M\n67.64\n4.84\nTable 2: Effect of Field-Of-View. We show the performance (after CRF) and training speed on the\nPASCAL VOC 2012 \u2018val\u2019 set as the function of (1) the kernel size of \ufb01rst fully connected layer, (2)\nthe input stride value employed in the atrous algorithm.\nDeepLab model (denoted as DeepLab-MSc) improves about 1.5% performance, and further incor-\nporating the fully connected CRF (denoted as DeepLab-MSc-CRF) yields about 4% improvement.\nThe qualitative comparisons between DeepLab and DeepLab-MSc are shown in Fig. 4. Leveraging\nthe multi-scale features can slightly re\ufb01ne the object boundaries.\nField of View\nThe \u2018atrous algorithm\u2019 we employed allows us to arbitrarily control the Field-of-\nView (FOV) of the models by adjusting the input stride, as illustrated in Fig. 1. In Tab. 2, we\nexperiment with several kernel sizes and input strides at the \ufb01rst fully connected layer. The method,\nDeepLab-CRF-7x7, is the direct modi\ufb01cation from VGG-16 net, where the kernel size = 7\u00d77 and\ninput stride = 4. This model yields performance of 67.64% on the \u2018val\u2019 set, but it is relatively slow\n(1.44 images per second during training). We have improved model speed to 2.9 images per second\nby reducing the kernel size to 4\u00d74. We have experimented with two such network variants with\ndifferent FOV sizes, DeepLab-CRF and DeepLab-CRF-4x4; the latter has large FOV (i.e., large\ninput stride) and attains better performance. Finally, we employ kernel size 3\u00d73 and input stride =\n12, and further change the \ufb01lter sizes from 4096 to 1024 for the last two layers. Interestingly, the\nresulting model, DeepLab-CRF-LargeFOV, matches the performance of the expensive DeepLab-\nCRF-7x7. At the same time, it is 3.36 times faster to run and has signi\ufb01cantly fewer parameters\n(20.5M instead of 134.3M).\nThe performance of several model variants is summarized in Tab. 1, showing the bene\ufb01t of exploiting\nmulti-scale features and large FOV.\nFigure 4: Incorporating multi-scale features improves the boundary segmentation. We show the\nresults obtained by DeepLab and DeepLab-MSc in the \ufb01rst and second row, respectively. Best\nviewed in color.\nMean Pixel IOU along Object Boundaries\nTo quantify the accuracy of the proposed model near\nobject boundaries, we evaluate the segmentation accuracy with an experiment similar to Kohli et al.\n(2009); Kr\u00a8ahenb\u00a8uhl & Koltun (2011). Speci\ufb01cally, we use the \u2018void\u2019 label annotated in val set,\nwhich usually occurs around object boundaries. We compute the mean IOU for those pixels that\nare located within a narrow band (called trimap) of \u2018void\u2019 labels. As shown in Fig. 5, exploiting\nthe multi-scale features from the intermediate layers and re\ufb01ning the segmentation results by a fully\nconnected CRF signi\ufb01cantly improve the results around object boundaries.\nComparison with State-of-art\nIn Fig. 6, we qualitatively compare our proposed model, DeepLab-\nCRF, with two state-of-art models: FCN-8s (Long et al., 2014) and TTI-Zoomout-16 (Mostajabi\net al., 2014) on the \u2018val\u2019 set (the results are extracted from their papers). Our model is able to\ncapture the intricate object boundaries.\n8\nPublished as a conference paper at ICLR 2015\n0\n5\n10\n15\n20\n25\n30\n35\n40\n55\n60\n65\n70\n75\n80\n85\n90\nPixelwise Accuracy (%)\nTrimap Width (pixels)\n \n \nDL\u2212MSc\u2212CRF\nDeepLab\u2212CRF\nDeepLab\u2212MSc\nDeepLab\n0\n5\n10\n15\n20\n25\n30\n35\n40\n35\n40\n45\n50\n55\n60\n65\nmean IOU (%)\nTrimap Width (pixels)\n \n \nDL\u2212MSc\u2212CRF\nDeepLab\u2212CRF\nDeepLab\u2212MSc\nDeepLab\n(a)\n(b)\n(c)\nFigure 5: (a) Some trimap examples (top-left: image. top-right: ground-truth. bottom-left: trimap\nof 2 pixels. bottom-right: trimap of 10 pixels). Quality of segmentation result within a band around\nthe object boundaries for the proposed methods. (b) Pixelwise accuracy. (c) Pixel mean IOU.\n(a) FCN-8s vs. DeepLab-CRF\n(b) TTI-Zoomout-16 vs. DeepLab-CRF\nFigure 6: Comparisons with state-of-the-art models on the val set. First row: images. Second row:\nground truths. Third row: other recent models (Left: FCN-8s, Right: TTI-Zoomout-16). Fourth\nrow: our DeepLab-CRF. Best viewed in color.\nReproducibility\nWe have implemented the proposed methods by extending the excellent Caffe\nframework (Jia et al., 2014). We share our source code, con\ufb01guration \ufb01les, and trained models that\nallow reproducing the results in this paper at a companion web site https://bitbucket.org/\ndeeplab/deeplab-public.\nTest set results\nHaving set our model choices on the validation set, we evaluate our model variants\non the PASCAL VOC 2012 of\ufb01cial \u2018test\u2019 set. As shown in Tab. 3, our DeepLab-CRF and DeepLab-\nMSc-CRF models achieve performance of 66.4% and 67.1% mean IOU1, respectively. Our models\noutperform all the other state-of-the-art models (speci\ufb01cally, TTI-Zoomout-16 (Mostajabi et al.,\n2014), FCN-8s (Long et al., 2014), and MSRA-CFM (Dai et al., 2014)). When we increase the FOV\nof the models, DeepLab-CRF-LargeFOV yields performance of 70.3%, the same as DeepLab-CRF-\n7x7, while its training speed is faster. Furthermore, our best model, DeepLab-MSc-CRF-LargeFOV,\nattains the best performance of 71.6% by employing both multi-scale features and large FOV.\n1http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?\nchallengeid=11&compid=6\n9\nPublished as a conference paper at ICLR 2015\nFigure 7: Visualization results on VOC 2012-val. For each row, we show the input image, the\nsegmentation result delivered by the DCNN (DeepLab), and the re\ufb01ned segmentation result of the\nFully Connected CRF (DeepLab-CRF). We show our failure modes in the last three rows. Best\nviewed in color.\n10\nPublished as a conference paper at ICLR 2015\nMethod\nbkg aero bike bird boat bottle\nbus\ncar\ncat\nchair cow table dog horse mbike person plant sheep sofa train\ntv\nmean\nMSRA-CFM\n-\n75.7 26.7 69.5 48.8\n65.6\n81.0 69.2 73.3 30.0 68.7 51.5 69.1\n68.1\n71.7\n67.5\n50.4\n66.5\n44.4 58.9 53.5\n61.8\nFCN-8s\n-\n76.8 34.2 68.9 49.4\n60.3\n75.3 74.7 77.6 21.4 62.5 46.8 71.8\n63.9\n76.5\n73.9\n45.2\n72.4\n37.4 70.9 55.1\n62.2\nTTI-Zoomout-16\n89.8 81.9 35.1 78.2 57.4\n56.5\n80.5 74.0 79.8 22.4 69.6 53.7 74.0\n76.0\n76.6\n68.8\n44.3\n70.2\n40.2 68.9 55.3\n64.4\nDeepLab-CRF\n92.1 78.4 33.1 78.2 55.6\n65.3\n81.3 75.5 78.6 25.3 69.2 52.7 75.2\n69.0\n79.1\n77.6\n54.7\n78.3\n45.1 73.3 56.2\n66.4\nDeepLab-MSc-CRF\n92.6 80.4 36.8 77.4 55.2\n66.4\n81.5 77.5 78.9 27.1 68.2 52.7 74.3\n69.6\n79.4\n79.0\n56.9\n78.8\n45.2 72.7 59.3\n67.1\nDeepLab-CRF-7x7\n92.8 83.9 36.6 77.5 58.4\n68.0\n84.6 79.7 83.1 29.5 74.6 59.3 78.9\n76.0\n82.1\n80.6\n60.3\n81.7\n49.2 78.0 60.7\n70.3\nDeepLab-CRF-LargeFOV\n92.6 83.5 36.6 82.5 62.3\n66.5\n85.4 78.5 83.7 30.4 72.9 60.4 78.5\n75.5\n82.1\n79.7\n58.2\n82.0\n48.8 73.7 63.3\n70.3\nDeepLab-MSc-CRF-LargeFOV\n93.1 84.4 54.5 81.5 63.6\n65.9\n85.1 79.1 83.4 30.7 74.1 59.8 79.0\n76.1\n83.2\n80.8\n59.7\n82.2\n50.4 73.1 63.7\n71.6\nTable 3: Labeling IOU (%) on the PASCAL VOC 2012 test set, using the trainval set for training.\n6\nDISCUSSION\nOur work combines ideas from deep convolutional neural networks and fully-connected conditional\nrandom \ufb01elds, yielding a novel method able to produce semantically accurate predictions and de-\ntailed segmentation maps, while being computationally ef\ufb01cient. Our experimental results show that\nthe proposed method signi\ufb01cantly advances the state-of-art in the challenging PASCAL VOC 2012\nsemantic image segmentation task.\nThere are multiple aspects in our model that we intend to re\ufb01ne, such as fully integrating its two\nmain components (CNN and CRF) and train the whole system in an end-to-end fashion, similar to\nKr\u00a8ahenb\u00a8uhl & Koltun (2013); Chen et al. (2014); Zheng et al. (2015). We also plan to experiment\nwith more datasets and apply our method to other sources of data such as depth maps or videos. Re-\ncently, we have pursued model training with weakly supervised annotations, in the form of bounding\nboxes or image-level labels (Papandreou et al., 2015).\nAt a higher level, our work lies in the intersection of convolutional neural networks and probabilistic\ngraphical models. We plan to further investigate the interplay of these two powerful classes of\nmethods and explore their synergistic potential for solving challenging computer vision tasks.\nACKNOWLEDGMENTS\nThis work was partly supported by ARO 62250-CS, NIH Grant 5R01EY022247-03, EU Project\nRECONFIG FP7-ICT-600825 and EU Project MOBOT FP7-ICT-2011-600796. We also gratefully\nacknowledge the support of NVIDIA Corporation with the donation of GPUs used for this research.\nWe would like to thank the anonymous reviewers for their detailed comments and constructive feed-\nback.\nPAPER REVISIONS\nHere we present the list of major paper revisions for the convenience of the readers.\nv1\nSubmission to ICLR 2015. Introduces the model DeepLab-CRF, which attains the performance\nof 66.4% on PASCAL VOC 2012 test set.\nv2\nRebuttal for ICLR 2015. Adds the model DeepLab-MSc-CRF, which incorporates multi-scale\nfeatures from the intermediate layers. DeepLab-MSc-CRF yields the performance of 67.1% on\nPASCAL VOC 2012 test set.\nv3\nCamera-ready for ICLR 2015. Experiments with large Field-Of-View. On PASCAL VOC 2012\ntest set, DeepLab-CRF-LargeFOV achieves the performance of 70.3%. When exploiting both mutli-\nscale features and large FOV, DeepLab-MSc-CRF-LargeFOV attains the performance of 71.6%.\nv4\nReference to our updated \u201cDeepLab\u201d system (Chen et al., 2016) with much improved results.\nREFERENCES\nAdams, A., Baek, J., and Davis, M. A. Fast high-dimensional \ufb01ltering using the permutohedral\nlattice. In Computer Graphics Forum, 2010.\nArbel\u00b4aez, P., Pont-Tuset, J., Barron, J. T., Marques, F., and Malik, J. Multiscale combinatorial\ngrouping. In CVPR, 2014.\n11\nPublished as a conference paper at ICLR 2015\nBell, S., Upchurch, P., Snavely, N., and Bala, K. Material recognition in the wild with the materials\nin context database. arXiv:1412.0623, 2014.\nCarreira, J. and Sminchisescu, C. Cpmc: Automatic object segmentation using constrained para-\nmetric min-cuts. PAMI, 2012.\nCarreira, J., Caseiro, R., Batista, J., and Sminchisescu, C. Semantic segmentation with second-order\npooling. In ECCV, 2012.\nChen, L.-C., Papandreou, G., and Yuille, A. Learning a dictionary of shape epitomes with applica-\ntions to image labeling. In ICCV, 2013.\nChen, L.-C., Schwing, A., Yuille, A., and Urtasun, R.\nLearning deep structured models.\narXiv:1407.2538, 2014.\nChen, L.-C., Papandreou, G., Kokkinos, I., Murphy, K., and Yuille, A. L.\nDeeplab: Semantic\nimage segmentation with deep convolutional nets, atrous convolution, and fully connected crfs.\narXiv:1606.00915, 2016.\nChen, X. and Yuille, A. L. Articulated pose estimation by a graphical model with image dependent\npairwise relations. In NIPS, 2014.\nCogswell, M., Lin, X., Purushwalkam, S., and Batra, D. Combining the best of graphical models\nand convnets for semantic segmentation. arXiv:1412.4313, 2014.\nDai, J., He, K., and Sun, J. Convolutional feature masking for joint object and stuff segmentation.\narXiv:1412.1283, 2014.\nDelong, A., Osokin, A., Isack, H. N., and Boykov, Y. Fast approximate energy minimization with\nlabel costs. IJCV, 2012.\nEigen, D. and Fergus, R. Predicting depth, surface normals and semantic labels with a common\nmulti-scale convolutional architecture. arXiv:1411.4734, 2014.\nEveringham, M., Eslami, S. M. A., Gool, L. V., Williams, C. K. I., Winn, J., and Zisserma, A. The\npascal visual object classes challenge a retrospective. IJCV, 2014.\nFarabet, C., Couprie, C., Najman, L., and LeCun, Y. Learning hierarchical features for scene label-\ning. PAMI, 2013.\nGeiger, D. and Girosi, F. Parallel and deterministic algorithms from mrfs: Surface reconstruction.\nPAMI, 13(5):401\u2013412, 1991.\nGeiger, D. and Yuille, A. A common framework for image segmentation. IJCV, 6(3):227\u2013243,\n1991.\nGirshick, R., Donahue, J., Darrell, T., and Malik, J. Rich feature hierarchies for accurate object\ndetection and semantic segmentation. In CVPR, 2014.\nGiusti, A., Ciresan, D., Masci, J., Gambardella, L., and Schmidhuber, J. Fast image scanning with\ndeep max-pooling convolutional neural networks. In ICIP, 2013.\nGonfaus, J. M., Boix, X., Van de Weijer, J., Bagdanov, A. D., Serrat, J., and Gonzalez, J. Harmony\npotentials for joint classi\ufb01cation and segmentation. In CVPR, 2010.\nHariharan, B., Arbel\u00b4aez, P., Bourdev, L., Maji, S., and Malik, J. Semantic contours from inverse\ndetectors. In ICCV, 2011.\nHariharan, B., Arbel\u00b4aez, P., Girshick, R., and Malik, J. Hypercolumns for object segmentation and\n\ufb01ne-grained localization. arXiv:1411.5752, 2014a.\nHariharan, B., Arbel\u00b4aez, P., Girshick, R., and Malik, J. Simultaneous detection and segmentation.\nIn ECCV, 2014b.\n12\nPublished as a conference paper at ICLR 2015\nHe, X., Zemel, R. S., and Carreira-Perpindn, M. Multiscale conditional random \ufb01elds for image\nlabeling. In CVPR, 2004.\nJia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadarrama, S., and Darrell,\nT. Caffe: Convolutional architecture for fast feature embedding. arXiv:1408.5093, 2014.\nKohli, P., Ladicky, L., and Torr, P. H. Robust higher order potentials for enforcing label consistency.\nIJCV, 2009.\nKokkinos, I., Deriche, R., Faugeras, O., and Maragos, P. Computational analysis and learning for a\nbiologically motivated model of boundary detection. Neurocomputing, 71(10):1798\u20131812, 2008.\nKr\u00a8ahenb\u00a8uhl, P. and Koltun, V. Ef\ufb01cient inference in fully connected crfs with gaussian edge poten-\ntials. In NIPS, 2011.\nKr\u00a8ahenb\u00a8uhl, P. and Koltun, V. Parameter learning and convergent inference for dense random \ufb01elds.\nIn ICML, 2013.\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classi\ufb01cation with deep convolutional\nneural networks. In NIPS, 2013.\nLadicky, L., Russell, C., Kohli, P., and Torr, P. H. Associative hierarchical crfs for object class image\nsegmentation. In ICCV, 2009.\nLeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-based learning applied to document\nrecognition. In Proc. IEEE, 1998.\nLempitsky, V., Vedaldi, A., and Zisserman, A. Pylon model for semantic segmentation. In NIPS,\n2011.\nLong, J., Shelhamer, E., and Darrell, T. Fully convolutional networks for semantic segmentation.\narXiv:1411.4038, 2014.\nLucchi, A., Li, Y., Boix, X., Smith, K., and Fua, P. Are spatial and global constraints really necessary\nfor segmentation? In ICCV, 2011.\nMallat, S. A Wavelet Tour of Signal Processing. Acad. Press, 2 edition, 1999.\nMostajabi, M., Yadollahpour, P., and Shakhnarovich, G. Feedforward semantic segmentation with\nzoom-out features. arXiv:1412.0774, 2014.\nPapandreou, G., Kokkinos, I., and Savalle, P.-A. Untangling local and global deformations in deep\nconvolutional networks for image classi\ufb01cation and sliding window detection. arXiv:1412.0296,\n2014.\nPapandreou, G., Chen, L.-C., Murphy, K., and Yuille, A. L. Weakly- and semi-supervised learning\nof a DCNN for semantic image segmentation. arXiv:1502.02734, 2015.\nRother, C., Kolmogorov, V., and Blake, A. Grabcut: Interactive foreground extraction using iterated\ngraph cuts. In SIGGRAPH, 2004.\nSermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., and LeCun, Y. Overfeat: Integrated\nrecognition, localization and detection using convolutional networks. arXiv:1312.6229, 2013.\nShotton, J., Winn, J., Rother, C., and Criminisi, A. Textonboost for image understanding: Multi-\nclass object recognition and segmentation by jointly modeling texture, layout, and context. IJCV,\n2009.\nSimonyan, K. and Zisserman, A. Very deep convolutional networks for large-scale image recogni-\ntion. arXiv:1409.1556, 2014.\nSzegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., and\nRabinovich, A. Going deeper with convolutions. arXiv:1409.4842, 2014.\n13\nPublished as a conference paper at ICLR 2015\nTompson, J., Jain, A., LeCun, Y., and Bregler, C. Joint Training of a Convolutional Network and a\nGraphical Model for Human Pose Estimation. In NIPS, 2014.\nUijlings, J., van de Sande, K., Gevers, T., and Smeulders, A. Selective search for object recognition.\nIJCV, 2013.\nWang, P., Shen, X., Lin, Z., Cohen, S., Price, B., and Yuille, A. Towards uni\ufb01ed depth and semantic\nprediction from a single image. In CVPR, 2015.\nYadollahpour, P., Batra, D., and Shakhnarovich, G. Discriminative re-ranking of diverse segmenta-\ntions. In CVPR, 2013.\nZeiler, M. D. and Fergus, R. Visualizing and understanding convolutional networks. In ECCV, 2014.\nZhang, N., Donahue, J., Girshick, R., and Darrell, T. Part-based r-cnns for \ufb01ne-grained category\ndetection. In ECCV, 2014.\nZheng, S., Jayasumana, S., Romera-Paredes, B., Vineet, V., Su, Z., Du, D., Huang, C., and Torr, P.\nConditional random \ufb01elds as recurrent neural networks. arXiv:1502.03240, 2015.\n14\n",
        "sentence": " Recent works have also shown that DCNNs can equally well apply to pixel-level labelling tasks, including semantic segmentation (Long et al., 2014; Chen et al., 2015) or normal estimation (Sermanet et al. A convenient component of such works is that the inherently convolutional nature of DCNNS allows for simple and efficient \u2018fully convolutional\u2019 implementations (Sermanet et al., 2014; Eigen et al., 2014; Oquab et al., 2015; Long et al., 2014; Chen et al., 2015). Since our model is fully-convolutional we can easily combine it with the recent line of works around FCNN-based semantic segmentation(Long et al., 2014; Chen et al., 2015; Papandreou et al., 2015a; Zheng et al., 2015). , 2014; Chen et al., 2015; Papandreou et al., 2015a; Zheng et al., 2015). These have delivered excellent results, and in particular the use of the Dense Conditional Random Field (DenseCRF) of Kr\u00e4henb\u00fchl & Koltun (2011) by Chen et al. , 2014; Chen et al., 2015; Papandreou et al., 2015a; Zheng et al., 2015). These have delivered excellent results, and in particular the use of the Dense Conditional Random Field (DenseCRF) of Kr\u00e4henb\u00fchl & Koltun (2011) by Chen et al. (2015); Papandreou et al. , 2014; Chen et al., 2015; Papandreou et al., 2015a; Zheng et al., 2015). These have delivered excellent results, and in particular the use of the Dense Conditional Random Field (DenseCRF) of Kr\u00e4henb\u00fchl & Koltun (2011) by Chen et al. (2015); Papandreou et al. (2015a); Zheng et al. , 2014; Chen et al., 2015; Papandreou et al., 2015a; Zheng et al., 2015). These have delivered excellent results, and in particular the use of the Dense Conditional Random Field (DenseCRF) of Kr\u00e4henb\u00fchl & Koltun (2011) by Chen et al. (2015); Papandreou et al. (2015a); Zheng et al. (2015), has enhanced the discriminative power of FCNNs with local evidence gathered by the image intensity. Following Chen et al. (2015) we define the CRF distribution as: 7 DeepLab-MSc-CRF-LF-COCO-CJ (Chen et al., 2015) 73. 9 DeepLab-CRF-COCO-LF(Chen et al., 2015) 72.",
        "context": "responses at the \ufb01nal DCNN layer with a fully connected Conditional Random\nField (CRF). Qualitatively, our \u201cDeepLab\u201d system is able to localize segment\nboundaries at a level of accuracy which is beyond previous methods. Quantita-\nPublished as a conference paper at ICLR 2015\nSEMANTIC IMAGE SEGMENTATION WITH DEEP CON-\nVOLUTIONAL NETS AND FULLY CONNECTED CRFS\nLiang-Chieh Chen\nUniv. of California, Los Angeles\nlcchen@cs.ucla.edu\nGeorge Papandreou \u2217\nGoogle Inc.\ngpapan@google.com\n2008), but recently Kr\u00a8ahenb\u00a8uhl & Koltun (2011) showed that the inference can be very ef\ufb01cient for\nfully connected CRF and particularly effective in the context of semantic segmentation."
    },
    {
        "title": "Spectral segmentation with multiscale graph decomposition",
        "author": [
            "Cour",
            "Timoth\u00e9e",
            "B\u00e9n\u00e9zit",
            "Florence",
            "Shi",
            "Jianbo"
        ],
        "venue": "In Proc. CVPR,",
        "citeRegEx": "Cour et al\\.,? \\Q2005\\E",
        "shortCiteRegEx": "Cour et al\\.",
        "year": 2005,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation",
        "author": [
            "Dai",
            "Jifeng",
            "He",
            "Kaiming",
            "Sun",
            "Jian"
        ],
        "venue": "arXiv preprint arXiv:1503.01640,",
        "citeRegEx": "Dai et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "Dai et al\\.",
        "year": 2015,
        "abstract": "Recent leading approaches to semantic segmentation rely on deep convolutional\nnetworks trained with human-annotated, pixel-level segmentation masks. Such\npixel-accurate supervision demands expensive labeling effort and limits the\nperformance of deep networks that usually benefit from more training data. In\nthis paper, we propose a method that achieves competitive accuracy but only\nrequires easily obtained bounding box annotations. The basic idea is to iterate\nbetween automatically generating region proposals and training convolutional\nnetworks. These two steps gradually recover segmentation masks for improving\nthe networks, and vise versa. Our method, called BoxSup, produces competitive\nresults supervised by boxes only, on par with strong baselines fully supervised\nby masks under the same setting. By leveraging a large amount of bounding\nboxes, BoxSup further unleashes the power of deep convolutional networks and\nyields state-of-the-art results on PASCAL VOC 2012 and PASCAL-CONTEXT.",
        "full_text": "BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks\nfor Semantic Segmentation\nJifeng Dai\nKaiming He\nJian Sun\nMicrosoft Research\n{jifdai,kahe,jiansun}@microsoft.com\nAbstract\nRecent leading approaches to semantic segmentation\nrely on deep convolutional networks trained with human-\nannotated, pixel-level segmentation masks.\nSuch pixel-\naccurate supervision demands expensive labeling effort and\nlimits the performance of deep networks that usually bene\ufb01t\nfrom more training data. In this paper, we propose a method\nthat achieves competitive accuracy but only requires eas-\nily obtained bounding box annotations. The basic idea is\nto iterate between automatically generating region propos-\nals and training convolutional networks. These two steps\ngradually recover segmentation masks for improving the\nnetworks, and vise versa. Our method, called \u201cBoxSup\u201d,\nproduces competitive results (e.g., 62.0% mAP for valida-\ntion) supervised by boxes only, on par with strong base-\nlines (e.g., 63.8% mAP) fully supervised by masks under\nthe same setting. By leveraging a large amount of bounding\nboxes, BoxSup further unleashes the power of deep convo-\nlutional networks and yields state-of-the-art results on PAS-\nCAL VOC 2012 and PASCAL-CONTEXT [24].\n1. Introduction\nIn the past few months, tremendous progress has been\nmade in the \ufb01eld of semantic segmentation [12, 22, 13, 6, 5,\n23]. Deep convolutional neural networks (CNNs) [19, 18]\nthat play as rich hierarchical feature extractors are a key to\nthese methods. These networks are trained on large-scale\ndatasets [7, 27] as classi\ufb01ers, and transferred to the seman-\ntic segmentation tasks based on the annotated segmentation\nmasks as supervision.\nBut pixel-level mask annotations are time-consuming,\nfrustrating, and in the end commercially expensive to ob-\ntain. According to the annotation report of the large-scale\nMicrosoft COCO dataset [21], the workload of labeling seg-\nmentation masks is more than 15 times heavier than that of\nspotting object locations. Further, the crowdsourcing anno-\ntators need to be specially trained for the tedious and dif\ufb01-\ncult task of labeling per-pixel masks. These facts limit the\namount of available segmentation mask annotations, and\nthus hinder the performance of CNNs that in general de-\nsire large-scale data for training. On the contrary, bounding\nbox annotations are more economical than masks. There\nhave already existed a large number of available box-level\nannotations in datasets like PASCAL VOC 20071 [8] and\nImageNet [27]. Though these box-level annotations are less\nprecise than pixel-level masks, their amount may help im-\nprove training deep networks for semantic segmentation.\nIn addition, current leading approaches have not fully\nutilized the detailed pixel-level annotations. For example,\nin the Convolutional Feature Masking (CFM) method [6],\nthe \ufb01ne-resolution masks are used to generate very low-\nresolution (e.g., 6 \u00d7 6) masks on the feature maps. In the\nFully Convolutional Network (FCN) method [22], the net-\nwork predictions are regressed to the ground-truth masks\nusing a large stride (e.g., 8 pixels). These methods yield\ncompetitive results without explicitly harnessing the \ufb01ner\nmasks. If we consider the box-level annotations as very\ncoarse masks, can we still retain comparably good results\nwithout using the segmentation masks?\nIn this work, we investigate bounding box annotations\nas an alternative or extra source of supervision to train con-\nvolutional networks for semantic segmentation2. We resort\nto unsupervised region proposal methods [31, 2] to gener-\nate candidate segmentation masks. The convolutional net-\nwork is trained under the supervision of these approximate\nmasks. The updated network in turn improves the estimated\nmasks used for training. This process is iterated. Although\nthe masks are coarse at the beginning, they are gradually\nimproved and then provide useful information for network\ntraining. Fig. 1 illustrates our training algorithm.\nWe extensively evaluate our method, called \u201cBoxSup\u201d,\non the PASCAL segmentation benchmarks [8, 24].\nOur\n1The PASCAL VOC 2007 dataset only has bounding box annotations.\n2The idea of using bounding box annotations for CNN-based semantic\nsegmentation is developed concurrently and independently in [25]. We\nalso compare with the results of [25].\n1\narXiv:1503.01640v2  [cs.CV]  18 May 2015\ntrain image \nwith gt boxes\ndog\nperson\n\u2026\ncandidate masks\nupdate\nnetwork\nupdate\nmasks\nBoxSup training\nfeedback\niteration\nestimated masks\nepoch\n#5\nepoch\n#1\nepoch\n#20\nnetwork for \nsegmentation\nFigure 1: Overview of our training approach supervised by bounding boxes.\nbox-supervised (i.e., using bounding box annotations)\nmethod shows a graceful degradation compared with its\nmask-supervised (i.e., using mask annotations) counterpart.\nAs such, our method waives the requirement of pixel-level\nmasks for training. Further, our semi-supervised variant in\nwhich 9/10 mask annotations are replaced with bounding\nbox annotations yields comparable accuracy with the fully\nmask-supervised counterpart. This suggests that we may\nsave expensive labeling effort by using bounding box anno-\ntations dominantly. Moreover, our method makes it possible\nto harness the large number of available box annotations to\nimprove the mask-supervised results. Using the limited pro-\nvided mask annotations and extra large-scale bounding box\nannotations, our method achieves state-of-the-art results on\nboth PASCAL VOC 2012 and PASCAL-CONTEXT [24]\nbenchmarks.\nWhy can a large amount of bounding boxes help im-\nprove convolutional networks? Our error analysis reveals\nthat a BoxSup model trained with a large set of boxes ef-\nfectively increases the object recognition accuracy (the ac-\ncuracy in the middle of an object), and its improvement on\nobject boundaries is secondary. Though a box is too coarse\nto contain detailed segmentation information, it provides an\ninstance for learning to distinguish object categories. The\nlarge-scale object instances improve the feature quality of\nthe learned convolutional networks, and thus impact the\noverall performance for semantic segmentation.\n2. Related Work\nDeep convolutional networks in general have better ac-\ncuracy with the growing size of training data, as is evi-\ndenced in [18, 34]. The ImageNet classi\ufb01cation dataset [27]\nis one of the largest datasets with quality labels, but the cur-\nrent available datasets for object detection, semantic seg-\nmentation, and many other vision tasks mostly have orders\nof magnitudes fewer labeled samples. The milestone work\nof R-CNN [9] proposes to pre-train deep networks as classi-\n\ufb01ers on the large-scale ImageNet dataset and go on training\n(\ufb01ne-tuning) them for other tasks that have limited number\nof training data. This transfer learning strategy is widely\nadopted for object detection [9, 14, 30], semantic segmen-\ntation [12, 22, 13, 6, 5, 23], visual tracking [32], and other\nvisual recognition tasks.\nWith the continuously improv-\ning deep convolutional models [34, 28, 4, 14, 29, 30, 15],\nthe accuracy of these vision tasks also improves thanks to\nthe more powerful generic features learned from large-scale\ndatasets.\nAlthough pre-training partially relieves the problem of\nlimited data, the amount of the task-speci\ufb01c data for \ufb01ne-\ntuning still matters.\nIn [1], it has been found that aug-\nmenting the object detection training set by combining the\nVOC 2007 and VOC 2012 sets improves object detection\naccuracy compared with using VOC 2007 only. In [20],\nthe training set for object detection is augmented by visual\ntracking results obtained from videos and improves detec-\ntion accuracy. These experiments demonstrate the impor-\ntance of dataset sizes for task-speci\ufb01c network training.\nFor semantic segmentation, there have been existing pa-\npers [33, 10] that investigate exploiting bounding box anno-\ntations instead of masks. But the box-level annotations have\nnot been used to supervised deep convolutional networks in\nthose works.\n3. Baseline\nOur BoxSup method is in general applicable for many\nexisting CNN-based mask-supervised semantic segmenta-\ntion methods, such as FCN [22], improvements on FCN\n[5, 35], and others [13, 6, 23]. In this paper, we adopt our\nimplementation of the FCN method [22] re\ufb01ned by CRF [5]\nas the mask-supervised baseline, which we brie\ufb02y introduce\n2\n(d) GrabCut\n(e) ours\n(b) ground-truth\n(c) rectangles\n(a) training image\n(d) GrabCut\n(e) ours\n(a) training image\nperson\nhorse\n(b) ground-truth\n(c) rectangles\nFigure 2: Segmentation masks used as supervision. (a) A training image. (b) Ground-truth. (c) Each box is na\u00a8\u0131vely considered\nas a rectangle mask. (d) A segmentation mask is generated by GrabCut [26]. (e) For our method, the supervision is estimated\nfrom region proposals (MCG [2]) by considering bounding box annotations and network feedbacks.\nas follows.\nThe network training of FCN [22] is formulated as a per-\npixel regression problem to the ground-truth segmentation\nmasks. Formally, the objective function can be written as:\nE(\u03b8) =\nX\np\ne(X\u03b8(p), l(p)),\n(1)\nwhere p is a pixel index, l(p) is the ground-truth seman-\ntic label at a pixel, and X\u03b8(p) is the per-pixel labeling pro-\nduced by the fully convolutional network with parameters \u03b8.\ne(X\u03b8(p), l(p)) is the per-pixel loss function. The network\nparameters \u03b8 are updated by back-propagation and stochas-\ntic gradient descent (SGD). A CRF is used to post-process\nthe FCN results [5].\nThe objective function in Eqn.(1) demands pixel-level\nsegmentation masks l(p) as supervision. It is not directly\napplicable if only bounding box annotations are given as\nsupervision. Next we introduce our method for addressing\nthis problem.\n4. Approach\n4.1. Unsupervised Segmentation for Supervised Training\nTo harness the bounding boxes annotations, it is desired\nto estimate segmentation masks from them. This is a widely\nstudied supervised image segmentation problem, and can\nbe addressed by, e.g., GrabCut [26]. But GrabCut can only\ngenerate one or a few samples from one box, which may be\ninsuf\ufb01cient for deep network training.\nWe propose to generate a set of candidate segments us-\ning unsupervised region proposal methods (e.g., Selective\nSearch [31]) due to their nice properties. First, region pro-\nposal methods have high recall rates [2] of having a good\ncandidate in the proposal pool. Second, region proposal\nmethods generate candidates of greater variance, which pro-\nvide a kind of data augmentation [18] for network training.\nWe will show by experiments the improvements of these\nproperties.\nThe candidate segments are used to update the deep con-\nvolutional network. The semantic features learned by the\nnetwork are then used to pick better candidates. This proce-\ndure is iterated. We formulate this procedure as an objective\nfunction as we will describe below.\nIt is worth noticing that the region proposal is only used\nfor networking training. For inference, the trained FCN is\ndirectly applied on the image and produces pixel-wise pre-\ndictions. So our usage of region proposals does not impact\nthe test-time ef\ufb01ciency.\n4.2. Formulation\nAs a pre-processing, we use a region proposal method to\ngenerate segmentation masks. We adopt Multiscale Combi-\nnatorial Grouping (MCG) [2] by default, while other meth-\nods [31, 17] are also evaluated.\nThe proposal candidate\nmasks are \ufb01xed throughout the training procedure. But dur-\ning training, each candidate mask will be assigned a label\nwhich can be a semantic category or background. The la-\nbels assigned to the masks will be updated.\nWith a ground-truth bounding box annotation, we expect\nit to pick out a candidate mask that overlaps the box as much\nas possible. Formally, we de\ufb01ne an overlapping objective\nfunction Eo as:\nEo = 1\nN\nX\nS\n(1 \u2212IoU(B, S))\u03b4(lB, lS).\n(2)\nHere S represents a candidate segment mask, and B repre-\nsents a ground-truth bounding box annotation. IoU(B, S) \u2208\n[0, 1] is the intersection-over-union ratio computed from the\nground-truth box B and the tight bounding box of the seg-\nment S. The function \u03b4 is equal to one if the semantic label\nlS assigned to segment S is the same as the ground-truth\nlabel lB of the bounding box B, and zero otherwise. Min-\nimizing Eo favors higher IoU scores when the semantic la-\nbels are consistent. This objective function is normalized\nby the number of candidate segments N.\n3\ntraining image\nepoch #1\nepoch #20\nepoch #5\ntraining image\nepoch #1\nepoch #20\nepoch #5\nperson\nchair\nFigure 3: Update of segmentation masks during training. Here we show the masks in epoch #1, epoch #5, and epoch #20.\nEach segmentation mask will be used as the supervision for the next epoch.\nWith the candidate masks and their estimated semantic\nlabels, we can supervise the deep convolutional network as\nin Eqn.(1). Formally, we consider the following regression\nobjective function Er:\nEr =\nX\np\ne(X\u03b8(p), lS(p)).\n(3)\nHere lS is the estimated semantic label used as supervision\nfor the network training. This objective function is the same\nas Eqn.(1) except that its regression target is the estimated\ncandidate segment.\nWe minimize an objective function that combines the\nabove two terms:\nmin\n\u03b8,{lS}\nX\ni\n(Eo + \u03bbEr)\n(4)\nHere the summation P\ni runs over the training images, and\n\u03bb = 3 is a \ufb01xed weighting parameter. The variables to\nbe optimized are the network parameters \u03b8 and the labeling\n{lS} of all candidate segments {S}. If only the term Eo\nexists, the optimization problem in Eqn.(4) trivially \ufb01nds a\ncandidate segment that has the largest IoU score with the\nbox; if only the term Er exists, the optimization problem in\nEqn.(4) is equivalent to FCN. Our formulation simultane-\nously considers both cases.\n4.3. Training Algorithm\nThe objective function in Eqn.(4) involves a problem of\nassigning labels to the candidate segments. Next we pro-\npose a greedy iterative solution to \ufb01nd a local optimum.\nWith the network parameters \u03b8 \ufb01xed, we update the se-\nmantic labeling {lS} for all candidate segments.\nIn our\nimplementation, we only consider the case in which one\nground-truth bounding box can \u201cactivate\u201d (i.e., assign a\nnon-background label to) one and only one candidate. As\nsuch, we can simply update the semantic labeling by select-\ning a single candidate segment for each ground-truth bound-\ning box, such that its cost Eo + \u03bbEr is the smallest among\nall candidates. The selected segment is assigned the ground-\ntruth semantic label associated with that bounding box. All\nother pixels are assigned the background label.\nThe above winner-takes-all selection tends to repeatedly\nuse the same or very similar candidate segments, and the op-\ntimization procedure may be trapped in poor local optima.\nTo increase the sample variance for better stochastic train-\ning, we further adopt a random sampling method to select\nthe candidate segment for each ground-truth bounding box.\nInstead of selecting the single segment with the largest cost\nEo + \u03bbEr, we randomly sample a segment from the \ufb01rst k\nsegments with the largest costs. In this paper we use k = 5.\nThis random sampling strategy improves the accuracy by\nabout 2% on the validation set.\nWith the semantic labeling {lS} of all candidate seg-\nments \ufb01xed, we update the network parameters \u03b8. In this\ncase, the problem becomes the FCN problem [22] as in\nEqn.(1). This problem is minimized by SGD.\nWe iteratively perform the above two steps, \ufb01xing one set\nof variables and solving for the other set. For each iteration,\nwe update the network parameters using one training epoch\n(i.e., all training images are visited once), and after that we\nupdate the segment labeling of all images. Fig.3 shows the\ngradually updated segmentation masks during training. The\nnetwork is initialized by the model pre-trained in the Ima-\ngeNet classi\ufb01cation dataset, and our algorithm starts from\nthe step of updating segment labels.\nOur method is applicable for the semi-supervised case\n(the ground-truth annotations are mixtures of segmentation\nmasks and bounding boxes). The labeling l(p) is given by\ncandidate proposals as above if a sample only has ground-\ntruth boxes, and is simply assigned as the true label if a\nsample has ground-truth masks.\nIn the SGD training of updating the network, we use a\nmini-batch size of 20, following [22]. The learning rate\nis initialized to be 0.001 and divided by 10 after every 15\nepochs. The training is terminated after 45 epochs.\n5. Experiments\nIn all our experiments, we use the publicly released\nVGG-16 model3 [29] that is pre-trained on ImageNet [27].\nThe VGG model is also used by all competitors [22, 13, 6,\n3www.robots.ox.ac.uk/\u02dcvgg/research/very_deep/\n4\ndata\nVOC train\nVOC train + COCO\ntotal #\n10,582\n133,869\nsupervision\nmask\nbox\nsemi\nmask\nsemi\nmask #\n10,582\n-\n1,464\n133,869\n10,582\nbox #\n-\n10,582\n9,118\n-\n123,287\nmean IoU\n63.8\n62.0\n63.5\n68.1\n68.2\nTable 1: Comparisons of supervision in PASCAL VOC\n2012 validation.\n5, 23] compared in this paper.\n5.1. Experiments on PASCAL VOC 2012\nWe \ufb01rst evaluate our method on the PASCAL VOC\n2012 semantic segmentation benchmark [8]. This dataset\ninvolves 20 semantic categories of objects.\nWe use the\n\u201ccomp6\u201d evaluation protocol. The accuracy is evaluated by\nmean IoU scores. The original training data has 1,464 im-\nages. Following [11], the training data with ground-truth\nsegmentation masks are augmented to 10,582 images. The\nvalidation and test sets have 1,449 and 1,456 images respec-\ntively. When evaluating the validation set or the test set, we\nonly use the training set for training. A held-out 100 ran-\ndom validation images are used for cross-validation to set\nhyper-parameters.\nComparisons of Supervision Strategies\nTable 1 compares the results of using different strategies\nof supervision on the validation set. When all ground-truth\nmasks are used as supervision, the result is our implemen-\ntation of the baseline DeepLab-CRF [5]. Our reproduction\nhas a score of 63.8 (Table 1, \u201cmask only\u201d), which is very\nclose to 63.74 reported in [5] under the same setting. So we\nbelieve that our reproduced baseline is convincing.\nWhen all 10,582 training samples are replaced with\nbounding box annotations, our method yields a score of\n62.0 (Table 1, \u201cbox only\u201d).\nThough the supervision in-\nformation is substantially weakened, our method shows a\ngraceful degradation (1.8%) compared with the strongly su-\npervised baseline of 63.8. This indicates that in practice we\ncan avoid the expensive mask labeling effort by using only\nbounding boxes, with small accuracy loss.\nTable 1 also shows the semi-supervised result of our\nmethod.\nThis result uses the ground-truth masks of the\noriginal 1,464 training images and the bounding box an-\nnotations of the rest 9k images. The score is 63.5 (Table 1,\n\u201csemi\u201d), on par with the strongly supervised baseline. Such\nsemi-supervision replaces 9/10 of the segmentation mask\nannotations with bounding box annotations. This means\nthat we can greatly reduce the labeling effort by dominantly\nusing bounding box annotations.\nAs a proof of concept, we further evaluate using a sub-\n\u0002\n\u0003\n\u0004\n\u0005\n\u0006\u0007\n\u0006\u0002\n\u0006\u0003\n\u0006\u0004\n\u0006\u0005\n\u0002\u0007\n\u0003\u0007\n\u0003\b\n\b\u0007\n\b\b\n\u0004\u0007\n\u0004\b\n\t\u0007\n\t\b\n\u0005\u0007\n\u0002\u0003\u0004\u0005\u0006\u0007\b\t\u0004\n\u000b\f\b\r\u0007\u0004\u000e\u000f\u0010\u0011\u0012\n\u0002\u0003\u0004\u0005\u0006\u0007\b\t\u0006\n\u000b\f\n \n \n\u0002\u0003\u0004\u0005\u0006\u0007\b\t\u0005\n\u0005\u0004\u0003\u0004\u0003\u0005\u000b\f\r\u0005\u000e\u000f\u0010\u0011\u0012\u0013\u000f\f\u0013\u0014\n\u0002\u0003\u0004\u0005\u0006\u0007\b\t\u0005\u000e\u000f\u0010\u0011\u0012\u0013\u000f\f\u0013\u0014\n\u0002\u0003\u0004\u0005\u0006\u0007\b\t\u0005\n\u0005\u0004\u0003\u0004\u0003\u0005\u000b\f\r\u0005\u000e\u000b\f\u0015\u0010\u0016\u0007\u0013\u0017\u0014\n\u0002\u0003\u0004\u0005\u0006\u0007\b\t\u0005\u000e\u000b\f\u0015\u0010\u0016\u0007\u0013\u0017\u0014\nimage\nground-truth\nboundary\ninterior\nFigure 4: Error analysis on the validation set. Top: (from\nleft to right) image, ground-truth, boundary regions marked\nas white, interior regions marked as white).\nBottom:\nboundary and interior mean IoU, using VOC masks only\n(blue) and using extra COCO boxes (red).\nstantially larger set of boxes. We use the Microsoft COCO\ndataset [21] that has 123,287 images with available ground-\ntruth segmentation masks. This dataset has 80 semantic cat-\negories, and we only use the 20 categories that also present\nin PASCAL VOC. For our mask-supervised baseline, the\nresult is a score of 68.1 (Table 1). Then we replace the\nground-truth segmentation masks in COCO with their tight\nbounding boxes. Our semi-supervised result is 68.2 (Ta-\nble 1), on par with the strongly supervised baseline. Fig. 5\nshows some visual results in the validation set.\nThe semi-supervised result (68.2) that uses VOC+COCO\nis considerably better than the strongly supervised result\n(63.8) that uses VOC only. The 4.4% gain is contributed\nby the extra large-scale bounding boxes in the 123k COCO\nimages. This comparison suggests a promising strategy -\nwe may make use of the larger amount of existing bounding\nboxes annotations to improve the overall semantic segmen-\ntation results, as further analyzed below.\nError Analysis\nWhy can a large set of bounding boxes help improve\nconvolutional networks? The error in semantic segmenta-\ntion can be roughly thought of as two types: (i) recogni-\ntion error that is due to confusions of recognizing object\ncategories, and (ii) boundary error that is due to misalign-\nments of pixel-level labels on object boundaries. Although\nthe bounding box annotations have no information about the\nobject boundaries, they provide extra object instances for\nrecognizing them. We may expect that the large amount of\n5\nmasks\nmean IoU\nrectangles\n52.3\nGrabCut\n55.2\nWSSL [25]\n58.5\nours w/o sampling\n59.7\nours\n62.0\nTable 2: Comparisons of estimated masks for supervision\nin PASCAL VOC 2012 validation. All methods only use\n10,582 bounding boxes as annotations, with no ground-\ntruth segmentation mask used.\nSS\nGOP\nMCG\nmean IoU\n59.5\n60.4\n62.0\nTable 3: Comparisons of the effects of region proposal\nmethods on our method in PASCAL VOC 2012 validation.\nAll methods only use 10,582 bounding boxes as annota-\ntions, with no ground-truth segmentation mask used.\nboxes mainly improve the recognition accuracy.\nTo analyze the error, we separately evaluate the perfor-\nmance on the boundary regions and interior regions. Fol-\nlowing [16, 5], we generate a \u201ctrimap\u201d near the ground-truth\nboundaries (Fig. 4, top). We evaluate mean IoU scores in-\nside/outside the bands, referred to as boundary/interior re-\ngions. Fig. 4 (bottom) shows the results of using different\nband widths for the trimaps.\nFor the interior region, the accuracy of using the extra\nCOCO boxes (red solid line, Fig. 4) is considerably higher\nthan that of using VOC masks only (blue solid line). On the\ncontrary, the improvement on the boundary regions is rela-\ntively smaller (red dash line vs. blue dash line). Note that\ncorrectly recognizing the interior may also help improve the\nboundaries (e.g., due to the CRF post-processing). So the\nimprovement of the extra boxes on the boundary regions is\nsecondary.\nBecause the accuracy in the interior region is mainly de-\ntermined by correctly recognizing objects, this analysis sug-\ngests that the large amount of boxes improve the feature\nquality of a learned BoxSup model for better recognition.\nComparisons of Estimated Masks for Supervision\nIn Table 2 we evaluate different methods of estimating\nmasks from bounding boxes for supervision. As a na\u00a8\u0131ve\nbaseline, we \ufb01ll each bounding box with its semantic la-\nbel, and consider it as a rectangular mask (Fig. 2(c)). Us-\ning these rectangular masks as the supervision throughout\ntraining, the score is 52.3 on the validation set. We also use\nGrabCut [26] to generate segmentation masks from boxes\n(Fig. 2(d)).\nWith the GrabCut masks as the supervision\nmethod\nsup.\nmask #\nbox #\nmIoU\nFCN [22]\nmask\nV 10k\n-\n62.2\nDeepLabCRF [5]\nmask\nV 10k\n-\n66.4\nWSSL [25]\nbox\n-\nV 10k\n60.4\nBoxSup\nbox\n-\nV 10k\n64.6\nBoxSup\nsemi\nV 1.4k\nV 9k\n66.2\nWSSL [25]\nmask\nV+C 133k\n-\n70.4\nBoxSup\nsemi\nV 10k\nC 123k\n71.0\nBoxSup\nsemi\nV 10k\nV07+C 133k\n73.1\nBoxSup+\nsemi\nV 10k\nV07+C 133k\n75.2\nTable 4: Results on PASCAL VOC 2012 test set. In the su-\npervision (\u201csup\u201d) column, \u201cmask\u201d means all training sam-\nples are with segmentation mask annotations, \u201cbox\u201d means\nall training samples are with bounding box annotations, and\n\u201csemi\u201d means mixtures. \u201cV\u201d denotes the VOC data, \u201cC\u201d\ndenotes the COCO data, and \u201cV07\u201d denotes the VOC 2007\ndata which only has bounding boxes available.\nthroughout training, the score is 55.2. In both cases, the\nmasks are not updated by the network feedbacks.\nOur method has a score 62.0 (Table 2) using the same\nset of bounding box annotations. This is a considerable gain\nover the baseline using \ufb01xed GrabCut masks. This indicates\nthe importance of the mask quality for supervision. Fig. 3\nshows that our method iteratively updates the masks by the\nnetwork, which in turn improves the network training.\nWe also evaluate a variant of our method where each\ntime the updated mask is the candidate with the largest cost,\ninstead of randomly sampled from the \ufb01rst k candidates (see\nSec. 4.3). This variant has a lower score of 59.7 (Table 2).\nThe random sampling strategy, which is data augmentation\nand increases sample variances, is bene\ufb01cial for training.\nTable 2 also shows the result of the concurrent method\nWSSL [5] under the same evaluation setting. Its results is\n58.5. This result suggests that our method estimates more\naccurate masks than [5] for supervision.\nComparisons of Region Proposals\nOur method resorts to unsupervised region proposals for\ntraining. In Table 3, we compare the effects of various re-\ngion proposals on our method: Selective Search (SS) [31],\nGeodesic Object Proposals (GOP) [17], and MCG [2]. Ta-\nble 3 shows that MCG [2] has the best accuracy, which is\nconsistent with its segmentation quality evaluated by other\nmetrics in [2]. Note that at test-time our method does not\nneed region proposals. So the better accuracy of using MCG\nimplies that our method effectively makes use of the higher\nquality segmentation masks to train a better network.\nComparisons on the Test Set\nNext we compare with the state-of-the-art methods on\n6\n(c) box, VOC\n(a) image\n(b) mask, VOC\n(d) semi, VOC mask +COCO box\nFigure 5: Example semantic segmentation results on PASCAL VOC 2012 validation using our method. (a) Images. (b)\nSupervised by masks in VOC. (c) Supervised by boxes in VOC. (d) Supervised by masks in VOC and boxes in COCO.\nthe PASCAL VOC 2012 test set. In Table 4, the methods\nare based on the same FCN baseline and thus fair compar-\nisons are made to evaluate the impact of mask/box/semi-\nsupervision.\nAs shown in Table 4, our box-supervised result that only\nuses VOC bounding boxes is 64.6. This compares favor-\nably with the WSSL [25] counterpart (60.4) under the same\nsetting. On the other hand, our box-supervised result has\na graceful degradation (1.8%) compared with the mask-\nsupervised DeepLab-CRF (66.4 [5]) using the VOC training\ndata. Moreover, our semi-supervised variant which replaces\n9/10 segmentation mask annotations with bounding boxes\nhas a score of 66.2. This is on par with the mask-supervised\ncounterpart of DeepLab-CRF, but the supervision informa-\ntion used by our method is much weaker.\nIn the WSSL paper [25], by using all segmentation\nmask annotations in VOC and COCO, the strongly mask-\nsupervised result is 70.4.\nOur semi-supervised method\nshows a higher score of 71.0. Remarkably, our result uses\nthe bounding box annotations from the 123k COCO images.\nSo our method has a more accurate result but uses much\nweaker annotations than [25].\nOn the other hand, compared with the DeepLab-CRF re-\nsult (66.4), our method has a 4.6% gain enjoyed from ex-\nploiting the bounding box annotations of the COCO dataset.\nThis comparison demonstrates the power of our method that\nexploits large-scale bounding box annotations to improve\naccuracy.\nExploiting Boxes in PASCAL VOC 2007\nTo further demonstrate the effect of BoxSup, we exploit\nthe bounding boxes in the PASCAL VOC 2007 dataset [8].\nThis dataset has no mask annotations. It is a de facto dataset\nwhich mask-supervised methods are not able to use.\nWe exploit all 10k images in the VOC 2007 trainval and\ntest sets. We train a BoxSup model using the union set of\nVOC 2007 boxes, COCO boxes, and the augmented VOC\n2012 training set. The score improves from 71.0 to 73.1 (Ta-\nble 4) because of the extra box training data. It is reasonable\nfor us to expect further improvement if more bounding box\nannotations are available.\nBaseline Improvement\nAlthough our focus is mainly on exploiting boxes as su-\npervision, it is worth noticing that our method may also\nbene\ufb01t from other improvements on the mask-sup baseline\n(FCN in our case). Concurrent with our work, there are a se-\nries of improvements [35, 5] made on FCN, which achieve\nexcellent results using strong mask-supervision from VOC\nand COCO data.\nTo show the potential of our BoxSup method in parallel\nwith improvements on the baseline, we use a simple test-\ntime augmentation to boost our results. Instead of comput-\n7\n2008_002965\n2008_000423\n2010_003540\n(c) baseline\n(a) image\n(b) ground-truth\n(d) BoxSup\n2010_000524\nFigure 6: Example results on PASCAL-CONTEXT validation. (a) Images. (b) Results of our baseline (35.7 mean IoU),\ntrained using VOC masks. (c) Results of BoxSup (40.5 mean IoU), trained using VOC masks and COCO boxes.\nmethod\nsup.\nmask #\nbox #\nmean IoU\nO2P [3]\nmask\nV 5k\n-\n18.1\nCFM [6]\nmask\nV 5k\n-\n34.4\nFCN [22]\nmask\nV 5k\n-\n35.1\nbaseline\nmask\nV 5k\n-\n35.7\nBoxSup\nsemi\nV 5k\nC 123k\n40.5\nTable 5: Results on PASCAL-CONTEXT [24] validation.\nOur baseline is our implementation of FCN+CRF. \u201cV\u201d de-\nnotes the VOC data, and \u201cC\u201d denotes the COCO data.\ning pixel-wise predictions on a single scale, we compute\nthe score maps from two extra scales (\u00b120% of the orig-\ninal image size) and bilinearly re-scale the score maps to\nthe original size. The scores from three scales are aver-\naged. This simple modi\ufb01cation boosts our result from 73.1\nto 75.2 (BoxSup+, Table 4) in the VOC 2012 test set. This\nresult is on par with the latest results using strong mask-\nsupervision from both VOC and COCO, but in our case the\nCOCO dataset only provides bounding boxes.\n5.2. Experiments on PASCAL-CONTEXT\nWe further perform experiments on the recently labeled\nPASCAL-CONTEXT dataset [24]. This dataset provides\nground-truth semantic labels for the whole scene, including\nobject and stuff (e.g., grass, sky, water). Following the pro-\ntocol in [24, 6, 22], the semantic segmentation is performed\non the most frequent 59 categories (identi\ufb01ed by [24]) plus\na background category. The accuracy is measured by mean\nIoU scores. The training and evaluation are performed on\nthe training and validation sets that have 4,998 and 5,105\nimages respectively.\nTo train a BoxSup model for this dataset, we \ufb01rst use the\nbox annotations from all 80 object categories in the COCO\ndataset to train the FCN (using VGG-16). This network\nends with an 81-way (with an extra one for background)\nlayer. Then we remove this last layer and add a new 60-\nway layer for the 59 categories of PASCAL-CONTEXT. We\n\ufb01ne-tune this model in the 5k training images of PASCAL-\nCONTEXT. A CRF for post-processing is also used. We do\nno use the test-time scale augmentation.\nTable 5 shows the results in PASCAL-CONTEXT. The\nmethods of CFM [6] and FCN [22] are both based on the\nVGG-16 model. Our baseline method, which is our imple-\nmentation of FCN+CRF, has a score of 35.7 using masks\nof the 5k training images. Using our BoxSup model pre-\ntrained using the COCO boxes, the result is improved to\n40.5. The 4.8% gain is solely because of the bounding box\nannotations in COCO that improve our network training.\nFig. 6 shows some examples of our results for joint object\nand stuff segmentation.\n8\n6. Conclusion\nThe proposed BoxSup method can effectively harness\nbounding box annotations to train deep networks for se-\nmantic segmentation. Our BoxSup method that uses 133k\nbounding boxes and 10k masks achieves state-of-the-art re-\nsults. Our error analysis suggests that semantic segmen-\ntation accuracy is hampered by the failure of recognizing\nobjects, which large-scale data may help with.\nReferences\n[1] P. Agrawal, R. Girshick, and J. Malik. Analyzing the perfor-\nmance of multilayer neural networks for object recognition.\nIn ECCV, 2014.\n[2] P. Arbel\u00b4aez, J. Pont-Tuset, J. T. Barron, F. Marques, and\nJ. Malik.\nMultiscale combinatorial grouping.\nIn CVPR,\n2014.\n[3] J. Carreira, R. Caseiro, J. Batista, and C. Sminchisescu. Se-\nmantic segmentation with second-order pooling. In ECCV.\n2012.\n[4] K. Chat\ufb01eld, K. Simonyan, A. Vedaldi, and A. Zisserman.\nReturn of the devil in the details: Delving deep into convo-\nlutional nets. In BMVC, 2014.\n[5] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and\nA. L. Yuille. Semantic image segmentation with deep con-\nvolutional nets and fully connected crfs. In ICLR, 2015.\n[6] J. Dai, K. He, and J. Sun. Convolutional feature masking for\njoint object and stuff segmentation. In CVPR, 2015.\n[7] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\nFei. Imagenet: A large-scale hierarchical image database. In\nCVPR, 2009.\n[8] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and\nA. Zisserman. The PASCAL Visual Object Classes (VOC)\nChallenge. IJCV, 2010.\n[9] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-\nture hierarchies for accurate object detection and semantic\nsegmentation. 2014.\n[10] M. Guillaumin, D. K\u00a8uttel, and V. Ferrari. Imagenet auto-\nannotation with segmentation propagation. IJCV, 2014.\n[11] B. Hariharan, P. Arbel\u00b4aez, L. Bourdev, S. Maji, and J. Malik.\nSemantic contours from inverse detectors. In ICCV, 2011.\n[12] B. Hariharan, P. Arbel\u00b4aez, R. Girshick, and J. Malik. Simul-\ntaneous detection and segmentation. In ECCV. 2014.\n[13] B. Hariharan, P. Arbel\u00b4aez, R. Girshick, and J. Malik. Hyper-\ncolumns for object segmentation and \ufb01ne-grained localiza-\ntion. In CVPR, 2015.\n[14] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling\nin deep convolutional networks for visual recognition.\nIn\nECCV, 2014.\n[15] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into\nrecti\ufb01ers: Surpassing human-level performance on imagenet\nclassi\ufb01cation. arXiv:1502.01852, 2015.\n[16] P. Kohli, P. H. Torr, et al. Robust higher order potentials for\nenforcing label consistency. IJCV, pages 302\u2013324, 2009.\n[17] P. Kr\u00a8ahenb\u00a8uhl and V. Koltun. Geodesic object proposals. In\nECCV, 2014.\n[18] A. Krizhevsky, I. Sutskever, and G. E. Hinton.\nImagenet\nclassi\ufb01cation with deep convolutional neural networks. In\nNIPS, 2012.\n[19] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E.\nHoward, W. Hubbard, and L. D. Jackel. Backpropagation\napplied to handwritten zip code recognition. Neural compu-\ntation, 1989.\n[20] X. Liang, S. Liu, Y. Wei, L. Liu, L. Lin, and S. Yan. Compu-\ntational baby learning. arXiv:1411.2861, 2014.\n[21] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-\nmanan, P. Doll\u00b4ar, and C. L. Zitnick. Microsoft COCO: Com-\nmon objects in context. In ECCV. 2014.\n[22] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional\nnetworks for semantic segmentation. In CVPR, 2015.\n[23] M. Mostajabi, P. Yadollahpour, and G. Shakhnarovich.\nFeedforward semantic segmentation with zoom-out features.\narXiv preprint arXiv:1412.0774, 2014.\n[24] R. Mottaghi, X. Chen, X. Liu, N.-G. Cho, S.-W. Lee, S. Fi-\ndler, R. Urtasun, and A. Yuille. The role of context for object\ndetection and semantic segmentation in the wild. In CVPR.\n2014.\n[25] G. Papandreou, L.-C. Chen, K. Murphy, and A. L. Yuille.\nWeakly- and semi-supervised learning of a dcnn for seman-\ntic image segmentation. arXiv preprint arXiv:1502.02734,\n2015.\n[26] C. Rother, V. Kolmogorov, and A. Blake. Grabcut: Interac-\ntive foreground extraction using iterated graph cuts. ACM\nTransactions on Graphics, 2004.\n[27] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,\nS. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,\net al.\nImagenet large scale visual recognition challenge.\narXiv:1409.0575, 2014.\n[28] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,\nand Y. LeCun. Overfeat: Integrated recognition, localization\nand detection using convolutional networks. In ICLR, 2014.\n[29] K. Simonyan and A. Zisserman. Very deep convolutional\nnetworks for large-scale image recognition. In ICLR, 2015.\n[30] C. Szegedy,\nW. Liu,\nY. Jia,\nP. Sermanet,\nS. Reed,\nD. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.\nGoing deeper with convolutions. In CVPR, 2015.\n[31] J. R. Uijlings, K. E. van de Sande, T. Gevers, and A. W.\nSmeulders. Selective search for object recognition. IJCV,\n2013.\n[32] N. Wang, S. Li, A. Gupta, and D.-Y. Yeung.\nTrans-\nferring rich feature hierarchies for robust visual tracking.\narXiv:1501.04587, 2015.\n[33] W. Xia, C. Domokos, J. Dong, L.-F. Cheong, and S. Yan. Se-\nmantic segmentation without annotating segments. In ICCV,\n2013.\n[34] M. D. Zeiler and R. Fergus. Visualizing and understanding\nconvolutional neural networks. In ECCV, 2014.\n[35] S. Zheng, S. Jayasumana, B. Romera-Paredes, V. Vineet,\nZ. Su, D. Du, C. Huang, and P. Torr. Conditional random\n\ufb01elds as recurrent neural networks. arXiv:1502.03240, 2015.\n9\n",
        "sentence": " 2 MSRA-BoxSup (Dai et al., 2015) 75.",
        "context": "isons are made to evaluate the impact of mask/box/semi-\nsupervision.\nAs shown in Table 4, our box-supervised result that only\nuses VOC bounding boxes is 64.6. This compares favor-\nably with the WSSL [25] counterpart (60.4) under the same\n8\n6. Conclusion\nThe proposed BoxSup method can effectively harness\nbounding box annotations to train deep networks for se-\nmantic segmentation. Our BoxSup method that uses 133k\nbounding boxes and 10k masks achieves state-of-the-art re-\nexcellent results using strong mask-supervision from VOC\nand COCO data.\nTo show the potential of our BoxSup method in parallel\nwith improvements on the baseline, we use a simple test-\ntime augmentation to boost our results. Instead of comput-\n7"
    },
    {
        "title": "Solving the multiple-instance problem with axis-parallel rectangles",
        "author": [
            "Dietterich",
            "Thomas G",
            "Lathrop",
            "Richard H",
            "Lozano-perez",
            "Tomas"
        ],
        "venue": "Artificial Intelligence,",
        "citeRegEx": "Dietterich et al\\.,? \\Q1997\\E",
        "shortCiteRegEx": "Dietterich et al\\.",
        "year": 1997,
        "abstract": "",
        "full_text": "",
        "sentence": " Our approach builds on Kokkinos (2010a), where Multiple Instance Learning (MIL) (Dietterich et al., 1997) is used to accommodate orientation inconsistencies during the learning of an orientationsensitive boundary detector. Our approach builds on Kokkinos (2010a), where Multiple Instance Learning (MIL) (Dietterich et al., 1997) is used to accommodate orientation inconsistencies during the learning of an orientationsensitive boundary detector. That work was aimed at learning orientation-sensitive classifiers in the presence of orientation ambiguity in the annotations - we take a similar approach in order to deal with positional ambiguity in the annotations while learning a position-sensitive detector. Standard, \u2018single instance\u2019 learning assumes training samples come in feature-label pairs -or, as in HED above, every pixel is either a boundary or not. Instead, MIL takes as a training sample a set of features (\u2018bag\u2019) and its label. A bag should be labelled positive if at least one of its features is classified as positive, and negative otherwise. In particular, since human annotations come with some positional uncertainty, the standard evaluation protocol of Martin et al. (2004) allows for some slack in the predicted position of a pixel (a",
        "context": null
    },
    {
        "title": "Supervised Learning of Edges and Object Boundaries",
        "author": [
            "P. Dollar",
            "Z. Tu",
            "S. Belongie"
        ],
        "venue": "In Proc. CVPR,",
        "citeRegEx": "Dollar et al\\.,? \\Q2006\\E",
        "shortCiteRegEx": "Dollar et al\\.",
        "year": 2006,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " (Dollar et al., 2006; Arbelaez et al., 2011; Ren, 2008; Kokkinos, 2010a; Ren & Bo, 2012; Doll\u00e1r & Zitnick, 2015), we use machine learning to optimize the performance of our boundary detector.",
        "context": null
    },
    {
        "title": "Fast edge detection using structured forests",
        "author": [
            "Doll\u00e1r",
            "Piotr",
            "Zitnick",
            "C. Lawrence"
        ],
        "venue": "PAMI, 37(8):1558\u20131570,",
        "citeRegEx": "Doll\u00e1r et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "Doll\u00e1r et al\\.",
        "year": 2015,
        "abstract": "Edge detection is a critical component of many vision systems, including\nobject detectors and image segmentation algorithms. Patches of edges exhibit\nwell-known forms of local structure, such as straight lines or T-junctions. In\nthis paper we take advantage of the structure present in local image patches to\nlearn both an accurate and computationally efficient edge detector. We\nformulate the problem of predicting local edge masks in a structured learning\nframework applied to random decision forests. Our novel approach to learning\ndecision trees robustly maps the structured labels to a discrete space on which\nstandard information gain measures may be evaluated. The result is an approach\nthat obtains realtime performance that is orders of magnitude faster than many\ncompeting state-of-the-art approaches, while also achieving state-of-the-art\nedge detection results on the BSDS500 Segmentation dataset and NYU Depth\ndataset. Finally, we show the potential of our approach as a general purpose\nedge detector by showing our learned edge models generalize well across\ndatasets.",
        "full_text": "1\nFast Edge Detection Using Structured Forests\nPiotr Doll\u00b4ar and C. Lawrence Zitnick\nMicrosoft Research\n{pdollar,larryz}@microsoft.com\nAbstract\u2014Edge detection is a critical component of many vision systems, including object detectors and image segmentation\nalgorithms. Patches of edges exhibit well-known forms of local structure, such as straight lines or T-junctions. In this paper we take\nadvantage of the structure present in local image patches to learn both an accurate and computationally ef\ufb01cient edge detector. We\nformulate the problem of predicting local edge masks in a structured learning framework applied to random decision forests. Our\nnovel approach to learning decision trees robustly maps the structured labels to a discrete space on which standard information gain\nmeasures may be evaluated. The result is an approach that obtains realtime performance that is orders of magnitude faster than many\ncompeting state-of-the-art approaches, while also achieving state-of-the-art edge detection results on the BSDS500 Segmentation\ndataset and NYU Depth dataset. Finally, we show the potential of our approach as a general purpose edge detector by showing our\nlearned edge models generalize well across datasets.\n!\n1\nINTRODUCTION\nEdge detection has remained a fundamental task in computer\nvision since the early 1970\u2019s [18], [15], [43]. The detection\nof edges is a critical preprocessing step for a variety of\ntasks, including object recognition [47], [17], segmentation\n[33], [1], and active contours [26]. Traditional approaches to\nedge detection use a variety of methods for computing color\ngradients followed by non-maximal suppression [7], [19], [50].\nUnfortunately, many visually salient edges do not correspond\nto color gradients, such as texture edges [34] and illusory\ncontours [39]. State-of-the-art edge detectors [1], [41], [31],\n[21] use multiple features as input, including brightness, color,\ntexture and depth gradients computed over multiple scales.\nSince visually salient edges correspond to a variety of visual\nphenomena, \ufb01nding a uni\ufb01ed approach to edge detection is\ndif\ufb01cult. Motivated by this observation several recent papers\nhave explored the use of learning techniques for edge detection\n[13], [49], [31], [27]. These approaches take an image patch\nand compute the likelihood that the center pixel contains an\nedge. Optionally, the independent edge predictions may then\nbe combined using global reasoning [1], [41], [49], [2].\nEdges in a local patch are highly interdependent [31].\nThey often contain well-known patterns, such as straight lines,\nparallel lines, T-junctions or Y-junctions [40], [31]. Recently, a\nfamily of learning approaches called structured learning [36]\nhas been applied to problems exhibiting similar characteristics.\nFor instance, [29] applies structured learning to the problem\nof semantic image labeling for which local image labels are\nalso highly interdependent.\nIn this paper we propose a generalized structured learning\napproach that we apply to edge detection. This approach\nallows us to take advantage of the inherent structure in edge\npatches, while being surprisingly computationally ef\ufb01cient.\nWe can compute edge maps in realtime, which is orders of\nmagnitude faster than competing state-of-the-art approaches.\nA random forest framework is used to capture the structured\nFig. 1.\nEdge detection results using three versions of our\nStructured Edge (SE) detector demonstrating tradeoffs in accu-\nracy vs. runtime. We obtain realtime performance while simul-\ntaneously achieving state-of-the-art results. ODS numbers were\ncomputed on BSDS [1] on which the popular gPb detector [1]\nachieves a score of .73. The variants shown include SE, SE+SH,\nand SE+MS+SH, see \u00a74 for details.\ninformation [29]. We formulate the problem of edge detection\nas predicting local segmentation masks given input image\npatches. Our novel approach to learning decision trees uses\nstructured labels to determine the splitting function at each\nbranch in the tree. The structured labels are robustly mapped to\na discrete space on which standard information gain measures\nmay be evaluated. Each forest predicts a patch of edge pixel\nlabels that are aggregated across the image to compute our\n\ufb01nal edge map, see Figure 1. Since the aggregated edge maps\nmay be diffuse, the edge maps may optionally be sharpened\nusing local color and depth cues. We show state-of-the-art\nresults on both the BSDS500 [1] and the NYU Depth dataset\n[44]. We demonstrate the potential of our approach as a general\npurpose edge detector by showing the strong cross dataset\ngeneralization of our learned edge models.\narXiv:1406.5549v2  [cs.CV]  25 Nov 2014\n2\n1.1\nRelated work\nWe now discuss related work in edge detection and structured\nlearning. An earlier version of this work appeared in [14].\nEdge detection: Numerous papers have been written on\nedge detection over the past 50 years. Early work [18], [15],\n[7], [37], [19] focused on the detection of intensity or color\ngradients. The popular Canny detector [7] \ufb01nds the peak\ngradient orthogonal to edge direction. An evaluation of various\nlow-level edge detectors can be found in [4] and an overview in\n[50]. More recent work [34], [32], [28], [1], [48], [30] explores\nedge detection under more challenging conditions.\nSeveral techniques have explored the use of learning for\nedge detection [13], [49], [32], [41], [31], [27]. Doll\u00b4ar et al.\n[13] used a boosted classi\ufb01er to independently label each pixel\nusing its surrounding image patch as input. Zheng et al. [49]\ncombine low, mid, and high-level cues and show improved\nresults for object-speci\ufb01c edge detection. Ren and Bo [41] im-\nproved the result of [1] by computing gradients across learned\nsparse codes of patch gradients. While [41] achieved good\nresults, their approach further increased the high computational\ncost of [1]. Catanzaro et al. [8] improve the runtime of [1]\nusing parallel algorithms. Recently, Kivinen et al. [27] applied\ndeep networks to edge detection achieving competitive results.\nFinally, Lim et al. [31] propose an edge detection approach\nthat classi\ufb01es edge patches into sketch tokens using random\nforest classi\ufb01ers, that, like in our work, attempt to capture local\nedge structure. Sketch tokens bear resemblance to earlier work\non shapemes [40] but are computed directly from color image\npatches rather than from pre-computed edge maps. The result\nis an ef\ufb01cient approach for detecting edges that also shows\npromising results for object detection. In contrast to previous\nwork, we do not require the use of pre-de\ufb01ned classes of edge\npatches. This allows us to learn more subtle variations in edge\nstructure and leads to a more accurate and ef\ufb01cient algorithm.\nStructured learning: Structured learning addresses the\nproblem of learning a mapping where the input or output space\nmay be arbitrarily complex representing strings, sequences,\ngraphs, object pose, bounding boxes etc. [46], [45], [3]. We\nrefer readers to [36] for a comprehensive survey.\nOur structured random forests differ from these works in\nseveral respects. First, we assume that the output space is\nstructured but operate on a standard input space. Second, by\ndefault our model can only output examples observed during\ntraining, which implicitly assumes the existence of a set of\nrepresentative samples (this shortcoming can be ameliorated\nwith custom ensemble models). On the other hand, typical\nstructured predictors learn parameters to a scoring function and\nat inference perform an optimization to obtain predictions [46],\n[36]. This requires de\ufb01ning a scoring function and an ef\ufb01cient\n(possibly approximate) inference procedure. In contrast, in-\nference using our structured random forest is straightforward,\ngeneral and fast (same as for standard random forests).\nFinally, our work was inspired by recent work from\nKontschieder et al. [29] on learning random forests for struc-\ntured class labels for the speci\ufb01c case where the output labels\nrepresent a semantic image labeling for an image patch. The\nkey observation made by Kontschieder et al. is that given\na color image patch, the leaf node reached in a tree is\nindependent of the structured semantic labels, and any type of\noutput can be stored at each leaf. Building on this, we propose\na general learning framework for structured output forests that\ncan be used with a broad class of output spaces. We apply our\nframework to learning an accurate and fast edge detector.\n2\nRANDOM DECISION FORESTS\nWe begin with a review of random decision forests [6],\n[5], [20]. Throughout our presentation we adopt the notation\nand terminology of the extensive recent survey by Criminisi\net al. [11], somewhat simpli\ufb01ed for ease of presentation. The\nnotation in [11] is suf\ufb01ciently general to support our extension\nto random forests with structured outputs.\nA decision tree ft(x) classi\ufb01es a sample x \u2208X by\nrecursively branching left or right down the tree until a leaf\nnode is reached. Speci\ufb01cally, each node j in the tree is\nassociated with a binary split function:\nh(x, \u03b8j) \u2208{0, 1}\n(1)\nwith parameters \u03b8j. If h(x, \u03b8j) = 0 node j sends x left,\notherwise right, with the process terminating at a leaf node.\nThe output of the tree on an input x is the prediction stored\nat the leaf reached by x, which may be a target label y \u2208Y\nor a distribution over the labels Y.\nWhile the split function h(x, \u03b8) may be arbitrarily complex,\na common choice is a \u2018stump\u2019 where a single feature dimen-\nsion of x is compared to a threshold. Speci\ufb01cally, \u03b8 = (k, \u03c4)\nand h(x, \u03b8) = [x(k) < \u03c4], where [\u00b7] denotes the indicator\nfunction. Another popular choice is \u03b8 = (k1, k2, \u03c4) and\nh(x, \u03b8) = [x(k1) \u2212x(k2) < \u03c4]. Both are computationally\nef\ufb01cient and effective in practice [11].\nA decision forest is an ensemble of T independent trees ft.\nGiven a sample x, the predictions ft(x) from the set of trees\nare combined using an ensemble model into a single output.\nChoice of ensemble model is problem speci\ufb01c and depends on\nY, common choices include majority voting for classi\ufb01cation\nand averaging for regression, although more sophisticated\nensemble models may be employed [11].\nObserve that arbitrary information may be stored at the\nleaves of a decision tree. The leaf node reached by the tree\ndepends only on the input x, and while predictions of multiple\ntrees must be merged in some useful way (the ensemble\nmodel), any type of output y can be stored at each leaf. This\nallows use of complex output spaces Y, including structured\noutputs as observed by Kontschieder et al. [29].\nWhile prediction is straightforward, training random de-\ncision forests with structured Y is more challenging. We\nreview the standard learning procedure next and describe our\ngeneralization to learning with structured outputs in \u00a73.\n2.1\nTraining Decision Trees\nEach tree is trained independently in a recursive manner. For\na given node j and training set Sj \u2282X \u00d7 Y, the goal is to\n\ufb01nd parameters \u03b8j of the split function h(x, \u03b8j) that result in a\n\u2018good\u2019 split of the data. This requires de\ufb01ning an information\ngain criterion of the form:\nIj = I(Sj, SL\nj , SR\nj )\n(2)\n3\nwhere SL\nj = {(x, y) \u2208Sj|h(x, \u03b8j) = 0}, SR\nj = Sj\\SL\nj . Split-\nting parameters \u03b8j are chosen to maximize the information\ngain Ij; training then proceeds recursively on the left node\nwith data SL\nj and similarly for the right node. Training stops\nwhen a maximum depth is reached or if information gain or\ntraining set size fall below \ufb01xed thresholds.\nFor multiclass classi\ufb01cation (Y \u2282Z) the standard de\ufb01nition\nof information gain can be used:\nIj = H(Sj) \u2212\nX\nk\u2208{L,R}\n|Sk\nj |\n|Sj| H(Sk\nj )\n(3)\nwhere H(S) = \u2212P\ny py log(py) denotes the Shannon entropy\nand py is the fraction of elements in S with label y. Alterna-\ntively the Gini impurity H(S) = P\ny py(1\u2212py) has also been\nused in conjunction with Eqn. (3) [6].\nFor regression, entropy and information gain can be ex-\ntended to continuous variables [11]. Alternatively, a common\napproach for single-variate regression (Y = R) is to minimize\nthe variance of labels at the leaves [6]. If we write the variance\nas H(S) =\n1\n|S|\nP\ny(y \u2212\u00b5)2 where \u00b5 =\n1\n|S|\nP\ny y, then\nsubstituting H for entropy in Eqn. (3) leads to the standard\ncriterion for single-variate regression.\nCan we de\ufb01ne a more general information gain criterion for\nEqn. (2) that generalizes well for arbitrary output spaces Y?\nSurprisingly yes, given mild additional assumptions about Y.\nBefore going into detail in \u00a73, we discuss the key role that\nrandomness plays in the training of decision forests next.\n2.2\nRandomness and Optimality\nIndividual decision trees exhibit high variance and tend to\nover\ufb01t [24], [6], [5], [20]. Decision forests ameliorate this\nby training multiple de-correlated trees and combining their\noutput. A crucial component of the training procedure is\ntherefore to achieve a suf\ufb01cient diversity of trees.\nDiversity of trees can be obtained either by randomly\nsubsampling the data used to train each tree [6] or randomly\nsubsampling the features and splits used to train each node\n[24]. Injecting randomness at the level of nodes tends to\nproduce higher accuracy models [20] and has proven more\npopular [11]. Speci\ufb01cally, when optimizing Eqn. (2), only a\nsmall set of possible \u03b8j are sampled and tested when choosing\nthe optimal split. E.g., for stumps where \u03b8 = (k, \u03c4) and\nh(x, \u03b8) = [x(k) < \u03c4], [20] advocates sampling\n\u221a\nd features\nwhere X = Rd and a single threshold \u03c4 per feature.\nIn effect, accuracy of individual trees is sacri\ufb01ced in favor\nof a high diversity ensemble [20]. Leveraging similar intuition\nallows us to introduce an approximate information gain cri-\nterion for structured labels, described next, and leads to our\ngeneralized structured forest formulation.\n3\nSTRUCTURED RANDOM FORESTS\nIn this section we extend random decision forests to general\nstructured output spaces Y. Of particular interest for computer\nvision is the case where x \u2208X represents an image patch\nand y \u2208Y encodes the corresponding local image annotation\n(e.g., a segmentation mask or set of semantic image labels).\nHowever, we keep our derivation general.\nTraining random forests with structured labels poses two\nmain challenges. First, structured output spaces are often high\ndimensional and complex. Thus scoring numerous candidate\nsplits directly over structured labels may be prohibitively\nexpensive. Second, and more critically, information gain over\nstructured labels may not be well de\ufb01ned.\nWe use the observation that even approximate measures\nof information gain suf\ufb01ce to train effective random forest\nclassi\ufb01ers [20], [29]. \u2018Optimal\u2019 splits are not necessary or even\ndesired, see \u00a72.2. Our core idea is to map all the structured\nlabels y \u2208Y at a given node into a discrete set of labels c \u2208C,\nwhere C = {1, . . . , k}, such that similar structured labels y are\nassigned to the same discrete label c.\nGiven the discrete labels C, information gain calculated\ndirectly and ef\ufb01ciently over C can serve as a proxy for the\ninformation gain over the structured labels Y. As a result at\neach node we can leverage existing random forest training\nprocedures to learn structured random forests effectively.\nOur approach to calculating information gain relies on mea-\nsuring similarity over Y. However, for many structured output\nspaces, including those used for edge detection, computing\nsimilarity over Y is not well de\ufb01ned. Instead, we de\ufb01ne a\nmapping of Y to an intermediate space Z in which distance\nis easily measured. We therefore utilize a broadly applicable\ntwo-stage approach of \ufb01rst mapping Y \u2192Z followed by a\nstraightforward mapping of Z \u2192C.\nWe describe the proposed approach in more detail next and\nreturn to its application to edge detection in \u00a74.\n3.1\nIntermediate Mapping \u03a0\nOur key assumption is that for many structured output spaces,\nincluding for structured learning of edge detection, we can\nde\ufb01ne a mapping of the form:\n\u03a0 : Y \u2192Z\n(4)\nsuch that we can approximate dissimilarity of y \u2208Y by\ncomputing Euclidean distance in Z. For example, as we\ndescribe in detail in \u00a74, for edge detection the labels y \u2208Y\nare 16 \u00d7 16 segmentation masks and we de\ufb01ne z = \u03a0(y) to\nbe a long binary vector that encodes whether every pair of\npixels in y belong to the same or different segments. Distance\nis easily measured in the resulting space Z.\nZ may be high dimensional which presents a challenge\ncomputationally. For example, for edge detection there are\n\u000016\u00b716\n2\n\u0001\n= 32640 unique pixel pairs in a 16 \u00d7 16 segmentation\nmask, so computing z for every y would be expensive. How-\never, as only an approximate distance measure is necessary,\nthe dimensionality of Z can be reduced.\nIn order to reduce dimensionality, we sample m dimen-\nsions of Z, resulting in a reduced mapping \u03a0\u03c6 : Y \u2192Z\nparametrized by \u03c6. During training, a distinct mapping \u03a0\u03c6\nis randomly generated and applied to training labels Yj at\neach node j. This serves two purposes. First, \u03a0\u03c6 can be\nconsiderably faster to compute than \u03a0. Second, sampling Z\n4\nFig. 2. Illustration of the decision tree node splits: (a) Given a set of structured labels such as segments, a splitting function must\nbe determined. Intuitively a good split (b) groups similar segments, whereas a bad split (c) does not. In practice we cluster the\nstructured labels into two classes (d). Given the class labels, a standard splitting criterion, such as Gini impurity, may be used (e).\ninjects additional randomness into the learning process and\nhelps ensure a suf\ufb01cient diversity of trees, see \u00a72.2.\nFinally, Principal Component Analysis (PCA) [25] can be\nused to further reduce the dimensionality of Z. PCA denoises\nZ while approximately preserving Euclidean distance. In\npractice, we use \u03a0\u03c6 with m = 256 dimensions followed by a\nPCA projection to at most 5 dimensions.\n3.2\nInformation Gain Criterion\nGiven the mapping \u03a0\u03c6 : Y \u2192Z, a number of choices for the\ninformation gain criterion are possible. For discrete Z multi-\nvariate joint entropy could be computed directly. Kontschieder\net al. [29] proposed such an approach, but due to its complexity\nof O(|Z|m), were limited to using m \u22642. Our experiments\nindicate m \u226564 is necessary to accurately capture similarities\nbetween elements in Z. Alternatively, given continuous Z,\nvariance or a continuous formulation of entropy [11] can be\nused to de\ufb01ne information gain. In this work we propose a\nsimpler, extremely ef\ufb01cient approach.\nWe map a set of structured labels y \u2208Y into a discrete\nset of labels c \u2208C, where C = {1, . . . , k}, such that labels\nwith similar z are assigned to the same discrete label c, see\nFigure 2. The discrete labels may be binary (k = 2) or\nmulticlass (k > 2). This allows us to use standard information\ngain criteria based on Shannon entropy or Gini impurity as\nde\ufb01ned in Eqn. (3). Critically, discretization is performed\nindependently when training each node and depends on the\ndistribution of labels at a given node (contrast with [31]).\nWe consider two straightforward approaches to obtaining\nthe discrete label set C given Z. Our \ufb01rst approach is to\ncluster z into k clusters using K-means (projecting z onto 5\ndimensions prior to clustering). Alternatively, we can quantize\nz based on the top log2(k) PCA dimensions, assigning z a\ndiscrete label c according to the orthant (generalization of\nquadrant) into which z falls. Both approaches perform sim-\nilarly but the latter is slightly faster. We use PCA quantization\nto obtain k = 2 labels unless otherwise speci\ufb01ed.\n3.3\nEnsemble Model\nFinally, we de\ufb01ne how to combine a set of n labels y1 . . . yn\ninto a single prediction for both training (to set leaf labels)\nand testing (to merge predictions). As before, we sample an\nm dimensional mapping \u03a0\u03c6 and compute zi = \u03a0\u03c6(yi) for\neach i. We select the label yk whose zk is the medoid, i.e. the\nzk that minimizes the sum of distances to all other zi1. Note\nthat typically we only need to compute the medoid for small\nn (either for training a leaf node or merging the output of\nmultiple trees), hence using a coarse distance metric suf\ufb01ces.\nThe biggest limitation is that any prediction y \u2208Y must\nhave been observed during training; the ensemble model is\nunable to synthesize novel labels. Indeed, this is impossible\nwithout additional information about Y. In practice, domain\nspeci\ufb01c ensemble models are preferable. For example, in edge\ndetection we apply structured prediction to obtain edge maps\nfor each image patch independently and merge overlapping\npredictions by averaging (note that in this case structured\nprediction operates at the patch level and not the image level).\n4\nEDGE DETECTION\nWe now describe how to apply our structured forests to edge\ndetection. As input our method takes an image that may\ncontain multiple channels, such as an RGB or RGBD image.\nThe task is to label each pixel with a binary variable indicating\nwhether the pixel contains an edge or not. Similar to the task\nof semantic image labeling [29], the labels within a small\nimage patch are highly interdependent, providing a promising\ncandidate problem for our structured forest approach.\nWe assume we are given a set of segmented training images,\nin which the boundaries between the segments correspond to\ncontours [1], [44]. Given an image patch, its annotation can\nbe speci\ufb01ed either as a segmentation mask indicating segment\nmembership for each pixel (de\ufb01ned up to a permutation) or a\nbinary edge map. We use y \u2208Y = Zd\u00d7d to denote the former\nand y\u2032 \u2208Y\u2032 = {0, 1}d\u00d7d for the latter, where d indicates\npatch width. An edge map y\u2032 can always be trivially derived\nfrom segmentation mask y, but not vice versa. We utilize both\nrepresentations in our approach.\nNext, we describe how we compute the input features x,\nthe mapping functions \u03a0\u03c6 used to determine splits, and the\nensemble model used to combine multiple predictions.\nInput features: Our learning approach predicts a structured\n16\u00d716 segmentation mask from a larger 32\u00d732 image patch.\nWe begin by augmenting each image patch with multiple\nadditional channels of information, resulting in a feature vector\n1. The medoid zk minimizes P\nij(zkj \u2212zij)2. This is equivalent to\nmink\nP\nj(zkj \u2212\u00afzj)2 and can be computed ef\ufb01ciently in time O(nm).\n5\nx \u2208R32\u00d732\u00d7K where K is the number of channels. We use\nfeatures of two types: pixel lookups x(i, j, k) and pairwise\ndifferences x(i1, j1, k) \u2212x(i2, j2, k), see \u00a72.\nInspired by Lim et al. [31], we use a similar set of color\nand gradient channels (originally developed for fast pedestrian\ndetection [12]). We compute 3 color channels in CIE-LUV\ncolor space along with normalized gradient magnitude at 2\nscales (original and half resolution). Additionally, we split\neach gradient magnitude channel into 4 channels based on\norientation. The result is 3 color, 2 magnitude and 8 orientation\nchannels, for a total of 13 channels.\nWe blur the channels with a radius 2 triangle \ufb01lter and\ndownsample by a factor of 2, resulting in 32\u00b732\u00b713/4 = 3328\ncandidate features x. Motivated by [31], we also compute\npairwise difference features. We apply a large triangle blur to\neach channel (8 pixel radius), and downsample to a resolution\nof 5 \u00d7 5. Sampling all candidate pairs and computing their\ndifferences yields an additional\n\u00005\u00b75\n2\n\u0001\n= 300 candidate features\nper channel, resulting in 7228 total candidate features.\nMapping function: To train decision trees, we need to de\ufb01ne\na mapping \u03a0 : Y \u2192Z as described in \u00a73. Recall that our\nstructured labels y are 16\u00d716 segmentation masks. One option\nis to use \u03a0 : Y \u2192Y\u2032, where y\u2032 represents the binary edge map\ncorresponding to y. Unfortunately Euclidean distance over Y\u2032\nyields a brittle distance measure.\nWe therefore de\ufb01ne an alternate mapping \u03a0. Let y(j) for\n1 \u2264j \u2264256 denote the segment index of the jth pixel of y.\nIndividually a single value y(j) yields no information about\ny, since y is de\ufb01ned only up to a permutation. Instead we can\nsample a pair of locations j1 \u0338= j2 and check if they belong\nto the same segment, y(j1) = y(j2). This allows us to de\ufb01ne\nz = \u03a0(y) as a large binary vector that encodes [y(j1) =\ny(j2)] for every unique pair of indices j1 \u0338= j2. While Z has\n\u0000256\n2\n\u0001\ndimensions, in practice we only compute a subset of\nm dimensions as discussed in \u00a73.2. We found a setting of\nm = 256 and k = 2 gives good results, effectively capturing\nthe similarity of segmentation masks.\nEnsemble model: Random forests achieve robust results by\ncombining the output of multiple trees. While merging seg-\nmentation masks y \u2208Y for overlapping patches is dif\ufb01cult,\nmultiple overlapping edge maps y\u2032 \u2208Y\u2032 can be averaged to\nyield a soft edge response. Thus in addition to the learned\nmask y, we also store the corresponding edge map y\u2032 at each\nleaf node, thus allowing predictions to be combined quickly\nand simply through averaging during inference.\nEf\ufb01ciency: The surprising ef\ufb01ciency of our approach derives\nfrom the use of structured labels that predict information for an\nentire image neighborhood. This greatly reduces the number of\ntrees T that need to be evaluated. We compute our structured\noutput densely on the image with a stride of 2 pixels, thus with\n16 \u00d7 16 output patches, each pixel receives 162T/4 \u224864T\npredictions. In practice we use T = 4 and thus the score of\neach pixel in the output edge map is averaged over 256 votes.\nA critical assumption is that predictions are uncorrelated.\nSince both the inputs and outputs of each tree overlap, we\ntrain 2T total trees and evaluate an alternating set of T trees\nat each adjacent location. Use of such a \u2018checkerboard pattern\u2019\nimproves results somewhat, introducing larger separation be-\ntween the trees did not improve results further.\n4.1\nMultiscale Detection (SE+MS)\nWe now describe the \ufb01rst of two enhancements to our base\nalgorithm. Inspired by the work of Ren [38], we implement a\nmultiscale version of our edge detector. Given an input image\nI, we run our structured edge detector on the original, half,\nand double resolution version of I and average the result of\nthe three edge maps after resizing to the original image dimen-\nsions. Although somewhat inef\ufb01cient, the approach noticeably\nimproves edge quality. We refer to the multiscale version of\nour structured edge detector as SE+MS.\n4.2\nEdge Sharpening (SE+SH)\nWe observed that predicted edge maps from our structured\nedge detector are somewhat diffuse. For strong, isolated edges\nnon-maximal suppression can be used to effectively detect\nedge peaks. However, given \ufb01ne image structures edge re-\nsponses can \u2018bleed\u2019 together resulting in missed detections;\nlikewise, for weak edges that receive few votes no clear peak\nmay emerge. The underlying cause for the diffuse edge re-\nsponses is that the individually predicted edge maps are noisy\nand are not perfectly aligned to each other or the underlying\nimage data. Speci\ufb01cally, each overlapping prediction may be\nshifted by a few pixels from the true edge location.\nTo address this phenomenon, we introduce a new sharp-\nening procedure that aligns edge responses from overlapping\npredictions. Our core observation is that local image color and\ndepth values can be used to more precisely localize predicted\nresponses. Intuitively, given a predicted segmentation mask,\nthe mask can be morphed slightly so that it better matches the\nunderlying image patch. Aligning overlapping masks to the\nunderlying image data implicitly aligns the masks with each\nother, resulting in sharper, better localized edge responses.\nSharpening takes a predicted segmentation mask y \u2208Y\nand the corresponding image patch x \u2208X and produces a\nnew mask that better aligns to x. As before, let y(j) denote\nthe segment index of the jth pixel of mask y. First, for each\nsegment s, we compute its mean color \u00b5s = E[x(j)|y(j) = s]\nusing all pixels j in s. Next, we iteratively update the assigned\nsegment for each pixel by assigning it to the segment which\nminimizes \u2225\u00b5s \u2212x(j)\u22252. For each pixel we restrict the set\nof assignable segments to the segments that are immediately\nadjacent to it (4-connected neighborhood). Given the new\nsharpened segmentation masks, we compute and average their\ncorresponding edge maps as before. However, since the edge\nmaps are better aligned to the image data the resulting aggre-\ngated edge map is sharper.\nSharpening can be repeated multiple times prior to averag-\ning the corresponding edge maps. Experiments reveal that the\n\ufb01rst sharpening step produces the largest gains, and in practice\ntwo steps suf\ufb01ce. Taking advantage of the sparsity of edges,\nthe sharpening procedure can be implemented ef\ufb01ciently. For\ndetails we direct readers to source code. Note that sharpening\nis not guaranteed to improve results but we \ufb01nd it is quite\neffective in practice. We refer to the sharpened versions of\nour structured edge detector as SE+SH and SE+MS+SH.\n6\nground truth\ngPb+owt+ucm\nSketchTokens\nSCG\nSE\nSE+MS\nSE+SH\nSE+MS+SH\nFig. 3. Illustration of edge detection results on the BSDS500 dataset on \ufb01ve sample images. The \ufb01rst two rows show the original\nimage and ground truth. The next three rows contain results for gPb-owt-ucm [1], Sketch Tokens [31], and SCG [41]. The \ufb01nal four\nrows show our results for variants of SE. Use viewer zoom functionality to see \ufb01ne details.\n7\nground truth\nhigh precision\noptimal thresh\nhigh recall\nFig. 4. Visualizations of matches and errors of SE+MS+SH compared to BSDS ground truth edges. Edges are thickened to two\npixels for better visibility; the color coding is green=true positive, blue=false positive, red=false negative. Results are shown at three\nthresholds: high precision (T\u2248.26, P\u22480.88, R=.50), ODS threshold (T\u2248.14, P=R\u2248.75), and high recall (T\u2248.05, P=.50, R\u22480.93).\n5\nRESULTS\nIn this section we analyze the performance of our structured\nedge (SE) detector in detail. First we analyze the in\ufb02uence\nof parameters in \u00a75.1 and test SE variants in \u00a75.2. Next, we\ncompare results on the BSDS [1] and NYUD [44] datasets\nto the state-of-the-art in \u00a75.3 and \u00a75.4, respectively, reporting\nboth accuracy and runtime. We conclude by demonstrating the\ncross dataset generalization of our approach in \u00a75.5.\nThe majority of our experiments are performed on the\nBerkeley Segmentation Dataset and Benchmark (BSDS500)\n[35], [1]. The dataset contains 200 training, 100 validation,\nand 200 testing images. Each image has hand labeled ground\ntruth contours. Edge detection accuracy is evaluated using\nthree standard measures: \ufb01xed contour threshold (ODS), per-\nimage best threshold (OIS), and average precision (AP) [1].\nTo evaluate accuracy in the high recall regime, we additionally\nintroduce a new measure, recall at 50% precision (R50), in\n\u00a75.2. Prior to evaluation, we apply a standard non-maximal\nsuppression technique to our edge maps to obtain thinned\nedges [7]. Example detections on BSDS are shown in Figure 3\nand visualizations of edge accuracy are shown in Figure 4.\n5.1\nParameter Sweeps\nWe set all parameters with the help of the BSDS validation set\nwhich is fully independent of the test set. Parameters include:\nstructured forest splitting parameters (e.g., m and k), feature\nparameters (e.g., image and channel blurring), and model\nand tree parameters (e.g. number of trees and data quantity).\nTraining takes \u223c20 minute per tree using one million patches\nand is parallelized over trees. Evaluation of trees is parallelized\nas well, we use a quad-core machine for all reported runtimes.\nIn Figures 5-7 we explore the effect of choices of splitting,\nmodel and feature parameters. For each experiment we train\non the 200 image training set and measure edge detection\naccuracy on the 100 image validation set (using the standard\nODS performance metric). All results are averaged over 5\ntrials. First, we set all parameters to their default values\nindicated by orange markers in the plots. Then, keeping all but\none parameter \ufb01xed, we explore the effect on edge detection\naccuracy as a single parameter is varied.\nSince we explore a large number of parameters settings, we\nperform our experiments using a slightly reduced accuracy\nmodel that is faster to train. Speci\ufb01cally we train using fewer\npatches (2 \u00b7 105 versus 106) and utilize sharpening (SH) but\nnot multiscale detection (MS). Also, the validation set is\nmore challenging than the test set and we evaluate using 25\nthresholds instead of 99, further reducing accuracy (.71 ODS).\nFinally, we note that sweep details have changed slightly from\nthe our previous work [14]; most notably, the sweeps now\nutilize sharpening but not multiscale detection.\n8\n1\n4\n16\n64\n256\n65\n70\nODS \u00d7 100\n2\n4\n8\n16\n32\n66\n68\n70\n72\nODS \u00d7 100\n(a) m (size of Z)\n(b) k (size of C)\npca\nkmeans\n66\n68\n70\n72\nODS \u00d7 100\ngini\nentropy\ntwoing\n66\n68\n70\n72\nODS \u00d7 100\n(c) discretization type\n(d) information gain\nFig. 5.\nSplitting parameter sweeps. See text for details.\n1\n3\n5\n7\n66\n68\n70\n72\nODS \u00d7 100\n0\n2\n4\n6\n8\n66\n68\n70\n72\nODS \u00d7 100\n(a) # grid cells\n(b) # gradient orients\n0\n1\n2\n4\n8\n66\n68\n70\n72\nODS \u00d7 100\n1\n2\n4\n66\n68\n70\n72\nODS \u00d7 100\n(c) normalization radius\n(d) channel downsample\n0\n1\n2\n4\n8\n16\n66\n68\n70\n72\nODS \u00d7 100\n0\n1\n2\n4\n8\n16\n66\n68\n70\n72\nODS \u00d7 100\n(e) channel blur\n(f) self-similarity blur\nFig. 6.\nFeature parameter sweeps. See text for details.\nSplitting Parameters: In Figure 5 we explore how best\nto measure information gain over structured labels. Recall we\nutilize a two-stage approach of mapping Y \u2192Z followed by\nZ \u2192C. Plots (a) and (b) demonstrate that m = |Z| should\nbe large and k = |C| small. Results are robust to both the\ndiscretization method and the discrete measure of information\ngain as shown in plots (c) and (d).\nFeature Parameters: Figure 6 shows how varying the\nchannel features affects accuracy. We refer readers to \u00a74 and\nsource code for details, here we only note that performance is\nrelatively insensitive to a broad range of parameter settings.\nModel Parameters: In Figure 7 we plot the in\ufb02uence of\nparameters governing the model and training data. (a) and (b)\nshow the effect of image and label patch sizes on accuracy,\n32 \u00d7 32 image patches and 16 \u00d7 16 label patches are best.\n(c) and (d) show that increasing the number of patches and\ntraining images improves accuracy. (e) shows that about half\nthe sampled patches should be \u2018positive\u2019 (have more than\none ground truth segment) and (f) shows that training each\ntree with a fraction of total features has negligible impact on\naccuracy (but results in proportionally lower memory usage).\nIn (g)-(i) we see that many, deep, un-pruned trees give best\nperformance (nevertheless, we prune trees so every node has\nat least 8 training samples to decrease model size). Finally (j)\nshows that two sharpening steps give best results. Impact of\nsharpening is explored in more detail next in \u00a75.2.\n8\n16\n24\n32\n48\n64\n66\n68\n70\n72\nODS \u00d7 100\n4\n8\n12\n16\n24\n32\n66\n68\n70\n72\nODS \u00d7 100\n(a) patch size for x\n(b) patch size for y\n1\n2\n5\n10 20\n50 100 200\n66\n68\n70\n72\nODS \u00d7 100\n10\n20\n50\n100\n200\n66\n68\n70\n72\nODS \u00d7 100\n(c) # train patches \u00d7104\n(d) # train images\n.2\n.3\n.4\n.5\n.6\n.7\n.8\n66\n68\n70\n72\nODS \u00d7 100\n1/4\n1/8\n1/16\n1/32\n1/64\n66\n68\n70\n72\nODS \u00d7 100\n(e) fraction \u2018positives\u2019\n(f) fraction features\n1\n2\n4\n8\n16\n66\n68\n70\n72\nODS \u00d7 100\n4\n8\n16\n32\n64\n66\n68\n70\n72\nODS \u00d7 100\n(g) # decision trees\n(h) max tree depth\n2\n4\n6\n8\n10\n12\n14\n16\n66\n68\n70\n72\nODS \u00d7 100\n0\n1\n2\n3\n4\n66\n68\n70\n72\nODS \u00d7 100\n(i) min samples per node\n(j) # sharpening steps\nFig. 7.\nModel parameter sweeps. See text for details.\n5.2\nStructured Edge Variants\nGiven our trained detector, sharpening (SH) and multiscale\ndetection (MS) can be used to enhance results. In this section\nwe analyze the performance of the four resulting combinations.\nWe emphasize that the same trained model can be used with\nsharpening and multiscale detection enabled at runtime.\nIn Figure 8 we plot precision/recall curves for the four vari-\nants of our approach: SE, SE+MS, SE+SH, and SE+MS+SH.\nSummary statistics are reported in the bottom rows of Ta-\nble 1. SE has an ODS score of .73 and SE+MS and SE+SH\nboth achieve an ODS of .74. SE+MS+SH, which combines\nmultiscale detection and sharpening2, yields an ODS of .75.\nIn all cases OIS, which is measured using a separate optimal\nthreshold per image, is about 2 points higher than ODS.\nAs these results indicate, both sharpening and multiscale\ndetection improve accuracy. To further analyze these enhance-\nments, we introduce a new evaluation metric: recall at 50%\nprecision (R50), which measures accuracy in the high recall\nregime. SE achieves R50 of .90 and SE+MS does not improve\nthis. In contrast, SE+SH boosts R50 considerably to .93. This\nincrease in recall for the SE variants can clearly be seen\nin Figure 8. The MS variants, on the other hand, improve\nprecision in the low-recall regime. Overall, both SH and MS\nimprove AP3, with SE+SH+MS achieving an AP of .80.\n2. We trained the top-performing SE+MS+SH model with 4 \u00b7 106 patches\ncompared to 106 for the other SE models, further increasing ODS by \u223c.004.\n3. We discovered an error in the BSDS evaluation which overestimates AP\nby 1%. For consistency with past results, however, we used the code as is.\n9\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nRecall\nPrecision\n \n \n[F=.80] Human\n[F=.75] SE+MS+SH\n[F=.74] SE+MS\n[F=.74] SE+SH\n[F=.73] SE\nFig. 8. Results of structured edges (SE) with sharpening (+SH)\nand multiscale detection (+MS). SH increases recall while MS\nincreases precision; their combination gives best results.\nODS\nOIS\nAP\nR50\nFPS\nHuman\n.80\n.80\n-\n-\n-\nCanny\n.60\n.63\n.58\n.75\n15\nFelz-Hutt [16]\n.61\n.64\n.56\n.78\n10\nNormalized Cuts [10]\n.64\n.68\n.45\n.81\n-\nMean Shift [9]\n.64\n.68\n.56\n.79\n-\nHidayat-Green [23]\n.62\u2020\n-\n-\n-\n20\nBEL [13]\n.66\u2020\n-\n-\n-\n1/10\nGb [30]\n.69\n.72\n.72\n.85\n1/6\ngPb + GPU [8]\n.70\u2020\n-\n-\n-\n1/2\u2021\nISCRA [42]\n.72\n.75\n.46\n.89\n1/30\u2021\ngPb-owt-ucm [1]\n.73\n.76\n.73\n.89\n1/240\nSketch Tokens [31]\n.73\n.75\n.78\n.91\n1\nDeepNet [27]\n.74\n.76\n.76\n-\n1/5\u2021\nSCG [41]\n.74\n.76\n.77\n.91\n1/280\nSE+multi-ucm [2]\n.75\n.78\n.76\n.91\n1/15\nSE\n.73\n.75\n.77\n.90\n30\nSE+SH\n.74\n.76\n.79\n.93\n12.5\nSE+MS\n.74\n.76\n.78\n.90\n6\nSE+MS+SH\n.75\n.77\n.80\n.93\n2.5\nTABLE 1\nResults on BSDS500. \u2020BSDS300 results. \u2021Utilizes the GPU.\nThe runtime of the four variants is reported in the last\ncolumn of Table 1. SE runs at a frame rate of 30hz, enabling\nreal time processing. Both SH and MS slow the detector, with\nMS incurring a higher cost. Nevertheless, SE+SH runs at over\n12hz while achieving excellent accuracy. Indeed, in the high\nrecall regime, which is necessary for many common scenarios,\nSE+SH achieves top results. Given its speed and high recall,\nwe expect SE+SH to be the default variant used in practice.\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nRecall\nPrecision\n \n \n[F=.80] Human\n[F=.75] SE+multi\u2212ucm\n[F=.75] SE (ours)\n[F=.74] SCG\n[F=.73] Sketch Tokens\n[F=.73] gPb\u2212owt\u2212ucm\n[F=.72] ISCRA\n[F=.69] Gb\n[F=.64] Mean Shift\n[F=.64] Normalized Cuts\n[F=.61] Felz\u2212Hutt\n[F=.60] Canny\nFig. 9.\nResults on BSDS500. Structured edges (SE) and SE\ncoupled with hierarchical multiscale segmentation (SE+multi-\nucm) [2] achieve top results. For the SE result we report the\nSE+MS+SH variant. See Table 1 for additional details including\nmethod citations and runtimes. SE is orders of magnitude faster\nthan nearly all edge detectors with comparable accuracy.\n5.3\nBSDS500 Results\nWe compare our edge detector against competing methods,\nreporting both accuracy and runtime. Precision/recall curves\nare shown in Figure 9 and summary statistics are in Table 1.\nOur full approach, SE+MS+SH, outperforms all state-of-\nthe-art approaches [41], [27], [31], [1], [42]. We improve\nODS/OIS by 1 point over competing methods and AP/R50 by\n2 points. Our edge detector is particularly effective in the high\nrecall regime. The only method with comparable accuracy is\nSE+multi-ucm [2] which couples our SE+MS detector with a\nhierarchical multiscale segmentation approach.\nSE+MS+SH is orders of magnitude faster than nearly all\nedge detectors with comparable accuracy, see last column of\nTable 1. All runtimes are reported on 480 \u00d7 320 images. Our\napproach scales linearly with image size and and is parallelized\nacross four cores. While many competing methods are likewise\nlinear, they have a much higher cost per pixel. The single scale\nvariant of our detector, SE+SH, further improves speed by 5\u00d7\nwith only minor loss in accuracy and no loss in recall. Finally,\nSE runs at 30hz while still achieving competitive accuracy.\nIn comparison to other learning-based approaches to edge\ndetection, we considerably outperform BEL [13] which com-\nputes edges independently at each pixel given its surrounding\nimage patch. We also outperform Sketch Tokens [31] in both\naccuracy and runtime performance. This may be the result\nof Sketch Tokens using a \ufb01xed set of classes for selecting\nsplit criterion at each node, whereas our structured forests can\ncapture \ufb01ner patch edge structure. Moreover, our structured\noutput leads to signi\ufb01cantly smoother edge maps, see Figure 3.\nFinally, Kivinen et al. [27] recently trained deep networks for\nedge detection; unfortunately, we were unable to obtain results\nfrom the authors to perform detailed comparisons.\n10\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nRecall\nPrecision\n \n \n[F=.69] SE\u2212RGBD\n[F=.65] SE\u2212RGB\n[F=.64] SE\u2212BSDS\n[F=.64] SE\u2212D\nFig. 10. Precision/recall curves on NYUD using different image\nmodalities. SE-BSDS is the RGB model trained on the BSDS\ndataset. See Table 2 and 3 and text for details.\nODS\nOIS\nAP\nR50\nFPS\ngPb-owt-ucm [1]\n.63\n.66\n.56\n.79\n1/360\nSilberman [44]\n.65\n.66\n.29\n.84\n1/360+\ngPb+NG [21]\n.68\n.71\n.63\n.86\n1/375\nSE+NG+ [22]\n.71\n.72\n.74\n.90\n1/15\nSE-D\n.64\n.65\n.66\n.80\n7.5\nSE-RGB\n.65\n.67\n.65\n.84\n7.5\nSE-RGBD\n.69\n.71\n.72\n.89\n5\nTABLE 2\nResults on the NYUD dataset [44].\n5.4\nNYUD Results\nThe NYU Depth (NYUD) dataset [44] is composed of 1449\npairs of RGB and depth images with corresponding semantic\nsegmentations. The dataset was adopted independently for\nedge detection by Ren and Bo [41] and by Gupta et al. [21]. In\npractice, the two dataset variants have signi\ufb01cant differences.\nSpeci\ufb01cally, [41] proposed a different train/test split and used\nhalf resolution images. Instead, [21] use the train/test split\noriginally proposed by Silberman et al. [44] and full resolution\nimages. In our previous work we used the version from [41] in\nour experiments; in the present work we switch to the version\nfrom Gupta et al. [21] as more methods have been evaluated\non this variant and it utilizes the full resolution images.\nGupta et al. [21] split the NYUD dataset into 381 training,\n414 validation, and 654 testing images and generated ground\ntruth edges from the semantic segmentations provided by [44].\nThe original 640\u00d7480 images are cropped to discard boundary\nregions with missing ground truth. Finally the maximum slop\nallowed for correct matches of edges to ground truth during\nevaluation is increased from .0075 of the image diagonal to\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nRecall\nPrecision\n \n \n[F=.71] SE+NG+\n[F=.69] SE (ours)\n[F=.68] gPb+NG\n[F=.65] Silberman\n[F=.63] gPb\u2212owt\u2212ucm\nFig. 11.\nResults on NYUD. Structured edges (SE) and SE\ncoupled with depth normal gradient (SE+NG+) [21], [22] achieve\ntop results. For the SE result we report the SE+SH variant. See\nTable 2 for additional details including citations and runtimes.\n.011. This is necessary to compensate for the relatively inexact\nlocalization of the ground truth.\nExample SE results are shown in Figure 12. We treat\nthe depth channel in the same manner as the other color\nchannels. Speci\ufb01cally, we recompute the gradient channels\nover the depth channel (with identical parameters) resulting\nin 11 additional channels. Precision/recall curves for SE+SH\nwith different image modalities are shown in Figure 10. Use of\ndepth information only (SE-D) gives good precision as strong\ndepth discontinuities nearly always correspond to edges. Use\nof intensity information only (SE-RGB) gives better recall as\nnearly all edges have intensity discontinuities but not all edges\nhave depth discontinuities. As expected, simultaneous use of\nintensity and depth (SE-RGBD) substantially improves results.\nSummary statistics are given in Table 2. Runtime is slower\nthan on BSDS as NYUD images are higher resolution and\nfeatures must be computed over both intensity and depth. For\nthese results we utilized the SE+SH variant which slightly\noutperformed SE+MS+SH on this dataset.\nIn Table 2 and Figure 11 we compare our approach, SE+SH,\nto a number of state-of-the-art approaches, including gPb-\nowt-ucm (color only), Silberman et al.\u2019s RGBD segmentation\nalgorithm [44], and detectors from Gupta et al. [21], [22]\nthat explicitly estimate depth normal gradient (NG). While\nour approach naively utilizes depth (treating the depth image\nidentically to the intensity image), we outperform nearly all\ncompeting methods, including gPb+NG [21]. Gupta et al. [22]\nobtain top results by coupling our structured edges detector\nwith depth normal gradients and additional cues (SE+NG+).\nFinally, for a comparison of SE to Ren and Bo\u2019s SCG [41] on\nthe alternate version of NYUD we refer readers to our previous\nwork [14] (in the alternate setup SE likewise outperforms SCG\nacross all modalities while retaining its speed advantage).\n11\ndepth\nground truth\nSE-D\nSE-RGB\nSE-RGBD\nFig. 12. Edge detection results on the NYUD dataset. Edges from depth features (SE-D) have good precision, edges from intensity\nfeatures (SE-RGB) give better recall, and simultaneous use of intensity and depth (SE-RGBD) gives best results. For details about\nvisualizations of matches and errors see Figure 4; all visualizations were generated using each detector\u2019s ODS threshold.\n12\nODS\nOIS\nAP\nR50\nFPS\nNYUD / NYUD\n.65\n.67\n.65\n.84\n7.5\nBSDS / NYUD\n.64\n.66\n.63\n.83\n7.5\nBSDS / BSDS\n.75\n.77\n.80\n.93\n2.5\nNYUD / BSDS\n.73\n.74\n.77\n.91\n2.5\nTABLE 3\nCross-dataset generalization for Structured Edges.\nTRAIN/TEST indicates the training/testing dataset used.\n5.5\nCross dataset generalization\nTo study the ability of our approach to generalize across\ndatasets we ran a \ufb01nal set of experiments. In Table 3 we show\nresults on NYUD using structured forests trained on BSDS\nand also results on BSDS using structured forests trained\non NYUD. For these experiments we use intensity images\nonly. Note that images in the BSDS and NYUD datasets are\nqualitatively quite different, see Figure 3 and 12, respectively.\nTable 3, top, compares results on NYUD of the NYUD\nand BSDS trained models. Across all performance measure,\nscores degrade by about 1 point when using the BSDS dataset\nfor training. Precision/recall curves on NYUD for the NYUD\nmodel (SE-RGB) and the BSDS model (SE-BSDS) are shown\nin Figure 10. The resulting curves align closely. The minor\nperformance change is surprising given the different statistics\nof the datasets. Results on BSDS of the BSDS and NYUD\nmodels, shown in Table 3, bottom, are likewise similar.\nThese experiments provide strong evidence that our ap-\nproach could serve as a general purpose edge detector without\nthe necessity of retraining. We expect this to be a critical aspect\nof our detector allowing for its widespread applicability.\n6\nDISCUSSION\nOur approach is capable of realtime frame rates while achiev-\ning state-of-the-art accuracy. This may enable new applications\nthat require high-quality edge detection and ef\ufb01ciency. For\ninstance, our approach may be well suited for video segmen-\ntation or for time sensitive object recognition tasks such as\npedestrian detection.\nOur approach to learning structured decision trees may be\napplied to a variety of problems. The fast and direct inference\nprocedure is ideal for applications requiring computational\nef\ufb01ciency. Given that many vision applications contain struc-\ntured data, there is signi\ufb01cant potential for structured forests\nin other applications.\nIn conclusion, we propose a structured learning approach\nto edge detection. We describe a general purpose method for\nlearning structured random decision forest that robustly uses\nstructured labels to select splits in the trees. We demonstrate\nstate-of-the-art accuracies on two edge detection datasets,\nwhile being orders of magnitude faster than most competing\nstate-of-the-art methods.\nSource code is available online.\nREFERENCES\n[1]\nP. Arbelaez, M. Maire, C. Fowlkes, and J. Malik. Contour detection and\nhierarchical image segmentation. PAMI, 33, 2011. 1, 2, 4, 6, 7, 9, 10\n[2]\nP. Arbel\u00b4aez, J. Pont-Tuset, J. T. Barron, F. Marques, and J. Malik.\nMultiscale combinatorial grouping. In CVPR, 2014. 1, 9\n[3]\nM. Blaschko and C. Lampert.\nLearning to localize objects with\nstructured output regression. In ECCV, 2008. 2\n[4]\nK. Bowyer, C. Kranenburg, and S. Dougherty. Edge detector evaluation\nusing empirical roc curves. Computer Vision and Image Understanding,\n84(1):77\u2013103, 2001. 2\n[5]\nL. Breiman. Random forests. Machine Learning, 45(1):5\u201332, Oct. 2001.\n2, 3\n[6]\nL. Breiman, J. Friedman, C. J. Stone, and R. A. Olshen. Classif\ufb01cation\nand Regression Trees. Chapman and Hall/CRC, 1984. 2, 3\n[7]\nJ. Canny. A computational approach to edge detection. PAMI, 8(6):679\u2013\n698, November 1986. 1, 2, 7\n[8]\nB. Catanzaro, B.-Y. Su, N. Sundaram, Y. Lee, M. Murphy, and\nK. Keutzer. Ef\ufb01cient, high-quality image contour detection. In ICCV,\n2009. 2, 9\n[9]\nD. Comaniciu and P. Meer. Mean shift: A robust approach toward feature\nspace analysis. PAMI, 24:603\u2013619, 2002. 9\n[10] T. Cour, F. Benezit, and J. Shi. Spectral segmentation with multiscale\ngraph decomposition. In CVPR, 2005. 9\n[11] A. Criminisi, J. Shotton, and E. Konukoglu. Decision forests: A uni\ufb01ed\nframework for classi\ufb01cation, regression, density estimation, manifold\nlearning and semi-supervised learning.\nFoundations and Trends in\nComputer Graphics and Vision, 7(2-3), February 2012. 2, 3, 4\n[12] P. Doll\u00b4ar, S. Belongie, and P. Perona. The fastest pedestrian detector in\nthe west. In BMVC, 2010. 5\n[13] P. Doll\u00b4ar, Z. Tu, and S. Belongie. Supervised learning of edges and\nobject boundaries. In CVPR, 2006. 1, 2, 9\n[14] P. Doll\u00b4ar and C. L. Zitnick. Structured forests for fast edge detection.\nIn ICCV, 2013. 2, 7, 10\n[15] R. O. Duda, P. E. Hart, et al. Pattern classi\ufb01cation and scene analysis,\nvolume 3. Wiley New York, 1973. 1, 2\n[16] P. F. Felzenszwalb and D. P. Huttenlocher. Ef\ufb01cient graph-based image\nsegmentation. IJCV, 59(2):167\u2013181, 2004. 9\n[17] V. Ferrari, L. Fevrier, F. Jurie, and C. Schmid.\nGroups of adjacent\ncontour segments for object detection. PAMI, 30(1):36\u201351, 2008. 1\n[18] J. R. Fram and E. S. Deutsch. On the quantitative evaluation of edge\ndetection schemes and their comparison with human performance. IEEE\nTOC, 100(6), 1975. 1, 2\n[19] W. T. Freeman and E. H. Adelson. The design and use of steerable\n\ufb01lters. PAMI, 13:891\u2013906, 1991. 1, 2\n[20] P. Geurts, D. Ernst, and L. Wehenkel.\nExtremely randomized trees.\nMachine Learn, 63(1):3\u201342, Apr. 2006. 2, 3\n[21] S. Gupta, P. Arbelaez, and J. Malik.\nPerceptual organization and\nrecognition of indoor scenes from RGB-D images.\nIn CVPR, 2013.\n1, 10\n[22] S. Gupta, R. Girshick, P. Arbelaez, and J. Malik. Learning rich features\nfrom RGB-D images for object detection and segmentation. In ECCV,\n2014. 10\n[23] R. Hidayat and R. Green. Real-time texture boundary detection from\nridges in the standard deviation space. In BMVC, 2009. 9\n[24] T. K. Ho. The random subspace method for constructing decision forests.\nPAMI, 20(8):832\u2013844, 1998. 3\n[25] I. T. Joliffe. Principal Component Analysis. Springer-Verlag, 1986. 4\n[26] M. Kass, A. Witkin, and D. Terzopoulos. Snakes: Active contour models.\nIJCV, 1(4):321\u2013331, 1988. 1\n[27] J. J. Kivinen, C. K. Williams, and N. Heess. Visual boundary prediction:\nA deep neural prediction network and quality dissection. In AISTATS,\n2014. 1, 2, 9\n[28] I. Kokkinos. Boundary detection using f-measure-, \ufb01lter- and feature-\n(F3) boost. In ECCV, 2010. 2\n[29] P. Kontschieder, S. Bulo, H. Bischof, and M. Pelillo. Structured class-\nlabels in random forests for semantic image labelling. In ICCV, 2011.\n1, 2, 3, 4\n[30] M. Leordeanu, R. Sukthankar, and C. Sminchisescu.\nGeneralized\nboundaries from multiple image interpretations. PAMI, 2014. 2, 9\n[31] J. Lim, C. L. Zitnick, and P. Doll\u00b4ar. Sketch tokens: A learned mid-level\nrepresentation for contour and object detection. In CVPR, 2013. 1, 2,\n4, 5, 6, 9\n[32] J. Mairal, M. Leordeanu, F. Bach, M. Hebert, and J. Ponce. Discrimi-\nnative sparse image models for class-speci\ufb01c edge detection and image\ninterpretation. In ECCV, 2008. 2\n[33] J. Malik, S. Belongie, T. Leung, and J. Shi. Contour and texture analysis\nfor image segmentation. IJCV, 43, 2001. 1\n[34] D. Martin, C. Fowlkes, and J. Malik. Learning to detect natural image\nboundaries using local brightness, color, and texture cues.\nPAMI,\n26(5):530\u2013549, 2004. 1, 2\n13\n[35] D. Martin, C. Fowlkes, D. Tal, and J. Malik.\nA database of human\nsegmented natural images and its application to evaluating segmentation\nalgorithms and measuring ecological statistics. In ICCV, 2001. 7\n[36] S. Nowozin and C. H. Lampert. Structured learning and prediction in\ncomputer vision. Foundations and Trends in Computer Graphics and\nVision, 6:185\u2013365, 2011. 1, 2\n[37] P. Perona and J. Malik. Scale-space and edge detection using anisotropic\ndiffusion. PAMI, 12(7):629\u2013639, 1990. 2\n[38] X. Ren. Multi-scale improves boundary detection in natural images. In\nICCV, 2008. 5\n[39] X. Ren, C. Fowlkes, and J. Malik. Scale-invariant contour completion\nusing cond. random \ufb01elds. In ICCV, 2005. 1\n[40] X. Ren, C. Fowlkes, and J. Malik. Figure/ground assignment in natural\nimages. In ECCV, 2006. 1, 2\n[41] X. Ren and B. Liefeng. Discriminatively trained sparse code gradients\nfor contour detection. In NIPS, 2012. 1, 2, 6, 9, 10\n[42] Z. Ren and G. Shakhnarovich. Image segmentation by cascaded region\nagglomeration. In CVPR, 2013. 9\n[43] G. S. Robinson. Color edge detection. Optical Engineering, 16(5), 1977.\n1\n[44] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus. Indoor segmentation\nand support inference from rgbd images. In ECCV, 2012. 1, 4, 7, 10\n[45] B. Taskar, V. Chatalbashev, D. Koller, and C. Guestrin.\nLearning\nstructured prediction models: a large margin approach. In ICML, 2005.\n2\n[46] I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun. Learning for\ninterdependent and structured output spaces. In ICML, 2004. 2\n[47] S. Ullman and R. Basri. Recognition by linear combinations of models.\nPAMI, 13(10), 1991. 1\n[48] N. Widynski and M. Mignotte. A particle \ufb01lter framework for contour\ndetection. In ECCV, 2012. 2\n[49] S. Zheng, Z. Tu, and A. Yuille. Detecting object boundaries using low-,\nmid-, and high-level information. In CVPR, 2007. 1, 2\n[50] D. Ziou, S. Tabbone, et al.\nEdge detection techniques-an overview.\nPattern Recognition and Image Analysis, 8:537\u2013559, 1998. 1, 2\n",
        "sentence": "",
        "context": "Pattern Recognition and Image Analysis, 8:537\u2013559, 1998. 1, 2\ncomputer vision. Foundations and Trends in Computer Graphics and\nVision, 6:185\u2013365, 2011. 1, 2\n[37] P. Perona and J. Malik. Scale-space and edge detection using anisotropic\ndiffusion. PAMI, 12(7):629\u2013639, 1990. 2\nFoundations and Trends in\nComputer Graphics and Vision, 7(2-3), February 2012. 2, 3, 4\n[12] P. Doll\u00b4ar, S. Belongie, and P. Perona. The fastest pedestrian detector in\nthe west. In BMVC, 2010. 5"
    },
    {
        "title": "Predicting depth, surface normals and semantic labels with a common multiscale convolutional architecture",
        "author": [
            "Eigen",
            "David",
            "Fergus",
            "Rob"
        ],
        "venue": null,
        "citeRegEx": "Eigen et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Eigen et al\\.",
        "year": 2014,
        "abstract": "",
        "full_text": "",
        "sentence": " , 2015) or normal estimation (Sermanet et al., 2014; Eigen et al., 2014). A convenient component of such works is that the inherently convolutional nature of DCNNS allows for simple and efficient \u2018fully convolutional\u2019 implementations (Sermanet et al., 2014; Eigen et al., 2014; Oquab et al., 2015; Long et al., 2014; Chen et al., 2015).",
        "context": null
    },
    {
        "title": "Depth map prediction from a single image using a multiscale deep network",
        "author": [
            "Eigen",
            "David",
            "Puhrsch",
            "Christian",
            "Fergus",
            "Rob"
        ],
        "venue": "In NIPS,",
        "citeRegEx": "Eigen et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Eigen et al\\.",
        "year": 2014,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "N\u02c6 4-fields: Neural network nearest neighbor fields for image transforms",
        "author": [
            "Ganin",
            "Yaroslav",
            "Lempitsky",
            "Victor"
        ],
        "venue": "In Computer Vision\u2013ACCV",
        "citeRegEx": "Ganin et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Ganin et al\\.",
        "year": 2014,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Rich feature hierarchies for accurate object detection and semantic segmentation",
        "author": [
            "Girshick",
            "Ross",
            "Donahue",
            "Jeff",
            "Darrell",
            "Trevor",
            "Malik",
            "Jitendra"
        ],
        "venue": "In CVPR,",
        "citeRegEx": "Girshick et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Girshick et al\\.",
        "year": 2014,
        "abstract": "Object detection performance, as measured on the canonical PASCAL VOC\ndataset, has plateaued in the last few years. The best-performing methods are\ncomplex ensemble systems that typically combine multiple low-level image\nfeatures with high-level context. In this paper, we propose a simple and\nscalable detection algorithm that improves mean average precision (mAP) by more\nthan 30% relative to the previous best result on VOC 2012---achieving a mAP of\n53.3%. Our approach combines two key insights: (1) one can apply high-capacity\nconvolutional neural networks (CNNs) to bottom-up region proposals in order to\nlocalize and segment objects and (2) when labeled training data is scarce,\nsupervised pre-training for an auxiliary task, followed by domain-specific\nfine-tuning, yields a significant performance boost. Since we combine region\nproposals with CNNs, we call our method R-CNN: Regions with CNN features. We\nalso compare R-CNN to OverFeat, a recently proposed sliding-window detector\nbased on a similar CNN architecture. We find that R-CNN outperforms OverFeat by\na large margin on the 200-class ILSVRC2013 detection dataset. Source code for\nthe complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn.",
        "full_text": "Rich feature hierarchies for accurate object detection and semantic segmentation\nTech report (v5)\nRoss Girshick Jeff Donahue Trevor Darrell Jitendra Malik\nUC Berkeley\n{rbg,jdonahue,trevor,malik}@eecs.berkeley.edu\nAbstract\nObject detection performance, as measured on the\ncanonical PASCAL VOC dataset, has plateaued in the last\nfew years. The best-performing methods are complex en-\nsemble systems that typically combine multiple low-level\nimage features with high-level context. In this paper, we\npropose a simple and scalable detection algorithm that im-\nproves mean average precision (mAP) by more than 30%\nrelative to the previous best result on VOC 2012\u2014achieving\na mAP of 53.3%. Our approach combines two key insights:\n(1) one can apply high-capacity convolutional neural net-\nworks (CNNs) to bottom-up region proposals in order to\nlocalize and segment objects and (2) when labeled training\ndata is scarce, supervised pre-training for an auxiliary task,\nfollowed by domain-speci\ufb01c \ufb01ne-tuning, yields a signi\ufb01cant\nperformance boost.\nSince we combine region proposals\nwith CNNs, we call our method R-CNN: Regions with CNN\nfeatures. We also compare R-CNN to OverFeat, a recently\nproposed sliding-window detector based on a similar CNN\narchitecture. We \ufb01nd that R-CNN outperforms OverFeat\nby a large margin on the 200-class ILSVRC2013 detection\ndataset. Source code for the complete system is available at\nhttp://www.cs.berkeley.edu/\u02dcrbg/rcnn.\n1. Introduction\nFeatures matter. The last decade of progress on various\nvisual recognition tasks has been based considerably on the\nuse of SIFT [29] and HOG [7]. But if we look at perfor-\nmance on the canonical visual recognition task, PASCAL\nVOC object detection [15], it is generally acknowledged\nthat progress has been slow during 2010-2012, with small\ngains obtained by building ensemble systems and employ-\ning minor variants of successful methods.\nSIFT and HOG are blockwise orientation histograms,\na representation we could associate roughly with complex\ncells in V1, the \ufb01rst cortical area in the primate visual path-\nway.\nBut we also know that recognition occurs several\nstages downstream, which suggests that there might be hier-\n1. Input \nimage\n2. Extract region \nproposals (~2k)\n3. Compute \nCNN features\naeroplane? no.\n...\nperson? yes.\ntvmonitor? no.\n4. Classify \nregions\nwarped region\n...\nCNN\nR-CNN: Regions with CNN features\nFigure 1: Object detection system overview. Our system (1)\ntakes an input image, (2) extracts around 2000 bottom-up region\nproposals, (3) computes features for each proposal using a large\nconvolutional neural network (CNN), and then (4) classi\ufb01es each\nregion using class-speci\ufb01c linear SVMs. R-CNN achieves a mean\naverage precision (mAP) of 53.7% on PASCAL VOC 2010. For\ncomparison, [39] reports 35.1% mAP using the same region pro-\nposals, but with a spatial pyramid and bag-of-visual-words ap-\nproach. The popular deformable part models perform at 33.4%.\nOn the 200-class ILSVRC2013 detection dataset, R-CNN\u2019s\nmAP is 31.4%, a large improvement over OverFeat [34], which\nhad the previous best result at 24.3%.\narchical, multi-stage processes for computing features that\nare even more informative for visual recognition.\nFukushima\u2019s\n\u201cneocognitron\u201d\n[19],\na\nbiologically-\ninspired hierarchical and shift-invariant model for pattern\nrecognition, was an early attempt at just such a process.\nThe neocognitron, however, lacked a supervised training\nalgorithm. Building on Rumelhart et al. [33], LeCun et\nal. [26] showed that stochastic gradient descent via back-\npropagation was effective for training convolutional neural\nnetworks (CNNs), a class of models that extend the neocog-\nnitron.\nCNNs saw heavy use in the 1990s (e.g., [27]), but then\nfell out of fashion with the rise of support vector machines.\nIn 2012, Krizhevsky et al. [25] rekindled interest in CNNs\nby showing substantially higher image classi\ufb01cation accu-\nracy on the ImageNet Large Scale Visual Recognition Chal-\nlenge (ILSVRC) [9, 10]. Their success resulted from train-\ning a large CNN on 1.2 million labeled images, together\nwith a few twists on LeCun\u2019s CNN (e.g., max(x, 0) rectify-\ning non-linearities and \u201cdropout\u201d regularization).\nThe signi\ufb01cance of the ImageNet result was vigorously\n1\narXiv:1311.2524v5  [cs.CV]  22 Oct 2014\ndebated during the ILSVRC 2012 workshop. The central\nissue can be distilled to the following: To what extent do\nthe CNN classi\ufb01cation results on ImageNet generalize to\nobject detection results on the PASCAL VOC Challenge?\nWe answer this question by bridging the gap between\nimage classi\ufb01cation and object detection. This paper is the\n\ufb01rst to show that a CNN can lead to dramatically higher ob-\nject detection performance on PASCAL VOC as compared\nto systems based on simpler HOG-like features. To achieve\nthis result, we focused on two problems: localizing objects\nwith a deep network and training a high-capacity model\nwith only a small quantity of annotated detection data.\nUnlike image classi\ufb01cation, detection requires localiz-\ning (likely many) objects within an image. One approach\nframes localization as a regression problem. However, work\nfrom Szegedy et al. [38], concurrent with our own, indi-\ncates that this strategy may not fare well in practice (they\nreport a mAP of 30.5% on VOC 2007 compared to the\n58.5% achieved by our method). An alternative is to build a\nsliding-window detector. CNNs have been used in this way\nfor at least two decades, typically on constrained object cat-\negories, such as faces [32, 40] and pedestrians [35]. In order\nto maintain high spatial resolution, these CNNs typically\nonly have two convolutional and pooling layers. We also\nconsidered adopting a sliding-window approach. However,\nunits high up in our network, which has \ufb01ve convolutional\nlayers, have very large receptive \ufb01elds (195 \u00d7 195 pixels)\nand strides (32\u00d732 pixels) in the input image, which makes\nprecise localization within the sliding-window paradigm an\nopen technical challenge.\nInstead, we solve the CNN localization problem by oper-\nating within the \u201crecognition using regions\u201d paradigm [21],\nwhich has been successful for both object detection [39] and\nsemantic segmentation [5]. At test time, our method gener-\nates around 2000 category-independent region proposals for\nthe input image, extracts a \ufb01xed-length feature vector from\neach proposal using a CNN, and then classi\ufb01es each region\nwith category-speci\ufb01c linear SVMs. We use a simple tech-\nnique (af\ufb01ne image warping) to compute a \ufb01xed-size CNN\ninput from each region proposal, regardless of the region\u2019s\nshape. Figure 1 presents an overview of our method and\nhighlights some of our results. Since our system combines\nregion proposals with CNNs, we dub the method R-CNN:\nRegions with CNN features.\nIn this updated version of this paper, we provide a head-\nto-head comparison of R-CNN and the recently proposed\nOverFeat [34] detection system by running R-CNN on the\n200-class ILSVRC2013 detection dataset. OverFeat uses a\nsliding-window CNN for detection and until now was the\nbest performing method on ILSVRC2013 detection. We\nshow that R-CNN signi\ufb01cantly outperforms OverFeat, with\na mAP of 31.4% versus 24.3%.\nA second challenge faced in detection is that labeled data\nis scarce and the amount currently available is insuf\ufb01cient\nfor training a large CNN. The conventional solution to this\nproblem is to use unsupervised pre-training, followed by su-\npervised \ufb01ne-tuning (e.g., [35]). The second principle con-\ntribution of this paper is to show that supervised pre-training\non a large auxiliary dataset (ILSVRC), followed by domain-\nspeci\ufb01c \ufb01ne-tuning on a small dataset (PASCAL), is an\neffective paradigm for learning high-capacity CNNs when\ndata is scarce. In our experiments, \ufb01ne-tuning for detection\nimproves mAP performance by 8 percentage points. After\n\ufb01ne-tuning, our system achieves a mAP of 54% on VOC\n2010 compared to 33% for the highly-tuned, HOG-based\ndeformable part model (DPM) [17, 20]. We also point read-\ners to contemporaneous work by Donahue et al. [12], who\nshow that Krizhevsky\u2019s CNN can be used (without \ufb01ne-\ntuning) as a blackbox feature extractor, yielding excellent\nperformance on several recognition tasks including scene\nclassi\ufb01cation, \ufb01ne-grained sub-categorization, and domain\nadaptation.\nOur system is also quite ef\ufb01cient. The only class-speci\ufb01c\ncomputations are a reasonably small matrix-vector product\nand greedy non-maximum suppression. This computational\nproperty follows from features that are shared across all cat-\negories and that are also two orders of magnitude lower-\ndimensional than previously used region features (cf. [39]).\nUnderstanding the failure modes of our approach is also\ncritical for improving it, and so we report results from the\ndetection analysis tool of Hoiem et al. [23]. As an im-\nmediate consequence of this analysis, we demonstrate that\na simple bounding-box regression method signi\ufb01cantly re-\nduces mislocalizations, which are the dominant error mode.\nBefore developing technical details, we note that because\nR-CNN operates on regions it is natural to extend it to the\ntask of semantic segmentation. With minor modi\ufb01cations,\nwe also achieve competitive results on the PASCAL VOC\nsegmentation task, with an average segmentation accuracy\nof 47.9% on the VOC 2011 test set.\n2. Object detection with R-CNN\nOur object detection system consists of three modules.\nThe \ufb01rst generates category-independent region proposals.\nThese proposals de\ufb01ne the set of candidate detections avail-\nable to our detector. The second module is a large convo-\nlutional neural network that extracts a \ufb01xed-length feature\nvector from each region. The third module is a set of class-\nspeci\ufb01c linear SVMs. In this section, we present our design\ndecisions for each module, describe their test-time usage,\ndetail how their parameters are learned, and show detection\nresults on PASCAL VOC 2010-12 and on ILSVRC2013.\n2.1. Module design\nRegion proposals. A variety of recent papers offer meth-\nods for generating category-independent region proposals.\n2\naeroplane\nbicycle\nbird\ncar\nFigure 2: Warped training samples from VOC 2007 train.\nExamples include: objectness [1], selective search [39],\ncategory-independent object proposals [14], constrained\nparametric min-cuts (CPMC) [5], multi-scale combinatorial\ngrouping [3], and Cires\u00b8an et al. [6], who detect mitotic cells\nby applying a CNN to regularly-spaced square crops, which\nare a special case of region proposals. While R-CNN is ag-\nnostic to the particular region proposal method, we use se-\nlective search to enable a controlled comparison with prior\ndetection work (e.g., [39, 41]).\nFeature extraction. We extract a 4096-dimensional fea-\nture vector from each region proposal using the Caffe [24]\nimplementation of the CNN described by Krizhevsky et\nal. [25]. Features are computed by forward propagating\na mean-subtracted 227 \u00d7 227 RGB image through \ufb01ve con-\nvolutional layers and two fully connected layers. We refer\nreaders to [24, 25] for more network architecture details.\nIn order to compute features for a region proposal, we\nmust \ufb01rst convert the image data in that region into a form\nthat is compatible with the CNN (its architecture requires\ninputs of a \ufb01xed 227 \u00d7 227 pixel size). Of the many possi-\nble transformations of our arbitrary-shaped regions, we opt\nfor the simplest. Regardless of the size or aspect ratio of the\ncandidate region, we warp all pixels in a tight bounding box\naround it to the required size. Prior to warping, we dilate the\ntight bounding box so that at the warped size there are ex-\nactly p pixels of warped image context around the original\nbox (we use p = 16). Figure 2 shows a random sampling\nof warped training regions. Alternatives to warping are dis-\ncussed in Appendix A.\n2.2. Test-time detection\nAt test time, we run selective search on the test image\nto extract around 2000 region proposals (we use selective\nsearch\u2019s \u201cfast mode\u201d in all experiments). We warp each\nproposal and forward propagate it through the CNN in or-\nder to compute features. Then, for each class, we score\neach extracted feature vector using the SVM trained for that\nclass. Given all scored regions in an image, we apply a\ngreedy non-maximum suppression (for each class indepen-\ndently) that rejects a region if it has an intersection-over-\nunion (IoU) overlap with a higher scoring selected region\nlarger than a learned threshold.\nRun-time analysis. Two properties make detection ef\ufb01-\ncient. First, all CNN parameters are shared across all cate-\ngories. Second, the feature vectors computed by the CNN\nare low-dimensional when compared to other common ap-\nproaches, such as spatial pyramids with bag-of-visual-word\nencodings. The features used in the UVA detection system\n[39], for example, are two orders of magnitude larger than\nours (360k vs. 4k-dimensional).\nThe result of such sharing is that the time spent com-\nputing region proposals and features (13s/image on a GPU\nor 53s/image on a CPU) is amortized over all classes. The\nonly class-speci\ufb01c computations are dot products between\nfeatures and SVM weights and non-maximum suppression.\nIn practice, all dot products for an image are batched into\na single matrix-matrix product. The feature matrix is typi-\ncally 2000\u00d74096 and the SVM weight matrix is 4096\u00d7N,\nwhere N is the number of classes.\nThis analysis shows that R-CNN can scale to thousands\nof object classes without resorting to approximate tech-\nniques, such as hashing. Even if there were 100k classes,\nthe resulting matrix multiplication takes only 10 seconds on\na modern multi-core CPU. This ef\ufb01ciency is not merely the\nresult of using region proposals and shared features. The\nUVA system, due to its high-dimensional features, would\nbe two orders of magnitude slower while requiring 134GB\nof memory just to store 100k linear predictors, compared to\njust 1.5GB for our lower-dimensional features.\nIt is also interesting to contrast R-CNN with the recent\nwork from Dean et al. on scalable detection using DPMs\nand hashing [8]. They report a mAP of around 16% on VOC\n2007 at a run-time of 5 minutes per image when introducing\n10k distractor classes. With our approach, 10k detectors can\nrun in about a minute on a CPU, and because no approxi-\nmations are made mAP would remain at 59% (Section 3.2).\n2.3. Training\nSupervised pre-training. We discriminatively pre-trained\nthe CNN on a large auxiliary dataset (ILSVRC2012 clas-\nsi\ufb01cation) using image-level annotations only (bounding-\nbox labels are not available for this data).\nPre-training\nwas performed using the open source Caffe CNN library\n[24]. In brief, our CNN nearly matches the performance\nof Krizhevsky et al. [25], obtaining a top-1 error rate 2.2\npercentage points higher on the ILSVRC2012 classi\ufb01cation\nvalidation set. This discrepancy is due to simpli\ufb01cations in\nthe training process.\nDomain-speci\ufb01c \ufb01ne-tuning. To adapt our CNN to the\nnew task (detection) and the new domain (warped proposal\nwindows), we continue stochastic gradient descent (SGD)\ntraining of the CNN parameters using only warped region\nproposals.\nAside from replacing the CNN\u2019s ImageNet-\nspeci\ufb01c 1000-way classi\ufb01cation layer with a randomly ini-\ntialized (N + 1)-way classi\ufb01cation layer (where N is the\nnumber of object classes, plus 1 for background), the CNN\narchitecture is unchanged.\nFor VOC, N = 20 and for\nILSVRC2013, N = 200. We treat all region proposals with\n3\n\u22650.5 IoU overlap with a ground-truth box as positives for\nthat box\u2019s class and the rest as negatives. We start SGD at\na learning rate of 0.001 (1/10th of the initial pre-training\nrate), which allows \ufb01ne-tuning to make progress while not\nclobbering the initialization. In each SGD iteration, we uni-\nformly sample 32 positive windows (over all classes) and\n96 background windows to construct a mini-batch of size\n128. We bias the sampling towards positive windows be-\ncause they are extremely rare compared to background.\nObject category classi\ufb01ers. Consider training a binary\nclassi\ufb01er to detect cars.\nIt\u2019s clear that an image region\ntightly enclosing a car should be a positive example. Simi-\nlarly, it\u2019s clear that a background region, which has nothing\nto do with cars, should be a negative example. Less clear\nis how to label a region that partially overlaps a car. We re-\nsolve this issue with an IoU overlap threshold, below which\nregions are de\ufb01ned as negatives. The overlap threshold, 0.3,\nwas selected by a grid search over {0, 0.1, . . . , 0.5} on a\nvalidation set. We found that selecting this threshold care-\nfully is important. Setting it to 0.5, as in [39], decreased\nmAP by 5 points. Similarly, setting it to 0 decreased mAP\nby 4 points. Positive examples are de\ufb01ned simply to be the\nground-truth bounding boxes for each class.\nOnce features are extracted and training labels are ap-\nplied, we optimize one linear SVM per class. Since the\ntraining data is too large to \ufb01t in memory, we adopt the\nstandard hard negative mining method [17, 37]. Hard neg-\native mining converges quickly and in practice mAP stops\nincreasing after only a single pass over all images.\nIn Appendix B we discuss why the positive and negative\nexamples are de\ufb01ned differently in \ufb01ne-tuning versus SVM\ntraining. We also discuss the trade-offs involved in training\ndetection SVMs rather than simply using the outputs from\nthe \ufb01nal softmax layer of the \ufb01ne-tuned CNN.\n2.4. Results on PASCAL VOC 2010-12\nFollowing the PASCAL VOC best practices [15], we\nvalidated all design decisions and hyperparameters on the\nVOC 2007 dataset (Section 3.2). For \ufb01nal results on the\nVOC 2010-12 datasets, we \ufb01ne-tuned the CNN on VOC\n2012 train and optimized our detection SVMs on VOC 2012\ntrainval. We submitted test results to the evaluation server\nonly once for each of the two major algorithm variants (with\nand without bounding-box regression).\nTable 1 shows complete results on VOC 2010. We com-\npare our method against four strong baselines, including\nSegDPM [18], which combines DPM detectors with the\noutput of a semantic segmentation system [4] and uses ad-\nditional inter-detector context and image-classi\ufb01er rescor-\ning. The most germane comparison is to the UVA system\nfrom Uijlings et al. [39], since our systems use the same re-\ngion proposal algorithm. To classify regions, their method\nbuilds a four-level spatial pyramid and populates it with\ndensely sampled SIFT, Extended OpponentSIFT, and RGB-\nSIFT descriptors, each vector quantized with 4000-word\ncodebooks. Classi\ufb01cation is performed with a histogram\nintersection kernel SVM. Compared to their multi-feature,\nnon-linear kernel SVM approach, we achieve a large im-\nprovement in mAP, from 35.1% to 53.7% mAP, while also\nbeing much faster (Section 2.2). Our method achieves sim-\nilar performance (53.3% mAP) on VOC 2011/12 test.\n2.5. Results on ILSVRC2013 detection\nWe ran R-CNN on the 200-class ILSVRC2013 detection\ndataset using the same system hyperparameters that we used\nfor PASCAL VOC. We followed the same protocol of sub-\nmitting test results to the ILSVRC2013 evaluation server\nonly twice, once with and once without bounding-box re-\ngression.\nFigure 3 compares R-CNN to the entries in the ILSVRC\n2013 competition and to the post-competition OverFeat re-\nsult [34]. R-CNN achieves a mAP of 31.4%, which is sig-\nni\ufb01cantly ahead of the second-best result of 24.3% from\nOverFeat.\nTo give a sense of the AP distribution over\nclasses, box plots are also presented and a table of per-\nclass APs follows at the end of the paper in Table 8. Most\nof the competing submissions (OverFeat, NEC-MU, UvA-\nEuvision, Toronto A, and UIUC-IFP) used convolutional\nneural networks, indicating that there is signi\ufb01cant nuance\nin how CNNs can be applied to object detection, leading to\ngreatly varying outcomes.\nIn Section 4, we give an overview of the ILSVRC2013\ndetection dataset and provide details about choices that we\nmade when running R-CNN on it.\n3. Visualization, ablation, and modes of error\n3.1. Visualizing learned features\nFirst-layer \ufb01lters can be visualized directly and are easy\nto understand [25]. They capture oriented edges and oppo-\nnent colors. Understanding the subsequent layers is more\nchallenging. Zeiler and Fergus present a visually attrac-\ntive deconvolutional approach in [42]. We propose a simple\n(and complementary) non-parametric method that directly\nshows what the network learned.\nThe idea is to single out a particular unit (feature) in the\nnetwork and use it as if it were an object detector in its own\nright. That is, we compute the unit\u2019s activations on a large\nset of held-out region proposals (about 10 million), sort the\nproposals from highest to lowest activation, perform non-\nmaximum suppression, and then display the top-scoring re-\ngions. Our method lets the selected unit \u201cspeak for itself\u201d\nby showing exactly which inputs it \ufb01res on. We avoid aver-\naging in order to see different visual modes and gain insight\ninto the invariances computed by the unit.\n4\nVOC 2010 test aero bike bird boat bottle\nbus\ncar\ncat\nchair cow table\ndog\nhorse mbike person plant sheep sofa train\ntv\nmAP\nDPM v5 [20]\u2020\n49.2 53.8 13.1 15.3\n35.5\n53.4 49.7 27.0\n17.2\n28.8 14.7 17.8\n46.4\n51.2\n47.7\n10.8\n34.2\n20.7 43.8 38.3\n33.4\nUVA [39]\n56.2 42.4 15.3 12.6\n21.8\n49.3 36.8 46.1\n12.9\n32.1 30.0 36.5\n43.5\n52.9\n32.9\n15.3\n41.1\n31.8 47.0 44.8\n35.1\nRegionlets [41] 65.0 48.9 25.9 24.6\n24.5\n56.1 54.5 51.2\n17.0\n28.9 30.2 35.8\n40.2\n55.7\n43.5\n14.3\n43.9\n32.6 54.0 45.9\n39.7\nSegDPM [18]\u2020\n61.4 53.4 25.6 25.2\n35.5\n51.7 50.6 50.8\n19.3\n33.8 26.8 40.4\n48.3\n54.4\n47.1\n14.8\n38.7\n35.0 52.8 43.1\n40.4\nR-CNN\n67.1 64.1 46.7 32.0\n30.5\n56.4 57.2 65.9\n27.0\n47.3 40.9 66.6\n57.8\n65.9\n53.6\n26.7\n56.5\n38.1 52.8 50.2\n50.2\nR-CNN BB\n71.8 65.8 53.0 36.8\n35.9\n59.7 60.0 69.9\n27.9\n50.6 41.4 70.0\n62.0\n69.0\n58.1\n29.5\n59.4\n39.3 61.2 52.4\n53.7\nTable 1: Detection average precision (%) on VOC 2010 test. R-CNN is most directly comparable to UVA and Regionlets since all\nmethods use selective search region proposals. Bounding-box regression (BB) is described in Section C. At publication time, SegDPM\nwas the top-performer on the PASCAL VOC leaderboard. \u2020DPM and SegDPM use context rescoring not used by the other methods.\n0\n20\n40\n60\n80\n100\nUIUC\u2212IFP \nDelta \nGPU_UCLA \nSYSU_Vision \nToronto A \n*OverFeat (1) \n*NEC\u2212MU \nUvA\u2212Euvision \n*OverFeat (2) \n*R\u2212CNN BB \nmean average precision (mAP) in %\nILSVRC2013 detection test set mAP\n \n \n1.0%\n6.1%\n9.8%\n10.5%\n11.5%\n19.4%\n20.9%\n22.6%\n24.3%\n31.4%\ncompetition result\npost competition result\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n*R\u2212CNN BB\nUvA\u2212Euvision\n*NEC\u2212MU\n*OverFeat (1)\nToronto A\nSYSU_Vision\nGPU_UCLA\nDelta\nUIUC\u2212IFP\naverage precision (AP) in %\nILSVRC2013 detection test set class AP box plots\nFigure 3: (Left) Mean average precision on the ILSVRC2013 detection test set. Methods preceeded by * use outside training data\n(images and labels from the ILSVRC classi\ufb01cation dataset in all cases). (Right) Box plots for the 200 average precision values per\nmethod. A box plot for the post-competition OverFeat result is not shown because per-class APs are not yet available (per-class APs for\nR-CNN are in Table 8 and also included in the tech report source uploaded to arXiv.org; see R-CNN-ILSVRC2013-APs.txt). The red\nline marks the median AP, the box bottom and top are the 25th and 75th percentiles. The whiskers extend to the min and max AP of each\nmethod. Each AP is plotted as a green dot over the whiskers (best viewed digitally with zoom).\n1.0\n1.0\n0.9\n0.9\n0.9\n0.9\n0.9\n0.9\n0.9\n0.9\n0.9\n0.9\n0.9\n0.9\n0.9\n0.9\n1.0\n0.9\n0.9\n0.8\n0.8\n0.8\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.6\n0.6\n1.0\n0.8\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.6\n0.6\n1.0\n0.9\n0.8\n0.8\n0.8\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n1.0\n1.0\n0.9\n0.9\n0.9\n0.8\n0.8\n0.8\n0.8\n0.8\n0.8\n0.8\n0.8\n0.8\n0.8\n0.8\n1.0\n0.9\n0.8\n0.8\n0.8\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\nFigure 4: Top regions for six pool5 units. Receptive \ufb01elds and activation values are drawn in white. Some units are aligned to concepts,\nsuch as people (row 1) or text (4). Other units capture texture and material properties, such as dot arrays (2) and specular re\ufb02ections (6).\n5\nVOC 2007 test\naero bike bird boat bottle\nbus\ncar\ncat\nchair cow table\ndog\nhorse mbike person plant sheep sofa train\ntv\nmAP\nR-CNN pool5\n51.8 60.2 36.4 27.8\n23.2\n52.8 60.6 49.2\n18.3\n47.8 44.3 40.8\n56.6\n58.7\n42.4\n23.4\n46.1\n36.7 51.3 55.7\n44.2\nR-CNN fc6\n59.3 61.8 43.1 34.0\n25.1\n53.1 60.6 52.8\n21.7\n47.8 42.7 47.8\n52.5\n58.5\n44.6\n25.6\n48.3\n34.0 53.1 58.0\n46.2\nR-CNN fc7\n57.6 57.9 38.5 31.8\n23.7\n51.2 58.9 51.4\n20.0\n50.5 40.9 46.0\n51.6\n55.9\n43.3\n23.3\n48.1\n35.3 51.0 57.4\n44.7\nR-CNN FT pool5\n58.2 63.3 37.9 27.6\n26.1\n54.1 66.9 51.4\n26.7\n55.5 43.4 43.1\n57.7\n59.0\n45.8\n28.1\n50.8\n40.6 53.1 56.4\n47.3\nR-CNN FT fc6\n63.5 66.0 47.9 37.7\n29.9\n62.5 70.2 60.2\n32.0\n57.9 47.0 53.5\n60.1\n64.2\n52.2\n31.3\n55.0\n50.0 57.7 63.0\n53.1\nR-CNN FT fc7\n64.2 69.7 50.0 41.9\n32.0\n62.6 71.0 60.7\n32.7\n58.5 46.5 56.1\n60.6\n66.8\n54.2\n31.5\n52.8\n48.9 57.9 64.7\n54.2\nR-CNN FT fc7 BB 68.1 72.8 56.8 43.0\n36.8\n66.3 74.2 67.6\n34.4\n63.5 54.5 61.2\n69.1\n68.6\n58.7\n33.4\n62.9\n51.1 62.5 64.8\n58.5\nDPM v5 [20]\n33.2 60.3 10.2 16.1\n27.3\n54.3 58.2 23.0\n20.0\n24.1 26.7 12.7\n58.1\n48.2\n43.2\n12.0\n21.1\n36.1 46.0 43.5\n33.7\nDPM ST [28]\n23.8 58.2 10.5\n8.5\n27.1\n50.4 52.0\n7.3\n19.2\n22.8 18.1\n8.0\n55.9\n44.8\n32.4\n13.3\n15.9\n22.8 46.2 44.9\n29.1\nDPM HSC [31]\n32.2 58.3 11.5 16.3\n30.6\n49.9 54.8 23.5\n21.5\n27.7 34.0 13.7\n58.1\n51.6\n39.9\n12.4\n23.5\n34.4 47.4 45.2\n34.3\nTable 2: Detection average precision (%) on VOC 2007 test. Rows 1-3 show R-CNN performance without \ufb01ne-tuning. Rows 4-6 show\nresults for the CNN pre-trained on ILSVRC 2012 and then \ufb01ne-tuned (FT) on VOC 2007 trainval. Row 7 includes a simple bounding-box\nregression (BB) stage that reduces localization errors (Section C). Rows 8-10 present DPM methods as a strong baseline. The \ufb01rst uses\nonly HOG, while the next two use different feature learning approaches to augment or replace HOG.\nVOC 2007 test\naero bike bird boat bottle\nbus\ncar\ncat\nchair cow table\ndog\nhorse mbike person plant sheep sofa train\ntv\nmAP\nR-CNN T-Net\n64.2 69.7 50.0 41.9\n32.0\n62.6 71.0 60.7\n32.7\n58.5 46.5 56.1\n60.6\n66.8\n54.2\n31.5\n52.8\n48.9 57.9 64.7\n54.2\nR-CNN T-Net BB\n68.1 72.8 56.8 43.0\n36.8\n66.3 74.2 67.6\n34.4\n63.5 54.5 61.2\n69.1\n68.6\n58.7\n33.4\n62.9\n51.1 62.5 64.8\n58.5\nR-CNN O-Net\n71.6 73.5 58.1 42.2\n39.4\n70.7 76.0 74.5\n38.7\n71.0 56.9 74.5\n67.9\n69.6\n59.3\n35.7\n62.1\n64.0 66.5 71.2\n62.2\nR-CNN O-Net BB 73.4 77.0 63.4 45.4\n44.6\n75.1 78.1 79.8\n40.5\n73.7 62.2 79.4\n78.1\n73.1\n64.2\n35.6\n66.8\n67.2 70.4 71.1\n66.0\nTable 3: Detection average precision (%) on VOC 2007 test for two different CNN architectures. The \ufb01rst two rows are results from\nTable 2 using Krizhevsky et al.\u2019s architecture (T-Net). Rows three and four use the recently proposed 16-layer architecture from Simonyan\nand Zisserman (O-Net) [43].\nWe visualize units from layer pool5, which is the max-\npooled output of the network\u2019s \ufb01fth and \ufb01nal convolutional\nlayer.\nThe pool5 feature map is 6 \u00d7 6 \u00d7 256 = 9216-\ndimensional. Ignoring boundary effects, each pool5 unit has\na receptive \ufb01eld of 195\u00d7195 pixels in the original 227\u00d7227\npixel input. A central pool5 unit has a nearly global view,\nwhile one near the edge has a smaller, clipped support.\nEach row in Figure 4 displays the top 16 activations for\na pool5 unit from a CNN that we \ufb01ne-tuned on VOC 2007\ntrainval. Six of the 256 functionally unique units are visu-\nalized (Appendix D includes more). These units were se-\nlected to show a representative sample of what the network\nlearns. In the second row, we see a unit that \ufb01res on dog\nfaces and dot arrays. The unit corresponding to the third row\nis a red blob detector. There are also detectors for human\nfaces and more abstract patterns such as text and triangular\nstructures with windows. The network appears to learn a\nrepresentation that combines a small number of class-tuned\nfeatures together with a distributed representation of shape,\ntexture, color, and material properties. The subsequent fully\nconnected layer fc6 has the ability to model a large set of\ncompositions of these rich features.\n3.2. Ablation studies\nPerformance layer-by-layer, without \ufb01ne-tuning. To un-\nderstand which layers are critical for detection performance,\nwe analyzed results on the VOC 2007 dataset for each of the\nCNN\u2019s last three layers. Layer pool5 was brie\ufb02y described\nin Section 3.1. The \ufb01nal two layers are summarized below.\nLayer fc6 is fully connected to pool5. To compute fea-\ntures, it multiplies a 4096\u00d79216 weight matrix by the pool5\nfeature map (reshaped as a 9216-dimensional vector) and\nthen adds a vector of biases. This intermediate vector is\ncomponent-wise half-wave recti\ufb01ed (x \u2190max(0, x)).\nLayer fc7 is the \ufb01nal layer of the network. It is imple-\nmented by multiplying the features computed by fc6 by a\n4096 \u00d7 4096 weight matrix, and similarly adding a vector\nof biases and applying half-wave recti\ufb01cation.\nWe start by looking at results from the CNN without\n\ufb01ne-tuning on PASCAL, i.e.\nall CNN parameters were\npre-trained on ILSVRC 2012 only. Analyzing performance\nlayer-by-layer (Table 2 rows 1-3) reveals that features from\nfc7 generalize worse than features from fc6. This means\nthat 29%, or about 16.8 million, of the CNN\u2019s parameters\ncan be removed without degrading mAP. More surprising is\nthat removing both fc7 and fc6 produces quite good results\neven though pool5 features are computed using only 6% of\nthe CNN\u2019s parameters. Much of the CNN\u2019s representational\npower comes from its convolutional layers, rather than from\nthe much larger densely connected layers. This \ufb01nding sug-\ngests potential utility in computing a dense feature map, in\nthe sense of HOG, of an arbitrary-sized image by using only\nthe convolutional layers of the CNN. This representation\nwould enable experimentation with sliding-window detec-\ntors, including DPM, on top of pool5 features.\nPerformance layer-by-layer, with \ufb01ne-tuning. We now\nlook at results from our CNN after having \ufb01ne-tuned its pa-\n6\nrameters on VOC 2007 trainval. The improvement is strik-\ning (Table 2 rows 4-6): \ufb01ne-tuning increases mAP by 8.0\npercentage points to 54.2%. The boost from \ufb01ne-tuning is\nmuch larger for fc6 and fc7 than for pool5, which suggests\nthat the pool5 features learned from ImageNet are general\nand that most of the improvement is gained from learning\ndomain-speci\ufb01c non-linear classi\ufb01ers on top of them.\nComparison to recent feature learning methods. Rela-\ntively few feature learning methods have been tried on PAS-\nCAL VOC detection. We look at two recent approaches that\nbuild on deformable part models. For reference, we also in-\nclude results for the standard HOG-based DPM [20].\nThe \ufb01rst DPM feature learning method, DPM ST [28],\naugments HOG features with histograms of \u201csketch token\u201d\nprobabilities. Intuitively, a sketch token is a tight distri-\nbution of contours passing through the center of an image\npatch. Sketch token probabilities are computed at each pixel\nby a random forest that was trained to classify 35\u00d735 pixel\npatches into one of 150 sketch tokens or background.\nThe second method, DPM HSC [31], replaces HOG with\nhistograms of sparse codes (HSC). To compute an HSC,\nsparse code activations are solved for at each pixel using\na learned dictionary of 100 7 \u00d7 7 pixel (grayscale) atoms.\nThe resulting activations are recti\ufb01ed in three ways (full and\nboth half-waves), spatially pooled, unit \u21132 normalized, and\nthen power transformed (x \u2190sign(x)|x|\u03b1).\nAll R-CNN variants strongly outperform the three DPM\nbaselines (Table 2 rows 8-10), including the two that use\nfeature learning. Compared to the latest version of DPM,\nwhich uses only HOG features, our mAP is more than 20\npercentage points higher: 54.2% vs. 33.7%\u2014a 61% rela-\ntive improvement. The combination of HOG and sketch to-\nkens yields 2.5 mAP points over HOG alone, while HSC\nimproves over HOG by 4 mAP points (when compared\ninternally to their private DPM baselines\u2014both use non-\npublic implementations of DPM that underperform the open\nsource version [20]).\nThese methods achieve mAPs of\n29.1% and 34.3%, respectively.\n3.3. Network architectures\nMost results in this paper use the network architecture\nfrom Krizhevsky et al. [25]. However, we have found that\nthe choice of architecture has a large effect on R-CNN de-\ntection performance. In Table 3 we show results on VOC\n2007 test using the 16-layer deep network recently proposed\nby Simonyan and Zisserman [43]. This network was one of\nthe top performers in the recent ILSVRC 2014 classi\ufb01ca-\ntion challenge. The network has a homogeneous structure\nconsisting of 13 layers of 3 \u00d7 3 convolution kernels, with\n\ufb01ve max pooling layers interspersed, and topped with three\nfully-connected layers. We refer to this network as \u201cO-Net\u201d\nfor OxfordNet and the baseline as \u201cT-Net\u201d for TorontoNet.\nTo use O-Net in R-CNN, we downloaded the pub-\nlicly\navailable\npre-trained\nnetwork\nweights\nfor\nthe\nVGG ILSVRC 16 layers model from the Caffe Model\nZoo.1 We then \ufb01ne-tuned the network using the same pro-\ntocol as we used for T-Net. The only difference was to use\nsmaller minibatches (24 examples) as required in order to\n\ufb01t within GPU memory. The results in Table 3 show that R-\nCNN with O-Net substantially outperforms R-CNN with T-\nNet, increasing mAP from 58.5% to 66.0%. However there\nis a considerable drawback in terms of compute time, with\nthe forward pass of O-Net taking roughly 7 times longer\nthan T-Net.\n3.4. Detection error analysis\nWe applied the excellent detection analysis tool from\nHoiem et al.\n[23] in order to reveal our method\u2019s error\nmodes, understand how \ufb01ne-tuning changes them, and to\nsee how our error types compare with DPM. A full sum-\nmary of the analysis tool is beyond the scope of this pa-\nper and we encourage readers to consult [23] to understand\nsome \ufb01ner details (such as \u201cnormalized AP\u201d). Since the\nanalysis is best absorbed in the context of the associated\nplots, we present the discussion within the captions of Fig-\nure 5 and Figure 6.\n3.5. Bounding-box regression\nBased on the error analysis, we implemented a sim-\nple method to reduce localization errors. Inspired by the\nbounding-box regression employed in DPM [17], we train a\nlinear regression model to predict a new detection window\ngiven the pool5 features for a selective search region pro-\nposal. Full details are given in Appendix C. Results in Ta-\nble 1, Table 2, and Figure 5 show that this simple approach\n\ufb01xes a large number of mislocalized detections, boosting\nmAP by 3 to 4 points.\n3.6. Qualitative results\nQualitative detection results on ILSVRC2013 are pre-\nsented in Figure 8 and Figure 9 at the end of the paper. Each\nimage was sampled randomly from the val2 set and all de-\ntections from all detectors with a precision greater than 0.5\nare shown. Note that these are not curated and give a re-\nalistic impression of the detectors in action. More qualita-\ntive results are presented in Figure 10 and Figure 11, but\nthese have been curated. We selected each image because it\ncontained interesting, surprising, or amusing results. Here,\nalso, all detections at precision greater than 0.5 are shown.\n4. The ILSVRC2013 detection dataset\nIn Section 2 we presented results on the ILSVRC2013\ndetection dataset. This dataset is less homogeneous than\n1https://github.com/BVLC/caffe/wiki/Model-Zoo\n7\nocc\ntrn\nsize\nasp\nview part\n0\n0.2\n0.4\n0.6\n0.8\n0.212\n0.612\n0.420\n0.557\n0.201\n0.720\n0.344\n0.606\n0.351\n0.677\n0.244\n0.609\n0.516\nnormalized AP\nR\u2212CNN fc6: sensitivity and impact\nocc\ntrn\nsize\nasp\nview part\n0\n0.2\n0.4\n0.6\n0.8\n0.179\n0.701\n0.498\n0.634\n0.335\n0.766\n0.442\n0.672\n0.429\n0.723\n0.325\n0.685\n0.593\nnormalized AP\nR\u2212CNN FT fc7: sensitivity and impact\nocc\ntrn\nsize\nasp\nview part\n0\n0.2\n0.4\n0.6\n0.8\n0.211\n0.731\n0.542\n0.676\n0.385\n0.786\n0.484\n0.709\n0.453\n0.779\n0.368\n0.720\n0.633\nnormalized AP\nR\u2212CNN FT fc7 BB: sensitivity and impact\nocc\ntrn\nsize\nasp\nview part\n0\n0.2\n0.4\n0.6\n0.8\n0.132\n0.339\n0.216\n0.347\n0.056\n0.487\n0.126\n0.453\n0.137\n0.391\n0.094\n0.388\n0.297\nnormalized AP\nDPM voc\u2212release5: sensitivity and impact\nFigure 6: Sensitivity to object characteristics. Each plot shows the mean (over classes) normalized AP (see [23]) for the highest and\nlowest performing subsets within six different object characteristics (occlusion, truncation, bounding-box area, aspect ratio, viewpoint, part\nvisibility). We show plots for our method (R-CNN) with and without \ufb01ne-tuning (FT) and bounding-box regression (BB) as well as for\nDPM voc-release5. Overall, \ufb01ne-tuning does not reduce sensitivity (the difference between max and min), but does substantially improve\nboth the highest and lowest performing subsets for nearly all characteristics. This indicates that \ufb01ne-tuning does more than simply improve\nthe lowest performing subsets for aspect ratio and bounding-box area, as one might conjecture based on how we warp network inputs.\nInstead, \ufb01ne-tuning improves robustness for all characteristics including occlusion, truncation, viewpoint, and part visibility.\ntotal false positives\npercentage of each type\nR\u2212CNN fc6: animals\n \n \n25\n100\n400\n1600 6400\n0\n20\n40\n60\n80\n100\nLoc\nSim\nOth\nBG\ntotal false positives\npercentage of each type\nR\u2212CNN FT fc7: animals\n \n \n25\n100\n400\n1600 6400\n0\n20\n40\n60\n80\n100\nLoc\nSim\nOth\nBG\ntotal false positives\npercentage of each type\nR\u2212CNN FT fc7 BB: animals\n \n \n25\n100\n400\n1600 6400\n0\n20\n40\n60\n80\n100\nLoc\nSim\nOth\nBG\ntotal false positives\npercentage of each type\nR\u2212CNN fc6: furniture\n \n \n25\n100\n400\n1600 6400\n0\n20\n40\n60\n80\n100\nLoc\nSim\nOth\nBG\ntotal false positives\npercentage of each type\nR\u2212CNN FT fc7: furniture\n \n \n25\n100\n400\n1600 6400\n0\n20\n40\n60\n80\n100\nLoc\nSim\nOth\nBG\ntotal false positives\npercentage of each type\nR\u2212CNN FT fc7 BB: furniture\n \n \n25\n100\n400\n1600 6400\n0\n20\n40\n60\n80\n100\nLoc\nSim\nOth\nBG\nFigure 5: Distribution of top-ranked false positive (FP) types.\nEach plot shows the evolving distribution of FP types as more FPs\nare considered in order of decreasing score. Each FP is catego-\nrized into 1 of 4 types: Loc\u2014poor localization (a detection with\nan IoU overlap with the correct class between 0.1 and 0.5, or a du-\nplicate); Sim\u2014confusion with a similar category; Oth\u2014confusion\nwith a dissimilar object category; BG\u2014a FP that \ufb01red on back-\nground. Compared with DPM (see [23]), signi\ufb01cantly more of\nour errors result from poor localization, rather than confusion with\nbackground or other object classes, indicating that the CNN fea-\ntures are much more discriminative than HOG. Loose localiza-\ntion likely results from our use of bottom-up region proposals and\nthe positional invariance learned from pre-training the CNN for\nwhole-image classi\ufb01cation. Column three shows how our simple\nbounding-box regression method \ufb01xes many localization errors.\nPASCAL VOC, requiring choices about how to use it. Since\nthese decisions are non-trivial, we cover them in this sec-\ntion.\n4.1. Dataset overview\nThe ILSVRC2013 detection dataset is split into three\nsets: train (395,918), val (20,121), and test (40,152), where\nthe number of images in each set is in parentheses. The\nval and test splits are drawn from the same image distribu-\ntion. These images are scene-like and similar in complexity\n(number of objects, amount of clutter, pose variability, etc.)\nto PASCAL VOC images. The val and test splits are exhaus-\ntively annotated, meaning that in each image all instances\nfrom all 200 classes are labeled with bounding boxes. The\ntrain set, in contrast, is drawn from the ILSVRC2013 clas-\nsi\ufb01cation image distribution. These images have more vari-\nable complexity with a skew towards images of a single cen-\ntered object. Unlike val and test, the train images (due to\ntheir large number) are not exhaustively annotated. In any\ngiven train image, instances from the 200 classes may or\nmay not be labeled. In addition to these image sets, each\nclass has an extra set of negative images. Negative images\nare manually checked to validate that they do not contain\nany instances of their associated class. The negative im-\nage sets were not used in this work. More information on\nhow ILSVRC was collected and annotated can be found in\n[11, 36].\nThe nature of these splits presents a number of choices\nfor training R-CNN. The train images cannot be used for\nhard negative mining, because annotations are not exhaus-\ntive. Where should negative examples come from? Also,\nthe train images have different statistics than val and test.\nShould the train images be used at all, and if so, to what\nextent? While we have not thoroughly evaluated a large\nnumber of choices, we present what seemed like the most\nobvious path based on previous experience.\nOur general strategy is to rely heavily on the val set and\nuse some of the train images as an auxiliary source of pos-\nitive examples.\nTo use val for both training and valida-\ntion, we split it into roughly equally sized \u201cval1\u201d and \u201cval2\u201d\nsets. Since some classes have very few examples in val (the\nsmallest has only 31 and half have fewer than 110), it is\nimportant to produce an approximately class-balanced par-\ntition. To do this, a large number of candidate splits were\ngenerated and the one with the smallest maximum relative\n8\nclass imbalance was selected.2\nEach candidate split was\ngenerated by clustering val images using their class counts\nas features, followed by a randomized local search that may\nimprove the split balance. The particular split used here has\na maximum relative imbalance of about 11% and a median\nrelative imbalance of 4%. The val1/val2 split and code used\nto produce them will be publicly available to allow other re-\nsearchers to compare their methods on the val splits used in\nthis report.\n4.2. Region proposals\nWe followed the same region proposal approach that was\nused for detection on PASCAL. Selective search [39] was\nrun in \u201cfast mode\u201d on each image in val1, val2, and test (but\nnot on images in train). One minor modi\ufb01cation was re-\nquired to deal with the fact that selective search is not scale\ninvariant and so the number of regions produced depends\non the image resolution. ILSVRC image sizes range from\nvery small to a few that are several mega-pixels, and so we\nresized each image to a \ufb01xed width (500 pixels) before run-\nning selective search. On val, selective search resulted in an\naverage of 2403 region proposals per image with a 91.6%\nrecall of all ground-truth bounding boxes (at 0.5 IoU thresh-\nold). This recall is notably lower than in PASCAL, where\nit is approximately 98%, indicating signi\ufb01cant room for im-\nprovement in the region proposal stage.\n4.3. Training data\nFor training data, we formed a set of images and boxes\nthat includes all selective search and ground-truth boxes\nfrom val1 together with up to N ground-truth boxes per\nclass from train (if a class has fewer than N ground-truth\nboxes in train, then we take all of them). We\u2019ll call this\ndataset of images and boxes val1+trainN. In an ablation\nstudy, we show mAP on val2 for N \u2208{0, 500, 1000} (Sec-\ntion 4.5).\nTraining data is required for three procedures in R-CNN:\n(1) CNN \ufb01ne-tuning, (2) detector SVM training, and (3)\nbounding-box regressor training. CNN \ufb01ne-tuning was run\nfor 50k SGD iteration on val1+trainN using the exact same\nsettings as were used for PASCAL. Fine-tuning on a sin-\ngle NVIDIA Tesla K20 took 13 hours using Caffe.\nFor\nSVM training, all ground-truth boxes from val1+trainN\nwere used as positive examples for their respective classes.\nHard negative mining was performed on a randomly se-\nlected subset of 5000 images from val1. An initial experi-\nment indicated that mining negatives from all of val1, versus\na 5000 image subset (roughly half of it), resulted in only a\n0.5 percentage point drop in mAP, while cutting SVM train-\ning time in half. No negative examples were taken from\n2Relative imbalance is measured as |a \u2212b|/(a + b) where a and b are\nclass counts in each half of the split.\ntrain because the annotations are not exhaustive. The ex-\ntra sets of veri\ufb01ed negative images were not used.\nThe\nbounding-box regressors were trained on val1.\n4.4. Validation and evaluation\nBefore submitting results to the evaluation server, we\nvalidated data usage choices and the effect of \ufb01ne-tuning\nand bounding-box regression on the val2 set using the train-\ning data described above. All system hyperparameters (e.g.,\nSVM C hyperparameters, padding used in region warp-\ning, NMS thresholds, bounding-box regression hyperpa-\nrameters) were \ufb01xed at the same values used for PAS-\nCAL. Undoubtedly some of these hyperparameter choices\nare slightly suboptimal for ILSVRC, however the goal of\nthis work was to produce a preliminary R-CNN result on\nILSVRC without extensive dataset tuning. After selecting\nthe best choices on val2, we submitted exactly two result\n\ufb01les to the ILSVRC2013 evaluation server. The \ufb01rst sub-\nmission was without bounding-box regression and the sec-\nond submission was with bounding-box regression.\nFor\nthese submissions, we expanded the SVM and bounding-\nbox regressor training sets to use val+train1k and val, re-\nspectively.\nWe used the CNN that was \ufb01ne-tuned on\nval1+train1k to avoid re-running \ufb01ne-tuning and feature\ncomputation.\n4.5. Ablation study\nTable 4 shows an ablation study of the effects of differ-\nent amounts of training data, \ufb01ne-tuning, and bounding-\nbox regression. A \ufb01rst observation is that mAP on val2\nmatches mAP on test very closely.\nThis gives us con\ufb01-\ndence that mAP on val2 is a good indicator of test set per-\nformance. The \ufb01rst result, 20.9%, is what R-CNN achieves\nusing a CNN pre-trained on the ILSVRC2012 classi\ufb01ca-\ntion dataset (no \ufb01ne-tuning) and given access to the small\namount of training data in val1 (recall that half of the classes\nin val1 have between 15 and 55 examples).\nExpanding\nthe training set to val1+trainN improves performance to\n24.1%, with essentially no difference between N = 500\nand N = 1000. Fine-tuning the CNN using examples from\njust val1 gives a modest improvement to 26.5%, however\nthere is likely signi\ufb01cant over\ufb01tting due to the small number\nof positive training examples. Expanding the \ufb01ne-tuning\nset to val1+train1k, which adds up to 1000 positive exam-\nples per class from the train set, helps signi\ufb01cantly, boosting\nmAP to 29.7%. Bounding-box regression improves results\nto 31.0%, which is a smaller relative gain that what was ob-\nserved in PASCAL.\n4.6. Relationship to OverFeat\nThere is an interesting relationship between R-CNN and\nOverFeat: OverFeat can be seen (roughly) as a special case\nof R-CNN. If one were to replace selective search region\n9\ntest set\nval2\nval2\nval2\nval2\nval2\nval2\ntest\ntest\nSVM training set\nval1 val1+train.5k val1+train1k val1+train1k val1+train1k val1+train1k\nval+train1k\nval+train1k\nCNN \ufb01ne-tuning set\nn/a\nn/a\nn/a\nval1\nval1+train1k val1+train1k val1+train1k val1+train1k\nbbox reg set\nn/a\nn/a\nn/a\nn/a\nn/a\nval1\nn/a\nval\nCNN feature layer\nfc6\nfc6\nfc6\nfc7\nfc7\nfc7\nfc7\nfc7\nmAP\n20.9\n24.1\n24.1\n26.5\n29.7\n31.0\n30.2\n31.4\nmedian AP\n17.7\n21.0\n21.4\n24.8\n29.2\n29.6\n29.0\n30.3\nTable 4: ILSVRC2013 ablation study of data usage choices, \ufb01ne-tuning, and bounding-box regression.\nproposals with a multi-scale pyramid of regular square re-\ngions and change the per-class bounding-box regressors to\na single bounding-box regressor, then the systems would\nbe very similar (modulo some potentially signi\ufb01cant differ-\nences in how they are trained: CNN detection \ufb01ne-tuning,\nusing SVMs, etc.). It is worth noting that OverFeat has\na signi\ufb01cant speed advantage over R-CNN: it is about 9x\nfaster, based on a \ufb01gure of 2 seconds per image quoted from\n[34]. This speed comes from the fact that OverFeat\u2019s slid-\ning windows (i.e., region proposals) are not warped at the\nimage level and therefore computation can be easily shared\nbetween overlapping windows. Sharing is implemented by\nrunning the entire network in a convolutional fashion over\narbitrary-sized inputs. Speeding up R-CNN should be pos-\nsible in a variety of ways and remains as future work.\n5. Semantic segmentation\nRegion classi\ufb01cation is a standard technique for seman-\ntic segmentation, allowing us to easily apply R-CNN to the\nPASCAL VOC segmentation challenge. To facilitate a di-\nrect comparison with the current leading semantic segmen-\ntation system (called O2P for \u201csecond-order pooling\u201d) [4],\nwe work within their open source framework. O2P uses\nCPMC to generate 150 region proposals per image and then\npredicts the quality of each region, for each class, using\nsupport vector regression (SVR). The high performance of\ntheir approach is due to the quality of the CPMC regions\nand the powerful second-order pooling of multiple feature\ntypes (enriched variants of SIFT and LBP). We also note\nthat Farabet et al. [16] recently demonstrated good results\non several dense scene labeling datasets (not including PAS-\nCAL) using a CNN as a multi-scale per-pixel classi\ufb01er.\nWe follow [2, 4] and extend the PASCAL segmentation\ntraining set to include the extra annotations made available\nby Hariharan et al. [22]. Design decisions and hyperparam-\neters were cross-validated on the VOC 2011 validation set.\nFinal test results were evaluated only once.\nCNN features for segmentation. We evaluate three strate-\ngies for computing features on CPMC regions, all of which\nbegin by warping the rectangular window around the re-\ngion to 227 \u00d7 227. The \ufb01rst strategy (full) ignores the re-\ngion\u2019s shape and computes CNN features directly on the\nwarped window, exactly as we did for detection. However,\nthese features ignore the non-rectangular shape of the re-\ngion. Two regions might have very similar bounding boxes\nwhile having very little overlap. Therefore, the second strat-\negy (fg) computes CNN features only on a region\u2019s fore-\nground mask. We replace the background with the mean\ninput so that background regions are zero after mean sub-\ntraction. The third strategy (full+fg) simply concatenates\nthe full and fg features; our experiments validate their com-\nplementarity.\nfull R-CNN\nfg R-CNN\nfull+fg R-CNN\nO2P [4]\nfc6\nfc7\nfc6\nfc7\nfc6\nfc7\n46.4\n43.0\n42.5\n43.7\n42.1\n47.9\n45.8\nTable 5: Segmentation mean accuracy (%) on VOC 2011 vali-\ndation. Column 1 presents O2P; 2-7 use our CNN pre-trained on\nILSVRC 2012.\nResults on VOC 2011. Table 5 shows a summary of our\nresults on the VOC 2011 validation set compared with O2P.\n(See Appendix E for complete per-category results.) Within\neach feature computation strategy, layer fc6 always outper-\nforms fc7 and the following discussion refers to the fc6 fea-\ntures. The fg strategy slightly outperforms full, indicating\nthat the masked region shape provides a stronger signal,\nmatching our intuition. However, full+fg achieves an aver-\nage accuracy of 47.9%, our best result by a margin of 4.2%\n(also modestly outperforming O2P), indicating that the con-\ntext provided by the full features is highly informative even\ngiven the fg features. Notably, training the 20 SVRs on our\nfull+fg features takes an hour on a single core, compared to\n10+ hours for training on O2P features.\nIn Table 6 we present results on the VOC 2011 test\nset, comparing our best-performing method, fc6 (full+fg),\nagainst two strong baselines. Our method achieves the high-\nest segmentation accuracy for 11 out of 21 categories, and\nthe highest overall segmentation accuracy of 47.9%, aver-\naged across categories (but likely ties with the O2P result\nunder any reasonable margin of error). Still better perfor-\nmance could likely be achieved by \ufb01ne-tuning.\n10\nVOC 2011 test\nbg\naero bike bird boat bottle\nbus\ncar\ncat\nchair cow table\ndog\nhorse mbike person plant sheep sofa train\ntv\nmean\nR&P [2]\n83.4 46.8 18.9 36.6 31.2\n42.7\n57.3 47.4 44.1\n8.1\n39.4 36.1 36.3\n49.5\n48.3\n50.7\n26.3\n47.2\n22.1 42.0 43.2\n40.8\nO2P [4]\n85.4 69.7 22.3 45.2 44.4\n46.9\n66.7 57.8 56.2\n13.5\n46.1 32.3 41.2\n59.1\n55.3\n51.0\n36.2\n50.4\n27.8 46.9 44.6\n47.6\nours (full+fg R-CNN fc6) 84.2 66.9 23.7 58.3 37.4\n55.4\n73.3 58.7 56.5\n9.7\n45.5 29.5 49.3\n40.1\n57.8\n53.9\n33.8\n60.7\n22.7 47.1 41.3\n47.9\nTable 6: Segmentation accuracy (%) on VOC 2011 test. We compare against two strong baselines: the \u201cRegions and Parts\u201d (R&P)\nmethod of [2] and the second-order pooling (O2P) method of [4]. Without any \ufb01ne-tuning, our CNN achieves top segmentation perfor-\nmance, outperforming R&P and roughly matching O2P.\n6. Conclusion\nIn recent years, object detection performance had stag-\nnated.\nThe best performing systems were complex en-\nsembles combining multiple low-level image features with\nhigh-level context from object detectors and scene classi-\n\ufb01ers. This paper presents a simple and scalable object de-\ntection algorithm that gives a 30% relative improvement\nover the best previous results on PASCAL VOC 2012.\nWe achieved this performance through two insights. The\n\ufb01rst is to apply high-capacity convolutional neural net-\nworks to bottom-up region proposals in order to localize\nand segment objects. The second is a paradigm for train-\ning large CNNs when labeled training data is scarce. We\nshow that it is highly effective to pre-train the network\u2014\nwith supervision\u2014for a auxiliary task with abundant data\n(image classi\ufb01cation) and then to \ufb01ne-tune the network for\nthe target task where data is scarce (detection). We conjec-\nture that the \u201csupervised pre-training/domain-speci\ufb01c \ufb01ne-\ntuning\u201d paradigm will be highly effective for a variety of\ndata-scarce vision problems.\nWe conclude by noting that it is signi\ufb01cant that we\nachieved these results by using a combination of classi-\ncal tools from computer vision and deep learning (bottom-\nup region proposals and convolutional neural networks).\nRather than opposing lines of scienti\ufb01c inquiry, the two are\nnatural and inevitable partners.\nAcknowledgments. This research was supported in part\nby DARPA Mind\u2019s Eye and MSEE programs, by NSF\nawards\nIIS-0905647,\nIIS-1134072,\nand\nIIS-1212798,\nMURI N000014-10-1-0933, and by support from Toyota.\nThe GPUs used in this research were generously donated\nby the NVIDIA Corporation.\nAppendix\nA. Object proposal transformations\nThe convolutional neural network used in this work re-\nquires a \ufb01xed-size input of 227 \u00d7 227 pixels. For detec-\ntion, we consider object proposals that are arbitrary image\nrectangles. We evaluated two approaches for transforming\nobject proposals into valid CNN inputs.\nThe \ufb01rst method (\u201ctightest square with context\u201d) en-\ncloses each object proposal inside the tightest square and\n(A)\n(B)\n(C)\n(D)\n(A)\n(B)\n(C)\n(D)\nFigure 7: Different object proposal transformations. (A) the\noriginal object proposal at its actual scale relative to the trans-\nformed CNN inputs; (B) tightest square with context; (C) tight-\nest square without context; (D) warp. Within each column and\nexample proposal, the top row corresponds to p = 0 pixels of con-\ntext padding while the bottom row has p = 16 pixels of context\npadding.\nthen scales (isotropically) the image contained in that\nsquare to the CNN input size. Figure 7 column (B) shows\nthis transformation.\nA variant on this method (\u201ctightest\nsquare without context\u201d) excludes the image content that\nsurrounds the original object proposal. Figure 7 column\n(C) shows this transformation. The second method (\u201cwarp\u201d)\nanisotropically scales each object proposal to the CNN in-\nput size. Figure 7 column (D) shows the warp transforma-\ntion.\nFor each of these transformations, we also consider in-\ncluding additional image context around the original object\nproposal. The amount of context padding (p) is de\ufb01ned as a\nborder size around the original object proposal in the trans-\nformed input coordinate frame. Figure 7 shows p = 0 pix-\nels in the top row of each example and p = 16 pixels in\nthe bottom row. In all methods, if the source rectangle ex-\ntends beyond the image, the missing data is replaced with\nthe image mean (which is then subtracted before inputing\nthe image into the CNN). A pilot set of experiments showed\nthat warping with context padding (p = 16 pixels) outper-\nformed the alternatives by a large margin (3-5 mAP points).\nObviously more alternatives are possible, including using\nreplication instead of mean padding. Exhaustive evaluation\nof these alternatives is left as future work.\n11\nB. Positive vs. negative examples and softmax\nTwo design choices warrant further discussion. The \ufb01rst\nis: Why are positive and negative examples de\ufb01ned differ-\nently for \ufb01ne-tuning the CNN versus training the object de-\ntection SVMs? To review the de\ufb01nitions brie\ufb02y, for \ufb01ne-\ntuning we map each object proposal to the ground-truth in-\nstance with which it has maximum IoU overlap (if any) and\nlabel it as a positive for the matched ground-truth class if the\nIoU is at least 0.5. All other proposals are labeled \u201cback-\nground\u201d (i.e., negative examples for all classes). For train-\ning SVMs, in contrast, we take only the ground-truth boxes\nas positive examples for their respective classes and label\nproposals with less than 0.3 IoU overlap with all instances\nof a class as a negative for that class. Proposals that fall\ninto the grey zone (more than 0.3 IoU overlap, but are not\nground truth) are ignored.\nHistorically speaking, we arrived at these de\ufb01nitions be-\ncause we started by training SVMs on features computed\nby the ImageNet pre-trained CNN, and so \ufb01ne-tuning was\nnot a consideration at that point in time. In that setup, we\nfound that our particular label de\ufb01nition for training SVMs\nwas optimal within the set of options we evaluated (which\nincluded the setting we now use for \ufb01ne-tuning). When we\nstarted using \ufb01ne-tuning, we initially used the same positive\nand negative example de\ufb01nition as we were using for SVM\ntraining. However, we found that results were much worse\nthan those obtained using our current de\ufb01nition of positives\nand negatives.\nOur hypothesis is that this difference in how positives\nand negatives are de\ufb01ned is not fundamentally important\nand arises from the fact that \ufb01ne-tuning data is limited.\nOur current scheme introduces many \u201cjittered\u201d examples\n(those proposals with overlap between 0.5 and 1, but not\nground truth), which expands the number of positive exam-\nples by approximately 30x. We conjecture that this large\nset is needed when \ufb01ne-tuning the entire network to avoid\nover\ufb01tting. However, we also note that using these jittered\nexamples is likely suboptimal because the network is not\nbeing \ufb01ne-tuned for precise localization.\nThis leads to the second issue: Why, after \ufb01ne-tuning,\ntrain SVMs at all? It would be cleaner to simply apply the\nlast layer of the \ufb01ne-tuned network, which is a 21-way soft-\nmax regression classi\ufb01er, as the object detector. We tried\nthis and found that performance on VOC 2007 dropped\nfrom 54.2% to 50.9% mAP. This performance drop likely\narises from a combination of several factors including that\nthe de\ufb01nition of positive examples used in \ufb01ne-tuning does\nnot emphasize precise localization and the softmax classi-\n\ufb01er was trained on randomly sampled negative examples\nrather than on the subset of \u201chard negatives\u201d used for SVM\ntraining.\nThis result shows that it\u2019s possible to obtain close to\nthe same level of performance without training SVMs af-\nter \ufb01ne-tuning. We conjecture that with some additional\ntweaks to \ufb01ne-tuning the remaining performance gap may\nbe closed. If true, this would simplify and speed up R-CNN\ntraining with no loss in detection performance.\nC. Bounding-box regression\nWe use a simple bounding-box regression stage to im-\nprove localization performance. After scoring each selec-\ntive search proposal with a class-speci\ufb01c detection SVM,\nwe predict a new bounding box for the detection using a\nclass-speci\ufb01c bounding-box regressor.\nThis is similar in\nspirit to the bounding-box regression used in deformable\npart models [17]. The primary difference between the two\napproaches is that here we regress from features computed\nby the CNN, rather than from geometric features computed\non the inferred DPM part locations.\nThe input to our training algorithm is a set of N train-\ning pairs {(P i, Gi)}i=1,...,N, where P i = (P i\nx, P i\ny, P i\nw, P i\nh)\nspeci\ufb01es the pixel coordinates of the center of proposal P i\u2019s\nbounding box together with P i\u2019s width and height in pixels.\nHence forth, we drop the superscript i unless it is needed.\nEach ground-truth bounding box G is speci\ufb01ed in the same\nway: G = (Gx, Gy, Gw, Gh). Our goal is to learn a trans-\nformation that maps a proposed box P to a ground-truth box\nG.\nWe parameterize the transformation in terms of four\nfunctions dx(P), dy(P), dw(P), and dh(P).\nThe \ufb01rst\ntwo specify a scale-invariant translation of the center of\nP\u2019s bounding box, while the second two specify log-space\ntranslations of the width and height of P\u2019s bounding box.\nAfter learning these functions, we can transform an input\nproposal P into a predicted ground-truth box \u02c6G by apply-\ning the transformation\n\u02c6Gx = Pwdx(P) + Px\n(1)\n\u02c6Gy = Phdy(P) + Py\n(2)\n\u02c6Gw = Pw exp(dw(P))\n(3)\n\u02c6Gh = Ph exp(dh(P)).\n(4)\nEach function d\u22c6(P) (where \u22c6is one of x, y, h, w) is\nmodeled as a linear function of the pool5 features of pro-\nposal P, denoted by \u03c65(P). (The dependence of \u03c65(P)\non the image data is implicitly assumed.) Thus we have\nd\u22c6(P) = wT\n\u22c6\u03c65(P), where w\u22c6is a vector of learnable\nmodel parameters. We learn w\u22c6by optimizing the regu-\nlarized least squares objective (ridge regression):\nw\u22c6= argmin\n\u02c6w\u22c6\nN\nX\ni\n(ti\n\u22c6\u2212\u02c6wT\n\u22c6\u03c65(P i))2 + \u03bb \u2225\u02c6w\u22c6\u22252 .\n(5)\n12\nThe regression targets t\u22c6for the training pair (P, G) are de-\n\ufb01ned as\ntx = (Gx \u2212Px)/Pw\n(6)\nty = (Gy \u2212Py)/Ph\n(7)\ntw = log(Gw/Pw)\n(8)\nth = log(Gh/Ph).\n(9)\nAs a standard regularized least squares problem, this can be\nsolved ef\ufb01ciently in closed form.\nWe\nfound\ntwo\nsubtle\nissues\nwhile\nimplementing\nbounding-box regression.\nThe \ufb01rst is that regularization\nis important: we set \u03bb = 1000 based on a validation set.\nThe second issue is that care must be taken when selecting\nwhich training pairs (P, G) to use. Intuitively, if P is far\nfrom all ground-truth boxes, then the task of transforming\nP to a ground-truth box G does not make sense. Using ex-\namples like P would lead to a hopeless learning problem.\nTherefore, we only learn from a proposal P if it is nearby\nat least one ground-truth box. We implement \u201cnearness\u201d by\nassigning P to the ground-truth box G with which it has\nmaximum IoU overlap (in case it overlaps more than one) if\nand only if the overlap is greater than a threshold (which we\nset to 0.6 using a validation set). All unassigned proposals\nare discarded. We do this once for each object class in order\nto learn a set of class-speci\ufb01c bounding-box regressors.\nAt test time, we score each proposal and predict its new\ndetection window only once. In principle, we could iterate\nthis procedure (i.e., re-score the newly predicted bounding\nbox, and then predict a new bounding box from it, and so\non). However, we found that iterating does not improve\nresults.\nD. Additional feature visualizations\nFigure 12 shows additional visualizations for 20 pool5\nunits. For each unit, we show the 24 region proposals that\nmaximally activate that unit out of the full set of approxi-\nmately 10 million regions in all of VOC 2007 test.\nWe label each unit by its (y, x, channel) position in the\n6 \u00d7 6 \u00d7 256 dimensional pool5 feature map. Within each\nchannel, the CNN computes exactly the same function of\nthe input region, with the (y, x) position changing only the\nreceptive \ufb01eld.\nE. Per-category segmentation results\nIn Table 7 we show the per-category segmentation ac-\ncuracy on VOC 2011 val for each of our six segmentation\nmethods in addition to the O2P method [4]. These results\nshow which methods are strongest across each of the 20\nPASCAL classes, plus the background class.\nF. Analysis of cross-dataset redundancy\nOne concern when training on an auxiliary dataset is that\nthere might be redundancy between it and the test set. Even\nthough the tasks of object detection and whole-image clas-\nsi\ufb01cation are substantially different, making such cross-set\nredundancy much less worrisome, we still conducted a thor-\nough investigation that quanti\ufb01es the extent to which PAS-\nCAL test images are contained within the ILSVRC 2012\ntraining and validation sets. Our \ufb01ndings may be useful to\nresearchers who are interested in using ILSVRC 2012 as\ntraining data for the PASCAL image classi\ufb01cation task.\nWe performed two checks for duplicate (and near-\nduplicate) images. The \ufb01rst test is based on exact matches\nof \ufb02ickr image IDs, which are included in the VOC 2007\ntest annotations (these IDs are intentionally kept secret for\nsubsequent PASCAL test sets). All PASCAL images, and\nabout half of ILSVRC, were collected from \ufb02ickr.com. This\ncheck turned up 31 matches out of 4952 (0.63%).\nThe second check uses GIST [30] descriptor matching,\nwhich was shown in [13] to have excellent performance at\nnear-duplicate image detection in large (> 1 million) image\ncollections. Following [13], we computed GIST descrip-\ntors on warped 32 \u00d7 32 pixel versions of all ILSVRC 2012\ntrainval and PASCAL 2007 test images.\nEuclidean distance nearest-neighbor matching of GIST\ndescriptors revealed 38 near-duplicate images (including all\n31 found by \ufb02ickr ID matching). The matches tend to vary\nslightly in JPEG compression level and resolution, and to a\nlesser extent cropping. These \ufb01ndings show that the overlap\nis small, less than 1%. For VOC 2012, because \ufb02ickr IDs\nare not available, we used the GIST matching method only.\nBased on GIST matches, 1.5% of VOC 2012 test images\nare in ILSVRC 2012 trainval. The slightly higher rate for\nVOC 2012 is likely due to the fact that the two datasets\nwere collected closer together in time than VOC 2007 and\nILSVRC 2012 were.\nG. Document changelog\nThis document tracks the progress of R-CNN. To help\nreaders understand how it has changed over time, here\u2019s a\nbrief changelog describing the revisions.\nv1 Initial version.\nv2 CVPR 2014 camera-ready revision. Includes substan-\ntial improvements in detection performance brought about\nby (1) starting \ufb01ne-tuning from a higher learning rate (0.001\ninstead of 0.0001), (2) using context padding when prepar-\ning CNN inputs, and (3) bounding-box regression to \ufb01x lo-\ncalization errors.\nv3 Results on the ILSVRC2013 detection dataset and com-\nparison with OverFeat were integrated into several sections\n(primarily Section 2 and Section 4).\n13\nVOC 2011 val\nbg\naero bike bird boat bottle\nbus\ncar\ncat\nchair cow table\ndog\nhorse mbike person plant sheep sofa train\ntv\nmean\nO2P [4]\n84.0 69.0 21.7 47.7 42.2\n42.4\n64.7 65.8 57.4\n12.9\n37.4 20.5 43.7\n35.7\n52.7\n51.0\n35.8\n51.0\n28.4 59.8 49.7\n46.4\nfull R-CNN fc6\n81.3 56.2 23.9 42.9 40.7\n38.8\n59.2 56.5 53.2\n11.4\n34.6 16.7 48.1\n37.0\n51.4\n46.0\n31.5\n44.0\n24.3 53.7 51.1\n43.0\nfull R-CNN fc7\n81.0 52.8 25.1 43.8 40.5\n42.7\n55.4 57.7 51.3\n8.7\n32.5 11.5 48.1\n37.0\n50.5\n46.4\n30.2\n42.1\n21.2 57.7 56.0\n42.5\nfg R-CNN fc6\n81.4 54.1 21.1 40.6 38.7\n53.6\n59.9 57.2 52.5\n9.1\n36.5 23.6 46.4\n38.1\n53.2\n51.3\n32.2\n38.7\n29.0 53.0 47.5\n43.7\nfg R-CNN fc7\n80.9 50.1 20.0 40.2 34.1\n40.9\n59.7 59.8 52.7\n7.3\n32.1 14.3 48.8\n42.9\n54.0\n48.6\n28.9\n42.6\n24.9 52.2 48.8\n42.1\nfull+fg R-CNN fc6 83.1 60.4 23.2 48.4 47.3\n52.6\n61.6 60.6 59.1\n10.8\n45.8 20.9 57.7\n43.3\n57.4\n52.9\n34.7\n48.7\n28.1 60.0 48.6\n47.9\nfull+fg R-CNN fc7 82.3 56.7 20.6 49.9 44.2\n43.6\n59.3 61.3 57.8\n7.7\n38.4 15.1 53.4\n43.7\n50.8\n52.0\n34.1\n47.8\n24.7 60.1 55.2\n45.7\nTable 7: Per-category segmentation accuracy (%) on the VOC 2011 validation set.\nv4 The softmax vs. SVM results in Appendix B contained\nan error, which has been \ufb01xed. We thank Sergio Guadar-\nrama for helping to identify this issue.\nv5 Added results using the new 16-layer network architec-\nture from Simonyan and Zisserman [43] to Section 3.3 and\nTable 3.\nReferences\n[1] B. Alexe, T. Deselaers, and V. Ferrari. Measuring the object-\nness of image windows. TPAMI, 2012. 2\n[2] P. Arbel\u00b4aez, B. Hariharan, C. Gu, S. Gupta, L. Bourdev, and\nJ. Malik. Semantic segmentation using regions and parts. In\nCVPR, 2012. 10, 11\n[3] P. Arbel\u00b4aez, J. Pont-Tuset, J. Barron, F. Marques, and J. Ma-\nlik. Multiscale combinatorial grouping. In CVPR, 2014. 3\n[4] J. Carreira, R. Caseiro, J. Batista, and C. Sminchisescu. Se-\nmantic segmentation with second-order pooling. In ECCV,\n2012. 4, 10, 11, 13, 14\n[5] J. Carreira and C. Sminchisescu.\nCPMC: Automatic ob-\nject segmentation using constrained parametric min-cuts.\nTPAMI, 2012. 2, 3\n[6] D. Cires\u00b8an, A. Giusti, L. Gambardella, and J. Schmidhu-\nber. Mitosis detection in breast cancer histology images with\ndeep neural networks. In MICCAI, 2013. 3\n[7] N. Dalal and B. Triggs. Histograms of oriented gradients for\nhuman detection. In CVPR, 2005. 1\n[8] T. Dean, M. A. Ruzon, M. Segal, J. Shlens, S. Vijaya-\nnarasimhan, and J. Yagnik.\nFast, accurate detection of\n100,000 object classes on a single machine. In CVPR, 2013.\n3\n[9] J. Deng, A. Berg, S. Satheesh, H. Su, A. Khosla, and L. Fei-\nFei. ImageNet Large Scale Visual Recognition Competition\n2012 (ILSVRC2012). http://www.image-net.org/\nchallenges/LSVRC/2012/. 1\n[10] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\nFei. ImageNet: A large-scale hierarchical image database.\nIn CVPR, 2009. 1\n[11] J. Deng, O. Russakovsky, J. Krause, M. Bernstein, A. C.\nBerg, and L. Fei-Fei. Scalable multi-label annotation. In\nCHI, 2014. 8\n[12] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang,\nE. Tzeng, and T. Darrell. DeCAF: A Deep Convolutional\nActivation Feature for Generic Visual Recognition. In ICML,\n2014. 2\n[13] M. Douze, H. J\u00b4egou, H. Sandhawalia, L. Amsaleg, and\nC. Schmid. Evaluation of gist descriptors for web-scale im-\nage search. In Proc. of the ACM International Conference on\nImage and Video Retrieval, 2009. 13\n[14] I. Endres and D. Hoiem. Category independent object pro-\nposals. In ECCV, 2010. 3\n[15] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and\nA. Zisserman. The PASCAL Visual Object Classes (VOC)\nChallenge. IJCV, 2010. 1, 4\n[16] C. Farabet, C. Couprie, L. Najman, and Y. LeCun. Learning\nhierarchical features for scene labeling. TPAMI, 2013. 10\n[17] P. Felzenszwalb, R. Girshick, D. McAllester, and D. Ra-\nmanan. Object detection with discriminatively trained part\nbased models. TPAMI, 2010. 2, 4, 7, 12\n[18] S. Fidler, R. Mottaghi, A. Yuille, and R. Urtasun. Bottom-up\nsegmentation for top-down detection. In CVPR, 2013. 4, 5\n[19] K. Fukushima.\nNeocognitron:\nA self-organizing neu-\nral network model for a mechanism of pattern recogni-\ntion unaffected by shift in position. Biological cybernetics,\n36(4):193\u2013202, 1980. 1\n[20] R. Girshick, P. Felzenszwalb, and D. McAllester. Discrimi-\nnatively trained deformable part models, release 5. http:\n//www.cs.berkeley.edu/\u02dcrbg/latent-v5/.\n2,\n5, 6, 7\n[21] C. Gu, J. J. Lim, P. Arbel\u00b4aez, and J. Malik. Recognition\nusing regions. In CVPR, 2009. 2\n[22] B. Hariharan, P. Arbel\u00b4aez, L. Bourdev, S. Maji, and J. Malik.\nSemantic contours from inverse detectors. In ICCV, 2011.\n10\n[23] D. Hoiem, Y. Chodpathumwan, and Q. Dai. Diagnosing error\nin object detectors. In ECCV. 2012. 2, 7, 8\n[24] Y. Jia.\nCaffe:\nAn open source convolutional archi-\ntecture for fast feature embedding.\nhttp://caffe.\nberkeleyvision.org/, 2013. 3\n[25] A. Krizhevsky, I. Sutskever, and G. Hinton. ImageNet clas-\nsi\ufb01cation with deep convolutional neural networks. In NIPS,\n2012. 1, 3, 4, 7\n[26] Y. LeCun, B. Boser, J. Denker, D. Henderson, R. Howard,\nW. Hubbard, and L. Jackel.\nBackpropagation applied to\nhandwritten zip code recognition. Neural Comp., 1989. 1\n[27] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-\nbased learning applied to document recognition. Proc. of the\nIEEE, 1998. 1\n[28] J. J. Lim, C. L. Zitnick, and P. Doll\u00b4ar. Sketch tokens: A\nlearned mid-level representation for contour and object de-\ntection. In CVPR, 2013. 6, 7\n14\nclass\nAP class\nAP class\nAP class\nAP class\nAP\naccordion\n50.8 centipede\n30.4 hair spray\n13.8 pencil box\n11.4 snowplow\n69.2\nairplane\n50.0 chain saw\n14.1 hamburger\n34.2 pencil sharpener\n9.0 soap dispenser\n16.8\nant\n31.8 chair\n19.5 hammer\n9.9 perfume\n32.8 soccer ball\n43.7\nantelope\n53.8 chime\n24.6 hamster\n46.0 person\n41.7 sofa\n16.3\napple\n30.9 cocktail shaker\n46.2 harmonica\n12.6 piano\n20.5 spatula\n6.8\narmadillo\n54.0 coffee maker\n21.5 harp\n50.4 pineapple\n22.6 squirrel\n31.3\nartichoke\n45.0 computer keyboard 39.6 hat with a wide brim 40.5 ping-pong ball\n21.0 star\ufb01sh\n45.1\naxe\n11.8 computer mouse\n21.2 head cabbage\n17.4 pitcher\n19.2 stethoscope\n18.3\nbaby bed\n42.0 corkscrew\n24.2 helmet\n33.4 pizza\n43.7 stove\n8.1\nbackpack\n2.8 cream\n29.9 hippopotamus\n38.0 plastic bag\n6.4 strainer\n9.9\nbagel\n37.5 croquet ball\n30.0 horizontal bar\n7.0 plate rack\n15.2 strawberry\n26.8\nbalance beam 32.6 crutch\n23.7 horse\n41.7 pomegranate\n32.0 stretcher\n13.2\nbanana\n21.9 cucumber\n22.8 hotdog\n28.7 popsicle\n21.2 sunglasses\n18.8\nband aid\n17.4 cup or mug\n34.0 iPod\n59.2 porcupine\n37.2 swimming trunks\n9.1\nbanjo\n55.3 diaper\n10.1 isopod\n19.5 power drill\n7.9 swine\n45.3\nbaseball\n41.8 digital clock\n18.5 jelly\ufb01sh\n23.7 pretzel\n24.8 syringe\n5.7\nbasketball\n65.3 dishwasher\n19.9 koala bear\n44.3 printer\n21.3 table\n21.7\nbathing cap\n37.2 dog\n76.8 ladle\n3.0 puck\n14.1 tape player\n21.4\nbeaker\n11.3 domestic cat\n44.1 ladybug\n58.4 punching bag\n29.4 tennis ball\n59.1\nbear\n62.7 dragon\ufb02y\n27.8 lamp\n9.1 purse\n8.0 tick\n42.6\nbee\n52.9 drum\n19.9 laptop\n35.4 rabbit\n71.0 tie\n24.6\nbell pepper\n38.8 dumbbell\n14.1 lemon\n33.3 racket\n16.2 tiger\n61.8\nbench\n12.7 electric fan\n35.0 lion\n51.3 ray\n41.1 toaster\n29.2\nbicycle\n41.1 elephant\n56.4 lipstick\n23.1 red panda\n61.1 traf\ufb01c light\n24.7\nbinder\n6.2 face powder\n22.1 lizard\n38.9 refrigerator\n14.0 train\n60.8\nbird\n70.9 \ufb01g\n44.5 lobster\n32.4 remote control\n41.6 trombone\n13.8\nbookshelf\n19.3 \ufb01ling cabinet\n20.6 maillot\n31.0 rubber eraser\n2.5 trumpet\n14.4\nbow tie\n38.8 \ufb02ower pot\n20.2 maraca\n30.1 rugby ball\n34.5 turtle\n59.1\nbow\n9.0 \ufb02ute\n4.9 microphone\n4.0 ruler\n11.5 tv or monitor\n41.7\nbowl\n26.7 fox\n59.3 microwave\n40.1 salt or pepper shaker 24.6 unicycle\n27.2\nbrassiere\n31.2 french horn\n24.2 milk can\n33.3 saxophone\n40.8 vacuum\n19.5\nburrito\n25.7 frog\n64.1 miniskirt\n14.9 scorpion\n57.3 violin\n13.7\nbus\n57.5 frying pan\n21.5 monkey\n49.6 screwdriver\n10.6 volleyball\n59.7\nbutter\ufb02y\n88.5 giant panda\n42.5 motorcycle\n42.2 seal\n20.9 waf\ufb02e iron\n24.0\ncamel\n37.6 gold\ufb01sh\n28.6 mushroom\n31.8 sheep\n48.9 washer\n39.8\ncan opener\n28.9 golf ball\n51.3 nail\n4.5 ski\n9.0 water bottle\n8.1\ncar\n44.5 golfcart\n47.9 neck brace\n31.6 skunk\n57.9 watercraft\n40.9\ncart\n48.0 guacamole\n32.3 oboe\n27.5 snail\n36.2 whale\n48.6\ncattle\n32.3 guitar\n33.1 orange\n38.8 snake\n33.8 wine bottle\n31.2\ncello\n28.9 hair dryer\n13.0 otter\n22.2 snowmobile\n58.8 zebra\n49.6\nTable 8: Per-class average precision (%) on the ILSVRC2013 detection test set.\n[29] D. Lowe.\nDistinctive image features from scale-invariant\nkeypoints. IJCV, 2004. 1\n[30] A. Oliva and A. Torralba. Modeling the shape of the scene:\nA holistic representation of the spatial envelope. IJCV, 2001.\n13\n[31] X. Ren and D. Ramanan. Histograms of sparse codes for\n15\nlemon 0.79\nlemon 0.70\nlemon 0.56\nlemon 0.50\nperson 0.88\nperson 0.72\ncocktail shaker 0.56\ndog 0.97\ndog 0.85\ndog 0.57\nbird 0.63\ndog 0.97\ndog 0.95\ndog 0.64\nhelmet 0.65\nhelmet 0.52\nmotorcycle 0.65\nperson 0.75\nperson 0.58\nsnowmobile 0.83\nsnowmobile 0.83\nbow tie 0.86\nperson 0.82\nbird 0.61\ndog 0.66\ndog 0.61\ndomestic cat 0.57\nbird 0.96\ndog 0.91\ndog 0.77\nsofa 0.71\ndog 0.95\ndog 0.55\nladybug 1.00\nperson 0.87\ncar 0.96\ncar 0.66\ncar 0.63\nbird 0.98\nperson 0.65\nwatercraft 1.00\nwatercraft 0.69\npretzel 0.78\ncar 0.96\nperson 0.65\nperson 0.58\nperson 0.52\nperson 0.52\nbird 0.99\nbird 0.91\nbird 0.75\ndog 0.98\nflower pot 0.62\ndog 0.97\ndog 0.56\ntrain 1.00\ntrain 0.53\narmadillo 1.00\narmadillo 0.56\nbird 0.93\ndog 0.92\nswine 0.88\nbird 1.00\nbutterfly 0.96\nperson 0.90\nflower pot 0.62\nsnake 0.70\nturtle 0.54\nbell pepper 0.81\nbell pepper 0.62\nbell pepper 0.54\nruler 1.00\nantelope 0.53\nmushroom 0.93\ntv or monitor 0.82\ntv or monitor 0.76\ntv or monitor 0.54\nbird 0.89\nlipstick 0.80\nlipstick 0.61\nperson 0.58\ndog 0.97\nsoccer ball 0.90\nFigure 8: Example detections on the val2 set from the con\ufb01guration that achieved 31.0% mAP on val2. Each image was sampled randomly\n(these are not curated). All detections at precision greater than 0.5 are shown. Each detection is labeled with the predicted class and the\nprecision value of that detection from the detector\u2019s precision-recall curve. Viewing digitally with zoom is recommended.\n16\nbaby bed 0.55\nhelmet 0.51\npitcher 0.57\ndog 0.98\nhat with a wide brim 0.78\nperson 0.86\nbird 0.52\ntable 0.60\nmonkey 0.97\ntable 0.68\nwatercraft 0.55\nperson 0.88\ncar 0.61\nperson 0.87\nperson 0.51\nsunglasses 0.51\ndog 0.94\ndog 0.55\nbird 0.52\nmonkey 0.87\nmonkey 0.81\nswine 0.50\ndog 0.97\nhat with a wide brim 0.96\nsnake 0.74\ndog 0.93\nperson 0.77\ndog 0.97\nguacamole 0.64\npretzel 0.69\ntable 0.54\ndog 0.71\nperson 0.85\nladybug 0.90\nperson 0.52\nzebra 0.83\nzebra 0.80\nzebra 0.55\nzebra 0.52\ndog 0.98\nhat with a wide brim 0.60\nperson 0.85\nperson 0.81\nperson 0.73\nelephant 1.00\nbird 0.99\nperson 0.58\ndog 0.98\ncart 1.00\nchair 0.79\nchair 0.64\nperson 0.91\nperson 0.87\nperson 0.57\nperson 0.52\ncomputer keyboard 0.52\ndog 0.97\ndog 0.92\nperson 0.77\nbird 0.94\nbutterfly 0.98\nperson 0.73\nperson 0.61\nbird 1.00\nbird 0.78\nperson 0.91\nperson 0.75\nstethoscope 0.83\nbird 0.83\nFigure 9: More randomly selected examples. See Figure 8 caption for details. Viewing digitally with zoom is recommended.\n17\nperson 0.81\nperson 0.57\nperson 0.53\nmotorcycle 0.64\nperson 0.73\nperson 0.51\nbagel 0.57\npineapple 1.00\nbowl 0.63\nguacamole 1.00\ntennis ball 0.60\nlemon 0.88\nlemon 0.86\nlemon 0.80\nlemon 0.78\norange 0.78\norange 0.73\norange 0.71\ngolf ball 1.00\ngolf ball 1.00\ngolf ball 0.89\ngolf ball 0.81\ngolf ball 0.79\ngolf ball 0.76golf ball 0.60\ngolf ball 0.60\ngolf ball 0.51\nlemon 0.53\nsoccer ball 0.67\nlamp 0.61\ntable 0.59\nbee 0.85\njellyfish 0.71\nbowl 0.54\nhamburger 0.78\ndumbbell 1.00\nperson 0.52\nmicrophone 1.00\nperson 0.85\nhead cabbage 0.83\nhead cabbage 0.75\ndog 0.74\ngoldfish 0.76\nperson 0.57\nguitar 1.00\nguitar 1.00\nguitar 0.88\ntable 0.63\ncomputer keyboard 0.78\nmicrowave 0.60\ntable 0.53\ntick 0.64\nlemon 0.80\ntennis ball 0.67\nrabbit 1.00\ndog 0.98\nperson 0.81\nperson 0.92\nsunglasses 0.52\nwatercraft 0.86\nmilk can 1.00\nmilk can 1.00\nbookshelf 0.50\nchair 0.86\ngiant panda 0.61\nperson 0.87\nantelope 0.74\ncattle 0.81\ndog 0.87\nhorse 0.78\npomegranate 1.00\nchair 0.86\ntv or monitor 0.52\nantelope 0.68\nbird 0.94\nsnake 0.60\ndog 0.98\ndog 0.88\nperson 0.79\nsnake 0.76\ntable 0.62\ntv or monitor 0.80\ntv or monitor 0.58\ntv or monitor 0.54\nlamp 0.86\nlamp 0.65\ntable 0.83\nmonkey 1.00\nmonkey 1.00\nmonkey 0.90\nmonkey 0.88\nmonkey 0.52\ndog 0.88\nfox 1.00\nfox 0.81\nperson 0.88\nwatercraft 0.91\nwatercraft 0.56\nbird 0.95\nbird 0.78\nisopod 0.56\nbird 0.69\nstarfish 0.67\ndragonfly 0.70\ndragonfly 0.60\nhamburger 0.72\nhamburger 0.60\ncup or mug 0.72\nelectric fan 1.00\nelectric fan 0.83\nelectric fan 0.78\nhelmet 0.64\nsoccer ball 0.63\nFigure 10: Curated examples. Each image was selected because we found it impressive, surprising, interesting, or amusing. Viewing\ndigitally with zoom is recommended.\n18\nobject detection. In CVPR, 2013. 6, 7\n[32] H. A. Rowley, S. Baluja, and T. Kanade. Neural network-\nbased face detection. TPAMI, 1998. 2\n[33] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learn-\ning internal representations by error propagation. Parallel\nDistributed Processing, 1:318\u2013362, 1986. 1\n[34] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,\nand Y. LeCun. OverFeat: Integrated Recognition, Localiza-\ntion and Detection using Convolutional Networks. In ICLR,\n2014. 1, 2, 4, 10\n[35] P. Sermanet, K. Kavukcuoglu, S. Chintala, and Y. LeCun.\nPedestrian detection with unsupervised multi-stage feature\nlearning. In CVPR, 2013. 2\n[36] H. Su, J. Deng, and L. Fei-Fei. Crowdsourcing annotations\nfor visual object detection. In AAAI Technical Report, 4th\nHuman Computation Workshop, 2012. 8\n[37] K. Sung and T. Poggio. Example-based learning for view-\nbased human face detection. Technical Report A.I. Memo\nNo. 1521, Massachussets Institute of Technology, 1994. 4\n[38] C. Szegedy, A. Toshev, and D. Erhan. Deep neural networks\nfor object detection. In NIPS, 2013. 2\n[39] J. Uijlings, K. van de Sande, T. Gevers, and A. Smeulders.\nSelective search for object recognition. IJCV, 2013. 1, 2, 3,\n4, 5, 9\n[40] R. Vaillant, C. Monrocq, and Y. LeCun. Original approach\nfor the localisation of objects in images. IEE Proc on Vision,\nImage, and Signal Processing, 1994. 2\n[41] X. Wang, M. Yang, S. Zhu, and Y. Lin. Regionlets for generic\nobject detection. In ICCV, 2013. 3, 5\n[42] M. Zeiler, G. Taylor, and R. Fergus. Adaptive deconvolu-\ntional networks for mid and high level feature learning. In\nCVPR, 2011. 4\n[43] K. Simonyan and A. Zisserman.\nVery Deep Convolu-\ntional Networks for Large-Scale Image Recognition. arXiv\npreprint, arXiv:1409.1556, 2014. 6, 7, 14\n19\nperson 0.82\nsnake 0.76\nfrog 0.78\nbird 0.79\ngoldfish 0.76\ngoldfish 0.76\ngoldfish 0.58\nperson 0.94\nstethoscope 0.56\nperson 0.95\nperson 0.92\nperson 0.67\nperson 0.60\ntable 0.81\njellyfish 0.67\nlemon 0.52\nperson 0.78\nperson 0.65\nwatercraft 0.55\nbaseball 1.00\nperson 0.94\nperson 0.82\nperson 0.80\nperson 0.61\nperson 0.55\nperson 0.52\ncomputer keyboard 0.81\ndog 0.60\nperson 0.88\nperson 0.79\nperson 0.68\nperson 0.59\ntv or monitor 0.82\nlizard 0.58\nchair 0.50\nperson 0.74\ntable 0.82\nperson 0.94\nperson 0.94\nperson 0.95\nperson 0.81\nperson 0.69\nrugby ball 0.91\nperson 0.84\nperson 0.59\nvolleyball 0.70\npineapple 1.00\nbrassiere 0.71\nperson 0.95\nperson 0.94\nperson 0.94\nperson 0.81\nperson 0.80\nperson 0.80\nperson 0.79\nperson 0.79\nperson 0.69\nperson 0.66\nperson 0.58\nperson 0.56\nperson 0.54\nswimming trunks 0.56\nbaseball 0.86\nhelmet 0.74\nperson 0.75\nminiskirt 0.64\nperson 0.92\nvacuum 1.00\ndog 0.98\ndog 0.93\nperson 0.94\nperson 0.75\nperson 0.65\nperson 0.53\nski 0.80\nski 0.80\nbird 0.55\ntiger 1.00\ntiger 0.67\ntiger 0.59\nbird 0.56\nwhale 1.00\nchair 0.53\nperson 0.92\nperson 0.92\nperson 0.82\nperson 0.78\nbowl 0.52\nstrawberry 0.79\nstrawberry 0.70\nburrito 0.54\ncroquet ball 0.91\ncroquet ball 0.91\ncroquet ball 0.91\ncroquet ball 0.91\nmushroom 0.57\nwatercraft 0.91\nwatercraft 0.87\nwatercraft 0.58\nplastic bag 0.62\nplastic bag 0.62\nwhale 0.88\ncar 0.70\ndog 0.94\ntv or monitor 0.57\ncart 0.80\nperson 0.79\nperson 0.53\nhat with a wide brim 0.89\nperson 0.88\nperson 0.82\nperson 0.79\nperson 0.56\nperson 0.54\ntraffic light 0.79\nbird 0.59\ncucumber 0.53\ncucumber 0.52\nantelope 1.00\nantelope 1.00\nantelope 0.94\nantelope 0.73\nantelope 0.63\nantelope 0.63\nfox 0.57\nbalance beam 0.50\nhorizontal bar 1.00\nperson 0.80\nperson 0.90\nsnake 0.64\ndog 0.98\ndog 0.97\nhelmet 0.69\nhorse 0.92\nhorse 0.69\nperson 0.82\nperson 0.72\norange 0.79\norange 0.71\norange 0.66\norange 0.66\norange 0.59\norange 0.56\nbird 0.97\nbird 0.96\nbird 0.96\nbird 0.94\nbird 0.89\nbird 0.64\nbird 0.56\nbird 0.53\nbird 0.52\nguitar 1.00\nperson 0.82\nbicycle 0.92\nperson 0.90\nperson 0.83\ncar 1.00\ncar 0.97\ndog 0.98\ndog 0.86\ndog 0.85\ndog 0.65\ndog 0.50\nperson 0.83\nperson 0.80\nperson 0.74\nperson 0.54\nelephant 0.60\nFigure 11: More curated examples. See Figure 10 caption for details. Viewing digitally with zoom is recommended.\n20\npool5 feature: (3,3,1) (top 1 \u2212 24)\n1.0\n0.9\n0.8\n0.8\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\npool5 feature: (3,3,2) (top 1 \u2212 24)\n1.0\n0.9\n0.9\n0.9\n0.9\n0.8\n0.8\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\npool5 feature: (3,3,3) (top 1 \u2212 24)\n0.9\n0.8\n0.8\n0.8\n0.8\n0.8\n0.8\n0.7\n0.7\n0.7\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\npool5 feature: (3,3,4) (top 1 \u2212 24)\n0.9\n0.8\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.6\n0.6\n0.6\n0.6\n0.6\npool5 feature: (3,3,5) (top 1 \u2212 24)\n0.9\n0.8\n0.8\n0.8\n0.8\n0.8\n0.8\n0.8\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\npool5 feature: (3,3,6) (top 1 \u2212 24)\n0.9\n0.8\n0.8\n0.8\n0.8\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\npool5 feature: (3,3,7) (top 1 \u2212 24)\n0.9\n0.8\n0.8\n0.8\n0.8\n0.8\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.6\n0.6\n0.6\n0.6\npool5 feature: (3,3,8) (top 1 \u2212 24)\n0.9\n0.8\n0.8\n0.8\n0.8\n0.8\n0.8\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\npool5 feature: (3,3,9) (top 1 \u2212 24)\n0.8\n0.8\n0.8\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\npool5 feature: (3,3,10) (top 1 \u2212 24)\n0.9\n0.8\n0.8\n0.7\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.5\n0.5\npool5 feature: (3,3,11) (top 1 \u2212 24)\n0.7\n0.7\n0.7\n0.7\n0.7\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\npool5 feature: (3,3,12) (top 1 \u2212 24)\n0.9\n0.8\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\npool5 feature: (3,3,13) (top 1 \u2212 24)\n0.9\n0.9\n0.8\n0.8\n0.8\n0.8\n0.8\n0.8\n0.8\n0.8\n0.8\n0.8\n0.8\n0.8\n0.8\n0.8\n0.8\n0.8\n0.8\n0.8\n0.8\n0.8\n0.8\n0.8\npool5 feature: (3,3,14) (top 1 \u2212 24)\n0.9\n0.9\n0.9\n0.8\n0.8\n0.8\n0.8\n0.8\n0.8\n0.8\n0.8\n0.8\n0.8\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\npool5 feature: (3,3,15) (top 1 \u2212 24)\n0.8\n0.8\n0.8\n0.8\n0.8\n0.8\n0.8\n0.8\n0.8\n0.8\n0.8\n0.8\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\npool5 feature: (3,3,16) (top 1 \u2212 24)\n0.9\n0.8\n0.8\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\npool5 feature: (3,3,17) (top 1 \u2212 24)\n0.9\n0.9\n0.8\n0.8\n0.8\n0.8\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\npool5 feature: (3,3,18) (top 1 \u2212 24)\n0.8\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\npool5 feature: (3,3,19) (top 1 \u2212 24)\n0.9\n0.8\n0.8\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\npool5 feature: (3,3,20) (top 1 \u2212 24)\n1.0\n0.9\n0.7\n0.7\n0.7\n0.7\n0.7\n0.7\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\nFigure 12: We show the 24 region proposals, out of the approximately 10 million regions in VOC 2007 test, that most strongly\nactivate each of 20 units. Each montage is labeled by the unit\u2019s (y, x, channel) position in the 6 \u00d7 6 \u00d7 256 dimensional pool5 feature map.\nEach image region is drawn with an overlay of the unit\u2019s receptive \ufb01eld in white. The activation value (which we normalize by dividing by\nthe max activation value over all units in a channel) is shown in the receptive \ufb01eld\u2019s upper-left corner. Best viewed digitally with zoom.\n21\n",
        "sentence": " , 2015b) or object detection (Girshick et al., 2014). , 2013) into the RCNN work of Girshick et al. (2014), making them the dominant paradigm for current object detection.",
        "context": "in how CNNs can be applied to object detection, leading to\ngreatly varying outcomes.\nIn Section 4, we give an overview of the ILSVRC2013\ndetection dataset and provide details about choices that we\nmade when running R-CNN on it.\nin object detectors. In ECCV. 2012. 2, 7, 8\n[24] Y. Jia.\nCaffe:\nAn open source convolutional archi-\ntecture for fast feature embedding.\nhttp://caffe.\nberkeleyvision.org/, 2013. 3\n[25] A. Krizhevsky, I. Sutskever, and G. Hinton. ImageNet clas-\nNo. 1521, Massachussets Institute of Technology, 1994. 4\n[38] C. Szegedy, A. Toshev, and D. Erhan. Deep neural networks\nfor object detection. In NIPS, 2013. 2\n[39] J. Uijlings, K. van de Sande, T. Gevers, and A. Smeulders."
    },
    {
        "title": "Pixel-wise deep learning for contour detection",
        "author": [
            "Hwang",
            "J.-J",
            "Liu",
            "T.-L"
        ],
        "venue": "In ICLR,",
        "citeRegEx": "Hwang et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "Hwang et al\\.",
        "year": 2015,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Visual boundary prediction: A deep neural prediction network and quality dissection",
        "author": [
            "Kivinen",
            "Jyri J",
            "Williams",
            "Christopher K. I",
            "Heess",
            "Nicolas"
        ],
        "venue": "In AISTATS,",
        "citeRegEx": "Kivinen et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Kivinen et al\\.",
        "year": 2014,
        "abstract": "",
        "full_text": "",
        "sentence": " Over the past three years Deep Convolutional Neural Networks (DCNNs) LeCun et al. (1998) have delivered compelling results in high-level vision tasks, such as image classification (Krizhevsky et al. Recent works (Bertasius et al., 2015; Kivinen et al., 2014; Hwang & Liu, 2015) have shown hat DCNNs yield substantial improvements over flat classifiers; the Holistic Edge Detection approach of Xie & Tu (2015) recently achieved dramatic improvements over the previous state-of-the-art, from an F-measure of 0. 803 DeepNets (Kivinen et al., 2014) 0.",
        "context": null
    },
    {
        "title": "Highly accurate boundary detection and grouping",
        "author": [
            "Kokkinos",
            "Iasonas"
        ],
        "venue": "In Proc. CVPR,",
        "citeRegEx": "Kokkinos and Iasonas.,? \\Q2010\\E",
        "shortCiteRegEx": "Kokkinos and Iasonas.",
        "year": 2010,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Statistical edge detection: Learning and evaluating edge cues",
        "author": [
            "S. Konishi",
            "A. Yuille",
            "J. Coughlan",
            "Zhu",
            "S.-C"
        ],
        "venue": "IEEE Trans. PAMI,",
        "citeRegEx": "Konishi et al\\.,? \\Q2003\\E",
        "shortCiteRegEx": "Konishi et al\\.",
        "year": 2003,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " As in all works following the introduction of human-annotated datasets (Konishi et al., 2003; Martin et al., 2004), e.",
        "context": null
    },
    {
        "title": "Efficient inference in fully connected crfs with gaussian edge potentials",
        "author": [
            "Kr\u00e4henb\u00fchl",
            "Philipp",
            "Koltun",
            "Vladlen"
        ],
        "venue": "In NIPS,",
        "citeRegEx": "Kr\u00e4henb\u00fchl et al\\.,? \\Q2011\\E",
        "shortCiteRegEx": "Kr\u00e4henb\u00fchl et al\\.",
        "year": 2011,
        "abstract": "Most state-of-the-art techniques for multi-class image segmentation and\nlabeling use conditional random fields defined over pixels or image regions.\nWhile region-level models often feature dense pairwise connectivity,\npixel-level models are considerably larger and have only permitted sparse graph\nstructures. In this paper, we consider fully connected CRF models defined on\nthe complete set of pixels in an image. The resulting graphs have billions of\nedges, making traditional inference algorithms impractical. Our main\ncontribution is a highly efficient approximate inference algorithm for fully\nconnected CRF models in which the pairwise edge potentials are defined by a\nlinear combination of Gaussian kernels. Our experiments demonstrate that dense\nconnectivity at the pixel level substantially improves segmentation and\nlabeling accuracy.",
        "full_text": "Ef\ufb01cient Inference in Fully Connected CRFs with\nGaussian Edge Potentials\nPhilipp Kr\u00a8ahenb\u00a8uhl\nComputer Science Department\nStanford University\nphilkr@cs.stanford.edu\nVladlen Koltun\nComputer Science Department\nStanford University\nvladlen@cs.stanford.edu\nAbstract\nMost state-of-the-art techniques for multi-class image segmentation and labeling\nuse conditional random \ufb01elds de\ufb01ned over pixels or image regions. While region-\nlevel models often feature dense pairwise connectivity, pixel-level models are con-\nsiderably larger and have only permitted sparse graph structures. In this paper, we\nconsider fully connected CRF models de\ufb01ned on the complete set of pixels in an\nimage. The resulting graphs have billions of edges, making traditional inference\nalgorithms impractical. Our main contribution is a highly ef\ufb01cient approximate\ninference algorithm for fully connected CRF models in which the pairwise edge\npotentials are de\ufb01ned by a linear combination of Gaussian kernels. Our experi-\nments demonstrate that dense connectivity at the pixel level substantially improves\nsegmentation and labeling accuracy.\n1\nIntroduction\nMulti-class image segmentation and labeling is one of the most challenging and actively studied\nproblems in computer vision. The goal is to label every pixel in the image with one of several prede-\ntermined object categories, thus concurrently performing recognition and segmentation of multiple\nobject classes. A common approach is to pose this problem as maximum a posteriori (MAP) infer-\nence in a conditional random \ufb01eld (CRF) de\ufb01ned over pixels or image patches [8, 12, 18, 19, 9].\nThe CRF potentials incorporate smoothness terms that maximize label agreement between similar\npixels, and can integrate more elaborate terms that model contextual relationships between object\nclasses.\nBasic CRF models are composed of unary potentials on individual pixels or image patches and pair-\nwise potentials on neighboring pixels or patches [19, 23, 7, 5]. The resulting adjacency CRF struc-\nture is limited in its ability to model long-range connections within the image and generally results\nin excessive smoothing of object boundaries. In order to improve segmentation and labeling accu-\nracy, researchers have expanded the basic CRF framework to incorporate hierarchical connectivity\nand higher-order potentials de\ufb01ned on image regions [8, 12, 9, 13]. However, the accuracy of these\napproaches is necessarily restricted by the accuracy of unsupervised image segmentation, which is\nused to compute the regions on which the model operates. This limits the ability of region-based\napproaches to produce accurate label assignments around complex object boundaries, although sig-\nni\ufb01cant progress has been made [9, 13, 14].\nIn this paper, we explore a different model structure for accurate semantic segmentation and labeling.\nWe use a fully connected CRF that establishes pairwise potentials on all pairs of pixels in the image.\nFully connected CRFs have been used for semantic image labeling in the past [18, 22, 6, 17], but the\ncomplexity of inference in fully connected models has restricted their application to sets of hundreds\nof image regions or fewer. The segmentation accuracy achieved by these approaches is again limited\nby the unsupervised segmentation that produces the regions. In contrast, our model connects all\n1\narXiv:1210.5644v1  [cs.CV]  20 Oct 2012\n(a) Image\n(b) Unary classi\ufb01ers\n(c) Robust P n CRF\n(d) Fully connected CRF,\nMCMC inference, 36 hrs\nsky\ntree\ngrass\nbench\ntree\nroad\ngrass\n(e) Fully connected CRF,\nour approach, 0.2 seconds\nFigure 1: Pixel-level classi\ufb01cation with a fully connected CRF. (a) Input image from the MSRC-21 dataset. (b)\nThe response of unary classi\ufb01ers used by our models. (c) Classi\ufb01cation produced by the Robust P n CRF [9].\n(d) Classi\ufb01cation produced by MCMC inference [17] in a fully connected pixel-level CRF model; the algorithm\nwas run for 36 hours and only partially converged for the bottom image. (e) Classi\ufb01cation produced by our\ninference algorithm in the fully connected model in 0.2 seconds.\npairs of individual pixels in the image, enabling greatly re\ufb01ned segmentation and labeling. The\nmain challenge is the size of the model, which has tens of thousands of nodes and billions of edges\neven on low-resolution images.\nOur main contribution is a highly ef\ufb01cient inference algorithm for fully connected CRF models in\nwhich the pairwise edge potentials are de\ufb01ned by a linear combination of Gaussian kernels in an ar-\nbitrary feature space. The algorithm is based on a mean \ufb01eld approximation to the CRF distribution.\nThis approximation is iteratively optimized through a series of message passing steps, each of which\nupdates a single variable by aggregating information from all other variables. We show that a mean\n\ufb01eld update of all variables in a fully connected CRF can be performed using Gaussian \ufb01ltering\nin feature space. This allows us to reduce the computational complexity of message passing from\nquadratic to linear in the number of variables by employing ef\ufb01cient approximate high-dimensional\n\ufb01ltering [16, 2, 1]. The resulting approximate inference algorithm is sublinear in the number of\nedges in the model.\nFigure 1 demonstrates the bene\ufb01ts of the presented algorithm on two images from the MSRC-21\ndataset for multi-class image segmentation and labeling. Figure 1(d) shows the results of approxi-\nmate MCMC inference in fully connected CRFs on these images [17]. The MCMC procedure was\nrun for 36 hours and only partially converged for the bottom image. We have also experimented with\ngraph cut inference in the fully connected models [11], but it did not converge within 72 hours. In\ncontrast, a single-threaded implementation of our algorithm produces a detailed pixel-level labeling\nin 0.2 seconds, as shown in Figure 1(e). A quantitative evaluation on the MSRC-21 and the PAS-\nCAL VOC 2010 datasets is provided in Section 6. To the best of our knowledge, we are the \ufb01rst to\ndemonstrate ef\ufb01cient inference in fully connected CRF models at the pixel level.\n2\nThe Fully Connected CRF Model\nConsider a random \ufb01eld X de\ufb01ned over a set of variables {X1, . . . , XN}. The domain of each\nvariable is a set of labels L = {l1, l2, . . . , lk}. Consider also a random \ufb01eld I de\ufb01ned over variables\n{I1, . . . , IN}. In our setting, I ranges over possible input images of size N and X ranges over\npossible pixel-level image labelings. Ij is the color vector of pixel j and Xj is the label assigned to\npixel j.\nA\nconditional\nrandom\n\ufb01eld\n(I, X)\nis\ncharacterized\nby\na\nGibbs\ndistribution\nP(X|I) =\n1\nZ(I) exp(\u2212P\nc\u2208CG \u03c6c(Xc|I)), where G = (V, E) is a graph on X and each clique c\n2\nin a set of cliques CG in G induces a potential \u03c6c [15]. The Gibbs energy of a labeling x \u2208LN\nis E(x|I) = P\nc\u2208CG \u03c6c(xc|I). The maximum a posteriori (MAP) labeling of the random \ufb01eld is\nx\u2217= arg maxx\u2208LN P(x|I). For notational convenience we will omit the conditioning in the rest of\nthe paper and use \u03c8c(xc) to denote \u03c6c(xc|I).\nIn the fully connected pairwise CRF model, G is the complete graph on X and CG is the set of all\nunary and pairwise cliques. The corresponding Gibbs energy is\nE(x) =\nX\ni\n\u03c8u(xi) +\nX\ni<j\n\u03c8p(xi, xj),\n(1)\nwhere i and j range from 1 to N. The unary potential \u03c8u(xi) is computed independently for each\npixel by a classi\ufb01er that produces a distribution over the label assignment xi given image features.\nThe unary potential used in our implementation incorporates shape, texture, location, and color\ndescriptors and is described in Section 5. Since the output of the unary classi\ufb01er for each pixel\nis produced independently from the outputs of the classi\ufb01ers for other pixels, the MAP labeling\nproduced by the unary classi\ufb01ers alone is generally noisy and inconsistent, as shown in Figure 1(b).\nThe pairwise potentials in our model have the form\n\u03c8p(xi, xj) = \u00b5(xi, xj) PK\nm=1 w(m)k(m)(fi, fj)\n|\n{z\n}\nk(fi,fj)\n,\n(2)\nwhere each k(m) is a Gaussian kernel k(m)(fi, fj) = exp(\u22121\n2(fi \u2212fj)T\u039b(m)(fi \u2212fj)), the vectors fi\nand fj are feature vectors for pixels i and j in an arbitrary feature space, w(m) are linear combination\nweights, and \u00b5 is a label compatibility function. Each kernel k(m) is characterized by a symmetric,\npositive-de\ufb01nite precision matrix \u039b(m), which de\ufb01nes its shape.\nFor multi-class image segmentation and labeling we use contrast-sensitive two-kernel potentials,\nde\ufb01ned in terms of the color vectors Ii and Ij and positions pi and pj:\nk(fi, fj) = w(1) exp\n \n\u2212|pi \u2212pj|2\n2\u03b82\u03b1\n\u2212|Ii \u2212Ij|2\n2\u03b82\n\u03b2\n!\n|\n{z\n}\nappearance kernel\n+w(2) exp\n\u0012\n\u2212|pi \u2212pj|2\n2\u03b82\u03b3\n\u0013\n|\n{z\n}\nsmoothness kernel\n.\n(3)\nThe appearance kernel is inspired by the observation that nearby pixels with similar color are likely\nto be in the same class. The degrees of nearness and similarity are controlled by parameters \u03b8\u03b1 and\n\u03b8\u03b2. The smoothness kernel removes small isolated regions [19]. The parameters are learned from\ndata, as described in Section 4.\nA simple label compatibility function \u00b5 is given by the Potts model, \u00b5(xi, xj) = [xi \u0338= xj]. It\nintroduces a penalty for nearby similar pixels that are assigned different labels. While this simple\nmodel works well in practice, it is insensitive to compatibility between labels. For example, it\npenalizes a pair of nearby pixels labeled \u201csky\u201d and \u201cbird\u201d to the same extent as pixels labeled \u201csky\u201d\nand \u201ccat\u201d. We can instead learn a general symmetric compatibility function \u00b5(xi, xj) that takes\ninteractions between labels into account, as described in Section 4.\n3\nEf\ufb01cient Inference in Fully Connected CRFs\nOur algorithm is based on a mean \ufb01eld approximation to the CRF distribution. This approxima-\ntion yields an iterative message passing algorithm for approximate inference. Our key observation\nis that message passing in the presented model can be performed using Gaussian \ufb01ltering in fea-\nture space. This enables us to utilize highly ef\ufb01cient approximations for high-dimensional \ufb01ltering,\nwhich reduce the complexity of message passing from quadratic to linear, resulting in an approxi-\nmate inference algorithm for fully connected CRFs that is linear in the number of variables N and\nsublinear in the number of edges in the model.\n3.1\nMean Field Approximation\nInstead of computing the exact distribution P(X), the mean \ufb01eld approximation computes a dis-\ntribution Q(X) that minimizes the KL-divergence D(Q\u2225P) among all distributions Q that can be\nexpressed as a product of independent marginals, Q(X) = Q\ni Qi(Xi) [10].\n3\nMinimizing the KL-divergence, while constraining Q(X) and Qi(Xi) to be valid distributions,\nyields the following iterative update equation:\nQi(xi = l) = 1\nZi\nexp\n\uf8f1\n\uf8f2\n\uf8f3\u2212\u03c8u(xi) \u2212\nX\nl\u2032\u2208L\n\u00b5(l, l\u2032)\nK\nX\nm=1\nw(m) X\nj\u0338=i\nk(m)(fi, fj)Qj(l\u2032)\n\uf8fc\n\uf8fd\n\uf8fe.\n(4)\nA detailed derivation of Equation 4 is given in the supplementary material. This update equation\nleads to the following inference algorithm:\nAlgorithm 1 Mean \ufb01eld in fully connected CRFs\nInitialize Q\n\u25b7Qi(xi) \u2190\n1\nZi exp{\u2212\u03c6u(xi)}\nwhile not converged do\n\u25b7See Section 6 for convergence analysis\n\u02dcQ(m)\ni\n(l) \u2190P\nj\u0338=i k(m)(fi, fj)Qj(l) for all m\n\u25b7Message passing from all Xj to all Xi\n\u02c6Qi(xi) \u2190P\nl\u2208L \u00b5(m)(xi, l) P\nm w(m) \u02dcQ(m)\ni\n(l)\n\u25b7Compatibility transform\nQi(xi) \u2190exp{\u2212\u03c8u(xi) \u2212\u02c6Qi(xi)}\n\u25b7Local update\nnormalize Qi(xi)\nend while\nEach iteration of Algorithm 1 performs a message passing step, a compatibility transform, and a\nlocal update. Both the compatibility transform and the local update run in linear time and are highly\nef\ufb01cient. The computational bottleneck is message passing. For each variable, this step requires\nevaluating a sum over all other variables. A naive implementation thus has quadratic complexity in\nthe number of variables N. Next, we show how approximate high-dimensional \ufb01ltering can be used\nto reduce the computational cost of message passing to linear.\n3.2\nEf\ufb01cient Message Passing Using High-Dimensional Filtering\nFrom a signal processing standpoint, the message passing step can be expressed as a convolution\nwith a Gaussian kernel G\u039b(m) in feature space:\n\u02dcQ(m)\ni\n(l) = P\nj\u2208V k(m)(fi, fj)Qj(l) \u2212Qi(l)\n|\n{z\n}\nmessage passing\n= [G\u039b(m) \u2297Q(l)] (fi)\n|\n{z\n}\nQ\n(m)\ni\n(l)\n\u2212Qi(l).\n(5)\nWe subtract Qi(l) from the convolved function Q\n(m)\ni\n(l) because the convolution sums over all vari-\nables, while message passing does not sum over Qi.\nThis convolution performs a low-pass \ufb01lter, essentially band-limiting Q\n(m)\ni\n(l). By the sampling\ntheorem, this function can be reconstructed from a set of samples whose spacing is proportional\nto the standard deviation of the \ufb01lter [20]. We can thus perform the convolution by downsampling\nQ(l), convolving the samples with G\u039b(m), and upsampling the result at the feature points [16].\nAlgorithm 2 Ef\ufb01cient message passing: Q\n(m)\ni\n(l) = P\nj\u2208V k(m)(fi, fj)Qj(l)\nQ\u2193(l) \u2190downsample(Q(l))\n\u25b7Downsample\n\u2200i\u2208V\u2193Q\n(m)\n\u2193i (l) \u2190P\nj\u2208V\u2193k(m)(f\u2193i, f\u2193j)Q\u2193j(l)\n\u25b7Convolution on samples f\u2193\nQ\n(m)(l) \u2190upsample(Q\n(m)\n\u2193\n(l))\n\u25b7Upsample\nA common approximation to the Gaussian kernel is a truncated Gaussian, where all values beyond\ntwo standard deviations are set to zero. Since the spacing of the samples is proportional to the stan-\ndard deviation, the support of the truncated kernel contains only a constant number of sample points.\nThus the convolution can be approximately computed at each sample by aggregating values from\nonly a constant number of neighboring samples. This implies that approximate message passing can\nbe performed in O(N) time [16].\nHigh-dimensional \ufb01ltering algorithms that follow this approach can still have computational com-\nplexity exponential in d. However, a clever \ufb01ltering scheme can reduce the complexity of the con-\nvolution operation to O(Nd). We use the permutohedral lattice, a highly ef\ufb01cient convolution data\n4\nstructure that tiles the feature space with simplices arranged along d+1 axes [1]. The permutohedral\nlattice exploits the separability of unit variance Gaussian kernels. Thus we need to apply a whitening\ntransform \u02dcf = Uf to the feature space in order to use it. The whitening transformation is found us-\ning the Cholesky decomposition of \u039b(m) into UU T. In the transformed space, the high-dimensional\nconvolution can be separated into a sequence of one-dimensional convolutions along the axes of the\nlattice. The resulting approximate message passing procedure is highly ef\ufb01cient even with a fully\nsequential implementation that does not make use of parallelism or the streaming capabilities of\ngraphics hardware, which can provide further acceleration if desired.\n4\nLearning\nWe learn the parameters of the model by piecewise training. First, the boosted unary classi\ufb01ers are\ntrained using the JointBoost algorithm [21], using the features described in Section 5. Next we learn\nthe appearance kernel parameters w(1), \u03b8\u03b1, and \u03b8\u03b2 for the Potts model. w(1) can be found ef\ufb01ciently\nby a combination of expectation maximization and high-dimensional \ufb01ltering. Unfortunately, the\nkernel widths \u03b8\u03b1 and \u03b8\u03b2 cannot be computed effectively with this approach, since their gradient\ninvolves a sum of non-Gaussian kernels, which are not amenable to the same acceleration techniques.\nWe found it to be more ef\ufb01cient to use grid search on a holdout validation set for all three kernel\nparameters w(1), \u03b8\u03b1 and \u03b8\u03b2.\nThe smoothness kernel parameters w(2) and \u03b8\u03b3 do not signi\ufb01cantly affect classi\ufb01cation accuracy,\nbut yield a small visual improvement. We found w(2) = \u03b8\u03b3 = 1 to work well in practice.\nThe compatibility parameters \u00b5(a, b) = \u00b5(b, a) are learned using L-BFGS to maximize the log-\nlikelihood \u2113(\u00b5 : I, T ) of the model for a validation set of images I with corresponding ground\ntruth labelings T . L-BFGS requires the computation of the gradient of \u2113, which is intractable to\nestimate exactly, since it requires computing the gradient of the partition function Z. Instead, we\nuse the mean \ufb01eld approximation described in Section 3 to estimate the gradient of Z. This leads to\na simple approximation of the gradient for each training image:\n\u2202\n\u2202\u00b5(a, b)\u2113(\u00b5 : I(n), T (n)) \u2248\u2212\nX\ni\nT (n)\ni\n(a)\nX\nj\u0338=i\nk(fi, fj)T (n)\nj\n(b) +\nX\ni\nQi(a)\nX\nj\u0338=i\nk(fi, fj)Qi(b),\n(6)\nwhere (I(n), T (n)) is a single training image with its ground truth labeling and T (n)(a) is a binary\nimage in which the ith pixel T (n)\ni\n(a) has value 1 if the ground truth label at the ith pixel of T (n) is\na and 0 otherwise. A detailed derivation of Equation 6 is given in the supplementary material.\nThe sums P\nj\u0338=i k(fi, fj)Tj(b) and P\nj\u0338=i k(fi, fj)Qi(b) are both computationally expensive to eval-\nuate directly. As in Section 3.2, we use high-dimensional \ufb01ltering to compute both sums ef\ufb01ciently.\nThe runtime of the \ufb01nal learning algorithm is linear in the number of variables N.\n5\nImplementation\nThe unary potentials used in our implementation are derived from TextonBoost [19, 13]. We use\nthe 17-dimensional \ufb01lter bank suggested by Shotton et al. [19], and follow Ladick\u00b4y et al. [13] by\nadding color, histogram of oriented gradients (HOG), and pixel location features. Our evaluation\non the MSRC-21 dataset uses this extended version of TextonBoost for the unary potentials. For\nthe VOC 2010 dataset we include the response of bounding box object detectors [4] for each object\nclass as 20 additional features. This increases the performance of the unary classi\ufb01ers on the VOC\n2010 from 13% to 22%. We gain an additional 5% by training a logistic regression classi\ufb01er on the\nresponses of the boosted classi\ufb01er.\nFor ef\ufb01cient high-dimensional \ufb01ltering, we use a publicly available implementation of the permuto-\nhedral lattice [1]. We found a downsampling rate of one standard deviation to work best for all our\nexperiments. Sampling-based \ufb01ltering algorithms underestimate the edge strength k(fi, fj) for very\nsimilar feature points. Proper normalization can cancel out most of this error. The permutohedral\nlattice allows for two types of normalizations. A global normalization by the average kernel strength\n5\n\u02c6k = 1\nN\nP\ni,j k(fi, fj) can correct for constant error. A pixelwise normalization by \u02c6ki = P\nj k(fi, fj)\nhandles regional errors as well, but violates the CRF symmetry assumption \u03c8p(xi, xj) = \u03c8p(xj, xi).\nWe found the pixelwise normalization to work better in practice.\n6\nEvaluation\nWe evaluate the presented algorithm on two standard benchmarks for multi-class image segmen-\ntation and labeling. The \ufb01rst is the MSRC-21 dataset, which consists of 591 color images of size\n320 \u00d7 213 with corresponding ground truth labelings of 21 object classes [19]. The second is the\nPASCAL VOC 2010 dataset, which contains 1928 color images of size approximately 500 \u00d7 400,\nwith a total of 20 object classes and one background class [3]. The presented approach was evalu-\nated alongside the adjacency (grid) CRF of Shotton et al. [19] and the Robust P n CRF of Kohli et\nal. [9], using publicly available reference implementations. To ensure a fair comparison, all models\nused the unary potentials described in Section 5. All experiments were conducted on an Intel i7-930\nprocessor clocked at 2.80GHz. Eight CPU cores were used for training; all other experiments were\nperformed on a single core. The inference algorithm was implemented in a single CPU thread.\nConvergence.\nWe \ufb01rst evaluate the convergence of the mean \ufb01eld approximation by analyzing\nthe KL-divergence between Q and P. Figure 2 shows the KL-divergence between Q and P over\nsuccessive iterations of the inference algorithm. The KL-divergence was estimated up to a constant\nas described in supplementary material. Results are shown for different standard deviations \u03b8\u03b1 and\n\u03b8\u03b2 of the kernels. The graphs were aligned at 20 iterations for visual comparison. The number of\niterations was set to 10 in all subsequent experiments.\nMSRC-21 dataset.\nWe use the standard split of the dataset into 45% training, 10% validation and\n45% test images [19]. The unary potentials were learned on the training set, while the parameters of\nall CRF models were learned using holdout validation. The total CRF training time was 40 minutes.\nThe learned label compatibility function performed on par with the Potts model on this dataset.\nFigure 3 provides qualitative and quantitative results on the dataset. We report the standard measures\nof multi-class segmentation accuracy: \u201cglobal\u201d denotes the overall percentage of correctly classi\ufb01ed\nimage pixels and \u201caverage\u201d is the unweighted average of per-category classi\ufb01cation accuracy [19, 9].\nThe presented inference algorithm on the fully connected CRF signi\ufb01cantly outperforms the other\nmodels, evaluated against the standard ground truth data provided with the dataset.\nThe ground truth labelings provided with the MSRC-21 dataset are quite imprecise. In particular,\nregions around object boundaries are often left unlabeled. This makes it dif\ufb01cult to quantitatively\nevaluate the performance of algorithms that strive for pixel-level accuracy. Following Kohli et al. [9],\nwe manually produced accurate segmentations and labelings for a set of images from the MSRC-21\ndataset. Each image was fully annotated at the pixel level, with careful labeling around complex\nboundaries. This labeling was performed by hand for 94 representative images from the MSRC-\n21 dataset. Labeling a single image took 30 minutes on average. A number of images from this\n\u201caccurate ground truth\u201d set are shown in Figure 3. Figure 3 reports segmentation accuracy against\nthis ground truth data alongside the evaluation against the standard ground truth. The results were\nobtained using 5-fold cross validation, where 4\n5 of the 94 images were used to train the CRF pa-\n 0\n 5\n 10\n 15\n 20\nKL-divergence\nNumber of iterations\n  \u03b8\u03b1=\u03b8\u03b2=10\n  \u03b8\u03b1=\u03b8\u03b2=30\n  \u03b8\u03b1=\u03b8\u03b2=50\n  \u03b8\u03b1=\u03b8\u03b2=70\n  \u03b8\u03b1=\u03b8\u03b2=90\n(a) KL-divergence\nImage\nQ(sky)\nQ(bird)\n0 iterations\n1 iteration\n2 iterations\n10 iterations\n(b) Distributions Q(Xi =\u201cbird\u201d) (top) and Q(Xi =\u201csky\u201d) (bottom)\nFigure 2: Convergence analysis. (a) KL-divergence of the mean \ufb01eld approximation during successive itera-\ntions of the inference algorithm, averaged across 94 images from the MSRC-21 dataset. (b) Visualization of\nconvergence on distributions for two class labels over an image from the dataset.\n6\nImage\nGrid CRF\nRobust Pn CRF\nOur approach\nAccurate ground truth\nbird\nwater\nroad\ncar\nsky\ntree\nbuilding\ngrass\ncow\nsky\ntree\ngrass\ngrass\nwater\nbird\nRuntime\nStandard ground truth\nAccurate ground truth\nGlobal\nAverage\nGlobal\nAverage\nUnary classi\ufb01ers\n\u2212\n84.0\n76.6\n83.2 \u00b1 1.5\n80.6 \u00b1 2.3\nGrid CRF\n1s\n84.6\n77.2\n84.8 \u00b1 1.5\n82.4 \u00b1 1.8\nRobust P n CRF\n30s\n84.9\n77.5\n86.5 \u00b1 1.0\n83.1 \u00b1 1.5\nFully connected CRF\n0.2s\n86.0\n78.3\n88.2 \u00b1 0.7\n84.7 \u00b1 0.7\nFigure 3: Qualitative and quantitative results on the MSRC-21 dataset.\nrameters. The unary potentials were learned on a separate training set that did not include the 94\naccurately annotated images.\nWe also adopt the methodology proposed by Kohli et al. [9] for evaluating segmentation accuracy\naround boundaries. Speci\ufb01cally, we count the relative number of misclassi\ufb01ed pixels within a nar-\nrow band (\u201ctrimap\u201d) surrounding actual object boundaries, obtained from the accurate ground truth\nimages. As shown in Figure 4, our algorithm outperforms previous work across all trimap widths.\nPASCAL VOC 2010.\nDue to the lack of a publicly available ground truth labeling for the test\nset in the PASCAL VOC 2010, we use the training and validation data for all our experiments. We\nrandomly partitioned the images into 3 groups: 40% training, 15% validation, and 45% test set. Seg-\nmentation accuracy was measured using the standard VOC measure [3]. The unary potentials were\nlearned on the training set and yielded an average classi\ufb01cation accuracy of 27.6%. The parameters\nfor the Potts potentials in the fully connected CRF model were learned on the validation set. The\nImage\nGround truth\nTrimap (4px)\nTrimap (8px)\n(a) Trimaps of different widths\n 20\n 30\n 40\n 50\n 0\n 4\n 8\n 12\n 16\n 20\nPixelwise Classifiaction Error [%]\nTrimap Width [Pixels]\nUnary classifiers\nGrid CRF\nRobust Pn CRF\nFully connected CRF\n(b) Segmentation accuracy within trimap\nFigure 4: Segmentation accuracy around object boundaries. (a) Visualization of the \u201ctrimap\u201d measure. (b)\nPercent of misclassi\ufb01ed pixels within trimaps of different widths.\n7\nImage\nGround truth\ncat\nbackground\nOur approach\nGround truth\nboat\nbackground\nsheep\nbackground\nOur approach\nImage\nFigure 5: Qualitative results on the PASCAL VOC 2010 dataset. Average segmentation accuracy was 30.2%.\nfully connected model with Potts potentials yielded an average classi\ufb01cation accuracy of 29.1%.\nThe label compatibility function, learned on the validation set, further increased the classi\ufb01cation\naccuracy to 30.2%. For comparison, the grid CRF achieves 28.3%. Training time was 2.5 hours and\ninference time is 0.5 seconds. Qualitative results are provided in Figure 5.\nLong-range connections.\nWe have examined the value of long-range connections in our model by\nvarying the spatial and color ranges \u03b8\u03b1 and \u03b8\u03b2 of the appearance kernel and analyzing the resulting\nclassi\ufb01cation accuracy. For this experiment, w(1) was held constant and w(2) was set to 0. The\nresults are shown in Figure 6. Accuracy steadily increases as longer-range connections are added,\npeaking at spatial standard deviation of \u03b8\u03b1 = 61 pixels and color standard deviation \u03b8\u03b2 = 11. At this\nsetting, more than 50% of the pairwise potential energy in the model was assigned to edges of length\n35 pixels or higher. However, long-range connections can also propagate misleading information,\nas shown in Figure 7.\n0\n100\n200\n\u03b8\u03b1\n25\n50\n\u03b8\u03b2\n82%\n84%\n86%\n88%\n(a) Quantitative\n\u03b8\u03b1\n1.0\n121.0\n\u03b8\u03b2\n1.0\n41.0\n(b) Qualitative\nFigure 6: In\ufb02uence of long-range connections on classi\ufb01cation accuracy. (a) Global classi\ufb01cation accuracy on\nthe 94 MSRC images with accurate ground truth, as a function of kernel parameters \u03b8\u03b1 and \u03b8\u03b2. (b) Results for\none image across two slices in parameter space, shown as black lines in (a).\nDiscussion.\nWe have presented a highly ef\ufb01cient approximate inference algorithm for fully con-\nnected CRF models. Our results demonstrate that dense pixel-level connectivity leads to signif-\nicantly more accurate pixel-level classi\ufb01cation performance. Our single-threaded implementation\nprocesses benchmark images in a fraction of a second and the algorithm can be parallelized for\nfurther performance gains.\nAcknowledgements.\nPhilipp Kr\u00a8ahenb\u00a8uhl was supported in part by a Stanford Graduate Fellow-\nship. We are grateful to Daphne Koller, Andrew Adams and Jongmin Baek for helpful discussions.\nSergey Levine and Vangelis Kalogerakis provided comments on a draft of this paper.\nImage\nOur approach\nGround truth\nImage\nOur approach\nGround truth\nbird\nbackground\nvoid\nroad\ncat\nFigure 7: Failure cases on images from the PASCAL VOC 2010 (left) and the MSRC-21 (right). Long-range\nconnections propagated misleading information, eroding the bird wing in the left image and corrupting the legs\nof the cat on the right.\n8\nReferences\n[1] A. Adams, J. Baek, and M. A. Davis. Fast high-dimensional \ufb01ltering using the permutohedral lattice.\nComputer Graphics Forum, 29(2), 2010. 2, 5\n[2] A. Adams, N. Gelfand, J. Dolson, and M. Levoy. Gaussian kd-trees for fast high-dimensional \ufb01ltering.\nACM Transactions on Graphics, 28(3), 2009. 2\n[3] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The PASCAL Visual Object\nClasses (VOC) challenge. IJCV, 88(2), 2010. 6, 7\n[4] P. F. Felzenszwalb, R. B. Girshick, and D. A. McAllester. Cascade object detection with deformable part\nmodels. In Proc. CVPR, 2010. 5\n[5] B. Fulkerson, A. Vedaldi, and S. Soatto. Class segmentation and object localization with superpixel\nneighborhoods. In Proc. ICCV, 2009. 1\n[6] C. Galleguillos, A. Rabinovich, and S. Belongie. Object categorization using co-occurrence, location and\nappearance. In Proc. CVPR, 2008. 1\n[7] S. Gould, J. Rodgers, D. Cohen, G. Elidan, and D. Koller. Multi-class segmentation with relative location\nprior. IJCV, 80(3), 2008. 1\n[8] X. He, R. S. Zemel, and M. A. Carreira-Perpinan. Multiscale conditional random \ufb01elds for image labeling.\nIn Proc. CVPR, 2004. 1\n[9] P. Kohli, L. Ladick\u00b4y, and P. H. S. Torr. Robust higher order potentials for enforcing label consistency.\nIJCV, 82(3), 2009. 1, 2, 6, 7\n[10] D. Koller and N. Friedman. Probabilistic Graphical Models: Principles and Techniques. MIT Press,\n2009. 3\n[11] V. Kolmogorov and R. Zabih. What energy functions can be minimized via graph cuts? PAMI, 26(2),\n2004. 2\n[12] S. Kumar and M. Hebert. A hierarchical \ufb01eld framework for uni\ufb01ed context-based classi\ufb01cation. In Proc.\nICCV, 2005. 1\n[13] L. Ladick\u00b4y, C. Russell, P. Kohli, and P. H. S. Torr. Associative hierarchical crfs for object class image\nsegmentation. In Proc. ICCV, 2009. 1, 5\n[14] L. Ladick\u00b4y, C. Russell, P. Kohli, and P. H. S. Torr. Graph cut based inference with co-occurrence statistics.\nIn Proc. ECCV, 2010. 1\n[15] J. D. Lafferty, A. McCallum, and F. C. N. Pereira. Conditional random \ufb01elds: Probabilistic models for\nsegmenting and labeling sequence data. In Proc. ICML, 2001. 3\n[16] S. Paris and F. Durand. A fast approximation of the bilateral \ufb01lter using a signal processing approach.\nIJCV, 81(1), 2009. 2, 4\n[17] N. Payet and S. Todorovic. (RF)2 \u2013 random forest random \ufb01eld. In Proc. NIPS. 2010. 1, 2\n[18] A. Rabinovich, A. Vedaldi, C. Galleguillos, E. Wiewiora, and S. Belongie. Objects in context. In Proc.\nICCV, 2007. 1\n[19] J. Shotton, J. M. Winn, C. Rother, and A. Criminisi. Textonboost for image understanding: Multi-class\nobject recognition and segmentation by jointly modeling texture, layout, and context. IJCV, 81(1), 2009.\n1, 3, 5, 6\n[20] S. W. Smith. The scientist and engineer\u2019s guide to digital signal processing. California Technical Pub-\nlishing, 1997. 4\n[21] A. Torralba, K. P. Murphy, and W. T. Freeman. Sharing visual features for multiclass and multiview object\ndetection. PAMI, 29(5), 2007. 5\n[22] T. Toyoda and O. Hasegawa. Random \ufb01eld model for integration of local information and global infor-\nmation. PAMI, 30, 2008. 1\n[23] J. J. Verbeek and B. Triggs. Scene segmentation with crfs learned from partially labeled images. In Proc.\nNIPS, 2007. 1\n9\n",
        "sentence": "",
        "context": "Sergey Levine and Vangelis Kalogerakis provided comments on a draft of this paper.\nImage\nOur approach\nGround truth\nImage\nOur approach\nGround truth\nbird\nbackground\nvoid\nroad\ncat\nAcknowledgements.\nPhilipp Kr\u00a8ahenb\u00a8uhl was supported in part by a Stanford Graduate Fellow-\nship. We are grateful to Daphne Koller, Andrew Adams and Jongmin Baek for helpful discussions.\nlishing, 1997. 4\n[21] A. Torralba, K. P. Murphy, and W. T. Freeman. Sharing visual features for multiclass and multiview object\ndetection. PAMI, 29(5), 2007. 5"
    },
    {
        "title": "Learning to propose objects",
        "author": [
            "Kr\u00e4henb\u00fchl",
            "Philipp",
            "Koltun",
            "Vladlen"
        ],
        "venue": "In Proc. CVPR,",
        "citeRegEx": "Kr\u00e4henb\u00fchl et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "Kr\u00e4henb\u00fchl et al\\.",
        "year": 2015,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Imagenet classification with deep convolutional neural networks",
        "author": [
            "A. Krizhevsky",
            "I. Sutskever",
            "G.E. Hinton"
        ],
        "venue": "In NIPS,",
        "citeRegEx": "Krizhevsky et al\\.,? \\Q2013\\E",
        "shortCiteRegEx": "Krizhevsky et al\\.",
        "year": 2013,
        "abstract": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called \"dropout\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.",
        "full_text": "",
        "sentence": " (1998) have delivered compelling results in high-level vision tasks, such as image classification (Krizhevsky et al., 2013; Sermanet et al., 2013; Simonyan & Zisserman, 2014; Szegedy et al., 2014; Papandreou et al., 2015b) or object detection (Girshick et al.",
        "context": null
    },
    {
        "title": "Gradient-based learning applied to document recognition",
        "author": [
            "Y. LeCun",
            "L. Bottou",
            "Y. Bengio",
            "P. Haffner"
        ],
        "venue": "In Proc. IEEE,",
        "citeRegEx": "LeCun et al\\.,? \\Q1998\\E",
        "shortCiteRegEx": "LeCun et al\\.",
        "year": 1998,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Efficient piecewise training of deep structured models for semantic segmentation",
        "author": [
            "Lin",
            "Guosheng",
            "Shen",
            "Chunhua",
            "Reid",
            "Ian"
        ],
        "venue": "arXiv preprint arXiv:1504.01013,",
        "citeRegEx": "Lin et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "Lin et al\\.",
        "year": 2015,
        "abstract": "Recent advances in semantic image segmentation have mostly been achieved by\ntraining deep convolutional neural networks (CNNs). We show how to improve\nsemantic segmentation through the use of contextual information; specifically,\nwe explore `patch-patch' context between image regions, and `patch-background'\ncontext. For learning from the patch-patch context, we formulate Conditional\nRandom Fields (CRFs) with CNN-based pairwise potential functions to capture\nsemantic correlations between neighboring patches. Efficient piecewise training\nof the proposed deep structured model is then applied to avoid repeated\nexpensive CRF inference for back propagation. For capturing the\npatch-background context, we show that a network design with traditional\nmulti-scale image input and sliding pyramid pooling is effective for improving\nperformance. Our experimental results set new state-of-the-art performance on a\nnumber of popular semantic segmentation datasets, including NYUDv2, PASCAL VOC\n2012, PASCAL-Context, and SIFT-flow. In particular, we achieve an\nintersection-over-union score of 78.0 on the challenging PASCAL VOC 2012\ndataset.",
        "full_text": "Ef\ufb01cient Piecewise Training of Deep Structured Models for Semantic\nSegmentation\nGuosheng Lin, Chunhua Shen, Anton van den Hengel, Ian Reid\nThe University of Adelaide; and Australian Centre for Robotic Vision\nAbstract\nRecent advances in semantic image segmentation have\nmostly been achieved by training deep convolutional neural\nnetworks (CNNs). We show how to improve semantic seg-\nmentation through the use of contextual information; specif-\nically, we explore \u2018patch-patch\u2019 context between image re-\ngions, and \u2018patch-background\u2019 context. For learning from\nthe patch-patch context, we formulate Conditional Random\nFields (CRFs) with CNN-based pairwise potential func-\ntions to capture semantic correlations between neighboring\npatches. Ef\ufb01cient piecewise training of the proposed deep\nstructured model is then applied to avoid repeated expen-\nsive CRF inference for back propagation. For capturing the\npatch-background context, we show that a network design\nwith traditional multi-scale image input and sliding pyra-\nmid pooling is effective for improving performance. Our ex-\nperimental results set new state-of-the-art performance on a\nnumber of popular semantic segmentation datasets, includ-\ning NYUDv2, PASCAL VOC 2012, PASCAL-Context, and\nSIFT-\ufb02ow. In particular, we achieve an intersection-over-\nunion score of 78.0 on the challenging PASCAL VOC 2012\ndataset.\n1. Introduction\nSemantic image segmentation aims to predict a category\nlabel for every image pixel, which is an important yet chal-\nlenging task for image understanding. Recent approaches\nhave applied convolutional neural network (CNNs) [13, 32,\n3] to this pixel-level labeling task and achieved remarkable\nsuccess. Among these CNN-based methods, fully convo-\nlutional neural networks (FCNNs) [32, 3] have become a\npopular choice, because of their computational ef\ufb01ciency\nfor dense prediction and end-to-end style learning.\nContextual relationships are ubiquitous and provide im-\nportant cues for scene understanding tasks. Spatial context\ncan be formulated in terms of semantic compatibility re-\nlations between one object and its neighboring objects or\nimage patches (stuff), in which a compatibility relation is\nan indication of the co-occurrence of visual patterns. For\n...\nPairwise potential net\nMulti-scale CNN\n...\nUnary potential net:\nMulti-scale CNN \nDeep structured model: contextual deep CRF\nPrediction refinement stage:\nup-sample & boundary refine\nCoarse-level prediction stage:\ninference on contextual CRF \nLow-resolution \nprediction\nFigure 1. An illustration of the prediction process of our method.\nBoth our unary and pairwise potentials are formulated as multi-\nscale CNNs for capturing semantic relations between image re-\ngions. Our method outputs low-resolution prediction after CRF\ninference, then the prediction is up-sampled and re\ufb01ned in a stan-\ndard post-processing stage to output the \ufb01nal prediction.\nexample, a car is likely to appear over a road, and a glass\nis likely to appear over a table. Context can also encode in-\ncompatibility relations. For example, a car is not likely to be\nsurrounded by sky. These relations also exist at \ufb01ner scales,\nfor example, in object part-to-part relations, and part-to-\nobject relations. In some cases, contextual information is\nthe most important cue, particularly when a single object\nshows signi\ufb01cant visual ambiguities. A more detailed dis-\ncussion of the value of spatial context can be found in [21].\nWe explore two types of spatial context to improve the\nsegmentation performance: patch-patch context and patch-\nbackground context.\nThe patch-patch context is the se-\nmantic relation between the visual patterns of two image\npatches. Likewise, patch-background context is the seman-\ntic relation between a patch and a large background region.\nExplicitly modeling the patch-patch contextual relations\nhas not been well studied in recent CNN-based segmenta-\ntion methods. In this work, we propose to explicitly model\nthe contextual relations using conditional random \ufb01elds\n(CRFs). We formulate CNN-based pairwise potential func-\ntions to capture semantic correlations between neighboring\n1\narXiv:1504.01013v4  [cs.CV]  6 Jun 2016\npatches. Some recent methods combine CNNs and CRFs\nfor semantic segmentation, e.g., the dense CRFs applied in\n[3, 40, 48, 5]. The purpose of applying the dense CRFs in\nthese methods is to re\ufb01ne the upsampled low-resolution pre-\ndiction to sharpen object/region boundaries. These methods\nconsider Potts-model-based pairwise potentials for enforc-\ning local smoothness. There the pairwise potentials are con-\nventional log-linear functions. In contrast, we learn more\ngeneral pairwise potentials using CNNs to model the se-\nmantic compatibility between image regions.\nOur CNN\npairwise potentials aim to improve the coarse-level predic-\ntion rather than doing local smoothness, and thus have a\ndifferent purpose compared to Potts-model-based pairwise\npotentials. Since these two types of potentials have different\neffects, they can be combined to improve the segmentation\nsystem. Fig. 1 illustrates our prediction process.\nIn contrast to patch-patch context, patch-background\ncontext is widely explored in the literature.\nFor CNN-\nbased methods, background information can be effectively\ncaptured by combining features from a multi-scale image\nnetwork input, and has shown good performance in some\nrecent segmentation methods [13, 33].\nA special case\nof capturing patch-background context is considering the\nwhole image as the background region and incorporating\nthe image-level label information into learning. In our ap-\nproach, to encode rich background information, we con-\nstruct multi-scale networks and apply sliding pyramid pool-\ning on feature maps. The traditional pyramid pooling (in a\nsliding manner) on the feature map is able to capture infor-\nmation from background regions of different sizes.\nIncorporating general pairwise (or high-order) potentials\nusually involves expensive inference, which brings chal-\nlenges for CRF learning. To facilitate ef\ufb01cient learning we\napply piecewise training of the CRF [43] to avoid repeated\ninference during back propagation training.\nThus our main contributions are as follows.\n1. We formulate CNN-based general pairwise potential\nfunctions in CRFs to explicitly model patch-patch semantic\nrelations.\n2. Deep CNN-based general pairwise potentials are chal-\nlenging for ef\ufb01cient CNN-CRF joint learning. We perform\napproximate training, using piecewise training of CRFs\n[43], to avoid the repeated inference at every stochastic gra-\ndient descent iteration and thus achieve ef\ufb01cient learning.\n3. We explore background context by applying a network\narchitecture with traditional multi-scale image input [13]\nand sliding pyramid pooling [26]. We empirically demon-\nstrate the effectiveness of this network architecture for se-\nmantic segmentation.\n4. We set new state-of-the-art performance on a num-\nber of popular semantic segmentation datasets, including\nNYUDv2, PASCAL VOC 2012, PASCAL-Context, and\nSIFT-\ufb02ow. In particular, we achieve an intersection-over-\nunion score of 78.0 on the PASCAL VOC 2012 dataset,\nwhich is the best reported result to date.\n1.1. Related work\nExploiting contextual information has been widely stud-\nied in the literature (e.g., [39, 21, 7]). For example, the early\nwork \u201cTAS\u201d [21] models different types of spatial context\nbetween Things and Stuff using a generative probabilistic\ngraphical model.\nThe most successful recent methods for semantic image\nsegmentation are based on CNNs. A number of these CNN-\nbased methods for segmentation are region-proposal-based\nmethods [14, 19], which \ufb01rst generate region proposals and\nthen assign category labels to each. Very recently, FCNNs\n[32, 3, 5] have become a popular choice for semantic seg-\nmentation, because of their effective feature generation and\nend-to-end training. FCNNs have also been applied to a\nrange of other dense-prediction tasks recently, such as im-\nage restoration [10], image super-resolution [8] and depth\nestimation [11, 29]. The method we propose here is simi-\nlarly built upon fully convolution-style networks.\nThe direct prediction of FCNN based methods usually\nare in low-resolution.\nTo obtain high-resolution predic-\ntions, a number of recent methods focus on re\ufb01ning the\nlow-resolution prediction to obtain high resolution predic-\ntion. DeepLab-CRF [3] performs bilinear upsampling of\nthe prediction score map to the input image size and ap-\nply the dense CRF method [24] to re\ufb01ne the object bound-\nary by leveraging the color contrast information. CRF-RNN\n[48] extends this approach by implementing recurrent lay-\ners for end-to-end learning of the dense CRF and the FCNN\nnetwork. The work in [35] learns deconvolution layers to\nupsample the low-resolution predictions. The depth esti-\nmation method [30] explores super-pixel pooling for build-\ning the gap between low-resolution feature map and high-\nresolution \ufb01nal prediction. Eigen et al. [9] perform coarse-\nto-\ufb01ne learning of multiple networks with different resolu-\ntion outputs for re\ufb01ning the coarse prediction. The methods\nin [18, 32] explore middle layer features (skip connections)\nfor high-resolution prediction. Unlike these methods, our\nmethod focuses on improving the coarse (low-resolution)\nprediction by learning general CNN pairwise potentials to\ncapture semantic relations between patches. These re\ufb01ne-\nment methods are complementary to our method.\nCombining the strengths of CNNs and CRFs for seg-\nmentation has been the focus of several recently developed\napproaches. DeepLab-CRF in [3] trains FCNNs and ap-\nplies a dense CRF [24] method as a post-processing step.\nCRF-RNN [48] and the method in [40] extend DeepLab\nand [25] by jointly learning the dense CRFs and CNNs.\nThey consider Potts-model based pairwise potential func-\ntions which enforce smoothness only.\nThe CRF model\nin these methods is for re\ufb01ning the up-sampled predic-\ntion.\nUnlike these methods, our approach learns CNN-\nbased pairwise potential functions for modeling semantic\nPublished in Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR) 2016.\nSurrounding\nAbove/Below\nFigure 2. An illustration of constructing pairwise connections in\na CRF graph. A node is connected to all other nodes which lie\nwithin the range box (dashed box in the \ufb01gure). Two types of\nspatial relations are described in the \ufb01gure, which correspond to\ntwo types of pairwise potential functions.\nrelations between patches.\nJointly learning CNNs and CRFs has also been explored\nin other applications apart from segmentation. The recent\nwork in [29, 30] proposes to jointly learn continuous CRFs\nand CNNs for depth estimation from single monocular im-\nages. The work in [45] combines CRFs and CNNs for hu-\nman pose estimation. The authors of [4] explore joint train-\ning of Markov random \ufb01elds and deep neural networks for\npredicting words from noisy images and image s classi\ufb01-\ncation. Different from these methods, we explore ef\ufb01cient\npiecewise training of CRFs with CNN pairwise potentials.\n2. Modeling semantic pairwise relations\nFig. 3 conceptualizes our architecture at a high level.\nGiven an image, we \ufb01rst apply a convolutional network\nto generate a feature map.\nWe refer to this network as\n\u2018FeatMap-Net\u2019.\nThe resulting feature map is at a lower\nresolution than the original image because of the down-\nsampling operations in the pooling layers.\nWe then create the CRF graph as follows: for each lo-\ncation in the feature map (which corresponds to a rect-\nangular region in the input image) we create one node in\nthe CRF graph.\nPairwise connections in the CRF graph\nare constructed by connecting one node to all other nodes\nwhich lie within a spatial range box (the dashed box in\nFig. 2). We consider different spatial relations by de\ufb01ning\ndifferent types of range box, and each type of spatial re-\nlation is modeled by a speci\ufb01c pairwise potential function.\nAs shown in Fig. 2, our method models the \u201csurrounding\u201d\nand \u201cabove/below\u201d spatial relations. In our experiments,\nthe size of the range box (dash box in the \ufb01gure) size is\n0.4a \u00d7 0.4a. Here we denote by a the length of the short\nedge of the feature map.\nNote that although \u2018FeatMap-Net\u2019 de\ufb01nes a common ar-\nchitecture, in fact we train three such networks: one for the\nunary potential and one each for the two types of pairwise\npotential.\n3. Contextual Deep CRFs\nHere we describe the details of our deep CRF model.\nWe denote by x \u2208X one input image and y \u2208Y the\nlabeling mask which describes the label con\ufb01guration of\nEdge \nfeature vector\nFeature map\nOne connection\nIn CRF graph\nFeatMap-Net\nUnary-Net\nNode \nfeature vector\nUnary \npotential output\nOne node \nin CRF graph\nCRF graph\nPairwise-Net\nPairiwise \npotential output\nGenerate features\n(low resolution)\nConstruct CRF graph\nFigure 3. An illustration of generating unary or pairwise potential\nfunction outputs. First a feature map is generated by a FeatMap-\nNet, and a CRF graph is constructed based on the spatial resolution\nof the feature map. Finally the Unary-Net (or Pairwise-Net) pro-\nduces potential function outputs.\neach node in the CRF graph. The energy function is de-\nnoted by E(y, x; \u03b8) which models the compatibility of the\ninput-output pair, with a small output value indicating high\ncon\ufb01dence in the prediction y. All network parameters are\ndenoted by \u03b8 which we need to learn. The conditional like-\nlihood for one image is formulated as follows:\nP(y|x) =\n1\nZ(x) exp[\u2212E(y, x)].\n(1)\nHere Z(x) = P\ny exp[\u2212E(y, x)] is the partition function.\nThe energy function is typically formulated by a set of unary\nand pairwise potentials:\nE(y, x) =\nX\nU\u2208U\nX\np\u2208NU\nU(yp, xp) +\nX\nV \u2208V\nX\n(p,q)\u2208SV\nV (yp, yq, xpq).\nHere U is a unary potential function, and to make the ex-\nposition more general, we consider multiple types of unary\npotentials with U the set of all such unary potentials. NU is\na set of nodes for the potential U. Likewise, V is a pairwise\npotential function with V the set of all types of pairwise po-\ntential. SV is the set of edges for the potential V . xp and\nxpq indicates the corresponding image regions which asso-\nciate to the speci\ufb01ed node and edge.\n3.1. Unary potential functions\nWe formulate the unary potential function by stacking\nthe FeatMap-Net for generating feature maps and a shallow\nfully connected network (referred to as Unary-Net) to gen-\nerate the \ufb01nal output of the unary potential function. The\nunary potential function is written as follows:\nU(yp, xp; \u03b8U) = \u2212zp,yp(x; \u03b8U).\n(2)\nHere zp,yp is the output value of Unary-Net, which corre-\nsponds to the p-th node and the yp-th class.\nFig. 3 includes an illustration of the Unary-Net and how\nPublished in Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR) 2016.\nscale level 1\nImage pyramid\nscale level 2\nscale level 3\nMulti-scale\nfeature maps\nd\nd\nd\n9d\nup-sample\nconcatenate\nFinal \nfeature map\nConv Block 1-5\n(shared across scales)\nConv Block 6\n(for 1 scale only)\n3d\nSliding \npyramid pooling\n3d\n3d\nSliding \npyramid pooling\nSliding \npyramid pooling\nFeatMap-Net:\nConv Block 6\n(for 1 scale only)\nConv Block 6\n(for 1 scale only)\nFigure 4. The details of our FeatMap-Net. An input image is \ufb01rst resized into 3 scales, then each resized image goes through 6 convolution\nblocks to output one feature map. Top 5 convolution blocks are shared for all scales. Every scale has a speci\ufb01c convolution block (Conv\nBlock 6). We perform 2-level sliding pyramid pooling (see Fig. 5 for details). d indicates the feature dimension.\nit integrates with FeatMap-Net. The unary potential at each\nCRF node is simply the K-dimensional output (where K\nis the number of classes) of Unary-Net applied to the node\nfeature vector from the correpsonding location in the feature\nmap (i.e. the output of FeatMap-Net).\n3.2. Pairwise potential functions\nFig. 3 likewise illustrates how the pairwise potentials are\ngenerated. The edge features are formed by concatenating\nthe corresponding feature vectors of two connected nodes\n(similar to [23]). The feature vector for each node in the pair\nis from the feature map output by FeatMap-Net. The edge\nfeatures of one pair are then fed to a shallow fully connected\nnetwork (referred to as Pairwise-Net) to generate the \ufb01nal\noutput that is the pairwise potential. The size of this is K \u00d7\nK to match the number of possible label combinations for a\npair of nodes. The pairwise potential function is written as\nfollows:\nV (yp, yq, xpq; \u03b8V ) = \u2212zp,q,yp,yq(x; \u03b8V ).\n(3)\nHere zp,q,yp,yq is the output value of Pairwise-Net. It is\nthe con\ufb01dence value for the node pair (p, q) when they are\nlabeled with the class value (yp, yq), which measures the\ncompatibility of the label pair (yp, yq) given the input image\nx. \u03b8V is the corresponding set of CNN parameters for the\npotential V , which we need to learn.\nOur formulation of pairwise potentials is different from\nthe Potts-model-based formulation in the existing methods\nof [3, 48]. The Potts-model-based pairwise potentials are\na log-linear functions and employ a special formulation for\nenforcing neighborhood smoothness. In contrast, our pair-\nwise potentials model the semantic compatibility between\ntwo nodes with the output for every possible value of the\nlabel pair (yp, yq) individually parameterized by CNNs.\nIn our system, after obtaining the coarse level prediction,\nwe still need to perform a re\ufb01nement step to obtain the \ufb01nal\nhigh-resolution prediction (as shown in Fig. 1). Hence we\nalso apply the dense CRF method [24], as in many other re-\ncent methods, in the prediction re\ufb01nement step. Therefore,\nour system takes advantage of both contextual CNN poten-\ntials and the traditional smoothness potentials to improve\nthe \ufb01nal system. More details are described in Sec. 5.\nAs in [47, 20], modeling asymmetric relations requires\nthe potential function is capable of modeling input orders,\nsince we have: V (yp, yq, xpq) \u0338= V (yq, yp, xqp). Take the\nasymmetric relation \u201cabove/below\u201d as an example; we take\nadvantage of the input pair order to indicate the spatial con-\n\ufb01guration of two nodes, thus the input (yp, yq, xpq) indi-\ncates the con\ufb01guration that the node p is spatially lies above\nthe node q.\nThe asymmetric property is readily achieved with our\ngeneral formulation of pairwise potentials. The potential\noutput for every possible pairwise label combination for\n(p, q) is individually parameterized by the pairwise CNNs.\n4. Exploiting background context\nTo encode rich background information, we use multi-\nscale CNNs and sliding pyramid pooling [26] for our\nFeatMap-Net. Fig. 4 shows the details of the FeatMap-Net.\nCNNs with multi-scale image network inputs have\nshown good performance in some recent segmentation\nmethods [13, 33]. The traditional pyramid pooling (in a\nsliding manner) on the feature map is able to capture infor-\nmation from background regions of different sizes. We ob-\nserve that these two techniques (multi-scale network design\nand pyramid pooling) for encoding background information\nare very effective for improving performance.\nApplying CNNs on multi-scale images has shown good\nperformance in some recent segmentation methods [13, 33].\nIn our multi-scale network, an input image is \ufb01rst resized\ninto 3 scales, then each resized image goes through 6 convo-\nlution blocks to output one feature map. In our experiment,\nthe 3 scales for the input image are set to 1.2, 0.8 and 0.4.\nAll scales share the same top 5 convolution blocks. In addi-\ntion, each scale has an exclusive convolution block (\u201cConv\nPublished in Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR) 2016.\nSliding pooling\nwindow size: 5x5\nSliding pooling\nwindow size: 9x9\nFeature map\nd\nConcatenated\nfeature map\nPooled \nfeature map\nPooled \nfeature map\nd\nd\nd\nd\n3d\nFigure 5. Details for sliding pyramid pooling. We perform 2-level\nsliding pyramid pooling on the feature map for capturing patch-\nbackground context, which encode rich background information\nand increase the \ufb01eld-of-view for the feature map.\nBlock 6\u201d in the \ufb01gure) which captures scale-dependent in-\nformation. The resulting 3 feature maps (corresponding to\n3 scales) are of different resolutions, therefore we upscale\nthe two smaller ones to the size of the largest feature map\nusing bilinear interpolation. These feature maps are then\nconcatenated to form one feature map.\nWe perform spatial pyramid pooling [26] (a modi\ufb01ed\nversion using sliding windows) on the feature map to cap-\nture information from background regions in multiple sizes.\nThis increases the \ufb01eld-of-view for the feature map and thus\nit is able to capture the information from a large image re-\ngion. Increasing the \ufb01eld-of-view generally helps to im-\nprove performance [3].\nThe details of spatial pyramid pooling are illustrated in\nFig. 5. In our experiment, we perform 2-level pooling for\neach image scale. We de\ufb01ne 5\u00d75 and 9\u00d79 sliding pooling\nwindows (max-pooling) to generate 2 sets of pooled feature\nmaps, which are then concatenated to the original feature\nmap to construct the \ufb01nal feature map.\nThe detailed network layer con\ufb01guration for all networks\nare described in Fig. 6.\n5. Prediction\nIn the prediction stage, our deep structured model will\ngenerate low-resolution prediction (as shown in Fig. 1),\nwhich is 1/16 of the input image size.\nThis is due to\nthe stride setting of pooling or convolution layers for sub-\nsampling. Therefore, we apply two prediction stages for ob-\ntaining the \ufb01nal high-resolution prediction: the coarse-level\nprediction stage and the prediction re\ufb01nement stage.\n5.1. Coarse-level prediction stage\nWe perform CRF inference on our contextual structured\nmodel to obtain the coarse prediction of a test image. We\nconsider the marginal inference over nodes for prediction:\n\u2200p \u2208N :\nP(yp|x) = P\ny\\yp P(y|x).\n(4)\nThe obtained marginal distribution can be further applied in\nthe next prediction stage for boundary re\ufb01nement.\nConv block 1:\n3 x 3 conv 64\n3 x 3 conv 64\n2 x 2 pooling\nConv block 2:\n3 x 3 conv 128\n3 x 3 conv 128\n2 x 2 pooling\nConv block 3:\n3 x 3 conv 256\n3 x 3 conv 256\n3 x 3 conv 256\n2 x 2 pooling\nFeatMap-Net\nUnary-Net\n2 fully-connected layers:\nFully-con 512\nFully-con K\nConv block 4:\n3 x 3 conv 512\n3 x 3 conv 512\n3 x 3 conv 512\n2 x 2 pooling\nConv block 5:\n3 x 3 conv 512\n3 x 3 conv 512\n3 x 3 conv 512\n2 x 2 pooling\n7 x 7 conv 4096\nConv block 6:\n3 x 3 conv 512\n3 x 3 conv 512\nPairwise-Net\n2 fully-connected layers:\nFully-con 512\nFully-con K2\nFigure 6. The detailed con\ufb01guration of the networks: FeatMap-\nNet, Unary-Net and Pairwise-Net. K is the number of classes.\nFor FeatMap-Net, the top 5 convolution blocks share the same\ncon\ufb01guration as the convolution blocks in the VGG-16 network.\nThe stride of the last max pooling layer is 1, and for the other max\npooling layers we use the same stride setting as VGG-16.\nOur CRF graph does not form a tree structure, nor are\nthe potentials submodular, hence we need to an apply ap-\nproximate inference. To address this we apply an ef\ufb01cient\nmessage passing algorithm which is based on the mean \ufb01eld\napproximation [36]. The mean \ufb01eld algorithm constructs a\nsimpler distribution Q(y), e.g., a product of independent\nmarginals: Q(y) = Q\np\u2208N Qp(yp), which minimizes the\nKL-divergence between the distribution Q(y) and P(y). In\nour experiments, we perform 3 mean \ufb01eld iterations.\n5.2. Prediction re\ufb01nement stage\nWe generate the score map for the coarse prediction\nfrom the marginal distribution which we obtain from the\nmean-\ufb01eld inference.\nWe \ufb01rst bilinearly up-sample the\nscore map of the coarse prediction to the size of the in-\nput image.\nThen we apply a common post-processing\nmethod [24] (dense CRF) to sharpen the object boundary for\ngenerating the \ufb01nal high-resolution prediction. This post-\nprocessing method leverages low-level pixel intensity infor-\nmation (color contrast) for boundary re\ufb01nement. Note that\nmost recent work on image segmentation similarly produces\nlow-resolution prediction and have a upsampling and re\ufb01ne-\nment process/model for the \ufb01nal prediction, e.g., [3, 48, 5].\nIn summary, we simply perform bilinear upsampling of\nthe coarse score map and apply the boundary re\ufb01nement\npost-processing. We argue that this stage can be further im-\nproved by applying more sophisticated re\ufb01nement methods,\ne.g., training deconvolution networks [35], training multi-\nple coarse to \ufb01ne learning networks [9], and exploring mid-\ndle layer features for high-resolution prediction [18, 32]. It\nis expected that applying better re\ufb01nement approaches will\ngain further performance improvement.\nPublished in Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR) 2016.\n6. CRF training\nA common approach for CRF learning is to maximize\nthe likelihood, or equivalently minimize the negative log-\nlikelihood, which can be written for one image as:\n\u2212log P(y|x; \u03b8) = E(y, x; \u03b8) + log Z(x; \u03b8).\n(5)\nAdding regularization to the CNN parameter \u03b8, the opti-\nmization problem for CRF learning is:\nmin\n\u03b8\n\u03bb\n2 \u2225\u03b8\u22252\n2 +\nN\nX\ni=1\n\u0014\nE(y(i), x(i); \u03b8) + log Z(x(i); \u03b8)\n\u0015\n.\n(6)\nHere x(i), y(i) denote the i-th training image and its seg-\nmentation mask; N is the number of training images; \u03bb is\nthe weight decay parameter. We can apply stochastic gradi-\nent (SGD) based methods to optimize the above problem for\nlearning \u03b8. The energy function E(y, x; \u03b8) is constructed\nfrom CNNs, and its gradient \u2207\u03b8E(y, x; \u03b8) easily computed\nby applying the chain rule as in conventional CNNs. How-\never, the partition function Z brings dif\ufb01culties for opti-\nmization. Its gradient is:\n\u2207\u03b8 logZ(x; \u03b8)\n=\nX\ny\nexp[\u2212E(y, x; \u03b8)]\nP\ny\u2032 exp[\u2212E(y\u2032, x; \u03b8)]\u2207\u03b8[\u2212E(y, x; \u03b8)]\n= \u2212Ey\u223cP (y|x;\u03b8)\u2207\u03b8E(y, x; \u03b8)\n(7)\nGenerally the size of the output space Y is exponential in the\nnumber of nodes, which prohibits the direct calculation of Z\nand its gradient. The CRF graph we considered for segmen-\ntation here is a loopy graph (not tree-structured), for which\nthe inference is generally computationally expensive. More\nimportantly, usually a large number of SGD iterations (tens\nor hundreds of thousands) are required for training CNNs.\nThus performing inference at each SGD iteration is very\ncomputationally expensive.\n6.1. Piecewise training of CRFs\nInstead of directly solving the optimization in (6), we\npropose to apply an approximate CRF learning method.\nIn the literature, there are two popular types of learning\nmethods which approximate the CRF objective : pseudo-\nlikelihood learning [1] and piecewise learning [43]. The\nmain advantage of these methods in term of training deep\nCRF is that they do not involve marginal inference for gradi-\nent calculation, which signi\ufb01cantly improves the ef\ufb01ciency\nof training. Decision tree \ufb01elds [37] and regression tree\n\ufb01elds [22] are based on pseudo-likelihood learning, while\npiecewise learning has been applied in the work [43, 23].\nHere we develop this idea for the case of training the\nCRF with the CNN potentials. In piecewise training, the\nconditional likelihood is formulated as a number of inde-\npendent likelihoods de\ufb01ned on potentials, written as:\nP(y|x) =\nY\nU\u2208U\nY\np\u2208NU\nP U(yp|x)\nY\nV \u2208V\nY\n(p,q)\u2208SV\nP V (yp, yq|x).\nThe likelihood P U(yp|x) is constructed from the unary po-\ntential U. Likewise, P V (yp, yq|x) is constructed from the\npairwise potential V . P U and P V are written as:\nP U(yp|x) =\nexp[\u2212U(yp, xp)]\nP\ny\u2032p exp[\u2212U(y\u2032p, xp)],\n(8)\nP V (yp, yq|x) =\nexp[\u2212V (yp, yq, xpq)]\nP\ny\u2032p,y\u2032q exp[\u2212V (y\u2032p, y\u2032q, xpq)].\n(9)\nThus the optimization for piecewise training is to minimize\nthe negative log likelihood with regularization:\nmin\n\u03b8\n\u03bb\n2 \u2225\u03b8\u22252\n2 \u2212\nN\nX\ni=1\n\u0014 X\nU\u2208U\nX\np\u2208N(i)\nU\nlog P U(yp|x(i); \u03b8U)\n+\nX\nV \u2208V\nX\n(p,q)\u2208S(i)\nV\nlog P V (yp, yq|x(i); \u03b8V )\n\u0015\n.\n(10)\nCompared to the objective in (6) for direct maximum like-\nlihood learning, the above objective does not involve the\nglobal partition function Z(x; \u03b8). To calculate the gradi-\nent of the above objective, we only need to calculate the\ngradient \u2207\u03b8U log P U and \u2207\u03b8V log P V . With the de\ufb01nition\nin (8), P U is a conventional Softmax normalization func-\ntion over only K (the number of classes) elements. Similar\nanalysis can also be applied to P V . Hence, we can eas-\nily calculate the gradient without involving expensive infer-\nence. Moreover, we are able to perform parallel training of\npotential functions, since the above objective is formulated\nas a summation of independent log-likelihoods.\nAs previously discussed, CNN training usually involves\na large number of gradient update iterations. However this\nmeans that expensive inference during every gradient iter-\nation becomes impractical. Our piecewise approach here\nprovides a practical solution for learning CRFs with CNN\npotentials on large-scale data.\n7. Experiments\nWe evaluate our method on 4 popular semantic segmen-\ntation datasets: PASCAL VOC 2012, NYUDv2, PASCAL-\nContext and SIFT-\ufb02ow. The segmentation performance is\nmeasured by the intersection-over-union (IoU) score [12],\nthe pixel accuracy and the mean accuracy [32].\nThe \ufb01rst 5 convolution blocks and the \ufb01rst convo-\nlution layer in the 6th convolution block are initialized\nfrom the VGG-16 network [42]. All remaining layers are\nrandomly initialized.\nAll layers are trained using back-\npropagation/SGD. As illustrated in Fig. 2, we use 2 types\nof pairwise potential functions. In total, we have 1 type of\nunary potential function and 2 types of pairwise potential\nfunctions. We formulate one speci\ufb01c FeatMap-Net and po-\nPublished in Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR) 2016.\nTable 1. Segmentation results on NYUDv2 dataset (40 classes).\nWe compare to a number of recent methods. Our method signi\ufb01-\ncantly outperforms the existing methods.\nmethod\ntraining data\npixel accuracy\nmean accuracy\nIoU\nGupta et al. [16]\nRGB-D\n60.3\n-\n28.6\nFCN-32s [32]\nRGB\n60.0\n42.2\n29.2\nFCN-HHA [32]\nRGB-D\n65.4\n46.1\n34.0\nours\nRGB\n70.0\n53.6\n40.6\ntential network (Unary-Net or Pairwise-Net) for one type of\npotential function. We apply simple data augmentation in\nthe training stage; speci\ufb01cally, we perform random scaling\n(from 0.7 to 1.2) and \ufb02ipping of the images for training.\nOur system is built on MatConvNet [46].\n7.1. Results on NYUDv2\nWe \ufb01rst evaluate our method on the dataset NYUDv2\n[41]. NYUDv2 dataset has 1449 RGB-D images. We use\nthe segmentation labels provided in [15] in which labels are\nprocessed into 40 classes. We use the standard training set\nwhich contains 795 images and the test set which contains\n654 images. We train our models only on RGB images\nwithout using the depth information.\nResults are shown in Table 1. Unless otherwise spec-\ni\ufb01ed, our models are initialized using the VGG-16 net-\nwork. VGG-16 is also used in the competing method FCN\n[32]. Our contextual model with CNN pairwise potentials\nachieves the best performance, which sets a new state-of-\nthe-art result on the NYUDv2 dataset. Note that we do not\nuse any depth information in our model.\nComponent Evaluation We evaluate the performance\ncontribution of different components of the FeatMap-Net\nfor capturing patch-background context on the NYUDv2\ndataset. We present the results of adding different compo-\nnents of FeatMap-Net in Table 2. We start from a base-\nline setting of our FeatMap-Net (\u201cFullyConvNet Baseline\u201d\nin the result table), for which multi-scale and sliding pool-\ning is removed. This baseline setting is the conventional\nfully convolution network for segmentation, which can be\nconsidered as our implementation of the FCN method in\n[32]. The result shows that our CNN baseline implementa-\ntion (\u201cFullyConvNet\u201d) achieves very similar performance\n(slightly better) than the FCN method.\nApplying multi-\nscale network design and sliding pyramid pooling signi\ufb01-\ncantly improve the performance, which clearly shows the\nbene\ufb01ts of encoding rich background context in our ap-\nproach. Applying the dense CRF method [24] for bound-\nary re\ufb01nement gains further improvement. Finally, adding\nour contextual CNN pairwise potentials brings signi\ufb01cant\nfurther improvement, for which we achieve the best perfor-\nmance in this dataset.\n7.2. Results on PASCAL VOC 2012\nPASCAL VOC 2012 [12] is a well-known segmentation\nevaluation dataset which consists of 20 object categories\nTable 2. Ablation Experiments.\nThe table shows the value\nadded by the different system components of our method on the\nNYUDv2 dataset (40 classes).\nmethod\npixel accuracy\nmean accuracy\nIoU\nFCN-32s [32]\n60.0\n42.2\n29.2\nFullyConvNet Baseline\n61.5\n43.2\n30.5\n+ sliding pyramid pooling\n63.5\n45.3\n32.4\n+ multi-scales\n67.0\n50.1\n37.0\n+ boundary re\ufb01nement\n68.5\n50.9\n38.3\n+ CNN contextual pairwise\n70.0\n53.6\n40.6\n(a) Testing\n(b) Truth\n(c) Predict\n(d) Testing\n(e) Truth\n(f) Predict\nFigure 7. Some prediction examples of our method.\nand one background category. This dataset is split into a\ntraining set, a validation set and a test set, which respec-\ntively contain 1464, 1449 and 1456 images. Following a\nconventional setting in [19, 3], the training set is augmented\nby extra annotated VOC images provided in [17], which re-\nsults in 10582 training images. We verify our performance\non the PASCAL VOC 2012 test set. We compare with a\nnumber of recent methods with competitive performance.\nSince the ground truth labels are not available for the test\nset, we report the result through the VOC evaluation server.\nThe results of IoU scores are shown in the last column\nof Table 3. We \ufb01rst train our model only using the VOC\nimages. We achieve 75.3 IoU score which is the best result\namongst methods that only use the VOC training data.\nTo improve the performance, following the setting in re-\ncent work [3, 5], we train our model with the extra images\nfrom the COCO dataset [27]. With these extra training im-\nages, we achieve an IoU score of 77.2.\nFor further improvement, we also exploit the the middle-\nlayer features as in the recent methods [3, 32, 18].\nWe\nlearn extra re\ufb01nement layers on the feature maps from mid-\ndle layers to re\ufb01ne the coarse prediction. The feature maps\nfrom the middle layers encode lower level visual informa-\ntion which helps to predict details in the object boundaries.\nSpeci\ufb01cally, we add 3 re\ufb01nement convolution layers on top\nPublished in Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR) 2016.\nTable 3. Individual category results on the PASCAL VOC 2012 test set (IoU scores). Our method performs the best\nmethod\naero\nbike\nbird\nboat\nbottle\nbus\ncar\ncat\nchair\ncow\ntable\ndog\nhorse\nmbike\nperson\npotted\nsheep\nsofa\ntrain\ntv\nmean\nOnly using VOC training data\nFCN-8s [32]\n76.8\n34.2\n68.9\n49.4\n60.3\n75.3\n74.7\n77.6\n21.4\n62.5\n46.8\n71.8\n63.9\n76.5\n73.9\n45.2\n72.4\n37.4\n70.9\n55.1\n62.2\nZoom-out [33]\n85.6\n37.3\n83.2\n62.5\n66.0\n85.1\n80.7\n84.9\n27.2\n73.2\n57.5\n78.1\n79.2\n81.1\n77.1\n53.6\n74.0\n49.2\n71.7\n63.3\n69.6\nDeepLab [3]\n84.4\n54.5\n81.5\n63.6\n65.9\n85.1\n79.1\n83.4\n30.7\n74.1\n59.8\n79.0\n76.1\n83.2\n80.8\n59.7\n82.2\n50.4\n73.1\n63.7\n71.6\nCRF-RNN [48]\n87.5\n39.0\n79.7\n64.2\n68.3\n87.6\n80.8\n84.4\n30.4\n78.2\n60.4\n80.5\n77.8\n83.1\n80.6\n59.5\n82.8\n47.8\n78.3\n67.1\n72.0\nDeconvNet [35]\n89.9\n39.3\n79.7\n63.9\n68.2\n87.4\n81.2\n86.1\n28.5\n77.0\n62.0\n79.0\n80.3\n83.6\n80.2\n58.8\n83.4\n54.3\n80.7\n65.0\n72.5\nDPN [31]\n87.7\n59.4\n78.4\n64.9\n70.3\n89.3\n83.5\n86.1\n31.7\n79.9\n62.6\n81.9\n80.0\n83.5\n82.3\n60.5\n83.2\n53.4\n77.9\n65.0\n74.1\nours\n90.6\n37.6\n80.0\n67.8\n74.4\n92.0\n85.2\n86.2\n39.1\n81.2\n58.9\n83.8\n83.9\n84.3\n84.8\n62.1\n83.2\n58.2\n80.8\n72.3\n75.3\nUsing VOC+COCO training data\nDeepLab [3]\n89.1\n38.3\n88.1\n63.3\n69.7\n87.1\n83.1\n85.0\n29.3\n76.5\n56.5\n79.8\n77.9\n85.8\n82.4\n57.4\n84.3\n54.9\n80.5\n64.1\n72.7\nCRF-RNN [48]\n90.4\n55.3\n88.7\n68.4\n69.8\n88.3\n82.4\n85.1\n32.6\n78.5\n64.4\n79.6\n81.9\n86.4\n81.8\n58.6\n82.4\n53.5\n77.4\n70.1\n74.7\nBoxSup [5]\n89.8\n38.0\n89.2\n68.9\n68.0\n89.6\n83.0\n87.7\n34.4\n83.6\n67.1\n81.5\n83.7\n85.2\n83.5\n58.6\n84.9\n55.8\n81.2\n70.7\n75.2\nDPN [31]\n89.0\n61.6\n87.7\n66.8\n74.7\n91.2\n84.3\n87.6\n36.5\n86.3\n66.1\n84.4\n87.8\n85.6\n85.4\n63.6\n87.3\n61.3\n79.4\n66.4\n77.5\nours+\n94.1\n40.7\n84.1\n67.8\n75.9\n93.4\n84.3\n88.4\n42.5\n86.4\n64.7\n85.4\n89.0\n85.8\n86.0\n67.5\n90.2\n63.8\n80.9\n73.0\n78.0\nTable 4. Segmentation results on PASCAL-Context dataset (60\nclasses). Our method performs the best.\nmethod\npixel accuracy\nmean accuracy\nIoU\nO2P [2]\n-\n-\n18.1\nCFM [6]\n-\n-\n34.4\nFCN-8s [32]\n65.9\n46.5\n35.1\nBoxSup [5]\n-\n-\n40.5\nours\n71.5\n53.9\n43.3\nTable 5. Segmentation results on SIFT-\ufb02ow dataset (33 classes).\nOur method performs the best.\nmethod\npixel accuracy\nmean accuracy\nIoU\nLiu et al. [28]\n76.7\n-\n-\nTighe et al. [44]\n75.6\n41.1\n-\nTighe et al. (MRF) [44]\n78.6\n39.2\n-\nFarabet et al. (balance) [13]\n72.3\n50.8\n-\nFarabet et al. [13]\n78.5\n29.6\n-\nPinheiro et al. [38]\n77.7\n29.8\n-\nFCN-16s [32]\n85.2\n51.7\n39.5\nours\n88.1\n53.4\n44.9\nof the feature maps from the \ufb01rst 5 max-pooling layers\nand the input image. The resulting feature maps and the\ncoarse prediction score map are then concatenated and go\nthrough another 3 re\ufb01nement convolution layers to output\nthe re\ufb01ned prediction. The resolution of the prediction is\nincreased from 1/16 (coarse prediction) to 1/4 of the in-\nput image. With this re\ufb01ned prediction, we further perform\nboundary re\ufb01nement [24] to generate the \ufb01nal prediction.\nFinally, we achieve an IoU score of 78.0, which is best re-\nported result on this challenging dataset. 1\nThe results for each category are shown in Table 3. We\noutperform competing methods in most categories. For only\nusing the VOC training set, our method outperforms the sec-\nond best method, DPN [31], on 18 categories out of 20.\nUsing VOC+COCO training set, our method outperforms\nDPN [31] on 15 categories out of 20. Some prediction ex-\namples of our method are shown in Fig. 7.\n7.3. Results on PASCAL-Context\nThe PASCAL-Context [34] dataset provides the segmen-\ntation labels of the whole scene (including the \u201cstuff\u201d la-\n1The result link at the VOC evaluation server:\nhttp://host.\nrobots.ox.ac.uk:8080/anonymous/XTTRFF.html\nbels) for the PASCAL VOC images. We use the segmen-\ntation labels which contain 60 classes (59 classes plus the\n\u201c background\u201d class ) for evaluation. We use the provided\ntraining/test splits. The training set contains 4998 images\nand the test set has 5105 images.\nResults are shown in Table 4. Our method signi\ufb01cantly\noutperforms the competing methods. To our knowledge,\nours is the best reported result on this dataset.\n7.4. Results on SIFT-\ufb02ow\nWe further evaluate our method on the SIFT-\ufb02ow dataset.\nThis dataset contains 2688 images and provide the segmen-\ntation labels for 33 classes. We use the standard split for\ntraining and evaluation. The training set has 2488 images\nand the rest 200 images are for testing. Since images are\nin small sizes, we upscale the image by a factor of 2 for\ntraining. Results are shown in Table 5. We achieve the best\nperformance for this dataset.\n8. Conclusions\nWe have proposed a method which combines CNNs and\nCRFs to exploit complex contextual information for seman-\ntic image segmentation. We formulate CNN based pairwise\npotentials for modeling semantic relations between image\nregions. Our method shows best performance on several\npopular datasets including the PASCAL VOC 2012 dataset.\nThe proposed method is potentially widely applicable to\nother vision tasks.\nAcknowledgments This research was supported by the\nData to Decisions Cooperative Research Centre and by the\nAustralian Research Council through the Australian Cen-\ntre for Robotic Vision (CE140100016).\nC. Shen\u2019s par-\nticipation was supported by an ARC Future Fellowship\n(FT120100969). I. Reid\u2019s participation was supported by\nan ARC Laureate Fellowship (FL130100102).\nC. Shen is the corresponding author (e-mail:\nchun-\nhua.shen@adelaide.edu.au).\nPublished in Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR) 2016.\nReferences\n[1] J. Besag. Ef\ufb01ciency of pseudolikelihood estimation for sim-\nple Gaussian \ufb01elds. Biometrika, 1977. 6\n[2] J. Carreira, R. Caseiro, J. Batista, and C. Sminchisescu. Se-\nmantic segmentation with second-order pooling.\nIn Proc.\nEur. Conf. Comp. Vis., 2012. 8\n[3] L. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L.\nYuille.\nSemantic image segmentation with deep convolu-\ntional nets and fully connected CRFs. In Proc. Int. Conf.\nLearning Representations, 2015. 1, 2, 4, 5, 7, 8\n[4] L.-C. Chen, A. G. Schwing, A. L. Yuille, and R. Urtasun.\nLearning deep structured models. In Proc. Int. Conf. Ma-\nchine Learn., 2015. 3\n[5] J. Dai, K. He, and J. Sun. BoxSup: Exploiting bounding\nboxes to supervise convolutional networks for semantic seg-\nmentation. In Proc. Int. Conf. Comp. Vis., 2015. 2, 5, 7,\n8\n[6] J. Dai, K. He, and J. Sun. Convolutional feature masking\nfor joint object and stuff segmentation. In Proc. IEEE Conf.\nComp. Vis. Pattern Recogn., 2015. 8\n[7] C. Doersch, A. Gupta, and A. A. Efros. Context as supervi-\nsory signal: Discovering objects with predictable context. In\nProc. European Conf. Computer Vision, 2014. 2\n[8] C. Dong, C. C. Loy, K. He, and X. Tang. Learning a deep\nconvolutional network for image super-resolution. In Proc.\nEur. Conf. Comp. Vis., 2014. 2\n[9] D. Eigen and R. Fergus. Predicting depth, surface normals\nand semantic labels with a common multi-scale convolu-\ntional architecture. In Proceedings of the IEEE International\nConference on Computer Vision, 2015. 2, 5\n[10] D. Eigen, D. Krishnan, and R. Fergus. Restoring an image\ntaken through a window covered with dirt or rain. In Proc.\nInt. Conf. Comp. Vis., 2013. 2\n[11] D. Eigen, C. Puhrsch, and R. Fergus. Depth map prediction\nfrom a single image using a multi-scale deep network. In\nProc. Adv. Neural Info. Process. Syst., 2014. 2\n[12] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and\nA. Zisserman. The pascal visual object classes (voc) chal-\nlenge. In Proc. Int. J. Comp. Vis., 2010. 6, 7\n[13] C. Farabet, C. Couprie, L. Najman, and Y. LeCun. Learn-\ning hierarchical features for scene labeling. IEEE T. Pattern\nAnalysis & Machine Intelligence, 2013. 1, 2, 4, 8\n[14] R. B. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich\nfeature hierarchies for accurate object detection and seman-\ntic segmentation. In Proc. IEEE Conf. Comp. Vis. Pattern\nRecogn., 2014. 2\n[15] S. Gupta, P. Arbelaez, and J. Malik. Perceptual organization\nand recognition of indoor scenes from rgb-d images. In Proc.\nIEEE Conf. Comp. Vis. Pattern Recogn., 2013. 7\n[16] S. Gupta, R. Girshick, P. Arbel\u00b4aez, and J. Malik. Learning\nrich features from rgb-d images for object detection and seg-\nmentation. In Proc. Eur. Conf. Comp. Vis., 2014. 7\n[17] B. Hariharan, P. Arbelaez, L. D. Bourdev, S. Maji, and J. Ma-\nlik. Semantic contours from inverse detectors. In Proc. Int.\nConf. Comp. Vis., 2011. 7\n[18] B. Hariharan, P. Arbel\u00b4aez, R. Girshick, and J. Malik. Hyper-\ncolumns for object segmentation and \ufb01ne-grained localiza-\ntion. In Proc. IEEE Conf. Comp. Vis. Pattern Recogn., 2014.\n2, 5, 7\n[19] B. Hariharan, P. Arbel\u00b4aez, R. Girshick, and J. Malik. Si-\nmultaneous detection and segmentation. In Proc. European\nConf. Computer Vision, 2014. 2, 7\n[20] D. Heesch and M. Petrou. Markov random \ufb01elds with asym-\nmetric interactions for modelling spatial context in structured\nscene labelling. Journal of Signal Processing Systems, 2010.\n4\n[21] G. Heitz and D. Koller. Learning spatial context: Using stuff\nto \ufb01nd things. In Proc. European Conf. Computer Vision,\n2008. 1, 2\n[22] J. Jancsary, S. Nowozin, T. Sharp, and C. Rother. Regression\ntree \ufb01eldsan ef\ufb01cient, non-parametric approach to image la-\nbeling problems. In Proc. IEEE Conf. Comp. Vis. Pattern\nRecogn., 2012. 6\n[23] A. Kolesnikov, M. Guillaumin, V. Ferrari, and C. H. Lam-\npert. Closed-form training of conditional random \ufb01elds for\nlarge scale image segmentation. In Proc. Eur. Conf. Comp.\nVis., 2014. 4, 6\n[24] P. Kr\u00a8ahenb\u00a8uhl and V. Koltun.\nEf\ufb01cient inference in fully\nconnected CRFs with Gaussian edge potentials. In Proc. Adv.\nNeural Info. Process. Syst., 2012. 2, 4, 5, 7, 8\n[25] P. Kr\u00a8ahenb\u00a8uhl and V. Koltun. Parameter learning and con-\nvergent inference for dense random \ufb01elds. In Proc. Int. Conf.\nMach. Learn., 2013. 2\n[26] S. Lazebnik, C. Schmid, and J. Ponce.\nBeyond bags of\nfeatures: Spatial pyramid matching for recognizing natural\nscene categories. In Proc. IEEE Conf. Comp. Vis. Pattern\nRecogn., 2006. 2, 4, 5\n[27] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-\nmanan, P. Doll\u00b4ar, and C. L. Zitnick. Microsoft COCO: Com-\nmon objects in context. In Proc. Eur. Conf. Comp. Vis., 2014.\n7\n[28] C. Liu, J. Yuen, and A. Torralba. Sift \ufb02ow: Dense correspon-\ndence across scenes and its applications. IEEE T. Pattern\nAnalysis & Machine Intelligence, 2011. 8\n[29] F. Liu, C. Shen, and G. Lin. Deep convolutional neural \ufb01elds\nfor depth estimation from a single image.\nIn Proc. IEEE\nConf. Comp. Vis. Pattern Recogn., 2015. 2, 3\n[30] F. Liu, C. Shen, G. Lin, and I. Reid. Learning depth from sin-\ngle monocular images using deep convolutional neural \ufb01elds,\n2015. http://arxiv.org/abs/1502.07411. 2, 3\n[31] Z. Liu, X. Li, P. Luo, C. C. Loy, and X. Tang. Semantic\nimage segmentation via deep parsing network. In Proc. Int.\nConf. Comp. Vis., 2015. 8\n[32] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional\nnetworks for semantic segmentation. In Proc. IEEE Conf.\nComp. Vis. Pattern Recogn., 2015. 1, 2, 5, 6, 7, 8\n[33] M. Mostajabi, P. Yadollahpour, and G. Shakhnarovich. Feed-\nforward semantic segmentation with zoom-out features. In\nProc. IEEE Conf. Comp. Vis. Pattern Recogn., 2015. 2, 4, 8\n[34] R. Mottaghi, X. Chen, X. Liu, N.-G. Cho, S.-W. Lee, S. Fi-\ndler, R. Urtasun, et al. The role of context for object detection\nand semantic segmentation in the wild. In Proc. IEEE Conf.\nComp. Vis. Pattern Recogn., 2014. 8\nPublished in Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR) 2016.\n[35] H. Noh, S. Hong, and B. Han. Learning deconvolution net-\nwork for semantic segmentation. In Proc. Int. Conf. Comp.\nVis., 2015. 2, 5, 8\n[36] S. Nowozin and C. Lampert. Structured learning and pre-\ndiction in computer vision. Found. Trends. Comput. Graph.\nVis., 2011. 5\n[37] S. Nowozin, C. Rother, S. Bagon, T. Sharp, B. Yao, and\nP. Kohli. Decision tree \ufb01elds. In Proc. Int. Conf. Comp.\nVis., 2011. 6\n[38] P. H. Pinheiro and R. Collobert.\nRecurrent convolutional\nneural networks for scene parsing. In Proc. Int. Conf. Ma-\nchine Learn., 2014. 8\n[39] A. Rabinovich, A. Vedaldi, C. Galleguillos, E. Wiewiora,\nand S. Belongie.\nObjects in context.\nIn Proc. Int. Conf.\nComp. Vis., 2007. 2\n[40] A. G. Schwing and R. Urtasun.\nFully connected deep\nstructured networks, 2015. http://arxiv.org/abs/\n1503.02351. 2\n[41] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus. Indoor\nsegmentation and support inference from rgbd images. In\nProc. Eur. Conf. Comp. Vis., 2012. 7\n[42] K. Simonyan and A. Zisserman. Very deep convolutional\nnetworks for large-scale image recognition.\nIn Proc. Int.\nConf. Learning Representations, 2015. 6\n[43] C. A. Sutton and A. McCallum. Piecewise training for undi-\nrected models. In Proc. Conf. Uncertainty Arti\ufb01cial Intelli,\n2005. 2, 6\n[44] J. Tighe and S. Lazebnik. Finding things: Image parsing with\nregions and per-exemplar detectors.\nIn Proc. IEEE Conf.\nComp. Vis. Pattern Recogn., 2013. 8\n[45] J. Tompson, A. Jain, Y. LeCun, and C. Bregler. Joint training\nof a convolutional network and a graphical model for human\npose estimation. In Proc. Adv. Neural Info. Process. Syst.,\n2014. 3\n[46] A. Vedaldi and K. Lenc. MatConvNet \u2013 convolutional neural\nnetworks for matlab, 2014. 7\n[47] J. Winn and J. Shotton. The layout consistent random \ufb01eld\nfor recognizing and segmenting partially occluded objects.\nIn Proc. IEEE Conf. Comp. Vis. Pattern Recogn., 2006. 4\n[48] S. Zheng, S. Jayasumana, B. Romera-Paredes, V. Vineet,\nZ. Su, D. Du, C. Huang, and P. Torr. Conditional random\n\ufb01elds as recurrent neural networks. In Proc. Int. Conf. Comp.\nVis., 2015. 2, 4, 5, 8\nPublished in Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR) 2016.\n",
        "sentence": " Method mAP % Adelaide-Context-CNN-CRF-COCO (Lin et al., 2015) 77. 5 Adelaide-Context-CNN-CRF-COCO (Lin et al., 2015) 77.",
        "context": "DeepLab [3]\n89.1\n38.3\n88.1\n63.3\n69.7\n87.1\n83.1\n85.0\n29.3\n76.5\n56.5\n79.8\n77.9\n85.8\n82.4\n57.4\n84.3\n54.9\n80.5\n64.1\n72.7\nCRF-RNN [48]\n90.4\n55.3\n88.7\n68.4\n69.8\n88.3\n82.4\n85.1\n32.6\n78.5\n64.4\n79.6\n81.9\n86.4\n81.8\n58.6\n82.4\n53.5\n77.4\n70.1\n74.7\nBoxSup [5]\n89.8\n38.0\n80.7\n84.9\n27.2\n73.2\n57.5\n78.1\n79.2\n81.1\n77.1\n53.6\n74.0\n49.2\n71.7\n63.3\n69.6\nDeepLab [3]\n84.4\n54.5\n81.5\n63.6\n65.9\n85.1\n79.1\n83.4\n30.7\n74.1\n59.8\n79.0\n76.1\n83.2\n80.8\n59.7\n82.2\n50.4\n73.1\n63.7\n71.6\nCRF-RNN [48]\n87.5\n39.0\n79.7\n64.2\n68.3\n87.6\n80.8\n84.4\n30.4\n78.2\n78.4\n64.9\n70.3\n89.3\n83.5\n86.1\n31.7\n79.9\n62.6\n81.9\n80.0\n83.5\n82.3\n60.5\n83.2\n53.4\n77.9\n65.0\n74.1\nours\n90.6\n37.6\n80.0\n67.8\n74.4\n92.0\n85.2\n86.2\n39.1\n81.2\n58.9\n83.8\n83.9\n84.3\n84.8\n62.1\n83.2\n58.2\n80.8\n72.3\n75.3\nUsing VOC+COCO training data\nDeepLab [3]\n89.1\n38.3"
    },
    {
        "title": "Semantic image segmentation via deep parsing network",
        "author": [
            "Liu",
            "Ziwei",
            "Li",
            "Xiaoxiao",
            "Luo",
            "Ping",
            "Loy",
            "Chen Change",
            "Tang",
            "Xiaoou"
        ],
        "venue": "arXiv preprint arXiv:1509.02634,",
        "citeRegEx": "Liu et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "Liu et al\\.",
        "year": 2015,
        "abstract": "This paper addresses semantic image segmentation by incorporating rich\ninformation into Markov Random Field (MRF), including high-order relations and\nmixture of label contexts. Unlike previous works that optimized MRFs using\niterative algorithm, we solve MRF by proposing a Convolutional Neural Network\n(CNN), namely Deep Parsing Network (DPN), which enables deterministic\nend-to-end computation in a single forward pass. Specifically, DPN extends a\ncontemporary CNN architecture to model unary terms and additional layers are\ncarefully devised to approximate the mean field algorithm (MF) for pairwise\nterms. It has several appealing properties. First, different from the recent\nworks that combined CNN and MRF, where many iterations of MF were required for\neach training image during back-propagation, DPN is able to achieve high\nperformance by approximating one iteration of MF. Second, DPN represents\nvarious types of pairwise terms, making many existing works as its special\ncases. Third, DPN makes MF easier to be parallelized and speeded up in\nGraphical Processing Unit (GPU). DPN is thoroughly evaluated on the PASCAL VOC\n2012 dataset, where a single DPN model yields a new state-of-the-art\nsegmentation accuracy.",
        "full_text": "Semantic Image Segmentation via Deep Parsing Network\u2217\nZiwei Liu\u2020\nXiaoxiao Li\u2020\nPing Luo\nChen Change Loy\nXiaoou Tang\nDepartment of Information Engineering, The Chinese University of Hong Kong\n{lz013,lx015,pluo,ccloy,xtang}@ie.cuhk.edu.hk\nAbstract\nThis paper addresses semantic image segmentation by\nincorporating rich information into Markov Random Field\n(MRF), including high-order relations and mixture of label\ncontexts.\nUnlike previous works that optimized MRFs\nusing iterative algorithm, we solve MRF by proposing a\nConvolutional Neural Network (CNN), namely Deep Pars-\ning Network (DPN), which enables deterministic end-to-\nend computation in a single forward pass.\nSpeci\ufb01cally,\nDPN extends a contemporary CNN architecture to model\nunary terms and additional layers are carefully devised to\napproximate the mean \ufb01eld algorithm (MF) for pairwise\nterms. It has several appealing properties. First, different\nfrom the recent works that combined CNN and MRF, where\nmany iterations of MF were required for each training\nimage during back-propagation, DPN is able to achieve\nhigh performance by approximating one iteration of MF.\nSecond, DPN represents various types of pairwise terms,\nmaking many existing works as its special cases. Third,\nDPN makes MF easier to be parallelized and speeded up\nin Graphical Processing Unit (GPU). DPN is thoroughly\nevaluated on the PASCAL VOC 2012 dataset, where a sin-\ngle DPN model yields a new state-of-the-art segmentation\naccuracy of 77.5%.\n1. Introduction\nMarkov Random Field (MRF) or Conditional Random\nField (CRF) has achieved great successes in semantic im-\nage segmentation, which is one of the most challenging\nproblems in computer vision.\nExisting works such as\n[31, 29, 9, 34, 11, 2, 8, 25, 22] can be generally categorized\ninto two groups based on their de\ufb01nitions of the unary and\npairwise terms of MRF.\nIn the \ufb01rst group, researchers improved labeling ac-\ncuracy by exploring rich information to de\ufb01ne the pair-\nwise functions, including long-range dependencies [16, 17],\n\u2217This work has been accepted to appear in ICCV 2015. This is the pre-\nprinted version. Content may slightly change prior to the \ufb01nal publication.\n\u2020indicates shared \ufb01rst authorship.\nhigh-order potentials [37, 36], and semantic label contexts\n[21, 26, 38]. For example, Kr\u00a8ahenb\u00a8uhl et al. [16] attained\naccurate segmentation boundary by inferring on a fully-\nconnected graph.\nVineet et al. [37] extended [16] by\nde\ufb01ning both high-order and long-range terms between\npixels. Global or local semantic contexts between labels\nwere also investigated by [38]. Although they accomplished\npromising results, they modeled the unary terms as SVM or\nAdaboost, whose learning capacity becomes a bottleneck.\nThe learning and inference of complex pairwise terms are\noften expensive.\nIn the second group, people learned a strong unary clas-\nsi\ufb01er by leveraging the recent advances of deep learning,\nsuch as the Convolutional Neural Network (CNN). With\ndeep models, these works [23, 24, 25, 22, 3, 28, 39, 30, 19]\ndemonstrated encouraging results using simple de\ufb01nition of\nthe pairwise function or even ignore it. For instance, Long\net al. [22] transformed fully-connected layers of CNN into\nconvolutional layers, making accurate per-pixel classi\ufb01ca-\ntion possible using the contemporary CNN architectures\nthat were pre-trained on ImageNet [6].\nChen et al. [3]\nimproved [22] by feeding the outputs of CNN into a MRF\nwith simple pairwise potentials, but it treated CNN and\nMRF as separated components.\nA recent advance was\nobtained by [30], which jointly trained CNN and MRF by\npassing the error of MRF inference backward into CNN, but\niterative inference of MRF such as the mean \ufb01eld algorithm\n(MF) [27] is required for each training image during back-\npropagation (BP). Zheng et al. [39] further showed that\nthe procedure of MF inference can be represented as a\nRecurrent Neural Network (RNN), but their computational\ncosts are similar.\nWe found that directly combing CNN\nand MRF as above is inef\ufb01cient, because CNN typically\nhas millions of parameters while MRF infers thousands of\nlatent variables; and even worse, incorporating complex\npairwise terms into MRF becomes impractical, limiting the\nperformance of the entire system.\nThis work proposes a novel Deep Parsing Network\n(DPN), which is able to jointly train CNN and complex\npairwise terms.\nDPN has several appealing properties.\n(1) DPN solves MRF with a single feed-forward pass,\narXiv:1509.02634v2  [cs.CV]  24 Sep 2015\nreducing computational cost and meanwhile maintaining\nhigh performance. Speci\ufb01cally, DPN models unary terms\nby extending the VGG-16 network (VGG16) [32] pre-\ntrained on ImageNet, while additional layers are carefully\ndesigned to model complex pairwise terms. Learning of\nthese terms is transformed into deterministic end-to-end\ncomputation by BP, instead of embedding MF into BP as\n[30, 19] did. Although MF can be represented by RNN [39],\nit needs to recurrently compute the forward pass so as to\nachieve good performance and thus is time-consuming, e.g.\neach forward pass contains hundred thousands of weights.\nDPN approximates MF by using only one iteration. This\nis made possible by joint learning strong unary terms and\nrich pairwise information.\n(2) Pairwise terms determine\nthe graphical structure. In previous works, if the former is\nchanged, so is the latter as well as its inference procedure.\nBut with DPN, modifying the complexity of pairwise terms,\ne.g. range of pixels and contexts, is as simple as modifying\nthe receptive \ufb01elds of convolutions, without varying BP.\nDPN is able to represent multiple types of pairwise terms,\nmaking many previous works [3, 39, 30] as its special\ncases. (3) DPN approximates MF with convolutional and\npooling operations, which can be speeded up by low-\nrank approximation [14] and easily parallelized [4] in a\nGraphical Processing Unit (GPU).\nOur contributions are summarized as below.\n(1) A\nnovel DPN is proposed to jointly train VGG16 and rich\npairwise information, i.e. mixture of label contexts and\nhigh-order relations. Compared to existing deep models,\nDPN can approximate MF with only one iteration, reducing\ncomputational cost but still maintaining high performance.\n(2) We disclose that DPN represents multiple types of\nMRFs, making many previous works such as RNN [39] and\nDeepLab [3] as its special cases. (3) Extensive experiments\ninvestigate which component of DPN is crucial to achieve\nhigh performance. A single DPN model achieves a new\nstate-of-the-art accuracy of 77.5% on the PASCAL VOC\n2012 [7] test set. (4) We analyze the time complexity of\nDPN on GPU.\n2. Our Approach\nDPN learns MRF by extending VGG16 to model unary\nterms and additional layers are carefully designed for pair-\nwise terms.\nOverview MRF [10] is an undirected graph where each\nnode represents a pixel in an image I, and each edge repre-\nsents relation between pixels. Each node is associated with\na binary latent variable, yi\nu \u2208{0, 1}, indicating whether\na pixel i has label u. We have \u2200u \u2208L = {1, 2, ..., l},\nrepresenting a set of l labels. The energy function of MRF\nis written as\nE(y) =\nX\n\u2200i\u2208V\n\u03a6(yu\ni ) +\nX\n\u2200i,j\u2208E\n\u03a8(yu\ni , yv\nj ),\n(1)\nwhere y, V, and E denote a set of latent variables, nodes,\nand edges, respectively. \u03a6(yu\ni ) is the unary term, measuring\nthe cost of assigning label u to the i-th pixel. For instance,\nif pixel i belongs to the \ufb01rst category other than the second\none, we should have \u03a6(y1\ni ) < \u03a6(y2\ni ). Moreover, \u03a8(yu\ni , yv\nj )\nis the pairwise term that measures the penalty of assigning\nlabels u, v to pixels i, j respectively.\nIntuitively, the unary terms represent per-pixel classi\ufb01ca-\ntions, while the pairwise terms represent a set of smoothness\nconstraints. The unary term in Eqn.(1) is typically de\ufb01ned\nas\n\u03a6(yu\ni ) = \u2212ln p(yu\ni = 1|I)\n(2)\nwhere p(yu\ni = 1|I) indicates the probability of the presence\nof label u at pixel i, modeling by VGG16.\nTo simplify\ndiscussions, we abbreviate it as pu\ni . The smoothness term\ncan be formulated as\n\u03a8(yu\ni , yv\nj ) = \u00b5(u, v)d(i, j),\n(3)\nwhere the \ufb01rst term learns the penalty of global co-\noccurrence between any pair of labels, e.g. the output value\nof \u00b5(u, v) is large if u and v should not coexist, while the\nsecond term calculates the distances between pixels, e.g.\nd(i, j) = \u03c91\u2225Ii \u2212Ij\u22252 + \u03c92\u2225[xi yi] \u2212[xj yj]\u22252. Here,\nIi indicates a feature vector such as RGB values extracted\nfrom the i-th pixel, x, y denote coordinates of pixels\u2019\npositions, and \u03c91, \u03c92 are the constant weights.\nEqn.(3)\nimplies that if two pixels are close and look similar, they\nare encouraged to have labels that are compatible. It has\nbeen adopted by most of the recent deep models [3, 39, 30]\nfor semantic image segmentation.\nHowever, Eqn.(3) has two main drawbacks. First, its\n\ufb01rst term captures the co-occurrence frequency of two\nlabels in the training data, but neglects the spatial context\nbetween objects. For example, \u2018person\u2019 may appear beside\n\u2018table\u2019, but not at its bottom.\nThis spatial context is a\nmixture of patterns, as different object con\ufb01gurations may\nappear in different images.\nSecond, it de\ufb01nes only the\npairwise relations between pixels, missing their high-order\ninteractions.\nTo resolve these issues, we de\ufb01ne the smoothness term\nby leveraging rich information between pixels, which is one\nof the advantages of DPN over existing deep models. We\nhave\n\u03a8(yu\ni , yv\nj ) =\nK\nX\nk=1\n\u03bbk\u00b5k(i, u, j, v)\nX\n\u2200z\u2208Nj\nd(j, z)pv\nz.\n(4)\nThe \ufb01rst term in Eqn.(4) learns a mixture of local label\ncontexts, penalizing label assignment in a local region,\nwhere K is the number of components in mixture and \u03bbk\nis an indicator, determining which component is activated.\nWe de\ufb01ne \u03bbk \u2208{0, 1} and PK\nk=1 \u03bbk = 1. An intuitive\n(\ud835\udc8b\ud835\udc8b, \ud835\udc97\ud835\udc97) \n\u0dcd\ud835\udc85\ud835\udc85\ud835\udc8b\ud835\udc8b, \ud835\udc9b\ud835\udc9b\n\ud835\udc9b\ud835\udc9b\u2208\ud835\udf28\ud835\udf28\ud835\udc8b\ud835\udc8b\n\ud835\udc91\ud835\udc91\ud835\udc97\ud835\udc97 \n\ud835\udc9b\ud835\udc9b \ud835\udc8b\n\ud835\udc97\ud835\udc97) \n(\ud835\udc9b\ud835\udc9b, \ud835\udc97\ud835\udc97) \n(a) \n(\ud835\udc8a\ud835\udc8a, \ud835\udc96\ud835\udc96) \n\ud835\udf41\ud835\udf41\ud835\udc8c\ud835\udc8c\ud835\udc8a\ud835\udc8a, \ud835\udc96\ud835\udc96, \ud835\udc8b\ud835\udc8b, \ud835\udc97\ud835\udc97 \n(\ud835\udc8a\ud835\udc8a, \ud835\udc96\ud835\udc96) \n\ud835\udc8b\ud835\udc8b, \ud835\udc97\ud835\udc97,  \n\ud835\udc8b\ud835\udc8b\u2208\ud835\udf28\ud835\udf28\ud835\udc8a\ud835\udc8a \n(b) \n\ud835\udf41\ud835\udf41\ud835\udc8a\ud835\udc8a, \ud835\udc96\ud835\udc96, \ud835\udc8b\ud835\udc8b, \ud835\udc97\ud835\udc97 \n\ud835\udc85\ud835\udc85\ud835\udc8b\ud835\udc8b, \ud835\udc9b\ud835\udc9b\ud835\udc92\ud835\udc92\ud835\udc97\ud835\udc97,  \n\ud835\udc8b\ud835\udc8b \ud835\udc9b\ud835\udc9b\u2208\ud835\udf28\ud835\udf28\ud835\udc8b\ud835\udc8b \n(c) \n\ud835\udc8a\ud835\udc8a \n\ud835\udf41\ud835\udf41\ud835\udc8c\ud835\udc8c\ud835\udc8a\ud835\udc8a, \ud835\udc96\ud835\udc96, \ud835\udc8b\ud835\udc8b, \ud835\udc97\ud835\udc97,  \n(d) \n\ud835\udc78\ud835\udc78\ud835\udc97\ud835\udc97 \n\ud835\udc8b\ud835\udc8b \n\ud835\udc8e\ud835\udc8e \n\ud835\udc8e\ud835\udc8e \n\ud835\udc8f\ud835\udc8f \n\ud835\udc8f\ud835\udc8f \n\ud835\udc78\ud835\udc78\ud835\udc97\ud835\udc97\u2032 \n\ud835\udc8b\ud835\udc8b\u2208\ud835\udf28\ud835\udf28\ud835\udc8a\ud835\udc8a \n\ud835\udc8e\ud835\udc8e\u00d7 \ud835\udc8e\ud835\udc8e \n\ud835\udc8f\ud835\udc8f\u00d7 \ud835\udc8f\ud835\udc8f \nFigure 1: (a) Illustration of the pairwise terms in DPN. (b) explains the\nlabel contexts. (c) and (d) show that mean \ufb01eld update of DPN corresponds\nto convolutions.\nillustration is given in Fig.1 (b), where the dots in red and\nblue represent a center pixel i and its neighboring pixels j,\ni.e. j \u2208Ni, and (i, u) indicates assigning label u to pixel i.\nHere, \u00b5(i, u, j, v) outputs labeling cost between (i, u) and\n(j, v) with respect to their relative positions. For instance,\nif u, v represent \u2018person\u2019 and \u2018table\u2019, the learned penalties\nof positions j that are at the bottom of center i should be\nlarge. The second term basically models a triple penalty,\nwhich involves pixels i, j, and j\u2019s neighbors, implying that\nif (i, u) and (j, v) are compatible, then (i, u) should be also\ncompatible with j\u2019s nearby pixels (z, v), \u2200z \u2208Nj, as shown\nin Fig.1 (a).\nLearning parameters (i.e. weights of VGG16 and costs\nof label contexts) in Eqn.(1) is to minimize the distances\nbetween ground-truth label map and y, which needs to be\ninferred subject to the smoothness constraints.\nInference Overview Inference of Eqn.(1) can be\nobtained\nby\nthe\nmean\n\ufb01eld\n(MF)\nalgorithm\n[27],\nwhich\nestimates\nthe\njoint\ndistribution\nof\nMRF,\nP(y)= 1\nZ exp{\u2212E(y)},\nby\nusing\na\nfully-factorized\nproposal distribution, Q(y) = Q\n\u2200i\u2208V\nQ\n\u2200u\u2208Lqu\ni , where\neach qu\ni is a variable we need to estimate, indicating the\npredicted probability of assigning label u to pixel i. To\nsimplify the discussion, we denote \u03a6(yu\ni ) and \u03a8(yu\ni , yv\nj ) as\n\u03a6u\ni and \u03a8uv\nij , respectively. Q(y) is typically optimized by\nminimizing a free energy function [15] of MRF,\nF(Q)\n=\nX\n\u2200i\u2208V\nX\n\u2200u\u2208L\nqu\ni \u03a6u\ni +\nX\n\u2200i,j\u2208E\nX\n\u2200u\u2208L\nX\n\u2200v\u2208L\nqu\ni qv\nj \u03a8uv\nij\n+\nX\n\u2200i\u2208V\nX\n\u2200u\u2208L\nqu\ni ln qu\ni .\n(5)\nSpeci\ufb01cally, the \ufb01rst term in Eqn.(5) characterizes the cost\nof each pixel\u2019s predictions, while the second term char-\nacterizes the consistencies of predictions between pixels.\nThe last term is the entropy, measuring the con\ufb01dences of\npredictions. To estimate qu\ni , we differentiate Eqn.(5) with\nrespect to it and equate the resulting expression to zero. We\nthen have a closed-form expression,\nqu\ni \u221dexp\n\b\n\u2212(\u03a6u\ni +\nX\n\u2200j\u2208Ni\nX\n\u2200v\u2208L\nqv\nj \u03a8uv\nij )\n\t\n,\n(6)\nsuch that the predictions for each pixel is independently\nattained by repeating Eqn.(6), which implies whether pixel i\nhave label u is proportional to the estimated probabilities of\nall its neighboring pixels, weighted by their corresponding\nsmoothness penalties.\nSubstituting Eqn.(4) into (6), we\nhave\nqu\ni\n\u221d\nexp\nn\n\u2212\u03a6u\ni \u2212\nK\nX\nk=1\n\u03bbk\nX\n\u2200v\u2208L\nX\n\u2200j\u2208Ni\n(7)\n\u00b5k(i, u, j, v)\nX\n\u2200z\u2208Nj\nd(j, z)qv\nj qv\nz\no\n,\nwhere each qu\ni is initialized by the corresponding pu\ni in\nEqn.(2), which is the unary prediction of VGG16. Eqn.(7)\nsatis\ufb01es the smoothness constraints.\nIn the following, DPN approximates one iteration of\nEqn.(7) by decomposing it into two steps. Let Qv be a\npredicted label map of the v-th category. In the \ufb01rst step\nas shown in Fig.1 (c), we calculate the triple penalty term\nin (7) by applying a m \u00d7 m \ufb01lter on each position j, where\neach element of this \ufb01lter equals d(j, z)qv\nj , resulting in Qv\u2032.\nApparently, this step smoothes the prediction of pixel j with\nrespect to the distances between it and its neighborhood. In\nthe second step as illustrated in (d), the labeling contexts\ncan be obtained by convolving Qv\u2032 with a n \u00d7 n \ufb01lter, each\nelement of which equals \u00b5k(i, u, j, v), penalizing the triple\nrelations as shown in (a).\n3. Deep Parsing Network\nThis section describes the implementation of Eq.(7) in\na Deep Parsing Network (DPN). DPN extends VGG16 as\nunary term and additional layers are designed to approxi-\nmate one iteration of MF inference as the pairwise term.\nThe hyper-parameters of VGG16 and DPN are compared in\nTable 1.\nVGG16 As listed in Table 1 (a), the \ufb01rst row represents\nthe name of layer and \u2018x-y\u2019 in the second row represents\nthe size of the receptive \ufb01eld and the stride of convolution,\nrespectively. For instance, \u20183-1\u2019 in the convolutional layer\nimplies that the receptive \ufb01eld of each \ufb01lter is 3\u00d73 and it\nis applied on every single pixel of an input feature map,\nwhile \u20182-2\u2019 in the max-pooling layer indicates each feature\nmap is pooled over every other pixel within a 2\u00d72 local\nregion. The last three rows show the number of the output\nfeature maps, activation functions, and the size of output\n(a) VGG16: 224\u00d7224\u00d73 input image; 1\u00d71000 output labels\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\nlayer\n\ufb01lter\u2013stride\n#channel\nactivation\nsize\n2\u00d7conv\n3-1\n64\nrelu\n224\nmax\n2-2\n64\nidn\n112\n2\u00d7conv\n3-1\n128\nrelu\n112\nmax\n2-2\n128\nidn\n56\n3\u00d7conv\n3-1\n256\nrelu\n56\nmax\n2-2\n256\nidn\n28\n3\u00d7conv\n3-1\n512\nrelu\n28\nmax\n2-2\n512\nidn\n14\n3\u00d7conv\n3-1\n512\nrelu\n14\nmax\n2-2\n512\nidn\n7\n2\u00d7fc\n-\n1\nrelu\n4096\nfc\n-\n1\nsoft\n1000\n(b) DPN: 512\u00d7512\u00d73 input image; 512\u00d7512\u00d721 output label maps\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\nlayer\n\ufb01lter\u2013stride\n#channel\nactivation\nsize\n2\u00d7conv\n3-1\n64\nrelu\n512\nmax\n2-2\n64\nidn\n256\n2\u00d7conv\n3-1\n128\nrelu\n256\nmax\n2-2\n128\nidn\n128\n3\u00d7conv\n3-1\n256\nrelu\n128\nmax\n2-2\n256\nidn\n64\n3\u00d7conv\n3-1\n512\nrelu\n64\n3\u00d7conv\n5-1\n512\nrelu\n64\nconv\n25-1\n4096\nrelu\n64\nconv\n1-1\n4096\nrelu\n64\nconv\n1-1\n21\nsigm\n512\nlconv\n50-1\n21\nlin\n512\nconv\n9-1\n105\nlin\n512\nbmin\n1-1\n21\nidn\n512\nsum\n1-1\n21\nsoft\n512\nTable 1: The comparisons between the network architectures of VGG16 and DPN, as shown in (a) and (b) respectively. Each table contains \ufb01ve rows,\nrepresenting the \u2018name of layer\u2019, \u2018receptive \ufb01eld of \ufb01lter\u2019\u2212\u2018stride\u2019, \u2018number of output feature maps\u2019, \u2018activation function\u2019 and \u2018size of output feature\nmaps\u2019, respectively. Furthermore, \u2018conv\u2019, \u2018lconv\u2019,\u2018max\u2019, \u2018bmin\u2019, \u2018fc\u2019, and \u2018sum\u2019 represent the convolution, local convolution, max pooling, block min\npooling, fully connection, and summation, respectively. Moreover, \u2018relu\u2019, \u2018idn\u2019, \u2018soft\u2019, \u2018sigm\u2019, and \u2018lin\u2019 represent the activation functions, including recti\ufb01ed\nlinear unit [18], identity, softmax, sigmoid, and linear, respectively.\nfeature maps, respectively. As summarized in Table 1 (a),\nVGG16 contains thirteen convolutional layers, \ufb01ve max-\npooling layers, and three fully-connected layers.\nThese\nlayers can be partitioned into twelve groups, each of which\ncovers one or more homogenous layers. For example, the\n\ufb01rst group comprises two convolutional layers with 3\u00d73\nreceptive \ufb01eld and 64 output feature maps, each of which\nis 224\u00d7224.\n3.1. Modeling Unary Terms\nTo make full use of VGG16, which is pre-trained by\nImageNet, we adopt all its parameters to initialize the\n\ufb01lters of the \ufb01rst ten groups of DPN. To simplify the\ndiscussions, we take PASCAL VOC 2012 (VOC12) [7] as\nan example. Note that DPN can be easily adapted to any\nother semantic image segmentation dataset by modifying\nits hyper-parameters. VOC12 contains 21 categories and\neach image is rescaled to 512\u00d7512 in training. Therefore,\nDPN needs to predict totally 512\u00d7512\u00d721 labels, i.e. one\nlabel for each pixel. To this end, we extends VGG16 in two\naspects.\nIn particular, let ai and bi denote the i-th group in Table\n1 (a) and (b), respectively. First, we increase resolution of\nVGG16 by removing its max pooling layers at a8 and a10,\nbecause most of the information is lost after pooling, e.g.\na10 reduces the input size by 32 times, i.e. from 224\u00d7224\nto 7\u00d77. As a result, the smallest size of feature map in\nDPN is 64\u00d764, keeping much more information compared\nwith VGG16. Note that the \ufb01lters of b8 are initialized as\nthe \ufb01lters of a9, but the 3\u00d73 receptive \ufb01eld is padded into\n5\u00d75 as shown in Fig.2 (a), where the cells in white are the\noriginal values of the a9\u2019s \ufb01lter and the cells in gray are\nzeros. This is done because a8 is not presented in DPN, such\nthat each \ufb01lter in a9 should be convolved on every other\npixel of a7. To maintain the convolution with one stride, we\npad the \ufb01lters with zeros. Furthermore, the feature maps in\nb11 are up-sampled to 512\u00d7512 by bilinear interpolation.\n25 \n5 \n(a) \n(b) \n5 \n512 \n512 \n21 channels \n(c) Local \nconvolution \nof b12 \n(\ud835\udc8b\ud835\udc8b, \ud835\udc97\ud835\udc97) \n. \n50 \n512 \n512 \n(i, j) \n. \n9 \n(a) Convolution of b13 \n9 \n. (\ud835\udc8a\ud835\udc8a, \ud835\udc96\ud835\udc96= \ud835\udfcf\ud835\udfcf) \n512 \n(b) Max pooling of b14 \n5 \n512 \n512 \n105 \n5 \n105 \n(\ud835\udc8b\ud835\udc8b, \ud835\udc97\ud835\udc97) \n\ud835\udc97\ud835\udc97 \n\ud835\udc97\ud835\udc97 \nb12 \nb13 \n21 channels \n\ud835\udc96\ud835\udc96\u2208{\ud835\udfcf\ud835\udfcf, \u2026 , \ud835\udfd0\ud835\udfd0\ud835\udfd0\ud835\udfd0} \n50 \n\ud835\udc8c\ud835\udc8c(\ud835\udc8b\ud835\udc8b,\ud835\udc97\ud835\udc97)  \nFigure 2: (a) and (b) show the padding of the \ufb01lters. (c) illustrates local\nconvolution of b12.\nSince DPN is trained with label maps of the entire images,\nthe missing information in the preceding layers of b11 can\nbe recovered by BP.\nSecond, two fully-connected layers at a11 are trans-\nformed to two convolutional layers at b9 and b10, respec-\ntively. As shown in Table 1 (a), the \ufb01rst \u2018fc\u2019 layer learns\n7\u00d77\u00d7512\u00d74096 parameters, which can be altered to 4096\n\ufb01lters in b9, each of which is 25\u00d725\u00d7512. Since a8 and a10\nhave been removed, the 7\u00d77 receptive \ufb01eld is padded into\n25\u00d725 similar as above and shown in Fig.2 (b). The second\n\u2018fc\u2019 layer learns a 4096\u00d74096 weight matrix, corresponding\nto 4096 \ufb01lters in b10. Each \ufb01lter is 1\u00d71\u00d74096.\nOverall, b11 generates the unary labeling results, pro-\nducing twenty-one 512\u00d7512 feature maps, each of which\nrepresents the probabilistic label map of each category.\n3.2. Modeling Smoothness Terms\nThe last four layers of DPN, i.e. from b12 to b15, are\ncarefully designed to smooth the unary labeling results.\n\u2022 b12 As listed in Table 1 (b), \u2018lconv\u2019 in b12 indicates\na locally convolutional layer, which is widely used in\nface recognition [33, 35] to capture different information\nfrom different facial positions. Similarly, distinct spatial\npositions of b12 have different \ufb01lters, and each \ufb01lter is\nshared across 21 input channels, as shown in Fig.2 (c). It\ncan be formulated as\no12\n(j,v) = lin(k(j,v) \u2217o11\n(j,v)),\n(8)\n25 \n5 \n512 \n512 \n(\ud835\udc8b\ud835\udc8b, \ud835\udc97\ud835\udc97) \n. \n50 \n512 \n512 \n. \n9 \n(a) Convolution of b13 \n9 \n. (\ud835\udc8a\ud835\udc8a, \ud835\udc96\ud835\udc96= \ud835\udfcf\ud835\udfcf) \n512 \n(b) Pooling in b14 \n21 channels \n5 \n512 \n512 \n105 \n5 \n105 \n(\ud835\udc8b\ud835\udc8b, \ud835\udc97\ud835\udc97) \ud835\udc97\n\ud835\udc97\ud835\udc97 \nb12 \nb13 \n21 channels \n\ud835\udc96\ud835\udc96\u2208{\ud835\udfcf\ud835\udfcf, \u2026 , \ud835\udfd0\ud835\udfd0\ud835\udfd0\ud835\udfd0} \n\ud835\udc8c\ud835\udc8c(\ud835\udc8b\ud835\udc8b,\ud835\udc97\ud835\udc97)  \nFigure 3: (a) and (b) illustrates the convolutions of b13 and the poolings\nin b14.\nwhere lin(x) = ax + b representing the linear activation\nfunction, \u2018\u2217\u2019 is the convolutional operator, and k(j,v) is\na 50\u00d750\u00d71 \ufb01lter at position j of channel v.\nWe have\nk(j,1) = k(j,2) = ... = k(j,21) shared across 21 channels.\no11\n(j,v) indicates a local patch in b11, while o12\n(j,v) is the\ncorresponding output of b12.\nSince b12 has stride one,\nthe result of kj \u2217o11\n(j,v) is a scalar. In summary, b12 has\n512\u00d7512 different \ufb01lters and produces 21 output feature\nmaps.\nEqn.(8) implements the triple penalty of Eqn.(7). Recall\nthat each output feature map of b11 indicates a probabilistic\nlabel map of a speci\ufb01c object appearing in the image. As\na result, Eqn.(8) suggests that the probability of object v\npresented at position j is updated by weighted averaging\nover the probabilities at its nearby positions. Thus, as shown\nin Fig.1 (c), o11\n(j,v) corresponds to a patch of Qv centered at\nj, which has values pv\nz, \u2200z \u2208N 50\u00d750\nj\n. Similarly, k(j,v)\nis initialized by d(j, z)pv\nj, implying each \ufb01lter captures\ndissimilarities between positions. These \ufb01lters remain \ufb01xed\nduring BP, other than learned as in conventional CNN1.\n\u2022 b13 As shown in Table 1 (b) and Fig.3 (a), b13 is\na convolutional layer that generates 105 feature maps by\nusing 105 \ufb01lters of size 9\u00d79\u00d721. For example, the value\nof (i, u = 1) is attained by applying a 9\u00d79\u00d721 \ufb01lter at\npositions {(j, v = 1, ..., 21)}. In other words, b13 learns\na \ufb01lter for each category to penalize the probabilistic label\nmaps of b12, corresponding to the local label contexts in\nEqn.(7) by assuming K = 5 and n = 9, as shown in Fig.1\n(d).\n\u2022 b14 As illustrated in Table 1 and Fig.3 (b), b14 is a\nblock min pooling layer that pools over every 1\u00d71 region\nwith one stride across every 5 input channels, leading to\n21 output channels, i.e. 105\u00f75=21.\nb14 activates the\ncontextual pattern with the smallest penalty.\n\u2022 b15 This layer combines both the unary and smooth-\nness terms by summing the outputs of b11 and b14 in an\n1Each \ufb01lter in b12 actually represents a distance metric between pixels\nin a speci\ufb01c region. In VOC12, the patterns of all the training images\nin a speci\ufb01c region are heterogenous, because of various object shapes.\nTherefore, we initialize each \ufb01lter with Euclidean distance. Nevertheless,\nEqn.(8) is a more general form than the triple penalty in Eqn.(7), i.e. \ufb01lters\nin (8) can be automatically learned from data, if the patterns in a speci\ufb01c\nregion are homogenous, such as face or human images, which have more\nregular shapes than images in VOC12.\nelement-wise manner similar to Eqn.(7),\no15\n(i,u) =\nexp\n\b\nln(o11\n(i,u)) \u2212o14\n(i,u)\n\t\nP21\nu=1 exp\n\b\nln(o11\n(i,u)) \u2212o14\n(i,u)\n\t,\n(9)\nwhere probability of assigning label u to pixel i is normal-\nized over all the labels.\nRelation to Previous Deep Models Many existing deep\nmodels such as [39, 3, 30] employed Eqn.(3) as the pairwise\nterms, which are the special cases of Eqn.(7). To see this,\nlet K=1 and j=i, the right hand side of (7) reduces to\nexp{\u2212\u03a6u\ni \u2212\nX\nv\u2208L\n\u03bb1\u00b51(i, u, i, v)\nX\nz\u2208Ni\nd(i, z)pv\ni pv\nz}\n=\nexp{\u2212\u03a6u\ni \u2212\nX\nv\u2208L\n\u00b5(u, v)\nX\nz\u2208Ni,z\u0338=i\nd(i, z)pv\nz},\n(10)\nwhere \u00b5(u, v) and d(i, z) represent the global label co-\noccurrence and pairwise pixel similarity of Eqn.(3), respec-\ntively. This is because \u03bb1 is a constant, d(i, i) = 0, and\n\u00b5(i, u, i, v) = \u00b5(u, v). Eqn.(10) is the corresponding MF\nupdate equation of (3).\n3.3. Learning Algorithms\nLearning The \ufb01rst ten groups of DPN are initialized\nby VGG162, while the last four groups can be initialized\nrandomly. DPN is then \ufb01ne-tuned in an incremental manner\nwith four stages. During \ufb01ne-tuning, all these stages solve\nthe pixelwise softmax loss [22], but updating different sets\nof parameters.\nFirst, we add a loss function to b11 and \ufb01ne-tune the\nweights from b1 to b11 without the last four groups, in\norder to learn the unary terms.\nSecond, to learn the\ntriple relations, we stack b12 on top of b11 and update its\nparameters (i.e. \u03c91, \u03c92 in the distance measure), but the\nweights of the preceding groups (i.e. b1\u223cb11) are \ufb01xed.\nThird, b13 and b14 are stacked onto b12 and similarly,\ntheir weights are updated with all the preceding parameters\n\ufb01xed, so as to learn the local label contexts. Finally, all the\nparameters are jointly \ufb01ne-tuned.\nImplementation DPN transforms Eqn.(7) into convo-\nlutions and poolings in the groups from b12 to b15, such\nthat \ufb01ltering at each pixel can be performed in a parallel\nmanner. Assume we have f input and f \u2032 output feature\nmaps, N \u00d7 N pixels, \ufb01lters with s \u00d7 s receptive \ufb01eld, and a\nmini-batch with M samples. b12 takes a total f \u00b7N 2 \u00b7s2 \u00b7M\noperations, b13 takes f \u00b7 f \u2032 \u00b7 N 2 \u00b7 s2 \u00b7 M operations,\nwhile both b14 and b15 require f \u00b7 N 2 \u00b7 M operations.\nFor example, when M=10 as in our experiment, we have\n21\u00d75122\u00d7502\u00d710=1.3\u00d71011 operations in b12, which\n2We use the released VGG16 model, which is public available at\nhttp://www.robots.ox.ac.uk/\u02dcvgg/research/very_\ndeep/\nhas the highest complexity in DPN. We parallelize these\noperations using matrix multiplication on GPU as [4] did,\nb12 can be computed within 30ms. The total runtime of the\nlast four layers of DPN is 75ms. Note that convolutions in\nDPN can be further speeded up by low-rank decompositions\n[14] of the \ufb01lters and model compressions [13].\nHowever, direct calculation of Eqn.(7) is accelerated by\nfast Gaussian \ufb01ltering [1]. For a mini-batch of ten 512\u00d7512\nimages, a recently optimized implementation [16] takes 12\nseconds on CPU to compute one iteration of (7). Therefore,\nDPN makes (7) easier to be parallelized and speeded up.\n4. Experiments\nDataset We evaluate the proposed approach on the PAS-\nCAL VOC 2012 (VOC12) [7] dataset, which contains 20\nobject categories and one background category. Following\nprevious works such as [12, 22, 3], we employ 10, 582\nimages for training, 1, 449 images for validation, and 1, 456\nimages for testing.\nEvaluation Metrics All existing works employed mean\npixelwise intersection-over-union (denoted as mIoU) [22]\nto evaluate their performance. To fully examine the effec-\ntiveness of DPN, we introduce another three metrics, in-\ncluding tagging accuracy (TA), localization accuracy (LA),\nand boundary accuracy (BA). (1) TA compares the pre-\ndicted image-level tags with the ground truth tags, calculat-\ning the accuracy of multi-class image classi\ufb01cation. (2) LA\nevaluates the IoU between the predicted object bounding\nboxes3 and the ground truth bounding boxes (denoted as\nbIoU), measuring the precision of object localization. (3)\nFor those objects that have been correctly localized, we\ncompare the predicted object boundary with the ground\ntruth boundary, measuring the precision of semantic bound-\nary similar to [12].\nComparisons\nDPN\nis\ncompared\nwith\nthe\nbest-\nperforming methods on VOC12, including FCN [22],\nZoom-out [25], DeepLab [3], WSSL [28], BoxSup [5],\nPiecewise [19], and RNN [39].\nAll these methods are\nbased on CNNs and MRFs, and trained on VOC12 data\nfollowing [22]. They can be grouped according to different\naspects:\n(1) joint-train:\nPiecewise and RNN; (2) w/o\njoint-train: DeepLab, WSSL, FCN, and BoxSup; (3) pre-\ntrain on COCO: RNN, WSSL, and BoxSup.\nThe \ufb01rst\nand the second groups are the methods with and without\njoint training CNNs and MRFs, respectively. Methods in\nthe last group also employed MS-COCO [20] to pre-train\ndeep models. To conduct a comprehensive comparison, the\nperformance of DPN are reported on both settings, i.e., with\nand without pre-training on COCO.\nIn the following, Sec.4.1 investigates the effectiveness of\ndifferent components of DPN on the VOC12 validation set.\n3They are the bounding boxes of the predicted segmentation regions.\nReceptive Field\nbaseline\n10\u00d710\n50\u00d750\n100\u00d7100\nmIoU (%)\n63.4\n63.8\n64.7\n64.3\n(a) Comparisons between different receptive \ufb01elds of b12.\nReceptive Field\n1\u00d71\n5\u00d75\n9\u00d79\n9\u00d79 mixtures\nmIoU (%)\n64.8\n66.0\n66.3\n66.5\n(b) Comparisons between different receptive \ufb01elds of b13.\nPairwise Terms\nDSN [30]\nDeepLab [3]\nDPN\nimprovement (%)\n2.6\n3.3\n5.4\n(c) Comparing pairwise terms of different methods.\nTable 2: Ablation study of hyper-parameters.\nSec.4.2 compares DPN with the state-of-the-art methods on\nthe VOC12 test set.\n4.1. Effectiveness of DPN\nAll the models evaluated in this section are trained and\ntested on VOC12.\nTriple Penalty\nThe receptive \ufb01eld of b12 indicates\nthe range of triple relations for each pixel. We examine\ndifferent settings of the receptive \ufb01elds, including \u201810\u00d710\u2019,\n\u201850\u00d750\u2019, and \u2018100\u00d7100\u2019, as shown in Table 2 (a), where\n\u201850\u00d750\u2019 achieves the best mIoU, which is sightly better\nthan \u2018100\u00d7100\u2019. For a 512\u00d7512 image, this result implies\nthat 50\u00d750 neighborhood is suf\ufb01cient to capture relations\nbetween pixels, while smaller or larger regions tend to\nunder-\ufb01t or over-\ufb01t the training data. Moreover, all models\nof triple relations outperform the \u2018baseline\u2019 method that\nmodels dense pairwise relations, i.e. VGG16+denseCRF\n[16].\nLabel Contexts\nReceptive \ufb01eld of b13 indicates the\nrange of local label context. To evaluate its effectiveness,\nwe \ufb01x the receptive \ufb01eld of b12 as 50\u00d750. As summarized\nin Table 2 (b), \u20189\u00d79 mixtures\u2019 improves preceding settings\nby 1.7, 0.5, and 0.2 percent respectively. We observe large\ngaps exist between \u20181\u00d71\u2019 and \u20185\u00d75\u2019. Note that the 1\u00d71\nreceptive \ufb01eld of b13 corresponds to learning a global label\nco-occurrence without considering local spatial contexts.\nTable 2 (c) shows that the pairwise terms of DPN are more\neffective than DSN and DeepLab4.\nMore importantly, mIoU of all the categories can be\nimproved when increasing the size of receptive \ufb01eld and\nlearning a mixture. Speci\ufb01cally, for each category, the im-\nprovements of the last three settings in Table 2 (b) over the\n\ufb01rst one are 1.2\u00b10.2, 1.5\u00b10.2, and 1.7\u00b10.3, respectively.\nWe also visualize the learned label compatibilities and\ncontexts in Fig.4 (a) and (b), respectively. (a) is obtained\nby summing each \ufb01lter in b13 over 9\u00d79 region, indicating\nhow likely a column object would present when a row\nobject is presented. Blue represents high possibility. (a)\n4The other deep models such as RNN and Piecewise did not report the\nexact imrprovements after combining unary and pairwise terms.\nbkg\nareo\nbike\ntv\ntrain\nsofa\nbird\nboat\nbottle\nbus\ncar\ncat\nchair\ncow\ntable\ndog\nhorse\nsheep\nmbike\nperson\nplant\nbkg\nareo\nbike\nbird\nboat\nbottle\nbus\ncar\ncat\nchair\ncow\ntable\ndog\nhorse\nmbike\nperson\nplant\nsheep\nsofa\ntrain\ntv\n(a)\n(b)\ntrain : bkg\nperson : mbike\nbottle : bottle\nchair : person\n \n \nFigure 4: Visualization of (a) learned label compatibility (b) learned\ncontextual information. (Best viewed in color)\n(a) Original Image\n(d) +Triple Penalty\n(c) Unary Term\n(e) +Label Contexts\n(f) +Joint Tuning\n(b) Ground Truth\nFigure 5: Step-by-step visualization of DPN. (Best viewed in color)\nis non-symmetry. For example, when \u2018horse\u2019 is presented,\n\u2018person\u2019 is more likely to present than the other objects.\nAlso, \u2018chair\u2019 is compatible with \u2018table\u2019 and \u2018bkg\u2019 is com-\npatible with all the objects. (b) visualizes some contextual\npatterns, where \u2018A:B\u2019 indicates that when \u2018A\u2019 is presented,\nwhere \u2018B\u2019 is more likely to present. For example, \u2018bkg\u2019 is\naround \u2018train\u2019, \u2018motor bike\u2019 is below \u2018person\u2019, and \u2018person\u2019\nis sitting on \u2018chair\u2019.\nIncremental Learning As discussed in Sec.3.3, DPN is\ntrained in an incremental manner. The right hand side of Ta-\nble 3 (a) demonstrates that each stage leads to performance\ngain compared to its previous stage. For instance, \u2018triple\npenalty\u2019 improves \u2018unary term\u2019 by 2.3 percent, while \u2018label\ncontexts\u2019 improves \u2018triple penalty\u2019 by 1.8 percent. More\nimportantly, joint \ufb01ne-tuning all the components (i.e. unary\nterms and pairwise terms) in DPN achieves another gain\nof 1.3 percent. A step-by-step visualization is provided in\nFig.5.\nWe also compare \u2018incremental learning\u2019 with \u2018joint\nlearning\u2019, which \ufb01ne-tunes all the components of DPN at\nthe same time. The training curves of them are plotted in\nFig.6 (a), showing that the former leads to higher and more\nstable accuracies with respect to different iterations, while\nthe latter may get stuck at local minima. This difference\nis easy to understand, because incremental learning only\nintroduces new parameters until all existing parameters\n0\n1000\n2000\n3000\n4000\n5000\n0.64\n0.66\n0.68\n0.7\nNumber of Training Iterations\nmIoU\n \n \nIncremental Learning\nJoint Learning\n0\n1\n2\n3\n4\n5\n0.62\n0.64\n0.66\n0.68\n0.7\nNumber of MF Iterations\nmIoU\n \n \nDPN pairwise terms\ndenseCRF [16]\n(a)\n(b)\nFigure 6:\nAblation study of (a) training strategy (b) required MF\niterations. (Best viewed in color)\n65%\n67%\n69%\n71%\n73%\nmean BBox IoU\n91%\n92%\n93%\n94%\nmean Pixel Acc\n(a) TA\n(b) LA \n(c) BA\n94%\n95%\n96%\n97%\n98%\nmean Tag Acc\nUnary Term\nTriple Penalty\nLabel Contexts\nJoint Tuning\n96.0%\n96.5%\n97.1%\n96.4%\n67.0%\n69.5%70.1%\n72.1%\n91.9%\n93.1%93.1%\n93.3%\nFigure 7: Stage-wise analysis of (a) mean tagging accuracy (b) mean\nlocalization accuracy (c) mean boundary accuracy.\nhave been \ufb01ne-tuned.\nOne-iteration MF DPN approximates one iteration of\nMF. Fig.6 (b) illustrates that DPN reaches a good accuracy\nwith one MF iteration. A CRF [16] with dense pairwise\nedges needs more than 5 iterations to converge.\nIt also\nhas a large gap compared to DPN. Note that the existing\ndeep models such as [3, 39, 30] required 5\u223c10 iterations to\nconverge as well.\nDifferent Components Modeling Different Informa-\ntion We further evaluate DPN using three metrics. The\nresults are given in Fig.7. For example, (a) illustrates that\nthe tagging accuracy can be improved in the third stage, as\nit captures label co-occurrence with a mixture of contextual\npatterns. However, TA is decreased a little after the \ufb01nal\nstage.\nSince joint tuning maximizes segmentation accu-\nracies by optimizing all components together, extremely\nsmall objects, which rarely occur in VOC training set,\nare discarded.\nAs shown in (b), accuracies of object\nlocalization are signi\ufb01cantly improved in the second and the\n\ufb01nal stages. This is intuitive because the unary prediction\ncan be re\ufb01ned by long-range and high-order pixel relations,\nand joint training further improves results.\n(c) discloses\nthat the second stage also captures object boundary, since\nit measures dissimilarities between pixels.\nPer-class Analysis\nTable 3 (a) reports the per-class\naccuracies of four evaluation metrics, where the \ufb01rst four\nrows represent the mIoU of four stages, while the last\nthree rows represent TA, LA, and BA, respectively.\nWe\nhave several valuable observations, which motivate future\nresearches. (1) Joint training bene\ufb01ts most of the categories,\nexcept animals such as \u2018bird\u2019, \u2018cat\u2019, and \u2018cow\u2019.\nSome\ninstances of these categories are extremely small so that\nareo\nbike\nbird\nboat\nbottle bus\ncar\ncat\nchair\ncow\ntable\ndog\nhorse mbike person plant\nsheep sofa\ntrain\ntv\nAvg.\nUnary Term (mIoU)\n77.5\n34.1\n76.2\n58.3\n63.3\n78.1\n72.5\n76.5\n26.6\n59.9\n40.8\n70.0\n62.9\n69.3\n76.3\n39.2\n70.4\n37.6\n72.5\n57.3\n62.4\n+ Triple Penalty\n82.3\n35.9\n80.6\n60.1\n64.8\n79.5\n74.1\n80.9\n27.9\n63.5\n40.4\n73.8\n66.7\n70.8\n79.0\n42.0\n74.1\n39.1\n73.2\n58.5\n64.7\n+ Label Contexts\n83.2\n35.6\n82.6\n61.6\n65.5\n80.5\n74.3\n82.6\n29.9\n67.9\n47.5\n75.2\n70.3\n71.4\n79.6\n42.7\n77.8\n40.6\n75.3\n59.1\n66.5\n+ Joint Tuning\n84.8\n37.5\n80.7\n66.3\n67.5\n84.2\n76.4\n81.5\n33.8\n65.8\n50.4\n76.8\n67.1\n74.9\n81.1\n48.3\n75.9\n41.8\n76.6\n60.4\n67.8\nTA (tagging Acc.)\n98.8\n97.9\n98.4\n97.7\n96.1\n98.6\n95.2\n96.8\n90.1\n97.5\n95.7\n96.7\n96.3\n98.1\n93.3\n96.1\n98.7\n92.2\n97.4\n96.3\n96.4\nLA (bIoU)\n81.7\n76.3\n75.5\n70.3\n54.4\n86.4\n70.6\n85.6\n51.8\n79.6\n57.1\n83.3\n79.2\n80.0\n74.1\n53.1\n79.1\n68.4\n76.3\n58.8\n72.1\nBA (boundary Acc.)\n95.9\n83.9\n96.9\n92.6\n93.8\n94.0\n95.7\n95.6\n89.5\n93.3\n91.4\n95.2\n94.2\n92.7\n94.5\n90.4\n94.8\n90.5\n93.7\n96.6\n93.3\n(a) Per-class results on VOC12 val.\nareo\nbike\nbird\nboat\nbottle bus\ncar\ncat\nchair\ncow\ntable\ndog\nhorse mbike person plant\nsheep sofa\ntrain\ntv\nmIoU\nFCN [22]\n76.8\n34.2\n68.9\n49.4\n60.3\n75.3\n74.7\n77.6\n21.4\n62.5\n46.8\n71.8\n63.9\n76.5\n73.9\n45.2\n72.4\n37.4\n70.9\n55.1\n62.2\nZoom-out [25]\n85.6\n37.3\n83.2\n62.5\n66.0\n85.1\n80.7\n84.9\n27.2\n73.2\n57.5\n78.1\n79.2\n81.1\n77.1\n53.6\n74.0\n49.2\n71.7\n63.3\n69.6\nPiecewise [19]\n87.5\n37.7\n75.8\n57.4\n72.3\n88.4\n82.6\n80.0\n33.4\n71.5\n55.0\n79.3\n78.4\n81.3\n82.7\n56.1\n79.8\n48.6\n77.1\n66.3\n70.7\nDeepLab [3]\n84.4\n54.5\n81.5\n63.6\n65.9\n85.1\n79.1\n83.4\n30.7\n74.1\n59.8\n79.0\n76.1\n83.2\n80.8\n59.7\n82.2\n50.4\n73.1\n63.7\n71.6\nRNN [39]\n87.5\n39.0\n79.7\n64.2\n68.3\n87.6\n80.8\n84.4\n30.4\n78.2\n60.4\n80.5\n77.8\n83.1\n80.6\n59.5\n82.8\n47.8\n78.3\n67.1\n72.0\nWSSL\u2020 [28]\n89.2\n46.7\n88.5\n63.5\n68.4\n87.0\n81.2\n86.3\n32.6\n80.7\n62.4\n81.0\n81.3\n84.3\n82.1\n56.2\n84.6\n58.3\n76.2\n67.2\n73.9\nRNN\u2020 [39]\n90.4\n55.3\n88.7\n68.4\n69.8\n88.3\n82.4\n85.1\n32.6\n78.5\n64.4\n79.6\n81.9\n86.4\n81.8\n58.6\n82.4\n53.5\n77.4\n70.1\n74.7\nBoxSup\u2020 [5]\n89.8\n38.0\n89.2\n68.9\n68.0\n89.6\n83.0\n87.7\n34.4\n83.6\n67.1\n81.5\n83.7\n85.2\n83.5\n58.6\n84.9\n55.8\n81.2\n70.7\n75.2\nDPN\n87.7\n59.4\n78.4\n64.9\n70.3\n89.3\n83.5\n86.1\n31.7\n79.9\n62.6\n81.9\n80.0\n83.5\n82.3\n60.5\n83.2\n53.4\n77.9\n65.0\n74.1\nDPN\u2020\n89.0\n61.6\n87.7\n66.8\n74.7\n91.2\n84.3\n87.6\n36.5\n86.3\n66.1\n84.4\n87.8\n85.6\n85.4\n63.6\n87.3\n61.3\n79.4\n66.4\n77.5\n(b) Per-class results on VOC12 test. The approaches pre-trained on COCO [20] are marked with \u2020.\nTable 3: Per-class results on VOC12.\njoint training discards them for smoother results.\n(2)\nTraining DPN with pixelwise label maps implicitly models\nimage-level tags, since it achieves a high averaged TA of\n96.4%.\n(3) Object localization always helps.\nHowever,\nfor the object with complex boundary such as \u2018bike\u2019, its\nmIoU is low even it can be localized, e.g. \u2018bike\u2019 has\nhigh LA but low BA and mIoU. (4) Failures of different\ncategories have different factors. With these three metrics,\nthey can be easily identi\ufb01ed. For example, the failures of\n\u2018chair\u2019, \u2018table\u2019, and \u2018plant\u2019 are caused by the dif\ufb01culties\nto accurately capture their bounding boxes and boundaries.\nAlthough \u2018bottle\u2019 and \u2018tv\u2019 are also dif\ufb01cult to localize, they\nachieve moderate mIoU because of their regular shapes. In\nother words, mIoU of \u2018bottle\u2019 and \u2018tv\u2019 can be signi\ufb01cantly\nimproved if they can be accurately localized.\n4.2. Overall Performance\nAs shown in Table 3 (b), we compare DPN with the\nbest-performing methods5 on VOC12 test set based on two\nsettings, i.e. with and without pre-training on COCO. The\napproaches pre-trained on COCO are marked with \u2018\u2020\u2019. We\nevaluate DPN on several scales of the images and then\naverage the results following [3, 19].\nDPN outperforms all the existing methods that were\ntrained on VOC12, but DPN needs only one MF iteration\nto solve MRF, other than 10 iterations of RNN, DeepLab,\nand Piecewise. By averaging the results of two DPNs, we\nachieve 74.1% accuracy on VOC12 without outside training\ndata.\nAs discussed in Sec.3.3, MF iteration is the most\ncomplex step even when it is implemented as convolutions.\nTherefore, DPN at least reduces 10\u00d7 runtime compared to\n5The results of these methods were presented in either the published\npapers or arXiv pre-prints.\nprevious works.\nFollowing [39, 5], we pre-train DPN with COCO, where\n20 object categories that are also presented in VOC12 are\nselected for training. A single DPN\u2020 has achieved 77.5%\nmIoU on VOC12 test set. As shown in Table 3 (b), we\nobserve that DPN\u2020 achieves best performances on more\nthan half of the object classes. Please refer to the appendices\nfor visual quality comparisons.\n5. Conclusion\nWe proposed Deep Parsing Network (DPN) to address\nsemantic image segmentation, which has several appealing\nproperties. First, DPN uni\ufb01es the inference and learning\nof unary term and pairwise terms in a single convolutional\nnetwork. No iterative inference are required during back-\npropagation.\nSecond, high-order relations and mixtures\nof label contexts are incorporated to its pairwise terms\nmodeling, making existing works serve as special cases.\nThird, DPN is built upon conventional operations of CNN,\nthus easy to be parallelized and speeded up.\nDPN achieves state-of-the-art performance on VOC12,\nand multiple valuable facts about semantic image segmen-\ntion are revealed through extensive experiments.\nFuture\ndirections include investigating the generalizability of DPN\nto more challenging scenarios, e.g. large number of object\nclasses and substantial appearance/scale variations.\nReferences\n[1] A. Adams, J. Baek, and M. A. Davis. Fast high-dimensional\n\ufb01ltering using the permutohedral lattice.\nIn Computer\nGraphics Forum, volume 29, pages 753\u2013762, 2010. 6\n[2] P. Arbelaez, M. Maire, C. Fowlkes, and J. Malik.\nCon-\ntour detection and hierarchical image segmentation. PAMI,\n33(5):898\u2013916, 2011. 1\n[3] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and\nA. L. Yuille.\nSemantic image segmentation with deep\nconvolutional nets and fully connected crfs. In ICLR, 2015.\n1, 2, 5, 6, 7, 8, 10, 11\n[4] S. Chetlur, C. Woolley, P. Vandermersch, J. Cohen, J. Tran,\nB. Catanzaro, and E. Shelhamer. cudnn: Ef\ufb01cient primitives\nfor deep learning. In NIPS Deep Learning Workshop, 2014.\n2, 6\n[5] J. Dai, K. He, and J. Sun.\nBoxsup: Exploiting bounding\nboxes to supervise convolutional networks for semantic seg-\nmentation. arXiv:1503.01640v2, 18 May 2015. 6, 8\n[6] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\nFei. Imagenet: A large-scale hierarchical image database. In\nCVPR, pages 248\u2013255, 2009. 1\n[7] M. Everingham, L. Van Gool, C. K. Williams, J. Winn,\nand A. Zisserman.\nThe pascal visual object classes (voc)\nchallenge. IJCV, 88(2):303\u2013338, 2010. 2, 4, 6\n[8] C. Farabet, C. Couprie, L. Najman, and Y. LeCun. Learning\nhierarchical features for scene labeling. PAMI, 35(8):1915\u2013\n1929, 2013. 1\n[9] P. F. Felzenszwalb and D. P. Huttenlocher. Ef\ufb01cient belief\npropagation for early vision. IJCV, 70(1):41\u201354, 2006. 1\n[10] W. T. Freeman, E. C. Pasztor, and O. T. Carmichael. Learn-\ning low-level vision. IJCV, 40(1):25\u201347, 2000. 2\n[11] B. Fulkerson, A. Vedaldi, and S. Soatto. Class segmentation\nand object localization with superpixel neighborhoods. In\nICCV, pages 670\u2013677, 2009. 1\n[12] B. Hariharan, P. Arbel\u00b4aez, L. Bourdev, S. Maji, and J. Malik.\nSemantic contours from inverse detectors. In ICCV, pages\n991\u2013998, 2011. 6\n[13] G. E. Hinton, O. Vinyals, and J. Dean.\nDistilling the\nknowledge in a neural network.\nIn NIPS Deep Learning\nWorkshop, 2014. 6\n[14] M. Jaderberg, A. Vedaldi, and A. Zisserman. Speeding up\nconvolutional neural networks with low rank expansions. In\nBMVC, 2014. 2, 6\n[15] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul.\nAn introduction to variational methods for graphical models.\nMachine learning, 37(2):183\u2013233, 1999. 3\n[16] P. Kr\u00a8ahenb\u00a8uhl and V. Koltun.\nEf\ufb01cient inference in fully\nconnected crfs with gaussian edge potentials. NIPS, 2011. 1,\n6, 7\n[17] P. Kr\u00a8ahenb\u00a8uhl and V. Koltun.\nParameter learning and\nconvergent inference for dense random \ufb01elds.\nIn ICML,\npages 513\u2013521, 2013. 1\n[18] A. Krizhevsky, I. Sutskever, and G. E. Hinton.\nImagenet\nclassi\ufb01cation with deep convolutional neural networks. In\nNIPS, pages 1097\u20131105, 2012. 4\n[19] G. Lin, C. Shen, I. Reid, and A. Hengel. Ef\ufb01cient piecewise\ntraining of deep structured models for semantic segmenta-\ntion. arXiv:1504.01013v2, 23 Apr 2015. 1, 2, 6, 8\n[20] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona,\nD. Ramanan, P. Doll\u00b4ar, and C. L. Zitnick. Microsoft coco:\nCommon objects in context. In ECCV, pages 740\u2013755. 2014.\n6, 8\n[21] C. Liu, J. Yuen, and A. Torralba.\nNonparametric scene\nparsing via label transfer. PAMI, 33(12):2368\u20132382, 2011.\n1\n[22] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional\nnetworks for semantic segmentation. In CVPR, pages 3431\u2013\n3440, 2015. 1, 5, 6, 8, 10, 11\n[23] P. Luo, X. Wang, and X. Tang. Hierarchical face parsing via\ndeep learning. In CVPR, pages 2480\u20132487, 2012. 1\n[24] P. Luo, X. Wang, and X. Tang. Pedestrian parsing via deep\ndecompositional network. In ICCV, pages 2648\u20132655, 2013.\n1\n[25] M. Mostajabi, P. Yadollahpour, and G. Shakhnarovich. Feed-\nforward semantic segmentation with zoom-out features. In\nCVPR, pages 3376\u20133385, 2015. 1, 6, 8\n[26] R. Mottaghi, X. Chen, X. Liu, N.-G. Cho, S.-W. Lee,\nS. Fidler, R. Urtasun, and A. Yuille.\nThe role of context\nfor object detection and semantic segmentation in the wild.\nIn CVPR, pages 891\u2013898, 2014. 1\n[27] M. Opper, O. Winther, et al. From naive mean \ufb01eld theory to\nthe tap equations. 2001. 1, 3\n[28] G. Papandreou, L.-C. Chen, K. Murphy, and A. L. Yuille.\nWeakly-and semi-supervised learning of a dcnn for semantic\nimage segmentation. arXiv:1502.02734v2, 8 May 2015. 1,\n6, 8\n[29] X. Ren and J. Malik. Learning a classi\ufb01cation model for\nsegmentation. In ICCV, pages 10\u201317, 2003. 1\n[30] A. G. Schwing and R. Urtasun.\nFully connected deep\nstructured networks. arXiv:1503.02351v1, 9 Mar 2015. 1,\n2, 5, 6, 7\n[31] J. Shi and J. Malik. Normalized cuts and image segmenta-\ntion. PAMI, 22(8):888\u2013905, 2000. 1\n[32] K. Simonyan and A. Zisserman. Very deep convolutional\nnetworks for large-scale image recognition. In ICLR, 2015.\n2\n[33] Y. Sun, X. Wang, and X. Tang. Deep learning face repre-\nsentation by joint identi\ufb01cation-veri\ufb01cation. In NIPS, 2014.\n4\n[34] M. Szummer, P. Kohli, and D. Hoiem. Learning crfs using\ngraph cuts. In ECCV, pages 582\u2013595. 2008. 1\n[35] Y. Taigman, M. Yang, M. Ranzato, and L. Wolf. Deepface:\nClosing the gap to human-level performance in face veri\ufb01ca-\ntion. In CVPR, pages 1701\u20131708, 2014. 4\n[36] V. Vineet, G. Sheasby, J. Warrell, and P. H. Torr. Pose\ufb01eld:\nAn ef\ufb01cient mean-\ufb01eld based method for joint estimation of\nhuman pose, segmentation, and depth. In Energy Minimiza-\ntion Methods in Computer Vision and Pattern Recognition,\npages 180\u2013194. Springer, 2013. 1\n[37] V. Vineet, J. Warrell, and P. H. Torr.\nFilter-based mean-\n\ufb01eld inference for random \ufb01elds with higher-order terms and\nproduct label-spaces. In ECCV, pages 31\u201344. 2012. 1\n[38] J. Yang, B. Price, S. Cohen, and M.-H. Yang. Context driven\nscene parsing with attention to rare classes. In CVPR, pages\n3294\u20133301, 2014. 1\n[39] S. Zheng, S. Jayasumana, B. Romera-Paredes, V. Vineet,\nZ. Su, D. Du, C. Huang, and P. Torr. Conditional random\n\ufb01elds as recurrent neural networks. arXiv:1502.03240v2, 30\nApr 2015. 1, 2, 5, 6, 7, 8\nAppendices\nA. Fast Implementation of Locally Convolu-\ntion\nb12 in DPN is a locally convolutional layer. As men-\ntioned in Eqn.(3), the local \ufb01lters in b12 are computed\nby the distances between RGB values of the pixels. XY\ncoordinates are omitted here because they could be pre-\ncomputed. To accelerate the computation of locally convo-\nlution, lookup table-based \ufb01ltering approach is employed.\nWe \ufb01rst construct a lookup table storing distances between\nany two pixel intensities (ranging from 0 to 255), which\nresults in a 256 \u00d7 256 matrix. Then when we perform lo-\ncally convolution, the kernels\u2019 coef\ufb01cients can be obtained\nef\ufb01ciently by just looking up the table.\nB. Visual Quality Comparisons\nIn the following, we inspect visual quality of obtained\nlabel maps. Fig.8 demonstrates the comparisons of DPN\nwith FCN [22] and DeepLab [3].\nWe use the publicly\nreleased model6 to re-generate label maps of FCN while\nthe results of DeepLab are extracted from their published\npaper. DPN generally makes more accurate predictions in\nboth image-level and instance-level.\nWe also include more examples of DPN label maps in\nFig.9. We observe that learning local label contexts helps\ndifferentiate confusing objects and learning triple penalty\nfacilitates the capturing of intrinsic object boundaries.\n6http://dl.caffe.berkeleyvision.org/\nfcn-8s-pascal.caffemodel\n(a)\n(b)\n(c)\n(d)\n(e)\nFigure 8: Visual quality comparison of different semantic image segmentation methods: (a) input image (b) ground truth (c)\nFCN [22] (d) DeepLab [3] and (e) DPN.\n(a)\n(b)\n(c)\n(a)\n(b)\n(c)\nFigure 9: Visual quality of DPN label maps: (a) input image (b) ground truth (white labels indicating ambiguous regions)\nand (c) DPN.\n",
        "sentence": " 8 CUHK-DPN-COCO (Liu et al., 2015) 77.",
        "context": "selected for training. A single DPN\u2020 has achieved 77.5%\nmIoU on VOC12 test set. As shown in Table 3 (b), we\nobserve that DPN\u2020 achieves best performances on more\nthan half of the object classes. Please refer to the appendices\nsheep sofa\ntrain\ntv\nmIoU\nFCN [22]\n76.8\n34.2\n68.9\n49.4\n60.3\n75.3\n74.7\n77.6\n21.4\n62.5\n46.8\n71.8\n63.9\n76.5\n73.9\n45.2\n72.4\n37.4\n70.9\n55.1\n62.2\nZoom-out [25]\n85.6\n37.3\n83.2\n62.5\n66.0\n85.1\n80.7\n84.9\n27.2\n73.2\n57.5\n78.1\n79.2\n81.1\n77.1\n53.6\n74.0\n49.2\n71.7\n63.3\n34.4\n83.6\n67.1\n81.5\n83.7\n85.2\n83.5\n58.6\n84.9\n55.8\n81.2\n70.7\n75.2\nDPN\n87.7\n59.4\n78.4\n64.9\n70.3\n89.3\n83.5\n86.1\n31.7\n79.9\n62.6\n81.9\n80.0\n83.5\n82.3\n60.5\n83.2\n53.4\n77.9\n65.0\n74.1\nDPN\u2020\n89.0\n61.6\n87.7\n66.8\n74.7\n91.2\n84.3\n87.6\n36.5\n86.3\n66.1\n84.4\n87.8\n85.6\n85.4"
    },
    {
        "title": "Fully convolutional networks for semantic segmentation",
        "author": [
            "Long",
            "Jonathan",
            "Shelhamer",
            "Evan",
            "Darrell",
            "Trevor"
        ],
        "venue": "CoRR, abs/1411.4038,",
        "citeRegEx": "Long et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Long et al\\.",
        "year": 2014,
        "abstract": "Convolutional networks are powerful visual models that yield hierarchies of\nfeatures. We show that convolutional networks by themselves, trained\nend-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic\nsegmentation. Our key insight is to build \"fully convolutional\" networks that\ntake input of arbitrary size and produce correspondingly-sized output with\nefficient inference and learning. We define and detail the space of fully\nconvolutional networks, explain their application to spatially dense prediction\ntasks, and draw connections to prior models. We adapt contemporary\nclassification networks (AlexNet, the VGG net, and GoogLeNet) into fully\nconvolutional networks and transfer their learned representations by\nfine-tuning to the segmentation task. We then define a novel architecture that\ncombines semantic information from a deep, coarse layer with appearance\ninformation from a shallow, fine layer to produce accurate and detailed\nsegmentations. Our fully convolutional network achieves state-of-the-art\nsegmentation of PASCAL VOC (20% relative improvement to 62.2% mean IU on 2012),\nNYUDv2, and SIFT Flow, while inference takes one third of a second for a\ntypical image.",
        "full_text": "Fully Convolutional Networks for Semantic Segmentation\nJonathan Long\u2217\nEvan Shelhamer\u2217\nTrevor Darrell\nUC Berkeley\n{jonlong,shelhamer,trevor}@cs.berkeley.edu\nAbstract\nConvolutional networks are powerful visual models that\nyield hierarchies of features.\nWe show that convolu-\ntional networks by themselves, trained end-to-end, pixels-\nto-pixels, exceed the state-of-the-art in semantic segmen-\ntation. Our key insight is to build \u201cfully convolutional\u201d\nnetworks that take input of arbitrary size and produce\ncorrespondingly-sized output with ef\ufb01cient inference and\nlearning. We de\ufb01ne and detail the space of fully convolu-\ntional networks, explain their application to spatially dense\nprediction tasks, and draw connections to prior models. We\nadapt contemporary classi\ufb01cation networks (AlexNet [19],\nthe VGG net [31], and GoogLeNet [32]) into fully convolu-\ntional networks and transfer their learned representations\nby \ufb01ne-tuning [4] to the segmentation task. We then de-\n\ufb01ne a novel architecture that combines semantic informa-\ntion from a deep, coarse layer with appearance information\nfrom a shallow, \ufb01ne layer to produce accurate and detailed\nsegmentations. Our fully convolutional network achieves\nstate-of-the-art segmentation of PASCAL VOC (20% rela-\ntive improvement to 62.2% mean IU on 2012), NYUDv2,\nand SIFT Flow, while inference takes less than one \ufb01fth of a\nsecond for a typical image.\n1. Introduction\nConvolutional networks are driving advances in recog-\nnition. Convnets are not only improving for whole-image\nclassi\ufb01cation [19, 31, 32], but also making progress on lo-\ncal tasks with structured output. These include advances in\nbounding box object detection [29, 12, 17], part and key-\npoint prediction [39, 24], and local correspondence [24, 9].\nThe natural next step in the progression from coarse to\n\ufb01ne inference is to make a prediction at every pixel. Prior\napproaches have used convnets for semantic segmentation\n[27, 2, 8, 28, 16, 14, 11], in which each pixel is labeled with\nthe class of its enclosing object or region, but with short-\ncomings that this work addresses.\n\u2217Authors contributed equally\n96\n384\n256 4096\n4096\n21\n21\nbackward/learning\nforward/inference\npixelwise prediction\nsegmentation g.t.\n256\n384\nFigure 1.\nFully convolutional networks can ef\ufb01ciently learn to\nmake dense predictions for per-pixel tasks like semantic segmen-\ntation.\nWe show that a fully convolutional network (FCN),\ntrained end-to-end, pixels-to-pixels on semantic segmen-\ntation exceeds the state-of-the-art without further machin-\nery. To our knowledge, this is the \ufb01rst work to train FCNs\nend-to-end (1) for pixelwise prediction and (2) from super-\nvised pre-training. Fully convolutional versions of existing\nnetworks predict dense outputs from arbitrary-sized inputs.\nBoth learning and inference are performed whole-image-at-\na-time by dense feedforward computation and backpropa-\ngation. In-network upsampling layers enable pixelwise pre-\ndiction and learning in nets with subsampled pooling.\nThis method is ef\ufb01cient, both asymptotically and abso-\nlutely, and precludes the need for the complications in other\nworks. Patchwise training is common [27, 2, 8, 28, 11], but\nlacks the ef\ufb01ciency of fully convolutional training. Our ap-\nproach does not make use of pre- and post-processing com-\nplications, including superpixels [8, 16], proposals [16, 14],\nor post-hoc re\ufb01nement by random \ufb01elds or local classi\ufb01ers\n[8, 16]. Our model transfers recent success in classi\ufb01ca-\ntion [19, 31, 32] to dense prediction by reinterpreting clas-\nsi\ufb01cation nets as fully convolutional and \ufb01ne-tuning from\ntheir learned representations. In contrast, previous works\nhave applied small convnets without supervised pre-training\n[8, 28, 27].\nSemantic segmentation faces an inherent tension be-\ntween semantics and location: global information resolves\nwhat while local information resolves where. Deep feature\n1\narXiv:1411.4038v2  [cs.CV]  8 Mar 2015\nhierarchies jointly encode location and semantics in a local-\nto-global pyramid. We de\ufb01ne a novel \u201cskip\u201d architecture\nto combine deep, coarse, semantic information and shallow,\n\ufb01ne, appearance information in Section 4.2 (see Figure 3).\nIn the next section, we review related work on deep clas-\nsi\ufb01cation nets, FCNs, and recent approaches to semantic\nsegmentation using convnets. The following sections ex-\nplain FCN design and dense prediction tradeoffs, introduce\nour architecture with in-network upsampling and multi-\nlayer combinations, and describe our experimental frame-\nwork. Finally, we demonstrate state-of-the-art results on\nPASCAL VOC 2011-2, NYUDv2, and SIFT Flow.\n2. Related work\nOur approach draws on recent successes of deep nets\nfor image classi\ufb01cation [19, 31, 32] and transfer learning\n[4, 38]. Transfer was \ufb01rst demonstrated on various visual\nrecognition tasks [4, 38], then on detection, and on both\ninstance and semantic segmentation in hybrid proposal-\nclassi\ufb01er models [12, 16, 14]. We now re-architect and \ufb01ne-\ntune classi\ufb01cation nets to direct, dense prediction of seman-\ntic segmentation. We chart the space of FCNs and situate\nprior models, both historical and recent, in this framework.\nFully convolutional networks To our knowledge, the\nidea of extending a convnet to arbitrary-sized inputs \ufb01rst\nappeared in Matan et al. [25], which extended the classic\nLeNet [21] to recognize strings of digits. Because their net\nwas limited to one-dimensional input strings, Matan et al.\nused Viterbi decoding to obtain their outputs. Wolf and Platt\n[37] expand convnet outputs to 2-dimensional maps of de-\ntection scores for the four corners of postal address blocks.\nBoth of these historical works do inference and learning\nfully convolutionally for detection. Ning et al. [27] de\ufb01ne\na convnet for coarse multiclass segmentation of C. elegans\ntissues with fully convolutional inference.\nFully convolutional computation has also been exploited\nin the present era of many-layered nets. Sliding window\ndetection by Sermanet et al. [29], semantic segmentation\nby Pinheiro and Collobert [28], and image restoration by\nEigen et al. [5] do fully convolutional inference. Fully con-\nvolutional training is rare, but used effectively by Tompson\net al. [35] to learn an end-to-end part detector and spatial\nmodel for pose estimation, although they do not exposit on\nor analyze this method.\nAlternatively,\nHe\net\nal.\n[17]\ndiscard\nthe\nnon-\nconvolutional portion of classi\ufb01cation nets to make a\nfeature extractor.\nThey combine proposals and spatial\npyramid pooling to yield a localized, \ufb01xed-length feature\nfor classi\ufb01cation.\nWhile fast and effective, this hybrid\nmodel cannot be learned end-to-end.\nDense prediction with convnets Several recent works\nhave applied convnets to dense prediction problems, includ-\ning semantic segmentation by Ning et al. [27], Farabet et al.\n[8], and Pinheiro and Collobert [28]; boundary prediction\nfor electron microscopy by Ciresan et al. [2] and for natural\nimages by a hybrid neural net/nearest neighbor model by\nGanin and Lempitsky [11]; and image restoration and depth\nestimation by Eigen et al. [5, 6]. Common elements of these\napproaches include\n\u2022 small models restricting capacity and receptive \ufb01elds;\n\u2022 patchwise training [27, 2, 8, 28, 11];\n\u2022 post-processing by superpixel projection, random \ufb01eld\nregularization, \ufb01ltering, or local classi\ufb01cation [8, 2,\n11];\n\u2022 input shifting and output interlacing for dense output\n[28, 11] as introduced by OverFeat [29];\n\u2022 multi-scale pyramid processing [8, 28, 11];\n\u2022 saturating tanh nonlinearities [8, 5, 28]; and\n\u2022 ensembles [2, 11],\nwhereas our method does without this machinery. However,\nwe do study patchwise training 3.4 and \u201cshift-and-stitch\u201d\ndense output 3.2 from the perspective of FCNs. We also\ndiscuss in-network upsampling 3.3, of which the fully con-\nnected prediction by Eigen et al. [6] is a special case.\nUnlike these existing methods, we adapt and extend deep\nclassi\ufb01cation architectures, using image classi\ufb01cation as su-\npervised pre-training, and \ufb01ne-tune fully convolutionally to\nlearn simply and ef\ufb01ciently from whole image inputs and\nwhole image ground thruths.\nHariharan et al. [16] and Gupta et al. [14] likewise adapt\ndeep classi\ufb01cation nets to semantic segmentation, but do\nso in hybrid proposal-classi\ufb01er models. These approaches\n\ufb01ne-tune an R-CNN system [12] by sampling bounding\nboxes and/or region proposals for detection, semantic seg-\nmentation, and instance segmentation. Neither method is\nlearned end-to-end.\nThey achieve state-of-the-art results on PASCAL VOC\nsegmentation and NYUDv2 segmentation respectively, so\nwe directly compare our standalone, end-to-end FCN to\ntheir semantic segmentation results in Section 5.\n3. Fully convolutional networks\nEach layer of data in a convnet is a three-dimensional\narray of size h \u00d7 w \u00d7 d, where h and w are spatial dimen-\nsions, and d is the feature or channel dimension. The \ufb01rst\nlayer is the image, with pixel size h \u00d7 w, and d color chan-\nnels. Locations in higher layers correspond to the locations\nin the image they are path-connected to, which are called\ntheir receptive \ufb01elds.\nConvnets are built on translation invariance. Their ba-\nsic components (convolution, pooling, and activation func-\ntions) operate on local input regions, and depend only on\nrelative spatial coordinates. Writing xij for the data vector\nat location (i, j) in a particular layer, and yij for the follow-\ning layer, these functions compute outputs yij by\nyij = fks ({xsi+\u03b4i,sj+\u03b4j}0\u2264\u03b4i,\u03b4j\u2264k)\nwhere k is called the kernel size, s is the stride or subsam-\npling factor, and fks determines the layer type: a matrix\nmultiplication for convolution or average pooling, a spatial\nmax for max pooling, or an elementwise nonlinearity for an\nactivation function, and so on for other types of layers.\nThis functional form is maintained under composition,\nwith kernel size and stride obeying the transformation rule\nfks \u25e6gk\u2032s\u2032 = (f \u25e6g)k\u2032+(k\u22121)s\u2032,ss\u2032.\nWhile a general deep net computes a general nonlinear\nfunction, a net with only layers of this form computes a\nnonlinear \ufb01lter, which we call a deep \ufb01lter or fully convolu-\ntional network. An FCN naturally operates on an input of\nany size, and produces an output of corresponding (possibly\nresampled) spatial dimensions.\nA real-valued loss function composed with an FCN de-\n\ufb01nes a task. If the loss function is a sum over the spatial\ndimensions of the \ufb01nal layer, \u2113(x; \u03b8) = P\nij \u2113\u2032(xij; \u03b8), its\ngradient will be a sum over the gradients of each of its spa-\ntial components. Thus stochastic gradient descent on \u2113com-\nputed on whole images will be the same as stochastic gradi-\nent descent on \u2113\u2032, taking all of the \ufb01nal layer receptive \ufb01elds\nas a minibatch.\nWhen these receptive \ufb01elds overlap signi\ufb01cantly, both\nfeedforward computation and backpropagation are much\nmore ef\ufb01cient when computed layer-by-layer over an entire\nimage instead of independently patch-by-patch.\nWe next explain how to convert classi\ufb01cation nets into\nfully convolutional nets that produce coarse output maps.\nFor pixelwise prediction, we need to connect these coarse\noutputs back to the pixels. Section 3.2 describes a trick that\nOverFeat [29] introduced for this purpose. We gain insight\ninto this trick by reinterpreting it as an equivalent network\nmodi\ufb01cation. As an ef\ufb01cient, effective alternative, we in-\ntroduce deconvolution layers for upsampling in Section 3.3.\nIn Section 3.4 we consider training by patchwise sampling,\nand give evidence in Section 4.3 that our whole image train-\ning is faster and equally effective.\n3.1. Adapting classi\ufb01ers for dense prediction\nTypical recognition nets, including LeNet [21], AlexNet\n[19], and its deeper successors [31, 32], ostensibly take\n\ufb01xed-sized inputs and produce nonspatial outputs. The fully\nconnected layers of these nets have \ufb01xed dimensions and\nthrow away spatial coordinates. However, these fully con-\nnected layers can also be viewed as convolutions with ker-\nnels that cover their entire input regions. Doing so casts\nthem into fully convolutional networks that take input of\nany size and output classi\ufb01cation maps. This transformation\n`tabby cat\"\n`\n96 256 384\n384\n256\n4096\n4096\n1000\n96\n384\n2564096\n4096\n1000\n256\n384\ntabby cat heatmap\nconvolutionalization\nFigure 2.\nTransforming fully connected layers into convolution\nlayers enables a classi\ufb01cation net to output a heatmap. Adding\nlayers and a spatial loss (as in Figure 1) produces an ef\ufb01cient ma-\nchine for end-to-end dense learning.\nis illustrated in Figure 2. (By contrast, nonconvolutional\nnets, such as the one by Le et al. [20], lack this capability.)\nFurthermore, while the resulting maps are equivalent to\nthe evaluation of the original net on particular input patches,\nthe computation is highly amortized over the overlapping\nregions of those patches. For example, while AlexNet takes\n1.2 ms (on a typical GPU) to produce the classi\ufb01cation\nscores of a 227 \u00d7 227 image, the fully convolutional ver-\nsion takes 22 ms to produce a 10 \u00d7 10 grid of outputs from\na 500 \u00d7 500 image, which is more than 5 times faster than\nthe na\u00a8\u0131ve approach1.\nThe spatial output maps of these convolutionalized mod-\nels make them a natural choice for dense problems like se-\nmantic segmentation. With ground truth available at ev-\nery output cell, both the forward and backward passes are\nstraightforward, and both take advantage of the inherent\ncomputational ef\ufb01ciency (and aggressive optimization) of\nconvolution.\nThe corresponding backward times for the AlexNet ex-\nample are 2.4 ms for a single image and 37 ms for a fully\nconvolutional 10 \u00d7 10 output map, resulting in a speedup\nsimilar to that of the forward pass. This dense backpropa-\ngation is illustrated in Figure 1.\nWhile our reinterpretation of classi\ufb01cation nets as fully\nconvolutional yields output maps for inputs of any size, the\noutput dimensions are typically reduced by subsampling.\nThe classi\ufb01cation nets subsample to keep \ufb01lters small and\ncomputational requirements reasonable. This coarsens the\noutput of a fully convolutional version of these nets, reduc-\ning it from the size of the input by a factor equal to the pixel\nstride of the receptive \ufb01elds of the output units.\n1Assuming ef\ufb01cient batching of single image inputs. The classi\ufb01cation\nscores for a single image by itself take 5.4 ms to produce, which is nearly\n25 times slower than the fully convolutional version.\n3.2. Shift-and-stitch is \ufb01lter rarefaction\nInput shifting and output interlacing is a trick that yields\ndense predictions from coarse outputs without interpola-\ntion, introduced by OverFeat [29]. If the outputs are down-\nsampled by a factor of f, the input is shifted (by left and top\npadding) x pixels to the right and y pixels down, once for\nevery value of (x, y) \u2208{0, . . . , f \u22121} \u00d7 {0, . . . , f \u22121}.\nThese f 2 inputs are each run through the convnet, and the\noutputs are interlaced so that the predictions correspond to\nthe pixels at the centers of their receptive \ufb01elds.\nChanging only the \ufb01lters and layer strides of a convnet\ncan produce the same output as this shift-and-stitch trick.\nConsider a layer (convolution or pooling) with input stride\ns, and a following convolution layer with \ufb01lter weights fij\n(eliding the feature dimensions, irrelevant here). Setting the\nlower layer\u2019s input stride to 1 upsamples its output by a fac-\ntor of s, just like shift-and-stitch. However, convolving the\noriginal \ufb01lter with the upsampled output does not produce\nthe same result as the trick, because the original \ufb01lter only\nsees a reduced portion of its (now upsampled) input. To\nreproduce the trick, rarefy the \ufb01lter by enlarging it as\nf \u2032\nij =\n\u001a fi/s,j/s\nif s divides both i and j;\n0\notherwise,\n(with i and j zero-based). Reproducing the full net output\nof the trick involves repeating this \ufb01lter enlargement layer-\nby-layer until all subsampling is removed.\nSimply decreasing subsampling within a net is a tradeoff:\nthe \ufb01lters see \ufb01ner information, but have smaller receptive\n\ufb01elds and take longer to compute. We have seen that the\nshift-and-stitch trick is another kind of tradeoff: the output\nis made denser without decreasing the receptive \ufb01eld sizes\nof the \ufb01lters, but the \ufb01lters are prohibited from accessing\ninformation at a \ufb01ner scale than their original design.\nAlthough we have done preliminary experiments with\nshift-and-stitch, we do not use it in our model. We \ufb01nd\nlearning through upsampling, as described in the next sec-\ntion, to be more effective and ef\ufb01cient, especially when\ncombined with the skip layer fusion described later on.\n3.3. Upsampling is backwards strided convolution\nAnother way to connect coarse outputs to dense pixels\nis interpolation. For instance, simple bilinear interpolation\ncomputes each output yij from the nearest four inputs by a\nlinear map that depends only on the relative positions of the\ninput and output cells.\nIn a sense, upsampling with factor f is convolution with\na fractional input stride of 1/f. So long as f is integral, a\nnatural way to upsample is therefore backwards convolution\n(sometimes called deconvolution) with an output stride of\nf. Such an operation is trivial to implement, since it simply\nreverses the forward and backward passes of convolution.\nThus upsampling is performed in-network for end-to-end\nlearning by backpropagation from the pixelwise loss.\nNote that the deconvolution \ufb01lter in such a layer need not\nbe \ufb01xed (e.g., to bilinear upsampling), but can be learned.\nA stack of deconvolution layers and activation functions can\neven learn a nonlinear upsampling.\nIn our experiments, we \ufb01nd that in-network upsampling\nis fast and effective for learning dense prediction. Our best\nsegmentation architecture uses these layers to learn to up-\nsample for re\ufb01ned prediction in Section 4.2.\n3.4. Patchwise training is loss sampling\nIn stochastic optimization, gradient computation is\ndriven by the training distribution. Both patchwise train-\ning and fully-convolutional training can be made to pro-\nduce any distribution, although their relative computational\nef\ufb01ciency depends on overlap and minibatch size. Whole\nimage fully convolutional training is identical to patchwise\ntraining where each batch consists of all the receptive \ufb01elds\nof the units below the loss for an image (or collection of\nimages). While this is more ef\ufb01cient than uniform sampling\nof patches, it reduces the number of possible batches. How-\never, random selection of patches within an image may be\nrecovered simply. Restricting the loss to a randomly sam-\npled subset of its spatial terms (or, equivalently applying a\nDropConnect mask [36] between the output and the loss)\nexcludes patches from the gradient computation.\nIf the kept patches still have signi\ufb01cant overlap, fully\nconvolutional computation will still speed up training. If\ngradients are accumulated over multiple backward passes,\nbatches can include patches from several images.2\nSampling in patchwise training can correct class imbal-\nance [27, 8, 2] and mitigate the spatial correlation of dense\npatches [28, 16]. In fully convolutional training, class bal-\nance can also be achieved by weighting the loss, and loss\nsampling can be used to address spatial correlation.\nWe explore training with sampling in Section 4.3, and do\nnot \ufb01nd that it yields faster or better convergence for dense\nprediction. Whole image training is effective and ef\ufb01cient.\n4. Segmentation Architecture\nWe cast ILSVRC classi\ufb01ers into FCNs and augment\nthem for dense prediction with in-network upsampling and\na pixelwise loss. We train for segmentation by \ufb01ne-tuning.\nNext, we build a novel skip architecture that combines\ncoarse, semantic and local, appearance information to re-\n\ufb01ne prediction.\nFor this investigation, we train and validate on the PAS-\nCAL VOC 2011 segmentation challenge [7]. We train with\n2Note that not every possible patch is included this way, since the re-\nceptive \ufb01elds of the \ufb01nal layer units lie on a \ufb01xed, strided grid. However,\nby shifting the image left and down by a random value up to the stride,\nrandom selection from all possible patches may be recovered.\na per-pixel multinomial logistic loss and validate with the\nstandard metric of mean pixel intersection over union, with\nthe mean taken over all classes, including background. The\ntraining ignores pixels that are masked out (as ambiguous\nor dif\ufb01cult) in the ground truth.\n4.1. From classi\ufb01er to dense FCN\nWe begin by convolutionalizing proven classi\ufb01cation ar-\nchitectures as in Section 3. We consider the AlexNet3 ar-\nchitecture [19] that won ILSVRC12, as well as the VGG\nnets [31] and the GoogLeNet4 [32] which did exception-\nally well in ILSVRC14. We pick the VGG 16-layer net5,\nwhich we found to be equivalent to the 19-layer net on this\ntask. For GoogLeNet, we use only the \ufb01nal loss layer, and\nimprove performance by discarding the \ufb01nal average pool-\ning layer. We decapitate each net by discarding the \ufb01nal\nclassi\ufb01er layer, and convert all fully connected layers to\nconvolutions. We append a 1 \u00d7 1 convolution with chan-\nnel dimension 21 to predict scores for each of the PAS-\nCAL classes (including background) at each of the coarse\noutput locations, followed by a deconvolution layer to bi-\nlinearly upsample the coarse outputs to pixel-dense outputs\nas described in Section 3.3. Table 1 compares the prelim-\ninary validation results along with the basic characteristics\nof each net. We report the best results achieved after con-\nvergence at a \ufb01xed learning rate (at least 175 epochs).\nFine-tuning from classi\ufb01cation to segmentation gave rea-\nsonable predictions for each net.\nEven the worst model\nachieved \u223c75% of state-of-the-art performance.\nThe\nsegmentation-equippped VGG net (FCN-VGG16) already\nappears to be state-of-the-art at 56.0 mean IU on val, com-\npared to 52.6 on test [16]. Training on extra data raises\nperformance to 59.4 mean IU on a subset of val7. Training\ndetails are given in Section 4.3.\nDespite similar classi\ufb01cation accuracy, our implementa-\ntion of GoogLeNet did not match this segmentation result.\n4.2. Combining what and where\nWe de\ufb01ne a new fully convolutional net (FCN) for seg-\nmentation that combines layers of the feature hierarchy and\nre\ufb01nes the spatial precision of the output. See Figure 3.\nWhile fully convolutionalized classi\ufb01ers can be \ufb01ne-\ntuned to segmentation as shown in 4.1, and even score\nhighly on the standard metric, their output is dissatisfyingly\ncoarse (see Figure 4). The 32 pixel stride at the \ufb01nal predic-\ntion layer limits the scale of detail in the upsampled output.\nWe address this by adding links that combine the \ufb01nal\nprediction layer with lower layers with \ufb01ner strides. This\n3Using the publicly available CaffeNet reference model.\n4Since there is no publicly available version of GoogLeNet, we use\nour own reimplementation. Our version is trained with less extensive data\naugmentation, and gets 68.5% top-1 and 88.4% top-5 ILSVRC accuracy.\n5Using the publicly available version from the Caffe model zoo.\nTable 1. We adapt and extend three classi\ufb01cation convnets to seg-\nmentation. We compare performance by mean intersection over\nunion on the validation set of PASCAL VOC 2011 and by infer-\nence time (averaged over 20 trials for a 500 \u00d7 500 input on an\nNVIDIA Tesla K40c). We detail the architecture of the adapted\nnets as regards dense prediction: number of parameter layers, re-\nceptive \ufb01eld size of output units, and the coarsest stride within the\nnet. (These numbers give the best performance obtained at a \ufb01xed\nlearning rate, not best performance possible.)\nFCN-\nAlexNet\nFCN-\nVGG16\nFCN-\nGoogLeNet4\nmean IU\n39.8\n56.0\n42.5\nforward time\n50 ms\n210 ms\n59 ms\nconv. layers\n8\n16\n22\nparameters\n57M\n134M\n6M\nrf size\n355\n404\n907\nmax stride\n32\n32\n32\nturns a line topology into a DAG, with edges that skip ahead\nfrom lower layers to higher ones (Figure 3). As they see\nfewer pixels, the \ufb01ner scale predictions should need fewer\nlayers, so it makes sense to make them from shallower net\noutputs. Combining \ufb01ne layers and coarse layers lets the\nmodel make local predictions that respect global structure.\nBy analogy to the multiscale local jet of Florack et al. [10],\nwe call our nonlinear local feature hierarchy the deep jet.\nWe \ufb01rst divide the output stride in half by predicting\nfrom a 16 pixel stride layer. We add a 1 \u00d7 1 convolution\nlayer on top of pool4 to produce additional class predic-\ntions. We fuse this output with the predictions computed\non top of conv7 (convolutionalized fc7) at stride 32 by\nadding a 2\u00d7 upsampling layer and summing6 both predic-\ntions. (See Figure 3). We initialize the 2\u00d7 upsampling to\nbilinear interpolation, but allow the parameters to be learned\nas described in Section 3.3. Finally, the stride 16 predictions\nare upsampled back to the image. We call this net FCN-16s.\nFCN-16s is learned end-to-end, initialized with the param-\neters of the last, coarser net, which we now call FCN-32s.\nThe new parameters acting on pool4 are zero-initialized so\nthat the net starts with unmodi\ufb01ed predictions. The learning\nrate is decreased by a factor of 100.\nLearning this skip net improves performance on the val-\nidation set by 3.0 mean IU to 62.4. Figure 4 shows im-\nprovement in the \ufb01ne structure of the output. We compared\nthis fusion with learning only from the pool4 layer (which\nresulted in poor performance), and simply decreasing the\nlearning rate without adding the extra link (which results\nin an insigni\ufb01cant performance improvement, without im-\nproving the quality of the output).\nWe continue in this fashion by fusing predictions from\npool3 with a 2\u00d7 upsampling of predictions fused from\npool4 and conv7, building the net FCN-8s. We obtain\n6Max fusion made learning dif\ufb01cult due to gradient switching.\nFCN-32s\nFCN-16s\nFCN-8s\nGround truth\nFigure 4. Re\ufb01ning fully convolutional nets by fusing information\nfrom layers with different strides improves segmentation detail.\nThe \ufb01rst three images show the output from our 32, 16, and 8\npixel stride nets (see Figure 3).\nTable 2.\nComparison of skip FCNs on a subset of PASCAL\nVOC2011 validation7. Learning is end-to-end, except for FCN-\n32s-\ufb01xed, where only the last layer is \ufb01ne-tuned. Note that FCN-\n32s is FCN-VGG16, renamed to highlight stride.\npixel\nacc.\nmean\nacc.\nmean\nIU\nf.w.\nIU\nFCN-32s-\ufb01xed\n83.0\n59.7\n45.4\n72.0\nFCN-32s\n89.1\n73.3\n59.4\n81.4\nFCN-16s\n90.0\n75.7\n62.4\n83.0\nFCN-8s\n90.3\n75.9\n62.7\n83.2\na minor additional improvement to 62.7 mean IU, and \ufb01nd\na slight improvement in the smoothness and detail of our\noutput. At this point our fusion improvements have met di-\nminishing returns, both with respect to the IU metric which\nemphasizes large-scale correctness, and also in terms of the\nimprovement visible e.g. in Figure 4, so we do not continue\nfusing even lower layers.\nRe\ufb01nement by other means Decreasing the stride of\npooling layers is the most straightforward way to obtain\n\ufb01ner predictions. However, doing so is problematic for our\nVGG16-based net. Setting the pool5 layer to have stride 1\nrequires our convolutionalized fc6 to have a kernel size of\n14 \u00d7 14 in order to maintain its receptive \ufb01eld size. In addi-\ntion to their computational cost, we had dif\ufb01culty learning\nsuch large \ufb01lters. We made an attempt to re-architect the\nlayers above pool5 with smaller \ufb01lters, but were not suc-\ncessful in achieving comparable performance; one possible\nexplanation is that the initialization from ImageNet-trained\nweights in the upper layers is important.\nAnother way to obtain \ufb01ner predictions is to use the shift-\nand-stitch trick described in Section 3.2. In limited exper-\niments, we found the cost to improvement ratio from this\nmethod to be worse than layer fusion.\n4.3. Experimental framework\nOptimization We train by SGD with momentum. We\nuse a minibatch size of 20 images and \ufb01xed learning rates of\n10\u22123, 10\u22124, and 5\u22125 for FCN-AlexNet, FCN-VGG16, and\nFCN-GoogLeNet, respectively, chosen by line search. We\nuse momentum 0.9, weight decay of 5\u22124 or 2\u22124, and dou-\nbled the learning rate for biases, although we found training\nto be insensitive to these parameters (but sensitive to the\nlearning rate). We zero-initialize the class scoring convo-\nlution layer, \ufb01nding random initialization to yield neither\nbetter performance nor faster convergence. Dropout was in-\ncluded where used in the original classi\ufb01er nets.\nFine-tuning\nWe\n\ufb01ne-tune\nall\nlayers\nby\nback-\npropagation through the whole net.\nFine-tuning the\noutput classi\ufb01er alone yields only 70% of the full \ufb01ne-\ntuning performance as compared in Table 2. Training from\nscratch is not feasible considering the time required to\nlearn the base classi\ufb01cation nets. (Note that the VGG net is\ntrained in stages, while we initialize from the full 16-layer\nversion.) Fine-tuning takes three days on a single GPU for\nthe coarse FCN-32s version, and about one day each to\nupgrade to the FCN-16s and FCN-8s versions.\nPatch Sampling As explained in Section 3.4, our full\nimage training effectively batches each image into a regu-\nimage\npool4\npool5\npool1\npool2\npool3\n32x upsampled\nprediction (FCN-32s)\n2x upsampled\nprediction\n16x upsampled\nprediction (FCN-16s)\n8x upsampled\nprediction (FCN-8s)\npool4\nprediction\n2x upsampled\nprediction\npool3\nprediction\nP\nP\nFigure 3. Our DAG nets learn to combine coarse, high layer information with \ufb01ne, low layer information. Layers are shown as grids that\nreveal relative spatial coarseness. Only pooling and prediction layers are shown; intermediate convolution layers (including our converted\nfully connected layers) are omitted. Solid line (FCN-32s): Our single-stream net, described in Section 4.1, upsamples stride 32 predictions\nback to pixels in a single step. Dashed line (FCN-16s): Combining predictions from both the \ufb01nal layer and the pool4 layer, at stride\n16, lets our net predict \ufb01ner details, while retaining high-level semantic information. Dotted line (FCN-8s): Additional predictions from\npool3, at stride 8, provide further precision.\n500\n1000\n1500\niteration number\n0.4\n0.6\n0.8\n1.0\n1.2\nloss\nfull images\n50% sampling\n25% sampling\n10000\n20000\n30000\nrelative time (num. images processed)\n0.4\n0.6\n0.8\n1.0\n1.2\nloss\nFigure 5. Training on whole images is just as effective as sampling\npatches, but results in faster (wall time) convergence by making\nmore ef\ufb01cient use of data. Left shows the effect of sampling on\nconvergence rate for a \ufb01xed expected batch size, while right plots\nthe same by relative wall time.\nlar grid of large, overlapping patches. By contrast, prior\nwork randomly samples patches over a full dataset [27, 2,\n8, 28, 11], potentially resulting in higher variance batches\nthat may accelerate convergence [22]. We study this trade-\noff by spatially sampling the loss in the manner described\nearlier, making an independent choice to ignore each \ufb01nal\nlayer cell with some probability 1\u2212p. To avoid changing the\neffective batch size, we simultaneously increase the number\nof images per batch by a factor 1/p. Note that due to the ef-\n\ufb01ciency of convolution, this form of rejection sampling is\nstill faster than patchwise training for large enough values\nof p (e.g., at least for p > 0.2 according to the numbers\nin Section 3.1). Figure 5 shows the effect of this form of\nsampling on convergence. We \ufb01nd that sampling does not\nhave a signi\ufb01cant effect on convergence rate compared to\nwhole image training, but takes signi\ufb01cantly more time due\nto the larger number of images that need to be considered\nper batch. We therefore choose unsampled, whole image\ntraining in our other experiments.\nClass Balancing Fully convolutional training can bal-\nance classes by weighting or sampling the loss. Although\nour labels are mildly unbalanced (about 3/4 are back-\nground), we \ufb01nd class balancing unnecessary.\nDense Prediction The scores are upsampled to the in-\nput dimensions by deconvolution layers within the net. Fi-\nnal layer deconvolutional \ufb01lters are \ufb01xed to bilinear inter-\npolation, while intermediate upsampling layers are initial-\nized to bilinear upsampling, and then learned. Shift-and-\nstitch (Section 3.2), or the \ufb01lter rarefaction equivalent, are\nnot used.\nAugmentation We tried augmenting the training data\nby randomly mirroring and \u201cjittering\u201d the images by trans-\nlating them up to 32 pixels (the coarsest scale of prediction)\nin each direction. This yielded no noticeable improvement.\nMore Training Data The PASCAL VOC 2011 segmen-\ntation challenge training set, which we used for Table 1,\nlabels 1112 images. Hariharan et al. [15] have collected\nlabels for a much larger set of 8498 PASCAL training im-\nages, which was used to train the previous state-of-the-art\nsystem, SDS [16]. This training data improves the FCN-\nVGG16 validation score7 by 3.4 points to 59.4 mean IU.\nImplementation All models are trained and tested with\nCaffe [18] on a single NVIDIA Tesla K40c. The models\nand code will be released open-source on publication.\n5. Results\nWe test our FCN on semantic segmentation and scene\nparsing, exploring PASCAL VOC, NYUDv2, and SIFT\nFlow. Although these tasks have historically distinguished\nbetween objects and regions, we treat both uniformly as\npixel prediction. We evaluate our FCN skip architecture8\non each of these datasets, and then extend it to multi-modal\ninput for NYUDv2 and multi-task prediction for the seman-\ntic and geometric labels of SIFT Flow.\nMetrics We report four metrics from common semantic\nsegmentation and scene parsing evaluations that are varia-\ntions on pixel accuracy and region intersection over union\n(IU). Let nij be the number of pixels of class i predicted to\nbelong to class j, where there are ncl different classes, and\nlet ti = P\nj nij be the total number of pixels of class i. We\ncompute:\n\u2022 pixel accuracy: P\ni nii/ P\ni ti\n\u2022 mean accuraccy: (1/ncl) P\ni nii/ti\n\u2022 mean IU: (1/ncl) P\ni nii/\n\u0010\nti + P\nj nji \u2212nii\n\u0011\n\u2022 frequency weighted IU:\n(P\nk tk)\u22121 P\ni tinii/\n\u0010\nti + P\nj nji \u2212nii\n\u0011\nPASCAL VOC Table 3 gives the performance of our\nFCN-8s on the test sets of PASCAL VOC 2011 and 2012,\nand compares it to the previous state-of-the-art, SDS [16],\nand the well-known R-CNN [12]. We achieve the best re-\nsults on mean IU9 by a relative margin of 20%. Inference\ntime is reduced 114\u00d7 (convnet only, ignoring proposals and\nre\ufb01nement) or 286\u00d7 (overall).\nTable 3. Our fully convolutional net gives a 20% relative improve-\nment over the state-of-the-art on the PASCAL VOC 2011 and 2012\ntest sets, and reduces inference time.\nmean IU\nmean IU\ninference\nVOC2011 test\nVOC2012 test\ntime\nR-CNN [12]\n47.9\n-\n-\nSDS [16]\n52.6\n51.6\n\u223c50 s\nFCN-8s\n62.7\n62.2\n\u223c175 ms\nNYUDv2 [30] is an RGB-D dataset collected using the\n7There are training images from [15] included in the PASCAL VOC\n2011 val set, so we validate on the non-intersecting set of 736 images. An\nearlier version of this paper mistakenly evaluated on the entire val set.\n8Our\nmodels\nand\ncode\nare\npublicly\navailable\nat\nhttps://github.com/BVLC/caffe/wiki/Model-Zoo#fcn.\n9This is the only metric provided by the test server.\nTable 4.\nResults on NYUDv2. RGBD is early-fusion of the\nRGB and depth channels at the input. HHA is the depth embed-\nding of [14] as horizontal disparity, height above ground, and\nthe angle of the local surface normal with the inferred gravity\ndirection. RGB-HHA is the jointly trained late fusion model\nthat sums RGB and HHA predictions.\npixel\nacc.\nmean\nacc.\nmean\nIU\nf.w.\nIU\nGupta et al. [14]\n60.3\n-\n28.6\n47.0\nFCN-32s RGB\n60.0\n42.2\n29.2\n43.9\nFCN-32s RGBD\n61.5\n42.4\n30.5\n45.5\nFCN-32s HHA\n57.1\n35.2\n24.2\n40.4\nFCN-32s RGB-HHA\n64.3\n44.9\n32.8\n48.0\nFCN-16s RGB-HHA\n65.4\n46.1\n34.0\n49.5\nMicrosoft Kinect. It has 1449 RGB-D images, with pixel-\nwise labels that have been coalesced into a 40 class seman-\ntic segmentation task by Gupta et al. [13]. We report results\non the standard split of 795 training images and 654 testing\nimages. (Note: all model selection is performed on PAS-\nCAL 2011 val.) Table 4 gives the performance of our model\nin several variations. First we train our unmodi\ufb01ed coarse\nmodel (FCN-32s) on RGB images. To add depth informa-\ntion, we train on a model upgraded to take four-channel\nRGB-D input (early fusion). This provides little bene\ufb01t,\nperhaps due to the dif\ufb01cultly of propagating meaningful\ngradients all the way through the model. Following the suc-\ncess of Gupta et al. [14], we try the three-dimensional HHA\nencoding of depth, training nets on just this information, as\nwell as a \u201clate fusion\u201d of RGB and HHA where the predic-\ntions from both nets are summed at the \ufb01nal layer, and the\nresulting two-stream net is learned end-to-end. Finally we\nupgrade this late fusion net to a 16-stride version.\nSIFT Flow is a dataset of 2,688 images with pixel labels\nfor 33 semantic categories (\u201cbridge\u201d, \u201cmountain\u201d, \u201csun\u201d),\nas well as three geometric categories (\u201chorizontal\u201d, \u201cverti-\ncal\u201d, and \u201csky\u201d). An FCN can naturally learn a joint repre-\nsentation that simultaneously predicts both types of labels.\nWe learn a two-headed version of FCN-16s with seman-\ntic and geometric prediction layers and losses. The learned\nmodel performs as well on both tasks as two independently\ntrained models, while learning and inference are essentially\nas fast as each independent model by itself. The results in\nTable 5, computed on the standard split into 2,488 training\nand 200 test images,10 show state-of-the-art performance on\nboth tasks.\n10Three of the SIFT Flow categories are not present in the test set. We\nmade predictions across all 33 categories, but only included categories ac-\ntually present in the test set in our evaluation. (An earlier version of this pa-\nper reported a lower mean IU, which included all categories either present\nor predicted in the evaluation.)\nTable 5.\nResults on SIFT Flow10 with class segmentation\n(center) and geometric segmentation (right).\nTighe [33] is\na non-parametric transfer method.\nTighe 1 is an exemplar\nSVM while 2 is SVM + MRF. Farabet is a multi-scale con-\nvnet trained on class-balanced samples (1) or natural frequency\nsamples (2). Pinheiro is a multi-scale, recurrent convnet, de-\nnoted RCNN3 (\u25e63). The metric for geometry is pixel accuracy.\npixel\nacc.\nmean\nacc.\nmean\nIU\nf.w.\nIU\ngeom.\nacc.\nLiu et al. [23]\n76.7\n-\n-\n-\n-\nTighe et al. [33]\n-\n-\n-\n-\n90.8\nTighe et al. [34] 1\n75.6\n41.1\n-\n-\n-\nTighe et al. [34] 2\n78.6\n39.2\n-\n-\n-\nFarabet et al. [8] 1\n72.3\n50.8\n-\n-\n-\nFarabet et al. [8] 2\n78.5\n29.6\n-\n-\n-\nPinheiro et al. [28]\n77.7\n29.8\n-\n-\n-\nFCN-16s\n85.2\n51.7\n39.5\n76.1\n94.3\nFCN-8s\nSDS [16]\nGround Truth\nImage\nFigure 6.\nFully convolutional segmentation nets produce state-\nof-the-art performance on PASCAL. The left column shows the\noutput of our highest performing net, FCN-8s. The second shows\nthe segmentations produced by the previous state-of-the-art system\nby Hariharan et al. [16]. Notice the \ufb01ne structures recovered (\ufb01rst\nrow), ability to separate closely interacting objects (second row),\nand robustness to occluders (third row). The fourth row shows a\nfailure case: the net sees lifejackets in a boat as people.\n6. Conclusion\nFully convolutional networks are a rich class of mod-\nels, of which modern classi\ufb01cation convnets are a spe-\ncial case. Recognizing this, extending these classi\ufb01cation\nnets to segmentation, and improving the architecture with\nmulti-resolution layer combinations dramatically improves\nthe state-of-the-art, while simultaneously simplifying and\nspeeding up learning and inference.\nAcknowledgements This work was supported in part\nby DARPA\u2019s MSEE and SMISC programs, NSF awards IIS-\n1427425, IIS-1212798, IIS-1116411, and the NSF GRFP,\nToyota, and the Berkeley Vision and Learning Center. We\ngratefully acknowledge NVIDIA for GPU donation. We\nthank Bharath Hariharan and Saurabh Gupta for their ad-\nvice and dataset tools. We thank Sergio Guadarrama for\nreproducing GoogLeNet in Caffe. We thank Jitendra Malik\nfor his helpful comments. Thanks to Wei Liu for pointing\nout an issue wth our SIFT Flow mean IU computation and\nan error in our frequency weighted mean IU formula.\nA. Upper Bounds on IU\nIn this paper, we have achieved good performance on\nthe mean IU segmentation metric even with coarse semantic\nprediction. To better understand this metric and the limits\nof this approach with respect to it, we compute approximate\nupper bounds on performance with prediction at various\nscales. We do this by downsampling ground truth images\nand then upsampling them again to simulate the best results\nobtainable with a particular downsampling factor. The fol-\nlowing table gives the mean IU on a subset of PASCAL\n2011 val for various downsampling factors.\nfactor\nmean IU\n128\n50.9\n64\n73.3\n32\n86.1\n16\n92.8\n8\n96.4\n4\n98.5\nPixel-perfect prediction is clearly not necessary to\nachieve mean IU well above state-of-the-art, and, con-\nversely, mean IU is a not a good measure of \ufb01ne-scale ac-\ncuracy.\nB. More Results\nWe further evaluate our FCN for semantic segmentation.\nPASCAL-Context [26] provides whole scene annota-\ntions of PASCAL VOC 2010. While there are over 400 dis-\ntinct classes, we follow the 59 class task de\ufb01ned by [26] that\npicks the most frequent classes. We train and evaluate on\nthe training and val sets respectively. In Table 6, we com-\npare to the joint object + stuff variation of Convolutional\nFeature Masking [3] which is the previous state-of-the-art\non this task. FCN-8s scores 35.1 mean IU for an 11% rela-\ntive improvement.\nChangelog\nThe arXiv version of this paper is kept up-to-date with\ncorrections and additional relevant material. The following\ngives a brief history of changes.\nTable 6. Results on PASCAL-Context. CFM is the best result of\n[3] by convolutional feature masking and segment pursuit with the\nVGG net. O2P is the second order pooling method [1] as reported\nin the errata of [26]. The 59 class task includes the 59 most fre-\nquent classes while the 33 class task consists of an easier subset\nidenti\ufb01ed by [26].\n59 class\npixel\nacc.\nmean\nacc.\nmean\nIU\nf.w.\nIU\nO2P\n-\n-\n18.1\n-\nCFM\n-\n-\n31.5\n-\nFCN-32s\n63.8\n42.7\n31.8\n48.3\nFCN-16s\n65.7\n46.2\n34.8\n50.7\nFCN-8s\n65.9\n46.5\n35.1\n51.0\n33 class\nO2P\n-\n-\n29.2\n-\nCFM\n-\n-\n46.1\n-\nFCN-32s\n69.8\n65.1\n50.4\n54.9\nFCN-16s\n71.8\n68.0\n53.4\n57.5\nFCN-8s\n71.8\n67.6\n53.5\n57.7\nv2 Add Appendix A giving upper bounds on mean IU and\nAppendix B with PASCAL-Context results. Correct PAS-\nCAL validation numbers (previously, some val images were\nincluded in train), SIFT Flow mean IU (which used an in-\nappropriately strict metric), and an error in the frequency\nweighted mean IU formula. Add link to models and update\ntiming numbers to re\ufb02ect improved implementation (which\nis publicly available).\nReferences\n[1] J. Carreira, R. Caseiro, J. Batista, and C. Sminchisescu. Se-\nmantic segmentation with second-order pooling. In ECCV,\n2012. 9\n[2] D. C. Ciresan, A. Giusti, L. M. Gambardella, and J. Schmid-\nhuber. Deep neural networks segment neuronal membranes\nin electron microscopy images. In NIPS, pages 2852\u20132860,\n2012. 1, 2, 4, 7\n[3] J. Dai, K. He, and J. Sun.\nConvolutional feature mask-\ning for joint object and stuff segmentation. arXiv preprint\narXiv:1412.1283, 2014. 9\n[4] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang,\nE. Tzeng, and T. Darrell. DeCAF: A deep convolutional acti-\nvation feature for generic visual recognition. In ICML, 2014.\n1, 2\n[5] D. Eigen, D. Krishnan, and R. Fergus. Restoring an image\ntaken through a window covered with dirt or rain. In Com-\nputer Vision (ICCV), 2013 IEEE International Conference\non, pages 633\u2013640. IEEE, 2013. 2\n[6] D. Eigen, C. Puhrsch, and R. Fergus. Depth map prediction\nfrom a single image using a multi-scale deep network. arXiv\npreprint arXiv:1406.2283, 2014. 2\n[7] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn,\nand A. Zisserman.\nThe PASCAL Visual Object Classes\nChallenge 2011 (VOC2011) Results.\nhttp://www.pascal-\nnetwork.org/challenges/VOC/voc2011/workshop/index.html.\n4\n[8] C. Farabet, C. Couprie, L. Najman, and Y. LeCun. Learning\nhierarchical features for scene labeling. Pattern Analysis and\nMachine Intelligence, IEEE Transactions on, 2013. 1, 2, 4,\n7, 8\n[9] P. Fischer, A. Dosovitskiy, and T. Brox. Descriptor matching\nwith convolutional neural networks: a comparison to SIFT.\nCoRR, abs/1405.5769, 2014. 1\n[10] L. Florack, B. T. H. Romeny, M. Viergever, and J. Koen-\nderink. The gaussian scale-space paradigm and the multi-\nscale local jet. International Journal of Computer Vision,\n18(1):61\u201375, 1996. 5\n[11] Y. Ganin and V. Lempitsky. N4-\ufb01elds: Neural network near-\nest neighbor \ufb01elds for image transforms. In ACCV, 2014. 1,\n2, 7\n[12] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-\nture hierarchies for accurate object detection and semantic\nsegmentation. In Computer Vision and Pattern Recognition,\n2014. 1, 2, 7\n[13] S. Gupta, P. Arbelaez, and J. Malik. Perceptual organization\nand recognition of indoor scenes from RGB-D images. In\nCVPR, 2013. 8\n[14] S. Gupta, R. Girshick, P. Arbelaez, and J. Malik. Learning\nrich features from RGB-D images for object detection and\nsegmentation. In ECCV. Springer, 2014. 1, 2, 8\n[15] B. Hariharan, P. Arbelaez, L. Bourdev, S. Maji, and J. Malik.\nSemantic contours from inverse detectors. In International\nConference on Computer Vision (ICCV), 2011. 7\n[16] B. Hariharan, P. Arbel\u00b4aez, R. Girshick, and J. Malik. Simul-\ntaneous detection and segmentation. In European Confer-\nence on Computer Vision (ECCV), 2014. 1, 2, 4, 5, 7, 8\n[17] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling\nin deep convolutional networks for visual recognition.\nIn\nECCV, 2014. 1, 2\n[18] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-\nshick, S. Guadarrama, and T. Darrell.\nCaffe: Convolu-\ntional architecture for fast feature embedding. arXiv preprint\narXiv:1408.5093, 2014. 7\n[19] A. Krizhevsky, I. Sutskever, and G. E. Hinton.\nImagenet\nclassi\ufb01cation with deep convolutional neural networks. In\nNIPS, 2012. 1, 2, 3, 5\n[20] Q. V. Le, R. Monga, M. Devin, K. Chen, G. S. Corrado,\nJ. Dean, and A. Y. Ng. Building high-level features using\nlarge scale unsupervised learning. In ICML, 2012. 3\n[21] Y. LeCun, B. Boser, J. Denker, D. Henderson, R. E. Howard,\nW. Hubbard, and L. D. Jackel. Backpropagation applied to\nhand-written zip code recognition. In Neural Computation,\n1989. 2, 3\n[22] Y. A. LeCun, L. Bottou, G. B. Orr, and K.-R. M\u00a8uller. Ef-\n\ufb01cient backprop. In Neural networks: Tricks of the trade,\npages 9\u201348. Springer, 1998. 7\n[23] C. Liu, J. Yuen, and A. Torralba. Sift \ufb02ow: Dense correspon-\ndence across scenes and its applications. Pattern Analysis\nand Machine Intelligence, IEEE Transactions on, 33(5):978\u2013\n994, 2011. 8\n[24] J. Long, N. Zhang, and T. Darrell. Do convnets learn corre-\nspondence? In NIPS, 2014. 1\n[25] O. Matan, C. J. Burges, Y. LeCun, and J. S. Denker. Multi-\ndigit recognition using a space displacement neural network.\nIn NIPS, pages 488\u2013495. Citeseer, 1991. 2\n[26] R. Mottaghi, X. Chen, X. Liu, N.-G. Cho, S.-W. Lee, S. Fi-\ndler, R. Urtasun, and A. Yuille. The role of context for object\ndetection and semantic segmentation in the wild. In Com-\nputer Vision and Pattern Recognition (CVPR), 2014 IEEE\nConference on, pages 891\u2013898. IEEE, 2014. 9\n[27] F. Ning, D. Delhomme, Y. LeCun, F. Piano, L. Bottou, and\nP. E. Barbano. Toward automatic phenotyping of developing\nembryos from videos. Image Processing, IEEE Transactions\non, 14(9):1360\u20131371, 2005. 1, 2, 4, 7\n[28] P. H. Pinheiro and R. Collobert.\nRecurrent convolutional\nneural networks for scene labeling. In ICML, 2014. 1, 2,\n4, 7, 8\n[29] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,\nand Y. LeCun. Overfeat: Integrated recognition, localization\nand detection using convolutional networks. In ICLR, 2014.\n1, 2, 3, 4\n[30] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus. Indoor\nsegmentation and support inference from rgbd images. In\nECCV, 2012. 7\n[31] K. Simonyan and A. Zisserman.\nVery deep convolu-\ntional networks for large-scale image recognition.\nCoRR,\nabs/1409.1556, 2014. 1, 2, 3, 5\n[32] C. Szegedy,\nW. Liu,\nY. Jia,\nP. Sermanet,\nS. Reed,\nD. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.\nGoing deeper with convolutions.\nCoRR, abs/1409.4842,\n2014. 1, 2, 3, 5\n[33] J. Tighe and S. Lazebnik. Superparsing: scalable nonpara-\nmetric image parsing with superpixels. In ECCV, pages 352\u2013\n365. Springer, 2010. 8\n[34] J. Tighe and S. Lazebnik. Finding things: Image parsing with\nregions and per-exemplar detectors. In CVPR, 2013. 8\n[35] J. Tompson, A. Jain, Y. LeCun, and C. Bregler. Joint training\nof a convolutional network and a graphical model for human\npose estimation. CoRR, abs/1406.2984, 2014. 2\n[36] L. Wan, M. Zeiler, S. Zhang, Y. L. Cun, and R. Fergus. Reg-\nularization of neural networks using dropconnect. In Pro-\nceedings of the 30th International Conference on Machine\nLearning (ICML-13), pages 1058\u20131066, 2013. 4\n[37] R. Wolf and J. C. Platt. Postal address block location using\na convolutional locator network. Advances in Neural Infor-\nmation Processing Systems, pages 745\u2013745, 1994. 2\n[38] M. D. Zeiler and R. Fergus. Visualizing and understanding\nconvolutional networks. In Computer Vision\u2013ECCV 2014,\npages 818\u2013833. Springer, 2014. 2\n[39] N. Zhang, J. Donahue, R. Girshick, and T. Darrell. Part-\nbased r-cnns for \ufb01ne-grained category detection. In Com-\nputer Vision\u2013ECCV 2014, pages 834\u2013849. Springer, 2014.\n1\n",
        "sentence": " Recent works have also shown that DCNNs can equally well apply to pixel-level labelling tasks, including semantic segmentation (Long et al., 2014; Chen et al., 2015) or normal estimation (Sermanet et al. A convenient component of such works is that the inherently convolutional nature of DCNNS allows for simple and efficient \u2018fully convolutional\u2019 implementations (Sermanet et al., 2014; Eigen et al., 2014; Oquab et al., 2015; Long et al., 2014; Chen et al., 2015). Since our model is fully-convolutional we can easily combine it with the recent line of works around FCNN-based semantic segmentation(Long et al., 2014; Chen et al., 2015; Papandreou et al., 2015a; Zheng et al., 2015).",
        "context": "Fully Convolutional Networks for Semantic Segmentation\nJonathan Long\u2217\nEvan Shelhamer\u2217\nTrevor Darrell\nUC Berkeley\n{jonlong,shelhamer,trevor}@cs.berkeley.edu\nAbstract\nConvolutional networks are powerful visual models that\nyield hierarchies of features.\nmake dense predictions for per-pixel tasks like semantic segmen-\ntation.\nWe show that a fully convolutional network (FCN),\ntrained end-to-end, pixels-to-pixels on semantic segmen-\ntation exceeds the state-of-the-art without further machin-\ntissues with fully convolutional inference.\nFully convolutional computation has also been exploited\nin the present era of many-layered nets. Sliding window\ndetection by Sermanet et al. [29], semantic segmentation"
    },
    {
        "title": "Learning to detect natural image boundaries using local brightness, color, and texture cues",
        "author": [
            "D. Martin",
            "C. Fowlkes",
            "J. Malik"
        ],
        "venue": "IEEE Trans. PAMI,",
        "citeRegEx": "Martin et al\\.,? \\Q2004\\E",
        "shortCiteRegEx": "Martin et al\\.",
        "year": 2004,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " As in all works following the introduction of human-annotated datasets (Konishi et al., 2003; Martin et al., 2004), e.",
        "context": null
    },
    {
        "title": "The role of context for object detection and semantic segmentation in the wild",
        "author": [
            "Mottaghi",
            "Roozbeh",
            "Chen",
            "Xianjie",
            "Liu",
            "Xiaobai",
            "Cho",
            "Nam-Gyu",
            "Lee",
            "Seong-Whan",
            "Fidler",
            "Sanja",
            "Urtasun",
            "Raquel",
            "Yuille",
            "Alan"
        ],
        "venue": "In Proc. CVPR,",
        "citeRegEx": "Mottaghi et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Mottaghi et al\\.",
        "year": 2014,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " We have not used these additional scalings in our experiments due to time constraints, but have considered the use of boundaries from the VOC Context dataset (Mottaghi et al., 2014), where all objects and \u2018stuff\u2019 present in the scene are manually segmented.",
        "context": null
    },
    {
        "title": "Is object localization for free? - weaklysupervised learning with convolutional neural networks",
        "author": [
            "Oquab",
            "Maxime",
            "Bottou",
            "L\u00e9on",
            "Laptev",
            "Ivan",
            "Sivic",
            "Josef"
        ],
        "venue": "In Proc. CVPR,",
        "citeRegEx": "Oquab et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "Oquab et al\\.",
        "year": 2015,
        "abstract": "",
        "full_text": "",
        "sentence": " A convenient component of such works is that the inherently convolutional nature of DCNNS allows for simple and efficient \u2018fully convolutional\u2019 implementations (Sermanet et al., 2014; Eigen et al., 2014; Oquab et al., 2015; Long et al., 2014; Chen et al., 2015).",
        "context": null
    },
    {
        "title": "Weakly- and semi-supervised learning of a DCNN for semantic image segmentation",
        "author": [
            "Papandreou",
            "George",
            "Chen",
            "Liang-Chieh",
            "Murphy",
            "Kevin",
            "Yuille",
            "Alan L"
        ],
        "venue": "In Proc. ICCV,",
        "citeRegEx": "Papandreou et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "Papandreou et al\\.",
        "year": 2015,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Modeling local and global deformations in deep learning: Epitomic convolution, multiple instance learning, and sliding window detection",
        "author": [
            "Papandreou",
            "George",
            "Kokkinos",
            "Iasonas",
            "Savalle",
            "Pierre-Andr\u00e9"
        ],
        "venue": "In Proc. CVPR,",
        "citeRegEx": "Papandreou et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "Papandreou et al\\.",
        "year": 2015,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Multiscale helps boundary detection",
        "author": [
            "Ren",
            "Xiaofeng"
        ],
        "venue": "In ECCV,",
        "citeRegEx": "Ren and Xiaofeng.,? \\Q2008\\E",
        "shortCiteRegEx": "Ren and Xiaofeng.",
        "year": 2008,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Discriminatively trained sparse code gradients for contour detection",
        "author": [
            "Ren",
            "Xiaofeng",
            "Bo",
            "Liefeng"
        ],
        "venue": "In NIPS,",
        "citeRegEx": "Ren et al\\.,? \\Q2012\\E",
        "shortCiteRegEx": "Ren et al\\.",
        "year": 2012,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Overfeat: Integrated recognition, localization and detection using convolutional networks",
        "author": [
            "P. Sermanet",
            "D. Eigen",
            "X. Zhang",
            "M. Mathieu",
            "R. Fergus",
            "LeCun",
            "Yann"
        ],
        "venue": "In ICLR,",
        "citeRegEx": "Sermanet et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Sermanet et al\\.",
        "year": 2014,
        "abstract": "We present an integrated framework for using Convolutional Networks for\nclassification, localization and detection. We show how a multiscale and\nsliding window approach can be efficiently implemented within a ConvNet. We\nalso introduce a novel deep learning approach to localization by learning to\npredict object boundaries. Bounding boxes are then accumulated rather than\nsuppressed in order to increase detection confidence. We show that different\ntasks can be learned simultaneously using a single shared network. This\nintegrated framework is the winner of the localization task of the ImageNet\nLarge Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very\ncompetitive results for the detection and classifications tasks. In\npost-competition work, we establish a new state of the art for the detection\ntask. Finally, we release a feature extractor from our best model called\nOverFeat.",
        "full_text": "arXiv:1312.6229v4  [cs.CV]  24 Feb 2014\nOverFeat:\nIntegrated Recognition, Localization and Detection\nusing Convolutional Networks\nPierre Sermanet\nDavid Eigen\nXiang Zhang\nMichael Mathieu\nRob Fergus\nYann LeCun\nCourant Institute of Mathematical Sciences, New York University\n719 Broadway, 12th Floor, New York, NY 10003\nsermanet,deigen,xiang,mathieu,fergus,yann@cs.nyu.edu\nAbstract\nWe present an integrated framework for using Convolutional Networks for classi-\n\ufb01cation, localization and detection. We show how a multiscale and sliding window\napproach can be ef\ufb01ciently implemented within a ConvNet. We also introduce a\nnovel deep learning approach to localization by learning to predict object bound-\naries. Bounding boxes are then accumulated rather than suppressed in order to\nincrease detection con\ufb01dence. We show that different tasks can be learned simul-\ntaneously using a single shared network. This integrated framework is the winner\nof the localization task of the ImageNet Large Scale Visual Recognition Challenge\n2013 (ILSVRC2013) and obtained very competitive results for the detection and\nclassi\ufb01cations tasks. In post-competition work, we establish a new state of the art\nfor the detection task. Finally, we release a feature extractor from our best model\ncalled OverFeat.\n1\nIntroduction\nRecognizing the category of the dominant object in an image is a tasks to which Convolutional\nNetworks (ConvNets) [17] have been applied for many years, whether the objects were handwritten\ncharacters [16], house numbers [24], textureless toys [18], traf\ufb01c signs [3, 26], objects from the\nCaltech-101 dataset [14], or objects from the 1000-category ImageNet dataset [15]. The accuracy\nof ConvNets on small datasets such as Caltech-101, while decent, has not been record-breaking.\nHowever, the advent of larger datasets has enabled ConvNets to signi\ufb01cantly advance the state of\nthe art on datasets such as the 1000-category ImageNet [5].\nThe main advantage of ConvNets for many such tasks is that the entire system is trained end to\nend, from raw pixels to ultimate categories, thereby alleviating the requirement to manually design\na suitable feature extractor. The main disadvantage is their ravenous appetite for labeled training\nsamples.\nThe main point of this paper is to show that training a convolutional network to simultaneously\nclassify, locate and detect objects in images can boost the classi\ufb01cation accuracy and the detection\nand localization accuracy of all tasks. The paper proposes a new integrated approach to object\ndetection, recognition, and localization with a single ConvNet. We also introduce a novel method for\nlocalization and detection by accumulating predicted bounding boxes. We suggest that by combining\nmany localization predictions, detection can be performed without training on background samples\nand that it is possible to avoid the time-consuming and complicated bootstrapping training passes.\nNot training on background also lets the network focus solely on positive classes for higher accuracy.\n1\nExperiments are conducted on the ImageNet ILSVRC 2012 and 2013 datasets and establish state of\nthe art results on the ILSVRC 2013 localization and detection tasks.\nWhile images from the ImageNet classi\ufb01cation dataset are largely chosen to contain a roughly-\ncentered object that \ufb01lls much of the image, objects of interest sometimes vary signi\ufb01cantly in size\nand position within the image. The \ufb01rst idea in addressing this is to apply a ConvNet at multiple\nlocations in the image, in a sliding window fashion, and over multiple scales. Even with this,\nhowever, many viewing windows may contain a perfectly identi\ufb01able portion of the object (say,\nthe head of a dog), but not the entire object, nor even the center of the object. This leads to decent\nclassi\ufb01cation but poor localization and detection. Thus, the second idea is to train the system to not\nonly produce a distribution over categories for each window, but also to produce a prediction of the\nlocation and size of the bounding box containing the object relative to the window. The third idea is\nto accumulate the evidence for each category at each location and size.\nMany authors have proposed to use ConvNets for detection and localization with a sliding window\nover multiple scales, going back to the early 1990\u2019s for multi-character strings [20], faces [30], and\nhands [22]. More recently, ConvNets have been shown to yield state of the art performance on text\ndetection in natural images [4], face detection [8, 23] and pedestrian detection [25].\nSeveral authors have also proposed to train ConvNets to directly predict the instantiation parameters\nof the objects to be located, such as the position relative to the viewing window, or the pose of\nthe object. For example Osadchy et al. [23] describe a ConvNet for simultaneous face detection\nand pose estimation. Faces are represented by a 3D manifold in the nine-dimensional output space.\nPositions on the manifold indicate the pose (pitch, yaw, and roll). When the training image is a\nface, the network is trained to produce a point on the manifold at the location of the known pose.\nIf the image is not a face, the output is pushed away from the manifold. At test time, the distance\nto the manifold indicate whether the image contains a face, and the position of the closest point on\nthe manifold indicates pose. Taylor et al. [27, 28] use a ConvNet to estimate the location of body\nparts (hands, head, etc) so as to derive the human body pose. They use a metric learning criterion\nto train the network to produce points on a body pose manifold. Hinton et al. have also proposed\nto train networks to compute explicit instantiation parameters of features as part of a recognition\nprocess [12].\nOther authors have proposed to perform object localization via ConvNet-based segmentation. The\nsimplest approach consists in training the ConvNet to classify the central pixel (or voxel for vol-\numetric images) of its viewing window as a boundary between regions or not [13]. But when the\nregions must be categorized, it is preferable to perform semantic segmentation. The main idea is to\ntrain the ConvNet to classify the central pixel of the viewing window with the category of the ob-\nject it belongs to, using the window as context for the decision. Applications range from biological\nimage analysis [21], to obstacle tagging for mobile robots [10] to tagging of photos [7]. The ad-\nvantage of this approach is that the bounding contours need not be rectangles, and the regions need\nnot be well-circumscribed objects. The disadvantage is that it requires dense pixel-level labels for\ntraining. This segmentation pre-processing or object proposal step has recently gained popularity in\ntraditional computer vision to reduce the search space of position, scale and aspect ratio for detec-\ntion [19, 2, 6, 29]. Hence an expensive classi\ufb01cation method can be applied at the optimal location\nin the search space, thus increasing recognition accuracy. Additionally, [29, 1] suggest that these\nmethods improve accuracy by drastically reducing unlikely object regions, hence reducing potential\nfalse positives. Our dense sliding window method, however, is able to outperform object proposal\nmethods on the ILSVRC13 detection dataset.\nKrizhevsky et al. [15] recently demonstrated impressive classi\ufb01cation performance using a large\nConvNet. The authors also entered the ImageNet 2012 competition, winning both the classi\ufb01cation\nand localization challenges. Although they demonstrated an impressive localization performance,\nthere has been no published work describing how their approach. Our paper is thus the \ufb01rst to\nprovide a clear explanation how ConvNets can be used for localization and detection for ImageNet\ndata.\nIn this paper we use the terms localization and detection in a way that is consistent with their use in\nthe ImageNet 2013 competition, namely that the only difference is the evaluation criterion used and\nboth involve predicting the bounding box for each object in the image.\n2\nFigure 1: Localization (top) and detection tasks (bottom). The left images contains our predic-\ntions (ordered by decreasing con\ufb01dence) while the right images show the groundtruth labels. The\ndetection image (bottom) illustrates the higher dif\ufb01culty of the detection dataset, which can contain\nmany small objects while the classi\ufb01cation and localization images typically contain a single large\nobject.\n2\nVision Tasks\nIn this paper, we explore three computer vision tasks in increasing order of dif\ufb01culty: (i) classi-\n\ufb01cation, (ii) localization, and (iii) detection. Each task is a sub-task of the next. While all tasks\nare adressed using a single framework and a shared feature learning base, we will describe them\nseparately in the following sections.\nThroughout the paper, we report results on the 2013 ImageNet Large Scale Visual Recognition Chal-\nlenge (ILSVRC2013). In the classi\ufb01cation task of this challenge, each image is assigned a single\nlabel corresponding to the main object in the image. Five guesses are allowed to \ufb01nd the correct\nanswer (this is because images can also contain multiple unlabeled objects). The localization task\nis similar in that 5 guesses are allowed per image, but in addition, a bounding box for the predicted\nobject must be returned with each guess. To be considered correct, the predicted box must match\nthe groundtruth by at least 50% (using the PASCAL criterion of union over intersection), as well as\nbe labeled with the correct class (i.e. each prediction is a label and bounding box that are associated\ntogether). The detection task differs from localization in that there can be any number of objects\nin each image (including zero), and false positives are penalized by the mean average precision\n3\n(mAP) measure. The localization task is a convenient intermediate step between classi\ufb01cation and\ndetection, and allows us to evaluate our localization method independently of challenges speci\ufb01c to\ndetection (such as learning a background class). In Fig. 1, we show examples of images with our\nlocalization/detection predictions as well as corresponding groundtruth. Note that classi\ufb01cation and\nlocalization share the same dataset, while detection also has additional data where objects can be\nsmaller. The detection data also contain a set of images where certain objects are absent. This can\nbe used for bootstrapping, but we have not made use of it in this work.\n3\nClassi\ufb01cation\nOur classi\ufb01cation architecture is similar to the best ILSVRC12 architecture by Krizhevsky et al. [15].\nHowever, we improve on the network design and the inference step. Because of time constraints,\nsome of the training features in Krizhevsky\u2019s model were not explored, and so we expect our results\ncan be improved even further. These are discussed in the future work section 6\nFigure 2: Layer 1 (top) and layer 2 \ufb01lters (bottom).\n3.1\nModel Design and Training\nWe train the network on the ImageNet 2012 training set (1.2 million images and C = 1000 classes)\n[5]. Our model uses the same \ufb01xed input size approach proposed by Krizhevsky et al. [15] during\ntraining but turns to multi-scale for classi\ufb01cation as described in the next section. Each image is\ndownsampled so that the smallest dimension is 256 pixels. We then extract 5 random crops (and\ntheir horizontal \ufb02ips) of size 221x221 pixels and present these to the network in mini-batches of\nsize 128. The weights in the network are initialized randomly with (\u00b5, \u03c3) = (0, 1 \u00d7 10\u22122). They\nare then updated by stochastic gradient descent, accompanied by momentum term of 0.6 and an \u21132\nweight decay of 1 \u00d7 10\u22125. The learning rate is initially 5 \u00d7 10\u22122 and is successively decreased by\na factor of 0.5 after (30, 50, 60, 70, 80) epochs. DropOut [11] with a rate of 0.5 is employed on the\nfully connected layers (6th and 7th) in the classi\ufb01er.\nWe detail the architecture sizes in tables 1 and 3. Note that during training, we treat this architecture\nas non-spatial (output maps of size 1x1), as opposed to the inference step, which produces spatial\noutputs. Layers 1-5 are similar to Krizhevsky et al. [15], using recti\ufb01cation (\u201crelu\u201d) non-linearities\nand max pooling, but with the following differences: (i) no contrast normalization is used; (ii)\npooling regions are non-overlapping and (iii) our model has larger 1st and 2nd layer feature maps,\nthanks to a smaller stride (2 instead of 4). A larger stride is bene\ufb01cial for speed but will hurt accuracy.\n4\nOutput\nLayer\n1\n2\n3\n4\n5\n6\n7\n8\nStage\nconv + max\nconv + max\nconv\nconv\nconv + max\nfull\nfull\nfull\n# channels\n96\n256\n512\n1024\n1024\n3072\n4096\n1000\nFilter size\n11x11\n5x5\n3x3\n3x3\n3x3\n-\n-\n-\nConv. stride\n4x4\n1x1\n1x1\n1x1\n1x1\n-\n-\n-\nPooling size\n2x2\n2x2\n-\n-\n2x2\n-\n-\n-\nPooling stride\n2x2\n2x2\n-\n-\n2x2\n-\n-\n-\nZero-Padding size\n-\n-\n1x1x1x1\n1x1x1x1\n1x1x1x1\n-\n-\n-\nSpatial input size\n231x231\n24x24\n12x12\n12x12\n12x12\n6x6\n1x1\n1x1\nTable 1: Architecture speci\ufb01cs for fast model. The spatial size of the feature maps depends on\nthe input image size, which varies during our inference step (see Table 5 in the Appendix). Here\nwe show training spatial sizes. Layer 5 is the top convolutional layer. Subsequent layers are fully\nconnected, and applied in sliding window fashion at test time. The fully-connected layers can also\nbe seen as 1x1 convolutions in a spatial setting. Similar sizes for accurate model can be found in\nthe Appendix.\nIn Fig. 2, we show the \ufb01lter coef\ufb01cients from the \ufb01rst two convolutional layers. The \ufb01rst layer \ufb01lters\ncapture orientated edges, patterns and blobs. In the second layer, the \ufb01lters have a variety of forms,\nsome diffuse, others with strong line structures or oriented edges.\n3.2\nFeature Extractor\nAlong with this paper, we release a feature extractor named \u201cOverFeat\u201d 1 in order to provide power-\nful features for computer vision research. Two models are provided, a fast and accurate one. Each\narchitecture is described in tables 1 and 3. We also compare their sizes in Table 4 in terms of param-\neters and connections. The accurate model is more accurate than the fast one (14.18% classi\ufb01cation\nerror as opposed to 16.39% in Table 2), however it requires nearly twice as many connections. Using\na committee of 7 accurate models reaches 13.6% classi\ufb01cation error as shown in Fig. 4.\n3.3\nMulti-Scale Classi\ufb01cation\nIn [15], multi-view voting is used to boost performance: a \ufb01xed set of 10 views (4 corners and center,\nwith horizontal \ufb02ip) is averaged. However, this approach can ignore many regions of the image, and\nis computationally redundant when views overlap. Additionally, it is only applied at a single scale,\nwhich may not be the scale at which the ConvNet will respond with optimal con\ufb01dence.\nInstead, we explore the entire image by densely running the network at each location and at multiple\nscales. While the sliding window approach may be computationally prohibitive for certain types\nof model, it is inherently ef\ufb01cient in the case of ConvNets (see section 3.5). This approach yields\nsigni\ufb01cantly more views for voting, which increases robustness while remaining ef\ufb01cient. The result\nof convolving a ConvNet on an image of arbitrary size is a spatial map of C-dimensional vectors at\neach scale.\nHowever, the total subsampling ratio in the network described above is 2x3x2x3, or 36. Hence\nwhen applied densely, this architecture can only produce a classi\ufb01cation vector every 36 pixels in\nthe input dimension along each axis. This coarse distribution of outputs decreases performance\ncompared to the 10-view scheme because the network windows are not well aligned with the objects\nin the images. The better aligned the network window and the object, the strongest the con\ufb01dence of\nthe network response. To circumvent this problem, we take an approach similar to that introduced\nby Giusti et al. [9], and apply the last subsampling operation at every offset. This removes the loss\nof resolution from this layer, yielding a total subsampling ratio of x12 instead of x36.\nWe now explain in detail how the resolution augmentation is performed. We use 6 scales of input\nwhich result in unpooled layer 5 maps of varying resolution (see Table 5 for details). These are then\npooled and presented to the classi\ufb01er using the following procedure, illustrated in Fig. 3:\n(a) For a single image, at a given scale, we start with the unpooled layer 5 feature maps.\n1http://cilvr.nyu.edu/doku.php?id=software:overfeat:start\n5\n(b) Each of unpooled maps undergoes a 3x3 max pooling operation (non-overlapping regions),\nrepeated 3x3 times for (\u2206x, \u2206y) pixel offsets of {0, 1, 2}.\n(c) This produces a set of pooled feature maps, replicated (3x3) times for different (\u2206x, \u2206y) com-\nbinations.\n(d) The classi\ufb01er (layers 6,7,8) has a \ufb01xed input size of 5x5 and produces a C-dimensional output\nvector for each location within the pooled maps. The classi\ufb01er is applied in sliding-window\nfashion to the pooled maps, yielding C-dimensional output maps (for a given (\u2206x, \u2206y) combi-\nnation).\n(e) The output maps for different (\u2206x, \u2206y) combinations are reshaped into a single 3D output map\n(two spatial dimensions x C classes).\nFigure 3: 1D illustration (to scale) of output map computation for classi\ufb01cation, using y-dimension\nfrom scale 2 as an example (see Table 5). (a): 20 pixel unpooled layer 5 feature map. (b): max\npooling over non-overlapping 3 pixel groups, using offsets of \u2206= {0, 1, 2} pixels (red, green, blue\nrespectively). (c): The resulting 6 pixel pooled maps, for different \u2206. (d): 5 pixel classi\ufb01er (layers\n6,7) is applied in sliding window fashion to pooled maps, yielding 2 pixel by C maps for each \u2206.\n(e): reshaped into 6 pixel by C output maps.\nThese operations can be viewed as shifting the classi\ufb01er\u2019s viewing window by 1 pixel through pool-\ning layers without subsampling and using skip-kernels in the following layer (where values in the\nneighborhood are non-adjacent). Or equivalently, as applying the \ufb01nal pooling layer and fully-\nconnected stack at every possible offset, and assembling the results by interleaving the outputs.\nThe procedure above is repeated for the horizontally \ufb02ipped version of each image. We then produce\nthe \ufb01nal classi\ufb01cation by (i) taking the spatial max for each class, at each scale and \ufb02ip; (ii) averaging\nthe resulting C-dimensional vectors from different scales and \ufb02ips and (iii) taking the top-1 or top-5\nelements (depending on the evaluation criterion) from the mean class vector.\nAt an intuitive level, the two halves of the network \u2014 i.e. feature extraction layers (1-5) and classi\ufb01er\nlayers (6-output) \u2014 are used in opposite ways. In the feature extraction portion, the \ufb01lters are\nconvolved across the entire image in one pass. From a computational perspective, this is far more\nef\ufb01cient than sliding a \ufb01xed-size feature extractor over the image and then aggregating the results\nfrom different locations2. However, these principles are reversed for the classi\ufb01er portion of the\nnetwork. Here, we want to hunt for a \ufb01xed-size representation in the layer 5 feature maps across\ndifferent positions and scales. Thus the classi\ufb01er has a \ufb01xed-size 5x5 input and is exhaustively\napplied to the layer 5 maps. The exhaustive pooling scheme (with single pixel shifts (\u2206x, \u2206y))\nensures that we can obtain \ufb01ne alignment between the classi\ufb01er and the representation of the object\nin the feature map.\n3.4\nResults\nIn Table 2, we experiment with different approaches, and compare them to the single network model\nof Krizhevsky et al. [15] for reference. The approach described above, with 6 scales, achieves a\ntop-5 error rate of 13.6%. As might be expected, using fewer scales hurts performance: the single-\nscale model is worse with 16.97% top-5 error. The \ufb01ne stride technique illustrated in Fig. 3 brings a\nrelatively small improvement in the single scale regime, but is also of importance for the multi-scale\ngains shown here.\n2Our network with 6 scales takes around 2 secs on a K20x GPU to process one image\n6\nTop-1\nTop-5\nApproach\nerror %\nerror %\nKrizhevsky et al. [15]\n40.7\n18.2\nOverFeat - 1 fast model, scale 1, coarse stride\n39.28\n17.12\nOverFeat - 1 fast model, scale 1, \ufb01ne stride\n39.01\n16.97\nOverFeat - 1 fast model, 4 scales (1,2,4,6), \ufb01ne stride\n38.57\n16.39\nOverFeat - 1 fast model, 6 scales (1-6), \ufb01ne stride\n38.12\n16.27\nOverFeat - 1 accurate model, 4 corners + center + \ufb02ip\n35.60\n14.71\nOverFeat - 1 accurate model, 4 scales, \ufb01ne stride\n35.74\n14.18\nOverFeat - 7 fast models, 4 scales, \ufb01ne stride\n35.10\n13.86\nOverFeat - 7 accurate models, 4 scales, \ufb01ne stride\n33.96\n13.24\nTable 2: Classi\ufb01cation experiments on validation set. Fine/coarse stride refers to the number of\n\u2206values used when applying the classi\ufb01er. Fine: \u2206= 0, 1, 2; coarse: \u2206= 0.\nFigure 4: Test set classi\ufb01cation results. During the competition, OverFeat yielded 14.2% top 5\nerror rate using an average of 7 fast models. In post-competition work, OverFeat ranks \ufb01fth with\n13.6% error using bigger models (more features and more layers).\nWe report the test set results of the 2013 competition in Fig. 4 where our model (OverFeat) obtained\n14.2% accuracy by voting of 7 ConvNets (each trained with different initializations) and ranked 5th\nout of 18 teams. The best accuracy using only ILSVRC13 data was 11.7%. Pre-training with extra\ndata from the ImageNet Fall11 dataset improved this number to 11.2%. In post-competition work,\nwe improve the OverFeat results down to 13.6% error by using bigger models (more features and\nmore layers). Due to time constraints, these bigger models are not fully trained, more improvements\nare expected to appear in time.\n3.5\nConvNets and Sliding Window Ef\ufb01ciency\nIn contrast to many sliding-window approaches that compute an entire pipeline for each window of\nthe input one at a time, ConvNets are inherently ef\ufb01cient when applied in a sliding fashion because\nthey naturally share computations common to overlapping regions. When applying our network\nto larger images at test time, we simply apply each convolution over the extent of the full image.\nThis extends the output of each layer to cover the new image size, eventually producing a map of\noutput class predictions, with one spatial location for each \u201cwindow\u201d (\ufb01eld of view) of input. This\n7\ninput\n1st stage\noutput\nclassifier\nconvolution\npooling\nconv\nconv\nconv\ninput\n1st stage\noutput\nclassifier\nconvolution\npooling\nconv\nconv\nconv\nFigure 5: The ef\ufb01ciency of ConvNets for detection. During training, a ConvNet produces only a\nsingle spatial output (top). But when applied at test time over a larger image, it produces a spatial\noutput map, e.g. 2x2 (bottom). Since all layers are applied convolutionally, the extra computa-\ntion required for the larger image is limited to the yellow regions. This diagram omits the feature\ndimension for simplicity.\nis diagrammed in Fig. 5. Convolutions are applied bottom-up, so that the computations common to\nneighboring windows need only be done once.\nNote that the last layers of our architecture are fully connected linear layers. At test time, these\nlayers are effectively replaced by convolution operations with kernels of 1x1 spatial extent. The\nentire ConvNet is then simply a sequence of convolutions, max-pooling and thresholding operations\nexclusively.\n4\nLocalization\nStarting from our classi\ufb01cation-trained network, we replace the classi\ufb01er layers by a regression\nnetwork and train it to predict object bounding boxes at each spatial location and scale. We then\ncombine the regression predictions together, along with the classi\ufb01cation results at each location, as\nwe now describe.\n4.1\nGenerating Predictions\nTo generate object bounding box predictions, we simultaneously run the classi\ufb01er and regressor\nnetworks across all locations and scales. Since these share the same feature extraction layers, only\nthe \ufb01nal regression layers need to be recomputed after computing the classi\ufb01cation network. The\noutput of the \ufb01nal softmax layer for a class c at each location provides a score of con\ufb01dence that\nan object of class c is present (though not necessarily fully contained) in the corresponding \ufb01eld of\nview. Thus we can assign a con\ufb01dence to each bounding box.\n4.2\nRegressor Training\nThe regression network takes as input the pooled feature maps from layer 5. It has 2 fully-connected\nhidden layers of size 4096 and 1024 channels, respectively. The \ufb01nal output layer has 4 units which\nspecify the coordinates for the bounding box edges. As with classi\ufb01cation, there are (3x3) copies\nthroughout, resulting from the \u2206x, \u2206y shifts. The architecture is shown in Fig. 8.\n8\nFigure 6: Localization/Detection pipeline. The raw classi\ufb01er/detector outputs a class and a con-\n\ufb01dence for each location (1st diagram). The resolution of these predictions can be increased using\nthe method described in section 3.3 (2nd diagram). The regression then predicts the location scale\nof the object with respect to each window (3rd diagram). These bounding boxes are then merge and\naccumulated to a small number of objects (4th diagram).\n9\nFigure 7: Examples of bounding boxes produced by the regression network, before being com-\nbined into \ufb01nal predictions. The examples shown here are at a single scale. Predictions may be\nmore optimal at other scales depending on the objects. Here, most of the bounding boxes which are\ninitially organized as a grid, converge to a single location and scale. This indicates that the network\nis very con\ufb01dent in the location of the object, as opposed to being spread out randomly. The top left\nimage shows that it can also correctly identify multiple location if several objects are present. The\nvarious aspect ratios of the predicted bounding boxes shows that the network is able to cope with\nvarious object poses.\nWe \ufb01x the feature extraction layers (1-5) from the classi\ufb01cation network and train the regression\nnetwork using an \u21132 loss between the predicted and true bounding box for each example. The \ufb01nal\nregressor layer is class-speci\ufb01c, having 1000 different versions, one for each class. We train this\nnetwork using the same set of scales as described in Section 3. We compare the prediction of the\nregressor net at each spatial location with the ground-truth bounding box, shifted into the frame of\nreference of the regressor\u2019s translation offset within the convolution (see Fig. 8). However, we do\nnot train the regressor on bounding boxes with less than 50% overlap with the input \ufb01eld of view:\nsince the object is mostly outside of these locations, it will be better handled by regression windows\nthat do contain the object.\nTraining the regressors in a multi-scale manner is important for the across-scale prediction combi-\nnation. Training on a single scale will perform well on that scale and still perform reasonably on\nother scales. However training multi-scale will make predictions match correctly across scales and\nexponentially increase the con\ufb01dence of the merged predictions. In turn, this allows us to perform\nwell with a few scales only, rather than many scales as is typically the case in detection. The typical\nratio from one scale to another in pedestrian detection [25] is about 1.05 to 1.1, here however we use\na large ratio of approximately 1.4 (this number differs for each scale since dimensions are adjusted\nto \ufb01t exactly the stride of our network) which allows us to run our system faster.\n4.3\nCombining Predictions\nWe combine the individual predictions (see Fig. 7) via a greedy merge strategy applied to the regres-\nsor bounding boxes, using the following algorithm.\n(a) Assign to Cs the set of classes in the top k for each scale s \u22081 . . . 6, found by taking the\nmaximum detection class outputs across spatial locations for that scale.\n(b) Assign to Bs the set of bounding boxes predicted by the regressor network for each class in Cs,\nacross all spatial locations at scale s.\n10\nFigure 8: Application of the regression network to layer 5 features, at scale 2, for example. (a)\nThe input to the regressor at this scale are 6x7 pixels spatially by 256 channels for each of the\n(3x3) \u2206x, \u2206y shifts. (b) Each unit in the 1st layer of the regression net is connected to a 5x5 spatial\nneighborhood in the layer 5 maps, as well as all 256 channels. Shifting the 5x5 neighborhood around\nresults in a map of 2x3 spatial extent, for each of the 4096 channels in the layer, and for each of\nthe (3x3) \u2206x, \u2206y shifts. (c) The 2nd regression layer has 1024 units and is fully connected (i.e. the\npurple element only connects to the purple element in (b), across all 4096 channels). (d) The output\nof the regression network is a 4-vector (specifying the edges of the bounding box) for each location\nin the 2x3 map, and for each of the (3x3) \u2206x, \u2206y shifts.\n(c) Assign B \u2190S\ns Bs\n(d) Repeat merging until done:\n(e)\n(b\u2217\n1, b\u2217\n2) = argminb1\u0338=b2\u2208Bmatch score(b1, b2)\n(f)\nIf match score(b\u2217\n1, b\u2217\n2) > t , stop.\n(g)\nOtherwise, set B \u2190B\\{b\u2217\n1, b\u2217\n2} \u222abox merge(b\u2217\n1, b\u2217\n2)\nIn the above, we compute match score using the sum of the distance between centers of the two\nbounding boxes and the intersection area of the boxes. box merge compute the average of the\nbounding boxes\u2019 coordinates.\nThe \ufb01nal prediction is given by taking the merged bounding boxes with maximum class scores. This\nis computed by cumulatively adding the detection class outputs associated with the input windows\nfrom which each bounding box was predicted. See Fig. 6 for an example of bounding boxes merged\ninto a single high-con\ufb01dence bounding box. In that example, some turtle and whale bounding boxes\nappear in the intermediate multi-scale steps, but disappear in the \ufb01nal detection image. Not only do\nthese bounding boxes have low classi\ufb01cation con\ufb01dence (at most 0.11 and 0.12 respectively), their\ncollection is not as coherent as the bear bounding boxes to get a signi\ufb01cant con\ufb01dence boost. The\nbear boxes have a strong con\ufb01dence (approximately 0.5 on average per scale) and high matching\nscores. Hence after merging, many bear bounding boxes are fused into a single very high con\ufb01dence\nbox, while false positives disappear below the detection threshold due their lack of bounding box\ncoherence and con\ufb01dence. This analysis suggest that our approach is naturally more robust to false\npositives coming from the pure-classi\ufb01cation model than traditional non-maximum suppression, by\nrewarding bounding box coherence.\n11\nFigure 9: Localization experiments on ILSVRC12 validation set. We experiment with different\nnumber of scales and with the use of single-class regression (SCR) or per-class regression (PCR).\n4.4\nExperiments\nWe apply our network to the Imagenet 2012 validation set using the localization criterion speci\ufb01ed\nfor the competition. The results for this are shown in Fig. 9. Fig. 10 shows the results of the 2012\nand 2013 localization competitions (the train and test data are the same for both of these years). Our\nmethod is the winner of the 2013 competition with 29.9% error.\nOur multiscale and multi-view approach was critical to obtaining good performance, as can be seen\nin Fig. 9: Using only a single centered crop, our regressor network achieves an error rate of 40%. By\ncombining regressor predictions from all spatial locations at two scales, we achieve a vastly better\nerror rate of 31.5%. Adding a third and fourth scale further improves performance to 30.0% error.\nUsing a different top layer for each class in the regressor network for each class (Per-Class Regres-\nsor (PCR) in Fig. 9) surprisingly did not outperform using only a single network shared among all\nclasses (44.1% vs. 31.3%). This may be because there are relatively few examples per class an-\nnotated with bounding boxes in the training set, while the network has 1000 times more top-layer\nparameters, resulting in insuf\ufb01cient training. It is possible this approach may be improved by shar-\ning parameters only among similar classes (e.g. training one network for all classes of dogs, another\nfor vehicles, etc.).\n5\nDetection\nDetection training is similar to classi\ufb01cation training but in a spatial manner. Multiple location of\nan image may be trained simultaneously. Since the model is convolutional, all weights are shared\namong all locations. The main difference with the localization task, is the necessity to predict a\nbackground class when no object is present. Traditionally, negative examples are initially taken at\nrandom for training. Then the most offending negative errors are added to the training set in boot-\nstrapping passes. Independent bootstrapping passes render training complicated and risk potential\nmismatches between the negative examples collection and training times. Additionally, the size of\nbootstrapping passes needs to be tuned to make sure training does not over\ufb01t on a small set. To cir-\ncumvent all these problems, we perform negative training on the \ufb02y, by selecting a few interesting\nnegative examples per image such as random ones or most offending ones. This approach is more\ncomputationally expensive, but renders the procedure much simpler. And since the feature extraction\nis initially trained with the classi\ufb01cation task, the detection \ufb01ne-tuning is not as long anyway.\nIn Fig. 11, we report the results of the ILSVRC 2013 competition where our detection system ranked\n3rd with 19.4% mean average precision (mAP). We later established a new detection state of the art\nwith 24.3% mAP. Note that there is a large gap between the top 3 methods and other teams (the 4th\n12\nFigure 10: ILSVRC12 and ILSVRC13 competitions results (test set). Our entry is the winner of\nthe ILSVRC13 localization competition with 29.9% error (top 5). Note that training and testing data\nis the same for both years. The OverFeat entry uses 4 scales and a single-class regression approach.\nFigure 11: ILSVRC13 test set Detection results. During the competition, UvA ranked \ufb01rst with\n22.6% mAP. In post competition work, we establish a new state of the art with 24.3% mAP. Systems\nmarked with * were pre-trained with the ILSVRC12 classi\ufb01cation data.\nmethod yields 11.5% mAP). Additionally, our approach is considerably different from the top 2 other\nsystems which use an initial segmentation step to reduce candidate windows from approximately\n200,000 to 2,000. This technique speeds up inference and substantially reduces the number of\npotential false positives. [29, 1] suggest that detection accuracy drops when using dense sliding\nwindow as opposed to selective search which discards unlikely object locations hence reducing\nfalse positives. Combined with our method, we may observe similar improvements as seen here\nbetween traditional dense methods and segmentation based methods. It should also be noted that\n13\nwe did not \ufb01ne tune on the detection validation set as NEC and UvA did. The validation and test\nset distributions differ signi\ufb01cantly enough from the training set that this alone improves results by\napproximately 1 point. The improvement between the two OverFeat results in Fig. 11 are due to\nlonger training times and the use of context, i.e. each scale also uses lower resolution scales as\ninput.\n6\nDiscussion\nWe have presented a multi-scale, sliding window approach that can be used for classi\ufb01cation, lo-\ncalization and detection. We applied it to the ILSVRC 2013 datasets, and it currently ranks 4th in\nclassi\ufb01cation, 1st in localization and 1st in detection. A second important contribution of our paper\nis explaining how ConvNets can be effectively used for detection and localization tasks. These were\nnever addressed in [15] and thus we are the \ufb01rst to explain how this can be done in the context of Im-\nageNet 2012. The scheme we propose involves substantial modi\ufb01cations to networks designed for\nclassi\ufb01cation, but clearly demonstrate that ConvNets are capable of these more challenging tasks.\nOur localization approach won the 2013 ILSVRC competition and signi\ufb01cantly outperformed all\n2012 and 2013 approaches. The detection model was among the top performers during the compe-\ntition, and ranks \ufb01rst in post-competition results. We have proposed an integrated pipeline that can\nperform different tasks while sharing a common feature extraction base, entirely learned directly\nfrom the pixels.\nOur approach might still be improved in several ways. (i) For localization, we are not currently\nback-propping through the whole network; doing so is likely to improve performance. (ii) We are\nusing \u21132 loss, rather than directly optimizing the intersection-over-union (IOU) criterion on which\nperformance is measured. Swapping the loss to this should be possible since IOU is still differen-\ntiable, provided there is some overlap. (iii) Alternate parameterizations of the bounding box may\nhelp to decorrelate the outputs, which will aid network training.\nReferences\n[1] J. Carreira, F. Li, and C. Sminchisescu. Object recognition by sequential \ufb01gure-ground ranking. Interna-\ntional journal of computer vision, 98(3):243\u2013262, 2012.\n[2] J. Carreira and C. Sminchisescu. Constrained parametric min-cuts for automatic object segmentation,\nrelease 1. http://sminchisescu.ins.uni-bonn.de/code/cpmc/.\n[3] D. C. Ciresan, J. Meier, and J. Schmidhuber. Multi-column deep neural networks for image classi\ufb01cation.\nIn CVPR, 2012.\n[4] M. Delakis and C. Garcia. Text detection with convolutional neural networks. In International Conference\non Computer Vision Theory and Applications (VISAPP 2008), 2008.\n[5] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical\nImage Database. In CVPR09, 2009.\n[6] I. Endres and D. Hoiem. Category independent object proposals. In Computer Vision\u2013ECCV 2010, pages\n575\u2013588. Springer, 2010.\n[7] C. Farabet, C. Couprie, L. Najman, and Y. LeCun. Learning hierarchical features for scene labeling. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 2013. in press.\n[8] C. Garcia and M. Delakis.\nConvolutional face \ufb01nder: A neural architecture for fast and robust face\ndetection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2004.\n[9] A. Giusti, D. C. Ciresan, J. Masci, L. M. Gambardella, and J. Schmidhuber. Fast image scanning with deep\nmax-pooling convolutional neural networks. In International Conference on Image Processing (ICIP),\n2013.\n[10] R. Hadsell, P. Sermanet, M. Scof\ufb01er, A. Erkan, K. Kavackuoglu, U. Muller, and Y. LeCun. Learning\nlong-range vision for autonomous off-road driving. Journal of Field Robotics, 26(2):120\u2013144, February\n2009.\n[11] G. Hinton, N. Srivastave, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov. Improving neural net-\nworks by preventing co-adaptation of feature detectors. arXiv:1207.0580, 2012.\n[12] G. E. Hinton, A. Krizhevsky, and S. D. Wang. Transforming auto-encoders. In Arti\ufb01cial Neural Networks\nand Machine Learning\u2013ICANN 2011, pages 44\u201351. Springer Berlin Heidelberg, 2011.\n[13] V. Jain, J. F. Murray, F. Roth, S. Turaga, V. Zhigulin, K. Briggman, M. Helmstaedter, W. Denk, and H. S.\nSeung. Supervised learning of image restoration with convolutional networks. In ICCV\u201907.\n[14] K. Jarrett, K. Kavukcuoglu, M. Ranzato, and Y. LeCun. What is the best multi-stage architecture for\nobject recognition? In Proc. International Conference on Computer Vision (ICCV\u201909). IEEE, 2009.\n14\n[15] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classi\ufb01cation with deep convolutional neural net-\nworks. In NIPS, 2012.\n[16] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Hand-\nwritten digit recognition with a back-propagation network. In D. Touretzky, editor, Advances in Neural\nInformation Processing Systems (NIPS 1989), volume 2, Denver, CO, 1990. Morgan Kaufman.\n[17] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition.\nProceedings of the IEEE, 86(11):2278\u20132324, November 1998.\n[18] Y. LeCun, F.-J. Huang, and L. Bottou. Learning methods for generic object recognition with invariance\nto pose and lighting. In Proceedings of CVPR\u201904. IEEE Press, 2004.\n[19] S. Manen, M. Guillaumin, and L. Van Gool. Prime object proposals with randomized prims algorithm. In\nInternational Conference on Computer Vision (ICCV), 2013.\n[20] O. Matan, J. Bromley, C. Burges, J. Denker, L. Jackel, Y. LeCun, E. Pednault, W. Satter\ufb01eld, C. Stenard,\nand T. Thompson. Reading handwritten digits: A zip code recognition system. IEEE Computer, 25(7):59\u2013\n63, July 1992.\n[21] F. Ning, D. Delhomme, Y. LeCun, F. Piano, L. Bottou, and P. Barbano. Toward automatic phenotyping of\ndeveloping embryos from videos. IEEE Transactions on Image Processing, 14(9):1360\u20131371, September\n2005. Special issue on Molecular and Cellular Bioimaging.\n[22] S. Nowlan and J. Platt. A convolutional neural network hand tracker. pages 901\u2013908, San Mateo, CA,\n1995. Morgan Kaufmann.\n[23] M. Osadchy, Y. LeCun, and M. Miller. Synergistic face detection and pose estimation with energy-based\nmodels. Journal of Machine Learning Research, 8:1197\u20131215, May 2007.\n[24] P. Sermanet, S. Chintala, and Y. LeCun. Convolutional neural networks applied to house numbers digit\nclassi\ufb01cation. In International Conference on Pattern Recognition (ICPR 2012), 2012.\n[25] P. Sermanet, K. Kavukcuoglu, S. Chintala, and Y. LeCun. Pedestrian detection with unsupervised multi-\nstage feature learning. In Proc. International Conference on Computer Vision and Pattern Recognition\n(CVPR\u201913). IEEE, June 2013.\n[26] P. Sermanet and Y. LeCun. Traf\ufb01c sign recognition with multi-scale convolutional networks. In Proceed-\nings of International Joint Conference on Neural Networks (IJCNN\u201911), 2011.\n[27] G. Taylor, R. Fergus, G. Williams, I. Spiro, and C. Bregler. Pose-sensitive embedding by nonlinear nca\nregression. In NIPS, 2011.\n[28] G. Taylor, I. Spiro, C. Bregler, and R. Fergus. Learning invarance through imitation. In CVPR, 2011.\n[29] J. R. R. Uijlings, K. E. A. van de Sande, T. Gevers, and A. W. M. Smeulders. Selective search for object\nrecognition. International Journal of Computer Vision, 104(2):154\u2013171, 2013.\n[30] R. Vaillant, C. Monrocq, and Y. LeCun. Original approach for the localisation of objects in images. IEE\nProc on Vision, Image, and Signal Processing, 141(4):245\u2013250, August 1994.\n15\nAppendix: Additional Model Details\nOutput\nLayer\n1\n2\n3\n4\n5\n6\n7\n8\n9\nStage\nconv + max\nconv + max\nconv\nconv\nconv\nconv + max\nfull\nfull\nfull\n# channels\n96\n256\n512\n512\n1024\n1024\n4096\n4096\n1000\nFilter size\n7x7\n7x7\n3x3\n3x3\n3x3\n3x3\n-\n-\n-\nConv. stride\n2x2\n1x1\n1x1\n1x1\n1x1\n1x1\n-\n-\n-\nPooling size\n3x3\n2x2\n-\n-\n-\n3x3\n-\n-\n-\nPooling stride\n3x3\n2x2\n-\n-\n-\n3x3\n-\n-\n-\nZero-Padding size\n-\n-\n1x1x1x1\n1x1x1x1\n1x1x1x1\n1x1x1x1\n-\n-\n-\nSpatial input size\n221x221\n36x36\n15x15\n15x15\n15x15\n15x15\n5x5\n1x1\n1x1\nTable 3: Architecture speci\ufb01cs for accurate model. It differs from the fast model mainly in the\nstride of the \ufb01rst convolution, the number of stages and the number of feature maps.\nmodel\n# parameters (in millions)\n# connections (in millions)\nKrizhevsky\n60\n-\nfast\n145\n2810\naccurate\n144\n5369\nTable 4: Number of parameters and connections for different models.\nInput\nLayer 5\nLayer 5\nClassi\ufb01er\nClassi\ufb01er\nScale\nsize\npre-pool\npost-pool\nmap (pre-reshape)\nmap size\n1\n245x245\n17x17\n(5x5)x(3x3)\n(1x1)x(3x3)xC\n3x3xC\n2\n281x317\n20x23\n(6x7)x(3x3)\n(2x3)x(3x3)xC\n6x9xC\n3\n317x389\n23x29\n(7x9)x(3x3)\n(3x5)x(3x3)xC\n9x15xC\n4\n389x461\n29x35\n(9x11)x(3x3)\n(5x7)x(3x3)xC\n15x21xC\n5\n425x497\n32x35\n(10x11)x(3x3)\n(6x7)x(3x3)xC\n18x24xC\n6\n461x569\n35x44\n(11x14)x(3x3)\n(7x10)x(3x3)xC\n21x30xC\nTable 5: Spatial dimensions of our multi-scale approach. 6 different sizes of input images are\nused, resulting in layer 5 unpooled feature maps of differing spatial resolution (although not indi-\ncated in the table, all have 256 feature channels). The (3x3) results from our dense pooling operation\nwith (\u2206x, \u2206y) = {0, 1, 2}. See text and Fig. 3 for details for how these are converted into output\nmaps.\n16\n",
        "sentence": " , 2015) or normal estimation (Sermanet et al., 2014; Eigen et al., 2014). A convenient component of such works is that the inherently convolutional nature of DCNNS allows for simple and efficient \u2018fully convolutional\u2019 implementations (Sermanet et al., 2014; Eigen et al., 2014; Oquab et al., 2015; Long et al., 2014; Chen et al., 2015).",
        "context": "ageNet 2012. The scheme we propose involves substantial modi\ufb01cations to networks designed for\nclassi\ufb01cation, but clearly demonstrate that ConvNets are capable of these more challenging tasks.\n[24] P. Sermanet, S. Chintala, and Y. LeCun. Convolutional neural networks applied to house numbers digit\nclassi\ufb01cation. In International Conference on Pattern Recognition (ICPR 2012), 2012.\nof model, it is inherently ef\ufb01cient in the case of ConvNets (see section 3.5). This approach yields\nsigni\ufb01cantly more views for voting, which increases robustness while remaining ef\ufb01cient. The result"
    },
    {
        "title": "Overfeat: Integrated recognition, localization and detection using convolutional networks",
        "author": [
            "Sermanet",
            "Pierre",
            "Eigen",
            "David",
            "Zhang",
            "Xiang",
            "Mathieu",
            "Micha\u00ebl",
            "Fergus",
            "Rob",
            "LeCun",
            "Yann"
        ],
        "venue": null,
        "citeRegEx": "Sermanet et al\\.,? \\Q2013\\E",
        "shortCiteRegEx": "Sermanet et al\\.",
        "year": 2013,
        "abstract": "We present an integrated framework for using Convolutional Networks for\nclassification, localization and detection. We show how a multiscale and\nsliding window approach can be efficiently implemented within a ConvNet. We\nalso introduce a novel deep learning approach to localization by learning to\npredict object boundaries. Bounding boxes are then accumulated rather than\nsuppressed in order to increase detection confidence. We show that different\ntasks can be learned simultaneously using a single shared network. This\nintegrated framework is the winner of the localization task of the ImageNet\nLarge Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very\ncompetitive results for the detection and classifications tasks. In\npost-competition work, we establish a new state of the art for the detection\ntask. Finally, we release a feature extractor from our best model called\nOverFeat.",
        "full_text": "arXiv:1312.6229v4  [cs.CV]  24 Feb 2014\nOverFeat:\nIntegrated Recognition, Localization and Detection\nusing Convolutional Networks\nPierre Sermanet\nDavid Eigen\nXiang Zhang\nMichael Mathieu\nRob Fergus\nYann LeCun\nCourant Institute of Mathematical Sciences, New York University\n719 Broadway, 12th Floor, New York, NY 10003\nsermanet,deigen,xiang,mathieu,fergus,yann@cs.nyu.edu\nAbstract\nWe present an integrated framework for using Convolutional Networks for classi-\n\ufb01cation, localization and detection. We show how a multiscale and sliding window\napproach can be ef\ufb01ciently implemented within a ConvNet. We also introduce a\nnovel deep learning approach to localization by learning to predict object bound-\naries. Bounding boxes are then accumulated rather than suppressed in order to\nincrease detection con\ufb01dence. We show that different tasks can be learned simul-\ntaneously using a single shared network. This integrated framework is the winner\nof the localization task of the ImageNet Large Scale Visual Recognition Challenge\n2013 (ILSVRC2013) and obtained very competitive results for the detection and\nclassi\ufb01cations tasks. In post-competition work, we establish a new state of the art\nfor the detection task. Finally, we release a feature extractor from our best model\ncalled OverFeat.\n1\nIntroduction\nRecognizing the category of the dominant object in an image is a tasks to which Convolutional\nNetworks (ConvNets) [17] have been applied for many years, whether the objects were handwritten\ncharacters [16], house numbers [24], textureless toys [18], traf\ufb01c signs [3, 26], objects from the\nCaltech-101 dataset [14], or objects from the 1000-category ImageNet dataset [15]. The accuracy\nof ConvNets on small datasets such as Caltech-101, while decent, has not been record-breaking.\nHowever, the advent of larger datasets has enabled ConvNets to signi\ufb01cantly advance the state of\nthe art on datasets such as the 1000-category ImageNet [5].\nThe main advantage of ConvNets for many such tasks is that the entire system is trained end to\nend, from raw pixels to ultimate categories, thereby alleviating the requirement to manually design\na suitable feature extractor. The main disadvantage is their ravenous appetite for labeled training\nsamples.\nThe main point of this paper is to show that training a convolutional network to simultaneously\nclassify, locate and detect objects in images can boost the classi\ufb01cation accuracy and the detection\nand localization accuracy of all tasks. The paper proposes a new integrated approach to object\ndetection, recognition, and localization with a single ConvNet. We also introduce a novel method for\nlocalization and detection by accumulating predicted bounding boxes. We suggest that by combining\nmany localization predictions, detection can be performed without training on background samples\nand that it is possible to avoid the time-consuming and complicated bootstrapping training passes.\nNot training on background also lets the network focus solely on positive classes for higher accuracy.\n1\nExperiments are conducted on the ImageNet ILSVRC 2012 and 2013 datasets and establish state of\nthe art results on the ILSVRC 2013 localization and detection tasks.\nWhile images from the ImageNet classi\ufb01cation dataset are largely chosen to contain a roughly-\ncentered object that \ufb01lls much of the image, objects of interest sometimes vary signi\ufb01cantly in size\nand position within the image. The \ufb01rst idea in addressing this is to apply a ConvNet at multiple\nlocations in the image, in a sliding window fashion, and over multiple scales. Even with this,\nhowever, many viewing windows may contain a perfectly identi\ufb01able portion of the object (say,\nthe head of a dog), but not the entire object, nor even the center of the object. This leads to decent\nclassi\ufb01cation but poor localization and detection. Thus, the second idea is to train the system to not\nonly produce a distribution over categories for each window, but also to produce a prediction of the\nlocation and size of the bounding box containing the object relative to the window. The third idea is\nto accumulate the evidence for each category at each location and size.\nMany authors have proposed to use ConvNets for detection and localization with a sliding window\nover multiple scales, going back to the early 1990\u2019s for multi-character strings [20], faces [30], and\nhands [22]. More recently, ConvNets have been shown to yield state of the art performance on text\ndetection in natural images [4], face detection [8, 23] and pedestrian detection [25].\nSeveral authors have also proposed to train ConvNets to directly predict the instantiation parameters\nof the objects to be located, such as the position relative to the viewing window, or the pose of\nthe object. For example Osadchy et al. [23] describe a ConvNet for simultaneous face detection\nand pose estimation. Faces are represented by a 3D manifold in the nine-dimensional output space.\nPositions on the manifold indicate the pose (pitch, yaw, and roll). When the training image is a\nface, the network is trained to produce a point on the manifold at the location of the known pose.\nIf the image is not a face, the output is pushed away from the manifold. At test time, the distance\nto the manifold indicate whether the image contains a face, and the position of the closest point on\nthe manifold indicates pose. Taylor et al. [27, 28] use a ConvNet to estimate the location of body\nparts (hands, head, etc) so as to derive the human body pose. They use a metric learning criterion\nto train the network to produce points on a body pose manifold. Hinton et al. have also proposed\nto train networks to compute explicit instantiation parameters of features as part of a recognition\nprocess [12].\nOther authors have proposed to perform object localization via ConvNet-based segmentation. The\nsimplest approach consists in training the ConvNet to classify the central pixel (or voxel for vol-\numetric images) of its viewing window as a boundary between regions or not [13]. But when the\nregions must be categorized, it is preferable to perform semantic segmentation. The main idea is to\ntrain the ConvNet to classify the central pixel of the viewing window with the category of the ob-\nject it belongs to, using the window as context for the decision. Applications range from biological\nimage analysis [21], to obstacle tagging for mobile robots [10] to tagging of photos [7]. The ad-\nvantage of this approach is that the bounding contours need not be rectangles, and the regions need\nnot be well-circumscribed objects. The disadvantage is that it requires dense pixel-level labels for\ntraining. This segmentation pre-processing or object proposal step has recently gained popularity in\ntraditional computer vision to reduce the search space of position, scale and aspect ratio for detec-\ntion [19, 2, 6, 29]. Hence an expensive classi\ufb01cation method can be applied at the optimal location\nin the search space, thus increasing recognition accuracy. Additionally, [29, 1] suggest that these\nmethods improve accuracy by drastically reducing unlikely object regions, hence reducing potential\nfalse positives. Our dense sliding window method, however, is able to outperform object proposal\nmethods on the ILSVRC13 detection dataset.\nKrizhevsky et al. [15] recently demonstrated impressive classi\ufb01cation performance using a large\nConvNet. The authors also entered the ImageNet 2012 competition, winning both the classi\ufb01cation\nand localization challenges. Although they demonstrated an impressive localization performance,\nthere has been no published work describing how their approach. Our paper is thus the \ufb01rst to\nprovide a clear explanation how ConvNets can be used for localization and detection for ImageNet\ndata.\nIn this paper we use the terms localization and detection in a way that is consistent with their use in\nthe ImageNet 2013 competition, namely that the only difference is the evaluation criterion used and\nboth involve predicting the bounding box for each object in the image.\n2\nFigure 1: Localization (top) and detection tasks (bottom). The left images contains our predic-\ntions (ordered by decreasing con\ufb01dence) while the right images show the groundtruth labels. The\ndetection image (bottom) illustrates the higher dif\ufb01culty of the detection dataset, which can contain\nmany small objects while the classi\ufb01cation and localization images typically contain a single large\nobject.\n2\nVision Tasks\nIn this paper, we explore three computer vision tasks in increasing order of dif\ufb01culty: (i) classi-\n\ufb01cation, (ii) localization, and (iii) detection. Each task is a sub-task of the next. While all tasks\nare adressed using a single framework and a shared feature learning base, we will describe them\nseparately in the following sections.\nThroughout the paper, we report results on the 2013 ImageNet Large Scale Visual Recognition Chal-\nlenge (ILSVRC2013). In the classi\ufb01cation task of this challenge, each image is assigned a single\nlabel corresponding to the main object in the image. Five guesses are allowed to \ufb01nd the correct\nanswer (this is because images can also contain multiple unlabeled objects). The localization task\nis similar in that 5 guesses are allowed per image, but in addition, a bounding box for the predicted\nobject must be returned with each guess. To be considered correct, the predicted box must match\nthe groundtruth by at least 50% (using the PASCAL criterion of union over intersection), as well as\nbe labeled with the correct class (i.e. each prediction is a label and bounding box that are associated\ntogether). The detection task differs from localization in that there can be any number of objects\nin each image (including zero), and false positives are penalized by the mean average precision\n3\n(mAP) measure. The localization task is a convenient intermediate step between classi\ufb01cation and\ndetection, and allows us to evaluate our localization method independently of challenges speci\ufb01c to\ndetection (such as learning a background class). In Fig. 1, we show examples of images with our\nlocalization/detection predictions as well as corresponding groundtruth. Note that classi\ufb01cation and\nlocalization share the same dataset, while detection also has additional data where objects can be\nsmaller. The detection data also contain a set of images where certain objects are absent. This can\nbe used for bootstrapping, but we have not made use of it in this work.\n3\nClassi\ufb01cation\nOur classi\ufb01cation architecture is similar to the best ILSVRC12 architecture by Krizhevsky et al. [15].\nHowever, we improve on the network design and the inference step. Because of time constraints,\nsome of the training features in Krizhevsky\u2019s model were not explored, and so we expect our results\ncan be improved even further. These are discussed in the future work section 6\nFigure 2: Layer 1 (top) and layer 2 \ufb01lters (bottom).\n3.1\nModel Design and Training\nWe train the network on the ImageNet 2012 training set (1.2 million images and C = 1000 classes)\n[5]. Our model uses the same \ufb01xed input size approach proposed by Krizhevsky et al. [15] during\ntraining but turns to multi-scale for classi\ufb01cation as described in the next section. Each image is\ndownsampled so that the smallest dimension is 256 pixels. We then extract 5 random crops (and\ntheir horizontal \ufb02ips) of size 221x221 pixels and present these to the network in mini-batches of\nsize 128. The weights in the network are initialized randomly with (\u00b5, \u03c3) = (0, 1 \u00d7 10\u22122). They\nare then updated by stochastic gradient descent, accompanied by momentum term of 0.6 and an \u21132\nweight decay of 1 \u00d7 10\u22125. The learning rate is initially 5 \u00d7 10\u22122 and is successively decreased by\na factor of 0.5 after (30, 50, 60, 70, 80) epochs. DropOut [11] with a rate of 0.5 is employed on the\nfully connected layers (6th and 7th) in the classi\ufb01er.\nWe detail the architecture sizes in tables 1 and 3. Note that during training, we treat this architecture\nas non-spatial (output maps of size 1x1), as opposed to the inference step, which produces spatial\noutputs. Layers 1-5 are similar to Krizhevsky et al. [15], using recti\ufb01cation (\u201crelu\u201d) non-linearities\nand max pooling, but with the following differences: (i) no contrast normalization is used; (ii)\npooling regions are non-overlapping and (iii) our model has larger 1st and 2nd layer feature maps,\nthanks to a smaller stride (2 instead of 4). A larger stride is bene\ufb01cial for speed but will hurt accuracy.\n4\nOutput\nLayer\n1\n2\n3\n4\n5\n6\n7\n8\nStage\nconv + max\nconv + max\nconv\nconv\nconv + max\nfull\nfull\nfull\n# channels\n96\n256\n512\n1024\n1024\n3072\n4096\n1000\nFilter size\n11x11\n5x5\n3x3\n3x3\n3x3\n-\n-\n-\nConv. stride\n4x4\n1x1\n1x1\n1x1\n1x1\n-\n-\n-\nPooling size\n2x2\n2x2\n-\n-\n2x2\n-\n-\n-\nPooling stride\n2x2\n2x2\n-\n-\n2x2\n-\n-\n-\nZero-Padding size\n-\n-\n1x1x1x1\n1x1x1x1\n1x1x1x1\n-\n-\n-\nSpatial input size\n231x231\n24x24\n12x12\n12x12\n12x12\n6x6\n1x1\n1x1\nTable 1: Architecture speci\ufb01cs for fast model. The spatial size of the feature maps depends on\nthe input image size, which varies during our inference step (see Table 5 in the Appendix). Here\nwe show training spatial sizes. Layer 5 is the top convolutional layer. Subsequent layers are fully\nconnected, and applied in sliding window fashion at test time. The fully-connected layers can also\nbe seen as 1x1 convolutions in a spatial setting. Similar sizes for accurate model can be found in\nthe Appendix.\nIn Fig. 2, we show the \ufb01lter coef\ufb01cients from the \ufb01rst two convolutional layers. The \ufb01rst layer \ufb01lters\ncapture orientated edges, patterns and blobs. In the second layer, the \ufb01lters have a variety of forms,\nsome diffuse, others with strong line structures or oriented edges.\n3.2\nFeature Extractor\nAlong with this paper, we release a feature extractor named \u201cOverFeat\u201d 1 in order to provide power-\nful features for computer vision research. Two models are provided, a fast and accurate one. Each\narchitecture is described in tables 1 and 3. We also compare their sizes in Table 4 in terms of param-\neters and connections. The accurate model is more accurate than the fast one (14.18% classi\ufb01cation\nerror as opposed to 16.39% in Table 2), however it requires nearly twice as many connections. Using\na committee of 7 accurate models reaches 13.6% classi\ufb01cation error as shown in Fig. 4.\n3.3\nMulti-Scale Classi\ufb01cation\nIn [15], multi-view voting is used to boost performance: a \ufb01xed set of 10 views (4 corners and center,\nwith horizontal \ufb02ip) is averaged. However, this approach can ignore many regions of the image, and\nis computationally redundant when views overlap. Additionally, it is only applied at a single scale,\nwhich may not be the scale at which the ConvNet will respond with optimal con\ufb01dence.\nInstead, we explore the entire image by densely running the network at each location and at multiple\nscales. While the sliding window approach may be computationally prohibitive for certain types\nof model, it is inherently ef\ufb01cient in the case of ConvNets (see section 3.5). This approach yields\nsigni\ufb01cantly more views for voting, which increases robustness while remaining ef\ufb01cient. The result\nof convolving a ConvNet on an image of arbitrary size is a spatial map of C-dimensional vectors at\neach scale.\nHowever, the total subsampling ratio in the network described above is 2x3x2x3, or 36. Hence\nwhen applied densely, this architecture can only produce a classi\ufb01cation vector every 36 pixels in\nthe input dimension along each axis. This coarse distribution of outputs decreases performance\ncompared to the 10-view scheme because the network windows are not well aligned with the objects\nin the images. The better aligned the network window and the object, the strongest the con\ufb01dence of\nthe network response. To circumvent this problem, we take an approach similar to that introduced\nby Giusti et al. [9], and apply the last subsampling operation at every offset. This removes the loss\nof resolution from this layer, yielding a total subsampling ratio of x12 instead of x36.\nWe now explain in detail how the resolution augmentation is performed. We use 6 scales of input\nwhich result in unpooled layer 5 maps of varying resolution (see Table 5 for details). These are then\npooled and presented to the classi\ufb01er using the following procedure, illustrated in Fig. 3:\n(a) For a single image, at a given scale, we start with the unpooled layer 5 feature maps.\n1http://cilvr.nyu.edu/doku.php?id=software:overfeat:start\n5\n(b) Each of unpooled maps undergoes a 3x3 max pooling operation (non-overlapping regions),\nrepeated 3x3 times for (\u2206x, \u2206y) pixel offsets of {0, 1, 2}.\n(c) This produces a set of pooled feature maps, replicated (3x3) times for different (\u2206x, \u2206y) com-\nbinations.\n(d) The classi\ufb01er (layers 6,7,8) has a \ufb01xed input size of 5x5 and produces a C-dimensional output\nvector for each location within the pooled maps. The classi\ufb01er is applied in sliding-window\nfashion to the pooled maps, yielding C-dimensional output maps (for a given (\u2206x, \u2206y) combi-\nnation).\n(e) The output maps for different (\u2206x, \u2206y) combinations are reshaped into a single 3D output map\n(two spatial dimensions x C classes).\nFigure 3: 1D illustration (to scale) of output map computation for classi\ufb01cation, using y-dimension\nfrom scale 2 as an example (see Table 5). (a): 20 pixel unpooled layer 5 feature map. (b): max\npooling over non-overlapping 3 pixel groups, using offsets of \u2206= {0, 1, 2} pixels (red, green, blue\nrespectively). (c): The resulting 6 pixel pooled maps, for different \u2206. (d): 5 pixel classi\ufb01er (layers\n6,7) is applied in sliding window fashion to pooled maps, yielding 2 pixel by C maps for each \u2206.\n(e): reshaped into 6 pixel by C output maps.\nThese operations can be viewed as shifting the classi\ufb01er\u2019s viewing window by 1 pixel through pool-\ning layers without subsampling and using skip-kernels in the following layer (where values in the\nneighborhood are non-adjacent). Or equivalently, as applying the \ufb01nal pooling layer and fully-\nconnected stack at every possible offset, and assembling the results by interleaving the outputs.\nThe procedure above is repeated for the horizontally \ufb02ipped version of each image. We then produce\nthe \ufb01nal classi\ufb01cation by (i) taking the spatial max for each class, at each scale and \ufb02ip; (ii) averaging\nthe resulting C-dimensional vectors from different scales and \ufb02ips and (iii) taking the top-1 or top-5\nelements (depending on the evaluation criterion) from the mean class vector.\nAt an intuitive level, the two halves of the network \u2014 i.e. feature extraction layers (1-5) and classi\ufb01er\nlayers (6-output) \u2014 are used in opposite ways. In the feature extraction portion, the \ufb01lters are\nconvolved across the entire image in one pass. From a computational perspective, this is far more\nef\ufb01cient than sliding a \ufb01xed-size feature extractor over the image and then aggregating the results\nfrom different locations2. However, these principles are reversed for the classi\ufb01er portion of the\nnetwork. Here, we want to hunt for a \ufb01xed-size representation in the layer 5 feature maps across\ndifferent positions and scales. Thus the classi\ufb01er has a \ufb01xed-size 5x5 input and is exhaustively\napplied to the layer 5 maps. The exhaustive pooling scheme (with single pixel shifts (\u2206x, \u2206y))\nensures that we can obtain \ufb01ne alignment between the classi\ufb01er and the representation of the object\nin the feature map.\n3.4\nResults\nIn Table 2, we experiment with different approaches, and compare them to the single network model\nof Krizhevsky et al. [15] for reference. The approach described above, with 6 scales, achieves a\ntop-5 error rate of 13.6%. As might be expected, using fewer scales hurts performance: the single-\nscale model is worse with 16.97% top-5 error. The \ufb01ne stride technique illustrated in Fig. 3 brings a\nrelatively small improvement in the single scale regime, but is also of importance for the multi-scale\ngains shown here.\n2Our network with 6 scales takes around 2 secs on a K20x GPU to process one image\n6\nTop-1\nTop-5\nApproach\nerror %\nerror %\nKrizhevsky et al. [15]\n40.7\n18.2\nOverFeat - 1 fast model, scale 1, coarse stride\n39.28\n17.12\nOverFeat - 1 fast model, scale 1, \ufb01ne stride\n39.01\n16.97\nOverFeat - 1 fast model, 4 scales (1,2,4,6), \ufb01ne stride\n38.57\n16.39\nOverFeat - 1 fast model, 6 scales (1-6), \ufb01ne stride\n38.12\n16.27\nOverFeat - 1 accurate model, 4 corners + center + \ufb02ip\n35.60\n14.71\nOverFeat - 1 accurate model, 4 scales, \ufb01ne stride\n35.74\n14.18\nOverFeat - 7 fast models, 4 scales, \ufb01ne stride\n35.10\n13.86\nOverFeat - 7 accurate models, 4 scales, \ufb01ne stride\n33.96\n13.24\nTable 2: Classi\ufb01cation experiments on validation set. Fine/coarse stride refers to the number of\n\u2206values used when applying the classi\ufb01er. Fine: \u2206= 0, 1, 2; coarse: \u2206= 0.\nFigure 4: Test set classi\ufb01cation results. During the competition, OverFeat yielded 14.2% top 5\nerror rate using an average of 7 fast models. In post-competition work, OverFeat ranks \ufb01fth with\n13.6% error using bigger models (more features and more layers).\nWe report the test set results of the 2013 competition in Fig. 4 where our model (OverFeat) obtained\n14.2% accuracy by voting of 7 ConvNets (each trained with different initializations) and ranked 5th\nout of 18 teams. The best accuracy using only ILSVRC13 data was 11.7%. Pre-training with extra\ndata from the ImageNet Fall11 dataset improved this number to 11.2%. In post-competition work,\nwe improve the OverFeat results down to 13.6% error by using bigger models (more features and\nmore layers). Due to time constraints, these bigger models are not fully trained, more improvements\nare expected to appear in time.\n3.5\nConvNets and Sliding Window Ef\ufb01ciency\nIn contrast to many sliding-window approaches that compute an entire pipeline for each window of\nthe input one at a time, ConvNets are inherently ef\ufb01cient when applied in a sliding fashion because\nthey naturally share computations common to overlapping regions. When applying our network\nto larger images at test time, we simply apply each convolution over the extent of the full image.\nThis extends the output of each layer to cover the new image size, eventually producing a map of\noutput class predictions, with one spatial location for each \u201cwindow\u201d (\ufb01eld of view) of input. This\n7\ninput\n1st stage\noutput\nclassifier\nconvolution\npooling\nconv\nconv\nconv\ninput\n1st stage\noutput\nclassifier\nconvolution\npooling\nconv\nconv\nconv\nFigure 5: The ef\ufb01ciency of ConvNets for detection. During training, a ConvNet produces only a\nsingle spatial output (top). But when applied at test time over a larger image, it produces a spatial\noutput map, e.g. 2x2 (bottom). Since all layers are applied convolutionally, the extra computa-\ntion required for the larger image is limited to the yellow regions. This diagram omits the feature\ndimension for simplicity.\nis diagrammed in Fig. 5. Convolutions are applied bottom-up, so that the computations common to\nneighboring windows need only be done once.\nNote that the last layers of our architecture are fully connected linear layers. At test time, these\nlayers are effectively replaced by convolution operations with kernels of 1x1 spatial extent. The\nentire ConvNet is then simply a sequence of convolutions, max-pooling and thresholding operations\nexclusively.\n4\nLocalization\nStarting from our classi\ufb01cation-trained network, we replace the classi\ufb01er layers by a regression\nnetwork and train it to predict object bounding boxes at each spatial location and scale. We then\ncombine the regression predictions together, along with the classi\ufb01cation results at each location, as\nwe now describe.\n4.1\nGenerating Predictions\nTo generate object bounding box predictions, we simultaneously run the classi\ufb01er and regressor\nnetworks across all locations and scales. Since these share the same feature extraction layers, only\nthe \ufb01nal regression layers need to be recomputed after computing the classi\ufb01cation network. The\noutput of the \ufb01nal softmax layer for a class c at each location provides a score of con\ufb01dence that\nan object of class c is present (though not necessarily fully contained) in the corresponding \ufb01eld of\nview. Thus we can assign a con\ufb01dence to each bounding box.\n4.2\nRegressor Training\nThe regression network takes as input the pooled feature maps from layer 5. It has 2 fully-connected\nhidden layers of size 4096 and 1024 channels, respectively. The \ufb01nal output layer has 4 units which\nspecify the coordinates for the bounding box edges. As with classi\ufb01cation, there are (3x3) copies\nthroughout, resulting from the \u2206x, \u2206y shifts. The architecture is shown in Fig. 8.\n8\nFigure 6: Localization/Detection pipeline. The raw classi\ufb01er/detector outputs a class and a con-\n\ufb01dence for each location (1st diagram). The resolution of these predictions can be increased using\nthe method described in section 3.3 (2nd diagram). The regression then predicts the location scale\nof the object with respect to each window (3rd diagram). These bounding boxes are then merge and\naccumulated to a small number of objects (4th diagram).\n9\nFigure 7: Examples of bounding boxes produced by the regression network, before being com-\nbined into \ufb01nal predictions. The examples shown here are at a single scale. Predictions may be\nmore optimal at other scales depending on the objects. Here, most of the bounding boxes which are\ninitially organized as a grid, converge to a single location and scale. This indicates that the network\nis very con\ufb01dent in the location of the object, as opposed to being spread out randomly. The top left\nimage shows that it can also correctly identify multiple location if several objects are present. The\nvarious aspect ratios of the predicted bounding boxes shows that the network is able to cope with\nvarious object poses.\nWe \ufb01x the feature extraction layers (1-5) from the classi\ufb01cation network and train the regression\nnetwork using an \u21132 loss between the predicted and true bounding box for each example. The \ufb01nal\nregressor layer is class-speci\ufb01c, having 1000 different versions, one for each class. We train this\nnetwork using the same set of scales as described in Section 3. We compare the prediction of the\nregressor net at each spatial location with the ground-truth bounding box, shifted into the frame of\nreference of the regressor\u2019s translation offset within the convolution (see Fig. 8). However, we do\nnot train the regressor on bounding boxes with less than 50% overlap with the input \ufb01eld of view:\nsince the object is mostly outside of these locations, it will be better handled by regression windows\nthat do contain the object.\nTraining the regressors in a multi-scale manner is important for the across-scale prediction combi-\nnation. Training on a single scale will perform well on that scale and still perform reasonably on\nother scales. However training multi-scale will make predictions match correctly across scales and\nexponentially increase the con\ufb01dence of the merged predictions. In turn, this allows us to perform\nwell with a few scales only, rather than many scales as is typically the case in detection. The typical\nratio from one scale to another in pedestrian detection [25] is about 1.05 to 1.1, here however we use\na large ratio of approximately 1.4 (this number differs for each scale since dimensions are adjusted\nto \ufb01t exactly the stride of our network) which allows us to run our system faster.\n4.3\nCombining Predictions\nWe combine the individual predictions (see Fig. 7) via a greedy merge strategy applied to the regres-\nsor bounding boxes, using the following algorithm.\n(a) Assign to Cs the set of classes in the top k for each scale s \u22081 . . . 6, found by taking the\nmaximum detection class outputs across spatial locations for that scale.\n(b) Assign to Bs the set of bounding boxes predicted by the regressor network for each class in Cs,\nacross all spatial locations at scale s.\n10\nFigure 8: Application of the regression network to layer 5 features, at scale 2, for example. (a)\nThe input to the regressor at this scale are 6x7 pixels spatially by 256 channels for each of the\n(3x3) \u2206x, \u2206y shifts. (b) Each unit in the 1st layer of the regression net is connected to a 5x5 spatial\nneighborhood in the layer 5 maps, as well as all 256 channels. Shifting the 5x5 neighborhood around\nresults in a map of 2x3 spatial extent, for each of the 4096 channels in the layer, and for each of\nthe (3x3) \u2206x, \u2206y shifts. (c) The 2nd regression layer has 1024 units and is fully connected (i.e. the\npurple element only connects to the purple element in (b), across all 4096 channels). (d) The output\nof the regression network is a 4-vector (specifying the edges of the bounding box) for each location\nin the 2x3 map, and for each of the (3x3) \u2206x, \u2206y shifts.\n(c) Assign B \u2190S\ns Bs\n(d) Repeat merging until done:\n(e)\n(b\u2217\n1, b\u2217\n2) = argminb1\u0338=b2\u2208Bmatch score(b1, b2)\n(f)\nIf match score(b\u2217\n1, b\u2217\n2) > t , stop.\n(g)\nOtherwise, set B \u2190B\\{b\u2217\n1, b\u2217\n2} \u222abox merge(b\u2217\n1, b\u2217\n2)\nIn the above, we compute match score using the sum of the distance between centers of the two\nbounding boxes and the intersection area of the boxes. box merge compute the average of the\nbounding boxes\u2019 coordinates.\nThe \ufb01nal prediction is given by taking the merged bounding boxes with maximum class scores. This\nis computed by cumulatively adding the detection class outputs associated with the input windows\nfrom which each bounding box was predicted. See Fig. 6 for an example of bounding boxes merged\ninto a single high-con\ufb01dence bounding box. In that example, some turtle and whale bounding boxes\nappear in the intermediate multi-scale steps, but disappear in the \ufb01nal detection image. Not only do\nthese bounding boxes have low classi\ufb01cation con\ufb01dence (at most 0.11 and 0.12 respectively), their\ncollection is not as coherent as the bear bounding boxes to get a signi\ufb01cant con\ufb01dence boost. The\nbear boxes have a strong con\ufb01dence (approximately 0.5 on average per scale) and high matching\nscores. Hence after merging, many bear bounding boxes are fused into a single very high con\ufb01dence\nbox, while false positives disappear below the detection threshold due their lack of bounding box\ncoherence and con\ufb01dence. This analysis suggest that our approach is naturally more robust to false\npositives coming from the pure-classi\ufb01cation model than traditional non-maximum suppression, by\nrewarding bounding box coherence.\n11\nFigure 9: Localization experiments on ILSVRC12 validation set. We experiment with different\nnumber of scales and with the use of single-class regression (SCR) or per-class regression (PCR).\n4.4\nExperiments\nWe apply our network to the Imagenet 2012 validation set using the localization criterion speci\ufb01ed\nfor the competition. The results for this are shown in Fig. 9. Fig. 10 shows the results of the 2012\nand 2013 localization competitions (the train and test data are the same for both of these years). Our\nmethod is the winner of the 2013 competition with 29.9% error.\nOur multiscale and multi-view approach was critical to obtaining good performance, as can be seen\nin Fig. 9: Using only a single centered crop, our regressor network achieves an error rate of 40%. By\ncombining regressor predictions from all spatial locations at two scales, we achieve a vastly better\nerror rate of 31.5%. Adding a third and fourth scale further improves performance to 30.0% error.\nUsing a different top layer for each class in the regressor network for each class (Per-Class Regres-\nsor (PCR) in Fig. 9) surprisingly did not outperform using only a single network shared among all\nclasses (44.1% vs. 31.3%). This may be because there are relatively few examples per class an-\nnotated with bounding boxes in the training set, while the network has 1000 times more top-layer\nparameters, resulting in insuf\ufb01cient training. It is possible this approach may be improved by shar-\ning parameters only among similar classes (e.g. training one network for all classes of dogs, another\nfor vehicles, etc.).\n5\nDetection\nDetection training is similar to classi\ufb01cation training but in a spatial manner. Multiple location of\nan image may be trained simultaneously. Since the model is convolutional, all weights are shared\namong all locations. The main difference with the localization task, is the necessity to predict a\nbackground class when no object is present. Traditionally, negative examples are initially taken at\nrandom for training. Then the most offending negative errors are added to the training set in boot-\nstrapping passes. Independent bootstrapping passes render training complicated and risk potential\nmismatches between the negative examples collection and training times. Additionally, the size of\nbootstrapping passes needs to be tuned to make sure training does not over\ufb01t on a small set. To cir-\ncumvent all these problems, we perform negative training on the \ufb02y, by selecting a few interesting\nnegative examples per image such as random ones or most offending ones. This approach is more\ncomputationally expensive, but renders the procedure much simpler. And since the feature extraction\nis initially trained with the classi\ufb01cation task, the detection \ufb01ne-tuning is not as long anyway.\nIn Fig. 11, we report the results of the ILSVRC 2013 competition where our detection system ranked\n3rd with 19.4% mean average precision (mAP). We later established a new detection state of the art\nwith 24.3% mAP. Note that there is a large gap between the top 3 methods and other teams (the 4th\n12\nFigure 10: ILSVRC12 and ILSVRC13 competitions results (test set). Our entry is the winner of\nthe ILSVRC13 localization competition with 29.9% error (top 5). Note that training and testing data\nis the same for both years. The OverFeat entry uses 4 scales and a single-class regression approach.\nFigure 11: ILSVRC13 test set Detection results. During the competition, UvA ranked \ufb01rst with\n22.6% mAP. In post competition work, we establish a new state of the art with 24.3% mAP. Systems\nmarked with * were pre-trained with the ILSVRC12 classi\ufb01cation data.\nmethod yields 11.5% mAP). Additionally, our approach is considerably different from the top 2 other\nsystems which use an initial segmentation step to reduce candidate windows from approximately\n200,000 to 2,000. This technique speeds up inference and substantially reduces the number of\npotential false positives. [29, 1] suggest that detection accuracy drops when using dense sliding\nwindow as opposed to selective search which discards unlikely object locations hence reducing\nfalse positives. Combined with our method, we may observe similar improvements as seen here\nbetween traditional dense methods and segmentation based methods. It should also be noted that\n13\nwe did not \ufb01ne tune on the detection validation set as NEC and UvA did. The validation and test\nset distributions differ signi\ufb01cantly enough from the training set that this alone improves results by\napproximately 1 point. The improvement between the two OverFeat results in Fig. 11 are due to\nlonger training times and the use of context, i.e. each scale also uses lower resolution scales as\ninput.\n6\nDiscussion\nWe have presented a multi-scale, sliding window approach that can be used for classi\ufb01cation, lo-\ncalization and detection. We applied it to the ILSVRC 2013 datasets, and it currently ranks 4th in\nclassi\ufb01cation, 1st in localization and 1st in detection. A second important contribution of our paper\nis explaining how ConvNets can be effectively used for detection and localization tasks. These were\nnever addressed in [15] and thus we are the \ufb01rst to explain how this can be done in the context of Im-\nageNet 2012. The scheme we propose involves substantial modi\ufb01cations to networks designed for\nclassi\ufb01cation, but clearly demonstrate that ConvNets are capable of these more challenging tasks.\nOur localization approach won the 2013 ILSVRC competition and signi\ufb01cantly outperformed all\n2012 and 2013 approaches. The detection model was among the top performers during the compe-\ntition, and ranks \ufb01rst in post-competition results. We have proposed an integrated pipeline that can\nperform different tasks while sharing a common feature extraction base, entirely learned directly\nfrom the pixels.\nOur approach might still be improved in several ways. (i) For localization, we are not currently\nback-propping through the whole network; doing so is likely to improve performance. (ii) We are\nusing \u21132 loss, rather than directly optimizing the intersection-over-union (IOU) criterion on which\nperformance is measured. Swapping the loss to this should be possible since IOU is still differen-\ntiable, provided there is some overlap. (iii) Alternate parameterizations of the bounding box may\nhelp to decorrelate the outputs, which will aid network training.\nReferences\n[1] J. Carreira, F. Li, and C. Sminchisescu. Object recognition by sequential \ufb01gure-ground ranking. Interna-\ntional journal of computer vision, 98(3):243\u2013262, 2012.\n[2] J. Carreira and C. Sminchisescu. Constrained parametric min-cuts for automatic object segmentation,\nrelease 1. http://sminchisescu.ins.uni-bonn.de/code/cpmc/.\n[3] D. C. Ciresan, J. Meier, and J. Schmidhuber. Multi-column deep neural networks for image classi\ufb01cation.\nIn CVPR, 2012.\n[4] M. Delakis and C. Garcia. Text detection with convolutional neural networks. In International Conference\non Computer Vision Theory and Applications (VISAPP 2008), 2008.\n[5] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical\nImage Database. In CVPR09, 2009.\n[6] I. Endres and D. Hoiem. Category independent object proposals. In Computer Vision\u2013ECCV 2010, pages\n575\u2013588. Springer, 2010.\n[7] C. Farabet, C. Couprie, L. Najman, and Y. LeCun. Learning hierarchical features for scene labeling. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 2013. in press.\n[8] C. Garcia and M. Delakis.\nConvolutional face \ufb01nder: A neural architecture for fast and robust face\ndetection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2004.\n[9] A. Giusti, D. C. Ciresan, J. Masci, L. M. Gambardella, and J. Schmidhuber. Fast image scanning with deep\nmax-pooling convolutional neural networks. In International Conference on Image Processing (ICIP),\n2013.\n[10] R. Hadsell, P. Sermanet, M. Scof\ufb01er, A. Erkan, K. Kavackuoglu, U. Muller, and Y. LeCun. Learning\nlong-range vision for autonomous off-road driving. Journal of Field Robotics, 26(2):120\u2013144, February\n2009.\n[11] G. Hinton, N. Srivastave, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov. Improving neural net-\nworks by preventing co-adaptation of feature detectors. arXiv:1207.0580, 2012.\n[12] G. E. Hinton, A. Krizhevsky, and S. D. Wang. Transforming auto-encoders. In Arti\ufb01cial Neural Networks\nand Machine Learning\u2013ICANN 2011, pages 44\u201351. Springer Berlin Heidelberg, 2011.\n[13] V. Jain, J. F. Murray, F. Roth, S. Turaga, V. Zhigulin, K. Briggman, M. Helmstaedter, W. Denk, and H. S.\nSeung. Supervised learning of image restoration with convolutional networks. In ICCV\u201907.\n[14] K. Jarrett, K. Kavukcuoglu, M. Ranzato, and Y. LeCun. What is the best multi-stage architecture for\nobject recognition? In Proc. International Conference on Computer Vision (ICCV\u201909). IEEE, 2009.\n14\n[15] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classi\ufb01cation with deep convolutional neural net-\nworks. In NIPS, 2012.\n[16] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Hand-\nwritten digit recognition with a back-propagation network. In D. Touretzky, editor, Advances in Neural\nInformation Processing Systems (NIPS 1989), volume 2, Denver, CO, 1990. Morgan Kaufman.\n[17] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition.\nProceedings of the IEEE, 86(11):2278\u20132324, November 1998.\n[18] Y. LeCun, F.-J. Huang, and L. Bottou. Learning methods for generic object recognition with invariance\nto pose and lighting. In Proceedings of CVPR\u201904. IEEE Press, 2004.\n[19] S. Manen, M. Guillaumin, and L. Van Gool. Prime object proposals with randomized prims algorithm. In\nInternational Conference on Computer Vision (ICCV), 2013.\n[20] O. Matan, J. Bromley, C. Burges, J. Denker, L. Jackel, Y. LeCun, E. Pednault, W. Satter\ufb01eld, C. Stenard,\nand T. Thompson. Reading handwritten digits: A zip code recognition system. IEEE Computer, 25(7):59\u2013\n63, July 1992.\n[21] F. Ning, D. Delhomme, Y. LeCun, F. Piano, L. Bottou, and P. Barbano. Toward automatic phenotyping of\ndeveloping embryos from videos. IEEE Transactions on Image Processing, 14(9):1360\u20131371, September\n2005. Special issue on Molecular and Cellular Bioimaging.\n[22] S. Nowlan and J. Platt. A convolutional neural network hand tracker. pages 901\u2013908, San Mateo, CA,\n1995. Morgan Kaufmann.\n[23] M. Osadchy, Y. LeCun, and M. Miller. Synergistic face detection and pose estimation with energy-based\nmodels. Journal of Machine Learning Research, 8:1197\u20131215, May 2007.\n[24] P. Sermanet, S. Chintala, and Y. LeCun. Convolutional neural networks applied to house numbers digit\nclassi\ufb01cation. In International Conference on Pattern Recognition (ICPR 2012), 2012.\n[25] P. Sermanet, K. Kavukcuoglu, S. Chintala, and Y. LeCun. Pedestrian detection with unsupervised multi-\nstage feature learning. In Proc. International Conference on Computer Vision and Pattern Recognition\n(CVPR\u201913). IEEE, June 2013.\n[26] P. Sermanet and Y. LeCun. Traf\ufb01c sign recognition with multi-scale convolutional networks. In Proceed-\nings of International Joint Conference on Neural Networks (IJCNN\u201911), 2011.\n[27] G. Taylor, R. Fergus, G. Williams, I. Spiro, and C. Bregler. Pose-sensitive embedding by nonlinear nca\nregression. In NIPS, 2011.\n[28] G. Taylor, I. Spiro, C. Bregler, and R. Fergus. Learning invarance through imitation. In CVPR, 2011.\n[29] J. R. R. Uijlings, K. E. A. van de Sande, T. Gevers, and A. W. M. Smeulders. Selective search for object\nrecognition. International Journal of Computer Vision, 104(2):154\u2013171, 2013.\n[30] R. Vaillant, C. Monrocq, and Y. LeCun. Original approach for the localisation of objects in images. IEE\nProc on Vision, Image, and Signal Processing, 141(4):245\u2013250, August 1994.\n15\nAppendix: Additional Model Details\nOutput\nLayer\n1\n2\n3\n4\n5\n6\n7\n8\n9\nStage\nconv + max\nconv + max\nconv\nconv\nconv\nconv + max\nfull\nfull\nfull\n# channels\n96\n256\n512\n512\n1024\n1024\n4096\n4096\n1000\nFilter size\n7x7\n7x7\n3x3\n3x3\n3x3\n3x3\n-\n-\n-\nConv. stride\n2x2\n1x1\n1x1\n1x1\n1x1\n1x1\n-\n-\n-\nPooling size\n3x3\n2x2\n-\n-\n-\n3x3\n-\n-\n-\nPooling stride\n3x3\n2x2\n-\n-\n-\n3x3\n-\n-\n-\nZero-Padding size\n-\n-\n1x1x1x1\n1x1x1x1\n1x1x1x1\n1x1x1x1\n-\n-\n-\nSpatial input size\n221x221\n36x36\n15x15\n15x15\n15x15\n15x15\n5x5\n1x1\n1x1\nTable 3: Architecture speci\ufb01cs for accurate model. It differs from the fast model mainly in the\nstride of the \ufb01rst convolution, the number of stages and the number of feature maps.\nmodel\n# parameters (in millions)\n# connections (in millions)\nKrizhevsky\n60\n-\nfast\n145\n2810\naccurate\n144\n5369\nTable 4: Number of parameters and connections for different models.\nInput\nLayer 5\nLayer 5\nClassi\ufb01er\nClassi\ufb01er\nScale\nsize\npre-pool\npost-pool\nmap (pre-reshape)\nmap size\n1\n245x245\n17x17\n(5x5)x(3x3)\n(1x1)x(3x3)xC\n3x3xC\n2\n281x317\n20x23\n(6x7)x(3x3)\n(2x3)x(3x3)xC\n6x9xC\n3\n317x389\n23x29\n(7x9)x(3x3)\n(3x5)x(3x3)xC\n9x15xC\n4\n389x461\n29x35\n(9x11)x(3x3)\n(5x7)x(3x3)xC\n15x21xC\n5\n425x497\n32x35\n(10x11)x(3x3)\n(6x7)x(3x3)xC\n18x24xC\n6\n461x569\n35x44\n(11x14)x(3x3)\n(7x10)x(3x3)xC\n21x30xC\nTable 5: Spatial dimensions of our multi-scale approach. 6 different sizes of input images are\nused, resulting in layer 5 unpooled feature maps of differing spatial resolution (although not indi-\ncated in the table, all have 256 feature channels). The (3x3) results from our dense pooling operation\nwith (\u2206x, \u2206y) = {0, 1, 2}. See text and Fig. 3 for details for how these are converted into output\nmaps.\n16\n",
        "sentence": " (1998) have delivered compelling results in high-level vision tasks, such as image classification (Krizhevsky et al., 2013; Sermanet et al., 2013; Simonyan & Zisserman, 2014; Szegedy et al., 2014; Papandreou et al., 2015b) or object detection (Girshick et al.",
        "context": "Krizhevsky et al. [15] recently demonstrated impressive classi\ufb01cation performance using a large\nConvNet. The authors also entered the ImageNet 2012 competition, winning both the classi\ufb01cation\nobject recognition? In Proc. International Conference on Computer Vision (ICCV\u201909). IEEE, 2009.\n14\n[15] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classi\ufb01cation with deep convolutional neural net-\nworks. In NIPS, 2012.\n[18] Y. LeCun, F.-J. Huang, and L. Bottou. Learning methods for generic object recognition with invariance\nto pose and lighting. In Proceedings of CVPR\u201904. IEEE Press, 2004."
    },
    {
        "title": "Deepcontour: A deep convolutional feature learned by positive-sharing loss for contour detection",
        "author": [
            "Shen",
            "Wei",
            "Wang",
            "Xinggang",
            "Yan",
            "Bai",
            "Xiang",
            "Zhang",
            "Zhijiang"
        ],
        "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
        "citeRegEx": "Shen et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "Shen et al\\.",
        "year": 2015,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " 798 DeepContour (Shen et al., 2015) 0.",
        "context": null
    },
    {
        "title": "Normalized cuts and image segmentation",
        "author": [
            "Shi",
            "Jianbo",
            "Malik",
            "Jitendra"
        ],
        "venue": "In Proc. CVPR,",
        "citeRegEx": "Shi et al\\.,? \\Q1997\\E",
        "shortCiteRegEx": "Shi et al\\.",
        "year": 1997,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Very deep convolutional networks for large-scale image recognition",
        "author": [
            "Simonyan",
            "Karen",
            "Zisserman",
            "Andrew"
        ],
        "venue": null,
        "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Simonyan et al\\.",
        "year": 2014,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Learning-based symmetry detection in natural images",
        "author": [
            "Tsogkas",
            "Stavros",
            "Kokkinos",
            "Iasonas"
        ],
        "venue": "In Proc. ECCV,",
        "citeRegEx": "Tsogkas et al\\.,? \\Q2012\\E",
        "shortCiteRegEx": "Tsogkas et al\\.",
        "year": 2012,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Selective search for object recognition",
        "author": [
            "Uijlings",
            "Jasper R. R",
            "van de Sande",
            "Koen E. A",
            "Gevers",
            "Theo",
            "Smeulders",
            "Arnold W. M"
        ],
        "venue": null,
        "citeRegEx": "Uijlings et al\\.,? \\Q2013\\E",
        "shortCiteRegEx": "Uijlings et al\\.",
        "year": 2013,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " However, image boundaries reside in multiple image resolutions (Witkin, 1983) and it has repeatedly been shown that fusing information from multiple resolutions improves boundary detection, e.g. in Doll\u00e1r & Zitnick (2015); Arbelaez et al. The generation of regions of interest has been established as a standard pre-processing step for object detection; one big push to the broad adoption of such methods has been the successful incorporation of Selective Search (Uijlings et al., 2013) into the RCNN work of Girshick et al.",
        "context": null
    },
    {
        "title": "Scale-space filtering",
        "author": [
            "A.P. Witkin"
        ],
        "venue": "In Proc. Int. Joint Conf. on Artificial Intel., pp. 1019\u20131022,",
        "citeRegEx": "Witkin,? \\Q1983\\E",
        "shortCiteRegEx": "Witkin",
        "year": 1983,
        "abstract": "",
        "full_text": "",
        "sentence": " However, image boundaries reside in multiple image resolutions (Witkin, 1983) and it has repeatedly been shown that fusing information from multiple resolutions improves boundary detection, e.",
        "context": null
    },
    {
        "title": "Holistically-nested edge detection",
        "author": [
            "Xie",
            "Saining",
            "Tu",
            "Zhuowen"
        ],
        "venue": "In Proc. ICCV,",
        "citeRegEx": "Xie et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "Xie et al\\.",
        "year": 2015,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Conditional random fields as recurrent neural networks",
        "author": [
            "Zheng",
            "Shuai",
            "Jayasumana",
            "Sadeep",
            "Romera-Paredes",
            "Bernardino",
            "Vineet",
            "Vibhav",
            "Su",
            "Zhizhong",
            "Du",
            "Dalong",
            "Huang",
            "Chang",
            "Torr",
            "Philip H. S"
        ],
        "venue": "In Proc. ICCV,",
        "citeRegEx": "Zheng et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "Zheng et al\\.",
        "year": 2015,
        "abstract": "Pixel-level labelling tasks, such as semantic segmentation, play a central\nrole in image understanding. Recent approaches have attempted to harness the\ncapabilities of deep learning techniques for image recognition to tackle\npixel-level labelling tasks. One central issue in this methodology is the\nlimited capacity of deep learning techniques to delineate visual objects. To\nsolve this problem, we introduce a new form of convolutional neural network\nthat combines the strengths of Convolutional Neural Networks (CNNs) and\nConditional Random Fields (CRFs)-based probabilistic graphical modelling. To\nthis end, we formulate mean-field approximate inference for the Conditional\nRandom Fields with Gaussian pairwise potentials as Recurrent Neural Networks.\nThis network, called CRF-RNN, is then plugged in as a part of a CNN to obtain a\ndeep network that has desirable properties of both CNNs and CRFs. Importantly,\nour system fully integrates CRF modelling with CNNs, making it possible to\ntrain the whole deep network end-to-end with the usual back-propagation\nalgorithm, avoiding offline post-processing methods for object delineation. We\napply the proposed method to the problem of semantic image segmentation,\nobtaining top results on the challenging Pascal VOC 2012 segmentation\nbenchmark.",
        "full_text": "Conditional Random Fields as Recurrent Neural Networks\nShuai Zheng\u22171, Sadeep Jayasumana*1, Bernardino Romera-Paredes1, Vibhav Vineet\u20201,2, Zhizhong Su3,\nDalong Du3, Chang Huang3, and Philip H. S. Torr1\n1University of Oxford\n2Stanford University\n3Baidu Institute of Deep Learning\nAbstract\nPixel-level labelling tasks, such as semantic segmenta-\ntion, play a central role in image understanding. Recent ap-\nproaches have attempted to harness the capabilities of deep\nlearning techniques for image recognition to tackle pixel-\nlevel labelling tasks. One central issue in this methodology\nis the limited capacity of deep learning techniques to de-\nlineate visual objects. To solve this problem, we introduce\na new form of convolutional neural network that combines\nthe strengths of Convolutional Neural Networks (CNNs)\nand Conditional Random Fields (CRFs)-based probabilistic\ngraphical modelling. To this end, we formulate mean-\ufb01eld\napproximate inference for the Conditional Random Fields\nwith Gaussian pairwise potentials as Recurrent Neural Net-\nworks. This network, called CRF-RNN, is then plugged in\nas a part of a CNN to obtain a deep network that has de-\nsirable properties of both CNNs and CRFs. Importantly,\nour system fully integrates CRF modelling with CNNs, mak-\ning it possible to train the whole deep network end-to-end\nwith the usual back-propagation algorithm, avoiding of\ufb02ine\npost-processing methods for object delineation.\nWe apply the proposed method to the problem of seman-\ntic image segmentation, obtaining top results on the chal-\nlenging Pascal VOC 2012 segmentation benchmark.\n1. Introduction\nLow-level computer vision problems such as semantic\nimage segmentation or depth estimation often involve as-\nsigning a label to each pixel in an image. While the feature\nrepresentation used to classify individual pixels plays an im-\nportant role in this task, it is similarly important to consider\nfactors such as image edges, appearance consistency and\nspatial consistency while assigning labels in order to obtain\naccurate and precise results.\nDesigning a strong feature representation is a key chal-\n\u2217Authors contributed equally.\n\u2020Work conducted while authors at the University of Oxford.\nlenge in pixel-level labelling problems. Work on this topic\nincludes: TextonBoost [52], TextonForest [51], and Ran-\ndom Forest-based classi\ufb01ers [50].\nRecently, supervised\ndeep learning approaches such as large-scale deep Convolu-\ntional Neural Networks (CNNs) have been immensely suc-\ncessful in many high-level computer vision tasks such as\nimage recognition [31] and object detection [20]. This mo-\ntivates exploring the use of CNNs for pixel-level labelling\nproblems. The key insight is to learn a strong feature rep-\nresentation end-to-end for the pixel-level labelling task in-\nstead of hand-crafting features with heuristic parameter tun-\ning. In fact, a number of recent approaches including the\nparticularly interesting works FCN [37] and DeepLab [10]\nhave shown a signi\ufb01cant accuracy boost by adapting state-\nof-the-art CNN based image classi\ufb01ers to the semantic seg-\nmentation problem.\nHowever, there are signi\ufb01cant challenges in adapting\nCNNs designed for high level computer vision tasks such as\nobject recognition to pixel-level labelling tasks. Firstly, tra-\nditional CNNs have convolutional \ufb01lters with large recep-\ntive \ufb01elds and hence produce coarse outputs when restruc-\ntured to produce pixel-level labels [37]. Presence of max-\npooling layers in CNNs further reduces the chance of get-\nting a \ufb01ne segmentation output [10]. This, for instance, can\nresult in non-sharp boundaries and blob-like shapes in se-\nmantic segmentation tasks. Secondly, CNNs lack smooth-\nness constraints that encourage label agreement between\nsimilar pixels, and spatial and appearance consistency of the\nlabelling output. Lack of such smoothness constraints can\nresult in poor object delineation and small spurious regions\nin the segmentation output [59, 58, 32, 39].\nOn a separate track to the progress of deep learning\ntechniques, probabilistic graphical models have been devel-\noped as effective methods to enhance the accuracy of pixel-\nlevel labelling tasks. In particular, Markov Random Fields\n(MRFs) and its variant Conditional Random Fields (CRFs)\nhave observed widespread success in this area [32, 29] and\nhave become one of the most successful graphical models\nused in computer vision. The key idea of CRF inference\nfor semantic labelling is to formulate the label assignment\n1\narXiv:1502.03240v3  [cs.CV]  13 Apr 2016\nproblem as a probabilistic inference problem that incor-\nporates assumptions such as the label agreement between\nsimilar pixels. CRF inference is able to re\ufb01ne weak and\ncoarse pixel-level label predictions to produce sharp bound-\naries and \ufb01ne-grained segmentations. Therefore, intuitively,\nCRFs can be used to overcome the drawbacks in utilizing\nCNNs for pixel-level labelling tasks.\nOne way to utilize CRFs to improve the semantic la-\nbelling results produced by a CNN is to apply CRF infer-\nence as a post-processing step disconnected from the train-\ning of the CNN [10]. Arguably, this does not fully harness\nthe strength of CRFs since it is not integrated with the deep\nnetwork. In this setup, the deep network is unaware of the\nCRF during the training phase.\nIn this paper, we propose an end-to-end deep learn-\ning solution for the pixel-level semantic image segmenta-\ntion problem. Our formulation combines the strengths of\nboth CNNs and CRF based graphical models in one uni-\n\ufb01ed framework. More speci\ufb01cally, we formulate mean-\ufb01eld\napproximate inference for the dense CRF with Gaussian\npairwise potentials as a Recurrent Neural Network (RNN)\nwhich can re\ufb01ne coarse outputs from a traditional CNN in\nthe forward pass, while passing error differentials back to\nthe CNN during training. Importantly, with our formula-\ntion, the whole deep network, which comprises a traditional\nCNN and an RNN for CRF inference, can be trained end-\nto-end utilizing the usual back-propagation algorithm.\nArguably, when properly trained, the proposed network\nshould outperform a system where CRF inference is applied\nas a post-processing method on independent pixel-level pre-\ndictions produced by a pre-trained CNN. Our experimental\nevaluation con\ufb01rms that this indeed is the case. We evalu-\nate the performance of our network on the popular Pascal\nVOC 2012 benchmark, achieving a new state-of-the-art ac-\ncuracy of 74.7%. Our source code and models are publicly\navailable 1.\n2. Related Work\nIn this section we review approaches that make use of\ndeep learning and CNNs for low-level computer vision\ntasks, with a focus on semantic image segmentation. A wide\nvariety of approaches have been proposed to tackle the se-\nmantic image segmentation task using deep learning. These\napproaches can be categorized into two main strategies.\nThe \ufb01rst strategy is based on utilizing separate mecha-\nnisms for feature extraction, and image segmentation ex-\nploiting the edges of the image [2, 38]. One representative\ninstance of this scheme is the application of a CNN for the\nextraction of meaningful features, and using superpixels to\naccount for the structural pattern of the image. Two repre-\nsentative examples are [19, 38], where the authors \ufb01rst ob-\n1https://github.com/torrvision/crfasrnn\ntained superpixels from the image and then used a feature\nextraction process on each of them. The main disadvantage\nof this strategy is that errors in the initial proposals (e.g:\nsuper-pixels) may lead to poor predictions, no matter how\ngood the feature extraction process is. Pinheiro and Col-\nlobert [46] employed an RNN to model the spatial depen-\ndencies during scene parsing. In contrast to their approach,\nwe show that a typical graphical model such as a CRF can\nbe formulated as an RNN to form a part of a deep network,\nto perform end-to-end training combined with a CNN.\nThe second strategy is to directly learn a nonlinear model\nfrom the images to the label map. This, for example, was\nshown in [17], where the authors replaced the last fully con-\nnected layers of a CNN by convolutional layers to keep spa-\ntial information. An important contribution in this direction\nis [37], where Long et al. used the concept of fully con-\nvolutional networks, and the notion that top layers obtain\nmeaningful features for object recognition whereas low lay-\ners keep information about the structure of the image, such\nas edges. In their work, connections from early layers to\nlater layers were used to combine these cues. Bell et al. [5]\nand Chen et al. [10, 41] used a CRF to re\ufb01ne segmentation\nresults obtained from a CNN. Bell et al. focused on material\nrecognition and segmentation, whereas Chen et al. reported\nvery signi\ufb01cant improvements on semantic image segmen-\ntation. In contrast to these works, which employed CRF\ninference as a standalone post-processing step disconnected\nfrom the CNN training, our approach is an end-to-end train-\nable network that jointly learns the parameters of the CNN\nand the CRF in one uni\ufb01ed deep network.\nWorks that use neural networks to predict structured out-\nput are found in different domains. For example, Do et\nal. [14] proposed an approach to combine deep neural net-\nworks and Markov networks for sequence labeling tasks.\nJain et al. [26] has shown Convolutional Neural Networks\ncan perform well like MRFs/CRFs approaches in image\nrestoration application.\nAnother domain which bene\ufb01ts\nfrom the combination of CNNs and structured loss is hand-\nwriting recognition. In natural language processing, Yao\net al. [60] shows that the performance of an RNN-based\nwords tagger can be signi\ufb01cantly improved by incorporat-\ning elements of the CRF model. In [6], the authors com-\nbined a CNN with Hidden Markov Models for that purpose,\nwhereas more recently, Peng et al. [45] used a modi\ufb01ed ver-\nsion of CRFs. Related to this line of works, in [25] a joint\nCNN and CRF model was used for text recognition on nat-\nural images. Tompson et al. [57] showed the use of joint\ntraining of a CNN and an MRF for human pose estimation,\nwhile Chen et al. [11] focused on the image classi\ufb01cation\nproblem with a similar approach. Another prominent work\nis [21], in which the authors express deformable part mod-\nels, a kind of MRF, as a layer in a neural network. In our\napproach, we cast a different graphical model as a neural\n2\nnetwork layer.\nA number of approaches have been proposed for au-\ntomatic learning of graphical model parameters and joint\ntraining of classi\ufb01ers and graphical models. Barbu et al. [4]\nproposed a joint training of a MRF/CRF model together\nwith an inference algorithm in their Active Random Field\napproach. Domke [15] advocated back-propagation based\nparameter optimization in graphical models when approxi-\nmate inference methods such as mean-\ufb01eld and belief prop-\nagation are used. This idea was utilized in [28], where a bi-\nnary dense CRF was used for human pose estimation. Sim-\nilarly, Ross et al. [47] and Stoyanov et al. [54] showed how\nback-propagation through belief propagation can be used to\noptimize model parameters. Ross et al. [21], in particular\nproposes an approach based on learning messages. Many\nof these ideas can be traced back to [55], which proposes\nunrolling message passing algorithms as simpler operations\nthat could be performed within a CNN. In a different setup,\nKr\u00a8ahenb\u00a8uhl and Koltun [30] demonstrated automatic pa-\nrameter tuning of dense CRF when a modi\ufb01ed mean-\ufb01eld\nalgorithm is used for inference. An alternative inference ap-\nproach for dense CRF, not based on mean-\ufb01eld, is proposed\nin [61].\nIn contrast to the works described above, our approach\nshows that it is possible to formulate dense CRF as an RNN\nso that one can form an end-to-end trainable system for se-\nmantic image segmentation which combines the strengths\nof deep learning and graphical modelling.\nAfter our initial publication of the technical report of this\nwork on arXiv.org, a number of independent works [49, 35]\nappeared on arXiv.org presenting similar joint training ap-\nproaches for semantic image segmentation.\n3. Conditional Random Fields\nIn this section we provide a brief overview of Condi-\ntional Random Fields (CRF) for pixel-wise labelling and\nintroduce the notation used in the paper. A CRF, used in\nthe context of pixel-wise label prediction, models pixel la-\nbels as random variables that form a Markov Random Field\n(MRF) when conditioned upon a global observation. The\nglobal observation is usually taken to be the image.\nLet Xi be the random variable associated to pixel i,\nwhich represents the label assigned to the pixel i and\ncan take any value from a pre-de\ufb01ned set of labels L =\n{l1, l2, . . . , lL}. Let X be the vector formed by the ran-\ndom variables X1, X2, . . . , XN, where N is the number of\npixels in the image. Given a graph G = (V, E), where\nV\n= {X1, X2, . . . , XN}, and a global observation (im-\nage) I, the pair (I, X) can be modelled as a CRF charac-\nterized by a Gibbs distribution of the form P(X = x|I) =\n1\nZ(I) exp(\u2212E(x|I)).\nHere E(x) is called the energy of\nthe con\ufb01guration x \u2208LN and Z(I) is the partition func-\nFigure 1. A mean-\ufb01eld iteration as a CNN. A single iteration of\nthe mean-\ufb01eld algorithm can be modelled as a stack of common\nCNN layers.\ntion [33]. From now on, we drop the conditioning on I in\nthe notation for convenience.\nIn the fully connected pairwise CRF model of [29], the\nenergy of a label assignment x is given by:\nE(x) =\nX\ni\n\u03c8u(xi) +\nX\ni<j\n\u03c8p(xi, xj),\n(1)\nwhere the unary energy components \u03c8u(xi) measure the\ninverse likelihood (and therefore, the cost) of the pixel\ni taking the label xi, and pairwise energy components\n\u03c8p(xi, xj) measure the cost of assigning labels xi, xj to\npixels i, j simultaneously. In our model, unary energies are\nobtained from a CNN, which, roughly speaking, predicts la-\nbels for pixels without considering the smoothness and the\nconsistency of the label assignments. The pairwise ener-\ngies provide an image data-dependent smoothing term that\nencourages assigning similar labels to pixels with similar\nproperties. As was done in [29], we model pairwise poten-\ntials as weighted Gaussians:\n\u03c8p(xi, xj) = \u00b5(xi, xj)\nM\nX\nm=1\nw(m)k(m)\nG (fi, fj),\n(2)\nwhere each k(m)\nG\nfor m = 1, . . . , M, is a Gaussian kernel\napplied on feature vectors. The feature vector of pixel i,\ndenoted by fi, is derived from image features such as spatial\nlocation and RGB values [29]. We use the same features as\nin[29]. The function \u00b5(., .), called the label compatibility\nfunction, captures the compatibility between different pairs\nof labels as the name implies.\nMinimizing the above CRF energy E(x) yields the most\nprobable label assignment x for the given image. Since this\nexact minimization is intractable, a mean-\ufb01eld approxima-\ntion to the CRF distribution is used for approximate max-\nimum posterior marginal inference. It consists in approxi-\nmating the CRF distribution P(X) by a simpler distribution\nQ(X), which can be written as the product of independent\nmarginal distributions, i.e., Q(X) = Q\ni Qi(Xi). The steps\nof the iterative algorithm for approximate mean-\ufb01eld infer-\nence and its reformulation as an RNN are discussed next.\n3\nAlgorithm 1 Mean-\ufb01eld in dense CRFs [29], broken down\nto common CNN operations.\nQi(l) \u2190\n1\nZi exp (Ui(l)) for all i\n\u25b7Initialization\nwhile not converged do\n\u02dcQ(m)\ni\n(l) \u2190P\nj\u0338=i k(m)(fi, fj)Qj(l) for all m\n\u25b7Message Passing\n\u02c7Qi(l) \u2190P\nm w(m) \u02dcQ(m)\ni\n(l)\n\u25b7Weighting Filter Outputs\n\u02c6Qi(l) \u2190P\nl\u2032\u2208L \u00b5(l, l\u2032) \u02c7Qi(l\u2032)\n\u25b7Compatibility Transform\n\u02d8Qi(l) \u2190Ui(l) \u2212\u02c6Qi(l)\n\u25b7Adding Unary Potentials\nQi \u2190\n1\nZi exp\n\u0010\n\u02d8Qi(l)\n\u0011\n\u25b7Normalizing\nend while\n4. A Mean-\ufb01eld Iteration as a Stack of CNN\nLayers\nA key contribution of this paper is to show that the mean-\n\ufb01eld CRF inference can be reformulated as a Recurrent\nNeural Network (RNN). To this end, we \ufb01rst consider in-\ndividual steps of the mean-\ufb01eld algorithm summarized in\nAlgorithm 1 [29], and describe them as CNN layers. Our\ncontribution is based on the observation that \ufb01lter-based ap-\nproximate mean-\ufb01eld inference approach for dense CRFs\nrelies on applying Gaussian spatial and bilateral \ufb01lters on\nthe mean-\ufb01eld approximates in each iteration. Unlike the\nstandard convolutional layer in a CNN, in which \ufb01lters are\n\ufb01xed after the training stage, we use edge-preserving Gaus-\nsian \ufb01lters [56, 42], coef\ufb01cients of which depend on the\noriginal spatial and appearance information of the image.\nThese \ufb01lters have the additional advantages of requiring a\nsmaller set of parameters, despite the \ufb01lter size being po-\ntentially as big as the image.\nWhile reformulating the steps of the inference algorithm\nas CNN layers, it is essential to be able to calculate error\ndifferentials in each layer w.r.t. its inputs in order to be able\nto back-propagate the error differentials to previous layers\nduring training. We also discuss how to calculate error dif-\nferentials with respect to the parameters in each layer, en-\nabling their optimization through the back-propagation al-\ngorithm. Therefore, in our formulation, CRF parameters\nsuch as the weights of the Gaussian kernels and the label\ncompatibility function can also be optimized automatically\nduring the training of the full network.\nOnce the individual steps of the algorithm are broken\ndown as CNN layers, the full algorithm can then be for-\nmulated as an RNN. We explain this in Section 5 after dis-\ncussing the steps of Algorithm 1 in detail below. In Algo-\nrithm 1 and the remainder of this paper, we use Ui(l) to\ndenote the negative of the unary energy introduced in the\nprevious section, i.e., Ui(l) = \u2212\u03c8u(Xi = l). In the con-\nventional CRF setting, this input Ui(l) to the mean-\ufb01eld al-\ngorithm is obtained from an independent classi\ufb01er.\n4.1. Initialization\nIn the initialization step of the algorithm, the operation\nQi(l) \u2190\n1\nZi exp (Ui(l)), where Zi = P\nl exp(Ui(l)), is\nperformed. Note that this is equivalent to applying a soft-\nmax function over the unary potentials U across all the la-\nbels at each pixel. The softmax function has been exten-\nsively used in CNN architectures before and is therefore\nwell known in the deep learning community. This operation\ndoes not include any parameters and the error differentials\nreceived at the output of the step during back-propagation\ncould be passed down to the unary potential inputs after per-\nforming usual backward pass calculations of the softmax\ntransformation.\n4.2. Message Passing\nIn the dense CRF formulation, message passing is imple-\nmented by applying M Gaussian \ufb01lters on Q values. Gaus-\nsian \ufb01lter coef\ufb01cients are derived based on image features\nsuch as the pixel locations and RGB values, which re\ufb02ect\nhow strongly a pixel is related to other pixels. Since the\nCRF is potentially fully connected, each \ufb01lter\u2019s receptive\n\ufb01eld spans the whole image, making it infeasible to use a\nbrute-force implementation of the \ufb01lters. Fortunately, sev-\neral approximation techniques exist to make computation\nof high dimensional Gaussian \ufb01ltering signi\ufb01cantly faster.\nFollowing [29], we use the Permutohedral lattice imple-\nmentation [1], which can compute the \ufb01lter response in\nO(N) time, where N is the number of pixels of the im-\nage [1].\nDuring back-propagation, error derivatives w.r.t. the \ufb01l-\nter inputs are calculated by sending the error derivatives\nw.r.t. the \ufb01lter outputs through the same M Gaussian \ufb01l-\nters in reverse direction. In terms of permutohedral lattice\noperations, this can be accomplished by only reversing the\norder of the separable \ufb01lters in the blur stage, while building\nthe permutohedral lattice, splatting, and slicing in the same\nway as in the forward pass. Therefore, back-propagation\nthrough this \ufb01ltering stage can also be performed in O(N)\ntime. Following [29], we use two Gaussian kernels, a spa-\ntial kernel and a bilateral kernel. In this work, for simplic-\nity, we keep the bandwidth values of the \ufb01lters \ufb01xed. It\nis also possible to use multiple spatial and bilateral kernels\nwith different bandwidth values and learn their optimal lin-\near combination.\n4.3. Weighting Filter Outputs\nThe next step of the mean-\ufb01eld iteration is taking a\nweighted sum of the M \ufb01lter outputs from the previous step,\nfor each class label l. When each class label is considered\nindividually, this can be viewed as usual convolution with\n4\na 1 \u00d7 1 \ufb01lter with M input channels, and one output chan-\nnel. Since both inputs and the outputs to this step are known\nduring back-propagation, the error derivative w.r.t. the \ufb01lter\nweights can be computed, making it possible to automat-\nically learn the \ufb01lter weights (relative contributions from\neach Gaussian \ufb01lter output from the previous stage). Er-\nror derivative w.r.t. the inputs can also be computed in the\nusual manner to pass the error derivatives down to the previ-\nous stage. To obtain a higher number of tunable parameters,\nin contrast to [29], we use independent kernel weights for\neach class label. The intuition is that the relative impor-\ntance of the spatial kernel vs the bilateral kernel depends on\nthe visual class. For example, bilateral kernels may have\non the one hand a high importance in bicycle detection, be-\ncause similarity of colours is determinant; on the other hand\nthey may have low importance for TV detection, given that\nwhatever is inside the TV screen may have many different\ncolours.\n4.4. Compatibility Transform\nIn the compatibility transform step, outputs from the pre-\nvious step (denoted by \u02c7Q in Algorithm 1) are shared be-\ntween the labels to a varied extent, depending on the com-\npatibility between these labels. Compatibility between the\ntwo labels l and l\u2032 is parameterized by the label compatibil-\nity function \u00b5(l, l\u2032). The Potts model, given by \u00b5(l, l\u2032) =\n[l \u0338= l\u2032], where [.] is the Iverson bracket, assigns a \ufb01xed\npenalty if different labels are assigned to pixels with simi-\nlar properties. A limitation of this model is that it assigns\nthe same penalty for all different pairs of labels. Intuitively,\nbetter results can be obtained by taking the compatibility\nbetween different label pairs into account and penalizing\nthe assignments accordingly. For example, assigning labels\n\u201cperson\u201d and \u201cbicycle\u201d to nearby pixels should have a lesser\npenalty than assigning labels \u201csky\u201d and \u201cbicycle\u201d. There-\nfore, learning the function \u00b5 from data is preferred to \ufb01xing\nit in advance with Potts model. We also relax our compat-\nibility transform model by assuming that \u00b5(l, l\u2032) \u0338= \u00b5(l\u2032, l)\nin general.\nCompatibility transform step can be viewed as another\nconvolution layer where the spatial receptive \ufb01eld of the \ufb01l-\nter is 1 \u00d7 1, and the number of input and output channels\nare both L. Learning the weights of this \ufb01lter is equivalent\nto learning the label compatibility function \u00b5. Transferring\nerror differentials from the output of this step to the input\ncan be done since this step is a usual convolution operation.\n4.5. Adding Unary Potentials\nIn this step, the output from the compatibility transform\nstage is subtracted element-wise from the unary inputs U.\nWhile no parameters are involved in this step, transferring\nerror differentials can be done trivially by copying the dif-\nferentials at the output of this step to both inputs with the\nappropriate sign.\n4.6. Normalization\nFinally, the normalization step of the iteration can be\nconsidered as another softmax operation with no parame-\nters. Differentials at the output of this step can be passed on\nto the input using the softmax operation\u2019s backward pass.\n5. The End-to-end Trainable Network\nWe now describe our end-to-end deep learning system\nfor semantic image segmentation. To pave the way for this,\nwe \ufb01rst explain how repeated mean-\ufb01eld iterations can be\norganized as an RNN.\n5.1. CRF as RNN\nIn the previous section, it was shown that one iteration\nof the mean-\ufb01eld algorithm can be formulated as a stack of\ncommon CNN layers (see Fig. 1). We use the function f\u03b8\nto denote the transformation done by one mean-\ufb01eld iter-\nation: given an image I, pixel-wise unary potential values\nU and an estimation of marginal probabilities Qin from the\nprevious iteration, the next estimation of marginal distribu-\ntions after one mean-\ufb01eld iteration is given by f\u03b8(U, Qin, I).\nThe vector \u03b8 = {w(m), \u00b5(l, l\n\u2032)}, m \u2208{1, ..., M}, l, l\n\u2032 \u2208\n{l1, ..., lL} represents the CRF parameters described in Sec-\ntion 4.\nMultiple mean-\ufb01eld iterations can be implemented by re-\npeating the above stack of layers in such a way that each\niteration takes Q value estimates from the previous iteration\nand the unary values in their original form. This is equiv-\nalent to treating the iterative mean-\ufb01eld inference as a Re-\ncurrent Neural Network (RNN) as shown in Fig. 2. Using\nthe notation in the \ufb01gure, the behaviour of the network is\ngiven by the following equations where T is the number of\nmean-\ufb01eld iterations:\nH1(t) =\n(\nsoftmax(U),\nt = 0\nH2(t \u22121),\n0 < t \u2264T,\n(3)\nH2(t) = f\u03b8(U, H1(t), I),\n0 \u2264t \u2264T,\n(4)\nY (t) =\n(\n0,\n0 \u2264t < T\nH2(t),\nt = T.\n(5)\nWe name this RNN structure CRF-RNN. Parameters of\nthe CRF-RNN are the same as the mean-\ufb01eld parameters\ndescribed in Section 4 and denoted by \u03b8 here. Since the cal-\nculation of error differentials w.r.t. these parameters in a sin-\ngle iteration was described in Section 4, they can be learnt\nin the RNN setting using the standard back-propagation\nthrough time algorithm [48, 40]. It was shown in [29] that\nthe mean-\ufb01eld iterative algorithm for dense CRF converges\nin less than 10 iterations. Furthermore, in practice, after\n5\nMean\ufb01eld\nIteration\nH2 =\nf\u03b8(U, H1, I)\nI\nU\nSoftmax\nNormalization\nG1\nG2\nY\nH2\nH1\n1\nFigure 2. The CRF-RNN Network. We formulate the iterative\nmean-\ufb01eld algorithm as a Recurrent Neural Network (RNN). Gat-\ning functions G1 and G2 are \ufb01xed as described in the text.\nFCN\nCRF-RNN\nFigure 3. The End-to-end Trainable Network. Schematic vi-\nsualization of our full network which consists of a CNN and the\nCNN-CRF network. Best viewed in colour.\nabout 5 iterations, increasing the number of iterations usu-\nally does not signi\ufb01cantly improve results [29]. Therefore,\nit does not suffer from the vanishing and exploding gradient\nproblem inherent to deep RNNs [7, 43]. This allows us to\nuse a plain RNN architecture instead of more sophisticated\narchitectures such as LSTMs in our network.\n5.2. Completing the Picture\nOur approach comprises a fully convolutional network\nstage, which predicts pixel-level labels without consid-\nering structure, followed by a CRF-RNN stage, which\nperforms CRF-based probabilistic graphical modelling for\nstructured prediction. The complete system, therefore, uni-\n\ufb01es strengths of both CNNs and CRFs and is trainable\nend-to-end using the back-propagation algorithm [34] and\nthe Stochastic Gradient Descent (SGD) procedure. During\ntraining, a whole image (or many of them) can be used as\nthe mini-batch and the error at each pixel output of the net-\nwork can be computed using an appropriate loss function\nsuch as the softmax loss with respect to the ground truth\nsegmentation of the image. We used the FCN-8s architec-\nture of [37] as the \ufb01rst part of our network, which provides\nunary potentials to the CRF. This network is based on the\nVGG-16 network [53] but has been restructured to perform\npixel-wise prediction instead of image classi\ufb01cation. The\ncomplete architecture of our network, including the FCN-\n8s part can be found in the appendix.\nIn the forward pass through the network, once the com-\nputation enters the CRF-RNN after passing through the\nCNN stage, it takes T iterations for the data to leave the\nloop created by the RNN. Neither the CNN that provides\nunary values nor the layers after the CRF-RNN (i.e., the\nloss layers) need to perform any computations during this\ntime since the re\ufb01nement happens only inside the RNN\u2019s\nloop. Once the output Y leaves the loop, next stages of the\ndeep network after the CRF-RNN can continue the forward\npass. In our setup, a softmax loss layer directly follows the\nCRF-RNN and terminates the network.\nDuring the backward pass, once the error differentials\nreach the CRF-RNN\u2019s output Y , they similarly spend T it-\nerations within the loop before reaching the RNN input U\nin order to propagate to the CNN which provides the unary\ninput. In each iteration inside the loop, error differentials\nare computed inside each component of the mean-\ufb01eld it-\neration as described in Section 4. We note that unnecessar-\nily increasing the number of mean-\ufb01eld iterations T could\npotentially result in the vanishing and exploding gradient\nproblems in the CRF-RNN. We, however, did not experi-\nence this problem during our experiments.\n6. Implementation Details\nIn the present section we describe the implementation\ndetails of the proposed network, as well as its training pro-\ncess. The high-level architecture of our system, which was\nimplemented using the popular Caffe [27] deep learning li-\nbrary, is shown in Fig. 3. Complete architecture of the deep\nnetwork can be found in the appendix. The full source code\nand the trained models of our approach will be made pub-\nlicly available.\nWe initialized the \ufb01rst part of the network using the pub-\nlicly available weights of the FCN-8s network [37]. The\ncompatibility transform parameters of the CRF-RNN were\ninitialized using the Potts model, and kernel width and\nweight parameters were obtained from a cross-validation\nprocess. We found that such initialization results in faster\nconvergence of training. During the training phase, param-\neters of the whole network were optimized end-to-end using\nthe back-propagation algorithm. In particular we used full\nimage training described in [37], with learning rate \ufb01xed at\n10\u221213 and momentum set to 0.99. These extreme values of\nthe parameters were used since we employed only one im-\nage per batch to avoid reaching memory limits of the GPU.\nIn all our experiments, during training, we set the num-\nber of mean-\ufb01eld iterations T in the CRF-RNN to 5 to avoid\n6\nvanishing/exploding gradient problems and to reduce the\ntraining time. During the test time, iteration count was in-\ncreased to 10. The effect of this parameter value on the\naccuracy is discussed in section 7.1.\nLoss function During the training of the models that\nachieved the best results reported in this paper, we used the\nstandard softmax loss function, that is, the log-likelihood\nerror function described in [30]. The standard metric used\nin the Pascal VOC challenge is the average intersection over\nunion (IU), which we also use here to report the results. In\nour experiments we found that high values of IU on the val-\nidation set were associated to low values of the averaged\nsoftmax loss, to a large extent. We also tried the robust log-\nlikelihood in [30] as a loss function for CRF-RNN training.\nHowever, this did not result in increased accuracy nor faster\nconvergence.\nNormalization techniques As described in Section 4,\nwe use the exponential function followed by pixel-wise nor-\nmalization across channels in several stages of the CRF-\nRNN. Since this operation has a tendency to result in small\ngradients with respect to the input when the input value is\nlarge, we conducted several experiments where we replaced\nthis by a recti\ufb01er linear unit (ReLU) operation followed by\na normalization across the channels. Our hypothesis was\nthat this approach may approximate the original operation\nadequately while speeding up the training due to improved\ngradients. Furthermore, ReLU would induce sparsity on the\nprobability of labels assigned to pixels, implicitly pruning\nlow likelihood con\ufb01gurations, which could have a positive\neffect. However, this approach did not lead to better re-\nsults, obtaining 1% IU lower than the original setting per-\nformance.\n7. Experiments\nWe present experimental results with the proposed CRF-\nRNN framework. We use these datasets: the Pascal VOC\n2012 dataset, and the Pascal Context dataset. We use the\nPascal VOC 2012 dataset as it has become the golden stan-\ndard to comprehensively evaluate any new semantic seg-\nmentation approach in comparison to existing methods. We\nalso use the Pascal Context dataset to assess how well our\napproach performs on a dataset with different characteris-\ntics.\nPascal VOC Datasets\nIn order to evaluate our approach with existing methods un-\nder the same circumstances, we conducted two main exper-\niments with the Pascal VOC 2012 dataset, followed by a\nqualitative experiment.\nIn the \ufb01rst experiment, following [37, 38, 41], we used\na training set consisted of VOC 2012 training data (1464\nimages), and training and validation data of [23], which\nInput Image\nGround Truth\nCRF-RNN\nDeepLab\nFCN-8s\nB-ground Aero plane\nBicycle\nBird\nBoat\nBottle\nBus\nCar\nCat\nChair\nCow\nDining-Table\nDog\nHorse\nMotorbike\nPerson\nPotted-Plant\nSheep\nSofa\nTrain\nTV/Monitor\nFigure 4. Qualitative results on the validation set of Pascal\nVOC 2012. FCN [37] is a CNN-based model that does not em-\nploy CRF. Deeplab [10] is a two-stage approach, where the CNN\nis trained \ufb01rst, and then CRF is applied on top of the CNN output.\nOur approach is an end-to-end trained system that integrates both\nCNN and CRF-RNN in one deep network. Best viewed in colour.\namounts to a total of 11,685 images. After removing the\noverlapping images between VOC 2012 validation data and\nthis training dataset, we were left with 346 images from the\noriginal VOC 2012 validation set to validate our models on.\nWe call this set the reduced validation set in the sequel. An-\nnotations of the VOC 2012 test set, which consists of 1456\nimages, are not publicly available and hence the \ufb01nal results\non the test set were obtained by submitting the results to the\nPascal VOC challenge evaluation server [18]. Regardless\nof the smaller number of images, we found that the relative\nimprovements of the accuracy on our validation set were in\ngood agreement with the test set.\nAs a \ufb01rst step we directly compared the potential advan-\ntage of learning the model end-to-end with respect to alter-\nnative learning strategies. These are plain FCN-8s without\napplying CRF, and with CRF as a postprocessing method\ndisconnected from the training of FCN, which is compara-\nble to the approach described in [10] and [41]. The results\nare reported in Table 1 and show a clear advantage of the\nend-to-end strategy over the of\ufb02ine application of CRF as a\npost-processing method. This can be attributed to the fact\n7\nthat during the SGD training of the CRF-RNN, the CNN\ncomponent and the CRF component learn how to co-operate\nwith each other to produce the optimum output of the whole\nnetwork.\nWe then proceeded to compare our approach with all\nstate-of-the-art methods that used training data from the\nstandard VOC 2012 training and validation sets, and from\nthe dataset published with [22]. The results are shown in\nTable 2, above the bar, and we can see that our approach\noutperforms all competitors.\nIn the second experiment, in addition to the above train-\ning set, we used data from the Microsoft COCO dataset [36]\nas was done in [41] and [12]. We selected images from\nMS COCO 2014 training set where the ground truth seg-\nmentation has at least 200 pixels marked with classes la-\nbels present in the VOC 2012 dataset. With this selection,\nwe ended up using 66,099 images from the COCO dataset\nand therefore a total of 66,099 + 11,685 = 77,784 training\nimages were used in the second experiment. The same re-\nduced validation set was used in this second experiment as\nwell. In this case, we \ufb01rst \ufb01ne-tuned the plain FCN-32s\nnetwork (without the CRF-RNN part) on COCO data, then\nwe built an FCN-8s network with the learnt weights and \ufb01-\nnally train the CRF-RNN network end-to-end using VOC\n2012 training data only. Since the MS COCO ground truth\nsegmentation data contains somewhat coarse segmentation\nmasks where objects are not delineated properly, we found\nthat \ufb01ne-tuning our model with COCO did not yield signif-\nicant improvements. This can be understood because the\nprimary advantage of our model comes from delineating\nthe objects and improving \ufb01ne segmentation boundaries.\nThe VOC 2012 training dataset therefore helps our model\nlearn this task effectively. The results of this experiment are\nshown in Table 2, below the bar, and we see that our ap-\nproach sets a new state-of-the-art on the VOC 2012 dataset.\nNote that in both setups, our approach outperforms com-\npeting methods due to the end-to-end training of the CNN\nand CRF in the uni\ufb01ed CRF-RNN framework.\nWe also\nevaluated our models on the VOC 2010, and VOC 2011 test\nset (see Table 2). In all cases our method achieves the state-\nof-the-art performance.\nIn order to have a qualitative evidence about how CRF-\nRNN learns, we visualize the compatibility function learned\nafter the training stage of the CRF-RNN as a matrix repre-\nsentation in Fig. 5. Element (i, j) of this matrix corresponds\nto \u00b5(i, j) de\ufb01ned earlier: a high value at (i, j) implies high\npenalty for assigning label i to a pixel when a similar pixel\n(spatially or appearance wise) is assigned label j. For exam-\nple we can appreciate that the learned compatibility matrix\nassigns a low penalty to pairs of labels that tend to appear\ntogether, such as [Motorbike, Person], and [Dining table,\nChair].\nMethod\nWithout COCO\nWith COCO\nPlain FCN-8s\n61.3\n68.3\nFCN-8s and CRF\ndisconnected\n63.7\n69.5\nEnd-to-end training of\nCRF-RNN\n69.6\n72.9\nTable 1. Mean IU accuracy of our approach, CRF-RNN, compared\nwith similar methods, evaluated on the reduced VOC 2012 valida-\ntion set.\nMethod\nVOC 2010\ntest\nVOC 2011\ntest\nVOC 2012\ntest\nBerkeleyRC [3]\nn/a\n39.1\nn/a\nO2PCPMC [8]\n49.6\n48.8\n47.8\nDivmbest [44]\nn/a\nn/a\n48.1\nNUS-UDS [16]\nn/a\nn/a\n50.0\nSDS [23]\nn/a\nn/a\n51.6\nMSRA-\nCFM [13]\nn/a\nn/a\n61.8\nFCN-8s [37]\nn/a\n62.7\n62.2\nHypercolumn [24]\nn/a\nn/a\n62.6\nZoomout [38]\n64.4\n64.1\n64.4\nContext-Deep-\nCNN-CRF [35]\nn/a\nn/a\n70.7\nDeepLab-\nMSc [10]\nn/a\nn/a\n71.6\nOur method\nw/o COCO\n73.6\n72.4\n72.0\nBoxSup [12]\nn/a\nn/a\n71.0\nDeepLab [10,\n41]\nn/a\nn/a\n72.7\nOur method\nwith COCO\n75.7\n75.0\n74.7\nTable 2. Mean IU accuracy of our approach, CRF-RNN, com-\npared to the other approaches on the Pascal VOC 2010-2012 test\ndatasets. Methods from the \ufb01rst group do not use MS COCO data\nfor training. The methods from the second group use both COCO\nand VOC datasets for training.\nPascal Context Dataset\nWe conducted an experiment on the Pascal Context dataset\n[39], which differs from the previous one in the larger num-\nber of classes considered, 59. We used the provided parti-\ntions of training and validation sets, and the obtained results\nare reported in Table 3.\nMethod\nO2P [8]\nCFM [13]\nFCN-\n8s [37]\nCRF-\nRNN\nMean IU\n18.1\n34.4\n37.78\n39.28\nTable 3. Mean IU accuracy of our approach, CRF-RNN, evaluated\non the Pascal Context validation set.\n8\n \n \n \n-0.9\n-0.8\n-0.7\n-0.6\n-0.5\n-0.4\n-0.3\n-0.2\n-0.1\n00.0\n-1.0\n-0.5\nB-Ground\nAeroplane\nBicycle\nBird\nBoat\nBus\nCar\nCat\nChair\nCow\nDining-Table\nDog\nHorse\nMotorbike\nPerson\nPottlePlant\nSheep\nSofa\nTrain\nTV/Monitor\nBottle\nB-Ground\nAeroplane\nBicycle\nBird\nBoat\nBus\nCar\nCat\nChair\nCow\nDining-Table\nDog\nHorse\nMotorbike\nPerson\nPottlePlant\nSheep\nSofa\nTrain\nTV/Monitor\nBottle\nFigure 5. Visualization of the learnt label compatibility ma-\ntrix. In the standard Potts model, diagonal entries are equal to \u22121,\nwhile off-diagonal entries are zero. These values have changed af-\nter the end-to-end training of our network. Best viewed in colour.\n7.1. Effect of Design Choices\nWe performed a number of additional experiments on the\nPascal VOC 2012 validation set described above to study\nthe effect of some design choices we made.\nWe \ufb01rst studied the performance gains attained by our\nmodi\ufb01cations to the CRF over the CRF approach proposed\nby [29]. We found that using different \ufb01lter weights for dif-\nferent classes improved the performance by 1.8 percentage\npoints, and that introducing the asymmetric compatibility\ntransform further boosted the performance by 0.9 percent-\nage points.\nRegarding the RNN parameter iteration count T, incre-\nmenting it to T = 10 during the test time, from T = 5\nduring the train time, produced an accuracy improvement\nof 0.2 percentage points. Setting T = 10 also during train-\ning reduced the accuracy by 0.7 percentage points. We be-\nlieve that this might be due to a vanishing gradient effect\ncaused by using too many iterations. In practice that leads\nto the \ufb01rst part of the network (the one producing unary po-\ntentials) receiving a very weak error gradient signal during\ntraining, thus hampering its learning capacity.\nEnd-to-end training after the initialization of CRF pa-\nrameters improved performance by 3.4 percentage points.\nWe also conducted an experiment where we froze the FCN-\n8s part and \ufb01ne-tuned only the RNN part (i.e., CRF param-\neters). It improved the performance over initialization by\nonly 1 percentage point. We therefore conclude that end-to-\nend training signi\ufb01cantly contributed to boost the accuracy\nof the system.\nTreating each iteration of mean-\ufb01eld inference as an in-\ndependent step with its own parameters, and training end-\nto-end with 5 such iterations yielded a \ufb01nal mean IU score\nof only 70.9, supporting the hypothesis that the recurrent\nstructure of our approach is important for its success.\n8. Conclusion\nWe presented CRF-RNN, an interpretation of dense\nCRFs as Recurrent Neural Networks.\nOur formulation\nfully integrates CRF-based probabilistic graphical mod-\nelling with emerging deep learning techniques. In partic-\nular, the proposed CRF-RNN can be plugged in as a part\nof a traditional deep neural network: It is capable of pass-\ning on error differentials from its outputs to inputs dur-\ning back-propagation based training of the deep network\nwhile learning CRF parameters. We demonstrate the use\nof this approach by utilizing it for the semantic segmenta-\ntion task: we form an end-to-end trainable deep network\nby combining a fully convolutional neural network with the\nCRF-RNN. Our system achieves a new state-of-the-art on\nthe popular Pascal VOC segmentation benchmark. This im-\nprovement can be attributed to the uniting of the strengths\nof CNNs and CRFs in a single deep network.\nIn the future,\nwe plan to investigate the advan-\ntages/disadvantages of restricting the capabilities of the\nRNN part of our network to mean-\ufb01eld inference of dense\nCRF. A sensible baseline to the work presented here would\nbe to use more standard RNNs (e.g. LSTMs) that learn to\niteratively improve the input unary potentials to make them\ncloser to the ground-truth.\nAcknowledgement\nThis work was supported by grants\nLeverhulme Trust, EPSRC EP/I001107/2 and ERC 321162-\nHELIOS. We thank the Caffe team, Baidu IDL, and the Ox-\nford ARC team for their support. We gratefully acknowl-\nedge GPU donations from NVIDIA.\nReferences\n[1] A. Adams, J. Baek, and M. A. Davis.\nFast high-\ndimensional \ufb01ltering using the permutohedral lattice.\nComputer Graphics Forum, 29(2):753\u2013762, 2010.\n[2] P. Arbel\u00b4aez, B. Hariharan, C. Gu, S. Gupta, L. Bour-\ndev, and J. Malik. Semantic segmentation using re-\ngions and parts. In IEEE CVPR, 2012.\n[3] P. Arbel\u00b4aez, M. Maire, C. Fowlkes, and J. Malik. Con-\ntour detection and hierarchical image segmentation.\nIEEE TPAMI, 33(5):898\u2013916, 2011.\n[4] A. Barbu. Training an active random \ufb01eld for real-\ntime image denoising. IEEE TIP, 18(11):2451\u20132462,\n2009.\n[5] S. Bell, P. Upchurch, N. Snavely, and K. Bala. Mate-\nrial recognition in the wild with the materials in con-\ntext database. In IEEE CVPR, 2015.\n9\n[6] Y. Bengio, Y. LeCun, and D. Henderson.\nGlobally\ntrained handwritten word recognizer using spatial rep-\nresentation, convolutional neural networks, and hid-\nden markov models. In NIPS, pages 937\u2013937, 1994.\n[7] Y. Bengio, P. Simard, and P. Frasconi. Learning long-\nterm dependencies with gradient descent is dif\ufb01cult.\nIEEE Transactions on Neural Networks, 1994.\n[8] J. Carreira, R. Caseiro, J. Batista, and C. Sminchis-\nescu. Free-form region description with second-order\npooling. IEEE TPAMI, 2014.\n[9] K. Chat\ufb01eld, K. Simonyan, A. Vedaldi, and A. Zisser-\nman. Return of the devil in the details: Delving deep\ninto convolutional nets. In BMVC, 2014.\n[10] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy,\nand A. L. Yuille. Semantic image segmentation with\ndeep convolutional nets and fully connected crfs. In\nICLR, 2015.\n[11] L.-C. Chen, A. G. Schwing, A. L. Yuille, and R. Ur-\ntasun. Learning deep structured models. In ICLRW,\n2015.\n[12] J. Dai, K. He, and J. Sun. Boxsup: Exploiting bound-\ning boxes to supervise convolutional networks for se-\nmantic segmentation. In arXiv:1503.01640, 2015.\n[13] J. Dai, K. He, and J. Sun. Convolutional feature mask-\ning for joint object and stuff segmentation. In IEEE\nCVPR, 2015.\n[14] T.-M.-T. Do and T. Artieres. Neural conditional ran-\ndom \ufb01elds. In NIPS, 2010.\n[15] J. Domke.\nLearning graphical model parameters\nwith approximate marginal inference. IEEE TPAMI,\n35(10):2454\u20132467, 2013.\n[16] J. Dong, Q. Chen, S. Yan, and A. Yuille.\nTowards\nuni\ufb01ed object detection and semantic segmentation. In\nECCV, 2014.\n[17] D. Eigen, C. Puhrsch, and R. Fergus. Depth map pre-\ndiction from a single image using a multi-scale deep\nnetwork. In NIPS, 2014.\n[18] M. Everingham, S. M. A. Eslami, L. Van Gool, C. K. I.\nWilliams, J. Winn, and A. Zisserman. The pascal vi-\nsual object classes challenge: A retrospective. IJCV,\n111(1):98\u2013136, 2015.\n[19] C. Farabet, C. Couprie, L. Najman, and Y. Le-\nCun. Learning hierarchical features for scene labeling.\nIEEE TPAMI, 2013.\n[20] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich\nfeature hierarchies for accurate object detection and\nsemantic segmentation. In IEEE CVPR, 2014.\n[21] R. Girshick, F. Iandola, T. Darrell, and J. Malik. De-\nformable part models are convolutional neural net-\nworks. In CVPR, 2015.\n[22] B. Hariharan, P. Arbelaez, L. D. Bourdev, S. Maji, and\nJ. Malik. Semantic contours from inverse detectors. In\nIEEE ICCV, 2011.\n[23] B. Hariharan, P. Arbel\u00b4aez, R. Girshick, and J. Malik.\nSimultaneous detection and segmentation. In ECCV,\n2014.\n[24] B. Hariharan, P. Arbelaez, R. Girshick, and J. Ma-\nlik. Hypercolumns for object segmentation and \ufb01ne-\ngrained localization. In IEEE CVPR, 2015.\n[25] M. Jaderberg, K. Simonyan, A. Vedaldi, and A. Zis-\nserman. Deep structured output learning for uncon-\nstrained text recognition. In ICLR, 2015.\n[26] V. Jain, J. F. Murray, F. Roth, S. C. Turaga, V. P.\nZhigulin, K. L. Briggman, M. Helmstaedter, W. Denk,\nand H. S. Seung.\nSupervised learning of image\nrestoration with convolutional networks.\nIn IEEE\nICCV, 2007.\n[27] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long,\nR. Girshick, S. Guadarrama, and T. Darrell. Caffe:\nConvolutional architecture for fast feature embedding.\nIn ACM Multimedia, pages 675\u2013678, 2014.\n[28] M. Kiefel and P. V. Gehler. Human pose estmation\nwith \ufb01elds of parts. In ECCV, 2014.\n[29] P. Kr\u00a8ahenb\u00a8uhl and V. Koltun. Ef\ufb01cient inference in\nfully connected crfs with gaussian edge potentials. In\nNIPS, 2011.\n[30] P. Kr\u00a8ahenb\u00a8uhl and V. Koltun.\nParameter learning\nand convergent inference for dense random \ufb01elds. In\nICML, 2013.\n[31] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Im-\nagenet classi\ufb01cation with deep convolutional neural\nnetworks. In NIPS, 2012.\n[32] L. Ladicky, C. Russell, P. Kohli, and P. H. Torr. As-\nsociative hierarchical crfs for object class image seg-\nmentation. In IEEE ICCV, 2009.\n[33] J. D. Lafferty, A. McCallum, and F. C. N. Pereira.\nConditional random \ufb01elds: Probabilistic models for\nsegmenting and labeling sequence data.\nIn ICML,\n2001.\n[34] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner.\nGradient-based learning applied to document recog-\nnition. Proceedings of the IEEE, 86(11):2278\u20132324,\n1998.\n[35] G. Lin, C. Shen, I. Reid, and A. van dan Hengel. Ef\ufb01-\ncient piecewise training of deep structured models for\nsemantic segmentation. In arXiv:1504.01013, 2015.\n[36] T.-Y. Lin, M. Maire, S. Belongie, L. Bourdev, R. Gir-\nshick, J. Hays, P. Perona, D. Ramanan, C. L. Zitnick,\nand P. Dollar. Microsoft coco: Common objects in\ncontext. In arXiv:1405.0312, 2014.\n10\n[37] J. Long, E. Shelhamer, and T. Darrell. Fully convolu-\ntional networks for semantic segmentation. In IEEE\nCVPR, 2015.\n[38] M. Mostajabi, P. Yadollahpour, and G. Shakhnarovich.\nFeedforward semantic segmentation with zoom-out\nfeatures. In IEEE CVPR, 2015.\n[39] R. Mottaghi, X. Chen, X. Liu, N.-G. Cho, S.-W. Lee,\nS. Fidler, R. Urtasun, and A. Yuille. The role of con-\ntext for object detection and semantic segmentation in\nthe wild. In IEEE CVPR, 2014.\n[40] M. C. Mozer.\nBackpropagation.\nchapter A Fo-\ncused Backpropagation Algorithm for Temporal Pat-\ntern Recognition. L. Erlbaum Associates Inc., 1995.\n[41] G. Papandreou, L.-C. Chen, K. Murphy, and A. L.\nYuille.\nWeakly- and semi-supervised learning of\na dcnn for semantic image segmentation.\nIn\narXiv:1502.02734, 2015.\n[42] S. Paris and F. Durand. A fast approximation of the bi-\nlateral \ufb01lter using a signal processing approach. IJCV,\n81(1):24\u201352, 2013.\n[43] R. Pascanu, C. Gulcehre, K. Cho, and Y. Bengio. On\nthe dif\ufb01culty of training recurrent neural networks. In\nICML, 2013.\n[44] G. S. Payman Yadollahpour, Dhruv Batra. Discrimi-\nnative re-ranking of diverse segmentations. In IEEE\nCVPR, 2013.\n[45] J. Peng, L. Bo, and J. Xu. Conditional neural \ufb01elds.\nIn NIPS, 2009.\n[46] P. H. O. Pinheiro and R. Collobert. Recurrent convo-\nlutional neural networks for scene labeling. In ICML,\n2014.\n[47] S. Ross, D. Munoz, M. Hebert, and J. A. Bag-\nnell.\nLearning message-passing inference machines\nfor structured prediction. In IEEE CVPR, 2011.\n[48] D. E. Rumelhart, G. E. Hinton, and R. J. Williams.\nNeurocomputing: Foundations of research.\nchapter\nLearning Internal Representations by Error Propaga-\ntion. MIT Press, 1988.\n[49] A. G. Schwing and R. Urtasun. Fully connected deep\nstructured networks. In arXiv:1503.02351, 2015.\n[50] J. Shotton, A. Fitzgibbon, M. Cook, T. Sharp,\nM. Finocchio, R. Moore, A. Kipman, and A. Blake.\nReal-time human pose recognition in parts from sin-\ngle depth images. In IEEE CVPR, 2011.\n[51] J. Shotton, M. Johnson, and R. Cipolla. Semantic tex-\nton forests for image categorization and segmentation.\nIn IEEE CVPR, 2008.\n[52] J. Shotton, J. Winn, C. Rother, and A. Criminisi. Tex-\ntonboost for image understanding: Multi-class object\nrecognition and segmentation by jointly modeling tex-\nture, layout, and context. IJCV, 81(1):2\u201323, 2009.\n[53] K. Simonyan and A. Zisserman. Very deep convolu-\ntional networks for large-scale image recognition. In\narXiv:1409.1556, 2014.\n[54] V. Stoyanov, A. Ropson, and J. Eisner. Empirical risk\nminimization of graphical model parameters given ap-\nproximate inference, decoding, and model structure.\nIn AISTATS, 2011.\n[55] S. C. Tatikonda and M. I. Jordan. Loopy belief prop-\nagation and gibbs measures.\nIn Proceedings of the\nEighteenth Conference on Uncertainty in Arti\ufb01cial In-\ntelligence, 2002.\n[56] C. Tomasi and R. Manduchi.\nBilateral \ufb01ltering for\ngray and color images. In IEEE CVPR, 1998.\n[57] J. J. Tompson, A. Jain, Y. LeCun, and C. Bregler. Joint\ntraining of a convolutional network and a graphical\nmodel for human pose estimation. In NIPS, 2014.\n[58] Z. Tu. Auto-context and its application to high-level\nvision tasks. In IEEE CVPR, 2008.\n[59] Z. Tu, X. Chen, A. L. Yuille, and S.-C. Zhu. Image\nparsing: Unifying segmentation, detection, and recog-\nnition. IJCV, 63(2):113\u2013140, 2005.\n[60] K. Yao, B. Peng, G. Zweig, D. Yu, X. Li, and F. Gao.\nRecurrent conditional random \ufb01eld for language un-\nderstanding. In ICASSP, 2014.\n[61] Y. Zhang and T. Chen. Ef\ufb01cient inference for fully-\nconnected crfs with stationarity. In CVPR, 2012.\n11\nMethods trained with COCO\nMean IU\nOur method\n74.7\n90.4\n55.3\n88.7\n68.4\n69.8\n88.3\n82.4\n85.1\n32.6\nDeepLab[10, 41]\n72.7\n89.1\n38.3\n88.1\n63.3\n69.7\n87.1\n83.1\n85.0\n29.3\nBoxSup[12]\n71.0\n86.4\n35.5\n79.7\n65.2\n65.2\n84.3\n78.5\n83.7\n30.5\nMethods trained w/o COCO\nOur method trained w/o COCO\n72.0\n87.5\n39.0\n79.7\n64.2\n68.3\n87.6\n80.8\n84.4\n30.4\nDeepLab-MSc-CRF-LargeFOV[10]\n71.6\n84.4\n54.5\n81.5\n63.6\n65.9\n85.1\n79.1\n83.4\n30.7\nContext Deep CNN CRF[35]\n70.7\n87.5\n37.7\n75.8\n57.4\n72.3\n88.4\n82.6\n80.0\n33.4\nZoomout[38]\n64.4\n81.9\n35.1\n78.2\n57.4\n56.5\n80.5\n74.0\n79.8\n22.4\nHypercolumn[24]\n62.6\n68.7\n33.5\n69.8\n51.3\n70.2\n81.1\n71.9\n74.9\n23.9\nFCN-8s[37]\n62.2\n76.8\n34.2\n68.9\n49.4\n60.3\n75.3\n74.7\n77.6\n21.4\nMSRA CFM[13]\n61.8\n75.7\n26.7\n69.5\n48.8\n65.6\n81.0\n69.2\n73.3\n30.0\nSDS[23]\n51.6\n63.3\n25.7\n63.0\n39.8\n59.2\n70.9\n61.4\n54.9\n16.8\nNUS UDS [16]\n50.0\n67.0\n24.5\n47.2\n45.0\n47.9\n65.3\n60.6\n58.5\n15.5\nTTIC-divmbest-rerank[44]\n48.1\n62.7\n25.6\n46.9\n43.0\n54.8\n58.4\n58.6\n55.6\n14.6\nBONN O2PCPMC FGT SEGM [8]\n47.8\n64.0\n27.3\n54.1\n39.2\n48.7\n56.6\n57.7\n52.5\n14.2\nMethods trained with COCO\nOur method\n78.5\n64.4\n79.6\n81.9\n86.4\n81.8\n58.6\n82.4\n53.5\n77.4\n70.1\nDeepLab[10, 41]\n76.5\n56.5\n79.8\n77.9\n85.8\n82.4\n57.4\n84.3\n54.9\n80.5\n64.1\nBoxSup[12]\n76.2\n62.6\n79.3\n76.1\n82.1\n81.3\n57.0\n78.2\n55.0\n72.5\n68.1\nMethods trained w/o COCO\nOur method trained w/o COCO\n78.2\n60.4\n80.5\n77.8\n83.1\n80.6\n59.5\n82.8\n47.8\n78.3\n67.1\nDeepLab-MSc-CRF-LargeFOV [10]\n74.1\n59.8\n79.0\n76.1\n83.2\n80.8\n59.7\n82.2\n50.4\n73.1\n63.7\nContext Deep CNN CRF[35]\n71.5\n55.0\n79.3\n78.4\n81.3\n82.7\n56.1\n79.8\n48.6\n77.1\n66.3\nTTI zoomout 16[38]\n69.6\n53.7\n74.0\n76.0\n76.6\n68.8\n44.3\n70.2\n40.2\n68.9\n55.3\nHypercolumn[24]\n60.6\n46.9\n72.1\n68.3\n74.5\n72.9\n52.6\n64.4\n45.4\n64.9\n57.4\nFCN-8s[37]\n62.5\n46.8\n71.8\n63.9\n76.5\n73.9\n45.2\n72.4\n37.4\n70.9\n55.1\nMSRA CFM[13]\n68.7\n51.5\n69.1\n68.1\n71.7\n67.5\n50.4\n66.5\n44.4\n58.9\n53.5\nSDS[23]\n45.0\n48.2\n50.5\n51.0\n57.7\n63.3\n31.8\n58.7\n31.2\n55.7\n48.5\nNUS UDS[16]\n50.8\n37.4\n45.8\n59.9\n62.0\n52.7\n40.8\n48.2\n36.8\n53.1\n45.6\nTTIC-divmbest-rerank[44]\n47.5\n31.2\n44.7\n51.0\n60.9\n53.5\n36.6\n50.9\n30.1\n50.2\n46.8\nBONN O2PCPMC FGT SEGM[8]\n54.8\n29.6\n42.2\n58.0\n54.8\n50.2\n36.6\n58.6\n31.6\n48.4\n38.6\nTable 4. Intersection over Union (IU) accuracy of our approach, CRF-RNN, compared to the other state-of-the-art approaches on the Pascal\nVOC 2012 test set. Scores for other methods were taken the results published by the original authors. The symbols are from Chat\ufb01eld et\nal. [9].\n12\nInput Image\nCRF-RNN\nGround Truth\nB-ground Aero plane\nBicycle\nBird\nBoat\nBottle\nBus\nCar\nCat\nChair\nCow\nDining-Table\nDog\nHorse\nMotorbike\nPerson\nPotted-Plant\nSheep\nSofa\nTrain\nTV/Monitor\nFigure 6. Typical good quality segmentation results I. Illustration of sample results on the validation set of the Pascal VOC 2012 dataset.\nNote that in some cases our method is able to pick correct segmentations that are not marked correctly in the ground truth. Best viewed in\ncolour.\n13\nInput Image\nCRF-RNN\nGround Truth\nB-ground Aero plane\nBicycle\nBird\nBoat\nBottle\nBus\nCar\nCat\nChair\nCow\nDining-Table\nDog\nHorse\nMotorbike\nPerson\nPotted-Plant\nSheep\nSofa\nTrain\nTV/Monitor\nFigure 7. Typical good quality segmentation results II. Illustration of sample results on the validation set of the Pascal VOC 2012 dataset.\nNote that in some cases our method is able to pick correct segmentations that are not marked correctly in the ground truth. Best viewed in\ncolour.\n14\nInput Image\nCRF-RNN\nGround Truth\nB-ground Aero plane\nBicycle\nBird\nBoat\nBottle\nBus\nCar\nCat\nChair\nCow\nDining-Table\nDog\nHorse\nMotorbike\nPerson\nPotted-Plant\nSheep\nSofa\nTrain\nTV/Monitor\nFigure 8. Failure cases I. Illustration of sample failure cases on the validation set of the Pascal VOC 2012 dataset. Best viewed in colour.\n15\nInput Image\nCRF-RNN\nGround Truth\nB-ground Aero plane\nBicycle\nBird\nBoat\nBottle\nBus\nCar\nCat\nChair\nCow\nDining-Table\nDog\nHorse\nMotorbike\nPerson\nPotted-Plant\nSheep\nSofa\nTrain\nTV/Monitor\nFigure 9. Failure cases II. Illustration of sample failure cases on the validation set of the Pascal VOC 2012 dataset. Best viewed in colour.\n16\nB-ground Aero plane\nBicycle\nBird\nBoat\nBottle\nBus\nCar\nCat\nChair\nCow\nDinging-table\nDog\nHorse\nMotorbike\nPerson\nPotted-Plant\nSheep\nSofa\nTrain\nTV/Monitor\nInput Image\nGround Truth\nCRF-RNN\nDeepLab\nFCN-8s\nB-ground Aero plane\nBicycle\nBird\nBoat\nBottle\nBus\nCar\nCat\nChair\nCow\nDining-Table\nDog\nHorse\nMotorbike\nPerson\nPotted-Plant\nSheep\nSofa\nTrain\nTV/Monitor\nFigure 10. Qualitative comparison with the other approaches. Sample results with our method on the validation set of the Pascal VOC\n2012 dataset, compared with previous state-of-the-art methods. Segmentation results with DeepLap approach were reproduced from the\noriginal publication. Best viewed in colour.\n17\n",
        "sentence": " The boundary detector only implicitly exploits grouping cues such as closedness or continuity that can often yield improvements in the high-precision regime (Zhu et al., 2007; Kokkinos, 2010b). To capture such information we use the Normalized Cuts (NCuts) technique of Shi & Malik (1997); Arbelaez et al. Since our model is fully-convolutional we can easily combine it with the recent line of works around FCNN-based semantic segmentation(Long et al., 2014; Chen et al., 2015; Papandreou et al., 2015a; Zheng et al., 2015). 2 Oxford-TVG-CRF-RNN-COCO (Zheng et al., 2015) 74.",
        "context": "and Chen et al. [10, 41] used a CRF to re\ufb01ne segmentation\nresults obtained from a CNN. Bell et al. focused on material\nrecognition and segmentation, whereas Chen et al. reported\nvery signi\ufb01cant improvements on semantic image segmen-\ninto convolutional nets. In BMVC, 2014.\n[10] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy,\nand A. L. Yuille. Semantic image segmentation with\ndeep convolutional nets and fully connected crfs. In\nICLR, 2015.\nthe popular Pascal VOC segmentation benchmark. This im-\nprovement can be attributed to the uniting of the strengths\nof CNNs and CRFs in a single deep network.\nIn the future,\nwe plan to investigate the advan-"
    },
    {
        "title": "Untangling cycles for contour grouping",
        "author": [
            "Zhu",
            "Qihui",
            "Song",
            "Gang",
            "Shi",
            "Jianbo"
        ],
        "venue": "In Proc. CVPR,",
        "citeRegEx": "Zhu et al\\.,? \\Q2007\\E",
        "shortCiteRegEx": "Zhu et al\\.",
        "year": 2007,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " The boundary detector only implicitly exploits grouping cues such as closedness or continuity that can often yield improvements in the high-precision regime (Zhu et al., 2007; Kokkinos, 2010b).",
        "context": null
    }
]