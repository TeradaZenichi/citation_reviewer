[
    {
        "title": "A fast and simple randomized parallel algorithm for the maximal independent set problem",
        "author": [
            "Noga Alon",
            "Laszlo Babai",
            "Alon Itai"
        ],
        "venue": "Technical report,",
        "citeRegEx": "Alon et al\\.,? \\Q1985\\E",
        "shortCiteRegEx": "Alon et al\\.",
        "year": 1985,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Quantum lower bounds for the collision and the element distinctness problems",
        "author": [
            "Scott Aaronson",
            "Yaoyun Shi"
        ],
        "venue": "J. ACM,",
        "citeRegEx": "Aaronson and Shi.,? \\Q2004\\E",
        "shortCiteRegEx": "Aaronson and Shi.",
        "year": 2004,
        "abstract": "Given a function f as an oracle, the collision problem is to find two\ndistinct inputs i and j such that f(i)=f(j), under the promise that such inputs\nexist. Since the security of many fundamental cryptographic primitives depends\non the hardness of finding collisions, quantum lower bounds for the collision\nproblem would provide evidence for the existence of cryptographic primitives\nthat are immune to quantum cryptanalysis.\n  In this paper, we prove that any quantum algorithm for finding a collision in\nan r-to-one function must evaluate the function Omega((n/r)^{1/3}) times, where\nn is the size of the domain and r|n. This improves the previous best lower\nbound of Omega((n/r)^{1/5}) evaluations due to Aaronson [quant-ph/0111102], and\nis tight up to a constant factor.\n  Our result also implies a quantum lower bound of Omega(n^{2/3}) queries to\nthe inputs for the element distinctness problem, which is to determine whether\nor not the given n real numbers are distinct. The previous best lower bound is\nOmega(sqrt{n}} queries in the black-box model; and Omega(sqrt{n}log{n})\ncomparisons in the comparisons-only model, due to H{\\o}yer, Neerbek, and Shi\n[ICALP'01, quant-ph/0102078].",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Polylogarithmic independence can fool DNF formulas",
        "author": [
            "Louay M.J. Bazzi"
        ],
        "venue": "SIAM J. Comput.,",
        "citeRegEx": "Bazzi.,? \\Q2009\\E",
        "shortCiteRegEx": "Bazzi.",
        "year": 2009,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Quantum lower bounds by polynomials",
        "author": [
            "Robert Beals",
            "Harry Buhrman",
            "Richard Cleve",
            "Michele Mosca",
            "Ronald de Wolf"
        ],
        "venue": "J. ACM,",
        "citeRegEx": "Beals et al\\.,? \\Q2001\\E",
        "shortCiteRegEx": "Beals et al\\.",
        "year": 2001,
        "abstract": "We examine the number T of queries that a quantum network requires to compute\nseveral Boolean functions on {0,1}^N in the black-box model. We show that, in\nthe black-box model, the exponential quantum speed-up obtained for partial\nfunctions (i.e. problems involving a promise on the input) by Deutsch and Jozsa\nand by Simon cannot be obtained for any total function: if a quantum algorithm\ncomputes some total Boolean function f with bounded-error using T black-box\nqueries then there is a classical deterministic algorithm that computes f\nexactly with O(T^6) queries.\n  We also give asymptotically tight characterizations of T for all symmetric f\nin the exact, zero-error, and bounded-error settings. Finally, we give new\nprecise bounds for AND, OR, and PARITY. Our results are a quantum extension of\nthe so-called polynomial method, which has been successfully applied in\nclassical complexity theory, and also a quantum extension of results by Nisan\nabout a polynomial relationship between randomized and deterministic decision\ntree complexity.",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "The polynomial method in circuit complexity. In Structure in Complexity Theory Conference, pages 82\u201395",
        "author": [
            "Richard Beigel"
        ],
        "venue": "IEEE Computer Society,",
        "citeRegEx": "Beigel.,? \\Q1993\\E",
        "shortCiteRegEx": "Beigel.",
        "year": 1993,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Perceptrons, PP, and the polynomial hierarchy",
        "author": [
            "Richard Beigel"
        ],
        "venue": "Computational Complexity,",
        "citeRegEx": "Beigel.,? \\Q1994\\E",
        "shortCiteRegEx": "Beigel.",
        "year": 1994,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Le probl\u00e8me de l\u2019approximation des fonctions continues sur tout l\u2019axe",
        "author": [
            "S.N. Bernstein"
        ],
        "venue": "re\u0301el et l\u2019une de ses applications. Bull. Math. Soc. France,",
        "citeRegEx": "Bernstein.,? \\Q1924\\E",
        "shortCiteRegEx": "Bernstein.",
        "year": 1924,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Weakly learning DNF and characterizing statistical query learning using Fourier analysis",
        "author": [
            "Avrim Blum",
            "Merrick Furst",
            "Jeffrey Jackson",
            "Michael Kearns",
            "Yishay Mansour",
            "Steven Rudich"
        ],
        "venue": "In Proceedings of the twenty-sixth annual ACM symposium on Theory of computing,",
        "citeRegEx": "Blum et al\\.,? \\Q1994\\E",
        "shortCiteRegEx": "Blum et al\\.",
        "year": 1994,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "\u00c9tude des coefficients de fourier des fonctions de lp(g)",
        "author": [
            "Aline Bonami"
        ],
        "venue": "Annales de l\u2019institut Fourier,",
        "citeRegEx": "Bonami.,? \\Q1970\\E",
        "shortCiteRegEx": "Bonami.",
        "year": 1970,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Polynomial regression under arbitrary product distributions",
        "author": [
            "Eric Blais",
            "Ryan O\u2019Donnell",
            "Karl Wimmer"
        ],
        "venue": "Machine Learning,",
        "citeRegEx": "Blais et al\\.,? \\Q2010\\E",
        "shortCiteRegEx": "Blais et al\\.",
        "year": 2010,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Randomness-efficient oblivious sampling",
        "author": [
            "M. Bellare",
            "J. Rompel"
        ],
        "venue": "In FOCS,",
        "citeRegEx": "Bellare and Rompel.,? \\Q1994\\E",
        "shortCiteRegEx": "Bellare and Rompel.",
        "year": 1994,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Pseudorandom generators for regular branching programs",
        "author": [
            "Mark Braverman",
            "Anup Rao",
            "Ran Raz",
            "Amir Yehudayoff"
        ],
        "venue": null,
        "citeRegEx": "Braverman et al\\.,? \\Q2010\\E",
        "shortCiteRegEx": "Braverman et al\\.",
        "year": 2010,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "The coin problem and pseudorandomness for branching programs",
        "author": [
            "Joshua Brody",
            "Elad Verbin"
        ],
        "venue": "In FOCS,",
        "citeRegEx": "Brody and Verbin.,? \\Q2010\\E",
        "shortCiteRegEx": "Brody and Verbin.",
        "year": 2010,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Bernstein\u2019s approximation problem",
        "author": [
            "Lennart Carleson"
        ],
        "venue": "Proc. Amer. Math. Soc.,",
        "citeRegEx": "Carleson.,? \\Q1951\\E",
        "shortCiteRegEx": "Carleson.",
        "year": 1951,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Introduction to Approximation Theory",
        "author": [
            "E.W. Cheney"
        ],
        "venue": "AMS Chelsea Publishing Series. AMS Chelsea Pub.,",
        "citeRegEx": "Cheney.,? \\Q1982\\E",
        "shortCiteRegEx": "Cheney.",
        "year": 1982,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Improved pseudorandom generators for depth 2 circuits",
        "author": [
            "Anindya De",
            "Omid Etesami",
            "Luca Trevisan",
            "Madhur Tulsiani"
        ],
        "venue": "Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques,",
        "citeRegEx": "De et al\\.,? \\Q2010\\E",
        "shortCiteRegEx": "De et al\\.",
        "year": 2010,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Approximate resilience, monotonicity, and the complexity of agnostic learning",
        "author": [
            "Dana Dachman-Soled",
            "Vitaly Feldman",
            "Li-Yang Tan",
            "Andrew Wan",
            "Karl Wimmer"
        ],
        "venue": "CoRR, abs/1405.5268,",
        "citeRegEx": "Dachman.Soled et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Dachman.Soled et al\\.",
        "year": 2014,
        "abstract": "A function $f$ is $d$-resilient if all its Fourier coefficients of degree at\nmost $d$ are zero, i.e., $f$ is uncorrelated with all low-degree parities. We\nstudy the notion of $\\mathit{approximate}$ $\\mathit{resilience}$ of Boolean\nfunctions, where we say that $f$ is $\\alpha$-approximately $d$-resilient if $f$\nis $\\alpha$-close to a $[-1,1]$-valued $d$-resilient function in $\\ell_1$\ndistance. We show that approximate resilience essentially characterizes the\ncomplexity of agnostic learning of a concept class $C$ over the uniform\ndistribution. Roughly speaking, if all functions in a class $C$ are far from\nbeing $d$-resilient then $C$ can be learned agnostically in time $n^{O(d)}$ and\nconversely, if $C$ contains a function close to being $d$-resilient then\nagnostic learning of $C$ in the statistical query (SQ) framework of Kearns has\ncomplexity of at least $n^{\\Omega(d)}$. This characterization is based on the\nduality between $\\ell_1$ approximation by degree-$d$ polynomials and\napproximate $d$-resilience that we establish. In particular, it implies that\n$\\ell_1$ approximation by low-degree polynomials, known to be sufficient for\nagnostic learning over product distributions, is in fact necessary.\n  Focusing on monotone Boolean functions, we exhibit the existence of\nnear-optimal $\\alpha$-approximately\n$\\widetilde{\\Omega}(\\alpha\\sqrt{n})$-resilient monotone functions for all\n$\\alpha>0$. Prior to our work, it was conceivable even that every monotone\nfunction is $\\Omega(1)$-far from any $1$-resilient function. Furthermore, we\nconstruct simple, explicit monotone functions based on ${\\sf Tribes}$ and ${\\sf\nCycleRun}$ that are close to highly resilient functions. Our constructions are\nbased on a fairly general resilience analysis and amplification. These\nstructural results, together with the characterization, imply nearly optimal\nlower bounds for agnostic learning of monotone juntas.",
        "full_text": "arXiv:1405.5268v2  [cs.LG]  9 Jul 2014\nApproximate resilience, monotonicity, and the complexity of\nagnostic learning\nDana Dachman-Soled\nUniversity of Maryland\nVitaly Feldman\nIBM Research - Almaden\nLi-Yang Tan\u2217\nColumbia University\nAndrew Wan\nSimons Institute, UC Berkeley\nKarl Wimmer\u2020\nDuquesne University\nAbstract\nA function f is d-resilient if all its Fourier coef\ufb01cients of degree at most d are zero, i.e. f is uncorre-\nlated with all low-degree parities. We study the notion of approximate resilience of Boolean functions,\nwhere we say that f is \u03b1-approximately d-resilient if f is \u03b1-close to a [\u22121, 1]-valued d-resilient function\nin \u21131 distance. We show that approximate resilience essentially characterizes the complexity of agnos-\ntic learning of a concept class C over the uniform distribution. Roughly speaking, if all functions in a\nclass C are far from being d-resilient then C can be learned agnostically in time nO(d) and conversely,\nif C contains a function close to being d-resilient then agnostic learning of C in the statistical query\n(SQ) framework of Kearns has complexity of at least n\u2126(d). This characterization is based on the dual-\nity between \u21131 approximation by degree-d polynomials and approximate d-resilience that we establish.\nIn particular, it implies that \u21131 approximation by low-degree polynomials, known to be suf\ufb01cient for\nagnostic learning over product distributions, is in fact necessary.\nFocusing on monotone Boolean functions, we exhibit the existence of near-optimal \u03b1-approximately\ne\u2126(\u03b1\u221an)-resilient monotone functions for all \u03b1 > 0. Prior to our work, it was conceivable even that every\nmonotone function is \u2126(1)-far from any 1-resilient function. Furthermore, we construct simple, explicit\nmonotone functions based on Tribes and CycleRun that are close to highly resilient functions. Our\nconstructions are based on general resilience analysis and ampli\ufb01cation techniques we introduce. These\nstructural results, together with the characterization, imply nearly optimal lower bounds for agnostic\nlearning of monotone juntas, a natural variant of the well-studied junta learning problem. In particular\nwe show that no SQ algorithm can ef\ufb01ciently agnostically learn monotone k-juntas for any k = \u03c9(1)\nand any constant error less than 1/2.\n1\nIntroduction\nThe agnostic learning framework [Hau92, KSS94], models learning from examples in the presence of worst-\ncase noise. In this framework the learning algorithm is given random examples (x, f(x)) where x is chosen\nfrom some distribution D and f is an arbitrary Boolean function. The goal of the agnostic learning algorithm\nfor a concept class C is to output a hypothesis h that agrees with f almost as well as the best function in C;\nthat is:\nPrD[h(x) \u0338= f(x)] \u2264min\nc\u2208C PrD[c(x) \u0338= f(x)] + \u03b5,\nwhere \u03b5 is an error parameter given to the algorithm.\n\u2217Supported by NSF grants CCF-1115703 and CCF-1319788.\n\u2020Supported in part by NSF-CCF-1117079. Most of this work was done while the author was visiting Simons Institute for the\nTheory of Computing, University of California-Berkeley\nUnderstanding the complexity of learning in the agnostic model is central to both theory and practice in\nmachine learning research. Learning in this model is notoriously hard, and despite two decades of intensive\nresearch our formal understanding of the complexity of agnostic learning is still very limited. Even when\nD is the uniform distribution over {\u22121, 1}n, agnostic learning has proven extremely challenging: few non-\ntrivial classes are known to be learnable agnostically. The primary technique used for agnostic learning\nin this setting is the polynomial \u21131 regression algorithm introduced in the in\ufb02uential work of Kalai et al\n[KKMS08]. This algorithm \ufb01nds a low-degree polynomial that minimizes the \u21131 distance to the target\nfunction, and can be applied to agnostically learn classes which are well approximated by polynomials.\nThis approach has lead to the \ufb01rst agnostic learning algorithm for AC0 circuits (in quasi-polynomial time)\nand halfspaces (in nO(1/\u03b52) time) over the uniform distribution [KKMS08] and was used in many other\nagnostic learning results.\nIn this work we address the complexity of agnostic learning relative to the uniform and, more generally,\nproduct distributions. In addition to running time, a critical but often unstated parameter in lower bounds\non agnostic learning is the value of OPTC(D, f) = minc\u2208C Pr[c(x) \u0338= f(x)] to which the lower bound\napplies (note that OPT is essentially the noise rate). If a hardness result requires learning functions f for\nwhich OPTC(D, f) is close to 1/2, then it does not apply to most practical learning applications. (If C\ndoes not have any useful classi\ufb01ers, it does not make much sense to use C as a performance benchmark.)\nTherefore it is more important to understand the complexity of agnostic learning in which OPT is a small\nconstant close to 0 (or even approaches 0 as n grows). However essentially all known lower bounds for\nagnostic learning are in the hardest regime when OPTC(D, f) goes to 1/2 as dimension and other problem\nparameters grow (although there are some notable exceptions in restricted models and the more challenging\ndistribution-independent setting [KS10, FGRW12]). In this work we aim to precisely characterize the value\nof OPT for which agnostic learning becomes hard and therefore will make this parameter explicit in our\nlower bounds.\nIn machine learning literature it is more common to specify the excess error which is the difference\nbetween OPTC(D, f) and the error of the produced hypothesis that an algorithm can achieve. It is easy\nto see that lower bounds showing that excess error of \u03ba cannot be achieved is equivalent to stating that the\nlower bound applies to a setting where OPT = 1/2 \u2212\u03ba (since error of 1/2 can always be achieved).\n1.1\nApproximate resilience and agnostic learning\nIn this work we explain why the polynomial \u21131 regression algorithm is the best approach known to date\nfor agnostically learning over product distributions. Speci\ufb01cally, we prove that the complexity of agnostic\nlearning C over a product distribution in the statistical query model is characterized by how well C can\nbe approximated in the \u21131 norm by low-degree polynomials over the same distribution. The statistical\nquery (SQ) model [Kea98] is a well-studied restriction of the PAC learning model in which the learner\nrelies on approximate expectations of functions of an example rather than examples themselves. With the\nexception of Gaussian elimination1 all known techniques used in the theory and practice of machine learning\nhave statistical query analogues. Polynomial \u21131 regression is no exception, and therefore to prove our\ncharacterization it suf\ufb01ces to establish a lower bound on learning by statistical query algorithms for function\nclasses that are not well-approximated by low-degree polynomials.\nThe optimality of \u21131 regression for agnostic learning over product distributions that we prove is based\non a formal connection between agnostic learning and a basic structural property of Boolean functions. We\nsay that a function g : {\u22121, 1}n \u2192R is d-resilient if bg(S) = 0 for all |S| \u2264d, i.e. g is uncorrelated with\nevery low-degree parity. Equivalently, g is d-resilient if and only if E[g\u03c1] = E[g] for any restriction \u03c1 to\nat most d out of n variables and E[g] = 0. Functions which satisfy the \ufb01rst property are called correlation\n1Note that Gaussian elimination fails in the presence of even minor amounts of random noise and is not applicable in the agnostic\nframework.\n2\nimmune and are widely-studied for cryptographic applications. The structural question we will be interested\nin is:\nHow close can a Boolean function be to a highly resilient function with range in [\u22121, 1]?\nMore precisely, we say that f : {\u22121, 1}n \u2192[\u22121, 1] is \u03b1-approximately d-resilient if there exists a d-\nresilient g : {\u22121, 1}n \u2192[\u22121, 1] such that \u2225f \u2212g\u22251 = E[|f(x) \u2212g(x)|] \u2264\u03b1, and we will be interested in\nfunctions that are \u03b1-approximately d-resilient for small values of \u03b1 and large values of d. We note that for\nsimplicity and convenience the de\ufb01nitions here are for the uniform distribution on the hypercube but can be\neasily extended to general product distributions over other n-dimensional domains (see Section A).\nThe notion of resilience is well-studied and has applications in cryptography, pseudorandomness, in-\napproximability, circuit complexity and more (for a few examples, see [CGH+85, LW95, AM09, AH11,\nShe11]). However, to the best of our knowledge our notion of approximate resilience does not appear to\nhave been explicitly studied before.\nAt a high level we show that if a concept class C contains an \u03b1-approximately d-resilient function then\nthe complexity of learning C agnostically in the SQ model is n\u2126(d). Further, learning is hard even for\nOPT \u2264\u03b1/2 (in other words when noise rate is \u03b1/2). For simplicity the complexity of an SQ algorithm\nrefers to a polynomial upper-bounding both the running time and the inverse of query tolerance. Naturally,\nthe presence of a single \u03b1-approximately d-resilient function would not suf\ufb01ce for a hardness result since a\nconcept class with a single function can be easily learned agnostically. We therefore need some assumptions\nunder which existence of a single \u03b1-approximately d-resilient function will imply that there are many of\nthem. One such assumption that we adopt is that the \u03b1-approximately d-resilient function c depends on\nat most n1/3 variables (such a function is called a n1/3-junta) and the concept class C is closed under\nrenaming of variables. Alternatively, if we consider an ensemble of concept classes {Cn}\u221e\nn=1 parameterized\nby dimension n it would be suf\ufb01cient to assume that the ensemble is closed under addition of irrelevant\nvariables. For brevity we omit the closed-ness under renaming since it is satis\ufb01ed by all commonly-studied\nconcept classes. We now state our lower bound in terms of resilience informally.\nTheorem 1.1. Let C be a concept class. Fix d and let \u03b1(d) be such that, there exists a \u03b1(d)-approximately\nd-resilient n1/3-junta c \u2208C. Then any SQ algorithm for agnostically learning C with excess error of at most\n1\u2212\u03b1(d)\n2\n\u2212n\u2212o(d) has complexity of at least n\u2126(d).\nAlternatively, this result can be stated as saying that if for every function f satisfying OPTC(D, f) \u2264\n\u03b1(d)/2 the algorithm outputs h such that PrD[h(x) \u0338= f(x)] \u22641/2 \u2212n\u2212o(d) then its SQ complexity is\nn\u2126(d). An immediate implication of this theorem is that a concept class containing an o(1)-approximately\nd-resilient function cannot be learned with noise rate larger than o(1) in time n\u2126(d).\nThe proof of this theorem is based on the simple observation that agnostic learning of C is at least as hard\nas weak learning of a class of d-resilient functions which are close to functions in C. From there we rely\non hardness of SQ learning of pairwise nearly orthogonal functions to obtain the claim. This result relies\ncrucially on the distribution being a product distribution and it is was recently demonstrated that is does not\nhold for some non-product distributions [FK14].\nThe lower bounds obtained from this technique are closest in spirit to lower bounds based on cryp-\ntographic assumptions and those based on hardness of learning sparse parities with noise. Cryptographic\nhardness relies on a certain problem being hard for all known \u201cattacks\u201d. As pointed out above, SQ algo-\nrithms capture all known agnostic learning algorithms and learning techniques in general. Therefore the\nlower bounds hold against all known learning algorithms. Further, as in our lower bounds, degree of re-\nsilience of a predicate is the primary hardness parameter in many cryptographic constructions (cf. [OW14]).\nThis simple technique might appear to be a relatively limited approach to obtaining lower bounds. Yet,\nit turns out that the lower bounds it achieves are essentially optimal. This follows from the duality between\n3\napproximate resilience and \u21131 approximation by low-degree polynomials that we establish. More formally,\nlet Pd be the class of degree at most d real-valued polynomials. For a Boolean function f, let \u2206Pd(f) =\nminp\u2208Pd E[|f \u2212p|].\nTheorem 1.2. For f : {\u22121, 1}n \u2192{\u22121, 1} and 0 \u2264d \u2264n and \u03b1 \u22650, f is \u03b1-approximately d-resilient if\nand only if \u2206Pd(f) \u22651 \u2212\u03b1.\nThe proof of this result is a fairly simple application of a classical result on duality of norms by Ioffe\nand Tikhomirov [IT68].\nNow for a concept class C, let \u2206Pd(C) = maxf\u2208C \u2206Pd(f). To see how this quantity characterizes ag-\nnostic learning in the statistical query model, we state the error and running time achieved by the polynomial\n\u21131 regression algorithm of Kalai et al. for agnostic learning [KKMS08]. This algorithm is easy to implement\nin the SQ model2.\nTheorem 1.3 ([KKMS08]). Let C be a concept class over {\u22121, 1}n and \ufb01x d. There exists a SQ algorithm\nwhich for any \u03b5 > 0 agnostically learns C with excess error \u2206Pd(C)/2+\u03b5 and has complexity poly(nd, 1/\u03b5).\nOn the other hand, we may apply Theorems 1.2 and 1.1 to show that this is the best any SQ algorithm can\ndo; by Theorem 1.2 there exists an \u03b1(d)-approximately d-resilient function in C with 1 \u2212\u03b1(d) = \u2206Pd(C).\nTherefore Theorem 1.1 essentially matches the upper bound of Theorem 1.3 in excess error and complexity,\nimplying the optimality of \u21131-regression based algorithms for agnostic learning over the uniform distribution.\nThe extension to other product distributions is fairly straightforward and we discuss it in Sec. A.\n1.2\nLearning monotone juntas\nWith this characterization in hand, we would like to better understand what classes of functions we can\nhope to agnostically learn on the uniform distribution. Uniform distribution learning is challenging even in\nthe noiseless setting, with ef\ufb01cient algorithms out of reach for natural classes such as polynomial size DNF\nformulas and decision trees. However, learning monotone functions and their corresponding subclasses\nseems signi\ufb01cantly easier; for example, monotone decision trees [OS07] and monotone DNFs with few\nterms [Ser01] are ef\ufb01ciently learnable in the SQ model (for other examples see [OW13, BBL98, BT96]).\nThis difference is demonstrated most dramatically in the junta learning problem, which is considered by\nmany to be the single most important open problem in uniform distribution learning. In this problem, the\ntarget function is an unknown k-junta, a Boolean function which depends on at most k \u226an variables. The\njunta problem also lies at the heart of the notorious DNF and decision tree learning problems: Since s-term\nDNFs and s-leaf decision trees can compute arbitrary (log s)-juntas, learning either of these classes requires\nthat we \ufb01rst be able to ef\ufb01ciently learn \u03c9(1)-juntas. Progress has remained slow in the 20 years since Blum\nposed the junta problem, with the current fastest algorithm running in time n.60k [Val12], improving on the\n\ufb01rst non-trivial algorithm which runs in time n.704k [MOS04] (the trivial algorithm exhaustively checks all\nk-subsets of [n] and runs in time O(nk)). In contrast, monotone juntas are easy to learn using an extremely\nsimple algorithm: the relevant variables can be identi\ufb01ed by estimating their correlations with the target\nfunction E[f(x)xi] = bf({i}), and thus monotone k-juntas can be learned in time O(n + 2k). Does the\nadvantage of monotonicity hold in the agnostic setting as well? We \ufb01rst consider the simplest problem\nof agnostic learning monotone juntas. While it appears to be a hard problem, known hardness results for\nspeci\ufb01c monotone functions do not rule out polynomial time algorithms for any constant \u03b5. Speci\ufb01cally, the\nbest known lower bound is n\u2126(1/\u03b52) for majority functions [KKMS08] and is based on the assumption that\n2To the best of our knowledge this is not proved anywhere explicitly but is fairly well-known and used in some other works [?].\nIt follows from the fact that LPs can optimized approximately using approximate evaluations of the optimized function (in our case\nexpected \u21131 error) for example via the Ellipsoid algorithm [Lov87]. See [FPV13] for more details on this general technique.\n4\nlearning sparse noisy parities is hard. Further, this hardness result only applies when OPT \u22651/2\u2212\u03b5 which\nleaves open the possibility that the problem is solvable ef\ufb01ciently when the noise rate is a constant smaller\nthan 1/2.\nAs we saw in Theorem 1.1, the complexity of agnostic learning of C is characterized by the approximate\nresilience of functions in C. Therefore we consider the structural question of how close monotone functions\nare to bounded resilient functions. The structure of monotone functions over the Boolean hypercube has\nbeen investigated in many in\ufb02uential works (see [BBL98, BT96, MO02, O\u2019D03, OW13]). While to the best\nof our knowledge our notion has not been studied before, several works have examined the total spectral\nweight that monotone functions have on low-degree coef\ufb01cients [BT96, MO02]. Spectral weight indicates\nthe distance to the closest (not necessarily bounded) resilient function in \u21132 norm. Both differences of\nbounded/unbounded and \u21131/\u21132 are signi\ufb01cant, but we show how bounds on low-degree spectral weight can\nserve as a basis for bounds on our notion of distance to resilience (see Thm. 3.2).\nIt is easy to see that monotone functions cannot be 1-resilient, and prior to our work, it was possible that\nevery monotone function was \u2126(1)-far from 1-resilient. Our \ufb01rst structural result rules out this possibility\nin a very strong way:\nTheorem 1.4. For every \u03b1 > 0 there exists an \u03b1-approximately d-resilient monotone Boolean function\nwhere d = \u2126(\u03b1\u221an/ log n).\nOur proof of this result is indirect and relies crucially on the duality of approximate resilience and \u21131-\napproximation of monotone functions by polynomials. We use a lower bound for PAC learning of monotone\nfunctions by Blum et al. [BBL98] to obtain strong lower bounds on \u21131-approximation of monotone functions\nby polynomials. We can then use Theorem 1.2 to obtain bounds on distance to resilience.\nThis degree of resilience is essentially optimal: combining basic facts from discrete Fourier analysis, it\nis straightforward to see that every monotone Boolean function is \u03b1-far from any \u2126(\u03b1\u221an)-resilient func-\ntion [BT96]. Applying our connection between approximate resilience and agnostic learning, we get as a\ncorollary our main application:\nCorollary 1.5. Any SQ algorithm for agnostically learning the class of monotone k-juntas with excess error\nof 1/2 \u2212\u03b1 has complexity of n\u2126(\u03b1\n\u221a\nk/ log k).\nQualitatively, Corollary 1.5 gives the \ufb01rst super-polynomial lower bound on the complexity of SQ algo-\nrithms for agnostically learning monotone k-juntas with constant (and even sub-constant) noise. It also rules\nout the possibility of ef\ufb01cient SQ algorithms for agnostic learning monotone decision trees and monotone\nDNFs with few terms (which, as previously mentioned, do have ef\ufb01cient SQ algorithms in the noiseless\nsetting). Quantitatively, our lower bound essentially matches the upper bound of nO(\n\u221a\nk/\u03b5) that follows as\na corollary of the low-degree concentration bound of [BT96] and the polynomial \u21131 regression algorithm\n[KKMS08]. Note that lower bounds on PAC learning of monotone functions [BBL98] cannot be translated\ndirectly to lower bounds in the junta learning setting since these lower bounds are subexponential in k while\njunta learning algorithms are allowed to run in time polynomial in 2k.\nWhile Theorem 1.4 yields a near-optimal lower bound on the complexity of agnostically learning gen-\neral monotone juntas, the construction is not explicit: it is based on a randomized DNF construction (similar\nto Talagrand\u2019s randomized DNF construction [Tal96]), and contains functions of high complexity. Further-\nmore, for more general classes such as monotone DNFs, the hardness results implied are not optimal. We\n\ufb01rst show that even the simple Tribes function, a read-once DNF, is close to a resilient function (which gives\na stronger hardness result for learning small monotone DNFs).\nTheorem 1.6. Tribes is \u03b1-approximately d-resilient, where \u03b1 = O(n\u22121/3) and d = \u2126(log n/ log log n).\nOur proof of Theorem 1.6 is based on a general technique for obtaining bounds on approximate re-\nsilience from bounds on spectral weight on low-degree coef\ufb01cients. Roughly, our result states that for\n5\na suf\ufb01ciently small \u03b3, if the total spectral weight on degree \u2264d coef\ufb01cients of f is at most \u03b3, then f\nis \u2248\u221a\u03b3ed-approximately d-resilient (see Thm. 3.2). The proof relies on a concentration inequality for\nlow-degree polynomials over independent Rademacher random variables that follows from the hypercon-\ntractivity inequalities of Bonami and Beckner [Bon70, Bec75].\nWe then describe a general technique for amplifying the degree of approximate resilience of functions\nvia iterative composition and apply it to Tribes to obtain an explicit function that is o(1)-approximately\n2\u2126(\u221alog n)-resilient (see Section 3.4 for details).\nBoth Theorems 1.4 and 1.6 give monotone Boolean functions which are close to resilient functions,\nhowever the resilient functions are not necessarily Boolean-valued. In most cryptographic applications\nresilience is studied speci\ufb01cally for Boolean functions (e.g., [Sie84, MOS04, OW14]), and therefore it is\nnatural to ask if there are such functions that are close to monotone Boolean functions. Using a new function\ncalled CycleRun [Wie], we show that this is indeed possible, and furthermore we nearly match the resilience\nof the iterated Tribes construction:\nTheorem 1.7. There is an explicit \u03b1-approximately d-resilient monotone Boolean function f where \u03b1 =\non(1) and d = 2\u2126(\u221alog n/ log log n). Furthermore, f is \u03b1-close to a Boolean d-resilient function.\nWe prove Theorem 1.7 by \ufb01rst showing that CycleRun is O(\np\nlog n/n)-approximately 1-resilient, where\nour witness to this approximate resilience is a Boolean function. Our argument crucially relies on four key\nproperties of CycleRun: monotonicity, low in\ufb02uence, oddness, and invariance under cyclic shifts; as far as\nwe know, CycleRun is the only explicit Boolean function known to have all four properties. These properties\nallow us to use a structured combinatorial argument, unlike our argument for Tribes that relies on properties\nof polynomials and produces a witness that is a bounded function (and applying this style of argument to\nTribes quickly gets unruly). Having established O(\np\nlog n/n)-approximate 1-resilience, we then apply the\naforementioned general ampli\ufb01cation technique to increase the degree of resilience to 2e\u2126(\u221alog n).\nWe remark that while the degrees of resilience obtained in Theorems 1.7 and 1.6 are not as strong as that\nof Theorem 1.4, both are suf\ufb01cient to rule out the existence of ef\ufb01cient SQ algorithms for learning monotone\nk-juntas for any k = \u03c9n(1) and subconstant error-rate.\n1.3\nRelated work\nLower bounds for statistical query algorithms were \ufb01rst shown by Kearns [Kea98] who proved that parities\ncannot be learned by SQ algorithms. Soon after this Blum et al. [BFJ+94] characterized the weak PAC\nlearnability of every function class C in the SQ model in terms of the statistical query dimension of C;\nroughly speaking, this is the largest number of functions from C that are pairwise nearly orthogonal to each\nother (we give a precise de\ufb01nition in Section 2). These lower bound techniques were extended to strong PAC\nlearning and agnostic learning in more recent work [Sim07, Fel12, Sz\u00a8o09]. Lower bounds for SQ algorithms\nwere proved for many learning problems including, for example, PAC learning of juntas [BFJ+94], weak-\nlearning of intersections of halfspaces [KS07] and learning of monotone depth-3 formulas [FLS11]. These\nlower bounds are information-theoretic but capture remarkably well the computational hardness of learning\nproblems. In some cases, such as learning juntas over the uniform distribution, this is the only known formal\nevidence of the hardness of the problem.\nGiven the lack of general lower bounds for several basic problems in agnostic learning, many works\nconcentrate on lower bounds against speci\ufb01c popular algorithms such as \u21131-regression [KS10] and margin-\nbased linear methods [LS11, BDLSS12, DLSS14]. These techniques are captured by SQ algorithms and\ntherefore our lower bounds are substantially more general.\nSeveral previously known lower bounds for agnostic learning are based on the reduction to learning\nof k-sparse noisy parities. This is a notoriously hard problem for which the only non-trivial algorithm is\nthe recent breakthrough result of Valiant that gives an algorithm running in time n0.8k [Val12]. Assuming\n6\nthat this problem requires n\u2126(k) time we get that agnostic learning of majorities on the uniform distribution\nrequires n\u2126(1/\u03b52) time [KKMS08] and conjunctions require n\u2126(log(1/\u03b5)) time [Fel12]. Learning k-sparse\nparities in the SQ model has complexity of n\u2126(k) and therefore these results also give unconditional SQ\nlower bounds. These lower bounds can be interpreted as special cases of our approach. They are based on\nshowing that a parity of high-degree has a signi\ufb01cant correlation with a function in C. Clearly a k-sparse\nparity function is (k \u22121)-resilient and correlation implies that distance to that parity is slightly better than\nthe trivial 1. The main limitation of this approach is that in most cases it can only lead to hardness results\nwhen the noise rate is close to 1/2. In particular this approach cannot lead to the strong hardness results we\nprove here for monotone juntas.\nIn a recent work Feldman and Kothari [FK14] show that the equivalence between \u21131 approximation by\npolynomials and agnostic learning does not extend to non-product distributions. They exhibit a distribution\nD for which any polynomial that is 1/3-close to the disjunction of all the variables in \u21131 (measured relative\nto D) must have degree \u2126(\u221an). At the same time disjunctions are SQ learnable in time nO(log(1/\u03b5)) over\nthat distribution.\nOur approach to proving lower bounds is closest in spirit and shares technical elements with the in\ufb02uen-\ntial pattern matrix method of Sherstov [She11]. His method shows that lower bounds on the approximation\nby polynomials in \u2113\u221enorm of a function f can be translated into lower bounds on randomized communica-\ntion complexity of a certain communication problem corresponding to evaluation of f on different subsets\nof variables (which were previously thought as stronger than lower bounds on approximation in \u2113\u221eby poly-\nnomials). A crucial step in his result is an application of duality that is in some sense symmetric to ours\nand shows the existence of an unbounded resilient function g that is correlated with f. Such g then serves\nto upper bound discrepancy for the communication problem (from which a lower bound on randomized\ncommunication complexity follows).\n1.4\nPreliminaries\nAll probabilities and expectations are with respect to the uniform distribution unless otherwise stated, and we\nwill use boldface (e.g. x and y) to denote random variables. Given f, g : {\u22121, 1}n \u2192R, we say that f and\ng are \u03b5-close if \u2225f \u2212g\u22251 = E[|f(x) \u2212g(x)|] \u2264\u03b5. We say that g is bounded if it takes values in the interval\n[\u22121, 1]. Note that if f is Boolean valued and g is bounded, then \u2225f \u2212g\u22251 = 1 \u2212E[fg]. Every function\ng : {\u22121, 1}n \u2192R can be uniquely written as a multilinear polynomial such that g(x) =\nX\nS\u2286[n]\nbg(S)\nY\ni\u2208S\nxi\nfor all x \u2208{\u22121, 1}n; the coef\ufb01cients bg(S) are called the Fourier coef\ufb01cients of g. The total in\ufb02uence of\na Boolean function f : {\u22121, 1}n \u2192{\u22121, 1}, denoted Inf[f], is Pn\ni=1 Pr[f(x) \u0338= f(x\u2295i)], where x\u2295i\ndenotes x with its i-th coordinate \ufb02ipped.\nDe\ufb01nition 1.8. A function g : {\u22121, 1}n \u2192R is d-resilient if bg(S) = 0 for all |S| \u2264d. We say that\na Boolean function f : {\u22121, 1}n \u2192{\u22121, 1} is \u03b1-approximately d-resilient if there exists a d-resilient\nbounded function g such that \u2225f \u2212g\u22251 \u2264\u03b1.\nLearning background In the agnostic learning framework, the learning algorithm is given labeled exam-\nples (x, y) where x \u2208{\u22121, 1}n and y \u2208{\u22121, 1} are drawn from a distribution D over {\u22121, 1}n \u00d7{\u22121, 1}.\nAs usual we describe such distributions by a pair (D, g), where D is the marginal distribution on {\u22121, 1}n\nand g : {\u22121, 1}n \u2192[\u22121, 1], where g(x) = E(x,y)\u223cD[ y | x = x ] is expectation of the label for each input.\nNote that for every Boolean function f, if U denotes the uniform distribution then E(x,y)\u223c(U,g)[f(x) \u0338=\ny] = \u2225f \u2212g\u22251/2.\nDe\ufb01nition 1.9. Let C be a class of Boolean functions on {\u22121, 1}n. An algorithm A agnostically learns\nC over distribution D on {\u22121, 1}n if for any g : {\u22121, 1}n \u2192[\u22121, 1] and \u03b5 > 0, given examples from\n7\ndistribution D = (D, g) and \u03b5, it outputs with probability at least 2/3 hypothesis h : {\u22121, 1}n \u2192{\u22121, 1}\nsuch that:\nPr[h(x) \u0338= y] \u2264OPTC(D, g) + \u03b5,\nwhere OPT = minc\u2208C Pr(x,y)\u223c(D,g)[c(x) \u0338= y]. The algorithm is said to learn with excess error \u03ba if h\ninstead satis\ufb01es\nPr[h(x) \u0338= y] \u2264OPTC(D, g) + \u03ba.\nDe\ufb01nition 1.10. A statistical query is de\ufb01ned by a bounded function of an example \u03c6 : {\u22121, 1}n \u00d7\n{\u22121, 1} \u2192[\u22121, 1] and positive tolerance \u03c4. A valid reply to such a query relative to a distribution D\nover examples is a value v that satis\ufb01es:\n|E(x,y)\u223cD[\u03c6(x, y)] \u2212v| \u2264\u03c4.\nA statistical query learning algorithm is an algorithm which relies solely on statistical queries and does\nnot have access to actual examples. We say that an SQ algorithm has statistical query complexity T if it\nmakes at most q statistical queries of tolerance at least \u03c4 and T \u2265max{q, 1/\u03c4}.\n2\nCharacterization of Agnostic Learning\nIn this section we show that approximate resilience implies hardness of agnostic learning for statistical query\nalgorithms (Lemma 2.1). We then show that the implication works in the reverse direction as well: if a class\ndoes not contain approximately resilient functions, then it can be agnostically learned by SQ algorithms. We\nprove this equivalence using the duality between approximate resilience and approximation by low-degree\npolynomials stated in Theorem 1.2. This simple observation turns out to be surprisingly useful, leading both\nto a characterization of agnostic learning and to a proof of our \ufb01rst structural result for monotone functions\n(Theorem 1.4).\nTo connect our notion of approximate resilience to the hardness of agnostic learning we will use the\nfollowing standard notion of designs of sets with small overlap. A (n, k, d)-design of size m is a collection\nof sets S1, . . . , Sm \u2286[n] such that |Si| = k and |Si \u2229Sj| \u2264d for all i \u0338= j. Let M(n, k, d) denote the size\nof the largest (n, k, d)-design. Standard probabilistic/greedy argument implies that\nM(n, k, d) \u2265\n\u0000n\nk\n\u0001\n\u0000k\nd\n\u0001\u0000n\u2212d\nk\u2212d\n\u0001 =\n\u0000n\nd\n\u0001\n\u0000k\nd\n\u00012 \u2265\n\u0012 nd\ne2k2\n\u0013d\n.\n(1)\nFor a function f : {\u22121, 1}k \u2192{\u22121, 1} and set S \u2286[n] of size k we use fS : {\u22121, 1}n \u2192{\u22121, 1} to\ndenote f(x|S) where x|S refers to the restriction of x to coordinates with indices in S (in the usual order).\nLemma 2.1. Let f : {\u22121, 1}k \u2192{\u22121, 1} be an \u03b1-approximately d-resilient function. Let S1, . . . , Sm be a\n(n, k, d)-design. If {fSi}m\ni=1 \u2286C, then any SQ algorithm for agnostically learning C with excess error of at\nmost 1\u2212\u03b1\n2\n\u2212m\u22121/3 has complexity of at least m1/3.\nTo prove Lemma 2.1, we will use the following result implicit in [Fel12] that is a simple generalization\nof the well-known SQ-DIM bounds from [BFJ+94] and their strengthening in [Yan05, Sz\u00a8o09].\nTheorem 2.2. Let D be a distribution and let g1, . . . , gm be bounded real-valued functions such that\n|\u27e8gi, gj\u27e9D| \u22641/m for i \u0338= j, where \u27e8gi, gj\u27e9D = ED[gi(x) \u00b7 gj(x)]. Then any SQ algorithm that for\nevery i, given access to statistical queries with respect to distribution (D, gi) outputs a hypothesis h such\nthat E(x,y)\u223c(D,gi)[h(x) \u0338= y] \u22641\n2 \u2212\n1\nm1/3 has complexity of at least m1/3.\n8\nWe can now prove Lemma 2.1.\nProof. By our assumption, the function f is \u03b1-close to a d-resilient bounded function g : {\u22121, 1}k \u2192\n[\u22121, 1]. We \ufb01rst note that each pair of functions gSi gSj shares at most d relevant variables. These functions\nare d-resilient and therefore there is no single set T such that c\ngSi(T) \u00b7 c\ngSj(T) \u0338= 0. This, by linearity of\nexpectation implies that for i \u0338= j, E[gSigSj] = 0.\nLet A be an agnostic algorithm for C with excess error of at most 1\u2212\u03b1\n2 \u2212m\u22121/3. For every i, fSi is \u03b1-close\nto gSi. Therefore if the input distribution is (U, gi) then OPTC(U, gi) \u2264\u2225fSi \u2212gSi\u22251/2 = \u2225f \u2212g\u22251/2 \u2264\n\u03b1/2. This implies that A will output a hypothesis h with error of at most \u03b1/2+ 1\u2212\u03b1\n2 \u2212m\u22121/3 = 1/2\u2212m\u22121/3.\nBy Theorem 2.2 and orthogonality of gSis we get that the complexity of A is at least m1/3.\nAn immediate corollary of Lemma 2.1 is the following lower bound that generalizes Theorem 1.1.\nTheorem 2.3. Let C be a concept class closed under renaming of variables and assume that C contains an\n\u03b1-approximately d-resilient k-junta. Then any SQ algorithm for agnostically learning C with excess error\nof at most 1\u2212\u03b1\n2\n\u2212m\u22121/3 has complexity of at least m1/3, where m = M(n, k, d). In particular, for any\nconstant \u03b4 > 0 and k = n1/2+\u03b4, we have m = n\u2126(d).\nTo show that Theorem 2.3 is essentially tight we prove the duality stated in Theorem 1.2 (which we\nrestate here for convenience).\nTheorem. [Thm. 1.2 restated] For f : {\u22121, 1}n \u2192{\u22121, 1} and 0 \u2264d \u2264n let \u03b1 denote the \u21131 distance of\nf to the closest d-resilient bounded function. Then \u2206Pd(f) = 1 \u2212\u03b1.\nProof. Our proof is an adaptation of the general results on duality of norms [IT68] to the case where f is\nBoolean and g is bounded. In this case it is easy to see that \u2225f \u2212g\u22251 = 1\u2212E[fg] and therefore minimization\nof distance to resilience can be expressed as maximization of P\nx f(x)g(x) subject to resilience constraints\non g. Viewing values of g(x) as variables we get:\nmax\nX\nx\nf(x)g(x)\nsubject to\nX\nx\ng(x)\u03c7S(x) = 0\n\u2200|S| \u2264d\nand |g(x)| \u22641\n\u2200x \u2208{\u22121, 1}n\nThe dual LP can be easily veri\ufb01ed to be the following program with variables pS for every S \u2286[n] of size\nat most d.\nmin\nX\nx\n|q(x)|\nsubject to q(x) = f(x) \u2212\nX\nS:|S|\u2264d\npS\u03c7S(x)\n\u2200x \u2208{\u22121, 1}n\nNow the claim of the theorem follows from LP duality. By de\ufb01nition the maximum value of the primal is\n2n \u00b7E[fg] = 2n(1\u2212\u2225f \u2212g\u22251) = 2n(1\u2212\u03b1). This is therefore also the minimum of the dual program which,\nby de\ufb01nition, is exactly 2n \u00b7 \u2206Pd(f).\nNote that (1 \u2212\u03b1)/2 in the excess error term in the statement of Theorem 2.3 is equal to \u2206Pd(C)/2 in\nthe excess error term in the statement Theorem 1.3. Therefore combining the duality with the upper-bounds\non polynomial \u21131 regression stated in Theorem 1.3 we get our claimed characterization of the complexity of\nagnostic learning in terms of \u2206Pd(C) or, alternatively, distance to d-resilience.\n9\n3\nMonotonicity and approximate resilience\nIn this section we prove bounds on the approximate resilience of monotone functions. First, we give a bound\nfor general monotone functions (Theorem 1.4) in Section 3.1. In Sections 3.2 and 3.3 we show that Tribes\nand CycleRun are approximately resilient (Theorems 1.6 and 1.7). Finally, in Section 3.4 we show how these\nfunctions can be used in an iterated construction to yield explicit functions with high approximate resilience.\n3.1\nA monotone function with nearly-optimal approximate resilience\nOur characterization suggests an approach for proving Theorem 1.4: since the \u21131-minimization algorithm\ncharacterizes SQ agnostic learning, we seek monotone functions where the \u21131-minimization algorithm will\nbadly fail. In other words, our \ufb01rst step will be to move to the dual problem: Theorem 1.2 tells us that we\nmay equivalently show the existence of a monotone function f which is far from from every low-degree\npolynomial p. Strangely, to show that no dual solution exists, we will use the fact that if every monotone\nfunction had a weak approximation by some low-degree polynomial, then the \u21131-minimization algorithm\nwould learn monotone functions, contradicting known information-theoretic lower bounds [BBL98]. Note\nthat while the \u21131-minimization algorithm is presented as an agnostic learning algorithm, we may apply it\ndirectly to the class of monotone functions.\nWe now prove Theorem 1.4:\nTheorem. For every \u03b1 > 0, there is a monotone function that is \u03b1-approximately d-resilient for d =\n\u2126(\u03b1\u221an/ log n).\nProof. We show the existence of a monotone function f such that E[|f(x)\u2212p(x)|] > 1\u2212\u03b1 for every degree-\nd polynomial p and then apply Theorem 1.2. Suppose that every monotone f satis\ufb01es E[|f(x) \u2212p(x)|] \u2264\n1 \u2212\u03b1. Then for \u03b5 = \u03b1/4, Theorem 1.3 gives an algorithm for learning monotone functions which uses\ns = poly(nd/\u03b1) examples and has error 1/2 \u2212\u03b1/2 + \u03b1/4 = 1/2 \u2212\u03b1/4. We now use an information-\ntheoretic lower bound on the number of random examples needed to weakly learn monotone functions; the\nproof in [BBL98] uses a randomized construction of DNF formulas:\nTheorem 3.1 ([BBL98]). Let A be a any learning algorithm that uses s random examples and outputs a\nhypothesis h. Then there is some monotone f : {\u22121, 1}n \u2192{\u22121, 1} such that\nPr[f(x) = h(x)] \u22641\n2 + O\n\u0012log sn\n\u221an\n\u0013\n.\nTheorem 3.1 tells us that \u03b1 = O\n\u0010\nd log n+log 1/\u03b1\n\u221an\n\u0011\n, which completes the proof.\nThe function from Theorem 1.4 gives us a k-junta that is \u03b1-approximately d-resilient for d = \u2126(\u03b1\n\u221a\nk/ log k).\nPlugging this into Theorem 2.3 and using eq.(1) (assuming k \u2264n1/2) we obtain the proof of Corollary 1.5.\nWhile the degree of resilience in Theorem 1.4 is nearly optimal, the proof is non-constructive and relies\ncrucially on the fact that monotone functions can have high complexity. In the following sections we show\nthat even simple, explicit monotone functions can exhibit high approximate resilience.\n3.2\nTribes is approximately resilient\nThe Tribesw,s : {\u22121, 1}sw \u2192{\u22121, 1} function is the disjunction of s disjoint monotone conjunctions, each\nof width w; i.e. a read-once width-w DNF. For notational brevity we write Tribes to denote Tribesw,s with\ns = (ln 2)2w (so w \u2248log n \u2212log ln n and s \u2248n/(log n)).\nOur construction of a highly resilient function close to Tribes is based on a general result relating the\nlow-degree Fourier weight of a Boolean function and its approximate resilience.\n10\nTheorem 3.2. There exists a universal K > 0 such that the following holds. Let f : {\u22121, 1}n \u2192{\u22121, 1}\nbe a Boolean function that satis\ufb01es P\n|S|\u2264d bf(S)2 \u2264\u03b3 for some d \u2208[n] and \u03b3 \u2208[0, 1]. Then for all\n\u03c4 > ed\u221a\u03b3, we have that f is O(\u03c4 + \u03b4n2d+2)-approximately d-resilient, where \u03b4 = exp\n\u0000\u2212K(\u03c4 2/\u03b3)1/d\u0001\n.\nWe now prove Theorem 3.2, and in Section 3.2.1 we show how Theorem 1.6 (i.e. the approximate\nresilience of Tribes) follows as a consequence of Theorem 3.2.\nWe begin our construction with the Fourier polynomial for f and discard the low-degree terms. That\nwe may do so and hope to arrive at a bounded, resilient function comes from hypercontractivity: since\nthe discarded polynomial has low-degree, it will by highly concentrated around its mean. The following\nChernoff-type concentration inequality for low-degree polynomials over independent Rademacher random\nvariables follows from the hypercontractivity inequalities of Bonami and Beckner [Bon70, Bec75] (see for\nexample [O\u2019D13]).\nTheorem 3.3 (concentration of degree-d polynomials). There exists a universal constant K > 0 such that\nfor every degree-d polynomial {\u22121, 1}n \u2192R and t > ed, we have\nPr\nx [|p(x)| \u2265t \u00b7 \u2225p\u22252] \u2264exp\n\u0010\n\u2212Kt2/d\u0011\n.\nWe now begin the proof of Theorem 3.2. Let\n\u2113(x) =\nX\n|S|\u2264d\nbf(S)\u03c7S(x),\nand\nh(x) = f(x) \u2212\u2113(x).\nOur \ufb01nal resilient, bounded function p will be based on h, the high-degree part of f. Note that while h is\nd-resilient by de\ufb01nition, it may not be uniformly bounded. However, the degree-d Chernoff bound applied\nto \u2113(the low-degree part), together with our assumption on the variance of \u2113(i.e. the low-degree Fourier\nweight of f), tell us that \u2113does not attain large values very often. Therefore, while h may not be uniformly\nbounded, we have that h is bounded on almost all inputs x since h(x) + \u2113(x) = f(x) \u2208{\u22121, 1}.\nMore formally, we set t = \u03c4/\u221a\u03b3 in Theorem 3.3 (since \u03c4 > ed\u221a\u03b3, we have that indeed t > ed)\nPr\nx [|\u2113(x)| \u2265\u03c4] \u2264exp\n\u0000\u2212K(\u03c4 2/\u03b3)1/d\u0001\n:= \u03b4.\nNext, we de\ufb01ne q : {\u22121, 1}n \u2192R to be such that\nq(x) =\n(\n0\nif |\u2113(x)| > \u03c4\nh(x)\nif |\u2113(x)| \u2264\u03c4.\nSince h(x) = f(x) \u2212\u2113(x) and f is {\u22121, 1}-valued, the range of q is [\u22121 \u2212\u03c4, 1 + \u03c4]. While q is bounded, it\nmay now have correlations with low-degree terms (i.e. q is no longer resilient like h is). However, we may\nalso write q as q(x) = h(x) \u2212h(x) \u00b7 1[\u2113>\u03c4](x), where h is d-resilient and 1[\u2113>\u03c4] has very small support.\nThus, we will show that we may discard the low-degree terms of q and the effect on boundedness will be\nuniformly small.\nLet q>d(x) = P\n|S|\u2265d+1 bq(S)\u03c7S(x), q\u2264d = q \u2212q>d and p(x) =\nq>d(x)\n\u2225q>d\u2225\u221e. Certainly, the range of p is\n[\u22121, 1]; it remains to bound the correlation of p with f. We have that:\nE[p \u00b7 f] = E\n\u0014(q \u2212q\u2264d)\n\u2225q>d\u2225\u221e\n\u00b7 f\n\u0015\n\u2265\n1\n\u2225q\u2225\u221e+ \u2225q\u2264d\u2225\u221e\n\u00b7 (E[q \u00b7 f] \u2212\u2225q\u2264d\u2225\u221e)\n(2)\n11\nThe correlation of f with q is large:\nE\nx[q(x) \u00b7 f(x)] \u2265(1 \u2212\u03c4)(1 \u2212\u03b4) \u22651 \u2212\u03c4 \u2212\u03b4.\n(3)\nThe above holds because the contribution to the correlation is 0 when q(x) = 0, which happens on at most a\n\u03b4 fraction of the inputs. On the remaining inputs, q(x) = h(x) = f(x) \u2212\u2113(x), and we assumed |\u2113(x)| \u2264\u03c4.\nThus the contribution on such x is\nq(x) \u00b7 f(x) = (f(x) \u2212\u2113(x)) \u00b7 f(x) = 1 \u2212\u2113(x) \u00b7 f(x) \u22651 \u2212|\u2113(x)| \u22651 \u2212\u03c4.\nThus, it only remains to bound the maximum value of the low-degree part of q:\nClaim 3.4.\n\u2225q\u2264d\u2225\u221e\u2264\u03b4n2d+2\nProof. We will show that |bq(S)| < \u03b4nd+1 holds for any |S| \u2264d. Recalling that q(x) = h(x)\u22121|\u2113|>\u03c4 \u00b7h(x),\nwe have:\nbq(S) = bh(S) \u2212\n\\\n1|\u2113|>\u03c4 \u00b7 h(S)\n|bq(S)| \u2264|bh(S)| + E[|1|\u2113|>\u03c4 \u00b7 h|]\n\u22640 + \u03b4 \u00b7 \u2225h\u2225\u221e\n\u2264\u03b4(\u2225\u2113\u2225\u221e+ 1),\nwhere the second inequality holds when |S| \u2264d because h is d-resilient, and the last inequality holds\nbecause |h(x)| \u2264|\u2113(x)| + 1 for all x. As f is a Boolean function, each of the non-zero Fourier coef\ufb01cients\nof \u2113is at most 1 in magnitude. The rough bound of nd+1 on the number of non-zero coef\ufb01cients of \u2113gives\na bound of nd+1 on \u2225\u2113\u2225\u221e; summing over at most nd+1 terms of degree at most d gives the claim.\nLet \u03ba = \u03b4n2d+2. Substituting into Equations (2) and (3), we have that\nE\nx[p(x) \u00b7 Tribes(x)] \u22651 \u2212\u03c4 \u2212\u03b4 \u2212\u03ba\n1 + \u03c4 + \u03ba\n\u22651 \u2212\u03b4 \u22122\u03c4 \u22122\u03ba,\nusing the fact that 1/(1 + x) \u22651 \u2212x for x \u22650, and this completes the proof of Theorem 3.2.\n3.2.1\nProof of Theorem 1.6\nTo apply Theorem 3.2 we will need the following upper bound on the low-degree Fourier weight of Tribes,\nwhose proof is given in Appendix B, can be obtained using the explicit values of each Fourier coef\ufb01cient\ngiven in [Man95],\nProposition 3.5. For any d \u2264w the Fourier weight of Tribes on degree d and below is at most\nX\n|S|\u2264d\n\\\nTribes(S)2 \u22642(2 ln n)2d+4\nn\n.\nTo derive Theorem 1.6 from Theorem 3.2, we set \u03c4 = (2 ln n)3dn\u22122/5, so that t := \u03c4/\u221a\u03b3 \u2265n1/10.\nNow there exists a small constant c > 0 such that for d = c log n/ log log n and large enough n, we\nhave that \u03c4 = O(n\u22121/3), t > ed and t2/d \u2265n1/(5d) \u2265\n3\nK (log n)2 \u2265\n(2d+3)\nK\nln n. This implies that\n\u03b4 := exp\n\u0000\u2212Kt2/d\u0001\n\u2264n\u22122d\u22123 and so \u03b4n2d+2 \u22641/n. We conclude that Tribes is \u03b1-approximately d-\nresilient where \u03b1 = O(\u03c4 + n\u22121) = O(n\u22121/3), and this completes the proof of Theorem 1.6.\n12\n3.3\nCycleRun is approximately resilient: Proof of Theorem 1.7\nDe\ufb01nition 3.6. For every n, the CycleRun Boolean function CycleRun : {\u22121, 1}n \u2192{\u22121, 1} is de\ufb01ned as\nfollows: Call a consecutive sequence of 1\u2019s a 1-run. Similarly, a consecutive sequence of \u22121\u2019s is a \u22121-run.\nWe allow runs to wrap around, so if a run reaches xn it may continue with x1. The value of CycleRun is the\nwinner (1 for 1-player or \u22121 for \u22121-player) from the following procedure:\n1. Check which player has the longest run.\n2. In case of tie check which player has a larger number of maximum-length runs.\n3. In case of tie check the total length of segments between maximum-length runs, where a segment start-\ning from a 1-run clockwise is counted for the 1-player and a segment starting at a \u22121-run clockwise\nis counted for the \u22121-player. The player that has a larger total count is declared the winner.\nWe will need that fact that CycleRun has in\ufb02uence O(log n). Since the proof of this fact has not appeared\nin the literature before, we include a proof in Appendix C.1 for completeness.\nTheorem 3.7. There exist universal constants c1, c2 such that for every n \u2265c2, there exists a Boolean\nfunction f : {\u22121, 1}n \u2192{\u22121, 1} such that:\n1. For all S \u2286[n] such that |S| \u22641, bf(S) = 0, and\n2. Ex[f(x) \u00b7 CycleRun(x)] \u22651 \u2212c1\np\n(log n)/n.\nOur proof of Theorem 3.7 relies on four key properties of CycleRun: monotonicity, low in\ufb02uence, odd-\nness, and invariance under cyclic shifts; as far as we know, CycleRun is the only explicit Boolean function\nknown to have all four properties. First, as CycleRun is monotone and transitive, we note that\n\\\nCycleRun({i}) =\n\\\nCycleRun({j}) = O\n\u0012log n\nn\n\u0013\nfor all i \u0338= j \u2208[n].\nThe high level intuition behind our proof is simple: we show that by \ufb02ipping the values of CycleRun from the\ntop of the hypercube downwards and bottom upwards simultaneously, we obtain a balanced function with\nno Fourier weight at the \ufb01rst level. This can be done without changing too many points because CycleRun\nhas small in\ufb02uence; we are able to do it in a controlled way because it is additionally odd and invariant\nunder cyclic shifts. We defer the proof of Theorem 3.7 to Appendix C.\nIt is natural to wonder how close a monotone function can be to a 1-resilient Boolean function. We show\nin Appendix C.2 that Theorem 3.7 is tight:\nTheorem 3.8. For every monotone function f : {\u22121, 1}n \u2192{\u22121, 1} and 1-resilient g : {\u22121, 1}n \u2192\n{\u22121, 1}, we have Prx[f(x) \u0338= g(x)] \u2265\u2126\n\u0012q\nlog n\nn\n\u0013\n.\n3.4\nResilience ampli\ufb01cation\nIn this section we prove a general ampli\ufb01cation lemma for resilience. Given a value t \u2208[\u22121, 1], we write\nb(t) to denote a random \u00b11 bit with expected value t:\nb(t) =\n\u001a\n1\nwith probability (1 + t)/2\n\u22121\nwith probability (1 \u2212t)/2.\n(In particular, b(1) is the constant 1 and b(\u22121) is the constant \u22121). Given bounded functions G : {\u22121, 1}m \u2192\n[\u22121, 1] and g : {\u22121, 1}n \u2192[\u22121, 1], we de\ufb01ne their (disjoint) composition G \u25e6g : {\u22121, 1}mn \u2192[\u22121, 1] to\n13\nbe (G \u25e6g)(x1, . . . , xm) := E[G(b(g(x1)), . . . , b(g(xm))]. Note that if E[g(x)] = 0, then E[b(g(x))] = 0\nas well. Throughout this section we write dist(f, g) to denote 1\n2E[|f(x)\u2212g(x)|] for notational brevity (this\nis simply the fractional Hamming distance Pr[f(x) \u0338= g(x)] when f and g are {\u00b11}-valued).\nThe main result in this section is the following ampli\ufb01cation lemma:\nTheorem 3.9. Let f : {\u22121, 1}n \u2192{\u22121, 1} and g : {\u22121, 1}n \u2192[\u22121, 1] where E[f(x)] = E[g(x)] = 0,\nand suppose g is d-resilient. Consider the recursively-de\ufb01ned functions where fk = f \u25e6fk\u22121 and gk =\ng \u25e6gk\u22121 for all k \u2208N, and f0 = f and g0 = g. Then for k \u22651:\n1. fk and gk are functions over nk+1 variables,\n2. gk is ((d + 1)k+1 \u22121)-resilient,\n3. dist(fk, gk) \u2264dist(f, g) Pk\nt=0 Inf[f]t.\nThe \ufb01rst claim is straightforward to verify, and so we focus on the second and third claims. For a\nBoolean-valued function F : {\u22121, 1}m \u2192{\u22121, 1} and \u03b4 \u2208[0, 1], recall that the noise-sensitivity of F\nat noise rate \u03b4 is de\ufb01ned as NS\u03b4[F] := Pry,z[F(y) \u0338= F(z)], where y is uniform in {\u22121, 1}m and z is\nobtained from y by independently \ufb02ipping each of its coordinates with probability \u03b4.\nLemma 3.10. Given F, f : {\u22121, 1}m \u2192{\u22121, 1} and G, g : {\u22121, 1}m \u2192[\u22121, 1] where E[f(x)] =\nE[g(x)] = 0, we have\ndist(F \u25e6f, G \u25e6g) \u2264dist(F, G) + NS\u03b4[F],\nwhere \u03b4 := dist(f, g).\nProof. We \ufb01rst apply the triangle inequality and note that\ndist(F \u25e6f, G \u25e6g) \u2264dist(F \u25e6f, F \u25e6g) + dist(F \u25e6g, G \u25e6g).\nSince E[g(x)] = 0, we have that \u27e8b(g(x1)), . . . , b(g(xm))\u27e9is uniformly distributed on {\u22121, 1}m when\nx1, . . . , xm are independently and uniformly distributed on {\u22121, 1}n, and therefore the second distance\non the right hand side is exactly dist(F, G). Since Pr[b(f(x)) \u0338= b(g(x))] = Pr[f(x) \u0338= b(g(x))] =\n1\n2|f(x) \u2212g(x)| for all x \u2208{\u22121, 1}n, it follows that Pr[b(f(x)) \u0338= b(g(x))] = 1\n2E[|f(x) \u2212g(x)|] = \u03b4 and\nso\ndist(F \u25e6f, F \u25e6g) = Pr\ny,z[F(y) \u0338= F(z)],\nwhere y is uniform in {\u22121, 1}m and z is obtained from y by independently \ufb02ipping each of its coordinates\nwith probability \u03b4. This completes the proof, since the probability on the right hand side is precisely NS\u03b4[F].\nUsing the union bound, we have\nNS\u03b4[F] \u2264\u03b4\nn\nX\ni=1\nPr\nx [F(x) \u0338= F(x\u2295i)] = \u03b4 \u00b7 Inf[F] = dist(f, g) \u00b7 Inf[F],\nwhere x\u2295i is the string x with the i-th bit \ufb02ipped, and \u03b4 = dist(f, g) as in the previous lemma. This, along\nwith a straightforward recursion, yields the following corollary.\nCorollary 3.11. Let f : {\u22121, 1}n \u2192{\u22121, 1} and g : {\u22121, 1}n \u2192[\u22121, 1] where E[f(x)] = E[g(x)] = 0,\nand suppose g is d-resilient. Consider the recursively-de\ufb01ned functions where fk = f \u25e6fk\u22121 and gk =\ng \u25e6gk\u22121 for all k \u2208N, and f0 = f and g0 = g. Then for k \u22651:\ndist(fk, gk) \u2264dist(f, g)\nk\nX\nt=0\nInf[f]t.\n14\nLemma 3.12. If G : {\u22121, 1}m \u2192[\u22121, 1] is d1-resilient and g : {\u22121, 1}n \u2192[\u22121, 1] is d2-resilient, then\nG \u25e6g is (d1d2)-resilient.\nProof. By linearity of the Fourier transform it suf\ufb01ces to prove this claim when G(x1, . . . , xm) = Q\ni\u2208T xi\nand |T| > d1, the parity function over d1 + 1 or more variables. We begin by noting that\n(G \u25e6g)(x1, . . . , xm)\n=\nE\n\" Y\ni\u2208T\nb(g(xi))\n#\n=\nY\ni\u2208T\nE[b(g(xi))]\n=\nY\ni\u2208T\n\u00141 + g(xi)\n2\n\u22121 \u2212g(xi)\n2\n\u0015\n=\nY\ni\u2208T\ng(xi).\nWe view the mn coordinates of the composed function G \u25e6g as the disjoint union of A1 \u222a\u00b7 \u00b7 \u00b7 \u222aAm, where\neach Ai has size n. With this notation in hand, every subset S of the mn coordinates may be viewed as the\ndisjoint union S1 \u222a\u00b7 \u00b7 \u00b7 \u222aSm, where Aj \u2286Sj for all j \u2208[m]. Fix S = S1 \u222a\u00b7 \u00b7 \u00b7 \u222aSm of cardinality at\nmost d1d2, and recall that our goal is to show that \\\n(G \u25e6g)(S) = 0. There exists at least one set Sj where\n|Sj| \u2264d2, and we assume without loss of generality that |S1| \u2264d2. Since g is d2-resilient (in particular,\nbg(S1) = 0), we see that indeed\n\\\n(G \u25e6g)(S) = E\n\" Y\ni\u2208T\ng(xi)\nY\nj\u2208[m]\nY\n\u2113\u2208Sj\nxj\n\u2113\n#\n=\nY\ni\u2208T\nbg(Si)\nY\nj /\u2208T\nY\n\u2113\u2208Sj\nE[xj\n\u2113] = 0,\nand the proof is complete.\nCombining Corollary 3.11 and Lemma 3.12 yields Theorem 3.9.\n3.4.1\nAmplifying Tribes and CycleRun\nWe now apply Theorem 3.9 to Tribes and CycleRun.\nTheorem 3.13. There is an explicit \u03b1-approximately d-resilient monotone Boolean function F where \u03b1 =\non(1) and d = 2\u2126(\u221alog n).\nProof. We apply Theorem 3.9 with f being Tribes and g the bounded resilient function that results from ap-\nplying Theorem 1.6. Since Inf[Tribes] = \u0398(log n) (see e.g. [KKL88]), taking k := c log n/ log log n where\nc > 0 is a suf\ufb01ciently small universal constant gives functions fk, gk over N := nk = 2O(log2 n/ log log n)\nvariables, where\ndist(fk, gk) = O(Inf[Tribes]k+1 \u00b7 n\u22121/3) = n\u2212\u2126(1) = oN(1),\nand gk is d-resilient for\nd = \u2126((log n/ log log n)k+1) = 2\u2126(\u221alog N).\nAnalogous calculations for CycleRun yield the following:\nTheorem 1.7. There is an explicit \u03b1-approximately d-resilient monotone Boolean function F where \u03b1 =\non(1) and d = 2\u2126(\u221alog n/ log log n). Furthermore, F is \u03b1-close to a d-resilient function that is Boolean-valued\nas well.\n15\nProof. We apply Theorem 3.9 with f being CycleRun and g the Boolean-valued resilient function that\nresults from applying Theorem 3.7. Since Inf[CycleRun] = O(log n) (Theorem C.6), we again take k =\nc log n/ log log n where c > 0 is a suf\ufb01ciently small universal constant to get Boolean-valued functions\nfk, gk over N = 2O(log2 n/ log log n) variables, where Pr[fk(x) \u0338= gk(x)] = dist(fk, gk) = n\u2212\u2126(1) = oN(1),\nand gk is d-resilient for d = n\u2126(1/ log log n) = 2O(\n\u221alog N/ log log N).\n4\nConclusions\nWe have demonstrated that complexity of agnostic learning over product distributions has a natural charac-\nterization via either of two dual notions: \u21131-approximation by polynomials and approximate resilience. The\nnotion of distance to resilience that we introduce appears to be interesting its own right. It is also better\nsuited for proving lower bounds since a single close resilient function witnesses the hardness of agnostic\nlearning. Our proof of this result is relatively simple and remarkably, up to the choice of norms, is identical\nto Sherstov\u2019s powerful pattern matrix method in communication complexity [She11].\nAn application of our characterization and our second contribution is new and detailed picture of the\nhardness of agnostic learning of monotone functions over the uniform distribution. Some evidence that\nagnostic learning of several monotone classes is hard is already known and relies on cryptographic assump-\ntions [KKMS08, FGKP09, KS09]. Yet the existing evidence is restricted to the very hard regime when OPT\nis near 1/2 and does exclude learning with excess error of just 1% that would suf\ufb01ce for most practical\napplications. We give the \ufb01rst general lower bounds for monotone functions that establish hardness in the\nlow-error regime. We also describe simple and explicit monotone functions that are very close to being\nresilient.\nFinally, we give general tools for analysis of approximate resilience. Such tools might \ufb01nd use for\nproving new agnostic learning lower bounds.\nAcknowledgements\nTheorem 1.2 and a special case of Theorem 1.1 for symmetric functions were \ufb01rst derived in V.F.\u2019s collab-\noration with Pravesh Kothari. We thank Pravesh for his permission to include the result in this work. We\nthank Justin Thaler for his help in deriving Theorem 1.2 and illuminating discussions on the relationship of\nour characterization of agnostic learning to the pattern matrix method of Sherstov [She11]. We thank Udi\nWieder and Yuval Peres for helpful information about the CycleRun function, and Ryan O\u2019Donnell, Johan\nH\u02daastad, Rocco Servedio and Jan Vondr\u00b4ak for helpful conversations.\nReferences\n[AH11]\nPer Austrin and Johan H\u02daastad. Randomly supported independence and resistance. SIAM Jour-\nnal on Computing, 40(1):1\u201327, 2011. 1.1\n[AM09]\nPer Austrin and Elchanan Mossel. Approximation resistant predicates from pairwise indepen-\ndence. Computational Complexity, 18(2):249\u2013271, 2009. 1.1\n[BBL98]\nA. Blum, C. Burch, and J. Langford. On learning monotone boolean functions. In Proceedings\nof FOCS, pages 408\u2013415, 1998. 1.2, 1.2, 1.2, 3.1, 3.1\n[BDLSS12] Shai Ben-David, David Loker, Nathan Srebro, and Karthik Sridharan. Minimizing the misclas-\nsi\ufb01cation error rate using a surrogate convex loss. In ICML, 2012. 1.3\n16\n[Bec75]\nWilliam Beckner. Inequalities in Fourier analysis. Ann. of Math. (2), 102(1):159\u2013182, 1975.\n1.2, 3.2\n[BFJ+94]\nAvrim Blum, Merrick Furst, Jeffrey Jackson, Michael Kearns, Yishay Mansour, and Steven\nRudich. Weakly learning dnf and characterizing statistical query learning using fourier analy-\nsis. In Proceedings of the twenty-sixth annual ACM symposium on Theory of computing, pages\n253\u2013262. ACM, 1994. 1.3, 2\n[Bon70]\nAline Bonami. \u00b4Etude des coef\ufb01cients de Fourier des fonctions de Lp(G). Ann. Inst. Fourier\n(Grenoble), 20(fasc. 2):335\u2013402 (1971), 1970. 1.2, 3.2\n[Bro]\nDaniel G. Brown. How I wasted too long \ufb01nding a concentration inequality for sums of geo-\nmetric variables. C.1\n[BT96]\nN. Bshouty and C. Tamon. On the Fourier spectrum of monotone functions. Journal of the\nACM, 43(4):747\u2013770, 1996. 1.2, 1.2, 1.2\n[CGH+85]\nBenny Chor, Oded Goldreich, Johan Hasted, Joel Freidmann, Steven Rudich, and Roman\nSmolensky. The bit extraction problem or t-resilient functions. In Foundations of Computer\nScience, 1985., 26th Annual Symposium on, pages 396\u2013407. IEEE, 1985. 1.1\n[DLSS14]\nAmit Daniely, Nati Linial, and Shai Shalev-Shwartz. The complexity of learning halfspaces\nusing generalized linear methods. In COLT, pages 244\u2013286, 2014. 1.3\n[Fel68]\nW. Feller. An introduction to probability theory and its applications. John Wiley & Sons, 1968.\nC\n[Fel12]\nV. Feldman.\nA complete characterization of statistical query learning with applications to\nevolvability. Journal of Computer System Sciences, 78(5):1444\u20131459, 2012. 1.3, 2\n[FGKP09]\nV. Feldman, P. Gopalan, S. Khot, and A. Ponuswami. On agnostic learning of parities, mono-\nmials and halfspaces. SIAM Journal on Computing, 39(2):606\u2013645, 2009. 4\n[FGRW12] Vitaly Feldman, Venkatesan Guruswami, Prasad Raghavendra, and Yi Wu. Agnostic learning\nof monomials by halfspaces is hard. SIAM J. Comput., 41(6):1558\u20131590, 2012. 1\n[FK14]\nV. Feldman and P. Kothari. Agnostic learning of disjunctions on symmetric distributions. arXiv,\nCoRR, abs/1405.6791, 2014. 1.1, 1.3\n[FLS11]\nV. Feldman, H. Lee, and R. Servedio. Lower bounds and hardness ampli\ufb01cation for learning\nshallow monotone formulas. In Journal of Machine Learning Research - COLT Proceedings,\nvolume 19, pages 273\u2013292, 2011. 1.3\n[FPV13]\nVitaly Feldman, Will Perkins, and Santosh Vempala. On the complexity of random satis\ufb01ability\nproblems with planted solutions. CoRR, abs/1311.4821, 2013. 2\n[Hau92]\nD. Haussler. Decision theoretic generalizations of the PAC model for neural net and other\nlearning applications. Information and Computation, 100(1):78\u2013150, 1992. 1\n[IT68]\nAleksandr Ioffe and Vladimir Tikhomirov. Duality of convex functions and extremum prob-\nlems. Russ. Math. Surv., 23, 1968. 1.1, 2\n[Kea98]\nM. Kearns. Ef\ufb01cient noise-tolerant learning from statistical queries. Journal of the ACM,\n45(6):983\u20131006, 1998. 1.1, 1.3\n17\n[KKL88]\nJ. Kahn, G. Kalai, and N. Linial. The in\ufb02uence of variables on Boolean functions. In Proceed-\nings of FOCS, pages 68\u201380, 1988. 3.4.1, C.2\n[KKMS08] A. Kalai, A. Klivans, Y. Mansour, and R. Servedio. Agnostically learning halfspaces. SIAM\nJournal on Computing, 37(6):1777\u20131805, 2008. 1, 1.1, 1.3, 1.2, 1.2, 1.3, 4, A, A.1\n[KS07]\nAdam R Klivans and Alexander A Sherstov. Unconditional lower bounds for learning inter-\nsections of halfspaces. Machine Learning, 69(2-3):97\u2013114, 2007. 1.3\n[KS09]\nAdam R. Klivans and Alexander A. Sherstov. Cryptographic hardness for learning intersections\nof halfspaces. J. Comput. Syst. Sci., 75(1):2\u201312, 2009. 4\n[KS10]\nAdam R. Klivans and Alexander A. Sherstov. Lower bounds for agnostic learning via approx-\nimate rank. Computational Complexity, 19(4):581\u2013604, 2010. 1, 1.3\n[KSS94]\nM. Kearns, R. Schapire, and L. Sellie. Toward Ef\ufb01cient Agnostic Learning. Machine Learning,\n17(2/3):115\u2013141, 1994. 1\n[Lov87]\nL\u00b4aszl\u00b4o Lov\u00b4asz. An algorithmic theory of numbers, graphs and convexity, volume 50. SIAM,\n1987. 2\n[LS11]\nPhilip M. Long and Rocco A. Servedio. Learning large-margin halfspaces with more malicious\nnoise. In NIPS, pages 91\u201399, 2011. 1.3\n[LW95]\nMichael Luby and Avi Wigderson. Pairwise independence and derandomization. Citeseer,\n1995. 1.1\n[Man95]\nY. Mansour. An O(nlog log n) learning algorithm for DNF under the uniform distribution. Jour-\nnal of Computer and System Sciences, 50:543\u2013550, 1995. 3.2.1, B\n[MO02]\nElchanan Mossel and Ryan O\u2019Donnell. On the noise sensitivity of monotone functions. In\nMathematics and Computer Science II, pages 481\u2013495. Springer, 2002. 1.2\n[MOS04]\nE. Mossel, R. O\u2019Donnell, and R. Servedio. Learning functions of k relevant variables. Journal\nof Computer & System Sciences, 69(3):421\u2013434, 2004. Previously published as \u201cLearning\njuntas\u201d. 1.2, 1.2\n[O\u2019D03]\nR. O\u2019Donnell. Computational Applications of Noise Sensitivity. PhD thesis, 2003. 1.2, B\n[O\u2019D13]\nRyan O\u2019Donnell. Analysis of boolean functions. http://analysisofbooleanfunctions.org,\n2013. 3.2\n[OS07]\nR. O\u2019Donnell and R. Servedio. Learning monotone decision trees in polynomial time. SIAM\nJ. Comput., 37(3):827\u2013844, 2007. 1.2\n[OW13]\nRyan O\u2019Donnell and Karl Wimmer. Kkl, kruskal-katona, and monotone nets. SIAM J. Comput.,\n42(6):2375\u20132399, 2013. 1.2, C\n[OW14]\nRyan O\u2019Donnell and David Witmer. Goldreich\u2019s prg: Evidence for near-optimal polynomial\nstretch. In Conference on Computational Complexity, 2014. 1.1, 1.2\n[Sch90]\nMark F Schilling. The longest run of heads. College Math. J, 21(3):196\u2013207, 1990. C.1\n[Ser01]\nR. Servedio. On learning monotone DNF under product distributions. In Proceedings of the\nFourteenth Annual Conference on Computational Learning Theory, pages 558\u2013573, 2001. 1.2\n18\n[She11]\nAlexander A. Sherstov. The pattern matrix method. SIAM J. Comput., 40(6):1969\u20132000, 2011.\n1.1, 1.3, 4, 4\n[Sie84]\nThomas Siegenthaler.\nCorrelation-immunity of nonlinear combining functions for crypto-\ngraphic applications. IEEE Transactions on Information Theory, 30(5):776\u2013780, 1984. 1.2\n[Sim07]\nH. Simon. A characterization of strong learnability in the statistical query model. In Pro-\nceedings of Symposium on Theoretical Aspects of Computer Science, pages 393\u2013404, 2007.\n1.3\n[Sz\u00a8o09]\nBal\u00b4azs Sz\u00a8or\u00b4enyi. Characterizing statistical query learning: simpli\ufb01ed notions and proofs. In\nAlgorithmic Learning Theory, pages 186\u2013200. Springer, 2009. 1.3, 2\n[Tal93]\nM. Talagrand. Isoperimetry, logarithmic Sobolev inequalities on the discrete cube and Mar-\ngulis\u2019 graph connectivity theorem. GAFA, 3(3):298\u2013314, 1993. C.2\n[Tal96]\nM. Talagrand. How much are increasing sets positively correlated? Combinatorica, 16(2):243\u2013\n258, 1996. 1.2\n[Val12]\nGregory Valiant. Finding correlations in subquadratic time, with applications to learning pari-\nties and juntas. In Foundations of Computer Science (FOCS), 2012 IEEE 53rd Annual Sympo-\nsium on, pages 11\u201320. IEEE, 2012. 1.2, 1.3\n[Wie]\nUdi Wieder. Tennis for the people ii. http://windowsontheory.org/2012/11/16/tennis-for-t\n1.2\n[Yan05]\nKe Yang. New lower bounds for statistical query learning. Journal of Computer and System\nSciences, 70(4):485\u2013509, 2005. 2\nA\nExtension to Product Distributions\nWe now outline the extension of our characterization of the SQ complexity of agnostic learning to more\ngeneral product distributions. Let X be the domain of each individual variable, that is our leaning problem\nis de\ufb01ned over Xn. We will start with symmetric product distributions and let \u03a0 be a distribution over\nX. Let B = {B0(x), B1(x), . . .} be the basis obtained via Gram-Schmidt orthonormalization on the basis\n1, x, x2, . . . with respect to the inner product \u27e8f, g\u27e9\u03a0 = E\u03a0[f(x)g(x)]. By de\ufb01nition we obtain that the\npolynomial degree of Bi is i (for i \u2264|X| \u22121). As special cases this process gives {1, 1\u2212\u00b5\u00b7x\n\u221a\n1\u2212\u00b52 } basis if\nX = {\u22121, 1} and \u00b5 = E\u03a0[x]; Legendre polynomials when X = [\u22121, 1] and \u03a0 is uniform; and Hermite\npolynomials when X = R and \u03a0 is the Gaussian N(1, 0) distribution.\nFor S \u2286[n] and a function t : S \u2192N let \u03a6S,t(x) = \u03a0i\u2208Sxt(i)\ni\nand \u03a8S,t(x) = \u03a0i\u2208SBt(i)(xi). For a\n\ufb01nite X we restrict the range of such t\u2019s to [|X| \u22121]. Clearly, \u03a8\u2019s are orthonormal functions relative to the\ninner product \u27e8f, g\u27e9\u03a0n = E\u03a0[f(x)g(x)].\nWe now say that a function g is d-resilient relative to \u03a0n if for every S \u2286[n] of size at most d and any\nfunction t : S \u2192N, \u27e8g, \u03a8S,t\u27e9\u03a0n = 0. Note that equivalently this can be de\ufb01ned as \u27e8g, \u03a6S,t\u27e9\u03a0n = 0 for all\nS \u2286[n] of size at most d and t : S \u2192N.\nWe say that a Boolean f is \u03b1-approximately d-resilient relative to \u03a0n if there exists a d-resilient g :\nXn \u2192[\u22121, 1] such that E\u03a0n[|f(x) \u2212g(x)|] \u2264\u03b1. In the following discussion functions are over Xn and\nall norms and inner products relative to \u03a0n.\nWe now describe generalizations of Theorems 1.3, 1.2 and 2.3. Let Pd,\u2113denote the class of polynomials\nwhere each monomial has at most d different variables each of degree at most \u2113; let Pd = Pd,\u221e. Note\n19\nthat by de\ufb01nition this is the span of {\u03a6S,t}|S|\u2264d,t:S\u2192[\u2113] but is also equal to the span of {\u03a8S,t}|S|\u2264d,t:S\u2192[\u2113].\nFor a function f, let \u2206Pd,\u2113(f) = minp\u2208Pd,\u2113E\u03a0n[|f(x) \u2212p(x)|] and for a concept class C, let \u2206Pd,\u2113(C) =\nmaxf\u2208C \u2206Pd,\u2113(f).\nThe polynomial \u21131 regression algorithm of Kalai et al. for agnostic learning [KKMS08] applies to this\ngeneral setting and gives the following bound.\nTheorem A.1 ([KKMS08]). Let C be a concept class over Xn and \ufb01x d and \u2113. There exists a SQ algorithm\nwhich for any \u03b5 > 0 agnostically learns C over \u03a0n with excess error \u2206Pd,\u2113(C)/2 + \u03b5 and has complexity\npoly((n\u2113)d, 1/\u03b5).\nOur SQ lower bound can be easily seen to generalize to the following statement.\nTheorem A.2. Let C be a concept class over Xn closed under renaming of variables and assume that C\ncontains a k-junta which is \u03b1-approximately d-resilient over \u03a0n. Then any SQ algorithm for agnostically\nlearning C over \u03a0n with excess error of at most 1\u2212\u03b1\n2\n\u2212m\u22121/3 has complexity of at least m1/3, where\nm = M(n, k, d). In particular, for any constant \u03b4 > 0 and k = n1/2+\u03b4, we have m = n\u2126(d).\nFinally, the duality is also easy to verify in this case.\nTheorem A.3. For f : Xn \u2192{\u22121, 1} and 0 \u2264d \u2264n let \u03b1 denote the \u21131 distance of f to the closest\nd-resilient bounded function. Then \u2206Pd(f) = 1 \u2212\u03b1.\nNow the upper bound is (n\u2113)O(d) with excess error \u2206Pd,\u2113(C)/2 and the lower bound is n\u2126(d) with ex-\ncess error of \u2206Pd(C)/2 (if k is not too large). Therefore tightness depends on how fast \u2206Pd,\u2113(C) approaches\n\u2206Pd(C) as \u2113grows. Note that if C contains only functions that depend on at most k-variables then conver-\ngence of \u2206Pd,\u2113(C) to \u2206Pd(C) depends only on k (and not on n) and also as long as \u2113= nO(1) the bounds\nare still within a polynomial factor.\nNon-symmetric product distributions. Now let the domain be X1 \u00d7X2 \u00d7\u00b7 \u00b7 \u00b7 \u00d7Xn and the product distri-\nbution be \u03a0 = \u03a01 \u00d7\u03a02 \u00d7\u00b7 \u00b7 \u00b7\u00d7\u03a0n. We \ufb01rst note that the upper bound in Thm. A.1 and the duality hold even\nif the distribution is not symmetric (that is different variables might have different marginal distributions).\nTherefore we only need to adapt Thm. A.2 to this setting.\nOur lower-bound construction requires closed-ness with respect to renaming of variables. That would\nnot suf\ufb01ce if different variables have different marginal distributions. For example \u21131 distance to polynomi-\nals clearly depends on the marginal distributions of variables and therefore we can no longer claim that the\nanalogue of \u2225fSi \u2212gSi\u22251 = \u2225f \u2212g\u22251 holds in this setting (as we did in the proof of Lemma 2.1). Therefore\nwe will need an additional assumption. Let S be the set of variables of the optimal (in terms of distance to\nd-resilience) k-junta. We will assume that for every variable i \u2208S, there are many other variables that have\nthe same marginal distribution as variable i. Speci\ufb01cally, there exists a set Ii \u2286[n], such that for j1, j2 \u2208Ii,\n\u03a0j1 = \u03a0j2 and the size of Ii is at least s. In addition, we need C to be closed under renaming of variables,\nwhere a variable that is in Ii is renamed to another variable in Ii.\nNow we can construct a family of ordered sets S1, . . . , Sm (each of size k) such that the intersection of\nany two sets is at most d, and the i\u2019th element of each set Sj (recall that we think of Sj as an ordered set) is\nfrom Ii. This means that X and \u03a0 restricted to variables in Sj (ordered in the same way as they are in Sj)\nare exactly the same as X and \u03a0 restricted to variables in S. This means that the proof of the lower bound\nin Lemma 2.1 applies to this setting, as before essentially verbatim. The complexity is now determined by\nthe size of the largest family of sets with the property we described. By the same argument as in eq.(1) there\nexists a family of size:\nsk\n\u0000k\nd\n\u0001\nsk\u2212d = \u2126\n \u0012sd\nk\n\u0013d!\n.\nThis family has size n\u2126(d) for s = n\u2126(1) and a large range of parameters k and d (e.g. d = k1\u2212\u2126(1)).\n20\nB\nBound on the low-degree Fourier weight of Tribes\nThe Tribesw,s : {\u22121, 1}sw \u2192{\u22121, 1} function is the disjunction of s disjoint conjunctions, each of width\nw. For a set T \u2286[n] let Ti denote the intersection of T with the variables in the i-th conjunction. We use\nthe following expressions proved in [Man95]:\n\\\nTribesw,s(T) =\n(\n2(1 \u22122\u2212w)s \u22121\nT = \u2205\n2(\u22121)k+|T|2\u2212kw(1 \u22122\u2212w)s\u2212k\nk = #{i : Ti \u0338= \u2205} > 0\n(4)\nRecall that we write Tribes to denote Tribesw,s with s = (ln 2)2w; thus w \u2248log n \u2212log n ln n and\ns \u2248n/(log n).\nProposition B.1. For any d \u2264w the Fourier weight of Tribes on degree d and below is at most\nX\n|S|\u2264d\n\\\nTribes(S)2 \u22642(2 ln n)2d+4\nn\n.\nProof. The proof follows Ryan O\u2019Donnell\u2019s thesis, pages 66 \u221267 [O\u2019D03]. Using the calculations above,\nwe have that for any T \u2286[n] with k = #{i : Ti \u0338= \u2205} :\n\\\nTribes(T)2 \u2264\n\u00122 ln n\nn\n\u00132k\n.\nFor any k, the number of coef\ufb01cients that have degree at most d and intersect k conjunctions is at most\nd\nX\nj=0\n\u0012s\nk\n\u0013\u0012kw\nj\n\u0013\n\u2264(d + 1)sk(kw + 1)d \u2264nkw2d+2.\nThe last inequality holds because s \u2264n and k \u2264d (and we assume that d \u2264w). Summing over 1 \u2264k \u2264d,\nwe obtain:\nX\n|T|\u2264d\n\\\nTribes(T)2 \u2264\nd\nX\nk=1\nnkw2d+2\n\u00122 ln n\nn\n\u00132k\n\u2264w2d+2\nd\nX\nk=1\n\u0012(2 ln n)2\nn\n\u0013k\n\u22642w2d+2 (2 ln n)2\nn\n\u22642(2 ln n)2d+4\nn\n,\nwhere we used w \u22642 ln n in the last step.\nC\nProofs concerning CycleRun\nTo aid us in proving properties of CycleRun, we will require several bounds involving Gaussian approxima-\ntions. Speci\ufb01cally, we will make use of the functions ft : {\u22121, 1}n \u2192{\u22121, 0, 1} that appear in [OW13].\nWe de\ufb01ne |x| = Pn\ni=1 xi for a string x \u2208{\u22121, 1}n. These functions ft are de\ufb01ned so that\n21\nft(x) =\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\n1\nif |x| > t\u221an\n0\nif \u2212t\u221an \u2264|x| \u2264t\u221an\n\u22121\nif |x| < \u2212t\u221an\nWe use three properties (implicitly) appearing in [OW13] that follow from error estimates for the Central\nLimit Theorem [Fel68]: for large enough n and \u221alog n/100 < t < n1/10, we have\n\u03c6(t)\u221an/3 \u2264Inf(ft) \u22643\u03c6(t)\u221an\n(5)\n\u03c6(t)/(3t) \u2264Prx[ft(x) \u0338= 0] \u22643\u03c6(t)/t\n(6)\nPr[|x| = t] \u22644\u03c6(t)/\u221an.\n(7)\nwhere \u03c6 is the probability density function of the standard Gaussian distribution: \u03c6(u) =\n1\n2\u03c0 exp(\u2212u2/2);\nand Inf(ft) = Ex[ft(x) \u00b7 |x|] = P\ni\u2208[n] bft({i}). We note that Inf(g) = Ex[g(x) \u00b7 |x|] = P\ni\u2208[n] bg({i}) for\na monotone Boolean function g : {\u22121, 1}n \u2192{\u22121, 1}.\nDe\ufb01nition C.1. For every x \u2208{\u22121, 1}n, de\ufb01ne the set Shiftx to contain the following:\n\u2022 x\u03b1 = x(1+\u03b1 mod n) . . . x(n+\u03b1 mod n), for 0 \u2264\u03b1 \u2264n \u22121.\n\u2022 \u2212x\u03b1 = \u2212x(1+\u03b1 mod n) . . . \u2212x(n+\u03b1 mod n), for 0 \u2264\u03b1 \u2264n \u22121.\nNote that |Shiftx| always divides 2n, and if the Hamming weight of x is relatively prime to n, then\n|Shiftx| = 2n. Because CycleRun is odd and invariant under cyclic shifts, CycleRun is 1 on exactly half the\npoints of Shiftx.\nTheorem 3.7. There exist universal constants c1, c2 such that for every n \u2265c2, there exists a Boolean\nfunction f : {\u22121, 1}n \u2192{\u22121, 1} such that:\n1. For all S \u2286[n] such that |S| \u22641, bf(S) = 0, and\n2. Ex[f(x)\u00b7CycleRun(x)] \u22651\u22122c1\u00b7\nq\nlog(n)\nn\n, which implies Prx[f(x) \u0338= CycleRun(x)] \u2264c1\u00b7\nq\nlog(n)\nn\n.\nProof. Given CycleRun : {\u22121, 1}n \u2192{\u22121, 1}, we construct a set S \u2286{\u22121, 1}n using the greedy algo-\nrithm ConstS(CycleRun, n) described in Figure 1.\nGiven the set S outputted by ConstS(CycleRun, n), the function f : {\u22121, 1}n \u2192{\u22121, 1} is de\ufb01ned in\nthe following way:\nf(x) =\n(\nCycleRun(x)\nif x /\u2208S\n\u2212CycleRun(x)\nif x \u2208S.\nClearly, Ex[f(x) \u00b7 CycleRun(x)] \u22651 \u22122c1 \u00b7\nq\nlog(n)\nn\n, since the set S satis\ufb01es |S| \u2264c1 \u00b7\nq\nlog(n)\nn\n\u00b7 2n.\nAdditionally, f is clearly balanced due to the structure of the set Shiftx of modi\ufb01ed points in each iteration\nof ConstS and the fact that CycleRun is odd. Thus, it remains to show that bf(S) = 0 for all S \u2286[n] such\nthat |S| \u22641.\n22\nConstS(CycleRun, n)\n1. Initialize S = \u2205, S\n\u2032 = \u2205.\n2. Initialize \u03c3 = 2n \u00b7 P\ni\u2208[n]\n\\\nCycleRun({i}).\n3. While |S| \u2264c1 \u00b7\nq\nlog(n)\nn\n\u00b7 2n, do the following:\n3a. Find some x with maximal value of |x| such that CycleRun(x) = 1 and such that x /\u2208S.\n3b. If \u03c3 \u22122|Shiftx| \u00b7 |x| < 0, then \ufb01nd an x\u2217/\u2208S such that |x\u2217| = 1 and CycleRun(x\u2217) = 1 (if no\nsuch x\u2217exists, exit loop and output \u201cFail.\u201d). Then set S := S \u222aShiftx\u2217, set S\n\u2032 = S\n\u2032 \u222aShiftx\u2217,\nand set \u03c3 := \u03c3 \u22124n. If \u03c3 = 0, exit the loop.\n3c. If \u03c3 \u22122|Shiftx| \u00b7 |x| > 0, set S := S \u222aShiftx and set \u03c3 := \u03c3 \u22122|Shiftx| \u00b7 |x|.\n4. Return S.\nFigure 1: Algorithm for constructing a set of points S used to de\ufb01ne the 1-resilient function f.\nClaim C.2. Consider an execution of ConstS. At the end of the i-th iteration, 1 \u2264i \u2264c1 \u00b7\nq\nlog(n)\nn\n\u00b7 2n, if\nConstS has not terminated, let S\ni denote the current set of points in S, let \u03c3i denote the current setting of\nthe variable \u03c3 and let f i denote the following Boolean function:\nf i(x) =\n(\nCycleRun(x)\nif x /\u2208S\ni\n\u2212CycleRun(x)\nif x \u2208S\ni.\nAdditionally, we de\ufb01ne S\n0 = \u2205, \u03c30 = 2n \u00b7 P\ni\u2208[n]\n\\\nCycleRun({i}), and f 0 = CycleRun.\nFor every 0 \u2264i \u2264c1 \u00b7 log(n)\n2n\u221an \u00b7 2n the following invariants hold:\n1. bf i({1}) = bf i({2}) = \u00b7 \u00b7 \u00b7 = bf i({n}).\n2. \u03c3i = 2n \u00b7 P\nj\u2208[n] bf i({j}).\n3. \u03c3i = 4nw \u22650, for some integer w.\nProof. Proof by induction.\nBase Case: The base case follows trivially from the de\ufb01nition of CycleRun and the de\ufb01nition of S\n0, \u03c30,\nf 0.\nInductive Case: Assume the invariants hold for all 0 \u2264j \u2264i < c1 \u00b7\nq\nlog(n)\nn\n\u00b7 2n, we show that the\ninvariants must also hold for i + 1.\nFor every j \u2208[n], let us consider the quantity 2n \u0010\nbf i({j}) \u2212bf i+1({j})\n\u0011\n. Note that by \ufb02ipping\nthe value of f i on the points in the set Shiftx, bf i({j}) is reduced by exactly 1/2n \u00b7 4 \u00b7 |Shiftx|\u00b7|x|\n2n\nfor each j \u2208[n] and so we have that bf i+1({1}) = bf i+1({2}) = \u00b7 \u00b7 \u00b7 = bf i+1({n}). Moreover,\n23\n2n \u0010P\nj\u2208[n] bf i({j}) \u2212P\nj\u2208[n] bf i+1({j})\n\u0011\n= 2|Shiftx| \u00b7 |x| and so we have that\n\u03c3i+1\n=\n\u03c3i \u22122|Shiftx| \u00b7 |x|\n=\n2n \u00b7\nX\nj\u2208[n]\nbf i({j}) \u22122|Shiftx| \u00b7 |x|\n=\n2n \u00b7\nX\nj\u2208[n]\nbf i+1({j}),\nwhere the second equality holds by the induction hypothesis.\nFinally, since \u03c3i+1 = 2n \u00b7 P\nj\u2208[n] bf i+1({j}) and f i+1 is an odd {\u22121, 1}-valued function, we have\nthat \u03c3i+1 = 4nw for some integer w \u22650.\nWe proceed to show that ConstS terminates. Our goal is to show that at the termination of the algorithm,\nwe have \u03c3 = 0.\nClaim C.3. The algorithm ConstS always reaches a point where the condition in line 3b is true.\nProof. We use the functions ft from the beginning of this section. Take t\u2032 = \u221alog n \u22122 log log n \u2212C for\na constant C to be determined later. Then \u03c6(t\u2032) =\n1\n2\u03c0eC/2(log n)/\u221an, so Inf(ft\u2032) \u2265\n1\n6\u03c0eC/2 log n and\nPrx[ft\u2032(x) \u0338= 0] \u2264\n3\n2\u03c0eC/2/t\u2032 \u22643\n\u03c0eC/2p\nlog n/n by Equations 5 and 6 respectively. We choose C so that\nInf(ft\u2032) \u22653 \u00b7 Inf(CycleRun), which can be done since Inf(CycleRun) = O(log n).\nWe claim that ConstS does not include any strings x in S with 3 \u2264|x| < t\u2032 (and thus none with\n\u2212t\u2032 < |x| \u2264\u22123). Suppose that this claim is false. Because the algorithm is greedy, then every string x\nwhere CycleRun(x) = 1 with t\u2032 \u2264|x| \u2264n is corrupted and in S. Since CycleRun is odd and monotone,\nat least half of the strings where |x| = k are corrupted for t\u2032 \u2264k \u2264n. The contribution to be reduc-\ntion in the \ufb01rst-order Fourier coef\ufb01cients when we \ufb02ip the value on these strings from 1 to \u22121 is at least\n(1/2)Inf(f \u2032\nt) \u2265(3/2)Inf(CycleRun). But this implies that the sum of \ufb01rst-order Fourier coef\ufb01cients for\nthe corrupted function is at most \u2212(1/2)Inf(CycleRun) < 0. This implies that \u03c3 < 0 in the execution of\nConstS, which is a contradiction since \u03c3 stays nonnegative during the execution of the algorithm.\nIt remains to show that the condition in line 3 is satis\ufb01ed throughout the execution of ConstS. Because\nno strings with 3 \u2264|x| < t\u2032 or t\u2032 < |x| \u2264\u22123 are corrupted, the fraction of strings corrupted is at most\nPrx[ft\u2032(x) \u0338= 0] + Prx[|x| = \u00b11] = O(\np\nlog n/n). Thus at most c1\nq\nlog n\nn 2n strings are in S, so the\ncondition in line 3 holds.\nNext, we argue that when ConstS reaches the point where the condition in line 3b evaluates true, there\nalways exists a point x\u2217/\u2208S such that CycleRun(x\u2217) = 1 and |x\u2217| = 1. We \ufb01rst prove two lemmas.\nLemma C.4. Let S1\n1 be the set of x \u2208{\u22121, 1}n such that |x| = 1 and CycleRun(x) = 1. Then |S1\n1| \u22652n2.\nProof. Note that since CycleRun is odd, we have that P\nx:|x|=\u00b11 CycleRun(x) = 0.\nMoreover, since\nCycleRun is monotone, we must have that P\nx:|x|=1 CycleRun(x) \u2265P\nx:|x|=\u22121 CycleRun(x). Therefore,\nwe must have that P\nx:|x|=1 CycleRun(x) \u22650. Since CycleRun is {\u22121, 1}-valued, this immediately implies\nthat at least half of the points x where |x| = 1 are such that CycleRun(x) = 1. There are\n\u0000n\n(n\u22121)/2\n\u0001\n\u22654n2\nsuch strings where |x| = 1, so we have that |S1\n1| \u22652n2. This concludes the proof of Lemma C.4.\nLemma C.5. |S\n\u2032| \u22642n2.\n24\nProof. Consider the \ufb01rst time the condition in line 3b evaluates to true. Then there is some some x such that\nCycleRun(x) = 1 and such that \u03c3 \u22122|Shiftx|\u00b7|x| < 0. Since |x| \u2264n, this implies that \u03c3 \u22644n2. Moreover,\nin each iteration 2n points are added to S\n\u2032, and \u03c3 is reduced by 4n. Thus, after at most n iterations, \u03c3\nis reduced to 0. These iterations are the only iterations that contribute to S\n\u2032, so |S\n\u2032| \u2264n \u00b7 2n = 2n2 as\nclaimed.\nWe proceed to show that the when the condition in line 3b is true, there is an x\u2217/\u2208S such that\nCycleRun(x\u2217) = 1 and |x\u2217| = 1. By Lemma C.4, there exist at least 2n2 number of points x\u2217such that\nCycleRun(x\u2217) = 1 and |x\u2217| = 1. Thus, if ConstS reaches a point where the condition in line 3b evaluates\nto true and there is no point x\u2217/\u2208S such that CycleRun(x\u2217) = 1 and |x\u2217| = 1, then it must be the case that\nall such x\u2217are already contained in S. But since we have by Lemma C.5 that |S\n\u2032| \u22642n2 then we must have\nthat some point y such that CycleRun(y) = 1 and |y| = 1 was added to S before the \ufb01rst time the condition\nin line 3b evaluates to true. But the \ufb01rst time the condition in line 3b evaluates to true, we must have that\n|x| > 1, and since ConstS always chooses to add points y with maximal |y| \u2265|x| > 1 to the set S, this is\nimpossible.\nWe have now argued that ConstS always reaches a point where the condition in line 3b is true, and\nthat whenever this occurs there always exists a point x\u2217/\u2208S such that CycleRun(x\u2217) = 1 and |x\u2217| = 1.\nThis immediately implies that when ConstS completes, we have \u03c3 = 0 and |S| \u2264c1\nq\nlog n\nn 2n. As in the\nbeginning of the proof, we take f to be function to be the function such that\nf(x) =\n(\nCycleRun(x)\nif x /\u2208S\n\u2212CycleRun(x)\nif x \u2208S.\nClearly, Prx[f(x) \u0338= CycleRun(x)] = |S| \u2264c1\nq\nlog n\nn 2n, and applying the invariants of Claim C.2\nshows that f is 1-resilient, concluding the proof of Theorem 3.7.\nThis analysis almost works for any balanced monotone function with in\ufb02uence O(log n), such as Tribes.\nWhile the above could be adapted in a straightforward matter to show that there is a Boolean function close\nto Tribes with very small constant and \ufb01rst-order Fourier coef\ufb01cients, showing that all of these Fourier\ncoef\ufb01cients can be made exactly zero seems challenging. Since we are applying these results to juntas, our\nproofs can not tolerate even exponentially small Fourier coef\ufb01cients. The structure of CycleRun is quite\namenable to \u201clocal\u201d changes while retaining structure.\nC.1\nIn\ufb02uence bound for Cycle Run\nThe main result of this section is the following:\nTheorem C.6. Inf(CycleRun) = O(log n).\nThe condition on CycleRun given in De\ufb01nition 3.6 implies that for every in\ufb02uential edge (x, x\u2295i), at\nleast one of the endpoints is in the \ufb01rst two cases in De\ufb01nition 3.6, and the pivotal coordinate i occurs in a\nmaximum length run. Thus Inf(CycleRun) \u22642 Ex\u223cU[\u2113(x) \u00b7 (r\u2113(x)(x) + 1)], where \u2113(x) is the maximum\nlength run in the string x, ri(x) is the number of maximal runs of length exactly i in x, and U is the uniform\ndistribution on {\u22121, 1}n. In this section, we will not consider the runs wrapping around, and the +1 here\ntakes care of the case that we \u201csplit\u201d the cycle in a maximum length run to lay out the bits in a line.\nWe make use of a result from [Sch90]:\nTheorem C.7. Ex\u223cU[\u2113(x)] = O(log n)\n25\nThus Inf(CycleRun) \u22642 Ex\u223cU[\u2113(x)\u00b7r\u2113(x)(x)]+O(log n), so the remainder of the section is devoted to\nshowing Ex\u223cU[\u2113(x) \u00b7 r\u2113(x)(x)] = O(log n). To aid in our analysis, we will consider different distributions\nover binary strings. Consider the following method of generating a string x \u223cU:\n1. Initialize x to the empty string, and set b to a uniform \u00b11 random bit b.\n2. (Iterative step) Assuming there are still j > 0 bits of x to determine, then draw g \u223cGeometric(1/2)\nconditioned on g being at most j, and set the next g bits of x to b.\n3. If not all n bits of x are set, set b to \u2212b and return to step 2.\n4. If all bits of x are set, then x is a uniformly random string in {\u22121, 1}n.\nFurther, if we want to condition on the maximum run in x being at most some value t, we can replace\nthe conditioning in step 2 from \u201cbeing at most j\u201d to \u201cbeing at most min{t, j}\u201d.\nLemma C.8. For g \u223cGeometric(1/2), and 1 \u2264g \u2264t, we have Pr[g = g|g \u2264t] \u22642Pr[g = g].\nProof. Follows directly from conditional probability and the fact that Pr[g \u2264t] \u22651/2 for all t \u22651.\nFor an integer k > 0, we de\ufb01ne the distribution Gk on binary strings of varying length such that a draw\nfrom Gk is bg1(\u2212b)g2bg3 \u00b7 \u00b7 \u00b7 bgk if k is odd and bg1(\u2212b)g2bg3 \u00b7 \u00b7 \u00b7 (\u2212b)gk if k is even. Here, the gi\u2019s are\nindependent Geometric(1/2) variables, and b is a uniform \u00b11 bit.\nLemma C.9.\nE\nx\u223cU[\u2113(x) \u00b7 r\u2113(x)(x)|\u2113(x) = t] \u2264t(21\u2212tn + 1)\nProof. We \ufb01rst claim that\nE\nx\u223cU[\u2113(x) \u00b7 r\u2113(x)(x)|\u2113(x) = t] \u2264t + E\nx\u223cU[\u2113(x) \u00b7 rt(x)|\u2113(x) \u2264t]\nTo see this, note that if we further condition on the \ufb01rst run of length t selected, this expectation is\nmaximized when the \ufb01rst run is of length t. Also, the expectation can only increase if we allow all n more\nbits to be set rather than n \u2212t. Since the \ufb01rst run is of length t, we only need the maximum length run to be\nat most t in the rest of the string.\nNow we have\nt + E\nx\u223cU[\u2113(x) \u00b7 rt(x)|\u2113(x) \u2264t] \u2264t + t E\nx\u223cU[rt(x)|\u2113(x) \u2264t] \u2264t + t E\ny\u223cGn[rt(y)|\u2113(y) \u2264t]\nwhere the second inequality comes from the fact that x is generated by at most n runs, and not bounding\nthe length of the string only increases the possible number of runs of length t, conditioned on the maximum\nlength run being at most t. By Lemma C.8, the probability of a single run being of length t is at most 21\u2212t,\nso we have\nt + t\nE\ny\u223cGn[rt(y)|\u2113(y) \u2264t] \u2264t + t(21\u2212tn) = t(21\u2212tn + 1)\ncompleting the proof.\nLemma C.10.\nPrx\u223cU[\u2113(x) \u2264t] \u2264(1 \u22122\u2212t)n/8 + exp(\u2212n/32)\n26\nProof. For x \u2208{\u22121, 1}n, let runs(x) be the number of runs in x. We \ufb01rst show that with probability at\nleast 1 \u2212exp(\u2212n/32), a string x \u223cU has runs(x) \u2265n/8 . To do this, we prove that with probability\n1 \u2212exp(\u2212n/32), the \ufb01rst n/8 runs of x contain at most n/2 bits. Note that we may instead bound the\nnumber of bits in y \u223cGn/8, since each run of Gn/8 can only be longer.\nThe expected number of bits in Gn/8 generated is n/4, and this number of bits is concentrated around\nits mean; the number of bits has a negative binomial distribution. By [Bro], we have\nPry\u223cGn/8[bits(y) > 2(n/4)] \u2264exp(\u2212n/32)\nwhere the second inequality holds because the number of runs does not increase the probability of getting\na longer run, and the distributions of the lengths of each run in x are identical to (or conditioned on being\nshorter than) the lengths of the runs in Gn/8. We then have:\nPrx\u223cU[\u2113(x) \u2264t] \u2264Prx\u223cU[\u2113(x) \u2264t, runs(x) \u2265n/8] + exp(\u2212n/32)\n\u2264Pry\u223cGn/8[\u2113(y) \u2264t] + exp(\u2212n/32)\nwhere the second inequality holds because the length of each run of x is distributed identically (or condi-\ntioned to be shorter) to each run of y, and considering fewer runs only decreases the chances of obtaining\na run longer than t. It is then straightforward to calculate Pry\u223cGn/8[\u2113(y) \u2264t] = (1 \u22122\u2212t)n/8, since\nPr[g \u2264t] = 1 \u22122\u2212t for g \u223cGeometric(1/2).\nWe now proceed to show Ex\u223cU[\u2113(x) \u00b7 r\u2113(x)] = O(log n), starting by applying total expectation and\napplying Lemma C.9:\nE\nx\u223cU[\u2113(x) \u00b7 r\u2113(x)(x)] =\nn\nX\nt=1\nPrx\u223cU[\u2113(x) = t]Ex\u223cU[\u2113(x) \u00b7 r\u2113|\u2113(x) = t]\n\u2264\nn\nX\nt=1\nPrx\u223cU[\u2113(x) = t]t(21\u2212tn + 1)\n\u2264Ex\u223cU[\u2113(x)] +\nn\nX\nt=1\nPrx\u223cU[\u2113(x) = t]t21\u2212tn\n\u2264O(log n) +\nn\nX\nt=1\n((1 \u22122\u2212t)n/8 + exp(\u2212n/32))t21\u2212tn\n\u2264O(log n) +\nn\nX\nt=1\n(1 \u22122\u2212t)n/8)t21\u2212tn\n\u2264O(log n) +\nn\nX\nt=1\ntn21\u2212t exp(\u22122\u2212tn/8)\nLetting at = tn21\u2212t exp(\u22122\u2212tn/8), we see that at\u22121/at < 3/4 when 2 \u2264t \u2264log n \u221210, and\nat+1/at < 3/4 when log n + 10 \u2264t \u2264n. Also, at \u2264O(log n) for each term where log n \u221210 \u2264t \u2264\nlog n + 10. So the proof is completed by noting the above is at most\n27\nO(log n) +\nlog n\u221210\nX\nt=2\nalog n\u221210(3/4)log n\u221210\u2212t +\nt=log n+9\nX\nt=log n\u22129\nat +\nn\nX\nt=log n+10\nalog n+10(3/4)t\u2212(log n+10)\n\u2264O(log n)\n\uf8eb\n\uf8ed\nlog n\u221210\nX\nt=2\n(3/4)log n\u221210\u2212t +\nt=log n+9\nX\nt=log n\u22129\n1 +\nn\nX\nt=log n+10\n(3/4)t\u2212(log n+10)\n\uf8f6\n\uf8f8= O(log n).\nC.2\nLower bound for monotonicity-resiliency distance\nWe give a lower bound for distance between monotonicity and resiliency that matches the bound for CycleRun\nup to constant factors.\nTheorem C.11. For every monotone function f : {\u22121, 1}n \u2192{\u22121, 1} and 1-resilient g : {\u22121, 1}n \u2192\n{\u22121, 1}, we have Prx[f(x) \u0338= g(x)] \u2265\u2126(\nq\nlog n\nn ).\nProof. If Var[f] < 1/2, then bf(\u2205)2 > 1/2, and Pr[f \u0338= g] \u22651\n4E[(f \u2212g)2] \u22651/8 for any balanced (hence\n1-resilient) Boolean function g. If bf({i}) > n\u22120.49 for some i, then f is \u2126(n\u22120.49)-far from every Boolean\nfunction g where bg({i}) = 0.\nWe assume Var[f] \u22651/2 and bf({i}) \u2264n\u22120.49 for all i \u2208[n]. Since f is monotone, Infi(f) \u2264n\u22120.49\nfor all i \u2208[n], and by (Talagrand\u2019s strengthening of) the KKL Theorem [Tal93, KKL88], Inf(f) \u2265K log n\nfor some constant K, and P\ni\u2208[n] bf({i}) \u2265K log n. Let g : {\u22121, 1}n \u2192{\u22121, 1} be a 1-resilient Boolean\nfunction; we will show that Prx[f(x) \u0338= g(x)] = \u2126(\nq\nlog n\nn ).\nRecall the functions ft de\ufb01ned earlier:\nft(x) =\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\n1\nif |x| > t\u221an\n0\nif \u2212t\u221an \u2264|x| \u2264t\u221an\n\u22121\nif |x| < \u2212t\u221an\nSelect t to be the largest t such that ft satis\ufb01es Pr[ft(x) \u0338= 0] \u2265Pr[(f \u2212g)(x) \u0338= 0] = Pr[f(x) \u0338=\ng(x)]. We then have K log n \u2264P\ni\u2208[n] [\nf \u2212g({i}) \u2264P\ni\u2208[n] bft({i}), where the second inequality holds\nbecause ft maximizes the sum of the linear coef\ufb01cients for any function with support size Pr[ft(x) \u0338= 0],\nand the support size of ft is at least the support size of f \u2212g.\nAgain, because ft is monotone, Inf(ft) = P\ni\u2208[n] bft({i}). Equation 5 implies that (3K log n)/\u221an \u2265\n\u03c6(t) \u2265(K log n)/(3\u221an), and it follows that t \u22644\u221alog n. From Equation 6, we have Prx[ft(x) \u0338= 0] \u2265\n(4K/3)\nq\nlog n\nn . By the choice of t, we have\nPrx[f(x) \u0338= g(x)] > Prx[ft+1(x) \u0338= 0]\n\u2265Prx[ft(x) \u0338= 0] \u22122Prx[|x| = t]\n\u22654K\n3\nr\nlog n\nn\n\u221224K log n\nn\n= \u2126\n r\nlog n\nn\n!\n,\nwhere the \ufb01rst inequality is an application of the union bound, and the second is an application of\nEquation 7.\n28\n",
        "sentence": "",
        "context": "Equation 7.\n28\n\u2020Supported in part by NSF-CCF-1117079. Most of this work was done while the author was visiting Simons Institute for the\nTheory of Computing, University of California-Berkeley\nevolvability. Journal of Computer System Sciences, 78(5):1444\u20131459, 2012. 1.3, 2\n[FGKP09]\nV. Feldman, P. Gopalan, S. Khot, and A. Ponuswami. On agnostic learning of parities, mono-\nmials and halfspaces. SIAM Journal on Computing, 39(2):606\u2013645, 2009. 4"
    },
    {
        "title": "Bounded independence fools halfspaces",
        "author": [
            "Ilias Diakonikolas",
            "Parikshit Gopalan",
            "Ragesh Jaiswal",
            "Rocco A. Servedio",
            "Emanuele Viola"
        ],
        "venue": "In In Proc. 50th Annual Symposium on Foundations of Computer Science (FOCS),",
        "citeRegEx": "Diakonikolas et al\\.,? \\Q2009\\E",
        "shortCiteRegEx": "Diakonikolas et al\\.",
        "year": 2009,
        "abstract": "We show that any distribution on {-1,1}^n that is k-wise independent fools\nany halfspace h with error \\eps for k = O(\\log^2(1/\\eps) /\\eps^2). Up to\nlogarithmic factors, our result matches a lower bound by Benjamini,\nGurel-Gurevich, and Peled (2007) showing that k = \\Omega(1/(\\eps^2 \\cdot\n\\log(1/\\eps))). Using standard constructions of k-wise independent\ndistributions, we obtain the first explicit pseudorandom generators G: {-1,1}^s\n--> {-1,1}^n that fool halfspaces. Specifically, we fool halfspaces with error\neps and seed length s = k \\log n = O(\\log n \\cdot \\log^2(1/\\eps) /\\eps^2).\n  Our approach combines classical tools from real approximation theory with\nstructural results on halfspaces by Servedio (Computational Complexity 2007).",
        "full_text": "arXiv:0902.3757v1  [cs.CC]  21 Feb 2009\nBounded Independence Fools Halfspaces\nIlias Diakonikolas\u2217\nColumbia University\nParikshit Gopalan\u2020\nMSR-Silicon Valley\nRagesh Jaiswal\u2021\nColumbia University\nRocco A. Servedio\u00a7\nColumbia University\nEmanuele Viola\u00b6\nNortheastern University\nOctober 22, 2018\nAbstract\nWe show that any distribution on {\u22121, +1}n that is k-wise independent fools any\nhalfspace h : {\u22121, +1}n \u2192{\u22121, +1} with error \u01eb for k = O(log2(1/\u01eb)/\u01eb2).\nUp to\nlogarithmic factors, our result matches a lower bound by Benjamini, Gurel-Gurevich,\nand Peled (2007) showing that k = \u2126(1/(\u01eb2\u00b7log(1/\u01eb))). Using standard constructions of\nk-wise independent distributions, we obtain the \ufb01rst explicit pseudorandom generators\nG : {\u22121, +1}s \u2192{\u22121, +1}n that fool halfspaces. Speci\ufb01cally, we fool halfspaces with\nerror \u01eb and seed length s = k \u00b7 log n = O(log n \u00b7 log2(1/\u01eb)/\u01eb2).\nOur approach combines classical tools from real approximation theory with struc-\ntural results on halfspaces by Servedio (Comput. Complexity 2007).\n\u2217Research supported by NSF grants CCF-0728736, CCF-0525260, and by an Alexander S. Onassis Foun-\ndation Fellowship. Email: ilias@cs.columbia.edu\n\u2020Email: parik@microsoft.com\n\u2021Research supported by DARPA award HR0011-08-1-0069. Email: rjaiswal@cs.columbia.edu\n\u00a7Supported in part by NSF grants CCF-0347282, CCF-0523664 and CNS-0716245, and by DARPA award\nHR0011-08-1-0069. Email: rocco@cs.columbia.edu\n\u00b6This work was partially done while the author was a postdoctoral fellow at Columbia University, sup-\nported by grants NSF award CCF-0347282 and NSF award CCF-0523664. Email: viola@ccs.neu.edu\n1\nIntroduction\nHalfspaces, or threshold functions, are a central class of Boolean functions h : {\u22121, +1}n \u2192\n{\u22121, +1} of the form:\nh(x) = sign(w1x1 + \u00b7 \u00b7 \u00b7 + wnxn \u2212\u03b8),\nwhere the weights w1, . . . , wn and the threshold \u03b8 are arbitrary real numbers. These functions\nhave been studied extensively in a variety of contexts. In computer science, the work on\nhalfspaces dates back to the study of switching functions, see for instance the books [Der65,\nHu65, LC67, She69, Mur71]. In computational complexity, much e\ufb00ort has been put into\nunderstanding constant-depth circuits of halfspaces.\nOn the one hand this has resulted\nin surprising inclusions (such as the simulation of depth-d circuits of halfspaces by depth-\n(d + 1) circuits of majority gates [GHR92, GK98]), but on the other hand many seemingly\nbasic questions remain unsolved: for instance it is conceivable that every function in NP is\ncomputable by a depth-2 circuit of halfspaces [HMP+93, Kra91, KW91, FKL+01]. In learning\ntheory, the problem of learning an unknown halfspace has arguably been the most in\ufb02uential\nproblem in the development of the \ufb01eld, with algorithms such as Perceptron, Weighted\nMajority, Boosting, and Support Vector Machines emerging from this study.\nHalfspaces\n(with non-negative weights) have also been studied extensively in game theory and social\nchoice theory, where they are referred to as \u201cweighted majority games\u201d and have been\nanalyzed as models for voting, see e.g., [Pen46, Isb69, DS79, TZ92].\nIn this work we make progress on a natural complexity-theoretic question about halfs-\npaces. We construct the \ufb01rst explicit pseudorandom generators G : {\u22121, +1}s \u2192{\u22121, +1}n\nwith short seed length s that fool any halfspace h : {\u22121, +1}n \u2192{\u22121, +1}, i.e. satisfy\n| Ex\u2208{\u22121,+1}s[h(G(x))] \u2212Ex\u2208{\u22121,+1}n[h(x)]| \u2264\u01eb,\nfor a small \u01eb. We actually prove that the class of distributions known as k-wise independent\nhas this \u201cfooling\u201d property for a suitable k; as pointed out below, a generator can then be\nobtained using any of the standard explicit constructions of such distributions.\nDe\ufb01nition 1.1. A distribution D on {\u22121, +1}n is k-wise independent if the projection of\nD on any k indices is uniformly distributed over {\u22121, +1}k.\nTheorem 1.2 (Main). Let D be a k-wise independent distribution on {\u22121, +1}n, and let\nh : {\u22121, +1}n \u2192{\u22121, +1} be a halfspace. Then D fools h with error \u01eb, i.e.,\n| Ex\u2190D[h(x)] \u2212Ex\u2190U[h(x)]| \u2264\u01eb,\nprovided\nk \u2265C\n\u01eb2 log2\n\u00121\n\u01eb\n\u0013\n,\nwhere C is an absolute constant and U is the uniform distribution over {\u22121, +1}n.\nOur Theorem 1.2 matches up to logarithmic factors a lower bound by Benjamini, Gurel-\nGurevich and Peled [BGGP07] establishing that if k = o(1/(\u01eb2 \u00b7 log(1/\u01eb))) then there exists\na k-wise independent distribution D on {\u22121, +1}n such that Prx\u2190D[P\ni xi \u22650] > 1/2 + \u01eb,\ni.e. D does not fool the majority function with error \u01eb.\n1\nStandard explicit constructions of k-wise independent distributions over {\u22121, +1}n have\nseed length O(k \u00b7 log n) [CG89, ABI86], which is optimal up to constant factors [CGH+85].\nPlugging these in Theorem 1.2, we obtain explicit pseudorandom generators G : {\u22121, +1}s \u2192\n{\u22121, +1}n that fool any halfspace h : {\u22121, +1}n \u2192{\u22121, +1} with error \u01eb and have seed\nlength s = O(log n \u00b7 log2(1/\u01eb)/\u01eb2).\nBackground and comparison with previous explicit generators.\nThe literature is\nrich with explicit generators for various classes, such as small constant-depth circuits with\nvarious gates [AW, Nis91, LVW93, Vio07, Baz07, Bra09], low-degree polynomials [NN93,\nAGHP92, BV07, Lov08, Vio08], and one-way small-space algorithms [Nis92].\nMany of\nthese classes (such as low-degree polynomials and AC0 circuits) provably cannot imple-\nment halfspaces, and it is not known how to implement an arbitrary halfspace in any\nof these classes, so none of these results gives Theorem 1.2. However, some of these re-\nsults [Nis92, LVW93, Vio07] give generators for the restricted class of halfspaces given by\nh(x) = sign(Pn\ni=1 wixi \u2212\u03b8) where the weights are integers of magnitude at most poly(n).\nWhile it is well known that every halfspace has a representation with integer weights, in\ngeneral it is not possible to represent an arbitrary halfspace with poly(n) integer weights.\nIndeed, an easy counting argument (see e.g. [MT94, H\u02daas94]) shows that if the weights are\nrequired to be integers then almost all halfspaces require weights of magnitude 2\u2126(n), and\nin fact some halfspaces require weights of magnitude 2\u0398(n log n) [H\u02daas94]. Our result is for\nthe entire class of halfspaces with no restriction on the weights, and much of the richness\nof halfspaces only comes in this setting; for example, the \u201codd-max-bit\u201d function [Bei94],\nthe \u201cuniversal halfspace\u201d [GHR92], and other important halfspaces [H\u02daas94] all require expo-\nnentially large integer weights. Moreover, even for the restricted class of halfspaces where\nthe weights are integers of magnitude at most poly(n), previous techniques [Nis92] give seed\nlength s = O(log2 n) at best, while we achieve s = O(log n) for constant error.\nOther related results.\nSeveral recent papers have studied the power of k-wise indepen-\ndent distributions. An exciting recent result of Braverman [Bra09], which builds on an earlier\nbreakthrough of Bazzi [Baz07] (simpli\ufb01ed by Razborov [Raz08]), shows that polylog(n)-wise\nindependent distributions fool small constant-depth circuits, settling a conjecture of Linial\nand Nisan [LN90]. Benjamini et al. [BGGP07] showed that any O(1/\u01eb2)-wise independent\ndistribution D on {\u22121, +1}n satis\ufb01es | Prx\u2190D[P\ni xi \u22650] \u22121/2| \u2264\u01eb, i.e., such distributions\nfool the majority function. (We discuss [BGGP07] in more detail shortly. Here we note that\ntheir result does not seem immediately relevant for constructing generators, because to fool\nthe majority function, with optimal error 0, one can just output 1n with probability 1/2 and\n(\u22121)n with probability 1/2.) None of these results applies to general halfspaces.\nThe problem of constructing pseudorandom generators for halfspaces has been considered\nby several authors in the recent literature. Rabani and Shpilka give an explicit construc-\ntion of an \u01eb-net, or \u01eb-hitting set, for halfspaces [RS08]: a set of size poly(n, 1/\u01eb) which is\nguaranteed to contain at least one point where h(x) = +1 and at least one point where\nh(x) = \u22121 for any halfspace h which takes on both values with probability at least \u01eb under\n2\nthe uniform distribution. However, their construction does not o\ufb00er any guarantees about\nthe distribution of these values. [RS08] pose as a research goal \u201cto build methodically a\ntheory of pseudorandom generators for geometric functions\u201d such as halfspaces.\nThe problem of pseudorandom generators for halfspaces also arose in recent work by\nGopalan and Radhakrishnan [GR09] on \ufb01nding duplicates in a data stream. They required\na pseudorandom distribution that allows one to estimate the in\ufb02uence of a variable in a\nhalfspace, a problem which is in fact equivalent to constructing a pseudorandom generator\nfor a related halfspace. They observe that Nisan\u2019s space generator [Nis92] su\ufb03ces for the\nhalfspaces arising in their context, but they raise the problem of constructing pseudorandom\ngenerators for general halfspaces. Our result does not improve the space bounds for their\nproblem, but it does make the analysis simpler.\n1.1\nTechniques\nOur proof combines tools from real approximation theory with structural results regarding\nhalfspaces. An important notion is that of an \u01eb-regular halfspace; which is a halfspace h(x) =\nsign(P\ni wixi \u2212\u03b8) where no more than an \u01eb-fraction of the 2-norm of its coe\ufb03cient vector\n(w1, . . . , wn) comes from any single coe\ufb03cient wi. We \ufb01rst show that k-wise independence\nfools all \u01eb-regular halfspaces, and then use this to prove that k-wise independence fools all\nhalfspaces. Our proof can be broken down conceptually into three steps.\nStep 1: Fooling regular halfspaces.\nOur starting point is Bazzi\u2019s observation ([Baz07],\nTheorem 4.2)\nthat to establish that every k-wise independent distribution on {\u22121, +1}n\nfools a Boolean function f : {\u22121, +1}n \u2192{\u22121, +1} with error \u01eb, it is su\ufb03cient to exhibit\ntwo \u201csandwiching\u201d polynomials q\u2113, qu : {\u22121, +1}n \u2192{\u22121, +1} of degree at most k such\nthat:\n\u2022 qu(x) \u2265f(x) \u2265q\u2113(x) for all x \u2208{\u22121, +1}n; and\n\u2022 EU[qu(x) \u2212f(x)], EU[f(x) \u2212q\u2113(x)] \u2264\u01eb.\nUsing classical tools from real approximation theory, we give a self-contained proof of\nthe existence of univariate polynomials of degree K(\u01eb) := \u02dcO(1/\u01eb2) which, roughly speaking,\nprovide a good sandwich approximator to the univariate function sign(t) under the normal\ndistribution on R. This is useful for us because of the following simple but crucial insight:\nfor any regular halfspace h(x) = sign(w \u00b7 x \u2212\u03b8), the argument w \u00b7 x \u2212\u03b8 is well-approximated\nby a normal random variable (a precise error-estimate for this approximation is given by\nthe Berry-Ess\u00b4een theorem). For any \u01eb-regular halfspace, we can thus plug w \u00b7 x \u2212\u03b8 into our\nunivariate polynomials, and obtain low-degree sandwich polynomials for h. This establishes\nthat K(\u01eb)-wise independence fools all \u01eb-regular halfspaces.\nOf course, there are halfspaces sign(w \u00b7 x \u2212\u03b8) that are far from being \u01eb-regular and have\nw\u00b7x\u2212\u03b8 distributed very unlike a Gaussian. To tackle general halfspaces, we use the notion of\nthe \u01eb-critical index of a halfspace, which was (implicitly) introduced in [Ser07] and has since\n3\nplayed a useful role in several recent results on halfspaces [OS08, MORS09, DS09]. Brie\ufb02y,\nassuming that the weights w1, . . . , wn are sorted by absolute value, the \u01eb-critical index is the\n\ufb01rst index \u2113so that the weight vector (w\u2113, w\u2113+1, . . . , wn) is \u01eb-regular. The previous Step 1\nhandled halfspaces that are regular, corresponding to \u2113= 1. We now proceed by analyzing\ntwo cases, based on whether or not 1 < \u2113< L(\u01eb), or \u2113\u2265L(\u01eb), for L(\u01eb) := \u02dcO(1/\u01eb2). In both\ncases, it is convenient to think of the variables as partitioned into a \u201chead\u201d part consisting the\n\ufb01rst L(\u01eb) variables and corresponding to the largest weights, and of a \u201ctail\u201d part consisting\nof the rest.\nStep 2: Fooling halfspaces with small critical Index (\u2113< L(\u01eb)).\nWe argue that for\nevery setting of the head variables, the \u01eb-regularity of the tail is su\ufb03cient to ensure that the\noverall halfspace gives the right bias. More precisely, we assume that our distribution D is\n(K(\u01eb) + L(\u01eb))-wise independent, and note that each setting of the \u2113head variables gives an\n\u01eb-regular halfspace sign(w \u00b7 x \u2212\u03b8\u2032) over the tail variables (with the constant \u03b8\u2032 depending\non the values of the head variables). Since the marginal distribution on the tail variables is\nK(\u01eb)-wise independent for every setting of the head variables, the distribution D fools all\nsuch halfspaces.\nStep 3: Fooling halfspaces with large critical index (\u2113\u2265L(\u01eb)).\nIn this case, we\nshow that the setting of the head variables alone is very likely to determine the value of the\nfunction. More precisely, we show that a uniform random assignment to the head variables\nis very likely to yield a halfspace sign(wT \u00b7 xT \u2212\u03b8\u2032) over the tail variables T in which\n|\u03b8\u2032| > \u2225wT\u22252/\u01eb.\n(\u22c6)\nNow, as long as the tail variables are pairwise independent, by Chebyshev\u2019s inequality it\nfollows that the value wT \u00b7 xT will be sharply concentrated within [\u2212\u2225wT \u22252, +\u2225wT\u22252]. So,\nfor most settings of the head variables, we get something very close to a constant function\nover the tail variables. Since a (K(\u01eb) + 2)-wise independent distribution gives us uniform\nrandomness for the head variables and pairwise independence for the tail variables, bounded\nindependence fools these halfspaces as well.\nThe key idea behind the proof of (\u22c6) is that up to the critical index \u2113\u2013 which in this case\nis large (\u2113\u2265L(\u01eb)) \u2013 the weights (w1, . . . , w\u2113\u22121) must be decreasing fairly rapidly; this allows\nus to prove strong anti-concentration for the distribution of \u03b8\u2032, which in turn yields (\u22c6).\nOverall, the amount of independence required for all the three steps to work is:\nmax{K(\u01eb), K(\u01eb) + L(\u01eb), K(\u01eb) + 2} = \u02dcO(1/\u01eb2),\nconcluding this sketch of the proof of Theorem 1.2.\nUnivariate approximations to the sign function.\nAs mentioned above, our approach\nrelies on the existence of low-degree univariate sandwich approximators to the sign function\nunder the normal distribution on R. Low-degree approximations to the sign function have\n4\nbeen studied in both computer science and mathematics (see for instance [Pat92, EY07,\nKS07] and the references therein). However it appears that these results do not \ufb01t all our\nrequirements. Below we discuss how our approach relates to the work of Benjamini et al.\n[BGGP07] and Eremenko and Yuditskii [EY07].\nBenjamini et al. prove that O(1/\u01eb2)-wise independence su\ufb03ces to fool the majority func-\ntion, using machinery from the theory of the classical moment problem. However, their proof\nseems to be tailored quite speci\ufb01cally to the majority function, where the moments can be\nunderstood in terms of Krawtchouk polynomials and known bounds on such polynomials\ncan be applied, so it seems di\ufb03cult to extend their approach to general halfspaces (or indeed\neven to slight variants of the majority function).\nBazzi\u2019s condition on the existence of degree-k sandwiching polynomials mentioned above\nis in fact both necessary and su\ufb03cient for all k-wise independent distributions to fool a\nfunction f. Thus the [BGGP07] theorem implies the existence of O(1/\u01eb2)-degree multivariate\nsandwich polynomials for the majority function; symmetrization then implies that there exist\nunivariate polynomials which, roughly speaking, provide good sandwich approximation to\nthe function sign(t) under the binomial distribution. This is similar in spirit to the result\nwe establish (mentioned in Step 1 above) about univariate polynomial approximators, but\nthere is a crucial di\ufb00erence: since the binomial distribution is supported only on the integers\n{\u2212n, . . . , n}, it seems di\ufb03cult to infer much about the behavior of the univariate polynomial\nimplicit in [BGGP07] on values outside of {\u2212n, . . . , n}. Hence, it is unclear whether these\npolynomials can be used for general (or even regular) halfspaces.\nIn contrast, we work with the best possible pointwise approximation to the function\nsign(t) on the (piecewise) continuous domain [\u22121, \u2212a] \u222a[a, 1]. This uniform error bound is\nconvenient for dealing with regular halfspaces; moreover, working with the optimal pointwise\napproximator allows us to exploit various properties of optimal approximators that follow\nfrom the theory of Chebyshev approximation, in a way that is crucial for us to obtain the\nrequired \u201cunivariate sandwich approximators.\u201d\nWe note that a recent work in approximation theory [EY07] analyzes the error achieved\nby this optimal polynomial and in particular establishes the limiting behavior of the error.\nFor our purposes, though, we require the error to converge to the limit fairly rapidly and it\nis unclear whether the results of [EY07] guarantee this. We present an error analysis which\nis elementary (it only uses basic approximation theory) and moreover matches the limiting\nbounds of [EY07] up to a constant factor.\nOrganization.\nIn Section 3 we show how a certain univariate polynomial approximator\nto sign(t) yields low-degree sandwich polynomials for \u01eb-regular halfspaces over {\u22121, 1}n. In\nSection 4 we construct the required univariate polynomial, which essentially gives sandwich\npolynomials for sign(t) under the normal distribution. In Section 5 we show how non-regular\nhalfspaces can be fooled using our results for regular halfspaces, concluding the proof or our\nmain theorem.\n5\n2\nPreliminaries\nRecall that the univariate function sign(t) takes value +1 for t \u22650 and \u22121 for t < 0.\nDe\ufb01nition 2.1 (Halfspace). A halfspace is a Boolean function f : {\u22121, +1}n \u2192{\u22121, +1}\nwhich can be expressed as f(x) = sign (P\ni wixi \u2212\u03b8) for some \u03b8 \u2208R, (w1, . . . , wn) \u2208Rn.\nThroughout this paper we assume without loss of generality that halfspaces are normal-\nized to satisfy w2\n1+\u00b7 \u00b7 \u00b7+w2\nn = 1. Such a representation can always be obtained by appropriate\nscaling.\nDe\ufb01nition 2.2 (Fooling a Function Class). Let f : {\u22121, +1}n \u2192{\u22121, +1} be any function.\nWe say that a distribution D over {\u22121, +1}n fools f with error \u01eb, or \u01eb-fools f, if\n| Ex\u2190D[f(x)] \u2212Ex\u2190U[f(x)]| \u2264\u01eb,\nwhere U denotes the uniform distribution over {\u22121, +1}n. We say that D fools a class of\nfunctions F if D fools every f \u2208F.\nWe require a few basic facts from probability theory: the Berry-Ess\u00b4een theorem and the\nstandard tail bounds of Hoe\ufb00ding and Chebyshev. We discuss them next.\nThe Berry-Ess\u00b4een theorem is a version of the Central Limit Theorem with explicit error\nbounds:\nTheorem 2.3. (Berry-Ess\u00b4een) Let X1, . . . , Xn be a sequence of independent random vari-\nables satisfying E[Xi] = 0 for all i,\npP\ni E[X2\ni ] = \u03c3, and P\ni E[|Xi|3] = \u03c13.\nLet S =\n(X1 + \u00b7 \u00b7 \u00b7 + Xn)/\u03c3 and let F denote the cumulative distribution function (cdf) of S. Then\nsup\nx |F(x) \u2212\u03a6(x)| \u2264C\u03c13/\u03c33,\nwhere \u03a6 is the cdf of a standard Gaussian random variable (with mean zero and variance\none), and C is a universal constant. [Shi86] has shown that one can take C = .7915.\nCorollary 2.4. Let x1, . . . , xn denote independent uniformly \u00b11 random signs and let w1, . . . , wn \u2208\nR. Write \u03c3 =\npP\ni w2\ni , and assume |wi|/\u03c3 \u2264\u03c4 for all i. Then for any interval [a, b] \u2286R,\n\f\fPr[a \u2264w1x1 + \u00b7 \u00b7 \u00b7 + wnxn \u2264b] \u2212\u03a6([ a\n\u03c3, b\n\u03c3])\n\f\f \u22642\u03c4,\nwhere \u03a6([c, d]) := \u03a6(d) \u2212\u03a6(c). In particular,\nPr[a \u2264w1x1 + \u00b7 \u00b7 \u00b7 + wnxn \u2264b] \u2264|b \u2212a|\n\u03c3\n+ 2\u03c4.\nFor completeness we recall the Hoe\ufb00ding and Chebyshev bounds:\nTheorem 2.5 (Hoe\ufb00ding). Fix any w \u2208Rn. For any \u03b3 > 0, we have\nPr\nx\u2190U[w \u00b7 x \u2265\u03b3\u2225w\u2225] \u2264e\u2212\u03b32/2\nand\nPr\nx\u2190U[w \u00b7 x \u2264\u2212\u03b3\u2225w\u2225] \u2264e\u2212\u03b32/2.\nTheorem 2.6 (Chebyshev). For any random variable X with E[X] = \u00b5 and Var[X] = \u03c32\nand any k > 0,\nPr[|X \u2212\u00b5| \u2265k\u03c3] \u22641\nk2.\n6\n3\nFooling regular halfspaces\nIn this section we show how to fool regular halfspaces, de\ufb01ned next (recall all our halfspaces\nare normalized to satisfy w2\n1 + \u00b7 \u00b7 \u00b7 + w2\nn = 1).\nDe\ufb01nition 3.1 (Regular Halfspace). A halfspace f is said to be \u01eb-regular if it can be\nexpressed as f(x) = sign(w \u00b7 x \u2212\u03b8) where for all i = 1, . . . , n, we have\n|wi| \u2264\u01eb.\nAn \u01eb-regular halfspace f(x) = sign(w \u00b7 x \u2212\u03b8) has the convenient property that the\ncumulative distribution function (cdf) of w \u00b7 x \u2212\u03b8 is everywhere within \u00b1O(\u01eb) of the cdf of\nthe shifted Gaussian N(\u2212\u03b8, 1). This is a direct consequence of the Berry-Ess\u00b4een Theorem.\nGiven \u01eb > 0, we de\ufb01ne the following parameters:\na(\u01eb) :=\n\u01eb2\nC log(1/\u01eb),\nK(\u01eb) := 4c log(1/\u01eb)\na\n+ 2 < 5c\na log(1/\u01eb) = O\n\u0000log2(1/\u01eb)/\u01eb2\u0001\n.\nWe assume without loss of generality that \u01eb is a su\ufb03ciently small power of 2 (i.e., \u01eb = 2\u2212i for\nsome integer i). The positive constants C and c will be chosen later; but (with foresight),\nwe will require that C \u226bc.\nIn this section we prove the following:\nTheorem 3.2 (Fooling \u01eb-regular halfspaces). Any K(\u01eb)-wise independent distribution fools\n\u01eb-regular halfspaces with error 12\u01eb.\nTo prove the theorem we construct certain \u201csandwiching\u201d polynomials. We now de\ufb01ne\nsuch polynomials and then explain why they are su\ufb03cient for our purposes.\nDe\ufb01nition 3.3. Let f : {\u22121, +1}n \u2192{\u22121, +1} be a Boolean function. A pair of real-valued\npolynomials q\u2113(x1, . . . , xn), qu(x1, . . . , xn) are said to be \u01eb-sandwich polynomials of degree k\nfor f if they have the following properties:\n\u2022 deg(qu), deg(q\u2113) \u2264k;\n\u2022 qu(x) \u2265f(x) \u2265q\u2113(x) for all x \u2208{\u22121, +1}n;\n\u2022 Ex\u2190U[qu(x) \u2212f(x)] \u2264\u01eb and Ex\u2190U[f(x) \u2212q\u2113(x)] \u2264\u01eb.\nThe following fact relates sandwiching polynomials to fooling:\nLemma 3.4 (Bazzi). Let f : {\u22121, +1}n \u2192{\u22121, +1} be a Boolean function. Every k-wise\nindependent distribution \u01eb-fools f if and only if there exist \u01eb-sandwich polynomials of degree\nk for f.\n7\nFigure 1: Qualitative plot of polynomial P.\nWe only use the \u201cif\u201d direction of this lemma for our proof, which follows straightforwardly\nby linearity of expectation. The other direction is a consequence of LP\u2013duality (see [Baz07]\nfor a proof).\nTo construct appropriate sandwiching polynomials, we start by exhibiting a univariate\npolynomial P : R \u2192R that approximates the function sign : R \u2192{\u22121, 1} in a certain\nspecialized way which we discuss shortly. Let us be given an \u01eb-regular halfspace h(x) =\nsign(w\u00b7x\u2212\u03b8), and assume that |\u03b8| is small (the case where |\u03b8| is large is simpler). We obtain\nthe upper sandwiching polynomial qu in De\ufb01nition 3.3 by plugging into P the value w \u00b7 x\u2212\u03b8\nscaled by a large Z \u22650:\nqu(x) := P\n\u0012w \u00b7 x \u2212\u03b8\nZ\n\u0013\n.\nFor the lower sandwiching polynomial q\u2113we will use \u2212P(\u2212t). The key properties of P(t)\nare that (1) P(t) \u2265sign(t) for every t \u2208R, (2) P(t) gives a good (error \u01eb) pointwise\napproximation to sign(t) for t \u2208[\u22121/2, 1/2] except for t in the small interval [\u22122a, 0] where\nthe error is bounded by a constant, and (3) P(t) does not grow too quickly for |t| \u22651/2.\nFor a qualitative depiction of P we refer the reader to Figure 1 (this \ufb01gure is not an actual\nplot but rather is intended to qualitatively illustrate the guarantees on the behavior of P\non various intervals; also the parameter 1/2 is replaced by 1 \u2212a \u22651/2 for later needs).\nProperty (1), together with the fact that scaling by Z does not change the value of the\nhalfspace, immediately gives the sandwiching property qu(x) \u2265h(x) in De\ufb01nition 3.3. To\n8\nestablish the small-error property Ex[qu(x) \u2212h(x)] in De\ufb01nition 3.3, we reason by case\nanalysis. First, by our choice of parameters, the small interval [\u22122a, 0] remains small even\nafter scaling by Z, and so we use an anti-concentration argument to show that the input\nt = (w \u00b7 x \u2212\u03b8)/Z to the polynomial is unlikely to land there, and thus the contribution\ntowards the error Ex[qu(x) \u2212h(x)] is negligible in this case. Also, whenever the input t to\nthe polynomial lands in [\u22121/2, 1/2] \\ [\u22122a, 0], the contribution to the error Ex[qu(x) \u2212h(x)]\nis small because by Property (2) the polynomial approximates the sign function well there:\nqu(x) \u2212h(x) \u2264\u01eb. Finally, the event that the input t = (w \u00b7 x \u2212\u03b8)/Z to P has absolute value\nbigger than 1/2 corresponds to the event that |w \u00b7x\u2212\u03b8| \u2265Z/2. The scaling factor Z is large\nand the halfspace is \u01eb-regular, and so we can apply standard tail estimates to bound from\nabove this probability. Since by Property (3) the polynomial P(t) does not grow too quickly\nfor |t| \u22651/2, the contribution to the error Ex[qu(x) \u2212h(x)] is small even in this case.\nWe now proceed with the formal proof. We start with recording in the following theorem\nthe properties of P.\nTheorem 3.5. Let 0 < \u01eb < 0.1 and let a and K be as de\ufb01ned above. There is a univariate\npolynomial P(t) such that deg(P) \u2264K with the following properties:\n(1) P(t) \u2265sign(t) \u2265\u2212P(\u2212t) for all t \u2208R;\n(2) P(t) \u2208[sign(t), sign(t) + \u01eb] for t \u2208[\u22121/2, \u22122a] S[0, 1/2];\n(3) P(t) \u2208[\u22121, 1 + \u01eb] for t \u2208(\u22122a, 0);\n(4) |P(t)| \u22642 \u00b7 (4t)K for all |t| \u22651/2.\nWe defer the proof of Theorem 3.5 to Section 4 and we proceed with the proof of Theorem\n3.2.\n3.1\nProof of Theorem 3.2\nLet h(x) = sign(w \u00b7 x \u2212\u03b8) be an \u01eb-regular halfspace (and recall w2\n1 + \u00b7 \u00b7 \u00b7 + w2\nn = 1.) Let\nZ := \u01eb\n2a = C log(1/\u01eb)\n2\u01eb\n.\nWe break the analysis into the following two cases, based on the magnitude of the threshold\n\u03b8.\n3.1.1\n|\u03b8| is small (|\u03b8| \u2264Z/4)\nThe sandwich polynomials we use are:\nqu(x) := P\n\u0012w \u00b7 x \u2212\u03b8\nZ\n\u0013\n,\nql(x) := \u2212P\n\u0012\u03b8 \u2212w \u00b7 x\nZ\n\u0013\n.\n(1)\n9\nFirst, observe that for every x \u2208{\u22121, +1}n we have\nqu(x) \u2265h(x) \u2265ql(x).\nThis is because from Theorem 3.5 with t = (w \u00b7 x \u2212\u03b8)/Z we get\nqu(x) \u2265sign\n\u0012w \u00b7 x \u2212\u03b8\nZ\n\u0013\n= sign(w \u00b7 x \u2212\u03b8) = h(x) \u2265ql(x).\nIn the rest of this section we bound the error of the approximation.\nLemma 3.6. Ex[qu(x) \u2212h(x)] < 10\u01eb.\nProof. De\ufb01ne the random variable H(x) = (w \u00b7 x\u2212\u03b8)/Z. We prove the desired upper bound\nby partitioning the space into three events and bounding the contribution from each:\n1. S1 is the event that H(x) \u2208[\u2212\u01eb/Z, 0].\n2. S2 is the event that |H(x)| \u22641/2, but S1 does not happen.\n3. S3 is the event that |H(x)| > 1/2.\nWe have\nEx[qu(x) \u2212h(x)] =\n3\nX\ni=1\nPr\nx [Si] Ex[qu(x) \u2212h(x)|Si].\nCase 1:\nIn this case, the pointwise error is moderate \u2013 at most (2 + \u01eb) \u2013 and we use\ngaussian anti-concentration to argue that the event has small probability mass. The event\nH(x) \u2208[\u2212\u01eb/Z, 0] implies that\nw \u00b7 x \u2212\u03b8\nZ\n\u2208[\u22122a, 0] \u21d2qu(x) \u22641 + \u01eb \u21d2qu(x) \u2212h(x) \u22642 + \u01eb,\nusing Item (3) in Theorem 3.5.\nSince h is \u01eb-regular, from Corollary 2.4 it follows that Prx[H(x) \u2208[\u2212\u01eb/Z, 0]] \u22643\u01eb. So,\nPr\nx [S1] Ex[qu(x) \u2212h(x)|S1] \u2264(2 + \u01eb) \u00b7 3\u01eb < 8\u01eb.\nCase 2:\nThis event has high probability, but in this range we get good pointwise\napproximation. The event S2 implies that\nH(x) \u2208[\u22121/2, 1/2] \\ [\u22122a, 0] \u21d2qu(x) \u2264h(x) + \u01eb \u21d2qu(x) \u2212h(x) \u2264\u01eb,\nwhere we used Item (2)in Theorem 3.5. So,\nPr\nx [S2] Ex[qu(x) \u2212h(x)|S2] \u22641 \u00b7 \u01eb \u2264\u01eb.\n10\nCase 3: Here we trade o\ufb00the large magnitude of error (Item (4) in Theorem 3.5) with\nthe small probability of the event (bounded by the Hoe\ufb00ding bound). De\ufb01ne the intervals\nI+\nj =\n\u0014j\n2, (j + 1)\n2\n\u0013\nfor j = 1, 2, . . .\nI\u2212\nk =\n\u0012\u2212(k + 1)\n2\n, \u2212k\n2\n\u0015\nfor k = 1, 2, . . . .\nWe can write\nPr\nx [S3] Ex[qu(x) \u2212h(x)|S3] =\nX\nj\u22651\nPr\nx [H(x) \u2208I+\nj ] Ex[qu(x) \u2212h(x)|H(x) \u2208I+\nj ]\n+\nX\nk\u22651\nPr\nx [H(x) \u2208I\u2212\nk ] Ex[qu(x) \u2212h(x)|H(x) \u2208I\u2212\nk ].\n(2)\nFix any integer j \u22651. If H(x) \u2208I+\nj , then\nj\n2 \u2264H(x) < j + 1\n2\n.\nRecalling that we have |P(t)| \u22642 \u00b7 (4t)K for t \u22651/2, we get that\nqu(x) = P(H(x)) \u22642(2j + 2)K.\nSince h(x) = 1, we get\nqu(x) \u2212h(x) = q(x) \u22121 \u22642(2j + 2)K \u22121.\n(3)\nNext we bound Prx[H(x) \u2208I+\nj ] using the Hoe\ufb00ding bound.\nPr[H(x) \u2208I+\nj ] \u2264Pr\nx\n\u0014\nw \u00b7 x \u2212\u03b8 \u2265jZ\n2\n\u0015\n\u2264Pr\nx\n\u0014\nw \u00b7 x \u2265jZ\n4\n\u0015\n\u2264e\u2212j2Z2/32\n(4)\nwhere the second inequality uses the fact that |\u03b8| \u2264Z/4.\nThe analysis of the intervals I\u2212\nk is similar (except h(x) = \u22121). For H(x) \u2208I\u2212\nk we get\n|H(x)| \u2264k + 1\n2\n\u21d2qu(x) \u22642(k + 1)K \u21d2qu(x) \u2212h(x) \u22642(2k + 2)K + 1.\n(5)\nSimilarly, the Hoe\ufb00ding bound gives\nPr[H(x) \u2208I\u2212\nk ] \u2264Pr\nx\n\u0014\nw \u00b7 x \u2212\u03b8 \u2264\u2212kZ\n2\n\u0015\n\u2264Pr\nx\n\u0014\nw \u00b7 x \u2264\u2212kZ\n4\n\u0015\n\u2264e\u2212k2Z2/32.\n(6)\n11\nPlugging equations (3), (4), (5), (6) back into (2), we get\nPr\nx [S3] Ex[qu(x) \u2212h(x)|S3] \u2264\nX\nj\u22651\n2(2j + 2)K \u22121\nej2Z2/32\n+\nX\nk\u22651\n2(2k + 2)K + 1\nek2Z2/32\n= 4\nX\nj\u22651\n(2j + 2)K\nej2Z2/32\n< 4\nX\nj\u22651\nej(2K\u2212Z2/32)\nwhere the last inequality follows by noting that, for j \u22651, (2j + 2)K < e2Kj and ej2Z2/32 \u2265\nejZ2/32. But now observe that\n2K \u2212Z2\n32 < C log2(1/\u01eb)\n\u01eb2\n\u0012\n10c \u2212C\n128\n\u0013\n.\nFor a suitable choice of C \u226bc, we have that 10c \u2212C/128 \u2264\u22121, so\nPr\nx [S3] Ex[qu(x) \u2212h(x)|S3] < 4\nX\nj\ne\u2212jC log2(1/\u01eb)\n\u01eb2\n< \u01eb.\nThus overall, we have Ex[qu(x) \u2212h(x)] \u226410\u01eb.\nThe lower sandwich bound follows by symmetry:\nLemma 3.7. Ex[h(x) \u2212ql(x)] < 10\u01eb.\nProof. Since ql(x) \u2264h(x) for every x, we also have \u2212h(x) \u2264\u2212ql(x). Thus\n\u2212ql(x) = P\n\u0012\u03b8 \u2212w \u00b7 x\nZ\n\u0013\nis an upper sandwich for the function \u2212h(x) = sign(\u03b8 \u2212w \u00b7 x). As this does not change the\nmagnitude of \u03b8, we can apply the analysis of Lemma 3.6 to conclude that\nEx[h(x) \u2212ql(x)] = Ex[\u2212ql(x) \u2212(\u2212h(x))] < 10\u01eb.\n3.1.2\n|\u03b8| is large (|\u03b8| > Z/4)\nWe assume for simplicity that \u03b8 \u2265Z/4 (the case when \u03b8 is negative is handled similarly).\nThe sandwich polynomials we use are:\nru(x) = P\n\u0012w \u00b7 x \u2212Z/4\nZ\n\u0013\n,\nrl(x) = \u22121.\n(7)\n12\nLemma 3.8. h(x) \u2265rl(x) for all x \u2208{\u22121, +1}n. Further, Ex[h(x) \u2212rl(x)] \u22642\u01eb.\nProof. Note that Ex[h(x)\u2212rl(x)] = 2 Prx[h(x) = 1]. For large enough C we have Prx[h(x) =\n1] = Prx[w \u00b7 x \u2265\u03b8] < e\u2212Z2/32 < \u01eb.\nLemma 3.9. ru(x) \u2265h(x) for all x \u2208{\u22121, +1}n. Further, Ex[ru(x) \u2212h(x)] \u226412\u01eb.\nProof. Observe that ru(x) is the upper sandwich polynomial for the halfspace h\u2032(x) = sign(w\u00b7\nx \u2212Z/4) as speci\ufb01ed in Section 3.1.1. Thus we have\nru(x) \u2265h\u2032(x) \u2265h(x)\nhence\nEx[ru(x) \u2212h(x)] = Ex[ru(x) \u2212h\u2032(x)] + Ex[h\u2032(x) \u2212h(x)].\nBy Lemma 3.6, Ex[ru(x)\u2212h\u2032(x)] \u226410\u01eb whereas by the Hoe\ufb00ding bound Ex[h\u2032(x)\u2212h(x)] \u22642\u01eb\nwhich completes the proof.\n4\nProof of Theorem 3.5\nThis section contains our proof of Theorem 3.5. The key step is to exhibit a low-degree\nunivariate polynomial that approximates sign(t) well when |t| \u2208[a, 1] and is well-behaved\neven for larger values of |t| to be compatible with the sandwich condition. We phrase this as\na problem in univariate approximation. The solution we use is a low-degree polynomial p(t)\nwhich is an optimal pointwise approximator to sign(t) on [\u22121, \u2212a] \u222a[a, 1]. Such an optimal\npolynomial exists and we prove that it is well-behaved for large |t|, using ideas from classical\napproximation theory. However, it seems di\ufb03cult to construct this polynomial explicitly and\nbound its error.\nRecent work by [EY07] analyzes the error achieved by such a polynomial and in particular\nestablishes the limiting behavior of the error function. For our purposes, though, we require\nthe error to converge to the limit fairly rapidly and it is unclear whether the results of [EY07]\nguarantee this.\nInstead, we bound the error by constructing a small error approximator q(t) using Jack-\nson\u2019s theorem together with standard ampli\ufb01cation ideas. While q(t) might not be well-\nbehaved for large value of t, we only use it to bound from above the error of p(t) on\n[\u22121, \u2212a] \u222a[a, 1]. Our approach has the advantage of being self-contained and elementary\n(using only standard ingredients from basic approximation theory) and matches the limiting\nbounds of [EY07] up to a constant factor.\nFor a bounded continuous function f : [\u22121, 1] \u2192R, we de\ufb01ne its modulus of continuity\n\u03c9f(\u03b4) as\n\u03c9f(\u03b4) := sup{|f(x) \u2212f(y)| : x, y \u2208[\u22121, 1]; |x \u2212y| \u2264\u03b4}.\nA classical result of Dunham Jackson from the early twentieth century bounds the error of\nthe best degree-\u2113approximation to f.\n13\nTheorem 4.1. (Jackson\u2019s Theorem)\n[Che66] For f as above and any integer \u2113\u22651,\nthere exists a polynomial J(t) with deg(J) \u2264\u2113so that\nmax\nt\u2208[\u22121,1] |J(t) \u2212f(t)| \u22646\u03c9f\n\u00121\n\u2113\n\u0013\n.\nRecall the parameter a =\n\u01eb2\nC log(1/\u01eb) from the previous section. We now de\ufb01ne the following\nparameter:\nm := c log(1/\u01eb)\na\n.\nIt will be crucial for us that m is even (see in particular the last paragraph of the proof\nof Theorem 4.5.); for this condition to be satis\ufb01ed, it is of course enough that c is even. (We\nalso note that the parameters K and m are such that K = 4m + 2.)\nLemma 4.2. For a, m as above, there is a polynomial q(t) of degree at most 2m such that\nmax\n|t|\u2208[a,1] |q(t) \u2212sign(t)| \u2264\u01eb2.\nProof. De\ufb01ne the continuous and piecewise linear function f(x) as\nf(x) =\n(\nsign(t)\na \u2264|t| \u22641\nt/a\n|t| \u2264a.\nThus f(x) increases linearly from \u22121 to 1 in the range [\u2212a, a]. A simple calculation shows\nthat \u03c9f( 1\n\u2113) = 1/(a\u2113). Taking \u2113\u226525/a, Jackson\u2019s theorem implies the existence of a polyno-\nmial J(t) of degree at most \u2113such that\nmax\na\u2264|t|\u22641 |J(t) \u2212sign(t)| \u2264max\nt\u2208[\u22121,1] |J(t) \u2212f(t)| \u22646\na\u2113< 1\n4.\nOur goal is to bring the error down to \u01eb2. Rather than using Jackson\u2019s theorem for this\n(which would require degree \u02dcO(\u01eb\u22124)), we use the degree-k amplifying polynomial\nAk(u) :=\nX\nj\u2265k\n2\n\u0012k\nj\n\u0013 \u00121 + u\n2\n\u0013j \u00121 \u2212u\n2\n\u0013k\u2212j\n.\n(8)\nThis polynomial has the following properties (easily proved via elementary calculation\nand also following from the Cherno\ufb00bound):\nClaim 4.3. The polynomial Ak(u) satis\ufb01es:\n1. If u \u2208[3/5, 1], then 2Ak(u) \u22121 \u2208[1 \u22122e\u2212k/6, 1].\n2. If u \u2208[\u22121, \u22123/5], then 2Ak(u) \u22121 \u2208[\u22121, \u22121 + 2e\u2212k/6].\n14\nWe de\ufb01ne the polynomial\nq(t) := 2Ak\n\u00124\n5J(t)\n\u0013\n\u22121\nwhere k = 15 log(1/\u01eb). Scaling J(t) by 4\n5 ensures that the argument to Ak lies in the range\n[\u22121, \u22123/5] \u222a[3/5, 1] whenever |t| \u2264a. Applying Claim 4.3 with k = 15 log(1/\u01eb) gives\nmax\n|t|\u2208[a,1] |q(t) \u2212sign(t)| < 2e\u2212k/6 < \u01eb2.\nFinally, by selecting c large enough, we have\ndeg(q) \u2264deg(J) deg(Ak) \u226425\na \u00b7 15 log(1/\u01eb) < 2c\na log(1/\u01eb) = 2m.\nWe now present the \u201cwell-behaved\u201d polynomial p(t) mentioned at the beginning of this\nsection. We will use Chebyshev\u2019s classical theorem on (weighted) real polynomial approxi-\nmation ([Ach56], Chapter II).\nTheorem 4.4. (Chebyshev\u2019s Theorem)[Ach56] Let f : [a, b] \u2192R be a continuous func-\ntion.\nLet s : [a, b] \u2192R be a continuous function that does not vanish on [a, b].\nThe\npolynomial r(z) of degree m that minimizes\nM(m) = max\nt\u2208[a,b] |f(z) \u2212s(z)r(z)|\nis unique, and it is characterized by the property that there exist m + 2 points a \u2264z0 <\nz1 \u00b7 \u00b7 \u00b7 < zm+1 \u2264b such that for each zi\nM(m) = |f(zi) \u2212s(zi)r(zi)|\nand the sign of the error at the zi\u2019s alternates.\nTheorem 4.5. Let a and m be as speci\ufb01ed in Section 3. There is a univariate polynomial\np(t) where deg(p) \u22642m + 1 such that:\n1. p(t) \u2208[sign(t) \u2212\u01eb2, sign(t) + \u01eb2] for all |t| \u2208[a, 1];\n2. p(t) \u2208[\u2212(1 + \u01eb2), 1 + \u01eb2] for all t \u2208[\u2212a, a];\n3. p(t) is monotonically increasing on the intervals (\u2212\u221e, \u22121] and [1, \u221e).\nProof. The polynomial p(t) is a best uniform approximation (such an approximation is guar-\nanteed to exist [Riv74]) to the function sign(t) of degree at most 2m + 1 over the domain\n[\u22121, \u2212a] \u222a[a, 1]. Applying Lemma 4.2, we get\nmax\n|t|\u2208[a,1] |p(t) \u2212sign(t)| \u2264max\n|t|\u2208[a,1] |q(t) \u2212sign(t)| \u2264\u01eb2\n15\nwhich gives Property (1).\nWe can assume that p(t) is odd (by replacing it with (p(t) \u2212p(\u2212t))/2 if needed). So we\ncan write p(t) = t \u00b7 r(t2), where r(z) is a polynomial of degree m that minimizes\nM(m) =\nmin\nr: deg(r)\u2264m sup\nz\u2208[a2,1]\n|1 \u2212\u221azr(z)|.\nInvoking Chebyshev\u2019s theorem with f(z) = 1 and s(z) = \u221az (which does not vanish\non [a2, 1]), we infer that the optimal polynomial r(z) of degree m is unique and it has an\nalternating sequence of points\na2 \u2264z0 < z1 . . . < zm+1 \u22641\nso that the error 1 \u2212\u221azr(z) achieves its maximum magnitude exactly at the points zi, and\nthe sign of the error alternates.\nSet ti = \u221azi > 0 so that\na \u2264t0 < t1 . . . < tm+1 \u22641.\nLet \u03c6(t) be the error function \u03c6(t) = p(t) \u2212sign(t). Note that for t \u2265a, we have\n\u03c6(t) = p(t) \u22121,\n\u03c6(\u2212t) = p(\u2212t) \u2212(\u22121) = \u2212p(t) + 1 = \u2212\u03c6(t).\nFor each ti, we have\n|\u03c6(ti)| = |\u03c6(\u2212ti)| = M(m).\nNow consider the interval [a, 1], on which \u03c6(t) = p(t) \u22121. Note that \u03c6\u2032(t) is well de\ufb01ned\nand equals p\u2032(t) at any point in (a, 1). The points t1, . . . , tm lie in (a, 1) and they are local\nmaxima/minima, since \u03c6(t) cannot increase in magnitude in the neighborhood of ti. Thus\n\u03c6\u2032(ti) = p\u2032(ti) = 0 for each i \u2208[m]. Similarly, we can show that \u03c6\u2032(\u2212ti) = p\u2032(\u2212ti) = 0 for\ni \u2208[m]. But deg(p\u2032) is at most 2m, and so we have located all its roots. As we now show,\nthis allows us to determine the sign of p in the intervals [\u2212\u221e, \u22121], [\u2212a, a] and [1, \u221e].\nNote that p(t1) is close to 1 whereas p(\u2212t1) is close to \u22121, and thus p increases mono-\ntonically in the interval (\u2212t1, t1) which includes [\u2212a, a]. Also t1 is a local maximum for p,\nwhich shows that the tis are maxima when i is odd, and minima when i is even. Thus, since\nm is even, p(tm) is a local minimum, so p(t) increase monotonically in the range (tm, \u221e),\nwhich includes [1, \u221e). Whereas \u2212t1 is a local minimum for p, so p(\u2212ti) are local minima for\nodd i and maxima for even i, hence p(t) is monotonically increasing in the range (\u2212\u221e, tm)\nwhich contains (\u2212\u221e, \u22121].\nUsing the polynomial p(t), we now construct the polynomial P(t) which is a good \u201cupper\u201d\napproximator to sign(t) (i.e. P(t) \u2265sign(t) for all t), thus completing the proof of Theorem\n3.5.\nTo help the reader visualize p(t), we provide a schematic representation in Figure 2. (We\nremark that, as before, this \ufb01gure is not an actual plot, but rather is intended to qualitatively\nillustrate the guarantees on the behavior of p on various intervals.)\nLet us recall the statement of Theorem 3.5:\n16\nFigure 2: Qualitative representation of polynomial p.\nTheorem 3.5. (Restated.) Let 0 < \u01eb < 0.1 and let a and K be as de\ufb01ned above. There is\na univariate polynomial P(t) such that deg(P) \u2264K with the following properties:\n(1) P(t) \u2265sign(t) \u2265\u2212P(\u2212t) for all t \u2208R;\n(2) P(t) \u2208[sign(t), sign(t) + \u01eb] for t \u2208[\u22121/2, \u22122a] S[0, 1/2];\n(3) P(t) \u2208[\u22121, 1 + \u01eb] for t \u2208(\u22122a, 0);\n(4) |P(t)| \u22642 \u00b7 (4t)K for all |t| \u22651/2.\nProof. Let p denote the polynomial of degree 2m + 1 from Theorem 4.5.\nConsider the\nfollowing polynomial:\nP(t) = 1\n2(1 + \u01eb2 + p(t + a))2 \u22121.\nNote that deg(P) = 2 deg(p) \u2264K. We now consider the behavior of P on the relevant\nintervals. We repeatedly use the inequality\n1\n2(2 + 2\u01eb2)2 \u22121 = 1 + 4\u01eb2 + 2\u01eb4 \u22641 + \u01eb\nwhich holds since \u01eb <\n1\n10. Note that P(t) \u2265\u22121 holds for all t. We now analyze the behavior\nof P(t) interval by interval:\n17\n(a) t \u2208[\u22121 \u2212a, \u22122a]. Here p(t + a) \u2208[\u22121 \u2212\u01eb2, \u22121 + \u01eb2], hence P(t) \u2208[\u22121, \u22121 + \u01eb].\n(b) t \u2208(\u22122a, 0). Here p(t + a) \u2208[\u22121 \u2212\u01eb2, 1 + \u01eb2], hence P(t) \u2208[\u22121, 1 + \u01eb].\n(c) t \u2208[0, 1 \u2212a]. Here p(t + a) \u2208[1 \u2212\u01eb2, 1 + \u01eb2], hence P(t) \u2208[1, 1 + \u01eb].\n(d) t \u2208(1 \u2212a, \u221e]. Here p(t + a) \u22651 \u2212\u01eb2, hence P(t) \u22651.\nThis shows that P(t) \u2265sign(t) for all t \u2208R. Thus we also have\nP(\u2212t) \u2265sign(\u2212t) \u21d2sign(t) \u2265\u2212P(\u2212t)\nwhich establishes Property (1). Properties (2) and (3) follow immediately from (a), (b) and\n(c) above.\nFor Property (d), we use the following standard fact from approximation theory.\nFact 4.7. [Riv74] Let a(t) be a polynomial of degree at most d for which |a(t)| \u2264b in the\ninterval [\u22121, 1]. Then |a(t)| \u2264b|2t|d for all |t| \u22651.\nTaking a(t) to be P(t/2), properties (2) and (3) give us that |P(t/2)| \u22642 for t \u2208[\u22121, 1].\nSo the fact gives |P(t/2)| < 2|2t|4m+2 for |t| \u22651, i.e. |P(t)| < 2|4t|4m+2 for |t| \u22651/2.\n5\nFooling non-regular halfspaces\nIn this section we show how to fool halfspaces that are not regular. We proceed by case\nanalysis based on the critical index of the halfspace, which we de\ufb01ne shortly. Throughout\nthis section we assume that the weights of the halfspace are decreasing:\n|w1| \u2265|w2| . . . \u2265|wn|.\nWe can assume this without loss of generality because we are going to prove that, for a\nsuitable k, any k-wise independent distribution fools such halfspaces, and the property of\nbeing k-wise independent is clearly invariant under permutation of the variables.\nSome notation: For T \u2286[n] we denote by \u03c3T the quantity \u03c3T :=\npP\ni\u2208T w2\ni . For k \u2208[n]\nwe also write \u03c3k for \u03c3{k,k+1,...,n}.\nDe\ufb01nition 5.1 (Critical index). We de\ufb01ne the \u03c4-critical index \u2113(\u03c4) of a halfspace h =\nsign(w \u00b7 x \u2212\u03b8) as the smallest index i \u2208[n] for which\n|wi| \u2264\u03c4 \u00b7 \u03c3i.\nIf this inequality does not hold for any i \u2208[n], we de\ufb01ne \u2113(\u03c4) = \u221e.\n18\nNote that a halfspace is \u03c4-regular if \u2113(\u03c4) = 1; in this section we handle the case \u2113(\u03c4) > 1.\nWe assume without loss of generality that \u01eb is su\ufb03ciently small. Given \u01eb, our threshold\nfor the critical index is\nL(\u01eb) := 8 log2(10/\u01eb)\n\u01eb2\n.\nWe argue separately depending on whether \u2113(\u01eb) > L(\u01eb) or not. Both proofs rely on the\nfollowing simple property of k-wise independent distributions.\nFact 5.2. Let D be a k-wise independent distribution over {\u22121, +1}n. Condition on any\n\ufb01xed values for any t \u2264k bits of D, and let D\u2032 be the projection of D on the other n \u2212t\nbits. Then D\u2032 is (k \u2212t)-wise independent.\nThe \ufb01rst theorem addresses the simpler case when \u2113(\u01eb) \u2264L(\u01eb).\nTheorem 5.3 (Fooling non-regular halfspaces with small critical index). Let h(x) be a half-\nspace with \u01eb-critical index \u2113(\u01eb) \u2264L(\u01eb). Then any (K(\u01eb)+L(\u01eb))-wise independent distribution\nO(\u01eb)-fools h.\nProof. Condition on any setting to the \ufb01rst \u2113\u22121 variables. Each of these de\ufb01nes a halfspace\nof the form\nh\u2032(x) = sign\n X\ni\u2265\u2113\nwixi \u2212\u03b8\u2032\n!\nwhere \u03b8\u2032 depends on the values assigned to the head. Every such halfspace is \u01eb-regular by the\nde\ufb01nition of \u01eb-critical index. Also, the conditional distribution on the remaining variables\nis K(\u01eb)-wise independent by Fact 5.2. Thus, Theorem 3.2 implies that we fool h\u2032 with error\n\u01eb. Since both the uniform distribution and D induce the same (uniform) distribution on the\n\ufb01rst \u2113\u22121 variables, an averaging argument concludes the proof of the theorem.\nIn the rest of this section we study the case of large critical index \u2113(\u01eb) > L(\u01eb), and prove\nthe following theorem.\nTheorem 5.4 (Fooling non-regular halfspaces with large critical index). Let h(x) be a half-\nspace with critical index \u2113(\u01eb) > L(\u01eb). Any (L(\u01eb) + 2)-wise independent distribution D fools h\nwith error 9\u01eb.\nTo prove Theorem 5.4 we partition the coordinate set [n] into a head H consisting of\nthe \ufb01rst L(\u01eb) coordinates, and a tail T = [n] \\ H consisting of the rest. We then show\nthat a random setting of the head variables induces with high probability a partial sum\nP\ni\u2208H wixi\u2212\u03b8 which is so large in magnitude that the values of the tail variables are essentially\nirrelevant, in the sense that they are very unlikely to change the sign of w \u00b7 x \u2212\u03b8 and hence\nthe value of the halfspace.\nWe will show that this statement holds both for the uniform distribution and for the\ndistribution D with limited independence. For the latter we will use that after restricting\n19\nthe variables in the head we still have a 2-wise independent distribution on the tail (by\nFact 5.2), which is enough for Chebyshev\u2019s concentration bound to apply. To show that the\npartial sum is likely to be large we use ideas from [Ser07], in particular that the weights\ndecrease geometrically up to the critical index.\n5.1\nProof of Theorem 5.4\nWe partition the coordinate set [n] into a head H consisting of the \ufb01rst L(\u01eb) coordinates,\nand a tail T = [n] \\ H consisting of the rest. Any \ufb01xing of the variables in H results in a\nhalfspace\nh\u2032(xT) := sign\n X\ni\u2208T\nwixi \u2212\u03b8\u2032\nH\n!\nover the tail variables xT where\n\u03b8\u2032\nH := \u03b8 \u2212\nX\ni\u2208H\nwixi.\nAs discussed before, our goal is to show that, for a random setting of the head variables,\n\u03b8\u2032\nH is likely to be so large in magnitude that the value of the tail sum P\ni\u2208T wixi is unlikely\nto in\ufb02uence the outcome of h(x). The key idea here is the following lemma from [Ser07]\nshowing that the weights decrease geometrically up to the critical index.\nLemma 5.5. For any 1 \u2264i < j \u2264\u2113+ 1 we have\n|wj| \u2264\u03c3j <\n\u0010\u221a\n1 \u2212\u01eb2\n\u0011j\u2212i\n\u03c3i \u2264\n\u0010\u221a\n1 \u2212\u01eb2\n\u0011j\u2212i\n|wi|/\u01eb.\nIn particular, if j \u2265i + (4/\u01eb2) ln(1/\u01eb) then\n|wj| \u2264|wi|/3.\nProof. For any k \u2264\u2113, we have by the de\ufb01nition of \u01eb-critical index that\nw2\nk > \u01eb2\u03c32\nk.\nHence\n\u03c32\nk+1 = \u03c32\nk \u2212w2\nk < (1 \u2212\u01eb2)\u03c32\nk.\nRepeating this calculation yields\n\u03c32\nj < (1 \u2212\u01eb2)j\u2212i\u03c32\ni .\nTo conclude the \ufb01rst chain of inequalities in the statement of the lemma, use again \u03c32\ni <\nw2\ni /\u01eb2 and the obvious inequality \u03c32\nj \u2265w2\nj. The \u201cin particular\u201d part can be veri\ufb01ed by\nstraightforward calculation, using that \u01eb is su\ufb03ciently small.\n20\nNow consider the set of\nt := log(10/\u01eb)\n\u201cnicely separated\u201d coordinates (variables)\nG := {ki := 1 + i \u00b7\n\u00004/\u01eb2\u0001\nln(1/\u01eb) : i = 0, 1, . . . , t \u22121} \u2286H.\nObserve that indeed G \u2286H because the maximum index in G is at most 1+t\u00b7(4/\u01eb2) log(1/\u01eb) \u2264\n(4/\u01eb2) log2(10/\u01eb), whereas H consists of all the \ufb01rst L(\u01eb) = (8/\u01eb2) log2(10/\u01eb) indices. The\nkey features of G are that we can apply the \u2018in particular\u201d part of Lemma 5.5 and prove the\nfollowing claim.\nClaim 5.6. \u03c3T < \u01eb|wkt|.\nProof. By our choice of L(\u01eb), t, and kt, we have\nL(\u01eb) \u2212kt \u22658 log2(10/\u01eb)/\u01eb2 \u22124 log2(10/\u01eb)/\u01eb2 \u2265log2(1/\u01eb)/\u01eb2.\nAn application of Lemma 5.5 gives\n\u03c3T <\n\u221a\n1 \u2212\u01eb2log2(1/\u01eb)/\u01eb2\n|wkt|/\u01eb \u2264\u01eb2|wkt|/\u01eb = \u01eb|wkt|\nwhere we use that \u01eb is su\ufb03ciently small.\nWe now show that a random setting of H is likely to result in a value of |\u03b8\u2032\nH| which is at\nleast |wkt|/4. The proof relies on the following claim.\nClaim 5.7. Let v1 > v2 > \u00b7 \u00b7 \u00b7 > vt > 0 be a sequence of numbers so that vi+1 \u2264vi/3. Then\nfor any two points x \u0338= y \u2208{\u22121, +1}t, we have |v \u00b7 x \u2212v \u00b7 y| \u2265vt.\nProof. Let z := x \u2212y \u2208{\u22122, 0, 2}t, which is not zero. Let j \u2264t be the smallest index such\nthat zj \u0338= 0. Then\n|v \u00b7 x \u2212v \u00b7 y| = |v \u00b7 z| = |\nX\ni\u2265j\nvizi| \u2265|vjzj| \u2212\nX\ni>j\n|vizi| \u22652(vj \u2212\nX\ni>j\nvi)\n\u22652(vj \u2212\nX\ni>j\nvj\n3i\u2212j ) \u22652(vj \u2212vj/2) = vj \u2265vt,\nusing vi \u2264vj/3i\u2212j by assumption.\nWe are now ready to show our intended lemma:\nLemma 5.8. Prxi:i\u2208H\n\u0002\f\f\u03b8 \u2212P\ni\u2208H wixi\n\f\f \u2264|wkt|/4\n\u0003\n\u2264\u01eb/10.\n21\nProof. Fix any assignment to the variables in H \\ G.\nFor this \ufb01xing, the event |\u03b8 \u2212\nP\ni\u2208H wixi| \u2264|wkt|/4 happens only if\nX\ni\u2208G\nwixi \u2208\n\uf8ee\n\uf8f0\u03b8 \u2212\nX\ni\u2208H\\G\nwixi \u2212|wkt|/4, \u03b8 \u2212\nX\ni\u2208H\\G\nwixi + |wkt|/4\n\uf8f9\n\uf8fb,\ni.e., P\ni\u2208G wixi falls in an interval of length |wkt|/2. Applying Claim 5.7 to the weights in\nG, any two possible outcomes of P\ni\u2208G wixi di\ufb00er by at least |wkt|. So there is at most one\nsetting xk1 = a1, . . . , xkt = at of the variables in G for which this event occurs. This setting\nhas probability at most 2\u2212t = \u01eb/10.\nWith this lemma in hand, we can show that limited independence su\ufb03ces to fool halfs-\npaces with a large critical index.\nProof of Theorem 5.4. We compare the behavior of h(x) on D and the uniform distribution\nU. In either case, the marginal distribution for the variables in H is uniform. For each setting\nof these variables, we are left with a halfspace of the form h\u2032(xT) = sign(P\ni\u2208T wixi \u2212\u03b8\u2032\nH) on\nthe variables in T. The combination of Lemma 5.8 and Claim 5.6 shows that with probability\nat least 1 \u2212\u01eb/10 we have\n|\u03b8 \u2212\nX\ni\u2208H\nwi \u00b7 xi| \u2265|wkt|\n4\n\u2265\u03c3T\n4\u01eb .\n(\u22c6)\nWe condition on this event (\u22c6). Consider the projections U\u2032 and D\u2032 of U and D on xT .\nBy Fact 5.2, D\u2032 is 2-wise independent. We now argue that for both U\u2032 and D\u2032, it is very\nlikely that h\u2032(xT) = \u2212sign(\u03b8\u2032\nH) (for small enough \u01eb). Indeed if this does not happen, then\nwe have\n|\nX\ni\u2208T\nwixi| \u2265|\u03b8 \u2212\nX\ni\u2208H\nwi \u00b7 xi| \u2265|wkt|\n4\n\u2265\u03c3T\n4\u01eb .\nUnder the uniform distribution, by a Hoe\ufb00ding bound (Theorem 2.5), the probability of\nthis event is bounded by\nPr\nx\u223cU\u2032\n\"\f\f\f\f\f\nX\ni\u2208T\nwixi\n\f\f\f\f\f \u2265\u03c3T\n4\u01eb\n#\n\u22642e\u2212\n1\n32\u01eb2 \u226a4\u01eb.\nWhile by Chebyshev\u2019s inequality (Theorem 2.6) we get\nPr\nx\u223cD\u2032\n\"\f\f\f\f\f\nX\ni\u2208T\nwixi\n\f\f\f\f\f \u2265\u03c3T\n4\u01eb\n#\n\u226416\u01eb2 \u22644\u01eb.\nThus, we have\n| ED\u2032[h\u2032(xT)] \u2212EU\u2032[h\u2032(xT)]| \u22642| Pr\nD\u2032 [h\u2032(xT ) = \u2212sign(\u03b8\u2032\nH)] \u2212Pr\nU\u2032 [h\u2032(xT ) = \u2212sign(\u03b8\u2032\nH)]| \u22648\u01eb.\n22\nTo conclude, our goal was to bound from above | EU[h(x)] \u2212ED[h(x)]|. Using the fact\nthat both distributions induce the uniform distribution on variables in H, and conditioning\non the event (\u22c6), we get\n| EU[h(x)] \u2212ED[h(x)]| \u22648\u01eb + 2 \u00b7 \u01eb/10 < 9\u01eb.\nOur main result, Theorem 1.2, follows immediately from Theorem 3.2, Theorem 5.3 and\nTheorem 5.4.\n5.2\nProof of the main theorem\nFor completeness in this section we summarize what is needed to prove our main theorem.\nTheorem 1.2 (Main). (Restated.) Let D be a k-wise independent distribution on {\u22121, +1}n,\nand let h : {\u22121, +1}n \u2192{\u22121, +1} be a halfspace. Then D fools h with error \u01eb, i.e.,\n| Ex\u2190D[h(x)] \u2212Ex\u2190U[h(x)]| \u2264\u01eb,\nprovided\nk \u2265C\n\u01eb2 log2\n\u00121\n\u01eb\n\u0013\n,\nwhere C is an absolute constant and U is the uniform distribution over {\u22121, +1}n.\nProof. Consider the parameters K(\u01eb), L(\u01eb) de\ufb01ned in Sections 3 and 5, respectively, and\nrecall that they are both O(log2(1/\u01eb)/\u01eb2). For a given halfspace, consider its critical index\n\u2113. If \u2113\u2264L(\u01eb) we apply Theorem 5.3, otherwise we apply Theorem 5.4.\n6\nConclusions\nWe feel that Theorem 1.2 is of independent interest and may \ufb01nd other applications aside\nfrom pseudorandomness.\nFor instance, consider the problem of estimating the in\ufb02uence\nof a variable in a halfspace [GR09].\nIt is easy to verify that for any halfspace h(x) =\nsign(Pn\ni=1 wixi \u2212\u03b8) the in\ufb02uence of the i-th variable equals Ey\u2190U[h\u2032(y)], where h\u2032 is the\nhalfspace de\ufb01ned by h\u2032(y) = sign(P\nj,j\u0338=i wjyj \u2212\u03b8yi + wi). Thus, one can use \u02dcO(\u01eb\u22122)-wise\nindependence to estimate the in\ufb02uence to within an additive \u01eb. Note that, for any halfspace,\nthe bias and the in\ufb02uences together are (respectively) the Fourier coe\ufb03cients at levels 0\nand 1. They are collectively called the Chow parameters of a halfspace (after a theorem of\nC.K. Chow showing that these numbers uniquely specify the halfspace [Cho61]) and have\nbeen well-studied in the literature [Gol06, Ser07, OS08, MORS09]. Our result implies that\nthe Chow parameters of a halfspace can be estimated to within accuracy \u01eb using bounded\nindependence.\nOur results, together with the lower bound of [BGGP07], are essentially optimal in terms\nof characterizing the degree of independence that is required to \u01eb-fool halfspaces. However,\nmany natural and interesting directions remain for future work.\n23\nOne obvious goal is to construct unconditional pseudorandom generators for halfspaces\nthat have a better dependence on \u01eb than our construction. The ultimate goal here is to\nachieve the information-theoretic optimal possible seed length, i.e. s = O(log(n/\u01eb)).\nAnother natural, though perhaps challenging, goal is to understand the degree of inde-\npendence that is required to \u01eb-fool degree-d polynomial threshold functions over {\u22121, +1}n.\nFor constant \u01eb and constant d, does \u0398(1)-wise independence su\ufb03ce to fool degree-d PTFs?\nAs far as we know nothing is known about this question, even for d = 2.\nA third question that is related to our work is whether it is possible to derandomize\nthe problem of approximately counting the number of satisfying assignments for a given\nhalfspace. Our results give a single \ufb01xed and easily constructible set of n \u02dcO(1/\u01eb2) many points\nwhich can be used to deterministically obtain a \u00b1\u01eb-accurate estimate of PrU[f(x) = 1] for\nany halfspace f in time n \u02dcO(1/\u01eb2). However, there is a deterministic algorithm of [Ser07] which\ntakes integer weights and threshold w1, . . . , wn, \u03b8 (each poly(n) bits long) as input and runs\nin time poly(n) \u00b7 2 \u02dcO(1/\u01eb2). Can a poly(n, 1/\u01eb)-time deterministic algorithm be obtained?\nAcknowledgements.\nRocco Servedio thanks Troy Lee for a helpful conversation about\namplifying polynomials and Adam Klivans for useful conversations about Jackson\u2019s Theorem.\nParikshit Gopalan would like to thank Jaikumar Radhakrishnan and Amir Shpilka for many\nstimulating discussions about this problem.\nReferences\n[ABI86]\nN. Alon, L. Babai, and A. Itai. A fast and simple randomized algorithm for the\nmaximal independent set problem. Journal of Algorithms, 7:567\u2013583, 1986. 2\n[Ach56]\nN.I. Achieser. Theory of Approximation. Frederik Ungar Publishing Co, New\nYork, 1956. 15\n[AGHP92] Noga Alon, Oded Goldreich, Johan H\u02daastad, and Ren\u00b4e Peralta. Simple construc-\ntions of almost k-wise independent random variables.\nRandom Structures &\nAlgorithms, 3(3):289\u2013304, 1992. 2\n[AW]\nM. Ajtai and A. Wigderson. Deterministic simulation of probabilistic constant\ndepth circuits.\nIn Proc. 26th IEEE Symposium on Foundations of Computer\nScience (FOCS). 2\n[Baz07]\nLouay Bazzi.\nPolylogarithmic independence can fool DNF formulas. In 48th\nAnnual IEEE Symposium on Foundations of Computer Science (FOCS 2007).\nIEEE Computer Society, 2007. 2, 3, 8\n[Bei94]\nR. Beigel. Perceptrons, PP, and the Polynomial Hierarchy. Computational Com-\nplexity, 4:339\u2013349, 1994. 2\n24\n[BGGP07] I.\nBenjamini,\nO.\nGurel-Gurevich,\nand\nR.\nPeled.\nOn\nk-wise\nin-\ndependent\ndistributions\nand\nboolean\nfunctions.\navailable\nat\nhttp://www.wisdom.weizmann.ac.il/ origurel/, 2007. 1, 2, 5, 23\n[Bra09]\nM. Braverman. Poly-logarithmic independence fools AC0 circuits. Available at\nhttp://www.cs.toronto.edu/mbraverm/, 2009. 2\n[BV07]\nAndrej Bogdanov and Emanuele Viola. Pseudorandom bits for polynomials. In\n48th Annual Symposium on Foundations of Computer Science (FOCS), pages\n41\u201351. IEEE, 2007. 2\n[CG89]\nBenny Chor and Oded Goldreich. On the power of two-point based sampling.\nJournal of Complexity, 5(1):96\u2013106, March 1989. 2\n[CGH+85] B. Chor, O. Goldreich, J. Hastad, J. Friedman, S. Rudich, and R. Smolensky.\nThe bit extraction problem and t-resilient functions. In 26th Annual Symposium\non Foundations of Computer Science, pages 396\u2013407, Portland, Oregon, 21\u201323\nOctober 1985. IEEE. 2\n[Che66]\nE. Cheney. Introduction to approximation theory. McGraw-Hill, New York, New\nYork, 1966. 14\n[Cho61]\nC.K. Chow. On the characterization of threshold functions. In Proceedings of\nthe Symposium on Switching Circuit Theory and Logical Design (FOCS), pages\n34\u201338, 1961. 23\n[Der65]\nM. Dertouzos. Threshold logic: a synthesis approach. MIT Press, Cambridge,\nMA, 1965. 1\n[DS79]\nP. Dubey and L.S. Shapley. Mathematical properties of the banzhaf power index.\nMathematics of Operations Research, 4:99\u2013131, 1979. 1\n[DS09]\nI. Diakonikolas and R. Servedio.\nImproved approximation of linear threshold\nfunctions. Manuscript, 2009. 3\n[EY07]\nA. Eremenko and P. Yuditskii. Uniform approximation of sgn(x) by polynomials\nand entire functions. J. d\u2019Analyse Math., 12:313\u2013324, 2007. 4, 5, 13\n[FKL+01] J. Forster, M. Krause, S.V. Lokam, R. Mubarakzjanov, N. Schmitt, and H.-U.\nSimon. Relations between communication complexity, linear arrangements, and\ncomputational complexity. In FSTTCS, pages 171\u2013182, 2001. 1\n[GHR92]\nM. Goldmann, J. H\u02daastad, and A. Razborov. Majority gates vs. general weighted\nthreshold gates. Computational Complexity, 2:277\u2013300, 1992. 1, 2\n[GK98]\nM. Goldmann and M. Karpinski. Simulating threshold circuits by majority cir-\ncuits. SIAM Journal on Computing, 27(1):230\u2013246, 1998. 1\n25\n[Gol06]\nP. Goldberg. A Bound on the Precision Required to Estimate a Boolean Per-\nceptron from its Average Satisfying Assignment.\nSIAM Journal on Discrete\nMathematics, 20:328\u2013343, 2006. 23\n[GR09]\nP. Gopalan and J. Radhakrishnan.\nFinding duplicates in a data stream.\nIn\nProc. 20th Annual Symposium on Discrete Algorithms (SODA\u201909), pages 402\u2013\n411, 2009. 3, 23\n[H\u02daas94]\nJ. H\u02daastad. On the size of weights for threshold gates. SIAM Journal on Discrete\nMathematics, 7(3):484\u2013492, 1994. 2\n[HMP+93] A. Hajnal, W. Maass, P. Pudlak, M. Szegedy, and G. Turan. Threshold circuits\nof bounded depth. Journal of Computer and System Sciences, 46:129\u2013154, 1993.\n1\n[Hu65]\nS.T. Hu. Threshold Logic. University of California Press, 1965. 1\n[Isb69]\nJ.R. Isbell. A Counterexample in Weighted Majority Games. Proceedings of the\nAMS, 20(2):590\u2013592, 1969. 1\n[Kra91]\nM. Krause. Geometric arguments yield better bounds for threshold circuits and\ndistributed computing. In Proc. 6th Structure in Complexity Theory Conference,\npages 314\u2013322, 1991. 1\n[KS07]\nAdam R. Klivans and Alexander A. Sherstov. A lower bound for agnostically\nlearning disjunctions. In Conference on Learning Theory (COLT\u201907), pages 409\u2013\n423, 2007. 4\n[KW91]\nM. Krause and S. Waack. Variation ranks of communication matrices and lower\nbounds for depth two circuits having symmetric gates with unbounded fanin.\nIn Proc. 32nd IEEE Symposium on Foundations of Computer Science (FOCS),\npages 777\u2013782, 1991. 1\n[LC67]\nP.M. Lewis and C.L. Coates. Threshold Logic. New York, Wiley, 1967. 1\n[LN90]\nN. Linial and N. Nisan.\nApproximate inclusion-exclusion.\nCombinatorica,\n10(4):349\u2013365, 1990. 2\n[Lov08]\nShachar Lovett. Unconditional pseudorandom generators for low degree polyno-\nmials. In 40th Annual Symposium on the Theory of Computing (STOC), pages\n557\u2013562. ACM, 2008. 2\n[LVW93]\nMichael Luby, Boban Velickovic, and Avi Wigderson. Deterministic approximate\ncounting of depth-2 circuits. In Proceedings of the 2nd ISTCS, pages 18\u201324, 1993.\n2\n26\n[MORS09] K. Matulef, R. O\u2019Donnell, R. Rubinfeld, and R. Servedio. Testing halfspaces. In\nProc. 20th Annual Symposium on Discrete Algorithms (SODA), 2009. 3, 23\n[MT94]\nW. Maass and G. Turan. How fast can a threshold gate learn?, pages 381\u2013414.\nMIT Press, 1994. 2\n[Mur71]\nS. Muroga. Threshold logic and its applications. Wiley-Interscience, New York,\n1971. 1\n[Nis91]\nNoam Nisan. Pseudorandom bits for constant depth circuits. Combinatorica,\n11(1):63\u201370, 1991. 2\n[Nis92]\nNoam Nisan. Pseudorandom generators for space-bounded computation. Com-\nbinatorica, 12(4):449\u2013461, 1992. 2, 3\n[NN93]\nJ. Naor and M. Naor.\nSmall-bias probability spaces:\ne\ufb03cient constructions\nand applications. SIAM J. on Comput., 22(4):838\u2013856, 1993. Earlier version\nin STOC\u201990. 2\n[OS08]\nR. O\u2019Donnell and R. Servedio. The Chow Parameters Problem. In Proc. 40th\nAnnual ACM Symposium on Theory of Computing (STOC), pages 517\u2013526, 2008.\n3, 23\n[Pat92]\nR. Paturi. On the degree of polynomials that approximate symmetric Boolean\nfunctions. In Proceedings of the 24th Symposium on Theory of Computing, pages\n468\u2013474, 1992. 4\n[Pen46]\nL.S. Penrose. The elementary statistics of majority voting. Journal of the Royal\nStatistical Society, 109(1):53\u201357, 1946. 1\n[Raz08]\nA. Razborov. A simple proof of bazzi\u2019s theorem. available at http://eccc.hpi-\nweb.de/eccc-reports/2008/TR08-081/index.html, 2008. 2\n[Riv74]\nTheodore J. Rivlin. The Chebyshev Polynomials. John Wiley and Sons, 1974.\n15, 18\n[RS08]\nY.\nRabani\nand\nA.\nShpilka.\nExplicit\nconstruction\nof\na\nsmall\nepsilon-net\nfor\nlinear\nthreshold\nfunctions.\nAvailable\nat\nhttp://www.cs.technion.ac.il/shpilka/publications.html, 2008. 2, 3\n[Ser07]\nR. Servedio.\nEvery linear threshold function has a low-weight approximator.\nComputational Complexity, 16(2):180\u2013209, 2007. 3, 20, 23, 24\n[She69]\nQ. Sheng. Threshold Logic. London, New York, Academic Press, 1969. 1\n[Shi86]\nI.S. Shiganov. Re\ufb01nement of the upper bound of the constant in the central limit\ntheorem. Journal of Soviet Mathematics, pages 2545\u20132550, 1986. 6\n27\n[TZ92]\nA. Taylor and W. Zwicker. A Characterization of Weighted Voting. Proceedings\nof the AMS, 115(4):1089\u20131094, 1992. 1\n[Vio07]\nEmanuele Viola. Pseudorandom bits for constant-depth circuits with few arbi-\ntrary symmetric gates.\nSIAM Journal on Computing, 36(5):1387\u20131403, 2007.\n2\n[Vio08]\nEmanuele Viola. The sum of d small-bias generators fools polynomials of degree\nd. In 23nd Annual Conference on Computational Complexity. IEEE, June 23\u201326\n2008. http://www.ccs.neu.edu/home/viola/. 2\n28\n",
        "sentence": "",
        "context": "HR0011-08-1-0069. Email: rocco@cs.columbia.edu\n\u00b6This work was partially done while the author was a postdoctoral fellow at Columbia University, sup-\nported by grants NSF award CCF-0347282 and NSF award CCF-0523664. Email: viola@ccs.neu.edu\n1\nIntroduction\ndation Fellowship. Email: ilias@cs.columbia.edu\n\u2020Email: parik@microsoft.com\n\u2021Research supported by DARPA award HR0011-08-1-0069. Email: rjaiswal@cs.columbia.edu\n\u00a7Supported in part by NSF grants CCF-0347282, CCF-0523664 and CNS-0716245, and by DARPA award\n3.5.\nTo help the reader visualize p(t), we provide a schematic representation in Figure 2. (We\nremark that, as before, this \ufb01gure is not an actual plot, but rather is intended to qualitatively"
    },
    {
        "title": "The complexity of learning halfspaces using generalized linear methods",
        "author": [
            "Amit Daniely",
            "Nati Linial",
            "Shai Shalev-Shwartz"
        ],
        "venue": "CoRR, abs/1211.0616,",
        "citeRegEx": "Daniely et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Daniely et al\\.",
        "year": 2014,
        "abstract": "Many popular learning algorithms (E.g. Regression, Fourier-Transform based\nalgorithms, Kernel SVM and Kernel ridge regression) operate by reducing the\nproblem to a convex optimization problem over a vector space of functions.\nThese methods offer the currently best approach to several central problems\nsuch as learning half spaces and learning DNF's. In addition they are widely\nused in numerous application domains. Despite their importance, there are still\nvery few proof techniques to show limits on the power of these algorithms.\n  We study the performance of this approach in the problem of (agnostically and\nimproperly) learning halfspaces with margin $\\gamma$. Let $\\mathcal{D}$ be a\ndistribution over labeled examples. The $\\gamma$-margin error of a hyperplane\n$h$ is the probability of an example to fall on the wrong side of $h$ or at a\ndistance $\\le\\gamma$ from it. The $\\gamma$-margin error of the best $h$ is\ndenoted $\\mathrm{Err}_\\gamma(\\mathcal{D})$. An $\\alpha(\\gamma)$-approximation\nalgorithm receives $\\gamma,\\epsilon$ as input and, using i.i.d. samples of\n$\\mathcal{D}$, outputs a classifier with error rate $\\le\n\\alpha(\\gamma)\\mathrm{Err}_\\gamma(\\mathcal{D}) + \\epsilon$. Such an algorithm\nis efficient if it uses $\\mathrm{poly}(\\frac{1}{\\gamma},\\frac{1}{\\epsilon})$\nsamples and runs in time polynomial in the sample size.\n  The best approximation ratio achievable by an efficient algorithm is\n$O\\left(\\frac{1/\\gamma}{\\sqrt{\\log(1/\\gamma)}}\\right)$ and is achieved using an\nalgorithm from the above class. Our main result shows that the approximation\nratio of every efficient algorithm from this family must be $\\ge\n\\Omega\\left(\\frac{1/\\gamma}{\\mathrm{poly}\\left(\\log\\left(1/\\gamma\\right)\\right)}\\right)$,\nessentially matching the best known upper bound.",
        "full_text": "The complexity of learning halfspaces using generalized\nlinear methods\nAmit Daniely \u2217\nNati Linial \u2020\nShai Shalev-Shwartz\u2021\nMay 13, 2014\nAbstract\nMany popular learning algorithms (E.g. Regression, Fourier-Transform based al-\ngorithms, Kernel SVM and Kernel ridge regression) operate by reducing the problem\nto a convex optimization problem over a set of functions. These methods o\ufb00er the\ncurrently best approach to several central problems such as learning half spaces and\nlearning DNF\u2019s. In addition they are widely used in numerous application domains.\nDespite their importance, there are still very few proof techniques to show limits on\nthe power of these algorithms.\nWe study the performance of this approach in the problem of (agnostically and\nimproperly) learning halfspaces with margin \u03b3. Let D be a distribution over labeled\nexamples. The \u03b3-margin error of a hyperplane h is the probability of an example to\nfall on the wrong side of h or at a distance \u2264\u03b3 from it. The \u03b3-margin error of the best\nh is denoted Err\u03b3(D). An \u03b1(\u03b3)-approximation algorithm receives \u03b3, \u03f5 as input and,\nusing i.i.d. samples of D, outputs a classi\ufb01er with error rate \u2264\u03b1(\u03b3) Err\u03b3(D) + \u03f5. Such\nan algorithm is e\ufb03cient if it uses poly( 1\n\u03b3 , 1\n\u03f5) samples and runs in time polynomial in\nthe sample size.\nThe best approximation ratio achievable by an e\ufb03cient algorithm is O\n\u0012\n1/\u03b3\n\u221a\nlog(1/\u03b3)\n\u0013\nand is achieved using an algorithm from the above class.\nOur main result shows\nthat the approximation ratio of every e\ufb03cient algorithm from this family must be\n\u2265\u2126\n\u0010\n1/\u03b3\npoly(log(1/\u03b3))\n\u0011\n, essentially matching the best known upper bound.\n\u2217Department of Mathematics, Hebrew University, Jerusalem 91904, Israel. amit.daniely@mail.huji.ac.il\n\u2020School\nof\nComputer\nScience\nand\nEngineering,\nHebrew\nUniversity,\nJerusalem\n91904,\nIsrael.\nnati@cs.huji.ac.il\n\u2021School\nof\nComputer\nScience\nand\nEngineering,\nHebrew\nUniversity,\nJerusalem\n91904,\nIsrael.\nshais@cs.huji.ac.il\narXiv:1211.0616v4  [cs.LG]  10 May 2014\n1\nIntroduction\nLet X be some set and let D be a distribution on X \u00d7 {\u00b11}.\nThe basic learning\ntask is, based on an i.i.d.\nsample, to \ufb01nd a function f : X\n\u2192{\u00b11} whose error,\nErrD,0\u22121(f) := Pr(X,Y )\u223cD (f(X) \u0338= Y ), is as small as possible. A learning problem is de\ufb01ned\nby specifying a class H of competitors (e.g. H is a class of functions from X to {\u00b11}).\nGiven such a class, the corresponding learning problem is to \ufb01nd f : X \u2192{\u00b11} whose error\nis small relatively to the error of the best competitor in H. Ignoring computational aspects,\nthe celebrated PAC/VC theory essentially tells us that the best algorithm for every learning\nproblem is an Empirical Risk Minimizer (=ERM) \u2013 namely, one that returns the competitor\nin H of least empirical error. Unfortunately, for many learning problems, implementing the\nERM paradigm is NP-hard and even NP-hard to approximate.\nWe consider here a very popular family of algorithms to cope with this hardness, which we\ncollectively call \u201cthe generalized linear family\u201d. It proceeds as follows: \ufb01x some set W \u2282RX\nand return a function of the form f(x) = sign(g(x) \u2212b) where the pair (g, b) \u2208W \u00d7 R\nempirically minimizes some convex loss. In order that such a method be useful, the set W\nshould be \u201csmall\u201d (to prevent over\ufb01tting) and \u201cnicely behaved\u201d (to make the optimization\nproblem computationally feasible). The two main choices for such a set W are\n\u2022 A (usually convex) subset of a \ufb01nite dimensional space of functions (e.g. if X \u2282Rd\nthen W can be the space of all polynomials of degree \u226417 and coe\ufb03cients bounded\nby d3). We refer to such algorithms as \ufb01nite dimensional learners.\n\u2022 A ball in a reproducing kernel Hilbet space. We refer to such algorithms as as kernel\nbased learners.\nThe generalized linear family has been applied extensively to tackle learning problems\n(e.g. Linial et al. (1989), Kushilevitz and Mansour (1991), Klivans and Servedio (2001),\nKalai et al. (2005), Blais et al. (2008), Shalev-Shwartz et al. (2011) \u2013 see section 1.4).\nTheir statistical charactersitics have been thoroughly studied as well (Vapnik, 1998, Anthony\nand Bartlet, 1999, Sch\u00a8olkopf et al., 1998, Cristianini and Shawe-Taylor, 2000, Steinwart\nand Christmann, 2008). Moreover, the signi\ufb01cance of this approach is by no means only\ntheoretical \u2013 algorithms from this family are widely used by practitioners.\nIn spite of all that, very few lower bounds are known on the performance of this family of\nalgorithms (i.e., theorems of the form \u201cFor every kernel-based/\ufb01nite-dimensional algorithm\nfor the learning problem X, there exists a distribution under which the algorithm performs\npoorly\u201d). Such a lower bound must quantify over all possible choices of \u201csmall and nicely\nbehaved\u201d sets W. In order to address this di\ufb03culty we employ a variety of mathematical\nmethods some of which are new in this domain. In particular, we make intensive use of\nharmonic analysis on the sphere, reproducing kernel Hilbert spaces, orthogonal polynomials,\nJohn\u2019s Lemma as well as a new symmetrization technique.\nWe also prove a new result, which is of independent interest: a fundamental fact that\nstands behind the theoretical analysis of kernel based learners is that for every subset X of\na unit ball in a Hilbert space H, it is possible to learn a\ufb03ne functionals of norm \u2264C over\nX w.r.t. the hinge loss using C2\n\u03f52 examples. We show a (weak) inverse of this fact. Namely,\nwe show that for every X, if a\ufb03ne functionals can be learnt using m examples, then there\n1\nexists an equivalent inner product on H under which X is contained in a unit ball, and the\na\ufb03ne functional retuned by any learning algorithm must have norm \u2264O (m3).\nOur lower bounds are established for the basic problem of learning large margin halfspaces\n(to be de\ufb01ned precisely in Section 1.1). The best known e\ufb03cient (in 1\n\u03b3) algorithm for this\nproblem (Birnbaum and Shalev-Shwartz, 2012) is a kernel based learner that achieves an\napproximation ratio of\n1/\u03b3\n\u221a\nlog(1/\u03b3). (We note, however, that this approximation ratio was \ufb01rst\nobtained by (Long and Servedio, 2011) using a \u201cboosting based\u201d algorithm that does not\nbelong to the generalized linear family). The best known exact algorithm (that is, \u03b1(\u03b3) = 1),\nis also a kernel based learner and runs in time exp\n\u0010\n\u0398\n\u0010\n1\n\u03b3 log\n\u0010\n1\n\u03b3\n\u0011\u0011\u0011\n(Shalev-Shwartz et al.,\n2011).\nOur main results show that e\ufb03cient kernel based learners cannot achieve better approxi-\nmation ratio than \u2126\n\u0010\n1/\u03b3\npoly(log(1/\u03b3))\n\u0011\n, essentially matching the best known upper bound. Also,\nwe show that e\ufb03cient \ufb01nite dimensional learners cannot achieve better approximation ratio\nthan \u2126\n\u0010\n1/\u221a\u03b3\npoly(log(1/\u03b3))\n\u0011\n. In addition we show that the running time of kernel based learners\nwith approximation ratio of\n\u0010\n1\n\u03b3\n\u00111\u2212\u03f5\nas well as of \ufb01nite dimensional learners with approxi-\nmation ratio of\n\u0010\n1\n\u03b3\n\u0011 1\n2 \u2212\u03f5\nmust be exponential in 1/\u03b3.\nNext, we formulate the problem of learning large margin halfspaces and survey some rel-\nevant background to motivate our de\ufb01nitions of kernel-based and \ufb01nite dimensional learners\ngiven in Section 2.\n1.1\nLearning large margin halfspaces\nWe view Rd as a subspace of the Hilbert space H = \u21132 corresponding to the \ufb01rst d coordinates.\nSince the notion of margin is de\ufb01ned relative to a suitable scaling of the examples, we consider\nthroughout only distributions that are supported in the unit ball, B, of H. Also, all the\ndistributions we consider are supported in Rd for some d < \u221e. We denote by Sd\u22121 the unit\nsphere of Rd.\nIt will be convenient to use loss functions. A loss function is any function l : R \u2192[0, \u221e).\nGiven a loss function l and f : B \u2192R, we denote ErrD,l(f) = E(x,y)\u223cD(l(yf(x))). Two loss\nfunctions of particular relevance are the 0 \u22121 loss function, l0\u22121(x) =\n(\n1\nx \u22640\n0\nx > 0, and the\n\u03b3-margin loss function, l\u03b3(x) =\n(\n1\nx \u2264\u03b3\n0\nx > \u03b3 . We use shorthands such as ErrD,0\u22121 instead of\nErrD,l0\u22121.\nA halfspace, parameterized by w \u2208B and b \u2208R, is the classi\ufb01er f(x) = sign(\u039bw,b(x)),\nwhere \u039bw,b(x) := \u27e8w, x\u27e9+ b. Given a distribution D over B \u00d7 {\u00b11}, the error rate of \u039bw,b is\nErrD,0\u22121(\u039bw,b) =\nPr\n(x,y)\u223cD (sign(\u039bw,b(x)) \u0338= y) =\nPr\n(x,y)\u223cD (y\u039bw,b(x) \u22640) .\nThe \u03b3-margin error rate of \u039bw,b is\nErrD,\u03b3(\u039bw,b) =\nPr\n(x,y)\u223cD (y\u039bw,b(x) \u2264\u03b3) .\n2\nNote that if \u2225w\u2225= 1 then |\u039bw,b(x)| is the distance of x from the separating hyperplane.\nTherefore, the \u03b3-margin error rate is the probability of x to either be in the wrong side of\nthe hyperplane or to be at a distance of at most \u03b3 from the hyperplane. The least \u03b3-margin\nerror rate of a halfspace classi\ufb01er is denoted Err\u03b3(D) = minw\u2208B,b\u2208R ErrD,\u03b3(\u039bw,b).\nA learning algorithm receives \u03b3, \u03f5 and access to i.i.d. samples from D. The algorithm\nshould return a classi\ufb01er (which need not be an a\ufb03ne function). We say that the algorithm\nhas approximation ratio \u03b1(\u03b3) if for every \u03b3, \u03f5 and for every distribution, it outputs (w.h.p.\nover the i.i.d.\nD-samples) a classi\ufb01er with error rate \u2264\u03b1(\u03b3) Err\u03b3(D) + \u03f5.\nAn e\ufb03cient\nalgorithm uses poly(1/\u03b3, 1/\u03f5) samples, runs in time polynomial in the size of the sample1\nand outputs a classi\ufb01er f such that f(x) can be evaluated in time polynomial in the sample\nsize.\n1.2\nKernel-SVM and kernel-based learners\nThe SVM paradigm, introduced by Vapnik is inspired by the idea of separation with margin.\nFor the reader\u2019s convenience we \ufb01rst describe the basic (kernel-free) variant of SVM. It is\nwell known (e.g. Anthony and Bartlet (1999)) that the a\ufb03ne function that minimizes the\nempirical \u03b3-margin error rate over an i.i.d.\nsample of size poly(1/\u03b3, 1/\u03f5) has error rate\n\u2264Err\u03b3(D) + \u03f5.\nHowever, this minimization problem is NP-hard and even NP-hard to\napproximate (Guruswami and Raghavendra, 2006, Feldman et al., 2006).\nSVM deals with this hardness by replacing the margin loss with a convex surrogate loss,\nin particular, the hinge loss2 lhinge(x) = (1 \u2212x)+. Note that for x \u2208[\u22122, 2],\nl0\u22121(x) \u2264lhinge(x/\u03b3) \u2264(1 + 2/\u03b3)l\u03b3(x) ,\nfrom which it easily follows that by solving\nmin\nw,b\nErrD,hinge\n\u0010\n1\n\u03b3 \u039bw,b\n\u0011\ns.t.\nw \u2208H, b \u2208R,\n\u2225w\u2225H \u22641\nwe obtain an approximation ratio of \u03b1(\u03b3) = 1 + 2/\u03b3. It is more convenient to consider the\nproblem\nmin\nw,b\nErrD,hinge (\u039bw,b)\ns.t.\nw \u2208H, b \u2208R,\n\u2225w\u2225H \u2264C ,\n(1)\nwhich is equivalent for C = 1\n\u03b3. The basic (kernel-free) variant of SVM essentially solves Prob-\nlem (1), which can be approximated, up to an additive error of \u03f5, by an e\ufb03cient algorithm\nrunning on a sample of size poly( 1\n\u03b3, 1\n\u03f5).\nKernel-free SVM minimizes the hinge loss over the space of a\ufb03ne functionals of bounded\nnorm. The family of Kernel-SVM algorithms is obtained by replacing the space of a\ufb03ne\nfunctionals with other, possibly much larger, spaces (e.g., a polynomial kernel of degree t\nextends the repertoire of possible output functions from a\ufb03ne functionals to all polynomials\nof degree at most t). This is accomplished by embedding B into the unit ball of another\nHilbert space on which we apply basic-SVM. Concretely, let \u03c8 : B \u2192B1, where B1 is the\nunit ball of a Hilbert space H1. The embedding \u03c8 need not be computed directly. Rather, it\n1The size of a vector x \u2208H is taken to be the largest index j for which xj \u0338= 0.\n2As usual, z+ := max(z, 0).\n3\nis enough that we can e\ufb03ciently compute the corresponding kernel, k(x, y) := \u27e8\u03c8(x), \u03c8(y)\u27e9H1\n(this property, sometimes crucial, is called the kernel trick). It remains to solve the following\nproblem\nmin\nw,b\nErrD,hinge (\u039bw,b \u25e6\u03c8)\ns.t.\nw \u2208H1, b \u2208R, \u2225w\u2225H1 \u2264C .\n(2)\nThis problem can be approximated, up to an additive error of \u03f5, using poly(C/\u03f5) samples\nand time. We prove lower bounds to all approximate solutions of program (2). In fact, our\nresults work with arbitrary (not just hinge loss) convex surrogate losses and arbitrary (not\njust e\ufb03ciently computable) kernels.\nAlthough we formulate our results for Problem (2), they apply as well to the following\ncommonly used formulation of the kernel SVM problem, where the constraint \u2225w\u2225H1 \u2264C is\nreplaced by a regularization term. Namely\nmin\nw\u2208H1,b\u2208R\n1\nC2\u2225w\u22252\nH1 + ErrD,hinge (\u039bw,b \u25e6\u03c8)\n(3)\nThe optimum of program (3) is \u22641 as shown by the zero solution w = 0, b = 0. Thus, if\nw, b is an approximate optimal solution, then\n\u2225w\u22252\nH1\nC2\n\u22642 \u21d2\u2225w\u2225H1 \u22642C. This observation\nmakes it easy to modify our results on program (2) to apply to program (3).\n1.3\nFinite dimensional learners\nThe SVM algorithms embed the data in a (possibly in\ufb01nite dimensional) Hilbert space,\nand minimize the hinge loss over all a\ufb03ne functionals of bounded norm. The kernel trick\nsometimes allows us to work in in\ufb01nite dimensional Hilbert spaces. Even without it, we can\nstill embed the data in Rm for some m, and minimize a convex loss over a collection of a\ufb03ne\nfunctionals. For example, some algorithms do not constraint the a\ufb03ne functional, while in\nthe Lasso method (Tibshirani, 1996) the a\ufb03ne functional (represented as a vector in Rm)\nmust have small L1-norm.\nWithout the kernel trick, such algorithms work directly in Rm. Thus, every algorithm\nmust have time complexity \u2126(m), and therefore m is a lower bound on the complexity of\nthe algorithm. In this work we will lower bound the performance of any algorithm with\nm \u2264poly (1/\u03b3).\nConcretely, we prove lower bounds for any approximate solution to a\nproblem of the form\nmin\nw,b\nErrD,l (\u039bw,b \u25e6\u03c8)\ns.t.\nw \u2208W \u2282Rm, b \u2208R ,\n(4)\nwhere l is some surrogate loss function (see formal de\ufb01nition in the next section) and\n\u03c8 : B \u2192Rm.\nIt is not hard to see that for any m-dimensional space V of functions over the ball, there\nexists an embedding \u03c8 : B \u2192Rm such that\n{f + b : f \u2208V, b \u2208R} = {\u039bw,b \u25e6\u03c8 : w \u2208Rm, b \u2208R}\nHence, our lower bounds hold for any method that optimizes a surrogate loss over a subset\nof a \ufb01nite dimensional space of functions, and return the threshold function corresponding\nto the optimum.\n4\n1.4\nPrevious Results and Related Work\nThe problem of learning halfspaces and in particular large margin halfspaces is as old as the\n\ufb01eld of machine learning, starting with the perceptron algorithm (Rosenblatt, 1958). Since\nthen it has been a fundamental challenge in machine learning and has inspired much of the\nexisting theory as well as many popular algorithms.\nThe generalized linear method has its roots in the work of Gauss and Legendre who used\nthe least squares method for astronomical computations. This method has played a key role\nin modern statistics. Its \ufb01rst application in computational learning theory is in (Linial et al.,\n1989) where it is shown that AC0 functions are learnable in quasi-polynomial time w.r.t. the\nuniform distribution. Subsequently, many authors have used the method to tackle various\nlearning problems. For example, Klivans and Servedio (2001) derived the fastest algorithm\nfor learning DNF and Kushilevitz and Mansour (1991) used it to develop an algorithm for\ndecision trees. The main uses of the linear method in the problem of learning halfspaces\nappear in the next paragraph. Needless to say we are unable here to o\ufb00er a comprehensive\nsurvey of its uses in computational learning theory in general.\nThe best currently known approximation ratios in the problem of learning large margin\nhalfspaces are due to (Birnbaum and Shalev-Shwartz, 2012) and (Long and Servedio, 2011)\nand achieve an approximation ratio of\n1\n\u03b3\u00b7\u221a\nlog(1/\u03b3). The algorithm of (Birnbaum and Shalev-\nShwartz, 2012) is a kernel based learner, while (Long and Servedio, 2011) used a \u201cboosting\nbased\u201d approach (that does not belong to the generalized linear method). The fastest exact\nalgorithm is due to Shalev-Shwartz et al. (2011) and runs it time exp\n\u0010\n\u0398\n\u0010\n1\n\u03b3 log\n\u0010\n1\n\u03f5\u03b3\n\u0011\u0011\u0011\n, and\nis also a kernel based learner. Better running times can be achieved under distributional\nassumptions. For data which is separable with margin \u03b3, i.e. Err\u03b3(D) = 0, the perceptron\nalgorithm (as well as SVM with a linear kernel) can \ufb01nd a classi\ufb01er with error \u2264\u03f5 with time\nand sample complexity \u2264poly(1/\u03b3, 1/\u03f5). Kalai et al. (2005) gave a \ufb01nite dimensional learner\nwhich is the fastest known algorithm for learning halfspaces w.r.t. the uniform distribution\nover Sd\u22121 and the d-dimensional boolean cube (running in time dO(1/\u03f5)4). They also designed\na \ufb01nite dimensional learner of halfspaces w.r.t. log-concave distributions. Blais et al. (2008)\nextended these results from uniform to product distributions. In this work, we focus on\nalgorithms which work for any distribution and whose runtime is polynomial in both 1/\u03b3\nand 1/\u03f5.\nThe problem of proper3 learning of halfspaces in the non-separable case was shown to be\nhard to approximate within any constant approximation factor (Feldman et al., 2006, Gu-\nruswami and Raghavendra, 2006). It has been recently shown (Shalev-Shwartz et al., 2011)\nthat improper learning under the margin assumption is also hard (under some cryptographic\nassumptions). Namely, no polynomial time algorithm can achieve an approximation ratio of\n\u03b1(\u03b3) = 1. In another recent result Daniely et al. (2013) have shown that under a certain\ncomplexity assumption, for every constant \u03b1, no polynomial time algorithm can achieve an\napproximation ratio of \u03b1.\nBen-David et al. (2012) (see also Long and Servedio (2011)) addressed the performance of\nmethods that minimize a convex loss over the class of a\ufb03ne functional of bounded norm (in\n3A proper learner must output a halfspace classi\ufb01er. Here we consider improper learning where the learner\ncan output any classi\ufb01er.\n5\nour terminology, they considered the narrow class of \ufb01nite dimensional learners that optimize\nover the space of linear functionals). They showed that the best approximation ratio of such\nmethods is \u0398(1/\u03b3). Our results can be seen as a substantial generalization of their results.\nThe learning theory literature contains consistency results for learning with the so-called\nuniversal kernels and well-calibrated surrogate loss functions. This includes the study of\nasymptotic relations between surrogate convex loss functions and the 0-1 loss function\n(Zhang, 2004, Bartlett et al., 2006, Steinwart and Christmann, 2008).\nIt is shown that\nthe approximation ratio of SVM with a universal kernel tends to 1 as the sample size grows.\nOur result implies that this convergence is very slow, e.g., an exponentially large (in\n1\n\u03b3)\nsample is needed to make the error < 2 Err\u03b3(D).\nAlso related are Ben-David et al. (2003) and Warmuth and Vishwanathan (2005). These\npapers show the existence of learning problems with limitations on the ability to learn them\nusing linear methods.\n2\nResults\nWe \ufb01rst de\ufb01ne the two families of algorithms to which our lower bounds apply. We start\nwith the class of surrogate loss functions. This class includes the most popular choices such\nas the absolute loss |1 \u2212x|, the squared loss (1 \u2212x)2, the logistic loss log2 (1 + e\u2212x), the\nhinge loss (1 \u2212x)+ etc.\nDe\ufb01nition 2.1 (Surrogate loss function) A function l : R \u2192R is called a surrogate loss\nfunction if l is convex and is bounded below by the 0-1 loss.\nThe \ufb01rst family of algorithms contains kernel based algorithms, such as kernel SVM. In the\nde\ufb01nitions below we set the accuracy parameter \u03f5 to be \u221a\u03b3. Since our goal is to prove lower\nbounds, this choice is without loss of generality, and is intended for the sake of simplifying\nthe theorems statements.\nDe\ufb01nition 2.2 (Kernel based learner) Let l : R \u2192R be a surrogate loss function. A\nkernel based learning algorithm, A, receives as input \u03b3 \u2208(0, 1). It then selects C = CA(\u03b3)\nand an absolutely continuous feature mapping, \u03c8 = \u03c8A(\u03b3), which maps the original space H\ninto the unit ball of a new space H1 (see Section 1.2). The algorithm returns a function\nA(\u03b3) \u2208{\u039bw,b \u25e6\u03c8 : w \u2208H1, b \u2208R, \u2225w\u2225H1 \u2264C}\nsuch that, with probability \u22651 \u2212exp(\u22121/\u03b3),\nErrD,l(A(\u03b3)) \u2264inf{ErrD,l(\u039bw,b \u25e6\u03c8) : w \u2208H1, b \u2208R, \u2225w\u2225H1 \u2264C} + \u221a\u03b3 .\nWe denote by mA(\u03b3) the maximal number of examples A uses. We say that A is e\ufb03cient if\nmA(\u03b3) \u2264poly(1/\u03b3).\nNote that the de\ufb01nition of kernel based learner allows for any prede\ufb01ned convex surrogate\nloss, not just the hinge loss. Namely, we consider the program\nmin\nw,b\nErrD,l (\u039bw,b \u25e6\u03c8)\ns.t.\nw \u2208H1, b \u2208R, \u2225w\u2225H1 \u2264C .\n(5)\n6\nWe note that our results hold even if the kernel corresponds to \u03c8 is hard to compute.\nThe second family of learning algorithms involves an arbitrary feature mapping and\ndomain constraint on the vector w, as in program (4).\nDe\ufb01nition 2.3 (Finite dimensional learner) Let l : R \u2192R be some surrogate loss func-\ntion. A \ufb01nite dimensional learning algorithm, A, receives as input \u03b3 \u2208(0, 1). It then selects\na continuous embedding \u03c8 = \u03c8A(\u03b3) : B \u2192Rm and a constraint set W = WA(\u03b3) \u2286Rm. The\nalgorithm returns, with probability \u22651 \u2212exp(\u22121/\u03b3), a function\nA(\u03b3) \u2208{\u039bw,b \u25e6\u03c8 : w \u2208W, b \u2208R}\nsuch that\nErrD,l(A(\u03b3)) \u2264inf{ErrD,l(\u039bw,b \u25e6\u03c8) : w \u2208W, b \u2208R} + \u221a\u03b3 .,\nWe say that A is e\ufb03cient if m = mA(\u03b3) \u2264poly(1/\u03b3).\n2.1\nMain Results\nWe begin with a lower bound on the performance of e\ufb03cient kernel-based algorithms.\nTheorem 2.4 Let l be an arbitrary surrogate loss and let A be an e\ufb03cient kernel-based\nlearner w.r.t. l. Then, for every \u03b3 > 0, there exists a distribution D on B such that, w.p.\n\u22651 \u221210 exp(\u22121/\u03b3),\nErrD,0\u22121(A(\u03b3))\nErr\u03b3(D)\n\u2265\u2126\n\u0012\n1\n\u03b3 \u00b7 poly(log(1/\u03b3))\n\u0013\n.\nNext we show that kernel-based learners that achieve approximation ratio of\n\u0010\n1\n\u03b3\n\u00111\u2212\u03f5\nfor some\nconstant \u03f5 > 0 must su\ufb00er exponential complexity.\nTheorem 2.5 Let l be an arbitrary surrogate loss, let \u03f5 > 0 and let A be a kernel-based\nlearner w.r.t. l such that for every \u03b3 > 0 and every distribution D on B, w.p. \u22651/2,\nErrD,0\u22121(A(\u03b3))\nErr\u03b3(D)\n\u2264\n\u00121\n\u03b3\n\u00131\u2212\u03f5\n.\nThen, for some a = a(\u03f5) > 0, mA(\u03b3) = \u2126(exp ((1/\u03b3)a)).\nThese two theorems follow from the following result.\nTheorem 2.6 Let l be an arbitrary surrogate loss and let A be a kernel-based learner w.r.t.\nl for which mA(\u03b3) = exp(o(\u03b3\u22122/7)). Then, for every \u03b3 > 0, there exists a distribution D on\nB such that, w.p. \u22651 \u221210 exp(\u22121/\u03b3),\nErrD,0\u22121(A(\u03b3))\nErr\u03b3(D)\n\u2265\u2126\n\u0012\n1\n\u03b3 \u00b7 poly(log(mA(\u03b3)))\n\u0013\n.\n7\nIt is shown in (Birnbaum and Shalev-Shwartz, 2012) that solving kernel SVM with some\nspeci\ufb01c kernel (i.e. some speci\ufb01c \u03c8) yields an approximation ratio of O\n\u0012\n1\n\u03b3\u221a\nlog(1/\u03b3)\n\u0013\n. It\nfollows that our lower bound in Theorem 2.6 is essentially tight. Also, this theorem can\nbe viewed as a substantial generalization of (Ben-David et al., 2012, Long and Servedio,\n2011), who give an approximation ratio of \u2126\n\u0010\n1\n\u03b3\n\u0011\nwith no embedding (i.e., \u03c8 is the identity\nmap). Also relevant is (Shalev-Shwartz et al., 2011), which shows that for a certain \u03c8, and\nmA(\u03b3) = poly (exp ((1/\u03b3) \u00b7 log (1/(\u03b3))), kernel SVM has approximation ratio of 1. Theorem\n2.6 shows that for kernel-based learner to achieve a constant approximation ratio, mA must\nbe exponential in 1/\u03b3.\nNext we give lower bounds on the performance of \ufb01nite dimensional learners.\nTheorem 2.7 Let l be a Lipschitz surrogate loss and let A be a \ufb01nite dimensional learner\nw.r.t.\nl.\nAssume that mA(\u03b3) = exp(o(\u03b3\u22121/8)).\nThen, for every \u03b3 > 0, there exists a\ndistribution D on Sd\u22121 \u00d7 {\u00b11} with d = O(log(mA(\u03b3)/\u03b3)) such that, w.p. \u22651 \u2212exp(\u22121/\u03b3),\nErrD,0\u22121(A(\u03b3))\nErr\u03b3(D)\n\u2265\u2126\n\u0012\n1\n\u221a\u03b3 poly(log(mA(\u03b3)/\u03b3))\n\u0013\n.\nCorollary 2.8 Let l be a Lipschitz surrogate loss and let A be an e\ufb03cient \ufb01nite dimensional\nlearner w.r.t. l. Then, for every \u03b3 > 0, there exists a distribution D on Sd\u22121 \u00d7 {\u00b11} with\nd = O(log(1/\u03b3)) such that, w.p. \u22651 \u2212exp(\u22121/\u03b3),\nErrD,0\u22121(A(\u03b3))\nErr\u03b3(D)\n\u2265\u2126\n\u0012\n1\n\u221a\u03b3 poly(log(1/\u03b3))\n\u0013\n.\nCorollary 2.9 Let l be a Lipschitz surrogate loss, let \u03f5 > 0 and let A be a \ufb01nite dimensional\nlearner w.r.t. l such that for every \u03b3 > 0 and every distribution D on Bd with d = \u03c9(log(1/\u03b3))\nit holds that w.p. \u22651/2,\nErrD,0\u22121(A(\u03b3))\nErr\u03b3(D)\n\u2264\n\u00121\n\u03b3\n\u0013 1\n2 \u2212\u03f5\nThen, for some a = a(\u03f5) > 0, mA(\u03b3) = \u2126(exp ((1/\u03b3)a)).\n2.2\nReview of the proofs\u2019 main ideas\nTo give the reader some idea of our arguments, we sketch some of the main ingredients of the\nproof of Theorem 2.6. At the end of this section we sketch the idea of the proof of Theorem\n2.7. We note, however, that the actual proofs are organized somewhat di\ufb00erently.\nWe will construct a distribution D over Sd\u22121\u00d7{\u00b11} (recall that Rd is viewed as standardly\nembedded in H = \u21132). Thus, we can assume that the program is formulated in terms of the\nunit sphere, S\u221e\u2282\u21132, and not the unit ball.\nFix an embedding \u03c8 and C > 0. Denote by k : S\u221e\u00d7 S\u221e\u2192R the corresponding kernel\nk(x, y) = \u27e8\u03c8(x), \u03c8(y)\u27e9H1 and consider the following set of functions over S\u221e\nHk = {\u039bv,0 \u25e6\u03c8 : v \u2208H1} .\n8\nHk is a Hilbert space with norm ||f||Hk = inf{||v||H1 : \u039bv,0 \u25e6\u03c8 = f}. The subscript k\nindicates that Hk is uniquely determined (as a Hilbert space) given the kernel k. With this\ninterpretation, program (5) is equivalent to the program\nmin\nf\u2208Hk,b\u2208R ErrD,l (f + b)\ns.t. ||f||Hk \u2264C .\n(6)\nFor simplicity we focus on l being the hinge-loss (the generalization to other surrogate loss\nfunctions is rather technical).\nThe proof may be split into four steps:\n1. Our \ufb01rst step is to show that we can restrict to the case C = poly\n\u0010\n1\n\u03b3\n\u0011\n. We show\nthat for every subset X of a Hilbert space H, if a\ufb03ne functionals on X can be learnt\nusing m examples w.r.t. the hinge loss, then there exists an equivalent inner product\non H under which X is contained in a unit ball, and the a\ufb03ne functional returned by\nany learning algorithm must have norm \u2264O (m3). Since we consider algorithms with\npolynomial sample complexity, this allows us to argue as if C = poly\n\u0010\n1\n\u03b3\n\u0011\n.\n2. We consider the one-dimensional problem of improperly learning halfspaces (i.e.\nthresholds on the line) by optimizing the hinge loss over the space of univariate polyno-\nmials of degree bounded by log(C). We construct a distribution D over [\u22121, 1] \u00d7 {\u00b11}\nthat is a convex combination of two distributions. One that is separable by a \u03b3-margin\nhalfspace and the other representing a tiny amount of noise. We show that each so-\nlution of the problem of minimizing the hinge-loss w.r.t. D over the space of such\npolynomials has the property that f(\u03b3) \u2248f(\u2212\u03b3).\n3. We pull back the distribution D w.r.t. a direction e \u2208Sd\u22121 to a distribution over\nSd\u22121 \u00d7 {\u00b11}. Let f be an approximate solution of program (6). We show that f takes\nalmost the same value on instances for which \u27e8x, e\u27e9= \u03b3 and \u27e8x, e\u27e9= \u2212\u03b3. This step\ncan be further broken into three substeps \u2013\n(a) First, we assume that the kernel is symmetric and f(x) depends only on \u27e8x, e\u27e9.\nThis substep uses a characterization of Hilbert spaces corresponding to symmetric\nkernels, from which it follows that f has the form\nf(x) =\n\u221e\nX\nn=1\n\u03b1nPd,n(\u27e8x, e\u27e9) .\nHere Pd,n are the d-dimensional Legendre polynomials and P\u221e\nn=0 \u03b12\nn < C2. This\nallows us to rely on the results for the one-dimensional case from step (1).\n(b) By symmetrizing f, we relax the assumption that f depends only on \u27e8x, e\u27e9.\n(c) By averaging the kernel over the group of linear isometries on Rd, we relax the\nassumption that the kernel is symmetric.\n4. Finally, we show that for the distribution from the previous step, if f is an approximate\nsolution to program (6) then f predicts the same value, 1, on instances for which\n\u27e8x, e\u27e9= \u03b3 and \u27e8x, e\u27e9= \u2212\u03b3. This establishes our claim, as the constructed distribution\nassigns the value \u22121 to instances for which \u27e8x, e\u27e9= \u2212\u03b3.\n9\nWe now expand on this brief description of the main steps.\nPolynomial sample implies small C\nLet X be a subset of the unit ball of some Hilbert space H and let C > 0. Assume that\na\ufb03ne functionals over X with norm \u2264C can be learnt using m examples with error \u03f5 and\ncon\ufb01dence \u03b4. That is, assume that there is an algorithm such that\n\u2022 Its input is a sample of m points in X \u00d7 {\u00b11} and its output is an a\ufb03ne functional\n\u039bw,b with \u2225w\u2225\u2264C.\n\u2022 For every distribution D on X \u00d7 {\u00b11}, it returns, with probability 1 \u2212\u03b4, w, b with\nErrD,hinge(\u039bw,b) \u2264inf\u2225w\u2032\u2225\u2264C,b\u2032\u2208R ErrD,hinge(\u039bw\u2032,b\u2032) + \u03f5.\nWe will show that there is an equivalent inner product \u27e8\u00b7, \u00b7\u27e9\u2032 on H under which X is contained\nin a unit ball (not necessarily around 0) and the a\ufb03ne functional returned by any learning\nalgorithm as above, must have norm \u2264O (m3) w.r.t. the new norm.\nThe construction of the norm is done as follows. We \ufb01rst \ufb01nd an a\ufb03ne subspace M \u2282H\nof dimension d \u2264m that is very close to X in the sense that the distance of every point\nin X from M is \u2264m\nC . To \ufb01nd such an M, we assume toward a contradiction that there is\nno such M, and use this to show that there is a subset A \u2282X such that every function\nf : A \u2192[\u22121, 1] can be realized by some a\ufb03ne functional with norm \u2264C. This contradicts\nthe assumption that a\ufb03ne functionals with norm \u2264C can be learnt using m examples.\nHaving the subspace M at hand, we construct, using John\u2019s lemma (e.g.\nMatousek\n(2002)), an inner product \u27e8\u00b7, \u00b7\u27e9\u2032\u2032 on M and a distribution \u00b5N on X with the property that\nthe projection of X on M is contained in a ball of radius 1\n2 w.r.t. \u27e8\u00b7, \u00b7\u27e9\u2032\u2032, and the hinge error\nof every a\ufb03ne functional w.r.t. \u00b5N is lower bounded by the norm of the a\ufb03ne functional,\ndivided by m2. We show that that this entails that any a\ufb03ne functional returned by the\nalgorithm must have a norm \u2264O (m3) w.r.t. the inner product \u27e8\u00b7, \u00b7\u27e9\u2032\u2032.\nFinally, we construct an inner product \u27e8\u00b7, \u00b7\u27e9\u2032 on H by putting the norm \u27e8\u00b7, \u00b7\u27e9\u2032\u2032 on M and\nmultiplying the original inner product by\nC2\n2m2 on M \u22a5.\nThe one dimensional distribution\nWe de\ufb01ne a distribution D on [\u22121, 1] as follows. Start with the distribution D1 that takes the\nvalues \u00b1(\u03b3, 1), where D1(\u03b3, 1) = 0.7 and D1(\u2212\u03b3, \u22121) = 0.3. Clearly, for this distribution,\nthe threshold 0 has zero error rate. To construct D, we perturb D1 with \u201cnoise\u201d as follows.\nLet D = (1 \u2212\u03bb)D1 + \u03bbD2, where D2 is de\ufb01ned as follows. The probability of the labels is\nuniform and independent of the instance and the marginal probability over the instances is\nde\ufb01ned by the density function\n\u03c1(x) =\n(\n0\nif |x| > 1/8\n8\n\u03c0\u221a\n1\u2212(8x)2\nif |x| \u22641/8\n.\nThis choice of \u03c1 simpli\ufb01es our calculations due to its relation to Chebyshev polynomials.\nHowever, other choices of \u03c1 which are supported on a small interval around zero can also\nwork.\n10\nNote that the error rate of the threshold 0 on D is \u03bb/2. We next show that each poly-\nnomial f of degree K = log(C) that satis\ufb01es ErrD,hinge(f) \u22641 must have f(\u03b3) \u2248f(\u2212\u03b3).\nIndeed, if\n1 \u2265ErrD,hinge(f) = (1 \u2212\u03bb) ErrD1,hinge(f) + \u03bb ErrD2,hinge(f)\nthen ErrD2,hinge(f) \u22641\n\u03bb. But,\nErrD2,hinge(f) = 1\n2\nZ 1\n\u22121\nlhinge(f(x))\u03c1(x)dx + 1\n2\nZ 1\n\u22121\nlhinge(\u2212f(x))\u03c1(x)dx\n\u22651\n2\nZ 1\n\u22121\nlhinge(\u2212|f(x)|)\u03c1(x)dx\nand using the convexity of lhinge we obtain from Jensen\u2019s inequality that\n\u22651\n2lhinge\n\u0012Z 1\n\u22121\n\u2212|f(x)|\u03c1(x)dx\n\u0013\n= 1\n2\n\u0012\n1 +\nZ 1\n\u22121\n|f(x)|\u03c1(x)dx\n\u0013\n\u22651\n2\nZ 1\n\u22121\n|f(x)|\u03c1(x)dx =: 1\n2\u2225f\u22251,d\u03c1 .\nThis shows that \u2225f\u22251,d\u03c1 \u22642\n\u03bb. We next write f = PK\ni=1 \u03b1i \u02dcTi, where { \u02dcTi} are the orthonormal\npolynomials corresponding to the measure d\u03c1. Since \u02dcTi are related to Chebyshev polynomials\nwe can uniformly bound their \u2113\u221enorm, hence obtain that\nsX\ni\n\u03b12\ni = \u2225f\u22252,d\u03c1 \u2264O(\n\u221a\nK) \u2225f\u22251,d\u03c1 \u2264O\n \u221a\nK\n\u03bb\n!\n.\nBased on the above, and using a bound on the derivatives of Chebyshev polynomials, we can\nbound the derivative of the polynomial f\n|f \u2032(x)| \u2264\nX\ni\n|\u03b1i|| \u02dcT \u2032\ni(x)| \u2264O\n\u0012K3\n\u03bb\n\u0013\n.\nHence, by choosing \u03bb = \u03c9(\u03b3K3) = \u03c9(\u03b3 log3(C)) we obtain\n|f(\u03b3) \u2212f(\u2212\u03b3)| \u22642 \u03b3 max\nx\n|f \u2032(x)| = O\n\u0012\u03b3 K3\n\u03bb\n\u0013\n= o(1) ,\nas required.\nPulling back to the d \u22121 dimensional sphere\nGiven the distribution D over [\u22121, 1] \u00d7 {\u00b11} described before, and some e \u2208Sd\u22121, we now\nde\ufb01ne a distribution De on Sd\u22121 \u00d7 {\u00b11}. To sample from De, we \ufb01rst sample (\u03b1, \u03b2) from D\n11\nand (uniformly and independently) a vector z from the 1-codimensional sphere of Sd\u22121 that\nis orthogonal to e. The constructed point is (\u03b1e +\n\u221a\n1 \u2212\u03b12z, \u03b2).\nFor any f \u2208Hk and a \u2208[\u22121, 1] de\ufb01ne \u00aff(a) to be the expectation of f over the 1-\ncodimensional sphere {x \u2208Sd\u22121 : \u27e8x, e\u27e9= a}. We will show that for any f \u2208Hk, such that\n\u2225f\u2225Hk \u2264C and ErrDe,hinge(f) \u22641, we have that | \u00aff(\u03b3) \u2212\u00aff(\u2212\u03b3)| = o(1).\nTo do so, let us \ufb01rst assume that f is symmetric with respect to e, and hence can be\nwritten as\nf(x) =\n\u221e\nX\nn=0\n\u03b1nPd,n(\u27e8x, e\u27e9) ,\nwhere \u03b1n \u2208R and Pd,n is the d-dimensional Legendre polynomial of degree n. Furthermore,\nby a characterization of Hilbert spaces corresponding to symmetric kernels, it follows that\nP \u03b12\nn \u2264C2.\nSince f is symmetric w.r.t. e we have,\n\u00aff(a) =\n\u221e\nX\nn=0\n\u03b1nPd,n(a) .\nFor |a| \u22641/8, we have that |Pd,n(a)| tends to zero exponentially fast with both d and n.\nHence, if d is large enough then\n\u00aff(a) \u2248\nlog(C)\nX\nn=0\n\u03b1nPd,n(a) =: \u02dcf(a) .\nNote that \u02dcf is a polynomial of degree bounded by log(C). In addition, by construction,\nErrDe,hinge(f) = ErrD,hinge( \u00aff) \u2248ErrD,hinge( \u02dcf). Hence, if 1 \u2265ErrDe,hinge(f) then using the\nprevious subsection we conclude that | \u00aff(\u03b3) \u2212\u00aff(\u2212\u03b3)| = o(1).\nSymmetrization of f\nIn the above, we assumed that both the kernel function is symmetric and that f is symmetric\nw.r.t. e. Our next step is to relax the latter assumption, while still assuming that the kernel\nfunction is symmetric.\nLet O(e) be the group of linear isometries that \ufb01x e, namely, O(e) = {A \u2208O(d) : Ae = e}.\nBy assuming that k is a symmetric kernel, we have that for all A \u2208O(e), the func-\ntion g(x) = f(Ax) is also in Hk.\nFurthermore, \u2225g\u2225Hk = \u2225f\u2225Hk and by the construc-\ntion of De we also have ErrDe,hinge(g) = ErrDe,hinge(f).\nLet Pef(x) =\nR\nO(e) f(Ax)dA be\nthe symmetrization of f w.r.t.\ne.\nOn one hand, Pef \u2208Hk, \u2225Pef\u2225Hk \u2264\u2225f\u2225Hk, and\nErrDe,hinge(Pef) \u2264ErrDe,hinge(f).\nOn the other hand, \u00aff = Pef.\nSince for Pef we have\nalready shown that |Pef(\u03b3)\u2212Pef(\u2212\u03b3)| = o(1), it follows that | \u00aff(\u03b3)\u2212\u00aff(\u2212\u03b3)| = o(1) as well.\nSymmetrization of the kernel\nOur \ufb01nal step is to remove the assumption that the kernel is symmetric. To do so, we \ufb01rst\nsymmetrize the kernel as follows. Recall that O(d) is the group of linear isometries of Rd.\n12\nDe\ufb01ne the following symmetric kernel:\nks(x, y) =\nZ\nO(d)\nk(Ax, Ay)dA .\nWe show that the corresponding Hilbert space consists of functions of the form\nf(x) =\nZ\nO(d)\nfA(Ax)dA ,\nwhere for every A fA \u2208Hk. Moreover,\n\u2225f\u22252\nHks \u2264\nZ\nO(d)\n\u2225fA\u22252\nHkdA .\n(7)\nLet \u03b1 be the maximal number such that\n\u2200e \u2208Sd\u22121\u2203fe \u2208Hk s.t. \u2225fe\u2225Hk \u2264C, ErrDe,hinge(fe) \u22641, | \u00affe(\u03b3) \u2212\u00affe(\u2212\u03b3)| > \u03b1 .\nSince Hk is closed to negation, it follows that \u03b1 satis\ufb01es\n\u2200e \u2208Sd\u22121\u2203fe \u2208Hk s.t. \u2225fe\u2225Hk \u2264C, ErrDe,hinge(fe) \u22641, \u00affe(\u03b3) \u2212\u00affe(\u2212\u03b3) > \u03b1 .\nFix some v \u2208Sd\u22121 and de\ufb01ne f \u2208Hks to be\nf(x) =\nZ\nO(d)\nfAv(Ax)dA .\nBy Equation (7) we have that \u2225f\u2225Hks \u2264C.\nIt is also possible to show that for all A\nErrDv,hinge(fAv \u25e6A) = ErrDAv,hinge(fAv) \u22641.\nTherefore, by the convexity of the loss,\nErrDv,hinge(f) \u22641. It follows, by the previous sections, that | \u00aff(\u03b3) \u2212\u00aff(\u2212\u03b3)| = o(1). But, we\nshow that \u00aff(\u03b3) \u2212\u00aff(\u2212\u03b3) > \u03b1. It therefore follows that \u03b1 = o(1), as required.\nConcluding the proof\nWe have shown that for every kernel, there exists some direction e such that for all f \u2208Hk\nthat satis\ufb01es \u2225f\u2225Hk \u2264C and ErrDe,hinge(f) \u22641 we have that | \u00aff(\u03b3) \u2212\u00aff(\u2212\u03b3)| = o(1).\nNext, consider f which is also an (approximated) optimal solution of program (2) with\nrespect to De.\nSince ErrDe,hinge(0) = 1, we clearly have that ErrDe,hinge(f) \u22641, hence\n| \u00aff(\u03b3) \u2212\u00aff(\u2212\u03b3)| = o(1). Next we show that \u00aff(\u2212\u03b3) > 1/2, which will imply that f predicts\nthe label 1 for most instances on the 1 co-dimensional sphere such that \u27e8x, e\u27e9= \u2212\u03b3. Hence,\nits 0-1 error is close to 0.3(1\u2212\u03bb) \u22650.2 while Err\u03b3(De) = \u03bb/2. By choosing \u03bb = O(\u03b3 log3.1(C))\nwe obtain that the approximation ratio is \u2126\n\u0010\n1\n\u03b3 log3.1(C)\n\u0011\n.\nIt is therefore left to show that \u00aff(\u2212\u03b3) > 1/2. Let a = \u00aff(\u03b3) \u2248\u00aff(\u2212\u03b3). On (1 \u2212\u03bb)\nfraction fraction of the distribution, the hinge-loss would be (on average and roughly)\n0.3[1 + a]+ + 0.7[1 \u2212a]+. This function is minimized for a = 1, which concludes our proof\nsince \u03bb is o(1).\n13\nThe proof of Theorem 2.7\nTo prove Theorem 2.7, we prove, using John\u2019s Lemma (Matousek, 2002), that for every em-\nbedding \u03c8 : Sd\u22121 \u2192B1, we can construct a kernel k : Sd\u22121 \u00d7 Sd\u22121 \u2192R and a probability\nmeasure \u00b5N over Sd\u22121 with the following properties: If f is an approximate solution of pro-\ngram (4), where \u03b3 fraction of the distribution D is perturbed by \u00b5N, then \u2225f\u2225k \u2264O\n\u0010\nm1.5\n\u03b3\n\u0011\n.\nUsing this, we adapt the proof as sketched above to prove Theorem 2.7.\n3\nAdditional Results\nLow dimensional distributions. It is of interest to examine Theorem 2.6 when D is\nsupported on Bd for d small. We show that for d = O(log(1/\u03b3)), the approximation ratio is\n\u2126\n\u0010\n1\n\u221a\u03b3\u00b7poly(log(1/\u03b3))\n\u0011\n. Most commonly used kernels (e.g., the polynomial, RBF, and Hyperbolic\ntangent kernels, as well as the kernel used in (Shalev-Shwartz et al., 2011)) are symmetric.\nNamely, for all unit vectors x, y \u2208B, k(x, y) := \u27e8\u03c8(x), \u03c8(y)\u27e9H1 depends only on \u27e8x, y\u27e9H.\nFor symmetric kernels, we show that even with the restriction that d = O(log(1/\u03b3)), the\napproximation ratio is still \u2126\n\u0010\n1\n\u03b3\u00b7poly(log(1/\u03b3))\n\u0011\n. However, the result for symmetric kernels is\nonly proved for (idealized) algorithms that return the exact solution of program (5).\nTheorem 3.1 Let A be a kernel-based learner corresponding to a Lipschitz surrogate. As-\nsume that mA(\u03b3) = exp(o(\u03b3\u22121/8)). Then, for every \u03b3 > 0, there exists a distribution D on\nBd, for d = O(log(mA(\u03b3)/\u03b3)), such that, w.p. \u22651 \u2212exp(\u22121/\u03b3),\nErrD,0\u22121(A(\u03b3))\nErr\u03b3(D)\n\u2265\u2126\n\u0012\n1\n\u221a\u03b3 \u00b7 poly(log(mA(\u03b3)))\n\u0013\n.\nTheorem 3.2 Assume that mA(\u03b3) = exp(o(\u03b3\u22122/7)) and \u03c8 is continuous and symmetric.\nFor every \u03b3 > 0, there exists a distribution D on Bd, for d = O(log(mA(\u03b3))) and a solution\nto program (5) whose 0-1-error is \u2126\n\u0010\n1\n\u03b3 poly(log(mA(\u03b3)))\n\u0011\n\u00b7 Err\u03b3(D).\nThe integrality gap. In bounding the approximation ratio, we considered a prede\ufb01ned\nloss l. We believe that similar bounds hold as well for algorithms that can choose l according\nto \u03b3. However, at the moment, we only know to lower bound the integrality gap, as de\ufb01ned\nbelow.\nIf we let l depend on \u03b3, we should rede\ufb01ne the complexity of Program (5) to be C \u00b7 L,\nwhere L is the Lipschitz constant of l. (See the discussion following Program (5)). The (\u03b3-\n)integrality gap of program (5) and (4) is de\ufb01ned as the worst case, over all possible choices\nof D, of the ratio between the optimum of the program, running on the input \u03b3, to Err\u03b3(D).\nWe note that ErrD,0\u22121(f) \u2264ErrD,l(f) for every convex surrogate l. Thus, the integrality gap\nalways upper bounds the approximation ratio. Moreover, this fact establishes most (if not\nall) guarantees for algorithms that solve Program (5) or Program (4).\nWe denote by \u2202+f the right derivative of the real function f. Note that \u2202+f always exists\nfor f convex. Also, \u2200x \u2208R, |\u2202+f(x)| \u2264L if f is L-Lipschitz. We prove:\n14\nTheorem 3.3 Assume that C = exp\n\u0000o(\u03b3\u22122/7)\n\u0001\nand \u03c8 is continuous. For every \u03b3 > 0, there\nexists a distribution D on Bd, for d = O(log(C)) such that the optimum of Program (5) is\n\u2126\n\u0010\n1\n\u03b3 poly(log(C\u00b7|\u2202+l(0)|))\n\u0011\n\u00b7 Err\u03b3(D).\nThus Program (5) has itegrality gap \u2126\n\u0010\n1\n\u03b3 poly(log(C\u00b7L))\n\u0011\n. For Program (4) we prove a similar\nlower bound:\nTheorem 3.4 Let m, d, \u03b3 such that d = \u03c9(log(m/\u03b3)) and m = exp\n\u0000o(\u03b3\u22122/7)\n\u0001\n.\nThere\nexist a distribution D on Sd\u22121 \u00d7 {\u00b11} such that the optimum of Program (4) is\n\u2126\n\u0010\n1\n\u03b3 poly(log(m/\u03b3))\n\u0011\n\u00b7 Err\u03b3(D).\n4\nConclusion\nWe prove impossibility results for the family of generalized linear methods in the task of\nlearning large margin halfspaces. Some of our lower bounds nearly match the best known\nupper bounds and we conjecture that the rest of our bounds can be improved as well to\nmatch the best known upper bounds. As we describe next, our work leaves much for future\nresearch.\nFirst, regarding the task of learning large margin halfspaces, our analysis suggests that if\nbetter approximation ratios are at all possible then they would require methods other than\noptimizing a convex surrogate over a regularized linear class of classi\ufb01ers.\nSecond, similar to the problem of learning large margin halfsapces, for many learning\nproblems the best known algorithms belong to the generalized linear family. Understanding\nthe limits of the generalized linear method for these problems is therefore of particular interest\nand might indicate where is the line discriminating between feasibility and infeasibility for\nthese problems. We believe that our techniques will prove useful in proving lower bounds on\nthe performance of generalized linear methods for these and other learning problems. E.g.,\nour techniques yield lower bounds on the performance of generalized linear algorithms that\nlearn halfspaces over the boolean cube {\u00b11}n: it can be shown that these methods cannot\nachieve approximation ratios better than \u02dc\u2126(\u221an) even if the algorithm competes only with\nhalfspaces de\ufb01ned by vectors in {\u00b11}n. These ideas will be elaborated on in a long version\nof this manuscript.\nThird, while our results indicate the limitations of generalized linear methods, it is an\nempirical fact that these methods perform very well in practice. Therefore, it is of great\ninterest to \ufb01nd conditions on distributions that hold in practice, under which these methods\nguaranteed to perform well. We note that learning halfspaces under distributional assump-\ntions, has already been addressed to a certain degree. For example, (Kalai et al., 2005, Blais\net al., 2008) show positive results under several assumptions on the marginal distribution\n(namely, they assume that the distribution is either uniform, log-concave or a product distri-\nbution). There is still much to do here, speci\ufb01cally in search of better runtimes. Currently\nthese results yield a runtime which is exponential in poly(1/\u03f5), where \u03f5 is the excess error of\nthe learnt hypothesis.\nFourth, as part of our proof, we have shown a (weak) inverse (lemma 5.15) of the famous\nfact that a\ufb03ne functionals of norm \u2264C can be learnt using poly(C) samples. We made no\n15\nattempts to prove a quantitative optimal result in this vein, and we strongly believe that\nmuch sharper versions can be proved. This interesting direction is largely left as an open\nproblem.\nThere are several limitations of our analysis that deserve further work.\nIn our work\nthe surrogate loss is \ufb01xed in advance. We believe that similar results hold even if the loss\ndepends on \u03b3. This belief is supported by our results about the integrality gap. As explained\nin Section 6, this is a subtle issue that related to questions about sample complexity. Finally,\nin view of Theorems 3.3 and 3.4, we believe that, as in Theorem 2.6, the lower bound in\nTheorems 2.7 and 3.1 can be improved to depend on 1\n\u03b3 rather than on\n1\n\u221a\u03b3.\n5\nProofs\n5.1\nBackground and Notation\nHere we introduce some notations and terminology to be used throughout. The Lp norm\ncorresponding to a measure \u00b5 is denoted ||\u00b7||p,\u00b5. Also, N = {1, 2, . . .} and N0 = {0, 1, 2, . . .}.\nFor a collection of function F \u2282YX and A \u2282X we denote F|A = {f|A | f \u2208F}. Let H be\na Hilbert space. We denote the projection on a closed convex subset of H by PC. We denote\nthe norm of an a\ufb03ne functional \u039b on H by \u2225\u039b\u2225H = sup\u2225x\u2225H=1 |\u039b(x) \u2212\u039b(0)|.\n5.1.1\nReproducing Kernel Hilbert Spaces\nAll the theorems we quote here are standard and can be found, e.g., in Chapter 2 of (Saitoh,\n1988). Let H be a Hilbert space of functions from a set S to C. Note that H consists of\nfunctions and not of equivalence classes of functions. We say that H is a reproducing kernel\nHilbert space (RKHS for short) if, for every x \u2208S, the linear functional f \u2192f(x) is bounded.\nA function k : S\u00d7S \u2192C is a reproducing kernel (or just a kernel) if, for every x1, . . . , xn \u2208\nS, the matrix {k(xi, xj)}1\u2264i,j\u2264n is positive semi-de\ufb01nite.\nKernels and RKHSs are essentially synonymous:\nTheorem 5.1\n1. For every kernel k there exists a unique RKHS Hk such that for every\ny \u2208S, k(\u00b7, y) \u2208Hk and \u2200f \u2208H, f(y) = \u27e8f(\u00b7), k(\u00b7, y)\u27e9Hk.\n2. A Hilbert space H \u2286CS is a RKHS if and only if there exists a kernel k : S \u00d7 S \u2192R\nsuch that H = Hk.\n3. For every kernel k, span{k(\u00b7, y)}y\u2208S = Hk. Moreover,\n\u27e8\nn\nX\ni=1\n\u03b1ik(\u00b7, xi),\nn\nX\ni=1\n\u03b2ik(\u00b7, yi)\u27e9Hk =\nX\n1\u2264i,j,\u2264n\n\u03b1i \u00af\u03b2jk(yj, xi)\n4. If the kernel k : S \u00d7S \u2192R takes only real values, then HR\nk := {Re(f) : f \u2208Hk} \u2282Hk.\nMoreover, HR\nk is a real Hilbert space with the inner product induced from Hk.\n5. For\nevery\nkernel\nk,\nconvergence\nin\nHk\nimplies\npoint-wise\nconvergence.\nIf\nsupx\u2208S k(x, x) < \u221ethen this convergence is uniform.\n16\nThere is also a tight connection between embeddings of S into a Hilbert space and RKHSs.\nTheorem 5.2 A function k : S \u00d7 S \u2192R is a kernel i\ufb00there exists a mapping \u03c6 : S \u2192H\nto some real Hilbert space for which k(x, y) = \u27e8\u03c6(y), \u03c6(x)\u27e9H. Also,\nHk = {fv : v \u2208H}\nWhere fv(x) = \u27e8v, \u03c6(x)\u27e9H. The mapping v 7\u2192fv, restricted to span(\u03c6(S)), is a Hilbert space\nisomorphism.\nA kernel k : S \u00d7 S \u2192R is called normalized if supx\u2208S k(x, x) = 1. Also,\nTheorem 5.3 Let k : S \u00d7 S \u2192R be a kernel and let {fn}\u221e\nn=1 be an orthonormal basis of a\nHk. Then, k(x, y) = P\u221e\nn=1 fn(x)fn(y).\n5.1.2\nUnitary Representations of Compact Groups\nProofs of the results stated here can be found in (Folland, 1994), chapter 5. Let G be a\ncompact group. A unitary representation (or just a representation) of G is a group homo-\nmorphism \u03c1 : G \u2192U(H) where U(H) is the class of unitary operators over a Hilbert space\nH, such that, for every v \u2208H, the mapping a 7\u2192\u03c1(a)v is continuous.\nWe say that a closed subspace M \u2282H is invariant (to \u03c1) if for every a \u2208G, v \u2208M,\n\u03c1(a)v \u2208M. We note that if M is invariant then so is M \u22a5. We denote by \u03c1|M : G \u2192U(M)\nthe restriction of \u03c1 to M. That is, \u2200a \u2208G, \u03c1|M(a) = \u03c1(a)|M. We say that \u03c1 : G \u2192U(H) is\nreducible if H = M \u2295M \u22a5such that M, M \u22a5are both non zero closed and invariant subspaces\nof H. A basic result is that every representation of a compact group is a sum of irreducible\nrepresentation.\nTheorem 5.4 Let \u03c1 : G \u2192U(H) be a representation of a compact group G. Then, H =\n\u2295n\u2208IHn, where every Hn is invariant to \u03c1 and \u03c1|Hn is irreducible.\nWe shall also use the following Lemma.\nLemma 5.5 Let G be a compact group, V a \ufb01nite dimensional vector space and let \u03c1 : G \u2192\nGL(V ) be a continuous homomorphism of groups (here, GL(V ) is the group of invertible\nlinear operators over V ). Then,\n1. There exists an inner product on V making \u03c1 a unitary representation.\n2. Moreover, if V has no non-trivial invariant subspaces (here a subspace U \u2282V is called\ninvariant if, \u2200a \u2208G, f \u2208U, \u03c1(a)f \u2208U) then this inner product is unique up to scalar\nmultiple.\n17\n5.1.3\nHarmonic Analysis on the Sphere\nAll the results stated here can be found in (Atkinson and Han, 2012), chapters 1 and\n2.\nDenote by O(d) the group of unitary operators over Rd and by dA the uniform\nprobability measure over O(d) (that is, dA is the unique probability measure satisfying\nR\nO(d) f(A)dA =\nR\nO(d) f(AB)dA =\nR\nO(d) f(BA)dA for every B \u2208O(d) and every integrable\nfunction f : O(d) \u2192C). Denote by dx = dxd\u22121 the Lebesgue (area) measure over Sd\u22121 and\nlet L2(Sd\u22121) := L2(Sd\u22121, dx). Given a measurable set Z \u2286Sd\u22121, we sometime denote its\nLebesgue measure by |Z|. Also, denote dm =\ndx\n|Sd\u22121| the Lebesgue measure, normalized to be\na probability measure.\nFor every n \u2208N0, we denote by Yd\nn the linear space of d-variables harmonic (i.e., satisfying\n\u2206p = 0) homogeneous polynomials of degree n. It holds that\nNd,n = dim(Yd\nn) =\n\u0012d + n \u22121\nd \u22121\n\u0013\n\u2212\n\u0012d + n \u22123\nd \u22121\n\u0013\n= (2n + d \u22122)(n + d \u22123)!\nn!(d \u22122)!\n(8)\nDenote by Pd,n : L2(Sd\u22121) \u2192Yd\nn the orthogonal projection onto Yd\nn.\nWe denote by \u03c1 : O(d) \u2192U(L2(Sd\u22121)) the unitary representation de\ufb01ned by\n\u03c1(A)f = f \u25e6A\u22121\nWe say that a closed subspace M \u2282L2(Sd\u22121) is invariant if it is invariant w.r.t. \u03c1 (that is,\n\u2200f \u2208M, A \u2208O(d), f \u25e6A \u2208M). We say that an invariant space M is primitive if \u03c1|M is\nirreducible.\nTheorem 5.6\n1. L2(Sd\u22121) = \u2295\u221e\nn=0Yd\nn.\n2. The primitive \ufb01nite dimensional subspaces of L2(Sd\u22121) are exactly {Yd\nn}\u221e\nn=0.\nLemma 5.7 Fix an orthonormal basis Y d\nn,j, 1 \u2264j \u2264Nd,n to Yd\nn. For every x \u2208Sd\u22121 it\nholds that\nNd,n\nX\nj=1\n|Y d\nn,j(x)|2 = Nd,n\n|Sd\u22121|\n5.1.4\nLegendre and Chebyshev Polynomials\nThe results stated here can be found at (Atkinson and Han, 2012). Fix d \u22652. The d\ndimensional Legendre polynomials are the sequence of polynomials over [\u22121, 1] de\ufb01ned by\nthe recursion formula\nPd,n(x) = 2n+d\u22124\nn+d\u22123 xPd,n\u22121(x) +\nn\u22121\nn+d\u22123Pd,n\u22122(x)\nPd,0 \u22611, Pd,1(x) = x\nWe shall make use of the following properties of the Legendre polynomials.\nProposition 5.8\n1. For every d \u22652, the sequence {Pd,n} is orthogonal basis of the\nHilbert space L2 \u0010\n[\u22121, 1], (1 \u2212x2)\nd\u22123\n2 dx\n\u0011\n.\n18\n2. For every n, d, ||Pd,n||\u221e= 1.\nThe Chebyshev polynomials of the \ufb01rst kind are de\ufb01ned as Tn := P2,n. The Chebyshev\npolynomials of the second kind are the polynomials over [\u22121, 1] de\ufb01ned by the recursion\nformula\nUn(x) = 2xUn\u22121(x) \u2212Un\u22122(x)\nU0 \u22611, U1(x) = 2x\nWe shall make use of the following properties of the Chebyshev polynomials.\nProposition 5.9\n1. For every n \u22651, T \u2032\nn = nUn\u22121.\n2. ||Un||\u221e= n + 1.\nGiven a measure \u00b5 over [\u22121, 1], the orthogonal polynomials corresponding to \u00b5 are the se-\nquence of polynomials obtained upon the Gram-Schmidt procedure applied to 1, x, x2, x3, . . ..\nWe note that the 1,\n\u221a\n2T1,\n\u221a\n2T2,\n\u221a\n2T3, . . . are the orthogonal polynomials corresponding to\nthe probability measure d\u00b5 =\ndx\n\u03c0\n\u221a\n1\u2212x2\n5.1.5\nBochner Integral and Bochner Spaces\nProofs and elaborations on the material appearing in this section can be found in (Kosaku\nYosida, 1963). Let (X, m, \u00b5) be a measure space and let H be a Hilbert space. A function\nf : X \u2192H is (Bochner) measurable if there exits a sequence of function fn : X \u2192H such\nthat\n\u2022 For almost every x \u2208X, f(x) = limn\u2192\u221efn(x).\n\u2022 The range of every fn is countable and, for every v \u2208H, f \u22121(v) is measurable.\nA measurable function f : X \u2192H is (Bochner) integrable if there exists a sequence of simple\nmeasurable functions (in the usual sense) sn such that limn\u2192\u221e\nR\nX ||f(x)\u2212sn(x)||Hd\u00b5(x) = 0.\nWe de\ufb01ne the integral of f to be\nR\nX fd\u00b5 = limn\u2192\u221e\nR\nsnd\u00b5, where the integral of a simple\nfunction s = Pn\ni=1 1Aivi, Ai \u2208m, vi \u2208H is\nR\nX sd\u00b5 = Pn\ni=1 \u00b5(Ai)vi.\nDe\ufb01ne by L2(X, H) the Kolmogorov quotient (by equality almost everywhere) of all\nmeasurable functions f : X \u2192H such that\nR\nX ||f||2\nHd\u00b5 < \u221e.\nTheorem 5.10 L2(X, H) in a Hilbert space w.r.t.\nthe inner product \u27e8f, g\u27e9L2(X,H) =\nR\nX\u27e8f(x), g(x)\u27e9Hd\u00b5(x)\n5.2\nLearnability implies small radius\nThe purpose of this section is to show that if X is a subset of some Hilbert space H such\nthat it is possible to learn a\ufb03ne functionals over X w.r.t. some loss, then we can essentially\nassume that X is contained is a unit ball and the returned a\ufb03ne functional is of norm O (m3),\nwhere m is the number of examples.\n19\nLemma 5.11 (John\u2019s Lemma) (Matousek, 2002) Let V be an m-dimensional real vector\nspace and let K be a full-dimensional compact convex set. There exists an inner product on\nV so that K is contained in a unit ball and contains a ball of radius\n1\nm, both are centered at\n(the same) x \u2208K. Moreover, if K is 0-symmetric it is possible to take x = 0 and the ratio\nbetween the radiuses can be improved to \u221am.\nLemma 5.12 Let l be a convex surrogate, let V an m-dimensional vector space and let\nX \u2282V be a bounded subset that spans V as an a\ufb03ne space. There exists an inner product\n\u27e8\u00b7, \u00b7\u27e9on V and a probability measure \u00b5N such that\n\u2022 For every w \u2208V, b \u2208R, ||w|| \u22644m2 Err\u00b5N,hinge(\u039bw,b)\n\u2022 X is contained in a unit ball.\nProof Let us apply John\u2019s Lemma to K = conv(X). It yields an inner product on V with\nK contained in a unit ball and containing the ball with radius\n1\nm both centered at the same\nx \u2208V . It remains to prove the existence of the measure \u00b5N. W.l.o.g., we assume that x = 0.\nLet e1, . . . , em \u2208V be an orthonormal basis. For every i \u2208[m], represent both\n1\nmei and\n\u22121\nmei as a convex combination of m + 1 elements from X:\n1\nmei =\nm+1\nX\nj=1\n\u03bbj\nixj\ni, \u22121\nmei =\nm+1\nX\nj=1\n\u03c1j\nizj\ni .\nNow, de\ufb01ne\n\u00b5N(xj\ni, 1) = \u00b5N(xj\ni, \u22121) = \u03bbj\ni\n4m, \u00b5N(zj\ni , 1) = \u00b5N(zj\ni , \u22121) = \u03c1j\ni\n4m .\n20\nFinally, let v \u2208V, b \u2208R. We have\nErr\u00b5N,hinge(\u039bw,b)\n=\nm\nX\ni=1\nm+1\nX\nj=1\n\u03bbj\ni\n4m\n\u0002\nlhinge(\u039bw,b(xj\ni)) + lhinge(\u2212\u039bw,b(xj\ni))\n\u0003\n+ \u03c1j\ni\n4m\n\u0002\nlhinge(\u039bw,b(zj\ni )) + lhinge(\u2212\u039bw,b(zj\ni ))\n\u0003\n\u2265\n1\n4m\nm\nX\ni=1\n\"\nlhinge\n m+1\nX\nj=1\n\u03bbj\ni(\u039bw,b(xj\ni))\n!\n+ lhinge\n \n\u2212\nm+1\nX\nj=1\n\u03bbj\ni(\u039bw,b(xj\ni))\n!#\n+\n\"\nlhinge\n m+1\nX\nj=1\n\u03c1j\ni(\u039bw,b(zj\ni ))\n!\n+ lhinge\n \n\u2212\nm+1\nX\nj=1\n\u03c1j\ni(\u039bw,b(zj\ni ))\n!#\n=\n1\n4m\nm\nX\ni=1\nlhinge\n\u0010D\nw, ei\nm\nE\n+ b\n\u0011\n+ lhinge\n\u0010\n\u2212\nD\nw, ei\nm\nE\n\u2212b\n\u0011\n+lhinge\n\u0010\n\u2212\nD\nw, ei\nm\nE\n+ b\n\u0011\n+ lhinge\n\u0010D\nw, ei\nm\nE\n\u2212b\n\u0011\n\u2265\n1\n4m\nm\nX\ni=1\nlhinge\n\u0012\n\u2212|\u27e8w, ei\u27e9|\nm\n\u0013\n\u2265\n1\n4m2\nm\nX\ni=1\n|\u27e8w, ei\u27e9|\n\u2265\n1\n4m2||w||\n2\nLet X be a bounded subset of some Hilbert space H and let C > 0. Denote\nFH(X, C) = {\u039bw,b|X | \u2225w\u2225H \u2264C, b \u2208R} .\nDenote by M the collection of all a\ufb03ne subspaces of H that are spanned by points from X.\nDenote by t = tH(X, C) be the maximal number such that for every a\ufb03ne subspace M \u2208M\nof dimension less than t there is x \u2208X such that d(x, M) > t\nC.\nLemma 5.13 Let X be a bounded subset of some Hilbert space H and let C > 0. There is\nA \u2282X with |A| = tH(X, C) such that [\u22121, 1]A \u2282FH(X, C)|A.\nProof Denote t = tH(X, C). Let x0, . . . , xt \u2208X be points such that the (t dimensional)\nvolume of the parallelogram Q de\ufb01ned by the vectors\nx1 \u2212x0, . . . , xt \u2212x0\nis maximal (if the supremum is not attained, the argument can be carried out with a parallel-\nogram whose volume is su\ufb03ciently close to the supremum). Let A = {x1, . . . , xt}. We claim\nthat for every 1 \u2264i \u2264t, the distance of xi from the a\ufb03ne span, Mi, of A\u222a{x0}\\{xi} is \u2265t\nC.\nIndeed, the volume of Q is the (t\u22121 dimensional) volume of the parallelogram Q\u2229(Mi \u2212x0)\ntimes d(xi, Mi). By the maximality of x0, . . . , xt and the de\ufb01nition of t, d(xi, Mi) \u2265t\nC.\n21\nFor 1 \u2264i \u2264t Let vi = xi \u2212PMixi. Note that \u2225vi\u2225H = d(xi, Mi) \u2265\nt\nC Now, given a\nfunction f : A \u2192[\u22121, 1], we will show that f \u2208FH(X, C). Consider the a\ufb03ne functional\n\u039b(x) =\nt\nX\ni=1\nf(xi)\n\u2225vi\u22252\nH\n\u27e8vi, x \u2212PMi(x)\u27e9H =\n*\nt\nX\ni=1\nf(xi)vi\n\u2225vi\u22252\nH\n, x\n+\nH\n\u2212\nt\nX\ni=1\n\u001cf(xi)vi\n\u2225vi\u22252\nH\n, PMi(x)\n\u001d\nH\n.\nNote that since vi is perpendicular to Mi, b := \u2212Pt\ni=1\nD\nf(xi)vi\n\u2225vi\u22252\nH , PMi(x)\nE\nH does not depend\non x. Let w := Pt\ni=1\nf(xi)vi\n\u2225vi\u22252\nH . We have\n\u2225w\u2225H \u2264\nt\nX\ni=1\n|f(xi)| 1\n\u2225vi\u2225\u2264tC\nt = C.\nTherefore, \u039b|X \u2208FH(X, C). Finally, for every 1 \u2264j \u2264t we have\n\u039b(xj) =\nt\nX\ni=1\nf(xi)\n\u2225vi\u22252\nH\n\u27e8vi, vj \u2212Pi(vj)\u27e9= f(xi).\nHere, the last inequality follows form that fact that for i \u0338= j, since vi is perpendicular to\nMi, \u27e8vi, vj \u2212Pi(vj)\u27e9= 0. Therefore, f = \u039b|A.\n2\nLet l : R \u2192R be a surrogate loss function. We say that an algorithm (\u03f5, \u03b4)-learns F \u2282RX\nusing m examples w.r.t. l if:\n\u2022 Its input is a sample of m points in X \u00d7 {\u00b11} and its output is a hypothesis in F.\n\u2022 For every distribution D on X \u00d7 {\u00b11}, it returns, with probability 1 \u2212\u03b4, \u02c6f \u2208F with\nErrD,l( \u02c6f) \u2264inff\u2208F ErrD,l(f) + \u03f5\nLemma 5.14 Suppose that an algorithm A (\u03f5, \u03b4)-learns F using m examples w.r.t. a sur-\nrogate loss l. Then, for every pair of distributions D and D\u2032 on X \u00d7 {\u00b11}, if \u02c6f \u2208F is the\nhypothesis returned by A running on D, then ErrD\u2032,l(f) \u2264m(l(0) + \u03f5) w.p. \u22651 \u22122e\u03b4.\nProof\nSuppose toward a contradiction that w.p. \u22652e\u03b4 we have ErrD\u2032,l(f) > a for a >\nm(l(0) + \u03f5). Consider the following distribution, \u02dcD: w.p.\n1\nm we sample from D\u2032 and with\nprobability 1 \u22121\nm we sample from D. Suppose now that we run the algorithm A on \u02dcD.\nConditioning on the event that all the samples are from D, we have, w.p.\n\u22652e\u03b4,\nErrD\u2032,l( \u02c6f) > a and therefore, Err \u02dcD,l( \u02c6f) > a\nm. The probability that indeed all the m samples\nare from D is\n\u00001 \u22121\nm\n\u0001m >\n1\n2e. Hence, with probability > \u03b4, we have Err \u02dcD,l( \u02c6f) > a\nm.\nOn the other hand, With probability \u22651 \u2212\u03b4 we have Err \u02dcD,l( \u02c6f) \u2264inff\u2208F Err \u02dcD(f) + \u03f5 \u2264\nErrD,l(0) + \u03f5 = l(0) + \u03f5. Hence, with positive probability,\na\nm \u2264l(0) + \u03f5 .\nIt follows that a \u2264m(l(0) + \u03f5).\n22\n2\nLemma 5.15 For every surrogate loss l with \u2202+l(0) < 0 there is a constant c > 0 such\nthat the following holds. Let X be a bounded subset of a Hilbert space H. If FH(X, C) is\n(\u03f5, \u03b4)-learnable using m example w.r.t. l then there is an inner product \u27e8\u00b7, \u00b7\u27e9s on H such that\n\u2022 X is contained in a unit ball w.r.t. \u2225\u00b7 \u2225s.\n\u2022 If A (\u03f5, \u03b4)-learns FH(X, C) then for every distribution D on X \u00d7 {\u00b11}, the hypothesis\n\u039bw,b returned by A has \u2225\u039bw,b\u2225s \u2264c \u00b7 m3 w.p. 1 \u22122e\u03b4.\n\u2022 The norm \u2225\u00b7 \u2225s is equivalent4 to \u2225\u00b7 \u2225H.\nRemark 5.16 If X is the image of some mapping \u03c8 : Z \u2192H then it follows from the lemma\nthat there is a normalized kernel k on Z such that the hypothesis returned by the learning\nalgorithm (interpreted as a function from Z to R) if the form f +b with \u2225f\u2225k \u2264c\u00b7m3. Also,\nif \u03c8 is continuous/absolutely continuous, then so is k.\nProof\nLet t = tH(X, C).\nBy lemma 5.13 there is some A \u2282X such that [\u22121, 1]A \u2282\nFH(X, C)|A. Since FH(X, C) is (\u03f5, \u03b4)-learnable using m examples, it is not hard to see that\nwe must have t \u2264c\u2032 \u00b7 m for some c\u2032 > 0 that depends only on l.\nTherefore, there exists an a\ufb03ne subspace M \u2282H of dimension d \u2264c\u2032m, such that\nfor every x \u2208X, d(x, M) <\nd\nC \u2264\nc\u2032m\nC . Moreover, we can assume that M is spanned by\nsome subset of X. Denote by \u02dc\nM the linear space corresponding to to M (i.e.,\n\u02dc\nM is the\ntranslation of M that contains 0). By lemma 5.12, there is an inner product \u27e8\u00b7, \u00b7\u27e91 on \u02dc\nM,\nand a probability measure \u00b5N on P \u02dc\nMX such that\n\u2022 For every w \u2208H, b \u2208R we have \u2225\u039bP \u02dc\nMw,b\u22251 \u22644d2 Err\u00b5N,hinge(\u039bw,b).\n\u2022 For all x \u2208X, \u2225P \u02dc\nMx\u22251 \u22641.\nFinally, de\ufb01ne\n\u27e8x, y\u27e9s = 1\n2\u27e8P \u02dc\nM(x), P \u02dc\nM(y)\u27e91 + C2\n2d2\u27e8P \u02dc\nM\u22a5(x), P \u02dc\nM\u22a5(y)\u27e9H .\nThe \ufb01rst and last assertions of the lemma are easy to verify, so we proceed to the second.\nLet \u039bw,b be the hypothesis returned by A after running on some m examples sampled from\nsome distribution D. Let DN be a probability measure on X whose projection on \u02dc\nM is \u00b5N.\nBy lemma 5.14 we have, with probability \u22651 \u22122e\u03b4, ErrDN,l(\u039bw,b) \u2264(l(0) + 1)m. We claim\n4Two norms \u2225\u00b7 \u2225and \u2225\u00b7 \u2225\u2032 on a vector space X are equivalent if for some c1, c2 > 0, \u2200x \u2208A, c1 \u00b7 \u2225x\u2225\u2264\n\u2225x\u2225\u2032 \u2264c2\u2225x\u2225\n23\nthat in this case Err\u00b5N,hinge(\u039bw,b) \u2264\n\u0010\nl(0)+1\n\u2202+l(0) + 2\n\u0011\nm. Indeed,\nErr\u00b5N,hinge(\u039bw,b)\n=\nE\n(x,y)\u223c\u00b5N lhinge(y\u039bw,b(x))\n=\nE\n(x,y)\u223cDN lhinge(y\u039bw,b(P \u02dc\nMx))\n=\nE\n(x,y)\u223cDN lhinge(y\u039bw,b(x + (x \u2212P \u02dc\nMx)))\n\u2264\nE\n(x,y)\u223cDN lhinge(y\u039bw,b(x)) + \u2225w\u2225H \u00b7 d(x, P \u02dc\nMx)\n\u2264\nErrDN,hinge(\u039bw,b) + C \u00b7 m\nC\n\u2264\n1\n|\u2202+l(0)| ErrDN,l(\u039bw,b) + 1 + m\n\u2264\nl(0) + 1\n\u2202+l(0) m + 2m\nBy the properties of \u00b5N, it follows that \u2225\u039bP \u02dc\nMw,b\u22251 = O (m3) (here, the constant in the big-O\nnotation depends only on l). Finally, we have,\n\u2225\u039bw\u22252\ns\n=\n\u2225\u039bP \u02dc\nMw\u22252\ns + \u2225\u039bP \u02dc\nM\u22a5w\u22252\ns\n=\n2\u2225\u039bP \u02dc\nMw\u22252\n1 + 2m2\nC2 \u2225\u039bP \u02dc\nM\u22a5w\u22252\nH\n\u2264\nO\n\u0000m6\u0001\n+ 2m2 \u2264O\n\u0000m6\u0001\n2\n5.3\nSymmetric Kernels and Symmetrization\nIn this section we concern symmetric kernels. Fix d \u22652 and let k : Sd\u22121 \u00d7 Sd\u22121 \u2192R be a\ncontinuous positive de\ufb01nite kernel. We say that k is symmetric if\n\u2200A \u2208O(d), x, y \u2208Sd\u22121, k(Ax, Ay) = k(x, y)\nIn other words, k(x, y) depends only on \u27e8x, y\u27e9Rd. A RKHS is called symmetric if its repro-\nducing kernel is symmetric. The next theorem characterize symmetric RKHSs. We note\nthat Theorems of the same spirit have already been proved (e.g. (Schoenberg, 1942)).\nTheorem 5.17 Let k : Sd\u22121\u00d7Sd\u22121 \u2192R be a normalized, symmetric and continuous kernel.\nThen,\n1. The group O(d) acts on Hk. That is, for every A \u2208O(d) and every f \u2208Hk if holds\nthat f \u25e6A \u2208Hk and ||f||Hk = ||f \u25e6A||Hk.\n2. The mapping \u03c1 : O(d) \u2192U(Hk) de\ufb01ned by \u03c1(A)f = f \u25e6A\u22121 is a unitary representation.\n3. The space Hk consists of continuous functions.\n24\n4. The decomposition of \u03c1 into a sum of irreducible representation is H = \u2295n\u2208IYd\nn for\nsome set I \u2282N0. Moreover,\n\u2200f, g \u2208Hk, \u27e8f, g\u27e9Hk =\nX\nn\u2208I\na2\nn\u27e8Pd,nf, Pd,ng\u27e9L2(Sd\u22121)\nWhere {an}n\u2208I are positive numbers.\n5. It holds that P\nn\u2208I\nNd,n\n|Sd\u22121|a\u22122\nn = 1.\nProof Let f \u2208Hk, A \u2208O(d). To prove part 1, assume \ufb01rst that\n\u2200x \u2208Sd\u22121, f(x) =\nn\nX\ni=1\n\u03b1ik(x, yi)\n(9)\nFor some y1, . . . , yn \u2208Sd\u22121 and \u03b11, . . . , \u03b1n \u2208C. We have, since k is symmetric, that\nf \u25e6A(x)\n=\nn\nX\ni=1\n\u03b1ik(Ax, yi)\n=\nn\nX\ni=1\n\u03b1ik(A\u22121Ax, A\u22121yi)\n=\nn\nX\ni=1\n\u03b1ik(x, A\u22121yi)\nThus, by Theorem 5.1, f \u25e6A \u2208Hk. Moreover, it holds that\n||f \u25e6A||2\nHk\n=\nX\n1\u2264i,j\u2264n\n\u03b1i\u00af\u03b1jk(A\u22121yj, A\u22121yi)\n=\nX\n1\u2264i,j\u2264n\n\u03b1i\u00af\u03b1jk(yj, yi) = ||f||2\nHk\nThus, part 1 holds for function f \u2208Hk of the form (9). For general f \u2208Hk, by Theorem 5.1,\nthere is a sequence fn \u2208Hk of functions of the from (9) that converges to f in Hk. From what\nwe have shown for functions of the form (9) if follows that ||fn\u2212fm||Hk = ||fn\u25e6A\u2212fm\u25e6A||Hk,\nthus fn \u25e6A is a Cauchy sequence, hence, has a limit g \u2208Hk. By Theorem 5.1, convergence\nin Hk entails point wise convergence, thus, g = f \u25e6A. Finally,\n||f||Hk = lim\nn\u2192\u221e||fn||Hk = lim\nn\u2192\u221e||fn \u25e6A||Hk = ||f \u25e6A||Hk\nWe proceed to part 2. It is not hard to check that \u03c1 is group homomorphism, so it\nonly remains to validate that for every f \u2208H the mapping A 7\u2192\u03c1(A)f is continuous. Let\n\u03f5 > 0 and let A \u2208O(d). We must show that there exists a neighbourhood U of A such that\n\u2200B \u2208U, ||f \u25e6A\u22121 \u2212f \u25e6B\u22121||Hk < \u03f5. Choose g(\u00b7) = Pn\ni=1 \u03b1ik(\u00b7, yi) such that ||g \u2212f||Hk < \u03f5\n3.\nBy part 1, it holds that\n||f \u25e6A\u22121 \u2212f \u25e6B\u22121||Hk\n\u2264\n||f \u25e6A\u22121 \u2212g \u25e6A\u22121||Hk + ||g \u25e6A\u22121 \u2212g \u25e6B\u22121||Hk + ||g \u25e6B\u22121 \u2212f \u25e6B\u22121||Hk\n=\n||f \u2212g||Hk + ||g \u25e6A\u22121 \u2212g \u25e6B\u22121||Hk + ||g \u2212f||Hk\n<\n\u03f5\n3 + ||g \u25e6A\u22121 \u2212g \u25e6B\u22121||Hk + \u03f5\n3\n25\nThus, it is enough to \ufb01nd a neighbourhood U of A such that \u2200B \u2208U, ||g\u25e6A\u22121\u2212g\u25e6B\u22121||Hk <\n\u03f5\n3. However,\n||g \u25e6A\u22121 \u2212g \u25e6B\u22121||2\nHk\n=\n||g \u25e6A\u22121||2\nHk + ||g \u25e6B\u22121||2\nHk \u22122 Re\n\"\n\u27e8\nn\nX\ni=1\n\u03b1ik(\u00b7, yi) \u25e6A\u22121,\nn\nX\ni=1\n\u03b1ik(\u00b7, yi) \u25e6B\u22121\u27e9\n#\n=\n2||g \u25e6A\u22121||2\nHk \u22122 Re\n\"\n\u27e8\nn\nX\ni=1\n\u03b1ik(\u00b7, Ayi),\nn\nX\ni=1\n\u03b1ik(\u00b7, Byi)\u27e9\n#\n=\n2||g \u25e6A\u22121||2\nHk \u2212Re\n\"\nn\nX\ni,j=1\n\u03b1i\u00af\u03b1jk(Byj, Ayi)\n#\nSince\nk\nis\ncontinuous,\nthe\nlast\nexpres-\nsion tends to 2||g \u25e6A\u22121||2\nHk \u2212Re\nhPn\ni,j=1 \u03b1i\u00af\u03b1jk(Ayj, Ayi)\ni\n= ||g \u25e6A \u2212g \u25e6A||2\nHk = 0 as\nB \u2192A. Thus, there exists a neighbourhood U such that \u2200B \u2208U, ||g \u25e6A\u22121 \u2212g \u25e6B\u22121||Hk < \u03f5\n3\nas required.\nTo see part 3, note that every function in Hk is a limit in Hk of functions of the form\n(9). Since k is continuous, every function in Hk is a limit in Hk of continuous functions.\nHowever, by Theorem 5.1, every function is in fact a uniform limit of continuous function,\nthus \u2013 continuous itself.\nWe proceed to part 4. By Theorem 5.4 Hk = \u2295i\u2208IVi where each Vi is a \ufb01nite dimensional\nspace that is invariant to \u03c1. By Theorem 5.6 each Vi must be Yn for some n, thus, H =\n\u2295n\u2208IYd\nn. By the uniqueness part in Lemma 5.5 and Theorem 5.6, the restriction of \u27e8\u00b7, \u00b7\u27e9Hk to\neach Yd\nn, n \u2208I equals to \u27e8\u00b7, \u00b7\u27e9L2(Sd\u22121) up to scalar multiple, proving the formula for \u27e8\u00b7, \u00b7\u27e9Hk\nFinally, to see equation part 5, note that if for every n \u2208I, {Y d\nn,j}j\u2208[Nd,n] in an orthonormal\nbasis of Yd\nn w.r.t.\n\u27e8\u00b7, \u00b7\u27e9L2(Sd\u22121) then { 1\nanY d\nn,j}n\u2208I,j\u2208[Nd,n] is an orthogonal basis of H.\nBy\nTheorem 5.3 and Lemma 5.7, it follows that, for every x \u2208Sd\u22121,\n1 = k(x, x) =\nX\nn\u2208I\na\u22122\nn\nNd,n\nX\nj=1\n(Y d\nn,j(x))2 =\nX\nn\u2208I\nNd,n\n|Sd\u22121|a\u22122\nn\n2\nSymmetrization\nLet k : Sd\u22121 \u00d7 Sd\u22121 \u2192R be a normalized continuous kernel. We de\ufb01ne its symmetrization\nby\n\u2200x, y \u2208Sd\u22121, ks(x, y) =\nZ\nO(d)\nk(Ax, Ay)dA\nTheorem 5.18\n1. ks is symmetric continuous kernel with supx\u2208Sd\u22121 ks(x, x) \u22641.\n2. For every \u03a6 \u2208L2(O(d), Hk) de\ufb01ne \u00af\u03a6 : Sd\u22121 \u2192C by \u00af\u03a6(x) =\nR\nO(d) \u03a6(A)(Ax)dA. Then\nHks = {\u00af\u03a6 : \u03a6 \u2208L2(O(d), Hk)}\nMoreover, for every \u03a6 \u2208L2(O(d), Hk), ||\u00af\u03a6||Hks \u2264||\u03a6||L2(O(d),Hk).\n26\nProof Part 1. follows readily from the de\ufb01nition. We proceed to part 2. De\ufb01ne \u03c6 : Sd\u22121 \u2192\nL2(O(d), Hk) by\n\u03c6(x)(A)(\u00b7) = k(Ax, \u00b7)\nNote that\n\u27e8\u03c6(x), \u03c6(y)\u27e9L2(O(d),Hk)\n=\nZ\nO(d)\n\u27e8\u03c6(x)(A), \u03c6(y)(A)\u27e9\n=\nZ\nO(d)\n\u27e8\u03c6(x)(A), \u03c6(y)(A)\u27e9\n=\nks(x, y)\nThus, the Theorem follows from Theorem 5.1.1\n2\n5.4\nLemma 5.22 and its proof\nLemma 5.19 For every n > 0, d \u22655 and t \u2208[\u22121, 1] it holds that\n|Pd,n(t)| \u2264min\n(\n\u0393\n\u0000 d\u22121\n2\n\u0001\n\u221a\u03c0\n\u0014\n4\nn(1 \u2212t2)\n\u0015 d\u22122\n2\n,\n\u0012\nn\nn + d \u22122 + 2|t|\n\u0013 n\n2 )\nMoreover, if\nn\nn+d\u22122 + 2|t| \u22641 we also have\n|Pd,n(t)| \u2264\nv\nu\nu\nt\nn\nY\ni=1\n\u0012\ni\ni + d \u22122 + 2|t|\n\u0013\nFinally, there exist constants E > 0 and 0 < r, s < 1 such that for every K > 0, d \u22655 and\nt \u2208\n\u0002\n\u22121\n8, 1\n8\n\u0003\nwe have\n\u221e\nX\nn=K\n|Pd,n(t)| \u2264ErK + Esd\nProof\nIn (Atkinson and Han, 2012) it is shown that |Pd,n(t)| \u2264\n\u0393( d\u22121\n2 )\n\u221a\u03c0\nh\n4\nn(1\u2212t2)\ni d\u22122\n2 . We\nshall prove, by induction on k that\n|Pd,n(t)| \u2264\nv\nu\nu\nt\nn\nY\ni=1\n\u0012\ni\ni + d \u22122 + 2|t|\n\u0013\nWhenever\nn\nn+d\u22122 + 2|t| \u22641.\nFor n = 0, 1 it follows from the fact that Pd,0 \u22611 and\nPd,1(t) = t. Let n > 1. By the induction hypothesis and the recursion formula for the\n27\nLegendre polynomials we have\n|Pd,n(t)|\n\u2264\n2n + d \u22124\nn + d \u22123 |t||Pd,n\u22121(t)| +\nn \u22121\nn + d \u22123|Pd,n\u22122(t)|\n\u2264\n2|t||Pd,n\u22121(t)| +\nn \u22121\nn + d \u22123|Pd,n\u22122(t)|\n\u2264\n2|t|\nv\nu\nu\nt\nn\u22121\nY\ni=1\n\u0012\ni\ni + d \u22122 + 2|t|\n\u0013\n+\nn \u22121\nn + d \u22123\nv\nu\nu\nt\nn\u22122\nY\ni=1\n\u0012\ni\ni + d \u22122 + 2|t|\n\u0013\n\u2264\n2|t|\nv\nu\nu\nt\nn\u22122\nY\ni=1\n\u0012\ni\ni + d \u22122 + 2|t|\n\u0013\n+\nn \u22121\nn + d \u22123\nv\nu\nu\nt\nn\u22122\nY\ni=1\n\u0012\ni\ni + d \u22122 + 2|t|\n\u0013\n\u2264\ns\u0012\n2|t| +\nn \u22121\nn + d \u22123\n\u0013 \u0012\n2|t| +\nn\nn + d \u22122\n\u0013v\nu\nu\nt\nn\u22122\nY\ni=1\n\u0012\ni\ni + d \u22122 + 2|t|\n\u0013\n=\nv\nu\nu\nt\nn\nY\ni=1\n\u0012\ni\ni + d \u22122 + 2|t|\n\u0013\nNow, every K, \u00afK \u22650 such that\n\u0010\n\u00af\nK\n\u00af\nK+d\u22122 + 2|t|\n\u0011 1\n2 < 1, we have\n\u221e\nX\nn=K\n|Pd,n(t)|\n\u2264\n\u00af\nK\nX\nn=K\n\u0012\nn\nn + d \u22122 + 2|t|\n\u0013 n\n2\n+\n\u221e\nX\nn= \u00af\nK+1\n\u0393\n\u0000 d\u22121\n2\n\u0001\n\u221a\u03c0\n\u0014\n4\nn(1 \u2212t2)\n\u0015 d\u22122\n2\n\u2264\n\u00af\nK\nX\nn=K\n\u0012\n\u00afK\n\u00afK + d \u22122 + 2|t|\n\u0013 n\n2\n+\n\u221e\nX\nn= \u00af\nK+1\n\u0393\n\u0000 d\u22121\n2\n\u0001\n\u221a\u03c0\n\u0014\n4\nn(1 \u2212t2)\n\u0015 d\u22122\n2\n\u2264\n\u221e\nX\nn=K\n\u0012\n\u00afK\n\u00afK + d \u22122 + 2|t|\n\u0013 n\n2\n+ \u0393\n\u0000 d\u22121\n2\n\u0001\n\u221a\u03c0\n\u0014\n4\n(1 \u2212t2)\n\u0015 d\u22122\n2\n\u221e\nX\nn= \u00af\nK+1\nn\u2212d\u22122\n2\n\u2264\n\u0010\n\u00af\nK\n\u00af\nK+d\u22122 + 2|t|\n\u0011 K\n2\n1 \u2212\n\u0010\n\u00af\nK\n\u00af\nK+d\u22122 + 2|t|\n\u0011 1\n2 + \u0393\n\u0000 d\u22121\n2\n\u0001\n\u221a\u03c0\n\u0014\n4\n(1 \u2212t2)\n\u0015 d\u22122\n2\n\u221e\nX\nn= \u00af\nK+1\nn\u2212d\u22122\n2\n\u2264\n\u0010\n\u00af\nK\n\u00af\nK+d\u22122 + 2|t|\n\u0011 K\n2\n1 \u2212\n\u0010\n\u00af\nK\n\u00af\nK+d\u22122 + 2|t|\n\u0011 1\n2 + \u0393\n\u0000 d\u22121\n2\n\u0001\n\u221a\u03c0\n\u0014\n4\n(1 \u2212t2)\n\u0015 d\u22122\n2 Z \u221e\n\u00af\nK\nx\u2212d\u22122\n2 dx\n=\n\u0010\n\u00af\nK\n\u00af\nK+d\u22122 + 2|t|\n\u0011 K\n2\n1 \u2212\n\u0010\n\u00af\nK\n\u00af\nK+d\u22122 + 2|t|\n\u0011 1\n2 + \u0393\n\u0000 d\u22121\n2\n\u0001\n\u221a\u03c0\n\u0014\n4\n(1 \u2212t2)\n\u0015 d\u22122\n2\n\u00afK\u2212d\u22124\n2\nd\u22124\n2\n28\n(We limit ourselves to d \u22655 to guarantee the convergence of P n\u2212d\u22122\n2 .) In particular, if\n|t| \u22641\n8 and \u00afK = d \u22122, we have,\n\u221e\nX\nn=K\n|Pd,n(t)|\n\u2264\n\uf8eb\n\uf8ed\n1\n1 \u2212\n\u0000 3\n4\n\u0001 1\n2\n\uf8f6\n\uf8f8\n\u00123\n4\n\u0013 K\n2\n+ \u0393\n\u0000 d\u22121\n2\n\u0001\n\u221a\u03c0\n\u0014 4.07\n(d \u22122)\n\u0015 d\u22122\n2\nd \u22122\nd\u22124\n2\n\u2264\n\uf8eb\n\uf8ed\n1\n1 \u2212\n\u0000 3\n4\n\u0001 1\n2\n\uf8f6\n\uf8f8\n\u00123\n4\n\u0013 K\n2\n+ 6\u0393\n\u0000 d\u22121\n2\n\u0001\n\u221a\u03c0\n\u0014 4.07\n(d \u22122)\n\u0015 d\u22122\n2\n\u223c\n\uf8eb\n\uf8ed\n1\n1 \u2212\n\u0000 3\n4\n\u0001 1\n2\n\uf8f6\n\uf8f8\n\u00123\n4\n\u0013 K\n2\n+ 6\n\u221a\u03c0\n\u0014 4.07\n(d \u22122)\n\u0015 d\u22122\n2 s\n2\u03c0\nd\u22122\n2\n\u0012d \u22122\n2e\n\u0013 d\u22122\n2\n=\n\uf8eb\n\uf8ed\n1\n1 \u2212\n\u0000 3\n4\n\u0001 1\n2\n\uf8f6\n\uf8f8\n\u00123\n4\n\u0013 K\n2\n+ 12\n\u00144.07\n2e\n\u0015 d\u22122\n2\n2\nLemma 5.20 Let \u00b5 be a probability measure on [\u22121, 1] and let p0, p1, . . . be the corresponding\northogonal polynomials. Then, for every f \u2208span{p0, . . . , pK\u22121} we have\n||f||2 \u2264\n\u221a\nK||f||1 \u00b7\nmax\n0\u2264i\u2264K\u22121 ||pi||\u221e\nHere, all Lp norms are w.r.t. \u00b5.\nProof Write f = PK\u22121\ni=0 \u03b1ipi and denote M = max0\u2264i\u2264K\u22121 ||pi||\u221e.We have\n||f||2\n2\n\u2264\n||f||1 \u00b7 ||f||\u221e\n\u2264\n||f||1 \u00b7 M\nK\u22121\nX\nn=0\n|\u03b1k|\n\u2264\n||f||1 \u00b7 M\nv\nu\nu\nt\nK\u22121\nX\nn=0\n\u03b12\nk \u00b7\n\u221a\nK\n=\n||f||1 \u00b7 M \u00b7 ||f||2\n\u221a\nK\n2\nLemma 5.21 Let d \u22655 and let f : [\u22121, 1] \u2192R be a continuous function whose expansion\nin the basis of d-dimensional Legendre polynomials is\nf =\n\u221e\nX\nn=0\n\u03b1nPd,n\nDenote C = supn |\u03b1n|. Let \u00b5 be the probability measure on [\u22121, 1] whose density function is\nw(x) =\n(\n0\n|x| > 1\n8\n8\n\u03c0\u221a\n1\u2212(8x)2\n|x| \u22641\n8\n29\nThen, for every K \u2208N, 1\n8 > \u03b3 > 0,\n|f(\u03b3) \u2212f(\u2212\u03b3)| \u226432\u03b3K3.5 \u00b7 ||f||1,\u00b5 +\n\u000032\u03b3K3.5 + 2\n\u0001\n\u00b7 C \u00b7 E \u00b7 (rK + sd)\nHere, E, r and s are the constants from Lemma 5.19.\nProof Let \u00aff = PK\u22121\nn=0 \u03b1nPd,n. We have || \u00aff||1,\u00b5 \u2264||f||1,\u00b5+|| \u00aff \u2212f||\u221e,\u00b5. De\ufb01ne g : [\u22121, 1] \u2192R\nby g(x) = \u00aff( x\n8) and denote by d\u03bb =\ndx\n\u03c0\n\u221a\n1\u2212x2. Write,\ng =\nK\u22121\nX\nn=0\n\u03b2nTn\nWhere Tn are the Chebyshev polynomials. By Lemma 5.20 it holds that, for every 0 \u2264n \u2264\nK \u22121,\n|\u03b2n| \u2264\n\u221a\n2||g||2,\u03bb \u22642\n\u221a\nK||g||1,\u03bb = 2\n\u221a\nK|| \u00aff||1,\u00b5\nNow,\ng\u2032 =\nK\u22121\nX\nn=1\n\u03b2knUn\u22121\nWhere Un are the Chebyshev polynomials of the second kind. Thus,\n||g\u2032||\u221e,\u03bb \u2264\nK\u22121\nX\nn=1\n|\u03b2k| \u00b7 n \u00b7 ||Un\u22121||\u221e,\u03bb =\nK\u22121\nX\nn=1\n|\u03b2k| \u00b7 n2 \u22642\n\u221a\nK|| \u00aff||1,\u00b5 \u00b7 K3\nFinally, by Lemma 5.19,\n|f(\u03b3) \u2212f(\u2212\u03b3)|\n\u2264\n|g(8\u03b3) \u2212g(\u22128\u03b3)| + 2||f \u2212\u00aff||\u221e,\u00b5\n\u2264\n32\u03b3K3.5 \u00b7 || \u00aff||1,\u00b5 + 2||f \u2212\u00aff||\u221e,\u00b5\n\u2264\n32\u03b3K3.5 \u00b7\n\u0000||f||1,\u00b5 + ||f \u2212\u00aff||\u221e,\u00b5\n\u0001\n+ 2||f \u2212\u00aff||\u221e,\u00b5\n\u2264\n32\u03b3K3.5 \u00b7 ||f||1,\u00b5 +\n\u000032\u03b3K3.5 + 2\n\u0001\n\u00b7 ||f \u2212\u00aff||\u221e,\u00b5\n\u2264\n32\u03b3K3.5 \u00b7 ||f||1,\u00b5 +\n\u000032\u03b3K3.5 + 2\n\u0001\n\u00b7 E \u00b7 C \u00b7 (rK + sd)\n2\nFor e \u2208Sd\u22121 we de\ufb01ne the group O(e) := {A \u2208O(d) : Ae = e}. If Hk be a symmetric\nRKHS and e \u2208Sd\u22121 we de\ufb01ne Symmetrization around e. This is the operator Pe : Hk \u2192Hk\nwhich is the projection on the subspace {f \u2208Hk : \u2200A \u2208O(e), f \u25e6A = f}. It is not hard\nto see that (Pef)(x) =\nR\n{x\u2032:\u27e8x\u2032,e\u27e9=\u27e8x,e\u27e9} f(x\u2032)dx\u2032 =\nR\nO(e) f \u25e6A(x)dA. Since Pef is a convex\ncombination of the functions {f \u25e6A}A\u2208O(e), it follows that if R : Hk \u2192R is a convex\nfunctional then R(Pef) \u2264\nR\nO(e) R(f \u25e6A)dA.\nLemma 5.22 (main) There exists a probability measure \u00b5 on [\u22121, 1] with the following\nproperties. For every continuous and normalized kernel k : Sd\u22121 \u00d7 Sd\u22121 \u2192R and C > 0,\nthere exists e \u2208Sd\u22121 such that, for every f \u2208Hk with ||f||Hk \u2264C, K \u2208N and 0 < \u03b3 < 1\n8,\n\f\f\f\f\nZ\n{x:\u27e8x,e\u27e9=\u03b3}\nf \u2212\nZ\n{x:\u27e8x,e\u27e9=\u2212\u03b3}\nf\n\f\f\f\f\n\u2264\n32\u03b3K3.5 \u00b7 ||f||1,\u00b5e +\n\u000032\u03b3K3.5 + 2\n\u0001\n\u00b7 E \u00b7 C \u00b7 (rK + sd)\n\u2264\n32\u03b3K3.5 \u00b7 ||f||1,\u00b5e + 10 \u00b7 E \u00b7 K3.5 \u00b7 C \u00b7 (rK + sd)\n30\nThe integrals are w.r.t. the uniform probability over {x : \u27e8x, e\u27e9= \u03b3} and {x : \u27e8x, e\u27e9= \u2212\u03b3}\nand E, r, s are the constants from Lemma 5.19.\nProof\nSuppose \ufb01rst that k is symmetric. Let \u00b5 be the distribution over [\u22121, 1] whose\ndensity function is\nw(x) =\n(\n0\n|x| > 1\n8\n8\n\u03c0\u221a\n1\u2212(8x)2\n|x| \u22641\n8\nWe can assume that f is O(e)-invariant. Otherwise, we can replace f with Pef, which does\nnot change the l.h.s. and does not increase the r.h.s. This assumption yields (see (Atkinson\nand Han, 2012), pages 17-18)\nf(x) =\n\u221e\nX\nn=0\n\u03b1nPd,n(\u27e8e, x\u27e9).\nThe L2(Sd\u22121)-norm of the map x 7\u2192Pd,n(\u27e8x, e\u27e9) is |Sd\u22121|\nNd,n (e.g. (Atkinson and Han, 2012),\npage 71). Therefore,\n||f||2\nk =\nX\nn\u2208I\n|Sd\u22121|\nNd,n\na2\nn\u03b12\nn\nwhere {an}n\u2208I are the numbers corresponding to Hk from Theorem 5.17. In particular (since\nalso for n \u0338\u2208I, \u03b1n = 0),\n|\u03b1n|2 \u2264Nd,n\n|Sd\u22121|a\u22122\nn ||f||2\nk \u2264||f||2\nk\nWrite\ng(t) = f(te), t \u2208[\u22121, 1]\nBy Lemma 5.21,\n|g(\u03b3) \u2212g(\u2212\u03b3)| \u226432\u03b3K3.5 \u00b7 ||f||1,\u00b5 +\n\u000032\u03b3K3.5 + 2\n\u0001\n\u00b7 E \u00b7 C \u00b7 (rK + sd)\nFinally,\nR\n{x:\u27e8x,e\u27e9=\u03b3} f = g(\u03b3),\nR\n{x:\u27e8x,e\u27e9=\u2212\u03b3} f = g(\u2212\u03b3) since f is O(e)-invariant. The Lemma\nfollows.\nWe proceed to the general case where k is not necessarily symmetric. Assume by way of\ncontradiction that for every e \u2208Sd\u22121, there exists a function fe such that\nZ\n{x:\u27e8x,e\u27e9=\u03b3}\nfe\u2212\nZ\n{x:\u27e8x,e\u27e9=\u2212\u03b3}\nfe > 32\u03b3K3.5\u00b7||fe||1,\u00b5e +\n\u000032\u03b3K3.5 + 2\n\u0001\n\u00b7||fe||Hk \u00b7C\u00b7(rK +sd) (10)\nFor convenience we normalize, so l.h.s.\nequals 1.\nFix a vector e0 \u2208Sd\u22121.\nDe\ufb01ne \u03a6 \u2208\nL2(O(d), Hk) by\n\u03a6(A) = fAe0\nand let f \u2208Hks be the function\nf(x) =\nZ\nO(d)\n\u03a6(A)(Ax)dA =\nZ\nO(d)\nfAe0(Ax)dA\n31\nNow, it holds that\nZ\n{x:\u27e8x,e0\u27e9=\u03b3}\nf \u2212\nZ\n{x:\u27e8x,e0\u27e9=\u2212\u03b3}\nf\n=\nZ\n{x:\u27e8x,e0\u27e9=\u03b3}\nZ\nO(d)\nfAe0(Ax)dAdx \u2212\nZ\n{x:\u27e8x,e0\u27e9=\u2212\u03b3}\nZ\nO(d)\nfAe0(Ax)dAdx\n=\nZ\nO(d)\nZ\n{x:\u27e8x,e0\u27e9=\u03b3}\nfAe0(Ax)dx \u2212\nZ\n{x:\u27e8x,e0\u27e9=\u2212\u03b3}\nfAe0(Ax)dxdA\n=\nZ\nO(d)\nZ\n{x:\u27e8x,Ae0\u27e9=\u03b3}\nfAe0(x)dx \u2212\nZ\n{x:\u27e8x,Ae0\u27e9=\u2212\u03b3}\nfAe0(x)dxdA\n=\n1\nOn the other hand\n||f||1,\u00b5e\n=\nZ\nSd\u22121\n\f\f\f\f\nZ\nO(d)\nfAe0(Ax)dA\n\f\f\f\f d\u00b5e0(x)\n\u2264\nZ\nO(d)\nZ\nSd\u22121 |fAe0(Ax)| d\u00b5e0(x)dA\n\u2264\nZ\nO(d)\nZ\nSd\u22121 |fAe0(x)| d\u00b5Ae0(x)dA\n=\nZ\nO(d)\n||fAe0||1,\u00b5Ae0dA\nMoreover, by Theorem 5.18,\n||f||2\nHks \u2264||\u03a6||2\nL2(O(d),Hk) =\nZ\nO(d)\n||fAe0||2\nHkdA \u2264C2\nSince the Lemma is already proved for symmetric kernels, it follows that\n1\n\u2264\n32\u03b3K3.5 \u00b7 ||f||1,\u00b5e0 +\n\u000032\u03b3K3.5 + 2\n\u0001\n\u00b7 E \u00b7 C \u00b7 (rK + sd)\n\u2264\n32\u03b3K3.5 \u00b7\nZ\nO(d)\n||fAe0||1,\u00b5Ae0dA +\n\u000032\u03b3K3.5 + 2\n\u0001\n\u00b7 E \u00b7 C \u00b7 (rK + sd)\n=\nZ\nO(d)\n32\u03b3K3.5 \u00b7 ||fAe0||1,\u00b5Ae0 +\n\u000032\u03b3K3.5 + 2\n\u0001\n\u00b7 E \u00b7 C \u00b7 (rK + sd)dA\nThus, for some A \u2208O(d)\n1 \u226432\u03b3K3.5 \u00b7 ||fAe0||1,\u00b5Ae0 +\n\u000032\u03b3K3.5 + 2\n\u0001\n\u00b7 E \u00b7 C \u00b7 (rK + sd)\nContradicting Equation (10).\n2\n5.5\nProofs of the main Theorems\nWe are now ready to prove Theorems 2.6 and 3.2.\nWe only consider distributions that\nsupported on the unit sphere, and we can therefore assume that the problem is formulated\n32\nit terms of the unit sphere and not the unit ball.\nAlso, we reformulate program (5) as\nfollows: Given l : R \u2192R a convex surrogate, a constant C > 0 and a continuous kernel\nk : S\u221e\u00d7 S\u221e\u2192R with supx\u2208S\u221ek(x, x) \u22641, we want to solve\nmin\nErrD,l (f + b)\ns.t.\nf \u2208Hk, b \u2208R\n(11)\n||f||Hk \u2264C\nWe can assume that \u2202+l(0) < 0, for otherwise the approximation ratio is \u221e. To see that,\nlet the distribution D be concentrated on a single point on the sphere and always return the\nlabel 1. Of course, Err\u03b3(D) = 0. However, if \u2202+l(0) \u22650, it is bot hard to see that if f, b is\nthe solution of program (11), then f(x) + b \u22640, so that Err0\u22121(f + b) = 1.\nLemma 5.23 Let l be a surrogate loss, \u00b5 a probability measure on Sd\u22121 and f \u2208C(Sd\u22121).\nLet \u00af\u00b5 be the probability measure on Sd\u22121 \u00d7 {\u00b11} which is the product measure of \u00b5 and the\nuniform distribution on {\u00b11}. Then\n||f||1,\u00b5 \u2264\n2\n|\u2202+l(0)| Err\u00af\u00b5,l(f)\nProof By Jansen\u2019s inequaliy, it holds that\nErr\u00af\u00b5,l(f)\n=\nE(x,y)\u223c\u00af\u00b5l(y \u00b7 f(x))\n=\n1\n2E(x,y)\u223c\u00af\u00b5l(f(x)) + l(\u2212f(x))\n\u2265\n1\n2E(x,y)\u223c\u00af\u00b5l(\u2212|f(x)|)\n\u2265\n1\n2l\n\u0000\u2212E(x,y)\u223c\u00af\u00b5|f(x)|\n\u0001\nIt follows that l (\u2212||f||1,\u00b5) \u22642 Err\u00af\u00b5,l(f). By the convexity of l, it follows that for every\nx \u2208R, l(x) \u2265l(0) + x \u00b7 \u2202+l(0) = l(0) \u2212x \u00b7 |\u2202+l(0)| \u2265\u2212x \u00b7 |\u2202+l(0)|. Thus,\n||f||1,\u00b5 \u2264\n2\n|\u2202+l(0)| Err\u00af\u00b5,l(f)\n2\n5.5.1\nTheorems 2.6 and 3.2\nWe will need Levy\u2019s measure concentration Lemma (e.g., (Milman and Schechtman, 2002)).\nLet f : X \u2192Y be an absolutely continuous map between metric spaces. We de\ufb01ne its\nmodulus of continuity as\n\u2200\u03f5 > 0, \u03c9f(\u03f5) = sup{d(f(x), f(y)) : x, y \u2208X, d(x, y) \u2264\u03f5}\nTheorem 5.24 (Levy\u2019s Lemma) There exists a constant \u03b7 > 0 such that for every con-\ntinuous function f : Sd\u22121 \u2192R,\nPr (|f \u2212Ef| > \u03c9f(\u03f5)) \u2264exp\n\u0000\u2212\u03b7d\u03f52\u0001\nHere, both probability and expectation are w.r.t. the uniform distribution.\n33\nWe note that \u03c9f\u25e6g \u2264\u03c9f \u00b7\u03c9g and that \u03c9\u039bv(\u03f5) = \u2225v\u2225\u00b7\u03f5. Thus, if \u03c8 : S\u221e\u2192H1 is an absolutely\ncontinuous embedding such that k(x, y) = \u27e8\u03c8(x), \u03c8(y)\u27e9H1, then for every v \u2208H1, it holds\nthat \u03c9\u039bv,0\u25e6\u03c8 \u2264||v||H1 \u00b7 \u03c9\u03c8. Suppose now that f \u2208Hk with \u2225f\u2225Hk \u2264C. Let v \u2208H1 such that\nf = \u039bv,0 \u25e6\u03c8 and ||v||H1 = ||f||Hk \u2264C. It follows from Levi\u2019s Lemma that\nPr (|f \u2212Ef| > C \u00b7 \u03c9\u03c8(\u03f5)) \u2264Pr (|f \u2212Ef| > \u03c9f(\u03f5)) \u2264exp\n\u0000\u2212\u03b7d\u03f52\u0001\n(12)\nAgain, when both probability and expectation are w.r.t. the uniform distribution over Sd\u22121.\nProof (of Theorems 2.6 and 3.2) Let \u03b2 > \u03b1 > 0 such that l(\u03b1) > l(\u03b2). Choose 0 < \u03b8 < 1\nlarge enough so that (1\u2212\u03b8)l(\u2212\u03b2)+\u03b8l(\u03b2) < \u03b8l(\u03b1). De\ufb01ne probability measures \u00b51, \u00b52, \u00b5 over\n[\u22121, 1] \u00d7 {\u00b11} as follows.\n\u00b51((\u2212\u03b3, \u22121)) = 1 \u2212\u03b8, \u00b51((\u03b3, 1)) = \u03b8\nThe measure \u00b52 is the product of uniform{\u00b11} and the measure on [\u22121, 1] whose density\nfunction is\nw(x) =\n(\n0\n|x| > 1\n8\n8\n\u03c0\u221a\n1\u2212(8x)2\n|x| \u22641\n8\nFinally, \u00b5 = (1 \u2212\u03bb)\u00b51 + \u03bb\u00b52 for \u03bb > 0, which will be chosen later.\nBy lemma 5.15 (see remark 5.16), there is a continuous normalized kernel k\u2032 such that\nw.p. \u22651 \u22122e exp(\u22121\n\u03b3) the function returned by the algorithm is of the form f + b with\n\u2225f\u2225Hk\u2032 \u2264c \u00b7 m3\nA(\u03b3) for some c > 0 (depending only on l). Let e \u2208Sd\u22121 be the vector from\nLemma 5.22, corresponding to the kernel k\u2032. The distribution D is the pullback of \u00b5 w.r.t.\ne. By considering the a\ufb03ne functional \u039be,0, it holds that Err\u03b3(D) \u2264\u03bb.\nLet g be the solution returned by the algorithm. With probability \u22651 \u2212exp(\u22121/\u03b3),\ng = f + b, where f, b is a solution to program (11) with C = CA(\u03b3) and with an additive\nerror \u2264\u221a\u03b3. Since the value of the zero solution for program (11) is l(0), it follows that\nl(0) + \u221a\u03b3 \u2265Err\u00b5,l(g) = (1 \u2212\u03bb) Err\u00b51e,l(g) + \u03bb Err\u00b52e,l(g)\nThus, Err\u00b52e,l(g) \u2264l(0)+\u221a\u03b3\n\u03bb\n\u22642l(0)\n\u03bb . Combining Lemma 5.23, Lemma 5.15, and Lemma 5.22 is\nfollows that w.p. \u22651 \u2212(1 + 2e) exp(\u22121\n\u03b3) \u22651 \u221210 exp(\u22121\n\u03b3), for m = mA(\u03b3)\n\f\f\f\f\nZ\n{x:\u27e8x,e\u27e9=\u03b3}\ng \u2212\nZ\n{x:\u27e8x,e\u27e9=\u2212\u03b3}\ng\n\f\f\f\f \u2264128l(0)\u03b3K3.5\n|\u2202+l(0)|\u03bb\n+ 10 \u00b7 c \u00b7 K3.5 \u00b7 E \u00b7 m3 \u00b7 (rK + sd)\nBy choosing K = \u0398(log(m)), \u03bb = \u0398 (\u03b3K3.5) = \u0398\n\u0000\u03b3 log3.5(m)\n\u0001\nand d = \u0398(log(m)), we can\nmake the last bound \u2264\u03b1\n2 . We claim that\nR\n{x:\u27e8x,e\u27e9=\u2212\u03b3} g > \u03b1\n2 . To see that, note that otherwise\nR\n{x:\u27e8x,e\u27e9=\u03b3} g \u2264\u03b1 thus,\nE(x,y)\u223cDl((f(x) + b)y)\n=\nE(x,y)\u223cDl(g(x)y)\n\u2265\n\u03b8(1 \u2212\u03bb) \u00b7\nZ\n{x:\u27e8x,e\u27e9=\u03b3}\nl(g(x))dx\n\u2265\n\u03b8(1 \u2212\u03bb) \u00b7 l\n\u0012Z\n{x:\u27e8x,e\u27e9=\u03b3}\ng(x)dx\n\u0013\n\u2265\n\u03b8 \u00b7 l (\u03b1) \u00b7 (1 \u2212\u03bb) = \u03b8 \u00b7 l (\u03b1) + o(1)\n34\nThis contradict the optimality of f, b, as for f \u2032 = 0, b\u2032 = \u03b2 it holds that\nE(x,y)\u223cDl((f \u2032(x) + b\u2032)y)\n\u2264\n\u03bbl(\u2212\u03b2) + (1 \u2212\u03bb) \u00b7 (1 \u2212\u03b8)l(\u2212\u03b2) + \u03b8 \u00b7 l(\u03b2))\n=\n(1 \u2212\u03b8)l(\u2212\u03b2) + \u03b8 \u00b7 l(\u03b2) + o(1)\nWe can conclude now the proof of Theorem 2.6.\nBy choosing d large enough\nand using Equation (12), we can guarantee that g|{x:\u27e8x,e\u27e9=\u2212\u03b3} is very concentrated\naround its expectation.\nIn particular, if (x, y) are sampled according to D, then w.p.\n> 0.5 \u00b7 (1 \u2212\u03b8) \u00b7 (1 \u2212\u03bb) = \u2126(1), it holds that yg(x) < 0. Thus, ErrD,0\u22121(g) = \u2126(1), while\nErr\u03b3(D) \u2264\u03bb = O (\u03b3 poly(log(m)))\nTo conclude the proof of Theorem 3.2, we note that we can assume that g is O(e)-\ninvariant. Otherwise, we can replace it with Pef + b. This does not increase ||f||Hk nor\nErrD,l(f + b), thus, the solution Pef + b is optimal as well. Now, it follows that g|{x:\u27e8x,e\u27e9=\u2212\u03b3}\nis constant and we \ufb01nish as before.\n2\n5.5.2\nTheorem 3.1\nLet L be the Lipschitz constant of l. Let \u03b2 > \u03b1 > 0 such that l(\u03b1) > l(\u03b2). Choose 0 < \u03b8 < 1\nlarge enough so that (1\u2212\u03b8)l(\u2212\u03b2)+\u03b8l(\u03b2) < \u03b8l(\u03b1). First, de\ufb01ne probability measures \u00b51, \u00b52, \u00b53\nand \u00b5 over [\u22121, 1] \u00d7 {\u00b11} as follows.\n\u00b51(\u03b3, 1) = \u03b8, \u00b51(\u2212\u03b3, \u22121) = 1 \u2212\u03b8\n\u00b52(\u2212\u03b3, 1) = 1\nThe measure \u00b53 is the product of uniform{\u00b11} and the measure over [\u22121, 1] whose density\nfunction is\nw(x) =\n(\n0\n|x| > 1\n8\n8\n\u03c0\u221a\n1\u2212(8x)2\n|x| \u22641\n8\nFinally, \u00b5 = (1 \u2212\u03bb1 \u2212\u03bb2)\u00b51 + \u03bb2\u00b52 + \u03bb3\u00b53 with \u03bb2, \u03bb3 > 0 to be chosen later.\nBy lemma 5.15 (see remark 5.16), there is a continuous normalized kernel k\u2032 such that\nw.p. \u22651 \u22122e exp(\u22121\n\u03b3) the function returned by the algorithm is of the form f + b with\n\u2225f\u2225Hk\u2032 \u2264c \u00b7 m3\nA(\u03b3) for some c > 0 (depending only on l). Now, let e \u2208Sd\u22121 be the vector\nfrom Lemma 5.22, corresponding to the kernel k\u2032. The distribution D is the pullback of \u00b5\nw.r.t. e. By considering the a\ufb03ne functional \u039be,0, it holds that Err\u03b3(D) \u2264\u03bb3 + \u03bb2.\nLet g be the solution returned by the algorithm. With probability \u22651 \u2212exp(\u22121/\u03b3),\ng = f + b, where f, b is a solution to program (11) with C = CA(\u03b3) and with an additive\nerror \u2264\u221a\u03b3. As in the proof of Theorem 2.6, it holds that, w.p. \u22651 \u221210 exp(\u22121\n\u03b3) for\nm = mA(\u03b3),\n\f\f\f\f\nZ\n{x:\u27e8x,e\u27e9=\u03b3}\ng \u2212\nZ\n{x:\u27e8x,e\u27e9=\u2212\u03b3}\ng\n\f\f\f\f \u2264128l(0)\u03b3K3.5\n|\u2202+l(0)|\u03bb3\n+ 10 \u00b7 c \u00b7 K3.5 \u00b7 E \u00b7 m3 \u00b7 (rK + sd)\n(13)\nDenote the last bound by \u03f5. It holds that\nErrD,l(g) = (1 \u2212\u03bb2 \u2212\u03bb3)E\u00b51el(yg(x)) + \u03bb2E\u00b52el(yg(x)) + \u03bb3E\u00b53el(yg(x))\n(14)\n35\nNow, denote \u03b4 =\nR\n{x:\u27e8x,e\u27e9=\u2212\u03b3} g. It holds that\nE\u00b51el(yg(x))\n=\n\u03b8\nZ\n{x:\u27e8x,e\u27e9=\u03b3}\nl(g(x)) + (1 \u2212\u03b8)\nZ\n{x:\u27e8x,e\u27e9=\u2212\u03b3}\nl(\u2212g(x))\n\u2265\n\u03b8 \u00b7 l\n\u0012Z\n{x:\u27e8x,e\u27e9=\u03b3}\ng\n\u0013\n+ (1 \u2212\u03b8) \u00b7 l\n\u0012\n\u2212\nZ\n{x:\u27e8x,e\u27e9=\u2212\u03b3}\ng\n\u0013\n(15)\n\u2265\n\u03b8 \u00b7 l(\u03b4) + (1 \u2212\u03b8) \u00b7 l(\u2212\u03b4) \u2212L\u03f5\nThus,\nErrD,l(g)\n\u2265\n(1 \u2212\u03bb2 \u2212\u03bb3)(\u03b8 \u00b7 l(\u03b4) + (1 \u2212\u03b8) \u00b7 l(\u2212\u03b4)) \u2212L\u03f5 + \u03bb2E\u00b52el(yg(x))\nHowever, by considering the constant solution \u03b4, it follows that\nErrD,l(g)\n\u2264\n(1 \u2212\u03bb2 \u2212\u03bb3)(\u03b8l(\u03b4) + (1 \u2212\u03b8) \u00b7 l(\u2212\u03b4)) + \u03bb2 \u00b7 l(\u03b4) + \u03bb3\n1\n2 (l(\u03b4) + l(\u2212\u03b4)) + \u221a\u03b3\n\u2264\n(1 \u2212\u03bb2 \u2212\u03bb3)(\u03b8 \u00b7 l(\u03b4) + (1 \u2212\u03b8) \u00b7 l(\u2212\u03b4)) + \u03bb2 \u00b7 l(\u03b4) + \u03bb3 \u00b7 l(\u2212|\u03b4|) + \u221a\u03b3\nThus,\nErr\u00b52e,l(g)\n\u2264\nL\u03f5\n\u03bb2\n+ l(\u03b4) + \u03bb3\n\u03bb2\nl(\u2212|\u03b4|) +\n\u221a\u03b3\n\u03bb2\n(16)\n=\nL \u00b7 l(0)128\u03b3K3.5\n|\u2202+l(0)|\u03bb2\u03bb3\n+ 10 \u00b7 c \u00b7 L \u00b7 K3.5\n\u03bb2\n\u00b7 E \u00b7 m3 \u00b7 (rK + sd) + l(\u03b4) + \u03bb3\n\u03bb2\nl(\u2212|\u03b4|) +\n\u221a\u03b3\n\u03bb2\nNow, relying on the assumption that \u03b3 \u00b7 log8(m) = o(1), it is possible to choose \u03bb2 =\n\u0398\n\u0000\u221a\u03b3K4\u0001\n= \u0398\n\u0000\u221a\u03b3 log4(m)\n\u0001\n, \u03bb3 = \u221a\u03b3, K = \u0398(log(m/\u03b3)), and d = \u0398(log(m/\u03b3)) such that\nthe bound in Equation (13), L\u00b7l(0)128\u03b3K3.5\n|\u2202+l(0)|\u03bb2\u03bb3 + 10\u00b7c\u00b7K3.5\n\u03bb2\n\u00b7 E \u00b7 m3 \u00b7 (rK + sd), \u03bb2, \u03bb3 and \u03bb3\n\u03bb2 are all\no(1).\nSince the bound in Equation (13) is o(1), it follows, as in the proof of Theorem 2.6, that\nl(\u03b4) \u2264l\n\u0000 \u03b1\n2\n\u0001\nand consequently, 0 < \u03b1\n2 \u2264\u03b4. From equations (14) and (15), it follows that\nl(\u2212|\u03b4|) = l(\u2212\u03b4) \u2264\nL\u03f5 + ErrD,l(g)\n1\u2212\u03bb2\u2212\u03bb3\n1 \u2212\u03b8\n\u2264\nL\u03f5 +\n2l(0)\n1\u2212\u03bb2\u2212\u03bb3\n1 \u2212\u03b8\n= O(1)\nIt now follows from Equation (16) that\nE(x,y)\u223c\u00b52l(g(x)y) = Err\u00b52e,l(g) \u2264l\n\u0010\u03b1\n2\n\u0011\n+ o(1)\nBy Markov\u2019s inequality,\nPr\n(x,y)\u223c\u00b52 (l(g(x)y) \u2265l(0)) \u2264l\n\u0000 \u03b1\n2\n\u0001\n+ o(1)\nl(0)\nThus, if (x, y) are chosen according to \u00b52\ne, then w.p. >\nl(0)\u2212l( \u03b1\n2)\nl(0)\n\u2212o(1), l(g(x)) < l(0) \u21d2\ng(x) > 0. Since the marginal distributions of \u00b51\ne and \u00b52\ne are the same, it follows that, if (x, y)\nare chosen according to D, then w.p. >\n\u0012\nl(0)\u2212l( \u03b1\n2)\nl(0)\n\u2212o(1)\n\u0013\n\u00b7 (1 \u2212\u03bb2 \u2212\u03bb3) \u00b7 (1 \u2212\u03b8) = \u2126(1),\nyg(x) < 0. Thus, ErrD,0\u22121(g) = \u2126(1) while Err\u03b3(D) \u2264\u03bb2 + \u03bb3 = O\n\u0000\u221a\u03b3 poly(log(m))\n\u0001\n.\n2\n36\n5.5.3\nThe integrality gap \u2013 Theorem 3.3\nOur \ufb01rst step is a reduction to the hinge loss. Let a = \u2202+l(0). De\ufb01ne\nl\u2217(x) =\n(\nax + 1\nx \u2264\n1\n\u2212a\n0\no/w\nit is not hard to see that l\u2217is a convex surrogate satisfying \u2200x, l\u2217(x) \u2264l(x) and \u2202+l\u2217(0) =\n\u2202+l(0). Thus, if we substitute l with l\u2217, we just decrease the integrality gap, hence can\nassume that l = l\u2217. Now, we note that if we consider program (11) with l = l\u2217the inegrality\ngap of coincides with what we get by replacing C with |a| \u00b7 C and l\u2217with the hinge loss.\nTo see that, note that for every f \u2208Hk, b \u2208R, ErrD,l\u2217(f + b) = ErrD,hinge(|a| \u00b7 f + |a| \u00b7 b),\nthus, minimizing ErrD,l\u2217over all functions f \u2208Hk that satisfy ||f||Hk \u2264C is equivalent to\nminimizing ErrD,hinge over all functions f \u2208Hk that satisfy ||f||Hk \u2264|a| \u00b7 C. Thus, it is\nenough to prove the Theorem for l = lhinge.\nNext, we show that we can assume that the embedding is symmetric (i.e., correspond to\na symmetric kernel). As the integrality gap is at least as large as the approximation ratio,\nusing Theorem 3.2 this will complete our argument. (The reduction to the hinge loss yields\nbounds with universal constants in the asymptotic terms).\nLet \u03b3 > 0 and let D be a distribution on Sd\u22121 \u00d7 {\u00b11}. It is enough to \ufb01nd (a possibly\ndi\ufb00erent) distribution D1 with the same \u03b3-margin error as D, for which the optimum of\nprogram (11) (with l = lhinge) is not smaller than the optimum of the program\nmin\nErrD,hinge (f + b)\ns.t.\nf \u2208Hks, b \u2208R\n(17)\n||f||Hks \u2264C\nDenote the optimal value of program (17) by \u03b1 and assume, towards contradiction, that\nwhenever Err\u03b3(D1) = Err\u03b3(D), the optimum of program (11) is strictly less then \u03b1.\nFor every A \u2208O(d), let DA, be the distribution of the r.v. (Ax, y) \u2208Sd\u22121 \u00d7 {\u00b11},\nwhere (x, y) \u223cD. Since clearly Err\u03b3(DA) = Err\u03b3(D), there exist fA \u2208Hk and bA \u2208R\nsuch that ||fA||Hk \u2264C and ErrDA,hinge(gA) < \u03b1, where gA := fA + bA. De\ufb01ne f \u2208Hks by\nf(x) =\nR\nO(d) fA(Ax)dA and let b =\nR\nO(d) bAdA and g = f +b. By Theorem 5.18, ||f||Hks \u2264C.\nFinally, for l = lhinge,\nErrD,hinge(g)\n=\nE(x,y)\u223cDl(yg(x))\n=\nE(x,y)\u223cDl(yEA\u223cO(d)gA(Ax))\n\u2264\nE(x,y)\u223cDEA\u223cO(d)l(ygA(Ax))\n=\nEA\u223cO(d)E(x,y)\u223cDl(ygA(Ax))\n=\nEA\u223cO(d)E(x,y)\u223cDAl(ygA(x)) < \u03b1\nContrary to the assumption that \u03b1 is the optimum of program (17).\n5.5.4\nFinite dimension - Theorems 2.7 and 3.4\nLet V \u2286C(Sd\u22121) be the linear space {\u039bv,b \u25e6\u03c8 : v \u2208Rm, b \u2208R} and denote \u00afW = {\u039bv,b \u25e6\u03c8 :\nv \u2208W, b \u2208R}. We note that dim(V ) \u2264m + 1. Instead of program (4) we consider the\n37\nequivalent formulation\nmin\nErrD,l (f)\ns.t.\nf \u2208\u00afW\n(18)\nThe following lemma is very similar to lemma 5.12, but with better dependency on m (m1.5\ninstead of m2).\nLemma 5.25 Let l be a convex surrogate and let V \u2282C(Sd\u22121) an m-dimensional vector\nspace. There exists a continuous kernel k : Sd\u22121 \u00d7 Sd\u22121 \u2192R with supx\u2208Sd\u22121 k(x, x) \u22641 such\nthat Hk = V as a vector space and there exists a probability measure \u00b5N such that\n\u2200f \u2208V, ||f||Hk \u2264\n2m1.5\n|\u2202+l(0)| Err\u00b5N,l(f)\nProof Let \u03c8 : Sd\u22121 \u2192V \u2217be the evaluation operator. It maps each x \u2208Sd\u22121 to the linear\nfunctional f \u2208V 7\u2192f(x). We claim that\n1. \u03c8 is continuous,\n2. a\ufb00(\u03c8(Sd\u22121) \u222a\u2212\u03c8(Sd\u22121)) = V \u2217,\n3. V = {v\u2217\u2217\u25e6\u03c8 : v\u2217\u2217\u2208V \u2217\u2217}.\nProof of 1: We need to show that \u03c8(xn) \u2192\u03c8(x) if xn \u2192x. Since V \u2217is \ufb01nite dimensional, it\nsu\ufb03ces to show that \u03c8(xn)(f) \u2192\u03c8(x)(f) for every f \u2208V , which follows from the continuity\nof f.\nProof of 2: Note that 0 \u2208U = a\ufb00(\u03c8(Sd\u22121) \u222a\u2212\u03c8(Sd\u22121)), so U is a linear space. Now, de\ufb01ne\nT : U \u2217\u2192V via T(u\u2217) = u\u2217\u25e6\u03c8. We claim that T is onto, whence dim(U) = dim(U \u2217) =\ndim(V ) = dim(V \u2217), so that U = V \u2217. Indeed, for f \u2208V , let u\u2217\nf \u2208U \u2217be the functional\nu\u2217\nf(u) = u(f). Now, T(u\u2217\nf)(x) = u\u2217\nf(\u03c8(x)) = \u03c8(x)(f) = f(x), thus T(u\u2217\nf) = f.\nProof of 3: From U = V \u2217it follows that U \u2217= V \u2217\u2217, so that the mapping T : V \u2217\u2217\u2192V is\nonto, showing that V = {v\u2217\u2217\u25e6\u03c8 : v\u2217\u2217\u2208V \u2217\u2217}.\nLet us apply John\u2019s Lemma to K = conv(\u03c8(Sd\u22121)\u222a\u2212\u03c8(Sd\u22121)). It yields an inner product\non V \u2217with K contained in the unit ball and containing the ball around 0 with radius\n1\n\u221am.\nLet k be the kernel k(x, y) = \u27e8\u03c8(x), \u03c8(y)\u27e9. Since \u03c8 is continuous, k is continuous as well.\nBy Theorem 5.1.1 and since T is onto, it follows that, as a vector space, V = Hk. Since K\nis contained in the unit ball, it follows that supx\u2208Sd\u22121 k(x, x) \u22641. It remains to prove the\nexistence of the measure \u00b5N.\nLet e1, . . . , em\n\u2208\nV \u2217be an orthonormal basis.\nFor every i\n\u2208\n[m],\nchoose\n(x1\ni , yi), . . . , (xm+1\ni\n, yi) \u2208Sd\u22121 \u00d7 {\u00b11} and \u03bb1\ni , . . . , \u03bbm+1\ni\n\u22650 such that Pm+1\nj=1 \u03bbj\ni = 1 and\n1\n\u221amei = Pm+1\nj=1 \u03bbj\niyi\u03c8(xj\ni). De\ufb01ne \u00b5N(xj\ni, 1) = \u00b5N(xj\ni, \u22121) = \u03bbj\ni\n2m.\nLet f \u2208V . By Theorem 5.1.1 there exists v \u2208V \u2217such that f = \u039bv,0 \u25e6\u03c8 and ||f||Hk =\n||v||V \u2217. It follows that, for a = \u2202+l(0),\n38\nErr\u00b5N,l(f)\n=\nm\nX\ni=1\nm+1\nX\nj=1\n\u03bbj\ni\n2m\n\u0002\nl(yif(xj\ni)) + l(\u2212yif(xj\ni))\n\u0003\n\u2265\n1\n2m\nm\nX\ni=1\n\"\nl\n m+1\nX\nj=1\n\u03bbj\niyif(xj\ni)\n!\n+ l\n \n\u2212\nm+1\nX\nj=1\n\u03bbj\niyif(xj\ni)\n!#\n=\n1\n2m\nm\nX\ni=1\n\"\nl\n m+1\nX\nj=1\n\u03bbj\niyi\u27e8v, \u03c8(xj\ni)\u27e9\n!\n+ l\n \n\u2212\nm+1\nX\nj=1\n\u03bbj\niyi\u27e8v, \u03c8(xj\ni)\u27e9\n!#\n=\n1\n2m\nm\nX\ni=1\nl\n \n\u27e8v,\nm+1\nX\nj=1\n\u03bbj\niyi\u03c8(xj\ni)\u27e9\n!\n+ l\n \n\u2212\u27e8v,\nm+1\nX\nj=1\n\u03bbj\niyi\u03c8(xj\ni)\u27e9\n!\n=\n1\n2m\nm\nX\ni=1\nl\n\u0012\n\u27e8v, ei\n\u221am\u27e9\n\u0013\n+ l\n\u0012\n\u2212\u27e8v, ei\n\u221am\u27e9\n\u0013\n\u2265\n1\n2m\nm\nX\ni=1\nl\n\u0012\n\u2212|\u27e8v, ei\u27e9|\n\u221am\n\u0013\n\u2265\n|a|\n2m1.5\nm\nX\ni=1\n|\u27e8v, ei\u27e9|\n\u2265\n|a|\n2m1.5||v||V \u2217=\n|a|\n2m1.5||f||Hk\n2\nProof (of Theorem 2.7) Let L be the Lipschitz constant of l. Let \u03b2 > \u03b1 > 0 such that\nl(\u03b1) > l(\u03b2). Choose 0 < \u03b8 < 1 large enough so that (1 \u2212\u03b8)l(\u2212\u03b2) + \u03b8l(\u03b2) < \u03b8l(\u03b1). First,\nde\ufb01ne probability measures \u00b51, \u00b52, \u00b53 and \u00b5 over [\u22121, 1] \u00d7 {\u00b11} as follows.\n\u00b51(\u03b3, 1) = \u03b8, \u00b51(\u2212\u03b3, \u22121) = 1 \u2212\u03b8\n\u00b52(\u2212\u03b3, 1) = 1\nThe measure \u00b53 is the product of uniform{\u00b11} and the measure over [\u22121, 1] whose density\nfunction is\nw(x) =\n(\n0\n|x| > 1\n8\n8\n\u03c0\u221a\n1\u2212(8x)2\n|x| \u22641\n8\nLet k, \u00b5N be the distribution and kernel from Lemma 5.25. Now, let e \u2208Sd\u22121 be the vector\nfrom Lemma 5.22. We de\ufb01ne the distribution D corresponding to the measure\n\u00b5 = (1 \u2212\u03bb2 \u2212\u03bb3 \u2212\u03bbN)\u00b51\ne + \u03bb2\u00b52\ne + \u03bb3\u00b53\ne + \u03bbN\u00b5N\nBy considering the a\ufb03ne functional \u039be,0, it holds that Err\u03b3(D) \u2264\u03bb3 + \u03bb2 + \u03bbN.\nLet g be the solution returned by the algorithm. With probability \u22651 \u2212exp(\u22121/\u03b3),\ng = f + b, where f, b is a solution to program (18) with an additive error \u2264\u221a\u03b3.\n39\nDenote ||g||Hk = C. By Lemma 5.25, it holds that\nC\n\u2264\n2m1.5\n|\u2202+l(0)| Err\u00b5N,l(g)\n\u2264\n2m1.5\n|\u2202+l(0)|\nErr\u00b5,l(g)\n\u03bbN\n\u2264\n2m1.5\n|\u2202+l(0)|\nl(0)\n\u03bbN\nAs in the proof of Theorem 2.6, it holds that\n\f\f\f\f\nZ\n{x:\u27e8x,e\u27e9=\u03b3}\ng \u2212\nZ\n{x:\u27e8x,e\u27e9=\u2212\u03b3}\ng\n\f\f\f\f \u2264128l(0)\u03b3K3.5\n|\u2202+l(0)|\u03bb3\n+ 10 \u00b7 K3.5 \u00b7 E \u00b7 C \u00b7 (rK + sd)\n(19)\nDenote the last bound by \u03f5. It holds that\nErrD,l(g) = (1 \u2212\u03bb2 \u2212\u03bb3 \u2212\u03bbN)E\u00b51el(yg(x)) + \u03bb2E\u00b52el(yg(x)) + \u03bb3E\u00b53el(yg(x)) + \u03bbNE\u00b5Nl(yg(x))\n(20)\nNow, denote \u03b4 =\nR\n{x:\u27e8x,e\u27e9=\u2212\u03b3} g. It holds that\nE\u00b51el(yg(x))\n=\n\u03b8\nZ\n{x:\u27e8x,e\u27e9=\u03b3}\nl(g(x)) + (1 \u2212\u03b8)\nZ\n{x:\u27e8x,e\u27e9=\u2212\u03b3}\nl(\u2212g(x))\n\u2265\n\u03b8 \u00b7 l\n\u0012Z\n{x:\u27e8x,e\u27e9=\u03b3}\ng\n\u0013\n+ (1 \u2212\u03b8) \u00b7 l\n\u0012\n\u2212\nZ\n{x:\u27e8x,e\u27e9=\u2212\u03b3}\ng\n\u0013\n(21)\n\u2265\n\u03b8 \u00b7 l(\u03b4) + (1 \u2212\u03b8) \u00b7 l(\u2212\u03b4) \u2212L\u03f5\nThus,\nErrD,l(g)\n\u2265\n(1 \u2212\u03bb2 \u2212\u03bb3 \u2212\u03bbN)(\u03b8 \u00b7 l(\u03b4) + (1 \u2212\u03b8) \u00b7 l(\u2212\u03b4)) \u2212L\u03f5 + \u03bb2E\u00b52el(yg(x))\nHowever, by considering the constant solution \u03b4, it follows that\nErrD,l(g)\n\u2264\n(1 \u2212\u03bb2 \u2212\u03bb3 \u2212\u03bbN)(a \u00b7 l(\u03b4) + (1 \u2212\u03b8) \u00b7 l(\u2212\u03b4)) + \u03bb2 \u00b7 l(\u03b4) + (\u03bb3 + \u03bbN)1\n2 (l(\u03b4) + l(\u2212\u03b4)) + \u221a\u03b3\n\u2264\n(1 \u2212\u03bb2 \u2212\u03bb3 \u2212\u03bbN)(\u03b8 \u00b7 l(\u03b4) + (1 \u2212\u03b8) \u00b7 l(\u2212\u03b4)) + \u03bb2 \u00b7 l(\u03b4) + (\u03bb3 + \u03bbN) \u00b7 l(\u2212|\u03b4|) + \u221a\u03b3\nThus,\nErr\u00b52e,l(g)\n\u2264\nL\u03f5\n\u03bb2\n+ l(\u03b4) + \u03bb3 + \u03bbN\n\u03bb2\nl(\u2212|\u03b4|) +\n\u221a\u03b3\n\u03bb2\n(22)\n\u2264\nL \u00b7 l(0)128\u03b3K3.5\n|\u2202+l(0)|\u03bb2\u03bb3\n+ 10 \u00b7 L \u00b7 K3.5\n\u03bb2\n\u00b7 E \u00b7 C \u00b7 (rK + sd) + l(\u03b4) + \u03bb3 + \u03bbN + \u221a\u03b3\n\u03bb2\nl(\u2212|\u03b4|)\nNow, relying on the assumption that \u03b3 \u00b7 log8(C) = o(1), it is possible to choose \u03bb2 =\n\u0398\n\u0000\u221a\u03b3K4\u0001\n= \u0398\n\u0000\u221a\u03b3 log4(C)\n\u0001\n, \u03bb3 = \u221a\u03b3, K = \u0398(log(C/\u03b3)), \u03bbN = \u03b3 and d = \u0398(log(C/\u03b3))\nsuch that the bound in Equation (19), L\u00b7l(0)128\u03b3K3.5\n|\u2202+l(0)|\u03bb2\u03bb3 + 10K3.5\n\u03bb2\n\u00b7 E \u00b7 C \u00b7 (rK + sd), \u03bb2, \u03bb3, \u03bbN and\n\u03bb3+\u03bbN+\u221a\u03b3\n\u03bb2\nare all o(1).\n40\nSince the bound in Equation (19) is o(1), it follows, as in the proof of Theorem 2.6, that\nl(\u03b4) \u2264l\n\u0000 \u03b1\n2\n\u0001\nand consequently, 0 < \u03b1\n2 \u2264\u03b4. From equations (20) and (21), it follows that\nl(\u2212|\u03b4|) = l(\u2212\u03b4) \u2264\nL\u03f5 +\nErrD,l(g)\n1\u2212\u03bb2\u2212\u03bb3\u2212\u03bbN\n1 \u2212\u03b8\n\u2264\nL\u03f5 +\n2l(0)\n1\u2212\u03bb2\u2212\u03bb3\u2212\u03bbN\n1 \u2212\u03b8\n= O(1)\nIt now follows from Equation (22) that\nE(x,y)\u223c\u00b52l(g(x)y) = Err\u00b52e,l(g) \u2264l\n\u0010\u03b1\n2\n\u0011\n+ o(1)\nBy Markov\u2019s inequality,\nPr\n(x,y)\u223c\u00b52 (l(g(x)y) \u2265l(0)) \u2264l\n\u0000 \u03b1\n2\n\u0001\n+ o(1)\nl(0)\nThus, if (x, y) are chosen according to \u00b52\ne, then w.p. >\nl(0)\u2212l( \u03b1\n2)\nl(0)\n\u2212o(1), l(g(x)) < l(0) \u21d2\ng(x) > 0. Since the marginal distributions of \u00b51\ne and \u00b52\ne are the same, it follows that, if (x, y)\nare chosen according to D, then w.p. >\n\u0012\nl(0)\u2212l( \u03b1\n2)\nl(0)\n\u2212o(1)\n\u0013\n\u00b7(1\u2212\u03bb2\u2212\u03bb3\u2212\u03bbN)\u00b7(1\u2212\u03b8) = \u2126(1),\nyg(x) < 0. Thus, ErrD,0\u22121(g) = \u2126(1) while Err\u03b3(D) \u2264\u03bb2+\u03bb3+\u03bbN = O\n\u0000\u221a\u03b3 poly(log(C))\n\u0001\n=\nO\n\u0000\u221a\u03b3 poly(log(m/\u03b3))\n\u0001\n.\n2\nProof\n(of Theorem 3.4) As in the proof of Theorem 3.3, we can assume w.l.o.g.\nthat\nl = lhinge. Let k, \u00b5N be the measure and the kernel from Lemma 5.25. Let C = 2m1.5/\u03b3.\nBy (the proof of) Theorem 3.3, there exists a probability measure \u00af\u00b5 over Sd\u22121 \u00d7 {\u00b11} such\nthat for every f \u2208Hk with ||f||Hk \u2264C it holds that Err\u00af\u00b5,l(f) = \u2126(1) but Err\u03b3(\u00af\u00b5) =\nO(\u03b3 \u00b7 poly(log(C))).\nConsider the distribution \u00b5 = (1 \u2212\u03b3)\u00af\u00b5 + \u03b3\u00b5N.\nIt still holds that\nErr\u03b3(\u00af\u00b5) = O(\u03b3 \u00b7 poly(log(C))) = O(\u03b3 \u00b7 poly(log(m/\u03b3))). Let f be an optimal for program\n(18). We have that 1 \u2265Err\u00b5,l(f) \u2265\u03b3 \u00b7 Err\u00b5N,l(f). By Lemma 5.25, ||f||Hk \u2264C. Thus,\nErr\u00b5,l(f) \u2265(1 \u2212\u03b3) Err\u00af\u00b5,l(f) = \u2126(1).\n2\n6\nChoosing a surrogate according to the margin\nThe purpose of this section is to demonstrate the subtleties relating to the possibility of\nchoosing a convex surrogate l according to the margin \u03b3. Let k : B \u00d7 B \u2192R be the kernel\nk(x, y) =\n1\n1 \u22121\n2\u27e8x, y\u27e9H\nand let \u03c8 : B \u2192H1 be a corresponding embedding (i.e., k(x, y) = \u27e8\u03c8(x), \u03c8(y)\u27e9H1). In\n(Shalev-Shwartz et al., 2011) it has been shown that the solution f, b to Program (2), with\nC = C(\u03b3) = poly(exp(1/\u03b3 \u00b7 log(1/\u03b3))) and the embedding \u03c8, satis\ufb01es\nErrhinge(f + b) \u2264Err\u03b3(D) + \u03b3 .\n41\nConsequently, every approximated solution to the Program with an additive error of at most\n\u03b3 will have a 0-1 loss bounded by Err\u03b3(D) + 2\u03b3.\nFor every \u03b3, de\ufb01ne a 1-Lipschitz convex surrogate by\nl\u03b3(x) =\n(\n1 \u2212x\nx \u22641/C(\u03b3)\n1 \u22121/C(\u03b3)\nx \u22651/C(\u03b3)\nClaim 1 A function g : B \u2192R is a solutions to Program (5) with l = l\u03b3, C = 1 and the\nembedding \u03c8, if and only if C(\u03b3) \u00b7 g is a solutions to Program (2) with C = C(\u03b3) and the\nembedding \u03c8.\nWe postpone the proof to the end of the section. We note that Program (5) with l = l\u03b3,\nC = 1 and the embedding \u03c8, have a complexity of 1, according to our conventions. Moreover,\nby Claim 1, the optimal solution to it has a 0-1 error of at most Err\u03b3(D) + \u03b3. Thus, if A is\nan algorithm that is only obligated to return an approximated solution to Program (5) with\nl = l\u03b3, C = 1 and the embedding \u03c8, we cannot lower bound its approximation ratio. In\nparticular, our Theorems regarding the approximation ratio are no longer true, as currently\nstated, if the algorithms are allowed to choose the surrogate according to \u03b3. One might be\ntempted to think that by the above construction (i.e. taking \u03c8 as our embedding, choosing\nC = 1 and l = l\u03b3, and approximate the program upon a sample of size poly(1/\u03b3)), we have\nactually gave 1-approximation algorithm. The crux of the matter is that algorithms that\napproximate the program according to a \ufb01nite sample of size poly(1/\u03b3) are only guaranteed\nto \ufb01nd a solution with an additive error of poly(\u03b3). For the loss l\u03b3, such an additive error\nis meaningless: Since for every function f, ErrD,l\u03b3(f) \u22651 \u22121/C(\u03b3), the 0 solution has an\nadditive error of poly(\u03b3). Therefore, we cannot argue that the solution returned by the\nalgorithm will have a small 0-1 error.\nIndeed we anticipate that the algorithm we have\ndescribed will su\ufb00er from serious over-\ufb01tting.\nTo summarize, we note that the lower bounds we have proved, relies on the fact that the\noptimal solutions of the programs we considered are very bad. For the algorithm we sketched\nabove, the optimal solution is very good. However, guaranties on approximated solutions\nobtained from a polynomial sample are meaningless. We conclude that lower bounds for\nsuch algorithms will have to involve over-\ufb01tting arguments, which are out of the scope of the\npaper.\nProof (of claim 1) De\ufb01ne\nl\u2217\n\u03b3(x) =\n(\n1 \u2212C(\u03b3)x\nx \u2264\n1\nC(\u03b3)\n0\nx \u2265\n1\nC(\u03b3)\nSince l\u2217\n\u03b3(x) = C(\u03b3) \u00b7 (l\u03b3(x) \u2212(1 \u2212\n1\nC(\u03b3))), it follows that the solutions to Program (5) with\nl = l\u2217\n\u03b3, C = 1 and \u03c8 coincide with the solutions with l = l\u03b3, C = 1 and \u03c8. Now, we note\nthat, for every function f : B \u2192R,\nErrD,l\u2217\u03b3(f) = ErrD,hinge(C(\u03b3) \u00b7 f)\nThus, w, b minimizes ErrD,l\u2217\u03b3(\u039bw,b \u25e6\u03c8) under the restriction that \u2225w\u2225\u22641 if and only if\nC(\u03b3) \u00b7 w, C(\u03b3) \u00b7 b minimizes ErrD,hinge(\u039bw,b \u25e6\u03c8) under the restriction that \u2225w\u2225\u2264C(\u03b3).\n2\n42\nAcknowledgements:\nAmit Daniely is a recipient of the Google Europe Fellowship in\nLearning Theory, and this research is supported in part by this Google Fellowship. Nati\nLinial is supported by grants from ISF, BSF and I-Core. Shai Shalev-Shwartz is supported\nby the Israeli Science Foundation grant number 590-10.\nReferences\nMartin Anthony and Peter Bartlet.\nNeural Network Learning: Theoretical Foundations.\nCambridge University Press, 1999.\nK. Atkinson and W. Han. Spherical Harmonics and Approximations on the Unit Sphere: An\nIntroduction, volume 2044. Springer, 2012.\nP. L. Bartlett, M. I. Jordan, and J. D. McAuli\ufb00e. Convexity, classi\ufb01cation, and risk bounds.\nJournal of the American Statistical Association, 101:138\u2013156, 2006.\nS. Ben-David, D. Loker, N. Srebro, and K. Sridharan. Minimizing the misclassi\ufb01cation error\nrate using a surrogate convex loss. In ICML, 2012.\nShai Ben-David, Nadav Eiron, and Hans Ulrich Simon. Limitations of learning via embed-\ndings in euclidean half spaces. The Journal of Machine Learning Research, 3:441\u2013461,\n2003.\nA. Birnbaum and S. Shalev-Shwartz. Learning halfspaces with the zero-one loss: Time-\naccuracy tradeo\ufb00s. In NIPS, 2012.\nE. Blais, R. O\u2019Donnell, and K Wimmer. Polynomial regression under arbitrary product\ndistributions. In COLT, 2008.\nN. Cristianini and J. Shawe-Taylor. An Introduction to Support Vector Machines. Cambridge\nUniversity Press, 2000.\nAmit Daniely, Nati Linial, and Shai Shalev-Shwartz. From average case complexity to im-\nproper learning complexity. arXiv preprint arXiv:1311.2272, 2013.\nV. Feldman, P. Gopalan, S. Khot, and A.K. Ponnuswami. New results for learning noisy\nparities and halfspaces. In In Proceedings of the 47th Annual IEEE Symposium on Foun-\ndations of Computer Science, 2006.\nG.B. Folland. A course in abstract harmonic analysis. CRC, 1994.\nV. Guruswami and P. Raghavendra. Hardness of learning halfspaces with noise. In Proceed-\nings of the 47th Foundations of Computer Science (FOCS), 2006.\nA. Kalai, A.R. Klivans, Y. Mansour, and R. Servedio. Agnostically learning halfspaces. In\nProceedings of the 46th Foundations of Computer Science (FOCS), 2005.\nA.R. Klivans and R. Servedio. Learning DNF in time 2 \u02dcO(n1/3). In STOC, pages 258\u2013265.\nACM, 2001.\n43\nKosaku Yosida. Functional Analysis. Springer-Verlag, Heidelberg, 1963.\nEyal Kushilevitz and Yishay Mansour. Learning decision trees using the Fourier spectrum.\nIn STOC, pages 455\u2013464, May 1991.\nNathan Linial, Yishay Mansour, and Noam Nisan. Constant depth circuits, Fourier trans-\nform, and learnability. In FOCS, pages 574\u2013579, October 1989.\nP.M. Long and R.A. Servedio. Learning large-margin halfspaces with more malicious noise.\nIn NIPS, 2011.\nJ. Matousek. Lectures on discrete geometry, volume 212. Springer, 2002.\nV.D. Milman and G. Schechtman. Asymptotic Theory of Finite Dimensional Normed Spaces:\nIsoperimetric Inequalities in Riemannian Manifolds, volume 1200. Springer, 2002.\nF. Rosenblatt. The perceptron: A probabilistic model for information storage and organiza-\ntion in the brain. Psychological Review, 65:386\u2013407, 1958. (Reprinted in Neurocomputing\n(MIT Press, 1988).).\nS. Saitoh. Theory of reproducing kernels and its applications. Longman Scienti\ufb01c & Technical\nEngland, 1988.\nIJ Schoenberg. Positive de\ufb01nite functions on spheres. Duke. Math. J., 1942.\nB. Sch\u00a8olkopf, C. Burges, and A. Smola, editors. Advances in Kernel Methods - Support\nVector Learning. MIT Press, 1998.\nS. Shalev-Shwartz, O. Shamir, and K. Sridharan. Learning kernel-based halfspaces with the\n0-1 loss. SIAM Journal on Computing, 40:1623\u20131646, 2011.\nI. Steinwart and A. Christmann. Support vector machines. Springer, 2008.\nR. Tibshirani. Regression shrinkage and selection via the lasso. J. Royal. Statist. Soc B., 58\n(1):267\u2013288, 1996.\nV. N. Vapnik. Statistical Learning Theory. Wiley, 1998.\nManfred K Warmuth and SVN Vishwanathan. Leaving the span. In Learning Theory, pages\n366\u2013381. Springer, 2005.\nT. Zhang. Statistical behavior and consistency of classi\ufb01cation methods based on convex\nrisk minimization. The Annals of Statistics, 32:56\u201385, 2004.\n44\n",
        "sentence": "",
        "context": "2\n42\nAcknowledgements:\nAmit Daniely is a recipient of the Google Europe Fellowship in\nLearning Theory, and this research is supported in part by this Google Fellowship. Nati\n\u2217Department of Mathematics, Hebrew University, Jerusalem 91904, Israel. amit.daniely@mail.huji.ac.il\n\u2020School\nof\nComputer\nScience\nand\nEngineering,\nHebrew\nUniversity,\nJerusalem\n91904,\nIsrael.\nnati@cs.huji.ac.il\n\u2021School\nof\nComputer\nScience\nand\nEngineering,\n15\nattempts to prove a quantitative optimal result in this vein, and we strongly believe that\nmuch sharper versions can be proved. This interesting direction is largely left as an open\nproblem."
    },
    {
        "title": "A regularity lemma, and low-weight approximators, for low-degree polynomial threshold functions",
        "author": [
            "Ilias Diakonikolas",
            "Rocco A. Servedio",
            "Li-Yang Tan",
            "Andrew Wan"
        ],
        "venue": "In Proceedings of the 2010 IEEE 25th Annual Conference on Computational Complexity,",
        "citeRegEx": "Diakonikolas et al\\.,? \\Q2010\\E",
        "shortCiteRegEx": "Diakonikolas et al\\.",
        "year": 2010,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Schwankung von polynomen zwischen gitterpunkten",
        "author": [
            "H. Ehlich",
            "K. Zeller"
        ],
        "venue": "Mathematische Zeitschrift,",
        "citeRegEx": "Ehlich and Zeller.,? \\Q1964\\E",
        "shortCiteRegEx": "Ehlich and Zeller.",
        "year": 1964,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "New results for learning noisy parities and halfspaces",
        "author": [
            "Vitaly Feldman",
            "Parikshit Gopalan",
            "Subhash Khot",
            "Ponnuswami"
        ],
        "venue": "In Proceedings of the 47th Annual IEEE Symposium on Foundations of Computer Science,",
        "citeRegEx": "Feldman et al\\.,? \\Q2006\\E",
        "shortCiteRegEx": "Feldman et al\\.",
        "year": 2006,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Agnostic learning of disjunctions on symmetric distributions",
        "author": [
            "Vitaly Feldman",
            "Pravesh Kothari"
        ],
        "venue": "CoRR, abs/1405.6791,",
        "citeRegEx": "Feldman and Kothari.,? \\Q2014\\E",
        "shortCiteRegEx": "Feldman and Kothari.",
        "year": 2014,
        "abstract": "We consider the problem of approximating and learning disjunctions (or\nequivalently, conjunctions) on symmetric distributions over $\\{0,1\\}^n$.\nSymmetric distributions are distributions whose PDF is invariant under any\npermutation of the variables. We give a simple proof that for every symmetric\ndistribution $\\mathcal{D}$, there exists a set of $n^{O(\\log{(1/\\epsilon)})}$\nfunctions $\\mathcal{S}$, such that for every disjunction $c$, there is function\n$p$, expressible as a linear combination of functions in $\\mathcal{S}$, such\nthat $p$ $\\epsilon$-approximates $c$ in $\\ell_1$ distance on $\\mathcal{D}$ or\n$\\mathbf{E}_{x \\sim \\mathcal{D}}[ |c(x)-p(x)|] \\leq \\epsilon$. This directly\ngives an agnostic learning algorithm for disjunctions on symmetric\ndistributions that runs in time $n^{O( \\log{(1/\\epsilon)})}$. The best known\nprevious bound is $n^{O(1/\\epsilon^4)}$ and follows from approximation of the\nmore general class of halfspaces (Wimmer, 2010). We also show that there exists\na symmetric distribution $\\mathcal{D}$, such that the minimum degree of a\npolynomial that $1/3$-approximates the disjunction of all $n$ variables is\n$\\ell_1$ distance on $\\mathcal{D}$ is $\\Omega( \\sqrt{n})$. Therefore the\nlearning result above cannot be achieved via $\\ell_1$-regression with a\npolynomial basis used in most other agnostic learning algorithms.\n  Our technique also gives a simple proof that for any product distribution\n$\\mathcal{D}$ and every disjunction $c$, there exists a polynomial $p$ of\ndegree $O(\\log{(1/\\epsilon)})$ such that $p$ $\\epsilon$-approximates $c$ in\n$\\ell_1$ distance on $\\mathcal{D}$. This was first proved by Blais et al.\n(2008) via a more involved argument.",
        "full_text": "arXiv:1405.6791v2  [cs.LG]  25 May 2015\nAgnostic Learning of Disjunctions on Symmetric Distributions\nVitaly Feldman\nIBM Research - Almaden\nPravesh Kothari\u2217\nThe University of Texas at Austin\nJuly 10, 2018\nAbstract\nWe consider the problem of approximating and learning disjunctions (or equivalently, con-\njunctions) on symmetric distributions over {0, 1}n. Symmetric distributions are distributions\nwhose PDF is invariant under any permutation of the variables.\nWe prove that for every\nsymmetric distribution D, there exists a set of nO(log (1/\u01eb)) functions S, such that for every dis-\njunction c, there is function p, expressible as a linear combination of functions in S, such that\np \u01eb-approximates c in \u21131 distance on D or Ex\u223cD[|c(x) \u2212p(x)|] \u2264\u01eb. This implies an agnostic\nlearning algorithm for disjunctions on symmetric distributions that runs in time nO(log (1/\u01eb)).\nThe best known previous bound is nO(1/\u01eb4) and follows from approximation of the more general\nclass of halfspaces [Wimmer, 2010]. We also show that there exists a symmetric distribution D,\nsuch that the minimum degree of a polynomial that 1/3-approximates the disjunction of all n\nvariables in \u21131 distance on D is \u2126(\u221an). Therefore the learning result above cannot be achieved\nvia \u21131-regression with a polynomial basis used in most other agnostic learning algorithms.\nOur technique also gives a simple proof that for any product distribution D and every\ndisjunction c, there exists a polynomial p of degree O(log (1/\u01eb)) such that p \u01eb-approximates c in\n\u21131 distance on D. This was \ufb01rst proved by Blais et al. [2008] via a more involved argument.\n1\nIntroduction\nThe goal of an agnostic learning algorithm for a concept class C is to produce, for any distribution\non examples, a hypothesis h whose error on a random example from the distribution is close to the\nbest possible by a concept from C. This model re\ufb02ects a common empirical approach to learning,\nwhere few or no assumptions are made on the process that generates the examples and a limited\nspace of candidate hypothesis functions is searched in an attempt to \ufb01nd the best approximation\nto the given data.\nAgnostic learning of disjunctions (or, equivalently, conjunctions) is a fundamental question\nin learning theory and a key step in learning algorithms for other concept classes such as DNF\nformulas and decision trees.\nAlgorithms for this problem, such as the Set Covering Machine\n[Marchand and Shawe-Taylor, 2002], are also used in practical applications. There is no known\ne\ufb03cient algorithm for the problem, in fact the fastest algorithm that does not make any distribu-\ntional assumptions runs in 2 \u02dcO(\u221an) time [Kalai et al., 2008]. Polynomial-time learnability is only\nknown when the examples are very close to being consistent with some disjunction [Awasthi et al.,\n2010].\n\u2217Work done while the author was at IBM Research - Almaden.\n1\nWhile the problem appears to be hard, strong hardness results are known only if the hy-\npothesis is restricted to be a disjunction or a linear threshold function [Ben-David et al., 2003,\nBshouty and Burroughs, 2006, Feldman et al., 2009, 2012], or for learning using \u21131-regression [Klivans and Sherstov\n2010]. Weaker, quasi-polynomial lower bounds are known assuming hardness of learning sparse\nparities with noise (see Section 5) and, very recently, hardness of refuting random SAT formulas\n[Daniely and Shalev-Shwartz, 2014]. It is also well-known that distribution-independent agnostic\nlearning of disjunctions implies PAC learning of DNF expressions [Kearns et al., 1994] (similar\nresults for distribution speci\ufb01c-learning are discussed below). Finally, agnostic learning of disjunc-\ntions is known to be closely related to the problem of di\ufb00erentially-private release of answers to\nconjunctive queries [Gupta et al., 2011].\nWe consider this problem with an additional assumption that example points are distributed\naccording to a symmetric or a product distribution.\nSymmetric and product distributions are\ntwo incomparable classes of distributions that generalize the well-studied uniform distribution.\nTheoretical study of learning over symmetric distributions was \ufb01rst done by Wimmer [2010] who\ngave nO(1/\u01eb4) time agnostic learning algorithm for the class of halfspaces.\nAgnostic learning\nof disjunctions over symmetric distributions on {0, 1}n also arises naturally in the well-studied\nproblem of privately releasing answers to all short conjunction queries with low average error\n[Feldman and Kothari, 2014].\n1.1\nOur Results\nWe prove that disjunctions (and conjunctions) are learnable agnostically over any symmetric distri-\nbution in time nO(log(1/\u01eb)). This matches the well-known upper bound for the uniform distribution.\nOur proof is based on \u21131-approximation of any disjunction by a linear combination of functions\nfrom a \ufb01xed set of functions. Such approximation directly gives an agnostic learning algorithm via\n\u21131-regression based approach introduced by Kalai et al. [2008].\nA natural and commonly used set of basis functions is the set of all monomials on {0, 1}n\nof some bounded degree. It is easy to see that on product distributions with constant bias, dis-\njunctions longer than some constant multiple of log(1/\u01eb) are \u01eb-close to the constant function 1.\nTherefore, polynomials of degree O(log(1/\u01eb)) su\ufb03ce for \u21131 (or \u21132) approximation on such distri-\nbutions. This simple argument does not work for general product distributions. However it was\nshown by Blais et al. [2008] that the same degree (up to a constant factor) still su\ufb03ces in this case.\nTheir argument is based on the analysis of noise sensitivity under product distributions and implies\nadditional interesting results.\nInterestingly, it turns out that low-degree polynomials cannot be used to obtain the same result\nfor all symmetric distributions: we show that there exists a symmetric distribution for which\ndisjunctions are no longer \u21131-approximated by low-degree polynomials.\nTheorem 1.1. There exists a symmetric distribution D such that for c = x1 \u2228x2 \u2228\u00b7 \u00b7 \u00b7 \u2228xn, any\npolynomial p that satis\ufb01es Ex\u223cD[|c(x) \u2212p(x)|] \u22641/3 is of degree \u2126(\u221an).\nTo prove this, we consider the standard linear program [see Klivans and Sherstov, 2010] to \ufb01nd\nthe coe\ufb03cients of a degree r polynomial that minimizes pointwise error with the disjunction c. The\nkey idea is to observe that an optimal point for the dual can be used to obtain a distribution on\nwhich the \u21131 error of the best \ufb01tting polynomial p for c is same as the value of minimum pointwise\nerror of any degree r polynomial with respect to c. When c is a symmetric function, one can\nfurther observe that the distribution so obtained is in fact symmetric. Combined with the degree\n2\nlower bound for uniform approximation by polynomials by Klivans and Sherstov [2010], we obtain\nthe result. The details of the proof appear in Section 3.1.\nOur approximation for general symmetric distributions is based on a proof that for the special\ncase of the uniform distribution on Sr (the points from {\u22121, 1}n with Hamming weight r), low-\ndegree polynomials still work, namely, for any disjunction c, there is a polynomial p of degree at\nmost O(log (1/\u01eb)) such that the \u21131 error Ex\u223cSr[|c(x) \u2212p(x)|] \u2264\u01eb.\nTheorem 1.2. For r \u2208{0, . . . , n}, let Sr denote the set of points in {0, 1}n that have exactly r 1\u2019s\nand let Dr denote the uniform distribution on Sr. For every disjunction c and \u01eb > 0, there exists\na polynomial p of degree at most O(log (1/\u01eb)) such that EDr[|c(x) \u2212p(x)|] \u2264\u01eb.\nThis result can be easily converted to a basis for approximating disjunctions over arbitrary\nsymmetric distributions. All we need is to partition the domain {0, 1}n into layers as \u222a0\u2264r\u2264nSr\nand use a (di\ufb00erent) polynomial for each layer. Formally, the basis now contains functions of the\nform IND(r) \u00b7 \u03c7, where IND is the indicator function of being in layer of Hamming weight r and \u03c7\nis a monomial of degree O(log(1/\u01eb)). We note that a related strategy, of constructing a collection\nof functions, one for each layer of the cube was used by Wimmer [2010] to give nO(1/\u01eb4) time\nagnostic learning algorithm for the class of halfspaces on symmetric distributions. However, his\nproof technique is based on an involved use of representation theory of the symmetric group and is\nnot related to ours.\nOur proof technique also gives a simpler proof for the result of Blais et al. [2008] that implies\napproximation of disjunction by low-degree polynomials on all product distributions.\nTheorem 1.3. For any disjunction c and product distribution D on {0, 1}n, there is a polynomial\np of degree O(log (1/\u01eb)) such that Ex\u223cD[|c(x) \u2212p(x)|] \u2264\u01eb.\n1.2\nApplications\nTheorem 1.2 together with a standard application of \u21131 regression [Kalai et al., 2008] yields an\nagnostic learning algorithm for the class of disjunctions running in time nO(log(1/\u01eb)).\nCorollary 1.4. There is an algorithm that agnostically learns the class of disjunctions on arbitrary\nsymmetric distributions on {0, 1}n in time nO(log (1/\u01eb)).\nThis learning algorithm was extended to the class of all coverage functions in [Feldman and Kothari,\n2014], and then applied to the well-studied problem of privately releasing answers to all short con-\njunction queries with low average error.\nIt was shown by Kalai et al. [2009] and Feldman [2010] that agnostic learning of conjunctions\nover a distribution D in time T(n, 1/\u01eb) implies learning of DNF formulas with s terms over D in\ntime poly(n, 1/\u01eb) \u00b7 T(n, (4s/\u01eb)). Further, under the same conditions distribution-speci\ufb01c agnostic\nboosting [Kalai and Kanade, 2009, Feldman, 2010] implies that there exists an agnostic learning\nalgorithm for decision trees with s leaves running in time poly(n, 1/\u01eb) \u00b7 T(n, s/\u01eb). Therefore we\nobtain quasi-polynomial learning algorithms for DNF formulas and decision trees over symmetric\ndistributions.\nCorollary 1.5.\n1. DNF formulas with s terms are PAC learnable with error \u01eb in time nO(log(s/\u01eb))\nover all symmetric distributions;\n3\n2. Decision trees with s leaves are agnostically learnable with excess error \u01eb in time nO(log(s/\u01eb))\nover all symmetric distributions.\nWe also observe that any algorithm that agnostically learns the class of disjunction on the\nuniform distribution in time no(log ( 1\n\u01eb )) would yield a faster algorithm for the notoriously hard\nproblem of Learning Sparse Parities with Noise. This is implicit in prior work [Kalai et al., 2008,\nFeldman, 2012] and we provide additional details in Section 5.\nDachman-Soled et al. [2015] recently showed that \u21131 approximation by polynomials is necessary\nand su\ufb03cient condition for agnostic learning over a product distribution (at least in the statistical\nquery framework of Kearns [1998]).\nOur agnostic learning algorithm (Theorem 1.4) and lower\nbound for polynomial approximation (Theorem 1.1) demonstrate that this equivalence does not\nhold for non-product distributions.\n2\nPreliminaries\nWe use {0, 1}n to denote the n-dimensional Boolean hypercube. Let [n] denote the set {1, 2, . . . , n}.\nFor S \u2286[n], we denote by ORS : {0, 1}n \u2192{0, 1}, the monotone Boolean disjunction on variables\nwith indices in S, that is, for any x \u2208{0, 1}n, ORS(x) = 0 \u21d4\u2200i \u2208S xi = 0.\nOne can de\ufb01ne norms and errors with respect to any distribution D on {0, 1}n.\nThus, for\nf : {0, 1}n \u2192R, we write the \u21131 and \u21132 norms of f as \u2225f\u22251 = Ex\u223cD[|f(x)|] and \u2225f\u22252 =\np\nE[f(x)2]\nrespectively.\nThe \u21131 and \u21132 error of f with respect to g are given by \u2225f \u2212g\u22251 and \u2225f \u2212g\u22252\nrespectively.\n2.1\nAgnostic Learning\nThe agnostic learning model is formally de\ufb01ned as follows [Haussler, 1992, Kearns et al., 1994].\nDe\ufb01nition 2.1. Let F be a class of Boolean functions and let D be any \ufb01xed distribution on\n{0, 1}n. For any distribution P over {0, 1}n \u00d7 {0, 1}, let opt(P, F) be de\ufb01ned as: opt(P, F) =\ninff\u2208F E(x,y)\u223cP[|y \u2212f(x)|]. An algorithm A, is said to agnostically learn F on D if for every excess\nerror \u01eb > 0 and any distribution P on {0, 1}n \u00d7 {0, 1} such that the marginal of P on {0, 1}n is D,\ngiven access to random independent examples drawn from P, with probability at least 2\n3, A outputs\na hypothesis h : {0, 1}n \u2192[0, 1], such that E(x,y)\u223cP[|h(x) \u2212y|] \u2264opt(P, F) + \u01eb.\nIt is easy to see that given a set of t examples {(xi, yi)}i\u2264t and a set of m functions \u03c61, \u03c62, . . . , \u03c6m\n\ufb01nding coe\ufb03cients \u03b11, . . . , \u03b1m which minimize\nX\ni\u2264t\n\f\f\f\f\f\f\nX\nj\u2264m\n\u03b1j\u03c6j(xi) \u2212yi\n\f\f\f\f\f\f\ncan be formulated as a linear program. This LP is referred to as Least-Absolute-Error (LAE) LP or\nLeast-Absolute-Deviation LP, or \u21131 linear regression. As observed by Kalai et al. [2008], \u21131 linear\nregression gives a general technique for agnostic learning of Boolean functions.\nTheorem 2.2. Let C be a class of Boolean functions, D be distribution on {0, 1}n and \u03c61, \u03c62, . . . , \u03c6m :\n{0, 1}n \u2192R be a set of functions that can be evaluated in time polynomial in n. Assume that there\n4\nexists \u2206such that for each f \u2208C, there exist reals \u03b11, \u03b12, . . . , \u03b1m such that\nE\nx\u223cD\n\uf8ee\n\uf8f0\n\f\f\f\f\f\f\nX\ni\u2264m\n\u03b1i\u03c6i(x) \u2212f(x)\n\f\f\f\f\f\f\n\uf8f9\n\uf8fb\u2264\u2206.\nThen there is an algorithm that for every \u01eb > 0 and any distribution P on {0, 1}n \u00d7 {0, 1} such\nthat the marginal of P on {0, 1}n is D, given access to random independent examples drawn from\nP, with probability at least 2/3, outputs a function h such that\nE\n(x,y)\u223cP\n[|h(x) \u2212y|] \u2264\u2206+ \u01eb.\nThe algorithm uses O(m/\u01eb2) examples, runs in time polynomial in n, m, 1/\u01eb and returns a linear\ncombination of \u03c6i\u2019s.\nThe output of this LP is not necessarily a Boolean function but can be converted to a Boolean\nfunction with disagreement error of \u2206+ 2\u01eb using \u201ch(x) \u2265\u03b8\u201d function as a hypothesis for an\nappropriately chosen \u03b8 [Kalai et al., 2008].\n3\n\u21131 Approximation on Symmetric Distributions\nIn this section, we show how to approximate the class of all disjunctions on any symmetric distri-\nbution by a linear combination of a small set of basis functions.\nAs discussed above, polynomials of degree O(log (1/\u01eb)) can \u01eb-approximate any disjunction in\n\u21131 distance on any product distribution.\nThis is equivalent to using low-degree monomials as\nbasis functions. We \ufb01rst show that this basis would not su\ufb03ce for approximating disjunctions on\nsymmetric distributions. Indeed, we construct a symmetric distribution on {0, 1}n, on which, any\npolynomial that approximates the monotone disjunction c = x1 \u2228x2 \u2228. . . \u2228xn within \u21131 error of\n1/3 must be of degree \u2126(\u221an).\n3.1\nLower Bound on \u21131 Approximation by Low-Degree Polynomials\nIn this section we give the proof of Theorem 1.1.\nProof of Theorem 1.1. Let d : [n] \u2192{0, 1} be the predicate corresponding to the disjunction x1 \u2228\nx2 \u2228. . . \u2228xn, that is, d(0) = 0 and d(i) = 1 for each i > 0.\nConsider a natural linear program to \ufb01nd a univariate polynomial f of degree at most d such that\n\u2225d\u2212f\u2225\u221e= max0\u2264i\u2264n |d(i)\u2212f(i)| is minimized. This program (and its dual) often comes up in prov-\ning polynomial degree lower bounds for various function classes [for example, Klivans and Sherstov,\n2010].\nmin \u01eb\ns.t. \u01eb \u2265|d(m) \u2212\nr\nX\ni=0\n\u03b1i \u00b7 mi|\n\u2200m \u2208{0, . . . , n}\n\u03b1i \u2208R\n\u2200i \u2208{0, . . . , r}\n5\nIf {\u03b10, \u03b11, . . . , \u03b1n} is a solution for the program above that has value \u01eb then f(m) = Pr\ni=0 \u03b1imi is\na degree r polynomial that approximates d within an error of at most \u01eb at every point in {0, . . . , n}.\nKlivans and Sherstov [2010] show that there exists an r\u2217= \u0398(\u221an), such that the optimal value of\nthe program above for r = r\u2217is \u01eb\u2217\u22651/3. Standard manipulations [see Klivans and Sherstov, 2010]\ncan be used to produce the dual of the program.\nmax\nn\nX\nm=0\n\u03b2m \u00b7 d(m)\ns.t.\nn\nX\nm=0\n\u03b2m \u00b7 mi = 0\n\u2200i \u2208{0, . . . , r}\nn\nX\nm=0\n|\u03b2m| \u22641\n\u03b2m \u2208R\n\u2200m \u2208{0, . . . , n}\nLet \u03b2\u2217= {\u03b2\u2217\nm}m\u2208{0,...,n} denote an optimal solution for the dual program with r = r\u2217. Then, by\nstrong duality, the value of the dual is also \u01eb\u2217. Observe that Pn\nm=0 |\u03b2\u2217\nm| = 1, since otherwise we can\nscale up all the \u03b2\u2217\nm by the same factor and increase the value of the program while still satisfying\nthe constraints.\nLet \u03c1 : {0, . . . , n} \u2192[0, 1] be de\ufb01ned by \u03c1(m) = |\u03b2\u2217\nm|. Then \u03c1 can be viewed as a density\nfunction of a distribution on {0, . . . , n} and we use it to de\ufb01ne a symmetric distribution D on\n{\u22121, 1}n as follows: D(x) = \u03c1(w(x))/\n\u0000n\nw(x)\n\u0001\n, where w(x) = Pn\ni=1 xi is the Hamming weight of point\nx. We now show that any polynomial p of degree r\u2217satis\ufb01es Ex\u223cD[|c(x) \u2212p(x)|] \u22651/3.\nWe now extract a univariate polynomial fp that approximates d on the distribution with the\ndensity function \u03c1 using p. Let pavg : {\u22121, 1}n \u2192R be obtained by averaging p over every layer.\nThat is, pavg(x) = Ez\u223cDw(x)[p(z)], where w(x) denotes the Hamming weight of x. It is easy to check\nthat since c is symmetric, pavg is at least as close to c as p in \u21131 distance.\nFurther, pavg is a symmetric function computed by a multivariate polynomial of degree at most\nr\u2217on {0, 1}n. Thus, the function fp(m) that gives the value of pavg on points of Hamming weight\nm can be computed by a univariate polynomial of degree r\u2217. Further,\nE\nx\u223cD[|c(x) \u2212p(x)|] \u2265E\nx\u223cD[|c(x) \u2212pavg(x)|] = E\nm\u223c\u03c1[|d(m) \u2212fp(m)|].\nLet us now estimate the error of fp w.r.t d on the distribution \u03c1. Using the fact that fp is of\ndegree at most r\u2217and thus Pn\nm=0 fp(m) \u00b7 \u03b2m = 0 (enforced by the dual constraints), we have:\nE\nm\u223c\u03c1[|d(m) \u2212fp(m)|] \u2265E\nm\u223c\u03c1[(d(m) \u2212fp(m)) \u00b7 sign(\u03b2\u2217\nm)]\n=\nn\nX\nm=0\nd(m) \u00b7 \u03b2\u2217\nm \u2212\nn\nX\nm=0\nfp(m) \u00b7 \u03b2\u2217\nm\n= \u01eb\u2217\u22120 = \u01eb\u2217\u22651/3.\nThus, the degree of any polynomial that approximates c on the distribution D with error of at most\n1/3 is \u2126(\u221an).\n6\n3.2\nUpper Bound\nIn this section, we describe how to approximate disjunctions on any symmetric distribution by\nusing a linear combination of functions from a set of small size. Recall that Sr denotes the set of\nall points from {0, 1}n with weight r.\nAs we have seen above, symmetric distributions can behave very di\ufb00erently when compared to\n(constant bounded) product distributions. However, for the special case of the uniform distribution\non Sr, denoted by Dr, we show that for every disjunction c, there is a polynomial of degree\nO(log (1/\u01eb)) that \u01eb-approximates it in \u21131 distance on Dr. As described in Section 1.1, one can stitch\ntogether polynomial approximations on each Sr to build a set of basis functions S such that every\ndisjunction is well approximated by some linear combination of functions in S. Thus, our goal is\nnow reduced to constructing approximating polynomials on Dr.\nProof of Theorem 1.2. We \ufb01rst assume that c is monotone and without loss of generality c = x1 \u2228\n\u00b7 \u00b7 \u00b7\u2228xk. We will also prove a slightly stronger claim that EDr[|c(x)\u2212p(x)|] \u2264EDr[(c(x)\u2212p(x))2] \u2264\u01eb\nin this case. Let d : {0, . . . , k} \u2192{0, 1} be the predicate associated with the disjunction, that is\nd(i) = 1 whenever i \u22651. Note that c(x) = d\n\u0010P\ni\u2208[k] xi\n\u0011\n. Therefore our goal is to \ufb01nd a univariate\npolynomial f that approximates d and then substitute pf(x) = f\n\u0010P\ni\u2208[k] xi\n\u0011\n. This substitution\npreserves the total degree of the polynomial. We break our construction into several cases based\non the relative magnitudes of r, k and \u01eb.\nIf k \u22642 ln (1/\u01eb), then the univariate polynomial that exactly computes the predicate d satis\ufb01es\nthe requirements. Thus assume that k > 2 ln(1/\u01eb). If r > n\u2212k, then, c always takes the value 1 on\nSr and thus the constant polynomial 1 achieves zero error. If on the other hand, if r \u2265(n/k) ln (1/\u01eb),\nthen,\nPr\nx\u223cDr[c(x) = 0] =\n\u0000n\u2212k\nr\n\u0001\n\u0000n\nr\n\u0001\n=\nr\u22121\nY\ni=0\n\u0012\n1 \u2212\nk\nn \u2212i\n\u0013\n\u2264(1 \u2212k/n)r \u2264e\u2212kr/n \u2264\u01eb.\nIn this case, the constant polynomial 1 achieves an \u21132\n2 error of at most Prx\u223cDr[c(x) = 0] \u00b7 1 \u2264\u01eb.\nFinally, observe that r \u2264(n/k) ln (1/\u01eb) and k > 2 ln(1/\u01eb) implies r \u2264n/2. Thus, for the remaining\npart of the proof, assume that r < min{n \u2212k, (n/k) ln (1/\u01eb), n/2}.\nConsider the univariate polynomial f : {0, . . . , k} \u2192R of degree t (for some t to be chosen\nlater) that computes the predicate d exactly on {0, . . . , t}. This polynomial is given by\nf(w) = 1 \u22121\nt!\ntY\ni=1\n(i \u2212w) =\n\u001a\n1\u2212(w\nt) for w > t\n1 for 0<w\u2264t\n0 for w=0\nLet\n\u03b4j = Pr\nx\u223cDr[|{i | xi = 1}| = j] =\n\u0000n\u2212k\nr\u2212j\n\u0001\n\u00b7\n\u0000k\nj\n\u0001\n\u0000n\nr\n\u0001\n.\nThe \u21132\n2 error of pf(x) on c satis\ufb01es,\n||pf \u2212c||2\n2 =\nE\nx\u223cDr\n[(c(x) \u2212pf(x))2] =\nk\nX\nj=t+1\n\u03b4j \u00b7\n\u0012j\nt\n\u00132\n.\nWe denote the RHS of this equality by \u2225d \u2212f\u22252\n2.\n7\nWe \ufb01rst upper bound \u03b4j as follows:\n\u03b4j =\n\u0000n\u2212k\nr\u2212j\n\u0001\n\u00b7\n\u0000k\nj\n\u0001\n\u0000n\nr\n\u0001\n=\n(n \u2212k)!\n(n \u2212k \u2212r + j)!(r \u2212j)! \u00b7\nk!\n(k \u2212j)!j! \u00b7 (n \u2212r)!r!\nn!\n= 1\nj! \u00b7\nr!\n(r \u2212j)! \u00b7\nk!\n(k \u2212j)! \u00b7 (n \u2212r)!\nn!\n\u00b7\n(n \u2212k)!\n(n \u2212k \u2212r + j)!\n\u22641\nj! \u00b7 (rk)j \u00b7 (n \u2212k) \u00b7 (n \u2212k \u22121) \u00b7 \u00b7 \u00b7 (n \u2212k \u2212r + j + 1)\nn \u00b7 (n \u22121) \u00b7 \u00b7 \u00b7 (n \u2212r + 1)\n\u22641\nj! \u00b7 (n ln (1/\u01eb))j \u00b7\n1\n(n \u2212r + j) \u00b7 (n \u2212r + j \u22121) \u00b7 \u00b7 \u00b7 (n \u2212r + 1),\nwhere, in the second to last inequality, we used that r < n/k ln (1/\u01eb) to conclude that rk \u2264\n(n ln (1/\u01eb)). Now, r < n/2 and thus (n \u2212r + 1) > n/2. Therefore,\n\u03b4j \u22642j \u00b7 (n ln (1/\u01eb))j\nnj \u00b7 j!\n= (2 ln (1/\u01eb))j\nj!\n,\nand thus:\n\u2225d \u2212f\u22252\n2 \u2264\nk\nX\nj=t+1\n\u0012j\nt\n\u00132 (2 ln (1/\u01eb))j\nj!\n.\nSet t = 8e2 ln (1/\u01eb). Using j! > (j/e)j > (t/e)j for every j \u2265t + 1, we obtain:\n\u2225d \u2212f\u22252\n2 \u2264\nk\nX\nj=t+1\n22j \u00b7\n\u0012 2 ln (1/\u01eb)\n8e ln (1/\u01eb)\n\u0013j\n\u2264\u01eb \u00b7\n\u221e\nX\nj=t+1\n1/ej \u2264\u01eb.\n(1)\nTo see that EDr[|c(x) \u2212p(x)|] \u2264EDr[(c(x) \u2212p(x))2] we note that in all cases and for all x,\n|p(x) \u2212c(x)| is either 0 or \u22651. This completes the proof of the monotone case.\nWe next consider the more general case when c = x1 \u2228x2\u2228. . .\u2228xk1 \u2228\u00afxk1+1 \u2228\u00afxk1+2 \u2228. . .\u2228\u00afxk1+k2.\nLet c1 = x1 \u2228x2 \u2228. . . \u2228xk1 and c2 = \u00afxk1+1 \u2228\u00afxk1+2 \u2228. . . \u2228\u00afxk1+k2 and k = k1 + k2. Observe that\nc = 1 \u2212(1 \u2212c1) \u00b7 (1 \u2212c2) = c1 + c2 \u2212c1c2.\nLet p1 be a polynomial of degree O(log (1/\u01eb)) such that \u2225c1 \u2212p1\u22251 \u2264\u2225c1 \u2212p1\u22252\n2 \u2264\u01eb/3. Note that\nif we swap 0 and 1 in {0, 1}n then c2 will be equal to a monotone disjunction \u00afc2 = xk1+1 \u2228xk1+2 \u2228\n. . . \u2228xk1+k2 and Dr will become Dn\u2212r. Therefore by the argument for the monotone case, there\nexists a polynomial \u00afp2 of degree O(log (1/\u01eb)) such that \u2225\u00afc2 \u2212\u00afp2\u22251 \u2264\u01eb/3. By renaming the variables\nback we will obtain a polynomial p2 of degree O(log (1/\u01eb)) such that \u2225c2 \u2212p2\u22251 \u2264\u2225c2 \u2212p2\u22252\n2 \u2264\u01eb/3.\nNow let p = p1 + p2 \u2212p1p2. Clearly the degree of p is O(log (1/\u01eb)). We now show that \u2225c \u2212p\u22251 \u2264\u01eb:\nE\nx\u223cDr\n[|c(x) \u2212p(x)|] =\nE\nx\u223cDr\n[|(1 \u2212c(x)) \u2212(1 \u2212p(x))|]\n=\nE\nx\u223cDr\n[|(1 \u2212c1)(1 \u2212c2) \u2212(1 \u2212p1)(1 \u2212p2)|]\n=\nE\nx\u223cDr\n[|(1 \u2212c1)(p2 \u2212c2) + (1 \u2212c2)(p1 \u2212c1) \u2212(c1 \u2212p1)(c2 \u2212p2)|]\n\u2264\nE\nx\u223cDr\n[|(1 \u2212c1)(p2 \u2212c2)|] +\nE\nx\u223cDr\n[|(1 \u2212c2)(p1 \u2212c1)|] +\nE\nx\u223cDr\n[|(c1 \u2212p1)(c2 \u2212p2)|]\n\u2264\nE\nx\u223cDr\n[|p2 \u2212c2|] +\nE\nx\u223cDr\n[|p1 \u2212c1|] +\nr\nE\nx\u223cDr\n[(c1 \u2212p1)2]\nE\nx\u223cDr\n[(c2 \u2212p2)2]\n\u2264\u01eb/3 + \u01eb/3 + \u01eb/3 = \u01eb.\n8\n4\nPolynomial Approximation on Product Distributions\nIn this section, we show that for every product distribution D = Q\ni\u2208[n] Di, every \u01eb > 0 and every\ndisjunction (or conjunction) c of length k, there exists a polynomial p : {0, 1}n \u2192R of degree\nO(log (1/\u01eb)) such that p \u01eb-approximates c in \u21131 distance on D.\nProof of Theorem 1.3. First, we note that without loss of generality we can assume that the dis-\njunction c is equal to x1 \u2228x2 \u2228\u00b7 \u00b7 \u00b7 \u2228xk for some k \u2208[n]. We can assume monotonicity since we can\nconvert negated variables to un-negated variables by swapping the roles of 0 and 1 for that variable.\nThe obtained distribution will remain product after this operation. Further we can assume that\nk = n since variables with indices i > k do not a\ufb00ect probabilities of variables with indices \u2264k or\nthe value of c(x).\nWe \ufb01rst note that we can assume that Prx\u223cD[x = 0k] > \u01eb since, otherwise, the constant\npolynomial 1 gives the desired approximation. Let \u00b5i = Prxi\u223cDi[xi = 1]. Since c is a symmetric\nfunction, its value at any x \u2208{0, 1}k depends only on the Hamming weight of x that we denote\nby w(x). Thus, we can equivalently work with the univariate predicate d : {0, 1, . . . , k} \u2192{0, 1},\nwhere d(i) = 1 for i > 0 and d(0) = 0.\nAs in the proof of Theorem 1.2, we will approximate d by a univariate polynomial f and then\nuse the polynomial pf(x) = f(w(x)) to approximate c.\nLet f : {0, 1, . . . , k} \u2192R be the univariate polynomial of degree t that matches d on all points\nin {0, 1, . . . , t}. Thus,\nf(w) = 1 \u22121\nt! \u00b7\ntY\ni=1\n(w \u2212i) =\n\u001a\n1\u2212(w\nt) for w > t\n1 for 0<w\u2264t\n0 for w=0\nWe have,\nE\nx\u223cDr\n[(c(x) \u2212pf(x))2] =\nk\nX\nj=0\nPr\nx\u223cD[w(x) = j] \u00b7 |d(j) \u2212f(j)|\nand we denote the RHS of this equation by \u2225d \u2212f\u22251.\nThen:\n\u2225d \u2212f\u22251 =\nk\nX\nj=t+1\nPr\nD [w(x) = j] \u00b7 |1 \u2212f(j)|\n=\nk\nX\nj=t+1\nPr\nD [w(x) = j] \u00b7\n\u0012j\nt\n\u0013\n.\n(2)\nLet us now estimate PrD[w(x) = j].\nPr\nD [w(x) = j] =\nX\nS\u2286[n], |S|=j\nY\ni\u2208S\n\u00b5i \u00b7\nY\ni/\u2208S\n(1 \u2212\u00b5i)\n\u2264\nX\nS\u2286[n], |S|=j\nY\ni\u2208S\n\u00b5i\n9\nObserve that in the expansion of (Pk\ni=1 \u00b5i)j, the term Q\ni\u2208S \u00b5i occurs exactly j! times. Thus,\nX\nS\u2286[n], |S|=j\nY\ni\u2208S\n\u00b5i \u2264(Pk\ni=1 \u00b5i)j\nj!\n.\nSet \u00b5avg = 1\nk\nPk\ni=1 \u00b5i. We have:\n\u01eb \u2264Pr\nx\u223cD[x = 0k] =\nk\nY\ni=1\n(1 \u2212\u00b5i) \u2264\n \n1 \u22121\nk \u00b7\nk\nX\ni=1\n\u00b5i\n!k\n= (1 \u2212\u00b5avg)k.\nThus, \u00b5avg = c/k for some c \u22642 ln (1/\u01eb) whenever k \u2265k0 where k0 is some universal constant.\nIn what follows, assume that k \u2265k0. (Otherwise, we can use the polynomial of degree equal to k\nthat exactly computes the predicate d on all points).\nWe are now ready to upper bound the error \u2225d \u2212f\u22251. From Equation (2), we have:\n\u2225d \u2212f\u22251 =\nk\nX\nj=t+1\nPr\nD [w(x) = j] \u00b7\n\u0012j\nt\n\u0013\n\u2264\nk\nX\nj=t+1\n(Pk\ni=1 \u00b5i)j\nj!\n\u00b7\n\u0012j\nt\n\u0013\n\u2264\nk\nX\nj=t+1\n\u0012j\nt\n\u0013\n\u00b7 (2 ln(1/\u01eb))j\nj!\nSetting t = 4e2 ln (1/\u01eb) and using the calculation from Equation (1) in the proof of Thm. 1.2, we\nobtain that the error \u2225d \u2212f\u22251 \u2264\u01eb.\n5\nAgnostic Learning of Disjunctions\nCombining Thm. 2.2 with the results of the previous section (and the discussion in Section 1.1),\nwe obtain an agnostic learning algorithm for the class of all disjunctions on product and symmetric\ndistributions running in time nO(log (1/\u01eb)).\nCorollary 5.1 (Cor. 1.4, restated). There is an algorithm that agnostically learns the class of\ndisjunctions on any product or symmetric distribution on {0, 1}n with excess error of at most \u01eb in\ntime nO(log (1/\u01eb)).\nWe now remark that any algorithm that agnostically learns the class of disjunctions (or con-\njunctions) on n inputs on the uniform distribution on {0, 1}n in time no(log ( 1\n\u01eb )) would yield a faster\nalgorithm for the notoriously hard problem of Learning Sparse Parities with Noise(SLPN). The\nreduction is based on the technique implicit in the work of Kalai et al. [2008] and Feldman [2012].\nFor S \u2286[n], we use \u03c7S to denote the parity of inputs with indices in S. Let U denote the\nuniform distribution on {0, 1}n. We say that random examples of a Boolean function f have noise\nof rate \u03b7 if the label of a random example equals f(x) with probability 1 \u2212\u03b7 and 1 \u2212f(x) with\nprobability \u03b7.\nProblem 1 (Learning Sparse Parities with Noise). For \u03b7 \u2208(0, 1/2) and k \u2264n the problem of\nlearning k-sparse parities with noise \u03b7 is the problem of \ufb01nding (with probability at least 2/3) the\nset S \u2286[n],|S| \u2264k, given access to random examples with noise of rate \u03b7 of parity function \u03c7S.\n10\nThe fastest known algorithm for learning k-sparse parities with noise \u03b7 is a recent breakthrough\nresult of Valiant [2012] which runs in time O(n0.8kpoly(\n1\n1\u22122\u03b7)) .\nKalai et al. [2008] and Feldman [2012] prove hardness of agnostic learning of majorities and\nconjunctions, respectively, based on correlation of concepts in these classes with parities. We state\nbelow this general relationship between correlation with parities and reduction to SLPN, a simple\nproof of which appears in [Feldman et al., 2013].\nLemma 5.2. Let C be a class of Boolean functions on {0, 1}n. Suppose, there exist \u03b3 > 0 and k \u2208N\nsuch that for every S \u2286[n], |S| \u2264k, there exists a function, fS \u2208C, such that | Ex\u223cU[fS(x)\u03c7S(x)]| \u2265\n\u03b3(k). If there exists an algorithm A that learns the class C agnostically with excess error \u01eb in time\nT(n, 1\n\u01eb) then, there exists an algorithm A\u2032 that learns k-sparse parities with noise \u03b7 < 1/2 in time\npoly(n,\n1\n(1\u22122\u03b7)\u03b3(k)) + 2T(n,\n2\n(1\u22122\u03b7)\u03b3(k)).\nThe correlation between a disjunction and a parity is easy to estimate.\nFact 5.3. For any S \u2286[n], | Ex\u223cU[ORS(x)\u03c7S(x)]| =\n1\n2|S|\u22121.\nWe thus immediately obtain the following corollary.\nTheorem 5.4. Suppose there exists an algorithm that learns the class of Boolean disjunctions over\nthe uniform distribution agnostically with excess error of \u01eb > 0 in time T(n, 1\n\u01eb). Then there exists\nan algorithm that learns k-sparse parities with noise \u03b7 < 1\n2 in time poly(n, 2k\u22121\n1\u22122\u03b7) + 2T(n, 2k\u22121\n1\u22122\u03b7). In\nparticular, if T(n, 1\n\u01eb) = no(log (1/\u01eb)), then, there exists an algorithm to solve k-SLPN in time no(k).\nThus, any algorithm that is asymptotically faster than the one from Cor. 1.4 yields a faster\nalgorithm for k-SLPN.\nReferences\nPranjal Awasthi, Avrim Blum, and Or She\ufb00et. Improved guarantees for agnostic learning of dis-\njunctions. In Proceedings of COLT, pages 359\u2013367, 2010.\nS. Ben-David, N. Eiron, and P. M. Long. On the di\ufb03culty of approximately maximizing agreements.\nJournal of Computer and System Sciences, 66(3):496\u2013514, 2003.\nE. Blais, R. O\u2019Donnell, and K. Wimmer. Polynomial regression under arbitrary product distribu-\ntions. In COLT, pages 193\u2013204, 2008.\nN. Bshouty and L. Burroughs. Maximizing agreements and coagnostic learning. Theoretical Com-\nputer Science, 350(1):24\u201339, 2006.\nDana Dachman-Soled, Vitaly Feldman, Li-Yang Tan, Andrew Wan, and Karl Wimmer. Approxi-\nmate resilience, monotonicity, and the complexity of agnostic learning. In Proceedings of SODA,\npages 498\u2013511, 2015.\nAmit Daniely and Shai Shalev-Shwartz. Complexity theoretic limitations on learning dnf\u2019s. CoRR,\nabs/1404.3378, 2014. URL http://arxiv.org/abs/1404.3378.\nV. Feldman. Distribution-speci\ufb01c agnostic boosting. In Proceedings of Innovations in Computer\nScience, pages 241\u2013250, 2010.\n11\nV. Feldman. A complete characterization of statistical query learning with applications to evolv-\nability. Journal of Computer System Sciences, 78(5):1444\u20131459, 2012.\nV. Feldman, P. Gopalan, S. Khot, and A. Ponuswami. On agnostic learning of parities, monomials\nand halfspaces. SIAM Journal on Computing, 39(2):606\u2013645, 2009.\nV. Feldman, P. Kothari, and J. Vondr\u00b4ak. Representation, approximation and learning of submod-\nular functions using low-rank decision trees. In COLT, pages 30:711\u2013740, 2013.\nVitaly Feldman and Pravesh Kothari. Learning coverage functions and private release of marginals.\nIn COLT, 2014.\nVitaly Feldman, Venkatesan Guruswami, Prasad Raghavendra, and Yi Wu. Agnostic learning of\nmonomials by halfspaces is hard. SIAM J. Comput., 41(6):1558\u20131590, 2012.\nA. Gupta, M. Hardt, A. Roth, and J. Ullman. Privately releasing conjunctions and the statistical\nquery barrier. In STOC. ACM, 2011.\nD. Haussler. Decision theoretic generalizations of the PAC model for neural net and other learning\napplications. Information and Computation, 100(1):78\u2013150, 1992. ISSN 0890-5401.\nA. Kalai and V. Kanade. Potential-based agnostic boosting. In Proceedings of NIPS, pages 880\u2013888,\n2009.\nA. Kalai, A. Klivans, Y. Mansour, and R. Servedio. Agnostically learning halfspaces. SIAM J.\nComput., 37(6):1777\u20131805, 2008.\nA. Kalai, V. Kanade, and Y. Mansour. Reliable agnostic learning. In Proceedings of COLT, 2009.\nM. Kearns. E\ufb03cient noise-tolerant learning from statistical queries. Journal of the ACM, 45(6):\n983\u20131006, 1998.\nM. Kearns, R. Schapire, and L. Sellie. Toward e\ufb03cient agnostic learning. Machine Learning, 17\n(2-3):115\u2013141, 1994.\nA. Klivans and A. Sherstov. Lower bounds for agnostic learning via approximate rank. Computa-\ntional Complexity, 19(4):581\u2013604, 2010.\nMario Marchand and John Shawe-Taylor. The set covering machine. Journal of Machine Learning\nResearch, 3:723\u2013746, 2002. URL http://www.jmlr.org/papers/v3/marchand02a.html.\nG. Valiant. Finding correlations in subquadratic time, with applications to learning parities and\njuntas. In FOCS, 2012.\nKarl Wimmer. Agnostically learning under permutation invariant distributions. In FOCS, pages\n113\u2013122, 2010.\n12\n",
        "sentence": "",
        "context": "Research, 3:723\u2013746, 2002. URL http://www.jmlr.org/papers/v3/marchand02a.html.\nG. Valiant. Finding correlations in subquadratic time, with applications to learning parities and\njuntas. In FOCS, 2012.\napplications. Information and Computation, 100(1):78\u2013150, 1992. ISSN 0890-5401.\nA. Kalai and V. Kanade. Potential-based agnostic boosting. In Proceedings of NIPS, pages 880\u2013888,\n2009.\nReferences\nPranjal Awasthi, Avrim Blum, and Or She\ufb00et. Improved guarantees for agnostic learning of dis-\njunctions. In Proceedings of COLT, pages 359\u2013367, 2010."
    },
    {
        "title": "Lower bounds and hardness amplification for learning shallow monotone formulas",
        "author": [
            "V. Feldman",
            "H. Lee",
            "R. Servedio"
        ],
        "venue": "Journal of Machine Learning Research - COLT Proceedings,",
        "citeRegEx": "Feldman et al\\.,? \\Q2011\\E",
        "shortCiteRegEx": "Feldman et al\\.",
        "year": 2011,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Agnostically learning decision trees",
        "author": [
            "Parikshit Gopalan",
            "Adam Tauman Kalai",
            "Adam R. Klivans"
        ],
        "venue": "In Cynthia Dwork, editor,",
        "citeRegEx": "Gopalan et al\\.,? \\Q2008\\E",
        "shortCiteRegEx": "Gopalan et al\\.",
        "year": 2008,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Pseudorandomness for concentration bounds and signed majorities",
        "author": [
            "Parikshit Gopalan",
            "Daniel Kane",
            "Raghu Meka"
        ],
        "venue": "CoRR, abs/1411.4584,",
        "citeRegEx": "Gopalan et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Gopalan et al\\.",
        "year": 2014,
        "abstract": "The problem of constructing pseudorandom generators that fool halfspaces has\nbeen studied intensively in recent times. For fooling halfspaces over the\nhypercube with polynomially small error, the best construction known requires\nseed-length O(log^2 n) (MekaZ13). Getting the seed-length down to O(log(n)) is\na natural challenge in its own right, which needs to be overcome in order to\nderandomize RL. In this work we make progress towards this goal by obtaining\nnear-optimal generators for two important special cases:\n  1) We give a near optimal derandomization of the Chernoff bound for\nindependent, uniformly random bits. Specifically, we show how to generate a x\nin {1,-1}^n using $\\tilde{O}(\\log (n/\\epsilon))$ random bits such that for any\nunit vector u, <u,x> matches the sub-Gaussian tail behaviour predicted by the\nChernoff bound up to error eps.\n  2) We construct a generator which fools halfspaces with {0,1,-1} coefficients\nwith error eps with a seed-length of $\\tilde{O}(\\log(n/\\epsilon))$. This\nincludes the important special case of majorities.\n  In both cases, the best previous results required seed-length of $O(\\log n +\n\\log^2(1/\\epsilon))$.\n  Technically, our work combines new Fourier-analytic tools with the iterative\ndimension reduction techniques and the gradually increasing independence\nparadigm of previous works (KaneMN11, CelisRSW13, GopalanMRTV12).",
        "full_text": "arXiv:1411.4584v1  [cs.CC]  17 Nov 2014\nPseudorandomness for concentration bounds and signed\nmajorities\nParikshit Gopalan\nMicrosoft\nDaniel M. Kane\nUniversity of California, San Diego\nRaghu Meka\nAbstract\nThe problem of constructing pseudorandom generators that fool halfspaces has been\nstudied intensively in recent times. For fooling halfspaces over {\u00b11}n with polynomi-\nally small error, the best construction known requires seed-length O(log2(n)) [MZ13].\nGetting the seed-length down to O(log(n)) is a natural challenge in its own right, which\nneeds to be overcome in order to derandomize RL. In this work we make progress to-\nwards this goal by obtaining near-optimal generators for two important special cases:\n\u2022 We give a near optimal derandomization of the Cherno\ufb00bound for independent,\nuniformly random bits. Speci\ufb01cally, we show how to generate x \u2208{\u00b11}n using\n\u02dcO(log(n/\u03b5)) random bits such that for any unit vector u, u \u00b7 x matches the sub-\nGaussian tail behaviour predicted by the Cherno\ufb00bound up to error \u03b5.\n\u2022 We construct a generator which fools halfspaces with {0, 1, \u22121} coe\ufb03cients with\nerror \u01eb with a seed-length of \u02dcO(log(n/\u01eb)). This includes the important special\ncase of majorities.\nIn both cases, the best previous results required seed-length of O(log n+log2(1/\u01eb)).\nTechnically, our work combines new Fourier-analytic tools with the iterative di-\nmension reduction techniques and the gradually increasing independence paradigm of\nprevious works [KMN11, CRSW13, GMR+12].\n1\n1\nIntroduction\nThe theory of pseudorandomness has given compelling evidence that very strong pseu-\ndorandom generators (PRGs) exist. For example, assuming that there are computational\nproblems solvable in exponential time that require exponential-sized circuits, Impagliazzo\nand Wigderson [IW97] showed that there exist very strong PRGs which allow us to simu-\nlate every randomized algorithm deterministically with only a polynomial slowdown, and\nthus BPP = P. These results, however, are conditional on a circuit complexity assumption\nwhose proof seems far o\ufb00. Since PRGs that fool a class of Boolean circuits also imply lower\nbounds for that class, we cannot hope to circumvent this assumption. Thus unconditional\ngenerators are only possible for restricted models of computation for which we have strong\nlower bounds.\nBounded-space algorithms are a natural computational model for which we know how to\nconstruct strong PRGs unconditionally. Let RL denote the class of randomized algorithms\nwith O(log n) work space which can access the random bits in a read-once pre-speci\ufb01ed\norder.\nNisan [Nis92] devised a PRG of seed length O(log2(n/\u03b5)) that fools RL.\nThis\ngenerator was used by Nisan himself to show that RL \u2286SC [Nis94] and by Saks and\nZhou [SZ99] to prove that RL can be simulated in space O(log3/2 n). Constructing PRGs\nwith the optimal O(log(n/\u01eb)) seed length for this class and showing that RL = L is arguably\nthe outstanding open problem in derandomization (which might not require a breakthrough\nin lower bounds). Despite much progress in this area [INW94, NZ96, RR99, Rei08, RTV06,\nBRRY14, BV10, KNP11, De11, GMR+12], there are few cases where we can improve on\nNisan\u2019s twenty year old bound of O(log2 n) [Nis92].\nHalfspaces are Boolean functions h : {\u00b11}n \u2192{\u00b11} described as h(x) = sgn(\u27e8w, x\u27e9\u2212\u03b8)\nfor some weight vector w \u2208Rn and threshold \u03b8 \u2208R. They are of central importance in\ncomputatonal complexity, learning theory and social choice. Lower bounds for halfspaces\nare trivial, whereas the problem of proving lower bounds against depth-2 TC0 or halfspaces\nof halfspaces is a frontier open problem in computational complexity.\nThe problem of\nconstructing explicit PRGs that can fool halfspaces is a natural challenge that has seen a\nlot of exciting progress recently [DGJ+09, MZ13, Kan11b, Kan14]. The best known PRG\nconstruction for halfspaces is that of Meka and Zuckerman [MZ13] who gave a PRG with\nseed-length O(log n + log2(1/\u01eb)), which is O(log2(n)) for polynomially small error. They\nalso made a connection to space bounded algorithms by showing that PRGs against RL with\ninverse polynomial error can be used to fool halfspaces. Thus constructing better PRGs for\nhalfspaces seems to be a necessary step towards progress for bounded-space algorithms.\nBeyond computational complexity, the problem of constructing better PRGs for half-\nspaces has ample algorithmic motivation; perhaps the most compelling of which comes\nfrom the ubiquitous applications in computer science of Cherno\ufb00-like bounds for weighted\nsums of the form P\ni wixi where the xis are uniformly random bits. There has been a\nlong line of work on showing sharp tail bounds for pseudorandom sequences starting from\n[SSS95]. A PRG for halfspaces with seed-length O(log(n/\u03b5)) would give a space of support\n1\nsize poly(n) where Cherno\ufb00-like tail bounds hold. This in turn would yield a black-box de-\nrandomization with only a polynomial slow-down of any algorithm which relies on uniform\nrandomness only for such tail bounds. PRGs for halfspaces also have other algorithmic\napplications to streaming algorithms for duplicate detection [GR09] and e\ufb03cient revenue\nmaximization for certain kinds of auctions [GNR14].\n1.1\nOur results\nA PRG is a function G : {\u00b11}r \u2192{\u00b11}n. We refer to r as the seed-length of the generator.\nThe \u02dcO() notation hides polylogarithmic factors in its argument. We say G is explicit if the\noutput of G can be computed in time poly(n).\nDe\ufb01nition 1. A PRG G : {\u00b11}r \u2192{\u00b11}n fools a class of functions F = {f : {\u00b11}n \u2192\n{\u00b11}} with error \u03b5 (or \u03b5-fools F) if for every f \u2208F,\n\f\f\f\f\nPr\nx\u2208{\u00b11}n[f(x) = 1] \u2212\nPr\ny\u2208{\u00b11}r[f(G(y)) = 1]\n\f\f\f\f < \u03b5.\nDerandomized Cherno\ufb00bounds\nCherno\ufb00bounds are a basic tool in the analysis of randomized algorithms. A ubiquitous\nversion that applies to the setting of independent random bits is the following:\nClaim 1 (Cherno\ufb00bound). There exist constants c1, c2 > 0 such that for every unit vector\nw \u2208Rn and t \u22651, Prx\u2208{\u00b11}n [ |\u27e8w, x\u27e9| > t] \u2264c1e\u2212c2t2.\nWe obtain a near-optimal derandomization of this result.\nTheorem 1. There exists an explicit generator G1 : {\u00b11}r \u2192{\u00b11}n and constants d1, d2\nsuch that for every unit vector w \u2208Rn, t \u22651 and \u03b5 > 0,\nPr\ny\u2208u{\u00b11}r [ |\u27e8w, G1(y)\u27e9| > t] \u2264d1e\u2212d2t2 + \u03b5.\nThe generator has seed-length r = \u02dcO(log(n/\u03b5)).\nTo contrast this with what was known previously, consider the setting where \u03b5 =\n1/poly(n). The Cherno\ufb00bound asserts that the probability that |\u27e8w, x\u27e9| = \u2126(\np\nlog(n)) is\ninverse polynomially small. A PRG for halfspaces with error parameter \u03b5 = 1/poly(n) would\nalso gurarantee such tails, but the best known construction requires seed-length O(log2(n))\n[MZ13]. One could also get such tail bounds using limited indpendence [SSS95]; however,\nwe would need O(log(n))-wise independence, which again requires O(log2(n)) seed-length.\n2\nFooling signed majorities\nAn important sub-class of halfspaces are those whose weight vectors have {0, 1, \u22121}-valued\nentries.\nThis corresponds to selecting a subset of variables, assiging each of them an\norientation and then taking a threshold. We henceforth refer to this class of halfspaces\nas signed majorities. Signed majorities arise naturally in voting theory, learning theory\nand property testing - see [MOO05, MORS09, RS13, BO10]. Fooling such tests requires\nfooling the sum of arbitrary subsets of variables in statistical distance, a problem that\nwas studied by [GMRZ13] in their work on fooling combinatorial shapes. Fooling sums\nin statistical distance includes as a special case modular tests on sums of variables with\nunrestricted modulus [LRTV09, MZ09]. PRGs for modular sums are a strong generalization\nof the versatile small-bias spaces [NN93] which correspond to fooling modular sums with\nmodulus two. The best previously known PRGs due to Lovett et al. for such tests require\nseed-length O(log2 n) [LRTV09] for large modulii, but their result can also handle sums\nwith non-binary coe\ufb03cients. Finally, signed majorities seem to capture several technical\nhurdles in designing optimal PRGs for halfspaces.\nWe construct a PRG which \u03b5-fools signed majorities with a seed-length of \u02dcO(log(n/\u03b5)).\nTheorem 2. There exists an explicit generator G2 : {\u00b11}r \u2192{\u00b11}n with seed-length\nr = \u02dcO(log(n/\u03b5)) which \u03b5-fools signed majorities .\nThe best previous result even for signed majorities had a seed-length of O(log n +\nlog2(1/\u03b5)) [MZ13]. For the important case of polynomially small error, \u03b5 = 1/poly(n), our\nresult gives the \ufb01rst improvement over the O(log2 n) bound implied by directly applying\nknown PRGs for space-bounded machines [Nis92, INW94].\nIndependently and concurrently De [De14] gave a PRG for combinatorial shapes intro-\nduced by [GMRZ13] with a seed-length of O(log3/2(n/\u03b5)). These objects are more general\nthan signed majorities but De\u2019s seed-length is worse than ours.\n1.2\nOther related work\nStarting with the work of Diakonikolas et al. [DGJ+09], there has been a lot of work\non constructing PRGs for halfspaces and related classes of intersections of halfspaces and\npolynomial threshold functions over the domain {\u00b11}n [DKN10, GOWZ10, HKM12, MZ13,\nKan11b, Kan11a, Kan14]. Rabani and Shpilka [RS10] construct optimal hitting set gen-\nerators for halfspaces over {\u00b11}n; hitting set generators are in general weaker than PRGs\nhowever.\nAnother line of work gives constructions of PRGs for halfspaces for the uniform distribu-\ntion over the sphere (spherical caps) or the Gaussian distribution. This case is easier than\nconstructing PRGs for halfspaces over the hypercube; the latter objects are known to imply\nthe former with comparable parameters. For spherical caps, Karnin, Rabani and Shpilka\n[KRS12] gave a PRG with a seed-length of O(log n + log2(1/\u03b5)). For the Gaussian distri-\nbution, [Kan14] gave a PRG which achieves a seed-length of O(log n + log3/2(1/\u03b5)). Very\n3\nrecently, Kothari and Meka [KM14] gave a PRG for spherical caps with a seed-length of\n\u02dcO(log(n/\u03b5)). At a high level, [KM14] also uses the iterative dimension reduction approach\nlike in [KMN11, CRSW13, GMR+12]; however, the \ufb01nal construction and its analysis are\nsigni\ufb01cantly di\ufb00erent.\n1.3\nOverview of our constructions\nDerandomized Cherno\ufb00bounds\nOur \ufb01rst attempt at constructing a PRG for the Cherno\ufb00bound applies a simple dimension\nreduction step iteratively.\n1. Starting from a linear function Pn\ni=1 wixi, (pseudo)randomly hash the variables into\n\u221an buckets using a hash function h.\n2. Use an \u03b5-biased string x to sum up coe\ufb03cients within a bucket. This gives a new\nlinear function P\u221an\nj=1 vjyj in \u221an dimensions where vj = P\ni:h(i)=j wixi.\nRepeating this step log log(n) times, we get down to \u03b8 \u2208R which is the value we\noutput. Call this generator G\u2032. It is easy to see that each output bit of G\u2032 is the xor of\nlog log(n) bits from independent \u03b5-biased strings, where the hash functions are used to\nselect co-ordinates from each string. This technique of applying pseudorandom dimension\nreduction iteratively is similar to [KMN11, CRSW13, GMR+12].\nDoes this generator give the desired tail behavior? Assume that we start from a unit\nvector w \u2208Rn. To get tail bounds, we would like to control the \u21132 norm, which starts\nat 1 but could increase substantially for particular choices of x.\nThe Cherno\ufb00bound\nsays that for truly random x, the \u21132 norm is unlikely to increase by more than a factor\nof c\np\nlog(n). Even if we manage to match this tail behavior in each step by choosing x\npseudorandomly (which is the problem we are trying to solve), the \ufb01nal bound we get\nwould be O((log n)log log(n)/2). Using \u03b5-biased x, we show a weaker bound of polylog(n) for\neach step, giving an overall bound of d(n) = (log(n))O(log log(n)). Showing this bound for\none step requires a fair amount of technical work, it works by decomposing the vector into\nweight scales and tuning the amount of independence to the scale like in [GMR+12]. We\nleave open the question of whther G\u2032 can itself give Cherno\ufb00-like tail bounds.\nNext we show that one gets the desired tail behaviour by hashing variables into m =\npoly(d(n)) buckets and using an independent copy of G\u2032 for each bucket. The reason is\nthe output of the resulting generator can be viewed as the sum of m independent bounded\nrandom variables, which lets us apply Bernstein\u2019s inequality which guarantees Cherno\ufb00-like\ntails for such variables. The boundedness comes from the tail guarantee of G\u2032: since large\ndeviations are very unlikely, we can condition on the event that they do not occur in any\nof the buckets. The \ufb01nal step is to reduce to seed-length, we do this by recycling the seed\nfor the various independent copies of G\u2032 using the INW generator [INW94], like in [MZ13].\n4\nFooling signed majorities\nLet us \ufb01x a test vector v \u2208{\u22121, 0, 1}n and error \u03b5 = 1/poly(n). Fooling signed majorities\nwith polynomially small error is equivalent to fooling linear sums of the form \u27e8v, x\u27e9in\nstatistical distance with error 1/poly(n). We shall adopt this view from now on.\nWe start with a generator that uses iterated dimension reduction and gradually-increasing\nindependence as we did for derandomizing the Cherno\ufb00bound. This by itself is not enough\nfor fooling sums in statistical distance. The reason is that there exist small-bias spaces with\nexponentially small bias that are far from fooling linear sums in statistical distance, like\nthe set of strings whose weight is divisible by 3 [VW08]. We design a di\ufb00erent generator to\ndeal with such tests and then combine the two generators by xoring independent copies.\nNext, note that showing closeness in statistical distance for discrete random variables\nis equivalent to showing that their Fourier transforms are close. Using this, it su\ufb03ces to\ndesign a generator G : {\u00b11}r \u2192{\u00b11}n such that for all \u03b1 \u2208R, the corresponding Fourier\ncoe\ufb03cient Ey[exp(2\u03c0i\u03b1 \u27e8v, G(y)\u27e9)] is close to its value under the uniform distribution. Note\nthat in order to fool the mod m test, it su\ufb03ces to fool all \u03b1 = j/m for integers j. We\nconsider two cases based on how large \u03b1 is relative to \u2225v\u22250 = k.\nLarge \u03b1:\nHere we consider \u03b1 \u226b1/\n\u221a\nk.\nThis includes the case of modular tests\nwhere the modulus is much smaller than\n\u221a\nk. We fool such tests using an error reduction\nprocedure. We start with the generator of [GMRZ13] which requires seed-length O(log n)\nto fool such tests with constant error. We then reduce the error to inverse polynomial at\nthe expense of a O(log log n) factor in seed-length using standard machinery from pseudo-\nrandomness. While technically simple, this step is the botteleneck in extending our result\nto more general halfpsaces: there is no analog of the [GMRZ13] generator to start from.\nSmaller \u03b1: This case which includes modular tests where the modulus is \u2126(\n\u221a\nk) is the\nharder case and technically the most novel portion of this work. The qualitative di\ufb00erence\nfrom the other case can be seen from the fact that when we sum k random bits modulo\nm = \u03c9(\n\u221a\nk), the resulting distribution is no longer uniform over congruence classes.\nThe generator uses dimension reduction in a manner similar to what we used to de-\nrandomize the Cherno\ufb00bound. Like before, the plan is to show that a single dimension\nreduction step does not incur too much error. However, the analysis is very di\ufb00erent and\nrequires several new tools. This step critically exploits the recursive structure of the gen-\nerator: to analyze the error we can work as if the variables in the reduced space are given\ntruly random signs and then recursively analyze the error in the reduced space. Working\nwith truly random bits in the smaller-dimensional space helps us reduce bounding the error\nto \ufb01nding good low-degree polynomial approximators for a certain product of cosines. In\nthe most technically, involved part of our argument we use various analytic tools to \ufb01nd\nsuch low-degree approximators. One additional ingredient is that the above approach does\nnot actually work for all test vectors but only for su\ufb03ciently well-spread out vectors as\n5\nmeasured by their \u21132, \u21134 norms. The \ufb01nal piece is to argue that the \u21132, \u21134 norms are not\ndistorted too much by the dimension reduction steps.\n2\nPreliminaries\nWe start with some notation:\n\u2022 For vectors x \u2208Rn, let \u2225x\u2225p denote the usual \u2113p-norms, and let \u2225x\u22250 denote the size\nof the support of x. For a random variable X and p > 0, let \u2225X\u2225p = E[|X|p]1/p.\n\u2022 For a multi-linear polynomial Q : Rn \u2192R, \u2225Q\u22252\n2 denotes the sum of squares of\ncoe\ufb03cients of Q and \u2225Q\u22251 denotes the sum of absolute values of the coe\ufb03cients.\n\u2022 For vectors u, v \u2208Rn, let u \u22c6v = (uivi)n\ni=1 denote the coordinate-wise product.\n\u2022 For v \u2208{\u00b11}n and \u03b1 \u2208R, de\ufb01ne \u03c6v,\u03b1(x) = exp(2\u03c0i\u03b1(v \u00b7 x)).\n\u2022 For v \u2208Rn and a hash function h : [n] \u2192[m], de\ufb01ne\nh(v) =\nm\nX\nj=1\n\u2225v|h\u22121(j)\u22254\n2\n(1)\n\u2022 For a hash function h : [n] \u2192[m], let A(h) \u2208{0, 1}m\u00d7n, be the matrix with A(h)ji = 1\nif and only if h(i) = j.\n\u2022 For a string x \u2208{\u00b11}n, let D(x) \u2208Rn\u00d7n be the diagonal matrix formed by x.\n\u2022 For two random variables X, Y over a domain \u2126, their statistical distance is de\ufb01ned\nas dTV (X, Y ) = maxA\u2286\u2126| Pr[X \u2208A] \u2212Pr[Y \u2208A]|.\n\u2022 Unless otherwise stated c, C denote universal constants.\nThroughout we assume that n is su\ufb03ciently large and that \u03b4, \u01eb > 0 are su\ufb03ciently small.\nAll vectors here will be row vectors rather than column vectors.\nDe\ufb01nition. For n, m, \u03b4 > 0 we say that a family of hash functions H = {h : [n] \u2192[m]} is\n\u03b4-biased if for any r \u2264n distinct indices i1, i2, . . . , ir \u2208[n] and j1, . . . , jr \u2208[m],\nPr\nh\u2208uH [h(i1) = j1 \u2227h(i2) = j2 \u2227\u00b7 \u00b7 \u00b7 \u2227h(ir) = jr] = 1\nmr \u00b1 \u03b4.\nWe say that such a family is k-wise independent if the above holds with \u03b4 = 0 for all\nr \u2264k. We say that a distribution over {\u00b11}n is \u03b4-biased or k-wise independent if the\ncorresponding family of functions h : [n] \u2192[2] is.\n6\nSuch families of functions can be generated using small seeds.\nFact 3. For n, m, k, \u03b4 > 0, there exist explicit \u03b4-biased families of hash functions h : [n] \u2192\n[m] that are generated from a seed of length s = O(log(n/\u03b4)).\nThere are also, explicit\nk-wise independent families that are generated from a seed of length s = O(k log(nm)).\nTaking the pointwise sum of such generators modulo m gives a family of hash func-\ntions that is both \u03b4-biased and k-wise independent generated from a seed of length s =\nO(log(n/\u03b4) + k log(nm)).\n2.1\nBasic Results\nWe collect some known results about pseudorandomness and prove some other technical\nresults that will be used later. We defer all proofs for this section to Appendix A.\nWe will use the following result from [GMRZ13] giving PRGs for signed majorities.\nTheorem 4. [GMRZ13] For n, \u03b5 > 0 there exists an explicit pseudorandom generator,\nY \u2208{\u00b11}n generated from a seed of length s = O(log(n) + log2(1/\u01eb)) so that for any\nv \u2208{\u22121, 0, 1}n and X \u2208u {\u00b11}n, we have that dTV (v \u00b7 Y, v \u00b7 X) \u2264\u01eb.\nWe shall use PRGs for small-space machines or read-once branching programs (ROBP)\nof Nisan [Nis92] and Impagliazzo, Nisan and Wigderson [INW94].\nDe\ufb01nition 2 ((S, D, T)-ROBP). An (S, D, T)-ROBP M is a layered directed graph with\nT + 1 layers and 2S vertices per layer with the following properties.\n\u2022 The \ufb01rst layer has a single start node and the last layer has two nodes labeled 0, 1\nrespectively.\n\u2022 A vertex v in layer i, 0 \u2264i < T has 2D edges to layer i + 1 each labeled with an\nelement of {0, 1}D.\nA graph M as above naturally de\ufb01nes a function M :\n\u0000{0, 1}D\u0001T \u2192{0, 1} where on in-\nput (z1, . . . , zT ) \u2208\n\u0000{0, 1}D\u0001T one traverses the edges of the graph according to the labels\nz1, . . . , zT and outputs the label of the \ufb01nal vertex reached.\nTheorem 5 ([Nis92], [INW94]). There exists an explicit PRG GINW : {0, 1}r \u2192\n\u0000{0, 1}D\u0001T\nwhich \u01eb-fools (S, D, T)-branching programs and has seed-length r = O(D+S log T+log(T/\u03b4)\u00b7\n(log T)).\nWe will need to make use of the hypercontractive inequality (see [O\u2019D14]):\nLemma 6 (Hypercontractivity). Let x \u223cu {\u00b11}n. Then, for a degree d polynomial Q and\nan even integer p \u22652,\nE [Q(x)p] \u2264(p \u22121)pd/2 \u00b7 \u2225Q\u2225p\n2.\n7\nLemma 7 (Hypercontractivity \u03b4-biased). Let x \u223cD be drawn from a \u03b4-biased distribution.\nThen, for a degree d polynomial Q and an even integer p \u22652,\nE [Q(x)p] \u2264(p \u22121)pd/2 \u00b7 \u2225Q\u2225p\n2 + \u2225Q\u2225p\n1\u03b4.\nWe will use the following Cherno\ufb00-like tail bound for small-bias spaces.\nLemma 8. For all v \u2208Rn with \u2225v\u22252 = 1 and x \u223cD \u01eb-biased over {\u00b11}n, and t \u22651\nPr [|\u27e8v, x\u27e9| > t] \u22642 exp(\u2212t2/4) + \u2225v\u2225t2\n0 \u00b7 \u01eb.\nThe next two lemmas quantify load-balancing properties of \u03b4-biased hash functions in\nterms of the \u2113p-norms of vectors.\nLemma 9. Let p \u22652 be an integer. Let v \u2208Rn and H = {h : [n] \u2192[m]} be either a\n\u03b4-biased hash family for \u03b4 > 0 or a p-wise independent family for \u03b4 = 0. Then\nE[h(v)p] \u2264O(p)2p\n\u0012\u2225v\u22254\n2\nm\n\u0013p\n+ O(p)2p\u2225v\u22254p\n4 + mp\u2225v\u22254p\n2 \u03b4.\nLemma 10. For all v \u2208Rn\n+ and H = {h : [n] \u2192[m]} a \u03b4-biased family, and j \u2208[m], and\nall even p \u22652,\nPr\nh\f\f\f\n\r\rv|h\u22121(j)\n\r\r\n1 \u2212\u2225v\u22251 /m\n\f\f\f \u2265t\ni\n\u2264O(p)p/2 \u2225v\u2225p\n2 + \u2225v\u2225p\n1 \u03b4\ntp\n.\n3\nDerandomizing the Cherno\ufb00Bounds\nIn this section we present a pseudorandom generator that gives Cherno\ufb00-like tail bounds.\nTheorem 11. For all \u03b4 > 0, there exists an explicit generator G : {0, 1}r \u2192{\u00b11}n with\nseed-length r = \u02dcO(log(n/\u03b4)) such that for all unit vectors w \u2208Rn, and t \u22650,\nPr\ny\u2208u{0,1}r[|\u27e8w, G(y)\u27e9| \u2265t] \u22644 exp(\u2212t2/16) + \u03b4.\nOur construction proceeds in two steps.\nWe \ufb01rst construct a generator which has\nmoderate tail bounds but does not match the tail behaviour of truly random distribution.\nWe then boost the tail bounds to match the behaviour of truly random distributions using\nPRGs for small-space machines.\n8\n3.1\nModerately Decaying Tails\nThe main result of this section is a generator with the following tail behaviour.\nLemma 12. For n and \u03b3 \u2208(0, 1), there exists an explicit generator G\u2032 : {0, 1}r\u2032 \u2192{\u00b11}n\nwith seed-length r\u2032 = O(log(n/\u03b3) log log(n)) such that for all unit vectors w \u2208Rn,\nPr\ny\u2208u{0,1}r\nh\n|\u27e8w, G\u2032(y)\u27e9| \u2265(C1 log(n/\u03b3))C2 log log(n)i\n\u2264\u03b3.\nThe generator G\u2032 is recursively de\ufb01ned. We \ufb01rst specify the one-step generator G\u2032\u2032 that\nis used in de\ufb01ning G\u2032. Fix \u03b4 > 0, n. Let H = {h : [n] \u2192[m]} be a family of \u03b4-biased hash\nfunctions. Let D be a \u03b4-biased distribution over {\u00b11}n. The generator G\u2032\u2032 takes as input\na hash function h \u2208H, x \u2208D and z \u2208u {\u00b11}m, the output is\nG\u2032\u2032(h, x, z) = zA(h)D(x).\nThus we have for any i \u2208[n],\nG\u2032\u2032(h, x, z)j = zh(i)xi.\nThus the generator G\u2032\u2032 starts with the \u03b4-biased string x \u2208D as output, hashes the coordi-\nnates into [m] bins and \ufb02ips the signs of all coordinates in each bin by picking a uniformly\nrandom independent bit for each bin. This takes O(log(n/\u03b4) + m) random bits.\nThe generator G\u2032 is obtained by taking m \u2248\u221an and then recursively using G\u2032\u2032 to\ngenerate z \u2208{\u00b11}n. The base case of the recursion is reached when m = O(log(n/\u03b4))\nat which point we use a truly random string z. This requires k \u2264log log(n) stages of\nrecursion, so that the seed length is O(log(n/\u03b4) log log(n)). Unrolling the recursion, we see\nthat if we set n\u2113= n2\u2212\u2113for \u2113\u2208{0, . . . , k} then G\u2032 takes as input two sequences:\n\u2022 A sequence of hash functions h1, . . . , hk where h\u2113: [n\u2113\u22121] \u2192[n\u2113] is drawn from a\n\u03b4-biased family of hash functions.\n\u2022 A sequence of strings x1, . . . , xk where x\u2113\u2208{\u00b11}n\u2113is drawn from a \u03b4-biased distri-\nbution.\nFor each coordinate i, consider the sequence {i\u2113\u2208n\u2113}k\n\u2113=0 obtained by successively applying\nthe hash functions:\ni0 = i,\ni\u2113= h\u2113(i\u2113\u22121) for \u2113\u22651.\nThen we have\nG\u2032(h1, . . . , hk, x1, . . . , xk) =\nk\nY\n\u2113=1\nx\u2113\ni\u2113.\nThe analysis of G\u2032 proceeds step by step and each step reduces to analyzing G\u2032\u2032. Note\nthat\n\u27e8w, G\u2032\u2032(h, x, z)\u27e9= \u27e8w, zA(h)D(x)\u27e9= \u27e8A(h)D(x)wT , z\u27e9.\n9\nWe can view A(h)D(x)wT as projection of w \u2208Rn down to Rm where we \ufb01rst hash\ncoordinates into buckets, and then sum the coordinates in a bucket with signs given by x.\nThe next lemma saying that the transformation A(h)D(x) is unlikely to stretch Euclidean\nnorms too much serves as the base case for the recursion.\nLemma 13. Let n \u22651 and m = \u221an and \u03b4 < 1/10n2. Let D be a \u03b4-biased distribution\nover {\u00b11}n and H = {h : [n] \u2192[m]} be a \u03b4-biased hash family. There exists a constant C\nsuch that for all unit vectors w \u2208Rn,\nPr\nx\u2208D,h\u2208H\nh\r\rA(h)D(x)wT \r\r\n2 \u2265C(log log(n)) log(1/\u03b4)3/4i\n\u22643(log log(n))m\n\u221a\n\u03b4\nWe prove this lemma by decomposing the vector w across various weight scales. Fix a\nunit vector w \u2208Rn. Without loss of generality, we ignore all coordinates i where |wi| \u22641/n\nas they can only e\ufb00ect the \u21132-norm by at most 1. For \u2113\u2208{1, . . . , log log n}, de\ufb01ne w(\u2113) \u2208Rn\nas\nw(\u2113)i =\n(\nwi\nif |wi| \u2208\n\u0010\n1\n22\u2113,\n1\n22\u2113\u22121\ni\n0\notherwise.\nThus w(\u2113) picks out the entries in the \u2113th weight scale. In addition, de\ufb01ne w(0) to consist\nof entries that lie in the interval (1/2, 1]. We will show that for every \u2113, the bound\n\r\rA(h)D(x)w(\u2113)T \r\r2\n2 \u2264O(1) log(1/\u03b4)1.5\nholds with high (inverse polynomial) probability. Here we tailor the amount of indepen-\ndence we use in the argument to the scale, in a manner similar to [CRSW13, GMR+12].\nOnce this is done, Lemma 13 follows by the triangle inequality.\nWe start with a simple bound which su\ufb03ces for small constant \u2113.\nLemma 14. For all \u2113, we have\n\r\rA(h)D(x)w(\u2113)T \r\r\n2 \u226422\u2113.\nProof. Observe that\n\r\rA(h)D(x)w(\u2113)T \r\r\n\u221e\u2264\u2225w(\u2113)\u22251\n\r\rA(h)D(x)w(\u2113)T \r\r\n1 \u2264\u2225w(\u2113)\u22251\nhence by Holder\u2019s inequality\n\r\rA(h)D(x)w(\u2113)T \r\r\n2 \u2264\u2225w(\u2113)\u22251 .\nSince \u2225w(\u2113)\u22252 \u22641 and every non-zero entry is at least 2\u22122\u2113we have \u2225w(\u2113)\u22251 \u226422\u2113and\nhence\n\r\rA(h)D(x)w(\u2113)T \r\r\n2 \u226422\u2113.\n10\nGiven this lemma, we can assume that \u2113is a su\ufb03ciently large constant. We show that\nthe weight vector is hashed fairly regularly with high probability over the choice of h \u2208H,\nwhere the regularity is measured by h(v).\nLemma 15. Fix \u2113\u22652. Then\nPr\nh\u2208H\n\"\nh(w(\u2113)) \u22642C4\np\nlog(1/\u03b4)\n22\u2113\u22122\n#\n\u22651 \u22122m\n\u221a\n\u03b4.\n(2)\nProof. By Lemma 10 applied to the vector (w(\u2113)2\ni )n\ni=1, there is a constant C4 such that for\neven q \u22652\nPr\nh\u2208H [h(w(\u2113)) \u22651/m + t] \u2264m\n  \nC4\u221aq \u2225w(\u2113)\u22252\n4\nt\n!q\n+ \u03b4\n \n\u2225w(\u2113)\u22252\n2\nt\n!q!\n.\nPlugging in the bounds\n\u2225w(\u2113)\u22252\n4 \u2264\u2225w(\u2113)\u2225\u221e\u22642\u22122\u2113\u22121, \u2225w(\u2113)\u22252 \u22641\nwe get\nPr\nh\u2208H [h(w(\u2113)) \u22651/m + t] \u2264m\n\u0012\u0012C4\u221aq\n22\u2113\u22121t\n\u0013q\n+ \u03b4\n\u00121\nt\n\u0013q\u0013\n.\nTherefore, taking\nq = log(1/\u03b4)\n2\u2113\u22121\n,\nt = C4\u221aq\n22\u2113\u22122\nin the above equation, we get\nC4\u221aq \u2225w(\u2113)\u22252\n4\nt\n\u226422\u2113\u22122\n22\u2113\u22121 \u2264\n1\n22\u2113\u22122 ,\n \nC4\u221aq \u2225w(\u2113)\u22252\n4\nt\n!q\n\u2264\n1\n22\u2113\u22122 log(1/\u03b4)/2\u2113\u22121 \u2264\n\u221a\n\u03b4.\n\u00121\nt\n\u0013q\n\u2264(22\u2113\u22122)log(1/\u03b4)/2\u2113\u22121 = 1\n\u221a\n\u03b4\n.\nhence\nPr\nh\u2208H\n\"\nh(w(\u2113)) \u22651/m + C4\np\nlog(1/\u03b4)\n22\u2113\u22122\n#\n\u22642m\n\u221a\n\u03b4.\nHence with probability 1 \u22122m\n\u221a\n\u03b4 over the choice of h, we have\nh(w(\u2113)) \u22641\nm + C4\np\nlog(1/\u03b4)\n22\u2113\u22122\n\u22642C4\np\nlog(1/\u03b4)\n22\u2113\u22122\nsince 1/m = 1/\u221an \u22642\u22122\u2113\u22122.\n11\nConditioned on the hash function h being good (i.e., satisfying the condition of the\nprevious lemma), we will show that\n\r\rA(h)D(x)w(\u2113)T \r\r2\n2 is small with high-probability.\nLemma 16. Fix \u2113\u22652 and assume that h is such that the event described in Equation (2)\nholds. There exists a constant C6 such that\nPr\nx\u2208D\n\"\n\r\rA(h)D(x)w(\u2113)T \r\r\n2 \u2264C6 log\n\u00121\n\u03b4\n\u00135/8#\n\u22651 \u2212\n\u221a\n\u03b4 \u2212\u03b416.\n(3)\nProof. We will show that\n\r\rA(h)D(x)w(\u2113)T \r\r2\n2 is concentrated around its mean (which is\n\u2225w(\u2113)\u22252\n2) by bounding its moments. The deviation is given by the polynomial\nQ\u2113(x) =\n\r\rA(h)D(x)w(\u2113)T \r\r2\n2 \u2212\u2225w(\u2113)\u22252\n2\n=\nX\nj\u2208[m]\n\uf8eb\n\uf8ed\nX\ni\u2208h\u22121(j)\nw(\u2113)ixi\n\uf8f6\n\uf8f8\n2\n\u2212\u2225w(\u2113)\u22252\n2\n=\nX\nj\u2208[m]\nX\ni1\u0338=i2\u2208h\u22121(j)\nw(\u2113)i1w(\u2113)i2xi1xi2\nWe have\nE\nx\u2208u{\u00b11}n[Q\u2113(x)2] =\nm\nX\nj=1\nX\ni1\u0338=i2\u2208h\u22121(j)\n(w(\u2113)i1)2(w(\u2113)i2)2\n\u2264\nm\nX\nj=1\n\u2225w(\u2113)|h\u22121(j)\u22254\n2\n= h(w(\u2113)).\n\u2225Q\u2113\u22251 =\nX\nj\u2208[m]\nX\ni1\u0338=i2\u2208h\u22121(j)\n|w(\u2113)i1w(\u2113)i2|\n\u2264\nX\nj\u2208m\n\r\rw(\u2113)|h\u22121(j)\n\r\r2\n1\n\u2264\u2225w(\u2113)\u22252\n1\n\u2264\u2225w(\u2113)\u22250 .\nBy Lemma 7 applied to Q with d = 2, there exists a constant C3 so that for all even\np \u22652,\nE\nx\u2208D[Q(x)p] \u2264\n\u0010\nC3p\np\nh(w(\u2113))\n\u0011p\n+ \u2225w(\u2113)\u2225p\n0 \u03b4.\n(4)\nWe bound h(w\u2113) using Equation (2). We also have \u2225w(\u2113)\u22250 \u226422\u2113. Plugging these into\nEquation (4),\nE\nx\u2208D[Q(x)p] \u2264Cp\n5pp log(1/\u03b4)p/4\n22\u2113\u22123p\n+ 22\u2113p\u03b4\n12\nNow setting\np = log(1/\u03b4)\n2\u2113+1\n, \u03b8 = C5 log(1/\u03b4)5/4\nand using Markov\u2019 inequality gives\nPr\nx\u2208D[Q(x) \u2265\u03b8] \u2264\n \nC5p log(1/\u03b4)1/4\n22\u2113\u22123\u03b8\n!p\n+\n \n22\u2113\n\u03b8\n!p\n\u03b4\nTo bound the \ufb01rst term, note that\n \nC5p log(1/\u03b4)1/4\n22\u2113\u22123\u03b8\n!p\n\u2264\n\u0012\n1\n22\u2113\u22123\n\u0013log(1/\u03b4)/2\u2113+1\n\u2264\u03b416.\nFor the second term, note that since \u03b8 \u22651,\n \n22\u2113\n\u03b8\n!p\n\u03b4 \u226422\u2113log(1/\u03b4)/2\u2113+1\u03b4 \u2264\n\u221a\n\u03b4\nTherefore, except with probability at least\n\u221a\n\u03b4 + \u03b416 we have\n\r\rA(h)D(x)w(\u2113)T \r\r2\n2 \u2264\u2225w\u2113\u22252\n2 + C5 log(1/\u03b4)5/4\nhence\n\r\rA(h)D(x)w(\u2113)T \r\r\n2 \u2264C6 log(1/\u03b4)5/8\nWe now \ufb01nish the proof of Lemma 13.\nProof of Lemma 13. Note that\nw =\nlog log(n)\nX\ni=0\nw(\u2113).\nWe will assume that h and x are chosen so that the conditions in Equations (2) and (3)\nhold for all \u2113. By the union bound, this happens except with probability\nlog log(n)(2m\n\u221a\n\u03b4 +\n\u221a\n\u03b4 + \u03b416) < 3 log log(n)m\n\u221a\n\u03b4.\n13\nIn which case, we have\n\u2225A(h)D(x)w\u22252 =\n\r\r\r\r\r\r\nlog log(n)\nX\n\u2113=0\nA(h)D(x)w(\u2113)T\n\r\r\r\r\r\r\n2\n\u2264\nlog log(n)\nX\n\u2113=0\n\r\rA(h)D(x)w(\u2113)T \r\r\n2\n\u2264C6 log log(n) log(1/\u03b4)5/8\n\u2264C6 log(1/\u03b4)3/4.\nWe now prove the main lemma of this section:\nProof of Lemma 12. Let k = log log(n) be the number of recursive stages. Let w be a unit\nvector. Given h1, . . . , hk and x1, . . . , xk, we have\n\u27e8w, G\u2032(h1, . . . , hk, x1, . . . , xk)\u27e9=\nk\nY\n\u2113=1\nA(h\u2113)D(x\u2113)wT\nLet C8n\n\u221a\n\u03b4 = \u03b3, so that \u03b4 = \u2126(\u03b32/n2). Note that log(1/\u03b4) \u226blog log(n)4.\nBy applying Lemma 13 inductively and using the union bound, except with probability\n3(log log(n))2m\n\u221a\n\u03b4 \u2264C8n\n\u221a\n\u03b4\nwe have that for every i \u2264k\n\r\r\r\r\r\niY\n\u2113=1\nA(h\u2113)D(x\u2113)wT\n\r\r\r\r\r\n2\n\u2264(C6 log log(n) log(1/\u03b4)3/4)i \u2264log(1/\u03b4)i/2\nand hence\n|\u27e8w, G\u2032(h1, . . . , hk, x1, . . . , xk)\u27e9| \u2264log(1/\u03b4)log log(n)/2.\nThus, with probability 1 \u2212\u03b3, the deviation is bounded by\nd(n, \u03b3) := (C1 log(n/\u03b3))C2 log log(n)\nand the seedlength of this generator is\nr\u2032 = O(log(n/\u03b4) log log(n)) = O(log(n/\u03b3) log log(n)).\n14\n3.2\nGetting sub-Gaussian tail bounds\nThe generator G\u2032 gives a tail probability of 1 \u2212\u03b3 pseudorandomly for d(n, \u03b3) standard\ndeviations. We now boost this to obtain sub-Gaussian tails by starting with independent\ncopies of G\u2032 and then reuse the seeds for using a PRG for space bounded computations.\nWe will make some added assumptions about G\u2032:\n\u2022 The output is \u03b5-biased for some \u03b5 \u226a\u03b3. We can ensure this by xor-ing the output\nwith an \u03b5-biased string.\n\u2022 The distribution is symmetric: for every x, Pry[G\u2032(y) = x] = Pry[G\u2032(y) = \u2212x]. We\nensure this by outputting either G\u2032(y) or \u2212G\u2032(y) with probability 1/2.\nLet D1, D2 be constants such that\nm = (D1 log(n/\u03b3))D2 log log(n) > 10d(n, \u03b3)2 log(1/\u03b3).\nNote that for \u03b3 = 1/poly(n), log(m) = O(log log(n)2).\nLet H = {h : [n] \u2192[m]} be a family of \u03b3-biased hash functions. De\ufb01ne a new generator\n\u00afG : ({0, 1}r)m \u00d7 H \u2192{\u00b11}n as follows:\n\u00afG(z1, . . . , zT , h)i = G\u2032(zj)i, if h(i) = j.\n(5)\nThe seed-length of the generator is \u00afr = log(n/\u03b4) + m \u00b7 r\u2032 which we will later improve to\nlog(n/\u03b4) + r\u2032 + log(n/\u03b3) log(m) using PRGs for space bounded computations.\nThe following claim characterizes the tail behaviour of the output of \u00afG.\nLemma 17. Let 0 < \u03b5 < \u03b3 \u22641/n3. For all unit vectors w \u2208Rn, the generator \u00afG satis\ufb01es\nPr\ny\u2208{0,1}\u00afr\n\u0002\f\f\u27e8w, \u00afG(y)\u27e9\n\f\f \u2265t\n\u0003\n\u22644(exp(\u2212t2/16) + m\u221a\u03b3 + (1/\u03b3)4 log(m)\u03b5).\nProof. Note that it su\ufb03ces to prove the claim for t \u22642\np\nlog(1/\u03b3) since tail probabilities\ncan only decrease with t and beyond this value, the tail bound is dominated by the additive\nterms.\nFix a unit vector w \u2208Rn. Let\n\u03b2 =\n1\nm2p\nlog(1/\u03b3)\n(6)\nand de\ufb01ne u, v \u2208Rn to consist of the heavy and light indices respectively\nui =\n(\nwi\nif |wi| \u2265\u03b2\n0\notherwise. ,\nvi =\n(\nwi\nif |wi| < \u03b2\n0\notherwise. .\n15\nSince w = u+v, it su\ufb03ces to bound the probability that either of |\u27e8u, \u00afG(y)\u27e9| and |\u27e8v, \u00afGy\u27e9|\nexceed t/2. We will consider u \ufb01rst. Note that\n\u2225u\u22250 \u22641\n\u03b22 \u2264m4 log\n\u00121\n\u03b3\n\u0013\n.\nSince \u00afG(y) is \u03b5-biased, by Lemma 8 applied for t/2 \u2264\np\nlog(1/\u03b3),\nPr\n\u0002\f\f\u27e8u, \u00afG(y)\u27e9\n\f\f > t/2\n\u0003\n\u22642 exp(\u2212t2/16) + \u2225u\u2225log(1/\u03b3)\n0\n\u03b5\n\u22642 exp(\u2212t2/16) + (m log(1/\u03b3))log(1/\u03b3)\u03b5.\n(7)\nWe will show a tail bound for \u27e8v, \u00afG(y)\u27e9by bounding its higher order moments. Fix a\nhash function h \u2208u H and for j \u2208{1, . . . , m}, let\nZj = \u27e8v|h\u22121(j), G\u2032(zj)\u27e9.\nNote that the random variables Zj are independent of one another, and\n\u27e8v, \u00afG(y)\u27e9=\nm\nX\nj=1\nZj.\nWe use Lemma 10 to bound\n\r\rv|h\u22121(j)\n\r\r\n2. We defer the proof of the following technical\nlemma.\nLemma 18. With probability 1 \u22122m\u221a\u03b3, for all j \u2208[m] we have\n\r\rv|h\u22121(j)\n\r\r2\n2 \u22642\nm\n(8)\nWe condition on the hash function h satisfying Equation (8), and call this event A.\nRecall that Zj = \u27e8v|h\u22121(j), G\u2032(zj)\u27e9. By Lemma 12, with probability 1 \u2212\u03b3 over zj, we\nhave the bound\n|Zj| \u2264d(n, \u03b3)\n\r\rv|h\u22121(j)\n\r\r\n2 \u2264\n\u221a\n2d(n, \u03b3)\n\u221am\n\u2264\n1\n2\np\nlog(1/\u03b3)\n:= M\n(9)\nwhere the last inequality is by the choice of m. By the union bound, Equation (9) holds\nwith probability at least 1 \u2212m\u03b3 over z1, . . . , zm, for all j \u2208[m]. We further condition on\nthe event |Zj| \u2264M for all j \u2208[m] which we denote by B.\nConditioning on a high probability event preserves the small-bias property of G(zj)\u2019s\nup to a small additive error. In particular, conditioned on the event B, G\u2032(zj) is (\u03b5 + m\u03b3)-\nbiased. Since\n\r\rv|h\u22121(j)\n\r\r\n1 \u2264\u221an we have\nm\nX\nj=1\nE[Z2\nj |B] \u2264\nm\nX\nj=1\n(\n\r\rv|h\u22121(j)\n\r\r2\n2 + n(\u03b5 + m\u03b3))\n\u22641 + nm(\u03b5 + m\u03b3)\n\u22642.\n16\nFurther, since G\u2032(zj) is symmetric, it continues to be symmetric after we condition on B\n(which is a symmetric event in G\u2032(zj)).\nSince t/2 \u2264\np\nlog(1/\u03b3), and M = 1/2\np\nlog(1/\u03b3) we have Mt \u22641/2. We now apply\nBernstein\u2019s inequality [Fel71] to the random variables {Zj|B}m\nj=1 which are mean zero and\nare bounded by M to get\nPr\n\uf8ee\n\uf8f0\n\f\f\f\f\f\f\nm\nX\nj=1\nZj|B\n\f\f\f\f\f\f\n> t/2\n\uf8f9\n\uf8fb\u22642 exp\n \n\u2212\nt2\n4(P\nj \u2225Zj|B\u22252\n2 + Mt/3)\n!\n\u22642 exp\n\u0012\n\u2212t2\n28/3\n\u0013\n\u22642 exp\n\u0012\n\u2212t2\n16\n\u0013\n.\nCombining the above arguments, we get that\nPr\n\u0002\f\f\u27e8v, G\u2032(y)\u27e9\n\f\f > t/2\n\u0003\n\u22642 exp(\u2212t2/16) + 2m\u221a\u03b3.\n(10)\nThe claim now follows from combining Equations (10) and (7)\nProof of Lemma 18. Note that\n\u2225v \u22c6v\u22251 \u22641, \u2225v \u22c6v\u22252 = \u2225v\u22252\n4 \u2264\u03b2.\nTherefore, by setting\np = log(1/\u03b3)\nlog(1/\u03b2), t = D6\np\n\u03b2p\nin Lemma 10 we get\nPr\nh\u2208H\n\"\n\r\rv|h\u22121(j)\n\r\r2\n2 \u22651\nm + D6\ns\n\u03b2 log(1/\u03b3)\nlog(1/\u03b2)\n#\n\u2264Dp\n6pp\u03b2p + \u03b3\n(D6\n\u221a\u03b2p)p \u22642\u221a\u03b3.\nBy a union bound, with probability at least 1 \u22122m\u221a\u03b3 over h, for all j \u2208[m],\n\r\rv|h\u22121(j)\n\r\r2\n2 \u22641\nm +\ns\n\u03b2 log(1/\u03b3)\nlog(1/\u03b2)\n\u22642\nm\nwhere the last inequality follows from our choice of \u03b2 in Equation (6).\n17\n3.3\nPutting things together\nWe are now ready to prove Theorem 11.\nProof of Theorem 11. Let \u00afG :\n\u0010\n{0, 1}r\u2032\u0011m\n\u00d7 H \u2192{\u00b11}n be the generator as in Lemma 17.\nLet \u03b4 be the \ufb01nal additive error desired. Set\n\u03b3 =\n\u03b42\n(D5 log(n/\u03b4))D6 log log(n) \u2264\n\u03b42\n50m2\n\u03b5 =\n\u0012 \u03b4\nn\n\u0013D7 log log(n/\u03b4)3\n\u2264\n\u03b4\n20\u03b34 log(m) .\nIt can be veri\ufb01ed that with these parameter seedings, the error probability in Lemma 17\nis at most 4e\u2212t2/16 + \u03b4, and the seed-length of \u00afG is log(n/\u03b3) + mr\u2032, where r\u2032 = O(log(n/\u03b4) \u00b7\n(log log(n/\u03b4))3).\nObserve that once we \ufb01x the has function h, the inner product \u27e8w, G\u2032(z1, . . . , zm, h)\u27e9\ncan be computed by a (S, r\u2032, m)-ROBP where S = O(log n) which reads one z1, z2, . . . , zm\nin order. The reason is that we can round each weight wi up to a multiple of 1/n2. This\ncan only increase |\u27e8w, z\u27e9| by 1/n for any z \u2208{\u00b11}n. This also ensures that \u27e8w, z\u27e9lies in\nthe interval [\u2212\u221an, \u221an] and that it is a multiple of 1/n2 and thus can be computed with\nO(log n)-bits of precision.\nNow, let GINW : {0, 1}rs \u2192\n\u0010\n{0, 1}r\u2032\u0011m\nbe a generator fooling O(S, r\u2032, m)-ROBP with\nerror \u03b4. By Theorem 5, there exist such generators with seed-length\nrs = O(r\u2032 + (log n)(log m) + log(m/\u03b4) \u00b7 (log m)) = O((log(n/\u03b4))(log log(n/\u03b4))3).\nNow, if we de\ufb01ne our \ufb01nal generator Gf : H \u00d7 {0, 1}rs \u2192{\u00b11}n by\nGf(h, z) = G\u2032(GINW (z), h).\nFrom the above arguments it follows that the output of Gf only has an additional \u03b4 error\ncompared to G\u2032. The theorem now follows from the above bound on seed-length.\n4\nA PRG for signed majorities\nIn this section we construct generator to fool signed majorities to polynomial error with\nseedlength \u02dcO(log n) proving Theorem 2. As the generator and its analysis is quite technical,\nwe \ufb01rst give a high-level description at the risk of repeating parts of Section 1.3.\n18\nProof overview\nFor simplicity, in this discussion let us \ufb01x a test vector v \u2208{\u22121, 0, 1}n and error \u01eb =\n1/poly(n). We start by noting that it su\ufb03ces to design a PRG G : {0, 1}r \u2192{\u00b11}n such\nthat dTV (\u27e8v, G(y)\u27e9, \u27e8v, X\u27e9) \u226a1/poly(n) where y \u2208u {0, 1}r and X \u2208u {\u00b11}n.\nIn the\nfollowing let X \u2208u {\u00b11}n and Y \u223cG(y), where y \u2208u {0, 1}r be the output of the desired\ngenerator.\nThe starting point of our analysis and construction is to note that showing closeness in\nstatistical distance for discrete random variables is equivalent to showing that the Fourier\ntransforms of the random variables are close. This will allow us to use various analytic\ntools. Concretely, we shall use the following elementary fact about the discrete Fourier\ntransform.\nClaim 2. Let Z1, Z2 be two discrete random variables with support sizes at most B. Then,\ndTV (Z1, Z2) \u2264\n\u221a\n2B \u00b7 max\n\u03b1\u2208R |E[exp(2\u03c0i\u03b1Z1)] \u2212E[exp(2\u03c0i\u03b1Z2)]| .\nProof. Note that the distribution Z1 \u2212Z2 is supported on at most 2B points. Therefore,\ndTV (Z1, Z2) = \u2225Z1 \u2212Z2\u22251 \u2264\n\u221a\n2B\u2225Z1 \u2212Z2\u22252.\nOn the other hand, the Plancherel identity implies that\n\u2225Z1 \u2212Z2\u22252 \u2264max\n\u03b1\u2208R |E[exp(2\u03c0i\u03b1Z1)] \u2212E[exp(2\u03c0i\u03b1Z2)]| .\nThis completes the proof.\nHenceforth, we will focus on designing a generator so as to fool the test function\nexp(2\u03c0i\u03b1 \u27e8v, x\u27e9) \u2261\u03c6v,\u03b1(x). To do so, we will consider two cases based on how large \u03b1 \u2208\n[0, 1] is. The two cases we consider capture the shift in the behaviour of E[exp(2\u03c0\u03b1(v \u00b7X))]\n- the \u201c\u03b1-th Fourier coe\ufb03cient\u201d. We can combine the generators for the two cases easily at\nthe end.\nLarge \u03b1: \u03b1 \u226b(log n)O(1)/\np\n\u2225v\u22250\nRoughly speaking, the reason for considering this threshold is that all values of \u03b1 greater\nthan this value yield similar Fourier coe\ufb03cients: |E[\u03c6v,\u03b1(X)]| \u226a1/poly(n) for \u03b1 in this\nrange. Thus, it su\ufb03ces to ensure that E[\u03c6v,\u03b1(Y )] is small. We achieve this by exhibiting a\nway to \u201camplify\u201d the error, i.e., go from fooling \u03c6v,\u03b1 with constant error to fooling them\nwith polynomially small error at the expense of a O(log log n) factor in seed-length. We\nthen instantiate this ampli\ufb01cation procedure with the generator of Gopalan, Meka, Rein-\ngold, Zuckerman [GMRZ13] which requires seed-length O(log n) to fool such test functions\n(\u03c6v,\u03b1( )) with constant error. We leave the details of the ampli\ufb01cation procedure to the\ncorresponding section.\n19\nSmall \u03b1: \u03b1 \u226a(log n)O(1)/\np\n\u2225v\u22250\nThis is the harder of the two cases and the core of our construction and analysis. The\ngenerator we use is essentially the same as the one based on iterative dimension reduction\nused in derandomizng the Cherno\ufb00bound. The main di\ufb00erence will be that instead of\nusing small-bias spaces in each dimension reduction step we use k-wise independent spaces\nfor suitable k. However, the analysis is quite di\ufb00erent and requires several new analytic\ntools.\nWe next formally describe our generator for handling this case. Let n, \u03b4 > 0. Let C be\na su\ufb03ciently large constant. We de\ufb01ne a generator as follows. Let n = n1 > n2 > . . . > nt\nso that ni+1 = n1/2\ni\n+ O(1) and log2C(n/\u03b4) \u2265nt \u2265logC(n/\u03b4).\nNote that this implies\nthat t = O(log log(n)).\nFor 1 \u2264i < t, let Hi = {h : [ni] \u2192[ni+1]} be a family of\nC log(n/\u03b4)\nlog(ni) -wise independent hash functions. Let hi \u2208u Hi. Let Zi be a random element\nof {\u00b11}ni chosen from a distribution that is both (\u03b4/n)C-biased and and C log(n/\u03b4)\nlog(ni) -wise\nindependent. Finally, let Z be a random variable in {\u00b11}nt be chosen to fool weight at\nmost n halfspaces to variational distance \u03b4/n as described in Theorem 4. We de\ufb01ne our\nrandom variable Y \u2208{\u00b11}n to be\nY = ZA(ht\u22121)D(Zt\u22121)A(ht\u22122)D(Zt\u22122) \u00b7 \u00b7 \u00b7 A(h1)D(Z1).\n(11)\nInformally, this generator begins with the string Z1, then uses h1 to divide the coordi-\nnates into n2 bins and then for each bin multiplies the elements in this bin by a random\nsign, these n2 signs being chosen recursively by a similar generator, until at the \ufb01nal level\nthey are picked using the generator from Theorem 4 instead.\nIt is easy to see from Theorem 4 and Fact 3 that the random variable Y can be produced\nfrom a random seed of length s = O(log(n/\u03b4) log log(n/\u03b4)). We also claim that it fools\n\u03c6v,\u03b1 for |\u03b1| \u2264log3(1/\u03b4)/\u2225v\u22252. This in turn implies our claimed pseudorandomness for\nhalfspaces in lieu of Claim 2.\nTo analyze the generator we shall use a hybrid argument to exploit the recursive nature\nof the generator. To this end, for 1 \u2264i < t, let Xi \u2208u {\u00b11}ni and de\ufb01ne\nYi := XiA(hi\u22121)D(Zi\u22121) \u00b7 \u00b7 \u00b7 A(h1)D(Z1)\n(12)\n(note that Y1 = X1) and let Yt = Y .\nThe crux of the analysis is then in showing the following claim analyzing a single\ndimension reduction step: for 1 \u2264i \u2264t and \u03b1 \u2264log3(1/\u03b4)/ \u2225v\u22252,\n|E[\u03c6v,\u03b1(Yi)] \u2212E[\u03c6v,\u03b1(Yi+1)]| \u2264\u03b4/n.\nIf we let v0 = v and vi = A(hi\u22121)D(Zi\u22121 \u00b7 \u00b7 \u00b7 A(h1)D(Z1)v, then the above claim amounts\nto bounding\n|E[\u03c6vi,\u03b1(Xi)] \u2212E[\u03c6vi,\u03b1(Xi\u22121A(hi)D(Zi))]| .\n(13)\n20\nThus, intuitively, we need to argue that a single step of dimension reduction (i.e.,\napplying A(hi)D(Zi)) does not cause too much error. Ideally, we would have liked to make\nsuch a claim for all test functions of the form \u03c6w,\u03b1; this turns out to be false.\nWhat\nremains true however is that a single dimension reduction step fools test functions of the\nform \u03c6w,\u03b1 when the test vector w \u2208Rni is su\ufb03ciently well-spread out (as measured by the\n\u21132, \u21134-norms of w) and \u03b1 is not too large. In particular, in the most technically intensive\npart of our argument we bound the error from the above step as a function of the \u21132, \u21134\nnorms of the vector vi. We then argue separately that the \u21132, \u21134 norms of the test vector v\nare close to their true values under the above transformations.\nIn order to analyze expectations as in Equation 13, it is critical to note that Xi\u22121 is\nuniformly distributed. This implies (for \ufb01xed hi) that the given expectation over Xi\u22121 is\na product of cosines of linear functions of Zi. We take advantage of the fact that cosine\nis a smooth function of its input, allowing us to approximate this product by a Taylor\npolynomial.\nIf \u03b1 is su\ufb03ciently small, the higher order terms will be small enough to\nignore, and therefore the limited independence of Zi will be su\ufb03cient to guarantee the\ndesired approximation.\n4.1\nGenerator for large \u03b1\nWe now develop a generator that works when \u03b1 is large, in particular, we prove:\nProposition 19. There exists an explicit generator Gb : {0, 1}r \u2192{\u00b11}n with seed-\nlength r = O((log(n/\u01eb))(log log(n))) such that the following holds. For all v \u2208{\u22121, 0, 1}n,\n\u03b1 \u2208(\u22121/4, 1/4) with \u03b1 \u2265log3(1/\u01eb)/ \u2225v\u22252,\n\f\f\f\f\nE\ny\u2208u{0,1}r\nh\n\u03c6v,\u03b1(Gb(y))\ni\n\u2212\nE\nX\u2208u{\u00b11}n [\u03c6v,\u03b1(X)]\n\f\f\f\f \u2264\u01eb.\n4.1.1\nSpreading hashes\nIn order to prove Proposition 19 we will need to study a certain property of hash families.\nDe\ufb01nition 3. A family of hash functions H = {h : [n] \u2192[m]} is said to be (k, \u2113, \u01eb)-\nspreading if the following holds: for every I \u2286[n] with |I| \u2265k, and h \u2208u H with probability\nat least 1 \u2212\u01eb, then for all j \u2208[m], |h\u22121(j) \u2229I| \u2264|I|/\u2113.\nThe above de\ufb01nition quanti\ufb01es the intuition that when a su\ufb03ciently large (so that\nstandard tail bounds apply) collection of items I \u2286[n] is hashed into m bins, the max-load\nis not much more than the average load of |I|/m. It will be important for us to be able to\nconstruct such families explicitly.\nLemma 20. For all \u01eb \u22650, there exists an explicit hash family H = {h : [n] \u2192[m]} where\nm = O(log5(1/\u01eb)) which is ((log5(1/\u01eb)), log(1/\u01eb), \u01eb)-spreading and h \u2208u H can be sampled\nwith O(log(n/\u01eb)) bits.\n21\nProof. Let m = \u0398(log5(1/\u01eb)) and let H = {h : [n] \u2192[m]} be a \u03b4-biased family for\n\u03b4 = exp(\u2212C(log(1/\u01eb))) for C a su\ufb03ciently large constant. We argue that H satis\ufb01es the\nconditions of the lemma by standard moment bounds.\nLet p = 2 log(1/\u01eb)/ log log(1/\u01eb)). Let |I| > log5(1/\u01eb) and let v \u2208{0, 1}n be the indicator\nvector of the set I. Note that if some h has |h\u22121(j) \u2229I| > |I|/ log(1/\u01eb) for some j, then\nh(v) \u2265|I|2/ log2(1/\u01eb) (recall the de\ufb01nition of h(v) from Equation 1). Therefore, by Lemma\n9 and Markov\u2019s inequality, the probability that this happens is at most\nE[h(v)p] log2p(1/\u01eb)\n|I|2p\n\u2264O\n\u0012p2 log2(1/\u01eb)\nm\n+ p2 log2(1/\u01eb)|I|\u22121\n\u0013p\n+ mp log2p(1/\u01eb)\u03b4\n= O(log(1/\u01eb))\u2212p + O(log(1/\u01eb))5p\u03b4\n\u2264\u01eb.\n4.1.2\nThe PRG\nWe begin with a simpler version of our generator which has the desired pseudorandomness\nproperty but has too large a seed. We will then improve the seed-length using PRGs for\nsmall-space machines.\nLet H = {h : [n] \u2192[m]} be a (k, C log(1/\u01eb), \u01eb)-spreading family for parameters k, C, \u01eb\nto be chosen later. Let GCS : {0, 1}r \u2192{\u00b11}n be a generator as in Theorem 4 with error\n1/4. Now, de\ufb01ne the generator Gb : H \u00d7 ({0, 1}r)m \u2192{\u00b11}n as follows: for i \u2208[n],\nGb(h, z1, z2, . . . , zm)i = GCS(zh(i))i.\n(14)\nWe claim that the above generator fools tests of the form \u03c6v,\u03b1( ) for \u2225v\u22250 \u2265k and\n\u03b1 \u226b\u221am/\u2225v\u22252.\nLemma 21. Let C be a su\ufb03ciently large constant. Let H = {h : [n] \u2192[m]} for some\nm \u2265log(1/\u01eb) be a (k, \u2113, \u01eb/4)-spreading family with \u2113= C log(1/\u01eb). Let GCS be a generator\nas in Theorem 4 with error 1/4. Let Y \u2208{\u00b11}n be the output of the generator Gb as\nde\ufb01ned in Equation (14) on a uniformly random seed and X \u2208u {\u00b11}n. Then, for all\nv \u2208{\u22121, 0, 1}n with \u2225v\u22250 \u2265k, and C\u221am/\u2225v\u22252 \u2264\u03b1 \u22641/4,\n|E[\u03c6v,\u03b1(Y )] \u2212E[\u03c6v,\u03b1(X)]| \u2264\u01eb.\nProof. Fix the test vector v \u2208{\u22121, 0, 1}n. Let I = Supp(v) and let |I| = K \u2265k. Let\nY = Gb(h, z1, z2, . . . , zm) and for j \u2208[m], let Y j = GCS(zj) and let Xj \u2208u {\u00b11}n be\nindependent uniformly random strings. Suppose that the hash function h \u2208u H is such\nthat the condition of (k, \u2113, \u01eb/4)-spreading holds for I.\nThis assumption only incurs an\nadditive \u01eb/2 in the error.\n22\nFirst note that,\nE[\u03c6v,\u03b1(X)] = (cos 2\u03c0\u03b1)K \u2264exp(\u2212\u2126(\u03b12K)) \u2264exp(\u2212Cm) \u2264\u01eb/4.\nThus, we need only show that\nE[\u03c6v,\u03b1(Y )] \u2264\u01eb/4.\nNow, for j \u2208[m] let vj = vh\u22121(j) and Kj = |I \u2229h\u22121(j)|. Observe that by de\ufb01nition,\ndTV (vj \u00b7 Y j, vj \u00b7 Xj) \u22641/4. Therefore,\n\f\fE[\u03c6vj,\u03b1(Y j)] \u2212E[\u03c6vj,\u03b1(Xj)]\n\f\f \u22641/2.\nFurther,\nE[\u03c6vj,\u03b1(Xj)] = (cos 2\u03c0\u03b1)Kj = exp(\u2212\u2126(\u03b12Kj)).\nCombining the above two equations, we get\n|E[\u03c6v,\u03b1(Y )]| =\n\f\f\f\f\f\nm\nY\ni=1\nE[\u03c6vj,\u03b1(Y j)]\n\f\f\f\f\f \u2264\nm\nY\ni=1\nmin\n\u0012\u00121\n2 + exp(\u2212\u2126(\u03b12Kj))\n\u0013\n, 1\n\u0013\n.\nNow, because h has the well-spreading property, Kj = |h\u22121(j) \u2229I| \u2264|I|/\u2113for all\nj \u2208[m]. On the other hand, P\nj Kj = K. Since the sum of the Kj which are at most\nK/(2m) totals at most K/2 and since none of the other Kj are too large, there must be\nat least \u2113/2 values of j so that Kj \u2265K/(2m). For these j we have that\n1\n2 + exp(\u2212\u2126(\u03b12Kj)) \u22641\n2 + exp(\u2212\u2126((C2m/K)(K/2m))) = 1\n2 + exp(\u2212\u2126(C)) \u22643\n4\nfor C su\ufb03ciently large. Thus, for C su\ufb03ciently large\n|E[\u03c6v,\u03b1(Y )]| \u2264\n\u00123\n4\n\u0013\u2113/2\n\u2264\u01eb/4.\nThis completes the proof.\nWe are now ready to prove Proposition 19.\nProof of Proposition 19. Let C be a su\ufb03ciently large constant, m = C log5(1/\u01eb), let H =\n{h : [n] \u2192[m]} be a (k, \u2113, \u01eb/4)-spreading family with k \u2264C log5(1/\u01eb), and \u2113= C log(1/\u01eb)\nas given in Lemma 20. Note that if 1/4 \u2265\u03b1 \u2265log3(1/\u01eb)/\u2225v\u22252 for some \u03b1, it must be the\ncase that \u2225v\u22250 \u2265log6(1/\u01eb) \u2265k. Therefore, Lemma 21 provides us with a generator, Gb,\nso that for any such \u03b1 that if Y is an output of Gb and X a uniform random element of\n{\u00b11}n and if \u2225v\u22250 \u2265k, then\n|E[\u03c6v,\u03b1(Y )] \u2212E[\u03c6v,\u03b1(X)]| \u2264\u01eb/2.\n23\nUnfortunately, the seed-length of Gb is log(H) + O(log n) \u00b7 m. We improve this using the\nPRGs for ROBPs of Theorem 5. It is easy to see that for a \ufb01xed hash function h and\ntest vector v, the computation of\n\nv, Gb(h, z1, . . . , zm)\n\u000b\ncan be done by a (S, D, m)-ROBP\nwhere S = O(log n) and D = O(log n). Thus, we can further derandomize the choice of\nz1, . . . , zm using the PRG from Theorem 5. Formally, let GINW : {0, 1}r \u2192\n\u0000{0, 1}D\u0001m be\na generator fooling (S, D, m)-ROBPs as in Theorem 5 with error \u01eb/4 and de\ufb01ne\nGf(h, z) = Gb(h, GINW (z)).\nThen, from the above arguments it follows that Gf fools \u03c6v,\u03b1 with error at most \u01eb and\nhas seed-length O(log(n/\u01eb) \u00b7 (log log(n/\u01eb))) proving the claim.\n4.2\nGenerator for small \u03b1\nWe next argue that the generator de\ufb01ned in Equation 11 fools Fourier coe\ufb03cients \u03c6v,\u03b1 for\nsu\ufb03ciently small \u03b1.\nThe main claim of this section is the following.\nProposition 22. Let v \u2208{\u22121, 0, 1}n and \u03b1 \u2208R with |\u03b1| \u2264log3(1/\u03b4)/\u2225v\u22252. Let C be a\nsu\ufb03ciently large constant and let \u03b4 > 0. Let Y be as de\ufb01ned by Equation (11) and let\nX \u2208u {\u00b11}n. Then\n|E[\u03c6v,\u03b1(Y )] \u2212E[\u03c6v,\u03b1(X)]| \u2264\u03b4.\nAs described in the overview, we will prove the claim by a hybrid argument. For ease\nof notation, we repeat some notation from the overview section. For 1 \u2264i < t, letting Xi\nbe a uniform random element of {\u00b11}ni we de\ufb01ne\nYi := XiA(hi\u22121)D(Zi\u22121) \u00b7 \u00b7 \u00b7 A(h1)D(Z1)\n(15)\n(note that Y1 = X1) and let Yt = Y . Our Proposition will follow from the following Lemma.\nLemma 23. With Yi de\ufb01ned as above for C su\ufb03ciently large and v \u2208{\u22121, 0, 1}n and\n\u03b1 \u2208R with |\u03b1| \u2264log3(1/\u03b4)/\u2225v\u22252, then for t > i \u22651\n|E[\u03c6v,\u03b1(Yi+1)] \u2212E[\u03c6v,\u03b1(Yi)]| \u2264\u03b4/n.\nThe proof of the Lemma 23 will be further split into two main cases based upon whether\nor not the vector v is sparse relative to ni. Intuitively, the case of sparse v is easier as\nhashing takes care of most issues here.\n4.2.1\nAnalysis for sparse vectors\nWe begin with the case where v is sparse.\nLemma 24. With Yi, C, n, v, \u03b1, \u03b4 as in Lemma 23 with i < t, if \u2225v\u22253\n0 < ni+1 then\n|E[\u03c6v,\u03b1(Yi+1)] \u2212E[\u03c6v,\u03b1(Yi)]| \u2264\u03b4/n.\n24\nProof. We claim that this holds even after \ufb01xing the values of hj, Zj for all j < i. In\nparticular, if we let\nw = vD(X1)A(h1)T \u00b7 \u00b7 \u00b7 D(hi\u22121)A(hi\u22121)T\nthen we need to show that\n|E[\u03c6w,\u03b1(Xi+1A(hi)D(Zi))] \u2212E[\u03c6w,\u03b1(Xi)]| \u2264\u03b4/n.\nWe will show the stronger claim that\ndTV (w \u00b7 Xi+1A(hi)D(Zi), w \u00b7 Xi) \u2264\u03b4/(2n).\nIntuitively, this will hold because v (and hence w) is sparse. This means that with high\nprobability hi will cause few collisions within the support of w. If this is the case, then Zi\nwill nearly randomize the relative signs of elements mapped to the same bin and Xi will\nrandomize the signs between bins. To show that we have few collisions, we will need the\nfollowing lemma:\nLemma 25. Let n and m be positive integers, \u01eb > 0 and C a su\ufb03ciently large constant. Let\nH = {h : [n] \u2192[m]} be a k-wise independent family of hash functions for k = C log(m/\u01eb)\nlog(m)\n.\nLet I \u2282[n] be such that |I|3 \u2264m. Then for h \u2208u H, with probability at least 1 \u2212\u01eb we have\nthat\n|I| \u2212|h(I)| \u2264k.\nProof. Note that if |I| \u2212|h(I)| > k then at least k elements of I were sent to the same\nlocation as some other element of I. This implies that there must be at least k/3 disjoint\npairs of elements xi, yi \u2208I so that h(xi) = h(yi) (for each element j \u2208[m] so that\n|h\u22121(j)| = \u2113> 1 we can \ufb01nd at least \u2113/3 pairs).\nThus, it su\ufb03ces to show that the\nexpected number of collections of distinct elements x1, y1, x2, y2, . . . , xk/3, yk/3 \u2208I so that\nh(xi) = h(yi) for each i is less than \u01eb. On the other hand, the number of sequences xi, yi \u2208I\nis at most |I|2k/3 and the probability that any given sequence has the desired property is\nm\u2212k/3 by k-wise independence of h. Thus the expected number of such sets of pairs is at\nmost\n|I|2k/3m\u2212k/3 \u2264m2k/9m\u2212k/3 = m\u2212k/9 \u2264\u01eb.\nThis completes the proof.\nApplying this lemma to I = supp(w), we \ufb01nd that except with probability \u03b4/(4n) we\nhave that at most log(n/\u03b4) elements of I collide with any other element of I under hi. Let\nJ be the set of such coordinates. It is clear that the distribution of w \u00b7 (Xi+1A(hi)D(Zi))\nas we vary Xi+1 depends only on hi and the signs of the Zi on the coordinates of J. On\nthe other hand, it is easy to see that the restriction of Zi to these coordinates is within\n2|J|(\u03b4/n)C < \u03b4/(4n) of uniform. Thus,\n\u03b4/(2n) \u2265dTV (w\u00b7(Xi+1A(hi)D(Zi)), w\u00b7(Xi+1A(hi)D(Xi))) = dTV (w\u00b7Xi+1A(hi)D(Zi), w\u00b7Xi).\n25\nWhere the equality above is because (Xi+1A(hi)D(Xi)) and Xi are identically distributed.\nThis completes the proof.\n4.2.2\nAnalysis for dense vectors\nFor relatively dense vectors v, we will need a di\ufb00erent, more analytic approach.\nThe\nfollowing crucial lemma analyzes the e\ufb00ect of a single dimension reduction step and bounds\nthe error in terms of the norms of the test vector v.\nWe will then apply the lemma\niteratively.\nLemma 26. Let \u03b4 > 0, n, m \u22651, and p \u22652 and even integer.\nLet D be a 2p-wise\nindependent distribution over {\u00b11}n and H = {h : [n] \u2192[m]} be a 2p-wise independent\nhash family. Then, for all v \u2208Rn, X \u2208u {\u00b11}n,Y \u223cD, h \u2208u H and Z \u2208u {\u00b11}m,\n| E[\u03c6v,\u03b1(Z \u00b7 A(h) \u00b7 D(Y ))] \u2212E[\u03c6v,\u03b1(Z \u00b7 A(h) \u00b7 D(X))]| < O(p)2p\n\u0012\u03b14\u2225v\u22254\n2\nm\n+ \u03b14\u2225v\u22254\n4\n\u0013p/8\n.\n(16)\nTo prove the lemma we shall exploit the independence of the zi\u2019s in Equation (18) to\nreduce the problem to that of analyzing a product of cosines as in the following lemma.\nThe lemma gives a low-degree (multivariate) polynomial approximation for a product of\ncosines.\nLemma 27. For all \u03b1 \u2208(0, 1/4) and even integer p, there exists a polynomial P : Rm \u2192R\nof degree at most p such that for all S1, . . . , Sm, T \u2208R,\nm\nY\nj=1\ncos(2\u03c0\u03b1Sj) = exp(\u22122\u03c02\u03b12T) \u00b7\n\uf8eb\n\uf8ed\np/2\u22121\nX\nt=0\n\u0000\u22122\u03c02\u03b12 \u0000Pm\ni=1 S2\ni \u2212T\n\u0001\u0001t\nt!\nP(Si)\n\uf8f6\n\uf8f8\n(17)\n+ O(1)p\n\uf8eb\n\uf8ed\n \n\u03b12\n m\nX\ni=1\nS2\ni \u2212T\n!!p/2\n+\n \n\u03b12\n m\nX\ni=1\nS2\ni \u2212T\n!!p\n+\n m\nX\ni=1\n(\u03b1Si)4\n!p/8\n+\n m\nX\ni=1\n(\u03b1Si)4\n!p/2\uf8f6\n\uf8f8.\nProof of Lemma 26. Let Y s = ZA(h)D(Y ). We \ufb01rst \ufb01x a hash function h \u2208H and then\nbound the error as a function of the hash function. We then average the error bound for\na uniformly random hash function from H using Lemma 9.\nFor j \u2208[m], let random variable Sj = P\ni:h(i)=j viYi. Note that \u27e8v, Y s\u27e9= Pm\nj=1 zjSj.\nTherefore, as z \u2208u {\u00b11}m,\nE\nz [\u03c6v,\u03b1(Y s)] =\nm\nY\nj=1\ncos(2\u03c0\u03b1Sj).\n26\nLet T = P\nj E[S2\nj ] = \u2225v\u22252\n2.\nLet Q(X) \u2261R(S1, . . . , Sm) denote the degree 2p polyno-\nmial corresponding in the \ufb01rst term of Equation (17), and let E(X) be the error term\ncorresponding to the second term. Then, from the above calculations,\nE\nz [\u03c6v,\u03b1(Xs)] = Q(X) + E(X).\nObserve that\nQ2(X) := \u03b12(\nm\nX\nj=1\nS2\nj \u2212T) = \u03b12\nm\nX\nj=1\nX\ni\u0338=i\u2032\u2208h\u22121(j)\nvivi\u2032XiXi\u2032,\n(18)\nis a degree two polynomial in X with, \u2225Q2\u22252\n2 \u2264\u03b14h(v) (recall Equation (1)).\nBy hypercontractivity - Lemma 6, for all even r \u2264p,\nE[Q2(X1, . . . , Xn)r] \u2264O(r)r \u0000\u03b14h(v)\n\u0001r/2 .\nA similar calculation for the polynomial Q4(X1, . . . , Xn) := \u03b14(P\nj S4\nj ) shows that for\nall even r \u2264p/2,\nE[Q4(X1, . . . , Xn)r] \u2264O(r)2r \u0000\u03b14h(v)\n\u0001r .\nBy 2p-wise independence, the above bounds also hold for E[Q2(Y )r], E[Q4(Y )r].\nNow, let Xs = ZA(h)D(X), where X \u2208u {\u00b11}n. Then, clearly Xs \u2208u {\u00b11}n. Com-\nbining the above expressions and noting that they also work for X \u2208u {\u00b11}n, we get\n\f\f\f\fE\nX E\nz [\u03c6v,\u03b1(Xs)] \u2212E\nY E\nz [\u03c6v,\u03b1(Y s)]\n\f\f\f\f \u2264|E [Q(X) \u2212Q(Y )]| + E[|E(X)|] + E[|E(Y )|]\n\u22640 + O(p)p \u0000\u03b14h(v)\n\u0001p/4 + O(p)p \u0000\u03b14h(v)\n\u0001p/8\n\u2264O(p)p \u0000\u03b14h(v)\n\u0001p/8 .\nBy taking expectation with respect to h \u2208u H and applying Lemma 9, we get\n|E[\u03c6v,\u03b1(Xs)] \u2212E[\u03c6v,\u03b1(Y s)]| \u2264O(p)2p\n\u0012\u03b14\u2225v\u22254\n2\nm\n+ \u03b14\u2225v\u22254\n4\n\u0013p/8\n,\nproving the lemma.\nWe defer the proof of Lemma 27 to Section 4.2.5 and continue with the analysis of our\ngenerator. We do so by applying Lemma 26 iteratively to the vectors\nvi := vD(Z1)A(h1)T \u00b7 \u00b7 \u00b7 D(Zi\u22121)A(hi\u22121)T .\nIn order for it to be useful, we need to have good bounds on the low order moments of the\nvi. We deal with these issues in the next section.\n27\n4.2.3\nControlling moments\nIn particular we will need the following Lemma:\nLemma 28. Let v \u2208{\u22121, 0, 1}n with \u2225v\u22250 \u2265logC/4(n/\u03b4). Let Zi, hi, vi be de\ufb01ned as above.\nFor any 1 \u2264i \u2264t we have with probability at least 1 \u2212\u03b4/(4n) that\n\u2225vi\u22252 \u22642i\u2225v\u22252\nand\n\u2225vi\u22254 \u2264\n\u2225v\u22252\nmin(\u2225v\u22251/3\n2\n, n1/20\ni\n)\n.\nIn order to prove this we will \ufb01rst need some controls over how the procedure used to\nobtain vi+1 from vi a\ufb00ects these norms. In particular, we show:\nLemma 29. Let p \u22652 be an even integer.\nLet H = {h : [n] \u2192[m]} be a 4p-wise\nindependent hash family and D be a 4p-wise independent distribution over {\u00b11}n. Then,\nfor h \u2208u H, x \u223cD and a vector v \u2208Rn,\nE\nh\u0010\n\u2225v\u22252\n2 \u2212\n\r\rvD(x)A(h)T \r\r2\n2\n\u0011pi\n\u2264O(p)2p\n\u0012\u2225v\u22254\n2\nm\n\u0013p/2\n+ O(p)2p\u2225v\u22252p\n4 .\nSimilarly,\nE[\n\r\rvD(x)A(h)T \r\r4p\n4 ] \u2264O(p)4p\n\u0012\u2225v\u22254\n2\nm\n\u0013p\n+ O(p)4p\u2225v\u22254p\n4 .\nProof. Note that in either case the independence is su\ufb03cient that the expectations would be\nthe same if x and h were chosen uniformly at random from {\u00b11}n and [m][n], respectively.\nApplying Lemma 6 to the polynomial Ph(x) = \u2225vD(x)A(h)T \u22252\n2 \u2212\u2225v\u22252\n2, we \ufb01nd that for\n\ufb01xed h\nE\nx[Ph(x)p] \u2264O(p)ph(v)p/2.\nAveraging over h and applying Lemma 9 yields the \ufb01rst line.\nApplying Lemma 6 to the polynomial Qh(x) = \u2225vD(x)A(h)T \u22254\n4, we \ufb01nd that for \ufb01xed\nh,\nE\nx[\n\r\rvD(x)A(h)T \r\r4p\n4 ] \u2264O(p)2ph(v)p.\nTaking an expectation over h and applying Lemma 9, we get that\nE[\n\r\rvD(x)A(h)T \r\r4p\n4 ] \u2264O(p)4p\n\u0012\u2225v\u22254\n2\nm\n\u0013p\n+ O(p)4p\u2225v\u22254p\n4 .\nThis completes the proof.\nWe are now prepared to prove Lemma 28.\n28\nProof of Lemma 28. We proceed by induction on i proving that the desired inequalities\nhold with probability at least 1 \u2212i(\u03b4/n)2. As a base case we consider i so that \u2225v\u22253\n0 \u2264ni.\nIn this case, by repeated application of Lemma 25, we \ufb01nd that with at least the desired\nprobability that \u2225v\u22250 \u2212\u2225vi\u22250 \u2264i log(n/\u03b4). This implies that other than its zero coe\ufb03cients,\nvi has \u2225v\u22250 \u22122i log(n/\u03b4) coe\ufb03cients of norm 1, and at most i log(n/\u03b4) other coe\ufb03cients\neach of norm at most i log(n/\u03b4). This means that\n\u2225vi\u22252\n2 = \u2225v\u22252\n2 + O(i3 log3(n/\u03b4)),\nand\n\u2225vi\u22254\n4 = \u2225v\u22252\n2 + O(i5 log5(n/\u03b4)).\nOur bounds follow immediately.\nOtherwise, for \u2225v0\u22256 > ni, we proceed by induction on i. As a base case, note that the\ndesired inequalities hold for i = 1 as v1 = v, and \u2225v\u22254 =\np\n\u2225v\u22252. We claim that if \u2225vi\u2225\nsatis\ufb01es the desired inequalities, then vi+1 also does with probability at least 1 \u2212(\u03b4/n)2.\nNote that vi+1 = viD(Zi)A(hi)T . Note also that Zi and hi are k-wise independent for\nk = C log(n/\u03b4)/ log(ni). Applying Lemma 29 with p = \u230ak/4\u230b, we \ufb01nd that\nE\nh\u0000\u2225vi\u22252\n2 \u2212\u2225vi+1\u22252\n2\n\u00012pi\n\u2264O(p)4p\n\u0012\u2225vi\u22254\n2\nni+1\n\u0013p\n+ O(p)4p\u2225vi\u22254p\n4 ,\nand\nE[\u2225vi+1\u22254p\n4 ] \u2264O(p)4p\n\u0012\u2225vi\u22254\n2\nni+1\n\u0013p\n+ O(p)4p\u2225vi\u22254p\n4 .\nApplying the Markov bound to the \ufb01rst of these equations we \ufb01nd that the probability\nthat \u2225vi+1\u22252\n2 \u2265\u2225vi\u22252\n2 + 4i\u2225v\u22252\n2 is at most\n\u0012O(p4)\nni+1\n\u0013p\n+ O\n\u0012p\u2225vi\u22254\n\u2225v\u22252\n\u00134p\n\u2264n\u2212p/2\ni+1 + O(pn\u22121/10\ni+1\n)4p\n\u2264n\u2212p/2\ni+1 + n\u2212p/11\ni+1\n\u2264(\u03b4/n)2/2.\nWhere the \ufb01rst inequality above is by the inductive hypothesis. This implies that \u2225vi+1\u22252 \u2264\n2i+1\u2225v\u22252 with the desired probability.\nApplying the Markov bound to the latter of these equations we \ufb01nd that the probability\nthat \u2225vi+1\u22254 > \u2225v\u22252/n1/20\ni+1 is at most\nO\n \np4\u2225vi\u22254\n2\n\u2225v\u22254\n2n4/5\ni+1\n+ p4\u2225vi\u22254\n4n1/5\ni+1\n\u2225v2\u22254\n2\n!p\n\u2264O\n \nn\u22121/2\ni\n+ n1/5\ni+1\nn1/5\ni\n!p\n\u2264O(n\u22121/10\ni\n)p \u2264(\u03b4/n)2/2.\nWhere above we use that\n\u2225vi\u22254 \u2264\n\u2225v\u22252\nmin(\u2225v\u22251/3\n2\n, n1/20\ni\n)\n= \u2225v\u22252\nn1/20\ni\n.\nThus, with the desired probability \u2225vi+1\u22254 \u2264\u2225v\u22252/n1/20\ni\n. This completes the inductive\nstep, and \ufb01nishes the proof.\n29\n4.2.4\nCombined analysis\nWe are now ready to prove Lemma 23.\nProof of Lemma 23. First note that if i = t \u22121, the lemma follows immediately from the\npseudorandomness properties of Z. We thus consider only i < t \u22121.\nWe note that v \u00b7 Yi = vi \u00b7 Xi and v \u00b7 Yi+1 = vi \u00b7 Xi+1A(hi)D(Zi). If \u2225v\u22253\n0 \u2264ni+1, we\nare done by Lemma 24. Otherwise, assume that \u2225v\u22253\n0 > ni+1. By Lemma 28 we have that\nexcept for an event of probability \u03b4/(4n) we have that\n\u2225vi\u22252 \u2264log(n)\u2225v\u22252\nand\n\u2225vi\u22254 \u2264\u2225v\u22252n\u22121/20\ni\n.\nBy ignoring the possibility that these are violated, we introduce an error of at most \u03b4/(2n),\nthus it su\ufb03ces to only consider the case where the choice of h1, Z1, . . . , hi\u22121, Zi\u22121 are such\nthat the above holds. We now need to bound\n|E[\u03c6vi,\u03b1(Xi)] \u2212E[\u03c6vi,\u03b1(Xi+1A(hi)D(Zi))]| .\nSince Xi has the same distribution as Xi+1A(hi)D(Xi), we may apply Lemma 26 that for\np = \u2126\n\u0010\nC log(n/\u03b4)\nlog(ni)\n\u0011\nthat the above is bounded by\nO(p)2p\n\u0012\u03b14\u2225vi\u22254\n2\nni+1\n+ \u03b14\u2225vi\u22254\n4\n\u0013p/8\n\u2264O(p)2p \u0010\n24i log12(1/\u03b4)n\u22121/2\ni\n+ log12(1/\u03b4)n\u22121/5\ni\n\u0011p/8\n\u2264\n\u0010\nlog52(n/\u03b4)n\u22121/5\ni\n\u0011p/8\n\u2264n\u2212p/50\ni\n\u2264\u03b4/(2n).\nThis completes the proof.\nProposition 22 now follows immediately after noting that\n|E[\u03c6v,\u03b1(X)] \u2212E[\u03c6v,\u03b1(Y )]| \u2264\nt\u22121\nX\ni=1\n|E[\u03c6v,\u03b1(Yi)] \u2212E[\u03c6v,\u03b1(Yi+1)]| .\n4.2.5\nApproximating a product of cosines\nHere we prove Lemma 27.\n30\nProof of Lemma 27. Note that so long as \u03b1Si < 1/10 for all i that by Taylor expansion we\nhave that\nm\nY\ni=1\ncos(2\u03c0\u03b1Si)\n= exp\n\uf8eb\n\uf8ed\u22122\u03c02\u03b12\nm\nX\ni=1\nS2\ni +\np/2\u22121\nX\nj=2\n \ncj\nm\nX\ni=1\n(\u03b1Si)2j\n!\n+\nm\nX\ni=1\nO(\u03b1Si)p\n\uf8f6\n\uf8f8\n= exp(\u22122\u03c02\u03b12T) exp\n\uf8eb\n\uf8ed\u22122\u03c02\u03b12\n m\nX\ni=1\nS2\ni \u2212T\n!\n+\np/2\u22121\nX\nj=2\n \ncj\nm\nX\ni=1\n(\u03b1Si)2j\n!\n+\nm\nX\ni=1\nO(\u03b1Si)p\n\uf8f6\n\uf8f8\nwhere the cj are constants obtained from the Taylor expansion of log(cos(z)). Furthermore,\nsince log(cos(z)) is analytic in a disk around z = 0, we have that cj = O(1)j. Note by\nconditioning on whether or not Pm\ni=1 O(\u03b1Si)p is more than 1, we \ufb01nd that the above is\nequal to\nexp(\u22122\u03c02\u03b12T) exp\n\uf8eb\n\uf8ed\u22122\u03c02\u03b12\n m\nX\ni=1\nS2\ni \u2212T\n!\n+\np/2\u22121\nX\nj=2\n \ncj\nm\nX\ni=1\n(\u03b1Si)2j\n!\uf8f6\n\uf8f8\n \n1 +\nm\nX\ni=1\nO(\u03b1Si)p\n!\n+\nm\nX\ni=1\nO(\u03b1Si)p.\nFor each j, let pj be the ceiling of p/(2j). Note that p \u22642j \u00b7 pj \u22642p. Under the additional\nassumption that \u03b12 \u0000Pm\ni=1 S2\ni \u2212T\n\u0001\n, Pm\ni=1(\u03b1Si)4 < a, for some su\ufb03ciently small constant\na we have that the above is equal to\nexp(\u22122\u03c02\u03b12T) \u00b7\n p2\u22121\nX\nt=0\n\u0000\u22122\u03c02\u03b12 \u0000Pm\ni=1 S2\ni \u2212T\n\u0001\u0001t\nt!\n!\n\u00b7\n \n1 + O\n \n\u03b12\n m\nX\ni=1\nS2\ni \u2212T\n!!p2!\n\u00b7\np/2\u22121\nY\nj=2\n\uf8eb\n\uf8ed\npj\u22121\nX\nt=0\n\u0000cj\nPm\ni=1(\u03b1Si)2j\u0001t\nt!\n\uf8f6\n\uf8f8\u00b7\n \n1 + O(1)p\n m\nX\ni=1\n(\u03b1Si)2j\n!pj!\n\u00b7\n \n1 +\nm\nX\ni=1\nO(\u03b1Si)p\n!\n+\nm\nX\ni=1\nO(\u03b1Si)p\n= exp(\u22122\u03c02\u03b12T) \u00b7\n p2\u22121\nX\nt=0\n\u0000\u22122\u03c02\u03b12 \u0000Pm\ni=1 S2\ni \u2212T\n\u0001\u0001t\nt!\n!\n\u00b7\np/2\u22121\nY\nj=2\n\uf8eb\n\uf8ed\npj\u22121\nX\nt=0\n\u0000cj\nPm\ni=1(\u03b1Si)2j\u0001t\nt!\n\uf8f6\n\uf8f8\n+ O(1)p\n\uf8eb\n\uf8ed\n \n\u03b12\n m\nX\ni=1\nS2\ni \u2212T\n!!p2\n+\n m\nX\ni=1\n(\u03b1Si)4\n!p/4\n+\nm\nX\ni=1\n(\u03b1Si)p\n\uf8f6\n\uf8f8\n31\nNext consider the above term\np/2\u22121\nY\nj=2\n\uf8eb\n\uf8ed\npj\u22121\nX\nt=0\n\u0000cj\nPm\ni=1(\u03b1Si)2j\u0001t\nt!\n\uf8f6\n\uf8f8.\nLet it equal P(Si) + E(Si) where P is the polynomial consisting of all the terms of total\ndegree at most p. We note that for any j that cj\nPm\ni=1(\u03b1Si)2j is at most\nO(a)j/4\n m\nX\ni=1\n(\u03b1Si)4\n!j/4\n.\nTherefore, |E(Si)| is at most\n\u0000Pm\ni=1(\u03b1Si)4\u0001p/8 times the sum of the degree more than p\ncoe\ufb03cients in the Taylor expansion of\nexp\n\u0012\n1\n1 \u2212O(a1/8z)\n\u0013\n.\nFor a su\ufb03ciently small, the above has radius of convergence more than 1, and thus the\nsum of the degree more than p terms is bounded. Thus, E(Si) is\nO\n m\nX\ni=1\n(\u03b1Si)4\n!p/8\n.\nTherefore, assuming that \u03b1Si < 1/10 for all i, and \u03b12 \u0000Pm\ni=1 S2\ni \u2212T\n\u0001\n, Pm\ni=1(\u03b1Si)4 < a,\nthen\nm\nY\ni=1\ncos(2\u03c0\u03b1Si)\nequals\nexp(\u22122\u03c02\u03b12T) \u00b7\n p2\u22121\nX\nt=0\n\u0000\u22122\u03c02\u03b12 \u0000Pm\ni=1 S2\ni \u2212T\n\u0001\u0001t\nt!\n!\nP(Si)\n+ O(1)p\n\uf8eb\n\uf8ed\n \n\u03b12\n m\nX\ni=1\nS2\ni \u2212T\n!!p2\n+\n m\nX\ni=1\n(\u03b1Si)4\n!p/8\u22121\n+\nm\nX\ni=1\n(\u03b1Si)p\n\uf8f6\n\uf8f8.\nOn the other hand, if the stated assumptions fail, the main term above is bounded by a\npolynomial in \u03b12 \u0000Pm\ni=1 S2\ni \u2212T\n\u0001\nand Pm\ni=1(\u03b1Si)4 with total degree at most 2p and sum of\ncoe\ufb03cients O(1)p. Therefore, under no additional assumptions we have that\nm\nY\ni=1\ncos(2\u03c0\u03b1Si)\n32\nequals\nexp(\u22122\u03c02\u03b12T) \u00b7\n p2\u22121\nX\nt=0\n\u0000\u22122\u03c02\u03b12 \u0000Pm\ni=1 S2\ni \u2212T\n\u0001\u0001t\nt!\n!\nP(Si)\n+ O(1)p\n\uf8eb\n\uf8ed\n \n\u03b12\n m\nX\ni=1\nS2\ni \u2212T\n!!p/2\n+\n \n\u03b12\n m\nX\ni=1\nS2\ni \u2212T\n!!p\n+\n m\nX\ni=1\n(\u03b1Si)4\n!p/8\n+\n m\nX\ni=1\n(\u03b1Si)4\n!p/2\uf8f6\n\uf8f8.\nThe claim now follows.\n4.3\nFinal analysis\nWe can \ufb01nally state our main generator and prove Theorem 2.\nProof of Theorem 2. Let Y1, Y2 be the generators from Propositions 19 and 22 for \u03b4 = \u01eb/6n.\nLet Y be the the coordinate-wise product of the strings Y1, Y2. We claim that for any\nv \u2208{\u22121, 0, 1}n and X \u2208u {\u00b11}n,\ndTV (v \u00b7 X, v \u00b7 Y ) \u2264\u01eb.\n(19)\nThe theorem follows immediately from the above claim and the bounds on the seed-lengths\nfrom Propositions 19 and 22.\nTo prove the theorem, we \ufb01rst prove that for all \u03b1 \u2208R,\n|E[\u03c6v,\u03b1(X)] \u2212E[\u03c6v,\u03b1(Y )]| \u2264\u01eb/(2n).\nNow, if log3(1/\u03b4)/ \u2225v\u22252 \u2264\u03b1, then\nE[\u03c6v,\u03b1(Y )] = E[\u03c6D(Y2)v,\u03b1(Y1)]\nand\nE[\u03c6v,\u03b1(X)] = E[\u03c6v,\u03b1(D(Y2)X)] = E[\u03c6D(Y2)v,\u03b1(X)].\nHowever by Proposition 19, we have that\n\f\fE[\u03c6D(Y2)v,\u03b1(Y1)] \u2212E[\u03c6D(Y2)v,\u03b1(X)]\n\f\f \u2264\u01eb/(3n).\nSimilarly, if \u03b1 \u2264log3(1/\u03b4)/ \u2225v\u22252, then then note that\nE[\u03c6v,\u03b1(Y )] = E[\u03c6D(Y1)v,\u03b1(Y2)]\nand\nE[\u03c6v,\u03b1(X)] = E[\u03c6v,\u03b1(D(Y1)X)] = E[\u03c6D(Y1)v,\u03b1(X)].\n33\nHowever by Proposition 22, we have that\n\f\fE[\u03c6D(Y1)v,\u03b1(Y2)] \u2212E[\u03c6D(Y1)v,\u03b1(X)]\n\f\f \u2264\u01eb/(3n).\nThus, we have our result for all \u03b1 \u2208[0, 1/4]. Noting that \u03c6v,\u2212\u03b1(X) = \u03c6v,\u03b1(X), we determine\nthat the statement in question holds for \u03b1 if and only if it holds for \u2212\u03b1.\nThus, the\ninequality in question holds for all \u03b1 \u2208[\u22121/4, 1/4]. Next, note that for any X \u2208{\u00b11}n that\n\u03c6v,\u03b1+1/2(X) = exp(\u03c0iv \u00b7 X)\u03c6v,\u03b1(X) = (\u22121)\u2225v\u22250\u03c6v,\u03b1(X). Thus, the statement in question\nholds for \u03b1 if and only if it holds for \u03b1 + 1/2. Thus, it holds for all real \u03b1. Equation 19\nnow follows from the above argument and Claim 2 applied to Z1 = \u27e8v, X\u27e9and Z2 = \u27e8v, Y \u27e9.\nReferences\n[BO10]\nEric Blais and Ryan O\u2019Donnell. Lower bounds for testing function isomorphism.\nIn Computational Complexity (CCC), 2010 IEEE 25th Annual Conference on,\npages 235\u2013246. IEEE, 2010.\n[BRRY14] Mark Braverman, Anup Rao, Ran Raz, and Amir Yehudayo\ufb00. Pseudorandom\ngenerators for regular branching programs. SIAM J. Comput., 43(3):973\u2013986,\n2014.\n[BV10]\nJoshua Brody and Elad Verbin.\nThe coin problem and pseudorandomness\nfor branching programs.\nIn 51th Annual IEEE Symposium on Foundations\nof Computer Science, FOCS 2010, October 23-26, 2010, Las Vegas, Nevada,\nUSA, pages 30\u201339, 2010.\n[CRSW13] L. Elisa Celis, Omer Reingold, Gil Segev, and Udi Wieder. Balls and bins:\nSmaller hash families and faster evaluation. SIAM J. Comput., 42(3):1030\u2013\n1050, 2013.\n[De11]\nAnindya De. Pseudorandomness for permutation and regular branching pro-\ngrams. In Proceedings of the 26th Annual IEEE Conference on Computational\nComplexity, CCC 2011, San Jose, California, June 8-10, 2011, pages 221\u2013231,\n2011.\n[De14]\nAnindya De. Beyond the central limit theorem: asymptotic expansions and\npseudorandomness for combinatorial sums, 2014. ECCC, TR14-125.\n[DGJ+09]\nIlias Diakonikolas, Parikshit Gopalan, Ragesh Jaiswal, Rocco A. Servedio, and\nEmanuele Viola. Bounded independence fools halfspaces. In Proceedings of the\n50th Annual IEEE Symposium on Foundations of Computer Science (FOCS\n\u201909), 2009.\n34\n[DKN10]\nIlias Diakonikolas, Daniel Kane, and Jelani Nelson.\nBounded independence\nfools degree-2 threshold functions. In Proceedings of the 51st Annual IEEE\nSymposium on Foundations of Computer Science (FOCS \u201910), 2010.\n[Fel71]\nWilliam Feller. An Introduction to Probability Theory and Its Applications,\nVol. 2 (Volume 2). Wiley, New York, London, Sydney, 2nd edition, 1971.\n[GMR+12] Parikshit Gopalan, Raghu Meka, Omer Reingold, Luca Trevisan, and Salil P.\nVadhan. Better pseudorandom generators from milder pseudorandom restric-\ntions. In 53rd Annual IEEE Symposium on Foundations of Computer Science,\nFOCS 2012, New Brunswick, NJ, USA, October 20-23, 2012, pages 120\u2013129,\n2012.\n[GMRZ13] Parikshit Gopalan, Raghu Meka, Omer Reingold, and David Zuckerman. Pseu-\ndorandom generators for combinatorial shapes. SIAM J. Comput., 42(3):1051\u2013\n1076, 2013.\n[GNR14]\nParikshit Gopalan, Noam Nisan, and Tim Roughgarden, 2014. Manuscript in\npreparation.\n[GOWZ10] Parikshit Gopalan, Ryan O\u2019Donnell, Yi Wu, and David Zuckerman. Fooling\nfunctions of halfspaces under product distributions.\nIn 25th Annual IEEE\nConference on Computational Complexity, pages 223\u2013234, 2010.\n[GR09]\nParikshit Gopalan and Jaikumar Radhakrishnan. Finding duplicates in a data\nstream. In Proceedings of the Twentieth Annual ACM-SIAM Symposium on\nDiscrete Algorithms, SODA 2009, New York, NY, USA, January 4-6, 2009,\npages 402\u2013411, 2009.\n[HKM12]\nPrahladh Harsha, Adam Klivans, and Raghu Meka. An invariance principle\nfor polytopes. J. ACM, 59(6):29, 2012.\n[INW94]\nRussell Impagliazzo, Noam Nisan, and Avi Wigderson. Pseudorandomness for\nnetwork algorithms. In Proceedings of the Twenty-Sixth Annual ACM Sympo-\nsium on Theory of Computing, 23-25 May 1994, Montr\u00b4eal, Qu\u00b4ebec, Canada,\npages 356\u2013364, 1994.\n[IW97]\nRussell Impagliazzo and Avi Wigderson. P = BPP if E requires exponential\ncircuits: Derandomizing the XOR lemma. In STOC, pages 220\u2013229, 1997.\n[Kan11a]\nDaniel M. Kane. k-independent gaussians fool polynomial threshold functions.\nIn Proceedings of the 26th Annual IEEE Conference on Computational Com-\nplexity, CCC 2011, San Jose, California, June 8-10, 2011, pages 252\u2013261,\n2011.\n35\n[Kan11b]\nDaniel M. Kane. A small PRG for polynomial threshold functions of gaussians.\nIn IEEE 52nd Annual Symposium on Foundations of Computer Science, FOCS\n2011, Palm Springs, CA, USA, October 22-25, 2011, pages 257\u2013266, 2011.\n[Kan14]\nDaniel M. Kane. A pseudorandom generator for polynomial threshold functions\nof gaussian with subpolynomial seed length.\nIn IEEE 29th Conference on\nComputational Complexity, CCC 2014, Vancouver, BC, Canada, June 11-13,\n2014, pages 217\u2013228, 2014.\n[KM14]\nPravesh Kothari and Raghu Meka. Almost-optimal pseudorandom generators\nfor spherical caps, 2014. Manuscript.\n[KMN11]\nDaniel M. Kane, Raghu Meka, and Jelani Nelson.\nAlmost optimal explicit\njohnson-lindenstrauss families. In Approximation, Randomization, and Combi-\nnatorial Optimization. Algorithms and Techniques - 14th International Work-\nshop, APPROX 2011, and 15th International Workshop, RANDOM 2011,\nPrinceton, NJ, USA, August 17-19, 2011. Proceedings, pages 628\u2013639, 2011.\n[KNP11]\nMichal Kouck\u00b4y, Prajakta Nimbhorkar, and Pavel Pudl\u00b4ak. Pseudorandom gen-\nerators for group products: extended abstract. In Proceedings of the 43rd ACM\nSymposium on Theory of Computing, STOC 2011, San Jose, CA, USA, 6-8\nJune 2011, pages 263\u2013272, 2011.\n[KRS12]\nZohar Shay Karnin, Yuval Rabani, and Amir Shpilka. Explicit dimension re-\nduction and its applications. SIAM J. Comput., 41(1):219\u2013249, 2012.\n[LRTV09]\nShachar Lovett, Omer Reingold, Luca Trevisan, and Salil P. Vadhan. Pseu-\ndorandom bit generators that fool modular sums.\nIn Approximation, Ran-\ndomization, and Combinatorial Optimization. Algorithms and Techniques, 12th\nInternational Workshop, APPROX 2009, and 13th International Workshop,\nRANDOM 2009, Berkeley, CA, USA, August 21-23, 2009. Proceedings, pages\n615\u2013630, 2009.\n[MOO05]\nElchanan Mossel, Ryan O\u2019Donnell, and Krzysztof Oleszkiewicz. Noise stability\nof functions with low in\ufb02uences: invariance and optimality. In Proceedings of\nthe 46th Annual IEEE Symposium on Foundations of Computer Science (FOCS\n\u201905), pages 21\u201330, 2005.\n[MORS09] Kevin Matulef, Ryan O\u2019Donnell, Ronitt Rubinfeld, and Rocco A. Servedio.\nTesting \u00b11-weight halfspace. In Approximation, Randomization, and Combina-\ntorial Optimization. Algorithms and Techniques, 12th International Workshop,\nAPPROX 2009, and 13th International Workshop, RANDOM 2009, Berkeley,\nCA, USA, August 21-23, 2009. Proceedings, pages 646\u2013657, 2009.\n36\n[MZ09]\nRaghu Meka and David Zuckerman. Small-bias spaces for group products. In\nApproximation, Randomization, and Combinatorial Optimization. Algorithms\nand Techniques, 12th International Workshop, APPROX 2009, and 13th Inter-\nnational Workshop, RANDOM 2009, Berkeley, CA, USA, August 21-23, 2009.\nProceedings, pages 658\u2013672, 2009.\n[MZ13]\nRaghu Meka and David Zuckerman. Pseudorandom generators for polynomial\nthreshold functions. SIAM J. Comput., 42(3):1275\u20131301, 2013.\n[Nis92]\nNoam Nisan. Pseudorandom generators for space-bounded computation. Com-\nbinatorica, 12(4):449\u2013461, 1992.\n[Nis94]\nNoam Nisan. RL \u2286SC. Computational Complexity, 4(1):1\u201311, 1994.\n[NN93]\nJoseph Naor and Moni Naor. Small-bias probability spaces: E\ufb03cient construc-\ntions and applications. SIAM J. on Comput., 22(4):838\u2013856, 1993.\n[NZ96]\nNoam Nisan and David Zuckerman. Randomness is linear in space. J. Comput.\nSystem Sci., 52(1):43\u201352, 1996.\n[O\u2019D14]\nRyan O\u2019Donnell. Analysis of Boolean functions. Cambridge University Press,\n2014.\n[Rei08]\nOmer Reingold. Undirected connectivity in log-space. J. ACM, 55(4), 2008.\n[RR99]\nRan Raz and Omer Reingold. On recycling the randomness of states in space\nbounded computation. In Proceedings of the Thirty-First Annual ACM Sympo-\nsium on Theory of Computing, May 1-4, 1999, Atlanta, Georgia, USA, pages\n159\u2013168, 1999.\n[RS10]\nYuval Rabani and Amir Shpilka. Explicit construction of a small epsilon-net\nfor linear threshold functions. SIAM J. Comput., 39(8):3501\u20133520, 2010.\n[RS13]\nD. Ron and R. Servedio. Exponentially improved algorithms and lower bounds\nfor testing signed majorities.\nIn ACM-SIAM Symposium on Discrete Algo-\nrithms (SODA), pages 1319\u20131336, 2013.\n[RTV06]\nOmer Reingold, Luca Trevisan, and Salil P. Vadhan. Pseudorandom walks on\nregular digraphs and the RL vs. L problem. In Proceedings of the 38th Annual\nACM Symposium on Theory of Computing, Seattle, WA, USA, May 21-23,\n2006, pages 457\u2013466, 2006.\n[SSS95]\nJeanette P. Schmidt, Alan Siegel, and Aravind Srinivasan. Cherno\ufb00-hoe\ufb00ding\nbounds for applications with limited independence. SIAM J. Discrete Math.,\n8(2):223\u2013250, 1995.\n37\n[SZ99]\nMichael E. Saks and Shiyu Zhou.\nBP hspace(s) subseteq dspace(s3/2).\nJ.\nComput. Syst. Sci., 58(2):376\u2013403, 1999.\n[VW08]\nEmanuele Viola and Avi Wigderson. Norms, xor lemmas, and lower bounds\nfor polynomials and protocols. Theory of Computing, 4(7):137\u2013168, 2008.\nA\nProofs from Section 2\nProof of Lemma 7. This follows from the fact that \u2225Qp\u22251 \u2264\u2225Q\u2225p\n1. Therefore,\nE [Q(x)p] \u2264\nE\nX\u2208u{\u00b11}n[Q(X)p] + \u2225Q\u2225p\n1\u03b4 \u2264(p \u22121)pd/2 \u00b7 \u2225Q\u2225p\n2 + \u2225Q\u2225p\n1\u03b4.\nProof of Lemma 8. Note that because \u2225v\u22252 = 1 that \u2225v\u22251 \u2264\u2225v\u22251/2\n0\nby Cauchy-Schwarz.\nWe note by the Markov inequality that for even p that\nPr[|\u27e8v, x\u27e9| > t] \u2264t\u2212p E[|\u27e8v, x\u27e9|p].\nWe need a slightly strengthened version of Lemma 7 to bound this. Note that if f(x) =\n\u27e8v, x\u27e9\nE[f(x)] \u2264\u2225f p\u22250\u01eb + \u2225f\u2225p\np \u2264\u2225v\u2225p\n0\u01eb + (p \u22121)(p \u22123) \u00b7 \u00b7 \u00b7 1.\nThe bound on \u2225f\u2225p comes from noting that the expectation of f p under Gaussian inputs is\n(p \u22121)(p \u22123) \u00b7 \u00b7 \u00b7 1 and that the expectation under Bernoulli inputs is at most this (which\ncan be seen by expanding and comparing terms). Therefore, we have that\nPr[|\u27e8v, x\u27e9| > t] \u2264t\u2212p\u221a\n2(p/e)p/2 + t\u2212p\u2225v\u2225p\n0\u01eb\nLetting p be the largest even integer less than t2, we \ufb01nd that this is at most\n\u221a\n2 exp(\u2212p/2) + \u2225v\u2225t2\n0 \u01eb,\nwhich is su\ufb03cient when t \u22652. For 1 \u2264t \u2264\n\u221a\n2, the trivial upper bound of 1 is su\ufb03cient,\nand for\n\u221a\n2 \u2264t \u22642, we may instead use the bound for p = 2.\nProof of Lemma 9. Let Ii,k be the indicator function of the event that h(i) = k. Note that\nh(v) = P\ni,j,k Ii,kIj,kv2\ni v2\nj . Therefore,\nh(v)p =\nX\ni1,...,ip,j1,...,jp\nX\nk1,...,kp\np\nY\nt=1\nIit,ktIjt,kt\np\nY\nt=1\nv2\nitv2\njt.\n38\nLet R(it, jt, kt) be 0 if for some t, t\u2032 kt \u0338= k\u2032\nt but one of it or jt equals it\u2032 or jt\u2032 and otherwise\nbe equal to m\u2212T where T is the number of distinct values taken by it or jt. Notice that\nby the \u03b4-biasedness of h that\nE\n\" p\nY\nt=1\nIit,ktIjt,kt\n#\n\u2264R(it, jt, kt) + \u03b4.\nCombining with the above we \ufb01nd that\nE[h(v)p] \u2264\nX\ni1,...,ip,j1,...,jp\nX\nk1,...,kp\n(R(it, jt, kt) + \u03b4)\np\nY\nt=1\nv2\nitv2\njt\n\u2264\nX\ni1,...,ip,j1,...,jp\nX\nk1,...,kp\nR(it, jt, kt)\np\nY\nt=1\nv2\nitv2\njt + \u03b4mp\nX\ni1,...,ip,j1,...,jp\np\nY\nt=1\nv2\nitv2\njt\n\u2264\nX\ni1,...,ip,j1,...,jp\nX\nk1,...,kp\nR(it, jt, kt)\np\nY\nt=1\nv2\nitv2\njt + \u03b4mp\u2225v\u22254p\n2 .\nNext we consider\nX\nk1,...,kp\nR(it, jt, kt)\nfor \ufb01xed values of i1, . . . , ip, j1, . . . , jp.\nWe claim that it is at most m\u2212S/2 where S is\nagain the number of distinct elements of the form it or jt that appear in this way an odd\nnumber of times. Letting T be the number of distinct elements of the form it or jt, the\nexpression in question is m\u2212T times the number of choices of kt so that each value of it\nor jt appears with only one value of kt. In other words this is m\u2212T times the number of\nfunctions f : {it, jt} \u2192[m] so that f(it) = f(jt) for all t. This last relation splits {it, jt}\ninto equivalence classes given by the transitive closure of the operation that x \u223cy if x = it\nand y = jt for some t. We note that any x that appears an odd number of times as an\nit or jt must be in an equivalence class of size at least 2 because it must appear at least\nonce with some other element. Therefore, the number of equivalence classes, E is at least\nT \u2212S/2. Thus, the sum in question is at most m\u2212T mE \u2264m\u2212S/2. Therefore, we have that\nE[h(v)p] \u2264(2p)!\nX\nMultisets M\u2282[n],|M|=2p\nm\u2212{Odd(M)}/2 Y\ni\u2208M\nv2\ni + \u03b4mp\u2225v\u22254p\n2 .\nWhere Odd(M) is the number of elements occurring in M an odd number of times. This\n39\nequals\nE[h(v)p] \u2264(2p)!\np\nX\nk=0\nX\nMultisets M\u2282[n],|M|=2p,Odd(M)=2k\nm\u2212k Y\ni\u2208M\nv2\ni + \u03b4mp\u2225v\u22254p\n2\n\u2264(2p)!\np\nX\nk=0\nm\u2212k\nX\ni1,...,i2k\nX\nj1,...,jp\u2212k\nY\nv2\nit\nY\nv4\njt + \u03b4mp\u2225v\u22254p\n2\n= (2p)!\np\nX\nk=0\n\u0012\u2225v\u22254\n2\nm\n\u0013k\n\u2225v\u22254(p\u2212k)\n4\n+ \u03b4mp\u2225v\u22254p\n2\n\u2264O(p)2p\n\u0012\u2225v\u22254\n2\nm\n\u0013p\n+ O(p)2p\u2225v\u22254p\n4 + \u03b4mp\u2225v\u22254p\n2 .\nNote that the second line above comes from taking M to be the multiset\n{i1, i2, . . . , i2k, j1, j1, j2, j2, . . . , jp\u2212k, jp\u2212k}.\nThis completes our proof.\nProof of Lemma 10. Let Xi denote the indicator random variable which is 1 if h(i) = j\nand 0 otherwise. Let Z = P\ni viXi. Now, if h were a truly random hash function, then, by\nHoe\ufb00ding\u2019s inequality,\nPr [|Z \u2212\u2225v\u22251 /m| \u2265t] \u22642 exp\n \n\u2212t2/2\nX\ni\nv2\ni\n!\n.\nTherefore, for a truly random hash function and even integer p \u22652, \u2225Z\u2225p = O(\u2225v\u22252)\u221ap.\nTherefore, for a \u03b4-biased hash family, we get \u2225Z\u2225p\np \u2264O(p)p/2 \u2225v\u2225p\n2 + \u2225v\u2225p\n1 \u03b4. Hence, by\nMarkov\u2019s inequality, for any t > 0,\nPr [|Z \u2212\u2225v\u22251 /m| \u2265t] \u2264O(p)p/2 \u2225v\u2225p\n2 + \u2225v\u2225p\n1 \u03b4\ntp\n.\n40\n",
        "sentence": "",
        "context": "2\nPreliminaries\nWe start with some notation:\n\u2022 For vectors x \u2208Rn, let \u2225x\u2225p denote the usual \u2113p-norms, and let \u2225x\u22250 denote the size\nof the support of x. For a random variable X and p > 0, let \u2225X\u2225p = E[|X|p]1/p.\ntions. In 53rd Annual IEEE Symposium on Foundations of Computer Science,\nFOCS 2012, New Brunswick, NJ, USA, October 20-23, 2012, pages 120\u2013129,\n2012.\n[GMRZ13] Parikshit Gopalan, Raghu Meka, Omer Reingold, and David Zuckerman. Pseu-\nfrom the ubiquitous applications in computer science of Cherno\ufb00-like bounds for weighted\nsums of the form P\ni wixi where the xis are uniformly random bits. There has been a"
    },
    {
        "title": "Fooling functions of halfspaces under product distributions",
        "author": [
            "Parikshit Gopalan",
            "Ryan O\u2019Donnell",
            "Yi Wu",
            "David Zuckerman"
        ],
        "venue": "In Proceedings of the 2010 IEEE 25th Annual Conference on Computational Complexity,",
        "citeRegEx": "Gopalan et al\\.,? \\Q2010\\E",
        "shortCiteRegEx": "Gopalan et al\\.",
        "year": 2010,
        "abstract": "We construct pseudorandom generators that fool functions of halfspaces\n(threshold functions) under a very broad class of product distributions. This\nclass includes not only familiar cases such as the uniform distribution on the\ndiscrete cube, the uniform distribution on the solid cube, and the multivariate\nGaussian distribution, but also includes any product of discrete distributions\nwith probabilities bounded away from 0.\n  Our first main result shows that a recent pseudorandom generator construction\nof Meka and Zuckerman [MZ09], when suitably modifed, can fool arbitrary\nfunctions of d halfspaces under product distributions where each coordinate has\nbounded fourth moment. To eps-fool any size-s, depth-d decision tree of\nhalfspaces, our pseudorandom generator uses seed length O((d log(ds/eps)+log n)\nlog(ds/eps)). For monotone functions of d halfspaces, the seed length can be\nimproved to O((d log(d/eps)+log n) log(d/eps)). We get better bounds for larger\neps; for example, to 1/polylog(n)-fool all monotone functions of (log n)= log\nlog n halfspaces, our generator requires a seed of length just O(log n). Our\nsecond main result generalizes the work of Diakonikolas et al. [DGJ+09] to show\nthat bounded independence suffices to fool functions of halfspaces under\nproduct distributions. Assuming each coordinate satisfies a certain stronger\nmoment condition, we show that any function computable by a size-s, depth-d\ndecision tree of halfspaces is eps-fooled by O(d^4s^2/eps^2)-wise independence.",
        "full_text": "arXiv:1001.1593v1  [cs.CC]  11 Jan 2010\nFooling Functions of Halfspaces under Product Distributions\nParikshit Gopalan\nMicrosoft Research SVC\nparik@microsoft.com\nRyan O\u2019Donnell\u2217\nCarnegie Mellon University\nodonnell@cs.cmu.edu\nYi Wu\u2020\nCarnegie Mellon University\nyiwu@cs.cmu.edu\nDavid Zuckerman\u2021\nUT Austin\ndiz@cs.utexas.edu\nOctober 30, 2018\nAbstract\nWe construct pseudorandom generators that fool functions of halfspaces (threshold functions)\nunder a very broad class of product distributions. This class includes not only familiar cases such\nas the uniform distribution on the discrete cube, the uniform distribution on the solid cube, and\nthe multivariate Gaussian distribution, but also includes any product of discrete distributions\nwith probabilities bounded away from 0.\nOur \ufb01rst main result shows that a recent pseudorandom generator construction of Meka\nand Zuckerman [MZ09], when suitably modi\ufb01ed, can fool arbitrary functions of d halfspaces\nunder product distributions where each coordinate has bounded fourth moment.\nTo \u01eb-fool\nany size-s, depth-d decision tree of halfspaces, our pseudorandom generator uses seed length\nO((d log(ds/\u01eb) + log n) \u00b7 log(ds/\u01eb)). For monotone functions of d halfspaces, the seed length can\nbe improved to O((d log(d/\u01eb)+log n)\u00b7log(d/\u01eb)). We get better bounds for larger \u01eb; for example,\nto 1/polylog(n)-fool all monotone functions of (log n)/ log log n halfspaces, our generator requires\na seed of length just O(log n).\nOur second main result generalizes the work of Diakonikolas et al. [DGJ+09] to show that\nbounded independence su\ufb03ces to fool functions of halfspaces under product distributions. As-\nsuming each coordinate satis\ufb01es a certain stronger moment condition, we show that any function\ncomputable by a size-s, depth-d decision tree of halfspaces is \u01eb-fooled by \u02dcO(d4s2/\u01eb2)-wise inde-\npendence.\nOur technical contributions include: a new multidimensional version of the classical Berry-\nEsseen theorem; a derandomization thereof; a generalization of Servedio [Ser07]\u2019s regularity\nlemma for halfspaces which works under any product distribution with bounded fourth moments;\nan extension of this regularity lemma to functions of many halfspaces; and, new analysis of the\nsandwiching polynomials technique of Bazzi [Baz09] for arbitrary product distributions.\n\u2217Work was partially done while the author consulted at Microsoft Research SVC. Supported by NSF grants\nCCF-0747250 and CCF-0915893, BSF grant 2008477, and Sloan and Okawa fellowships.\n\u2020Work done while an intern at Microsoft Research SVC.\n\u2021Work was partially done while the author consulted at Microsoft Research SVC. Partially supported by NSF\nGrants CCF-0634811 and CCF-0916160 and THECB ARP Grant 003658-0113-2007.\n1\nIntroduction\nHalfspaces, or threshold functions, are a central class of Boolean-valued functions. A halfspace is a\nfunction h : Rn \u2192{0, 1} of the form h(x1, . . . , xn) = 1[w1x1 + \u00b7 \u00b7 \u00b7 + wnxn \u2265\u03b8] where the weights\nw1, . . . , wn and the threshold \u03b8 are arbitrary real numbers. These functions have been studied ex-\ntensively in theoretical computer science, social choice theory, and machine learning. In computer\nscience, they were \ufb01rst studied in the context of switching circuits; see for instance [Der65, Hu65,\nLC67, She69, Mur71]. Halfspaces (with non-negative weights) have also been studied extensively\nin game theory and social choice theory as models for voting; see e.g. [Pen46, Isb69, DS79, TZ92].\nHalfspaces are also ubiquitous in machine learning contexts, playing a key role in many impor-\ntant algorithmic techniques, such as Perceptron , Support Vector Machine, Neural Networks,\nand AdaBoost. One of the outstanding open problems in circuit lower bounds is to \ufb01nd an ex-\nplicit function that cannot be computed by a depth two circuit (\u201cneural network\u201d) of threshold\ngates [HMP+93, Kra91, KW91, FKL+01].\nIn this work we investigate the problem of constructing explicit pseudorandom generators for\nfunctions of halfspaces.\nDe\ufb01nition 1.1. A function G : {0, 1}s \u2192B is a pseudorandom generator (PRG) with seed length\ns and error \u01eb for a class F of functions from B to {0, 1} under distribution D on B \u2014 or more\nsuccinctly, G \u01eb-fools F under D with seed length s \u2014 if for all f \u2208F,\n\f\f\f Pr\nX\u223cD [f(X) = 1] \u2212\nPr\nY \u223c{0,1}s [f(G(Y )) = 1]\n\f\f\f \u2264\u01eb.\nUnder the widely-believed complexity-theoretic assumption BPP = P, there must be a determin-\nistic algorithm that can approximate the fraction of satisfying assignments to any polynomial-size\ncircuit of threshold gates. Finding such an algorithm even for simple functions of halfspaces has\nproven to be a di\ufb03cult derandomization problem. Very recently, however, there has been a burst\nof progress on constructing PRGs for halfspaces [RS08, DGJ+09, MZ09]. The present paper makes\nprogress on this problem in several di\ufb00erent directions, as do several concurrent and independent\nworks [HKM09, DKN09, BELY09].\nThis \ufb02urry of work on PRGs for functions of halfspaces has several motivations beyond its status\nas a fundamental derandomization task. For one, it can be seen as a natural geometric problem,\nwith connections to deterministic integration; for instance, the problem of constructing PRGs for\nhalfspaces under the uniform distribution on the n-dimensional sphere amounts to constructing a\npoly(n)-sized set that hits every spherical cap with roughly the right frequency [RS08]. Second,\nPRGs for halfspaces have applications in streaming algorithms [GR09], while PRGs for functions\nof halfspaces can be used to derandomize the Goemans-Williamson Max-Cut algorithm, algorithms\nfor approximate counting, algorithms for dimension reduction and intractability results in compu-\ntational learning [KS08]. Finally, proving lower bounds for the class TC0 of small depth threshold\ncircuits is an outstanding open problem in circuit complexity. An explicit PRG for a class is easily\nseen to imply lower bounds against that class. Constructions of explicit PRGs might shed light on\nstructural properties of threshold circuits and the lower bound problem.\n1.1\nPrevious work\nThe work of Rabani and Shpilka [RS08] constructed a hitting set generator for halfspaces under\nthe uniform distribution on the sphere. Diakonikolas et al. [DGJ+09] constructed the \ufb01rst PRG\nfor halfspaces over bits; i.e., the uniform distribution on {\u22121, 1}n. They showed that any k-wise\n1\nindependent distribution \u01eb-fools halfspaces with respect to the uniform distribution for k = \u02dcO(1/\u01eb2),\ngiving PRGs with seed length (log n) \u00b7 \u02dcO(1/\u01eb2).\nMeka and Zuckerman constructed a pseudorandom generator that \u01eb-fools degree-d polynomial\nthreshold functions (\u201cPTFs\u201d, a generalization of halfspaces) over uniformly random bits with seed\nlength (log n)/\u01ebO(d) [MZ09]. Their generator is a simpli\ufb01ed version of Rabani and Shpilka\u2019s hitting\nset generator. In the case of halfspaces, they combine their generator with generators for small-\nwidth branching programs due to Nisan and Nisan-Zuckerman [Nis92, NZ96] to bring the seed\nlength down to O((log n) log(1/\u01eb)). This is the only previous or independent work where the seed\nlength depends logarithmically on 1/\u01eb.\n1.2\nIndependent concurrent work\nIndependently and concurrently, a number of other researchers have extended some of the afore-\nmentioned results, mostly to intersections of halfspaces and polynomial threshold functions over\nthe hypercube or Gaussian space.\nDiakonikolas et al. [DKN09] showed that O(1/\u01eb9)-wise independence su\ufb03ces to fool degree-2\nPTFs under the uniform distribution on the hypercube and under the Gaussian distribution. They\nalso prove that poly(d, 1/\u01eb)-wise independence su\ufb03ces to fool intersections of d degree-2 PTFs in\nthese settings.\nHarsha et al. [HKM09] obtain a PRG that fools intersections of d halfspaces under the Gaus-\nsian distribution with seed length O((log n) \u00b7 poly(log d, 1/\u01eb)). They obtain similar parameters for\nintersections of d \u201cregular\u201d halfspaces under the uniform distribution on {\u22121, 1}n (a halfspace is\nregular if all of its coe\ufb03cients have small magnitude compared to their sum of squares).\nBen-Eliezer et al. [BELY09] showed that roughly exp((d/\u01eb)d)-wise independence \u01eb-fools degree-d\nPTFs which depend on a small number of linear functions.\n1.3\nOur Results\nIn this work, we construct pseudorandom generators for arbitrary functions of halfspaces under (al-\nmost) arbitrary product distributions. Our work diverges from previous work in making minimal\nassumptions about the distribution we are interested in, and in allowing general functions of halfs-\npaces. For both of our main results, we only assume that the distribution is a product distribution\nwhere each coordinate satis\ufb01es some mild conditions on its moments. These conditions include\nmost distributions of interest, such as the Gaussian distribution, the uniform distribution on the\nhypercube, the uniform distribution on the solid cube, and discrete distributions with probabilities\nbounded away from 0. Our results can also be used to fool the uniform distribution on the sphere,\neven though it is not a product distribution. This allows us to derandomize the hardness result of\nKhot and Saket [KS08] for learning intersections of halfspaces.\nWe also allow for arbitrary functions of d halfspaces, although the seed length improves sig-\nni\ufb01cantly if we consider monotone functions or small decision trees. In particular, we get strong\nresults for intersections of halfspaces.\n1.3.1\nThe Meka-Zuckerman Generator\nWe show that a suitable modi\ufb01cation of the Meka-Zuckerman (MZ) generator can fool arbi-\ntrary functions of d halfspaces under any product distribution, where the distribution on each\ncoordinate has bounded fourth moments.\nMore precisely, we consider product distributions on\nX = (x1, . . . , xn) where for every i \u2208[n], E[xi] = 0, E[x2\ni ] = 1, E[x4\ni ] \u2264C where C \u22651 is a\nparameter of the generator G. We say that the distribution X has C-bounded fourth moments.\n2\nWe get our best results for monotone functions of d halfspaces, such as intersections of d halfs-\npaces. For distributions with polynomially bounded fourth moments, our modi\ufb01ed MZ PRG fools\nthe intersection of d halfspaces with polynomially small error using a seed of length O(d log2 n).\nMany natural distributions have O(1)-bounded fourth moments.\nEven for polylog(n)-bounded\nfourth moments, our PRG fools the intersection of (log n)/ log log n halfspaces with error 1/polylog(n)\nusing a seed of length just O(log n). Both of these cases are captured in the following theorem.\nTheorem 1.2. Let X be sampled from a product distribution on Rn with C-bounded fourth mo-\nments. The modi\ufb01ed MZ generator \u01eb-fools any monotone function of d halfspaces with seed length\nO((d log(Cd/\u01eb) + log n) log(Cd/\u01eb)). When Cd/\u01eb \u2265log\u2212c n for any c > 0, the seed length becomes\nO(d log(Cd/\u01eb) + log n).\nAs a corollary, we get small seed length for functions of halfspaces that have small decision\ntree complexity. In the theorem below we could even take s to be the minimum of the number of\n0-leaves and 1-leaves.\nTheorem 1.3. Let X be as in Theorem 1.2. The modi\ufb01ed MZ generator \u01eb-fools any size-s, depth-d\nfunction of halfspaces, using a seed of length O((d log(Cds/\u01eb) + log n) log(Cds/\u01eb)). When Cds/\u01eb \u2265\nlog\u2212c n for any c > 0, the seed length becomes O(d log(Cds/\u01eb) + log n).\nSince the decision tree complexity is at most 2d, we deduce the following.\nCorollary 1.4. Let X be as in theorem 1.2. The modi\ufb01ed MZ generator \u01eb-fools any function of\nd halfspaces, using a seed of length O((d2 + d log(Cd/\u01eb) + log n)(d + log(Cd/\u01eb))). When Cd2d/\u01eb \u2265\nlog\u2212c n for any c > 0, the seed length becomes O(d2 + d log(Cd/\u01eb) + log n).\n1.3.2\nBounded Independence fools functions of halfspaces\nWe prove that under a large class of product distributions, bounded independence su\ufb03ces to fool\nfunctions of d halfspaces. This signi\ufb01cantly generalizes the result of Diakonikolas et al. [DGJ+09]\nwho proved that bounded independence fools halfspaces under the uniform distribution on {\u22121, 1}n.\nThe condition necessary on the product distributions is unfortunately somewhat technical; we state\nhere a theorem that covers the main cases of interest:\nTheorem 1.5. Suppose f is computable as a size-s, depth-d function of halfspaces over the inde-\npendent random variables x1, . . . , xn. If we assume the xj\u2019s are discrete, then k-wise independence\nsu\ufb03ces to \u01eb-fool f, where\nk = eO(d4s2/\u01eb2) \u00b7 poly(1/\u03b1).\nHere 0 < \u03b1 \u22641 is the least nonzero probability of any outcome for an xj. Moreover, the same\nresult holds with \u03b1 = 1 for certain continuous random variables xj, including Gaussians (possibly\nof di\ufb00erent variance) and random variables which are uniform on (possibly di\ufb00erent) intervals.\nFor example, whenever \u03b1 \u22651/polylog(d/\u01eb) it holds that eO(d6/\u01eb2)-wise independence su\ufb03ces\nto \u01eb-fool intersections of m halfspaces. For random variables that do not satisfy the hypotheses of\nTheorem 1.5, it may still be possible to extract a similar statement from our techniques. Roughly\nspeaking, the essential requirement is that the random variables xj be \u201c(p, 2, p\u2212c)-hypercontractive\u201d\nfor large values of p and some constant c < 1.\n3\nNotation:\nThroughout, all random variables take values in R or Rd. Random variables will\nbe in boldface.\nReal scalars will be lower-case letters; real vectors will be upper-case letters.\nIf X is a d-dimensional vector, we will write X[1], X[2], . . . , X[d] for its coordinates values and\n\u2225X\u2225=\nqPd\ni=1 X[i]2 for its Euclidean length.\nWhen M is a matrix, we also use the notation\nM[i, j] for its (i, j) entry. If X is a vector-valued random variable, we write \u2225\u2225\u2225X\u2225\u2225\u2225p = E[\u2225X\u2225p]1/p.\nWe typically use i to index dimensions and j to index sequences. Given x \u2208R we de\ufb01ne sgn(x) = 1\nif x \u22650 and sgn(x) = \u22121 if x < 0. If X is a d-dimensional vector, then \u2212\u2192\nsgn(X) denotes the vector\nin {\u22121, 1}d with \u2212\u2192\nsgn(X)[i] = sgn(X[i]).\nOur results concern arbitrary functions of d halfspaces. Thus we have vectors W1, . . . , Wn, \u0398 \u2208\nRd, and we\u2019re interested in functions f : {\u22121, 1}d \u2192{0, 1} of the vector \u2212\u2192\nsgn(x1W1+. . .+xnWn\u2212\u0398),\nwhich we abbreviate to \u2212\u2192\nsgn(W \u00b7 X \u2212\u0398) where W = (W1, . . . , Wn) and X = (x1, . . . , xn).\nOrganization:\nWe give an overview of our results and their proofs in 2. We prove the multi-\ndimensional Berry-Esseen type theorems in Section 4. In Section 5, we prove a regularity lemma\nfor multiple halfspaces in the general setting of hypercontractive variables. We state modi\ufb01ed MZ\ngenerator in Section 6, and analyze it using the machinery above in Section 7. In Section 8, we\nshow how to combine it with PRGs for branching programs to get our Theorems 1.2 and 1.3. We\nprove Theorem 1.5 in Section 10. In Section 11, we show how our results apply to fooling the\nuniform distribution on the sphere, and use it to derandomize the hardness result of [KS08].\n2\nOverview of the main results\nIn this section, we give an overview on how we construct and analyze the following two types\nof PRGs for functions of halfspaces under general product distributions: i) the modi\ufb01ed Meka-\nZuckerman generator (in Section 2.1) and ii) the bounded independence generator (in Section 2.2)\n2.1\nThe Meka-Zuckerman Generator\nThere are \ufb01ve steps in the analysis:\n1. Discretize the distribution X so that it is the product of discrete distributions whose moments\nnearly match those of X.\n2. Prove a multidimensional version of the classical Berry-Esseen theorem, and a derandomization\nthereof under general product distributions.\nThis allows us to handle functions of regular\nhalfspaces. See Subsection 2.1.1.\n3. Generalize the regularity lemma/critical index lemma (see [Ser07, DGJ+09]) to d halfspaces\nunder general product distributions. This gives a small set of variables such that after condi-\ntioning on these variables, each halfspace becomes either regular or close to a constant function.\nSee Subsection 2.1.2.\n4. Use the regularity lemma to reduce analyzing functions of d arbitrary halfspaces to analyzing\nfunctions of d (or fewer) regular halfspaces.\n5. Finally, generalize the monotone trick from [MZ09], which previously worked only for a single\n\u201cmonotone\u201d branching program, to monotone functions of monotone branching programs. This\nenables us to get seed length logarithmic in 1/\u01eb. See Subsection 2.1.3.\n4\n2.1.1\nMulti-Dimensional Berry-Esseen Theorem\nThe classic Berry-Esseen Theorem is a quantitative version of the Central Limit Theorem. This\ntheorem is essential in the analyses of [MZ09] and [DGJ+09] for one halfspace.\nSince we seek\nto fool functions of several halfspaces, we prove a multi-dimensional version of the Berry-Esseen\ntheorem, which approximates the distribution of P\ni xiWi. The error of the approximation is small\nwhen all the halfspaces are regular (no coe\ufb03cient is too large). While there are multi-dimensional\nversions known, we were unable to \ufb01nd in the literature any theorems which we could use in a\n\u201cblack-box\u201d fashion. The reason for this is twofold: known results tend to focus on measuring the\ndi\ufb00erence between probability distributions vis-a-vis convex sets; whereas, we are interested in more\nspecialized sets, unions of orthants. Second, results in the literature tend to assume a nonsingular\ncovariance matrix and/or have a dependence in the error bound on its least eigenvalue; whereas,\nwe need to work with potentially singular covariance matrices. We believe this theorem could be\nof independent interest.\nNext we show how this theorem can be derandomized in a certain sense. This derandomization\nenables us to show that our modi\ufb01ed MZ PRG fools regular halfspaces.\n2.1.2\nMulti-Dimensional Critical Index\nThe concept of critical index was introduced in the work of Servedio [Ser07].\nIt is used to\nprove a regularity lemma for halfspaces, which asserts that every halfspace contains a head con-\nsisting of constantly many variables, such that once these variables are set randomly, the re-\nsulting function is either close to constant, or close to a regular halfspace.\nThis lemma has\nfound numerous applications in complexity and learning theoretic questions related to halfspaces\n[Ser07, OS08, FGRW09, DGJ+09, MZ09].\nThe obvious generalization of the one-dimensional theorem to multiple halfspaces would be to\ntake the union of the heads of each halfspace. This does not work, since setting variables in a\nregular halfspace can make it irregular. We prove a multidimensional version of this lemma, which\nmoreover holds in the setting of product distributions with bounded fourth moments. Our analysis\nshows that the lemma only requires some basic concentration and anti-concentration properties,\nwhich are enjoyed by any random variable with bounded fourth moments.\n2.1.3\nMonotone Branching Programs\nThe only known method to get logarithmic dependence on 1/\u01eb for PRGs for halfspaces, due to Meka\nand Zuckerman, considers the natural branching program accepting a halfspace. This branching\nprogram is \u201cmonotone,\u201d in the sense that in every layer the set of accepting su\ufb03xes forms a\ntotal order under inclusion. Meka and Zuckerman showed that any monotone branching program\nof arbitrary width can be sandwiched between two small-width monotone branching programs.\nTherefore, PRGs for small-width branching programs, such as those by Nisan [Nis92] can be used.\nSince we deal with several halfspaces, we get several monotone branching programs. We consider\nmonotone functions of monotone branching programs, to encompass intersections of halfspaces.\nHowever, such functions are not necessarily computable by monotone branching programs. Nev-\nertheless, we show how to sandwich such functions between two small-width branching programs,\nand thus can use the PRGs like Nisan\u2019s.\n5\n2.2\nBounded Independence fools functions of halfspaces\n2.2.1\nSandwiching \u201cpolynomials\u201d\nTo prove that bounded independence can fool functions of halfspaces (Theorem 1.5), we use the\n\u201csandwiching polynomials\u201d method as introduced by Bazzi [Baz09] and used by [DGJ+09]. However\nin our setting of general random variables it is not appropriate to use polynomials per se. The\nessence of the sandwiching polynomial method is showing that only groups of d random variables\nneed to be \u201csimultaneously controlled\u2019. When the random variables are \u00b11-valued, controlling sub-\nfunctions of at most d random variables is equivalent to controlling polynomials of degree at most d.\nBut for random variables with more than two outcomes, a function of d random variables requires\ndegree higher than d in general, a price we should not be forced to pay. We instead introduce the\nfollowing notions:\nDe\ufb01nition 2.1. Let \u2126= \u21261 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 \u2126n be a product set. We say that p : \u2126\u2192R is a k-junta if\nf(x1, . . . , xn) depends on at most k of the xj\u2019s. We say that p is a generalized polynomial of order\n(at most) k if it is expressible as a sum of simple functions of order at most k. In the remainder\nof this section we typically drop the word \u201cgeneralized\u201d from \u201cgeneralized polynomial\u201d, and add the\nmodi\ufb01er \u201cordinary\u201d when referring to \u201cordinary polynomials\u201d.\nWe now give the simple connection to fooling functions with bounded independence:\nDe\ufb01nition 2.2. Let X = (x1, . . . , xn) be a vector of independent random variables, where xj has\nrange \u2126j. Let f : \u2126\u2192R, where \u2126= \u21261 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 \u2126n. We say that polynomials pl, pu : \u2126\u2192R are\n\u01eb-sandwiching for f if\npl(X) \u2264f(X) \u2264pu(X) for all X \u2208\u2126, and E[pu(X)] \u2212\u01eb \u2264E[f(X)] \u2264E[pl(X)] + \u01eb.\nProposition 2.3. Suppose pl, pu are \u01eb-sandwiching for f as in De\ufb01nition 2.2 and have order at\nmost k. Then f is \u01eb-fooled by k-wise independence. I.e., if Y = (y1, . . . , yn) is a vector of random\nvariables such that each marginal of the form (yj1, . . . , yjk) matches the corresponding marginal\n(xj1, . . . , xjk), then\n|E[f(X)] \u2212E[f(Y )]| \u2264\u01eb.\nProof. Write pu = P\nt qt, where each qt is a k-junta. Then\nE[f(Y )] \u2264E[pu(Y )] = E[P\nt\nqt(Y )] = P\nt\nE[qt(Y )] = P\nt\nE[qt(X)] = E[pu(X)] \u2264E[f(X)] + \u01eb,\nwhere in addition to the sandwiching properties of pu we used the fact that qt is a k-junta to deduce\nE[qt(Y )] = E[qt(X)]. We obtain the bound E[f(Y )] \u2265E[f(bX)] \u2212\u01eb similarly, using pl.\n2.2.2\nUpper polynomials for intersections su\ufb03ce\nWe begin with a trivial observation:\nProposition 2.4. Let C be a class of functions \u2126\u2192{0, 1}, and suppose that for every f \u2208C we\nhave just the \u201cupper sandwiching polynomial\u201d, pu, of an \u01eb-sandwiching pair for f. Then if C is\nclosed under Boolean negation, we obtain a matching \u201clower polynomial\u201d pl of the same order as\npu automatically.\n6\nThis is simply because given pu for f, we may take pl = 1\u2212pu. Since the Boolean negation of a\nhalfspace is a halfspace, this observation could have been used for slight simpli\ufb01cation in [DGJ+09].\nOur Theorem 1.5 is concerned with the class of 0-1 functions f computable as size-s, depth-d\nfunctions of halfspaces. This class is closed under Boolean negation; hence it su\ufb03ces for us to\nobtain upper sandwiching polynomials. Furthermore, every such f can be written as f = Ps\u2032\nt=1 Ht,\nwhere s\u2032 \u2264s and Ht is an intersection (AND) of up to d halfspaces. To see this, simply sum the\nindicator function for each root-to-leaf path in the decision tree (this again uses the fact that the\nnegation of a halfspace is a halfspace). Thus if we have (\u01eb/s)-sandwiching upper polynomials of\norder k for each Ht, by summing them we obtain an \u01eb-sandwiching upper polynomial for f of the\nsame order. Hence to prove our main Theorem 1.5, it su\ufb03ces to prove the following:\nTheorem 2.5. Suppose f is the intersection of d halfspaces h1, . . . , hd over the independent random\nvariables x1, . . . , xn. Suppose \u03b1 is as in Theorem 1.5. Then there exists an \u01eb-sandwiching upper\npolynomial for f of order k \u2264eO(d4/\u01eb2) \u00b7 poly(1/\u03b1).\n2.2.3\nPolynomial construction techniques\nSuppose for simplicity we are only concerned with the intersection f of d halfspaces h1, . . . , hd over\nuniform random \u00b11 bits xj. The work of Diakonikolas et al. [DGJ+09] implies that there are is an\n\u01eb0-sandwiching upper polynomial pi of order eO(1/\u01eb2\n0) for each hi. To obtain an \u01eb-sandwiching upper\npolynomial for the intersection h1h2 \u00b7 \u00b7 \u00b7 hd, a natural \ufb01rst idea is simply to try p = p1p2 \u00b7 \u00b7 \u00b7 pd. This\nis certainly an upper-bounding polynomial; however the \u01eb-sandwiching aspect is unclear. We can\nbegin the analysis as follows. Let hi = hi(X) and pi = pi(X). By telescoping,\nE[p1 \u00b7 \u00b7 \u00b7 pd] \u2212E[h1 \u00b7 \u00b7 \u00b7 hd]\n=\nE[(p1 \u2212h1)p2 \u00b7 \u00b7 \u00b7 pd]\n+\n\u00b7 \u00b7 \u00b7\n. . .\n+\nE[h1 \u00b7 \u00b7 \u00b7 hi\u22121(pi \u2212hi)pi+1 \u00b7 \u00b7 \u00b7 pd]\n+\n\u00b7 \u00b7 \u00b7\n(1)\n. . .\n+\nE[h1 \u00b7 \u00b7 \u00b7 hd\u22121(pd \u2212hd)].\nNow the last term here could be upper-bounded as\nE[h1 \u00b7 \u00b7 \u00b7 hd\u22121(pd \u2212hd)] \u2264E[pd \u2212hd] \u2264\u01eb0,\nsince each 0 \u2264hi \u22641 with probability 1.\nBut we cannot make an analogous bound for the\nremaining terms because we have no a priori control over the values of the pi\u2019s beyond the individual\nsandwiching inequalities\nE[pi \u2212hi] \u2264\u01eb0.\nNevertheless, we will be able to make this strategy work by establishing additional boundedness\nconditions on the polynomials pi; speci\ufb01cally, that each pi exceeds 1 + 1/d2 extremely rarely, and\nthat even the high 2d-norm of pi is not much more than 1.\nEstablishing these extra properties requires signi\ufb01cant reworking the construction in [DGJ+09].\nEven in the case of uniform random \u00b11 bits, the calculations are not straightforward, since the\nupper sandwiching polynomials implied by [DGJ+09] are only fully explicit in the case of regular\nhalfspaces. And to handle general random variables xj, we need more than just our new Regularity\nLemma 5.3 for halfspaces. We also need to assume a stronger hypercontractivity property of the\nrandom variables to ensure they have rapidly decaying tails.\n7\n3\nHypercontractivity\nThe notion of of hypercontractive random variables was introduced in [KS88] and developed by\nKrakowiak, Kwapie\u00b4n, and Szulga:\nDe\ufb01nition 3.1. We say that a real random variable x is (p, q, \u03b7)-hypercontractive for 1 \u2264q \u2264\np < \u221eand 0 < \u03b7 < 1 if \u2225\u2225\u2225x\u2225\u2225\u2225p < \u221e, and for all a \u2208R, \u2225\u2225\u2225a + \u03b7x\u2225\u2225\u2225p \u2264\u2225\u2225\u2225a + x\u2225\u2225\u2225q.\nIn this paper we will be almost exclusively concerned with the simplest case, p = 4, q = 2. Let\nus abbreviate the de\ufb01nition in this case (and also exclude constantly-0 random variables):\nDe\ufb01nition 3.2. A real random variable x is \u03b7-HC for 0 < \u03b7 < 1 if 0 < \u2225\u2225\u2225x\u2225\u2225\u22254 < \u221eand for all\na \u2208R, \u2225\u2225\u2225a + \u03b7x\u2225\u2225\u22254 \u2264\u2225\u2225\u2225a + x\u2225\u2225\u22252, i.e. E[(a + \u03b7x)4] \u2264E[(a + x)2]2.\nEssentially, a mean 0 real random variable is \u03b7-HC with large \u03b7 if and only if it has a small 4th\nmoment (compared to its 2nd moment). Random variables with small 4th moment are known to\nenjoy some basic concentration and anti-concentration properties. We work with hypercontractivity\nrather than 4th moments because it tends to slightly shorten proofs and improve constants; the\nmain convenience is that a linear combination of \u03b7-HC random variables is also \u03b7-HC.\nHere we list some basic and useful properties of \u03b7-HC random variables, all of which have\nelementary proofs. Note that Facts 3 and 4 imply that the upper bound on the 4th norm C =\n\u0398(1/\u03b74).\nFact 3.3. [KS88, MOO05, Wol06a, Wol06b]\n1. If x is \u03b7-HC then it is also \u03b7\u2032-HC for all \u03b7\u2032 < \u03b7.\n2. If x is \u03b7-HC then x is centered, E[x] = 0.\n3. If x is \u03b7-HC then E[x4] \u2264(1/\u03b7)4 E[x2]2.\n4. Conversely, if E[x] = 0 and E[x4] \u2264(1/\u03b7)4 E[x2]2, then x is (\u03b7/2\n\u221a\n3)-HC. If x is also sym-\nmetric (i.e., \u2212x has the same distribution as x) then X is min(\u03b7, 1/\n\u221a\n3)-HC.\n5. If x is \u00b11 with probability 1/2 each, then x is (1/\n\u221a\n3)-HC. The same is true if x has the standard\nGaussian distribution or the uniform distribution on [\u22121, 1].\n6. If x is \u03b7-HC then in fact \u03b7 \u22641/\n\u221a\n3.\n7. If x is a centered discrete random variable and \u03b1 \u22641/2 is the least nonzero value of x\u2019s\nprobability mass function, then x is \u03b7-HC for \u03b7 = \u03b11/4/2\n\u221a\n3.\n8. If x1, . . . , xn are independent \u03b7-HC random variables, then so is c1x1 + \u00b7 \u00b7 \u00b7 cnxn for any real\nconstants c1, . . . , cn, not all 0. (Indeed, 4-wise independence su\ufb03ces.)\n9. If x is \u03b7-HC, and y is a random variable with the same rth moments as x for all r = 0, 1, 2, 3, 4,\nthen y is also \u03b7-HC.\nThe notion of hypercontractivity can be extended to Rd-valued random variables:\nDe\ufb01nition 3.4. An Rd-random variable X is \u03b7-HC for 0 < \u03b7 < 1 if \u2225\u2225\u2225X\u2225\u2225\u22254 < \u221eand for all\nA \u2208Rd, \u2225\u2225\u2225A + \u03b7X\u2225\u2225\u22254 \u2264\u2225\u2225\u2225A + X\u2225\u2225\u22252.\nWe require the following facts about vector-valued hypercontractivity:\nFact 3.5. [Szu90]\n1. If W \u2208Rd is a \ufb01xed vector and x is an \u03b7-HC real random variable, then X = xW is an \u03b7-HC.\n2. If X1, . . . , Xn are independent \u03b7-HC random vectors, then so is c1X1 + \u00b7 \u00b7 \u00b7 cnXn for any real\nconstants c1, . . . , cn. (Again, 4-wise independence also su\ufb03ces.)\n8\nHypercontractive real random variables possess the following good concentration and anti-\nconcentration properties.\nProposition 3.6. If x is \u03b7-HC then for all t > 0, Pr[|x| \u2265t\u2225\u2225\u2225x\u2225\u2225\u22252] \u2264\n1\n\u03b74t4 .\nProof. Apply Markov to the event \u201cx4 \u2265t4 E[x2]2\u201d.\nProposition 3.7. If x is \u03b7-HC then for all \u03b8 \u2208R and 0 < t < 1, Pr[|x \u2212\u03b8| > t\u2225\u2225\u2225x\u2225\u2225\u22252] \u2265\u03b74(1\u2212t2)2.\nProof. By scaling x it su\ufb03ces to consider the case \u2225\u2225\u2225x\u2225\u2225\u22252 = 1.\nConsider the random variable\ny = (x \u2212\u03b8)2. We have\nE[y] = E[x2] \u22122\u03b8 E[x] + \u03b82 = 1 + \u03b82,\nE[y2] = \u03b7\u22124 E[(\u2212\u03b7\u03b8 + \u03b7x)4] \u2264\u03b7\u22124 E[(\u2212\u03b7\u03b8 + x)2]2 = \u03b7\u22124(1 + \u03b72\u03b82)2 = (\u03b7\u22122 + \u03b82)2,\nwhere we used the fact that x is \u03b7-HC in the second calculation (and then used the \ufb01rst calculation\nagain). We now apply the Paley-Zygmund inequality (with parameter 0 < t2/(1 + \u03b82) < 1):\nPr[|x \u2212\u03b8| > t] = Pr[y > t2] = Pr\n\u0014\ny >\nt2\n1 + \u03b82 E[y]\n\u0015\n\u2265\n\u0012\n1 \u2212\nt2\n1 + \u03b82\n\u00132 E[y]2\nE[y2]\n\u2265\n\u0012\n1 \u2212\nt2\n1 + \u03b82\n\u00132\n(1 + \u03b82)2\n(\u03b7\u22122 + \u03b82)2 =\n\u0012\u03b72(1 \u2212t2) + \u03b72\u03b82\n1 + \u03b72\u03b82\n\u00132\n.\n(2)\nTreat \u03b7 and t as \ufb01xed and \u03b8 as varying. Writing u = \u03b72(1 \u2212t2), we have 0 < u < 1; hence the\nfraction (u + \u03b72\u03b82)/(1 + \u03b72\u03b82) appearing in (2) is positive and increasing as \u03b72\u03b82 increases. Thus it\nis minimized when \u03b8 = 0; substituting this into (2) gives the claimed lower bound.\n4\nThe Multi-Dimensional Berry-Esseen Theorem\nIn this section we prove a Berry-Esseen-style results in the setting of multidimensional random\nvariables, and a derandomization of it.\nWe assume the following setup: X1, . . . , Xn are independent Rd-valued \u03b7-HC random variables,\nnot necessarily identically distributed, satisfying E[Xj] = 0 for all j \u2208[n]. We let S = X1+\u00b7 \u00b7 \u00b7+Xn.\nWe write Mj = Cov[Xj] \u2208Rd\u00d7d for the covariance matrix of Xj, which is positive semide\ufb01nite.\nWe also write M = Cov[S] for the covariance matrix of S; by the independence and mean-zero\nassumptions we have M = M1 + \u00b7 \u00b7 \u00b7 + Mn. We will also assume that\nM[i, i] =\nn\nX\nj=1\nE\n\u0002\nXj[i]2\u0003\n= 1\nfor all i \u2208[d].\nIf we write \u03c32\nj = \u2225\u2225\u2225Xj\u2225\u2225\u22252, it follows that Pn\nj=1 \u03c32\nj = d. We introduce new independent random\nvariables G1, . . . , Gn, where Gj is a d-dimensional Gaussian random variable with covariance matrix\nMj; we also write also G = G1 +\u00b7 \u00b7 \u00b7+Gn. We say that A \u2286Rd is a translate of a union of orthants\nif there exists some vector \u0398 \u2208Rd such that X \u2208A depends only on \u2212\u2192\nsgn(X \u2212\u0398).\nTheorem 4.1. Let S and G be as above. Let A \u2286Rd be a translate of a union of orthants. Then\n|Pr[S \u2208A] \u2212Pr[G \u2208A]| \u2264O(\u03b7\u22121/2d13/8) \u00b7\n\u0010 nP\nj=1\n\u03c34\nj\n\u00111/8\n.\n9\nWe now show that this result can be \u201cderandomized\u201d using the output of the MZ generator Y\nin place of X. We describe here a simpli\ufb01ed version of the output of their generator.\nDe\ufb01nition 4.2. A family H = {h : [n] \u2192[t]} of hash functions is b-collision preserving if\n1. For all i \u2208[n], \u2113\u2208[t], Prh\u2208uH[h(i) = \u2113] \u2264b/t.\n2. For all i \u0338= j \u2208[n], Prh\u2208uH[h(i) = h(j)] \u2264b/t.\nE\ufb03cient constructions of size |H| = O(nt) are known for any constant b \u22651. b = 1 is optimal,\nand can be achieved by a pairwise independent family. In our construction we use b = 1, but we\nwill need larger b in our analysis. A hash function induces a partition of [n].\nWe choose a partition H1, . . . , Ht of [n] into t buckets using a b-collision preserving family of\nhash functions (where b \u22642). The vector of variables {Yj}j\u2208H\u2113is generated 4-wise independently.\nThere is full independence across di\ufb00erent buckets. Let T = Y1 + \u00b7 \u00b7 \u00b7 + Yn.\nTheorem 4.3. Let T and G be as above. Let A \u2286Rd be a translate of a union of orthants. Then\n|Pr[T \u2208A] \u2212Pr[G \u2208A]| \u2264O(\u03b7\u22121/2d13/8) \u00b7\n\u0010d2\nt +\nnP\nj=1\n\u03c34\nj\n\u00111/8\n.\nPutting these two theorems together, we have shown the following statement\nTheorem 4.4. Let S and T be as above. Let A \u2286Rd be a translate of a union of orthants. Then\n|Pr[S \u2208A] \u2212Pr[T \u2208A]| \u2264O(\u03b7\u22121/2d13/8) \u00b7\n\u0010d2\nt +\nnP\nj=1\n\u03c34\nj\n\u00111/8\n.\nIn the rest of the section, we prove above theorems; our aim is not to get the best bounds possible\n(for which one might pursue the methods of Bentkus [Ben04]). Rather, we aim to provide a simple\nmethod which achieves a reasonable bound, and thus use the Lindeberg method, following [MOO05,\nMos08] very closely.\n4.1\nThe basic lemma\nIn what follows, K will denote a d-dimensional multi-index (k1, . . . , kd) \u2208Nd, with |K| denoting\nj1 + \u00b7 \u00b7 \u00b7 + jd and K! denoting k1!k2! \u00b7 \u00b7 \u00b7 kd!. Given a vector H \u2208Rd, the expression HK denotes\nQd\ni=1 H[i]ki. Given a function \u03c8 : Rd \u2192R, the expression \u03c8(K) denotes the mixed partial derivative\ntaken ki times in the ith coordinate; we will always assume \u03c8 is smooth enough that the order of\nthe derivatives does not matter.\nThe following lemma is essentially proven in, e.g., [Mos08, Theorem 4.1]. To obtain it, simply\nrepeat Mossel\u2019s proof in the degree 1 case, until equation (31). (Although Mossel assumes that\nthe covariance matrices Mj are identity matrices, this is not actually necessary; it su\ufb03ces that\nCov[Xj] = Cov[Gj].) Then instead of using hypercontractivity, skip directly to summing the\nerror terms over all coordinates.\nLemma 4.5. Let \u03c8 : Rd \u2192R be a C3 function with\n\f\f\u03c8(K)\f\f \u2264b for all |K| = 3. Then\n|E[\u03c8(S)] \u2212E[\u03c8(G)]| \u2264b\nX\n|K|=3\n1\nK!\nn\nX\nj=1\n\u0000E\n\u0002\f\fXK\nj\n\f\f\u0003\n+ E\n\u0002\f\fGK\nj\n\f\f\u0003\u0001\n.\n(3)\nWe further deduce:\n10\nCorollary 4.6. In the setting of Lemma 4.5,\n|E[\u03c8(S)] \u2212E[\u03c8(G)]| \u22642bd3\nn\nX\nj=1\n\u2225\u2225\u2225Xj\u2225\u2225\u22253\n3.\nProof. Fix a multi-index K with |K| = 3 and also an index j. We will show that\nE\n\u0002\f\fXK\nj\n\f\f\u0003\n+ E\n\u0002\f\fGK\nj\n\f\f\u0003\n\u22642.6\u2225\u2225\u2225Xj\u2225\u2225\u22253\n3.\n(4)\nSubstituting this into (3) completes the proof, since\nb\nX\n|K|=3\n2.6\nK! \u22642bd3.\nLet the nonzero coordinates in K be i1, i2, i3 \u2208[d], written with multiplicity. Write also\n\u03c32\ni = Mj[i, i] = E\n\u0002\nGj[i]2\u0003\n= E\n\u0002\nXj[i]2\u0003\n.\nOn one hand, by H\u00a8older we have\nE\n\u0002\f\fGK\nj\n\f\f\u0003\n= E [|Gj[i1]Gj[i2]Gj[i3]|] \u2264\n3\nr\nE\nh\n|Gj[i1]|3i\nE\nh\n|Gj[i2]|3i\nE\nh\n|Gj[i3]|3i\n.\nNote that the distribution of Gj[i1] is N(0, \u03c32\ni1). It is elementary that such a random variable has\nthird absolute moment equal to 2\np\n2/\u03c0 \u00b7\u03c33\ni1 \u22642.6\u03c33\ni1. As the same is true for i2 and i3, we conclude\nthat\nE\n\u0002\f\fGK\nj\n\f\f\u0003\n\u22641.6\u03c3i1\u03c3i2\u03c3i3.\n(5)\nOn the other hand, we can similarly upper-bound\nE\n\u0002\f\fXK\nj\n\f\f\u0003\n\u2264\n3\nr\nE\nh\n|Xj[i1]|3i\nE\nh\n|Xj[i2]|3i\nE\nh\n|Xj[i3]|3i\n(6)\nBut\n3\nr\nE\nh\n|Xj[i1]|3i\nE\nh\n|Xj[i2]|3i\nE\nh\n|Xj[i3]|3i\n\u2265\n3\nr\nE\nh\n|Xj[i1]|2i3/2\nE\nh\n|Xj[i2]|2i3/2\nE\nh\n|Xj[i3]|2i3/2\n= \u03c3i1\u03c3i2\u03c3i3,\nand hence from (5) and (6) we conclude\nE\n\u0002\f\fXK\nj\n\f\f\u0003\n+ E\n\u0002\f\fGK\nj\n\f\f\u0003\n\u22642.6 3\nr\nE\nh\n|Xj[i1]|3i\nE\nh\n|Xj[i2]|3i\nE\nh\n|Xj[i3]|3i\n.\nFinally, we clearly have |Xj[i1]| \u2264\u2225Xj\u2225always, and similarly for j2, j3. Hence\nE\n\u0002\f\fXK\nj\n\f\f\u0003\n+ E\n\u0002\f\fGK\nj\n\f\f\u0003\n\u22642.6\n3\nr\nE\nh\n\u2225Xj\u22253i\nE\nh\n\u2225Xj\u22253i\nE\nh\n\u2225Xj\u22253i\n= 2.6\u2225\u2225\u2225Xj\u2225\u2225\u22253\n3,\ncon\ufb01rming (4).\nCorollary 4.7. In the setting of Lemma 4.5,\n|E[\u03c8(S)] \u2212E[\u03c8(G)]| \u22642bd7/2\nv\nu\nu\nt\nn\nX\nj=1\n\u2225\u2225\u2225Xj\u2225\u2225\u22254\n4.\n11\nProof. Using Cauchy-Schwarz twice,\nn\nX\nj=1\n\u2225\u2225\u2225Xj\u2225\u2225\u22253\n3 =\nn\nX\nj=1\nE\nh\n\u2225Xj\u22253i\n=\nn\nX\nj=1\nE\nh\n\u2225Xj\u2225\u2225Xj\u22252i\n\u2264\nn\nX\nj=1\nr\nE\nh\n\u2225Xj\u22252ir\nE\nh\n\u2225Xj\u22254i\n\u2264\nv\nu\nu\nt\nn\nX\nj=1\nE\nh\n\u2225Xj\u22252i\nv\nu\nu\nt\nn\nX\nj=1\nE\nh\n\u2225Xj\u22254i\n=\n\u221a\nd\nv\nu\nu\nt\nn\nX\nj=1\n\u2225\u2225\u2225Xj\u2225\u2225\u22254\n4,\nwhere we also used P \u03c32\nj = d.\n4.2\nDerandomization and hypercontractivity\nWe now show that this result can be \u201cderandomized\u201d in a certain sense. This idea is essentially\ndue to Meka and Zuckerman [MZ09, Sec. 4.1].\nDe\ufb01nition 4.8. We say that the sequences of Rd-valued random vectors X1, . . . , Xn and Y1, . . . , Yn\nsatisfy the r-matching-moments condition, r \u2208N, if the following holds: E[X K] = E[YK] for\nall multi-indices |K| \u2264r, where X is the Rdn-valued random vector gotten by concatenating\nX1, . . . , Xn, and Y is de\ufb01ned similarly.\nIn this section, we suppose that Y1, . . . , Yn satisfy the 4-matching-moments condition with\nrespect to X1, . . . , Xn. We will not suppose that they are independent, but rather that they have\nsome limited independence. Let T = Y1 + \u00b7 \u00b7 \u00b7 + Yn.\nProposition 4.9. Let H1, . . . , Ht form a partition of [n], and write Z\u2113= P\nj\u2208H\u2113Yj. Assume that\nZ1, . . . , Zt are independent. Then\n|E[\u03c8(T )] \u2212E[\u03c8(G)]| \u22642bd7/2\nv\nu\nu\nt\nt\nX\n\u2113=1\n\u2225\u2225\u2225P\nj\u2208H\u2113\nXj\u2225\u2225\u22254\n4.\nProof. We simply apply Corollary 4.7 to the random variables Z1, . . . , Zt.\nTo check that it is\napplicable, we note the following: The random variables are independent. They satisfy E[Z\u2113] = 0,\nbecause each E[Yj] = 0 by 1-matching-moments. The covariance matrix Pt\n\u2113=1 Cov[Z\u2113] = M, by\n2-matching-moments.\nThus Corollary 4.7 gives\n|E[\u03c8(T )] \u2212E[\u03c8(G)]| \u22642bd7/2\ns\ntP\n\u2113=1\n\u2225\u2225\u2225Z\u2113\u2225\u2225\u22254\n4.\nBut for each \u2113,\n\u2225\u2225\u2225Z\u2113\u2225\u2225\u22254\n4 = \u2225\u2225\u2225P\nj\u2208H\u2113\nYj\u2225\u2225\u22254\n4 = E\nh\n\u27e8P\nj\u2208H\u2113\nYj, P\nj\u2208H\u2113\nYj\u27e92i\n= E\nh\n\u27e8P\nj\u2208H\u2113\nXj, P\nj\u2208H\u2113\nXj\u27e92i\n= \u2225\u2225\u2225P\nj\u2208H\u2113\nXj\u2225\u2225\u22254\n4,\nusing 4-matching-moments, completing the proof.\nRemark 4.10. The full 4-matching-moments condition is not essential for our results; it would\nsu\ufb03ce to have 2-matching-moments, along with a good upper bound on the 4th moments of the Yj\u2019s\nwith respect to those of the Xj\u2019s.\n12\nWe can simplify the previous bounds if we assume hypercontractivity.\nCorollary 4.11. If we additionally assume that the random vectors X1, . . . , Xn are \u03b7-HC, then\nwe have\n|E[\u03c8(S)] \u2212E[\u03c8(G)]|\n\u2264\n(2bd7/2/\u03b72)\ns\nnP\nj=1\n\u03c34\nj ,\n|E[\u03c8(T )] \u2212E[\u03c8(G)]|\n\u2264\n(2bd7/2/\u03b72)\ns\ntP\n\u2113=1\n\u0010 P\nj\u2208H\u2113\n\u03c32\nj\n\u00112\n.\nProof. We prove only the second statement, the \ufb01rst being simpler. It su\ufb03ces to show\n\u2225\u2225\u2225P\nj\u2208H\u2113\nXj\u2225\u2225\u22254\n4 \u2264(1/\u03b7)4\u0010 P\nj\u2208H\u2113\n\u03c32\nj\n\u00112\n.\nSince the random variables {Xj : j \u2208H\u2113} are independent and \u03b7-HC, it follows that the (vector-\nvalued) random variable P\nj\u2208H\u2113Xj is \u03b7-HC. Hence\n\u2225\u2225\u2225P\nj\u2208H\u2113\nXj\u2225\u2225\u22254\n4 \u2264(1/\u03b7)4\u0010\n\u2225\u2225\u2225P\nj\u2208H\u2113\nXj\u2225\u2225\u22252\n2\n\u00112\n.\nBut\n\u2225\u2225\u2225P\nj\u2208H\u2113\nXj\u2225\u2225\u22252\n2 = P\nj\u2208H\u2113\n\u03c32\nj\nby the Pythagorean Theorem.\nWe now consider the case when the partition H1, . . . , Ht chosen randomly using a b-collison\npreserving family of hash functions (see De\ufb01nition 4.2).\nProposition 4.12. In the setting of Corollary 4.11, if the partition H1, . . . , Ht is chosen using a\nb-collision preserving family of hash functions, then\n|E[\u03c8(T )] \u2212E[\u03c8(G)]| \u2264(2bb1/2d7/2/\u03b72)\ns\nd2\nt +\nnP\nj=1\n\u03c34\nj .\nwhere the expectation E[\u03c8(T )] is with respect to both the choice of H1, . . . , Ht and Y1, . . . , Yn.\nProof. By the triangle inequality for real numbers, it su\ufb03ces to show\nE\nH1,...,Ht\n\"s\ntP\n\u2113=1\n\u0010 P\nj\u2208H\u2113\n\u03c32\nj\n\u00112\n#\n\u2264\nv\nu\nu\ntb\n \nd2\nt +\nnP\nj=1\n\u03c34\nj\n!\n.\nBy Cauchy-Schwarz, this reduces to showing\nE\nH1,...,Ht\n\"\ntP\n\u2113=1\n\u0010 P\nj\u2208H\u2113\n\u03c32\nj\n\u00112\n#\n\u2264b\n\uf8eb\n\uf8edd2\nt +\nn\nX\nj=1\n\u03c34\nj\n\uf8f6\n\uf8f8.\n13\nBut\nE\nH1,...,Ht\n\"\ntP\n\u2113=1\n\u0010 P\nj\u2208H\u2113\n\u03c32\nj\n\u00112\n#\n=\nt\nX\n\u2113=1\nE\n\"\u0010 nP\nj=1\n1{j\u2208H\u2113}\u03c32\nj\n\u00112\n#\n=\nt\nX\n\u2113=1\nn\nX\nj1,j2=1\n\u03c32\nj1\u03c32\nj2 E[1{j1\u2208H\u2113}1{j2\u2208H\u2113}]\n\u2264\nt\nX\n\u2113=1\n\uf8eb\n\uf8edb\nt\nn\nX\nj=1\n\u03c34\nj\n\uf8f6\n\uf8f8+\nX\nj1\u0338=j2\n\u03c32\nj1\u03c32\nj2\nt\nX\n\u2113=1\nE[1{j1\u2208H\u2113}1{j2\u2208H\u2113}] \u2264b\nn\nX\nj=1\n\u03c34\nj+b\nt\nX\nj1\u0338=j2\n\u03c32\nj1\u03c32\nj2 \u2264bd2\nt +b\nn\nX\nj=1\n\u03c34\nj ,\nas needed, because\nX\nj1\u0338=j2\n\u03c32\nj1\u03c32\nj2 \u2264\n\uf8eb\n\uf8ed\nn\nX\nj=1\n\u03c32\nj\n\uf8f6\n\uf8f8\n2\n= d2.\n4.3\nSmoothing\nIdeally we would like to use the results from the previous sections with \u03c8 equal to certain indicator\nfunctions \u03c7 : Rd \u2192{0, 1}; however these are not C3. As usual in the Lindeberg method (see,\ne.g., [MOO05]), we overcome this by working with molli\ufb01ed versions of these functions. For most of\nthis section, we will work with our underandomized result, the statement about S in Corollary 4.7.\nIdentical considerations apply to the statement about T in Proposition 4.12, and we will draw the\nnecessary conclusions at the end.\nLet \u03be : R \u2192R be the \u201cstandard molli\ufb01er\u201d, a smooth density function supported on [\u22121, 1]. We\nwill use the fact that there is some universal constant b0 such that\nR \f\f\u03be(k)\f\f dx \u2264b0 for k = 1, 2, 3\n(where \u03be(k) denotes the kth derivative of \u03be). Given \u01eb > 0 we de\ufb01ne \u03be\u01eb(x) = \u03be(x/\u01eb)/\u01eb, the standard\nmolli\ufb01er with support [\u2212\u01eb, \u01eb]. Finally, de\ufb01ne the density function \u039e\u01eb on Rd by \u039e\u01eb(x1, . . . , xd) =\nQd\ni=1 \u03be\u01eb(xi). We now prove an elementary lemma:\nLemma 4.13. Let \u03c7 : Rd \u2192[\u22121, 1] be measurable, let \u01eb > 0, and de\ufb01ne \u03c8 = \u039e\u01eb \u2217\u03c7, a smooth\nfunction. Then for any multi-index |K| = 3 we have\n\f\f\u03c8(K)\f\f \u2264(b0/\u01eb)3.\nProof. Using the fact that |\u03c7| \u22641 everywhere, we have\n\f\f\f\u03c8(K)(a)\n\f\f\f =\n\f\f\f\u039e(K)\n\u01eb\n\u2217\u03c7(a)\n\f\f\f \u2264\nZ \f\f\f\u039e(K)\n\u01eb\n\f\f\f =\nZ\n[\u2212\u01eb,\u01eb]d\n\f\f\f\f\f\nd\nY\ni=1\n\u2202ki\n\u2202xki\ni\n\u03be\u01eb(xi)\n\f\f\f\f\f dx1 \u00b7 \u00b7 \u00b7 dxd =\nd\nY\ni=1\nZ \u01eb\n\u2212\u01eb\n\f\f\f\f\n\u2202ki\n\u2202xki \u03be\u01eb(x)\n\f\f\f\f dx.\nNote that\n\u2202k\n\u2202xk \u03be\u01eb(x) = \u03be(k)(x/\u01eb)/\u01ebk+1, from which it follows that\nZ \u01eb\n\u2212\u01eb\n\f\f\f\f\n\u2202k\n\u2202xk \u03be\u01eb(x)\n\f\f\f\f dx \u2264b0/\u01ebk\nfor k = 1, 2, 3. For k = 0 we of course have\nZ \u01eb\n\u2212\u01eb\n|\u03be\u01eb(x)| dx =\nZ \u01eb\n\u2212\u01eb\n\u03be\u01eb(x) dx = 1.\nSince |K| = 3, we therefore achieve the claimed upper bound of (b0/\u01eb)3.\n14\nSuppose now A \u2286Rd is a measurable set. We de\ufb01ne:\nA+\u01eb = {x \u2208Rd : x+[\u2212\u01eb/2, \u01eb/2]d\u2229A \u0338= \u2205},\nA\u2212\u01eb = {x \u2208Rd : x+[\u2212\u01eb/2, \u01eb/2]d \u2286A},\n\u2141\u01ebA = A+\u01eb\\A\u2212\u01eb.\nWe also de\ufb01ne \u03c8A+\u01eb = \u039e\u01eb \u2217\u03c7A+\u01eb as in Lemma 4.13, where \u03c7A+\u01eb is the 0-1 indicator of A+\u01eb, and\nsimilarly de\ufb01ne \u03c8A\u2212\u01eb. Applying now Corollary 4.7, we conclude:\nLemma 4.14. For \u03c8 = \u03c8A+\u01eb or \u03c8 = \u03c8A\u2212\u01eb it holds that\n|E[\u03c8(S)] \u2212E[\u03c8(G)]| \u2264(2b0d7/2/\u03b72\u01eb3)\ns\nnP\nj=1\n\u03c34\nj .\nIt is clear from the de\ufb01nitions that both \u03c8A+\u01eb and \u03c8A\u2212\u01eb have range [0, 1], and that pointwise,\n\u03c8A\u2212\u01eb \u2264\u03c7A \u2264\u03c8A+\u01eb. Thus\nE[\u03c8A\u2212\u01eb(S)] \u2264Pr[S \u2208A] \u2264E[\u03c8A+\u01eb(S)],\nE[\u03c8A\u2212\u01eb(G)] \u2264Pr[G \u2208A] \u2264E[\u03c8A+\u01eb(G)].\nFrom Lemma 4.14 we have that the two left-hand sides above are close and that the two right-hand\nsides are close. Because of good anti-concentration of Gaussians, it may also be that the left-hand\nand right-hand sides on the second line are also close, in which Pr[S \u2208A] and Pr[G \u2208A] will also\nbe close. This motivates the following observation: \u03c8A+\u01eb = \u03c8A\u2212\u01eb = 1 on A\u2212\u01eb and \u03c8A+\u01eb = \u03c8A\u2212\u01eb = 0\non the complement of A+\u01eb. Hence\nE[\u03c8A+\u01eb(G)] \u2212E[\u03c8A\u2212\u01eb(G)] \u2264Pr[G \u2208\u2141\u01ebA].\nPutting together these observations, we conclude:\nTheorem 4.15. We have\n|Pr[S \u2208A] \u2212Pr[G \u2208A]| \u2264(2b0d7/2/\u03b72\u01eb3)\ns\nnP\nj=1\n\u03c34\nj + Pr[G \u2208\u2141\u01ebA].\n4.4\nTranslates of unions of orthants\nLet us now specialize to the case where A \u2286Rd is a translate of a union of orthants. Recall that\nthis means that there exists some vector \u0398 \u2208Rd such that X \u2208A depends only on \u2212\u2192\nsgn(X \u2212\u0398).\nWe make the following observation, whose proof is trivial.\nProposition 4.16. If A \u2286Rd is a union of orthants then\n\u2141\u01ebA \u2286\nd[\ni=1\nW \u01eb\ni ,\nwhere\nW \u01eb\ni = {X \u2208Rd : |X[j] \u2212\u0398[j]| \u2264\u01eb/2}.\nBut we also have the following:\n15\nProposition 4.17. Assuming the d-dimensional Gaussian G with covariance matrix M satis\ufb01es\nM[i, i] = 1 for all i \u2208[d], it holds that\nPr\n\"\nG \u2208\nd[\ni=1\nW \u01eb\ni\n#\n\u2264d\u01eb/\n\u221a\n2\u03c0.\nProof. By a union bound it su\ufb03ces to prove that Pr[|G[i] \u2212\u0398[i]| \u2264\u01eb/2] \u2264\u01eb/\n\u221a\n2\u03c0. This is straight-\nforward, as G[i] has distribution N(0, 1) and hence has pdf bounded above by 1/\np\n2/\u03c0.\nWe now prove Theorem 4.1\nProof. (Theorem 4.1) For any \u01eb > 0, we may combine Propositions 4.16 and 4.17 with Theo-\nrem 4.15 and conclude\n|Pr[S \u2208A] \u2212Pr[G \u2208A]| \u2264(2b0d2/\u03b72\u01eb3)\ns\nnP\nj=1\n\u03c34\nj + d\u01eb/\n\u221a\n2\u03c0.\nThe proof is completed by taking \u01eb = \u03b7\u22121/2d5/8(Pn\nj=1 \u03c34\nj)1/8 (which is strictly positive since P \u03c34\nj =\n0 is impossible).\nIdentical reasoning gives the proof of Theorem 4.3. Combining Theorems 4.1 and 4.3 gives\nTheorem 4.4.\n5\nCritical Index for Hypercontractive Random Variables\nIn this section, we generalize the critical index to random variables that are hypercontractive. We\nwill consider \u03b7-HC random variables x0, . . . , xn which are at least pairwise independent. Write\n\u03c32\nj = \u2225\u2225\u2225xj\u2225\u2225\u22252\n2, and note that pairwise independence implies \u2225\u2225\u2225x0 + \u00b7 \u00b7 \u00b7 + xn\u2225\u2225\u22252\n2 = \u03c32\n0 + \u00b7 \u00b7 \u00b7 + \u03c32\nn. We\nalso write \u03c4 2\ni = \u2225\u2225\u2225xi + xi+1 + \u00b7 \u00b7 \u00b7 + xn\u2225\u2225\u22252\n2 = P\nj\u2265i \u03c32\nj .\nDe\ufb01nition 5.1. For 0 < \u03b4 < 1, we say that the collection of random variables x0, . . . , xn is\n\u03b4-regular if Pn\nj=0 \u2225\u2225\u2225xj\u2225\u2225\u22254\n4 \u2264\u03b4\n\u0010Pn\nj=0 \u2225\u2225\u2225xj\u2225\u2225\u22252\n2\n\u00112\n= \u03b4\u03c4 4\n0 .\nDe\ufb01nition 5.2. Suppose the sequence x0, . . . , xn is ordered, meaning that \u03c32\n0 \u2265\u03c32\n1 \u2265\u03c32\n2 \u2265\u00b7 \u00b7 \u00b7 .\nThen for 0 < \u03b4 < 1, the \u03b4-critical index is de\ufb01ned to be the smallest index \u2113such that the sequence\nx\u2113, x\u2113+1, . . . , xn is \u03b4-regular, or \u2113= \u221eno such index exists.\nTheorem 5.3. Let 0 < \u03b4 < 1, 0 < \u01eb < 1/2, and s > 1 be parameters.\nLet L = br, where\nb = \u2308(2/\u03b74) ln(1/\u01eb)\u2309and r = \u2308(1/\u03b74\u03b4) ln(1 + 16s2)\u2309; note that\nL \u2264O\n\u0012log(s) log(1/\u01eb)\n\u03b78\n\u0013\n\u00b7 1\n\u03b4 .\nAssume the sequence x0, . . . , xn is ordered, that n \u2265L, and that x0, . . . , xL\u22121 are independent.\nThen if \u2113is the \u03b4-critical index for the sequence, and \u2113\u2265L, then for all \u03b8 \u2208R,\nPr [|x0 + \u00b7 \u00b7 \u00b7 + xL\u22121 \u2212\u03b8| \u2264s \u00b7 \u03c4L] \u2264\u01eb + O(ln(1/\u01eb))\n\u03b78s4\n.\n16\nProof. For any 0 \u2264j < L, since the critical index \u2113is at least j we have\n\u03b4\u03c4 4\nj <\nX\ni\u2265j\n\u2225\u2225\u2225xi\u2225\u2225\u22254\n4 \u2264(1/\u03b74)\nX\ni\u2265j\n\u03c34\ni (since each xi is \u03b7-HC) \u2264(\u03c32\nj /\u03b74)\nX\ni\u2265j\n\u03c32\ni = (\u03c32\nj /\u03b74)\u03c4 2\nj .\nwhere we used hypercontractivity and the fact that \u03c3is are ordered. Hence for all 0 \u2264j < L,\n\u03b74\u03b4\u03c4 2\nj < \u03c32\nj = \u03c4 2\nj \u2212\u03c4 2\nj+1\n\u21d2\n\u03c4 2\nj+1 < (1 \u2212\u03b74\u03b4)\u03c4 2\nj .\nIt follows that for all 0 \u2264k < b,\n\u03c4 2\n(k+1)r < (1 \u2212\u03b74\u03b4)r\u03c4 2\nkr <\n1\n1 + 16s2 \u03c4 2\nkr,\n(7)\nwhere we used the de\ufb01nition of r.\nNow for each 0 \u2264k < b de\ufb01ne yk = xkr + xkr+1 + xkr+2 + \u00b7 \u00b7 \u00b7 + x(k+1)r\u22121 and \u03c52\nk = \u2225\u2225\u2225yk\u2225\u2225\u22252\n2 =\n\u03c4 2\nkr \u2212\u03c4 2\n(k+1)r. Using (7) we have immediately conclude\n\u03c52\nk > 16s2\u03c4 2\n(k+1)r\n\u21d2\n\u03c5k > 4s\u03c4(k+1)r.\n(8)\nSince all of x0, . . . , xL\u22121 are independent and \u03b7-HC, we have that y0, y1, . . . , yb\u22121 are independent\n\u03b7-HC random variables. For 0 \u2264k < b, de\ufb01ne the event Ak = \u201c|y0 + y1 + \u00b7 \u00b7 \u00b7 + yk \u2212\u03b8| \u2264(1/2)\u03c5k\u201d,.\nWe claim that for any 0 \u2264k < b,\nPr[Ak | A0 \u2227A1 \u2227\u00b7 \u00b7 \u00b7 \u2227Ak\u22121] < 1 \u2212\u03b74/2.\nTo see this, note that conditioning only a\ufb00ects the values of random variables y0, . . . , yk\u22121, of which\nyk is independent. Further, for every choice of values for y0, . . . , yk\u22121, the event Ak is an anti-\nconcentration event of the type in Proposition 3.7, with some shifted \u03b8. Hence the claim follows\nfrom this Proposition, as (1 \u2212(1/2)2)2 > 1/2. Having established the claim, we conclude\nPr[A0 \u2227A1 \u2227\u00b7 \u00b7 \u00b7 \u2227Ab\u22121] < (1 \u2212\u03b74/2)b \u2264\u01eb.\n(9)\nLet us now de\ufb01ne, for each 1 \u2264k < b, random variables zk = yk + yk+1 + \u00b7 \u00b7 \u00b7 + yb\u22121. These\nrandom variables are also \u03b7-HC, and they satisfy \u2225\u2225\u2225zk\u2225\u2225\u22252\n2 \u2264\u03c4 2\nkr. If we de\ufb01ne the events Bk = \u201c|zk| \u2265\ns\u03c4kr\u201d, then Proposition 3.6 implies Pr[Bk] \u22641/\u03b74s4. Hence\nPr[B1 \u2228B2 \u2228\u00b7 \u00b7 \u00b7 \u2228Bb\u22121] \u2264(b \u22121)/\u03b74s4 < b/\u03b74s4.\n(10)\nCombining (9) and (10) we see that except with probability less than \u01eb+b/\u03b74s4 \u2264\u01eb+ O(ln(1/\u01eb))\n\u03b78s4\n,\nat least one event Ak occurs, and none of the events Bk occurs. Since this is the error bound in the\nTheorem, it remains to show that in this case, the desired result \u201c|x0 + \u00b7 \u00b7 \u00b7 + xL\u22121 \u2212\u03b8| > s \u00b7 \u03c4L\u201d\noccurs. Assume then that Am occurs and Bm+1 does not occur, 0 \u2264m < b. (For m = b \u22121 we\nneed not make the latter assumption.) Thus\n|y0 + y1 + \u00b7 \u00b7 \u00b7 + ym \u2212\u03b8| > (1/2)\u03c5m\nand\n|zm+1| \u2264s\u03c4(m+1)r < (1/4)\u03c5m,\nwhere we used (8). (This makes sense also in the case m = b \u22121 if we naturally de\ufb01ne zb \u22610.) By\nde\ufb01nition of zm+1, we therefore obtain\n|y0 + y1 + \u00b7 \u00b7 \u00b7 + yb\u22121 \u2212\u03b8| = |x0 + \u00b7 \u00b7 \u00b7 + xL\u22121 \u2212\u03b8| > (1/4)\u03c5m \u2265(1/4)\u03c5b\u22121 \u2265s\u03c4br = s\u03c4L,\nas desired, where we used (8).\n17\nWe now state the high-dimensional generalization of Theorem 5.3. Assume x1, . . . , xn are \u03b7-HC\nreal random variables which are at least pairwise independent. Assume also that W1, . . . , Wn are\narbitrary \ufb01xed vectors in Rd, and write Xj = xjWj.\nTheorem 5.4. Let \u03b4, \u01eb, s, L be as in Theorem 5.3. Then there exists a set of coordinates H0 \u2286[n],\n|H0| \u2264dL, with the following property. Assuming the collection of random variables {xj : j \u2208H0}\nis independent, for each coordinate i \u2208[d] we have either:\n1. the sequence of real random variables {Xj[i] : j \u0338\u2208H0} is \u03b4-regular; or,\n2. for all \u03b8 \u2208R,\nPr\n\uf8ee\n\uf8f0\n\f\f\f\nX\nj\u2208H0\nXj[i] \u2212\u03b8\n\f\f\f \u2264s \u00b7\ns X\nj\u0338\u2208H0\n\u2225\u2225\u2225Xj[i]\u2225\u2225\u22252\n2\n\uf8f9\n\uf8fb\u2264\u01eb + O(ln(1/\u01eb))\n\u03b78s4\n.\nThe fact that the sequence x0, . . . , xn was ordered by decreasing 2-norm in Theorem 5.3 was\nmainly used for notational convenience. We can extract from the proof the following corollary for\nunordered sequences (whose proof we omit):\nCorollary 5.5. Let \u03b4, \u01eb, s, b, r, L be as in Theorem 5.3. For the unordered collection x0, . . . , xn,\nassume we have a sequence of indices 0 \u2264j0 < j1 < \u00b7 \u00b7 \u00b7 < jL\u22121 < n such that:\n\u2022 for each 0 \u2264t < L, \u03c32\njt \u2265\u03c32\nj\u2032 for all j\u2032 > jt;\n\u2022 for each 0 \u2264t < L, {xjt, xjt+1, . . . , xn} is not \u03b4-regular.\nAssume also that x0, . . . , xjL are independent. Then for all \u03b8 \u2208R,\nPr\n\u0002\f\fx0 + \u00b7 \u00b7 \u00b7 + xjL\u22121 \u2212\u03b8\n\f\f \u2264s \u00b7 \u03c4jL\u22121+1\n\u0003\n\u2264\u01eb + O(ln(1/\u01eb))\n\u03b78s4\n.\nThe case when jt = t for 0 \u2264t < L corresponds to Theorem 5.3.\nWe now prove Theorem 5.4.\nProof. We construct H0 according to an iterative process. Initially, H0 = \u2205, and we de\ufb01ne ci = 0\nfor all i \u2208[d]. In each step of the process, we do the following: First, we select any i such that\nci < L and such that the collection {Xj[i] : j \u0338\u2208H0} is not \u03b4-regular. If there is no such i then we\nstop the whole process. Otherwise, we continue the step by choosing j \u2208[n]\\H0 so as to maximize\n\u2225\u2225\u2225Xj[i]\u2225\u2225\u22252\n2. We then end the step by adding j into H0 and incrementing ci.\nNote that the process must terminate with |H0| \u2264dL; this is because each step increments one\nof c1, . . . , cd, but no ci can exceed L. When the process terminates, for each i we have either that\n{Xj[i] : j \u0338\u2208H0} is \u03b4-regular or that ci = L.\nIt su\ufb03ces then to show that when ci = L, the anti-concentration statement holds for i. To see\nthis, \ufb01rst reorder the sequence of random variables (Xj[i])j so that the \ufb01rst |H0| are in the order\nthat the indices were added to H0, and the remaining n \u2212|H0| are in an arbitrary order. Write\n1 \u2264j0 < j1 < \u00b7 \u00b7 \u00b7 < jL\u22121 \u2264|H0| for the indices that were added to H0 on those steps which\nincremented ci. Then by the de\ufb01nition of the iterative process, for each 0 \u2264t < L we have that\n\u2225\u2225\u2225Xjt[i]\u2225\u2225\u22252\n2 \u2265\u2225\u2225\u2225Xj\u2032[i]\u2225\u2225\u22252\n2 for all j\u2032 > jt and that {Xjt[i], Xjt+1[i], \u00b7 \u00b7 \u00b7 , Xn[i]} is not \u03b4-regular. The\nanti-concentration statement now follows from Corollary 5.5.\n18\n6\nThe Meka-Zuckerman Generator\nFor the Meka-Zuckerman generator, the \ufb01rst step is to reduce the problem of fooling functions of\nhalfspaces under an arbitrary C-bounded product distribution to fooling an O(C)-bounded discrete\nproduct distribution with support poly(n, C, \u01eb\u22121) in each co-ordinate.\nLemma 6.1. Given a C-bounded distribution X, there is a discrete product distribution Y such\nthat if f : Rn \u2192{\u22121, 1} is a function of d halfspaces {hi : Rn \u2192{\u22121, 1}}i\u2208[d], then\n| E[f(X)] \u2212E[f(Y )]| \u2264O\n\u0012 d\u01eb2\nnC2\n\u0013\n.\nEach yi is distributed uniformly over a multiset \u2126i = {b1(i) \u2264\u00b7 \u00b7 \u00b7 \u2264bg(i)} where |bj(i)| \u2264\n(nC2\u01eb\u22121)\n1\n4 .\nFor every i, we have |\u2126i| = 2s = O(n2C2\u01eb\u22122) and Further E[yi] = 0, E[y2\ni ] =\n1, E[y4\ni ] \u2264O(C).\nWe are interested in d << n, so the error in going from X to Y is o(\u01eb2). Since |\u2126i| = 2s for\nall i, sampling k-wise independently from Y reduces to generating n strings of length s in a k-wise\nindependent manner: this can be done using k max(log n, s) = O(k log(nC/\u01eb)) random bits.\nThis lemma is proved by sandwiching X between two discrete product distributions Y u and\nY \u2113which are close to each other in statistical distance. The proof is in section 9. Henceforth, we\nwill rename Y as X and focus on fooling discrete product distributions.\nWe now describe the main generator of Meka-Zuckerman, modi\ufb01ed so that random variables\ntake values in Q\nj \u2126j instead of simply \u00b11. At a high level, the generator hashes variables into\nbuckets and uses bounded independence for the variables within each bucket. We use a weaker\nproperty of hash functions than used in [MZ09].\nThe generator \ufb01rst picks a partition of [n] = H1 \u222a. . . \u222aHt using a random element from H,\na 1-collision preserving family of hash functions.\nFor each i \u2208[t], it then generates a 5-wise\nindependent distribution (yj)j\u2208Hi on Q\nj\u2208Hi \u2126j. Such a distribution on n random variables can be\ngenerated using a seed of length k log max(n, |\u2126|). These t distributions are chosen independently.\nThe generator outputs Y = (y1, . . . , yn). The seedlength required is log(2n) + 5t log max(n, |\u2126|)\nwhere log(2n) are required for the hash function and 5 log max(n, |\u2126|) bits are needed for each Hi,\ni \u2208[t].\n7\nAnalyzing the Meka-Zuckerman Generator\nWe \ufb01rst prove that the indices in the set H0 are likely to be hashed into distinct buckets.\nDe\ufb01nition 7.1. A hash function h : [n] \u2192[t] is S-isolating if for all x \u0338= y \u2208S, h(x) \u0338= h(y). A\nfamily of hash functions H = {h : [n] \u2192[t]} is (\u2113, \u03b2)-isolating if for any S \u2286[n], |S| \u2264\u2113,\nPr\nh\u2208uH[h is not S-isolating] \u2264\u03b2.\nA b-collision preserving hash family is likely to be isolating for small sets:\nLemma 7.2. Assume t is a power of 2. A b-collision preserving family of hash functions H = {h :\n[n] \u2192[t]} is (\u2113, \u03b2)-collision free for \u03b2 = b\u21132/(2t).\n19\nProof. The expected number of collisions for a set S is at most\n\u0012|S|\n2\n\u0013b\nt \u2264b\u21132\n2t .\nBy increasing n to the next largest power of 2, since t is a power of 2, there is a \ufb01eld F of size\nn where t|n. Then there is a hash family of size n for b = 1. For any element a \u2208F, de\ufb01ne a hash\nfunction ha(x) = (ax) mod t. Here x is viewed as a \ufb01eld element, the multiplication is done in the\n\ufb01eld, and the product is then viewed as a nonnegative integer less than n before taking the mod.\nWe can increase n and t to be the nearest powers of 2. We can therefore take H to have size at\nmost 2n for b = 1.\nWe want the set H0 to be isolated with error \u01eb, so we want \u2113= dL and \u03b2 = \u01eb. Hence we set t\nto be the smallest power of 2 larger than \u21132/\u01eb = (dL)2/\u01eb.\nWe will aim to achieve error O(d\u01eb) (rather than O(\u01eb)), as this makes the notation easier. We\nset the parameters s and \u03b4 in Theorem 5.4 as\ns = 1/(\u03b72\u221a\u01eb),\n\u03b4 = \u03b74\u01eb8\nd7 .\nThis implies that\nL = O\n\u0012log(s) log(1/\u01eb)\n\u03b78\n\u0013\n\u00b7 1\n\u03b4 = O\n\u0012d7 log2(\u01eb\u03b7)\n\u03b712\u01eb8\n\u0013\n,\nt = O\n\u0012(dL)2\n\u01eb\n\u0013\n= O\n\u0012d15 log4(\u01eb\u03b7)\n\u03b724\u01eb17\n\u0013\n.\n7.1\nAnalysis for functions of regular halfspaces\nRecall that our goal is to fool functions of \u2212\u2192\nsgn(P\nj xjWj \u2212\u03b8). Let Yj = yjWj and T = Pn\nj=1 Yj.\nSimilarly let Xj = xjWj and S = Pn\nj=1 Xj. Thus we are interested in bounding\n| Pr\nX [S \u2208A] \u2212Pr\nY [T \u2208A]|\nwhere A is a translate of union of orthants: membership of a point X \u2208Rd in A is a function of\n\u2212\u2192\nsgn(X \u2212\u0398). By rescaling the Wj and \u0398, we may assume without loss of generality that\nM[i, i] =\nn\nX\nj=1\nE\n\u0002\nXj[i]2\u0003\n= 1\nfor all i \u2208[d].\nThe regular case is when the vectors W1, . . . , Wn are such that for every i, the sequence of random\nvariables {Xj[i]}n\nj=1 is \u03b4-regular. In this case, we can directly appeal to the Berry-Esseen theorem\nto prove th correctness of the MZ generator.\nTheorem 7.3. If the sequence of random variables {Xj[i]}n\nj=1 is \u03b4-regular for all i \u2208[d], then the\nMZ generator O(d\u01eb)-fools any function of sgn(W \u00b7 X \u2212\u0398) for all \u0398 \u2208Rd.\nProof. We can therefore apply the machinery developed above. For the regular case, we only need\nto use 4-wise independence. Thus, the random variables Y1, . . . , Yn satisfy the 4-matching-moments\ncondition with respect to X1, . . . , Xn, as de\ufb01ned in Subsection 4.2.\nThe de\ufb01nition of \u03b4-regular is given in De\ufb01nition 5.1. Let \u03c3i,j = \u2225\u2225\u2225Xj[i]\u2225\u2225\u22252. Suppose that for all i,\nthe set of real random variables {Xj[i]} is \u03b4-regular, i.e.,\nn\nX\nj=1\n\u2225\u2225\u2225Xj[i]\u2225\u2225\u22254\n4 \u2264\u03b4\n\u0010 n\nX\nj=1\n\u2225\u2225\u2225Xj[i]\u2225\u2225\u22252\n2\n\u00112\n= \u03b4(\u03c32\ni,1 + \u00b7 \u00b7 \u00b7 + \u03c32\ni,n)2 = \u03b4,\n20\nwhere the last equality is from our normalization. We wish to apply Theorem 4.4. Since \u03c3i,j =\n\u2225\u2225\u2225Xj[i]\u2225\u2225\u22252 \u2264\u2225\u2225\u2225Xj[i]\u2225\u2225\u22254, we conclude that for all i,\nn\nX\nj=1\n\u03c34\ni,j \u2264\nn\nX\nj=1\n\u2225\u2225\u2225Xj[i]\u2225\u2225\u22254\n4 \u2264\u03b4.\nSince \u03c32\nj = Pd\ni=1 \u03c32\ni,j, by Cauchy-Schwarz we get\n\u03c34\nj =\n d\nX\ni=1\n\u03c32\ni,j\n!2\n\u2264d\n\uf8eb\n\uf8ed\nn\nX\nj=1\n\u03c34\ni,j\n\uf8f6\n\uf8f8\u2264d\u03b4.\nTherefore Pn\nj=1 \u03c34\nj \u2264d2\u03b4. Hence we can apply Theorem 4.4 to obtain\n|Pr[S \u2208A] \u2212Pr[T \u2208A]| \u2264O\n\u0010\n(1/\u03b7)1/2d15/8) \u00b7 (t\u22121 + \u03b4)1/8\u0011\n\u2264O(d\u01eb).\nwhere the last inequality follows from the choice of t, \u03b4.\n7.2\nAnalysis for functions of general halfspaces\nWe now combine Theorem 5.4 with the analysis of the Regular case (Theorem 7.3), to prove that\nthe MZ generator fools functions of arbitrary halfspaces.\nTheorem 7.4. The MZ generator O(d\u01eb)-fools any function of d halfspaces with seed length\nO(t log(max(n, |\u2126|))) = O\n\u0012d15 log4(\u01eb\u03b7) log(n/\u01eb\u03b7)\n\u03b724\u01eb17\n\u0013\n.\nProof. Apply Theorem 5.4 with these parameters. Then there exists a set H0 \u2286[n] of size at most\ndL such that the coordinates [d] can be partitioned into two sets, REG and JUNTA, such that the\nfollowing holds.\n1. For i \u2208REG, the set of real random variables {Xj[i] : j \u0338\u2208H0} is \u03b4-regular.\n2. For i \u2208JUNTA, for all \u03b8 \u2208R,\nPr\n\uf8ee\n\uf8f0\n\f\f\f\nX\nj\u2208H0\nXj[i] \u2212\u03b8\n\f\f\f \u2264s \u00b7\ns X\nj\u0338\u2208H0\n\u2225\u2225\u2225Xj[i]\u2225\u2225\u22252\n2\n\uf8f9\n\uf8fb\u2264\u01eb + O(log(1/\u01eb))\n\u03b78s4\n\u22642\u01eb\n(11)\nWe condition on the hash function h being S-collision free, which happens with probability at\nleast 1 \u2212\u01eb. Therefore, at most one variable from H0 lands in each set in the partition. Since the\ndistribution in each partition set is 5-wise independent, this means that the distribution on H0 is\nfully independent. This allows us to construct a coupling of X and Y : let Xj = Yj for j \u2208H0,\nand then sample the rest according to the correct marginal distribution.\nWe say that the variables in H0 are good if\n\f\f\f\nX\nj\u2208H0\nYj[i] \u2212\u03b8[i]\n\f\f\f > s \u00b7\ns X\nj\u0338\u2208H0\n\u2225\u2225\u2225Yj[i]\u2225\u2225\u22252\n2 for all i \u2208V\n21\nBy Equation 11,\nPr[{Xj = Yj}j\u2208H0 are not good] \u22642d\u01eb.\n(12)\nWe condition on these variables being good.\nWith this conditioning, we show that the halfspaces in JUNTA are nearly constant: with high\nprobability they do not depend on the variables outside H0. To see this, observe that conditioned\non the variables in H0, the remaining variables are still 4-wise independent (in both X and Y ), so\nby Chebychev\nPr\n\uf8ee\n\uf8f0\n\f\f\f\nX\nj\u0338\u2208H0\nYj[i]\n\f\f\f \u2265s \u00b7\ns X\nj\u0338\u2208H0\n\u2225\u2225\u2225Yj[i]\u2225\u2225\u22252\n2\n\uf8f9\n\uf8fb\u22641/s2 \u2264\u01eb.\n(13)\nBut if this does not happen, then\nsign(\nn\nX\nj=1\nYj[i] \u2212\u0398[i]) = sign(\nX\nj\u2208H0\nYj[i] \u2212\u0398[i])].\nA similar analysis holds for X. Thus for both X and Y , with error probability at most 2d/s2 \u22642d\u01eb,\nwe can assume that the halfspaces in JUNTA are \ufb01xed to constant functions for a good choice of\nvariables in H0.\nRecall that we are interested in fooling functions of the form g(h1(X), . . . , hk(X)). Conditioned\non the variables in H0 being good, the halfspaces hj for j \u2208JUNTA are close to constant functions.\nThus, the function g is 2d\u01eb close to a function g\u2032 of halfspace {hj}j\u2208REG under both distributions\nX and Y . Thus it su\ufb03ces to show that the bias of g\u2032 under X and Y is close.\nConditioning on Xj = Yj for j \u2208H0 gives a halfspace on the remaining variables in each\ncoordinate i \u2208REG. De\ufb01ne\n\u0398\u2032[i] = (\u0398[i] \u2212\nX\nj\u0338\u2208H0\nXj[i])),\nS\u2032[i] =\nX\nj\u0338\u2208H0\nXj[i], T \u2032[i] =\nX\nj\u0338\u2208H0\nYj[i].\nthen\nsgn(S[i] \u2212\u0398[i]) = sgn(S\u2032[i] \u2212\u0398\u2032[i]).\nThus there exists a union of orthants A\u2032 \u2208R|REG| such that g\u2032(X) = 1 if X \u2208A\u2032. Our goal is to\nbound\n\f\fPr[S\u2032 \u2208A\u2032] \u2212Pr[T \u2032 \u2208A\u2032]\n\f\f .\nThe set of random variables {Xj[i] : j \u0338\u2208H0} is \u03b4-regular.\nHence we can apply our result\nfor the regular case. We\u2019ve already conditioned on the hash function h being H0-collision free.\nSince this happens with probability at least 1\u2212\u01eb, the resulting function is b-collision preserving for\nb = 1/(1 \u2212\u01eb) \u22642, since conditioning on an event which happens with probability p can increase\nthe probability of any other event by a factor of at most 1/p. So now applying the analysis from\nthe regular case,\n\f\f\f\fPr\nS\u2032 [S\u2032 \u2208A\u2032] \u2212Pr\nT \u2032 [T \u2032 \u2208A\u2032]\n\f\f\f\f \u2264O\n\u0012\n\u03b7\u22121/2d15/8 \u00b7 (1\nt + \u03b4)1/8\n\u0013\n\u2264O(d\u01eb).\n(14)\nHence, conditioned on h and the variables in H0 being good, we have\n\f\f\f\fPr\nS [S \u2208A] \u2212Pr\nT [T \u2208A]\n\f\f\f\f \u2264O(d\u01eb) + 2d\u01eb.\n(15)\n22\nRemoving the conditioning gives\n\f\f\f\fPr\nS [S \u2208A] \u2212Pr\nT [T \u2208A]\n\f\f\f\f \u2264O(d\u01eb) + 2d\u01eb + \u01eb + 2d\u01eb = O(d\u01eb)\n8\nGeneralized Monotone Trick\nWe generalize the \u201cmonotone trick\u201d introduced in Meka and Zuckerman [MZ09] and show that a\ngenerator that fools small-width \u201cmonotone\u201d branching programs also fools any monotone function\nof several arbitrary-width monotone branching programs.\nFirst we de\ufb01ne read-once branching programs. Branching programs corresponding to space S\nhave width 2S. We use the following notation from [MZ09].\nDe\ufb01nition 8.1 (ROBP). An (S, D, T)-branching program B is a layered multi-graph with a layer\nfor each 0 \u2264i \u2264T and at most 2S vertices (states) in each layer. The \ufb01rst layer has a single vertex\nv0 and each vertex in the last layer is labeled with 0 (rejecting) or 1 (accepting). For 0 \u2264i \u2264T,\na vertex v in layer i has at most 2D outgoing edges each labeled with an element of {0, 1}D and\npointing to a vertex in layer i + 1.\nLet B be an (S, D, T)-branching program and v a vertex in layer i of B. We now de\ufb01ne the set\nof accepting su\ufb03xes.\nDe\ufb01nition 8.2. We say z is an accepting su\ufb03x from vertex v if the path in B starting at v and\nfollowing edges labeled according to z leads to an accepting state. We let AccB(v) denote the set of\naccepting su\ufb03xes from v. If B is understood we may abbreviate this Acc(v).\nNisan [Nis92] and Impagliazzo et al. [INW94] gave PRGs that fool (S, D, T)-branching programs\nwith error exp(2\u2212\u2126(S+D)) and seed length r = O((S + D + log T) log T). For T = poly(S, D),\nthe PRG of Nisan and Zuckerman [NZ96] fools (S, D, T)-branching programs with seed length\nr = O(S + D). Meka and Zuckerman showed that the above PRGs in fact fool arbitrary width\nbranching programs of a certain form called monotone, de\ufb01ned next.\nDe\ufb01nition 8.3 (Monotone ROBP). An (S, D, T)-branching program B is said to be monotone if\nfor all 0 \u2264i < T, there exists an ordering {v1 \u227av2 \u227a. . . \u227avLi} of the vertices in layer i such that\nv \u227aw implies AccB(v) \u2286AccB(w).\nNote that the natural ROBP accepting a halfspace, where states correspond to partial sums, is\nmonotone. However, the natural ROBP accepting the intersection of just two halfspaces may not\nbe monotone.\nThe following theorem is the only known way to obtain PRGs for halfspaces using seed length\nwhich depends logarithmically on 1/\u01eb (and polylogarithmically on n).\nTheorem 8.4. [MZ09] Let 0 < \u01eb < 1 and G : {0, 1}R \u2192({0, 1}D)T be a PRG that \u03b4-fools monotone\n(log(4T/\u01eb), D, T)-branching programs. Then G (\u01eb+\u03b4)-fools monotone (S, D, T)-branching programs\nfor arbitrary S with error at most \u01eb + \u03b4.\nWe now generalize Theorem 8.4 to the intersection of monotone branching programs, or even to\nany monotone function of monotone branching programs. (Of course, the intersection corresponds\nto the monotone function AND.)\n23\nTheorem 8.5. Let 0 < \u01eb < 1 and G : {0, 1}R \u2192({0, 1}D)T be a PRG that \u03b4-fools monotone\n(d log(4Td/\u01eb), D, T)-branching programs. Then G (\u01eb + \u03b4)-fools any monotone function of d mono-\ntone (S, D, T)-branching programs for arbitrary S.\nWe now generalize monotone functions to decision trees. First note that the complement of\na monotone branching program is a monotone branching program.\nNow consider any decision\ntree, where each node of the decision tree is a monotone branching program. Any leaf of this tree\nrepresents the intersection of monotone branching programs. Thus, the error of the function above\nfor such decision trees is at most s times the error for each leaf. This gives the following corollary.\nCorollary 8.6. Let 0 < \u01eb < 1 and G : {0, 1}R \u2192({0, 1}D)T be a PRG that \u03b4-fools monotone\n(d log(4Td/\u01eb), D, T)-branching programs. Then G (s(\u01eb + \u03b4))-fools any decision tree with s leaves,\nwhere each decision tree node is a monotone (S, D, T)-branching programs for arbitrary S.\nIn the above, we can even take s to be the minimum of the number of 0 and 1 leaves. We\nnow prove Theorem 8.5, using the ideas of [MZ09] based on \u201csandwiching\u201d monotone branching\nprograms between small-width branching programs.\nDe\ufb01nition 8.7. A pair of functions (fdown, fup), each with the same domain and range as a function\nf : B \u2192{0, 1}, is said to \u01eb-sandwich f if the following hold.\n1. For all z \u2208B, fdown(z) \u2264f(z) \u2264fup(z).\n2. Prz\u2208uB[fup(z) = 1] \u2212Prz\u2208uB[fdown(z) = 1] \u2264\u01eb.\nThe following lemma shows that it su\ufb03ces to fool functions which sandwich the given target\nfunction. Bazzi [Baz09] used sandwiching in showing that polylog-wise independence fools DNF\nformulas. The lemma below is a small modi\ufb01cation of a lemma in [MZ09].\nLemma 8.8. If (fdown, fup) \u01eb-sandwich f, and a PRG G \u03b4-fools fdown and fup, then G (\u01eb + \u03b4)-\nfools f.\nMeka and Zuckerman then showed that any monotone branching program can be sandwiched\nbetween two small-width branching programs.\nLemma 8.9. [MZ09] For any monotone (S, D, T)-branching program B, there exist monotone\n(log(4T/\u01eb), D, T)-branching programs (Bdown, Bup) that \u01eb-sandwich B.\nUsing this, we can show that any monotone function of monotone branching programs is sand-\nwiched by a small-width branching program.\nLemma 8.10. Any monotone function of d (S, D, T)-branching programs has a pair of (d log(4T/\u01eb), D, T)-\nbranching programs (Bdown, Bup) that (d\u01eb)-sandwich it.\nProof. For a monotone branching program B, let (Bdown, Bup) denote monotone (log(4T/\u01eb), D, T)-\nbranching programs that \u01eb-sandwich B, as given by Lemma 8.9. Suppose our given function is\nf(z) = g(B1(z), B2(z), . . . , Bd(z)) for g monotone. Then f(z) is sandwiched by (fdown, fup) given\nby\nfdown(z)\n=\nf\n\u0010\nBdown\n1\n(z), Bdown\n2\n(z), . . . , Bdown\nd\n(z)\n\u0011\nfup(z)\n=\nf\n\u0000Bup\n1 (z), Bup\n2 (z), . . . , Bup\nd (z)\n\u0001\n.\n24\nMoreover,\nf \u22121\nup (1) \u2212f \u22121\ndown(1) \u2286\nd[\ni=1\n\u0010\n(Bup\ni )\u22121(1) \u2212(Bdown\ni\n)\u22121(1)\n\u0011\n.\nSince Prz[Bup\ni (z) = 1] \u2212Prz[Bdown\ni\n(z) = 1] \u2264\u01eb, it follows that Prz[fup(z) = 1] \u2212Prz[fdown(z) =\n1] \u2264d\u01eb.\nTheorem 8.5 now follows from Lemmas 8.8 and 8.10. Without using any of the hard work we\u2019ve\ndone in other sections, this theorem gives us PRGs for monotone functions of halfspaces (such as\nintersections) using a random seed of length O(d(log n) log(n/\u01eb)). We improve this seed length now.\n8.1\nCombining the Monotone Trick and the main construction\nFix a hash function h, which \ufb01xes the partition into t sets.\nThen any monotone function of\n\u2212\u2192\nsgn(y1W1 + . . . + ynWn \u2212\u0398) may be computed by a monotone function of d monotone branching\nprograms, with t layers each.\nThus, we can apply Theorem 8.5 and Corollary 8.6 to deduce\nTheorem 1.2.\nWe can set T = t and D = O(log n) to store the seed for the 5-wise independent distribution.\nAlso note that log \u03b7\u22121 = \u0398(log C). With these parameters, using Nisan\u2019s PRG gives a seed length of\nO((d log(dT/\u01eb)+D +log T) log T) = O((d log(Cd/\u01eb)+log n) log(Cd/\u01eb)) to fool monotone functions\nof d halfspaces. For functions computable by size s decision trees of halfspaces, the seed length\nbecomes O((d log(Cds/\u01eb) + log n) log(Cds/\u01eb)).\nWhen Cd/\u01eb \u2265log\u2212c n for any c > 0, then t = polylog(n) and we can use the Nisan-Zuckerman\nPRG. This gives a seed length of O(d log(dT/\u01eb)+D +log T) = O(d log(Cd/\u01eb)+log n) for monotone\nfunctions of d halfspaces. For functions computable by size s decision trees of halfspaces, the seed\nlength becomes O(d log(Cds/\u01eb) + log n).\nMore generally, using Armoni\u2019s interpolation of Nisan and Nisan-Zuckerman will shave o\ufb00an\nextra log log n factor o\ufb00of Nisan\u2019s PRG when t/\u01eb \u2264exp(\u2212(log n)1\u2212\u03b3) for some \u03b3 > 0. We omit the\ndetails.\n9\nDiscretizing the distribution\nThe \ufb01rst step is to truncate each xi to lie in the range (\u2212B, B).\nLemma 9.1. Set B = (nC2\u01eb\u22121)\n1\n4 . For each i \u2208[n], let yi = xi \u00b7 I(|xi| < B). De\ufb01ne the product\nrandom variable Y = (y1, y2, . . . , yn) where the yis are independent. Then we have\n\u2022 SD(X, Y ) \u2264\u01eb.\n\u2022 E[y2\ni ] \u22651\n2, E[y4\ni ] \u2264C.\nProof. Note that xi = yi when |xi| \u2264B and yi = 0 otherwise. But we have\nPr[|xi| \u2265B] = Pr[|xi|4 \u2265B4] \u2264C\nB4 =\n\u01eb\nnC .\nThus it follows that\nSD(xi, yi) \u2264\n\u01eb\nnC \u21d2SD(X, Y ) \u2264\u01eb\nC \u2264\u01eb.\nIt is clear that E[y4\ni ] \u2264E[x4\ni ] \u2264C. Thus we only need to prove the claim about the two-norm.\nWe have\nxi = xi \u00b7 I(|xi| < B) + xi \u00b7 I(|xi| \u2265B) = yi + xi \u00b7 I(|xi| \u2265B)\n25\nfrom which it follows that\nE[x2\ni ] = E[y2\ni ] + E[x2\ni \u00b7 I(|xi| \u2265B)].\nBy the Cauchy-Schwartz inequality, we have\nE[x2\ni \u00b7 I(|xi| \u2265B)] \u2264E[x4\ni ]\n1\n2(Pr[|xi| \u2265B)])\n1\n2 \u2264\n\u221a\nC\nr \u01eb\nnC =\nr \u01eb\nn < 1\n2\nHence we have E[y2\ni ] \u22651\n2.\nBy a similar argument, one can show that | E[yi]| \u2264\n\u01eb\nnC .\nBy suitable shifting and rescaling, we can assume that the distribution satis\ufb01es E[xi] = 0, E[x2\ni ] =\n1, E[x4\ni ] \u2264C and |xi| < B .\nThe next step is to suitably discretize the distribution. Assume that the random variable xi has\na cumulative distibution function Fi where Fi(x) = Pr[xi \u2264x]. Since |xi| < B we have F(\u2212B) = 0\nand F(B) = 1. We will de\ufb01ne two sandwiching discrete distributions x\u2113\ni and xu\ni whose cdfs F \u2113\ni and\nF i\nu satisfy:\nF \u2113\ni (x) \u2264Fi(x) \u2264F \u2113\ni (x) + \u03b3\nF u\ni (x) \u2212\u03b3 \u2264Fi(x) \u2264F u\ni (x)\nwhere \u03b3 is a granularity paramater (which will be chosen as inverse polynomial in n).\nLet g = 1\n\u03b3 . Our goal is to de\ufb01ne bucket boundaries b0, . . . , bg by picking bk that stisfy Fi(bk) =\nk\u03b3.\nDe\ufb01nition 9.2. For k \u2208{0, . . . , g}, let bk be the smallest x \u2208[\u2212B, B] so that Fi(x) \u2265k\u03b3.\nWe can sample xi by \ufb01rst picking a bucket k \u2208{0, . . . , g \u22121} and then sampling from this\nbucket according to the suitable conditional distribution, resulting in xi \u2208[bk, bk+1].\nWe now de\ufb01ne the sandwiching distributions:\nDe\ufb01nition 9.3. The random variable x\u2113\ni is uniformly distributed on {b0, . . . , bg\u22121} while xu\ni the\nuniform distributed on {b1, . . . , bg}. We de\ufb01ne the family F of 2n product distributions on Rn where\neach co-ordinate is distributed independently according to x\u2113\ni or xu\ni .\nIt follows that SD(x\u2113\ni, xu\ni ) \u2264\u03b3. Hence if we take any pair of variables Y , Z from F, by the\nunion bound we have SD(Y , Z) \u2264\u03b3n. The following lemma allows us to reduce the problem of\nfooling halfspaces under the distribution X to the problem of fooling a single distribution from the\nfamily F.\nLemma 9.4. Let h : Rn \u2192{\u22121, 1} for i \u2208[k] be a halfspace and let Y \u2208F. Then\n| E[h(X)] \u2212E[h(Y )]| \u22644\u03b3n.\nProof. We will pick sandwiching distributions Y \u2113= (y\u2113\n1, . . . , y\u2113\nn) and Y u = (yu\n1, . . . , yu\nn) from F\n(depending on the halfspace h) and construct a coupling of the three distributions Y \u2113, X and Y u\nso that\nh(Y \u2113) \u2264h(X) \u2264h(Y u).\n(16)\nLet h(x) = sgn(P\ni wixi \u2212\u03b8). If wi \u22650 for all i, then we set\ny\u2113\ni = x\u2113\ni,\nyu\ni = xu\ni .\n26\nWhereas if wi < 0, then we set\ny\u2113\ni = xu\ni ,\nyu\ni = x\u2113\ni.\nNext we describe the coupling, co-ordinate by co-ordinate.\nFix co-ordinate i.\nPick k \u2208\n{0, . . . , g \u22121} at random. Set x\u2113\ni = bk and xu\ni = bk+1. We now set the random variables yi, yi\u2113\nand yu\ni to be eiher x\u2113\ni or xu\ni , based on their de\ufb01ntion. We pick xi conditioned on the kth bucket,\nso that bk \u2264xi \u2264bk+1. It follows that\nwiy\u2113\ni \u2264wixi \u2264wiyu\ni\nand hence\nX\ni\nwiy\u2113\ni \u2264\nX\ni\nwixi \u2264\nX\ni\nwiyu\ni\nwhich implies Equation 16.\nSince a halfspace is a statistical test, we have\nPr[h(X) \u0338= h(Y u)] \u2264Pr[h(Y \u2113) \u0338= h(Y u)] \u2264SD(Y u, Y \u2113) \u2264\u03b3n.\n(17)\nIf we replace Y u with Y \u2208F, we have\nPr[h(X) \u0338= h(Y )] \u2264Pr[h(X) \u0338= h(Y u)]| + Pr[h(Y ) \u0338= h(Y u)]| \u22642\u03b3n\nwhere we use Equations 17 and the fact that SD(Y , Y u) \u2264\u03b3n. The claim follows since h(X) and\nh(Y ) take values over {\u22121, 1}.\nThis lemma extends to fooling functions of halfspaces.\nLemma 9.5. Let f : Rn \u2192{\u22121, 1} be a function of d halfsapces hi : Rn \u2192{\u22121, 1} given by\nf = g(h1, . . . , hd) where g : {\u22121, 1}k \u2192{\u22121, 1}. Then for any Y \u2208F,\n| E[f(X)] \u2212E[f(Y )]| \u22644\u03b3dn.\nProof. We consider the same coupling used in Lemma 9.4. We have\nPr[g(X) \u0338= g(Y )] \u2264Pr[(hi(X), . . . , hd(X)) \u0338= (h1(Y ), . . . , hd(Y ))] \u2264\nX\ni\nPr[hi(X) \u0338= hi(Y )] \u22642\u03b3dn.\nThe claim now follows since g is Boolean valued.\nFinally, we need to show that for a suitable choice of \u03b3, the expectation and the second and\nfourth moments of x\u2113\ni and xu\ni are nearly the same as those of xi. We prove the claim for the fourth\nmoment, the other arguments are similar.\nLemma 9.6. We have\n| E[(xi)4] \u2212E[(xu\ni )4]| \u22642B4\u03b3,\n| E[(xi)4] \u2212E[(x\u2113\ni)4]| \u22642B4\u03b3\nProof. It is clear that\nE[(x\u2113\ni)4] = \u03b3(\ng\u22121\nX\nk=0\nb4\nk),\nE[(xu\ni )4] = \u03b3(\ng\nX\nk=1\nb4\nk).\n27\nOur goal is to compare these with the 4th moment of xi. The contribution of the kth bucket to\nE[x4\ni ] can be upper bounded by \u03b3 max(b4\nk, b4\nk+1) and lower bounded by \u03b3 min(b4\nk, b4\nk+1). Hence\n\u03b3\ng\u22121\nX\nk=0\nmin(b4\nk, b4\nk+1) \u2264E[x4\ni ] \u2264\u03b3\ng\u22121\nX\nk=0\nmax(b4\nk, b4\nk+1).\nBy case analysis, the sequence max(b4\nk, b4\nk+1) takes on g distinct values from {b0, . . . , bg}. Simi-\nlarly, min(b4\nk, b4\nk+1) can take some value twice but every other value at most once. Hence both the\nupper and lower bounds are within 2B4\u03b3 of both E[(x\u2113\ni)4] and E[(xu\ni )4].\nA similar argument shows that the second moment changes by at most 2B2\u03b3 and the expectation\nby 2B\u03b3. We pick \u03b3 <\n\u01eb\n2nB4 = O(\n\u01eb2\nn2C2 ), which is of the form 2\u2212s for some integer s. We have\n2s < O(n2C2\n\u01eb2 ) hence s = log(n2C2/\u01eb2) + O(1). To sample from x\u2113\ni (Xu\ni ), we pick a random bit-\nstring of length s, treat it as a number j \u2208{0, g \u22121}, and output bj (bj+1).\nFinally we rescale and shift, so that we again have E[yi] = 0, E[y2\ni = 1] and E[y4\ni ] \u2264C.\n10\nBounded Independence fools functions of halfspaces\nIn this section, we prove Theorem 1.5.\n10.1\nReduction to upper polynomials for single halfspaces\nWe now \ufb02esh out the reduction described in Section 2, i.e., we show how to prove Theorem 2.5\ngiven upper sandwiching polynomials for a single halfspace with extra properties.\nLemma 10.1. Let X be a random vector on the product set \u2126, and suppose we have order-k\npolynomials p1, . . . , pd : \u2126\u2192R, as well as functions h1, . . . , hd : \u2126\u2192{0, 1}. Write p = pi(X) and\nhi = hi(X). Assume that for each i \u2208[k]:\n1. p \u2265hi with probability 1;\n2. E[p \u2212hi] \u2264\u01eb0;\n3. Pr[p > 1 + 1/d2] \u2264\u03b3;\n4. \u2225\u2225\u2225p\u2225\u2225\u22252d \u22641 + 2/d2.\nIf we write p = p1p2 \u00b7 \u00b7 \u00b7 pd, h = h1h2 \u00b7 \u00b7 \u00b7 hd, then p is a polynomial of order at most dk, p(X) \u2265h(X)\nwith probability 1, and\nE[p(X) \u2212h(X)] \u22642d\u01eb0 + 3d2\u221a\u03b3.\n(18)\nProof. The \ufb01rst two parts of the claim are immediate, so it su\ufb03ces to verify (18). We use the\ntelescoping sum (1), and thus it su\ufb03ces to bound the general term as follows:\nE[h1 \u00b7 \u00b7 \u00b7 hi\u22121(p \u2212hi)pi+1 \u00b7 \u00b7 \u00b7 pd] \u22642\u01eb0 + 3d\u221a\u03b3.\n(19)\nWe have\nE[h1 \u00b7 \u00b7 \u00b7 hi\u22121(pi \u2212hi)pi+1 \u00b7 \u00b7 \u00b7 pd]\n\u2264\nE[p1 \u00b7 \u00b7 \u00b7 pi\u22121(pi \u2212hi)pi+1 \u00b7 \u00b7 \u00b7 pd]\n<\n2 E[pi \u2212hi] + E[1[p1 \u00b7 \u00b7 \u00b7 pi\u22121pi+1 \u00b7 \u00b7 \u00b7 pd \u22652] p1 \u00b7 \u00b7 \u00b7 pi\u22121(pi \u2212hi)pi+1 \u00b7 \u00b7 \u00b7 pd]\n\u2264\n2\u01eb0 + E\n\u0014\u0012 dP\ni\u2032=1\n1\n\u0002\npi\u2032 > 1 + 1/d2\u0003\u0013\ndQ\ni=1\npi\n\u0015\n,\n28\nwhere in the last term we used the bounds (1 + 1/d2)d\u22121 < 2 and pi \u2212hi \u2264pi. Thus we can\nestablish (19) by showing the bound\nd\nX\ni\u2032=1\nE\n\u0014\n1\n\u0002\npi\u2032 > 1 + 1/d2\u0003\ndQ\ni=1\npi\n\u0015\n\u22643d\u221a\u03b3.\nThis follows by bounding each summand individually:\nE\n\u0014\n1\n\u0002\npi\u2032 > 1 + 1/d2\u0003\ndQ\ni=1\npi\n\u0015\n\u2264\n\u2225\u2225\u22251\n\u0002\npi\u2032 > 1 + 1/d2\u0003\n\u2225\u2225\u22252 \u00b7\nd\nY\ni=1\n\u2225\u2225\u2225pi\u2225\u2225\u22252d\n(H\u00a8older\u2019s inequality)\n\u2264\n\u221a\u03b3 \u00b7 (1 + 2/d2)d\n\u2264\n3\u221a\u03b3,\nas needed.\n10.2\nTools for upper polynomials\nWe construct the upper sandwiching polynomial needed in Lemma 10.1 using two key tools:\n\u201cDGJSV Polynomials\u201d, the family of univariate real polynomial constructed in [DGJ+09] for ap-\nproximating the sgn function; and, our Regularity Lemma for halfspaces over general random\nvariables 5.3.\nRegarding the DGJSV Polynomials, the following is a key theorem from [DGJ+09] (slightly\nadjusted for our purposes):\nTheorem 10.2. ([DGJ+09]) Let 0 < a, b < 1. Then there exists an even integer K = Ka,b with\nK \u2264C0\nlog(2/b)\na\n(C0 is a universal constant)\nas well as an ordinary univariate real polynomial P = Pa,b : R \u2192R of degree K with the following\nbehavior:\n\u2022 P(x) \u22650 for x \u2208(\u2212\u221e, \u22121],\n\u2022 0 \u2264P(x) \u2264b for x \u2208[\u22121, \u2212a];\n\u2022 0 \u2264P(x) \u22641 for x \u2208[\u2212a, 0];\n\u2022 1 \u2264P(x) \u22641 + b for x \u2208[0, 1];\n\u2022 P(x) \u22651 for x \u2208[1, \u221e);\n\u2022 P(x) \u2264(4x)K for all |x| \u22651.\nNote that the \ufb01rst \ufb01ve conditions imply P(x) \u22651[x \u22650] for all x \u2208R.\nRegarding our Regularity Lemma for general halfspaces, we will use the following rephrasing of\nTheorem 5.3 with simpli\ufb01ed parameters:\n29\nTheorem 10.3. Let t > 1, 0 < \u03b4 < 1 and 0 < \u03b7 be parameters. Then there exists an integer L\nsatisfying\nL \u2264poly(log t, 1/\u03b7) \u00b7 1\n\u03b4\nsuch that the following holds. Suppose x1, . . . , xn is a sequence of independent \u03b7-HC random vari-\nables, \u03b8 \u2208R, and n \u2265L. Then there exists a set of coordinates H \u2286[n] of cardinality L such that,\ndenoting\n\u03b8\u2032 = \u03b8 \u2212\nX\nj\u2208H\nxj,\nz =\nX\nj\u0338\u2208H\nxj\n(these random variables are independent), we have three mutually exclusive and collectively exhaus-\ntive events depending only on \u03b8\u2032:\n\u2022 Event BAD: |\u03b8\u2032| \u2264t\u2225\u2225\u2225z\u2225\u2225\u22252 and the collection {xj : j \u0338\u2208H} is not \u03b4-regular;\n\u2022 Event NEAR: |\u03b8\u2032| \u2264t\u2225\u2225\u2225z\u2225\u2225\u22252 and the collection {xj : j \u0338\u2208H} is \u03b4-regular;\n\u2022 Event FAR: |\u03b8\u2032| > t\u2225\u2225\u2225z\u2225\u2225\u22252.\nFurthermore, BAD has probability at most O(1/t4).\nThe reader will note that events BAD, NEAR, and FAR are de\ufb01ned somewhat peculiarly:\nNeither \u2225\u2225\u2225z\u2225\u2225\u22252 nor the (ir)regularity of {xj : j \u0338\u2208H} is actually random. Furthermore, by our original\nTheorem 5.3, we either have that {xj : j \u0338\u2208H} is \u03b4-regular, in which case NEAR and FAR are\nthe only possible events, or the collection is not \u03b4-regular, in which case BAD and FAR are the\nonly possible events. Nevertheless, this tripartition of events makes our future analysis simpler.\n10.3\nStatement of the main technical theorem, and how it completes the proof\nThe main technical result we will prove is the following:\nTheorem 10.4. Let k \u22651, 0 < \u03b4 < 1, and t > 4 be parameters. Let X = (x1, . . . , xn) be a vector\nof independent \u03b7-HC random variables. Furthermore, let T be an even integer such that the xi\u2019s\nare (T, 2, 4/t)-hypercontractive. Assume T \u2265C1d log(dt), where C1 is a universal constant. Let\n\u03b8 \u2208R and let\nh(x1, . . . , xn) = 1[x1 + \u00b7 \u00b7 \u00b7 + xn \u2212\u03b8 \u22650] .\nThen there exists a polynomial p(x1, . . . , xn) of order k, with\nk \u2264poly(log t, 1/\u03b7) \u00b7 1\n\u03b4 + O(T/d),\nsatisfying the 4 properties appearing in Lemma 10.1, with\n\u01eb0 = O(\n\u221a\n\u03b4) + O(\u01eb1),\n\u01eb1 = dt log(dt)\nT\n,\n\u03b3 = 2\u2212T/d.\nAs we now show, using Theorem 10.4 and Lemma 10.1, we can deduce Theorem 2.5 and hence\nTheorem 1.5 simply by choosing parameters appropriately. Note that it is su\ufb03cient to prove The-\norem 2.5 with \u01eb \u00b7 polylog(d/\u01eb) in place of \u01eb.\nWe will apply Theorem 10.4 with \u03b4 = \u0398(\u01eb2/d2) and\nt = C2\nd2\n\u01eb\u03b1,\n30\nwhere C2 is a large constant of our choosing. Regarding the hypercontractivity parameters, using\nFact 3.3, we may take\n\u03b7 = \u0398(\u03b1\u22121/4),\nT = \u0398(t2 \u00b7 \u03b1 ln(2/\u03b1)).\nThe necessary assumption that\nT \u2265C1d log(td)\n\u21d4\nC2\n2 \u00b7 \u0398\n\u0012d4 ln(2/\u03b1)\n\u01eb2\u03b1\n\u0013\n\u2265C1d log\n\u0012\nC2\nd3\n\u01eb\u03b1\n\u0013\nis valid provided that C2 is a su\ufb03ciently large constant.\nWe obtain from the theorem an upper \u01eb2-sandwiching polynomial for h with order\nk = eO(d2/\u01eb2) \u00b7 poly(1/\u03b1) + O(d3/\u01eb2) \u00b7 poly(1/\u03b1) \u2264O(d3/\u01eb2) \u00b7 poly(1/\u03b1),\nwhere\n\u01eb0 = O(\u01eb/d) + eO(\u01eb/d) = eO(\u01eb/d)\nand \u03b3 is exponentially small in d/(\u01eb\u03b1). By using such polynomials in Lemma 10.1, we get upper\nsandwiching polynomials for intersections of d halfspaces with the claimed degree kd = eO(d4/\u01eb2) \u00b7\npoly(1/\u03b1) and the claimed error d\u01eb0 = \u01eb \u00b7 polylog(d/\u01eb).\n10.4\nProof of Theorem 10.4\nIn this section, we prove Theorem 10.4. Let H be the set of cardinality L = poly(log t, 1/\u03b7) \u00b7 (1/\u03b4)\ncoming from Theorem 10.3, and assume without loss of generality that H = {1, . . . , L}. We use\nthe notation \u03b8\u2032 = \u03b8 \u2212(x1 + \u00b7 \u00b7 \u00b7 + xL), z = xL+1 + \u00b7 \u00b7 \u00b7 + xn, BAD = BAD(x1, . . . , xL) etc., with\nboldface indicating randomness as usual. Given the outcomes for x1, . . . , xL, we will handle the\nthree events BAD, NEAR, and FAR with separate ordinary real polynomials. More precisely,\nour \ufb01nal (generalized) polynomial will be\np(x1, . . . , xn) = 1[BAD] \u00b7 1 + 1[NEAR] \u00b7 pnear\n\u03b8\u2032\n(z) + 1[FAR] \u00b7 pfar\n\u03b8\u2032 (z),\nwhere\npnear\n\u03b8\u2032\n(z) = P\n\u0012 z \u2212\u03b8\u2032\n2t\u2225\u2225\u2225z\u2225\u2225\u22252\n\u0013\n,\nand\npfar\n\u03b8\u2032 (z) = 1\n\u0002\n\u03b8\u2032 > 0\n\u0003\n\u00b7 1 + 1\n\u0002\n\u03b8\u2032 \u22640\n\u0003\n\u00b7\n\u0010 z\n\u03b8\u2032\n\u0011q\n,\nwhere q is a positive integer and P is an ordinary real univariate polynomial to be speci\ufb01ed later.\nFor typographic simplicity, we will write simply p\u03b8\u2032 in place of pnear\n\u03b8\u2032\nand pfar\n\u03b8\u2032 , with context dictating\nwhich we are referring to.\nLet us walk through the properties of p we need to prove. Regarding its order, we will prove\nthat both\nq \u2264O(T/d),\ndeg P \u2264O(T/d);\ni.e. when \u03b8\u2032 is \ufb01xed, p\u03b8\u2032(xL+1, . . . , xn) has degree at most O(T/d) as an ordinary multivariate real\npolynomial. Since \u03b8\u2032, BAD, NEAR, and FAR are determined by x1, . . . , xL alone, it follows that\nour \ufb01nal polynomial p is a generalized polynomial of order at most L + O(T/d), as needed for the\ntheorem.\n31\nNext, we discuss Condition 1, that p(X) \u2265h(X) always. For the BAD outcomes for x1, . . . , xL\nwe have p(X) = 1 \u2265h(X). For the remaining outcomes, we will have p(X) \u2265h(X) as required\nprovided that in all cases\np\u03b8\u2032(z) \u2265h\u03b8\u2032(z)\nfor all \u03b8\u2032 and z\n(20)\nwhere\nh\u03b8\u2032(z) = 1\n\u0002\nz \u2212\u03b8\u2032 \u22650\n\u0003\n.\nNext, we discuss Condition 2, the bound E[p(X) \u2212h(X)] \u2264\u01eb1. It su\ufb03ces to prove an upper\nbound of O(\u01eb1). Recall that\n\u01eb1 = dt log(dt)\nT\n.\nNote also that we will always T \u2264t2, since no random variable has stronger hypercontractivity\nthan do Gaussians, for which T \u22641 + t2/16. It follows that we will always have \u01eb1 \u22651/t. Thus\nthe probability of BAD, which is at most O(1/t4), is much smaller than O(\u01eb1) and can therefore\nbe neglected. Hence it su\ufb03ces to show that\nE[p\u03b8\u2032(z) \u2212h\u03b8\u2032(z)] \u2264O(\u01eb1)\n(21)\nholds in both of the following cases:\nCase Near:\n|\u03b8\u2032| \u2264t\u2225\u2225\u2225z\u2225\u2225\u22252 and the collection {xL+1, . . . , xn} is \u03b4-regular.\nCase Far:\n|\u03b8\u2032| > t\u2225\u2225\u2225z\u2225\u2225\u22252.\nNext we discuss Condition 3, the bound Pr[p(X) > 1 + 1/d2] \u22642\u2212T/d. Again, since p(X) = 1\nfor the bad outcomes x1, . . . , xL, it su\ufb03ces to show that\nPr[p\u03b8\u2032(z) > 1 + 1/d2] \u22642\u2212T/d\n(22)\nholds in both Case a and Case b.\nFinally, we discuss the bound \u2225\u2225\u2225p(X)\u2225\u2225\u22252d \u22641 + 2/d2. We have\nE[p(X)2k] \u2264(1+1/d2)2d+E[p(X)2d\u00b71\n\u0002\np(X) > 1 + 1/d2\u0003\n] \u22641+3/d+E[p(X)2d\u00b71\n\u0002\np(X) > 1 + 1/d2\u0003\n].\nIf we can show that\nE[p(X)2d \u00b7 1\n\u0002\np(X) > 1 + 1/d2\u0003\n] \u22641/d,\nthen we will have shown\nE[p(X)2d] \u22641 + 4/d \u2264(1 + 2/d2)2d,\nas required. Thus it remains to establish the previous upper bound. Again, since p(X) = 1 for the\nBAD outcomes x1, . . . , xL, it su\ufb03ces to show that\nE[p\u03b8\u2032(z)2d \u00b7 1\n\u0002\np\u03b8\u2032(z) > 1 + 1/d2\u0003\n] \u22641/d\n(23)\nholds in both Case Near and Case Far.\nSummarizing, our goal is to construct univariate polynomials p\u03b8\u2032(z) of degree at most O(T/d)\nfor each of Case Near and Case Far so that (20), (21), (22), and (23) all hold. We will \ufb01rst handle\nCase Near, the more di\ufb03cult case.\n32\n10.4.1\nCase Near\nIn this case we have |\u03b8\u2032| \u2264t\u2225\u2225\u2225z\u2225\u2225\u22252, where z = xL+1 + \u00b7 \u00b7 \u00b7 + xn is the sum of a \u03b4-regular collection\nof independent random variables. Our task is to construct a real polynomial p\u03b8\u2032(z) of degree at\nmost O(T/d) such that bounds (20), (21), (22), and (23) all hold with respect to the function\nh\u03b8\u2032(z) = 1[z \u2212\u03b8\u2032 \u22650].\nGiven the parameters d and t, choose\na = 16C0\nd log(td)\nT\n,\nb = min(1/d2, 1/t4);\nwe have a < 1 assuming that the C1 in our assumption on T is large enough. Let K = Ka,b and\nP = Pa,b be the resulting even integer and univariate polynomial from Theorem 10.2. Our choice\nof a was arranged so that\nK \u2264T\n4d.\n(24)\nWe will de\ufb01ne\np\u03b8\u2032(z) = pnear(\u03b8\u2032, z) = P(w),\nwhere w = z \u2212\u03b8\u2032\n2t\u2225\u2225\u2225z\u2225\u2225\u22252\n.\nThus p\u03b8\u2032(z) has degree K = O(T/d) as necessary, and it also satis\ufb01es (20), using the property\nthat P \u22650 on (\u2212\u221e, 0] and P \u22651 on [0, \u221e).\nNext we check (23). i.e.,\nE[p\u03b8\u2032(z)2d \u00b7 1\n\u0002\np\u03b8\u2032(z) > 1 + 1/d2\u0003\n] \u22641/d.\nSince b \u22641/d2, we have that p\u03b8\u2032(z) > 1 + 1/d2 only if |w| \u22651.\nAlso notice that p\u03b8\u2032(z) \u2264(4w)K, it su\ufb03ce to bound E[1[|w| \u22651] \u00b7 (4w)2dK] and we will prove a\nstronger result:\nE[(4w)2dK \u00b7 1[|w| \u22651]] \u22642\u2212T .\n(25)\nTo see this, since we are in Case Near we have |\u03b8\u2032| < t\u2225\u2225\u2225z\u2225\u2225\u22252. Thus if |w| \u22651, we must have\n|z| > t\u2225\u2225\u2225z\u2225\u2225\u22252. This also implies |z \u2212\u03b8\u2032| < 2 |z|; hence we have\n|4w| = 2|z \u2212\u03b8\u2032|\nt\u2225\u2225\u2225z\u2225\u2225\u22252\n< 4\nt \u00b7\n\f\f\f\f\nz\n\u2225\u2225\u2225z\u2225\u2225\u22252\n\f\f\f\f .\nThus we have\nE\nh\n1[|w| \u22651] \u00b7 (4w)2dKi\n\u2264\nE\n\"\n1[|z| > t\u2225\u2225\u2225z\u2225\u2225\u22252] \u00b7\n\u00124\nt\n\u00132dK \u0012\nz\n\u2225\u2225\u2225z\u2225\u2225\u22252\n\u00132dK#\n=\n\u00124\nt\n\u00132dK\n\u00b7 E\n\"\n1\n\u0014\f\f\f\f\nz\n\u2225\u2225\u2225z\u2225\u2225\u22252\n\f\f\f\f > t\n\u0015\n\u00b7\n\u0012\nz\n\u2225\u2225\u2225z\u2225\u2225\u22252\n\u00132dK#\n.\n(26)\nIt is easy to check that\n1\n\u0014\f\f\f\f\nz\n\u2225\u2225\u2225z\u2225\u2225\u22252\n\f\f\f\f > t\n\u0015\n\u00b7\n\u0012\nz\n\u2225\u2225\u2225z\u2225\u2225\u22252\n\u00132dK\n\u2264\n\u0012\nz\nt\u2225\u2225\u2225z\u2225\u2225\u22252\n\u0013T\n\u00b7 t2dK,\n33\nusing the fact that 2dK \u2264T. Thus we may upper-bound (26) by\n42dKt\u2212T \u2225\u2225\u2225z\u2225\u2225\u2225T\nT\n\u2225\u2225\u2225z\u2225\u2225\u2225T\n2\n\u226442dKt\u2212T(t/4)T = 42dK\u2212T ,\nwhere we used the (T, 2, 4/t)-hypercontractivity of z. Since we have\n2dK \u2264T/2,\n(27)\nby virtue of (24), we conclude\nE[p\u03b8\u2032(z)2d \u00b7 1\n\u0002\np\u03b8\u2032(z) > 1 + 1/d2\u0003\n] \u22644\u2212T/2 = 2\u2212T \u22641/d.\n(28)\nLet us move on to showing (22) in this Case Near; i.e., upper-bounding Pr[p\u03b8\u2032(z) > 1 + 1/d2].\nSince b \u22641/d2, again we have that p\u03b8\u2032(z) > 1/d2 only if |w| \u22651. But by (25)\nE[1[|w| \u22651] \u00b7 (4w)dK] \u22642\u2212T ,\nand the left-hand side is clearly an upper bound on Pr[|w| \u22651]. Thus we have established (22) in\nCase Near.\nLast, we will work to upper bound E[p\u03b8\u2032(z) \u2212h\u03b8\u2032(z)] so as to show (21) in Case Near. We\nanalyze three subcases, depending on the magnitude of w.\nCase i: \u2212a \u2264w \u22640.\nIn this case, we upper-bound p\u03b8\u2032(z) \u2212h\u03b8\u2032(z) simply by 1, and argue that\nCase i occurs with low probability. Speci\ufb01cally,\nPr[\u2212a \u2264w \u22640] \u2264Pr[|w| \u2264a] = Pr[\n\f\fz \u2212\u03b8\u2032\f\f \u22642ta \u00b7 \u2225\u2225\u2225z\u2225\u2225\u22252].\nWe can upper-bound this probability using the Berry-Esseen Theorem [MZ09, Corollary 4.5]. Since\nwe have \u03b4-regularity of xL+1, . . . , xn in Case Near, we get\nPr[\n\f\fz \u2212\u03b8\u2032\f\f \u22642ta \u00b7 \u2225\u2225\u2225z\u2225\u2225\u22252] \u2264O(\n\u221a\n\u03b4 + ta)\nBy de\ufb01nition of a we have O(ta) = O(\u01eb1). Thus we conclude for Case i,\nE[1[Case i] \u00b7 (p\u03b8\u2032(z) \u2212h\u03b8\u2032(z))] \u2264O(\n\u221a\n\u03b4 + \u01eb1).\n(29)\nCase ii:\n|w| \u22641 but not Case i.\nIn this case, we have p\u03b8\u2032(z) \u2212h\u03b8\u2032(z) \u2264b \u22641/t4, by\nconstruction. Thus\nE[1[Case ii] \u00b7 (p\u03b8\u2032(z) \u2212h\u03b8\u2032(z))] \u22641/t4 \u2264O(\u01eb1).\n(30)\nCase iii: |w| > 1.\nI.e., |z \u2212\u03b8\u2032| > 2t\u2225\u2225\u2225z\u2225\u2225\u22252. Notice that p\u03b8\u2032(z) \u2212h\u03b8\u2032(z) \u2264p\u03b8\u2032(z) and therefore\nE[1[Case iii] \u00b7 (p\u03b8\u2032(z) \u2212h\u03b8\u2032(z))] \u2264E[p\u03b8\u2032(z) \u00b7 1[|w| \u22651]] \u2264E\nh\n1[|w| \u22651] (4w)dKi\n\u22642\u2212T \u2264O(\u01eb1)\n(the second last inequality is due to (25)).\n34\n10.4.2\nCase Far\nIf \u03b8 < 0 then h\u03b8\u2032 is almost always 1. As stated, in this case we simply have p\u03b8\u2032(z) \u22611. Bounds (20),\n(22), and (23) become trivial; for (21) it su\ufb03ces to show\nPr[z \u2264\u03b8\u2032] \u2264\u01eb1.\n(31)\nWe will show a stronger statement in the course of handling the case that \u03b8\u2032 > 0.\nSo it remains to handle the \u03b8\u2032 > 0 case. As stated, in this case we de\ufb01ne\np\u03b8\u2032(z) = pfar(\u03b8\u2032, z) =\n\u0010 z\n\u03b8\u2032\n\u0011q\n,\nwhere\nq =\n\u0016 T\n2d\n\u0017\neven\n,\nmeaning T/2d rounded down to the nearest even integer. Note that p\u03b8\u2032(z) has the claimed degree\nbound O(T/d) (treating \u03b8\u2032 as a constant). Also note that p\u03b8\u2032(z) \u22651 if and only if |z| \u2265\u03b8\u2032. This\nestablishes (20).\nLet\u2019s move to (22); we need\nPr[p\u03b8\u2032(z) \u22651 + 1/d2] \u22642\u2212T/d.\nCertainly\np\u03b8\u2032(z) \u22651 + 1/d2\n\u21d2\np\u03b8\u2032(z) \u22651\n\u21d2\n|z| \u2265\n\f\f\u03b8\u2032\f\f .\nIt thus su\ufb03ces to show\nPr[|z| \u2265\n\f\f\u03b8\u2032\f\f] \u22642\u2212T/d,\nwhich, once shown, also establishes (31), since 2\u2212T/d \u226a\u01eb1. We will in fact show the stronger\nstatement\nE\nh\u0010 z\n\u03b8\u2032\n\u0011qi\n\u22642\u2212T/d.\n(32)\nAnd this stronger statement establishes (21), again because 2\u2212T/d \u2264\u01eb1.\nTo prove (32) we appeal to the condition of Case Far, |\u03b8\u2032| > t\u2225\u2225\u2225z\u2225\u2225\u22252. Thus\nE\nh\u0010 z\n\u03b8\u2032\n\u0011qi\n\u2264\nE\n\u0014\u0012\nz\nt\u2225\u2225\u2225z\u2225\u2225\u22252\n\u0013q\u0015\n\u2264\nE\n\"\u0012\nz\nt\u2225\u2225\u2225z\u2225\u2225\u22252\n\u0013T #q/T\n(Jensen, since T/q \u22651)\n=\nt\u2212q\n\u0012\u2225\u2225\u2225z\u2225\u2225\u2225T\nT\n\u2225\u2225\u2225z\u2225\u2225\u2225T\n2\n\u0013q/T\n\u2264\nt\u2212q\n\u0012 t\n4\n\u0013q\n(by (T, 2, 4/t)-hypercontractivity of z)\n=\n4\u2212q\n=\n2\u2212T/d,\nusing the de\ufb01nition of q.\n35\nFinally, to prove (23) it certainly su\ufb03ces to show\n1/d \u2265E[p\u03b8\u2032(z)2d] = E\n\u0014\u0010 z\n\u03b8\u2032\n\u00112d\u0015\n.\nBy repeating the previous inequality with 2d in place of q (we still have T/2d \u22651), we can upper-\nbound the expectation by 4\u22122d, which is indeed at most 1/d. This concludes the veri\ufb01cation of\nCase Far, and thus all of Theorem 10.4.\n11\nFooling the uniform distribution on the sphere\nIn this section, we will show that our PRG can also be used to fool any function of d halfspaces\nover the uniform distribution on the n dimensional unit sphere; building such a PRG also has an\napplication in derandomizing the hardness of learning reduction in [KS08].\nThe main idea is to show that the n dimensional Gaussian distribution can be use to fool the\nuniform distribution on the sphere. Therefore, it su\ufb03ce to fool the n dimensional Gaussian which is\nstudied in the previous sections (either using the modi\ufb01ed MZ generator or k-wise independence).\nSpeci\ufb01cally, we \ufb01rst show the following connection between the n dimensional Gaussian distri-\nbution N(0, 1/\u221an)n and the uniform distribution on the n dimensional unit sphere Sn\u22121.\nLemma 11.1.\nFor any \u03b81, \u03b82, ..\u03b8d \u2208R and W1, W2, ..Wd \u2208Rn and hi(X) = sgn(Wi \u00b7 X \u2212\u03b8i) and\nf : {0, 1}d \u2192{0, 1}, there is some universal constant C such that\n\f\f\nE\nX\u2208uSn\u22121[f(h1(X), .., hd(X))] \u2212\nE\nX\u2208uN (0,1/\u221an)n[f(h1(X), h2(X)..hd(X)]\n\f\f \u2264Cd log n\nn1/4\n(33)\nProof. Notice that if we choose x \u2208u N(0, 1/\u221an)n, then\nx\n\u2225x\u22252 follows the uniform distribution on\nthe sphere. Therefore, we only need to bound:\n\f\f\nE\nX\u2208uN (0,1/\u221an)n(f(h1(\nX\n\u2225X\u22252\n), .., hd(\nX\n\u2225X\u22252\n)) \u2212\nE\nx\u2208uN (0,1/\u221an)n f(h1(X), h2(X)..hd(X))|\n\u2264\nPr\nx\u2208uN (0,1/\u221an)n\n\u0000f(h1(\nX\n\u2225X\u22252\n), .., hd(\nX\n\u2225X\u22252\n)) \u0338= f(h1(X), h2(X)..hd(X))\n\u0001\n\u2264\nd\nX\ni=1\nPr\nx\u2208uN (0,1/\u221an)n(hi(\nX\n\u2225X\u22252\n) \u0338= hi(X))\n(34)\nBy Lemma 6.2 in [MZ09], we know that:\nPr\nX\u2208uN (0,1/\u221an)n(hi(\nX\n\u2225X\u22252\n) \u0338= hi(x)) \u2264C log n\nn1/4 .\nCombining above inequality with (34), we prove (33).\nTherefore to fool any function of d halfspaces over the uniform distribution on the n dimensional\nsphere with accuracy \u2126(C log n\nn1/4 ), it su\ufb03ce to build a PRG for n dimensional Gaussian distribution\nwith the same accuracy.\n36\n11.1\nDerandomized hardness of learning intersections of halfspaces\nOne of the application of above PRG is that we can use it to derandomize the hardness of learning\nresult in [KS08]. In [KS08], Khot and Saket showed that assuming NP\u0338=RP, for any \u01eb > 0 and\npositive integer d, given a set of examples such that there is a intersection of two halfspaces that\nis consistent with all the examples, it is NP-hard to \ufb01nd a function of any d halfspaces that is\nconsistent with a 1/2 + O(\u01eb) fraction of the examples. Our PRGs can be used to derandomize the\nhardness reduction and obtain the same hardness result assuming NP\u0338= P.\nTo see why our PRG works, we need to look into the details of [KS08].\nLet us explain in\nhigh level why our PRG helps, without entering into the details of the reduction. The hardness of\nlearning result in [KS08] is based on a reduction from a Label Cover instance L to a distribution D0\non negative examples and a distribution D1 on positive examples. Such a reduction would preserve\nthe following two properties:\n\u2022 (Completeness) if the optimum value of L is 1, then there is a intersection of two halfspaces\nf(x) that agrees with all the examples; i.e., ED1[f(X)] = ED0[f(X)] + 1.\n\u2022 (Soundness) if the optimum value of L is small, then for any h(x) which is a function of d\nhalfspaces, we have that\n\f\f ED0[h(X)] \u2212ED1[h(X)]\n\f\f = O(\u01eb) which implies that h(x) agrees\nwith at most 1/2 + O(\u01eb) fraction of the examples.\nThe Di for (i = 0, 1) constructed in [KS08] is a mixture of uniform distribution on the sphere\nlocated at di\ufb00erent center and the number of the di\ufb00erent spheres is poly(n), where n is the size\nof the Label Cover instance. Then by the PRG in this paper, we can derandomize each sphere\nwith some distribution that only has support of size poly(n) to \u01eb-fool functions of d halfspaces; and\noverall we can get distribution P0 and P1 with poly(n) support and it has the property that for any\nfunction h(x) of l halfspaces, | EDi[f(X)]\u2212EPi[f(X)]| \u2264O(\u01eb) for i = 0, 1. If we replace Di with Pi in\nthe hardness reduction, we still get the soundness guarantee that | EP1[f(X)]\u2212EP0[f(X)]| = O(\u01eb).\nWe also need to verify that the completeness property will hold if we replace Di with Pi. If we\nlook into the reduction of [KS08], as long as the distribution Pi has all its support points on the\nsphere, the reduction will preserve the completeness property. Therefore, to make the reduction\nwork, we need to build a PRG for functions of d-halfspaces over the uniform distribution on the\nsphere with the additional property that all the points generated by the PRG are all on the unit\nsphere as well.\nThis is also achievable and we summarize the high level idea here. As is shown in Lemma 11.1,\nit su\ufb03ce to fool functions of d halfspaces over n dimensional Gaussian instead of the uniform\ndistribution on the sphere. In addition, by the proof of Theorem 4.4, if we only want to fool any\nfunctions of d \u01eb-regular halfspaces, it su\ufb03ce just to fool uniform distribution on {\u22121/\u221an, 1/\u221an}n\ninstead. For the uniform distribution over {\u22121/\u221an, 1/\u221an}n. we know that it can be fooled by\nPRG with all the support points in {\u22121/\u221an, 1/\u221an}n which is a subset of the unit sphere. To\nhandle the case that d halfspaces are not all \u01eb-regular, we can follow the idea of [MZ09] Lemma 6.3\nby showing that there exists a set of poly(n) unitary rotations and with high probability that all\nof the d halfspaces become regular under a rotation randomly chosen from the set.\nReferences\n[Baz09]\nL. Bazzi.\nPolylogarithmic independence can fool DNF formulas. SIAM Journal on\nComputing, 38:2220\u20132272, 2009.\n37\n[BELY09]\nIdo Ben-Eliezer, Shachar Lovett, and Ariel Yadin.\nPolynomial threshold functions:\nStructure, approximation and pseudorandomness. In Submitted, 2009.\n[Ben04]\nVidmantas Bentkus. A Lyapunov type bound in Rd. Theory of Probability and its\nApplications, 49(2):311\u2013322, 2004.\n[Der65]\nM. Dertouzos. Threshold logic: a synthesis approach.\nMIT Press, Cambridge, MA,\n1965.\n[DGJ+09]\nIlias Diakonikolas,\nParikshit Gopalan, Ragesh Jaiswal, Rocco A. Servedio, and\nEmanuele Viola. Bounded independence fools halfspaces. In Proceedings of the 50th\nIEEE Symposium on Foundations of Computer Science, 2009.\n[DKN09]\nI. Diakonikolas, D. Kane, and J. Nelson. Bounded independence fools degree-2 threshold\nfunctions. In Submitted, 2009.\n[DS79]\nP. Dubey and L.S. Shapley.\nMathematical properties of the banzhaf power index.\nMathematics of Operations Research, 4:99\u2013131, 1979.\n[FGRW09] V. Feldman, V. Guruswami, P. Raghavendra, and Y. Wu. Agnostic learning of mono-\nmials by halfspaces is hard. In FOCS, 2009.\n[FKL+01]\nJ. Forster, M. Krause, S.V. Lokam, R. Mubarakzjanov, N. Schmitt, and H.-U. Simon.\nRelations between communication complexity, linear arrangements, and computational\ncomplexity. In FSTTCS, pages 171\u2013182, 2001.\n[GR09]\nP. Gopalan and J. Radhakrishnan. Finding duplicates in a data stream. In Proc. 20th\nAnnual Symposium on Discrete Algorithms (SODA\u201909), pages 402\u2013411, 2009.\n[HKM09]\nPrahladh Harsha, Adam Klivans, and Raghu Meka. An invariance principle for poly-\ntopes. In Submitted, 2009.\n[HMP+93] A. Hajnal, W. Maass, P. Pudlak, M. Szegedy, and G. Turan. Threshold circuits of\nbounded depth. Journal of Computer and System Sciences, 46:129\u2013154, 1993.\n[Hu65]\nS.T. Hu. Threshold Logic. University of California Press, 1965.\n[INW94]\nRussell Impagliazzo, Noam Nisan, and Avi Wigderson. Pseudorandomness for network\nalgorithms. In STOC, pages 356\u2013364, 1994.\n[Isb69]\nJ.R. Isbell. A Counterexample in Weighted Majority Games. Proceedings of the AMS,\n20(2):590\u2013592, 1969.\n[Kra91]\nM. Krause. Geometric arguments yield better bounds for threshold circuits and dis-\ntributed computing. In Proc. 6th Structure in Complexity Theory Conference, pages\n314\u2013322, 1991.\n[KS88]\nWies law Krakowiak and Jerzy Szulga. Hypercontraction principle and random multi-\nlinear forms. Probability Theory and Related Fields, 77(3):325\u2013342, 1988.\n[KS08]\nS. Khot and R. Saket. On hardness of learning intersection of two halfspaces. In STOC,\n2008.\n38\n[KW91]\nM. Krause and S. Waack. Variation ranks of communication matrices and lower bounds\nfor depth two circuits having symmetric gates with unbounded fanin. In Proc. 32nd\nIEEE Symposium on Foundations of Computer Science (FOCS), pages 777\u2013782, 1991.\n[LC67]\nP.M. Lewis and C.L. Coates. Threshold Logic. New York, Wiley, 1967.\n[MOO05]\nElchanan Mossel, Ryan O\u2019Donnell, and Krzysztof Oleszkiewicz. Noise stability of func-\ntions with low in\ufb02uences: invariance and optimality. In Proceedings of the 46th IEEE\nSymposium on Foundations of Computer Science, pages 21\u201330, 2005. To appear, Annals\nof Mathematics 2010.\n[Mos08]\nElchanan Mossel. Gaussian bounds for noise correlation of functions and tight analysis\nof long codes. In Proceedings of the 49th IEEE Symposium on Foundations of Computer\nScience, pages 156\u2013165, 2008.\n[Mur71]\nS. Muroga. Threshold logic and its applications. Wiley-Interscience, New York, 1971.\n[MZ09]\nRaghu Meka and David Zuckerman. Pseudorandom generators for polynomial threshold\nfunctions, 2009. arXiv:0910.4122 [cs.CC].\n[Nis92]\nNoam Nisan. Pseudorandom generators for space-bounded computation. Combinator-\nica, 12(4):449\u2013461, 1992.\n[NZ96]\nNoam Nisan and David Zuckerman. Randomness is linear in space. Journal of Computer\nand System Sciences, 52(1):43\u201352, 1996.\n[OS08]\nR. O\u2019Donnell and R. Servedio. The Chow Parameters Problem. In STOC, pages 517\u2013\n526, 2008.\n[Pen46]\nL.S. Penrose. The elementary statistics of majority voting. Journal of the Royal Sta-\ntistical Society, 109(1):53\u201357, 1946.\n[RS08]\nY. Rabani and A. Shpilka. Explicit construction of a small epsilon-net for linear thresh-\nold functions. In STOC, 2008.\n[Ser07]\nR. Servedio. Every linear threshold function has a low-weight approximator. Compu-\ntational Complexity, 16(2):180\u2013209, 2007.\n[She69]\nQ. Sheng. Threshold Logic. London, New York, Academic Press, 1969.\n[Szu90]\nJerzy Szulga. A note on hypercontractivity of stable random variables. The Annals of\nProbability, 18(4):1746\u20131758, 1990.\n[TZ92]\nA. Taylor and W. Zwicker. A Characterization of Weighted Voting. Proceedings of the\nAMS, 115(4):1089\u20131094, 1992.\n[Wol06a]\nPawe l Wol\ufb00. Hypercontractivity of random variables and geometry of linear normed\nspaces, 2006. Unpublished.\n[Wol06b]\nPawe l Wol\ufb00.\nHypercontractivity of simple random variables.\nStudia Mathematica,\n180(3):219\u2013236, 2006.\n39\n",
        "sentence": "",
        "context": "tions with low in\ufb02uences: invariance and optimality. In Proceedings of the 46th IEEE\nSymposium on Foundations of Computer Science, pages 21\u201330, 2005. To appear, Annals\nof Mathematics 2010.\n[Mos08]\nCCF-0747250 and CCF-0915893, BSF grant 2008477, and Sloan and Okawa fellowships.\n\u2020Work done while an intern at Microsoft Research SVC.\n\u2021Work was partially done while the author consulted at Microsoft Research SVC. Partially supported by NSF\ntensively in theoretical computer science, social choice theory, and machine learning. In computer\nscience, they were \ufb01rst studied in the context of switching circuits; see for instance [Der65, Hu65,"
    },
    {
        "title": "Hardness of learning halfspaces with noise",
        "author": [
            "V. Guruswami",
            "P. Raghavendra"
        ],
        "venue": "In Proceedings of FOCS",
        "citeRegEx": "Guruswami and Raghavendra.,? \\Q2006\\E",
        "shortCiteRegEx": "Guruswami and Raghavendra.",
        "year": 2006,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Sketching and streaming entropy via approximation theory",
        "author": [
            "Nicholas J.A. Harvey",
            "Jelani Nelson",
            "Krzysztof Onak"
        ],
        "venue": "In FOCS,",
        "citeRegEx": "Harvey et al\\.,? \\Q2008\\E",
        "shortCiteRegEx": "Harvey et al\\.",
        "year": 2008,
        "abstract": "We conclude a sequence of work by giving near-optimal sketching and streaming\nalgorithms for estimating Shannon entropy in the most general streaming model,\nwith arbitrary insertions and deletions. This improves on prior results that\nobtain suboptimal space bounds in the general model, and near-optimal bounds in\nthe insertion-only model without sketching. Our high-level approach is simple:\nwe give algorithms to estimate Renyi and Tsallis entropy, and use them to\nextrapolate an estimate of Shannon entropy. The accuracy of our estimates is\nproven using approximation theory arguments and extremal properties of\nChebyshev polynomials, a technique which may be useful for other problems. Our\nwork also yields the best-known and near-optimal additive approximations for\nentropy, and hence also for conditional entropy and mutual information.",
        "full_text": "arXiv:0804.4138v1  [cs.DS]  25 Apr 2008\nSketching and Streaming Entropy\nvia Approximation Theory\nNicholas J. A. Harvey\u2217\nJelani Nelson\u2020\nKrzysztof Onak\u2021\nAbstract\nWe conclude a sequence of work by giving near-optimal sketching and streaming algorithms\nfor estimating Shannon entropy in the most general streaming model, with arbitrary inser-\ntions and deletions. This improves on prior results that obtain suboptimal space bounds\nin the general model, and near-optimal bounds in the insertion-only model without sketch-\ning. Our high-level approach is simple: we give algorithms to estimate Renyi and Tsallis\nentropy, and use them to extrapolate an estimate of Shannon entropy.\nThe accuracy of\nour estimates is proven using approximation theory arguments and extremal properties of\nChebyshev polynomials, a technique which may be useful for other problems. Our work also\nyields the best-known and near-optimal additive approximations for entropy, and hence also\nfor conditional entropy and mutual information.\n1\nIntroduction\nStreaming algorithms have attracted much attention in several computer science communities,\nnotably theory, databases, and networking. Many algorithmic problems in this model are now\nwell-understood, for example, the problem of estimating frequency moments [1, 2, 10, 18, 32, 35].\nMore recently, several researchers have studied the problem of estimating the empirical entropy\nof a stream [3, 6, 7, 12, 13, 37].\nMotivation. There are two key motivations for studying entropy.\nThe \ufb01rst is that it is a\nfundamentally important quantity with useful algebraic properties (chain rule, etc.). The second\nstems from several practical applications in computer networking, such as network anomaly\ndetection. Let us consider a concrete example. One form of malicious activity on the internet\nis port scanning, in which attackers probe target machines, trying to \ufb01nd open ports which\ncould be leveraged for further attacks. In contrast, typical internet tra\ufb03c is directed to a small\nnumber of heavily used ports for web tra\ufb03c, email delivery, etc. Consequently, when a port\nscanning attack is underway, there is a signi\ufb01cant change in the distribution of port numbers in\nthe packets being delivered. It has been shown that measuring the entropy of the distribution\nof port numbers provides an e\ufb00ective means to detect such attacks. See Lakhina et al. [19] and\nXu et al. [36] for further information about such problems and methods for their solution.\n\u2217MIT Computer Science and Arti\ufb01cial Intelligence Laboratory.\nnickh@mit.edu.\nSupported by a Natural\nSciences and Engineering Research Council of Canada PGS Scholarship, by NSF contract CCF-0515221 and by\nONR grant N00014-05-1-0148.\n\u2020MIT Computer Science and Arti\ufb01cial Intelligence Laboratory. minilek@mit.edu. Supported by a National\nDefense Science and Engineering Graduate (NDSEG) Fellowship.\n\u2021MIT Computer Science and Arti\ufb01cial Intelligence Laboratory. konak@mit.edu. Supported in part by NSF\ncontract 0514771.\n1\nOur Techniques. In this paper, we give an algorithm for estimating empirical Shannon entropy\nwhile using a nearly optimal amount of space. Our algorithm is actually a sketching algorithm,\nnot just a streaming algorithm, and it applies to general streams which allow insertions and\ndeletions of elements. One attractive aspect of our work is its clean high-level approach: we\nreduce the entropy estimation problem to the well-studied frequency moment problem. More\nconcretely, we give algorithms for estimating other notions of entropy, R\u00b4enyi and Tsallis entropy,\nwhich are closely related to frequency moments. The link to Shannon entropy is established by\nproving bounds on the rate at which these other entropies converge toward Shannon entropy.\nRemarkably, it seems that such an analysis was not previously known.\nThere are several technical obstacles that arise with this approach. Unfortunately, it does\nnot seem that the optimal amount of space can be obtained while using just a single estimate of\nR\u00b4enyi or Tsallis entropy. We overcome this obstacle by using several estimates, together with\napproximation theory arguments and certain infrequently-used extremal properties of Chebyshev\npolynomials. To our knowledge, this is the \ufb01rst use of such techniques in the context of streaming\nalgorithms, and it seems likely that these techniques could be applicable to many other problems.\nSuch arguments yield good algorithms for additively estimating entropy, but obtaining a\ngood multiplicative approximation is more di\ufb03cult when the entropy is very small. In such a\nscenario, there is necessarily a very heavy element, and the task that one must solve is to estimate\nthe moment of all elements excluding this heavy element. This task has become known as the\nresidual moment estimation problem, and it is emerging as a useful building block for other\nstreaming problems [3, 5, 10]. To estimate the \u03b1th residual moment for \u03b1 \u2208(0, 2], we show that\n\u02dcO(\u03b5\u22122 log m) bits of space su\ufb03ce with a random oracle and \u02dcO(\u03b5\u22122 log2 m) bits without. This\ncompares with existing algorithms that use O(\u03b5\u22122 log2 m) bits for \u03b1 = 2 [11], and O(\u03b5\u22122 log m)\nfor \u03b1 = 1 [10]. No non-trivial algorithms were previously known for \u03b1 \u0338\u2208{1, 2}. Though, the\npreviously known algorithms were more general in ways unrelated to the needs of our work: they\ncan remove the k heaviest elements without requiring that they are su\ufb03ciently heavy.\nMultiplicative Entropy Estimation. Let us now state the performance of these algorithms\nmore explicitly. We focus exclusively on single-pass algorithms unless otherwise noted. The \ufb01rst\nalgorithms for approximating entropy in the streaming model are due to Guha et al. [13]; they\nachieved O(\u03b5\u22122 + log n) words of space but assumed a randomly ordered stream. Chakrabarti,\nDo Ba and Muthukrishnan [7] then gave an algorithm for worst-case ordered streams us-\ning O(\u03b5\u22122 log2 m) words of space, but required two passes over the input. The algorithm of\nChakrabarti, Cormode and McGregor [6] uses O(\u03b5\u22122 log m) words of space to give a multiplica-\ntive 1 + \u03b5 approximation, although their algorithm cannot produce sketches and only applies to\ninsertion-only streams. In contrast, the algorithm of Bhuvanagiri and Ganguly [3] provides a\nsketch and can handle deletions but requires roughly \u02dcO(\u03b5\u22123 log4 m) words1.\nOur work focuses primarily in the strict turnstile model (de\ufb01ned in Section 2), which allows\ndeletions.\nOur algorithm for multiplicatively estimating Shannon entropy uses \u02dcO(\u03b5\u22122 log m)\nwords of space. These bounds are nearly-optimal in terms of the dependence on \u03b5, since there\nis an \u02dc\u2126(\u03b5\u22122) lower bound even for insertion-only streams. Our algorithms assume access to\na random oracle. This assumption can be removed through the use of Nisan\u2019s pseudorandom\ngenerator [23], increasing the space bounds by a factor of O(log m).\nAdditive Entropy Estimation. Additive approximations of entropy are also useful, as they\ndirectly yield additive approximations of conditional entropy and mutual information, which\ncannot be approximated multiplicatively in small space [17]. Chakrabarti et al. noted that since\n1A recent, yet unpublished improvement by the same authors [4] improves this to \u02dcO(\u03b5\u22123 log3 m) words.\n2\nShannon entropy is bounded above by log m, a multiplicative (1 + (\u03b5/ log m)) approximation\nyields an additive \u03b5-approximation. In this way, the work of Chakrabarti et al. [6] and Bhuvana-\ngiri and Ganguly [3] yield additive \u03b5 approximations using O(\u03b5\u22122 log3 m) and \u02dcO(\u03b5\u22123 log7 m)\nwords of space respectively.\nOur algorithm yields an additive \u03b5 approximation using only\n\u02dcO(\u03b5\u22122 log m) words of space. In particular, our space bounds for multiplicative and additive\napproximation di\ufb00er by only log log m factors. Zhao et al. [37] give practical methods for addi-\ntively estimating the so-called entropy norm of a stream. Their algorithm can be viewed as a\nspecial case of ours since it interpolates Shannon entropy using two estimates of Tsallis entropy,\nalthough this interpretation was seemingly unknown to those authors.\nOther Information Statistics. We also give algorithms for approximating R\u00b4enyi [26] and\nTsallis [33] entropy. R\u00b4enyi entropy plays an important role in expanders [15], pseudorandom\ngenerators, quantum computation [34, 38], and ecology [22, 27]. Tsallis entropy is a important\nquantity in physics that generalizes Boltzmann-Gibbs entropy, and also plays a role in the\nquantum context. R\u00b4enyi and Tsallis entropy are both parameterized by a scalar \u03b1 \u22650. The\ne\ufb03ciency of our estimation algorithms depends on \u03b1, and is stated precisely in Section 5.\nA preliminary version of this work appeared in the IEEE Information Theory Workshop [14].\n2\nPreliminaries\nLet A = (A1, . . . , An) \u2208Zn be a vector initialized as \u20d70 which is modi\ufb01ed by a stream of m\nupdates. Each update is of the form (i, v), where i \u2208[n] and v \u2208{\u2212M, . . . , M}, and causes\nthe change Ai \u2190Ai + v. For simplicity in stating bounds, we henceforth assume m \u2265n and\nM = 1; the latter can be simulated by increasing m by a factor of M and representing an update\n(i, v) with |v| separate updates (though in actuality our algorithm can perform all |v| updates\nsimultaneously in the time it takes to do one update). The vector A gives rise to a probability\ndistribution x = (x1, . . . , xn) with xi = |Ai|/ \u2225A\u22251. Thus for each i either xi = 0 or xi \u22651/m.\nIn the strict turnstile model, we assume Ai \u22650 for all i \u2208[n] at the end of the stream. In the\ngeneral update model we make no such assumption. For the remainder of this paper, we assume\nthe strict turnstile model and assume access to a random oracle, unless stated otherwise. Our\nalgorithms also extend to the general update model, typically increasing bounds by a factor of\nO(log m). As remarked above, the random oracle can be removed, using [23], while increasing\nthe space by another O(log m) factor. When giving bounds, we often use the following tilde\nnotation: we say f(m, \u03b5) = \u02dcO(g(m, \u03b5)) if f(m, \u03b5) = O(g(m, \u03b5)(log log m + log(1/\u03b5))O(1)).\nWe now de\ufb01ne some functions commonly used in future sections. The \u03b1th norm of a vector is\ndenoted \u2225\u00b7\u2225\u03b1. We de\ufb01ne the \u03b1th moment as F\u03b1 = Pn\ni=1|Ai|\u03b1 = \u2225A\u2225\u03b1\n\u03b1. We de\ufb01ne the \u03b1th R\u00b4enyi\nentropy as H\u03b1 = log(\u2225x\u2225\u03b1\n\u03b1)/(1 \u2212\u03b1) and the \u03b1th Tsallis entropy as T\u03b1 = (1 \u2212\u2225x\u2225\u03b1\n\u03b1)/(\u03b1 \u22121).\nShannon entropy H = H1 is de\ufb01ned by H = \u2212Pn\ni=1 xi log xi. A straightforward application of\nl\u2019H\u02c6opital\u2019s rule shows that H = lim\u03b1\u21921 H\u03b1 = lim\u03b1\u21921 T\u03b1. It will often be convenient to focus on\nthe quantity \u03b1 \u22121 instead of \u03b1 itself. Thus, we often write H(a) = H1+a and T(a) = T1+a.\nWe will often need to approximate frequency moments, for which we use the following:\nFact 2.1 (Indyk [16], Li [20], [21]). There is an algorithm for multiplicative approximation of\nF\u03b1 for any \u03b1 \u2208(0, 2]. The algorithm needs O(\u03b5\u22122 log m) bits of space in the general update\nmodel, and O\n\u0010\u0000|\u03b1\u22121|\n\u03b52\n+ 1\n\u03b5\n\u0001\nlog m\n\u0011\nbits of space in the strict turnstile model.\nFor any function a 7\u2192f(a), we denote its kth derivative with respect to a by f (k)(a).\n3\n3\nEstimating Shannon Entropy\n3.1\nOverview\nWe begin by describing a general algorithm for computing an additive approximation to Shannon\nentropy. The remainder of this paper describes and analyzes various details and incarnations of\nthis algorithm, including extensions to give a multiplicative approximation in Section 3.4. We\nassume that m, the length of the stream, is known in advance. Computing \u2225A\u22251 is trivial since\nwe assume the strict turnstile model at present.\nAlgorithm 1. Our algorithm for additively approximating empirical Shannon entropy.\nChoose error parameter \u02dc\u03b5 and k points {y0, . . . , yk}\nProcess the entire stream:\nFor each i, compute \u02dcF1+yi, a (1 + \u02dc\u03b5)-approximation of the frequency moment F1+yi\nFor each i, compute \u02dcH(yi) = \u2212log( \u02dcF1+yi/||A||1+yi\n1\n)/yi and \u02dcT(yi) =\n\u00001 \u2212\u02dcF1+yi/||A||1+yi\n1\n\u0001\n/yi\nReturn an estimate of H(0) or T(0) by interpolation using the points \u02dcH(yi) or \u02dcT(yi)\n3.2\nOne-point Interpolation\nThe easiest implementation of this algorithm is to set k = 0, and estimate Shannon entropy H\nusing a single estimate of R\u00b4enyi entropy H(y0). We choose y0 = \u02dc\u0398(\u03b5/(log n log m)) and \u02dc\u03b5 = \u03b5\u00b7y0.\nBy Fact 2.1, the space required is \u02dcO(\u03b5\u22123 log n log m) words. The following argument shows this\ngives an additive O(\u03b5) approximation. With constant probability, \u02dcF1+y0 = (1 \u00b1 \u02dc\u03b5)F1+y0. Then\n\u02dcH(y0) = \u22121\ny0\nlog\n\u0010\n\u02dcF1+y0\n||A||1+y0\n1\n\u0011\n= \u22121\ny0\nlog\n\u0010\n(1 \u00b1 O(\u02dc\u03b5))\nn\nX\ni=1\nx1+y0\ni\n\u0011\n= H(y0) \u00b1 O\n\u0010 \u02dc\u03b5\ny0\n\u0011\n= H \u00b1 O(\u03b5).\n(3.1)\nThe last equality follows from the following theorem, which bounds the rate of convergence of\nR\u00b4enyi entropy towards Shannon entropy. A proof is given in Appendix A.1.\nTheorem 3.1. Let x \u2208Rn be a probability distribution whose smallest positive value is at least\n1/m, where m \u2265n. Let 0 < \u03b5 < 1 be arbitrary. De\ufb01ne \u00b5 = \u03b5/(4 log m), \u03bd = \u03b5/(4 log n log m),\n\u03b1 = 1 + \u00b5/\n\u000016 log(1/\u00b5)\n\u0001\n, and \u03b2 = 1 + \u03bd/\n\u000016 log(1/\u03bd)\n\u0001\n. Then\n1 \u2264H1\nH\u03b1\n\u22641 + \u03b5\nand\n0 \u2264H1 \u2212H\u03b2 \u2264\u03b5.\n3.3\nMulti-point Interpolation\nThe algorithm of Section 3.2 is limited by the following tradeo\ufb00: if we choose the point y0\nto be close to 0, the accuracy increases, but the space usage also increases. In this section,\nwe avoid that tradeo\ufb00by interpolating with multiple points. This allows us to obtain good\naccuracy without taking the points too close to 0.\nWe formalize this using approximation\ntheory arguments and properties of Chebyshev polynomials.\nThe algorithm estimates the Tsallis entropy with error parameter \u02dc\u03b5 = \u03b5/(12(k + 1)3 log m)\nusing points y0, y1, . . . , yk, chosen as follows.\nFirst, the number of points is k = log(1/\u03b5) +\nlog log m.\nTheir values are chosen to be an a\ufb03ne transformation of the extrema of the kth\nChebyshev polynomial. Formally, set \u2113= 1/(2(k + 1) log m) and de\ufb01ne the map f : R \u2192R by\nf(y) = (k2 \u2113) \u00b7 y \u2212\u2113\u00b7 (k2 + 1)\n2k2 + 1\n,\nthen de\ufb01ne\nyi = f\n\u0000cos(i\u03c0/k)\n\u0001\n.\n(3.2)\n4\nThe correctness of this algorithm is proven in Section 3.3.2. Let us now analyze the space\nrequirements. Computing the estimate \u02dcF1+yi uses only \u02dcO(\u02dc\u03b5\u22122/ log m) words of space by Fact 2.1\nsince |yi| \u22641/(2(k+1) log m) for each i. By our choice of k = \u02dcO(1) and \u02dc\u03b5, the total space required\nis \u02dcO(\u03b5\u22122 log m) words.\nWe argue correctness of this algorithm in Section 3.3.2. Before doing so, we must mention\nsome properties of Chebyshev polynomials.\n3.3.1\nChebyshev Polynomials\nOur algorithm exploits certain extremal properties of Chebyshev polynomials.\nFor a basic\nintroduction to Chebyshev polynomials we refer the reader to [24, 25, 28]. A thorough treatment\nof these objects can be found in [29]. We now present the background relevant for our purposes.\nDe\ufb01nition 3.2. The set Pk consists of all polynomials of degree at most k with real coe\ufb03cients.\nThe Chebyshev polynomial of degree k, Pk(x), is de\ufb01ned by the recurrence\nPk(x) =\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\n1,\nk = 0\nx,\nk = 1\n2xPk\u22121(x) \u2212Pk\u22122(x),\nk \u22652\nand satis\ufb01es |Pk(x)| \u22641 for all x \u2208[\u22121, 1]. The value |Pk(x)| equals 1 for exactly k + 1 values\nof x in [\u22121, 1]; speci\ufb01cally, Pk(\u03b7j,k) = (\u22121)j for 0 \u2264j \u2264k, where \u03b7j,k = cos(j\u03c0/k). The set Ck\nis de\ufb01ned as the set of all polynomials p \u2208Pk satisfying max0\u2264j\u2264k |p(\u03b7j,k)| \u22641.\nFact 3.3 (Extremal Growth Property). If p \u2208Ck and |t| \u22651, then |p(t)| \u2264|Pk(t)|.\nProof. See [29, Ex. 1.5.11] or Rogosinski [30].\n\u25a0\nFact 3.3 states that all polynomials which are bounded on certain \u201ccritical points\u201d of the\ninterval I = [\u22121, 1] cannot grow faster than Chebyshev polynomials once leaving I.\n3.3.2\nCorrectness\nTo analyze our algorithm, let us \ufb01rst suppose that our algorithm could exactly compute the\nTsallis entropies T(yi) for 0 \u2264i \u2264k. Let p be the degree-k polynomial obtained by interpolating\nat the chosen points, i.e., p(yi) = T(yi) for 0 \u2264i \u2264k. The algorithm uses p(0) as its estimate\nfor T(0). We analyze the accuracy of this estimate using the following fact. Recall that the\nnotation g(k) denotes the kth derivative of a function g.\nFact 3.4 (Phillips and Taylor [25], Theorem 4.2).\nLet y0, y1, . . . , yk be points in the interval\n[a, b]. Let g : R \u2192R be such that g(1), . . . , g(k) exist and are continuous on [a, b], and g(k+1)\nexists on (a, b). Then, for every y \u2208[a, b], there exists \u03bey \u2208(a, b) such that\ng(y) \u2212p(y) =\n k\nY\ni=0\n(y \u2212yi)\n!\ng(k+1)(\u03bey)\n(k + 1)!\nwhere p(y) is the degree-k polynomial obtained by interpolating the points (yi, g(yi)), 0 \u2264i \u2264k.\nTo apply this fact, a bound on |T (k+1)(y)| is needed. It su\ufb03ces to consider the interval\n[\u2212\u2113, 0), since the map f de\ufb01ned in Eq. (3.2) sends \u22121 7\u2192\u2212\u2113and 1 7\u2192\u2212\u2113/(2k2 + 1), and hence\nEq. (3.2) shows that yi \u2208[\u2212\u2113, 0) for all i. Since \u2113= 1/(2(k + 1) log m), it follows from the\nfollowing lemma that\n|T (k+1)(yi)| \u22644 logk+1(m) H\nk + 2\n\u22000 \u2264i \u2264k.\n(3.3)\n5\nLemma 3.5. Let \u03b5 be in (0, 1/2]. Then, |T (k)(\u2212\n\u03b5\n(k+1) log m)| \u22644 logk(m) H/(k + 1).\nBy Fact 3.4 and Eq. (3.3), we have\n|T(0) \u2212p(0)|\n\u2264\n|\u2113|k+1 \u00b7 4 logk+1(m) H\n(k + 1)! (k + 2)\n=\n1\n2k+1 logk+1(m)\n\u00b7 4 logk+1(m) H\n(k + 1)! (k + 2)\n\u2264\n2\u03b5\n(k + 1)! (k + 2)\n\u2264\n\u03b5\n2 ,\n(3.4)\nsince 2k = (log m)/\u03b5 and H \u2264log m. This demonstrates that our algorithm computes a good\napproximation of T(0) = H, under the assumption that the values T(yi) can be computed\nexactly. The remainder of this section explains how to remove this assumption.\nAlgorithm 1 does not compute the exact values T(yi), it only computes approximations. The\naccuracy of these approximations can be determined as follows. Then\n\u02dcT(yi) = 1 \u2212\u02dcF1+yi/||A||1+yi\n1\nyi\n\u2264T(yi) \u2212\u02dc\u03b5 \u00b7\nPn\nj=1 x1+yi\nj\nyi\n.\n(3.5)\nNow recall that xj \u22651/m for each i and yi \u2265\u2212\u2113, so that xyi\ni \u2264m\u2113= m1/2(k+1) log m < 2. Thus\nPn\nj=1 x1+yi\nj\n\u22642 Pn\nj=1 xj = 2. Since \u02dc\u03b5/\u2113= \u03b5/(6k2), we have\nT(yi) \u2264\n\u02dcT(yi) \u2264T(yi) + \u03b5/(3k2).\n(3.6)\nNow let \u02dcp(x) be the degree-k polynomial de\ufb01ned by \u02dcp(yi) = \u02dcT(yi) for all 0 \u2264i \u2264k. Then\nEq. (3.6) shows that r(x) = p(x) \u2212\u02dcp(x) is a polynomial of degree at most k satisfying |r(yi)| \u2264\n\u03b5/(3k2) for all 0 \u2264i \u2264k.\nLet P : R \u2192R be the Chebyshev polynomial of degree k, and let Q(y) = P\n\u0000f \u22121(y)\n\u0001\nbe an\na\ufb03ne transformation of P. Then the polynomial r\u2032(y) = (3k2/\u03b5) \u00b7 r(y) satis\ufb01es |r\u2032(yi)| \u2264|Q(yi)|\nfor all 0 \u2264i \u2264k. Thus Fact 3.3 implies that |r\u2032(0)| \u2264|Q(0)|. By de\ufb01nition of Q, Q(0) =\nP(f \u22121(0)) = P(1 + 1/k2). The following lemma shows that this is at most e2.\nLemma 3.6. Let P be the kth Chebyshev polynomial, k \u22651, and let x = 1 + k\u2212c. Then\n|Pk(x)| \u2264\nk\nY\nj=1\n\u0012\n1 + 2j\nkc\n\u0013\n\u2264e2k2\u2212c.\nThus |r\u2032(0)| \u2264e2 and |r(0)| \u2264\u03b5/2 since k \u22652. To conclude, we have shown |p(0) \u2212\u02dcp(0)| =\n|r(0)| \u2264\u03b5/2. Combining with Eq. (3.4) via the triangle inequality shows |\u02dcp(0) \u2212H| \u2264\u03b5.\n3.4\nMultiplicative Approximation of Shannon Entropy\nWe now discuss how to extend the multi-point interpolation algorithm to obtain a multiplicative\napproximation of Shannon entropy. The main tool that we require is a multiplicative estimate of\nTsallis entropy, rather than the additive estimates used above. Section 5 shows that the required\nmultiplicative estimates can be e\ufb03ciently computed; Section 4 provides tools for doing this.\nThe modi\ufb01cations to the multi-point interpolation algorithm are as follows.\nWe set the\nnumber of interpolation points to be k = max{5, log(1/\u03b5)}, then argue as in Eq. (3.4) to\nhave |T(0) \u2212p(0)| \u2264\u03b5H/2, where p is the interpolated polynomial of degree k.\nWe then\n6\nuse Algorithm 1, but we compute \u02dcT(yi) to be a (1 + \u02dc\u03b5)-multiplicative estimation of T(yi) in-\nstead of an \u02dc\u03b5-additive estimation by using Theorem 5.6. By arguing as in Eq. (3.6), we have\nT(yi) \u2264\u02dcT(yi) \u2264T(yi) + \u03b5T(yi)/(3k2) \u2264T(yi) + 4\u03b5H/(3k2). The \ufb01nal inequality follows from\nLemma 3.5 with k = 0. From this point, the argument remains identical as Section 3.3.2 to show\nthat |p(0) \u2212\u02dcp(0)| \u22644\u03b5e2H/(3k2) < \u03b5H/2, yielding |\u02dcp(0) \u2212H| \u2264\u03b5H by the triangle inequality.\n4\nEstimating Residual Moments\nTo multiplicatively approximate Shannon entropy, the algorithm of Section 3.4 requires a mul-\ntiplicative approximation of Tsallis entropy. Section 5 shows that the required quantities can\nbe computed. The main tool needed is an e\ufb03cient algorithm for estimating residual moments.\nThat is the topic of the present section.\nDe\ufb01ne the residual \u03b1th moment to be F res\n\u03b1\n:= Pn\ni=2 |Ai|\u03b1 = F\u03b1 \u2212|A1|\u03b1, where we reorder the\nitems such that |A1| \u2265|A2| \u2265. . . \u2265|An|. In this section, we present two e\ufb03cient algorithms to\ncompute a 1 + \u03b5 multiplicative approximation to F res\n\u03b1\nfor \u03b1 \u2208(0, 2]. These algorithms succeed\nwith constant probability under the assumption that a heavy hitter exists, say |A1| \u22654\n5 \u2225A\u22251.\nThe algorithm of Section 4.2 is valid only in the strict turnstile model. Its space usage has\na complicated dependence on \u03b1; for the primary range of interest, \u03b1 \u2208[1/3, 1), the bound is\nO((\u03b5\u22121/\u03b1 +\u03b5\u22122(1\u2212\u03b1)+log n) log m). The algorithm of Section 4.3 is valid in the general update\nmodel and uses \u02dcO(\u03b5\u22122 log m) bits of space.\n4.1\nFinding a Heavy Element\nA subroutine that is needed for both of our algorithms is to detect whether a heavy hitter exists\n(|Ai| \u22654\n5 \u2225A\u22251) and to \ufb01nd the identity of that element. We will describe a procedure for doing\nso in the general update model. We use the following result, which essentially follows from the\ncount-min sketch [8]. For completeness, a self-contained proof is given in Appendix A.5.\nFact 4.1.\nLet w \u2208Rn\n+ be a weight vector on n elements so that P\ni wi = 1. There exists a\nfamily H of hash functions mapping the n elements to O(1/\u03b5) bins with |H| = nO(1) such that\na random h \u2208H satis\ufb01es the following two properties with probability at least 15/16.\n(1) If wi \u22651/2 then the weight of elements that collide with element i is at most \u03b5 \u00b7 P\nj\u0338=i wj.\n(2) If maxi wi < 1/2 then the weight of elements hashing to each bin is at most 3/4.\nWe use the hash function from Fact 4.1 with \u03b5 = 1/10 to partition the elements into bins,\nand for each bin maintain a counter of the net L1 weight that hash to it. If there is a heavy\nhitter, then the net weight in its bin is more than 4/5 \u2212\u03b5(1/5) > 3/4. Conversely, if there is a\nbin with at least 3/4 of the weight then Fact 4.1 implies then there is a heavy element.\nWe determine the identity of the heavy element via a group-testing type of argument: we\nmaintain \u2308log2 n\u2309counters, of which the ith counts the number of elements which have their ith\nbit set. Thus, if there is heavy element, we can determine its ith bit by checking whether the\nfraction of elements with their ith bit is at least 3/5.\n4.2\nBucketing Algorithm\nIn this section, we describe an algorithm for estimating F res\n\u03b1\nthat works only in the strict turnstile\nmodel. The algorithm has several cases, depending on the value of \u03b1.\nCase 1: \u03b1 = 1. This is the simplest case for our algorithm. We use the hash function from\nFact 4.1 to partition the elements into bins, and for each bin maintain a count of the number\nof elements that hash to it. If there is a bin with more than 3/4 elements at the end of the\nprocedure, then there is a heavy element, and it su\ufb03ces to return the total number of elements\n7\nin the other bins. Otherwise, we announce that there is no heavy hitter. The correctness follows\nfrom Fact 4.1, and the space required is O\n\u0000 1\n\u03b5 log m\n\u0001\nbits.\nCase 2: \u03b1 = (0, 1\n3) \u222a(1, 2]. Again, we use the hash function from Fact 4.1 to partition the\nelements into bins. For each bin, we maintain a count of the number of elements, and a sketch\nof the \u03b1th moment using Fact 2.1. The counts allow us to detect if there is a heavy hitter, as\nin Case 1. If so, we combine the moment sketches of all bins other than the one containing the\nheavy hitter; this gives a good estimate with constant probability. By Fact 2.1, we need only\nO\n\u0010\n1\n\u03b5 \u00b7\n\u0010\n|\u03b1\u22121|\n\u03b52\n+ 1\n\u03b5\n\u0011\nlog m + 1\n\u03b5 log m\n\u0011\n= O\n\u0010\u0010\n|\u03b1\u22121|\n\u03b53\n+ 1\n\u03b52\n\u0011\nlog m\n\u0011\nbits.\nCase 3: \u03b1 = [1\n3, 1). This is the most interesting case. This idea is to keep just one sketch of\nthe \u03b1th moment for the entire stream. At the end, we estimate F res\n\u03b1\nby arti\ufb01cially appending\ndeletions to the stream which almost entirely remove the heavy hitter from the sketch.\nThe algorithm computes four quantities in parallel. First, \u02dcF res\n1\n= (1 \u00b1 \u03b5\u2032)F res\n1\nwith error\nparameter \u03b5\u2032 = \u03b51/\u03b1, using the above algorithm with \u03b1 = 1. Second, \u02dcF\u03b1 = (1 \u00b1 \u03b5)F\u03b1 using\nFact 2.1. Third, F1, which is trivial in the strict turnstile model. Lastly, we determine the\nidentity of the heavy hitter as in Section 4.1.\nNow we explain how to estimate F res\n\u03b1 .\nThe key observation is that F1 \u2212\u02dcF res\n1\nis a very\ngood approximation to A1 (assume this is the heavy hitter). So if we delete the heavy hitter\n(F1 \u2212\u02dcF res\n1 ) times, then there are at most A1 \u2264\u03b5\u2032F res\n1\nremaining occurrences. De\ufb01ne \u02dcF res\n\u03b1\nto\nbe the value of \u02dcF\u03b1 after processing these deletions. Clearly F res\n\u03b1\n\u2265(F res\n1 )\u03b1, by concavity of the\nfunction y 7\u2192y\u03b1. On the other hand, the remaining occurrences of the heavy hitter contribute\nat most (\u03b5\u2032F res\n1 )\u03b1. Hence, the remaining occurrences of the heavy hitter in\ufb02ate F res\n\u03b1\nby a factor\nof at most 1 + (\u03b5\u2032 \u00b7F res\n1 )\u03b1/(F res\n1 )\u03b1 = 1+\u03b5. Thus \u02dcF res\n\u03b1\n= (1+O(\u03b5))F res\n\u03b1 , as desired. The number\nof bits of space used by this algorithm is at most\nO\n\u0000 1\n\u03b5\u2032 log m +\n\u0000 1\u2212\u03b1\n\u03b52 + 1\n\u03b5\n\u0001\nlog m + log n log m\n\u0001\n= O\n\u0010\u00001\n\u03b51/\u03b1 + 1\u2212\u03b1\n\u03b52 + log n\n\u0001\nlog m\n\u0011\n.\n4.3\nGeometric Mean Algorithm\nThis section describes an algorithm for estimating F res\n\u03b1\nin the general update model. At a high\nlevel, the algorithm uses a hash function to partition the stream elements into two substreams,\nthen separately estimates the moment F\u03b1 for the substreams. The estimate for the substream\nwhich does not contain the heavy hitter yields a good estimate of F res\n\u03b1 . We improve accuracy of\nthis estimator by averaging many independent trials. Detailed description and analysis follow.\nWe use Li\u2019s geometric mean estimator [21] for estimating F\u03b1 since it is unbiased (its being\nunbiased will be useful later). The geometric mean estimator is de\ufb01ned as follows. Let k and\n\u03b1 be parameters. We let y = R \u00b7 A, where A is the vector representing the stream and R is a\nk \u00d7 n matrix whose entries are i.i.d. samples from an \u03b1-stable distribution. De\ufb01ne\n\u02dcF\u03b1 =\nQk\nj=1 |yj|\u03b1/k\n[ 2\n\u03c0\u0393(\u03b1\nk )\u0393(1 \u22121\nk) sin(\u03c0\u03b1\n2k )]k .\nThe space required to compute this estimator is easily seen to be O(k \u00b7 log m) bits. Li analyzed\nthe variance of \u02dcF\u03b1 as k \u2192\u221e, however for our purposes we are only interested in the case k = 3\nand henceforth restrict to only this case (one can show \u02dcF\u03b1 has unbounded variance for k < 3).\nBuilding on Li\u2019s analysis, we show the following result.\nLemma 4.2. There exists an absolute constant CGM such that Var\nh\n\u02dcF\u03b1\ni\n\u2264CGM \u00b7 E\nh\n\u02dcF\u03b1\ni2\n.\n8\nLet r denote the number of independent trials. For each j \u2208[r], the algorithm picks a function\nhj : [n] \u2192{0, 1} uniformly at random. For j \u2208[r] and l \u2208{0, 1}, de\ufb01ne F\u03b1,j,l = P\ni:hj(i)=l |Ai|\u03b1.\nThis is the \u03b1th moment for the lth substream during the jth trial.\nFor each j and l, our algorithm computes an estimate \u02dcF\u03b1,j,l of F\u03b1,j,l using the geometric\nmean estimator. We also run in parallel the algorithm of Section 4.1 to discover which i \u2208[n] is\nthe heavy hitter; henceforth assume i = 1. Our overall estimate for F res\n\u03b1\nis then\n\u02dcF res\n\u03b1\n= 2\nr\nr\nX\nj=1\n\u02dcF\u03b1,j,1\u2212hj(1)\nThe space used by our algorithm is simply the space required for r geometric mean estimators\nand the one heavy hitter algorithm. The latter uses \u02dcO(\u03b5\u22121 log n) bits of space [8, Theorem 7].\nThus the total space required is \u02dcO(r log m + \u03b5\u22121 log n) bits.\nWe now sketch an analysis of the algorithm; a formal argument is given in Appendix A.4.\nThe natural analysis would be to show that, for each item, the fraction of trials in which the\nitem doesn\u2019t collide with the heavy hitter is concentrated around 1/2. A union bound over all\nitems would require choosing the number of trials to be \u2126( 1\n\u03b52 log n). We obtain a signi\ufb01cantly\nsmaller number of trials by using a di\ufb00erent analysis. Instead of using a concentration bound\nfor each item, we observe that items with roughly the same weight (i.e., the value of |Ai|) are\nessentially equivalent for the purposes of this analysis. So we partition the items into classes\nsuch that all items in the a class have the same weight, up to a (1 + \u03b5) factor. We then apply\nconcentration bounds for each class, rather than separately for each item. The number of classes\nis only R = O(1\n\u03b5 log m), and a union bound over classes only requires \u0398( 1\n\u03b52 log R) trials.\nAs argued, the space usage of this algorithm is \u02dcO(r log m + \u03b5\u22121 log n) = \u02dcO(\u03b5\u22122 log m) bits.\n5\nEstimation of R\u00b4enyi and Tsallis Entropy\nThis section summarizes our algorithms for estimating R\u00b4enyi and Tsallis entropy. These al-\ngorithms are used as subroutines for estimating Shannon entropy in Section 3, and may be of\nindependent interest.\nThe techniques we use for both the entropies are almost identical. In particular, to compute\nadditive approximation of T\u03b1 or H\u03b1, it su\ufb03ces to compute a su\ufb03ciently precise multiplicative\napproximation of the \u03b1-th moment. Due to space constraints, we present proofs of all lemmas\nand theorems from this section in the appendix.\nTheorem 5.1.\nThere is an algorithm that computes an additive \u03b5-approximation of R\u00b4enyi\nentropy in O\n\u0010\nlog m\n|1\u2212\u03b1|\u00b7\u03b52\n\u0011\nbits of space for any \u03b1 \u2208(0, 1) \u222a(1, 2].\nTheorem 5.2. There is an algorithm for additive approximation of Tsallis entropy T\u03b1 using\n\u2022 O\n\u0010\nn2(1\u2212\u03b1) log m\n(1\u2212\u03b1)\u03b52\n\u0011\nbits, for \u03b1 \u2208(0, 1).\n\u2022 O\n\u0010\nlog m\n(\u03b1\u22121)\u03b52\n\u0011\nbits, for \u03b1 \u2208(1, 2].\nIn order to obtain a multiplicative approximation of Tsallis and R\u00b4enyi entropy, we must\nprove a few facts.\nThe next lemma says that if there is no heavy element in the empirical\ndistribution, then Tsallis entropy is at least a constant.\n9\nLemma 5.3.\nLet x1, x2, . . . , xn be values in [0, 1] of total sum 1.\nThere exists a positive\nconstant C such that if xi \u22645/6 for all i then, for \u03b1 \u2208(0, 1) \u222a(1, 2],\n\f\f\f1 \u2212\nn\nX\ni=1\nx\u03b1\ni\n\f\f\f \u2265C \u00b7 |\u03b1 \u22121|.\nCorollary 5.4.\nThere exists a constant C such that if the probability of each element is at\nmost 5/6, then the Tsallis entropy is at least C for any \u03b1 \u2208(0, 1) \u222a(1, 2].\nProof. We have\nT\u03b1 = 1 \u2212Pn\ni=1 x\u03b1\n\u03b1 \u22121\n= |1 \u2212Pn\ni=1 x\u03b1\ni |\n|\u03b1 \u22121|\n\u2265C.\n\u25a0\nWe now show how to deal with the case when there is an element of large probability. It\nturns out that in this case we can obtain a multiplicative approximation of Tsallis entropy by\ncombining two residual moments.\nLemma 5.5.\nThere is a positive constant C such that if there is an element i of probability\nxi \u22652/3, then the sum of a multiplicative (1 + C \u00b7 |1 \u2212\u03b1| \u00b7 \u03b5)-approximation to 1 \u2212xi and\na multiplicative (1 + C \u00b7 |1 \u2212\u03b1| \u00b7 \u03b5)-approximation to P\nj\u0338=i x\u03b1\nj gives a multiplicative (1 + \u03b5)-\napproximation to |1 \u2212P\ni x\u03b1\ni |, for any \u03b1 \u2208(0, 1) \u222a(1, 2].\nWe these collect those facts in the following theorem.\nTheorem 5.6.\nThere is a streaming algorithm for multiplicative (1 + \u03b5)-approximation of\nTsallis entropy for any \u03b1 \u2208(0, 1) \u222a(1, 2] using \u02dcO\n\u0000log m/(|1 \u2212\u03b1|\u03b52)\n\u0001\nbits of space.\nThe next lemma shows that we can handle the logarithm that appears in the de\ufb01nition of\nR\u00b4enyi entropy.\nLemma 5.7.\nIt su\ufb03ces to have a multiplicative (1 + \u03b5)-approximation to t \u22121, where t \u2208\n(4/9, \u221e) to compute a multiplicative (1 + C \u00b7 \u03b5) approximation to log(t), for some constant C.\nWe now have all necessary facts to estimate R\u00b4enyi entropy for \u03b1 \u2208(0, 2].\nTheorem 5.8.\nThere is a streaming algorithm for multiplicative (1 + \u03b5)-approximation of\nR\u00b4enyi entropy for any \u03b1 \u2208(0, 1) \u222a(1, 2]. The algorithm uses \u02dcO\n\u0000log m/(|1 \u2212\u03b1|\u03b52)\n\u0001\nbits of space.\nIn fact, Theorem 5.8 is tight in the sense that (1+\u03b5)-multiplicative approximation of H\u03b1 for\n\u03b1 > 2 requires polynomial space, as seen in the following theorem.\nTheorem 5.9.\nFor any \u03b1 > 2, any randomized one-pass streaming algorithm which (1 + \u03b5)-\napproximates H\u03b1(X) requires \u2126(n1\u22122/\u03b1\u22122\u03b5\u2212\u03b3(\u03b5+1/\u03b1)) bits of space for arbitrary constant \u03b3 > 0.\nTsallis entropy can be e\ufb03ciently approximated both multiplicatively and additively also for\n\u03b1 > 2, but we omit a proof of that fact in this version of the paper.\n6\nModi\ufb01cations for General Update Streams\nThe algorithms described in Section 3 and Section 5 are for the strict turnstile model. They can\nbe extended to work in the general updates model with a few modi\ufb01cations.\nFirst, we cannot e\ufb03ciently and exactly compute \u2225A\u22251 = F1 in the general update model.\nHowever, a (1 + \u03b5)-multiplicative approximation can be computed in O(\u03b5\u22122 log m) bits of space\nby Fact 2.1. In Section 3.2 and Section 3.3, the value of \u2225A\u22251 is used as a normalization factor to\nscale the estimate of F\u03b1 to an estimate of Pn\ni=1 x\u03b1\ni . (See, e.g., Eq. (3.1) and Eq. (3.5).) However,\n\u02dcF\u03b1\n( \u02dcF1)\u03b1 =\n(1 \u00b1 \u03b5) \u00b7 F\u03b1\n\u0000(1 \u00b1 \u03b5) \u00b7 F1\n\u0001\u03b1 =\n\u00001 \u00b1 O(\u03b5)\n\u0001\n\u00b7 F\u03b1\nF \u03b1\n1\n,\n10\nso the fact that F1 can only be approximated in the general update model a\ufb00ects the analysis\nonly by increasing the constant factor that multiplies \u03b5. A similar modi\ufb01cation must also be\napplied to all algorithms in Section 5; we omit the details.\nNext, the multiplicative algorithm Section 3.4 needs to compute a multiplicative estimate of\nT(yi) using Theorem 5.6. In the general updates model, a weaker result than Theorem 5.6 holds:\nwe obtain a multiplicative (1+\u03b5)-approximation of Tsallis entropy for any \u03b1 \u2208(0, 1)\u222a(1, 2] using\n\u02dcO\n\u0000log m/(|1 \u2212\u03b1| \u00b7 \u03b5)2\u0001\nbits of space. The proof is identical to the argument in Appendix A.6,\nexcept that the the moment estimator of Fact 2.1 uses more space, and we must use the residual\nmoment algorithm of Section 4.3 instead of Section 4.2. Similar modi\ufb01cations must be made to\nTheorem 5.1, Theorem 5.2 and Theorem 5.8, with a commensurate increase in the space bounds.\n7\nFuture Research\nWe hope that the techniques from approximation theory that we introduce may be useful for\nstreaming and sketching other functions. For instance, consider the following function G\u03b1,k(x) =\nP\ni x\u03b1\ni (log n)k, where k \u2208N and \u03b1 \u2208[0, \u221e). One can show that\nlim\n\u03b2\u2192\u03b1\nG\u03b1,k(x) \u2212G\u03b2,k(x)\n\u03b1 \u2212\u03b2\n= G\u03b2,k+1(x).\nNote that G\u03b1,0(x) is the \u03b1th moment of x, and one can attempt to estimate G\u03b1,k+1 by computing\nG\u03b2,k for \u03b2 = \u03b1 and \u03b2 close to \u03b1. It is not unlikely that our techniques can be generalized to\nestimation of functions G\u03b1,k for \u03b1 \u2208(0, 2]. Can one also use our techniques for approximation\nof other classes of functions?\nAcknowledgements\nWe thank Piotr Indyk and Ping Li for many helpful discussions. We also thank Jonathan Kelner\nfor some pointers to the approximation theory literature.\nReferences\n[1] Noga Alon, Yossi Matias, and Mario Szegedy. The Space Complexity of Approximating the\nFrequency Moments. Journal of Computer and System Sciences, 58(1):137\u2013147, 1999.\n[2] Ziv Bar-Yossef, T. S. Jayram, Ravi Kumar, and D. Sivakumar. An information statistics\napproach to data stream and communication complexity. J. Comput. Syst. Sci., 68(4):702\u2013\n732, 2004.\n[3] Lakshminath Bhuvanagiri and Sumit Ganguly. Estimating entropy over data streams. In\nProceedings of the 14th Annual European Symposium on Algorithms, pages 148\u2013159, 2006.\n[4] Lakshminath Bhuvanagiri and Sumit Ganguly. Hierarchical Sampling from Sketches: Esti-\nmating Functions over Data Streams, 2008. Manuscript.\n[5] Lakshminath Bhuvanagiri, Sumit Ganguly, Deepanjan Kesh, and Chandan Saha. Simpler\nalgorithm for estimating frequency moments of data streams. In Proceedings of the 17th\nAnnual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 708\u2013713, 2006.\n[6] Amit Chakrabarti, Graham Cormode, and Andrew McGregor. A near-optimal algorithm\nfor computing the entropy of a stream. In Proceedings of the 18th Annual ACM-SIAM\nSymposium on Discrete Algorithms (SODA), pages 328\u2013335, 2007.\n11\n[7] Amit Chakrabarti, Khanh Do Ba, and S. Muthukrishnan. Estimating Entropy and Entropy\nNorm on Data Streams.\nIn Proceedings of the 23rd Annual Symposium on Theoretical\nAspects of Computer Science (STACS), pages 196\u2013205, 2006.\n[8] Graham Cormode and S. Muthukrishnan. An improved data stream summary: the count-\nmin sketch and its applications. J. Algorithms, 55(1):58\u201375, 2005.\n[9] Thomas Cover and Joy Thomas. Elements of Information Theory.\nWiley Interscience,\n1991.\n[10] Sumit Ganguly and Graham Cormode. On estimating frequency moments of data streams.\nIn APPROX-RANDOM, pages 479\u2013493, 2007.\n[11] Sumit Ganguly, Deepanjan Kesh, and Chandan Saha. Practical algorithms for tracking\ndatabase join sizes. In Proceedings of the 25th International Conference on Foundations of\nSoftware Technology and Theoretical Computer Science (FSTTCS), pages 297\u2013309, 2005.\n[12] Yu Gu, Andrew McCallum, and Donald F. Towsley. Detecting anomalies in network tra\ufb03c\nusing maximum entropy estimation. In Internet Measurment Conference, pages 345\u2013350,\n2005.\n[13] Sudipto Guha, Andrew McGregor, and Suresh Venkatasubramanian. Streaming and sublin-\near approximation of entropy and information distances. In Proceedings of the 17th Annual\nACM-SIAM Symposium on Discrete Algorithms (SODA), pages 733\u2013742, 2006.\n[14] Nicholas J. A. Harvey, Jelani Nelson, and Krzysztof Onak. Streaming algorithms for esti-\nmating entropy. In Proceedings of IEEE Information Theory Workshop, 2008.\n[15] Shlomo Hoory, Nathan Linial, and Avi Wigderson. Expander graphs and their applications.\nBulletin of the American Mathematical Society, 43(4):439\u2013561, 2006.\n[16] Piotr Indyk. Stable distributions, pseudorandom generators, embeddings, and data stream\ncomputation. J. ACM, 53(3):307\u2013323, 2006.\n[17] Piotr Indyk and Andrew McGregor. Declaring independence via the sketching of sketches.\nIn Proceedings of the 19th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA),\n2008.\n[18] Piotr Indyk and David P. Woodru\ufb00. Optimal approximations of the frequency moments of\ndata streams. In Proceedings of the 37th Annual ACM Symposium on Theory of Computing\n(STOC), pages 202\u2013208, 2005.\n[19] A. Lakhina, M. Crovella, and C. Diot. Mining anomalies using tra\ufb03c feature distributions.\nIn Proceedings of the ACM SIGCOMM Conference, pages 217\u2013228, 2005.\n[20] Ping Li. Compressed counting. CoRR abs/0802.2305v2, 2008.\n[21] Ping Li. Estimators and tail bounds for dimension reduction in lp (0 < p \u22642) using stable\nrandom projections. In Proceedings of the 19th Annual ACM-SIAM Symposium on Discrete\nAlgorithms (SODA), pages 10\u201319, 2008.\n12\n[22] Canran Liu, Robert J. Whittaker, Keping Ma, and Jay R. Malcolm. Unifying and dis-\ntinguishing diversity ordering methods for comparing communities. Population Ecology,\n49(2):89\u2013100, 2006.\n[23] Noam Nisan. Pseudorandom generators for space-bounded computation. Combinatorica,\n12(4):449\u2013461, 1992.\n[24] George McArtney Phillips. Interpolation and Approximation by Polynomials.\nSpringer-\nVerlag, New York, 2003.\n[25] George McArtney Phillips and Peter John Taylor. Theory and Applications of Numerical\nAnalysis. Academic Press, 2nd edition, 1996.\n[26] Alfred R\u00b4enyi. On measures of entropy and information. In Proc. Fourth Berkeley Symp.\nMath. Stat. and Probability, volume 1, pages 547\u2013561, 1961.\n[27] Carlo Ricotta, Alessandra Pacini, and Giancarlo Avena. Parametric scaling from species\nto growth-form diversity: an interesting analogy with multifractal functions. Biosystems,\n65(2\u20133):179\u2013186, 2002.\n[28] Theodore J. Rivlin. An Introduction to the Approximation of Functions. Dover Publications,\nNew York, 1981.\n[29] Theodore J. Rivlin. Chebyshev Polynomials: From Approximation Theory to Algebra and\nNumber Theory. John Wiley & Sons, 2nd edition, 1990.\n[30] Werner Wolfgang Rogosinski. Some elementary inequalities for polynomials. The Mathe-\nmatical Gazette, 39(327):7\u201312, 1955.\n[31] Walter Rudin. Principles of Mathematical Analysis. McGraw-Hill, third edition, 1976.\n[32] Michael E. Saks and Xiaodong Sun. Space lower bounds for distance approximation in\nthe data stream model. In Proceedings of the 34th Annual ACM Symposium on Theory of\nComputing (STOC), pages 360\u2013369, 2002.\n[33] Constantino Tsallis. Possible generalization of boltzmann-gibbs statistics. Journal of Sta-\ntistical Physics, 52:479\u2013487, 1988.\n[34] Wim van Dam and Patrick Hayden. Renyi-entropic bounds on quantum communication.\narXiv:quant-ph/0204093, 2002.\n[35] David Woodru\ufb00. E\ufb03cient and Private Distance Approximation in the Communication and\nStreaming Models. PhD thesis, Massachusetts Institute of Technology, 2007.\n[36] Kuai Xu, Zhi-Li Zhang, and Supratik Bhattacharyya. Pro\ufb01ling internet backbone tra\ufb03c:\nbehavior models and applications.\nIn Proceedings of the ACM SIGCOMM Conference,\npages 169\u2013180, 2005.\n[37] Haiquan Zhao, Ashwin Lall, Mitsunori Ogihara, Oliver Spatscheck, Jia Wang, and Jun Xu.\nA Data Streaming Algorithm for Estimating Entropies of OD Flows. In Proceedings of the\nInternet Measurement Conference (IMC), 2007.\n[38] Karol \u02d9Zyczkowski. R\u00b4enyi Extrapolation of Shannon Entropy. Open Systems & Information\nDynamics, 10(3):297\u2013310, 2003.\n13\nA\nProofs\nA.1\nProofs from Section 3.2\nRecall that x \u2208Rn is a distribution whose smallest positive value is at least 1/m. The key\ntechnical lemma needed is as follows.\nLemma A.1. Let \u03b1 > 1, let \u03be = \u03be(\u03b1) denote 4(\u03b1 \u22121)H1(x), and let\ne(\u03b1) = 2\n\u0010\n\u03be log n + \u03be log(1/\u03be)\n\u0011\n.\nAssume that \u03be(\u03b1) < 1/4. Then H\u03b1 \u2264H1 \u2264H\u03b1 + e(\u03b1).\nWe require the following basic results.\nClaim A.2. The following inequalities follow from convexity.\n\u2022 Let 0 < y \u22641. Then ey < 1 + 2y.\n\u2022 Let y > 0. Then 1 \u2212y \u2264log(1/y).\n\u2022 Let 0 \u2264y \u22641/2. Then 1/(1 \u2212y) \u22641 + 2y.\nClaim A.3. Let 1 \u2264a \u2264b and let x \u2208Rn. Then \u2225x\u2225b \u2264\u2225x\u2225a \u2264n1/a\u22121/b \u2225x\u2225b.\nClaim A.4. If 0 \u2264\u03b1 \u2264\u03b2 then H\u03b1 \u2265H\u03b2\nClaim A.5. If \u03b1 > 1 then log\n\u00001/ \u2225x\u2225\u03b1\n\u0001\n< (\u03b1 \u22121) \u00b7 H1.\nProof. log\n\u00001/ \u2225x\u2225\u03b1\n\u0001\n=\n\u03b1\u22121\n\u03b1 H\u03b1(x) < (\u03b1 \u22121) \u00b7 H\u03b1(x) \u2264(\u03b1 \u22121) \u00b7 H1(x).\n\u25a0\nClaim A.6.\nLet y = (y1, . . . , yn) and z = (z1, . . . , zn) be probability distributions such that\n\u2225y \u2212z\u22251 \u22641/2. Then\n|H1(y) \u2212H1(z)| \u2264\u2225y \u2212z\u22251 \u00b7 log\n\u0010\nn\n\u2225y \u2212z\u22251\n\u0011\n.\nProof. See Cover and Thomas [9, 16.3.2].\n\u25a0\nProof (of Lemma A.1). The \ufb01rst inequality follows from Claim A.4 so we focus on the second\none. De\ufb01ne f(\u03b1) = log \u2225x\u2225\u03b1\n\u03b1 and g(\u03b1) = 1 \u2212\u03b1, so that H\u03b1 = f(\u03b1)/g(\u03b1). The derivatives are\nf \u2032(\u03b1) =\nPn\ni=1x\u03b1\ni log xi\n\u2225x\u2225\u03b1\n\u03b1\nand\ng\u2032(\u03b1) =\n\u22121,\nso lim\u03b1\u21921 f \u2032(\u03b1)/g\u2032(\u03b1) exists and equals H(x). Since lim\u03b1\u21921 f(\u03b1) = lim\u03b1\u21921 g(\u03b1) = 0, l\u2019H\u02c6opital\u2019s\nrule implies that lim\u03b1\u21921 H\u03b1 = H(x). A stronger version of L\u2019H\u02c6opital\u2019s rule is as follows.\nClaim A.7.\nLet f : R \u2192R and g : R \u2192R be di\ufb00erentiable functions such that the following\nlimits exist\nlim\n\u03b1\u21921 f(\u03b1) = 0,\nlim\n\u03b1\u21921 g(\u03b1) = 0,\nand\nlim\n\u03b1\u21921 f \u2032(\u03b1)/g\u2032(\u03b1) = L.\nLet \u03b5 and \u03b4 be such that |\u03b1 \u22121| < \u03b4 implies that |f \u2032(\u03b1)/g\u2032(\u03b1) \u2212L| < \u03b5. Then |\u03b1 \u22121| < \u03b4 also\nimplies that |f(\u03b1)/g(\u03b1) \u2212L| < \u03b5.\n14\nProof. See Rudin [31, p.109].\n\u2737\nThus, to prove our lemma, it su\ufb03ces to show that |f \u2032(\u03b1)/g\u2032(\u03b1) \u2212H1| < e(\u03b1). (In fact, we\nactually need |f \u2032(\u03b2)/g\u2032(\u03b2) \u2212H1| < e(\u03b1) for all \u03b2 \u2208(1, \u03b1], but this follows by monotonicity of\ne(\u03b2) for \u03b2 \u2208(1, \u03b1].)\nA key concept in this proof is the \u201cperturbed\u201d probability distribution x(\u03b1), de\ufb01ned by\nx(\u03b1)i = x\u03b1\ni / \u2225x\u2225\u03b1\n\u03b1. We have the following relationship.\nf \u2032(\u03b1)\ng\u2032(\u03b1) =\nPn\ni=1x\u03b1\ni log(1/xi)\n\u2225x\u2225\u03b1\n\u03b1\n=\nPn\ni=1x\u03b1\ni\n\u0000log(1/xi) + log \u2225x\u2225\u03b1 \u2212log \u2225x\u2225\u03b1\n\u0001\n\u2225x\u2225\u03b1\n\u03b1\n=\n\u0010Pn\ni=1x\u03b1\ni log(\u2225x\u2225\u03b1 /xi)\n\u0011\n\u2212\n\u0010Pn\ni=1x\u03b1\ni log \u2225x\u2225\u03b1\n\u0011\n\u2225x\u2225\u03b1\n\u03b1\n= 1\n\u03b1\nn\nX\ni=1\nx\u03b1\ni\n\u2225x\u2225\u03b1\n\u03b1\nlog\n \n\u2225x\u2225\u03b1\n\u03b1\nx\u03b1\ni\n!\n\u2212log \u2225x\u2225\u03b1\n= H1\n\u0000x(\u03b1)\n\u0001\n\u03b1\n+ log(1/ \u2225x\u2225\u03b1)\nIn summary, we have shown that\n\f\f\f\f\f\nf \u2032(\u03b1)\ng\u2032(\u03b1) \u2212H1\n\u0000x(\u03b1)\n\u0001\n\u03b1\n\f\f\f\f\f \u2264log(1/ \u2225x\u2225\u03b1) \u2264(\u03b1 \u22121) \u00b7 H1(x),\n(A.1)\nthe last inequality following from Claim A.5. To use this bound, we observe that:\n\f\f\f\f\nf \u2032(\u03b1)\ng\u2032(\u03b1) \u2212H1\n\u0000x(\u03b1)\n\u0001\f\f\f\f =\n\f\f\f\f\f\nf \u2032(\u03b1)\ng\u2032(\u03b1) \u2212H1\n\u0000x(\u03b1)\n\u0001\n\u03b1\n+\n \n1\n\u03b1 \u22121\n!\nH1\n\u0000x(\u03b1)\n\u0001\n\f\f\f\f\f\n\u2264\n\f\f\f\f\f\nf \u2032(\u03b1)\ng\u2032(\u03b1) \u2212H1\n\u0000x(\u03b1)\n\u0001\n\u03b1\n\f\f\f\f\f + |1/\u03b1 \u22121| \u00b7 H1\n\u0000x(\u03b1)\n\u0001\nWe now substitute Eq. (A.1) into this expression, and use |1/\u03b1 \u22121| \u2264\u03b1 \u22121 (valid since \u03b1 \u22651).\nThis yields:\n\f\f\f\f\nf \u2032(\u03b1)\ng\u2032(\u03b1) \u2212H1\n\u0000x(\u03b1)\n\u0001\f\f\f\f \u2264(\u03b1 \u22121) \u00b7 H1(x) + (\u03b1 \u22121) \u00b7 H1\n\u0000x(\u03b1)\n\u0001\n(A.2)\nRecall that our goal is to analyze |f \u2032(\u03b1)/g\u2032(\u03b1) \u2212H1(x)|.\nWe do this by showing that\nH1\n\u0000x(\u03b1)\n\u0001\n\u2248H1(x), and that the right-hand side of Eq. (A.2) is at most e(\u03b1). This is done\nusing Claim A.6; the key step is bounding \u2225x \u2212x(\u03b1)\u22251.\nClaim A.8. Suppose that 1 < \u03b1 \u22641 + 1/(2 log n). Then 1/ \u2225x\u2225\u03b1\n\u03b1 < 1 + 3(\u03b1 \u22121)H1(x).\nProof. From Claim A.3 and \u2225x\u22251 = 1, we obtain 1/ \u2225x\u2225\u03b1 \u2264n1\u22121/\u03b1 < n\u03b1\u22121. Our hypothesis on\n\u03b1 implies that\n\u03b1 \u00b7 log(1/ \u2225x\u2225\u03b1) < \u03b1 \u00b7 (\u03b1 \u22121) log n < 2 \u00b7 (\u03b1 \u22121) log n \u22641.\n(A.3)\n15\nThus\n1\n\u2225x\u2225\u03b1\n\u03b1\n= e\u03b1 log(1/\u2225x\u2225\u03b1) < 1 + 2 \u00b7 \u03b1 log(1/ \u2225x\u2225\u03b1) < 1 + 3(\u03b1 \u22121)H1(x).\nThe \ufb01rst inequality is from Claim A.2 and Eq. (A.3), and the second from Claim A.5.\n\u2737\nRecall that \u03be = 4(\u03b1 \u22121)H1(x).\nClaim A.9. \u2225x \u2212x(\u03b1)\u22251 \u2264\u03be.\nProof. To avoid the absolute values, we shall split the sum de\ufb01ning \u2225x \u2212x(\u03b1)\u22251 into two cases.\nFor that purpose, let S = { i : x(\u03b1)i \u2265xi }. Then\n\u2225x \u2212x(\u03b1)\u22251 =\nX\ni\u2208S\n\u0000x(\u03b1)i \u2212xi\n\u0001\n+\nX\ni\u0338\u2208S\n\u0000xi \u2212x(\u03b1)i\n\u0001\n=\nX\ni\u2208S\nxi \u00b7\n \nx\u03b1\u22121\ni\n\u2225x\u2225\u03b1\n\u03b1\n\u22121\n!\n+\nX\ni\u0338\u2208S\nxi \u00b7\n \n1 \u2212x\u03b1\u22121\ni\n\u2225x\u2225\u03b1\n\u03b1\n!\nThe \ufb01rst sum is upper-bounded using x\u03b1\u22121\ni\n\u22641 and P\ni\u2208S xi \u22641. The second sum is upper-\nbounded using \u2225x\u2225\u03b1\n\u03b1 \u22641 and 1 \u2212x\u03b1\u22121\ni\n\u2264log\n\u00001/x\u03b1\u22121\ni\n\u0001\n(see Claim A.2).\n\u2264\n \n1\n\u2225x\u2225\u03b1\n\u03b1\n\u22121\n!\n+ (\u03b1 \u22121)\nX\ni\u0338\u2208S\nxi log(1/xi)\n\u22643(\u03b1 \u22121)H1(x) + (\u03b1 \u22121)H1(x),\nusing Claim A.8. This completes the proof.\n\u2737\nThus, by our assumption that \u03be(\u03b1) < 1/4, by Claim A.6, by Claim A.9, and by the fact that\nx 7\u2192x log(1/x) is monotonically increasing for x \u2208(0, 1/4), we obtain that\n|H1(x) \u2212H1(x(\u03b1))| \u2264\u03be log n + \u03be log(1/\u03be).\nNow we assemble the error bounds. Our result from Eq. (A.2) yields\n\f\f\f\f\nf \u2032(\u03b1)\ng\u2032(\u03b1) \u2212H1(x)\n\f\f\f\f \u2264\n\f\f\f\f\nf \u2032(\u03b1)\ng\u2032(\u03b1) \u2212H1(x(\u03b1))\n\f\f\f\f + |H1(x) \u2212H1(x(\u03b1))|\n\u2264\n\u0010\n(\u03b1 \u22121)H1(x) + (\u03b1 \u22121)H1(x(\u03b1))\n\u0011\n+ |H1(x) \u2212H1(x(\u03b1))|\n\u22642(\u03b1 \u22121)H1(x) + \u03b1 \u00b7 |H1(x) \u2212H1(x(\u03b1))|\n\u22642\n\u0010\n\u03be log n + \u03be log(1/\u03be)\n\u0011\nThis completes the proof.\n\u25a0\nWe now use Lemma A.1 to show that H\u03b1 \u2248H1, if \u03b1 is su\ufb03ciently small.\nProof (of Theorem 3.1). First we focus on the multiplicative approximation. The lower bound\nis immediate from Claim A.4, so we show the upper-bound. For an arbitrary \u00b5 \u2208(0, 1), we have\n\u00b52 <\n\u00b5\n2 log(1/\u00b5) < \u00b5;\n16\nthis follows since \u00b5 log(1/\u00b5) < 1/2 for all \u00b5. Let \u02dc\u00b5 = \u00b5/\n\u00002 log(1/\u00b5)\n\u0001\n. Then\n\u02dc\u00b5 log(1/\u02dc\u00b5) < \u00b5.\nThis follows since \u00b52 < \u02dc\u00b5 =\u21d21/\u02dc\u00b5 < 1/\u00b52 =\u21d2log(1/\u02dc\u00b5) < 2 log(1/\u00b5).\nThe hypotheses of Theorem 3.1 give \u03b1 = 1 + \u02dc\u00b5/8. Hence,\ne(\u03b1) = 8(\u03b1 \u22121)H1\nh\nlog n + log\n\u0010\n1/\n\u00004(\u03b1 \u22121)H1\n\u0001\u0011i\n\u2264\u02dc\u00b5H1\nh\nlog n + log\n\u00002/(\u02dc\u00b5H1)\n\u0001i\nSince H1 \u2265(log m)/m for any distribution satisfying our hypotheses, this is at most\n\u2264\u02dc\u00b5H1\n\u0010\nlog n + log(1/\u02dc\u00b5) + log m\n\u0011\n\u2264(log m)\u00b5H1 < (\u03b5/2)H1,\nsince our hypotheses give \u00b5 = \u03b5/(4 log m). Applying Lemma A.1, we obtain that\nH1 \u2212H\u03b1 \u2264(\u03b5/2)H1\n=\u21d2\n(1 \u2212\u03b5/2)H1 \u2264H\u03b1\n=\u21d2\nH1\nH\u03b1\n\u2264\n1\n1 \u2212\u03b5/2 \u22641 + \u03b5,\nthe last inequality following from Claim A.2. This establishes the multiplicative approximation.\nLet us now consider the above argument, replacing \u00b5 with \u03bd = \u03b5/(4 log n log m). We obtain\ne(\u03b1) \u2264(log m)\u03bdH1 \u2264\u03b5/4,\nsince H1 \u2264log n. Thus, the additive approximation follows directly.\n\u25a0\nA.2\nProofs from Section 3.3\nOur \ufb01rst task is to prove Lemma 3.5. We require a de\ufb01nition and two preliminary technical\nresults. For any integer k \u22650 and real number a \u2265\u22121, de\ufb01ne\nGk(a) =\nn\nX\ni=1\nx1+a\ni\nlogk(xi),\nso G0(a) = F1+a/||A||1+a\n1\n. Note that G(1)\nk (a) = Gk+1(a) for k \u22650, and T(a) = (1 \u2212G0(a))/a.\nClaim A.10. The kth derivative of the Tsallis entropy has the following expression.\nT (k)(a) = (\u22121)k k!\n\u00001 \u2212G0(a)\n\u0001\nak+1\n\u2212\n\uf8eb\n\uf8ed\nk\nX\nj=1\n(\u22121)k\u2212j k! Gj(a)\nak\u2212j+1j!\n\uf8f6\n\uf8f8\n17\nProof. The proof is by induction, the case k = 0 being trivial. So assume k \u22651. Taking the\nderivative of the expression for T (k)(a) above, we obtain:\nT (k+1)(a)\n=\n\uf8eb\n\uf8ed\nk\nX\nj=1\nk!(k \u2212j + 1)(\u22121)(k+1)\u2212jGj(a)\na(k+1)\u2212j+1j!\n+ k!(\u22121)k\u2212jGj+1(a)\nak\u2212j+1j!\n\uf8f6\n\uf8f8\n+ (\u22121)k+1(k + 1)!(G0(a) \u22121)\nak+2\n+ (\u22121)kk!G1(a)\nak+1\n=\n\uf8eb\n\uf8ed\nk\nX\nj=1\nk!(\u22121)(k+1)\u2212jGj(a)\na(k+1)\u2212j+1(j \u22121)!\n\u0012\n1 + k \u2212j + 1\nj\n\u0013\uf8f6\n\uf8f8+ Gk+1(a)\na\n+ (\u22121)k+1(k + 1)!(G0(a) \u22121)\nak+2\n=\n\uf8eb\n\uf8ed\nk+1\nX\nj=1\n(k + 1)!(\u22121)(k+1)\u2212jGj(a)\na(k+1)\u2212j+1j!\n\uf8f6\n\uf8f8+ (\u22121)k+1(k + 1)!(G0(a) \u22121)\nak+2\nas claimed.\n\u25a0\nClaim A.11. De\ufb01ne Sk(a) = ak+1T (k)(a). Then, for 1 \u2264j \u2264k + 1,\nS(j)\nk (a) =\nj\u22121\nX\ni=0\n\u0012j \u22121\ni\n\u0013\nk!\n(k \u2212j + i + 1)!ak\u2212j+i+1Gk+1+i(a)\nIn particular, for 1 \u2264j \u2264k, we have\nlim\na\u21920 S(j)\nk (a) = 0\nand\nlim\na\u21920 S(k+1)\nk\n(a) = k! Gk+1(0)\nso that\nlim\na\u21920 T (k)(a) = Gk+1(0)\nk + 1 .\nProof. We prove the claim by induction on j. First, note\nSk(a) = (\u22121)kk!(1 \u2212G0(a)) \u2212\n\uf8eb\n\uf8ed\nk\nX\nj=1\naj(\u22121)k\u2212jk!Gj(a)\nj!\n\uf8f6\n\uf8f8\nso that\nS(1)\nk (a)\n=\n(\u22121)k\u22121k!G1(a) \u2212\n\uf8eb\n\uf8ed\nk\nX\nj=1\n\u2212a(j+1)\u22121(\u22121)k\u2212(j+1)k!Gj+1(a)\n((j + 1) \u22121)!\n+ aj\u22121(\u22121)k\u2212jk!Gj(a)\n(j \u22121)!\n\uf8f6\n\uf8f8\n=\nakGk+1(a)\nThus, the base case holds. For the inductive step with 2 \u2264j \u2264k + 1, we have\nS(j)\nk (a)\n=\n\u2202\n\u2202a\n j\u22122\nX\ni=0\n\u0012j \u22122\ni\n\u0013\nk!\n(k \u2212j + i + 2)!ak\u2212j+i+2Gk+1+i(a)\n!\n=\nj\u22122\nX\ni=0\n \u0012j \u22122\ni\n\u0013\nk!\n(k \u2212j + i + 1)!ak\u2212j+i+1Gk+1+i(a)\n+\n\u0012j \u22122\ni\n\u0013\nk!\n(k \u2212j + (i + 1) + 1)!ak\u2212j+(i+1)+1Gk+1+(i+1)(a)\n!\n=\nj\u22121\nX\ni=0\n\u0012j \u22121\ni\n\u0013\nk!\n(k \u2212j + i + 1)!ak\u2212j+i+1Gk+1+i(a)\n18\nThe \ufb01nal equality holds since\n\u0000j\u22122\n0\n\u0001\n=\n\u0000j\u22121\n0\n\u0001\n= 1,\n\u0000j\u22122\nj\u22122\n\u0001\n=\n\u0000j\u22121\nj\u22121\n\u0001\n= 1, and by Pascal\u2019s formula\n\u0000j\u22122\ni\n\u0001\n+\n\u0000j\u22122\ni+1\n\u0001\n=\n\u0000j\u22121\ni+1\n\u0001\nfor 0 \u2264i \u2264j \u22123.\nFor 1 \u2264j \u2264k, every term in the above sum is well-de\ufb01ned for a = 0 and contains a power\nof a which is at least 1, so lima\u21920 S(j)\nk (a) = 0. When j = k + 1, all terms but the \ufb01rst term\ncontain a power of a which is at least 1, and the \ufb01rst term is k!Gk+1(a), so lima\u21920 S(k+1)\nk\n(a) =\nk!Gk+1(0). The claim on lima\u21920 T(k)(a) thus follows by writing T (k)(a) = Sk(a)/ak+1 then\napplying l\u2019H\u02c6opital\u2019s rule k + 1 times.\n\u25a0\nProof (of Lemma 3.5). We will \ufb01rst show that\n\f\f\f\fT (k)\n\u0012\n\u2212\n\u03b5\n(k + 1) log m\n\u0013\n\u2212Gk+1(0)\nk + 1\n\f\f\f\f \u22646\u03b5 logk(m)H(x)\nk + 1\nLet Sk(a) = ak+1T (k)(a) and note T (k)(a) = Sk(a)/ak+1.\nBy Claim A.10, lima\u21920 Sk(a) =\n0.\nFurthermore, lima\u21920 S(j)\nk\n= 0 for all 1 \u2264j \u2264k by Claim A.11.\nThus, when analyzing\nlima\u21920 S(j)\nk (a)/(ak+1)(j) for 0 \u2264j \u2264k, both the numerator and denominator approach 0 and\nwe can apply l\u2019H\u02c6opital\u2019s rule (here (ak+1)(j) denotes the jth derivative of the function ak+1).\nBy k + 1 applications of l\u2019H\u02c6opital\u2019s rule, we can thus say that T (k)(a) converges to its limit\nat least as quickly as S(k+1)\nk\n(a)/(ak+1)(k+1) = S(k+1)\nk\n(a)/(k + 1)! does (using Claim A.7). We\nnote that Gj(a) is nonnegative for j even and nonpositive otherwise. Thus, for negative a, each\nterm in the summand of the expression for S(k+1)\nk\n(a) in Claim A.11 is nonnegative for odd k\nand nonpositive for even k. As the analyses for even and odd k are nearly identical, we focus\nbelow on odd k, in which case every term in the summand is nonnegative. For odd k, S(k+2)\nk\n(a)\nis nonpositive so that S(k+1)\nk\n(a) is monotonically decreasing.\nThus, it su\ufb03ces to show that\nS(k+1)\nk\n(\u2212\u03b5/((k + 1) log m))/(k + 1)! is not much larger than its limit.\nS(k+1)\nk\n\u0010\n\u2212\n\u03b5\n(k+1) log m\n\u0011\n(k + 1)!\n=\nPk\ni=0\n\u0000k\ni\n\u0001k!\ni!\n\u0010\n\u2212\n\u03b5\n(k+1) log m\n\u0011i\nGk+1+i\n\u0010\n\u2212\n\u03b5\n(k+1) log m\n\u0011\n(k + 1)!\n\u2264\n1 + 2\u03b5\nk + 1\nk\nX\ni=0\n\u0012k\ni\n\u0013 \u0012\n\u03b5\n(k + 1) log m\n\u0013i\n|Gk+1+i(0)|\n\u2264\n1 + 2\u03b5\nk + 1\nk\nX\ni=0\nki\n\u0012\n\u03b5\n(k + 1) log m\n\u0013i\n|Gk+1+i(0)|\n\u2264\n1 + 2\u03b5\nk + 1\nk\nX\ni=0\n\u0012\n\u03b5\nlog m\n\u0013i\n|Gk+1+i(0)|\n\u2264\n1 + 2\u03b5\nk + 1\nk\nX\ni=0\n\u03b5i|Gk+1(0)|\n\u2264\n(1 + 2\u03b5)|Gk+1(0)|\nk + 1\n+ 1 + 2\u03b5\nk + 1\nk\nX\ni=1\n\u03b5i|Gk+1(0)|\n\u2264\n(1 + 2\u03b5)|Gk+1(0)|\nk + 1\n+\n2\nk + 1\nk\nX\ni=1\n\u03b5i logk(m)H(x)\n19\n\u2264\n|Gk+1(0)|\nk + 1\n+ 6\u03b5 logk(m)H(x)\nk + 1\nThe \ufb01rst inequality holds since xi \u22651/m for each i, so that x\u2212\u03b5/((k+1) log m)\ni\n\u2264m\u03b5/((k+1) log m) \u2264\nm\u03b5/ log m \u2264e\u03b5 \u22641 + 2\u03b5 for \u03b5 \u22641/2. The \ufb01nal inequality above holds since \u03b5 \u22641/2.\nThe lemma follows since |Gk+1(0)| \u2264logk(m)H(x).\n\u25a0\nProof (of Lemma 3.6).\nLet Pj denote the jth Chebyshev polynomial. We will prove for all\nj \u22651 that\nPj\u22121(x) \u2264Pj(x) \u2264Pj\u22121(x)\n\u0012\n1 + 2j\nkc\n\u0013\n.\nFor the \ufb01rst inequality, we observe Pj\u22121 \u2208Cj, so we apply Fact 3.3 together with the fact that\nPj(y) is strictly positive for y > 1 for all j.\nFor the second inequality, we induct on j. For the sake of the proof de\ufb01ne P\u22121(x) = 1 so\nthat the inductive hypothesis holds at the base case d = 0. For the inductive step with j \u22651,\nwe use the recurrence de\ufb01nition of Pj(x) and we have\nPj+1(x)\n=\nPj(x)\n\u0012\n1 + 2\nkc\n\u0013\n+ (Pj(x) \u2212Pj\u22121(x))\n\u2264\nPj(x)\n\u0012\n1 + 2\nkc\n\u0013\n+ Pj\u22121(x)2j\nkc\n\u2264\nPj(x)\n\u0012\n1 + 2\nkc\n\u0013\n+ Pj(x)2j\nkc\n=\nPj(x)\n\u0012\n1 + 2(j + 1)\nkc\n\u0013\n\u25a0\nA.3\nProofs from Section 4\nFact A.12. For any real z > 0, \u0393(z + 1) = z\u0393(z).\nFact A.13. For any real z \u22650, sin(z) \u2264z.\nFact A.14 (Euler\u2019s Re\ufb02ection Formula). For any real z, \u0393(z)\u0393(1 \u2212z) = \u03c0/ sin(\u03c0z).\nDe\ufb01nition A.15. The function V : R+ \u2192R is de\ufb01ned by\nV (\u03b1) =\n\u0002 2\n\u03c0\u0393(2\u03b1\n3 )\u0393(1\n3) sin(\u03c0\u03b1\n3 )\n\u00033\n\u0002 2\n\u03c0\u0393(\u03b1\n3 )\u0393(2\n3) sin(\u03c0\u03b1\n6 )\n\u00036 \u22121\nLemma A.16.\nlim\n\u03b1\u21920 V (\u03b1) = \u0393\n\u0000 1\n3\n\u00013\n\u0393\n\u0000 2\n3\n\u00016\nProof. De\ufb01ne u(\u03b1) = \u0393(2\u03b1/3)(\u03c0\u03b1/3) = \u0393(2\u03b1/3)(2\u03b1/3)(\u03c0/2) = \u0393((2\u03b1/3)+1)(\u03c0/2) by Fact A.12.\nBy the continuity of \u0393(\u00b7) on R+, lim\u03b1\u21920 u(\u03b1) = \u0393(1)\u03c0/2 = \u03c0/2. De\ufb01ne f(\u03b1) = \u0393(2\u03b1/3) sin(\u03c0\u03b1/3).\nThen f(\u03b1) \u2264u(\u03b1) for all \u03b1 \u22650 by Fact A.13, and thus lim\u03b1\u21920 f(\u03b1) \u2264\u03c0/2.\nNow de\ufb01ne\n\u2113\u03b4(\u03b1) = \u0393(2\u03b1/3)(1\u2212\u03b4)(\u03c0\u03b1/3). By the de\ufb01nition of the derivative and the fact that the derivative\nof sin(\u03b1) evaluated at \u03b1 = 1 is 1, it follows that \u2200\u03b4 > 0 \u2203\u03b5 > 0 s.t. 0 \u2264\u03b1 < \u03b5 \u21d2sin(\u03b1) \u2265(1\u2212\u03b4)\u03b1.\nThus, \u2200\u03b4 > 0 \u2203\u03b5 > 0 s.t. 0 \u2264\u03b1 < \u03b5 \u21d2\u2113\u03b4(\u03b1) \u2264f(\u03b1), and so \u2200\u03b4 > 0 we have that lim\u03b1\u21920 f(\u03b1) \u2265\n20\nlim\u03b1\u21920 \u2113\u03b4(\u03b1) = (1 \u2212\u03b4)\u03c0/2. Thus, lim\u03b1\u21920 f(\u03b1) \u2265\u03c0/2, implying lim\u03b1\u21920 f(\u03b1) = \u03c0/2. Similarly\nwe can de\ufb01ne g(\u03b1) = \u0393(\u03b1/3) sin(\u03c0\u03b1/6) and show lim\u03b1\u21920 g(\u03b1) = \u03c0/2.\nNow,\nV (\u03b1) =\n\u0002 2\n\u03c0\u0393\n\u0000 1\n3\n\u0001\nf(\u03b1)\n\u00033\n\u0002 2\n\u03c0\u0393\n\u00002\n3\n\u0001\ng(\u03b1)\n\u00036\nThus lim\u03b1\u21920 V (\u03b1) = \u0393(1/3)3/\u0393(2/3)6 as claimed.\n\u25a0\nProof (of Lemma 4.2).\nLi shows in [21] that the variance of the geometric mean estimator\nwith k = 3 is V (\u03b1)F 2\n\u03b1. As \u0393(z) and sin(z) are continuous for z \u2208R+, so is V (\u03b1). Furthermore\nLemma A.16 shows that lim\u03b1\u21920 V (\u03b1) exists (and equals (\u0393(1/3)3/\u0393(2/3)6)\u22121). We de\ufb01ne V (0)\nto be this limit. Thus V (\u03b1) is continuous on [0, 2], and the extreme value theorem implies there\nexists a constant CGM such that V (\u03b1) \u2264CGM on [0, 2].\n\u25a0\nA.4\nDetailed Analysis of Geometric Mean Residual Moments Algorithm\nFormally, de\ufb01ne R =\nl\nlog1+ \u03b5\nc1 m\nm\n, and let Iz =\nn\ni : (1 + \u03b5\nc1)z \u2264|Ai| < (1 + \u03b5\nc1)z+1 o\nfor 0 \u2264\nz \u2264R. Let z\u2217satisfy (1 + \u03b5\nc1)z\u2217\u2264|A1| < (1 + \u03b5\nc1 )z\u2217+1. For 1 \u2264j \u2264r and 0 \u2264z \u2264R, de\ufb01ne\nXj,z = P\ni\u2208Iz 1hj(i)\u0338=hj(1). We now analyze the jth trial.\nClaim A.17. E\nh\n2 \u00b7 F\u03b1,j,1\u2212hj(1)\ni\n=\n\u00001 + O(\u03b5)\n\u0001\n\u00b7 F res\n\u03b1 .\nProof. We have\nE\nh\n2 \u00b7 F\u03b1,j,1\u2212hj(1)\ni\n= 2 \u00b7 E\n\" X\ni\n|Ai|\u03b1 \u00b7 1hj(i)\u0338=hj(1)\n#\n= 2 \u00b7\nX\nz\nE\n\" X\ni\u2208Iz\n|Ai|\u03b1 \u00b7 1hj(i)\u0338=hj(1)\n#\n= 2 \u00b7\nX\nz\nE\n\" X\ni\u2208Iz\n\u0000(1 \u00b1 \u03b5)(1 + \u03b5)z\u0001\u03b1 \u00b7 1hj(i)\u0338=hj(1)\n#\n= (1 \u00b1 \u03b5)\u03b1 \u00b7\nX\nz\n(1 + \u03b5)z\u03b1 E [ 2Xj,z ] .\nClearly E [ 2 \u00b7 Xj,z ] is |Iz| \u22121 if z = z\u2217and |Iz| otherwise. Thus\nX\nz\n(1 + \u03b5)z\u03b1 E [ 2 \u00b7 Xj,z ] =\nX\ni\u22652\n\u0000(1 \u00b1 \u03b5)|Ai|\n\u0001\u03b1 = (1 \u00b1 \u03b5)\u03b1 \u00b7 F res\n\u03b1 .\nSince \u03b1 < 2, (1 \u00b1 \u03b5)\u03b1 = 1 \u00b1 O(\u03b5), so this shows the desired result.\n\u25a0\nWe now show concentration for Xz := 1\nr\nP\n1\u2264j\u2264r Xj,z. By independence of the hj\u2019s, Cherno\ufb00\nbounds show that Xz = (1\u00b1\u03b5) E [ Xz ] with probability at least 1\u2212exp(\u2212\u0398(\u03b52r)). This quantity\nis at least 1 \u2212\n1\n8(R+1) if we choose r = c2\n\u0006\n\u03b5\u22122(log log ||A||1 + log(c3/\u03b5))\n\u0007\n. The good event is the\nevent that, for all z, Xz = (1\u00b1\u03b5) E [ Xz ]; a union bound shows that this occurs with probability\nat least 7/8. So suppose that the good event occurs. Then a calculation analogous to Claim A.17\n21\nshows that\nX\nj\n2\nr \u00b7 F\u03b1,j,1\u2212hj(1) = (1 \u00b1 \u03b5)\u03b1 \u00b7\nX\nz\n(1 + \u03b5)z\u03b1 \u00b7 2Xz\n= (1 \u00b1 \u03b5)\u03b1 \u00b7\nX\nz\n(1 + \u03b5)z\u03b1 \u00b7 (1 \u00b1 \u03b5) E [ 2Xz ]\n=\n\u00001 \u00b1 O(\u03b5)\n\u0001\n\u00b7 F res\n\u03b1 .\n(A.4)\nRecall that \u02dcF res\n\u03b1\n= Pr\nj=1\n2\nr \u02dcF\u03b1,j,1\u2212hj(1). Since the geometric mean estimator is unbiased, we\nalso have that\nE\nh\n\u02dcF res\n\u03b1\ni\n= E\n\uf8ee\n\uf8f0X\nj\n2\nr F\u03b1,j,1\u2212hj(1)\n\uf8f9\n\uf8fb.\n(A.5)\nWe conclude the analysis by showing that the random variable \u02dcF res\n\u03b1\nis concentrated.\nBy\nLemma 4.2 applied to each substream, and properties of variance, we have\nVar\nh\n\u02dcF res\n\u03b1\ni\n=\n4\nr2\nr\nX\nj=1\nVar\nh\n\u02dcF\u03b1,j,1\u2212hj(1)\ni\n\u22644 CGM\nr\n\u00b7 E\nh\n\u02dcF\u03b1,j,1\u2212hj(1)\ni2\n\u2264CGM\nr\n\u00b7 E\nh\n\u02dcF res\n\u03b1\ni2\n.\nChebyshev\u2019s inequality therefore shows that\nPr\nh\n\u02dcF res\n\u03b1\n= (1 \u00b1 \u03b5) E\nh\n\u02dcF res\n\u03b1\ni i\n\u22651 \u2212\nVar\nh\n\u02dcF res\n\u03b1\ni\n(\u03b5 \u00b7 E\nh\n\u02dcF res\n\u03b1\ni\n)2 \u22651 \u2212CGM\n\u03b52 r\n> 6/7,\nby appropriate choice of constants. This event and the good event both occur with probability\nat least 3/4. When this holds, we have\n\u02dcF res\n\u03b1\n= (1 \u00b1 \u03b5) E\nh\n\u02dcF res\n\u03b1\ni\n= (1 \u00b1 \u03b5) E\n\uf8ee\n\uf8f0X\nj\n2\nrF\u03b1,j,1\u2212hj(1)\n\uf8f9\n\uf8fb=\n\u00001 \u00b1 O(\u03b5)\n\u0001\n\u00b7 F res\n\u03b1 ,\nby Eq. (A.5) and Eq. (A.4).\nA.5\nProofs from Section 4.2\nProof (of Fact 4.1). Let B = \u230820/\u03b5\u2309be the number of bins. Let H be a pairwise independent\nfamily of hash functions, each function mapping [n] to [B]. Standard constructions yield such a\nfamily with |H| = nO(1). We will let h be a randomly chosen hash function from H.\nFor notational simplicity, suppose that x1 = maxi xi. Let Ei,j be the indicator variable for the\nevent that h(i) = j, so that E [ Ei,j ] = 1/B and Var [ Ei,j ] < 1/B. Let Xj be the random variable\ndenoting the weight of the items that hash to bin j, i.e., Xj = P\ni xi \u00b7 Ei,j. Since P\ni xi = 1, we\nhave E [ Xj ] = 1/B and Var [ Xj ] < \u2225x\u22252\n2 /B.\nSuppose that x1 \u22651/2. Let Y be the fraction of mass that hashes to x1\u2019s bin, excluding x1\nitself. That is, Y = P\ni\u22652 xi \u00b7 Ei,h(1). Note that E [ Y ] = (P\ni\u22652 xi)/B < (\u03b5/20) \u00b7 (P\ni\u22652 xi). By\nMarkov\u2019s inequality,\nPr\nh\nY \u2265\u03b5 \u00b7 (P\ni\u22652xi)\ni\n\u2264Pr [ Y \u226516 E [ Y ] ] \u22641/16.\n22\nSuppose that x1 < 1/2. This implies, by convexity, that \u2225x\u22252\n2 < 1/2. Let \u03b2 =\np\n2/3 < 5/6.\nThen\nPr [ |Xj \u22121/B| \u2265\u03b2 ] \u2264Var [ Xj ]\n\u03b22\n< 3\n4B .\nThus, by a union bound,\nPr [ \u2203j such that Xj \u2265\u03b2 + 1/B ] \u22643\n4.\nSuppose we want to test if x1 \u22651/2 by checking if there\u2019s a bin of mass at least 5/6. As\nargued above, the failure probability of one hash function is at most 3/4.\nIf we choose ten\nindependent hash functions and check that all of them have a bin of at least 5/6, then the\nfailure probability decreases to less than 1/16.\n\u25a0\nA.6\nProofs from Section 5\nProof (of Theorem 5.1).\nLet mi be the number of times the i-th element appears in the\nstream. Recall that m is the length of the stream. By computing a (1 + \u03b5\u2032)-approximation to\nthe \u03b1th moment (as in Fact 2.1) and dividing by ||A||\u03b1\n1 , we get a multiplicative approximation\nto F\u03b1/||A||\u03b1\n1 = ||x||\u03b1\n\u03b1. We can thus compute the value\n1\n1 \u2212\u03b1 log\n \n(1 \u00b1 \u03b5\u2032)\nn\nX\ni=1\nx\u03b1\ni\n!\n=\n1\n1 \u2212\u03b1 log\n n\nX\ni=1\nx\u03b1\ni\n!\n+ log(1 \u00b1 \u03b5\u2032)\n1 \u2212\u03b1\n= H\u03b1(X) \u00b1\n\u03b5\u2032\n1 \u2212\u03b1.\nSetting \u03b5\u2032 = \u03b5 \u00b7 |1 \u2212\u03b1|, we obtain an additive approximation algorithm using\nO\n\u0012\u0012\n|1 \u2212\u03b1|\n\u03b52 \u00b7 |\u03b1 \u22121|2 +\n1\n\u03b5 \u00b7 |\u03b1 \u22121|\n\u0013\nlog m\n\u0013\n= O(log m/(|1 \u2212\u03b1| \u00b7 \u03b52))\nbits, as claimed.\n\u25a0\nProof (of Theorem 5.2).\nIf \u03b1 \u2208(0, 1), then because the function x\u03b1 is concave, we get by\nJensen\u2019s inequality\nn\nX\ni=1\nxi\u03b1 \u2264n \u00b7\n\u0012 1\nn\n\u0013\u03b1\n= n1\u2212\u03b1.\nIf we compute a multiplicative (1 + (1 \u2212\u03b1) \u00b7 \u03b5 \u00b7 n\u03b1\u22121)-approximation to the \u03b1th moment, we\nobtain an additive (1 \u2212\u03b1) \u00b7 \u03b5-approximation to (Pn\ni=1 x\u03b1\ni ) \u22121. This in turn gives an additive\n\u03b5-approximation to T\u03b1. By Fact 2.1,\nO\n\u0012\u0012\n1 \u2212\u03b1\n((1 \u2212\u03b1) \u00b7 \u03b5 \u00b7 n\u03b1\u22121)2 +\n1\n(1 \u2212\u03b1) \u00b7 \u03b5 \u00b7 n\u03b1\u22121\n\u0013\nlog m\n\u0013\n= O(n2(1\u2212\u03b1) log m/((1 \u2212\u03b1)\u03b52))\nbits of space su\ufb03ce to achieve the required approximation to the \u03b1th moment.\nFor \u03b1 > 1, the value F\u03b1/||A||\u03b1\n1 is at most 1, so it su\ufb03ces to approximate F\u03b1 to within\na factor of 1 + (\u03b1 \u22121) \u00b7 \u03b5.\nFor \u03b1 \u2208(1, 2], again using Fact 2.1, we can achieve this using\nO(log m/((\u03b1 \u22121)\u03b52)) bits of space.\n\u25a0\nProof (of Lemma 5.3). Consider \ufb01rst \u03b1 \u2208(0, 1). For x \u2208(0, 5/6],\nx\u03b1\nx = x\u03b1\u22121 \u2265\n\u00125\n6\n\u0013\u03b1\u22121\n\u22651 + C1 \u00b7 (1 \u2212\u03b1),\n23\nfor some positive constant C1. The last equality follows from convexity of (5/6)y as a function\nof y. Hence,\nn\nX\ni=1\nx\u03b1\ni \u2265\nn\nX\ni=1\n(1 + C1(1 \u2212\u03b1))xi = 1 + C1(1 \u2212\u03b1),\nand furthermore,\n\f\f\f\f\f1 \u2212\nn\nX\ni=1\nx\u03b1\ni\n\f\f\f\f\f =\n n\nX\ni=1\nx\u03b1\ni\n!\n\u22121 \u2265C1 \u00b7 (1 \u2212\u03b1) = C1 \u00b7 |\u03b1 \u22121|\nWhen \u03b1 \u2208(1, 2], then for x \u2208(0, 5/6],\nx\u03b1\nx = x\u03b1\u22121 \u2264\n\u00125\n6\n\u0013\u03b1\u22121\n\u22641 \u2212C2 \u00b7 (\u03b1 \u22121),\nfor some positive constant C2. This implies that\nn\nX\ni=1\nx\u03b1\ni \u2264\nn\nX\ni=1\nxi(1 \u2212C2 \u00b7 (\u03b1 \u22121)) = 1 \u2212C2 \u00b7 (\u03b1 \u22121),\nand\n\f\f\f\f\f1 \u2212\nn\nX\ni=1\nx\u03b1\ni\n\f\f\f\f\f = 1 \u2212\nn\nX\ni=1\nx\u03b1\ni \u2265C2 \u00b7 (\u03b1 \u22121) = C2 \u00b7 |\u03b1 \u22121|.\nTo \ufb01nish the proof of the lemma, we set C = min{C1, C2}.\n\u25a0\nProof (of Lemma 5.5). We \ufb01rst argue that a multiplicative approximation to |1 \u2212x\u03b1\ni | can be\nobtained from a multiplicative approximation to 1 \u2212xi. Let g(y) = 1 \u2212(1 \u2212y)\u03b1. Note that\ng(1 \u2212xi) = 1 \u2212x\u03b1\ni . Since 1 \u2212xi \u2208[0, 1/3], we restrict the domain of g to [0, 1/3]. The derivative\nof g is g\u2032(y) = \u03b1(1\u2212y)\u03b1\u22121. Note that g is strictly increasing for \u03b1 \u2208(0, 1)\u222a(1, 2]. For \u03b1 \u2208(0, 1),\nthe derivative is in the range [\u03b1, 3\n2\u03b1]. For \u03b1 \u2208(1, 2], it always lies in the range [2\n3\u03b1, \u03b1]. In both\ncases, a (1 + 2\n3\u03b5)-approximation to y su\ufb03ces to compute a (1 + \u03b5)-approximation to g(y).\nWe now consider two cases:\n\u2022 Assume \ufb01rst that \u03b1 \u2208(0, 1). For any x \u2208(0, 1/3], we have\nx\u03b1\nx \u2265\n\u00121\n3\n\u0013\u03b1\u22121\n= 31\u2212\u03b1 \u22651 + C1(1 \u2212\u03b1),\nfor some positive constant C1. The last inequality follows from the convexity of the function\n31\u2212\u03b1. This means that if xi < 1, then\nP\nj\u0338=i x\u03b1\nj\n1 \u2212xi\n\u2265\nP\nj\u0338=i xj(1 + C1(1 \u2212\u03b1))\n1 \u2212xi\n= (1 \u2212xi)(1 + C1(1 \u2212\u03b1))\n1 \u2212xi\n= 1 + C1(1 \u2212\u03b1).\nSince xi \u2264x\u03b1\ni < 1, we also have\nP\nj\u0338=i x\u03b1\nj\n1 \u2212x\u03b1\ni\n\u2265\nP\nj\u0338=i x\u03b1\nj\n1 \u2212xi\n\u22651 + C1(1 \u2212\u03b1).\nThis implies that if we compute a multiplicative 1 + (1 \u2212\u03b1)\u03b5/D1-approximations to both\n1\u2212x\u03b1\ni and P\nj\u0338=i x\u03b1\nj , for su\ufb03ciently large constant D1, we compute a multiplicative (1+\u03b5)-\napproximation of (Pn\nj=1 x\u03b1\nj ) \u22121.\n24\n\u2022 The case of \u03b1 \u2208(1, 2] is similar. For any x \u2208(0, 1/3], we have\nx\u03b1\nx \u2264\n\u00121\n3\n\u0013\u03b1\u22121\n\u22641 \u2212C2(\u03b1 \u22121),\nfor some positive constant C2. Hence,\nP\nj\u0338=i x\u03b1\nj\n1 \u2212xi\n\u2264\nP\nj\u0338=i xj(1 \u2212C2(\u03b1 \u22121))\n1 \u2212xi\n= (1 \u2212xi)(1 \u2212C2(\u03b1 \u22121))\n1 \u2212xi\n= 1 \u2212C2(\u03b1 \u22121),\nand because x\u03b1\ni \u2264xi,\nP\nj\u0338=i x\u03b1\nj\n1 \u2212x\u03b1\ni\n\u2264\nP\nj\u0338=i x\u03b1\nj\n1 \u2212xi\n\u22641 \u2212C2(\u03b1 \u22121).\nThis implies that if we compute a multiplicative 1 + (\u03b1 \u22121)\u03b5/D2-approximations to both\n1 \u2212x\u03b1\ni and P\nj\u0338=i x\u03b1\nj , for su\ufb03ciently large constant D2, we can compute a multiplicative\n(1 + \u03b5)-approximation to 1 \u2212Pn\nj=1 x\u03b1\nj .\n\u25a0\nProof (of Theorem 5.6).\nWe run the algorithm of Section 4.1 to \ufb01nd out if there is a very\nheavy element. This only requires O(log n) words of space.\nIf there is no heavy element, then by Lemma 5.3 there is a constant C \u2208(0, 1) such that\n|1 \u2212P\ni x\u03b1\ni | \u2265C|\u03b1 \u22121|. We want to compute a multiplicative approximation to |1 \u2212P\ni x\u03b1\ni |. We\nknow that the di\ufb00erence between P\ni x\u03b1\ni and 1 is large. Therefore, if we compute a multiplicative\n(1+ 1\n2|\u03b1\u22121|C\u03b5)-approximation to P\ni x\u03b1\ni , we obtain an additive (1\n2|\u03b1\u22121|C\u03b5 P\ni x\u03b1\ni )-approximation\nto P\ni x\u03b1\ni . If P\ni x\u03b1\ni \u22642, then\n1\n2|\u03b1 \u22121|C\u03b5 P\ni x\u03b1\ni\n|1 \u2212P\ni x\u03b1\ni |\n\u2264|\u03b1 \u22121|C\u03b5\nC|\u03b1 \u22121| = \u03b5.\nIf P\ni x\u03b1\ni \u22652, then\n1\n2|\u03b1 \u22121|C\u03b5 P\ni x\u03b1\ni\n|1 \u2212P\ni x\u03b1\ni |\n\u22641\n2|\u03b1 \u22121|C\u03b5 \u00b7 2 \u2264\u03b5.\nIn either case, we obtain a multiplicative (1 + \u03b5)-approximation to |1 \u2212P\ni x\u03b1\ni |, which in turn\nyields a multiplicative approximation to the Tsallis entropy. We now need to bound the amount\nof space we use in this case. We use the estimator of Fact 2.1, which uses O(log m/(|\u03b1 \u22121|\u03b52))\nbits in our case.\nLet us focus now on the case when there is a heavy element. By Lemma 5.5 it su\ufb03ces to\napproximate F res\n1\nand F res\n\u03b1 , which we can do using the algorithm of Section 4.2. The number of\nbits required is\nO\n\u0012\nlog m\n\u03b5 \u00b7 |\u03b1 \u22121|\n\u0013\n+ \u02dcO\n\u0012|\u03b1 \u22121| \u00b7 log m\n(\u03b5 \u00b7 |\u03b1 \u22121|)2\n\u0013\n= \u02dcO\n\u0012\nlog m\n\u03b52 \u00b7 |\u03b1 \u22121|\n\u0013\n.\n\u25a0\nProof (of Lemma 5.7).\nFor t \u2208[4/9, 1], the derivative of the logarithm function lies in the\nrange [a, b], where a and b are constants such that 0 < a < b.\nThis implies that in this\n25\ncase, a (1 + \u03b5)-approximation to t \u22121 gives a 1 + b\na\u03b5 approximation to log(t). We are given\ny \u2208[1 \u2212t, (1 + \u03b5)(1 \u2212t)], and we can assume that y \u2208[1 \u2212t, min{5/9, (1 + \u03b5)(1 \u2212t)}]. We have\n\u2212log(t) \u2264\u2212log(1 \u2212y),\nand\n\u2212log(1 \u2212y)\n\u2212log(t)\n\u2264\n\u2212log(1 \u2212(1 + \u03b5)(1 \u2212t))\n\u2212log(t)\n= \u2212log(t \u2212\u03b5(1 \u2212t))\n\u2212log(t)\n\u2264\n\u2212log(t) + (\u2212log(t \u2212\u03b5(1 \u2212t)) + log(t))\n\u2212log(t)\n\u2264\n1 + \u2212log(t \u2212\u03b5(1 \u2212t)) + log(t)\n\u2212log(t)\n\u2264\n1 + \u03b5(1 \u2212t) \u00b7 maxz\u2208[max{t\u2212\u03b5(1\u2212t),4/9},t](log(z))\u2032\n(1 \u2212t) \u00b7 minz\u2208[4/9,1](log(z))\u2032\n\u2264\n1 + \u03b5(1 \u2212t) \u00b7 maxz\u2208[t,1](log(z))\u2032\n(1 \u2212t) \u00b7 minz\u2208[4/9,1](log(z))\u2032\n\u2264\n1 + \u03b5(1 \u2212t) \u00b7 b\n(1 \u2212t) \u00b7 a = 1 + b\na\u03b5.\nConsider now t > 1. We are given y \u2208[t \u22121, (1 + \u03b5)(t \u22121)], and we have\nlog(t) \u2264log(y + 1) \u2264log((1 + \u03b5)(t \u22121) + 1).\nFurthermore,\nlog((1 + \u03b5)(t \u22121) + 1)\nlog(t)\n\u2264\nlog(t) + log((1 + \u03b5)(t \u22121) + 1) \u2212log(t)\nlog(t)\n=\n1 + log(t + (t \u22121)\u03b5) \u2212log(t)\nlog(t)\n=\n1 +\nR t+(t\u22121)\u03b5\nt\n(log(z))\u2032dz\nR t\n1(log(z))\u2032dz\n\u2264\n1 + (t \u22121)\u03b5 maxz\u2208[t,t+(t\u22121)\u03b5](log(z))\u2032\n(t \u22121) maxz\u2208[1,t](log(z))\u2032\n\u2264\n1 + (t \u22121)\u03b5\nt \u22121\n= 1 + \u03b5.\nHence, we get a good multiplicative approximation to log(t).\n\u25a0\nProof (of Theorem 5.8).\nWe use the algorithm of Section 4.1 to check if there is a single\nelement of high frequency. This only requires O(log m) bits of space.\nIf there is no element of frequency greater than 5/6, then the R\u00b4enyi entropy for any \u03b1 is\ngreater than the min-entropy H\u221e= \u2212log maxi xi \u2265log(6/5). Therefore, in this case it su\ufb03ces\nto run the additive approximation algorithm with \u03b5\u2032 = log(6/5)\u03b5 to obtain a su\ufb03ciently good\nestimate. To run that algorithm, we use O\n\u0010\nlog m\n|1\u2212\u03b1|\u03b52\n\u0011\nbits of space.\nLet us consider the other case, when there is an element of frequency at least 2/3.\nFor\n\u03b1 \u2208(1, 2], we have\n\u00122\n3\n\u00132\n\u2264\nX\ni\nx\u03b1\ni \u22641,\n26\nand for \u03b1 \u2208(0, 1), Pn\ni=1 x\u03b1\ni \u22651. Therefore, by Lemma 5.7, it su\ufb03ces to compute a multiplicative\napproximation to |1 \u2212P\ni x\u03b1\ni |, which we can do by Lemma 5.5. By algorithms from Section 4.3\nand Section 4.2, we can compute the multiplicative (1 + \u0398(|1 \u2212\u03b1|\u03b5))-approximations required\nby Lemma 5.5 with the same space complexity as for the approximation of Tsallis entropy (see\nthe proof of Theorem 5.6).\n\u25a0\nProof (of Theorem 5.9).\nThe proof is nearly identical to that of Theorem 3.1 in [2]. We\nneed merely observe that if \u02dcH\u03b1 is a (1 + \u03b5)-approximation to H\u03b1, then m\u03b1(1+\u03b5)2(1\u2212\u03b1) \u02dc\nH\u03b1 is a\nmultiplicative m\u03b1\u03b5-approximation to F\u03b1. From here, we set t = cm\u03b5n1/\u03b1 and argue identically\nas in [2] via a reduction from t-party disjointness; we omit the details.\n\u25a0\n27\n",
        "sentence": "",
        "context": "nickh@mit.edu.\nSupported by a Natural\nSciences and Engineering Research Council of Canada PGS Scholarship, by NSF contract CCF-0515221 and by\nONR grant N00014-05-1-0148.\nONR grant N00014-05-1-0148.\n\u2020MIT Computer Science and Arti\ufb01cial Intelligence Laboratory. minilek@mit.edu. Supported by a National\nDefense Science and Engineering Graduate (NDSEG) Fellowship.\n[27] Carlo Ricotta, Alessandra Pacini, and Giancarlo Avena. Parametric scaling from species\nto growth-form diversity: an interesting analogy with multifractal functions. Biosystems,\n65(2\u20133):179\u2013186, 2002."
    },
    {
        "title": "Quasi-analytic class and closure of {tn} in the interval (\u2212\u221e,\u221e)",
        "author": [
            "S. Izumi",
            "T. Kawata"
        ],
        "venue": "Tohoku Math. J.,",
        "citeRegEx": "Izumi and Kawata.,? \\Q1937\\E",
        "shortCiteRegEx": "Izumi and Kawata.",
        "year": 1937,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Pseudorandomness for network algorithms",
        "author": [
            "Russell Impagliazzo",
            "Noam Nisan",
            "Avi Wigderson"
        ],
        "venue": "In STOC,",
        "citeRegEx": "Impagliazzo et al\\.,? \\Q1994\\E",
        "shortCiteRegEx": "Impagliazzo et al\\.",
        "year": 1994,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Efficient noise-tolerant learning from statistical queries",
        "author": [
            "M. Kearns"
        ],
        "venue": "Journal of the ACM,",
        "citeRegEx": "Kearns.,? \\Q1998\\E",
        "shortCiteRegEx": "Kearns.",
        "year": 1998,
        "abstract": "\n            In this paper, we study the problem of learning in the presence of classification noise in the probabilistic learning model of Valiant and its variants. In order to identify the class of \u201crobust\u201d learning algorithms in the most general way, we formalize a new but related model of learning from\n            statistical queries\n            . Intuitively, in this model a learning algorithm is forbidden to examine individual examples of the unknown target function, but is given acess to an oracle providing estimates of probabilities over the sample space of random examples.\n          \n          One of our main results shows that any class of functions learnable from statistical queries is in fact learnable with classification noise in Valiant's model, with a  noise rate approaching the  information-theoretic barrier of 1/2. We then demonstrate the generality of the statistical query model, showing that practically every class learnable in Valiant's model and its variants can also be learned in the new model (and thus can be learned in the presence of noise). A notable exception to this statement is the class of parity functions, which we prove is not learnable from statistical queries, and for which no noise-tolerant algorithm is known.",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Learning halfspaces under logconcave densities: Polynomial approximations and moment matching",
        "author": [
            "Daniel M. Kane",
            "Adam Klivans",
            "Raghu Meka"
        ],
        "venue": "In COLT,",
        "citeRegEx": "Kane et al\\.,? \\Q2013\\E",
        "shortCiteRegEx": "Kane et al\\.",
        "year": 2013,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Agnostically learning halfspaces",
        "author": [
            "Adam Tauman Kalai",
            "Adam R. Klivans",
            "Yishay Mansour",
            "Rocco A. Servedio"
        ],
        "venue": "SIAM J. Comput.,",
        "citeRegEx": "Kalai et al\\.,? \\Q2008\\E",
        "shortCiteRegEx": "Kalai et al\\.",
        "year": 2008,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Pseudorandom generators for group products",
        "author": [
            "Michal Kouck\u00fd",
            "Prajakta Nimbhorkar",
            "Pavel Pudl\u00e1k"
        ],
        "venue": "In STOC,",
        "citeRegEx": "Kouck\u00fd et al\\.,? \\Q2011\\E",
        "shortCiteRegEx": "Kouck\u00fd et al\\.",
        "year": 2011,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Learning geometric concepts via gaussian surface area",
        "author": [
            "Adam R. Klivans",
            "Ryan O\u2019Donnell",
            "Rocco A. Servedio"
        ],
        "venue": "In FOCS,",
        "citeRegEx": "Klivans et al\\.,? \\Q2008\\E",
        "shortCiteRegEx": "Klivans et al\\.",
        "year": 2008,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Learning DNF in time 2\u00f5(n 1/3)",
        "author": [
            "Adam R. Klivans",
            "Rocco A. Servedio"
        ],
        "venue": "J. Comput. Syst. Sci.,",
        "citeRegEx": "Klivans and Servedio.,? \\Q2004\\E",
        "shortCiteRegEx": "Klivans and Servedio.",
        "year": 2004,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Unconditional lower bounds for learning intersections of halfspaces",
        "author": [
            "Adam R Klivans",
            "Alexander A Sherstov"
        ],
        "venue": "Machine Learning,",
        "citeRegEx": "Klivans and Sherstov.,? \\Q2007\\E",
        "shortCiteRegEx": "Klivans and Sherstov.",
        "year": 2007,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Cryptographic hardness for learning intersections of halfspaces",
        "author": [
            "Adam R. Klivans",
            "Alexander A. Sherstov"
        ],
        "venue": "J. Comput. Syst. Sci.,",
        "citeRegEx": "Klivans and Sherstov.,? \\Q2009\\E",
        "shortCiteRegEx": "Klivans and Sherstov.",
        "year": 2009,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Lower bounds for agnostic learning via approximate rank",
        "author": [
            "Adam R. Klivans",
            "Alexander A. Sherstov"
        ],
        "venue": "Computational Complexity,",
        "citeRegEx": "Klivans and Sherstov.,? \\Q2010\\E",
        "shortCiteRegEx": "Klivans and Sherstov.",
        "year": 2010,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Toward efficient agnostic learning",
        "author": [
            "Michael Kearns",
            "Robert E. Schapire",
            "Linda M. Sellie",
            "Lisa Hellerstein"
        ],
        "venue": "In Machine Learning,",
        "citeRegEx": "Kearns et al\\.,? \\Q1994\\E",
        "shortCiteRegEx": "Kearns et al\\.",
        "year": 1994,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "On efficient agnostic learning of linear combinations of basis functions",
        "author": [
            "Wee Sun Lee",
            "Peter L. Bartlett",
            "Robert C. Williamson"
        ],
        "venue": "In Proceedings of the Eighth Annual Conference on Computational Learning Theory, COLT",
        "citeRegEx": "Lee et al\\.,? \\Q1995\\E",
        "shortCiteRegEx": "Lee et al\\.",
        "year": 1995,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "A survey of weighted polynomial approximation with exponential weights",
        "author": [
            "Doron Lubinsky"
        ],
        "venue": "Surveys in Approximation Theory,",
        "citeRegEx": "Lubinsky.,? \\Q2007\\E",
        "shortCiteRegEx": "Lubinsky.",
        "year": 2007,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Perceptrons: An Introduction to Computational Geometry",
        "author": [
            "Marvin Minsky",
            "Seymour Papert"
        ],
        "venue": null,
        "citeRegEx": "Minsky and Papert.,? \\Q1972\\E",
        "shortCiteRegEx": "Minsky and Papert.",
        "year": 1972,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Pseudorandom generators for polynomial threshold functions",
        "author": [
            "Raghu Meka",
            "David Zuckerman"
        ],
        "venue": "In Proceedings of the Forty-second ACM Symposium on Theory of Computing,",
        "citeRegEx": "Meka and Zuckerman.,? \\Q2010\\E",
        "shortCiteRegEx": "Meka and Zuckerman.",
        "year": 2010,
        "abstract": "We study the natural question of constructing pseudorandom generators (PRGs)\nfor low-degree polynomial threshold functions (PTFs). We give a PRG with\nseed-length log n/eps^{O(d)} fooling degree d PTFs with error at most eps.\nPreviously, no nontrivial constructions were known even for quadratic threshold\nfunctions and constant error eps. For the class of degree 1 threshold functions\nor halfspaces, we construct PRGs with much better dependence on the error\nparameter eps and obtain a PRG with seed-length O(log n + log^2(1/eps)).\nPreviously, only PRGs with seed length O(log n log^2(1/eps)/eps^2) were known\nfor halfspaces. We also obtain PRGs with similar seed lengths for fooling\nhalfspaces over the n-dimensional unit sphere.\n  The main theme of our constructions and analysis is the use of invariance\nprinciples to construct pseudorandom generators. We also introduce the notion\nof monotone read-once branching programs, which is key to improving the\ndependence on the error rate eps for halfspaces. These techniques may be of\nindependent interest.",
        "full_text": "arXiv:0910.4122v5  [cs.CC]  15 Nov 2011\nPseudorandom Generators for Polynomial Threshold Functions\u2217\nRaghu Meka\u2217\nDavid Zuckerman\u2020\nDepartment of Computer Science, University of Texas at Austin\n{raghu,diz}@cs.utexas.edu\nAbstract\nWe study the natural question of constructing pseudorandom generators (PRGs) for low-\ndegree polynomial threshold functions (PTFs). We give a PRG with seed-length log n/\u01ebO(d)\nfooling degree d PTFs with error at most \u01eb.\nPreviously, no nontrivial constructions were\nknown even for quadratic threshold functions and constant error \u01eb.\nFor the class of degree\n1 threshold functions or halfspaces, previously only PRGs with seedlength O(log n log2(1/\u01eb)/\u01eb2)\nwere known. We improve this dependence on the error parameter and construct PRGs with\nseedlength O(log n + log2(1/\u01eb)) that \u01eb-fool halfspaces. We also obtain PRGs with similar seed\nlengths for fooling halfspaces over the n-dimensional unit sphere.\nThe main theme of our constructions and analysis is the use of invariance principles to con-\nstruct pseudorandom generators. We also introduce the notion of monotone read-once branching\nprograms, which is key to improving the dependence on the error rate \u01eb for halfspaces. These\ntechniques may be of independent interest.\n\u2217A preliminary version of this work appeared in STOC 2010.\n\u2020Partially supported by NSF Grants CCF-0634811 and CCF-0916160 and THECB ARP Grant 003658-0113-2007.\n1\nIntroduction\nPolynomial threshold functions are a fundamental class of functions with many important applica-\ntions in complexity theory [Bei93], learning theory [KS04], quantum complexity theory [BBC+01],\nvoting theory [ABFR94] and more. A polynomial threshold function (PTF) of degree d is a function\nf : {1, \u22121}n \u2192{1, \u22121} of the form f(x) = sign(P(x)\u2212\u03b8), where P : {1, \u22121}n \u2192R is a multi-linear\npolynomial of degree d. Of particular importance are the class of degree 1 threshold functions, also\nknown as halfspaces, which have been instrumental in the development of many fundamental tools\nin learning theory such as perceptrons, support vector machines and boosting.\nHere we address the natural problem of explicitly constructing pseudorandom generators (PRGs)\nfor PTFs. Derandomizing natural complexity classes is a fundamental problem in complexity the-\nory, with several applications outside complexity theory. For instance, PRGs for PTFs facilitate\nestimating the accuracy of PTF classi\ufb01ers in machine learning with a small number of deterministic\nsamples; PRGs for spherical caps and PRGs for intersections of halfspaces can help derandomize\nrandomized algorithms such as the Goemans-Williamson Max-Cut algorithm.\nIn this work, we give the \ufb01rst nontrivial pseudorandom generators for low-degree PTFs.\nDe\ufb01nition 1.1. A function G : {0, 1}r \u2192{1, \u22121}n is a PRG with error \u01eb for (or \u01eb-fools) PTFs of\ndegree d, if\n|\nE\nx\u2208u{1,\u22121}n[f(x)] \u2212\nE\ny\u2208u{0,1}r[f(G(y))] | \u2264\u01eb,\nfor all PTFs f of degree at most d. (Here x \u2208u S denotes a uniformly random element of S.)\nWe refer to the parameter r as the seed-length of the generator G and say the generator\nis explicit if it is computable by a (deterministic) polynomial time algorithm. It can be shown\nby the probabilistic method that there exist PRGs that \u01eb-fool degree d PTFs with seed length\nr = O(d log n+log(1/\u01eb)) (see Appendix A). However, despite their long history, until recently very\nlittle was known about explicitly constructing such PRGs, even for the special class of halfspaces.\nIn this work, we present a PRG that \u01eb-fools degree d PTFs with seed length log n/\u01ebO(d). Pre-\nviously, PRGs with seed length o(n) were not known even for degree 2 PTFs and constant \u01eb.\nTheorem 1.2. For 0 < \u01eb < 1, there exists an explicit PRG fooling PTFs of degree d with error at\nmost \u01eb and seed length 2O(d) log n/\u01eb8d+3.\nIndependent of our work, Diakonikolas et al. [DKN10] showed that bounded independence fools\ndegree 2 PTFs and in particular give a PRG with seed-length (log n) \u00b7 \u02dcO(1/\u01eb9) for degree 2 PTFs\n(here \u02dcO hides poly-logarithmic factors). In another independent work, Ben-Eliezer et al. [BELY09]\nshowed that bounded independence fools certain special classes of PTFs.\nFor the d = 1 case of halfspaces, Diakonikolas et al. [DGJ+09] constructed PRGs with seed\nlength O(log n) for constant error rates.\nPRGs with seed length O(log2 n) for halfspaces with\npolynomially bounded weights follow easily from known results. However, nothing nontrivial was\nknown for general halfspaces, for instance, when \u01eb = 1/\u221an. In this work we construct PRGs with\nexponentially better dependence on the error parameter \u01eb.\nTheorem 1.3. For all constants c, \u01eb \u22651/nc, there exists an explicit PRG fooling halfspaces with\nerror at most \u01eb and seed length O(log n + log2(1/\u01eb)).\nWe also obtain results similar to the above for spherical caps. The problem of constructing\nPRGs for spherical caps was brought to our attention by Amir Shpilka; Karnin et al. [KRS09]\nwere the \ufb01rst to obtain a PRG with similar parameters using di\ufb00erent methods. They achieve a\nseed-length of (1 + o(1)) log n + O(log2(1/\u01eb)).\n1\nTheorem 1.4. There exists a constant c > 0 such that for all \u01eb > c log n/n1/4, there exists an\nexplicit PRG fooling spherical caps with error at most \u01eb and seed length O(log n + log2(1/\u01eb)).\nWe brie\ufb02y summarize the previous constructions for halfspaces.\n1. Halfspaces with polynomially bounded integer weights can be computed by polynomial width\nread-once branching programs (ROBPs). Thus, the PRGs for ROBPs such as those of Nisan\n[Nis92] and Impagliazzo et al. [INW94] fool halfspaces with polynomially bounded integer\nweights with seed length O(log2 n). However, a simple counting argument ([MT94], [H\u02daas94])\nshows that almost all halfspaces have exponentially large weights.\n2. Diakonikolas et al. [DGJ+09] showed that k-wise independent spaces fool halfspaces for k =\nO(log2(1/\u01eb)/\u01eb2). By using the known e\ufb03cient constructions of k-wise independent spaces\nthey obtain PRGs for halfspaces with seed length O(log n log2(1/\u01eb)/\u01eb2).\n3. Rabani and Shpilka [RS09] gave explicit constructions of polynomial size hitting sets for\nhalfspaces.\nThe overarching theme behind all our constructions is the use of invariance principles to get\npseudorandom generators. Broadly speaking, invariance principles for a class of functions say that\nunder mild conditions (typically on the \ufb01rst few moments) the distribution of the functions is\nessentially invariant for all product distributions. Intuitively, invariance principles could be helpful\nin constructing pseudorandom generators as we can hope to exploit the invariance with respect\nto product distributions by replacing a product distribution with a \u201csmaller product distribution\u201d\nthat still satis\ufb01es the conditions for applying the invariance principle. We believe that the above\ntechnique could be helpful for other derandomization problems.\nAnother aspect of our constructions is what we call the \u201cmonotone trick\u201d.\nThe PRGs for\nsmall-width read-once branching programs (ROBP) from the works of Nisan [Nis92], Impagliazzo\net al. [INW94], and Nisan and Zuckerman [NZ96], have been a fundamental tool in derandomization\nwith several applications [Siv02], [RV05], [GR09]. An important ingredient in our PRG for halfs-\npaces is our observation that any PRG for small-width ROBPs fools arbitrary width \u201cmonotone\u201d\nROBPs. Roughly speaking, we say an ROBP is monotone if there exists an ordering on the nodes\nin each layer of the program so that the corresponding sets of accepting strings respect the ordering\n(see De\ufb01nition 2.4). We believe that this notion of monotone ROBP is quite natural and combined\nwith the \u201cmonotone trick\u201d could be useful elsewhere.\nThe above techniques have recently found other applications that we brie\ufb02y describe in Sec-\ntion 1.2. We now give a high level view of our constructions and their analyses.\n1.1\nOutline of Constructions\nOur constructions build mainly on the hitting set construction for halfspaces of Rabani and Shpilka.\nAlthough the constructions and analyses are similar in spirit for halfspaces and higher degree PTFs,\nfor clarity, we deal with the two classes separately, at the cost of some repetition. The analysis is\nsimpler for halfspaces and provides intuition for the more complicated analysis for higher degree\nPTFs.\n1.1.1\nPRGs for Halfspaces\nOur \ufb01rst step in constructing PRGs for halfspaces is to use our \u201cmonotone trick\u201d to show that PRGs\nfor polynomial width read-once branching programs (ROBPs) also fool halfspaces.\nPreviously,\n2\nPRGs for polynomial width ROBPs were only known to fool halfspaces with polynomially bounded\nweights. Although the natural simulation of halfspaces by ROBP may require polynomially large\nwidth, we note that the resulting ROBP is what we call monotone (see De\ufb01nition 2.4). We show\nthat PRGs for polynomial width ROBP fool monotone ROBPs of arbitrary width.\nTheorem 1.5. A PRG that \u03b4-fools monotone ROBP of width log(4T/\u01eb) and length T fools mono-\ntone ROBP of arbitrary width and length T with error at most \u01eb + \u03b4.\nSee Theorem 2.5 for a more formal statement. As a corollary we get the following.\nCorollary 1.6. For all \u01eb > 0, a PRG that \u03b4-fools width log(4n/\u01eb) and length n ROBPs fools\nhalfspaces on n variables with error at most \u01eb + \u03b4.\nThe above result already improves on the previous constructions for small \u01eb, giving a PRG\nwith seed length O(log2 n) for \u01eb = 1/poly(n). However, the randomness used is O(log2 n) even for\nconstant \u01eb.\nWe next improve the dependence of the seed length on the error parameter \u01eb to obtain our\nmain results for fooling halfspaces. Following the approach of Diakonikolas et al. [DGJ+09] we \ufb01rst\nconstruct PRGs fooling regular halfspaces. A halfspace with coe\ufb03cients (w1, . . . , wn) is regular if\nno coe\ufb03cient is signi\ufb01cantly larger than the others. Such halfspaces are easier to analyze because\nfor regular w, the distribution of \u27e8w, x\u27e9with x uniformly distributed in {1, \u22121}n is close to a normal\ndistribution by the Central Limit Theorem. Using a quantitative form of the above statement, the\nBerry-Ess\u00b4een theorem, we show that a simpli\ufb01ed version of the hitting set construction of Rabani\nand Shpilka gives a PRG fooling regular halfspaces.\nHaving fooled regular halfspaces, we use the structural results on halfspaces of Servedio [Ser06]\nand Diakonikolas et al. [DGJ+09] to fool arbitrary halfspaces. The structural results of Servedio\nand Diakonikolas et al. roughly show that either a halfspace is regular or is close to a function\ndepending only on a small number of coordinates. Given this, we proceed by a case analysis as\nin Diakonikolas et al.: if a halfspace is regular, we use the analysis for regular halfspaces; else, we\nargue that bounded independence su\ufb03ces.\nThe above analysis gives a PRG fooling halfspaces with seed length O(log n log2(1/\u01eb)/\u01eb2), match-\ning the PRG of Diakonikolas et al. [DGJ+09]. However, not only is our construction simpler to\nanalyze (for the regular case), but we can also apply our \u201cmonotone trick\u201d to derandomize the\nconstruction. Derandomizing using the PRG for ROBPs of Impagliazzo et al. [INW94] gives The-\norem 1.3.\nFor spherical caps, we give a simpler more direct construction based on our generator for regular\nhalfspaces. We use an idea of Ailon and Chazelle [AC06] and the invariance of spherical caps with\nrespect to unitary rotations to convert the case of arbitrary spherical caps to regular spherical caps.\nWe defer the details to Section 6.\n1.1.2\nPRGs for PTFs\nWe next extend our PRG for halfspaces to fool higher degree polynomial threshold functions. The\nconstruction we use to fool PTFs is a natural extension of our underandomized PRG for halfspaces.\nThe analysis, though similar in outline, is signi\ufb01cantly more complicated and at a high level proceeds\nas follows.\nAs was done for halfspaces we \ufb01rst study the case of regular PTFs. The mainstay of our analysis\nfor regular halfspaces is the Berry-Ess\u00b4een theorem for sums of independent random variables. By\nusing the generalized Berry-Ess\u00b4een type theorem, or invariance principle, for low-degree multi-linear\npolynomials, proved by Mossel et al. [MOO05], we extend our analysis for regular halfspaces to\n3\nregular PTFs. We remark that unlike the case for halfspaces, we cannot use the invariance principle\nof Mossel et al. directly, but instead adapt their proof technique for our generator. In particular,\nwe crucially use the fact that most of the arguments of Mossel et al. work even for distributions\nwith bounded independence.\nWe then use structural results for PTFs of Diakonikolas et al. [DSTW10] and Harsha et\nal. [HKM09] that generalize the results of Servedio [Ser06] and Diakonikolas et al. [DGJ+09] for\nhalfspaces. Roughly speaking, these results show the following: with at least a constant probabil-\nity, upon randomly restricting a small number of variables, the resulting restricted PTF is either\nregular or has high bias. However, we cannot yet use the above observation to do a case analysis\nas was done for halfspaces; instead, we give a more delicate argument with recursive application of\nthe results on random restrictions.\n1.2\nOther Applications\nGopalan et al. [GOWZ10] showed that our generator, when suitably modi\ufb01ed, fools arbitrary func-\ntions of d halfspaces under product distributions where each coordinate has bounded fourth mo-\nment. To \u01eb-fool any size-s, depth-d decision tree of halfspaces, their generator uses seed length\nO((d log(ds/\u01eb) + log n) \u00b7 log(ds/\u01eb)). For monotone functions of k halfspaces, their seed length be-\ncomes O((k log(k/\u01eb) + log n) \u00b7 log(k/\u01eb)).\nThey get better bounds for larger \u01eb; for example, to\n1/poly(log n)-fool all monotone functions of (log n)/ log log n halfspaces, their generator requires a\nseed of length just O(log n).\nBuilding on techniques from this work and a new invariance principle for polytopes, Harsha\net al. [HKM10] obtained pseudorandom generators that \u01eb-fool certain classes of intersections of k\nhalfspaces with seed length (log n) \u00b7 poly(log k, 1/\u01eb). As an application of their results, Harsha et\nal. obtained the \ufb01rst deterministic quasi-polynomial time approximate-counting algorithms for a\nlarge class of integer programs.\nIn other subsequent work, Gopalan et al. [GKM10] used ideas motivated by the monotone trick\nto give the \ufb01rst deterministic polynomial time, relative error approximate-counting algorithms for\nknapsack and related problems.\nWe \ufb01rst present our result on fooling arbitrary width monotone ROBPs with PRGs for small-\nwidth ROBPs.\n2\nPRGs for Monotone ROBPs\nWe start with some de\ufb01nitions.\nDe\ufb01nition 2.1 (ROBP). An (S, D, T)-branching program M is a layered multi-graph with a layer\nfor each 0 \u2264i \u2264T and at most 2S vertices (states) in each layer. The \ufb01rst layer has a single vertex\nv0 and each vertex in the last layer is labeled with 0 (rejecting) or 1 (accepting). For 0 \u2264i < T, a\nvertex v in layer i has exactly 2D outgoing edges each labeled with an element of {0, 1}D and ending\nat a vertex in layer i + 1.\nNote that by de\ufb01nition, an (S, D, T)-branching program is read-once. We also use the following\nnotation. Let M be an (S, D, T)-branching program and v a vertex in layer i of M.\n1. For z = (zi, zi+1, . . . , zT ) \u2208({0, 1}D)T+1\u2212i call (v, z) an accepting pair if starting from v and\ntraversing the path with edges labeled z in M leads to an accepting state.\n4\n2. For z \u2208({0, 1}D)T , let M(z) = 1 if (v1, z) is an accepting pair, and M(z) = 0 otherwise.\n3. AM(v) = {z : (v, z) is accepting in M} and PM(v) is the probability that (v, z) is an accepting\npair for z chosen uniformly at random.\n4. For brevity, let U denote the uniform distribution over ({0, 1}D)T .\nDe\ufb01nition 2.2. A function G : {0, 1}r \u2192({0, 1}D)T is said to \u01eb-fool (S, D, T)-branching programs\nif, for all (S, D, T)-branching programs M,\n| Pr\nz\u2190U [M(z) = 1] \u2212\nPr\ny\u2208u{0,1}r [M(G(y)) = 1] | \u2264\u01eb.\nNisan [Nis92] and Impagliazzo et al. [INW94] gave PRGs that \u03b4-fool (S, D, T)-branching pro-\ngrams with seed length r = O((S + D) log T + log(T/\u03b4) log T). For T = poly(S, D), the PRG of\nNisan and Zuckerman [NZ96] fools (S, D, T)-branching programs with seed length r = O(S + D).\nWe state the bounds of the generator of Impagliazzo et al. below.\nTheorem 2.3 (Impagliazzo et al. [INW94]). There exists an explicit generator GINW : {0, 1}r \u2192\n({0, 1}D)T that \u03b4-fools (S, D, T)-branching programs with seed-length r = O(D+(S+log(T/\u03b4) log T)).\nHere we show that the above PRGs in fact fool arbitrary width monotone branching programs\nas de\ufb01ned below.\nDe\ufb01nition 2.4 (Monotone ROBP). An (S, D, T)-branching program M is said to be monotone if\nfor all 0 \u2264i < T, there exists an ordering {v1 \u227av2 \u227a. . . \u227avli} of the vertices in layer i such that\nfor 1 \u2264j < k \u2264li, AM(vj) \u2286AM(vk).\nTheorem 2.5. Let 0 < \u01eb < 1 and G : {0, 1}R \u2192({0, 1}D)T be a PRG that \u03b4-fools monotone\n(log(2T/\u01eb), D, T)-branching programs. Then G fools monotone (S, D, T)-branching programs for\narbitrary S with error at most \u01eb + \u03b4.\nIn particular, for \u03b4 = 1/poly(T) the above theorem gives a PRG fooling monotone (S, D, T)-\nbranching programs with error at most \u03b4 + \u01eb and seed length O(D + log(T/\u01eb) log T). Note that the\nseed length does not depend on the space S. Given the above result, Corollary 1.6 follows easily.\nProof of Corollary 1.6. A halfspace with weight vector w \u2208Rn and threshold \u03b8 \u2208R can be naturally\ncomputed by an (S, 1, n)-branching program Mw,\u03b8, for S large enough, by letting the states in layer\ni correspond to the partial sums Pi\nj=1 wjxj.\nIt is easy to check that Mw,\u03b8 is monotone.\nThe\ntheorem now follows from Theorem 2.5.\nWe now prove Theorem 2.5. The proof is based on the simple idea of \u201csandwiching\u201d monotone\nbranching programs between small-width branching programs. To this end, let M be a mono-\ntone (S, D, T)-branching program and call a pair of (s, D, T)-branching programs (Mdown, Mup),\n\u01eb-sandwiching for M if the following hold.\n1. For all z \u2208({0, 1}D)T , Mdown(z) \u2264M(z) \u2264Mup(z).\n2. Prz\u2190U[Mup(z) = 1] \u2212Prz\u2190U[Mdown(z) = 1] \u2264\u01eb.\nWe \ufb01rst show that to fool monotone branching programs it su\ufb03ces to fool small-width sandwiching\nprograms between which the monotone branching program is sandwiched. We then show that every\nmonotone branching program can be sandwiched between two small-width branching programs.\n5\nLemma 2.6. If a PRG G \u03b4-fools (s, D, T)-branching programs, and there exist (s, D, T)-branching\nprograms (Mdown, Mup) that are \u01eb-sandwiching for M, then G (\u01eb + \u03b4)-fools M.\nProof. Let D denote the output distribution of G. Then,\nPr\nz\u2190U[Mdown(z) = 1] \u2264Pr\nz\u2190U[M(z) = 1],\nPr\nz\u2190D[M(z) = 1] \u2264Pr\nz\u2190D[Mup(z) = 1].\nFurther, since D \u03b4-fools Mup,\nPr\nz\u2190D[Mup(z) = 1] \u2264Pr\nz\u2190U[Mup(z) = 1] + \u03b4.\nThus,\nPr\nz\u2190D[M(z) = 1] \u2212Pr\nz\u2190U[M(z) = 1] \u2264Pr\nz\u2190U[Mup(z) = 1] \u2212Pr\nz\u2190U[Mdown(z) = 1] + \u03b4 \u2264\u01eb + \u03b4.\nBy a similar argument with the roles of Mup, Mdown interchanged, we get\n| Pr\nz\u2190D[M(z) = 1] \u2212Pr\nz\u2190U[M(z) = 1]| \u2264\u01eb + \u03b4.\nLemma 2.7. For any monotone (S, D, T)-branching program M, there exist (log(2T/\u01eb), D, T)-\nbranching programs (Mdown, Mup) that are \u01eb-sandwiching for M.\nProof. We \ufb01rst set up some notation. For 0 \u2264i \u2264T, let the vertices in layer i of M be V i = {vi\n1 \u227a\nvi\n2 \u227a. . . \u227avi\nli}. For J \u2286V i, let min(J), max(J) denote the minimum and maximum elements of J\nunder \u227a. Call J \u2286V i an interval if there exist indices p \u2264q such that J = {vi\np, vi\np+1, . . . , vi\nq}.\nFor each 1 \u2264i \u2264T, partition the vertices of layer i into at most ti \u22642T/\u01eb intervals Ji\n1, Ji\n2, . . . , Ji\nti\nso that for any interval Ji\nk and v, v\u2032 \u2208Ji\nk,\n|PM(v) \u2212PM(v\u2032)| \u2264\u01eb\n2T .\n(2.1)\nLet s = log(2T/\u01eb) and de\ufb01ne an (s, D, T)-branching program Mup as follows. The vertices in layer i\nof Mup are Bi = {max(Ji\n1), max(Ji\n2), . . . , max(Ji\nti)} and the edges are placed by rounding the edges\nof M upwards as follows. For v \u2208Bi suppose there is an edge labeled z between v and a vertex\nw \u2208J = Ji+1\nk\n. Then, we place an edge labeled z between v and max(J). Mdown is de\ufb01ned similarly\nby using min(J) instead of max(J) as above. We claim that Mup, Mdown are \u01eb-sandwiching for M.\nWe analyze Mup below; the analysis for Mdown is similar.\nClaim 2.8. For 0 \u2264i \u2264T and v \u2208Bi, AM(v) \u2286AMup(v). In particular, for any z, M(z) \u2264\nMup(z).\nProof. Follows from the monotonicity of M.\nClaim 2.9. For 0 \u2264i \u2264T, and v \u2208Bi, PMup(v) \u2212PM(v) \u2264(T \u2212i) \u01eb\n2T . In particular, for z chosen\nuniformly at random, Pr[Mup(z) = 1] \u2212Pr[M(z) = 1] \u2264\u01eb/2.\nProof. The second part of the claim follows from the \ufb01rst. The proof is by downward induction\non i. For i = T, the statement is true trivially. Now, suppose the claim is true for all j \u2265i + 1.\nLet v \u2208Bi and let z = (zi+1, \u00afz) be uniformly chosen from ({0, 1}D)T\u2212i with zi+1 \u2208u {0, 1}D. Let\n6\n\u0393(v, zi+1) \u2208J(v, zi+1) = Ji+1\nk\nfor one of the intervals of layer i + 1. Then, the edge labeled zi+1\nfrom v goes to max(J(v, zi+1)) in Mup. Now,\nPM(v) =\nX\nu\u2208{0,1}D\nPr[zi+1 = u] PM(\u0393(v, u))\n\u2265\nX\nu\u2208{0,1}D\nPr[zi+1 = u]\n\u0010\nPM(max(J(v, u))) \u2212\u01eb\n2T\n\u0011\n(Equation (2.1))\n\u2265\nX\nu\u2208{0,1}D\nPr[zi+1 = u]\n\u0012\nPMup(max(J(v, u))) \u2212(T \u2212i \u22121)\u01eb\n2T\n\u2212\u01eb\n2T\n\u0013\n(Induction hypothesis)\n=\nX\nu\u2208{0,1}D\nPr[zi+1 = u] PMup(max(J(v, u))) \u2212(T \u2212i)\u01eb\n2T\n= PMup(v) \u2212(T \u2212i)\u01eb\n2T\n(De\ufb01nition of Mup).\nThe claim now follows from the above equation and induction.\nLemma 2.7 now follows from Claims 2.8, 2.9 and similar arguments for Mdown.\n3\nMain Generator Construction\nWe now describe our main construction G that serves as a blueprint for all of our constructions.\nThe generator G is essentially a simpli\ufb01cation of the hitting set construction for halfspaces by\nRabani and Shpilka [RS09]. We use the following building blocks. Unless otherwise stated we shall\nassume without loss of generality that the parameters n, t are powers of 2.\n1. A family H = {h : [n] \u2192[t]} of hash functions that is \u03b1-pairwise independent. That is, for a\n\ufb01xed k \u2208[t] and i \u0338= j \u2208[n],\nPr\nh\u2208uH[h(i) = k \u2227h(j) = k] \u22641 + \u03b1\nt2\n.\n(3.1)\nE\ufb03cient constructions of size |H| = O(nt) are known for any constant \u03b1, even \u03b1 = 0 (see,\ne.g., [CW77]).\n2. A generator G0 : {0, 1}r0 \u2192{1, \u22121}m of a \u03b4-almost k-wise independent space over {1, \u22121}m.\nA distribution D over {1, \u22121}m is \u03b4-almost k-wise independent if, for all {i1, . . . , ik} \u2286[m]\nX\nb1,...,bk\u2208{1,\u22121}k\n\f\f\f\f Pr\nx\u2190D[xi1 = b1, . . . , xik = bk] \u22121\n2k\n\f\f\f\f \u2264\u03b4.\nE\ufb03cient generators G0 as above with seed length r0 = O(k + log m + log(1/\u03b4)) are known\n[NN93]. Without loss of generality we also assume that for random x output by G0, E[xi] = 0\nfor all i \u2264n.\nAlthough e\ufb03cient constructions of hash families H and generators G0 as above are known even\nfor \u03b1 = 0, \u03b4 = 0 and constant k, we work with small but non-zero \u03b1, \u03b4, as we will need the more\ngeneral objects for our analyses.\n7\nThe basic idea behind the generator is as follows. We \ufb01rst use the hash functions to distribute\nthe coordinates ([n]) into buckets. The purpose of this step is to spread out the \u201cin\ufb02uences\u201d of\nthe coordinates across buckets.\nThen, for each bucket we use an independently chosen sample\nfrom a \u03b4-almost k-wise independent distribution to generate the bits for the coordinate positions\nmapped to the bucket. The purpose of this step is, roughly, to \u201cmatch the \ufb01rst few moments\u201d\nof functions restricted to the coordinates in each bucket. The hope then is to subsequently use\ninvariance principles to show closeness in distribution.\nFix the error parameter \u01eb > 0 and let t at most poly(log(1/\u01eb))/\u01eb2 to be chosen later. Let m = n/t\n(assuming without loss of generality that t divides n) and let H be an \u03b1-pairwise independent hash\nfamily.\nTo avoid some technicalities that can be overcome easily, we assume that every hash\nfunction h \u2208H is evenly distributed, meaning \u2200h, i \u2208[t], |{j : h(j) = i, j \u2208[n]}| = n/t. Let\nG0 : {0, 1}r0 \u2192{1, \u22121}m generate a \u03b4-almost k-wise independent space for \u03b4 \u2265poly(\u01eb, 1/n) to be\nchosen later.\nDe\ufb01ne G : H \u00d7 ({0, 1}r0)t \u2192{0, 1}n by\nG(h, z1, . . . , zt) = x, where x|h\u22121(i) = G0(zi) for i \u2208[t].\n(3.2)\nWe will show that for the parameters t, \u03b1, \u03b4, k and H, G0 chosen appropriately, the above gener-\nator fools halfspaces as well as degree d PTFs. In particular, we fool progressively stronger classes,\nfrom halfspaces to degree d PTFs by choosing H and G0 progressively stronger. The table below\ngives a simpli\ufb01ed summary of the results we get for di\ufb00erent choices of H, G0. We de\ufb01ne balanced\nhash functions in De\ufb01nition 4.9.\nHash Family H\nGenerator G0\nFooling class\nPairwise independent\n4-wise independent\nRegular halfspaces, Theorem 4.3\nPairwise independent, Balanced\n\u0398(log t)-wise independent\nHalfspaces, Theorem 4.11\nPairwise independent\n4d-wise independent\nRegular degree d PTFs, Theorem 5.2\nPairwise independent, Balanced\n\u0398(t)-wise independent\nDegree d PTFs, Theorem 5.18.\n4\nPRGs for Halfspaces\nIn this section we show that for appropriately chosen parameters, G fools halfspaces. We \ufb01rst\nshow that G fools \u201cregular\u201d halfspaces to obtain a PRG with seed length O(log n/\u01eb2) for regular\nhalfspaces. We then extend the analysis to arbitrary halfspaces to get a PRG with seed length\nO(log n log2(1/\u01eb)/\u01eb2) and apply the monotone trick to prove Theorem 1.3.\nIn the following let Hw,\u03b8 : {1, \u22121}n \u2192{1, \u22121} denote a halfspace Hw,\u03b8(x) = sign(\u27e8w, x\u27e9\u2212\u03b8).\nUnless stated otherwise, we assume throughout that a halfspace Hw,\u03b8 is normalized, meaning \u2225w\u2225=\n1 (here \u2225\u00b7 \u2225is the l2-norm). We measure distance between real-valued distributions P, Q by\nd(P, Q) = \u2225CDF(P) \u2212CDF(Q)\u2225\u221e= sup\nt\u2208R\n| Pr\nx\u2190P[x < t] \u2212Pr\nx\u2190Q[x < t]|,\nalso known as Kolmogorov-Smirnov distance. In particular, we say two real-valued distributions\nP, Q are \u03b5-close if d(P, Q) \u2264\u03b5. We use the fact that Kolmogorov-Smirnov distance is convex.\nLemma 4.1. For \ufb01xed Q, the distance function d(P, Q) de\ufb01ned for probability distributions over\nR is a convex function.\nFor \u03c3 > 0, let N(0, \u03c3) denote the normal distribution with mean 0 and variance \u03c32. We also\nassume that \u01eb > 1/n.49 as otherwise, Theorem 1.3 follows from Corollary 1.6.\n8\n4.1\nPRGs for Regular Halfspaces\nAs was done in Diakonikolas et al. we \ufb01rst deal with regular halfspaces.\nDe\ufb01nition 4.2. A vector w \u2208Rn \u01eb-regular if |wi| \u2264\u01eb\u2225w\u2225for all i. A halfspace Hw,\u03b8 is \u01eb-regular\nif w is \u01eb-regular.\nLet t = 1/\u01eb2. We claim that for H pairwise independent and G0 generating an almost 4-wise\nindependent distribution, G fools regular halfspaces. Note that the randomness used by G in this\nsetting is O(log n/\u01eb2).\nTheorem 4.3. Let H be an \u03b1-almost pairwise independent family for \u03b1 = O(1) and let G0 generate\na \u03b4-almost 4-wise independent distribution for \u03b4 = \u01eb2/4n5.\nThen, G de\ufb01ned by Equation 3.2\nfools \u01eb-regular halfspaces with error at most O(\u01eb) and seed length O(log n/\u01eb2). In particular, for\nx \u2208{1, \u22121}n generated from G and \u01eb-regular w with \u2225w\u2225= 1, the distribution of \u27e8w, x\u27e9is O(\u01eb)-close\nto N(0, 1).\nTo prove the theorem we will need the Berry-Ess\u00b4een theorem, which gives a quantitative form\nof the central limit theorem and can be seen as an invariance principle for halfspaces.\nTheorem 4.4 (Theorem 1, XVI.5, [Fel71], [She07]). Let Y1, . . . , Yt be independent random variables\nwith E[Yi] = 0, P\ni E[Y 2\ni ] = \u03c32, P\ni E[|Yi|3] \u2264\u03c1. Let F(.) denote the cdf of the random variable\nSn = (Y1 + . . . Yn)/\u03c3, and \u03a6(.) denote the cdf of the normal distribution N(0, 1). Then,\n\u2225F \u2212\u03a6\u2225\u221e= sup\nz |F(z) \u2212\u03a6(z)| \u2264\u03c1\n\u03c33 .\nCorollary 4.5. Let Y1, . . . , Yt be independent random variables with E[Yi] = 0, P\ni E[Y 2\ni ] = \u03c32,\nP\ni E[|Yi|4] \u2264\u03c14. Let F(.) denote the cdf of the random variable Sn = (Y1 + . . . Yn)/\u03c3, and \u03a6(.)\ndenote the cdf of the normal distribution N(0, 1). Then,\n\u2225F \u2212\u03a6\u2225\u221e= sup\nz |F(z) \u2212\u03a6(z)| \u2264\n\u221a\u03c14\n\u03c32 .\nProof. For 1 \u2264i \u2264n, by Cauchy-Schwarz, E[|Yi|3] \u2264\nq\nE[Y 2\ni ] \u00b7\nq\nE[Y 4\ni ]. Therefore,\nX\ni\nE[|Yi|3] \u2264\nX\ni\nq\nE[Y 2\ni ] \u00b7\nq\nE[Y 4\ni ] \u2264\n X\ni\nE[Y 2\ni ]\n!1/2  X\ni\nE[Y 4\ni ]\n!1/2\n.\nThe claim now follows from Theorem 4.4.\nLemma 4.6. For \u01eb-regular w with \u2225w\u2225= 1 and x \u2208u {1, \u22121}n, the distribution of \u27e8w, x\u27e9is \u01eb-close\nto N(0, 1).\nProof. Let Yi = wixi. Then, P\ni E[Y 2\ni ] = 1 and P\ni E[Y 4\ni ] = P\ni w4\ni \u2264\u01eb2. The lemma now follows\nfrom Corollary 4.5.\nThe following lemma says that for a pairwise-independent family of hash functions H and\nw \u2208Rn, the weight of the coe\ufb03cients is almost equidistributed among the buckets.\nLemma 4.7. Let H be an \u03b1-almost pairwise independent family of hash functions from [n] to [t].\nFor \u01eb-regular w with \u2225w\u2225= 1, Pt\ni=1 E[\u2225wh\u22121(i)\u22254] \u2264(1 + \u03b1)\u01eb2 + 1+\u03b1\nt .\n9\nProof. Fix i \u2208[t]. For 1 \u2264j \u2264n, let Xj be the indicator variable that is 1 if h(j) = i and 0\notherwise. Then, E[\u2225wh\u22121(i)\u22252] = 1/t and\n\u2225wh\u22121(i)\u22254 =\n\uf8eb\n\uf8ed\nn\nX\nj=1\n(Xjwj)2\n\uf8f6\n\uf8f8\n2\n=\nn\nX\nj=1\nX4\nj w4\nj +\nX\nj\u0338=k\nX2\nj X2\nkw2\njw2\nk.\nNow, E[X4\nj ] \u2264(1 + \u03b1)/t and for j \u0338= k, E[X2\nj X2\nk] \u2264(1 + \u03b1)/t2. Thus, taking expectations of the\nabove equation,\nE[\u2225wh\u22121(i)\u22254] \u22641 + \u03b1\nt\nX\nj\nw4\nj + 1 + \u03b1\nt2\nX\nj\u0338=k\nw2\njw2\nk\n\u22641 + \u03b1\nt\n(max\ni\n|wi|2) + 1 + \u03b1\nt2\n\u2264(1 + \u03b1) \u01eb2\nt\n+ 1 + \u03b1\nt2\n.\nThe lemma follows by summing over all i \u2208[t].\nProof of Theorem 4.3. Fix a hash function h \u2208H. Let wi = w|h\u22121(i) for i \u2208[t]. Then,\n\u27e8w, G(h, z)\u27e9=\nt\nX\ni=1\n\u27e8wi, G0(zi)\u27e9.\nLet random variables Y h\ni \u2261Yi \u2261\u27e8wi, G0(zi)\u27e9and Y h = Y1 +. . . +Yt. Then, E[Yi] = 0 and since\nG0(zi) is \u03b4-almost 4-wise independent, | E[Y 2\ni ] \u2212\u2225wi\u22252| \u2264\u03b4n2. Further, for 1 \u2264i \u2264t,\nE\nx\u2208u{1,\u22121}m[ \u27e8wi, x\u27e94 ] =\nm\nX\nj=1\n(wi\nj)4 + 3\nX\np\u0338=q\u2208[m]\n(wi\np)2(wi\nq)2 \u22643\u2225wi\u22254.\nSince, the above equation depends only on the \ufb01rst four moments of random variable x and\nG0(Zi) is \u03b4-almost 4-wise independent, it follows that E[Y 4\ni ] \u22643\u2225wi\u22254 + \u03b4n4. Thus, P\ni E[Y 2\ni ] \u2265\n1 \u2212\u03b4n2t \u22651/2 and Pt\ni=1 E[Y 4\ni ] \u22643 Pt\ni=1 \u2225wi\u22254 + \u03b4n5. Let \u03c1h = P\ni \u2225wi\u22254. Then, by Corollary 4.5,\nsince \u03b4 \u2264\u01eb2/4n5, for a \ufb01xed h the distribution of Y h is (\u221a3\u03c1h + \u01eb)-close to N(0, 1).\nObserve that for random h, z the distribution of Y = \u27e8w, G(h, z)\u27e9is a convex-combination of\nthe distributions of Y h for h \u2208H. Thus, from Lemma 4.1, the distribution of Y is O(E[\u221a\u03c1h] + \u01eb)-\nclose to N(0, 1). Now, by Cauchy-Schwarz E[\u221a\u03c1h] \u2264\np\nE[\u03c1h]. Further, since w is \u01eb-regular and\nt = 1/\u01eb2, it follows from Lemma 4.7 that E[\u03c1h] = P\ni E[\u2225wi\u22254] = P\ni E[\u2225wh\u22121(i)\u22254] \u22642(1 + \u03b1)\u01eb2.\nThus, the distribution of Y is O(\u01eb)-close to N(0, 1). The theorem now follows from combining this\nwith Lemma 4.6.\n4.2\nPRGs for Arbitrary Halfspaces\nWe now study arbitrary halfspaces and show that the generator G fools arbitrary halfspaces if\nthe family of hash functions H and generator G0 satisfy certain stronger properties.\nWe use\nthe following structural result on halfspaces that follows from the results of Servedio [Ser06] and\nDiakonikolas et al. [DGJ+09].\nTheorem 4.8. Let Hw,\u03b8 be a halfspace with w1 \u2265. . . \u2265wn, P w2\ni = 1. There exists K = K(\u01eb) =\nO(log2(1/\u01eb)/\u01eb2) such that one of the following two conditions holds.\n10\n1. wK = (wK(\u01eb)+1, . . . , wn) is \u01eb-regular.\n2. Let w\u2032 = (w1, . . . , wK(\u01eb)) and let Hw\u2032,\u03b8(x) = sgn(PK\ni=1 wixi \u2212\u03b8). Then,\n| Pr\nx\u2190D[Hw,\u03b8(x) \u0338= Hw\u2032,\u03b8(x)]| \u22642\u01eb,\n(4.1)\nwhere D is any distribution satisfying the following conditions for x \u2190D.\n(a) The distribution of (x1, . . . , xK) is \u01eb-close to uniform.\n(b) With probability at least 1\u2212\u01eb over the choice of (x1, . . . , xK), the distribution of (xK+1, . . . , xn)\nconditioned on (x1, . . . , xK) is (1/n2)-almost pairwise independent.\nIn particular, for distributions D as above\n|\nE\nx\u2190D[Hw,\u03b8(x)] \u2212\nE\nx\u2190D[Hw\u2032,\u03b8(x)] | \u22642\u01eb.\n(4.2)\nServedio and Diakonikolas et al. show the above result when D is the uniform distribution.\nHowever, their arguments extend straightforwardly to any distribution D as above.\nGiven the above theorem, we use a case analysis to analyze G. If the \ufb01rst condition of the\ntheorem above holds, we use the results of the previous section, Theorem 4.3, showing that G fools\nregular halfspaces. If the second condition holds, we argue that for x distributed as the output of\nthe generator, the distribution of (x1, . . . , xK(\u01eb)) is O(\u01eb)-close to uniform.\nLet t = K(\u01eb). We need the family of hash functions H : [n] \u2192[t] in the construction of G to be\nbalanced along with being \u03b1-pairwise independent as in Equation (3.1). Intuitively, a hash family\nis balanced if with high probability the maximum size of a bucket is small.\nDe\ufb01nition 4.9 (Balanced Hash Functions). A family of hash functions H = {h : [n] \u2192[t] is\n(K, L, \u03b2)-balanced if for any S \u2286[n], |S| \u2264K,\nPr\nh\u2208uH[ max\nj\u2208[t] (|h\u22121(j) \u2229S|) \u2265L ] \u2264\u03b2.\n(4.3)\nWe use the following construction of balanced hash families due to Lovett et al. [LRTV09].\nTheorem 4.10 (See Lemma 2.12 in [LRTV09]). Let t = log(1/\u01eb)/\u01eb2 and K = K(\u01eb) as in\nTheorem 4.8. Then, there exists a (K, O(log(1/\u01eb)), 1/t2)-balanced hash family H : [n] \u2192[t] that is\nalso pairwise independent with |H| = exp(O(log n + log2(1/\u01eb))). Moreover, H is e\ufb03ciently sam-\nplable.\nLet m = n/t and \ufb01x L to be one of O(log t), O(log n). We also need the generator G0 : {0, 1}r0 \u2192\n{1, \u22121}m to be exactly 4-wise independent and \u03b4-almost (L + 4)-wise independent for \u03b4 = \u01eb3/tn5.\nGenerators G0 as above with r0 = O(log n + log(1/\u03b4) + L) = O(log(n/\u01eb)) are known [NN93].\nWe now show that with H, G0 as above, G fools halfspaces with error O(\u01eb). The randomness\nused by the generator is log |H| + r0t = O(log n log2(1/\u01eb)/\u01eb2) and matches the randomness used in\nthe results of Diakonikolas et al. [DGJ+09].\nTheorem 4.11. With H, G0 chosen as above, G de\ufb01ned by Equation (3.2) fools halfspaces with\nerror at most O(\u01eb) and seed length O(log n log2(1/\u01eb)/\u01eb2).\nProof. Let Hw,\u03b8 be a halfspace and without loss of generality suppose that w1 \u2265. . . \u2265wn and\nP\ni w2\ni = 1. Let S = {1, . . . , K(\u01eb)}. Call a hash function S-good if for all j \u2208[t], |Sj| = |S\u2229h\u22121(j)| \u2264\nL. From De\ufb01nition 4.9, a random hash function h \u2208u H is S-good with probability at least 1\u22121/t2.\nRecall that G(h, z1, . . . , zt) = x, where x|h\u22121(j) = G0(zj) for j \u2208[t]. Let D denote the distribution\nof the output of G and let x \u2190D.\n11\nClaim 4.12. Given an S-good hash function h, the distribution of x|S is \u01eb-close to uniform. More-\nover, with probability at least 1 \u2212\u01eb over the random choices of x|S, the distribution of x in the\ncoordinates not in S conditioned on x|S is (\u01eb2/4n5)-almost 4-wise independent.\nProof. Fix an S-good hash function h. Since z1, . . . , zt are chosen independently, given the hash\nfunction h, x|S1, . . . , x|St are independent of each other. Moreover, since the output of G0 is \u03b4-\nalmost (L+4)-wise independent and |Sj| \u2264L for all j \u2208[t], x|Sj is \u03b4-close to uniform for all j \u2208[t].\nIt follows that given an S-good hash function h, x|S is (t\u03b4)-close to uniform. Further, by a similar\nargument, for any set I \u2286[n] \\ S with |I| = 4, the distribution of x|(S\u222aI) is (t\u03b4)-close to uniform. It\nfollows that, with probability at least 1\u2212\u01eb, the distribution of x|I conditioned on x|S is (t\u03b4/\u01eb)-close\nto uniform. The claim now follows from the above observations and noting that t\u03b4 = \u01eb3/4n5.\nWe can now prove the theorem by a case analysis. Suppose that the weight vector w satis\ufb01es\ncondition (2) of Theorem 4.8. Observe that from the above claim, D satis\ufb01es the conditions of\nTheorem 4.8 (2). Let Hw|S,\u03b8(x) = sgn(\u27e8w|S, x|S\u27e9\u2212\u03b8). Then, from Equation (4.2),\n|\nE\nx\u2190Un[Hw,\u03b8(x)] \u2212\nE\nx\u2190Un[Hw|S,\u03b8(x)] | \u22642\u01eb,\n|\nE\nx\u2190D[Hw,\u03b8(x)] \u2212\nE\nx\u2190D[Hw|S,\u03b8(x)] | \u22642\u01eb.\nMoreover, since the distribution of x|S is \u01eb-close to uniform under D and Hw|S,\u03b8(x) only depends\non x|S,\n|\nE\nx\u2190Un[Hw|S,\u03b8(x)] \u2212\nE\nx\u2190D[Hw|S,\u03b8(x)]| \u2264\u01eb.\nCombining the above three equations, we get that\n|\nE\nx\u2190Un[Hw,\u03b8(x)] \u2212\nE\nx\u2190D[Hw,\u03b8(x)]| \u22645\u01eb,\nand thus G fools halfspace Hw,\u03b8 with error at most 5\u01eb.\nNow suppose that condition (1) of Theorem 4.8 holds and w \u00afS = (wK(\u01eb)+1, . . . , wn) is \u01eb-regular.\nFix an assignment to the variables x|S = u|S and let x \u00afS = (xk+1, . . . , xn) and Hu(xk+1, . . . , xn) =\nsgn(\u27e8w \u00afS, x \u00afS\u27e9\u2212\u03b8u), where \u03b8u = \u03b8 \u2212\u27e8w|S, x|S\u27e9. We will argue that with probability at least 1 \u2212\u01eb,\nconditioned on the values of x|S, the output of G fools the \u01eb-regular halfspace Hu with error\nO(\u01eb). Given the last statement it follows that D fools the halfspace Hw,\u03b8 with error O(\u01eb) since the\ndistribution of x|S under D is \u01eb-close to uniform.\nSince H is a family of pairwise independent hash functions and a random hash function h \u2208u H\nis S-good with probability at least 1 \u22121/t2, even when conditioned on being S-good, a random\nhash function h \u2208u H is \u03b1-pairwise independent for \u03b1 = 1. Further, from Claim 4.12, conditioned\non the hash function h being S-good, with probability at least 1 \u2212\u01eb, even conditioned on x|S, the\ndistribution of x|[n]\\S is (\u01eb2/4n5)-almost 4-wise independent. Thus, we can apply Theorem 4.31\nshowing that with probability at least 1 \u2212\u01eb, conditioned on the values of x|S, the output of G fools\nHu with error O(\u01eb).\n4.3\nDerandomizing G\nWe now derandomize the generator from the previous section and prove Theorem 1.3. The de-\nrandomization is motivated by the fact that for a \ufb01xed hash function h and w \u2208Rn, \u03b8 \u2208R,\nsgn( \u27e8w, G(h, z1, . . . , zt)\u27e9\u2212\u03b8 ) can be computed by a monotone ROBP with t layers. Given this\n1Though Theorem 4.3 was stated for t = 1/\u01eb2, the same argument works for all t \u22651/\u01eb2.\n12\nobservation, by Theorem 2.5, we can use PRGs for small-width ROBP to generate z1, . . . , zt instead\nof generating them independently as before.\nLet r0, t, m, H, G0 be set as in the context of Theorem 4.11. Let s0 = log(2t/\u01eb) = O(log(1/\u01eb))\nand let GBP : {0, 1}r \u2192({0, 1}s)t be a PRG fooling (s0, r0, t)-branching programs with error \u03b4.\nDe\ufb01ne GD : H \u00d7 {0, 1}r \u2192{1, \u22121}n by\nGD(h, y) = G(h, GBP (y)).\n(4.4)\nThe randomness used by the above generator is log |H|+r. We claim that GD fools halfspaces with\nerror at most O(\u01eb + \u03b4).\nTheorem 4.13. GD fools halfspaces with error O(\u01eb + \u03b4).\nProof. Fix a halfspace Hw,\u03b8 and without loss of generality (see [LC67] for instance) suppose that\nw1, . . . , wn, \u03b8 are integers. Let N = P\nj |wj| + |\u03b8|. Observe that for any x \u2208{1, \u22121}n, \u27e8w, x\u27e9\u2212\u03b8 \u2208\n{\u2212N, \u2212N +1, . . . , 0, . . . , N}. Fix a hash function h \u2208H. We de\ufb01ne a (log(2N +1), r0, t)-branching\nprogram Mh,w that for z = (z1, . . . , zt) \u2208({0, 1}r0)t computes \u27e8w, G(h, z)\u27e9.\nFor i \u2208[t], let wi = w|h\u22121(i). Then, for z = (z1, . . . , zt) \u2208({0, 1}r0)t, by de\ufb01nition of G in\nEquation 3.2,\n\u27e8w, G(h, z1, . . . , zt)\u27e9=\nt\nX\ni=1\n\u27e8wi, G0(zi)\u27e9.\nDe\ufb01ne a space-bounded machine Mh,w as follows. For each 0 \u2264i \u2264t, put N nodes in layer i\nwith labels 1, . . . , N. The vertices in layer i correspond to the partial sums Zi = Pi\nl=1\u27e8wl, G0(zl)\u27e9.\nNote that all partial sums Zi lie in {\u2212N, \u2212N + 1, . . . , N}. Now, given the partial sum Zi there\nare 2r0 possible values for Zi+1 ranging in {Zi + \u27e8wi+1, G0(z)\u27e9: z \u2208{0, 1}r0}. We add 2r0 edges\ncorrespondingly. Finally, label all vertices in the \ufb01nal layer corresponding to values less than \u03b8 as\nrejecting and label all other vertices as accepting states.\nIt follows from the de\ufb01nition of Mh,w that Mh,w is monotone and for z = (z1, . . . , zt) \u2208\n({0, 1}r0)t, Mh,w(z) is an accepting state if and only if sgn(P\ni\u27e8wi, G0(zi)\u27e9\u2212\u03b8) = Hw,\u03b8(G(h, z)) = 1.\nThus, from Theorem 2.5, for a \ufb01xed h \u2208H,\n|\nPr\nz\u2208u({0,1}r0)t [Hw,\u03b8(G(h, z)) = 1] \u2212\nPr\ny\u2208u{0,1}r [Hw,\u03b8(G(h, GBP (y))) = 1]| \u2264\u03b4 + \u01eb.\nThe theorem now follows from the above equation and Theorem 4.11.\nBy choosing the hash family H from Theorem 4.10 and using the PRG of Impagliazzo et al. we\nget our main result for fooling halfspaces.\nProof of Theorem 1.3. Choose GBP in the above theorem to be the PRG of Impagliazzo et al. [INW94].\nTo \u01eb-fool (S, D, T)-ROBPs, the generator of Impagliazzo et al., Theorem 2.3, has a seed-length of\nO(D + (S + log(1/\u01eb)) log T). Thus, the seed-length of GBP is r = O(r0 + (s0 + log(1/\u01eb)) log t) =\nO(log n + log2(1/\u01eb)). The theorem follows by choosing the hash family H as in Theorem 4.10.\n5\nPRGs for Polynomial Threshold Functions\nWe now extend our results from the previous sections to construct PRGs for degree d PTFs. We\nset the parameters of G as in Theorem 4.11, with the main di\ufb00erence being that we take G0 to\ngenerate a k-wise independent space for k = O(log2(1/\u01eb)/\u01ebO(d) + 4d) instead of O(log2(1/\u01eb)/\u01eb2) as\nwas done for fooling halfspaces. The analysis of the construction is, however, more complicated\nand proceeds as follows.\n13\n1. We \ufb01rst use the invariance principle of Mossel et al. [MOO05] to deal with regular PTFs.\n2. We then use the structural results on random restrictions of PTFs of Diakonikolas et al. [DSTW10]\nand Harsha et al. [HKM09] to reduce the case of fooling arbitrary PTFs to that of fooling\nregular PTFs and functions depending only on a few variables.\nWe carry out the \ufb01rst step above by an extension of the hybrid argument of Mossel et al. where\nwe replace blocks of variables instead of single variables as done by Mossel et al. For this part of the\nanalysis, we also need the anti-concentration results of Carbery and Wright [CW01] for low-degree\npolynomials over Gaussian distributions.\nThe second step relies on properties of random restrictions of PTFs similar in spirit to those\nin Theorem 4.8 for halfspaces. Roughly speaking, we use the following results. There exists a set\nS \u2286[n] of at most L = 1/\u01ebO(d) variables such that for a random restriction of these variables, with\nprobability at least \u2126(1) one of the following happens.\n1. The resulting PTF on the variables in [n]/S is \u01eb-regular.\n2. The resulting PTF on the variables in [n]/S has high bias.\nWe then \ufb01nish the analysis by recursively applying the above claim to show that a generator\nfooling regular PTFs and having bounded independence also fools arbitrary PTFs.\n5.1\nPRGs for Regular PTFs\nHere we extend our result for fooling regular halfspaces, Theorem 4.3, to regular PTFs.\nDe\ufb01nition 5.1. Let P(u1, . . . , un) = P\nI \u03b1I\nQ\ni\u2208I ui be a multi-linear polynomial of degree d. Let\n\u2225P\u22252\n2 = P\nI \u03b12\nI and the in\ufb02uence of i\u2019th coordinate \u03c4i(P) = P\nI\u220bi \u03b12\nI. We say P is \u01eb-regular if\nX\ni\n\u03c4i(P)2 \u2264\u01eb2\u2225P\u22252\n2.\nWe say a polynomial threshold function f(x) = sgn(P(x) \u2212\u03b8) is \u01eb-regular if P is \u01eb-regular.\nUnless stated otherwise, we will assume throughout that P is normalized with \u2225P\u22252\n2 = 1. Fix\nd > 0. Let t = 1/\u01eb2, m = n/t and let H be an \u03b1-pairwise independent family as in Theorem 4.3.\nWe assume G0 : {0, 1}r0 \u2192{1, \u22121}m generates a 4d-wise independent space, generalizing the\nassumption of 4-wise independence used for fooling regular halfspaces.\nTheorem 5.2. Let H be an \u03b1-pairwise independent family for \u03b1 = O(1) and let G0 generate a\n4d-wise independent distribution. Then, G de\ufb01ned by Equation (3.2) fools \u01eb-regular PTFs of degree\nat most d with error at most O(d\u01eb2/(4d+1)).\nWe \ufb01rst prove some useful lemmas. The \ufb01rst lemma is simple.\nLemma 5.3. For a multi-linear polynomial P of degree d with \u2225P\u2225= 1, P\nj \u03c4j(P) \u2264d.\nThe following lemma generalizes Lemma 4.7 and says that for pairwise independent hash func-\ntions and regular polynomials, the total in\ufb02uence is almost equidistributed among the buckets.\n14\nLemma 5.4. Let H = {h : [n] \u2192[t]} be a \u03b1-pairwise independent family of hash functions. Let P\nbe a multi-linear polynomial of degree d with coe\ufb03cients (\u03b1J)J\u2286[n] and \u2225P\u2225\u22641. For h \u2208H let\n\u03c4(h, i) =\nX\nJ\u2229h\u22121(i)\u0338=\u2205\n\u03b12\nJ.\nThen, for h \u2208u H\nE\nh\n\"\nt\nX\ni=1\n\u03c4(h, i)2\n#\n\u2264(1 + \u03b1)\nn\nX\nj=1\n\u03c4j(P)2 + (1 + \u03b1)d2\nt\n.\n(5.1)\nProof. Fix i \u2208[t] and for 1 \u2264j \u2264n, let Xj be the indicator variable that is 1 if h(j) = i and 0\notherwise. For brevity, let \u03c4j = \u03c4j(P) for j \u2208[n]. Now,\n\u03c4(h, i) =\nX\nJ\u2229h\u22121(i)\u0338=\u2205\n\u03b12\nJ =\nX\nJ\n\u03b12\nJ (\u2228j\u2208JXj)\n\u2264\nX\nJ\n\u03b12\nJ\n\uf8eb\n\uf8edX\nj\u2208J\nXj\n\uf8f6\n\uf8f8\n=\nX\nj\nXj\nX\nJ:J\u220bj\n\u03b12\nJ\n=\nX\nj\nXj\u03c4j.\nThus,\n\u03c4(h, i)2 \u2264\n\uf8eb\n\uf8ed\nn\nX\nj=1\nXj\u03c4j\n\uf8f6\n\uf8f8\n2\n=\nX\nj\nX2\nj \u03c4 2\nj +\nX\nj\u0338=k\nXjXk\u03c4j\u03c4k.\nNote that E[Xj] \u2264(1 + \u03b1)/t and for j \u0338= k, E[XjXk] \u2264(1 + \u03b1)/t2. Thus,\nE[ \u03c4(h, i)2 ] \u22641 + \u03b1\nt\nX\nj\n\u03c4 2\nj +\nX\nj\u0338=k\n\u03c4j\u03c4k\n1 + \u03b1\nt2\n\u22641 + \u03b1\nt\nX\nj\n\u03c4 2\nj + 1 + \u03b1\nt2\n(\nX\nj\n\u03c4j)2.\nThe lemma follows by using Lemma 5.3 and summing over all i \u2208[t].\nWe also use (2, 4)-hypercontractivity for degree d polynomials, the anti-concentration bounds for\npolynomials over log-concave distributions due to Carbery and Wright [CW01], and the invariance\nprinciple of Mossel et al [MOO05]. We state the relevant results below.\nLemma 5.5 ((2, 4)-hypercontractivity). If Q, R are degree d multilinear polynomials, then for\nX \u2208u {1, \u22121}n,\nE\nX [Q2 \u00b7 R2] \u22649d \u00b7 E\nX[Q2] \u00b7 E\nX[R2].\nIn particular, E[Q4] \u22649d \u00b7 E[Q2]2.\nThe following is a special case of Theorem 8 of Carbery-Wright [CW01] (in their notation, set\nq = 2d and the distribution \u00b5 to be N(0, 1)n).\n15\nTheorem 5.6 (Carbery-Wright). There exists an absolute constant C such that for any multi-linear\npolynomial P of degree at most d with \u2225P\u2225= 1 and any interval I \u2286R of length \u03b1 > 0,\nPr\nX\u2190N (0,1)n[P(X) \u2208I] \u2264Cd \u03b11/d.\nWe use the following structural result of Mossel et al. [MOO05] that reduces the problem of\nfooling threshold functions to that of fooling certain nice functions which are easier to analyze.\nDe\ufb01nition 5.7. A function \u03c8 : R \u2192R is B-nice, if \u03c8 is smooth and |\u03c8\n\u2032\u2032\u2032\u2032(t)| \u2264B for all t \u2208R.\nLemma 5.8 (Mossel et al.). Let X, Y be two real-valued random variables such that the following\nhold.\n1. For any interval I \u2286R of length at most \u03b1, Pr[ X \u2208I ] \u2264C\u03b11/d, where C is a constant\nindependent of \u03b1.\n2. For all 1-nice functions \u03c8, |E[\u03c8(X)] \u2212E[\u03c8(Y )]| \u2264\u01eb2.\nThen, for all t > 0, | Pr[X > t] \u2212Pr[Y > t] | \u22642C \u01eb2/(4d+1).\nThe following theorem is a restatement of the main result of Mossel et al. who obtain the bound\nO(d 9d maxi \u03c4i(P))) instead of the one below. However, their arguments extend straightforwardly\nto the following.\nTheorem 5.9 (Mossel et al.). Let P be a multi-linear polynomial of degree at most d with \u2225P\u2225= 1,\nX \u2190N(0, 1)n and Y \u2208u {1, \u22121}n. Then, for any 1-nice function \u03c8,\n| E[\u03c8(P(X))] \u2212E[\u03c8(P(Y ))] | \u22649d\n12\nX\ni\n\u03c4i(P)2.\nWe \ufb01rst prove Theorem 5.2, assuming the following lemma which says that the generator G\nfools nice functions of regular polynomials.\nLemma 5.10. Let P be an \u01eb-regular multi-linear polynomial of degree at most d with \u2225P\u2225= 1. Let\nY \u2208u {1, \u22121}n and Z be distributed as the output of G. Then, for any 1-nice function \u03c8,\n| E[\u03c8(P(Y ))] \u2212E[\u03c8(P(Z))] | \u22641 + \u03b1\n6\nd2 9d \u01eb2\nProof of Theorem 5.2. Let P be an \u01eb-regular polynomial of degree at most d and let X \u2190N(0, 1)n.\nLet X, Y, Z be real-valued random variables de\ufb01ned by X = P(X), Y = P(Y ) and Z = P(Z).\nThen, by Theorem 5.9 and Lemma 5.10, for any 1-nice function \u03c8,\n|E[\u03c8(X)] \u2212E[\u03c8(Y )]| \u22649d\n12\u01eb2,\n|E[\u03c8(Y )] \u2212E[\u03c8(Z)]| \u2264(1 + \u03b1) d2 9d \u01eb2\n6\n.\nHence,\n|E[\u03c8(X)] \u2212E[\u03c8(Z)]| = O(d2 9d \u01eb2).\nFurther, by Theorem 5.6, for any interval I \u2286R of length at most \u03b1, Pr[ X \u2208I ] = O( d \u03b11/d ).\nTherefore, we can apply, Lemma 5.8 to X, Y and X, Z to get\n| Pr[X > t] \u2212Pr[Y > t]| = O(d \u01eb2/(4d+1)),\n| Pr[X > t] \u2212Pr[Z > t]| = O(d \u01eb2/(4d+1)).\nThus,\n| Pr[Y > t] \u2212Pr[Z > t]| = O(d \u01eb2/(4d+1)).\n16\nProof of Lemma 5.10. Fix a hash function h \u2208H. Let Z1, . . . , Zt be t independent samples gener-\nated from the 4d-wise independent space. Let Y1, . . . , Yt be t independent samples chosen uniformly\nfrom {1, \u22121}m. We will prove the claim via a hybrid argument where we replace the blocks Y1, . . . , Yt\nwith Z1, . . . , Zt progressively.\nFor 0 \u2264i \u2264t, let Xi be the distribution with Xi\n|h\u22121(j) = Zj for 1 \u2264j \u2264i and Xi\n|h\u22121(j) = Yj for\ni < j \u2264t. Then, for a \ufb01xed hash function h, X0 is uniformly distributed over {1, \u22121}n and Xt is\ndistributed as the output of the generator. For i \u2208[t], let \u03c4(h, i) be the in\ufb02uence of the i\u2019th bucket\nunder h,\n\u03c4(h, i) =\nX\nJ\u2229h\u22121(i)\u0338=\u2205\n\u03b12\nJ.\nClaim 5.11. For 1 \u2264i \u2264t,\n| E[\u03c8(P(Xi))] \u2212E[\u03c8(P(Xi\u22121))]| \u22649d\n12 \u03c4(h, i)2.\nWe will use the following form of the classical Taylor series.\nFact 5.12. For any 1-nice function \u03c8 : R \u2192R, \u03b1, \u03b2 \u2208R\n\u03c8(\u03b1 + \u03b2) = \u03c8(\u03b1) + \u03c8\u2032(\u03b1)\u03b2 + \u03c8\u2032\u2032(\u03b1)\n2\n\u03b22 + \u03c8\u2032\u2032\u2032(\u03b1)\n6\n\u03b23 + err(\u03b1, \u03b2),\nwhere |err(\u03b1, \u03b2)| \u2264\u03b24/24.\nProof. Let I = h\u22121(i) be the variables that have been changed from Xi\u22121 to Xi. Without loss of\ngenerality suppose that I = {1, . . . , m}. Let\nP(u1, . . . , un) = R(um+1, . . . , un) +\nX\nJ:J\u2229[m]\u0338=\u2205\n\u03b1J\n\uf8eb\n\uf8edY\nj\u2208J\nuj\n\uf8f6\n\uf8f8,\nwhere R( ) is a multi-linear polynomial of degree at most d. Let S(u1, . . . , um, um+1, . . . , un) denote\nthe degree d multi-linear polynomial given by the second term in the above expression.\nObserve that Xi\u22121, Xi agree on coordinates not in [m]. Let Xi = (Z1, . . . , Zm, Xm+1, . . . , Xn) =\n(Z, X) and Xi\u22121 = (Y1, . . . , Ym, Xm+1, . . . , Xn) = (Y, X). Then,\nP(Xi) = R(X) + S(Z, X),\nP(Xi\u22121) = R(X) + S(Y, X).\nNow, by using the Taylor series expansion, Fact 5.12, for \u03c8 at R(X),\nE[\u03c8(P(Xi))] \u2212E[\u03c8(P(Xi\u22121))] = E[\u03c8(R + S(Z, X))] \u2212E[\u03c8(R + S(Y, X))]\n= E[ \u03c8(R) + \u03c8\n\u2032(R)S(Z, X) + \u03c8\n\u2032\u2032(R)\n2\nS(Z, X)2 + \u03c8\n\u2032\u2032\u2032(R)\n6\nS(Z, X)3 \u00b1 {\u22641\n24S(Z, X)4} ]\u2212\nE[ \u03c8(R) + \u03c8\n\u2032(R)S(Y, X) + \u03c8\n\u2032\u2032(R)\n2\nS(Y, X)2 + \u03c8\n\u2032\u2032\u2032(R)\n6\nS(Y, X)3 \u00b1 {\u22641\n24S(Y, X)4} ]\nObserve that X, Y, Z are independent of one another and are 4d-wise independent individually.\nSince S( ) has degree at most d, it follows that for a \ufb01xed assignment of the variables Xm+1, . . . , Xn\nin X,\nE[S(Z, X)] = E[S(Y, X)], E[S(Z, X)2] = E[S(Y, X)2],\n17\nE[S(Z, X)3] = E[S(Y, X)3], E[S(Z, X)4] = E[S(Y, X)4].\nCombining the above equations we get\n| E[\u03c8(P(Xi))] \u2212E[\u03c8(P(Xi\u22121))]| \u22641\n12 E[ S(Y, X)4 ].\n(5.2)\nNow, using the fact that S( ) is a multi-linear polynomial of degree at most d and since (Y, X) is\n4d-wise independent, E[ S(Y, X)4 ] = E[ S(W)4 ], where W is uniformly distributed over {1, \u22121}n.\nAlso note that\nE[S(W)2] = E\n\uf8ee\n\uf8f0\n\uf8eb\n\uf8ed\nX\nJ:J\u2229[m]\u0338=\u2205\n\u03b1J\n\uf8eb\n\uf8edY\nj\u2208J\nWj\n\uf8f6\n\uf8f8\n\uf8f6\n\uf8f8\n2 \uf8f9\n\uf8fb\n=\nX\nJ:J\u2229I\u0338=\u2205\n\u03b12\nJ\n= \u03c4(h, i).\nTherefore, using the (2, 4)-hypercontractivity inequality, Lemma 5.5, E[S(W)4] \u22649d E[S(W)2]2\nand Equation (5.2),\n| E[\u03c8(P(Xi))] \u2212E[\u03c8(P(Xi\u22121))]| \u22641\n12 E[ S(Y, X)4 ] = 1\n12 E[ S(W)4 ]\n\u22649d\n12 E[S(W)2]2 = 9d\n12 \u03c4(h, i)2.\nProof of Lemma 5.10 Continued. From Claim 5.11, for a \ufb01xed hash function h we have\n| E[\u03c8(P(Y ))] \u2212E[\u03c8(P(Z))]| \u2264\nt\nX\ni=1\n| E[\u03c8(P(Xi))] \u2212E[\u03c8(P(Xi\u22121))]| \u22649d\n12\nt\nX\ni=1\n\u03c4(h, i)2.\nTherefore, for h \u2208u H, using Lemma 5.4 and t = 1/\u01eb2,\n| E[\u03c8(P(Y ))] \u2212E[\u03c8(P(Z))]| \u22649d\n12 E\nh\n\"X\ni\n\u03c4(h, i)2\n#\n= 9d\n12 (1 + \u03b1)(1 + d2)\u01eb2 \u2264(1 + \u03b1) d2 9d \u01eb2\n6\n.\n5.2\nRandom Restrictions of PTFs\nWe use the following results on random restrictions of Diakonikolas et al. [DSTW10] and Harsha et\nal. [HKM09]. We mainly use the exact statements from the work of Harsha et al., as the notion of\nregular polynomials from Diakonikolas et al. is slightly di\ufb00erent from ours. Speci\ufb01cally, Diakonikolas\net al. de\ufb01ne regularity of a polynomial P by bounding maxi(\u03c4i(P)), but in our analysis we use the\nbound of P\ni \u03c4i(P)2. Diakonikolas et al. have a statement similar to Lemma 5.17 below; however,\nwe give a simple argument starting from the main lemmas of Harsha et al. for completeness.\nFix a polynomial P of degree at most d and suppose that \u03c41(P) \u2265\u03c42(P) . . . \u2265\u03c4n(P). Let\nK(P, \u01eb) = K be the least index i such that,\n\u03c4i+1(P) \u2264\u01eb2 X\nl>i\n\u03c4l(P).\n18\nLemma 5.13 (Lemma 5.1 in Harsha et al. [HKM09]). The polynomial PxK(Yk+1, . . . , Yn) =\nP(x1, . . . , xK, YK+1, . . . , Yn) in variables YK+1, . . . , Yn obtained by choosing x1, . . . , xK \u2208u {1, \u22121}\nis cd\u01eb-regular with probability at least \u03b3d, for some universal constants cd, \u03b3d > 0.\nLemma 5.14 (Lemma 5.2 in Harsha et al. [HKM09]). There exist universal constants c, cd, \u03b4d > 0\nsuch that for K(P, \u01eb) \u2265c log(1/\u01eb)/\u01eb2 = L, the following holds for all \u03b8 \u2208R. For a random partial\nassignment (x1, . . . , xL) \u2208u {1, \u22121}L with probability at least \u03b4d the following happens. There exists\nb \u2208{1, \u22121} such that\nPr\n(YL+1,...,Yn)\u2190D [ sign(P(x1, x2, . . . , xL, YL+1, . . . , Yn) \u2212\u03b8) \u0338= b ] \u2264cd\u01eb,\n(5.3)\nfor any 2d-wise independent distribution D over {1, \u22121}n\u2212L.\nThe above lemma is proven by Harsha et al. when D is the uniform distribution over {1, \u22121}n\u2212L.\nHowever, their argument extends straightforwardly to 2d-wise independent distributions D.\nBy repeatedly applying the above lemmas, we show that arbitrary low-degree PTFs can be\napproximated by small depth decision trees in which the leaf nodes either compute a regular PTF\nor a function with high bias. We \ufb01rst introduce some notation to this end.\nDe\ufb01nition 5.15. A block decision tree T with block-size L is a decision tree with the following\nproperties. Each internal node of the decision tree reads at most L variables. For each leaf node\n\u03c1 \u2208T, the output upon reaching the leaf node \u03c1 is a function f\u03c1 : {1, \u22121}V\u03c1 \u2192{1, \u22121}, where V\u03c1\nis the set of variables not occurring on the path to the node \u03c1. The depth of T is the length of the\nlongest path from the root of T to a leaf in T.\nDe\ufb01nition 5.16. Given a block decision tree T computing a function f, we say that a leaf node\n\u03c1 \u2208T is (\u01eb, d)-good if the function f\u03c1 satis\ufb01es one of the following two properties.\n1. There exists b \u2208{1, \u22121}, such that for any 2d-wise independent distribution D over {1, \u22121}V\u03c1,\nPr\nY \u2190D[f\u03c1(Y ) \u0338= b] \u2264\u01eb.\n2. f\u03c1 is a \u01eb-regular degree d PTF.\nWe now show a lemma on writing low-degree PTFs as a \u201cdecision tree of regular PTFs\u201d.\nLemma 5.17. There exist universal constants c\u2032\nd, c\u2032\u2032\nd such that the following holds for any degree\nd polynomial P and PTF f = sign(P( ) \u2212\u03b8). There exists a block decision tree T computing f of\nblock-size L = c\u2032\nd log(1/\u01eb)/\u01eb2 and depth at most c\u2032\u2032\nd log(1/\u01eb), such that with probability at least 1 \u2212\u01eb\na uniformly random walk on the tree leads to an (\u01eb, d)-good leaf node.\nProof. The proof is by recursively applying Lemmas 5.13 and 5.14. Let c, cd, \u03b3d, \u03b4d be constants\nfrom the above lemmas. Let L be de\ufb01ned as in Lemma 5.14 and let \u03b1 = min(\u03b3d, \u03b4d). For S \u2286\n[n] and a partial assignment y \u2208{1, \u22121}S, let Py : {1, \u22121}[n]/S \u2192R be the degree at most d\npolynomial de\ufb01ned by Py(Y ) = P(Z), where Zi = yi for i \u2208S and Zi = Yi for i /\u2208S.\nLet\nL(y) = min(K(Py, \u01eb), L) and let I(y) be the L(y) largest in\ufb02uence coordinates in the polynomial\nPy. We now de\ufb01ne a block-decision tree computing f inductively.\nLet y0 = \u2205and let I0 = I(y0). The root of the decision tree reads the variables in I0. For\n0 \u2264q \u2264log1/(1\u2212\u03b1)(1/\u01eb) suppose that after q steps we are at a node \u03b2 having read the variables in\nS(\u03b2) \u2286[n] and a corresponding partial assignment y. Then, if Py is cd\u01eb-regular or if Py satis\ufb01es\nEquation (5.3) we stop. Else, we make another step and read the values of variables in I(y).\n19\nFor any leaf node \u03c1, let y(\u03c1) denote the partial assignment that leads to \u03c1. Then the leaf node\n\u03c1 outputs the function f\u03c1(Y ) = sign(Py(\u03c1)(Y ) \u2212\u03b8).\nIt follows from the construction that T is a block-decision tree computing f with block-size L\nand depth at most log1/(1\u2212\u03b1)(1/\u01eb). Further, for any internal node \u03b2 \u2208T, by Lemmas 5.13, 5.14\nat least \u03b1 fraction of its children are (cd\u01eb, d)-good. Since any leaf node that is not (cd\u01eb, d)-good is\nat least log1/(1\u2212\u03b1)(1/\u01eb) far away from the root of T, it follows that a uniformly random walk on T\nleads to a (cd\u01eb, d)-good node with probability at least 1 \u2212\u01eb. The lemma now follows.\n5.3\nPRGs for Arbitrary PTFs\nWe now study the case of arbitrary degree d PTFs. As was done for halfspaces, we will show\nthat the generator G of Equation (3.2) fools arbitrary PTFs if the family of hash functions H and\ngenerator G0 satisfy stronger properties.\nLet t = cdc\u2032\nd log2(1/\u01eb)/\u01eb2, m = n/t, where cd, c\u2032\nd are the constants from Lemma 5.17. We use a\nfamily of hash functions H : [n] \u2192[t] that are \u03b1-pairwise independent for \u03b1 = O(1). We choose the\ngenerator G0 : {0, 1}r0 \u2192{1, \u22121}m to generate a (t + 4d)-wise independent space. Generators G0\nwith r0 = O(t log n) are known. We claim that with the above setting of parameter the generator\nG fools all degree d PTFs.\nTheorem 5.18. With H, G0 chosen as above, G de\ufb01ned by Equation (3.2) fools degree d PTFs\nwith error at most O(\u01eb2/(4d+1)) and seed length Od(log n log4(1/\u01eb)/\u01eb4).\nThe bound on the seed length of the generator follows directly from the parameter settings. By\ncarefully tracing the constants involved in our calculations and those in the results of Harsha et al.\nwe need, the exact seed length can be shown to be ad log n log4(1/\u01eb)/\u01eb4 for a universal constant a.\nFix a polynomial P of degree d and a PTF f(x) = sign(P(x) \u2212\u03b8) and let T denote the block-\ndecision tree computing f as given by Lemma 5.17. Let DP TF denote the output distribution of\nthe generator G with parameters set as above. The intuition behind the proof of the theorem is as\nfollows.\n1. As DP TF has su\ufb03cient bounded independence, the distribution on the leaf nodes of T obtained\nby taking a walk on T according to inputs chosen from DP TF is the same as the case when\ninputs are chosen uniformly. In particular, a random walk on T according to DP TF leads to\na (\u01eb, d)-good leaf node with high probability.\n2. As G fools regular PTFs by Theorem 5.2, DP TF will fool the function f\u03c1 computed at a\n(\u01eb, d)-good leaf node. We also need to address the subtle issue that we really need DP TF to\nfool a regular PTF f\u03c1 even when conditioned on reaching a particular leaf node \u03c1.\nWe \ufb01rst set up some notation. For a leaf node \u03c1 \u2208T, let U\u03c1 = [n] \\ V\u03c1 be the set of variables\nseen on the path to \u03c1 and let a\u03c1 be the corresponding assignment of variables in U\u03c1 that lead to \u03c1.\nFurther, given an assignment x, let Leaf(x) denote the leaf node reached by taking a walk according\nto x on T.\nLemma 5.19. For any leaf node \u03c1 of T,\nPr\nx\u2190DP T F[Leaf(x) = \u03c1] =\nPr\nx\u2208u{1,\u22121}n[Leaf(x) = \u03c1].\n20\nProof. Observe that DP TF is a t-wise independent distribution and that for any \u03c1, |U\u03c1| \u2264cdc\u2032\nd log2(1/\u01eb)/\u01eb2 =\nt. Thus,\nPr\nx\u2190DP T F[Leaf(x) = \u03c1] =\nPr\nx\u2190DP T F[x|U\u03c1 = a\u03c1] =\n1\n2|U\u03c1|\n=\nPr\nx\u2208u{1,\u22121}n[x|U\u03c1 = a\u03c1] =\nPr\nx\u2208u{1,\u22121}n[Leaf(x) = \u03c1].\nLemma 5.20. Fix an (\u01eb, d)-good leaf node \u03c1 of T. Then,\n|\nPr\nx\u2190DP T F[f\u03c1(x|V\u03c1) = 1 | x|U\u03c1 = a\u03c1] \u2212\nPr\ny\u2190{1,\u22121}V\u03c1[f\u03c1(y) = 1]| = O(\u01eb2/(4d+1)).\nProof. We consider two cases depending on which of the two conditions of De\ufb01nition 5.16 f\u03c1 satis\ufb01es.\nCase (1) - f\u03c1 has high bias. Note that DP TF is a (t + 4d)-wise independent distribution. Since\n|U\u03c1| \u2264t, it follows that for x \u2190DP TF, even conditioned on x|U\u03c1 = a\u03c1, the distribution is 2d-wise\nindependent. The lemma then follows from the fact that for some b \u2208{1, \u22121}, f\u03c1 evaluates to b\nwith high probability.\nCase (2) - f\u03c1 is an \u01eb-regular degree d PTF. We deal with this case by using Theorem 5.2. Let\nx = G(h, z1, . . . , zt) for h \u2208u H, z1, . . . , zt \u2208u {0, 1}r0, so x \u2190DP TF as in the de\ufb01nition of G. Let\nh\u03c1 : V\u03c1 \u2192[t] be the restriction of a hash function h to indices in V\u03c1. For brevity, let x(\u03c1) = x|V\u03c1 and\nlet E\u03c1 be the event x|U\u03c1 = a\u03c1. We show that the distribution of x(\u03c1), conditioned on E\u03c1, satis\ufb01es\nthe conditions of Theorem 5.2.\nObserve that conditioning on E\u03c1 does not change the distribution of the hash function h \u2208u H\nbecause |U\u03c1| \u2264t and DP TF is t-wise independent. Thus, even when conditioned on E\u03c1, the hash\nfunctions h\u03c1 are almost pairwise independent. For a hash function h, i \u2208[t], let B\u03c1(h, i) = h\u22121(i) \\\nV\u03c1 = h\u22121\n\u03c1 (i). Now, since G0 generates a (t + 4d)-wise independent distribution, even conditioned\non E\u03c1, for a \ufb01xed hash function h, the random variables x(\u03c1)|B\u03c1(h,1), x(\u03c1)|B\u03c1(h,2), . . . , x(\u03c1)|B\u03c1(h,t)\nare independent of one another. Moreover, each x(\u03c1)|B\u03c1(h,i) is 4d-wise independent for i \u2208[t].\nThus, even conditioned on E\u03c1, the distribution of x(\u03c1) satis\ufb01es the conditions of Theorem 5.2\nand hence fools the regular degree d PTF f\u03c1 with error at most O(\u01eb2/(4d+1)). The lemma now\nfollows.\nProof of Theorem 5.18. Observe that\nPr\nx\u2190{1,\u22121}n[f(x) = 1] =\nX\n\u03c1\u2208Leaves(T)\nPr\nx\u2208u{1,\u22121}n[x|U\u03c1 = a\u03c1] \u00b7\nPr\ny\u2190{1,\u22121}V\u03c1[f\u03c1(y) = 1].\nSimilarly,\nPr\nx\u2190DP T F[f(x) = 1] =\nX\n\u03c1\u2208Leaves(T)\nPr\nx\u2190DP T F[x|U\u03c1 = a\u03c1] \u00b7\nPr\nx\u2190DP T F[f\u03c1(x|V\u03c1) = 1 | x|U\u03c1 = a\u03c1].\nFrom the above equations and Lemma 5.19 it follows that\n|\nPr\nx\u2190{1,\u22121}n[f(x) = 1] \u2212\nPr\nx\u2190DP T F[f(x) = 1]| \u2264\nX\n\u03c1\u2208Leaves(T)\nPr\nx\u2190DP T F[x|U\u03c1 = a\u03c1] \u00b7\n\f\f\f\f\f\nPr\nx\u2190DP T F[f\u03c1(x|V\u03c1) = 1 | x|U\u03c1 = a\u03c1] \u2212\nPr\ny\u2190{1,\u22121}V\u03c1[f\u03c1(y) = 1]\n\f\f\f\f\f .\n21\nNow, by Lemma 5.20 for any (\u01eb, d)-good leaf \u03c1 the corresponding term on the right hand side of\nthe above equation is O(\u01eb2/(4d+1)). Further, from Lemma 5.17 we know that a random walk ends\nat a good leaf with probability at least 1 \u2212\u01eb. It follows that\n|\nPr\nx\u2190{1,\u22121}n[f(x) = 1] \u2212\nPr\nx\u2190DP T F[f(x) = 1]| \u2264\u01eb t = O(\u01eb2/(4d+1)).\nOur main theorem on fooling degree d PTFs, Theorem 1.2, follows immediately from the above\ntheorem.\n6\nPRGs for Spherical Caps\nWe now show how to extend the generator for fooling regular halfspaces and its analysis from\nSection 4.1 to get a PRG for spherical caps and prove Theorem 1.4.\nLet \u00b5 be a discrete distribution (if not, let\u2019s suppose we can discretize \u00b5) over a set U \u2286R.\nAlso, suppose that for X \u2190\u00b5, E[X] = 0, E[X2] = 1, E[|X|3] = O(1). Given such a distribution\n\u00b5, a natural approach for extending G to \u00b5 is to replace the k-wise independent space generator\nG0 : {0, 1}r \u2192{1, \u22121}m from Equation (3.2) with a generator G\u00b5 : {0, 1}r \u2192U m that generates a\nk-wise independent space over U m. It follows from the analysis of Section 4.1 that for G\u00b5 chosen\nwith appropriate parameters, the above generator fools regular halfspaces over \u00b5n. It then remains\nto fool non-regular halfspaces over \u00b5n. It is reasonable to expect that an analysis similar to that\nin Section 4.2 can be applied to \u00b5n, provided we have analogues of the results of Servedio and\nDiakonikolas et al., Theorem 4.8, for \u00b5n.\nThe above ideas can be used to get a PRG for spherical caps by noting that a) the uniform dis-\ntribution over the sphere is close to a product of Gaussians (when the test functions are halfspaces)\nand b) analogues of Theorem 4.8 for product of Gaussians follow from known anti-concentration\nproperties of the univariate Gaussian distribution. Building on the above argument, Gopalan et\nal. [GOWZ10] recently obtained PRGs fooling halfspaces over \u201creasonable\u201d product distributions.\nHere we take a di\ufb00erent approach and give a simpler, more direct construction for spherical caps\nbased on an idea of Ailon and Chazelle [AC06] and the invariance of spherical caps with respect to\nunitary rotations.\nLet Sn\u22121 = {x \u2208Rn : \u2225x\u22252 = 1} denote the n-dimensional sphere. By a spherical cap Sw,\u03b8 we\nmean the section of Sn\u22121 cut by a halfspace, i.e., Sw,\u03b8\ndef\n= {x : x \u2208Sn\u22121, Hw,\u03b8(x) = 1}.\nDe\ufb01nition 6.1. A function G : {0, 1}r \u2192Sn\u22121 is said to \u01eb-fool spherical caps if, for all spherical\ncaps Sw,\u03b8,\n|\nPr\nx\u2208uSn\u22121[x \u2208Sw,\u03b8] \u2212\nPr\ny\u2208u{0,1}r[G(y) \u2208Sw,\u03b8]| \u2264\u01eb.\nNote that the uniform distribution over Sn\u22121, Usp, is not a product distribution. We \ufb01rst show\nthat Usp is close to N(0, 1/\u221an)n when the test functions are halfspaces.\nLemma 6.2. There exists a universal constant C such that for any halfspace Hw,\u03b8,\n|\nPr\nx\u2190Usp[Hw,\u03b8(x) = 1] \u2212\nPr\nx\u2190N (0,1/\u221an)n[Hw,\u03b8(x) = 1]| \u2264C log n\nn1/4\n.\nIn particular, for x \u2190Usp, the distribution of \u27e8w, x\u27e9is O(\u221alog n/n1/4)-close to N(0, 1/\u221an).\n22\nProof. Observe that for x \u2190N(0, 1/\u221an)n, x/\u2225x\u22252 is distributed uniformly over Sn\u22121. Thus,\nPr\nx\u2208uSn\u22121[Hw,\u03b8(x) = 1] =\nPr\nx\u2190N (0,1/\u221an)n[Hw,\u03b8\n\u0012\nx\n\u2225x\u22252\n\u0013\n= 1].\nNow, for any x \u2208Rn,\n\f\f\f\f\u27e8w, x\u27e9\u2212\u27e8w, x\u27e9\n\u2225x\u22252\n\f\f\f\f = |\u27e8w, x\u27e9|\n\u2225x\u22252\n\u00b7 |\u2225x\u22252 \u22121|.\nSince for x \u2190N(0, 1/\u221an), \u27e8w, x\u27e9is distributed as N(0, 1/\u221an), for some constant c1,\nPr\nx\u2190N (0,1/\u221an)n\n\u0014\n|\u27e8w, x\u27e9| \u2265c1\n\u221alog n\nn1/2\n\u0015\n\u22641\nn.\nFurther, by well-known concentration bounds for the norm of a random Gaussian vector (see [LT91],\nfor instance), it follows that for some constant c2 > 0,\nPr\nx\u2190N (0,1/\u221an)n\n\u0014\n|\u2225x\u22252 \u22121| \u2265c2\n\u221alog n\nn1/4\n\u0015\n\u22641\nn,\nCombining the above equations we get\nPr\nx\u2190N (0,1/\u221an)n\n\u0014 \f\f\f\f\u27e8w, x\u27e9\u2212\u27e8w, x\u27e9\n\u2225x\u22252\n\f\f\f\f \u2265c1c2 log n\nn3/4\n\u0015\n\u22642\nn.\nTherefore, for C = c1c2,\nPr\nx\u2190N (0,1/\u221an)n\n\u0014\nHw,\u03b8\n\u0012\nx\n\u2225x\u22252\n\u0013\n\u0338= Hw,\u03b8(x)\n\u0015\n\u2264\nPr\nx\u2190N (0,1/\u221an)n\n\u0014\n|\u27e8w, x\u27e9\u2212\u03b8| \u2264\n\f\f\f\f\u27e8w, x\u27e9\u2212\u27e8w, x\u27e9\n\u2225x\u22252\n\f\f\f\f\n\u0015\n\u2264\nPr\nx\u2190N (0,1/\u221an)n\n\u0014\n|\u27e8w, x\u27e9\u2212\u03b8| \u2264c1c2 log n\nn3/4\n\u0015\n+ 2\nn\n\u2264C log n\nn1/4 ,\nwhere the last inequality follows from the fact that \u27e8w, x\u27e9is distributed as N(0, 1/\u221an) and for any\ninterval I \u2286R, Prx\u2190N (0,1)[x \u2208I] = O(|I|).\nNow, by Theorem 4.3, for \u01eb-regular w and x generated from G with parameters as in Theo-\nrem 4.3, the distribution of \u27e8w, x/\u221an\u27e9is O(\u01eb)-close to N(0, 1/\u221an). It then follows from the above\nlemma that G \u01eb-fools spherical caps Sw,\u03b8 when w is \u01eb-regular and \u01eb \u2265C log n/n1/4. We now reduce\nthe case of arbitrary spherical caps to regular spherical caps.\nObserve that the volume of a spherical cap Sw,\u03b8 is invariant under rotations: for any unitary\nmatrix A \u2208Rn\u00d7n with AT A = In,\nPr\nx\u2190Usp[x \u2208Sw,\u03b8] =\nPr\nx\u2190Usp[Ax \u2208Sw,\u03b8].\nWe exploit this fact by using a family of rotations R of Ailon and Chazelle [AC06] which\nsatis\ufb01es the property that for any w \u2208Rn and a random rotation V \u2208u R, V w is regular with high\nprobability. Let H \u2208Rn\u00d7n be the normalized Hadamard matrix such that HT H = In and each\nentry Hij \u2208{\u00b11/\u221an}. For a vector x \u2208Rn, let D(x) denote the diagonal matrix with diagonal\nentries given by x. Observe that for x \u2208{1, \u22121}n, HD(x) is a unitary matrix. Ailon and Chazelle\n(essentially) show that for any w \u2208Rn and x \u2208u {1, \u22121}n, HD(x)w is O(\u221alog n/\u221an)-regular. We\nderandomize their construction by showing that similar guarantees hold for x chosen from a 8-wise\nindependent distribution.\n23\nLemma 6.3. For all w \u2208Rn, \u2225w\u2225= 1, and x \u2208{1, \u22121}n chosen from an 8-wise independent\ndistribution the following holds. For v = HD(x)w, \u03b3 > 0,\nPr[\nX\ni\nv4\ni \u2265\u03b3\nn ] = O\n\u0012 1\n\u03b32\n\u0013\n.\nProof. Let random variable Z = P\ni v4\ni . Observe that each vi is a linear function of x and\nE[v2\ni ] = E[ (\nX\nj\nHijxjwj )2 ] =\nX\nj\nH2\nijw2\nj = 1\nn.\nNote that since x is 8-wise independent, we can apply (2, 4)-hypercontractivity, Lemma 5.5, to vi.\nThus,\nE[Z] =\nX\ni\nE[v4\ni ] \u22649\nX\ni\nE[v2\ni ]2 \u22649\nn.\nSimilarly, by (2, 4)-hypercontractivity applied to the quadratics v2\ni , v2\nj ,\nE[Z2] =\nX\ni,j\nE[v4\ni v4\nj ] \u2264\nX\ni,j\n92 E[v4\ni ] E[v4\nj ] \u226492 E[Z]2 \u226494\nn2 .\nThe lemma now follows from the above equation and Markov\u2019s inequality applied to Z2.\nCombining the above lemmas we get the following analogue of Theorem 4.3 for spherical caps.\nLet G be as in Theorem 4.3 and let D be a 8-wise independent distribution over {1, \u22121}n. De\ufb01ne\nGsph : {1, \u22121}n \u00d7 {0, 1}r \u2192Sn\u22121 by\nGsph(x, y) = D(x)HT G(y)\n\u221an\n.\nTheorem 6.4. For any spherical cap Sw,\u03b8 with \u2225w\u2225= 1 and \u01eb > C log n/n1/4,\n|\nPr\nz\u2190Usp[ \u27e8w, z\u27e9\u2265\u03b8 ] \u2212\nPr\nx\u2190D,y\u2208u{0,1}r[ \u27e8w, Gsph(x, y)\u27e9\u2265\u03b8 ]| = O(\u01eb).\nProof. By Lemma 6.2, for z \u2190Usp, \u27e8w, z\u27e9is O(\u01eb)-close to N(0, 1/\u221an).\nFurther, by applying\nLemma 6.3 for \u03b3 = 1/\u221a\u01eb, we get that v = HD(x)w is \u03b4-regular with probability at least 1 \u2212O(\u01eb)\nfor \u03b4 = 1/(\u221an\u01eb1/4) < \u01eb. Now, by Theorem 4.3 for v \u01eb-regular and y \u2208u {0, 1}r, the distribution of\n\u27e8v, G(y)\u27e9is O(\u01eb)-close to N(0, 1). The theorem now follows from combining the above claims and\nnoting that \u27e8v, G(y)/\u221an\u27e9= \u27e8w, Gsph(x, y)\u27e9.\nTheorem 1.4 now follows from the above theorem and derandomizing G as done in Section 4.3\nfor proving Theorem 1.3.\nAcknowledgements\nWe thank Omer Reingold for allowing us to use his observation improving the seed-length of\nTheorem 1.3 from the conference version. The preliminary version of this work appearing in STOC\n2010 had a worse seed-length of O(log n log(1/\u01eb)). However, a minor change in the argument where\nwe use the PRG for small space machines of Impagliazzo et al. [INW94] instead of the PRG of Nisan\n[Nis92] in the monotone trick leads to the new improved parameters. We thank Amir Shpilka for\ndrawing to our attention the problem of fooling spherical caps and pointing us to the work of Ailon\nand Chazelle. We thank Parikshit Gopalan, Prahladh Harsha, Adam Klivans and Ryan O\u2019Donnell\nfor useful discussions and comments.\n24\nReferences\n[ABFR94]\nJames Aspnes, Richard Beigel, Merrick L. Furst, and Steven Rudich. The expressive\npower of voting polynomials. Combinatorica, 14(2):135\u2013148, 1994. (Preliminary version in 23rd\nSTOC, 1991). doi:10.1007/BF01215346.\n[AC06]\nNir Ailon and Bernard Chazelle. Approximate nearest neighbors and the fast Johnson-\nLindenstrauss transform.\nIn STOC \u201906:\nProceedings of the thirty-eighth annual ACM\nsymposium on Theory of computing, pages 557\u2013563. ACM, New York, NY, USA, 2006.\ndoi:http://doi.acm.org/10.1145/1132516.1132597.\n[BBC+01]\nRobert Beals, Harry Buhrman, Richard Cleve, Michele Mosca, and Ronald\nde Wolf.\nQuantum lower bounds by polynomials.\nJournal of the ACM, 48(4):778\u2013\n797,\n2001.\n(Preliminary\nversion in\n39th\nFOCS,\n1998).\narXiv:quant-ph/9802049,\ndoi:10.1145/502090.502097.\n[Bei93]\nRichard Beigel. The polynomial method in circuit complexity. In Proc. of 8th Annual Structure\nin Complexity Theory Conference, pages 82\u201395. 1993. doi:10.1109/SCT.1993.336538.\n[BELY09]\nIdo Ben-Eliezer, Shachar Lovett, and Ariel Yadin.\nPolynomial threshold functions:\nStructure, approximation and pseudorandomness, 2009. arXiv:0911.3473.\n[CW77]\nLarry Carter and Mark N. Wegman. Universal classes of hash functions (extended ab-\nstract). In STOC, pages 106\u2013112. 1977.\n[CW01]\nAnthony Carbery and James Wright. Distributional and lq norm inequalities for polyno-\nmials over convex bodies in Rn. Mathematical Research Letters, 8(3):233\u2013248, 2001.\n[DGJ+09]\nIlias Diakonikolas, Parikshit Gopalan, Ragesh Jaiswal, Rocco A. Servedio, and\nEmanuele Viola. Bounded independence fools halfspaces. In FOCS. 2009.\n[DKN10]\nIlias Diakonikolas, Daniel Kane, and Jelani Nelson. Bounded independence fools degree-\n2 threshold functions. In FOCS. 2010.\n[DSTW10] Ilias Diakonikolas, Rocco A. Servedio, Li-Yang Tan, and Andrew Wan. A regularity\nlemma, and low-weight approximators, for low-degree polynomial threshold functions. In IEEE\nConference on Computational Complexity, pages 211\u2013222. 2010.\n[Fel71]\nWilliam Feller. An Introduction to Probability Theory and Its Applications, Vol. 2 (Volume\n2). Wiley, 2 edition, January 1971.\n[GKM10]\nParikshit Gopalan, Adam Klivans, and Raghu Meka.\nPolynomial-time approxima-\ntion schemes for knapsack and related counting problems using branching programs, 2010.\narXiv:1008.3187.\n[GOWZ10] Parikshit Gopalan, Ryan O\u2019Donnell, Yi Wu, and David Zuckerman. Fooling functions\nof halfspaces under product distributions. In IEEE Conference on Computational Complexity,\npages 223\u2013234. 2010.\n[GR09]\nParikshit Gopalan and Jaikumar Radhakrishnan. Finding duplicates in a data stream.\nIn SODA, pages 402\u2013411. 2009. doi:10.1145/1496770.1496815.\n[H\u02daas94]\nJohan H\u02daastad. On the size of weights for threshold gates. SIAM J. Discret. Math., 7(3):484\u2013\n492, 1994. doi:http://dx.doi.org/10.1137/S0895480192235878.\n[HKM09]\nPrahladh Harsha, Adam Klivans, and Raghu Meka. Bounding the sensitivity of polyno-\nmial threshold functions, 2009. arXiv:0909.5175.\n[HKM10]\n\u2014\u2014\u2014. An invariance principle for polytopes. In STOC, pages 543\u2013552. 2010.\n[INW94]\nRussell Impagliazzo, Noam Nisan, and Avi Wigderson. Pseudorandomness for network al-\ngorithms. In STOC, pages 356\u2013364. 1994. doi:http://doi.acm.org/10.1145/195058.195190.\n25\n[KRS09]\nZohar Shay Karnin, Yuval Rabani, and Amir Shpilka. Explicit dimension reduction and\nits applications. Electronic Colloquium on Computational Complexity (ECCC), 16(121), 2009.\n[KS04]\nAdam R. Klivans and Rocco A. Servedio. Learning DNF in time 2O(n1/3). Journal of\nComputer and System Sciences, 68(2):303\u2013318, 2004. (Preliminary version in 33rd STOC, 2001).\ndoi:10.1016/j.jcss.2003.07.007.\n[LC67]\nP. M. Lewis and C. L. Coates. Threshold Logic. John Wiley, New York, 1967.\n[LRTV09]\nShachar Lovett, Omer Reingold, Luca Trevisan, and Salil P. Vadhan. Pseudorandom\nbit generators that fool modular sums. In APPROX-RANDOM, pages 615\u2013630. 2009.\n[LT91]\nMichel Ledoux and Michel Talagrand. Probability in Banach spaces: isoperimetry and\nprocesses. Springer, 1991.\n[MOO05]\nElchanan Mossel, Ryan O\u2019Donnell, and Krzysztof Oleszkiewicz.\nNoise stability\nof functions with low in\ufb02uences: invariance and optimality.\nIn FOCS, pages 21\u201330. 2005.\ndoi:10.1109/SFCS.2005.53.\n[MT94]\nWolfgang Maass and Gy\u00a8orgy Tur\u00b4an. How fast can a threshold gate learn?\nIn Proceed-\nings of a workshop on Computational learning theory and natural learning systems (vol. 1) :\nconstraints and prospects, pages 381\u2013414. MIT Press, Cambridge, MA, USA, 1994.\n[Nis92]\nNoam Nisan.\nPseudorandom generators for space-bounded computation.\nCombinatorica,\n12(4):449\u2013461, 1992.\n[NN93]\nJoseph Naor and Moni Naor.\nSmall-bias probability spaces: E\ufb03cient constructions and\napplications. SIAM Journal on Computing, 22(4):838\u2013856, 1993. doi:10.1137/0222053.\n[NZ96]\nNoam Nisan and David Zuckerman. Randomness is linear in space. J. Comput. Syst. Sci.,\n52(1):43\u201352, 1996.\n[OS08]\nRyan O\u2019Donnell and Rocco A. Servedio. The Chow parameters problem. In STOC, pages\n517\u2013526. 2008.\n[RS09]\nYuval Rabani and Amir Shpilka.\nExplicit construction of a small epsilon-net for linear\nthreshold functions. In STOC, pages 649\u2013658. 2009.\n[RSOK91]\nV. Roychowdhury, K. Y. Siu, A. Orlitsky, and T. Kailath.\nA geometric approach\nto threshold circuit complexity. In COLT \u201991: Proceedings of the fourth annual workshop on\nComputational learning theory, pages 97\u2013111. Morgan Kaufmann Publishers Inc., San Francisco,\nCA, USA, 1991.\n[RV05]\nEyal Rozenman and Salil P. Vadhan. Derandomized squaring of graphs. In APPROX-\nRANDOM, pages 436\u2013447. 2005. doi:10.1007/11538462_37.\n[Ser06]\nRocco A. Servedio. Every linear threshold function has a low-weight approximator. In IEEE\nConference on Computational Complexity, pages 18\u201332. 2006. doi:10.1109/CCC.2006.18.\n[She07]\nI.\nG.\nShevtsova.\nSharpening of the upper bound of the absolute constant in the\nBerry\u2013Ess\u00b4een inequality.\nTheory of Probability and its Applications, 51(3):549\u2013553, 2007.\ndoi:10.1137/S0040585X97982591.\n[Siv02]\nD. Sivakumar. Algorithmic derandomization via complexity theory. In STOC \u201902: Proceedings\nof the thiry-fourth annual ACM symposium on Theory of computing, pages 619\u2013626. ACM, New\nYork, NY, USA, 2002. doi:10.1145/509907.509996.\n26\nA\nNon-Explicit Bounds\nIt is known ([LC67], [RSOK91]) that the number of distinct halfspaces on n bits is at most 2n2. One\nway of extending this bound to degree d PTFs is as follows. It is known that the Fourier coe\ufb03cients\nof the \ufb01rst d + 1 levels of a degree d PTF, also known as the Chow parameters, determine the PTF\ncompletely (see [OS08]). Thus, a PTF f is completely determined by ChowParam(f) = ( E[f \u00b7 \u03c7I] :\nI \u2286[n], |I| \u2264d ), where \u03c7I(x) = Q\ni\u2208I xi denotes the parity over the coordinates in I. Observe that\nfor any I \u2286[n], E[ f \u00b7 \u03c7I ] \u2208{i/2n : i \u2208Z, |i| \u22642n}. Therefore, the number of distinct degree d\nPTFs is at most the number of distinct sequences ChowParam( ), which in turn is at most (2n)nd.\nThe non-explicit bound now follows by observing that any class of boolean functions F can be\nfooled with error at most \u01eb by a set of size at most O(log(|F|)/\u01eb2). Thus, degree d PTFs can be\nfooled by a sample space of size at most O(nd+1/\u01eb2).\n27\n",
        "sentence": "",
        "context": "bound of P\ni \u03c4i(P)2. Diakonikolas et al. have a statement similar to Lemma 5.17 below; however,\nwe give a simple argument starting from the main lemmas of Harsha et al. for completeness.\nIn Proceed-\nings of a workshop on Computational learning theory and natural learning systems (vol. 1) :\nconstraints and prospects, pages 381\u2013414. MIT Press, Cambridge, MA, USA, 1994.\n[Nis92]\nNoam Nisan.\n\u2032\u2032\u2032\u2032(t)| \u2264B for all t \u2208R.\nLemma 5.8 (Mossel et al.). Let X, Y be two real-valued random variables such that the following\nhold.\n1. For any interval I \u2286R of length at most \u03b1, Pr[ X \u2208I ] \u2264C\u03b11/d, where C is a constant\nindependent of \u03b1."
    },
    {
        "title": "G\u00e9za Freud, orthogonal polynomials and Christoffel functions. A case study",
        "author": [
            "Paul Nevai"
        ],
        "venue": "Journal of Approximation Theory,",
        "citeRegEx": "Nevai.,? \\Q1986\\E",
        "shortCiteRegEx": "Nevai.",
        "year": 1986,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Small-bias probability spaces: Efficient constructions and applications",
        "author": [
            "Joseph Naor",
            "Moni Naor"
        ],
        "venue": "SIAM J. Computing,",
        "citeRegEx": "Naor and Naor.,? \\Q1993\\E",
        "shortCiteRegEx": "Naor and Naor.",
        "year": 1993,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "On the degree of boolean functions as real polynomials",
        "author": [
            "N. Nisan",
            "M. Szegedy"
        ],
        "venue": "Computational Complexity,",
        "citeRegEx": "Nisan and Szegedy.,? \\Q1994\\E",
        "shortCiteRegEx": "Nisan and Szegedy.",
        "year": 1994,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Weighted polynomial inequalities",
        "author": [
            "Paul Nevai",
            "Vilmos Totik"
        ],
        "venue": "Constructive Approximation,",
        "citeRegEx": "Nevai and Totik.,? \\Q1986\\E",
        "shortCiteRegEx": "Nevai and Totik.",
        "year": 1986,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Sharp Nikolskii inequalities with exponential weights",
        "author": [
            "P. Nevai",
            "V. Totik"
        ],
        "venue": "Analysis Mathematica,",
        "citeRegEx": "Nevai and Totik.,? \\Q1987\\E",
        "shortCiteRegEx": "Nevai and Totik.",
        "year": 1987,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Analysis of Boolean Functions",
        "author": [
            "Ryan O\u2019Donnell"
        ],
        "venue": null,
        "citeRegEx": "O.Donnell.,? \\Q2014\\E",
        "shortCiteRegEx": "O.Donnell.",
        "year": 2014,
        "abstract": "Scribe notes from the 2012 Barbados Workshop on Computational Complexity. A\nseries of lectures on Analysis of Boolean Functions by Ryan O'Donnell, with a\nguest lecture by Per Austrin.",
        "full_text": "arXiv:1205.0314v1  [cs.CC]  2 May 2012\nAnalysis of Boolean Functions\nNotes from a series of lectures by\nRyan O\u2019Donnell\nGuest lecture by Per Austrin\nBarbados Workshop on Computational Complexity\nFebruary 26th \u2013 March 4th, 2012\nOrganized by Denis Th\u00b4erien\nScribe notes by Li-Yang Tan\nContents\n1\nLinearity testing and Arrow\u2019s theorem\n3\n1.1\nThe Fourier expansion\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n1.2\nBlum-Luby-Rubinfeld . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n1.3\nVoting and in\ufb02uence\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n1.4\nNoise stability and Arrow\u2019s theorem . . . . . . . . . . . . . . . . . . . . . .\n12\n2\nNoise stability and small set expansion\n15\n2.1\nSheppard\u2019s formula and Stab\u03c1(MAJ) . . . . . . . . . . . . . . . . . . . . . .\n15\n2.2\nThe noisy hypercube graph . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n2.3\nBonami\u2019s lemma . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n3\nKKL and quasirandomness\n20\n3.1\nSmall set expansion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n3.2\nKahn-Kalai-Linial . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\n3.3\nDictator versus Quasirandom tests . . . . . . . . . . . . . . . . . . . . . . .\n22\n4\nCSPs and hardness of approximation\n26\n4.1\nConstraint satisfaction problems . . . . . . . . . . . . . . . . . . . . . . . .\n26\n4.2\nBerry-Ess\u00b4een . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\n5\nMajority Is Stablest\n30\n5.1\nBorell\u2019s isoperimetric inequality\n. . . . . . . . . . . . . . . . . . . . . . . .\n30\n5.2\nProof outline of MIST\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n32\n5.3\nThe invariance principle\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\n6\nTesting dictators and UGC-hardness\n37\n1\nLinearity testing and Arrow\u2019s theorem\nMonday, 27th February 2012\n\u2022 Open Problem [Guy86, HK92]: Let a \u2208Rn with \u2225a\u22252 = 1. Prove Prx\u2208{\u22121,1}n[|\u27e8a, x\u27e9| \u2264\n1] \u22651\n2.\n\u2022 Open Problem (S. Srinivasan): Suppose g : {\u22121, 1}n \u2192\u00b1\n\u0002 2\n3, 1\n\u0003\nwhere g(x) \u2208\n\u0002 2\n3, 1\n\u0003\nif\nPn\ni=1 xi \u2265n\n2 and g(x) \u2208\n\u0002\n\u22121, \u22122\n3\n\u0003\nif Pn\ni=1 xi \u2264\u2212n\n2 . Prove deg(f) = \u2126(n).\nIn this workshop we will study the analysis of boolean functions and its applications to\ntopics such as property testing, voting, pseudorandomness, Gaussian geometry and the\nhardness of approximation. Two recurring themes that we will see throughout the week\nare:\n\u2022 The noisy hypercube graph is a small set expander.\n\u2022 Every boolean function has a \u201cjunta part\u201d and a \u201cGaussian part\u201d.\n1.1\nThe Fourier expansion\nBroadly speaking, the analysis of boolean functions is concerned with properties of boolean\nfunctions f : {\u22121, 1}n \u2192{\u22121, 1} viewed as multilinear polynomials over R. Consider the\nmajority function over 3 variables MAJ3(x) = sgn(x1 + x2 + x3). It is easy to check that\nMAJ3(x) =\n1\n2x1 + 1\n2x2 + 1\n2x3 \u22121\n2x1x2x3, and this can be derived by summing 23 = 8\npolynomials py : {\u22121, 1}3 \u2192{\u22121, 0, 1}, one for each y \u2208{\u22121, 1}3, where py(x) takes value\nMAJ3(x) when y = x and 0 otherwise. For example,\np(\u22121,1,\u22121)(x) =\n\u00101 \u2212x1\n2\n\u0011\u00101 + x2\n2\n\u0011\u00101 \u2212x3\n2\n\u0011\n\u00b7 MAJ3(\u22121, 1, \u22121).\nNote that the \ufb01nal polynomial that results from expanding and simplifying the sum of\npy\u2019s is indeed always multilinear (i.e. no variable xi occurs squared, or cubed, etc.) since\nx2\ni = 1 for bits xi \u2208{\u22121, 1}. The same interpolation procedure can be carried out for any\nf : {\u22121, 1}n \u2192R:\nTheorem 1 (Fourier expansion) Every f : {\u22121, 1}n \u2192R can be uniquely expressed as\na multilinear polynomial R,\nf(x) =\nX\nS\u2286[n]\ncS\nY\ni\u2208S\nxi,\nwhere each cS \u2208R.\nWe will write \u02c6f(S) to denote the coe\ufb03cient cS and \u03c7S(x) for the function Q\ni\u2208S xi, and call\nf(x) = P\nS\u2286[n] \u02c6f(S)\u03c7S(x) the Fourier expansion of f. We adopt the convention that \u03c7\u2205\u22611,\nthe identically 1 function. We will write deg(f) to denote maxS\u2286[n]{|S| : \u02c6f(S) \u0338= 0}, and\ncall this quantity the Fourier degree of f.\n3\nWe will sometimes refer to \u03c7S(x) : {\u22121, 1}n \u2192{\u22121, 1} as the \u201cparity-on-S\u201d function, since\nit takes value 1 if there are an even number of \u22121 coordinates in x and \u22121 otherwise.\nUsing the notation of Theorem 1, we have that \\\nMAJ3({1}) = 1\n2, \\\nMAJ3({1, 2, 3}) = \u22121\n2,\n\\\nMAJ3({1, 2}) = 0, and deg(MAJ3) = 3.\nWe have already seen that every function f : {\u22121, 1}n \u2192R can be expressed as a multilinear\npolynomial over R (via the interpolation procedure described for MAJ3); to complete the\nproof of Theorem 1 it remains to show uniqueness. Let V be the vector space of all functions\nf : {\u22121, 1}n \u2192R. Here we are viewing f as a 2n-dimensional vector in R2n, with each\ncoordinate being the value of f on some input x \u2208{\u22121, 1}n; if f is a boolean function this\nis simply the truth table of f. Note that the parity functions \u03c7S(x) are all elements of V ,\nand furthermore every f \u2208V can be expressed as a linear combination of them (i.e. the\n\u03c7S(x)\u2019s are a spanning set for V ). Since there are 2n parity functions and dim(V ) = 2n,\nit follows that {\u03c7S : S \u2286[n]} is a basis for V , and this establishes the uniqueness of the\nFourier expansion.\nDe\ufb01nition 2 (inner product) Let f, g : {\u22121, 1}n \u2192{\u22121, 1}. We de\ufb01ne the inner prod-\nuct between f and g as\n\u27e8f, g\u27e9:=\nX\nx\u2208{\u22121,1}n\nf(x) \u00b7 g(x)\n2n\n=\nE\nx\u2208{\u22121,1}n[f(x)g(x)].\nNote that this is simply the dot product between f and g viewed as vectors in R2n, normal-\nized by a factor of 2\u2212n. Given this de\ufb01nition, every boolean function f : {\u22121, 1}n \u2192{\u22121, 1}\nis a unit vector in R2n since \u27e8f, f\u27e9= 1. We will also write \u2225f\u22252\n2 to denote \u27e8f, f\u27e9, and more\ngenerally, \u2225f\u2225p := E[|f(x)|p]1/p.\nTheorem 3 (orthonormality) The set of parity functions {\u03c7S(x) : S \u2286[n]} is an or-\nthonormal basis for R2n. That is, for every S, T \u2286[n],\n\u27e8\u03c7S, \u03c7T \u27e9=\n\u001a 1\nif S = T\n0\notherwise.\nProof. First note that \u03c7S \u00b7 \u03c7T = \u03c7S\u2206T since Q\ni\u2208S xi\nQ\nj\u2208T xj = Q\ni\u2208S\u2206T xi\nQ\nj\u2208S\u2229T x2\nj =\nQ\ni\u2208S\u2206T xi, where the \ufb01nal equality uses the fact that x2\ni = 1 for xi \u2208{\u22121, 1}. Next, we\nclaim that\nE[\u03c7U] =\n\u001a 1\nif U = \u2205\n0\notherwise.\nnoting that this implies the theorem since S\u2206T = \u2205i\ufb00S = T. Recall that we have de\ufb01ned\n\u03c7\u2205to be the identically 1 function, and if U \u0338= \u2205then exactly half the inputs x \u2208{\u22121, 1}n\nhave \u03c7U(x) = 1 and the other half \u03c7U(x) = \u22121.\nProposition 4 (Fourier coe\ufb03cient) Let f : {\u22121, 1}n \u2192R.\nThen \u02c6f(S) = \u27e8f, \u03c7S\u27e9=\nE[f(x)\u03c7S(x)].\n4\nProof. To see that this holds, we check that\n\u27e8f, \u03c7S\u27e9=\nD P\nT\u2286[n]\n\u02c6f(T)\u03c7T , \u03c7S\nE\n=\nX\nT\u2286[n]\n\u02c6f(T) \u00b7 \u27e8\u03c7T , \u03c7S\u27e9= \u02c6f(S).\nHere we have used the Fourier expansion of f for the \ufb01rst equality, linearity of the inner\nproduct for the second, and orthonormality of parity functions (Theorem 3) for the last.\nNext we have Plancherel\u2019s theorem, which states that the inner product of f and g is\nprecisely the dot product of their vectors of Fourier coe\ufb03cients.\nTheorem 5 (Plancherel) Let f, g : {\u22121, 1}n \u2192R. Then, \u27e8f, g\u27e9= P\nS\u2286[n] \u02c6f(S)\u02c6g(S).\nProof. Again we use the Fourier expansions of f and g to check that\n\u27e8f, g\u27e9=\nD P\nS\u2286[n]\n\u02c6f(S)\u03c7S, P\nT\u2286[n]\n\u02c6g(T)\u03c7T\nE\n=\nX\nS\nX\nT\n\u02c6f(S)\u02c6g(T) \u00b7 \u27e8\u03c7S, \u03c7T \u27e9=\nX\nS\u2286[n]\n\u02c6f(S)\u02c6g(S).\nThe second equality holds by linearity of inner product, and the last by orthonormality.\nAn important corollary of Plancherel\u2019s theorem is Parseval\u2019s identity: if f : {\u22121, 1}n \u2192R,\nthen \u2225f\u22252\n2 = \u27e8f, f\u27e9= P\nS\u2286[n] \u02c6f(S)2 (i.e. the Fourier transform preserves L2-norm). In\nparticular, if f is a boolean function then P\nS\u2286[n] \u02c6f(S)2 = E[f(x)2] = 1, which we may view\nas a probability distribution over the 2n possible subsets S of [n]. Note that if f and g are\nboolean functions then f(x)g(x) = 1 i\ufb00f(x) = g(x), and so E[f(x)g(x)] = 1 \u22122 \u00b7 dist(f, g),\nwhere dist(f, g) = Pr[f(x) \u0338= g(x)] is the normalized Hamming distance between f and g.\nOne of the advantages of analyzing f via its Fourier expansion is that this polynomial\nencodes a lot of combinatorial information about f, and these combinatorial quantities can\nbe \u201cread o\ufb00\u201d its Fourier coe\ufb03cients easily. We give two basic examples now. Recall that\nfor functions f : {\u22121, 1}n \u2192R, the mean of f is E[f(x)] and its variance is Var(f) :=\nE[f(x)2] \u2212E[f(x)]2. Note that if f is a boolean function then E[f(x)] measures the bias of\nf towards 1 or \u22121, and Var(f) = 4 \u00b7 Pr[f(x) = 1] \u00b7 Pr[f(x) = \u22121]. If f has mean 0 and\nvariance 1 we say that f is balanced, or unbiased.\nProposition 6 (expectation and variance) Let f : {\u22121, 1}n \u2192R.\nThen E[f(x)] =\n\u02c6f(\u2205), and Var(f) = P\nS\u0338=\u2205\u02c6f(S)2.\nProof. For the \ufb01rst equality, we check that \u02c6f(\u2205) = E[f(x)\u03c7\u2205(x)] = E[f(x)]. The second\nequality holds because\nVar(f) = E[f(x)2] \u2212E[f(x)]2 =\n\u0010 P\nS\u2286[n]\n\u02c6f(S)2\u0011\n\u2212\u02c6f(\u2205)2 =\nX\nS\u0338=\u2205\n\u02c6f(S)2.\nHere the second equality uses an application of Parseval\u2019s identity.\nIt is nice to think of \u02c6f(S)2 as the \u201cweight\u201d of f on S, with the sum of weights of f on all\n2n subsets S of [n] being 1 by Parseval\u2019s. Often it will also be convenient to stratify these\nweights according to the cardinality of the set S.\n5\nDe\ufb01nition 7 (level weights) Let f : {\u22121, 1}n \u2192R and k \u2208{0, . . . , n}. The weight of f\nat level k, or the degree-k weight of f, is de\ufb01ned to be Wk(f) := P\n|S|=k \u02c6f(S)2.\nFor example, in this notation we have W0(MAJ3) = W2(MAJ3) = 0 and W1(MAJ3) =\nW3(MAJ3) = 1/2.\n1.1.1\nDensity functions and convolutions\nSo far we have been viewing domain {\u22121, 1}n of our functions simply as strings of bits\nrepresented by real numbers \u00b11.\nOften we would like to be able to \u201cadd\u201d two inputs,\nin which case we will view our functions as f : Fn\n2 \u2192R instead. The mapping from F2\nto {\u22121, 1} is given by (\u22121)b, sending 0 \u2208F2 to 1 \u2208R and 1 \u2208F2 to \u22121 \u2208R.\nWe\nwill sometimes also associate a boolean function Fn\n2 \u2192{\u22121, 1} with its corresponding F2\npolynomial Fn\n2 \u2192F2. For example, the di\ufb00erent representations of the parity function \u03c7S\nare given in Table 1.\n\u03c7S(x) : {\u22121, 1}n \u2192{\u22121, 1}\nx 7\u2192Q\ni\u2208S xi\n\u03c7S(x) : Fn\n2 \u2192{\u22121, 1}\nx 7\u2192Q\ni\u2208S(\u22121)xi\n\u03c7S(x) : Fn\n2 \u2192F2\nx 7\u2192P\ni\u2208S xi\nTable 1: Di\ufb00erent representations of \u03c7S(x)\nThe F2 degree of a boolean function f : Fn\n2 \u2192{\u22121, 1}, denoted degF2(f), is its degree as\nan F2 polynomial Fn\n2 \u2192F2. For example, the parity functions are degree-1 F2 polynomials;\nin contrast, recall that deg(\u03c7S), the Fourier degree of \u03c7S, is |S|. In general we have the\ninequality degF2(f) \u2264deg(f) for all boolean functions f : Fn\n2 \u2192{\u22121, 1}. We remark that\nunlike Fourier degree, it is not known how to infer the F2 degree of a boolean function from\nits Fourier expansion. The following useful fact can be easily veri\ufb01ed:\nFact 8 Let \u03c7S : Fn\n2 \u2192{\u22121, 1}. Then \u03c7S(x + y) = \u03c7S(x) \u00b7 \u03c7S(y).\nDe\ufb01nition 9 (probability density function) \u03d5 : Fn\n2 \u2192R\u22650 is a probability density\nfunction of Ex\u2208Fn\n2 [\u03d5(x)] = 1.\nNote that a probability density function \u03d5 : Fn\n2 \u2192R\u22650 corresponds to the probability\ndistribution over Fn\n2 where Pr[x] = \u03d5(x) \u00b7 2n. For example, the constant function \u03d5 \u22611\ncorresponds to the uniform distribution over Fn\n2. For any a \u2208Fn\n2, the density function \u03d5a(x)\nthat takes value 2n if x = a and 0 otherwise corresponds to the distribution that puts all\nits weight on a single point a \u2208Fn\n2.\nDe\ufb01nition 10 (convolution) Let f, g : Fn\n2 \u2192R.\nThe convolution of f and g is the\nfunction f \u2217g : Fn\n2 \u2192R de\ufb01ned by (f \u2217g)(x) := Ey\u2208Fn\n2 [f(y)g(x + y)].\n6\nNote that (f \u2217g)(x) = (g \u2217f)(x), since (y, y + x) is just a uniformly random pair of inputs\nwith distance x and therefore has the same distribution as (y + x, y). Similarly it can be\nchecked that the convolution operator is commutative: (f \u2217g)\u2217h = f \u2217(g\u2217h). The following\nfacts also follow easily from de\ufb01nitions:\nFact 11 Let f : Fn\n2 \u2192R and \u03d52, \u03d52 be density functions. Then\n1. \u27e8\u03d5, f\u27e9= Ey\u223c\u03d5[f(y)].\n2. (\u03d5 \u2217f)(x) = Ey\u223c\u03d5[f(x + y)].\n3. The density for z = y1 + y2, where y1 \u223c\u03d51 and y2 \u223c\u03d52, is \u03d51 \u2217\u03d52.\nTheorem 12 (Fourier coe\ufb03cients of convolutions) Let f, g : Fn\n2 \u2192R. Then [\nf \u2217g(S) =\n\u02c6f(S) \u00b7 \u02c6g(S).\nProof. We check that\n[\nf \u2217g(S) = E\nx [(f \u2217g)(x)\u03c7S(x)]\n=\nE\nx\nh\nE\ny [f(y)g(x + y)] \u00b7 \u03c7S(x)\ni\n=\nE\ny\nh\nf(y) \u00b7 E\nx [g(x + y)\u03c7S(x)]\ni\n=\nE\ny\nh\nf(y) \u00b7 E\nz [g(z)\u03c7S(z + y)]\ni\n(1)\n=\nE\ny\nh\nf(y) \u00b7 E\nz [g(z)\u03c7S(z)\u03c7S(y)]\ni\n(2)\n=\nE\ny [f(y)\u03c7S(y)] \u00b7 E[g(z)\u03c7S(z)] =\n\u02c6f(S) \u00b7 \u02c6g(S).\nHere (1) uses the fact that z \u2212y = z + y for y, z \u2208Fn\n2, and (2) is an application of Fact 8.\nTheorem 13 Let f, g, h : Fn\n2 \u2192R. Then \u27e8f \u2217g, h\u27e9= \u27e8f, g \u2217h\u27e9.\nProof. By Theorem 12 and Plancherel, both sides of the identity equal P\nS\u2286[n] \u02c6f(S)\u02c6g(S)\u02c6h(S).\n1.2\nBlum-Luby-Rubinfeld\nWe begin by considering two notions of what it means for a function f : Fn\n2 \u2192F2 to be\nlinear.\nDe\ufb01nition 14 (linear #1) A boolean function f : Fn\n2 \u2192F2 is linear if f(x + y) = f(x) +\nf(y) for all x, y \u2208Fn\n2.\nDe\ufb01nition 15 (linear #2) A boolean function f : Fn\n2 \u2192F2 is linear if there exists a1, . . . , an \u2208\nF2 such that f(x) = a1x1 + . . . + anxn. Equivalently, there exists some S \u2286[n] such that\nf(x) = P\ni\u2208S xi.\n7\nProposition 16 (#1 \u21d0\u21d2#2) These two de\ufb01nitions are equivalent.\nProof. Suppose f satis\ufb01es f(x + y) = f(x) + f(y) for all x, y \u2208Fn\n2. Let \u03b1i = f(ei) \u2208F2,\nwhere ei is the i-th canonical basis vector for Fn\n2. It follows that f(x) = f(Pn\ni=1 xiei) =\nPn\ni=1 xif(ei) = Pn\ni=1 \u03b1ixi, where the second equality uses De\ufb01nition 14 repeatedly, along\nwith the fact that f(xiei) = xif(ei). For the reverse implication, note that f(x + y) =\nP\ni\u2208S(x + y)i = P\ni\u2208S xi + P\ni\u2208S yi = f(x) + f(y), where the \ufb01rst and \ufb01nal equalities uses\nDe\ufb01nition 15.\nIt is natural to consider analogous notions for approximate linearity.\nDe\ufb01nition 17 (approximately linear #1) A boolean function f : Fn\n2 \u2192F2 is approxi-\nmately linear if f(x + y) = f(x) + f(y) for most pairs x, y \u2208Fn\n2.\nDe\ufb01nition 18 (approximately linear #2) A boolean function f : Fn\n2 \u2192F2 is approxi-\nmately linear if there exists some S \u2286[n] such that f(x) = P\ni\u2208S xi for most x \u2208Fn\n2. Equiv-\nalently, there exists an S \u2286[n] such that f is close in Hamming distance to g(x) = P\ni\u2208S xi.\nA straightforward generalization of argument given in the proof of Proposition 16 shows that\nDe\ufb01nition 18 (approximately linear #2) implies De\ufb01nition 17 (approximately linear #1).\nHowever, the argument for the reverse implication no longer holds. We will adopt De\ufb01nition\n18 as our notion of approximate linearity for now, and we will see that the linearity test\nof Blum, Luby, and Rubinfeld [BLR93] implies that both de\ufb01nitions are in fact equivalent.\nThe Fourier-analytic proof we present here is due to Bellare et. al [BCH+96].\nDe\ufb01nition 19 (BLR linearity test) Given blackbox access to a function f : Fn\n2 \u2192F2,\n1. Pick x, y \u2208Fn\n2 independently and uniformly.\n2. Query f on x, y and x + y.\n3. Accept i\ufb00f(x) + f(y) = f(x + y).\nTheorem 20 (soundness of BLR) If Pr[BLR accepts f] \u22651\u2212\u03b5 then f is \u03b5-close to being\nlinear (in the sense of De\ufb01nition 18).\nProof. It will be convenient to think of f as Fn\n2 \u2192{\u22121, 1}, and so the acceptance criterion\n(i.e. step 3) becomes f(x)f(y) = f(x + y). Viewing f this way, now note that\nPr[BLR accepts f]\n=\nE\nx,y\n\u0002\n1(f(x) \u00b7 f(y) = f(x + y)\n\u0003\n=\nE\nx,y\n\u0002 1\n2 + 1\n2 \u00b7 f(x)f(y)f(x + y)\n\u0003\n=\n1\n2 + 1\n2 E\nx [f(x) \u00b7 (f \u2217f)(x)]\n=\n1\n2 + 1\n2\nX\nS\u2286[n]\n\u02c6f(S) \u00b7 [\nf \u2217f(S)\n(3)\n=\n1\n2 + 1\n2\nX\nS\u2286[n]\n\u02c6f(S)3.\n(4)\n8\nHere (3) uses Parseval\u2019s identity and (4) uses Theorem 12. Therefore, if Pr[BLR accepts f] \u2265\n1\u2212\u03b5 then 1\u22122\u03b5 \u2264P\nS\u2286[n] \u02c6f(S)3 \u2264max{ \u02c6f(S)}\u00b7P\nS\u2286[n] \u02c6f(S)2 = max{ \u02c6f(S)}, or equivalently,\nthere exists an S\u2217\u2286[n] such that \u02c6f(S\u2217) \u22651 \u22122\u03b5.\nSince \u02c6f(S\u2217) = E[f(x)\u03c7S\u2217(x)] =\n1 \u22122 \u00b7 dist(f, \u03c7S\u2217), we have shown that dist(f, \u03c7S\u2217) \u2264\u03b5 and the proof is complete.\nTheorem 20 says that if Pr[BLR accepts f] \u22651\u2212\u03b5 then f is \u03b5-close to some linear function\n\u03c7S\u2217; however, we do not know which of the 2n possible linear functions \u03c7S\u2217this is. The\nfollowing theorem tells us that we can nevertheless obtain the correct value of \u03c7S\u2217(x) with\nhigh probability for all x \u2208Fn\n2.\nTheorem 21 (local decodability of linear functions) Let f : Fn\n2 \u2192F2 be \u03b5-close to\nsome linear function \u03c7S\u2217, and let x \u2208Fn\n2. The following algorithm outputs \u03c7S\u2217(x) with\nprobability at least 1 \u22122\u03b5:\n1. Pick y \u2208Fn\n2 uniformly.\n2. Output f(y) + f(x + y).\nProof. Since x and x + y are both uniform (though not independent), with probability at\nleast 1 \u22122\u03b5 we have f(y) = \u03c7S\u2217(y) and f(x + y) = \u03c7S\u2217(x + y). The claim follows by noting\nthat \u03c7S\u2217(y) + \u03c7S\u2217(x + y) = \u03c7S\u2217(y + (x + y)) = \u03c7S\u2217(x), where we have used the linearity of\n\u03c7S\u2217along with the fact that x + y = x \u2212y for x, y \u2208Fn\n2.\n1.3\nVoting and in\ufb02uence\n\u2022 Puzzle: Is it possible for f : {\u22121, 1}n \u2192{\u22121, 1} to have exactly k non-zero Fourier\ncoe\ufb03cients, for k = 0, 1, 2, 3, 4, 5, 6, 7? Classify all functions with 2 non-zero Fourier\ncoe\ufb03cients.\n\u2022 Puzzle: Find all f : {\u22121, 1}n \u2192{\u22121, 1} with W1(f) = 1.\nWe may think of a boolean function f : {\u22121, 1}n \u2192{\u22121, 1} as a voting scheme for an\nelection with 2 candidates (\u00b11) and n voters (x1, . . . , xn).\nMany boolean functions are\nnamed after the voting schemes they correspond to: the i-th dictator DICTi(x) = xi (i.e.\nDICTi \u2261\u03c7i); k-juntas (functions that depend only on k of its n variables, where we think\nof k as \u226an, or even a constant); the majority function MAJ(x) = sgn(x1 + . . . + xn). The\nmajority function is special instance of linear threshold functions f(x) = sgn(a0 + a1x1 +\n. . . + anxn), ai \u2208R, also known as weighted-majority functions, or halfspaces. Another\nimportant voting scheme in boolean function analysis is TRIBESw,s : {\u22121, 1}ws \u2192{\u22121, 1},\nthe s-way OR of w-way AND\u2019s of disjoint sets of variables (where we think of \u22121 as true\nand 1 as false). In TRIBESw,s, the candidate \u22121 is elected i\ufb00at least one member of each\nof the s disjoint tribes of w members votes for \u22121.\nThe following are a few reasonable properties one may expect of a voting scheme:\n\u2022 Monotone: if xi \u2264yi for all i \u2208[n] then f(x) \u2264f(y).\n\u2022 Symmetric: f(\u03c0(x)) = f(x) for all permutations \u03c0 \u2208Sn and x \u2208{\u22121, 1}n.\n9\n\u2022 Transitive-symmetric (weaker than symmetric): for all i, j \u2208[n] there exists a permu-\ntation \u03c0 \u2208Sn such that \u03c0(i) = j and f(x) = f(\u03c0(x)) for all x \u2208{\u22121, 1}n.\nLater in this section (for the proof of Arrow\u2019s theorem) we will also assume that voters vote\nindependently and uniformly; this is known as the impartial culture assumption in social\nchoice theory.\nDe\ufb01nition 22 (in\ufb02uence) Let f : {\u22121, 1}n \u2192{\u22121, 1}. We say that variable i \u2208[n] is\npivotal for x \u2208{\u22121, 1}n if f(x) \u0338= f(x\u2295i), where x\u2295i is the string x with its i-th bit \ufb02ipped.\nThe in\ufb02uence of variable i on f, denoted Infi(f), is the fraction of inputs for which i is\npivotal. That is, Infi(f) := Pr[f(x) \u0338= f(x\u2295i)].\nFor example, Infi(DICTj) is 1 if i = j and 0 otherwise. For the majority function over\nan odd number n of variables, Infi(MAJ) =\n\u0000n\u22121\n(n\u22121)/2\n\u0001\n\u00b7 2\u2212(n\u22121) since voter i is pivotal i\ufb00\nthe votes are split evenly among the other n \u22121 voters. By Stirling\u2019s approximation, this\nquantity is \u223c\np\n2/\u03c0n = \u0398(1/\u221an).\nDe\ufb01nition 23 (derivative) Let f : {\u22121, 1}n \u2192R. The i-th derivative of f is the function\n(Dif)(x) := 1\n2(f(xi\u21901) \u2212f(xi\u2190\u22121)), where xi\u2190b is the string x with its i-th bit set to b.\nNote that if f is a boolean function then (Dif)(x) = \u00b11 if i is pivotal for f at x, and 0 oth-\nerwise, and therefore E[(Dif)(x)2] = Infi(f). We will adopt E[(Dif)(x)2] as the generalized\nde\ufb01nition of the in\ufb02uence of variable i on f for real-valued functions f : {\u22121, 1}n \u2192R.\nTheorem 24 (Fourier expressions for derivatives and in\ufb02uence) Let f : {\u22121, 1}n \u2192\nR. Then\n1. (Dif)(x) = P\nS\u220bi \u02c6f(S)\u03c7S\\i(x).\n2. Infi(f) = P\nS\u220bi \u02c6f(S)2.\nProof. The \ufb01rst identity holds by noting that (Di\u03c7S)(x) = \u03c7S\\i(x) if i \u2208S and 0 otherwise,\nalong with the fact that Di is a linear operator, i.e. Di(\u03b1f + g) = \u03b1(Dif) + (Dig). The\nsecond identity then follows by applying Parseval to Infi(f) = E[(Dif)(x)2].\nProposition 25 (in\ufb02uence of monotone functions) Let f : {\u22121, 1}n \u2192{\u22121, 1} be a\nmonotone function. Then Infi(f) = \u02c6f(i).\nProof. If f is monotone then (Dif)(x) \u2208{0, 1} and so Infi(f) = E[(Dif)(x)2] = E[(Dif)(x)] =\nd\nDif(\u2205) = \u02c6f(i). Here the \ufb01nal equality uses the Fourier expansion of Dif given by Theorem\n24.\nAn immediate corollary of Proposition 25 is that monotone, transitive-symmetric func-\ntions f have Infi(f) \u22641/\u221an for all i \u2208[n]. This follows from the fact that transitive-\nsymmetric functions satisfy \u02c6f(i) = \u02c6f(j) for all i, j \u2208[n], along with the bound Pn\ni=1 \u02c6f(i)2 \u2264\nP\nS\u2286[n] \u02c6f(S)2 = 1.\n10\nDe\ufb01nition 26 (total in\ufb02uence) Let f : {\u22121, 1}n \u2192R.\nThe total in\ufb02uence of f is\nInf(f) := Pn\ni=1 Infi(f).\nIf f is a boolean function, then\nInf(f) =\nn\nX\ni=1\nPr[f(x) \u0338= f(x\u2295i)] =\nn\nX\ni=1\nE[1(f(x) \u0338= f(x\u2295i))] = E\nh nP\ni=1\n1(f(x) \u0338= f(x\u2295i))\ni\n.\nThe quantity Pn\ni=1 1(f(x) \u0338= f(x\u2295i)) is known as the sensitivity of f at x, and so the total\nin\ufb02uence of a boolean function is also known as its average sensitivity. If f is viewed as a\n2-coloring of the boolean hypercube, the total in\ufb02uence can also be seen to be equal to n\ntimes the fraction of bichromatic edges.\nIf f is a monotone boolean function, we see that Inf(f) = Pn\ni=1 \u02c6f(i) = Pn\ni=1 E[f(x)xi] =\nE[f(x)(x1 + . . . + xn)]. Recall that if f is boolean then f(x)(x1 + . . . + xn) = 1 if f(x) =\nsgn(x1 + . . . + xn) and \u22121 otherwise. Therefore the total in\ufb02uence a monotone boolean\nfunction, when viewed as a voting scheme, measures the expected di\ufb00erence between the\nnumber of voters whose vote agrees with the outcome of the election and the number whose\nvote disagrees. It is reasonable to expect this quantity to large, and the next proposition\nstates that (if n is odd) it is maximized by the majority function.\nProposition 27 (MAJ maximizes sum of linear coe\ufb03cients) Let n be odd.\nAmong\nall boolean functions f : {\u22121, 1}n \u2192{\u22121, 1}, the quantity Pn\ni=1 \u02c6f(i) is maximized by\nMAJ(x) = sgn(x1 + . . . + xn). Consequently, if f is monotone then Inf(f) \u2264Inf(MAJ) \u223c\np\n2n/\u03c0.\nProof. Note that Pn\ni=1 \u02c6f(i) = E[f(x)(x1 + . . . + xn)] \u2264E[|x1 + . . . + xn|] since f is (\u00b11)-\nvalued, where the inequality is tight i\ufb00f(x) = sgn(x1 +. . . +xn) = MAJ(x). For the second\nclaim recall that Infi(f) = \u02c6f(i) if f is monotone (Proposition 25), and Infi(MAJ) \u223c\np\n2/\u03c0n\nfor all i \u2208[n].\nProposition 28 (Fourier expression for total in\ufb02uence) Let f : {\u22121, 1}n \u2192R. Then\nInfi(f) = P\nS\u2286[n] |S| \u00b7 \u02c6f(S)2 = Pn\nk=1 k \u00b7 Wk(f).\nThe proof of this proposition follows immediately from the Fourier expression for variable\nin\ufb02uence given by Theorem 24.\nNotice that each Fourier coe\ufb03cient is weighted by its\ncardinality in the sum, and so total in\ufb02uence may also be viewed as a measure of the\n\u201caverage degree\u201d of f\u2019s Fourier expansion.\nRecall that for functions f : {\u22121, 1}n \u2192R we have Var(f) = P\nS\u0338=\u2205\u02c6f(S)2 (Proposi-\ntion 6), and comparing this quantity with the Fourier expression for total in\ufb02uence yields\nVar(f) \u2264Inf(f), the Poincar\u00b4e inequality for the boolean hypercube.\nThe inequality\nis tight i\ufb00W1(f) = 1, which for boolean functions implies that f = \u00b1DICTi(f) for\nsome i \u2208[n].\nIf f is a boolean function and p := Pr[f(x) = 1], it is easy to check\nthat Var(p) = 4p(1 \u2212p), and so the Poincar\u00b4e inequality can be equivalently stated as\n11\n4p(1 \u2212p) \u2264Inf(f) = n \u00b7 (fraction of bichromatic edges). The Poincar\u00b4e inequality is there-\nfore an edge-isoperimetric inequality for the boolean hypercube (where we view boolean\nfunctions as indicators of subsets of {\u22121, 1}n): for any p, it gives a lower bound on the\nnumber of boundary edges between A and A where A \u2286{\u22121, 1}n has density p. This is\na sharp bound when p = 1/2, but not when p is small. For smaller densities we have the\nbound 2\u03b1 log2(1/\u03b1) \u2264Inf(f), where \u03b1 := min {Pr[f(x) = 1], Pr[f(x) = \u22121]}. This in turn\nis sharp whenever \u03b1 = 2k, achieved by the AND of k coordinates.\n1.4\nNoise stability and Arrow\u2019s theorem\nLet \u03c1 \u2208[0, 1] and \ufb01x x \u2208{\u22121, 1}n.\nLet N\u03c1(x) be the distribution on {\u22121, 1}n where\ny \u223cN\u03c1(x) if for all i \u2208[n], yi = xi with probability \u03c1, and yi is uniformly random \u00b11 with\nprobability 1 \u2212\u03c1. More generally, for \u03c1 \u2208[\u22121, 1], we have that N\u03c1(x) is the distribution on\nstrings y where\nyi =\n\u001a\nxi\nwith probability 1\n2 + 1\n2\u03c1\n\u2212xi\nwith probability 1\n2 \u22121\n2\u03c1.\nIf x \u223c{\u22121, 1}n is uniformly random and y \u223cN\u03c1(x), we say that x and y are \u03c1-correlated\nstrings; equivalently, x and y are \u03c1-correlated if they are both uniformly random and\nE[xiyi] = \u03c1 for all i \u2208[n].\nDe\ufb01nition 29 (noise stability) Let f : {\u22121, 1}n \u2192R and \u03c1 \u2208[\u22121, 1]. The noise stability\nof f at noise rate \u03c1 is\nStab\u03c1(f) := E[f(x)f(y)],\nwhere x, y are \u03c1-correlated strings.\nFor example Stab\u03c1(\u00b11) = 1, Stab\u03c1(DICTi) = \u03c1, and Stab\u03c1(\u03c7S) = \u03c1|S|. Tomorrow we will\nprove Sheppard\u2019s formula: limn\u2192\u221eStab\u03c1(MAJ) = 1\u22122\n\u03c0 arccos(\u03c1). In particular, if \u03c1 = 1\u2212\u03b4\nwe have Stab\u03c1(MAJ) = \u0398(\n\u221a\n\u03b4).\nDe\ufb01nition 30 (noise operator) Let \u03c1 \u2208[\u22121, 1].\nThe noise operator T\u03c1 on functions\nf : {\u22121, 1}n \u2192R acts as follows: (T\u03c1f)(x) := Ey\u223cN\u03c1(x)[f(y)].\nProposition 31 (Fourier expressions for noise operator and stability) Let \u03c1 \u2208[\u22121, 1]\nand f : {\u22121, 1}n \u2192R. Then\n1. (T\u03c1f)(x) = P\nS\u2286[n] \u03c1|S| \u02c6f(S)\u03c7S(x).\n2. Stab\u03c1(f) = P\nS\u2286[n] \u03c1|S| \u02c6f(S)2.\nProof. The \ufb01rst identity follows from the linearity of the noise operator, along with the\nobservation that (T\u03c1\u03c7S)(x) = \u03c1|S|\u03c7S(x). The second holds by noting that\nStab\u03c1(f) =\nE\n(x,y) \u03c1-corr[f(x)f(y)] = E\nx [f(x)(T\u03c1f)(x)] =\nX\nS\u2286[n]\n\u02c6f(S)d\nT\u03c1f(S) =\nX\nS\u2286[n]\n\u03c1|S| \u02c6f(S)2.\n12\nSuppose there is an election with n voters and three candidates: A, B and C. Each voter\nranks the candidates by submitting three bits indicating her preferences: whether the prefers\nA to B (say, \u22121 if so and 1 otherwise), and similarly for B versus C and C versus A. Clearly\na rational voter cannot simultaneously prefer A to B, B to C and C to A; her ordering of\nthe candidates must be non-cyclic.\nDe\ufb01nition 32 (rational) A triple (a, b, c) \u2208{\u22121, 1}3 is rational if not all three bits are\nequal (i.e., (a, b, c) de\ufb01nes a total ordering, and is a valid preference pro\ufb01le). We de\ufb01ne the\nfunction NAE : {\u22121, 1}3 \u2192{1, 0} to be 1 i\ufb00not all three bits are equal.\nNow suppose the preferences of the n voters are aggregated into three n-bit strings x, y and z,\nand the aggregate preference of the electorate is represented by the triple (f(x), f(y), f(z))\nfor some boolean function f : {\u22121, 1}n \u2192{\u22121, 1}. Clearly we would like for the the outcome\nof the election to be rational; that is, NAE(f(x), f(y), f(z)) = 1.\nFact 33 (Condorcet\u2019s paradox [Con85]) With MAJ as the aggregating function it is\npossible that all voters submit rational preferences and yet the aggregated preference string\nis irrational.\nv1\nv2\nv3\nMAJ\nA > B ?\n+1\n+1\n\u22121\n+1\nB > C ?\n+1\n\u22121\n+1\n+1\nC > A ?\n\u22121\n+1\n+1\n+1\nFigure 1: An instance of Condorcet\u2019s paradox\nTheorem 34 (Arrow\u2019s impossibility theorem [Arr50]) Suppose f is an aggregating\nfunction that always produces a rational outcome if all voters vote rationally. Then f =\n\u00b1DICTi for some i \u2208[n]. If f is further restricted to be unanimous (i.e. f(1, . . . , 1) = 1 and\nf(\u22121, . . . , \u22121) = \u22121; certainly a very reasonable assumption) then f must be a dictator.\nThe main result of this section is a robust version of Arrow\u2019s impossibility theory due\nGil Kalai [Kal02]. It expresses the probability that an aggregating function f produces a\nrational outcome in terms of the noise stability of f, under the impartial culture assumption\n(each voter selects an NAE-triple (xi, yi, zi) uniformly and independently).\nTheorem 35 (Kalai) E[NAE(f(x), f(y), f(z))] = 3\n4 \u22123\n4 \u00b7 Stab\u22121/3(f), where the expecta-\ntion is taken with respect to the impartial culture assumption.\nProof. Using the arithmetization NAE(a, b, c) = 3\n4 \u22121\n4(ab + bc + ac), we \ufb01rst note that\nE[NAE(f(x), f(y), f(z))]\n=\n3\n4 \u22121\n4\n\u0000E[f(x)f(y)] + E[f(y)f(z)] + E[f(x)f(z)]\n\u0001\n=\n3\n4 \u22123\n4 E[f(x)f(y)],\n13\nwhere again, all expectations are taken with respect to the impartial culture assumption.\nSince E[xiyi] = \u22121/3 if (xi, yi, zi) is an NAE-triple, the quantity E[f(x)f(y)] is exactly\nStab\u22121/3(f) and the proof is complete.\nTheorem 35 does indeed imply Arrow\u2019s impossibility theorem since\n3\n4 \u22123\n4 \u00b7 Stab\u22121/3(f) = 3\n4 \u22123\n4\nn\nX\nk=0\n(\u22121\n3)k \u00b7 Wk(f) \u22647\n9 + 2\n9 \u00b7 W1(f),\nand so if E[NAE(f(x), f(y), f(z))] = 1 then W1(f) \u22651. Furthermore note that the proba-\nbility of an irrational outcome is at least 1 \u2212\u03b5 then W1 \u22651 \u2212O(\u03b5). By a theorem of E.\nFriedgut, G. Kalai and A. Naor [FKN02], if W1(f) \u22651 \u2212\u03b5 then f is O(\u03b5)-close to \u00b1DICTi\nfor some i \u2208[n]. Therefore Kalai\u2019s theorem is in fact a robust version of Arrow\u2019s impossi-\nbility theorem: if most rational voter preference pro\ufb01les aggregate to a rational outcome,\nthen the aggregating function must be close to a dictator or anti-dictator.\nWe conclude by giving an upper bound on level-1 Fourier weight of transitive-symmetric\nfunctions. By Theorem 35, this gives an upper bound on the probability that such functions\naggregate rational voter preference pro\ufb01les to a rational outcome. We will also prove a\ngeneralization of this fact (Proposition 42) using the Berry-Ess\u00b4een theorem tomorrow.\nProposition 36 (W1(f) of transitive-symmetric functions) Suppose \u02c6f(i) = \u02c6f(j) for\nall i, j \u2208[n]. Then W1(f) \u22642\n\u03c0 + on(1).\nProof. First note that Pn\ni=1 \u02c6f(i)2 = n \u00b7 \u02c6f(1)2 = n \u00b7\n\u0000 1\nn\nPn\ni=1 \u02c6f(i)\n\u00012 = 1\nn\n\u0000 Pn\ni=1 \u02c6f(i)\n\u00012. The\nclaim then follows since we have seen that Pn\ni=1 \u02c6f(i) \u2264Pn\ni=1 [\nMAJ(i) \u223c\np\n2n/\u03c0 (Proposition\n27).\n14\n2\nNoise stability and small set expansion\nTuesday, 28th February 2012\n\u2022 Puzzle: Compute the Fourier expansion of MAJn. Hint: consider T\u03c1DiMAJ(1, . . . , 1).\nRoughly speaking, the central limit theorem states that if X1, . . . , Xn are independent ran-\ndom variables where none of the Xi\u2019s are \u201ctoo dominant\u201d, then S = Pn\ni=1 Xi is distributed\nlike a Gaussian as n \u2192\u221e. As a warm-up, we begin by giving another proof of the fact\nthat Inf(MAJ) \u223c\np\n2n/\u03c0, this time using the central limit theorem. First note that\nInf(MAJ) = E\nh\nMAJ(x)\nnP\ni=1\nxi\ni\n= E\nh\f\f\f\nnP\ni=1\nxi\n\f\f\f\ni\n= \u221an \u00b7 E\nh\f\f\f\nnP\ni=1\nxi\n\u221an\n\f\f\f\ni\nNow by the central limit theorem we know that\n1\n\u221an\nPn\ni=1 xi \u2192G \u223cN(0, 1) as n \u2192\u221e, and\nsince E[|G|] =\np\n2/\u03c0, we conclude that Inf(MAJ) \u223c\np\n2/\u03c0 \u00b7 \u221an.\nDe\ufb01nition 37 (reasonable r.v.) Let B \u22651. We say that a random variable X is B-\nreasonable if E[X4] \u2264B \u00b7 E[X2]2. Equivalently, \u2225X\u22254 \u2264B1/4 \u00b7 \u2225X\u22252.\nFor example, a uniformly random \u00b11 bit (i.e.\na Rademacher random variable) is 1-\nreasonable, and a standard Gaussian is 3-reasonable. The Berry-Ess\u00b4een theorem [Ber41,\nEss42] is a \ufb01nitary version of the central limit theorem, giving explicit bounds on the rate\nat which reasonable random variables converge towards the Gaussian distribution.\nTheorem 38 (Berry-Ess\u00b4een) Let X1, . . . , Xn be independent, B-reasonable random vari-\nables satisfying E[Xi] = 0. Let \u03c32\ni := E[X2\ni ] and suppose Pn\ni=1 \u03c32\ni = 1. Let S = X1+. . .+Xn\nand G \u223cN(0, 1). For all t \u2208R,\n| Pr[S \u2264t] \u2212Pr[G \u2264t]| \u2264O(\u03b5)\nwhere \u03b5 =\n\u0000B \u00b7 Pn\ni=1 \u03c34\ni\n\u00011/2 \u2264\n\u221a\nB \u00b7 max {|\u03c3i|}.\nWe prove the Berry-Ess\u00b4een theorem with a weaker bound of \u03b5 =\n\u0000B \u00b7Pn\ni=1 \u03c34\ni\n\u00011/5 in Section\n4.2.\n2.1\nSheppard\u2019s formula and Stab\u03c1(MAJ)\nDe\ufb01nition 39 (\u03c1-correlated Gaussians) Let G, G\u2032 \u223cN(0, 1) be independent standard\nGaussians.\nSet H = (G, G\u2032) \u00b7 (\u03c1,\np\n1 \u2212\u03c12) := \u03c1 \u00b7 G +\np\n1 \u2212\u03c12 \u00b7 G\u2032. Then G and H are\n\u03c1-correlated Gaussians. Note that if G and H are \u03c1-correlated Gaussians then E[GH] =\n\u03c1 \u00b7 E[G2] +\np\n1 \u2212\u03c12 \u00b7 E[G] E[G\u2032] = \u03c1.\nTheorem 40 ([She99]) Let G and H be \u03c1-correlated Gaussians. Then Pr[sgn(G) \u0338= sgn(H)] =\narccos(\u03c1)/\u03c0.\n15\nProof. First recall that sgn(\u20d7u \u00b7 \u20d7v) is determined by which side of the halfspace normal to \u20d7u\nthe vector \u20d7v falls on. Since H = (\u03c1,\np\n1 \u2212\u03c12) \u00b7 (G, G\u2032), and G = (1, 0) \u00b7 (G, G\u2032), it follows that\nPr[sgn(G) \u0338= sgn(H)] is precisely the probability that the halfspace normal to (G, G\u2032) splits\nthe vectors (\u03c1,\np\n1 \u2212\u03c12) and (1, 0). Therefore,\nPr[sgn(G) \u0338= sgn(H)] = 1\n\u03c0 \u00b7 (angle between (\u03c1,\np\n1 \u2212\u03c12) and (1, 0)) = 1\n\u03c0 arccos(\u03c1).\nNext, we use Sheppard\u2019s formula to prove that Stab\u03c1(MAJ) \u21921 \u22122\n\u03c0 arccos(\u03c1) as n \u2192\u221e.\nFirst recall that\nStab\u03c1(MAJ) =\nE\n(x,y) \u03c1-corr[MAJ(x)MAJ(y)] = 1 \u22122 Pr[MAJ(x) \u0338= MAJ(y)],\nand so it su\ufb03ces to argue that Pr[MAJ(x) \u0338= MAJ(y)] \u21921\n\u03c0 arccos(\u03c1). Next, we view\nMAJ(x) = sgn\n\u0010x1 + . . . + xn\n\u221an\n\u0011\n,\nMAJ(y) = sgn\n\u0010x1 + . . . + xn\n\u221an\n\u0011\n,\nand note that\nE\n\u0014\u0010 nP\ni=1\nxi\n\u221an\n\u0011\n\u00b7\n\u0010 nP\ni=1\nyi\n\u221an\n\u0011\u0015\n= E\n\u0014\n1\nn\nnP\ni=1\nxi \u00b7\nnP\ni=1\nyi\n\u0015\n= 1\nn\nn\nX\ni=1\nE[xiyi] = \u03c1.\nWhile the standard central limit theorem tells us that \u20d7X = (x1+. . .+xn)/\u221an and \u20d7Y = (y1+\n. . . + yn)/\u221an each individually converges towards the standard Gaussian G \u223cN(0, 1), the\ntwo-dimensional central limit theorem states that ( \u20d7X, \u20d7Y ) actually converge to \u03c1-correlated\nGaussians (G, H) as n \u2192\u221e. In fact, the two-dimensional Berry-Ess\u00b4een theorem quanti\ufb01es\nthis rate of convergence, bounding the error by \u00b1O(1/\u221an) as long as \u03c1 is bounded away\nfrom \u00b11. Combining this with Sheppard\u2019s formula, we conclude that\nPr[MAJ(x) \u0338= MAJ(y)] \u2192Pr[sgn(G) \u0338= sgn(H)] = 1\n\u03c0 arccos(\u03c1)\nas n \u2192\u221e.\nSince\nStab\u03c1(MAJ) \u21921 \u22122\n\u03c0 arccos(\u03c1) = 2\n\u03c0 + 3\n\u03c0\u03c13 + . . . +\n\u0000k\u22121\n(k\u22121)/2\n\u0001\n4\n\u03c0k2k \u00b7 \u03c1k + . . .\nand Stab\u03c1(MAJ) = Pn\nk=0 \u03c1k\u00b7Wk(MAJ), we have that Wk(MAJ) \u2192\n\u0000k\u22121\n(k\u22121)/2\n\u0001\n4\n\u03c0k2k as n \u2192\u221e.\n2.2\nThe noisy hypercube graph\nLet \u03c1 \u2208[\u22121, 1]. The \u03c1-noisy hypercube graph is a complete weighted graph on the vertex\nset {\u22121, 1}n, where the weight on an edge (x, y) is the probability of getting x and y when\ndrawing \u03c1-correlated strings from {\u22121, 1}n. Equivalently, wt(x, y) = Pr[x \u2190U] Pr[y \u2190\nN\u03c1(x)] = 2\u2212n \u00b7 (1\n2 \u22121\n2\u03c1)\u2206(x,y)(1\n2 + 1\n2\u03c1)n\u2212\u2206(x,y), and the sum of weights of all edges incident\nto any x \u2208{\u22121, 1}n is exactly 2\u2212n.\n16\nRecall that if f is a boolean function, then Stab\u03c1(f) = E(x,y) \u03c1-corr[f(x)f(y)] = 1\u22122 Pr[f(x) \u0338=\nf(y)], or equivalently, Pr(x,y) \u03c1-corr[f(x) \u0338= f(y)] = 1\n2 \u22121\n2 \u00b7 Stab\u03c1(f). Viewing f as the in-\ndicator of a subset Af \u2286{\u22121, 1}n, the quantity Pr[f(x) \u0338= f(y)] is the sum of weights of\nedges going from Af to its complement Af. Therefore, roughly speaking, a function f is\nnoise stable i\ufb00the sum of weights of edges contained within Af and Af is large. On Friday\nwe will prove the Majority Is Stablest theorem due to E. Mossel, R. O\u2019Donnell, and K.\nOlezkiewicz [MOO10]:\nMajority Is Stablest. Fix a constant 0 < \u03c1 < 1, and let f : {\u22121, 1}n \u2192\n{\u22121, 1} be a balanced function. It is easy to see that Stab\u03c1(f) is maximized\nwhen W1(f) = 1, in which case f = \u00b1DICTi and Stab\u03c1(DICTi) = \u03c1. However,\nif we additionally require that Infi(f) \u2264\u03c4 for all i \u2208[n], then the theorem\nstates that Stab\u03c1(f) \u2264Stab\u03c1(MAJ) + o\u03c4(1); i.e. the set A of density 1/2 with\nall in\ufb02uences small that maximizes sum of weights of edges within A and A is\nAMAJ.\nLet A \u2286{\u22121, 1}n and 1A(x) : {\u22121, 1}n \u2192{0, 1} be its indicator function, and let \u03b1 :=\nE[1A(x)] = |A| \u00b7 2\u2212n denote its density. Recall that Stab\u03c1(1A) = E[1A(x)1A(y)] = Pr[x \u2208\nA & y \u2208A], or equivalently, Pr[y \u2208A | x \u2208A] =\n1\n\u03b1 \u00b7 Stab\u03c1(1A), and so Stab\u03c1(1A) is\nthe probability that a random walk (where each coordinate is independently \ufb02ipped with\nprobability (1\n2 \u22121\n2\u03c1)) starting at a point x \u2208A remains in A, normalized by the density of\nA. Tomorrow we will prove (a speci\ufb01c instance of) the small set expansion theorem:\nSmall Set Expansion. Stab\u03c1(1A) \u2264\u03b12/(1+\u03c1), or equivalently, Pr[y \u2208A | x \u2208\nA] \u2264\u03b1(1\u2212\u03c1)/(1+\u03c1). In particular, if \u03b1 is small, the probability that a random\nwalk starting in A landing outside A is very high.\nRecall that Stab\u03c1(1A) = W0(1A)+\u03c1\u00b7W1(1A)+\u03c12\u00b7W2(1A)+. . ., and so d\nd\u03c1Stab\u03c1(1A)\n\f\f\n\u03c1=0 =\nW1(1A). As a direct corollary of the small set expansion theorem we have the following\nbound on W1(1A):\nW1(1A) =\nd\nd\u03c1Stab\u03c1(1A)\n\f\f\f\n\u03c1=0 \u2264\nd\nd\u03c1\u03b12/(1+\u03c1)\f\f\f\n\u03c1=0 = 2\u03b12 ln(1/\u03b1).\nWe now give a self-contained proof this fact, due to Talagrand [Tal96]:\nTheorem 41 (level-1 inequality) Let f : {\u22121, 1}n \u2192{0, 1} and \u03b1 = E[f].\nThen\nW1(f) = O(\u03b12 ln(1/\u03b1)).\nProof. First consider an arbitrary linear form \u2113(x) = Pn\ni=1 aixi normalized to satisfy P a2\ni =\n1. For any t0 \u22651, we partition x \u2208{\u22121, 1}n according to whether |\u2113(x)| < t0 or |\u2113(x)| \u2265t0,\nand note that\nE[f(x)\u2113(x)] = E\n\u0002\n1(|\u2113(x)| < t0) \u00b7 f(x)\u2113(x)\n\u0003\n+ E\n\u0002\n1(|\u2113(x)| \u2265t0) \u00b7 f(x)\u2113(x)\n\u0003\n.\n17\nThe \ufb01rst summand is at most \u03b1 \u00b7 t0, and the second is at most\nZ \u221e\nt0\n2e\u2212t2/2 dt \u22642\nZ \u221e\nt0\nte\u2212t2/2 dt =\nh\n\u22122e\u2212t2/2i\u221e\nt0 = 2 \u00b7 e\u2212t2\n0/2\nby Hoe\ufb00ding, where the inequality holds since t0 \u22651. Choosing t0 = (2 ln(1/\u03b1))1/2 \u22651, we\nget\nE[f(x)\u2113(x)] \u2264O(\u03b1\np\nln(1/\u03b1)).\n(5)\nNow let \u2113(x) = 1\n\u03c3\nPn\ni=1 \u02c6f(i)xi where \u03c3 =\np\nW1(f) (if \u03c3 = 0 we are done). Note that\nE[f(x)\u2113(x)] =\nn\nX\ni=1\n\u02c6f(i)\u02c6\u2113(i) = 1\n\u03c3\nn\nX\ni=1\n\u02c6f(i)2 =\np\nW1(f).\nThe claimed inequality then follows by applying (5).\nProposition 42 ( 2\n\u03c0 theorem) Let f : {\u22121, 1}n \u2192{\u22121, 1} with | \u02c6f(i)| \u2264\u03b5 for all i \u2208[n].\nThen W1(f) \u22642\n\u03c0 + O(\u03b5).\nProof. Let \u03c3 =\np\nW1(f) and assume without loss of generality that \u03c3 \u22651/2 (otherwise\nW1(f) < 1/4 < 2\n\u03c0 and we are done). Let \u2113(x) = 1\n\u03c3\nPn\ni=1 \u02c6f(i)xi where |\u02c6\u2113(i)| \u22642\u03b5 for all\ni \u2208[n]. Note that E[f(x)\u2113(x)] = \u03c3 and E[\u2113(x)f(x)] \u2264E[|\u2113(x)|]. Applying Berry-Ess\u00b4een\ngives us E[|\u2113(x)|] \u2248O(\u03b5) E[|G|] =\np\n2/\u03c0 and this completes the proof (technically Berry-\nEss\u00b4een only yields a bound on closeness in cdf-distance, but this can be translated into a\nbound on closeness in \ufb01rst moments).\nA dual to Proposition 42 holds as well: if W1(f) \u2265\n2\n\u03c0 \u2212\u03b5, then f is O(\u221a\u03b5)-close to a\nlinear threshold function (in fact the LTF is simply sgn\n\u0000 Pn\ni=1 \u02c6f(i)xi\n\u0001\n). This is the crux\nof the result that the class of linear threshold functions is testable with poly(1/\u03b5)-queries\n[MORS10].\n2.3\nBonami\u2019s lemma\nThe next theorem, due to Bonami [Bon70], states that low degree multilinear polynomi-\nals of Rademachers are reasonable random variables (this is sometimes known as (4, 2)-\nhypercontractivity).\nTheorem 43 (Bonami) Let f : {\u22121, 1}n \u2192R be a multilinear polynomial of degree at\nmost d. Then \u2225f\u22254 \u2264\n\u221a\n3\nd\u2225f\u22252. Equivalently, E[f(x)4] \u22649d \u00b7 E[f(x)2]2\nProof. We proceed by induction on n. If n = 0 then f is the constant and the inequality\nholds trivially for all d. For the inductive step, let\nf(x1, . . . , xn) = g(x1, . . . , xn\u22121) + xnh(x1, . . . , xn\u22121)\n18\nNotice that g has degree at most d, h has degree at most d \u22121, and both are polynomials\nin n \u22121 variables. Notice also that the random variable xn is independent of both g and h.\nTherefore, we have:\nE[f 4]\n=\nE[(g + xnh)4]\n=\nE[g4] + 3 E[xn] E[g3h] + 6 E[x2\nn] E[g2h2] + 3 E[x3\nn] E[gh3] + E[x4\nn] E[h4]\nwhere we used independence for the second equality. Now note that E[xn] = E[x3\nn] = 0,\nE[x2\nn] = E[x4\nn] = 1, and E[g2h2] \u2264\np\nE[g4]\np\nE[h4] by Cauchy-Schwarz. Therefore,\nE[f 4]\n\u2264\nE[g4] + 6\np\nE[g4]\np\nE[h4] + E[h4]\n\u2264\n9d E[g2]2 + 6\nq\n9d E[g2]2\nq\n9d\u22121 E[h2]2 + 9d\u22121 E[h2]2\n=\n9d \u00b7 (E[g2]2 + 2 E[g2] E[h2] + 1\n9 E[h2]2)\n\u2264\n9d \u00b7 (E[g2] + E[h2])2\nTo complete the proof, notice that:\nE[f 2]\n=\nE[(g + xnh)2]\n=\nE[g2] + 2 E[xn] E[gh] + E[x2\nn] E[h2]\n=\nE[g2] + E[h2]\nand so we have shown that E[f 4] \u22649d \u00b7 E[f 2]2.\n19\n3\nKKL and quasirandomness\nWednesday, 29th February 2012\n\u2022 Open Problem: Prove that among all functions f : {\u22121, 1}n \u2192{\u22121, 1} with deg(f) \u2264\nd, the quantity Pn\ni=1 \u02c6f(i) is maximized by MAJd. Less ambitiously, show Pn\ni=1 \u02c6f(i) =\nO(\np\ndeg(f)).\n3.1\nSmall set expansion\nWe begin by proving the \u03c1 = 1/3 case of the small set expansion theorem: let A \u2286{\u22121, 1}n\nbe a set of density \u03b1 = |A| \u00b7 2\u2212n, and 1A : {\u22121, 1}n \u2192{0, 1} be its indicator function. We\nwill need the following variant of Bonami\u2019s lemma; its proof is identical to that of Theorem\n43.\nTheorem 44 (Bonami\u2019) Let f : {\u22121, 1}n \u2192R. Then \u2225T1/\n\u221a\n3f\u22254 \u2264\u2225f\u22252.\nTheorem 44 is a special case of the hypercontractivity inequality [Bon70, Gro75, Bec75]: if\n1 \u2264p \u2264q \u2264\u221eand \u03c1 \u2264\nq\np\u22121\nq\u22121, then \u2225T\u03c1f\u2225q \u2264\u2225f\u2225p.\nTheorem 45 (SSE for \u03c1 = 1/3) Let A \u2286{\u22121, 1}n. Then Stab1/3(1A) \u2264\u03b13/2, where \u03b1\nis the density of A.\nProof. We will need a corollary of Theorem 44 that will also be useful for us when proving\nthe KKL theorem in the next section: \u2225T1\n\u221a\n3f\u22252\n2 \u2264\u2225f\u22252\n4/3. To see that this holds, we check\nthat\n\u2225T1/\n\u221a\n3f\u22252\n2\n=\nE[f(x)(T1/3f)(x)]\n\u2264\n\u2225f\u22254/3 \u00b7 \u2225T1/3f\u22254\n(6)\n=\n\u2225f\u22254/3 \u00b7 \u2225T1/\n\u221a\n3 (T1/\n\u221a\n3f)\u22254\n\u2264\n\u2225f\u22254/3 \u00b7 \u2225T1/\n\u221a\n3f\u22252.\n(7)\nHere (6) is by H\u00a8older\u2019s inequality and (7) by applying Theorem 44 to T1/\n\u221a\n3f; dividing both\nsides by \u2225T1/\n\u221a\n3f\u22252 yields the claim. Applying this corollary to f = 1A completes the proof:\nStab1/3(1A) =\nX\nS\u2286[n]\n\u00001\n3\n\u0001|S| \u02c6f(S)2 =\n\r\rT1/\n\u221a\n31A\n\r\r2\n2 \u2264E\n\u0002\n1A(x)4/3\u00033/2 = \u03b13/2.\nHere the second equality is an application of Parseval\u2019s, and the \ufb01nal uses the fact that 1A\nis {0, 1}-valued.\n20\n3.2\nKahn-Kalai-Linial\nTheorem 46 ([KKL98]) Let f : {\u22121, 1}n \u2192{\u22121, 1} with E[f] = 0, and set \u03b1 :=\nmaxi\u2208[n] {Infi(f)}. Then Inf(f) = \u2126(log(1/\u03b1)).\nProof. Recall that variable i is pivotal for x in f i\ufb00(Dif)(x) = \u00b11, and so E[|Dif|] =\nInfi(f) \u2264\u03b1; the plan is to apply the small set expansion theorem to Dif and sum over all\ni \u2208[n]. We have shown in the proof of Theorem 45 that \u2225T1/\n\u221a\n3g\u22252\n2 \u2264\u2225g\u22252\n4/3, and applying\nit to g = Dif gives us\nStab1/3(Dif) \u2264\u2225Dif\u22252\n4/3 = E\n\u0002\n|Dif|4/3\u00033/2 = Infi(f)3/2.\n(8)\nWe sum both sides of the inequality over i \u2208[n], starting with the left hand side. Recall\nthat Stab1/3(Dif) = P\nS\u2286[n]\n\u0000 1\n3\n\u0001|S| d\nDif(S)2 = P\nS\u220bi\n\u00001\n3\n\u0001|S|\u22121 \u02c6f(S)2, and so:\nn\nX\ni=1\nStab1/3(Dif)\n=\nn\nX\ni=1\nX\nS\u220bi\n\u0000 1\n3\n\u0001|S|\u22121 \u02c6f(S)2\n=\nX\n|S|\u22651\n|S|\n\u0000 1\n3\n\u0001|S|\u22121 \u02c6f(S)2\n\u2265\nX\n1\u2264|S|\u22642 Inf(f)\n|S|\n\u0000 1\n3\n\u0001|S|\u22121 \u02c6f(S)2\n\u2265\nX\n1\u2264|S|\u22642 Inf(f)\n2 Inf(f) \u00b7\n\u0000 1\n3\n\u00012 Inf(f)\u22121 \u02c6f(S)2\n(9)\n\u2265\n3 Inf(f) \u00b7\n\u0000 1\n9\n\u0001Inf(f).\n(10)\nHere (9) uses the fact that x \u00b7 3\u2212(x\u22121) is a decreasing function when x \u22651, and (10) uses\nMarkov\u2019s inequality P\n|S|\u22652 Inf(f) \u02c6f(S)2 \u22641\n2 along with the assumption that f is balanced.\nNow summing the right hand side of (8) gives us Pn\ni=1 Infi(f)3/2 \u2264\u03b1 \u00b7 Inf(f). Combining\nboth inequalities yields 3 \u00b7\n\u0000 1\n9\n\u0001Inf(f) \u2264\u03b11/2, and therefore Inf(f) = \u2126(log(1/\u03b1)) as claimed.\nCorollary 47 Let f : {\u22121, 1}n \u2192{\u22121, 1} with E[f] = 0.\nThen maxi\u2208[n] {Infi(f)} =\n\u2126\n\u0000log(n)\nn\n\u0001\n.\nThis bound on the maximum in\ufb02uence is tight for the Ben-Or Linial TRIBES function\n[BL89]: the 2k-way OR of k-way AND\u2019s of disjoint sets of variables (so n = k \u00b7 2k). We\nremark that while Corollary 47 only gives a log(n) improvement over the 1/n bound that\nfollows directly from the Poincar\u00b4e inequality, this factor makes a crucial di\ufb00erence in many\napplications (e.g.\nit is the crux of Khot and Vishnoi\u2019s [KV05] counter-example to the\nGoemans-Linial conjecture [Goe97, Lin02]).\nCorollary 48 Let f : {\u22121, 1}n \u2192{\u22121, 1} be a balanced, monotone function viewed as a\nvoting scheme. Both candidates can bias the outcome of the election in their favor to 99%\nprobability by bribing a O\n\u00001\nlog(n)\n\u0001\nfraction of voters.\n21\n3.3\nDictator versus Quasirandom tests\nDe\ufb01nition 49 (noisy in\ufb02uence) Let f : {\u22121, 1}n \u2192R and \u03c1 \u2208[0, 1]. The i-th \u03c1-noisy\nin\ufb02uence of f is Inf(\u03c1)\ni (f) := Stab\u03c1(Dif) = P\nS\u220bi \u03c1|S|\u22121 \u02c6f(S)2.\nNote that Inf(1)\ni (f) = Infi(f) and Inf(0)\ni (f) = \u02c6f(i)2, and intermediate values of \u03c1 \u2208(0, 1)\ninterpolates between these two extremes \u2013 the larger the value of \u03c1 is, the more the weight\n\u02c6f(S)2 on larger sets S is dampened by the attenuating factor \u03c1|S|\u22121.\nDe\ufb01nition 50 (quasirandom) Let f : {\u22121, 1}n \u2192R and \u03b5, \u03b4 \u2208[0, 1]. We say that f is\n(\u03b5, \u03b4)-quasirandom, or f has (\u03b5, \u03b4)-small noisy in\ufb02uences, if Inf(1\u2212\u03b4)\ni\n(f) \u2264\u03b5 for all i \u2208[n].\nA few prototypical quasirandom functions are the constants \u00b11 (these are (0, 0)-quasirandom),\nthe majority function ((O( 1\n\u221an), 0)-quasirandom), and large parities \u03c7S ((1 \u2212\u03b4)|S|\u22121, 0)-\nquasirandom). Unbiased juntas, and dictators in particular, are prototypical examples of\nfunctions far from quasirandom. The next proposition states that even functions far from\nbeing quasirandom can only have a small number of variables with large noisy in\ufb02uence:\nProposition 51 Let f : {\u22121, 1}n \u2192R with Var(f) \u22641, and let J = {i \u2208[n] : Inf(1\u2212\u03b4)\ni\n(f) \u2265\n\u03b5} be the set of coordinates with large noisy in\ufb02uences. Then |J| \u22641/\u03b5\u03b4.\nProof. We \ufb01rst note that\n\u03b5 \u00b7 |J| \u2264\nX\ni\u2208J\nInf(1\u2212\u03b4)\ni\n(f) \u2264\nn\nX\ni=1\nInf(1\u2212\u03b4)\ni\n(f) =\nX\n|S|\u22651\n|S| \u00b7 (1 \u2212\u03b4)|S|\u22121 \u02c6f(S)2.\nIt remains to check that |S| \u00b7 (1 \u2212\u03b4)|S|\u22121 \u22641/\u03b4 for any S \u2286[n]: to see this holds, note\nthat (1 \u2212\u03b4)|S|\u22121 \u2264(1 \u2212\u03b4)i\u22121 for any i \u2264|S|, and so summing over i from 1 to |S| gives\nus |S| \u00b7 (1 \u2212\u03b4)|S|\u22121 \u2264P|S|\ni=1(1 \u2212\u03b4)i\u22121 \u2264P\u221e\ni=1(1 \u2212\u03b4)i\u22121 = 1/\u03b4.\nWe have shown that\n\u03b5 \u00b7 |J| \u2264(1/\u03b4) \u00b7 Var(f) \u22641/\u03b4, and the proof is complete.\nConsider the problem of testing dictators: given blackbox access to a boolean function f, if f\nis a dictator the test accepts with probability 1, and if f is \u03b5-far from any of the n dictators it\naccepts with probability 1\u2212\u2126(\u03b5). Implicit in Kalai\u2019s proof of Arrow\u2019s impossibility theorem\n(Theorem 35) is a 3-query test that comes close to achieving this:\n3-query NAE test\n1. For each i \u2208[n], pick (xi, yi, zi) uniformly from the 6 possible NAE\ntriples.\n2. Query f on x, y, and z.\n3. Accept i\ufb00NAE(f(x), f(y), f(z)) = 1.\n22\nRecall that the NAE test accepts f with probability 3\n4 \u22123\n4 \u00b7Stab\u22121/3(f), and so if f = DICTi\nfor some i \u2208[n] (in particular, Stab\u22121/3(f) = \u22121/3) the test accepts with probability 1 (i.e.\nwe have perfect completeness). However, the NAE test does not quite satisfy the soundness\ncriterion. We saw that if the test accepts f with probability 1 \u2212\u03b5 then W1(f) \u22651 \u2212O(\u03b5),\nand by a theorem of Friedgut, Kalai and Naor, f has to O(\u03b5)-close to a dictator or an\nanti-dictator; the soundness criterion requires f to be O(\u03b5)-close to a dictator. It therefore\nremains to rule out functions that are close to anti-dictators, and we will do this using the\nBlum-Luby-Rubinfeld linearity test. Recall that the BLR test is a 3-query test that accepts\nf with probability 1 if f = \u03c7S for some S \u2286[n], and with probability 1 \u2212\u2126(\u03b5) if f is \u03b5-far\nfrom all the parity functions. Combining the BLR and NAE tests, we have a 6-query test\nfor dictatorship with perfect completeness and soundness 1 \u2212\u2126(\u03b5). In fact it is easy to\nshow that we can perform just one of the two tests, each with probability 1\n2, and reduce the\nquery complexity to 3 while only incurring a constant factor in the rejection probability.\nAs we will see on Saturday, for applications to hardness of approximation (UGC-hardness in\nparticular) it su\ufb03ces to design a test that distinguishes dictators from (\u03b5, \u03b5)-quasirandom\nfunctions, instead of one that distinguishes dictators from functions \u03b5-far from dictators.\nDe\ufb01nition 52 (DICT vs QRAND) Let 0 \u2264s < c \u22641. A (c, s) dictator versus quasiran-\ndom test is de\ufb01ned as follows. Given blackbox access to a function f : {\u22121, 1}n \u2192{\u22121, 1},\n1. The test makes O(1) non-adaptive queries to f.\n2. If f is a dictator, it accepts with probability at least c.\n3. If f is (\u03b5, \u03b5)-quasirandom, it accepts with probability at most s + o\u03b5(1).\nAs we will see, often we will need to assume that f is odd (i.e. f(\u2212x) = \u2212f(x) for all\nx \u2208{\u22121, 1}n, or equivalently, \u02c6f(S) = 0 for all even |S|). Let us consider the NAE test as a\ndictator versus quasirandom test, under the promise that f is odd. We have seen that the\ntest has perfect completeness (i.e. c = 1), and now we determine the value of s. First note\nthat since f is odd,\nPr[NAE accepts f]\n=\n3\n4 \u22123\n4 \u00b7\n\u0000W0(f) \u22121\n3W1(f) + 1\n9W2(f) \u22121\n27W3(f) + . . .\n\u0001\n=\n3\n4 \u22123\n4 \u00b7\n\u0000\u22121\n3W1(f) \u22121\n27W3(f) \u2212\n1\n243W5(f) \u2212. . .\n\u0001\n=\n3\n4 + 3\n4 \u00b7\n\u0000 1\n3W1(f) + 1\n27W3(f) +\n1\n243W5(f) + . . .\n\u0001\n=\n3\n4 + 3\n4 \u00b7 Stab1/3(f).\nNow let f be a (\u03b5, \u03b5)-quasirandom function. Applying the Majority Is Stablest theorem (we\nwill need a statement of it for functions with \u03b5-small \u03b5-noisy in\ufb02uences instead of \u03b5-small\nregular in\ufb02uences), we have\nPr[NAE accepts f]\n=\n3\n4 + 3\n4 \u00b7 Stab1/3(f)\n\u2264\n3\n4 + 3\n4 \u00b7 Stab1/3(MAJ) + o\u03b5(1)\n\u2192\n3\n4 + 3\n4 \u00b7\n\u00001 \u22122\n\u03c0 arccos(1\n3)\n\u0001\n+ o\u03b5(1)\n(11)\n=\n0.91226 . . . + o\u03b5(1),\n23\nwhere (11) uses the estimate we proved for Stab\u03c1(MAJ) using Sheppard\u2019s formula and the\ncentral limit theorem in Section 2.1.\nTherefore we have shown that the NAE test is a\n(1, 0.91226 . . .) dictator versus quasirandom test.\nWe consider two more examples of dictator versus quasirandom tests and compute their c\nand s values: the \u03c1-noise test of S. Khot, G. Kindler, E. Mossel and R. O\u2019Donnell [KKMO07],\nand J. H\u02daastad\u2019s 3XOR\u03b4 test [H\u02daas01].\nKKMO 2-query \u03c1-noise test\n1. Let \u03c1 \u2208[0, 1]. Pick x \u2208{\u22121, 1}n uniformly, and y \u223cN\u03c1(x).\n2. Query f on x and y.\n3. Accept i\ufb00f(x) = f(y).\nFirst note the \u03c1-noise test accepts f = DICTi with probability 1\n2 + 1\n2\u03c1, the probability that\nxi is not \ufb02ipped in y. For soundness, let f be an odd (\u03b5, \u03b5)-quasirandom function and note\nthat\nPr[KKMO accepts f]\n=\nE\n\u0002 1\n2 + 1\n2f(x)f(y)\n\u0003\n=\n1\n2 + 1\n2 \u00b7 Stab\u03c1(f)\n\u2264\n1\n2 + 1\n2 \u00b7 Stab\u03c1(MAJ) + o\u03b5(1)\n\u2192\n1\n2 + 1\n2 \u00b7\n\u00001 \u22122\n\u03c0 arccos(\u03c1)\n\u0001\n+ o\u03b5(1),\nwhere once again we have used the Majority Is Stablest theorem along with Sheppard\u2019s\nformula (the assumption that f is odd is used in the application of the Majority Is Stablest\ntheorem, which requires need E[f] = 0). Di\ufb00erent values of \u03c1 result in di\ufb00erent c versus s\nratios; for example, if \u03c1 = 1/\n\u221a\n2 then c = 0.85 and s = 0.75.\nFor H\u02daastad\u2019s test it will be convenient for us to view our functions as f : Fn\n2 \u2192{\u22121, 1}.\nLike in the analyses of the NAE and \u03c1-noise tests, we will have to assume that f is odd.\nH\u02daastad\u2019s 3-query 3XOR\u03b4 test\n1. Let \u03b4 \u2208[0, 1]. Pick x, y \u2208Fn\n2 uniformly and independently, and set\nz = x + y.\n2. Pick x\u2032 \u223cN1\u2212\u03b4(x).\n3. Query f on x\u2032, y, and z.\n4. Accept i\ufb00f(x\u2032)f(y)f(z) = 1.\nNote that this is identical to the BLR linearity test, except with the noisy x\u2032 instead of x.\nOnce again it is easy to see that dictators pass with probability 1\n2 + 1\n2(1 \u2212\u03b4) = 1 \u2212\u03b4\n2, and\n24\nit remains to analyze soundness:\nPr[3XOR\u03b4 accepts f]\n=\nE\nx,y,x\u2032\n\u0002 1\n2 + 1\n2f(x\u2032)f(y)f(z)\n\u0003\n=\n1\n2 + 1\n2 E\nx,y\nh\nE\nx\u2032[f(x\u2032)]f(y)f(z)\ni\n=\n1\n2 + 1\n2 E\nx,y\n\u0002\n(T1\u2212\u03b4f)(x)f(y)f(z)\n\u0003\n=\n1\n2 + 1\n2 E\nx\nh\n(T1\u2212\u03b4f)(x) E\ny [f(y)f(x + y)]\ni\n=\n1\n2 + 1\n2 E\nx\n\u0002\n(T1\u2212\u03b4f)(x)[\nf \u2217f(x)\n\u0003\n=\n1\n2 + 1\n2\nX\nS\u2286[n]\n\\\nT1\u2212\u03b4f(S)[\nf \u2217f(S) =\n1\n2 + 1\n2\nX\nS\u2286[n]\n(1 \u2212\u03b4)|S| \u02c6f(S)3.\nNote that P\nS\u2286[n](1 \u2212\u03b4)|S| \u02c6f(S)3 \u2264maxS\u2286[n]{(1 \u2212\u03b4)|S|| \u02c6f(S)|} by Parseval\u2019s. Next we claim\nthat for all \u03b5 < \u03b4, if f is (\u03b5, \u03b5)-quasirandom then (1 \u2212\u03b4)|S|| \u02c6f(S)| \u2264\u221a\u03b5 for all S with odd\ncardinality (in particular, S \u0338= \u2205). To see this, assume for the sake of contradiction that\nthere exist an S of odd cardinality for which this inequality does not hold. Then \u221a\u03b5 <\n(1 \u2212\u03b4)|S|| \u02c6f(S)| \u2264(1 \u2212\u03b5)|S|| \u02c6f(S)|, and squaring both sides gives us \u03b5 < (1 \u2212\u03b5)2|S| \u02c6f(S)2 \u2264\n(1 \u2212\u03b5)|S|\u22121 \u02c6f(S)2 \u2264Inf(1\u2212\u03b5)\ni\n(f) for all i \u2208S. Since S \u0338= \u2205, this contradicts the assumption\nthat f is (\u03b5, \u03b5)-quasirandom.\nSince the 3XOR\u03b4 test accepts (\u03b5, \u03b5)-quasirandom functions with probability at most 1\n2 + 1\n2\n\u221a\u03b5\n(i.e. s = 1\n2), we have shown that it is a (1 \u2212\u03b4\n2, 1\n2) dictator versus quasirandom test.\n25\n4\nCSPs and hardness of approximation\nThursday, 1st March 2012\n4.1\nConstraint satisfaction problems\nWe begin by noting that function testers can be viewed more generally as string testers:\nthe tester is given blackbox access to a string w \u2208{\u22121, 1}N (i.e. the truth-table of f,\nso N = 2n), and if w satis\ufb01es some property P1 \u2286{\u22121, 1}N (e.g. dictatorship) the tester\naccepts with probability say at least 2\n3, and if w satis\ufb01es some other property P2 \u2286{\u22121, 1}N\n(e.g. quasirandomness, far from dictatorship, etc.) it rejects with probability at least 2\n3.\nWe may view (non-adaptive) string testers simply as a list of instructions. For example,\nwith probability p1 query w1, w5, w10 and accept i\ufb00\u03c62(w1, w5, w10)\nwith probability p2 query w17, w4, w3 and accept i\ufb00\u03c68(w17, w4, w3)\nwith probability p3 query w2, w12, w7 and accept i\ufb00\u03c64(w2, w12, w7)\n...\nHere \u03c61, \u03c62, . . . are predicates {\u22121, 1}k \u2192{T, F}.\nFrom this point-of-view, we see that\na string tester naturally de\ufb01nes a weighted constraint satisfaction problem (CSP) over a\ndomain of N boolean variables, with the predicates \u03c6i\u2019s as constraints and the associated\npi\u2019s as weights. The question of determining which string w \u2208{\u22121, 1}N passes the test with\nhighest probability is then equivalent to the question of \ufb01nding an optimal assignment that\nsatis\ufb01es the largest weighted fraction of predicates.\nUnder this correspondence, an explicit (c, s) dictator versus quasirandom test for functions\nf : {\u22121, 1}\u2113\u2192{\u22121, 1} de\ufb01nes an explicit instance of a weighted CSP over L = 2\u2113boolean\nvariables. Since all \u2113dictators pass with probability at least c, there are \u2113special assignments\neach of which satisfy at least a c weighted fraction of constraints. Furthermore, since all\nquasirandom functions pass the test with probability at most s+o(1), any CSP assignment\nwhich is, roughly speaking, \u201cvery unlike\u201d the \u2113special assignments will satisfy at most a\ns + o(1) fraction of constraints. In other words, any CSP assignment that satis\ufb01es at least\nan s+\u2126(1) fraction of constraints must be at least \u201cslightly suggestive\u201d of at least one of the\n\u2113special assignments (we say that f is suggestive of the i-th coordinate if Inf(1\u2212\u03b5)\ni\n(f) > \u03b5,\nand in particular, an (\u03b5, \u03b5)-quasirandom function suggests none of its coordinates).\nOn Saturday Per will prove the following theorem establishing a formal connection between\ndictator versus quasirandom tests, the Unique-Label-Cover problem, and the hardness\nof approximating certain CSPs [Kho02, KR03, KKMO07, Aus08]:\nTheorem 53 Suppose there is an explicit (c, s) dictator versus quasirandom test that uses\npredicates \u03c61, . . . , \u03c6r. For every \u03b5 > 0 there exists a polynomial-time reduction where:\n26\nUnique-Label-Cover\n\u2212\u2192\nCSP with constraints \u03c61, . . . , \u03c6r\nYES instance\n\u2212\u2192\nthere exists an assignment that satis\ufb01es\na (c \u2212\u03b5) fraction of constraints\nNO instance\n\u2212\u2192\nevery assignment satis\ufb01es at most\nan (s + \u03b5) fraction of constraints\nThe Unique Games Conjecture [Kho02] asserts that approximating the Unique-Label-\nCover problem is NP-hard. Theorem 53 therefore says that assuming the UGC, for any\nconstant \u03b5 > 0 an explicit (c, s) dictator versus quasirandom test implies the NP-hardness\nof ((s/c) + \u03b5)-factor approximating CSPs with constraints corresponding to the predicates\nused by the test.\nRecall that our analyses of all three dictator versus quasirandom tests we have seen so\nfar depend on the promise that f is odd (in particular, note that the (0, 0)-quasirandom\nconstant function f \u22611 pass both the KKMO \u03c1-noise test and H\u02daastad\u2019s 3XOR\u03b4 test with\nprobability 1). One way to elide this assumption is to view them as testers for general\nfunctions g : {\u22121, 1}\u2113\u22121 \u2192{\u22121, 1} (corresponding to half of the truth table of an odd\nfunction f : {\u22121, 1}\u2113\u2192{\u22121, 1}, say for the inputs with xi = 1) where the respective\npredicates allow literals instead of just variables. Although the string w = 1N (i.e. f \u22611)\ntrivially satis\ufb01es any CSP with constraints of the form wi = wj (i.e. the KKMO \u03c1-noise\ntest) or wiwjwk = 1 (i.e. the H\u02daastad 3XOR\u03b4 test), the same is no longer true if literals are\nallowed in the constraints. Given this, we may apply Theorem 53 to the NAE test, KKMO\n\u03c1-noise test, and H\u02daastad\u2019s 3XOR\u03b4 test to conclude that under the UGC,\n\u2022 Approximating MAX-3NAE-SAT to a factor of 0.91226 . . . + \u03b5 is NP-hard.\n\u2022 Approximating MAX-2LIN to a factor of (1 \u22121\n\u03c0 arccos(\u03c1))/(1\n2 + 1\n2\u03c1) + \u03b5 is NP-hard.\n\u2022 Approximating MAX-3LIN to a factor of (1\n2 + \u03b5) is NP-hard.\n4.2\nBerry-Ess\u00b4een\nIn this section we prove the Berry-Ess\u00b4een theorem [Ber41, Ess42], a \ufb01nitary version of the\ncentral limit theorem with explicit error bounds. Actually we will give a proof that only\nyields a polynomially weaker error bound, the upshot being that the proof is relatively\nsimple and can be easily generalized to other settings (as we will see tomorrow, the Mossel-\nO\u2019Donnell-Olezkiewicz proof of the invariance principle, an extension of the Berry-Ess\u00b4een\ntheorem to low-degree polynomials, is very similar in spirit). We will need Taylor\u2019s theorem:\nLemma 54 (Taylor) Let \u03c8 be a smooth function and r \u2208N. For all x \u2208R and \u03b5 > 0\nthere exists a y \u2208[x, x + \u03b5] such that\n\u03c8(x + \u03b5) = \u03c8(x) + \u03b5\u03c8(1)(x) + 1\n2! \u00b7 \u03b52\u03c8(2)(x) + . . . +\n1\n(r\u22121)! \u00b7 \u03b5r\u22121\u03c8(r\u22121)(x) + 1\nr! \u00b7 \u03b5r\u03c8(r)(y).\nIn particular, \u03c8(x + \u03b5) = \u03c8(x) + \u03b5 \u00b7 \u03c8(1)(x) + 1\n2 \u00b7 \u03b52\u03c8(2)(x) + 1\n6 \u00b7 \u03b53\u03c8(3)(x) + error, where the\nerror term has magnitude at most \u2225\u03c8(4)\u2225\u221e\u00b7 \u03b54/24.\n27\nProposition 55 (hybrid argument) Let X1, . . . , Xn be independent random variables\nsatisfying E[Xi] = 0.\nLet \u03c32\ni := E[X2\ni ] and suppose Pn\ni=1 \u03c32\ni = 1.\nLet X = Pn\ni=1 Xi,\nG \u223cN(0, 1) and \u03c8 : R \u2192R. Then\n| E[\u03c8(X)] \u2212E[\u03c8(G)]| = O\n\u0010\n\u2225\u03c8(4)\u2225\u221e\nnP\ni=1\nE[X4\ni ]\n\u0011\n.\nNote that if each Xi is B-reasonable then Pn\ni=1 E[X4\ni ] \u2264B \u00b7 Pn\ni=1 \u03c34\ni \u2264B \u00b7 max\n\b\n\u03c32\ni\n\t\n.\nProof. We will view G as the sum of independent Gaussians G1 + . . . + Gn, where each\nGi \u223cN(0, \u03c32\ni ). The proof proceeds by a hybrid argument, showing that only a small error\nis introduced whenever each Xi in X is replaced by the corresponding Gaussian. More\nprecisely, for each i = 0, . . . , n, we de\ufb01ne the random variable Zi := G1 + . . . + Gi + Xi+1 +\n. . . + Xn; these n + 1 random variables interpolate between Z0 = X and Zn = G. We will\nprove the inequality\n| E[\u03c8(Zi\u22121)] \u2212E[\u03c8(Zi)]| = O(\u2225\u03c8(4)\u2225\u221e\u00b7 E[X4\ni ]).\nfor all i \u2208[n], noting that this implies the theorem by the triangle inequality. Fix i \u2208[n]\nand de\ufb01ne the random variable R := G1 + . . . + Gi\u22121 + Xi+1 + . . . + Xn, so Zi\u22121 = R + Xi\nand Zi = R + Gi. Our goal is therefore to bound | E[\u03c8(R + \u03c3i \u00b7 Xi)] \u2212E[\u03c8(R + \u03c3i \u00b7 Gi)]|.\nApplying Taylor\u2019s theorem twice, we get\n| E[\u03c8(Zi\u22121)] \u2212E[\u03c8(Zi)]|\n=\n\f\f\f E\n\u0002\n\u03c8(R) + Xi \u00b7 \u03c8(1)(R) + 1\n2X2\ni \u00b7 \u03c8(2)(R) + 1\n6X3\ni \u00b7 \u03c8(3)(R) + error1\n\u0003\n\u2212\nE\n\u0002\n\u03c8(R) + Gi \u00b7 \u03c8(1)(R) + 1\n2G2\ni \u00b7 \u03c8(2)(R) + 1\n6G3\ni \u00b7 \u03c8(3)(R) + error2\n\u0003\f\f\f\n=\n| E[error1 \u2212error2]|.\nHere we have used that fact that R is independent of Xi and Gi, along with the assumption\nthat Xi and Gi have matching \ufb01rst and second moments. Substituting bounds on the error\nterms error1 and error2 given by Taylor\u2019s theorem, we complete the proof:\n| E[error1 \u2212error2]| \u2264E\n\u0014\u2225\u03c8(4)\u2225\u221e\u00b7 X4\ni\n24\n+ \u2225\u03c8(4)\u2225\u221e\u00b7 G4\ni\n24\n\u0015\n= O(\u2225\u03c8(4)\u2225\u221e\u00b7 E[X4\ni ]).\nThe same proof can be rewritten to show that if Y = Y1+. . .+Yn is the sum of independent\nrandom variables satisfying E[Xi] = E[Yi], E[X2\ni ] = E[X2\ni ], and E[X3\ni ] = E[X3\ni ] (the\nmatching moments property), then | E[\u03c8(X) \u2212\u03c8(Y)]| \u2264\u2225\u03c8(4)\u2225\u221e\u00b7 Pn\ni=1 E[X4\ni ] + E[Y 4\ni ].\nLet \u03c8t : R \u2192R be the threshold function that takes value 1 if x < t, and 0 otherwise.\nOf course the 4th derivative of this function is not uniformly bounded, but note that if it\nwere the Berry-Ess\u00b4een theorem would follow as an immediate corollary of Proposition 55.\nInstead, we will use the fact that \u03c8t is well-approximated by a function which does have\na uniformly bounded 4th derivative, which then implies a slightly weaker version of the\nBerry-Ess\u00b4een theorem.\n28\nLemma 56 (smooth approximators of thresholds) Let t \u2208R and 0 < \u03bb < 1. There\nexists a function \u03c8t,\u03bb : R \u2192R with \u2225\u03c8(4)\nt,\u03bb\u2225\u221e= O(1/\u03bb4) that approximates \u03c8t in the following\nsense:\n1. \u03c8t,\u03bb(x) = \u03c8t(x) = 1 if x < t \u2212\u03bb.\n2. \u03c8t,\u03bb(x) \u2208[0, 1] if x \u2208[t \u2212\u03bb, t + \u03bb].\n3. \u03c8t,\u03bb(x) = \u03c8t(x) = 0 if x > t + \u03bb.\nWe are now ready to prove a weak version of the Berry-Ess\u00b4een theorem.\nProposition 57 (weak Berry-Ess\u00b4een) Let X1, . . . , Xn be independent, B-reasonable ran-\ndom variables satisfying E[Xi] = 0.\nLet \u03c32\ni\n:= E[X2\ni ], \u03c4 := max\n\b\n\u03c32\ni\n\t\n, and suppose\nPn\ni=1 \u03c32\ni = 1. Let S = X1 + . . . + Xn and G \u223cN(0, 1). For all t \u2208R,\n| Pr[S \u2264t] \u2212Pr[G \u2264t]| \u2264O((B\u03c4)1/5).\nProof. Since \u03c8t+\u03bb,\u03bb(x) = 1 for all x < t we have Pr[S \u2264t] \u2264E[\u03c8t+\u03bb,\u03bb(S)]. Now using the\nfact that \u2225\u03c8(4)\nt+\u03bb,\u03bb\u2225\u221e= O(1/\u03bb4), we apply Proposition 55 to get\nE[\u03c8t+\u03bb,\u03bb(S)] = E[\u03c8t+\u03bb,\u03bb(G)] \u00b1 O(B\u03c4/\u03bb4)\nSince \u03c8t+\u03bb,\u03bb(x) is at most 1 for all x \u2264t + 2\u03bb and 0 otherwise, we have\nE[\u03c8t+\u03bb,\u03bb(G)] \u2264Pr[G < t + 2\u03bb] = Pr[G < t] + O(\u03bb),\nand so combining both error bounds gives us E[\u03c8t+\u03bb,\u03bb(S)] \u2264Pr[G < t]+O(B\u03c4/\u03bb4)+O(\u03bb).\nArguing symmetrically for \u03c8t\u2212\u03bb,\u03bb(x) gives us | Pr[S < t] \u2212Pr[G < t]| = O(B\u03c4/\u03bb4) + O(\u03bb),\nand taking \u03bb = (B\u03c4)1/5 yields the claim.\n29\n5\nMajority Is Stablest\nFriday, 2nd March 2012\nOur de\ufb01nition of \u03c1-correlated Gaussians (De\ufb01nition 39) extend naturally to higher dimen-\nsions: let \u20d7G and \u20d7G\u2032 be independent standard n-dimensional Gaussians (i.e. \u20d7G = (G1, . . . , Gn)\nwhere each Gi \u223cN(0, 1) is an independent standard Gaussian, and similarly for \u20d7G\u2032). Then\n\u20d7G and \u20d7H := \u03c1 \u00b7 \u20d7G +\np\n1 \u2212\u03c12 \u00b7 \u20d7G\u2032 are \u03c1-correlated Gaussians. Just like in the one-dimension\ncase, we have E[GiHi] = \u03c1 for all i \u2208[n].\nDe\ufb01nition 58 (Gaussian noise stability) Let f : Rn \u2192R and \u03c1 \u2208[\u22121, 1]. The Gaus-\nsian noise stability of f at noise rate \u03c1 is\nGStab\u03c1(f) := E[f(\u20d7G)f( \u20d7H)],\nwhere \u20d7G, \u20d7H are \u03c1-correlated Gaussians.\nWe begin by showing that GStab\u03c1(f) = Stab\u03c1(f) for multilinear polynomials f : Rn \u2192R.\nTo see this, let f(x) = P\nS\u2286[n] cS\nQ\ni\u2208S xi and note that\nE\nh\u0010 P\nS\u2286[n]\ncS\nQ\ni\u2208S\n\u20d7Gi\n\u0011\u0010 P\nT\u2286[n]\ncT\nQ\ni\u2208T\n\u20d7Hi\n\u0011i\n=\nX\nS,T\u2286[n]\ncScT E\nh Q\ni\u2208S\nGi\nQ\ni\u2208T\nHi\ni\n=\nX\nS\u2286[n]\nc2\nS E\nh Q\ni\u2208S\nGiHi\ni\n,\nwhere we have used the independence of Gi from Gj, Hj for j \u0338= i, along with the fact that\nE[Hi] = E[Gi] = 0. Now again by independence and the fact that E[GiHi] = \u03c1 we conclude\nthat GStab\u03c1(f) = P\nS\u2286[n] \u03c1|S|c2\nS, which agrees with the formula for Stab\u03c1(f) we derived in\nProposition 31.\n5.1\nBorell\u2019s isoperimetric inequality\nTheorem 59 ([Bor85]) Let f : Rn \u2192{\u22121, 1} with E[f( \u20d7G)] = 0, where \u20d7G is a standard\nn-dimensional Gaussian. Let 0 \u2264\u03c1 \u22641. Then GStab\u03c1(f) \u22641 \u22122\n\u03c0 arccos(\u03c1).\nIn this section we present Kindler and O\u2019Donnell\u2019s recent simple proof of (a special case\nof) Borell\u2019s theorem [KO12].\nWe \ufb01rst introduce a few de\ufb01nitions and give a geometric\ninterpretation of the theorem as an isoperimetric inequality in multidimensional Gaussian\nspace.\nDe\ufb01nition 60 (rotation sensitivity) Let f : Rn \u2192{\u22121, 1} and \u03b4 \u2208[0, \u03c0]. The rotation\nsensitivity of f at \u03b4 is de\ufb01ned to be RSf(\u03b4) := Pr[f(\u20d7G) \u0338= f( \u20d7H)], where \u20d7G and \u20d7H are\ncos(\u03b4)-correlated Gaussians.\nRecall that \u20d7G and \u20d7H are cos(\u03b4)-correlated if H = cos(\u03b4) \u00b7 \u20d7G + sin(\u03b4) \u00b7 \u20d7G\u2032 where \u20d7G\u2032 is a\nstandard n-dimensional Gaussian independent of \u20d7G; we typically we think of \u03b4 as small, so\ncos(\u03b4) \u22481\u22121\n2\u03b42 is close to 1 and sin(\u03b4) \u2248\u03b4 is a small quantity. If we view f : Rn \u2192{\u22121, 1}\nas the indicator of a subset 1f of Rn, the quantity RSf(\u03b4) measures the probability that\nthe set 1f separates a random Gaussian vector G from a noisy copy of it (roughly speaking,\n30\nG with sin(\u03b4) \u00b7 \u20d7G\u2032 of noise added).\nTherefore we may think of the rotation sensitivity\nof f as a measure of the boundary size of 1f. Indeed, Kindler and O\u2019Donnell show that\nlim sup\u03b4\u21920+ RSf(\u03b4)/\u03b4 is within a constant factor of the traditional de\ufb01nition of the Guassian\nsurface area of 1f for \u201csu\ufb03ciently nice\u201d sets 1f.\nSince RSf(\u03b4) = Pr[f(\u20d7G) \u0338= f( \u20d7H)] = 1\n2 \u22121\n2 \u00b7 GStabcos(\u03b4)(f), Borell\u2019s theorem can be equiv-\nalently stated as RSf(\u03b4) \u2265\u03b4\n\u03c0 for functions f with E[f(\u20d7G)] = 0; it gives a lower bound on\nthe boundary size of sets 1f with Gaussian volume 1\n2. Sheppard\u2019s formula tells us that if\nf = sgn(a1x1 + . . . + anxn) then Pr[f(\u20d7G) \u0338= f( \u20d7H)] = 1\n\u03c0 arccos(cos(\u03b4)) = \u03b4\n\u03c0, and so Borell\u2019s\ninequality is tight when 1f is any halfspace de\ufb01ned by a hyperplane passing through the\norigin.\nKindler and O\u2019Donnell prove Borell\u2019s theorem for \u03b4 =\n\u03c0\n2\u2113where \u2113\u2208N (we may assume\n\u2113\u22652 since the inequality is trivially true for uncorrelated Gaussians). As a warm-up, we\nconsider the case of \u03b4 = \u03c0\n4 (i.e. \u2113= 2). Let f : Rn \u2192{\u22121, 1} with E[f(\u20d7G)] = 0 and note\nthat our goal is to prove RSf(\u03c0\n4 ) \u22651\n4. Let \u20d7G and \u20d7G\u2032 be independent standard n-dimensional\nGaussians and set \u20d7H =\n1\n\u221a\n2 \u00b7 \u20d7G+ 1\n\u221a\n2 \u00b7 \u20d7G\u2032. Note that (\u20d7G, \u20d7H) and (\u20d7G\u2032, \u20d7H) are both ( 1\n\u221a\n2)-correlated\nGaussians, and so we have\nPr[f(\u20d7G) \u0338= f( \u20d7H)] + Pr[f( \u20d7H) + f(\u20d7G\u2032)] = 2 \u00b7 RSf(\u03c0\n4 ).\nBy a union bound, the quantity on the left hand side is at most Pr[f(\u20d7G) \u0338= f(\u20d7G\u2032)], and\nsince f is balanced this probability is exactly 1\n2 and the proof is complete.\nWe remark\nthat an identical proof can be carried out for bounded functions f : Rn \u2192[\u22121, 1], with\nRSf(\u03b4) := E[1\n2 \u22121\n2f(\u20d7G)f( \u20d7H)] as the generalized de\ufb01nition of rotation sensitivity; all the\nsteps remain the same, except that the inequality (1\n2 \u22121\n2ac) \u2264(1\n2 \u22121\n2ab) + (1\n2 \u22121\n2bc) for all\na, b, c \u2208[\u22121, 1] will be used in place of the union bound.\nTheorem 61 ([KO12]) Let f : Rn \u2192{\u22121, 1} with E[f(\u20d7G)] = 0, where \u20d7G is a standard\nn-dimensional Gaussian. Let \u03b4 = \u03c0\n2\u2113for some \u2113\u2208N, \u2113\u22652. Then RSf(\u03b4) \u22651\n2\u2113.\nProof. Let \u20d7G and \u20d7G\u2032 be independent standard n-dimensional Gaussians. For j = 0, . . . , \u2113,\nwe de\ufb01ne the hybrid random variable \u20d7G(j) := cos(j\u03b4) \u00b7 \u20d7G + sin(j\u03b4) \u00b7 \u20d7G\u2032, and note that they\ninterpolate between \u20d7G(0) = \u20d7G and \u20d7G(\u2113) = \u20d7G\u2032. Next, we claim that \u20d7G(j\u22121) and \u20d7G(j) are cos(\u03b4)-\ncorrelated for every j \u2208[\u2113]. Since the n coordinates are independent, it su\ufb03ces to consider\nthe \ufb01rst coordinates of \u20d7G(j\u22121)\n1\nand \u20d7G(j)\n1\nand show that they are cos(\u03b4)-correlated. We write\nG(j\u22121) for \u20d7G(j\u22121)\n1\n= cos((j\u22121)\u03b4)\u00b7G+sin((j\u22121)\u03b4)\u00b7G\u2032 and G(j) for \u20d7G(j)\n1\n= cos(j\u03b4)\u00b7G+sin(j\u03b4)\u00b7G\u2032,\nand expand the expectation of their product to check that\nE\n\u0002\nG(j\u22121)G(j)\u0003\n=\ncos((j \u22121)\u03b4) cos(j\u03b4) E[GG]\n+ cos((j \u22121)\u03b4) sin(j\u03b4) E[GG\u2032]\n+ sin((j \u22121)\u03b4) cos(j\u03b4) E[G\u2032G]\n+ sin((j \u22121)\u03b4) sin(j\u03b4) E[G\u2032G\u2032]\n=\ncos((j \u22121)\u03b4) cos(j\u03b4) + sin((j \u22121)\u03b4) sin(j\u03b4)\n=\ncos((j \u22121)\u03b4 \u2212j\u03b4) = cos(\u03b4).\n31\nHere we have used the trigonometric identity cos(\u03b8)cos(\u03b8\u2032) + sin(\u03b8) sin(\u03b8\u2032) = cos(\u03b8 \u2212\u03b8\u2032).\nSince \u20d7G(j\u22121) and \u20d7G(j) are cos(\u03b4)-correlated, we apply the union bound to get 1\n2 = Pr[f(\u20d7G) \u0338=\nf(\u20d7G\u2032)] \u2264P\u2113\ni=1 Pr[f(\u20d7G(\u2113\u22121)) \u0338= f(\u20d7G(\u2113))] = \u2113\u00b7 RSf(\u03b4), and the proof is complete.\n5.2\nProof outline of MIST\nIn this section we sketch the proof of the Majority Is Stablest theorem (MIST):\nTheorem 62 ([KKMO07, MOO10]) Let f : {\u22121, 1}n \u2192{\u22121, 1} and \u03b5, \u03c1 > 0. Suppose\nE[f] = 0 and Infi(f) \u2264\u03b5 for all i \u2208[n]. Then Stab\u03c1(f) \u22641 \u22122\n\u03c0 arccos(\u03c1) + o\u03b5(1).\nStep 1. First consider T1\u2212\u03b3f for some small \u03b3 > 0. Note that\n(a) Infi(T1\u2212\u03b3f) \u2264Infi(f) \u2264\u03b5 for all i \u2208[n].\n(b) T1\u2212\u03b3f is bounded since T1\u2212\u03b3 is an averaging operator.\n(c) Stab\u03c1(f) = Stab\u03c1\u2032(T1\u2212\u03b3f) where \u03c1\u2032 :=\n\u03c1\n(1\u2212\u03b3)2 .\nStep 2. Next we truncate T1\u2212\u03b3f to get g := (T1\u2212\u03b3f)\u2264k = P\n|S|\u2264k(1 \u2212\u03b3)|S| \u02c6f(S)\u03c7S, where\nk := poly( 1\n\u03b3 ). Note that g has low-degree but it may no longer bounded. Nevertheless we\ncan say that\n\u2225(T1\u2212\u03b3f) \u2212g\u22252\n2 =\nX\n|S|>k\n(1 \u2212\u03b3)2\u00b7|S| \u02c6f(S)2 \u2264(1 \u2212\u03b3)2k X\n|S|>k\n\u02c6f(S)2 \u2264\u03b3,\nand since g is bounded this implies E[sqdist[\u22121,1](g(X1, . . . , Xn))] \u2264\u03b3. For the same reason,\nStab\u03c1\u2032(T1\u2212\u03b3f) \u2264Stab\u03c1\u2032(g) + \u03b3. Here sqdist[\u22121,1] : R \u2192R\u22650 is the function that gives the\nsquared distance to the interval [\u22121, 1]; i.e. sqdist[\u22121,1](t) = 0 if t \u2208[\u22121, 1], and (|t| \u22121)2\notherwise.\nStep 3. We apply the invariance principle (an extension of the Berry-Ess\u00b4een theorem to\nlow-degree multilinear polynomials, proved in the next section) to g and the test function\nsqdist[\u22121,1] to bound\nE[sqdist[\u22121,1](g(\u20d7G))] \u2264E[sqdist[\u22121,1](g( \u20d7X))] + poly(2k, \u03b5) = \u03b3 + poly(2k, \u03b5).\nWe are omitting a few details here since the invariance principle requires test functions to\nhave uniformly bounded 4th derivatives, just like in Berry-Ess\u00b4een, so we actually need a\nsmooth approximation of sqdist[\u22121,1].\nStep 4. Finally we consider g\u2032 : Rn \u2192[\u22121, 1], the truncation of g to the interval [\u22121, 1]; i.e.\ng\u2032(u) = g(u) if g(u) \u2208[\u22121, 1], and sgn(g(u)) otherwise. We have Stab\u03c1\u2032(g) = GStab\u03c1\u2032(g)\nsince g is multilinear, and GStab\u03c1\u2032(g\u2032) \u22641 \u22122\n\u03c0 arccos(\u03c1\u2032) by Borell\u2019s theorem (again we are\neliding some details here since g\u2032 may not satisfy E[g\u2032(G)] = 0). It remains to argue that\nGStab\u03c1\u2032(g) \u2248GStab\u03c1\u2032(g\u2032), and for this we need to de\ufb01ne the Ornstein-Uhlenbeck operator\nU\u03c1, the Gaussian analogue of the noise operator T\u03c1:\n32\nDe\ufb01nition 63 (Ornstein-Uhlenbeck) Let f : Rn \u2192R and \u03c1 \u2208[\u22121, 1]. The Ornstein-\nUhlenbeck operator U\u03c1 acts of f as follows: (U\u03c1f)(x) := E[f(\u03c1 \u00b7 x +\np\n1 \u2212\u03c12 \u00b7 \u20d7G)], where \u20d7G\nis a standard n-dimensional Gaussian.\nWith this de\ufb01nition, we may express Gaussian noise stability as GStab\u03c1(f) = E[f(\u20d7G)U\u03c1(\u20d7G)]\nand compute\n|GStab\u03c1\u2032(g) \u2212GStab\u03c1\u2032(g\u2032)|\n=\n| E[g \u00b7 U\u03c1g \u2212g\u2032 \u00b7 U\u03c1g\u2032]|\n\u2264\n| E[g \u00b7 U\u03c1g \u2212g\u2032 \u00b7 U\u03c1g]| + | E[g\u2032 \u00b7 U\u03c1g \u2212g\u2032 \u00b7 U\u03c1g\u2032]|\n=\n| E[(g \u2212g\u2032) \u00b7 U\u03c1g]| + | E[(g \u2212g\u2032) \u00b7 U\u03c1g\u2032]|\n(12)\n=\n\u0000E[(g \u2212g\u2032)2]\n\u00011/2\u0000E[(U\u03c1g)2]\n\u00011/2\n+\n\u0000E[(g \u2212g\u2032)2]\n\u00011/2\u0000E[(U\u03c1g\u2032)2]\n\u00011/2\n(13)\n\u2264\n2\n\u0000E[(g \u2212g\u2032)2]\n\u00011/2.\n(14)\nHere (12) holds since E[g\u2032 \u00b7 U\u03c1g] = E[U\u03c1g\u2032 \u00b7 g], (13) is an application of Cauchy-Schwarz,\nand (14) uses the fact that U\u03c1 is a contraction on L2. Finally we note that E[(g \u2212g\u2032)2] is\nsimply E[sqdist[\u22121,1](g)] and the proof is complete.\n5.3\nThe invariance principle\nIn this section we prove (a special case of) the Mossel-O\u2019Donnell-Olezkiewicz invariance\nprinciple [MOO10] for multilinear polynomials with low in\ufb02uences and bounded degree;\nin full generality the principle states that the distribution of such polynomials is essen-\ntially invariant for all product spaces. The crux of the proof is a low-degree analogue of\nProposition 55; once again we proceed by a hybrid argument, showing that a small error is\nintroduced whenever we replace a Rademacher random variable with a standard Gaussian.\nThis is sometimes known as the Lindeberg replacement trick, \ufb01rst appearing in Lindeberg\u2019s\nproof of the central limit theorem [Lin22]. There has been other work generalizing Linde-\nberg\u2019s argument to the non-linear case [Rot75, Rot79, Cha06], but these results either yield\nweaker error bounds or require stronger conditions (e.g. worst-case in\ufb02uences rather than\naverage-case).\nProposition 64 (hybrid argument) Let Q be a degree-d multilinear polynomial Q(u) =\nP\n|S|\u2264d cS\nQ\ni\u2208S ui and assume:\n1. The coe\ufb03cients cS \u2208R are normalized to satisfy P\nS\u0338=\u2205c2\nS = 1.\n2. We write \u03c4i to denote Infi(Q) = P\nS\u220bi c2\nS, and let \u03c4 = maxi\u2208[n] Infi(Q).\n3. \u03c8 : R \u2192R is a function satisfying |\u03c8(4)(x)| \u2264C for all x \u2208R.\n4. X = Q(X1, . . . , Xn) and Y = Q(G1, . . . , Gn) where X1, . . . , Xn are independent Rademach-\ners and G1, . . . , Gn are independent standard Gaussians.\nThen | E[\u03c8(X)] \u2212E[\u03c8(Y)]| \u2264d \u00b7 9d \u00b7 C \u00b7 \u03c4.\n33\nProof. We \ufb01rst de\ufb01ne a sequence of hybrid random variables that interpolate between X and\nY. For each i = 0, . . . , n we de\ufb01ne the random variable Zi = Q(G1, . . . , Gi, Xi+1, . . . , Xn),\nand note that Z0 = X and Zn = Y. As before, it su\ufb03ces to prove\n| E[\u03c8(Zi\u22121)] \u2212E[\u03c8(Zi)]| \u2264C \u00b7 9d \u00b7 \u03c4 2\ni\n(15)\nfor all i \u2208[n]. Note that the overall claim follows from the above by telescoping, the triangle\ninequality, and the fact that\nn\nX\ni=1\n\u03c4 2\ni \u2264\u03c4 \u00b7\nn\nX\ni=1\n\u03c4i = \u03c4 \u00b7\nn\nX\ni=1\nX\nS\u220bi\nc2\nS = \u03c4 \u00b7\nX\n|S|\u2264d\n|S| \u00b7 c2\nS \u2264\u03c4 \u00b7 d.\nHere in the \ufb01nal inequality we have used our assumption that the coe\ufb03cients are normalized\nto satisfy P\nS\u0338=\u2205c2\nS = Var(X) = Var(Y) = 1. It remains to prove (15). Fix i \u2208[n] and\n\ufb01rst express Q(u1, . . . , un) as the sum of two polynomials R and S, the former comprising\nall terms not containing ui, and the latter the rest with ui factored out. That is,\nQ(u1, . . . , un) = R(u1, . . . , ui\u22121, ui+1, . . . , un) + ui \u00b7 S(u1, . . . , ui\u22121, ui+1, . . . , un),\nwhere R has degree at most d, and S at most d \u22121 (note that if d = 1 then S is simply the\ncoe\ufb03cient \u03b1i of ui in the linear polynomial L). Next de\ufb01ne the random variables\nR\n=\nR(G1, . . . , Gi\u22121, Xi+1, . . . , Xn)\nS\n=\nS(G1, . . . , Gi\u22121, Xi+1, . . . , Xn),\nand note that Zi\u22121 = R + Xi \u00b7 S and Zi = R + Gi \u00b7 S. We bound | E[\u03c8(R + Xi \u00b7 S)] \u2212\nE[\u03c8(R + Gi \u00b7 S)]| by considering their Taylor expansions:\n| E[\u03c8(Zi\u22121)] \u2212E[\u03c8(Zi)]|\n=\n\f\f\f E\n\u0002\n\u03c8(R) + (Xi \u00b7 S)\u03c8(1)(R) + 1\n2(Xi \u00b7 S)2\u03c8(2)(R) + 1\n6(Xi \u00b7 S)3\u03c8(3)(R) + error1\n\u0003\n\u2212E\n\u0002\n\u03c8(R) + (Gi \u00b7 S)\u03c8(1)(R) + 1\n2(Gi \u00b7 S)2\u03c8(2)(R) + 1\n6(Gi \u00b7 S)3\u03c8(3)(R) + error2\n\u0003\f\f\f.\nNote that the \ufb01rst four terms cancel out since Xi and Gi are independent of S and R, and\nthe random variables Xi and Gi have matching \ufb01rst, second and third moments. Applying\nthe bounds on the error terms given by Taylor\u2019s theorem, we see that\n| E[error1 \u2212error2]|\n\u2264\n1\n24 \u00b7 C E[X4\ni \u00b7 S4] + 1\n24 \u00b7 C E[G4\ni \u00b7 S4]\n(16)\n=\n1\n24 \u00b7 C E[S4] + 3\n24 \u00b7 C E[S4]\n(17)\n<\nC \u00b7 E[S4].\nHere (16) uses our assumption that \u03c8(4) is uniformly bounded by C, and (17) is by inde-\npendence along with the fact that E[G4\ni ] = 3. Next, since S is a degree-d polynomial, we\nmay apply Bonami\u2019s lemma (Theorem 43) to get C \u00b7 E[S4] \u2264C \u00b7 9d \u00b7 E[S2]2 and it remains\n34\nto argue that E[S2] upper bounded by \u03c4i = Infi(Q) = P\nS\u220bi c2\nS. Indeed, recall that S is the\npolynomial comprising all terms of Q containing ui with ui factored out, and so\nE[S2] = E\nh\u0000 P\nS\u220bi\ncS\nQ\nj\u2208S\\i\nYj\n\u00012i\n=\nX\nS\u220bi\nc2\nS = \u03c4i,\nwhere each Yj is either a Rademacher or standard Gaussian random variable, depending on\nwhether j < i. We have shown that | E[\u03c8(Zi\u22121)] \u2212E[\u03c8(Zi)]| \u2264C \u00b7 9d \u00b7 \u03c4 2\ni , and the proof is\ncomplete.\nIn the proof of the Berry-Ess\u00b4een theorem we needed the fact that the anti-concentration of a\nstandard Gaussian G at radius \u03b5 is O(\u03b5). That is, for all t \u2208R we have Pr[|G\u2212t| < \u03b5] = O(\u03b5).\nThe low-degree analogue of this small ball probability is given by the following proposition\ndue to Carbery and Wright [CW01, Kan11].\nLemma 65 (Carbery-Wright) There exists a universal constant C such that the follow-\ning holds.\nLet Q be a multilinear polynomial of degree d over G1, . . . , Gn, a sequence of\nindependent standard Gaussians, and \u03b5 > 0. Then\nPr[|Q(G1, . . . , Gn)| \u2264\u03b5] \u2264C \u00b7 d \u00b7 (\u03b5/\u2225Q(G)\u22252)1/d.\nIn particular, if the coe\ufb03cients of Q are normalized to satisfy Var(Q) = 1 then for all t \u2208R\nand \u03b5 > 0 we have Pr[|Q(G1, . . . , Gn) \u2212t| \u2264\u03b5] = O(d \u00b7 \u03b51/d).\nLemma 66 (smooth test functions) Let r \u22652 be an integer. There exists a constant\nBr such that for all 0 < \u03bb \u22641/2 and t \u2208R there exists a function \u2206\u03bb,t : R \u2192R satisfying\nthe following:\n1. \u2206\u03bb,t is smooth and \u2225(\u2206\u03bb,t)(r)\u2225\u221e\u2264Br \u00b7 \u03bb\u2212r.\n2. \u2206\u03bb,t(x) = 1 for all x \u2264t \u22122\u03bb.\n3. \u2206\u03bb,t(x) \u2208[0, 1] for all x \u2208(t \u22122\u03bb, t + 2\u03bb).\n4. \u2206\u03bb,t(x) = 0 for all x \u2265t + 2\u03bb.\nWe are now ready to prove the invariance principle.\nTheorem 67 (invariance) Let Q(u1, . . . , un) = P\nS\u2286[n] cS\nQ\ni\u2208S ui be a degree-d multilin-\near polynomial and assume\n1. The coe\ufb03cients cS \u2208R are normalized to satisfy P\nS\u0338=\u2205c2\nS = 1.\n2. We write \u03c4i to denote Infi(Q) = P\nS\u220bi c2\nS, and let \u03c4 = maxi\u2208[n] Infi(Q).\n3. X = Q(X1, . . . , Xn) and Y = Q(G1, . . . , Gn) where X1, . . . , Xn are independent Rademach-\ners and G1, . . . , Gn are independent standard Gaussians.\nThen for all t \u2208R, | Pr[Q(X1, . . . , Xn) \u2264t]\u2212Pr[Q(G1, . . . , Gn) \u2264t]| = O\n\u0000d\u00b7(10d\u00b7\u03c4)1/(4d+1)\u0001\n.\n35\nProof. Let t \u2208R. We will write Q(X) to denote Q(X1, . . . , Xn), Q(G) for Q(G1, . . . , Gn),\nand \u03c8 for \u2206\u03bb,t+2\u03bb (r = 4), for some value of \u03bb > 0 to be determined later. Recall that\n\u03c8(x) = 1 for all x \u2265(t + 2\u03bb) \u22122\u03bb = t and so Pr[Q(X) \u2264t] \u2264E[\u03c8(Q(X))]. Since \u03c8(4) is\nuniformly bounded by O(1/\u03bb4), we apply Proposition 64 to get that\nE[\u03c8(Q(X))]\n\u2264\nE[\u03c8(Q(G))] + O(10d \u00b7 \u03c4 \u00b7 \u03bb\u22124).\n\u2264\nPr[Q(G) \u2264t + 4\u03bb] + O(10d \u00b7 \u03c4 \u00b7 \u03bb\u22124).\n(18)\n=\nPr[Q(G) \u2264t] + Pr[Q(G) \u2208(t, t + 4\u03bb)] + O(10d \u00b7 \u03c4 \u00b7 \u03bb\u22124)\n=\nPr[Q(G) \u2264t] + O(d \u00b7 (4\u03bb)1/d) + O(10d \u00b7 \u03c4 \u00b7 \u03bb\u22124).\n(19)\nHere (18) is again by the properties of \u03c8, this time using the fact that \u03c8(x) = 0 for all\nx \u2265(t + 2\u03bb) + 2\u03bb, and (19) is by Carbery-Wright. Choosing \u03bb = (10d \u00b7 \u03c4)d/(4d+1), we have\nshown that\nE[\u03c8(Q(X))] \u2264Pr[Q(G) \u2264t] + O\n\u0000d \u00b7 (10d \u00b7 \u03c4)1/(4d+1)\u0001\n.\nA symmetric argument establishes the analogous lower bound on E[\u03c8(Q(X))], and this\ncompletes the proof.\n36\n6\nTesting dictators and UGC-hardness\nSaturday, 3rd March 2012\nGuest lecture by Per Austrin\nDe\ufb01nition 68 (unique label cover) Let L be a positive integer. An instance \u03a8 of the\nL-Unique-Label-Cover problem is a graph G = (V, E) where each edge e \u2208E has an\nassociated constraint that is a permutation \u03c0e : [L] \u2192[L]. A labelling of \u03a8 is a assignment\nto the vertices \u2113: V \u2192[L]. We say that an edge (x, y) is satis\ufb01ed by \u2113if \u2113(x) = \u03c0e(\u2113(y)),\nand the value of \u2113is the fraction of edges satis\ufb01ed by \u2113. The optimum of \u03a8, denoted opt(\u03a8),\nis the maximum value of the optimal assignment \u2113.\nThe Unique-Label-Cover problem is a special case of the Label-Cover problem where\nthe constraints \u03c0e : [L] \u2192[L] are not required to be permutations. In particular, in the\nUnique-Label-Cover problem assigning a label to a vertex necessarily determines the\nlabels of all its neighbors, whereas this is not the case for the Label-Cover problem.\nConsequently, for Unique-Label-Cover the task of deciding whether there is an assign-\nment that satis\ufb01es all the edges (i.e.\ndistinguishing opt(\u03a8) = 1 versus opt(\u03a8) < 1) is\neasy: assume a label for a vertex v and deduce the labels for the remaining vertices in a\nbreadth-\ufb01rst fashion. If there is a con\ufb02ict at some vertex we choose another label for v and\nrepeat the same process. If no consistent labeling can be found after iterating through all\nL possible labels for v then opt(\u03a8) < 1; otherwise opt(\u03a8) = 1. This is in sharp contrast to\nthe situation for Label-Cover: it is known that for every \u03b5 > 0 there is an L such that it\nis NP-hard to distinguish between opt(\u03a8) = 1 versus opt(\u03a8) < \u03b5 where \u03a8 is an instance of\nL-Label-Cover; we sometimes refer to this as the (1, \u03b5)-hardness of Label-Cover.\nThe Unique Games Conjecture of S. Khot [Kho02] asserts that the Unique-Label-Cover\nproblem is nevertheless very hard to approximate as soon as we move to almost-satis\ufb01able\ninstances.\nConjecture 69 (unique games) For every \u03b5 > 0 there exists an L such that the it is\nNP-hard to distinguish between opt(\u03a8) \u22651 \u2212\u03b5 versus opt(\u03a8) < \u03b5, where \u03a8 is an instance\nof L-Unique-Label-Cover. Equivalently, for every \u03b5 > 0 there exists an L such that the\nL-Unique-Label-Cover problem is (1 \u2212\u03b5, \u03b5)-hard.\nRecent work of S. Arora, B. Barak and D. Steurer [ABS10] gives an algorithm for Unique-\nLabel-Cover running in time exp(npoly(\u03b5)).\nToday we will prove the following theorem showing how explicit (c, s) dictator versus quasir-\nandom tests yield Unique Games-based hardness results for certain constraint satisfaction\nproblems [Kho02, KR03, KKMO07, Aus08]:\nTheorem 70 Suppose we have a (c, s) dictator versus quasirandom using predicates from\na set T. For every L there exist a polynomial time reduction R from L-Unique-Label-\nCover to MAX-CSP(T) such that for every instance \u03a8 of L-Unique-Label-Cover and\nevery \u03b5 > 0, there exists a \u03b4 > 0 satisfying\n37\n1. (Completeness) If opt(\u03a8) \u22651 \u2212\u03b4 then opt(R(\u03a8)) \u2265c \u2212\u03b5.\n2. (Soundness) If opt(\u03a8) < \u03b4 then opt(R(\u03a8)) < s + \u03b5.\nFirst, a small catch:\nthe dictator versus quasirandom test have to work not only for\nboolean functions but also for bounded functions f : {\u22121, 1}n \u2192[\u22121, 1]. We may view\nany predicate \u03c6 : {\u22121, 1}k \u2192{0, 1} as \u03c6\u2217: [\u22121, 1]k \u2192[0, 1], where \u03c6\u2217(y1, . . . , yk) :=\nE[\u03c6(x1, . . . , xk)], the expectation taken with respect to {\u22121, 1}-valued random variables\nxi satisfying E[xi] = yi. It is easy to check that \u03c6\u2217(x) = \u03c6(x) for all x \u2208{\u22121, 1}k, and\nin fact we have \u03c6\u2217(y1, . . . , yk) = P\nS\u2286[k] \u02c6\u03c6(S) Q\ni\u2208S yi.\nWith this observation any tester\nfor boolean functions using predicate \u03c6 can be extended to one for all bounded func-\ntions: instead of accepting i\ufb00\u03c6(f(x1), . . . , f(xk)) = 1, the tester accepts with probability\n\u03c6\u2217(f(x1), . . . , f(xk)) \u2208[0, 1].\nThe reduction from Unique-Label-Cover to MAX-CSP\nWith this caveat out of the way, we are now ready to describe the reduction R:\nLet \u03a8 be an instance of L-Unique-Label-Cover de\ufb01ned over a graph\nG = (V, E). Suppose we have a (c, s) dictator versus quasirandom test\nfor functions {\u22121, 1}L \u2192[\u22121, 1] using k-ary predicates from a set T.\nConsider the following instance R(\u03a8) of MAX-CSP(T):\nVariables. There will be |V | \u00b7 2L variables: for each u \u2208V we de\ufb01ne\n2L boolean variables {Zu,x : x \u2208{\u22121, 1}L}.\nConstraints. A random constraint will be sampled as follows:\n1. Pick u \u2208V uniformly.\n2. Pick k neighbors v1, . . . , vk \u2208N(u) of u uniformly independently.\n3. De\ufb01ne g\nfu,vi := fvi(x \u25e6\u03c0u,vi).\n4. Pick x1, . . . , xk \u2208{\u22121, 1}L according to the distribution over k-\ntuples induced by the tester, and set yi := g\nfu,vi(xi).\n5. Return the constraint \u03c6(y1, . . . , yk) = 1.\nIn step 3, \u03c0u,vi is the permutation associated with the edge (u, vi) \u2208E, and x \u25e6\u03c0u,vi is\nthe string x with its coordinates permuted according to \u03c0u,vi. For each u \u2208V , it will be\nconvenient for us to think of an assignment to the corresponding 2L variables of the CSP as a\nboolean function fu : {\u22121, 1}L \u2192{\u22121, 1}, where Zu,x \u2190fu(x); an assignment to all |V |\u00b72L\nvariables can then be de\ufb01ned as a set of |V | boolean functions {fu : {\u22121, 1}L \u2192{\u22121, 1}}u\u2208V .\nWe will assume that G is regular; this is without loss of generality by a result of Khot and\nRegev [KR03].\n38\nCompleteness\nSuppose opt(\u03a8) \u22651 \u2212\u03b4, and let \u2113: V \u2192[L] be a labelling achieving this. Our goal is to\nexhibit an assignment to the variables of the CSP R(\u03a8) that satis\ufb01es at least a c\u2212\u03b5 fraction\nof constraints. We consider the fraction satis\ufb01ed by the assignment fu(x) := DICT\u2113(u)(x) =\nx\u2113(u) for all u \u2208V .\nConsider an edge (u, vi) \u2208E satis\ufb01ed by \u2113(i.e. \u2113(u) = \u03c0u,vi(\u2113(vi)), and note that\ng\nfu,vi(x) = fvi(x \u25e6\u03c0u,vi) = (x \u25e6\u03c0u,vi)\u2113(vi) = x\u03c0u,vi(\u2113(vi)) = x\u2113(u) = DICT\u2113(u).\nTherefore if the k edges incident to u (chosen in step 2 above) are all satis\ufb01ed by \u2113then\nE[\u03c6(y1, . . . , yk)] is the probability that the test accepts DICT\u2113(u), at least c by our assump-\ntion. Since G is regular and \u2113satis\ufb01es a 1 \u2212\u03b4 fraction of all edges, the probability that k\nuniformly random edges incident to a random u \u2208V is satis\ufb01ed by \u2113is at least 1 \u2212k\u03b4, and\nso we have opt(R(\u03a8)) \u2265c \u00b7 (1 \u2212k\u03b4) \u2265c \u2212\u03b5 (for su\ufb03ciently small \u03b4).\nSoundness\nWe will assume that opt(R(\u03a8)) \u2265s + \u03b5 and prove opt(\u03a8) = \u2126\u03b5(1). We \ufb01rst express the\nfraction of satis\ufb01ed constraints as\ns + \u03b5 \u2264\nE\nu,v1,...,vk\nx1,...,xk\n\u0002\n\u03c6\n\u0000 g\nfu,v1(x1), . . . , g\nfu,vk(xj)\n\u0001\u0003\n=\nE\nu,x1,...,xk\n\u0002\n\u03c6\n\u0000E\nv1[ g\nfu,v1(x1)], . . . , E\nvk[ g\nfu,vk(xk)]\n\u0001\u0003\nFor each u \u2208V , let gu{\u22121, 1}L \u2192[\u22121, 1] be the function de\ufb01ned by gu(x) := Ev\u2208N(u)[g\nfu,v(x)],\nand so the above can be rewritten as Eu,x1,...,xn[\u03c6(gu(x1), . . . , gu(xk))] \u2265s + \u03b5. By an aver-\naging argument, at least an \u03b5\n2 fraction of all u \u2208V satisfy Ex1,...,xk[\u03c6(gu(x1), . . . , gu(xk))] \u2265\ns+ \u03b5\n2; call these u \u2208V \u201cgood\u201d. By the soundness condition of a dictator versus quasirandom\ntest, it follows that if u is good gu cannot be (\u03b3, \u03b3)-quasirandom for some \u03b3 = \u2126\u03b5(1); in\nparticular, there must exist at least one i \u2208[L] such that Inf(1\u2212\u03b3)\ni\n(gu) \u2265\u03b3.\nLet Ju = {i \u2208[L] : Inf(1\u2212\u03b3)\ni\n(gu) \u2265\u03b3}, and note that |Ju| \u2264\n1\n\u03b32 by Proposition 51. We claim\nthat every i \u2208Ju satis\ufb01es Prv\u2208N(u)\n\u0002\nInf(1\u2212\u03b3)\n\u03c0\u22121\nu,v(i)(fv) > \u03b3\n2\n\u0003\n> \u03b3\n2 (once again, at least one such i\nexists if u is good). To see this, it su\ufb03ces to check that\nE\nv\u2208N(u)\n\u0002\nInf(1\u2212\u03b3)\n\u03c0u,v(i)(fv)\n\u0003\n= E\nv\n\u0002\nInf(1\u2212\u03b3)\ni\n\u0000g\nfu,v\n\u0001\u0003\n\u2265Inf(1\u2212\u03b3)\ni\n\u0000E\nv\n\u0002g\nfu,v\n\u0003\u0001\n= Inf(1\u2212\u03b3)\ni\n(gu) \u2265\u03b3;\nthe claim then follows by Markov\u2019s inequality. For every u \u2208V we also de\ufb01ne J\u2032\nu = {j \u2208\n[L] : Inf(1\u2212\u03b3)\nj\n(fu) \u2265\u03b3\n2 }, noting that |J\u2032\nu| \u2264\n2\n\u03b32.\nConsider the following labelling \u2113: V \u2192[L]: for each u \u2208V , if Ju \u222aJ\u2032\nu is non-empty we\nassign u a uniformly random label in Ju \u222aJ\u2032\nu, otherwise we assign u an arbitrary label. It\nremains to prove that \u2113satis\ufb01es an \u2126\u03b5(1) fraction of edges. We have shown that for at\nleast an \u03b5\n2 \u00b7 \u03b3\n2 = \u03b5\u03b3\n4 fraction of edges (u, v) there exists an i \u2208[L] such that i \u2208Ju \u222aJ\u2032\nu\nand \u03c0\u22121\nu,v(i) \u2208Jv \u222aJ\u2032\nv. Conditioned on the existence of such an i, the edge is satis\ufb01ed if\n39\n\u2113(u) = i and \u2113(v) = \u03c0\u22121\nu,v(i) (recall that (u, v) is satis\ufb01ed if \u2113(u) = \u03c0u,v(\u2113(v))), and this\nhappens with probability at least (|Ju \u222aJ\u2032\nu||Jv \u222aJ\u2032\nv|)\u22121 \u2265\n\u0000 1\n\u03b32 + 2\n\u03b32\n\u00012 = \u03b34\n9 . We conclude\nthat opt(\u03a8) = \u2126(\u03b5\u03b3 \u00b7 \u03b34) = \u2126(\u03b5\u03b35) = \u2126\u03b5(1), and the proof is complete.\nFor further details see [Aus08].\n40\nReferences\n[ABS10] S. Arora, B. Barak, and D. Steurer. Subexponential algorithms for unique games\nand related problems. In Proceedings of the 51st IEEE Symposium on Foundations of\nComputer Science (FOCS), pp. 563\u2013572, 2010.\n[Arr50] K. Arrow. A di\ufb03culty in the concept of social welfare. J. of Political Economy, 58\n(4), pp. 328\u2013346, 1950.\n[Aus08] P. Austrin. Conditional Inapproximability and Limited Independence, Ph.D. thesis,\nRoyal Institute of Technology (KTH), 2008.\n[Bec75] W. Beckner. Inequalities in Fourier Analysis. Annals of Mathematics, 102, pp.\n159\u2013182, 1975.\n[BCH+96] M. Bellare, D. Coppersmith, J. H\u02daastad, M. Kiwi, and M. Sudan. Linearity testing\nin characteristic two. IEEE Transactions on Information Theory, 42 (6), pp. 1781\u2013\n1795, 1996.\n[Ber41] A. Berry. The accuracy of Gaussian approximation to the sum of independent\nvariates. Transactions of the American Mathematical Society, 49(1), pp. 122\u2013136, 1941.\n[BL89] M. Ben-Or and N. Linial. Collective coin \ufb02ipping. In S. Micali, editor, Randomness\nand Computation. Academic Press, 1989.\n[BLR93] M. Blum, M. Luby, and R. Rubinfeld. Self-testing/correcting with applications to\nnumerical problems. J. Comput. Syst. Sci., 47, pp. 549\u2013595, 1993. Preliminary version\nin STOC 1990.\n[Bon70] A. Bonami. \u00b4Etude des coe\ufb03cients de Fourier des fonctions de Lp(G). Ann. Inst.\nFourier, 20 (2), pp. 335\u2013402, 1970.\n[Bor85] C. Borell. Geometric bounds on the Ornstein-Uhlenbeck velocity process. Probabil-\nity Theory and Related Fields, 70 (1), pp. 1\u201313, 1985.\n[CW01] A. Carbery and J. Wright. Distributional and Lq norm inequalities for polynomials\nover convex bodies in Rn. Mathematical Research Letters, 8 3, pp. 233\u2013248, 2001.\n[Cha06] S. Chatterjee. A generalization of the Lindeberg principle. Annals Probab., 24 (6),\npp. 2061\u20132076, 2006.\n[Con85] N. de Condorcet. Essai sur l\u2019application de l\u2019analyse `a la probabilit\u00b4e des d\u00b4ecisions\nrendues `a la pluralit\u00b4e des voix. Imprimerie Royale, Paris, 1785.\n[Ess42] C.-G. Ess\u00b4een. On the Liapuno\ufb00limit of error in the theory of probability. Arkiv f\u00a8or\nmatematik, astronomi och fysik, A28, pp. 1\u201319, 1942.\n[FKN02] E. Friedgut, G. Kalai, and A. Naor. Boolean functions whose Fourier transform\nis concentrated on the \ufb01rst two levels. Adv. in Appl. Math., 29 (3), pp. 427\u2013437, 2002.\n41\n[Goe97] M. X. Goemans. Semide\ufb01nite programming in comibinatorial optimization. Math-\nematical Programming, 79, pp. 143\u2013161, 1997.\n[Gro75] L. Gross. Logarithmic Sobolev inequalities. Amer. J. Math., 97, pp. 1061\u20131083,\n1975.\n[Guy86] R. K. Guy. Any answers anent these anytical enigmas?, Amer. Math Monthly, 93,\npp. 279\u2013281, 1986.\n[H\u02daas01] J. H\u02daastad. Some optimal inapproximability results. Journal of the ACM, 48 4, pp.\n798\u2013859, 2001.\n[HK92] R. Holzman and D. Kleitman, On the product of sign vectors and unit vectors,\nCombinatorica, 12 (3), pp. 303\u2013316, 1992\n[KKL98] J. Kahn, G. Kalai, and N. Linial. The in\ufb02uence of variables on boolean functions.\nIn Proceedings of the 29th IEEE Symposium on Foundations of Computer Science\n(FOCS), pp. 68\u201380, 1988.\n[Kal02] G. Kalai. A Fourier-theoretic perspective on the Condorcet paradox and Arrow\u2019s\ntheorem. Adv. in Appl. Math., 29 (3), pp. 412\u2013426, 2002.\n[Kan11] D. Kane. On Elliptic curves, the ABC Conjecture, and Polynomial Threshold Func-\ntions, Ph.D. thesis, Harvard University, 2011.\n[Kho02] S. Khot. On the power of unique 2-prover 1-round games. In Proceedsings of the\n24th ACM Symposium on Theory of Computing (STOC), pp. 767\u2013775, 2002.\n[KR03] S. Khot and O. Regev. Vertex cover might be hard to approximate to within 2 \u2212\u03b5.\nJournal of Computer and System Sciences, 74 (3), pp. 335\u2013349, 2003.\n[KV05] S. Khot and N. Vishnoi. The unique games conjecture, integrality gap for cut prob-\nlems and the embeddability of negative type metrics into \u21131. In Proceedings of the 46th\nIEEE Symposium on Foundations of Computer Science (FOCS), pp. 53\u201363, 2005.\n[KKMO07] S. Khot, G. Kindler, E. Mossel, and R. O\u2019Donnell. Optimal inapproximability\nresults for Max-Cut and other 2-variable CSPs? SIAM Journal on Computing, 37 (1),\npp. 319\u2013357, 2007.\n[KO12] G. Kindler and R. O\u2019Donnell. Gaussian noise sensitivity and Fourier tails. In Pro-\nceedings of the 27th IEEE Conference on Computational Complexity (CCC), 2012.\n[Lin22] J. W. Lindeberg. Eine neue Herleitung des Exponentialgesetzes in der Wahrschein-\nlichkeitsrechnung, Math Z., 15, pp. 211\u2013225, 1922.\n[Lin02] N. Linial. Squared \u21132 metrics into \u21131. In J. Matou\u02c7sek, editor, Open Problems, Work-\nshop on Discrete Metric Spaces and their Algorithmic Applications. Haifa, March 2002.\n[MORS10] K. Matulef, R. O\u2019Donnell, R. Rubinfeld, and R. Servedio. Testing halfspaces.\nSIAM Journal on Computing, 39 (5), pp. 2004\u20132047, 2010. Preliminary version in\nSODA 2009.\n42\n[MOO10] E. Mossel, R. O\u2019Donnell, and K. Olezkiewicz. Noise stability of functions with\nlow in\ufb02uences: invariance and optimality. Annals of Mathematics, 171 1, pp. 295\u2013341,\n2010.\n[Rot75] V. I. Rotar\u2019. Limit theorems for multilinear forms and quasipolynomial functions.\nTeor. Verojatnost. i Primenen., 20, pp. 527-546, 1975.\n[Rot79] V. I. Rotar\u2019. Limit theorems for polylinear forms. J. Multivariate Anal., 9, pp.\n511\u2013530, 1979.\n[She99] W. Sheppard. On the application of the theory of error to cases of normal distri-\nbution and normal correlation. Phil. Trans. Royal Soc. London, Series A, 192, pp.\n101\u2013531, 1899.\n[Tal96] Michel Talagrand. How much are increasing sets positively correlated? Combina-\ntorica, 16 (2), pp. 243\u2013258, 1996.\n43\n",
        "sentence": "",
        "context": "3\n1.2\nBlum-Luby-Rubinfeld . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n1.3\nVoting and in\ufb02uence\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n1.4\nProposition 55; once again we proceed by a hybrid argument, showing that a small error is\nintroduced whenever we replace a Rademacher random variable with a standard Gaussian.\n[Arr50] K. Arrow. A di\ufb03culty in the concept of social welfare. J. of Political Economy, 58\n(4), pp. 328\u2013346, 1950.\n[Aus08] P. Austrin. Conditional Inapproximability and Limited Independence, Ph.D. thesis,\nRoyal Institute of Technology (KTH), 2008."
    },
    {
        "title": "On the degree of polynomials that approximate symmetric boolean functions (preliminary version)",
        "author": [
            "Ramamohan Paturi"
        ],
        "venue": null,
        "citeRegEx": "Paturi.,? \\Q1992\\E",
        "shortCiteRegEx": "Paturi.",
        "year": 1992,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "A comparison of uniform approximations on an interval and a finite subset thereof",
        "author": [
            "T.J. Rivlin",
            "E.W. Cheney"
        ],
        "venue": "SIAM J. Numer. Anal.,",
        "citeRegEx": "Rivlin and Cheney.,? \\Q1966\\E",
        "shortCiteRegEx": "Rivlin and Cheney.",
        "year": 1966,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Undirected connectivity in log-space",
        "author": [
            "Omer Reingold"
        ],
        "venue": "J. ACM,",
        "citeRegEx": "Reingold.,? \\Q2008\\E",
        "shortCiteRegEx": "Reingold.",
        "year": 2008,
        "abstract": "\n            We present a\n            deterministic\n            , log-space algorithm that solves st-connectivity in undirected graphs. The previous bound on the space complexity of undirected st-connectivity was log\n            4/3\n            (\u22c5) obtained by Armoni, Ta-Shma, Wigderson and Zhou (JACM 2000). As undirected st-connectivity is complete for the class of problems solvable by symmetric, nondeterministic, log-space computations (the class SL), this algorithm implies that SL = L (where L is the class of problems solvable by deterministic log-space computations). Independent of our work (and using different techniques), Trifonov (STOC 2005) has presented an\n            O\n            (log\n            n\n            log log\n            n\n            )-space, deterministic algorithm for undirected st-connectivity.\n          \n          \n            Our algorithm also implies a way to construct in log-space a\n            fixed\n            sequence of directions that guides a deterministic walk through all of the vertices of any connected graph. Specifically, we give log-space constructible universal-traversal sequences for graphs with restricted labeling and log-space constructible universal-exploration sequences for general graphs.\n          ",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Pseudorandomness for regular branching programs via fourier analysis",
        "author": [
            "Omer Reingold",
            "Thomas Steinke",
            "Salil Vadhan"
        ],
        "venue": "In APPROX-RANDOM,",
        "citeRegEx": "Reingold et al\\.,? \\Q2013\\E",
        "shortCiteRegEx": "Reingold et al\\.",
        "year": 2013,
        "abstract": "We present an explicit pseudorandom generator for oblivious, read-once,\npermutation branching programs of constant width that can read their input bits\nin any order. The seed length is $O(\\log^2 n)$, where $n$ is the length of the\nbranching program. The previous best seed length known for this model was\n$n^{1/2+o(1)}$, which follows as a special case of a generator due to\nImpagliazzo, Meka, and Zuckerman (FOCS 2012) (which gives a seed length of\n$s^{1/2+o(1)}$ for arbitrary branching programs of size $s$). Our techniques\nalso give seed length $n^{1/2+o(1)}$ for general oblivious, read-once branching\nprograms of width $2^{n^{o(1)}}$, which is incomparable to the results of\nImpagliazzo et al.Our pseudorandom generator is similar to the one used by\nGopalan et al. (FOCS 2012) for read-once CNFs, but the analysis is quite\ndifferent; ours is based on Fourier analysis of branching programs. In\nparticular, we show that an oblivious, read-once, regular branching program of\nwidth $w$ has Fourier mass at most $(2w^2)^k$ at level $k$, independent of the\nlength of the program.",
        "full_text": "arXiv:1306.3004v2  [cs.CC]  19 Jun 2013\nPseudorandomness for Regular Branching Programs\nvia Fourier Analysis\nOmer Reingold\u2217\nOmer.Reingold@microsoft.com\nThomas Steinke\u2020\ntsteinke@seas.harvard.edu\nSalil Vadhan\u2021\nsalil@seas.harvard.edu\n19 June 2013\nAbstract\nWe present an explicit pseudorandom generator for oblivious, read-once, permutation branching pro-\ngrams of constant width that can read their input bits in any order.\nThe seed length is O(log2 n),\nwhere n is the length of the branching program. The previous best seed length known for this model\nwas n1/2+o(1), which follows as a special case of a generator due to Impagliazzo, Meka, and Zuckerman\n(FOCS 2012) (which gives a seed length of s1/2+o(1) for arbitrary branching programs of size s). Our\ntechniques also give seed length n1/2+o(1) for general oblivious, read-once branching programs of width\n2no(1), which is incomparable to the results of Impagliazzo et al.\nOur pseudorandom generator is similar to the one used by Gopalan et al. (FOCS 2012) for read-once\nCNFs, but the analysis is quite di\ufb00erent; ours is based on Fourier analysis of branching programs. In\nparticular, we show that an oblivious, read-once, regular branching program of width w has Fourier mass\nat most (2w2)k at level k, independent of the length of the program.\n\u2217Microsoft Research Silicon Valley, 1065 La Avenida, Mountain View, CA.\n\u2020School of Engineering and Applied Sciences, Harvard University, 33 Oxford Street, Cambridge MA. Work done in part\nwhile at Stanford University. Supported by NSF grant CCF-1116616 and the Lord Rutherford Memorial Research Fellowship.\n\u2021School of Engineering and Applied Sciences, Harvard University, 33 Oxford Street, Cambridge MA. Work done in part when\non leave as a Visiting Researcher at Microsoft Research Silicon Valley and a Visiting Scholar at Stanford University. Supported\nin part by NSF grant CCF-1116616 and US-Israel BSF grant 2010196.\n1\nIntroduction\nA major open problem in the theory of pseudorandomness is to construct an \u201coptimal\u201d pseudorandom gen-\nerator for space-bounded computation. That is, we want an explicit pseudorandom generator that stretches\na uniformly random seed of length O(log n) to n bits that cannot be distinguished from uniform by any\nO(log n)-space algorithm (which receives the pseudorandom bits one at a time, in a streaming fashion, and\nmay be nonuniform).\nSuch a generator would imply that every randomized algorithm can be derandomized with only a constant-\nfactor increase in space (RL = L), and would also have a variety of other applications, such as in streaming\nalgorithms [15], deterministic dimension reduction and SDP rounding [29], hashing [7], hardness ampli\ufb01-\ncation [12], almost k-wise independent permutations [16], and cryptographic pseudorandom generator con-\nstructions [11].\nUnfortunately, for fooling general logspace algorithms, there has been essentially no improvement since the\nclassic work of Nisan [21], which provided a pseudorandom generator of seed length O(log2 n). Instead, a\nvariety of works have improved the seed length for various restricted classes of logspace algorithms, such\nas algorithms that use no(1) random bits [22, 23], combinatorial rectangles [9, 18, 2, 19] random walks\non graphs [24, 25], branching programs of width 2 or 3 [27, 3, 32], and regular or permutation branching\nprograms (of bounded width) [5, 6, 17, 8, 30].\nThe vast majority of these works are based on Nisan\u2019s generator or its variants by Impagliazzo, Nisan, and\nWigderson [14] and Nisan and Zuckerman [22], and show how the analysis (and hence the \ufb01nal parameters)\nof these generators can be improved for logspace algorithms that satisfy the additional restrictions. All three\nof these generators are based on recursive use of the following principle: if we consider two consecutive time\nintervals I1, I2 in a space s computation and use some randomness r to generate the pseudorandom bits fed\nto the algorithm during interval I1, then at the start of I2, the algorithm will \u2018remember\u2019 at most s bits of\ninformation about r. So we can use a randomness extractor to extract roughly |r| \u2212s almost uniform bits\nfrom r (while investing only a small additional amount of randomness for the extraction). This paradigm\nseems unlikely to yield pseudorandom generators for general logspace computations that have a seed length\nof log1.99 n (see [6]).\nThus, there is a real need for a di\ufb00erent approach to constructing pseudorandom generators for space-\nbounded computation. One new approach has been suggested in the recent work of Gopalan et al. [10],\nwhich constructed improved pseudorandom generators for read-once CNF formulas and combinatorial rect-\nangles, and hitting set generators for width 3 branching programs. Their basic generator (e.g. for read-once\nCNF formulas) works as follows: Instead of considering a \ufb01xed partition of the bits into intervals, they pseu-\ndorandomly partition the bits into two groups, assign the bits in one group using a small-bias generator [20],\nand then recursively generate bits for the second group. While it would not work to assign all the bits using\na single sample from a small-bias generator, it turns out that generating a pseudorandom partial assignment\nis a signi\ufb01cantly easier task.\nAn added feature of the Gopalan et al. generator is that its pseudorandomness properties are independent\nof the order in which the output bits are read by a potential distinguisher. In contrast, Nisan\u2019s generator\nand its variants depend heavily on the ordering of bits (the intervals I1 and I2 above cannot be interleaved),\nand in fact it is known that a particular instantiation of Nisan\u2019s generator fails to be pseudorandom if the\n(space-bounded) distinguisher can read the bits in a di\ufb00erent order [31, Corollary 3.18]. Recent works [4, 13]\nhave constructed nontrivial pseudorandom generators for space-bounded algorithms that can read their bits\nin any order, but the seed length achieved is larger than \u221an.\nIn light of the above, a natural question is whether the approach of Gopalan et al. can be extended to a\nwider class of space-bounded algorithms. We make progress on this question by using the same approach\nto construct a pseudorandom generator with seed length O(log2 n) for constant-width, read-once, oblivious\npermutation branching programs that can read their bits in any order.\nIn analysing our generator, we\n1\ndevelop new Fourier-analytic tools for proving pseudorandomness against space-bounded algorithms.\n1.1\nModels of Space-Bounded Computation\nA (layered) branching program B is a nonuniform model of space-bounded computation. The program\nmaintains a state from the set [w] = {1, . . ., w} and, at each time step i, reads one bit of its input x \u2208{0, 1}n\nand updates its state according to a transition function Bi : {0, 1} \u00d7 [w] \u2192[w]. The parameter w is called\nthe width of the program, and corresponds to a space bound of log w bits. We allow the transition function\nBi to be di\ufb00erent at each time step i. We consider several restricted forms of branching programs:\n\u2022 Read-once branching programs read each input bit at most once.\n\u2022 Oblivious branching programs choose which input bit to read depending only on the time step i,\nand not on the current state\n\u2022 Ordered branching programs (a.k.a. streaming algorithms) always read input bit i in time step i\n(hence are necessarily both read-once and oblivious).\nTo derandomize randomized space-bounded computations (e.g.\nprove RL = L), it su\ufb03ces to construct\npseudorandom generators that fool ordered branching programs of polynomial width ( w = poly(n)), and\nhence this is the model addressed by most previous constructions (including Nisan\u2019s generator). However,\nthe more general models of oblivious and read-once branching programs are also natural to study, and, as\ndiscussed above, can spark the development of new techniques for reasoning about pseudorandomness.\nAs mentioned earlier, Nisan\u2019s pseudorandom generator [21] achieves O(log2 n) seed length for ordered branch-\ning programs of polynomial width. It is known how to achieve O(log n) seed length for ordered branching\nprograms width 2 [3], and for width 3, it is only known how to construct \u201chitting-set generators\u201d (a weaker\nform of pseudorandom generators) with seed length O(log n) [32, 10]. (The seed length is \u02dcO(log n) if we want\nthe error of the hitting set generator to be subconstant.) For pseudorandom generators for width w \u22653 and\nhitting-set generators for width w \u22654, there is no known construction with seed length o(log2 n).\nThe study of pseudorandomness against non-ordered branching programs started more recently. Tzur [31]\nshowed that there are oblivious, read-once, constant-width branching programs that can distinguish the\noutput of Nisan\u2019s generator from uniform. Bogdanov, Papakonstantinou, and Wan [4] exhibited a pseudo-\nrandom generator with seed length (1 \u2212\u2126(1)) \u00b7 n for oblivious read-once branching programs of width w\nfor w = 2\u2126(n). Impagliazzo, Meka, and Zuckerman [13] gave a pseudorandom generator with seed length\ns1/2+o(1) for arbitrary branching programs of size s; note that s = O(nw) for a read-once branching program\nof width w and length n.\nWe consider two further restrictions on branching programs:\n\u2022 Regular branching programs are oblivious branching programs with the property that, if the\ndistribution on states in any layer is uniformly random and the input bit read by the program at\nthat layer is uniformly random, then the resulting distribution on states in the next layer is uniformly\nrandom. This is equivalent to requiring that the bipartite graph associated with each layer of the\nprogram, where we have edges from each state u \u2208[w] in layer i to the possible next-states u0, u1 \u2208[w]\nin layer i + 1 (if the input bit is b, the state goes to ub), is a regular graph.\n\u2022 Permutation branching programs are a further restriction, where we require that for each setting\nof the input string, the mappings between layers are permutations. This is equivalent to saying that\n(regular) bipartite graphs corresponding to each layer are decomposed into two perfect matchings, one\ncorresponding to each value of the current input bit being read.\n2\nThe fact that pseudorandomness for permutation branching programs might be easier than for general\nbranching programs was suggested by the proof that Undirected S-T Connectivity is in Logspace [24] and its\nfollow-ups [25, 26]. Speci\ufb01cally, the latter works construct \u201cpseudorandom walk generators\u201d for \u201cconsistently\nlabelled\u201d graphs. Interpreted for permutation branching programs, these results ensure that if an ordered\npermutation branching program has the property that every layer has a nonnegligible amount of \u201cmixing\u201d\n\u2014 meaning that the distribution on states becomes closer to uniform, on a truly random input \u2014 then the\noverall program will also have mixing when run on the output of the pseudorandom generator (albeit at a\nslower rate). The generator has a seed length of O(log n) even for ordered permutation branching programs\nof width poly(n). Reingold, Trevisan, and Vadhan [25] also show that if a generator with similar properties\ncould be constructed for (ordered) regular branching programs of polynomial width, then this would su\ufb03ce\nto prove RL = L. Thus, in the case of polynomial width, regularity is not a signi\ufb01cant constraint.\nRecently, there has been substantial progress on constructing pseudorandom generators for ordered regular\nand permutation branching programs of constant width. Braverman, Rao, Raz, and Yehudayo\ufb00[5] and\nBrody and Verbin [6] gave pseudorandom generators with seed length \u02dcO(log n) for ordered regular branching\nprograms of constant width. Kouck\u00b4y, Nimbhorkar and Pudl\u00b4ak [17] showed that the seed length could be\nfurther improved to O(log n) for ordered, permutation branching programs of constant width; see [8, 30] for\nsimpli\ufb01cations and improvements.\nAll of these generators for ordered regular and permutation branching programs are based on re\ufb01ned analyses\nof the pseudorandom generator construction of Impagliazzo, Nisan, and Wigderson [14].\n1.2\nOur Results and Techniques\nOur main result is a pseudorandom generator for read-once, oblivious, (unordered) permutation branching\nprograms of constant width:\nTheorem 1.1 (Main Result). For every constant w, there is an explicit pseudorandom generator G :\n{0, 1}O(log2 n) \u2192{0, 1}n fooling oblivious, read-once (but unordered), permutation branching programs of\nwidth w and length n.\nTo be precise, the seed length and space complexity of the pseudorandom generator is\nO(w2 log(w) log(n) log(nw/\u03b5) + w4 log2(w/\u03b5))\nfor oblivious, read-once, permutation branching programs of length n and width w, where \u03b5 is the error.\nPreviously, it was only known how to achieve a seed length of n1/2+o(1) for this model, as follows from\nthe aforementioned results of Impagliazzo, Meka, and Zuckerman [13] (which actually holds for arbitrary\nbranching programs).\nOur techniques also achieve seed length n1/2+o(1) for arbitrary read-once, oblivious branching programs of\nwidth up to 2no(1):\nTheorem 1.2. There is an explicit pseudorandom generator G : {0, 1} \u02dcO(\u221an log w) \u2192{0, 1}n fooling oblivious,\nread-once (but unordered) branching programs of width w and length n.\nThis result is incomparable to that of Impagliazzo et al. [13]. Their seed length depends polynomially on\nthe width w, so require width w = no(1) to achieve seed length n1/2+o(1). On the other hand, our result is\nrestricted to read-once, oblivious branching programs.\nOur construction of the generator in Theorem 1.1 is essentially the same as the generator of Gopalan et al. [10]\nfor read-once CNF formulas, but with a new analysis (and di\ufb00erent setting of parameters) for read-once,\noblivious, permutation branching programs. The generator works by selecting a subset T \u2282[n] of output\n3\ncoordinates in a pseudorandom way, assigning the bits in T using another pseudorandom distribution X,\nand then recursively assigning the bits outside T . We generate T using an almost O(log n)-wise independent\ndistribution, including each coordinate i \u2208T with a constant probability pw depending only on the width w.\nWe assign the bits in T using a small-bias distribution X on {0, 1}n [20]; such a generator has the property\nthat for every nonempty subset S \u2282[n], the parity \u2295i\u2208SXi of bits in S has bias at most \u03b5. Generating T\nrequires O(log n) random bits, generating X requires O(log n) bits (even for \u03b5 = 1/poly(n)), and we need\nO(log n) levels of recursion to assign all the bits. This gives us our O(log2 n) seed length.\nLet B : {0, 1}n \u2192{0, 1} be a function computed by an oblivious, read-once, permutation branching program\nof width w. Following [10], to show that our pseudorandom generator fools B, it su\ufb03ces to show that the\npartial assignment generated in a single level of recursion approximately preserves the acceptance probability\nof B (on average). To make this precise, we need a bit of notation. For a set t \u2282[n], a string x \u2208{0, 1}n,\nand y \u2208{0, 1}n\u2212|t|, de\ufb01ne Select(t, x, y) \u2208{0, 1}n as follows:\nSelect(t, x, y)i =\n(\nxi\nif i \u2208t\ny|{j\u2264i:j /\u2208t}|\nif i /\u2208t\nOnce we choose a set t \u2190T and an assignment x \u2190X to the variables in t, the residual acceptance probability\nof B is P\nU [B(Select(t, x, U)) = 1], where U is the uniform distribution on {0, 1}n. So, the average acceptance\nprobability over t \u2190T and x \u2190X is\nP\nT,X,U [B(Select(T, X, U)) = 1]. We would like this to be close to the\nacceptance probability under uniformly random bits, namely P\nU [B(U) = 1] =\nP\nT,U\u2032,U [B(Select(T, U \u2032, U) = 1].\nThat is, we would like our small-bias distribution X to fool the function B\u2032(x) := E\nT,U [B(Select(T, x, U))].\nThe key insight in [10] is that B\u2032 can be a signi\ufb01cantly easier function to fool than B, and even than \ufb01xed\nrestrictions of B (like B(Select(t, \u00b7, y)) for \ufb01xed t and y). We show that the same phenomenon holds for\noblivious, read-once, regular branching programs. (The reason that the analysis of our overall pseudorandom\ngenerator applies only for permutation branching programs is that regularity is not preserved under restriction\n(as needed for the recursion), whereas the permutation property is.)\nTo show that a small-bias space fools B\u2032(x), it su\ufb03ces to show that the Fourier mass of B\u2032, namely\nP\ns\u2208{0,1}n,s\u0338=0 |c\nB\u2032[s]|, is bounded by poly(n).\n(Here c\nB\u2032[s] = E\nU\n\u0002\nB\u2032[U] \u00b7 (\u22121)s\u00b7U\u0003\nis the standard Fourier\ntransform over Zn\n2. So c\nB\u2032[s] measures the correlation of B\u2032 with the parity function de\ufb01ned by s.) We show\nthat this is indeed the case (for most choices of the set t \u2190T ):\nTheorem 1.3 (Main Lemma). For every constant w, there are constants pw > 0 and dw \u2208N such that the\nfollowing holds. Let B : {0, 1}n \u2192{0, 1} be computed by an oblivious, read-once, regular branching program\nof width w and length n \u2265dw. Let T \u2282[n] be a randomly chosen set so that every coordinate i \u2208[n] is\nplaced in T with probability pw and these choices are n\u2212dw-almost (dw log n)-wise independent. Then with\nhigh probability over t \u2190T B\u2032(x) = E\nU [B(Select(t, x, U))] has Fourier mass at most ndw.\nAs a warm-up, we begin by analysing the Fourier mass in the case the set T is chosen completely at random,\nwith every coordinate included independently with probability pw. In this case, it is more convenient to\naverage over T and work with B\u2032(x) = E\nT,U [B(Select(T, x, U))]. Then it turns out that c\nB\u2032[s] = p|s|\nw \u00b7 bB[s],\nwhere |s| denotes the Hamming weight of the vector s. Thus, it su\ufb03ces to analyse the original program B\nand show that for each k \u2208{1, \u00b7 \u00b7 \u00b7 , n}, the Fourier mass of B restricted to s of weight k is at most ck\nw, where\ncw is a constant depending only on w (not on n). We prove that this is indeed the case for regular branching\nprograms:\nTheorem 1.4. Let B : {0, 1}n \u2192{0, 1} be a function computed by an oblivious, read-once, regular branching\nprogram of width w. Then for every k \u2208{1, . . . , n}, we have\nX\ns\u2208{0,1}n:|s|=k\n| bB[s]| \u2264(2w2)k.\n4\nOur proof of Theorem 1.4 relies on the main lemma of Braverman et al. [5], which intuitively says that\nin a bounded-width, read-once, oblivious, regular branching program, only a constant number of bits have\na signi\ufb01cant e\ufb00ect on the acceptance probability. More formally, if we sum, for every time step i and all\npossible states v at time i, the absolute di\ufb00erence between the acceptance probability after reading a 0 versus\nreading a 1 from state v, the total will be bounded by poly(w) (independent of n). This directly implies a\nbound of poly(w) on the Fourier mass of B at the \ufb01rst level: the correlation of B with a parity of weight 1 is\nbounded by the e\ufb00ect of a single bit on the output of B. We then bound the correlation of B with a parity\nof weight k by the correlation of a pre\ufb01x of B with a parity of weight k \u22121 times the e\ufb00ect of the remaining\nbit on B. Thus we inductively obtain the bound on the Fourier mass of B at level k.\nOur proof of Theorem 1.3 for the case of a pseudorandom restriction T uses the fact that we can decompose\nthe high-order Fourier coe\ufb03cients of an oblivious, read-once branching program B\u2032 into products of low-order\nFourier coe\ufb03cients of \u201csubprograms\u201d (intervals of consecutive layers) of B\u2032. Using an almost O(log n)-wise\nindependent choice of T enables us to control the Fourier mass at level O(log n) for all subprograms of B\u2032,\nwhich su\ufb03ces to control the total Fourier mass of B\u2032.\n1.3\nOrganization\nIn Section 2 we introduce the de\ufb01nitions and tools we use in our proof. In Section 2.1 we formally de\ufb01ne\nbranching programs and explain our view of them as matrix-valued functions. In Sections 2.3 and 2.5 we\nde\ufb01ne the matrix-valued Fourier transform and explain how we use it.\nOur results use Fourier analysis of regular branching programs to analyse pseudorandom generators. In\nSection 3, we give a bound on the low-order Fourier coe\ufb03cients of a read-once, oblivious, regular branching\nprogram (Theorem 1.4) using the main lemma of Braverman et al. [5]. This yields a result about random\nrestrictions, which we de\ufb01ne and discuss in Section 4. We extend the results about random restrictions to\npseudorandom restrictions in Section 5 and prove our main lemma (Theorem 1.3). Finally, in Section 6 we\nconstruct and analyse our pseudorandom generator, which proves the main result (Theorem 1.1).\nIn Section 7 we show how to extend our techniques to general read-once, oblivious branching programs\n(Theorem 1.2). We conclude in Section 8 by discussing directions for further work.\n2\nPreliminaries\n2.1\nBranching Programs\nWe de\ufb01ne a length-n, width-w program to be a function B : {0, 1}n \u00d7 [w] \u2192[w], which takes a start state\nu \u2208[w] and an input string x \u2208{0, 1}n and outputs a \ufb01nal state B[x](u).\nOften we think of B as having a \ufb01xed start state u0 and a set of accept states S \u2282[w]. Then B accepts\nx \u2208{0, 1}n if B[x](u0) \u2208S. We say that B computes the function f : {0, 1}n \u2192{0, 1} if f(x) = 1 if and\nonly if B[x](u0) \u2208S.\nIn our applications, the input x is randomly (or pseudorandomly) chosen, in which case a program can\nbe viewed as a Markov chain randomly taking initial states to \ufb01nal states. For each x \u2208{0, 1}n, we let\nB[x] \u2208{0, 1}w\u00d7w be a matrix de\ufb01ned by\nB[x](u, v) = 1 \u21d0\u21d2B[x](u) = v.\nFor a random variable X on {0, 1}n, we have E\nX [B[X]] \u2208[0, 1]w\u00d7w, where E\nR [f(R)] is the expectation of a\nfunction f with respect to a random variable R. Then the entry in the uth row and vth column E\nX [B[X]] (u, v)\n5\nis the probability that B takes the initial state u to the \ufb01nal state v when given a random input from the\ndistribution X\u2014that is,\nE\nX [B[X]] (u, v) = P\nX [B[X](u) = v] ,\nwhere P\nR [e(R)] is the probability of an event e with respect to the random variable R.\nA branching program reads one bit of the input at a time (rather than reading x all at once) maintaining\nonly a state in [w] = {1, 2, \u00b7 \u00b7 \u00b7 , w} at each step. We capture this restriction by demanding that the program\nbe composed of several smaller programs, as follows.\nLet B and B\u2032 be width-w programs of length n and n\u2032 respectively. We de\ufb01ne the concatenation B \u25e6B\u2032 :\n{0, 1}n+n\u2032 \u00d7 [w] \u2192[w] of B and B\u2032 by\n(B \u25e6B\u2032)[x \u25e6x\u2032](u) := B\u2032[x\u2032](B[x](u)),\nwhich is a width-w, length-(n + n\u2032) program. That is, we run B and B\u2032 on separate inputs, but the \ufb01nal\nstate of B becomes the start state of B\u2032.\nConcatenation corresponds to matrix multiplication\u2014that is,\n(B \u25e6B\u2032)[x \u25e6x\u2032] = B[x] \u00b7 B\u2032[x\u2032], where the two programs are concatenated on the left hand side and the two\nmatrices are multiplied on the right hand side.\nA length-n, width-w, ordered branching program is a program B that can be written B = B1\u25e6B2\u25e6\u00b7 \u00b7 \u00b7\u25e6Bn,\nwhere each Bi is a length-1 width-w program.\nWe refer to Bi as the ith layer of B.\nWe denote the\nsubprogram of B from layer i to layer j by Bi\u00b7\u00b7\u00b7j := Bi \u25e6Bi+1 \u25e6\u00b7 \u00b7 \u00b7 \u25e6Bj.\nGeneral read-once, oblivious branching programs (a.k.a. unordered branching programs) can be reduced\nto the ordered case by a permutation of the input bits. Formally, a read-once, oblivious branching\nprogram B is an ordered branching program B\u2032 composed with a permutation \u03c0. That is, B[x] = B\u2032[\u03c0(x)],\nwhere the ith bit of \u03c0(x) is the \u03c0(i)th bit of x\nFor a program B and an arbitrary distribution X, the matrix E\nX [B[X]] is stochastic\u2014that is,\nX\nv\nE\nX [B[X]] (u, v) = 1\nfor all u and E\nX [B[X]] (u, v) \u22650 for all u and v. A program B is called a regular program if the matrix\nE\nU [B[U]] is doubly stochastic\u2014that is, both E\nU [B[U]] and its transpose E\nU [B[U]]\u2217are stochastic. A program\nB is called a permutation program if B[x] is a permutation matrix for every x or, equivalently, B[x] is\ndoubly stochastic. Note that a permutation program is necessarily a regular program and, if both B and B\u2032\nare regular or permutation programs, then so is their concatenation.\nA regular program B has the property that the uniform distribution is a stationary distribution of the\nMarkov chain E\nU [B[U]], whereas, if B is a permutation program, the uniform distribution is stationary for\nE\nX [B[X]] for any distribution X.\nA regular branching program is a branching program where each layer Bi is a regular program and\nlikewise for a permutation branching program.\n2.2\nNorms\nWe are interested in constructing a random variable X (the output of the pseudorandom generator) such\nthat E\nX [B[X]] \u2248E\nU [B[U]], where U is uniform on {0, 1}n. Throughout we use U to denote the uniform\ndistribution.\nThe error of the pseudorandom generator will be measured by the norm of the matrix\nE\nX [B[X]] \u2212E\nU [B[U]].\n6\nFor a matrix A \u2208Rw\u00d7w, de\ufb01ne the \u03c1 operator norm of A by\n||A||\u03c1 = max\nx\n||xA||\u03c1\n||x||\u03c1\n,\nwhere \u03c1 speci\ufb01es a vector norm (usually 1, 2, or \u221enorm). De\ufb01ne the Frobenius norm of A \u2208Rw\u00d7w by\n||A||2\nFr =\nX\nu,v\nA(u, v)2 = trace(A\u2217A) =\nX\n\u03bb\n|\u03bb|2,\nwhere A\u2217is the (conjugate) transpose of A and the last sum is over the singular values \u03bb of A. Note that\n||A||2 \u2264||A||Fr for all A.\nWe almost exclusively use the Euclidean norm (||x||2 =\npP\ni x(i)2) and the corresponding spectral norm\n(||A||2 = max\u03bb |\u03bb|). This is not crucial; our results would work with any reasonable norm.\n2.3\nFourier Analysis\nLet B : {0, 1}n \u2192Rw\u00d7w be a matrix-valued function (such as given by a length-n, width-w branching\nprogram). Then we de\ufb01ne the Fourier transform of B as a matrix-valued function bB : {0, 1}n \u2192Rw\u00d7w\ngiven by\nbB[s] := E\nU [B[U]\u03c7s(U)] ,\nwhere s \u2208{0, 1}n (or, equivalently, s \u2282[n]) and\n\u03c7s(x) = (\u22121)\nP\ni x(i)\u00b7s(i) =\nY\ni\u2208s\n(\u22121)x(i).\nWe refer to bB[s] as the sth Fourier coe\ufb03cient of B. The order of a Fourier coe\ufb03cient bB[s] is |s|\u2014the\nHamming weight of s, which is the size of the set s or the number of 1s in the string s. Note that this is\nequivalent to taking the real-valued Fourier transform of each of the w2 entries of B separately, but we will\nsee below that this matrix-valued Fourier transform is nicely compatible with matrix algebra.\nFor a random variable X over {0, 1}n we de\ufb01ne its sth Fourier coe\ufb03cient as\nb\nX(s) := E\nX [\u03c7s(X)] ,\nwhich, up to scaling, is the same as taking the real-valued Fourier transform of the probability mass function\nof X. We have the following useful properties.\nLemma 2.1. Let A, B : {0, 1}n \u2192Rw\u00d7w be matrix valued functions. Let X, Y , and U be independent\nrandom variables over {0, 1}n, where U is uniform. Let s, t \u2208{0, 1}n. Then we have the following.\n\u2022 Decomposition: If C[x \u25e6y] = A[x] \u00b7 B[y] for all x, y \u2208{0, 1}n, then bC[s \u25e6t] = bA[s] \u00b7 bB[t].\n\u2022 Expectation: E\nX [B[X]] = P\ns bB[s] b\nX(s).\n\u2022 Fourier Inversion for Matrices: B[x] = P\ns bB[s]\u03c7s(x).\n\u2022 Fourier Inversion for Distributions: P\nX [X = x] = E\nU\nh\nb\nX(U)\u03c7U(x)\ni\n.\n\u2022 Convolution for Distributions: If Z = X \u2295Y , then bZ(s) = b\nX(s) \u00b7 bY (s).\n7\n\u2022 Parseval\u2019s Identity: P\ns\u2208{0,1}n\n\f\f\f\n\f\f\f bB[s]\n\f\f\f\n\f\f\f\n2\nFr = E\nU\nh\n||B[U]||2\nFr\ni\n.\n\u2022 Convolution for Matrices: If, for all x \u2208{0, 1}n, C[x] = E\nU [A[U] \u00b7 B[U \u2295x]], then bC[s] = bA[s] \u00b7 bB[s].\nThe Decomposition property is what makes the matrix-valued Fourier transform more convenient than\nseparately taking the Fourier transform of the matrix entries as done in [4]. If B is a length-n width-w\nbranching program, then, for all s \u2208{0, 1}n,\nbB[s] = bB1[s1] \u00b7 bB2[s2] \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 bBn[sn].\n2.4\nSmall-Bias Distributions\nThe bias of a random variable X over {0, 1}n is de\ufb01ned as\nbias(X) := max\ns\u0338=0 | b\nX(s)|.\nA distribution is \u03b5-biased if it has bias at most \u03b5. Note that a distribution has bias 0 if and only if it is\nuniform. Thus a distribution with small bias is an approximation to the uniform distribution. We can sample\nan \u03b5-biased distribution X on {0, 1}n with seed length O(log(n/\u03b5)) and using space O(log(n/\u03b5)) [20, 1].\nSmall-bias distributions are useful pseudorandom generators: A \u03b5-biased random variable X is indistin-\nguishable from uniform by any linear function (a parity of a subset of the bits of X). That is, for any\ns \u2282[n], we have\n\f\f\fE\nX\n\u0002L\ni\u2208s Xi\n\u0003\n\u22121/2\n\f\f\f \u22642\u03b5. Small bias distributions are known to be good pseudorandom\ngenerators for width-2 branching programs [3], but not width-3. For example, the uniform distribution over\n{x \u2208{0, 1}n : |x| mod 3 = 0} has bias 2\u2212\u2126(n), but does not fool width-3, ordered, permutation branching\nprograms.\n2.5\nFourier Mass\nWe analyse small bias distributions as pseudorandom generators for branching programs using Fourier analy-\nsis. Intuitively, the Fourier transform of a branching program expresses that program as a linear combination\nof linear functions (parities), which can then be fooled using a small-bias space.\nDe\ufb01ne the Fourier mass of a matrix-valued function B to be\nL\u03c1(B) :=\nX\ns\u0338=0\n\f\f\f\n\f\f\f bB[s]\n\f\f\f\n\f\f\f\n\u03c1 .\nAlso, de\ufb01ne the Fourier mass of B at level k as\nLk\n\u03c1(B) :=\nX\ns\u2208{0,1}n:|s|=k\n\f\f\f\n\f\f\f bB[s]\n\f\f\f\n\f\f\f\n\u03c1 .\nNote that L\u03c1(B) = P\nk\u22651 Lk\n\u03c1(B).\nThe Fourier mass is una\ufb00ected by order:\nLemma 2.2. Let B, B\u2032 : {0, 1}n \u2192Rw\u00d7w be matrix-valued functions satisfying B[x] = B\u2032[\u03c0(x)], where\n\u03c0 : [n] \u2192[n] is a permutation. Then, for all s \u2208{0, 1}n, bB[s] = c\nB\u2032[\u03c0(s)]. In particular, L\u03c1(B) = L\u03c1(B\u2032)\nand Lk\n\u03c1(B) = Lk\n\u03c1(B\u2032) for all k and \u03c1.\n8\nLemma 2.2 implies that the Fourier mass of any read-once, oblivious branching program is equal to the\nFourier mass of the corresponding ordered branching program.\nIf L\u03c1(B) is small, then B is fooled by a small-bias distribution:\nLemma 2.3. Let B be a length-n, width-w branching program. Let X be a \u03b5-biased random variable on\n{0, 1}n. For any matrix norm ||\u00b7||\u03c1, we have\n\f\f\f\n\f\f\fE\nX [B[X]] \u2212E\nU [B[U]]\n\f\f\f\n\f\f\f\n\u03c1 =\n\f\f\f\f\f\f\n\f\f\f\f\f\f\nX\ns\u0338=0\nbB[s] b\nX(s)\n\f\f\f\f\f\f\n\f\f\f\f\f\f\n\u03c1\n\u2264L\u03c1(B)\u03b5.\nIn the worst case L2(B) = 2\u0398(n), even for a length-n width-3 permutation branching program B.\nFor\nexample, the program Bmod 3 that computes the Hamming weight of its input modulo 3 has exponential\nFourier mass.\nWe show that, using \u2018restrictions\u2019, we can ensure that L\u03c1(B) is small.\n3\nFourier Analysis of Regular Branching Programs\nWe use a result by Braverman et al. [5]. The following is a Fourier-analytic reformulation of their result.\nLemma 3.1 ([5, Lemma 4]). Let B be a length-n, width-w, ordered, regular branching program. Then\nX\n1\u2264i\u2264n\n\f\f\f\n\f\f\f [\nBi\u00b7\u00b7\u00b7n[1 \u25e60n\u2212i]\n\f\f\f\n\f\f\f\n2 \u22642w2.\nBraverman et al. instead consider the sum, over all i \u2208[n] and all states u \u2208[w] at layer i, of the di\ufb00erence\nin acceptance probabilities if we run the program starting at v with a 0 followed by random bits versus a 1\nfollowed by random bits. They refer to this quantity as the weight of B. Their result can be expressed in\nFourier-analytic terms by considering subprograms Bi\u00b7\u00b7\u00b7n that are the original program with the \ufb01rst i \u22121\nlayers removed:\nX\n1\u2264i\u2264n\n\f\f\f\n\f\f\f [\nBi\u00b7\u00b7\u00b7n[1 \u25e60n\u2212i]q\n\f\f\f\n\f\f\f\n1 \u22642(w \u22121)\nfor any q \u2208{0, 1}w with P\nu q(u) = 1. (The vector q can be used to specify the accept states of B, and\nthe vth row of [\nBi\u00b7\u00b7\u00b7n[1 \u25e60n\u2212i]q is precisely the di\ufb00erence in acceptance probabilities mentioned above.) By\nsumming over all w possible q, we obtain\nX\ni\u2208[n]\nX\nu\n\f\f\f\n\f\f\f [\nBi\u00b7\u00b7\u00b7n[1 \u25e60n\u2212i](\u00b7, u)\n\f\f\f\n\f\f\f\n1 \u22642w(w \u22121).\nThis implies Lemma 3.1, as the spectral norm of a matrix is bounded by the sum of the 1-norms of the\ncolumns. For completeness, we include a direct proof of Lemma 3.1 in Appendix A.\nLemma 3.1 is similar (but not identical) to a bound on the \ufb01rst-order Fourier coe\ufb03cients of a regular\nbranching program: The term [\nBi\u00b7\u00b7\u00b7n[1 \u25e60n\u2212i] measures the e\ufb00ect of the ith bit on the output of B when we\nstart the program at layer i, whereas the ith \ufb01rst-order Fourier coe\ufb03cient bB[0i\u22121 \u25e61 \u25e60n\u2212i] measures the\ne\ufb00ect of the ith bit when we start at the \ufb01rst layer and run the \ufb01rst i \u22121 layers with random bits. This\ndi\ufb00erence allows us to use Lemma 3.1 to obtain a bound on all low-order Fourier coe\ufb03cients of a regular\nbranching program:\n9\nTheorem 3.2. Let B be a length-n, width-w, read-once, oblivious, regular branching program. Then\nLk\n2(B) :=\nX\ns\u2208{0,1}n:|s|=k\n\f\f\f\n\f\f\f bB[s]\n\f\f\f\n\f\f\f\n2 \u2264(2w2)k.\nThe key point is that the bound does not depend on n, even though we are summing\n\u0000n\nk\n\u0001\nterms.\nProof. By Lemma 2.2, we may assume that B is ordered. We perform an induction on k. If k = 0, then there\nis only one Fourier coe\ufb03cient to bound\u2014namely, bB[0n] = E\nU [B[U]]. Since E\nU [B[U]] is doubly stochastic, the\nbase case follows from the fact that every doubly stochastic matrix has spectral norm 1. Now suppose the\nbound holds for k and consider k + 1. We split the Fourier coe\ufb03cients based on where the last 1 is:\nX\ns\u2208{0,1}n:|s|=k+1\n\f\f\f\n\f\f\f bB[s]\n\f\f\f\n\f\f\f\n2\n=\nX\n1\u2264i\u2264n\nX\ns\u2208{0,1}i\u22121:|s|=k\n\f\f\f\n\f\f\f bB[s \u25e61 \u25e60n\u2212i]\n\f\f\f\n\f\f\f\n2\n=\nX\n1\u2264i\u2264n\nX\ns\u2208{0,1}i\u22121:|s|=k\n\f\f\f\n\f\f\f \\\nB1\u00b7\u00b7\u00b7i\u22121[s] \u00b7 [\nBi\u00b7\u00b7\u00b7n[1 \u25e60n\u2212i]\n\f\f\f\n\f\f\f\n2\n(by Lemma 2.1 (Decomposition))\n\u2264\nX\n1\u2264i\u2264n\nX\ns\u2208{0,1}i\u22121:|s|=k\n\f\f\f\n\f\f\f \\\nB1\u00b7\u00b7\u00b7i\u22121[s]\n\f\f\f\n\f\f\f\n2 \u00b7\n\f\f\f\n\f\f\f [\nBi\u00b7\u00b7\u00b7n[1 \u25e60n\u2212i]\n\f\f\f\n\f\f\f\n2\n\u2264\nX\n1\u2264i\u2264n\n(2w2)k \u00b7\n\f\f\f\n\f\f\f [\nBi\u00b7\u00b7\u00b7n[1 \u25e60n\u2212i]\n\f\f\f\n\f\f\f\n2\n(by the induction hypothesis)\n\u2264(2w2)k \u00b7 2w2\n(by Lemma 3.1)\n=(2w2)k+1,\nas required.\n4\nRandom Restrictions\nOur results involve restricting branching programs. However, our use of restrictions is di\ufb00erent from elsewhere\nin the literature. Here, as in [10], we use (pseudorandom) restrictions in the usual way, but we analyse them\nby averaging over the unrestricted bits. Formally, we de\ufb01ne a restriction as follows.\nDe\ufb01nition 4.1. For t \u2208{0, 1}n and a length-n branching program B, let B|t be the restriction of B to\nt\u2014that is, B|t : {0, 1}n \u2192Rw\u00d7w is a matrix-valued function given by B|t[x] := E\nU [B[Select(t, x, U)]], where\nU is uniform on {0, 1}n.\nHere Select takes a set t \u2282[n], a string x \u2208{0, 1}n, and a string y of length at least n \u2212|t| and produces a\nstring of length n given by\nSelect(t, x, y)(i) =\n\u001a\nx(i)\ni \u2208t\ny(|[i]\\t|)\ni \u2208[n]\\t\n\u001b\n.\nIntuitively, Select(t, x, y) \u2018stretches\u2019 y by \u2018skipping\u2019 the bits in t and using bits from x instead. For example,\nSelect(0101000, 1111111, 00001) = 0101001.\nThe most important aspect of restrictions is how they relate to the Fourier transform: For all B, s, and t,\nwe have c\nB|t[s] = bB[s] if s \u2282t and c\nB|t[s] = 0 otherwise. The restriction t \u2018kills\u2019 all the Fourier coe\ufb03cients\nthat are not contained in it. This means that a restriction signi\ufb01cantly reduces the Fourier mass:\n10\nLemma 4.2. Let B be a length-n, width-w program. Let T be n independent random bits each with expec-\ntation p. Then\nE\nT [L2(B|T )] =\nX\ns\u0338=0\np|s| \f\f\f\n\f\f\f bB[s]\n\f\f\f\n\f\f\f\n2 .\nProof.\nE\nT [L2(B|T )] = E\nT\n\uf8ee\n\uf8f0\nX\ns\u0338=0:s\u2282T\n\f\f\f\n\f\f\f bB[s]\n\f\f\f\n\f\f\f\n2\n\uf8f9\n\uf8fb=\nX\ns\u0338=0\nP\nT [s \u2282T ]\n\f\f\f\n\f\f\f bB[s]\n\f\f\f\n\f\f\f\n2 =\nX\ns\u0338=0\np|s| \f\f\f\n\f\f\f bB[s]\n\f\f\f\n\f\f\f\n2 .\nWe will overload notation as follows. Let B = B\u2032 \u25e6B\u2032\u2032 be a branching program, where B\u2032 has length n\u2032 and\nB\u2032\u2032 has length n\u2032\u2032 and B has length n = n\u2032 + n\u2032\u2032. For t \u2208{0, 1}n, we de\ufb01ne B\u2032|t = B\u2032|t\u2032 and B\u2032\u2032|t = B\u2032|t\u2032\u2032\nwhere t\u2032 \u2208{0, 1}n\u2032, t\u2032\u2032 \u2208{0, 1}n\u2032\u2032 and t = t\u2032 \u25e6t\u2032\u2032. Then B|t = B\u2032|t\u2032 \u25e6B\u2032\u2032|t\u2032\u2032 = B\u2032|t \u25e6B\u2032\u2032|t.\nNow we use Theorem 3.2 to prove a result about random restrictions of regular branching programs:\nProposition 4.3. Let B be a length-n, width-w, read-once, oblivious, regular branching program. Let T be\nn independent random bits each with expectation p < 1/2w2. Then\nE\nT [L2(B|T )] \u2264\n2w2 \u00b7 p\n1 \u22122w2 \u00b7 p.\nIn particular, if p \u22641/4w2, then E\nT [L2(B|T )] \u22641.\nProof. By Lemma 4.2, we have,\nE\nT [L2(B|T )] =\nX\ns\u0338=0\np|s| \u00b7\n\f\f\f\n\f\f\f bB[s]\n\f\f\f\n\f\f\f\n2\n=\nX\n1\u2264k\u2264n\npk \u00b7\nX\ns:|s|=k\n\f\f\f\n\f\f\f bB[s]\n\f\f\f\n\f\f\f\n2\n\u2264\nX\n1\u2264k\u2264n\npk \u00b7 (2w2)k\n(by Theorem 3.2)\n\u2264\nX\nk\u22651\n(2w2 \u00b7 p)k\n=\n2w2 \u00b7 p\n1 \u22122w2 \u00b7 p.\nRelation to Coin Theorem\nThe Coin Theorem of Brody and Verbin [6] shows that general (non-regular)\noblivious, read-once branching programs of width w cannot distinguish n independent and unbiased coin\n\ufb02ips from ones with bias 1/(log n)\u0398(w), and they show that this bound is the best possible. Braverman et\nal. [5] show that regular, oblivious, read-once branching programs of width w cannot distinguish coins with\nbias \u0398(1/w) from unbiased ones. (They state this result in terms of \u03b1-biased spaces.)\n11\nIf Z is n independent coin \ufb02ips with bias p and B is a branching program, then\n\f\f\f\n\f\f\fE\nZ [B[Z]] \u2212E\nU [B[U]]\n\f\f\f\n\f\f\f =\n\f\f\f\f\f\f\n\f\f\f\f\f\f\nX\ns\u0338=0\nbB[s] bZ(s)\n\f\f\f\f\f\f\n\f\f\f\f\f\f\n=\n\f\f\f\f\f\f\n\f\f\f\f\f\f\nX\ns\u0338=0\np|s| bB[s]\n\f\f\f\f\f\f\n\f\f\f\f\f\f\n.\nThus Proposition 4.3 implies a Coin Theorem showing that read-once, oblivious, regular branching programs\ncannot distinguish coins with bias p for some p = \u0398(1/w2) from unbiased ones. This Coin Theorem is weaker\nthan the Braverman et al. result, which gives p = \u0398(1/w). However, Proposition 4.3 gives more than a Coin\nTheorem, as the sum is taken outside the norm\u2014that is, we bound\nX\ns\u0338=0\np|s| \f\f\f\n\f\f\f bB[s]\n\f\f\f\n\f\f\f\n2 .\nThis distinction is important for our purposes, as it will allow us to reason about small-bias distributions\nand restrictions together.\n5\nPseudorandom Restrictions\nTo analyse our generator, we need a pseudorandom version of Proposition 4.3. That is, we need to prove\nthat, for a pseudorandom T (generated using few random bits), L2(B|T ) is small. We will generate T using\nan almost O(log n)-wise independent distribution:\nDe\ufb01nition 5.1. A random variable X on \u2126n is \u03b4-almost k-wise independent if, for any\nI = {i1, i2, \u00b7 \u00b7 \u00b7 , ik} \u2282[n] with |I| = k, the coordinates (Xi1, Xi2, \u00b7 \u00b7 \u00b7 , Xik) \u2208\u2126k are \u03b4 statistically close to\nbeing independent\u2014that is, for all T \u2282\u2126k,\n\f\f\f\f\f\f\nX\nx\u2208T\n\uf8eb\n\uf8edP\nX [(Xi1, Xi2, \u00b7 \u00b7 \u00b7 , Xik) = x] \u2212\nY\nl\u2208[k]\nP\nX [Xil = xl]\n\uf8f6\n\uf8f8\n\f\f\f\f\f\f\n\u2264\u03b4.\nWe say that X is k-wise independent if it is 0-almost k-wise independent.\nWe can sample a random variable X on {0, 1}n that is \u03b4-almost k-wise independent such that each bit has\nexpectation p = 2\u2212d using O(kd + log(1/\u03b4) + d log(nd)) random bits. See Lemma B.2 for more details.\nOur main lemma (stated informally as Theorem 1.3) is as follows.\nTheorem 5.2 (Main Lemma). Let B be a length-n, width-w, read-once, oblivious, regular branching program.\nLet T be a random variable over {0, 1}n where each bit has expectation p and the bits are \u03b4-almost 2k-wise\nindependent. Suppose p \u2264(2w)\u22122 and \u03b4 \u2264(2w)\u22124k. Then\nP\nT\n\u0002\nL2(B|T ) \u2264(2w2)k\u0003\n\u22651 \u2212n4 \u00b7 2\n2k .\nIn particular, we show that, for w = O(1), k = O(log n), and \u03b4 = 1/poly(n), we have L2(B|T ) \u2264poly(n)\nwith probability 1 \u22121/poly(n).\nFirst we show that the Fourier mass at level O(log n) is bounded by 1/n with high probability. This also\napplies to all subprograms\u2014that is,\nP\nT\n\u0002\n\u2200i, j Lk\n2(Bi\u00b7\u00b7\u00b7j|T ) \u22641/n\n\u0003\n\u22651 \u22121/poly(n).\n12\nLemma 5.3. Let B be a length-n, width-w, ordered, regular branching program. Let T be a random variable\nover {0, 1}n where each bit has expectation p and the bits are \u03b4-almost k-wise independent. If p \u2264(2w)\u22122\nand \u03b4 \u2264(2w)\u22122k, then, for all \u03b2 > 0,\nP\nT\n\u0002\n\u22001 \u2264i \u2264j \u2264n Lk\n2(Bi\u00b7\u00b7\u00b7j|T ) \u2264\u03b2\n\u0003\n\u22651 \u2212n2 2\n2k\u03b2 .\nProof. By Theorem 3.2, for all i and j,\nE\nT\n\u0002\nLk\n2(Bi\u00b7\u00b7\u00b7j|T )\n\u0003\n=\nX\ns\u2282{i\u00b7\u00b7\u00b7j}:|s|=k\nP\nT [s \u2282T ]\n\f\f\f\n\f\f\f[\nBi\u00b7\u00b7\u00b7j[s]\n\f\f\f\n\f\f\f\n2 \u2264(2w2)k(pk + \u03b4) \u22642\n2k .\nThe result now follows from Markov\u2019s inequality and a union bound.\nNow we use Lemma 5.3 to bound the Fourier mass at higher levels. We decompose high-order (k\u2032 \u22652k)\nFourier coe\ufb03cients into low-order (k \u2264k\u2032 < 2k) ones:\nLemma 5.4. Let B be a length-n, ordered branching program and t \u2208{0, 1}n. Suppose that, for all i, j,\nand k\u2032 with 1 \u2264i \u2264j \u2264n and k \u2264k\u2032 < 2k, Lk\u2032\n2 (Bi\u00b7\u00b7\u00b7j|t) \u22641/n. Then, for all k\u2032\u2032 \u2265k and all i and j,\nLk\u2032\u2032\n2 (Bi\u00b7\u00b7\u00b7j|t) \u22641/n.\nProof. Suppose otherwise and let k\u2032\u2032 be the smallest k\u2032\u2032 \u2265k such that Lk\u2032\u2032\n2 (Bi\u00b7\u00b7\u00b7j|t) > 1/n for some i and j.\nClearly k\u2032\u2032 \u22652k. So, by minimality, the result holds for k\u2032\u2032 \u2212k. Fix i and j. Now\nLk\u2032\u2032\n2 (Bi\u00b7\u00b7\u00b7j|t) =\nX\ns\u2208{0,1}j\u2212i+1:|s|=k\u2032\u2032\n\f\f\f\n\f\f\f[\nBi\u00b7\u00b7\u00b7j[s]\n\f\f\f\n\f\f\f\n2\n\u2264\nX\nl\u2208{i\u00b7\u00b7\u00b7j}\nX\ns\u2208{0,1}l\u2212i+1:|s|=k\nX\ns\u2032\u2208{0,1}j\u2212l:|s\u2032|=k\u2032\u2032\u2212k\n\f\f\f\n\f\f\f[\nBi\u00b7\u00b7\u00b7l[s] \u00b7 \\\nBl+1\u00b7\u00b7\u00b7j[s\u2032]\n\f\f\f\n\f\f\f\n2\n\u2264\nX\nl\u2208{i\u00b7\u00b7\u00b7j}\n\uf8eb\n\uf8ed\nX\ns\u2208{0,1}l\u2212i+1:|s|=k\n\f\f\f\n\f\f\f[\nBi\u00b7\u00b7\u00b7l[s]\n\f\f\f\n\f\f\f\n2\n\uf8f6\n\uf8f8\n\uf8eb\n\uf8ed\nX\ns\u2032\u2208{0,1}j\u2212l:|s\u2032|=k\u2032\u2032\u2212k\n\f\f\f\n\f\f\f \\\nBl+1\u00b7\u00b7\u00b7j[s\u2032]\n\f\f\f\n\f\f\f\n2\n\uf8f6\n\uf8f8\n\u2264\nX\nl\u2208{i\u00b7\u00b7\u00b7j}\n1\nn2\n\u22641\nn.\nSince i and j were arbitrary, this contradicts our supposition and proves the result.\nProof of Theorem 5.2. By Lemma 2.2, we may assume that B is ordered. By Lemma 5.3 and a union bound,\nP\nT\n\u0014\n\u2200k \u2264k\u2032 < 2k \u22001 \u2264i \u2264j \u2264n Lk\u2032\n2 (Bi\u00b7\u00b7\u00b7j|T ) \u22641\nn\n\u0015\n\u22651 \u2212n4 \u00b7 2\n2k .\nLemma 5.4 thus implies that\nP\nT\n\u0014\n\u2200k\u2032\u2032 \u2265k \u22001 \u2264i \u2264j \u2264n Lk\u2032\u2032\n2 (Bi\u00b7\u00b7\u00b7j|T ) \u22641\nn\n\u0015\n\u22651 \u2212n4 \u00b7 2\n2k .\nThus\nP\nT\n\uf8ee\n\uf8f0X\nk\u2032\u2032\u2265k\nLk\u2032\u2032\n2 (B|T ) \u22641\n\uf8f9\n\uf8fb\u22651 \u2212n4 \u00b7 2\n2k .\n13\nBy Theorem 3.2,\nX\n0<k\u2032<k\nLk\u2032\n2 (B|T ) \u2264\nX\n0<k\u2032<k\nLk\u2032\n2 (B) \u2264\nX\n0\u2264k\u2032\u2264k\u22121\n(2w2)k\u2032 = (2w2)k \u22121\n2w2 \u22121\n\u2264(2w2)k \u22121.\nThe result now follows.\n6\nThe Pseudorandom Generator\nOur main result Theorem 1.1 is stated more formally as follows.\nTheorem 6.1 (Main Result). There exists a pseudorandom generator family Gn,w,\u03b5 : {0, 1}sn,w,\u03b5 \u2192{0, 1}n\nwith seed length\nsn,w,\u03b5 = O(w2 log(w) log(n) log(nw/\u03b5) + w4 log2(w/\u03b5))\nsuch that, for any length-n, width-w, read-once, oblivious (but unordered), permutation branching program\nB and \u03b5 > 0,\n\f\f\f\f\n\f\f\f\f\nE\nUsn,w,\u03b5\n\u0002\nB[Gn,w,\u03b5(Usn,w,\u03b5)]\n\u0003\n\u2212E\nU [B[U]]\n\f\f\f\f\n\f\f\f\f\n2\n\u2264\u03b5.\nMoreover, Gn,w,\u03b5 can be computed in space O(sn,w,\u03b5).\nThe following lemma gives the basis of our pseudorandom generator.\nLemma 6.2. Let B be a length-n, width-w, read-once, oblivious, regular branching program. Let \u03b5 \u2208(0, 1).\nLet T be a random variable over {0, 1}n that is \u03b4-almost 2k-wise independent and each bit has expectation\np, where we require\np \u22641/4w2,\nk \u2265log2\n\u00004\u221awn4/\u03b5\n\u0001\n,\nand\n\u03b4 \u2264(2w)\u22124k.\nLet U be uniform over {0, 1}n. Let X be a \u00b5-biased random variable over {0, 1}n with \u00b5 \u2264\u03b5(2w2)\u2212k. Then\n\f\f\f\f\n\f\f\f\f\nE\nT,X,U [B[Select(T, X, U)]] \u2212E\nU [B[U]]\n\f\f\f\f\n\f\f\f\f\n2\n\u22642\u03b5.\nTheorem 5.2 says that with high probability over T , B|T = E\nU [B[Select(T, \u00b7, U)]] has small Fourier mass.\nThis implies that B|T is fooled by small bias X and thus\nE\nT,X,U [B[Select(T, X, U)]] \u2248\nE\nT,U,U\u2032 [B[Select(T, U \u2032, U)]] = E\nU [B[U]] .\nProof. For t \u2208{0, 1}n, we have\n\f\f\f\f\n\f\f\f\f E\nX,U [B[Select(t, X, U)]] \u2212E\nU [B[U]]\n\f\f\f\f\n\f\f\f\f\n2\n=\n\f\f\f\n\f\f\fE\nX [B|t[X]] \u2212E\nU [B[U]]\n\f\f\f\n\f\f\f\n2\n=\n\f\f\f\f\f\f\n\f\f\f\f\f\f\nX\ns\u0338=0\nc\nB|t[s] b\nX(s)\n\f\f\f\f\f\f\n\f\f\f\f\f\f\n2\n\u2264\nX\ns\u0338=0\n\f\f\f\n\f\f\f c\nB|t[s]\n\f\f\f\n\f\f\f\n2 | b\nX(s)|\n\u2264L2(B|t)\u00b5.\n14\nWe apply Theorem 5.2 and, with probability at least 1 \u22122 \u00b7 n4/2k over T , we have L2(B|T )\u00b5 \u2264\u03b5. Thus\n\f\f\f\f\n\f\f\f\f\nE\nT,X,U [B[Select(T, X, U)]] \u2212E\nU [B[U]]\n\f\f\f\f\n\f\f\f\f\n2\n\u2264P\nT [L2(B|T )\u00b5 > \u03b5] max\nt\n\f\f\f\f\n\f\f\f\f E\nX,U [B[Select(t, X, U)]] \u2212E\nU [B[U]]\n\f\f\f\f\n\f\f\f\f\n2\n+ P\nT [L2(B|T )\u00b5 \u2264\u03b5] \u03b5\n\u22642n4/2k \u00b7 2\u221aw + \u03b5\n\u22642\u03b5.\nNow we use the above results to construct our pseudorandom generator for a read-once, oblivious, permuta-\ntion branching program B.\nLemma 6.2 says that, if we de\ufb01ne Bt,x[y] := B[Select(t, x, y)], then\nE\nT,X,U\n\u0002\nBT,X[U]\n\u0003\n\u2248E\nU [B[U]], where T is\nalmost k-wise independent with each bit having expectation p and X has small bias. So now we need only\nconstruct a pseudorandom generator for Bt,x, which is a length-(n \u2212|t|) permutation branching program.\nThen\nE\nT,X, \u02dcU\nh\nBT,X[ \u02dcU]\ni\n\u2248\nE\nT,X,U\n\u0002\nBT,X[U]\n\u0003\n\u2248E\nU [B[U]] ,\nwhere \u02dcU is the output of the pseudorandom generator for Bt,x. We construct \u02dcU \u2208{0, 1}n\u2212|T | recursively;\neach time we recurse, the required output length is reduced to n \u2212|T | \u2248n(1 \u2212p). Thus after O(log(n)/p)\nlevels of recursion the required output length is constant.\nThe only place where the analysis breaks down for regular branching programs is when we recurse. If B\nis only a regular branching program, Bt,x may not be regular. However, if B is a permutation branching\nprogram, then Bt,x is too. Essentially, the only obstacle to generalising the analysis to regular branching\nprograms is that regular branching programs are not closed under restrictions.\nThe pseudorandom generator is formally de\ufb01ned as follows.\nAlgorithm for Gn,w,\u03b5 : {0, 1}sn,w,\u03b5 \u2192{0, 1}n.\nParameters: n \u2208N, w \u2208N, \u03b5 > 0.\nInput: A random seed of length sn,w,\u03b5.\n1. Compute appropriate values of p \u2208[1/8w2, 1/4w2], k \u2265log2\n\u00004\u221awn4/\u03b5\n\u0001\n, \u03b4 = \u03b5(2w)\u22124k, and\n\u00b5 = \u03b5(2w2)\u2212k.1\n2. If n \u2264(4 \u00b7 log2(2/\u03b5)/p)2, output n truly random bits and stop.\n3. Sample T \u2208{0, 1}n where each bit has expectation p and the bits are \u03b4-almost 2k-wise\nindependent.\n4. If |T | < pn/2, output 0n and stop.\n5. Recursively sample \u02dcU \u2208{0, 1}\u230an(1\u2212p/2)\u230b. i.e. \u02dcU = G\u230an(1\u2212p/2)\u230b,w,\u03b5(U).\n6. Sample X \u2208{0, 1}n from a \u00b5-biased distribution.\n7. Output Select(T, X, \u02dcU) \u2208{0, 1}n.\nThe analysis of the algorithm proceeds roughly as follows.\n1For the purposes of the analysis we assume that p, k, \u03b4, and \u00b5 are the same at every level of recursion. So if Gn,w,\u03b5 is being\ncalled recursively, use the same values of p, k, \u03b4, and \u00b5 as at the previous level of recursion.\n15\n\u2022 Every time we recurse, n is decreased to \u230an(1 \u2212p/2)\u230b. After O(log(n)/p) recursions, n is reduced to\nO(1). So the maximum recursion depth is r = O(log(n)/p).\n\u2022 The probability of failing because |T | < pn/2 is small by a Cherno\ufb00bound for limited independence.\n(This requires that n is not too small and, hence, step 2.)\n\u2022 The output is pseudorandom, as\nE\nU [B[Gn,w,\u03b5(U)]] =\nE\nT,X, \u02dcU\nh\nB[Select(T, X, \u02dcU)]\ni\n\u2248\nE\nT,X,U [B[Select(T, X, U)]] \u2248E\nU [B[U]] .\nThe \ufb01rst approximate equality holds because we inductively assume that \u02dcU is pseudorandom. The\nsecond approximate equality holds by Lemma 6.2.\n\u2022 The total seed length is the seed length needed to sample X and T at each level of recursion and\nO((log(1/\u03b5)/p)2) truly random bits at the last level. Sampling X requires seed length O(log(n/\u00b5)) =\nO(log(n/\u03b5)+k log(w)) and sampling T requires seed length O(k log(1/p)+log(log(n)/\u03b4)) = O(k log(w)+\nlog(log(n)/\u03b5)) so the total seed length is\nO(r \u00b7 (k log(w) + log(n/\u03b5)) + w4 log2(1/\u03b5)) = O(w2 log(w) log(n) log(nw/\u03b5) + w4 log2(1/\u03b5)).\nLemma 6.3. The probability that Gn,w,\u03b5 fails at step 4 is bounded by 2\u03b5\u2014that is, P\nT [|T | < pn/2] \u22642\u03b5.\nProof. By a Cherno\ufb00bound for limited independence (see Lemma B.1),\nP\nT [|T | < pn/2] \u2264\n\u0012\nk\u20322\n4n(p/2)2\n\u0013\u230ak\u2032/2\u230b\n+\n\u03b4\n(p/2)k\u2032 ,\nwhere k\u2032 \u22642k is arbitrary. Set k\u2032 = 2\u2308log2(1/\u03b5)\u2309. Step 2 ensures that n > (4 \u00b7 log2(2/\u03b5)/p)2 > (2k\u2032/p)2.\nThus we have\nP\nT [|T | < pn/2] \u2264\n\u0012\nk\u20322\n4(2k\u2032/p)2(p/2)2\n\u0013log2(1/\u03b5)\n+ \u03b5(2w)\u22124k\n(p/2)k\n\u22642\u03b5.\nThe following bounds the error of Gn,w,\u03b5.\nLemma 6.4. Let B be a length-n, width-w, read-once, oblivious, permutation branching program. Then\n\f\f\f\f\n\f\f\f\f\nE\nUsn,w,\u03b5\n\u0002\nB[Gn,w,\u03b5(Usn,w,\u03b5)]\n\u0003\n\u2212E\nU [B[U]]\n\f\f\f\f\n\f\f\f\f\n2\n\u22646\u221awr\u03b5 = O(w2.5 log(n)\u03b5),\nwhere r = O(log(n)/p) is the maximum recursion depth of Gn,w,\u03b5.\nProof. For 0 \u2264i < r, let ni, Ti, Xi, and \u02dcUi be the values of n, T , X, and \u02dcU at recursion level i. We have\nni+1 = \u230ani(1 \u2212p/2)\u230b\u2264n(1 \u2212p/2)i+1 and \u02dcUi\u22121 = Select(Ti, Xi, \u02dcUi). Let \u2206i be the error of the output from\nthe ith level of recursion\u2014that is,\n\u2206i := max\nB\u2032\n\f\f\f\f\n\f\f\f\f\nE\nTi,Xi, \u02dcUi\nh\nB\u2032[Select(Ti, Xi, \u02dcUi)]\ni\n\u2212E\nU [B\u2032[U]]\n\f\f\f\f\n\f\f\f\f\n2\n,\nwhere the maximum is taken over all length-ni, width-w, read-once, oblivious, permutation branching pro-\ngrams B\u2032.\n16\nSince the last level of recursion outputs uniform randomness, \u2206r = 0. For 0 \u2264i < r, we have, for some B\u2032,\n\u2206i \u2264\n\f\f\f\f\n\f\f\f\f\nE\nTi,Xi, \u02dcUi\nh\nB\u2032[Select(Ti, Xi, \u02dcUi)]\ni\n\u2212E\nU [B\u2032[U]]\n\f\f\f\f\n\f\f\f\f\n2\n\u00b7 P\nT [|T | \u2265pn/2]\n+ 2\u221aw \u00b7 P\nT [|T | < pn/2]\n\u2264\n\f\f\f\f\n\f\f\f\f\nE\nTi,Xi, \u02dcUi\nh\nB\u2032[Select(Ti, Xi, \u02dcUi)]\ni\n\u2212\nE\nTi,Xi,U [B\u2032[Select(Ti, Xi, U)]]\n\f\f\f\f\n\f\f\f\f\n2\n+\n\f\f\f\f\n\f\f\f\f\nE\nTi,Xi,U [B\u2032[Select(Ti, Xi, U)]] \u2212E\nU [B\u2032[U]]\n\f\f\f\f\n\f\f\f\f\n2\n+ 2\u221aw \u00b7 P\nT [|T | < pn/2]\nBy Lemma 6.2,\n\f\f\f\f\n\f\f\f\f\nE\nTi,Xi,U [B\u2032[Select(Ti, Xi, U)]] \u2212E\nU [B\u2032[U]]\n\f\f\f\f\n\f\f\f\f\n2\n\u22642\u03b5.\nBy Lemma 6.3,\nP\nT [|T | < pn/2] \u22642\u03b5.\nWe claim that\n\f\f\f\f\n\f\f\f\f\nE\nTi,Xi, \u02dcUi\nh\nB\u2032[Select(Ti, Xi, \u02dcUi)]\ni\n\u2212\nE\nTi,Xi,U [B\u2032[Select(Ti, Xi, U)]]\n\f\f\f\f\n\f\f\f\f\n2\n\u2264\u2206i+1.\nBefore we prove the claim, we complete the proof: This gives \u2206i \u2264\u2206i+1 + 2\u03b5 + 2\u221aw \u00b7 2\u03b5. It follows that\n\u22060 \u22646\u221awr\u03b5, as required.\nNow to prove the claim. In fact, we prove a stronger result: for every \ufb01xed Ti = t and Xi = x. We have\n\f\f\f\f\n\f\f\f\fE\n\u02dcUi\nh\nB\u2032[Select(t, x, \u02dcUi)]\ni\n\u2212E\nU [B\u2032[Select(t, x, U)]]\n\f\f\f\f\n\f\f\f\f\n2\n\u2264\u2206i+1.\nConsider Bx,t[y] := B\u2032[Select(t, x, y)] as a function of y \u2208{0, 1}ni\u2212|t|. Then Bx,t is a width-w, length-\n(ni \u2212|t|), read-once, oblivious, permutation branching program\u2014Bx,t is obtained from B\u2032 by \ufb01xing the bits\nin t to the values from x and \u2018collapsing\u2019 those layers. (If B\u2032 is a regular branching program, then Bx,t is\nnot necessarily a regular branching program. This is the only part of the proof where we need to assume\nthat B is a permutation branching program.)\nWe inductively know that \u02dcUi is pseudorandom for Bx,t\u2014that is,\n\f\f\f\f\n\f\f\f\fE\n\u02dcUi\nh\nBx,t[ \u02dcUi]\ni\n\u2212E\nU\n\u0002\nBx,t[U]\n\u0003\f\f\f\f\n\f\f\f\f\n2\n\u2264\u2206i+1.\nThus\n\f\f\f\f\n\f\f\f\fE\n\u02dcUi\nh\nB\u2032[Select(t, x, \u02dcUi)]\ni\n\u2212E\nU [B\u2032[Select(t, x, U)]]\n\f\f\f\f\n\f\f\f\f\n2\n=\n\f\f\f\f\n\f\f\f\fE\n\u02dcUi\nh\nBx,t[ \u02dcUi]\ni\n\u2212E\nU\n\u0002\nBx,t[U]\n\u0003\f\f\f\f\n\f\f\f\f\n2\n\u2264\u2206i+1,\nas required.\nProof of Theorem 6.1. Choose \u03b5\u2032 = \u0398(\u03b5/w2.5 log(n)) such that Gn,w,\u03b5\u2032 has error \u03b5. The seed length is\nsn,w,\u03b5\u2032 = O(w2 log(w) log(n) log(nw/\u03b5) + w4 log2(w log(n)/\u03b5)),\nas required.\n17\n7\nGeneral Read-Once, Oblivious Branching Programs\nWith a di\ufb00erent setting of parameters, our pseudorandom generator can fool arbitrary oblivious, read-once\nbranching programs, rather than just permutation branching programs.\nTheorem 7.1. There exists a pseudorandom generator family G\u2032\nn,w,\u03b5 : {0, 1}s\u2032\nn,w,\u03b5 \u2192{0, 1}n with seed\nlength s\u2032\nn,w,\u03b5 = O(\u221an log3(n) log(nw/\u03b5)) such that, for any length-n, width-w, oblivious, read-once branching\nprogram B and \u03b5 > 0,\n\f\f\f\f\f\n\f\f\f\f\f\nE\nUs\u2032n,w,\u03b5\nh\nB[G\u2032\nn,w,\u03b5(Us\u2032n,w,\u03b5)]\ni\n\u2212E\nU [B[U]]\n\f\f\f\f\f\n\f\f\f\f\f\n2\n\u2264\u03b5.\nMoreover, G\u2032\nn,w,\u03b5 is computable in space O(sn,w,\u03b5).\nTheorem 7.1 implies Theorem 1.2.\nCompared to Theorem 6.1, the seed length has a worse dependence on the length (\u221an versus log2 n), but\nhas a much better dependence on the width (log w versus poly(w)).\nImpagliazzo, Meka, and Zuckerman [13] obtain the seed length \u221as \u00b7 2O(\u221alog s) = s1/2+o(1) for arbitrary\nbranching programs of size s. For a width-w, length-n, read-once branching program, s = O(wn). Our\nresult is incomparable to that of Impagliazzo et al. Our result only covers oblivious, read-once branching\nprograms, while that of Impagliazzo et al. covers non-read-once and non-oblivious branching programs.\nHowever, our seed length depends logarithmically on the width, while theirs depends polynomially on the\nwidth; we can achieve seed length n1/2+o(1) for width 2no(1) while they require width no(1).\nThe key to proving Theorem 7.1 is the following Fourier mass bound for arbitrary branching programs.\nLemma 7.2. Let B be a length-n, width-w, read-once, oblivious branching program. Then, for all k \u2208[n],\nLk\n2(B) :=\nX\ns\u2208{0,1}n:|s|=k\n\f\f\f\n\f\f\f bB[s]\n\f\f\f\n\f\f\f\n2 \u2264\ns\nw\n\u0012n\nk\n\u0013\n\u2264\n\u221a\nwnk.\nProof. By Parseval\u2019s Identity,\nX\ns\u2208{0,1}n:|s|=k\n\f\f\f\n\f\f\f bB[s]\n\f\f\f\n\f\f\f\n2\n2 \u2264\nX\ns\u2208{0,1}n\n\f\f\f\n\f\f\f bB[s]\n\f\f\f\n\f\f\f\n2\nFr = E\nU\nh\n||B[U]||2\nFr\ni\n= w.\nThe result follows from Cauchy-Schwartz.\nThe bound of Lemma 7.2 is di\ufb00erent that of Theorem 3.2. This leads to the di\ufb00erent seed length in Theorem\n7.1 versus Theorem 6.1.\nLemma 7.2 gives a di\ufb00erent version of our main lemma (Theorem 5.2).\nLemma 7.3. Let B be a length-n, width-w, read-once, oblivious branching program. Let T be a random\nvariable over {0, 1}n where each bit has expectation p and the bits are 2k-wise independent. Suppose p \u2264\n1/\n\u221a\n4n. Then\nP\nT\nh\nL2(B|T ) \u2264\u221aw \u00b7 nk/2i\n\u22651 \u2212k \u00b7 \u221aw \u00b7 n3 \u00b7 2\u2212k.\nUsing Lemma 7.3, we can construct a pseudorandom generator fooling general read-once, oblivious branching\nprograms (Theorem 7.1), similarly to the proof of Theorem 6.1. For more details, see Appendix C.\n18\n8\nFurther Work\nOne open problem is to extend the main result (Theorem 6.1) to regular or even non-regular branching\nprograms while maintaining polylog(n) seed length. As discussed in Section 6, the only part of our analysis\nthat fails for regular branching programs is the recursive analysis. The problem is that regular branching\nprograms are not closed under restriction\u2014that is, setting some of the bits of a regular branching program\ndoes not necessarily yield a regular branching program. In particular, we cannot bound\n\f\f\f\f\n\f\f\f\fE\n\u02dcU\nh\nB[Select(t, x, \u02dcU)]\ni\n\u2212E\nU [B[Select(t, x, U)]]\n\f\f\f\f\n\f\f\f\f\nfor \ufb01xed t and x by the distinguishability of \u02dcU and U by another read-once, oblivious, regular branching\nprogram Bx,t. We have two options:\n\u2022 Find another way to bound\n\f\f\f\f\n\f\f\f\f\nE\nT,X, \u02dcU\nh\nB[Select(T, X, \u02dcU)]\ni\n\u2212E\nT,X, [B[Select(T, X, U)]]\n\f\f\f\f\n\f\f\f\f.\n\u2022 Extend the main lemma (Theorem 5.2) to non-regular branching programs.\nTowards the latter option, we have the following conjecture.\nConjecture 8.1. For every constant w, the following holds.\nLet B be a length-n, width-w, read-once,\noblivious branching program. Then\nLk\n2(B) =\nX\ns\u2208{0,1}n:|s|=k\n\f\f\f\n\f\f\f bB[s]\n\f\f\f\n\f\f\f\n2 \u2264nO(1)(log n)O(k)\nfor all k \u22651.\nThis conjecture relates to the Coin Theorem of Brody and Verbin (see the discussion in Section 4). Speci\ufb01-\ncally, if we remove the nO(1) factor, this conjecture implies the Coin Theorem.\nConjecture 8.1 would su\ufb03ce to construct a pseudorandom generator for constant-width, read-once, oblivious\nbranching programs with seed length polylog(n).\nThe seed length of our generators is worse than that of generators for ordered branching programs. Indeed, for\nordered permutation branching programs of constant width, it is known how to achieve seed length O(log n)\n[17], whereas we only achieve seed length O(log2 n) in Theorem 6.1. For general ordered branching programs,\nNisan [21] obtains seed length O(log(nw) log(n)), whereas Theorem 7.1 gives seed length \u02dcO(\u221an log(w)). It\nwould be interesting to close these gaps.\nReferences\n[1] Noga Alon, Oded Goldreich, Johan H\u02daastad, and Ren\u00b4e Peralta. Simple constructions of almost k-wise independent\nrandom variables. In FOCS, pages 544\u2013553, 1990.\n[2] Roy Armoni, Michael E. Saks, Avi Wigderson, and Shiyu Zhou. Discrepancy sets and pseudorandom generators\nfor combinatorial rectangles. In FOCS, pages 412\u2013421, 1996.\n[3] Andrej Bogdanov, Zeev Dvir, Elad Verbin, and Amir Yehudayo\ufb00. Pseudorandomness for width 2 branching\nprograms. Electronic Colloquium on Computational Complexity (ECCC), 16:70, 2009.\n[4] Andrej Bogdanov, Periklis A. Papakonstantinou, and Andrew Wan. Pseudorandomness for read-once formulas.\nIn FOCS, pages 240\u2013246, 2011.\n19\n[5] Mark Braverman, Anup Rao, Ran Raz, and Amir Yehudayo\ufb00. Pseudorandom generators for regular branching\nprograms. Foundations of Computer Science, IEEE Annual Symposium on, 0:40\u201347, 2010.\n[6] Joshua Brody and Elad Verbin. The coin problem and pseudorandomness for branching programs. In Proceed-\nings of the 2010 IEEE 51st Annual Symposium on Foundations of Computer Science, FOCS \u201910, pages 30\u201339,\nWashington, DC, USA, 2010. IEEE Computer Society.\n[7] L. Elisa Celis, Omer Reingold, Gil Segev, and Udi Wieder. Balls and bins: Smaller hash families and faster\nevaluation. In FOCS, pages 599\u2013608, 2011.\n[8] Anindya De. Pseudorandomness for permutation and regular branching programs. In Proceedings of the 2011\nIEEE 26th Annual Conference on Computational Complexity, CCC \u201911, pages 221\u2013231, Washington, DC, USA,\n2011. IEEE Computer Society.\n[9] Guy Even, Oded Goldreich, Michael Luby, Noam Nisan, and Boban Velickovic.\nE\ufb03cient approximation of\nproduct distributions. Random Struct. Algorithms, 13(1):1\u201316, 1998.\n[10] Parikshit Gopalan, Raghu Meka, Omer Reingold, Luca Trevisan, and Salil Vadhan.\nBetter pseudorandom\ngenerators from milder pseudorandom restrictions. In Foundations of Computer Science (FOCS), 2012 IEEE\n53rd Annual Symposium on, pages 120\u2013129, 2012.\n[11] Iftach Haitner, Danny Harnik, and Omer Reingold. On the power of the randomized iterate. In C. Dwork,\neditor, Advances in Cryptology\u2014CRYPTO \u201806, Lecture Notes in Computer Science. Springer-Verlag, 2006.\n[12] Alexander Healy, Salil Vadhan, and Emanuele Viola. Using nondeterminism to amplify hardness. SIAM Journal\non Computing, 35(4):903\u2013931 (electronic), 2006.\n[13] R. Impagliazzo, R. Meka, and D. Zuckerman. Pseudorandomness from shrinkage. In Foundations of Computer\nScience (FOCS), 2012 IEEE 53rd Annual Symposium on, pages 111\u2013119, 2012.\n[14] Russell Impagliazzo, Noam Nisan, and Avi Wigderson. Pseudorandomness for network algorithms. In In Pro-\nceedings of the 26th Annual ACM Symposium on Theory of Computing, pages 356\u2013364, 1994.\n[15] Piotr Indyk. Stable distributions, pseudorandom generators, embeddings, and data stream computation. J.\nACM, 53(3):307\u2013323, 2006.\n[16] Eyal Kaplan, Moni Naor, and Omer Reingold.\nDerandomized constructions of k-wise (almost) independent\npermutations. In Proceedings of the 8th International Workshop on Randomization and Computation (RANDOM\n\u201805), number 3624 in Lecture Notes in Computer Science, pages 354 \u2013 365, Berkeley, CA, August 2005. Springer.\n[17] Michal Kouck\u00b4y, Prajakta Nimbhorkar, and Pavel Pudl\u00b4ak. Pseudorandom generators for group products. In\nProceedings of the 43rd annual ACM symposium on Theory of computing, STOC \u201911, pages 263\u2013272, New York,\nNY, USA, 2011. ACM.\n[18] Nathan Linial, Michael Luby, Michael E. Saks, and David Zuckerman. E\ufb03cient construction of a small hitting\nset for combinatorial rectangles in high dimension. Combinatorica, 17(2):215\u2013234, 1997.\n[19] Chi-Jen Lu. Improved pseudorandom generators for combinatorial rectangles. Combinatorica, 22(3):417\u2013434,\n2002.\n[20] Joseph Naor and Moni Naor. Small-bias probability spaces: E\ufb03cient constructions and applications. SIAM J.\nComput, 22:838\u2013856, 1993.\n[21] Noam Nisan. RL \u2282SC. In Proceedings of the twenty-fourth annual ACM symposium on Theory of computing,\nSTOC \u201992, pages 619\u2013623, New York, NY, USA, 1992. ACM.\n[22] Noam Nisan and David Zuckerman. More deterministic simulation in logspace. In Proceedings of the twenty-\ufb01fth\nannual ACM symposium on Theory of computing, STOC \u201993, pages 235\u2013244, New York, NY, USA, 1993. ACM.\n[23] Ran Raz and Omer Reingold. On recycling the randomness of states in space bounded computation. In In\nProceedings of the Thirty-First Annual ACM Symposium on the Theory of Computing, pages 159\u2013168, 1999.\n[24] Omer Reingold. Undirected connectivity in log-space. J. ACM, 55(4):17:1\u201317:24, September 2008.\n[25] Omer Reingold, Luca Trevisan, and Salil Vadhan. Pseudorandom walks on regular digraphs and the RL vs. L\nproblem. In Proceedings of the thirty-eighth annual ACM symposium on Theory of computing, STOC \u201906, pages\n457\u2013466, New York, NY, USA, 2006. ACM.\n[26] Eyal Rozenman and Salil Vadhan.\nDerandomized squaring of graphs.\nIn Proceedings of the 8th interna-\ntional workshop on Approximation, Randomization and Combinatorial Optimization Problems, and Proceed-\nings of the 9th international conference on Randamization and Computation: algorithms and techniques, AP-\nPROX\u201905/RANDOM\u201905, pages 436\u2013447, Berlin, Heidelberg, 2005. Springer-Verlag.\n20\n[27] Michael Saks and Shiyu Zhou. BPHSPACE(S) \u2282DSPACE(S3/2). Journal of Computer and System Sciences,\n58(2):376 \u2013 403, 1999.\n[28] J. Schmidt, A. Siegel, and A. Srinivasan. Cherno\ufb00Hoe\ufb00ding bounds for applications with limited independence.\nSIAM Journal on Discrete Mathematics, 8(2):223\u2013250, 1995.\n[29] D. Sivakumar. Algorithmic derandomization via complexity theory. In IEEE Conference on Computational\nComplexity, page 10, 2002.\n[30] Thomas Steinke. Pseudorandomness for permutation branching programs without the group theory. Electronic\nColloquium on Computational Complexity (ECCC), 19:83, 2012.\n[31] Yoav Tzur. Notions of weak pseudorandomness and GF(2n)-polynomials. Master\u2019s thesis, Weizmann Institute\nof Science, 2009.\n[32] Ji\u02c7r\u00b4\u0131 \u02c7S\u00b4\u0131ma and Stanislav \u02c7Z\u00b4ak. A su\ufb03cient condition for sets hitting the class of read-once branching programs\nof width 3. In Proceedings of the 38th international conference on Current Trends in Theory and Practice of\nComputer Science, SOFSEM\u201912, pages 406\u2013418, Berlin, Heidelberg, 2012. Springer-Verlag.\nA\nProof of Lemma 3.1\nLemma 3.1. Let B be a length-n, width-w, ordered, regular branching program. Then\nX\n1\u2264i\u2264n\n\f\f\f\n\f\f\f [\nBi\u00b7\u00b7\u00b7n[1 \u25e60n\u2212i]\n\f\f\f\n\f\f\f\n2 \u22642w2.\nThis proof is adapted from [5, Lemma 4].\nProof. De\ufb01ne \u03c1 : Rw\u00d7w \u2192R by\n\u03c1(X) :=\nX\n1\u2264u<v\u2264w\n||X(u, \u00b7) \u2212X(v, \u00b7)||2 ,\nwhere X(u, \u00b7) and X(v, \u00b7) are the uth and vth rows of X respectively. We claim that, for all i \u2208[n] and\nX \u2208Rw\u00d7w,\n\f\f\f\n\f\f\fc\nBi[1]X\n\f\f\f\n\f\f\f\n2 \u22642(\u03c1(X) \u2212\u03c1(c\nBi[0]X)).\nIt follows that\nX\ni\u2208[n]\n\f\f\f\n\f\f\fc\nBi[1] \\\nBi+1\u00b7\u00b7\u00b7n[0n\u2212i]\n\f\f\f\n\f\f\f\n2 \u22642\nX\ni\u2208[n]\n\u03c1( \\\nBi+1\u00b7\u00b7\u00b7n[0n\u2212i]) \u2212\u03c1( [\nBi\u00b7\u00b7\u00b7n[0n\u2212i+1]) = 2(\u03c1(I) \u2212\u03c1( bB[0n])).\nNoting that \u03c1(I) \u2264w2, we obtain the result.\nIntuitively, \u03c1( [\nBi\u00b7\u00b7\u00b7n[0n]) measures the \u2018correlation\u2019 between the state at layer i and the state at the last\nlayer when run on uniform randomness; \u03c1 measures how much the distribution of the \ufb01nal state changes if\nthe state at layer i is changed from u to v and this is summed over all pairs {u, v}. On the other hand,\n\f\f\f\n\f\f\f [\nBi\u00b7\u00b7\u00b7n[1 \u25e60n\u2212i]\n\f\f\f\n\f\f\f\n2 measures the correlation between bit i and the \ufb01nal state; the Fourier coe\ufb03cient shows\nhow much Bi\u00b7\u00b7\u00b7n correlates with bit i. Our claim simply states that this correlation is \u2018conserved\u2019\u2014that is,\nthe correlation of the state at layer i plus the correlation of bit i is bounded by the correlation of the state\nat layer i + 1. This makes sense as the state at layer i + 1 is determined by the state at layer i and bit i.\nNow we prove our claim: Consider the rows of X and c\nBi[0]X as points in Rw. We start with 2w points\ncorresponding to each row of X repeated twice and we move these points one by one until they correspond\nto the rows of c\nBi[0]X repeated twice: In step u \u2208[w], we take one point corresponding to row Bi[0](u) of\n21\nX (that is, Bi[0](u, \u00b7)X) and one point corresponding to row Bi[1](u) of X (Bi[1](u, \u00b7)X) and move both to\ntheir midpoint c\nBi[0](u, \u00b7)X = (Bi[0](u, \u00b7)X + Bi[1](u, \u00b7)X)/2.\nLet Pu be the multiset of points at step u \u2208[w] \u222a{0}. That is,\nP0 =\n[\nu\u2208[w]\n{X(u, \u00b7), X(u, \u00b7)}\nand, for all u \u2208[w],\nPu = (Pu\u22121\\{Bi[0](u, \u00b7)X, Bi[1](u, \u00b7)X}) \u222a{c\nBi[0](u, \u00b7)X, c\nBi[0](u, \u00b7)X}.\nBy regularity, we have Pw = S\nu\u2208[w]{c\nBi[0](u, \u00b7)X, c\nBi[0](u, \u00b7)X}.\nNow we can consider \u03c1 as a function on the collection of points Pu:\n\u03c1\u2032(Pu) :=\nX\nx,y\u2208Pu:x\u227ay\n||x \u2212y||2 ,\nwhere the comparison x \u227ay is with respect to some arbitrary ordering on the multiset of points. (We simply\ndo not want to double count pairs.) We have \u03c1\u2032(P0) = 4\u03c1(X) and \u03c1\u2032(Pw) = 4\u03c1(c\nBi[0]X). Note that the 4\nfactor comes from the fact that every pair of rows (X(u, \u00b7), X(v, \u00b7)) becomes four pairs of points in P0, as\nevery row corresponds to two points.\nFix u \u2208[w]. Now we bound \u03c1\u2032(Pu\u22121) \u2212\u03c1\u2032(Pu): Let x = Bi[0](u, \u00b7)X, y = Bi[1](u, \u00b7)X, and z = c\nBi[0](u, \u00b7)X.\nThen Pu = (Pu\u22121\\{x, y}) \u222a{z, z} and z = (x + y)/2. We have\n\u03c1\u2032(Pu\u22121) \u2212\u03c1\u2032(Pu) = ||x \u2212y||2 +\nX\nw\u2208(Pu\u22121\\{x,y})\n||w \u2212x||2 + ||w \u2212y||2 \u22122 ||w \u2212z||2 .\nBy the triangle inequality,\n2 ||w \u2212z||2 = ||2w \u2212(x + y)||2 \u2264||w \u2212x||2 + ||w \u2212y||2 .\nSo\n\u03c1\u2032(Pu\u22121) \u2212\u03c1\u2032(Pu) \u2265||x \u2212y||2 = ||Bi[0](u, \u00b7)X \u2212Bi[1](u, \u00b7)X||2 =\n\f\f\f\n\f\f\f2c\nBi[1](u, \u00b7)X\n\f\f\f\n\f\f\f\n2 .\nIt follows that\n4\u03c1(X) \u22124\u03c1(c\nBi[0]X) = \u03c1\u2032(P0) \u2212\u03c1\u2032(Pw) =\nX\nu\u2208[w]\n\u03c1\u2032(Pu\u22121) \u2212\u03c1\u2032(Pu) \u22652\nX\nu\u2208[w]\n\f\f\f\n\f\f\fc\nBi[1](u, \u00b7)X\n\f\f\f\n\f\f\f\n2 \u22652\n\f\f\f\n\f\f\fc\nBi[1]X\n\f\f\f\n\f\f\f\n2 .\nB\nLimited Independence\nWe use the following fact.\nLemma B.1 (Cherno\ufb00Bound for Limited Independence). Let X1 \u00b7 \u00b7 \u00b7 X\u2113be \u03b4-almost k-wise independent\nrandom variables with 0 \u2264Xi \u22641 for all i. Set X = P\ni Xi. Then, for all \u03b6 \u2208(0, 1),\nP\nX\nh\f\f\fX \u2212E\nX [X]\n\f\f\f \u2265\u2113\u03b6\ni\n\u2264\n\u0012 k2\n4\u2113\u03b62\n\u0013\u230ak/2\u230b\n+ \u03b4\n\u03b6k\n22\nThe following proof is based on [28, Theorem 4]. The only di\ufb00erence is that we extend to almost k-wise\nindependence from k-wise independence.\nProof. Assume, without loss of generality, that k is even. Let \u00b5 = E\nX [X] and \u00b5i = E\nX [Xi]. We have\nE\nX\n\u0002\n(X \u2212\u00b5)k\u0003\n= E\nX\n\uf8ee\n\uf8ef\uf8f0\n\uf8eb\n\uf8edX\n1\u2264i\u2264\u2113\nXi \u2212\u00b5i\n\uf8f6\n\uf8f8\nk\uf8f9\n\uf8fa\uf8fb= E\nX\n\uf8ee\n\uf8f0X\nS\u2208[\u2113]k\nY\ni\u2208S\n(Xi \u2212\u00b5i)\n\uf8f9\n\uf8fb=\nX\nS\u2208[\u2113]k\nE\nX\n\"Y\ni\u2208S\n(Xi \u2212\u00b5i)\n#\n.\nNote that E\nX [Xi \u2212\u00b5i] = 0 for all i. By \u03b4-almost k-wise independence, each term in the product Q\ni\u2208S(Xi \u2212\n\u00b5i) is almost independent unless it is a repeated term.\nThus, unless every term is a repeated term,\n\f\f\fE\nX\n\u0002Q\ni\u2208S(Xi \u2212\u00b5i)\n\u0003\f\f\f \u2264\u03b4.\nIf every term is repeated, then there are at most k/2 di\ufb00erent terms.\nThis\nmeans we need only consider\n\u0000 \u2113\nk/2\n\u0001\n(k/2)k values of S.\nAlso |Xi \u2212\u00b5i| \u22641, so E\nX\n\u0002Q\ni\u2208S(Xi \u2212\u00b5i)\n\u0003\n\u22641. Thus\nE\nX\n\u0002\n(X \u2212\u00b5)k\u0003\n\u2264\n\u0012 \u2113\nk/2\n\u0013\n(k/2)k + \u2113k\u03b4 \u2264\n\u0000(k/2)2\u2113\n\u0001k/2 + \u2113k\u03b4.\nAnd, by Markov\u2019s inequality,\nP\nX [|X \u2212\u00b5| \u2265\u2113\u03b6] = P\nX\n\u0002\n(X \u2212\u00b5)k \u2265(\u2113\u03b6)k\u0003\n\u2264\nE\nX\n\u0002\n(X \u2212\u00b5)k\u0003\n(\u2113\u03b6)k\n\u2264\n\u0012 k2\n4\u2113\u03b62\n\u0013k/2\n+ \u03b4\n\u03b6k ,\nas required.\nLemma B.2. We can sample a \u03b4-almost k-wise independent random variable T over {0, 1}n with each bit\nhaving expectation p = 2\u2212d using O(kd + log(log(n)/\u03b4)) random bits.\nProof. The algorithm is as follows.\n1. Sample X \u2208{0, 1}nd that is \u03b4-almost kd-wise independent.\n2. Sample Y \u2208{0, 1}d uniformly at random.\n3. Let Z = X \u2295(Y, Y, \u00b7 \u00b7 \u00b7 , Y ). That is, XOR X with n copies of Y .\n4. Let T (i) = Q\n(i\u22121)d<j\u2264id Z(j) for all i.\n5. Output T .\nSampling X requires O(kd + log(log(n)/\u03b4)) random bits [20, 1]. Sampling Y requires d random bits. By\nXORing, Z has the property that it is both \u03b4-almost kd-wise independent and every block of d consecutive\nbits is independent. The latter property ensures that, for all i,\nP\nT [T (i) = 1] = P\nZ [Z((i \u22121)d + 1) = Z((i \u22121)d + 2) = \u00b7 \u00b7 \u00b7 = Z(id) = 1] = 2\u2212d = p.\nThe former property ensures that T is \u03b4-almost k-wise independent, as required.\n23\nC\nGeneral Read-Once, Oblivious Branching Programs\nFirst we prove the main lemma for general branching programs:\nLemma 7.3. Let B be a length-n, width-w, read-once, oblivious branching program. Let T be a random\nvariable over {0, 1}n where each bit has expectation p and the bits are 2k-wise independent. Suppose p \u2264\n1/\n\u221a\n4n. Then\nP\nT\nh\nL2(B|T ) \u2264\u221aw \u00b7 nk/2i\n\u22651 \u2212k \u00b7 \u221aw \u00b7 n3 \u00b7 2\u2212k.\nProof. Fix i, j, and k\u2032 with 1 \u2264i \u2264j \u2264n and k \u2264k\u2032 < 2k. Then\nE\nT\nh\nLk\u2032\n2 (Bi\u00b7\u00b7\u00b7j|T )\ni\n=\nX\ns\u2282[n]:|s|=k\u2032\nP\nT [s \u2282T ]\n\f\f\f\n\f\f\f[\nBi\u00b7\u00b7\u00b7j[s]\n\f\f\f\n\f\f\f \u2264\n\u221a\nwnk\u2032pk\u2032 \u2264\u221aw2\u2212k\u2032 \u2264\u221aw2\u2212k.\nBy Markov\u2019s inequality and a union bound,\nP\nT\nh\n\u22001 \u2264i \u2264j \u2264n \u2200k \u2264k\u2032 < 2k Lk\u2032\n2 (Bi\u00b7\u00b7\u00b7j|T ) \u22641/n\ni\n\u22651 \u2212\u221aw \u00b7 n4 \u00b7 2\u2212k.\nLemma 5.4 now implies that\nP\nT\nh\n\u22001 \u2264i \u2264j \u2264n \u2200k\u2032 \u2265k Lk\u2032\n2 (Bi\u00b7\u00b7\u00b7j|T ) \u22641/n\ni\n\u22651 \u2212\u221aw \u00b7 n4 \u00b7 2\u2212k.\nThus\nP\nT\n\uf8ee\n\uf8f0X\nk\u2032\u2265k\nLk\u2032\n2 (B|T ) \u22641\n\uf8f9\n\uf8fb\u22651 \u2212\u221aw \u00b7 n4 \u00b7 2\u2212k.\nNote that\nX\n1\u2264k\u2032<k\nLk\u2032\n2 (B|T ) \u2264\nX\n1\u2264k\u2032<k\n\u221a\nwnk\u2032 = \u221awnn(k\u22121)/2 \u22121\n\u221an \u22121\n\u2264\u221aw \u00b7 nk/2 \u22121.\nThe result follows.\nNow we prove the result for general read-once, oblivious branching programs:\nTheorem 7.1. There exists a pseudorandom generator family G\u2032\nn,w,\u03b5 : {0, 1}s\u2032\nn,w,\u03b5 \u2192{0, 1}n with seed\nlength s\u2032\nn,w,\u03b5 = O(\u221an log3(n) log(nw/\u03b5)) such that, for any length-n, width-w, oblivious, read-once branching\nprogram B and \u03b5 > 0,\n\f\f\f\f\f\n\f\f\f\f\f\nE\nUs\u2032n,w,\u03b5\nh\nB[G\u2032\nn,w,\u03b5(Us\u2032n,w,\u03b5)]\ni\n\u2212E\nU [B[U]]\n\f\f\f\f\f\n\f\f\f\f\f\n2\n\u2264\u03b5.\nMoreover, G\u2032\nn,w,\u03b5 is computable in space O(sn,w,\u03b5).\nThe pseudorandom generator is formally de\ufb01ned as follows.\nAlgorithm for G\u2032\nn,w,\u03b5 : {0, 1}s\u2032\nn,w,\u03b5 \u2192{0, 1}n.\nParameters: n \u2208N, w \u2208N, \u03b5 > 0.\nInput: A random seed of length s\u2032\nn,w,\u03b5.\n1. Compute appropriate values of p \u2208[1/4\u221an, 1/2\u221an], k \u2265log2\n\u00002wn4/\u03b5\n\u0001\n, and \u00b5 = \u03b5/\n\u221a\nwnk.2\n2For the purposes of the analysis we assume that p, k, and \u00b5 are the same at every level of recursion. So if G\u2032\nn,w,\u03b5 is being\ncalled recursively, use the same values of p, k, and \u00b5 as at the previous level of recursion.\n24\n2. If n \u226416 log2(2/\u03b5)/p, output n truly random bits and stop.\n3. Sample T \u2208{0, 1}n where each bit has expectation p and the bits are 2k-wise independent.\n4. If |T | < pn/2, output 0n and stop.\n5. Recursively sample \u02dcU \u2208{0, 1}\u230an(1\u2212p/2)\u230b. i.e. \u02dcU = G\u2032\n\u230an(1\u2212p/2)\u230b,w,\u03b5(U).\n6. Sample X \u2208{0, 1}n from a \u00b5-biased distribution.\n7. Output Select(T, X, \u02dcU) \u2208{0, 1}n.\nLemma C.1. The probability that G\u2032\nn,w,\u03b5 fails at step 4 is bounded by \u03b5\u2014that is, P\nT [|T | < pn/2] \u2264\u03b5.\nWe need the following Cherno\ufb00bound for limited independence, which gives a better dependence on p than\nLemma B.1.\nLemma C.2 ([28, Theorem 4 III]). Let X1, X2, \u00b7 \u00b7 \u00b7 , Xn be k-wise independent random variables on [0, 1].\nLet X = P\ni Xi. Let \u00b5 = E\nX [X] and \u03c32 \u2265Var[X]. If k \u22652 is even, then\nP\nX [|X \u2212\u00b5| \u2265\u03b1] \u2264\n\u0012k \u00b7 max{k, \u03c32}\ne2/3\u03b12\n\u0013k/2\n.\nProof of Lemma C.1. We have Var[|T |] = nVar[T (i)] = np(1 \u2212p) \u2264np. By Lemma C.2,\nP\nT [|T | < pn/2] \u2264\n\u0012\n2k\u2032np\ne2/3(pn/2)2\n\u0013k\u2032\n,\nwhere k\u2032 \u2264k is arbitrary. Set k\u2032 = \u2308log2(1/\u03b5)\u2309. Step 2 ensures that n > 16 log2(2/\u03b5)/p > 16k\u2032/p. Thus we\nhave\nP\nT [|T | < pn/2] \u2264\n\u0012\n8k\u2032\ne2/3np\n\u0013log2(1/\u03b5)\n\u2264\n\u0012\n8k\u2032\ne2/3(16k\u2032)\n\u0013log2(1/\u03b5)\n\u2264\u03b5\n.\nLemma C.3. Let B be a length-n, width-w, read-once, oblivious branching program. Let \u03b5 \u2208(0, 1). Let T be\na random variable over {0, 1}n that is 2k-wise independent and each bit has expectation p, where we require\np \u22641/\n\u221a\n4n,\nk \u2265log2\n\u00002wn4/\u03b5\n\u0001\n.\nLet U be uniform over {0, 1}n. Let X be a \u00b5-biased random variable over {0, 1}n with \u00b5 \u2264\u03b5/\n\u221a\nwnk. Then\n\f\f\f\f\n\f\f\f\f\nE\nT,X,U [B[Select(T, X, U)]] \u2212E\nU [B[U]]\n\f\f\f\f\n\f\f\f\f\n2\n\u22642\u03b5.\nProof. For t \u2208{0, 1}n, we have\n\f\f\f\f\n\f\f\f\f E\nX,U [B[Select(t, X, U)]] \u2212E\nU [B[U]]\n\f\f\f\f\n\f\f\f\f\n2\n=\n\f\f\f\n\f\f\fE\nX [B|t[X]] \u2212E\nU [B[U]]\n\f\f\f\n\f\f\f\n2 \u2264L2(B|t)\u00b5.\nWe apply Lemma 7.3 and with probability at least 1 \u2212\u221awn4/2k over T , we have L2(B|T )\u00b5 \u2264\u03b5. Thus\n\f\f\f\f\n\f\f\f\f\nE\nT,X,U [B[Select(T, X, U)]] \u2212E\nU [B[U]]\n\f\f\f\f\n\f\f\f\f\n2\n\u2264P\nT [L2(B|T )\u00b5 > \u03b5] 2\u221aw + P\nT [L2(B|T )\u00b5 \u2264\u03b5] \u03b5\n\u2264\u221awn4/2k \u00b7 2\u221aw + \u03b5\n\u22642\u03b5.\n25\nLemma C.4. Let B be a length-n, width-w, read-once, oblivious branching program. Then\n\f\f\f\f\f\n\f\f\f\f\f\nE\nUs\u2032n,w,\u03b5\nh\nB[G\u2032\nn,w,\u03b5(Us\u2032n,w,\u03b5)]\ni\n\u2212E\nU [B[U]]\n\f\f\f\f\f\n\f\f\f\f\f\n2\n\u22644\u221awr\u03b5,\nwhere r = O(log(n)/p) is the recursion depth of G\u2032\nn,w,\u03b5.\nThe proof of this lemma is exactly as before (Lemma 6.4), except we no longer need the assumption that B\nis a permutation branching program.\nSetting \u03b5\u2032 = O(\u03b5/\u221anw log(n)) we can ensure that that G\u2032\nn,w,\u03b5\u2032 has error at most \u03b5. The overall seed length is\nO(\u221an log3(n) log(nw/\u03b5)): Each of the r = O(\u221an log(n)) levels of recursion requires O(log(w/\u03b5) + k log(n))\nrandom bits to sample X and O(k log2 n) bits to sample T . Finally we need O(log(1/\u03b5)/p) random bits at\nthe last level. Since k = O(log(nw/\u03b5)), this gives the required seed length.\n26\n",
        "sentence": "",
        "context": "while at Stanford University. Supported by NSF grant CCF-1116616 and the Lord Rutherford Memorial Research Fellowship.\n\u2021School of Engineering and Applied Sciences, Harvard University, 33 Oxford Street, Cambridge MA. Work done in part when\non leave as a Visiting Researcher at Microsoft Research Silicon Valley and a Visiting Scholar at Stanford University. Supported\nin part by NSF grant CCF-1116616 and US-Israel BSF grant 2010196.\n1\nIntroduction\nings of the 2010 IEEE 51st Annual Symposium on Foundations of Computer Science, FOCS \u201910, pages 30\u201339,\nWashington, DC, USA, 2010. IEEE Computer Society."
    },
    {
        "title": "Communication lower bounds using dual polynomials",
        "author": [
            "Alexander A. Sherstov"
        ],
        "venue": "Bulletin of the EATCS,",
        "citeRegEx": "Sherstov.,? \\Q2008\\E",
        "shortCiteRegEx": "Sherstov.",
        "year": 2008,
        "abstract": "Representations of Boolean functions by real polynomials play an important\nrole in complexity theory. Typically, one is interested in the least degree of\na polynomial p(x_1,...,x_n) that approximates or sign-represents a given\nBoolean function f(x_1,...,x_n). This article surveys a new and growing body of\nwork in communication complexity that centers around the dual objects, i.e.,\npolynomials that certify the difficulty of approximating or sign-representing a\ngiven function. We provide a unified guide to the following results, complete\nwith all the key proofs:\n  (1) Sherstov's Degree/Discrepancy Theorem, which translates lower bounds on\nthe threshold degree of a Boolean function into upper bounds on the discrepancy\nof a related function;\n  (2) Two different methods for proving lower bounds on bounded-error\ncommunication based on the approximate degree: Sherstov's pattern matrix method\nand Shi and Zhu's block composition method;\n  (3) Extension of the pattern matrix method to the multiparty model, obtained\nby Lee and Shraibman and by Chattopadhyay and Ada, and the resulting improved\nlower bounds for DISJOINTNESS;\n  (4) David and Pitassi's separation of NP and BPP in multiparty communication\ncomplexity for k=(1-eps)log n players.",
        "full_text": "arXiv:0805.2135v1  [cs.CC]  14 May 2008\nCommunication Lower Bounds\nUsing Dual Polynomials\u2217\nAlexander A. Sherstov\nUniv. of Texas at Austin, Dept. of Comp. Sciences, sherstov@cs.utexas.edu\nNovember 9, 2018\nAbstract\nRepresentations of Boolean functions by real polynomials play an im-\nportant role in complexity theory. Typically, one is interested in the least\ndegree of a polynomial p(x1, . . . , xn) that approximates or sign-represents a\ngiven Boolean function f(x1, . . . , xn). This article surveys a new and grow-\ning body of work in communication complexity that centers around the dual\nobjects, i.e., polynomials that certify the di\ufb03culty of approximating or sign-\nrepresenting a given function. We provide a uni\ufb01ed guide to the following\nresults, complete with all the key proofs:\n\u2022 Sherstov\u2019s Degree/Discrepancy Theorem,\nwhich translates lower\nbounds on the threshold degree of a Boolean function into upper\nbounds on the discrepancy of a related function;\n\u2022 Two di\ufb00erent methods for proving lower bounds on bounded-error\ncommunication based on the approximate degree: Sherstov\u2019s pattern\nmatrix method and Shi and Zhu\u2019s block composition method;\n\u2022 Extension of the pattern matrix method to the multiparty model, ob-\ntained by Lee and Shraibman and by Chattopadhyay and Ada, and the\nresulting improved lower bounds for disjointness;\n\u2022 David and Pitassi\u2019s separation of NP and BPP in multiparty communi-\ncation complexity for k \u2a7d(1 \u2212\u01eb) log n players.\n\u2217Invited survey for The Bulletin of the European Association for Theoretical Computer Science\n(EATCS).\n1\nIntroduction\nRepresentations of Boolean functions by real polynomials are of considerable im-\nportance in complexity theory.\nThe ease or di\ufb03culty of representing a given\nBoolean function by polynomials from a given set often yields valuable insights\ninto the structural complexity of that function.\nWe focus on two concrete representation schemes that involve polynomi-\nals.\nThe \ufb01rst of these corresponds to threshold computation.\nFor a Boolean\nfunction f\n: {0, 1}n \u2192{0, 1}, its threshold degree deg\u00b1( f) is the minimum\ndegree of a polynomial p(x1, . . . , xn) such that p(x) is positive if f(x) = 1\nand negative otherwise. In other words, the threshold degree of f is the least\ndegree of a polynomial that represents f in sign.\nSeveral authors have ana-\nlyzed the threshold degree of common Boolean functions [MP88, BRS95, OS03].\nThe results of these investigations have found numerous applications to circuit\ncomplexity [ABFR94, BRS95, KP97, KP98] and computational learning the-\nory [KS04, KOS04, KS07b].\nThe other representation scheme that we consider is approximation in the\nuniform norm.\nFor a Boolean function f\n: {0, 1}n \u2192{0, 1} and a constant\n\u01eb \u2208(0, 1/2), the \u01eb-approximate degree of f is the least degree of a polynomial\np(x1, . . . , xn) with | f(x) \u2212p(x)| \u2a7d\u01eb for all x \u2208{0, 1}n. Note that this repre-\nsentation is strictly stronger than the \ufb01rst: no longer are we content with rep-\nresenting f in sign, but rather we wish to closely approximate f on every in-\nput.\nThere is a considerable literature on the approximate degree of speci\ufb01c\nBoolean functions [NS92, Pat92, KLS96, BCWZ99, AS04, She08, Wol08]. This\nclassical notion has been crucial to progress on a variety of questions, includ-\ning quantum query complexity [BCWZ99, BBC+01, AS04], communication com-\nplexity [BW01, Raz03, BVW07] and computational learning theory [TT99, KS04,\nKKMS05, KS07a].\nThe approximate degree and threshold degree can be conveniently analyzed by\nmeans of a linear program. In particular, whenever a given function f cannot be ap-\nproximated or sign-represented by polynomials of low degree, linear-programming\nduality implies the existence of a certain dual object to witness that fact. This dual\nobject, which is a real function or a probability distribution, reveals useful new\ninformation about the structural complexity of f. The purpose of this article is to\nsurvey a very recent and growing body of work in communication complexity that\nrevolves around the dual formulations of the approximate degree and threshold\ndegree. Our ambition here is to provide a uni\ufb01ed view of these diverse results,\ncomplete with all the key proofs, and thereby to encourage further inquiry into the\npotential of the dual approach.\nIn the remainder of this section, we give an intuitive overview of our survey.\n1\nDegree/Discrepancy Theorem.\nThe \ufb01rst result that we survey, in Section 3, is\nthe author\u2019s Degree/Discrepancy Theorem [She07a]. This theorem and its proof\ntechnique are the foundation for much of the subsequent work surveyed in this\narticle [She07b, Cha07, LS07, CA08, DP08]. Fix a Boolean function f : {0, 1}n \u2192\n{0, 1} and let N be a given integer, N \u2a7en. In [She07a], we introduced the two-party\ncommunication problem of computing\nf(x|V),\nwhere the Boolean string x \u2208{0, 1}N is Alice\u2019s input and the set V \u2282{1, 2, . . . , N}\nof size |V| = n is Bob\u2019s input. The symbol x|V stands for the projection of x onto\nthe indices in V, in other words, x|V = (xi1, xi2, . . . , xin) \u2208{0, 1}n, where i1 < i2 <\n\u00b7 \u00b7 \u00b7 < in are the elements of V. Intuitively, this problem models a situation when\nAlice and Bob\u2019s joint computation depends on only n of the inputs x1, x2, . . . , xN.\nAlice knows the values of all the inputs x1, x2, . . . , xN but does not know which n\nof them are relevant. Bob, on the other hand, knows which n inputs are relevant\nbut does not know their values.\nWe proved in [She07a] that the threshold degree d of f is a lower bound\non the communication requirements of this problem.\nMore precisely, the De-\ngree/Discrepancy Theorem shows that this communication problem has discrep-\nancy exp(\u2212\u2126(d)) as soon as N \u2a7e11n2/d. This exponentially small discrepancy\nimmediately gives an \u2126(d) lower bound on communication in a variety of models\n(deterministic, nondeterministic, randomized, quantum with and without entangle-\nment). Moreover, the resulting lower bounds on communication hold even if the\ndesired error probability is vanishingly close to 1/2.\nThe proof of the Degree/Discrepancy Theorem introduces a novel technique\nbased on the dual formulation of the threshold degree. In fact, it appears to be the\n\ufb01rst use of the threshold degree (in its primal or dual form) to prove communication\nlower bounds. As an application, we exhibit in [She07a] the \ufb01rst AC0 circuit with\nexponentially small discrepancy, thereby separating AC0 from depth-2 majority\ncircuits and solving an open problem of Krause and Pudl\u00b4ak [KP97, \u00a76]. Indepen-\ndently of the author, Buhrman et al. [BVW07] exhibited another AC0 function with\nexponentially small discrepancy, using much di\ufb00erent techniques.\nBounded-Error Communication.\nNext, we present two recent results on\nbounded-error communication complexity, due to Sherstov [She07b] and Shi and\nZhu [SZ07]. These papers use the notion of approximate degree to contribute\nstrong lower bounds for rather broad classes of functions, subsuming Razborov\u2019s\nbreakthrough work on symmetric predicates [Raz03]. The lower bounds are valid\nnot only in the randomized model, but also in the quantum model with and without\nprior entanglement.\n2\nThe setting in which to view these two works is the generalized discrepancy\nmethod, a simple but very useful principle introduced by Klauck [Kla01] and refor-\nmulated in its current form by Razborov [Raz03]. Let f(x, y) be a Boolean function\nwhose quantum communication complexity is of interest. The method asks for a\nBoolean function h(x, y) and a distribution \u00b5 on (x, y)-pairs such that:\n(1) the functions f and h are highly correlated under \u00b5; and\n(2) all low-cost protocols have negligible advantage in computing h under \u00b5.\nIf such h and \u00b5 indeed exist, it follows that no low-cost protocol can compute f to\nhigh accuracy (or else it would be a good predictor for the hard function h as well!).\nThis method is in no way restricted to the quantum model but, rather, applies to\nany model of communication [She07b, \u00a72.4]. The importance of the generalized\ndiscrepancy method is that it makes it possible, in theory, to prove lower bounds\nfor functions such as disjointness, to which the traditional discrepancy method\ndoes not apply. In Section 4, we provide detailed historical background on the\ngeneralized discrepancy method and compile its quantitative versions for several\nmodels.\nThe hard part, of course, is \ufb01nding h and \u00b5. Except in rather restricted\ncases [Kla01, Thm. 4], it was not known how to do it. As a result, the generalized\ndiscrepancy method was of limited practical use. This di\ufb03culty was overcome\nindependently by Sherstov [She07b] and Shi and Zhu [SZ07], who used the dual\ncharacterization of the approximate degree to obtain h and \u00b5 for a broad range of\nproblems. To our knowledge, the work in [She07b] and [SZ07] is the \ufb01rst use of\nthe dual characterization of the approximate degree to prove communication lower\nbounds. The speci\ufb01cs of these two works are very di\ufb00erent. The construction of h\nand \u00b5 in [She07b], which we called the pattern matrix method for lower bounds on\nbounded-error communication, is built around a new matrix-analytic technique (the\npattern matrix) inspired by the author\u2019s Degree/Discrepancy Theorem. The con-\nstruction in [SZ07], the block-composition method, is based on the idea of hardness\nampli\ufb01cation by composition. These two methods exhibit quite di\ufb00erent behavior,\ne.g., the pattern matrix method further extends to the multiparty model. We present\nthe two methods individually in Sections 5.1 and 5.2 and provide a detailed com-\nparison of their strength and applicability in Section 5.3.\nExtensions to the Multiparty Model.\nBoth the Degree/Discrepancy Theo-\nrem [She07a] and the pattern matrix method [She07b] generalize to the multiparty\nnumber-on-the-forehead model. In the case of [She07a], this extension was for-\nmalized by Chattopadhyay [Cha07]. As before, let f : {0, 1}n \u2192{0, 1} be a given\nfunction. Recall that in the two-party case, there was a Boolean string x \u2208{0, 1}N\n3\nand a single set V \u2282{1, 2, . . . , N}. The k-party communication problem features\na Boolean string x \u2208{0, 1}Nk\u22121 and sets V1, . . . , Vk\u22121 \u2282{1, 2, . . . , N}. The k inputs\nx, V1, . . . , Vk\u22121 are distributed among the k parties as usual. The goal is to compute\nf(x|V1,...,Vk\u22121)\ndef=\nf\n\u0012\nxi1\n1,...,ik\u22121\n1 , . . . , xi1n,...,ik\u22121\nn\n\u0013\n,\n(1.1)\nwhere ij\n1 < ij\n2 < \u00b7 \u00b7 \u00b7 < ij\nn are the elements of V j (for j = 1, 2, . . . , k \u22121). This\nway, again no party knows at once the Boolean string x and the relevant bits in\nit. With this setup in place, it becomes relatively straightforward to bound the\ndiscrepancy by traversing the same line of reasoning as in [She07a]. The extension\nof the pattern matrix method [She07b] to the multiparty model uses a similar setup\nand was done by Lee and Shraibman [LS07] and independently by Chattopadhyay\nand Ada [CA08]. We present the proofs of these extensions in Section 6, placing\nthem in close correspondence with the two-party case. These extensions do not\nsubsume the two-party results, however (see Section 6 for details).\nThe authors of [LS07] and [CA08] gave important applications of their work\nto the k-party randomized communication complexity of disjointness, improving\nit from \u2126(1\nk log n) to n\u2126(1/k)2\u2212O(2k). As a corollary, they separated the multiparty\ncommunication classes NPcc\nk and BPPcc\nk for k = (1\u2212o(1)) log2 log2 n parties. They\nalso obtained new results for Lov\u00b4asz-Schrijver proof systems, in light of the work\ndue to Beame, Pitassi, and Segerlind [BPS07].\nSeparation of NPcc\nk and BPPcc\nk .\nThe separation of the classes NPcc\nk and BPPcc\nk\nin [LS07, CA08] for k = (1 \u2212o(1)) log2 log2 n parties was followed by another ex-\nciting development, due to David and Pitassi [DP08], who separated these classes\nfor k \u2a7d(1 \u2212\u01eb) log2 n parties. Here \u01eb > 0 is an arbitrary constant. Since the current\nbarrier for explicit lower bounds on multiparty communication complexity is pre-\ncisely k = log2 n, David and Pitassi\u2019s separation matches the state of the art. We\npresent this work in Section 7. The powerful idea in this result was to rede\ufb01ne\nthe projection operator x|V1,...,Vk\u22121 in (1.1). Speci\ufb01cally, David and Pitassi observed\nthat it su\ufb03ces to de\ufb01ne the projection operator at random, using the probabilistic\nmethod. This insight removed the key technical obstacle present in [LS07, CA08].\nIn a follow-up work by David, Pitassi, and Viola [DPV08], the probabilistic con-\nstruction was derandomized to yield an explicit separation.\nOther Related Work.\nFor completeness, we will mention several duality-based\nresults in communication complexity that fall outside the scope of this survey. Re-\ncent work has seen other applications of dual polynomials [She07c, RS08], which\nare considerably more complicated and no longer correspond to the approximate\n4\ndegree or threshold degree. More broadly, several recent results feature other forms\nof duality [LS07b, LS\u02c7S08], such as the duality of norms or semide\ufb01nite program-\nming duality.\n2\nPreliminaries\nThis section reviews our notation and provides relevant technical background.\n2.1\nGeneral Background\nA Boolean function is a mapping X \u2192{0, 1}, where X is a \ufb01nite set such as X =\n{0, 1}n or X = {0, 1}n \u00d7 {0, 1}n. The notation [n] stands for the set {1, 2, . . . , n}. For\nintegers N, n with N \u2a7en, the symbol\n\u0010[N]\nn\n\u0011\ndenotes the family of all size-n subsets\nof {1, 2, . . . , N}. For x \u2208{0, 1}n, we write |x| = x1 + \u00b7 \u00b7 \u00b7 + xn. For x, y \u2208{0, 1}n, the\nnotation x \u2227y refers as usual to the component-wise AND of x and y. In particular,\n|x \u2227y| stands for the number of positions where x and y both have a 1. Throughout\nthis manuscript, \u201clog\u201d refers to the logarithm to base 2.\nFor tensors A, B : X1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Xk \u2192R (where Xi is a \ufb01nite set, i = 1, 2, . . . , k),\nde\ufb01ne \u27e8A, B\u27e9= P\n(x1,...,xk)\u2208X1\u00d7\u00b7\u00b7\u00b7\u00d7Xk A(x1, . . . , xk)B(x1, . . . , xk). When A and B are\nvectors or matrices, this is the standard de\ufb01nition of inner product. The Hadamard\nproduct of A and B is the tensor A \u25e6B : X1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Xk\n\u2192R given by\n(A \u25e6B)(x1, . . . , xk) = A(x1, . . . , xk)B(x1, . . . , xk).\nThe symbol Rm\u00d7n refers to the family of all m \u00d7 n matrices with real entries.\nThe (i, j)th entry of a matrix A is denoted by Aij. We frequently use \u201cgeneric-entry\u201d\nnotation to specify a matrix succinctly: we write A = [F(i, j)]i, j to mean that the\n(i, j)th entry of A is given by the expression F(i, j). In most matrices that arise in\nthis work, the exact ordering of the columns (and rows) is irrelevant. In such cases\nwe describe a matrix by the notation [F(i, j)]i\u2208I, j\u2208J, where I and J are some index\nsets.\nLet A \u2208Rm\u00d7n. We use the following standard notation: \u2225A\u2225\u221e= maxi, j |Aij|\nand \u2225A\u22251 = P\ni, j |Aij|. We denote the singular values of A by \u03c31(A) \u2a7e\u03c32(A) \u2a7e\n. . . \u2a7e\u03c3min{m,n}(A) \u2a7e0. Recall that the spectral norm of A is given by \u2225A\u2225=\nmaxx\u2208Rn, \u2225x\u2225=1 \u2225Ax\u2225= \u03c31(A). An excellent reference on matrix analysis is [HJ86].\nWe conclude with a review of the Fourier transform over Zn\n2. Consider the\nvector space of functions {0, 1}n \u2192R, equipped with the inner product \u27e8f, g\u27e9=\n2\u2212n P\nx\u2208{0,1}n f(x)g(x). For S \u2286[n], de\ufb01ne \u03c7S : {0, 1}n \u2192{\u22121, +1} by \u03c7S (x) =\n(\u22121)\nP\ni\u2208S xi. Then {\u03c7S }S \u2286[n] is an orthonormal basis for the inner product space in\nquestion. As a result, every function f : {0, 1}n \u2192R has a unique representation of\nthe form f(x) = P\nS \u2286[n] \u02c6f(S ) \u03c7S (x), where \u02c6f (S ) = \u27e8f, \u03c7S \u27e9. The reals \u02c6f(S ) are called\n5\nthe Fourier coe\ufb03cients of f. The following fact is immediate from the de\ufb01nition of\n\u02c6f (S ):\nProposition 2.1. Fix f : {0, 1}n \u2192R. Then\nmax\nS \u2286[n] | \u02c6f(S )| \u2a7d2\u2212n X\nx\u2208{0,1}n\n| f(x)|.\n2.2\nCommunication Complexity\nThis survey features several standard models of communication. In the case of two\ncommunicating parties, one considers a function f : X \u00d7 Y \u2192{0, 1}, where X and\nY are some \ufb01nite sets. Alice receives an input x \u2208X, Bob receives y \u2208Y, and their\nobjective is to predict f(x, y) with good accuracy. To this end, Alice and Bob share\na communication channel (classical or quantum, depending on the model). Alice\nand Bob\u2019s communication protocol is said to have error \u01eb if it outputs the correct\nanswer f(x, y) with probability at least 1 \u2212\u01eb on every input. The cost of a given\nprotocol is the maximum number of bits exchanged on any input. The two-party\nmodels of interest to us are the randomized model, the quantum model without\nprior entanglement, and the quantum model with prior entanglement. The least cost\nof an \u01eb-error protocol for f in these models is denoted by R\u01eb( f), Q\u01eb( f), and Q\u2217\n\u01eb( f),\nrespectively. It is standard practice to omit the subscript \u01eb when error parameter\nis \u01eb = 1/3. Recall that the error probability of a protocol can be decreased from\n1/3 to any other constant \u01eb > 0 at the expense of increasing the communication\ncost by a constant factor; we will use this fact in many proofs of this survey, often\nwithout explicitly mentioning it. Excellent references on these communication\nmodels are [KN97] and [Wol01].\nA generalization of two-party communication is number-on-the-forehead mul-\ntiparty communication. Here one considers a function f : X1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Xk \u2192{0, 1}\nfor some \ufb01nite sets X1, . . . , Xk. There are k players. A given input (x1, . . . , xk) \u2208\nX1\u00d7\u00b7 \u00b7 \u00b7\u00d7Xk is distributed among the players by placing xi on the forehead of player\ni (for i = 1, . . . , k). In other words, player i knows x1, . . . , xi\u22121, xi+1, . . . , xk but not\nxi. The players can communicate by writing bits on a shared blackboard, visible to\nall. They additionally have access to a shared source of random bits. Their goal is\nto devise a communication protocol that will allow them to accurately predict the\nvalue of f on every input. Analogous to the two-party case, the randomized com-\nmunication complexity Rk\n\u01eb( f) is the least cost of an \u01eb-error communication protocol\nfor f in this model. The \ufb01nal section of this paper also considers the nondetermin-\nistic communication complexity Nk( f), which is the minimum cost of a protocol\n6\nfor f that always outputs the correct answer on the inputs f \u22121(0) and has error prob-\nability less than 1 on each of the inputs f \u22121(1). Analogous to computational com-\nplexity, BPPcc\nk (respectively, NPcc\nk ) is the class of functions f : ({0, 1}n)k \u2192{0, 1}\nwith Rk( f) \u2a7d(log n)O(1) (respectively, Nk( f) \u2a7d(log n)O(1)). See [KN97] for further\ndetails.\nA crucial tool for proving communication lower bounds is the discrepancy\nmethod. Given a function f : X \u00d7 Y \u2192{0, 1} and a distribution \u00b5 on X \u00d7 Y,\nthe discrepancy of f with respect to \u00b5 is de\ufb01ned as\ndisc\u00b5( f) = max\nS \u2286X,\nT\u2286Y\n\f\f\f\f\f\f\f\f\nX\nx\u2208S\nX\ny\u2208T\n(\u22121)f(x,y)\u00b5(x, y)\n\f\f\f\f\f\f\f\f\n.\nThis de\ufb01nition generalizes to the multiparty case as follows. Fix f : X1\u00d7\u00b7 \u00b7 \u00b7\u00d7Xk \u2192\n{0, 1} and a distribution \u00b5 on X1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Xk. The discrepancy of f with respect to \u00b5\nis de\ufb01ned as\ndisc\u00b5( f) = max\n\u03c61,...,\u03c6k\n\f\f\f\f\f\f\f\f\f\f\f\f\nX\n(x1,...,xk)\n\u2208X1\u00d7\u00b7\u00b7\u00b7\u00d7Xk\n\u03c8(x1, . . . , xk)\nk\nY\ni=1\n\u03c6i(x1, . . . , xi\u22121, xi+1, . . . , xk)\n\f\f\f\f\f\f\f\f\f\f\f\f\n,\nwhere \u03c8(x1, . . . , xk) = (\u22121)f(x1,...,xk)\u00b5(x1, . . . , xk) and the maximum ranges over all\nfunctions \u03c6i : X1 \u00d7 \u00b7 \u00b7 \u00b7 Xi\u22121 \u00d7 Xi+1 \u00d7 \u00b7 \u00b7 \u00b7 Xk \u2192{0, 1}, for i = 1, 2, . . . , k. Note that\nfor k = 2, this de\ufb01nition is identical to the one given previously for the two-party\nmodel. We put disc( f) = min\u00b5 disc\u00b5( f). We identify a function f : X1 \u00d7 \u00b7 \u00b7 \u00b7\u00d7 Xk \u2192\n{0, 1} with its communication tensor M(x1, . . . , xk) = (\u22121)f(x1,...,xk) and speak of\nthe discrepancy of M and f interchangeably (and likewise for other complexity\nmeasures, such as Rk( f)).\nDiscrepancy is di\ufb03cult to analyze as de\ufb01ned. Typically, one uses the following\nwell-known estimate, derived by repeated applications of the Cauchy-Schwartz\ninequality.\nTheorem 2.2 ([BNS92, CT93, Raz00]). Fix f : X1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Xk \u2192{0, 1} and a\ndistribution \u00b5 on X1 \u00d7\u00b7 \u00b7 \u00b7\u00d7 Xk. Put \u03c8(x1, . . . , xk) = (\u22121)f(x1,...,xk)\u00b5(x1, . . . , xk). Then\n disc\u00b5( f)\n|X1| \u00b7 \u00b7 \u00b7 |Xk|\n!2k\u22121\n\u2a7d\nE\nx0\n1\u2208X1\nx1\n1\u2208X1\n\u00b7 \u00b7 \u00b7\nE\nx0\nk\u22121\u2208Xk\u22121\nx1\nk\u22121\u2208Xk\u22121\n\f\f\f\f\f\f\f\f\nE\nxk\u2208Xk\nY\nz\u2208{0,1}k\u22121\n\u03c8(xz1\n1 , . . . , xzk\u22121\nk\u22121, xk)\n\f\f\f\f\f\f\f\f\n.\n7\nIn the case of k = 2 parties, there are other ways to estimate the discrepancy, e.g.,\nusing the spectral norm of a matrix.\nFor a function f : X1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Xk \u2192{0, 1} and a distribution \u00b5 over X1 \u00d7\n\u00b7 \u00b7 \u00b7 \u00d7 Xk, let Dk,\u00b5\n\u01eb ( f) denote the least cost of a deterministic protocol for f whose\nprobability of error with respect to \u00b5 is at most \u01eb. This quantity is known as the\n\u00b5-distributional complexity of f. Since a randomized protocol can be viewed as\na probability distribution over deterministic protocols, we immediately have that\nRk\n\u01eb( f) \u2a7emax\u00b5 Dk,\u00b5\n\u01eb ( f). We are now ready to state the discrepancy method.\nTheorem 2.3 (Discrepancy method; see [KN97]). For every f : X1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Xk \u2192\n{0, 1}, every distribution \u00b5 on X1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Xk, and every \u03b3 \u2208(0, 1],\nRk\n1/2\u2212\u03b3/2 \u2a7eDk,\u00b5\n1/2\u2212\u03b3/2( f) \u2a7elog2\n\u03b3\ndisc\u00b5( f).\nIn other words, a function with small discrepancy is hard to compute to any non-\ntrivial advantage over random guessing (let alone compute it to high accuracy). In\nthe case of k = 2 parties, discrepancy yields analogous lower bounds even in the\nquantum model, regardless of prior entanglement [Kre95, Kla01, LS07b].\n3\nThe Degree/Discrepancy Theorem\nThis section presents the author\u2019s Degree/Discrepancy Theorem, whose proof tech-\nnique is the foundation for much of the subsequent work surveyed in this arti-\ncle [She07b, Cha07, LS07, CA08, DP08].\nThe original motivation behind this result came from circuit complexity. A nat-\nural and well-studied computational model is that of a polynomial-size circuit of\nmajority gates. Research has shown that majority circuits of depth 2 and 3 already\npossess surprising computational power. Indeed, it is a long-standing open prob-\nlem [KP97] to exhibit a Boolean function that cannot be computed by a depth-3\nmajority circuit of polynomial size.\nAnother extensively studied model is that of polynomial-size constant-depth\ncircuits with and, or, not gates, denoted by AC0. Allender\u2019s classic result [All89]\nstates that every function in AC0 can be computed by a depth-3 majority circuit of\nquasipolynomial size. Krause and Pudl\u00b4ak [KP97, \u00a76] ask whether this simulation\ncan be improved, i.e., whether every function in AC0 can be computed by a depth-\n2 majority circuit of quasipolynomial size. We recently gave a strong negative\nanswer to this question:\n8\nTheorem 3.1 ([She07a]). There is a function F : {0, 1}n \u2192{0, 1}, explicitly given\nand computable by an AC0 circuit of depth 3, whose computation requires a ma-\njority vote of exp(\u2126(n1/5)) threshold gates.\nWe proved Theorem 3.1 by exhibiting an AC0 function with exponentially small\ndiscrepancy. All previously known functions with exponentially small discrep-\nancy (e.g., [GHR92, Nis93]) contained parity or majority as a subfunction and\ntherefore could not be computed in AC0. Buhrman et al. [BVW07] obtained, inde-\npendently of the author and with much di\ufb00erent techniques, another AC0 function\nwith exponentially small discrepancy, thereby also answering Krause and Pudl\u00b4ak\u2019s\nquestion.\n3.1\nBounding the Discrepancy via the Threshold Degree\nTo construct an AC0 function with small discrepancy, we developed in [She07a] a\nnovel technique for generating low-discrepancy functions, which we now describe.\nThis technique is not specialized in any way to AC0 but, rather, is based on the\nabstract notion of threshold degree.\nFor a Boolean function f : {0, 1}n \u2192{0, 1}, recall from Section 1 that its\nthreshold degree deg\u00b1( f) is the minimum degree of a polynomial p(x1, . . . , xn)\nwith p(x) > 0 \u21d4f(x) = 1 and p(x) < 0 \u21d4f(x) = 0. In many cases [MP88],\nit is straightforward to obtain strong lower bounds on the threshold degree. Since\nthe threshold degree is a measure of the complexity of a given Boolean function,\nit is natural to wonder whether it can yield lower bounds on communication in a\nsuitable setting. As we prove in [She07a], this intuition turns out to be correct for\nevery f.\nMore precisely, \ufb01x a Boolean function f : {0, 1}n \u2192{0, 1} with threshold\ndegree d. Let N be a given integer, N \u2a7en. In [She07a], we introduced the two-\nparty communication problem of computing\nf(x|V),\nwhere the Boolean string x \u2208{0, 1}N is Alice\u2019s input and the set V \u2282{1, 2, . . . , N}\nof size |V| = n is Bob\u2019s input. The symbol x|V stands for the projection of x onto\nthe indices in V, in other words, x|V = (xi1, xi2, . . . , xin) \u2208{0, 1}n, where i1 < i2 <\n\u00b7 \u00b7 \u00b7 < in are the elements of V. Intuitively, this problem models a situation when\nAlice and Bob\u2019s joint computation depends on only n of the inputs x1, x2, . . . , xN.\nAlice knows the values of all the inputs x1, x2, . . . , xN but does not know which n\nof them are relevant. Bob, on the other hand, knows which n inputs are relevant\nbut does not know their values. As one would hope, it turns out that d is a lower\nbound on the communication requirements of this problem:\n9\nTheorem 3.2 (Degree/Discrepancy Theorem [She07a]). Let f : {0, 1}n \u2192{0, 1} be\ngiven with threshold degree d \u2a7e1. Let N be a given integer, N \u2a7en. De\ufb01ne F =\n[ f(x|V)]x,V, where the rows are indexed by x \u2208{0, 1}N and columns by V \u2208\n\u0010[N]\nn\n\u0011\n.\nThen\ndisc(F) \u2a7d\n 4en2\nNd\n!d/2\n.\nTo our knowledge, Theorem 3.2 is the \ufb01rst use of the threshold degree to prove\ncommunication lower bounds. Given a function f with threshold degree d, The-\norem 3.2 generates a communication problem with discrepancy at most 2\u2212d (by\nsetting N \u2a7e16en2/d). This exponentially small discrepancy immediately gives an\n\u2126(d) lower bound on communication in a variety of models (deterministic, nonde-\nterministic, randomized, quantum with and without entanglement; see Section 2.2).\nMoreover, the resulting lower bounds on communication remain valid when Alice\nand Bob merely seek to predict the answer with nonnegligible advantage, a critical\naspect for lower bounds against threshold circuits.\nWe will give a detailed proof of the Degree/Discrepancy Theorem in the next\nsubsection. For now we will brie\ufb02y sketch how we used it in [She07a] to prove the\nmain result of that paper, Theorem 3.1 above, on the existence of an AC0 function\nthat requires a depth-2 majority circuit of exponential size. Consider the function\nf(x) =\nm\n_\ni=1\n4m2\n^\nj=1\nxij,\nfor which Minsky and Papert [MP88] showed that deg\u00b1( f) = m. Since f has high\nthreshold degree, an application of Theorem 3.2 to f yields a communication prob-\nlem with low discrepancy. This communication problem itself can be viewed as an\nAC0 circuit of depth 3. Recalling that its discrepancy is exponentially small, we\nconclude that it cannot be computed by a depth-2 majority circuit of subexponen-\ntial size.\n3.2\nProof of the Degree/Discrepancy Theorem\nA key ingredient in our proof is the following dual characterization of the threshold\ndegree, which is a classical result known in greater generality as Gordan\u2019s Trans-\nposition Theorem [Sch98, \u00a77.8]:\nTheorem 3.3. Let f : {0, 1}n \u2192{0, 1} be arbitrary, d a nonnegative integer. Then\nexactly one of the following holds: (1) f has threshold degree at most d; (2) there is\na distribution \u00b5 over {0, 1}n such that Ex\u223c\u00b5[(\u22121)f(x)\u03c7S (x)] = 0 for |S | = 0, 1, . . . , d.\n10\nTheorem 3.3 follows from linear-programming duality. We will also make the\nfollowing simple observation.\nObservation 3.4. Let \u03ba(x) be a probability distribution on {0, 1}r. Fix i1, . . . , ir\n\u2208{1, 2, . . . , r}. Then P\nx\u2208{0,1}r \u03ba(xi1, . . . , xir) \u2a7d2r\u2212|{i1,...,ir}|, where |{i1, . . . , ir}| denotes\nthe number of distinct integers among i1, . . . , ir.\nWe are now ready for the proof of the Degree/Discrepancy Theorem.\nTheorem 3.2 (Restated from p. 10). Let f : {0, 1}n \u2192{0, 1} be given with thresh-\nold degree d \u2a7e1. Let N be a given integer, N \u2a7en. De\ufb01ne F = [ f(x|V)]x,V, where\nthe rows are indexed by x \u2208{0, 1}N and columns by V \u2208\n\u0010[N]\nn\n\u0011\n. Then\ndisc(F) \u2a7d\n 4en2\nNd\n!d/2\n.\nProof [She07a]. Let \u00b5 be a probability distribution over {0, 1}n with respect to\nwhich Ez\u223c\u00b5[(\u22121)f(z)p(z)] = 0 for every real-valued function p of d \u22121 or fewer\nof the variables z1, . . . , zn. The existence of \u00b5 is assured by Theorem 3.3. We will\nanalyze the discrepancy of F with respect to the distribution\n\u03bb(x, V) = 2\u2212N+n\n N\nn\n!\u22121\n\u00b5(x|V).\nDe\ufb01ne \u03c8 : {0, 1}n \u2192R by \u03c8(z) = (\u22121)f(z)\u00b5(z). By Theorem 2.2,\ndisc\u03bb(F)2 \u2a7d4n E\nV,W |\u0393(V, W)|,\n(3.1)\nwhere we put \u0393(V, W) = Ex[\u03c8(x|V)\u03c8(x|W)]. To analyze this expression, we prove\ntwo key claims.\nClaim 3.5. Assume that |V \u2229W| \u2a7dd \u22121. Then \u0393(V, W) = 0.\nProof. The claim is immediate from the fact that the Fourier transform of \u03c8 is\nsupported on characters of order d and higher. For completeness, we will now give\na more detailed and elementary explanation. Assume for notational convenience\nthat V = {1, 2, . . . , n}. Then\n\u0393(V, W) = E\nx [\u00b5(x1, . . . , xn)(\u22121)f(x1,...,xn)\u03c8(x|W)]\n= 1\n2N\nX\nx1,...,xn\n\u00b5(x1, . . . , xn)(\u22121)f(x1,...,xn)\nX\nxn+1,...,xN\n\u03c8(x|W)\n= 1\n2N\nE\n(x1,...,xn)\u223c\u00b5\n\uf8ee\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0(\u22121)f(x1,...,xn) \u00b7\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nX\nxn+1,...,xN\n\u03c8(x|W)\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n|               {z               }\n\u2217\n\uf8f9\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb.\n11\nSince |V \u2229W| \u2a7dd \u22121, the starred expression is a real-valued function of at most\nd \u22121 variables. The claim follows by the de\ufb01nition of \u00b5.\n\u25a1\nClaim 3.6. Assume that |V \u2229W| = i. Then |\u0393(V, W)| \u2a7d2i\u22122n.\nProof. The claim is immediate from Observation 3.4. For completeness, we will\ngive a more detailed explanation. For notational convenience, assume that\nV = {1, 2, . . . , n},\nW = {1, 2, . . . , i} \u222a{n + 1, n + 2, . . . , n + (n \u2212i)}.\nWe have:\n|\u0393(V, W)| \u2a7dE\nx [|\u03c8(x|V)\u03c8(x|W)|]\n=\nE\nx1,...,x2n\u2212i[\u00b5(x1, . . . , xn)\u00b5(x1, . . . , xi, xn+1, . . . , x2n\u2212i)]\n\u2a7d\nE\nx1,...,xn[\u00b5(x1, . . . , xn)]\n|                   {z                   }\n=2\u2212n\n\u00b7 max\nx1,...,xi\nE\nxn+1,...,x2n\u2212i[\u00b5(x1, . . . , xk, xn+1, . . . , x2n\u2212i)]\n|                                              {z                                              }\n\u2a7d2\u2212(n\u2212i)\n.\nThe bounds 2\u2212n and 2\u2212(n\u2212i) follow because \u00b5 is a probability distribution.\n\u25a1\nIn view of Claims 3.5 and 3.6, inequality (3.1) simpli\ufb01es to\ndisc\u03bb(F)2 \u2a7d\nn\nX\ni=d\n2i P[|V \u2229W| = i],\nwhich completes the proof of Theorem 3.2 after some routine calculations.\n\u25a1\nThe discrepancy bound in Theorem 3.2 is not tight. In follow-up work (see\nSection 5.1), the author proved a substantially stronger bound using matrix-analytic\ntechniques. However, that matrix-analytic approach does not seem to extend to the\nmultiparty model, and as we will see later in Sections 6 and 7, all multiparty papers\nin this survey use adaptations of the analysis just presented.\n4\nThe Generalized Discrepancy Method\nAs we saw in Section 2.2, the discrepancy method is particularly strong in that\nit gives communication lower bounds not only for bounded-error protocols but\nalso for protocols with error vanishingly close to 1\n2. Ironically, this strength of the\ndiscrepancy method is also its weakness. For example, the disjointness function\ndisj(x, y) = Wn\ni=1(xi \u2227yi) has a simple low-cost protocol with error 1\n2 \u2212\u2126\n\u00101\nn\n\u0011\n.\n12\nAs a result, disjointness has high discrepancy, and no useful lower bounds can\nbe obtained for it via the discrepancy method.\nYet it is well-known that dis-\njointness has bounded-error communication complexity \u2126(n) in the randomized\nmodel [KS92, Raz92] and \u2126(\u221an) in the quantum model [Raz03].\nThe remainder of this survey (Sections 5\u20137) is concerned with bounded-\nerror communication.\nCrucial to this development is the generalized discrep-\nancy method, an ingenious extension of the traditional discrepancy method that\navoids the di\ufb03culty just cited.\nTo our knowledge, this idea originated in a\npaper by Klauck [Kla01, Thm. 4] and was reformulated in its current form by\nRazborov [Raz03]. The development in [Kla01] and [Raz03] takes place in the\nquantum model of communication. However, the basic mathematical technique is\nin no way restricted to the quantum model, and we will focus here on a model-\nindependent version of the generalized discrepancy method from [She07b, \u00a72.4].\nSpeci\ufb01cally, consider an arbitrary communication model and let f : X \u00d7 Y \u2192\n{0, 1} be a given function whose communication complexity we wish to estimate.\nSuppose we can \ufb01nd a function h : X \u00d7 Y \u2192{0, 1} and a distribution \u00b5 on X \u00d7 Y\nthat satisfy the following two properties.\n1. Correlation of f and h. The functions f and h are well correlated under \u00b5:\nE\n(x,y)\u223c\u00b5\nh\n(\u22121)f(x,y)+h(x,y)i\n\u2a7e\u01eb,\n(4.1)\nwhere \u01eb > 0 is typically a constant.\n2. Hardness of h. No low-cost protocol \u03a0 in the given model of communication\ncan compute h to a substantial advantage under \u00b5. Formally, if \u03a0 is a protocol\nin the given model with cost C, then\nE\n(x,y)\u223c\u00b5\nh\n(\u22121)h(x,y) E\nh\n(\u22121)\u03a0(x,y)ii\n\u2a7d2O(C)\u03b3,\n(4.2)\nwhere \u03b3 = o(1). The inner expectation in (4.2) is over the internal operation\nof the protocol on the \ufb01xed input (x, y).\nIf the above two conditions hold, we claim that any protocol in the given model that\ncomputes f with error at most \u01eb/3 on each input must have cost \u2126\n\u0010\nlog \u01eb\n\u03b3\n\u0011\n. Indeed,\nlet \u03a0 be a protocol with P[\u03a0(x, y) , f(x, y)] \u2a7d\u01eb/3 for all x, y. Then standard\nmanipulations reveal:\nE\n(x,y)\u223c\u00b5\nh\n(\u22121)h(x,y) E\nh\n(\u22121)\u03a0(x,y)ii\n\u2a7e\nE\n(x,y)\u223c\u00b5\nh\n(\u22121)f(x,y)+h(x,y)i\n\u22122 \u00b7 \u01eb\n3\n(4.1)\n\u2a7e\n\u01eb\n3.\nIn view of (4.2), this shows that \u03a0 must have cost \u2126\n\u0010\nlog \u01eb\n\u03b3\n\u0011\n.\n13\nThe above framework from [She07b] is meant to emphasize the basic mathe-\nmatical technique in question, which is independent of the communication model.\nIndeed, the communication model enters the picture only in (4.2). It is here that\nthe analysis must exploit the particularities of the model. To place an upper bound\non the advantage under \u00b5 in the quantum model with entanglement, one considers\nthe quantity \u2225K\u2225\u221a|X| |Y|, where K = [(\u22121)h(x,y)\u00b5(x, y)]x,y. In the randomized model\nand the quantum model without entanglement, the quantity to estimate happens to\nbe disc\u00b5(h). (In fact, Linial and Shraibman [LS07b] recently showed that disc\u00b5(h)\nalso works in the quantum model with entanglement.)\nFor future reference, we now record a quantitative version of the generalized\ndiscrepancy method for the quantum model.\nTheorem 4.1 ([She07b], implicit in [Raz03, SZ07]). Let X, Y be \ufb01nite sets and\nf : X \u00d7 Y \u2192{0, 1} a given function. Let K = [Kxy]x\u2208X, y\u2208Y be any real matrix with\n\u2225K\u22251 = 1. Then for each \u01eb > 0,\n4Q\u01eb(f) \u2a7e4Q\u2217\n\u01eb(f) \u2a7e\u27e8F, K\u27e9\u22122\u01eb\n3 \u2225K\u2225\u221a|X| |Y|\n,\nwhere F =\nh\n(\u22121)f(x,y)i\nx\u2208X, y\u2208Y.\nObserve that Theorem 4.1 uses slightly more succinct notation (matrix vs. function;\nweighted sum vs. expectation) but is equivalent to the abstract formulation above.\nSo far, we have focused on two-party communication. This discussion extends\nessentially word-for-word to the multiparty model, with discrepancy serving once\nagain as the natural measure of the advantage attainable by low-cost protocols.\nThis extension was formalized by Lee and Shraibman [LS07, Thms. 6, 7] and in-\ndependently by Chattopadhyay and Ada [CA08, Lem. 3.2], who proved (4.3) and\n(4.4) below, respectively:\nTheorem 4.2 (cf. [LS07, CA08]). Fix F : X1\u00d7\u00b7 \u00b7 \u00b7\u00d7Xk \u2192{\u22121, +1} and \u01eb \u2208[0, 1/2).\nThen\n2Rk\n\u01eb(F) \u2a7e(1 \u2212\u01eb) max\nH,P\n\uf8f1\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f3\n\u27e8H \u25e6P, F\u27e9\u2212\n1\n1\u2212\u01eb \u01eb\ndiscP(H)\n\uf8fc\uf8f4\uf8f4\uf8fd\uf8f4\uf8f4\uf8fe\n(4.3)\nand\n2Rk\n\u01eb(F) \u2a7emax\nH,P\n(\u27e8H \u25e6P, F\u27e9\u22122\u01eb\ndiscP(H)\n)\n,\n(4.4)\nwhere in both cases H ranges over sign tensors and P ranges over tensors with\nP \u2a7e0 and \u2225P\u22251 = 1.\n14\nProof. Fix an optimal \u01eb-error protocol \u03a0 for F. De\ufb01ne\n\u02dcF(x1, . . . , xk)\n=\nE[(\u22121)\u03a0(x1,...,xk)], where the expectation is over any internal randomization in \u03a0.\nLet \u03b4 \u2208(0, 1] be a parameter to be \ufb01xed later. Then\n2Rk\n\u01eb(F) discP(H) \u2a7e\u27e8H \u25e6P, \u02dcF\u27e9\n= \u03b4\n(\n\u27e8H \u25e6P, F\u27e9+\n*\nH \u25e6P, 1\n\u03b4\n\u02dcF \u2212F\n+)\n\u2a7e\u03b4\n(\n\u27e8H \u25e6P, F\u27e9\u22121\n\u03b4 max{|1 \u2212\u03b4 \u22122\u01eb|, 1 \u2212\u03b4}\n)\n.\nwhere the \ufb01rst inequality restates the original discrepancy method (Theorem 2.3).\nNow (4.3) and (4.4) follow by setting \u03b4 = 1 \u2212\u01eb and \u03b4 = 1, respectively.\n\u25a1\nThe proof in [CA08] is similar to the one just given for the special case \u03b4 = 1.\nThe proof in [LS07] is rather di\ufb00erent and works by de\ufb01ning a suitable norm and\npassing to its dual. The norm-based approach was employed earlier by Linial and\nShraibman [LS07b] and can be thought of as a purely analytic analogue of the\ngeneralized discrepancy method.\n5\nTwo-Party Bounded-Error Communication\nFor a function f : {0, 1}n \u2192R, recall from Section 1 that its \u01eb-approximate degree\ndeg\u01eb( f) is the least degree of a polynomial p(x1, . . . , xn) with | f(x) \u2212p(x)| \u2a7d\u01eb for\nall x \u2208{0, 1}n. We move on to discuss two recent papers on bounded-error com-\nmunication that use the notion of approximate degree to contribute strong lower\nbounds for rather broad classes of functions, subsuming Razborov\u2019s breakthrough\nwork on symmetric predicates [Raz03]. These lower bounds are valid not only in\nthe randomized model, but also in the quantum model (regardless of entanglement).\nThe setting in which to view these two works is Klauck and Razborov\u2019s gen-\neralized discrepancy method (see Sections 1 and 4). Let F be a sign matrix whose\nbounded-error quantum communication complexity is of interest. The quantum\nversion of this method (Theorem 4.1) states that to prove a communication lower\nbound for F, it su\ufb03ces to exhibit a real matrix K such that \u27e8F, K\u27e9is large but \u2225K\u2225\nis small. The importance of the generalized discrepancy method is that it makes\nit possible, in theory, to prove lower bounds for functions such as disjointness, to\nwhich the traditional discrepancy method (Theorem 2.3) does not apply.\nThe hard part, of course, is \ufb01nding the matrix K. Except in rather restricted\ncases [Kla01, Thm. 4], it was not known how to do it. As a result, the general-\nized discrepancy method was of limited practical use. (In particular, Razborov\u2019s\ncelebrated work [Raz03] did not use the generalized discrepancy method. Instead,\n15\nhe introduced a novel alternate technique that was restricted to symmetric func-\ntions.) This di\ufb03culty was overcome independently by Sherstov [She07b] and Shi\nand Zhu [SZ07], who used the dual characterization of the approximate degree to\nobtain the matrix K for a broad range of problems. To our knowledge, the work\nin [She07b] and [SZ07] is the \ufb01rst use of the dual characterization of the approxi-\nmate degree to prove communication lower bounds.\nThe speci\ufb01cs of these two works are very di\ufb00erent. The construction of K\nin [She07b], which we called the pattern matrix method for lower bounds on\nbounded-error communication, is built around a new matrix-analytic technique\n(the pattern matrix) inspired by the author\u2019s Degree/Discrepancy Theorem. The\nconstruction of K in [SZ07], the block-composition method, is based on the idea\nof hardness ampli\ufb01cation by composition. What unites them is use of the dual\ncharacterization of the approximate degree, given by the following theorem.\nTheorem 5.1 ([She07b, SZ07]). Fix \u01eb \u2a7e0. Let f : {0, 1}n \u2192R be given with\nd = deg\u01eb( f) \u2a7e1. Then there is a function \u03c8 : {0, 1}n \u2192R such that:\n\u02c6\u03c8(S ) = 0\nfor |S | < d,\nX\nz\u2208{0,1}n\n|\u03c8(z)| = 1,\nX\nz\u2208{0,1}n\n\u03c8(z) f(z) > \u01eb.\nTheorem 5.1 follows from linear-programming duality. We shall \ufb01rst cover the\ntwo papers individually in Sections 5.1 and 5.2 and then compare them in detail\nin Section 5.3.\n5.1\nThe Pattern Matrix Method\nThe setting for this work resembles that of the Degree/Discrepancy Theorem\nin [She07a] (see Section 3). Let N and n be positive integers, where n \u2a7dN/2.\nFor convenience, we will further assume that n | N. Fix an arbitrary function\nf : {0, 1}n \u2192{0, 1}. Consider the communication problem of computing\nf(x|V),\nwhere the bit string x \u2208{0, 1}N is Alice\u2019s input and the set V \u2282{1, 2, . . . , N} with\n|V| = n is Bob\u2019s input. As before, x|V denotes the projection of x onto the indices\nin V, i.e., x|V = (xi1, xi2, . . . , xin) \u2208{0, 1}n where i1 < i2 < \u00b7 \u00b7 \u00b7 < in are the elements\nof V.\n16\nThe similarities with [She07a], however, do not extend beyond this point. Un-\nlike that earlier work, we will actually study the easier communication problem\nin which Bob\u2019s input V is restricted to a rather special form. Namely, we will\nonly allow those sets V that contain precisely one element from each block in the\nfollowing partition of {1, 2, . . . , N}:\n\u001a\n1, 2, . . . , N\nn\n\u001b\n\u222a\n(N\nn + 1, . . . , 2N\nn\n)\n\u222a\u00b7 \u00b7 \u00b7 \u222a\n((n \u22121)N\nn\n+ 1, . . . , N\n)\n.\n(5.1)\nEven for this easier communication problem, we will prove a much stronger re-\nsult than what would have been possible in the original setting with the methods\nof [She07a]. In particular, we will considerably improve the Degree/Discrepancy\nTheorem from [She07a] along the way. The main results of this work are as fol-\nlows.\nTheorem 5.2 ([She07b]). Any classical or quantum protocol, with or without prior\nentanglement, that computes f(x|V) with error probability at most 1/5 on each\ninput has communication cost at least\n1\n4 deg1/3( f) \u00b7 log\n\u0016 N\n2n\n\u0017\n\u22122.\nIn view of the restricted form of Bob\u2019s inputs, we can restate Theorem 5.2 in\nterms of function composition. Setting N = 4n for concreteness, we have:\nCorollary 5.3 ([She07b]). Let f : {0, 1}n \u2192{0, 1} be given. De\ufb01ne F : {0, 1}4n \u00d7\n{0, 1}4n \u2192{0, 1} by\nF(x, y) = f\n\u0010\nx1y1\n\u2228\nx2y2\n\u2228\nx3y3\n\u2228\nx4y4 ,\nx5y5\n\u2228\nx6y6\n\u2228\nx7y7\n\u2228\nx8y8 ,\n...\nx4n\u22123y4n\u22123\n\u2228\nx4n\u22122y4n\u22122\n\u2228\nx4n\u22121y4n\u22121\n\u2228\nx4ny4n\n\u0011\n,\nwhere xiyi = (xi \u2227yi). Any classical or quantum protocol, with or without prior\nentanglement, that computes F(x, y) with error probability at most 1/5 on each\ninput has cost at least 1\n4 deg1/3( f) \u22122.\nWe now turn to the proof. Let V(N, n) denote the set of Bob\u2019s inputs, i.e., the\nfamily of subsets V \u2286[N] that have exactly one element in each of the blocks of the\npartition (5.1). Clearly, |V(N, n)| = (N/n)n. We will be working with the following\nfamily of matrices.\n17\nDe\ufb01nition 5.4 (Pattern matrix [She07b]). For \u03c6 : {0, 1}n \u2192R, the (N, n, \u03c6)-pattern\nmatrix is the real matrix A given by\nA =\nh\n\u03c6(x|V \u2295w)\ni\nx\u2208{0,1}N, (V,w)\u2208V(N,n)\u00d7{0,1}n .\nIn words, A is the matrix of size 2N by 2n(N/n)n whose rows are indexed by\nstrings x \u2208{0, 1}N, whose columns are indexed by pairs (V, w) \u2208V(N, n) \u00d7 {0, 1}n,\nand whose entries are given by Ax,(V,w) = \u03c6(x|V \u2295w). The logic behind the term\n\u201cpattern matrix\u201d is as follows: a mosaic arises from repetitions of a pattern in the\nsame way that A arises from applications of \u03c6 to various subsets of the variables.\nOur intermediate goal will be to determine the spectral norm of any given pat-\ntern matrix A. Toward that end, we will actually end up determining every singular\nvalue of A and its multiplicity. Our approach will be to represent A as the sum of\nsimpler matrices and analyze them instead. For this to work, we need to be able to\nreconstruct the singular values of A from those of the simpler matrices. Just when\nthis can be done is the subject of the following lemma from [She07b].\nLemma 5.5 (Singular values of a matrix sum [She07b]). Let A, B be real matrices\nwith ABT = 0 and ATB = 0. Then the nonzero singular values of A + B, counting\nmultiplicities, are \u03c31(A), . . . , \u03c3rank A(A), \u03c31(B), . . . , \u03c3rank B(B).\nWe are ready to analyze the singular values of a pattern matrix.\nTheorem 5.6 (Singular values of a pattern matrix [She07b]). Let \u03c6 : {0, 1}n \u2192R\nbe given. Let A be the (N, n, \u03c6)-pattern matrix. Then the nonzero singular values\nof A, counting multiplicities, are:\n[\nS :\u02c6\u03c6(S ),0\n\uf8f1\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f3\nr\n2N+n\n\u0012N\nn\n\u0013n\n\u00b7 |\u02c6\u03c6(S )|\n\u0012 n\nN\n\u0013|S |/2\n,\nrepeated\n\u0012N\nn\n\u0013|S |\ntimes\n\uf8fc\uf8f4\uf8f4\uf8fd\uf8f4\uf8f4\uf8fe.\nIn particular,\n\u2225A\u2225=\nr\n2N+n\n\u0012N\nn\n\u0013n\nmax\nS \u2286[n]\n(\n|\u02c6\u03c6(S )|\n\u0012 n\nN\n\u0013|S |/2)\n.\nProof [She07b]. For each S \u2286[n], let AS be the (N, n, \u03c7S )-pattern matrix. Then\nA = P\nS \u2286[n] \u02c6\u03c6(S )AS . For any S, T \u2286[n] with S , T, a calculation reveals that\nAS AT\nT = 0 and AT\nS AT = 0. By Lemma 5.5, this means that the nonzero singular\nvalues of A are the union of the nonzero singular values of all \u02c6\u03c6(S )AS , counting\nmultiplicities. Therefore, the proof will be complete once we show that the only\nnonzero singular value of AT\nS AS is 2N+n(N/n)n\u2212|S |, with multiplicity (N/n)|S |.\n18\nFor this, it is convenient to write AT\nS AS as the Kronecker product\nAT\nS AS\n=\n[\u03c7S (w)\u03c7S (w\u2032)]w,w\u2032 \u2297\n\uf8ee\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\nX\nx\u2208{0,1}N\n\u03c7S (x|V) \u03c7S (x|V\u2032)\n\uf8f9\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\nV,V\u2032\n.\nThe \ufb01rst matrix in this factorization has rank 1 and entries \u00b11, which means that\nits only nonzero singular value is 2n with multiplicity 1. The other matrix, call\nit M, is permutation-similar to 2N diag(J, J, . . . , J), where J is the all-ones square\nmatrix of order (N/n)n\u2212|S |. This means that the only nonzero singular value of M\nis 2N(N/n)n\u2212|S | with multiplicity (N/n)|S |. It follows from elementary properties of\nthe Kronecker product that the spectrum of AT\nS AS is as desired.\n\u25a1\nWe are now prepared to formulate and prove the pattern matrix method for\nlower bounds on bounded-error communication, which gives strong lower bounds\nfor every pattern matrix generated by a Boolean function with high approximate\ndegree. Theorem 5.2 and its corollary will fall out readily as consequences.\nTheorem 5.7 (Pattern matrix method [She07b]). Let F be the (N, n, f)-pattern\nmatrix, where f : {0, 1}n \u2192{0, 1} is given. Put d = deg1/3( f). Then\nQ1/5(F) \u2a7eQ\u2217\n1/5(F) > 1\n4 d log\n\u0012N\nn\n\u0013\n\u22122.\nProof [She07b]. De\ufb01ne f \u2217: {0, 1}n \u2192{\u22121, +1} by f \u2217(z) = (\u22121)f(z). Then it is easy\nto verify that deg2/3( f \u2217) = d. By Theorem 5.1, there is a function \u03c8 : {0, 1}n \u2192R\nsuch that:\n\u02c6\u03c8(S ) = 0\nfor |S | < d,\n(5.2)\nX\nz\u2208{0,1}n\n|\u03c8(z)| = 1,\n(5.3)\nX\nz\u2208{0,1}n\n\u03c8(z) f \u2217(z) > 2\n3.\n(5.4)\nLet M be the (N, n, f \u2217)-pattern matrix. Let K be the (N, n, 2\u2212N(N/n)\u2212n\u03c8)-pattern\nmatrix. Immediate consequences of (5.3) and (5.4) are:\n\u2225K\u22251 = 1,\n\u27e8K, M\u27e9> 2\n3.\n(5.5)\nOur last task is to calculate \u2225K\u2225. By (5.3) and Proposition 2.1,\nmax\nS \u2286[n] | \u02c6\u03c8(S )| \u2a7d2\u2212n.\n(5.6)\n19\nTheorem 5.6 yields, in view of (5.2) and (5.6):\n\u2225K\u2225\u2a7d\n\u0012 n\nN\n\u0013d/2  \n2N+n \u0012N\nn\n\u0013n!\u22121/2\n.\n(5.7)\nThe desired lower bounds on quantum communication now follow directly from\n(5.5) and (5.7) by the generalized discrepancy method (Theorem 4.1).\n\u25a1\nRemark 5.8. In the proof of Theorem 5.7, we bounded \u2225K\u2225using the subtle cal-\nculations of the spectrum of a pattern matrix. Another possibility would be to\nbound \u2225K\u2225precisely in the same way that we bounded the discrepancy in the De-\ngree/Discrepancy Theorem (see Section 3). This, however, would result in polyno-\nmially weaker lower bounds on communication.\nTheorem 5.7 immediately implies Theorem 5.2 above and its corollary:\nProof of Theorem 5.2 [She07b]. The\n\u0010j N\n2n\nk\nn, n, f\n\u0011\n-pattern matrix occurs as a sub-\nmatrix of [ f(x|V)]x\u2208{0,1}N,V\u2208V(N,n).\n\u25a1\nImproved Degree/Discrepancy Theorem.\nWe will mention a few more appli-\ncations of this work. The \ufb01rst of these is an improved version of the author\u2019s\nDegree/Discrepancy Theorem (Theorem 3.2).\nTheorem 5.9 ([She07b]). Let F be the (N, n, f)-pattern matrix, where f : {0, 1}n \u2192\n{0, 1} has threshold degree d. Then disc(F) \u2a7d(n/N)d/2.\nThe proof is similar to the proof of the pattern matrix method. Theorem 5.9 im-\nproves considerably on the original Degree/Discrepancy Theorem. To illustrate,\nconsider f(x) = Wm\ni=1\nVm2\nj=1 xij, a function on n = m3 variables. Applying Theo-\nrem 5.9 to f leads to an exp(\u2212\u0398(n1/3)) upper bound on the discrepancy of AC0, im-\nproving on the previous bound of exp(\u2212\u0398(n1/5)) from [She07a]. The exp(\u2212\u0398(n1/3))\nbound is also the bound obtained by Buhrman et al. [BVW07] independently of the\nauthor [She07a, She07b], using a di\ufb00erent function and di\ufb00erent techniques.\nRazborov\u2019s Lower Bounds for Symmetric Functions.\nAs another application,\nwe are able to give an alternate proof of Razborov\u2019s breakthrough result on the\nquantum communication complexity of symmetric functions [Raz03]. Consider a\ncommunication problem in which Alice has a string x \u2208{0, 1}n, Bob has a string\ny \u2208{0, 1}n, and their objective is to compute\nD(|x \u2227y|)\n20\nfor some predicate D : {0, 1, . . . , n} \u2192{0, 1} \ufb01xed in advance. This general setting\nencompasses several familiar functions, such as disjointness (determining if x and\ny intersect) and inner product modulo 2 (determining if x and y intersect in an odd\nnumber of positions).\nAs it turns out, the hardness of this general communication problem depends on\nwhether D changes value close to the middle of the range {0, 1, . . . , n}. Speci\ufb01cally,\nde\ufb01ne \u21130(D) \u2208{0, 1, . . . , \u230an/2\u230b} and \u21131(D) \u2208{0, 1, . . . , \u2308n/2\u2309} to be the smallest\nintegers such that D is constant in the range [\u21130(D), n\u2212\u21131(D)]. Razborov established\noptimal lower bounds on the quantum communication complexity of every function\nof the form D(|x \u2227y|):\nTheorem 5.10 (Razborov [Raz03]). Let D : {0, 1, . . . , n} \u2192{0, 1} be an arbitrary\npredicate. Put f(x, y) = D(|x \u2227y|). Then\nQ1/3( f) \u2a7eQ\u2217\n1/3( f) \u2a7e\u2126\n\u0010p\nn\u21130(D) + \u21131(D)\n\u0011\n.\nIn particular, disjointness has quantum communication complexity \u2126(\u221an), regard-\nless of entanglement. Prior to Razborov\u2019s result, the best lower bound [BW01,\nASTS+03] for disjointness was only \u2126(log n).\nIn [She07b], we give a new proof of Razborov\u2019s Theorem 5.10 using a straight-\nforward application of the pattern matrix method.\n5.2\nThe Block Composition Method\nGiven functions f : {0, 1}n \u2192{0, 1} and g : {0, 1}k \u00d7 {0, 1}k \u2192{0, 1}, let f \u25e6gn\ndenote the composition of f with n independent copies of g. More formally, the\nfunction f \u25e6gn : {0, 1}nk \u00d7 {0, 1}nk \u2192{0, 1} is given by\n( f \u25e6gn)(x, y) = f(. . . , g(x(i), y(i)), . . . ),\nwhere x = (. . . , x(i), . . . ) \u2208{0, 1}nk and y = (. . . , y(i), . . . ) \u2208{0, 1}nk.\nThis section presents Shi and Zhu\u2019s block composition method [SZ07], which\ngives a lower bound on the communication complexity of f \u25e6gn in terms of certain\nproperties of f and g. The relevant property of f is simply its approximate degree.\nThe relevant property of g is its spectral discrepancy, formalized next.\nDe\ufb01nition 5.11 (Spectral discrepancy [SZ07]). Given g : {0, 1}k \u00d7 {0, 1}k \u2192{0, 1},\nits spectral discrepancy \u03c1(g) is the least \u03c1 \u2a7e0 for which there exist sets A, B \u2286\n21\n{0, 1}k and a distribution \u00b5 on A \u00d7 B such that\n\r\r\r\r\r\nh\n\u00b5(x, y)(\u22121)g(x,y)i\nx\u2208A,y\u2208B\n\r\r\r\r\r \u2a7d\n\u03c1\n\u221a|A| |B|\n,\n(5.8)\n\r\r\r\r\r\nh\n\u00b5(x, y)\ni\nx\u2208A,y\u2208B\n\r\r\r\r\r \u2a7d\n1 + \u03c1\n\u221a|A| |B|\n,\n(5.9)\nand\nX\n(x,y)\u2208A\u00d7B\n\u00b5(x, y)(\u22121)g(x,y) = 0.\n(5.10)\nIn view of (5.8) alone, the spectral discrepancy \u03c1(g) is an upper bound on the\ndiscrepancy disc(g). The key additional requirement (5.9) is satis\ufb01ed, for example,\nby doubly stochastic matrices [HJ86, \u00a78.7]: if A = B and all row and column sums\nin [\u00b5(x, y)]x\u2208A,y\u2208A are 1/|A|, then \u2225[\u00b5(x, y)]x\u2208A,y\u2208A\u2225= 1/|A|.\nAs an illustration, consider the familiar function inner product modulo 2, given\nby ipk(x, y) =\nLk\ni=1(xi \u2227yi).\nProposition 5.12 ([SZ07]). The function ipk has \u03c1(ipk) \u2a7d1/\n\u221a\n2k \u22121.\nProof [SZ07]. Take \u00b5 to be the uniform distribution over A\u00d7B, where A = {0, 1}k\\\n{0k} and B = {0, 1}k.\n\u25a1\nWe are prepared to state the general method.\nTheorem 5.13 (Block composition method [SZ07]). Fix f : {0, 1}n \u2192{0, 1} and\ng : {0, 1}k \u00d7 {0, 1}k \u2192{0, 1}. Put d = deg1/3( f) and \u03c1 = \u03c1(g). If \u03c1 \u2a7dd/(2en), then\nQ( f \u25e6gn) \u2a7eQ\u2217( f \u25e6gn) = \u2126(d).\nProof (adapted from [SZ07]). Fix sets A, B \u2286{0, 1}k and a distribution \u00b5 on A \u00d7 B\nwith respect to which \u03c1 = \u03c1(g) is achieved. De\ufb01ne f \u2217: {0, 1}n \u2192{\u22121, +1} by\nf \u2217(z) = (\u22121)f(z). Then one readily veri\ufb01es that deg2/3( f \u2217) = d. By Theorem 5.1,\nthere exists \u03c8 : {0, 1}n \u2192R such that\n\u02c6\u03c8(S ) = 0\nfor |S | < d,\n(5.11)\nX\nz\u2208{0,1}n\n|\u03c8(z)| = 1,\n(5.12)\nX\nz\u2208{0,1}n\n\u03c8(z) f \u2217(z) > 2\n3.\n(5.13)\n22\nDe\ufb01ne matrices\nF =\nh\nf \u2217(. . . , g(x(i), y(i)), . . . )\ni\nx,y,\nK =\n\uf8ee\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f02n\u03c8(. . . , g(x(i), y(i)), . . . )\nn\nY\ni=1\n\u00b5(x(i), y(i))\n\uf8f9\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\nx,y\n,\nwhere in both cases the row index x = (. . . , x(i), . . . ) ranges over An and the column\nindex y = (. . . , y(i), . . . ) ranges over Bn. In view of (5.10) and (5.13),\n\u27e8F, K\u27e9> 2\n3.\n(5.14)\nWe proceed to bound \u2225K\u2225. Put\nMS =\n\uf8ee\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\nY\ni\u2208S\n(\u22121)g(x(i),y(i)) \u00b7\nn\nY\ni=1\n\u00b5(x(i), y(i))\n\uf8f9\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\nx,y\n,\nS \u2286[n].\nThen (5.8) and (5.9) imply, in view of the tensor structure of MS , that\n\u2225MS \u2225\u2a7d|A|\u2212n/2 |B|\u2212n/2 \u03c1|S |(1 + \u03c1)n\u2212|S |.\n(5.15)\nOn the other hand,\n\u2225K\u2225\u2a7d\nX\nS \u2286[n]\n2n| \u02c6\u03c8(S )| \u2225MS \u2225\n=\nX\n|S |\u2a7ed\n2n| \u02c6\u03c8(S )| \u2225MS \u2225\nby (5.11)\n\u2a7d\nX\n|S |\u2a7ed\n\u2225MS \u2225\nby (5.12) and Proposition 2.1\n\u2a7d|A|\u2212n/2 |B|\u2212n/2\nn\nX\ni=d\n n\ni\n!\n\u03c1i(1 + \u03c1)n\u2212i\nby (5.15).\nSince \u03c1 \u2a7dd/(2en), we further have\n\u2225K\u2225\u2a7d|A|\u2212n/2 |B|\u2212n/2 2\u2212\u0398(d).\n(5.16)\nIn view of (5.14) and (5.16), the desired lower bound on Q\u2217(F) now follows by the\ngeneralized discrepancy method (Theorem 4.1).\n\u25a1\nProposition 5.12 and Theorem 5.13 have the following consequence:\n23\nTheorem 5.14 ([SZ07]). Fix a function f : {0, 1}n \u2192{0, 1} and an integer k \u2a7e\n2 log2 n + 5. Then Q( f \u25e6ipn\nk) \u2a7eQ\u2217( f \u25e6ipn\nk) \u2a7e\u2126(deg1/3( f)).\nFor the disjointness function disjk(x, y) = Wk\ni=1(xi \u2227yi), Shi and Zhu prove that\n\u03c1(disjk) = O(1/k). Unlike Proposition 5.12, this fact requires a nontrivial proof\nusing Knuth\u2019s calculation of the eigenvalues of certain combinatorial matrices. In\nconjunction with Theorem 5.13, this upper bound on \u03c1(disjk) leads with some work\nto the following implication:\nTheorem 5.15 ([SZ07]). De\ufb01ne f : {0, 1}n\u00d7{0, 1}n \u2192{0, 1} by f(x, y) = D(|x\u2227y|),\nwhere D : {0, 1, . . . , n} \u2192{0, 1} is given. Then\nQ( f) \u2a7eQ\u2217( f) \u2a7e\u2126\n\u0010\nn1/3\u21130(D)2/3 + \u21131(D)\n\u0011\n.\nThe symbols \u21130(D) and \u21131(D) have their meaning from Section 5.1. Theorem 5.15\nis of course a weaker version of Razborov\u2019s celebrated lower bounds for symmetric\nfunctions (Theorem 5.10), obtained with a di\ufb00erent proof.\n5.3\nPattern Matrix Method vs. Block Composition Method\nTo restate the block composition method,\nQ\u2217( f \u25e6gn) \u2a7e\u2126(deg1/3( f))\nprovided that\n\u03c1(g) \u2a7d\ndeg1/3( f)\n2en\n.\nThe key player in this method is the quantity \u03c1(g), which needs to be small. This\nposes two complications. First, the function g will generally need to depend on\nmany variables, from k = \u0398(log n) to k = n\u0398(1), which weakens the \ufb01nal lower\nbounds on communication (recall that \u03c1(g) \u2a7e2\u2212k always). For example, the lower\nbounds obtained in [SZ07] for symmetric functions are polynomially weaker than\nRazborov\u2019s optimal lower bounds (see Theorems 5.15 and 5.10, respectively).\nA second complication, as Shi and Zhu note, is that \u201cestimating the quantity\n\u03c1(g) is unfortunately di\ufb03cult in general\u201d [SZ07, \u00a74.1]. For example, re-proving\nRazborov\u2019s lower bounds reduces to estimating \u03c1(g) with g being the disjointness\nfunction. Shi and Zhu accomplish this using Hahn matrices, an advanced tool that\nis also the centerpiece of Razborov\u2019s own proof (Razborov\u2019s use of Hahn matrices\nis somewhat more demanding).\nThese complications do not arise in the pattern matrix method. For example, it\nimplies (by setting N = 2n in Theorem 5.7) that\nQ\u2217( f \u25e6gn) \u2a7e\u2126(deg1/3( f))\n24\nfor any function g : {0, 1}k\u00d7{0, 1}k \u2192{0, 1} such that the matrix [g(x, y)]x,y contains\nthe following submatrix, up to permutations of rows and columns:\n\uf8ee\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n1\n0\n1\n0\n1\n0\n0\n1\n0\n1\n1\n0\n0\n1\n0\n1\n\uf8f9\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n.\n(5.17)\nTo illustrate, one can take g to be\ng(x, y)\n=\nx1y1\n\u2228\nx2y2\n\u2228\nx3y3\n\u2228\nx4y4,\nor\ng(x, y)\n=\nx1y1y2\n\u2228\nx1 y1y2\n\u2228\nx2 y1 y2\n\u2228\nx2 y1 y2.\n(In particular, the pattern matrix method subsumes Theorem 5.14.) To summarize,\nthere is a simple function g on only k = 2 variables that works universally for all f.\nThis means no technical conditions to check, such as \u03c1(g), and no blow-up in the\nnumber of variables. As a result, in [She07b] we are able to re-prove Razborov\u2019s\noptimal lower bounds exactly. Moreover, the technical machinery involved is self-\ncontained and disjoint from Razborov\u2019s proof.\nWe have just seen that the pattern matrix method gives strong lower bounds for\nmany functions to which the block composition method does not apply. However,\nthis does not settle the exact relationship between the scopes of applicability of the\ntwo methods. Several natural questions arise. If a function g : {0, 1}k \u00d7 {0, 1}k \u2192\n{0, 1} has spectral discrepancy \u03c1(g) \u2a7d1\n2e, does the matrix [g(x, y)]x,y contain (5.17)\nas a submatrix, up to permutations of rows and columns? An a\ufb03rmative answer\nwould mean that the pattern matrix method has a strictly greater scope of applica-\nbility; a negative answer would mean that the block composition method works in\nsome situations where the pattern matrix method does not apply. If the answer is\nnegative, what can be said for \u03c1(g) = o(1) or \u03c1(g) = n\u2212\u0398(1)?\nAnother intriguing issue concerns multiparty communication. As we will see\nin Section 6, the pattern matrix method extends readily to the multiparty model.\nThis extension makes heavy use of the fact that the rows of a pattern matrix are\napplications of the same function to di\ufb00erent subsets of the variables. In the gen-\neral context of block composition (Section 5.2), it is unclear how to carry out this\nextension. It is inviting to explore a synthesis of the two methods in the multiparty\nmodel or another suitable context.\n6\nExtensions to the Multiparty Model\nIn this section, we present extensions of the Degree/Discrepancy Theorem and of\nthe pattern matrix method to the multiparty model. We start with some notation.\n25\nFix a function \u03c6 : {0, 1}n \u2192R and an integer N with n | N. De\ufb01ne the (k, N, n, \u03c6)-\npattern tensor as the k-argument function A : {0, 1}n(N/n)k\u22121\u00d7[N/n]n\u00d7\u00b7 \u00b7 \u00b7\u00d7[N/n]n \u2192\nR given by A(x, V1, . . . , Vk\u22121) = \u03c6(x|V1,...,Vk\u22121), where\nx|V1,...,Vk\u22121\ndef=\n\u0000x1,V1[1],...,Vk\u22121[1], . . . , xn,V1[n],...,Vk\u22121[n]\n\u0001 \u2208{0, 1}n\nand V j[i] denotes the ith element of the n-dimensional vector V j. (Note that we\nindex the string x by viewing it as a k-dimensional array of n\u00d7(N/n)\u00d7\u00b7 \u00b7 \u00b7\u00d7(N/n) =\nn(N/n)k\u22121 bits.) This de\ufb01nition generalizes the author\u2019s pattern matrices if one\nignores the \u2295operator (Section 5.1).\nWe are ready for the \ufb01rst result of this section, namely, an extension of the De-\ngree/Discrepancy Theorem (Theorem 3.2) to the multiparty model. This extension\nwas originally obtained by Chattopadhyay [Cha07, Lem. 2] for slightly di\ufb00erent\ntensors and has since been revisited in one form or another: [LS07, Thm. 19],\n[CA08, Lem. 4.2]. The proofs of these several versions are quite similar and are in\nclose correspondence with the original two-party case.\nTheorem 6.1 ([Cha07, LS07, CA08]). Let f : {0, 1}n \u2192{0, 1} be given with thresh-\nold degree d \u2a7e1. Let N be a given integer, n | N. Let F be the (k, N, n, f)-pattern\ntensor. If N \u2a7e4en2(k \u22121)22k\u22121/d, then disc(F) \u2a7d2\u2212d/2k\u22121.\nProof (adapted from [Cha07, LS07, CA08]). As\nin\nthe\nproof\nof\nthe\nDe-\ngree/Discrepancy Theorem, let \u00b5 be a probability distribution over {0, 1}n\nwith respect to which Ez\u223c\u00b5[(\u22121)f(z)p(z)] = 0 for every real-valued function p\nof d \u22121 or fewer of the variables z1, . . . , zn. The existence of \u00b5 is assured by\nTheorem 3.3. We will analyze the discrepancy of F with respect to the distribution\n\u03bb(x, V1, . . . , Vk\u22121) = 2\u2212n(N/n)k\u22121+n \u0012N\nn\n\u0013\u2212n(k\u22121)\n\u00b5(x|V1,...,Vk\u22121).\nDe\ufb01ne \u03c8 : {0, 1}n \u2192R by \u03c8(z) = (\u22121)f(z)\u00b5(z). By Theorem 2.2,\ndisc\u03bb(F)2k\u22121 \u2a7d2n2k\u22121 E\nV |\u0393(V)|,\n(6.1)\nwhere we put V = (V0\n1, V1\n1, . . . , V0\nk\u22121, V1\nk\u22121) and\n\u0393(V) = E\nx\n\uf8ee\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\u03c8\n\u0012\nx|V0\n1,V0\n2,...,V0\nk\u22121\n\u0013\n|              {z              }\n(\u2020)\nY\nz\u2208{0,1}k\u22121\\{0k\u22121}\n\u03c8\n\u0012\nx|Vz1\n1 ,Vz2\n2 ,...,V\nzk\u22121\nk\u22121\n\u0013\n|                                   {z                                   }\n(\u2021)\n\uf8f9\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb.\n26\nFor a \ufb01xed choice of V, de\ufb01ne sets\nA =\nn\n(i, V0\n1[i], . . . , V0\nk\u22121[i]) : i = 1, 2, . . . , n\no\n,\nB =\nn\n(i, Vz1\n1 [i], . . . , Vzk\u22121\nk\u22121 [i]) : i = 1, 2, . . . , n; z \u2208{0, 1}k\u22121 \\ {0k\u22121}\no\n.\nClearly, A and B are the sets of variables featured in the expressions (\u2020) and (\u2021)\nabove, respectively. To analyze \u0393(V), we prove two key claims analogous to those\nin the Degree/Discrepancy Theorem.\nClaim 6.2. Assume that |A \u2229B| \u2a7dd \u22121. Then \u0393(V) = 0.\nProof. Immediate from the fact that the Fourier transform of \u03c8 is supported on\ncharacters of order d and higher.\n\u25a1\nClaim 6.3. Assume that |A \u2229B| = i. Then |\u0393(V)| \u2a7d2i2k\u22121\u2212n2k\u22121.\nProof. Observation 3.4 shows that |\u0393(V)| \u2a7d2\u2212n2k\u221212n2k\u22121\u2212|A\u222aB|. Furthermore, it is\nstraightforward to verify that |A \u222aB| \u2a7en2k\u22121 \u2212|A \u2229B| 2k\u22121.\n\u25a1\nIn view of Claims 6.2 and 6.3, inequality (6.1) simpli\ufb01es to\ndisc\u03bb(F)2k\u22121 \u2a7d\nn\nX\ni=d\n2i2k\u22121 P[|A \u2229B| = i].\nIt remains to bound P[|A\u2229B| = i]. For a \ufb01xed element a, we have P[a \u2208B | a \u2208A] \u2a7d\n(k \u22121)n/N by the union bound. Moreover, given two distinct elements a, a\u2032 \u2208\nA, the corresponding events a \u2208B and a\u2032 \u2208B are independent.\nTherefore,\nP[|A \u2229B| = i] \u2a7d\n\u0010n\ni\n\u0011 \u0010 (k\u22121)n\nN\n\u0011i , which yields the desired bound on disc\u03bb(F).\n\u25a1\nRemark 6.4. Recall from Section 5.1 that the two-party Degree/Discrepancy The-\norem was considerably improved in [She07b] using matrix-analytic techniques.\nThose techniques, however, do not extend to the multiparty model. As a result,\nTheorem 6.1 that we have just presented does not subsume the improved De-\ngree/Discrepancy Theorem (Theorem 5.9).\nWe now present an adaptation of the pattern matrix method (Theorem 5.7) to\nthe multiparty model, obtained by Lee and Shraibman [LS07] and independently\nby Chattopadhyay and Ada [CA08]. The proof is closely analogous to the two-\nparty case. However, the spectral calculations for pattern matrices do not extend\nto the multiparty model, and one is forced to fall back on the less precise calcula-\ntions introduced in the Degree/Discrepancy Theorem (Theorem 3.2). In particular,\nthe result we are about to present does not subsume the two-party pattern matrix\nmethod.\n27\nTheorem 6.5 ([LS07, CA08]). Let f : {0, 1}n \u2192{0, 1} be given with deg1/3( f) =\nd \u2a7e1. Let N be a given integer, n | N. Let F be the (k, N, n, f)-pattern tensor. If\nN \u2a7e4en2(k \u22121)22k\u22121/d, then Rk(F) \u2a7e\u2126(d/2k).\nProof (adapted from [LS07, CA08]). De\ufb01ne f \u2217: {0, 1}n \u2192{\u22121, +1} by f \u2217(z) =\n(\u22121)f(z). Then it is easy to verify that deg2/3( f \u2217) = d. By Theorem 5.1, there is a\nfunction \u03c8 : {0, 1}n \u2192R such that:\n\u02c6\u03c8(S ) = 0\nfor |S | < d,\nX\nz\u2208{0,1}n\n|\u03c8(z)| = 1,\nX\nz\u2208{0,1}n\n\u03c8(z) f \u2217(z) > 2\n3.\n(6.2)\nFix a function h : {0, 1}n \u2192{\u22121, +1} and a distribution \u00b5 on {0, 1}n such\nthat \u03c8(z) \u2261h(x)\u00b5(x). Let H be the (k, N, n, h)-pattern tensor.\nLet P be the\n(k, N, n, 2\u2212n(N/n)k\u22121+n(N/n)\u2212n(k\u22121)\u00b5)-pattern tensor. Then P is a probability distri-\nbution. By (6.2),\n\u27e8H \u25e6P, F\u2217\u27e9> 2\n3,\n(6.3)\nwhere F\u2217is the (k, N, n, f \u2217)-pattern tensor. As we saw in the proof of Theorem 6.1,\ndiscP(H) \u2a7d2\u2212d/2k\u22121.\n(6.4)\nThe theorem now follows by the generalized discrepancy method (Theorem 4.2) in\nview of (6.3) and (6.4).\n\u25a1\nThe authors of [LS07] and [CA08] gave important applications of their work\nto the k-party randomized communication complexity of disjointness, improving\nit from \u2126(1\nk log n) to n\u2126(1/k)2\u2212O(2k). As a corollary, they separated the multiparty\ncommunication classes NPcc\nk and BPPcc\nk for k = (1\u2212o(1)) log2 log2 n parties. They\nalso obtained new results for Lov\u00b4asz-Schrijver proof systems, in light of the work\ndue to Beame, Pitassi, and Segerlind [BPS07].\n7\nSeparation of NPcc\nk and BPPcc\nk\nWe conclude this survey with a separation of NPcc\nk and BPPcc\nk for k = (1\u2212\u01eb) log2 n\nparties, due to David and Pitassi [DP08]. This is an exponential improvement over\nthe previous separation in [LS07, CA08]. The crucial insight in this new work is\nto rede\ufb01ne the projection operator x|V1,...,Vk\u22121 from Section 6 using the probabilistic\n28\nmethod. This removes the key bottleneck in the previous analyses [LS07, CA08].\nUnlike the previous work, however, this new approach no longer applies to dis-\njointness.\nWe start with some notation. Fix integers n, m with n > m. Let \u03c8 : {0, 1}m \u2192R\nbe a given function with P\nz\u2208{0,1}m |\u03c8(z)| = 1. Let d denote the least order of a\nnonzero Fourier coe\ufb03cient of \u03c8. Fix a Boolean function h : {0, 1}m \u2192{\u22121, +1} and\na distribution \u00b5 on {0, 1}m such that \u03c8(z) \u2261h(z)\u00b5(z). For a mapping \u03b1 : ({0, 1}n)k \u2192\n\u0010[n]\nm\n\u0011\n, de\ufb01ne a (k + 1)-party communication problem H\u03b1 : ({0, 1}n)k+1 \u2192{\u22121, +1}\nby H(x, y1, . . . , yk) = h(x|\u03b1(y1,...,yk)). Analogously, de\ufb01ne a distribution \u03bb\u03b1 on\n({0, 1}n)k+1 by \u03bb(x, y1, . . . , yk) = 2\u2212(k+1)n+m\u00b5(x|\u03b1(y1,...,yk)).\nTheorem 7.1 ([DP08]). Assume that n \u2a7e16em22k. Then for a uniformly random\nchoice of \u03b1 : ({0, 1}n)k \u2192\n\u0010[n]\nm\n\u0011\n,\nE\n\u03b1\nh\ndisc\u03bb\u03b1(H\u03b1)2ki\n\u2a7d2\u2212n/2 + 2\u2212d2k+1.\nProof (adapted from [DP08]). By Theorem 2.2,\ndisc\u03bb\u03b1(H\u03b1)2k \u2a7d2m2k E\nY |\u0393(Y)|,\n(7.1)\nwhere we put Y = (y0\n1, y1\n1, . . . , y0\nk, y1\nk) and\n\u0393(Y) = E\nx\n\uf8ee\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\nY\nz\u2208{0,1}k\n\u03c8\n\u0012\nx|\u03b1\n\u0010\ny\nz1\n1 ,y\nz2\n2 ,...,y\nzk\nk\n\u0011\n\u0013\uf8f9\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb.\nFor a \ufb01xed choice of Y, we will use the shorthand S z = \u03b1(yz1\n1 , . . . , yzk\nk ). To ana-\nlyze \u0393(Y), we prove two key claims analogous to those in the Degree/Discrepancy\nTheorem and in Theorem 6.1.\nClaim 7.2. Assume that | S S z| > m2k \u2212d2k\u22121. Then \u0393(Y) = 0.\nProof. If | S S z| > m2k\u2212d2k\u22121, then some S z must feature more than m\u2212d elements\nthat do not occur in S\nu,z S u. But this forces \u0393(Y) = 0 since the Fourier transform\nof \u03c8 is supported on characters of order d and higher.\n\u25a1\nClaim 7.3. For every Y,\n|\u0393(Y)| \u2a7d2\u2212| S S z|.\nProof. Immediate from Observation 3.4.\n\u25a1\n29\nIn view of (7.1) and Claims 7.2 and 7.3, we have\nE\n\u03b1\nh\ndisc\u03bb\u03b1(H\u03b1)2ki\n\u2a7d\nm2k\u2212m\nX\ni=d2k\u22121\n2i P\nY,\u03b1\n\u0014\f\f\f\f\n[\nS z\n\f\f\f\f = m2k \u2212i\n\u0015\n.\nIt remains to bound the probabilities in the last expression. With probability at\nleast 1 \u2212k2\u2212n over the choice of Y, the strings y0\n1, y0\n1 . . . , y0\nk, y1\nk will all be distinct.\nConditioning on this event, the fact that \u03b1 is chosen uniformly at random means that\nthe 2k sets S z are distributed independently and uniformly over\n\u0010[n]\nm\n\u0011\n. A calculation\nnow reveals that\nP\nY,\u03b1\n\u0014\f\f\f\f\n[\nS z\n\f\f\f\f = m2k \u2212i\n\u0015\n\u2a7dk2\u2212n +\n m2k\ni\n!  m2k\nn\n!i\n\u2a7dk2\u2212n + 8\u2212i.\n\u25a1\nWe are ready to present the separation of NPcc\nk and BPPcc\nk .\nTheorem 7.4 (Separation of NPcc\nk and BPPcc\nk [DP08]). Let k \u2a7d(1\u2212\u01eb) log2 n, where\n\u01eb > 0 is a given constant. Then there exists a function F\u03b1 : ({0, 1}n)k+1 \u2192{\u22121, +1}\nwith Nk+1(F\u03b1) = O(log n) but Rk+1(F\u03b1) = n\u2126(1).\nProof (adapted from [DP08]). Let m = \u230an\u03b6\u230bfor a su\ufb03ciently small constant \u03b6 =\n\u03b6(\u01eb) > 0. As usual, de\ufb01ne orm : {0, 1}m \u2192{\u22121, +1} by orm(z) = 1 \u21d4z = 0m.\nIt is known [NS92, Pat92] that deg1/3(orm) = \u0398(\u221am). As a result, Theorem 5.1\nguarantees the existence of a function \u03c8 : {0, 1}m \u2192R such that:\n\u02c6\u03c8(S ) = 0\nfor |S | < \u0398(\u221am),\nX\nz\u2208{0,1}m\n|\u03c8(z)| = 1,\nX\nz\u2208{0,1}m\n\u03c8(z)orm(z) > 1\n3.\nFix a function h : {0, 1}m \u2192{\u22121, +1} and a distribution \u00b5 on {0, 1}m such that\n\u03c8(z) \u2261h(z)\u00b5(z). For a mapping \u03b1 : ({0, 1}n)k \u2192\n\u0010[n]\nm\n\u0011\n, let H\u03b1 and \u03bb\u03b1 be as de\ufb01ned\nat the beginning of this section. Then Theorem 7.1 shows the existence of \u03b1 such\nthat\ndisc\u03bb\u03b1(H\u03b1) \u2a7d2\u2212\u2126(\u221am).\nUsing the properties of \u03c8, one readily veri\ufb01es that \u27e8H \u25e6\u03bb\u03b1, F\u03b1\u27e9\u2a7e1/3, where\nF\u03b1 : ({0, 1}n)k+1 \u2192{\u22121, +1} is given by F\u03b1(x, y1, . . . , yk) = orm(x|\u03b1(y1,...,yk)). By the\ngeneralized discrepancy method (Theorem 4.2),\nRk+1(F\u03b1) \u2a7e\u2126(\u221am) = n\u2126(1).\n30\nOn the other hand, F\u03b1 has nondeterministic complexity O(log n). Namely,\nPlayer 1 (who knows y1, . . . , yk) nondeterministically selects an element i \u2208\n\u03b1(y1, . . . , yk) and announces i. Player 2 (who knows x) then announces xi as the\noutput of the protocol.\n\u25a1\nA recent follow-up result due to David, Pitassi, and Viola [DPV08] derandomizes\nthe choice of \u03b1 in Theorem 7.4, yielding an explicit separation of NPcc\nk and BPPcc\nk\nfor k \u2a7d(1 \u2212\u01eb) log2 n.\nAcknowledgments\nI would like to thank Anil Ada, Boaz Barak, Arkadev Chattopadhyay, Adam Kli-\nvans, Troy Lee, Yaoyun Shi, and Ronald de Wolf for their helpful feedback on a\npreliminary version of this survey.\nReferences\n[ABFR94]\nJames Aspnes, Richard Beigel, Merrick L. Furst, and Steven Rudich. The ex-\npressive power of voting polynomials. Combinatorica, 14(2):135\u2013148, 1994.\n[All89]\nEric Allender. A note on the power of threshold circuits. In Proc. of the 30th\nSymposium on Foundations of Computer Science (FOCS), pages 580\u2013584,\n1989.\n[AS04]\nScott Aaronson and Yaoyun Shi. Quantum lower bounds for the collision and\nthe element distinctness problems. J. ACM, 51(4):595\u2013605, 2004.\n[ASTS+03] Andris Ambainis, Leonard J. Schulman, Amnon Ta-Shma, Umesh V. Vazi-\nrani, and Avi Wigderson. The quantum communication complexity of sam-\npling. SIAM J. Comput., 32(6):1570\u20131585, 2003.\n[BBC+01]\nRobert Beals, Harry Buhrman, Richard Cleve, Michele Mosca, and Ronald\nde Wolf. Quantum lower bounds by polynomials. J. ACM, 48(4):778\u2013797,\n2001.\n[BCWZ99] Harry Buhrman, Richard Cleve, Ronald de Wolf, and Christof Zalka. Bounds\nfor small-error and zero-error quantum algorithms. In Proc. of the 40th Sym-\nposium on Foundations of Computer Science (FOCS), pages 358\u2013368, 1999.\n[BNS92]\nL\u00b4aszl\u00b4o Babai, Noam Nisan, and Mario Szegedy. Multiparty protocols, pseu-\ndorandom generators for logspace, and time-space trade-o\ufb00s. J. Comput. Syst.\nSci., 45(2):204\u2013232, 1992.\n[BPS07]\nPaul Beame, Toniann Pitassi, and Nathan Segerlind.\nLower bounds for\nLov\u00b4asz-Schrijver systems and beyond follow from multiparty communication\ncomplexity. SIAM J. Comput., 37(3):845\u2013869, 2007.\n31\n[BRS95]\nRichard Beigel, Nick Reingold, and Daniel A. Spielman. PP is closed under\nintersection. J. Comput. Syst. Sci., 50(2):191\u2013202, 1995.\n[BVW07]\nHarry Buhrman, Nikolai K. Vereshchagin, and Ronald de Wolf. On com-\nputation and communication with small bias. In Proc. of the 22nd Conf. on\nComputational Complexity (CCC), pages 24\u201332, 2007.\n[BW01]\nHarry Buhrman and Ronald de Wolf.\nCommunication complexity lower\nbounds by polynomials. In Proc. of the 16th Conf. on Computational Com-\nplexity (CCC), pages 120\u2013130, 2001.\n[CA08]\nArkadev Chattopadhyay and Anil Ada. Multiparty communication complex-\nity of disjointness. Preprint at arXiv:0801.3624v3, February 2008. Prelimi-\nnary version in ECCC Report TR08-002, January 2008.\n[Cha07]\nArkadev Chattopadhyay.\nDiscrepancy and the power of bottom fan-in in\ndepth-three circuits. In Proc. of the 48th Symposium on Foundations of Com-\nputer Science (FOCS), pages 449\u2013458, 2007.\n[CT93]\nFan R. K. Chung and Prasad Tetali. Communication complexity and quasi\nrandomness. SIAM J. Discrete Math., 6(1):110\u2013123, 1993.\n[DP08]\nMatei David and Toniann Pitassi. Separating NOF communication complex-\nity classes RP and NP. ECCC Report TR08-014, February 2008.\n[DPV08]\nMatei David, Toniann Pitassi, and Emanuele Viola. Improved separations be-\ntween nondeterministic and randomized multiparty communication. Preprint,\nApril 2008.\n[GHR92]\nMikael Goldmann, Johan H\u00e5stad, and Alexander A. Razborov. Majority gates\nvs. general weighted threshold gates. Computational Complexity, 2:277\u2013300,\n1992.\n[HJ86]\nRoger A. Horn and Charles R. Johnson. Matrix analysis. Cambridge Univer-\nsity Press, New York, 1986.\n[KKMS05] Adam Tauman Kalai, Adam R. Klivans, Yishay Mansour, and Rocco A.\nServedio. Agnostically learning halfspaces. In Proc. of the 46th Symposium\non Foundations of Computer Science (FOCS), pages 11\u201320, 2005.\n[Kla01]\nHartmut Klauck. Lower bounds for quantum communication complexity. In\nProc. of the 42nd Symposium on Foundations of Computer Science (FOCS),\npages 288\u2013297, 2001.\n[KLS96]\nJe\ufb00Kahn, Nathan Linial, and Alex Samorodnitsky. Inclusion-exclusion: Ex-\nact and approximate. Combinatorica, 16(4):465\u2013477, 1996.\n[KN97]\nEyal Kushilevitz and Noam Nisan. Communication complexity. Cambridge\nUniversity Press, New York, 1997.\n[KOS04]\nAdam R. Klivans, Ryan O\u2019Donnell, and Rocco A. Servedio. Learning inter-\nsections and thresholds of halfspaces. J. Comput. Syst. Sci., 68(4):808\u2013840,\n2004.\n32\n[KP97]\nMatthias Krause and Pavel Pudl\u00b4ak. On the computational power of depth-2\ncircuits with threshold and modulo gates. Theor. Comput. Sci., 174(1-2):137\u2013\n156, 1997.\n[KP98]\nMatthias Krause and Pavel Pudl\u00b4ak. Computing Boolean functions by polyno-\nmials and threshold circuits. Comput. Complex., 7(4):346\u2013370, 1998.\n[Kre95]\nI. Kremer. Quantum communication. Master\u2019s thesis, Hebrew University,\nComputer Science Department, 1995.\n[KS92]\nBala Kalyanasundaram and Georg Schnitger. The probabilistic communica-\ntion complexity of set intersection. SIAM J. Discrete Math., 5(4):545\u2013557,\n1992.\n[KS04]\nAdam R. Klivans and Rocco A. Servedio. Learning DNF in time 2 \u02dcO(n1/3). J.\nComput. Syst. Sci., 68(2):303\u2013318, 2004.\n[KS07a]\nAdam R. Klivans and Alexander A. Sherstov. A lower bound for agnostically\nlearning disjunctions. In Proc. of the 20th Conf. on Learning Theory (COLT),\npages 409\u2013423, 2007.\n[KS07b]\nAdam R. Klivans and Alexander A. Sherstov. Unconditional lower bounds\nfor learning intersections of halfspaces. Machine Learning, 69(2\u20133):97\u2013114,\n2007.\n[LS07]\nTroy Lee and Adi Shraibman. Disjointness is hard in the multi-party number-\non-the-forehead model. Preprint at arXiv:0712.4279, December 2007. To\nappear in Proc. of the 23rd Conf. on Computational Complexity (CCC), 2008.\n[LS07b]\nNati Linial and Adi Shraibman. Lower bounds in communication complexity\nbased on factorization norms. In Proc. of the 39th Symposium on Theory of\nComputing (STOC), pages 699\u2013708, 2007.\n[LS\u02c7S08]\nTroy Lee, Adi Shraibman, and Robert \u02c7Spalek. A direct product theorem for\ndiscrepancy. In Proc. of the 23rd Conf. on Computational Complexity (CCC),\n2008. To appear.\n[MP88]\nMarvin L. Minsky and Seymour A. Papert. Perceptrons: expanded edition.\nMIT Press, Cambridge, Mass., 1988.\n[Nis93]\nNoam Nisan. The communication complexity of threshold gates. In Combi-\nnatorics, Paul Erd\u02ddos is Eighty, pages 301\u2013315, 1993.\n[NS92]\nNoam Nisan and Mario Szegedy. On the degree of Boolean functions as\nreal polynomials. In Proc. of the 24th Symposium on Theory of Computing\n(STOC), pages 462\u2013467, 1992. Final version in Computational Complexity,\n4:301\u2013313, 1994.\n[OS03]\nRyan O\u2019Donnell and Rocco A. Servedio. New degree bounds for polynomial\nthreshold functions. In Proc. of the 35th Symposium on Theory of Computing\n(STOC), pages 325\u2013334, 2003.\n33\n[Pat92]\nRamamohan Paturi. On the degree of polynomials that approximate symmet-\nric Boolean functions. In Proc. of the 24th Symposium on Theory of Comput-\ning (STOC), pages 468\u2013474, 1992.\n[Raz92]\nAlexander A. Razborov.\nOn the distributional complexity of disjointness.\nTheor. Comput. Sci., 106(2):385\u2013390, 1992.\n[Raz00]\nRan Raz. The BNS-Chung criterion for multi-party communication complex-\nity. Computational Complexity, 9(2):113\u2013122, 2000.\n[Raz03]\nAlexander A. Razborov. Quantum communication complexity of symmetric\npredicates. Izvestiya: Mathematics, 67(1):145\u2013159, 2003.\n[RS08]\nAlexander A. Razborov and Alexander A. Sherstov. The sign-rank of AC0.\nECCC Report TR08-016, February 2008.\n[Sch98]\nAlexander Schrijver. Theory of linear and integer programming. John Wiley\n& Sons, Inc., New York, 1998.\n[She07a]\nAlexander A. Sherstov. Separating AC0 from depth-2 majority circuits. In\nProc. of the 39th Symposium on Theory of Computing (STOC), pages 294\u2013\n301, June 2007.\n[She07b]\nAlexander A. Sherstov.\nThe pattern matrix method for lower bounds on\nquantum communication. Technical Report TR-07-46, The Univ. of Texas\nat Austin, Dept. of Computer Sciences, September 2007. To appear in Proc.\nof the 40th Symposium on Theory of Computing (STOC), 2008.\n[She07c]\nAlexander A. Sherstov. Unbounded-error communication complexity of sym-\nmetric functions. Technical Report TR-07-53, The Univ. of Texas at Austin,\nDept. of Computer Sciences, September 2007.\n[She08]\nAlexander A. Sherstov. Approximate inclusion-exclusion for arbitrary sym-\nmetric functions. In Proc. of the 23nd Conf. on Computational Complexity\n(CCC), 2008. To appear.\n[SZ07]\nYaoyun Shi and Yufan Zhu. Quantum communication complexity of block-\ncomposed functions. Preprint at arXiv:0710.0095v1, 29 September 2007.\n[TT99]\nJun Tarui and Tatsuie Tsukiji. Learning DNF by approximating inclusion-\nexclusion formulae. In Proc. of the 14th Conf. on Computational Complexity\n(CCC), pages 215\u2013221, 1999.\n[Wol01]\nRonald de Wolf. Quantum Computing and Communication Complexity. PhD\nthesis, University of Amsterdam, 2001.\n[Wol08]\nRonald de Wolf. A note on quantum algorithms and the minimal degree of\n\u01eb-error polynomials for symmetric functions. Preprint at arXiv:0802.1816,\nFebruary 2008.\n34\n",
        "sentence": "",
        "context": "Acknowledgments\nI would like to thank Anil Ada, Boaz Barak, Arkadev Chattopadhyay, Adam Kli-\nvans, Troy Lee, Yaoyun Shi, and Ronald de Wolf for their helpful feedback on a\npreliminary version of this survey.\nReferences\n[ABFR94]\npaper by Klauck [Kla01, Thm. 4] and was reformulated in its current form by\nRazborov [Raz03]. The development in [Kla01] and [Raz03] takes place in the\nquantum model of communication. However, the basic mathematical technique is\n1992.\n[HJ86]\nRoger A. Horn and Charles R. Johnson. Matrix analysis. Cambridge Univer-\nsity Press, New York, 1986.\n[KKMS05] Adam Tauman Kalai, Adam R. Klivans, Yishay Mansour, and Rocco A."
    },
    {
        "title": "Separating AC0 from depth-2 majority circuits",
        "author": [
            "Alexander A. Sherstov"
        ],
        "venue": "SIAM J. Comput.,",
        "citeRegEx": "Sherstov.,? \\Q2009\\E",
        "shortCiteRegEx": "Sherstov.",
        "year": 2009,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Chernoff\u2013Hoeffding bounds for applications with limited independence",
        "author": [
            "J. Schmidt",
            "A. Siegel",
            "A. Srinivasan"
        ],
        "venue": "SIAM J. Discrete Mathematics,",
        "citeRegEx": "Schmidt et al\\.,? \\Q1995\\E",
        "shortCiteRegEx": "Schmidt et al\\.",
        "year": 1995,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Learning kernel-based halfspaces with the 0-1 loss",
        "author": [
            "S. Shalev-Shwartz",
            "O. Shamir",
            "K. Sridharan"
        ],
        "venue": "SIAM Journal on Computing,",
        "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2011\\E",
        "shortCiteRegEx": "Shalev.Shwartz et al\\.",
        "year": 2011,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Faster algorithms via approximation theory",
        "author": [
            "Sushant Sachdeva",
            "Nisheeth K. Vishnoi"
        ],
        "venue": "Foundations and Trends in Theoretical Computer Science,",
        "citeRegEx": "Sachdeva and Vishnoi.,? \\Q2014\\E",
        "shortCiteRegEx": "Sachdeva and Vishnoi.",
        "year": 2014,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "A theory of the learnable",
        "author": [
            "Leslie G. Valiant"
        ],
        "venue": "Commun. ACM,",
        "citeRegEx": "Valiant.,? \\Q1984\\E",
        "shortCiteRegEx": "Valiant.",
        "year": 1984,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Agnostically learning under permutation invariant distributions",
        "author": [
            "Karl Wimmer"
        ],
        "venue": "In Proceedings of the 2010 IEEE 51st Annual Symposium on Foundations of Computer Science,",
        "citeRegEx": "Wimmer.,? \\Q2010\\E",
        "shortCiteRegEx": "Wimmer.",
        "year": 2010,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    }
]