[
    {
        "title": "Semantic parsing on Freebase from question-answer pairs",
        "author": [
            "Jonathan Berant",
            "Andrew Chou",
            "Roy Frostig",
            "Percy Liang."
        ],
        "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational",
        "citeRegEx": "Berant et al\\.,? 2013",
        "shortCiteRegEx": "Berant et al\\.",
        "year": 2013,
        "abstract": "",
        "full_text": "",
        "sentence": " for many applications such as question answering (Berant et al., 2013; Yih et al., 2015; Dai et al., 2016) and information extraction (Mintz et al.",
        "context": null
    },
    {
        "title": "Theano: a cpu and gpu math expression compiler",
        "author": [
            "James Bergstra",
            "Olivier Breuleux",
            "Fr\u00e9d\u00e9ric Bastien",
            "Pascal Lamblin",
            "Razvan Pascanu",
            "Guillaume Desjardins",
            "Joseph Turian",
            "David Warde-Farley",
            "Yoshua Bengio."
        ],
        "venue": "Proceedings of the Python",
        "citeRegEx": "Bergstra et al\\.,? 2010",
        "shortCiteRegEx": "Bergstra et al\\.",
        "year": 2010,
        "abstract": "",
        "full_text": "",
        "sentence": " All the models are implemented with Theano (Bergstra et al., 2010).",
        "context": null
    },
    {
        "title": "Phonologically aware neural model for named entity recognition in low resource transfer settings",
        "author": [
            "Akash Bharadwaj",
            "David Mortensen",
            "Chris Dyer",
            "Jaime Carbonell."
        ],
        "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Lan-",
        "citeRegEx": "Bharadwaj et al\\.,? 2016",
        "shortCiteRegEx": "Bharadwaj et al\\.",
        "year": 2016,
        "abstract": "",
        "full_text": "",
        "sentence": " For example, Bharadwaj et al. (2016) transfers models on resource-rich languages to low resource languages by parameter sharing through common phonological features in name entity recognition. For example, Bharadwaj et al. (2016) transfers models on resource-rich languages to low resource languages by parameter sharing through common phonological features in name entity recognition. Zoph et al. (2016) initialize from models trained by resource-rich languages to translate low-resource languages.",
        "context": null
    },
    {
        "title": "Freebase: A Collaboratively Created Graph Database for Structuring Human Knowledge",
        "author": [
            "Kurt Bollacker",
            "Colin Evans",
            "Praveen Paritosh",
            "Tim Sturge",
            "Jamie Taylor."
        ],
        "venue": "Proceedings of the 2008 ACM SIGMOD International Conference on Man-",
        "citeRegEx": "Bollacker et al\\.,? 2008",
        "shortCiteRegEx": "Bollacker et al\\.",
        "year": 2008,
        "abstract": "",
        "full_text": "",
        "sentence": " Knowledge bases (KB), such as WordNet (Fellbaum, 1998), Freebase (Bollacker et al., 2008), YAGO (Suchanek et al.",
        "context": null
    },
    {
        "title": "Embedding semantic relations into word representations",
        "author": [
            "Danushka Bollegala",
            "Takanori Maehara",
            "Ken-ichi Kawarabayashi."
        ],
        "venue": "Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence.",
        "citeRegEx": "Bollegala et al\\.,? 2015",
        "shortCiteRegEx": "Bollegala et al\\.",
        "year": 2015,
        "abstract": "Learning representations for semantic relations is important for various\ntasks such as analogy detection, relational search, and relation\nclassification. Although there have been several proposals for learning\nrepresentations for individual words, learning word representations that\nexplicitly capture the semantic relations between words remains under\ndeveloped. We propose an unsupervised method for learning vector\nrepresentations for words such that the learnt representations are sensitive to\nthe semantic relations that exist between two words. First, we extract lexical\npatterns from the co-occurrence contexts of two words in a corpus to represent\nthe semantic relations that exist between those two words. Second, we represent\na lexical pattern as the weighted sum of the representations of the words that\nco-occur with that lexical pattern. Third, we train a binary classifier to\ndetect relationally similar vs. non-similar lexical pattern pairs. The proposed\nmethod is unsupervised in the sense that the lexical pattern pairs we use as\ntrain data are automatically sampled from a corpus, without requiring any\nmanual intervention. Our proposed method statistically significantly\noutperforms the current state-of-the-art word representations on three\nbenchmark datasets for proportional analogy detection, demonstrating its\nability to accurately capture the semantic relations among words.",
        "full_text": "Embedding Semantic Relations into Word Representations\nDanushka Bollegala Takanori Maehara Ken-ichi Kawarabayashi\nUniversity of Liverpool Shizuoka University National Institute of Informatics\nJST, ERATO, Kawarabayashi Large Graph Project.\nAbstract\nLearning representations for semantic relations is\nimportant for various tasks such as analogy de-\ntection, relational search, and relation classi\ufb01ca-\ntion. Although there have been several proposals\nfor learning representations for individual words,\nlearning word representations that explicitly cap-\nture the semantic relations between words re-\nmains under developed. We propose an unsuper-\nvised method for learning vector representations\nfor words such that the learnt representations are\nsensitive to the semantic relations that exist be-\ntween two words. First, we extract lexical patterns\nfrom the co-occurrence contexts of two words in a\ncorpus to represent the semantic relations that ex-\nist between those two words. Second, we represent\na lexical pattern as the weighted sum of the rep-\nresentations of the words that co-occur with that\nlexical pattern. Third, we train a binary classi\ufb01er\nto detect relationally similar vs. non-similar lexi-\ncal pattern pairs. The proposed method is unsuper-\nvised in the sense that the lexical pattern pairs we\nuse as train data are automatically sampled from a\ncorpus, without requiring any manual intervention.\nOur proposed method statistically signi\ufb01cantly out-\nperforms the current state-of-the-art word repre-\nsentations on three benchmark datasets for propor-\ntional analogy detection, demonstrating its ability\nto accurately capture the semantic relations among\nwords.\n1\nIntroduction\nRepresenting the semantics of words and relations are funda-\nmental tasks in Knowledge Representation (KR). Numerous\nmethods for learning distributed word representations have\nbeen proposed in the NLP community [Turian et al., 2010;\nCollobert et al., 2011; Mikolov et al., 2013b; Mikolov et al.,\n2013a; Pennington et al., 2014]. Distributed word representa-\ntions have shown to improve performance in a wide-range of\ntasks such as, machine translation [Cho et al., 2014], seman-\ntic similarity measurement [Mikolov et al., 2013d; Penning-\nton et al., 2014], and word sense disambiguation [Huang et\nal., 2012].\nDespite the impressive performance of representation\nlearning methods for individual words, existing methods use\nonly co-occurrences between words, ignoring the rich se-\nmantic relational structure. The context in which two words\nco-occur provides useful insights into the semantic relations\nthat exist between those two words. For example, the sen-\ntence ostrich is a large bird not only provides the infor-\nmation that ostrich and bird are co-occurring, but also de-\nscribes how they are related via the lexical pattern X is a\nlarge Y, where slot variables X and Y correspond to the\ntwo words between which the relation holds. If we can\nsomehow embed the information about the semantic rela-\ntions R that are associated with a particular word w into\nthe representation of w, then we can construct richer seman-\ntic representation than the pure co-occurrence-based word\nrepresentations. Although the word representations learnt by\nco-occurrence prediction methods [Mikolov et al., 2013d;\nPennington et al., 2014] have implicitly captured a certain\ndegree of relational structure, it remains unknown how to ex-\nplicitly embed the information about semantic relations into\nword representations.\nWe propose a method for learning word representations\nthat explicitly encode the information about the semantic re-\nlations that exist between words. Given a large corpus, we\nextract lexical patterns that correspond to numerous seman-\ntic relations that exist between word-pairs (xi, xj). Next, we\nrepresent each word xi in the vocabulary by a d-dimensional\nvector xi \u2208Rd. Word representations can be initialized ei-\nther randomly or by using pre-trained word representations.\nNext, we represent a pattern p by the weighted average of\nthe vector differences (xi \u2212xj) corresponding to word-\npairs (xi, xj) that co-occur with p in the corpus. This en-\nables us to represent a pattern p by a d-dimensional vector\np \u2208Rd in the same embedding space as the words. Using\nvector difference between word representations to represent\nsemantic relations is motivated by the observations in prior\nwork on word representation learning [Mikolov et al., 2013d;\nPennington et al., 2014] where, for example, the difference of\nvectors representing king and queen has shown to be similar\nto the difference of vectors representing man and woman.\nWe model the problem of embedding semantic relations\ninto word representations as an analogy prediction task\nwhere, given two lexical patterns, we train a binary classi\ufb01er\nthat predicts whether they are relationally similar. Our pro-\narXiv:1505.00161v1  [cs.CL]  1 May 2015\nposed method is unsupervised in the sense that both positive\nand negative training instances that we use for training are\nautomatically selected from a corpus, without requiring any\nmanual intervention. Speci\ufb01cally, pairs of lexical patterns that\nco-occur with the same set of word-pairs are selected as pos-\nitive training instances, whereas negative training instances\nare randomly sampled from pairs of patterns with low rela-\ntional similarities. Our proposed method alternates between\ntwo steps (Algorithm 1). In the \ufb01rst step, we construct pat-\ntern representations from current word representations. In the\nsecond step, we predict whether a given pair of patterns is re-\nlationally similar using the computed representations of pat-\nterns in the previous step. We update the word representations\nsuch that the prediction loss is minimized.\nDirect evaluation of word representations is dif\ufb01cult be-\ncause there is no agreed gold standard for semantic represen-\ntation of words. Following prior work on representation learn-\ning, we evaluate the proposed method using the learnt word\nrepresentations in an analogy detection task. For example, de-\nnoting the word representation for a word w by v(w), the\nvector v(king)\u2212v(man)+v(woman) is required to be sim-\nilar to v(queen), than all the other words in the vocabulary.\nSimilarity between two vectors is computed by the cosine of\nthe angle between the corresponding vectors. The accuracy\nobtained in the analogy detection task with a particular word\nrepresentation method is considered as a measure of its ac-\ncuracy. In our evaluations, we use three previously proposed\nbenchmark datasets for word analogy detection: SAT anal-\nogy dataset [Turney, 2005], Google analogy dataset [Mikolov\net al., 2013c], and SemEval analogy dataset [Jurgens et al.,\n2012]. The word representations produced by our proposed\nmethod statistically signi\ufb01cantly outperform the current state-\nof-the-art word representation learning methods on all three\nbenchmark datasets in an analogy detection task, demonstrat-\ning the accuracy of the proposed method for embedding se-\nmantic relations in word representations.\n2\nRelated Work\nRepresenting words using vectors (or tensors in general) is\nan essential task in text processing. For example, in distri-\nbutional semantics [Baroni and Lenci, 2010], a word x is\nrepresented by a vector that contains other words that co-\noccur with x in a corpus. Numerous methods for selecting co-\noccurrence contexts (e.g. proximity-based windows, depen-\ndency relations), and word association measures (e.g. point-\nwise mutual information (PMI), log-likelihood ratio (LLR),\nlocal mutual information (LLR)) have been proposed [Tur-\nney and Pantel, 2010]. Despite the successful applications of\nco-occurrence counting-based distributional word representa-\ntions, their high dimensionality and sparsity is often problem-\natic when applied in NLP tasks. Consequently, further post-\nprocessing such as dimensionality reduction, and feature se-\nlection is often required when using distributional word rep-\nresentations.\nOn the other hand, distributed word representation learn-\ning methods model words as d-dimensional real vectors and\nlearn those vector representations by applying them to solve\nan auxiliary task such as language modeling. The dimen-\nsionality d is \ufb01xed for all the words in the vocabulary and,\nunlike distributional word representations, is much smaller\n(e.g. d \u2208[10, 1000] in practice) compared to the vocabulary\nsize. A pioneering work on word representation learning is\nthe neural network language model (NNLMs) [Bengio et al.,\n2003], where word representations are learnt such that we can\naccurately predict the next word in a sentence using the word\nrepresentations for the previous words. Using backpropaga-\ntion, word vectors are updated such that the prediction error\nis minimized.\nAlthough NNLMs learn word representations as a by-\nproduct, the main focus on language modeling is to predict\nthe next word in a sentence given the previous words, and\nnot on learning word representations that capture word se-\nmantics. Moreover, training multi-layer neural networks with\nlarge text corpora is often time consuming. To overcome\nthose limitations, methods that speci\ufb01cally focus on learn-\ning word representations that capture word semantics us-\ning large text corpora have been proposed. Instead of using\nonly the previous words in a sentence as in language mod-\neling, these methods use all the words in a contextual win-\ndow for the prediction task [Collobert et al., 2011]. Meth-\nods that use one or no hidden layers are proposed to im-\nprove the scalability of the learning algorithms. For exam-\nple, the skip-gram model [Mikolov et al., 2013c] predicts\nthe words c that appear in the local context of a word x,\nwhereas the continuous bag-of-words model (CBOW) pre-\ndicts a word x conditioned on all the words c that appear in\nx\u2019s local context [Mikolov et al., 2013a]. However, meth-\nods that use global co-occurrences in the entire corpus to\nlearn word representations have shown to outperform meth-\nods that use only local co-occurrences [Huang et al., 2012;\nPennington et al., 2014]. Word representations learnt us-\ning above-mentioned representation learning methods have\nshown superior performance over word representations con-\nstructed using the traditional counting-based methods [Baroni\net al., 2014].\nWord representations can be further classi\ufb01ed depending\non whether they are task-speci\ufb01c or task-independent. For ex-\nample, methods for learning word representations for speci\ufb01c\ntasks such as sentiment classi\ufb01cation [Socher et al., 2011],\nand semantic composition [Hashimoto et al., 2014] have been\nproposed. These methods use label data for the target task to\ntrain supervised models, and learn word representations that\noptimize the performance on this target task. Whether the\nmeaning of a word is task-speci\ufb01c or task-independent re-\nmains an interesting open question. Our proposal can be seen\nas a third alternative in the sense that we use task-independent\npre-trained word representations as the input, and embed the\nknowledge related to the semantic relations into the word rep-\nresentations. However, unlike the existing task-speci\ufb01c word\nrepresentation learning methods, we do not require manually\nlabeled data for the target task (i.e. analogy detection).\n3\nLearning Word Representations\nThe local context in which two words co-occur provides use-\nful information regarding the semantic relations that exist be-\ntween those two words. For example, from the sentence Os-\ntrich is a large bird that primarily lives in Africa, we can\ninfer that the semantic relation IS-A-LARGE exists between\nostrich and bird. Prior work on relational similarity measure-\nment have successfully used such lexical patterns as features\nto represent the semantic relations that exist between two\nwords [Duc et al., 2010; Duc et al., 2011]. According to the\nrelational duality hypothesis [Bollegala et al., 2010], a se-\nmantic relation R can be expressed either extensionally by\nenumerating word-pairs for which R holds, or intensionally\nby stating lexico-syntactic patterns that de\ufb01ne the properties\nof R.\nFollowing these prior work, we extract lexical patterns\nfrom the co-occurring contexts of two words to represent the\nsemantic relations between those two words. Speci\ufb01cally, we\nextract unigrams and bigrams of tokens as patterns from the\nmid\ufb01x (i.e. the sequence of tokens that appear in between\nthe given two words in a context) [Bollegala et al., 2007b;\nBollegala et al., 2007a]. Although we use lexical patterns as\nfeatures for representing semantic relations in this work, our\nproposed method is not limited to lexical patterns, and can be\nused in principle with any type of features that represent rela-\ntions. The strength of association between a word pair (u, v)\nand a pattern p is measured using the positive pointwise mu-\ntual information (PPMI), f(p, u, v), which is de\ufb01ned as fol-\nlows,\nf(p, u, v) = max(0, log\n\u0012g(p, u, v)g(\u2217, \u2217, \u2217)\ng(p, \u2217, \u2217)g(\u2217, u, v)\n\u0013\n).\n(1)\nHere, g(p, u, v) denotes the number of co-occurrences be-\ntween p and (u, v), and \u2217denotes the summation taken over\nall words (or patterns) corresponding to the slot variable. We\nrepresent a pattern p by the set R(p) of word-pairs (u, v) for\nwhich f(p, u, v) > 0. Formally, we de\ufb01ne R(p) and its norm\n|R(p)| as follows,\nR(p) = {(u, v)|f(p, u, v) > 0}\n(2)\n|R(p)| =\nX\n(u,v)\u2208R(p)\nf(p, u, v)\n(3)\nWe represent a word x using a vector x \u2208Rd. The dimen-\nsionality of the representation, d, is a hyperparameter of the\nproposed method. Prior work on word representation learn-\ning have observed that the difference between the vectors that\nrepresent two words closely approximates the semantic re-\nlations that exist between those two words. For example, the\nvector v(king)\u2212v(queen) has shown to be similar to the vec-\ntor v(man) \u2212v(woman). We use this property to represent a\npattern p by a vector p \u2208Rd as the weighted sum of dif-\nferences between the two words in all word-pairs (u, v) that\nco-occur with p as follows,\np =\n1\n|R(p)|\nX\n(u,v)\u2208R(p)\nf(p, u, v)(u \u2212v).\n(4)\nFor example, consider Fig. 1, where the two word-pairs\n(lion, cat), and (ostrich, bird) co-occur respectively with\nthe two lexical patterns, p1\n=\nlarge Ys such as Xs, and\np2 = X is a huge Y. Assuming that there are no other co-\noccurrences between word-pairs and patterns in the corpus,\nx1\nx2\np1\n1\nx3\nx4\np2\nlion\ncat\nostrich\nbird\nlarge Ys such as Xs\nX is a huge Y\nf(p1, x1, x2)\n\u03c3(p1\n>p2)\n-f(p1, x1, x2)\nf(p2, x3, x4)\n-f(p2, x3, x4)\nFigure 1: Computing the similarity between two patterns.\nthe representations of the patterns p1 and p2 are given respec-\ntively by p1 = x1 \u2212x2, and p2 = x3 \u2212x4. We measure the\nrelational similarity between (x1, x2) and (x3, x4) using the\ninner-product p1\n\u22a4p2.\nWe model the problem of learning word representations as\na binary classi\ufb01cation task, where we learn representations\nfor words such that they can be used to accurately predict\nwhether a given pair of patterns are relationally similar. In\nour previous example, we would learn representations for the\nfour words lion, cat, ostrich, and bird such that the similarity\nbetween the two patterns large Ys such as Xs, and X is a huge\nY is maximized. Later in Section 3.1, we propose an unsuper-\nvised method for selecting relationally similar (positive) and\ndissimilar (negative) pairs of patterns as training instances to\ntrain a binary classi\ufb01er.\nLet us denote the target label for two patterns p1, p2 by\nt(p1, p2) \u2208{1, 0}, where the value 1 indicates that p1 and\np2 are relationally similar, and 0 otherwise. We compute the\nprediction loss for a pair of patterns (p1, p2) as the squared\nloss between the target and the predicted labels as follows,\nL(t(p1, p2), p1, p2) = 1\n2(t(p1, p2) \u2212\u03c3(p1\n\u22a4p2))\n2.\n(5)\nDifferent non-linear functions can be used as the prediction\nfunction \u03c3(\u00b7) such as the logistic-sigmoid, hyperbolic tan-\ngent, or recti\ufb01ed linear units. In our preliminary experiments\nwe found hyperbolic tangent, tanh, given by\n\u03c3(\u03b8) = tanh(\u03b8) = exp(\u03b8) \u2212exp(\u2212\u03b8)\nexp(\u03b8) + exp(\u2212\u03b8)\n(6)\nto work particularly well among those different non-\nlinearities.\nTo derive the update rule for word representations, let us\nconsider the derivative of the loss w.r.t. the word representa-\ntion x of a word x,\n\u2202L\n\u2202x = \u2202L\n\u2202p1\n\u2202p1\n\u2202x + \u2202L\n\u2202p2\n\u2202p2\n\u2202x ,\n(7)\nwhere the partial derivative of the loss w.r.t. pattern represen-\ntations are given by,\n\u2202L\n\u2202p1\n= \u03c3\u2032(p1\n\u22a4p2)(\u03c3(p1\n\u22a4p2) \u2212t(p1, p2))p2,\n(8)\n\u2202L\n\u2202p2\n= \u03c3\u2032(p1\n\u22a4p2)(\u03c3(p1\n\u22a4p2) \u2212t(p1, p2))p1.\n(9)\nHere, \u03c3\u2032 denotes the \ufb01rst derivative of tanh, which is given\nby 1\u2212\u03c3(\u03b8)2. To simplify the notation we drop the arguments\nof the loss function.\nFrom Eq. 4 we get,\n\u2202p1\n\u2202x =\n1\n|R(p1)| (h(p1, u = x, v) \u2212h(p1, u, v = x)) ,\n(10)\n\u2202p2\n\u2202x =\n1\n|R(p2)| (h(p2, u = x, v) \u2212h(p2, u, v = x)) ,\n(11)\nwhere,\nh(p, u = x, v) =\nX\n(x,v)\u2208{(u,v)|(u,v)\u2208R(p),u=x}\nf(p, x, v),\nand\nh(p, u, v = x) =\nX\n(u,x)\u2208{(u,v)|(u,v)\u2208R(p),v=x}\nf(p, u, x).\nSubstituting the partial derivatives given by Eqs. 8-11 in\nEq. 7 we get,\n\u2202L\n\u2202x = \u03bb(p1, p2)[H(p1, x)\nX\n(u,v)\u2208R(p2)\nf(p2, u, v)(u \u2212v)\n+H(p2, x)\nX\n(u,v)\u2208R(p1)\nf(p1, u, v)(u \u2212v)],\n(12)\nwhere \u03bb(p1, p2) is de\ufb01ned as\n\u03bb(p1, p2) = \u03c3\u2032(p1\n\u22a4p2)(t(p1, p2) \u2212\u03c3(p1\n\u22a4p2))\n|R(p1)||R(p2)|\n,\n(13)\nand H(p, x) is de\ufb01ned as\nH(p, x) = h(p, u = x, v) \u2212h(p, u, v = x).\n(14)\nWe use stochastic gradient decent (SGD) with learning rate\nadapted by AdaGrad [Duchi et al., 2011] to update the word\nrepresentations. The pseudo code for the proposed method is\nshown in Algorithm 1. Given a set of N relationally similar\nand dissimilar pattern-pairs, {(p(i)\n1 , p(i)\n2 , t(p(i)\n1 , p(i)\n2 )}N\ni=1, Al-\ngorithm 1 initializes each word xj in the vocabulary with a\nvector xj \u2208Rd. The initialization can be conducted either\nusing randomly sampled vectors from a zero mean and unit\nvariance Gaussian distribution, or by pre-trained word rep-\nresentations. In our preliminary experiments, we found that\nthe word vectors learnt by GloVe [Pennington et al., 2014]\nto perform consistently well over random vectors when used\nas the initial word representations in the proposed method.\nBecause word vectors trained using existing word representa-\ntions already demonstrate a certain degree of relational struc-\nture with respect to proportional analogies, we believe that\ninitializing using pre-trained word vectors assists the subse-\nquent optimization process.\nAlgorithm 1 Learning word representations.\nInput: Training pattern-pairs {(p(i)\n1 , p(i)\n2 , t(p(i)\n1 , p(i)\n2 )}N\ni=1,\ndimensionality d of the word representations, and the\nmaximum number of iterations T.\nOutput: Representation xj \u2208Rd, of a word xj for j =\n1, . . . , M, where M is the vocabulary size.\n1: Initialize word vectors {xj}M\nj=1.\n2: for t = 1 to T do\n3:\nfor k = 1 to K do\n4:\npk =\n1\n|R(pk)|\nP\n(u,v)\u2208R(pk) f(pk, u, v)(u \u2212v)\n5:\nend for\n6:\nfor i = 1 to N do\n7:\nfor j = 1 to M do\n8:\nxj = xj \u2212\u03b1(t)\nj\n\u2202L\n\u2202xj\n9:\nend for\n10:\nend for\n11: end for\n12: return {xj}M\nj=1.\nDuring each iteration, Algorithm 1 alternates between two\nsteps. First, in Lines 3-5, it computes pattern representations\nusing Eq. 4 from the current word representations for all the\npatterns (K in total) in the training dataset. Second, in Lines\n6-10, for each train pattern-pair we compute the derivative of\nthe loss according to Eq. 12, and update the word represen-\ntations. These two steps are repeated for T iterations, after\nwhich the \ufb01nal set of word representations are returned.\nThe\ncomputational\ncomplexity\nof\nAlgorithm\n1\nis\nO(TKd + TNMd), where d is the dimensionality of\nthe word representations. Naively iterating over N training\ninstances and M words in the vocabulary can be prohibitively\nexpensive for large training datasets and vocabularies. How-\never, in practice we can ef\ufb01ciently compute the updates\nusing two tricks: delayed updates and indexing. Once we\nhave computed the pattern representations for all K patterns\nin the \ufb01rst iteration, we can postpone the update of a\nrepresentation for a pattern until that pattern next appears\nin a training instance. This reduces the number of patterns\nthat are updated in each iteration to a maximum of 2 instead\nof K for the iterations t > 1. Because of the sparseness in\nco-occurrences, only a handful (ca. 100) of patterns co-occur\nwith any given word-pair. Therefore, by pre-compiling an\nindex from a pattern to the words with which that pattern\nco-occurs, we can limit the update of word representations\nin Line 8 to a much smaller number than M. Moreover, the\nvector subtraction can be parallized across the dimensions.\nAlthough the loss function de\ufb01ned by Eq. 5 is non-convex\nw.r.t. to word representations, in practice, Algorithm 1\nconverges after a few (less than 5) iterations. In practice,\nit requires less than an hour to train from a 2 billion word\ncorpus where we have N = 100, 000, T = 10, K = 10, 000\nand M = 210, 914.\nLexical patterns contain sequences of multiple words.\nTherefore, exact occurrences of lexical patterns are rare com-\npared to that of individual words even in large corpora. Di-\nrectly learning representations for lexical patterns using their\nco-occurrence statistics leads to data sparseness issues, which\nbecomes problematic when applying existing methods pro-\nposed for learning representations for single words to learn\nrepresentations for lexical patterns that consist of multiple\nwords. The proposal made in Eq. 4 to compute representa-\ntions for patterns circumvent this data sparseness issue by in-\ndirectly modeling patterns through word representations.\n3.1\nSelecting Similar/Dissimilar Pattern-Pairs\nWe use the ukWaC corpus1 to extract relationally similar\n(positive) and dissimilar (negative) pairs of patterns (pi, pj)\nto train the proposed method. The ukWaC is a 2 billion word\ncorpus constructed from the Web limiting the crawl to the .uk\ndomain. We select word-pairs that co-occur at least in 50 sen-\ntences within a co-occurrence window of 5 tokens. Moreover,\nusing a stop word list, we ignore word-pairs that purely con-\nsists of stop words. We obtain 210, 914 word-pairs from this\nstep. Next, we extract lexical patterns for those word-pairs\nby replacing the \ufb01rst and second word in a word-pair respec-\ntively by slot variables X and Y in a co-occurrence window of\nlength 5 tokens to extract numerous lexical patterns. We select\nthe top occurring 10, 000 lexical patterns (i.e. K = 10, 000)\nfor further processing.\nWe represent a pattern p by a vector where the elements\ncorrespond to the PPMI values f(p, u, v) between p and all\nthe word-pairs (u, v) that co-occur with p. Next, we com-\npute the cosine similarity between all pairwise combinations\nof the 10, 000 patterns, and rank the pattern pairs in the de-\nscending order of their cosine similarities. We select the top\nranked 50, 000 pattern-pairs as positive training instances. We\nselect 50, 000 pattern-pairs from the bottom of the list which\nhave non-zero similarity scores as negative training instances.\nThe reason for not selecting pattern-pairs with zero similar-\nity scores is that such patterns do not share any word-pairs in\ncommon, and are not informative as training data for updat-\ning word representations. Thus, the total number of training\ninstances we select is N = 50, 000 + 50, 000 = 100, 000.\n4\nEvaluating Word Representations using\nProportional Analogies\nTo evaluate the ability of the proposed method to learn word\nrepresentations that embed information related to semantic\nrelations, we apply it to detect proportional analogies. For\nexample, consider the proportional analogy, man:woman ::\nking:queen. Given, the \ufb01rst three words, a word represen-\ntation learning method is required to \ufb01nd the fourth word\nfrom the vocabulary that maximizes the relational similar-\nity between the two word-pairs in the analogy. Three bench-\nmark datasets have been popularly used in prior work for\nevaluating analogies: Google dataset [Mikolov et al., 2013c]\n(10, 675 syntactic analogies and 8869 semantic analogies),\nSemEval dataset [Jurgens et al., 2012] (79 questions), and\nSAT dataset [Turney, 2006] (374 questions). For the Google\ndataset, the set of candidates for the fourth word consists of\nall the words in the vocabulary. For the SemEval and SAT\ndatasets, each question word-pair is assigned with a limited\n1http://wacky.sslmit.unibo.it\nTable 1: Word analogy results on benchmark datasets.\nMethod\nsem.\nsynt.\ntotal\nSAT\nSemEval\nivLBL CosAdd\n63.60\n61.80\n62.60\n20.85\n34.63\nivLBL CosMult\n65.20\n63.00\n64.00\n19.78\n33.42\nivLBL PairDiff\n52.60\n48.50\n50.30\n22.45\n36.94\nskip-gram CosAdd\n31.89\n67.67\n51.43\n29.67\n40.89\nskip-gram CosMult\n33.98\n69.62\n53.45\n28.87\n38.54\nskip-gram PairDiff\n7.20\n19.73\n14.05\n35.29\n43.99\nCBOW CosAdd\n39.75\n70.11\n56.33\n29.41\n40.31\nCBOW CosMult\n38.97\n70.39\n56.13\n28.34\n38.19\nCBOW PairDiff\n5.76\n13.43\n9.95\n33.16\n42.89\nGloVe CosAdd\n86.67\n82.81\n84.56\n27.00\n40.11\nGloVe CosMult\n86.84\n84.80\n85.72\n25.66\n37.56\nGloVe PairDiff\n45.93\n41.23\n43.36\n44.65\n44.67\nProp CosAdd\n86.70\n85.35\n85.97\n29.41\n41.86\nProp CosMult\n86.91\n87.04\n86.98\n28.87\n39.67\nProp PairDiff\n41.85\n42.86\n42.40\n45.99\n44.88\nnumber of candidate word-pairs out of which only one is cor-\nrect. The accuracy of a word representation is evaluated by\nthe percentage of the correctly answered analogy questions\nout of all the questions in a dataset. We do not skip any ques-\ntions in our evaluations.\nGiven a proportional analogy a : b :: c : d, we use the\nfollowing measures proposed in prior work for measuring the\nrelational similarity between (a, b) and (c, d).\nCosAdd proposed by Mikolov et al. [2013d] ranks candi-\ndates c according to the formula\nCosAdd(a:b, c:d) = cos(b \u2212a + c, d),\n(15)\nand selects the top-ranked candidate as the correct an-\nswer.\nCosMult: CosAdd measure can be decomposed into the\nsummation of three cosine similarities, where in practice\none of the three terms often dominates the sum. To over-\ncome this bias in CosAdd, Levy and Goldberg [2014]\nproposed the CosMult measure given by,\nCosMult(a:b, c:d) = cos(b, d) cos(c, d)\ncos(a, d) + \u03f5\n.\n(16)\nWe convert all cosine values x \u2208[\u22121, 1] to positive val-\nues using the transformation (x+1)/2. Here, \u03f5 is a small\nconstant value to prevent denominator becoming zero,\nand is set to 10\u22125 in the experiments.\nPairDiff measures the cosine similarity between the two\nvectors that correspond to the difference of the word\nrepresentations of the two words in each word-pair. It\nfollows from our hypothesis that the semantic relation\nbetween two words can be represented by the vector dif-\nference of their word representations. PairDiff has been\nused by Mikolov et al. [2013d] for detecting semantic\nanalogies and is given by,\nPairDi\ufb00(a:b, c:d) = cos(b \u2212a, d \u2212c).\n(17)\n5\nExperiments and Results\nIn Table 1, we compare the proposed method against pre-\nviously proposed word representation learning methods:\nsemantic\nsyntactic\ntotal\n82\n83\n84\n85\n86\n87\n88\nAccuracy\n10k\n100k\nFigure 2: Accuracy on Google dataset when the proposed\nmethod is trained using 10k and 100k instances.\nivLBL [Mnih and Kavukcuoglu, 2013], skip-gram [Mikolov\net al., 2013c], CBOW [Mikolov et al., 2013a], and\nGloVe [Pennington et al., 2014]. All methods compared in\nTable 1 are trained on the same ukWaC corpus of 2B tokens\nto produce 300 dimensional word vectors. We use the pub-\nlicly available implementations2,3 by the original authors for\ntraining the word representations using the recommended pa-\nrameter values. Therefore, any differences in performances\nreported in Table 1 can be directly attributable to the differ-\nences in the respective word representation learning methods.\nIn all of our experiments, the proposed method converged\nwith less than 5 iterations.\nFrom Table 1 we see that the proposed method (denoted by\nProp) achieves the best results for the semantic (sem), syn-\ntactic (synt) and their union (total) analogy questions in the\nGoogle dataset using CosMult measure. For analogy ques-\ntions in SAT and SemEval datasets the best performance is\nreported by the proposed method using the PairDiff measure.\nThe PairDiff measure computes the cosine similarity between\nthe two difference vectors b \u2212a and d \u2212c, ignoring the\nspatial distances between the individual words as opposed to\nCosAdd or CosMult. Recall that in the Google dataset we\nare required to \ufb01nd analogies from a large open vocabulary\nwhereas in SAT and SemEval datasets the set of candidates\nis limited to a closed pre-de\ufb01ned set. Relying on direction\nalone, while ignoring spatial distance is problematic when\nconsidering the entire vocabulary as candidates because, we\nare likely to \ufb01nd candidates d that have the same relation\nto c as re\ufb02ected by a \u2212b. For example, given the analogy\nman:woman::king:?, we are likely to recover feminine en-\ntities, but not necessarily royal ones using PairDiff. On the\nother hand, in both SemEval and SAT datasets, the set of can-\ndidate answers already contains the related candidates, leav-\ning mainly the direction to be decided. For the remainder of\nthe experiments described in the paper, we use CosMult for\nevaluations on the Google dataset, whereas PairDiff is used\n2https://code.google.com/p/word2vec/\n3http://nlp.stanford.edu/projects/glove/\n100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\nDimensionality\n35\n40\n45\n50\n55\n60\n65\n70\n75\nAccuracy\nsemantic\nsyntactic\ntotal\nSAT\nSemEval\nFigure 3: Accuracy of the proposed method on benchmark\ndatasets for dimensionalities of the word representations.\nfor the SAT and SemEval datasets. Results reported in Table 1\nreveal that according to the binomial exact test with 95% con-\n\ufb01dence the proposed method statistically signi\ufb01cantly outper-\nforms GloVe, the current state-of-the-art word representation\nlearning method, on all three benchmark datasets.\nTo study the effect of the train dataset size on the perfor-\nmance of the proposed method, following the procedure de-\nscribed in Section 3.1, we sample two balanced datasets con-\ntaining respectively 10, 000 and 100, 000 instances. Figure 2\nshows the performance reported by the proposed method on\nthe Google dataset. We see that the overall performance in-\ncreases with the dataset size, and the gain is more for syntac-\ntic analogies. This result can be explained considering that se-\nmantic relations are more rare compared to syntactic relations\nin the ukWaC corpus, a generic web crawl, used in our exper-\niments. However, the proposed train data selection method\nprovides us with a potentially unlimited source of positive\nand negative training instances which we can use to further\nimprove the performance.\nTo study the effect of the dimensionality d of the represen-\ntation on the performance of the proposed method, we hold\nthe train data size \ufb01xed and produce word representations for\ndifferent dimensionalities. As shown in Figure 3, the perfor-\nmance increases until around 600 dimensions on the Google,\nand the SAT datasets after which it stabilizes. The perfor-\nmance on the SemEval dataset remains relatively unaffected\nby the dimensionality of the representation.\n6\nConclusions\nWe proposed a method to learn word representations that\nembeds information related to semantic relations between\nwords. A two step algorithm that alternates between pat-\ntern and word representations was proposed. The proposed\nmethod signi\ufb01cantly outperforms the current state-of-the-art\nword representation learning methods on three datasets con-\ntaining proportional analogies.\nSemantic relations that can be encoded as attributes in\nwords are only a fraction of all types of semantic relations.\nWhether we can accurately embed semantic relations that in-\nvolve multiple entities, or semantic relations that are only ex-\ntrinsically and implicitly represented remains unknown. We\nplan to explore these possibilities in our future work.\nReferences\n[Baroni and Lenci, 2010] Marco\nBaroni\nand\nAlessandro\nLenci.\nDistributional memory: A general framework\nfor corpus-based semantics. Computational Linguistics,\n36(4):673 \u2013 721, 2010.\n[Baroni et al., 2014] Marco Baroni, Georgiana Dinu, and\nGerm\u00b4an Kruszewski. Don\u2019t count, predict! a systematic\ncomparison of context-counting vs. context-predicting se-\nmantic vectors. In ACL\u201914, pages 238\u2013247, 2014.\n[Bengio et al., 2003] Yoshua Bengio, R\u00b4ejean Ducharme,\nPascal Vincent, and Christian Jauvin.\nA neural proba-\nbilistic language model. Journal of Machine Learning Re-\nsearch, 3:1137 \u2013 1155, 2003.\n[Bollegala et al., 2007a] D.\nBollegala,\nY.\nMatsuo,\nand\nM. Ishizuka.\nAn integrated approach to measuring\nsemantic similarity between words using information\navailable on the web.\nIn Proceedings of NAACL HLT,\npages 340\u2013347, 2007.\n[Bollegala et al., 2007b] Danushka Bollegala, Yutaka Mat-\nsuo, and Mitsuru Ishizuka. Websim: A web-based seman-\ntic similarity measure. In Proc. of 21st Annual Conference\nof the Japanese Society of Artiti\ufb01cial Intelligence, pages\n757 \u2013 766, 2007.\n[Bollegala et al., 2010] Danushka Bollegala, Yutaka Matsuo,\nand Mitsuru Ishizuka. Relational duality: Unsupervised\nextraction of semantic relations between entities on the\nweb. In WWW 2010, pages 151 \u2013 160, 2010.\n[Cho et al., 2014] Kyunghyun Cho, Bart van Merrienboer,\nCaglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-\nger Schwenk, and Yoshua Bengio. Learning phrase rep-\nresentations using rnn encoder\u2013decoder for statistical ma-\nchine translation. In EMNP\u201914, pages 1724\u20131734, 2014.\n[Collobert et al., 2011] Ronan\nCollobert,\nJason\nWeston,\nLeon Bottou, Michael Karlen, Koray Kavukcuoglu, and\nPavel Kuska. Natural language processing (almost) from\nscratch. Journal of Machine Learning Research, 12:2493\n\u2013 2537, 2011.\n[Duc et al., 2010] Nguyen Tuan Duc, Danushka Bollegala,\nand Mitsuru Ishizuka. Using relational similarity between\nword pairs for latent relational search on the web.\nIn\nIEEE/WIC/ACM International Conference on Web Intelli-\ngence and Intelligent Agent Technology, pages 196 \u2013 199,\n2010.\n[Duc et al., 2011] Nguyen Tuan Duc, Danushka Bollegala,\nand Mitsuru Ishizuka.\nCross-language latent relational\nsearch: Mapping knowledge across languages. In Proc.\nof the Twenty-Fifth AAAI Conference on Arti\ufb01cial Intelli-\ngence, pages 1237 \u2013 1242, 2011.\n[Duchi et al., 2011] John Duchi, Elad Hazan, and Yoram\nSinger. Adaptive subgradient methods for online learning\nand stochastic optimization. Journal of Machine Learning\nResearch, 12:2121 \u2013 2159, July 2011.\n[Hashimoto et al., 2014] Kazuma Hashimoto, Pontus Stene-\ntorp, Makoto Miwa, and Yoshimasa Tsuruoka.\nJointly\nlearning word representations and composition functions\nusing predicate-argument structures. In EMNLP\u201914, pages\n1544\u20131555, 2014.\n[Huang et al., 2012] Eric\nH.\nHuang,\nRichard\nSocher,\nChristopher D. Manning, and Andrew Y. Ng. Improving\nword representations via global context and multiple word\nprototypes. In ACL\u201912, pages 873 \u2013 882, 2012.\n[Jurgens et al., 2012] David A. Jurgens, Saif Mohammad,\nPeter D. Turney, and Keith J. Holyoak. Measuring degrees\nof relational similarity. In SemEval\u201912, 2012.\n[Levy and Goldberg, 2014] Omer Levy and Yoav Goldberg.\nLinguistic regularities in sparse and explicit word repre-\nsentations. In CoNLL, 2014.\n[Mikolov et al., 2013a] Tomas Mikolov, Kai Chen, and Jef-\nfrey Dean. Ef\ufb01cient estimation of word representation in\nvector space. CoRR, 2013.\n[Mikolov et al., 2013b] Tomas Mikolov, Quoc V. Le, and\nIlya Sutskever. Exploiting similarities among languages\nfor machine translation. arXiv, 2013.\n[Mikolov et al., 2013c] Tomas Mikolov, Ilya Sutskever, Kai\nChen, Gregory S. Corrado, and Jeffrey Dean. Distributed\nrepresentations of words and phrases and their composi-\ntionality. In NIPS, pages 3111 \u2013 3119, 2013.\n[Mikolov et al., 2013d] Tomas Mikolov, Wen tau Yih, and\nGeoffrey Zweig. Linguistic regularities in continous space\nword representations.\nIn NAACL\u201913, pages 746 \u2013 751,\n2013.\n[Mnih and Kavukcuoglu, 2013] Andriy Mnih and Koray\nKavukcuoglu. Learning word embeddings ef\ufb01ciently with\nnoise-contrastive estimation. In NIPS, 2013.\n[Pennington et al., 2014] Jeffery\nPennington,\nRichard\nSocher, and Christopher D. Manning.\nGlove: global\nvectors for word representation. In EMNLP, 2014.\n[Socher et al., 2011] Richard Socher, Jeffrey Pennington,\nEric H. Huang, Andrew Y. Ng, and Christopher D. Man-\nning. Semi-supervised recursive autoencoders for predict-\ning sentiment distributions.\nIn EMNLP\u201911, pages 151\u2013\n161, 2011.\n[Turian et al., 2010] Joseph Turian, Lev Ratinov, and Yoshua\nBengio.\nWord representations: A simple and general\nmethod for semi-supervised learning. In ACL, pages 384\n\u2013 394, 2010.\n[Turney and Pantel, 2010] Peter D. Turney and Patrick Pan-\ntel. From frequency to meaning: Vector space models of\nsemantics.\nJournal of Ariti\ufb01cial Intelligence Research,\n37:141 \u2013 188, 2010.\n[Turney, 2005] P.D. Turney. Measuring semantic similarity\nby latent relational analysis. In Proc. of IJCAI\u201905, pages\n1136\u20131141, 2005.\n[Turney, 2006] P.D. Turney. Similarity of semantic relations.\nComputational Linguistics, 32(3):379\u2013416, 2006.\n",
        "sentence": " Learning the association between semantic relations has been used in related problems such as relational similarity measurement (Turney, 2012) and relation adaptation (Bollegala et al., 2015).",
        "context": "1136\u20131141, 2005.\n[Turney, 2006] P.D. Turney. Similarity of semantic relations.\nComputational Linguistics, 32(3):379\u2013416, 2006.\nAbstract\nLearning representations for semantic relations is\nimportant for various tasks such as analogy de-\ntection, relational search, and relation classi\ufb01ca-\ntion. Although there have been several proposals\nEmbedding Semantic Relations into Word Representations\nDanushka Bollegala Takanori Maehara Ken-ichi Kawarabayashi\nUniversity of Liverpool Shizuoka University National Institute of Informatics\nJST, ERATO, Kawarabayashi Large Graph Project.\nAbstract"
    },
    {
        "title": "A Semantic Matching Energy Function for Learning with Multi-relational Data",
        "author": [
            "Antoine Bordes",
            "Xavier Glorot",
            "Jason Weston",
            "Yoshua Bengio."
        ],
        "venue": "Machine Learning 94(2):233\u2013259.",
        "citeRegEx": "Bordes et al\\.,? 2014",
        "shortCiteRegEx": "Bordes et al\\.",
        "year": 2014,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " As a seminal work, Bordes et al. (2013) proposes the TransE, which models the statistical regularities with linear translations between entity embeddings operated by a relation embed- dependent space (Bordes et al., 2014; Ji et al., 2015; Lin et al., 2015b; Nguyen et al., 2016b), and then model the translation property in the projected space. To evaluate link prediction, we conduct experiments on the WN18 (WordNet) and FB15k (Freebase) introduced by Bordes et al. (2013) and use the same training/validation/test split as in (Bordes et al. We follow Bordes et al. (2013) to report the filter results, i. by TransE (Bordes et al., 2013), following Lin et al. (2015b); Ji et al. by TransE (Bordes et al., 2013), following Lin et al. (2015b); Ji et al. (2015); Garc\u0131\u0301a-Dur\u00e1n et al. by TransE (Bordes et al., 2013), following Lin et al. (2015b); Ji et al. (2015); Garc\u0131\u0301a-Dur\u00e1n et al. (2016, 2015); Lin et al. (2015a). We ran minibatch SGD until convergence. 8 Unstructured (Bordes et al., 2014) No 304 38.",
        "context": null
    },
    {
        "title": "Translating Embeddings for Modeling Multirelational Data",
        "author": [
            "Antoine Bordes",
            "Nicolas Usunier",
            "Alberto GarciaDuran",
            "Jason Weston",
            "Oksana Yakhnenko."
        ],
        "venue": "Advances in Neural Information Processing Systems 26, pages 2787\u20132795.",
        "citeRegEx": "Bordes et al\\.,? 2013",
        "shortCiteRegEx": "Bordes et al\\.",
        "year": 2013,
        "abstract": "",
        "full_text": "",
        "sentence": " , 2013), TransE (Bordes et al., 2013) represents the head entity h, the relation r and the tail entity t with vectors h, r and t \u2208 Rn respectively, which were trained so that h+r \u2248 t. Previous work (Bordes et al., 2013; Lin et al., 2015b; Yang et al., 2015; Nguyen et al., 2016b) generally constructs a set of corrupted triples by replacing the head entity or tail entity with a random entity uniformly sampled from the KB. (2013) and use the same training/validation/test split as in (Bordes et al., 2013). by TransE (Bordes et al., 2013), following Lin et al. 3 TransE (Bordes et al., 2013) No 251 89.",
        "context": null
    },
    {
        "title": "Learning Structured Embeddings of Knowledge Bases",
        "author": [
            "Antoine Bordes",
            "Jason Weston",
            "Ronan Collobert",
            "Yoshua Bengio."
        ],
        "venue": "Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence. pages 301\u2013306.",
        "citeRegEx": "Bordes et al\\.,? 2011",
        "shortCiteRegEx": "Bordes et al\\.",
        "year": 2011,
        "abstract": "\n      \n        Many Knowledge Bases (KBs) are now readily available and encompass colossal quantities of information thanks to either a long-term funding effort (e.g. WordNet, OpenCyc) or a collaborative process (e.g. Freebase, DBpedia). However, each of them is based on a different rigorous symbolic framework which makes it hard to use their data in other systems. It is unfortunate because such rich structured knowledge might lead to a huge leap forward in many other areas of AI like nat- ural language processing (word-sense disambiguation, natural language understanding, ...), vision (scene classification, image semantic annotation, ...) or collaborative filtering. In this paper, we present a learning process based on an innovative neural network architecture designed to embed any of these symbolic representations into a more flexible continuous vector space in which the original knowledge is kept and enhanced. These learnt embeddings would allow data from any KB to be easily used in recent machine learning meth- ods for prediction and information retrieval. We illustrate our method on WordNet and Freebase and also present a way to adapt it to knowledge extraction from raw text.\n      \n    ",
        "full_text": "",
        "sentence": " Model Additional Information WN18 FB15k Mean Rank Hits@10 Mean Rank Hits@10 SE (Bordes et al., 2011) No 985 80.",
        "context": null
    },
    {
        "title": "Cfo: Conditional focused neural question answering with largescale knowledge bases",
        "author": [
            "Zihang Dai",
            "Lei Li",
            "Wei Xu."
        ],
        "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Asso-",
        "citeRegEx": "Dai et al\\.,? 2016",
        "shortCiteRegEx": "Dai et al\\.",
        "year": 2016,
        "abstract": "",
        "full_text": "",
        "sentence": " for many applications such as question answering (Berant et al., 2013; Yih et al., 2015; Dai et al., 2016) and information extraction (Mintz et al.",
        "context": null
    },
    {
        "title": "Sparse overcomplete word vector representations",
        "author": [
            "Manaal Faruqui",
            "Yulia Tsvetkov",
            "Dani Yogatama",
            "Chris Dyer",
            "Noah A. Smith."
        ],
        "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International",
        "citeRegEx": "Faruqui et al\\.,? 2015",
        "shortCiteRegEx": "Faruqui et al\\.",
        "year": 2015,
        "abstract": "Current distributed representations of words show little resemblance to\ntheories of lexical semantics. The former are dense and uninterpretable, the\nlatter largely based on familiar, discrete classes (e.g., supersenses) and\nrelations (e.g., synonymy and hypernymy). We propose methods that transform\nword vectors into sparse (and optionally binary) vectors. The resulting\nrepresentations are more similar to the interpretable features typically used\nin NLP, though they are discovered automatically from raw corpora. Because the\nvectors are highly sparse, they are computationally easy to work with. Most\nimportantly, we find that they outperform the original vectors on benchmark\ntasks.",
        "full_text": "Sparse Overcomplete Word Vector Representations\nManaal Faruqui\nYulia Tsvetkov\nDani Yogatama\nChris Dyer\nNoah A. Smith\nLanguage Technologies Institute\nCarnegie Mellon University\nPittsburgh, PA, 15213, USA\n{mfaruqui,ytsvetko,dyogatama,cdyer,nasmith}@cs.cmu.edu\nAbstract\nCurrent\ndistributed\nrepresentations\nof\nwords show little resemblance to theo-\nries of lexical semantics.\nThe former\nare dense and uninterpretable, the lat-\nter largely based on familiar, discrete\nclasses (e.g., supersenses) and relations\n(e.g., synonymy and hypernymy). We pro-\npose methods that transform word vec-\ntors into sparse (and optionally binary)\nvectors. The resulting representations are\nmore similar to the interpretable features\ntypically used in NLP, though they are dis-\ncovered automatically from raw corpora.\nBecause the vectors are highly sparse, they\nare computationally easy to work with.\nMost importantly, we \ufb01nd that they out-\nperform the original vectors on benchmark\ntasks.\n1\nIntroduction\nDistributed representations of words have been\nshown to bene\ufb01t NLP tasks like parsing (Lazari-\ndou et al., 2013; Bansal et al., 2014), named en-\ntity recognition (Guo et al., 2014), and sentiment\nanalysis (Socher et al., 2013). The attraction of\nword vectors is that they can be derived directly\nfrom raw, unannotated corpora. Intrinsic evalua-\ntions on various tasks are guiding methods toward\ndiscovery of a representation that captures many\nfacts about lexical semantics (Turney, 2001; Tur-\nney and Pantel, 2010).\nYet word vectors do not look anything like the\nrepresentations described in most lexical seman-\ntic theories, which focus on identifying classes of\nwords (Levin, 1993; Baker et al., 1998; Schuler,\n2005) and relationships among word meanings\n(Miller, 1995).\nThough expensive to construct,\nconceptualizing word meanings symbolically is\nimportant for theoretical understanding and also\nwhen we incorporate lexical semantics into com-\nputational models where interpretability is de-\nsired. On the surface, discrete theories seem in-\ncommensurate with the distributed approach, a\nproblem now receiving much attention in compu-\ntational linguistics (Lewis and Steedman, 2013;\nKiela and Clark, 2013; Vecchi et al., 2013; Grefen-\nstette, 2013; Lewis and Steedman, 2014; Paperno\net al., 2014).\nOur contribution to this discussion is a new,\nprincipled sparse coding method that transforms\nany distributed representation of words into sparse\nvectors, which can then be transformed into binary\nvectors (\u00a72). Unlike recent approaches of incorpo-\nrating semantics in distributional word vectors (Yu\nand Dredze, 2014; Xu et al., 2014; Faruqui et al.,\n2015), the method does not rely on any external\ninformation source. The transformation results in\nlonger, sparser vectors, sometimes called an \u201cover-\ncomplete\u201d representation (Olshausen and Field,\n1997). Sparse, overcomplete representations have\nbeen motivated in other domains as a way to in-\ncrease separability and interpretability, with each\ninstance (here, a word) having a small number\nof active dimensions (Olshausen and Field, 1997;\nLewicki and Sejnowski, 2000), and to increase\nstability in the presence of noise (Donoho et al.,\n2006).\nOur work builds on recent explorations of spar-\nsity as a useful form of inductive bias in NLP and\nmachine learning more broadly (Kazama and Tsu-\njii, 2003; Goodman, 2004; Friedman et al., 2008;\nGlorot et al., 2011; Yogatama and Smith, 2014,\ninter alia). Introducing sparsity in word vector di-\nmensions has been shown to improve dimension\ninterpretability (Murphy et al., 2012; Fyshe et al.,\n2014) and usability of word vectors as features in\ndownstream tasks (Guo et al., 2014). The word\nvectors we produce are more than 90% sparse; we\nalso consider binarizing transformations that bring\nthem closer to the categories and relations of lex-\narXiv:1506.02004v1  [cs.CL]  5 Jun 2015\nical semantic theories. Using a number of state-\nof-the-art word vectors as input, we \ufb01nd consis-\ntent bene\ufb01ts of our method on a suite of standard\nbenchmark evaluation tasks (\u00a73). We also evalu-\nate our word vectors in a word intrusion experi-\nment with humans (Chang et al., 2009) and \ufb01nd\nthat our sparse vectors are more interpretable than\nthe original vectors (\u00a74).\nWe anticipate that sparse, binary vectors can\nplay an important role as features in statistical\nNLP models, which still rely predominantly on\ndiscrete, sparse features whose interpretability en-\nables error analysis and continued development.\nWe have made an implementation of our method\npublicly available.1\n2\nSparse Overcomplete Word Vectors\nWe consider methods for transforming dense word\nvectors to sparse, binary overcomplete word vec-\ntors. Fig. 1 shows two approaches. The one on the\ntop, method A, converts dense vectors to sparse\novercomplete vectors (\u00a72.1).\nThe one beneath,\nmethod B, converts dense vectors to sparse and bi-\nnary overcomplete vectors (\u00a72.2 and \u00a72.4).\nLet V be the vocabulary size. In the following,\nX \u2208RL\u00d7V is the matrix constructed by stack-\ning V non-sparse \u201cinput\u201d word vectors of length\nL (produced by an arbitrary word vector estima-\ntor). We will refer to these as initializing vectors.\nA \u2208RK\u00d7V contains V sparse overcomplete word\nvectors of length K. \u201cOvercomplete\u201d representa-\ntion learning implies that K > L.\n2.1\nSparse Coding\nIn sparse coding (Lee et al., 2006), the goal is to\nrepresent each input vector xi as a sparse linear\ncombination of basis vectors, ai. Our experiments\nconsider four initializing methods for these vec-\ntors, discussed in Appendix A. Given X, we seek\nto solve\narg min\nD,A\n\u2225X \u2212DA\u22252\n2 + \u03bb\u2126(A) + \u03c4\u2225D\u22252\n2,\n(1)\nwhere D \u2208RL\u00d7K is the dictionary of basis vec-\ntors. \u03bb is a regularization hyperparameter, and \u2126is\nthe regularizer. Here, we use the squared loss for\nthe reconstruction error, but other loss functions\ncould also be used (Lee et al., 2009). To obtain\nsparse word representations we will impose an \u21131\n1https://github.com/mfaruqui/\nsparse-coding\npenalty on A. Eq. 1 can be broken down into loss\nfor each word vector which can be optimized sep-\narately in parallel (\u00a72.3):\narg min\nD,A\nV\nX\ni=1\n\u2225xi \u2212Dai\u22252\n2 +\u03bb\u2225ai\u22251 +\u03c4\u2225D\u22252\n2 (2)\nwhere mi denotes the ith column vector of matrix\nM. Note that this problem is not convex. We refer\nto this approach as method A.\n2.2\nSparse Nonnegative Vectors\nNonnegativity in the feature space has often been\nshown to correspond to interpretability (Lee and\nSeung, 1999; Cichocki et al., 2009; Murphy et al.,\n2012; Fyshe et al., 2014; Fyshe et al., 2015). To\nobtain nonnegative sparse word vectors, we use a\nvariation of the nonnegative sparse coding method\n(Hoyer, 2002). Nonnegative sparse coding further\nconstrains the problem in Eq. 2 so that D and ai\nare nonnegative. Here, we apply this constraint\nonly to the representation vectors {ai}. Thus, the\nnew objective for nonnegative sparse vectors be-\ncomes:\narg min\nD\u2208RL\u00d7K\n\u22650\n,A\u2208RK\u00d7V\n\u22650\nV\nX\ni=1\n\u2225xi\u2212Dai\u22252\n2+\u03bb\u2225ai\u22251+\u03c4\u2225D\u22252\n2\n(3)\nThis problem will play a role in our second ap-\nproach, method B, to which we will return shortly.\nThis nonnegativity constraint can be easily incor-\nporated during optimization, as explained next.\n2.3\nOptimization\nWe use online adaptive gradient descent (Ada-\nGrad; Duchi et al., 2010) for solving the optimiza-\ntion problems in Eqs. 2\u20133 by updating A and D.\nIn order to speed up training we use asynchronous\nupdates to the parameters of the model in parallel\nfor every word vector (Duchi et al., 2012; Heigold\net al., 2014).\nHowever, directly applying stochastic subgradi-\nent descent to an \u21131-regularized objective fails to\nproduce sparse solutions in bounded time, which\nhas motivated several specialized algorithms that\ntarget such objectives. We use the AdaGrad vari-\nant of one such learning algorithm, the regular-\nized dual averaging algorithm (Xiao, 2009), which\nkeeps track of the online average gradient at time\nt: \u00afgt = 1\nt\nPt\nt\u2032=1 gt\u2032 Here, the subgradients do not\ninclude terms for the regularizer; they are deriva-\ntives of the unregularized objective (\u03bb = 0, \u03c4 = 0)\nX\nL\nV\nK\nx\nV\nD\nA\nK\nK\nx\nV\nD\nV\nK\nB\nSparse overcomplete vectors\nSparse, binary overcomplete \nvectors\nProjection\nSparse coding\nNon-negative sparse coding\nInitial dense vectors\nFigure 1: Methods for obtaining sparse overcomplete vectors (top, method A, \u00a72.1) and sparse, binary\novercomplete word vectors (bottom, method B, \u00a72.2 and \u00a72.4). Observed dense vectors of length L (left)\nare converted to sparse non-negative vectors (center) of length K which are then projected into the binary\nvector space (right), where L \u226aK. X is dense, A is sparse, and B is the binary word vector matrix.\nStrength of colors signify the magnitude of values; negative is red, positive is blue, and zero is white.\nwith respect to ai. We de\ufb01ne\n\u03b3 = \u2212sign(\u00afgt,i,j)\n\u03b7t\np\nGt,i,j\n(|\u00afgt,i,j| \u2212\u03bb),\nwhere Gt,i,j = Pt\nt\u2032=1 g2\nt\u2032,i,j. Now, using the av-\nerage gradient, the \u21131-regularized objective is op-\ntimized as follows:\nat+1,i,j =\n(\n0,\nif |\u00afgt,i,j| \u2264\u03bb\n\u03b3,\notherwise\n(4)\nwhere, at+1,i,j is the jth element of sparse vector\nai at the tth update and \u00afgt,i,j is the correspond-\ning average gradient. For obtaining nonnegative\nsparse vectors we take projection of the updated ai\nonto RK\n\u22650 by choosing the closest point in RK\n\u22650 ac-\ncording to Euclidean distance (which corresponds\nto zeroing out the negative elements):\nat+1,i,j =\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\n0,\nif |\u00afgt,i,j| \u2264\u03bb\n0,\nif \u03b3 < 0\n\u03b3,\notherwise\n(5)\n2.4\nBinarizing Transformation\nOur aim with method B is to obtain word rep-\nresentations that can emulate the binary-feature\nX\nL\n\u03bb\n\u03c4\nK\n% Sparse\nGlove\n300\n1.0\n10\u22125\n3000\n91\nSG\n300\n0.5\n10\u22125\n3000\n92\nGC\n50\n1.0\n10\u22125\n500\n98\nMulti\n48\n0.1\n10\u22125\n960\n93\nTable 1:\nHyperparameters for learning sparse\novercomplete vectors tuned on the WS-353 task.\nTasks are explained in \u00a7B. The four initial vector\nrepresentations X are explained in \u00a7A.\nhot, fresh, \ufb01sh, 1/2, wine, salt\nseries, tv, appearances, episodes\n1975, 1976, 1968, 1970, 1977, 1969\ndress, shirt, ivory, shirts, pants\nupscale, af\ufb02uent, catering, clientele\nTable 2: Highest frequency words in randomly\npicked word clusters of binary sparse overcom-\nplete Glove vectors.\nspace designed for various NLP tasks. We could\nstate this as an optimization problem:\narg min\nD\u2208RL\u00d7K\nB\u2208{0,1}K\u00d7V\nV\nX\ni=1\n\u2225xi \u2212Dbi\u22252\n2 + \u03bb\u2225bi\u22251\n1 + \u03c4\u2225D\u22252\n2\n(6)\nwhere B denotes the binary (and also sparse) rep-\nresentation. This is an mixed integer bilinear pro-\ngram, which is NP-hard (Al-Khayyal and Falk,\n1983). Unfortunately, the number of variables in\nthe problem is \u2248KV which reaches 100 million\nwhen V = 100, 000 and K = 1, 000, which is\nintractable to solve using standard techniques.\nA more tractable relaxation to this hard prob-\nlem is to \ufb01rst constrain the continuous represen-\ntation A to be nonnegative (i.e, ai \u2208RK\n\u22650; \u00a72.2).\nThen, in order to avoid an expensive computation,\nwe take the nonnegative word vectors obtained us-\ning Eq. 3 and project nonzero values to 1, preserv-\ning the 0 values. Table 2 shows a random set of\nword clusters obtained by (i) applying our method\nto Glove initial vectors and (ii) applying k-means\nclustering (k = 100). In \u00a73 we will \ufb01nd that these\nvectors perform well quantitatively.\n2.5\nHyperparameter Tuning\nMethods A and B have three hyperparameters: the\n\u21131-regularization penalty \u03bb, the \u21132-regularization\npenalty \u03c4, and the length of the overcomplete word\nvector representation K. We perform a grid search\non \u03bb \u2208{0.1, 0.5, 1.0} and K \u2208{10L, 20L}, se-\nlecting values that maximizes performance on one\n\u201cdevelopment\u201d word similarity task (WS-353, dis-\ncussed in \u00a7B) while achieving at least 90% sparsity\nin overcomplete vectors. \u03c4 was tuned on one col-\nlection of initializing vectors (Glove, discussed in\n\u00a7A) so that the vectors in D are near unit norm.\nThe four vector representations and their corre-\nsponding hyperparameters selected by this proce-\ndure are summarized in Table 1. These hyperpa-\nrameters were chosen for method A and retained\nfor method B.\n3\nExperiments\nUsing methods A and B, we constructed sparse\novercomplete vector representations A and B\nresp., starting from four initial vector representa-\ntions X; these are explained in Appendix A. We\nused one benchmark evaluation (WS-353) to tune\nhyperparameters, resulting in the settings shown\nin Table 1; seven other tasks were used to evalu-\nate the quality of the sparse overcomplete repre-\nsentations. The \ufb01rst of these is a word similar-\nity task, where the score is correlation with hu-\nman judgments, and the others are classi\ufb01cation\naccuracies of an \u21132-regularized logistic regression\nmodel trained using the word vectors. These tasks\nare described in detail in Appendix B.\n3.1\nEffects of Transforming Vectors\nFirst, we quantify the effects of our transforma-\ntions by comparing their output to the initial (X)\nvectors. Table 3 shows consistent improvements\nof sparsifying vectors (method A). The exceptions\nare on the SimLex task, where our sparse vectors\nare worse than the skip-gram initializer and on par\nwith the multilingual initializer. Sparsi\ufb01cation is\nbene\ufb01cial across all of the text classi\ufb01cation tasks,\nfor all initial vector representations. On average\nacross all vector types and all tasks, sparse over-\ncomplete vectors outperform their corresponding\ninitializers by 4.2 points.2\nBinarized vectors (from method B) are also usu-\nally better than the initial vectors (also shown in\nTable 3), and tend to outperform the sparsi\ufb01ed\nvariants, except when initializing with Glove. On\naverage across all vector types and all tasks, bina-\nrized overcomplete vectors outperform their cor-\nresponding initializers by 4.8 points and the con-\ntinuous, sparse intermediate vectors by 0.6 points.\nFrom here on, we explore more deeply the\nsparse overcomplete vectors from method A (de-\nnoted by A), leaving binarization (method B)\naside.\n3.2\nEffect of Vector Length\nHow does the length of the overcomplete vector\n(K) affect performance? We focus here on the\nGlove vectors, where L = 300, and report av-\nerage performance across all tasks. We consider\nK = \u03b1L where \u03b1 \u2208{1, 2, 3, 5, 10, 15, 20}. Fig-\nure 2 plots the average performance across tasks\nagainst \u03b1. The earlier selection of K = 3, 000\n(\u03b1 = 10) gives the best result; gains are mono-\ntonic in \u03b1 to that point and then begin to diminish.\n3.3\nAlternative Transformations\nWe consider two alternative transformations. The\n\ufb01rst preserves the original vector length but\n2We report correlation on a 100 point scale, so that the\naverage which includes accuracuies and correlation is equally\nrepresentatitve of both.\nVectors\nSimLex\nSenti.\nTREC\nSports\nComp.\nRelig.\nNP\nAverage\nCorr.\nAcc.\nAcc.\nAcc.\nAcc.\nAcc.\nAcc.\nGlove\nX\n36.9\n77.7\n76.2\n95.9\n79.7\n86.7\n77.9\n76.2\nA\n38.9\n81.4\n81.5\n96.3\n87.0\n88.8\n82.3\n79.4\nB\n39.7\n81.0\n81.2\n95.7\n84.6\n87.4\n81.6\n78.7\nSG\nX\n43.6\n81.5\n77.8\n97.1\n80.2\n85.9\n80.1\n78.0\nA\n41.7\n82.7\n81.2\n98.2\n84.5\n86.5\n81.6\n79.4\nB\n42.8\n81.6\n81.6\n95.2\n86.5\n88.0\n82.9\n79.8\nGC\nX\n9.7\n68.3\n64.6\n75.1\n60.5\n76.0\n79.4\n61.9\nA\n12.0\n73.3\n77.6\n77.0\n68.3\n81.0\n81.2\n67.2\nB\n18.7\n73.6\n79.2\n79.7\n70.5\n79.6\n79.4\n68.6\nMulti\nX\n28.7\n75.5\n63.8\n83.6\n64.3\n81.8\n79.2\n68.1\nA\n28.1\n78.6\n79.2\n93.9\n78.2\n84.5\n81.1\n74.8\nB\n28.7\n77.6\n82.0\n94.7\n81.4\n85.6\n81.9\n75.9\nTable 3: Performance comparison of transformed vectors to initial vectors X. We show sparse over-\ncomplete representations A and also binarized representations B. Initial vectors are discussed in \u00a7A and\ntasks in \u00a7B.\nFigure 2: Average performace across all tasks\nfor sparse overcomplete vectors (A) produced by\nGlove initial vectors, as a function of the ratio of\nK to L.\nachieves a binary, sparse vector (B) by applying:\nbi,j =\n\u001a 1\nif xi,j > 0\n0\notherwise\n(7)\nThe second transformation was proposed by\nGuo et al. (2014). Here, the original vector length\nis also preserved, but sparsity is achieved through:\nai,j =\n\uf8f1\n\uf8f2\n\uf8f3\n1\nif xi,j \u2265M+\n\u22121\nif xi,j \u2264M\u2212\n0\notherwise\n(8)\nwhere M+ (M\u2212) is the mean of positive-valued\n(negative-valued) elements of X. These vectors\nare, obviously, not binary.\nWe \ufb01nd that on average, across initializing vec-\ntors and across all tasks that our sparse overcom-\nplete (A) vectors lead to better performance than\neither of the alternative transformations.\n4\nInterpretability\nOur hypothesis is that the dimensions of sparse\novercomplete vectors are more interpretable than\nthose of dense word vectors. Following Murphy\net al. (2012), we use a word intrusion experiment\n(Chang et al., 2009) to corroborate this hypothesis.\nIn addition, we conduct qualitative analysis of in-\nterpretability, focusing on individual dimensions.\n4.1\nWord Intrusion\nWord intrusion experiments seek to quantify the\nextent to which dimensions of a learned word rep-\nresentation are coherent to humans.\nIn one in-\nstance of the experiment, a human judge is pre-\nsented with \ufb01ve words in random order and asked\nto select the \u201cintruder.\u201d The words are selected by\nthe experimenter by choosing one dimension j of\nthe learned representation, then ranking the words\non that dimension alone. The dimensions are cho-\nsen in decreasing order of the variance of their\nvalues across the vocabulary. Four of the words\nare the top-ranked words according to j, and the\n\u201ctrue\u201d intruder is a word from the bottom half of\nthe list, chosen to be a word that appears in the top\n10% of some other dimension. An example of an\ninstance is:\nnaval, industrial, technological, marine, identity\nX:\nGlove\nSG\nGC\nMulti\nAverage\nX\n76.2\n78.0\n61.9\n68.1\n71.0\nEq. 7\n75.7\n75.8\n60.5\n64.1\n69.0\nEq. 8 (Guo et al., 2014)\n75.8\n76.9\n60.5\n66.2\n69.8\nA\n79.4\n79.4\n67.2\n74.8\n75.2\nTable 4: Average performance across all tasks and vector models using different transformations.\nVectors\nA1\nA2\nA3\nAvg.\nIAA\n\u03ba\nX\n61\n53\n56\n57\n70\n0.40\nA\n71\n70\n72\n71\n77\n0.45\nTable 5: Accuracy of three human annotators on\nthe word intrusion task, along with the average\ninter-annotator agreement (Artstein and Poesio,\n2008) and Fleiss\u2019 \u03ba (Davies and Fleiss, 1982).\n(The last word is the intruder.)\nWe formed instances from initializing vectors\nand from our sparse overcomplete vectors (A).\nEach of these two combines the four different ini-\ntializers X. We selected the 25 dimensions d in\neach case. Each of the 100 instances per condition\n(initial vs. sparse overcomplete) was given to three\njudges.\nResults in Table 5 con\ufb01rm that the sparse over-\ncomplete vectors are more interpretable than the\ndense vectors. The inter-annotator agreement on\nthe sparse vectors increases substantially, from\n57% to 71%, and the Fleiss\u2019 \u03ba increases from\n\u201cfair\u201d to \u201cmoderate\u201d agreement (Landis and Koch,\n1977).\n4.2\nQualitative Evaluation of Interpretability\nIf a vector dimension is interpretable, the top-\nranking words for that dimension should display\nsemantic or syntactic groupings.\nTo verify this\nqualitatively, we select \ufb01ve dimensions with the\nhighest variance of values in initial and sparsi-\n\ufb01ed GC vectors. We compare top-ranked words in\nthe dimensions extracted from the two representa-\ntions. The words are listed in Table 6, a dimension\nper row. Subjectively, we \ufb01nd the semantic group-\nings better in the sparse vectors than in the initial\nvectors.\nFigure 3 visualizes the sparsi\ufb01ed GC vectors for\nsix words. The dimensions are sorted by the aver-\nage value across the three \u201canimal\u201d vectors. The\nanimal-related words use many of the same di-\nmensions (102 common active dimensions out of\n500 total); in constrast, the three city names use\nX\ncombat, guard, honor, bow, trim, naval\n\u2019ll, could, faced, lacking, seriously, scored\nsee, n\u2019t, recommended, depending, part\ndue, positive, equal, focus, respect, better\nsergeant, comments, critics, she, videos\nA\nfracture, breathing, wound, tissue, relief\nrelationships, connections, identity, relations\n\ufb01les, bills, titles, collections, poems, songs\nnaval, industrial, technological, marine\nstadium, belt, championship, toll, ride, coach\nTable 6: Top-ranked words per dimension for ini-\ntial and sparsi\ufb01ed GC representations. Each line\nshows words from a different dimension.\nmostly distinct vectors.\n5\nRelated Work\nTo the best of our knowledge, there has been no\nprior work on obtaining overcomplete word vec-\ntor representations that are sparse and categorical.\nHowever, overcomplete features have been widely\nused in image processing, computer vision (Ol-\nshausen and Field, 1997; Lewicki and Sejnowski,\n2000) and signal processing (Donoho et al., 2006).\nNonnegative matrix factorization is often used for\ninterpretable coding of information (Lee and Se-\nung, 1999; Liu et al., 2003; Cichocki et al., 2009).\nSparsity constraints are in general useful in NLP\nproblems (Kazama and Tsujii, 2003; Friedman\net al., 2008; Goodman, 2004), like POS tagging\n(Ganchev et al., 2009), dependency parsing (Mar-\ntins et al., 2011), text classi\ufb01cation (Yogatama and\nSmith, 2014), and representation learning (Ben-\ngio et al., 2013; Yogatama et al., 2015). Includ-\ning sparsity constraints in Bayesian models of lex-\nical semantics like LDA in the form of sparse\nDirichlet priors has been shown to be useful for\ndownstream tasks like POS-tagging (Toutanova\nand Johnson, 2007), and improving interpretation\n(Paul and Dredze, 2012; Zhu and Xing, 2012).\nV379\nV353\nV76\nV186\nV339\nV177\nV114\nV342\nV332\nV270\nV222\nV91\nV303\nV473\nV355\nV358\nV164\nV348\nV324\nV192\nV24\nV281\nV82\nV46\nV277\nV466\nV465\nV128\nV11\nV413\nV98\nV131\nV445\nV199\nV475\nV208\nV431\nV299\nV357\nV149\nV80\nV247\nV231\nV42\nV44\nV376\nV152\nV74\nV254\nV141\nV341\nV349\nV234\nV55\nV477\nV272\nV217\nV457\nV57\nV159\nV223\nV310\nV436\nV325\nV211\nV117\nV360\nV483\nV363\nV439\nV403\nV119\nV329\nV83\nV371\nV424\nV179\nV214\nV268\nV38\nV102\nV93\nV89\nV12\nV172\nV173\nV285\nV344\nV78\nV227\nV426\nV430\nV241\nV384\nV460\nV347\nV171\nV289\nV380\nV8\nV2\nV3\nV5\nV6\nV7\nV10\nV14\nV15\nV16\nV17\nV18\nV19\nV20\nV21\nV22\nV25\nV26\nV28\nV29\nV30\nV31\nV32\nV33\nV35\nV36\nV37\nV39\nV40\nV41\nV43\nV45\nV47\nV49\nV50\nV51\nV52\nV54\nV56\nV58\nV59\nV60\nV63\nV64\nV65\nV67\nV68\nV69\nV70\nV72\nV75\nV77\nV81\nV87\nV90\nV92\nV94\nV99\nV101\nV103\nV105\nV106\nV108\nV110\nV111\nV116\nV118\nV122\nV123\nV125\nV130\nV132\nV133\nV136\nV137\nV138\nV139\nV140\nV143\nV144\nV147\nV148\nV150\nV155\nV158\nV160\nV162\nV165\nV166\nV167\nV168\nV169\nV170\nV174\nV175\nV178\nV180\nV181\nV182\nV183\nV185\nV188\nV189\nV190\nV191\nV193\nV194\nV195\nV196\nV202\nV203\nV204\nV205\nV212\nV213\nV215\nV218\nV220\nV224\nV226\nV228\nV232\nV233\nV235\nV236\nV238\nV239\nV240\nV242\nV243\nV244\nV248\nV249\nV250\nV251\nV252\nV253\nV255\nV258\nV259\nV260\nV261\nV262\nV263\nV264\nV265\nV266\nV271\nV273\nV274\nV278\nV282\nV284\nV287\nV288\nV290\nV292\nV293\nV294\nV296\nV300\nV302\nV304\nV307\nV308\nV311\nV312\nV313\nV314\nV316\nV317\nV318\nV319\nV320\nV321\nV322\nV323\nV327\nV330\nV331\nV333\nV334\nV336\nV338\nV340\nV343\nV345\nV346\nV352\nV356\nV361\nV362\nV366\nV368\nV369\nV370\nV372\nV373\nV375\nV377\nV378\nV381\nV382\nV383\nV385\nV386\nV387\nV388\nV389\nV390\nV391\nV392\nV394\nV395\nV396\nV398\nV399\nV400\nV401\nV402\nV404\nV405\nV406\nV407\nV408\nV409\nV410\nV412\nV414\nV415\nV416\nV417\nV418\nV419\nV420\nV422\nV423\nV425\nV427\nV428\nV429\nV433\nV434\nV435\nV437\nV441\nV442\nV444\nV446\nV449\nV450\nV451\nV452\nV453\nV455\nV456\nV458\nV459\nV461\nV462\nV463\nV464\nV467\nV468\nV469\nV471\nV472\nV478\nV479\nV480\nV481\nV482\nV484\nV485\nV486\nV488\nV489\nV490\nV491\nV492\nV493\nV494\nV495\nV497\nV499\nV500\nV501\nV487\nV200\nV326\nV4\nV121\nV267\nV230\nV438\nV134\nV97\nV104\nV351\nV219\nV13\nV88\nV129\nV286\nV229\nV350\nV96\nV107\nV153\nV145\nV154\nV34\nV301\nV374\nV109\nV397\nV156\nV161\nV297\nV115\nV151\nV245\nV447\nV53\nV337\nV79\nV448\nV283\nV443\nV201\nV393\nV365\nV48\nV126\nV257\nV246\nV295\nV120\nV367\nV27\nV184\nV209\nV306\nV269\nV124\nV470\nV112\nV187\nV62\nV474\nV354\nV454\nV279\nV146\nV275\nV221\nV207\nV71\nV335\nV73\nV85\nV440\nV95\nV23\nV225\nV411\nV328\nV305\nV198\nV163\nV9\nV135\nV315\nV142\nV498\nV291\nV86\nV476\nV210\nV359\nV84\nV100\nV309\nV176\nV216\nV432\nV206\nV421\nV276\nV237\nV61\nV157\nV364\nV127\nV66\nV256\nV280\nV113\nV298\nV197\nV496\nboston\nseattle\nchicago\ndog\nhorse\nfish\nFigure 3: Visualization of sparsi\ufb01ed GC vectors. Negative values are red, positive values are blue, zeroes\nare white.\n6\nConclusion\nWe have presented a method that converts word\nvectors obtained using any state-of-the-art word\nvector model into sparse and optionally binary\nword vectors. These transformed vectors appear to\ncome closer to features used in NLP tasks and out-\nperform the original vectors from which they are\nderived on a suite of semantics and syntactic eval-\nuation benchmarks. We also \ufb01nd that the sparse\nvectors are more interpretable than the dense vec-\ntors by humans according to a word intrusion de-\ntection test.\nAcknowledgments\nWe thank Alona Fyshe for discussions on vec-\ntor interpretability and three anonymous review-\ners for their feedback.\nThis research was sup-\nported in part by the National Science Foundation\nthrough grant IIS-1251131 and the Defense Ad-\nvanced Research Projects Agency through grant\nFA87501420244. This work was supported in part\nby the U.S. Army Research Laboratory and the\nU.S. Army Research Of\ufb01ce under contract/grant\nnumber W911NF-10-1-0533.\nA\nInitial Vector Representations (X)\nOur experiments consider four publicly available\ncollections of pre-trained word vectors. They vary\nin the amount of data used and the estimation\nmethod.\nGlove.\nGlobal vectors for word representations\n(Pennington et al., 2014) are trained on aggregated\nglobal word-word co-occurrence statistics from a\ncorpus. These vectors were trained on 6 billion\nwords from Wikipedia and English Gigaword and\nare of length 300.3\n3http://www-nlp.stanford.edu/projects/\nglove/\nSkip-Gram (SG).\nThe word2vec tool (Mikolov\net al., 2013) is fast and widely-used. In this model,\neach word\u2019s Huffman code is used as an input to\na log-linear classi\ufb01er with a continuous projection\nlayer and words within a given context window are\npredicted. These vectors were trained on 100 bil-\nlion words of Google news data and are of length\n300.4\nGlobal\nContext\n(GC).\nThese\nvectors\nare\nlearned using a recursive neural network that\nincorporates both local and global (document-\nlevel) context features (Huang et al., 2012). These\nvectors were trained on the \ufb01rst 1 billion words of\nEnglish Wikipedia and are of length 50.5\nMultilingual (Multi).\nFaruqui and Dyer (2014)\nlearned vectors by \ufb01rst performing SVD on text\nin different languages, then applying canonical\ncorrelation analysis on pairs of vectors for words\nthat align in parallel corpora. These vectors were\ntrained on WMT-2011 news corpus containing\n360 million words and are of length 48.6\nB\nEvaluation Benchmarks\nOur comparisons of word vector quality consider\n\ufb01ve benchmark tasks. We now describe the differ-\nent evaluation benchmarks for word vectors.\nWord Similarity.\nWe evaluate our word repre-\nsentations on two word similarity tasks. The \ufb01rst\nis the WS-353 dataset (Finkelstein et al., 2001),\nwhich contains 353 pairs of English words that\nhave been assigned similarity ratings by humans.\nThis dataset is used to tune sparse vector learning\nhyperparameters (\u00a72.5), while the remaining of the\ntasks discussed in this section are completely held\nout.\n4https://code.google.com/p/word2vec\n5http://nlp.stanford.edu/\u02dcsocherr/\nACL2012_wordVectorsTextFile.zip\n6http://cs.cmu.edu/\u02dcmfaruqui/soft.html\nA more recent dataset, SimLex-999 (Hill et al.,\n2014), has been constructed to speci\ufb01cally focus\non similarity (rather than relatedness).\nIt con-\ntains a balanced set of noun, verb, and adjective\npairs. We calculate cosine similarity between the\nvectors of two words forming a test item and re-\nport Spearman\u2019s rank correlation coef\ufb01cient (My-\ners and Well, 1995) between the rankings pro-\nduced by our model against the human rankings.\nSentiment\nAnalysis\n(Senti).\nSocher\net\nal.\n(2013) created a treebank of sentences anno-\ntated with \ufb01ne-grained sentiment labels on phrases\nand sentences from movie review excerpts. The\ncoarse-grained treebank of positive and negative\nclasses has been split into training, development,\nand test datasets containing 6,920, 872, and 1,821\nsentences, respectively.\nWe use average of the\nword vectors of a given sentence as feature for\nclassi\ufb01cation.\nThe classi\ufb01er is tuned on the\ndev. set and accuracy is reported on the test set.\nQuestion Classi\ufb01cation (TREC).\nAs an aid to\nquestion answering, a question may be classi-\n\ufb01ed as belonging to one of many question types.\nThe TREC questions dataset involves six differ-\nent question types, e.g., whether the question is\nabout a location, about a person, or about some nu-\nmeric information (Li and Roth, 2002). The train-\ning dataset consists of 5,452 labeled questions, and\nthe test dataset consists of 500 questions. An av-\nerage of the word vectors of the input question is\nused as features and accuracy is reported on the\ntest set.\n20 Newsgroup Dataset.\nWe consider three bi-\nnary categorization tasks from the 20 News-\ngroups dataset.7\nEach task involves categoriz-\ning a document according to two related cate-\ngories with training/dev./test split in accordance\nwith Yogatama and Smith (2014):\n(1) Sports:\nbaseball vs. hockey (958/239/796) (2) Comp.:\nIBM vs. Mac (929/239/777) (3) Religion: atheism\nvs. christian (870/209/717). We use average of the\nword vectors of a given sentence as features. The\nclassi\ufb01er is tuned on the dev. set and accuracy is\nreported on the test set.\nNP bracketing (NP).\nLazaridou et al. (2013)\nconstructed a dataset from the Penn Treebank\n(Marcus et al., 1993) of noun phrases (NP) of\n7http://qwone.com/\u02dcjason/20Newsgroups\nlength three words, where the \ufb01rst can be an ad-\njective or a noun and the other two are nouns. The\ntask is to predict the correct bracketing in the parse\ntree for a given noun phrase. For example, local\n(phone company) and (blood pressure) medicine\nexhibit right and left bracketing, respectively. We\nappend the word vectors of the three words in the\nNP in order and use them as features for binary\nclassi\ufb01cation.\nThe dataset contains 2,227 noun\nphrases split into 10 folds. The classi\ufb01er is tuned\non the \ufb01rst fold and cross-validation accuracy is\nreported on the remaining nine folds.\nReferences\nFaiz A. Al-Khayyal and James E. Falk. 1983. Jointly\nconstrained biconvex programming. Mathematics of\nOperations Research, pages 273\u2013286.\nRon Artstein and Massimo Poesio. 2008. Inter-coder\nagreement for computational linguistics. Computa-\ntional Linguistics, 34(4):555\u2013596.\nCollin F. Baker, Charles J. Fillmore, and John B. Lowe.\n1998. The Berkeley FrameNet project. In Proc. of\nACL.\nMohit Bansal, Kevin Gimpel, and Karen Livescu.\n2014. Tailoring continuous word representations for\ndependency parsing. In Proc. of ACL.\nYoshua Bengio, Aaron Courville, and Pascal Vincent.\n2013. Representation learning: A review and new\nperspectives. IEEE Transactions on Pattern Analy-\nsis and Machine Intelligence, 35(8):1798\u20131828.\nJonathan Chang, Sean Gerrish, Chong Wang, Jordan L.\nBoyd-Graber, and David M. Blei. 2009. Reading\ntea leaves: How humans interpret topic models. In\nNIPS.\nAndrzej Cichocki, Rafal Zdunek, Anh Huy Phan, and\nShun-ichi Amari. 2009. Nonnegative Matrix and\nTensor Factorizations: Applications to Exploratory\nMulti-way Data Analysis and Blind Source Separa-\ntion. John Wiley & Sons.\nMark Davies and Joseph L Fleiss. 1982. Measuring\nagreement for multinomial data. Biometrics, pages\n1047\u20131051.\nDavid L. Donoho, Michael Elad, and Vladimir N.\nTemlyakov. 2006. Stable recovery of sparse over-\ncomplete representations in the presence of noise.\nIEEE Transactions on Information Theory, 52(1).\nJohn Duchi, Elad Hazan, and Yoram Singer.\n2010.\nAdaptive subgradient methods for online learn-\ning and stochastic optimization. Technical Report\nEECS-2010-24, University of California Berkeley.\nJohn C. Duchi, Alekh Agarwal, and Martin J. Wain-\nwright. 2012. Dual averaging for distributed opti-\nmization: Convergence analysis and network scal-\ning.\nIEEE Transactions on Automatic Control,\n57(3):592\u2013606.\nManaal Faruqui and Chris Dyer.\n2014.\nImproving\nvector space word representations using multilingual\ncorrelation. In Proc. of EACL.\nManaal Faruqui, Jesse Dodge, Sujay K. Jauhar, Chris\nDyer, Eduard Hovy, and Noah A. Smith.\n2015.\nRetro\ufb01tting word vectors to semantic lexicons. In\nProc. of NAACL.\nLev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,\nEhud Rivlin, Zach Solan, Gadi Wolfman, and Ey-\ntan Ruppin. 2001. Placing search in context: the\nconcept revisited. In Proc. of WWW.\nJerome Friedman, Trevor Hastie, and Robert Tibshi-\nrani.\n2008.\nSparse inverse covariance estimation\nwith the graphical lasso.\nBiostatistics, 9(3):432\u2013\n441.\nAlona Fyshe, Partha P. Talukdar, Brian Murphy, and\nTom M. Mitchell. 2014. Interpretable semantic vec-\ntors from a joint model of brain- and text- based\nmeaning. In Proc. of ACL.\nAlona Fyshe, Leila Wehbe, Partha P. Talukdar, Brian\nMurphy, and Tom M. Mitchell. 2015. A composi-\ntional and interpretable semantic space. In Proc. of\nNAACL.\nKuzman Ganchev, Ben Taskar, Fernando Pereira, and\nJo\u02dcao Gama. 2009. Posterior vs. parameter sparsity\nin latent variable models. In NIPS.\nXavier Glorot, Antoine Bordes, and Yoshua Bengio.\n2011. Domain adaptation for large-scale sentiment\nclassi\ufb01cation: A deep learning approach. In Proc. of\nICML.\nJoshua Goodman. 2004. Exponential priors for maxi-\nmum entropy models. In Proc. of NAACL.\nE. Grefenstette. 2013. Towards a formal distributional\nsemantics: Simulating logical calculi with tensors.\narXiv:1304.5823.\nJiang Guo, Wanxiang Che, Haifeng Wang, and Ting\nLiu. 2014. Revisiting embedding features for sim-\nple semi-supervised learning. In Proc. of EMNLP.\nGeorg Heigold, Erik McDermott, Vincent Vanhoucke,\nAndrew Senior, and Michiel Bacchiani.\n2014.\nAsynchronous stochastic optimization for sequence\ntraining of deep neural networks.\nIn Proc. of\nICASSP.\nFelix Hill, Roi Reichart, and Anna Korhonen. 2014.\nSimlex-999: Evaluating semantic models with (gen-\nuine) similarity estimation. CoRR, abs/1408.3456.\nPatrik O. Hoyer. 2002. Non-negative sparse coding. In\nNeural Networks for Signal Processing, 2002. Proc.\nof IEEE Workshop on.\nEric H. Huang, Richard Socher, Christopher D. Man-\nning, and Andrew Y. Ng. 2012. Improving word\nrepresentations via global context and multiple word\nprototypes. In Proc. of ACL.\nJun\u2019ichi Kazama and Jun\u2019ichi Tsujii. 2003. Evaluation\nand extension of maximum entropy models with in-\nequality constraints. In Proc. of EMNLP.\nDouwe Kiela and Stephen Clark.\n2013.\nDetecting\ncompositionality of multi-word expressions using\nnearest neighbours in vector space models. In Proc.\nof EMNLP.\nJ. Richard Landis and Gary G. Koch. 1977. The mea-\nsurement of observer agreement for categorical data.\nBiometrics, 33(1):159\u2013174.\nAngeliki Lazaridou, Eva Maria Vecchi, and Marco\nBaroni.\n2013.\nFish transporters and miracle\nhomes: How compositional distributional semantics\ncan help NP parsing. In Proc. of EMNLP.\nDaniel D. Lee and H. Sebastian Seung. 1999. Learning\nthe parts of objects by non-negative matrix factoriza-\ntion. Nature, 401(6755):788\u2013791.\nHonglak Lee, Alexis Battle, Rajat Raina, and An-\ndrew Y. Ng.\n2006.\nEf\ufb01cient sparse coding algo-\nrithms. In NIPS.\nHonglak Lee, Rajat Raina, Alex Teichman, and An-\ndrew Y. Ng. 2009. Exponential family sparse cod-\ning with application to self-taught learning. In Proc.\nof IJCAI.\nBeth Levin. 1993. English Verb Classes and Alter-\nnations: A Preliminary Investigation. University of\nChicago Press.\nMichael Lewicki and Terrence Sejnowski.\n2000.\nLearning overcomplete representations.\nNeural\nComputation, 12(2):337\u2013365.\nMike Lewis and Mark Steedman.\n2013.\nCombined\ndistributional and logical semantics.\nTransactions\nof the ACL, 1:179\u2013192.\nMike Lewis and Mark Steedman. 2014. Combining\nformal and distributional models of temporal and in-\ntensional semantics. In Proc. of ACL.\nXin Li and Dan Roth. 2002. Learning question classi-\n\ufb01ers. In Proc. of COLING.\nWeixiang Liu, Nanning Zheng, and Xiaofeng Lu.\n2003. Non-negative matrix factorization for visual\ncoding. In Proc. of ICASSP.\nMitchell P. Marcus, Mary Ann Marcinkiewicz, and\nBeatrice Santorini. 1993. Building a large anno-\ntated corpus of english: The penn treebank. Compu-\ntational Linguistics, 19(2):313\u2013330.\nAndr\u00b4e F. T. Martins, Noah A. Smith, Pedro M. Q.\nAguiar, and M\u00b4ario A. T. Figueiredo. 2011. Struc-\ntured sparsity in structured prediction. In Proc. of\nEMNLP.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey\nDean. 2013. Ef\ufb01cient estimation of word represen-\ntations in vector space.\nGeorge A. Miller.\n1995.\nWordNet:\na lexical\ndatabase for English. Communications of the ACM,\n38(11):39\u201341.\nBrian Murphy, Partha Talukdar, and Tom Mitchell.\n2012. Learning effective and interpretable seman-\ntic models using non-negative sparse embedding. In\nProc. of COLING.\nJerome L. Myers and Arnold D. Well. 1995. Research\nDesign & Statistical Analysis. Routledge.\nBruno A. Olshausen and David J. Field. 1997. Sparse\ncoding with an overcomplete basis set: A strategy\nemployed by v1?\nVision Research, 37(23):3311 \u2013\n3325.\nDenis Paperno, Nghia The Pham, and Marco Baroni.\n2014. A practical and linguistically-motivated ap-\nproach to compositional distributional semantics. In\nProc. of ACL.\nMichael Paul and Mark Dredze. 2012. Factorial LDA:\nSparse multi-dimensional text models. In NIPS.\nJeffrey Pennington, Richard Socher, and Christo-\npher D. Manning. 2014. Glove: Global vectors for\nword representation. In Proc. of EMNLP.\nKarin Kipper Schuler.\n2005.\nVerbnet: A Broad-\ncoverage, Comprehensive Verb Lexicon. Ph.D. the-\nsis, University of Pennsylvania.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Y. Ng,\nand Christopher Potts. 2013. Recursive deep mod-\nels for semantic compositionality over a sentiment\ntreebank. In Proc. of EMNLP.\nKristina Toutanova and Mark Johnson.\n2007.\nA\nbayesian lda-based model for semi-supervised part-\nof-speech tagging. In NIPS.\nPeter D. Turney and Patrick Pantel. 2010. From fre-\nquency to meaning : Vector space models of seman-\ntics. JAIR, 37(1):141\u2013188.\nPeter D. Turney. 2001. Mining the web for synonyms:\nPMI-IR versus LSA on TOEFL. In Proc. of ECML.\nEva Maria Vecchi, Roberto Zamparelli, and Marco Ba-\nroni.\n2013.\nStudying the recursive behaviour of\nadjectival modi\ufb01cation with compositional distribu-\ntional semantics. In Proc. of EMNLP.\nLin Xiao. 2009. Dual averaging methods for regular-\nized stochastic learning and online optimization. In\nNIPS.\nChang Xu, Yalong Bai, Jiang Bian, Bin Gao, Gang\nWang, Xiaoguang Liu, and Tie-Yan Liu. 2014. Rc-\nnet: A general framework for incorporating knowl-\nedge into word representations. In Proc. of CIKM.\nDani Yogatama and Noah A Smith. 2014. Linguistic\nstructured sparsity in text categorization. In Proc. of\nACL.\nDani Yogatama, Manaal Faruqui, Chris Dyer, and\nNoah A. Smith. 2015. Learning word representa-\ntions with hierarchical sparse coding.\nIn Proc. of\nICML.\nMo Yu and Mark Dredze.\n2014.\nImproving lexical\nembeddings with semantic knowledge. In Proc. of\nACL.\nJun Zhu and Eric P Xing. 2012. Sparse topical coding.\narXiv:1202.3778.\n",
        "sentence": " (Faruqui et al., 2015). We use the toolkit provided by (Faruqui et al., 2015).",
        "context": "tection test.\nAcknowledgments\nWe thank Alona Fyshe for discussions on vec-\ntor interpretability and three anonymous review-\ners for their feedback.\nThis research was sup-\nported in part by the National Science Foundation\nthrough grant IIS-1251131 and the Defense Ad-\nvanced Research Projects Agency through grant\nFA87501420244. This work was supported in part\nby the U.S. Army Research Laboratory and the\nU.S. Army Research Of\ufb01ce under contract/grant\nnumber W911NF-10-1-0533.\nand Dredze, 2014; Xu et al., 2014; Faruqui et al.,\n2015), the method does not rely on any external\ninformation source. The transformation results in\nlonger, sparser vectors, sometimes called an \u201cover-\ncomplete\u201d representation (Olshausen and Field,"
    },
    {
        "title": "WordNet: An Electronic Lexical Database",
        "author": [
            "Christiane D. Fellbaum."
        ],
        "venue": "MIT Press.",
        "citeRegEx": "Fellbaum.,? 1998",
        "shortCiteRegEx": "Fellbaum.",
        "year": 1998,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Knowledge bases (KB), such as WordNet (Fellbaum, 1998), Freebase (Bollacker et al.",
        "context": null
    },
    {
        "title": "Composing Relationships with Translations",
        "author": [
            "Alberto Garc\u0131\u0301a-Dur\u00e1n",
            "Antoine Bordes",
            "Nicolas Usunier"
        ],
        "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
        "citeRegEx": "Garc\u0131\u0301a.Dur\u00e1n et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "Garc\u0131\u0301a.Dur\u00e1n et al\\.",
        "year": 2015,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " In parallel, recent work has proposed to model regularities beyond local facts by considering multirelation paths (Garc\u0131\u0301a-Dur\u00e1n et al., 2015; Lin et al., 2015a; Shen et al., 2016). RTransE (Garc\u0131\u0301a-Dur\u00e1n et al., 2015) Path 50 76.",
        "context": null
    },
    {
        "title": "Combining Two and Three-Way Embedding Models for Link Prediction in Knowledge Bases",
        "author": [
            "Alberto Garc\u0131\u0301a-Dur\u00e1n",
            "Antoine Bordes",
            "Nicolas Usunier",
            "Yves Grandvalet"
        ],
        "venue": "Journal of Artificial Intelligence Research",
        "citeRegEx": "Garc\u0131\u0301a.Dur\u00e1n et al\\.,? \\Q2016\\E",
        "shortCiteRegEx": "Garc\u0131\u0301a.Dur\u00e1n et al\\.",
        "year": 2016,
        "abstract": "This paper tackles the problem of endogenous link prediction for knowledge base completion. Knowledge bases can be represented as directed graphs whose nodes correspond to entities and edges to relationships. Previous attempts either consist of powerful systems with high capacity to model complex connectivity patterns, which unfortunately usually end up overfitting on rare relationships, or in approaches that trade capacity for simplicity in order to fairly model all relationships, frequent or not. In this paper, we propose Tatec, a happy medium obtained by complementing a high-capacity model with a simpler one, both pre-trained separately and then combined. We present several variants of this model with different kinds of regularization and combination strategies and show that this approach outperforms existing methods on different types of relationships by achieving state-of-the-art results on four benchmarks of the literature.",
        "full_text": "",
        "sentence": " 3 TATEC (Garc\u0131\u0301a-Dur\u00e1n et al., 2016) No 58 76.",
        "context": null
    },
    {
        "title": "Traversing Knowledge Graphs in Vector Space",
        "author": [
            "Kelvin Guu",
            "John Miller",
            "Percy Liang."
        ],
        "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. pages 318\u2013327.",
        "citeRegEx": "Guu et al\\.,? 2015",
        "shortCiteRegEx": "Guu et al\\.",
        "year": 2015,
        "abstract": "Path queries on a knowledge graph can be used to answer compositional\nquestions such as \"What languages are spoken by people living in Lisbon?\".\nHowever, knowledge graphs often have missing facts (edges) which disrupts path\nqueries. Recent models for knowledge base completion impute missing facts by\nembedding knowledge graphs in vector spaces. We show that these models can be\nrecursively applied to answer path queries, but that they suffer from cascading\nerrors. This motivates a new \"compositional\" training objective, which\ndramatically improves all models' ability to answer path queries, in some cases\nmore than doubling accuracy. On a standard knowledge base completion task, we\nalso demonstrate that compositional training acts as a novel form of structural\nregularization, reliably improving performance across all base models (reducing\nerrors by up to 43%) and achieving new state-of-the-art results.",
        "full_text": "Traversing Knowledge Graphs in Vector Space\nKelvin Guu\nStanford University\nkguu@stanford.edu\nJohn Miller\nStanford University\nmillerjp@stanford.edu\nPercy Liang\nStanford University\npliang@cs.stanford.edu\nAbstract\nPath queries on a knowledge graph can\nbe used to answer compositional ques-\ntions such as \u201cWhat languages are spoken\nby people living in Lisbon?\u201d.\nHowever,\nknowledge graphs often have missing facts\n(edges) which disrupts path queries. Re-\ncent models for knowledge base comple-\ntion impute missing facts by embedding\nknowledge graphs in vector spaces. We\nshow that these models can be recursively\napplied to answer path queries, but that\nthey suffer from cascading errors.\nThis\nmotivates a new \u201ccompositional\u201d training\nobjective, which dramatically improves all\nmodels\u2019 ability to answer path queries, in\nsome cases more than doubling accuracy.\nOn a standard knowledge base comple-\ntion task, we also demonstrate that com-\npositional training acts as a novel form of\nstructural regularization, reliably improv-\ning performance across all base models\n(reducing errors by up to 43%) and achiev-\ning new state-of-the-art results.\n1\nIntroduction\nBroad-coverage knowledge bases such as Free-\nbase (Bollacker et al., 2008) support a rich array\nof reasoning and question answering applications,\nbut they are known to suffer from incomplete cov-\nerage (Min et al., 2013). For example, as of May\n2015, Freebase has an entity Tad Lincoln (Abra-\nham Lincoln\u2019s son), but does not have his ethnic-\nity. An elegant solution to incompleteness is using\nvector space representations: Controlling the di-\nmensionality of the vector space forces generaliza-\ntion to new facts (Nickel et al., 2011; Nickel et al.,\n2012; Socher et al., 2013; Riedel et al., 2013; Nee-\nlakantan et al., 2015). In the example, we would\nhope to infer Tad\u2019s ethnicity from the ethnicity of\nhis parents.\nFigure 1: We propose performing path queries\nsuch as\ntad lincoln/parents/location (\u201cWhere\nare Tad Lincoln\u2019s parents located?\u201d) in a parallel\nlow-dimensional vector space. Here, entity sets\n(boxed) are represented as real vectors, and edge\ntraversal is driven by vector-to-vector transforma-\ntions (e.g., matrix multiplication).\nHowever, what is missing from these vector\nspace models is the original strength of knowledge\nbases: the ability to support compositional queries\n(Ullman, 1985).\nFor example, we might ask\nwhat the ethnicity of Abraham Lincoln\u2019s daugh-\nter would be. This can be formulated as a path\nquery on the knowledge graph, and we would like\na method that can answer this ef\ufb01ciently, while\ngeneralizing over missing facts and even missing\nor hypothetical entities (Abraham Lincoln did not\nin fact have a daughter).\nIn this paper, we present a scheme to answer\npath queries on knowledge bases by \u201ccomposi-\ntionalizing\u201d a broad class of vector space mod-\nels that have been used for knowledge base com-\npletion (see Figure 1). At a high level, we inter-\npret the base vector space model as implementing\na soft edge traversal operator. This operator can\nthen be recursively applied to predict paths. Our\ninterpretation suggests a new compositional train-\ning objective that encourages better modeling of\narXiv:1506.01094v2  [cs.CL]  19 Aug 2015\npaths. Our technique is applicable to a broad class\nof composable models that includes the bilinear\nmodel (Nickel et al., 2011) and TransE (Bordes et\nal., 2013).\nWe have two key empirical \ufb01ndings: First, we\nshow that compositional training enables us to\nanswer path queries up to at least length 5 by\nsubstantially reducing cascading errors present in\nthe base vector space model.\nSecond, we \ufb01nd\nthat somewhat surprisingly, compositional train-\ning also improves upon state-of-the-art perfor-\nmance for knowledge base completion, which is a\nspecial case of answering unit length path queries.\nTherefore, compositional training can also be seen\nas a new form of structural regularization for ex-\nisting models.\n2\nTask\nWe now give a formal de\ufb01nition of the task of an-\nswering path queries on a knowledge base. Let\nE be a set of entities and R be a set of binary\nrelations. A knowledge graph G is de\ufb01ned as a\nset of triples of the form (s, r, t) where s, t \u2208E\nand r \u2208R. An example of a triple in Freebase is\n(tad lincoln, parents, abraham lincoln).\nA path query q consists of an initial anchor en-\ntity, s, followed by a sequence of relations to be\ntraversed, p = (r1, . . . , rk). The answer or deno-\ntation of the query, JqK, is the set of all entities that\ncan be reached from s by traversing p. Formally,\nthis can be de\ufb01ned recursively:\nJsK def\n= {s},\n(1)\nJq/rK def\n= {t : \u2203s \u2208JqK, (s, r, t) \u2208G} .\n(2)\nFor example, tad lincoln/parents/location is a\nquery q that asks: \u201cWhere did Tad Lincoln\u2019s par-\nents live?\u201d.\nFor evaluation (see Section 5 for details), we de-\n\ufb01ne the set of candidate answers to a query C(q)\nas the set of all entities that \u201ctype match\u201d, namely\nthose that participate in the \ufb01nal relation of q at\nleast once; and let N(q) be the incorrect answers:\nC (s/r1/ \u00b7 \u00b7 \u00b7 /rk) def\n= {t | \u2203e, (e, rk, t) \u2208G} (3)\nN (q) def\n= C (q) \\JqK.\n(4)\nKnowledge base completion.\nKnowledge base\ncompletion (KBC) is the task of predicting\nwhether a given edge (s, r, t) belongs in the graph\nor not. This can be formulated as a path query\nq = s/r with candidate answer t.\n3\nCompositionalization\nIn this section, we show how to compositional-\nize existing KBC models to answer path queries.\nWe start with a motivating example in Section 3.1,\nthen present the general technique in Section 3.2.\nThis suggests a new compositional training objec-\ntive, described in Section 3.3. Finally, we illus-\ntrate the technique for several more models in Sec-\ntion 3.4, which we use in our experiments.\n3.1\nExample\nA common vector space model for knowledge\nbase completion is the bilinear model (Nickel et\nal., 2011). In this model, we learn a vector xe \u2208\nRd for each entity e \u2208E and a matrix Wr \u2208Rd\u00d7d\nfor each relation r \u2208R. Given a query s/r (ask-\ning for the set of entities connected to s via relation\nr), the bilinear model scores how likely t \u2208Js/rK\nholds using\nscore(s/r, t) = x\u22a4\ns Wrxt.\n(5)\nTo motivate our compositionalization tech-\nnique, take d = |E| and suppose Wr is the ad-\njacency matrix for relation r and entity vector xe\nis the indicator vector with a 1 in the entry corre-\nsponding to entity e. Then, to answer a path query\nq = s/r1/ . . . /rk, we would then compute\nscore(q, t) = x\u22a4\ns Wr1 . . . Wrkxt.\n(6)\nIt is easy to verify that the score counts the number\nof unique paths between s and t following rela-\ntions r1/ . . . /rk. Hence, any t with positive score\nis a correct answer (JqK = {t : score(q, t) > 0}).\nLet us interpret (6) recursively. The model be-\ngins with an entity vector xs, and sequentially\napplies traversal operators Tri(v) = v\u22a4Wri for\neach ri.\nEach traversal operation results in a\nnew \u201cset vector\u201d representing the entities reached\nat that point in traversal (corresponding to the\nnonzero entries of the set vector). Finally, it ap-\nplies the membership operator M(v, xt) = v\u22a4xt\nto check if t \u2208Js/r1/ . . . /rkK. Writing graph\ntraversal in this way immediately suggests a useful\ngeneralization: take d much smaller than |E| and\nlearn the parameters Wr and xe.\n3.2\nGeneral technique\nThe strategy used to extend the bilinear model of\n(5) to the compositional model in (6) can be ap-\nplied to any composable model: namely, one that\nhas a scoring function of the form:\nscore(s/r, t) = M(Tr(xs), xt)\n(7)\nfor some choice of membership operator M : Rd\u00d7\nRd \u2192R and traversal operator Tr : Rd \u2192Rd.\nWe can now de\ufb01ne the vector denotation of a\nquery JqKV analogous to the de\ufb01nition of JqK in\n(1) and (2):\nJsKV\ndef\n= xs,\n(8)\nJq/rKV\ndef\n= Tr (JqKV) .\n(9)\nThe score function for a compositionalized\nmodel is then\nscore(q, t) = M(JqKV, JtKV).\n(10)\nWe would like JqKV to approximately represent\nthe set JqK in the sense that for every e \u2208JqK,\nM (JqKV, JeKV) is larger than the values for e \u0338\u2208\nJqK. Of course it is not possible to represent all\nsets perfectly, but in the next section, we present a\ntraining objective that explicitly optimizes T and\nM to preserve path information.\n3.3\nCompositional training\nThe score function in (10) naturally suggests a new\ncompositional training objective. Let {(qi, ti)}N\ni=1\ndenote a set of path query training examples with\npath lengths ranging from 1 to L. We minimize\nthe following max-margin objective:\nJ(\u0398) =\nN\nX\ni=1\nX\nt\u2032\u2208N(qi)\n\u0002\n1 \u2212margin(qi, ti, t\u2032)\n\u0003\n+ ,\nmargin(q, t, t\u2032) = score(q, t) \u2212score(q, t\u2032),\nwhere the parameters are the membership opera-\ntor, the traversal operators, and the entity vectors:\n\u0398 = {M} \u222a{Tr : r \u2208R} \u222a\nn\nxe \u2208Rd : e \u2208E\no\n.\nThis objective encourages the construction of\n\u201cset vectors\u201d: because there are path queries of\ndifferent lengths and types, the model must learn\nto produce an accurate set vector JqKV after any\nsequence of traversals.\nAnother perspective is\nthat each traversal operator is trained such that\nits transformation preserves information in the\nset vector which might be needed in subsequent\ntraversal steps.\nIn contrast, previously proposed training objec-\ntives for knowledge base completion only train on\nqueries of path length 1. We will refer to this spe-\ncial case as single-edge training.\nIn Section 5, we show that compositional train-\ning leads to substantially better results for both\npath query answering and knowledge base com-\npletion. In Section 6, we provide insight into why.\n3.4\nOther composable models\nThere are many possible candidates for T and M.\nFor example, T could be one\u2019s favorite neural net-\nwork mapping from Rd to Rd. Here, we focus on\ntwo composable models that were both recently\nshown to achieve state-of-the-art performance on\nknowledge base completion.\nTransE.\nThe TransE model of Bordes et al.\n(2013) uses the scoring function\nscore(s/r, t) = \u2212\u2225xs + wr \u2212xt\u22252\n2.\n(11)\nwhere xs, wr and xt are all d-dimensional vectors.\nIn this case, the model can be expressed using\nmembership operator\nM(v, xt) = \u2212\u2225v \u2212xt\u22252\n2\n(12)\nand traversal operator Tr(xs)\n=\nxs + wr.\nHence, TransE can handle a path query q\n=\ns/r1/r2/ \u00b7 \u00b7 \u00b7 /rk using\nscore(q, t) = \u2212\u2225xs + wr1 + \u00b7 \u00b7 \u00b7 + wrk \u2212xt\u22252\n2.\nWe visualize the compositional TransE model in\nFigure 2.\nBilinear-Diag.\nThe\nBilinear-Diag\nmodel\nof\nYang et al. (2015) is a special case of the bilinear\nmodel with the relation matrices constrained to be\ndiagonal. Alternatively, the model can be viewed\nas a variant of TransE with multiplicative interac-\ntions between entity and relation vectors.\nNot all models can be compositionalized.\nIt\nis important to point out that some models are\nnot naturally composable\u2014for example, the latent\nfeature model of Riedel et al. (2013) and the neu-\nral tensor network of Socher et al. (2013). These\napproaches have scoring functions which combine\ns, r and t in a way that does not involve an inter-\nmediate vector representing s/r alone without t,\nso they do not decompose according to (7).\nWordNet\nFreebase\nRelations\n11\n13\nEntities\n38,696\n75,043\nBase\nTrain\n112,581\n316,232\nTest\n10,544\n23,733\nPaths\nTrain\n2,129,539\n6,266,058\nTest\n46,577\n109,557\nTable 1: WordNet and Freebase statistics for base\nand path query datasets.\n3.5\nImplementation\nWe use AdaGrad (Duchi et al., 2010) to optimize\nJ(\u0398), which is in general non-convex.\nInitial-\nization scale, mini-batch size and step size were\ncross-validated for all models. We initialize all\nparameters with i.i.d. Gaussians of variance 0.1 in\nevery entry, use a mini-batch size of 300 examples,\nand a step size in [0.001, 0.1] (chosen via cross-\nvalidation) for all of the models. For each exam-\nple q, we sample 10 negative entities t\u2032 \u2208N(q).\nDuring training, all of the entity vectors are con-\nstrained to lie on the unit ball, and we clipped the\ngradients to the median of the observed gradients\nif the update exceeded 3 times the median.\nWe \ufb01rst train on path queries of length 1 until\nconvergence and then train on all path queries until\nconvergence. This guarantees that the model mas-\nters basic edges before composing them to form\npaths. When training on path queries, we explic-\nitly parameterize inverse relations. For the bilinear\nmodel, we initialize Wr\u22121 with W \u22a4\nr . For TransE,\nwe initialize wr\u22121 with \u2212wr. For Bilinear-Diag,\nwe found initializing wr\u22121 with the exact inverse\n1/wr is numerically unstable, so we instead ran-\ndomly initialize wr\u22121 with i.i.d Gaussians of vari-\nance 0.1 in every entry. Additionally, for the bi-\nlinear model, we replaced the sum over N(qi) in\nthe objective with a max since it yielded slightly\nhigher accuracy. Our models are implemented us-\ning Theano (Bastien et al., 2012; Bergstra et al.,\n2010).\n4\nDatasets\nIn Section 4.1, we describe two standard knowl-\nedge base completion datasets. These consist of\nsingle-edge queries, so we call them base datasets.\nIn Section 4.2, we generate path query datasets\nfrom these base datasets.\n4.1\nBase datasets\nOur experiments are conducted using the sub-\nsets of WordNet and Freebase from Socher et al.\n(2013). The statistics of these datasets and their\nsplits are given in Table 1.\nThe WordNet and Freebase subsets exhibit sub-\nstantial differences that can in\ufb02uence model per-\nformance. The Freebase subset is almost bipartite\nwith most of the edges taking the form (s, r, t) for\nsome person s, relation r and property t. In Word-\nNet, both the source and target entities are arbi-\ntrary words.\nBoth the raw WordNet and Freebase contain\nmany relations that are almost perfectly correlated\nwith an inverse relation. For example, WordNet\ncontains both has part and part of, and Freebase\ncontains both parents and children. At test time,\na query on an edge (s, r, t) is easy to answer if the\ninverse triple (t, r\u22121, s) was observed in the train-\ning set. Following Socher et al. (2013), we ac-\ncount for this by excluding such \u201ctrivial\u201d queries\nfrom the test set.\n4.2\nPath query datasets\nGiven a base knowledge graph, we generate path\nqueries by performing random walks on the graph.\nIf we view compositional training as a form of reg-\nularization, this approach allows us to generate ex-\ntremely large amounts of auxiliary training data.\nThe procedure is given below.\nLet Gtrain be the training graph, which consists\nonly of the edges in the training set of the base\ndataset. We then repeatedly generate training ex-\namples with the following procedure:\n1. Uniformly sample a path length L\n\u2208\n{1, . . . , Lmax}, and uniformly sample a start-\ning entity s \u2208E.\n2. Perform a random walk beginning at entity s\nand continuing L steps.\n(a) At step i of the walk, choose a relation\nri uniformly from the set of relations in-\ncident on the current entity e.\n(b) Choose the next entity uniformly from\nthe set of entities reachable via ri.\n3. Output a query-answer pair, (q, t), where q =\ns/r1/ \u00b7 \u00b7 \u00b7 /rL and t is the \ufb01nal entity of the\nrandom walk.\nIn practice, we do not sample paths of length 1 and\ninstead directly add all of the edges from Gtrain to\nthe path query dataset.\nTo generate a path query test set, we repeat\nthe above procedure except using the graph Gfull,\nwhich is Gtrain plus all of the test edges from the\nbase dataset. Then we remove any queries from\nthe test set that also appeared in the training set.\nThe statistics for the path query datasets are pre-\nsented in Table 1.\n5\nMain results\nWe evaluate the models derived in Section 3 on\ntwo tasks: path query answering and knowledge\nbase completion. On both tasks, we show that the\ncompositional training strategy proposed in Sec-\ntion 3.3 leads to substantial performance gains\nover standard single-edge training. We also com-\npare directly against the KBC results of Socher et\nal. (2013), demonstrating that previously inferior\nmodels now match or outperform state-of-the-art\nmodels after compositional training.\nEvaluation metric.\nNumerous metrics have\nbeen used to evaluate knowledge base queries, in-\ncluding hits at 10 (percentage of correct answers\nranked in the top 10) and mean rank. We evaluate\non hits at 10, as well as a normalized version of\nmean rank, mean quantile, which better accounts\nfor the total number of candidates. For a query q,\nthe quantile of a correct answer t is the fraction of\nincorrect answers ranked after t:\n|{t\u2032 \u2208N (q) : score(q, t\u2032) < score(q, t)}|\n|N (q)|\n(13)\nThe quantile ranges from 0 to 1, with 1 being opti-\nmal. Mean quantile is then de\ufb01ned to be the aver-\nage quantile score over all examples in the dataset.\nTo illustrate why normalization is important, con-\nsider a set of queries on the relation gender. A\nmodel that predicts the incorrect gender on ev-\nery query would receive a mean rank of 2 (since\nthere are only 2 candidate answers), which is fairly\ngood in absolute terms, whereas the mean quantile\nwould be 0, rightfully penalizing the model.\nAs a \ufb01nal note, several of the queries in the\nFreebase path dataset are \u201ctype-match trivial\u201d in\nthe sense that all of the type matching candidates\nC(q) are correct answers to the query. In this case,\nmean quantile is unde\ufb01ned and we exclude such\nqueries from evaluation.\nOverview.\nThe upper half of Table 2 shows\nthat compositional training improves path query-\ning performance across all models and metrics on\nboth datasets, reducing error by up to 76.2%.\nThe lower half of Table 2 shows that surpris-\ningly, compositional training also improves per-\nformance on knowledge base completion across\nalmost all models, metrics and datasets. On Word-\nNet, TransE bene\ufb01ts the most, with a 43.3% re-\nduction in error. On Freebase, Bilinear bene\ufb01ts\nthe most, with a 38.8% reduction in error.\nIn terms of mean quantile, the best overall\nmodel is TransE (COMP). In terms of hits at 10, the\nbest model on WordNet is Bilinear (COMP), while\nthe best model on Freebase is TransE (COMP).\nDeduction and Induction.\nTable 3 takes a\ndeeper look at performance on path query answer-\ning. We divided path queries into two subsets: de-\nduction and induction. The deduction subset con-\nsists of queries q = s/p where the source and tar-\nget entities JqK are connected via relations p in the\ntraining graph Gtrain, but the speci\ufb01c query q was\nnever seen during training. Such queries can be\nanswered by performing explicit traversal on the\ntraining graph, so this subset tests a model\u2019s abil-\nity to approximate the underlying training graph\nand predict the existence of a path from a collec-\ntion of single edges. The induction subset consists\nof all other queries. This means that at least one\nedge was missing on all paths following p from\nsource to target in the training graph. Hence, this\nsubset tests a model\u2019s generalization ability and its\nrobustness to missing edges.\nPerformance on the deduction subset of the\ndataset is disappointingly low for models trained\nwith single-edge training: they struggle to answer\npath queries even when all edges in the path query\nhave been seen at training time. Compositional\ntraining dramatically reduces these errors, some-\ntimes doubling mean quantile. In Section 6, we\nanalyze how this might be possible. After com-\npositional training, performance on the harder in-\nduction subset is also much stronger. Even when\nedges are missing along a path, the models are able\nto infer them.\nInterpretable\nqueries.\nAlthough\nour\npath\ndatasets consists of random queries, both datasets\ncontain a large number of useful, interpretable\nqueries. Results on a few illustrative examples are\nshown in Table 4.\nBilinear\nBilinear-Diag\nTransE\nPath query task\nSINGLE\nCOMP\n(%red)\nSINGLE\nCOMP\n(%red)\nSINGLE\nCOMP\n(%red)\nWordNet\nMQ\n84.7\n89.4\n30.7\n59.7\n90.4\n76.2\n83.7\n93.3\n58.9\nH@10\n43.6\n54.3\n19.0\n7.9\n31.1\n25.4\n13.8\n43.5\n34.5\nFreebase\nMQ\n58.0\n83.5\n60.7\n57.9\n84.8\n63.9\n86.2\n88\n13.0\nH@10\n25.9\n42.1\n21.9\n23.1\n38.6\n20.2\n45.4\n50.5\n9.3\nKBC task\nSINGLE\nCOMP\n(%red)\nSINGLE\nCOMP\n(%red)\nSINGLE\nCOMP\n(%red)\nWordNet\nMQ\n76.1\n82.0\n24.7\n76.5\n84.3\n33.2\n75.5\n86.1\n43.3\nH@10\n19.2\n27.3\n10.0\n12.9\n14.4\n1.72\n4.6\n16.5\n12.5\nFreebase\nMQ\n85.3\n91.0\n38.8\n84.6\n89.1\n29.2\n92.7\n92.8\n1.37\nH@10\n70.2\n76.4\n20.8\n63.2\n67.0\n10.3\n78.8\n78.6\n-0.9\nTable 2: Path query answering and knowledge base completion. We compare the performance of\nsingle-edge training (SINGLE) vs compositional training (COMP). MQ: mean quantile, H@10: hits at 10,\n%red: percentage reduction in error.\nInterpretable Queries\nBilinear SINGLE\nBilinear COMP\nX/institution/institution\u22121/profession\n50.0\n93.6\nX/parents/religion\n81.9\n97.1\nX/nationality/nationality\u22121/ethnicity\u22121\n68.0\n87.0\nX/has part/has instance\u22121\n92.6\n95.1\nX/type of/type of/type of\n72.8\n79.4\nTable 4: Path query performance (mean quantile) on a selection of interpretable queries. We compare\nBilinear SINGLE and Bilinear COMP. Meanings of each query (descending): \u201cWhat professions are there\nat X\u2019s institution?\u201d; \u201cWhat is the religion of X\u2019s parents?\u201d; \u201cWhat are the ethnicities of people from the\nsame country as X?\u201d; \u201cWhat types of parts does X have?\u201d; and the transitive \u201cWhat is X a type of?\u201d.\n(Note that a relation r and its inverse r\u22121 do not necessarily cancel out if r is not a one-to-one mapping.\nFor example, X/institution/institution\u22121 denotes the set of all people who work at the institution X\nworks at, which is not just X.)\nPath query task\nWordNet\nFreebase\nDed.\nInd.\nDed.\nInd.\nBilinear\nSINGLE\n96.9\n66.0\n49.3\n49.4\nCOMP\n98.9\n75.6\n82.1\n70.6\nBi-Diag\nSINGLE\n56.3\n51.6\n49.3\n50.2\nCOMP\n98.5\n78.2\n84.5\n72.8\nTransE\nSINGLE\n92.6\n71.7\n85.3\n72.4\nCOMP\n99.0\n87.4\n87.5\n76.3\nTable 3: Deduction and induction. We compare\nmean quantile performance of single-edge training\n(SINGLE) vs compositional training (COMP). Length\n1 queries are excluded.\nComparison with Socher et al. (2013).\nHere,\nwe measure performance on the KBC task in terms\nof the accuracy metric of Socher et al. (2013).\nThis evaluation involves sampled negatives, and is\nhence noisier than mean quantile, but makes our\nresults directly comparable to Socher et al. (2013).\nOur results show that previously inferior models\nsuch as the bilinear model can outperform state-\nof-the-art models after compositional training.\nSocher et al. (2013) proposed parametrizing\neach entity vector as the average of vectors of\nwords in the entity (wtad lincoln =\n1\n2(wtad +\nwlincoln), and pretraining these word vectors us-\ning the method of Turian et al. (2010). Table 5\nreports results when using this approach in con-\njunction with compositional training. We initial-\nized all models with word vectors from Penning-\nton et al. (2014).\nWe found that composition-\nally trained models outperform the neural tensor\nnetwork (NTN) on WordNet, while being only\nslightly behind on Freebase. (We did not use word\nvectors in any of our other experiments.)\nWhen the strategy of averaging word vectors to\nform entity vectors is not applied, our composi-\ntional models are signi\ufb01cantly better on WordNet\nand slightly better on Freebase. It is worth noting\nthat in many domains, entity names are not lexi-\ncally meaningful, so word vector averaging is not\nAccuracy\nWordNet\nFreebase\nEV\nWV\nEV\nWV\nNTN\n70.6\n86.2\n87.2\n90.0\nBilinear COMP\n77.6\n87.6\n86.1\n89.4\nTransE COMP\n80.3\n84.9\n87.6\n89.6\nTable 5: Model performance in terms of accu-\nracy. EV: entity vectors are separate (initialized\nrandomly); WV: entity vectors are average of word\nvectors (initialized with pretrained word vectors).\nalways meaningful.\n6\nAnalysis\nIn this section, we try to understand why com-\npositional training is effective.\nFor concrete-\nness, everything is described in terms of the bi-\nlinear model. We will refer to the compositionally\ntrained model as COMP, and the model trained with\nsingle-edge training as SINGLE.\n6.1\nWhy does compositional training\nimprove path query answering?\nIt is tempting to think that if SINGLE has accurately\nmodeled individual edges in a graph, it should ac-\ncurately model the paths that result from those\nedges. This intuition turns out to be incorrect, as\nrevealed by SINGLE\u2019s relatively weak performance\non the path query dataset. We hypothesize that this\nis due to cascading errors along the path. For a\ngiven edge (s, r, t) on the path, single-edge train-\ning encourages xt to be closer to x\u22a4\ns Wr than any\nother incorrect xt\u2032. However, once this is achieved\nby a margin of 1, it does not push xt any closer to\nx\u22a4\ns Wr. The remaining discrepancy is noise which\ngets added at each step of path traversal. This is\nillustrated schematically in Figure 2.\nTo observe this phenomenon empirically, we\nexamine how well a model handles each interme-\ndiate step of a path query.\nWe can do this by\nmeasuring the reconstruction quality (RQ) of the\nset vector produced after each traversal operation.\nSince each intermediate stage is itself a valid path\nquery, we de\ufb01ne RQ to be the average quantile\nover all entities that belong in JqK:\nRQ (q) =\n1\n|JqK|\nX\nt\u2208JqK\nquantile (q, t)\n(14)\nWhen all entities in JqK are ranked above all in-\ncorrect entities, RQ is 1. In Figure 3, we illustrate\nhow RQ changes over the course of a query.\nFigure 2:\nCascading errors visualized for\nTransE. Each node represents the position of an\nentity in vector space.\nThe relation parent is\nideally a simple horizontal translation, but each\ntraversal introduces noise. The red circle is where\nwe expect Tad\u2019s parent to be. The red square is\nwhere we expect Tad\u2019s grandparent to be. Dotted\nred lines show that error grows larger as we tra-\nverse farther away from Tad. Compositional train-\ning pulls the entity vectors closer to the ideal ar-\nrangement.\nGiven the nature of cascading errors, it might\nseem reasonable to address the problem by adding\na term to our objective which explicitly encour-\nages x\u22a4\ns Wr to be as close as possible to xt. With\nthis motivation, we tried adding \u03bb\u2225x\u22a4\ns Wr \u2212xt\u22252\n2\nterm to the objective of the bilinear model and a\n\u03bb\u2225xs + wr \u2212xt\u22252\n2 term to the objective of TransE.\nWe experimented with different settings of \u03bb over\nthe range [0.001, 100]. In no case did this addi-\ntional \u21132 term improve SINGLE\u2019s performance on\nthe path query or single edge dataset. These re-\nsults suggest that compositional training is a more\neffective way to combat cascading errors.\n6.2\nWhy does compositional training\nimprove knowledge base completion?\nTable 2 reveals that COMP also performs better on\nthe single-edge task of knowledge base comple-\ntion. This is somewhat surprising, since SINGLE\nis trained on a training set which distributionally\nmatches the test set, whereas COMP is not. How-\never, COMP\u2019s better performance on path queries\nsuggests that there must be another factor at play.\nAt a high level, training on paths must be provid-\ning some form of structural regularization which\nreduces cascading errors.\nIndeed, paths in a\nknowledge graph have proven to be important fea-\ntures for predicting the existence of single edges\n(Lao et al., 2011; Neelakantan et al., 2015). For\nexample, consider the following Horn clause:\nparents (x, y) \u2227location (y, z) \u21d2place of birth (x, z) ,\nFigure 3: Reconstruction quality (RQ) at each step\nof the query tad lincoln/parents/place of birth/\nplace of birth\u22121/profession.\nCOMP experiences\nsigni\ufb01cantly less degradation in RQ as path length\nincreases. Correspondingly, the set of 5 highest\nscoring entities computed at each step using COMP\n(green) is signi\ufb01ciantly more accurate than the set\ngiven by SINGLE (blue). Correct entities are bolded.\nwhich states that if x has a parent with location\nz, then x has place of birth z. The body of the\nHorn clause expresses a path from x to z. If COMP\nmodels the path better, then it should be better able\nto use that knowledge to infer the head of the Horn\nclause.\nMore generally, consider Horn clauses of the\nform p \u21d2r, where p = r1/ . . . /rk is a path type\nand r is the relation being predicted. Let us focus\non Horn clauses with high precision as de\ufb01ned by:\nprec(p) = |JpK \u2229JrK|\n|JpK|\n,\n(15)\nwhere JpK is the set of entity pairs connected by p,\nand similarly for JrK.\nIntuitively, one way for the model to implicitly\nlearn and exploit such a Horn clause would be to\nsatisfy the following two criteria:\n1. The model should ensure a consistent spa-\ntial relationship between entity pairs that are\nrelated by the path type p; that is, keeping\nx\u22a4\ns Wr1 . . . Wrk close to xt for all valid (s, t)\npairs.\n2. The model\u2019s representation of the path type p\nand relation r should capture that spatial re-\nlationship; that is, x\u22a4\ns Wr1 . . . Wrk \u2248xt im-\nplies x\u22a4\ns Wr \u2248xt, or simply Wr1 . . . Wrk \u2248\nWr.\nWe have already seen empirically that SINGLE does\nnot meet criterion 1, because cascading errors\ncause it to put incorrect entity vectors xt\u2032 closer\nto x\u22a4\ns Wr1 . . . Wrk than the correct entity.\nCOMP\nmitigates these errors.\nTo empirically verify that COMP also does a bet-\nter job of meeting criterion 2, we perform the\nfollowing: for a path type p and relation r, de-\n\ufb01ne dist(p, r) to be the angle between their corre-\nsponding matrices (treated as vectors in Rd2). This\nis a natural measure because x\u22a4\ns Wrxt computes\nthe matrix inner product between Wr and xsx\u22a4\nt .\nHence, any matrix with small distance from Wr\nwill produce nearly the same scores as Wr for the\nsame entity pairs.\nIf COMP is better at capturing the correlation be-\ntween p and r, then we would expect that when\nprec(p) is high, compositional training should\nshrink dist(p, r) more. To con\ufb01rm this hypothe-\nsis, we enumerated over all 676 possible paths of\nlength 2 (including inverted relations), and exam-\nined the proportional reduction in dist(p, r) caused\nby compositional training,\n\u2206dist(p, r) = distCOMP(p, r) \u2212distSINGLE(p, r)\ndistSINGLE(p, r)\n.\n(16)\nFigure 4 shows that higher precision paths indeed\ncorrespond to larger reductions in dist(p, r).\n7\nRelated work\nKnowledge base completion with vector space\nmodels.\nMany models have been proposed for\nknowledge base completion, including those re-\nviewed in Section 3.4 (Nickel et al., 2011; Bor-\ndes et al., 2013; Yang et al., 2015; Socher et al.,\n2013). Dong et al. (2014) demonstrated that KBC\nmodels can improve the quality of relation extrac-\ntion by serving as graph-based priors. Riedel et\nal. (2013) showed that such models can be also be\ndirectly used for open-domain relation extraction.\nOur compositional training technique is an orthog-\nonal improvement that could help any composable\nmodel.\nDistributional compositional semantics.\nPre-\nvious works have explored compositional vector\nspace representations in the context of logic and\nsentence interpretation. In Socher et al. (2012), a\nmatrix is associated with each word of a sentence,\nand can be used to recursively modify the mean-\ning of nearby constituents. Grefenstette (2013) ex-\nFigure 4: We divide paths of length 2 into high\nprecision (> 0.3), low precision (\u22640.3), and not\nco-occuring with r. Here r = nationality. Each\nbox plot shows the min, max, and \ufb01rst and third\nquartiles of \u2206dist(p, r). As hypothesized, com-\npositional training results in large decreases in\ndist(p, r) for high precision paths p, modest de-\ncreases for low precision paths, and little to no de-\ncreases for irrelevant paths.\nplored the ability of tensors to simulate logical cal-\nculi. Bowman et al. (2014) showed that recursive\nneural networks can learn to distinguish impor-\ntant semantic relations. Socher et al. (2014) found\nthat compositional models were powerful enough\nto describe and retrieve images.\nWe demonstrate that compositional representa-\ntions are also useful in the context of knowledge\nbase querying and completion. In the aforemen-\ntioned work, compositional models produce vec-\ntors which represent truth values, sentiment or im-\nage features. In our approach, vectors represent\nsets of entities constituting the denotation of a\nknowledge base query.\nPath modeling.\nNumerous methods have been\nproposed to leverage path information for knowl-\nedge base completion and question answering.\nNickel et al. (2014) proposed combining low-rank\nmodels with sparse path features. Lao and Cohen\n(2010) used random walks as features and Gard-\nner et al. (2014) extended this approach by us-\ning vector space similarity to govern random walk\nprobabilities. Neelakantan et al. (2015) addressed\nthe problem of path sparsity by embedding paths\nusing a recurrent neural network. Perozzi et al.\n(2014) sampled random walks on social networks\nas training examples, with a different goal to clas-\nsify nodes in the network. Bordes et al. (2014) em-\nbed paths as a sum of relation vectors for question\nanswering. Our approach is unique in modeling\nthe denotation of each intermediate step of a path\nquery, and using this information to regularize the\nspatial arrangement of entity vectors.\n8\nDiscussion\nWe introduced the task of answering path queries\non an incomplete knowledge base, and presented a\ngeneral technique for compositionalizing a broad\nclass of vector space models.\nOur experiments\nshow that compositional training leads to state-of-\nthe-art performance on both path query answering\nand knowledge base completion.\nThere are several key ideas from this paper: reg-\nularization by augmenting the dataset with paths,\nrepresenting sets as low-dimensional vectors in\na context-sensitive way, and performing function\ncomposition using vectors. We believe these three\ncould all have greater applicability in the develop-\nment of vector space models for knowledge repre-\nsentation and inference.\nReproducibility\nOur code, data, and exper-\niments are available on the CodaLab platform\nat\nhttps://www.codalab.org/worksheets/\n0xfcace41fdeec45f3bc6ddf31107b829f.\nAcknowledgments\nWe would like to thank Ga-\nbor Angeli for fruitful discussions and the anony-\nmous reviewers for their valuable feedback. We\ngratefully acknowledge the support of the Google\nNatural Language Understanding Focused Pro-\ngram and the National Science Foundation Grad-\nuate Research Fellowship under Grant No. DGE-\n114747.\nReferences\nF. Bastien, P. Lamblin, R. Pascanu, J. Bergstra, I. J.\nGoodfellow, A. Bergeron, N. Bouchard, and Y. Ben-\ngio.\n2012.\nTheano: new features and speed im-\nprovements. Deep Learning and Unsupervised Fea-\nture Learning NIPS 2012 Workshop.\nJ. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pas-\ncanu, G. Desjardins, J. Turian, D. Warde-Farley, and\nY. Bengio. 2010. Theano: a CPU and GPU math\nexpression compiler. In Proceedings of the Python\nfor Scienti\ufb01c Computing Conference (SciPy).\nK. Bollacker, C. Evans, P. Paritosh, T. Sturge, and\nJ. Taylor. 2008. Freebase: a collaboratively created\ngraph database for structuring human knowledge. In\nInternational Conference on Management of Data\n(SIGMOD), pages 1247\u20131250.\nA. Bordes, N. Usunier, A. Garcia-Duran, J. Weston,\nand O. Yakhnenko. 2013. Translating embeddings\nfor modeling multi-relational data.\nIn Advances\nin Neural Information Processing Systems (NIPS),\npages 2787\u20132795.\nA. Bordes, S. Chopra, and J. Weston. 2014. Ques-\ntion answering with subgraph embeddings. In Em-\npirical Methods in Natural Language Processing\n(EMNLP).\nS. R. Bowman, C. Potts, and C. D. Manning. 2014.\nCan recursive neural tensor networks learn logical\nreasoning? In International Conference on Learn-\ning Representations (ICLR).\nX. Dong, E. Gabrilovich, G. Heitz, W. Horn, N. Lao,\nK. Murphy, T. Strohmann, S. Sun, and W. Zhang.\n2014.\nKnowledge vault: A web-scale approach\nto probabilistic knowledge fusion. In International\nConference on Knowledge Discovery and Data Min-\ning (KDD), pages 601\u2013610.\nJ. Duchi, E. Hazan, and Y. Singer. 2010. Adaptive sub-\ngradient methods for online learning and stochastic\noptimization.\nIn Conference on Learning Theory\n(COLT).\nM. Gardner,\nP. Talukdar,\nJ. Krishnamurthy,\nand\nT. Mitchell. 2014. Incorporating vector space sim-\nilarity in random walk inference over knowledge\nbases. In Empirical Methods in Natural Language\nProcessing (EMNLP).\nE. Grefenstette. 2013. Towards a formal distributional\nsemantics: Simulating logical calculi with tensors.\narXiv preprint arXiv:1304.5823.\nN. Lao and W. W. Cohen.\n2010.\nRelational re-\ntrieval using a combination of path-constrained ran-\ndom walks. Machine learning, 81(1):53\u201367.\nN. Lao, T. Mitchell, and W. W. Cohen. 2011. Random\nwalk inference and learning in a large scale knowl-\nedge base. In Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 529\u2013539.\nB. Min,\nR. Grishman,\nL. Wan,\nC. Wang,\nand\nD. Gondek. 2013. Distant supervision for relation\nextraction with an incomplete knowledge base. In\nNorth American Association for Computational Lin-\nguistics (NAACL), pages 777\u2013782.\nA. Neelakantan, B. Roth, and A. McCallum.\n2015.\nCompositional vector space models for knowledge\nbase completion. In Association for Computational\nLinguistics (ACL).\nM. Nickel, V. Tresp, and H. Kriegel.\n2011.\nA\nthree-way model for collective learning on multi-\nrelational data. In International Conference on Ma-\nchine Learning (ICML), pages 809\u2013816.\nM. Nickel, V. Tresp, and H. Kriegel. 2012. Factorizing\nYAGO. In World Wide Web (WWW).\nM. Nickel, X. Jiang, and V. Tresp. 2014. Reducing the\nrank in relational factorization models by including\nobservable patterns. In Advances in Neural Informa-\ntion Processing Systems (NIPS), pages 1179\u20131187.\nJ. Pennington, R. Socher, and C. D. Manning. 2014.\nGlove: Global vectors for word representation. In\nEmpirical Methods in Natural Language Processing\n(EMNLP).\nB. Perozzi, R. Al-Rfou, and S. Skiena. 2014. Deep-\nwalk: Online learning of social representations. In\nInternational Conference on Knowledge Discovery\nand Data Mining (KDD), pages 701\u2013710.\nS. Riedel, L. Yao, and A. McCallum.\n2013.\nRe-\nlation extraction with matrix factorization and uni-\nversal schemas. In North American Association for\nComputational Linguistics (NAACL).\nR. Socher, B. Huval, C. D. Manning, and A. Y. Ng.\n2012. Semantic compositionality through recursive\nmatrix-vector spaces. In Empirical Methods in Nat-\nural Language Processing and Computational Nat-\nural Language Learning (EMNLP/CoNLL), pages\n1201\u20131211.\nR. Socher, D. Chen, C. D. Manning, and A. Ng. 2013.\nReasoning with neural tensor networks for knowl-\nedge base completion. In Advances in Neural Infor-\nmation Processing Systems (NIPS), pages 926\u2013934.\nR. Socher, A. Karpathy, Q. V. Le, C. D. Manning, and\nA. Y. Ng. 2014. Grounded compositional semantics\nfor \ufb01nding and describing images with sentences.\nTransactions of the Association for Computational\nLinguistics (TACL), 2:207\u2013218.\nJ. Turian, L. Ratinov, and Y. Bengio. 2010. Word rep-\nresentations: a simple and general method for semi-\nsupervised learning. In Proceedings of the 48th an-\nnual meeting of the association for computational\nlinguistics, pages 384\u2013394.\nJ. D. Ullman. 1985. Implementation of logical query\nlanguages for databases.\nACM Transactions on\nDatabase Systems (TODS), 10(3):289\u2013321.\nB. Yang, W. Yih, X. He, J. Gao, and L. Deng.\n2015. Embedding entities and relations for learning\nand inference in knowledge bases. arXiv preprint\narXiv:1412.6575.\n",
        "sentence": " Most of the embedding models for knowledge base completion define an energy function fr(h, t) according to the fact\u2019s plausibility (Bordes et al., 2011, 2014, 2013; Socher et al., 2013; Wang et al., 2014; Yang et al., 2015; Guu et al., 2015; Nguyen et al., 2016b).",
        "context": "B. Yang, W. Yih, X. He, J. Gao, and L. Deng.\n2015. Embedding entities and relations for learning\nand inference in knowledge bases. arXiv preprint\narXiv:1412.6575.\nknowledge base completion, including those re-\nviewed in Section 3.4 (Nickel et al., 2011; Bor-\ndes et al., 2013; Yang et al., 2015; Socher et al.,\n2013). Dong et al. (2014) demonstrated that KBC\nmodels can improve the quality of relation extrac-\nFor example, T could be one\u2019s favorite neural net-\nwork mapping from Rd to Rd. Here, we focus on\ntwo composable models that were both recently\nshown to achieve state-of-the-art performance on\nknowledge base completion.\nTransE."
    },
    {
        "title": "Learning to Represent Knowledge Graphs with Gaussian Embedding",
        "author": [
            "Shizhu He",
            "Kang Liu",
            "Guoliang Ji",
            "Jun Zhao."
        ],
        "venue": "Proceedings of the 24th ACM International on Conference on Information and Knowledge Management. pages 623\u2013632.",
        "citeRegEx": "He et al\\.,? 2015",
        "shortCiteRegEx": "He et al\\.",
        "year": 2015,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " (2015b), He et al. (2015), Ji et al. (2015b), He et al. (2015), Ji et al. (2015) and Lin et al. (2015b), He et al. (2015), Ji et al. (2015) and Lin et al. (2015a). 2 KG2E (He et al., 2015) No 348 93.",
        "context": null
    },
    {
        "title": "Knowledge Graph Embedding via Dynamic Mapping Matrix",
        "author": [
            "Guoliang Ji",
            "Shizhu He",
            "Liheng Xu",
            "Kang Liu",
            "Jun Zhao."
        ],
        "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint",
        "citeRegEx": "Ji et al\\.,? 2015",
        "shortCiteRegEx": "Ji et al\\.",
        "year": 2015,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " dependent space (Bordes et al., 2014; Ji et al., 2015; Lin et al., 2015b; Nguyen et al., 2016b), and then model the translation property in the projected space. 0 TransD (Ji et al., 2015) No 212 92.",
        "context": null
    },
    {
        "title": "DBpedia - A Large-scale, Multilingual Knowledge Base",
        "author": [
            "Jens Lehmann",
            "Robert Isele",
            "Max Jakob",
            "Anja Jentzsch",
            "Dimitris Kontokostas",
            "Pablo N. Mendes",
            "Sebastian Hellmann",
            "Mohamed Morsey",
            "Patrick van Kleef",
            "S\u00f6ren Auer",
            "Christian Bizer"
        ],
        "venue": null,
        "citeRegEx": "Lehmann et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "Lehmann et al\\.",
        "year": 2015,
        "abstract": "",
        "full_text": "",
        "sentence": " , 2007) and DBpedia (Lehmann et al., 2015), are useful resources (2014), Lin et al. (2015b), He et al.",
        "context": null
    },
    {
        "title": "LightRNN: Memory and Computation-Efficient Recurrent Neural Networks",
        "author": [
            "Xiang Li",
            "Tao Qin",
            "Jian Yang",
            "Tieyan Liu."
        ],
        "venue": "Advances in Neural Information Processing Systems 29.",
        "citeRegEx": "Li et al\\.,? 2016",
        "shortCiteRegEx": "Li et al\\.",
        "year": 2016,
        "abstract": "",
        "full_text": "",
        "sentence": " The block iterative optimization algorithm in our work is inspired by LightRNN (Li et al., 2016).",
        "context": null
    },
    {
        "title": "Modeling Relation Paths for Representation Learning of Knowledge Bases",
        "author": [
            "Yankai Lin",
            "Zhiyuan Liu",
            "Huanbo Luan",
            "Maosong Sun",
            "Siwei Rao",
            "Song Liu."
        ],
        "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language",
        "citeRegEx": "Lin et al\\.,? 2015a",
        "shortCiteRegEx": "Lin et al\\.",
        "year": 2015,
        "abstract": "Representation learning of knowledge bases (KBs) aims to embed both entities\nand relations into a low-dimensional space. Most existing methods only consider\ndirect relations in representation learning. We argue that multiple-step\nrelation paths also contain rich inference patterns between entities, and\npropose a path-based representation learning model. This model considers\nrelation paths as translations between entities for representation learning,\nand addresses two key challenges: (1) Since not all relation paths are\nreliable, we design a path-constraint resource allocation algorithm to measure\nthe reliability of relation paths. (2) We represent relation paths via semantic\ncomposition of relation embeddings. Experimental results on real-world datasets\nshow that, as compared with baselines, our model achieves significant and\nconsistent improvements on knowledge base completion and relation extraction\nfrom text.",
        "full_text": "Modeling Relation Paths for Representation Learning of Knowledge Bases\nYankai Lin1, Zhiyuan Liu1 \u2217, Huanbo Luan1, Maosong Sun1, Siwei Rao2, Song Liu2\n1 Department of Computer Science and Technology, State Key Lab on Intelligent Technology and Systems,\nNational Lab for Information Science and Technology, Tsinghua University, Beijing, China\n2 Samsung R&D Institute of China, Beijing, China\nAbstract\nRepresentation\nlearning\nof\nknowledge\nbases aims to embed both entities and\nrelations into a low-dimensional space.\nMost existing methods only consider\ndirect relations in representation learning.\nWe argue that multiple-step relation paths\nalso contain rich inference patterns be-\ntween entities, and propose a path-based\nrepresentation learning model. This model\nconsiders relation paths as translations\nbetween entities for representation learn-\ning, and addresses two key challenges: (1)\nSince not all relation paths are reliable,\nwe design a path-constraint resource allo-\ncation algorithm to measure the reliability\nof relation paths. (2) We represent relation\npaths via semantic composition of relation\nembeddings.\nExperimental results on\nreal-world datasets show that, as com-\npared with baselines, our model achieves\nsigni\ufb01cant and consistent improvements\non knowledge base completion and re-\nlation extraction from text.\nThe source\ncode of this paper can be obtained from\nhttps://github.com/mrlyk423/\nrelation_extraction.\n1\nIntroduction\nPeople have recently built many large-scale\nknowledge bases (KBs) such as Freebase, DBpe-\ndia and YAGO. These KBs consist of facts about\nthe real world, mostly in the form of triples, e.g.,\n(Steve Jobs, FounderOf, Apple Inc.). KBs are\nimportant resources for many applications such as\nquestion answering and Web search.\nAlthough\ntypical KBs are large in size, usually containing\nthousands of relation types, millions of entities\nand billions of facts (triples), they are far from\n\u2217Corresponding author: Z. Liu (liuzy@tsinghua.edu.cn)\ncomplete. Hence, many efforts have been invested\nin relation extraction to enrich KBs.\nRecent studies reveal that, neural-based repre-\nsentation learning methods are scalable and ef-\nfective to encode relational knowledge with low-\ndimensional representations of both entities and\nrelations, which can be further used to extract\nunknown relational facts. TransE (Bordes et al.,\n2013) is a typical method in the neural-based ap-\nproach, which learns vectors (i.e., embeddings) for\nboth entities and relations. The basic idea behind\nTransE is that, the relationship between two enti-\nties corresponds to a translation between the em-\nbeddings of the entities, that is, h + r \u2248t when\nthe triple (h, r, t) holds. Since TransE has issues\nwhen modeling 1-to-N, N-to-1 and N-to-N rela-\ntions, various methods such as TransH (Wang et\nal., 2014) and TransR (Lin et al., 2015) are pro-\nposed to assign an entity with different represen-\ntations when involved in various relations.\nDespite their success in modeling relational\nfacts, TransE and its extensions only take di-\nrect relations between entities into considera-\ntion.\nIt is known that there are also substan-\ntial multiple-step relation paths between entities\nindicating their semantic relationships.\nThe re-\nlation paths re\ufb02ect complicated inference pat-\nterns among relations in KBs. For example, the\nrelation path h\nBornInCity\n\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192e1\nCityInState\n\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192\ne2\nStateInCountry\n\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192\nt indicates the relation\nNationality between h and t,\ni.e.,\n(h,\nNationality, t).\nIn this paper, we aim at extending TransE to\nmodel relation paths for representation learning of\nKBs, and propose path-based TransE (PTransE).\nIn PTransE, in addition to direct connected rela-\ntional facts, we also build triples from KBs us-\ning entity pairs connected with relation paths.\nAs shown in Figure 1, TransE only considers\ndirect relations between entities, e.g., h\nr\u2212\u2192t,\nbuilds a triple (h, r, t), and optimizes the objec-\narXiv:1506.00379v2  [cs.CL]  15 Aug 2015\ntive h + r = t.\nPTransE generalizes TransE\nby regarding multiple-step relation paths as con-\nnections between entities.\nTake the 2-step path\nh\nr1\n\u2212\u2192e1\nr2\n\u2212\u2192t for example as shown in Figure 1.\nBesides building triples (h, r1, e1) and (e1, r2, t)\nfor learning as in TransE, PTransE also builds a\ntriple (h, r1 \u25e6r2, t), and optimizes the objective\nh + (r1 \u25e6r2) = t, where \u25e6is an operation to join\nthe relations r1 and r2 together into a uni\ufb01ed rela-\ntion path representation.\nTransE\nPTransE\nKB\nTriples\nObjectives\nFigure 1: TransE and PTransE.\nAs compared with TransE, PTransE takes rich\nrelation paths in KBs for learning. There are two\ncritical challenges that make PTransE nontrivial to\nlearn from relation paths:\nRelation Path Reliability.\nNot all relation\npaths are meaningful and reliable for learning. For\nexample, there is often a relation path h\nFriend\n\u2212\u2212\u2212\u2212\u2212\u2192\ne1\nProfession\n\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192t, but actually it does not indicate\nany semantic relationship between h and t. Hence,\nit is inappropriate to consider all relation paths in\nour model. In experiments, we \ufb01nd that those re-\nlation paths that lead to lots of possible tail enti-\nties are mostly unreliable for the entity pair. In\nthis paper, we propose a path-constraint resource\nallocation algorithm to measure the reliability of\nrelation paths. Afterwards, we select the reliable\nrelation paths for representation learning.\nRelation Path Representation. In order to take\nrelation paths into consideration, relation paths\nshould also be represented in a low-dimensional\nspace.\nIt is straightforward that the semantic\nmeaning of a relation path depends on all relations\nin this path. Given a relation path p = (r1, . . . , rl)\n, we will de\ufb01ne and learn a binary operation func-\ntion (\u25e6) to obtain the path embedding p by re-\ncursively composing multiple relations, i.e., p =\nr1 \u25e6. . . \u25e6rl.\nWith relation path selection and representation,\nPTransE learns entity and relation embeddings by\nregarding relation paths as translations between\nthe corresponding entities.\nIn experiments, we\nselect a typical KB, Freebase, to build datasets\nand carry out evaluation on three tasks, including\nentity prediction, relation prediction and relation\nextraction from text. Experimental results show\nthat, PTransE signi\ufb01cantly outperforms TransE\nand other baseline methods on all three tasks.\n2\nOur Model\nIn this section, we introduce path-based TransE\n(PTransE) that learns representations of entities\nand relations considering relation paths. In TransE\nand PTransE, we have entity set E and relation\nset R, and learn to encode both entities and re-\nlations in Rk. Given a KB represented by a set of\ntriples S = {(h, r, t)} with each triple composed\nof two entities h, t \u2208E and their relation r \u2208R.\nOur model is expected to return a low energy score\nwhen the relation holds, and a high one otherwise.\n2.1\nTransE and PTransE\nFor each triple (h, r, t), TransE regards the relation\nas a translation vector r between two entity vectors\nh and t. The energy function is de\ufb01ned as\nE(h, r, t) = ||h + r \u2212t||,\n(1)\nwhich is expected to get a low score when (h, r, t)\nholds, and high otherwise.\nTransE only learns from direct relations be-\ntween entities but ignores multiple-step relation\npaths, which also contain rich inference patterns\nbetween entities. PTransE take relation paths into\nconsideration for representation learning.\nSuppose there are multiple relation paths\nP(h, t) = {p1, . . . , pN} connecting two entities\nh and t, where relation path p = (r1, . . . , rl) indi-\ncates h\nr1\n\u2212\u2192. . .\nrl\n\u2212\u2192t. For each triple (h, r, t), the\nenergy function is de\ufb01ned as\nG(h, r, t) = E(h, r, t) + E(h, P, t),\n(2)\nwhere E(h, r, t) models correlations between rela-\ntions and entities with direct relation triples, as de-\n\ufb01ned in Equation (1). E(h, P, t) models the infer-\nence correlations between relations with multiple-\nstep relation path triples, which is de\ufb01ned as\nE(h, P, t) = 1\nZ\nX\np\u2208P(h,t)\nR(p|h, t)E(h, p, t), (3)\nwhere R(p|h, t) indicates the reliability of the re-\nlation path p given the entity pair (h, t), Z =\nP\np\u2208P(h,t) R(p|h, t) is a normalization factor, and\nE(h, p, t) is the energy function of the triple\n(h, p, t).\nFor the energy of each triple (h, p, t), the com-\nponent R(p|h, t) concerns about relation path reli-\nability, and E(h, p, t) concerns about relation path\nrepresentation. We introduce the two components\nin detail as follows.\n2.2\nRelation Path Reliability\nWe propose a path-constraint resource allocation\n(PCRA) algorithm to measure the reliability of a\nrelation path. Resource allocation over networks\nwas originally proposed for personalized recom-\nmendation (Zhou et al., 2007), and has been suc-\ncessfully used in information retrieval for measur-\ning relatedness between two objects (L\u00a8u and Zhou,\n2011). Here we extend it to PCRA to measure the\nreliability of relation paths. The basic idea is, we\nassume that a certain amount of resource is associ-\nated with the head entity h, and will \ufb02ow following\nthe given path p. We use the resource amount that\neventually \ufb02ows to the tail entity t to measure the\nreliability of the path p as a meaningful connection\nbetween h and t.\nFormally, for a path triple (h, p, t), we compute\nthe resource amount \ufb02owing from h to t given the\npath p = (r1, . . . , rl) as follows. Starting from h\nand following the relation path p, we can write the\n\ufb02owing path as S0\nr1\n\u2212\u2192S1\nr2\n\u2212\u2192. . .\nrl\n\u2212\u2192Sl, where\nS0 = h and t \u2208Sl.\nFor any entity m \u2208Si, we denote its direct pre-\ndecessors along relation ri in Si\u22121 as Si\u22121(\u00b7, m).\nThe resource \ufb02owing to m is de\ufb01ned as\nRp(m) =\nX\nn\u2208Si\u22121(\u00b7,m)\n1\n|Si(n, \u00b7)|Rp(n),\n(4)\nwhere Si(n, \u00b7) is the direct successors of n \u2208Si\u22121\nfollowing the relation ri, and Rp(n) is the resource\nobtained from the entity n.\nFor each relation path p, we set the initial re-\nsource in h as Rp(h) = 1. By performing re-\nsource allocation recursively from h through the\npath p, the tail entity t eventually obtains the re-\nsource Rp(t) which indicates how much informa-\ntion of the head entity h can be well translated. We\nuse Rp(t) to measure the reliability of the path p\ngiven (h, t), i.e., R(p|h, t) = Rp(t).\n2.3\nRelation Path Representation\nBesides relation path reliability, we also need to\nde\ufb01ne energy function E(h, p, t) for the path triple\n(h, p, t) in Equation (2).\nSimilar with the en-\nergy function of TransE in Equation (1), we will\nalso represent the relation path p in the embedding\nspace.\nUnited \nState\nCaliforni\na\nSan \nFrancisco\nSteve \nJobs\nComposition\nBornInCity\nCityInState\nStateInCountry\nFigure 2: Path representations are computed by se-\nmantic composition of relation embeddings.\nThe semantic meaning of a relation path con-\nsiderably relies on its involved relations. It is thus\nreasonable for us to build path embeddings via se-\nmantic composition of relation embeddings. As\nillustrated in Figure 2, the path embedding p is\ncomposed by the embeddings of BorninCity,\nCityInState and StateInCountry.\nFormally, for a path p = (r1, . . . , rl), we de\ufb01ne\na composition operation \u25e6and obtain path embed-\nding as p = r1 \u25e6. . .\u25e6rl. In this paper, we consider\nthree types of composition operation:\nAddition (ADD). The addition operation ob-\ntains the vector of a path by summing up the vec-\ntors of all relations, which is formalized as\np = r1 + . . . + rl.\n(5)\nMultiplication (MUL). The multiplication op-\neration obtains the vector of a path as the cumula-\ntive product of the vectors of all relations, which\nis formalized as\np = r1 \u00b7 . . . \u00b7 rl.\n(6)\nBoth addition and multiplication operations are\nsimple and have been extensively investigated in\nsemantic composition of phrases and sentences\n(Mitchell and Lapata, 2008).\nRecurrent Neural Network (RNN). RNN is a\nrecent neural-based model for semantic composi-\ntion (Mikolov et al., 2010). The composition op-\neration is realized using a matrix W:\nci = f(W[ci\u22121; ri]),\n(7)\nwhere f is a non-linearity or identical function,\nand [a; b] represents the concatenation of two vec-\ntors. By setting c1 = r1 and recursively perform-\ning RNN following the relation path, we will \ufb01-\nnally obtain p = cn. RNN has also been used\nfor representation learning of relation paths in KBs\n(Neelakantan et al., 2015).\nFor a multiple-step relation path triple (h, p, t),\nwe could have followed TransE and de\ufb01ne the\nenergy function as E(h, p, t) = ||h + p \u2212t||.\nHowever, since we have minimized ||h + r \u2212t||\nwith the direct relation triple (h, r, t) to make sure\nr \u2248t\u2212h, we may directly de\ufb01ne the energy func-\ntion of (h, p, t) as\nE(h, p, t) = ||p\u2212(t\u2212h)|| = ||p\u2212r|| = E(p, r),\n(8)\nwhich is expected to be a low score when the\nmultiple-relation path p is consistent with the di-\nrect relation r, and high otherwise, without using\nentity embeddings.\n2.4\nObjective Formalization\nWe\nformalize\nthe\noptimization\nobjective\nof\nPTransE as\nL(S) =\nX\n(h,r,t)\u2208S\n\u0002\nL(h, r, t)+ 1\nZ\nX\np\u2208P (h,t)\nR(p|h, t)L(p, r)\n\u0003\n.\n(9)\nFollowing TransE, L(h, r, t) and L(p, r) are\nmargin-based loss functions with respect to the\ntriple (h, r, t) and the pair (p, r):\nL(h, r, t) =\nX\n(h\u2032,r\u2032,t\u2032)\u2208S\u2212\n[\u03b3 + E(h, r, t) \u2212E(h\u2032, r\u2032, t\u2032)]+,\n(10)\nand\nL(p, r) =\nX\n(h,r\u2032,t)\u2208S\u2212\n[\u03b3 + E(p, r) \u2212E(p, r\u2032)]+,\n(11)\nwhere [x]+ = max(0, x) returns the maximum be-\ntween 0 and x, \u03b3 is the margin, S is the set of valid\ntriples existing in a KB and S\u2212is the set of invalid\ntriples. The objective will favor lower scores for\nvalid triples as compared with invalid triples.\nThe invalid triple set with respect to (h, r, t) is\nde\ufb01ned as\nS\u2212= {(h\u2032, r, t)}\u222a{(h, r\u2032, t)}\u222a{(h, r, t\u2032)}. (12)\nThat is, the set of invalid triples is composed of\nthe original valid triple (h, r, t) with one of three\ncomponents replaced.\n2.5\nOptimization and Implementation Details\nFor optimization, we employ stochastic gradient\ndescent (SGD) to minimize the loss function. We\nrandomly select a valid triple from the training set\niteratively for learning. In the implementation, we\nalso enforce constraints on the norms of the em-\nbeddings h, r, t. That is, we set\n\u2225h\u22252 \u22641,\n\u2225r\u22252 \u22641,\n\u2225t\u22252 \u22641.\n\u2200h, r, t.\n(13)\nThere are also some implementation details that\nwill signi\ufb01cantly in\ufb02uence the performance of\nrepresentation learning, which are introduced as\nfollows.\nReverse Relation Addition. In some cases, we\nare interested in the reverse version of a relation,\nwhich may not be presented in KBs. For exam-\nple, according to the relation path e1\nBornInCity\n\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192\ne2\nCityOfCountry\n\u2190\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212e3 we expect to infer the fact\nthat (e1, Nationality, e3). In this paper, how-\never, we only consider the relation paths follow-\ning one direction. Hence, we add reverse relations\nfor each relation in KBs. That is, for each triple\n(h, r, t) we build another (t, r\u22121, h). In this way,\nour method can consider the above-mentioned\npath as e1\nBornInCity\n\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192e2\nCityOfCountry\u22121\n\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192e3\nfor learning.\nPath Selection Limitation. There are usually\nlarge amount of relations and facts about each en-\ntity pair. It will be impractical to enumerate all\npossible relation paths between head and tail en-\ntities. For example, if each entity refers to more\nthan 100 relations on average, which is common\nin Freebase, there will be billions of 4-step paths.\nEven for 2-step or 3-step paths, it will be time-\nconsuming to consider all of them without limita-\ntion. For computational ef\ufb01ciency, in this paper\nwe restrict the length of paths to at most 3-steps\nand consider those relation paths with the reliabil-\nity score larger than 0.01.\n2.6\nComplexity Analysis\nWe denote Ne as the number of entities, Nr as\nthe number of relations and K as the vector di-\nmension. The model parameter size of PTransE\nis (NeK + NrK), which is the same as TransE.\nPTransE follows the optimization procedure in-\ntroduced by (Bordes et al., 2013) to solve Equa-\ntion (9). We denote S as the number of triples\nfor learning, P as the expected number of relation\npaths between two entities, and L as the expected\nlength of relation paths. For each iteration in opti-\nmization, the complexity of TransE is O(SK) and\nthe complexity of PTransE is O(SKPL) for ADD\nand MUL, and O(SK2PL) for RNN.\n3\nExperiments and Analysis\n3.1\nData Sets and Experimental Setting\nWe evaluate our method on a typical large-scale\nKB Freebase (Bollacker et al., 2008). In this pa-\nper, we adopt two datasets extracted from Free-\nbase, i.e., FB15K and FB40K. The statistics of the\ndatasets are listed in Table 1.\nTable 1: Statistics of data sets.\nDataset\n#Rel\n#Ent\n#Train\n#Valid\n# Test\nFB15K\n1,345\n14,951\n483,142\n50,000\n59,071\nFB40K\n1,336\n39,528\n370,648\n67,946\n96,678\nWe evaluate the performance of PTransE and\nother baselines by predicting whether testing\ntriples hold.\nWe consider two scenarios:\n(1)\nKnowledge base completion, aiming to predict the\nmissing entities or relations in given triples only\nbased on existing KBs.\n(2) Relation extraction\nfrom texts, aiming to extract relations between en-\ntities based on information from both plain texts\nand KBs.\n3.2\nKnowledge Base Completion\nThe task of knowledge base completion is to com-\nplete the triple (h, r, t) when one of h, t, r is miss-\ning. The task has been used for evaluation in (Bor-\ndes et al., 2011; Bordes et al., 2012; Bordes et\nal., 2013). We conduct the evaluation on FB15K,\nwhich has 483, 142 relational triples and 1, 345 re-\nlation types, among which there are rich inference\nand reasoning patterns.\nIn the testing phase, for each testing triple\n(h, r, t), we de\ufb01ne the following score function for\nprediction\nS(h, r, t) = G(h, r, t) + G(t, r\u22121, h),\n(14)\nand the score function G(h, r, t) is further de\ufb01ned\nas\nG(h, r, t) =||h + r \u2212t||+\n1\nZ\nX\np\u2208P(h,t)\nPr(r|p)R(p|h, t)||p \u2212r||.\n(15)\nThe score function is similar to the energy func-\ntion de\ufb01ned in Section 2.1. The difference is that,\nhere we consider the reliability of a path p is also\nrelated to the inference strength given r, which is\nquanti\ufb01ed as Pr(r|p) = Pr(r, p)/ Pr(p) obtained\nfrom the training data.\nWe divide the stage into two sub-tasks, i.e., en-\ntity prediction and relation prediction.\n3.2.1\nEntity Prediction\nIn the sub-task of entity prediction, we follow the\nsetting in (Bordes et al., 2013).\nFor each test-\ning triple with missing head or tail entity, vari-\nous methods are asked to compute the scores of\nG(h, r, t) for all candidate entities and rank them\nin descending order.\nWe use two measures as our evaluation metrics:\nthe mean of correct entity ranks and the proportion\nof valid entities ranked in top-10 (Hits@10). As\nmentioned in (Bordes et al., 2013), the measures\nare desirable but \ufb02awed when an invalid triple\nends up being valid in KBs. For example, when\nthe testing triple is (Obama, PresidentOf,\nUSA) with the head entity Obama is missing, the\nhead entity Lincoln may be regarded invalid for\nprediction, but in fact it is valid in KBs. The eval-\nuation metrics will under-estimate those methods\nthat rank these triples high. Hence, we can \ufb01lter\nout all these valid triples in KBs before ranking.\nThe \ufb01rst evaluation setting was named as \u201cRaw\u201d\nand the second one as \u201cFilter\u201d.\nFor comparison, we select all methods in (Bor-\ndes et al., 2013; Wang et al., 2014) as our base-\nlines and use their reported results directly since\nthe evaluation dataset is identical.\nIdeally, PTransE has to \ufb01nd all possible relation\npaths between the given entity and each candidate\nentity. However, it is time consuming and imprac-\ntical, because we need to iterate all candidate en-\ntities in |E| for each testing triple. Here we adopt\na re-ranking method: we \ufb01rst rank all candidate\nentities according to the scores from TransE, and\nthen re-rank top-500 candidates according to the\nscores from PTransE.\nFor PTransE, we \ufb01nd the best hyperparameters\naccording to the mean rank in validation set. The\noptimal con\ufb01gurations of PTransE we used are\n\u03bb = 0.001, \u03b3 = 1, k = 100 and taking L1 as\ndissimilarity. For training, we limit the number of\nepochs over all the training triples to 500.\nEvaluation results of entity prediction are\nshown in Table 2. The baselines include RESCAL\n(Nickel et al., 2011), SE (Bordes et al., 2011),\nSME (linear) (Bordes et al., 2012), SME (bilinear)\nTable 2: Evaluation results on entity prediction.\nMetric\nMean Rank\nHits@10 (%)\nRaw\nFilter\nRaw\nFilter\nRESCAL\n828\n683\n28.4\n44.1\nSE\n273\n162\n28.8\n39.8\nSME (linear)\n274\n154\n30.7\n40.8\nSME (bilinear)\n284\n158\n31.3\n41.3\nLFM\n283\n164\n26.0\n33.1\nTransE\n243\n125\n34.9\n47.1\nTransH\n212\n87\n45.7\n64.4\nTransR\n198\n77\n48.2\n68.7\nTransE (Our)\n205\n63\n47.9\n70.2\nPTransE (ADD, 2-step)\n200\n54\n51.8\n83.4\nPTransE (MUL, 2-step)\n216\n67\n47.4\n77.7\nPTransE (RNN, 2-step)\n242\n92\n50.6\n82.2\nPTransE (ADD, 3-step)\n207\n58\n51.4\n84.6\n(Bordes et al., 2012), LFM (Jenatton et al., 2012),\nTransE (Bordes et al., 2013) (original version and\nour implementation considering reverse relations),\nTransH (Wang et al., 2014), and TransR (Lin et al.,\n2015).\nFor PTransE, we consider three composition op-\nerations for relation path representation: addition\n(ADD), multiplication (MUL) and recurrent neu-\nral networks (RNN). We also consider relation\npaths with at most 2-steps and 3-steps. With the\nsame con\ufb01gurations of PTransE, our TransE im-\nplementation achieves much better performance\nthan that reported in (Bordes et al., 2013).\nFrom Table 2 we observe that: (1) PTransE\nsigni\ufb01cantly and consistently outperforms other\nbaselines including TransE. It indicates that rela-\ntion paths provide a good supplement for repre-\nsentation learning of KBs, which have been suc-\ncessfully encoded by PTransE. For example, since\nboth George W. Bush and Abraham Lincoln were\npresidents of the United States, they exhibit simi-\nlar embeddings in TransE. This may lead to con-\nfusion for TransE to predict the spouse of Laura\nBush.\nIn contrast, since PTransE models rela-\ntion paths, it can take advantage of the relation\npaths between George W. Bush and Laura Bush,\nand leads to more accurate prediction.\n(2) For\nPTransE, the addition operation outperforms other\ncomposition operations in both Mean Rank and\nHits@10. The reason is that, the addition opera-\ntion is compatible with the learning objectives of\nboth TransE and PTransE. Take h r1\n\u2212\u2192e1\nr2\n\u2212\u2192t for\nexample. The optimization objectives of two di-\nrect relations h + r1 = e1 and e1 + r2 = t can\neasily derive the path objective h + r1 + r2 = t.\n(3) PTransE of considering relation paths with at\nmost 2-step and 3-step achieve comparable results.\nThis indicates that it may be unnecessary to con-\nsider those relation paths that are too long.\nAs de\ufb01ned in (Bordes et al., 2013), relations in\nKBs can be divided into various types according\nto their mapping properties such as 1-to-1, 1-to-\nN, N-to-1 and N-to-N. Here we demonstrate the\nperformance of PTransE and some baselines with\nrespect to different types of relations in Table 3.\nIt is observed that, on all mapping types of re-\nlations, PTransE consistently achieves signi\ufb01cant\nimprovement as compared with TransE.\n3.2.2\nRelation Prediction\nRelation prediction aims to predict relations given\ntwo entities. We also use FB15K for evaluation.\nIn this sub-task, we can use the score function of\nPTransE to rank candidate relations instead of re-\nranking like in entity prediction.\nSince\nour\nimplementation\nof\nTransE\nhas\nachieved the best performance among all base-\nlines for entity prediction, here we only com-\npare PTransE with TransE due to limited space.\nEvaluation results are shown in Table 4, where\nwe report Hits@1 instead of Hits@10 for com-\nparison, because Hits@10 for both TransE and\nPTransE exceeds 95%.\nIn this table, we report\nthe performance of TransE without reverse rela-\ntions (TransE), with reverse relations (+Rev) and\nconsidering relation paths for testing like that in\nPTransE (+Rev+Path). We report the performance\nof PTransE with only considering relation paths (-\nTransE), only considering the part in Equation (1)\n(-Path) and considering both (PTransE).\nThe optimal con\ufb01gurations of PTransE for re-\nlation prediction are identical to those for entity\nprediction: \u03bb = 0.001, \u03b3 = 1, k = 100 and taking\nL1 as dissimilarity.\nFrom Table 4 we observe that: (1) PTransE out-\nperforms TransE+Rev+Path signi\ufb01cantly for rela-\ntion prediction by reducing 41.8% prediction er-\nrors. (2) Even for TransE itself, considering re-\nlation paths for testing can reduce 17.3% errors\nas compared with TransE+Rev. It indicates that\nencoding relation paths will bene\ufb01t for predict-\ning relations. (3) PTransE with only considering\nrelation paths (PTransE-TransE) gets surprisingly\nhigh mean rank. The reason is that, not all entity\npairs in testing triples have relation paths, which\nwill lead to random guess and the expectation of\nrank of these entity pairs is |R|/2. Meanwhile,\nHits@1 of PTransE-TransE is relatively reason-\nable, which indicates the worth of modeling rela-\nTable 3: Evaluation results on FB15K by mapping properties of relations. (%)\nTasks\nPredicting Head Entities (Hits@10)\nPredicting Tail Entities (Hits@10)\nRelation Category\n1-to-1\n1-to-N\nN-to-1\nN-to-N\n1-to-1\n1-to-N\nN-to-1\nN-to-N\nSE\n35.6\n62.6\n17.2\n37.5\n34.9\n14.6\n68.3\n41.3\nSME (linear)\n35.1\n53.7\n19.0\n40.3\n32.7\n14.9\n61.6\n43.3\nSME (bilinear)\n30.9\n69.6\n19.9\n38.6\n28.2\n13.1\n76.0\n41.8\nTransE\n43.7\n65.7\n18.2\n47.2\n43.7\n19.7\n66.7\n50.0\nTransH\n66.8\n87.6\n28.7\n64.5\n65.5\n39.8\n83.3\n67.2\nTransR\n78.8\n89.2\n34.1\n69.2\n79.2\n37.4\n90.4\n72.1\nTransE (Our)\n74.6\n86.6\n43.7\n70.6\n71.5\n49.0\n85.0\n72.9\nPTransE (ADD, 2-step)\n91.0\n92.8\n60.9\n83.8\n91.2\n74.0\n88.9\n86.4\nPTransE (MUL, 2-step)\n89.0\n86.8\n57.6\n79.8\n87.8\n71.4\n72.2\n80.4\nPTransE (RNN, 2-step)\n88.9\n84.0\n56.3\n84.5\n88.8\n68.4\n81.5\n86.7\nPTrasnE (ADD, 3-step)\n90.1\n92.0\n58.7\n86.1\n90.7\n70.7\n87.5\n88.7\ntion paths. As compared with TransE, the inferior\nof PTransE-TransE also indicates that entity repre-\nsentations are informative and crucial for relation\nprediction.\nTable 4: Evaluation results on relation prediction.\nMetric\nMean Rank\nHits@1 (%)\nRaw\nFilter\nRaw\nFilter\nTransE (Our)\n2.8\n2.5\n65.1\n84.3\n+Rev\n2.6\n2.3\n67.1\n86.7\n+Rev+Path\n2.4\n1.9\n65.2\n89.0\nPTransE (ADD, 2-step)\n1.7\n1.2\n69.5\n93.6\n-TransE\n135.8\n135.3\n51.4\n78.0\n-Path\n2.0\n1.6\n69.7\n89.0\nPTransE (MUL, 2-step)\n2.5\n2.0\n66.3\n89.0\nPTransE (RNN, 2-step)\n1.9\n1.4\n68.3\n93.2\nPTransE (ADD, 3-step)\n1.8\n1.4\n68.5\n94.0\n3.3\nRelation Extraction from Text\nRelation extraction from text aims to extract re-\nlational facts from plain text to enrich existing\nKBs. Many works regard large-scale KBs as dis-\ntant supervision to annotate sentences as training\ninstances and build relation classi\ufb01ers according to\nfeatures extracted from the sentences (Mintz et al.,\n2009; Riedel et al., 2010; Hoffmann et al., 2011;\nSurdeanu et al., 2012). All these methods reason\nnew facts only based on plain text. TransE was\nused to enrich a text-based model and achieved a\nsigni\ufb01cant improvement (Weston et al., 2013), and\nso do TransH (Wang et al., 2014) and TransR (Lin\net al., 2015). In this task, we explore the effective-\nness of PTransE for relation extraction from text.\nWe use New York Times corpus (NYT) released\nby (Riedel et al., 2010) as training and testing data.\nNYT aligns Freebase with the articles in New York\nTimes, and extracts sentence-level features such\nas part-of-speech tags, dependency tree paths for\neach mention. There are 53 relations (including\nnon-relation denoted as NA) and 121, 034 training\nmentions. We use FB40K as the KB, consisting all\nentities mentioned in NYT and 1, 336 relations.\nIn the experiments, we implemented the text-\nbased model Sm2r presented in (Weston et al.,\n2013).\nWe combine the ranking scores from\nthe text-based model with those from KB rep-\nresentations to rank testing triples, and gener-\nate precision-recall curves for both TransE and\nPTransE. For learning of TransE and PTransE,\nwe set the dimensions of entities/relations embed-\ndings k = 50, the learning rate \u03bb = 0.001, the\nmargin \u03b3 = 1.0 and dissimilarity metric as L1.\nWe also compare with MIMLRE (Surdeanu et al.,\n2012) which is the state-of-art method using dis-\ntant supervision. The evaluation curves are shown\nin Figure 3.\n0\n0.05\n0.1\n0.15\n0.2\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nRecall\nPrecision\n \n \nSm2r\nTransE\nPTransE\nMIMLRE\nFigure 3: Precision-recall curves of Sm2r, TransE\nand PTransE combine with Sm2r.\nFrom Figure 3 we can observe that, by combin-\ning with the text-based model Sm2r, the precision\nof PTransE signi\ufb01cantly outperforms TransE espe-\ncially for the top-ranked triples. This indicates that\nencoding relation paths is also useful for relation\nextraction from text.\nNote that TransE used here does not consider\nreverse relations and relation paths because the\nperformance does not change much. We analyze\nthe reason as follows. In the task of knowledge\nbase completion, each testing triple has at least\none valid relation. In contrast, many testing triples\nin this task correspond to non-relation (NA), and\nthere are usually several relation paths between\ntwo entities in these non-relation triples. TransE\ndoes not encode relation paths during the training\nphase like PTransE, which results in worse perfor-\nmance for predicting non-relation when consider-\ning relation paths in the testing phase, and com-\npensates the improvement on those triples that do\nhave relations. This indicates it is non-trivial to\nencode relation paths, and also con\ufb01rms the effec-\ntiveness of PTransE.\n3.4\nCase Study of Relation Inference\nWe have shown that PTransE can achieve high per-\nformance for knowledge base completion and re-\nlation extraction from text.\nIn this section, we\npresent some examples of relation inference ac-\ncording to relation paths.\nAccording to the learning results of PTransE,\nwe can \ufb01nd new facts from KBs. As shown in\nFigure 4, two entities Forrest Gump and English\nare connected by three relation paths, which give\nus more con\ufb01dence to predict the relation between\nthe two entities to LanguageOfFilm.\nForrest \nGump\nRobert \nZemeckis\nUnited \nStates\nNorway\nParamount \nPictures\nEnglish\nRelease Region\nDirector\nOfficial Language\nOfficial Language\nCountry\nCompany\nLanguage of Film\nLanguage\nFigure 4: An inference example in Freebase.\n4\nRelated Work\nRecent years have witnessed great advances of\nmodeling multi-relational data such as social net-\nworks and KBs.\nMany works cope with rela-\ntional learning as a multi-relational representation\nlearning problem, encoding both entities and re-\nlations in a low-dimensional latent space, based\non Bayesian clustering (Kemp et al., 2006; Miller\net al., 2009; Sutskever et al., 2009; Zhu, 2012),\nenergy-based models (Bordes et al., 2011; Chen et\nal., 2013; Socher et al., 2013; Bordes et al., 2013;\nBordes et al., 2014), matrix factorization (Singh\nand Gordon, 2008; Nickel et al., 2011; Nickel et\nal., 2012) . Among existing representation mod-\nels, TransE (Bordes et al., 2013) regards a relation\nas translation between head and tail entities for\noptimization, which achieves a good trade-off be-\ntween prediction accuracy and computational ef\ufb01-\nciency. All existing representation learning meth-\nods of knowledge bases only use direct relations\nbetween entities, ignoring rich information in re-\nlation paths.\nRelation paths have already been widely con-\nsidered in social networks and recommender sys-\ntems. Most of these works regard each relation and\npath as discrete symbols, and deal with them us-\ning graph-based algorithms, such as random walks\nwith restart (Tong et al., 2006).\nRelation paths\nhave also been used for inference on large-scale\nKBs, such as Path Ranking algorithm (PRA) (Lao\nand Cohen, 2010), which has been adopted for ex-\npert \ufb01nding (Lao and Cohen, 2010) and informa-\ntion retrieval (Lao et al., 2012). PRA has also been\nused for relation extraction based on KB structure\n(Lao et al., 2011; Gardner et al., 2013). (Nee-\nlakantan et al., 2015) further learns a recurrent\nneural network (RNN) to represent unseen rela-\ntion paths according to involved relations.\nWe\nnote that, these methods focus on modeling rela-\ntion paths for relation extraction without consid-\nering any information of entities. In contrast, by\nsuccessfully integrating the merits of modeling en-\ntities and relation paths, PTransE can learn supe-\nrior representations of both entities and relations\nfor knowledge graph completion and relation ex-\ntraction as shown in our experiments.\n5\nConclusion and Future Work\nThis paper presents PTransE, a novel representa-\ntion learning method for KBs, which encodes re-\nlation paths to embed both entities and relations\nin a low-dimensional space. To take advantages\nof relation paths, we propose path-constraint re-\nsource allocation to measure relation path reliabil-\nity, and employ semantic composition of relations\nto represent paths for optimization. We evaluate\nPTransE on knowledge base completion and re-\nlation extraction from text. Experimental results\nshow that PTransE achieves consistent and signif-\nicant improvements as compared with TransE and\nother baselines.\nIn future, we will explore the following research\ndirections: (1) This paper only considers the infer-\nence patterns between direct relations and relation\npaths between two entities for learning. There are\nmuch complicated patterns among relations. For\nexample, the inference form Queen(e)\nInference\n=====\u21d2\nFemale(e) cannot be handled by PTransE. We\nmay take advantages of \ufb01rst-order logic to encode\nthese inference patterns for representation learn-\ning. (2) There are some extensions for TransE,\ne.g., TransH and TransR. It is non-trivial for them\nto adopt the idea of PTransE, and we will explore\nto extend PTransE to these models to better deal\nwith complicated scenarios of KBs.\n6\nAcknowledgments\nZhiyuan Liu and Maosong Sun are supported by\nthe 973 Program (No. 2014CB340501) and the\nNational Natural Science Foundation of China\n(NSFC No.\n61133012) and Tsinghua-Samsung\nJoint Lab.\nHuanbo Luan is supported by the\nNational Natural Science Foundation of China\n(NSFC No. 61303075). We sincerely thank Yan-\nsong Feng for insightful discussions, and thank all\nanonymous reviewers for their constructive com-\nments.\nReferences\n[Bollacker et al.2008] Kurt Bollacker,\nColin Evans,\nPraveen Paritosh, Tim Sturge, and Jamie Taylor.\n2008.\nFreebase: a collaboratively created graph\ndatabase for structuring human knowledge. In Pro-\nceedings of KDD, pages 1247\u20131250.\n[Bordes et al.2011] Antoine Bordes, Jason Weston, Ro-\nnan Collobert, Yoshua Bengio, et al. 2011. Learn-\ning structured embeddings of knowledge bases. In\nProceedings of AAAI, pages 301\u2013306.\n[Bordes et al.2012] Antoine Bordes, Xavier Glorot, Ja-\nson Weston, and Yoshua Bengio. 2012. Joint learn-\ning of words and meaning representations for open-\ntext semantic parsing. In Proceedings of AISTATS,\npages 127\u2013135.\n[Bordes et al.2013] Antoine Bordes, Nicolas Usunier,\nAlberto Garcia-Duran, Jason Weston, and Oksana\nYakhnenko.\n2013.\nTranslating embeddings for\nmodeling multi-relational data. In Proceedings of\nNIPS, pages 2787\u20132795.\n[Bordes et al.2014] Antoine Bordes, Xavier Glorot, Ja-\nson Weston, and Yoshua Bengio. 2014. A semantic\nmatching energy function for learning with multi-\nrelational data. Machine Learning, 94(2):233\u2013259.\n[Chen et al.2013] Danqi\nChen,\nRichard\nSocher,\nChristopher D Manning, and Andrew Y Ng. 2013.\nLearning new facts from knowledge bases with\nneural tensor networks and semantic word vectors.\nProceedings of ICLR.\n[Gardner et al.2013] Matt\nGardner,\nPartha\nPratim\nTalukdar, Bryan Kisiel, and Tom M Mitchell.\n2013. Improving learning and inference in a large\nknowledge-base using latent syntactic cues.\nIn\nProceedings of EMNLP, pages 833\u2013838.\n[Hoffmann et al.2011] Raphael\nHoffmann,\nCongle\nZhang, Xiao Ling, Luke Zettlemoyer, and Daniel S\nWeld. 2011. Knowledge-based weak supervision\nfor information extraction of overlapping relations.\nIn Proceedings of ACL-HLT, pages 541\u2013550.\n[Jenatton et al.2012] Rodolphe\nJenatton,\nNicolas\nL\nRoux, Antoine Bordes, and Guillaume R Obozin-\nski. 2012. A latent factor model for highly multi-\nrelational data.\nIn Proceedings of NIPS, pages\n3167\u20133175.\n[Kemp et al.2006] Charles Kemp, Joshua B Tenen-\nbaum, Thomas L Grif\ufb01ths, Takeshi Yamada, and\nNaonori Ueda. 2006. Learning systems of concepts\nwith an in\ufb01nite relational model. In Proceedings of\nAAAI, volume 3, page 5.\n[Lao and Cohen2010] Ni Lao and William W Cohen.\n2010. Relational retrieval using a combination of\npath-constrained random walks. Machine learning,\n81(1):53\u201367.\n[Lao et al.2011] Ni Lao, Tom Mitchell, and William W\nCohen. 2011. Random walk inference and learning\nin a large scale knowledge base. In Proceedings of\nEMNLP, pages 529\u2013539.\n[Lao et al.2012] Ni Lao, Amarnag Subramanya, Fer-\nnando Pereira, and William W Cohen. 2012. Read-\ning the web with learned syntactic-semantic infer-\nence rules.\nIn Proceedings of EMNLP-CoNLL,\npages 1017\u20131026.\n[Lin et al.2015] Yankai Lin, Zhiyuan Liu, Maosong\nSun, Yang Liu, and Xuan Zhu. 2015. Learning en-\ntity and relation embeddings for knowledge graph\ncompletion. In Proceedings of AAAI, pages 2181\u2013\n2187.\n[L\u00a8u and Zhou2011] Linyuan L\u00a8u and Tao Zhou. 2011.\nLink prediction in complex networks: A survey.\nPhysica A: Statistical Mechanics and its Applica-\ntions, 390(6):1150\u20131170.\n[Mikolov et al.2010] Tomas Mikolov, Martin Kara\ufb01\u00b4at,\nLukas Burget, Jan Cernock`y, and Sanjeev Khudan-\npur.\n2010.\nRecurrent neural network based lan-\nguage model. In Proceedings of Interspeech, pages\n1045\u20131048.\n[Miller et al.2009] Kurt Miller, Michael I Jordan, and\nThomas L Grif\ufb01ths.\n2009.\nNonparametric latent\nfeature models for link prediction. In Proceedings\nof NIPS, pages 1276\u20131284.\n[Mintz et al.2009] Mike Mintz,\nSteven Bills,\nRion\nSnow, and Dan Jurafsky.\n2009.\nDistant supervi-\nsion for relation extraction without labeled data. In\nProceedings of ACL-IJCNLP, pages 1003\u20131011.\n[Mitchell and Lapata2008] Jeff Mitchell and Mirella\nLapata.\n2008.\nVector-based models of semantic\ncomposition. In Proceedings of ACL, pages 236\u2013\n244.\n[Neelakantan et al.2015] Arvind\nNeelakantan,\nBen-\njamin Roth,\nand Andrew McCallum.\n2015.\nCompositional vector space models for knowledge\nbase inference.\nIn 2015 AAAI Spring Symposium\nSeries.\n[Nickel et al.2011] Maximilian Nickel, Volker Tresp,\nand Hans-Peter Kriegel. 2011. A three-way model\nfor collective learning on multi-relational data. In\nProceedings of ICML, pages 809\u2013816.\n[Nickel et al.2012] Maximilian Nickel, Volker Tresp,\nand Hans-Peter Kriegel. 2012. Factorizing yago:\nscalable machine learning for linked data. In Pro-\nceedings of WWW, pages 271\u2013280.\n[Riedel et al.2010] Sebastian Riedel, Limin Yao, and\nAndrew McCallum. 2010. Modeling relations and\ntheir mentions without labeled text. In Proceedings\nof ECML-PKDD, pages 148\u2013163.\n[Singh and Gordon2008] Ajit P Singh and Geoffrey J\nGordon.\n2008.\nRelational learning via collective\nmatrix factorization. In Proceedings of KDD, pages\n650\u2013658.\n[Socher et al.2013] Richard\nSocher,\nDanqi\nChen,\nChristopher D Manning, and Andrew Ng.\n2013.\nReasoning with neural tensor networks for knowl-\nedge base completion.\nIn Proceedings of NIPS,\npages 926\u2013934.\n[Surdeanu et al.2012] Mihai Surdeanu, Julie Tibshirani,\nRamesh Nallapati, and Christopher D Manning.\n2012. Multi-instance multi-label learning for rela-\ntion extraction. In Proceedings of EMNLP, pages\n455\u2013465.\n[Sutskever et al.2009] Ilya Sutskever, Joshua B Tenen-\nbaum, and Ruslan Salakhutdinov. 2009. Modelling\nrelational data using bayesian clustered tensor fac-\ntorization.\nIn Proceedings of NIPS, pages 1821\u2013\n1828.\n[Tong et al.2006] Hanghang Tong, Christos Faloutsos,\nand Jia-Yu Pan. 2006. Fast random walk with restart\nand its applications. In Proceedings of ICDM, pages\n613\u2013622.\n[Wang et al.2014] Zhen Wang, Jianwen Zhang, Jianlin\nFeng, and Zheng Chen. 2014. Knowledge graph\nembedding by translating on hyperplanes. In Pro-\nceedings of AAAI, pages 1112\u20131119.\n[Weston et al.2013] Jason Weston,\nAntoine Bordes,\nOksana Yakhnenko, and Nicolas Usunier.\n2013.\nConnecting language and knowledge bases with em-\nbedding models for relation extraction. In Proceed-\nings of EMNLP, pages 1366\u20131371.\n[Zhou et al.2007] Tao Zhou, Jie Ren, Mat\u00b4u\u02c7s Medo, and\nYi-Cheng Zhang. 2007. Bipartite network projec-\ntion and personal recommendation. Physical Review\nE, 76(4):046115.\n[Zhu2012] Jun Zhu. 2012. Max-margin nonparamet-\nric latent feature models for link prediction. In Pro-\nceedings of ICML, pages 719\u2013726.\n",
        "sentence": " In parallel, recent work has proposed to model regularities beyond local facts by considering multirelation paths (Garc\u0131\u0301a-Dur\u00e1n et al., 2015; Lin et al., 2015a; Shen et al., 2016). 2 PTransE (Lin et al., 2015a) Path 58 84.",
        "context": "relations into a low-dimensional space.\nMost existing methods only consider\ndirect relations in representation learning.\nWe argue that multiple-step relation paths\nalso contain rich inference patterns be-\ntween entities, and propose a path-based\nRelation paths\nhave also been used for inference on large-scale\nKBs, such as Path Ranking algorithm (PRA) (Lao\nand Cohen, 2010), which has been adopted for ex-\npert \ufb01nding (Lao and Cohen, 2010) and informa-\ntities and relation paths, PTransE can learn supe-\nrior representations of both entities and relations\nfor knowledge graph completion and relation ex-\ntraction as shown in our experiments.\n5\nConclusion and Future Work"
    },
    {
        "title": "Learning Entity and Relation Embeddings for Knowledge Graph Completion",
        "author": [
            "Yankai Lin",
            "Zhiyuan Liu",
            "Maosong Sun",
            "Yang Liu",
            "Xuan Zhu."
        ],
        "venue": "Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence Learning, pages",
        "citeRegEx": "Lin et al\\.,? 2015b",
        "shortCiteRegEx": "Lin et al\\.",
        "year": 2015,
        "abstract": "\n      \n        Knowledge graph completion aims to perform link prediction between entities. In this paper, we consider the approach of knowledge graph embeddings. Recently, models such as TransE and TransH build entity and relation embeddings by regarding a relation as translation from head entity to tail entity. We note that these models simply put both entities and relations within the same semantic space. In fact, an entity may have multiple aspects and various relations may focus on different aspects of entities, which makes a common space insufficient for modeling. In this paper, we propose TransR to build entity and relation embeddings in separate entity space and relation spaces. Afterwards, we learn embeddings by first projecting entities from entity space to corresponding relation space and then building translations between projected entities. In experiments, we evaluate our models on three tasks including link prediction, triple classification and relational fact extraction. Experimental results show significant and consistent improvements compared to state-of-the-art baselines including TransE and TransH.\n      \n    ",
        "full_text": "",
        "sentence": " dependent space (Bordes et al., 2014; Ji et al., 2015; Lin et al., 2015b; Nguyen et al., 2016b), and then model the translation property in the projected space. To better model relation-specific aspects of the same entity, TransR (Lin et al., 2015b) uses projection matrices and projects the head entity and the tail entity to a relation-dependent space. Previous work (Bordes et al., 2013; Lin et al., 2015b; Yang et al., 2015; Nguyen et al., 2016b) generally constructs a set of corrupted triples by replacing the head entity or tail entity with a random entity uniformly sampled from the KB. 4 TransR (Lin et al., 2015b) No 225 92. 7 CTransR (Lin et al., 2015b) No 218 92. In KBC, CTransR (Lin et al., 2015b) enables relation embedding sharing across similar relations, but they cluster relations before training rather than learning it in a principled way.",
        "context": null
    },
    {
        "title": "K-sparse autoencoders",
        "author": [
            "Alireza Makhzani",
            "Brendan Frey."
        ],
        "venue": "Proceedings of the International Conference on Learning Representations.",
        "citeRegEx": "Makhzani and Frey.,? 2014",
        "shortCiteRegEx": "Makhzani and Frey.",
        "year": 2014,
        "abstract": "Recently, it has been observed that when representations are learnt in a way\nthat encourages sparsity, improved performance is obtained on classification\ntasks. These methods involve combinations of activation functions, sampling\nsteps and different kinds of penalties. To investigate the effectiveness of\nsparsity by itself, we propose the k-sparse autoencoder, which is an\nautoencoder with linear activation function, where in hidden layers only the k\nhighest activities are kept. When applied to the MNIST and NORB datasets, we\nfind that this method achieves better classification results than denoising\nautoencoders, networks trained with dropout, and RBMs. k-sparse autoencoders\nare simple to train and the encoding stage is very fast, making them\nwell-suited to large problem sizes, where conventional sparse coding algorithms\ncannot be applied.",
        "full_text": "arXiv:1312.5663v2  [cs.LG]  22 Mar 2014\nk-Sparse Autoencoders\nAlireza Makhzani\nmakhzani@psi.utoronto.ca\nBrendan Frey\nfrey@psi.utoronto.ca\nUniversity of Toronto, 10 King\u2019s College Rd. Toronto, Ontario M5S 3G4, Canada\nAbstract\nRecently, it has been observed that when rep-\nresentations are learnt in a way that encour-\nages sparsity, improved performance is ob-\ntained on classi\ufb01cation tasks. These meth-\nods involve combinations of activation func-\ntions, sampling steps and di\ufb00erent kinds of\npenalties.\nTo investigate the e\ufb00ectiveness\nof sparsity by itself, we propose the \u201ck-\nsparse autoencoder\u201d, which is an autoen-\ncoder with linear activation function, where\nin hidden layers only the k highest activities\nare kept. When applied to the MNIST and\nNORB datasets, we \ufb01nd that this method\nachieves better classi\ufb01cation results than de-\nnoising autoencoders, networks trained with\ndropout, and RBMs. k-sparse autoencoders\nare simple to train and the encoding stage\nis very fast, making them well-suited to large\nproblem sizes, where conventional sparse cod-\ning algorithms cannot be applied.\n1. Introduction\nSparse feature learning algorithms range from sparse\ncoding\napproaches\n(Olshausen & Field,\n1997)\nto\ntraining\nneural\nnetworks\nwith\nsparsity\npenalties\n(Nair & Hinton, 2009; Lee et al., 2007). These meth-\nods typically comprise two steps: a learning algorithm\nthat produces a dictionary W that sparsely represents\nthe data {xi}N\ni=1, and an encoding algorithm that,\ngiven the dictionary, de\ufb01nes a mapping from a new\ninput vector x to a feature vector.\nA practical problem with sparse coding is that both\nthe dictionary learning and the sparse encoding steps\nare computationally expensive. Dictionaries are usu-\nally learnt o\ufb04ine by iteratively recovering sparse codes\nInternational Conference on Learning Representations,\nICLR 2014\nand updating the dictionary. Sparse codes are com-\nputed using the current dictionary W and a pursuit\nalgorithm to solve\n\u02c6zi = argmin\nz\n\u2225xi \u2212Wz\u22252\n2 s.t. \u2225z\u22250 < k\n(1)\nwhere zi, i = 1,..,N are the columns of Z. Convex\nrelaxation methods such as \u21131 minimization or greedy\nmethods such as OMP (Tropp & Gilbert, 2007) are\nused to solve the above optimization. While greedy al-\ngorithms are faster, they are still slow in practice. The\ncurrent sparse codes are then used to update the dic-\ntionary, using techniques such as the method of opti-\nmal directions (MOD) (Engan et al., 1999) or K-SVD\n(Aharon et al., 2005).\nThese methods are computa-\ntionally expensive; MOD requires inverting the data\nmatrix at each step and K-SVD needs to compute a\nSVD in order to update every column of the dictionary.\nTo achieve speedups,\nin (Gregor & LeCun, 2010;\nKavukcuoglu et al., 2010), a parameterized non-linear\nencoder function is trained to explicitly predict sparse\ncodes using a soft thresholding operator.\nHowever,\nthey assume that the dictionary is already given and\ndo not address the o\ufb04ine phase.\nAnother approach that has been taken recently is to\ntrain autoencoders in a way that encourages sparsity.\nHowever, these methods usually involve combinations\nof activation functions, sampling steps and di\ufb00erent\nkinds of penalties, and are sometimes not guaranteed\nto produce sparse representations for each input. For\nexample, in (Lee et al., 2007; Nair & Hinton, 2009),\na \u201clifetime sparsity\u201d penalty function proportional to\nthe negative of the KL divergence between the hidden\nunit marginals and the target sparsity probability is\nadded to the cost function. This results in sparse acti-\nvation of hidden units across training points, but does\nnot guarantee that each input has a sparse represen-\ntation.\nThe contributions of this paper are as follows. (i) We\ndescribe \u201ck-sparse autoencoders\u201d and show that they\ncan be e\ufb03ciently learnt and used for sparse coding.\nk-Sparse Autoencoders\n(ii) We explore how di\ufb00erent sparsity levels (k) im-\npact representations and classi\ufb01cation performance.\n(iii) We show that by solely relying on sparsity as the\nregularizer and as the only nonlinearity, we can achieve\nmuch better results than the other methods, including\nRBMs, denoising autoencoders (Vincent et al., 2008)\nand dropout (Hinton et al., 2012).\n(iv) We demon-\nstrate that k-sparse autoencoders are suitable for pre-\ntraining and achieve results comparable to state-of-\nthe-art on MNIST and NORB datasets.\nIn this paper, \u0393 is an estimated support set and \u0393c is\nits complement. W \u2020 is the pseudo-inverse of W and\nsuppk(x) is an operator that returns the indices of\nthe k largest coe\ufb03cients of its input vector.\nz\u0393 is\nthe vector obtained by restricting the elements of z\nto the indices of \u0393 and W\u0393 is the matrix obtained by\nrestricting the columns of W to the indices of \u0393.\n2. Description of the Algorithm\n2.1. The Basic Autoencoder\nA shallow autoencoder maps an input vector x to a\nhidden representation using the function z = f(Px+b),\nparameterized by {P,b}. f is the activation function,\ne.g., linear, sigmoidal or ReLU. The hidden represen-\ntation is then mapped linearly to the output using\n\u02c6x = Wz + b\u2032. The parameters are optimized to mini-\nmize the mean square error of \u2225\u02c6x\u2212x\u22252\n2 over all training\npoints. Often, tied weights are used, so that P = W \u22ba.\n2.2. The k-Sparse Autoencoder\nThe k-sparse autoencoder is based on an autoencoder\nwith linear activation functions and tied weights. In\nthe feedforward phase, after computing the hidden\ncode z = W \u22bax+b, rather than reconstructing the input\nfrom all of the hidden units, we identify the k largest\nhidden units and set the others to zero. This can be\ndone by sorting the activities or by using ReLU hid-\nden units with thresholds that are adaptively adjusted\nuntil the k larges activities are identi\ufb01ed.\nThis re-\nsults in a vector of activities with the support set of\nsuppk(W \u22bax+b). Note that once the k largest activities\nare selected, the function computed by the network is\nlinear. So the only non-linearity comes from the se-\nlection of the k largest activities. This selection step\nacts as a regularizer that prevents the use of an overly\nlarge number of hidden units when reconstructing the\ninput.\nOnce the weights are trained, the resulting sparse rep-\nresentations may be used for learning to perform down-\nstream classi\ufb01cation tasks. However, it has been ob-\nserved that often, better results are obtained when the\nsparse encoding stage used for classi\ufb01cation does not\nexactly match the encoding used for dictionary train-\ning (Coates & Ng, 2011).\nFor example, while in k-\nmeans, it is natural to have a hard-assignment of the\npoints to the nearest cluster in the encoding stage, it\nhas been shown in (Van Gemert et al., 2008) that soft\nassignments result in better classi\ufb01cation performance.\nSimilarly, for the k-sparse autoencoder, instead of us-\ning the k largest elements of W \u22bax+b as the features, we\nhave observed that slightly better performance is ob-\ntained by using the \u03b1k largest hidden units where \u03b1 \u22651\nis selected using validation data. So at the test time,\nwe use the support set de\ufb01ned by supp\u03b1k(W \u22bax + b).\nThe algorithm is summarized as follows.\nk-Sparse Autoencoders:\nTraining:\n1) Perform the feedforward phase and compute\nz = W \u22bax + b\n2) Find the k largest activations of z and set\nthe rest to zero.\nz(\u0393)c = 0\nwhere\n\u0393 = suppk(z)\n3) Compute the output and the error using the\nsparsi\ufb01ed z.\n\u02c6x = Wz + b\u2032\nE = \u2225x \u2212\u02c6x\u22252\n2\n3) Backpropagate the error through the k largest\nactivations de\ufb01ned by \u0393 and iterate.\nSparse Encoding:\nCompute the features h = W \u22bax + b. Find its \u03b1k\nlargest activations and set the rest to zero.\nh(\u0393)c = 0\nwhere\n\u0393 = supp\u03b1k(h)\n3. Analysis of the k-Sparse\nAutoencoder\nIn this section, we explain how the k-sparse autoen-\ncoder can be viewed in the context of sparse coding\nwith incoherent matrices. This perspective sheds light\non why the k-sparse autoencoders work and why they\nachieve invariant features and consequently good clas-\nsi\ufb01cation results. We \ufb01rst explain a sparse recovery al-\ngorithm and then show that the k-sparse autoencoder\niterates between an approximation of this algorithm\nand a dictionary update stage.\n3.1. Iterative Thresholding with Inversion (ITI)\nIterative hard thresholding (Blumensath & Davies,\n2009) is a class of low complexity algorithms, which\nhas recently been proposed for the reconstruction of\nsparse signals. In this work, we use a variant called\n\u201citerative thresholding with inversion\u201d (Maleki, 2009).\nGiven a \ufb01xed x and W, starting from z0 = 0, ITI iter-\natively \ufb01nds the sparsest solution of x = Wz using the\nk-Sparse Autoencoders\nfollowing steps.\n1. Support Estimation Step:\n\u0393 = suppk(zn + W \u22ba(x \u2212Wzn))\n(2)\n2. Inversion Step:\nzn+1\n\u0393\n= W \u2020\n\u0393x = (W \u22ba\n\u0393W\u0393)\u22121W \u22ba\n\u0393x\nzn+1\n(\u0393)c = 0\n(3)\nAssume H = W \u22baW \u2212I and z0 is the true sparse so-\nlution.\nThe \ufb01rst step of ITI estimates the support\nset as \u0393 = suppk(W \u22bax) = suppk(z0 + Hz0).\nIf W\nwas orthogonal, we would have Hz0 = 0 and the algo-\nrithm would succeed in the \ufb01rst iteration. But if W\nis overcomplete, Hz0 behaves as a noise vector whose\nvariance decreases after each iteration. After estimat-\ning the support set of z as \u0393, we restrict W to the\nindices included in \u0393 and form W\u0393. We then use the\npseudo-inverse of W\u0393 to estimate the non-zero values\nminimizing \u2225x\u2212W\u0393z\u0393\u22252\n2. Lastly, we re\ufb01ne the support\nestimation and repeat the whole process until conver-\ngence.\n3.2. Sparse Coding with the k-Sparse\nAutoencoder\nHere, we show that we can derive the k-sparse autoen-\ncoder tarining algorithm by approximating a sparse\ncoding algorithm that uses the ITI algorithm jointly\nwith a dictionary update stage.\nThe conventional approach of sparse coding is to \ufb01x\nthe sparse code matrix Z, while updating the dictio-\nnary. However, here, after estimating the support set\nin the \ufb01rst step of the ITI algorithm, we jointly per-\nform the inversion step of ITI and the dictionary up-\ndate step, while \ufb01xing just the support set of the sparse\ncode Z. In other words, we update the atoms of the\ndictionary and allow the corresponding non-zero values\nto change at the same time to minimize \u2225X \u2212W\u0393Z\u0393\u22252\n2\nover both W\u0393 and Z\u0393.\nWhen we are performing sparse recovery with the ITI\nalgorithm using a \ufb01xed dictionary, we should perform\na \ufb01xed number of iterations to get the perfect recon-\nstruction of the signal. But, in sparse coding, since we\nlearnt a dictionary that is adapted to the signals, as\nshown in Section 3.3, we can \ufb01nd the support set just\nwith the \ufb01rst iteration of ITI:\n\u0393z = suppk(W \u22bax)\n(4)\nIn the inversion step of the ITI algorithm, once we\nestimate the support set, we use the pseudo-inverse of\nW\u0393 to \ufb01nd the non-zero values of the support set. The\npseudo-inverse of the matrix W\u0393 is a matrix, such as\nP\u0393, that minimizes the following cost function.\nW \u2020\n\u0393 = arg min\nP\u0393\n\u2225x \u2212W\u0393z\u0393\u22252\n2\n= arg min\nP\u0393\n\u2225x \u2212W\u0393P\u0393x\u22252\n2\n(5)\nFinding the exact pseudo-inverse of W\u0393 is computa-\ntionally expensive, so instead, we perform a single step\nof gradient descent. The gradient with respect to P\u0393\nis found as follows:\n\u2202\u2225x \u2212W\u0393z\u0393\u22252\n2\n\u2202P\u0393\n= \u2202\u2225x \u2212W\u0393z\u0393\u22252\n2\n\u2202z\u0393\nx\n(6)\nThe \ufb01rst term of the right hand side of the Equation\n(6) is the dictionary update stage, which is computed\nas follows:\n\u2202\u2225x \u2212W\u0393z\u0393\u22252\n2\n\u2202z\u0393\n= (W\u0393z\u0393 \u2212x)z\u22ba\n\u0393\n(7)\nTherefore, in order to approximate the pseudo-inverse,\nwe \ufb01rst \ufb01nd the dictionary derivative and then \u201cback-\npropagate\u201d it to \ufb01nd the update of the pseudo-inverse.\nWe can view these operations in the context of an au-\ntoencoder with linear activations where P is the en-\ncoder weight matrix and W is the decoder weight ma-\ntrix.\nAt each iteration, instead of back-propagating\nthrough all the hidden units, we just back-propagate\nthrough the units with the k largest activities, de\ufb01ned\nby suppk(W \u22bax), which is the \ufb01rst iteration of ITI.\nKeeping the k largest hidden activities and ignoring\nthe others is the same as forming W\u0393 by restricting\nW to the estimated support set. Back-propagation on\nthe decoder weights is the same as gradient descent on\nthe dictionary and back-propagation on the encoder\nweights is the same as approximating the pseudo-\ninverse of the corresponding W\u0393.\nWe can perform support estimation in the feedforward\nphase by assuming P = W \u22ba(i.e., the autoencoder has\ntied weights).\nIn this case, support estimation can\nbe done by computing z = W \u22bax + b and picking the k\nlargest activations; the bias just accounts for the mean\nand subtracts its contribution. Then the \u201cinversion\u201d\nand \u201cdictionary update\u201d steps are done at the same\ntime by back-propagation through just the units with\nthe k largest activities.\nIn summary, we can view k-sparse autoencoders as the\napproximation of a sparse coding algorithm which uses\nITI in the sparse recovery stage.\nk-Sparse Autoencoders\n3.3. Importance of Incoherence\nThe coherence of a dictionary indicates the degree of\nsimilarity between di\ufb00erent atoms or di\ufb00erent collec-\ntions of atoms. Since the dictionary is overcomplete,\nwe can represent each column of the dictionary as a\nlinear combination of other columns. But what inco-\nherence implies is that we should not be able to repre-\nsent a column as a sparse linear combination of other\ncolumns and the coe\ufb03cients of the linear combination\nshould be dense. For example, if two columns are ex-\nactly the same, then the dictionary is highly coherent\nsince we can represent one of those columns as the\nsparse linear combination of the rest of the columns.\nA naive measure of coherence that has been proposed\nin the literature is the mutual coherence \u00b5(W) which is\nde\ufb01ned as the maximum absolute inner product across\nall the possible pairs of the atoms of the dictionary.\n\u00b5(W) = max\ni\u2260j \u2223\u27e8wi,wj\u27e9\u2223\n(8)\nThere is a close relationship between the coherency of\nthe dictionary and the uniqueness of the sparse solu-\ntion of x = Wz.\nIn (Donoho & Elad, 2003), it has\nbeen proven that if k \u2264(1 + \u00b5\u22121), then the sparsest\nsolution is unique.\nWe can show that if the dictionary is incoherent\nenough, there is going to be an attraction ball around\nthe signal x and there is only one unique sparse lin-\near combination of the columns that can get into this\nattraction ball. So even if we perturb the input with\na small amount of noise, translation, rotation, etc.,\nwe can still achieve perfect reconstruction of the orig-\ninal signal and the sparse features are always roughly\nconserved.\nTherefore, incoherency of the dictionary\nis a measure of invariance and stability of the fea-\ntures.\nThis is related to the denoising autoencoder\n(Vincent et al., 2008) in which we achieve invariant\nfeatures by trying to reconstruct the original signal\nfrom its noisy versions.\nHere we show that if the dictionary is incoherent\nenough, the \ufb01rst step of the ITI algorithm is su\ufb03cient\nfor perfect sparse recovery.\nTheorem 3.1. Assume x = Wz and the columns of\nthe dictionary have unit \u21132-norm. Also without loss of\ngenerality, assume that the non-zero elements of z are\nits \ufb01rst k elements and are sorted as z1 \u2265z2 \u2265... \u2265zk.\nThen, if k\u00b5 \u2264\nzk\n2z1 , we can recover the support set of z\nusing suppk(W \u22bax).\nProof: Let us assume 0 \u2264i \u2264k and y = W \u22bax. Then,\nwe can write:\nyi = zi +\nk\n\u2211\nj=1,j\u2260i\n\u27e8wi,wj\u27e9zj\n\u2265zi \u2212\u00b5\nk\n\u2211\nj=1,j\u2260i\nzj \u2265zk \u2212k\u00b5z1\n(9)\nOn the other hand:\nmax\ni>k {yi} = max\ni>k\n\u23a7\u23aa\u23aa\u23a8\u23aa\u23aa\u23a9\nk\n\u2211\nj=1\n\u27e8wi,wj\u27e9zj\n\u23ab\u23aa\u23aa\u23ac\u23aa\u23aa\u23ad\n\u2264k\u00b5z1\n(10)\nSo if k\u00b5 \u2264\nzk\n2z1 , all the \ufb01rst k elements of y are guaran-\nteed to be greater than the rest of its elements.\nAs we can see from Theorem 3.1, the chances of \ufb01nding\nthe true support set with the encoder part of the k-\nsparse autoencoder depends on the incoherency of the\nlearnt dictionary. As the k-sparse autoencoder con-\nverges (i.e., the reconstruction error goes to zero), the\nalgorithm learns a dictionary that satis\ufb01es x \u2248Wz, so\nthe support set of z can be estimated using the \ufb01rst\nstep of ITI. Since suppk(W \u22bax) succeeds in \ufb01nding the\nsupport set when the algorithm converges, the learnt\ndictionary must be su\ufb03ciently incoherent.\n4. Experiments\nIn this section, we evaluate the performance of k-\nsparse autoencoders in both unsupervised learning and\nin shallow and deep discriminative learning tasks.\n4.1. Datasets\nWe use the MNIST handwritten digit dataset, which\nconsists of 60,000 training images and 10,000 test im-\nages.\nWe randomly separate the training set into\n50,000 training cases and 10,000 cases for validation.\nWe also use the small NORB normalized-uniform\ndataset (LeCun et al., 2004), which contains 24,300\ntraining examples and 24,300 test examples.\nThis\ndatabase contains images of 50 toys from 5 generic cat-\negories: four-legged animals, human \ufb01gures, airplanes,\ntrucks, and cars. Each image consists of two channels,\neach of size 96 \u00d7 96 pixels. We take the inner 64 \u00d7 64\npixels of each channel and resize it using bicubic in-\nterpolation to the size of 32 \u00d7 32 pixels from which we\nform a vector with 2048 dimensions as the input. Data\npoints are subtracted by the mean and divided by the\nstandard deviation along each input dimension across\nthe whole training set to normalize the contrast. The\ntraining set is separated into 20,000 for training and\n4,300 for validation.\nWe also test our method on natural image patches ex-\ntracted from CIFAR-10 dataset. We randomly extract\n1000000 patches of size 8\u00d78 from the 50000 32\u00d732 im-\nk-Sparse Autoencoders\nages of CIFAR-10. Each patch is then locally contrast-\nnormalized and ZCA whitened.\nThis preprocessing\npipeline is the same as the one used in (Coates et al.,\n2011) for feature extraction.\n4.2. Training of k-Sparse Autoencoders\n4.2.1. Scheduling of the Sparsity Level\nWhen we are enforcing low sparsity levels in k-sparse\nautoencoders (e.g., k=15 on MNIST), one issue that\nmight arise is that in the \ufb01rst few epochs, the al-\ngorithm greedily assigns individual hidden units to\ngroups of training cases, in a manner similar to k-\nmeans clustering. In subsequent epochs, these hidden\nunits will be picked and re-enforced and other hidden\nunits will not be adjusted. That is, too much sparsity\ncan prevent gradient back-propagation from adjusting\nthe weights of these other \u2018dead\u2019 hidden units. We can\naddress this problem by scheduling the sparsity level\nover epochs as follows.\nSuppose we are aiming for a sparsity level of k = 15.\nThen, we start o\ufb00with a large sparsity level (e.g.\nk = 100) for which the k-sparse autoencoder can train\nall the hidden units.\nWe then linearly decrease the\nsparsity level from k = 100 to k = 15 over the \ufb01rst\nhalf of the epochs. This initializes the autoencoder in\na good regime, for which all of the hidden units have\na signi\ufb01cant chance of being picked. Then, we keep\nk = 15 for the second half of the epochs. With this\nscheduling, we can train all of the \ufb01lters, even for low\nsparsity levels.\n4.2.2. Training Hyper-parameters\nWe optimized the model parameters using stochastic\ngradient descent with momentum as follows.\nvk+1 = mkvk \u2212\u03b7k\u2207f(xk)\nxk+1 = xk + vk\n(11)\nHere, vk is the velocity vector, mk is the momentum\nand \u03b7k is the learning rate at the k-th iteration. We\nalso use a Gaussian distribution with a standard devi-\nation of \u03c3 for initialization of the weights. We use dif-\nferent momentum values, learning rates and initializa-\ntions based on the task and the dataset, and validation\nis used to select hyperparameters. In the unsupervised\nMNIST task, the values were \u03c3 = 0.01 , mk = 0.9 and\n\u03b7k = 0.01, for 5000 epochs. In the supervised MNIST\ntask, training started with mk = 0.25 and \u03b7k = 1, and\nthen the learning rate was linearly decreased to 0.001\nover 200 epochs. In the unsupervised NORB task, the\nvalues were \u03c3 = 0.01, mk = 0.9 and \u03b7k = 0.0001, for\n5000 epochs. In the supervised NORB task, training\nstarted with mk = 0.9 and \u03b7k = 0.01, and then the\nlearning rate was linearly decreased to 0.001 over 200\nepochs.\n4.2.3. Implementations\nWhile most of the conventional sparse coding algo-\nrithms require complex matrix operations such as ma-\ntrix inversion or SVD decomposition, the k-sparse au-\ntoencoders only need matrix multiplications and sort-\ning operations in both dictionary learning stage and\nthe sparse encoding stage. (For a parallel, distributed\nimplementation, the sorting operation can be replaced\nby a method that recursively applies a threshold until\nk values remain.) We used an e\ufb03cient GPU implemen-\ntation obtained using the publicly available gnumpy\nlibrary (Tieleman, 2010) on a single Nvidia GTX 680\nGPU.\n4.3. E\ufb00ect of Sparsity Level\nIn k-sparse autoencoders, we are able to tune the value\nof k to obtain the desirable sparsity level which makes\nthe algorithm suitable for a wide variety of datasets.\nFor example, one application could be pre-training a\nshallow or deep discriminative neural network.\nFor\nlarge values of k (e.g., k = 100 on MNIST), the algo-\nrithm tends to learn very local features as is shown in\nFigure 1a and 2a. These features are too primitive to\nbe used for classi\ufb01cation using a shallow architecture\nsince a naive linear classi\ufb01er does not have enough ca-\npacity to combine these features and achieve a good\nclassi\ufb01cation rate. However, these features could be\nused for pre-training deep neural nets.\nAs we decrease the the sparsity level (e.g., k = 40 on\nMNIST), the output is reconstructed using a smaller\nnumber of hidden units and thus the features tend to\nbe more global, as can be seen in Figure 1b,1c and 2b.\nFor example, in the MNIST dataset, the lengths of the\nstrokes increase when the sparsity level is decreased.\nThese less local features are suitable for classi\ufb01cation\nusing a shallow architecture. Nevertheless, forcing too\nmuch sparsity (e.g., k = 10 on MNIST), results in fea-\ntures that are too global and do not factor the input\ninto parts, as depicted Figure 1d and 2c.\nFig. 3 shows the visualization of \ufb01lters of the k-sparse\nautoencoder with 1000 hidden units and sparsity level\nof k = 50 learnt from random image patches extracted\nfrom CIFAR-10 dataset. We can see that the k-sparse\nautoencoder has learnt localized Gabor \ufb01lters from\nnatural image patches.\nFig. 4 plots histograms of the hidden unit activities\nfor various unsupervised learning algorithms, includ-\nk-Sparse Autoencoders\n(a) k = 70\n(b) k = 40\n(c) k = 25\n(d) k = 10\nFigure 1. Filters of the k-sparse autoencoder for di\ufb00erent sparsity levels k, learnt from MNIST with 1000 hidden units.\n(a) k = 200\n(b) k = 150\n(c) k = 50\nFigure 2. Filters of the k-sparse autoencoder for di\ufb00erent sparsity levels k, learnt from NORB with 4000 hidden units.\nk-Sparse Autoencoders\nError Rate\nRaw Pixels\n7.20%\nRBM\n1.81%\nDropout Autoencoder (50% hidden)\n1.80%\nDenoising Autoencoder\n1.95%\n(20% input dropout)\nDropout + Denoising Autoencoder\n1.60%\n(20% input and 50% hidden)\nk-Sparse Autoencoder, k = 40\n1.54%\nk-Sparse Autoencoder, k = 25\n1.35%\nk-Sparse Autoencoder, k = 10\n2.10%\nTable 1. Performance\nof\nunsupervised\nlearning\nmethods\n(without \ufb01ne-tuning) with 1000 hidden units on MNIST.\nError Rate\nRaw Pixels\n23%\nRBM (weight decay)\n10.6%\nDropout Autoencoder\n10.1%\nDenoising Autoencoder\n9.5%\n(20% input dropout)\nk-Sparse Autoencoder, k = 200\n10.4%\nk-Sparse Autoencoder, k = 150\n8.6%\nk-Sparse Autoencoder, k = 75\n9.5%\nTable 2. Performance of unsupervised learning\nmethods (without \ufb01ne-tuning) with 4000 hidden\nunits on NORB.\ning the k-sparse autoencoder (k=70 and k=15), ap-\nplied to the MNIST data. This \ufb01gure contrasts the\nsparsity achieved by the k-sparse autoencoder with\nthat of other algorithms.\nFigure 3. Filters of k-sparse autoencoder with 1000 hidden\nunits and k = 50, learnt from CIFAR-10 random patches.\n4.4. Unsupervised Feature Learning Results\nIn order to compare the quality of the features learnt\nby our algorithm with those learnt by other unsuper-\nvised learning methods, we \ufb01rst extracted features us-\ning each unsupervised learning algorithm.\nThen we\n\ufb01xed the features and trained a logistic regression clas-\nsi\ufb01er using those features. The usefulness of the fea-\ntures is then evaluated by examining the error rate of\nthe classi\ufb01er.\nWe trained a number of architectures on the MNIST\nand NORB datasets, including RBM, dropout autoen-\ncoder and denoising autoencoder.\nIn dropout, after\n\ufb01nding the features using dropout regularization with\na dropout rate of 50%, we used all of the hidden units\nas the features (this worked best).\nFor the denois-\ning autoencoder, after training the network by drop-\nping the input pixels with a rate of 20%, we used\n0\n1\n2\n3\n4\n5\n6\n0\n1\n2\n3\n4\n5\n6\n7\n8\nLog histogram of hidden activities\nReLU Autoencoder\nDropout Autoencoder, 50% hidden and 20% input\nk-Sparse Autoencoder, k=70\nk-Sparse Autoencoder, k=15\nFigure 4. Histogram of hidden unit activities for various\nunsupervised learning methods.\nall of the uncorrupted input pixels to \ufb01nd the fea-\ntures for classi\ufb01cation (this worked best). In the k-\nsparse autoencoder, after training the dictionary, we\nused h = supp\u03b1k(W \u22bax + b) to \ufb01nd the features as ex-\nplained in Section 2.2, where \u03b1 was determined using\nvalidation data. Results for di\ufb00erent architectures are\ncompared in Tables 1, 2. We can see that the perfor-\nmance of our k-sparse autoencoder is better than the\nrest of the algorithms. In our algorithm, the best re-\nsult is achieved by k = 25,\u03b1 = 3 with 1000 hidden units\non MNIST dataset and by k = 150,\u03b1 = 2 with 4000\nhidden units on NORB dataset.\n4.5. Shallow Supervised Learning Results\nIn supervised learning,\nit is a common practice\nto use the encoder weights learnt by an unsuper-\nvised learning method to initialize the early layers\nof a multilayer discriminative model (Erhan et al.,\n2010). The back-propagation algorithm is then used\nk-Sparse Autoencoders\nError\nWithout Pre-Training\n1.60%\nRBM + F.T.\n1.24%\nShallow Dropout AE + F.T.\n1.05%\n(%50 hidden)\nDenoising AE + F.T.\n1.20%\n(%20 input dropout)\nDeep Dropout AE + F.T.\n0.85%\n(Layer-wise pre-training, %50 hidden)\nk-Sparse AE + F.T.\n1.08%\n(k=25)\nDeep k-Sparse AE + F.T.\n0.97%\n(Layer-wise pre-training)\nTable 3. Performance of supervised learning methods on\nMNIST. Pre-training was performed using the correspond-\ning unsupervised learning algorithm with 1000 hidden units,\nand then the model was \ufb01ne-tuned.\nError\nWithout Pre-Training\n12.7%\nDBN\n8.3%\nDBM\n7.2%\nthird-order RBM\n6.5%\nShallow Dropout AE + F.T.\n8.2%\n(%50 hidden)\nShallow Denoising AE + F.T.\n7.9%\n(%20 input dropout)\nDeep Dropout AE + F.T.\n7.0%\n(Layer-wise pre-training, %50 hidden)\nShallow k-Sparse AE + F.T.\n7.8%\n(k=150)\nDeep k-Sparse AE + F.T.\n7.4%\n(k=150, Layer-wise pre-training)\nTable 4. Performance\nof\nsupervised\nlearning\nmethods on NORB. Pre-training was performed\nusing the corresponding unsupervised learning\nalgorithm with 4000 hidden units, and then the\nmodel was \ufb01ne-tuned.\nto adjust the weights of the last hidden layer and\nalso to \ufb01ne-tune the weights in the previous layers.\nThis procedure is often referred to as discriminative\n\ufb01ne-tuning.\nIn this section, we report results us-\ning unsupervised learning algorithms such as RBMs,\nDBNs\n(Salakhutdinov & Larochelle,\n2010),\nDBMs\n(Salakhutdinov & Larochelle, 2010), third-order RBM\n(Nair & Hinton, 2009), dropout autoencoders, denois-\ning autoencoders and k-sparse autoencoders to ini-\ntialize a shallow discriminative neural network for\nthe MNIST and NORB datasets.\nWe used back-\npropagation to \ufb01ne-tune the weights. The regulariza-\ntion method used in the \ufb01ne-tuning stage of di\ufb00erent\nalgorithms is the same as the one used in the train-\ning of the corresponding unsupervised learning task.\nFor instance, we \ufb01ne-tuned the weights obtained from\ndropout autoencoder with dropout regularization or in\ndenoising autoencoder, we \ufb01ne-tuned the discrimina-\ntive neural net by adding noise to the input. In a sim-\nilar manner, in the \ufb01ne-tuning stage of the k-sparse\nautoencoder, we used the \u03b1k largest hidden units in\nthe corresponding discriminative neural network, as\nexplained in Section 2.2. Tables 3 and 4 reports the\nerror rates obtained by di\ufb00erent methods.\n4.6. Deep Supervised Learning Results\nThe k-sparse autoencoder can be used as a building\nblock of a deep neural network, using greedy layer-\nwise pre-training (Bengio et al., 2007). We \ufb01rst train\na shallow k-sparse autoencoder and obtain the hidden\ncodes. We then \ufb01x the features and train another k-\nsparse autoencoder on top of them to obtain another\nset of hidden codes. Then we use the parameters of\nthese autoencoders to initialize a discriminative neural\nnetwork with two hidden layers.\nIn the \ufb01ne-tuning stage of the deep neural net, we \ufb01rst\n\ufb01x the parameters of the \ufb01rst and second layers and\ntrain a softmax classi\ufb01er on top of the second layer.\nWe then hold the weights of the \ufb01rst layer \ufb01xed and\ntrain the second layer and softmax jointly using the\ninitialization of the softmax that we found in the pre-\nvious step. Finally, we jointly \ufb01ne-tune all of the layers\nwith the previous initialization. We have observed that\nthis method of layer-wise \ufb01ne-tuning can improve the\nclassi\ufb01cation performance compared to the case where\nwe \ufb01ne-tune all the layers at the same time.\nIn all of the \ufb01ne-tuning steps, we keep the \u03b1k largest\nhidden codes, where k = 25,\u03b1 = 3 in MNIST and k =\n150,\u03b1 = 2 in NORB in both hidden layers. Tables 3\nand 4 report the classi\ufb01cation results of di\ufb00erent deep\nsupervised learning methods.\n5. Conclusion\nIn this work, we proposed a very fast sparse coding\nmethod called k-sparse autoencoder, which achieves\nexact sparsity in the hidden representation. The main\nmessage of this paper is that we can use the result-\ning representations to achieve state-of-the-art classi\ufb01-\ncation results, solely by enforcing sparsity in the hid-\nden units and without using any other nonlinearity or\nregularization. We also discussed how the k-sparse au-\ntoencoder could be used for pre-training shallow and\nk-Sparse Autoencoders\ndeep supervised architectures.\n6. Acknowledgment\nWe would like to thank Andrew Delong, Babak Ali-\npanahi and Lei Jimmy Ba for the valuable comments.\nReferences\nAharon, Michal, Elad, Michael, and Bruckstein, Al-\nfred. K-svd: Design of dictionaries for sparse repre-\nsentation. Proceedings of SPARS, 5:9\u201312, 2005.\nBengio, Yoshua, Lamblin, Pascal, Popovici, Dan, and\nLarochelle, Hugo. Greedy layer-wise training of deep\nnetworks. Advances in neural information process-\ning systems, 19:153, 2007.\nBlumensath, Thomas and Davies, Mike E. Iterative\nhard thresholding for compressed sensing. Applied\nand Computational Harmonic Analysis, 27(3):265\u2013\n274, 2009.\nCoates, Adam and Ng, Andrew. The importance of\nencoding versus training with sparse coding and vec-\ntor quantization. In Proceedings of the 28th Interna-\ntional Conference on Machine Learning (ICML-11),\npp. 921\u2013928, 2011.\nCoates, Adam, Ng, Andrew Y, and Lee, Honglak. An\nanalysis of single-layer networks in unsupervised fea-\nture learning. In International Conference on Arti-\n\ufb01cial Intelligence and Statistics, pp. 215\u2013223, 2011.\nDonoho, David L and Elad, Michael. Optimally sparse\nrepresentation in general (nonorthogonal) dictionar-\nies via 1 minimization. Proceedings of the National\nAcademy of Sciences, 100(5):2197\u20132202, 2003.\nEngan, Kjersti, Aase, Sven Ole, and Hakon Husoy,\nJ. Method of optimal directions for frame design.\nIn Acoustics, Speech, and Signal Processing, 1999.\nProceedings., 1999 IEEE International Conference\non, volume 5, pp. 2443\u20132446. IEEE, 1999.\nErhan, Dumitru, Bengio, Yoshua, Courville, Aaron,\nManzagol, Pierre-Antoine, Vincent, Pascal, and\nBengio, Samy. Why does unsupervised pre-training\nhelp deep learning? The Journal of Machine Learn-\ning Research, 11:625\u2013660, 2010.\nGregor, Karol and LeCun, Yann.\nLearning fast ap-\nproximations of sparse coding. In Proceedings of the\n27th International Conference on Machine Learning\n(ICML-10), pp. 399\u2013406, 2010.\nHinton, Geo\ufb00rey E, Srivastava, Nitish, Krizhevsky,\nAlex, Sutskever, Ilya, and Salakhutdinov, Rus-\nlan R.\nImproving neural networks by preventing\nco-adaptation of feature detectors. arXiv preprint\narXiv:1207.0580, 2012.\nKavukcuoglu, Koray, Ranzato, Marc\u2019Aurelio, and Le-\nCun, Yann.\nFast inference in sparse coding al-\ngorithms with applications to object recognition.\narXiv preprint arXiv:1010.3467, 2010.\nLeCun, Yann, Huang, Fu Jie, and Bottou, Leon.\nLearning methods for generic object recognition\nwith invariance to pose and lighting. In Computer\nVision and Pattern Recognition, CVPR, volume 2,\npp. II\u201397. IEEE, 2004.\nLee, Honglak, Ekanadham, Chaitanya, and Ng, An-\ndrew. Sparse deep belief net model for visual area\nv2.\nIn Advances in neural information processing\nsystems, pp. 873\u2013880, 2007.\nMaleki, Arian. Coherence analysis of iterative thresh-\nolding algorithms. In Communication, Control, and\nComputing, 2009. Allerton 2009. 47th Annual Aller-\nton Conference on, pp. 236\u2013243. IEEE, 2009.\nNair, Vinod and Hinton, Geo\ufb00rey E. 3d object recog-\nnition with deep belief nets. In Advances in Neu-\nral Information Processing Systems, pp. 1339\u20131347,\n2009.\nOlshausen, Bruno A and Field, David J. Sparse coding\nwith an overcomplete basis set: A strategy employed\nby v1? Vision research, 37(23):3311\u20133325, 1997.\nSalakhutdinov, Ruslan and Larochelle, Hugo.\nE\ufb03-\ncient learning of deep boltzmann machines. In In-\nternational Conference on Arti\ufb01cial Intelligence and\nStatistics, pp. 693\u2013700, 2010.\nTieleman, Tijmen. Gnumpy: an easy way to use gpu\nboards in python. Department of Computer Science,\nUniversity of Toronto, 2010.\nTropp, Joel A and Gilbert, Anna C. Signal recovery\nfrom random measurements via orthogonal match-\ning pursuit.\nInformation Theory, IEEE Transac-\ntions on, 53(12):4655\u20134666, 2007.\nVan Gemert, Jan C, Geusebroek, Jan-Mark, Veenman,\nCor J, and Smeulders, Arnold WM. Kernel code-\nbooks for scene categorization. In Computer Vision\u2013\nECCV 2008, pp. 696\u2013709. Springer, 2008.\nVincent, Pascal, Larochelle, Hugo, Bengio, Yoshua,\nand Manzagol, Pierre-Antoine. Extracting and com-\nposing robust features with denoising autoencoders.\nIn Proceedings of the 25th international conference\non Machine learning, pp. 1096\u20131103. ACM, 2008.\n",
        "sentence": " Several works on obtaining a sparse attention (Martins and Astudillo, 2016; Makhzani and Frey, 2014; Shazeer et al., 2017) share a similar idea of sorting the values before softmax and only keeping theK largest values.",
        "context": "the sparse encoding stage. (For a parallel, distributed\nimplementation, the sorting operation can be replaced\nby a method that recursively applies a threshold until\nk values remain.) We used an e\ufb03cient GPU implemen-\nAbstract\nRecently, it has been observed that when rep-\nresentations are learnt in a way that encour-\nages sparsity, improved performance is ob-\ntained on classi\ufb01cation tasks. These meth-\nods involve combinations of activation func-\nin hidden layers only the k highest activities\nare kept. When applied to the MNIST and\nNORB datasets, we \ufb01nd that this method\nachieves better classi\ufb01cation results than de-\nnoising autoencoders, networks trained with"
    },
    {
        "title": "From softmax to sparsemax: A sparse model of attention and multi-label classification",
        "author": [
            "Andr\u00e9 FT Martins",
            "Ram\u00f3n Fernandez Astudillo."
        ],
        "venue": "Proceedings of the 33th International Conference on Machine Learning.",
        "citeRegEx": "Martins and Astudillo.,? 2016",
        "shortCiteRegEx": "Martins and Astudillo.",
        "year": 2016,
        "abstract": "",
        "full_text": "",
        "sentence": " Several works on obtaining a sparse attention (Martins and Astudillo, 2016; Makhzani and Frey, 2014; Shazeer et al., 2017) share a similar idea of sorting the values before softmax and only keeping theK largest values.",
        "context": null
    },
    {
        "title": "Distributed representations of words and phrases and their compositionality",
        "author": [
            "Tomas Mikolov",
            "Ilya Sutskever",
            "Kai Chen",
            "Greg S Corrado",
            "Jeff Dean."
        ],
        "venue": "Advances in neural information processing systems. pages 3111\u20133119.",
        "citeRegEx": "Mikolov et al\\.,? 2013",
        "shortCiteRegEx": "Mikolov et al\\.",
        "year": 2013,
        "abstract": "The recently introduced continuous Skip-gram model is an efficient method for\nlearning high-quality distributed vector representations that capture a large\nnumber of precise syntactic and semantic word relationships. In this paper we\npresent several extensions that improve both the quality of the vectors and the\ntraining speed. By subsampling of the frequent words we obtain significant\nspeedup and also learn more regular word representations. We also describe a\nsimple alternative to the hierarchical softmax called negative sampling. An\ninherent limitation of word representations is their indifference to word order\nand their inability to represent idiomatic phrases. For example, the meanings\nof \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\".\nMotivated by this example, we present a simple method for finding phrases in\ntext, and show that learning good vector representations for millions of\nphrases is possible.",
        "full_text": "arXiv:1310.4546v1  [cs.CL]  16 Oct 2013\nDistributed Representations of Words and Phrases\nand their Compositionality\nTomas Mikolov\nGoogle Inc.\nMountain View\nmikolov@google.com\nIlya Sutskever\nGoogle Inc.\nMountain View\nilyasu@google.com\nKai Chen\nGoogle Inc.\nMountain View\nkai@google.com\nGreg Corrado\nGoogle Inc.\nMountain View\ngcorrado@google.com\nJeffrey Dean\nGoogle Inc.\nMountain View\njeff@google.com\nAbstract\nThe recently introduced continuous Skip-gram model is an ef\ufb01cient method for\nlearning high-quality distributed vector representations that capture a large num-\nber of precise syntactic and semantic word relationships. In this paper we present\nseveral extensions that improve both the quality of the vectors and the training\nspeed. By subsampling of the frequent words we obtain signi\ufb01cant speedup and\nalso learn more regular word representations. We also describe a simple alterna-\ntive to the hierarchical softmax called negative sampling.\nAn inherent limitation of word representations is their indifference to word order\nand their inability to represent idiomatic phrases. For example, the meanings of\n\u201cCanada\u201d and \u201cAir\u201d cannot be easily combined to obtain \u201cAir Canada\u201d. Motivated\nby this example, we present a simple method for \ufb01nding phrases in text, and show\nthat learning good vector representations for millions of phrases is possible.\n1\nIntroduction\nDistributed representations of words in a vector space help learning algorithms to achieve better\nperformance in natural language processing tasks by grouping similar words. One of the earliest use\nof word representations dates back to 1986 due to Rumelhart, Hinton, and Williams [13]. This idea\nhas since been applied to statistical language modeling with considerable success [1]. The follow\nup work includes applications to automatic speech recognition and machine translation [14, 7], and\na wide range of NLP tasks [2, 20, 15, 3, 18, 19, 9].\nRecently, Mikolov et al. [8] introduced the Skip-gram model, an ef\ufb01cient method for learning high-\nquality vector representations of words from large amounts of unstructured text data. Unlike most\nof the previously used neural network architectures for learning word vectors, training of the Skip-\ngram model (see Figure 1) does not involve dense matrix multiplications. This makes the training\nextremely ef\ufb01cient: an optimized single-machine implementation can train on more than 100 billion\nwords in one day.\nThe word representations computed using neural networks are very interesting because the learned\nvectors explicitly encode many linguistic regularities and patterns. Somewhat surprisingly, many of\nthese patterns can be represented as linear translations. For example, the result of a vector calcula-\ntion vec(\u201cMadrid\u201d) - vec(\u201cSpain\u201d) + vec(\u201cFrance\u201d) is closer to vec(\u201cParis\u201d) than to any other word\nvector [9, 8].\n1\n\u0001\u0002\u0003\u0004\n\u0005\u0006\u0007\b\u0003\t\t\t\t\t\t\t\t\t\t\t\u0007\n\u000b\f\r\u000e\u0003\u000f\u000b\u0006\t\t\t\t\t\t\u000b\b\u0003\u0007\b\u0003\n\u0001\u0002\u0003\u0010\u0011\u0004\n\u0001\u0002\u0003\u0010\u0012\u0004\n\u0001\u0002\u0003\u0013\u0012\u0004\n\u0001\u0002\u0003\u0013\u0011\u0004\nFigure 1: The Skip-gram model architecture. The training objective is to learn word vector representations\nthat are good at predicting the nearby words.\nIn this paper we present several extensions of the original Skip-gram model. We show that sub-\nsampling of frequent words during training results in a signi\ufb01cant speedup (around 2x - 10x), and\nimproves accuracy of the representations of less frequent words. In addition, we present a simpli-\n\ufb01ed variant of Noise Contrastive Estimation (NCE) [4] for training the Skip-gram model that results\nin faster training and better vector representations for frequent words, compared to more complex\nhierarchical softmax that was used in the prior work [8].\nWord representations are limited by their inability to represent idiomatic phrases that are not com-\npositions of the individual words. For example, \u201cBoston Globe\u201d is a newspaper, and so it is not a\nnatural combination of the meanings of \u201cBoston\u201d and \u201cGlobe\u201d. Therefore, using vectors to repre-\nsent the whole phrases makes the Skip-gram model considerably more expressive. Other techniques\nthat aim to represent meaning of sentences by composing the word vectors, such as the recursive\nautoencoders [15], would also bene\ufb01t from using phrase vectors instead of the word vectors.\nThe extension from word based to phrase based models is relatively simple. First we identify a large\nnumber of phrases using a data-driven approach, and then we treat the phrases as individual tokens\nduring the training. To evaluate the quality of the phrase vectors, we developed a test set of analogi-\ncal reasoning tasks that contains both words and phrases. A typical analogy pair from our test set is\n\u201cMontreal\u201d:\u201cMontreal Canadiens\u201d::\u201cToronto\u201d:\u201cToronto Maple Leafs\u201d. It is considered to have been\nanswered correctly if the nearest representation to vec(\u201cMontreal Canadiens\u201d) - vec(\u201cMontreal\u201d) +\nvec(\u201cToronto\u201d) is vec(\u201cToronto Maple Leafs\u201d).\nFinally, we describe another interesting property of the Skip-gram model. We found that simple\nvector addition can often produce meaningful results. For example, vec(\u201cRussia\u201d) + vec(\u201criver\u201d) is\nclose to vec(\u201cVolga River\u201d), and vec(\u201cGermany\u201d) + vec(\u201ccapital\u201d) is close to vec(\u201cBerlin\u201d). This\ncompositionality suggests that a non-obvious degree of language understanding can be obtained by\nusing basic mathematical operations on the word vector representations.\n2\nThe Skip-gram Model\nThe training objective of the Skip-gram model is to \ufb01nd word representations that are useful for\npredicting the surrounding words in a sentence or a document. More formally, given a sequence of\ntraining words w1, w2, w3, . . . , wT , the objective of the Skip-gram model is to maximize the average\nlog probability\n1\nT\nT\nX\nt=1\nX\n\u2212c\u2264j\u2264c,j\u0338=0\nlog p(wt+j|wt)\n(1)\nwhere c is the size of the training context (which can be a function of the center word wt). Larger\nc results in more training examples and thus can lead to a higher accuracy, at the expense of the\n2\ntraining time. The basic Skip-gram formulation de\ufb01nes p(wt+j|wt) using the softmax function:\np(wO|wI) =\nexp\n\u0010\nv\u2032\nwO\n\u22a4vwI\n\u0011\nPW\nw=1 exp\n\u0010\nv\u2032w\n\u22a4vwI\n\u0011\n(2)\nwhere vw and v\u2032\nw are the \u201cinput\u201d and \u201coutput\u201d vector representations of w, and W is the num-\nber of words in the vocabulary. This formulation is impractical because the cost of computing\n\u2207log p(wO|wI) is proportional to W, which is often large (105\u2013107 terms).\n2.1\nHierarchical Softmax\nA computationally ef\ufb01cient approximation of the full softmax is the hierarchical softmax. In the\ncontext of neural network language models, it was \ufb01rst introduced by Morin and Bengio [12]. The\nmain advantage is that instead of evaluating W output nodes in the neural network to obtain the\nprobability distribution, it is needed to evaluate only about log2(W) nodes.\nThe hierarchical softmax uses a binary tree representation of the output layer with the W words as\nits leaves and, for each node, explicitly represents the relative probabilities of its child nodes. These\nde\ufb01ne a random walk that assigns probabilities to words.\nMore precisely, each word w can be reached by an appropriate path from the root of the tree. Let\nn(w, j) be the j-th node on the path from the root to w, and let L(w) be the length of this path, so\nn(w, 1) = root and n(w, L(w)) = w. In addition, for any inner node n, let ch(n) be an arbitrary\n\ufb01xed child of n and let [[x]] be 1 if x is true and -1 otherwise. Then the hierarchical softmax de\ufb01nes\np(wO|wI) as follows:\np(w|wI) =\nL(w)\u22121\nY\nj=1\n\u03c3\n\u0010\n[[n(w, j + 1) = ch(n(w, j))]] \u00b7 v\u2032\nn(w,j)\n\u22a4vwI\n\u0011\n(3)\nwhere \u03c3(x) = 1/(1 + exp(\u2212x)). It can be veri\ufb01ed that PW\nw=1 p(w|wI) = 1. This implies that the\ncost of computing log p(wO|wI) and \u2207log p(wO|wI) is proportional to L(wO), which on average\nis no greater than log W. Also, unlike the standard softmax formulation of the Skip-gram which\nassigns two representations vw and v\u2032\nw to each word w, the hierarchical softmax formulation has\none representation vw for each word w and one representation v\u2032\nn for every inner node n of the\nbinary tree.\nThe structure of the tree used by the hierarchical softmax has a considerable effect on the perfor-\nmance. Mnih and Hinton explored a number of methods for constructing the tree structure and the\neffect on both the training time and the resulting model accuracy [10]. In our work we use a binary\nHuffman tree, as it assigns short codes to the frequent words which results in fast training. It has\nbeen observed before that grouping words together by their frequency works well as a very simple\nspeedup technique for the neural network based language models [5, 8].\n2.2\nNegative Sampling\nAn alternative to the hierarchical softmax is Noise Contrastive Estimation (NCE), which was in-\ntroduced by Gutmann and Hyvarinen [4] and applied to language modeling by Mnih and Teh [11].\nNCE posits that a good model should be able to differentiate data from noise by means of logistic\nregression. This is similar to hinge loss used by Collobert and Weston [2] who trained the models\nby ranking the data above noise.\nWhile NCE can be shown to approximately maximize the log probability of the softmax, the Skip-\ngram model is only concerned with learning high-quality vector representations, so we are free to\nsimplify NCE as long as the vector representations retain their quality. We de\ufb01ne Negative sampling\n(NEG) by the objective\nlog \u03c3(v\u2032\nwO\n\u22a4vwI) +\nk\nX\ni=1\nEwi\u223cPn(w)\nh\nlog \u03c3(\u2212v\u2032\nwi\n\u22a4vwI)\ni\n(4)\n3\n-2\n-1.5\n-1\n-0.5\n 0\n 0.5\n 1\n 1.5\n 2\n-2\n-1.5\n-1\n-0.5\n 0\n 0.5\n 1\n 1.5\n 2\nCountry and Capital Vectors Projected by PCA\nChina\nJapan\nFrance\nRussia\nGermany\nItaly\nSpain\nGreece\nTurkey\nBeijing\nParis\nTokyo\nPoland\nMoscow\nPortugal\nBerlin\nRome\nAthens\nMadrid\nAnkara\nWarsaw\nLisbon\nFigure 2: Two-dimensional PCA projection of the 1000-dimensional Skip-gram vectors of countries and their\ncapital cities. The \ufb01gure illustrates ability of the model to automatically organize concepts and learn implicitly\nthe relationships between them, as during the training we did not provide any supervised information about\nwhat a capital city means.\nwhich is used to replace every log P(wO|wI) term in the Skip-gram objective. Thus the task is to\ndistinguish the target word wO from draws from the noise distribution Pn(w) using logistic regres-\nsion, where there are k negative samples for each data sample. Our experiments indicate that values\nof k in the range 5\u201320 are useful for small training datasets, while for large datasets the k can be as\nsmall as 2\u20135. The main difference between the Negative sampling and NCE is that NCE needs both\nsamples and the numerical probabilities of the noise distribution, while Negative sampling uses only\nsamples. And while NCE approximately maximizes the log probability of the softmax, this property\nis not important for our application.\nBoth NCE and NEG have the noise distribution Pn(w) as a free parameter. We investigated a number\nof choices for Pn(w) and found that the unigram distribution U(w) raised to the 3/4rd power (i.e.,\nU(w)3/4/Z) outperformed signi\ufb01cantly the unigram and the uniform distributions, for both NCE\nand NEG on every task we tried including language modeling (not reported here).\n2.3\nSubsampling of Frequent Words\nIn very large corpora, the most frequent words can easily occur hundreds of millions of times (e.g.,\n\u201cin\u201d, \u201cthe\u201d, and \u201ca\u201d). Such words usually provide less information value than the rare words. For\nexample, while the Skip-gram model bene\ufb01ts from observing the co-occurrences of \u201cFrance\u201d and\n\u201cParis\u201d, it bene\ufb01ts much less from observing the frequent co-occurrences of \u201cFrance\u201d and \u201cthe\u201d, as\nnearly every word co-occurs frequently within a sentence with \u201cthe\u201d. This idea can also be applied\nin the opposite direction; the vector representations of frequent words do not change signi\ufb01cantly\nafter training on several million examples.\nTo counter the imbalance between the rare and frequent words, we used a simple subsampling ap-\nproach: each word wi in the training set is discarded with probability computed by the formula\nP(wi) = 1 \u2212\ns\nt\nf(wi)\n(5)\n4\nMethod\nTime [min]\nSyntactic [%]\nSemantic [%]\nTotal accuracy [%]\nNEG-5\n38\n63\n54\n59\nNEG-15\n97\n63\n58\n61\nHS-Huffman\n41\n53\n40\n47\nNCE-5\n38\n60\n45\n53\nThe following results use 10\u22125 subsampling\nNEG-5\n14\n61\n58\n60\nNEG-15\n36\n61\n61\n61\nHS-Huffman\n21\n52\n59\n55\nTable 1: Accuracy of various Skip-gram 300-dimensional models on the analogical reasoning task\nas de\ufb01ned in [8]. NEG-k stands for Negative Sampling with k negative samples for each positive\nsample; NCE stands for Noise Contrastive Estimation and HS-Huffman stands for the Hierarchical\nSoftmax with the frequency-based Huffman codes.\nwhere f(wi) is the frequency of word wi and t is a chosen threshold, typically around 10\u22125.\nWe chose this subsampling formula because it aggressively subsamples words whose frequency\nis greater than t while preserving the ranking of the frequencies. Although this subsampling for-\nmula was chosen heuristically, we found it to work well in practice. It accelerates learning and even\nsigni\ufb01cantly improves the accuracy of the learned vectors of the rare words, as will be shown in the\nfollowing sections.\n3\nEmpirical Results\nIn this section we evaluate the Hierarchical Softmax (HS), Noise Contrastive Estimation, Negative\nSampling, and subsampling of the training words. We used the analogical reasoning task1 introduced\nby Mikolov et al. [8]. The task consists of analogies such as \u201cGermany\u201d : \u201cBerlin\u201d :: \u201cFrance\u201d : ?,\nwhich are solved by \ufb01nding a vector x such that vec(x) is closest to vec(\u201cBerlin\u201d) - vec(\u201cGermany\u201d)\n+ vec(\u201cFrance\u201d) according to the cosine distance (we discard the input words from the search). This\nspeci\ufb01c example is considered to have been answered correctly if x is \u201cParis\u201d. The task has two\nbroad categories: the syntactic analogies (such as \u201cquick\u201d : \u201cquickly\u201d :: \u201cslow\u201d : \u201cslowly\u201d) and the\nsemantic analogies, such as the country to capital city relationship.\nFor training the Skip-gram models, we have used a large dataset consisting of various news articles\n(an internal Google dataset with one billion words). We discarded from the vocabulary all words\nthat occurred less than 5 times in the training data, which resulted in a vocabulary of size 692K.\nThe performance of various Skip-gram models on the word analogy test set is reported in Table 1.\nThe table shows that Negative Sampling outperforms the Hierarchical Softmax on the analogical\nreasoning task, and has even slightly better performance than the Noise Contrastive Estimation. The\nsubsampling of the frequent words improves the training speed several times and makes the word\nrepresentations signi\ufb01cantly more accurate.\nIt can be argued that the linearity of the skip-gram model makes its vectors more suitable for such\nlinear analogical reasoning, but the results of Mikolov et al. [8] also show that the vectors learned\nby the standard sigmoidal recurrent neural networks (which are highly non-linear) improve on this\ntask signi\ufb01cantly as the amount of the training data increases, suggesting that non-linear models also\nhave a preference for a linear structure of the word representations.\n4\nLearning Phrases\nAs discussed earlier, many phrases have a meaning that is not a simple composition of the mean-\nings of its individual words. To learn vector representation for phrases, we \ufb01rst \ufb01nd words that\nappear frequently together, and infrequently in other contexts. For example, \u201cNew York Times\u201d and\n\u201cToronto Maple Leafs\u201d are replaced by unique tokens in the training data, while a bigram \u201cthis is\u201d\nwill remain unchanged.\n1code.google.com/p/word2vec/source/browse/trunk/questions-words.txt\n5\nNewspapers\nNew York\nNew York Times\nBaltimore\nBaltimore Sun\nSan Jose\nSan Jose Mercury News\nCincinnati\nCincinnati Enquirer\nNHL Teams\nBoston\nBoston Bruins\nMontreal\nMontreal Canadiens\nPhoenix\nPhoenix Coyotes\nNashville\nNashville Predators\nNBA Teams\nDetroit\nDetroit Pistons\nToronto\nToronto Raptors\nOakland\nGolden State Warriors\nMemphis\nMemphis Grizzlies\nAirlines\nAustria\nAustrian Airlines\nSpain\nSpainair\nBelgium\nBrussels Airlines\nGreece\nAegean Airlines\nCompany executives\nSteve Ballmer\nMicrosoft\nLarry Page\nGoogle\nSamuel J. Palmisano\nIBM\nWerner Vogels\nAmazon\nTable 2: Examples of the analogical reasoning task for phrases (the full test set has 3218 examples).\nThe goal is to compute the fourth phrase using the \ufb01rst three. Our best model achieved an accuracy\nof 72% on this dataset.\nThis way, we can form many reasonable phrases without greatly increasing the size of the vocabu-\nlary; in theory, we can train the Skip-gram model using all n-grams, but that would be too memory\nintensive. Many techniques have been previously developed to identify phrases in the text; however,\nit is out of scope of our work to compare them. We decided to use a simple data-driven approach,\nwhere phrases are formed based on the unigram and bigram counts, using\nscore(wi, wj) =\ncount(wiwj) \u2212\u03b4\ncount(wi) \u00d7 count(wj).\n(6)\nThe \u03b4 is used as a discounting coef\ufb01cient and prevents too many phrases consisting of very infre-\nquent words to be formed. The bigrams with score above the chosen threshold are then used as\nphrases. Typically, we run 2-4 passes over the training data with decreasing threshold value, allow-\ning longer phrases that consists of several words to be formed. We evaluate the quality of the phrase\nrepresentations using a new analogical reasoning task that involves phrases. Table 2 shows examples\nof the \ufb01ve categories of analogies used in this task. This dataset is publicly available on the web2.\n4.1\nPhrase Skip-Gram Results\nStarting with the same news data as in the previous experiments, we \ufb01rst constructed the phrase\nbased training corpus and then we trained several Skip-gram models using different hyper-\nparameters. As before, we used vector dimensionality 300 and context size 5. This setting already\nachieves good performance on the phrase dataset, and allowed us to quickly compare the Negative\nSampling and the Hierarchical Softmax, both with and without subsampling of the frequent tokens.\nThe results are summarized in Table 3.\nThe results show that while Negative Sampling achieves a respectable accuracy even with k = 5,\nusing k = 15 achieves considerably better performance. Surprisingly, while we found the Hierar-\nchical Softmax to achieve lower performance when trained without subsampling, it became the best\nperforming method when we downsampled the frequent words. This shows that the subsampling\ncan result in faster training and can also improve accuracy, at least in some cases.\n2code.google.com/p/word2vec/source/browse/trunk/questions-phrases.txt\nMethod\nDimensionality\nNo subsampling [%]\n10\u22125 subsampling [%]\nNEG-5\n300\n24\n27\nNEG-15\n300\n27\n42\nHS-Huffman\n300\n19\n47\nTable 3:\nAccuracies of the Skip-gram models on the phrase analogy dataset. The models were\ntrained on approximately one billion words from the news dataset.\n6\nNEG-15 with 10\u22125 subsampling\nHS with 10\u22125 subsampling\nVasco de Gama\nLingsugur\nItalian explorer\nLake Baikal\nGreat Rift Valley\nAral Sea\nAlan Bean\nRebbeca Naomi\nmoonwalker\nIonian Sea\nRuegen\nIonian Islands\nchess master\nchess grandmaster\nGarry Kasparov\nTable 4: Examples of the closest entities to the given short phrases, using two different models.\nCzech + currency\nVietnam + capital\nGerman + airlines\nRussian + river\nFrench + actress\nkoruna\nHanoi\nairline Lufthansa\nMoscow\nJuliette Binoche\nCheck crown\nHo Chi Minh City\ncarrier Lufthansa\nVolga River\nVanessa Paradis\nPolish zolty\nViet Nam\n\ufb02ag carrier Lufthansa\nupriver\nCharlotte Gainsbourg\nCTK\nVietnamese\nLufthansa\nRussia\nCecile De\nTable 5: Vector compositionality using element-wise addition. Four closest tokens to the sum of two\nvectors are shown, using the best Skip-gram model.\nTo maximize the accuracy on the phrase analogy task, we increased the amount of the training data\nby using a dataset with about 33 billion words. We used the hierarchical softmax, dimensionality\nof 1000, and the entire sentence for the context. This resulted in a model that reached an accuracy\nof 72%. We achieved lower accuracy 66% when we reduced the size of the training dataset to 6B\nwords, which suggests that the large amount of the training data is crucial.\nTo gain further insight into how different the representations learned by different models are, we did\ninspect manually the nearest neighbours of infrequent phrases using various models. In Table 4, we\nshow a sample of such comparison. Consistently with the previous results, it seems that the best\nrepresentations of phrases are learned by a model with the hierarchical softmax and subsampling.\n5\nAdditive Compositionality\nWe demonstrated that the word and phrase representations learned by the Skip-gram model exhibit\na linear structure that makes it possible to perform precise analogical reasoning using simple vector\narithmetics. Interestingly, we found that the Skip-gram representations exhibit another kind of linear\nstructure that makes it possible to meaningfully combine words by an element-wise addition of their\nvector representations. This phenomenon is illustrated in Table 5.\nThe additive property of the vectors can be explained by inspecting the training objective. The word\nvectors are in a linear relationship with the inputs to the softmax nonlinearity. As the word vectors\nare trained to predict the surrounding words in the sentence, the vectors can be seen as representing\nthe distribution of the context in which a word appears. These values are related logarithmically\nto the probabilities computed by the output layer, so the sum of two word vectors is related to the\nproduct of the two context distributions. The product works here as the AND function: words that\nare assigned high probabilities by both word vectors will have high probability, and the other words\nwill have low probability. Thus, if \u201cVolga River\u201d appears frequently in the same sentence together\nwith the words \u201cRussian\u201d and \u201criver\u201d, the sum of these two word vectors will result in such a feature\nvector that is close to the vector of \u201cVolga River\u201d.\n6\nComparison to Published Word Representations\nMany authors who previously worked on the neural network based representations of words have\npublished their resulting models for further use and comparison: amongst the most well known au-\nthors are Collobert and Weston [2], Turian et al. [17], and Mnih and Hinton [10]. We downloaded\ntheir word vectors from the web3. Mikolov et al. [8] have already evaluated these word representa-\ntions on the word analogy task, where the Skip-gram models achieved the best performance with a\nhuge margin.\n3http://metaoptimize.com/projects/wordreprs/\n7\nModel\nRedmond\nHavel\nninjutsu\ngraf\ufb01ti\ncapitulate\n(training time)\nCollobert (50d)\nconyers\nplauen\nreiki\ncheesecake\nabdicate\n(2 months)\nlubbock\ndzerzhinsky\nkohona\ngossip\naccede\nkeene\nosterreich\nkarate\ndioramas\nrearm\nTurian (200d)\nMcCarthy\nJewell\n-\ngun\ufb01re\n-\n(few weeks)\nAlston\nArzu\n-\nemotion\n-\nCousins\nOvitz\n-\nimpunity\n-\nMnih (100d)\nPodhurst\nPontiff\n-\nanaesthetics\nMavericks\n(7 days)\nHarlang\nPinochet\n-\nmonkeys\nplanning\nAgarwal\nRodionov\n-\nJews\nhesitated\nSkip-Phrase\nRedmond Wash.\nVaclav Havel\nninja\nspray paint\ncapitulation\n(1000d, 1 day)\nRedmond Washington\npresident Vaclav Havel\nmartial arts\ngra\ufb01tti\ncapitulated\nMicrosoft\nVelvet Revolution\nswordsmanship\ntaggers\ncapitulating\nTable 6: Examples of the closest tokens given various well known models and the Skip-gram model\ntrained on phrases using over 30 billion training words. An empty cell means that the word was not\nin the vocabulary.\nTo give more insight into the difference of the quality of the learned vectors, we provide empirical\ncomparison by showing the nearest neighbours of infrequent words in Table 6. These examples show\nthat the big Skip-gram model trained on a large corpus visibly outperforms all the other models in\nthe quality of the learned representations. This can be attributed in part to the fact that this model\nhas been trained on about 30 billion words, which is about two to three orders of magnitude more\ndata than the typical size used in the prior work. Interestingly, although the training set is much\nlarger, the training time of the Skip-gram model is just a fraction of the time complexity required by\nthe previous model architectures.\n7\nConclusion\nThis work has several key contributions. We show how to train distributed representations of words\nand phrases with the Skip-gram model and demonstrate that these representations exhibit linear\nstructure that makes precise analogical reasoning possible. The techniques introduced in this paper\ncan be used also for training the continuous bag-of-words model introduced in [8].\nWe successfully trained models on several orders of magnitude more data than the previously pub-\nlished models, thanks to the computationally ef\ufb01cient model architecture. This results in a great\nimprovement in the quality of the learned word and phrase representations, especially for the rare\nentities. We also found that the subsampling of the frequent words results in both faster training\nand signi\ufb01cantly better representations of uncommon words. Another contribution of our paper is\nthe Negative sampling algorithm, which is an extremely simple training method that learns accurate\nrepresentations especially for frequent words.\nThe choice of the training algorithm and the hyper-parameter selection is a task speci\ufb01c decision,\nas we found that different problems have different optimal hyperparameter con\ufb01gurations. In our\nexperiments, the most crucial decisions that affect the performance are the choice of the model\narchitecture, the size of the vectors, the subsampling rate, and the size of the training window.\nA very interesting result of this work is that the word vectors can be somewhat meaningfully com-\nbined using just simple vector addition. Another approach for learning representations of phrases\npresented in this paper is to simply represent the phrases with a single token. Combination of these\ntwo approaches gives a powerful yet simple way how to represent longer pieces of text, while hav-\ning minimal computational complexity. Our work can thus be seen as complementary to the existing\napproach that attempts to represent phrases using recursive matrix-vector operations [16].\nWe made the code for training the word and phrase vectors based on the techniques described in this\npaper available as an open-source project4.\n4code.google.com/p/word2vec\n8\nReferences\n[1] Yoshua Bengio, R\u00b4ejean Ducharme, Pascal Vincent, and Christian Janvin. A neural probabilistic language\nmodel. The Journal of Machine Learning Research, 3:1137\u20131155, 2003.\n[2] Ronan Collobert and Jason Weston. A uni\ufb01ed architecture for natural language processing: deep neu-\nral networks with multitask learning. In Proceedings of the 25th international conference on Machine\nlearning, pages 160\u2013167. ACM, 2008.\n[3] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Domain adaptation for large-scale sentiment classi-\n\ufb01cation: A deep learning approach. In ICML, 513\u2013520, 2011.\n[4] Michael U Gutmann and Aapo Hyv\u00a8arinen. Noise-contrastive estimation of unnormalized statistical mod-\nels, with applications to natural image statistics. The Journal of Machine Learning Research, 13:307\u2013361,\n2012.\n[5] Tomas Mikolov, Stefan Kombrink, Lukas Burget, Jan Cernocky, and Sanjeev Khudanpur. Extensions of\nrecurrent neural network language model. In Acoustics, Speech and Signal Processing (ICASSP), 2011\nIEEE International Conference on, pages 5528\u20135531. IEEE, 2011.\n[6] Tomas Mikolov, Anoop Deoras, Daniel Povey, Lukas Burget and Jan Cernocky. Strategies for Training\nLarge Scale Neural Network Language Models. In Proc. Automatic Speech Recognition and Understand-\ning, 2011.\n[7] Tomas Mikolov. Statistical Language Models Based on Neural Networks. PhD thesis, PhD Thesis, Brno\nUniversity of Technology, 2012.\n[8] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Ef\ufb01cient estimation of word representations\nin vector space. ICLR Workshop, 2013.\n[9] Tomas Mikolov, Wen-tau Yih and Geoffrey Zweig. Linguistic Regularities in Continuous Space Word\nRepresentations. In Proceedings of NAACL HLT, 2013.\n[10] Andriy Mnih and Geoffrey E Hinton. A scalable hierarchical distributed language model. Advances in\nneural information processing systems, 21:1081\u20131088, 2009.\n[11] Andriy Mnih and Yee Whye Teh. A fast and simple algorithm for training neural probabilistic language\nmodels. arXiv preprint arXiv:1206.6426, 2012.\n[12] Frederic Morin and Yoshua Bengio. Hierarchical probabilistic neural network language model. In Pro-\nceedings of the international workshop on arti\ufb01cial intelligence and statistics, pages 246\u2013252, 2005.\n[13] David E Rumelhart, Geoffrey E Hintont, and Ronald J Williams. Learning representations by back-\npropagating errors. Nature, 323(6088):533\u2013536, 1986.\n[14] Holger Schwenk. Continuous space language models. Computer Speech and Language, vol. 21, 2007.\n[15] Richard Socher, Cliff C. Lin, Andrew Y. Ng, and Christopher D. Manning. Parsing natural scenes and\nnatural language with recursive neural networks. In Proceedings of the 26th International Conference on\nMachine Learning (ICML), volume 2, 2011.\n[16] Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. Semantic Compositionality\nThrough Recursive Matrix-Vector Spaces. In Proceedings of the 2012 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), 2012.\n[17] Joseph Turian, Lev Ratinov, and Yoshua Bengio. Word representations: a simple and general method for\nsemi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computa-\ntional Linguistics, pages 384\u2013394. Association for Computational Linguistics, 2010.\n[18] Peter D. Turney and Patrick Pantel. From frequency to meaning: Vector space models of semantics. In\nJournal of Arti\ufb01cial Intelligence Research, 37:141-188, 2010.\n[19] Peter D. Turney. Distributional semantics beyond words: Supervised learning of analogy and paraphrase.\nIn Transactions of the Association for Computational Linguistics (TACL), 353\u2013366, 2013.\n[20] Jason Weston, Samy Bengio, and Nicolas Usunier. Wsabie: Scaling up to large vocabulary image annota-\ntion. In Proceedings of the Twenty-Second international joint conference on Arti\ufb01cial Intelligence-Volume\nVolume Three, pages 2764\u20132770. AAAI Press, 2011.\n9\n",
        "sentence": " Motivated by the linear translation phenomenon observed in well trained word embeddings (Mikolov et al., 2013), TransE (Bordes et al.",
        "context": "University of Technology, 2012.\n[8] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Ef\ufb01cient estimation of word representations\nin vector space. ICLR Workshop, 2013.\nlinear analogical reasoning, but the results of Mikolov et al. [8] also show that the vectors learned\nby the standard sigmoidal recurrent neural networks (which are highly non-linear) improve on this\ntask signi\ufb01cantly as the amount of the training data increases, suggesting that non-linear models also\nhave a preference for a linear structure of the word representations.\n4\nLearning Phrases"
    },
    {
        "title": "Distant supervision for relation extraction without labeled data",
        "author": [
            "Mike Mintz",
            "Steven Bills",
            "Rion Snow",
            "Daniel Jurafsky."
        ],
        "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on",
        "citeRegEx": "Mintz et al\\.,? 2009",
        "shortCiteRegEx": "Mintz et al\\.",
        "year": 2009,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " , 2016) and information extraction (Mintz et al., 2009).",
        "context": null
    },
    {
        "title": "Neighborhood mixture model for knowledge base completion",
        "author": [
            "Dat Quoc Nguyen",
            "Kairit Sirts",
            "Lizhen Qu",
            "Mark Johnson."
        ],
        "venue": "Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning (CoNLL). Association for Com-",
        "citeRegEx": "Nguyen et al\\.,? 2016a",
        "shortCiteRegEx": "Nguyen et al\\.",
        "year": 2016,
        "abstract": "Knowledge bases are useful resources for many natural language processing\ntasks, however, they are far from complete. In this paper, we define a novel\nentity representation as a mixture of its neighborhood in the knowledge base\nand apply this technique on TransE-a well-known embedding model for knowledge\nbase completion. Experimental results show that the neighborhood information\nsignificantly helps to improve the results of the TransE model, leading to\nbetter performance than obtained by other state-of-the-art embedding models on\nthree benchmark datasets for triple classification, entity prediction and\nrelation prediction tasks.",
        "full_text": "Neighborhood Mixture Model for Knowledge Base Completion\u2217\nDat Quoc Nguyen1, Kairit Sirts1, Lizhen Qu2 and Mark Johnson1\n1 Department of Computing, Macquarie University, Sydney, Australia\ndat.nguyen@students.mq.edu.au, {kairit.sirts, mark.johnson}@mq.edu.au\n2 Data61 & Australian National University\nlizhen.qu@data61.csiro.au\nAbstract\nKnowledge bases are useful resources for\nmany natural language processing tasks,\nhowever, they are far from complete. In\nthis paper, we de\ufb01ne a novel entity rep-\nresentation as a mixture of its neighbor-\nhood in the knowledge base and apply\nthis technique on TransE\u2014a well-known\nembedding model for knowledge base\ncompletion.\nExperimental results show\nthat the neighborhood information signi\ufb01-\ncantly helps to improve the results of the\nTransE model, leading to better perfor-\nmance than obtained by other state-of-the-\nart embedding models on three benchmark\ndatasets for triple classi\ufb01cation, entity pre-\ndiction and relation prediction tasks.\nKeywords: Knowledge base completion,\nembedding model, mixture model, link\nprediction, triple classi\ufb01cation, entity pre-\ndiction, relation prediction.\n1\nIntroduction\nKnowledge bases (KBs), such as WordNet (Miller,\n1995), YAGO (Suchanek et al., 2007), Freebase\n(Bollacker et al., 2008) and DBpedia (Lehmann\net al., 2015), represent relationships between enti-\nties as triples (head entity, relation, tail entity).\nEven very large knowledge bases are still far from\ncomplete (Socher et al., 2013; West et al., 2014).\nKnowledge base completion or link prediction sys-\ntems (Nickel et al., 2016a) predict which triples\nnot in a knowledge base are likely to be true\n(Taskar et al., 2004; Bordes et al., 2011).\nEmbedding models for KB completion associate\nentities and/or relations with dense feature vec-\ntors or matrices. Such models obtain state-of-the-\n\u2217A revised version of our CoNLL 2016 paper to update\nlatest related work.\nart performance (Bordes et al., 2013; Wang et al.,\n2014; Guu et al., 2015; Xiao et al., 2016a; Nguyen\net al., 2016; Das et al., 2017) and generalize to\nlarge KBs (Krompa\u00df et al., 2015).\nMost embedding models for KB completion\nlearn only from triples and by doing so, ignore lots\nof information implicitly provided by the structure\nof the knowledge graph. Recently, several authors\nhave addressed this issue by incorporating relation\npath information into model learning (Lin et al.,\n2015a; Guu et al., 2015; Toutanova et al., 2016)\nand have shown that the relation paths between\nentities in KBs provide useful information and im-\nprove knowledge base completion. For instance, a\nthree-relation path\n(head, born in hospital/r1, e1)\n\u21d2(e1, hospital located in city/r2, e2)\n\u21d2(e2, city in country/r3, tail)\nis\nlikely\nto\nindicate\nthat\nthe\nfact\n(head, nationality, tail)\ncould\nbe\ntrue,\nso\nthe relation path here p = {r1, r2, r3} is useful for\npredicting the relationship \u201cnationality\u201d between\nthe head and tail entities.\nBesides the relation paths, there could be other\nuseful information implicitly presented in the\nknowledge base that could be exploited for better\nKB completion. For instance, the whole neigh-\nborhood of entities could provide lots of useful in-\nformation for predicting the relationship between\ntwo entities. Consider for example a KB fragment\ngiven in Figure 1. If we know that Ben Af\ufb02eck has\nwon an Oscar award and Ben Af\ufb02eck lives in Los\nAngeles, then this can help us to predict that Ben\nAf\ufb02eck is an actor or a \ufb01lm maker, rather than a\nlecturer or a doctor. If we additionally know that\nBen Af\ufb02eck\u2019s gender is male then there is a higher\nchance for him to be a \ufb01lm maker. This intuition\ncan be formalized by representing an entity vector\narXiv:1606.06461v3  [cs.CL]  9 Mar 2017\nBen Af\ufb02eck\nmale\ngender\nactor\noccupation?\n\ufb01lm maker\noccupation?\nOscar award\nwon\nLos Angeles\nlive in\nlecturer\noccupation?\nViolet Anne\nchild of\nFigure 1: An example fragment of a KB.\nas a relation-speci\ufb01c mixture of its neighborhood\nas follows:\nBen A\ufb04eck = \u03c9r,1(Violet Anne, child of)\n+ \u03c9r,2(male, gender\u22121)\n+ \u03c9r,3(Los Angeles, lives in\u22121)\n+ \u03c9r,4(Oscar award, won\u22121),\nwhere \u03c9r,i are the mixing weights that indicate\nhow important each neighboring relation is for\npredicting the relation r. For example, for pre-\ndicting the occupation relationship, the knowl-\nedge about the child of relationship might not be\nthat informative and thus the corresponding mix-\ning coef\ufb01cient can be close to zero, whereas it\ncould be relevant for predicting some other re-\nlationship, such as parent or spouse, in which\ncase the relation-speci\ufb01c mixing coef\ufb01cient for the\nchild of relationship could be high.\nThe primary contribution of this paper is intro-\nducing and formalizing the neighborhood mixture\nmodel. We demonstrate its usefulness by apply-\ning it to the well-known TransE model (Bordes et\nal., 2013). However, it could be applied to other\nembedding models as well, such as Bilinear mod-\nels (Bordes et al., 2012; Yang et al., 2015) and\nSTransE (Nguyen et al., 2016).\nWhile relation\npath models exploit extra information using longer\npaths existing in the KB, the neighborhood mix-\nture model effectively incorporates information\nabout many paths simultaneously. Our extensive\nexperiments on three benchmark datasets show\nthat it achieves superior performance over com-\npetitive baselines in three KB completion tasks:\ntriple classi\ufb01cation, entity prediction and relation\nprediction.\n2\nNeighborhood mixture modeling\nIn this section, we start by explaining how to\nformally construct the neighbor-based entity rep-\nresentations in section 2.1, and then describe\nthe Neighborhood Mixture Model applied to the\nTransE model (Bordes et al., 2013) in section 2.2.\nSection 2.3 explains how we train our model.\n2.1\nNeighbor-based entity representation\nLet E denote the set of entities and R the set of\nrelation types. Denote by R\u22121 the set of inverse\nrelations r\u22121. Denote by G the knowledge graph\nconsisting of a set of correct tiples (h, r, t), such\nthat h, t \u2208E and r \u2208R. Let K denote the sym-\nmetric closure of G, i.e. if a triple (h, r, t) \u2208G,\nthen both (h, r, t) and (t, r\u22121, h) \u2208K.\nDe\ufb01ne:\nNe,r = {e\u2032|(e\u2032, r, e) \u2208K}\nas a set of neighboring entities connected to entity\ne with relation r. Then\nNe = {(e\u2032, r)|r \u2208R \u222aR\u22121, e\u2032 \u2208Ne,r}\nis the set of all entity and relation pairs that are\nneighbors for entity e.\nEach entity e is associated with a k-dimensional\nvector ve \u2208Rk and relation-dependent vectors\nue,r \u2208Rk, r \u2208R \u222aR\u22121. Now we can de\ufb01ne the\nneighborhood-based entity representation \u03d1e,r for\nan entity e \u2208E for predicting the relation r \u2208R\nas follows:\n\u03d1e,r = aeve +\nX\n(e\u2032,r\u2032)\u2208Ne\nbr,r\u2032ue\u2032,r\u2032,\n(1)\nae and br,r\u2032 are the mixture weights that are con-\nstrained to sum to 1 for each neighborhood:\nae \u221d\u03b4 + exp \u03b1e\n(2)\nbr,r\u2032 \u221dexp \u03b2r,r\u2032\n(3)\nwhere \u03b4 \u2a7e0 is a hyper-parameter that controls\nthe contribution of the entity vector ve to the\nneighbor-based mixture, \u03b1e and \u03b2r,r\u2032 are the learn-\nable exponential mixture parameters.\nIn real-world factual KBs, e.g. Freebase (Bol-\nlacker et al., 2008), some entities, such as \u201cmale\u201d,\ncan have thousands or millions neighboring enti-\nties sharing the same relation \u201cgender.\u201d For such\nentities, computing the neighbor-based vectors can\nbe computationally very expensive. To overcome\nthis problem, we introduce in our implementa-\ntion a \ufb01ltering threshold \u03c4 and consider in the\nneighbor-based entity representation construction\nonly those relation-speci\ufb01c neighboring entity sets\nfor which |Ne,r| \u2264\u03c4.\n2.2\nTransE-NMM: applying neighborhood\nmixtures to TransE\nEmbedding\nmodels\nde\ufb01ne\nfor\neach\ntriple\n(h, r, t)\n\u2208\nG, a score function f(h, r, t) that\nmeasures its implausibility. The goal is to choose\nf such that the score f(h, r, t) of a plausible triple\n(h, r, t) is smaller than the score f(h\u2032, r\u2032, t\u2032) of an\nimplausible triple (h\u2032, r\u2032, t\u2032).\nTransE (Bordes et al., 2013) is a simple em-\nbedding model for knowledge base completion,\nwhich, despite of its simplicity, obtains very com-\npetitive results (Garc\u00b4\u0131a-Dur\u00b4an et al., 2016; Nickel\net al., 2016b). In TransE, both entities e and rela-\ntions r are represented with k-dimensional vectors\nve \u2208Rk and vr \u2208Rk, respectively. These vectors\nare chosen such that for each triple (h, r, t) \u2208G:\nvh + vr \u2248vt\n(4)\nThe score function of the TransE model is the\nnorm of this translation:\nf(h, r, t)TransE = \u2225vh + vr \u2212vt\u2225\u21131/2\n(5)\nWe de\ufb01ne the score function of our new model\nTransE-NMM in terms of the neighbor-based en-\ntity vectors as follows:\nf(h, r, t) = \u2225\u03d1h,r + vr \u2212\u03d1t,r\u22121\u2225\u21131/2,\n(6)\nusing either the \u21131 or the \u21132-norm, and \u03d1h,r and\n\u03d1t,r\u22121 are de\ufb01ned following the Equation 1. The\nrelation-speci\ufb01c entity vectors ue,r used to con-\nstruct the neighbor-based entity vectors \u03d1e,r are\nde\ufb01ned based on the TransE translation operator:\nue,r = ve + vr\n(7)\nin which vr\u22121 = \u2212vr. For each correct triple\n(h, r, t), the sets of neighboring entities Nh,r and\nNt,r\u22121 exclude the entities t and h, respectively.\nIf we set the \ufb01ltering threshold \u03c4 = 0 then\n\u03d1h,r = vh and \u03d1t,r\u22121 = vt for all triples. In this\ncase, TransE-NMM reduces to the plain TransE\nmodel. In all our experiments presented in section\n4, the baseline TransE results are obtained with the\nTransE-NMM with \u03c4 = 0.\n2.3\nParameter optimization\nThe TransE-NMM model parameters include the\nvectors ve, vr for entities and relation types, the\nentity-speci\ufb01c weights \u03b1 = {\u03b1e|e \u2208E} and\nrelation-speci\ufb01c weights \u03b2 = {\u03b2r,r\u2032|r, r\u2032 \u2208R \u222a\nR\u22121}. To learn these parameters, we minimize the\nL2-regularized margin-based objective function:\nL =\nX\n(h,r,t)\u2208G\n(h\u2032,r,t\u2032)\u2208G\u2032\n(h,r,t)\n[\u03b3 + f(h, r, t) \u2212f(h\u2032, r, t\u2032)]+\n+ \u03bb\n2\n\u0010\n\u2225\u03b1\u22252\n2 + \u2225\u03b2\u22252\n2\n\u0011\n,\n(8)\nwhere [x]+ = max(0, x), \u03b3 is the margin hyper-\nparameter, \u03bb is the L2 regularization parameter\nand\nG\u2032\n(h,r,t) = {(h\u2032, r, t) | h\u2032 \u2208E, (h\u2032, r, t) /\u2208G}\n\u222a{(h, r, t\u2032) | t\u2032 \u2208E, (h, r, t\u2032) /\u2208G}\nis the set of incorrect triples generated by corrupt-\ning the correct triple (h, r, t) \u2208G. We applied\nthe \u201cBernoulli\u201d trick to choose whether to gener-\nate the head or tail entities when sampling incor-\nrect triples (Wang et al., 2014; Lin et al., 2015b;\nHe et al., 2015; Ji et al., 2015; Ji et al., 2016).\nWe use Stochastic Gradient Descent (SGD)\nwith RMSProp adaptive learning rate to minimize\nL, and impose the following hard constraints dur-\ning training: \u2225ve\u22252 \u2a7d1 and \u2225vr\u22252 \u2a7d1. We em-\nploy alternating optimization to minimize L. We\n\ufb01rst initialize the entity and relation-speci\ufb01c mix-\ning parameters \u03b1 and \u03b2 to zero and only learn the\nrandomly initialized entity and relation vectors ve\nand vr. Then we \ufb01x the learned vectors and only\noptimize the mixing parameters. In the \ufb01nal step,\nwe \ufb01x again the mixing parameters and \ufb01ne-tune\nthe vectors. In all experiments presented in sec-\ntion 4, we train for 200 epochs during each three\noptimization step.\n3\nRelated work\nTable 1 summarizes related embedding models for\nlink prediction and KB completion. The models\nModel\nScore function f(h, r, t)\nOpt.\nSTransE\n\u2225Wr,1vh + vr \u2212Wr,2vt\u2225\u21131/2 ; Wr,1, Wr,2 \u2208Rk\u00d7k; vr \u2208Rk\nSGD\nSE\n\u2225Wr,1vh \u2212Wr,2vt\u2225\u21131/2 ; Wr,1, Wr,2 \u2208Rk\u00d7k\nSGD\nUnstructured\n\u2225vh \u2212vt\u2225\u21131/2\nSGD\nTransE\n\u2225vh + vr \u2212vt\u2225\u21131/2 ; vr \u2208Rk\nSGD\nTransH\n\u2225(I \u2212rpr\u22a4\np )vh + vr \u2212(I \u2212rpr\u22a4\np )vt\u2225\u21131/2\nSGD\nrp, vr \u2208Rk ; I: Identity matrix size k \u00d7 k\nTransD\n\u2225(I + rph\u22a4\np )vh + vr \u2212(I + rpt\u22a4\np )vt\u2225\u21131/2\nAdaDelta\nrp, vr \u2208Rn ; hp, tp \u2208Rk ; I: Identity matrix size n \u00d7 k\nTransR\n\u2225Wrvh + vr \u2212Wrvt\u2225\u21131/2 ; Wr \u2208Rn\u00d7k ; vr \u2208Rn\nSGD\nTranSparse\n\u2225Wh\nr(\u03b8h\nr )vh + vr \u2212Wt\nr(\u03b8t\nr)vt\u2225\u21131/2 ; Wh\nr, Wt\nr \u2208Rn\u00d7k; \u03b8h\nr , \u03b8t\nr \u2208R ; vr \u2208Rn\nSGD\nSME\n(W1,1vh + W1,2vr + b1)\u22a4(W2,1vt + W2,2vr + b2)\nSGD\nb1, b2 \u2208Rn; W1,1, W1,2, W2,1, W2,2 \u2208Rn\u00d7k\nDISTMULT\nv\u22a4\nh Wrvt ; Wr is a diagonal matrix \u2208Rk\u00d7k\nAdaGrad\nNTN\nv\u22a4\nr tanh(v\u22a4\nh Mrvt + Wr,1vh + Wr,2vt + br)\nL-BFGS\nvr, br \u2208Rn; Mr \u2208Rk\u00d7k\u00d7n; Wr,1, Wr,2 \u2208Rn\u00d7k\nBilinear-COMP\nv\u22a4\nh Wr1Wr2...Wrmvt ; Wr1, Wr2, ..., Wrm \u2208Rk\u00d7k\nAdaGrad\nTransE-COMP\n\u2225vh + vr1 + vr2 + ... + vrm \u2212vt\u2225\u21131/2 ; vr1, vr2, ..., vrm \u2208Rk\nAdaGrad\nTable 1: The score functions f(h, r, t) and the optimization methods (Opt.) of several prominent embed-\nding models for KB completion. In all of these models, the entities h and t are represented by vectors vh\nand vt \u2208Rk respectively.\ndiffer in their score function f(h, r, t) and the al-\ngorithm used to optimize their margin-based ob-\njective function, e.g., SGD, AdaGrad (Duchi et al.,\n2011), AdaDelta (Zeiler, 2012) or L-BFGS (Liu\nand Nocedal, 1989).\nThe Unstructured model (Bordes et al., 2012)\nassumes that the head and tail entity vectors are\nsimilar. As the Unstructured model does not take\nthe relationship into account, it cannot distinguish\ndifferent relation types. The Structured Embed-\nding (SE) model (Bordes et al., 2011) extends the\nUnstructured model by assuming that the head and\ntail entities are similar only in a relation-dependent\nsubspace, where each relation is represented by\ntwo different matrices.\nFuthermore, the SME\nmodel (Bordes et al., 2012) uses four different ma-\ntrices to project entity and relation vectors into a\nsubspace. The TransH model (Wang et al., 2014)\nassociates each relation with a relation-speci\ufb01c\nhyperplane and uses a projection vector to project\nentity vectors onto that hyperplane. TransD (Ji et\nal., 2015) and TransR/CTransR (Lin et al., 2015b)\nextend the TransH model by using two projection\nvectors and a matrix to project entity vectors into\na relation-speci\ufb01c space, respectively. TEKE H\n(Wang and Li, 2016) extends TransH to incorpo-\nrate rich context information in an external text\ncorpus. lppTransD (Yoon et al., 2016) extends the\nTransD model to additionally use two projection\nvectors for representing each relation.\nSTransE\n(Nguyen et al., 2016) and TranSparse (Ji et al.,\n2016) can be also viewed as extensions of the\nTransR model, where head and tail entities are as-\nsociated with their own projection matrices.\nThe DISTMULT model (Yang et al., 2015) is\nbased on the Bilinear model (Nickel et al., 2011;\nBordes et al., 2012; Jenatton et al., 2012) where\neach relation is represented by a diagonal rather\nthan a full matrix.\nThe neural tensor network\n(NTN) model (Socher et al., 2013) uses a bi-\nlinear tensor operator to represent each relation\nwhile the ProjE model (Shi and Weninger, 2017)\ncould be viewed as a simpli\ufb01ed version of NTN\nwith diagonal matrices. Quadratic forms are also\nused to model entities and relations in KG2E (He\net al., 2015), ComplEx (Trouillon et al., 2016),\nTATEC (Garc\u00b4\u0131a-Dur\u00b4an et al., 2016) and RSTE\n(Tay et al., 2017). In addition, HolE (Nickel et\nal., 2016b) uses circular correlation\u2014a composi-\ntional operator\u2014which could be interpreted as a\ncompression of the tensor product.\nRecently, several authors have shown that rela-\ntion paths between entities in KBs provide richer\ninformation and improve the relationship predic-\ntion (Neelakantan et al., 2015; Lin et al., 2015a;\nGarc\u00b4\u0131a-Dur\u00b4an et al., 2015; Guu et al., 2015; Wang\net al., 2016; Feng et al., 2016b; Liu et al., 2016;\nNiepert, 2016; Wei et al., 2016; Toutanova et al.,\n2016). In fact, our TransE-NMM model can be\nalso viewed as a three-relation path model as it\ntakes into account the neighborhood entity and re-\nlation information of both head and tail entities in\neach triple.\nLuo et al. (2015) constructed relation paths be-\ntween entities and viewing entities and relations\nin the path as pseudo-words applied Word2Vec al-\ngorithms (Mikolov et al., 2013) to produce pre-\ntrained vectors for these pseudo-words. Luo et al.\n(2015) showed that using these pre-trained vectors\nfor initialization helps to improve the performance\nof the TransE, SME and SE models.\nRTransE\n(Garc\u00b4\u0131a-Dur\u00b4an et al., 2015), PTransE (Lin et al.,\n2015a) and TransE-COMP (Guu et al., 2015) are\nextensions of the TransE model.\nThese mod-\nels similarly represent a relation path by a vec-\ntor which is the sum of the vectors of all relations\nin the path, whereas in the Bilinear-COMP model\n(Guu et al., 2015), each relation is a matrix and\nso it represents the relation path by matrix multi-\nplication. Our neighborhood mixture model can\nbe adapted to both relation path models Bilinear-\nCOMP and TransE-COMP, by replacing head and\ntail entity vectors by the neighbor-based vec-\ntor representations, thus combining advantages of\nboth path and neighborhood information. Nickel\net al. (2016a) reviews other approaches for learn-\ning from KBs and multi-relational data.\n4\nExperiments\nTo investigate the usefulness of the neighbor mix-\ntures, we compare the performance of the TransE-\nNMM against the results of the baseline TransE\nand other state-of-the-art embedding models on\nthe triple classi\ufb01cation, entity prediction and re-\nlation prediction tasks.\n4.1\nDatasets\nWe conduct experiments using three publicly\navailable datasets WN11, FB13 and NELL186.\nFor all of them, the validation and test sets con-\ntaining both correct and incorrect triples have al-\nready been constructed.\nStatistical information\nabout these datasets is given in Table 2.\nThe two benchmark datasets1,\nWN11 and\n1http://cs.stanford.edu/people/danqi/data/nips13-dataset.tar.bz2\nDataset:\nWN11\nFB13\nNELL186\n#R\n11\n13\n186\n#E\n38,696\n75,043\n14,463\n#Train\n112,581\n316,232\n31,134\n#Valid\n2,609\n5,908\n5,000\n#Test\n10,544\n23,733\n5,000\nTable 2: Statistics of the experimental datasets\nused in this study (and previous works).\n#E is\nthe number of entities, #R is the number of rela-\ntion types, and #Train, #Valid and #Test are the\nnumbers of correct triples in the training, valida-\ntion and test sets, respectively. Each validation and\ntest set also contains the same number of incorrect\ntriples as the number of correct triples.\nFB13, were produced by Socher et al. (2013)\nfor triple classi\ufb01cation.\nWN11 is derived from\nthe large lexical KB WordNet (Miller, 1995) in-\nvolving 11 relation types. FB13 is derived from\nthe large real-world fact KB FreeBase (Bollacker\net al., 2008) covering 13 relation types.\nThe\nNELL186 dataset2 was introduced by Guo et al.\n(2015) for both triple classi\ufb01cation and entity pre-\ndiction tasks, containing 186 most frequent rela-\ntions in the KB of the CMU Never Ending Lan-\nguage Learning project (Carlson et al., 2010).\n4.2\nEvaluation tasks\nWe evaluate our model on three commonly used\nbenchmark tasks: triple classi\ufb01cation, entity pre-\ndiction and relation prediction. This subsection\ndescribes those tasks in detail.\nTriple classi\ufb01cation:\nThe triple classi\ufb01cation\ntask was \ufb01rst introduced by Socher et al. (2013),\nand since then it has been used to evaluate vari-\nous embedding models. The aim of the task is to\npredict whether a triple (h, r, t) is correct or not.\nFor classi\ufb01cation, we set a relation-speci\ufb01c\nthreshold \u03b8r for each relation type r. If the im-\nplausibility score of an unseen test triple (h, r, t)\nis smaller than \u03b8r then the triple will be classi\ufb01ed\nas correct, otherwise incorrect. Following Socher\net al. (2013), the relation-speci\ufb01c thresholds are\ndetermined by maximizing the micro-averaged ac-\ncuracy, which is a per-triple average, on the vali-\ndation set. We also report the macro-averaged ac-\ncuracy, which is a per-relation average.\n2http://aclweb.org/anthology/attachments/P/P15/\nP15-1009.Datasets.zip\nEntity prediction:\nThe entity prediction task\n(Bordes et al., 2013) predicts the head or the tail\nentity given the relation type and the other en-\ntity, i.e. predicting h given (?, r, t) or predicting\nt given (h, r, ?) where ? denotes the missing el-\nement. The results are evaluated using a ranking\ninduced by the function f(h, r, t) on test triples.\nNote that the incorrect triples in the validation and\ntest sets are not used for evaluating the entity pre-\ndiction task nor the relation prediction task.\nEach correct test triple (h, r, t) is corrupted by\nreplacing either its head or tail entity by each of\nthe possible entities in turn, and then we rank these\ncandidates in ascending order of their implausi-\nbility score. This is called as the \u201cRaw\u201d setting\nprotocol. For the \u201cFiltered\u201d setting protocol de-\nscribed in Bordes et al. (2013), we also \ufb01lter out\nbefore ranking any corrupted triples that appear in\nthe KB. Ranking a corrupted triple appearing in\nthe KB (i.e. a correct triple) higher than the origi-\nnal test triple is also correct, but is penalized by the\n\u201cRaw\u201d score, thus the \u201cFiltered\u201d setting provides\na clearer view on the ranking performance.\nIn addition to the mean rank and the Hits@10\n(i.e., the proportion of test triples for which the\ntarget entity was ranked in the top 10 predictions),\nwhich were originally used in the entity predic-\ntion task (Bordes et al., 2013), we also report the\nmean reciprocal rank (MRR), which is commonly\nused in information retrieval. In both \u201cRaw\u201d and\n\u201cFiltered\u201d settings, mean rank is always greater\nor equal to 1 and lower mean rank indicates bet-\nter entity prediction performance. The MRR and\nHits@10 scores always range from 0.0 to 1.0, and\nhigher score re\ufb02ects better prediction result.\nRelation prediction:\nThe relation prediction\ntask (Lin et al., 2015a) predicts the relation type\ngiven the head and tail entities, i.e. predicting r\ngiven (h, ?, t) where ? denotes the missing ele-\nment. We corrupt each correct test triple (h, r, t)\nby replacing its relation r by each possible rela-\ntion type in turn, and then rank these candidates in\nascending order of their implausibility score. Just\nas in the entity prediction task, we use two setting\nprotocols, \u201cRaw\u201d and \u201cFiltered\u201d, and evaluate on\nmean rank, MRR and Hits@10.\n4.3\nHyper-parameter tuning\nFor all evaluation tasks, results for TransE are ob-\ntained with TransE-NMM with the \ufb01ltering thresh-\nold \u03c4 = 0, while we set \u03c4 = 10 for TransE-NMM.\nFor triple classi\ufb01cation, we \ufb01rst performed\na grid search to choose the optimal hyper-\nparameters for TransE by monitoring the micro-\naveraged triple classi\ufb01cation accuracy after each\ntraining epoch on the validation set.\nFor all\ndatasets, we chose either the \u21131 or \u21132 norm in the\nscore function f and the initial RMSProp learning\nrate \u03b7 \u2208{0.001, 0.01}. Following the previous\nwork (Wang et al., 2014; Lin et al., 2015b; Ji et al.,\n2015; He et al., 2015; Ji et al., 2016), we selected\nthe margin hyper-parameter \u03b3 \u2208{1, 2, 4} and the\nnumber of vector dimensions k \u2208{20, 50, 100}\non WN11 and FB13. On NELL186, we set \u03b3 = 1\nand k = 50 (Guo et al., 2015; Luo et al., 2015).\nThe highest accuracy on the validation set was ob-\ntained when using \u03b7 = 0.01 for all three datasets,\nand when using \u21132 norm for NELL186, \u03b3 = 4,\nk = 20 and \u21131 norm for WN11, and \u03b3 = 1,\nk = 100 and \u21132 norm for FB13.\nWe set the hyper-parameters \u03b7, \u03b3, k, and the\n\u21131 or the \u21132-norm in our TransE-NMM model to\nthe same optimal hyper-parameters searched for\nTransE. We then used a grid search to select the\nhyper-parameter \u03b4 \u2208{0, 1, 5, 10} and L2 regu-\nlarizer \u03bb \u2208{0.005, 0.01, 0.05} for TransE-NMM.\nBy monitoring the micro-averaged accuracy after\neach training epoch, we obtained the highest ac-\ncuracy on validation set when using \u03b4 = 1 and\n\u03bb = 0.05 for both WN11 and FB13, and \u03b4 = 0\nand \u03bb = 0.01 for NELL186.\nFor both entity prediction and relation predic-\ntion tasks, we set the hyper-parameters \u03b7, \u03b3,\nk, and the \u21131 or the \u21132-norm for both TransE\nand TransE-NMM to be the same as the opti-\nmal parameters found for the triple classi\ufb01ca-\ntion task. We then monitored on TransE the \ufb01l-\ntered MRR on validation set after each training\nepoch. We chose the model with highest valida-\ntion MRR, which was then used to evaluate the\ntest set. For TransE-NMM, we searched the hyper-\nparameter \u03b4 \u2208{0, 1, 5, 10} and L2 regularizer\n\u03bb \u2208{0.005, 0.01, 0.05}. By monitoring the \ufb01l-\ntered MRR after each training epoch, we selected\nthe best model with the highest \ufb01ltered MRR on\nthe validation set. Speci\ufb01cally, for the entity pre-\ndiction task, we selected \u03b4 = 10 and \u03bb = 0.005 for\nWN11, \u03b4 = 5 and \u03bb = 0.01 for FB13, and \u03b4 = 5\nand \u03bb = 0.005 for NELL186. For the relation pre-\ndiction task, we selected \u03b4 = 10 and \u03bb = 0.005\nfor WN11, \u03b4 = 10 and \u03bb = 0.05 for FB13, and\n\u03b4 = 1 and \u03bb = 0.05 for NELL186.\nData\nMethod\nTriple class.\nEntity prediction\nRelation prediction\nMic.\nMac.\nMR\nMRR\nH@10\nMR\nMRR\nH@10\nWN11\nR\nTransE\n85.21\n82.53\n4324\n0.102\n19.21\n2.37\n0.679\n99.93\nTransE-NMM\n86.82\n84.37\n3687\n0.094\n17.98\n2.14\n0.687\n99.92\nF\nTransE\n4304\n0.122\n21.86\n2.37\n0.679\n99.93\nTransE-NMM\n3668\n0.109\n20.12\n2.14\n0.687\n99.92\nFB13\nR\nTransE\n87.57\n86.66\n9037\n0.204\n35.39\n1.01\n0.996\n99.99\nTransE-NMM\n88.58\n87.99\n8289\n0.258\n35.53\n1.01\n0.996\n100.0\nF\nTransE\n5600\n0.213\n36.28\n1.01\n0.996\n99.99\nTransE-NMM\n5018\n0.267\n36.36\n1.01\n0.996\n100.0\nNELL186\nR\nTransE\n92.13\n88.96\n309\n0.192\n36.55\n8.43\n0.580\n77.18\nTransE-NMM\n94.57\n90.95\n238\n0.221\n37.55\n6.15\n0.677\n82.16\nF\nTransE\n279\n0.268\n47.13\n8.32\n0.602\n77.26\nTransE-NMM\n214\n0.292\n47.82\n6.08\n0.690\n82.20\nTable 3: Experimental results of TransE (i.e. TransE-NMM with \u03c4 = 0) and TransE-NMM with \u03c4 = 10.\nMicro-averaged (labeled as Mic.) and Macro-averaged (labeled as Mac.) accuracy results are for the\ntriple classi\ufb01cation task. MR, MRR and H@10 abbreviate the mean rank, the mean reciprocal rank and\nHits@10 (in %), respectively. \u201cR\u201d and \u201cF\u201d denote the \u201cRaw\u201d and \u201cFiltered\u201d settings used in the entity\nprediction and relation prediction tasks, respectively.\n5\nResults\n5.1\nQuantitative results\nTable 3 presents the results of TransE and TransE-\nNMM on triple classi\ufb01cation, entity prediction\nand relation prediction tasks on all experimental\ndatasets. The results show that TransE-NMM gen-\nerally performs better than TransE in all three eval-\nuation tasks.\nSpeci\ufb01cally, TransE-NMM obtains higher triple\nclassi\ufb01cation results than TransE in all three ex-\nperimental datasets, for example, with 2.44% ab-\nsolute improvement in the micro-averaged accu-\nracy on the NELL186 dataset (i.e.\n31% reduc-\ntion in error).\nIn terms of entity prediction,\nTransE-NMM obtains better mean rank, MRR and\nHits@10 scores than TransE on both FB13 and\nNELL186 datasets.\nSpeci\ufb01cally, on NELL186\nTransE-NMM gains a signi\ufb01cant improvement of\n279 \u2212214 = 65 in the \ufb01ltered mean rank (which\nis about 23% relative improvement), while on\nthe FB13 dataset, TransE-NMM improves with\n0.267\u22120.213 = 0.054 in the \ufb01ltered MRR (which\nis about 25% relative improvement).\nOn the\nWN11 dataset, TransE-NMM only achieves bet-\nter mean rank for entity prediction. The relation\nprediction results of TransE-NMM and TransE are\nrelatively similar on both WN11 and FB13 be-\ncause the number of relation types is small in these\ntwo datasets.\nOn NELL186, however, TransE-\nMethod\nW11\nF13\nTransR (Lin et al., 2015b)\n85.9\n82.5\nCTransR (Lin et al., 2015b)\n85.7\n-\nTEKE H (Wang and Li, 2016)\n84.8\n84.2\nTransD (Ji et al., 2015)\n86.4\n89.1\nTranSparse-S (Ji et al., 2016)\n86.4\n88.2\nTranSparse-US (Ji et al., 2016)\n86.8\n87.5\nNTN (Socher et al., 2013)\n70.6\n87.2\nTransH (Wang et al., 2014)\n78.8\n83.3\nSLogAn (Liang and Forbus, 2015)\n75.3\n85.3\nKG2E (He et al., 2015)\n85.4\n85.3\nBilinear-COMP (Guu et al., 2015)\n77.6\n86.1\nTransE-COMP (Guu et al., 2015)\n80.3\n87.6\nTransR-FT (Feng et al., 2016a)\n86.6\n82.9\nTransG (Xiao et al., 2016b)\n87.4\n87.3\nlppTransD (Yoon et al., 2016)\n86.2\n88.6\nTransE\n85.2\n87.6\nTransE-NMM\n86.8\n88.6\nTable 4: Micro-averaged accuracy results (in %)\nfor triple classi\ufb01cation on WN11 (labeled as W11)\nand FB13 (labeled as F13) test sets. Scores in bold\nand underline are the best and second best scores,\nrespectively.\nNMM does signi\ufb01cantly better than TransE.\nIn Table 4, we compare the micro-averaged\ntriple classi\ufb01cation accuracy of our TransE-NMM\nmodel with the previously reported results on the\nWN11 and FB13 datasets. The \ufb01rst 6 rows re-\nport the performance of models that use TransE\nMethod\nTriple class.\nEntity pred.\nMic.\nMac.\nMR\nH@10\nTransE-LLE\n90.08\n84.50\n535\n20.02\nSME-LLE\n93.64\n89.39\n253\n37.14\nSE-LLE\n93.95\n88.54\n447\n31.55\nTransE-SkipG\n85.33\n80.06\n385\n30.52\nSME-SkipG\n92.86\n89.65\n293\n39.70\nSE-SkipG\n93.07\n87.98\n412\n31.12\nTransE\n92.13\n88.96\n309\n36.55\nTransE-NMM\n94.57\n90.95\n238\n37.55\nTable 5: Results on the NELL186 test set. Results\nfor the entity prediction task are in the \u201cRaw\u201d set-\nting. \u201c-SkipG\u201d abbreviates \u201c-Skip-gram\u201d.\nto initialize the entity and relation vectors. The\nlast 11 rows present the accuracy of models with\nrandomly initialized parameters.\nTable 4 shows that our TransE-NMM model ob-\ntains the second highest result on both WN11 and\nFB13. Note that there are higher results reported\nfor NTN (Socher et al., 2013), Bilinear-COMP and\nTransE-COMP (Guu et al., 2015) when entity vec-\ntors are initialized by averaging the pre-trained\nword vectors (Mikolov et al., 2013; Pennington\net al., 2014).\nIt is not surprising as many en-\ntity names in WordNet and FreeBase are lexically\nmeaningful. It is possible for all other embedding\nmodels to utilize the pre-trained word vectors as\nwell.\nHowever, as pointed out by Wang et al.\n(2014) and Guu et al. (2015), averaging the pre-\ntrained word vectors for initializing entity vectors\nis an open problem and it is not always useful since\nentity names in many domain-speci\ufb01c KBs are not\nlexically meaningful.\nTable 5 compares the accuracy for triple classi\ufb01-\ncation, the raw mean rank and raw Hits@10 scores\nfor entity prediction on the NELL186 dataset. The\n\ufb01rst three rows present the best results reported\nin Guo et al. (2015), while the next three rows\npresent the best results reported in Luo et al.\n(2015). TransE-NMM obtains the highest triple\nclassi\ufb01cation accuracy, the best raw mean rank and\nthe second highest raw Hits@10 on the entity pre-\ndiction task in this comparison.\n5.2\nQualitative results\nTable 6 presents some examples to illustrate the\nuseful information modeled by the neighbors. We\ntook the relation-speci\ufb01c mixture weights from the\nlearned TransE-NMM model optimized on the en-\nRelation\nTop 3-neighbor relations\nhas instance\ntype of\nsubordinate instance of\n(WN11)\ndomain topic\nsynset domain topic\ndomain region\nmember holonym\n(WN11)\nmember meronym\nnationality\nplace of birth\nplace of death\n(FB13)\nlocation\nspouse\nchildren, spouse, parents\n(FB13)\nCEOof\nWorksFor\nTopMemberOfOrganization\n(NELL186)\nPersonLeadsOrganization\nAnimalDevelopDisease\nAnimalSuchAsInsect\nAnimalThatFeedOnInsect\n(NELL186)\nAnimalDevelopDisease\nTable 6: Qualitative examples.\ntity prediction task, and then extracted three neigh-\nbor relations with the largest mixture weights\ngiven a relation.\nTable 6 shows that those relations are semanti-\ncally coherent. For example, if we know the place\nof birth and/or the place of death of a person and/or\nthe location where the person is living, it is likely\nthat we can predict the person\u2019s nationality. On\nthe other hand, if we know that a person works\nfor an organization and that this person is also the\ntop member of that organization, then it is possible\nthat this person is the CEO of that organization.\n5.3\nDiscussion\nDespite of the lower triple classi\ufb01cation scores of\nTransE reported in Wang et al. (2014), Table 4\nshows that TransE in fact obtains a very compet-\nitive accuracy. Particularly, compared to the rela-\ntion path model TransE-COMP (Guu et al., 2015),\nwhen model parameters were randomly initial-\nized, TransE obtains 85.2 \u221280.3 = 4.9% absolute\naccuracy improvement on the WN11 dataset while\nachieving similar score on the FB13 dataset. Our\nhigh results of the TransE model are probably due\nto a careful grid search and using the \u201cBernoulli\u201d\ntrick. Note that Lin et al. (2015b), Ji et al. (2015)\nand Ji et al. (2016) did not report the TransE\nresults used for initializing TransR, TransD and\nTranSparse, respectively. They directly copied the\nTransE results previously reported in Wang et al.\n(2014). So it is dif\ufb01cult to determine exactly how\nmuch TransR, TransD and TranSparse gain over\nTransE. These models might obtain better results\nFigure 2: Relative improvement of TransE-NMM\nagainst TransE for entity prediction task in WN11\nwhen the \ufb01ltering threshold \u03c4 = {10, 100, 500}\n(with other hyper-parameters being the same as\nselected in Section 4.3). Pre\ufb01xes \u201cR-\u201d and \u201cF-\u201d\ndenote the \u201cRaw\u201d and \u201cFiltered\u201d settings, respec-\ntively. Suf\ufb01xes \u201c-MR\u201d, \u201c-MRR\u201d and \u201c-H@10\u201d ab-\nbreviate the mean rank, the mean reciprocal rank\nand Hits@10, respectively.\nthan previously reported when the TransE used for\ninitalization performs as well as reported in this\npaper. Furthermore, Garc\u00b4\u0131a-Dur\u00b4an et al. (2015),\nLin et al. (2015a), Garc\u00b4\u0131a-Dur\u00b4an et al. (2016) and\nNickel et al. (2016b) also showed that for entity\nprediction TransE obtains very competitive results\nwhich are much higher than the TransE results\noriginally published in Bordes et al. (2013).3\nAs presented in Table 3, for entity predic-\ntion using WN11, TransE-NMM with the \ufb01lter-\ning threshold \u03c4 = 10 only obtains better mean\nrank than TransE (about 15% relative improve-\nment) but lower Hits@10 and mean reciprocal\nrank. The reason might be that in semantic lexi-\ncal KBs such as WordNet where relationships be-\ntween words or word groups are manually con-\nstructed, whole neighborhood information might\nbe useful. So when using a small \ufb01ltering thresh-\nold, the model ignores a lot of potential informa-\ntion that could help predicting relationships. Fig-\nure 2 presents relative improvements in entity pre-\ndiction of TransE-NMM over TransE on WN11\nwhen varying the \ufb01ltering threshold \u03c4.\nFigure\n2 shows that TransE-NMM gains better scores\n3They did not report the results on WN11 and FB13\ndatasets, which are used in this paper, though.\nwith higher \u03c4 value.\nSpeci\ufb01cally, when \u03c4\n=\n500 TransE-NMM does signi\ufb01cantly better than\nTransE in all entity prediction metrics.\n6\nConclusion and future work\nWe introduced a neighborhood mixture model\nfor knowledge base completion by constructing\nneighbor-based vector representations for entities.\nWe demonstrated its effect by extending TransE\n(Bordes et al., 2013) with our neighborhood mix-\nture model. On three different datasets, experi-\nmental results show that our model signi\ufb01cantly\nimproves TransE and obtains better results than\nthe other state-of-the-art embedding models on\ntriple classi\ufb01cation, entity prediction and relation\nprediction tasks. In future work, we plan to ap-\nply the neighborhood mixture model to other em-\nbedding models, especially to relation path mod-\nels such as TransE-COMP, to combine the useful\ninformation from both relation paths and entity\nneighborhoods.\nAcknowledgments\nThis research was supported by a Google award\nthrough the Natural Language Understanding Fo-\ncused Program, and under the Australian Research\nCouncil\u2019s Discovery Projects funding scheme\n(project number DP160102156).\nThis research\nwas also supported by NICTA, funded by the\nAustralian Government through the Department\nof Communications and the Australian Research\nCouncil through the ICT Centre of Excellence\nProgram. The \ufb01rst author was supported by an\nInternational Postgraduate Research Scholarship\nand a NICTA NRPA Top-Up Scholarship.\nReferences\n[Bollacker et al.2008] Kurt Bollacker,\nColin Evans,\nPraveen Paritosh, Tim Sturge, and Jamie Taylor.\n2008. Freebase: A Collaboratively Created Graph\nDatabase for Structuring Human Knowledge.\nIn\nProceedings of the 2008 ACM SIGMOD Interna-\ntional Conference on Management of Data, pages\n1247\u20131250.\n[Bordes et al.2011] Antoine Bordes, Jason Weston, Ro-\nnan Collobert, and Yoshua Bengio. 2011. Learn-\ning Structured Embeddings of Knowledge Bases. In\nProceedings of the Twenty-Fifth AAAI Conference\non Arti\ufb01cial Intelligence, pages 301\u2013306.\n[Bordes et al.2012] Antoine Bordes, Xavier Glorot, Ja-\nson Weston, and Yoshua Bengio. 2012. A Semantic\nMatching Energy Function for Learning with Multi-\nrelational Data. Machine Learning, 94(2):233\u2013259.\n[Bordes et al.2013] Antoine Bordes, Nicolas Usunier,\nAlberto Garcia-Duran, Jason Weston, and Oksana\nYakhnenko.\n2013.\nTranslating Embeddings for\nModeling Multi-relational Data.\nIn Advances in\nNeural Information Processing Systems 26, pages\n2787\u20132795.\n[Carlson et al.2010] Andrew Carlson, Justin Betteridge,\nBryan Kisiel, Burr Settles, Estevam R. Hruschka, Jr.,\nand Tom M. Mitchell. 2010. Toward an Architec-\nture for Never-ending Language Learning. In Pro-\nceedings of the Twenty-Fourth AAAI Conference on\nArti\ufb01cial Intelligence, pages 1306\u20131313.\n[Das et al.2017] Rajarshi Das,\nArvind Neelakantan,\nDavid Belanger, and Andrew McCallum.\n2017.\nChains of reasoning over entities, relations, and text\nusing recurrent neural networks. In Proceedings of\nthe 15th Conference of the European Chapter of the\nAssociation for Computational Linguistics.\n[Duchi et al.2011] John Duchi, Elad Hazan, and Yoram\nSinger. 2011. Adaptive Subgradient Methods for\nOnline Learning and Stochastic Optimization. The\nJournal of Machine Learning Research, 12:2121\u2013\n2159.\n[Feng et al.2016a] Jun Feng, Minlie Huang, Mingdong\nWang, Mantong Zhou, Yu Hao, and Xiaoyan Zhu.\n2016a.\nKnowledge graph embedding by \ufb02exible\ntranslation. In Principles of Knowledge Represen-\ntation and Reasoning: Proceedings of the Fifteenth\nInternational Conference, pages 557\u2013560.\n[Feng et al.2016b] Jun Feng,\nMinlie Huang,\nYang\nYang, and xiaoyan zhu.\n2016b.\nGAKE: Graph\nAware Knowledge Embedding. In Proceedings of\nCOLING 2016, the 26th International Conference\non Computational Linguistics: Technical Papers,\npages 641\u2013651.\n[Garc\u00b4\u0131a-Dur\u00b4an et al.2015] Alberto Garc\u00b4\u0131a-Dur\u00b4an, An-\ntoine Bordes, and Nicolas Usunier.\n2015.\nCom-\nposing Relationships with Translations. In Proceed-\nings of the 2015 Conference on Empirical Methods\nin Natural Language Processing, pages 286\u2013290.\n[Garc\u00b4\u0131a-Dur\u00b4an et al.2016] Alberto Garc\u00b4\u0131a-Dur\u00b4an, An-\ntoine Bordes, Nicolas Usunier, and Yves Grand-\nvalet. 2016. Combining Two and Three-Way Em-\nbedding Models for Link Prediction in Knowledge\nBases. Journal of Arti\ufb01cial Intelligence Research,\n55:715\u2013742.\n[Guo et al.2015] Shu Guo, Quan Wang, Bin Wang, Li-\nhong Wang, and Li Guo.\n2015.\nSemantically\nSmooth Knowledge Graph Embedding. In Proceed-\nings of the 53rd Annual Meeting of the Association\nfor Computational Linguistics and the 7th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers), pages 84\u201394.\n[Guu et al.2015] Kelvin Guu, John Miller, and Percy\nLiang. 2015. Traversing Knowledge Graphs in Vec-\ntor Space. In Proceedings of the 2015 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 318\u2013327.\n[He et al.2015] Shizhu He, Kang Liu, Guoliang Ji, and\nJun Zhao. 2015. Learning to Represent Knowledge\nGraphs with Gaussian Embedding. In Proceedings\nof the 24th ACM International on Conference on In-\nformation and Knowledge Management, pages 623\u2013\n632.\n[Jenatton et al.2012] Rodolphe Jenatton,\nNicolas L.\nRoux, Antoine Bordes, and Guillaume R Obozin-\nski. 2012. A latent factor model for highly multi-\nrelational data. In Advances in Neural Information\nProcessing Systems 25, pages 3167\u20133175.\n[Ji et al.2015] Guoliang Ji, Shizhu He, Liheng Xu,\nKang Liu, and Jun Zhao. 2015. Knowledge Graph\nEmbedding via Dynamic Mapping Matrix. In Pro-\nceedings of the 53rd Annual Meeting of the Associ-\nation for Computational Linguistics and the 7th In-\nternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 687\u2013\n696.\n[Ji et al.2016] Guoliang Ji, Kang Liu, Shizhu He, and\nJun Zhao.\n2016.\nKnowledge Graph Completion\nwith Adaptive Sparse Transfer Matrix. In Proceed-\nings of the Thirtieth AAAI Conference on Arti\ufb01cial\nIntelligence, pages 985\u2013991.\n[Krompa\u00df et al.2015] Denis Krompa\u00df, Stephan Baier,\nand Volker Tresp. 2015. Type-Constrained Repre-\nsentation Learning in Knowledge Graphs. In Pro-\nceedings of the 14th International Semantic Web\nConference, pages 640\u2013655.\n[Lehmann et al.2015] Jens\nLehmann,\nRobert\nIsele,\nMax Jakob, Anja Jentzsch, Dimitris Kontokostas,\nPablo N. Mendes, Sebastian Hellmann, Mohamed\nMorsey, Patrick van Kleef, S\u00a8oren Auer, and Chris-\ntian Bizer. 2015. DBpedia - A Large-scale, Multi-\nlingual Knowledge Base Extracted from Wikipedia.\nSemantic Web, 6(2):167\u2013195.\n[Liang and Forbus2015] Chen Liang and Kenneth D.\nForbus. 2015. Learning Plausible Inferences from\nSemantic Web Knowledge by Combining Analogi-\ncal Generalization with Structured Logistic Regres-\nsion. In Proceedings of the Twenty-Ninth AAAI Con-\nference on Arti\ufb01cial Intelligence, pages 551\u2013557.\n[Lin et al.2015a] Yankai Lin, Zhiyuan Liu, Huanbo\nLuan, Maosong Sun, Siwei Rao, and Song Liu.\n2015a. Modeling Relation Paths for Representation\nLearning of Knowledge Bases. In Proceedings of\nthe 2015 Conference on Empirical Methods in Nat-\nural Language Processing, pages 705\u2013714.\n[Lin et al.2015b] Yankai Lin, Zhiyuan Liu, Maosong\nSun, Yang Liu, and Xuan Zhu. 2015b. Learning En-\ntity and Relation Embeddings for Knowledge Graph\nCompletion.\nIn Proceedings of the Twenty-Ninth\nAAAI Conference on Arti\ufb01cial Intelligence Learn-\ning, pages 2181\u20132187.\n[Liu and Nocedal1989] D. C. Liu and J. Nocedal. 1989.\nOn the Limited Memory BFGS Method for Large\nScale Optimization.\nMathematical Programming,\n45(3):503\u2013528.\n[Liu et al.2016] Qiao Liu, Liuyi Jiang, Minghao Han,\nYao Liu, and Zhiguang Qin.\n2016.\nHierarchical\nRandom Walk Inference in Knowledge Graphs. In\nProceedings of the 39th International ACM SIGIR\nConference on Research and Development in Infor-\nmation Retrieval, pages 445\u2013454.\n[Luo et al.2015] Yuanfei Luo, Quan Wang, Bin Wang,\nand Li Guo. 2015. Context-Dependent Knowledge\nGraph Embedding. In Proceedings of the 2015 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 1656\u20131661.\n[Mikolov et al.2013] Tomas Mikolov, Wen-tau Yih, and\nGeoffrey Zweig. 2013. Linguistic Regularities in\nContinuous Space Word Representations.\nIn Pro-\nceedings of the 2013 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n746\u2013751.\n[Miller1995] George A. Miller.\n1995.\nWordNet: A\nLexical Database for English. Communications of\nthe ACM, 38(11):39\u201341.\n[Neelakantan et al.2015] Arvind\nNeelakantan,\nBen-\njamin Roth,\nand Andrew McCallum.\n2015.\nCompositional Vector Space Models for Knowledge\nBase Completion. In Proceedings of the 53rd An-\nnual Meeting of the Association for Computational\nLinguistics and the 7th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 156\u2013166.\n[Nguyen et al.2016] Dat Quoc Nguyen, Kairit Sirts,\nLizhen Qu, and Mark Johnson. 2016. STransE: a\nnovel embedding model of entities and relationships\nin knowledge bases.\nIn Proceedings of the 2016\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 460\u2013466.\n[Nickel et al.2011] Maximilian Nickel, Volker Tresp,\nand Hans-Peter Kriegel. 2011. A Three-Way Model\nfor Collective Learning on Multi-Relational Data. In\nProceedings of the 28th International Conference on\nMachine Learning, pages 809\u2013816.\n[Nickel et al.2016a] Maximilian Nickel, Kevin Mur-\nphy, Volker Tresp, and Evgeniy Gabrilovich. 2016a.\nA Review of Relational Machine Learning for\nKnowledge Graphs.\nProceedings of the IEEE,\n104(1):11\u201333.\n[Nickel et al.2016b] Maximilian\nNickel,\nLorenzo\nRosasco, and Tomaso Poggio. 2016b. Holographic\nembeddings of knowledge graphs. In Proceedings\nof the Thirtieth AAAI Conference on Arti\ufb01cial\nIntelligence, pages 1955\u20131961.\n[Niepert2016] Mathias Niepert. 2016. Discriminative\nGaifman Models. In Advances in Neural Informa-\ntion Processing Systems 29, pages 3405\u20133413.\n[Pennington et al.2014] Jeffrey\nPennington,\nRichard\nSocher, and Christopher Manning.\n2014.\nGlove:\nGlobal Vectors for Word Representation.\nIn Pro-\nceedings of the 2014 Conference on Empirical Meth-\nods in Natural Language Processing, pages 1532\u2013\n1543.\n[Shi and Weninger2017] Baoxu Shi and Tim Weninger.\n2017. ProjE: Embedding Projection for Knowledge\nGraph Completion. In Proceedings of the 31st AAAI\nConference on Arti\ufb01cial Intelligence.\n[Socher et al.2013] Richard\nSocher,\nDanqi\nChen,\nChristopher D Manning, and Andrew Ng.\n2013.\nReasoning\nWith\nNeural\nTensor\nNetworks\nfor\nKnowledge Base Completion.\nIn Advances in\nNeural Information Processing Systems 26, pages\n926\u2013934.\n[Suchanek et al.2007] Fabian M. Suchanek,\nGjergji\nKasneci, and Gerhard Weikum. 2007. YAGO: A\nCore of Semantic Knowledge. In Proceedings of the\n16th International Conference on World Wide Web,\npages 697\u2013706.\n[Taskar et al.2004] Ben Taskar, Ming fai Wong, Pieter\nAbbeel, and Daphne Koller. 2004. Link Prediction\nin Relational Data. In Advances in Neural Informa-\ntion Processing Systems 16, pages 659\u2013666.\n[Tay et al.2017] Yi Tay, Anh Tuan Luu, Siu Cheung\nHui, and Falk Brauer. 2017. Random Semantic Ten-\nsor Ensemble for Scalable Knowledge Graph Link\nPrediction. In Proceedings of the Tenth ACM Inter-\nnational Conference on Web Search and Data Min-\ning, pages 751\u2013760.\n[Toutanova et al.2016] Kristina\nToutanova,\nVictoria\nLin, Wen-tau Yih, Hoifung Poon, and Chris Quirk.\n2016. Compositional Learning of Embeddings for\nRelation Paths in Knowledge Base and Text.\nIn\nProceedings of the 54th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 1434\u20131444.\n[Trouillon et al.2016] Th\u00b4eo Trouillon, Johannes Welbl,\nSebastian Riedel,\n\u00b4Eric Gaussier, and Guillaume\nBouchard. 2016. Complex Embeddings for Sim-\nple Link Prediction. In Proceedings of the 33nd In-\nternational Conference on Machine Learning, pages\n2071\u20132080.\n[Wang and Li2016] Zhigang Wang and Juan-Zi Li.\n2016. Text-Enhanced Representation Learning for\nKnowledge Graph. In Proceedings of the Twenty-\nFifth International Joint Conference on Arti\ufb01cial In-\ntelligence, pages 1293\u20131299.\n[Wang et al.2014] Zhen Wang, Jianwen Zhang, Jianlin\nFeng, and Zheng Chen. 2014. Knowledge Graph\nEmbedding by Translating on Hyperplanes. In Pro-\nceedings of the Twenty-Eighth AAAI Conference on\nArti\ufb01cial Intelligence, pages 1112\u20131119.\n[Wang et al.2016] Quan Wang, Jing Liu, Yuanfei Luo,\nBin Wang, and Chin-Yew Lin. 2016. Knowledge\nBase Completion via Coupled Path Ranking. In Pro-\nceedings of the 54th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 1308\u20131318.\n[Wei et al.2016] Zhuoyu Wei, Jun Zhao, and Kang Liu.\n2016. Mining Inference Formulas by Goal-Directed\nRandom Walks. In Proceedings of the 2016 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 1379\u20131388.\n[West et al.2014] Robert West, Evgeniy Gabrilovich,\nKevin Murphy, Shaohua Sun, Rahul Gupta, and\nDekang Lin. 2014. Knowledge Base Completion\nvia Search-based Question Answering. In Proceed-\nings of the 23rd International Conference on World\nWide Web, pages 515\u2013526.\n[Xiao et al.2016a] Han Xiao, Minlie Huang, and Xi-\naoyan Zhu. 2016a. From One Point to a Manifold:\nKnowledge Graph Embedding for Precise Link Pre-\ndiction. In Proceedings of the Twenty-Fifth Inter-\nnational Joint Conference on Arti\ufb01cial Intelligence,\npages 1315\u20131321.\n[Xiao et al.2016b] Han Xiao, Minlie Huang, and Xi-\naoyan Zhu. 2016b. TransG : A Generative Model\nfor Knowledge Graph Embedding.\nIn Proceed-\nings of the 54th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 2316\u20132325.\n[Yang et al.2015] Bishan Yang, Wen-tau Yih, Xiaodong\nHe, Jianfeng Gao, and Li Deng. 2015. Embedding\nEntities and Relations for Learning and Inference in\nKnowledge Bases. In Proceedings of the Interna-\ntional Conference on Learning Representations.\n[Yoon et al.2016] Hee-Geun\nYoon,\nHyun-Je\nSong,\nSeong-Bae Park, and Se-Young Park.\n2016.\nA\nTranslation-Based Knowledge Graph Embedding\nPreserving Logical Property of Relations. In Pro-\nceedings of the 2016 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n907\u2013916.\n[Zeiler2012] Matthew D. Zeiler. 2012. ADADELTA:\nAn Adaptive Learning Rate Method.\nCoRR,\nabs/1212.5701.\n",
        "sentence": " Previously, a line of research makes use of external information such as textual relations from web-scale corpus or node features (Toutanova et al., 2015; Toutanova and Chen, 2015; Nguyen et al., 2016a), alleviating the sparsity problem. Following Nguyen et al. (2016b) and Shen et al. Following Nguyen et al. (2016b) and Shen et al. (2016), we divide the models into two groups.",
        "context": "STransE (Nguyen et al., 2016).\nWhile relation\npath models exploit extra information using longer\npaths existing in the KB, the neighborhood mix-\nture model effectively incorporates information\nabout many paths simultaneously. Our extensive\ninformation and improve the relationship predic-\ntion (Neelakantan et al., 2015; Lin et al., 2015a;\nGarc\u00b4\u0131a-Dur\u00b4an et al., 2015; Guu et al., 2015; Wang\net al., 2016; Feng et al., 2016b; Liu et al., 2016;\nNiepert, 2016; Wei et al., 2016; Toutanova et al.,\ninformation from both relation paths and entity\nneighborhoods.\nAcknowledgments\nThis research was supported by a Google award\nthrough the Natural Language Understanding Fo-\ncused Program, and under the Australian Research"
    },
    {
        "title": "STransE: a novel embedding model of entities and relationships in knowledge bases",
        "author": [
            "Dat Quoc Nguyen",
            "Kairit Sirts",
            "Lizhen Qu",
            "Mark Johnson."
        ],
        "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for",
        "citeRegEx": "Nguyen et al\\.,? 2016b",
        "shortCiteRegEx": "Nguyen et al\\.",
        "year": 2016,
        "abstract": "Knowledge bases of real-world facts about entities and their relationships\nare useful resources for a variety of natural language processing tasks.\nHowever, because knowledge bases are typically incomplete, it is useful to be\nable to perform link prediction or knowledge base completion, i.e., predict\nwhether a relationship not in the knowledge base is likely to be true. This\npaper combines insights from several previous link prediction models into a new\nembedding model STransE that represents each entity as a low-dimensional\nvector, and each relation by two matrices and a translation vector. STransE is\na simple combination of the SE and TransE models, but it obtains better link\nprediction performance on two benchmark datasets than previous embedding\nmodels. Thus, STransE can serve as a new baseline for the more complex models\nin the link prediction task.",
        "full_text": "STransE: a novel embedding model of entities and relationships\nin knowledge bases\u2217\nDat Quoc Nguyen1, Kairit Sirts1, Lizhen Qu2 and Mark Johnson1\n1 Department of Computing, Macquarie University, Sydney, Australia\ndat.nguyen@students.mq.edu.au, {kairit.sirts, mark.johnson}@mq.edu.au\n2 NICTA, ACT 2601, Australia\nlizhen.qu@nicta.com.au\nAbstract\nKnowledge bases of real-world facts about\nentities and their relationships are useful re-\nsources for a variety of natural language pro-\ncessing tasks. However, because knowledge\nbases are typically incomplete, it is useful to\nbe able to perform link prediction or knowl-\nedge base completion, i.e., predict whether\na relationship not in the knowledge base is\nlikely to be true. This paper combines insights\nfrom several previous link prediction models\ninto a new embedding model STransE that\nrepresents each entity as a low-dimensional\nvector, and each relation by two matrices and\na translation vector. STransE is a simple com-\nbination of the SE and TransE models, but it\nobtains better link prediction performance on\ntwo benchmark datasets than previous embed-\nding models. Thus, STransE can serve as a\nnew baseline for the more complex models in\nthe link prediction task.\n1\nIntroduction\nKnowledge bases (KBs), such as WordNet (Fell-\nbaum, 1998), YAGO (Suchanek et al., 2007), Free-\nbase (Bollacker et al., 2008) and DBpedia (Lehmann\net al., 2015), represent relationships between entities\nas triples (head entity, relation, tail entity). Even\nvery large knowledge bases are still far from com-\nplete (Socher et al., 2013; West et al., 2014). Link\nprediction or knowledge base completion systems\n(Nickel et al., 2016a) predict which triples not in\na knowledge base are likely to be true (Taskar et\n\u2217A revised version of our NAACL-HLT 2016 paper with\nadditional experimental results and latest related work.\nal., 2004; Bordes et al., 2011). A variety of differ-\nent kinds of information is potentially useful here,\nincluding information extracted from external cor-\npora (Riedel et al., 2013; Wang et al., 2014a) and\nthe other relationships that hold between the enti-\nties (Angeli and Manning, 2013; Zhao et al., 2015).\nFor example, Toutanova et al. (2015) used informa-\ntion from the external ClueWeb-12 corpus to signif-\nicantly enhance performance.\nWhile integrating a wide variety of information\nsources can produce excellent results (Das et al.,\n2017), there are several reasons for studying sim-\npler models that directly optimize a score function\nfor the triples in a knowledge base, such as the\none presented here.\nFirst, additional information\nsources might not be available, e.g., for knowledge\nbases for specialized domains. Second, models that\ndon\u2019t exploit external resources are simpler and thus\ntypically much faster to train than the more com-\nplex models using additional information.\nThird,\nthe more complex models that exploit external in-\nformation are typically extensions of these simpler\nmodels, and are often initialized with parameters es-\ntimated by such simpler models, so improvements to\nthe simpler models should yield corresponding im-\nprovements to the more complex models as well.\nEmbedding models for KB completion associate\nentities and/or relations with dense feature vectors\nor matrices. Such models obtain state-of-the-art per-\nformance (Nickel et al., 2011; Bordes et al., 2011;\nBordes et al., 2012; Bordes et al., 2013; Socher et\nal., 2013; Wang et al., 2014b; Guu et al., 2015) and\ngeneralize to large KBs (Krompa\u00df et al., 2015). Ta-\nble 1 summarizes a number of prominent embedding\narXiv:1606.08140v3  [cs.CL]  8 Mar 2017\nModel\nScore function fr(h, t)\nOpt.\nSE\n\u2225Wr,1h \u2212Wr,2t\u2225\u21131/2 ; Wr,1, Wr,2 \u2208Rk\u00d7k\nSGD\nUnstructured\n\u2225h \u2212t\u2225\u21131/2\nSGD\nTransE\n\u2225h + r \u2212t\u2225\u21131/2 ; r \u2208Rk\nSGD\nDISTMULT\nh\u22a4Wrt ; Wr is a diagonal matrix \u2208Rk\u00d7k\nAdaGrad\nNTN\nu\u22a4\nr tanh(h\u22a4Mrt + Wr,1h + Wr,2t + br) ; ur, br \u2208Rd; Mr \u2208Rk\u00d7k\u00d7d; Wr,1, Wr,2 \u2208Rd\u00d7k\nL-BFGS\nTransH\n\u2225(I \u2212rpr\u22a4\np )h + r \u2212(I \u2212rpr\u22a4\np )t\u2225\u21131/2 ; rp, r \u2208Rk ; I: Identity matrix size k \u00d7 k\nSGD\nTransD\n\u2225(I + rph\u22a4\np )h + r \u2212(I + rpt\u22a4\np )t\u2225\u21131/2 ; rp, r \u2208Rd ; hp, tp \u2208Rk ; I: Identity matrix size d \u00d7 k\nAdaDelta\nTransR\n\u2225Wrh + r \u2212Wrt\u2225\u21131/2 ; Wr \u2208Rd\u00d7k ; r \u2208Rd\nSGD\nTranSparse\n\u2225Wh\nr (\u03b8h\nr )h + r \u2212Wt\nr(\u03b8t\nr)t\u2225\u21131/2 ; Wh\nr , Wt\nr \u2208Rd\u00d7k; \u03b8h\nr , \u03b8t\nr \u2208R ; r \u2208Rd\nSGD\nOur STransE\n\u2225Wr,1h + r \u2212Wr,2t\u2225\u21131/2 ; Wr,1, Wr,2 \u2208Rk\u00d7k; r \u2208Rk\nSGD\nTable 1: The score functions fr(h, t) and the optimization methods (Opt.) of several prominent embedding models\nfor KB completion. In all of these the entities h and t are represented by vectors h and t \u2208Rk respectively.\nmodels for KB completion.\nLet (h, r, t) represent a triple. In all of the models\ndiscussed here, the head entity h and the tail entity\nt are represented by vectors h and t \u2208Rk respec-\ntively. The Unstructured model (Bordes et al., 2012)\nassumes that h \u2248t. As the Unstructured model\ndoes not take the relationship r into account, it can-\nnot distinguish different relation types. The Struc-\ntured Embedding (SE) model (Bordes et al., 2011)\nextends the unstructured model by assuming that h\nand t are similar only in a relation-dependent sub-\nspace. It represents each relation r with two matri-\nces Wr,1 and Wr,2 \u2208Rk\u00d7k, which are chosen so\nthat Wr,1h \u2248Wr,2t. The TransE model (Bordes et\nal., 2013) is inspired by models such as Word2Vec\n(Mikolov et al., 2013) where relationships between\nwords often correspond to translations in latent fea-\nture space. The TransE model represents each rela-\ntion r by a translation vector r \u2208Rk, which is cho-\nsen so that h + r \u2248t.\nThe primary contribution of this paper is that\ntwo very simple relation-prediction models, SE and\nTransE, can be combined into a single model, which\nwe call STransE.1 Speci\ufb01cally, we use relation-\nspeci\ufb01c matrices Wr,1 and Wr,2 as in the SE model\nto identify the relation-dependent aspects of both h\nand t, and use a vector r as in the TransE model\nto describe the relationship between h and t in this\nsubspace.\nSpeci\ufb01cally, our new KB completion\nmodel STransE chooses Wr,1, Wr,2 and r so that\n1Source code: https://github.com/datquocnguyen/STransE\nWr,1h + r \u2248Wr,2t. That is, a TransE-style rela-\ntionship holds in some relation-dependent subspace,\nand crucially, this subspace may involve very dif-\nferent projections of the head h and tail t. So Wr,1\nand Wr,2 can highlight, suppress, or even change the\nsign of, relation-speci\ufb01c attributes of h and t. For\nexample, for the \u201cpurchases\u201d relationship, certain\nattributes of individuals h (e.g., age, gender, mari-\ntal status) are presumably strongly correlated with\nvery different attributes of objects t (e.g., sports car,\nwashing machine and the like).\nAs we show below, STransE performs better than\nthe SE and TransE models and other state-of-the-art\nlink prediction models on two standard link predic-\ntion datasets WN18 and FB15k, so it can serve as\na new baseline for KB completion. We expect that\nthe STransE will also be able to serve as the basis\nfor extended models that exploit a wider variety of\ninformation sources, just as TransE does.\n2\nOur approach\nLet E denote the set of entities and R the set of re-\nlation types. For each triple (h, r, t), where h, t \u2208E\nand r \u2208R, the STransE model de\ufb01nes a score func-\ntion fr(h, t) of its implausibility.\nOur goal is to\nchoose f such that the score fr(h, t) of a plausi-\nble triple (h, r, t) is smaller than the score fr\u2032(h\u2032, t\u2032)\nof an implausible triple (h\u2032, r\u2032, t\u2032). We de\ufb01ne the\nSTransE score function f as follows:\nfr(h, t)\n=\n\u2225Wr,1h + r \u2212Wr,2t\u2225\u21131/2\nusing either the \u21131 or the \u21132-norm (the choice is made\nusing validation data; in our experiments we found\nthat the \u21131 norm gave slightly better results).\nTo\nlearn the vectors and matrices we minimize the fol-\nlowing margin-based objective function:\nL\n=\nX\n(h,r,t)\u2208G\n(h\u2032,r,t\u2032)\u2208G\u2032\n(h,r,t)\n[\u03b3 + fr(h, t) \u2212fr(h\u2032, t\u2032)]+\nwhere [x]+ = max(0, x), \u03b3 is the margin hyper-\nparameter, G is the training set consisting of correct\ntriples, and G\u2032\n(h,r,t) = {(h\u2032, r, t) | h\u2032 \u2208E, (h\u2032, r, t) /\u2208\nG} \u222a{(h, r, t\u2032) | t\u2032 \u2208E, (h, r, t\u2032) /\u2208G} is the set\nof incorrect triples generated by corrupting a correct\ntriple (h, r, t) \u2208G.\nWe use Stochastic Gradient Descent (SGD) to\nminimize L, and impose the following constraints\nduring training: \u2225h\u22252 \u2a7d1, \u2225r\u22252 \u2a7d1, \u2225t\u22252 \u2a7d1,\n\u2225Wr,1h\u22252 \u2a7d1 and \u2225Wr,2t\u22252 \u2a7d1.\n3\nRelated work\nTable 1 summarizes related embedding models for\nlink prediction and KB completion.\nThe models\ndiffer in the score functions fr(h, t) and the algo-\nrithms used to optimize the margin-based objective\nfunction, e.g., SGD, AdaGrad (Duchi et al., 2011),\nAdaDelta (Zeiler, 2012) and L-BFGS (Liu and No-\ncedal, 1989).\nDISTMULT (Yang et al., 2015) is based on a\nBilinear model (Nickel et al., 2011; Bordes et al.,\n2012; Jenatton et al., 2012) where each relation is\nrepresented by a diagonal rather than a full matrix.\nThe neural tensor network (NTN) model (Socher et\nal., 2013) uses a bilinear tensor operator to represent\neach relation while ProjE (Shi and Weninger, 2017)\ncould be viewed as a simpli\ufb01ed version of NTN\nwith diagonal matrices.\nSimilar quadratic forms\nare used to model entities and relations in KG2E\n(He et al., 2015), ComplEx (Trouillon et al., 2016),\nTATEC (Garc\u00b4\u0131a-Dur\u00b4an et al., 2016) and RSTE (Tay\net al., 2017).\nIn addition, HolE (Nickel et al.,\n2016b) uses circular correlation\u2014a compositional\noperator\u2014which could be interpreted as a compres-\nsion of the tensor product.\nThe TransH model (Wang et al., 2014b) asso-\nciates each relation with a relation-speci\ufb01c hyper-\nplane and uses a projection vector to project en-\ntity vectors onto that hyperplane. TransD (Ji et al.,\n2015) and TransR/CTransR (Lin et al., 2015b) ex-\ntend the TransH model using two projection vec-\ntors and a matrix to project entity vectors into a\nrelation-speci\ufb01c space, respectively. TransD learns\na relation-role speci\ufb01c mapping just as STransE, but\nrepresents this mapping by projection vectors rather\nthan full matrices, as in STransE. The lppTransD\nmodel (Yoon et al., 2016) extends TransD to ad-\nditionally use two projection vectors for represent-\ning each relation. In fact, our STransE model and\nTranSparse (Ji et al., 2016) can be viewed as direct\nextensions of the TransR model, where head and tail\nentities are associated with their own projection ma-\ntrices, rather than using the same matrix for both, as\nin TransR and CTransR.\nRecently, several authors have shown that relation\npaths between entities in KBs provide richer infor-\nmation and improve the relationship prediction (Lin\net al., 2015a; Garc\u00b4\u0131a-Dur\u00b4an et al., 2015; Guu et al.,\n2015; Wang et al., 2016; Feng et al., 2016; Liu et al.,\n2016; Niepert, 2016; Wei et al., 2016; Toutanova et\nal., 2016; Nguyen et al., 2016). In addition, Nickel\net al. (2016a) reviews other approaches for learning\nfrom KBs and multi-relational data.\n4\nExperiments\nFor link prediction evaluation, we conduct experi-\nments and compare the performance of our STransE\nmodel with published results on the benchmark\nWN18 and FB15k datasets (Bordes et al., 2013). In-\nformation about these datasets is given in Table 2.\nDataset\n#E\n#R\n#Train\n#Valid\n#Test\nWN18\n40,943\n18\n141,442\n5,000\n5,000\nFB15k\n14,951\n1,345\n483,142\n50,000\n59,071\nTable 2: Statistics of the experimental datasets used in\nthis study (and previous works). #E is the number of\nentities, #R is the number of relation types, and #Train,\n#Valid and #Test are the numbers of triples in the training,\nvalidation and test sets, respectively.\n4.1\nTask and evaluation protocol\nThe link prediction task (Bordes et al., 2011; Bordes\net al., 2012; Bordes et al., 2013) predicts the head or\ntail entity given the relation type and the other entity,\ni.e. predicting h given (?, r, t) or predicting t given\n(h, r, ?) where ? denotes the missing element. The\nMethod\nRaw\nFiltered\nWN18\nFB15k\nWN18\nFB15k\nMR\nH10\nMRR\nMR\nH10\nMRR\nMR\nH10\nMRR\nMR\nH10\nMRR\nSE (Bordes et al., 2011)\n1011\n68.5\n-\n273\n28.8\n-\n985\n80.5\n-\n162\n39.8\n-\nUnstructured (Bordes et al., 2012)\n315\n35.3\n-\n1074\n4.5\n-\n304\n38.2\n-\n979\n6.3\n-\nTransE (Bordes et al., 2013)\n263\n75.4\n-\n243\n34.9\n-\n251\n89.2\n-\n125\n47.1\n-\nTransH (Wang et al., 2014b)\n401\n73.0\n-\n212\n45.7\n-\n303\n86.7\n-\n87\n64.4\n-\nTransR (Lin et al., 2015b)\n238\n79.8\n-\n198\n48.2\n-\n225\n92.0\n-\n77\n68.7\n-\nCTransR (Lin et al., 2015b)\n231\n79.4\n-\n199\n48.4\n-\n218\n92.3\n-\n75\n70.2\n-\nKG2E (He et al., 2015)\n342\n80.2\n-\n174\n48.9\n-\n331\n92.8\n-\n59\n74.0\n-\nTransD (Ji et al., 2015)\n224\n79.6\n-\n194\n53.4\n-\n212\n92.2\n-\n91\n77.3\n-\nlppTransD (Yoon et al., 2016)\n283\n80.5\n-\n195\n53.0\n-\n270\n94.3\n-\n78\n78.7\n-\nTranSparse (Ji et al., 2016)\n223\n80.1\n-\n187\n53.5\n-\n211\n93.2\n-\n82\n79.5\n-\nTATEC (Garc\u00b4\u0131a-Dur\u00b4an et al., 2016)\n-\n-\n-\n-\n-\n-\n-\n-\n-\n58\n76.7\n-\nNTN (Socher et al., 2013)\n-\n-\n-\n-\n-\n-\n-\n66.1\n0.53\n-\n41.4\n0.25\nDISTMULT (Yang et al., 2015)\n-\n-\n-\n-\n-\n-\n-\n94.2\n0.83\n-\n57.7\n0.35\nHolE (Nickel et al., 2016b)\n-\n-\n0.616\n-\n-\n0.232\n-\n94.9\n0.938\n-\n73.9\n0.524\nOur STransE\n217\n80.9\n0.469\n219\n51.6\n0.252\n206\n93.4\n0.657\n69\n79.7\n0.543\nRTransE (Garc\u00b4\u0131a-Dur\u00b4an et al., 2015)\n-\n-\n-\n-\n-\n-\n-\n-\n-\n50\n76.2\n-\nPTransE (Lin et al., 2015a)\n-\n-\n-\n207\n51.4\n-\n-\n-\n-\n58\n84.6\n-\nGAKE (Feng et al., 2016)\n-\n-\n-\n228\n44.5\n-\n-\n-\n-\n119\n64.8\n-\nGaifman (Niepert, 2016)\n-\n-\n-\n-\n-\n-\n352\n93.9\n-\n75\n84.2\n-\nHiri (Liu et al., 2016)\n-\n-\n-\n-\n-\n-\n-\n90.8\n0.691\n-\n70.3\n0.603\nNLFeat (Toutanova and Chen, 2015)\n-\n-\n-\n-\n-\n-\n-\n94.3\n0.940\n-\n87.0\n0.822\nTEKE H (Wang and Li, 2016)\n127\n80.3\n-\n212\n51.2\n-\n114\n92.9\n-\n108\n73.0\n-\nSSP (Xiao et al., 2017)\n168\n81.2\n-\n163\n57.2\n-\n156\n93.2\n-\n82\n79.0\n-\nTable 3: Link prediction results. MR, H10 and MRR denote evaluation metrics of mean rank, Hits@10 (in %) and\nmean reciprocal rank, respectively. \u201cNLFeat\u201d abbreviates Node+LinkFeat. The results for NTN (Socher et al., 2013)\nlisted in this table are taken from Yang et al. (2015) since NTN was originally evaluated on different datasets.\nresults are evaluated using the ranking induced by\nthe score function fr(h, t) on test triples.\nFor each test triple (h, r, t), we corrupted it by re-\nplacing either h or t by each of the possible entities\nin turn, and then rank these candidates in ascend-\ning order of their implausibility value computed by\nthe score function. This is called as the \u201cRaw\u201d set-\nting protocol. For the \u201cFiltered\u201d setting protocol de-\nscribed in Bordes et al. (2013), we removed any cor-\nrupted triples that appear in the knowledge base, to\navoid cases where a correct corrupted triple might\nbe ranked higher than the test triple. The \u201cFiltered\u201d\nsetting thus provides a clearer view on the ranking\nperformance. Following Bordes et al. (2013), we re-\nport the mean rank and the Hits@10 (i.e., the pro-\nportion of test triples in which the target entity was\nranked in the top 10 predictions) for each model. In\naddition, we report the mean reciprocal rank, which\nis commonly used in information retrieval. In both\n\u201cRaw\u201d and \u201cFiltered\u201d settings, lower mean rank,\nhigher mean reciprocal rank or higher Hits@10 in-\ndicates better link prediction performance.\nFollowing TransR (Lin et al., 2015b), TransD (Ji\net al., 2015), RTransE (Garc\u00b4\u0131a-Dur\u00b4an et al., 2015),\nPTransE (Lin et al., 2015a), TATEC (Garc\u00b4\u0131a-Dur\u00b4an\net al., 2016) and TranSparse (Ji et al., 2016), we used\nthe entity and relation vectors produced by TransE\n(Bordes et al., 2013) to initialize the entity and re-\nlation vectors in STransE, and we initialized the re-\nlation matrices with identity matrices. We applied\nthe \u201cBernoulli\u201d trick used also in previous work for\ngenerating head or tail entities when sampling incor-\nrect triples (Wang et al., 2014b; Lin et al., 2015b; He\net al., 2015; Ji et al., 2015; Lin et al., 2015a; Yoon\net al., 2016; Ji et al., 2016). We ran SGD for 2,000\nepochs to estimate the model parameters. Following\nBordes et al. (2013) we used a grid search on vali-\ndation set to choose either the l1 or l2 norm in the\nscore function f, as well as to set the SGD learning\nrate \u03bb \u2208{0.0001, 0.0005, 0.001, 0.005, 0.01}, the\nmargin hyper-parameter \u03b3 \u2208{1, 3, 5} and the vector\nsize k \u2208{50, 100}. The lowest \ufb01ltered mean rank\non the validation set was obtained when using the\nl1 norm in f on both WN18 and FB15k, and when\n\u03bb = 0.0005, \u03b3 = 5, and k = 50 for WN18, and\n\u03bb = 0.0001, \u03b3 = 1, and k = 100 for FB15k.\n4.2\nMain results\nTable 3 compares the link prediction results of our\nSTransE model with results reported in prior work,\nusing the same experimental setup. The \ufb01rst 15 rows\nreport the performance of the models that do not\nexploit information about alternative paths between\nhead and tail entities. The next 5 rows report results\nof the models that exploit information about relation\npaths. The last 3 rows present results for the models\nwhich make use of textual mentions derived from a\nlarge external corpus.\nIt is clear that the models with the additional ex-\nternal corpus information obtained best results. In\nfuture work we plan to extend the STransE model\nto incorporate such additional information. Table 3\nalso shows that the models employing path infor-\nmation generally achieve better results than mod-\nels that do not use such information. In terms of\nmodels not exploiting path information or exter-\nnal information, the STransE model produces the\nhighest \ufb01ltered mean rank on WN18 and the high-\nest \ufb01ltered Hits@10 and mean reciprocal rank on\nFB15k. Compared to the closely related models SE,\nTransE, TransR, CTransR, TransD and TranSparse,\nour STransE model does better than these models on\nboth WN18 and FB15k.\nFollowing Bordes et al. (2013), Table 4 analyzes\nHits@10 results on FB15k with respect to the re-\nlation categories de\ufb01ned as follows: for each rela-\ntion type r, we computed the averaged number ah of\nheads h for a pair (r, t) and the averaged number at\nof tails t for a pair (h, r). If ah < 1.5 and at < 1.5,\nthen r is labeled 1-1. If ah \u22651.5 and at < 1.5, then\nr is labeled M-1. If ah < 1.5 and at \u22651.5, then r is\nlabeled as 1-M. If ah \u22651.5 and at \u22651.5, then r is\nlabeled as M-M. 1.4%, 8.9%, 14.6% and 75.1% of\nthe test triples belong to a relation type classi\ufb01ed as\n1-1, 1-M, M-1 and M-M, respectively.\nTable 4 shows that in comparison to prior mod-\nels not using path information, STransE obtains the\nsecond highest Hits@10 result for M-M relation cat-\negory at (80.1% + 83.1%)/2 = 81.6% which is\n0.5% smaller than the Hits@10 result of TranSparse\nfor M-M. However, STransE obtains 2.5% higher\nHits@10 result than TranSparse for M-1. In addi-\nMethod\nPredicting head h\nPredicting tail t\n1-1\n1-M M-1 M-M 1-1\n1-M M-1 M-M\nSE\n35.6 62.6 17.2 37.5\n34.9 14.6 68.3 41.3\nUnstr.\n34.5 2.5\n6.1\n6.6\n34.3 4.2\n1.9\n6.6\nTransE\n43.7 65.7 18.2 47.2\n43.7 19.7 66.7 50.0\nTransH\n66.8 87.6 28.7 64.5\n65.5 39.8 83.3 67.2\nTransR\n78.8 89.2 34.1 69.2\n79.2 37.4 90.4 72.1\nCTransR\n81.5 89.0 34.7 71.2\n80.8 38.6 90.1 73.8\nKG2E\n92.3 94.6 66.0 69.6\n92.6 67.9 94.4 73.4\nTATEC\n79.3 93.2 42.3 77.2\n78.5 51.5 92.7 80.7\nTransD\n86.1 95.5 39.8 78.5\n85.4 50.6 94.4 81.2\nlppTransD 86.0 94.2 54.4 82.2\n79.7 43.2 95.3 79.7\nTranSparse 86.8 95.5 44.3 80.9\n86.6 56.6 94.4 83.3\nSTransE\n82.8 94.2 50.4 80.1\n82.4 56.9 93.4 83.1\nTable 4: Hits@10 (in %) by the relation category on\nFB15k. \u201cUnstr.\u201d abbreviates Unstructured.\ntion, STransE also performs better than TransD for\n1-M and M-1 relation categories. We believe the\nimproved performance of the STransE model is due\nto its use of full matrices, rather than just projection\nvectors as in TransD. This permits STransE to model\ndiverse and complex relation categories (such as 1-\nM, M-1 and especially M-M) better than TransD\nand other similiar models. However, STransE is not\nas good as TransD for the 1-1 relations. Perhaps the\nextra parameters in STransE hurt performance in this\ncase (note that 1-1 relations are relatively rare, so\nSTransE does better overall).\n5\nConclusion and future work\nThis paper presented a new embedding model for\nlink prediction and KB completion. Our STransE\ncombines insights from several simpler embed-\nding models, speci\ufb01cally the Structured Embedding\nmodel (Bordes et al., 2011) and the TransE model\n(Bordes et al., 2013), by using a low-dimensional\nvector and two projection matrices to represent each\nrelation.\nSTransE, while being conceptually sim-\nple, produces highly competitive results on standard\nlink prediction evaluations, and scores better than\nthe embedding-based models it builds on. Thus it\nis a suitable candidate for serving as future baseline\nfor more complex models in the link prediction task.\nIn future work we plan to extend STransE to ex-\nploit relation path information in knowledge bases,\nin a manner similar to Lin et al. (2015a), Guu et al.\n(2015) or Nguyen et al. (2016).\nAcknowledgments\nThis research was supported by a Google award\nthrough the Natural Language Understanding Fo-\ncused Program,\nand under the Australian Re-\nsearch Council\u2019s Discovery Projects funding scheme\n(project number DP160102156).\nNICTA is funded by the Australian Government\nthrough the Department of Communications and the\nAustralian Research Council through the ICT Centre\nof Excellence Program. The \ufb01rst author is supported\nby an International Postgraduate Research Scholar-\nship and a NICTA NRPA Top-Up Scholarship.\nReferences\n[Angeli and Manning2013] Gabor Angeli and Christo-\npher Manning. 2013. Philosophers are Mortal: In-\nferring the Truth of Unseen Facts. In Proceedings of\nthe Seventeenth Conference on Computational Natural\nLanguage Learning, pages 133\u2013142.\n[Bollacker et al.2008] Kurt\nBollacker,\nColin\nEvans,\nPraveen Paritosh, Tim Sturge, and Jamie Taylor.\n2008.\nFreebase: A Collaboratively Created Graph\nDatabase for Structuring Human Knowledge.\nIn\nProceedings of the 2008 ACM SIGMOD Interna-\ntional Conference on Management of Data, pages\n1247\u20131250.\n[Bordes et al.2011] Antoine Bordes, Jason Weston, Ro-\nnan Collobert, and Yoshua Bengio. 2011. Learning\nStructured Embeddings of Knowledge Bases. In Pro-\nceedings of the Twenty-Fifth AAAI Conference on Ar-\nti\ufb01cial Intelligence, pages 301\u2013306.\n[Bordes et al.2012] Antoine Bordes, Xavier Glorot, Ja-\nson Weston, and Yoshua Bengio. 2012. A Semantic\nMatching Energy Function for Learning with Multi-\nrelational Data. Machine Learning, 94(2):233\u2013259.\n[Bordes et al.2013] Antoine Bordes,\nNicolas Usunier,\nAlberto Garcia-Duran, Jason Weston, and Oksana\nYakhnenko. 2013. Translating Embeddings for Mod-\neling Multi-relational Data. In Advances in Neural In-\nformation Processing Systems 26, pages 2787\u20132795.\n[Das et al.2017] Rajarshi\nDas,\nArvind\nNeelakantan,\nDavid Belanger, and Andrew McCallum.\n2017.\nChains of reasoning over entities, relations, and text\nusing recurrent neural networks.\nIn Proceedings of\nthe 15th Conference of the European Chapter of the\nAssociation for Computational Linguistics.\n[Duchi et al.2011] John Duchi, Elad Hazan, and Yoram\nSinger. 2011. Adaptive Subgradient Methods for On-\nline Learning and Stochastic Optimization. The Jour-\nnal of Machine Learning Research, 12:2121\u20132159.\n[Fellbaum1998] Christiane D. Fellbaum. 1998. WordNet:\nAn Electronic Lexical Database. MIT Press.\n[Feng et al.2016] Jun Feng, Minlie Huang, Yang Yang,\nand xiaoyan zhu. 2016. GAKE: Graph Aware Knowl-\nedge Embedding. In Proceedings of COLING 2016,\nthe 26th International Conference on Computational\nLinguistics: Technical Papers, pages 641\u2013651.\n[Garc\u00b4\u0131a-Dur\u00b4an et al.2015] Alberto\nGarc\u00b4\u0131a-Dur\u00b4an,\nAn-\ntoine Bordes, and Nicolas Usunier. 2015. Composing\nRelationships with Translations. In Proceedings of the\n2015 Conference on Empirical Methods in Natural\nLanguage Processing, pages 286\u2013290.\n[Garc\u00b4\u0131a-Dur\u00b4an et al.2016] Alberto\nGarc\u00b4\u0131a-Dur\u00b4an,\nAn-\ntoine Bordes, Nicolas Usunier, and Yves Grandvalet.\n2016.\nCombining Two and Three-Way Embed-\nding Models for Link Prediction in Knowledge\nBases.\nJournal of Arti\ufb01cial Intelligence Research,\n55:715\u2013742.\n[Guu et al.2015] Kelvin Guu, John Miller, and Percy\nLiang. 2015. Traversing Knowledge Graphs in Vec-\ntor Space. In Proceedings of the 2015 Conference on\nEmpirical Methods in Natural Language Processing,\npages 318\u2013327.\n[He et al.2015] Shizhu He, Kang Liu, Guoliang Ji, and\nJun Zhao. 2015. Learning to Represent Knowledge\nGraphs with Gaussian Embedding. In Proceedings of\nthe 24th ACM International on Conference on Infor-\nmation and Knowledge Management, pages 623\u2013632.\n[Jenatton et al.2012] Rodolphe\nJenatton,\nNicolas\nL.\nRoux, Antoine Bordes, and Guillaume R Obozinski.\n2012. A latent factor model for highly multi-relational\ndata. In Advances in Neural Information Processing\nSystems 25, pages 3167\u20133175.\n[Ji et al.2015] Guoliang Ji, Shizhu He, Liheng Xu, Kang\nLiu, and Jun Zhao. 2015. Knowledge Graph Embed-\nding via Dynamic Mapping Matrix. In Proceedings of\nthe 53rd Annual Meeting of the Association for Com-\nputational Linguistics and the 7th International Joint\nConference on Natural Language Processing (Volume\n1: Long Papers), pages 687\u2013696.\n[Ji et al.2016] Guoliang Ji, Kang Liu, Shizhu He, and Jun\nZhao.\n2016.\nKnowledge Graph Completion with\nAdaptive Sparse Transfer Matrix. In Proceedings of\nthe Thirtieth AAAI Conference on Arti\ufb01cial Intelli-\ngence, pages 985\u2013991.\n[Krompa\u00df et al.2015] Denis Krompa\u00df, Stephan Baier,\nand Volker Tresp. 2015. Type-Constrained Represen-\ntation Learning in Knowledge Graphs. In Proceedings\nof the 14th International Semantic Web Conference,\npages 640\u2013655.\n[Lehmann et al.2015] Jens Lehmann, Robert Isele, Max\nJakob, Anja Jentzsch, Dimitris Kontokostas, Pablo N.\nMendes, Sebastian Hellmann, Mohamed Morsey,\nPatrick van Kleef, S\u00a8oren Auer, and Christian Bizer.\n2015. DBpedia - A Large-scale, Multilingual Knowl-\nedge Base Extracted from Wikipedia. Semantic Web,\n6(2):167\u2013195.\n[Lin et al.2015a] Yankai Lin, Zhiyuan Liu, Huanbo Luan,\nMaosong Sun, Siwei Rao, and Song Liu. 2015a. Mod-\neling Relation Paths for Representation Learning of\nKnowledge Bases. In Proceedings of the 2015 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 705\u2013714.\n[Lin et al.2015b] Yankai Lin, Zhiyuan Liu, Maosong Sun,\nYang Liu, and Xuan Zhu.\n2015b.\nLearning Entity\nand Relation Embeddings for Knowledge Graph Com-\npletion.\nIn Proceedings of the Twenty-Ninth AAAI\nConference on Arti\ufb01cial Intelligence Learning, pages\n2181\u20132187.\n[Liu and Nocedal1989] D. C. Liu and J. Nocedal. 1989.\nOn the Limited Memory BFGS Method for Large\nScale Optimization.\nMathematical Programming,\n45(3):503\u2013528.\n[Liu et al.2016] Qiao Liu, Liuyi Jiang, Minghao Han, Yao\nLiu, and Zhiguang Qin. 2016. Hierarchical Random\nWalk Inference in Knowledge Graphs. In Proceedings\nof the 39th International ACM SIGIR Conference on\nResearch and Development in Information Retrieval,\npages 445\u2013454.\n[Mikolov et al.2013] Tomas Mikolov, Wen-tau Yih, and\nGeoffrey Zweig.\n2013.\nLinguistic Regularities in\nContinuous Space Word Representations. In Proceed-\nings of the 2013 Conference of the North American\nChapter of the Association for Computational Linguis-\ntics: Human Language Technologies, pages 746\u2013751.\n[Nguyen et al.2016] Dat Quoc Nguyen,\nKairit Sirts,\nLizhen Qu, and Mark Johnson. 2016. Neighborhood\nMixture Model for Knowledge Base Completion. In\nProceedings of The 20th SIGNLL Conference on Com-\nputational Natural Language Learning, pages 40\u201350.\n[Nickel et al.2011] Maximilian Nickel, Volker Tresp, and\nHans-Peter Kriegel. 2011. A Three-Way Model for\nCollective Learning on Multi-Relational Data. In Pro-\nceedings of the 28th International Conference on Ma-\nchine Learning, pages 809\u2013816.\n[Nickel et al.2016a] Maximilian Nickel, Kevin Murphy,\nVolker Tresp, and Evgeniy Gabrilovich. 2016a. A Re-\nview of Relational Machine Learning for Knowledge\nGraphs. Proceedings of the IEEE, 104(1):11\u201333.\n[Nickel et al.2016b] Maximilian\nNickel,\nLorenzo\nRosasco, and Tomaso Poggio.\n2016b.\nHolo-\ngraphic embeddings of knowledge graphs.\nIn\nProceedings of the Thirtieth AAAI Conference on\nArti\ufb01cial Intelligence, pages 1955\u20131961.\n[Niepert2016] Mathias Niepert.\n2016.\nDiscriminative\nGaifman Models. In Advances in Neural Information\nProcessing Systems 29, pages 3405\u20133413.\n[Riedel et al.2013] Sebastian Riedel, Limin Yao, Andrew\nMcCallum, and Benjamin M. Marlin.\n2013.\nRela-\ntion Extraction with Matrix Factorization and Univer-\nsal Schemas. In Proceedings of the 2013 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 74\u201384.\n[Shi and Weninger2017] Baoxu Shi and Tim Weninger.\n2017.\nProjE: Embedding Projection for Knowledge\nGraph Completion. In Proceedings of the 31st AAAI\nConference on Arti\ufb01cial Intelligence.\n[Socher et al.2013] Richard Socher, Danqi Chen, Christo-\npher D Manning, and Andrew Ng.\n2013.\nReason-\ning With Neural Tensor Networks for Knowledge Base\nCompletion. In Advances in Neural Information Pro-\ncessing Systems 26, pages 926\u2013934.\n[Suchanek et al.2007] Fabian M. Suchanek, Gjergji Kas-\nneci, and Gerhard Weikum. 2007. YAGO: A Core\nof Semantic Knowledge. In Proceedings of the 16th\nInternational Conference on World Wide Web, pages\n697\u2013706.\n[Taskar et al.2004] Ben Taskar, Ming fai Wong, Pieter\nAbbeel, and Daphne Koller. 2004. Link Prediction in\nRelational Data. In Advances in Neural Information\nProcessing Systems 16, pages 659\u2013666.\n[Tay et al.2017] Yi Tay, Anh Tuan Luu, Siu Cheung Hui,\nand Falk Brauer.\n2017.\nRandom Semantic Tensor\nEnsemble for Scalable Knowledge Graph Link Predic-\ntion. In Proceedings of the Tenth ACM International\nConference on Web Search and Data Mining, pages\n751\u2013760.\n[Toutanova and Chen2015] Kristina Toutanova and Danqi\nChen.\n2015.\nObserved Versus Latent Features for\nKnowledge Base and Text Inference. In Proceedings\nof the 3rd Workshop on Continuous Vector Space Mod-\nels and their Compositionality, pages 57\u201366.\n[Toutanova et al.2015] Kristina Toutanova, Danqi Chen,\nPatrick Pantel, Hoifung Poon, Pallavi Choudhury, and\nMichael Gamon. 2015. Representing Text for Joint\nEmbedding of Text and Knowledge Bases.\nIn Pro-\nceedings of the 2015 Conference on Empirical Meth-\nods in Natural Language Processing, pages 1499\u2013\n1509.\n[Toutanova et al.2016] Kristina Toutanova, Victoria Lin,\nWen-tau Yih, Hoifung Poon, and Chris Quirk. 2016.\nCompositional Learning of Embeddings for Relation\nPaths in Knowledge Base and Text. In Proceedings of\nthe 54th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers), pages\n1434\u20131444.\n[Trouillon et al.2016] Th\u00b4eo Trouillon, Johannes Welbl,\nSebastian Riedel,\n\u00b4Eric Gaussier,\nand Guillaume\nBouchard. 2016. Complex Embeddings for Simple\nLink Prediction. In Proceedings of the 33nd Interna-\ntional Conference on Machine Learning, pages 2071\u2013\n2080.\n[Wang and Li2016] Zhigang Wang and Juan-Zi Li. 2016.\nText-Enhanced Representation Learning for Knowl-\nedge Graph. In Proceedings of the Twenty-Fifth In-\nternational Joint Conference on Arti\ufb01cial Intelligence,\npages 1293\u20131299.\n[Wang et al.2014a] Zhen Wang, Jianwen Zhang, Jianlin\nFeng, and Zheng Chen.\n2014a.\nKnowledge Graph\nand Text Jointly Embedding. In Proceedings of the\n2014 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 1591\u20131601.\n[Wang et al.2014b] Zhen Wang, Jianwen Zhang, Jianlin\nFeng, and Zheng Chen.\n2014b.\nKnowledge Graph\nEmbedding by Translating on Hyperplanes. In Pro-\nceedings of the Twenty-Eighth AAAI Conference on\nArti\ufb01cial Intelligence, pages 1112\u20131119.\n[Wang et al.2016] Quan Wang, Jing Liu, Yuanfei Luo,\nBin Wang, and Chin-Yew Lin. 2016. Knowledge Base\nCompletion via Coupled Path Ranking. In Proceed-\nings of the 54th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 1308\u20131318.\n[Wei et al.2016] Zhuoyu Wei, Jun Zhao, and Kang Liu.\n2016. Mining Inference Formulas by Goal-Directed\nRandom Walks. In Proceedings of the 2016 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, pages 1379\u20131388.\n[West et al.2014] Robert\nWest,\nEvgeniy\nGabrilovich,\nKevin Murphy, Shaohua Sun, Rahul Gupta, and\nDekang Lin. 2014. Knowledge Base Completion via\nSearch-based Question Answering. In Proceedings of\nthe 23rd International Conference on World Wide Web,\npages 515\u2013526.\n[Xiao et al.2017] Han Xiao, Minlie Huang, and Xiaoyan\nZhu. 2017. SSP: semantic space projection for knowl-\nedge graph embedding with text descriptions. In Pro-\nceedings of the 31st AAAI Conference on Arti\ufb01cial In-\ntelligence.\n[Yang et al.2015] Bishan Yang, Wen-tau Yih, Xiaodong\nHe, Jianfeng Gao, and Li Deng. 2015. Embedding\nEntities and Relations for Learning and Inference in\nKnowledge Bases. In Proceedings of the International\nConference on Learning Representations.\n[Yoon et al.2016] Hee-Geun Yoon, Hyun-Je Song, Seong-\nBae Park, and Se-Young Park. 2016. A Translation-\nBased Knowledge Graph Embedding Preserving Log-\nical Property of Relations. In Proceedings of the 2016\nConference of the North American Chapter of the As-\nsociation for Computational Linguistics: Human Lan-\nguage Technologies, pages 907\u2013916.\n[Zeiler2012] Matthew D. Zeiler.\n2012.\nADADELTA:\nAn\nAdaptive\nLearning\nRate\nMethod.\nCoRR,\nabs/1212.5701.\n[Zhao et al.2015] Yu Zhao, Sheng Gao, Patrick Gallinari,\nand Jun Guo.\n2015.\nKnowledge Base Completion\nby Learning Pairwise-Interaction Differentiated Em-\nbeddings.\nData Mining and Knowledge Discovery,\n29(5):1486\u20131504.\n",
        "sentence": " dependent space (Bordes et al., 2014; Ji et al., 2015; Lin et al., 2015b; Nguyen et al., 2016b), and then model the translation property in the projected space. For instance, STransE (Nguyen et al., 2016b) utilizes two projection matrices per relation, one for the head entity and the other for the tail entity. Most of the embedding models for knowledge base completion define an energy function fr(h, t) according to the fact\u2019s plausibility (Bordes et al., 2011, 2014, 2013; Socher et al., 2013; Wang et al., 2014; Yang et al., 2015; Guu et al., 2015; Nguyen et al., 2016b). STransE (Nguyen et al., 2016b) extends TransR Previous work (Bordes et al., 2013; Lin et al., 2015b; Yang et al., 2015; Nguyen et al., 2016b) generally constructs a set of corrupted triples by replacing the head entity or tail entity with a random entity uniformly sampled from the KB. STransE (Nguyen et al., 2016b) is the most similar knowledge embedding model to ours except that they use distinct projection matrices for each relation. 7 STransE (Nguyen et al., 2016b) No 206 (244) 93.",
        "context": "ditionally use two projection vectors for represent-\ning each relation. In fact, our STransE model and\nTranSparse (Ji et al., 2016) can be viewed as direct\nextensions of the TransR model, where head and tail\nSTransE: a novel embedding model of entities and relationships\nin knowledge bases\u2217\nDat Quoc Nguyen1, Kairit Sirts1, Lizhen Qu2 and Mark Johnson1\n1 Department of Computing, Macquarie University, Sydney, Australia\nvector, and each relation by two matrices and\na translation vector. STransE is a simple com-\nbination of the SE and TransE models, but it\nobtains better link prediction performance on\ntwo benchmark datasets than previous embed-"
    },
    {
        "title": "A Review of Relational Machine Learning for Knowledge Graphs",
        "author": [
            "Maximilian Nickel",
            "Kevin Murphy",
            "Volker Tresp",
            "Evgeniy Gabrilovich."
        ],
        "venue": "Proceedings of the IEEE, to appear .",
        "citeRegEx": "Nickel et al\\.,? 2015",
        "shortCiteRegEx": "Nickel et al\\.",
        "year": 2015,
        "abstract": "Relational machine learning studies methods for the statistical analysis of\nrelational, or graph-structured, data. In this paper, we provide a review of\nhow such statistical models can be \"trained\" on large knowledge graphs, and\nthen used to predict new facts about the world (which is equivalent to\npredicting new edges in the graph). In particular, we discuss two fundamentally\ndifferent kinds of statistical relational models, both of which can scale to\nmassive datasets. The first is based on latent feature models such as tensor\nfactorization and multiway neural networks. The second is based on mining\nobservable patterns in the graph. We also show how to combine these latent and\nobservable models to get improved modeling power at decreased computational\ncost. Finally, we discuss how such statistical models of graphs can be combined\nwith text-based information extraction methods for automatically constructing\nknowledge graphs from the Web. To this end, we also discuss Google's Knowledge\nVault project as an example of such combination.",
        "full_text": "1\nA Review of Relational Machine Learning\nfor Knowledge Graphs\nMaximilian Nickel, Kevin Murphy, Volker Tresp, Evgeniy Gabrilovich\nAbstract\u2014Relational machine learning studies methods for the\nstatistical analysis of relational, or graph-structured, data. In this\npaper, we provide a review of how such statistical models can be\n\u201ctrained\u201d on large knowledge graphs, and then used to predict\nnew facts about the world (which is equivalent to predicting new\nedges in the graph). In particular, we discuss two fundamentally\ndifferent kinds of statistical relational models, both of which can\nscale to massive datasets. The \ufb01rst is based on latent feature mod-\nels such as tensor factorization and multiway neural networks.\nThe second is based on mining observable patterns in the graph.\nWe also show how to combine these latent and observable models\nto get improved modeling power at decreased computational cost.\nFinally, we discuss how such statistical models of graphs can be\ncombined with text-based information extraction methods for\nautomatically constructing knowledge graphs from the Web. To\nthis end, we also discuss Google\u2019s Knowledge Vault project as an\nexample of such combination.\nIndex\nTerms\u2014Statistical\nRelational\nLearning,\nKnowledge\nGraphs, Knowledge Extraction, Latent Feature Models, Graph-\nbased Models\nI. INTRODUCTION\nI am convinced that the crux of the problem of learning\nis recognizing relationships and being able to use them.\nChristopher Strachey in a letter to Alan Turing, 1954\nT\nRADITIONAL machine learning algorithms take as input\na feature vector, which represents an object in terms of\nnumeric or categorical attributes. The main learning task is to\nlearn a mapping from this feature vector to an output prediction\nof some form. This could be class labels, a regression score,\nor an unsupervised cluster id or latent vector (embedding). In\nStatistical Relational Learning (SRL), the representation of an\nobject can contain its relationships to other objects. Thus the\ndata is in the form of a graph, consisting of nodes (entities)\nand labelled edges (relationships between entities). The main\ngoals of SRL include prediction of missing edges, prediction\nof properties of nodes, and clustering nodes based on their\nconnectivity patterns. These tasks arise in many settings such\nas analysis of social networks and biological pathways. For\nfurther information on SRL see [1, 2, 3].\nIn this article, we review a variety of techniques from the\nSRL community and explain how they can be applied to\nlarge-scale knowledge graphs (KGs), i.e., graph structured\nknowledge bases (KBs) that store factual information in\nMaximilian Nickel is with LCSL, Massachusetts Institute of Technology\nand Istituto Italiano di Tecnologia.\nVolker Tresp is with Siemens AG, Corporate Technology and the Ludwig\nMaximilian University Munich.\nKevin Murphy and Evgeniy Gabrilovich are with Google Inc.\nManuscript received April 7, 2015; revised August 14, 2015.\nform of relationships between entities. Recently, a large\nnumber of knowledge graphs have been created, including\nYAGO [4], DBpedia [5], NELL [6], Freebase [7], and the\nGoogle Knowledge Graph [8]. As we discuss in Section II,\nthese graphs contain millions of nodes and billions of edges.\nThis causes us to focus on scalable SRL techniques, which\ntake time that is (at most) linear in the size of the graph.\nWe can apply SRL methods to existing KGs to learn a\nmodel that can predict new facts (edges) given existing facts.\nWe can then combine this approach with information extraction\nmethods that extract \u201cnoisy\u201d facts from the Web (see e.g., [9,\n10]). For example, suppose an information extraction method\nreturns a fact claiming that Barack Obama was born in Kenya,\nand suppose (for illustration purposes) that the true place of\nbirth of Obama was not already stored in the knowledge graph.\nAn SRL model can use related facts about Obama (such as his\nprofession being US President) to infer that this new fact is\nunlikely to be true and should be discarded. This provides us\na way to \u201cgrow\u201d a KG automatically, as we explain in more\ndetail in Section IX.\nThe remainder of this paper is structured as follows. In\nSection II we introduce knowledge graphs and some of their\nproperties. Section III discusses SRL and how it can be applied\nto knowledge graphs. There are two main classes of SRL\ntechniques: those that capture the correlation between the\nnodes/edges using latent variables, and those that capture\nthe correlation directly using statistical models based on the\nobservable properties of the graph. We discuss these two\nfamilies in Section IV and Section V, respectively. Section VI\ndescribes methods for combining these two approaches, in\norder to get the best of both worlds. Section VII discusses\nhow such models can be trained on KGs. In Section VIII we\ndiscuss relational learning using Markov Random Fields. In\nSection IX we describe how SRL can be used in automated\nknowledge base construction projects. In Section X we discuss\nextensions of the presented methods, and Section XI presents\nour conclusions.\nII. KNOWLEDGE GRAPHS\nIn this section, we introduce knowledge graphs, and discuss\nhow they are represented, constructed, and used.\nA. Knowledge representation\nKnowledge graphs model information in the form of entities\nand relationships between them. This kind of relational knowl-\nedge representation has a long history in logic and arti\ufb01cial\nintelligence [11], for example, in semantic networks [12] and\narXiv:1503.00759v3  [stat.ML]  28 Sep 2015\n2\nLeonard Nimoy\nSpock\nStar Trek\nScience Fiction\nStar Wars\nAlec Guinness\nObi-Wan Kenobi\nstarredIn\nplayed\ncharacterIn\ngenre\nstarredIn\nplayed\ncharacterIn\ngenre\nFig. 1. Sample knowledge graph. Nodes represent entities, edge labels represent\ntypes of relations, edges represent existing relationships.\nframes [13]. More recently, it has been used in the Semantic\nWeb community with the purpose of creating a \u201cweb of data\u201d\nthat is readable by machines [14]. While this vision of the\nSemantic Web remains to be fully realized, parts of it have\nbeen achieved. In particular, the concept of linked data [15, 16]\nhas gained traction, as it facilitates publishing and interlinking\ndata on the Web in relational form using the W3C Resource\nDescription Framework (RDF) [17, 18]. (For an introduction\nto knowledge representation, see e.g. [11, 19, 20]).\nIn this article, we will loosely follow the RDF standard and\nrepresent facts in the form of binary relationships, in particular\n(subject, predicate, object) (SPO) triples, where subject and\nobject are entities and predicate is the relation between\nthem. (We discuss how to represent higher-arity relations\nin Section X-A.) The existence of a particular SPO triple\nindicates an existing fact, i.e., that the respective entities are in\na relationship of the given type. For instance, the information\nLeonard Nimoy was an actor who played the char-\nacter Spock in the science-\ufb01ction movie Star Trek\ncan be expressed via the following set of SPO triples:\nsubject\npredicate\nobject\n(LeonardNimoy,\nprofession,\nActor)\n(LeonardNimoy,\nstarredIn,\nStarTrek)\n(LeonardNimoy,\nplayed,\nSpock)\n(Spock,\ncharacterIn,\nStarTrek)\n(StarTrek,\ngenre,\nScienceFiction)\nWe can combine all the SPO triples together to form a multi-\ngraph, where nodes represent entities (all subjects and objects),\nand directed edges represent relationships. The direction of an\nedge indicates whether entities occur as subjects or objects, i.e.,\nan edge points from the subject to the object. Different relations\nare represented via different types of edges (also called edge\nlabels). This construction is called a knowledge graph (KG),\nor sometimes a heterogeneous information network [21].) See\nFigure 1 for an example.\nIn addition to being a collection of facts, knowledge graphs\noften provide type hierarchies (Leonard Nimoy is an actor,\nwhich is a person, which is a living thing) and type constraints\n(e.g., a person can only marry another person, not a thing).\nB. Open vs. closed world assumption\nWhile existing triples always encode known true relationships\n(facts), there are different paradigms for the interpretation of\nTABLE I\nKNOWLEDGE BASE CONSTRUCTION PROJECTS\nMethod\nSchema\nExamples\nCurated\nYes\nCyc/OpenCyc [23], WordNet [24],\nUMLS [25]\nCollaborative\nYes\nWikidata [26], Freebase [7]\nAuto. Semi-Structured\nYes\nYAGO [4, 27], DBPedia [5],\nFreebase [7]\nAuto. Unstructured\nYes\nKnowledge Vault [28], NELL [6],\nPATTY [29], PROSPERA [30],\nDeepDive/Elementary [31]\nAuto. Unstructured\nNo\nReVerb [32], OLLIE [33],\nPRISMATIC [34]\nnon-existing triples:\n\u201a Under the closed world assumption (CWA), non-existing\ntriples indicate false relationships. For example, the fact\nthat in Figure 1 there is no starredIn edge from Leonard\nNimoy to Star Wars is interpreted to mean that Nimoy\nde\ufb01nitely did not star in this movie.\n\u201a Under the open world assumption (OWA), a non-existing\ntriple is interpreted as unknown, i.e., the corresponding\nrelationship can be either true or false. Continuing with the\nabove example, the missing edge is not interpreted to mean\nthat Nimoy did not star in Star Wars. This more cautious\napproach is justi\ufb01ed, since KGs are known to be very\nincomplete. For example, sometimes just the main actors\nin a movie are listed, not the complete cast. As another\nexample, note that even the place of birth attribute, which\nyou might think would be typically known, is missing for\n71% of all people included in Freebase [22].\nRDF and the Semantic Web make the open-world assumption.\nIn Section VII-B we also discuss the local closed world\nassumption (LCWA), which is often used for training relational\nmodels.\nC. Knowledge base construction\nCompleteness, accuracy, and data quality are important\nparameters that determine the usefulness of knowledge bases\nand are in\ufb02uenced by the way knowledge bases are constructed.\nWe can classify KB construction methods into four main\ngroups:\n\u201a In curated approaches, triples are created manually by a\nclosed group of experts.\n\u201a In collaborative approaches, triples are created manually\nby an open group of volunteers.\n\u201a In automated semi-structured approaches, triples are\nextracted automatically from semi-structured text (e.g.,\ninfoboxes in Wikipedia) via hand-crafted rules, learned\nrules, or regular expressions.\n\u201a In automated unstructured approaches, triples are ex-\ntracted automatically from unstructured text via machine\nlearning and natural language processing techniques (see,\ne.g., [9] for a review).\nConstruction of curated knowledge bases typically leads to\nhighly accurate results, but this technique does not scale well\n3\ndue to its dependence on human experts. Collaborative knowl-\nedge base construction, which was used to build Wikipedia\nand Freebase, scales better but still has some limitations. For\ninstance, as mentioned previously, the place of birth attribute\nis missing for 71% of all people included in Freebase, even\nthough this is a mandatory property of the schema [22]. Also,\na recent study [35] found that the growth of Wikipedia has\nbeen slowing down. Consequently, automatic knowledge base\nconstruction methods have been gaining more attention.\nSuch methods can be grouped into two main approaches. The\n\ufb01rst approach exploits semi-structured data, such as Wikipedia\ninfoboxes, which has led to large, highly accurate knowledge\ngraphs such as YAGO [4, 27] and DBpedia [5]. The accuracy\n(trustworthiness) of facts in such automatically created KGs is\noften still very high. For instance, the accuracy of YAGO2 has\nbeen estimated1 to be over 95% through manual inspection\nof sample facts [36], and the accuracy of Freebase [7] was\nestimated to be 99%2. However, semi-structured text still covers\nonly a small fraction of the information stored on the Web, and\ncompleteness (or coverage) is another important aspect of KGs.\nHence the second approach tries to \u201cread the Web\u201d, extracting\nfacts from the natural language text of Web pages. Example\nprojects in this category include NELL [6] and the Knowledge\nVault [28]. In Section IX, we show how we can reduce the\nlevel of \u201cnoise\u201d in such automatically extracted facts by using\nthe knowledge from existing, high-quality repositories.\nKGs, and more generally KBs, can also be classi\ufb01ed based\non whether they employ a \ufb01xed or open lexicon of entities and\nrelations. In particular, we distinguish two main types of KBs:\n\u201a In schema-based approaches, entities and relations are\nrepresented via globally unique identi\ufb01ers and all pos-\nsible relations are prede\ufb01ned in a \ufb01xed vocabulary. For\nexample, Freebase might represent the fact that Barack\nObama was born in Hawaii using the triple (/m/02mjmr,\n/people/person/born-in, /m/03gh4), where /m/02mjmr is\nthe unique machine ID for Barack Obama.\n\u201a In schema-free approaches, entities and relations are\nidenti\ufb01ed using open information extraction (OpenIE)\ntechniques [37], and represented via normalized but not\ndisambiguated strings (also referred to as surface names).\nFor example, an OpenIE system may contain triples such\nas (\u201cObama\u201d, \u201cborn in\u201d, \u201cHawaii\u201d), (\u201cBarack Obama\u201d,\n\u201cplace of birth\u201d, \u201cHonolulu\u201d), etc. Note that it is not clear\nfrom this representation whether the \ufb01rst triple refers to\nthe same person as the second triple, nor whether \u201cborn\nin\u201d means the same thing as \u201cplace of birth\u201d. This is the\nmain disadvantage of OpenIE systems.\n1For\ndetailed\nstatistics\nsee\nhttp://www.mpi-inf.mpg.de/departments/\ndatabases-and-information-systems/research/yago-naga/yago/statistics/\n2http://thenoisychannel.com/2011/11/15/cikm-2011-industry-event-john-\ngiannandrea-on-freebase-a-rosetta-stone-for-entities\n3Non-redundant triples, see [28, Table 1]\n4Last published numbers: https://tools.wm\ufb02abs.org/wikidata-todo/stats.php\nand https://www.wikidata.org/wiki/Category:All_Properties\n5English content, Version 2014 from http://wiki.dbpedia.org/data-set-2014\n6See [27, Table 5]\n7Last\npublished\nnumbers:\nhttp://insidesearch.blogspot.de/2012/12/get-\nsmarter-answers-from-knowledge_4.html\nTABLE II\nSIZE OF SOME SCHEMA-BASED KNOWLEDGE BASES\nNumber of\nKnowledge Graph\nEntities\nRelation Types\nFacts\nFreebase3\n40 M\n35,000\n637 M\nWikidata4\n18 M\n1,632\n66 M\nDBpedia (en)5\n4.6 M\n1,367\n538 M\nYAGO2 6\n9.8 M\n114\n447 M\nGoogle Knowledge Graph7\n570 M\n35,000\n18,000 M\nTable I lists current knowledge base construction projects\nclassi\ufb01ed by their creation method and data schema. In this\npaper, we will only focus on schema-based KBs. Table II shows\na selection of such KBs and their sizes.\nD. Uses of knowledge graphs\nKnowledge graphs provide semantically structured informa-\ntion that is interpretable by computers \u2014 a property that is\nregarded as an important ingredient to build more intelligent\nmachines [38]. Consequently, knowledge graphs are already\npowering multiple \u201cBig Data\u201d applications in a variety of\ncommercial and scienti\ufb01c domains. A prime example is the\nintegration of Google\u2019s Knowledge Graph, which currently\nstores 18 billion facts about 570 million entities, into the\nresults of Google\u2019s search engine [8]. The Google Knowledge\nGraph is used to identify and disambiguate entities in text, to\nenrich search results with semantically structured summaries,\nand to provide links to related entities in exploratory search.\n(Microsoft has a similar KB, called Satori, integrated with its\nBing search engine [39].)\nEnhancing search results with semantic information from\nknowledge graphs can be seen as an important step to transform\ntext-based search engines into semantically aware question\nanswering services. Another prominent example demonstrating\nthe value of knowledge graphs is IBM\u2019s question answering\nsystem Watson, which was able to beat human experts in the\ngame of Jeopardy!. Among others, this system used YAGO,\nDBpedia, and Freebase as its sources of information [40].\nRepositories of structured knowledge are also an indispensable\ncomponent of digital assistants such as Siri, Cortana, or Google\nNow.\nKnowledge graphs are also used in several specialized\ndomains. For instance, Bio2RDF [41], Neurocommons [42],\nand LinkedLifeData [43] are knowledge graphs that integrate\nmultiple sources of biomedical information. These have been\nused for question answering and decision support in the life\nsciences.\nE. Main tasks in knowledge graph construction and curation\nIn this section, we review a number of typical KG tasks.\nLink prediction is concerned with predicting the existence (or\nprobability of correctness) of (typed) edges in the graph (i.e.,\ntriples). This is important since existing knowledge graphs are\noften missing many facts, and some of the edges they contain\nare incorrect [44]. In the context of knowledge graphs, link\n4\nprediction is also referred to as knowledge graph completion.\nFor example, in Figure 1, suppose the characterIn edge from\nObi-Wan Kenobi to Star Wars were missing; we might be able\nto predict this missing edge, based on the structural similarity\nbetween this part of the graph and the part involving Spock\nand Star Trek. It has been shown that relational models that\ntake the relationships of entities into account can signi\ufb01cantly\noutperform non-relational machine learning methods for this\ntask (e.g., see [45, 46]).\nEntity resolution (also known as record linkage [47],\nobject identi\ufb01cation [48], instance matching [49], and de-\nduplication [50]) is the problem of identifying which objects\nin relational data refer to the same underlying entities. See\nFigure 2 for a small example. In a relational setting, the\ndecisions about which objects are assumed to be identical\ncan propagate through the graph, so that matching decisions\nare made collectively for all objects in a domain rather\nthan independently for each object pair (see, for example,\n[51, 52, 53]). In schema-based automated knowledge base\nconstruction, entity resolution can be used to match the\nextracted surface names to entities stored in the knowledge\ngraph.\nLink-based clustering extends feature-based clustering to a\nrelational learning setting and groups entities in relational data\nbased on their similarity. However, in link-based clustering,\nentities are not only grouped by the similarity of their features\nbut also by the similarity of their links. As in entity resolution,\nthe similarity of entities can propagate through the knowledge\ngraph, such that relational modeling can add important infor-\nmation for this task. In social network analysis, link-based\nclustering is also known as community detection [54].\nIII. STATISTICAL RELATIONAL LEARNING FOR\nKNOWLEDGE GRAPHS\nStatistical Relational Learning is concerned with the creation\nof statistical models for relational data. In the following sections\nwe discuss how statistical relational learning can be applied\nto knowledge graphs. We will assume that all the entities\nand (types of) relations in a knowledge graph are known. (We\ndiscuss extensions of this assumption in Section X-C). However,\ntriples are assumed to be incomplete and noisy; entities and\nrelation types may contain duplicates.\nNotation: Before proceeding, let us de\ufb01ne our mathematical\nnotation. (Variable names will be introduced later in the\nappropriate sections.) We denote scalars by lower case letters,\nsuch as a; column vectors (of size N) by bold lower case letters,\nsuch as a; matrices (of size N1\u02c6N2) by bold upper case letters,\nsuch as A; and tensors (of size N1 \u02c6 N2 \u02c6 N3) by bold upper\ncase letters with an underscore, such as A. We denote the\nk\u2019th \u201cfrontal slice\u201d of a tensor A by Ak (which is a matrix of\nsize N1 \u02c6 N2), and the pi, j, kq\u2019th element by aijk (which is a\nscalar). We use ra; bs to denote the vertical stacking of vectors\na and b, i.e., ra; bs \u201c\n\u02c6\na\nb\n\u02d9\n. We can convert a matrix A of size\nN1 \u02c6N2 into a vector a of size N1N2 by stacking all columns\nof A, denoted a \u201c vec pAq. The inner (scalar) product of two\nvectors (both of size N) is de\ufb01ned by aJb \u201c \u0159N\ni\u201c1 aibi. The\ntensor (Kronecker) product of two vectors (of size N1 and N2)\ni-th entity\nj-th entity\nk-th relation\nY\nyijk\nFig. 3. Tensor representation of binary relational data.\nis a vector of size N1N2 with entries abb \u201c\n\u00a8\n\u02da\n\u02dd\na1b\n...\naN1b\n\u02db\n\u2039\u201a. Matrix\nmultiplication is denoted by AB as usual. We denote the L2\nnorm of a vector by ||a||2 \u201c\na\u0159\ni a2\ni , and the Frobenius norm\nof a matrix by ||A||F \u201c\nb\u0159\ni\n\u0159\nj a2\nij. We denote the vector\nof all ones by 1, and the identity matrix by I.\nA. Probabilistic knowledge graphs\nWe now introduce some mathematical background so we can\nmore formally de\ufb01ne statistical models for knowledge graphs.\nLet E\n\u201c te1, . . . , eNeu be the set of all entities and\nR \u201c tr1, . . . , rNru be the set of all relation types in a knowl-\nedge graph. We model each possible triple xijk \u201c pei, rk, ejq\nover this set of entities and relations as a binary random variable\nyijk P t0, 1u that indicates its existence. All possible triples in\nE \u02c6 R \u02c6 E can be grouped naturally in a third-order tensor\n(three-way array) Y P t0, 1uNe\u02c6Ne\u02c6Nr, whose entries are set\nsuch that\nyijk \u201c\n#\n1,\nif the triple pei, rk, ejq exists\n0,\notherwise.\nWe will refer to this construction as an adjacency tensor (cf.\nFigure 3). Each possible realization of Y can be interpreted as\na possible world. To derive a model for the entire knowledge\ngraph, we are then interested in estimating the joint distribution\nPpYq, from a subset D \u010e E \u02c6 R \u02c6 E \u02c6 t0, 1u of observed\ntriples. In doing so, we are estimating a probability distribution\nover possible worlds, which allows us to predict the probability\nof triples based on the state of the entire knowledge graph.\nWhile yijk \u201c 1 in adjacency tensors indicates the existence of\na triple, the interpretation of yijk \u201c 0 depends on whether the\nopen world, closed world, or local-closed world assumption is\nmade. For details, see Section VII-B.\nNote that the size of Y can be enormous for large knowledge\ngraphs. For instance, in the case of Freebase, which currently\nconsists of over 40 million entities and 35, 000 relations, the\nnumber of possible triples |E \u02c6 R \u02c6 E| exceeds 1019 elements.\nOf course, type constraints reduce this number considerably.\nEven amongst the syntactically valid triples, only a tiny\nfraction are likely to be true. For example, there are over\n450,000 thousands actors and over 250,000 movies stored in\nFreebase. But each actor stars only in a small number of movies.\nTherefore, an important issue for SRL on knowledge graphs is\nhow to deal with the large number of possible relationships\nwhile ef\ufb01ciently exploiting the sparsity of relationships. Ideally,\n5\nThe Bridge on the River Kwai\nStar Wars\nDoctor Zhivago\nGuinness\nMovie\nBeer\n1\nA. Guinness\n2\nArthur Guinness\n3\nAlec Guinness\nknownFor\nknownFor\nknownFor\nknownFor\ntype\ntype\ntype\ntype\nFig. 2. Example of entity resolution in a toy knowledge graph. In this example, nodes 1 and 3 refer to the identical entity, the actor Alec Guinness. Node 2 on\nthe other hand refers to Arthur Guinness, the founder of the Guinness brewery. The surface name of node 2 (\u201cA. Guinness\u201d) alone would not be suf\ufb01cient to\nperform a correct matching as it could refer to both Alec Guinness and Arthur Guinness. However, since links in the graph reveal the occupations of the\npersons, a relational approach can perform the correct matching.\na relational model for large-scale knowledge graphs should\nscale at most linearly with the data size, i.e., linearly in the\nnumber of entities Ne, linearly in the number of relations Nr,\nand linearly in the number of observed triples |D| \u201c Nd.\nB. Statistical properties of knowledge graphs\nKnowledge graphs typically adhere to some deterministic\nrules, such as type constraints and transitivity (e.g., if Leonard\nNimoy was born in Boston, and Boston is located in the USA,\nthen we can infer that Leonard Nimoy was born in the USA).\nHowever, KGs have typically also various \u201csofter\u201d statistical\npatterns or regularities, which are not universally true but\nnevertheless have useful predictive power.\nOne example of such statistical pattern is known as ho-\nmophily, that is, the tendency of entities to be related to\nother entities with similar characteristics. This has been widely\nobserved in various social networks [55, 56]. For example,\nUS-born actors are more likely to star in US-made movies. For\nmulti-relational data (graphs with more than one kind of link),\nhomophily has also been referred to as autocorrelation [57].\nAnother statistical pattern is known as block structure. This\nrefers to the property where entities can be divided into distinct\ngroups (blocks), such that all the members of a group have\nsimilar relationships to members of other groups [58, 59, 60].\nFor example, we can group some actors, such as Leonard\nNimoy and Alec Guinness, into a science \ufb01ction actor block,\nand some movies, such as Star Trek and Star Wars, into a\nscience \ufb01ction movie block, since there is a high density of\nlinks from the sci\ufb01actor block to the sci\ufb01movie block.\nGraphs can also exhibit global and long-range statistical\ndependencies, i.e., dependencies that can span over chains of\ntriples and involve different types of relations. For example,\nthe citizenship of Leonard Nimoy (USA) depends statistically\non the city where he was born (Boston), and this dependency\ninvolves a path over multiple entities (Leonard Nimoy, Boston,\nUSA) and relations (bornIn, locatedIn, citizenOf). A distinctive\nfeature of relational learning is that it is able to exploit such\npatterns to create richer and more accurate models of relational\ndomains.\nWhen applying statistical models to incomplete knowledge\ngraphs, it should be noted that the distribution of facts in such\nKGs can be skewed. For instance, KGs that are derived from\nWikipedia will inherit the skew that exists in distribution of\nfacts in Wikipedia itself.8 Statistical models as discussed in\nthe following sections can be affected by such biases in the\ninput data and need to be interpreted accordingly.\nC. Types of SRL models\nAs we discussed, the presence or absence of certain triples\nin relational data is correlated with (i.e., predictive of) the\npresence or absence of certain other triples. In other words,\nthe random variables yijk are correlated with each other. We\nwill discuss three main ways to model these correlations:\nM1)\nAssume all yijk are conditionally independent given\nlatent features associated with subject, object and\nrelation type and additional parameters (latent feature\nmodels)\nM2)\nAssume all yijk are conditionally independent given\nobserved graph features and additional parameters\n(graph feature models)\nM3)\nAssume all yijk have local interactions (Markov\nRandom Fields)\nIn what follows we will mainly focus on M1 and M2 and their\ncombination; M3 will be the topic of Section VIII.\nThe model classes M1 and M2 predict the existence of a\ntriple xijk via a score function fpxijk; \u0398q which represents\nthe model\u2019s con\ufb01dence that a triple exists given the parameters\n\u0398. The conditional independence assumptions of M1 and M2\nallow the probability model to be written as follows:\nPpY|D, \u0398q \u201c\nNe\n\u017a\ni\u201c1\nNe\n\u017a\nj\u201c1\nNr\n\u017a\nk\u201c1\nBerpyijk | \u03c3pfpxijk; \u0398qqq\n(1)\nwhere \u03c3puq \u201c 1{p1 ` e\u00b4uq is the sigmoid (logistic) function,\nand\nBerpy|pq \u201c\n\"\np\nif y \u201c 1\n1 \u00b4 p\nif y \u201c 0\n(2)\nis the Bernoulli distribution.\nWe will refer to models of the form Equation (1) as\nprobabilistic models. In addition to probabilistic models, we\nwill also discuss models which optimize fp\u00a8q under other\ncriteria, for instance models which maximize the margin\n8As an example, there are currently 10,306 male and 7,586 female American\nactors listed in Wikipedia, while there are only 1,268 male and 1,354 female\nIndian, and 77 male and no female Nigerian actors. India and Nigeria, however,\nare the largest and second largest \ufb01lm industries in the world.\n6\nbetween existing and non-existing triples. We will refer to\nsuch models as score-based models. If desired, we can derive\nprobabilities for score-based models via Platt scaling [61].\nThere are many different methods for de\ufb01ning fp\u00a8q. In the\nfollowing Sections IV to VI and VIII we will discuss different\noptions for all model classes. In Section VII we will furthermore\ndiscuss aspects of how to train these models on knowledge\ngraphs.\nIV. LATENT FEATURE MODELS\nIn this section, we assume that the variables yijk are\nconditionally independent given a set of global latent features\nand parameters, as in Equation 1. We discuss various possible\nforms for the score function fpx; \u0398q below. What all models\nhave in common is that they explain triples via latent features\nof entities (This is justi\ufb01ed via various theoretical arguments\n[62]). For instance, a possible explanation for the fact that Alec\nGuinness received the Academy Award is that he is a good\nactor. This explanation uses latent features of entities (being a\ngood actor) to explain observable facts (Guinness receiving the\nAcademy Award). We call these features \u201clatent\u201d because they\nare not directly observed in the data. One task of all latent\nfeature models is therefore to infer these features automatically\nfrom the data.\nIn the following, we will denote the latent feature represen-\ntation of an entity ei by the vector ei P RHe where He denotes\nthe number of latent features in the model. For instance, we\ncould model that Alec Guinness is a good actor and that the\nAcademy Award is a prestigious award via the vectors\neGuinness \u201c\n\u201e\n0.9\n0.2\n\uf6be\n,\neAcademyAward \u201c\n\u201e\n0.2\n0.8\n\uf6be\nwhere the component ei1 corresponds to the latent feature\nGood Actor and ei2 correspond to Prestigious Award. (Note\nthat, unlike this example, the latent features that are inferred\nby the following models are typically hard to interpret.)\nThe key intuition behind relational latent feature models\nis that the relationships between entities can be derived from\ninteractions of their latent features. However, there are many\npossible ways to model these interactions, and many ways to\nderive the existence of a relationship from them. We discuss\nseveral possibilities below. See Table III for a summary of the\nnotation.\nA. RESCAL: A bilinear model\nRESCAL [63, 64, 65] is a relational latent feature model\nwhich explains triples via pairwise interactions of latent features.\nIn particular, we model the score of a triple xijk as\nf RESCAL\nijk\n:\u201c eJ\ni Wkej \u201c\nHe\n\u00ff\na\u201c1\nHe\n\u00ff\nb\u201c1\nwabkeiaejb\n(3)\nwhere Wk P RHe\u02c6He is a weight matrix whose entries wabk\nspecify how much the latent features a and b interact in the\nk-th relation. We call this a bilinear model, since it captures the\ninteractions between the two entity vectors using multiplicative\nterms. For instance, we could model the pattern that good\nTABLE III\nSUMMARY OF THE NOTATION.\nRelational data\nSymbol\nMeaning\nNe\nNumber of entities\nNr\nNumber of relations\nNd\nNumber of training examples\nei\ni-th entity in the dataset (e.g., LeonardNimoy)\nrk\nk-th relation in the dataset (e.g., bornIn)\nD`\nSet of observed positive triples\nD\u00b4\nSet of observed negative triples\nProbabilistic Knowledge Graphs\nSymbol\nMeaning\nSize\nY\n(Partially observed) labels for all triples\nNe \u02c6 Ne \u02c6 Nr\nF\nScore for all possible triples\nNe \u02c6 Ne \u02c6 Nr\nYk\nSlice of Y for relation rk\nNe \u02c6 Ne\nFk\nSlice of F for relation rk\nNe \u02c6 Ne\nGraph and Latent Feature Models\nSymbol\nMeaning\n\u03c6ijk\nFeature vector representation of triple pei, rk, ejq\nwk\nWeight vector to derive scores for relation k\n\u0398\nSet of all parameters of the model\n\u03c3p\u00a8q\nSigmoid (logistic) function\nLatent Feature Models\nSymbol\nMeaning\nSize\nHe\nNumber of latent features for entities\nHr\nNumber of latent features for relations\nei\nLatent feature repr. of entity ei\nHe\nrk\nLatent feature repr. of relation rk\nHr\nHa\nSize of ha layer\nHb\nSize of hb layer\nHc\nSize of hc layer\nE\nEntity embedding matrix\nNe \u02c6 He\nWk\nBilinear weight matrix for relation k\nHe \u02c6 He\nAk\nLinear feature map for pairs of entities\np2Heq \u02c6 Ha\nfor relation rk\nC\nLinear feature map for triples\np2He ` Hrq \u02c6 Hc\nactors are likely to receive prestigious awards via a weight\nmatrix such as\nWreceivedAward \u201c\n\u201e\n0.1\n0.9\n0.1\n0.1\n\uf6be\n.\nIn general, we can model block structure patterns via the\nmagnitude of entries in Wk, while we can model homophily\npatterns via the magnitude of its diagonal entries. Anti-\ncorrelations in these patterns can be modeled via negative\nentries in Wk.\nHence, in Equation (3) we compute the score of a triple\nxijk via the weighted sum of all pairwise interactions between\nthe latent features of the entities ei and ej. The parameters of\nthe model are \u0398 \u201c tteiuNe\ni\u201c1, tWkuNr\nk\u201c1u. During training we\njointly learn the latent representations of entities and how the\nlatent features interact for particular relation types.\nIn the following, we will discuss further important properties\nof the model for learning from knowledge graphs.\nRelational learning via shared representations: In equa-\ntion (3), entities have the same latent representation regardless\nof whether they occur as subjects or objects in a relationship.\nFurthermore, they have the same representation over all\ndifferent relation types. For instance, the i-th entity occurs\nin the triple xijk as the subject of a relationship of type k,\n7\n\u00ab\ni-th\nentity\nj-th entity\nk-th\nrelation\ni-th\nentity\nj-th\nentity\nk-th\nrelation\nYk\nEWkEJ\nFig. 4.\nRESCAL as a tensor factorization of the adjacency tensor Y.\nwhile it occurs in the triple xpiq as the object of a relationship\nof type q. However, the predictions fijk \u201c eJ\ni Wkej and\nfpiq \u201c eJ\np Wqei both use the same latent representation ei\nof the i-th entity. Since all parameters are learned jointly,\nthese shared representations permit to propagate information\nbetween triples via the latent representations of entities and the\nweights of relations. This allows the model to capture global\ndependencies in the data.\nSemantic embeddings: The shared entity representations\nin RESCAL capture also the similarity of entities in the\nrelational domain, i.e., that entities are similar if they are\nconnected to similar entities via similar relations [65]. For\ninstance, if the representations of ei and ep are similar, the\npredictions fijk and fpjk will have similar values. In return,\nentities with many similar observed relationships will have\nsimilar latent representations. This property can be exploited for\nentity resolution and has also enabled large-scale hierarchical\nclustering on relational data [63, 64]. Moreover, since relational\nsimilarity is expressed via the similarity of vectors, the latent\nrepresentations ei can act as proxies to give non-relational\nmachine learning algorithms such as k-means or kernel methods\naccess to the relational similarity of entities.\nConnection to tensor factorization: RESCAL is similar\nto methods used in recommendation systems [66], and to\ntraditional tensor factorization methods [67]. In matrix notation,\nEquation (3) can be written compactly as as Fk \u201c EWkEJ,\nwhere Fk P RNe\u02c6Ne is the matrix holding all scores for the\nk-th relation and the i-th row of E P RNe\u02c6He holds the latent\nrepresentation of ei. See Figure 4 for an illustration. In the\nfollowing, we will use this tensor representation to derive a\nvery ef\ufb01cient algorithm for parameter estimation.\nFitting the model: If we want to compute a probabilistic\nmodel, the parameters of RESCAL can be estimated by\nminimizing the log-loss using gradient-based methods such as\nstochastic gradient descent [68]. RESCAL can also be com-\nputed as a score-based model, which has the main advantage\nthat we can estimate the parameters \u0398 very ef\ufb01ciently: Due\nto its tensor structure and due to the sparsity of the data, it\nhas been shown that the RESCAL model can be computed\nvia a sequence of ef\ufb01cient closed-form updates when using\nthe squared-loss [63, 64]. In this setting, it has been shown\nanalytically that a single update of E and Wk scales linearly\nwith the number of entities Ne, linearly with the number of\nrelations Nr, and linearly with the number of observed triples,\ni.e., the number of non-zero entries in Y [64]. We call this\nalgorithm RESCAL-ALS.9 In practice, a small number (say 30\nto 50) of iterated updates are often suf\ufb01cient for RESCAL-ALS\nto arrive at stable estimates of the parameters. Given a current\nestimate of E, the updates for each Wk can be computed in\nparallel to improve the scalability on knowledge graphs with\na large number of relations. Furthermore, by exploiting the\nspecial tensor structure of RESCAL, we can derive improved\nupdates for RESCAL-ALS that compute the estimates for the\nparameters with a runtime complexity of OpH3\ne q for a single\nupdate (as opposed to a runtime complexity of OpH5\ne q for\nnaive updates) [65, 69]. In summary, for relational domains\nthat can be explained via a moderate number of latent features,\nRESCAL-ALS is highly scalable and very fast to compute.\nFor more detail on RESCAL-ALS see also Equation (26) in\nSection VII.\nDecoupled Prediction: In Equation (3), the probability\nof single relationship is computed via simple matrix-vector\nproducts in OpH2\ne q time. Hence, once the parameters have been\nestimated, the computational complexity to predict the score of\na triple depends only on the number of latent features and is\nindependent of the size of the graph. However, during parameter\nestimation, the model can capture global dependencies due to\nthe shared latent representations.\nRelational learning results: RESCAL has been shown\nto achieve state-of-the-art results on a number of relational\nlearning tasks. For instance, [63] showed that RESCAL\nprovides comparable or better relationship prediction results\non a number of small benchmark datasets compared to\nMarkov Logic Networks (with structure learning) [70], the\nIn\ufb01nite (Hidden) Relational model [71, 72], and Bayesian\nClustered Tensor Factorization [73]. Moreover, RESCAL has\nbeen used for link prediction on entire knowledge graphs such\nas YAGO and DBpedia [64, 74]. Aside from link prediction,\nRESCAL has also successfully been applied to SRL tasks such\nas entity resolution and link-based clustering. For instance,\nRESCAL has shown state-of-the-art results in predicting which\nauthors, publications, or publication venues are likely to be\nidentical in publication databases [63, 65]. Furthermore, the\nsemantic embedding of entities computed by RESCAL has\nbeen exploited to create taxonomies for uncategorized data via\nhierarchical clusterings of entities in the embedding space [75].\nB. Other tensor factorization models\nVarious other tensor factorization methods have been ex-\nplored for learning from knowledge graphs and multi-relational\ndata. [76, 77] factorized adjacency tensors using the CP\ntensor decomposition to analyze the link structure of Web\npages and Semantic Web data respectively. [78] applied\npairwise interaction tensor factorization [79] to predict triples\nin knowledge graphs. [80] applied factorization machines to\nlarge uni-relational datasets in recommendation settings. [81]\nproposed a tensor factorization model for knowledge graphs\nwith a very large number of different relations.\nIt is also possible to use discrete latent factors. [82] proposed\nBoolean tensor factorization to disambiguate facts extracted\nwith OpenIE methods and applied it to large datasets [83]. In\n9ALS stands for Alternating Least-Squares\n8\ncontrast to previously discussed factorizations, Boolean tensor\nfactorizations are discrete models, where adjacency tensors are\ndecomposed into binary factors based on Boolean algebra.\nC. Matrix factorization methods\nAnother approach for learning from knowledge graphs is\nbased on matrix factorization, where, prior to the factorization,\nthe adjacency tensor Y P RNe\u02c6Ne\u02c6Nr is reshaped into a matrix\nY P RN 2\ne \u02c6Nr by associating rows with subject-object pairs\npei, ejq and columns with relations rk (cf. [84, 85]), or into\na matrix Y P RNe\u02c6NeNr by associating rows with subjects\nei and columns with relation/objects prk, ejq (cf. [86, 87]).\nUnfortunately, both of these formulations lose information\ncompared to tensor factorization. For instance, if each subject-\nobject pair is modeled via a different latent representation, the\ninformation that the relationships yijk and ypjq share the same\nobject is lost. It also leads to an increased memory complexity,\nsince a separate latent representation is computed for each pair\nof entities, requiring OpN 2\ne He `NrHeq parameters (compared\nto OpNeHe ` NrH2\ne q parameters for RESCAL).\nD. Multi-layer perceptrons\nWe can interpret RESCAL as creating composite repre-\nsentations of triples and predicting their existence from this\nrepresentation. In particular, we can rewrite RESCAL as\nf RESCAL\nijk\n:\u201c wJ\nk \u03c6RESCAL\nij\n(4)\n\u03c6RESCAL\nij\n:\u201c ej b ei,\n(5)\nwhere wk \u201c vec pWkq. Equation (4) follows from Equation (3)\nvia the equality vec pAXBq \u201c pBJ b Aq vec pXq. Hence,\nRESCAL represents pairs of entities pei, ejq via the tensor\nproduct of their latent feature representations (Equation (5))\nand predicts the existence of the triple xijk from \u03c6ij via wk\n(Equation (4)). See also Figure 5a. For a further discussion of\nthe tensor product to create composite latent representations\nplease see [88, 89, 90].\nSince the tensor product explicitly models all pairwise\ninteractions, RESCAL can require a lot of parameters when\nthe number of latent features are large (each matrix Wk has\nH2\ne entries). This can, for instance, lead to scalability problems\non knowledge graphs with a large number of relations.\nIn the following we will discuss models based on multi-\nlayer perceptrons (MLPs), also known as feedforward neural\nnetworks. In the context of multidimensional data they can\nbe referred to a muliway neural networks. This approach\nallows us to consider alternative ways to create composite\ntriple representations and to use nonlinear functions to predict\ntheir existence.\nIn particular, let us de\ufb01ne the following E-MLP model (E\nfor entity):\nf E-MLP\nijk\n:\u201c wJ\nk gpha\nijkq\n(6)\nha\nijk :\u201c AJ\nk \u03c6E-MLP\nij\n(7)\n\u03c6E-MLP\nij\n:\u201c rei; ejs\n(8)\nTABLE IV\nSEMANTIC EMBEDDINGS OF KV-MLP ON FREEBASE\nRelation\nNearest Neighbors\nchildren\nparents\n(0.4)\nspouse\n(0.5)\nbirth-place (0.8)\nbirth-date\nchildren\n(1.24)\ngender\n(1.25)\nparents\n(1.29)\nedu-end10\njob-start (1.41)\nedu-start (1.61)\njob-end\n(1.74)\nwhere gpuq \u201c rgpu1q, gpu2q, . . .s is the function g applied\nelement-wise to vector u; one often uses the nonlinear function\ngpuq \u201c tanhpuq.\nHere ha is an additive hidden layer, which is deriving\nby adding together different weighed components of the\nentity representations. In particular, we create a composite\nrepresentation \u03c6E-MLP\nij\n\u201c rei; ejs P R2Ha via the concatenation\nof ei and ej. However, concatenation alone does not consider\nany interactions between the latent features of ei and ej.\nFor this reason, we add a (vector-valued) hidden layer ha\nof size Ha, from which the \ufb01nal prediction is derived via\nwJ\nk gphaq. The important difference to tensor-product models\nlike RESCAL is that we learn the interactions of latent\nfeatures via the matrix Ak (Equation (7)), while the tensor\nproduct considers always all possible interactions between\nlatent features. This adaptive approach can reduce the number\nof required parameters signi\ufb01cantly, especially on datasets with\na large number of relations.\nOne disadvantage of the E-MLP is that it has to de\ufb01ne\na vector wk and a matrix Ak for every possible relation,\nwhich requires Ha ` pHa \u02c6 2Heq parameters per relation.\nAn alternative is to embed the relation itself, using a Hr-\ndimensional vector rk. We can then de\ufb01ne\nf ER-MLP\nijk\n:\u201c wJgphc\nijkq\n(9)\nhc\nijk :\u201c CJ\u03c6ER-MLP\nijk\n(10)\n\u03c6ER-MLP\nijk\n:\u201c rei; ej; rks.\n(11)\nWe call this model the ER-MLP, since it applies an MLP to\nan embedding of the entities and relations. Please note that\nER-MLP uses a global weight vector for all relations. This\nmodel was used in the KV project (see Section IX), since it\nhas many fewer parameters than the E-MLP (see Table V); the\nreason is that C is independent of the relation k.\nIt has been shown in [91] that MLPs can learn to put\n\u201csemantically similar\u201d words close by in the embedding space,\neven if they are not explicitly trained to do so. In [28], they show\na similar result for the semantic embedding of relations using\nER-MLP. For example, Table IV shows the nearest neighbors\nof latent representations of selected relations that have been\ncomputed with a 60 dimensional model on Freebase. Numbers\nin parentheses represent squared Euclidean distances. It can\nbe seen that ER-MLP puts semantically related relations near\neach other. For instance, the closest relations to the children\nrelation are parents, spouse, and birthplace.\n10The relations edu-start, edu-end, job-start, job-end represent the start and\nend dates of attending an educational institution and holding a particular job,\nrespectively\n9\nei1\nei2\nei3\nej1\nej2\nej3\nfijk\nb\nwk\nsubject\nobject\n(a) RESCAL\nei1\nei2\nei3\nek1\nek2\nek3\nrj1\nrj2\nrj3\ng\nhc1\ng\nhc2\ng\nhc3\nfijk\nC\nw\nsubject\nobject\npredicate\n(b) ER-MLP\nFig. 5. Visualization of RESCAL and the ER-MLP model as Neural Networks. Here, He \u201c Hr \u201c 3 and Ha \u201c 3. Note, that the inputs are latent features.\nThe symbol g denotes the application of the function gp\u00a8q.\nTABLE V\nSUMMARY OF THE LATENT FEATURE MODELS. ha, hb AND hc ARE HIDDEN LAYERS OF THE NEURAL NETWORK; SEE TEXT FOR DETAILS.\nMethod\nfijk\nAk\nC\nBk\nNum. Parameters\nRESCAL [64]\nwJ\nk hb\nijk\n-\n-\nr\u03b41,1, . . . , \u03b4He,Hes\nNrH2\ne ` NeHe\nE-MLP [92]\nwJ\nk gpha\nijkq\nrAs\nk;\nAo\nks\n-\n-\nNrpHa ` Ha \u02c6 2Heq ` NeHe\nER-MLP [28]\nwJgphc\nijkq\n-\nC\n-\nHc ` Hc \u02c6 p2He ` Hrq ` NrHr ` NeHe\nNTN [92]\nwJ\nk gprha\nijk; hb\nijksq\nrAs\nk; Ao\nks\n-\nrB1\nk, . . . , BHb\nk s\nN2\ne Hb ` NrpHb ` Haq ` 2NrHeHa ` NeHe\nStructured Embeddings [93]\n\u00b4}ha\nijk}1\nrAs\nk; \u00b4Ao\nks\n-\n-\n2NrHeHa ` NeHe\nTransE [94]\n\u00b4p2ha\nijk \u00b4 2hb\nijk ` }rk}2\n2q\nrrk; \u00b4rks\n-\nI\nNrHe ` NeHe\nE. Neural tensor networks\nWe can combine traditional MLPs with bilinear models,\nresulting in what [92] calls a \u201cneural tensor network\u201d (NTN).\nMore precisely, we can de\ufb01ne the NTN model as follows:\nf NTN\nijk\n:\u201c wJ\nk gprha\nijk; hb\nijksq\n(12)\nha\nijk :\u201c AJ\nk rei; ejs\n(13)\nhb\nijk :\u201c\n\u201d\neJ\ni B1\nkej, . . . , eJ\ni BHb\nk ej\n\u0131\n(14)\nHere Bk is a tensor, where the \u2113-th slice B\u2113\nk has size He \u02c6\nHe, and there are Hb slices. We call hb\nijk a bilinear hidden\nlayer, since it is derived from a weighted combination of\nmultiplicative terms.\nNTN is a generalization of the RESCAL approach, as we\nexplain in Section XII-A. Also, it uses the additive layer from\nthe E-MLP model. However, it has many more parameters\nthan the E-MLP or RESCAL models. Indeed, the results in\n[95] and [28] both show that it tends to over\ufb01t, at least on the\n(relatively small) datasets uses in those papers.\nF. Latent distance models\nAnother class of models are latent distance models (also\nknown as latent space models in social network analysis),\nwhich derive the probability of relationships from the distance\nbetween latent representations of entities: entities are likely\nto be in a relationship if their latent representations are close\naccording to some distance measure. For uni-relational data,\n[96] proposed this approach \ufb01rst in the context of social\nnetworks by modeling the probability of a relationship xij\nvia the score function fpei, ejq \u201c \u00b4dpei, ejq where dp\u00a8, \u00a8q\nrefers to an arbitrary distance measure such as the Euclidean\ndistance.\nThe structured embedding (SE) model [93] extends this idea\nto multi-relational data by modeling the score of a triple xijk\nas:\nf SE\nijk :\u201c \u00b4}As\nkei \u00b4 Ao\nkej}1 \u201c \u00b4}ha\nijk}1\n(15)\nwhere Ak \u201c rAs\nk; \u00b4Ao\nks. In Equation (15) the matrices As\nk,\nAo\nk transform the global latent feature representations of entities\nto model relationships speci\ufb01cally for the k-th relation. The\ntransformations are learned using the ranking loss in a way\nsuch that pairs of entities in existing relationships are closer\nto each other than entities in non-existing relationships.\nTo reduce the number of parameters over the SE model, the\nTransE model [94] translates the latent feature representations\nvia a relation-speci\ufb01c offset instead of transforming them via\nmatrix multiplications. In particular, the score of a triple xijk\nis de\ufb01ned as:\nf TransE\nijk\n:\u201c \u00b4dpei ` rk, ejq.\n(16)\nThis model is inspired by the results in [91], who showed that\nsome relationships between words could be computed by their\nvector difference in the embedding space. As noted in [95],\nunder unit-norm constraints on ei, ej and using the squared\nEuclidean distance, we can rewrite Equation (16) as follows:\nf TransE\nijk\n\u201c \u00b4p2rJ\nk pei \u00b4 ejq \u00b4 2eJ\ni ej ` }rk}2\n2q\n(17)\nFurthermore, if we assume Ak\n\u201c\nrrk; \u00b4rks, so that\nha\nijk \u201c rrk; \u00b4rksT rei; ejs \u201c rT\nk pei \u00b4 ejq, and Bk \u201c I, so that\nhb\nijk \u201c eT\ni ej, then we can rewrite this model as follows:\nf TransE\nijk\n\u201c \u00b4p2ha\nijk \u00b4 2hb\nijk ` }rk}2\n2q.\n(18)\nG. Comparison of models\nTable V summarizes the different models we have discussed.\nA natural question is: which model is best? [28] showed that\n10\nthe ER-MLP model outperformed the NTN model on their\nparticular dataset. [95] performed more extensive experimental\ncomparison of these models, and found that RESCAL (called\nthe bilinear model) worked best on two link prediction tasks.\nHowever, clearly the best model will be dataset dependent.\nV. GRAPH FEATURE MODELS\nIn this section, we assume that the existence of an edge\ncan be predicted by extracting features from the observed\nedges in the graph. For example, due to social conventions,\nparents of a person are often married, so we could predict\nthe triple (John, marriedTo, Mary) from the existence of the\npath John\nparentOf\n\u00dd\u00dd\u00dd\u00dd\u00d1 Anne\nparentOf\n\u00d0\u00dd\u00dd\u00dd\u00dd Mary, representing a com-\nmon child. In contrast to latent feature models, this kind of\nreasoning explains triples directly from the observed triples in\nthe knowledge graph. We will now discuss some models of\nthis kind.\nA. Similarity measures for uni-relational data\nObservable graph feature models are widely used for link\nprediction in graphs that consist only of a single relation,\ne.g., social network analysis (friendships between people),\nbiology (interactions of proteins), and Web mining (hyperlinks\nbetween Web sites). The intuition behind these methods is that\nsimilar entities are likely to be related (homophily) and that\nthe similarity of entities can be derived from the neighborhood\nof nodes or from the existence of paths between nodes. For\nthis purpose, various indices have been proposed to measure\nthe similarity of entities, which can be classi\ufb01ed into local,\nglobal, and quasi-local approaches [97].\nLocal similarity indices such as Common Neighbors, the\nAdamic-Adar index [98] or Preferential Attachment [99] derive\nthe similarity of entities from their number of common neigh-\nbors or their absolute number of neighbors. Local similarity\nindices are fast to compute for single relationships and scale\nwell to large knowledge graphs as their computation depends\nonly on the direct neighborhood of the involved entities.\nHowever, they can be too localized to capture important\npatterns in relational data and cannot model long-range or\nglobal dependencies.\nGlobal similarity indices such as the Katz index [100] and\nthe Leicht-Holme-Newman index [101] derive the similarity of\nentities from the ensemble of all paths between entities, while\nindices like Hitting Time, Commute Time, and PageRank [102]\nderive the similarity of entities from random walks on the graph.\nGlobal similarity indices often provide signi\ufb01cantly better\npredictions than local indices, but are also computationally\nmore expensive [97, 56].\nQuasi-local similarity indices like the Local Katz index [56]\nor Local Random Walks [103] try to balance predictive accuracy\nand computational complexity by deriving the similarity of\nentities from paths and random walks of bounded length.\nIn Section V-C, we will discuss an approach that extends this\nidea of quasi-local similarity indices for uni-relational networks\nto learn from large multi-relational knowledge graphs.\nB. Rule Mining and Inductive Logic Programming\nAnother class of models that works on the observed variables\nof a knowledge graph extracts rules via mining methods and\nuses these extracted rules to infer new links. The extracted\nrules can also be used as a basis for Markov Logic as\ndiscussed in Section VIII. For instance, ALEPH is an Inductive\nLogic Programming (ILP) system that attempts to learn rules\nfrom relational data via inverse entailment [104] (For more\ninformation on ILP see e.g., [105, 3, 106]). AMIE is a rule\nmining system that extracts logical rules (in particular Horn\nclauses) based on their support in a knowledge graph [107, 108].\nIn contrast to ALEPH, AMIE can handle the open-world\nassumption of knowledge graphs and has shown to be up\nto three orders of magnitude faster on large knowledge\ngraphs [108]. The basis for the Semantic Web is Description\nLogic and [109, 110, 111] describe approaches for logic-\noriented machine learning approaches in this context. Also\nto mention are data mining approaches for knowledge graphs\nas described in [112, 113, 114]. An advantage of rule-based\nsystems is that they are easily interpretable as the model is given\nas a set of logial rules. However, rules over observed variables\ncover usually only a subset of patterns in knowledge graphs (or\nrelational data) and useful rules can be challenging to learn.\nC. Path Ranking Algorithm\nThe Path Ranking Algorithm (PRA) [115, 116] extends the\nidea of using random walks of bounded lengths for predicting\nlinks in multi-relational knowledge graphs. In particular, let\n\u03c0Lpi, j, k, tq denote a path of length L of the form ei\nr1\n\u00d1 e2\nr2\n\u00d1\ne3 \u00a8 \u00a8 \u00a8\nrL\n\u00d1 ej, where t represents the sequence of edge types\nt \u201c pr1, r2, . . . , rLq. We also require there to be a direct arc\nei\nrk\n\u00d1 ej, representing the existence of a relationship of type k\nfrom ei to ej. Let \u03a0Lpi, j, kq represent the set of all such paths\nof length L, ranging over path types t. (We can discover such\npaths by enumerating all (type-consistent) paths from entities\nof type ei to entities of type ej. If there are too many relations\nto make this feasible, we can perform random sampling.)\nWe can compute the probability of following such a path\nby assuming that at each step, we follow an outgoing link\nuniformly at random. Let Pp\u03c0Lpi, j, k, tqq be the probability\nof this particular path; this can be computed recursively by\na sampling procedure, similar to PageRank (see [116] for\ndetails). The key idea in PRA is to use these path probabilities\nas features for predicting the probability of missing edges.\nMore precisely, de\ufb01ne the feature vector\n\u03c6PRA\nijk \u201c rPp\u03c0q : \u03c0 P \u03a0Lpi, j, kqs\n(19)\nWe can then predict the edge probabilities using logistic\nregression:\nf PRA\nijk\n:\u201c wJ\nk \u03c6PRA\nijk\n(20)\nInterpretability: A useful property of PRA is that its model is\neasily interpretable. In particular, relation paths can be regarded\nas bodies of weighted rules \u2014 more precisely Horn clauses \u2014\nwhere the weight speci\ufb01es how predictive the body of the rule\nis for the head. For instance, Table VI shows some relation\npaths along with their weights that have been learned by PRA\n11\nTABLE VI\nEXAMPLES OF PATHS LEARNED BY PRA ON FREEBASE TO PREDICT WHICH\nCOLLEGE A PERSON ATTENDED\nRelation Path\nF1\nPrec\nRec\nWeight\n(draftedBy, school)\n0.03\n1.0\n0.01\n2.62\n(sibling(s), sibling, education, institution)\n0.05\n0.55\n0.02\n1.88\n(spouse(s), spouse, education, institution)\n0.06\n0.41\n0.02\n1.87\n(parents, education, institution)\n0.04\n0.29\n0.02\n1.37\n(children, education, institution)\n0.05\n0.21\n0.02\n1.85\n(placeOfBirth, peopleBornHere, education)\n0.13\n0.1\n0.38\n6.4\n(type, instance, education, institution)\n0.05\n0.04\n0.34\n1.74\n(profession, peopleWithProf., edu., inst.)\n0.04\n0.03\n0.33\n2.19\nin the KV project (see Section IX) to predict which college a\nperson attended, i.e., to predict triples of the form (p, college,\nc). The \ufb01rst relation path in Table VI can be interpreted as\nfollows: it is likely that a person attended a college if the\nsports team that drafted the person is from the same college.\nThis can be written in the form of a Horn clause as follows:\n(p, college, c) \u00d0 (p, draftedBy, t) ^ (t, school, c) .\nBy using a sparsity promoting prior on wk, we can perform\nfeature selection, which is equivalent to rule learning.\nRelational learning results: PRA has been shown to out-\nperform the ILP method FOIL [106] for link prediction in\nNELL [116]. It has also been shown to have comparable\nperformance to ER-MLP on link prediction in KV: PRA\nobtained a result of 0.884 for the area under the ROC curve,\nas compared to 0.882 for ER-MLP [28].\nVI. COMBINING LATENT AND GRAPH FEATURE MODELS\nIt has been observed experimentally (see, e.g., [28]) that\nneither state-of-the-art relational latent feature models (RLFMs)\nnor state-of-the-art graph feature models are superior for\nlearning from knowledge graphs. Instead, the strengths of latent\nand graph-based models are often complementary (see e.g.,\n[117]), as both families focus on different aspects of relational\ndata:\n\u201a Latent feature models are well-suited for modeling global\nrelational patterns via newly introduced latent variables.\nThey are computationally ef\ufb01cient if triples can be\nexplained with a small number of latent variables.\n\u201a Graph feature models are well-suited for modeling local\nand quasi-local graphs patterns. They are computationally\nef\ufb01cient if triples can be explained from the neighborhood\nof entities or from short paths in the graph.\nThere has also been some theoretical work comparing these\ntwo approaches [118]. In particular, it has been shown that\ntensor factorization can be inef\ufb01cient when relational data\nconsists of a large number of strongly connected components.\nFortunately, such \u201cproblematic\u201d relations can often be handled\nef\ufb01ciently via graph-based models. A good example is the\nmarriedTo relation: One marriage corresponds to a single\nstrongly connected component, so data with a large number of\nmarriages would be dif\ufb01cult to model with RLFMs. However,\npredicting marriedTo links via graph-based models is easy: the\nexistence of the triple (John, marriedTo, Mary) can be simply\npredicted from the existence of (Mary, marriedTo, John), by\nexploiting the symmetry of the relation. If the (Mary, marriedTo,\nJohn) edge is unknown, we can use statistical patterns, such\nas the existence of shared children.\nCombining the strengths of latent and graph-based models\nis therefore a promising approach to increase the predictive\nperformance of graph models. It typically also speeds up the\ntraining. We now discuss some ways of combining these two\nkinds of models.\nA. Additive relational effects model\n[118] proposed the additive relational effects (ARE), which\nis a way to combine RLFMs with observable graph models.\nIn particular, if we combine RESCAL with PRA, we get\nf RESCAL+PRA\nijk\n\u201c wp1qJ\nk\n\u03c6RESCAL\nij\n` wp2qJ\nk\n\u03c6PRA\nijk .\n(21)\nARE models can be trained by alternately optimizing the\nRESCAL parameters with the PRA parameters. The key bene\ufb01t\nis now RESCAL only has to model the \u201cresidual errors\u201d that\ncannot be modelled by the observable graph patterns. This\nallows the method to use much lower latent dimensionality,\nwhich signi\ufb01cantly speeds up training time. The resulting\ncombined model also has increased accuracy [118].\nB. Other combined models\nIn addition to ARE, further models have been explored to\nlearn jointly from latent and observable patterns on relational\ndata. [84, 85] combined a latent feature model with an additive\nterm to learn from latent and neighborhood-based information\non multi-relational data, as follows:11\nf ADD\nijk\n:\u201c wp1qJ\nk,j \u03c6SUB\ni\n` wp2qJ\nk,i \u03c6OBJ\nj\n` wp3qJ\nk\n\u03c6N\nijk\n(22)\n\u03c6N\nijk :\u201c ryijk1 : k1 \u2030 ks\n(23)\nHere, \u03c6SUB\ni\nis the latent representation of entity ei as a subject\nand \u03c6OBJ\nj\nis the latent representation of entity ej as an object.\nThe term \u03c6N\nijk captures patterns ef\ufb01ciently where the existence\nof a triple yijk1 is predictive of another triple yijk between\nthe same pair of entities (but of a different relation type). For\ninstance, if Leonard Nimoy was born in Boston, it is also likely\nthat he lived in Boston. This dependency between the relation\ntypes bornIn and livedIn can be modeled in Equation (23) by\nassigning a large weight to wbornIn,livedIn.\nARE and the models of [84] and [85] are similar in\nspirit to the model of [119], which augments SVD (i.e.,\nmatrix factorization) of a rating matrix with additive terms to\ninclude local neighborhood information. Similarly, factorization\nmachines [120] allow to combine latent and observable patterns,\nby modeling higher-order interactions between input variables\nvia low-rank factorizations [78].\nAn alternative way to combine different prediction systems\nis to \ufb01t them separately, and use their outputs as inputs to\nanother \u201cfusion\u201d system. This is called stacking [121]. For\ninstance, [28] used the output of PRA and ER-MLP as scalar\nfeatures, and learned a \ufb01nal \u201cfusion\u201d layer by training a binary\n11 [85] considered an additional term fUNI\nijk\n:\u201c\nfADD\nijk\n` wJ\nk \u03c6SUB+OBJ\nij\n,\nwhere \u03c6SUB+OBJ\nij\nis a (non-composite) latent feature representation of subject-\nobject pairs.\n12\nclassi\ufb01er. Stacking has the advantage that it is very \ufb02exible\nin the kinds of models that can be combined. However, it has\nthe disadvantage that the individual models cannot cooperate,\nand thus any individual model needs to be more complex than\nin a combined model which is trained jointly. For example, if\nwe \ufb01t RESCAL separately from PRA, we will need a larger\nnumber of latent features than if we \ufb01t them jointly.\nVII. TRAINING SRL MODELS ON KNOWLEDGE GRAPHS\nIn this section we discuss aspects of training the previously\ndiscussed models that are speci\ufb01c to knowledge graphs, such\nas how to handle the open-world assumption of knowledge\ngraphs, how to exploit sparsity, and how to perform model\nselection.\nA. Penalized maximum likelihood training\nLet us assume we have a set of Nd observed triples and\nlet the n-th triple be denoted by xn. Each observed triple is\neither true (denoted yn \u201c 1) or false (denoted yn \u201c 0). Let this\nlabeled dataset be denoted by D \u201c tpxn, ynq | n \u201c 1, . . . , Ndu.\nGiven this, a natural way to estimate the parameters \u0398 is to\ncompute the maximum a posteriori (MAP) estimate:\nmax\n\u0398\nNd\n\u00ff\nn\u201c1\nlog Berpyn | \u03c3pfpxn; \u0398qqq ` log pp\u0398 | \u03bbq\n(24)\nwhere \u03bb controls the strength of the prior. (If the prior is\nuniform, this is equivalent to maximum likelihood training.)\nWe can equivalently state this as a regularized loss minimization\nproblem:\nmin\n\u0398\nN\n\u00ff\nn\u201c1\nLp\u03c3pfpxn; \u0398qq, ynq ` \u03bb regp\u0398q\n(25)\nwhere Lpp, yq \u201c \u00b4 log Berpy|pq is the log loss function.\nAnother possible loss function is the squared loss, Lpp, yq \u201c\npp \u00b4 yq2. Using the squared loss can be especially ef\ufb01cient\nin combination with a closed-world assumption (CWA). For\ninstance, using the squared loss and the CWA, the minimization\nproblem for RESCAL becomes\nmin\nE,tWku\n\u00ff\nk\n}Yk \u00b4 EWkEJ}2\nF ` \u03bb1}E}2\nF ` \u03bb2\n\u00ff\nk\n}Wk}2\nF .\n(26)\nwhere \u03bb1, \u03bb2 \u011b 0 control the degree of regularization. The\nmain advantage of Equation (26) is that it can be optimized via\nRESCAL-ALS, which consists of a sequence of very ef\ufb01cient,\nclosed-form updates whose computational complexity depends\nonly on the non-zero entries in Y [63, 64]. We discuss some\nother loss functions below.\nB. Where do the negative examples come from?\nOne important question is where the labels yn come from.\nThe problem is that most knowledge graphs only contain\npositive training examples, since, usually, they do not encode\nfalse facts. Hence yn \u201c 1 for all pxn, ynq P D. To emphasize\nthis, we shall use the notation D` to represent the observed\npositive (true) triples: D` \u201c txn P D | yn \u201c 1u. Training on\nall-positive data is tricky, because the model might easily over\ngeneralize.\nOne way around this is as to make a closed world as-\nsumption and assume that all (type consistent) triples that\nare not in D` are false. We will denote this negative set as\nD\u00b4 \u201c txn P D | yn \u201c 0u. However, for incomplete knowledge\ngraphs this assumption will be violated. Moreover, D\u00b4 might\nbe very large, since the number of false facts is much larger\nthan the number of true facts. This can lead to scalability issues\nin training methods that have to consider all negative examples.\nAn alternative approach to generate negative examples is to\nexploit known constraints on the structure of a knowledge graph:\nType constraints for predicates (persons are only married to\npersons), valid value ranges for attributes (the height of humans\nis below 3 meters), or functional constraints such as mutual\nexclusion (a person is born exactly in one city) can all be used\nfor this purpose. Since such examples are based on the violation\nof hard constraints, it is certain that they are indeed negative\nexamples. Unfortunately, functional constraints are scarce and\nnegative examples based on type constraints and valid value\nranges are usually not suf\ufb01cient to train useful models: While it\nis relatively easy to predict that a person is married to another\nperson, it is dif\ufb01cult to predict to which person in particular.\nFor the latter, examples based on type constraints alone are not\nvery informative. A better way to generate negative examples\nis to \u201cperturb\u201d true triples. In particular, let us de\ufb01ne\nD\u00b4 \u201c tpe\u2113, rk, ejq | ei \u2030 e\u2113^ pei, rk, ejq P D`u\nY tpei, rk, e\u2113q | ej \u2030 e\u2113^ pei, rk, ejq P D`u\nTo understand the difference between this approach and the\nCWA (where we assumed all valid unknown triples were\nfalse), let us consider the example in Figure 1. The CWA\nwould generate \u201cgood\u201d negative triples such as (LeonardNimoy,\nstarredIn, StarWars), (AlecGuinness, starredIn, StarTrek), etc.,\nbut also type-consistent but \u201cirrelevant\u201d negative triples such\nas (BarackObama, starredIn, StarTrek), etc. (We are assuming\n(for the sake of this example) there is a type Person but not\na type Actor.) The second approach (based on perturbation)\nwould not generate negative triples such as (BarackObama,\nstarredIn, StarTrek), since BarackObama does not participate\nin any starredIn events. This reduces the size of D\u00b4, and\nencourages it to focus on \u201cplausible\u201d negatives. (An even\nbetter method, used in Section IX, is to generate the candidate\ntriples from text extraction methods run on the Web. Many of\nthese triples will be false, due to extraction errors, but they\nde\ufb01ne a good set of \u201cplausible\u201d negatives.)\nAnother option to generate negative examples for training is\nto make a local-closed world assumption (LCWA) [107, 28],\nin which we assume that a KG is only locally complete. More\nprecisely, if we have observed any triple for a particular subject-\npredicate pair ei, rk, then we will assume that any non-existing\ntriple pei, rk, \u00a8q is indeed false and include them in D\u00b4. (The\nassumption is valid for functional relations, such as bornIn,\nbut not for set-valued relations, such as starredIn.) However,\nif we have not observed any triple at all for the pair ei, rk,\nwe will assume that all triples pei, rk, \u00a8q are unknown and not\ninclude them in D\u00b4.\n13\nC. Pairwise loss training\nGiven that the negative training examples are not always\nreally negative, an alternative approach to likelihood training\nis to try to make the probability (or in general, some scoring\nfunction) to be larger for true triples than for assumed-to-be-\nfalse triples. That is, we can de\ufb01ne the following objective\nfunction:\nmin\n\u0398\n\u00ff\nx`PD`\n\u00ff\nx\u00b4PD\u00b4\nLpfpx`; \u0398q, fpx\u00b4; \u0398qq ` \u03bb regp\u0398q (27)\nwhere Lpf, f 1q is a margin-based ranking loss function such\nas\nLpf, f 1q \u201c maxp1 ` f 1 \u00b4 f, 0q.\n(28)\nThis approach has several advantages. First, it does not assume\nthat negative examples are necessarily negative, just that they\nare \u201cmore negative\u201d than the positive ones. Second, it allows\nthe fp\u00a8q function to be any function, not just a probability (but\nwe do assume that larger f values mean the triple is more\nlikely to be correct).\nThis kind of objective function is easily optimized by\nstochastic gradient descent (SGD) [122]: at each iteration,\nwe just sample one positive and one negative example. SGD\nalso scales well to large datasets. However, it can take a long\ntime to converge. On the other hand, as discussed previously,\nsome models, when combined with the squared loss objective,\ncan be optimized using alternating least squares (ALS), which\nis typically much faster.\nD. Model selection\nAlmost all models discussed in previous sections include\none or more user-given parameters that are in\ufb02uential for the\nmodel\u2019s performance (e.g., dimensionality of latent feature mod-\nels, length of relation paths for PRA, regularization parameter\nfor penalized maximum likelihood training). Typically, cross-\nvalidation over random splits of D into training-, validation-,\nand test-sets is used to \ufb01nd good values for such parameters\nwithout over\ufb01tting (for more information on model selection\nin machine learning see e.g., [123]). For link prediction and\nentity resolution, the area under the ROC curve (AUC-ROC) or\nthe area under the precision-recall curve (AUC-PR) are good\nevaluation criteria. For data with a large number of negative\nexamples (as it is typically the case for knowledge graphs),\nit has been shown that AUC-PR can give a clearer picture of\nan algorithm\u2019s performance than AUC-ROC [124]. For entity\nresolution, the mean reciprocal rank (MRR) of the correct\nentity is an alternative evaluation measure.\nVIII. MARKOV RANDOM FIELDS\nIn this section we drop the assumption that the random\nvariables yijk in Y are conditionally independent. However,\nin the case of relational data and without the conditional\nindependence assumption, each yijk can depend on any of\nthe other Ne \u02c6 Ne \u02c6 Nr \u00b4 1 random variables in Y. Due to\nthis enormous number of possible dependencies, it becomes\nquickly intractable to estimate the joint distribution PpYq\nwithout further constraints, even for very small knowledge\ngraphs. To reduce the number of potential dependencies and\narrive at tractable models, in this section we develop template-\nbased graphical models that only consider a small fraction of\nall possible dependencies.\n(See [125] for an introduction to graphical models.)\nA. Representation\nGraphical models use graphs to encode dependencies be-\ntween random variables. Each random variable (in our case, a\npossible fact yijk) is represented as a node in the graph, while\neach dependency between random variables is represented as an\nedge. To distinguish such graphs from knowledge graphs, we\nwill refer to them as dependency graphs. It is important to be\naware of their key difference: while knowledge graphs encode\nthe existence of facts, dependency graphs encode statistical\ndependencies between random variables.\nTo avoid problems with cyclical dependencies, it is common\nto use undirected graphical models, also called Markov Random\nFields (MRFs).12 A MRF has the following form:\nPpY|\u03b8q \u201c 1\nZ\n\u017a\nc\n\u03c8pyc|\u03b8q\n(29)\nwhere \u03c8pyc|\u03b8q \u011b 0 is a potential function on the c-th subset\nof variables, in particular the c-th clique in the dependency\ngraph, and Z \u201c \u0159\ny\n\u015b\nc \u03c8pyc|\u03b8q is the partition function,\nwhich ensures that the distribution sums to one. The potential\nfunctions capture local correlations between variables in each\nclique c in the dependency graph. (Note that in undirected\ngraphical models, the local potentials do not have any proba-\nbilistic interpretation, unlike in directed graphical models.) This\nequation again de\ufb01nes a probability distribution over \u201cpossible\nworlds\u201d, i.e., over joint distribution assigned to the random\nvariables Y.\nThe structure of the dependency graph (which de\ufb01nes the\ncliques in Equation (29)) is derived from a template mechanism\nthat can be de\ufb01ned in a number of ways. A common approach\nis to use Markov logic [126], which is a template language\nbased on logical formulae:\nGiven a set of formulae F \u201c tFiuL\ni\u201c1, we create an edge\nbetween nodes in the dependency graph if the corresponding\nfacts occur in at least one grounded formula. A grounding of\na formula Fi is given by the (type consistent) assignment of\nentities to the variables in Fi. Furthermore, we de\ufb01ne \u03c8pyc|\u03b8q\nsuch that\nPpY|\u03b8q \u201c 1\nZ\n\u017a\nc\nexpp\u03b8cxcq\n(30)\nwhere xc denotes the number of true groundings of Fc in Y,\nand \u03b8c denotes the weight for formula Fc. If \u03b8c \u0105 0, we prefer\nworlds where formula Fc is satis\ufb01ed; if \u03b8c \u0103 0, we prefer\nworlds where formula Fc is violated. If \u03b8c \u201c 0, then formula\nFc is ignored.\nTo explain this further, consider a KG involving two types\nof entities, adults and children, and two types of relations,\nparentOf and marriedTo. Figure 6a depicts a sample KG with\nthree adults and one child. Obviously, these relations (edges)\n12Technically, since we are conditioning on some observed features x, this\nis a Conditional Random Field (CRF), but we will ignore this distinction.\n14\nare correlated, since people who share a common child are\noften married, while people rarely marry their own children. In\nMarkov logic, we represent these dependencies using formulae\nsuch as:\nF1 : px, parentOf, zq ^ py, parentOf, zq \u00f1 px, marriedTo, yq\nF2 : px, marriedTo, yq \u00f1 \u2423py, parentOf, xq\nRather than encoding the rule that adults cannot marry their\nown children using a formula, we will encode this as a hard\nconstraint into the type system. Similarly, we only allow adults\nto be parents of children. Thus, there are 6 possible facts\nin the knowledge graph. To create a dependency graph for\nthis KG and for this set of logical formulae F, we assign a\nbinary random variable to each possible fact, represented by a\ndiamond in Figure 6b, and create edges between these nodes if\nthe corresponding facts occur in grounded formulae F1 or F2.\nFor instance, grounding F1 with x \u201c a1, y \u201c a3, and z \u201c c,\ncreates the edges m13 \u00d1 p1c, m13 \u00d1 p3c, and p1c \u00d1 p3c.\nThe full dependency graph is shown in Figure 6c.\nThe process of generating the MRF graph by applying\ntemplated rules to a set of entities is known as grounding\nor instantiation. We note that the topology of the resulting\ngraph is quite different from the original KG. In particular,\nwe have one node per possible KG edge, and these nodes are\ndensely connected. This can cause computational dif\ufb01culties,\nas we discuss below.\nB. Inference\nThe inference problem consists of estimating the most\nprobable con\ufb01guration, y\u02da \u201c arg maxy ppy|\u03b8q, or the posterior\nmarginals ppyi|\u03b8q. In general, both of these problems are\ncomputationally intractable [125], so heuristic approximations\nmust be used.\nOne approach for computing posterior marginals is to use\nGibbs sampling (see, or example, [31, 127]) or MC-SAT [128].\nOne approach for computing the MAP estimate is to use the\nMPLP (max product linear programming) method [129]. See\n[125] for more details.\nIf one restricts the class of potential functions to be just\ndisjunctions (using OR and NOT, but no AND), then one\nobtains a (special case of) hinge loss MRF (HL-MRFs) [130],\nfor which ef\ufb01cient convex algorithms can be applied, based\non a continuous relaxation of the binary random variables.\nProbabilistic Soft Logic (PSL) [131] provides a convenient\nform of \u201csyntactic sugar\u201d for de\ufb01ning HL-MRFs, just as MLNs\nprovide a form of syntactic sugar for regular (boolean) MRFs.\nHL-MRFs have been shown to scale to fairly large knowledge\nbases [132].\nC. Learning\nThe \u201clearning\u201d problem for MRFs deals with specifying the\nform of the potential functions (sometimes called \u201cstructure\nlearning\u201d) as well as the values for the numerical parameters\n\u03b8. In the case of MRFs for KGs, the potential functions are\noften speci\ufb01ed in the form of logical rules, as illustrated above.\nIn this case, structure learning is equivalent to rule learning,\nWeb\nFreebase\nLatent Model\nObservable Model\nCombined Model\nInformation Extraction\nFusion Layer\nKnowledge Vault\nFig. 7. Architecture of the Knowledge Vault.\nwhich has been studied in a number of published works (see\nSection V-C and [107, 95]).\nThe parameter estimation problem (which is usually cast as\nmaximum likelihood or MAP estimation), although convex, is\nin general quite expensive, since it needs to call inference as\na subroutine. Therefore, various faster approximations, such\nas pseudo likelihood, have been developed (cf. relational\ndependency networks [133]).\nD. Discussion\nAlthough approaches based on MRFs are very \ufb02exible, it\nis in general harder to make scalable inference and devise\nlearning algorithms for this model class, compared to methods\nbased on observable or even latent feature models. In this\narticle, we have chosen to focus primarily on latent and graph\nfeature models because we have more experience with such\nmethods in the context of KGs. However, all three kinds of\napproaches to KG modeling are useful.\nIX. KNOWLEDGE VAULT: RELATIONAL LEARNING FOR\nKNOWLEDGE BASE CONSTRUCTION\nThe Knowledge Vault (KV) [28] is a very large-scale\nautomatically constructed knowledge base, which follows the\nFreebase schema (KV uses the 4469 most common predicates).\nIt is constructed in three steps. In the \ufb01rst step, facts are\nextracted from a host of Web sources such as natural language\ntext, tabular data, page structure, and human annotations (the\nextractors are described in detail in [28]). Second, an SRL\nmodel is trained on Freebase to serve as a \u201cprior\u201d for computing\nthe probability of (new) edges. Finally, the con\ufb01dence in\nthe automatically extracted facts is evaluated using both the\nextraction scores and the prior SRL model.\nThe Knowledge Vault uses a combination of latent and\nobservable models to predict links in a knowledge graph. In\nparticular, it employs the ER-MLP model (Section IV-D) as a\nlatent feature model and PRA (Section V-C) as a graph feature\nmodel. In order to combine the two models, KV uses stacking\n15\na1\na2\na3\nc\n(a)\na1\na2\na3\nc\nm13\nm12\nm23\np3c\np2c\np1c\n(b)\nm13\nm12\nm23\np3c\np2c\np1c\n(c)\nFig. 6.\n(a) A small KG. There are 4 entities (circles): 3 adults (a1, a2, a3) and 1 child c There are 2 types of edges: adults may or may not be married to\neach other, as indicated by the red dashed edges, and the adults may or may not be parents of the child, as indicated by the blue dotted edges. (b) We add\nbinary random variables (represented by diamonds) to each KG edge. (c) We drop the entity nodes, and add edges between the random variables that belong to\nthe same clique potential, resulting in a standard MRF.\n(Section VI-B). To evaluate the link prediction performance,\nthese models were applied to a subset of Freebase. The ER-\nMLP system achieved an area under the ROC curve (AUC-\nROC) of 0.882, and the PRA approach achieved an almost\nidentical AUC-ROC of 0.884. The combination of both methods\nfurther increased the AUC-ROC to 0.911. To predict the \ufb01nal\nscore of a triple, the scores from the combined link-prediction\nmodel are further combined with various features derived from\nthe extracted triples. These include, for instance, the con\ufb01dence\nof the extractors and the number of (de-duplicated) Web pages\nfrom which the triples were extracted. Figure 7 provides a high\nlevel overview of the Knowledge Vault architecture.\nLet us give a qualitative example of the bene\ufb01ts of combining\nthe prior with the extractors (i.e., the Fusion Layer in Figure 7).\nConsider an extracted triple corresponding to the following\nrelation:13\n(Barry Richter, attended, University of Wisconsin-Madison).\nThe extraction con\ufb01dence for this triple (obtained by fusing\nmultiple extraction techniques) is just 0.14, since it was based\non the following two rather indirect statements:14\nIn the fall of 1989, Richter accepted a scholarship to\nthe University of Wisconsin, where he played for four\nyears and earned numerous individual accolades . . .\nand15\nThe Polar Caps\u2019 cause has been helped by the impact\nof knowledgable coaches such as Andringa, Byce\nand former UW teammates Chris Tancill and Barry\nRichter.\nHowever, we know from Freebase that Barry Richter was born\nand raised in Madison, Wisconsin. According to the prior\n13For clarity of presentation we show a simpli\ufb01ed triple. Please see [28]\nfor the actually extracted triples including compound value types (CVT).\n14Source:\nhttp://www.legendsofhockey.net/LegendsOfHockey/jsp/\nSearchPlayer.jsp?player=11377\n15Source:\nhttp://host.madison.com/sports/high-school/hockey/numbers-\ndwindling-for-once-mighty-madison-high-school-hockey-programs/article_\n95843e00-ec34-11df-9da9-001cc4c002e0.html\nmodel, people who were born and raised in a particular city\noften tend to study in the same city. This increases our prior\nbelief that Richter went to school there, resulting in a \ufb01nal\nfused belief of 0.61.\nCombining the prior model (learned using SRL methods)\nwith the information extraction model improved performance\nsigni\ufb01cantly, increasing the number of high con\ufb01dence triples16\nfrom 100M (based on extractors alone) to 271M (based on\nextractors plus prior). The Knowledge Vault is one of the\nlargest applications of SRL to knowledge base construction to\ndate. See [28] for further details.\nX. EXTENSIONS AND FUTURE WORK\nA. Non-binary relations\nSo far we completely focussed on binary relations; here we\ndiscuss how relations of other cardinalities can be handled.\nUnary relations: Unary relations refer to statements on\nproperties of entities, e.g., the height of a person. Such\ndata can naturally be represented by a matrix, in which\nrows represent entities, and columns represent attributes. [64]\nproposed a joint tensor-matrix factorization approach to learn\nsimultaneously from binary and unary relations via a shared\nlatent representation of entities. In this case, we may also need\nto modify the likelihood function, so it is Bernoulli for binary\nedge variables, and Gaussian (say) for numeric features and\nPoisson for count data (see [134]).\nHigher-arity relations: In knowledge graphs, higher-arity\nrelations are typically expressed via multiple binary rela-\ntions. In Section II, we expressed the ternary relationship\nplayedCharacterIn(LeonardNimoy, Spock, StarTrek-1) via two\nbinary relationships (LeonardNimoy, played, Spock) and (Spock,\ncharacterIn, StarTrek-1). However, there are multiple actors\nwho played Spock in different Star Trek movies, so we\nhave lost the correspondence between Leonard Nimoy and\nStarTrek-1. To model this using binary relations without loss\n16Triples with the calibrated probability of correctness above 90%.\n16\nof information, we can use auxiliary nodes to identify the\nrespective relationship. For instance, to model the relationship\nplayedCharacterIn(LeonardNimoy, Spock, StarTrek-1), we can\nwrite\nsubject\npredicate\nobject\n(LeonardNimoy,\nactor,\nMovieRole-1)\n(MovieRole-1,\nmovie,\nStarTreck-1)\n(MovieRole-1,\ncharacter,\nSpock)\nwhere we used the auxiliary entity MovieRole-1 to uniquely\nidentify this particular relationship. In most applications\nauxiliary entities get an identi\ufb01er; if not they are referred to as\nblank nodes. In Freebase auxiliary nodes are called Compound\nValue Types (CVT).\nSince higher-arity relations involving time and location\nare relatively common, the YAGO2 project extended the\nSPO triple format to the (subject, predicate, object, time,\nlocation) (SPOTL) format to model temporal and spatial\ninformation about relationships explicitly, without transforming\nthem to binary relations [27]. Furthermore, there has also been\nwork on extracting higher-arity relations directly from natural\nlanguage [135].\nA related issue is that the truth-value of a fact can change\nover time. For example, Google\u2019s current CEO is Larry Page,\nbut from 2001 to 2011 it was Eric Schmidt. Both facts are\ncorrect, but only during the speci\ufb01ed time interval. For this\nreason, Freebase allows some facts to be annotated with\nbeginning and end dates, using CVT (compound value type)\nconstructs, which represent n-ary relations via auxiliary nodes.\nIn the future, it is planned to extend the KV system to model\nsuch temporal facts. However, this is non-trivial, since it is not\nalways easy to infer the duration of a fact from text, since it is\nnot necessarily related to the timestamp of the corresponding\nsource (cf. [136]).\nAs an alternative to the usage of auxiliary nodes, a set of\nn\u00b4th-arity relations can be represented by a single n ` 1\u00b4th-\norder tensor. RESCAL can easily be generalized to higher-arity\nrelations and can be solved by higher-order tensor factorization\nor by neural network models with the corresponding number\nof entity representations as inputs [134].\nB. Hard constraints: types, functional constraints, and others\nImposing hard constraints on the allowed triples in knowl-\nedge graphs can be useful. Powerful ontology languages such as\nthe Web Ontology Language (OWL) [137] have been developed,\nin which complex constraints can be formulated. However,\nreasoning with ontologies is computationally demanding, and\nhard constraints are often violated in real-world data [138, 139].\nFortunately, machine learning methods can be robust in the\nface of contradictory evidence.\nDeterministic dependencies: Triples in relations such as\nsubClassOf and isLocatedIn follow clear deterministic depen-\ndencies such as transitivity. For example, if Leonard Nimoy\nwas born in Boston, we can conclude that he was born\nin Massachusetts, that he was born in the USA, that he\nwas born in North America, etc. One way to consider such\nontological constraints is to precompute all true triples that\ncan be derived from the constraints and to add them to\nthe knowledge graph prior to learning. The precomputation\nof triples according to ontological constraints is also called\nmaterialization. However, on large knowledge graphs, full\nmaterialization can be computationally demanding.\nType constraints: Often relations only make sense when\napplied to entities of the right type. For example, the domain\nand the range of marriedTo is limited to entities which are\npersons. Modelling type constraints explicitly requires complex\nmanual work. An alternative is to learn approximate type\nconstraints by simply considering the observed types of subjects\nand objects in a relation. The standard RESCAL model has\nbeen extended by [74] and [69] to handle type constraints of\nrelations ef\ufb01ciently. As a result, the rank required for a good\nRESCAL model can be greatly reduced. Furthermore, [85]\nconsidered learning latent representations for the argument\nslots in a relation to learn the correct types from data.\nFunctional constraints and mutual exclusiveness: Although\nthe methods discussed in Sections IV and V can model long-\nrange and global dependencies between triples, they do not\nexplicitly enforce functional constraints that induce mutual\nexclusivity between possible values. For instance, a person\nis born in exactly one city, etc. If one of the these values\nis observed, then observable graph models can prevent other\nvalues from being asserted, but if all the values are unknown,\nthe resulting mutual exclusion constraint can be hard to deal\nwith computationally.\nC. Generalizing to new entities and relations\nIn addition to missing facts, there are many entities that are\nmentioned on the Web but are currently missing in knowledge\ngraphs like Freebase and YAGO. If new entities or predicates\nare added to a KG, one might want to avoid retraining the\nmodel due to runtime considerations. Given the current model\nand a set of newly observed relationships, latent representations\nof new entities can be calculated approximately in both\ntensor factorization models and in neural networks, by \ufb01nding\nrepresentations that explain the newly observed relationships\nrelative to the current model. Similarly, it has been shown that\nthe relation-speci\ufb01c weights Wk in the RESCAL model can\nbe calculated ef\ufb01ciently for new relation types given already\nderived latent representations of entities [140].\nD. Querying probabilistic knowledge graphs\nRESCAL and KV can be viewed as probabilistic databases\n(see, e.g., [141, 142]). In the Knowledge Vault, only the\nprobabilities of triples are queried. Some applications might\nrequire more complex queries such as: Who is born in Rome\nand likes someone who is a child of Albert Einstein. It is known\nthat queries involving joins (existentially quanti\ufb01ed variables)\nare expensive to calculate in probabilistic databases ([141]).\nIn [140], it was shown how some queries involving joins can\nbe ef\ufb01ciently handled within the RESCAL framework.\nE. Trustworthiness of knowledge graphs\nAutomatically constructed knowledge bases are only as good\nas the sources from which the facts are extracted. Prior studies\n17\nin the \ufb01eld of data fusion have developed numerous approaches\nfor modelling the correctness of information supplied by\nmultiple sources in the presence of possible data con\ufb02icts (see\n[143, 144] for recent surveys). However, the key assumption in\ndata fusion\u2014namely, that the facts provided by the sources are\nindeed stated by them\u2014is often violated when the information\nis extracted automatically. If a given source contains a mistake,\nit could be because the source actually contains a false fact, or\nbecause the fact has been extracted incorrectly. A recent study\n[145] has formulated the problem of knowledge fusion, where\nthe above assumption is no longer made, and the correctness\nof information extractors is modeled explicitly. A follow-up\nstudy by the authors [146] developed several approaches for\nsolving the knowledge fusion problem, and applied them to\nestimate the trustworthiness of facts in the Knowledge Vault\n(cf. Section IX).\nXI. CONCLUDING REMARKS\nKnowledge graphs have found important applications in\nquestion answering, structured search, exploratory search, and\ndigital assistants. We provided a review of state-of-the-art\nstatistical relational learning (SRL) methods applied to very\nlarge knowledge graphs. We also demonstrated how statistical\nrelational learning can be used in conjunction with machine\nreading and information extraction methods to automatically\nbuild such knowledge repositories. As a result, we showed\nhow to create a truly massive, machine-interpretable \u201csemantic\nmemory\u201d of facts, which is already empowering numerous\npractical applications. However, although these KGs are\nimpressive in their size, they still fall short of representing\nmany kinds of knowledge that humans possess. Notably missing\nare representations of \u201ccommon sense\u201d facts (such as the fact\nthat water is wet, and wet things can be slippery), as well\nas \u201cprocedural\u201d or how-to knowledge (such as how to drive\na car or how to send an email). Representing, learning, and\nreasoning with these kinds of knowledge remains the next\nfrontier for AI and machine learning.\nXII. APPENDIX\nA. RESCAL is a special case of NTN\nHere we show how the RESCAL model of Section IV-A is a\nspecial case of the neural tensor model (NTN) of Section IV-E.\nTo see this, note that RESCAL has the form\nf RESCAL\nijk\n\u201c eJ\ni Wkej \u201c wJ\nk rej b eis\n(31)\nNext, note that\nv b u \u201c vec\n`\nuvJ\u02d8\n\u201c ruJB1v, . . . , uJBnvs\nwhere n \u201c |u||v|, and Bk is a matrix of all 0s except for a\nsingle 1 element in the k\u2019th position, which \u201cplucks out\u201d the\ncorresponding entries from the u and v matrices. For example,\n\u02c6\nu1\nu2\n\u02d9 `\nv1\nv2\n\u02d8\n\u201c\n\u00ab\u02c6\nu1\nu2\n\u02d9J \u02c6\n1\n0\n0\n0\n\u02d9 \u02c6\nv1\nv2\n\u02d9\n,\n. . . ,\n\u02c6\nu1\nu2\n\u02d9J \u02c6\n0\n0\n0\n1\n\u02d9 \u02c6\nv1\nv2\n\u02d9\ufb00\n.(32)\nIn general, de\ufb01ne \u03b4ij as a matrix of all 0s except for entry\npi, jq which is 1. Then if we de\ufb01ne Bk \u201c r\u03b41,1, . . . , \u03b4He,Hes\nwe have\nhb\nijk \u201c\n\u201d\neJ\ni B1\nkej, . . . , eJ\ni BHb\nk ej\n\u0131\n\u201c ej b ei\nFinally, if we de\ufb01ne Ak as the empty matrix (so ha\nijk is\nunde\ufb01ned), and gpuq \u201c u as the identity function, then the\nNTN equation\nf NTN\nijk \u201c wJ\nk gprha\nijk; hb\nijksq\nmatches Equation 31.\nACKNOWLEDGMENT\nMaximilian Nickel acknowledges support by the Center for\nBrains, Minds and Machines (CBMM), funded by NSF STC\naward CCF-1231216. Volker Tresp acknowledges support by\nthe German Federal Ministry for Economic Affairs and Energy,\ntechnology program \u201cSmart Data\u201d (grant 01MT14001).\nREFERENCES\n[1] L. Getoor and B. Taskar, Eds., Introduction to Statistical\nRelational Learning.\nMIT Press, 2007.\n[2] S. Dzeroski and N. Lavra\u02c7c, Relational Data Mining.\nSpringer Science & Business Media, 2001.\n[3] L. De Raedt, Logical and relational learning.\nSpringer,\n2008.\n[4] F. M. Suchanek, G. Kasneci, and G. Weikum, \u201cYago:\nA Core of Semantic Knowledge,\u201d in Proceedings of\nthe 16th International Conference on World Wide Web.\nNew York, NY, USA: ACM, 2007, pp. 697\u2013706.\n[5] S.\nAuer,\nC.\nBizer,\nG.\nKobilarov,\nJ.\nLehmann,\nR. Cyganiak, and Z. Ives, \u201cDBpedia: A Nucleus for a\nWeb of Open Data,\u201d in The Semantic Web.\nSpringer\nBerlin Heidelberg, 2007, vol. 4825, pp. 722\u2013735.\n[6] A. Carlson, J. Betteridge, B. Kisiel, B. Settles, E. R. H.\nJr, and T. M. Mitchell, \u201cToward an Architecture for\nNever-Ending Language Learning,\u201d in Proceedings of\nthe Twenty-Fourth Conference on Arti\ufb01cial Intelligence\n(AAAI 2010).\nAAAI Press, 2010, pp. 1306\u20131313.\n[7] K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and J. Tay-\nlor, \u201cFreebase: a collaboratively created graph database\nfor structuring human knowledge,\u201d in Proceedings of\nthe 2008 ACM SIGMOD international conference on\nManagement of data.\nACM, 2008, pp. 1247\u20131250.\n[8] A.\nSinghal,\n\u201cIntroducing\nthe\nKnowledge\nGraph:\nthings,\nnot\nstrings,\u201d\nMay\n2012.\n[Online].\nAvailable:\nhttp://googleblog.blogspot.com/2012/05/\nintroducing-knowledge-graph-things-not.html\n[9] G. Weikum and M. Theobald, \u201cFrom information to\nknowledge: harvesting entities and relationships from\nWeb sources,\u201d in Proceedings of the twenty-ninth ACM\nSIGMOD-SIGACT-SIGART symposium on Principles of\ndatabase systems.\nACM, 2010, pp. 65\u201376.\n[10] J. Fan, R. Hoffman, A. A. Kalyanpur, S. Riedel,\nF. Suchanek, and P. P. Talukdar, \u201cAKBC-WEKEX\n18\n2012: The Knowledge Extraction Workshop at NAACL-\nHLT,\u201d 2012. [Online]. Available: https://akbcwekex2012.\nwordpress.com/\n[11] R. Davis, H. Shrobe, and P. Szolovits, \u201cWhat is a\nknowledge representation?\u201d AI Magazine, vol. 14, no. 1,\npp. 17\u201333, 1993.\n[12] J. F. Sowa, \u201cSemantic networks,\u201d Encyclopedia of\nCognitive Science, 2006.\n[13] M. Minsky, \u201cA framework for representing knowledge,\u201d\nMIT-AI Laboratory Memo 306, 1974.\n[14] T. Berners-Lee, J. Hendler, and O. Lassila, \u201cThe\nSemantic Web,\u201d 2001. [Online]. Available: http://www.\nscienti\ufb01camerican.com/article/the-semantic-web/\n[15] T. Berners-Lee, \u201cLinked Data - Design Issues,\u201d Jul. 2006.\n[Online]. Available: http://www.w3.org/DesignIssues/\nLinkedData.html\n[16] C. Bizer, T. Heath, and T. Berners-Lee, \u201cLinked data-the\nstory so far,\u201d International Journal on Semantic Web\nand Information Systems, vol. 5, no. 3, pp. 1\u201322, 2009.\n[17] G. Klyne and J. J. Carroll, \u201cResource Description\nFramework (RDF): Concepts and Abstract Syntax,\u201d Feb.\n2004. [Online]. Available: http://www.w3.org/TR/2004/\nREC-rdf-concepts-20040210/\n[18] R. Cyganiak, D. Wood, and M. Lanthaler, \u201cRDF\n1.1\nConcepts\nand\nAbstract\nSyntax,\u201d\nFeb.\n2014.\n[Online]. Available: http://www.w3.org/TR/2014/REC-\nrdf11-concepts-20140225/\n[19] R. Brachman and H. Levesque, Knowledge Representa-\ntion and Reasoning.\nSan Francisco, CA, USA: Morgan\nKaufmann Publishers Inc., 2004.\n[20] J. F. Sowa, Knowledge Representation: Logical, Philo-\nsophical and Computational Foundations. Paci\ufb01c Grove,\nCA, USA: Brooks/Cole Publishing Co., 2000.\n[21] Y. Sun and J. Han, \u201cMining Heterogeneous Information\nNetworks: Principles and Methodologies,\u201d Synthesis\nLectures on Data Mining and Knowledge Discovery,\nvol. 3, no. 2, pp. 1\u2013159, 2012.\n[22] R. West, E. Gabrilovich, K. Murphy, S. Sun, R. Gupta,\nand D. Lin, \u201cKnowledge Base Completion via Search-\nBased Question Answering,\u201d in Proceedings of the 23rd\nInternational Conference on World Wide Web, 2014, pp.\n515\u2013526.\n[23] D. B. Lenat, \u201cCYC: A Large-scale Investment in\nKnowledge Infrastructure,\u201d Commun. ACM, vol. 38,\nno. 11, pp. 33\u201338, Nov. 1995.\n[24] G. A. Miller, \u201cWordNet: A Lexical Database for\nEnglish,\u201d Commun. ACM, vol. 38, no. 11, pp. 39\u201341,\nNov. 1995.\n[25] O. Bodenreider, \u201cThe Uni\ufb01ed Medical Language System\n(UMLS): integrating biomedical terminology,\u201d Nucleic\nAcids Research, vol. 32, no. Database issue, pp. D267\u2013\n270, Jan. 2004.\n[26] D. Vrande\u02c7ci\u00b4c and M. Kr\u00f6tzsch, \u201cWikidata: a free\ncollaborative knowledgebase,\u201d Communications of the\nACM, vol. 57, no. 10, pp. 78\u201385, 2014.\n[27] J.\nHoffart,\nF.\nM.\nSuchanek,\nK.\nBerberich,\nand\nG. Weikum, \u201cYAGO2: a spatially and temporally\nenhanced knowledge base from Wikipedia,\u201d Arti\ufb01cial\nIntelligence, vol. 194, pp. 28\u201361, 2013.\n[28] X.\nDong,\nE.\nGabrilovich,\nG.\nHeitz,\nW.\nHorn,\nN. Lao, K. Murphy, T. Strohmann, S. Sun, and\nW. Zhang, \u201cKnowledge Vault: A Web-scale Approach\nto Probabilistic Knowledge Fusion,\u201d in Proceedings\nof the 20th ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining.\nNew York,\nNY, USA: ACM, 2014, pp. 601\u2013610.\n[29] N. Nakashole, G. Weikum, and F. Suchanek, \u201cPATTY:\nA Taxonomy of Relational Patterns with Semantic\nTypes,\u201d in Proceedings of the 2012 Joint Conference on\nEmpirical Methods in Natural Language Processing and\nComputational Natural Language Learning, 2012, pp.\n1135\u20131145.\n[30] N. Nakashole, M. Theobald, and G. Weikum, \u201cScalable\nknowledge harvesting with high precision and high\nrecall,\u201d in Proceedings of the fourth ACM international\nconference on Web search and data mining.\nACM,\n2011, pp. 227\u2013236.\n[31] F. Niu, C. Zhang, C. R\u00e9, and J. Shavlik, \u201cElementary:\nLarge-scale knowledge-base construction via machine\nlearning and statistical inference,\u201d International Journal\non Semantic Web and Information Systems (IJSWIS),\nvol. 8, no. 3, pp. 42\u201373, 2012.\n[32] A. Fader, S. Soderland, and O. Etzioni, \u201cIdentifying\nrelations\nfor\nopen\ninformation\nextraction,\u201d\nin\nProceedings of the Conference on Empirical Methods in\nNatural Language Processing.\nStroudsburg, PA, USA:\nAssociation for Computational Linguistics, 2011, pp.\n1535\u20131545.\n[33] M. Schmitz, R. Bart, S. Soderland, O. Etzioni, and others,\n\u201cOpen language learning for information extraction,\u201d in\nProceedings of the 2012 Joint Conference on Empirical\nMethods in Natural Language Processing and Compu-\ntational Natural Language Learning.\nAssociation for\nComputational Linguistics, 2012, pp. 523\u2013534.\n[34] J. Fan, D. Ferrucci, D. Gondek, and A. Kalyanpur,\n\u201cPrismatic: Inducing knowledge from a large scale\nlexicalized relation resource,\u201d in Proceedings of the\nNAACL HLT 2010 First International Workshop on\nFormalisms and Methodology for Learning by Reading.\nAssociation for Computational Linguistics, 2010, pp.\n122\u2013127.\n[35] B. Suh, G. Convertino, E. H. Chi, and P. Pirolli, \u201cThe\nSingularity is Not Near: Slowing Growth of Wikipedia,\u201d\nin Proceedings of the 5th International Symposium on\nWikis and Open Collaboration.\nNew York, NY, USA:\nACM, 2009, pp. 8:1\u20138:10.\n[36] J. Biega, E. Kuzey, and F. M. Suchanek, \u201cInside\nYAGO2s:\nA\ntransparent\ninformation\nextraction\narchitecture,\u201d in Proceedings of the 22Nd International\nConference on World Wide Web.\nRepublic and Canton\nof Geneva, Switzerland: International World Wide Web\nConferences Steering Committee, 2013, pp. 325\u2013328.\n[37] O. Etzioni, A. Fader, J. Christensen, S. Soderland, and\nM. Mausam, \u201cOpen Information Extraction: The Second\nGeneration,\u201d\nin\nProceedings of the Twenty-Second\nInternational Joint Conference on Arti\ufb01cial Intelligence\n19\n- Volume Volume One.\nBarcelona, Catalonia, Spain:\nAAAI Press, 2011, pp. 3\u201310.\n[38] D. B. Lenat and E. A. Feigenbaum, \u201cOn the thresholds\nof knowledge,\u201d Arti\ufb01cial intelligence, vol. 47, no. 1, pp.\n185\u2013250, 1991.\n[39] R.\nQian,\n\u201cUnderstand\nYour\nWorld\nwith\nBing,\nbing\nsearch\nblog,\u201d\nMar.\n2013.\n[On-\nline]. Available: http://blogs.bing.com/search/2013/03/\n21/understand-your-world-with-bing/\n[40] D.\nFerrucci,\nE.\nBrown,\nJ.\nChu-Carroll,\nJ.\nFan,\nD. Gondek, A. A. Kalyanpur, A. Lally, J. W. Murdock,\nE. Nyberg, J. Prager, and others, \u201cBuilding Watson: An\noverview of the DeepQA project,\u201d AI magazine, vol. 31,\nno. 3, pp. 59\u201379, 2010.\n[41] F. Belleau, M.-A. Nolin, N. Tourigny, P. Rigault,\nand J. Morissette, \u201cBio2RDF: towards a mashup to\nbuild bioinformatics knowledge systems,\u201d Journal of\nBiomedical Informatics, vol. 41, no. 5, pp. 706\u2013716,\n2008.\n[42] A.\nRuttenberg,\nJ.\nA.\nRees,\nM.\nSamwald,\nand\nM. S. Marshall, \u201cLife sciences on the Semantic\nWeb: the Neurocommons and beyond,\u201d Brie\ufb01ngs in\nBioinformatics, vol. 10, no. 2, pp. 193\u2013204, Mar. 2009.\n[43] V. Momtchev, D. Peychev, T. Primov, and G. Georgiev,\n\u201cExpanding the pathway and interaction knowledge in\nlinked life data,\u201d Proc. of International Semantic Web\nChallenge, 2009.\n[44] G. Angeli and C. Manning, \u201cPhilosophers are Mortal:\nInferring the Truth of Unseen Facts,\u201d in Proceedings of\nthe Seventeenth Conference on Computational Natural\nLanguage Learning.\nSo\ufb01a, Bulgaria: Association for\nComputational Linguistics, Aug. 2013, pp. 133\u2013142.\n[45] B. Taskar, M.-F. Wong, P. Abbeel, and D. Koller, \u201cLink\nPrediction in Relational Data,\u201d in Advances in Neural\nInformation Processing Systems, S. Thrun, L. Saul, and\nB. Sch\u00f6lkopf, Eds., vol. 16.\nCambridge, MA: MIT\nPress, 2004.\n[46] L. Getoor and C. P. Diehl, \u201cLink mining: a survey,\u201d\nACM SIGKDD Explorations Newsletter, vol. 7, no. 2,\npp. 3\u201312, 2005.\n[47] H. B. Newcombe, J. M. Kennedy, S. J. Axford, and\nA. P. James, \u201cAutomatic Linkage of Vital Records\nComputers can be used to extract \"follow-up\" statistics\nof families from \ufb01les of routine records,\u201d Science, vol.\n130, no. 3381, pp. 954\u2013959, Oct. 1959.\n[48] S. Tejada, C. A. Knoblock, and S. Minton, \u201cLearning\nobject identi\ufb01cation rules for information integration,\u201d\nInformation Systems, vol. 26, no. 8, pp. 607\u2013633, 2001.\n[49] E. Rahm and P. A. Bernstein, \u201cA survey of approaches\nto automatic schema matching,\u201d the VLDB Journal,\nvol. 10, no. 4, pp. 334\u2013350, 2001.\n[50] A. Culotta and A. McCallum, \u201cJoint deduplication\nof\nmultiple\nrecord\ntypes\nin\nrelational\ndata,\u201d\nin\nProceedings of the 14th ACM international conference\non Information and knowledge management.\nACM,\n2005, pp. 257\u2013258.\n[51] P. Singla and P. Domingos, \u201cEntity Resolution with\nMarkov Logic,\u201d in Data Mining, 2006. ICDM \u201906. Sixth\nInternational Conference on, Dec. 2006, pp. 572\u2013582.\n[52] I. Bhattacharya and L. Getoor, \u201cCollective entity\nresolution in relational data,\u201d ACM Trans. Knowl.\nDiscov. Data, vol. 1, no. 1, Mar. 2007.\n[53] S. E. Whang and H. Garcia-Molina, \u201cJoint Entity\nResolution,\u201d in 2012 IEEE 28th International Conference\non Data Engineering.\nWashington, DC, USA: IEEE\nComputer Society, 2012, pp. 294\u2013305.\n[54] S. Fortunato, \u201cCommunity detection in graphs,\u201d Physics\nReports, vol. 486, no. 3, pp. 75\u2013174, 2010.\n[55] M.\nE.\nJ.\nNewman,\n\u201cThe\nstructure\nof\nscienti\ufb01c\ncollaboration networks,\u201d Proceedings of the National\nAcademy of Sciences, vol. 98, no. 2, pp. 404\u2013409, Jan.\n2001.\n[56] D. Liben-Nowell and J. Kleinberg, \u201cThe link-prediction\nproblem for social networks,\u201d Journal of the American\nsociety for information science and technology, vol. 58,\nno. 7, pp. 1019\u20131031, 2007.\n[57] D. Jensen and J. Neville, \u201cLinkage and Autocorrelation\nCause Feature Selection Bias in Relational Learning,\u201d in\nProceedings of the Nineteenth International Conference\non Machine Learning.\nSan Francisco, CA, USA:\nMorgan Kaufmann Publishers Inc., 2002, pp. 259\u2013266.\n[58] P. W. Holland, K. B. Laskey, and S. Leinhardt,\n\u201cStochastic blockmodels: First steps,\u201d Social networks,\nvol. 5, no. 2, pp. 109\u2013137, 1983.\n[59] C. J. Anderson, S. Wasserman, and K. Faust, \u201cBuilding\nstochastic blockmodels,\u201d Social Networks, vol. 14, no.\n1\u20132, pp. 137\u2013161, 1992, special Issue on Blockmodels.\n[60] P.\nHoff,\n\u201cModeling\nhomophily\nand\nstochastic\nequivalence in symmetric relational data,\u201d in Advances\nin Neural Information Processing Systems 20.\nCurran\nAssociates, Inc., 2008, pp. 657\u2013664.\n[61] J. C. Platt, \u201cProbabilities for SV Machines,\u201d in Advances\nin Large Margin Classi\ufb01ers.\nMIT Press, 1999, pp. 61\u2013\n74.\n[62] P. Orbanz and D. M. Roy, \u201cBayesian models of graphs,\narrays and other exchangeable random structures,\u201d IEEE\nTrans. Pattern Analysis and Machine Intelligence, 2015.\n[63] M. Nickel, V. Tresp, and H.-P. Kriegel, \u201cA Three-Way\nModel for Collective Learning on Multi-Relational Data,\u201d\nin Proceedings of the 28th International Conference on\nMachine Learning, 2011, pp. 809\u2013816.\n[64] \u2014\u2014, \u201cFactorizing YAGO: scalable machine learning\nfor linked data,\u201d in Proceedings of the 21st International\nConference on World Wide Web, 2012, pp. 271\u2013280.\n[65] M. Nickel, \u201cTensor factorization for relational learning,\u201d\nPhD Thesis, Ludwig-Maximilians-Universit\u00e4t M\u00fcnchen,\nAug. 2013.\n[66] Y. Koren, R. Bell, and C. Volinsky, \u201cMatrix factorization\ntechniques for recommender systems,\u201d IEEE Computer,\nvol. 42, no. 8, pp. 30\u201337, 2009.\n[67] T. G. Kolda and B. W. Bader, \u201cTensor Decompositions\nand Applications,\u201d SIAM Review, vol. 51, no. 3, pp.\n455\u2013500, 2009.\n[68] M. Nickel and V. Tresp, \u201cLogistic Tensor-Factorization\nfor Multi-Relational Data,\u201d in Structured Learning:\nInferring Graphs from Structured and Unstructured\n20\nInputs (SLG 2013). Workshop at ICML\u201913, 2013.\n[69] K.-W. Chang, W.-t. Yih, B. Yang, and C. Meek, \u201cTyped\nTensor Decomposition of Knowledge Bases for Relation\nExtraction,\u201d in Proceedings of the 2014 Conference\non Empirical Methods in Natural Language Processing.\nACL \u2013 Association for Computational Linguistics, Oct.\n2014.\n[70] S.\nKok\nand\nP.\nDomingos,\n\u201cStatistical\nPredicate\nInvention,\u201d in Proceedings of the 24th International\nConference on Machine Learning.\nNew York, NY,\nUSA: ACM, 2007, pp. 433\u2013440.\n[71] Z. Xu, V. Tresp, K. Yu, and H.-P. Kriegel, \u201cIn\ufb01nite\nHidden Relational Models,\u201d in Proceedings of the 22nd\nInternational Conference on Uncertainity in Arti\ufb01cial\nIntelligence.\nAUAI Press, 2006, pp. 544\u2013551.\n[72] C. Kemp, J. B. Tenenbaum, T. L. Grif\ufb01ths, T. Yamada,\nand\nN.\nUeda,\n\u201cLearning\nsystems\nof\nconcepts\nwith an in\ufb01nite relational model,\u201d in Proceedings\nof the Twenty-First National Conference on Arti\ufb01cial\nIntelligence, vol. 3, 2006, p. 5.\n[73] I. Sutskever, J. B. Tenenbaum, and R. R. Salakhutdinov,\n\u201cModelling Relational Data using Bayesian Clustered\nTensor Factorization,\u201d in Advances in Neural Information\nProcessing Systems 22, 2009, pp. 1821\u20131828.\n[74] D. Krompa\u00df, M. Nickel, and V. Tresp, \u201cLarge-Scale Fac-\ntorization of Type-Constrained Multi-Relational Data,\u201d\nin Proceedings of the 2014 International Conference\non Data Science and Advanced Analytics (DSAA\u20192014),\n2014.\n[75] M. Nickel and V. Tresp, \u201cLearning Taxonomies\nfrom Multi-Relational Data via Hierarchical Link-\nBased Clustering,\u201d in Learning Semantics. Workshop at\nNIPS\u201911, Granada, Spain, 2011.\n[76] T.\nG.\nKolda,\nB.\nW.\nBader,\nand\nJ.\nP.\nKenny,\n\u201cHigher-order web link analysis using multilinear\nalgebra,\u201d in Proceedings of the Fifth IEEE International\nConference on Data Mining.\nWashington, DC, USA:\nIEEE Computer Society, 2005, pp. 242\u2013249.\n[77] T. Franz, A. Schultz, S. Sizov, and S. Staab, \u201cTriplerank:\nRanking semantic web data by tensor decomposition,\u201d\nThe Semantic Web-ISWC 2009, pp. 213\u2013228, 2009.\n[78] L. Drumond, S. Rendle, and L. Schmidt-Thieme, \u201cPre-\ndicting RDF Triples in Incomplete Knowledge Bases\nwith Tensor Factorization,\u201d in Proceedings of the 27th\nAnnual ACM Symposium on Applied Computing.\nRiva\ndel Garda, Italy: ACM, 2012, pp. 326\u2013331.\n[79] S. Rendle and L. Schmidt-Thieme, \u201cPairwise interaction\ntensor factorization for personalized tag recommenda-\ntion,\u201d in Proceedings of the third ACM International\nConference on Web Search and Data Mining.\nACM,\n2010, pp. 81\u201390.\n[80] S.\nRendle,\n\u201cScaling\nfactorization\nmachines\nto\nrelational data,\u201d in Proceedings of the 39th international\nconference on Very Large Data Bases.\nTrento, Italy:\nVLDB Endowment, 2013, pp. 337\u2013348.\n[81] R. Jenatton, N. L. Roux, A. Bordes, and G. R. Obozinski,\n\u201cA latent factor model for highly multi-relational data,\u201d\nin Advances in Neural Information Processing Systems\n25.\nCurran Associates, Inc., 2012, pp. 3167\u20133175.\n[82] P. Miettinen, \u201cBoolean Tensor Factorizations,\u201d in 2011\nIEEE 11th International Conference on Data Mining,\nDec. 2011, pp. 447\u2013456.\n[83] D. Erdos and P. Miettinen, \u201cDiscovering Facts with\nBoolean Tensor Tucker Decomposition,\u201d in Proceedings\nof the 22nd ACM International Conference on Confer-\nence on Information & Knowledge Management.\nNew\nYork, NY, USA: ACM, 2013, pp. 1569\u20131572.\n[84] X. Jiang, V. Tresp, Y. Huang, and M. Nickel, \u201cLink\nPrediction in Multi-relational Graphs using Additive\nModels.\u201d in Proceedings of International Workshop on\nSemantic Technologies meet Recommender Systems &\nBig Data at the ISWC, M. de Gemmis, T. D. Noia,\nP. Lops, T. Lukasiewicz, and G. Semeraro, Eds., vol.\n919.\nCEUR Workshop Proceedings, 2012, pp. 1\u201312.\n[85] S. Riedel, L. Yao, B. M. Marlin, and A. McCallum,\n\u201cRelation Extraction with Matrix Factorization and Uni-\nversal Schemas,\u201d in Joint Human Language Technology\nConference/Annual Meeting of the North American\nChapter of the Association for Computational Linguistics\n(HLT-NAACL \u201913), Jun. 2013.\n[86] V. Tresp, Y. Huang, M. Bundschus, and A. Rettinger,\n\u201cMaterializing and querying learned knowledge,\u201d Proc.\nof IRMLeS, vol. 2009, 2009.\n[87] Y. Huang, V. Tresp, M. Nickel, A. Rettinger, and H.-P.\nKriegel, \u201cA scalable approach for statistical learning in\nsemantic graphs,\u201d Semantic Web journal SWj, 2013.\n[88] P. Smolensky, \u201cTensor product variable binding and the\nrepresentation of symbolic structures in connectionist\nsystems,\u201d Arti\ufb01cial intelligence, vol. 46, no. 1, pp.\n159\u2013216, 1990.\n[89] G.\nS.\nHalford,\nW.\nH.\nWilson,\nand\nS.\nPhillips,\n\u201cProcessing capacity de\ufb01ned by relational complexity:\nImplications\nfor\ncomparative,\ndevelopmental,\nand\ncognitive psychology,\u201d Behavioral and Brain Sciences,\nvol. 21, no. 06, pp. 803\u2013831, 1998.\n[90] T. Plate, \u201cA common framework for distributed represen-\ntation schemes for compositional structure,\u201d Connection-\nist systems for knowledge representation and deduction,\npp. 15\u201334, 1997.\n[91] T. Mikolov, K. Chen, G. Corrado, and J. Dean,\n\u201cEf\ufb01cient Estimation of Word Representations in Vector\nSpace,\u201d in Proceedings of Workshop at ICLR, 2013.\n[92] R. Socher, D. Chen, C. D. Manning, and A. Ng,\n\u201cReasoning With Neural Tensor Networks for Knowledge\nBase Completion,\u201d in Advances in Neural Information\nProcessing Systems 26.\nCurran Associates, Inc., 2013,\npp. 926\u2013934.\n[93] A. Bordes, J. Weston, R. Collobert, and Y. Bengio,\n\u201cLearning Structured Embeddings of Knowledge Bases,\u201d\nin Proceedings of the Twenty-Fifth AAAI Conference on\nArti\ufb01cial Intelligence, San Francisco, USA, 2011.\n[94] A. Bordes, N. Usunier, A. Garcia-Duran, J. Weston,\nand O. Yakhnenko, \u201cTranslating Embeddings for\nModeling Multi-relational Data,\u201d in Advances in Neural\nInformation Processing Systems 26.\nCurran Associates,\nInc., 2013, pp. 2787\u20132795.\n21\n[95] B. Yang, W.-t. Yih, X. He, J. Gao, and L. Deng,\n\u201cEmbedding\nEntities\nand\nRelations\nfor\nLearning\nand Inference in Knowledge Bases,\u201d CoRR, vol.\nabs/1412.6575, 2014.\n[96] P. D. Hoff, A. E. Raftery, and M. S. Handcock, \u201cLatent\nspace approaches to social network analysis,\u201d Journal\nof the American Statistical Association, vol. 97, no. 460,\npp. 1090\u20131098, 2002.\n[97] L. L\u00fc and T. Zhou, \u201cLink prediction in complex\nnetworks: A survey,\u201d Physica A: Statistical Mechanics\nand its Applications, vol. 390, no. 6, pp. 1150\u20131170,\nMar. 2011.\n[98] L. A. Adamic and E. Adar, \u201cFriends and neighbors on\nthe Web,\u201d Social Networks, vol. 25, no. 3, pp. 211\u2013230,\n2003.\n[99] A.-L. Barab\u00e1si and R. Albert, \u201cEmergence of Scaling\nin Random Networks,\u201d Science, vol. 286, no. 5439, pp.\n509\u2013512, 1999.\n[100] L. Katz, \u201cA new status index derived from sociometric\nanalysis,\u201d Psychometrika, vol. 18, no. 1, pp. 39\u201343,\n1953.\n[101] E. A. Leicht, P. Holme, and M. E. Newman, \u201cVertex\nsimilarity in networks,\u201d Physical Review E, vol. 73,\nno. 2, p. 026120, 2006.\n[102] S. Brin and L. Page, \u201cThe anatomy of a large-scale\nhypertextual Web search engine,\u201d Computer networks\nand ISDN systems, vol. 30, no. 1, pp. 107\u2013117, 1998.\n[103] W. Liu and L. L\u00fc, \u201cLink prediction based on local\nrandom walk,\u201d EPL (Europhysics Letters), vol. 89, no. 5,\np. 58007, 2010.\n[104] S. Muggleton, \u201cInverse entailment and progol,\u201d New\nGeneration Computing, vol. 13, no. 3-4, pp. 245\u2013286,\n1995.\n[105] \u2014\u2014, \u201cInductive logic programming,\u201d New generation\ncomputing, vol. 8, no. 4, pp. 295\u2013318, 1991.\n[106] J. R. Quinlan, \u201cLearning logical de\ufb01nitions from rela-\ntions,\u201d Machine Learning, vol. 5, pp. 239\u2013266, 1990.\n[107] L. A. Gal\u00e1rraga, C. Te\ufb02ioudi, K. Hose, and F. Suchanek,\n\u201cAMIE: Association Rule Mining Under Incomplete\nEvidence\nin\nOntological\nKnowledge\nBases,\u201d\nin\nProceedings of the 22nd International Conference on\nWorld Wide Web, 2013, pp. 413\u2013422.\n[108] L. Gal\u00e1rraga, C. Te\ufb02ioudi, K. Hose, and F. Suchanek,\n\u201cFast rule mining in ontological knowledge bases with\nAMIE+,\u201d The VLDB Journal, pp. 1\u201324, 2015.\n[109] F. A. Lisi, \u201cInductive logic programming in databases:\nFrom datalog to dl+log,\u201d TPLP, vol. 10, no. 3, pp.\n331\u2013359, 2010.\n[110] C. d\u2019Amato, N. Fanizzi, and F. Esposito, \u201cReasoning\nby analogy in description logics through instance-based\nlearning,\u201d in Proceedings of Semantic Wep Applications\nand Perspectives.\nCEUR-WS.org, 2006.\n[111] J. Lehmann, \u201cDL-learner: Learning concepts in descrip-\ntion logics,\u201d Journal of Machine Learning Research,\nvol. 10, pp. 2639\u20132642, 2009.\n[112] A. Rettinger, U. L\u00f6sch, V. Tresp, C. d\u2019Amato, and\nN. Fanizzi, \u201cMining the semantic web - statistical\nlearning for next generation knowledge bases,\u201d Data\nMin. Knowl. Discov., vol. 24, no. 3, pp. 613\u2013662, 2012.\n[113] U.\nL\u00f6sch,\nS.\nBloehdorn,\nand\nA.\nRettinger,\n\u201cGraph\nkernels\nfor\nrdf\ndata,\u201d\nin\nProceedings\nof the 9th International Conference on The Semantic\nWeb: Research and Applications.\nBerlin, Heidelberg:\nSpringer-Verlag, 2012, pp. 134\u2013148.\n[114] P. Minervini, C. d\u02d82019Amato, N. Fanizzi, and V. Tresp,\n\u201cLearning to propagate knowledge in web ontologies,\u201d in\n10th International Workshop on Uncertainty Reasoning\nfor the Semantic Web (URSW 2014), 2014, pp. 13\u201324.\n[115] N. Lao and W. W. Cohen, \u201cRelational retrieval using\na combination of path-constrained random walks,\u201d\nMachine learning, vol. 81, no. 1, pp. 53\u201367, 2010.\n[116] N. Lao, T. Mitchell, and W. W. Cohen, \u201cRandom\nwalk inference and learning in a large scale knowledge\nbase,\u201d in Proceedings of the Conference on Empirical\nMethods in Natural Language Processing.\nAssociation\nfor Computational Linguistics, 2011, pp. 529\u2013539.\n[117] K. Toutanova and D. Chen, \u201cObserved versus latent\nfeatures for knowledge base and text inference,\u201d in 3rd\nWorkshop on Continuous Vector Space Models and Their\nCompositionality, 2015.\n[118] M. Nickel, X. Jiang, and V. Tresp, \u201cReducing the\nRank in Relational Factorization Models by Including\nObservable Patterns,\u201d in Advances in Neural Information\nProcessing Systems 27.\nCurran Associates, Inc., 2014,\npp. 1179\u20131187.\n[119] Y. Koren, \u201cFactorization Meets the Neighborhood:\nA Multifaceted Collaborative Filtering Model,\u201d in\nProceedings of the 14th ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining.\nNew York, NY, USA: ACM, 2008, pp. 426\u2013434.\n[120] S.\nRendle,\n\u201cFactorization\nmachines\nwith\nlibFM,\u201d\nACM Transactions on Intelligent Systems and Technology\n(TIST), vol. 3, no. 3, p. 57, 2012.\n[121] D. H. Wolpert, \u201cStacked generalization,\u201d Neural net-\nworks, vol. 5, no. 2, pp. 241\u2013259, 1992.\n[122] L.\nBottou,\n\u201cLarge-Scale\nMachine\nLearning\nwith\nStochastic\nGradient\nDescent,\u201d\nin\nProceedings of\nCOMPSTAT\u20192010.\nPhysica-Verlag HD, 2010, pp.\n177\u2013186.\n[123] K. P. Murphy, Machine learning: a probabilistic per-\nspective.\nCambridge, MA: MIT Press, 2012.\n[124] J. Davis and M. Goadrich, \u201cThe Relationship Between\nPrecision-Recall and ROC Curves,\u201d in Proceedings of\nthe 23rd international conference on Machine learning.\nACM, 2006, pp. 233\u2013240.\n[125] D. Koller and N. Friedman, Probabilistic Graphical\nModels: Principles and Techniques.\nMIT Press, 2009.\n[126] M. Richardson and P. Domingos, \u201cMarkov logic net-\nworks,\u201d Machine Learning, vol. 62, no. 1, pp. 107\u2013136,\n2006.\n[127] C. Zhang and C. R\u00e9, \u201cTowards high-throughput Gibbs\nsampling at scale: A study across storage managers,\u201d in\nProceedings of the 2013 ACM SIGMOD International\nConference on Management of Data.\nACM, 2013, pp.\n397\u2013408.\n[128] H. Poon and P. Domingos, \u201cSound and ef\ufb01cient inference\n22\nwith probabilistic and deterministic dependencies,\u201d in\nAAAI, 2006.\n[129] A. Globerson and T. S. Jaakkola, \u201cFixing Max-Product:\nConvergent message passing algorithms for MAP LP-\nRelaxations,\u201d in NIPS, 2007.\n[130] S. H. Bach, M. Broecheler, B. Huang, and L. Getoor,\n\u201cHinge-loss markov random \ufb01elds and probabilistic soft\nlogic,\u201d arXiv:1505.04406 [cs.LG], 2015.\n[131] A. Kimmig, S. H. Bach, M. Broecheler, B. Huang, and\nL. Getoor, \u201cA Short Introduction to Probabilistic Soft\nLogic,\u201d in NIPS Workshop on Probabilistic Program-\nming: Foundations and Applications, 2012.\n[132] J. Pujara, H. Miao, L. Getoor, and W. W. Cohen, \u201cUsing\nsemantics and statistics to turn data into knowledge,\u201d AI\nMagazine, 2015.\n[133] J. Neville and D. Jensen, \u201cRelational dependency net-\nworks,\u201d The Journal of Machine Learning Research,\nvol. 8, pp. 637\u2013652, May 2007.\n[134] D.\nKrompa\u00df,\nX.\nJiang,\nM.\nNickel,\nand\nV.\nTresp,\n\u201cProbabilistic\nLatent-Factor\nDatabase\nModels,\u201d in Proceedings of the 1st Workshop on Linked\nData for Knowledge Discovery co-located with European\nConference on Machine Learning and Principles and\nPractice of Knowledge Discovery in Databases (ECML\nPKDD 2014), 2014.\n[135] H. Li, S. Krause, F. Xu, A. Moro, H. Uszkoreit, and\nR. Navigli, \u201cImprovement of n-ary relation extraction\nby adding lexical semantics to distant-supervision rule\nlearning,\u201d in ICAART 2015 - Proceedings of the 7th\nInternational Conference on Agents and Arti\ufb01cial Intelli-\ngence. International Conference on Agents and Arti\ufb01cial\nIntelligence (ICAART-15), 7th, January 10-12, Lisbon,\nPortugal.\nSciTePress, 2015.\n[136] H. Ji, T. Cassidy, Q. Li, and S. Tamang, \u201cTackling Rep-\nresentation, Annotation and Classi\ufb01cation Challenges\nfor Temporal Knowledge Base Population,\u201d Knowledge\nand Information Systems, pp. 1\u201336, Aug. 2013.\n[137] D. L. McGuinness, F. Van Harmelen, and others,\n\u201cOWL\nweb\nontology\nlanguage\noverview,\u201d\nW3C\nrecommendation, vol. 10, no. 10, p. 2004, 2004.\n[138] A. Hogan, A. Harth, A. Passant, S. Decker, and\nA. Polleres, \u201cWeaving the pedantic web,\u201d in 3rd\nInternational Workshop on Linked Data on the Web\n(LDOW2010), in conjunction with 19th International\nWorld Wide Web Conference.\nRaleigh, North Carolina,\nUSA: CEUR Workshop Proceedings, 2010.\n[139] H. Halpin, P. Hayes, J. McCusker, D. Mcguinness, and\nH. Thompson, \u201cWhen owl: sameAs isn\u2019t the same:\nAn analysis of identity in linked data,\u201d The Semantic\nWeb\u2013ISWC 2010, pp. 305\u2013320, 2010.\n[140] D. Krompa\u00df, M. Nickel, and V. Tresp, \u201cQuerying\nFactorized Probabilistic Triple Databases,\u201d in The\nSemantic Web\u2013ISWC 2014.\nSpringer, 2014, pp. 114\u2013\n129.\n[141] D. Suciu, D. Olteanu, C. Re, and C. Koch, Probabilistic\nDatabases.\nMorgan & Claypool, 2011.\n[142] D. Z. Wang, E. Michelakis, M. Garofalakis, and J. M.\nHellerstein, \u201cBayesStore: managing large, uncertain\ndata repositories with probabilistic graphical models,\u201d\nProceedings of the VLDB Endowment, vol. 1, no. 1, pp.\n340\u2013351, 2008.\n[143] J. Bleiholder and F. Naumann, \u201cData fusion,\u201d ACM\nComput. Surv., vol. 41, no. 1, pp. 1:1\u20131:41, Jan. 2009.\n[144] X. Li, X. L. Dong, K. Lyons, W. Meng, and\nD. Srivastava, \u201cTruth \ufb01nding on the deep web: Is the\nproblem solved?\u201d Proceedings of the VLDB Endowment,\nvol. 6, no. 2, pp. 97\u2013108, Dec. 2012.\n[145] X. L. Dong, E. Gabrilovich, G. Heitz, W. Horn,\nK. Murphy, S. Sun, and W. Zhang, \u201cFrom data\nfusion to knowledge fusion,\u201d Proceedings of the VLDB\nEndowment, vol. 7, no. 10, pp. 881\u2013892, Jun. 2014.\n[146] X. L. Dong, E. Gabrilovich, K. Murphy, V. Dang,\nW. Horn, C. Lugaresi, S. Sun, and W. Zhang,\n\u201cKnowledge-based trust: Estimating the trustworthiness\nof web sources,\u201d Proceedings of the VLDB Endowment,\nvol. 8, no. 9, pp. 938\u2013949, May 2015.\nMaximilian Nickel is a postdoctoral fellow with\nthe Laboratory for Computational and Statistical\nLearning at MIT and the Istituto Italiano di Tecnolo-\ngia. Furthermore, he is with the Center for Brains,\nMinds, and Machines at MIT. In 2013, he received\nhis PhD with summa cum laude from the Ludwig\nMaximilian University in Munich. From 2010 to\n2013 he worked as a research assistant at Siemens\nCorporate Technology, Munich. His research centers\naround machine learning from relational knowledge\nrepresentations and graph-structured data as well as\nits applications in arti\ufb01cial intelligence and cognitive science.\nKevin Murphy is a research scientist at Google in\nMountain View, California, where he works on AI,\nmachine learning, computer vision, knowledge base\nconstruction and natural language processing. Before\njoining Google in 2011, he was an associate professor\nof computer science and statistics at the University\nof British Columbia in Vancouver, Canada. Before\nstarting at UBC in 2004, he was a postdoc at MIT.\nKevin got his BA from U. Cambridge, his MEng from\nU. Pennsylvania, and his PhD from UC Berkeley. He\nhas published over 80 papers in refereed conferences\nand journals, as well as an 1100-page textbook called \u201cMachine Learning: a\nProbabilistic Perspective\u201d (MIT Press, 2012), which was awarded the 2013\nDeGroot Prize for best book in the \ufb01eld of Statistical Science. Kevin is also\nthe (co) Editor-in-Chief of JMLR (the Journal of Machine Learning Research).\n23\nVolker Tresp received a Diploma degree from the\nUniversity of Goettingen, Germany, in 1984 and\nthe M.Sc. and Ph.D. degrees from Yale University,\nNew Haven, CT, in 1986 and 1989 respectively.\nSince 1989 he is the head of various research\nteams in machine learning at Siemens, Research and\nTechnology. He \ufb01led more than 70 patent applications\nand was inventor of the year of Siemens in 1996.\nHe has published more than 100 scienti\ufb01c articles\nand administered over 20 Ph.D. theses. The company\nPanoratio is a spin-off out of his team. His research\nfocus in recent years has been \u201eMachine Learning in Information Networks\u201c\nfor modelling Knowledge Graphs, medical decision processes and sensor\nnetworks. He is the coordinator of one of the \ufb01rst nationally funded Big Data\nprojects for the realization of \u201ePrecision Medicine\u201c. In 2011 he became a\nHonorarprofessor at the Ludwig Maximilian University of Munich where he\nteaches an annual course on Machine Learning.\nEvgeniy Gabrilovich is a senior staff research\nscientist at Google, where he works on knowledge\ndiscovery from the web. Prior to joining Google\nin 2012, he was a director of research and head\nof the natural language processing and information\nretrieval group at Yahoo! Research. Evgeniy is an\nACM Distinguished Scientist, and is a recipient of\nthe 2014 IJCAI-JAIR Best Paper Prize. He is also a\nrecipient of the 2010 Karen Sparck Jones Award for\nhis contributions to natural language processing and\ninformation retrieval. He earned his PhD in computer\nscience from the Technion - Israel Institute of Technology.\n",
        "sentence": " , 2014), leading to a number of studies on automatic knowledge base completion (KBC) (Nickel et al., 2015) or link prediction.",
        "context": "triples). This is important since existing knowledge graphs are\noften missing many facts, and some of the edges they contain\nare incorrect [44]. In the context of knowledge graphs, link\n4\nprediction is also referred to as knowledge graph completion.\nlearning and natural language processing techniques (see,\ne.g., [9] for a review).\nConstruction of curated knowledge bases typically leads to\nhighly accurate results, but this technique does not scale well\n3\nand D. Lin, \u201cKnowledge Base Completion via Search-\nBased Question Answering,\u201d in Proceedings of the 23rd\nInternational Conference on World Wide Web, 2014, pp.\n515\u2013526.\n[23] D. B. Lenat, \u201cCYC: A Large-scale Investment in"
    },
    {
        "title": "A Three-Way Model for Collective Learning on Multi-Relational Data",
        "author": [
            "Maximilian Nickel",
            "Volker Tresp",
            "Hans-Peter Kriegel."
        ],
        "venue": "Proceedings of the 28th International Conference on Machine Learning. pages 809\u2013816.",
        "citeRegEx": "Nickel et al\\.,? 2011",
        "shortCiteRegEx": "Nickel et al\\.",
        "year": 2011,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "A survey on transfer learning",
        "author": [
            "Sinno Jialin Pan",
            "Qiang Yang."
        ],
        "venue": "IEEE Transactions on knowledge and data engineering 22(10):1345\u20131359.",
        "citeRegEx": "Pan and Yang.,? 2010",
        "shortCiteRegEx": "Pan and Yang.",
        "year": 2010,
        "abstract": "",
        "full_text": "",
        "sentence": " Transfer learning (Pan and Yang, 2010) has been shown to be promising to transfer knowl-",
        "context": null
    },
    {
        "title": "Outrageously large neural networks",
        "author": [
            "Noam Shazeer",
            "Azalia Mirhoseini",
            "Krzysztof Maziarz",
            "Andy Davis",
            "Quoc Le",
            "Geoffrey Hinton",
            "Jeff Dean"
        ],
        "venue": null,
        "citeRegEx": "Shazeer et al\\.,? \\Q2017\\E",
        "shortCiteRegEx": "Shazeer et al\\.",
        "year": 2017,
        "abstract": "",
        "full_text": "",
        "sentence": " Several works on obtaining a sparse attention (Martins and Astudillo, 2016; Makhzani and Frey, 2014; Shazeer et al., 2017) share a similar idea of sorting the values before softmax and only keeping theK largest values.",
        "context": null
    },
    {
        "title": "Implicit reasonet: Modeling large-scale structured relationships with shared memory",
        "author": [
            "Yelong Shen",
            "Po-Sen Huang",
            "Ming-Wei Chang",
            "Jianfeng Gao."
        ],
        "venue": "arXiv preprint arXiv:1611.04642 .",
        "citeRegEx": "Shen et al\\.,? 2016",
        "shortCiteRegEx": "Shen et al\\.",
        "year": 2016,
        "abstract": "",
        "full_text": "",
        "sentence": " In parallel, recent work has proposed to model regularities beyond local facts by considering multirelation paths (Garc\u0131\u0301a-Dur\u00e1n et al., 2015; Lin et al., 2015a; Shen et al., 2016). Note that although IRN (Shen et al., 2016) does not explicitly exploit path information, it performs multi-step inference through the multiple usages of external memory. 7 IRN (Shen et al., 2016) External Memory 249 95.",
        "context": null
    },
    {
        "title": "Reasoning With Neural Tensor Networks for Knowledge Base Completion",
        "author": [
            "Richard Socher",
            "Danqi Chen",
            "Christopher D Manning",
            "Andrew Ng."
        ],
        "venue": "Advances in Neural Information Processing Systems 26, pages 926\u2013934.",
        "citeRegEx": "Socher et al\\.,? 2013",
        "shortCiteRegEx": "Socher et al\\.",
        "year": 2013,
        "abstract": "",
        "full_text": "",
        "sentence": " sizes (Socher et al., 2013; West et al., 2014), leading to a number of studies on automatic knowledge base completion (KBC) (Nickel et al. Most of the embedding models for knowledge base completion define an energy function fr(h, t) according to the fact\u2019s plausibility (Bordes et al., 2011, 2014, 2013; Socher et al., 2013; Wang et al., 2014; Yang et al., 2015; Guu et al., 2015; Nguyen et al., 2016b). 7 NTN (Socher et al., 2013) No 66.",
        "context": null
    },
    {
        "title": "YAGO: A Core of Semantic Knowledge",
        "author": [
            "Fabian M. Suchanek",
            "Gjergji Kasneci",
            "Gerhard Weikum."
        ],
        "venue": "Proceedings of the 16th International Conference on World Wide Web. pages 697\u2013706.",
        "citeRegEx": "Suchanek et al\\.,? 2007",
        "shortCiteRegEx": "Suchanek et al\\.",
        "year": 2007,
        "abstract": "",
        "full_text": "",
        "sentence": " , 2008), YAGO (Suchanek et al., 2007) and DBpedia (Lehmann et al.",
        "context": null
    },
    {
        "title": "Regression shrinkage and selection via the lasso",
        "author": [
            "Robert Tibshirani."
        ],
        "venue": "Journal of the Royal Statistical Society. Series B (Methodological) pages 267\u2013288.",
        "citeRegEx": "Tibshirani.,? 1996",
        "shortCiteRegEx": "Tibshirani.",
        "year": 1996,
        "abstract": "SUMMARY\n               We propose a new method for estimation in linear models. The \u2018lasso\u2019 minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.",
        "full_text": "",
        "sentence": " However, directly posing `1 regularization (Tibshirani, 1996) on the attention vectors fails to produce sparse representations in our preliminary experiment, which motivates us to en- rect triples as used in Wang et al. (2014), Lin et al.",
        "context": null
    },
    {
        "title": "Observed Versus Latent Features for Knowledge Base and Text Inference",
        "author": [
            "Kristina Toutanova",
            "Danqi Chen."
        ],
        "venue": "Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality. pages 57\u201366.",
        "citeRegEx": "Toutanova and Chen.,? 2015",
        "shortCiteRegEx": "Toutanova and Chen.",
        "year": 2015,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Previously, a line of research makes use of external information such as textual relations from web-scale corpus or node features (Toutanova et al., 2015; Toutanova and Chen, 2015; Nguyen et al., 2016a), alleviating the sparsity problem. 6 NLFeat (Toutanova and Chen, 2015) Node + Link Features 94.",
        "context": null
    },
    {
        "title": "Representing Text for Joint Embedding of Text and Knowledge Bases",
        "author": [
            "Kristina Toutanova",
            "Danqi Chen",
            "Patrick Pantel",
            "Hoifung Poon",
            "Pallavi Choudhury",
            "Michael Gamon."
        ],
        "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural",
        "citeRegEx": "Toutanova et al\\.,? 2015",
        "shortCiteRegEx": "Toutanova et al\\.",
        "year": 2015,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Previously, a line of research makes use of external information such as textual relations from web-scale corpus or node features (Toutanova et al., 2015; Toutanova and Chen, 2015; Nguyen et al., 2016a), alleviating the sparsity problem.",
        "context": null
    },
    {
        "title": "Domain and function: A dualspace model of semantic relations and compositions",
        "author": [
            "Peter D Turney."
        ],
        "venue": "Journal of Artificial Intelligence Research 44:533\u2013 585.",
        "citeRegEx": "Turney.,? 2012",
        "shortCiteRegEx": "Turney.",
        "year": 2012,
        "abstract": "",
        "full_text": "",
        "sentence": " Learning the association between semantic relations has been used in related problems such as relational similarity measurement (Turney, 2012) and relation adaptation (Bollegala et al.",
        "context": null
    },
    {
        "title": "Knowledge Graph Embedding by Translating on Hyperplanes",
        "author": [
            "Zhen Wang",
            "Jianwen Zhang",
            "Jianlin Feng",
            "Zheng Chen."
        ],
        "venue": "Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, pages 1112\u20131119.",
        "citeRegEx": "Wang et al\\.,? 2014",
        "shortCiteRegEx": "Wang et al\\.",
        "year": 2014,
        "abstract": "\n      \n        We deal with embedding a large scale knowledge graph composed of entities and relations into a continuous vector space. TransE is a promising method proposed recently, which is very efficient while achieving state-of-the-art predictive performance. We discuss some mapping properties of relations which should be considered in embedding, such as reflexive, one-to-many, many-to-one, and many-to-many. We note that TransE does not do well in dealing with these properties. Some complex models are capable of preserving these mapping properties but sacrifice efficiency in the process. To make a good trade-off between model capacity and efficiency, in this paper we propose TransH which models a relation as a hyperplane together with a translation operation on it. In this way, we can well preserve the above mapping properties of relations with almost the same model complexity of TransE. Additionally, as a practical knowledge graph is often far from completed, how to construct negative examples to reduce false negative labels in training is very important. Utilizing the one-to-many/many-to-one mapping property of a relation, we propose a simple trick to reduce the possibility of false negative labeling. We conduct extensive experiments on link prediction, triplet classification and fact extraction on benchmark datasets like WordNet and Freebase. Experiments show TransH delivers significant improvements over TransE on predictive accuracy with comparable capability to scale up.\n      \n    ",
        "full_text": "",
        "sentence": " Most of the embedding models for knowledge base completion define an energy function fr(h, t) according to the fact\u2019s plausibility (Bordes et al., 2011, 2014, 2013; Socher et al., 2013; Wang et al., 2014; Yang et al., 2015; Guu et al., 2015; Nguyen et al., 2016b). 1 TransH (Wang et al., 2014) No 303 86.",
        "context": null
    },
    {
        "title": "Mining inference formulas by goal-directed random walks",
        "author": [
            "Zhuoyu Wei",
            "Jun Zhao",
            "Kang Liu."
        ],
        "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Austin, Texas,",
        "citeRegEx": "Wei et al\\.,? 2016",
        "shortCiteRegEx": "Wei et al\\.",
        "year": 2016,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " 0 Random Walk (Wei et al., 2016) Path 94.",
        "context": null
    },
    {
        "title": "Knowledge Base Completion via Searchbased Question Answering",
        "author": [
            "Robert West",
            "Evgeniy Gabrilovich",
            "Kevin Murphy",
            "Shaohua Sun",
            "Rahul Gupta",
            "Dekang Lin."
        ],
        "venue": "Proceedings of the 23rd International Conference on World Wide Web.",
        "citeRegEx": "West et al\\.,? 2014",
        "shortCiteRegEx": "West et al\\.",
        "year": 2014,
        "abstract": "",
        "full_text": "",
        "sentence": " sizes (Socher et al., 2013; West et al., 2014), leading to a number of studies on automatic knowledge base completion (KBC) (Nickel et al.",
        "context": null
    },
    {
        "title": "Embedding Entities and Relations for Learning and Inference in Knowledge Bases",
        "author": [
            "Bishan Yang",
            "Wen-tau Yih",
            "Xiaodong He",
            "Jianfeng Gao",
            "Li Deng."
        ],
        "venue": "Proceedings of the International Conference on Learning Representations.",
        "citeRegEx": "Yang et al\\.,? 2015",
        "shortCiteRegEx": "Yang et al\\.",
        "year": 2015,
        "abstract": "We consider learning representations of entities and relations in KBs using\nthe neural-embedding approach. We show that most existing models, including NTN\n(Socher et al., 2013) and TransE (Bordes et al., 2013b), can be generalized\nunder a unified learning framework, where entities are low-dimensional vectors\nlearned from a neural network and relations are bilinear and/or linear mapping\nfunctions. Under this framework, we compare a variety of embedding models on\nthe link prediction task. We show that a simple bilinear formulation achieves\nnew state-of-the-art results for the task (achieving a top-10 accuracy of 73.2%\nvs. 54.7% by TransE on Freebase). Furthermore, we introduce a novel approach\nthat utilizes the learned relation embeddings to mine logical rules such as\n\"BornInCity(a,b) and CityInCountry(b,c) => Nationality(a,c)\". We find that\nembeddings learned from the bilinear objective are particularly good at\ncapturing relational semantics and that the composition of relations is\ncharacterized by matrix multiplication. More interestingly, we demonstrate that\nour embedding-based rule extraction approach successfully outperforms a\nstate-of-the-art confidence-based rule mining approach in mining Horn rules\nthat involve compositional reasoning.",
        "full_text": "Published as conference paper at ICLR 2015\nEMBEDDING ENTITIES AND RELATIONS FOR LEARN-\nING AND INFERENCE IN KNOWLEDGE BASES\nBishan Yang1\u02da, Wen-tau Yih2, Xiaodong He2, Jianfeng Gao2 & Li Deng2\n1Department of Computer Science, Cornell University, Ithaca, NY, 14850, USA\nbishan@cs.cornell.edu\n2Microsoft Research, Redmond, WA 98052, USA\n{scottyih,xiaohe,jfgao,deng}@microsoft.com\nABSTRACT\nWe consider learning representations of entities and relations in KBs using the\nneural-embedding approach.\nWe show that most existing models, including\nNTN (Socher et al., 2013) and TransE (Bordes et al., 2013b), can be generalized\nunder a uni\ufb01ed learning framework, where entities are low-dimensional vectors\nlearned from a neural network and relations are bilinear and/or linear mapping\nfunctions. Under this framework, we compare a variety of embedding models\non the link prediction task. We show that a simple bilinear formulation achieves\nnew state-of-the-art results for the task (achieving a top-10 accuracy of 73.2%\nvs.\n54.7% by TransE on Freebase).\nFurthermore, we introduce a novel ap-\nproach that utilizes the learned relation embeddings to mine logical rules such\nas BornInCitypa, bq ^ CityInCountrypb, cq\n\u00f9\u00f1\nNationalitypa, cq. We\n\ufb01nd that embeddings learned from the bilinear objective are particularly good\nat capturing relational semantics, and that the composition of relations is char-\nacterized by matrix multiplication. More interestingly, we demonstrate that our\nembedding-based rule extraction approach successfully outperforms a state-of-\nthe-art con\ufb01dence-based rule mining approach in mining Horn rules that involve\ncompositional reasoning.\n1\nINTRODUCTION\nRecent years have witnessed a rapid growth of knowledge bases (KBs) such as Freebase1, DBPe-\ndia (Auer et al., 2007), and YAGO (Suchanek et al., 2007). These KBs store facts about real-world\nentities (e.g. people, places, and things) in the form of RDF triples2 (i.e. (subject, predicate, ob-\nject)). Today\u2019s KBs are large in size. For instance, Freebase contains millions of entities and billions\nof facts (triples) involving a large variety of predicates (relation types). Such large-scale multi-\nrelational data provide an excellent potential for improving a wide range of tasks, from information\nretrieval, question answering to biological data mining.\nRecently, much effort has been invested in relational learning methods that can scale to large knowl-\nedge bases. Tensor factorization (e.g. (Nickel et al., 2011; 2012)) and neural-embedding-based\nmodels (e.g. (Bordes et al., 2013a;b; Socher et al., 2013)) are two popular kinds of approaches that\nlearn to encode relational information using low-dimensional representations of entities and rela-\ntions. These representation learning methods have shown good scalability and reasoning ability in\nterms of validating unseen facts given the existing KB.\nIn this work, we focus on the study of neural-embedding models, where the representations\nare learned using neural networks with energy-based objectives.\nRecent embedding models\nTransE (Bordes et al., 2013b) and NTN (Socher et al., 2013) have shown state-of-the-art predic-\ntion performance compared to tensor factorization methods such as RESCAL (Nickel et al., 2012).\nThey are similar in model forms with slight differences on the choices of entity and relation rep-\nresentations. Without careful comparison, it is not clear how different design choices affect the\n\u02daWork conducted while interning at Microsoft Research.\n1http://freebase.com\n2http://www.w3.org/TR/rdf11-concepts/\n1\narXiv:1412.6575v4  [cs.CL]  29 Aug 2015\nPublished as conference paper at ICLR 2015\nlearning results. In addition, the performance of the embedding models are evaluated on the link\nprediction task (i.e. predicting the correctness of unseen triples). This only indirectly shows the\nmeaningfulness of low-dimensional embeddings. It is hard to explain what relational properties are\nbeing captured and to what extent they are captured during the embedding process.\nWe make three main contributions in this paper. (1) We present a general framework for multi-\nrelational learning that uni\ufb01es most multi-relational embedding models developed in the past, in-\ncluding NTN (Socher et al., 2013) and TransE (Bordes et al., 2013b). (2) We empirically evaluate\ndifferent choices of entity representations and relation representations under this framework on the\ncanonical link prediction task and show that a simple bilinear formulation achieves new state-of-\nthe-art results for the task (a top-10 accuracy of 73.2% vs. 54.7% by TransE when evaluated on\nFreebase). (3) We propose and evaluate a novel approach that utilizes the learned embeddings to\nmine logical rules such as BornInCitypa, bq ^ CityOfCountrypb, cq \u00f9\u00f1 Nationalitypa, cq.\nWe show that such rules can be effectively extracted by modeling the composition of relation\nembeddings, and that the embeddings learned from the bilinear objective are particularly good\nat capturing the compositional semantics of relations via matrix multiplication. Furthermore, we\ndemonstrate that our embedding-based approach outperforms a state-of-the-art rule mining system\nAMIE (Gal\u00b4arraga et al., 2013) on mining rules that involve compositional reasoning.\nThe rest of this paper is structured as follows. Section 2 discusses related work. Section 3 presents\nthe general framework for learning multi-relational representations. Sections 4 and 5 present two\ninference tasks: a canonical link prediction task and a novel rule extraction task where the learned\nembeddings are empirically evaluated. Section 6 concludes the paper.\n2\nRELATED WORK\nMulti-relational learning has been an active research area for the past couple of years. Traditional\nstatistical learning approaches (Getoor & Taskar, 2007) such as Markov-logic networks (Richard-\nson & Domingos, 2006) usually suffer from scalability issues. More recently, various types of\nrepresentation learning methods have been proposed to embed multi-relational knowledge into low-\ndimensional representations of entities and relations, including tensor/matrix factorization (Singh\n& Gordon, 2008; Nickel et al., 2011; 2012), Bayesian clustering framework (Kemp et al., 2006;\nSutskever et al., 2009), and neural networks (Paccanaro & Hinton, 2001; Bordes et al., 2013a;b;\nSocher et al., 2013). Our work focuses on the study of neural-embedding models as they have\nshown good scalability and strong generalizability on large-scale KBs.\nExisting neural embedding models (Bordes et al., 2013a;b; Socher et al., 2013) all represent entities\nas low-dimensional vectors and represent relations as operators that combine the representations of\ntwo entities. They differ in different parametrization of relation operators. For instance, given two\nentity vectors, the model of Neural Tensor Network (NTN) (Socher et al., 2013) represents each rela-\ntion as a bilinear tensor operator followed by a linear matrix operator. The model of TransE (Bordes\net al., 2013b), on the other hand, represents each relation as a single vector that linearly interacts\nwith the entity vectors. Likewise, variations on entity representations also exist. Most methods rep-\nresent each entity as a unit vector while NTN (Socher et al., 2013) represent entities as an average\nof word vectors and initializing word vectors with pre-trained vectors from external text corpora.\nThere has not been work that closely examines the effectiveness of these different design choices.\nOur work on embedding-based rule extraction presented in part of this paper is related to the ear-\nlier study on logical inference with learned continuous-space representations. Much existing work\nalong this line focuses on learning logic-based representations for natural language sentences. For\nexample, Socher et al. (2012) builds a neural network that recursively combines word representa-\ntions based on parse tree structures and shows that such neural network can simulate the behavior of\nconjunction and negation. Bowman (2014) further demonstrates that recursive neural network can\ncapture certain aspects of natural logical reasoning on examples involving quanti\ufb01ers like some and\nall. Recently, Grefenstette (2013) shows that in theory most aspects of predicate logic can be sim-\nulated using tensor calculus. Rockt\u00a8aschel et al. (2014) further implements the idea by introducing\na supervised objective that trains embeddings to be consistent with given logical rules. The evalua-\ntion was conducted on toy data and uses limited logical forms. Different from these earlier studies,\nwe propose a novel approach to utilizing embeddings learned without explicit logical constraints\nto directly mine logical rules from KBs. We demonstrate that the learned embeddings of relations\n2\nPublished as conference paper at ICLR 2015\ncan capture the compositional semantics of relations. Moreover, we systematically evaluate our ap-\nproach and compare it favorably with a state-of-the-art rule mining approach on the rule extraction\ntask on Freebase.\n3\nMULTI-RELATIONAL REPRESENTATION LEARNING\nIn this section, we present a general neural network framework for multi-relational representation\nlearning. We discuss different design choices for the representations of entities and relations which\nwill be empirically compared in Section 4.\nGiven a KB that is represented as a list of relation triplets pe1, r, e2q (denoting e1 (the subject) and\ne2 (the object) that are in a certain relationship r), we want to learn representations for entities\nand relations such that valid triplets receive high scores (or low energies). The embeddings can be\nlearned via a neural network. The \ufb01rst layer projects a pair of input entities to low dimensional\nvectors, and the second layer combines these two vectors to a scalar for comparison via a scoring\nfunction with relation-speci\ufb01c parameters.\n3.1\nENTITY REPRESENTATIONS\nEach input entity corresponds to a high-dimensional vector, either a \u201cone-hot\u201d index vector or a\n\u201cn-hot\u201d feature vector. Denote by xe1 and xe2 the input vectors for entity e1 and e2, respectively.\nDenote by W the \ufb01rst layer projection matrix. The learned entity representations, ye1 and ye2 can\nbe written as\nye1 \u201c f\n`\nWxe1\n\u02d8\n, ye2 \u201c f\n`\nWxe2\n\u02d8\nwhere f can be a linear or non-linear function, and W is a parameter matrix, which can be randomly\ninitialized or initialized using pre-trained vectors.\nMost existing embedding models adopt the \u201cone-hot\u201d input vectors except for NTN (Socher et al.,\n2013) which represents each entity as an average of its word vectors. This can be viewed as adopting\n\u201cbag-of-words\u201d vectors as input and learning a projection matrix consisting of word vectors.\n3.2\nRELATION REPRESENTATIONS\nThe choice of relation representations re\ufb02ects in the form of the scoring function. Most of the\nexisting scoring functions in the literature can be uni\ufb01ed based on a basic linear transformation ga\nr,\na bilinear transformation gb\nr or their combination, where ga\nr and gb\nr are de\ufb01ned as\nga\nrpye1, ye2q \u201c AT\nr\n\u02c6\nye1\nye2\n\u02d9\nand gb\nrpye1, ye2q \u201c yT\ne1Brye2,\n(1)\nwhich Ar and Br are relation-speci\ufb01c parameters.\nModels\nBr\nAT\nr\nScoring Function\nDistance (Bordes et al., 2011)\n-\n`\nQT\nr1\n\u00b4QT\nr2\n\u02d8\n\u00b4||ga\nr pye1, ye2q||1\nSingle Layer (Socher et al., 2013)\n-\n`\nQT\nr1\nQT\nr2\n\u02d8\nuT\nr tanhpga\nr pye1, ye2qq\nTransE (Bordes et al., 2013b)\nI\n`\nVT\nr\n\u00b4VT\nr\n\u02d8\n\u00b4p2ga\nr pye1, ye2q \u00b4 2gb\nrpye1, ye2q ` ||Vr||2\n2q\nNTN (Socher et al., 2013)\nTr\n`\nQT\nr1\nQT\nr2\n\u02d8\nuT\nr tanh\n`\nga\nr pye1, ye2q ` gb\nrpye1, ye2q\n\u02d8\nTable 1: Comparisons among several multi-relational models in their scoring functions.\nIn Table 1, we summarize several popular scoring functions in the literature for a relation\ntriplet pe1, r, e2q, reformulated in terms of the above two functions. Denote by ye1, ye2 P Rn\ntwo entity vectors. Denote by Qr1, Qr2 P Rn\u02c6m and Vr P Rn matrix or vector parameters\nfor linear transformation ga\nr . Denote by Tr P Rn\u02c6n\u02c6m tensor parameters for bilinear trans-\nformation gb\nr.\nI P Rn is an identity matrix.\nur P Rm is an additional parameter for rela-\ntion r. The scoring function for TransE (L2 formulation) is derived from ||ye1 \u00b4 ye2 ` Vr||2\n2 \u201c\n2V T\nr pye1 \u00b4 ye2q \u00b4 2yT\ne1ye2 ` ||Vr||2\n2 ` ||ye1||2\n2 ` ||ye2||2\n2, where ye1 and ye2 are unit vectors.\nNote that NTN is the most expressive model as it contains both linear and bilinear relation operators\nas special cases. In terms of the number of parameters, TransE is the simplest model which only\nparametrizes the linear relation operators with one-dimensional vectors.\n3\nPublished as conference paper at ICLR 2015\nIn this paper, we also consider the basic bilinear scoring function:\ngb\nrpye1, ye2q \u201c yT\ne1Mrye2\n(2)\nwhich is a special case of NTN without the non-linear layer and the linear operator, and uses a 2-d\nmatrix operator Mr P Rn\u02c6n instead of a tensor operator. Such bilinear formulation has been used in\nother matrix factorization models such as in (Nickel et al., 2011; Jenatton et al., 2012; Garc\u00b4\u0131a-Dur\u00b4an\net al., 2014) with different forms of regularization. Here, we consider a simple way to reduce the\nnumber of relation parameters by restricting Mr to be a diagonal matrix. This results in the same\nnumber of relation parameters as TransE. Our experiments in Section 4 demonstrate that this simple\nformulation enjoys the same scalable property as TransE and it achieves superior performance over\nTransE and other more expressive models on the task of link prediction.\nThis general framework for relationship modeling also applies to the recent deep-structured semantic\nmodel (Huang et al., 2013; Shen et al., 2014a;b; Gao et al., 2014; Yih et al., 2014), which learns the\nrelevance or a single relation between a pair of word sequences. The framework above applies when\nusing multiple neural network layers to project entities and using a relation-independent scoring\nfunction Gr\n`\nye1, ye2\n\u02d8\n\u201c cosrye1pWrq, ye2pWrqs. The cosine scoring function is a special case of\ngb\nr with normalized ye1, ye2 and with Br \u201c I.\n3.3\nPARAMETER LEARNING\nThe neural network parameters of all the models discussed above can be learned by minimizing\na margin-based ranking objective , which encourages the scores of positive relationships (triplets)\nto be higher than the scores of any negative relationships (triplets). Usually only positive triplets\nare observed in the data. Given a set of positive triplets T, we can construct a set of \u201cnegative\u201d\ntriplets T 1 by corrupting either one of the relation arguments, T 1 \u201c tpe1\n1, r, e2q|e1\n1 P E, pe1\n1, r, e2q R\nTu Y tpe1, r, e1\n2q|e1\n2 P E, pe1, r, e1\n2q R Tu. Denote the scoring function for triplet pe1, r, e2q as\nSpe1,r,e2q. The training objective is to minimize the margin-based ranking loss\nLp\u2126q \u201c\n\u00ff\npe1,r,e2qPT\n\u00ff\npe1\n1,r,e1\n2qPT 1\nmaxtSpe1\n1,r,e1\n2q \u00b4 Spe1,r,e2q ` 1, 0u\n(3)\n4\nINFERENCE TASK I: LINK PREDICTION\nWe \ufb01rst conduct a comparison study of different embedding models on the canonical link predic-\ntion task, which is to predict the correctness of unseen triplets. As in (Bordes et al., 2013b), we\nformulate link prediction as an entity ranking task. For each triplet in the test data, we treat each\nentity as the target entity to be predicted in turn. Scores are computed for the correct entity and all\nthe corrupted entities in the dictionary and are ranked in descending order. We consider Mean Re-\nciprocal Rank (MRR) (an average of the reciprocal rank of an answered entity over all test triplets),\nHITS@10 (top-10 accuracy), and Mean Average Precision (MAP) (as used in (Chang et al., 2014))\nas the evaluation metrics.\nWe examine \ufb01ve embedding models in decreasing order of complexity: (1) NTN with 4 tensor slices\nas in (Socher et al., 2013); (2) Bilinear+Linear, NTN with 1 tensor slice and without the non-linear\nlayer; (3) TransE, a special case of Bilinear+Linear (see Table 1); (4) Bilinear: using scoring function\nin Eq. (2); (5) Bilinear-diag: a special case of Bilinear where the relation matrix is a diagonal matrix.\nDatasets\nWe used the WordNet (WN) and Freebase (FB15k) datasets introduced in (Bordes et al.,\n2013b). WN contains 151, 442 triplets with 40, 943 entities and 18 relations, and FB15k consists of\n592, 213 triplets with 14, 951 entities and 1345 relations. We use the same training/validation/test\nsplit as in (Bordes et al., 2013b). We also consider a subset of FB15k (FB15k-401) containing only\nfrequent relations (relations with at least 100 training examples). This results in 560, 209 triplets\nwith 14, 541 entities and 401 relations.\nImplementation details\nAll the models were implemented in C# and using GPU. Training was\nimplemented using mini-batch stochastic gradient descent with AdaGrad (Duchi et al., 2011). At\neach gradient step, we sampled for each positive triplet two negative triplets, one with a corrupted\n4\nPublished as conference paper at ICLR 2015\nsubject entity and one with a corrupted object entity. The entity vectors are renormalized to have unit\nlength after each gradient step (it is an effective technique that empirically improved all the models).\nFor the relation parameters, we used standard L2 regularization. For all models, we set the number\nof mini-batches to 10, the dimensionality of the entity vector d \u201c 100, the regularization parameter\n0.0001, and the number of training epochs T \u201c 100 on FB15k and FB15k-401 and T \u201c 300 on WN\n(T was determined based on the learning curves where the performance of all models plateaued.)\nThe learning rate was initially set to 0.1 and then adapted during training by AdaGrad.\n4.1\nRESULTS\nFB15k\nFB15k-401\nWN\nMRR\nHITS@10\nMRR\nHITS@10\nMRR\nHITS@10\nNTN\n0.25\n41.4\n0.24\n40.5\n0.53\n66.1\nBlinear+Linear\n0.30\n49.0\n0.30\n49.4\n0.87\n91.6\nTransE (DISTADD)\n0.32\n53.9\n0.32\n54.7\n0.38\n90.9\nBilinear\n0.31\n51.9\n0.32\n52.2\n0.89\n92.8\nBilinear-diag (DISTMULT)\n0.35\n57.7\n0.36\n58.5\n0.83\n94.2\nTable 2: Performance comparisons among different embedding models\nTable 2 shows the results of all compared methods on all the datasets. In general, we observe\nthat the performance increases as the complexity of the model decreases on FB. NTN, the most\ncomplex model, provides the worst performance on both FB and WN, which suggests over\ufb01tting.\nCompared to the previously published results of TransE (Bordes et al., 2013b), our implementation\nachieves much better results (53.9% vs. 47.1% on FB15k and 90.9% vs. 89.2% on WN) using the\nsame evaluation metric (HITS@10). We attribute such discrepancy mainly to the different choice\nof SGD optimization: AdaGrad vs. constant learning rate. We also found that Bilinear consistently\nprovides comparable or better performance than TransE, especially on WN. Note that WN contains\nmuch more entities than FB, it may require the parametrization of relations to be more expressive\nto better handle the richness of entities. Interestingly, we found that a simple variant of Bilinear \u2013\nBILINEAR-DIAG, clearly outperforms all baselines on FB and achieves comparable performance to\nBilinear on WN. Note that BILINEAR-DIAG has the limitation of encoding the difference between a\nrelation and its inverse. Still, as there is a large variety of relations in FB and the average number\nof training examples seen by each relation is relatively small (compared to WN), the simple form of\nBILINEAR-DIAG is able to provide good prediction performance.\nMultiplicative vs. Additive Interactions Note that BILINEAR-DIAG and TRANSE have the same\nnumber of model parameters and their difference can be viewed as the operational choices of the\ncomposition of two entity vectors \u2013 BILINEAR-DIAG uses weighted element-wise dot product (mul-\ntiplicative operation) and TRANSE uses element-wise subtraction with a bias (additive operation).\nTo highlight the difference, here we use DISTMULT and DISTADD to refer to BILINEAR-DIAG and\nTRANSE, respectively. Comparisons between these two models can provide us more insights on the\neffect of two common choices of compositional operations \u2013 multiplication and addition for model-\ning entity relations. Overall, we observed superior performance of DISTMULT on all the datasets in\nTable 2. Table 3 shows the HITS@10 score on four types of relation categories (as de\ufb01ned in (Bordes\net al., 2013b)) on FB15k-401 when predicting the subject entity and the object entity respectively.\nWe can see that DISTMULT signi\ufb01cantly outperforms DISTADD in almost all the categories.\nPredicting subject entities\nPredicting object entities\n1-to-1\n1-to-n\nn-to-1\nn-to-n\n1-to-1\n1-to-n\nn-to-1\nn-to-n\nDISTADD\n70.0\n76.7\n21.1\n53.9\n68.7\n17.4\n83.2\n57.5\nDISTMULT\n75.5\n85.1\n42.9\n55.2\n73.7\n46.7\n81.0\n58.8\nTable 3: Results by relation categories: one-to-one, one-to-many, many-to-one and many-to-many\nInitialization of Entity Vectors In the following, we examine the learning of entity representations\nand introduce two further improvements: using non-linear projection and initializing entity vectors\nwith pre-trained vectors. We focus on DISTMULT as our baseline and compare it with the two\nmodi\ufb01cations DISTMULT-tanh (using f \u201c tanh for entity projection ) and DISTMULT-tanh-EV-init\n5\nPublished as conference paper at ICLR 2015\n(initializing the entity parameters with the 1000-dimensional pre-trained entity vectors released by\nword2vec (Mikolov et al., 2013)) on FB15k-401. We also reimplemented the initialization technique\nintroduced in (Socher et al., 2013) \u2013 each entity is represented as an average of its word vectors and\nthe word vectors are initialized using the 300-dimensional pre-trained word vectors released by\nword2vec. We denote this method as DISTMULT-tanh-WV-init. Inspired by (Chang et al., 2014),\nwe design a new evaluation setting where the predicted entities are automatically \ufb01ltered according\nto \u201centity types\u201d (entities that appear as the subjects/objects of a relation have the same type de\ufb01ned\nby that relation). This provides us with better understanding of the model performance when some\nentity type information is provided.\nMRR\nHITS@10\nMAP (w/ type checking)\nDISTMULT\n0.36\n58.5\n64.5\nDISTMULT-tanh\n0.39\n63.3\n76.0\nDISTMULT-tanh-WV-init\n0.28\n52.5\n65.5\nDISTMULT-tanh-EV-init\n0.42\n73.2\n88.2\nTable 4: Evaluation with pre-trained vectors\nIn Table 4, we can see that DISTMULT-tanh-EV-init provides the best performance on all the metrics.\nSurprisingly, we observed performance drops by DISTMULT-tanh-WV-init. We suspect that this\nis because word vectors are not appropriate for modeling entities described by non-compositional\nphrases (more than 73% of the entities in FB15k-401 are person names, locations, organizations and\n\ufb01lms). The promising performance of DISTMULT-tanh-EV-init suggests that the embedding model\ncan greatly bene\ufb01t from pre-trained entity-level vectors using external textual resources.\n5\nINFERENCE TASK II: RULE EXTRACTION\nIn this section, we focus on a complementary inference task, where we utilize the learned embed-\ndings to extract logical rules from the KB. For example, given the fact that a person was born in New\nYork and New York is a city of the United States, then the person\u2019s nationality is the United States:\nBornInCitypa, bq ^ CityOfCountrypb, cq \u00f9\u00f1 Nationalitypa, cq\nSuch logical rules can serve four important purposes. First, they can help deduce new facts and\ncomplete the existing KBs. Second, they can help optimize data storage by storing only rules instead\nof large amounts of extensional data, and generate facts only at inference time. Third, they can\nsupport complex reasoning. Lastly, they can provide explanations for inference results, e.g. we may\ninfer that people\u2019s professions usually involve the specialization of the \ufb01eld they study, etc.\nThe key problem of extracting Horn rules like the aforementioned example is how to effectively\nexplore the search space. Traditional rule mining approaches directly operate on the KB graph \u2013\nthey search for possible rules (i.e. closed-paths in the graph) by pruning rules with low statistical\nsigni\ufb01cance and relevance (Schoenmackers et al., 2010). These approaches often fail on large KB\ngraphs due to scalability issues. In the following, we introduce a novel embedding-based rule mining\napproach whose ef\ufb01ciency is not affected by the size of the KB graph but rather by the number of\ndistinct types of relations in the KB (which is usually relatively small). It can also mine better rules\ndue to its strong generalizability.\n5.1\nBACKGROUND AND NOTATIONS\nFor a better illustration, we adopt the graph view of KB. Each binary relation rpa, bq is a directed\nedge from node a to node b and with link type r. We are interested in extracting Horn rules that\nconsist of a head relation H and a sequence of body relations B1, ..., Bn:\nB1pa1, a2q ^ B2pa2, a3q ^ ... ^ Bnpan, an`1q \u00f9\u00f1 Hpa1, an`1q\n(4)\nwhere ai are variables that can be substituted by entities. We constrain the body relations B1, ..., Bn\nto form a directed path in the graph and the head relation H to from a directed edge that close\nthe path (from the start of the path to the end of the path).\nWe denote such property as the\nclosed-path property. For consecutive relations that share one variable but do not form a path,\n6\nPublished as conference paper at ICLR 2015\ne,g, Bi\u00b41pa, bq ^ Bipa, cq, we can replace one of the relations with its inverse relation, so that the\nrelations are connected by an object and an subject, e.g. B\u00b41\ni\u00b41pb, aq ^ Bipa, cq. We are interested\nin mining rules that re\ufb02ect relationships among different relation types, therefore we also constrain\nB1, ..., Bn, H to have distinct relation types. A rule is instantiated when all variables are substi-\ntuted by entities. We denote the length of the rule as the number of body relations. In general longer\nrules are harder to extract due to the exponential search space. In our experiments, we focus on\nextracting rules of length 2 and 3.\nIn KBs, entities usually have types and relations often can only take arguments of certain types. For\nexample, BornInCity relation can only take a person as the subject and a location as the object. For\neach relation r, we denote the domain of its subject argument (the set of entities that can appear\nin the subject position) as Xr and similarly the domain of its object argument as Yr. Such domain\ninformation can be extremely useful in restricting the search space of logical rules.\n5.2\nEMBEDDING-BASED RULE EXTRACTION\nFor simplicity, we consider Horn rules of length 2 (longer rules can be easily derived from this case):\nB1pa, bq ^ B2pb, cq \u00f9\u00f1 Hpa, cq\n(5)\nNote that the body of the rule can be viewed as the composition of relations B1 and B2, which is a\nnew relation that has the property that entities a and c are in a relation if and only if there is an entity\nb which simultaneously satis\ufb01es two relations B1pa, bq and B2pb, cq.\nWe model relation composition as multiplication or addition of two relation embeddings. Here we\nfocus on relation embeddings that are in the form of vectors (as in TRANSE) and matrices (as in\nBILINEAR and its variants). The composition results in a new embedding that lies in the same\nrelation space. Speci\ufb01cally, we use addition for relation vector embeddings and multiplication for\nrelation matrix embeddings. This is inspired by two different properties: (1) if a relation corresponds\nto a translation vector V and assume ya`V\u00b4yb \u00ab 0 when Bpa, bq holds, then we have the property\nthat ya ` V1 \u00ab yb and yb ` V2 \u00ab yc implies ya ` pV1 ` V2q \u00ab yc; (2) if a relation corresponds\nto a matrix M in the bilinear transformation and assume yT\na Myb \u00ab 1 when Bpa, bq holds, also ya\nand yb are unit vectors and yT\na M is still a unit vector 3, then we have the property that yT\na M1 \u00ab yT\nb\nand yT\nb M2 \u00ab yT\nc implies yT\na pM1M2q \u00ab yT\nc .\nTo simulate the implication in 5, we want the composition result of relation B1 and B2 to demon-\nstrate similar behavior to the embedding of relation H. We assume the similarity between relation\nembeddings is related to the Euclidean distance if the embeddings are vectors and to the Frobe-\nnius norm if the embeddings are matrices. This distance metric allows us to rank possible pairs of\nrelations with respect to the relevance of their composition to the target relation.\nNote that we do not need to enumerate all possible pairs of relations in the KB. For example, if\nthe relation in the head is r, then we are only interested in relation pairs pp, qq that satisfy the type\nconstraints, namely: (1) Yp X Xq \u2030 H; (2) Xp X Xr \u2030 H; (3) Yq X Yr \u2030 H. As mentioned before,\nthe arguments (entities) of relations are usually strongly typed in KBs. Applying such domain\nconstraints can effectively reduce the search space.\nIn Algorithm 1, we describe our rule extraction algorithm for general closed-path Horn rules as in\nEq. (4). In Step 7, \u02dd denotes vector addition or matrix multiplication. We apply a global threshold\nvalue \u03b4 in our experiments to \ufb01lter candidate sequences for each relation r, and then automatically\nselect the top remaining sequences by applying a heuristic thresholding strategy based on the differ-\nence of the distance scores: sort the sequences by increasing distance d1, ..., dT and set the cut-off\npoint to be the j-th sequence where j \u201c arg maxipdi`1 \u00b4 diq.\n5.3\nEXPERIMENTS\nWe evaluate our rule extraction method (denoted as EMBEDRULE) on the FB15k-401 dataset. In\nour experiments, we remove the equivalence relations and relations whose domains have cardinality\n3These assumptions may not hold in our implementations. The intuition still leads to surprisingly good em-\npirical performance on Horn rule extraction. How to effectively enforce these constraints is worth investigating\nin future work.\n7\nPublished as conference paper at ICLR 2015\nAlgorithm 1 EMBEDRULE\n1: Input: KB \u201c tpe1, r, e2qu, relation set R\n2: Output: Candidate rules Q\n3: for each r in R do\n4:\nSelect the set of start relations S \u201c ts : Xs X Xr \u2030 Hu\n5:\nSelect the set of end relations T \u201c tt : Yt X Yr \u2030 Hu\n6:\nFind all possible relation sequences\n7:\nSelect the K-NN sequences P 1 \u010e P for r based on distpMr, Mp1 \u02dd \u00a8 \u00a8 \u00a8 \u02dd Mpnq\n8:\nForm candidate rules using P 1 where r is the head relation and p P P 1 is the body in a rule\n9:\nAdd the candidate rules into Q\n10: end for\n1 since rules involving these relations are not interesting. This results in training data that contains\n485,741 facts, 14,417 entities, and 373 relations. Our EMBEDRULE algorithm identi\ufb01es 60,020\npossible length-2 relation sequences and 2,156,391 possible length-3 relation sequences. We then\napply the thresholding method described in Section 5.2 to further select top \u201e3.9K length-2 rules\nand \u201e2K length-3 rules 4. By default all the extracted rules are ranked by decreasing con\ufb01dence,\nwhich is computed as the ratio of the correct predictions to the total number of predictions, where\npredictions are triplets that are derived from the instantiated rules where the body relations are\nobserved.\nWe implemented four versions of EMBEDRULE using embeddings trained from TRANSE (DIS-\nTADD), BILINEAR, BILINEAR-DIAG (DISTMULT) and DISTMULT-tanh-EV-init with correspond-\ning composition functions. We also compare our approaches to AMIE (Gal\u00b4arraga et al., 2013), a\nstate-of-the-art rule mining system that can ef\ufb01ciently search for Horn rules in large-scale KBs by\nusing novel measurements of support and con\ufb01dence. The system extracts close rules \u2013 a super-\nset of the rules we consider in this paper: every relation in the body is connected to the following\nrelation by sharing an entity variable, and every entity variable in the rule appears at least twice.\nWe run AMIE with the default setting on the same training set. It extracts 2,201 possible length-1\nrules and 46,975 possible length-2 rules, among which 3,952 rules have the close-path property. We\ncompare these length-2 rules with the similar number of length-2 rules extracted by EMBEDRULE.\nBy default AMIE ranks rules by PCA con\ufb01dence (a normalized con\ufb01dence that takes into account\nthe incompleteness of KBs). However we found that ranking by the standard con\ufb01dence gives better\nperformance than the PCA con\ufb01dence on the Freebase dataset we use.\nFor computational cost, EmbedRule mines length-2 rules in 2 minutes and mines length-3 rules in\n20 minutes (the computational time is similar when using different types of embeddings). AMIE\nmines rules of length \u010f 2 in 9 minutes. All methods are evaluated on a machine with a 64-bit\nprocessor, 2 CPUs and 8GB memory.\nWe consider precision as the evaluation metric, which is the ratio of predictions that are in the\ntest (unseen) data to all the generated unseen predictions. Note that this is an estimation, since\na prediction is not necessarily \u201cincorrect\u201d if it is not seen. Gal\u00b4arraga et al. (2013) suggested to\nidentify incorrect predictions based on the functional property of relations. However, we \ufb01nd that\nmost relations in our datasets are not functional. For a better estimation, we manually labeled the\ntop 30 unseen facts predicted by each method by checking Wikipedia. We also remove rules where\nthe head relations are hard to justi\ufb01ed due to dynamic factors (i.e. involving the word \u201ccurrent\u201d).\n5.4\nRESULTS\nFigure 1 compares the predictions generated by the length-2 rules extracted by different meth-\nods. We plot the aggregated precision of the top rules that produce up to 10K predictions in to-\ntal. From left to right, the n-th data point represents the total number of predictions of the top\nn rules and the estimated precision of these predictions. We can see that EMBEDRULE that uses\nembeddings trained from the bilinear objective (BILINEAR, DISTMULT and DISTMULT-TANH-\n4We consider K=100 nearest-neighbor sequences for each method, and set \u03b4 to 9.2, 36.3, 1.9 and 3.4 for\nDISTMULT-TANH-EV-INIT, DISTMULT, BILINEAR and DISTADD respectively for length-2 rules, and set it\nto 9.1, 48.8, 2.9, and 1.1 for lengh-3 rules.\n8\nPublished as conference paper at ICLR 2015\nEV-INIT) consistently outperforms AMIE. This suggests that the bilinear embeddings contain\ngood amount of information about relations which allows for effective rule selection without look-\ning at the entities. For example, AMIE fails to extract TV ProgramCountryofOriginpa, bq ^\nCountryOfficialLanguagepb, cq \u00f9\u00f1 TV ProgramLanguagepa, cq by relying on the instan-\ntiations of the rule occurred in the observed KB while all the bilinear variants of EMBEDRULE\nsuccessfully extract the rule purely based on the embeddings of the three involved relations.\nWe can also see that in general, using multiplicative composition of matrix embeddings (from DIST-\nMULT and BILINEAR) results in better performance compared to using additive composition of\nvector embeddings (from DISTADD). We found many examples where DISTADD fails to retrieve\nrules because it assigns large distance between the composition of the body relations and the head\nrelation in the embedding space while its multiplicative counterpart DISTMULT ranks the composi-\ntion result much closer to the head relation. For example, DISTADD prunes the possible composition\nFilmDistributorInRegion^RegionGDPCurrency for relation FilmBudgetCurrency while\nDISTMULT ranks the composition as the nearest neighbor of FilmBudgetCurrency.\nFigure 1: Aggregated precision of top length-2 rules extracted by different methods\nFigure 2: Aggregated precision of top length-3 rules extracted by different methods\nWe also look at the results for length-3 rules generated by different implementations of EMBEDRULE\nin Figure 2. We can see that the initial length-3 rules extracted by EMBEDRULE can provide very\ngood precision in general. We can also see that BILINEAR consistently outperforms DISTMULT and\nDISTADD on the top 1K predictions and DISTMULT-TANH-EV-INIT tends to outperform the other\nmethods as more predictions are generated. We think that the fact that BILINEAR starts to show more\nadvantage over DISTMULT in extracting longer rules con\ufb01rm the limitation of representing relations\nby diagonal matrices, as longer rules requires the modeling of more complex relation semantics.\n9\nPublished as conference paper at ICLR 2015\n6\nCONCLUSION\nIn this paper, we present a general framework for learning representations of entities and relations\nin KBs. Under the framework, we empirically evaluate different embedding models on knowledge\ninference tasks. We show that a simple formulation of bilinear model can outperform the state-of-\nthe-art embedding models for link prediction on Freebase. Furthermore, we examine the learned\nembeddings by utilizing them to extract logical rules from KBs. We show that embeddings learned\nfrom the bilinear objective can capture compositional semantics of relations and be successfully\nused to extract Horn rules that involve compositional reasoning. For future work, we aim to exploit\ndeep structure in the neural network framework. As learning representations using deep networks\nhas shown great success in various applications (Hinton et al., 2012; Vinyals et al., 2012; Deng et al.,\n2013), it may also help capturing hierarchical structure hidden in the multi-relational data. Further,\ntensor constructs have been usefully applied to some deep learning architectures (Yu et al., 2013;\nHutchinson et al., 2013). Related constructs and architectures may help improve multi-relational\nlearning and inference.\nAPPENDIX\nA\nEXAMPLES OF THE EXTRACTED HORN RULES\nExamples of length-2 rules extracted by EMBEDRULE with embeddings learned from DISTMULT-\ntanh-EV-init:\nAwardInCeremanypa, bq ^ CeremanyEventTypepb, cq \u00f9\u00f1 AwardInEventTypepa, cq\nAtheletePlayInTeampa, bq ^ TeamPlaySportpb, cq \u00f9\u00f1 AtheletePlaySportpa, cq\nTV ProgramInTV Networkpa, bq^TV NetworkServiceLanguagepb, cq \u00f9\u00f1 TV ProgramLanguagepa, cq\nLocationInStatepa, bq ^ StateInCountrypb, cq \u00f9\u00f1 LocationInCountrypa, cq\nBornInLocationpa, bq ^ LocationInCountrypb, cq \u00f9\u00f1 Nationalitypa, cq\nExamples of length-3 rules extracted by EMBEDRULE with embeddings learned from DISTMULT-\ntanh-EV-init:\nSportPlayByTeampa, bq^TeamInClubpb, cq^ClubHasPlayerpc, dq \u00f9\u00f1 SportPlayByAtheletepa, dq\nMusicTrackPerformerpa, bq^PeerInfluencepb, cq^PerformRolepc, dq \u00f9\u00f1 MusicTrackRolepa, dq\nFilmHasActorpa, bq^CelebrityFriendshippb, cq^PersonLanguagepc, dq \u00f9\u00f1 FilmLanguagepa, dq\nB\nVISUALIZATION OF THE RELATION EMBEDDINGS\nVisualization of the relation embeddings learned by DISTMULT and DISTADD using t-SNE (see\n\ufb01gure 3 and 4). We selected 189 relations in the FB15k-401 dataset. The embeddings learned by\nDISTMULT nicely re\ufb02ect the clustering structures among these relations (e.g. /\ufb01lm/release region\nis closed to /\ufb01lm/country); whereas the embeddings learned by DISTADD present structure that is\nharder to interpret.\nREFERENCES\nAuer, S\u00a8oren, Bizer, Christian, Kobilarov, Georgi, Lehmann, Jens, Cyganiak, Richard, and Ives,\nZachary.\nDbpedia: A nucleus for a web of open data.\nIn The semantic web, pp. 722\u2013735.\nSpringer, 2007.\nBordes, Antoine, Weston, Jason, Collobert, Ronan, and Bengio, Yoshua. Learning structured em-\nbeddings of knowledge bases. In AAAI, 2011.\nBordes, Antoine, Glorot, Xavier, Weston, Jason, and Bengio, Yoshua. A semantic matching energy\nfunction for learning with multi-relational data. Machine Learning, pp. 1\u201327, 2013a.\nBordes, Antoine, Usunier, Nicolas, Garcia-Duran, Alberto, Weston, Jason, and Yakhnenko, Oksana.\nTranslating embeddings for modeling multi-relational data. In NIPS, 2013b.\n10\nPublished as conference paper at ICLR 2015\nFigure 3: Relation embeddings (DISTADD)\nFigure 4: Relation embeddings (DISTMULT)\nBowman, Samuel R. Can recursive neural tensor networks learn logical reasoning? In ICLR, 2014.\nChang, Kai-Wei, Yih, Wen-tau, Yang, Bishan, and Meek, Chris. Typed tensor decomposition of\nknowledge bases for relation extraction. In EMNLP, 2014.\nDeng, Li, Hinton, G., and Kingsbury, B. New types of deep neural network learning for speech\nrecognition and related applications: An overview. In in ICASSP, 2013.\nDuchi, John, Hazan, Elad, and Singer, Yoram. Adaptive subgradient methods for online learning\nand stochastic optimization. The Journal of Machine Learning Research, 12:2121\u20132159, 2011.\nGal\u00b4arraga, Luis Antonio, Te\ufb02ioudi, Christina, Hose, Katja, and Suchanek, Fabian. Amie: associa-\ntion rule mining under incomplete evidence in ontological knowledge bases. In WWW, 2013.\nGao, Jianfeng, Pantel, Patrick, Gamon, Michael, He, Xiaodong, Deng, Li, and Shen, Yelong. Mod-\neling interestingness with deep neural networks. In EMNLP, 2014.\nGarc\u00b4\u0131a-Dur\u00b4an, Alberto, Bordes, Antoine, and Usunier, Nicolas. Effective blending of two and three-\nway interactions for modeling multi-relational data. In Machine Learning and Knowledge Dis-\ncovery in Databases, pp. 434\u2013449. Springer, 2014.\nGetoor, Lise and Taskar, Ben (eds.). Introduction to Statistical Relational Learning. The MIT Press,\n2007.\nGrefenstette, Edward. Towards a formal distributional semantics: Simulating logical calculi with\ntensors. In *SEM, 2013.\n11\nPublished as conference paper at ICLR 2015\nHinton, Geoff, Deng, L., Yu, D., Dahl, G., Mohamed, A., Jaitly, N., Senior, A., Vanhoucke, V.,\nNguyen, P., Sainath, T., and Kingsbury, B. Deep neural networks for acoustic modeling in speech\nrecognition. IEEE Sig. Proc. Mag., 29:82\u201397, 2012.\nHuang, Po-Sen, He, Xiaodong, Gao, Jianfeng, Deng, Li, Acero, Alex, and Heck, Larry. Learning\ndeep structured semantic models for Web search using clickthrough data. In CIKM, 2013.\nHutchinson, B, Deng, L., and Yu, D. Tensor deep stacking networks. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 35(8):1944\u20131957, 2013.\nJenatton, Rodolphe, Le Roux, Nicolas, Bordes, Antoine, and Obozinski, Guillaume. A latent factor\nmodel for highly multi-relational data. In NIPS, 2012.\nKemp, Charles, Tenenbaum, Joshua B, Grif\ufb01ths, Thomas L, Yamada, Takeshi, and Ueda, Naonori.\nLearning systems of concepts with an in\ufb01nite relational model. In AAAI, volume 3, pp. 5, 2006.\nMikolov, Tomas, Sutskever, Ilya, Chen, Kai, Corrado, Greg S, and Dean, Jeff. Distributed represen-\ntations of words and phrases and their compositionality. In NIPS, pp. 3111\u20133119, 2013.\nNickel, Maximilian, Tresp, Volker, and Kriegel, Hans-Peter. A three-way model for collective learn-\ning on multi-relational data. In ICML, pp. 809\u2013816, 2011.\nNickel, Maximilian, Tresp, Volker, and Kriegel, Hans-Peter. Factorizing YAGO: scalable machine\nlearning for linked data. In WWW, pp. 271\u2013280, 2012.\nPaccanaro, Alberto and Hinton, Geoffrey E. Learning distributed representations of concepts using\nlinear relational embedding. IEEE Transactions on Knowledge and Data Engineering, 13(2):\n232\u2013244, 2001.\nRichardson, Matthew and Domingos, Pedro. Markov logic networks. Machine learning, 62(1-2):\n107\u2013136, 2006.\nRockt\u00a8aschel, Tim, Bo\u02c7snjak, Matko, Singh, Sameer, and Riedel, Sebastian. Low-dimensional em-\nbeddings of logic. In ACL Workshop on Semantic Parsing, 2014.\nSchoenmackers, Stefan, Etzioni, Oren, Weld, Daniel S, and Davis, Jesse. Learning \ufb01rst-order horn\nclauses from web text. In EMNLP, 2010.\nShen, Yelong, He, Xiaodong, Gao, Jianfeng, Deng, Li, and Mesnil, Gregoire. A latent semantic\nmodel with convolutional-pooling structure for information retrieval. In CIKM, 2014a.\nShen, Yelong, He, Xiaodong, Gao, Jianfeng, Deng, Li, and Mesnil, Gr\u00b4egoire. Learning semantic\nrepresentations using convolutional neural networks for Web search. In WWW, pp. 373\u2013374,\n2014b.\nSingh, Ajit P and Gordon, Geoffrey J. Relational learning via collective matrix factorization. In\nKDD, pp. 650\u2013658. ACM, 2008.\nSocher, Richard, Huval, Brody, Manning, Christopher D., and Ng, Andrew Y. Semantic composi-\ntionality through recursive matrix-vector spaces. In EMNLP-CoNLL, 2012.\nSocher, Richard, Chen, Danqi, Manning, Christopher D., and Ng, Andrew Y. Reasoning with neural\ntensor networks for knowledge base completion. In NIPS, 2013.\nSuchanek, Fabian M, Kasneci, Gjergji, and Weikum, Gerhard. Yago: a core of semantic knowledge.\nIn WWW, 2007.\nSutskever, Ilya, Tenenbaum, Joshua B, and Salakhutdinov, Ruslan. Modelling relational data using\nBayesian clustered tensor factorization. In NIPS, pp. 1821\u20131828, 2009.\nVinyals, O., Jia, Y., Deng, L., and Darrell, T. Learning with recursive perceptual representations. In\nNIPS, 2012.\nYih, Wen-tau, He, Xiaodong, and Meek, Christopher. Semantic parsing for single-relation question\nanswering. In ACL, 2014.\nYu, D., Deng, L., and Seide, F. The deep tensor neural network with applications to large vocabulary\nspeech recognition. IEEE Trans. Audio, Speech and Language Proc., 21(2):388 \u2013396, 2013.\n12\n",
        "sentence": " Most of the embedding models for knowledge base completion define an energy function fr(h, t) according to the fact\u2019s plausibility (Bordes et al., 2011, 2014, 2013; Socher et al., 2013; Wang et al., 2014; Yang et al., 2015; Guu et al., 2015; Nguyen et al., 2016b). Previous work (Bordes et al., 2013; Lin et al., 2015b; Yang et al., 2015; Nguyen et al., 2016b) generally constructs a set of corrupted triples by replacing the head entity or tail entity with a random entity uniformly sampled from the KB. 4 DISTMULT (Yang et al., 2015) No 94.",
        "context": "\ufb01lms). The promising performance of DISTMULT-tanh-EV-init suggests that the embedding model\ncan greatly bene\ufb01t from pre-trained entity-level vectors using external textual resources.\n5\nINFERENCE TASK II: RULE EXTRACTION\nare learned using neural networks with energy-based objectives.\nRecent embedding models\nTransE (Bordes et al., 2013b) and NTN (Socher et al., 2013) have shown state-of-the-art predic-\nPublished as conference paper at ICLR 2015\nEMBEDDING ENTITIES AND RELATIONS FOR LEARN-\nING AND INFERENCE IN KNOWLEDGE BASES\nBishan Yang1\u02da, Wen-tau Yih2, Xiaodong He2, Jianfeng Gao2 & Li Deng2"
    },
    {
        "title": "Semantic parsing via staged query graph generation: Question answering with knowledge base",
        "author": [
            "Wen-tau Yih",
            "Ming-Wei Chang",
            "Xiaodong He",
            "Jianfeng Gao."
        ],
        "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Lin-",
        "citeRegEx": "Yih et al\\.,? 2015",
        "shortCiteRegEx": "Yih et al\\.",
        "year": 2015,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " for many applications such as question answering (Berant et al., 2013; Yih et al., 2015; Dai et al., 2016) and information extraction (Mintz et al.",
        "context": null
    },
    {
        "title": "Transfer learning for low-resource neural machine translation",
        "author": [
            "Barret Zoph",
            "Deniz Yuret",
            "Jonathan May",
            "Kevin Knight."
        ],
        "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computa-",
        "citeRegEx": "Zoph et al\\.,? 2016",
        "shortCiteRegEx": "Zoph et al\\.",
        "year": 2016,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    }
]