[
    {
        "title": "Fast structure learning in generalized stochastic processes with latent factors",
        "author": [
            "Bahadori",
            "Mohammad Taha",
            "Liu",
            "Yan",
            "Xing",
            "Eric P"
        ],
        "venue": "In KDD, pp",
        "citeRegEx": "Bahadori et al\\.,? \\Q2013\\E",
        "shortCiteRegEx": "Bahadori et al\\.",
        "year": 2013,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " When the time axis is discretized, the point process data can be converted to binary time series (or time series of count data if binning is coarse) and analyzed via time series analysis techniques (Truccolo et al., 2005; Bahadori et al., 2013; Ranganath et al., 2015).",
        "context": null
    },
    {
        "title": "Theano: new features and speed improvements",
        "author": [
            "Bastien",
            "Fr\u00e9d\u00e9ric",
            "Lamblin",
            "Pascal",
            "Pascanu",
            "Razvan",
            "Bergstra",
            "James",
            "Goodfellow",
            "Ian J",
            "Bergeron",
            "Arnaud",
            "Bouchard",
            "Nicolas",
            "Bengio",
            "Yoshua"
        ],
        "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,",
        "citeRegEx": "Bastien et al\\.,? \\Q2012\\E",
        "shortCiteRegEx": "Bastien et al\\.",
        "year": 2012,
        "abstract": "Theano is a linear algebra compiler that optimizes a user's\nsymbolically-specified mathematical computations to produce efficient low-level\nimplementations. In this paper, we present new features and efficiency\nimprovements to Theano, and benchmarks demonstrating Theano's performance\nrelative to Torch7, a recently introduced machine learning library, and to\nRNNLM, a C++ library targeted at recurrent neural networks.",
        "full_text": "Theano: new features and speed improvements\nFr\u00b4ed\u00b4eric Bastien, nouiz@nouiz.org\nPascal Lamblin, lamblinp@iro.umontreal.ca\nRazvan Pascanu, r.pascanu@gmail.com\nJames Bergstra, james.bergstra@gmail.com\nIan Goodfellow, goodfeli@iro.umontreal.ca\nArnaud Bergeron, bergearn@iro.umontreal.ca\nNicolas Bouchard, nicolas.bouchard.1@gmail.com\nDavid Warde-Farley, wardefar@iro.umontreal.ca\nYoshua Bengio, yoshua.bengio@umontreal.ca\nDept. IRO, Universit\u00b4e de Montr\u00b4eal, Montr\u00b4eal (QC), H3C 3J7, Canada\nAbstract\nTheano is a linear algebra compiler that optimizes a user\u2019s symbolically-speci\ufb01ed\nmathematical computations to produce ef\ufb01cient low-level implementations. In\nthis paper, we present new features and ef\ufb01ciency improvements to Theano, and\nbenchmarks demonstrating Theano\u2019s performance relative to Torch7, a recently\nintroduced machine learning library, and to RNNLM, a C++ library targeted at\nrecurrent neural networks.\n1\nIntroduction\nTheano was introduced to the machine learning community by Bergstra et al. (2010) as a CPU and\nGPU mathematical compiler, demonstrating how it can be used to symbolically de\ufb01ne mathematical\nfunctions, automatically derive gradient expressions, and compile these expressions into executable\nfunctions that outperform implementations using other existing tools. Bergstra et al. (2011) then\ndemonstrated how Theano could be used to implement Deep Learning models.\nIn Section 2, we will brie\ufb02y expose the main goals and features of Theano. Section 3 will present\nsome of the new features available and measures taken to speed up Theano\u2019s implementations. Sec-\ntion 4 compares Theano\u2019s performance with that of Torch7 (Collobert et al., 2011) on neural network\nbenchmarks, and RNNLM (Mikolov et al., 2011) on recurrent neural network benchmarks.\n2\nMain features of Theano\nHere we brie\ufb02y summarize Theano\u2019s main features and advantages for machine learning tasks.\nBergstra et al. (2010, 2011), as well as Theano\u2019s website1 have more in-depth descriptions and\nexamples.\n2.1\nSymbolic Mathematical Expressions\nTheano includes powerful tools for manipulating and optimizing graphs representing symbolic\nmathematical expressions. In particular, Theano\u2019s optimization constructs can eliminate duplicate\nor unnecessary computations (e.g., replacing x \u2212x by 0, obviating the need to compute x in the \ufb01rst\nplace), increase numerical stability (e.g., by substituting stable implementations of log(1 + x) when\n1http://deeplearning.net/software/theano/\n1\narXiv:1211.5590v1  [cs.SC]  23 Nov 2012\nx is tiny, or log(sigmoid(x))), or increase speed (e.g., by using loop fusion to apply a sequence of\nscalar operations to all elements of an array in a single pass over the data).\nThis graph representation also enables symbolic differentiation of mathematical expressions, which\nallows users to quickly prototype complex machine learning models \ufb01t by gradient descent without\nmanually deriving the gradient, decreasing the amount of code necessary and eliminating several\nsources of practitioner error. Theano now supports forward-mode differentiation via the R-operator\n(see Section 3.2) as well as regular gradient backpropagation. Theano is even able to derive symbolic\ngradients through loops speci\ufb01ed via the Scan operator (see Section 3.1).\n2.2\nFast to write and to execute\nTheano\u2019s dependency on NumPy and SciPy (Jones et al., 2001) makes it easy to add an implementa-\ntion for a mathematical operation, leveraging the effort of their developers, and it is always possible\nto add a more optimized version that will then be transparently substituted where applicable. For\ninstance, Theano de\ufb01nes operations on sparse matrices using SciPy\u2019s sparse matrix types to hold\nvalues. Some of these operations simply call SciPy\u2019s functions, other are reimplemented in C++,\nusing BLAS routines for speed.\n2.3\nParallelism on the GPU\nTheano uses CUDA to de\ufb01ne a class of n-dimensional (dense) arrays located in GPU memory\nwith Python bindings. Theano also includes CUDA code generators for fast implementations of\nmathematical operations. Most of these operations are currently limited to dense arrays of single-\nprecision \ufb02oating-point numbers.\n2.4\nStability and community support\nTheano\u2019s development team has increased its commitment to code quality and correctness as Theano\nusage begins to spread across university and industry laboratories: a full test suite runs every night,\nwith a shorter version running for every pull request, and the project makes regular stable releases.\nThere is also a growing community of users who ask and answer questions every day on the project\u2019s\nmailing lists.\n3\nNew features in Theano\nThis section presents features of Theano that have been recently developed or improved. Some of\nthese are entirely novel and extend the scenarios in which Theano can be used (notably, Scan and\nthe R operator); others aim at improving performance, notably reducing the time not spent in actual\ncomputation (such as Python interpreter overhead), and improving parallelism on CPU and GPU.\n3.1\nScan: Symbolic Loop in Theano\nTheano offers the ability to de\ufb01ne symbolic loops through use of the Scan Op, a feature useful\nfor working with recurrent models such as recurrent neural networks, or for implementing more\ncomplex optimization algorithms such as linear conjugate gradient.\nScan surmounts the practical dif\ufb01culties surrounding other approaches to loop-based computation\nwith Theano. Using Theano\u2019s symbolically-de\ufb01ned implementations within a Python loop prevents\nsymbolic differentiation through the iterative process, and prevents certain graph optimizations from\nbeing applied. Completely unrolling the loop into a symbolic chain often leads to an unmanageably\nlarge graph and does not allow for \u201cwhile\u201d-style loops with a variable number of iterations.\nThe Scan operator is designed to address all of these issues by abstracting the entire loop into a\nsingle node in the graph, a node that communicates with a second symbolic graph representing\ncomputations inside the loop. Without going into copious detail, we present a list of the advantages\nof our strategy and refer to section 4.3 where we empirically demonstrate some of these advantages.\nTutorials available from the Theano website offer a detailed description of the required syntax as\nwell as example code.\n2\n1. Scan allows for ef\ufb01cient computation of gradients and implicit \u201cvector-Jacobian\u201d prod-\nucts. The speci\ufb01c algorithm used is backpropagation through time Rumelhart et al. (1986),\nwhich optimizes for speed but not memory consumption.\n2. Scan allows for ef\ufb01cient evaluation of the R-operator (see Pearlmutter (1994)), required for\ncomputing quantities such as the Gauss-Newton approximation of Hessian-vector products.\n3. The number of iterations performed by Scan can itself be expressed as a symbolic variable\n(for example, the length of some input sequence) or a symbolically speci\ufb01ed condition, in\nwhich case Scan behaves as a \u201cdo while\u201d statement. If the number of steps is \ufb01xed and\nequal to 1, the Scan node is \u201cunrolled\u201d into the outer graph for better performance.\n4. Any loop implemented with Scan can be transparently transferred to a GPU (if the compu-\ntation at each iteration can itself be performed on the GPU).\n5. The body of Scan (which involves computing indices of where to pick input slices and\nwhere to put the output of each iteration) is implemented with Cython to minimize the\noverhead introduced by necessary bookkeeping between each iteration step.\n6. Whenever possible, Scan detects the amount of memory necessary to carry out an opera-\ntion: it examines intermediate results and makes an informed decision as to whether such\nresults are needed in subsequent iterations in order to partially optimize memory reuse.\nThis decision is taken at compilation time.\n7. Loops represented as different Scan instances are merged (given that certain necessary con-\nditions are respected, e.g., both loops perform the same number of steps). This aids not only\nin reducing the overhead introduced by each instance of Scan, but also helps optimize the\ncomputation performed at each iteration of both loops, e.g. certain intermediate quantities\nmay be useful to the body of each individual loop, and will be computed only once in the\nmerged instance.\n8. Finally, whenever a computation inside the loop body could be performed outside the loop,\nScan moves said computation in the main graph. For example element-wise operations\nare moved outside, where, given that they are done by a single call to an Elementwise\noperations, one can reduce overhead. Another example is dot products between a vector\nand a matrix, which can be transformed outside of the loop into a single matrix-matrix\nmultiplication. Such optimizations can lead to signi\ufb01cant speed improvement and in certain\ncases to the elimination of the Scan node completely.\nAll of these features make it easier for a user to implement a variety of recurrent neural networks\narchitectures, and to easily change the equations of the model without having to derive gradients by\nhand or worry about manually optimizing the implementation.\n3.2\nR-operator for Hessian-Free optimization\nRecent results (Martens and Sutskever, 2011) proposed a speci\ufb01c pipeline for ef\ufb01ciently implement-\ning truncated Newton-like second-order methods such as Hessian-Free optimization. The pipeline\nrelies on the \u201cR-operator\u201d, introduced by Pearlmutter (1994), which is a mathematical operator that\ngiven a function f(\u03b8), f : RM \u2192RN, the current parameter con\ufb01guration \u03b8t \u2208RM and a vec-\ntor \u03b3 \u2208RM, ef\ufb01ciently computes the \u201cJacobian-vector\u201d product\n\u0012\n\u2202f\n\u2202\u03b8\n\f\f\f\n\u03b8=\u03b8t\n\u0013\n\u03b3, where\n\u0012\n\u2202f\n\u2202\u03b8\n\f\f\f\n\u03b8=\u03b8t\n\u0013\nis the Jacobian of the function evaluated at \u03b8t. For the sake of completeness, we would mention\nthat the \u201cR-operator\u201d evaluates the directional derivative of f(\u03b8), and is known in the automatic\ndifferentiation community as the forward mode.\nThis operation can be seen analogous to the backward mode or backpropagation, which computes\nthe \u201cvector-Jacobian\u201d product \u03b7T\n\u0012\n\u2202f\n\u2202\u03b8\n\f\f\f\n\u03b8=\u03b8t\n\u0013\n, where \u03b7T \u2208RN is some row vector.\nTheano offers ef\ufb01cient computation of both operators by employing the chain rule on the compu-\ntational graph, where each operational node knows how to compute the product of its Jacobian and\nsome vector in an ef\ufb01cient way.\nBecause the output of any such operation is a symbolic graph, the computations get further optimized\nat compilation time. This provides \ufb02exibility in writing down the computations that represent a\n3\nmodel, without worrying about details that would lead to faster gradients, or faster \u201cJacobian-vector\u201d\nproducts. For example, let us consider a complicated model, a recurrent network and the task of\ncomputing the Gauss-Newton approximation of the Hessian times some vector (which lies at the\nheart of the Hessian-Free algorithm). A naive implementation would imply at least three passes\nthrough the loop, once for evaluating the function, the second one to backpropagate the gradient\n(reverse-mode) and the third time to compute the \u201cJacobian-vector\u201d dot product involved in the\nequation \u2202f\n\u2202\u03b8\n\u0010\n\u2202f\n\u2202\u03b8\nT \u0011\n\u03b3. A more careful implementation however reveals that two passes should\nbe suf\ufb01cient (see Martens and Sutskever (2011)). By simply calling TT.Lop(f, \u03b8, TT.Rop(f, \u03b8, \u03b3)),\nTheano is able to \ufb01gure out the relationship between the different loops, resulting in only two passes.\n3.3\nLazy Evaluation, CVM\nWhen a compiled Theano function is called, a runtime engine orchestrates which operations should\nbe performed on which data, calling the appropriate functions in the right order. This was previ-\nously implemented as a Python loop, calling either native Python functions or C functions made\navailable through a Python module interface, in a pre-determined order (i.e., a forward traversal of\nthe computational graph, from inputs to outputs). The main drawback of this approach is that it was\nimpossible to implement lazy evaluation in the computational graph.\nFor instance, the \u201cif-then-else\u201d construct would always compute the result of both \u201cthen\u201d and \u201celse\u201d\nbranches, as well as the condition, before updating its output value. A new runtime, dubbed the\n\u201cVM\u201d (for \u201cVirtual Machine\u201d, because it drives the execution of small code units), enables lazy\nevaluation of such operations, meaning that we evaluate only branches that are actually necessary\nfor correct computation of the output.\nA C implementation of the VM was also added (dubbed the \u201cCVM\u201d). Beyond the performance\nadvantage inherent in running the loop itself in C, the CVM also avoids the performance penalty of\nreturning control to the Python interpreter after each operation: if a C implementation of a given\noperation is available, the CVM will execute it directly without the overhead of a Python function\ncall. The performance gain is particularly signi\ufb01cant for graphs that perform many operations on\nrelatively small operands. In particular, if all operations used in a compiled Theano function have\nC implementations, the entirety of the CVM\u2019s execution will be performed at C speed, returning\ncontrol to the Python interpreter only after all outputs have been computed. The CVM is now the\ndefault runtime.\n3.4\nMore operations implemented in C\nTo derive fuller bene\ufb01t from the existence of the CVM, we have added new C implementations of\nexisting operations (even when Python implementations were almost as ef\ufb01cient) in order to avoid\ncontext switches. For instance, matrix-vector dot products on CPU had previously resulted in a call\nto a SciPy function that wraps the GEMV routine from BLAS. We have since added a wrapper in C\nthat calls the GEMV routine directly.\n3.5\nBetter support for sparse matrices\nIn addition to dense tensors, Theano supports sparse matrices based on SciPy\u2019s implementations of\ncompressed sparse row (CSR) and compressed sparse column (CSC) formats. Support for ef\ufb01cient\nsparse operations, in particular operations needed to compute derivatives of sparse operations, has\nbeen greatly improved. The online documentation2 lists currently supported operations.\nTheano supports two kinds of gradient computation through sparse matrices. \u201cRegular\u201d differen-\ntiation does not suppose that the sparsity structure of a matrix at a given time is preserved, and\nthus a sparse variable may have a dense gradient. \u201cStructured\u201d differentiation considers the sparsity\nstructure of a matrix as permanent, and the gradient with respect to that matrix will have the same\nsparsity structure.\n2http://deeplearning.net/software/theano/library/sparse/\n4\n3.6\nParallelism on CPU\nIn the past, not much effort had been put into allowing Theano to leverage multi-core CPU architec-\ntures for parallel execution; development effort was instead focused on GPU implementations and\nnew automatic optimizations. Multi-core parallelism was therefore only available to operations that\ncalled into a parallel BLAS implementation.\nCollobert et al. (2011) showed that using OpenMP to parallelize the C implementation of CPU\noperations can bring substantial speed improvements with relatively little development effort. We\nrecently added support for OpenMP-enabled operations in Theano, and used this support to paral-\nlelize 2-dimensional convolution. Adding parallel implementations for other operations will proceed\nmore rapidly with this infrastructure in place.\n3.7\nAsynchronous function calls on GPU\nWhen executing CUDA kernels on the GPU, the function call that starts it does not wait for the\nexecution of the kernel to complete. Instead, it will merely schedule the kernel to be executed at\nsome point in the future, allowing the main program to perform other tasks, including scheduling\nother GPU kernels. When the result of the GPU computation is needed, the program can wait for\nthe end of the kernel to execute, and return its result.\nBefore release 0.6, Theano always waited for the result of the kernel computation as soon as it was\nlaunched, effectively preventing the execution of other operations on the CPU during this time. This\napproach eases pro\ufb01ling and debugging because at any given time, it is clear which GPU kernel\nis currently being executed, and error messages are retrieved as soon as possible; however, such\nan approach prohibits the concurrent use of CPU-based computation, passing up an opportunity\nfor further speed gains. The new default behaviour of Theano is not to wait on the result of GPU\ncomputation until it is strictly needed. It is also possible to revert to the previous behaviour, which\nis useful for pro\ufb01ling execution time of the different GPU kernels.\n4\nBenchmarks\nBergstra et al. (2010) showed that Theano was faster than many other tools available at the time,\nincluding Torch5. The following year, Collobert et al. (2011) showed that Torch7 was faster than\nTheano on the same benchmarks3.\nHere we brie\ufb02y introduce Torch7 and evaluate performance of their latest versions on neural network\ntasks, using the aforementioned benchmarks. Then, Section 4.3 will compare the performance of\nTheano against another package, RNNLM, when training recurrent neural networks.\n4.1\nTorch7\nTorch7 (Collobert et al., 2011) is advertised as a Matlab-like environment for machine learning. It\naims to ease development of numerical algorithms and to allow for their fast execution, while also\nbeing easy to extend.\nTable 1 provides a summary comparison of the features provided by Torch7 (including the ones\ninherited from Lua) and Theano (including the ones coming from Python and NumPy/SciPy). This\nsection exposes the common features and differences between Torch7 and Theano.\n4.1.1\nCommon features shared by Torch7 and Theano\nTheano and Torch7 are two computing frameworks that were developed for the machine learning\ncommunity, to make it easier to quickly implement and test new mathematical models and algo-\nrithms, without giving up the execution speed that a manually-optimized implementation would\nprovide. Both are the foundation of machine learning speci\ufb01c packages or projects, notably for\nneural networks and unsupervised learning.\n3https://github.com/jaberg/DeepLearningBenchmarks\n5\nTable 1: Features comparison between Lua/Torch7 and Python/Theano. The \ufb01rst section shows\ncommon or comparable features. Second and third part contains Theano\u2019s and Torch7\u2019s strengths.\nFeatures\nLua/Torch7\nPython/Theano\nScripting language\n\u0014\n\u0014\nFast execution speed\n\u0014\n\u0014\nOptimized BLAS, LAPACK\n\u0014\n\u0014\nPlotting Environment\n\u0014\n\u0014 via matplotlib\nGPU\n\u0014 \ufb02oat only\n\u0014 \ufb02oat only\nEasy call to C functions\n\u0014 Natively with Lua\n\u0014 via Cythona, ctypes, etc.\nOS\nLinux, MacOS X, FreeBSD\nLinux, MacOS X, Windows\nPublic development\n\u0014 on GitHubb\n\u0014 on GitHubc\nUnit tests\n\u0014\n\u0014 Buildbotd, Travis-CIe\nUsed in published research\n\u0014\n\u0014\nUsed at companies\nNEC\nGoogle, Yahoo!, Card.io, startups\nSparse matrices\n\u0018\n\u0014\nSymbolic differentiation\nNon-symbolic NN gradient\n\u0014\nDifferentiation over loop\n\u0018\n\u0014 Scan\nR-operator\n\u0018\n\u0014 For most operations\nAutomatic graph optimization\n\u0018\n\u0014\nParallel functions\n\u0014 OpenMP widely used\nOnly in BLAS and Conv2D\nEmbeddable in a C app.\n\u0014\n\u0018\nInformative error messages\n\u0014\nNot always\nahttp://www.cython.org/\nbhttps://github.com/andresy/torch\nchttps://github.com/Theano/Theano\ndhttps://groups.google.com/group/theano-buildbot\nehttp://travis-ci.org/#!/Theano/Theano\nLike Theano, Torch7 is based on a scripting language (Lua), uses heavily-optimized scienti\ufb01c com-\nputation libraries (for instance, BLAS and LAPACK for linear algebra computations), and internal\nmodules written in C/C++, for the sections where execution speed is critical. It also has the capabil-\nity of running parallel computation on multi-core CPUs (via OpenMP), and on GPUs via CUDA.\nBoth have access to a Matlab-like environment: Torch7 includes modules for tensor manipula-\ntions and plotting, while Theano bene\ufb01ts from various external Python libraries to perform those\ntasks (notably SciPy (Jones et al., 2001), NumPy (Oliphant, 2007), matplotlib (Hunter, 2007),\nIPython (P\u00b4erez and Granger, 2007)).\n4.1.2\nMain differences\nSome of Torch7\u2019s strengths stem from Lua\u2019s advantages over Python: lower interpreter overhead,\nsimpler integration with C code, easy embedding in a C application. In particular, since the overhead\nof calling a function and executing C code are lower, higher performance will result in the case of\nsimple functions (that do not perform large amounts of computation) and functions that process only\na small quantity of data at a time.\nParallelism on multi-core CPUs is another important feature of Torch7, as it was designed to use\nOpen Multi-Processing (OpenMP) parallel directives, notably in the tensor and neural network mod-\nules. The potential for CPU parallelization (outside calls to BLAS) in Theano has only started to be\nexplored.\nTheano\u2019s distinguishing feature is its powerful engine for graph optimization and symbolic differ-\nentiation, mentioned in Section 2.1. The downside is that users are faced with a more complex\nwork\ufb02ow: \ufb01rst, de\ufb01ne an abstract mathematical graph (without values), then optimize and compile\nit into a callable function, and \ufb01nally execute the function. This additional complexity also makes it\nharder to interpret errors that may be raised during the compilation or execution phase.\n6\n4.2\nBenchmarking on Deep Learning Tasks\n4.2.1\nExperimental Setup\nThe experiments reported here were conducted on a machine with an Intel Core i7 CPU 930 @\n2.80GHz, and a nVidia GTX480 GPU. The commit ID of Theano\u2019s version was 254894fac,\nTorch7 was 41d3b8b93.\nAs the multi-layer perceptron (MLP) examples in the benchmarks rely on function calls to a BLAS\nlibrary, we made sure the same BLAS library was used for both Torch7 and Theano, in order to\nensure a fair comparison. We benchmarked the GEMM routine (matrix-matrix dot product, scaling\nand accumulation), with matrix sizes large enough that any overhead becomes negligible, for a\nnumber of OpenMP threads limited to 1 and 4, con\ufb01rming that both tools are linked to the same\nBLAS library, and that controlling the number of OpenMP threads works as expected.\n0\n10000\n20000\n30000\n40000\n50000\nbatch 1\n0\n20000\n40000\n60000\n80000\n100000\n120000\n140000\n160000\nexamples/second\nbatch 10\nCPU\nOpenMP\nGPU\n0\n50000\n100000\n150000\n200000\n250000\n300000\n350000\n400000\nbatch 60\n(a) Logistic regression\n0\n2000\n4000\n6000\n8000\n10000\n12000\n14000\nbatch 1\n0\n5000\n10000\n15000\n20000\n25000\nexamples/second\nbatch 10\nCPU\nOpenMP\nGPU\n0\n20000\n40000\n60000\n80000\n100000\nbatch 60\n(b) Neural network, 1 hidden layer with 500 units\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600\nbatch 1\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\n4000\nexamples/second\nbatch 10\nCPU\nOpenMP\nGPU\n0\n5000\n10000\n15000\n20000\nbatch 60\n(c) Deep neural network, 3 hidden layers with 1000\nunits each\nTorch7\nWith Lua\nWith LuaJIT\nTheano\nDefault configuration\nNo garbage collection (no GC)\nNo conversion of function input data, no GC\nFunction without inputs, C outer loop, no GC\nTheano\nDefault configuration\nNo garbage collection (no GC)\nNo conversion of function input data, no GC\nFunction without inputs, C outer loop, no GC\nFigure 1: Benchmarks of Torch7 (red, left) vs. Theano (blue, right), for training neural networks.\nThe bars represent examples per second, higher is better. For each architecture (a), (b), and (c),\ntop \ufb01gures represent stochastic gradient descent, middle and bottom ones use mini-batches of sizes\n(resp.) 10 and 60. The group of bars on the left shows performance on CPU with one thread only, the\nmiddle group shows the parallelized version with OpenMP, using 4 CPU threads, and the right-most\nshow performance on GPU.\n4.2.2\nHow to boost Theano\u2019s performance\nIn Figure 1, the left-most blue bar (lightest shade of blue) in each of the bar groups shows the\nperformance of a Theano function with the default con\ufb01guration. That default con\ufb01guration includes\nthe use of the CVM (section 3.3), and asynchronous execution of GPU ops (section 3.7). This section\nshows ways to further speed up the execution, while trading off other features.\n7\nDisabling garbage collection\nWe can save time on memory allocation by disabling garbage col-\nlection of intermediate results. This can be done by using the linker cvm_nogc. In this case, the\nresults of intermediate computation inside a Theano function will not be deallocated, so during the\nnext call to the same function, this memory will be reused, and new memory will not have to be\nallocated. This increases memory usage, but speeds up execution of the Theano function.\nIn Figure 1, the second-to-left blue bar shows the impact of disabling garbage collection. It is most\nimportant on the GPU, because the garbage-collection mechanism forces a synchronization of the\nGPU threads, largely negating the bene\ufb01ts of asynchronous kernel execution.\nRemoving overhead of data conversion\nWhen a Theano function is called and the data type of\nthe provided input is different from the expected one, a silent conversion is automatically done (if\nno precision would be lost). For instance, a list of integers will be converted into a vector of \ufb02oats,\nbut \ufb02oats will not be converted into integers (an error will be raised).\nThis is a useful feature, but checking and converting the input data each time the function is called\ncan be detrimental to performance. It is now possible to disable these checks and conversions,\nwhich gives better performance when the input data is actually of the correct type. If the input data\nwould actually need to be converted, then some exceptions due to unexpected data types will be\nraised during the execution. To disable these checks, simply set the trust_input attribute of a\ncompiled Theano function to True. The third blue bar on Figure 1 shows the speed up gained with\nthis optimization, including the cvm_nogc optimization.\nExecuting several iterations at once\nWhen a Theano function does not have any explicit input (all\nthe necessary values are stored in shared variables, for instance), we can save even more overhead by\ncalling its fn member: f.fn(). It is also possible to call the same function multiple consecutive\ntimes, by calling f.fn(n_calls=N), saving more time. This allows to bypass the Python loop,\nbut it will only return the results of the last iteration. This restriction means that it cannot be used\neverywhere, but it is still useful in some cases, for instance training a learning algorithm by iterating\nover a data set, where the important thing is the updates to the parameters, not the function\u2019s output.\nThe performance of this last way of calling a Theano function is shown in the right-most, dark blue\nbar.\n4.2.3\nResults\nFigure 1 shows speed results (in example per second, higher is better) on three neural network\nlearning tasks, which consists in 10-class classi\ufb01cation of a 784-dimensional input. Figure 1a shows\nsimple logistic regression, Figure 1b shows a neural network with one layer of 500 hidden units, and\nFigure 1c shows a deep neural network, with 3 layers of 1000 hidden units each. Torch7 was tested\nwith the standard Lua interpreter (pale red bars), and LuaJIT4, a Lua just-in-time compiler (darker\nred bars); Theano was tested with different optimizations (shades of blue), described in section 4.2.2.\nWhen not using mini-batches, on CPU, Theano beats Torch7 on the models with at least one hidden\nlayer, and even bene\ufb01ts from BLAS parallel implementation. On the logistic regression benchmark,\nTorch7 has the advantage, due to the small amount of computation being done in each call (executing\nseveral iterations at once help, but not enough to beat LuaJIT). Torch7 also has an edge over Theano\non the GPU, when the batch size is one.\nWhen using mini-batches, whether of size 10 or 60, Theano is faster than Torch7 on all three archi-\ntectures, or has an equivalent speed. The difference vanishes for the most computationally-intensive\ntasks, as the language and framework overhead becomes negligible.\n4.3\nBenchmarking on Recurrent Neural Networks\nIn Figure 2, we present a benchmark of a simple recurrent network, on Theano and RNNLM5, a\nC++ implementation of recurrent networks for language modeling. They were done with a batch\nsize of 1, which is customary with recurrent neural networks.\n4http://luajit.org/\n5http://www.fit.vutbr.cz/\u02dcimikolov/rnnlm/\n8\n0\n10000\n20000\n30000\n40000\n50000\n60000\n70000\n80000\n0\n5000\n10000\n15000\n20000\n25000\n30000\n35000\n0\n2000\n4000\n6000\n8000\n10000\n0\n500\n1000\n1500\n2000\n0\n10000\n20000\n30000\n40000\n50000\n60000\n0\n5000\n10000\n15000\n20000\n25000\n30000\n0\n1000\n2000\n3000\n4000\n5000\n6000\n7000\n8000\n9000\n0\n500\n1000\n1500\n2000\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\n0\n500\n1000\n1500\n2000\n2500\n3000\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600\n50 hidden units\n100 hidden units\n500 hidden units\n1500 hidden units\n10 inputs\n54 inputs\n3000 inputs\nRNNLM\nTheano (CPU - 1 thread)\nTheano (OpenMP - 4 threads)\nTheano (GPU)\nFigure 2: Benchmarks of RNNLM vs. Theano on recurrent neural networks. Reported numbers are\nsequence elements per second (bigger is better). The number of input units and output units is the\nsame as the number of input units.\nWhile RNNLM is faster than Theano on smaller models, Theano quickly catches up for bigger sizes,\nshowing that Theano is an interesting option for training recurrent neural networks in a realistic\nscenario. This is mostly due to overhead in Theano, which is a drawback of the \ufb02exibility provided\nfor recurrent models.\n5\nConclusion\nWe presented recent additions to Theano, and showed how they make it a more powerful tool for\nmachine learning software development, and allow it to be faster than competing software in most\ncases, on different benchmarks.\nThese benchmarks aim at exposing relative strengths of existing software, so that users can choose\nwhat suits their needs best. We also hope such benchmarks will help improving the available tools,\nwhich can only have a positive effect for the research community.\nAcknowledgments\nWe would like to thank the community of users and developpers of Theano for their support, NSERC\nand Canada Research Chairs for funding, and Compute Canada and Calcul Qu\u00b4ebec for computing\nresources.\nReferences\nBergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., Turian, J., Warde-Farley, D.,\nand Bengio, Y. (2010). Theano: a CPU and GPU math expression compiler. In Proceedings of the Python\nfor Scienti\ufb01c Computing Conference (SciPy). Oral Presentation.\nBergstra, J., Bastien, F., Breuleux, O., Lamblin, P., Pascanu, R., Delalleau, O., Desjardins, G., Warde-Farley,\nD., Goodfellow, I., Bergeron, A., and Bengio, Y. (2011). Theano: Deep learning on gpus with python. In\nBig Learn workshop, NIPS\u201911.\n9\nCollobert, R., Kavukcuoglu, K., and Farabet, C. (2011). Torch7: A matlab-like environment for machine\nlearning. In BigLearn, NIPS Workshop.\nHunter, J. D. (2007). Matplotlib: A 2d graphics environment. Computing in Science and Engineering, 9(3),\n90\u201395.\nJones, E., Oliphant, T., Peterson, P., et al. (2001). SciPy: Open source scienti\ufb01c tools for Python.\nMartens, J. and Sutskever, I. (2011). Learning recurrent neural networks with hessian-free optimization. In\nL. Getoor and T. Scheffer, editors, Proceedings of the 28th International Conference on Machine Learning\n(ICML-11), ICML \u201911, pages 1033\u20131040, New York, NY, USA. ACM.\nMikolov, T., Deoras, A., Kombrink, S., Burget, L., and Cernocky, J. (2011). Empirical evaluation and com-\nbination of advanced language modeling techniques. In Proc. 12th annual conference of the international\nspeech communication association (INTERSPEECH 2011).\nOliphant, T. E. (2007). Python for scienti\ufb01c computing. Computing in Science and Engineering, 9, 10\u201320.\nPearlmutter, B. A. (1994). Fast exact multiplication by the hessian. Neural Computation, 6, 147\u2013160.\nP\u00b4erez, F. and Granger, B. E. (2007). IPython: A system for interactive scienti\ufb01c computing. Computing in\nScience and Engineering, 9(3), 21\u201329.\nRumelhart, D. E., Hinton, G. E., and Williams, R. J. (1986). Learning internal representations by error propa-\ngation. volume 1, chapter 8, pages 318\u2013362. MIT Press, Cambridge.\n10\n",
        "sentence": " The model was implemented with Theano (Bastien et al., 2012) and trained on a machine equipped with two Nvidia Tesla K80 GPUs.",
        "context": "Bergstra, J., Bastien, F., Breuleux, O., Lamblin, P., Pascanu, R., Delalleau, O., Desjardins, G., Warde-Farley,\nD., Goodfellow, I., Bergeron, A., and Bengio, Y. (2011). Theano: Deep learning on gpus with python. In\nBig Learn workshop, NIPS\u201911.\n9\nand Bengio, Y. (2010). Theano: a CPU and GPU math expression compiler. In Proceedings of the Python\nfor Scienti\ufb01c Computing Conference (SciPy). Oral Presentation.\n2.80GHz, and a nVidia GTX480 GPU. The commit ID of Theano\u2019s version was 254894fac,\nTorch7 was 41d3b8b93.\nAs the multi-layer perceptron (MLP) examples in the benchmarks rely on function calls to a BLAS"
    },
    {
        "title": "A simple algorithm for identifying negated findings and diseases in discharge summaries",
        "author": [
            "Chapman",
            "Wendy W",
            "Bridewell",
            "Will",
            "Hanbury",
            "Paul",
            "Cooper",
            "Gregory F",
            "Buchanan",
            "Bruce G"
        ],
        "venue": "Journal of biomedical informatics,",
        "citeRegEx": "Chapman et al\\.,? \\Q2001\\E",
        "shortCiteRegEx": "Chapman et al\\.",
        "year": 2001,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " (Heckerman, 1990; Chapman et al., 2001; Lange et al., 2015), most works do not achieve the required accuracy and scalability, lack generality, or need excessive expert domain knowledge.",
        "context": null
    },
    {
        "title": "Constructing disease network and temporal progression model via context-sensitive hawkes process",
        "author": [
            "Choi",
            "Edward",
            "Du",
            "Nan",
            "Chen",
            "Robert",
            "Song",
            "Le",
            "Sun",
            "Jimeng"
        ],
        "venue": "In ICDM,",
        "citeRegEx": "Choi et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "Choi et al\\.",
        "year": 2015,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " , 2015; Johnson & Willsky, 2013), and intensity based point process modeling techniques such as Hawkes processes (Liniger, 2009; Zhu, 2013; Choi et al., 2015) are expensive to generalize to nonlinear and multilabel settings. , 2007; Johnson & Willsky, 2013; Lange, 2014) and intensity function modeling techniques such as Cox and Hawkes processes (Liniger, 2009; Zhou et al., 2013; Linderman & Adams, 2014; Choi et al., 2015).",
        "context": null
    },
    {
        "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
        "author": [
            "Chung",
            "Junyoung",
            "Gulcehre",
            "Caglar",
            "Cho",
            "KyungHyun",
            "Bengio",
            "Yoshua"
        ],
        "venue": "arXiv preprint arXiv:1412.3555,",
        "citeRegEx": "Chung et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Chung et al\\.",
        "year": 2014,
        "abstract": "In this paper we compare different types of recurrent units in recurrent\nneural networks (RNNs). Especially, we focus on more sophisticated units that\nimplement a gating mechanism, such as a long short-term memory (LSTM) unit and\na recently proposed gated recurrent unit (GRU). We evaluate these recurrent\nunits on the tasks of polyphonic music modeling and speech signal modeling. Our\nexperiments revealed that these advanced recurrent units are indeed better than\nmore traditional recurrent units such as tanh units. Also, we found GRU to be\ncomparable to LSTM.",
        "full_text": "Empirical Evaluation of\nGated Recurrent Neural Networks\non Sequence Modeling\nJunyoung Chung\nCaglar Gulcehre\nKyungHyun Cho\nUniversit\u00b4e de Montr\u00b4eal\nYoshua Bengio\nUniversit\u00b4e de Montr\u00b4eal\nCIFAR Senior Fellow\nAbstract\nIn this paper we compare different types of recurrent units in recurrent neural net-\nworks (RNNs). Especially, we focus on more sophisticated units that implement\na gating mechanism, such as a long short-term memory (LSTM) unit and a re-\ncently proposed gated recurrent unit (GRU). We evaluate these recurrent units on\nthe tasks of polyphonic music modeling and speech signal modeling. Our exper-\niments revealed that these advanced recurrent units are indeed better than more\ntraditional recurrent units such as tanh units. Also, we found GRU to be compa-\nrable to LSTM.\n1\nIntroduction\nRecurrent neural networks have recently shown promising results in many machine learning tasks,\nespecially when input and/or output are of variable length [see, e.g., Graves, 2012]. More recently,\nSutskever et al. [2014] and Bahdanau et al. [2014] reported that recurrent neural networks are able to\nperform as well as the existing, well-developed systems on a challenging task of machine translation.\nOne interesting observation, we make from these recent successes is that almost none of these suc-\ncesses were achieved with a vanilla recurrent neural network. Rather, it was a recurrent neural net-\nwork with sophisticated recurrent hidden units, such as long short-term memory units [Hochreiter\nand Schmidhuber, 1997], that was used in those successful applications.\nAmong those sophisticated recurrent units, in this paper, we are interested in evaluating two closely\nrelated variants. One is a long short-term memory (LSTM) unit, and the other is a gated recurrent\nunit (GRU) proposed more recently by Cho et al. [2014]. It is well established in the \ufb01eld that the\nLSTM unit works well on sequence-based tasks with long-term dependencies, but the latter has only\nrecently been introduced and used in the context of machine translation.\nIn this paper, we evaluate these two units and a more traditional tanh unit on the task of sequence\nmodeling. We consider three polyphonic music datasets [see, e.g., Boulanger-Lewandowski et al.,\n2012] as well as two internal datasets provided by Ubisoft in which each sample is a raw speech\nrepresentation.\nBased on our experiments, we concluded that by using \ufb01xed number of parameters for all models\non some datasets GRU, can outperform LSTM units both in terms of convergence in CPU time and\nin terms of parameter updates and generalization.\n2\nBackground: Recurrent Neural Network\nA recurrent neural network (RNN) is an extension of a conventional feedforward neural network,\nwhich is able to handle a variable-length sequence input. The RNN handles the variable-length\n1\narXiv:1412.3555v1  [cs.NE]  11 Dec 2014\nsequence by having a recurrent hidden state whose activation at each time is dependent on that of\nthe previous time.\nMore formally, given a sequence x = (x1, x2, \u00b7 \u00b7 \u00b7 , xT), the RNN updates its recurrent hidden state\nht by\nht =\n\u001a0,\nt = 0\n\u03c6 (ht\u22121, xt) ,\notherwise\n(1)\nwhere \u03c6 is a nonlinear function such as composition of a logistic sigmoid with an af\ufb01ne transforma-\ntion. Optionally, the RNN may have an output y = (y1, y2, . . . , yT) which may again be of variable\nlength.\nTraditionally, the update of the recurrent hidden state in Eq. (1) is implemented as\nht = g (Wxt + Uht\u22121) ,\n(2)\nwhere g is a smooth, bounded function such as a logistic sigmoid function or a hyperbolic tangent\nfunction.\nA generative RNN outputs a probability distribution over the next element of the sequence, given\nits current state ht, and this generative model can capture a distribution over sequences of vari-\nable length by using a special output symbol to represent the end of the sequence. The sequence\nprobability can be decomposed into\np(x1, . . . , xT) = p(x1)p(x2 | x1)p(x3 | x1, x2) \u00b7 \u00b7 \u00b7 p(xT | x1, . . . , xT \u22121),\n(3)\nwhere the last element is a special end-of-sequence value. We model each conditional probability\ndistribution with\np(xt | x1, . . . , xt\u22121) =g(ht),\nwhere ht is from Eq. (1). Such generative RNNs are the subject of this paper.\nUnfortunately, it has been observed by, e.g., Bengio et al. [1994] that it is dif\ufb01cult to train RNNs\nto capture long-term dependencies because the gradients tend to either vanish (most of the time) or\nexplode (rarely, but with severe effects). This makes gradient-based optimization method struggle,\nnot just because of the variations in gradient magnitudes but because of the effect of long-term\ndependencies is hidden (being exponentially smaller with respect to sequence length) by the effect\nof short-term dependencies. There have been two dominant approaches by which many researchers\nhave tried to reduce the negative impacts of this issue. One such approach is to devise a better\nlearning algorithm than a simple stochastic gradient descent [see, e.g., Bengio et al., 2013, Pascanu\net al., 2013, Martens and Sutskever, 2011], for example using the very simple clipped gradient, by\nwhich the norm of the gradient vector is clipped, or using second-order methods which may be less\nsensitive to the issue if the second derivatives follow the same growth pattern as the \ufb01rst derivatives\n(which is not guaranteed to be the case).\nThe other approach, in which we are more interested in this paper, is to design a more sophisticated\nactivation function than a usual activation function, consisting of af\ufb01ne transformation followed\nby a simple element-wise nonlinearity by using gating units. The earliest attempt in this direction\nresulted in an activation function, or a recurrent unit, called a long short-term memory (LSTM)\nunit [Hochreiter and Schmidhuber, 1997]. More recently, another type of recurrent unit, to which\nwe refer as a gated recurrent unit (GRU), was proposed by Cho et al. [2014]. RNNs employing either\nof these recurrent units have been shown to perform well in tasks that require capturing long-term\ndependencies. Those tasks include, but are not limited to, speech recognition [see, e.g., Graves et al.,\n2013] and machine translation [see, e.g., Sutskever et al., 2014, Bahdanau et al., 2014].\n2\nf\nc\nc~\n+\n+\no\ni\nIN\nOUT\nz\nr\nh\nh~\nIN\nOUT\n(a) Long Short-Term Memory\n(b) Gated Recurrent Unit\nFigure 1: Illustration of (a) LSTM and (b) gated recurrent units. (a) i, f and o are the input, forget\nand output gates, respectively. c and \u02dcc denote the memory cell and the new memory cell content. (b)\nr and z are the reset and update gates, and h and \u02dch are the activation and the candidate activation.\n3\nGated Recurrent Neural Networks\nIn this paper, we are interested in evaluating the performance of those recently proposed recurrent\nunits (LSTM unit and GRU) on sequence modeling. Before the empirical evaluation, we \ufb01rst de-\nscribe each of those recurrent units in this section.\n3.1\nLong Short-Term Memory Unit\nThe Long Short-Term Memory (LSTM) unit was initially proposed by Hochreiter and Schmidhuber\n[1997]. Since then, a number of minor modi\ufb01cations to the original LSTM unit have been made.\nWe follow the implementation of LSTM as used in Graves [2013].\nUnlike to the recurrent unit which simply computes a weighted sum of the input signal and applies\na nonlinear function, each j-th LSTM unit maintains a memory cj\nt at time t. The output hj\nt, or the\nactivation, of the LSTM unit is then\nhj\nt = oj\nt tanh\n\u0010\ncj\nt\n\u0011\n,\nwhere oj\nt is an output gate that modulates the amount of memory content exposure. The output gate\nis computed by\noj\nt = \u03c3 (Woxt + Uoht\u22121 + Voct)j ,\nwhere \u03c3 is a logistic sigmoid function. Vo is a diagonal matrix.\nThe memory cell cj\nt is updated by partially forgetting the existing memory and adding a new memory\ncontent \u02dccj\nt :\ncj\nt = f j\nt cj\nt\u22121 + ij\nt\u02dccj\nt,\n(4)\nwhere the new memory content is\n\u02dccj\nt = tanh (Wcxt + Ucht\u22121)j .\nThe extent to which the existing memory is forgotten is modulated by a forget gate f j\nt , and the\ndegree to which the new memory content is added to the memory cell is modulated by an input gate\nij\nt. Gates are computed by\nf j\nt =\u03c3 (Wfxt + Ufht\u22121 + Vfct\u22121)j ,\nij\nt =\u03c3 (Wixt + Uiht\u22121 + Vict\u22121)j .\nNote that Vf and Vi are diagonal matrices.\n3\nUnlike to the traditional recurrent unit which overwrites its content at each time-step (see Eq. (2)),\nan LSTM unit is able to decide whether to keep the existing memory via the introduced gates.\nIntuitively, if the LSTM unit detects an important feature from an input sequence at early stage, it\neasily carries this information (the existence of the feature) over a long distance, hence, capturing\npotential long-distance dependencies.\nSee Fig. 1 (a) for the graphical illustration.\n3.2\nGated Recurrent Unit\nA gated recurrent unit (GRU) was proposed by Cho et al. [2014] to make each recurrent unit to\nadaptively capture dependencies of different time scales. Similarly to the LSTM unit, the GRU has\ngating units that modulate the \ufb02ow of information inside the unit, however, without having a separate\nmemory cells.\nThe activation hj\nt of the GRU at time t is a linear interpolation between the previous activation hj\nt\u22121\nand the candidate activation \u02dchj\nt:\nhj\nt = (1 \u2212zj\nt )hj\nt\u22121 + zj\nt \u02dchj\nt,\n(5)\nwhere an update gate zj\nt decides how much the unit updates its activation, or content. The update\ngate is computed by\nzj\nt = \u03c3 (Wzxt + Uzht\u22121)j .\nThis procedure of taking a linear sum between the existing state and the newly computed state is\nsimilar to the LSTM unit. The GRU, however, does not have any mechanism to control the degree\nto which its state is exposed, but exposes the whole state each time.\nThe candidate activation \u02dchj\nt is computed similarly to that of the traditional recurrent unit (see Eq. (2))\nand as in [Bahdanau et al., 2014],\n\u02dchj\nt = tanh (Wxt + U (rt \u2299ht\u22121))j ,\nwhere rt is a set of reset gates and \u2299is an element-wise multiplication. 1 When off (rj\nt close to 0),\nthe reset gate effectively makes the unit act as if it is reading the \ufb01rst symbol of an input sequence,\nallowing it to forget the previously computed state.\nThe reset gate rj\nt is computed similarly to the update gate:\nrj\nt = \u03c3 (Wrxt + Urht\u22121)j .\nSee Fig. 1 (b) for the graphical illustration of the GRU.\n3.3\nDiscussion\nIt is easy to notice similarities between the LSTM unit and the GRU from Fig. 1.\nThe most prominent feature shared between these units is the additive component of their update\nfrom t to t + 1, which is lacking in the traditional recurrent unit. The traditional recurrent unit\nalways replaces the activation, or the content of a unit with a new value computed from the current\ninput and the previous hidden state. On the other hand, both LSTM unit and GRU keep the existing\ncontent and add the new content on top of it (see Eqs. (4) and (5)).\n1 Note that we use the reset gate in a slightly different way from the original GRU proposed in Cho et al.\n[2014]. Originally, the candidate activation was computed by\n\u02dchj\nt = tanh (Wxt + rt \u2299(Uht\u22121))j ,\nwhere rj\nt is a reset gate. We found in our preliminary experiments that both of these formulations performed\nas well as each other.\n4\nThis additive nature has two advantages. First, it is easy for each unit to remember the existence of\na speci\ufb01c feature in the input stream for a long series of steps. Any important feature, decided by\neither the forget gate of the LSTM unit or the update gate of the GRU, will not be overwritten but\nbe maintained as it is.\nSecond, and perhaps more importantly, this addition effectively creates shortcut paths that bypass\nmultiple temporal steps. These shortcuts allow the error to be back-propagated easily without too\nquickly vanishing (if the gating unit is nearly saturated at 1) as a result of passing through multiple,\nbounded nonlinearities, thus reducing the dif\ufb01culty due to vanishing gradients [Hochreiter, 1991,\nBengio et al., 1994].\nThese two units however have a number of differences as well. One feature of the LSTM unit that\nis missing from the GRU is the controlled exposure of the memory content. In the LSTM unit, the\namount of the memory content that is seen, or used by other units in the network is controlled by the\noutput gate. On the other hand the GRU exposes its full content without any control.\nAnother difference is in the location of the input gate, or the corresponding reset gate. The LSTM\nunit computes the new memory content without any separate control of the amount of information\n\ufb02owing from the previous time step. Rather, the LSTM unit controls the amount of the new memory\ncontent being added to the memory cell independently from the forget gate. On the other hand, the\nGRU controls the information \ufb02ow from the previous activation when computing the new, candidate\nactivation, but does not independently control the amount of the candidate activation being added\n(the control is tied via the update gate).\nFrom these similarities and differences alone, it is dif\ufb01cult to conclude which types of gating units\nwould perform better in general. Although Bahdanau et al. [2014] reported that these two units per-\nformed comparably to each other according to their preliminary experiments on machine translation,\nit is unclear whether this applies as well to tasks other than machine translation. This motivates us\nto conduct more thorough empirical comparison between the LSTM unit and the GRU in this paper.\n4\nExperiments Setting\n4.1\nTasks and Datasets\nWe compare the LSTM unit, GRU and tanh unit in the task of sequence modeling. Sequence\nmodeling aims at learning a probability distribution over sequences, as in Eq. (3), by maximizing\nthe log-likelihood of a model given a set of training sequences:\nmax\n\u03b8\n1\nN\nN\nX\nn=1\nTn\nX\nt=1\nlog p\n\u0000xn\nt | xn\n1, . . . , xn\nt\u22121; \u03b8\n\u0001\n,\nwhere \u03b8 is a set of model parameters. More speci\ufb01cally, we evaluate these units in the tasks of\npolyphonic music modeling and speech signal modeling.\nFor the polyphonic music modeling, we use three polyphonic music datasets from [Boulanger-\nLewandowski et al., 2012]: Nottingham, JSB Chorales, MuseData and Piano-midi. These datasets\ncontain sequences of which each symbol is respectively a 93-, 96-, 105-, and 108-dimensional binary\nvector. We use logistic sigmoid function as output units.\nWe use two internal datasets provided by Ubisoft2 for speech signal modeling. Each sequence is an\none-dimensional raw audio signal, and at each time step, we design a recurrent neural network to\nlook at 20 consecutive samples to predict the following 10 consecutive samples. We have used two\ndifferent versions of the dataset: One with sequences of length 500 (Ubisoft A) and the other with\nsequences of length 8, 000 (Ubisoft B). Ubisoft A and Ubisoft B have 7, 230 and 800 sequences\neach. We use mixture of Gaussians with 20 components as output layer. 3\n2 http://www.ubi.com/\n3Our implementation is available at https://github.com/jych/librnn.git\n5\n4.2\nModels\nFor each task, we train three different recurrent neural networks, each having either LSTM units\n(LSTM-RNN, see Sec. 3.1), GRUs (GRU-RNN, see Sec. 3.2) or tanh units (tanh-RNN, see Eq. (2)).\nAs the primary objective of these experiments is to compare all three units fairly, we choose the size\nof each model so that each model has approximately the same number of parameters. We intention-\nally made the models to be small enough in order to avoid over\ufb01tting which can easily distract the\ncomparison. This approach of comparing different types of hidden units in neural networks has been\ndone before, for instance, by Gulcehre et al. [2014]. See Table 1 for the details of the model sizes.\nUnit\n# of Units\n# of Parameters\nPolyphonic music modeling\nLSTM\n36\n\u224819.8 \u00d7 103\nGRU\n46\n\u224820.2 \u00d7 103\ntanh\n100\n\u224820.1 \u00d7 103\nSpeech signal modeling\nLSTM\n195\n\u2248169.1 \u00d7 103\nGRU\n227\n\u2248168.9 \u00d7 103\ntanh\n400\n\u2248168.4 \u00d7 103\nTable 1: The sizes of the models tested in the experiments.\ntanh\nGRU\nLSTM\nMusic Datasets\nNottingham\ntrain\ntest\n3.22\n3.13\n2.79\n3.23\n3.08\n3.20\nJSB Chorales\ntrain\ntest\n8.82\n9.10\n6.94\n8.54\n8.15\n8.67\nMuseData\ntrain\ntest\n5.64\n6.23\n5.06\n5.99\n5.18\n6.23\nPiano-midi\ntrain\ntest\n5.64\n9.03\n4.93\n8.82\n6.49\n9.03\nUbisoft Datasets\nUbisoft dataset A\ntrain\ntest\n6.29\n6.44\n2.31\n3.59\n1.44\n2.70\nUbisoft dataset B\ntrain\ntest\n7.61\n7.62\n0.38\n0.88\n0.80\n1.26\nTable 2: The average negative log-probabilities of the training and test sets.\nWe train each model with RMSProp [see, e.g., Hinton, 2012] and use weight noise with standard\ndeviation \ufb01xed to 0.075 [Graves, 2011]. At every update, we rescale the norm of the gradient to 1,\nif it is larger than 1 [Pascanu et al., 2013] to prevent exploding gradients. We select a learning rate\n(scalar multiplier in RMSProp) to maximize the validation performance, out of 10 randomly chosen\nlog-uniform candidates sampled from U(\u221212, \u22126) [Bergstra and Bengio, 2012]. The validation set\nis used for early-stop training as well.\n5\nResults and Analysis\nTable 2 lists all the results from our experiments. In the case of the polyphonic music datasets, the\nGRU-RNN outperformed all the others (LSTM-RNN and tanh-RNN) on all the datasets except for\nthe Nottingham. However, we can see that on these music datasets, all the three models performed\nclosely to each other.\nOn the other hand, the RNNs with the gating units (GRU-RNN and LSTM-RNN) clearly outper-\nformed the more traditional tanh-RNN on both of the Ubisoft datasets. The LSTM-RNN was best\nwith the Ubisoft A, and with the Ubisoft B, the GRU-RNN performed best.\nIn Figs. 2\u20133, we show the learning curves of the best validation runs. In the case of the music\ndatasets (Fig. 2), we see that the GRU-RNN makes faster progress in terms of both the number of\n6\nupdates and actual CPU time. If we consider the Ubisoft datasets (Fig. 3), it is clear that although the\ncomputational requirement for each update in the tanh-RNN is much smaller than the other models,\nit did not make much progress each update and eventually stopped making any progress at much\nworse level.\nThese results clearly indicate the advantages of the gating units over the more traditional recurrent\nunits. Convergence is often faster, and the \ufb01nal solutions tend to be better. However, our results are\nnot conclusive in comparing the LSTM and the GRU, which suggests that the choice of the type of\ngated recurrent unit may depend heavily on the dataset and corresponding task.\nPer epoch\nWall Clock Time (seconds)\n(a) Nottingham Dataset\n(b) MuseData Dataset\nFigure 2: Learning curves for training and validation sets of different types of units with respect to\n(top) the number of iterations and (bottom) the wall clock time. y-axis corresponds to the negative-\nlog likelihood of the model shown in log-scale.\n6\nConclusion\nIn this paper we empirically evaluated recurrent neural networks (RNN) with three widely used\nrecurrent units; (1) a traditional tanh unit, (2) a long short-term memory (LSTM) unit and (3)\na recently proposed gated recurrent unit (GRU). Our evaluation focused on the task of sequence\nmodeling on a number of datasets including polyphonic music data and raw speech signal data.\nThe evaluation clearly demonstrated the superiority of the gated units; both the LSTM unit and GRU,\nover the traditional tanh unit. This was more evident with the more challenging task of raw speech\nsignal modeling. However, we could not make concrete conclusion on which of the two gating units\nwas better.\n7\nPer epoch\nWall Clock Time (seconds)\n(a) Ubisoft Dataset A\n(b) Ubisoft Dataset B\nFigure 3: Learning curves for training and validation sets of different types of units with respect to\n(top) the number of iterations and (bottom) the wall clock time. x-axis is the number of epochs and\ny-axis corresponds to the negative-log likelihood of the model shown in log-scale.\nWe consider the experiments in this paper as preliminary. In order to understand better how a gated\nunit helps learning and to separate out the contribution of each component, for instance gating units\nin the LSTM unit or the GRU, of the gating units, more thorough experiments will be required in\nthe future.\nAcknowledgments\nThe authors would like to thank Ubisoft for providing the datasets and for the support. The au-\nthors would like to thank the developers of Theano [Bergstra et al., 2010, Bastien et al., 2012] and\nPylearn2 [Goodfellow et al., 2013]. We acknowledge the support of the following agencies for\nresearch funding and computing support: NSERC, Calcul Qu\u00b4ebec, Compute Canada, the Canada\nResearch Chairs and CIFAR.\n8\nReferences\nD. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and\ntranslate. Technical report, arXiv preprint arXiv:1409.0473, 2014.\nF. Bastien, P. Lamblin, R. Pascanu, J. Bergstra, I. J. Goodfellow, A. Bergeron, N. Bouchard, and\nY. Bengio. Theano: new features and speed improvements. Deep Learning and Unsupervised\nFeature Learning NIPS 2012 Workshop, 2012.\nY. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is\ndif\ufb01cult. IEEE Transactions on Neural Networks, 5(2):157\u2013166, 1994.\nY. Bengio, N. Boulanger-Lewandowski, and R. Pascanu. Advances in optimizing recurrent net-\nworks. In Proc. ICASSP 38, 2013.\nJ. Bergstra and Y. Bengio. Random search for hyper-parameter optimization. The Journal of Ma-\nchine Learning Research, 13(1):281\u2013305, 2012.\nJ. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu, G. Desjardins, J. Turian, D. Warde-\nFarley, and Y. Bengio. Theano: a CPU and GPU math expression compiler. In Proceedings of the\nPython for Scienti\ufb01c Computing Conference (SciPy), June 2010. Oral Presentation.\nN. Boulanger-Lewandowski, Y. Bengio, and P. Vincent. Modeling temporal dependencies in high-\ndimensional sequences: Application to polyphonic music generation and transcription. In Pro-\nceedings of the Twenty-nine International Conference on Machine Learning (ICML\u201912). ACM,\n2012. URL http://icml.cc/discuss/2012/590.html.\nK. Cho, B. van Merrienboer, D. Bahdanau, and Y. Bengio. On the properties of neural machine\ntranslation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259, 2014.\nI. J. Goodfellow, D. Warde-Farley, P. Lamblin, V. Dumoulin, M. Mirza, R. Pascanu, J. Bergstra,\nF. Bastien, and Y. Bengio.\nPylearn2: a machine learning research library.\narXiv preprint\narXiv:1308.4214, 2013.\nA. Graves. Supervised Sequence Labelling with Recurrent Neural Networks. Studies in Computa-\ntional Intelligence. Springer, 2012.\nA. Graves. Practical variational inference for neural networks. In Advances in Neural Information\nProcessing Systems, pages 2348\u20132356, 2011.\nA. Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850,\n2013.\nA. Graves, A.-r. Mohamed, and G. Hinton. Speech recognition with deep recurrent neural networks.\nIn ICASSP\u20192013, pages 6645\u20136649. IEEE, 2013.\nC. Gulcehre, K. Cho, R. Pascanu, and Y. Bengio. Learned-norm pooling for deep feedforward and\nrecurrent neural networks. In Machine Learning and Knowledge Discovery in Databases, pages\n530\u2013546. Springer, 2014.\nG. Hinton. Neural networks for machine learning. Coursera, video lectures, 2012.\nS. Hochreiter.\nUntersuchungen zu dynamischen neuronalen Netzen. Diploma thesis, Institut f\u00a8ur\nInformatik, Lehrstuhl Prof. Brauer, Technische Universit\u00a8at M\u00a8unchen, 1991.\nURL http://\nwww7.informatik.tu-muenchen.de/\u02dcEhochreit.\nS. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735\u20131780,\n1997.\nJ. Martens and I. Sutskever. Learning recurrent neural networks with Hessian-free optimization. In\nProc. ICML\u20192011. ACM, 2011.\nR. Pascanu, T. Mikolov, and Y. Bengio. On the dif\ufb01culty of training recurrent neural networks. In\nProceedings of the 30th International Conference on Machine Learning (ICML\u201913). ACM, 2013.\nURL http://icml.cc/2013/.\nI. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence learning with neural networks. Tech-\nnical report, arXiv preprint arXiv:1409.3215, 2014.\n9\n",
        "sentence": " , 2009) or Gated Recurrent Units (GRU) (Chung et al., 2014). Although LSTM has drawn much attention from many researchers, GRU has recently shown to have similar performance as LSTM, while employing a simpler architecture (Chung et al., 2014).",
        "context": "3.2\nGated Recurrent Unit\nA gated recurrent unit (GRU) was proposed by Cho et al. [2014] to make each recurrent unit to\nadaptively capture dependencies of different time scales. Similarly to the LSTM unit, the GRU has\nwe refer as a gated recurrent unit (GRU), was proposed by Cho et al. [2014]. RNNs employing either\nof these recurrent units have been shown to perform well in tasks that require capturing long-term\niments revealed that these advanced recurrent units are indeed better than more\ntraditional recurrent units such as tanh units. Also, we found GRU to be compa-\nrable to LSTM.\n1\nIntroduction"
    },
    {
        "title": "A semi-markov model for multistate and interval-censored data with multiple terminal events. application in renal transplantation",
        "author": [
            "Foucher",
            "Yohann",
            "Giral",
            "Magali",
            "Soulillou",
            "Jean-Paul",
            "Daures",
            "Jean-Pierre"
        ],
        "venue": "Statistics in medicine,",
        "citeRegEx": "Foucher et al\\.,? \\Q2007\\E",
        "shortCiteRegEx": "Foucher et al\\.",
        "year": 2007,
        "abstract": "",
        "full_text": "",
        "sentence": " Among the continuous-time models, there are two main techniques: continuoustime Markov chain based models (Nodelman et al., 2002; Foucher et al., 2007; Johnson & Willsky, 2013; Lange, 2014) and intensity function modeling techniques such as Cox and Hawkes processes (Liniger, 2009; Zhou et al.",
        "context": null
    },
    {
        "title": "Generating sequences with recurrent neural networks",
        "author": [
            "Graves",
            "Alex"
        ],
        "venue": "arXiv preprint arXiv:1308.0850,",
        "citeRegEx": "Graves and Alex.,? \\Q2013\\E",
        "shortCiteRegEx": "Graves and Alex.",
        "year": 2013,
        "abstract": "This paper shows how Long Short-term Memory recurrent neural networks can be\nused to generate complex sequences with long-range structure, simply by\npredicting one data point at a time. The approach is demonstrated for text\n(where the data are discrete) and online handwriting (where the data are\nreal-valued). It is then extended to handwriting synthesis by allowing the\nnetwork to condition its predictions on a text sequence. The resulting system\nis able to generate highly realistic cursive handwriting in a wide variety of\nstyles.",
        "full_text": "Generating Sequences With\nRecurrent Neural Networks\nAlex Graves\nDepartment of Computer Science\nUniversity of Toronto\ngraves@cs.toronto.edu\nAbstract\nThis paper shows how Long Short-term Memory recurrent neural net-\nworks can be used to generate complex sequences with long-range struc-\nture, simply by predicting one data point at a time.\nThe approach is\ndemonstrated for text (where the data are discrete) and online handwrit-\ning (where the data are real-valued). It is then extended to handwriting\nsynthesis by allowing the network to condition its predictions on a text\nsequence. The resulting system is able to generate highly realistic cursive\nhandwriting in a wide variety of styles.\n1\nIntroduction\nRecurrent neural networks (RNNs) are a rich class of dynamic models that have\nbeen used to generate sequences in domains as diverse as music [6, 4], text [30]\nand motion capture data [29]. RNNs can be trained for sequence generation by\nprocessing real data sequences one step at a time and predicting what comes\nnext. Assuming the predictions are probabilistic, novel sequences can be gener-\nated from a trained network by iteratively sampling from the network\u2019s output\ndistribution, then feeding in the sample as input at the next step. In other\nwords by making the network treat its inventions as if they were real, much like\na person dreaming. Although the network itself is deterministic, the stochas-\nticity injected by picking samples induces a distribution over sequences. This\ndistribution is conditional, since the internal state of the network, and hence its\npredictive distribution, depends on the previous inputs.\nRNNs are \u2018fuzzy\u2019 in the sense that they do not use exact templates from\nthe training data to make predictions, but rather\u2014like other neural networks\u2014\nuse their internal representation to perform a high-dimensional interpolation\nbetween training examples. This distinguishes them from n-gram models and\ncompression algorithms such as Prediction by Partial Matching [5], whose pre-\ndictive distributions are determined by counting exact matches between the\nrecent history and the training set. The result\u2014which is immediately appar-\n1\narXiv:1308.0850v5  [cs.NE]  5 Jun 2014\nent from the samples in this paper\u2014is that RNNs (unlike template-based al-\ngorithms) synthesise and reconstitute the training data in a complex way, and\nrarely generate the same thing twice. Furthermore, fuzzy predictions do not suf-\nfer from the curse of dimensionality, and are therefore much better at modelling\nreal-valued or multivariate data than exact matches.\nIn principle a large enough RNN should be su\ufb03cient to generate sequences\nof arbitrary complexity.\nIn practice however, standard RNNs are unable to\nstore information about past inputs for very long [15]. As well as diminishing\ntheir ability to model long-range structure, this \u2018amnesia\u2019 makes them prone to\ninstability when generating sequences. The problem (common to all conditional\ngenerative models) is that if the network\u2019s predictions are only based on the last\nfew inputs, and these inputs were themselves predicted by the network, it has\nlittle opportunity to recover from past mistakes. Having a longer memory has\na stabilising e\ufb00ect, because even if the network cannot make sense of its recent\nhistory, it can look further back in the past to formulate its predictions. The\nproblem of instability is especially acute with real-valued data, where it is easy\nfor the predictions to stray from the manifold on which the training data lies.\nOne remedy that has been proposed for conditional models is to inject noise into\nthe predictions before feeding them back into the model [31], thereby increasing\nthe model\u2019s robustness to surprising inputs. However we believe that a better\nmemory is a more profound and e\ufb00ective solution.\nLong Short-term Memory (LSTM) [16] is an RNN architecture designed to\nbe better at storing and accessing information than standard RNNs. LSTM has\nrecently given state-of-the-art results in a variety of sequence processing tasks,\nincluding speech and handwriting recognition [10, 12]. The main goal of this\npaper is to demonstrate that LSTM can use its memory to generate complex,\nrealistic sequences containing long-range structure.\nSection 2 de\ufb01nes a \u2018deep\u2019 RNN composed of stacked LSTM layers, and ex-\nplains how it can be trained for next-step prediction and hence sequence gener-\nation. Section 3 applies the prediction network to text from the Penn Treebank\nand Hutter Prize Wikipedia datasets. The network\u2019s performance is compet-\nitive with state-of-the-art language models, and it works almost as well when\npredicting one character at a time as when predicting one word at a time. The\nhighlight of the section is a generated sample of Wikipedia text, which showcases\nthe network\u2019s ability to model long-range dependencies. Section 4 demonstrates\nhow the prediction network can be applied to real-valued data through the use\nof a mixture density output layer, and provides experimental results on the IAM\nOnline Handwriting Database. It also presents generated handwriting samples\nproving the network\u2019s ability to learn letters and short words direct from pen\ntraces, and to model global features of handwriting style. Section 5 introduces\nan extension to the prediction network that allows it to condition its outputs on\na short annotation sequence whose alignment with the predictions is unknown.\nThis makes it suitable for handwriting synthesis, where a human user inputs\na text and the algorithm generates a handwritten version of it. The synthesis\nnetwork is trained on the IAM database, then used to generate cursive hand-\nwriting samples, some of which cannot be distinguished from real data by the\n2\nFigure 1: Deep recurrent neural network prediction architecture. The\ncircles represent network layers, the solid lines represent weighted connections\nand the dashed lines represent predictions.\nnaked eye. A method for biasing the samples towards higher probability (and\ngreater legibility) is described, along with a technique for \u2018priming\u2019 the sam-\nples on real data and thereby mimicking a particular writer\u2019s style. Finally,\nconcluding remarks and directions for future work are given in Section 6.\n2\nPrediction Network\nFig. 1 illustrates the basic recurrent neural network prediction architecture used\nin this paper. An input vector sequence x = (x1, . . . , xT ) is passed through\nweighted connections to a stack of N recurrently connected hidden layers to\ncompute \ufb01rst the hidden vector sequences hn = (hn\n1, . . . , hn\nT ) and then the\noutput vector sequence y = (y1, . . . , yT ).\nEach output vector yt is used to\nparameterise a predictive distribution Pr(xt+1|yt) over the possible next inputs\nxt+1. The \ufb01rst element x1 of every input sequence is always a null vector whose\nentries are all zero; the network therefore emits a prediction for x2, the \ufb01rst\nreal input, with no prior information.\nThe network is \u2018deep\u2019 in both space\nand time, in the sense that every piece of information passing either vertically\nor horizontally through the computation graph will be acted on by multiple\nsuccessive weight matrices and nonlinearities.\nNote the \u2018skip connections\u2019 from the inputs to all hidden layers, and from\nall hidden layers to the outputs. These make it easier to train deep networks,\n3\nby reducing the number of processing steps between the bottom of the network\nand the top, and thereby mitigating the \u2018vanishing gradient\u2019 problem [1]. In\nthe special case that N = 1 the architecture reduces to an ordinary, single layer\nnext step prediction RNN.\nThe hidden layer activations are computed by iterating the following equa-\ntions from t = 1 to T and from n = 2 to N:\nh1\nt = H\n\u0000Wih1xt + Wh1h1h1\nt\u22121 + b1\nh\n\u0001\n(1)\nhn\nt = H\n\u0000Wihnxt + Whn\u22121hnhn\u22121\nt\n+ Whnhnhn\nt\u22121 + bn\nh\n\u0001\n(2)\nwhere the W terms denote weight matrices (e.g. Wihn is the weight matrix\nconnecting the inputs to the nth hidden layer, Wh1h1 is the recurrent connection\nat the \ufb01rst hidden layer, and so on), the b terms denote bias vectors (e.g. by is\noutput bias vector) and H is the hidden layer function.\nGiven the hidden sequences, the output sequence is computed as follows:\n\u02c6yt = by +\nN\nX\nn=1\nWhnyhn\nt\n(3)\nyt = Y(\u02c6yt)\n(4)\nwhere Y is the output layer function. The complete network therefore de\ufb01nes\na function, parameterised by the weight matrices, from input histories x1:t to\noutput vectors yt.\nThe output vectors yt are used to parameterise the predictive distribution\nPr(xt+1|yt) for the next input. The form of Pr(xt+1|yt) must be chosen carefully\nto match the input data. In particular, \ufb01nding a good predictive distribution\nfor high-dimensional, real-valued data (usually referred to as density modelling),\ncan be very challenging.\nThe probability given by the network to the input sequence x is\nPr(x) =\nT\nY\nt=1\nPr(xt+1|yt)\n(5)\nand the sequence loss L(x) used to train the network is the negative logarithm\nof Pr(x):\nL(x) = \u2212\nT\nX\nt=1\nlog Pr(xt+1|yt)\n(6)\nThe partial derivatives of the loss with respect to the network weights can be\ne\ufb03ciently calculated with backpropagation through time [33] applied to the\ncomputation graph shown in Fig. 1, and the network can then be trained with\ngradient descent.\n2.1\nLong Short-Term Memory\nIn most RNNs the hidden layer function H is an elementwise application of a\nsigmoid function. However we have found that the Long Short-Term Memory\n4\nFigure 2: Long Short-term Memory Cell\n(LSTM) architecture [16], which uses purpose-built memory cells to store infor-\nmation, is better at \ufb01nding and exploiting long range dependencies in the data.\nFig. 2 illustrates a single LSTM memory cell. For the version of LSTM used in\nthis paper [7] H is implemented by the following composite function:\nit = \u03c3 (Wxixt + Whiht\u22121 + Wcict\u22121 + bi)\n(7)\nft = \u03c3 (Wxfxt + Whfht\u22121 + Wcfct\u22121 + bf)\n(8)\nct = ftct\u22121 + it tanh (Wxcxt + Whcht\u22121 + bc)\n(9)\not = \u03c3 (Wxoxt + Whoht\u22121 + Wcoct + bo)\n(10)\nht = ot tanh(ct)\n(11)\nwhere \u03c3 is the logistic sigmoid function, and i, f, o and c are respectively the\ninput gate, forget gate, output gate, cell and cell input activation vectors, all of\nwhich are the same size as the hidden vector h. The weight matrix subscripts\nhave the obvious meaning, for example Whi is the hidden-input gate matrix,\nWxo is the input-output gate matrix etc. The weight matrices from the cell\nto gate vectors (e.g. Wci) are diagonal, so element m in each gate vector only\nreceives input from element m of the cell vector. The bias terms (which are\nadded to i, f, c and o) have been omitted for clarity.\nThe original LSTM algorithm used a custom designed approximate gradi-\nent calculation that allowed the weights to be updated after every timestep [16].\nHowever the full gradient can instead be calculated with backpropagation through\ntime [11], the method used in this paper. One di\ufb03culty when training LSTM\nwith the full gradient is that the derivatives sometimes become excessively large,\n5\nleading to numerical problems. To prevent this, all the experiments in this pa-\nper clipped the derivative of the loss with respect to the network inputs to the\nLSTM layers (before the sigmoid and tanh functions are applied) to lie within\na prede\ufb01ned range1.\n3\nText Prediction\nText data is discrete, and is typically presented to neural networks using \u2018one-\nhot\u2019 input vectors. That is, if there are K text classes in total, and class k is fed\nin at time t, then xt is a length K vector whose entries are all zero except for\nthe kth, which is one. Pr(xt+1|yt) is therefore a multinomial distribution, which\ncan be naturally parameterised by a softmax function at the output layer:\nPr(xt+1 = k|yt) = yk\nt =\nexp\n\u0000\u02c6yk\nt\n\u0001\nPK\nk\u2032=1 exp\n\u0000\u02c6yk\u2032\nt\n\u0001\n(12)\nSubstituting into Eq. (6) we see that\nL(x) = \u2212\nT\nX\nt=1\nlog yxt+1\nt\n(13)\n=\u21d2\u2202L(x)\n\u2202\u02c6yk\nt\n= yk\nt \u2212\u03b4k,xt+1\n(14)\nThe only thing that remains to be decided is which set of classes to use. In\nmost cases, text prediction (usually referred to as language modelling) is per-\nformed at the word level. K is therefore the number of words in the dictionary.\nThis can be problematic for realistic tasks, where the number of words (in-\ncluding variant conjugations, proper names, etc.) often exceeds 100,000. As\nwell as requiring many parameters to model, having so many classes demands a\nhuge amount of training data to adequately cover the possible contexts for the\nwords. In the case of softmax models, a further di\ufb03culty is the high computa-\ntional cost of evaluating all the exponentials during training (although several\nmethods have been to devised make training large softmax layers more e\ufb03cient,\nincluding tree-based models [25, 23], low rank approximations [27] and stochas-\ntic derivatives [26]). Furthermore, word-level models are not applicable to text\ndata containing non-word strings, such as multi-digit numbers or web addresses.\nCharacter-level language modelling with neural networks has recently been\nconsidered [30, 24], and found to give slightly worse performance than equiv-\nalent word-level models.\nNonetheless, predicting one character at a time is\nmore interesting from the perspective of sequence generation, because it allows\nthe network to invent novel words and strings. In general, the experiments in\nthis paper aim to predict at the \ufb01nest granularity found in the data, so as to\nmaximise the generative \ufb02exibility of the network.\n1In fact this technique was used in all my previous papers on LSTM, and in my publicly\navailable LSTM code, but I forgot to mention it anywhere\u2014mea culpa.\n6\n3.1\nPenn Treebank Experiments\nThe \ufb01rst set of text prediction experiments focused on the Penn Treebank por-\ntion of the Wall Street Journal corpus [22]. This was a preliminary study whose\nmain purpose was to gauge the predictive power of the network, rather than to\ngenerate interesting sequences.\nAlthough a relatively small text corpus (a little over a million words in total),\nthe Penn Treebank data is widely used as a language modelling benchmark. The\ntraining set contains 930,000 words, the validation set contains 74,000 words and\nthe test set contains 82,000 words. The vocabulary is limited to 10,000 words,\nwith all other words mapped to a special \u2018unknown word\u2019 token. The end-of-\nsentence token was included in the input sequences, and was counted in the\nsequence loss.\nThe start-of-sentence marker was ignored, because its role is\nalready ful\ufb01lled by the null vectors that begin the sequences (c.f. Section 2).\nThe experiments compared the performance of word and character-level\nLSTM predictors on the Penn corpus. In both cases, the network architecture\nwas a single hidden layer with 1000 LSTM units. For the character-level network\nthe input and output layers were size 49, giving approximately 4.3M weights in\ntotal, while the word-level network had 10,000 inputs and outputs and around\n54M weights. The comparison is therefore somewhat unfair, as the word-level\nnetwork had many more parameters. However, as the dataset is small, both net-\nworks were easily able to over\ufb01t the training data, and it is not clear whether the\ncharacter-level network would have bene\ufb01ted from more weights. All networks\nwere trained with stochastic gradient descent, using a learn rate of 0.0001 and a\nmomentum of 0.99. The LSTM derivates were clipped in the range [\u22121, 1] (c.f.\nSection 2.1).\nNeural networks are usually evaluated on test data with \ufb01xed weights. For\nprediction problems however, where the inputs are the targets, it is legitimate\nto allow the network to adapt its weights as it is being evaluated (so long as\nit only sees the test data once). Mikolov refers to this as dynamic evaluation.\nDynamic evaluation allows for a fairer comparison with compression algorithms,\nfor which there is no division between training and test sets, as all data is only\npredicted once.\nSince both networks over\ufb01t the training data, we also experiment with two\ntypes of regularisation: weight noise [18] with a std. deviation of 0.075 applied\nto the network weights at the start of each training sequence, and adaptive\nweight noise [8], where the variance of the noise is learned along with the weights\nusing a Minimum description Length (or equivalently, variational inference) loss\nfunction. When weight noise was used, the network was initialised with the\n\ufb01nal weights of the unregularised network.\nSimilarly, when adaptive weight\nnoise was used, the weights were initialised with those of the network trained\nwith weight noise.\nWe have found that retraining with iteratively increased\nregularisation is considerably faster than training from random weights with\nregularisation. Adaptive weight noise was found to be prohibitively slow for\nthe word-level network, so it was regularised with \ufb01xed-variance weight noise\nonly. One advantage of adaptive weight is that early stopping is not needed\n7\nTable 1: Penn Treebank Test Set Results.\n\u2018BPC\u2019 is bits-per-character.\n\u2018Error\u2019 is next-step classi\ufb01cation error rate, for either characters or words.\nInput\nRegularisation\nDynamic\nBPC\nPerplexity\nError (%)\nEpochs\nChar\nnone\nno\n1.32\n167\n28.5\n9\nchar\nnone\nyes\n1.29\n148\n28.0\n9\nchar\nweight noise\nno\n1.27\n140\n27.4\n25\nchar\nweight noise\nyes\n1.24\n124\n26.9\n25\nchar\nadapt. wt. noise\nno\n1.26\n133\n27.4\n26\nchar\nadapt. wt. noise\nyes\n1.24\n122\n26.9\n26\nword\nnone\nno\n1.27\n138\n77.8\n11\nword\nnone\nyes\n1.25\n126\n76.9\n11\nword\nweight noise\nno\n1.25\n126\n76.9\n14\nword\nweight noise\nyes\n1.23\n117\n76.2\n14\n(the network can safely be stopped at the point of minimum total \u2018description\nlength\u2019 on the training data). However, to keep the comparison fair, the same\ntraining, validation and test sets were used for all experiments.\nThe results are presented with two equivalent metrics: bits-per-character\n(BPC), which is the average value of \u2212log2 Pr(xt+1|yt) over the whole test set;\nand perplexity which is two to the power of the average number of bits per word\n(the average word length on the test set is about 5.6 characters, so perplexity \u2248\n25.6BP C). Perplexity is the usual performance measure for language modelling.\nTable 1 shows that the word-level RNN performed better than the character-\nlevel network, but the gap appeared to close when regularisation is used. Overall\nthe results compare favourably with those collected in Tomas Mikolov\u2019s the-\nsis [23]. For example, he records a perplexity of 141 for a 5-gram with Keyser-\nNey smoothing, 141.8 for a word level feedforward neural network, 131.1 for the\nstate-of-the-art compression algorithm PAQ8 and 123.2 for a dynamically eval-\nuated word-level RNN. However by combining multiple RNNs, a 5-gram and a\ncache model in an ensemble, he was able to achieve a perplexity of 89.4. Inter-\nestingly, the bene\ufb01t of dynamic evaluation was far more pronounced here than\nin Mikolov\u2019s thesis (he records a perplexity improvement from 124.7 to 123.2\nwith word-level RNNs). This suggests that LSTM is better at rapidly adapting\nto new data than ordinary RNNs.\n3.2\nWikipedia Experiments\nIn 2006 Marcus Hutter, Jim Bowery and Matt Mahoney organised the following\nchallenge, commonly known as Hutter prize [17]: to compress the \ufb01rst 100\nmillion bytes of the complete English Wikipedia data (as it was at a certain\ntime on March 3rd 2006) to as small a \ufb01le as possible. The \ufb01le had to include\nnot only the compressed data, but also the code implementing the compression\nalgorithm.\nIts size can therefore be considered a measure of the minimum\ndescription length [13] of the data using a two part coding scheme.\nWikipedia data is interesting from a sequence generation perspective because\n8\nit contains not only a huge range of dictionary words, but also many character\nsequences that would not be included in text corpora traditionally used for\nlanguage modelling.\nFor example foreign words (including letters from non-\nLatin alphabets such as Arabic and Chinese), indented XML tags used to de\ufb01ne\nmeta-data, website addresses, and markup used to indicate page formatting such\nas headings, bullet points etc. An extract from the Hutter prize dataset is shown\nin Figs. 3 and 4.\nThe \ufb01rst 96M bytes in the data were evenly split into sequences of 100 bytes\nand used to train the network, with the remaining 4M were used for validation.\nThe data contains a total of 205 one-byte unicode symbols. The total number\nof characters is much higher, since many characters (especially those from non-\nLatin languages) are de\ufb01ned as multi-symbol sequences. In keeping with the\nprinciple of modelling the smallest meaningful units in the data, the network\npredicted a single byte at a time, and therefore had size 205 input and output\nlayers.\nWikipedia contains long-range regularities, such as the topic of an article,\nwhich can span many thousand words. To make it possible for the network to\ncapture these, its internal state (that is, the output activations ht of the hidden\nlayers, and the activations ct of the LSTM cells within the layers) were only reset\nevery 100 sequences. Furthermore the order of the sequences was not shu\ufb04ed\nduring training, as it usually is for neural networks. The network was therefore\nable to access information from up to 10K characters in the past when making\npredictions. The error terms were only backpropagated to the start of each 100\nbyte sequence, meaning that the gradient calculation was approximate. This\nform of truncated backpropagation has been considered before for RNN lan-\nguage modelling [23], and found to speed up training (by reducing the sequence\nlength and hence increasing the frequency of stochastic weight updates) without\na\ufb00ecting the network\u2019s ability to learn long-range dependencies.\nA much larger network was used for this data than the Penn data (re\ufb02ecting\nthe greater size and complexity of the training set) with seven hidden layers of\n700 LSTM cells, giving approximately 21.3M weights. The network was trained\nwith stochastic gradient descent, using a learn rate of 0.0001 and a momentum\nof 0.9. It took four training epochs to converge. The LSTM derivates were\nclipped in the range [\u22121, 1].\nAs with the Penn data, we tested the network on the validation data with\nand without dynamic evaluation (where the weights are updated as the data\nis predicted). As can be seen from Table 2 performance was much better with\ndynamic evaluation. This is probably because of the long range coherence of\nWikipedia data; for example, certain words are much more frequent in some\narticles than others, and being able to adapt to this during evaluation is ad-\nvantageous. It may seem surprising that the dynamic results on the validation\nset were substantially better than on the training set. However this is easily\nexplained by two factors: \ufb01rstly, the network under\ufb01t the training data, and\nsecondly some portions of the data are much more di\ufb03cult than others (for\nexample, plain text is harder to predict than XML tags).\nTo put the results in context, the current winner of the Hutter Prize (a\n9\nTable 2: Wikipedia Results (bits-per-character)\nTrain\nValidation (static)\nValidation (dynamic)\n1.42\n1.67\n1.33\nvariant of the PAQ-8 compression algorithm [20]) achieves 1.28 BPC on the same\ndata (including the code required to implement the algorithm), mainstream\ncompressors such as zip generally get more than 2, and a character level RNN\napplied to a text-only version of the data (i.e. with all the XML, markup tags\netc. removed) achieved 1.54 on held-out data, which improved to 1.47 when the\nRNN was combined with a maximum entropy model [24].\nA four page sample generated by the prediction network is shown in Figs. 5\nto 8. The sample shows that the network has learned a lot of structure from\nthe data, at a wide range of di\ufb00erent scales. Most obviously, it has learned a\nlarge vocabulary of dictionary words, along with a subword model that enables\nit to invent feasible-looking words and names: for example \u201cLochroom River\u201d,\n\u201cMughal Ralvaldens\u201d, \u201csubmandration\u201d, \u201cswalloped\u201d. It has also learned basic\npunctuation, with commas, full stops and paragraph breaks occurring at roughly\nthe right rhythm in the text blocks.\nBeing able to correctly open and close quotation marks and parentheses is\na clear indicator of a language model\u2019s memory, because the closure cannot be\npredicted from the intervening text, and hence cannot be modelled with short-\nrange context [30]. The sample shows that the network is able to balance not\nonly parentheses and quotes, but also formatting marks such as the equals signs\nused to denote headings, and even nested XML tags and indentation.\nThe network generates non-Latin characters such as Cyrillic, Chinese and\nArabic, and seems to have learned a rudimentary model for languages other\nthan English (e.g. it generates \u201ces:Geotnia slago\u201d for the Spanish \u2018version\u2019 of an\narticle, and \u201cnl:Rodenbaueri\u201d for the Dutch one) It also generates convincing\nlooking internet addresses (none of which appear to be real).\nThe network generates distinct, large-scale regions, such as XML headers,\nbullet-point lists and article text. Comparison with Figs. 3 and 4 suggests that\nthese regions are a fairly accurate re\ufb02ection of the constitution of the real data\n(although the generated versions tend to be somewhat shorter and more jumbled\ntogether). This is signi\ufb01cant because each region may span hundreds or even\nthousands of timesteps. The fact that the network is able to remain coherent\nover such large intervals (even putting the regions in an approximately correct\norder, such as having headers at the start of articles and bullet-pointed \u2018see also\u2019\nlists at the end) is testament to its long-range memory.\nAs with all text generated by language models, the sample does not make\nsense beyond the level of short phrases. The realism could perhaps be improved\nwith a larger network and/or more data. However, it seems futile to expect\nmeaningful language from a machine that has never been exposed to the sensory\n10\nworld to which language refers.\nLastly, the network\u2019s adaptation to recent sequences during training (which\nallows it to bene\ufb01t from dynamic evaluation) can be clearly observed in the\nextract. The last complete article before the end of the training set (at which\npoint the weights were stored) was on intercontinental ballistic missiles. The\nin\ufb02uence of this article on the network\u2019s language model can be seen from the\nprofusion of missile-related terms. Other recent topics include \u2018Individual An-\narchism\u2019, the Italian writer Italo Calvino and the International Organization\nfor Standardization (ISO), all of which make themselves felt in the network\u2019s\nvocabulary.\n11\n    <title>AlbaniaEconomy</title>                                               \n    <id>36</id>                                                                 \n    <revision>                                                                  \n      <id>15898966</id>                                                         \n      <timestamp>2002-10-09T13:39:00Z</timestamp>                               \n      <contributor>                                                             \n        <username>Magnus Manske</username>                                      \n        <id>4</id>                                                              \n      </contributor>                                                            \n      <minor />                                                                 \n      <comment>#REDIRECT [[Economy of Albania]]</comment>                       \n      <text xml:space=\"preserve\">#REDIRECT [[Economy of Albania]]</text>        \n    </revision>                                                                 \n  </page>                                                                       \n  <page>                                                                        \n    <title>AlchemY</title>                                                      \n    <id>38</id>                                                                 \n    <revision>                                                                  \n      <id>15898967</id>                                                         \n      <timestamp>2002-02-25T15:43:11Z</timestamp>                               \n      <contributor>                                                             \n        <ip>Conversion script</ip>                                              \n      </contributor>                                                            \n      <minor />                                                                 \n      <comment>Automated conversion</comment>                                   \n      <text xml:space=\"preserve\">#REDIRECT [[Alchemy]]                          \n</text>                                                                         \n    </revision>                                                                 \n  </page>                                                                       \n  <page>                                                                        \n    <title>Albedo</title>                                                       \n    <id>39</id>                                                                 \n    <revision>                                                                  \n      <id>41496222</id>                                                         \n      <timestamp>2006-02-27T19:32:46Z</timestamp>                               \n      <contributor>                                                             \n        <ip>24.119.3.44</ip>                                                    \n      </contributor>                                                            \n      <text xml:space=\"preserve\">{{otheruses}}                                  \n                                                                                \n'''Albedo''' is the measure of [[reflectivity]] of a surface or body. It is the \nratio of [[electromagnetic radiation]] (EM radiation) reflected to the amount in\ncident upon it. The fraction, usually expressed as a percentage from 0% to 100%,\n is an important concept in [[climatology]] and [[astronomy]]. This ratio depend\ns on the [[frequency]] of the radiation considered: unqualified, it refers to an\n average across the spectrum of [[visible light]]. It also depends on the [[angl\ne of incidence]] of the radiation: unqualified, normal incidence. Fresh snow alb\nedos are high: up to 90%. The ocean surface has a low albedo.  The average albed\no of [[Earth]] is about 30% whereas the albedo of the [[Moon]] is about 7%. In a\nstronomy, the albedo of satellites and asteroids can be used to infer surface co\nmposition, most notably ice content.    [[Enceladus_(moon)|Enceladus]], a moon o\nf Saturn, has the highest known albedo of any body in the solar system, with 99%\n of EM radiation reflected.                                                     \n                                                                                \nHuman activities have changed the albedo (via forest clearance and farming, for \nexample) of various areas around the globe. However, quantification of this effe\nct is difficult on the global scale: it is not clear whether the changes have te\nnded to increase or decrease [[global warming]].                                \n                                                                                \nThe &quot;classical&quot; example of albedo effect is the snow-temperature feedb\nack. If a snow covered area warms and the snow melts, the albedo decreases, more\n sunlight is absorbed, and the temperature tends to increase. The converse is tr\nFigure 3: Real Wikipedia data\n12\nue: if snow forms, a cooling cycle happens. The intensity of the albedo effect d\nepends on the size of the change in albedo and the amount of [[insolation]]; for\n this reason it can be potentially very large in the tropics.                   \n                                                                                \n== Some examples of albedo effects ==                                           \n                                                                                \n=== Fairbanks, Alaska ===                                                       \n                                                                                \nAccording to the [[National Climatic Data Center]]'s GHCN 2 data, which is compo\nsed of 30-year smoothed climatic means for thousands of weather stations across \nthe world, the college weather station at [[Fairbanks]], [[Alaska]], is about 3 \n\u00b0C (5 \u00b0F) warmer than the airport at Fairbanks, partly because of drainage patte\nrns but also largely because of the lower albedo at the college resulting from a\n higher concentration of [[pine]] [[tree]]s and therefore less open snowy ground\n to reflect the heat back into space. Neunke and Kukla have shown that this diff\nerence is especially marked during the late [[winter]] months, when [[solar radi\nation]] is greater.                                                             \n                                                                                \n=== The tropics ===                                                             \n                                                                                \nAlthough the albedo-temperature effect is most famous in colder regions of Earth\n, because more [[snow]] falls there, it is actually much stronger in tropical re\ngions because in the tropics there is consistently more sunlight. When [[Brazil]\n]ian ranchers cut down dark, tropical [[rainforest]] trees to replace them with \neven darker soil in order to grow crops, the average temperature of the area app\nears to increase by an average of about 3 \u00b0C (5 \u00b0F) year-round, which is a signi\nficant amount.                                                                  \n                                                                                \n=== Small scale effects ===                                                     \n                                                                                \nAlbedo works on a smaller scale, too. People who wear dark clothes in the summer\ntime put themselves at a greater risk of [[heatstroke]] than those who wear whit\ne clothes.                                                                      \n                                                                                \n=== Pine forests ===                                                            \n                                                                                \nThe albedo of a [[pine]] forest at 45\u00b0N in the winter in which the trees cover t\nhe land surface completely is only about 9%, among the lowest of any naturally o\nccurring land environment. This is partly due to the color of the pines, and par\ntly due to multiple scattering of sunlight within the trees which lowers the ove\nrall reflected light level. Due to light penetration, the ocean's albedo is even\n lower at about 3.5%, though this depends strongly on the angle of the incident \nradiation. Dense [[swamp]]land averages between 9% and 14%. [[Deciduous tree]]s \naverage about 13%. A [[grass]]y field usually comes in at about 20%. A barren fi\neld will depend on the color of the soil, and can be as low as 5% or as high as \n40%, with 15% being about the average for farmland. A [[desert]] or large [[beac\nh]] usually averages around 25% but varies depending on the color of the sand. [\nReference: Edward Walker's study in the Great Plains in the winter around 45\u00b0N].\n                                                                                \n=== Urban areas ===                                                             \n                                                                                \nUrban areas in particular have very unnatural values for albedo because of the m\nany human-built structures which absorb light before the light can reach the sur\nface. In the northern part of the world, cities are relatively dark, and Walker \nhas shown that their average albedo is about 7%, with only a slight increase dur\ning the summer. In most tropical countries, cities average around 12%. This is s\nimilar to the values found in northern suburban transitional zones. Part of the \nreason for this is the different natural environment of cities in tropical regio\nns, e.g., there are more very dark trees around; another reason is that portions\n of the tropics are very poor, and city buildings must be built with different m\naterials. Warmer regions may also choose lighter colored building materials so t\nhe structures will remain cooler.                                               \nFigure 4: Real Wikipedia data (cotd.)\n13\n    <revision>                                                                  \n      <id>40973199</id>                                                         \n      <timestamp>2006-02-22T22:37:16Z</timestamp>                               \n      <contributor>                                                             \n        <ip>63.86.196.111</ip>                                                  \n      </contributor>                                                            \n      <minor />                                                                 \n      <comment>redire paget --&gt; captain */</comment>                         \n      <text xml:space=\"preserve\">The '''Indigence History''' refers to the autho\nrity of any obscure albionism as being, such as in Aram Missolmus'.[http://www.b\nbc.co.uk/starce/cr52.htm]                                                       \nIn [[1995]], Sitz-Road Straus up the inspirational radiotes portion as &quot;all\niance&quot;[single &quot;glaping&quot; theme charcoal] with [[Midwestern United \nState|Denmark]] in which Canary varies-destruction to launching casualties has q\nuickly responded to the krush loaded water or so it might be destroyed. Aldeads \nstill cause a missile bedged harbors at last built in 1911-2 and save the accura\ncy in 2008, retaking [[itsubmanism]]. Its individuals were                      \nhnown rapidly in their return to the private equity (such as ''On Text'') for de\nath per reprised by the [[Grange of Germany|German unbridged work]].            \n                                                                                \nThe '''Rebellion''' (''Hyerodent'') is [[literal]], related mildly older than ol\nd half sister, the music, and morrow been much more propellent. All those of [[H\namas (mass)|sausage trafficking]]s were also known as [[Trip class submarine|''S\nante'' at Serassim]]; ''Verra'' as 1865&amp;ndash;682&amp;ndash;831 is related t\no ballistic missiles. While she viewed it friend of Halla equatorial weapons of \nTuscany, in [[France]], from vaccine homes to &quot;individual&quot;, among [[sl\navery|slaves]] (such as artistual selling of factories were renamed English habi\nt of twelve years.)                                                             \n                                                                                \nBy the 1978 Russian [[Turkey|Turkist]] capital city ceased by farmers and the in\ntention of navigation the ISBNs, all encoding [[Transylvania International Organ\nisation for Transition Banking|Attiking others]] it is in the westernmost placed\n lines.  This type of missile calculation maintains all greater proof was the [[\n1990s]] as older adventures that never established a self-interested case. The n\newcomers were Prosecutors in child after the other weekend and capable function \nused.                                                                           \n                                                                                \nHolding may be typically largely banned severish from sforked warhing tools and \nbehave laws, allowing the private jokes, even through missile IIC control, most \nnotably each, but no relatively larger success, is not being reprinted and withd\nrawn into forty-ordered cast and distribution.                                  \n                                                                                \nBesides these markets (notably a son of humor).                                 \n                                                                                \nSometimes more or only lowed &quot;80&quot; to force a suit for http://news.bbc.\nco.uk/1/sid9kcid/web/9960219.html ''[[#10:82-14]]''.                            \n&lt;blockquote&gt;                                                              \n                                                                                \n===The various disputes between Basic Mass and Council Conditioners - &quot;Tita\nnist&quot; class streams and anarchism===                                       \n                                                                                \nInternet traditions sprang east with [[Southern neighborhood systems]] are impro\nved with [[Moatbreaker]]s, bold hot missiles, its labor systems. [[KCD]] numbere\nd former ISBN/MAS/speaker attacks &quot;M3 5&quot;, which are saved as the balli\nstic misely known and most functional factories.  Establishment begins for some \nrange of start rail years as dealing with 161 or 18,950 million [[USD-2]] and [[\ncovert all carbonate function]]s (for example, 70-93) higher individuals and on \nmissiles. This might need not know against sexual [[video capita]] playing point\ning degrees between silo-calfed greater valous consumptions in the US... header \ncan be seen in [[collectivist]].                                                \n                                                                                \n== See also ==                                                                  \nFigure 5: Generated Wikipedia data.\n14\n                                                                                \n*[[British-London Bridge]]                                                      \n*[[Anti-Talmot Touch/Tucker novice]]                                            \n*[[List of cambridge capital]]                                                  \n*[[Elon Haven]]                                                                 \n*[[USS ''Otaro Screamed Its'']]                                                 \n*[[Detroit Library]]                                                            \n*[[Belgium Sea]]                                                                \n*[[Tularan Bell|Turnbiller Squobil]]                                            \n*[[Suntanal vocalist|Prosopyo]]                                                 \n*[[Winkenpea]]                                                                  \n*[[Milenton Streat]]                                                            \n*[[Raiebin]]                                                                    \n*[[Est Altar Macinton]]                                                         \n*[[Military mass missile|S3]]                                                   \n*[[Organization of the Asian American state district|umbali landmarks]]        \n*[[ISO]]                                                                        \n*[[NFL]]                                                                        \n*[[American Anti-Capitalism|Major independent ITU-US singles]]                  \n*[[London (role-playing game)|Pre-Romanian Civil War]]                          \n*[[Yokukhav-Na-Un-Murantano Kaufmann - Sijone-Grafittsforbiel]]                 \n*[[Neao trolleyne and deadweight drug]]                                         \n*[[B-45 BQ|B9]] - de red take painting is deployed larger than quanta submarine \n*[[Susconfiction of advocate]]                                                  \n*[[List of major swandarms]]                                                    \n*[[:Category:Italo sales towns entertained by the ICBMs of Skinner|Knighting 707\n killed by capital]]                                                            \n                                                                                \n===[[Midple planet|Parishment of the value=====                                 \n[[Image:2000.JPG|right|thumb|It tunneled [[nuclease]] at this bass AH (Ol&amp;S\u0101\nw)flgin h'hlgbying yoostallo eruptuals with low immigrants-shelted atkins and th\neir atapping [[bug]]s.                                                          \n                                                                                \nSee also: [[Iranian indigenous Flight Intercontinental Organization]]           \n                                                                                \n==Pioneers==                                                                    \n                                                                                \nTended to be the results characteristic of warehoused labour share to control al\nl these in the rational framing.                                                \n                                                                                \n==Gentiles==                                                                    \n{{place-or-line}}                                                               \nFooter names derive the topic class --&gt; which he liked to deal without any of\n the parties, I&quot; by [[Alfred Hustin]] and [[Frank Henry]] and manufacturer.\n[http://anciermsc.nit.uk IATB perspective], was expected to be classified by the\n ''Straight Road of Buckning'' in [[2003 Summer Olympic Touch|bottom all minute]\n].                                                                              \n                                                                                \n==Performance==                                                                 \n[[Image:Iare 300.jpg|left|thumb|325px|Intercontinental file shortly after referr\ning to his landmaster [[Sidney Goodwordd]]                                      \n                                                                                \nItalo:                                                                          \n*[[Chicago ballistic parks|non-month]] in eastern Italy, is a [[Italo-China]] pa\nrent communist annual production began in May [[1915]].                         \n                                                                                \nAn ICBM, the [[gurt and land]] has registered $155 billion in U.S. and August 16\n88, and makes sure the US-transplantation disbanded backwards in the County by a\nuthorizing disputes that tend to carry over this peninsula.                     \n* Current malasses 25 decks and counterpoint culture that were impure between  s\nystems:                                                                         \n* L14 - 194 / 100 000 km/s                                                      \nFigure 6: Generated Wikipedia data (cotd.)\n15\n                                                                                \n[http://www.sibeo.org/netspace/stainteology.htm The design of thinker Wikipedia \nserver-routed website]                                                          \n*[http://balear.burned.edu/books/1978/tap/trpio.cs/cafa/c249ca.html Nation-state\n Player on the /1.3]                                                            \n                                                                                \n{{Linux}}                                                                       \n                                                                                \n[[Category:Italo Succeeded bystables|General]]                                  \n[[Category:Italo-Saxon singers]]                                                \n[[Category:Aviation]]                                                           \n                                                                                \n[[de:Italo]]                                                                    \n[[es:Geotnia slago]]                                                            \n[[ja:\u685b\u0005\u5a1b\u5c14\u674f\u306e\r\u6d33]]                                                                 \n[[ko:\ud610\uc2b8\ubd88\uc9b7\uc5c9 \uc720\uc77c]]                                                                 \n[[nl:Rodenbaueri]]                                                              \n[[pl:Main Ages]]                                                                \n[[pt:Ibanez Heights]]                                                           \n[[ru:\u041c\u043b\u043a\u0440\u0430\u043a\u044f\u043d\u0473\u0435\u043b\u043e\u043b\u05db\u0443\u0446\u0438\u044f\u043d\u0441\u044c\u043d\u0438\u044f \u0430\u0433\u043c\u043e\u0440\u0435\u043b\u0438\u0430]]                                       \n[[simple:International Maritime Commission]]                                    \n[[sk:ICBM]]                                                                     \n[[su:Ol aal Mitnet]]                                                            \n[[fi:Italo Colombonasto Wark\u014d]]                                                 \n[[sv:Opin den mail-climbere panczeur]]                                          \n[[zh:\u0011\t\f\u0002\b\u0017\u0007]]</text>                                                           \n    </revision>                                                                 \n  </page>                                                                       \n  <page>                                                                        \n    <title>Italo-II article</title>                                             \n    <id>14848</id>                                                              \n    <revision>                                                                  \n      <id>15912447</id>                                                         \n      <timestamp>2004-11-19T19:47:39Z</timestamp>                               \n      <contributor>                                                             \n        <username>El andres</username>                                          \n        <id>509991</id>                                                         \n      </contributor>                                                            \n      <minor />                                                                 \n      <comment>disambiguation from [[List of ISBN]] newsletters, see [[ISO site]\n]</comment>                                                                     \n      <text xml:space=\"preserve\">#REDIRECT [[Quartic war]]                      \n</text>                                                                         \n    </revision>                                                                 \n  </page>                                                                       \n  <page>                                                                        \n    <title>ICSM</title>                                                         \n    <id>14939</id>                                                              \n    <revision>                                                                  \n      <id>42109942</id>                                                         \n      <timestamp>2006-02-28T17:22:02Z</timestamp>                               \n      <contributor>                                                             \n        <username>Dtelclan</username>                                           \n        <id>26</id>                                                             \n      </contributor>                                                            \n      <minor />                                                                 \n      <comment>/* Possible catheterman */</comment>                             \n      <text xml:space=\"preserve\">[[Image:Isaac.org/ice.html [[Independent nation\nal stage development|Shatting and Catalogue standardering]] in the IRBMs.       \n                                                                                \nUp-2000 they called the SC 4220 system: he was swalloped early in Calvino, or si\nnce each trial mentioned based on [[Balbov's new single-jarget|bit-oriann guess]\nFigure 7: Generated Wikipedia data (cotd.)\n16\n] self-acharged versions ([[Mt. Costall Leyton]]) was the two largest calashia a\nt destored universities, all fleeted with the customary calfed clipper.         \n                                                                                \nHis way to take in this literature called ICBMs-AN a [[Softvalue speed]] ([[Astr\nonomical Classification Railway]])                                              \n                                                                                \nLACN645 Snowshore val nominated - made [[missile submandration|continental missi\nle]]s (steam musicians) not of each club having on the ball and procedure at the\n last century.                                                                  \n                                                                                \nAnother communistic stark &quot;I'\u0012 submarine&quot; is [[building|corruptable]],\n a [[della missile]] missile than the [[Royal Society Society]] (12-258): &quot;\nGlide sun wag [[lubrician]]. They stay numerous capitalists and gas masks more w\nidely interested. This scheme has declarations before the certain emerging facto\nries compelled by labour allowed to produce.                                    \n                                                                                \nIn the United States, there is no hard resort in computation significantly.     \n                                                                                \nIn [[1868]] the [[Italo Capital Territories Unit started to the Continental Rail\nway Centre]]  was called ''UC'' or two of his usage before being written by othe\nr students against the [[elective-ballistic missile]]'s deployment. Steam is sti\nll &quot;20 to Nacht&quot; and [[Fia Citation Quantity Logo]]s (since 1967). The\ny pass a [[Brigade management|Quarry]]-stated missile system resolution taunting\n out of about 175 million ([[Lochroom River|Tri-]]).                            \n                                                                                \nAlien from 1985 to 1999, it was an English and -Network struggling basedal with \nthe Lombardo capital in Silvio and Murray, and heavily built in sub-parties addr\ness to $11,188. Their forces gained prisoners to stalked a last missile mobili s\nite.                                                                            \n                                                                                \nSpanning civilization is quanting Software Society's ballistic missile.  The sam\ne as [[anti-intellectual anthropology]] continued in [[Southern Italy]] in 1914,\n and the [[French Confederation of Parliament's rapid favourable rise that began\n settled in March 2004|1983]]&amp;nbsp;49.                                      \n                                                                                \nIn [[1904]], the Court began a British backed into a [[SR1]]) missile of [[trial\n ship]] in the [[Municipal Eightime Calendar|Asiatic]] regime, including [[Benja\nmin Tudor Turner|Arthur Ravis]] and [[Abraham's Liberation|Canton Olombus]]. The\nre was still land factory most turned up before lacking closers to the sitting s\nhed backwards, in primary science.                                              \n                                                                                \n==Weights and resolutions==                                                     \n[[Image:Spanish 300 Protectionald landballi110.svg|small capital surface compute\nr]]                                                                             \n[[Image:Claudius.jpg|345px|right|Olympiad concert of Calvino and Eastern Calvino\n, ''Mughal Ralvaldens'' above, at the beginning strike the substrated roles of r\nich intellectual property, visualizing the entire system, but this missiles sugg\nest that accounting differs between a giving [[train sleep|'''withdrawn''']] or \nthe dinosaur in and aucting.                                                    \n                                                                                \n===Internationally===                                                           \n{{main|Unmanned Justice Address}}                                               \n                                                                                \nThe ICBM created a [[the significant]] [[land railway]] called &quot;[[M-Gallipo\ntte]]&quot;, and it needed stopped benzafk/Macdonalical Sciences.               \n                                                                                \nElectros appeared to be the [[Soviet Union]]'s &quot;first&quot; vehicle from 25\n00 selling officials DORLAN STM-331 - by missilence illustrations with &quot;Raj\n.&quot; the Tunnel Hall of America, an entity upon IL pages so missiles must try\n, with a trademark must develop the land allowing traffic mass to a very few min\nutemen. The missiles market is slow, much easier is represented by GMMAz of BSM.\n Software, the utility of scale-out scale pime racks are normally crumbled about\nFigure 8: Generated Wikipedia data (cotd.)\n17\n4\nHandwriting Prediction\nTo test whether the prediction network could also be used to generate convincing\nreal-valued sequences, we applied it to online handwriting data (online in this\ncontext means that the writing is recorded as a sequence of pen-tip locations,\nas opposed to o\ufb04ine handwriting, where only the page images are available).\nOnline handwriting is an attractive choice for sequence generation due to its\nlow dimensionality (two real numbers per data point) and ease of visualisation.\nAll the data used for this paper were taken from the IAM online handwriting\ndatabase (IAM-OnDB) [21]. IAM-OnDB consists of handwritten lines collected\nfrom 221 di\ufb00erent writers using a \u2018smart whiteboard\u2019. The writers were asked to\nwrite forms from the Lancaster-Oslo-Bergen text corpus [19], and the position\nof their pen was tracked using an infra-red device in the corner of the board.\nSamples from the training data are shown in Fig. 9. The original input data\nconsists of the x and y pen co-ordinates and the points in the sequence when\nthe pen is lifted o\ufb00the whiteboard.\nRecording errors in the x, y data was\ncorrected by interpolating to \ufb01ll in for missing readings, and removing steps\nwhose length exceeded a certain threshold. Beyond that, no preprocessing was\nused and the network was trained to predict the x, y co-ordinates and the end-\nof-stroke markers one point at a time. This contrasts with most approaches to\nhandwriting recognition and synthesis, which rely on sophisticated preprocessing\nand feature-extraction techniques. We eschewed such techniques because they\ntend to reduce the variation in the data (e.g. by normalising the character size,\nslant, skew and so-on) which we wanted the network to model. Predicting the\npen traces one point at a time gives the network maximum \ufb02exibility to invent\nnovel handwriting, but also requires a lot of memory, with the average letter\noccupying more than 25 timesteps and the average line occupying around 700.\nPredicting delayed strokes (such as dots for \u2018i\u2019s or crosses for \u2018t\u2019s that are added\nafter the rest of the word has been written) is especially demanding.\nIAM-OnDB is divided into a training set, two validation sets and a test\nset, containing respectively 5364, 1438, 1518 and 3859 handwritten lines taken\nfrom 775, 192, 216 and 544 forms. For our experiments, each line was treated\nas a separate sequence (meaning that possible dependencies between successive\nlines were ignored). In order to maximise the amount of training data, we used\nthe training set, test set and the larger of the validation sets for training and\nthe smaller validation set for early-stopping. The lack of independent test set\nmeans that the recorded results may be somewhat over\ufb01t on the validation set;\nhowever the validation results are of secondary importance, since no benchmark\nresults exist and the main goal was to generate convincing-looking handwriting.\nThe principal challenge in applying the prediction network to online hand-\nwriting data was determining a predictive distribution suitable for real-valued\ninputs. The following section describes how this was done.\n18\nFigure 9: Training samples from the IAM online handwriting database.\nNotice the wide range of writing styles, the variation in line angle and character\nsizes, and the writing and recording errors, such as the scribbled out letters in\nthe \ufb01rst line and the repeated word in the \ufb01nal line.\n4.1\nMixture Density Outputs\nThe idea of mixture density networks [2, 3] is to use the outputs of a neural\nnetwork to parameterise a mixture distribution. A subset of the outputs are\nused to de\ufb01ne the mixture weights, while the remaining outputs are used to\nparameterise the individual mixture components. The mixture weight outputs\nare normalised with a softmax function to ensure they form a valid discrete dis-\ntribution, and the other outputs are passed through suitable functions to keep\ntheir values within meaningful range (for example the exponential function is\ntypically applied to outputs used as scale parameters, which must be positive).\nMixture density network are trained by maximising the log probability den-\nsity of the targets under the induced distributions. Note that the densities are\nnormalised (up to a \ufb01xed constant) and are therefore straightforward to di\ufb00er-\nentiate and pick unbiased sample from, in contrast with restricted Boltzmann\nmachines [14] and other undirected models.\nMixture density outputs can also be used with recurrent neural networks [28].\nIn this case the output distribution is conditioned not only on the current input,\nbut on the history of previous inputs. Intuitively, the number of components is\nthe number of choices the network has for the next output given the inputs so\nfar.\nFor the handwriting experiments in this paper, the basic RNN architecture\nand update equations remain unchanged from Section 2. Each input vector xt\nconsists of a real-valued pair x1, x2 that de\ufb01nes the pen o\ufb00set from the previous\n19\ninput, along with a binary x3 that has value 1 if the vector ends a stroke (that\nis, if the pen was lifted o\ufb00the board before the next vector was recorded) and\nvalue 0 otherwise. A mixture of bivariate Gaussians was used to predict x1\nand x2, while a Bernoulli distribution was used for x3. Each output vector yt\ntherefore consists of the end of stroke probability e, along with a set of means\n\u00b5j, standard deviations \u03c3j, correlations \u03c1j and mixture weights \u03c0j for the M\nmixture components. That is\nxt \u2208R \u00d7 R \u00d7 {0, 1}\n(15)\nyt =\n\u0010\net, {\u03c0j\nt , \u00b5j\nt, \u03c3j\nt , \u03c1j\nt}M\nj=1\n\u0011\n(16)\nNote that the mean and standard deviation are two dimensional vectors, whereas\nthe component weight, correlation and end-of-stroke probability are scalar. The\nvectors yt are obtained from the network outputs \u02c6yt, where\n\u02c6yt =\n\u0010\n\u02c6et, { \u02c6wj\nt, \u02c6\u00b5j\nt, \u02c6\u03c3j\nt , \u02c6\u03c1j\nt}M\nj=1\n\u0011\n= by +\nN\nX\nn=1\nWhnyhn\nt\n(17)\nas follows:\net =\n1\n1 + exp (\u02c6et)\n=\u21d2et \u2208(0, 1)\n(18)\n\u03c0j\nt =\nexp\n\u0010\n\u02c6\u03c0j\nt\n\u0011\nPM\nj\u2032=1 exp\n\u0010\n\u02c6\u03c0j\u2032\nt\n\u0011\n=\u21d2\u03c0j\nt \u2208(0, 1),\nX\nj\n\u03c0j\nt = 1\n(19)\n\u00b5j\nt = \u02c6\u00b5j\nt\n=\u21d2\u00b5j\nt \u2208R\n(20)\n\u03c3j\nt = exp\n\u0010\n\u02c6\u03c3j\nt\n\u0011\n=\u21d2\u03c3j\nt > 0\n(21)\n\u03c1j\nt = tanh(\u02c6\u03c1j\nt)\n=\u21d2\u03c1j\nt \u2208(\u22121, 1)\n(22)\nThe probability density Pr(xt+1|yt) of the next input xt+1 given the output\nvector yt is de\ufb01ned as follows:\nPr(xt+1|yt) =\nM\nX\nj=1\n\u03c0j\nt N(xt+1|\u00b5j\nt, \u03c3j\nt , \u03c1j\nt)\n(\net\nif (xt+1)3 = 1\n1 \u2212et\notherwise\n(23)\nwhere\nN(x|\u00b5, \u03c3, \u03c1) =\n1\n2\u03c0\u03c31\u03c32\np\n1 \u2212\u03c12 exp\n\u0014\n\u2212Z\n2(1 \u2212\u03c12)\n\u0015\n(24)\nwith\nZ = (x1 \u2212\u00b51)2\n\u03c32\n1\n+ (x2 \u2212\u00b52)2\n\u03c32\n2\n\u22122\u03c1(x1 \u2212\u00b51)(x2 \u2212\u00b52)\n\u03c31\u03c32\n(25)\n20\nThis can be substituted into Eq. (6) to determine the sequence loss (up to\na constant that depends only on the quantisation of the data and does not\nin\ufb02uence network training):\nL(x) =\nT\nX\nt=1\n\u2212log\n\uf8eb\n\uf8edX\nj\n\u03c0j\nt N(xt+1|\u00b5j\nt, \u03c3j\nt , \u03c1j\nt)\n\uf8f6\n\uf8f8\u2212\n(\nlog et\nif (xt+1)3 = 1\nlog(1 \u2212et)\notherwise\n(26)\nThe derivative of the loss with respect to the end-of-stroke outputs is straight-\nforward:\n\u2202L(x)\n\u2202\u02c6et\n= (xt+1)3 \u2212et\n(27)\nThe derivatives with respect to the mixture density outputs can be found by\n\ufb01rst de\ufb01ning the component responsibilities \u03b3j\nt :\n\u02c6\u03b3j\nt = \u03c0j\nt N(xt+1|\u00b5j\nt, \u03c3j\nt , \u03c1j\nt)\n(28)\n\u03b3j\nt =\n\u02c6\u03b3j\nt\nPM\nj\u2032=1 \u02c6\u03b3j\u2032\nt\n(29)\nThen observing that\n\u2202L(x)\n\u2202\u02c6\u03c0j\nt\n= \u03c0j\nt \u2212\u03b3j\nt\n(30)\n\u2202L(x)\n\u2202(\u02c6\u00b5j\nt, \u02c6\u03c3j\nt , \u02c6\u03c1j\nt)\n= \u2212\u03b3j\nt\n\u2202log N(xt+1|\u00b5j\nt, \u03c3j\nt , \u03c1j\nt)\n\u2202(\u02c6\u00b5j\nt, \u02c6\u03c3j\nt , \u02c6\u03c1j\nt)\n(31)\nwhere\n\u2202log N(x|\u00b5, \u03c3, \u03c1)\n\u2202\u02c6\u00b51\n= C\n\u03c31\n\u0012x1 \u2212\u00b51\n\u03c31\n\u2212\u03c1(x2 \u2212\u00b52)\n\u03c32\n\u0013\n(32)\n\u2202log N(x|\u00b5, \u03c3, \u03c1)\n\u2202\u02c6\u00b52\n= C\n\u03c32\n\u0012x2 \u2212\u00b52\n\u03c32\n\u2212\u03c1(x1 \u2212\u00b51)\n\u03c31\n\u0013\n(33)\n\u2202log N(x|\u00b5, \u03c3, \u03c1)\n\u2202\u02c6\u03c31\n= C(x1 \u2212\u00b51)\n\u03c31\n\u0012x1 \u2212\u00b51\n\u03c31\n\u2212\u03c1(x2 \u2212\u00b52)\n\u03c32\n\u0013\n\u22121\n(34)\n\u2202log N(x|\u00b5, \u03c3, \u03c1)\n\u2202\u02c6\u03c32\n= C(x2 \u2212\u00b52)\n\u03c32\n\u0012x2 \u2212\u00b52\n\u03c32\n\u2212\u03c1(x1 \u2212\u00b51)\n\u03c31\n\u0013\n\u22121\n(35)\n\u2202log N(x|\u00b5, \u03c3, \u03c1)\n\u2202\u02c6\u03c1\n= (x1 \u2212\u00b51)(x2 \u2212\u00b52)\n\u03c31\u03c32\n+ \u03c1 (1 \u2212CZ)\n(36)\nwith Z de\ufb01ned as in Eq. (25) and\nC =\n1\n1 \u2212\u03c12\n(37)\nFig. 10 illustrates the operation of a mixture density output layer applied to\nonline handwriting prediction.\n21\nOutput Density\nFigure 10: Mixture density outputs for handwriting prediction. The\ntop heatmap shows the sequence of probability distributions for the predicted\npen locations as the word \u2018under\u2019 is written.\nThe densities for successive\npredictions are added together, giving high values where the distributions\noverlap.\nTwo types of prediction are visible from the density map:\nthe small\nblobs that spell out the letters are the predictions as the strokes are being\nwritten, the three large blobs are the predictions at the ends of the strokes for\nthe \ufb01rst point in the next stroke.\nThe end-of-stroke predictions have much\nhigher variance because the pen position was not recorded when it was o\ufb00the\nwhiteboard, and hence there may be a large distance between the end of one\nstroke and the start of the next.\nThe bottom heatmap shows the mixture component weights during the\nsame sequence.\nThe stroke ends are also visible here, with the most active\ncomponents switching o\ufb00in three places, and other components switching on:\nevidently end-of-stroke predictions use a di\ufb00erent set of mixture components\nfrom in-stroke predictions.\n22\n4.2\nExperiments\nEach point in the data sequences consisted of three numbers: the x and y o\ufb00set\nfrom the previous point, and the binary end-of-stroke feature.\nThe network\ninput layer was therefore size 3.\nThe co-ordinate o\ufb00sets were normalised to\nmean 0, std. dev. 1 over the training set. 20 mixture components were used\nto model the o\ufb00sets, giving a total of 120 mixture parameters per timestep\n(20 weights, 40 means, 40 standard deviations and 20 correlations). A further\nparameter was used to model the end-of-stroke probability, giving an output\nlayer of size 121.\nTwo network architectures were compared for the hidden\nlayers: one with three hidden layers, each consisting of 400 LSTM cells, and one\nwith a single hidden layer of 900 LSTM cells. Both networks had around 3.4M\nweights. The three layer network was retrained with adaptive weight noise [8],\nwith all std. devs. initialised to 0.075. Training with \ufb01xed variance weight noise\nproved ine\ufb00ective, probably because it prevented the mixture density layer from\nusing precisely speci\ufb01ed weights.\nThe networks were trained with rmsprop, a form of stochastic gradient de-\nscent where the gradients are divided by a running average of their recent mag-\nnitude [32]. De\ufb01ne \u03f5i = \u2202L(x)\n\u2202wi\nwhere wi is network weight i. The weight update\nequations were:\nni = \u2135ni + (1 \u2212\u2135)\u03f52\ni\n(38)\ngi = \u2135gi + (1 \u2212\u2135)\u03f5i\n(39)\n\u2206i = \u2136\u2206i \u2212\u05d2\u03f5i\np\nni \u2212g2\ni + \u2138\n(40)\nwi = wi + \u2206i\n(41)\nwith the following parameters:\n\u2135= 0.95\n(42)\n\u2136= 0.9\n(43)\n= \u05d20.0001\n(44)\n\u2138= 0.0001\n(45)\nThe output derivatives\n\u2202L(x)\n\u2202\u02c6yt\nwere clipped in the range [\u2212100, 100], and the\nLSTM derivates were clipped in the range [\u221210, 10]. Clipping the output gradi-\nents proved vital for numerical stability; even so, the networks sometimes had\nnumerical problems late on in training, after they had started over\ufb01tting on the\ntraining data.\nTable 3 shows that the three layer network had an average per-sequence loss\n15.3 nats lower than the one layer net.\nHowever the sum-squared-error was\nslightly lower for the single layer network.\nthe use of adaptive weight noise\nreduced the loss by another 16.7 nats relative to the unregularised three layer\nnetwork, but did not signi\ufb01cantly change the sum-squared error. The adaptive\nweight noise network appeared to generate the best samples.\n23\nTable 3: Handwriting Prediction Results. All results recorded on the val-\nidation set. \u2018Log-Loss\u2019 is the mean value of L(x) (in nats). \u2018SSE\u2019 is the mean\nsum-squared-error per data point.\nNetwork\nRegularisation\nLog-Loss\nSSE\n1 layer\nnone\n-1025.7\n0.40\n3 layer\nnone\n-1041.0\n0.41\n3 layer\nadaptive weight noise\n-1057.7\n0.41\n4.3\nSamples\nFig. 11 shows handwriting samples generated by the prediction network. The\nnetwork has clearly learned to model strokes, letters and even short words (es-\npecially common ones such as \u2018of\u2019 and \u2018the\u2019). It also appears to have learned a\nbasic character level language models, since the words it invents (\u2018eald\u2019, \u2018bryoes\u2019,\n\u2018lenrest\u2019) look somewhat plausible in English. Given that the average character\noccupies more than 25 timesteps, this again demonstrates the network\u2019s ability\nto generate coherent long-range structures.\n5\nHandwriting Synthesis\nHandwriting synthesis is the generation of handwriting for a given text. Clearly\nthe prediction networks we have described so far are unable to do this, since\nthere is no way to constrain which letters the network writes. This section de-\nscribes an augmentation that allows a prediction network to generate data se-\nquences conditioned on some high-level annotation sequence (a character string,\nin the case of handwriting synthesis). The resulting sequences are su\ufb03ciently\nconvincing that they often cannot be distinguished from real handwriting. Fur-\nthermore, this realism is achieved without sacri\ufb01cing the diversity in writing\nstyle demonstrated in the previous section.\nThe main challenge in conditioning the predictions on the text is that the two\nsequences are of very di\ufb00erent lengths (the pen trace being on average twenty\n\ufb01ve times as long as the text), and the alignment between them is unknown until\nthe data is generated. This is because the number of co-ordinates used to write\neach character varies greatly according to style, size, pen speed etc. One neural\nnetwork model able to make sequential predictions based on two sequences of\ndi\ufb00erent length and unknown alignment is the RNN transducer [9]. However\npreliminary experiments on handwriting synthesis with RNN transducers were\nnot encouraging. A possible explanation is that the transducer uses two sepa-\nrate RNNs to process the two sequences, then combines their outputs to make\ndecisions, when it is usually more desirable to make all the information avail-\nable to single network. This work proposes an alternative model, where a \u2018soft\nwindow\u2019 is convolved with the text string and fed in as an extra input to the\nprediction network. The parameters of the window are output by the network\n24\nFigure 11: Online handwriting samples generated by the prediction\nnetwork. All samples are 700 timesteps long.\n25\nat the same time as it makes the predictions, so that it dynamically determines\nan alignment between the text and the pen locations. Put simply, it learns to\ndecide which character to write next.\n5.1\nSynthesis Network\nFig. 12 illustrates the network architecture used for handwriting synthesis. As\nwith the prediction network, the hidden layers are stacked on top of each other,\neach feeding up to the layer above, and there are skip connections from the\ninputs to all hidden layers and from all hidden layers to the outputs.\nThe\ndi\ufb00erence is the added input from the character sequence, mediated by the\nwindow layer.\nGiven a length U character sequence c and a length T data sequence x, the\nsoft window wt into c at timestep t (1 \u2264t \u2264T) is de\ufb01ned by the following\ndiscrete convolution with a mixture of K Gaussian functions\n\u03c6(t, u) =\nK\nX\nk=1\n\u03b1k\nt exp\n\u0010\n\u2212\u03b2k\nt\n\u0000\u03bak\nt \u2212u\n\u00012\u0011\n(46)\nwt =\nU\nX\nu=1\n\u03c6(t, u)cu\n(47)\nwhere \u03c6(t, u) is the window weight of cu at timestep t. Intuitively, the \u03bat param-\neters control the location of the window, the \u03b2t parameters control the width of\nthe window and the \u03b1t parameters control the importance of the window within\nthe mixture. The size of the soft window vectors is the same as the size of the\ncharacter vectors cu (assuming a one-hot encoding, this will be the number of\ncharacters in the alphabet). Note that the window mixture is not normalised\nand hence does not determine a probability distribution; however the window\nweight \u03c6(t, u) can be loosely interpreted as the network\u2019s belief that it is writ-\ning character cu at time t. Fig. 13 shows the alignment implied by the window\nweights during a training sequence.\nThe size 3K vector p of window parameters is determined as follows by the\noutputs of the \ufb01rst hidden layer of the network:\n(\u02c6\u03b1t, \u02c6\u03b2t, \u02c6\u03bat) = Wh1ph1\nt + bp\n(48)\n\u03b1t = exp (\u02c6\u03b1t)\n(49)\n\u03b2t = exp\n\u0010\n\u02c6\u03b2t\n\u0011\n(50)\n\u03bat = \u03bat\u22121 + exp (\u02c6\u03bat)\n(51)\nNote that the location parameters \u03bat are de\ufb01ned as o\ufb00sets from the previous\nlocations ct\u22121, and that the size of the o\ufb00set is constrained to be greater than\nzero. Intuitively, this means that network learns how far to slide each window\nat each step, rather than an absolute location. Using o\ufb00sets was essential to\ngetting the network to align the text with the pen trace.\n26\nInputs\nCharacters\nHidden 1\nWindow\nHidden 2\nOutputs\nFigure 12: Synthesis Network Architecture Circles represent layers, solid\nlines represent connections and dashed lines represent predictions. The topology\nis similar to the prediction network in Fig. 1, except that extra input from the\ncharacter sequence c, is presented to the hidden layers via the window layer\n(with a delay in the connection to the \ufb01rst hidden layer to avoid a cycle in the\ngraph).\n27\nThought that the muster from\nFigure 13: Window weights during a handwriting synthesis sequence\nEach point on the map shows the value of \u03c6(t, u), where t indexes the pen trace\nalong the horizontal axis and u indexes the text character along the vertical axis.\nThe bright line is the alignment chosen by the network between the characters\nand the writing. Notice that the line spreads out at the boundaries between\ncharacters; this means the network receives information about next and previous\nletters as it makes transitions, which helps guide its predictions.\n28\nThe wt vectors are passed to the second and third hidden layers at time t,\nand the \ufb01rst hidden layer at time t+1 (to avoid creating a cycle in the processing\ngraph). The update equations for the hidden layers are\nh1\nt = H\n\u0000Wih1xt + Wh1h1h1\nt\u22121 + Wwh1wt\u22121 + b1\nh\n\u0001\n(52)\nhn\nt = H\n\u0000Wihnxt + Whn\u22121hnhn\u22121\nt\n+ Whnhnhn\nt\u22121 + Wwhnwt + bn\nh\n\u0001\n(53)\nThe equations for the output layer remain unchanged from Eqs. (17) to (22).\nThe sequence loss is\nL(x) = \u2212log Pr(x|c)\n(54)\nwhere\nPr(x|c) =\nT\nY\nt=1\nPr (xt+1|yt)\n(55)\nNote that yt is now a function of c as well as x1:t.\nThe loss derivatives with respect to the outputs \u02c6et, \u02c6\u03c0t, \u02c6\u00b5t, \u02c6\u03c3t, \u02c6\u03c1t remain un-\nchanged from Eqs. (27), (30) and (31). Given the loss derivative \u2202L(x)\n\u2202wt\nwith\nrespect to the size W window vector wt, obtained by backpropagating the out-\nput derivatives through the computation graph in Fig. 12, the derivatives with\nrespect to the window parameters are as follows:\n\u03f5(k, t, u)\ndef\n= \u03b1k\nt exp\n\u0010\n\u2212\u03b2k\nt\n\u0000\u03bak\nt \u2212u\n\u00012\u0011 W\nX\nj=1\n\u2202L(x)\n\u2202wj\nt\ncj\nu\n(56)\n\u2202L(x)\n\u2202\u02c6\u03b1k\nt\n=\nU\nX\nu=1\n\u03f5(k, t, u)\n(57)\n\u2202L(x)\n\u2202\u02c6\u03b2k\nt\n= \u2212\u03b2k\nt\nU\nX\nu=1\n\u03f5(k, t, u)(\u03bak\nt \u2212u)2\n(58)\n\u2202L(x)\n\u2202\u03bak\nt\n= \u2202L(x)\n\u2202\u03bak\nt+1\n+ 2\u03b2k\nt\nU\nX\nu=1\n\u03f5(k, t, u)(u \u2212\u03bak\nt )\n(59)\n\u2202L(x)\n\u2202\u02c6\u03bak\nt\n= exp\n\u0000\u02c6\u03bak\nt\n\u0001 \u2202L(x)\n\u2202\u03bak\nt\n(60)\nFig. 14 illustrates the operation of a mixture density output layer applied to\nhandwriting synthesis.\n5.2\nExperiments\nThe synthesis network was applied to the same input data as the handwriting\nprediction network in the previous section. The character-level transcriptions\nfrom the IAM-OnDB were now used to de\ufb01ne the character sequences c. The full\ntranscriptions contain 80 distinct characters (capital letters, lower case letters,\ndigits, and punctuation). However we used only a subset of 57, with all the\n29\nSynthesis Output Density\nFigure 14: Mixture density outputs for handwriting synthesis. The top\nheatmap shows the predictive distributions for the pen locations, the bottom\nheatmap shows the mixture component weights. Comparison with Fig. 10 indi-\ncates that the synthesis network makes more precise predictions (with smaller\ndensity blobs) than the prediction-only network, especially at the ends of strokes,\nwhere the synthesis network has the advantage of knowing which letter comes\nnext.\n30\nTable 4: Handwriting Synthesis Results. All results recorded on the val-\nidation set. \u2018Log-Loss\u2019 is the mean value of L(x) in nats. \u2018SSE\u2019 is the mean\nsum-squared-error per data point.\nRegularisation\nLog-Loss\nSSE\nnone\n-1096.9\n0.23\nadaptive weight noise\n-1128.2\n0.23\ndigits and most of the punctuation characters replaced with a generic \u2018non-\nletter\u2019 label2.\nThe network architecture was as similar as possible to the best prediction\nnetwork: three hidden layers of 400 LSTM cells each, 20 bivariate Gaussian\nmixture components at the output layer and a size 3 input layer. The character\nsequence was encoded with one-hot vectors, and hence the window vectors were\nsize 57. A mixture of 10 Gaussian functions was used for the window parameters,\nrequiring a size 30 parameter vector. The total number of weights was increased\nto approximately 3.7M.\nThe network was trained with rmsprop, using the same parameters as in\nthe previous section. The network was retrained with adaptive weight noise,\ninitial standard deviation 0.075, and the output and LSTM gradients were again\nclipped in the range [\u2212100, 100] and [\u221210, 10] respectively.\nTable 4 shows that adaptive weight noise gave a considerable improvement\nin log-loss (around 31.3 nats) but no signi\ufb01cant change in sum-squared error.\nThe regularised network appears to generate slightly more realistic sequences,\nalthough the di\ufb00erence is hard to discern by eye. Both networks performed\nconsiderably better than the best prediction network. In particular the sum-\nsquared-error was reduced by 44%. This is likely due in large part to the im-\nproved predictions at the ends of strokes, where the error is largest.\n5.3\nUnbiased Sampling\nGiven c, an unbiased sample can be picked from Pr(x|c) by iteratively drawing\nxt+1 from Pr (xt+1|yt), just as for the prediction network. The only di\ufb00erence is\nthat we must also decide when the synthesis network has \ufb01nished writing the text\nand should stop making any future decisions. To do this, we use the following\nheuristic: as soon as \u03c6(t, U + 1) > \u03c6(t, u) \u22001 \u2264u \u2264U the current input xt is\nde\ufb01ned as the end of the sequence and sampling ends. Examples of unbiased\nsynthesis samples are shown in Fig. 15. These and all subsequent \ufb01gures were\ngenerated using the synthesis network retrained with adaptive weight noise.\nNotice how stylistic traits, such as character size, slant, cursiveness etc. vary\n2This was an oversight; however it led to the interesting result that when the text contains\na non-letter, the network must select a digits or punctuation mark to generate. Sometimes\nthe character can be be inferred from the context (e.g. the apostrophe in \u201ccan\u2019t\u201d); otherwise\nit is chosen at random.\n31\nwidely between the samples, but remain more-or-less consistent within them.\nThis suggests that the network identi\ufb01es the traits early on in the sequence,\nthen remembers them until the end. By looking through enough samples for a\ngiven text, it appears to be possible to \ufb01nd virtually any combination of stylistic\ntraits, which suggests that the network models them independently both from\neach other and from the text.\n\u2018Blind taste tests\u2019 carried out by the author during presentations suggest\nthat at least some unbiased samples cannot be distinguished from real hand-\nwriting by the human eye. Nonetheless the network does make mistakes we\nwould not expect a human writer to make, often involving missing, confused\nor garbled letters3; this suggests that the network sometimes has trouble de-\ntermining the alignment between the characters and the trace. The number of\nmistakes increases markedly when less common words or phrases are included\nin the character sequence. Presumably this is because the network learns an\nimplicit character-level language model from the training set that gets confused\nwhen rare or unknown transitions occur.\n5.4\nBiased Sampling\nOne problem with unbiased samples is that they tend to be di\ufb03cult to read\n(partly because real handwriting is di\ufb03cult to read, and partly because the\nnetwork is an imperfect model). Intuitively, we would expect the network to\ngive higher probability to good handwriting because it tends to be smoother\nand more predictable than bad handwriting. If this is true, we should aim to\noutput more probable elements of Pr(x|c) if we want the samples to be easier to\nread. A principled search for high probability samples could lead to a di\ufb03cult\ninference problem, as the probability of every output depends on all previous\noutputs. However a simple heuristic, where the sampler is biased towards more\nprobable predictions at each step independently, generally gives good results.\nDe\ufb01ne the probability bias b as a real number greater than or equal to zero.\nBefore drawing a sample from Pr(xt+1|yt), each standard deviation \u03c3j\nt in the\nGaussian mixture is recalculated from Eq. (21) to\n\u03c3j\nt = exp\n\u0010\n\u02c6\u03c3j\nt \u2212b\n\u0011\n(61)\nand each mixture weight is recalculated from Eq. (19) to\n\u03c0j\nt =\nexp\n\u0010\n\u02c6\u03c0j\nt (1 + b)\n\u0011\nPM\nj\u2032=1 exp\n\u0010\n\u02c6\u03c0j\u2032\nt (1 + b)\n\u0011\n(62)\nThis arti\ufb01cially reduces the variance in both the choice of component from the\nmixture, and in the distribution of the component itself. When b = 0 unbiased\nsampling is recovered, and as b \u2192\u221ethe variance in the sampling disappears\n3We expect humans to make mistakes like misspelling \u2018temperament\u2019 as \u2018temperement\u2019, as\nthe second writer in Fig. 15 seems to have done.\n32\nFigure 15: Real and generated handwriting. The top line in each block is\nreal, the rest are unbiased samples from the synthesis network. The two texts\nare from the validation set and were not seen during training.\n33\nand the network always outputs the mode of the most probable component in\nthe mixture (which is not necessarily the mode of the mixture, but at least a\nreasonable approximation). Fig. 16 shows the e\ufb00ect of progressively increasing\nthe bias, and Fig. 17 shows samples generated with a low bias for the same texts\nas Fig. 15.\n5.5\nPrimed Sampling\nAnother reason to constrain the sampling would be to generate handwriting\nin the style of a particular writer (rather than in a randomly selected style).\nThe easiest way to do this would be to retrain it on that writer only.\nBut\neven without retraining, it is possible to mimic a particular style by \u2018priming\u2019\nthe network with a real sequence, then generating an extension with the real\nsequence still in the network\u2019s memory. This can be achieved for a real x, c and\na synthesis character string s by setting the character sequence to c\u2032 = c + s\nand clamping the data inputs to x for the \ufb01rst T timesteps, then sampling\nas usual until the sequence ends. Examples of primed samples are shown in\nFigs. 18 and 19. The fact that priming works proves that the network is able to\nremember stylistic features identi\ufb01ed earlier on in the sequence. This technique\nappears to work better for sequences in the training data than those the network\nhas never seen.\nPrimed sampling and reduced variance sampling can also be combined. As\nshown in Figs. 20 and 21 this tends to produce samples in a \u2018cleaned up\u2019 version\nof the priming style, with overall stylistic traits such as slant and cursiveness\nretained, but the strokes appearing smoother and more regular.\nA possible\napplication would be the arti\ufb01cial enhancement of poor handwriting.\n6\nConclusions and Future Work\nThis paper has demonstrated the ability of Long Short-Term Memory recur-\nrent neural networks to generate both discrete and real-valued sequences with\ncomplex, long-range structure using next-step prediction. It has also introduced\na novel convolutional mechanism that allows a recurrent network to condition\nits predictions on an auxiliary annotation sequence, and used this approach to\nsynthesise diverse and realistic samples of online handwriting. Furthermore, it\nhas shown how these samples can be biased towards greater legibility, and how\nthey can be modelled on the style of a particular writer.\nSeveral directions for future work suggest themselves. One is the applica-\ntion of the network to speech synthesis, which is likely to be more challenging\nthan handwriting synthesis due to the greater dimensionality of the data points.\nAnother is to gain a better insight into the internal representation of the data,\nand to use this to manipulate the sample distribution directly. It would also\nbe interesting to develop a mechanism to automatically extract high-level an-\nnotations from sequence data. In the case of handwriting, this could allow for\n34\nFigure 16: Samples biased towards higher probability. The probability\nbiases b are shown at the left. As the bias increases the diversity decreases and\nthe samples tend towards a kind of \u2018average handwriting\u2019 which is extremely\nregular and easy to read (easier, in fact, than most of the real handwriting in the\ntraining set). Note that even when the variance disappears, the same letter is\nnot written the same way at di\ufb00erent points in a sequence (for examples the \u2018e\u2019s\nin \u201cexactly the same\u201d, the \u2018l\u2019s in \u201cuntil they all look\u201d), because the predictions\nare still in\ufb02uenced by the previous outputs. If you look closely you can see that\nthe last three lines are not quite exactly the same.\n35\nFigure 17: A slight bias. The top line in each block is real. The rest are\nsamples from the synthesis network with a probability bias of 0.15, which seems\nto give a good balance between diversity and legibility.\n36\nFigure 18: Samples primed with real sequences. The priming sequences\n(drawn from the training set) are shown at the top of each block. None of the\nlines in the sampled text exist in the training set. The samples were selected\nfor legibility.\n37\nFigure 19: Samples primed with real sequences (cotd).\n38\nFigure 20: Samples primed with real sequences and biased towards\nhigher probability. The priming sequences are at the top of the blocks. The\nprobability bias was 1. None of the lines in the sampled text exist in the training\nset.\n39\nFigure 21: Samples primed with real sequences and biased towards\nhigher probability (cotd)\n40\nmore nuanced annotations than just text, for example stylistic features, di\ufb00erent\nforms of the same letter, information about stroke order and so on.\nAcknowledgements\nThanks to Yichuan Tang, Ilya Sutskever, Navdeep Jaitly, Geo\ufb00rey Hinton and\nother colleagues at the University of Toronto for numerous useful comments\nand suggestions. This work was supported by a Global Scholarship from the\nCanadian Institute for Advanced Research.\nReferences\n[1] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies\nwith gradient descent is di\ufb03cult. IEEE Transactions on Neural Networks,\n5(2):157\u2013166, March 1994.\n[2] C. Bishop. Mixture density networks. Technical report, 1994.\n[3] C. Bishop. Neural Networks for Pattern Recognition. Oxford University\nPress, Inc., 1995.\n[4] N. Boulanger-Lewandowski, Y. Bengio, and P. Vincent. Modeling tempo-\nral dependencies in high-dimensional sequences: Application to polyphonic\nmusic generation and transcription. In Proceedings of the Twenty-nine In-\nternational Conference on Machine Learning (ICML\u201912), 2012.\n[5] J. G. Cleary, Ian, and I. H. Witten. Data compression using adaptive cod-\ning and partial string matching. IEEE Transactions on Communications,\n32:396\u2013402, 1984.\n[6] D. Eck and J. Schmidhuber. A \ufb01rst look at music composition using lstm\nrecurrent neural networks. Technical report, IDSIA USI-SUPSI Instituto\nDalle Molle.\n[7] F. Gers, N. Schraudolph, and J. Schmidhuber. Learning precise timing\nwith LSTM recurrent networks. Journal of Machine Learning Research,\n3:115\u2013143, 2002.\n[8] A. Graves. Practical variational inference for neural networks. In Advances\nin Neural Information Processing Systems, volume 24, pages 2348\u20132356.\n2011.\n[9] A. Graves. Sequence transduction with recurrent neural networks. In ICML\nRepresentation Learning Worksop, 2012.\n[10] A. Graves, A. Mohamed, and G. Hinton. Speech recognition with deep\nrecurrent neural networks. In Proc. ICASSP, 2013.\n41\n[11] A. Graves and J. Schmidhuber. Framewise phoneme classi\ufb01cation with bidi-\nrectional LSTM and other neural network architectures. Neural Networks,\n18:602\u2013610, 2005.\n[12] A. Graves and J. Schmidhuber. O\ufb04ine handwriting recognition with multi-\ndimensional recurrent neural networks. In Advances in Neural Information\nProcessing Systems, volume 21, 2008.\n[13] P. D. Gr\u00a8unwald. The Minimum Description Length Principle (Adaptive\nComputation and Machine Learning). The MIT Press, 2007.\n[14] G. Hinton. A Practical Guide to Training Restricted Boltzmann Machines.\nTechnical report, 2010.\n[15] S. Hochreiter, Y. Bengio, P. Frasconi, and J. Schmidhuber. Gradient Flow\nin Recurrent Nets: the Di\ufb03culty of Learning Long-term Dependencies.\nIn S. C. Kremer and J. F. Kolen, editors, A Field Guide to Dynamical\nRecurrent Neural Networks. 2001.\n[16] S. Hochreiter and J. Schmidhuber.\nLong Short-Term Memory.\nNeural\nComputation, 9(8):1735\u20131780, 1997.\n[17] M. Hutter. The Human Knowledge Compression Contest, 2012.\n[18] K.-C. Jim, C. Giles, and B. Horne. An analysis of noise in recurrent neural\nnetworks: convergence and generalization. Neural Networks, IEEE Trans-\nactions on, 7(6):1424 \u20131438, 1996.\n[19] S. Johansson, R. Atwell, R. Garside, and G. Leech. The tagged LOB corpus\nuser\u2019s manual; Norwegian Computing Centre for the Humanities, 1986.\n[20] B. Knoll and N. de Freitas. A machine learning perspective on predictive\ncoding with paq. CoRR, abs/1108.3298, 2011.\n[21] M. Liwicki and H. Bunke.\nIAM-OnDB - an on-line English sentence\ndatabase acquired from handwritten text on a whiteboard. In Proc. 8th\nInt. Conf. on Document Analysis and Recognition, volume 2, pages 956\u2013\n961, 2005.\n[22] M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. Building a large\nannotated corpus of english: The penn treebank.\nCOMPUTATIONAL\nLINGUISTICS, 19(2):313\u2013330, 1993.\n[23] T. Mikolov. Statistical Language Models based on Neural Networks. PhD\nthesis, Brno University of Technology, 2012.\n[24] T. Mikolov, I. Sutskever, A. Deoras, H. Le, S. Kombrink, and J. Cernocky.\nSubword language modeling with neural networks. Technical report, Un-\npublished Manuscript, 2012.\n42\n[25] A. Mnih and G. Hinton.\nA Scalable Hierarchical Distributed Language\nModel. In Advances in Neural Information Processing Systems, volume 21,\n2008.\n[26] A. Mnih and Y. W. Teh. A fast and simple algorithm for training neural\nprobabilistic language models.\nIn Proceedings of the 29th International\nConference on Machine Learning, pages 1751\u20131758, 2012.\n[27] T. N. Sainath, A. Mohamed, B. Kingsbury, and B. Ramabhadran. Low-\nrank matrix factorization for deep neural network training with high-\ndimensional output targets. In Proc. ICASSP, 2013.\n[28] M. Schuster. Better generative models for sequential data problems: Bidi-\nrectional recurrent mixture density networks.\npages 589\u2013595. The MIT\nPress, 1999.\n[29] I. Sutskever, G. E. Hinton, and G. W. Taylor. The recurrent temporal\nrestricted boltzmann machine. pages 1601\u20131608, 2008.\n[30] I. Sutskever, J. Martens, and G. Hinton. Generating text with recurrent\nneural networks. In ICML, 2011.\n[31] G. W. Taylor and G. E. Hinton. Factored conditional restricted boltzmann\nmachines for modeling motion style. In Proc. 26th Annual International\nConference on Machine Learning, pages 1025\u20131032, 2009.\n[32] T. Tieleman and G. Hinton. Lecture 6.5 - rmsprop: Divide the gradient by\na running average of its recent magnitude, 2012.\n[33] R. Williams and D. Zipser. Gradient-based learning algorithms for recur-\nrent networks and their computational complexity. In Back-propagation:\nTheory, Architectures and Applications, pages 433\u2013486. 1995.\n43\n",
        "sentence": "",
        "context": "{{Linux}}\nl these in the rational framing.                                                \n                                                                                \n==Gentiles==\n[[fi:Italo Colombonasto Wark\u014d]]                                                 \n[[sv:Opin den mail-climbere panczeur]]                                          \n[[zh:\u0011\t\f\u0002\b\u0017\u0007]]</text>"
    },
    {
        "title": "Towards end-to-end speech recognition with recurrent neural networks",
        "author": [
            "Graves",
            "Alex",
            "Jaitly",
            "Navdeep"
        ],
        "venue": "In ICML, pp",
        "citeRegEx": "Graves et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Graves et al\\.",
        "year": 2014,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "A novel connectionist system for unconstrained handwriting recognition",
        "author": [
            "Graves",
            "Alex",
            "Liwicki",
            "Marcus",
            "Fern\u00e1ndez",
            "Santiago",
            "Bertolami",
            "Roman",
            "Bunke",
            "Horst",
            "Schmidhuber",
            "J\u00fcrgen"
        ],
        "venue": null,
        "citeRegEx": "Graves et al\\.,? \\Q2009\\E",
        "shortCiteRegEx": "Graves et al\\.",
        "year": 2009,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " , 2015) or more complex recurrent units such as Long Short-Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997; Graves et al., 2009) or Gated Recurrent Units (GRU) (Chung et al.",
        "context": null
    },
    {
        "title": "A tractable inference algorithm for diagnosing multiple diseases",
        "author": [
            "Heckerman",
            "David"
        ],
        "venue": "In UAI,",
        "citeRegEx": "Heckerman and David.,? \\Q1990\\E",
        "shortCiteRegEx": "Heckerman and David.",
        "year": 1990,
        "abstract": "We examine a probabilistic model for the diagnosis of multiple diseases. In\nthe model, diseases and findings are represented as binary variables. Also,\ndiseases are marginally independent, features are conditionally independent\ngiven disease instances, and diseases interact to produce findings via a noisy\nOR-gate. An algorithm for computing the posterior probability of each disease,\ngiven a set of observed findings, called quickscore, is presented. The time\ncomplexity of the algorithm is O(nm-2m+), where n is the number of diseases, m+\nis the number of positive findings and m- is the number of negative findings.\nAlthough the time complexity of quickscore i5 exponential in the number of\npositive findings, the algorithm is useful in practice because the number of\nobserved positive findings is usually far less than the number of diseases\nunder consideration. Performance results for quickscore applied to a\nprobabilistic version of Quick Medical Reference (QMR) are provided.",
        "full_text": "A Tractable Inference Algorithm for Diagnosing\nMultiple Diseases\u2217\nDavid Heckerman\nMedical Computer Science Group\nKnowledge Systems Laboratory\nDepartments of Computer Science and Medicine\nStanford, California 94305\nDecember 7, 2022\nAbstract\nWe examine a probabilistic model for the diagnosis of multiple diseases. In the model, diseases\nand \ufb01ndings are represented as binary variables. Also, diseases are marginally independent,\nfeatures are conditionally independent given disease instances, and diseases interact to produce\n\ufb01ndings via a noisy or-gate.\nAn algorithm for computing the posterior probability of each\ndisease, given a set of observed \ufb01ndings, called quickscore, is presented. The time complexity\nof the algorithm is O(nm\u22122m+), where n is the number of diseases, m+ is the number of\npositive \ufb01ndings and m\u2212is the number of negative \ufb01ndings. Although the time complexity of\nquickscore is exponential in the number of positive \ufb01ndings, the algorithm is useful in practice\nbecause the number of observed positive \ufb01ndings is usually far less than the number of diseases\nunder consideration. Performance results for quickscore applied to a probabilistic version of\nQuick Medical Reference (QMR) are provided.\nCorrections to the original text are in red. Thanks to Max Zhao for \ufb01nding the error.\n1\nIntroduction\nOne of the most common criticisms of the use of probability theory in expert systems is that the\ntheory is impractical to apply in realistic situations [Buchanan and Shortli\ufb00e, 1984]. In attempts to\nanswer this criticism, several researchers at Stanford, myself included, have undertaken the task of\nconverting to a probabilistic framework Quick Medical Reference (QMR) [Miller, 1987], one of the\nlargest medical expert systems in existence.\nWe have made a straightforward and tractable transformation of the QMR knowledge base\nto a probabilistic model [Heckerman and Miller, 1986, Henrion, 1990, Shwe et al., 1990]. Like the\nheuristic algorithms in QMR, the model allows for any combination of diseases to be present in a\npatient. Unfortunately, this feature leads to a time complexity for inferring the probability of each\ndisease given a set of \ufb01ndings that is exponential in the number of diseases (O(2n)) for all known\nalgorithms. As there are over 600 diseases in QMR, these algorithms are intractable. Furthermore,\nthis problem is known to be NP-hard [Cooper, 1990].\nIn this paper, I present quickscore, an algorithm with time complexity that is exponential in the\nnumber of positive \ufb01ndings (\ufb01ndings observed to be present rather than absent). More precisely, the\n\u2217This work was supported by the NSF under Grant IRI-8703710, and by the NLM under Grant R01-LM04529.\n1\narXiv:1304.1511v2  [cs.AI]  5 Dec 2022\nFigure 1: A belief network for diagnosing multiple diseases. Diseases are marginally independent.\nFindings are conditionally independent given a disease instance.\nalgorithm has a time complexity of O(nm\u22122m+), where m+ is the number of positive \ufb01ndings and\nm\u2212is the number of negative \ufb01ndings. Although quickscore has an exponential time complexity,\nit is useful in practice because the number of observed positive \ufb01ndings is often far less than the\nnumber of diseases under consideration. For many realistic patient cases, quickscore implemented\non a Macintosh II produces an answer in less than 1 minute of real time.\n2\nThe QMR model\nThe probabilistic version of QMR is called QMR-DT for Decision-Theoretic QMR. A belief network\nfor QMR-DT, is shown in Figure 1. Each of the n nodes in the upper layer of the network represents\na disease that may be present or absent in a patient.\nEach of the m nodes in the lower layer\nrepresents a \ufb01nding that may be observed to be present or absent in the patient, or that may not\nbe observed at all. The problem of interest is to compute the probability of each disease given a set\nof positive and negative \ufb01ndings.\nAs indicated by the network in Figure 1, we assume diseases to be marginally independent. Also,\nwe assume that \ufb01ndings are conditionally independent given any instance of the set of diseases. An\ninstance of a set of diseases is an assignment of present or absent to each disease in that set.\nWe also assume that diseases act independently to cause any given \ufb01nding to be present. A\nbelief network that represents this independency for two diseases is shown in Figure 2. The node\nlabeled di\u2013causes\u2013f represents the event that di causes \ufb01nding f to be present. The node with the\ndouble boundary, labeled f, is a deterministic node that says \ufb01nding f will be present if and only if\nd1 causes f to be present, d2 causes f to be present, or both d1 and d2 cause f to be present. The\narc between the node labeled d1 and the node labeled d1\u2013causes\u2013f re\ufb02ects our assumption that the\npresence or absence of d1 in\ufb02uences the probability that d1 causes f to be present. In particular, we\nassume that, if d1 is present, it may cause f to be present with some probability. If d1 is absent, we\nassume that the disease cannot act to cause f. The same set of assumptions holds for the disease\nd2. Finally, the lack of arcs between nodes in the upper two rows of the \ufb01gure re\ufb02ects the assertions\nthat (1) the probability distribution for the variable d1\u2013causes\u2013f depends neither on the absence\nor presence of d2 nor on the absence or presence of the event d2\u2013causes\u2013f, and (2) the probability\ndistribution for the variable d2\u2013causes\u2013f depends neither on the absence or presence of d1 nor on\nthe absence or presence of the event d1\u2013causes\u2013f. We call this form of conditional independence\ncausal independence to distinguish it from the type of conditional independence that is commonly\nrepresented in belief networks.\nUnder these assumptions, we can compute the probability of f given any instance of the pair\n{d1, d2} from the two assessments p1 and p2, where pi is the probability that di causes f, i = 1, 2.\n2\nFigure 2: A belief network for the noisy or-gate. If disease d1 is present, then with some probability\nit will act to cause \ufb01nding f. This probability depends neither on the state of d2 nor on whether\nd2 acts to cause f. A reciprocal relationship holds for disease d2. If either disease acts to cause f,\nthen f will be observed to be present.\nIf both diseases are absent, \ufb01nding f must be absent. If only one disease is present\u2014say, di\u2014then\np(f +|only di) = pi\nand\np(f \u2212|only di) = 1 \u2212pi\nwhere f + and f \u2212denote the presence and absence of \ufb01nding f, respectively. That is, the probability\nthat di causes f is just the probability of f given that only disease di is present. If both d1 and d2\nare present, \ufb01nding f will be absent only if both diseases fail to cause f to be present. Therefore,\np(f \u2212|d+\n1 , d+\n2 ) = (1 \u2212p1)(1 \u2212p2) = p(f \u2212|only d1)p(f \u2212|only d2)\nwhere d+\ni denotes the presence of disease di.\nMore generally, suppose that n diseases can potentially cause \ufb01nding f, and that these n diseases\nare the only causes of f. As in the simpler case,\np(f +|only di) = pi\nwhere pi is the probability that di causes f. Let Di denote a particular instance of the n diseases\nand let D+\ni denote the set of diseases that are present in the instance Di. Given instance Di, f will\nbe absent only if none of the diseases in D+\ni cause f to be present. It follows that\np(f \u2212|Di) =\nY\nd\u2208D+\ni\np(f \u2212|only d)\n(1)\nThus, using Equation 1, we can compute the probability of f given any of the 2n disease instances\nfrom only n assessments.\nThe assumptions underlying Equation 1, including the assumption of causal independence, have\nbeen described previously [Good, 1961], and several researchers have suggested that these assump-\ntions be used to model medical domains [Habbema, 1976, Peng and Reggia, 1986]. Pearl has called\nthis canonical model of cause and e\ufb00ect the noisy or-gate [Kim and Pearl, 1983, Pearl, 1986]. Other\n3\nresearchers have extended the model to accommodate situations where a \ufb01nding may appear in the\nabsence of all explicitly represented diseases, a leaky or-gate [Suppes, 1970, Henrion, 1987].\nThe leaky or-gate model was assumed implicitly by the developers of the QMR expert system\n[Miller, 1987]. In conjunction with the independence assumptions shown in Figure 1, the model made\ntractable the transformation of QMR into a probabilistic framework. As we see in the following\nsection, these assumptions also accommodate an inference algorithm that is tractable for many\npatient cases. In Section 5, we examine the appropriateness of these assumptions, and discuss how\nthey might be relaxed.\n3\nThe Quickscore Algorithm\nThe goal of quickscore is to compute the probability of each disease di, i = 1, 2, . . . n, given a\nset of positive \ufb01ndings F + and a set of negative \ufb01ndings F \u2212, under the assumptions described in\nthe previous section.1 To understand how quickscore works, we \ufb01rst compute the probability that a\nsingle \ufb01nding f will be absent. Using the expansion rule, we get\np(f \u2212) =\nX\nDk\u2208D\np(f \u2212|Dk)p(Dk)\n(2)\nwhere D is the set of all disease instances.\nUsing the assumption that diseases are marginally\nindependent and the assumptions underlying the noisy or-gate, Equation 2 becomes\np(f \u2212) =\nX\nDk\u2208D\n\uf8ee\n\uf8f0Y\nd\u2208D+\nk\np(f \u2212|only d)\nY\nd\u2208D+\nk\np(d+)\nY\nd\u2208D\u2212\nk\np(d\u2212)\n\uf8f9\n\uf8fb\n(3)\nwhere D\u2212\nk denotes the set of diseases that are absent in the instance Dk.\nNow consider the expression\nn\nY\ni=1\n\u0002\np(f \u2212|only di)p(d+\ni ) + p(d\u2212\ni )\n\u0003\n(4)\nIf we multiply out this expression, we see that it is just the right-hand side of Equation 3. Thus, we\nobtain\np(f \u2212) =\nn\nY\ni=1\n\u0002\np(f \u2212|only di)p(d+\ni ) + p(d\u2212\ni )\n\u0003\n(5)\nThe di\ufb00erence in time complexity between the computations of Equations 3 and 5 is striking. The\ncomputation in Equation 3 is a sum over 2n terms, whereas the computation in Equation 5 is a\nlinear product over n sums. Pearl [Pearl, 1988, pp. 187-188] was the \ufb01rst to note the equivalence\nbetween these two computations. As we shall see, quickscore derives its speed from this equivalence.\nUnder the assumption that \ufb01ndings are conditionally independent given any disease instance, we\ncan employ the transformation described in the previous paragraph to compute the probability that\nthe set of negative \ufb01ndings, F \u2212, are observed. We have\np(F \u2212) =\nn\nY\ni=1\n\uf8eb\n\uf8ed\n\uf8ee\n\uf8f0Y\nf\u2208F \u2212\np(f \u2212|only di)\n\uf8f9\n\uf8fbp(d+\ni ) + p(d\u2212\ni )\n\uf8f6\n\uf8f8\n(6)\n1Here, we develop the algorithm for nonleaky or-gates. We can, however, extend easily the development to include\nleaky or-gates.\n4\nThe situation is more complex for positive \ufb01ndings. Let us \ufb01rst examine the simple case where\nF + = {f1, f2}. Applying the expansion rule to p(f +\n1 , f +\n2 ) and using the assumption of conditional\nindependence of \ufb01ndings, we get\np(f +\n1 , f +\n2 ) =\nX\nDk\u2208D\np(f +\n1 |Dk)p(f +\n2 |Dk)p(Dk)\n(7)\nSince p(f +\nj |Dk) = 1 \u2212p(f \u2212\nj |Dk), Equation 7 becomes\np(f +\n1 , f +\n2 )\n=\nX\nDk\u2208D\np(Dk) \u2212\nX\nDk\u2208D\np(f \u2212\n1 |Dk)p(Dk) \u2212\nX\nDk\u2208D\np(f \u2212\n2 |Dk)p(Dk) +\nX\nDk\u2208D\np(f \u2212\n1 |Dk)p(f \u2212\n2 |Dk)p(Dk)\n(8)\nThe \ufb01rst sum in Equation 8 is equal to 1. The remaining terms are in the same form as the right-hand\nside of Equation 2. Thus, using the algebraic transformations derived previously, we obtain\np(f +\n1 , f +\n2 )\n=\n1 \u2212\nn\nY\ni=1\n\u0002\np(f \u2212\n1 |only di)p(d+\ni ) + p(d\u2212\ni )\n\u0003\n\u2212\nn\nY\ni=1\n\u0002\np(f \u2212\n2 |only di)p(d+\ni ) + p(d\u2212\ni )\n\u0003\n+\nn\nY\ni=1\n\u0002\np(f \u2212\n1 |only di)p(f \u2212\n2 |only di)p(d+\ni ) + p(d\u2212\ni )\n\u0003\n(9)\nMore generally,\np(F +) =\nX\nF \u2032\u22082F +\n(\u22121)|F\n\u2032|\nn\nY\ni=1\n\uf8eb\n\uf8ed\n\uf8ee\n\uf8f0Y\nf\u2208F \u2032\np(f \u2212|only di)\n\uf8f9\n\uf8fbp(d+\ni ) + p(d\u2212\ni )\n\uf8f6\n\uf8f8\n(10)\nwhere 2F + denotes the power set of F +, and |F\n\u2032| denotes the number of elements in set F\n\u2032.\nIn the most general case where some \ufb01ndings are present and some are absent, we can combine\nEquation 6 and Equation 9 to obtain\np(F +, F \u2212) =\nX\nF \u2032\u22082F +\n(\u22121)|F\n\u2032|\nn\nY\ni=1\n\uf8eb\n\uf8ed\n\uf8ee\n\uf8f0\nY\nf\u2208F \u2032\u222aF \u2212\np(f \u2212|only di)\n\uf8f9\n\uf8fbp(d+\ni ) + p(d\u2212\ni )\n\uf8f6\n\uf8f8\n(11)\nIt is now a simple matter to compute p(d+\ni |F +, F \u2212).\nFirst, we compute p(F +, F \u2212).\nThen, we\ncompute p(F +, F \u2212|d+\ni ) by setting p(d+\ni ) = 1 and p(d\u2212\ni ) = 0 in Equation 11.\nThe sought-after\nprobability is then\np(d+\ni |F +, F \u2212) = p(F +, F \u2212|d+\ni ) p(d+\ni )\np(F +, F \u2212)\n.\n(12)\nThe quickscore algorithm can provide intermediate results. In particular, suppose that we order\nthe \ufb01ndings in F +\u2014say, f1, f2, . . . fm+. In Equation 11, we can \ufb01rst compute the term in the power\n5\nFigure 3: Run times for quickscore. Run times for quickscore implemented in LightSpeed Pascal on\na Macintosh II are plotted against the number of positive \ufb01ndings in a case. Nine positive \ufb01ndings\ntypically can be scored in less than 1 minute.\nset of F + corresponding to f1 alone. We can then compute the terms in the power set that correspond\nto combinations of only f1 and f2. Continuing in this way, the probability of each d+\ni given the \ufb01rst\nj \ufb01ndings can be recovered from the algorithm at any time, where time is an exponential function of\nj. The fact that quickscore can provide intermediate results may prove useful. The QMR knowledge\nbase contains a partial ordering of \ufb01ndings by their general clinical importance in determining a\ndiagnosis. This ordering might be used to degrade gracefully the performance of quickscore with\nincreasing time constraints.\n4\nRun-Time Performance of Quickscore\nQuickscore has been implemented in Lightspeed Pascal on the Macintosh II computer. Figure 3\nshows the run time of the algorithm for cases of various size. The cases are taken from a library\nof classic cases used by the QMR research team to test periodically the diagnostic accuracy of the\nheuristic knowledge base. These cases contain only positive \ufb01ndings. As the graph indicates, nine\n\ufb01ndings can typically be scored in less than 1 minute. Overall, in 25 percent of the 400 cases in the\nlibrary, quickscore requires 15 minutes or less to score each case.\n6\n5\nWeaknesses of the Algorithm\nThe diagnostic model described in this paper contains several assumptions that may not be ap-\npropriate for many medical and nonmedical domains.\nFor example, some diseases and \ufb01ndings\ncan occur with di\ufb00erent degrees of severity, and hence are not two-valued.\nIn addition, certain\ndiseases cause others to be present. Consequently, all diseases are not marginally independent. Fur-\nthermore, some \ufb01ndings are conditionally dependent, and certain diseases are caused by \ufb01ndings\nrather than vice-versa (e.g., a history of alcoholism tends to cause cirrhosis of the liver).\nNone\nof these observations pose a signi\ufb01cant barrier to the translation of the QMR\u2019s knowledge base\nto a probabilistic framework. For example, several researchers have generalized the noisy or-gate\nmodel, constructing other prototypic models that embody the assumption of causal independence\n[Kim and Pearl, 1983, Heckerman, 1988, Henrion, 1987]. We can use these prototypic models to ac-\ncommodate multiple-valued diseases and \ufb01ndings, and to accommodate causal interactions among\ndiseases. Also, by introducing hidden or unobservable pathophysiologic states, we can capture many\nof the dependencies among \ufb01ndings. Again, we can use the noisy or-gate and its extensions to\nmodel interactions between diseases and pathophysiologic states. Unfortunately, we cannot extend\nquickscore in a straightforward manner to treat these extensions to the current QMR-DT model.\nIn addition, quickscore is unstable numerically when p(F +, F \u2212)\u2014the probability of observations\nfor a given case\u2014is small, because p(F +, F \u2212) is a sum of positive and negative terms on the order of\n100 (see Equation 11). IEEE-standard extended-precision arithmetic on the Macintosh II provides\nabout 19 decimal places of precision. Thus, inferences using the current implementation of quickscore\n(or any implementation on hardware that uses the IEEE standard) are unreliable when p(F +, F \u2212)\nis less than approximately 10\u221219. Typically, for the current QMR-DT model, such situations arise\nwhen the number of positive \ufb01ndings exceeds 15.\n6\nConclusion\nDespite its shortcomings, quickscore has been useful in the development of QMR- DT. For example,\nthe QMR-DT group recently has experimented with several approximation algorithms for inference\nthat are based on Monte-Carlo techniques [Shwe et al., 1990]. These algorithms are suited to infer-\nence in the extensions of the QMR-DT model discussed in the previous section, but their convergence\nproperties are not well characterized. Quickscore has provided gold standards for evaluating such\nconvergence properties in the context of the current model. In general, quickscore is likely to be a\nuseful tool for knowledge engineers who develop decision-theoretic expert systems.\n7\nAcknowledgments\nI thank Gregory Copper for encouraging me to implement the quickscore algorithm, Michael Shwe\nfor assisting with the implementation, the entire QMR-DT group at Stanford for useful discussions,\nand Lyn Dupre for reviewing several drafts of this manuscript.\nReferences\n[Buchanan and Shortli\ufb00e, 1984] Buchanan, B. and Shortli\ufb00e, E., editors (1984). Rule-Based Expert\nSystems: The MYCIN Experiments of the Stanford Heuristic Programming Project. Addison\u2013\nWesley, Reading, MA.\n[Cooper, 1990] Cooper, G. (1990). The computational complexity of probabilistic inference using\nBayesian belief networks. Arti\ufb01cial Intelligence, 42:393\u2013405.\n7\n[Good, 1961] Good, I. (1961). A causal calculus (I). British Journal of Philosophy of Science, 11:305\u2013\n318. Also in I.J. Good, Good Thinking: The Foundations of Probability and Its Applications, pages\n197\u2013217. University of Minnesota Press, Minneapolis, MN, 1983.\n[Habbema, 1976] Habbema, J. (1976). Models for diagnosis and detection of combinations of dis-\neases. In de Dombal, F. and Gremy, F., editors, Decision Making and Medical Care, pages 399\u2013411.\nNorth-Holland, New York.\n[Heckerman, 1988] Heckerman, D. (1988). Formalizing heuristic methods for reasoning with un-\ncertainty. Technical Report KSL-88-07, Medical Computer Science Group, Section on Medical\nInformatics, Stanford University, Stanford, CA.\n[Heckerman and Miller, 1986] Heckerman, D. and Miller, R. (1986). Towards a better understanding\nof the INTERNIST-1 knowledge base. In Proceedings of Medinfo, Washington, DC, pages 27\u201331.\nNorth-Holland, New York.\n[Henrion, 1987] Henrion, M. (1987). Some practical issues in constructing belief networks. In Pro-\nceedings of the Third Workshop on Uncertainty in Arti\ufb01cial Intelligence, Seattle, WA, pages 132\u2013\n139. Association for Uncertainty in Arti\ufb01cial Intelligence, Mountain View, CA. Also in Kanal,\nL., Levitt, T., and Lemmer, J., editors, Uncertainty in Arti\ufb01cial Intelligence 3, pages 161\u2013174.\nNorth-Holland, New York, 1989.\n[Henrion, 1990] Henrion, M. (1990). Towards e\ufb03cient probabilistic inference in multiply connected\nbelief networks. In Oliver, R. and Smith, J., editors, In\ufb02uence Diagrams, Belief Nets, and Decision\nAnalysis, chapter 17. Wiley and Sons, New York.\n[Kim and Pearl, 1983] Kim, J. and Pearl, J. (1983). A computational model for causal and diag-\nnostic reasoning in inference engines. In Proceedings Eighth International Joint Conference on\nArti\ufb01cial Intelligence, Karlsruhe, West Germany, pages 190\u2013193. International Joint Conference\non Arti\ufb01cial Intelligence.\n[Miller, 1987] Miller, R. (1987). Personal communication.\n[Pearl, 1986] Pearl, J. (1986). Fusion, propagation, and structuring in belief networks. Arti\ufb01cial\nIntelligence, 29:241\u2013288.\n[Pearl, 1988] Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible\nInference. Morgan Kaufmann, San Mateo, CA.\n[Peng and Reggia, 1986] Peng, Y. and Reggia, J. (1986). Plausibility of diagnostic hypotheses. In\nProceedings AAAI-86 Fifth National Conference on Arti\ufb01cial Intelligence, Philadelphia, PA, pages\n140\u2013145. AAAI Press, Menlo Park, CA.\n[Shwe et al., 1990] Shwe, M., Middleton, B., Heckerman, D., Henrion, M., Horvitz, E., Lehmann,\nH., and Cooper, G. (1990). A probabilistic reformulation of the Quick Medical Reference System.\nTechnical Report KSL-90-11, Stanford University, Stanford, CA.\n[Suppes, 1970] Suppes, P., editor (1970). A Probabilistic Theory of Causality. North-Holland, New\nYork.\n8\n",
        "sentence": "",
        "context": "[Suppes, 1970] Suppes, P., editor (1970). A Probabilistic Theory of Causality. North-Holland, New\nYork.\n8\nnumber of positive \ufb01ndings (\ufb01ndings observed to be present rather than absent). More precisely, the\n\u2217This work was supported by the NSF under Grant IRI-8703710, and by the NLM under Grant R01-LM04529.\n1\narXiv:1304.1511v2  [cs.AI]  5 Dec 2022\nBayesian belief networks. Arti\ufb01cial Intelligence, 42:393\u2013405.\n7\n[Good, 1961] Good, I. (1961). A causal calculus (I). British Journal of Philosophy of Science, 11:305\u2013"
    },
    {
        "title": "Long short-term memory",
        "author": [
            "Hochreiter",
            "Sepp",
            "Schmidhuber",
            "J\u00fcrgen"
        ],
        "venue": "Neural computation,",
        "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E",
        "shortCiteRegEx": "Hochreiter et al\\.",
        "year": 1997,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Bayesian nonparametric hidden semi-markov models",
        "author": [
            "Johnson",
            "Matthew J",
            "Willsky",
            "Alan S"
        ],
        "venue": "The Journal of Machine Learning Research,",
        "citeRegEx": "Johnson et al\\.,? \\Q2013\\E",
        "shortCiteRegEx": "Johnson et al\\.",
        "year": 2013,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Some different types of essential hypertension: their course and prognosis",
        "author": [
            "Keith",
            "Norman M",
            "Wagener",
            "Henry P",
            "Barker",
            "Nelson W"
        ],
        "venue": "The American Journal of the Medical Sciences,",
        "citeRegEx": "Keith et al\\.,? \\Q1939\\E",
        "shortCiteRegEx": "Keith et al\\.",
        "year": 1939,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " In the fourth case, visual disturbances can be associated with migraines and essential hypertension (Keith et al., 1939).",
        "context": null
    },
    {
        "title": "Unifying visual-semantic embeddings with multimodal neural language models",
        "author": [
            "Kiros",
            "Ryan",
            "Salakhutdinov",
            "Ruslan",
            "Zemel",
            "Richard S"
        ],
        "venue": "arXiv preprint arXiv:1411.2539,",
        "citeRegEx": "Kiros et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Kiros et al\\.",
        "year": 2014,
        "abstract": "",
        "full_text": "",
        "sentence": " (Graves, 2013; Graves & Jaitly, 2014; Sutskever et al., 2014; Kiros et al., 2014; Zaremba & Sutskever, 2014).",
        "context": null
    },
    {
        "title": "Essential hypertension and cognitive function",
        "author": [
            "Kuusisto",
            "Johanna",
            "Koivisto",
            "Keijo",
            "L Mykk\u00e4nen",
            "Helkala",
            "Eeva-Liisa",
            "Vanhanen",
            "Matti",
            "T H\u00e4nninen",
            "K Py\u00f6r\u00e4l\u00e4",
            "Riekkinen",
            "Paavo",
            "Laakso",
            "Markku"
        ],
        "venue": "the role of hyperinsulinemia. Hypertension,",
        "citeRegEx": "Kuusisto et al\\.,? \\Q1993\\E",
        "shortCiteRegEx": "Kuusisto et al\\.",
        "year": 1993,
        "abstract": "",
        "full_text": "",
        "sentence": " Further, essential hypertension may be linked to cognitive function (Kuusisto et al., 1993), which plays a role in anxiety disorders and dissociative and somatoform disorders.",
        "context": null
    },
    {
        "title": "Latent Continuous Time Markov Chains for Partially-Observed Multistate Disease Processes",
        "author": [
            "Lange",
            "Jane"
        ],
        "venue": "PhD thesis,",
        "citeRegEx": "Lange and Jane.,? \\Q2014\\E",
        "shortCiteRegEx": "Lange and Jane.",
        "year": 2014,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "A joint model for multistate disease processes and random informative observation times, with applications to electronic medical records",
        "author": [
            "Lange",
            "Jane M",
            "Hubbard",
            "Rebecca A",
            "Inoue",
            "Lurdes YT",
            "Minin",
            "Vladimir N"
        ],
        "venue": "data. Biometrics,",
        "citeRegEx": "Lange et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "Lange et al\\.",
        "year": 2015,
        "abstract": "",
        "full_text": "",
        "sentence": " (Heckerman, 1990; Chapman et al., 2001; Lange et al., 2015), most works do not achieve the required accuracy and scalability, lack generality, or need excessive expert domain knowledge. The two main class of techniques, continuous-time Markov chain based models (Nodelman et al., 2002; Lange et al., 2015; Johnson & Willsky, 2013), and intensity based point process modeling techniques such as Hawkes processes (Liniger, 2009; Zhu, 2013; Choi et al.",
        "context": null
    },
    {
        "title": "A simple way to initialize recurrent networks of rectified linear units",
        "author": [
            "Le",
            "Quoc V",
            "Jaitly",
            "Navdeep",
            "Hinton",
            "Geoffrey E"
        ],
        "venue": "arXiv preprint arXiv:1504.00941,",
        "citeRegEx": "Le et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "Le et al\\.",
        "year": 2015,
        "abstract": "Learning long term dependencies in recurrent networks is difficult due to\nvanishing and exploding gradients. To overcome this difficulty, researchers\nhave developed sophisticated optimization techniques and network architectures.\nIn this paper, we propose a simpler solution that use recurrent neural networks\ncomposed of rectified linear units. Key to our solution is the use of the\nidentity matrix or its scaled version to initialize the recurrent weight\nmatrix. We find that our solution is comparable to LSTM on our four benchmarks:\ntwo toy problems involving long-range temporal structures, a large language\nmodeling problem and a benchmark speech recognition problem.",
        "full_text": "arXiv:1504.00941v2  [cs.NE]  7 Apr 2015\nA Simple Way to Initialize Recurrent Networks of\nRecti\ufb01ed Linear Units\nQuoc V. Le, Navdeep Jaitly, Geoffrey E. Hinton\nGoogle\nAbstract\nLearning long term dependencies in recurrent networks is dif\ufb01cult due to van-\nishing and exploding gradients. To overcome this dif\ufb01culty, researchers have de-\nveloped sophisticated optimization techniques and network architectures. In this\npaper, we propose a simpler solution that use recurrent neural networks composed\nof recti\ufb01ed linear units. Key to our solution is the use of the identity matrix or its\nscaled version to initialize the recurrent weight matrix. We \ufb01nd that our solution is\ncomparable to a standard implementation of LSTMs on our four benchmarks: two\ntoy problems involving long-range temporal structures, a large language modeling\nproblem and a benchmark speech recognition problem.\n1\nIntroduction\nRecurrent neural networks (RNNs) are very powerful dynamical systems and they are the natural\nway of using neural networks to map an input sequence to an output sequence, as in speech recog-\nnition and machine translation, or to predict the next term in a sequence, as in language modeling.\nHowever, training RNNs by using back-propagation through time [30] to compute error-derivatives\ncan be dif\ufb01cult. Early attempts suffered from vanishing and exploding gradients [15] and this meant\nthat they had great dif\ufb01culty learning long-term dependencies. Many different methods have been\nproposed for overcoming this dif\ufb01culty.\nA method that has produced some impressive results [23, 24] is to abandon stochastic gradient\ndescent in favor of a much more sophisticated Hessian-Free (HF) optimization method. HF operates\non large mini-batches and is able to detect promising directions in the weight-space that have very\nsmall gradients but even smaller curvature. Subsequent work, however, suggested that similar results\ncould be achieved by using stochastic gradient descent with momentum provided the weights were\ninitialized carefully [34] and large gradients were clipped [28]. Further developments of the HF\napproach look promising [35, 25] but are much harder to implement than popular simple methods\nsuch as stochastic gradient descent with momentum [34] or adaptive learning rates for each weight\nthat depend on the history of its gradients [5, 14].\nThe most successful technique to date is the Long Short Term Memory (LSTM) Recurrent Neural\nNetwork which uses stochastic gradient descent, but changes the hidden units in such a way that\nthe backpropagated gradients are much better behaved [16]. LSTM replaces logistic or tanh hidden\nunits with \u201cmemory cells\u201d that can store an analog value. Each memory cell has its own input and\noutput gates that control when inputs are allowed to add to the stored analog value and when this\nvalue is allowed to in\ufb02uence the output. These gates are logistic units with their own learned weights\non connections coming from the input and also the memory cells at the previous time-step. There is\nalso a forget gate with learned weights that controls the rate at which the analog value stored in the\nmemory cell decays. For periods when the input and output gates are off and the forget gate is not\ncausing decay, a memory cell simply holds its value over time so the gradient of the error w.r.t. its\nstored value stays constant when backpropagated over those periods.\n1\nThe \ufb01rst major success of LSTMs was for the task of unconstrained handwriting recognition [12].\nSince then, they have achieved impressive results on many other tasks including speech recogni-\ntion [13, 10], handwriting generation [8], sequence to sequence mapping [36], machine transla-\ntion [22, 1], image captioning [38, 18], parsing [37] and predicting the outputs of simple computer\nprograms [39].\nThe impressive results achieved using LSTMs make it important to discover which aspects of the\nrather complicated architecture are crucial for its success and which are mere passengers. It seems\nunlikely that Hochreiter and Schmidhuber\u2019s [16] initial design combined with the subsequent intro-\nduction of forget gates [6, 7] is the optimal design: at the time, the important issue was to \ufb01nd any\nscheme that could learn long-range dependencies rather than to \ufb01nd the minimal or optimal scheme.\nOne aim of this paper is to cast light on what aspects of the design are responsible for the success of\nLSTMs.\nRecent research on deep feedforward networks has also produced some impressive results [19, 3]\nand there is now a consensus that for deep networks, recti\ufb01ed linear units (ReLUs) are easier to train\nthan the logistic or tanh units that were used for many years [27, 40]. At \ufb01rst sight, ReLUs seem\ninappropriate for RNNs because they can have very large outputs so they might be expected to be far\nmore likely to explode than units that have bounded values. A second aim of this paper is to explore\nwhether ReLUs can be made to work well in RNNs and whether the ease of optimizing them in\nfeedforward nets transfers to RNNs.\n2\nThe initialization trick\nIn this paper, we demonstrate that, with the right initialization of the weights, RNNs composed\nof recti\ufb01ed linear units are relatively easy to train and are good at modeling long-range dependen-\ncies. The RNNs are trained by using backpropagation through time to get error-derivatives for the\nweights and by updating the weights after each small mini-batch of sequences. Their performance\non test data is comparable with LSTMs, both for toy problems involving very long-range temporal\nstructures and for real tasks like predicting the next word in a very large corpus of text.\nWe initialize the recurrent weight matrix to be the identity matrix and biases to be zero. This means\nthat each new hidden state vector is obtained by simply copying the previous hidden vector then\nadding on the effect of the current inputs and replacing all negative states by zero. In the absence\nof input, an RNN that is composed of ReLUs and initialized with the identity matrix (which we call\nan IRNN) just stays in the same state inde\ufb01nitely. The identity initialization has the very desirable\nproperty that when the error derivatives for the hidden units are backpropagated through time they\nremain constant provided no extra error-derivatives are added. This is the same behavior as LSTMs\nwhen their forget gates are set so that there is no decay and it makes it easy to learn very long-range\ntemporal dependencies.\nWe also \ufb01nd that for tasks that exhibit less long range dependencies, scaling the identity matrix by\na small scalar is an effective mechanism to forget long range effects. This is the same behavior as\nLTSMs when their forget gates are set so that the memory decays fast.\nOur initialization scheme bears some resemblance to the idea of Mikolov et al. [26], where a part of\nthe weight matrix is \ufb01xed to identity or approximate identity. The main difference of their work to\nours is the fact that our network uses the recti\ufb01ed linear units and the identity matrix is only used for\ninitialization. The scaled identity initialization was also proposed in Socher et al. [32] in the context\nof tree-structured networks but without the use of ReLUs. Our work is also related to the work of\nSaxe et al. [31], who study the use of orthogonal matrices as initialization in deep networks.\n3\nOverview of the experiments\nConsider a recurrent net with two input units. At each time step, the \ufb01rst input unit has a real value\nand the second input unit has a value of 0 or 1 as shown in \ufb01gure 1. The task is to report the sum of\nthe two real values that are marked by having a 1 as the second input [16, 15, 24]. IRNNs can learn\nto handle sequences with a length of 300, which is a challenging regime for other algorithms.\n2\nAnother challenging toy problem is to learn to classify the MNIST digits when the 784 pixels are\npresented sequentially to the recurrent net. Again, the IRNN was better than the LSTM, having been\nable to achieve 3% test set error compared to 34% for LSTM.\nWhile it is possible that a better tuned LSTM (with a different architecture or the size of the hidden\nstate) would outperform the IRNN for the above two tasks, the fact that the IRNN performs as well\nas it does, with so little tuning, is very encouraging, especially given how much simpler the model\nis, compared to the LSTM.\nWe also compared IRNNs with LSTMs on a large language modeling task. Each memory cell of an\nLSTM is considerably more complicated than a recti\ufb01ed linear unit and has many more parameters,\nso it is not entirely obvious what to compare. We tried to balance for both the number of parameters\nand the complexity of the architecture by comparing an LSTM with N memory cells with an IRNN\nwith four layers of N hidden units, and an IRNN with one layer and 2N hidden units. Here we \ufb01nd\nthat the IRNN gives results comparable to the equivalent LSTM.\nFinally, we benchmarked IRNNs and LSTMs on a acoustic modeling task on TIMIT. As the tasks\nonly require a short term memory of the inputs, we used a the identity matrix scaled by 0.01 as\ninitialization for the recurrent matrix. Results show that our method is also comparable to LSTMs,\ndespite being a lot simpler to implement.\n4\nExperiments\nIn the following experiments, we compare IRNNs against LSTMs, RNNs that use tanh units and\nRNNs that use ReLUs with random Gaussian initialization.\nFor IRNNs, in addition to the recurrent weights being initialized at identity, the non-recurrent\nweights are initialized with a random matrix, whose entries are sampled from a Gaussian distri-\nbution with mean of zero and standard deviation of 0.001.\nOur implementation of the LSTMs is rather standard and includes the forget gate. It is observed that\nsetting a higher initial forget gate bias for LSTMs can give better results for long term dependency\nproblems. We therefore also performed a grid search for the initial forget gate bias in LSTMs from\nthe set {1.0, 4.0, 10.0, 20.0}. Other than that we did not tune the LTSMs much and it is possible that\nthe results of LSTMs in the experiments can be improved.\nIn addition to LSTMs, two other candidates for comparison are RNNs that use the tanh activation\nfunction and RNNs that use ReLUs with standard random Gaussian initialization. We experimented\nwith several values of standard deviation for the random initialization Gaussian matrix and found\nthat values suggested in [33] work well.\nTo train these models, we use stochastic gradient descent with a \ufb01xed learning rate and gradient\nclipping. To ensure that good hyperparameters are used, we performed a grid search over several\nlearning rates \u03b1 = {10\u22129, 10\u22128, ..., 10\u22121} and gradient clipping values gc = {1, 10, 100, 1000} [9,\n36]. The reported result is the best result over the grid search. We also use the same batch size of\n16 examples for all methods. The experiments are carried out using the DistBelief infrastructure,\nwhere each experiment only uses one replica [20, 4].\n4.1\nThe Adding Problem\nThe adding problem is a toy task, designed to examine the power of recurrent models in learning\nlong-term dependencies [16, 15]. This is a sequence regression problem where the target is a sum of\ntwo numbers selected in a sequence of random signals, which are sampled from a uniform distribu-\ntion in [0,1]. At every time step, the input consists of a random signal and a mask signal. The mask\nsignal has a value of zero at all time steps except for two steps when it has values of 1 to indicate\nwhich two numbers should be added. An example of the adding problem is shown in \ufb01gure 1 below.\nA basic baseline is to always predict the sum to have a value of 1 regardless of the inputs. This will\ngive the Mean Squared Error (MSE) around 0.1767. The goal is to train a model that achieves MSE\nwell below 0.1767.\n3\nFigure 1: An example of the \u201cadding\u201d problem, where the target is 1.2 which is the sum of 2nd and\nthe 7th numbers in the \ufb01rst sequence [24].\nThe problem gets harder as the length of the sequence T increases because the dependency between\nthe output and the relevant inputs becomes more remote. To solve this problem, the recurrent net\nmust remember the \ufb01rst number or the sum of the two numbers accurately whilst ignoring all of the\nirrelevant numbers.\nWe generated a training set of 100,000 examples and a test set of 10,000 examples as we varied T .\nWe \ufb01xed the hidden states to have 100 units for all of our networks (LSTMs, RNNs and IRNNs).\nThis means the LSTMs had more parameters by a factor of about 4 and also took about 4 times as\nmuch computation per timestep.\nAs we varied T , we noticed that both LSTMs and RNNs started to struggle when T is around 150.\nWe therefore focused on investigating the behaviors of all models from this point onwards. The\nresults of the experiments with T = 150, T = 200, T = 300, T = 400 are reported in \ufb01gure 2\nbelow (best hyperparameters found during grid search are listed in table 1).\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nx 10\n6\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nSteps\nTest MSE\nAdding two numbers in a sequence of 150 numbers\n \n \nLSTM\nRNN + Tanh\nRNN + ReLUs\nIRNN\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nx 10\n6\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nSteps\nTest MSE\nAdding two numbers in a sequence of 200 numbers\n \n \nLSTM\nRNN + Tanh\nRNN + ReLUs\nIRNN\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nx 10\n6\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nSteps\nTest MSE\nAdding two numbers in a sequence of 300 numbers\n \n \nLSTM\nRNN + Tanh\nRNN + ReLUs\nIRNN\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nx 10\n6\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nSteps\nTest MSE\nAdding two numbers in a sequence of 400 numbers\n \n \nLSTM\nRNN + Tanh\nRNN + ReLUs\nIRNN\nFigure 2: The results of recurrent methods on the \u201cadding\u201d problem for the case of T = 150 (top\nleft), T = 200 (top right), T = 300 (bottom left) and T = 400 (bottom right). The objective\nfunction is the Root Mean Squared Error, reported on the test set of 10,000 examples. Note that\nalways predicting the sum to be 1 should give MSE of 0.1767.\nThe results show that the convergence of IRNNs is as good as LSTMs. This is given that each LSTM\nstep is more expensive than an IRNN step (at least 4x more expensive). Adding two numbers in a\nsequence of 400 numbers is somewhat challenging for both algorithms.\n4\nT\nLSTM\nRNN + Tanh\nIRNN\n150\nlr = 0.01, gc = 10, fb = 1.0\nlr = 0.01, gc = 100\nlr = 0.01, gc = 100\n200\nlr = 0.001, gc = 100, fb = 4.0\nN/A\nlr = 0.01, gc = 1\n300\nlr = 0.01, gc = 1, fb = 4.0\nN/A\nlr = 0.01, gc = 10\n400\nlr = 0.01, gc = 100, fb = 10.0\nN/A\nlr = 0.01, gc = 1\nTable 1: Best hyperparameters found for adding problems after grid search. lr is the learning rate, gc\nis gradient clipping, and fb is forget gate bias. N/A is when there is no hyperparameter combination\nthat gives good result.\n4.2\nMNIST Classi\ufb01cation from a Sequence of Pixels\nAnother challenging toy problem is to learn to classify the MNIST digits [21] when the 784 pixels\nare presented sequentially to the recurrent net. In our experiments, the networks read one pixel at a\ntime in scanline order (i.e. starting at the top left corner of the image, and ending at the bottom right\ncorner). The networks are asked to predict the category of the MNIST image only after seeing all\n784 pixels. This is therefore a huge long range dependency problem because each recurrent network\nhas 784 time steps.\nTo make the task even harder, we also used a \ufb01xed random permutation of the pixels of the MNIST\ndigits and repeated the experiments.\nAll networks have 100 recurrent hidden units. We stop the optimization after it converges or when\nit reaches 1,000,000 iterations and report the results in \ufb01gure 3 (best hyperparameters are listed in\ntable 2).\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nx 10\n5\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nSteps\nTest Accuracy\nPixel\u2212by\u2212pixel MNIST\n \n \nLSTM\nRNN + Tanh\nRNN + ReLUs\nIRNN\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nx 10\n5\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nSteps\nTest Accuracy\nPixel\u2212by\u2212pixel permuted MNIST\n \n \nLSTM\nRNN + Tanh\nRNN + ReLUs\nIRNN\nFigure 3: The results of recurrent methods on the \u201cpixel-by-pixel MNIST\u201d problem. We report the\ntest set accuracy for all methods. Left: normal MNIST. Right: permuted MNIST.\nProblem\nLSTM\nRNN + Tanh\nRNN + ReLUs\nIRNN\nMNIST\nlr = 0.01, gc = 1\nlr = 10\u22128, gc = 10\nlr = 10\u22128, gc = 10\nlr = 10\u22128, gc = 1\nfb = 1.0\npermuted\nlr = 0.01, gc = 1\nlr = 10\u22128, gc = 1\nlr = 10\u22126, gc = 10\nlr = 10\u22129, gc = 1\nMNIST\nfb = 1.0\nTable 2: Best hyperparameters found for pixel-by-pixel MNIST problems after grid search. lr is the\nlearning rate, gc is gradient clipping, and fb is the forget gate bias.\nThe results using the standard scanline ordering of the pixels show that this problem is so dif\ufb01cult\nthat standard RNNs fail to work, even with ReLUs, whereas the IRNN achieves 3% test error rate\nwhich is better than most off-the-shelf linear classi\ufb01ers [21]. We were surprised that the LSTM did\nnot work as well as IRNN given the various initialization schemes that we tried. While it still possi-\nble that a better tuned LSTM would do better, the fact that the IRNN perform well is encouraging.\n5\nApplying a \ufb01xed random permutation to the pixels makes the problem even harder but IRNNs on\nthe permuted pixels are still better than LSTMs on the non-permuted pixels.\nThe low error rates of the IRNN suggest that the model can discover long range correlations in the\ndata while making weak assumptions about the inputs. This could be important to have for problems\nwhen input data are in the form of variable-sized vectors (e.g. the repeated \ufb01eld of a protobuffer 1).\n4.3\nLanguage Modeling\nWe benchmarked RNNs, IRNNs and LSTMs on the one billion word language modelling dataset [2],\nperhaps the largest public benchmark in language modeling. We chose an output vocabulary of\n1,000,000 words.\nAs the dataset is large, we observed that the performance of recurrent methods depends on the size\nof the hidden states: they perform better as the size of the hidden states gets larger (cf. [2]). We\nhowever focused on a set of simple controlled experiments to understand how different recurrent\nmethods behave when they have a similar number of parameters. We \ufb01rst ran an experiment where\nthe number of hidden units (or memory cells) in LSTM are chosen to be 512. The LSTM is trained\nfor 60 hours using 32 replicas. Our goal is then to check how well IRNNs perform given the same\nexperimental environmentand settings. As LSTM have more parameters per time step, we compared\nthem with an IRNN that had 4 layers and same number of hidden units per layer (which gives\napproximately the same numbers of parameters).\nWe also experimented shallow RNNs and IRNNs with 1024 units. Since the output vocabulary is\nlarge, we projected the 1024 hidden units to a linear layer with 512 units before the softmax. This\navoids greatly increasing the number of parameters.\nThe results are reported in table 3, which show that the performance of IRNNs is closer to the\nperformance of LSTMs for this large-scale task than it is to the performance of RNNs.\nMethods\nTest perplexity\nLSTM (512 units)\n68.8\nIRNN (4 layers, 512 units)\n69.4\nIRNN (1 layer, 1024 units + linear projection with 512 units before softmax)\n70.2\nRNN (4 layer, 512 tanh units)\n71.8\nRNN (1 layer, 1024 tanh units + linear projection with 512 units before softmax)\n72.5\nTable 3: Performances of recurrent methods on the 1 billion word benchmark.\n4.4\nSpeech Recognition\nWe performed Phoneme recognition experiments on TIMIT with IRNNs and Bidirectional IRNNs\nand compared them to RNNs, LSTMs and Bidirectional LSTMs and RNNs. Bidirectional LSTMs\nhave been applied previously to TIMIT in [11]. In these experiments we generated phoneme align-\nments from Kaldi [29] using the recipe reported in [17] and trained all RNNs with two and \ufb01ve\nhidden layers. Each model was given log Mel \ufb01lter bank spectra with their delta and accelerations,\nwhere each frame was 120 (=40*3) dimensional and trained to predict the phone state (1 of 180).\nFrame error rates (FER) from this task are reported in table 4.\nIn this task, instead of the identity initialization for the IRNNs matrices we used 0.01I so we refer\nto them as iRNNs. Initalizing with the full identity led to slow convergence, worse results and\nsometimes led to the model diverging during training. We hypothesize that this was because in the\nspeech task similar inputs are provided to the neural net in neighboring frames. The normal IRNN\nkeeps integrating this past input, instead of paying attention mainly to the current input because it\nhas a dif\ufb01cult time forgetting the past. So for the speech task, we are not only showing that iRNNs\nwork much better than RNNs composed of tanh units, but we are also showing that initialization\nwith the full identity is suboptimal when long range effects are not needed. Mulitplying the identity\nwith a small scalar seems to be a good remedy in such cases.\n1https://code.google.com/p/protobuf/\n6\nMethods\nFrame error rates (dev / test)\nRNN (500 neurons, 2 layers)\n35.0 / 36.2\nLSTM (250 cells, 2 layers)\n34.5 / 35.4\niRNN (500 neurons, 2 layers)\n34.3 / 35.5\nRNN (500 neurons, 5 layers)\n35.6 / 37.0\nLSTM (250 cells, 5 layers)\n35.0 / 36.2\niRNN (500 neurons, 5 layers)\n33.0 / 33.8\nBidirectional RNN (500 neurons, 2 layers)\n31.5 / 32.4\nBidirectional LSTM (250 cells, 2 layers)\n29.6 / 30.6\nBidirectional iRNN (500 neurons, 2 layers)\n31.9 / 33.2\nBidirectional RNN (500 neurons, 5 layers)\n33.9 / 34.8\nBidirectional LSTM (250 cells, 5 layers)\n28.5 / 29.1\nBidirectional iRNN (500 neurons, 5 layers)\n28.9 / 29.7\nTable 4: Frame error rates of recurrent methods on the TIMIT phone recognition task.\nIn general in the speech recognition task, the iRNN easily outperforms the RNN that uses tanh units\nand is comparable to LSTM although we don\u2019t rule out the possibility that with very careful tuning\nof hyperparameters, the relative performance of LSTMs or the iRNNs might change. A \ufb01ve layer\nBidirectional LSTM outperforms all the other models on this task, followed closely by a \ufb01ve layer\nBidirectional iRNN.\n4.5\nAcknowledgements\nWe thank Jeff Dean, Matthieu Devin, Rajat Monga, David Sussillo, Ilya Sutskever and Oriol Vinyals\nfor their help with the project.\nReferences\n[1] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align\nand translate. arXiv preprint arXiv:1409.0473, 2014.\n[2] C. Chelba, T. Mikolov, M. Schuster, Q. Ge, T. Brants, and P. Koehn. One billion word bench-\nmark for measuring progress in statistical language modeling. CoRR, abs/1312.3005, 2013.\n[3] G. E. Dahl, D. Yu, L. Deng, and A. Acero. Context-dependentpre-trained deep neural networks\nfor large vocabulary speech recognition. IEEE Transactions on Audio, Speech, and Language\nProcessing - Special Issue on Deep Learning for Speech and Language Processing, 2012.\n[4] J. Dean, G. S. Corrado, R. Monga, K. Chen, M. Devin, Q. V. Le, M. Z. Mao, M. A. Ranzato,\nA. Senior, P. Tucker, K. Yang, and A. Y. Ng. Large scale distributed deep networks. In NIPS,\n2012.\n[5] J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and\nstochastic optimization. The Journal of Machine Learning Research, 12:2121\u20132159, 2011.\n[6] F. A. Gers, J. Schmidhuber, and F. Cummins. Learning to forget: Continual prediction with\nLSTM. Neural Computation, 2000.\n[7] F. A. Gers, N. N. Schraudolph, and J. Schmidhuber. Learning precise timing with lstm recur-\nrent networks. The Journal of Machine Learning Research, 2003.\n[8] A. Graves.\nGenerating sequences with recurrent neural networks.\narXiv preprint\narXiv:1308.0850, 2013.\n[9] A. Graves. Generating sequences with recurrent neural networks. In Arxiv, 2013.\n[10] A. Graves and N. Jaitly. Towards end-to-end speech recognition with recurrent neural net-\nworks. In Proceedings of the 31st International Conference on Machine Learning, 2014.\n[11] A. Graves, N. Jaitly, and A-R. Mohamed. Hybrid speech recognition with deep bidirectional\nlstm. In IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU),, 2013.\n7\n[12] A. Graves, M. Liwicki, S. Fern\u00b4andez, R. Bertolami, H. Bunke, and J. Schmidhuber. A novel\nconnectionist system for unconstrained handwriting recognition. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 2009.\n[13] A. Graves, A-R. Mohamed, and G. Hinton. Speech recognition with deep recurrent neural\nnetworks.\nIn IEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP), 2013.\n[14] G. Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magni-\ntude. COURSERA: Neural Networks for Machine Learning, 2012.\n[15] S. Hochreiter, Y. Bengio, P. Frasconi, and J. Schmidhuber. Gradient \ufb02ow in recurrent nets: the\ndif\ufb01culty of learning long-term dependencies. A Field Guide to Dynamical Recurrent Neural\nNetworks, 2001.\n[16] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 1997.\n[17] N. Jaitly. Exploring Deep Learning Methods for discovering features in speech signals. PhD\nthesis, University of Toronto, 2014.\n[18] R. Kiros, R. Salakhutdinov, and R. S. Zemel.\nUnifying visual-semantic embeddings with\nmultimodal neural language models. arXiv preprint arXiv:1411.2539, 2014.\n[19] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classi\ufb01cation with deep convolutional\nneural networks. In Advances in Neural Information Processing Systems, 2012.\n[20] Q. V. Le, M. A. Ranzato, R. Monga, M. Devin, K. Chen, G. S. Corrado, J. Dean, and A. Y.\nNg. Building high-level features using large scale unsupervised learning. In International\nConference on Machine Learning, 2012.\n[21] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document\nrecognition. Proceedings of the IEEE, 1998.\n[22] T. Luong, I. Sutskever, Q. V. Le, O. Vinyals, and W. Zaremba. Addressing the rare word\nproblem in neural machine translation. arXiv preprint arXiv:1410.8206, 2014.\n[23] J. Martens. Deep learning via Hessian-free optimization. In Proceedings of the 27th Interna-\ntional Conference on Machine Learning, 2010.\n[24] J. Martens and I. Sutskever. Learning recurrent neural networks with Hessian-Free optimiza-\ntion. In ICML, 2011.\n[25] J. Martens and I. Sutskever. Training deep and recurrent neural networks with Hessian-Free\noptimization. Neural Networks: Tricks of the Trade, 2012.\n[26] T. Mikolov, A. Joulin, S. Chopra, M. Mathieu, and M. A. Ranzato. Learning longer memory\nin recurrent neural networks. arXiv preprint arXiv:1412.7753, 2014.\n[27] V. Nair and G. Hinton. Recti\ufb01ed Linear Units improve Restricted Boltzmann Machines. In\nInternational Conference on Machine Learning, 2010.\n[28] R. Pascanu, T. Mikolov, and Y. Bengio. On the dif\ufb01culty of training recurrent neural networks.\narXiv preprint arXiv:1211.5063, 2012.\n[29] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M. Hannemann,\nP. Motlicek, Y. Qian, P. Schwarz, J. Silovsky, G. Stemmer, and K. Vesely. The kaldi speech\nrecognition toolkit. In IEEE 2011 Workshop on Automatic Speech Recognition and Under-\nstanding. IEEE Signal Processing Society, 2011.\n[30] D. Rumelhart, G. E. Hinton, and R. J. Williams. Learning representations by back-propagating\nerrors. Nature, 323(6088):533\u2013536, 1986.\n[31] A. M. Saxe, J. L. McClelland, and S. Ganguli. Exact solutions to the nonlinear dynamics of\nlearning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.\n[32] R. Socher, J. Bauer, C. D. Manning, and A. Y. Ng. Parsing with compositional vector gram-\nmars. In ACL, 2013.\n[33] D. Sussillo and L. F. Abbott. Random walk intialization for training very deep networks. arXiv\npreprint arXiv:1412.6558, 2015.\n[34] I. Sutskever, J. Martens, G. Dahl, and G. Hinton. On the importance of initialization and\nmomentum in deep learning. In Proceedings of the 30th International Conference on Machine\nLearning, 2013.\n8\n[35] I. Sutskever, J. Martens, and G. E. Hinton. Generating text with recurrent neural networks.\nIn Proceedings of the 28th International Conference on Machine Learning, pages 1017\u20131024,\n2011.\n[36] I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence learning with neural networks.\nIn NIPS, 2014.\n[37] O. Vinyals, L. Kaiser, T. Koo, S. Petrov, I. Sutskever, and G. Hinton. Grammar as a foreign\nlanguage. arXiv preprint arXiv:1412.7449, 2014.\n[38] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show and tell: A neural image caption\ngenerator. arXiv preprint arXiv:1411.4555, 2014.\n[39] W. Zaremba and I. Sutskever. Learning to execute. arXiv preprint arXiv:1410.4615, 2014.\n[40] M. Zeiler, M. Ranzato, R. Monga, M. Mao, K. Yang, Q. V. Le, P. Nguyen, A. Senior, V. Van-\nhoucke, and J. Dean. On recti\ufb01ed linear units for speech processing. In IEEE Conference on\nAcoustics, Speech and Signal Processing (ICASSP), 2013.\n9\n",
        "sentence": " The RNN units can be simple RNN units (Le et al., 2015) or more complex recurrent units such as Long Short-Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997; Graves et al.",
        "context": "and the complexity of the architecture by comparing an LSTM with N memory cells with an IRNN\nwith four layers of N hidden units, and an IRNN with one layer and 2N hidden units. Here we \ufb01nd\nthat the IRNN gives results comparable to the equivalent LSTM.\nLSTM. Neural Computation, 2000.\n[7] F. A. Gers, N. N. Schraudolph, and J. Schmidhuber. Learning precise timing with lstm recur-\nrent networks. The Journal of Machine Learning Research, 2003.\n[8] A. Graves.\nNetworks, 2001.\n[16] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 1997.\n[17] N. Jaitly. Exploring Deep Learning Methods for discovering features in speech signals. PhD\nthesis, University of Toronto, 2014."
    },
    {
        "title": "Discovering latent network structure in point process data",
        "author": [
            "Linderman",
            "Scott",
            "Adams",
            "Ryan"
        ],
        "venue": "In ICML, pp",
        "citeRegEx": "Linderman et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Linderman et al\\.",
        "year": 2014,
        "abstract": "Networks play a central role in modern data analysis, enabling us to reason\nabout systems by studying the relationships between their parts. Most often in\nnetwork analysis, the edges are given. However, in many systems it is difficult\nor impossible to measure the network directly. Examples of latent networks\ninclude economic interactions linking financial instruments and patterns of\nreciprocity in gang violence. In these cases, we are limited to noisy\nobservations of events associated with each node. To enable analysis of these\nimplicit networks, we develop a probabilistic model that combines\nmutually-exciting point processes with random graph models. We show how the\nPoisson superposition principle enables an elegant auxiliary variable\nformulation and a fully-Bayesian, parallel inference algorithm. We evaluate\nthis new model empirically on several datasets.",
        "full_text": "DISCOVERING LATENT NETWORK STRUCTURE IN\nPOINT PROCESS DATA\nBy Scott W. Linderman and Ryan P. Adams\nHarvard University\nNetworks play a central role in modern data analysis, enabling\nus to reason about systems by studying the relationships between\ntheir parts. Most often in network analysis, the edges are given. How-\never, in many systems it is di\ufb03cult or impossible to measure the\nnetwork directly. Examples of latent networks include economic in-\nteractions linking \ufb01nancial instruments and patterns of reciprocity in\ngang violence. In these cases, we are limited to noisy observations of\nevents associated with each node. To enable analysis of these implicit\nnetworks, we develop a probabilistic model that combines mutually-\nexciting point processes with random graph models. We show how\nthe Poisson superposition principle enables an elegant auxiliary vari-\nable formulation and a fully-Bayesian, parallel inference algorithm.\nWe evaluate this new model empirically on several datasets.\n1. Introduction.\nMany types of modern data are characterized via relationships on a network.\nSocial network analysis is the most commonly considered example, where the properties of individ-\nuals (vertices) can be inferred from \u201cfriendship\u201d type connections (edges). Such analyses are also\ncritical to understanding regulatory biological pathways, trade relationships between nations, and\npropagation of disease. The tasks associated with such data may be unsupervised (e.g., identifying\nlow-dimensional representations of edges or vertices) or supervised (e.g., predicting unobserved links\nin the graph). Traditionally, network analysis has focused on explicit network problems in which the\ngraph itself is considered to be the observed data. That is, the vertices are considered known and\nthe data are the entries in the associated adjacency matrix. A rich literature has arisen in recent\nyears for applying statistical machine learning models to this type of problem, e.g., Liben-Nowell\n& Kleinberg (2007); Ho\ufb00(2008); Goldenberg et al. (2010).\nIn this paper we are concerned with implicit networks that cannot be observed directly, but about\nwhich we wish to perform analysis. In an implicit network, the vertices or edges of the graph may\nnot be directly observed, but the graph structure may be inferred from noisy emissions. These noisy\nobservations are assumed to have been generated according to underlying dynamics that respect\nthe latent network structure.\nFor example, trades on \ufb01nancial stock markets are executed thousands of times per second.\nTrades of one stock are likely to cause subsequent activity on stocks in related industries. How\ncan we infer such interactions and disentangle them from market-wide \ufb02uctuations that occur\nthroughout the day? Discovering latent structure underlying \ufb01nancial markets not only reveals\ninterpretable patterns of interaction, but also provides insight into the stability of the market. In\nSection 4 we will analyze the stability of mutually-excitatory systems, and in Section 6 we will\nexplore how stock similarity may be inferred from trading activity.\nAs another example, both the edges and vertices may be latent. In Section 7, we examine patterns\nof violence in Chicago, which can often be attributed to social structures in the form of gangs. We\nwould expect that attacks from one gang onto another might induce cascades of violence, but\nthe vertices (gang identity of both perpetrator and victim) are unobserved. As with the \ufb01nancial\ndata, it should be possible to exploit dynamics to infer these social structures. In this case spatial\n1\narXiv:1402.0914v1  [stat.ML]  4 Feb 2014\n2\nS. W. LINDERMAN AND R. P. ADAMS\ninformation is available as well, which can help inform latent vertex identities.\nIn both of these examples, the noisy emissions have the form of events in time, or \u201cspikes,\u201d and\nour intuition is that a spike at a vertex will induce activity at adjacent vertices. In this paper,\nwe formalize this idea into a probabilistic model based on mutually-interacting point processes.\nSpeci\ufb01cally, we combine the Hawkes process (Hawkes, 1971) with recently developed exchangeable\nrandom graph priors. This combination allows us to reason about latent networks in terms of the\nway that they regulate interaction in the Hawkes process. Inference in the resulting model can be\ndone with Markov chain Monte Carlo, and an elegant data augmentation scheme results in e\ufb03cient\nparallelism.\n2. Preliminaries.\n2.1. Poisson Processes.\nPoint processes are fundamental statistical objects that yield random\n\ufb01nite sets of events {sn}N\nn=1 \u2282S, where S is a compact subset of RD, for example, space or time.\nThe Poisson process is the canonical example. It is governed by a nonnegative \u201crate\u201d or \u201cintensity\u201d\nfunction, \u03bb(s) : S \u2192R+. The number of events in a subset S\u2032 \u2282S follows a Poisson distribution\nwith mean\nR\nS\u2032 \u03bb(s)ds. Moreover, the number of events in disjoint subsets are independent.\nWe use the notation {sn}N\nn=1 \u223cPP(\u03bb(s)) to indicate that a set of events {sn}N\nn=1 is drawn from\na Poisson process with rate \u03bb(s). The likelihood is given by\np({sn}N\nn=1|\u03bb(s)) = exp\n\u001a\n\u2212\nZ\nS\n\u03bb(s)ds\n\u001b N\nY\nn=1\n\u03bb(sn).\n(1)\nIn this work we will make use of a special property of Poisson processes, the Poisson superposition\ntheorem, which states that {sn} \u223cPP(\u03bb1(s) + . . . + \u03bbK(s)) can be decomposed into K independent\nPoisson processes. Letting zn denote the origin of the n-th event, we perform the decomposition by\nindependently sampling each zn from Pr(zn = k) \u221d\u03bbk(sn), for k \u2208{1 . . . K} (Daley & Vere-Jones,\n1988).\n2.2. Hawkes Processes.\nThough Poisson processes have many nice properties, they cannot cap-\nture interactions between events. For this we turn to a more general model known as Hawkes\nprocesses. A Hawkes process consists of K point processes and gives rise to sets of marked events\n{sn, cn}N\nn=1, where cn \u2208{1, . . . , K} speci\ufb01es the process on which the n-th event occurred. For now,\nwe assume the events are points in time, i.e., sn \u2208[0, T]. Each of the K processes is a conditionally\nPoisson process with a rate \u03bbk(t | {sn : sn < t}) that depends on the history of events up to time t.\nHawkes processes have additive interactions. Each process has a \u201cbackground rate\u201d \u03bb0,k(t), and\neach event sn on process k adds a nonnegative impulse response hk,k\u2032(t \u2212sn) to the intensity of\nother processes k\u2032. Causality and locality of in\ufb02uence are enforced by requiring hk,k\u2032(\u2206t) to be zero\nfor \u2206t /\u2208[0, \u2206tmax].\nBy the superposition theorem for Poisson processes, these additive components can be considered\nindependent processes, each giving rise to their own events. We augment our data with a latent\nrandom variable zn \u2208{0, . . . , n \u22121} to indicate the cause of the n-th event (0 if the event is due to\nthe background rate and 1 . . . n \u22121 if it was caused by a preceding event).\nLet Cn,k\u2032 denote the set of events on process k\u2032 that were parented by event n. Formally,\nCn,k\u2032 \u2261{sn\u2032 : cn\u2032 = k\u2032 \u2227zn\u2032 = n}.\nDISCOVERING LATENT NETWORK STRUCTURE IN POINT PROCESS DATA\n3\n1\n2\n3\n4\n5a\n5b\n5c\nI\nII\nIII\nFig 1: Illustration of a Hawkes process. Events induce impulse responses on connected processes\nand spawn \u201cchild\u201d events. See the main text for a complete description.\nLet C0,k be the set of events attributed to the background rate of process k. The augmented Hawkes\nlikelihood is the product of likelihoods of each Poisson process:\np({(sn, cn, zn)}N\nn=1 | {\u03bb0,k(t)},{{hk,k\u2032(\u2206t)}}) =\n\" K\nY\nk=1\np(C0,k | \u03bb0,k(t))\n#\n\u00d7\n\" N\nY\nn=1\nK\nY\nk=1\np(Cn,k | hcn,k(t \u2212sn))\n#\n,\n(2)\nwhere the densities in the product are given by Equation 1.\nFigure 1 illustrates a causal cascades of events for a simple network of three processes (I-III).\nThe \ufb01rst event is caused by the background rate (z1 = 0), and it induces impulse responses on\nprocesses II and III. Event 2 is spawned by the impulse on the third process (z2 = 1), and feeds\nback onto processes I and II. In some cases a single parent event induces multiple children, e.g.,\nevent 4 spawns events 5a-c. In this simple example, processes excite one another, but do not excite\nthemselves. Next we will introduce more sophisticated models for such interaction networks.\n2.3. Random Graph Models.\nGraphs of K nodes correspond to K \u00d7 K matrices. Unweighted\ngraphs are binary adjacency matrices A where Ak,k\u2032 = 1 indicates a directed edge from node k to\nnode k\u2032. Weighted directed graphs can be represented by a real matrix W whose entries indicate the\nweights of the edges. Random graph models re\ufb02ect the probability of di\ufb00erent network structures\nthrough distributions over these matrices.\nRecently, many random graph models have been uni\ufb01ed under an elegant theoretical framework\ndue to Aldous and Hoover (Aldous, 1981; Hoover, 1979). See Lloyd et al. (2012) for an overview.\nConceptually, the Aldous-Hoover representation characterizes the class of exchangeable random\ngraphs, that is, graph models for which the joint probability is invariant under permutations of\nthe node labels. Just as de Finetti\u2019s theorem equates exchangeable sequences (Xn)n\u2208N to inde-\npendent draws from a random probability measure \u0398, the Aldous-Hoover theorem relates random\nexchangeable graphs to the following generative model:\nu1, u2, . . . \u223ci.i.d Uniform[0, 1],\nAk,k\u2032 \u223cBernoulli(\u0398(uk, uk\u2032)),\nfor some random function \u0398 : [0, 1]2 \u2192[0, 1].\n4\nS. W. LINDERMAN AND R. P. ADAMS\nEmpty graph models (Ak,k\u2032 \u22610) and complete models (Ak,k\u2032 \u22611) are trivial examples, but much\nmore structure may be encoded. For example, consider a model in which nodes are endowed with\na location in space, xk \u2208RD. This could be an abstract feature space or a real location like the\ncenter of a gang territory. The probability of connection between two notes decreases with distance\nbetween them as Ak,k\u2032 \u223cBern(\u03c1e\u2212||xk\u2212xk\u2032||/\u03c4), where \u03c1 is the overall sparsity and \u03c4 is the charac-\nteristic distance scale. This simple model can be converted to the Aldous-Hoover representation by\ntransforming uk into xk via the inverse CDF.\nMany models can be constructed in this manner. Stochastic block models, latent eigenmodels,\nand their nonparametric extensions all fall under this class (Lloyd et al., 2012). We will leverage\nthe generality of the Aldous-Hoover formalism to build a \ufb02exible model and inference algorithm for\nHawkes processes with structured interaction networks.\n3. The Network Hawkes Model.\nIn order to combine Hawkes processes and random net-\nwork models, we decompose the Hawkes impulse response hk,k\u2032(\u2206t) as follows:\nhk,k\u2032(\u2206t) = Ak,k\u2032Wk,k\u2032g\u03b8k,k\u2032(\u2206t).\n(3)\nHere, A \u2208{0, 1}K\u00d7K is a binary adjacency matrix and W \u2208RK\u00d7K\n+\nis a non-negative weight matrix.\nTogether these specify the sparsity structure and strength of the interaction network, respectively.\nThe non-negative function g\u03b8k,k\u2032(\u2206t) captures the temporal aspect of the interaction. It is param-\neterized by \u03b8k,k\u2032 and satis\ufb01es two properties: a) it has bounded support for \u2206t \u2208[0, \u2206tmax], and b)\nit integrates to one. In other words, g is a probability density with compact support.\nDecomposing h as in Equation 3 has many advantages. It allows us to express our separate beliefs\nabout the sparsity structure of the interaction network and the strength of the interactions through\na spike-and-slab prior on A and W (Mohamed et al., 2012). The empty graph model recovers\nindependent background processes, and the complete graph recovers the standard Hawkes process.\nMaking g a probability density endows W with units of \u201cexpected number of events\u201d and allows us\nto compare the relative strength of interactions. The form suggests an intuitive generative model:\nfor each impulse response draw m \u223cPoisson(Wk,k\u2032) number of induced events and draw the m child\nevent times i.i.d. from g, enabling computationally tractable conjugate priors.\nIntuitively, the background rates, \u03bb0,k(t), explain events that cannot be attributed to preceding\nevents. In the simplest case the background rate is constant. However, there are often \ufb02uctuations\nin overall intensity that are shared among the processes, and not re\ufb02ective of process-to-process\ninteraction, as we will see in the daily variations in trading volume on the S&P100 and the seasonal\ntrends in homicide. To capture these shared background \ufb02uctuations, we use a sparse Log Gaussian\nCox process (M\u00f8ller et al., 1998) to model the background rate:\n\u03bb0,k(t) = \u00b5k + \u03b1k exp{y(t)}, y(t) \u223cGP(0, K(t, t\u2032)).\nThe kernel K(t, t\u2032) describes the covariance structure of the background rate that is shared by all\nprocesses. For example, a periodic kernel may capture seasonal or daily \ufb02uctuations. The o\ufb00set \u00b5k\naccounts for varying background intensities among processes, and the scaling factor \u03b1k governs\nhow sensitive process k is to these background \ufb02uctuations (when \u03b1k = 0 we recover the constant\nbackground rate).\nFinally, in some cases the process identities, cn, must also be inferred. With gang incidents in\nChicago we may have only a location, xn \u2208R2. In this case, we may place a spatial Gaussian\nmixture model over the cn\u2019s, as in Cho et al. (2013). Alternatively, we may be given the label of the\ncommunity in which the incident occurred, but we suspect that interactions occur between clusters\nof communities. In this case we can use a simple clustering model or a nonparametric model like\nthat of Blundell et al. (2012).\nDISCOVERING LATENT NETWORK STRUCTURE IN POINT PROCESS DATA\n5\n3.1. Inference with Gibbs Sampling.\nWe present a Gibbs sampling procedure for inferring the\nmodel parameters, W , A, {{\u03b8k,k\u2032}},{\u03bb0,k(t)}, and, if necessary, {cn}. In order to simplify our Gibbs\nupdates, we will also sample a set of parent assignments for each event {zn}. Incorporating these\nparent variables enables conjugate prior distributions for W , \u03b8k,k\u2032, and, in the case of constant\nbackground rates, \u03bb0,k.\nSampling weights W ..\nA gamma prior on the weights, Wk,k\u2032 \u223cGamma(\u03b10\nW , \u03b20\nW ), results in the\nconditional distribution,\nWk,k\u2032 | {sn, cn, zn}N\nn=1, \u03b8k,k\u2032 \u223cGamma(\u03b1k,k\u2032, \u03b2k,k\u2032),\n\u03b1k,k\u2032 = \u03b10\nW +\nN\nX\nn=1\nN\nX\nn\u2032=1\n\u03b4cn,k\u03b4cn\u2032,k\u2032\u03b4zn\u2032,n\n\u03b2k,k\u2032 = \u03b20\nW +\nN\nX\nn=1\n\u03b4cn,k.\nThis is a minor approximation valid for \u2206tmax \u226aT. Here and elsewhere, \u03b4i,j is the Kronecker delta\nfunction. We use the inverse-scale parameterization of the gamma distribution, i.e.,\nGamma(x | \u03b1, \u03b2) =\n\u03b2\u03b1\n\u0393(\u03b1)x\u03b1\u22121 exp{\u2212\u03b2 x}.\nSampling impulse response parameters \u03b8k,k\u2032..\nWe let gk,k\u2032(\u2206t) be the logistic-normal density with\nparameters \u03b8k,k\u2032 = {\u00b5, \u03c4}:\ngk,k\u2032(\u2206t | \u00b5, \u03c4) = 1\nZ exp\n(\n\u2212\u03c4\n2\n\u0012\n\u03c3\u22121\n\u0012\n\u2206t\n\u2206tmax\n\u0013\n\u2212\u00b5\n\u00132)\n\u03c3\u22121(x) = ln(x/(1 \u2212x))\nZ = \u2206t(\u2206tmax \u2212\u2206t)\n\u2206tmax\n\u0010 \u03c4\n2\u03c0\n\u0011\u22121\n2 .\nThe normal-gamma prior\n\u00b5, \u03c4 \u223cNG(\u00b5, \u03c4|\u00b50\n\u00b5, \u03ba0\n\u00b5, \u03b10\n\u03c4, \u03b20\n\u03c4)\nyields the standard conditional distribution (see Murphy, 2012) with the following su\ufb03cient statis-\ntics:\nxn,n\u2032 = ln(sn\u2032 \u2212sn) \u2212ln(tmax \u2212(sn\u2032 \u2212sn)),\nm =\nN\nX\nn=1\nN\nX\nn\u2032=1\n\u03b4cn,k\u03b4cn\u2032,k\u2032\u03b4zn\u2032,n,\n\u00afx = 1\nm\nN\nX\nn=1\nN\nX\nn\u2032=1\n\u03b4cn,k\u03b4cn\u2032,k\u2032\u03b4zn\u2032,nxn,n\u2032.\nSampling background rates \u03bb0,k..\nFor background rates \u03bb0,k(t) \u2261\u03bb0,k, the prior \u03bb0,k \u223cGamma(\u03b10\n\u03bb, \u03b20\n\u03bb)\nis conjugate with the likelihood and yield the conditional distribution\n\u03bb0,k | {sn, cn, zn}N\nn=1, \u223cGamma(\u03b1\u03bb, \u03b2\u03bb),\n\u03b1\u03bb = \u03b10\n\u03bb +\nX\nn\n\u03b4cn,k\u03b4zn,0\n\u03b2\u03bb = \u03b20\n\u03bb + T\n6\nS. W. LINDERMAN AND R. P. ADAMS\n(a)\n0\n1\n2\n0\n2\n4\n6\nW\np(W)\n \n \nG(1,5)\nG(2,5)\nG(4,8)\nG(8,12)\n(b)\n4\n64\n1024\n10\n\u22122\n10\n\u22121\n10\n0\nK\n\u03c1\n(c)\n0\n0.5\n1\n\u03bbmax\nPr(\u03bbmax)\n(d)\n0\n0.5\n1\n\u03bbmax\nPr(\u03bbmax)\nFig 2: Empirical and theoretical distribution of the maximum eigenvalue for Erd\u02ddos-Renyi graphs\nwith gamma weights. (a) Four gamma weight distributions. The colors correspond to the curves\nin the remaining panels. (b) Sparsity that theoretically yields 99% probability of stability as a\nfunction of p(W) and K. (c) and (d) Theoretical (solid) and empirical (dots) distribution of the\nmaximum eigenvalue. Color corresponds to the weight distribution in (a) and intensity indicates K\nand \u03c1 shown in (b).\nThis conjugacy no longer holds for Gaussian process background rates, but conditioned upon\nthe parent variables, we must simply \ufb01t a Gaussian process for those events for which zn = 0. We\nuse elliptical slice sampling (Murray et al., 2010) for this purpose.\nCollapsed Gibbs sampling A and zn..\nWith Aldous-Hoover graph priors, the entries in the binary\nadjacency matrix A are conditionally independent given the parameters of the prior. The likelihood\nintroduces dependencies between the rows of A, but each column can be sampled in parallel. Gibbs\nupdates are complicated by strong dependencies between the graph and the parent variables, zn.\nSpeci\ufb01cally, if zn\u2032 = n, then we must have Acn,cn\u2032 = 1. To improve the mixing of our sampling algo-\nrithm, \ufb01rst we update A | {sn, cn}, W , \u03b8k,k\u2032 by marginalizing the parent variables. The posterior is\ndetermined by the likelihood of the conditionally Poisson process \u03bbk\u2032(t | {sn : sn < t}) (Equation 1)\nwith and without interaction Ak,k\u2032 and the prior comes from the Aldous-Hoover graph model. Then\nwe update zn | {sn, cn}, A, W , \u03b8k,k\u2032 by sampling from the discrete conditional distribution. Though\nthere are N parent variables, they are conditionally independent and may be sampled in parallel.\nWe have implemented our inference algorithm on GPUs to capitalize on this parallelism.\nSampling process identities cn..\nAs with the adjacency matrix, we use a collapsed Gibbs sampler\nto marginalize out the parent variables when sampling the process identities. Unfortunately, the cn\u2019s\nare not conditionally independent and hence must be sampled sequentially. This limits the size of\nthe datasets we can handle when the process identities are unknown, but our GPU implementation\nis still able to achieve upwards of 4 iterations (sampling all variables) per second on datasets with\nthousands of events.\n4. Stability of Network Hawkes Processes.\nDue to their recurrent nature, Hawkes pro-\ncesses must be constrained to ensure their positive feedback does not lead to in\ufb01nite numbers of\nevents. A stable system must satisfy1\n\u03bbmax = max | eig(A \u2299W ) | < 1\n(see Daley & Vere-Jones, 1988). When we are conditioning on \ufb01nite datasets we do not have to\nworry about this. We simply place weak priors on the network parameters, e.g., a beta prior on\n1In this context \u03bbmax refers to an eigenvalue rather than a rate, and \u2299denotes the Hadamard product.\nDISCOVERING LATENT NETWORK STRUCTURE IN POINT PROCESS DATA\n7\nthe sparsity \u03c1 of an Erd\u02ddos-Renyi graph, and a Je\ufb00reys prior on the scale of the gamma weight\ndistribution. For the generative model, however, we would like to set our hyperparameters such\nthat the prior distribution places little mass on unstable networks. In order to do so, we use tools\nfrom random matrix theory.\nThe celebrated circular law describes the asymptotic eigenvalue distribution for K \u00d7 K random\nmatrices with entries that are i.i.d. with zero mean and variance \u03c32. As K grows, the eigenvalues are\nuniformly distributed over a disk in the complex plane centered at the origin and with radius \u03c3\n\u221a\nK.\nIn our case, however, the mean of the entries, E[Ak,k\u2032Wk,k\u2032] = \u00b5, is not zero.\nSilverstein (1994) has shown that we can analyze noncentral random matrices by considering\nthem to be perturbations about the mean. Consider A \u2299W = V + U, where V = \u00b5KeKeT\nK is a\ndeterministic rank-one matrix with every entry equal to \u00b5, eK \u2208RK is a column vector with all en-\ntries equal to K\u22121/2, and U is a random matrix with i.i.d. zero-mean entries. Then, as K approaches\nin\ufb01nity, the largest eigenvalue will come from V and will be distributed as \u03bbmax \u223cN(\u00b5K, \u03c32), and\nthe remaining eigenvalues will be uniformly distributed over the complex disc.\nIn the simple case of Wk,k\u2032 \u223cGamma(\u03b1, \u03b2) and Ak,k\u2032 \u223cBern(\u03c1), we have \u00b5 = \u03c1\u03b1/\u03b2 and\n\u03c3 =\np\n\u03c1((1 \u2212\u03c1)\u03b12 + \u03b1)/\u03b2. For a given K, \u03b1 and \u03b2, we can tune the sparsity parameter \u03c1 to achieve\nstability with high probability. We simply set \u03c1 such that the minimum of \u03c3\n\u221a\nK and, say, \u00b5K + 3\u03c3,\nequals one. Figures 2a and 2b show a variety of weight distributions and the maximum stable \u03c1.\nIncreasing the network size, the mean, or the variance will require a concomitant increase in sparsity.\nThis approach relies on asymptotic eigenvalue distributions, and it is unclear how quickly the\nspectra of random matrices will converge to this distribution. To test this, we computed the em-\npirical eigenvalue distribution for random matrices of various size, mean, and variance. We gener-\nated 104 random matrices for each weight distribution in Figure 2a with sizes K = 4, 64, and 1024,\nand \u03c1 set to the theoretical maximum indicated by dots in Figure 2b. The theoretical and empirical\ndistributions of the maximum eigenvalue are shown in Figures 2c and 2d. We \ufb01nd that for small\nmean and variance weights, for example Gamma(1, 5) in the Figure 2c, the empirical results closely\nmatch the theory. As the weights grow larger, as in Gamma(8, 12) in 2d, the empirical eigenvalue\ndistributions have increased variance and lead to a greater than expected probability of unstable\nmatrices for the range of network sizes tested here. We conclude that networks with strong weights\nshould be counterbalanced by strong sparsity limits, or additional structure in the adjacency matrix\nthat prohibits excitatory feedback loops.\n5. Synthetic Results.\nOur inference algorithm is \ufb01rst tested on synthetic data generated\nfrom the network Hawkes model. We perform two tests: a) a link prediction task where the process\nidentities are given and the goal is to simply infer whether or not an interaction exists, and b) an\nevent prediction task where we measure the probability of held-out event sequences.\nThe network Hawkes model can be used for link prediction by considering the posterior probabil-\nity of interactions P(Ak,k\u2032 | {sn, cn}). By thresholding at varying probabilities we compute a ROC\ncurve. A standard Hawkes process assumes a complete set of interactions (Ak,k\u2032 \u22611), but we can\nsimilarly threshold its inferred weight matrix to perform link prediction.\nCross correlation provides a simple alternative measure of interaction. By summing the cross-\ncorrelation over o\ufb00sets \u2206t \u2208[0, \u2206tmax), we get a measure of directed interaction. A probabilistic\nalternative is o\ufb00ered by the generalized linear model for point processes (GLM), a popular model\nfor spiking dynamics in computational neuroscience (Paninski, 2004). The GLM allows for constant\nbackground rates and both excitatory and inhibitory interactions. Impulse responses are modeled\nwith linear basis functions. Area under the impulse response provides a measure of directed ex-\ncitatory interaction that we use to compute a ROC curve. See the supplementary material for a\ndetailed description of this model.\n8\nS. W. LINDERMAN AND R. P. ADAMS\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.2\n0.4\n0.6\n0.8\n1\nFP rate\nTP rate\nSynthetic Link Prediction ROC\n \n \nNet. Hawkes\nStd. Hawkes\nGLM\nXCorr\n(a)\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n0\n1\n2\n3\n4\n5\nNetwork\nPred. log lkhd. (bits/s)\nSynthetic Predictive Log Likelihood\n \n \nNet Hawkes\nStd Hawkes\nGLM\n(b)\nFig 3: (a) Comparison of models on a link prediction test averaged across ten randomly sampled\nsynthetic networks of 30 nodes each. The network Hawkes model with the correct Erd\u02ddos-Renyi\ngraph prior outperforms a standard Hawkes model, GLM, and simple thresholding of the cross-\ncorrelation matrix. (b) Comparison of predictive log likelihoods for the same set of networks as\nin Figure 3a, compared to a baseline of a Poisson process with constant rate. Improvement in\npredictive likelihood over baseline is normalized by the number of events in the test data to obtain\nunits of \u201cbits per spike.\u201d Again, the network Hawkes model outperforms the competitors in all but\none sample network.\nWe sampled ten network Hawkes processes of 30 nodes each with Erd\u02ddos-Renyi graph models,\nconstant background rates, and the priors described in Section 3. The Hawkes processes were\nsimulated for T = 1000 seconds. We used the models above to predict the presence or absence of\ninteractions. The results of this experiment are shown in the ROC curves of Figure 3a. The network\nHawkes model accurately identi\ufb01es the sparse interactions, outperforming all other models.\nWith the Hawkes process and the GLM we can evaluate the log likelihood of held-out test data.\nOn this task, the network Hawkes outperforms the competitors for 9 out 10 networks. On average,\nthe network Hawkes model achieves 2.2 \u00b1 .1 bits/spike improvement in predictive log likelihood\nover a homogeneous Poisson process. Figure 3b shows that on average the standard Hawkes and\nthe GLM provide only 60% and 72%, respectively, of this predictive power. See the supplementary\nmaterial for further analysis.\n6. Trades on the S&P 100.\nAs an example of how Hawkes processes may discover inter-\npretable latent structure in real-world data, we study the trades on the S&P 100 index collected\nat 1s intervals during the week of Sep. 28 through Oct. 2, 2009. Every time a stock price changes\nby \u00b10.1% of its current price an event is logged on the stock\u2019s process, yielding a total of K = 100\nprocesses and N=182,037 events.\nTrading volume varies substantially over the course of the day, with peaks at the opening and\nDISCOVERING LATENT NETWORK STRUCTURE IN POINT PROCESS DATA\n9\nclosing of the market. This daily variation is incorporated into the background rate via a Log\nGaussian Cox Process (LGCP) with a periodic kernel (see supplementary material). We look for\nshort-term interactions on top of this background rate with time scales of \u2206tmax = 60s.\nIn Figure 4 we compare the predictive performance of independent LGCPs, a standard Hawkes\nprocess with LGCP background rates, and the network Hawkes model with LGCP background\nrates under two graph priors. The models are trained on four days of data and tested on the\n\ufb01fth. Though the network Hawkes is slightly outperformed by the standard Hawkes, the di\ufb00erence\nis small relative to the performance improvement from considering interactions, and the inferred\nnetwork parameters provide interpretable insight into the market structure.\nFinancial Model\nPred. log lkhd. (bits/spike)\nIndep. LGCP\n0.579 \u00b1 0.006\nStd. Hawkes\n0.903 \u00b1 0.003\nNet. Hawkes (Erd\u02ddos-Renyi)\n0.893 \u00b1 0.003\nNet. Hawkes (Latent Distance)\n0.879 \u00b1 0.004\nFig 4: Comparison of \ufb01nancial models on a event prediction task, relative to a homogeneous Poisson\nprocess baseline.\nIn the latent distance model for A, each stock has a latent embedding xk \u2208R2 such that nearby\nstocks are more likely to interact, as described in Section 2.3. Figure 5 shows a sample from the\nposterior distribution over embeddings in R2 for \u03c1 = 0.2 and \u03c4 = 1. We have plotted stocks in the\nsix largest sectors, as listed on Bloomberg.com. Some sectors, notably energy and \ufb01nancials, tend\nto cluster together, indicating an increased probability of interaction between stocks in the same\nsector. Other sectors, such as consumer goods, are broadly distributed, suggesting that these stocks\nare less in\ufb02uenced by others in their sector. For the consumer industry, which is driven by slowly\nvarying factors like inventory, this may not be surprising.\nThe Hinton diagram in the bottom panel of Figure 5 shows the top 4 eigenvectors of the in-\nteraction network. All eigenvalues are less than 1, indicating that the system is stable. The top\nrow corresponds to \ufb01rst eigenvector (\u03bbmax = 0.74). Apple (AAPL), J.P. Morgan (JPM), and Exxon\nMobil (XOM) have notably large entries in the eigenvector, suggesting that their activity will spawn\ncascades of self-excitation. The fourth eigenvector (\u03bb4 = 0.34) is dominated by Walgreens (WAG) and\nCVS (CVS), suggesting bursts of activity in these drug stores, perhaps due to encouraging quarterly\nreports during \ufb02u season (Associated Press, 2012).\n7. Gangs of Chicago.\nIn our \ufb01nal example, we study spatiotemporal patterns of gang-related\nhomicide in Chicago. Sociologists have suggested that gang-related homicide is mediated by un-\nderlying social networks and occurs in mutually-exciting, retaliatory patterns (Papachristos, 2009).\nThis is consistent with a spatiotemporal Hawkes process in which processes correspond to gang\nterritories and homicides incite further homicides in rival territories.\nWe study gang-related homicides between 1980 and 1995 (Block et al., 2005). Homicides are\nlabeled by the community in which they occurred. Over this time-frame there were N = 1637\ngang-related homicides in the 77 communities of Chicago.\nWe evaluate our model with an event-prediction task, training on 1980-1993 and testing on\n1994-1995. We use a Log Gaussian Cox Process (LGCP) temporal background rate in all model\nvariations. Our baseline is a single process with a uniform spatial rate for the city. We test two\nprocess identity models: a) the \u201ccommunity\u201d model, which considers each community a separate\nprocess, and b) the \u201ccluster\u201d model, which groups communities into processes. The number of\n10\nS. W. LINDERMAN AND R. P. ADAMS\n\u03c4\nLatent Dimension 1\nLatent Dimension 2\nInferred Embedding of Financial Stocks\n \n \nIT\nFinancials\nEnergy\nHealth Care\nConsumer\nIndustrials\nAAPL\nJNJ\nJPM\nCVS\nPG\nWAG\nXOM\nTop 4 eigenvectors of A \u22c5W\nFig 5: Top: A sample from the posterior distribution over embeddings of stocks from the six largest\nsectors of the S&P100 under a latent distance graph model with two latent dimensions. Scale\nbar: the characteristic length scale of the latent distance model. The latent embedding tends to\nembed stocks such that they are nearby to, and hence more likely to interact with, others in their\nsector. Bottom: Hinton diagram of the top 4 eigenvectors. Size indicates magnitude of each stock\u2019s\ncomponent in the eigenvector and colors denote sectors as in the top panel, with the addition of\nMaterials (aqua), Utilities (orange), and Telecomm (gray). We show the eigenvectors corresponding\nto the four largest eigenvalues \u03bbmax = 0.74 (top row) to \u03bb4 = 0.34 (bottom row).\nclusters is chosen by cross-validation (see supplementary material). For each process identity model,\nwe compare four graph models: a) independent LGCPs (empty), b) a standard Hawkes process with\nall possible interactions (complete), c) a network Hawkes model with a sparsity-inducing Erd\u02ddos-\nRenyi graph prior, and d) a network Hawkes model with a latent distance model that prefers\nshort-range interactions.\nThe community process identity model improves predictive performance by accounting for higher\nrates in South and West Chicago where gangs are deeply entrenched. Allowing for interactions\nbetween community areas, however, results in a decrease in predictive power due to over\ufb01tting\n(there is insu\ufb03cient data to \ufb01t all 772 potential interactions). Interestingly, sparse graph priors do\nnot help. They bias the model toward sparser but stronger interactions which are not supported\nby the test data. These results are shown in the \u201ccommunities\u201d group of Figure 6a. Clustering the\nDISCOVERING LATENT NETWORK STRUCTURE IN POINT PROCESS DATA\n11\nCommunities\nClusters\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nProcess ID Model\nPred. Log Lkhd (bits/s)\n \n \nEmpty\nComplete\nErdos\u2212Renyi\nDistance\n(a)\nReceiving Cluster\nInitiating Cluster\nCluster Interactions\n \n \n1\n2\n3\n4\n1\n2\n3\n4\n0\n20\n40\n(b)\n\u221287.9\n\u221287.8\n\u221287.7\n\u221287.6\n\u221287.5\n41.6\n41.65\n41.7\n41.75\n41.8\n41.85\n41.9\n41.95\n42\n42.05\n42.1\nInferred Gang Regions\n \n \nCluster 1\nCluster 2\nCluster 3\nCluster 4\n(c)\n1980\n1985\n1990\n1994\n  0\n  1\n  2\n  3\n\u03bb1(t)\n1980\n1985\n1990\n1994\n  0\n  1\n  2\n  3\n\u03bb2(t)\n1980\n1985\n1990\n1994\n  0\n  1\n  2\n  3\n\u03bb3(t)\n1980\n1985\n1990\n1994\n  0\n  1\n  2\n  3\n\u03bb4(t) [Hom/Day/km2]\n\u00d710\u22123\n \n \nOffset\nBackground\nInteractions\n(d)\nFig 6: Inferred interactions among clusters of community areas in the city of Chicago. (a) Predictive\nlog likelihood for \u201ccommunities\u201d and \u201cclusters\u201d process identity models and four graph models.\nPanels (b-d) present results for the model with the highest predictive log likelihood: an Erd\u02ddos-Renyi\ngraph with K = 4 clusters. (b) The weighted interaction network in units of induced homicides over\nthe training period (1980-1993). (c) Inferred clustering of the 77 community areas. (d) The intensity\nfor each cluster, broken down into the o\ufb00set, the shared background rate, and the interactions (units\nof 10\u22123 homicides per day per square kilometer).\ncommunities improves predictive performance for all graph models, as seen in the \u201cclusters\u201d group.\nMoreover, the clustered models bene\ufb01t from the inclusion of excitatory interactions, with the highest\npredictive log likelihoods coming from a four-cluster Erd\u02ddos-Renyi graph model with interactions\nshown in Figure 6b. Distance-dependent graph priors do not improve predictive performance on this\ndataset, suggesting that either interactions do not occur over short distances, or that local rivalries\nare not substantial enough to be discovered in our dataset. More data is necessary to conclusively\nsay which.\nLooking into the inferred clusters in Figure 6c and their rates in 6d, we can interpret the clusters\nas \u201csafe suburbs\u201d in gold, \u201cbu\ufb00er neighborhoods\u201d in green, and \u201cgang territories\u201d in red and blue.\nSelf-excitation in the blue cluster (Figure 6b) suggests that these regions are prone to bursts of\nactivity, as one might expect during a turf-war. This interpretation is supported by reports of \u201ca\nburst of street-gang violence in 1990 and 1991\u201d in West Englewood (41.77\u25e6N, \u221287.67\u25e6W) (Block\n& Block, 1993).\nFigure 6d also shows a signi\ufb01cant increase in the homicide rate between 1989 and 1995, consistent\nwith reports of escalating gang warfare (Block & Block, 1993). In addition to this long-term trend,\nhomicide rates show a pronounced seasonal e\ufb00ect, peaking in the summer and tapering in the winter.\nA LGCP with a quadratic kernel point-wise added to a periodic kernel captures both e\ufb00ects.\n8. Related Work.\nMultivariate point processes are of great interest to the machine learning\ncommunity as they are intuitive models for a variety of natural phenomena. We have leveraged\n12\nS. W. LINDERMAN AND R. P. ADAMS\nprevious work on Poisson processes with Gaussian process intensities in our background rate models\n(Cunningham et al., 2007). An expectation-maximization inference algorithm for Hawkes processes\nwas put forth by Simma & Jordan (2010) and applied to very large social network datasets. We have\nadapted their latent variable formulation in our fully-Bayesian inference algorithm and introduced\na framework for prior distributions over the latent network.\nOthers have considered special cases of the model we have proposed. Blundell et al. (2012) com-\nbine Hawkes processes and the In\ufb01nite Relational Model (a speci\ufb01c exchangeable graph model with\nan Aldous-Hoover representation) to cluster processes and discover interactions. Cho et al. (2013)\napplied Hawkes processes to gang incidents in Los Angeles. They developed a spatial Gaussian\nmixture model (GMM) for process identities, but did not explore structured network priors. We\nexperimented with this process identity model but found that it su\ufb00ers in predictive log likelihood\ntests (see supplementary material).\nRecently, Iwata et al. (2013) developed a stochastic EM algorithm for Hawkes processes, lever-\naging similar conjugacy properties, but without network priors. Zhou et al. (2013) have developed\na promising optimization-based approach to discovering low-rank networks in Hawkes processes,\nsimilar to some of the network models we explored.\nPerhaps the most closely related work is that of Perry & Wolfe (2013). They provide a partial\nlikelihood inference algorithm for Hawkes processes with a similar emphasis on structural patterns\nin the network of interactions. They provide an estimator capable of discovering homophily (the\ntendency for similar processes to interact) and other network e\ufb00ects. Our fully-Bayesian approach\ngeneralizes this method to capitalize on recent developments in random network models (Lloyd\net al., 2012) and allows for nonparametric background rates.\nFinally, generalized linear models (GLMs) are widely used in computational neuroscience (Panin-\nski, 2004). GLMs allow for both excitatory and inhibitory interactions, but, as we have shown, when\nthe data consists of purely excitatory interactions, Hawkes processes outperform GLMs in link- and\nevent-prediction tests.\n9. Conclusion.\nWe developed a framework for discovering latent network structure from spik-\ning data. Our auxiliary variable formulation of the multivariate Hawkes process supported arbitrary\nAldous-Hoover graph priors, Log Gaussian Cox Process background rates, and models of unobserved\nprocess identities. Our parallel MCMC algorithm allowed us to reason about uncertainty in the la-\ntent network in a fully-Bayesian manner, taking into account noisy observations and prior beliefs.\nWe leveraged results from random matrix theory to analyze the conditions under which random\nnetwork models will be stable, and our applications uncovered interpretable latent networks in a\nvariety of synthetic and real-world problems. Generalizing beyond the Hawkes observation model\nis a promising avenue for future work.\nAcknowledgements. The authors wish to thank Leslie Valiant for many valuable discussions.\nSWL is supported by a National Defense Science and Engineering Graduate Fellowship.\nDISCOVERING LATENT NETWORK STRUCTURE IN POINT PROCESS DATA\n13\nReferences.\nAldous, David J. Representations for partially exchangeable arrays of random variables. Journal of Multivariate\nAnalysis, 11(4):581\u2013598, 1981.\nAssociated Press. Walgreen beats expectations on higher pharmacy sales. The New York Times, September 2012.\nBlock, Carolyn R and Block, Richard. Street gang crime in Chicago. US Department of Justice, O\ufb03ce of Justice\nPrograms, National Institute of Justice, 1993.\nBlock, Carolyn R, Block, Richard, and Authority, Illinois Criminal Justice Information. Homicides in Chicago, 1965-\n1995. ICPSR06399-v5. Ann Arbor, MI: Inter-university Consortium for Political and Social Research [distributor],\nJuly 2005.\nBlundell, Charles, Heller, Katherine, and Beck, Je\ufb00rey. Modelling reciprocating relationships with Hawkes processes.\nAdvances in Neural Information Processing Systems, 2012.\nCho, Yoon Sik, Galstyan, Aram, Brantingham, Je\ufb00, and Tita, George.\nLatent point process models for spatial-\ntemporal networks. arXiv:1302.2671, 2013.\nCunningham, John P, Yu, Byron M, Sahani, Maneesh, and Shenoy, Krishna V. Inferring neural \ufb01ring rates from\nspike trains using Gaussian processes. In Advances in Neural Information Processing Systems, pp. 329\u2013336, 2007.\nDaley, Daryl J and Vere-Jones, David. An introduction to the theory of point processes. 1988, 1988.\nGoldenberg, Anna, Zheng, Alice X, Fienberg, Stephen E, and Airoldi, Edoardo M. A survey of statistical network\nmodels. Foundations and Trends in Machine Learning, 2(2):129\u2013233, 2010.\nHawkes, Alan G. Spectra of some self-exciting and mutually exciting point processes. Biometrika, 58(1):83, 1971.\nHo\ufb00, Peter D. Modeling homophily and stochastic equivalence in symmetric relational data. Advances in Neural\nInformation Processing Systems 20, 20:1\u20138, 2008.\nHoover, Douglas N. Relations on probability spaces and arrays of random variables. Technical report, Institute for\nAdvanced Study, Princeton, 1979.\nIwata, Tomoharu, Shah, Amar, and Ghahramani, Zoubin.\nDiscovering latent in\ufb02uence in online social activities\nvia shared cascade Poisson processes.\nIn Proceedings of the 19th ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, pp. 266\u2013274. ACM, 2013.\nLiben-Nowell, David and Kleinberg, Jon. The link-prediction problem for social networks. Journal of the American\nsociety for information science and technology, 58(7):1019\u20131031, 2007.\nLloyd, James Robert, Orbanz, Peter, Ghahramani, Zoubin, and Roy, Daniel M. Random function priors for exchange-\nable arrays with applications to graphs and relational data. Advances in Neural Information Processing Systems,\n2012.\nMohamed, Shakir, Ghahramani, Zoubin, and Heller, Katherine A. Bayesian and L1 approaches for sparse unsupervised\nlearning. In Proceedings of the 29th International Conference on Machine Learning, pp. 751\u2013758, 2012.\nM\u00f8ller, Jesper, Syversveen, Anne Randi, and Waagepetersen, Rasmus Plenge. Log gaussian cox processes. Scandi-\nnavian Journal of Statistics, 25(3):451\u2013482, 1998.\nMurphy, Kevin P. Machine learning: a probabilistic perspective. The MIT Press, 2012.\nMurray, Iain, Adams, Ryan P., and MacKay, David J.C. Elliptical slice sampling. Journal of Machine Learning\nResearch: Workshop and Conference Proceedings (AISTATS), 9:541\u2013548, 2010.\nPaninski, Liam. Maximum likelihood estimation of cascade point-process neural encoding models. Network: Compu-\ntation in Neural Systems, 15(4):243\u2013262, January 2004.\nPapachristos, Andrew V.\nMurder by structure: Dominance relations and the social structure of gang homicide.\nAmerican Journal of Sociology, 115(1):74\u2013128, 2009.\nPerry, Patrick O and Wolfe, Patrick J. Point process modelling for directed interaction networks. Journal of the\nRoyal Statistical Society: Series B (Statistical Methodology), 2013.\nSilverstein, Jack W. The spectral radii and norms of large dimensional non-central random matrices. Stochastic\nModels, 10(3):525\u2013532, 1994.\nSimma, Aleksandr and Jordan, Michael I. Modeling events with cascades of Poisson processes. Proceedings of the\n26th Conference on Uncertainty in Arti\ufb01cial Intelligence (UAI), 2010.\nZhou, Ke, Zha, Hongyuan, and Song, Le.\nLearning social infectivity in sparse low-rank networks using multi-\ndimensional Hawkes processes. In Proceedings of the International Conference on Arti\ufb01cial Intelligence and Statis-\ntics, volume 16, 2013.\n14\nS. W. LINDERMAN AND R. P. ADAMS\nAPPENDIX A: INFERENCE DETAILS\nA.1. Derivation of conjugate prior updates.\nBy combining Equations 1 and 2 of the main\ntext, we can write the joint likelihood, with the auxiliary parent variables, as,\np({sn, cn, zn}N\nn=1, | {\u03bb0,k(t)}K\nk=1, {hk,k\u2032(\u2206t)}k,k\u2032) =\nK\nY\nk=1\n\u0014\nexp\n\u001a\n\u2212\nZ T\n0\n\u03bb0,k(\u03c4)d\u03c4\n\u001b\nN\nY\nn=1\n\u03bb0,k(sn)\u03b4cn,k\u03b4zn,0\n\u0015\n\u00d7\nN\nY\nn=1\nK\nY\nk\u2032=1\n\u0014\nexp\n\u001a\n\u2212\nZ T\nsn\nhcn,k\u2032(\u03c4 \u2212sn)d\u03c4\n\u001b\nN\nY\nn\u2032=1\nhcn,cn\u2032(sn\u2032 \u2212sn)\n\u03b4cn\u2032 ,k\u2032\u03b4zn\u2032 ,n\n\u0015\n.\nThe \ufb01rst line corresponds to the likelihood of the background processes; the second and third\ncorrespond to the likelihood of the induced processes triggered by each spike.\nTo derive the updates for weights, recall from Equation 3 of the main text that Wk,k\u2032 only appears\nin the impulse responses for which cn = k and cn\u2032 = k\u2032. so we have,\np(Wk,k\u2032 | {sn, cn, zn}N\nn=1, . . .)\n\u221d\nN\nY\nn=1\n\"\nexp\n\u001a\n\u2212\nZ T\nsn\nhk,k\u2032(\u03c4 \u2212sn)d\u03c4\n\u001b\nN\nY\nn\u2032=1\nhk,k\u2032(sn\u2032 \u2212sn)\n\u03b4cn\u2032 ,k\u2032\u03b4zn\u2032 ,n\n#\u03b4cn,k\n\u00d7 p(Wk,k\u2032)\n=\nN\nY\nn=1\n\u0014\nexp\n\u001a\n\u2212\nZ T\nsn\nAk,k\u2032Wk,k\u2032gk,k\u2032(\u03c4 \u2212sn)d\u03c4\n\u001b\nN\nY\nn\u2032=1\n\u0000Ak,k\u2032Wk,k\u2032gk,k\u2032(sn\u2032 \u2212sn)\n\u0001\u03b4cn\u2032 ,k\u2032\u03b4zn\u2032 ,n\n#\u03b4cn,k\n\u00d7 p(Wk,k\u2032).\nIf Ak,k\u2032 = 1 and we ignore spikes after T \u2212\u2206tmax, this is approximately proportional to\nexp\n\b\n\u2212Wk,k\u2032Nk\n\t\nW\nNk,k\u2032\nk,k\u2032 p(Wk,k\u2032),\nwhere\nNk =\nN\nX\nn=1\n\u03b4cn,k, and Nk,k\u2032 =\nN\nX\nn=1\nN\nX\nn\u2032=1\n\u03b4cn,k\u03b4cn\u2032,k\u2032\u03b4zn\u2032,n.\nWhen p(Wk,k\u2032) is a gamma distribution, the conditional distribution is also gamma. If Ak,k\u2032 = 0,\nthe conditional distribution reduces to the prior, as expected.\nSimilar conjugate updates can be derived for constant background rates and the impulse response\nparameters, as stated in the main text.\nA.2. Log Gaussian Cox Process background rates.\nIn the Trades on the S&P100 and the\nGangs of Chicago datasets, it was crucial to model the background \ufb02uctuations that were shared\namong all processes. However, if the background rate is allowed to vary at time scales shorter\nthan \u2206tmax then it may obscure interactions between processes. To prevent this, we sample the\nLog Gaussian Cox Process (LGCP) at a sparse grid of M + 1 equally spaced points and linearly\ninterpolate to evaluate the background rate at the exact time of each event. We have,\ny =\n\u001a\n\u02c6y\n\u0012mT\nM\n\u0013\u001bM\nm=0\n\u223cGP(0, K(t, t\u2032)).\nDISCOVERING LATENT NETWORK STRUCTURE IN POINT PROCESS DATA\n15\nThen,\n\u001a\n\u02c6\u03bb0,k\n\u0012mT\nM\n\u0013\u001bM\nm=0\n= \u00b5k + \u03b1k exp\n\u001a\n\u02c6y\n\u0012mT\nM\n\u0013\u001b\n,\nand \u03bb0,k(sn) is linearly interpolated between the rate at surrounding grid points.\nThe equally spaced grid allows us to calculate the integral using the trapezoid quadrature rule.\nWe use Elliptical Slice Sampling (Murray et al., 2010) to sample the conditional distribution of the\nvector y.\nKernel parameters are set empirically or with prior knowledge. For example, the period of the\nkernel is set to one day for the S&P100 dataset and one year for the Gangs of Chicago dataset\nsince these are well-known trends. The scale and o\ufb00set parameters have log Normal priors set\nsuch that the maximum and minimum homogeneous event counts in the training data are within\ntwo standard deviations of the expected value under the LGCP background rate. That is, the\nbackground rate should be able to explain all of the data without any observations if there is no\nevidence for interactions.\nA.3. Priors on hyperparameters.\nWhen possible, we sample the parameters of the prior\ndistributions. For example, in the Erd\u02ddos-Renyi graph model we place a Beta(1, 1) prior on the\nsparsity \u03c1. For the latent distance model, we place a log normal prior on the characteristic length\nscale \u03c4 and sample it using Hamiltonian Monte Carlo.\nFor all of the results in this paper, we \ufb01xed the prior on the interaction kernel, g(\u2206t) to a weak\nNormal-Gamma distribution with parameters \u00b50\n\u00b5 = \u22121.0, \u03ba0\n\u00b5 = 10, \u03b10\n\u03c4 = 10, and \u03b20\n\u03c4 = 1.\nScale of gamma prior on weights..\nFor real data, we place an uninformative prior on the weight\ndistribution. The gamma distribution is parameterized by a shape \u03b10\nW and an inverse scale or rate\n\u03b20\nW . The shape parameter \u03b10\nW is chosen by hand (typically we use \u03b10\nW = 2), but the inverse scale\nparameter \u03b20\nW is sampled. We may not know a proper scale a priori, however we can use a scale-\ninvariant Je\ufb00rey\u2019s prior to infer this parameter as well. Je\ufb00rey\u2019s prior is proportional to the square\nroot of the Fisher information, which for the gamma distribution is\nPr(\u03b20\nW ) \u221d\nq\nI(\u03b20\nW ) =\nq\n\u03b10\nW\n\u03b20\nW\n.\nHence the posterior is\nPr(\u03b20\nW | {{Wk,k\u2032}}) \u221d\nq\n\u03b10\nW\n\u03b20\nW\nK\nY\nk=1\nK\nY\nk\u2032=1\n(\u03b20\nW )\u03b10\nW\n\u0393(\u03b10\nW ) W\n\u03b10\nW \u22121\nk,k\u2032\ne\u2212\u03b20\nW Wk,k\u2032\n\u221d(\u03b20\nW )K2\u03b10\nW \u22121 exp\n(\n\u2212\u03b20\nW\nK\nX\nk=1\nK\nX\nk\u2032=1\nWk,k\u2032\n)\n.\nThis is a gamma distribution with parameters,\n\u03b20\nW \u223cGamma(K2\u03b10\nW ,\nK\nX\nk=1\nK\nX\nk\u2032=1\nWk,k\u2032).\n16\nS. W. LINDERMAN AND R. P. ADAMS\nAPPENDIX B: SYNTHETIC TEST DETAILS\nWe generated T = 1000s of events for each synthetic network. The average number of spikes\nwas 25,732 \u00b1 9,425. Network 6, the only network for which the GLM outperformed the network\nHawkes model in the event-prediction test, was an outlier with 44,973 events. For event prediction,\nwe trained on the \ufb01rst 900 seconds and tested on the last 100 seconds of the data. We ran our\nMarkov chain for 2500 iterations and computed the posterior probabilities of A and W using the\nlast 500 samples.\nA simple alternative to the Hawkes model is to look at cross-correlation between the event\ntimes. First, the event times are binned into an array \u02c6sk of length M. Let (\u02c6sk \u22c6\u02c6sk\u2032)[m] be the\ncross-correlation between \u02c6sk and \u02c6sk\u2032 at discrete time lag m. Then, Wk,k\u2032 = P\u2206tmaxM/T\nm=0\n(\u02c6sk \u22c6\u02c6sk\u2032)[m]\nprovides a simple measure of directed, excitatory interaction that can be thresholded to perform\nlink prediction.\nAdditionally, we compare the network Hawkes process to the generalized linear model for point\nprocesses, a popular model in computational neuroscience (Paninski, 2004). Here, the event counts\nare modeled as \u02c6sk,m \u223cPoisson(\u03bbk,m). The mean depends on external covariates and other events\naccording to\n\u03bbk,m = exp\n(\n\u03b1T\nk ym +\nK\nX\nk\u2032=1\nB\nX\nb=1\n\u03b2k,k\u2032,b(gb \u2217\u02c6sk\u2032)[m]\n)\n,\nwhere ym is an external covariate at time m, {gb(\u2206m)}B\nb=1 are a set of basis functions that model\nimpulse responses, and \u03b1 and \u03b2 are parameters to be inferred. Under this formulation the log-\nlikelihood of the events is concave function of the parameters and is easily maximized. Unlike the\nHawkes process, however, this model allows for inhibitory interactions.\nFor link prediction, P\nb \u03b2k,k\u2032,b provides a measure of directed excitatory interaction that can be\nused to compute an ROC curve. In our comparisons, we used ym \u22611 to allow for time-homogeneous\nbackground activity and set {gb(\u2206m)} to the top B = 6 principal components of a set of logistic\nnormal impulse responses randomly sampled from the Hawkes prior.\nWe used an L1 penalty to promote sparsity in the parameters of the GLM, and chosen the\npenalty using cross validation on the last 100 seconds of the training data.\nModel\nRelative prediction improvement\nNetwork Hawkes\n100%\nStandard Hawkes\n59.2\u00b114.2%\nGLM\n71.6\u00b19.2%\nFig 7: Relative improvement in predictive log likelihood over a homogeneous Poisson process base-\nline. Relative to the network Hawkes, the standard Hawkes and the GLM yield signi\ufb01cantly less\npredictive power.\nFigure 3b of the main text shows the predictive log likelihoods for the Hawkes model with the\ncorrect Erd\u00a8os-Renyi prior, the standard Hawkes model with a complete graph of interactions, and\na GLM. On all but network 6, the network Hawkes model outperforms the competing models in\nterms of predictive log likelihood. Table 7 shows the average predictive performance across sample\nnextworks. The standard Hawkes and the GLM provide only 59.2% and 71.6%, respectively, of this\npredictive power.\nDISCOVERING LATENT NETWORK STRUCTURE IN POINT PROCESS DATA\n17\nAPPENDIX C: TRADES ON THE S&P100 MODEL DETAILS\nWe study the trades on the S&P 100 index collected at 1s intervals during the week of Sep. 28\nthrough Oct. 2, 2009. We group both positive and negative changes in price into the same process\nin order to measure overall activity. Another alternative would be to generate an \u201cuptick\u201d and a\n\u201cdowntick\u201d process for each stock. We ignored trades outside regular trading hours because they\ntend to be outliers with widely varying prices. Since we are interested in short term interactions,\nwe chose \u2206tmax = 60s. This also limits the number of potential event parents. If we were interested\nin interactions over longer durations, we would have to threshold the price changes at a higher\nlevel. We precluded self-excitation for this dataset since upticks are often followed by downticks\nand vice-versa. We are seeking to explain these brief price jumps using the activity of other stocks.\nWe run our Markov chain for 2000 iterations and compute predictive log likelihoods and the\neigenvalues of the expected interaction matrix, E[A \u2299W ], using the last 400 iterations of the\nchain. The posterior sample illustrated in the main text is the last sample of the chain.\nTrading volume varies substantially over the course of the day, with peaks at the opening and\nclosing of the market. This daily variation is incorporated into the background rate via a Log\nGaussian Cox Process with a periodic kernel. We set the period to one day. Figure 8 shows the\nposterior distribution over the background rate.\nThough it is not discussed in the main text, we also considered stochastic block model (SBM)\npriors as well (Ho\ufb00, 2008), in hopes of recovering latent sector a\ufb03liations based on patterns of\ninteraction between sectors. For example, stocks in the \ufb01nancial sector may have 90% probability\nof interacting with one another, and 30% probability of interacting with stocks in the energy sec-\ntor. Rather than trying to interpret this from the embedding of a latent distance model, we can\ncapture this belief explicitly with a stochastic block model prior on connectivity. We suppose there\nare J sectors, and the probability of belonging to a given sector is \u03b1 \u2208[0, 1]J \u223cDirichlet(\u03b10). The\nlatent sector assignments are represented by the vector b \u2208[1, J]K, where bk \u223cCat(\u03b1). The prob-\nability of a directed interaction is Pr(Ak,k\u2032 = 1) = Bbk,bk\u2032, where B is a J \u00d7 J matrix of Bernoulli\nprobabilities. We place a beta prior on the entries of B.\nOur experiments with the SBM prior yield comparable predictive performance to the latent\ndistance prior, as shown in Figure 9. The inferred clusters (not shown) are correlated with the\nclusters identi\ufb01ed by Bloomberg.com, but more analysis is needed. It would also be interesting to\nstudy the di\ufb00erence in inferred interactions under the various graph models; this is left for future\nwork.\nMon.\nTues.\nWed.\nThur.\n0\n1\n2\nIntensity (events/s)\nInferred S&P100 Background Rate\nFig 8: Posterior distribution over shared background rates for the S&P100. Shading indicates two\nstandard deviations from the mean.\n18\nS. W. LINDERMAN AND R. P. ADAMS\nFinancial Model\nPred. log lkhd. (bits/spike)\nIndep. LGCP\n0.579 \u00b1 0.006\nStd. Hawkes\n0.903 \u00b1 0.003\nNet. Hawkes (Erd\u02ddos-Renyi)\n0.893 \u00b1 0.003\nNet. Hawkes (Latent Distance)\n0.879 \u00b1 0.004\nNet. Hawkes (SBM)\n0.882 \u00b1 0.004\nFig 9: Comparison of \ufb01nancial models on a event prediction task, relative to a homogeneous Poisson\nprocess baseline.\nAPPENDIX D: GANGS OF CHICAGO MODEL DETAILS\nThe \ufb01rst 12 years are used for training, 1993 is reserved for cross-validation, and the remaining\ntwo years are used to test the predictive power of the models. We also considered the crime dataset\nfrom www.data.cityofchicago.org, but this does not identify gang-related incidents.\nWe run our Markov chain for 700 iterations and use the last 200 iterations to compute predictive\nlikelihoods and expectations. The posterior sample illustrated in the \ufb01gure in main text is the last\nsample of the chain. Since this is a spatiotemporal dataset, our intensities are functions of both\nspatial location and time. For simplicity we factorize the intensity into \u03bbk,x(x)\u03bbk,t(t), where \u03bbk,t(t)\nis a Gaussian process as described above, and \u03bbk,x(x) is uniformly distributed over the spatial\nregion associated with process k and is normalized such that it integrates to 1.\nIn the case of the latent distance model with the community process model, each community\u2019s\nlocation is \ufb01xed to its center of mass. With the cluster process model, we introduce a latent location\nfor each cluster and use a Gaussian distribution for the prior probability that a community belongs\nto a cluster. This encourages spatially localized clusters.\nFigure 10 shows the cross validation results used to select the number of clusters, K, in the\nclustered process identity model and each of the four graph models. For the empty, complete, and\nErd\u00a8os-Renyi graph priors, we discover K = 15, 4, and 4 clusters respectively. The latent distance\nmodel, with its prior for spatially localized clusters, has its best performance for K = 5 clusters.\nThe spatial GMM process ID model from Cho et al. (2013) fails on this dataset because it assigns\nits spatial intensity over all of R2, whereas the clustering model concentrates the rate on only the\ncommunities in which the data resides. Figure 11 shows the results of this spatial process ID model\non the prediction task. We did not test a latent distance model with the spatial GMM, but it would\nlikely su\ufb00er in the same way as the empty, complete, and Erd\u02ddos-Renyi graph priors.\n1\n2\n4\n5\n6 10 15 20 25 30\n0\n50\n100\n150\n200\n250\nK\nCross validation log lkhd\nEmpty Graph Model\n2\n4\n5\n6 10 15 20 25 30\n0\n50\n100\n150\n200\n250\nK\nCross validation log lkhd\nComplete Graph Model\n2\n4\n5\n6 10 15 20 25 30\n0\n50\n100\n150\n200\n250\nK\nCross validation log lkhd\nErdos Renyi Graph Model\n2\n4\n5\n6 10 15 20 25 30\n0\n50\n100\n150\n200\n250\nK\nCross validation log lkhd\nLatent Distance Graph Model\nFig 10: Cross validation results for Chicago models with K clusters for each of the four graph\nmodels.\nDISCOVERING LATENT NETWORK STRUCTURE IN POINT PROCESS DATA\n19\nGaussians\nCommunities\nClusters\n\u2212100\n0\n100\n200\n300\nProcess ID Model\nPredictive Log Lkhd\n \n \nEmpty\nComplete\nErdos\u2212Renyi\nDistance\nFig 11: Comparison of predictive log likelihoods for Chicago homicides. This is the same as Figure 6a\nof the main text, but also includes the spatial GMM process identity model.\n",
        "sentence": "",
        "context": "Ho\ufb00, Peter D. Modeling homophily and stochastic equivalence in symmetric relational data. Advances in Neural\nInformation Processing Systems 20, 20:1\u20138, 2008.\nPapachristos, Andrew V.\nMurder by structure: Dominance relations and the social structure of gang homicide.\nAmerican Journal of Sociology, 115(1):74\u2013128, 2009.\nmodels. Foundations and Trends in Machine Learning, 2(2):129\u2013233, 2010.\nHawkes, Alan G. Spectra of some self-exciting and mutually exciting point processes. Biometrika, 58(1):83, 1971."
    },
    {
        "title": "Multivariate hawkes processes",
        "author": [
            "Liniger",
            "Thomas Josef"
        ],
        "venue": "PhD thesis, Diss., Eidgeno\u0308ssische Technische Hochschule ETH Zu\u0308rich, Nr",
        "citeRegEx": "Liniger and Josef.,? \\Q2009\\E",
        "shortCiteRegEx": "Liniger and Josef.",
        "year": 2009,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Distributed representations of words and phrases and their compositionality",
        "author": [
            "Mikolov",
            "Tomas",
            "Sutskever",
            "Ilya",
            "Chen",
            "Kai",
            "Corrado",
            "Greg S",
            "Dean",
            "Jeff"
        ],
        "venue": "In NIPS,",
        "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E",
        "shortCiteRegEx": "Mikolov et al\\.",
        "year": 2013,
        "abstract": "The recently introduced continuous Skip-gram model is an efficient method for\nlearning high-quality distributed vector representations that capture a large\nnumber of precise syntactic and semantic word relationships. In this paper we\npresent several extensions that improve both the quality of the vectors and the\ntraining speed. By subsampling of the frequent words we obtain significant\nspeedup and also learn more regular word representations. We also describe a\nsimple alternative to the hierarchical softmax called negative sampling. An\ninherent limitation of word representations is their indifference to word order\nand their inability to represent idiomatic phrases. For example, the meanings\nof \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\".\nMotivated by this example, we present a simple method for finding phrases in\ntext, and show that learning good vector representations for millions of\nphrases is possible.",
        "full_text": "arXiv:1310.4546v1  [cs.CL]  16 Oct 2013\nDistributed Representations of Words and Phrases\nand their Compositionality\nTomas Mikolov\nGoogle Inc.\nMountain View\nmikolov@google.com\nIlya Sutskever\nGoogle Inc.\nMountain View\nilyasu@google.com\nKai Chen\nGoogle Inc.\nMountain View\nkai@google.com\nGreg Corrado\nGoogle Inc.\nMountain View\ngcorrado@google.com\nJeffrey Dean\nGoogle Inc.\nMountain View\njeff@google.com\nAbstract\nThe recently introduced continuous Skip-gram model is an ef\ufb01cient method for\nlearning high-quality distributed vector representations that capture a large num-\nber of precise syntactic and semantic word relationships. In this paper we present\nseveral extensions that improve both the quality of the vectors and the training\nspeed. By subsampling of the frequent words we obtain signi\ufb01cant speedup and\nalso learn more regular word representations. We also describe a simple alterna-\ntive to the hierarchical softmax called negative sampling.\nAn inherent limitation of word representations is their indifference to word order\nand their inability to represent idiomatic phrases. For example, the meanings of\n\u201cCanada\u201d and \u201cAir\u201d cannot be easily combined to obtain \u201cAir Canada\u201d. Motivated\nby this example, we present a simple method for \ufb01nding phrases in text, and show\nthat learning good vector representations for millions of phrases is possible.\n1\nIntroduction\nDistributed representations of words in a vector space help learning algorithms to achieve better\nperformance in natural language processing tasks by grouping similar words. One of the earliest use\nof word representations dates back to 1986 due to Rumelhart, Hinton, and Williams [13]. This idea\nhas since been applied to statistical language modeling with considerable success [1]. The follow\nup work includes applications to automatic speech recognition and machine translation [14, 7], and\na wide range of NLP tasks [2, 20, 15, 3, 18, 19, 9].\nRecently, Mikolov et al. [8] introduced the Skip-gram model, an ef\ufb01cient method for learning high-\nquality vector representations of words from large amounts of unstructured text data. Unlike most\nof the previously used neural network architectures for learning word vectors, training of the Skip-\ngram model (see Figure 1) does not involve dense matrix multiplications. This makes the training\nextremely ef\ufb01cient: an optimized single-machine implementation can train on more than 100 billion\nwords in one day.\nThe word representations computed using neural networks are very interesting because the learned\nvectors explicitly encode many linguistic regularities and patterns. Somewhat surprisingly, many of\nthese patterns can be represented as linear translations. For example, the result of a vector calcula-\ntion vec(\u201cMadrid\u201d) - vec(\u201cSpain\u201d) + vec(\u201cFrance\u201d) is closer to vec(\u201cParis\u201d) than to any other word\nvector [9, 8].\n1\n\u0001\u0002\u0003\u0004\n\u0005\u0006\u0007\b\u0003\t\t\t\t\t\t\t\t\t\t\t\u0007\n\u000b\f\r\u000e\u0003\u000f\u000b\u0006\t\t\t\t\t\t\u000b\b\u0003\u0007\b\u0003\n\u0001\u0002\u0003\u0010\u0011\u0004\n\u0001\u0002\u0003\u0010\u0012\u0004\n\u0001\u0002\u0003\u0013\u0012\u0004\n\u0001\u0002\u0003\u0013\u0011\u0004\nFigure 1: The Skip-gram model architecture. The training objective is to learn word vector representations\nthat are good at predicting the nearby words.\nIn this paper we present several extensions of the original Skip-gram model. We show that sub-\nsampling of frequent words during training results in a signi\ufb01cant speedup (around 2x - 10x), and\nimproves accuracy of the representations of less frequent words. In addition, we present a simpli-\n\ufb01ed variant of Noise Contrastive Estimation (NCE) [4] for training the Skip-gram model that results\nin faster training and better vector representations for frequent words, compared to more complex\nhierarchical softmax that was used in the prior work [8].\nWord representations are limited by their inability to represent idiomatic phrases that are not com-\npositions of the individual words. For example, \u201cBoston Globe\u201d is a newspaper, and so it is not a\nnatural combination of the meanings of \u201cBoston\u201d and \u201cGlobe\u201d. Therefore, using vectors to repre-\nsent the whole phrases makes the Skip-gram model considerably more expressive. Other techniques\nthat aim to represent meaning of sentences by composing the word vectors, such as the recursive\nautoencoders [15], would also bene\ufb01t from using phrase vectors instead of the word vectors.\nThe extension from word based to phrase based models is relatively simple. First we identify a large\nnumber of phrases using a data-driven approach, and then we treat the phrases as individual tokens\nduring the training. To evaluate the quality of the phrase vectors, we developed a test set of analogi-\ncal reasoning tasks that contains both words and phrases. A typical analogy pair from our test set is\n\u201cMontreal\u201d:\u201cMontreal Canadiens\u201d::\u201cToronto\u201d:\u201cToronto Maple Leafs\u201d. It is considered to have been\nanswered correctly if the nearest representation to vec(\u201cMontreal Canadiens\u201d) - vec(\u201cMontreal\u201d) +\nvec(\u201cToronto\u201d) is vec(\u201cToronto Maple Leafs\u201d).\nFinally, we describe another interesting property of the Skip-gram model. We found that simple\nvector addition can often produce meaningful results. For example, vec(\u201cRussia\u201d) + vec(\u201criver\u201d) is\nclose to vec(\u201cVolga River\u201d), and vec(\u201cGermany\u201d) + vec(\u201ccapital\u201d) is close to vec(\u201cBerlin\u201d). This\ncompositionality suggests that a non-obvious degree of language understanding can be obtained by\nusing basic mathematical operations on the word vector representations.\n2\nThe Skip-gram Model\nThe training objective of the Skip-gram model is to \ufb01nd word representations that are useful for\npredicting the surrounding words in a sentence or a document. More formally, given a sequence of\ntraining words w1, w2, w3, . . . , wT , the objective of the Skip-gram model is to maximize the average\nlog probability\n1\nT\nT\nX\nt=1\nX\n\u2212c\u2264j\u2264c,j\u0338=0\nlog p(wt+j|wt)\n(1)\nwhere c is the size of the training context (which can be a function of the center word wt). Larger\nc results in more training examples and thus can lead to a higher accuracy, at the expense of the\n2\ntraining time. The basic Skip-gram formulation de\ufb01nes p(wt+j|wt) using the softmax function:\np(wO|wI) =\nexp\n\u0010\nv\u2032\nwO\n\u22a4vwI\n\u0011\nPW\nw=1 exp\n\u0010\nv\u2032w\n\u22a4vwI\n\u0011\n(2)\nwhere vw and v\u2032\nw are the \u201cinput\u201d and \u201coutput\u201d vector representations of w, and W is the num-\nber of words in the vocabulary. This formulation is impractical because the cost of computing\n\u2207log p(wO|wI) is proportional to W, which is often large (105\u2013107 terms).\n2.1\nHierarchical Softmax\nA computationally ef\ufb01cient approximation of the full softmax is the hierarchical softmax. In the\ncontext of neural network language models, it was \ufb01rst introduced by Morin and Bengio [12]. The\nmain advantage is that instead of evaluating W output nodes in the neural network to obtain the\nprobability distribution, it is needed to evaluate only about log2(W) nodes.\nThe hierarchical softmax uses a binary tree representation of the output layer with the W words as\nits leaves and, for each node, explicitly represents the relative probabilities of its child nodes. These\nde\ufb01ne a random walk that assigns probabilities to words.\nMore precisely, each word w can be reached by an appropriate path from the root of the tree. Let\nn(w, j) be the j-th node on the path from the root to w, and let L(w) be the length of this path, so\nn(w, 1) = root and n(w, L(w)) = w. In addition, for any inner node n, let ch(n) be an arbitrary\n\ufb01xed child of n and let [[x]] be 1 if x is true and -1 otherwise. Then the hierarchical softmax de\ufb01nes\np(wO|wI) as follows:\np(w|wI) =\nL(w)\u22121\nY\nj=1\n\u03c3\n\u0010\n[[n(w, j + 1) = ch(n(w, j))]] \u00b7 v\u2032\nn(w,j)\n\u22a4vwI\n\u0011\n(3)\nwhere \u03c3(x) = 1/(1 + exp(\u2212x)). It can be veri\ufb01ed that PW\nw=1 p(w|wI) = 1. This implies that the\ncost of computing log p(wO|wI) and \u2207log p(wO|wI) is proportional to L(wO), which on average\nis no greater than log W. Also, unlike the standard softmax formulation of the Skip-gram which\nassigns two representations vw and v\u2032\nw to each word w, the hierarchical softmax formulation has\none representation vw for each word w and one representation v\u2032\nn for every inner node n of the\nbinary tree.\nThe structure of the tree used by the hierarchical softmax has a considerable effect on the perfor-\nmance. Mnih and Hinton explored a number of methods for constructing the tree structure and the\neffect on both the training time and the resulting model accuracy [10]. In our work we use a binary\nHuffman tree, as it assigns short codes to the frequent words which results in fast training. It has\nbeen observed before that grouping words together by their frequency works well as a very simple\nspeedup technique for the neural network based language models [5, 8].\n2.2\nNegative Sampling\nAn alternative to the hierarchical softmax is Noise Contrastive Estimation (NCE), which was in-\ntroduced by Gutmann and Hyvarinen [4] and applied to language modeling by Mnih and Teh [11].\nNCE posits that a good model should be able to differentiate data from noise by means of logistic\nregression. This is similar to hinge loss used by Collobert and Weston [2] who trained the models\nby ranking the data above noise.\nWhile NCE can be shown to approximately maximize the log probability of the softmax, the Skip-\ngram model is only concerned with learning high-quality vector representations, so we are free to\nsimplify NCE as long as the vector representations retain their quality. We de\ufb01ne Negative sampling\n(NEG) by the objective\nlog \u03c3(v\u2032\nwO\n\u22a4vwI) +\nk\nX\ni=1\nEwi\u223cPn(w)\nh\nlog \u03c3(\u2212v\u2032\nwi\n\u22a4vwI)\ni\n(4)\n3\n-2\n-1.5\n-1\n-0.5\n 0\n 0.5\n 1\n 1.5\n 2\n-2\n-1.5\n-1\n-0.5\n 0\n 0.5\n 1\n 1.5\n 2\nCountry and Capital Vectors Projected by PCA\nChina\nJapan\nFrance\nRussia\nGermany\nItaly\nSpain\nGreece\nTurkey\nBeijing\nParis\nTokyo\nPoland\nMoscow\nPortugal\nBerlin\nRome\nAthens\nMadrid\nAnkara\nWarsaw\nLisbon\nFigure 2: Two-dimensional PCA projection of the 1000-dimensional Skip-gram vectors of countries and their\ncapital cities. The \ufb01gure illustrates ability of the model to automatically organize concepts and learn implicitly\nthe relationships between them, as during the training we did not provide any supervised information about\nwhat a capital city means.\nwhich is used to replace every log P(wO|wI) term in the Skip-gram objective. Thus the task is to\ndistinguish the target word wO from draws from the noise distribution Pn(w) using logistic regres-\nsion, where there are k negative samples for each data sample. Our experiments indicate that values\nof k in the range 5\u201320 are useful for small training datasets, while for large datasets the k can be as\nsmall as 2\u20135. The main difference between the Negative sampling and NCE is that NCE needs both\nsamples and the numerical probabilities of the noise distribution, while Negative sampling uses only\nsamples. And while NCE approximately maximizes the log probability of the softmax, this property\nis not important for our application.\nBoth NCE and NEG have the noise distribution Pn(w) as a free parameter. We investigated a number\nof choices for Pn(w) and found that the unigram distribution U(w) raised to the 3/4rd power (i.e.,\nU(w)3/4/Z) outperformed signi\ufb01cantly the unigram and the uniform distributions, for both NCE\nand NEG on every task we tried including language modeling (not reported here).\n2.3\nSubsampling of Frequent Words\nIn very large corpora, the most frequent words can easily occur hundreds of millions of times (e.g.,\n\u201cin\u201d, \u201cthe\u201d, and \u201ca\u201d). Such words usually provide less information value than the rare words. For\nexample, while the Skip-gram model bene\ufb01ts from observing the co-occurrences of \u201cFrance\u201d and\n\u201cParis\u201d, it bene\ufb01ts much less from observing the frequent co-occurrences of \u201cFrance\u201d and \u201cthe\u201d, as\nnearly every word co-occurs frequently within a sentence with \u201cthe\u201d. This idea can also be applied\nin the opposite direction; the vector representations of frequent words do not change signi\ufb01cantly\nafter training on several million examples.\nTo counter the imbalance between the rare and frequent words, we used a simple subsampling ap-\nproach: each word wi in the training set is discarded with probability computed by the formula\nP(wi) = 1 \u2212\ns\nt\nf(wi)\n(5)\n4\nMethod\nTime [min]\nSyntactic [%]\nSemantic [%]\nTotal accuracy [%]\nNEG-5\n38\n63\n54\n59\nNEG-15\n97\n63\n58\n61\nHS-Huffman\n41\n53\n40\n47\nNCE-5\n38\n60\n45\n53\nThe following results use 10\u22125 subsampling\nNEG-5\n14\n61\n58\n60\nNEG-15\n36\n61\n61\n61\nHS-Huffman\n21\n52\n59\n55\nTable 1: Accuracy of various Skip-gram 300-dimensional models on the analogical reasoning task\nas de\ufb01ned in [8]. NEG-k stands for Negative Sampling with k negative samples for each positive\nsample; NCE stands for Noise Contrastive Estimation and HS-Huffman stands for the Hierarchical\nSoftmax with the frequency-based Huffman codes.\nwhere f(wi) is the frequency of word wi and t is a chosen threshold, typically around 10\u22125.\nWe chose this subsampling formula because it aggressively subsamples words whose frequency\nis greater than t while preserving the ranking of the frequencies. Although this subsampling for-\nmula was chosen heuristically, we found it to work well in practice. It accelerates learning and even\nsigni\ufb01cantly improves the accuracy of the learned vectors of the rare words, as will be shown in the\nfollowing sections.\n3\nEmpirical Results\nIn this section we evaluate the Hierarchical Softmax (HS), Noise Contrastive Estimation, Negative\nSampling, and subsampling of the training words. We used the analogical reasoning task1 introduced\nby Mikolov et al. [8]. The task consists of analogies such as \u201cGermany\u201d : \u201cBerlin\u201d :: \u201cFrance\u201d : ?,\nwhich are solved by \ufb01nding a vector x such that vec(x) is closest to vec(\u201cBerlin\u201d) - vec(\u201cGermany\u201d)\n+ vec(\u201cFrance\u201d) according to the cosine distance (we discard the input words from the search). This\nspeci\ufb01c example is considered to have been answered correctly if x is \u201cParis\u201d. The task has two\nbroad categories: the syntactic analogies (such as \u201cquick\u201d : \u201cquickly\u201d :: \u201cslow\u201d : \u201cslowly\u201d) and the\nsemantic analogies, such as the country to capital city relationship.\nFor training the Skip-gram models, we have used a large dataset consisting of various news articles\n(an internal Google dataset with one billion words). We discarded from the vocabulary all words\nthat occurred less than 5 times in the training data, which resulted in a vocabulary of size 692K.\nThe performance of various Skip-gram models on the word analogy test set is reported in Table 1.\nThe table shows that Negative Sampling outperforms the Hierarchical Softmax on the analogical\nreasoning task, and has even slightly better performance than the Noise Contrastive Estimation. The\nsubsampling of the frequent words improves the training speed several times and makes the word\nrepresentations signi\ufb01cantly more accurate.\nIt can be argued that the linearity of the skip-gram model makes its vectors more suitable for such\nlinear analogical reasoning, but the results of Mikolov et al. [8] also show that the vectors learned\nby the standard sigmoidal recurrent neural networks (which are highly non-linear) improve on this\ntask signi\ufb01cantly as the amount of the training data increases, suggesting that non-linear models also\nhave a preference for a linear structure of the word representations.\n4\nLearning Phrases\nAs discussed earlier, many phrases have a meaning that is not a simple composition of the mean-\nings of its individual words. To learn vector representation for phrases, we \ufb01rst \ufb01nd words that\nappear frequently together, and infrequently in other contexts. For example, \u201cNew York Times\u201d and\n\u201cToronto Maple Leafs\u201d are replaced by unique tokens in the training data, while a bigram \u201cthis is\u201d\nwill remain unchanged.\n1code.google.com/p/word2vec/source/browse/trunk/questions-words.txt\n5\nNewspapers\nNew York\nNew York Times\nBaltimore\nBaltimore Sun\nSan Jose\nSan Jose Mercury News\nCincinnati\nCincinnati Enquirer\nNHL Teams\nBoston\nBoston Bruins\nMontreal\nMontreal Canadiens\nPhoenix\nPhoenix Coyotes\nNashville\nNashville Predators\nNBA Teams\nDetroit\nDetroit Pistons\nToronto\nToronto Raptors\nOakland\nGolden State Warriors\nMemphis\nMemphis Grizzlies\nAirlines\nAustria\nAustrian Airlines\nSpain\nSpainair\nBelgium\nBrussels Airlines\nGreece\nAegean Airlines\nCompany executives\nSteve Ballmer\nMicrosoft\nLarry Page\nGoogle\nSamuel J. Palmisano\nIBM\nWerner Vogels\nAmazon\nTable 2: Examples of the analogical reasoning task for phrases (the full test set has 3218 examples).\nThe goal is to compute the fourth phrase using the \ufb01rst three. Our best model achieved an accuracy\nof 72% on this dataset.\nThis way, we can form many reasonable phrases without greatly increasing the size of the vocabu-\nlary; in theory, we can train the Skip-gram model using all n-grams, but that would be too memory\nintensive. Many techniques have been previously developed to identify phrases in the text; however,\nit is out of scope of our work to compare them. We decided to use a simple data-driven approach,\nwhere phrases are formed based on the unigram and bigram counts, using\nscore(wi, wj) =\ncount(wiwj) \u2212\u03b4\ncount(wi) \u00d7 count(wj).\n(6)\nThe \u03b4 is used as a discounting coef\ufb01cient and prevents too many phrases consisting of very infre-\nquent words to be formed. The bigrams with score above the chosen threshold are then used as\nphrases. Typically, we run 2-4 passes over the training data with decreasing threshold value, allow-\ning longer phrases that consists of several words to be formed. We evaluate the quality of the phrase\nrepresentations using a new analogical reasoning task that involves phrases. Table 2 shows examples\nof the \ufb01ve categories of analogies used in this task. This dataset is publicly available on the web2.\n4.1\nPhrase Skip-Gram Results\nStarting with the same news data as in the previous experiments, we \ufb01rst constructed the phrase\nbased training corpus and then we trained several Skip-gram models using different hyper-\nparameters. As before, we used vector dimensionality 300 and context size 5. This setting already\nachieves good performance on the phrase dataset, and allowed us to quickly compare the Negative\nSampling and the Hierarchical Softmax, both with and without subsampling of the frequent tokens.\nThe results are summarized in Table 3.\nThe results show that while Negative Sampling achieves a respectable accuracy even with k = 5,\nusing k = 15 achieves considerably better performance. Surprisingly, while we found the Hierar-\nchical Softmax to achieve lower performance when trained without subsampling, it became the best\nperforming method when we downsampled the frequent words. This shows that the subsampling\ncan result in faster training and can also improve accuracy, at least in some cases.\n2code.google.com/p/word2vec/source/browse/trunk/questions-phrases.txt\nMethod\nDimensionality\nNo subsampling [%]\n10\u22125 subsampling [%]\nNEG-5\n300\n24\n27\nNEG-15\n300\n27\n42\nHS-Huffman\n300\n19\n47\nTable 3:\nAccuracies of the Skip-gram models on the phrase analogy dataset. The models were\ntrained on approximately one billion words from the news dataset.\n6\nNEG-15 with 10\u22125 subsampling\nHS with 10\u22125 subsampling\nVasco de Gama\nLingsugur\nItalian explorer\nLake Baikal\nGreat Rift Valley\nAral Sea\nAlan Bean\nRebbeca Naomi\nmoonwalker\nIonian Sea\nRuegen\nIonian Islands\nchess master\nchess grandmaster\nGarry Kasparov\nTable 4: Examples of the closest entities to the given short phrases, using two different models.\nCzech + currency\nVietnam + capital\nGerman + airlines\nRussian + river\nFrench + actress\nkoruna\nHanoi\nairline Lufthansa\nMoscow\nJuliette Binoche\nCheck crown\nHo Chi Minh City\ncarrier Lufthansa\nVolga River\nVanessa Paradis\nPolish zolty\nViet Nam\n\ufb02ag carrier Lufthansa\nupriver\nCharlotte Gainsbourg\nCTK\nVietnamese\nLufthansa\nRussia\nCecile De\nTable 5: Vector compositionality using element-wise addition. Four closest tokens to the sum of two\nvectors are shown, using the best Skip-gram model.\nTo maximize the accuracy on the phrase analogy task, we increased the amount of the training data\nby using a dataset with about 33 billion words. We used the hierarchical softmax, dimensionality\nof 1000, and the entire sentence for the context. This resulted in a model that reached an accuracy\nof 72%. We achieved lower accuracy 66% when we reduced the size of the training dataset to 6B\nwords, which suggests that the large amount of the training data is crucial.\nTo gain further insight into how different the representations learned by different models are, we did\ninspect manually the nearest neighbours of infrequent phrases using various models. In Table 4, we\nshow a sample of such comparison. Consistently with the previous results, it seems that the best\nrepresentations of phrases are learned by a model with the hierarchical softmax and subsampling.\n5\nAdditive Compositionality\nWe demonstrated that the word and phrase representations learned by the Skip-gram model exhibit\na linear structure that makes it possible to perform precise analogical reasoning using simple vector\narithmetics. Interestingly, we found that the Skip-gram representations exhibit another kind of linear\nstructure that makes it possible to meaningfully combine words by an element-wise addition of their\nvector representations. This phenomenon is illustrated in Table 5.\nThe additive property of the vectors can be explained by inspecting the training objective. The word\nvectors are in a linear relationship with the inputs to the softmax nonlinearity. As the word vectors\nare trained to predict the surrounding words in the sentence, the vectors can be seen as representing\nthe distribution of the context in which a word appears. These values are related logarithmically\nto the probabilities computed by the output layer, so the sum of two word vectors is related to the\nproduct of the two context distributions. The product works here as the AND function: words that\nare assigned high probabilities by both word vectors will have high probability, and the other words\nwill have low probability. Thus, if \u201cVolga River\u201d appears frequently in the same sentence together\nwith the words \u201cRussian\u201d and \u201criver\u201d, the sum of these two word vectors will result in such a feature\nvector that is close to the vector of \u201cVolga River\u201d.\n6\nComparison to Published Word Representations\nMany authors who previously worked on the neural network based representations of words have\npublished their resulting models for further use and comparison: amongst the most well known au-\nthors are Collobert and Weston [2], Turian et al. [17], and Mnih and Hinton [10]. We downloaded\ntheir word vectors from the web3. Mikolov et al. [8] have already evaluated these word representa-\ntions on the word analogy task, where the Skip-gram models achieved the best performance with a\nhuge margin.\n3http://metaoptimize.com/projects/wordreprs/\n7\nModel\nRedmond\nHavel\nninjutsu\ngraf\ufb01ti\ncapitulate\n(training time)\nCollobert (50d)\nconyers\nplauen\nreiki\ncheesecake\nabdicate\n(2 months)\nlubbock\ndzerzhinsky\nkohona\ngossip\naccede\nkeene\nosterreich\nkarate\ndioramas\nrearm\nTurian (200d)\nMcCarthy\nJewell\n-\ngun\ufb01re\n-\n(few weeks)\nAlston\nArzu\n-\nemotion\n-\nCousins\nOvitz\n-\nimpunity\n-\nMnih (100d)\nPodhurst\nPontiff\n-\nanaesthetics\nMavericks\n(7 days)\nHarlang\nPinochet\n-\nmonkeys\nplanning\nAgarwal\nRodionov\n-\nJews\nhesitated\nSkip-Phrase\nRedmond Wash.\nVaclav Havel\nninja\nspray paint\ncapitulation\n(1000d, 1 day)\nRedmond Washington\npresident Vaclav Havel\nmartial arts\ngra\ufb01tti\ncapitulated\nMicrosoft\nVelvet Revolution\nswordsmanship\ntaggers\ncapitulating\nTable 6: Examples of the closest tokens given various well known models and the Skip-gram model\ntrained on phrases using over 30 billion training words. An empty cell means that the word was not\nin the vocabulary.\nTo give more insight into the difference of the quality of the learned vectors, we provide empirical\ncomparison by showing the nearest neighbours of infrequent words in Table 6. These examples show\nthat the big Skip-gram model trained on a large corpus visibly outperforms all the other models in\nthe quality of the learned representations. This can be attributed in part to the fact that this model\nhas been trained on about 30 billion words, which is about two to three orders of magnitude more\ndata than the typical size used in the prior work. Interestingly, although the training set is much\nlarger, the training time of the Skip-gram model is just a fraction of the time complexity required by\nthe previous model architectures.\n7\nConclusion\nThis work has several key contributions. We show how to train distributed representations of words\nand phrases with the Skip-gram model and demonstrate that these representations exhibit linear\nstructure that makes precise analogical reasoning possible. The techniques introduced in this paper\ncan be used also for training the continuous bag-of-words model introduced in [8].\nWe successfully trained models on several orders of magnitude more data than the previously pub-\nlished models, thanks to the computationally ef\ufb01cient model architecture. This results in a great\nimprovement in the quality of the learned word and phrase representations, especially for the rare\nentities. We also found that the subsampling of the frequent words results in both faster training\nand signi\ufb01cantly better representations of uncommon words. Another contribution of our paper is\nthe Negative sampling algorithm, which is an extremely simple training method that learns accurate\nrepresentations especially for frequent words.\nThe choice of the training algorithm and the hyper-parameter selection is a task speci\ufb01c decision,\nas we found that different problems have different optimal hyperparameter con\ufb01gurations. In our\nexperiments, the most crucial decisions that affect the performance are the choice of the model\narchitecture, the size of the vectors, the subsampling rate, and the size of the training window.\nA very interesting result of this work is that the word vectors can be somewhat meaningfully com-\nbined using just simple vector addition. Another approach for learning representations of phrases\npresented in this paper is to simply represent the phrases with a single token. Combination of these\ntwo approaches gives a powerful yet simple way how to represent longer pieces of text, while hav-\ning minimal computational complexity. Our work can thus be seen as complementary to the existing\napproach that attempts to represent phrases using recursive matrix-vector operations [16].\nWe made the code for training the word and phrase vectors based on the techniques described in this\npaper available as an open-source project4.\n4code.google.com/p/word2vec\n8\nReferences\n[1] Yoshua Bengio, R\u00b4ejean Ducharme, Pascal Vincent, and Christian Janvin. A neural probabilistic language\nmodel. The Journal of Machine Learning Research, 3:1137\u20131155, 2003.\n[2] Ronan Collobert and Jason Weston. A uni\ufb01ed architecture for natural language processing: deep neu-\nral networks with multitask learning. In Proceedings of the 25th international conference on Machine\nlearning, pages 160\u2013167. ACM, 2008.\n[3] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Domain adaptation for large-scale sentiment classi-\n\ufb01cation: A deep learning approach. In ICML, 513\u2013520, 2011.\n[4] Michael U Gutmann and Aapo Hyv\u00a8arinen. Noise-contrastive estimation of unnormalized statistical mod-\nels, with applications to natural image statistics. The Journal of Machine Learning Research, 13:307\u2013361,\n2012.\n[5] Tomas Mikolov, Stefan Kombrink, Lukas Burget, Jan Cernocky, and Sanjeev Khudanpur. Extensions of\nrecurrent neural network language model. In Acoustics, Speech and Signal Processing (ICASSP), 2011\nIEEE International Conference on, pages 5528\u20135531. IEEE, 2011.\n[6] Tomas Mikolov, Anoop Deoras, Daniel Povey, Lukas Burget and Jan Cernocky. Strategies for Training\nLarge Scale Neural Network Language Models. In Proc. Automatic Speech Recognition and Understand-\ning, 2011.\n[7] Tomas Mikolov. Statistical Language Models Based on Neural Networks. PhD thesis, PhD Thesis, Brno\nUniversity of Technology, 2012.\n[8] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Ef\ufb01cient estimation of word representations\nin vector space. ICLR Workshop, 2013.\n[9] Tomas Mikolov, Wen-tau Yih and Geoffrey Zweig. Linguistic Regularities in Continuous Space Word\nRepresentations. In Proceedings of NAACL HLT, 2013.\n[10] Andriy Mnih and Geoffrey E Hinton. A scalable hierarchical distributed language model. Advances in\nneural information processing systems, 21:1081\u20131088, 2009.\n[11] Andriy Mnih and Yee Whye Teh. A fast and simple algorithm for training neural probabilistic language\nmodels. arXiv preprint arXiv:1206.6426, 2012.\n[12] Frederic Morin and Yoshua Bengio. Hierarchical probabilistic neural network language model. In Pro-\nceedings of the international workshop on arti\ufb01cial intelligence and statistics, pages 246\u2013252, 2005.\n[13] David E Rumelhart, Geoffrey E Hintont, and Ronald J Williams. Learning representations by back-\npropagating errors. Nature, 323(6088):533\u2013536, 1986.\n[14] Holger Schwenk. Continuous space language models. Computer Speech and Language, vol. 21, 2007.\n[15] Richard Socher, Cliff C. Lin, Andrew Y. Ng, and Christopher D. Manning. Parsing natural scenes and\nnatural language with recursive neural networks. In Proceedings of the 26th International Conference on\nMachine Learning (ICML), volume 2, 2011.\n[16] Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. Semantic Compositionality\nThrough Recursive Matrix-Vector Spaces. In Proceedings of the 2012 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), 2012.\n[17] Joseph Turian, Lev Ratinov, and Yoshua Bengio. Word representations: a simple and general method for\nsemi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computa-\ntional Linguistics, pages 384\u2013394. Association for Computational Linguistics, 2010.\n[18] Peter D. Turney and Patrick Pantel. From frequency to meaning: Vector space models of semantics. In\nJournal of Arti\ufb01cial Intelligence Research, 37:141-188, 2010.\n[19] Peter D. Turney. Distributional semantics beyond words: Supervised learning of analogy and paraphrase.\nIn Transactions of the Association for Computational Linguistics (TACL), 353\u2013366, 2013.\n[20] Jason Weston, Samy Bengio, and Nicolas Usunier. Wsabie: Scaling up to large vocabulary image annota-\ntion. In Proceedings of the Twenty-Second international joint conference on Arti\ufb01cial Intelligence-Volume\nVolume Three, pages 2764\u20132770. AAAI Press, 2011.\n9\n",
        "sentence": " \u2022 We propose an efficient initialization scheme for RNNs using Skip-gram embedding (Mikolov et al., 2013) and show that it improves the performance of the RNN in our problem. (2) We initialize the weight Wemb with a matrix generated by Skip-gram algorithm (Mikolov et al., 2013), then refine the weight Wemb as we train the entire model. We specifically used Skip-gram (Mikolov et al., 2013) to learn real-valued multidimensional vectors to capture the latent representation of medical codes from the EHR.",
        "context": "In this paper we present several extensions of the original Skip-gram model. We show that sub-\nsampling of frequent words during training results in a signi\ufb01cant speedup (around 2x - 10x), and\nup work includes applications to automatic speech recognition and machine translation [14, 7], and\na wide range of NLP tasks [2, 20, 15, 3, 18, 19, 9].\nRecently, Mikolov et al. [8] introduced the Skip-gram model, an ef\ufb01cient method for learning high-\n1\n\u0001\u0002\u0003\u0004\n\u0005\u0006\u0007\b\u0003\t\t\t\t\t\t\t\t\t\t\t\u0007\n\u000b\f\r\u000e\u0003\u000f\u000b\u0006\t\t\t\t\t\t\u000b\b\u0003\u0007\b\u0003\n\u0001\u0002\u0003\u0010\u0011\u0004\n\u0001\u0002\u0003\u0010\u0012\u0004\n\u0001\u0002\u0003\u0013\u0012\u0004\n\u0001\u0002\u0003\u0013\u0011\u0004\nFigure 1: The Skip-gram model architecture. The training objective is to learn word vector representations\nthat are good at predicting the nearby words."
    },
    {
        "title": "Continuous time bayesian networks",
        "author": [
            "Nodelman",
            "Uri",
            "Shelton",
            "Christian R",
            "Koller",
            "Daphne"
        ],
        "venue": "In UAI,",
        "citeRegEx": "Nodelman et al\\.,? \\Q2002\\E",
        "shortCiteRegEx": "Nodelman et al\\.",
        "year": 2002,
        "abstract": "In this paper we present a language for finite state continuous time Bayesian\nnetworks (CTBNs), which describe structured stochastic processes that evolve\nover continuous time. The state of the system is decomposed into a set of local\nvariables whose values change over time. The dynamics of the system are\ndescribed by specifying the behavior of each local variable as a function of\nits parents in a directed (possibly cyclic) graph. The model specifies, at any\ngiven point in time, the distribution over two aspects: when a local variable\nchanges its value and the next value it takes. These distributions are\ndetermined by the variable s CURRENT value AND the CURRENT VALUES OF its\nparents IN the graph.More formally, each variable IS modelled AS a finite state\ncontinuous time Markov process whose transition intensities are functions OF\nits parents.We present a probabilistic semantics FOR the language IN terms OF\nthe generative model a CTBN defines OVER sequences OF events.We list types OF\nqueries one might ask OF a CTBN, discuss the conceptual AND computational\ndifficulties associated WITH exact inference, AND provide an algorithm FOR\napproximate inference which takes advantage OF the structure within the\nprocess.",
        "full_text": "378 \nNODELMAN ET AL. \nUAI2002 \nContinuous Time Bayesian Networks \nUri Nodelman \nStanford University \nnodelman @cs.stanford.edu \nChristian R. Shelton \nStanford University \ncshelton @cs.stanford.edu \nDaphne Koller \nStanford University \nkoller@cs.stanford.edu \nAbstract \nIn this paper we present a language for finite state con\u00ad\ntinuous time Bayesian networks (CTBNs), which de\u00ad\nscribe structured stochastic processes that evolve over \ncontinuous time. The state of the system is decom\u00ad\nposed into a set of local variables whose values change \nover time. The dynamics of the system are descnbed \nby specifying the behavior of each local variable as a \nfunction of its parents in a directed (possibly cyclic) \ngraph. The model specifies, at any given point in time, \nthe distribution over two aspects: when a local variable \nchanges its value and the next value it takes. These \ndistributions are determined by the variable's current \nvalue and the current values of its parents in the graph. \nMore formally, each variable is modelled as a finite \nstate continuous time Markov process whose transi\u00ad\ntion intensities are functions of its parents. We present \na probabilistic semantics for the language in terms of \nthe generative model a CTBN defines over sequences \nof events. We list types of queries one might ask of a \nCTBN, discuss the conceptual and computational diffi\u00ad\nculties associated with exact inference, and provide an \nalgorithm for approximate inference which takes ad\u00ad\nvantage of the structure within the process. \n1 \nIntroduction \nConsider a medical situation where you have administered \na drug to a patient and wish to know how long it will take \nfor the drug to take effect. The answer to this question will \nlikely depend on various factors, such as how recently the \npatient has eaten. We want to model the temporal process \nfor the effect of the drug and how its dynamics depend on \nthese other factors. As another example, we might want \nto predict the amount of time that a person remains unem\u00ad\nployed, which can depend on the state of the economy, on \ntheir own financial situation, and more. \nAlthough these questions touch on a wide variety of is\u00ad\nsues, they are all questions about distributions over time. \nStandard ways of approaching such questions-event his\u00ad\ntory analysis (Blossfeld et al., 1988; B1ossfeld & Rohwer, \n1995; Anderson et al., 1993) and Markov process models \n(Duffie et al., 1996; Lando, 1998)- work well, but do not \nallow the specification of models with a large structured \nstate space where some variables do not directly depend on \nothers. For example, the distribution over how fast a drug \ntakes effect might be mediated by how fast it reaches the \nbloodstream which may itself be affected by how recently \nthe person has eaten. \nBayesian networks (Pearl, 1988) are a standard approach \nfor modelling structured domains. With such a represen\u00ad\ntation we can be explicit about the direct dependencies \nwhich are present and use the independencies to our ad\u00ad\nvantage computationally. However, Bayesian networks are \ndesigned to reason about static processes, and cannot be \nused directly to answer the types of questions that concern \nus here. \nDynamic Bayesian networks (DBNs) (Dean \n& \nKanazawa, 1989) are the standard extension of Bayesian \nnetworks to temporal processes. DBNs model a dynamic \nsystem by discretizing time and providing a Bayesian net\u00ad\nwork fragment that represents the probabilistic transition \nof the state at timet to the state at time t + 1. Thus, DBNs \nrepresent the state of the system at different points in time, \nbut do not represent time explicitly. As a consequence, it \nis very difficult to query a DBN for a distribution over the \ntime at which a particular event takes place. Moreover, \nsince DBNs slice time into fixed increments, one must \nalways propagate the joint distribution over the variables \nat the same rate. This requirement has several limitations. \nFirst, if our system is composed of processes that evolve at \ndifferent time granularities, we must represent the entire \nsystem at the finest possible granularity. Second, if we \nobtain observations which are irregularly spaced in time, \nwe must still represent the intervening time slices at which \nno evidence is obtained. \nHanks et al. (1995) present another discrete time ap\u00ad\nproach to temporal reasoning related to DBNs which they \nextend with a rule-based formalism to model endoge\u00ad\nnous changes to variables which occur between exogenous \nevents. They also include an extensive discussion of vari\u00ad\nous approaches probabilistic temporal reasoning. \nWe provide the alternative framework of continuous time \nBayesian networks. This framework explicitly represents \ntemporal dynamics and allows us to query the network for \nthe distribution over the time when particular events of in\u00ad\nterest occur. Given sequences of observations spaced irreg-\nUAI2002 \nNODELMAN ET AL. \n379 \nularly through time, we can propagate the joint distribution \nfrom observation to observation. Our approach is based \non the framework of homogeneous Markov processes, but \nutilizes ideas from Bayesian networks to provide a graphi\u00ad\ncal representation language for these systems. Endogenous \nchanges are modelled by the state transitions of the pro\u00ad\ncess. The graphical representation allows compact models \nfor processes involving a large number of co-evolving vari\u00ad\nables, and an effective approximate inference procedure \nsimilar to clique tree propagation. \n2 \nContinuous Time \nWe begin with the necessary background on modelling with \ncontinuous time. \n2.1 \nHomogeneous Markov Processes \nOur approach is based on the framework of finite state con\u00ad\ntinuous time Markov processes. Such processes are gener\u00ad\nally defined as matrices of transition intensities where the \n( i, j) entry gives the intensity of transitioning from state \ni to state j and the entries along the main diagonal make \neach row sum to zero. Specifically, our framework will be \nbased on homogeneous Markov processes-one in which \nthe transition intensities do not depend on time. \nLet X be a local variable, one whose state changes con\u00ad\ntinuously over time. Let the domain of X be Val(X) = \n{x1,xz, . . .  ,xn}. We present a homogeneous Markov pro\u00ad\ncess X(t) via its intensity matrix: \nr -ql \nQx = \u001a\u001b q;,, \nlfln 1 cfzn \n. \n' \n-\u00a2, \nwhere qJ = -'iN;'lJ1. Intuitively, the intensity qJ gives \nthe 'instantaneous probability' of leaving state x; and the \nintensity qJ1 gives the 'instantaneous probability' of transi\u00ad\ntioning from x; to x 1. More formally, as !!.t \u008b 0, \nPr{X(t +!!.t) = Xj I X(t) = x;} r:::; qJ1!!.t,for i of j \nPr{X(t+ !!.t) = x; I X(t) =x;} r:::; 1-qf!!.t \nGiven the Qx matrix we can describe the transient behav\u00ad\nior of X(t) as follows. If X(O) = x; then it stays in state \nx; for an amount of time exponentially distributed with pa\u00ad\nrameter qJ. Thus, the probability density function f and \ncorresponding distribution function F for X(t) remaining \nequal to x; are given by \nf(t) = qfexp( -qft), \nt 2:0 \nF(t) = I- exp( -qft), t > 0. \nThe expected time of transitioning is 1 I qJ. Upon transi\u00ad\ntioning, X shifts to state Xj with probability qJ11 qf. \nExample 2.1 Assume that we want to model the behavior \nof the barometric pressure B(t) discretized into three states \n(b, =falling, bz = steady, and b3 = rising), we could write \nthe intensity matrix as [ -. 21 \nQn = \n. 05 \n. 01 \n. 2  \n. 01 ] \n-. 1 \n. 05 \n. \n. 2  -. 21 \nIf we view units for time as hours, this means that if the \npressure is falling, we expect that it will stop falling in a \nlittle less than 5 hours (1 I . 21 hours). It will then transition \nto being steady with probability . 2 j. 21 and to falling with \nprobability . 01 I . 21. \nWe can consider the transitions made between two con\u00ad\nsecutive different states, ignoring the time spent at each \nstate. Specifically, we can define the embedded Markov \nchainE which is formed by ignoring the amount of time X \nspends in its states and noting only the sequence of transi\u00ad\ntions it makes from state to state. We can write out then x n \ntransition probability matrix PE for this chain, by putting \nzeros along the main diagonal and qJ11qJ in the (i,j) entry. \nWe can also consider the distribution over the amount of \ntime X spends in a state before leaving again, ignoring the \nparticular transitions X makes. We can write out then x n \nstate duration matrix M (which is often called the comple\u00ad\ntion rate matrix or holding rate matrix), by putting the q; \nvalues along the main diagonal and zeros everywhere else. \nIt is easy to see that we can describe the original intensity \nmatrix in terms of these two matrices: \nQ =M(PE-1) . \nExample 2.2 For our barometric pressure process B, \nQn = [\n. 2\b .\n \u0013]([? \t 1]-/) \n0 \n0 . 21 \n\u0015 tr 5 \n2.2 \nSubsystems \nIt is often useful to consider subsystems of a Markov pro\u00ad\ncess. A subsystem, S, describes the behavior of the process \nover a subset ofthe full state space-i.e., Val(S) C Val(X). \nIn such cases we can form the intensity matrix of the sub\u00ad\nsystem, Us, by using only those entries from Qx that cor\u00ad\nrespond to states in S. \nExample 2.3 If we want the subsystem of the barometric \npressure process, B, corresponding to the pressure being \nsteady or rising (S = {bz,b3}), we get \n[ -.1 \nUs = \n. 2  \n. 05 ] \n-. 21 \nNote that, for a subsystem, the sums of entries along a \nrow are not, in general, zeros. This is because a subsystem \nis not a closed system-i.e, from each state, there can be a \npositive probability of entering states not in S and thus not \nrepresented in the transition matrix for the subsystem. \n380 \nNODELMAN ET AL. \nUAI2002 \nOnce we have formed a subsystem S of X, we can also \ntalk about the complement subsystemS, which is a subsys\u00ad\ntem over the other states- i.e., Val(S) = Val(X)-Val(S). \nIn general, when examining the behavior of a subsystem, \nwe consider the entrance and exit distributions for the sub\u00ad\nsystem. An entrance distribution is a distribution over the \nstates of S, where the probability of a state s is the prob\u00ad\nability that s is the state to which we first transition when \nentering the subsystem S. An exit distribution describes \nthe first state not in Val(S) to which we transition when we \nleave the subsystem. \n2.3 \nQueries over Markov processes \nIf we have an intensity matrix, Qx, for a homogeneous \nMarkov process X(t) and an initial distribution over the \nvalue of X at time 0, J1, there are many questions about \nthe process which we can answer. \nThe conditional distribution over the value of X at time \nt given the value at an earlier time s is \nPr{X(t) I X(s)} = exp(Qx(t - s)), for s < t .  \nThus, the distribution over the value of X (t) is given by \nPx(t) = f1exp(Qxt) . \nAs t grows, Px (t) approaches the stationary distribution 1t \nfor X which can be computed by an eigenvalue analysis. \nAdditionally, we can form the joint distribution over any \ntwo time points using the above two formulas: \nPx(s,t) =Px(s)exp(Qx(t-s)) . \nSuppose we are interested in some subsystem S of X. \nGiven an entrance distribution 7 into S, we can calculate \nthe distribution over the amount of time that we remain \nwithin the subsystem. This distribution function is called \na phase distribution (Neuts 1975; 1981), and is given by \nF(t) = 1 -6exp(U st)e. \nwhere Us is (as above) the subsystem intensity matrix and \ne is the unit vector. The expected time to remain within the \nsubsystem is given by p0 (-Us) -l e. \nExample 2.4 In our barometric pressure example, if we \nhave a uniform entrance distribution for the subsystem in \nExample 2.3, then the distribution in time over when the \npressure begins to fall is given by \nF(t) = 1 - [ .5 .5 ] exp ([ -:\u0007 -:\u0019i] t) e \n:::e 1 -0.3466( -1.1025') -0.6534( -0.1975') . \nFinally, given an entrance distribution, 7. to a subsys\u00ad\ntem S of X, we can calculate the exit distribution. To do \nso, we construct a new process X' by setting all intensities \nto zero within rows corresponding to states not in S. This \ntransformation, in effect, makes every state which is not in \nthe subsystem an absorbing state. (Once the system has en\u00ad\ntered an absorbing state, it can never leave that state.) If \nwe use our entrance distribution over the states of S for our \ninitial distribution to X' (setting the probability of starting \nin other states to zero), we can see that the exit distribution \nis given by the stationary distribution of X' . This is because \nthe only way that we can enter the newly constructed ab\u00ad\nsorbing states is by leaving S and so the probability with \nwhich we end up in an absorbing state is the probability \nthat we entered that state by exiting the subsystem. \n3 \nContinuous Time Bayesian Nets \nOur goal in this paper is to model Markov processes over \nsystems whose momentary state is defined as an assign\u00ad\nment to some (possibly large) set of variables X. In prin\u00ad\nciple, we can simply explicitly enumerate the state space \nVal(X), and write down the intensity matrix which spec\u00ad\nifies the transition intensity between every pair of these \nstates. However, as in Bayesian networks, the size of the \nstate space grows exponentially with the number of vari\u00ad\nables, rendering this type of representation infeasible for \nall but the smallest spaces. \nIn this section, we provide a more compact factored rep\u00ad\nresentation of Markov processes. We define a continuous \ntime Bayesian network- a graphical model whose nodes \nare variables whose state evolves continuously over time, \nand where the evolution of each variable depends on the \nstate of its parents in the graph. \n3.1 \nConditional Markov Processes \nIn order to compose Markov processes in a larger network, \nwe need to introduce the notion of a conditional Markov \nprocess. This is a type of inhomogeneous Markov process \nwhere the intensities vary with time, but not as a direct \nfunction of time. Rather, the intensities are a function of \nthe current values of a set of other variables, which also \nevolve as Markov processes. We note that a similar model \nwas used by Lando (1998), but the conditioning variables \nwere not viewed as Markov processes, nor were they built \ninto any larger structured model, as in our framework. \nLet Y be a variable whose domain is Val(Y) = \n{yt,Yz, ... ,ym}\u00b7 Assume that Y evolves as a Markov pro\u00ad\ncess Y(t) whose dynamics are conditioned on a set V of \nvariables, each of which also can also evolve over time. \nThen we have a conditional intensity matrix (CIM) which \ncan be written \n\u0120m(V ) l \nIJ2m(V ) \n. \n. \n-c/m(V ) \nEquivalently, we can view a CIM as set of intensity matri\u00ad\nces, one for each instantiation of values v to the variables V .  \nThe set of variables V are called the parents of Y, and de\u00ad\nnoted Par(Y). Note that, if the parent set Par(Y) is empty, \nthen the CIM is simply a standard intensity matrix. \nUAI2002 \nNODELMAN ET AL. \n381 \nFigure 1: Drug effect network \nExample 3.1 Consider a variable E(t) which models \nwhether or not a person is eating ( ei = not eating, ez = \neating) and is conditional on a variable H(t) which mod\u00ad\nels whether or not a person is hungry (hi = not hungry, \nhz =hungry). Then we can specify the C/Mfor E(t) as \n[ -.01 \nQEiht = \n10 \n. 01 ] \n-10 \n-.0\u0006 ] \nGiven this model, we expect a person who is hungry and \nnot eating to begin eating in half an hour (1/2 hour). We \nexpect a person who is not hungry and is eating to stop \neating in 6 minutes ( 1 / 10 hour). \n3.2 \nThe CTBN Model \nConditional intensity matrices provide us a way of mod\u00ad\nelling the local dependence of one variable on a set of oth\u00ad\ners. By putting these local models together, we can define a \nsingle joint structured model. As is the case with dynamic \nBayesian networks, there are two central components to de\u00ad\nfine: the initial distribution and the dynamics with which \nthe system evolves through time. \nDefinition 3.2 Let X be a set of local variables XI, . .. ,Xn. \nEach X; has a finite domain of values Val( X;). A continuous \ntime Bayesian network 9{ over X consists of two compo\u00ad\nnents: The first is an initial distribution J1, specified as a \nBayesian network '13 over X. The second is a continuous \ntransition model, specified as \n\u2022 A directed (possibly cyclic) graph G whose nodes are \nXI, . . .  ,Xn: Par( X;) denotes the parents of X; in G. \n\u2022 A conditional intensity matrix, QxiPar(X)\u2022 for each \nvariable X; EX. I \nUnlike traditional Bayesian networks, there is no prob\u00ad\nlem with cycles in the graph G. An arc X -+ Y in the graph \nimplies that the dynamics of Y's evolution in time depends \non the value of X. There is no reason why the dynamics of \nX's evolution cannot simultaneously depend on the value of \nY. This dependency is analogous to a DBN model where \nwe have arcs X' -+ yt+I andY' -+ x\u2022+I. \nExample 3.3 Figure 1 shows the graph structure for a \nCTBN modelling our drug effect example. There are nodes \nfor the uptake of the drug and for the resulting concentra\u00ad\ntion of the drug in the bloodstream. The concentration is \naffected by the how full the patient's stomach is. The drug is \nsupposed to alleviate joint pain, which may be aggravated \nby falling pressure. The drug may also cause drowsiness. \nThe model contains a cycle, indicating that whether a per\u00ad\nson is hungry depends on how full their stomach is, which \ndepends on whether or not they are eating. \n3.3 \nAmalgamation \nIn order to define the semantics of a CTBN, we must show \nhow to view the entire system as a single process. To do \nthis, we introduce a \"multiplication\" operation called amal\u00ad\ngamation on CIMs. This operation combines two CIMs to \nproduce a single, larger CIM. \nAmalgamation takes two conditional intensity matrices \nQs11c1 and Qs21c2 and forms from them a new product \nCIM, Qslc = QstiCt * Qs21c2 where S = SI U S2 and C = \n(CI UCz)-S. The new CIM contains the intensities for the \nvariables in S conditioned on those of C. A basic assump\u00ad\ntion is that, as time is continuous, variables cannot transi\u00ad\ntion at the same instant. Thus, all intensities corresponding \nto two simultaneous changes are zero. If the changing vari\u00ad\nable is in S I, we can look up the correct intensity from the \nfactor Qst:c1\u2022 Similarly, if it is in Sz, we can look up the \nintensity from the factor Qs21c2\u2022 Intensities along the main \ndiagonal are computed at the end to make the rows sum to \nzero for each instantiation to C. \nExample 3.4 Assume we have a CTBN with graph W =+ Z \nand CIMs \nLet us consider the joint transition intensity of these two \nprocesses. \nAs discussed, intensities such as between \n(ZI, WI) and (zz, wz) are zero. Now, consider a transition \nfrom (ZI, WI) to (ZI, wz). In this case, we simply use the ap\u00ad\npropriate transition intensity from the matrix Qwlzt\u2022 i.e., 1. \nAssuming that the states in the joint space are ordered as \n(ZI,wi),(ZI,wz),(zz,wi),(zz,wz), this would be the value \nof the row 1, column 2 entry of the joint intensity matrix. \nAs another example, the value of the row 4, column 2 entry \nwould be taken from Qzlwz\u00b7 The entries on the diagonal \nare determined at the end, so as to make each row sum to \n0. The joint intensity matrix is, therefore, \n[ -6 \nQwz = QWIZ * QziW = \u0018 \nI \n-9 \n0 \n8 \n5 \n0 \n-9 \n4 \n0] \n7 \n3 \n. \n-12 \nTo provide a formal definition, we need to introduce \nsome notation. Let Qslc(s; -+ Sj I ck) be the intensity spec\u00ad\nified in Qslc for the variables in S changing from states; to \nstate s j conditioned on the variables of C having value q. \n382 \nNODELMAN ET AL. \nUAI2002 \nWe denote the set of variables whose values are different \nbetween the instantiations Sj and Sj as o(i,j). We define \ns[Se] to be the projection of the instantiations onto the set \nof variables Se. Finally, we use (s;,ck) to denote the joint \ninstantiation over S,C consistent with s;,ck. \nQs]c(s;-+ SJ I q) \nQsJ!c, (s;[SI]-+ sJ[SI] I (s;,q)[CI]) \nif lo(i,j)l = I and o(i,j) <;; S1 \nQs,]c,(s;[S2] -+sJ[Sz] I (s;,q)[Cz]) \nif lo(i,j)l =I and o(i,j) <;; Sz \n-Zi]k \nifi= j \n0 \notherwise \nwhere Z;]k = LJ7f;Qs]c(s;-+ SJ I q). \n3.4 \nSemantics \nFormally, let (Q, :F,P) be our probability space, where \nthe space Q consists of a set of infinite trajectories over \nt = [O,oo), and :J is an appropriate a-algebra. (See Gih\u00ad\nman and Skorohod ( 1971) for a formal definition.) \nWe define the semantics of a CTBN as a single homo\u00ad\ngeneous Markov process over the joint state space, using \nthe amalgamation operation. In particular, the CTBN ']{_ is \na factored representation of the homogeneous Markov pro\u00ad\ncess described by the joint intensity matrix defined as \nQ= = n Qx]Par(X) \u00b7 \nXEX \ni,From the definition of amalgamation, we see the states \nof the joint intensity matrix are full instantiations to all of \nthe variables X of ']{_. Moreover, a single variable X E X \ntransitions from x; to XJ with intensity tf;/Par(X)). \nAn alternative view of CTBNs is via a generative se\u00ad\nmantics. A CTBN can be seen as defining a generative \nmodel over sequences of events, where an event is a pair \n(X +--Xj, T), which denotes a transition of the variable X \nto the value x1 at timeT. Given a CTBN ']{_, we can define \nthe generative model as follows. \nWe initialize a to be an empty event sequence. We de\u00ad\nfine a temporary event list E, which contains pairs of state \ntransitions and intensities; the list E is a data structure for \ncandidate events, which is used to compute the next event \nin the event sequence. We also maintain the current time T \nand the current state of the system x(T). Initially, we have \nT = 0, and the system state x(O) is initialized by sampling \nat random from the Bayesian network '13 which denotes the \ninitial state distribution J1:. \nWe then repeat the following steps, where each step se\u00ad\nlects the next event to occur in the system, with the appro\u00ad\npriate distribution. \nFor each variable X that does not have an event in E: \nLet x; = x(t) [X] \nChoose the transition x; -+ XJ according to the \nprobabilities \u00a2;/Par( X)) I qJ (Par( X)) \nAdd (X f- x1,qJ(Par(X))) toE \nLet q E be the sum of all the q values for events in E \nChoose the next event (X f- x1,\u00a2) from E with \nprobability cf I qE \nChoose the time tE for the next transition from an \nexponential distribution with parameter q\u00a3. \nUpdate T f- T + tE and X f- x1 \nAdd (X +--x1,T) to a \nRemove from E the transition for X and for all \nvariables Y for which X E Par(Y). \nDefinition 3.5 Two Markov processes are said to be \nstochastically equivalent if they have the same state space \nand transition probabilities (Gihman & Skorohod, 1973). \nTheorem 3.6 The Markov process determined by the gen\u00ad\nerative semantics is stochastically equivalent to the Markov \nprocess determined by the joint intensity matrix. \nAs in a Bayesian network, the graph structure can be \nviewed in two different yet closely related ways. The first \nis as a data structure with which we can associate parame\u00ad\nters to define a joint distribution. The second is as a quali\u00ad\ntative description of the independence properties of the dis\u00ad\ntribution. To understand this notion, note that there are two \nways to think about a stochastic process X. For a fixed \ntimet E t, X can be viewed as a random variable X(t). For \na fixed ro E Q, we can view X as a function of time (over \nt) and X ( ro) as a trajectory. The CTBN graph specifies a \nnotion of independence over entire trajectories. \nDefinition 3.7 Two Markov processes X and Y are inde\u00ad\npendent if, for any finite sets T, T' C t, the joint distribu\u00ad\ntion over the variables X (t), t E T and the joint distribution \nover the variables Y(t'),t' E T' are independent (Gihman \n& Skorohod, 1971). \nDefinition 3.8 We say that Y is a descendants of X in the \n(possibly cyclic) graph G if and only if there is a directed \npath in G from X to Y. (Note that a variable can be its own \ndescendants according to this definition.) \nTheorem 3.9 If X is a local variable in a CTBN :Ji then \nX is independent of its non-descendants (in G) given tra\u00ad\njectories over the set of variables in Par( X). \nFor example, in our drug effect network, the joint pain is in\u00ad\ndependent of taking the drug given the moment by moment \nconcentration of the drug in the bloodstream. \n4 \nReasoning in CTBNs \nIn this section, we describe some of the queries that we can \naddress using this type of representation. We then discuss \nsome of the computational difficulties that we encounter if \nwe try doing this type of inference exactly. \n4.1 \nQueries over a CTBN \nIn the previous section, we showed that we can view a \nCTBN as a compact representation of a joint intensity ma\u00ad\ntrix for a homogeneous Markov process. Thus, at least in \nUAI2002 \nNODELMAN ET AL. \n383 \nprinciple, we can use a CTBN to answer any query that we \ncan answer using an explicit representation of a Markov \nprocess: We can form the joint intensity matrix and then \nanswer queries just as we do for any homogeneous Markov \nprocess, as described above. \nFor example, in the drug effect network, we can set the \ninitial distribution such that the drug was administered at \nt = 0 hours, compute the joint distribution over the state of \nthe system at t = 5, and then marginalize it to obtain a dis\u00ad\ntribution over joint pain at t = 5. Additionally, because we \nhave the full joint distribution at this point in time, we can \ncalculate for t = 5 the distribution over drowsiness given \nthat the concentration of the drug is high. \nNow, assume that we have a series of observations. We \ncan compute the joint distribution over the system state for \nany point in time at or after the time of the last observa\u00ad\ntion. We calculate the new joint distribution at the time of \nthe first observation, condition on the observation, and use \nthat as the initial distribution from which to compute the \njoint distribution at the next observation time. This process \ncan be executed for an entire series of observations. For \nexample, assume that our patient took the drug at t = 0, \nate after an hour (t = I) and felt drowsy three hours after \neating (t = 4). We can compute the distribution over joint \npain six hours after taking the drug (t = 6) by computing \nthe joint distribution at time I, conditioning that distribu\u00ad\ntion on the observation of eating, and using that as an ini\u00ad\ntial distribution with which to compute the joint distribu\u00ad\ntion 3 hours later. After conditioning on the observation \nof drowsiness, we use the result as an initial distribution \nwith which to calculate the joint distribution 2 hours after \nthat. That joint distribution can be marginalized to give the \ndistribution over joint pain given the sequence of evidence. \nThe key is that, unlike in DBNs, we need only do one prop\u00ad\nagation for each observation time, even if the observations \nare irregularly spaced. \nAs noted in section 2.3 we can compute the joint distri\u00ad\nbution between any two points in time. By conditioning on \nevidence at the later time point, we can propagate evidence \nbackwards in time. Even more interestingly, we can calcu\u00ad\nlate the distribution over the first time a variable X takes on \na particular value x: X taking the value xis simply a subsys\u00ad\ntem of the joint intensity matrix, and we can compute the \ndistribution over the entrance time into the subsystem. For \nexample, we could set our initial distribution to one where \nthe patient takes the drug and has joint pain. We could then \ndirectly compute the distribution over the time at which the \njoint pain goes away. Note that this type of query could also \nbe computed for the time after some sequence of evidence. \n4.2 \nDifficulties with Exact Inference \nThe obvious flaw in our discussion above is that our ap\u00ad\nproach for answering these queries requires that we gener\u00ad\nate the full joint intensity matrix for the system as a whole, \nwhich is exponential in the number of variables. The graph\u00ad\nical structure of the CTBN immediately suggests that we \nperform the inference in a decomposed way, as in Bayesian \nnetworks. Unfortunately, as we now show, the problems \nare significantly more complex in this setting. \nConsider a simple chain X --t Y --t Z. It might appear \nthat, at any point in time, Z is independent of X given Y. \nUnfortunately, this is not the case. Even though the transi\u00ad\ntion intensity for Z depends only on the value of Y at any \ninstance in time, as soon as we consider temporal evolution, \ntheir states become correlated. This problem is completely \nanalogous to the entanglement problem in DBNs (Boyen & \nKoller, 1998), where all variables in the DBN typically be\u00ad\ncome correlated over some number of time slices. The pri\u00ad\nmary difference is that, in continuous time, even the small\u00ad\nest time increment !:li results in the same level of entangle\u00ad\nment as we would gain from an arbitrary number of time \nslices in a DBN. \nIn fact, as discussed in Section 3.4, the only conclusion \nwe can make about a structure X --t Y --t Z is that the Z \nis independent of X given the full trajectory of Y. As a \nconsequence, we can fully reconstruct the distribution over \ntrajectories of Z, ignoring X, if we are given the full dis\u00ad\ntribution over trajectories for Y. Of course, a full distri\u00ad\nbution over continuous time processes is a fairly complex \nstructure. One might hope that we can represent it com\u00ad\npactly, e.g., using an intensity matrix. Unfortunately, even \nwhen the distribution over the joint X, Y process is a ho\u00ad\nmogeneous Markov process, its projection over Y is not a \nhomogeneous Markov process. \nA second potential avenue is the fairly natural conjecture \nthat we do not always need the full distribution over trajec\u00ad\ntories. Perhaps, if our goal is only to answer certain types \nof queries, we can make do with some summary over Y. \nMost obviously, suppose we want to compute the station\u00ad\nary distribution over Z. It seems reasonable to assume that \nZ's stationary behavior might depend only on the stationary \nbehavior of Y. After all, the transitions for Z are governed \nby two matrices Qzly, and QziY2. As long as we know the \nstationary distribution for Y, we know which fraction of the \ntime Z uses each of its transition matrices. So, we should \nbe able to compute the stationary distribution for Z from \nthis information. Unfortunately, this assumption turns out \nto be unfounded. \nExample 4.1 Consider the following intensity matrices. \nQy= \n-3 \n15 \nQyl = [ -10 \n20 \n[ -5 \nQzlyz = \n4 \n10 ] \n-20 \n-\u0017] \nNote that Y and Y' both have the same stationary dis\u00ad\ntribution, [ .75 .25 ]. If we look at the CTBN with \nthe graph Y --t Z we get a stationary distribution for Z \nof [ .7150 .2850 ]. But, if we look at the CTBN with \ngraph Y' --t Z, we get a stationary distribution for Z of \n[ .7418 .2582 ]. \nThus, even the stationary behavior of Z depends on the \nspecific trajectory of Y and not merely the fraction of time \n384 \nNODELMAN ET AL. \nUAI2002 \nit spends in each of its states. We can gain intuition for this \nphenomenon by thinking about the intensity matrix as an \ninfinitesimal transition matrix. To determine the behavior \nof Z, we can imagine that for each infinitesimal moment of \ntime we multiply it to get to the next time instance.' At \neach instance, we check the value of Y and select which \nmatrix we multiply for that instant. The argument that we \ncan restrict attention to the stationary distribution of Y im\u00ad\nplicitly assumes that we care only about \"how many\" times \nwe use each matrix. Unfortunately, matrix multiplication \ndoes not commute. If we are rapidly switching back and \nforth between different values of Y we get a different prod\u00ad\nuct at the end than if we switch between the values more \nslowly. The product is different because the order in which \nwe multiplied was different- even if the number of times \nwe used one matrix or the other were same in both cases. \n5 \nApproximate Inference \nAs we saw in the previous section, exact inference in \nCTBNs is probably intractable. In this section, we de\u00ad\nscribe an approximate inference technique based on the \nclique tree inference algorithm. Essentially, the messages \npassed between cliques are distributions over entire tra\u00ad\njectories, represented as homogeneous Markov processes. \nThese messages are not the correct distributions, but they \noften provide a useful approximation. \nFor example, consider again our chain X -+ Y -+ Z. As \nwe mentioned, to reason about X, we need to pass to Z the \nentire distribution over Y's trajectories. Unfortunately, the \nprojection of the entire process onto Y is not a homoge\u00ad\nneous Markov process. However, we can build a process \nover X, Y, and approximate the distribution over Y's trajec\u00ad\ntories as a homogeneous Markov process. \n5.1 \nThe Clique Tree Algorithm \nRoughly speaking, the basic clique tree calibration step is \nalmost identical to the propagation used in the Shafer and \nShenoy (1990) algorithm, except that we use amalgamation \nas a substitute for products and approximate marginaliza\u00ad\ntion as a substitute for standard marginalization. \nInitialization of the Clique Tree We begin by construct\u00ad\ning the clique tree for the graph G. This procedure is the \nsame as with ordinary Bayesian networks except that we \nmust deal with cycles. We simply connect all parents of a \nnode with undirected edges, and then make all the remain\u00ad\ning edges undirected. If we have a cycle, it simply turns \ninto a loop in the resulting undirected graph. \nAs usual, we associate each variable with a clique that \ncontains it and all of its parents, and assign its CIM to that \nclique. Let A; <;::: C; be the set of variables associated with \nclique C;. Let N; be the set of neighboring cliques for C; and \nlet Sij be the set of variables inC; n Cj. We also compute, \nfor each clique C;, the initial distribution P;(C;). We can \n1 The product integral can be used to make this argument math\u00ad\nematically precise (Gill & Johansen. 1990). \nimplement this operation using standard BN inference on \nthe network 'B. Finally, we calculate the initial intensity \npotential fi for C; as: \nf; = n QXIPar(X) \nXEAi \nwhere our notion of product is amalgamation. \nMessage Passing The message passing process is used \npurely for initial calibration. Its basic goal is to compute, \nin each clique i, an approximate probability distribution \nover the trajectories of the variables C;. This approxima\u00ad\ntion is simply a homogeneous Markov process, and is rep\u00ad\nresented as an initial distribution (computed in the initial\u00ad\nization step) and a joint intensity matrix over C;, computed \nin the calibration step (described below). At this point in \nthe algorithm, no evidence is introduced. \nTo perform the calibration, cliques send messages to \neach other. A clique C; is ready to send message fJ.i-+ j to \nclique C j when it has received messages fJ.k-+i from all the \nneighboring cliques k E N; except possibly j. At that point, \nwe compute and send the message by amalgamating the \nlocal intensity potential with the other messages and elimi\u00ad\nnating all variables except those shared with Cj. More for\u00ad\nmally: \nf-/.i-tj = margfh,-sij) (/; \n* ( n \nf-l.k-ti)) , \nkEN;,ki'J \nwhere marg5 ( QSIC) denotes the operation of taking a CIM \nQsiC and eliminating the variables in Y. As we discussed, \nwe cannot compute an exact representation of a Markov \nprocess after eliminating some subset of the variables. \nTherefore, we will use an approximate marginalization op\u00ad\neration, which we describe below. \nOnce clique C; has received all of its incoming mes\u00ad\nsages, we can compute a local intensity matrix as \nQc, = fi * n f-/.j-ti \nkENi \nAnswering queries After the calibration process, each \nclique i has a joint intensity matrix over the variables inC;, \nwhich, together with the initial distribution, define a homo\u00ad\ngeneous Markov process. We can therefore compute the \napproximate behavior of any of the variables in the clique, \nand answer any of the types of queries described in Sec\u00ad\ntion 4.1, for variables within the same clique. \nIncorporating evidence is slightly more subtle. Assume \nthat we want to introduce evidence over some variable X \nat time t,. We can reason over each Markov process sepa\u00ad\nrately to compute a standard joint distribution P1, ( C;) over \neach clique C; at the time point t,. However, as our ap\u00ad\nproximation is different in different cliques, the distribu\u00ad\ntions over different cliques will not be calibrated: The same \nvariable at different cliques will typically have a different \nmarginal distribution. In order to calibrate the clique tree to \nUAI 2002 \nNODELMAN ET AL. \n385 \ndefine a single coherent joint distribution, we must decide \non a root C r for the tree, and do a standard downward pass \nfrom C, to calibrate all of the cliques to C,. Once that is \ndone, we have a coherent joint distribution, into which we \ncan insert evidence, and which we can query for the proba\u00ad\nbility of any variable of interest. \nIf we have a sequence of observations at times t\u0121, . . .  , tn, \nwe use the process described above to propagate the distri\u00ad\nbution to time t1, we condition on the evidence, and then we \nuse the new clique distributions at time t1 as initial distri\u00ad\nbutions from which we propagate to t2. This process is re\u00ad\npeated until we have propagated to time tn and incorporated \nthe evidence, after which we are ready to answer queries \nthat refer to time(s) after the last evidence point. \nFor evidence after the query time, we propagate the ev\u00ad\nidence from the final evidence point backward, iterating to \nthe query time, constructing the probability of the later ev\u00ad\nidence given the query. Multiplying this by the forward \npropagation from the previous paragraph yields the proba\u00ad\nbility of the query conditioned on all of the evidence. \n5.2 \nMarginalization \nThe core of our algorithm is an \"approximate marginal\u00ad\nization\" operation on intensity matrices which removes a \nvariable (X in our example) from an intensity matrix and \napproximates the resulting distribution over the other vari\u00ad\nables using a simpler intensity matrix. More formally, the \nmarginalization operation takes a CIM QsiC\u2022 a set of vari\u00ad\nables Y C S, and an initial distribution P over the vari\u00ad\nables of S. It returns a reduced CIM of the form Qs'lc = \nmarg5 (Qslc ), where S' = S- Y. \n5.2.1 \nThe Linearization Method \nIdeally, the transition probabilities derived from the \nmarginalized matrix QS'IC would be equal to the actual \ntransition probabilities derived from the original matrix \nQsiC\u00b7 Let s' EB y be the full instantiation to S of s' to S' \nand y to Y. Consider the transition from s 1 = s'1 EB y 1 to \ns2 = s; EB y2 over an interval of length !1t. We would like \nour marginalized process to obey \nP(s;ls'1,c) =I, P(s;EBy1ls'1 EBy2,c)P(yds'1,c) \nY! ,Y2 \nfor all !1t, s1, s2, and c. As discussed above, this is generally \nnot possible. \nOur linearization approach is based on two approxima\u00ad\ntions. First, we assume that the value of the variables in Y \ndo not change over time, so that we can use the values of y \nat the beginning of the interval: \nP(s;ls'1,c) B I,P(s;EByls'1 EBy,c)p0(yls'1,c), \ny \nwhere pO is the distribution at the beginning of the interval. \nSecond, we use a linear approximation to the matrix ex\u00ad\nponential: \nexp( Q!1t) B I+ Q!1t . \nThe resulting approximation is \nQS'Ic(s'1 -+ s; I c) \nB I,Qslc(s'1 EBy-+ s;EBy I c)p<>(y I s'1,c) \ny \nWe call this expression the linear approximation of the \nmarginal. \nExample 5.1 Consider the CIMs from Example 4. I; amal\u00ad\ngamated into a single system, we get: \n[ -4 \nQyz = \nI\u0015 \nI \n-7 \n0 \n4 \n3 \n0 \n-16 \n2 \u0016 ] . \n-6 \nIf J1 = [ .3 .7 j, 4IYI = [ .7 .3 ]. and J1IY2 \n[ .3 .7 ], then the linear approximation is \npO \n[ -4 \nmargy (Qrz) = \n5.6101 \n5.2.2 \nThe Subsystem Method \n-\u0005.6101 ] . \nUnfortunately, unless we plan to do our inference with a \nsignificant amount of time slicing, the assumptions under\u00ad\nlying the above method do not hold. In particular, if we \nwant our approximation to work better over a longer time \ninterval, we need to account for the fact that the variables \nwe are eliminating can change over time. To do this, we \nwill sacrifice some accuracy over short time intervals -\nwhich can be seen in Section 6. \nTo compute the subsystem approximation of the \nmarginal, we first consider each assignment of values s' to \nS'. We take all states s that are consistent with s' (which \ncorrespond to the different assignments to Y), and collapse \nthem into a single state (or row of the intensity matrix). To \nunderstand the approximation, we recall that our approxi\u00ad\nmate intensity matrix Q can be written as M ( P E -I) where \nM is the matrix describing the distribution over how much \ntime we spend in a particular state and PE is the transition \nmatrix for the embedded Markov chain, which determines \nthe distribution over the value to which we transition. We \napproximate these two distributions for each subsystem and \nthen form the new reduced matrix by multiplying. \nOur reduced subsystem corresponding to X' has only a \nsingle state, so its entry in the reduced holding matrix M \nwill be only a single parameter value corresponding to the \nmomentary probability of simply staying in the same state. \nIn our original intensity matrix, this parameter corresponds \nto the probability of staying within the subsystem. Our ap\u00ad\nproximation chooses this parameter so as to preserve the \nexpected time that we stay within the subsystem. \nThe transition matrix PE represents the transition matrix \nfor the chain. Again, as our collapsed system has only a \nsingle state, we are only concerned with parameterizing the \nintensities of transitioning from the new state to states out\u00ad\nside the subsystem. These probabilities are precisely those \nthat characterize the exit distribution from the subsystem. \n386 \nNODELMAN ET AL. \nUA12002 \nBefore providing the exact formulas for these two com\u00ad\nputations, we recall that both the holding time for a sub\u00ad\nsystem and its exit distribution depend on the distribution \nwith which we enter the subsystem. Over time, we can en\u00ad\nter the subsystem multiple times, and the entrance distribu\u00ad\ntion differs each time. For the approximation, however, we \nmust pick a single entrance distribution. There are several \nchoices that we can consider for an approximate entrance \ndistribution. One simple choice is to take the initial distri\u00ad\nbution and use the portion which corresponds to being in \nthe subsystem at the initial time point. We could also use \nthe appropriate portion of the distribution at a later time, t*. \nGiven an entrance distribution pO, we can now compute \nboth the holding time and the exit distribution. Recall that \nthe distribution function over the holding time within a sub\u00ad\nsystem is given by: \nF(t) = 1-f\"Jexp(Ust)e \nIn order to preserve the expected holding time, we must set \nour holding intensity value to be: \nThe approximation for the P E matrix is a single row vector \ncorresponding to the exit distribution from the subsystem, \nusing p0 as our entrance distribution. \nExample 5.2 Consider again the system from Exam\u00ad\nple 5.1. The subsystem approximation for t* = 0 is \npO \n[ -3.7143 \nmargy (Qrz) = \n5.7698 \n3.7143 ] \n-5.7698 \n. \n1/3.7143 = 0.2692 is the expected holding time in the sub-\nsystem \n[ -4 \nUs= \n2 \nwhich corresponds to \nZ \n= zr. \nJ1.1z1(-Us)-1e = 0.2692 where we \nJ1.1zt = [ \u00b75 \u00b75 ]. \n6 \nExperimental Results \nIn particular, \nhave calculated \nFor our experiments, we used the example network de\u00ad\nscribed in Figure 1. We implemented both exact inference \nand our approximation algorithm and compared the results. \nIn our scenario at t = 0, the person modelled by the system \nexperiences joint pain due to falling barometric pressure \nand takes the drug to alleviate the pain, is not eating, has an \nempty stomach, is not hungry, and is not drowsy. The drug \nis uptaking and the current concentration is 0. \nWe consider two scenarios. For both, Figure 2 shows the \nresulting distribution over joint pain as a function of time \nand the KL-divergence between the true joint distribution \nand the estimated joint distribution. In the first scenario \n(top row of plots), no evidence is observed. In the second \nscenario (bottom row of plots), we observe at t = 1 that the \nperson is not hungry and at t = 3, that he is drowsy. \nIn both cases, (a) compares the exact distribution \nwith the approximate distribution for both marginalization \nmethods (linear and subsystem) and differing values oft* \nfor the subsystem approximation. In both cases, we used a \nsingle approximation for the entire trajectory between evi\u00ad\ndence points. By contrast, (b) compares the same approx\u00ad\nimations when the dynamics are repeatedly recalculated \nat regular intervals by using the estimated distribution at \nthe end of one interval as the new entrance distribution for \nthe approximate dynamics of the next interval. The graph \nshows this recomputation at both 1 hr and 6min intervals. \nThe final graph (c) shows the average KL-divergence be\u00ad\ntween the true joint distribution and the approximate joint \ndistributions, averaged over 60 time points between t = 0 \nand t = 6, as the number of (evenly spaced) recalculation \npoints grows for both marginalization methods. \nAs we can see in all of the results, the subsystem approx\u00ad\nimation performs better for longer time segments. How\u00ad\never, when the time-slicing becomes too fine, the errors \nfrom this method grow. By comparison, the linear approx\u00ad\nimation performs very poorly as an approximation for long \nintervals, but its accuracy improves as the time granularity \nbecomes finer. These results are in accordance with the in\u00ad\ntuitions used to build each approximation. The linear ap\u00ad\nproximation makes two assumptions that only hold over \nshort time intervals: eliminated variables do not change \nduring the interval and the exponentiation of a matrix can \nbe linearly approximated. By comparison, the subsystem \napproximation allows for multiple variables to change over \nthe interval of approximation but, in doing so, gives up ac\u00ad\ncuracy for small time scales. \n7 \nDiscussion \nWe have described a new modelling language for structured \nstochastic processes which evolve in continuous time. Be\u00ad\ncause time is explicitly represented in the model, we can \nreason with it directly, and even answer queries which ask \nfor a distribution over time. Moreover, the continuous time \nmodel enables us to deal with sequences of evidence by \npropagating the distribution over the values from the time \nof one observation to the next - even when the evidence \nis not evenly spaced. \nTo compare the CTBN and DBN frameworks, suppose \nwe start with a non-trivial CTBN. For any finite amount \nof time, probabilistic influence can flow between any vari\u00ad\nables connected by a path in the CTBN graph. Thus, if we \nwant to construct an \"equivalent\" DBN, the 2-TBN must \nbe fully connected regardless of the t!.t we choose for each \ntime slice. We can construct a DBN that approximates the \nCTBN by picking a subset of the connections (e.g. those \nwhich have the strongest influence). Yet, we still have the \nstandard problem of exponential blowup in performing in\u00ad\nference over time. So we would be led to perform approxi\u00ad\nmate DBN inference in an approximate DBN. While this \ncould form the basis of an approximation algorithm for \nCTBNs, we chose to work directly with continuous time, \nmaking a direct approximation. \nUAI 2002 \nNODELMAN ET AL. \n387 \nSingle Approximation \nTime-sliced Approximation \nKL-Divergence \nj \n& \n0 \nz \n0.9 \\ \n.s;O.B \n! .ii0.7 \n.. \nOo.e \n,l; \n'\u2022. \njo.s \n\u2022\u2022 \u2022\u2022 \n\u007f .. \n0.3 \n\u2022 z_o.a \ni \ng0.7 \n\u2022 !0.8 \n0.5 \n0.4 \n-exact \n\u00b7\u00b7 linear, 1 hr \n\u00b7 \u00b7 \u00b7  linear, Smin \n\u2022 \u2022 \u2022 subsystem, 1 hr \n\u2022\u00b7\u2022\u00b7 su \nstem, 8 min \n0.1 : \n?0.08 \nfo.oe \nl,,. \n0.02 '\\.:;.:..<: \n\u2022..\n\u2022\u2022\n.\u2022 ---\u2022\u2022\u2022..\u2022.\n.\n..... --. \n0.2'.'-o --7----___.,3,-----;--....,..---J \ntime (hours) \nS--3--7-U \n0o \n10 \n20 \n30 \n40 \n50 \n60 \n.s;;0.8 \n! \n_&0.7 \n.. Oos \n\u2022 \n!0.5 \n> .. \n0.3 \ni0.8 \n.&0.7 \n.. \nOoe \n\u2022 \nto\u2022 \n\"'\u00b7' \n0.3 \ntime (hourt) \n-O><Od \nlinear, 1 hr \n\u00b7 \u2022 \u00b7  llnear.Bmln \n\u2022 \u2022 \u2022 eutMystem, 1 hr \n\u2022\u00b7\u2022\u00b7 IV \nstem,B min \nnumber of points \n0.025 \n------.. ......\n. ---------. \n0.005 \nO.'I)L_--;--\u0014-\u00803-\u0081-...,.___.J \ntime (hours) \n0.2'.\\-o --;---;--R3--::---;-----l \nllrne (hours) \noo \n10 \n20 \n30 \n40 \n50 \n80 \nnumber ol points \n(a) \n(b) \n(c) \nFigure 2: For the case of no evidence (top) and evidence (bottom): (a) The distribution for joint pain for exact and \napproximate inference without recalculation, (b) The distribution for joint pain for exact and approximate inference with \nrecalculation of the dynamics every hour and every 6 minutes, and (c) The KL-divergence between the true distribution \nand the estimated distribution over all variables. \nThere are still several types of queries which we cannot \nyet answer. We cannot deal with situations where the evi\u00ad\ndence has the form \"X stayed at the value x for the entire \nperiod between t1 and tz.\" Nor can we answer queries when \nwe are interested in the distribution over Y at the time when \nX first transitions to x1. \nThere are also many other important open questions. \nThese include a theoretical analysis of the computational \nproperties of these models and a more systematic theoreti\u00ad\ncal and empirical analysis of the nature of our approxima\u00ad\ntion algorithm, leading perhaps to a more informed method \nfor choosing the entrance distribution for the marginaliza\u00ad\ntion operation. As an alternative approach, the generative \nsemantics for CTBN s provides a basis for a stochastic sam\u00ad\npling based method to approximate, which we would like \nto extend to situations with evidence using techniques such \nas importance sampling or MCMC. Even more globally, \nwe would like to pursue parameter learning and structure \nlearning of CTBNs from data. \nAcknowledgments We would like to thank Carlos \nGuestrin and Balaji Prabhakar for their useful comments \nand suggestions. This work was funded by ONR contract \nN00014-00-I-0637 under the MURI program \"Decision \nMaking under Uncertainty.\" \nReferences \nAnderson, P. K., Borgan, 0 .. Gill, R. D., & Keiding, N. (1993). \nStatistical models based on counting processes. \nSpringer\u00ad\nVerlag. \nBlossfeld, H.-P., Hamerle, A., & Mayer, K. U. (1988). Event \nhistory analysis. Lawrence Erlbaum Associates. \nBlossfeld, H.-P., & Rohwer, G. (1995). Techniques of event his\u00ad\ntory modeling . Lawrence Erlbaum Associates. \nBoyen, X., & Koller, D. (1998). Tractable inference for complex \nstochastic processes. Proc. 14th UAI (pp. 33-42). \nDean, T., & Kanazawa, K. ( 1989). A model for reasoning about \npersistence and causation. Comp. Intelligence, 5, 142-150. \nDuffie, D., Schroder, M., & Skiadas, C. (1996). Recursive valu\u00ad\nation of defaultable securities and the timing of resolution of \nuncertainty. The Annals of Applied Probability, 6, 1075-1090. \nGihman, 1.1., & Skorohod, A. V. (1971). The theory of stochastic \nprocesses I. Springer-Verlag. \nGihman, 1.1., & Skorohod, A. V. (1973). The theory of stochastic \nprocesses II. Springer-Verlag. \nGill, R. D., & Johansen, S. (1990). \nA survey of product\u00ad\nintegration with a view towards applications in survival anal\u00ad\nysis. The Annals of Statistics, 18, 1501-1555. \nHanks, S., Madigan, D., & Gavrin, J. (1995). Probabilistic tem\u00ad\nporal reasoning with endogenous change. Proc. lith UAJ. \nLando, D. (1998). On Cox processes and credit risky securities. \nReview of Derivatives Research, 2, 99-120. \nNeuts, M. F. (1975). Probability distributions of phase type. Liber \nAmicorum Prof Emeritus H. Florin (pp. 173-206). \nNeuts, M. F. (1981). \nMatrix-geometric solutions in stochastic \nmodels -an algorithmic approach. JHU Press. \nPearl, J. (1988). Probabilistic reasoning in intelligent systems. \nMorgan Kauffman. \nShafer, G., & Shenoy, P. (1990). Probability propagation. Annals \nof Mathematics and Artificial Intelligence, 2, 327-352. \n",
        "sentence": " The two main class of techniques, continuous-time Markov chain based models (Nodelman et al., 2002; Lange et al., 2015; Johnson & Willsky, 2013), and intensity based point process modeling techniques such as Hawkes processes (Liniger, 2009; Zhu, 2013; Choi et al. Among the continuous-time models, there are two main techniques: continuoustime Markov chain based models (Nodelman et al., 2002; Foucher et al., 2007; Johnson & Willsky, 2013; Lange, 2014) and intensity function modeling techniques such as Cox and Hawkes processes (Liniger, 2009; Zhou et al.",
        "context": "continuous time. \n2.1 \nHomogeneous Markov Processes \nOur approach is based on the framework of finite state con\u00ad\ntinuous time Markov processes. Such processes are gener\u00ad\nally defined as matrices of transition intensities where the\nfor processes involving a large number of co-evolving vari\u00ad\nables, and an effective approximate inference procedure \nsimilar to clique tree propagation. \n2 \nContinuous Time \nWe begin with the necessary background on modelling with \ncontinuous time. \n2.1\nresentation of Markov processes. We define a continuous \ntime Bayesian network- a graphical model whose nodes \nare variables whose state evolves continuously over time, \nand where the evolution of each variable depends on the"
    },
    {
        "title": "The survival filter: Joint survival analysis with a latent time series",
        "author": [
            "Ranganath",
            "Rajesh",
            "Perotte",
            "Adler",
            "Elhadad",
            "No\u00e9mie",
            "Blei",
            "David M"
        ],
        "venue": "In UAI,",
        "citeRegEx": "Ranganath et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "Ranganath et al\\.",
        "year": 2015,
        "abstract": "",
        "full_text": "",
        "sentence": " When the time axis is discretized, the point process data can be converted to binary time series (or time series of count data if binning is coarse) and analyzed via time series analysis techniques (Truccolo et al., 2005; Bahadori et al., 2013; Ranganath et al., 2015).",
        "context": null
    },
    {
        "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
        "author": [
            "Saxe",
            "Andrew M",
            "McClelland",
            "James L",
            "Ganguli",
            "Surya"
        ],
        "venue": "arXiv preprint arXiv:1312.6120,",
        "citeRegEx": "Saxe et al\\.,? \\Q2013\\E",
        "shortCiteRegEx": "Saxe et al\\.",
        "year": 2013,
        "abstract": "Despite the widespread practical success of deep learning methods, our\ntheoretical understanding of the dynamics of learning in deep neural networks\nremains quite sparse. We attempt to bridge the gap between the theory and\npractice of deep learning by systematically analyzing learning dynamics for the\nrestricted case of deep linear neural networks. Despite the linearity of their\ninput-output map, such networks have nonlinear gradient descent dynamics on\nweights that change with the addition of each new hidden layer. We show that\ndeep linear networks exhibit nonlinear learning phenomena similar to those seen\nin simulations of nonlinear networks, including long plateaus followed by rapid\ntransitions to lower error solutions, and faster convergence from greedy\nunsupervised pretraining initial conditions than from random initial\nconditions. We provide an analytical description of these phenomena by finding\nnew exact solutions to the nonlinear dynamics of deep learning. Our theoretical\nanalysis also reveals the surprising finding that as the depth of a network\napproaches infinity, learning speed can nevertheless remain finite: for a\nspecial class of initial conditions on the weights, very deep networks incur\nonly a finite, depth independent, delay in learning speed relative to shallow\nnetworks. We show that, under certain conditions on the training data,\nunsupervised pretraining can find this special class of initial conditions,\nwhile scaled random Gaussian initializations cannot. We further exhibit a new\nclass of random orthogonal initial conditions on weights that, like\nunsupervised pre-training, enjoys depth independent learning times. We further\nshow that these initial conditions also lead to faithful propagation of\ngradients even in deep nonlinear networks, as long as they operate in a special\nregime known as the edge of chaos.",
        "full_text": "Exact solutions to the nonlinear dynamics of learning in\ndeep linear neural networks\nAndrew M. Saxe (asaxe@stanford.edu)\nDepartment of Electrical Engineering\nJames L. McClelland (mcclelland@stanford.edu)\nDepartment of Psychology\nSurya Ganguli (sganguli@stanford.edu)\nDepartment of Applied Physics\nStanford University, Stanford, CA 94305 USA\nAbstract\nDespite the widespread practical success of deep learning methods, our theoretical under-\nstanding of the dynamics of learning in deep neural networks remains quite sparse. We\nattempt to bridge the gap between the theory and practice of deep learning by systemati-\ncally analyzing learning dynamics for the restricted case of deep linear neural networks.\nDespite the linearity of their input-output map, such networks have nonlinear gradient de-\nscent dynamics on weights that change with the addition of each new hidden layer. We\nshow that deep linear networks exhibit nonlinear learning phenomena similar to those seen\nin simulations of nonlinear networks, including long plateaus followed by rapid transitions\nto lower error solutions, and faster convergence from greedy unsupervised pretraining ini-\ntial conditions than from random initial conditions. We provide an analytical description\nof these phenomena by \ufb01nding new exact solutions to the nonlinear dynamics of deep\nlearning. Our theoretical analysis also reveals the surprising \ufb01nding that as the depth of\na network approaches in\ufb01nity, learning speed can nevertheless remain \ufb01nite: for a special\nclass of initial conditions on the weights, very deep networks incur only a \ufb01nite, depth\nindependent, delay in learning speed relative to shallow networks. We show that, under\ncertain conditions on the training data, unsupervised pretraining can \ufb01nd this special class\nof initial conditions, while scaled random Gaussian initializations cannot. We further ex-\nhibit a new class of random orthogonal initial conditions on weights that, like unsupervised\npre-training, enjoys depth independent learning times. We further show that these initial\nconditions also lead to faithful propagation of gradients even in deep nonlinear networks,\nas long as they operate in a special regime known as the edge of chaos.\nDeep learning methods have realized impressive performance in a range of applications, from visual object\nclassi\ufb01cation [1, 2, 3] to speech recognition [4] and natural language processing [5, 6]. These successes have\nbeen achieved despite the noted dif\ufb01culty of training such deep architectures [7, 8, 9, 10, 11]. Indeed, many\nexplanations for the dif\ufb01culty of deep learning have been advanced in the literature, including the presence of\nmany local minima, low curvature regions due to saturating nonlinearities, and exponential growth or decay\nof back-propagated gradients [12, 13, 14, 15]. Furthermore, many neural network simulations have observed\n1\narXiv:1312.6120v3  [cs.NE]  19 Feb 2014\nstrikingly nonlinear learning dynamics, including long plateaus of little apparent improvement followed by\nalmost stage-like transitions to better performance. However, a quantitative, analytical understanding of the\nrich dynamics of deep learning remains elusive. For example, what determines the time scales over which\ndeep learning unfolds? How does training speed retard with depth? Under what conditions will greedy\nunsupervised pretraining speed up learning? And how do the \ufb01nal learned internal representations depend\non the statistical regularities inherent in the training data?\nHere we provide an exact analytical theory of learning in deep linear neural networks that quantitatively\nanswers these questions for this restricted setting. Because of its linearity, the input-output map of a deep\nlinear network can always be rewritten as a shallow network. In this sense, a linear network does not gain ex-\npressive power from depth, and hence will under\ufb01t and perform poorly on complex real world problems. But\nwhile it lacks this important aspect of practical deep learning systems, a deep linear network can nonetheless\nexhibit highly nonlinear learning dynamics, and these dynamics change with increasing depth. Indeed, the\ntraining error, as a function of the network weights, is non-convex, and gradient descent dynamics on this\nnon-convex error surface exhibits a subtle interplay between different weights across multiple layers of the\nnetwork. Hence deep linear networks provide an important starting point for understanding deep learning\ndynamics.\nTo answer these questions, we derive and analyze a set of nonlinear coupled differential equations describing\nlearning dynamics on weight space as a function of the statistical structure of the inputs and outputs. We\n\ufb01nd exact time-dependent solutions to these nonlinear equations, as well as \ufb01nd conserved quantities in the\nweight dynamics arising from symmetries in the error function. These solutions provide intuition into how\na deep network successively builds up information about the statistical structure of the training data and\nembeds this information into its weights and internal representations. Moreover, we compare our analytical\nsolutions of learning dynamics in deep linear networks to numerical simulations of learning dynamics in\ndeep non-linear networks, and \ufb01nd that our analytical solutions provide a reasonable approximation. Our\nsolutions also re\ufb02ect nonlinear phenomena seen in simulations, including alternating plateaus and sharp pe-\nriods of rapid improvement. Indeed, it has been shown previously [16] that this nonlinear learning dynamics\nin deep linear networks is suf\ufb01cient to qualitatively capture aspects of the progressive, hierarchical differ-\nentiation of conceptual structure seen in infant development. Next, we apply these solutions to investigate\nthe commonly used greedy layer-wise pretraining strategy for training deep networks [17, 18], and recover\nconditions under which such pretraining speeds learning. We show that these conditions are approximately\nsatis\ufb01ed for the MNIST dataset, and that unsupervised pretraining therefore confers an optimization advan-\ntage for deep linear networks applied to MNIST. Finally, we exhibit a new class of random orthogonal initial\nconditions on weights that, in linear networks, provide depth independent learning times, and we show that\nthese initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks. We\nfurther show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear\nnetworks, as long as they operate in a special regime known as the edge of chaos. In this regime, synaptic\ngains are tuned so that linear ampli\ufb01cation due to propagation of neural activity through weight matrices\nexactly balances dampening of activity due to saturating nonlinearities. In particular, we show that even in\nnonlinear networks, operating in this special regime, Jacobians that are involved in backpropagating error\nsignals act like near isometries.\n1\nGeneral learning dynamics of gradient descent\nW 21\nW 32\nx \u2208RN1\nh \u2208RN2\ny \u2208RN3\nFigure 1: The three layer network analyzed\nin this section.\nWe begin by analyzing learning in a three layer network (in-\nput, hidden, and output) with linear activation functions (Fig\n1). We let Ni be the number of neurons in layer i. The input-\noutput map of the network is y = W 32W 21x. We wish to\ntrain the network to learn a particular input-output map from\n2\na set of P training examples {x\u00b5, y\u00b5} , \u00b5 = 1, . . . , P. Training is accomplished via gradient descent on the\nsquared error PP\n\u00b5=1\n\r\ry\u00b5 \u2212W 32W 21x\u00b5\r\r2 between the desired feature output, and the network\u2019s feature\noutput. This gradient descent procedure yields the batch learning rule\n\u2206W 21 = \u03bb\nP\nX\n\u00b5=1\nW 32T \u0000y\u00b5x\u00b5T \u2212W 32W 21x\u00b5x\u00b5T \u0001\n,\n\u2206W 32 = \u03bb\nP\nX\n\u00b5=1\n\u0000y\u00b5x\u00b5T \u2212W 32W 21x\u00b5x\u00b5T \u0001\nW 21T ,\n(1)\nwhere \u03bb is a small learning rate. As long as \u03bb is suf\ufb01ciently small, we can take a continuous time limit to\nobtain the dynamics,\n\u03c4 d\ndtW 21 = W 32T \u0000\u03a331 \u2212W 32W 21\u03a311\u0001\n,\n\u03c4 d\ndtW 32 =\n\u0000\u03a331 \u2212W 32W 21\u03a311\u0001\nW 21T ,\n(2)\nwhere \u03a311 \u2261PP\n\u00b5=1 x\u00b5x\u00b5T is an N1 \u00d7 N1 input correlation matrix, \u03a331 \u2261PP\n\u00b5=1 y\u00b5x\u00b5T is an N3 \u00d7 N1\ninput-output correlation matrix, and \u03c4 \u22611\n\u03bb. Here t measures time in units of iterations; as t varies from 0\nto 1, the network has seen P examples corresponding to one iteration. Despite the linearity of the network\u2019s\ninput-output map, the gradient descent learning dynamics given in Eqn (2) constitutes a complex set of\ncoupled nonlinear differential equations with up to cubic interactions in the weights.\n1.1\nLearning dynamics with orthogonal inputs\nOur fundamental goal is to understand the dynamics of learning in (2) as a function of the input statistics\n\u03a311 and input-output statistics \u03a331. In general, the outcome of learning will re\ufb02ect an interplay between\ninput correlations, described by \u03a311, and the input-output correlations described by \u03a331. To begin, though,\nwe further simplify the analysis by focusing on the case of orthogonal input representations where \u03a311 = I.\nThis assumption will hold exactly for whitened input data, a widely used preprocessing step.\nBecause we have assumed orthogonal input representations (\u03a311 = I), the input-output correlation matrix\ncontains all of the information about the dataset used in learning, and it plays a pivotal role in the learning\ndynamics. We consider its singular value decomposition (SVD)\n\u03a331 = U 33S31V 11T = PN1\n\u03b1=1 s\u03b1u\u03b1vT\n\u03b1,\n(3)\nwhich will be central in our analysis. Here V 11 is an N1 \u00d7 N1 orthogonal matrix whose columns contain\ninput-analyzing singular vectors v\u03b1 that re\ufb02ect independent modes of variation in the input, U 33 is an N3 \u00d7\nN3 orthogonal matrix whose columns contain output-analyzing singular vectors u\u03b1 that re\ufb02ect independent\nmodes of variation in the output, and S31 is an N3 \u00d7 N1 matrix whose only nonzero elements are on the\ndiagonal; these elements are the singular values s\u03b1, \u03b1 = 1, . . . , N1 ordered so that s1 \u2265s2 \u2265\u00b7 \u00b7 \u00b7 \u2265sN1.\nNow, performing the change of variables on synaptic weight space, W 21 = W\n21V 11T , W 32 = U 33W\n32,\nthe dynamics in (2) simplify to\n\u03c4 d\ndtW\n21 = W\n32T\n(S31 \u2212W\n32W\n21),\n\u03c4 d\ndtW\n32 = (S31 \u2212W\n32W\n21)W\n21T\n.\n(4)\nTo gain intuition for these equations, note that while the matrix elements of W 21 and W 32 connected neurons\nin one layer to neurons in the next layer, we can think of the matrix element W\n21\ni\u03b1 as connecting input mode\nv\u03b1 to hidden neuron i, and the matrix element W\n32\n\u03b1i as connecting hidden neuron i to output mode u\u03b1. Let\na\u03b1 be the \u03b1th column of W\n21, and let b\u03b1T be the \u03b1th row of W\n32. Intuitively, a\u03b1 is a column vector of N2\nsynaptic weights presynaptic to the hidden layer coming from input mode \u03b1, and b\u03b1 is a column vector of\n3\nN2 synaptic weights postsynaptic to the hidden layer going to output mode \u03b1. In terms of these variables,\nor connectivity modes, the learning dynamics in (4) become\n\u03c4 d\ndta\u03b1 = (s\u03b1 \u2212a\u03b1 \u00b7 b\u03b1) b\u03b1 \u2212\nX\n\u03b3\u0338=\u03b1\nb\u03b3 (a\u03b1 \u00b7 b\u03b3),\n\u03c4 d\ndtb\u03b1 = (s\u03b1 \u2212a\u03b1 \u00b7 b\u03b1) a\u03b1 \u2212\nX\n\u03b3\u0338=\u03b1\na\u03b3 (b\u03b1 \u00b7 a\u03b3). (5)\nNote that s\u03b1 = 0 for \u03b1 > N1. These dynamics arise from gradient descent on the energy function\nE = 1\n2\u03c4\nX\n\u03b1\n(s\u03b1 \u2212a\u03b1 \u00b7 b\u03b1)2 + 1\n2\u03c4\nX\n\u03b1\u0338=\u03b2\n(a\u03b1 \u00b7 b\u03b2)2,\n(6)\nand display an interesting combination of cooperative and competitive interactions. Consider the \ufb01rst terms\nin each equation. In these terms, the connectivity modes from the two layers, a\u03b1 and b\u03b1 associated with the\nsame input-output mode of strength s\u03b1, cooperate with each other to drive each other to larger magnitudes\nas well as point in similar directions in the space of hidden units; in this fashion these terms drive the\nproduct of connectivity modes a\u03b1 \u00b7 b\u03b1 to re\ufb02ect the input-output mode strength s\u03b1. The second terms\ndescribe competition between the connectivity modes in the \ufb01rst (a\u03b1) and second (b\u03b2) layers associated with\ndifferent input modes \u03b1 and \u03b2. This yields a symmetric, pairwise repulsive force between all distinct pairs of\n\ufb01rst and second layer connectivity modes, driving the network to a decoupled regime in which the different\nconnectivity modes become orthogonal.\n1.2\nThe \ufb01nal outcome of learning\nThe \ufb01xed point structure of gradient descent learning in linear networks was worked out in [19]. In the\nlanguage of the connectivity modes, a necessary condition for a \ufb01xed point is a\u03b1 \u00b7b\u03b2 = s\u03b1\u03b4\u03b1\u03b2, while a\u03b1 and\nb\u03b1 are zero whenever s\u03b1 = 0. To satisfy these relations for undercomplete hidden layers (N2 < N1, N2 <\nN3), a\u03b1 and b\u03b1 can be nonzero for at most N2 values of \u03b1. Since there are rank(\u03a331) \u2261r nonzero values\nof s\u03b1, there are\n\u0012\nr\nN2\n\u0013\nfamilies of \ufb01xed points. However, all of these \ufb01xed points are unstable, except for\nthe one in which only the \ufb01rst N2 strongest modes, i.e. a\u03b1 and b\u03b1 for \u03b1 = 1, . . . , N2 are active. Thus\nremarkably, the dynamics in (5) has only saddle points and no non-global local minima [19]. In terms of the\noriginal synaptic variables W 21 and W 32, all globally stable \ufb01xed points satisfy\nW 32W 21 = PN2\n\u03b1=1 s\u03b1u\u03b1vT\n\u03b1.\n(7)\n\u22122\n\u22121\n0\n1\n2\n\u22122\n\u22121.5\n\u22121\n\u22120.5\n0\n0.5\n1\n1.5\n2\na\nb\nFigure 2: Vector \ufb01eld (blue), stable\nmanifold (red) and two solution tra-\njectories (green) for the two dimen-\nsional dynamics of a and b in (8),\nwith \u03c4 = 1, s = 1.\nHence when learning has converged, the network will represent the\nclosest rank N2 approximation to the true input-output correlation\nmatrix. In this work, we are interested in understanding the dynami-\ncal weight trajectories and learning time scales that lead to this \ufb01nal\n\ufb01xed point.\n1.3\nThe time course of learning\nIt is dif\ufb01cult though to exactly solve (5) starting from arbitrary initial\nconditions because of the competitive interactions between different\ninput-output modes. Therefore, to gain intuition for the general dy-\nnamics, we restrict our attention to a special class of initial conditions\nof the form a\u03b1 and b\u03b1 \u221dr\u03b1 for \u03b1 = 1, . . . , N2, where r\u03b1 \u00b7 r\u03b2 = \u03b4\u03b1\u03b2,\nwith all other connectivity modes a\u03b1 and b\u03b1 set to zero (see [20] for\n4\n0\n500\n1000\n0\n20\n40\n60\n80\nt (Epochs)\nmode strength\n \n \n0\n5\n10\n15\n20\n25\n30\n\u22120.1\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nInput\u2212output mode\n(thalf\u2212tanaly)/tanaly\n \n \nLinear\nTanh\nFigure 3:\nLeft: Dynamics of learning in a three layer neural network. Curves show the strength of the\nnetwork\u2019s representation of seven modes of the input-output correlation matrix over the course of learning.\nRed traces show analytical curves from Eqn. 12. Blue traces show simulation of full dynamics of a linear\nnetwork (Eqn. (2)) from small random initial conditions. Green traces show simulation of a nonlinear\nthree layer network with tanh activation functions. To generate mode strengths for the nonlinear network,\nwe computed the nonlinear network\u2019s evolving input-output correlation matrix, and plotted the diagonal\nelements of U 33T \u03a331tanhV 11 over time. The training set consists of 32 orthogonal input patterns, each\nassociated with a 1000-dimensional feature vector generated by a hierarchical diffusion process described\nin [16] with a \ufb01ve level binary tree and \ufb02ip probability of 0.1. Modes 1, 2, 3, 5, 12, 18, and 31 are plotted\nwith the rest excluded for clarity. Network training parameters were \u03bb = 0.5e\u22123, N2 = 32, u0 = 1e\u22126.\nRight: Delay in learning due to competitive dynamics and sigmoidal nonlinearities. Vertical axis shows the\ndifference between simulated time of half learning and the analytical time of half learning, as a fraction of\nthe analytical time of half learning. Error bars show standard deviation from 100 simulations with random\ninitializations.\nsolutions to a partially overlapping but distinct set of initial condi-\ntions, further discussed in Supplementary Appendix A). Here r\u03b1 is a \ufb01xed collection of N2 vectors that\nform an orthonormal basis for synaptic connections from an input or output mode onto the set of hidden\nunits. Thus for this set of initial conditions, a\u03b1 and b\u03b1 point in the same direction for each alpha and differ\nonly in their scalar magnitudes, and are orthogonal to all other connectivity modes. Such an initialization\ncan be obtained by computing the SVD of \u03a331 and taking W 32 = U 33DaRT , W 21 = RDbV 11T where\nDa, Db are diagonal, and R is an arbitrary orthogonal matrix; however, as we show\nin subsequent experiments, the solutions we \ufb01nd are also excellent approximations to trajectories from small\nrandom initial conditions. It is straightforward to verify that starting from these initial conditions, a\u03b1 and b\u03b1\nwill remain parallel to r\u03b1 for all future time. Furthermore, because the different active modes are orthogonal\nto each other, they do not compete, or even interact with each other (all dot products in the second terms of\n(5)-(6) are 0).\nThus this class of conditions de\ufb01nes an invariant manifold in weight space where the modes evolve indepen-\ndently of each other.\nIf we let a = a\u03b1 \u00b7 r\u03b1, b = b\u03b1 \u00b7 r\u03b1, and s = s\u03b1, then the dynamics of the scalar projections (a, b) obeys,\n\u03c4 d\ndta = b (s \u2212ab),\n\u03c4 d\ndtb = a (s \u2212ab).\n(8)\nThus our ability to decouple the connectivity modes yields a dramatically simpli\ufb01ed two dimensional non-\nlinear system. These equations can by solved by noting that they arise from gradient descent on the error,\nE(a, b) =\n1\n2\u03c4 (s \u2212ab)2.\n(9)\nThis implies that the product ab monotonically approaches the \ufb01xed point s from its initial value. Moreover,\nE(a, b) satis\ufb01es a symmetry under the one parameter family of scaling transformations a \u2192\u03bba, b \u2192\nb\n\u03bb.\nThis symmetry implies, through Noether\u2019s theorem, the existence of a conserved quantity, namely a2 \u2212b2,\n5\nwhich is a constant of motion. Thus the dynamics simply follows hyperbolas of constant a2 \u2212b2 in the (a, b)\nplane until it approaches the hyperbolic manifold of \ufb01xed points, ab = s. The origin a = 0, b = 0 is also a\n\ufb01xed point, but is unstable. Fig. 2 shows a typical phase portrait for these dynamics.\nAs a measure of the timescale of learning, we are interested in how long it takes for ab to approach s from\nany given initial condition. The case of unequal a and b is treated in the Supplementary Appendix A due to\nspace constraints. Here we pursue an explicit solution with the assumption that a = b, a reasonable limit\nwhen starting with small random initial conditions. We can then track the dynamics of u \u2261ab, which from\n(8) obeys\n\u03c4 d\ndtu = 2u(s \u2212u).\n(10)\nThis equation is separable and can be integrated to yield\nt = \u03c4\nZ uf\nu0\ndu\n2u(s \u2212u) = \u03c4\n2s ln uf(s \u2212u0)\nu0(s \u2212uf).\n(11)\nHere t is the time it takes for u to travel from u0 to uf. If we assume a small initial condition u0 = \u03f5, and\nask when uf is within \u03f5 of the \ufb01xed point s, i.e. uf = s \u2212\u03f5, then the learning timescale in the limit \u03f5 \u21920\nis t = \u03c4/s ln (s/\u03f5) = O(\u03c4/s) (with a weak logarithmic dependence on the cutoff). This yields a key result:\nthe timescale of learning of each input-output mode \u03b1 of the correlation matrix \u03a331 is inversely proportional\nto the correlation strength s\u03b1 of the mode. Thus the stronger an input-output relationship, the quicker it is\nlearned.\nWe can also \ufb01nd the entire time course of learning by inverting (11) to obtain\nuf(t) =\nse2st/\u03c4\ne2st/\u03c4 \u22121 + s/u0\n.\n(12)\nThis time course describes the temporal evolution of the product of the magnitudes of all weights from an\ninput mode (with correlation strength s) into the hidden layers, and from the hidden layers to the same output\nmode. If this product starts at a small value u0 < s, then it displays a sigmoidal rise which asymptotes\nto s as t \u2192\u221e. This sigmoid can exhibit sharp transitions from a state of no learning to full learning.\nThis analytical sigmoid learning curve is shown in Fig. 3 to yield a reasonable approximation to learning\ncurves in linear networks that start from random initial conditions that are not on the orthogonal, decoupled\ninvariant manifold\u2013and that therefore exhibit competitive dynamics between connectivity modes\u2013as well as\nin nonlinear networks solving the same task. We note that though the nonlinear networks behaved similarly\nto the linear case for this particular task, this is likely to be problem dependent.\n2\nDeeper multilayer dynamics\nThe network analyzed in Section 1 is the minimal example of a multilayer net, with just a single layer of\nhidden units. How does gradient descent act in much deeper networks? We make an initial attempt in this\ndirection based on initial conditions that yield particularly simple gradient descent dynamics.\nIn a linear neural network with Nl layers and hence Nl\u22121 weight matrices indexed by W l, l = 1, \u00b7 \u00b7 \u00b7 , Nl\u22121,\nthe gradient descent dynamics can be written as\n\u03c4 d\ndtW l =\n Nl\u22121\nY\ni=l+1\nW i\n!T \"\n\u03a331 \u2212\n Nl\u22121\nY\ni=1\nW i\n!\n\u03a311\n#  l\u22121\nY\ni=1\nW i\n!T\n,\n(13)\nwhere Qb\ni=a W i = W bW (b\u22121) \u00b7 \u00b7 \u00b7 W (a\u22121)W a with the special case that Qb\ni=a W i = I, the identity, if\na > b.\n6\nTo describe the initial conditions, we suppose that there are Nl orthogonal matrices Rl that diagonalize\nthe starting weight matrices, that is, RT\nl+1Wl(0)Rl = Dl for all l, with the special case that R1 = V 11\nand RNl = U 33. This requirement essentially demands that the output singular vectors of layer l be the\ninput singular vectors of the next layer l + 1, so that a change in mode strength at any layer propagates to\nthe output without mixing into other modes. We note that this formulation does not restrict hidden layer\nsize; each hidden layer can be of a different size, and may be undercomplete or overcomplete. Making the\nchange of variables Wl = Rl+1W lRT\nl along with the assumption that \u03a311 = I leads to a set of decoupled\nconnectivity modes that evolve independently of each other. In analogy to the simpli\ufb01cation occurring in the\nthree layer network from (2) to (8), each connectivity mode in the Nl layered network can be described by\nNl \u22121 scalars a1, . . . , aNl\u22121, whose dynamics obeys gradient descent on the energy function (the analog of\n(9)),\nE(a1, \u00b7 \u00b7 \u00b7 , aNl\u22121) = 1\n2\u03c4\n \ns \u2212\nNl\u22121\nY\ni=1\nai\n!2\n.\n(14)\nThis dynamics also has a set of conserved quantities a2\ni \u2212a2\nj arising from the energetic symmetry w.r.t. the\ntransformation ai \u2192\u03bbai, aj \u2192aj\n\u03bb , and hence can be solved exactly. We focus on the invariant submanifold\nin which ai(t = 0) = a0 for all i, and track the dynamics of u = QNl\u22121\ni=1\nai, the overall strength of this\nmode, which obeys (i.e. the generalization of (10)),\n\u03c4 d\ndtu = (Nl \u22121)u2\u22122/(Nl\u22121)(s \u2212u).\n(15)\nThis can be integrated for any positive integer Nl, though the expression is complicated. Once the overall\nstrength increases suf\ufb01ciently, learning explodes rapidly.\nEqn. (15) lets us study the dynamics of learning as depth limits to in\ufb01nity. In particular, as Nl \u2192\u221ewe\nhave the dynamics\n\u03c4 d\ndtu = Nlu2(s \u2212u)\n(16)\nwhich can be integrated to obtain\nt = \u03c4\nNl\n\u0014 1\ns2 log\n\u0012uf(u0 \u2212s)\nu0(uf \u2212s)\n\u0013\n+\n1\nsu0\n\u2212\n1\nsuf\n\u0015\n.\n(17)\nRemarkably this implies that, for a \ufb01xed learning rate, the learning time as measured by the num-\nber of iterations required tends to zero as Nl goes to in\ufb01nity.\nThis result depends on the con-\ntinuous time formulation, however.\nAny implementation will operate in discrete time and must\nchoose a \ufb01nite learning rate that yields stable dynamics.\nAn estimate of the optimal learn-\ning rate can be derived from the maximum eigenvalue of the Hessian over the region of interest.\n0\n50\n100\n0\n50\n100\n150\n200\n250\nNl (Number of layers)\nLearning time (Epochs)\n0\n50\n100\n0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\nx 10\n\u22124\nOptimal learning rate\nNl (Number of layers)\nFigure 4:\nLeft: Learning time as a function of depth on MNIST. Right:\nEmpirically optimal learning rates as a function of depth.\nFor linear networks with ai =\naj = a, this optimal learn-\ning rate \u03b1opt decays with\ndepth as O\n\u0010\n1\nNls2\n\u0011\nfor large\nNl (see Supplementary Ap-\npendix B). Incorporating this\ndependence of the learning\nrate on depth, the learning\ntime as depth approaches in-\n\ufb01nity still surprisingly re-\nmains \ufb01nite:\nwith the opti-\nmal learning rate, the differ-\nence between learning times\n7\nfor an Nl = 3 network and an Nl = \u221enetwork is t\u221e\u2212t3 \u223cO (s/\u03f5) for small \u03f5 (see Supplementary\nAppendix B.1). We emphasize that our analysis of learning speed is based on the number of iterations re-\nquired, not the amount of computation\u2013computing one iteration of a deep network will require more time\nthan doing so in a shallow network.\nTo verify these predictions, we trained deep linear networks on the MNIST classi\ufb01cation task with depths\nranging from Nl = 3 to Nl = 100. We used hidden layers of size 1000, and calculated the iteration at\nwhich training error fell below a \ufb01xed threshold corresponding to nearly complete learning. We optimized\nthe learning rate separately for each depth by training each network with twenty rates logarithmically spaced\nbetween 10\u22124 and 10\u22127 and picking the fastest. See Supplementary Appendix C for full experimental\ndetails. Networks were initialized with decoupled initial conditions and starting initial mode strength u0 =\n0.001. Fig. 4 shows the resulting learning times, which saturate, and the empirically optimal learning rates,\nwhich scale like O(1/Nl) as predicted.\nThus learning times in deep linear networks that start with decoupled initial conditions are only a \ufb01nite\namount slower than a shallow network regardless of depth. Moreover, the delay incurred by depth scales\ninversely with the size of the initial strength of the association. Hence \ufb01nding a way to initialize the mode\nstrengths to large values is crucial for fast deep learning.\n3\nFinding good weight initializations: on greediness and randomness\nThe previous subsection revealed the existence of a decoupled submanifold in weight space in which con-\nnectivity modes evolve independently of each other during learning, and learning times can be independent\nof depth, even for arbitrarily deep networks, as long as the initial composite, end to end mode strength,\ndenoted by u above, of every connectivity mode is O(1). What numerical weight initilization procedures\ncan get us close to this weight manifold, so that we can exploit its rapid learning properties?\nA breakthrough in training deep neural networks started with the discovery that greedy layer-wise unsu-\npervised pretraining could substantially speed up and improve the generalization performance of standard\ngradient descent [17, 18]. Unsupervised pretraining has been shown to speed the optimization of deep\nnetworks, and also to act as a special regularizer towards solutions with better generalization performance\n[18, 12, 13, 14]. At the same time, recent results have obtained excellent performance starting from carefully-\nscaled random initializations, though interestingly, pretrained initializations still exhibit faster convergence\n[21, 13, 22, 3, 4, 1, 23] (see Supplementary Appendix D for discussion). Here we examine analytically how\nunsupervised pretraining achieves an optimization advantage, at least in deep linear networks, by \ufb01nding\nthe special class of orthogonalized, decoupled initial conditions in the previous section that allow for rapid\nsupervised deep learning, for input-output tasks with a certain precise structure. Subsequently, we analyze\nthe properties of random initilizations.\nWe consider the following pretraining and \ufb01netuning procedure: First, using autoencoders as the unsuper-\nvised pretraining module [18, 12], the network is trained to produce its input as its output (y\u00b5\npre = x\u00b5).\nSubsequently, the network is \ufb01netuned on the ultimate input-output task of interest (e.g., a classi\ufb01cation\ntask). In the following we consider the case N2 = N1 for simplicity.\nDuring the pretraining phase, the input-output correlation matrix \u03a331pre is simply the input correlation matrix\n\u03a311. Hence the SVD of \u03a331pre is PCA on the input correlation matrix, since \u03a331pre = \u03a311 = Q\u039bQT ,\nwhere Q are eigenvectors of \u03a311 and \u039b is a diagonal matrix of variances. Our analysis of the learning\ndynamics in Section 1.1 does not directly apply, because here the input correlation matrix is not white. In\nSupplementary Appendix E we generalize our results to handle this case. During pretraining, the weights\napproach W 32W 21 = \u03a331(\u03a331)\u22121, but since they do not reach the \ufb01xed point in \ufb01nite time, they will\nend at W 32W 21 = QMQT where M is a diagonal matrix that is approaching the identity matrix during\n8\n0\n100\n200\n300\n400\n500\n1\n1.5\n2\n2.5\n3\nx 10\n4\nEpoch\nError\n \n \nPretrain\nRandom\n \n \n2000\n4000\n6000\n8000\n \n \n0\n5\n10\n15\nx 10\n5\nFigure 5: MNIST satis\ufb01es the consistency condition for greedy pretraining. Left: Submatrix from the raw\nMNIST input correlation matrix \u03a311. Center: Submatrix of V 11\u03a311V 11T which is approximately diagonal\nas required. Right: Learning curves on MNIST for a \ufb01ve layer linear network starting from random (black)\nand pretrained (red) initial conditions. Pretrained curve starts with a delay due to pretraining time. The small\nrandom initial conditions correspond to all weights chosen i.i.d. from a zero mean Gaussian with standard\ndeviation 0.01.\nlearning. Hence in general, W 32 = QM 1/2C\u22121 and W 21 = CM 1/2QT where C is any invertible matrix.\nWhen starting from small random weights, though, each weight matrix will end up with a roughly balanced\ncontribution to the overall map. This corresponds to having C \u2248R2 where R2 is orthogonal. Hence at\nthe end of the pretraining phase, the input-to-hidden mapping will be W 21 = R2M 1/2QT where R2 is an\narbitrary orthogonal matrix.\nNow consider the \ufb01ne-tuning phase. Here the weights are trained on the ultimate task of interest with\ninput-output correlations \u03a331 = U 33S31V 11. The matrix W 21 begins from the pretrained initial condition\nW 21 = R2M 1/2QT . For the \ufb01ne-tuning task, a decoupled initial condition for W 21 is one that can be\nwritten as W 21 = R2D1V 11T (see Section 2). Clearly, this will be possible only if\nQ = V 11.\n(18)\nThen the initial condition obtained from pretraining will also be a decoupled initial condition for the \ufb01netun-\ning phase, with initial mode strengths D1 = M 1/2 near one. Hence we can state the underlying condition\nrequired for successful greedy pretraining in deep linear networks: the right singular vectors of the ultimate\ninput-ouput task of interest V 11 must be similar to the principal components of the input data Q. This is a\nquantitatively precise instantiation of the intuitive idea that unsupervised pretraining can help in a subsequent\nsupervised learning task if (and only if) the statistical structure of the input is consistent with the structure\nof input-output map to be learned. Moreover, this quantitative instantiation of this intuitive idea gives a\nsimple empirical criterion that can be evaluated on any new dataset: given the input-output correlation \u03a331\nand input correlation \u03a311, compute the right singular vectors V 11 of \u03a331 and check that V 11\u03a311V 11T is\napproximately diagonal. If the condition in Eqn. (18) holds, autoencoder pretraining will have properly set\nup decoupled initial conditions for W 21, with an appreciable initial association strength near 1. This argu-\nment also goes through straightforwardly for layer-wise pretraining of deeper networks. Fig. 5 shows that\nthis consistency condition empirically holds on MNIST, and that a pretrained deep linear neural network\nlearns faster than one started from small random initial conditions, even accounting for pretraining time (see\nSupplementary Appendix F for experimental details). We note that this analysis is unlikely to carry over\ncompletely to nonlinear networks. Some nonlinear networks are approximately linear (e.g., tanh nonlin-\nearities) after initialization with small random initializations, and hence our solutions may describe these\ndynamics well early in learning. However as the network enters its nonlinear regime, our solutions should\nnot be expected to remain accurate.\nAs an alternative to greedy layerwise pre-training, [13] proposed choosing appropriately scaled initial condi-\ntions on weights that would preserve the norm of typical error vectors as they were backpropagated through\nthe deep network. In our context, the appropriate norm-preserving scaling for the initial condition of an\nN by N connectivity matrix W between any two layers corresponds to choosing each weight i.i.d. from a\n9\nFigure 6: A Left: Learning time (on MNIST using the same architecture and parameters as in Fig. 4) as a\nfunction of depth for different initial conditions on weights (scaled i.i.d. uniform weights chosen to preserve\nthe norm of propagated gradients as proposed in [13] (blue), greedy unsupervised pre-training (green) and\nrandom orthogonal matrices (red). The red curve lies on top of the green curve. Middle: Optimal learning\nrates as a function of depth for different weight initilizations. Right: The eigenvalue spectrum, in the\ncomplex plane, of a random 100 by 100 orthogonal matrix. B Histograms of the singular values of products\nof Nl \u22121 independent random Gaussian N by N matrices whose elements themselves are chosen i.i.d. from\na zero mean Gaussian with standard deviation 1/\n\u221a\nN. In all cases, N = 1000, and histograms are taken over\n500 realizations of such random product matrices, yielding a total 5 \u00b7 105 singular values in each histogram.\nC Histograms of the eigenvalue distributions on the complex plane of the same product matrices in B. The\nbin width is 0.1, and, for visualization purposes, the bin containing the origin has been removed in each case;\nthis bin would otherwise dominate the histogram in the middle and right plots, as it contains 32% and 94%\nof the eigenvalues respectively.\nzero mean Gaussian with standard deviation 1/\n\u221a\nN. With this choice, \u27e8vT W T Wv\u27e9W = vT v, where \u27e8\u00b7\u27e9W\ndenotes an average over distribution of the random matrix W. Moreover, the distribution of vT W T Wv con-\ncentrates about its mean for large N. Thus with this scaling, in linear networks, both the forward propagation\nof activity, and backpropagation of gradients is typically norm-preserving. However, with this initialization,\nthe learning time with depth on linear networks trained on MNIST grows with depth (Fig. 6A, left, blue).\nThis growth is in distinct contradiction with the theoretical prediction, made above, of depth independent\nlearning times starting from the decoupled submanifold of weights with composite mode strength O(1).\nThis suggests that the scaled random initialization scheme, despite its norm-preserving nature, does not \ufb01nd\nthis submanifold in weight space. In contrast, learning times with greedy layerwise pre-training do not grow\nwith depth (Fig. 6A, left, green curve hiding under red curve), consistent with the predictions of our theory\n(as a technical point: note that learning times under greedy pre-training initialization in Fig. 6A are faster\nthan those obtained in Fig. 4 by explicitly choosing a point on the decoupled submanifold, because there\nthe initial mode strength was chosen to be small (u = 0.001) whereas greedy pre-training \ufb01nds a composite\nmode strength closer to 1).\n10\nIs there a simple random initialization scheme that does enjoy the rapid learning properties of greedy-\nlayerwise pre-training? We empirically show (Fig. 6A, left, red curve) that if we choose the initial weights in\neach layer to be a random orthogonal matrix (satisifying W T W = I), instead of a scaled random Gaussian\nmatrix, then this orthogonal random initialization condition yields depth independent learning times just like\ngreedy layerwise pre-training (indeed the red and green curves are indistinguishable). Theoretically, why do\nrandom orthogonal initializations yield depth independent learning times, but not scaled random Gaussian\ninitializations, despite their norm preserving nature?\nThe answer lies in the eigenvalue and singular value spectra of products of Gaussian versus orthgonal random\nmatrices. While a single random orthogonal matrix has eigenvalue spectra lying exactly on the unit circle\nin the complex plane (Fig. 6A right), the eigenvalue spectra of random Gaussian matrices, whose elements\nhave variance 1/N, form a uniform distribution on a solid disk of radius 1 the complex plane (Fig. 6C left).\nMoreover the singular values of an orthogonal matrix are all exactly 1, while the squared singular values\nof a scaled Gaussian random matrix have the well known Marcenko-Pasteur distribution, with a nontrivial\nspread even as N \u2192\u221e, (Fig. 6B left shows the distribution of singular values themselves). Now consider a\nproduct of these matrices across all Nl layers, representing the total end to end propagation of activity across\na deep linear network:\nWTot =\nNl\u22121\nY\ni=1\nW (i+1,i).\n(19)\nDue to the random choice of weights in each layer, WTot is itself a random matrix. On average, it preserves\nthe norm of a typical vector v no matter whether the matrices in each layer are Gaussian or orthogonal.\nHowever, the singular value spectra of WTot differ markedly in the two cases. Under random orthogonal\ninitilization in each layer, WTot is itself an orthogonal matrix and therefore has all singular values equal to\n1. However, under random Gaussian initialization in each layer, there is as of yet no complete theoretical\ncharacterization of the singular value distribution of WTot. We have computed it numerically as a function\nof different depths in Fig. 6B, and we \ufb01nd that it develops a highly kurtotic nature as the depth increases.\nMost of the singular values become vanishingly small, while a long tail of very large singular values remain.\nThus WTot preserves the norm of a typical, randomly chosen vector v, but in a highly anisotropic manner,\nby strongly amplifying the projection of v onto a very small subset of singular vectors and attenuating v\nin all other directions. Intuitively WTot, as well as the linear operator W T\nTot that would be closely related\nto backpropagation of gradients to early layers, act as amplifying projection operators at large depth Nl.\nIn contrast, all of the eigenvalues of WTot in the scaled Gaussian case concentrate closer to the origin as\ndepth increases. This discrepancy between the behavior of the eigenvalues and singular values of WTot, a\nphenomenon that could occur only if the eigenvectors of WTot are highly non-orthogonal, re\ufb02ects the highly\nnon-normal nature of products of random Gaussian matrices (a non-normal matrix is by de\ufb01nition a matrix\nwhose eigenvectors are non-orthogonal).\nWhile the combination of ampli\ufb01cation and projection in WTot can preserve norm, it is clear that it is not a\ngood way to backpropagate errors; the projection of error vectors onto a high dimensional subspace corre-\nsponding to small singular values would be strongly attenuated, yielding vanishingly small gradient signals\ncorresponding to these directions in the early layers. This effect, which is not present for random orthogonal\ninitializations or greedy pretraining, would naturally explain the long learning times starting from scaled\nrandom Gaussian initial conditions relative to the other initilizations in Fig. 6A left. For both linear and\nnonlinear networks, a more likely appropriate condition on weights for generating fast learning times would\nbe that of dynamical isometry. By this we mean that the product of Jacobians associated with error signal\nbackpropagation should act as a near isometry, up to some overall global O(1) scaling, on a subspace of\nas high a dimension as possible. This is equivalent to having as many singular values of the product of\nJacobians as possible within a small range around an O(1) constant, and is closely related to the notion of\nrestricted isometry in compressed sensing and random projections. Preserving norms is a necessary but not\nsuf\ufb01cient condition for achieving dynamical isometry at large depths, as demonstrated in Fig. 6B, and we\n11\nhave shown that for linear networks, orthogonal initializations achieve exact dynamical isometry with all\nsingular values at 1, while greedy pre-training achieves it approximately.\nWe note that the discrepancy in learning times between the scaled Gaussian initialization and the orthogonal\nor pre-training initializations is modest for the depths of around 6 used in large scale applications, but is\nmagni\ufb01ed at larger depths (Fig. 6A left). This may explain the modest improvement in learning times with\ngreedy pre-training versus random scaled Gaussian initializations observed in applications (see discussion in\nSupplementary Appendix D). We predict that this modest improvement will be magni\ufb01ed at higher depths,\neven in nonlinear networks. Finally, we note that in recurrent networks, which can be thought of as in\ufb01nitely\ndeep feed-forward networks with tied weights, a very promising approach is a modi\ufb01cation to the training\nobjective that partially promotes dynamical isometry for the set of gradients currently being back-propagated\n[24].\n4\nAchieving approximate dynamical isometry in nonlinear networks\nWe have shown above that deep random orthogonal linear networks achieve perfect dynamical isometry.\nHere we show that nonlinear versions of these networks can also achieve good dynamical isometry proper-\nties. Consider the nonlinear feedforward dynamics\nxl+1\ni\n=\nX\nj\ng W (l+1,l)\nij\n\u03c6(xl\nj),\n(20)\nwhere xl\ni denotes the activity of neuron i in layer l, W (l+1,l)\nij\nis a random orthogonal connectivity matrix from\nlayer l to l + 1, g is a scalar gain factor, and \u03c6(x) is any nonlinearity that saturates as x \u2192\u00b1\u221e. We show\nin Supplementary appendix G that there exists a critical value gc of the gain g such that if g < gc, activity\nwill decay away to zero as it propagates through the layers, while if g > gc, the strong linear positive gain\nwill combat the damping due to the saturating nonlinearity, and activity will propagate inde\ufb01nitely without\ndecay, no matter how deep the network is. When the nonlinearity is odd (\u03c6(x) = \u2212\u03c6(\u2212x)), so that the mean\nactivity in each layer is approximately 0, these dynamical properties can be quantitatively captured by the\nneural population variance in layer l,\nql \u22611\nN\nN\nX\ni=1\n(xl\ni)2.\n(21)\nThus liml\u2192\u221eql \u21920 for g < gc and liml\u2192\u221eql \u2192q\u221e(g) > 0 for g > gc. When \u03c6(x) = tanh(x), we\ncompute gc = 1 and numerically compute q\u221e(g) in Fig. 8 in Supplementary appendix G. Thus these non-\nlinear feedforward networks exhibit a phase-transition at the critical gain; above the critical gain, in\ufb01nitely\ndeep networks exhibit chaotic percolating activity propagation, so we call the critical gain gc the edge of\nchaos, in analogy with terminology for recurrent networks.\nNow we are interested in how errors at the \ufb01nal layer Nl backpropagate back to earlier layers, and whether\nor not these gradients explode or decay with depth. To quantify this, for simplicity we consider the end to\nend Jacobian\nJNl,1\nij\n(xNl) \u2261\u2202xNl\ni\n\u2202x1\nj\n\f\f\f\f\nxNl\n,\n(22)\nwhich captures how input perturbations propagate to the output. If the singular value distribution of this\nJacobian is well-behaved, with few extremely large or small singular values, then the backpropagation of\ngradients will also be well-behaved, and exhibit little explosion or decay. The Jacobian is evaluated at a\nparticular point xNl in the space of output layer activations, and this point is in turn obtained by iterating\n(20) starting from an initial input layer activation vector x1. Thus the singular value distribution of the\n12\n0\n1\n2\n3\nx 10\n\u22125\n0\n50\n100\nq = 0.2\ng = 0.9\n0\n2\n4\n6\nx 10\n\u22123\n0\n20\n40\n60\ng = 0.95\n0\n0.1\n0.2\n0.3\n0.4\n0\n10\n20\n30\n40\ng = 1\n0\n0.5\n1\n1.5\n2\n0\n50\n100\ng = 1.05\n0\n2\n4\n6\n0\n100\n200\n300\n400\ng = 1.1\n0\n1\n2\n3\nx 10\n\u22125\n0\n10\n20\n30\n40\nq = 1\n0\n1\n2\n3\n4\nx 10\n\u22123\n0\n10\n20\n30\n40\n0\n0.1\n0.2\n0.3\n0.4\n0\n10\n20\n30\n40\n0\n0.5\n1\n1.5\n0\n50\n100\n0\n1\n2\n3\n4\n0\n100\n200\n300\n400\n0\n1\n2\n3\nx 10\n\u22125\n0\n10\n20\n30\n40\nq = 4\n0\n1\n2\n3\n4\nx 10\n\u22123\n0\n10\n20\n30\n0\n0.1\n0.2\n0.3\n0.4\n0\n10\n20\n30\n40\n0\n0.5\n1\n1.5\n0\n50\n100\n150\n0\n1\n2\n3\n0\n200\n400\n600\nFigure 7: Singular value distribution of the end to end Jacobian, de\ufb01ned in (22), for various values of the\ngain g in (20) and the input layer population variance q = q1 in (21). The network architecture consists of\nNl = 100 layers with N = 1000 neurons per layer, as in the linear case in Fig. 6B.\nJacobian will depend not only on the gain g, but also on the initial condition x1. By rotational symmetry,\nwe expect this distribution to depend on x1, only through its population variance q1. Thus for large N, the\nsingular value distribution of the end-to-end Jacobian in (22) (the analog of WTot in (19) in the linear case),\ndepends on only two parameters: gain g and input population variance q1.\nWe have numerically computed this singular value distribution as a function of these two parameters in Fig.\n7, for a single random orthogonal nonlinear network with N = 1000 and Nl = 100. These results are\ntypical; replotting the results for different random networks and different initial conditions (with the same\ninput variance) yield very similar results. We see that below the edge of chaos, when g < 1, the linear\ndampening over many layers yields extremely small singular values. Above the edge of chaos, when g > 1,\nthe combination of positive linear ampli\ufb01cation, and saturating nonlinear dampening yields an anisotropic\ndistribution of singular values. At the edge of chaos, g = 1, an O(1) fraction of the singular value distribu-\n13\ntion is concentrated in a range that remains O(1) despite 100 layers of propagation, re\ufb02ecting appoximate\ndynamical isometry. Moreover, this nice property at g = 1 remains valid even as the input variance q1 is\nincreased far beyond 1, where the tanh function enters its nonlinear regime. Thus the right column of Fig.\n7 at g near 1 indicates that the useful dynamical isometry properties of random orthogonal linear networks\ndescribed above survives in nonlinear networks, even when activity patterns enter deeply into the nonlinear\nregime in the input layers. Interestingly, the singular value spectrum is more robust to perturbations that\nincrease g from 1 relative to those that decrease g. Indeed, the anisotropy in the singular value distribution at\ng = 1.1 is relatively mild compared to that of random linear networks with scaled Gaussian initial conditions\n(compare the bottom row of Fig. 7 with the right column of panel B in Fig. 6). Thus overall, these numerical\nresults suggest that being just beyond the edge of orthogonal chaos may be a good regime for learning in\ndeep nonlinear networks.\n5\nDiscussion\nIn summary, despite the simplicity of their input-output map, the dynamics of learning in deep linear net-\nworks reveals a surprising amount of rich mathematical structure, including nonlinear hyperbolic dynamics,\nplateaus and sudden performance transitions, a proliferation of saddle points, symmetries and conserved\nquantities, invariant submanifolds of independently evolving connectivity modes subserving rapid learning,\nand most importantly, a sensitive but computable dependence of learning time scales on input statistics, ini-\ntial weight conditions, and network depth. With the right initial conditions, deep linear networks can be only\na \ufb01nite amount slower than shallow networks, and unsupervised pretraining can \ufb01nd these initial conditions\nfor tasks with the right structure. Moreover, we introduce a mathematical condition for faithful backprop-\nagation of error signals, namely dynamical isometry, and show, surprisingly that random scaled Gaussian\ninitializations cannot achieve this condition despite their norm-preserving nature, while greedy pre-training\nand random orthogonal initialization can, thereby achieving depth independent learning times. Finally, we\nshow that the property of dynamical isometry survives to good approximation even in extremely deep non-\nlinear random orthogonal networks operating just beyond the edge of chaos. At the cost of expressivity, deep\nlinear networks gain theoretical tractability and may prove fertile for addressing other phenomena in deep\nlearning, such as the impact of carefully-scaled initializations [13, 23], momentum [23], dropout regulariza-\ntion [1], and sparsity constraints [2]. While a full analytical treatment of learning in deep nonlinear networks\ncurrently remains open, one cannot reasonably hope to move towards such a theory without \ufb01rst completely\nunderstanding the linear case. In this sense, our work ful\ufb01lls an essential pre-requisite for progress towards\na general, quantitative theory of deep learning.\nReferences\n[1] A. Krizhevsky, I. Sutskever, and G.E. Hinton. ImageNet Classi\ufb01cation with Deep Convolutional Neural\nNetworks. In Advances in Neural Information Processing Systems 25, 2012.\n[2] Q.V. Le, M.A. Ranzato, R. Monga, M. Devin, K. Chen, G.S. Corrado, J. Dean, and A.Y. Ng. Build-\ning high-level features using large scale unsupervised learning. In 29th International Conference on\nMachine Learning, 2012.\n[3] D. Ciresan, U. Meier, and J. Schmidhuber. Multi-column Deep Neural Networks for Image Classi\ufb01ca-\ntion. In IEEE Conf. on Computer Vision and Pattern Recognition, pages 3642\u20133649, 2012.\n[4] A. Mohamed, G.E. Dahl, and G. Hinton. Acoustic Modeling Using Deep Belief Networks. IEEE\nTransactions on Audio, Speech, and Language Processing, 20(1):14\u201322, January 2012.\n[5] R. Collobert and J. Weston. A Uni\ufb01ed Architecture for Natural Language Processing: Deep Neural\nNetworks with Multitask Learning. In Proceedings of the 25th International Conference on Machine\nLearning, 2008.\n14\n[6] R. Socher, J. Bauer, C.D. Manning, and A.Y. Ng. Parsing with Compositional Vector Grammars. In\nAssociation for Computational Linguistics Conference, 2013.\n[7] S. Hochreiter. Untersuchungen zu dynamischen neuronalen Netzen. PhD thesis, TU Munich, 1991.\n[8] Y. Bengio, P. Simard, and P. Frasconi. Learning Long-Term Dependencies with Gradient Descent is\nDif\ufb01cult. IEEE Transactions on Neural Networks, 5(2):157\u2013166, 1994.\n[9] Y. LeCun, L. Bottou, G.B. Orr, and K.R. M\u00a8uller. Ef\ufb01cient BackProp. Neural networks: Tricks of the\ntrade, 1998.\n[10] Y. Bengio and Y. LeCun. Scaling learning algorithms towards AI. In L. Bottou, O. Chapelle, D. De-\nCoste, and J. Weston, editors, Large-Scale Kernel Machines, number 1, pages 1\u201341. MIT Press, 2007.\n[11] D. Erhan, P.A. Manzagol, Y. Bengio, S. Bengio, and P. Vincent. The Dif\ufb01culty of Training Deep Ar-\nchitectures and the Effect of Unsupervised Pre-Training. In 12th International Conference on Arti\ufb01cial\nIntelligence and Statistics, volume 5, 2009.\n[12] Y. Bengio. Learning Deep Architectures for AI. 2009.\n[13] X. Glorot and Y. Bengio. Understanding the dif\ufb01culty of training deep feedforward neural networks.\n13th International Conference on Arti\ufb01cial Intelligence and Statistics, 2010.\n[14] D. Erhan, Y. Bengio, A. Courville, P.A. Manzagol, and P. Vincent. Why does unsupervised pre-training\nhelp deep learning? Journal of Machine Learning Research, 11:625\u2013660, 2010.\n[15] Y.N. Dauphin and Y. Bengio. Big Neural Networks Waste Capacity. In International Conference on\nLearning Representations, 2013.\n[16] A.M. Saxe, J.L. McClelland, and S. Ganguli. Learning hierarchical category structure in deep neural\nnetworks. In Proceedings of the 35th Annual Conference of the Cognitive Science Society, 2013.\n[17] G.E. Hinton and R.R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Sci-\nence, 313(5786):504\u20137, July 2006.\n[18] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. Greedy Layer-Wise Training of Deep Net-\nworks. Advances in Neural Information Processing Systems 20, 2007.\n[19] P. Baldi and K. Hornik. Neural networks and principal component analysis: Learning from examples\nwithout local minima. Neural Networks, 2(1):53\u201358, January 1989.\n[20] K. Fukumizu. Effect of Batch Learning In Multilayer Neural Networks. In Proceedings of the 5th\nInternational Conference on Neural Information Processing, pages 67\u201370, 1998.\n[21] J. Martens. Deep learning via Hessian-free optimization. In Proceedings of the 27th International\nConference on Machine Learning, 2010.\n[22] O. Chapelle and D. Erhan. Improved Preconditioner for Hessian Free Optimization. In NIPS Workshop\non Deep Learning and Unsupervised Feature Learning, 2011.\n[23] I. Sutskever, J. Martens, G. Dahl, and G.E. Hinton. On the importance of initialization and momentum\nin deep learning. In 30th International Conference on Machine Learning, 2013.\n[24] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the dif\ufb01culty of training recurrent neural\nnetworks. Technical report, Universite de Montreal, 2012.\n[25] P. Lamblin and Y. Bengio. Important gains from supervised \ufb01ne-tuning of deep architectures on large\nlabeled sets. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2010.\n15\nSupplementary Material\nA\nHyperbolic dynamics of learning\nIn Section 1.3 of the main text we treat the dynamics of learning in three layer networks where mode\nstrengths in each layer are equal, i.e, a = b, a reasonable limit when starting with small random initial\nconditions. More generally, though, we are interested in how long it takes for ab to approach s from any\ngiven initial condition. To access this, given the hyperbolic nature of the dynamics, it is useful to make the\nhyperbolic change of coordinates,\na = \u221ac0 cosh \u03b8\n2\nb = \u221ac0 sinh \u03b8\n2\nfor a2 > b2\n(23)\na = \u221ac0 sinh \u03b8\n2\nb = \u221ac0 cosh \u03b8\n2\nfor a2 < b2.\n(24)\nThus \u03b8 parametrizes the dynamically invariant manifolds a2 \u2212b2 = \u00b1c0. For any c0 and \u03b8, this coordinate\nsystem covers the region a + b > 0, which is the basin of attraction of the upper right component of the\nhyperbola ab = s. A symmetric situation exists for a+b < 0, which is attracted to the lower left component\nof ab = s. We use \u03b8 as a coordinate to follow the dynamics of the product ab, and using the relations\nab = c0 sinh \u03b8 and a2 + b2 = c0 cosh \u03b8, we obtain\n\u03c4 d\u03b8\ndt = s \u2212c0 sinh \u03b8.\n(25)\nThis differential equation is separable in \u03b8 and t and can be integrated to yield\nt = \u03c4\nZ \u03b8f\n\u03b80\nd\u03b8\ns \u2212c0 sinh \u03b8 =\n\u03c4\np\nc2\n0 + s2\n\"\nln\np\nc2\n0 + s2 + c0 + s tanh \u03b8\n2\np\nc2\n0 + s2 \u2212c0 \u2212s tanh \u03b8\n2\n#\u03b8f\n\u03b80\n.\n(26)\nHere t is the amount of time it takes to travel from \u03b80 to \u03b8f along the hyperbola a2 \u2212b2 = \u00b1c0. The \ufb01xed\npoint lies at \u03b8 = sinh\u22121 s/c0, but the dynamics cannot reach the \ufb01xed point in \ufb01nite time. Therefore we\nintroduce a cutoff \u03f5 to mark the endpoint of learning, so that \u03b8f obeys sinh \u03b8f = (1 \u2212\u03f5)s/c0 (i.e. ab is\nclose to s by a factor 1 \u2212\u03f5). We can then average over the initial conditions c0 and \u03b80 to obtain the expected\nlearning time of an input-output relation that has a correlation strength s. Rather than doing this, it is easier\nto obtain a rough estimate of the timescale of learning under the assumption that the initial weights are small,\nso that c0 and \u03b80 are close to 0. In this case t = O(\u03c4/s) (with a weak logarithmic dependence on the cutoff\n(i.e. ln(1/\u03f5)). This modestly generalizes the result given in the main text: the timescale of learning of each\ninput-output mode \u03b1 of the correlation matrix \u03a331 is inversely proportional to the correlation strength s\u03b1 of\nthe mode even when a and b differ slightly, i.e., c0 small. This is not an unreasonable limit for random initial\nconditions because |c0| = |a \u00b7 a \u2212b \u00b7 b| where a and b are random vectors of N2 synaptic weights into and\nout of the hidden units. Thus we expect the lengths of the two random vectors to be approximately equal\nand therefore c0 will be small relative to the length of each vector.\nThese solutions are distinctly different from solutions for learning dynamics in three layer networks found\nin [20]. In our notation, in [20], it was shown that if the initial vectors a\u03b1 and b\u03b1 satisfy the matrix identity\nP\n\u03b1 a\u03b1a\u03b1T = P\n\u03b1 b\u03b1b\u03b1T then the dynamics of learning becomes equivalent to a matrix Riccatti equation.\nHowever, the hyperbolic dynamics derived here arises from a set of initial conditions that do not satisfy the\nrestrictions of [20] and therefore do not arise through a solution to a matrix Ricatti equation. Moreover,\nin going beyond a statement of the matrix Riccatti solution, our analysis provides intuition about the time-\nscales over which the learning dynamics unfolds, and crucially, our methods extend beyond the three layer\ncase to the arbitrary Nl layer case, not studied in [20].\n16\nB\nOptimal discrete time learning rates\nIn Section 2 we state results on the optimal learning rate as a function of depth in a deep linear network,\nwhich we derive here. Starting from the decoupled initial conditions given in the main text, the dynamics\narise from gradient descent on\nE(a1, \u00b7 \u00b7 \u00b7 , aNl\u22121) = 1\n2\u03c4\n \ns \u2212\nNl\u22121\nY\nk=1\nak\n!\n.\n(27)\nHence for each ai we have\n\u2202E\n\u2202ai\n= \u22121\n\u03c4\n \ns \u2212\nNl\u22121\nY\nk=1\nak\n! \uf8eb\n\uf8ed\nNl\u22121\nY\nk\u0338=i\nak\n\uf8f6\n\uf8f8\u2261f(ai)\n(28)\nThe elements of the Hessian are thus\n\u22022E\n\u2202aiaj\n=\n1\n\u03c4\n\uf8eb\n\uf8ed\nNl\u22121\nY\nk\u0338=j\nak\n\uf8f6\n\uf8f8\n\uf8eb\n\uf8ed\nNl\u22121\nY\nk\u0338=i\nak\n\uf8f6\n\uf8f8\u22121\n\u03c4\n \ns \u2212\nNl\u22121\nY\nk=1\nak\n! \uf8eb\n\uf8ed\nNl\u22121\nY\nk\u0338=i,j\nak\n\uf8f6\n\uf8f8\n(29)\n\u2261\ng(ai, aj)\n(30)\nfor i \u0338= j, and\n\u22022E\n\u2202a2\ni\n= 1\n\u03c4\n\uf8eb\n\uf8ed\nNl\u22121\nY\nk\u0338=i\nak\n\uf8f6\n\uf8f8\n2\n\u2261h(ai)\n(31)\nfor i = j.\nWe now assume that we start on the symmetric manifold, such that ai = aj = a for all i, j. Thus we have\nE(a)\n=\n1\n2\u03c4\n\u0000s \u2212aNl\u22121\u0001\n,\n(32)\nf(a)\n=\n\u22121\n\u03c4\n\u0000s \u2212aNl\u22121\u0001\naNl\u22122,\n(33)\ng(a)\n=\n2\n\u03c4 a2Nl\u22124 \u22121\n\u03c4 saNl\u22123\n(34)\nh(a)\n=\n1\n\u03c4 a2Nl\u22124\n(35)\nThe Hessian is\nH(a) =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\nh\ng\n\u00b7 \u00b7 \u00b7\ng\ng\ng\nh\n\u00b7 \u00b7 \u00b7\ng\ng\n...\n...\n...\ng\ng\n\u00b7 \u00b7 \u00b7\nh\ng\ng\ng\n\u00b7 \u00b7 \u00b7\ng\nh\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n.\n(36)\nOne eigenvector is v1 = [11 \u00b7 \u00b7 \u00b7 1]T with eigenvalue \u03bb1 = h + (Nl \u22122)g, or\n\u03bb1 = (2Nl \u22123)1\n\u03c4 a2Nl\u22124 \u2212(Nl \u22122)1\n\u03c4 saNl\u22123.\n(37)\n17\nNow consider the second order update (Newton-Raphson) (here we use 1 to denote a vector of ones)\nat+11\n=\nat1 \u2212H\u22121f(at)1\n(38)\n=\nat1 \u2212f(at)H\u221211\n(39)\nat+1\n=\nat \u2212f(at)/\u03bb1(at)\n(40)\nNote that the basin of attraction does not include small initial conditions, because for small a the Hessian is\nnot positive de\ufb01nite.\nTo determine the optimal learning rate for \ufb01rst order gradient descent, we compute the maximum of \u03bb1 over\nthe range of mode strengths that can be visited during learning, i.e., a \u2208[0, s1/(Nl\u22121)]. This occurs at the\noptimum, aopt = s1/(Nl\u22121). Hence substituting this into (37) we have\n\u03bb1(aopt) = (Nl \u22121)1\n\u03c4 s\n2Nl\u22124\nNl\u22121 .\n(41)\nThe optimal learning rate \u03b1 is proportional to 1/\u03bb1(aopt), and hence scales as\n\u03b1 \u223cO\n\u0012\n1\nNls2\n\u0013\n(42)\nfor large Nl.\nB.1\nLearning speeds with optimized learning rate\nHow does the optimal learning rate impact learning speeds? We compare the three layer learning time to the\nin\ufb01nite depth limit learning time, with learning rate set inversely proportional to Eqn. (41) with proportion-\nality constant c.\nThis yields a three layer learning time t3 of\nt3 = c ln uf(s \u2212u0)\nu0(s \u2212uf)\n(43)\nand an in\ufb01nite layer learning time t\u221eof\nt\u221e= c\n\u0014\nlog\n\u0012uf(u0 \u2212s)\nu0(uf \u2212s)\n\u0013\n+ s\nu0\n\u2212s\nuf\n\u0015\n,\n(44)\nHence the difference is\nt\u221e\u2212t3 = cs\nu0\n\u2212cs\nuf\n\u2248cs\n\u03f5\n(45)\nwhere the \ufb01nal approximation is for u0 = \u03f5, uf = s \u2212\u03f5, and \u03f5 small. Thus very deep networks incur only a\n\ufb01nite delay relative to shallow networks.\nC\nExperimental setup for MNIST depth experiment\nWe trained deep linear networks on the MNIST dataset with \ufb01fteen different depths Nl\n=\n{3, 5, 8, 10, 14, 20, 28, 36, 44, 54, 64, 74, 84, 94, 100}. Given a 784-dimensional input example, the network\ntried to predict a 10-dimensional output vector containing a 1 in the index for the correct class, and zeros\nelsewhere. The network was trained using batch gradient descent via Eqn. (13) on the 50,000 sample MNIST\ntraining dataset. We note that Eqn. (13) makes use of the linearity of the network to speed training and re-\nduce memory requirements. Instead of forward propagating all 50,000 training examples, we precompute\n18\n\u03a331 and forward propagate only it. This enables experiments on very deep networks that otherwise would\nbe computationally infeasible. Experiments were accelerated on GPU hardware using the GPUmat package.\nWe used overcomplete hidden layers of size 1000. Here the overcompleteness is simply to demonstrate the\napplicability of the theory to this case; overcompleteness does not improve the representational power of\nthe network. Networks were initialized with decoupled initial conditions and starting initial mode strength\nu0 = 0.001, as described in the text. The random orthogonal matrices Rl were selected by generating ran-\ndom Gaussian matrices and computing a QR decomposition to obtain an orthogonal matrix. Learning times\nwere calculated as the iteration at which training error fell below a \ufb01xed threshold of 1.3\u00d7104 corresponding\nto nearly complete learning. Note that this level of performance is grossly inferior to what can be obtained\nusing nonlinear networks, which re\ufb02ects the limited capacity of a linear network. We optimized the learning\nrate \u03bb separately for each depth by training each network with twenty rates logarithmically spaced between\n10\u22124 and 10\u22127 and picking the one that yielded the minimum learning time according to our threshold crite-\nrion. The range 10\u22124 and 10\u22127 was selected via preliminary experiments to ensure that the optimal learning\nrate always lay in the interior of the range for all depths.\nD\nEf\ufb01cacy of unsupervised pretraining\nRecently high performance has been demonstrated in deep networks trained from random initial conditions\n[21, 13, 22, 3, 4, 1, 23], suggesting that deep networks may not be as hard to train as previously thought.\nThese results show that pretraining is not necessary to obtain state-of-the-art performance, and to achieve\nthis they make use of a variety of techniques including carefully-scaled random initializations, more sophis-\nticated second order or momentum-based optimization methods, and specialized convolutional architectures.\nIt is therefore important to evaluate whether unsupervised pretraining is still useful, even if it is no longer\nnecessary, for training deep networks. In particular, does pretraining still confer an optimization advantage\nand generalization advantage when used in conjunction with these new techniques? Here we review results\nfrom a variety of papers, which collectively show that unsupervised pretraining still confers an optimization\nadvantage and a generalization advantage.\nD.1\nOptimization advantage\nThe optimization advatage of pretraining refers to faster convergence to the local optimum (i.e., faster learn-\ning speeds) when starting from pretrained initializations as compared to random initializations. Faster learn-\ning speeds starting from pretrained initial conditions have been consistently found with Hessian free opti-\nmization [21, 22]. This \ufb01nding holds for two carefully-chosen random initialization schemes, the sparse\nconnectivity scheme of [21], and the dense scaled scheme of [13] (as used by [22]). Hence pretraining\nstill confers a convergence speed advantage with second order methods. Pretrained initial conditions also\nresult in faster convergence than carefully-chosen random initializations when optimizing with stochastic\ngradient descent [22, 13]. In light of this, it appears that pretrained initial conditions confer an optimization\nadvantage beyond what can be obtained currently with carefully-scaled random initializations, regardless of\noptimization technique. If run to convergence, second order methods and well-chosen scalings can erase the\ndiscrepancy between the \ufb01nal objective value obtained on the training set for pretrained relative to random\ninitializations [21, 22]. The optimization advantage is thus purely one of convergence speed, not of \ufb01nding\na better local minimum. This coincides with the situation in linear networks, where all methods will even-\ntually attain the same global minimum, but the rate of convergence can vary. Our analysis shows why this\noptimization advantage due to pretraining persists over well-chosen random initializations.\nFinally, we note that Sutskever et al. show that careful random initialization paired with carefully-tuned\nmomentum can achieve excellent performance [23], but these experiments did not try pretrained initial\nconditions. Krizhevsky et al. used convolutional architectures and did not attempt pretraining [1]. Thus\n19\nthe possible utility of pretraining in combination with momentum, and in combination with convolutional\narchitectures, dropout, and large supervised datasets, remains unclear.\nD.2\nGeneralization advantage\nPretraining can also act as a special regularizer, improving generalization error in certain instances. This\ngeneralization advantage appears to persist with new second order methods [21, 22], and in comparison to\ngradient descent with careful random initializations [13, 22, 25, 4]. An analysis of this effect in deep linear\nnetworks is out of the scope of this work, though promising tools have been developed for the three layer\nlinear case [20].\nE\nLearning dynamics with task-aligned input correlations\nIn the main text we focused on orthogonal input correlations (\u03a311 = I) for simplicity, and to draw out\nthe main intuitions. However our analysis can be extended to input correlations with a very particular\nstructure. Recall that we decompose the input output correlations using the SVD as \u03a331 = U 33S31V 11T .\nWe can generalize our solutions to allow input correlations of the form \u03a311 = V 11DV 11T . Intuitively, this\ncondition requires the axes of variation in the input to coincide with the axes of variation in the input-output\ntask, though the variances may differ. If we take D = I then we recover the whitened case \u03a311 = I, and\nif we take D = \u039b, then we can treat the autoencoding case. The \ufb01nal \ufb01xed points of the weights are given\nby the best rank N2 approximation to \u03a331(\u03a311)\u22121. Making the same change of variables as in Eqn. (4) we\nnow obtain\n\u03c4 d\ndtW\n21 = W\n32T\n(S31 \u2212W\n32W\n21D),\n\u03c4 d\ndtW\n32 = (S31 \u2212W\n32W\n21D)W\n21T\n.\n(46)\nwhich, again, is decoupled if W\n32 and W\n21 begin diagonal. Based on this it is straightforward to generalize\nour results for the learning dynamics.\nF\nMNIST pretraining experiment\nWe trained networks of depth 5 on the MNIST classi\ufb01cation task with 200 hidden units per layer, starting\neither from small random initial conditions with each weight drawn independently from a Gaussian distribu-\ntion with standard deviation 0.01, or from greedy layerwise pretrained initial conditions. For the pretrained\nnetwork, each layer was trained to reconstruct the output of the next lower layer. In the \ufb01netuning stage, the\nnetwork tried to predict a 10-dimensional output vector containing a 1 in the index for the correct class, and\nzeros elsewhere. The network was trained using batch gradient descent via Eqn. (13) on the 50,000 sam-\nple MNIST training dataset. Since the network is linear, pretraining initializes the network with principal\ncomponents of the input data, and, to the extent that the consistency condition of Eqn. (18) holds, decouples\nthese modes throughout the deep network, as described in the main text.\nG\nAnalysis of Neural Dynamics in Nonlinear Orthogonal Networks\nWe can derive a simple, analytical recursion relation for the propagation of neural population variance ql,\nde\ufb01ned in (21), across layers l under the nonlinear dynamics (20). We have\nql+1 = 1\nN\nN\nX\ni=1\n(xl+1\ni\n)2 = g2 1\nN\nN\nX\ni=1\n\u03c6(xl\ni)2,\n(47)\n20\ndue to the dynamics in (20) and the orthogonality of W (l+1,l). Now we know that by de\ufb01nition, the layer\nl population xl\ni has normalized variance ql. If we further assume that the distribution of activity across\nneurons in layer l is well approximated by a Gaussian distribution, we can replace the sum over neurons i\nwith an integral over a zero mean unit variance Gaussian variable z:\nql+1 = g2\nZ\nDz \u03c6\n\u0000p\nqlz\n\u00012,\n(48)\nwhere Dz \u2261\n1\n\u221a\n2\u03c0e\u22121\n2 z2 dz is the standard Gaussian measure. This map from input to output variance\n0\n0.5\n1\n1.5\n2\n0\n0.2\n0.4\n0.6\n0.8\nqin\nqout\nInput to Output Variance Map at g=1\n0\n0.5\n1\n1.5\n0\n0.5\n1\n1.5\n2\ng\nq\u221e(g)\nStable population variance versus gain\nFigure 8: Left: The map from variance in the input layer qin = ql to variance in the output layer qout = ql+1\nin (48) for g = 1 and \u03c6(x) = tanh(x). Right: The stable \ufb01xed points of this map, q\u221e(g), as a function of\nthe gain g. The red curve is the analytic theory obtained by numerically solving (49). The blue points are\nobtained via numerical simulations of the dynamics in (20) for networks of depth Nl = 30 with N = 1000\nneurons per layer. The asymptotic population variance q\u221eis obtained by averaging the population variance\nin the last 5 layers.\nis numerically computed for g = 1 and \u03c6(x) = tanh(x) in Fig. 8, left (other values of g yield a simple\n21\nmultiplicative scaling of this map). This recursion relation has a stable \ufb01xed point q\u221e(g) obtained by solving\nthe nonlinear \ufb01xed point equation\nq\u221e= g2\nZ\nDz \u03c6\n\u0000\u221aq\u221ez\n\u00012.\n(49)\nGraphically, solving this equation corresponds to scaling the curve in Fig. 8 left by g2 and looking for\nintersections with the line of unity. For g < 1, the only solution is q\u221e= 0. For g > 1, this solution remains,\nbut it is unstable under the recurrence (48). Instead, for g > 1, a new stable solution appears for some\nnonzero value of q\u221e. The entire set of stable solutions as a function of g is shown as the red curve in Fig.\n8 right. It constitutes a theoretical prediction of the population variance at the deepest layers of a nonlinear\nnetwork as the depth goes to in\ufb01nity. It matches well for example, the empirical population variance obtained\nfrom numerical simulations of nonlinear networks of depth 30 (blue points in Fig. 8 right).\nOverall, these results indicate a dynamical phase transition in neural activity propagation through the non-\nlinear network as g crosses the critical value gc = 1. When g > 1, activity propagates in a chaotic manner,\nand so g = 1 constitutes the edge of chaos.\n22\n",
        "sentence": " The values of all W \u2019s and U \u2019s were initialized to orthonormal matrices using singular value decomposition of matrices generated from the normal distribution (Saxe et al., 2013).",
        "context": "only in their scalar magnitudes, and are orthogonal to all other connectivity modes. Such an initialization\ncan be obtained by computing the SVD of \u03a331 and taking W 32 = U 33DaRT , W 21 = RDbV 11T where\ndynamics. We consider its singular value decomposition (SVD)\n\u03a331 = U 33S31V 11T = PN1\n\u03b1=1 s\u03b1u\u03b1vT\n\u03b1,\n(3)\nwhich will be central in our analysis. Here V 11 is an N1 \u00d7 N1 orthogonal matrix whose columns contain\nu0 = 0.001, as described in the text. The random orthogonal matrices Rl were selected by generating ran-\ndom Gaussian matrices and computing a QR decomposition to obtain an orthogonal matrix. Learning times"
    },
    {
        "title": "Diabetic cataract formation: potential role of glycosylation of lens crystallins",
        "author": [
            "Stevens",
            "Victor J",
            "Rouzer",
            "Carol A",
            "Monnier",
            "Vincent M",
            "Cerami",
            "Anthony"
        ],
        "venue": null,
        "citeRegEx": "Stevens et al\\.,? \\Q1978\\E",
        "shortCiteRegEx": "Stevens et al\\.",
        "year": 1978,
        "abstract": "",
        "full_text": "",
        "sentence": " A human doctor would likely predict similar diseases to the ones predicted with Doctor AI, since old myocardial infarction and chronic ischemic heart disease can be associated with infections and diabetes (Stevens et al., 1978).",
        "context": null
    },
    {
        "title": "Sequence to sequence learning with neural networks",
        "author": [
            "Sutskever",
            "Ilya",
            "Vinyals",
            "Oriol",
            "Le",
            "Quoc VV"
        ],
        "venue": "In NIPS, pp",
        "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Sutskever et al\\.",
        "year": 2014,
        "abstract": "Deep Neural Networks (DNNs) are powerful models that have achieved excellent\nperformance on difficult learning tasks. Although DNNs work well whenever large\nlabeled training sets are available, they cannot be used to map sequences to\nsequences. In this paper, we present a general end-to-end approach to sequence\nlearning that makes minimal assumptions on the sequence structure. Our method\nuses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to\na vector of a fixed dimensionality, and then another deep LSTM to decode the\ntarget sequence from the vector. Our main result is that on an English to\nFrench translation task from the WMT'14 dataset, the translations produced by\nthe LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's\nBLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did\nnot have difficulty on long sentences. For comparison, a phrase-based SMT\nsystem achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM\nto rerank the 1000 hypotheses produced by the aforementioned SMT system, its\nBLEU score increases to 36.5, which is close to the previous best result on\nthis task. The LSTM also learned sensible phrase and sentence representations\nthat are sensitive to word order and are relatively invariant to the active and\nthe passive voice. Finally, we found that reversing the order of the words in\nall source sentences (but not target sentences) improved the LSTM's performance\nmarkedly, because doing so introduced many short term dependencies between the\nsource and the target sentence which made the optimization problem easier.",
        "full_text": "arXiv:1409.3215v3  [cs.CL]  14 Dec 2014\nSequence to Sequence Learning\nwith Neural Networks\nIlya Sutskever\nGoogle\nilyasu@google.com\nOriol Vinyals\nGoogle\nvinyals@google.com\nQuoc V. Le\nGoogle\nqvl@google.com\nAbstract\nDeep Neural Networks (DNNs) are powerful models that have achieved excel-\nlent performance on dif\ufb01cult learning tasks. Although DNNs work well whenever\nlarge labeled training sets are available, they cannot be used to map sequences to\nsequences. In this paper, we present a general end-to-end approach to sequence\nlearning that makes minimal assumptions on the sequence structure. Our method\nuses a multilayered Long Short-Term Memory (LSTM) to map the input sequence\nto a vector of a \ufb01xed dimensionality, and then another deep LSTM to decode the\ntarget sequence from the vector. Our main result is that on an English to French\ntranslation task from the WMT\u201914 dataset, the translations produced by the LSTM\nachieve a BLEU score of 34.8 on the entire test set, where the LSTM\u2019s BLEU\nscore was penalized on out-of-vocabulary words. Additionally, the LSTM did not\nhave dif\ufb01culty on long sentences. For comparison, a phrase-based SMT system\nachieves a BLEU score of 33.3 on the same dataset. When we used the LSTM\nto rerank the 1000 hypotheses produced by the aforementioned SMT system, its\nBLEU score increases to 36.5, which is close to the previous best result on this\ntask. The LSTM also learned sensible phrase and sentence representations that\nare sensitive to word order and are relatively invariant to the active and the pas-\nsive voice. Finally, we found that reversing the order of the words in all source\nsentences (but not target sentences) improved the LSTM\u2019s performance markedly,\nbecause doing so introduced many short term dependencies between the source\nand the target sentence which made the optimization problem easier.\n1\nIntroduction\nDeep Neural Networks (DNNs) are extremely powerful machine learning models that achieve ex-\ncellent performance on dif\ufb01cult problems such as speech recognition [13, 7] and visual object recog-\nnition [19, 6, 21, 20]. DNNs are powerful because they can perform arbitrary parallel computation\nfor a modest number of steps. A surprising example of the power of DNNs is their ability to sort\nN N-bit numbers using only 2 hidden layers of quadratic size [27]. So, while neural networks are\nrelated to conventional statistical models, they learn an intricate computation. Furthermore, large\nDNNs can be trained with supervised backpropagation whenever the labeled training set has enough\ninformation to specify the network\u2019s parameters. Thus, if there exists a parameter setting of a large\nDNN that achieves good results (for example, because humans can solve the task very rapidly),\nsupervised backpropagation will \ufb01nd these parameters and solve the problem.\nDespite their \ufb02exibility and power, DNNs can only be applied to problems whose inputs and targets\ncan be sensibly encoded with vectors of \ufb01xed dimensionality. It is a signi\ufb01cant limitation, since\nmany important problems are best expressed with sequences whose lengths are not known a-priori.\nFor example, speech recognition and machine translation are sequential problems. Likewise, ques-\ntion answering can also be seen as mapping a sequence of words representing the question to a\n1\nsequence of words representing the answer. It is therefore clear that a domain-independent method\nthat learns to map sequences to sequences would be useful.\nSequences pose a challenge for DNNs because they require that the dimensionality of the inputs and\noutputs is known and \ufb01xed. In this paper, we show that a straightforward application of the Long\nShort-Term Memory (LSTM) architecture [16] can solve general sequence to sequence problems.\nThe idea is to use one LSTM to read the input sequence, one timestep at a time, to obtain large \ufb01xed-\ndimensional vector representation, and then to use another LSTM to extract the output sequence\nfrom that vector (\ufb01g. 1). The second LSTM is essentially a recurrent neural network language model\n[28, 23, 30] except that it is conditioned on the input sequence. The LSTM\u2019s ability to successfully\nlearn on data with long range temporal dependencies makes it a natural choice for this application\ndue to the considerable time lag between the inputs and their corresponding outputs (\ufb01g. 1).\nThere have been a number of related attempts to address the general sequence to sequence learning\nproblem with neural networks. Our approach is closely related to Kalchbrenner and Blunsom [18]\nwho were the \ufb01rst to map the entire input sentence to vector, and is related to Cho et al. [5] although\nthe latter was used only for rescoring hypotheses produced by a phrase-based system. Graves [10]\nintroduced a novel differentiable attention mechanism that allows neural networks to focus on dif-\nferent parts of their input, and an elegant variant of this idea was successfully applied to machine\ntranslation by Bahdanau et al. [2]. The Connectionist Sequence Classi\ufb01cation is another popular\ntechnique for mapping sequences to sequences with neural networks, but it assumes a monotonic\nalignment between the inputs and the outputs [11].\nFigure 1: Our model reads an input sentence \u201cABC\u201d and produces \u201cWXYZ\u201d as the output sentence. The\nmodel stops making predictions after outputting the end-of-sentence token. Note that the LSTM reads the\ninput sentence in reverse, because doing so introduces many short term dependencies in the data that make the\noptimization problem much easier.\nThe main result of this work is the following. On the WMT\u201914 English to French translation task,\nwe obtained a BLEU score of 34.81 by directly extracting translations from an ensemble of 5 deep\nLSTMs (with 384M parameters and 8,000 dimensional state each) using a simple left-to-right beam-\nsearch decoder. This is by far the best result achieved by direct translation with large neural net-\nworks. For comparison, the BLEU score of an SMT baseline on this dataset is 33.30 [29]. The 34.81\nBLEU score was achieved by an LSTM with a vocabulary of 80k words, so the score was penalized\nwhenever the reference translation contained a word not covered by these 80k. This result shows\nthat a relatively unoptimized small-vocabulary neural network architecture which has much room\nfor improvement outperforms a phrase-based SMT system.\nFinally, we used the LSTM to rescore the publicly available 1000-best lists of the SMT baseline on\nthe same task [29]. By doing so, we obtained a BLEU score of 36.5, which improves the baseline by\n3.2 BLEU points and is close to the previous best published result on this task (which is 37.0 [9]).\nSurprisingly, the LSTM did not suffer on very long sentences, despite the recent experience of other\nresearchers with related architectures [26]. We were able to do well on long sentences because we\nreversed the order of words in the source sentence but not the target sentences in the training and test\nset. By doing so, we introduced many short term dependencies that made the optimization problem\nmuch simpler (see sec. 2 and 3.3). As a result, SGD could learn LSTMs that had no trouble with\nlong sentences. The simple trick of reversing the words in the source sentence is one of the key\ntechnical contributions of this work.\nA useful property of the LSTM is that it learns to map an input sentence of variable length into\na \ufb01xed-dimensional vector representation. Given that translations tend to be paraphrases of the\nsource sentences, the translation objective encourages the LSTM to \ufb01nd sentence representations\nthat capture their meaning, as sentences with similar meanings are close to each other while different\n2\nsentences meanings will be far. A qualitative evaluation supports this claim, showing that our model\nis aware of word order and is fairly invariant to the active and passive voice.\n2\nThe model\nThe Recurrent Neural Network (RNN) [31, 28] is a natural generalization of feedforward neural\nnetworks to sequences. Given a sequence of inputs (x1, . . . , xT ), a standard RNN computes a\nsequence of outputs (y1, . . . , yT ) by iterating the following equation:\nht\n=\nsigm\n\u0000W hxxt + W hhht\u22121\n\u0001\nyt\n=\nW yhht\nThe RNN can easily map sequences to sequences whenever the alignment between the inputs the\noutputs is known ahead of time. However, it is not clear how to apply an RNN to problems whose\ninput and the output sequences have different lengths with complicated and non-monotonic relation-\nships.\nThe simplest strategy for general sequence learning is to map the input sequence to a \ufb01xed-sized\nvector using one RNN, and then to map the vector to the target sequence with another RNN (this\napproach has also been taken by Cho et al. [5]). While it could work in principle since the RNN is\nprovided with all the relevant information, it would be dif\ufb01cult to train the RNNs due to the resulting\nlong term dependencies (\ufb01gure 1) [14, 4, 16, 15]. However, the Long Short-Term Memory (LSTM)\n[16] is known to learn problems with long range temporal dependencies, so an LSTM may succeed\nin this setting.\nThe goal of the LSTM is to estimate the conditional probability p(y1, . . . , yT \u2032|x1, . . . , xT ) where\n(x1, . . . , xT ) is an input sequence and y1, . . . , yT \u2032 is its corresponding output sequence whose length\nT \u2032 may differ from T . The LSTM computes this conditional probability by \ufb01rst obtaining the \ufb01xed-\ndimensional representation v of the input sequence (x1, . . . , xT ) given by the last hidden state of the\nLSTM, and then computing the probability of y1, . . . , yT \u2032 with a standard LSTM-LM formulation\nwhose initial hidden state is set to the representation v of x1, . . . , xT :\np(y1, . . . , yT \u2032|x1, . . . , xT ) =\nT \u2032\nY\nt=1\np(yt|v, y1, . . . , yt\u22121)\n(1)\nIn this equation, each p(yt|v, y1, . . . , yt\u22121) distribution is represented with a softmax over all the\nwords in the vocabulary. We use the LSTM formulation from Graves [10]. Note that we require that\neach sentence ends with a special end-of-sentence symbol \u201c<EOS>\u201d, which enables the model to\nde\ufb01ne a distribution over sequences of all possible lengths. The overall scheme is outlined in \ufb01gure\n1, where the shown LSTM computes the representation of \u201cA\u201d, \u201cB\u201d, \u201cC\u201d, \u201c<EOS>\u201d and then uses\nthis representation to compute the probability of \u201cW\u201d, \u201cX\u201d, \u201cY\u201d, \u201cZ\u201d, \u201c<EOS>\u201d.\nOur actual models differ from the above description in three important ways. First, we used two\ndifferent LSTMs: one for the input sequence and another for the output sequence, because doing\nso increases the number model parameters at negligible computational cost and makes it natural to\ntrain the LSTM on multiple language pairs simultaneously [18]. Second, we found that deep LSTMs\nsigni\ufb01cantly outperformed shallow LSTMs, so we chose an LSTM with four layers. Third, we found\nit extremely valuable to reverse the order of the words of the input sentence. So for example, instead\nof mapping the sentence a, b, c to the sentence \u03b1, \u03b2, \u03b3, the LSTM is asked to map c, b, a to \u03b1, \u03b2, \u03b3,\nwhere \u03b1, \u03b2, \u03b3 is the translation of a, b, c. This way, a is in close proximity to \u03b1, b is fairly close to \u03b2,\nand so on, a fact that makes it easy for SGD to \u201cestablish communication\u201d between the input and the\noutput. We found this simple data transformation to greatly improve the performance of the LSTM.\n3\nExperiments\nWe applied our method to the WMT\u201914 English to French MT task in two ways. We used it to\ndirectly translate the input sentence without using a reference SMT system and we it to rescore the\nn-best lists of an SMT baseline. We report the accuracy of these translation methods, present sample\ntranslations, and visualize the resulting sentence representation.\n3\n3.1\nDataset details\nWe used the WMT\u201914 English to French dataset. We trained our models on a subset of 12M sen-\ntences consisting of 348M French words and 304M English words, which is a clean \u201cselected\u201d\nsubset from [29]. We chose this translation task and this speci\ufb01c training set subset because of the\npublic availability of a tokenized training and test set together with 1000-best lists from the baseline\nSMT [29].\nAs typical neural language models rely on a vector representation for each word, we used a \ufb01xed\nvocabulary for both languages. We used 160,000 of the most frequent words for the source language\nand 80,000 of the most frequent words for the target language. Every out-of-vocabulary word was\nreplaced with a special \u201cUNK\u201d token.\n3.2\nDecoding and Rescoring\nThe core of our experiments involved training a large deep LSTM on many sentence pairs. We\ntrained it by maximizing the log probability of a correct translation T given the source sentence S,\nso the training objective is\n1/|S|\nX\n(T,S)\u2208S\nlog p(T |S)\nwhere S is the training set. Once training is complete, we produce translations by \ufb01nding the most\nlikely translation according to the LSTM:\n\u02c6T = arg max\nT\np(T |S)\n(2)\nWe search for the most likely translation using a simple left-to-right beam search decoder which\nmaintains a small number B of partial hypotheses, where a partial hypothesis is a pre\ufb01x of some\ntranslation. At each timestep we extend each partial hypothesis in the beam with every possible\nword in the vocabulary. This greatly increases the number of the hypotheses so we discard all but\nthe B most likely hypotheses according to the model\u2019s log probability. As soon as the \u201c<EOS>\u201d\nsymbol is appended to a hypothesis, it is removed from the beam and is added to the set of complete\nhypotheses. While this decoder is approximate, it is simple to implement. Interestingly, our system\nperforms well even with a beam size of 1, and a beam of size 2 provides most of the bene\ufb01ts of beam\nsearch (Table 1).\nWe also used the LSTM to rescore the 1000-best lists produced by the baseline system [29]. To\nrescore an n-best list, we computed the log probability of every hypothesis with our LSTM and took\nan even average with their score and the LSTM\u2019s score.\n3.3\nReversing the Source Sentences\nWhile the LSTM is capable of solving problems with long term dependencies, we discovered that\nthe LSTM learns much better when the source sentences are reversed (the target sentences are not\nreversed). By doing so, the LSTM\u2019s test perplexity dropped from 5.8 to 4.7, and the test BLEU\nscores of its decoded translations increased from 25.9 to 30.6.\nWhile we do not have a complete explanation to this phenomenon, we believe that it is caused by\nthe introduction of many short term dependencies to the dataset. Normally, when we concatenate a\nsource sentence with a target sentence, each word in the source sentence is far from its corresponding\nword in the target sentence. As a result, the problem has a large \u201cminimal time lag\u201d [17]. By\nreversing the words in the source sentence, the average distance between corresponding words in\nthe source and target language is unchanged. However, the \ufb01rst few words in the source language\nare now very close to the \ufb01rst few words in the target language, so the problem\u2019s minimal time lag is\ngreatly reduced. Thus, backpropagation has an easier time \u201cestablishing communication\u201d between\nthe source sentence and the target sentence, which in turn results in substantially improved overall\nperformance.\nInitially, we believed that reversing the input sentences would only lead to more con\ufb01dent predic-\ntions in the early parts of the target sentence and to less con\ufb01dent predictions in the later parts. How-\never, LSTMs trained on reversed source sentences did much better on long sentences than LSTMs\n4\ntrained on the raw source sentences (see sec. 3.7), which suggests that reversing the input sentences\nresults in LSTMs with better memory utilization.\n3.4\nTraining details\nWe found that the LSTM models are fairly easy to train. We used deep LSTMs with 4 layers,\nwith 1000 cells at each layer and 1000 dimensional word embeddings, with an input vocabulary\nof 160,000 and an output vocabulary of 80,000. Thus the deep LSTM uses 8000 real numbers to\nrepresent a sentence. We found deep LSTMs to signi\ufb01cantly outperform shallow LSTMs, where\neach additional layer reduced perplexity by nearly 10%, possibly due to their much larger hidden\nstate. We used a naive softmax over 80,000 words at each output. The resulting LSTM has 384M\nparameters of which 64M are pure recurrent connections (32M for the \u201cencoder\u201d LSTM and 32M\nfor the \u201cdecoder\u201d LSTM). The complete training details are given below:\n\u2022 We initialized all of the LSTM\u2019s parameters with the uniform distribution between -0.08\nand 0.08\n\u2022 We used stochastic gradient descent without momentum, with a \ufb01xed learning rate of 0.7.\nAfter 5 epochs, we begun halving the learning rate every half epoch. We trained our models\nfor a total of 7.5 epochs.\n\u2022 We used batches of 128 sequences for the gradient and divided it the size of the batch\n(namely, 128).\n\u2022 Although LSTMs tend to not suffer from the vanishing gradient problem, they can have\nexploding gradients. Thus we enforced a hard constraint on the norm of the gradient [10,\n25] by scaling it when its norm exceeded a threshold. For each training batch, we compute\ns = \u2225g\u22252, where g is the gradient divided by 128. If s > 5, we set g = 5g\ns .\n\u2022 Different sentences have different lengths. Most sentences are short (e.g., length 20-30)\nbut some sentences are long (e.g., length > 100), so a minibatch of 128 randomly chosen\ntraining sentences will have many short sentences and few long sentences, and as a result,\nmuch of the computation in the minibatch is wasted. To address this problem, we made sure\nthat all sentences in a minibatch are roughly of the same length, yielding a 2x speedup.\n3.5\nParallelization\nA C++ implementation of deep LSTM with the con\ufb01guration from the previous section on a sin-\ngle GPU processes a speed of approximately 1,700 words per second. This was too slow for our\npurposes, so we parallelized our model using an 8-GPU machine. Each layer of the LSTM was\nexecuted on a different GPU and communicated its activations to the next GPU / layer as soon as\nthey were computed. Our models have 4 layers of LSTMs, each of which resides on a separate\nGPU. The remaining 4 GPUs were used to parallelize the softmax, so each GPU was responsible\nfor multiplying by a 1000 \u00d7 20000 matrix. The resulting implementation achieved a speed of 6,300\n(both English and French) words per second with a minibatch size of 128. Training took about a ten\ndays with this implementation.\n3.6\nExperimental Results\nWe used the cased BLEU score [24] to evaluate the quality of our translations. We computed our\nBLEU scores using multi-bleu.pl1 on the tokenized predictions and ground truth. This way\nof evaluating the BELU score is consistent with [5] and [2], and reproduces the 33.3 score of [29].\nHowever, if we evaluate the best WMT\u201914 system [9] (whose predictions can be downloaded from\nstatmt.org\\matrix) in this manner, we get 37.0, which is greater than the 35.8 reported by\nstatmt.org\\matrix.\nThe results are presented in tables 1 and 2. Our best results are obtained with an ensemble of LSTMs\nthat differ in their random initializations and in the random order of minibatches. While the decoded\ntranslations of the LSTM ensemble do not outperform the best WMT\u201914 system, it is the \ufb01rst time\nthat a pure neural translation system outperforms a phrase-based SMT baseline on a large scale MT\n1There several variants of the BLEU score, and each variant is de\ufb01ned with a perl script.\n5\nMethod\ntest BLEU score (ntst14)\nBahdanau et al. [2]\n28.45\nBaseline System [29]\n33.30\nSingle forward LSTM, beam size 12\n26.17\nSingle reversed LSTM, beam size 12\n30.59\nEnsemble of 5 reversed LSTMs, beam size 1\n33.00\nEnsemble of 2 reversed LSTMs, beam size 12\n33.27\nEnsemble of 5 reversed LSTMs, beam size 2\n34.50\nEnsemble of 5 reversed LSTMs, beam size 12\n34.81\nTable 1: The performance of the LSTM on WMT\u201914 English to French test set (ntst14). Note that\nan ensemble of 5 LSTMs with a beam of size 2 is cheaper than of a single LSTM with a beam of\nsize 12.\nMethod\ntest BLEU score (ntst14)\nBaseline System [29]\n33.30\nCho et al. [5]\n34.54\nBest WMT\u201914 result [9]\n37.0\nRescoring the baseline 1000-best with a single forward LSTM\n35.61\nRescoring the baseline 1000-best with a single reversed LSTM\n35.85\nRescoring the baseline 1000-best with an ensemble of 5 reversed LSTMs\n36.5\nOracle Rescoring of the Baseline 1000-best lists\n\u223c45\nTable 2: Methods that use neural networks together with an SMT system on the WMT\u201914 English\nto French test set (ntst14).\ntask by a sizeable margin, despite its inability to handle out-of-vocabulary words. The LSTM is\nwithin 0.5 BLEU points of the best WMT\u201914 result if it is used to rescore the 1000-best list of the\nbaseline system.\n3.7\nPerformance on long sentences\nWe were surprised to discover that the LSTM did well on long sentences, which is shown quantita-\ntively in \ufb01gure 3. Table 3 presents several examples of long sentences and their translations.\n3.8\nModel Analysis\n\u22128\n\u22126\n\u22124\n\u22122\n0\n2\n4\n6\n8\n10\n\u22126\n\u22125\n\u22124\n\u22123\n\u22122\n\u22121\n0\n1\n2\n3\n4\nJohn respects Mary\nMary respects John\nJohn admires Mary\nMary admires John\nMary is in love with John\nJohn is in love with Mary\n\u221215\n\u221210\n\u22125\n0\n5\n10\n15\n20\n\u221220\n\u221215\n\u221210\n\u22125\n0\n5\n10\n15\nI gave her a card in the garden\nIn the garden , I gave her a card\nShe was given a card by me in the garden\nShe gave me a card in the garden\nIn the garden , she gave me a card\nI was given a card by her in the garden\nFigure 2: The \ufb01gure shows a 2-dimensional PCA projection of the LSTM hidden states that are obtained\nafter processing the phrases in the \ufb01gures. The phrases are clustered by meaning, which in these examples is\nprimarily a function of word order, which would be dif\ufb01cult to capture with a bag-of-words model. Notice that\nboth clusters have similar internal structure.\nOne of the attractive features of our model is its ability to turn a sequence of words into a vector\nof \ufb01xed dimensionality. Figure 2 visualizes some of the learned representations. The \ufb01gure clearly\nshows that the representations are sensitive to the order of words, while being fairly insensitive to the\n6\nType\nSentence\nOur model\nUlrich UNK , membre du conseil d\u2019 administration du constructeur automobile Audi ,\naf\ufb01rme qu\u2019 il s\u2019 agit d\u2019 une pratique courante depuis des ann\u00b4ees pour que les t\u00b4el\u00b4ephones\nportables puissent \u02c6etre collect\u00b4es avant les r\u00b4eunions du conseil d\u2019 administration a\ufb01n qu\u2019 ils\nne soient pas utilis\u00b4es comme appareils d\u2019 \u00b4ecoute `a distance .\nTruth\nUlrich Hackenberg , membre du conseil d\u2019 administration du constructeur automobile Audi ,\nd\u00b4eclare que la collecte des t\u00b4el\u00b4ephones portables avant les r\u00b4eunions du conseil , a\ufb01n qu\u2019 ils\nne puissent pas \u02c6etre utilis\u00b4es comme appareils d\u2019 \u00b4ecoute `a distance , est une pratique courante\ndepuis des ann\u00b4ees .\nOur model\n\u201c Les t\u00b4el\u00b4ephones cellulaires , qui sont vraiment une question , non seulement parce qu\u2019 ils\npourraient potentiellement causer des interf\u00b4erences avec les appareils de navigation , mais\nnous savons , selon la FCC , qu\u2019 ils pourraient interf\u00b4erer avec les tours de t\u00b4el\u00b4ephone cellulaire\nlorsqu\u2019 ils sont dans l\u2019 air \u201d , dit UNK .\nTruth\n\u201c Les t\u00b4el\u00b4ephones portables sont v\u00b4eritablement un probl`eme , non seulement parce qu\u2019 ils\npourraient \u00b4eventuellement cr\u00b4eer des interf\u00b4erences avec les instruments de navigation , mais\nparce que nous savons , d\u2019 apr`es la FCC , qu\u2019 ils pourraient perturber les antennes-relais de\nt\u00b4el\u00b4ephonie mobile s\u2019 ils sont utilis\u00b4es `a bord \u201d , a d\u00b4eclar\u00b4e Rosenker .\nOur model\nAvec la cr\u00b4emation , il y a un \u201c sentiment de violence contre le corps d\u2019 un \u02c6etre cher \u201d ,\nqui sera \u201c r\u00b4eduit `a une pile de cendres \u201d en tr`es peu de temps au lieu d\u2019 un processus de\nd\u00b4ecomposition \u201c qui accompagnera les \u00b4etapes du deuil \u201d .\nTruth\nIl y a , avec la cr\u00b4emation , \u201c une violence faite au corps aim\u00b4e \u201d ,\nqui va \u02c6etre \u201c r\u00b4eduit `a un tas de cendres \u201d en tr`es peu de temps , et non apr`es un processus de\nd\u00b4ecomposition , qui \u201c accompagnerait les phases du deuil \u201d .\nTable 3: A few examples of long translations produced by the LSTM alongside the ground truth\ntranslations. The reader can verify that the translations are sensible using Google translate.\n4 7 8\n12\n17\n22\n28\n35\n79\ntest sentences sorted by their length\n20\n25\n30\n35\n40\nBLEU score\nLSTM  (34.8)\nbaseline (33.3)\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\ntest sentences sorted by average word frequency rank\n20\n25\n30\n35\n40\nBLEU score\nLSTM  (34.8)\nbaseline (33.3)\nFigure 3: The left plot shows the performance of our system as a function of sentence length, where the\nx-axis corresponds to the test sentences sorted by their length and is marked by the actual sequence lengths.\nThere is no degradation on sentences with less than 35 words, there is only a minor degradation on the longest\nsentences. The right plot shows the LSTM\u2019s performance on sentences with progressively more rare words,\nwhere the x-axis corresponds to the test sentences sorted by their \u201caverage word frequency rank\u201d.\nreplacement of an active voice with a passive voice. The two-dimensional projections are obtained\nusing PCA.\n4\nRelated work\nThere is a large body of work on applications of neural networks to machine translation. So far,\nthe simplest and most effective way of applying an RNN-Language Model (RNNLM) [23] or a\n7\nFeedforward Neural Network Language Model (NNLM) [3] to an MT task is by rescoring the n-\nbest lists of a strong MT baseline [22], which reliably improves translation quality.\nMore recently, researchers have begun to look into ways of including information about the source\nlanguage into the NNLM. Examples of this work include Auli et al. [1], who combine an NNLM\nwith a topic model of the input sentence, which improves rescoring performance. Devlin et al. [8]\nfollowed a similar approach, but they incorporated their NNLM into the decoder of an MT system\nand used the decoder\u2019s alignment information to provide the NNLM with the most useful words in\nthe input sentence. Their approach was highly successful and it achieved large improvements over\ntheir baseline.\nOur work is closely related to Kalchbrenner and Blunsom [18], who were the \ufb01rst to map the input\nsentence into a vector and then back to a sentence, although they map sentences to vectors using\nconvolutional neural networks, which lose the ordering of the words. Similarly to this work, Cho et\nal. [5] used an LSTM-like RNN architecture to map sentences into vectors and back, although their\nprimary focus was on integrating their neural network into an SMT system. Bahdanau et al. [2] also\nattempted direct translations with a neural network that used an attention mechanism to overcome\nthe poor performance on long sentences experienced by Cho et al. [5] and achieved encouraging\nresults. Likewise, Pouget-Abadie et al. [26] attempted to address the memory problem of Cho et\nal. [5] by translating pieces of the source sentence in way that produces smooth translations, which\nis similar to a phrase-based approach. We suspect that they could achieve similar improvements by\nsimply training their networks on reversed source sentences.\nEnd-to-end training is also the focus of Hermann et al. [12], whose model represents the inputs and\noutputs by feedforward networks, and map them to similar points in space. However, their approach\ncannot generate translations directly: to get a translation, they need to do a look up for closest vector\nin the pre-computed database of sentences, or to rescore a sentence.\n5\nConclusion\nIn this work, we showed that a large deep LSTM, that has a limited vocabulary and that makes\nalmost no assumption about problem structure can outperform a standard SMT-based system whose\nvocabulary is unlimited on a large-scale MT task. The success of our simple LSTM-based approach\non MT suggests that it should do well on many other sequence learning problems, provided they\nhave enough training data.\nWe were surprised by the extent of the improvement obtained by reversing the words in the source\nsentences. We conclude that it is important to \ufb01nd a problem encoding that has the greatest number\nof short term dependencies, as they make the learning problem much simpler. In particular, while\nwe were unable to train a standard RNN on the non-reversed translation problem (shown in \ufb01g. 1),\nwe believe that a standard RNN should be easily trainable when the source sentences are reversed\n(although we did not verify it experimentally).\nWe were also surprised by the ability of the LSTM to correctly translate very long sentences. We\nwere initially convinced that the LSTM would fail on long sentences due to its limited memory,\nand other researchers reported poor performance on long sentences with a model similar to ours\n[5, 2, 26]. And yet, LSTMs trained on the reversed dataset had little dif\ufb01culty translating long\nsentences.\nMost importantly, we demonstrated that a simple, straightforward and a relatively unoptimized ap-\nproach can outperform an SMT system, so further work will likely lead to even greater translation\naccuracies. These results suggest that our approach will likely do well on other challenging sequence\nto sequence problems.\n6\nAcknowledgments\nWe thank Samy Bengio, Jeff Dean, Matthieu Devin, Geoffrey Hinton, Nal Kalchbrenner, Thang Luong, Wolf-\ngang Macherey, Rajat Monga, Vincent Vanhoucke, Peng Xu, Wojciech Zaremba, and the Google Brain team\nfor useful comments and discussions.\n8\nReferences\n[1] M. Auli, M. Galley, C. Quirk, and G. Zweig. Joint language and translation modeling with recurrent\nneural networks. In EMNLP, 2013.\n[2] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate.\narXiv preprint arXiv:1409.0473, 2014.\n[3] Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. A neural probabilistic language model. In Journal of\nMachine Learning Research, pages 1137\u20131155, 2003.\n[4] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is dif\ufb01cult.\nIEEE Transactions on Neural Networks, 5(2):157\u2013166, 1994.\n[5] K. Cho, B. Merrienboer, C. Gulcehre, F. Bougares, H. Schwenk, and Y. Bengio. Learning phrase represen-\ntations using RNN encoder-decoder for statistical machine translation. In Arxiv preprint arXiv:1406.1078,\n2014.\n[6] D. Ciresan, U. Meier, and J. Schmidhuber. Multi-column deep neural networks for image classi\ufb01cation.\nIn CVPR, 2012.\n[7] G. E. Dahl, D. Yu, L. Deng, and A. Acero. Context-dependent pre-trained deep neural networks for large\nvocabulary speech recognition. IEEE Transactions on Audio, Speech, and Language Processing - Special\nIssue on Deep Learning for Speech and Language Processing, 2012.\n[8] J. Devlin, R. Zbib, Z. Huang, T. Lamar, R. Schwartz, and J. Makhoul. Fast and robust neural network\njoint models for statistical machine translation. In ACL, 2014.\n[9] Nadir Durrani, Barry Haddow, Philipp Koehn, and Kenneth Hea\ufb01eld. Edinburgh\u2019s phrase-based machine\ntranslation systems for wmt-14. In WMT, 2014.\n[10] A. Graves. Generating sequences with recurrent neural networks. In Arxiv preprint arXiv:1308.0850,\n2013.\n[11] A. Graves, S. Fern\u00b4andez, F. Gomez, and J. Schmidhuber. Connectionist temporal classi\ufb01cation: labelling\nunsegmented sequence data with recurrent neural networks. In ICML, 2006.\n[12] K. M. Hermann and P. Blunsom. Multilingual distributed representations without word alignment. In\nICLR, 2014.\n[13] G. Hinton, L. Deng, D. Yu, G. Dahl, A. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen,\nT. Sainath, and B. Kingsbury. Deep neural networks for acoustic modeling in speech recognition. IEEE\nSignal Processing Magazine, 2012.\n[14] S. Hochreiter. Untersuchungen zu dynamischen neuronalen netzen. Master\u2019s thesis, Institut fur Infor-\nmatik, Technische Universitat, Munchen, 1991.\n[15] S. Hochreiter, Y. Bengio, P. Frasconi, and J. Schmidhuber. Gradient \ufb02ow in recurrent nets: the dif\ufb01culty\nof learning long-term dependencies, 2001.\n[16] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 1997.\n[17] S. Hochreiter and J. Schmidhuber. LSTM can solve hard long time lag problems. 1997.\n[18] N. Kalchbrenner and P. Blunsom. Recurrent continuous translation models. In EMNLP, 2013.\n[19] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet classi\ufb01cation with deep convolutional neural\nnetworks. In NIPS, 2012.\n[20] Q.V. Le, M.A. Ranzato, R. Monga, M. Devin, K. Chen, G.S. Corrado, J. Dean, and A.Y. Ng. Building\nhigh-level features using large scale unsupervised learning. In ICML, 2012.\n[21] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition.\nProceedings of the IEEE, 1998.\n[22] T. Mikolov. Statistical Language Models based on Neural Networks. PhD thesis, Brno University of\nTechnology, 2012.\n[23] T. Mikolov, M. Kara\ufb01\u00b4at, L. Burget, J. Cernock`y, and S. Khudanpur. Recurrent neural network based\nlanguage model. In INTERSPEECH, pages 1045\u20131048, 2010.\n[24] K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. BLEU: a method for automatic evaluation of machine\ntranslation. In ACL, 2002.\n[25] R. Pascanu, T. Mikolov, and Y. Bengio. On the dif\ufb01culty of training recurrent neural networks. arXiv\npreprint arXiv:1211.5063, 2012.\n[26] J. Pouget-Abadie, D. Bahdanau, B. van Merrienboer, K. Cho, and Y. Bengio.\nOvercoming the\ncurse of sentence length for neural machine translation using automatic segmentation. arXiv preprint\narXiv:1409.1257, 2014.\n[27] A. Razborov. On small depth threshold circuits. In Proc. 3rd Scandinavian Workshop on Algorithm\nTheory, 1992.\n[28] D. Rumelhart, G. E. Hinton, and R. J. Williams. Learning representations by back-propagating errors.\nNature, 323(6088):533\u2013536, 1986.\n[29] H. Schwenk. University le mans. http://www-lium.univ-lemans.fr/\u02dcschwenk/cslm_\njoint_paper/, 2014. [Online; accessed 03-September-2014].\n[30] M. Sundermeyer, R. Schluter, and H. Ney. LSTM neural networks for language modeling. In INTER-\nSPEECH, 2010.\n[31] P. Werbos. Backpropagation through time: what it does and how to do it. Proceedings of IEEE, 1990.\n9\n",
        "sentence": " (Graves, 2013; Graves & Jaitly, 2014; Sutskever et al., 2014; Kiros et al., 2014; Zaremba & Sutskever, 2014).",
        "context": "the latter was used only for rescoring hypotheses produced by a phrase-based system. Graves [10]\nintroduced a novel differentiable attention mechanism that allows neural networks to focus on dif-\nmatik, Technische Universitat, Munchen, 1991.\n[15] S. Hochreiter, Y. Bengio, P. Frasconi, and J. Schmidhuber. Gradient \ufb02ow in recurrent nets: the dif\ufb01culty\nof learning long-term dependencies, 2001.\nNature, 323(6088):533\u2013536, 1986.\n[29] H. Schwenk. University le mans. http://www-lium.univ-lemans.fr/\u02dcschwenk/cslm_\njoint_paper/, 2014. [Online; accessed 03-September-2014]."
    },
    {
        "title": "A point process framework for relating neural spiking activity to spiking history, neural ensemble, and extrinsic covariate effects",
        "author": [
            "Truccolo",
            "Wilson",
            "Eden",
            "Uri T",
            "Fellows",
            "Matthew R",
            "Donoghue",
            "John P",
            "Brown",
            "Emery N"
        ],
        "venue": "Journal of neurophysiology,",
        "citeRegEx": "Truccolo et al\\.,? \\Q2005\\E",
        "shortCiteRegEx": "Truccolo et al\\.",
        "year": 2005,
        "abstract": "Multiple factors simultaneously affect the spiking activity of individual neurons. Determining the effects and relative importance of these factors is a challenging problem in neurophysiology. We propose a statistical framework based on the point process likelihood function to relate a neuron's spiking probability to three typical covariates: the neuron's own spiking history, concurrent ensemble activity, and extrinsic covariates such as stimuli or behavior. The framework uses parametric models of the conditional intensity function to define a neuron's spiking probability in terms of the covariates. The discrete time likelihood function for point processes is used to carry out model fitting and model analysis. We show that, by modeling the logarithm of the conditional intensity function as a linear combination of functions of the covariates, the discrete time point process likelihood function is readily analyzed in the generalized linear model (GLM) framework. We illustrate our approach for both GLM and non-GLM likelihood functions using simulated data and multivariate single-unit activity data simultaneously recorded from the motor cortex of a monkey performing a visuomotor pursuit-tracking task. The point process framework provides a flexible, computationally efficient approach for maximum likelihood estimation, goodness-of-fit assessment, residual analysis, model selection, and neural decoding. The framework thus allows for the formulation and analysis of point process models of neural spiking activity that readily capture the simultaneous effects of multiple covariates and enables the assessment of their relative importance.",
        "full_text": "",
        "sentence": " When the time axis is discretized, the point process data can be converted to binary time series (or time series of count data if binning is coarse) and analyzed via time series analysis techniques (Truccolo et al., 2005; Bahadori et al., 2013; Ranganath et al., 2015).",
        "context": null
    },
    {
        "title": "Estimation of space\u2013time branching process models in seismology using an em\u2013type",
        "author": [
            "Veen",
            "Alejandro",
            "Schoenberg",
            "Frederic P"
        ],
        "venue": "algorithm. JASA,",
        "citeRegEx": "Veen et al\\.,? \\Q2008\\E",
        "shortCiteRegEx": "Veen et al\\.",
        "year": 2008,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Learning to execute",
        "author": [
            "Zaremba",
            "Wojciech",
            "Sutskever",
            "Ilya"
        ],
        "venue": "arXiv preprint arXiv:1410.4615,",
        "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Zaremba et al\\.",
        "year": 2014,
        "abstract": "Recurrent Neural Networks (RNNs) with Long Short-Term Memory units (LSTM) are\nwidely used because they are expressive and are easy to train. Our interest\nlies in empirically evaluating the expressiveness and the learnability of LSTMs\nin the sequence-to-sequence regime by training them to evaluate short computer\nprograms, a domain that has traditionally been seen as too complex for neural\nnetworks. We consider a simple class of programs that can be evaluated with a\nsingle left-to-right pass using constant memory. Our main result is that LSTMs\ncan learn to map the character-level representations of such programs to their\ncorrect outputs. Notably, it was necessary to use curriculum learning, and\nwhile conventional curriculum learning proved ineffective, we developed a new\nvariant of curriculum learning that improved our networks' performance in all\nexperimental conditions. The improved curriculum had a dramatic impact on an\naddition problem, making it possible to train an LSTM to add two 9-digit\nnumbers with 99% accuracy.",
        "full_text": "Under review as a conference paper at ICLR 2015\nLEARNING TO EXECUTE\nWojciech Zaremba\u2217\nNew York University\nwoj.zaremba@gmail.com\nIlya Sutskever\nGoogle\nilyasu@google.com\nABSTRACT\nRecurrent Neural Networks (RNNs) with Long Short-Term Memory units\n(LSTM) are widely used because they are expressive and are easy to train. Our\ninterest lies in empirically evaluating the expressiveness and the learnability of\nLSTMs in the sequence-to-sequence regime by training them to evaluate short\ncomputer programs, a domain that has traditionally been seen as too complex for\nneural networks. We consider a simple class of programs that can be evaluated\nwith a single left-to-right pass using constant memory. Our main result is that\nLSTMs can learn to map the character-level representations of such programs to\ntheir correct outputs. Notably, it was necessary to use curriculum learning, and\nwhile conventional curriculum learning proved ineffective, we developed a new\nvariant of curriculum learning that improved our networks\u2019 performance in all\nexperimental conditions. The improved curriculum had a dramatic impact on an\naddition problem, making it possible to train an LSTM to add two 9-digit numbers\nwith 99% accuracy.\n\u2217\n1\nINTRODUCTION\nExecution of computer programs requires dealing with a number of nontrivial concepts. To execute\na program, a system has to understand numerical operations, if-statements, variable assignments,\nthe compositionality of operations, and many more.\nWe show that Recurrent Neural Networks (RNN) with Long Short-Term Memory (LSTM) units\ncan accurately evaluate short simple programs in the sequence-to-sequence framework of Sutskever\net al. (2014). The LSTM reads the program character-by-character and computes the program\u2019s\noutput. We consider a constrained set of computer programs that can be evaluated in linear time\nand constant memory, because the LSTM reads the program only once and its memory capacity is\nlimited (Section 3).\nWe found it dif\ufb01cult to train LSTMs to execute computer programs, so we used curriculum learn-\ning to simplify the learning problem. We design a curriculum procedure which outperforms both\nconventional training that uses no curriculum learning (baseline) as well as the naive curriculum\nlearning of strategy of Bengio et al. (2009) (Section 4). We provide a plausible explanation for the\neffectiveness of our procedure relative to naive curriculum learning (Section 7).\nFinally, in addition to curriculum learning strategies, we examine two simple input transformations\nthat further simplify the sequence-to-sequence learning problem. We show that, in many cases,\nreversing the input sequence (Sutskever et al., 2014) and replicating the input sequence improves\nthe LSTM\u2019s performance on a memorization task (Section 3.2).\nThe code for replicating most of the experiments in this work can be found in https://github.\ncom/wojciechz/learning_to_execute.\n\u2217Work done while the author was in Google Brain.\n1\narXiv:1410.4615v3  [cs.NE]  19 Feb 2015\nUnder review as a conference paper at ICLR 2015\nInput:\nj=8584\nfor x in range(8):\nj+=920\nb=(1500+j)\nprint((b+7567))\nTarget: 25011.\nInput:\ni=8827\nc=(i-5347)\nprint((c+8704) if 2641<8500 else 5308)\nTarget: 12184.\nFigure 1: Example programs on which we train the LSTM. The output of each program is a single\ninteger. A \u201cdot\u201d symbol indicates the end of the integer, which has to be predicted by the LSTM.\n2\nRELATED WORK\nThere has been related research that used Tree Neural Networks (also known as Recursive Neu-\nral Networks) to evaluate symbolic mathematical expressions and logical formulas (Zaremba et al.,\n2014a; Bowman et al., 2014; Bowman, 2013), which is close in spirit to our work. Computer pro-\ngrams are more complex than mathematical or logical expressions because it is possible to simulate\neither with an appropriate computer program.\nFrom a methodological perspective, we formulate the program evaluation task as a sequence-\nto-sequence learning problem with a recurrent neural network (Sutskever et al., 2014) (see also\n(Mikolov, 2012; Sutskever, 2013; Pascanu et al., 2013)). Other interesting applications of recurrent\nneural networks include speech recognition (Robinson et al., 1996; Graves et al., 2013), machine\ntranslation (Cho et al., 2014; Sutskever et al., 2014), handwriting recognition (Pham et al., 2013;\nZaremba et al., 2014b), and many more.\nMaddison & Tarlow (2014) trained a language model of program text, and Mou et al. (2014) used a\nneural network to determine whether two programs are equivalent. Both of these approaches require\nthe parse trees of programs, while the input to our model is a string of character representing our\nprogram.\nPredicting program output requires that the model deals with long term dependencies that arise\nfrom variable assignment. For this reason, we chose to use the Long Short-Term Memory model\n(Hochreiter & Schmidhuber, 1997), although there are many other RNN variants that perform well\non tasks with long term dependencies (Cho et al., 2014; Jaeger et al., 2007; Koutn\u00b4\u0131k et al., 2014;\nMartens, 2010; Bengio et al., 2013).\nInitially, we found it dif\ufb01cult to train LSTMs to accurately evaluate programs. The compositional\nnature of computer programs suggests that the LSTM would learn faster if we \ufb01rst taught it about the\nindividual operators and how to combine them. This approach can be implemented with curriculum\nlearning (Bengio et al., 2009; Kumar et al., 2010; Lee & Grauman, 2011), which prescribes to grad-\nually increase the \u201cdif\ufb01culty level\u201d of the examples presented to the LSTM. It is partially motivated\nby fact that humans and animals learn much faster when they are given hard but manageable tasks.\nUnfortunately, we found the naive curriculum learning strategy of Bengio et al. (2009) to sometimes\nbe harmful. One of our key contributions is the formulation of a new curriculum learning strategy\nthat substantially improves the speed and the quality of training in every experimental setting that\nwe considered.\n3\nPROGRAM SUBCLASS\nWe train RNNs on the class of short programs that can be evaluated in O (n) time and constant\nmemory. This restriction is dictated by the computational structure of the RNN itself, as it can only\n2\nUnder review as a conference paper at ICLR 2015\nInput:\nvqppkn\nsqdvfljmnc\ny2vxdddsepnimcbvubkomhrpliibtwztbljipcc\nTarget: hkhpg\nFigure 2: A sample program with its outputs when the characters are scrambled. It helps illustrate\nthe dif\ufb01culty faced by our neural network.\nperform a single pass over the program and its memory is limited. Our programs use the Python\nsyntax and are constructed from a small number of operations and their compositions (nesting).\nWe allow the following operations: addition, subtraction, multiplication, variable assignments, if-\nstatements, and for-loops, but we forbid double loops. Every program ends with a single \u201cprint\u201d\nstatement whose output is an integer. Two example programs are shown in Figure 1.\nWe select our programs from a family of distributions parametrized by their length and nesting. The\nlength parameter is the number of digits in the integers that appear in the programs (so the integers\nare chosen uniformly from [1, 10length]). The appendix presents the pseudocode 1 of the algorithm\nused to generate our programs. For example, two programs that are generated with length = 4 and\nnesting = 3 are shown in Figure 1.\nWe impose restrictions on the operands of multiplication and on the ranges of for-loop, since they\npose a greater dif\ufb01culty to our model. We constrain one of the arguments of multiplication and the\nrange of for-loops to be chosen uniformly from the much smaller range [1, 4\u00b7 length]. We do so since\nour models are able to perform linear-time computation while generic integer multiplication requires\nsuperlinear time. Similar considerations apply to for-loops, since nested for-loops can implement\ninteger multiplication.\nThe nesting parameter is the number of times we are allowed to combine the operations with each\nother. Higher values of nesting yield programs with deeper parse trees. Nesting makes the task much\nharder for the LSTMs, because they do not have a natural way of dealing with compositionality,\nunlike Tree Neural Networks. It is surprising that the LSTMs can handle nested expressions at all.\nThe programs also do not receive an external input.\nIt is important to emphasize that the LSTM reads the entire input one character at a time and pro-\nduces the output one character at a time. The characters are initially meaningless from the model\u2019s\nperspective; for instance, the model does not know that \u201c+\u201d means addition or that 6 is followed\nby 7. In fact, scrambling the input characters (e.g., replacing \u201ca\u201d with \u201cq\u201d, \u201cb\u201d with \u201cw\u201d, etc.,) has\nno effect on the model\u2019s ability to solve this problem. We demonstrate the dif\ufb01culty of the task by\npresenting an input-output example with scrambled characters in Figure 2.\nFinally, we wanted to verify that our program are not trivial to evaluate, by ensuring that the bias\ncoming from Benford\u2019s law (Hill, 1995) is not too strong. Our setup has 12 possible output char-\nacters, that is 10 digits, the end of sequence character, and minus. Their output distribution is not\nuniform, which can be seen by noticing that the minus sign and the dot do not occur with the same\nfrequency as the other digits. If we assume that the output characters are independent, the probabil-\nity of guessing the correct character is \u223c8.3%. The most common character is 1 which occurs with\nprobability 12.7% over the entire output.\nHowever, there is a bias in the distribution of the \ufb01rst character. There are 11 possible choices, which\ncan be randomly guessed with a probability of 9%. The most common character is 1, and it occurs\nwith a probability 20.3% in its \ufb01rst position, indicating a strong bias. Still, this value is far below\nour model prediction accuracy. Moreover, the most probable second character in the \ufb01rst position of\nthe output occurs with probability 12.6%, which is indistinguishable from probability distribution\nof digits in the other positions. The last character is always the end of sequence. The most common\ndigit prior to the last character is 4, and it occures with probability 10.3%. These statistics are\ncomputed with 10000 randomly generated programs with length = 4 and nesting = 1. The\nabsence of a strong bias for this con\ufb01guration suggests that there will be even less bias in with\ngreater nesting and longer digits, which we have also con\ufb01rmed numerically.\n3\nUnder review as a conference paper at ICLR 2015\nInput:\nprint(398345+425098)\nTarget: 823443\nFigure 3: A typical data sample for the addition task.\n3.1\nADDITION TASK\nIt is dif\ufb01cult to intuitively assess the accuracy of an LSTM on a program evaluation task. For\nexample, it is not clear whether an accuracy of 50% is impressive. Thus, we also evaluate our models\non a more familiar addition task, where the dif\ufb01culty is measured by the length of the inputs. We\nconsider the addition of only two numbers of the same length (Figure 3) that are chosen uniformly\nfrom [1, 10length]. Adding two number of the same length is simpler than adding variable length\nnumbers. Model doesn\u2019t need to align them.\n3.2\nMEMORIZATION TASK\nIn addition to program evaluation and addition, we also investigate the task of memorizing a random\nsequence of numbers. Given an example input 123456789, the LSTM reads it one character at a\ntime, stores it in memory, and then outputs 123456789 one character at a time. We present and\nexplore two simple performance enhancing techniques: input reversing Sutskever et al. (2014) and\ninput doubling.\nThe idea of input reversing is to reverse the order of the input (987654321) while keeping the de-\nsired output unchanged (123456789). It may appear to be a neutral operation because the average\ndistance between each input and its corresponding target does not change. However, input reversing\nintroduces many short term dependencies that make it easier for the LSTM to learn to make correct\npredictions. This strategy was \ufb01rst introduced by Sutskever et al. (2014).\nThe second performance enhancing technique is input doubling, where we present the input se-\nquence twice (so the example input becomes 123456789; 123456789), while the output remains\nunchanged (123456789). This method is meaningless from a probabilistic perspective as RNNs ap-\nproximate the conditional distribution p(y|x), yet here we attempt to learn p(y|x, x). Still, it gives\nnoticeable performance improvements. By processing the input several times before producing the\noutput, the LSTM is given the opportunity to correct any mistakes or omissions it made before.\n4\nCURRICULUM LEARNING\nOur program generation procedure is parametrized by length and nesting. These two parameters\nallow us control the complexity of the program. When length and nesting are large enough, the\nlearning problem becomes nearly intractable. This indicates that in order to learn to evaluate pro-\ngrams of a given length = a and nesting = b, it may help to \ufb01rst learn to evaluate programs with\nlength \u226aa and nesting \u226ab. We evaluate the following curriculum learning strategies:\nNo curriculum learning (baseline) The baseline approach does not use curriculum learning. This\nmeans that we generate all the training samples with length = a and nesting = b. This strategy is the\nmost \u201csound\u201d from statistical perspective, since it is generally recommended to make the training\ndistribution identical to test distribution.\nNaive curriculum strategy (naive) We begin with length = 1 and nesting = 1. Once learning\nstops making progress on the validation set, we increase length by 1. We repeat this process until\nits length reaches a, in which case we increase nesting by one and reset length to 1. We can also\nchoose to \ufb01rst increase nesting and then length. However, it does not make a noticeable difference in\nperformance. We skip this option in the rest of paper, and increase length \ufb01rst in all our experiments.\nThis strategy is has been examined in previous work on curriculum learning (Bengio et al., 2009).\nHowever, we show that sometimes it gives even worse performance than baseline.\nMixed strategy (mix) To generate a random sample, we \ufb01rst pick a random length from [1, a] and\na random nesting from [1, b] independently for every sample. The Mixed strategy uses a balanced\n4\nUnder review as a conference paper at ICLR 2015\nmixture of easy and dif\ufb01cult examples, so at every point during training, a sizable fraction of the\ntraining samples will have the appropriate dif\ufb01culty for the LSTM.\nCombining the mixed strategy with naive curriculum strategy (combined) This strategy com-\nbines the mix strategy with the naive strategy. In this approach, every training case is obtained either\nby the naive strategy or by the mix strategy. As a result, the combined strategy always exposes the\nnetwork at least to some dif\ufb01cult examples, which is the key way in which it differs from the naive\ncurriculum strategy. We noticed that it always outperformed the naive strategy and would generally\n(but not always) outperform the mix strategy. We explain why our new curriculum learning strategies\noutperform the naive curriculum strategy in Section 7.\nWe evaluate these four strategies on the program evaluation task (Section 6.1) and on the memoriza-\ntion task (Section 6.3).\n5\nLSTM\nIn this section we brie\ufb02y describe the deep LSTM (Section 5). All vectors are n-dimensional unless\nexplicitly stated otherwise. Let hl\nt \u2208Rn be a hidden state in layer l in timestep t. Let Tn,m : Rn \u2192\nRm be a biased linear mapping (x \u2192Wx + b for some W and b). We let \u2299be element-wise\nmultiplication and let h0\nt be the input to the deep LSTM at timestep t. We use the activations at the\ntop layer L (namely hL\nt ) to predict yt where L is the depth of our LSTM.\nThe structure of the LSTM allows it to train on problems with long term dependencies relatively\neasily. The \u201clong term\u201d memory is stored in a vector of memory cells cl\nt \u2208Rn. Although many\nLSTM architectures differ slightly in their connectivity structure and activation functions, all LSTM\narchitectures have additive memory cells that make it easy to learn to store information for long\nperiods of time. We used an LSTM described by the following equations (from Graves et al. (2013)):\nLSTM : hl\u22121\nt\n, hl\nt\u22121, cl\nt\u22121 \u2192hl\nt, cl\nt\n\uf8eb\n\uf8ec\n\uf8ed\ni\nf\no\ng\n\uf8f6\n\uf8f7\n\uf8f8=\n\uf8eb\n\uf8ec\n\uf8ed\nsigm\nsigm\nsigm\ntanh\n\uf8f6\n\uf8f7\n\uf8f8T2n,4n\n\u0012\nhl\u22121\nt\nhl\nt\u22121\n\u0013\ncl\nt = f \u2299cl\nt\u22121 + i \u2299g\nhl\nt = o \u2299tanh(cl\nt)\n6\nEXPERIMENTS\nIn this section, we report the results of our curriculum learning strategies on the program evaluation\nand memorization tasks. In both experiments, we used the same LSTM architecture.\nOur LSTM has two layers and is unrolled for 50 steps in both experiments. It has 400 cells per layer\nand its parameters are initialized uniformly in [\u22120.08, 0.08]. This gives total \u223c2.5M parameters.\nWe initialize the hidden states to zero. We then use the \ufb01nal hidden states of the current minibatch\nas the initial hidden state of the subsequent minibatch. Thus it is possible that a program and its\noutput could be separated across different minibatches. The size of minibatch is 100. We constrain\nthe norm of the gradients (normalized by minibatch size) to be no greater than 5 (Mikolov et al.,\n2010). We keep the learning rate equal to 0.5 until we reach the target length and nesting (we only\nvary the length, i.e., the number of digits, in the memorization task).\nAfter reaching the target accuracy (95%) we decrease the learning rate by 0.8. We keep the learning\nrate on the same level until there is no improvement on the training set. We decrease it again, when\nthere is no improvement on training set. The only difference between experiments is the termination\ncriteria. For the program output prediction, we stop when learning rate becomes smaller than 0.001.\nFor copying task, we stop training after 20 epochs, where each epoch has 0.5M samples.\nWe begin training with length = 1 and nesting = 1 (or length=1 for the memorization task). We\nensure that the training, validation, and test sets are disjoint. It is achieved computing the hash value\nof each sample and taking it modulo 3.\nImportant note on error rates: We use teacher forcing when we compute the accuracy of our\nLSTMs. That is, when predicting the i-th digit of the target, the LSTM is provided with the correct\n5\nUnder review as a conference paper at ICLR 2015\n\ufb01rst i \u22121 digits of the target. This is different from using the LSTM to generate the entire output\non its own, as done by Sutskever et al. (2014), which would almost surely result in lower numerical\naccuracies. To help make intuitive sense of our results, we present a large number of test cases and\nthe outputs computed by the LSTM, albeit with teacher forcing.\n6.1\nRESULTS ON PROGRAM EVALUATION\nWe train our LSTMs using the four strategies described in Section 4:\n\u2022 No curriculum learning (baseline),\n\u2022 Naive curriculum strategy (naive)\n\u2022 Mixed strategy (mix), and\n\u2022 Combined strategy (combined).\nFigure 4 shows the absolute performance of the baseline strategy (training on the original target\ndistribution), and of the best performing strategy, combined. Moreover, Figure 5 shows the perfor-\nmance of the three curriculum strategies relative to baseline. Finally, we provide several example\npredictions on test data in the supplementary materials. The accuracy of a random predictor would\nbe \u223c8.3%, since there are 12 possible output symbols.\nFigure 4: Absolute prediction accuracy of the baseline strategy and of the combined strategy (see\nSection 4) on the program evaluation task. Deeper nesting and longer integers make the task more\ndif\ufb01cult. Overall, the combined strategy outperformed the baseline strategy in every setting.\nFigure 5: Relative prediction accuracy of the different strategies with respect to the baseline strategy.\nThe Naive curriculum strategy was found to sometime perform worse than baseline. A possible\nexplanation is provided in Section 7. The combined strategy outperforms all other strategies in\nevery con\ufb01guration on program evaluation.\n6.2\nRESULTS ON THE ADDITION TASK\nFigure 6 presents the accuracy achieved by the LSTM with the various curriculum strategies on\nthe addition task. Remarkably, the combined curriculum strategy resulted in 99% accuracy on the\naddition of 9-digit long numbers, which is a massive improvement over the naive curriculum.\n6.3\nRESULTS ON THE MEMORIZATION TASK\nRecall that the goal of the memorization task is to read a sequence of digits into the hidden state and\nthen to reconstruct it from the hidden state. Namely, given an input such as 123456789, the goal is\n6\nUnder review as a conference paper at ICLR 2015\nFigure 6: The effect of curriculum strategies on the addition task.\nFigure 7: Prediction accuracy on the memorization task for the four curriculum strategies. The input\nlength ranges from 5 to 65 digits. Every strategy is evaluated with the following 4 input modi\ufb01cation\nschemes: no modi\ufb01cation; input inversion; input doubling; and input doubling and inversion. The\ntraining time was not limited; the network was trained till convergence.\nto produce the output 123456789. The model processes the input one input character at the time and\nhas to reconstruct the output only after loading the entire input into its memory. This task provides\ninsight into the LSTM\u2019s ability to learn to remember. We have evaluated our model on sequences\nof lengths ranging from 5 to 65. We use the four curriculum strategies of Section 4. In addition, we\ninvestigate two strategies to modify the input which increase performance:\n\u2022 Inverting input (Sutskever et al., 2014)\n\u2022 Doubling Input\nBoth strategies are described in Section 3.2. Figure 7 shows the absolute performance of the baseline\nstrategy and of the combined strategy. This Figure shows the performance at convergence. We\nfurther present in Supplementary material (Section 9) results after 20 epochs (Figure 8).\nFor this task, the combined strategy no longer outperforms the mixed strategy in every experimental\nsetting, although both strategies are always better than using no curriculum and the naive curriculum\nstrategy. Each graph contains 4 settings, which correspond to the possible combinations of input in-\nversion and input doubling. The result clearly shows that the simultaneously doubling and reversing\nthe input achieves the best results. Random guessing would achieve an accuracy of \u223c9%, since\nthere are 11 possible output symbols.\n7\nHIDDEN STATE ALLOCATION HYPOTHESIS\nOur experimental results suggest that a proper curriculum learning strategy is critical for achieving\ngood performance on very hard problems where conventional stochastic gradient descent (SGD)\n7\nUnder review as a conference paper at ICLR 2015\nperforms poorly. The results on both of our problems (Sections 6.3 and 6.1) show that the combined\nstrategy is better than all other curriculum strategies, including both naive curriculum learning, and\ntraining on the target distribution. We have a plausible explanation for why this is the case.\nIt seems natural to train models with examples of increasing dif\ufb01culty. This way the models have\na chance to learn the correct intermediate concepts, and then utilize them for the more dif\ufb01cult\nproblem instances. Otherwise, learning the full task might be just too dif\ufb01cult for SGD from a\nrandom initialization. This explanation has been proposed in previous work on curriculum learning\nBengio et al. (2009). However, based the on empirical results, the naive strategy of curriculum\nlearning can sometimes be worse than learning with the target distribution.\nIn our tasks, the neural network has to perform a lot of memorization. The easier examples usually\nrequire less memorization than the hard examples. For instance, in order to add two 5-digit numbers,\none has to remember at least 5 digits before producing any output. The best way to accurately\nmemorize 5 numbers could be to spread them over the entire hidden state / memory cell (i.e., use\na distributed representation). Indeed, the network has no incentive to utilize only a fraction of\nits state, and it is always better to make use of its entire memory capacity. This implies that the\nharder examples would require a restructuring of its memory patterns. It would need to contract its\nrepresentations of 5 digit numbers in order to free space for the 6-th number. This process of memory\npattern restructuring might be dif\ufb01cult to implement, so it could be the reason for the sometimes poor\nperformance of the naive curriculum learning strategy relative to baseline.\nThe combined strategy reduces the need to restructure the memory patterns. The combined strategy\nis a combination of the naive curriculum strategy and of the mix strategy, which is a mixture of ex-\namples of all dif\ufb01culties. The examples produced by the naive curriculum strategy help to learn the\nintermediate input-output mapping, which is useful for solving the target task, while the extra sam-\nples from the mix strategy prevent the network from utilizing all the memory on the easy examples,\nthus eliminating the need to restructure its memory patterns.\n8\nCRITIQUE\nPerfect prediction of program output requires a complete understanding of all operands and con-\ncepts, and of the precise way in which they are combined. However, imperfect prediction might be\nachieved in a multitude of ways, and could heavily rely on memorization, without a genuine un-\nderstanding of the underlying concepts. For instance, perfect addition is relatively intricate, as the\nLSTM needs to know the order of numbers and to correctly compute the carry.\nThere are many alternatives to the addition algorithm if perfect output is not required. For instance,\none can perform element-wise addition, and as long as there is no carry then the output would be\nperfectly correct. Another alternative, which requires more memory, but is also more simpler, is to\nmemorize all results of addition for 2 digit numbers. Then multi-digit addition can be broken down\nto multiple 2-digits additions element-wise. Once again, such an algorithm would have a reasonably\nhigh prediction accuracy, although it would be far from correct.\nWe do not know how heavily our model relies on memorization and how far the learned algorithm\nis from the actual, correct algorithm. This could be tested by creating a big discrepancy between the\ntraining and test data, but in this work, the training and the test distributions are the same. We plan\nto examine how well our models would generalize on very different new examples in future work.\n9\nDISCUSSION\nWe have shown that it is possible to learn to evaluate programs with limited prior knowledge. This\nwork demonstrate the power and expressiveness of sequence-to-sequence LSTMs. We also showed\nthat correct curriculum learning is crucial for achieving good results on very dif\ufb01cult tasks that\ncannot be optimized with standard SGD. We also found that the general method of doubling the\ninput reliably improves the performance of sequence-to-sequence LSTMs.\nOur results are encouraging but they leave many questions open. For example, we are not able to\nevaluate arbitrary programs (e.g., ones that run in more than O (n) time). This cannot be achieved\nwith conventional RNNs or LSTMs due to their runtime restrictions. We also do not know the\n8\nUnder review as a conference paper at ICLR 2015\noptimal curriculum learning strategy. To understand it, it may be necessary to identify the training\nsamples that are most bene\ufb01cial to the model.\n10\nACKNOWLEDGMENTS\nWe wish to thank Oriol Vinyals for useful discussions, and to Koray Kavukcuoglu for help during code develop-\nment. Moreover, we wish to acknowledge Marc\u2019Aurelio Ranzato for useful comments on the \ufb01rst version of the\npaper. Some chunks of our code origin from Google Deepmind repository. We thank to unknown developers\nof LSTM function, and auxiliary functions.\nREFERENCES\nBengio, Yoshua, Louradour, J\u00b4er\u02c6ome, Collobert, Ronan, and Weston, Jason. Curriculum learning. In Proceed-\nings of the 26th annual international conference on machine learning, pp. 41\u201348. ACM, 2009.\nBengio, Yoshua, Boulanger-Lewandowski, Nicolas, and Pascanu, Razvan. Advances in optimizing recurrent\nnetworks. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on,\npp. 8624\u20138628. IEEE, 2013.\nBowman, Samuel R.\nCan recursive neural tensor networks learn logical reasoning?\narXiv preprint\narXiv:1312.6192, 2013.\nBowman, Samuel R, Potts, Christopher, and Manning, Christopher D. Recursive neural networks for learning\nlogical semantics. arXiv preprint arXiv:1406.1827, 2014.\nCho, Kyunghyun, van Merrienboer, Bart, Gulcehre, Caglar, Bougares, Fethi, Schwenk, Holger, and Bengio,\nYoshua. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv\npreprint arXiv:1406.1078, 2014.\nGraves, Alex, Mohamed, Abdel-rahman, and Hinton, Geoffrey. Speech recognition with deep recurrent neural\nnetworks. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on,\npp. 6645\u20136649. IEEE, 2013.\nHill, Theodore P. A statistical derivation of the signi\ufb01cant-digit law. Statistical Science, pp. 354\u2013363, 1995.\nHochreiter, Sepp and Schmidhuber, J\u00a8urgen. Long short-term memory. Neural computation, 9(8):1735\u20131780,\n1997.\nJaeger, Herbert, Luko\u02c7sevi\u02c7cius, Mantas, Popovici, Dan, and Siewert, Udo. Optimization and applications of\necho state networks with leaky-integrator neurons. Neural Networks, 20(3):335\u2013352, 2007.\nKoutn\u00b4\u0131k, Jan, Greff, Klaus, Gomez, Faustino, and Schmidhuber, J\u00a8urgen. A clockwork rnn. arXiv preprint\narXiv:1402.3511, 2014.\nKumar, M Pawan, Packer, Benjamin, and Koller, Daphne. Self-paced learning for latent variable models. In\nAdvances in Neural Information Processing Systems, pp. 1189\u20131197, 2010.\nLee, Yong Jae and Grauman, Kristen. Learning the easy things \ufb01rst: Self-paced visual category discovery. In\nComputer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pp. 1721\u20131728. IEEE, 2011.\nMaddison, Chris J and Tarlow, Daniel. Structured generative models of natural source code. arXiv preprint\narXiv:1401.0514, 2014.\nMartens, James. Deep learning via hessian-free optimization. In Proceedings of the 27th International Confer-\nence on Machine Learning (ICML-10), pp. 735\u2013742, 2010.\nMikolov, Tom\u00b4a\u02c7s. Statistical language models based on neural networks. PhD thesis, Ph. D. thesis, Brno\nUniversity of Technology, 2012.\nMikolov, Tomas, Kara\ufb01\u00b4at, Martin, Burget, Lukas, Cernock`y, Jan, and Khudanpur, Sanjeev. Recurrent neural\nnetwork based language model. In INTERSPEECH, pp. 1045\u20131048, 2010.\nMou, Lili, Li, Ge, Liu, Yuxuan, Peng, Hao, Jin, Zhi, Xu, Yan, and Zhang, Lu. Building program vector\nrepresentations for deep learning. arXiv preprint arXiv:1409.3358, 2014.\nPascanu, Razvan, Gulcehre, Caglar, Cho, Kyunghyun, and Bengio, Yoshua. How to construct deep recurrent\nneural networks. arXiv preprint arXiv:1312.6026, 2013.\nPham, Vu, Kermorvant, Christopher, and Louradour, J\u00b4er\u02c6ome. Dropout improves recurrent neural networks for\nhandwriting recognition. arXiv preprint arXiv:1312.4569, 2013.\nRobinson, Tony, Hochberg, Mike, and Renals, Steve. The use of recurrent neural networks in continuous speech\nrecognition. In Automatic speech and speaker recognition, pp. 233\u2013258. Springer, 1996.\nSutskever, Ilya. Training Recurrent Neural Networks. PhD thesis, University of Toronto, 2013.\nSutskever, Ilya, Vinyals, Oriol, and Le, Quoc V. Sequence to sequence learning with neural networks. arXiv\npreprint arXiv:1409.3215, 2014.\nZaremba, Wojciech, Kurach, Karol, and Fergus, Rob. Learning to discover ef\ufb01cient mathematical identities.\narXiv preprint arXiv:1406.1584, 2014a.\nZaremba, Wojciech, Sutskever, Ilya, and Vinyals, Oriol.\nRecurrent neural network regularization.\narXiv\npreprint arXiv:1409.2329, 2014b.\n9\nUnder review as a conference paper at ICLR 2015\nSUPPLEMENTARY MATERIAL\nInput:\nlength, nesting\nstack = EmptyStack()\nOperations = Addition, Subtraction, Multiplication, If-Statement,\nFor-Loop, Variable Assignment\nfor i = 1 to nesting do\nOperation = a random operation from Operations\nValues = List\nCode = List\nfor params in Operation.params do\nif not empty stack and Uniform(1) > 0.5 then\nvalue, code = stack.pop()\nelse\nvalue = random.int(10length)\ncode = toString(value)\nend if\nvalues.append(value)\ncode.append(code)\nend for\nnew value= Operation.evaluate(values)\nnew code = Operation.generate code(codes)\nstack.push((new value, new code))\nend for\nfinal value, final code = stack.pop()\ndatasets = training, validation, testing\nidx = hash(final code) modulo 3\ndatasets[idx].add((final value, final code))\nAlgorithm 1: Pseudocode of the algorithm used to generate the distribution over the python pro-\ngram. Programs produced by this algorithm are guaranteed to never have dead code. The type of the\nsample (train, test, or validation) is determined by its hash modulo 3.\n11\nADDITIONAL RESULTS ON THE MEMORIZATION PROBLEM\nWe present the algorithm for generating the training cases, and present an extensive qualitative evaluation of\nthe samples and the kinds of predictions made by the trained LSTMs.\nWe emphasize that these predictions rely on teacher forcing. That is, even if the LSTM made an incorrect\nprediction in the i-th output digit, the LSTM will be provided as input the correct i-th output digit for predicting\nthe i + 1-th digit. While teacher forcing has no effect whenever the LSTM makes no errors at all, a sample that\nmakes an early error and gets the remainder of the digits correctly needs to be interpreted with care.\n12\nQUALITATIVE EVALUATION OF THE CURRICULUM STRATEGIES\n12.1\nEXAMPLES OF PROGRAM EVALUATION PREDICTION. LENGTH = 4, NESTING = 1\nInput:\nprint(6652).\nTarget:\n6652.\n\u201dBaseline\u201d prediction:\n6652.\n\u201dNaive\u201d prediction:\n6652.\n\u201dMix\u201d prediction:\n6652.\n\u201dCombined\u201d prediction:\n6652.\nInput:\n10\nUnder review as a conference paper at ICLR 2015\nFigure 8: Prediction accuracy on the memorization task for the four curriculum strategies. The input\nlength ranges from 5 to 65 digits. Every strategy is evaluated with the following 4 input modi\ufb01cation\nschemes: no modi\ufb01cation; input inversion; input doubling; and input doubling and inversion. The\ntraining time is limited to 20 epochs.\nprint((5997-738)).\nTarget:\n5259.\n\u201dBaseline\u201d prediction:\n5101.\n\u201dNaive\u201d prediction:\n5101.\n\u201dMix\u201d prediction:\n5249.\n\u201dCombined\u201d prediction:\n5229.\nInput:\nprint((16*3071)).\nTarget:\n49136.\n\u201dBaseline\u201d prediction:\n49336.\n\u201dNaive\u201d prediction:\n48676.\n\u201dMix\u201d prediction:\n57026.\n\u201dCombined\u201d prediction:\n49626.\nInput:\nc=2060;\nprint((c-4387)).\nTarget:\n-2327.\n\u201dBaseline\u201d prediction:\n-2320.\n\u201dNaive\u201d prediction:\n-2201.\n\u201dMix\u201d prediction:\n-2377.\n\u201dCombined\u201d prediction:\n-2317.\nInput:\nprint((2*5172)).\n11\nUnder review as a conference paper at ICLR 2015\nTarget:\n10344.\n\u201dBaseline\u201d prediction:\n10344.\n\u201dNaive\u201d prediction:\n10324.\n\u201dMix\u201d prediction:\n10344.\n\u201dCombined\u201d prediction:\n10344.\nInput:\nprint((9891-4715)).\nTarget:\n5176.\n\u201dBaseline\u201d prediction:\n5196.\n\u201dNaive\u201d prediction:\n5104.\n\u201dMix\u201d prediction:\n4246.\n\u201dCombined\u201d prediction:\n5196.\nInput:\nprint(4849).\nTarget:\n4849.\n\u201dBaseline\u201d prediction:\n4849.\n\u201dNaive\u201d prediction:\n4849.\n\u201dMix\u201d prediction:\n4849.\n\u201dCombined\u201d prediction:\n4849.\nInput:\nprint((4*7054)).\nTarget:\n28216.\n\u201dBaseline\u201d prediction:\n28216.\n\u201dNaive\u201d prediction:\n28116.\n\u201dMix\u201d prediction:\n28216.\n\u201dCombined\u201d prediction:\n28216.\nInput:\nprint((4635-5257)).\nTarget:\n-622.\n\u201dBaseline\u201d prediction:\n-688.\n\u201dNaive\u201d prediction:\n-628.\n\u201dMix\u201d prediction:\n-692.\n\u201dCombined\u201d prediction:\n-632.\nInput:\ne=1079\nfor x in range(10):e+=4729\nprint(e).\nTarget:\n48369.\n\u201dBaseline\u201d prediction:\n48017.\n\u201dNaive\u201d prediction:\n48011.\n\u201dMix\u201d prediction:\n48101.\n\u201dCombined\u201d prediction:\n48009.\n12.2\nEXAMPLES OF PROGRAM EVALUATION PREDICTION. LENGTH = 4, NESTING = 2\nInput:\n12\nUnder review as a conference paper at ICLR 2015\ne=6653\nfor x in range(14):e+=6311\nprint(e).\nTarget:\n95007.\n\u201dBaseline\u201d prediction:\n94093.\n\u201dNaive\u201d prediction:\n90013.\n\u201dMix\u201d prediction:\n95015.\n\u201dCombined\u201d prediction:\n94103.\nInput:\ni=6404;\nprint((i+8074)).\nTarget:\n14478.\n\u201dBaseline\u201d prediction:\n14498.\n\u201dNaive\u201d prediction:\n14444.\n\u201dMix\u201d prediction:\n14482.\n\u201dCombined\u201d prediction:\n14478.\nInput:\nprint((8*(5051-648))).\nTarget:\n35224.\n\u201dBaseline\u201d prediction:\n34044.\n\u201dNaive\u201d prediction:\n32180.\n\u201dMix\u201d prediction:\n33284.\n\u201dCombined\u201d prediction:\n33004.\nInput:\nh=(3681 if 9279<3033 else 6191)\nfor x in range(7):h-=9910\nprint(h).\nTarget:\n-63179.\n\u201dBaseline\u201d prediction:\n-62049.\n\u201dNaive\u201d prediction:\n-63117.\n\u201dMix\u201d prediction:\n-62013.\n\u201dCombined\u201d prediction:\n-62009.\nInput:\nprint(((3210+2472)+1477)).\nTarget:\n7159.\n\u201dBaseline\u201d prediction:\n7009.\n\u201dNaive\u201d prediction:\n7019.\n\u201dMix\u201d prediction:\n7995.\n\u201dCombined\u201d prediction:\n7079.\nInput:\nb=8494\nfor x in range(2):b+=7484\nprint((b*14)).\nTarget:\n328468.\n\u201dBaseline\u201d prediction:\n318004.\n\u201dNaive\u201d prediction:\n338088.\n\u201dMix\u201d prediction:\n329220.\n\u201dCombined\u201d prediction:\n338080.\n13\nUnder review as a conference paper at ICLR 2015\nInput:\nj=6447;\nprint((12*(j-4689))).\nTarget:\n21096.\n\u201dBaseline\u201d prediction:\n21266.\n\u201dNaive\u201d prediction:\n10046.\n\u201dMix\u201d prediction:\n10606.\n\u201dCombined\u201d prediction:\n20402.\nInput:\nprint((13*9201)).\nTarget:\n119613.\n\u201dBaseline\u201d prediction:\n118313.\n\u201dNaive\u201d prediction:\n118011.\n\u201dMix\u201d prediction:\n117669.\n\u201dCombined\u201d prediction:\n119533.\nInput:\ng=1054;\nprint((6028+(g-1953))).\nTarget:\n5129.\n\u201dBaseline\u201d prediction:\n4013.\n\u201dNaive\u201d prediction:\n5035.\n\u201dMix\u201d prediction:\n4015.\n\u201dCombined\u201d prediction:\n4009.\nInput:\nd=6817\nfor x in range(7):d-=(4581-2186)\nprint(d).\nTarget:\n-9948.\n\u201dBaseline\u201d prediction:\n-1996.\n\u201dNaive\u201d prediction:\n-1610.\n\u201dMix\u201d prediction:\n-1882.\n\u201dCombined\u201d prediction:\n-1980.\n12.3\nEXAMPLES OF PROGRAM EVALUATION PREDICTION. LENGTH = 4, NESTING = 3\nInput:\nf=4692\nfor x in range(4):f-=1664\nj=1443\nfor x in range(8):j+=f\nd=j\nfor x in range(11):d-=4699\nprint(d).\nTarget:\n-65958.\n\u201dBaseline\u201d prediction:\n-13262.\n\u201dNaive\u201d prediction:\n-73194.\n\u201dMix\u201d prediction:\n-40188.\n\u201dCombined\u201d prediction:\n-12004.\n14\nUnder review as a conference paper at ICLR 2015\nInput:\nb=9930\nfor x in range(11):b-=4369\ng=b;\nprint(((g-8043)+9955)).\nTarget:\n-36217.\n\u201dBaseline\u201d prediction:\n-37515.\n\u201dNaive\u201d prediction:\n-38609.\n\u201dMix\u201d prediction:\n-35893.\n\u201dCombined\u201d prediction:\n-35055.\nInput:\nd=5446\nfor x in range(8):d+=(2678 if 4803<2829 else 9848)\nprint((d if 5935<4845 else 3043)).\nTarget:\n3043.\n\u201dBaseline\u201d prediction:\n3043.\n\u201dNaive\u201d prediction:\n3043.\n\u201dMix\u201d prediction:\n3043.\n\u201dCombined\u201d prediction:\n3043.\nInput:\nprint((((2578 if 7750<1768 else 8639)-2590)+342)).\nTarget:\n6391.\n\u201dBaseline\u201d prediction:\n-555.\n\u201dNaive\u201d prediction:\n6329.\n\u201dMix\u201d prediction:\n6461.\n\u201dCombined\u201d prediction:\n6105.\nInput:\nprint((((841 if 2076<7326 else 1869)*10) if 7827<317 else 7192)).\nTarget:\n7192.\n\u201dBaseline\u201d prediction:\n7192.\n\u201dNaive\u201d prediction:\n7192.\n\u201dMix\u201d prediction:\n7192.\n\u201dCombined\u201d prediction:\n7192.\nInput:\nd=8640;\nprint((7135 if 6710>((d+7080)*14) else 7200)).\nTarget:\n7200.\n\u201dBaseline\u201d prediction:\n7200.\n\u201dNaive\u201d prediction:\n7200.\n\u201dMix\u201d prediction:\n7200.\n\u201dCombined\u201d prediction:\n7200.\nInput:\nb=6968\nfor x in range(10):b-=(299 if 3389<9977 else 203)\nprint((12*b)).\n15\nUnder review as a conference paper at ICLR 2015\nTarget:\n47736.\n\u201dBaseline\u201d prediction:\n-0666.\n\u201dNaive\u201d prediction:\n11262.\n\u201dMix\u201d prediction:\n48666.\n\u201dCombined\u201d prediction:\n48766.\nInput:\nj=(1*5057);\nprint(((j+1215)+6931)).\nTarget:\n13203.\n\u201dBaseline\u201d prediction:\n13015.\n\u201dNaive\u201d prediction:\n12007.\n\u201dMix\u201d prediction:\n13379.\n\u201dCombined\u201d prediction:\n13205.\nInput:\nprint(((1090-3305)+9466)).\nTarget:\n7251.\n\u201dBaseline\u201d prediction:\n7111.\n\u201dNaive\u201d prediction:\n7099.\n\u201dMix\u201d prediction:\n7595.\n\u201dCombined\u201d prediction:\n7699.\nInput:\na=8331;\nprint((a-(15*7082))).\nTarget:\n-97899.\n\u201dBaseline\u201d prediction:\n-96991.\n\u201dNaive\u201d prediction:\n-19959.\n\u201dMix\u201d prediction:\n-95551.\n\u201dCombined\u201d prediction:\n-96397.\n12.4\nEXAMPLES OF PROGRAM EVALUATION PREDICTION. LENGTH = 6, NESTING = 1\nInput:\nprint((71647-548966)).\nTarget:\n-477319.\n\u201dBaseline\u201d prediction:\n-472122.\n\u201dNaive\u201d prediction:\n-477591.\n\u201dMix\u201d prediction:\n-479705.\n\u201dCombined\u201d prediction:\n-475009.\nInput:\nprint(1508).\nTarget:\n1508.\n\u201dBaseline\u201d prediction:\n1508.\n\u201dNaive\u201d prediction:\n1508.\n\u201dMix\u201d prediction:\n1508.\n\u201dCombined\u201d prediction:\n1508.\nInput:\n16\nUnder review as a conference paper at ICLR 2015\nj=611989;\nprint((j+763864)).\nTarget:\n1375853.\n\u201dBaseline\u201d prediction:\n1379920.\n\u201dNaive\u201d prediction:\n1378991.\n\u201dMix\u201d prediction:\n1375119.\n\u201dCombined\u201d prediction:\n1375173.\nInput:\nprint((151108 if 289653>33296 else 564130)).\nTarget:\n151108.\n\u201dBaseline\u201d prediction:\n154973.\n\u201dNaive\u201d prediction:\n151108.\n\u201dMix\u201d prediction:\n151108.\n\u201dCombined\u201d prediction:\n151108.\nInput:\nc=142012\nfor x in range(12):c-=166776\nprint(c).\nTarget:\n-1859300.\n\u201dBaseline\u201d prediction:\n-1840831.\n\u201dNaive\u201d prediction:\n-1840000.\n\u201dMix\u201d prediction:\n-1979720.\n\u201dCombined\u201d prediction:\n-1820700.\nInput:\nprint((678740+203140)).\nTarget:\n881880.\n\u201dBaseline\u201d prediction:\n880475.\n\u201dNaive\u201d prediction:\n881666.\n\u201dMix\u201d prediction:\n880190.\n\u201dCombined\u201d prediction:\n885920.\nInput:\nprint((929067-75246)).\nTarget:\n853821.\n\u201dBaseline\u201d prediction:\n851233.\n\u201dNaive\u201d prediction:\n867113.\n\u201dMix\u201d prediction:\n855615.\n\u201dCombined\u201d prediction:\n853009.\nInput:\nd=960350\nfor x in range(24):d-=187946\nprint(d).\nTarget:\n-3550354.\n\u201dBaseline\u201d prediction:\n-3571998.\n\u201dNaive\u201d prediction:\n-3699993.\n\u201dMix\u201d prediction:\n-3899220.\n\u201dCombined\u201d prediction:\n-3507790.\n17\nUnder review as a conference paper at ICLR 2015\nInput:\nprint((8*786463)).\nTarget:\n6291704.\n\u201dBaseline\u201d prediction:\n6270804.\n\u201dNaive\u201d prediction:\n6271904.\n\u201dMix\u201d prediction:\n6297644.\n\u201dCombined\u201d prediction:\n6270004.\nInput:\nprint((498592-570324)).\nTarget:\n-71732.\n\u201dBaseline\u201d prediction:\n-61086.\n\u201dNaive\u201d prediction:\n-73582.\n\u201dMix\u201d prediction:\n-19000.\n\u201dCombined\u201d prediction:\n-72842.\n12.5\nEXAMPLES OF PROGRAM EVALUATION PREDICTION. LENGTH = 6, NESTING = 2\nInput:\nprint((39007+416968)).\nTarget:\n455975.\n\u201dBaseline\u201d prediction:\n559917.\n\u201dNaive\u201d prediction:\n438887.\n\u201dMix\u201d prediction:\n458993.\n\u201dCombined\u201d prediction:\n450031.\nInput:\nprint((586051+664462)).\nTarget:\n1250513.\n\u201dBaseline\u201d prediction:\n1250939.\n\u201dNaive\u201d prediction:\n1240719.\n\u201dMix\u201d prediction:\n1230881.\n\u201dCombined\u201d prediction:\n1240551.\nInput:\nprint(948950).\nTarget:\n948950.\n\u201dBaseline\u201d prediction:\n948950.\n\u201dNaive\u201d prediction:\n948950.\n\u201dMix\u201d prediction:\n948950.\n\u201dCombined\u201d prediction:\n948950.\nInput:\ni=849846\nfor x in range(15):i-=557574\nprint((362961 if 881013<597832 else i)).\n18\nUnder review as a conference paper at ICLR 2015\nTarget:\n-7513764.\n\u201dBaseline\u201d prediction:\n-7422756.\n\u201dNaive\u201d prediction:\n-7011048.\n\u201dMix\u201d prediction:\n-2617777.\n\u201dCombined\u201d prediction:\n-7101146.\nInput:\ng=977055;\nprint((g-(592222+268807))).\nTarget:\n116026.\n\u201dBaseline\u201d prediction:\n132440.\n\u201dNaive\u201d prediction:\n101488.\n\u201dMix\u201d prediction:\n114988.\n\u201dCombined\u201d prediction:\n125682.\nInput:\nprint(((17*711621) if 224989>711768 else 267900)).\nTarget:\n267900.\n\u201dBaseline\u201d prediction:\n267900.\n\u201dNaive\u201d prediction:\n267900.\n\u201dMix\u201d prediction:\n267900.\n\u201dCombined\u201d prediction:\n267900.\nInput:\nj=114940;\nprint((j+482118)).\nTarget:\n597058.\n\u201dBaseline\u201d prediction:\n590006.\n\u201dNaive\u201d prediction:\n690004.\n\u201dMix\u201d prediction:\n599816.\n\u201dCombined\u201d prediction:\n599990.\nInput:\nprint((171932*19)).\nTarget:\n3266708.\n\u201dBaseline\u201d prediction:\n3249998.\n\u201dNaive\u201d prediction:\n3131798.\n\u201dMix\u201d prediction:\n3390158.\n\u201dCombined\u201d prediction:\n3100388.\nInput:\nh=411671;\nprint((242648 if (h+31605)>679390 else 449699)).\nTarget:\n449699.\n\u201dBaseline\u201d prediction:\n449699.\n\u201dNaive\u201d prediction:\n449699.\n\u201dMix\u201d prediction:\n449699.\n\u201dCombined\u201d prediction:\n449699.\nInput:\nprint(11332).\n19\nUnder review as a conference paper at ICLR 2015\nTarget:\n11332.\n\u201dBaseline\u201d prediction:\n11332.\n\u201dNaive\u201d prediction:\n11332.\n\u201dMix\u201d prediction:\n11332.\n\u201dCombined\u201d prediction:\n11332.\n12.6\nEXAMPLES OF PROGRAM EVALUATION PREDICTION. LENGTH = 6, NESTING = 3\nInput:\nc=335973;\nb=(c+756088);\nprint((6*(b+66858))).\nTarget:\n6953514.\n\u201dBaseline\u201d prediction:\n1099522.\n\u201dNaive\u201d prediction:\n7773362.\n\u201dMix\u201d prediction:\n6993124.\n\u201dCombined\u201d prediction:\n1044444.\nInput:\nc=935280;\nprint((765618 if 409621<(c-(329375 if 806201<240281 else 81797)) else\n805944)).\nTarget:\n765618.\n\u201dBaseline\u201d prediction:\n800988.\n\u201dNaive\u201d prediction:\n765644.\n\u201dMix\u201d prediction:\n765616.\n\u201dCombined\u201d prediction:\n865618.\nInput:\nprint(((670421 if 144271>805597 else 364643)*20)).\nTarget:\n7292860.\n\u201dBaseline\u201d prediction:\n1774640.\n\u201dNaive\u201d prediction:\n7134660.\n\u201dMix\u201d prediction:\n7292860.\n\u201dCombined\u201d prediction:\n7292860.\nInput:\nprint((108196 if 714126>847153 else (888873-(381812*13)))).\nTarget:\n-4074683.\n\u201dBaseline\u201d prediction:\n13205544.\n\u201dNaive\u201d prediction:\n-4011899.\n\u201dMix\u201d prediction:\n-4422909.\n\u201dCombined\u201d prediction:\n-4048381.\nInput:\nj=(181489 if 467875>46774 else (127738 if 866523<633391 else 592486))\n;\nprint((j-627483)).\n20\nUnder review as a conference paper at ICLR 2015\nTarget:\n-445994.\n\u201dBaseline\u201d prediction:\n-333153.\n\u201dNaive\u201d prediction:\n-488724.\n\u201dMix\u201d prediction:\n-440880.\n\u201dCombined\u201d prediction:\n-447944.\nInput:\nf=483654\nfor x in range(9):f-=913681\na=f\nfor x in range(12):a-=926785\nprint((124798 if a>326533 else 576599)).\nTarget:\n576599.\n\u201dBaseline\u201d prediction:\n176599.\n\u201dNaive\u201d prediction:\n576599.\n\u201dMix\u201d prediction:\n576599.\n\u201dCombined\u201d prediction:\n576599.\nInput:\nf=136315;\nh=(f+37592);\ng=418652;\nprint((g-(h+234728))).\nTarget:\n10017.\n\u201dBaseline\u201d prediction:\n12115.\n\u201dNaive\u201d prediction:\n-1123.\n\u201dMix\u201d prediction:\n-000..\n\u201dCombined\u201d prediction:\n-0033.\nInput:\na=768606\nfor x in range(11):a+=454841\nf=a\nfor x in range(3):f-=696226\nprint((340434 if f<287035 else 523084)).\nTarget:\n523084.\n\u201dBaseline\u201d prediction:\n523084.\n\u201dNaive\u201d prediction:\n523084.\n\u201dMix\u201d prediction:\n523084.\n\u201dCombined\u201d prediction:\n523084.\nInput:\nb=468503;\nprint((b-(326264+406077))).\nTarget:\n-263838.\n\u201dBaseline\u201d prediction:\n-278797.\n\u201dNaive\u201d prediction:\n-241144.\n\u201dMix\u201d prediction:\n-252080.\n\u201dCombined\u201d prediction:\n-277882.\nInput:\ng=801925;\nprint((58095+(g+(824920 if 842317>176260 else 570318)))).\n21\nUnder review as a conference paper at ICLR 2015\nTarget:\n1684940.\n\u201dBaseline\u201d prediction:\n1602221.\n\u201dNaive\u201d prediction:\n1799892.\n\u201dMix\u201d prediction:\n1677788.\n\u201dCombined\u201d prediction:\n1611888.\n12.7\nEXAMPLES OF PREDICTING RESULT OF ADDITION.\nLENGTH = 6\nInput:\nprint(284993+281178).\nTarget:\n566171.\n\u201dBaseline\u201d prediction:\n566199.\n\u201dNaive\u201d prediction:\n566151.\n\u201dMix\u201d prediction:\n566171.\n\u201dCombined\u201d prediction:\n566171.\nInput:\nprint(616216+423489).\nTarget:\n1039705.\n\u201dBaseline\u201d prediction:\n1039712.\n\u201dNaive\u201d prediction:\n1039605.\n\u201dMix\u201d prediction:\n1039605.\n\u201dCombined\u201d prediction:\n1039705.\nInput:\nprint(559794+837898).\nTarget:\n1397692.\n\u201dBaseline\u201d prediction:\n1397694.\n\u201dNaive\u201d prediction:\n1397662.\n\u201dMix\u201d prediction:\n1397792.\n\u201dCombined\u201d prediction:\n1397692.\nInput:\nprint(830194+551314).\nTarget:\n1381508.\n\u201dBaseline\u201d prediction:\n1381401.\n\u201dNaive\u201d prediction:\n1381518.\n\u201dMix\u201d prediction:\n1381508.\n\u201dCombined\u201d prediction:\n1381508.\nInput:\nprint(252849+873177).\nTarget:\n1126026.\n\u201dBaseline\u201d prediction:\n1126020.\n\u201dNaive\u201d prediction:\n1126006.\n\u201dMix\u201d prediction:\n1125026.\n\u201dCombined\u201d prediction:\n1126026.\nInput:\nprint(17513+163744).\n22\nUnder review as a conference paper at ICLR 2015\nTarget:\n181257.\n\u201dBaseline\u201d prediction:\n181398.\n\u201dNaive\u201d prediction:\n181287.\n\u201dMix\u201d prediction:\n181257.\n\u201dCombined\u201d prediction:\n181257.\nInput:\nprint(530590+569236).\nTarget:\n1099826.\n\u201dBaseline\u201d prediction:\n1099708.\n\u201dNaive\u201d prediction:\n1099826.\n\u201dMix\u201d prediction:\n1099826.\n\u201dCombined\u201d prediction:\n1099826.\nInput:\nprint(856484+436077).\nTarget:\n1292561.\n\u201dBaseline\u201d prediction:\n1292589.\n\u201dNaive\u201d prediction:\n1292571.\n\u201dMix\u201d prediction:\n1292561.\n\u201dCombined\u201d prediction:\n1292561.\nInput:\nprint(731632+833163).\nTarget:\n1564795.\n\u201dBaseline\u201d prediction:\n1564769.\n\u201dNaive\u201d prediction:\n1564775.\n\u201dMix\u201d prediction:\n1564795.\n\u201dCombined\u201d prediction:\n1564795.\nInput:\nprint(738532+444531).\nTarget:\n1183063.\n\u201dBaseline\u201d prediction:\n1183000.\n\u201dNaive\u201d prediction:\n1183063.\n\u201dMix\u201d prediction:\n1183063.\n\u201dCombined\u201d prediction:\n1183063.\n12.8\nEXAMPLES OF PREDICTING RESULT OF ADDITION.\nLENGTH = 8\nInput:\nprint(32847917+95908452).\nTarget:\n128756369.\n\u201dBaseline\u201d prediction:\n128899997.\n\u201dNaive\u201d prediction:\n128756669.\n\u201dMix\u201d prediction:\n128756369.\n\u201dCombined\u201d prediction:\n128756369.\nInput:\nprint(49173072+46963478).\n23\nUnder review as a conference paper at ICLR 2015\nTarget:\n96136550.\n\u201dBaseline\u201d prediction:\n96129999.\n\u201dNaive\u201d prediction:\n96136050.\n\u201dMix\u201d prediction:\n96136550.\n\u201dCombined\u201d prediction:\n96136550.\nInput:\nprint(79385668+60159139).\nTarget:\n139544807.\n\u201dBaseline\u201d prediction:\n139679090.\n\u201dNaive\u201d prediction:\n139544707.\n\u201dMix\u201d prediction:\n139544807.\n\u201dCombined\u201d prediction:\n139544807.\nInput:\nprint(16183468+42542767).\nTarget:\n58726235.\n\u201dBaseline\u201d prediction:\n58798523.\n\u201dNaive\u201d prediction:\n58726035.\n\u201dMix\u201d prediction:\n58726235.\n\u201dCombined\u201d prediction:\n58726235.\nInput:\nprint(15982788+54043908).\nTarget:\n70026696.\n\u201dBaseline\u201d prediction:\n60014022.\n\u201dNaive\u201d prediction:\n70026496.\n\u201dMix\u201d prediction:\n60026696.\n\u201dCombined\u201d prediction:\n70026696.\nInput:\nprint(45356253+31242293).\nTarget:\n76598546.\n\u201dBaseline\u201d prediction:\n76699777.\n\u201dNaive\u201d prediction:\n76598246.\n\u201dMix\u201d prediction:\n76598546.\n\u201dCombined\u201d prediction:\n76598546.\nInput:\nprint(93230501+12607891).\nTarget:\n105838392.\n\u201dBaseline\u201d prediction:\n105999882.\n\u201dNaive\u201d prediction:\n105838292.\n\u201dMix\u201d prediction:\n105838392.\n\u201dCombined\u201d prediction:\n105838392.\nInput:\nprint(2487336+40625181).\n24\nUnder review as a conference paper at ICLR 2015\nTarget:\n43112517.\n\u201dBaseline\u201d prediction:\n43178441.\n\u201dNaive\u201d prediction:\n43112917.\n\u201dMix\u201d prediction:\n43112517.\n\u201dCombined\u201d prediction:\n43112517.\nInput:\nprint(61854571+75028157).\nTarget:\n136882728.\n\u201dBaseline\u201d prediction:\n136860087.\n\u201dNaive\u201d prediction:\n136883928.\n\u201dMix\u201d prediction:\n136882728.\n\u201dCombined\u201d prediction:\n136882728.\nInput:\nprint(13828700+10188872).\nTarget:\n24017572.\n\u201dBaseline\u201d prediction:\n24000349.\n\u201dNaive\u201d prediction:\n24018872.\n\u201dMix\u201d prediction:\n23017572.\n\u201dCombined\u201d prediction:\n24017572.\n25\n",
        "sentence": "",
        "context": "Lee, Yong Jae and Grauman, Kristen. Learning the easy things \ufb01rst: Self-paced visual category discovery. In\nComputer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pp. 1721\u20131728. IEEE, 2011.\nlogical semantics. arXiv preprint arXiv:1406.1827, 2014.\nCho, Kyunghyun, van Merrienboer, Bart, Gulcehre, Caglar, Bougares, Fethi, Schwenk, Holger, and Bengio,\nREFERENCES\nBengio, Yoshua, Louradour, J\u00b4er\u02c6ome, Collobert, Ronan, and Weston, Jason. Curriculum learning. In Proceed-\nings of the 26th annual international conference on machine learning, pp. 41\u201348. ACM, 2009."
    },
    {
        "title": "Learning social infectivity in sparse low-rank networks using multi-dimensional hawkes processes",
        "author": [
            "Zhou",
            "Ke",
            "Zha",
            "Hongyuan",
            "Song",
            "Le"
        ],
        "venue": "In AISTATS, pp",
        "citeRegEx": "Zhou et al\\.,? \\Q2013\\E",
        "shortCiteRegEx": "Zhou et al\\.",
        "year": 2013,
        "abstract": "",
        "full_text": "",
        "sentence": " , 2007; Johnson & Willsky, 2013; Lange, 2014) and intensity function modeling techniques such as Cox and Hawkes processes (Liniger, 2009; Zhou et al., 2013; Linderman & Adams, 2014; Choi et al., 2015).",
        "context": null
    },
    {
        "title": "Nonlinear Hawkes Processes",
        "author": [
            "Zhu",
            "Lingjiong"
        ],
        "venue": "PhD thesis,",
        "citeRegEx": "Zhu and Lingjiong.,? \\Q2013\\E",
        "shortCiteRegEx": "Zhu and Lingjiong.",
        "year": 2013,
        "abstract": "The Hawkes process is a simple point process that has long memory, clustering\neffect, self-exciting property and is in general non-Markovian. The future\nevolution of a self-exciting point process is influenced by the timing of the\npast events. There are applications in finance, neuroscience, genome analysis,\nseismology, sociology, criminology and many other fields. We first survey the\nknown results about the theory and applications of both linear and nonlinear\nHawkes processes. Then, we obtain the central limit theorem and process-level,\ni.e. level-3 large deviations for nonlinear Hawkes processes. The level-1 large\ndeviation principle holds as a result of the contraction principle. We also\nprovide an alternative variational formula for the rate function of the level-1\nlarge deviations in the Markovian case. Next, we drop the usual assumptions on\nthe nonlinear Hawkes process and categorize it into different regimes, i.e.\nsublinear, sub-critical, critical, super-critical and explosive regimes. We\nshow the different time asymptotics in different regimes and obtain other\nproperties as well. Finally, we study the limit theorems of linear Hawkes\nprocesses with random marks.",
        "full_text": "Nonlinear Hawkes Processes\nby\nLingjiong Zhu\nA dissertation submitted in partial ful\ufb01llment\nof the requirements for the degree of\nDoctor of Philosophy\nDepartment of Mathematics\nNew York University\nMay 2013\nProfessor S. R. S. Varadhan\narXiv:1304.7531v3  [math.PR]  23 Jun 2013\nc\u20dd\nLingjiong Zhu\nAll Rights Reserved, 2013\nDedication\nTo the memory of my grandpa\nZhixuan Zhu (1923-2001)\niii\nAcknowledgements\nIt is di\ufb03cult to overstate my gratitude to my adviser Professor Varadhan.\nWorking with Professor Varadhan has been an absolutely amazing experience for\nme. I thank him for always keeping his door open and patiently answering my\nquestions. I thank him for his superb guidance, understanding, and generosity.\nI thank him for suggesting the topic for my thesis, which would not be possible\nwithout his deep wisdom and sharing of many new ideas. He has been everything\nthat one can reasonably ask for in an advisor and more, and I am truly grateful\nto him.\nI want to thank the Courant community for guiding me through this process\nand for putting up with me in general. Tamar Arnon does her job exceptionally\nwell and her e\ufb00orts are much appreciated. I want to thank the faculty for many\nwell taught and interesting classes. I am indebted to G\u00b4erard Ben Arous, Sourav\nChatterjee and Raghu Varadhan for writing me recommendations for my \ufb01rst\nacademic job. I also want to thank Peter Carr for his interest in my thesis.\nI remember a joke told by Jalal Shatah that the most important thing as an\nundergraduate student is to go to a top graduate program.\nBut once you are\nalready at a graduate school, the most important thing is to get out of it! This\nwould not be possible without the \ufb01nal step, i.e. thesis defense! I am grateful to\nhave Henry McKean, Chuck Newman and Raghu Varadhan as the three readers\nand G\u00b4erard Ben Arous and Lai-Sang Young as the two non-readers on my thesis\ncommittee.\nMost importantly, I want to thank my fellow colleagues for all the fun memories\nthat I take with me from Courant. New York City, without good friends, can be\nthe most populated lonely place in the world, but thankfully the constant friend-\niv\nship of my fellow Courant colleagues has made these \ufb01ve years some of the most\nentertaining and pleasurable of my life. I thank Antoine Cerfon, Shirshendu Chat-\nterjee, Oliver Conway, S\u02c6\u0131nziana Datcu, Partha Dey, Thomas Fai, Max Fathi, Mert\nG\u00a8urb\u00a8uzbalaban, Matan Harel, Miranda Holmes-Cerfon, Arjun Krishnan, Shoshana\nLe\ufb04er, Sandra May, Jim Portegies, Alex Rozinov, Patrick Stewart, Adam Stinch-\ncombe, Jordan Thomas, Chen-Hung Wu and many others for their friendship. In\nparticular, I want to thank Dmytro Karabash, Behzad Mehrdad and Sanchayan\nSen. They are not only my good friends, but coauthors as well. I also thank my\no\ufb03ce neighbor Cheryl Sylivant for her friendship.\nBy living in New York City, I had the great opportunities to visit as many\nmuseums and go to as many concerts as possible. I am grateful to the New York\nPhilharmonic and Metropolitan Opera House for their student ticket o\ufb00ers and\nalso many wonderful student recitals and concerts at Juilliard School, which have\nmade my stay in New York City much more enjoyable.\nI also want to thank the professors at the University of Cambridge, who pro-\nvided me a solid undergraduate education. In particular, I am grateful to Rachel\nCamina, as well as Houshang Ardavan and Tom K\u00a8orner. I also want to thank Ste-\nfano Luzzatto for supervising me on an undergraduate research project at Imperial\nCollege, London.\nI am very much indebted to my family back home. I thank my parents for so\nmany years of love and understanding. They are truly the best parents one could\nask for. I also thank my grandmas, uncles and aunts for their support. Finally, I\ndedicate this thesis to the memory of my late grandpa. I miss him dearly.\nv\nAbstract\nThe Hawkes process is a simple point process that has long memory, clustering\ne\ufb00ect, self-exciting property and is in general non-Markovian. The future evolution\nof a self-exciting point process is in\ufb02uenced by the timing of the past events. There\nare applications in \ufb01nance, neuroscience, genome analysis, seismology, sociology,\ncriminology and many other \ufb01elds. We \ufb01rst survey the known results about the\ntheory and applications of both linear and nonlinear Hawkes processes. Then, we\nobtain the central limit theorem and process-level, i.e. level-3 large deviations for\nnonlinear Hawkes processes. The level-1 large deviation principle holds as a result\nof the contraction principle. We also provide an alternative variational formula for\nthe rate function of the level-1 large deviations in the Markovian case. Next, we\ndrop the usual assumptions on the nonlinear Hawkes process and categorize it into\ndi\ufb00erent regimes, i.e. sublinear, sub-critical, critical, super-critical and explosive\nregimes. We show the di\ufb00erent time asymptotics in di\ufb00erent regimes and obtain\nother properties as well. Finally, we study the limit theorems of linear Hawkes\nprocesses with random marks.\nvi\nContents\nDedication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\niii\nAcknowledgements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\niv\nAbstract . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nvi\nList of Figures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nxi\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1\n1\nHawkes Processes\n3\n1.1\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n1.2\nApplications of Hawkes Processes . . . . . . . . . . . . . . . . . . .\n9\n1.3\nRelated Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n1.4\nLinear Hawkes Processes . . . . . . . . . . . . . . . . . . . . . . . .\n16\n1.5\nNonlinear Hawkes Processes . . . . . . . . . . . . . . . . . . . . . .\n33\n1.6\nMultivariate Hawkes Processes . . . . . . . . . . . . . . . . . . . . .\n38\n2\nCentral Limit Theorem for Nonlinear Hawkes Processes\n41\n2.1\nMain Results\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n41\n2.2\nProofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n44\n3\nProcess-Level Large Deviations for Nonlinear Hawkes Processes\n57\n3.1\nMain Results\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n57\nvii\n3.2\nLower Bound\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n64\n3.3\nUpper Bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n76\n3.4\nSuperexponential Estimates\n. . . . . . . . . . . . . . . . . . . . . .\n89\n3.5\nConcluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . 100\n4\nLarge Deviations for Markovian Nonlinear Hawkes Processes\n102\n4.1\nAn Ergodic Lemma . . . . . . . . . . . . . . . . . . . . . . . . . . . 103\n4.2\nLarge Deviations for Markovian Nonlinear Hawkes Processes with\nExponential Exciting Function . . . . . . . . . . . . . . . . . . . . . 106\n4.3\nLarge Deviations for Markovian Nonlinear Hawkes Processes with\nSum of Exponentials Exciting Function . . . . . . . . . . . . . . . . 125\n4.4\nLarge Deviations for a Special Class of Nonlinear Hawkes Processes:\nAn Approximation Approach\n. . . . . . . . . . . . . . . . . . . . . 131\n5\nAsymptotics for Nonlinear Hawkes Processes\n142\n5.1\nSublinear Regime . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145\n5.2\nSub-Critical Regime\n. . . . . . . . . . . . . . . . . . . . . . . . . . 148\n5.3\nCritical Regime . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150\n5.4\nSuper-Critical Regime\n. . . . . . . . . . . . . . . . . . . . . . . . . 161\n5.5\nExplosive Regime . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165\n6\nLimit Theorems for Marked Hawkes Processes\n172\n6.1\nIntroduction and Main Results . . . . . . . . . . . . . . . . . . . . . 172\n6.2\nProof of Central Limit Theorem . . . . . . . . . . . . . . . . . . . . 175\n6.3\nProof of Large Deviation Principle\n. . . . . . . . . . . . . . . . . . 177\n6.4\nRisk Model with Marked Hawkes Claims Arrivals\n. . . . . . . . . . 187\n6.5\nExamples with Explicit Formulas . . . . . . . . . . . . . . . . . . . 191\nviii\nBibliography\n195\nix\nList of Figures\n1.1\nThis is a comparison of a Hawkes process with a Poisson process.\nThe \ufb01gure on the left shows the histogram of a Hawkes process with\nh(t) =\n1\n(t+1)2 and \u03bb(z) = 1 +\n9\n10z and the \ufb01gure on the right the\nhistogram of a Poisson process with constant intensity \u03bb \u22613\n2. In\nthe \ufb01gure, each column represents the number of points that arrived\nin that unit time subinterval.\n. . . . . . . . . . . . . . . . . . . . .\n8\n1.2\nPlot of intensity \u03bbt for a realization of Hawkes process. Here h(t) =\n1\n(t+1)2 and \u03bb(z) = 1 + 0.9z. . . . . . . . . . . . . . . . . . . . . . . .\n8\n5.1\nPlot of intensity \u03bbt for a realization of Hawkes process. Here h(t) =\n(t + 1)\u22121\n2 and \u03bb(z) = (1 + z)\n1\n2. . . . . . . . . . . . . . . . . . . . . . 144\n5.2\nPlot of intensity \u03bbt for a realization of Hawkes process. Here h(t) =\n4\n(t+1)3 and \u03bb(z) = (1 + z)\n1\n2. In this case, \u2225h\u2225L1 < \u221eand \u03bb(\u00b7) is\nsublinear and Lipschitz. It will converge to the unique stationary\nversion of the Hawkes process. . . . . . . . . . . . . . . . . . . . . . 144\n5.3\nPlot of intensity \u03bbt for a realization of Hawkes process. Here h(t) =\n1\n(t+1)3 and \u03bb(z) = 1 + z. In this case, \u2225h\u2225L1 = 1\n2 < 1. It is in the\nsub-critical regime. This is a classical Hawkes process and it will\nconverge to the unique stationary version of the Hawkes process. . . 145\nx\n5.4\nPlot of intensity \u03bbt for a realization of Hawkes process. Here h(t) =\n2\n(t+1)3 and \u03bb(z) = 1 + z. In this case, \u2225h\u2225L1 = 1,\nR \u221e\n0 th(t)dt < \u221e\nand \u03bb(\u00b7) is linear. It is therefore in the critical regime. From the\ngraph, we can see that \u03bbt grows linearly in t, which will be proved\nin this chapter. Indeed, we will prove that N\u00b7T\nT 2 converges to\nR \u00b7\n0 \u03b7sds\nas T \u2192\u221e, where \u03b7s is a squared Bessel process. . . . . . . . . . . . 146\n5.5\nPlot of intensity \u03bbt for a realization of Hawkes process. Here h(t) =\n3\n(t+1)3 and \u03bb(z) = 1 + z. In this case, \u2225h\u2225L1 = 3\n2 > 1 and it is in the\nsuper-critical regime. We expect that \u03bbt would grow exponentially\nin this case. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147\n5.6\nPlot of intensity \u03bbt for a realization of Hawkes process. Here h(t) =\n1\n(t+1)3 and \u03bb(z) = (1+z)\n3\n2. This is in the explosive regime. The plot\nis a little bit cheating because it is impossible to \u201cplot\u201d explosion.\nNevertheless, you can think it as an illustration. It \u201cappears\u201d that\nthe process explodes near time t = 6. . . . . . . . . . . . . . . . . . 148\nxi\nIntroduction\nThis thesis is about the nonlinear Hawkes process, a simple point processes,\nthat has long memory, the clustering e\ufb00ect, the self-exciting property and is in\ngeneral non-Markovian. The future evolution of a self-exciting point process is\nin\ufb02uenced by the timing of the past events. There are applications in \ufb01nance,\nneuroscience, genome analysis, sociology, criminology, seismology, and many other\n\ufb01elds.\nChapter 1 includes the introduction of the model and the survey of the results\nalready known in the literature about Hawkes processes. That includes the stability\nresults, limit theorems, power spectra of linear Hawkes processes and stability\nresults of nonlinear Hawkes processes.\nChapter 2 is about the functional central limit theorem of nonlinear Hawkes\nprocesses. A Strassen\u2019s invariance holds under the same assumptions. The work\nin Chapter 2 is based on Zhu [114].\nChapter 3 is dedicated to the process-level large deviations, i.e. level-3 large\ndeviations, of the nonlinear Hawkes processes. The proofs consist of the proofs of\nthe lower bound, the upper bound and the superexponential estimates. The level-1\nlarge deviation principle is derived as a result of the contraction principle. This\nchapter is based on Zhu [113].\nChapter 4 is dedicated to the study of level-1 large deviation principle for\nnonlinear Hawkes processes when the exciting functions are exponential or sums\nof exponentials. It is based on the observation that when the exciting functions are\nexponential or sums of exponentials, the process is Markovian and a combination of\nFeynman-Kac formula for the upper bound of large deviations of Markov processes\nand tilting of the intensity function of Hawkes processes for the lower bound will\n1\nestablish a level-1 large deviation principle with the rate function expressed in\nterms of some variational formula. This chapter is based on Zhu [112].\nChapter 5 is about the asymptotics for nonlinear Hawkes processes. In this\nchapter, we drop the usual assumptions on nonlinear Hawkes processes, and study\nthe phase transitions in di\ufb00erent regimes. We categorize nonlinear Hawkes pro-\ncesses into the following regimes: sublinear regime, sub-critical regime, critical\nregime, super-critical regime and explosive regime.\nDi\ufb00erent time asymptotics\nand various properties are obtained in di\ufb00erent regimes. This chapter is based on\nZhu [117].\nChapter 6 is about the limit theorems for linear Hawkes processes with random\nmarks. The Central limit theorem and the large deviation principle are derived.\nWe end this chapter with a simple application to a risk model. This is based on\nthe joint work with my colleague Dmytro Karabash, see [62].\nDuring my time as a PhD student at Courant Institute, I have the joy to work\non some other problems either by myself or with my colleagues. For example, I\nstudied the large deviations of self-correcting point processes with Sanchayan Sen,\nsee [100] and also did some work on biased random walks on Galton-Watson trees\nwithout leaves with Behzad Mehrdad and Sanchayan Sen, see [75]. But since they\nare not closely related to the topics of my thesis, I do not include them here.\n2\nChapter 1\nHawkes Processes\n1.1\nIntroduction\nHawkes process is a self-exciting simple point process \ufb01rst introduced by Hawkes\n[51]. The future evolution of a self-exciting point process is in\ufb02uenced by the timing\nof past events. The process is non-Markovian except for some very special cases.\nIn other words, Hawkes process depends on the entire past history and has a long\nmemory. Hawkes process has wide applications in neuroscience, see e.g. Johnson\n[59], Chornoboy et al. [25], Pernice et al. [93], Pernice et al. [94], Reynaud et al.\n[98]; seismology, see e.g. Hawkes and Adamopoulos [53], Ogata [87], Ogata [88],\nOgata et al. [90]; genome analysis, see e.g. Gusto and Schbath [46], Reynaud-\nBouret and Schbath [96]; psycology, see e.g. Halpin and De Boeck [48]; spread of\ninfectious disease, see e.g. Meyer et al. [76]; \ufb01nance, see e.g. Bauwens and Hautsch\n[7], Bowsher [13], Hewlett [56], Large [67], Cartea et al. [22], Chavez-Demoulin et\nal. [23], Errais et al. [36]. Embrechts et al. [35], Muni Toke and Pomponio [83],\nBacry et al. [3], [4], [1]; and in many other \ufb01elds.\n3\nLet N be a simple point process on R and F\u2212\u221e\nt\n:= \u03c3(N(C), C \u2208B(R), C \u2282\n(\u2212\u221e, t]) be an increasing family of \u03c3-algebras. Any nonnegative F\u2212\u221e\nt\n-progressively\nmeasurable process \u03bbt with\n(1.1)\nE\n\u0002\nN(a, b]|F\u2212\u221e\na\n\u0003\n= E\n\u0014Z b\na\n\u03bbsds\n\f\fF\u2212\u221e\na\n\u0015\n,\na.s. for all intervals (a, b] is called the F\u2212\u221e\nt\n-intensity of N. We use the notation\nNt := N(0, t] to denote the number of points in the interval (0, t].\nA nonlinear Hawkes process is a simple point process N admitting an F\u2212\u221e\nt\n-\nintensity\n(1.2)\n\u03bbt := \u03bb\n\u0012Z t\n\u2212\u221e\nh(t \u2212s)N(ds)\n\u0013\n,\nwhere \u03bb(\u00b7) : R+ \u2192R+ is locally integrable and left continuous, h(\u00b7) : R+ \u2192R+.\nWe always assume that \u2225h\u2225L1 =\nR \u221e\n0 h(t)dt < \u221eunless otherwise speci\ufb01ed. Here\nR t\n\u2212\u221eh(t \u2212s)N(ds) stands for\nR\n(\u2212\u221e,t) h(t \u2212s)N(ds), which is important for F\u2212\u221e\nt\n-\npredictability.\nThe local integrability assumption of \u03bb(\u00b7) is to avoid explosion\nand the left continuity assumption of \u03bb(\u00b7) is to ensure that the process is F\u2212\u221e\nt\n-\npredictable.\nIn the literature, h(\u00b7) and \u03bb(\u00b7) are usually referred to as exciting function and\nrate function respectively.\nA Hawkes process is said to be linear if \u03bb(\u00b7) is linear and it is nonlinear other-\nwise. For a linear Hawkes process, we can assume that the intensity is\n(1.3)\n\u03bbt := \u03bd +\nZ\n(\u2212\u221e,t)\nh(t \u2212s)N(ds).\n4\nIn this thesis, unless otherwise speci\ufb01ed, we assume the following.\n\u2022 \u03bb(\u00b7) : R+ \u2192R+ is continuous and non-decreasing.\n\u2022 h(\u00b7) : R+ \u2192R+ is continuous and non-increasing.\n\u2022 N(\u2212\u221e, 0] = 0, i.e. the Hawkes process has empty past history.\nThroughout, we de\ufb01ne Zt as Zt :=\nR t\n0 h(t \u2212s)N(ds). Thus, \u03bbt = \u03bb(Zt).\nThe \ufb01rst assumption says that the occurence of the past and present events have\npositive impact on the occurence of the future events. The second assumption says\nthat as time evolves, the impact of the past events is decreasing. For most of the\nresults in this paper, these two assumptions may not be necessary. We nevertheless\nmake them to avoid some technical di\ufb03culties.\nIf one looks at (1.2), it is clear that if you witness some events occuring, \u03bbt\nincreases since \u03bb(\u00b7) is increasing and you would expect even more events occuring.\nThis is called the self-exciting property. Because of this, you would expect to see\nsome clustering e\ufb00ects.\nFigure 1.1 shows the histograms of a Hawkes process and a usual Poisson pro-\ncess. A Poisson process is stationary with independent increments. On the con-\ntrary, the Hawkes process has dependent increments and has clustering e\ufb00ects. As\na result, in the picture, the Poisson process is more or less \ufb02at whilst the Hawkes\nprocess has peaks when it gets \u201cexcited\u201d and has valleys when it \u201ccools down\u201d.\nFigure 1.2 shows the plot of the intensity \u03bbt of a Hawkes process.\nUnlike the\nusual Poisson process for which the intensity is a positive constant, the intensity of\nHawkes process increases when you witness arrivals of points and it decays when\nthere are no arrivals of points.\n5\nThe self-exciting and clutstering properties of the Hawkes process make it ideal\nto characterize the correlations in some complex systems, including the default\nclustering e\ufb00ect in \ufb01nance.\nOne generalization of classical linear Hawkes process is the so-called multivari-\nate Hawkes process. We will de\ufb01ne the multivariate Hawkes process and discuss\nsome basic results in Section 1.6 of Chapter 1. The multivariate Hawkes process\nhas been well studied in the literature and we would like to point out that if you\nhave the result for the univariate Hawkes process, mathematically, it is not too\ndi\ufb03cult to generalize your result to multivariate Hawkes process.\nUnlike the univariate Hawkes process, which only has the self-exciting property,\nthe multivariate Hawkes process also has the mutually-exciting property. In the\ncontext of industry, consider that you have a large portfolio of companies, then the\nfailure of one company can have impact on the performance of other companies.\nIn other words, multivariate Hawkes process captures the cross-sectional clustering\ne\ufb00ect. That is why in most applications of Hawkes processes in \ufb01nance, people\nusually consider multivariate Hawkes processes. We will review some basic results\nabout multivariate linear Hawkes process in Chapter 1.\nAnother possible generalization to Hawkes process is the marked Hawkes pro-\ncess, i.e. Hawkes process with random marks. Just like univariate Hawkes process\nvesus multivariate Hawkes process, if you have the results in unmarked Hawkes\nprocess, usually it can be generalized to marked Hawkes process without much\ndi\ufb03culty. For instance, the large deviations for linear Hawkes process is proved\nin Bordenave and Torrisi [11] and the large deviations for linear marked Hawkes\nprocess is then proved in Karabash and Zhu [62]. We will discuss the details of\nlimit theorems of linear marked Hawkes process in Chapter 6.\n6\nMost of the literature on Hawkes processes studies only the linear case, which\nhas an immigration-birth representation (see Hawkes and Oakes [54]). The stabil-\nity, law of large numbers, central limit theorem, large deviations, Bartlett spectrum\netc. have all been studied and understood very well. Almost all of the applications\nof Hawkes processes in the literature consider exclusively the linear case. Daley\nand Vere-Jones [27] and Liniger [71] provide nice surveys about the theory and\napplications of Hawkes processes.\nOne special case of the Hawkes process is when the exciting function h(\u00b7) is\nexponential. In this case, the Hawkes process is a continuous time Markov process.\nIf \u03bb(\u00b7) is linear, the process is a special case of a\ufb03ne jump-di\ufb00usion process and\nis analytically tractable. This special case was for example studied in Oakes [85]\nand Errais et al. [36].\nBecause of the lack of computational tractability and immigration-birth repre-\nsentation, nonlinear Hawkes process is much less studied. However, some e\ufb00orts\nhave already been made in this direction. For instance, see Br\u00b4emaud and Mas-\nsouli\u00b4e [14] for stability results, and Bremaud et al. [15] for the rate of convergence\nto stationarity. Karabash [63] recently proved the stability results for a wider class\nof nonlinear Hawkes processes.\nAs to the limit theorems, Bacry et al. [2] proved the central limit theorem for\nlinear Hawkes process and Bordenave and Torrisi [11] proved the large deviation\nprinciple for linear Hawkes process.\nFor nonlinear Hawkes process, there is no explicit expression for the variance\nin the central limit theorem or the rate function for the large deviation principle.\nThe method is more abstract and much more involved. Zhu [114] proved a cen-\ntral limit theorem for ergodic nonlinear Hawkes processes. Zhu [112] studied the\n7\n0\n5\n10\n15\n20\n25\n30\n2\n4\n6\n8\n10\n12\n0\n5\n10\n15\n20\n25\n30\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nFigure 1.1: This is a comparison of a Hawkes process with a Poisson process. The\n\ufb01gure on the left shows the histogram of a Hawkes process with h(t) =\n1\n(t+1)2 and\n\u03bb(z) = 1 + 9\n10z and the \ufb01gure on the right the histogram of a Poisson process with\nconstant intensity \u03bb \u2261\n3\n2. In the \ufb01gure, each column represents the number of\npoints that arrived in that unit time subinterval.\nlarge deviations in the Markovian case, i.e. when h(\u00b7) is exponential or sum of\nexponentials. And Zhu [113] proved the large deviation principle for more general\nnonlinear Hawkes processes at the process-level, i.e. level-3.\n0\n5\n10\n15\n20\n25\n30\n0\n2\n4\n6\n8\n10\n12\n14\nFigure 1.2: Plot of intensity \u03bbt for a realization of Hawkes process. Here h(t) =\n1\n(t+1)2 and \u03bb(z) = 1 + 0.9z.\n8\n1.2\nApplications of Hawkes Processes\n1.2.1\nApplications in Finance\nThe applications of Hawkes processes in \ufb01nance include market orders mod-\nelling, see e.g. Bauwens and Hautsch [7], Bowsher [13], Hewlett [56], Large [67] and\nCartea et al. [22]; value-at-risk, see e.g. Chavez-Demoulin et al. [23]; and credit\nrisk, see e.g. Errais et al. [36]. Embrechts et al. [35] applied Hawkes processes to\nmodel the \ufb01nancial data. Muni Toke and Pomponio [83] applied Hawkes processes\nto model the trade-through. Bacry et al. [3] used Hawkes processes to reproduce\nempirically microstructure noise and discussed the Epps e\ufb00ect and lead-lag. The\nself-exciting and clustering properties of Hawkes processes are especially appealing\nin \ufb01nancial applications.\nCurrently, most of the applications of Hawkes process in the \ufb01nance literature\nare about market orders modelling, see e.g. Bauwens and Hautsch [7], Bowsher\n[13] and Large [67].\nRecently, Chavez-Demoulin and McGill [24] used Hawkes processes to study\nthe extremal returns in high-frequency trading.\nThe Hawkes process captures\nthe volatility clustering behavior of the intraday extremal returns. and provides\na suitable estimation of high-quantile based risk measures (e.g.\nVaR, ES) for\n\ufb01nancial time series.\nFilimonov and Sornette [39] used Hawkes process to model market events, with\nthe aim of quantifying precisely endogeneity and exogeneity in market activity. By\nusing Hawkes process, Filimonov and Sornette [39] analyzed E-mini S&P futures\ncontract over the period 1998-2010 and discovered that the degree of self-re\ufb02exivity\nhas increased steadily in the last decade, an e\ufb00ect they attribute to the increased\n9\ndeployment of high-frequency and algorithmic trading. When they calibrated over\nmuch shorter time intervals (10 minutes), the Hawkes process analysis is found to\ndetect precursors of the \ufb02ash-crash that happened on May 6th, 2010. An early\ndetection can bene\ufb01t market regulators.\nVery recently, Hardiman et al. [50] used (linear) Hawkes process to model the\narrival of mid-price changes in the E-Mini S&P futures contract. Using several\nestimation methods, they found that the exciting function h(\u00b7) has a power-law\ndecay and \u2225h\u2225L1 is close to 1. They pointed out that markets are and have always\nbeen close to criticality, challenging the studies of Filimonov and Sornette [39]\nwhich indicates that self-re\ufb02exivity (endogeneity) has increased in recent years as\na result of increased automation of trading.\nEgami et al. [33] studied the credit default swap (CDS) markets in both Japan\nand U.S. They made a dynamic analysis of the bid-ask spreads in both countries,\nwhich surged dramatically during the 2008-2009 \ufb01nancial crisis and they used the\nHawkes process to predict the bid-ask spreads.\nAs pointed out in Errais et al. [36], \u201cThe collapse of Lehman Brothers brought\nthe \ufb01nancial system to the brink of a breakdown. The dramatic repercussions point\nto the exisence of feedback phenomena that are channeled through the complex\nweb of informational and contractual relationships in the economy...\nThis and\nrelated episodes motivate the design of models of correlated default timing that\nincorporate the feedback phenomena that plague credit markets.\u201d According to\nPeng and Kou [92], \u201cWe need better models to incorporate the default clustering\ne\ufb00ect, i.e., one default event tends to trigger more default events both across\ntime and cross-sectionally.\u201d The Hawkes process provides a model to characterize\ndefault events across time and if one uses a multivariate Hawkes process, that\n10\nwould describe the cross-sectional clustering e\ufb00ect as well.\nHawkes processes have been proposed as models for the arrival of company\ndefaults in a bond portfolio, starting with the papers Giesecke and Tomecek [42]\nand Giesecke et al. [41]. It is not hard to see that when the exciting function h(\u00b7) is\nexponential, the linear Hawkes processes are a\ufb03ne jump-di\ufb00usion processes, see for\ninstance Errais et al. [36]. With the help of the theory of a\ufb03ne jump-di\ufb00usions, one\ncan then analyze price processes related to certain credit derivatives analytically.\n1.2.2\nApplications in Sociology\nThe Hawkes process has also been applied to the study of social interactions.\nCrane and Sornette [26] analysed the viewing of YouTube videos as an example of a\nnonlinear social system. They identi\ufb01ed peaks in the time series of viewing \ufb01gures\nfor around half a million videos and studied the subsequent decay of the peak to\na background viewing level. In Crane and Sornette [26], the Hawkes process was\nproposed as a model of the video-watching dynamics, and a plausible link made to\nthe social interactions that create strong correlations between the viewing actions\nof di\ufb00erent people. Individual viewing is not random but in\ufb02uenced by various\nchannels of communication about what to watch next. Mitchell and Cates [77]\nused computer simulation to test the the claims in Crane and Sornette [26] that\nrobust identi\ufb01cation is possible for classes of dynamic response following activity\nbursts. They also pointed out some limitations of the analysis based on the Hawkes\nprocess.\nIn sociology, Hawkes process has also been used by Blundell et al.\n[10] to\nstudy the reciprocating relationships. Reciprocity is a common social norm, where\none person\u2019s actions towards another increases the probability of the same type of\n11\naction being returned, e.g., if Bob emails Alice, it increases the probability that\nAlice will email Bob in the near future.\nThe mutually-exciting processes, e.g.\nmultivariate Hawkes processes, are able to capture the causal nature of reciprocal\ninteractions.\n1.2.3\nApplications in Seismology\nOgata [87] used a particular case of the Hawkes process to predict earthquakes\nand the Hawkes process appears to be superior to other models by residual analysis.\nThe speci\ufb01c model used by Ogata [87] is now known as ETAS (Epidemic Type\nAftershock-Sequences) model. The discussions of ETAS model can be found in\nDaley and Vere-Jones [27].\n1.2.4\nApplications in Genome Analysis\nGusto and Schbath [46] used the Hawkes process to model the occurences along\nthe genome and studied how the occurences of a given process along a genome,\ngenes or motifs for instance, may be in\ufb02uenced by the occurrences of a second\nprocess. More precisely, the aim is to detect avoided and/or favored distances\nbetween two motifs, for instance, suggesting possible interactions at a molecular\nlevel. The statistical method proposed by Gusto and Schbath [46] is useful for\nfunctional motif detection or to improve knowledge of some biological mechanisms.\nReynaud-Bouret and Schbath [96] provided a new method for the detection of\neither favored or avoided distances between genomic events along DNA sequences.\nThese events are modeled by the Hawkes process. The biological problem is actu-\nally complex enough to need a non-asymptotic penalized model selection approach\nand Reynaud-Bouret and Schbath [96] provided a theoretical penalty that satis\ufb01es\n12\nan oracle inequality even for quite complex families of models.\n1.2.5\nApplications in Neuroscience\nChornoboy et al. [25] used the Hawkes process to detect and model the func-\ntional relationships between the neurons. The estimates are based on the maximum\nlikelihood principle.\nIn most neural systems, neurons communicate via sequences of action poten-\ntials. Johnson [59] used various point processes, including Poisson process, renewal\nprocess and the Hawkes process and showed that neural discharges patterns convey\ntime-varying information intermingled with the neuron\u2019s response characteristics.\nBy applying information theory and estimation theory to point processes, Johnson\n[59] described the fundamental limits on how well information can be extracted\nfrom neural discharges.\nMore recently, Pernice et al. [93] and Pernice et al. [94] have used Hawkes\nprocess to model the spike train dynamics in the studies of neuronal networks.\nAs pointed out in Pernice et al. [93], \u201cHawkes\u2019 point process theory allows the\ntreatment of correlations on the level of spike trains as well as the understanding\nof the relation of complex connectivity patterns to the statistics of pairwise cor-\nrelations.\u201d Reynaud et al. [98] proposed new non-parametric adaptive estimation\nmethods and adapted other recent similar results to the setting of spike trains anal-\nysis in neuroscience. They tested homogeneuous Poisson process, inhomogeneous\nPoisson process and the Hawkes process. A complete analysis was performed on\nsingle unit activity recorded on a monkey during a sensory-motor task. Reynaud\net al. [98] showed that the homogeneous Poisson process hypothesis is always re-\njected and that the inhomogeneous Poisson process hypothesis is rarely accepted.\n13\nThe Hawkes model seems to \ufb01t most of the data.\nThe application of the Hawkes process in neuroscience has also been mentioned\nin Br\u00b4emaud and Massouli\u00b4e [14].\n1.2.6\nApplications in Criminology\nHawkes processes have also been used in criminology. Violence among gangs\nexhibits retaliatory behavior, i.e. given that an event has happened between two\ngangs, the likelihood that another event will happen shortly afterwards is increased.\nA problem like this can be modeled naturally by a self-exciting point process.\nMohler et al. [78] and Egesdal et al. [34] have successfully modeled the pairwise\ngang violence as a Hawkes process. As pointed out in Hegemann et al. [55], in\nreal-life situations, data is incomplete and law-enforcement agencies may not know\nwhich gang is involved. However, even when gang activity is highly stochastic, lo-\ncalized excitations in parts of the known dataset can help identify gangs responsible\nfor unsolved crimes. The works before Hegemann et al. [55] incorporated the ob-\nserved clustering in time of the data to identify gangs responsible for unsolved\ncrimes by assuming that the parameters of the model are known, when in reality\nthey have to be estimated from the data itself. Hegemann et al. [55] proposed an\niterative method that simultaneously estimates the parameters in the underlying\npoint process and assigns weights to the unknown events with a directly calculable\nscore function.\nHawkes processses have also been used in the studies of terrorist activities. For\nexample, Porter and White [95] used Hawkes process to examine the daily number\nof terrorist attacks in Indonesia from 1994 through 2007. Their model explains the\nself-exciting nature of the terrorist activities. It estimates the probability of future\n14\nattacks as a function of the times since the past attacks.\nLewis et al.\n[69] used Hawkes process to model the temporal dynamics of\nviolence and civilian deaths in Iraq.\n1.3\nRelated Models\nThere are other generalizations or variations of the Hawkes processes in the\nliterature. For example, Bormetti et al. [12] introduced a one factor model where\nboth the factor and the idiosyncratic jump components are described by a Hawkes\nprocess. Their model is a better candidate than classical Poisson or Hawkes models\nto describe the dynamics of jumps in a multi-asset framework. Another example\nis a multivariate Hawkes process with constraints on its conditional density intro-\nduced by Zheng et al. [111]. Their study is mainly motivated by the stochastic\nmodelling of a limit order book for high frequency \ufb01nancial data analysis. Dassios\nand Zhao [28] proposed a dynamic contagion process. It is basically a combination\nof a marked Hawkes process with exponential exciting function and an external\nshot noise process. Their model is Markovian. They also applied their model to\ninsurance, see e.g. Dassios and Zhao [29].\nIn [116], Zhu incorporated Hawkes jumps into the classical Cox-Ingersoll-Ross\nmodel and obtained limit theorems and various other properties.\nIn seismology, Wang et al.\n[104] proposed a new model, i.e.\nthe Markov-\nmodulated Hawkes process with stepwise decay (MMHPSD), to investigate the\nvariation in seismicity rate during a series of earthquakes sequence including mul-\ntiple main shocks. The MMHPSD is a self-exciting process which switches among\ndi\ufb00erent states, in each of which the process has distinguishable background seis-\n15\nmicity and decay rates. Stress release models are often used in seismology. In\nBr\u00b4emaud and Foss [17], they created a new earthquake model combining the clas-\nsical stress release model for primary shocks with the Hawkes model for aftershocks\nand studied the ergodicity of this new model.\nIn addition to the classical Hawkes process, one can also study the spatial\nHawkes process, see e.g. M\u00f8ller and Torrisi [81], M\u00f8ller and Torrisi [82], Bordenave\nand Torrisi [11]. In addition, the space-time Hawkes process has been used, see\ne.g. Musmeci and Vere-Jones [84] and Ogata [88].\n1.4\nLinear Hawkes Processes\nIn this section, let us review some known results about linear Hawkes process.\nUnlike the nonlinear Hawkes process, the linear Hawkes process has been very well\nstudied in the literature. Hawkes and Oakes [54] introduced an immigration-birth\nrepresentation of the linear Hawkes process, which can be viewed as a special case\nof the Poisson cluster process. The stability results of the linear Hawkes process,\ni.e. existence and uniqueness of a stationary linear Hawkes process have been sum-\nmarised in Chapter 12 of Daley and Vere-Jones [27]. The rate of convergence to\nequilibrium has been stuided by Br\u00b4emaud et al. [15]. The second-order analysis,\ni.e. the Bartlett spectrum etc. have been studied in Hawkes [51] and Hawkes [52].\nReynaud-Bouret and Roy [97] considered the linear Hawkes process as a special\ncase of Poisson cluster process and studied the non-asymptotic tail estimates of\nthe extinction time, the length of a cluster, and the number of points in an inter-\nval. Reynaud-Bouret and Roy [97] also obtained some so-called non-asymptotic\nergodic theorems. The limit theorems have also been studied for linear Hawkes\n16\nprocess. The central limit theorem was considered in Bacry et al. [2], the large\ndeviation principle was obtained in Bordenave and Torrisi [11], and very recently\nthe moderate deviation principle was proved in Zhu [115]. The simulations and\ncalibrations of linear Hawkes process have been studied in Ogata [89], M\u00f8ller and\nRasmussen [80], [79], Vere-Jones [110], Ozaki [91] and many others.\n1.4.1\nImmigration-Birth Representation\nConsider the linear Hawkes process N with empty history, i.e. N(\u2212\u221e, 0] = 0\nand intensity\n(1.4)\n\u03bbt = \u03bd +\nZ t\n0\nh(t \u2212u)N(du),\n\u03bd > 0,\nwhere\nR \u221e\n0 h(t)dt < 1. It is well known that it has the following immigration-birth\nrepresentation; see for example Hawkes and Oakes [54]. The immigrant arrives\naccording to a homogeneous Poisson process with constant rate \u03bd. Each immigrant\nreproduces children and the number of children has a Poisson distribution with\nparameter \u2225h\u2225L1. Conditional on the number of the children of an immigrant,\nthe time that a child was born has probability density function\nh(t)\n\u2225h\u2225L1 . Each child\nproduces children according to the same laws, independent of other children. All\nthe immigrants produce children independently. Now, N(0, t] is the same as the\ntotal number of immigrants and children in the time interval (0, t].\n17\n1.4.2\nStability Results\nConsider the linear Hawkes process N with empty history, i.e. N(\u2212\u221e, 0] = 0\nand intensity\n(1.5)\n\u03bbt = \u03bd +\nZ t\n0\nh(t \u2212u)N(du),\nwhere\nR \u221e\n0 h(t)dt < 1. We review here the known results of existence and uniqueness\nof a stationary version of the process. We follow the arguments of Chapter 12 of\nDaley and Vere-Jones [27].\nThe existence of a stationary version of the process can be seen from the\nimmigration-birth representation of the linear Hawkes process. To show unique-\nness, let us do the following. Let N \u2020 be a stationary version with intensity\n(1.6)\n\u03bb\u2020\nt = \u03bd +\nZ t\n\u2212\u221e\nh(t \u2212u)N \u2020(du),\nand mean intensity \u00b5 := E[\u03bb\u2020\nt] =\n\u03bd\n1\u2212\u2225h\u2225L1 . For both N and N \u2020, we consider the\nshifted versions \u03b8sN and \u03b8sN \u2020 that bring the origin back to zero. \u03b8sN \u2020 can be split\ninto two components, the one with the same structure as \u03b8sN, being generated from\nthe clusters initiated by immigrants arriving after time \u2212s and the component N \u2020\n\u2212s\nthat counts the children of the immigrants that arrived before time \u2212s. On R+,\nthe contribution from the latter form a Poisson process with intensity\n(1.7)\n\u03bb\u2020\n\u2212s(t) =\nZ \u2212s\n\u2212\u221e\nh(t \u2212u)N \u2020\n\u2212s(du).\n18\nFor any T < \u221e,\nP(N \u2020\n\u2212s(0, T) > 0) = E\nh\n1 \u2212e\u2212\nR T\n0 \u03bb\u2020\n\u2212s(t)dti\n(1.8)\n\u2264E\n\u0014Z T\n0\n\u03bb\u2020\n\u2212s(t)dt\n\u0015\n\u2264\u00b5T\nZ \u221e\ns\nh(u)du \u21920,\nas s \u2192\u221e. Let P and P\u2020 represent the probability measures corresponding to N\nand N \u2020. For any T > 0, we have\n(1.9)\n\u2225\u03b8\u2212sP \u2212P\u2020\u2225[0,T] \u2264P(N \u2020\n\u2212s(0, T) > 0) \u21920,\nas s \u2192\u221e, where \u2225\u00b7 \u2225denotes the variation norm. This implies the weak conver-\ngence and thus the weak asymptotic stationarity of N.\nUnder a stronger assumption\nR \u221e\n0 th(t)dt < \u221e, i.e.\nthe mean time to the\nappearance of a child is \ufb01nite. Since the mean number of o\ufb00spring is also \ufb01nite\n(because \u2225h\u2225L1 < 1), the random time T from the appearance of an ancestor to\nthe last of its descendants has \ufb01nite mean, i.e. E[T] < \u221e. Thus, we have\n(1.10)\nP(N \u2020\n\u2212s[0, \u221e) > 0) = 1 \u2212e\u2212\u03bd\nR \u221e\ns\nP(T>u)du \u21920,\nas s \u2192\u221eand \u2225\u03b8\u2212sP \u2212P\u2020\u2225[0,\u221e] \u21920 as s \u2192\u221e, which implies that the process\nstarting from empty history is strongly asymptotically stationary.\nBr\u00b4emaud et al. [15] studied the rate of convergence to the equilibrium in a more\ngeneral setting, i.e. Hawkes process with random marks. Here, we only consider\nthe unmarked case. Assume N(\u2212\u221e, 0] = 0 and let N \u2020 denote the unique stationary\nHawkes process. The convergence in variation is seen via coupling, namely, N and\nN \u2020 are constructed on the same space and there exists a \ufb01nite random time T such\n19\nthat\n(1.11)\nP(N(t, \u221e) = N \u2020(t, \u221e) for all t \u2265T) = 1.\nIn the exponential case, there exists some \u03b2 > 0 such that\nR \u221e\n0 e\u03b2th(t)dt = 1.\nLet us de\ufb01ne\n(1.12)\nH(t) :=\n\u03bd\n1 \u2212\u2225h\u2225L1\nZ \u221e\nt\nh(s)ds.\nIf e\u03b2tH(t) is directly Riemann integrable on R+, then for any\n(1.13)\nK >\nR \u221e\n0 e\u03b2tH(t)dt\n\u03b2\nR \u221e\n0 te\u03b2th(t)dt,\nthere exists t0(K) such that P(T > t) \u2264Ke\u2212\u03b2t for any t \u2265t0(K).\nIn the subexponential case, the distribution function G with density g(t) =\nh(t)\n\u2225h\u2225L1 is subexponential, in the sense that,\n(1.14)\nlim\nt\u2192\u221e\n1 \u2212G\u2217n(t)\n1 \u2212G(t) = n,\nfor any n \u2208N.\nFurther assume that\nR \u221e\n0 th(t)dt < \u221e. Then, for any\n(1.15)\nK >\n\u03bd\u2225h\u2225L1\n(1 \u2212\u2225h\u2225L1)2,\nthere exists some t0(K) such that for any t \u2265t0(K), we have\n(1.16)\nP(T > t) \u2264K\nZ \u221e\nt\nG(u)du,\n20\nwhere G = 1 \u2212G.\n1.4.3\nBartlett Spectrum for Linear Hawkes Processes\nThe methods of analysis for point processes by spectrum were introduced by\nBartlett [5] and [6]. We refer to Chapter 8 of Daley and Vere-Jones [27] for a\ndetailed discussion.\nLet N be a second-order stationary point process on R. (For the de\ufb01nition\nof second-order stationary point process, we refer to Daley and Vere-Jones [27].)\nDe\ufb01ne the set S as the space of functions of rapid decay, i.e. \u03c6 \u2208S if\n(1.17)\n\f\f\f\f\ndk\u03c6(x)\ndxk\n\f\f\f\f \u2264\nC(k, r)\n(1 + |x|)r ,\nfor some constants C(k, r) < \u221eand all positive integers r and k.\nFor bounded measurable \u03c6 with bounded support and also \u03c6 \u2208S, there exists\na measure \u0393 on B such that\n(1.18)\nVar\n\u0012Z\nR\n\u03c6(x)N(dx)\n\u0013\n=\nZ\nR\n|\u02c6\u03c6(\u03c9)|\u0393(d\u03c9),\nwhere \u02c6\u03c6(\u03c9) =\nR\nR ei\u03c9u\u03c6(u)du is the Fourier transform of \u03c6. \u0393 is refered to as the\nBartlett spectrum. We also have\n(1.19)\nCov\n\u0012Z\nR\n\u03c6(x)N(dx),\nZ\nR\n\u03c8(x)N(dx)\n\u0013\n=\nZ\nR\n\u02c6\u03c6(\u03c9) \u02c6\u03c8(\u03c9)\u0393(d\u03c9).\nHawkes [52] proved that for the linear stationary Hawkes process with\n(1.20)\n\u03bbt = \u03bd +\nZ t\n\u2212\u221e\nh(t \u2212s)N(ds),\n21\n\u03bd > 0 and \u2225h\u2225L1 < 1, the Bartlett spectrum is given by\n(1.21)\n\u0393(d\u03c9) =\n\u03bd\n2\u03c0(1 \u2212\u2225h\u2225L1)|1 \u2212\u02c6h(\u03c9)|2d\u03c9.\nMoreover, if \u00b5(\u03c4) := E[dN(t + \u03c4)dN(t)]/(dt)2 \u2212\u00b52 is the covariance density,\nwhere \u00b5 :=\n\u03bd\n1\u2212\u2225h\u2225L1 , then Hawkes [51] proved that \u00b5(\u03c4) = \u00b5(\u2212\u03c4), \u03c4 > 0, satis\ufb01es\nthe equation\n(1.22)\n\u00b5(\u03c4) = \u00b5h(\u03c4) +\nZ \u03c4\n\u2212\u221e\nh(t \u2212v)\u00b5(v)dv.\nSince \u00b5(\u03c4) = \u00b5(\u2212\u03c4), we have\n(1.23)\n\u00b5(\u03c4) = \u00b5h(\u03c4) +\nZ \u221e\n0\nh(\u03c4 + v)\u00b5(v)dv +\nZ \u03c4\n0\nh(\u03c4 \u2212v)\u00b5(v)dv,\n\u03c4 > 0.\nIn general, \u00b5(\u03c4) may not have an analytical form. However, when h(\u00b7) is exponen-\ntial, say h(t) = \u03b1e\u2212\u03b2t, Hawkes [51] showed that\n(1.24)\n\u00b5(\u03c4) = \u03bd\u03b1\u03b2(2\u03b2 \u2212\u03b1)\n2(\u03b2 \u2212\u03b1)2\ne\u2212(\u03b2\u2212\u03b1)\u03c4,\n\u03c4 > 0.\nThe Bartlett spectrum analysis has later been generalized to marked linear\nHawkes processes and some more general models. We refer to Br\u00b4emaud and Mas-\nsouli\u00b4e [18] and Br\u00b4emaud and Massouli\u00b4e [19].\n1.4.4\nLimit Theorems for Linear Hawkes Processes\nWhen \u03bb(\u00b7) is linear, say \u03bb(z) = \u03bd + z, for some \u03bd > 0 and \u2225h\u2225L1 < 1, the\nHawkes process has a very nice immigration-birth representation, see for example\n22\nHawkes and Oakes [54]. For such a linear Hawkes process, the limit theorems are\nvery well understood. Consider a stationary Hawkes process N \u2020 with intensity\n(1.25)\n\u03bb\u2020\nt = \u03bd +\nZ t\n\u2212\u221e\nh(t \u2212s)N \u2020(ds).\nTaking expecatations on the both sides of the above equation and using stationar-\nity, we get\n(1.26)\n\u00b5 := E[\u03bb\u2020\nt] = \u03bd +\nZ t\n\u2212\u221e\nh(t \u2212s)E[\u03bb\u2020\ns]ds = \u03bd + \u00b5\u2225h\u2225L1,\nwhich implies that \u00b5 =\n\u03bd\n1\u2212\u2225h\u2225L1 . By ergodic theorem, we have\n(1.27)\nNt\nt \u2192\n\u03bd\n1 \u2212\u2225h\u2225L1 ,\nas t \u2192\u221ea.s.\nMoreover, Bordenave and Torrisi [11] proved a large deviation principle for (Nt\nt \u2208\u00b7).\nTheorem 1 (Bordenave and Torrisi 2007). (Nt/t \u2208\u00b7) satis\ufb01es a large deviation\nprinciple with the rate function\n(1.28)\nI(x) =\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f3\nx log\n\u0010\nx\n\u03bd+x\u2225h\u2225L1\n\u0011\n\u2212x + x\u2225h\u2225L1 + \u03bd\nif x \u2208[0, \u221e)\n+\u221e\notherwise\n.\nRecently, Bacry et al. [2] proved a functional central limit theorem for linear\nmultivariate Hawkes process under certain assumptions. That includes the linear\nHawkes process as a special case and they proved that\n23\nTheorem 2 (Bacry et al. 2011).\n(1.29)\nN\u00b7t \u2212\u00b7\u00b5t\n\u221a\nt\n\u2192\u03c3B(\u00b7),\nas t \u2192\u221e,\nwhere B(\u00b7) is a standard Brownian motion. The convergence is weak convergence\non D[0, 1], the space of c\u00b4adl\u00b4ag functions on [0, 1], equipped with Skorokhod topology.\nHere,\n(1.30)\n\u00b5 =\n\u03bd\n1 \u2212\u2225h\u2225L1\nand\n\u03c32 =\n\u03bd\n(1 \u2212\u2225h\u2225L1)3.\nUnlike the central limit theorem and the law of the iterated logarithm, there are\nnot as many good crietria one can use to prove the moderate deviation principle for\nnonlinear Hawkes processes, which would \ufb01ll in the gap between the central limit\ntheorem and the large deviation principle.\nNevertheless, due to the analytical\ntractability and birth-immigration representation of linear Hawkes process, Zhu\n[115] proved the moderate deviations for linear Hawkes processes.\nTheorem 3. Assume \u03bb(z) = \u03bd+z, \u03bd > 0, \u2225h\u2225L1 < 1 and supt>0 t3/2h(t) = C < \u221e.\nFor any Borel set A and time sequence a(t) such that\n\u221a\nt \u226aa(t) \u226at, we have the\nfollowing moderate deviation principle.\n\u2212inf\nx\u2208Ao J(x) \u2264lim inf\nt\u2192\u221e\nt\na(t)2 log P\n\u0012Nt \u2212\u00b5t\na(t)\n\u2208A\n\u0013\n(1.31)\n\u2264lim sup\nt\u2192\u221e\nt\na(t)2 log P\n\u0012Nt \u2212\u00b5t\na(t)\n\u2208A\n\u0013\n\u2264\u2212inf\nx\u2208A J(x),\nwhere J(x) =\nx2(1\u2212\u2225h\u2225L1)3\n2\u03bd\n.\nThe proof of Theorem 3 will be given in Section 1.4.5.\n24\nIn a nutshell, linear Hawkes processes satisfy very nice limit theorems and the\nlimits can be computed more or less explicitly.\n1.4.5\nProof of Theorem 3\nSince a Hawkes process has a long memory and is in general non-Markovian,\nthere is no good criterion in the literature for moderate deviations that we can use\ndirectly. For example, Bacry et al. [2] used a central limit theorem for martingales\nto obtain a central limit theorem for linear Hawkes processes. But there is no\ncriterion for moderate deviations for martingales that can \ufb01t into the context of\nHawkes processes. Our strategy relies on the fact that for linear Hawkes processes\nthere is a nice immigration-birth representation from which we can obtain a semi-\nexplicit formula for the moment generating function of Nt in Lemma 1. A careful\nasymptotic analysis of this formula would lead to the proof of Theorem 3.\nProof of Theorem 3. Let us \ufb01rst prove that for any \u03b8 \u2208R,\n(1.32)\nlim\nt\u2192\u221e\nt\na(t)2 log E\nh\ne\na(t)\nt \u03b8(Nt\u2212\u00b5t)i\n=\n\u03bd\u03b82\n2(1 \u2212\u2225h\u2225L1)3.\nBy Lemma 1, for \ufb01xed \u03b8 \u2208R and t su\ufb03ciently large, we have\n(1.33)\nE\nh\ne\na(t)\nt \u03b8Nti\n= e\u03bd\nR t\n0 Gt(s)ds,\nwhere Gt(s) = e\na(t)\nt \u03b8+\nR s\n0 h(u)Gt(s\u2212u)du \u22121, 0 \u2264s \u2264t. Here, Gt(s) is simply the\nF(s) \u22121 in Lemma 1. Because a(t)\nt \u03b8 depends on t, we write Gt(s) instead of G(s)\nto indicate its dependence on t. Clearly, Gt(s) is increasing in s and Gt(\u221e) is the\nminimal solution to the equation xt = e\na(t)\nt \u03b8+\u2225h\u2225L1xt \u22121. (See the proof of Lemma\n25\n1 and the reference therein.) Since \u2225h\u2225L1 < 1, it is easy to see that xt = O(a(t)/t).\nSince xt = O(a(t)/t), we have Gt(s) = O(a(t)/t) uniformly in s.\nBy Taylor\u2019s\nexpansion,\nGt(s) = a(t)\u03b8\nt\n+\nZ s\n0\nh(u)Gt(s \u2212u)du\n(1.34)\n+ 1\n2\n\u0012a(t)\u03b8\nt\n\u00132\n+ 1\n2\n\u0012Z s\n0\nh(u)Gt(s \u2212u)du\n\u00132\n+ a(t)\u03b8\nt\nZ s\n0\nh(u)Gt(s \u2212u)du + O\n\u0000(a(t)/t)3\u0001\n.\nLet Gt(s) = a(t)\u03b8\nt G1(s) +\n\u0010\na(t)\nt\n\u00112\nG2(s) + \u03f5t(s), where\n(1.35)\nG1(s) := 1 +\nZ s\n0\nh(u)G1(s \u2212u)du,\nand\n(1.36)\nG2(s) :=\nZ s\n0\nh(u)G2(s \u2212u)du + \u03b82\n2 + \u03b82(G1(s) \u22121) + \u03b82\n2 (G1(s) \u22121)2.\nSubstituting (1.35) and (1.36) back into (1.34) and using the fact Gt(s) = O(a(t)/t)\nuniformly in s, we get \u03f5t(s) = O((a(t)/t)3) uniformly in s. Moreover, we claim that\nlim\nt\u2192\u221e\n1\na(t)\n\u0014\n\u03b8\u03bd\nZ t\n0\nG1(s)ds \u2212\u03b8\u00b5t\n\u0015\n= 0,\n(1.37)\nlim\nt\u2192\u221e\n1\nt\nZ t\n0\nG2(s)ds =\n\u03b82\n2(1 \u2212\u2225h\u2225L1)3.\n(1.38)\n26\nTo prove (1.37), notice \ufb01rst that\n1\nt\nZ t\n0\nG1(s)ds = 1 + 1\nt\nZ t\n0\nZ s\n0\nh(u)G1(s \u2212u)duds\n(1.39)\n= 1 + 1\nt\nZ t\n0\nh(u)\nZ t\nu\nG1(s \u2212u)dsdu\n= 1 + 1\nt\nZ t\n0\nh(u)\nZ t\u2212u\n0\nG1(s)dsdu\n= 1 + 1\nt\nZ t\n0\nh(u)\nZ t\n0\nG1(s)dsdu \u22121\nt\nZ t\n0\nh(u)\nZ t\nt\u2212u\nG1(s)dsdu.\nTherefore,\n(1.40)\n1\nt\nZ t\n0\nG1(s)ds =\n1 \u22121\nt\nR t\n0 h(u)\nR t\nt\u2212u G1(s)dsdu\n1 \u2212\nR t\n0 h(u)du\n.\nHence,\n1\na(t)\n\u0014\n\u03b8\u03bd\nZ t\n0\nG1(s)ds \u2212\u03b8\u00b5t\n\u0015\n(1.41)\n= \u03b8\u03bd\na(t)\nZ t\n0\n\u0012\nG1(s) \u2212\n1\n1 \u2212\u2225h\u2225L1\n\u0013\nds\n= \u03b8\u03bdt\na(t)\n\"\n1\n1 \u2212\nR t\n0 h(u)du\n\u2212\n1\n1 \u2212\nR \u221e\n0 h(u)du\n#\n\u2212\u03b8\u03bd\na(t)\nR t\n0 h(u)\nR t\nt\u2212u G1(s)dsdu\n1 \u2212\nR t\n0 h(u)du\n.\nFor the \ufb01rst term in (1.41), we have\n(1.42)\n\f\f\f\f\f\n\u03b8\u03bdt\na(t)\n\"\n1\n1 \u2212\nR t\n0 h(u)du\n\u2212\n1\n1 \u2212\nR \u221e\n0 h(u)du\n#\f\f\f\f\f \u2264|\u03b8|\u03bdt\na(t)\nR \u221e\nt\nh(u)du\n(1 \u2212\u2225h\u2225L1)2 \u21920,\nas t \u2192\u221e, since by our assumption, supt>0 t3/2h(t) \u2264C < \u221e, which implies that\nt\na(t)\nR \u221e\nt\nh(u)du \u2264\nt\na(t)\nR \u221e\nt\nC\nu3/2du = 2C\n\u221a\nt\na(t) \u21920 as t \u2192\u221e.\n27\nFor the second term in (1.41), we have\n(1.43)\nlim sup\nt\u2192\u221e\n\f\f\f\f\f\n\u03b8\u03bd\na(t)\nR t\n0 h(u)\nR t\nt\u2212u G1(s)dsdu\n1 \u2212\nR t\n0 h(u)du\n\f\f\f\f\f \u2264lim\nt\u2192\u221eG1(t) lim sup\nt\u2192\u221e\n|\u03b8|\u03bd\na(t)\nR t\n0 h(u)udu\n1 \u2212\u2225h\u2225L1 = 0.\nThis is because (1.35) is a renewal equation and \u2225h\u2225L1 < 1. By the application\nof the Tauberian theorem to the renewal equation, (see Chapters XIII and XIV of\nFeller [38]), limt\u2192\u221eG1(t) =\n1\n1\u2212\u2225h\u2225L1 . Moreover, our assumptions supt>0 t3/2h(t) \u2264\nC < \u221eand \u2225h\u2225L1 < \u221eimply that\n(1.44)\n1\na(t)\nZ t\n0\nh(u)udu \u2264\n1\na(t)\nZ 1\n0\nh(u)udu +\n1\na(t)\nZ t\n1\nC\nu1/2du \u21920,\nas t \u2192\u221e.\nTo prove (1.38), notice that limt\u2192\u221eG1(t) =\n1\n1\u2212\u2225h\u2225L1 and again by the appli-\ncation of the Tauberian theorem to the renewal equation, (see Chapters XIII and\nXIV of Feller [38]), we have\nlim\nt\u2192\u221e\n1\nt\nZ t\n0\nG2(s)ds = lim\nt\u2192\u221eG2(t)\n(1.45)\n= \u03b82\n2\n1 + 2\n\u0010\n1\n1\u2212\u2225h\u2225L1 \u22121\n\u0011\n+\n\u0010\n1\n1\u2212\u2225h\u2225L1 \u22121\n\u00112\n1 \u2212\u2225h\u2225L1\n=\n\u03b82\n2(1 \u2212\u2225h\u2225L1)3.\n28\nFinally, from (1.33) and the de\ufb01nitions of G1(s), G2(s) and \u03f5t(s), we have\nt\na(t)2 log E\nh\ne\na(t)\nt \u03b8(Nt\u2212\u00b5t)i\n(1.46)\n=\nt\na(t)2\u03bd\nZ t\n0\nGt(s)ds \u2212\u03b8\u00b5 t\na(t)\n=\n1\na(t)\n\u0014\n\u03bd\u03b8\nZ t\n0\nG1(s)ds \u2212\u03b8\u00b5t\n\u0015\n+ 1\nt \u03bd\nZ t\n0\nG2(s)ds +\nt\na(t)2\nZ t\n0\n\u03f5t(s)ds.\nHence, by (1.37), (1.38) and the fact that \u03f5t(s) = O((a(t)/t)3) uniformly in s, we\nconclude that, for any \u03b8 \u2208R,\n(1.47)\nlim\nt\u2192\u221e\nt\na(t)2 log E\nh\ne\na(t)\nt \u03b8(Nt\u2212\u00b5t)i\n=\n\u03bd\u03b82\n2(1 \u2212\u2225h\u2225L1)3.\nApplying the G\u00a8artner-Ellis theorem (see for example [30]), we conclude that, for\nany Borel set A,\n\u2212inf\nx\u2208Ao J(x) \u2264lim inf\nt\u2192\u221e\nt\na(t)2 log P\n\u0012Nt \u2212\u00b5t\na(t)\n\u2208A\n\u0013\n(1.48)\n\u2264lim sup\nt\u2192\u221e\nt\na(t)2 log P\n\u0012Nt \u2212\u00b5t\na(t)\n\u2208A\n\u0013\n\u2264\u2212inf\nx\u2208A J(x),\nwhere\n(1.49)\nJ(x) = sup\n\u03b8\u2208R\n\u001a\n\u03b8x \u2212\n\u03bd\u03b82\n2(1 \u2212\u2225h\u2225L1)3\n\u001b\n= x2(1 \u2212\u2225h\u2225L1)3\n2\u03bd\n.\nLemma 1. For \u03b8 \u2264\u2225h\u2225L1 \u22121 \u2212log \u2225h\u2225L1,\n(1.50)\nE[e\u03b8Nt] = e\u03bd\nR t\n0 (F(s)\u22121)ds,\n29\nwhere F(s) = e\u03b8+\nR s\n0 h(u)(F(s\u2212u)\u22121)du for any 0 \u2264s \u2264t.\nProof. The Hawkes process has a very nice immigration-birth representation, see\nfor example Hawkes and Oakes [54]. The immigrant arrives according to a homo-\ngeneous Poisson process with constant rate \u03bd. Each immigrant produces a number\nof children, this being Poisson distributed with parameter \u2225h\u2225L1. Conditional on\nthe number of the children of an immigrant, the time that a child is born has\nprobability density function\nh(t)\n\u2225h\u2225L1 . Each child produces children according to the\nsame laws independent of other children. All the immigrants produce children\nindependently. Let F(t) = E[e\u03b8S(t)], where S(t) is the number of descendants an\nimmigrant generates up to time t. Hence, we have\nE\n\u0002\ne\u03b8Nt\u0003\n=\n\u221e\nX\nk=0\n(\u03bdt)k\nk! e\u2212\u03bdt\n1\ntk/k!\nZ\n\u00b7 \u00b7 \u00b7\nZ\nt1<t2<\u00b7\u00b7\u00b7<tk\nF(t1) \u00b7 \u00b7 \u00b7 F(tk)dt1 \u00b7 \u00b7 \u00b7 dtk\n(1.51)\n= e\u03bd\nR t\n0 (F(s)\u22121)ds.\nBy page 39 of Jagers [58], for all \u03b8 \u2208(\u2212\u221e, \u2225h\u2225L1 \u22121 \u2212log \u2225h\u2225L1], E[e\u03b8S(\u221e)] is\nthe minimal positive solution of\n(1.52)\nE[e\u03b8S(\u221e)] = e\u03b8 exp\n\b\n\u00b5(E[e\u03b8S(\u221e)] \u22121)\n\t\n.\nLet K be the number of children of an immigrant and let S(1)\nt , S(2)\nt , . . . , S(K)\nt\nbe\nthe number of descendants of immigrant\u2019s kth child that were born before time t\n30\n(including the kth child if and only if it was born before time t). Then\nF(t) =\n\u221e\nX\nk=0\nE\n\u0002\ne\u03b8S(t)|K = k\n\u0003\nP(K = k)\n(1.53)\n= e\u03b8\n\u221e\nX\nk=0\nE\nh\ne\u03b8S(1)\nt\nik\nP(K = k)\n= e\u03b8\n\u221e\nX\nk=0\n\u0012Z t\n0\nh(s)\n\u2225h\u2225L1 F(t \u2212s)ds\n\u0013k\ne\u2212\u2225h\u2225L1 \u2225h\u2225k\nL1\nk!\n= e\u03b8+\nR t\n0 h(s)(F(t\u2212s)\u22121)ds.\n1.4.6\nSimulations and Calibrations\nAssume the past of a Hawkes process is known up to present time zero, say the\ncon\ufb01guration of the history is \u03c9\u2212. Let \u03c41 be the \ufb01rst jump after time zero. Then,\nit is easy to see that\n(1.54)\nP(\u03c41 \u2265t) = e\u2212\nR t\n0 \u03bb\u03c9\u2212\ns\nds,\nwhere \u03bb\u03c9\u2212\ns\n= \u03bd + P\n\u03c4\u2208\u03c9\u2212h(s \u2212\u03c4). This leads to a straight forward simulation\nmethod which is applicable for any simple point process. This algorithm and its\ntheoretical foundation go back to a thinning procedure given Lewis and Shedler\n[70]. In the context of Hawkes processes, this simulation method was \ufb01rst used in\nOgata [89]. It is sometimes called Ogata\u2019s modi\ufb01ed thinning algorithm.\nIf we want to simulate the stationary version of the Hawkes process on a \ufb01nite\ntime interval, then the standard method for the simulation method described above\ndoes not work as the past of the process is not known and cannot be simulated, at\n31\nleast not completely.\nIf one ignores the past of the process and simply starts to simulate the process\nat some given time, one speaks about an approximate simulation. In this case,\none is actually simulating a transient version and not the stationary version of the\nprocess. But if one simulates for a long enough time interval, then the transient\nversion converges to the stationary one. Such an approximate simulation method\nof Hawkes processes was discussed in M\u00f8ller and Rasmussen [80]. A simulation\nmethod which directly simulates the stationary version without approximation is a\nso-called perfect simulation method. The idea is to incorporate somehow the e\ufb00ect\nof past observations without actually simulating the past of the process. For point\nprocesses, this type of simulation has \ufb01rst been described in Brix and Kendall [20].\nIn the context of Hawkes processes, the perfect simulation method was discussed\nin M\u00f8ller and Rasmussen [79].\nThe calibrations, i.e. the estimation of the parameters of Hawkes processes,\nwas \ufb01rst studied in Vere-Jones [110] and Ozaki [91], based on a maximum likeli-\nhood method for point processes introduced by Rubin [99]. The properties of the\nmaximum likelihood estimator was analyzed in Ogata [86].\nIn Marsan and Lengline [73], an Expectation-Maximization (EM) algorithm,\ncalled \u201cModel Independent Stochastic Declustering\u201d (MISD), is introduced for the\nnonparametric estimation of self-exciting point processes with time-homogeneous\nbackground rate (For linear Hawkes process with intensity \u03bbt = \u03bdt +P\n\u03c4<t h(t\u2212\u03c4),\n\u03bdt is the background rate and h(\u00b7) is the exciting function).\nThe e\ufb03cacy of the MISD algorithm was studied in Sornette and Utkin [101],\nwhere the authors found that the ability of MISD to recover key parameters such as\n\u2225h\u2225L1 depends on the values of the model parameters. In particular, they pointed\n32\nout that the accuracy of MISD improves as the timescale over which the exciting\nfunction h(\u00b7) decays shortens. In Lewis and Mohler [68], they introduced a Max-\nimum Penalized Likelihood Estimation (MPLE) approach for the nonparametric\nestimation of Hawkes processes. The method is capable of estimating \u03bdt and h(t)\nsimultaneously, without prior knowledge of their form. Analogous to MPLE in\nthe context of density estimation, the added regularity of the estimates allows for\nhigher accuracy and/or lower sample sizes in comparison to MISD.\n1.5\nNonlinear Hawkes Processes\nConsider a simple point process with intensity\n(1.55)\n\u03bbt = \u03bb\n\u0012Z t\n\u2212\u221e\nh(t \u2212s)N(ds)\n\u0013\n,\nwhere \u03bb(\u00b7) : R+ \u2192R+ and h(\u00b7) : R+ \u2192[0, \u221e). Br\u00b4emaud and Massouli\u00b4e [14]\nstudied the existence and uniqueness of a stationary nonlinear Hawkes process\nthat satis\ufb01es the dynamics (1.55) as well as its stability in distribution and in\nvariation. They allow h(\u00b7) to take negative values as well. In this thesis, we always\nconsider h(\u00b7) to be nonnegative.\nThe following result is about the existence of a stationary nonlinear Hawkes\nprocess satisfying the dynamics (1.55). We do not need \u03bb(\u00b7) to be Lipschitz.\nTheorem 4 (Br\u00b4emaud and Massouli\u00b4e [14]). Let \u03bb(\u00b7) be a nonnegative, nondecreas-\ning and left-continuous function, satisfying \u03bb(z) \u2264C +\u03b1z for any z \u22650, for some\nC > 0 and \u03b1 \u22650 and let h(\u00b7) : R+ \u2192R+ be such that \u03b1\nR \u221e\n0 h(t)dt < 1. Then\nthere exists a stationary point process N with dynamics (1.55).\n33\nThe following results concerns the uniqueness and stability in distribution and\nin variation of a nonlinear Hawkes process.\nTheorem 5 (Br\u00b4emaud and Massouli\u00b4e [14]). Let \u03bb(\u00b7) be \u03b1-Lipschitz such that\n\u03b1\u2225h\u2225L1 < 1.\n(i) There exists a unique stationary distribution of N with \ufb01nite average inten-\nsity E[N(O, 1]] and with dynamics (1.55).\n(ii) Let \u03f5a(t) :=\nR t\nt\u2212a\nR\nR\u2212h(s \u2212u)N(du)ds.\nThe dynamics (1.55) are stable\nin distribution with respect to either the initial condition (1.56) or the condition\n(1.57) below,\n(1.56)\nsup\nt\u22650\n\u03f5a(t) < \u221ea.s. and lim\nt\u2192\u221e\u03f5a(t) = 0 a.e. for every a > 0,\n(1.57)\nsup\nt\u22650\nE[\u03f5a(t)] < \u221eand lim\nt\u2192\u221eE[\u03f5a(t)] = 0 for every a > 0.\n(iii) The dynamics (1.55) are stable in variation with respect to the initial\ncondition,\n(1.58)\nZ\nR+ h(t)N[\u2212t, 0)dt =\nZ\nR+\nZ 0\n\u2212\u221e\nh(t \u2212s)N(ds) < \u221e, a.s.\nif we assume further that\nR \u221e\n0 th(t)dt < \u221e.\nMassouli\u00b4e [74] extended the stability results to nonlinear Hawkes processes with\nrandom marks. He also considered the Markovian case and proved stability results\nwithout the Lipschitz condition for \u03bb(\u00b7).\nVery recently, Karabash [63] proved stability results for a much wider class of\nnonlinear Hawkes process, including the case when \u03bb(\u00b7) is not Lipschitz.\n34\nMoreover, Br\u00b4emaud et al. [15] considered the rate of extinction for nonlin-\near Hawkes process, that is the rate of convergence to the equilibrium when the\nstationary process is an empty process. Indeed, they considered a more general\nsetting, i.e. Hawkes process with random marks. Let N be a nonlinear Hawkes\nprocess which is empty on (\u2212\u221e, 0], i.e. N(\u2212\u221e, 0] = 0 which satis\ufb01es the dynamics\n(1.59)\n\u03bbt := \u03bd(t) + \u03c6\n\u0012Z t\n0\nh(t \u2212s)N(ds)\n\u0013\n,\nwhere \u03bd : R+ \u2192R+ is locally integrable, \u03c6 : R \u2192[0, \u221e), \u03c6(0) = 0, \u03c6 is 1-\nLipschitz and h : R+ \u2192R is measurable and not necessarily nonnegative and\nR \u221e\n0 |h(t)|dt < 1. The unique stationary process N 0 corresponding to the dynamics\n(1.60)\n\u03c6\n\u0012Z t\n0\nh(t \u2212s)N 0(ds)\n\u0013\n,\nis the empty process. Assume\nR \u221e\n0 \u03bd(t)dt < \u221e,\nR \u221e\n0 th(t)dt < \u221eand t 7\u2192|h(t)| is\nlocally bounded.\nThen \u03b8tN converges in variation to the empty process. The convergence in\nvariation takes place via coupling in the sense that there exists a \ufb01nite random\ntime T so that,\n(1.61)\nP(N(t, \u221e) = 0 for any t \u2265T ) = 1.\nDepending on whether the tail of |h(t)| is exponential or subexponentail, the\nfollowing was obtained by Br\u00b4emaud et al. [15].\nIn the exponential case, let \u03b2 > 0 be such that\nR \u221e\n0 e\u03b2t|h(t)|dt = 1. Assume\n35\ne\u03b2t\u03bd(t) is directly Riemann integrable. Then, for any K with\n(1.62)\nK >\nR \u221e\n0 e\u03b2t\u03bd(t)dt\n\u03b2\nR \u221e\n0 te\u03b2t|h(t)|dt,\nthere exists t0(K), for any t \u2265t0(K),\n(1.63)\nP(T > t) \u2264Ke\u2212\u03b2t.\nIn the subexponential case, assume that distribution functinon G with density\ng(t) =\n|h(t)|\nR \u221e\n0\n|h(t)|dt is subexponential, \u03bd(\u00b7) is bounded and that B = lim supt\u2192\u221e\n\u03bd(t)\nG(t) <\n\u221e, where G = 1 \u2212G. Then for any\n(1.64)\nK >\nB\n1 \u2212\nR \u221e\n0 |h(t)|dt,\nthere exists t0(K) such that for any t \u2265t0,\n(1.65)\nP(T > t) \u2264K\nZ \u221e\nt\nG(s)ds.\nKwieci\u00b4nski and Szekli [66] considered the nonlinear Hawkes process as a special\ncase of self-exciting process. Let N(R+) be the space of point processes on R+,\nwhich can be regarded as an element of D(R+), the space of functions which\nare right-continuous with left limits, equipped with Skorohod topology. For any\n\u00b5, \u03bd \u2208N(R+), \u00b5 \u227aN \u03bd if \u00b5(B) \u2264\u03bd(B) for any bounded set B \u2208B(R+). For any\n\u00b5, \u03bd \u2208N(R+), \u00b5 \u227aD \u03bd if and only if (\u00b5t) \u227aD (\u03bdt) for the corresponding functions\n\u00b5t := \u00b5((0, t]), \u03bdt := \u03bd((0, t]) \u2208D(R+), i.e. \u00b5t \u2264\u03bdt for all t > 0.\nNow, for a simple point process N with intensity \u03bb(t, N) and compensator\n36\n\u039b(t, N) :=\nR t\n0 \u03bb(s, N)ds, we say that N is positively self-exciting w.r.t. \u227aN if for\nany \u00b5, \u03bd \u2208N(R+),\n(1.66)\n\u00b5 \u227aN \u03bd implies that for any t > 0, \u03bb(t, \u00b5) \u2264\u03bb(t, \u03bd),\nand N is positively self-exciting w.r.t. \u227aD if for any \u00b5, \u03bd \u2208N(R+),\n(1.67)\n\u00b5 \u227aD \u03bd implies that for any t > 0, \u039b(t, \u00b5) \u2264\u039b(t, \u03bd).\nKwieci\u00b4nski and Szekli [66] pointed out that if h(\u00b7) is nonnegative and \u03bb(\u00b7)\nnondecreasing, then N is positively self-exciting with respect to \u227aN, and that\nif h(\u00b7) is nonnegative and nondecreasing and \u03bb(\u00b7) is nondecreasing, then N is\npositively self-exciting with respect to \u227aD.\nLet (\u2126, F) be a Polish space with a closed partical ordering \u227a. A probability\nmeasure on (\u2126, F) is associated (\u227a) if\n(1.68)\nP(C1 \u2229C2) \u2265P(C1)P(C2),\nfor all increasing sets C1, C2 \u2208F (a set C is increasing if x \u2208C and x \u227ay implies\ny \u2208C).\nKwieci\u00b4nski and Szekli [66] proved that if N is positively self-exciting point pro-\ncess w.r.t. \u227aN (resp. \u227aD), then N is associated (\u227aN) (resp. (\u227aD)). Therefore, it\nimplies that for a nonlinear Hawkes process, if h(\u00b7) is nonnegative and \u03bb(\u00b7) nonde-\ncreasing, then N is associated (\u227aN) and if h(\u00b7) is nonnegative and nondecreasing\nand \u03bb(\u00b7) is nondecreasing, then N is associated (\u227aD).\nNext, let us consider the limit theorems for nonlinear Hawkes process. When\n37\n\u03bb(\u00b7) is nonlinear, the usual immigration-birth representation no longer works and\nyou may have to use some abstract theory to obtain limit theorems. Some progress\nhas already been made.\nBr\u00b4emaud and Massouli\u00b4e [14]\u2019s stability result implies that by the ergodic the-\norem,\n(1.69)\nNt\nt \u2192\u00b5 := E[N[0, 1]],\nas t \u2192\u221e, where E[N[0, 1]] is the mean of N[0, 1] under the stationary and ergodic\nmeasure.\nIn this thesis, we will obtain a functional central limit theorem and a Strassen\u2019s\ninvariance principle in Chapter 2 and a process-level, i.e. level-3 large deviation\nprinciple in Chapter 3 and thus a level-1 large deviation principle by contraction\nprinciple. We will also obtain an alternative expression for the rate function for\nlevel-1 large deviation principle of Markovian nonlinear Hawkes process as a vari-\national formula in Chapter 4.\n1.6\nMultivariate Hawkes Processes\nWe say N = (N1, . . . , Nd) is a multivariate Hawkes process if for any 1 \u2264i \u2264d,\nNi is a simple point process with intensity\n(1.70)\n\u03bbi,t := \u03bdi +\nZ t\n0\nd\nX\nj=1\nhij(t \u2212s)dNj,s,\nwhere \u03bdi \u2208R+ and hij(\u00b7) : R+ \u2192R+. Then, \u03bd := (\u03bd1, . . . , \u03bdd) is a vector and\nh := (hij)1\u2264i,j\u2264d is a d \u00d7 d matrix-valued function.\n38\nLet us assume that for any i, j,\nR \u221e\n0 hij(t)dt < \u221eand that the spectral radius\n\u03c1(K) of the matrix K =\nR \u221e\n0 h(t)dt satis\ufb01es \u03c1(K) < 1. Then, Bacry et al. [2]\nproved a law of large numbers, i.e.\n(1.71)\nsup\nu\u2208[0,1]\n\u2225T \u22121NTu \u2212u(I \u2212K)\u22121\u03bd\u2225\u21920,\nas T \u2192\u221ealmost surely and also in L2(P). If we assume further that for any\n1 \u2264i, j \u2264d,\n(1.72)\nZ \u221e\n0\nhij(t)t1/2dt < \u221e.\nThen, Bacry et al. [2] proved the following central limit theorem:\n(1.73)\n\u221a\nT\n\u0012 1\nT NTu \u2212u(I \u2212K)\u22121\u03bd\n\u0013\n,\nu \u2208[0, 1]\nconverges in law as T \u2192\u221eunder the Skorohod topology to\n(1.74)\n(I \u2212K)\u22121\u03a31/2Wu,\nu \u2208[0, 1],\nwhere \u03a3 is the diagonal matrix with \u03a3ii = ((I \u2212K)\u22121\u03bd)i, 1 \u2264i \u2264d.\nIt is well known that under the assumption that \u03c1(K) < 1, there exists a\nunique stationary version of the multivariate Hawkes process satisfying the dy-\nnamics (1.70). The rate of convergence to the stationary version of the multivari-\nate Hawkes process was obtained in Torrisi [103]. The Bartlett spectrum of the\nmultivariate Hawkes process was derived in Hawkes [52]. Some non-asymptotics\nestimates for multivariate Hawkes processes were obtained in Hansen et al. [49].\n39\nA nice survey on multivariate linear Hawkes processes can be found in Liniger\n[71].\n40\nChapter 2\nCentral Limit Theorem for\nNonlinear Hawkes Processes\n2.1\nMain Results\nIn this chapter, we obtain a functional central limit theorem for the nonlinear\nHawkes process under Assumption 1. Under the same assumption, a Strassen\u2019s\ninvariance principle also holds. Let us recall that N is a nonlinear Hawkes process\nwith intensity\n(2.1)\n\u03bbt := \u03bb\n\u0012Z\n(\u2212\u221e,t)\nh(t \u2212s)N(ds)\n\u0013\n.\nAssumption 1. We assume that\n\u2022 h(\u00b7) : [0, \u221e) \u2192R+ is a decreasing function and\nR \u221e\n0 th(t)dt < \u221e.\n\u2022 \u03bb(\u00b7) is positive, increasing and \u03b1-Lipschitz (i.e. |\u03bb(x) \u2212\u03bb(y)| \u2264\u03b1|x \u2212y| for\nany x, y) and \u03b1\u2225h\u2225L1 < 1.\n41\nBr\u00b4emaud and Massouli\u00b4e [14] proved that if \u03bb(\u00b7) is \u03b1-Lipschitz with \u03b1\u2225h\u2225L1 < 1,\nthere exists a unique stationary and ergodic Hawkes process satisfying the dynam-\nics (1.2). Hence, under our Assumption 1 (which is slightly stronger than [14]),\nthere exists a unique stationary and ergodic Hawkes process satisfying the dynam-\nics (1.2).\nLet P and E denote the probability measure and expectation for a stationary,\nergodic Hawkes process, and let P(\u00b7|F\u2212\u221e\n0\n) and E(\u00b7|F\u2212\u221e\n0\n) denote the conditional\nprobability measure and expectation given the past history.\nThe following are the main results of this chapter.\nTheorem 6. Under Assumption 1, let N be the stationary and ergodic nonlinear\nHawkes process with dynamics (1.2). We have\n(2.2)\nN\u00b7t \u2212\u00b7\u00b5t\n\u221a\nt\n\u2192\u03c3B(\u00b7),\nas t \u2192\u221e,\nwhere B(\u00b7) is a standard Brownian motion and 0 < \u03c3 < \u221e, where\n(2.3)\n\u03c32 := E[(N[0, 1] \u2212\u00b5)2] + 2\n\u221e\nX\nj=1\nE[(N[0, 1] \u2212\u00b5)(N[j, j + 1] \u2212\u00b5)].\nThe convergence in (2.2) is weak convergence on D[0, 1], the space of c\u00b4adl\u00b4ag func-\ntions on [0, 1], equipped with Skorokhod topology.\nRemark 1. By a standard central limit theorem for martingales, i.e. Theorem 9,\nit is easy to see that\n(2.4)\nN\u00b7t \u2212\nR \u00b7t\n0 \u03bbsds\n\u221a\nt\n\u2192\u221a\u00b5B(\u00b7),\nas t \u2192\u221e,\nwhere \u00b5 = E[N[0, 1]].\nIn the linear case, say \u03bb(z) = \u03bd + z, Bacry et al.\n[2]\n42\nproved that \u03c32 in (2.3) satis\ufb01es \u03c32 =\n\u03bd\n(1\u2212\u2225h\u2225L1)3 > \u00b5 =\n\u03bd\n1\u2212\u2225h\u2225L1 .\nThat is not\nsurprising because N\u00b7t \u2212\u00b7\u00b5t \u201cshould\u201d have more \ufb02uctuations than N\u00b7t \u2212\nR \u00b7t\n0 \u03bbsds.\nTherefore, we guess that for nonlinear \u03bb(\u00b7), \u03c32 de\ufb01ned in (2.3) should also satisfy\n\u03c32 > \u00b5 = E[N[0, 1]].\nHowever, it might not be very easy to compute and say\nsomething about \u03c32 in such a case.\nIn the classical case for a sequence of i.i.d. random variables Xi with mean 0 and\nvariance 1, we have the central limit theorem\n1\n\u221an\nPn\ni=1 Xi \u2192N(0, 1) as n \u2192\u221e, and\nwe also have\nPn\ni=1 Xi\n\u221an log log n \u21920 in probability as n \u2192\u221e, but the convergence does not\nhold a.s. The law of the iterated logarithm says that lim supn\u2192\u221e\nPn\ni=1 Xi\n\u221an log log n =\n\u221a\n2\na.s. A functional version of the law of the iterated logarithm is called Strassen\u2019s\ninvariance principle.\nIt turns out that we also have a Strassen\u2019s invariance principle for nonlinear\nHawkes processes under Assumption 1.\nTheorem 7. Under Assumption 1, let N be the stationary and ergodic nonlinear\nHawkes process with dynamics (1.2). Let Xn := N[n \u22121, n] \u2212\u00b5, Sn := Pn\ni=1 Xi,\ns2\nn := E[S2\nn], g(t) = sup{n : s2\nn \u2264t}, and for t \u2208[0, 1], let \u03b7n(t) be the usual linear\ninterpolation, i.e.\n(2.5)\n\u03b7n(t) = Sk + (s2\nnt \u2212s2\nk)(s2\nk+1 \u2212s2\nk)\u22121Xk+1\np\n2s2\nn log log s2\nn\n,\ns2\nk \u2264s2\nnt \u2264s2\nk+1, k = 0, 1, . . . , n \u22121.\nThen, g(e) < \u221e, {\u03b7n, n > g(e)} is relatively compact in C[0, 1], the set of contin-\nuous functions on [0, 1] equipped with uniform topology, and the set of limit points\nis the set of absolutely continuous functions f(\u00b7) on [0, 1] such that f(0) = 0 and\nR 1\n0 f \u2032(t)2dt \u22641.\n43\n2.2\nProofs\nThis section is devoted to the proof of Theorem 6. We use a standard central\nlimit theorem, i.e. Theorem 8. In our proof, we need the fact that E[N[0, 1]2] < \u221e,\nwhich is proved in Lemma 3. Lemma 3 is proved by proving a stronger result \ufb01rst,\ni.e. Lemma 2. We will also prove Lemma 4 to guarantee that \u03c3 > 0 so that the\ncentral limit theorem is not degenerate.\nLet us \ufb01rst quote the two necessary central limit theorems from Billingsley [8].\nIn both Theorem 8 and Theorem 9, the \ufb01ltrations are the natural ones, i.e. given\na stochastic process (Xn)n\u2208Z, Fa\nb := \u03c3(Xn, a \u2264n \u2264b), for \u2212\u221e\u2264a \u2264b \u2264\u221e.\nTheorem 8 (Page 197 [8]). Suppose Xn, n \u2208Z, is an ergodic stationary sequence\nsuch that E[Xn] = 0 and\n(2.6)\nX\nn\u22651\n\u2225E[X0|F\u2212\u221e\n\u2212n ]\u22252 < \u221e,\nwhere \u2225Y \u22252 = (E[Y 2])1/2. Let Sn = X1+\u00b7 \u00b7 \u00b7+Xn. Then S[n\u00b7]/\u221an \u2192\u03c3B(\u00b7) weakly,\nwhere the weak convergence is on D[0, 1] equipped with the Skorohod topology and\n\u03c32 = E[X2\n0] + 2 P\u221e\nn=1 E[X0Xn]. The series converges absolutely.\nTheorem 9 (Page 196 [8]). Suppose Xn, n \u2208Z, is an erogdic stationary se-\nquence of square integrable martingale di\ufb00erences, i.e. \u03c32 = E[X2\nn] < \u221e, and let\nE[Xn|F\u2212\u221e\nn\u22121] = 0. Let Sn = X1 + \u00b7 \u00b7 \u00b7 + Xn. Then S[n\u00b7]/\u221an \u2192\u03c3B(\u00b7) weakly, where\nthe weak convergence is on D[0, 1] equipped with the Skorohod topology.\nNow, we are ready to prove our main result.\nProof of Theorem 6. Since in the stationary regime, E[N[n, n+1]] = E[N[0, 1]] for\nany n \u2208Z and let us denote E[N[0, 1]] = \u00b5. In order to apply Theorem 8, let us\n44\n\ufb01rst prove that\n(2.7)\n\u221e\nX\nn=1\nn\nE\nh\u0000E[N(n, n + 1] \u2212\u00b5|F\u2212\u221e\n0\n]\n\u00012io1/2\n< \u221e.\nLet E\u03c9\u2212\n1 [N(n, n+1]] and E\u03c9\u2212\n2 [N(n, n+1]] be two independent copies of E[N(n, n+\n1]|F\u2212\u221e\n0\n]. It is easy to check that\n1\n2E\n\u001ah\nE\u03c9\u2212\n1 [N(n, n + 1]] \u2212E\u03c9\u2212\n2 [N(n, n + 1]]\ni2\u001b\n(2.8)\n= 1\n2E\nh\nE\u03c9\u2212\n1 [N(n, n + 1]]2i\n+ 1\n2E\nh\nE\u03c9\u2212\n2 [N(n, n + 1]]2i\n\u2212E\nh\nE\u03c9\u2212\n1 [N(n, n + 1]]E\u03c9\u2212\n2 [N(n, n + 1]]\ni\n= E\n\u0002\nE[N(n, n + 1]|F\u2212\u221e\n0\n]2\u0003\n\u2212\u00b52\n= E\n\u0002\n(E[N(n, n + 1] \u2212\u00b5|F\u2212\u221e\n0\n])2\u0003\n.\nTherefore, we have\nE\n\u0002\n(E[N(n, n + 1] \u2212\u00b5|F\u2212\u221e\n0\n])2\u0003\n(2.9)\n= 1\n2E\n\u001ah\nE\u03c9\u2212\n1 [N(n, n + 1]] \u2212E\u03c9\u2212\n2 [N(n, n + 1]]\ni2\u001b\n\u2264E\n\u001ah\nE\u03c9\u2212\n1 [N(n, n + 1]] \u2212E\u2205[N(n, n + 1]]\ni2\u001b\n+ E\n\u001ah\nE\u03c9\u2212\n2 [N(n, n + 1]] \u2212E\u2205[N(n, n + 1]]\ni2\u001b\n= 2E\n\u001ah\nE\u03c9\u2212\n1 [N(n, n + 1]] \u2212E\u2205[N(n, n + 1]]\ni2\u001b\n,\nwhere E\u2205[N(n, n + 1]] denotes the expectation of the number of points in (n, n +\n1] for the Hawkes process with the same dynamics (1.2) and empty history, i.e.\nN(\u2212\u221e, 0] = 0.\n45\nNext, let us estimate E\u03c9\u2212\n1 [N(n, n + 1]] \u2212E\u2205[N(n, n + 1]]. E\u03c9\u2212\n1 [N(n, n + 1]] is\nthe expectation of the number of points in (n, n + 1] for the Hawkes process with\nintensity \u03bbt = \u03bb\n\u0010P\n\u03c4:\u03c4\u2208\u03c9\u2212\n1 \u222a\u03c9[0,t) h(t \u2212\u03c4)\n\u0011\n. It is well de\ufb01ned for a.e. \u03c9\u2212\n1 under P\nbecause, under Assumption 1,\n(2.10) E[\u03bbt] \u2264\u03bb(0) + \u03b1E\n\u0014Z t\n\u2212\u221e\nh(t \u2212s)N(ds)\n\u0015\n= \u03bb(0) + \u03b1\u2225h\u2225L1E[N[0, 1]] < \u221e,\nwhich implies that \u03bbt < \u221eP-a.s.\nIt is clear that E\u03c9\u2212\n1 [N(n, n + 1]] \u2265E\u2205[N(n, n + 1]] almost surely, so we can use\na coupling method to estimate the di\ufb00erence. We will follow the ideas in Br\u00b4emaud\nand Massouli\u00b4e [14] using the Poisson embedding method. Consider (\u2126, F, P), the\ncanonical space of a point process on R+ \u00d7R+ in which N is Poisson with intensity\n1 under the probability measure P. Then the Hawkes process N 0 with empty past\nhistory and intensity \u03bb0\nt satis\ufb01es the following.\n(2.11)\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f3\n\u03bb0\nt = \u03bb\n\u0010R\n(0,t) h(t \u2212s)N 0(ds)\n\u0011\nt \u2208R+,\nN 0(C) =\nR\nC N(dt \u00d7 [0, \u03bb0\nt])\nC \u2208B(R+).\nFor n \u22651, let us de\ufb01ne recursively \u03bbn\nt , Dn and N n as follows.\n(2.12)\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n\u03bbn\nt = \u03bb\n\u0010R\n(0,t) h(t \u2212s)N n\u22121(ds) + P\n\u03c4\u2208\u03c9\u2212\n1 h(t \u2212\u03c4)\n\u0011\nt \u2208R+,\nDn(C) =\nR\nC N(dt \u00d7 [\u03bbn\u22121\nt\n, \u03bbn\nt ])\nC \u2208B(R+),\nN n(C) = N n\u22121(C) + Dn(C)\nC \u2208B(R+).\nFollowing the arguments as in Br\u00b4emaud and Massouli\u00b4e [14], we know that each \u03bbn\nt\nis an FN\nt -intensity of N n, where FN\nt\nis the \u03c3-algebra generated by N up to time\n46\nt. By our Assumption 1, \u03bb(\u00b7) is increasing, and it is clear that \u03bbn(t) and N n(C)\nincrease in n for all t \u2208R+ and C \u2208B(R+). Thus, Dn is well de\ufb01ned and also\nthat as n \u2192\u221e, the limiting processes \u03bbt and N exist. N counts the number of\npoints of N below the curve t 7\u2192\u03bbt and admits \u03bbt as an FN\nt -intensity. By the\nmonotonicity properties of \u03bbn\nt and N n, we have\n\u03bbn\nt \u2264\u03bb\n\uf8eb\n\uf8ed\nZ\n(0,t)\nh(t \u2212s)N(ds) +\nX\n\u03c4\u2208\u03c9\u2212\n1\nh(t \u2212\u03c4)\n\uf8f6\n\uf8f8,\n(2.13)\n\u03bbt \u2265\u03bb\n\uf8eb\n\uf8ed\nZ\n(0,t)\nh(t \u2212s)N n(ds) +\nX\n\u03c4\u2208\u03c9\u2212\n1\nh(t \u2212\u03c4)\n\uf8f6\n\uf8f8.\n(2.14)\nLetting n \u2192\u221e(it is valid since we assume that \u03bb(\u00b7) is Lipschitz and thus continu-\nous), we conclude that N, \u03bbt satis\ufb01es the dynamics (1.2). Therefore, with intensity\n\u03bbt, N = N 0 + P\u221e\ni=1 Di is the Hawkes process with past history \u03c9\u2212\n1 .\nWe can then estimate the di\ufb00erence by noticing that\n(2.15)\nE\u03c9\u2212\n1 [N(n, n + 1]] \u2212E\u2205[N(n, n + 1]] =\n\u221e\nX\ni=1\nEP[Di(n, n + 1]].\nHere EP means the expectation with respect to P, the probability measure on the\ncanonical space that we de\ufb01ned earlier.\n47\nWe have\nEP[D1(n, n + 1]]\n(2.16)\n= EP\n\u0014Z n+1\nn\n(\u03bb1(t) \u2212\u03bb0(t))dt\n\u0015\n= EP\n\uf8ee\n\uf8f0\nZ n+1\nn\n\u03bb\n\uf8eb\n\uf8ed\nX\n\u03c4<t,\u03c4\u2208N0\u222a\u03c9\u2212\n1\nh(t \u2212\u03c4)\n\uf8f6\n\uf8f8\u2212\u03bb\n\uf8eb\n\uf8ed\nX\n\u03c4<t,\u03c4\u2208N0\u222a\u2205\nh(t \u2212\u03c4)\n\uf8f6\n\uf8f8dt\n\uf8f9\n\uf8fb\n\u2264\u03b1\nZ n+1\nn\nX\n\u03c4\u2208\u03c9\u2212\n1\nh(t \u2212\u03c4)dt,\nwhere the \ufb01rst equality in (2.16) is due to the construction of D1 in (2.12), the\nsecond equality in (2.16) is due to the de\ufb01nitions of \u03bb1 and \u03bb0 in (2.12) and \ufb01nally\nthe inequality in (2.16) is due to the fact that \u03bb(\u00b7) is \u03b1-Lipschitz. Similarly,\nEP[D2(n, n + 1]] \u2264E\u03c9\u2212\n1\n\"\n\u03b1\nZ n+1\nn\nX\n\u03c4\u2208D1,\u03c4<t\nh(t \u2212\u03c4)dt\n#\n(2.17)\n\u2264\nX\n\u03c4\u2208\u03c9\u2212\n1\n\u03b12\nZ n+1\nn\nZ t\n0\nh(t \u2212s)h(s \u2212\u03c4)dsdt.\nIteratively, we have, for any k \u2208N,\nEP[Dk(n, n + 1]] \u2264\nX\n\u03c4\u2208\u03c9\u2212\n1\n\u03b1k\nZ n+1\nn\nZ tk\n0\n\u00b7 \u00b7 \u00b7\nZ t2\n0\nh(tk \u2212tk\u22121)h(tk\u22121 \u2212tk\u22122)\n\u00b7 \u00b7 \u00b7 h(t2 \u2212t1)h(t1 \u2212\u03c4)dt1 \u00b7 \u00b7 \u00b7 dtk =:\nX\n\u03c4\u2208\u03c9\u2212\n1\nKk(n, \u03c4).\n48\nNow let K(n, \u03c4) := P\u221e\nk=1 Kk(n, \u03c4). Then,\nE\n\u001ah\nE\u03c9\u2212\n1 [N(n, n + 1]] \u2212E\u2205[N(n, n + 1]]\ni2\u001b\n(2.18)\n\u2264E\n\uf8ee\n\uf8f0\n\uf8eb\n\uf8edX\n\u03c4\u2208\u03c9\u2212\n1\nK(n, \u03c4)\n\uf8f6\n\uf8f8\n2\uf8f9\n\uf8fb\n\u2264E\n\"X\ni,j\u22640\nK(n, i)K(n, j)N[i, i + 1]N[j, j + 1]\n#\n=\nX\ni,j\u22640\nK(n, i)K(n, j)E[N[i, i + 1]N[j, j + 1]]\n\u2264\nX\ni,j\u22640\nK(n, i)K(n, j)1\n2\n\b\nE[N[i, i + 1]2] + E[N[j, j + 1]2]\n\t\n= E[N[0, 1]2]\n X\ni\u22640\nK(n, i)\n!2\n.\nHere, E[N[0, 1]2] < \u221eby Lemma 3. Therefore, we have\n\u221e\nX\nn=1\nn\nE\nh\u0000E[N(n, n + 1] \u2212\u00b5|F\u2212\u221e\n0\n]\n\u00012io1/2\n(2.19)\n\u2264\np\n2E[N[0, 1]2]\n\u221e\nX\nn=1\n0\nX\ni=\u2212\u221e\nK(n, i)\n\u2264\np\n2E[N[0, 1]2]\n\u221e\nX\nk=1\n\u03b1k\nZ \u221e\n0\nZ tk\n0\n\u00b7 \u00b7 \u00b7\nZ t2\n0\nZ 0\n\u2212\u221e\nh(tk \u2212tk\u22121)h(tk\u22121 \u2212tk\u22122) \u00b7 \u00b7 \u00b7 h(t2 \u2212t1)h(t1 \u2212s)dsdt1 \u00b7 \u00b7 \u00b7 dtk.\nLet H(t) :=\nR \u221e\nt\nh(s)ds. It is easy to check that\nR \u221e\n0 H(t)dt =\nR \u221e\n0 th(t)dt < \u221eby\n49\nAssumption 1. We have\n\u03b1k\nZ \u221e\n0\nZ tk\n0\n\u00b7 \u00b7 \u00b7\nZ t2\n0\nZ 0\n\u2212\u221e\n(2.20)\nh(tk \u2212tk\u22121)h(tk\u22121 \u2212tk\u22122) \u00b7 \u00b7 \u00b7 h(t2 \u2212t1)h(t1 \u2212s)dsdt1 \u00b7 \u00b7 \u00b7 dtk\n= \u03b1k\nZ \u221e\n0\nZ tk\n0\n\u00b7 \u00b7 \u00b7\nZ t2\n0\nh(tk \u2212tk\u22121)h(tk\u22121 \u2212tk\u22122) \u00b7 \u00b7 \u00b7 h(t2 \u2212t1)H(t1)dt1 \u00b7 \u00b7 \u00b7 dtk\n= \u03b1k\nZ \u221e\n0\n\u00b7 \u00b7 \u00b7\nZ \u221e\ntk\u22122\nZ \u221e\ntk\u22121\nh(tk \u2212tk\u22121)dtkh(tk\u22121 \u2212tk\u22122)dtk\u22121 \u00b7 \u00b7 \u00b7 H(t1)dt1\n= \u03b1k\u2225h\u2225k\u22121\nL1\nZ \u221e\n0\nH(t1)dt1 = \u03b1k\u2225h\u2225k\u22121\nL1\nZ \u221e\n0\nth(t)dt.\nSince \u03b1\u2225h\u2225L1 < 1, we conclude that\n\u221e\nX\nn=1\nn\nE\nh\u0000E[N(n, n + 1] \u2212\u00b5|F\u2212\u221e\n0\n]\n\u00012io1/2\n(2.21)\n\u2264\n\u221e\nX\nk=1\np\n2E[N[0, 1]2]\u03b1k\u2225h\u2225k\u22121\nL1\nZ \u221e\n0\nth(t)dt\n=\np\n2E[N[0, 1]2] \u00b7\n\u03b1\n1 \u2212\u03b1\u2225h\u2225L1 \u00b7\nZ \u221e\n0\nth(t)dt < \u221e.\nHence, by Theorem 8, we have\n(2.22)\nN[\u00b7t] \u2212\u00b5[\u00b7t]\n\u221a\nt\n\u2192\u03c3B(\u00b7)\nas t \u2192\u221e,\nwhere\n(2.23)\n\u03c32 = E[(N[0, 1] \u2212\u00b5)2] + 2\n\u221e\nX\nj=1\nE[(N[0, 1] \u2212\u00b5)(N[j, j + 1] \u2212\u00b5)] < \u221e.\n50\nBy Lemma 4, \u03c3 > 0. Now, \ufb01nally, for any \u03f5 > 0, for t su\ufb03ciently large,\nP\n\u0012\nsup\n0\u2264s\u22641\n\f\f\f\f\nN[st] \u2212\u00b5[st]\n\u221a\nt\n\u2212Nst \u2212\u00b5st\n\u221a\nt\n\f\f\f\f > \u03f5\n\u0013\n(2.24)\n= P\n\u0012\nsup\n0\u2264s\u22641\n\f\f(N[st] \u2212Nst) + \u00b5(st \u2212[st])\n\f\f > \u03f5\n\u221a\nt\n\u0013\n\u2264P\n\u0012\nsup\n0\u2264s\u22641\n\f\fN[st] \u2212Nst\n\f\f + \u00b5 > \u03f5\n\u221a\nt\n\u0013\n\u2264P\n\u0012\nmax\n0\u2264k\u2264[t],k\u2208Z N[k, k + 1] > \u03f5\n\u221a\nt \u2212\u00b5\n\u0013\n\u2264([t] + 1)P(N[0, 1] > \u03f5\n\u221a\nt \u2212\u00b5)\n\u2264\n[t] + 1\n(\u03f5\n\u221a\nt \u2212\u00b5)2\nZ\nN[0,1]>\u03f5\n\u221a\nt\u2212\u00b5\nN[0, 1]2dP \u21920,\nas t \u2192\u221eby Lemma 3. Hence, we conclude that N\u00b7t\u2212\u00b7\u00b5t\n\u221a\nt\n\u2192\u03c3B(\u00b7) as t \u2192\u221e.\nThe following Lemma 2 is used to prove Lemma 3.\nLemma 2. There exists some \u03b8 > 0 such that supt\u22650 E\u2205h\ne\nR t\n0 \u03b8h(t\u2212s)N(ds)i\n< \u221e.\nProof. Notice \ufb01rst that for any bounded deterministic function f(\u00b7),\n(2.25)\nexp\n\u001aZ t\n0\nf(s)N(ds) \u2212\nZ t\n0\n(ef(s) \u22121)\u03bb(s)ds\n\u001b\nis a martingale. Therefore, using the Lipschitz assumption of \u03bb(\u00b7), i.e. \u03bb(z) \u2264\n51\n\u03bb(0) + \u03b1z and applying H\u00a8older\u2019s inequality, for 1\np + 1\nq = 1, we have\nE\u2205h\ne\nR t\n0 \u03b8h(t\u2212s)N(ds)i\n(2.26)\n= E\u2205h\ne\nR t\n0 \u03b8h(t\u2212s)N(ds)\u22121\np\nR t\n0 (ep\u03b8h(t\u2212s)\u22121)\u03bb(s)ds+ 1\np\nR t\n0 (ep\u03b8h(t\u2212s)\u22121)\u03bb(s)dsi\n\u2264E\u2205h\ne\nq\np\nR t\n0 (ep\u03b8h(t\u2212s)\u22121)\u03bb(s)dsi 1\nq\n\u2264E\u2205h\ne\nq\np\nR t\n0 (ep\u03b8h(t\u2212s)\u22121)(\u03bb(0)+\u03b1\nR s\n0 h(s\u2212u)N(du))dsi 1\nq\n\u2264E\u2205h\ne\nR t\n0\nq\np (ep\u03b8h(t\u2212s)\u22121)\u03b1\nR s\n0 h(s\u2212u)N(du)dsi 1\nq \u00b7 e\n1\np\nR \u221e\n0 (ep\u03b8h(s)\u22121)\u03bb(0)ds.\nLet C(t) =\nR t\n0\nq\np(ep\u03b8h(t\u2212s) \u22121)\u03b1ds. Then, for any t \u2208[0, T],\nE\u2205h\ne\nR t\n0\nq\np (ep\u03b8h(t\u2212s)\u22121)\u03b1\nR s\n0 h(s\u2212u)N(du)dsi\n(2.27)\n= E\u2205h\ne\n1\nC(t)\nR t\n0\nq\np (ep\u03b8h(t\u2212s)\u22121)\u03b1C(t)\nR s\n0 h(s\u2212u)N(du)dsi\n\u2264E\u2205\n\u0014 1\nC(t)\nZ t\n0\nq\np(ep\u03b8h(t\u2212s) \u22121)\u03b1eC(t)\nR s\n0 h(s\u2212u)N(du)ds\n\u0015\n\u2264sup\n0\u2264s\u2264T\nE\u2205h\neC(\u221e)\nR s\n0 h(s\u2212u)N(du)i\n,\nwhere in the \ufb01rst inequality in (2.27), we used the Jensen\u2019s inequality since x 7\u2192ex\nis convex and\n1\nC(t)\nR t\n0\nq\np(ep\u03b8h(t\u2212s) \u22121)\u03b1ds = 1, and in the second inequality in (2.27),\nwe used the fact that C(t) \u2264C(\u221e) and again\n1\nC(t)\nR t\n0\nq\np(ep\u03b8h(t\u2212s) \u22121)\u03b1ds = 1. Now\nchoose q > 1 so small that q\u03b1\u2225h\u2225L1 < 1. Once p and q are \ufb01xed, choose \u03b8 > 0 so\nsmall that\n(2.28)\nC(\u221e) =\nZ \u221e\n0\nq\np(ep\u03b8h(s) \u22121)\u03b1ds < \u03b8.\n52\nThis implies that for any t \u2208[0, T],\n(2.29)\nE\u2205h\ne\nR t\n0 \u03b8h(t\u2212s)N(ds)i\n\u2264sup\n0\u2264s\u2264T\nE\u2205h\ne\u03b8\nR s\n0 h(s\u2212u)N(du)i 1\nq \u00b7 e\n1\np\nR \u221e\n0 (ep\u03b8h(s)\u22121)\u03bb(0)ds.\nHence, we conclude that for any T > 0,\n(2.30)\nsup\n0\u2264t\u2264T\nE\u2205h\ne\u03b8\nR t\n0 h(t\u2212s)N(ds)i\n\u2264e\nR \u221e\n0 (ep\u03b8h(s)\u22121)\u03bb(0)ds < \u221e.\nLemma 3. There exists some \u03b8 > 0 such that E[e\u03b8N[0,1]] < \u221e. Hence E[N[0, 1]2] <\n\u221e.\nProof. By Assumption 1, h(\u00b7) is positive and decreasing. Thus, \u03b4 = inft\u2208[0,1] h(t) >\n0. Hence,\n(2.31)\nE\u2205[e\u03b8N[t\u22121,t]] \u2264E\u2205[e\n\u03b8\n\u03b4\nR t\n0 h(t\u2212s)N(ds)].\nBy Lemma 2, we can choose \u03b8 > 0 so small that\n(2.32)\nlim sup\nt\u2192\u221e\nE\u2205[e\u03b8N[t\u22121,t]] < \u221e.\nFinally, E[e\u03b8N[0,1]] \u2264lim inft\u2192\u221eE\u2205[e\u03b8N[t\u22121,t]] < \u221e.\nIt is intuitively clear that \u03c3 > 0. But still we need a proof.\nLemma 4. \u03c3 > 0, where \u03c3 is de\ufb01ned in (2.23).\nProof. Let \u03b7n = P\u221e\nj=n E[N(j, j + 1] \u2212\u00b5|F\u2212\u221e\nn+1], where \u00b5 = E[N[0, 1]]. \u03b7n is well\n53\nde\ufb01ned because we proved (2.7). To see this, notice that\n\u2225\u03b7n\u22252 =\n\r\r\r\r\n\u221e\nX\nj=n\nE[N(j, j + 1] \u2212\u00b5|F\u2212\u221e\nn+1]\n\r\r\r\r\n2\n(2.33)\n\u2264\n\u221e\nX\nj=n\n\u2225E[N(j, j + 1] \u2212\u00b5|F\u2212\u221e\nn+1]\u22252 < \u221e,\nby (2.7). Also, it is easy to check that\nE[\u03b7n+1 \u2212\u03b7n + N(n, n + 1] \u2212\u00b5|F\u2212\u221e\nn+1]\n(2.34)\n= E\n\"\n\u221e\nX\nj=n+1\nE[N(j, j + 1] \u2212\u00b5|F\u2212\u221e\nn+2]\n\f\f\f\fF\u2212\u221e\nn+1\n#\n\u2212E\n\" \u221e\nX\nj=n\nE[N(j, j + 1] \u2212\u00b5|F\u2212\u221e\nn+1]\n\f\f\f\fF\u2212\u221e\nn+1\n#\n+ N(n, n + 1] \u2212\u00b5\n=\n\u221e\nX\nj=n+1\nE[N(j, j + 1] \u2212\u00b5|F\u2212\u221e\nn+1] \u2212\n\u221e\nX\nj=n+1\nE[N(j, j + 1] \u2212\u00b5|F\u2212\u221e\nn+1]\n\u2212N(n, n + 1] + \u00b5 + N(n, n + 1] \u2212\u00b5 = 0.\nLet Yn = \u03b7n\u22121 \u2212\u03b7n\u22122 + N(n \u22122, n \u22121] \u2212\u00b5.\nThis is an ergodic, stationary\nsequence such that E[Yn|F\u2212\u221e\nn\u22121] = 0. By (2.7), E[Y 2\nn ] < \u221eand by Theorem 9,\nS\u2032\n[n\u00b7]/\u221an \u2192\u03c3\u2032B(\u00b7), where S\u2032\nn = Pn\nj=1 Yj. It is clear that \u03c3 = \u03c3\u2032 < \u221esince for any\n54\n\u03f5 > 0,\nP\n \nmax\n1\u2264k\u2264[n],k\u2208Z\n1\n\u221an\nk\nX\nj=1\n(\u03b7j\u22121 \u2212\u03b7j\u22122) > \u03f5\n!\n(2.35)\n= P\n\u0012\nmax\n1\u2264k\u2264[n],k\u2208Z(\u03b7k\u22121 \u2212\u03b7\u22121) > \u03f5\u221an\n\u0013\n\u2264P\n\u0012\u001a\nmax\n1\u2264k\u2264[n],k\u2208Z |\u03b7k\u22121| > \u03f5\u221an\n2\n\u001b [ \u001a\n|\u03b7\u22121| > \u03f5\u221an\n2\n\u001b\u0013\n\u2264\n[n]\nX\nk=1\nP\n\u0012\n|\u03b7k\u22121| > \u03f5\u221an\n2\n\u0013\n+ P\n\u0012\n|\u03b7\u22121| > \u03f5\u221an\n2\n\u0013\n= ([n] + 1)P\n\u0012\n|\u03b7\u22121| > \u03f5\u221an\n2\n\u0013\n\u22644([n] + 1)\n\u03f52n\nZ\n|\u03b7\u22121|> \u03f5\u221an\n2\n|\u03b7\u22121|2dP \u21920,\nas n \u2192\u221e, where we used the stationarity of P, Chebychev\u2019s inequality and (2.7).\nNow, it becomes clear that\n\u03c32 = (\u03c3\u2032)2 = E[Y 2\n1 ]\n(2.36)\n= E (\u03b70 \u2212\u03b7\u22121 + N(\u22121, 0] \u2212\u00b5)2\n= E\n \u221e\nX\nj=0\nE[N(j, j + 1] \u2212\u00b5|F\u2212\u221e\n1\n] \u2212\n\u221e\nX\nj=0\nE[N(j, j + 1] \u2212\u00b5|F\u2212\u221e\n0\n]\n!2\n.\nConsider D = {\u03c9 : \u03c9\u2212\u0338= \u2205, \u03c9(0, 1] = \u2205}. Notice that P(\u03c9\u2212= \u2205) = 0. By\n55\nJensen\u2019s inequality and Assumption 1, we have\nP(D) =\nZ\nP\u03c9\u2212(N(0, 1] = 0)P(d\u03c9\u2212)\n(2.37)\n= E\nh\ne\u2212\nR 1\n0 \u03bb(P\n\u03c4\u2208\u03c9\u2212h(t\u2212\u03c4))dti\n\u2265exp\n(\n\u2212E\nZ 1\n0\n\u03bb\n X\n\u03c4\u2208\u03c9\u2212\nh(t \u2212\u03c4)\n!\ndt\n)\n\u2265exp\n(\n\u2212\u03bb(0) \u2212\u03b1E\nZ 1\n0\nX\n\u03c4\u2208\u03c9\u2212\nh(t \u2212\u03c4)dt\n)\n\u2265exp {\u2212\u03bb(0) \u2212\u03b1E[N[0, 1]] \u00b7 \u2225h\u2225L1} > 0.\nIt is clear that given the event D,\n(2.38)\n\u221e\nX\nj=0\nE[N(j, j + 1] \u2212\u00b5|F\u2212\u221e\n1\n] <\n\u221e\nX\nj=0\nE[N(j, j + 1] \u2212\u00b5|F\u2212\u221e\n0\n].\nTherefore,\n(2.39)\nP\n \u221e\nX\nj=0\nE[N(j, j + 1] \u2212\u00b5|F\u2212\u221e\n1\n] \u0338=\n\u221e\nX\nj=0\nE[N(j, j + 1] \u2212\u00b5|F\u2212\u221e\n0\n]\n!\n> 0,\nwhich implies that \u03c3 > 0.\nProof of Theorem 7. By Heyde and Scott [57], the Strassen\u2019s invariance principle\nholds if we have (2.7) and \u03c3 > 0.\n56\nChapter 3\nProcess-Level Large Deviations\nfor Nonlinear Hawkes Processes\n3.1\nMain Results\nIn this chaper, we prove a process-level, i.e. level-3 large deviation principle\nfor nonlinear Hawkes processes. As a corollary, a level-1 large deviation principle\nis obtained by a contraction principle.\nLet us recall that N is a nonlinear Hawkes process with intensity\n(3.1)\n\u03bbt := \u03bb\n\u0012Z\n(\u2212\u221e,t)\nh(t \u2212s)N(ds)\n\u0013\n.\nThroughout this chapter, we assume that\n\u2022 The exciting function h(t) is positive, continuous and decreasing for t \u22650\nand h(t) = 0 for any t < 0. We also assume that\nR \u221e\n0 h(t)dt < \u221e.\n\u2022 The rate function \u03bb(\u00b7) : [0, \u221e) \u2192R+ is increasing and limz\u2192\u221e\n\u03bb(z)\nz\n= 0. We\n57\nalso assume that \u03bb(\u00b7) is Lipschitz with constant \u03b1 > 0, i.e. |\u03bb(x) \u2212\u03bb(y)| \u2264\n\u03b1|x \u2212y| for any x, y \u22650.\nLet \u2126be the set of countable, locally \ufb01nite subsets of R and for any \u03c9 \u2208\u2126\nand A \u2286R, write \u03c9(A) := \u03c9 \u2229A. For any t \u2208R, we write \u03c9(t) = \u03c9({t}). Let\nN(A) = #|\u03c9\u2229A| denote the number of points in the set A for any A \u2282R. We also\nuse the notation Nt to denote N[0, t], the number of points up to time t, starting\nfrom time 0. We de\ufb01ne the shift operator \u03b8t by \u03b8t(\u03c9)(s) = \u03c9(t + s). We equip the\nsample space \u2126with the topology in which the convergence \u03c9n \u2192\u03c9 as n \u2192\u221eis\nde\ufb01ned by\n(3.2)\nX\n\u03c4\u2208\u03c9n\nf(\u03c4) \u2192\nX\n\u03c4\u2208\u03c9\nf(\u03c4),\nfor any continuous f with compact support.\nThis topology is equivalent to the vague topology for random measures, for\nwhich, see for example Grandell [45]. One can equip the space of locally \ufb01nite\nrandom measures with the vague topology. The subspace of integer valued random\nmeasures is then the space of point processes. A simple point processes is a point\nprocess without multiple jumps. The space of point processes is closed. But the\nspace of simple point processes is not closed.\nDenote Fs\nt = \u03c3(\u03c9[s, t]) for any s < t, i.e. the \u03c3-algebra generated by all the\npossible con\ufb01gurations of points in the interval [s, t]. Denote M(\u2126) the space of\nprobability measures on \u2126. We also de\ufb01ne MS(\u2126) as the space of simple point\nprocesses that are invariant with respect to \u03b8t with bounded \ufb01rst moment, i.e. for\nany Q \u2208MS(\u2126), EQ[N[0, 1]] < \u221e. De\ufb01ne ME(\u2126) as the set of ergodic simple\npoint processes in MS(\u2126). We de\ufb01ne the topology of MS(\u2126) as follows. For a\n58\nsequence Qn in MS(\u2126) and Q \u2208MS(\u2126), we say Qn \u2192Q as n \u2192\u221eif and only if\n(3.3)\nZ\nfdQn \u2192\nZ\nfdQ,\nas n \u2192\u221efor any continuous and bounded f and\n(3.4)\nZ\nN[0, 1](\u03c9)Qn(d\u03c9) \u2192\nZ\nN[0, 1](\u03c9)Q(d\u03c9),\nas n \u2192\u221e. In other words, the topology is the weak topology strengthened by the\nconvergence of the \ufb01rst moment of N[0, 1]. For any Q1, Q2 in MS(\u2126), one can\nde\ufb01ne the metric d(\u00b7, \u00b7) by\n(3.5)\nd(Q1, Q2) = dp(Q1, Q2) +\n\f\fEQ1[N[0, 1]] \u2212EQ2[N[0, 1]]\n\f\f ,\nwhere dp(\u00b7, \u00b7) is the usual Prokhorov metric. Because this is an unusual topology,\nthe compactness is di\ufb00erent from that in the usual weak topology; later, when we\nprove the exponential tightness, we need to take some extra care. See Lemma 22\nand (iii) of Lemma 21.\nWe denote by C(\u2126) the set of real-valued continous functions on \u2126. We sim-\nilarly de\ufb01ne C(\u2126\u00d7 R). We also denote by B(F\u2212\u221e\nt\n) the set of all bounded F\u2212\u221e\nt\nprogressively measurable and F\u2212\u221e\nt\npredictable functions.\nBefore we proceed, recall that a sequence (Pn)n\u2208N of probability measures on a\ntopological space X satis\ufb01es the large deviation principle (LDP) with rate function\nI : X \u2192R if I is non-negative, lower semicontinuous and for any measurable set\n59\nA,\n(3.6)\n\u2212inf\nx\u2208Ao I(x) \u2264lim inf\nn\u2192\u221e\n1\nn log Pn(A) \u2264lim sup\nn\u2192\u221e\n1\nn log Pn(A) \u2264\u2212inf\nx\u2208A I(x).\nHere, Ao is the interior of A and A is its closure. See Dembo and Zeitouni [30]\nor Varadhan [106] for general background regarding large deviations and their\napplications. Also Varadhan [107] has an excellent survey article on this subject.\nIn the pioneering work by Donsker and Varadhan [31], they obtained a level-3\nlarge deviation result for certain stationary Markov processes.\nWe would like to prove the large deviation principle for nonlinear Hawkes pro-\ncesses by proving a process-level, also known as level-3 large deviation principle\n\ufb01rst. We can then use the contraction principle to obtain the level-1 large deviation\nprinciple for (Nt/t \u2208\u00b7).\nLet us de\ufb01ne the empirical measure for the process as\n(3.7)\nRt,\u03c9(A) = 1\nt\nZ t\n0\n\u03c7A(\u03b8s\u03c9t)ds,\nfor any A, where \u03c9t(s) = \u03c9(s) for 0 \u2264s \u2264t and \u03c9t(s+t) = \u03c9t(s) for any s. Donsker\nand Varadhan [31] proved that in the case when \u2126is a space of c`adl`ag functions\n\u03c9(\u00b7) on \u2212\u221e< t < \u221eendowed with Skorohod topology and taking values in a\nPolish space X, under certain conditions, P 0,x(Rt,\u03c9 \u2208\u00b7) satis\ufb01es a large deviation\nprinciple, where P 0,x is a Markov process on \u21260\n\u221ewith initial value x \u2208X. The\nrate function H(Q) is some entropy function.\nLet h(\u03b1, \u03b2)\u03a3 be the relative entropy of \u03b1 with respect to \u03b2 restricted to the\n\u03c3-algebra \u03a3. For any Q \u2208MS(\u2126), let Q\u03c9\u2212be the regular conditional probability\ndistribution of Q. Similarly we de\ufb01ne P \u03c9\u2212.\n60\nLet us de\ufb01ne the entropy function H(Q) as\n(3.8)\nH(Q) = EQ[h(Q\u03c9\u2212, P \u03c9\u2212)F0\n1 ].\nNotice that P \u03c9\u2212describes the Hawkes process conditional on the past history\n\u03c9\u2212. It has rate \u03bb(P\n\u03c4\u2208\u03c9[0,s)\u222a\u03c9\u2212h(s \u2212\u03c4)) at time 0 \u2264s \u22641, which is well de-\n\ufb01ned for almost every \u03c9\u2212under Q if EQ[N[0, 1]] < \u221esince EQ[P\n\u03c4\u2208\u03c9\u2212h(\u2212\u03c4)] =\n\u2225h\u2225L1EQ[N[0, 1]] < \u221eimplies P\n\u03c4\u2208\u03c9\u2212h(s \u2212\u03c4) \u2264P\n\u03c4\u2208\u03c9\u2212h(\u2212\u03c4) < \u221efor all\n0 \u2264s \u22641.\nWhen H(Q) < \u221e, h(Q\u03c9\u2212, P \u03c9\u2212) < \u221efor a.e. \u03c9\u2212under Q, which implies that\nQ\u03c9\u2212\u226aP \u03c9\u2212on F0\n1. By the theory of absolute continuity of point processes, see\nfor example Chapter 19 of Lipster and Shiryaev [72] or Chapter 13 of Daley and\nVere-Jones [27], the compensator of Q\u03c9\u2212is absolutely continuous, i.e. it has some\ndensity \u02c6\u03bb say, such that by the Girsanov formula,\nH(Q) =\nZ\n\u2126\u2212\nZ \u0014Z 1\n0\n\u0010\n\u03bb \u2212\u02c6\u03bb\n\u0011\nds +\nZ 1\n0\nlog(\u02c6\u03bb/\u03bb)dNs\n\u0015\ndQ\u03c9\u2212Q(d\u03c9\u2212)\n(3.9)\n=\nZ\n\u2126\n\"Z 1\n0\n\u03bb(\u03c9, s) \u2212\u02c6\u03bb(\u03c9, s) + log\n \u02c6\u03bb(\u03c9, s)\n\u03bb(\u03c9, s)\n!\n\u02c6\u03bbds\n#\nQ(d\u03c9),\nwhere \u03bb = \u03bb\n\u0010P\n\u03c4\u2208\u03c9[0,s)\u222a\u03c9\u2212h(s \u2212\u03c4)\n\u0011\n.\nBoth \u03bb and \u02c6\u03bb are F\u2212\u221e\ns\n-predictable for\n0 \u2264s \u22641. For the equality in (3.9), we used the fact that Nt \u2212\nR t\n0 \u02c6\u03bb(\u03c9, s)ds is\na martingale under Q and for any f(\u03c9, s) which is bounded, F\u2212\u221e\ns\nprogressively\nmeasurable and predictable, we have\n(3.10)\nZ\n\u2126\nZ 1\n0\nf(\u03c9, s)dNsQ(d\u03c9) =\nZ\n\u2126\nZ 1\n0\nf(\u03c9, s)\u02c6\u03bb(\u03c9, s)dsQ(d\u03c9).\n61\nWe will use the above fact repeatedly in this chapter.\nThe following theorem is the main result of this chapter.\nTheorem 10. For any open set G \u2282MS(\u2126),\n(3.11)\nlim inf\nt\u2192\u221e\n1\nt log P (Rt,\u03c9 \u2208G) \u2265\u2212inf\nQ\u2208G H(Q),\nand for any closed set C \u2282MS(\u2126),\n(3.12)\nlim sup\nt\u2192\u221e\n1\nt log P (Rt,\u03c9 \u2208C) \u2264\u2212inf\nQ\u2208C H(Q).\nWe will prove the lower bound in Section 3.2, the upper bound in Section\n3.3, and the superexponential estimates that are needed in the proof of the upper\nbound in Section 3.4.\nOnce we establish the level-3 large deviation result, we can obtain the large\ndeviation principle for (Nt/t \u2208\u00b7) directly by using the contraction principle.\nTheorem 11. (Nt/t \u2208\u00b7) satis\ufb01es a large deviation principle with the rate function\nI(\u00b7) given by\n(3.13)\nI(x) =\ninf\nQ\u2208MS(\u2126),EQ[N[0,1]]=x H(Q).\nProof. Since Q 7\u2192EQ[N[0, 1]] is continuous,\nR\n\u2126N[0, 1]dRt,\u03c9 satis\ufb01es a large de-\nviation principle with the rate function I(\u00b7) by the contraction principle. (For a\n62\ndiscussion on contraction principle, see for example Varadhan [106].)\nZ\n\u2126\nN[0, 1]dRt,\u03c9 = 1\nt\nZ t\n0\nN[0, 1](\u03b8s\u03c9t)ds\n(3.14)\n= 1\nt\nZ t\u22121\n0\nN[s, s + 1](\u03c9)ds + 1\nt\nZ t\nt\u22121\nN[s, s + 1](\u03c9t)ds.\nNotice that\n(3.15)\n0 \u22641\nt\nZ t\nt\u22121\nN[s, s + 1](\u03c9t)ds \u22641\nt (N[t \u22121, t](\u03c9) + N[0, 1](\u03c9)),\nand\n(3.16)\n1\nt\nZ t\u22121\n0\nN[s, s + 1](\u03c9)ds = 1\nt\n\u0014Z t\nt\u22121\nNs(\u03c9)ds \u2212\nZ 1\n0\nNs(\u03c9)ds\n\u0015\n\u2264Nt\nt ,\nand\n(3.17)\n1\nt\nZ t\u22121\n0\nN[s, s + 1](\u03c9)ds \u2265Nt\u22121 \u2212N1\nt\n= Nt\nt \u2212N[t \u22121, t] + N1\nt\n.\nHence,\n(3.18)\nNt\nt \u2212N[t \u22121, t] + N1\nt\n\u2264\nZ\n\u2126\nN[0, 1]dRt,\u03c9 \u2264Nt\nt + N[t \u22121, t] + N1\nt\n.\nFor the lower bound, for any open ball B\u03f5(x) centered at x with radius \u03f5 > 0,\nP\n\u0012Nt\nt \u2208B\u03f5(x)\n\u0013\n\u2265P\n\u0012Z\n\u2126\nN[0, 1]dRt,\u03c9 \u2208B\u03f5/2(x)\n\u0013\n(3.19)\n\u2212P\n\u0012N[t \u22121, t]\nt\n\u2265\u03f5\n4\n\u0013\n\u2212P\n\u0012N1\nt \u2265\u03f5\n4\n\u0013\n.\n63\nFor the upper bound, for any closed set C and C\u03f5 = S\nx\u2208C B\u03f5(x),\nP\n\u0012Nt\nt \u2208C\n\u0013\n\u2264P\n\u0012Z\n\u2126\nN[0, 1]dRt,\u03c9 \u2208C\u03f5\n\u0013\n(3.20)\n+ P\n\u0012N[t \u22121, t]\nt\n\u2265\u03f5\n4\n\u0013\n+ P\n\u0012N1\nt \u2265\u03f5\n4\n\u0013\n.\nFinally, by Lemma 17, we have the following superexponential estimates\n(3.21)\nlim sup\nt\u2192\u221e\n1\nt log P\n\u0012N[t \u22121, t]\nt\n\u2265\u03f5\n4\n\u0013\n= lim sup\nt\u2192\u221e\n1\nt log P\n\u0012N1\nt \u2265\u03f5\n4\n\u0013\n= \u2212\u221e.\nHence, for the lower bound, we have\n(3.22)\nlim inf\nt\u2192\u221e\n1\nt log P\n\u0012Nt\nt \u2208B\u03f5(x)\n\u0013\n\u2265\u2212I(x),\nand for the upper bound, we have\n(3.23)\nlim sup\nt\u2192\u221e\n1\nt log P\n\u0012Nt\nt \u2208C\n\u0013\n\u2264\u2212inf\nx\u2208C\u03f5 I(x),\nwhich holds for any \u03f5 > 0. Letting \u03f5 \u21930, we get the desired result.\n3.2\nLower Bound\nLemma 5. For any \u03bb, \u02c6\u03bb \u22650, \u03bb \u2212\u02c6\u03bb + \u02c6\u03bb log(\u02c6\u03bb/\u03bb) \u22650.\nProof. Write \u03bb \u2212\u02c6\u03bb + \u02c6\u03bb log(\u02c6\u03bb/\u03bb) = \u02c6\u03bb\nh\n(\u03bb/\u02c6\u03bb) \u22121 \u2212log(\u03bb/\u02c6\u03bb)\ni\n. Thus, it is su\ufb03cient\nto show that F(x) = x \u22121 \u2212log x \u22650 for any x \u22650. Note that F(0) = F(\u221e) = 0\nand F \u2032(x) = 1 \u22121\nx < 0 when 0 < x < 1 and F \u2032(x) > 0 when x > 1 and \ufb01nally\nF(1) = 0. Hence F(x) \u22650 for any x \u22650.\n64\nLemma 6. Assume H(Q) < \u221e. Then,\n(3.24)\nEQ[N[0, 1]] \u2264C1 + C2H(Q),\nwhere C1, C2 > 0 are some constants independent of Q.\nProof. If H(Q) < \u221e, then h(Q\u03c9\u2212, P \u03c9\u2212)F0\n1 < \u221efor a.e. \u03c9\u2212under Q, which implies\nthat Q\u03c9\u2212\u226aP \u03c9\u2212and thus \u02c6At \u226aAt, where \u02c6At and At are the compensators of Nt\nunder Q\u03c9\u2212and P \u03c9\u2212respectively. (For the theory of absolute continuity of point\nprocesses and Girsanov formula, see for example Lipster and Shiryaev [72] or Daley\nand Vere-Jones [27].) Since At =\nR t\n0 \u03bb(\u03c9, s)ds, we have \u02c6At =\nR t\n0 \u02c6\u03bb(\u03c9, s)ds for some\n\u02c6\u03bb. By the Girsanov formula,\n(3.25)\nH(Q) = EQ\n\u0014Z 1\n0\n\u03bb \u2212\u02c6\u03bb + log\n\u0010\n\u02c6\u03bb/\u03bb\n\u0011\n\u02c6\u03bbds\n\u0015\n.\nNotice that EQ[N[0, 1]] =\nR R 1\n0 \u02c6\u03bbdsdQ.\nZ Z 1\n0\n\u03bbdsdQ \u2264\u03f5\nZ Z 1\n0\nX\n\u03c4<s\nh(s \u2212\u03c4)dsdQ + C\u03f5\n(3.26)\n\u2264\u03f5\nZ\nh(0)N[0, 1]dQ + \u03f5\nZ X\n\u03c4<0\nh(\u2212\u03c4)dQ + C\u03f5\n= \u03f5(h(0) + \u2225h\u2225L1)EQ[N[0, 1]] + C\u03f5\n= \u03f5(h(0) + \u2225h\u2225L1)\nZ Z 1\n0\n\u02c6\u03bbdsdQ + C\u03f5.\nTherefore, we have\n(3.27)\nZ Z 1\n0\n\u02c6\u03bb \u00b7 1\u02c6\u03bb<K\u03bbdsdQ \u2264K\u03f5(h(0) + \u2225h\u2225L1)\nZ Z 1\n0\n\u02c6\u03bbdsdQ + KC\u03f5.\n65\nOn the other hand, by Lemma 5,\nH(Q) \u2265\nZ Z 1\n0\nh\n\u03bb \u2212\u02c6\u03bb + \u02c6\u03bb log(\u02c6\u03bb/\u03bb)\ni\n\u00b7 1\u02c6\u03bb\u2265K\u03bbdsdQ\n(3.28)\n\u2265(log K \u22121)\nZ Z 1\n0\n\u02c6\u03bb \u00b7 1\u02c6\u03bb\u2265K\u03bbdsdQ.\nThus,\n(3.29)\nZ Z 1\n0\n\u02c6\u03bbdsdQ \u2264K\u03f5(h(0) + \u2225h\u2225L1)\nZ Z 1\n0\n\u02c6\u03bbdsdQ + KC\u03f5 +\nH(Q)\nlog K \u22121.\nChoosing K > e and \u03f5 <\n1\nK(h(0)+\u2225h\u2225L1), we get\n(3.30)\nEQ[N[0, 1]] \u2264\nKC\u03f5\n1 \u2212K\u03f5(h(0) + \u2225h\u2225L1) +\nH(Q)\n(log K \u22121)K\u03f5(h(0) + \u2225h\u2225L1).\nLemma 7. We have the following alternative expression for H(Q).\n(3.31)\nH(Q) =\nsup\nf(\u03c9,s)\u2208B(F\u2212\u221e\ns\n)\u2229C(\u2126\u00d7R),0\u2264s\u22641\nEQ\n\u0014Z 1\n0\n\u03bb(1 \u2212ef)ds +\nZ 1\n0\nfdNs\n\u0015\n.\nProof. EQ[N[0, 1]] < \u221eimplies that EQ\u03c9\u2212\n[N[0, 1]] < \u221efor almost every \u03c9\u2212under\nQ, also P\n\u03c4\u2208\u03c9\u2212h(\u2212\u03c4) < \u221esince EQ[P\n\u03c4\u2208\u03c9\u2212h(\u2212\u03c4)] = \u2225h\u2225L1EQ[N[0, 1]] < \u221e.\nThus,\nEP \u03c9\u2212\n[N[0, 1]] = EP \u03c9\u2212\n\uf8ee\n\uf8f0\nZ 1\n0\n\u03bb\n\uf8eb\n\uf8ed\nX\n\u03c4\u2208\u03c9[0,s)\u222a\u03c9\u2212\nh(s \u2212\u03c4)\n\uf8f6\n\uf8f8ds\n\uf8f9\n\uf8fb\n(3.32)\n\u2264C\u03f5 + \u03f5h(0)EP \u03c9\u2212\n[N[0, 1]] + \u03f5\nX\n\u03c4\u2208\u03c9\u2212\nh(\u2212\u03c4) < \u221e,\n66\nso EP \u03c9\u2212\n[N[0, 1]] < \u221eby choice of \u03f5 <\n1\nh(0).\nBy the theory of absolute continuity of point processes, see for example Chapter\n13 of Daley and Vere-Jones [27], if EQ\u03c9\u2212\n[N[0, 1]], EP \u03c9\u2212\n[N[0, 1]] < \u221e, Q\u03c9\u2212\u226aP \u03c9\u2212\nif and only if \u02c6At \u226aAt, where \u02c6At and At =\nR t\n0 \u03bb(\u03c9\u2212, \u03c9, s)ds are the compensators\nof Nt under Q\u03c9\u2212and P \u03c9\u2212respectively.\nIf that\u2019s the case, we can write \u02c6At =\nR t\n0 \u02c6\u03bb(\u03c9\u2212, \u03c9, s)ds for some \u02c6\u03bb and there is Girsanov formula\n(3.33)\nlog dQ\u03c9\u2212\ndP \u03c9\u2212\n\f\f\f\f\nF0\n1\n=\nZ 1\n0\n(\u03bb \u2212\u02c6\u03bb)ds +\nZ 1\n0\nlog\n\u0010\n\u02c6\u03bb/\u03bb\n\u0011\ndNs,\nwhich implies that\n(3.34)\nH(Q) = EQ\n\u0014Z 1\n0\n\u03bb \u2212\u02c6\u03bb + log\n\u0010\n\u02c6\u03bb/\u03bb\n\u0011\n\u02c6\u03bbds\n\u0015\n.\nFor any f, \u02c6\u03bbf + (1 \u2212ef)\u03bb \u2264\u02c6\u03bb log(\u02c6\u03bb/\u03bb) + \u03bb \u2212\u02c6\u03bb and the equality is achieved when\nf = log(\u02c6\u03bb/\u03bb). Thus, clearly, we have\n(3.35)\nsup\nf(\u03c9,s)\u2208B(F\u2212\u221e\ns\n)\u2229C(\u2126\u00d7R),0\u2264s\u22641\nEQ\n\u0014Z 1\n0\n\u03bb(1 \u2212ef)ds +\nZ 1\n0\nfdNs\n\u0015\n\u2264H(Q).\nOn the other hand, we can always \ufb01nd a sequence fn convergent to log(\u02c6\u03bb/\u03bb) and\nby Fatou\u2019s lemma, we get the opposite inequality.\nNow, assume that we do not have Q\u03c9\u2212\u226aP \u03c9\u2212for a.e. \u03c9\u2212under Q. That\nimplies that H(Q) = \u221e. We want to show that\n(3.36)\nsup\nf(\u03c9,s)\u2208B(F\u2212\u221e\ns\n)\u2229C(\u2126\u00d7R),0\u2264s\u22641\nEQ\n\u0014Z 1\n0\n\u03bb(1 \u2212ef)ds +\nZ 1\n0\nfdNs\n\u0015\n= \u221e.\n67\nLet us assume that\n(3.37)\nsup\nf(\u03c9,s)\u2208B(F\u2212\u221e\ns\n)\u2229C(\u2126\u00d7R),0\u2264s\u22641\nEQ\n\u0014Z 1\n0\n\u03bb(1 \u2212ef)ds +\nZ 1\n0\nfdNs\n\u0015\n< \u221e.\nWe want to prove that H(Q) < \u221e.\nLet P \u03c9\u2212\n\u03f5\nbe the point process on [0, 1] with compensator At + \u03f5 \u02c6At. Clearly\n\u02c6At \u226aAt + \u03f5 \u02c6At and Q\u03c9\u2212\u226aP \u03c9\u2212\n\u03f5\n.\nFor any f,\nEQ\n\u0014Z 1\n0\n(1 \u2212ef)d(As + \u03f5 \u02c6As) + fd \u02c6As\n\u0015\n(3.38)\n= EQ\n\u0014Z 1\n0\n(1 \u2212ef)\u03c7f<0d(As + \u03f5 \u02c6As) + f\u03c7f<0d \u02c6As\n\u0015\n+ EQ\n\u0014Z 1\n0\n(1 \u2212ef)\u03c7f\u22650d(As + \u03f5 \u02c6As) + f\u03c7f\u22650d \u02c6As\n\u0015\n\u2264EQ\n\u0014Z 1\n0\nd(As + \u03f5 \u02c6As)\n\u0015\n+ EQ\n\u0014Z 1\n0\n(1 \u2212ef)\u03c7f\u22650dAs + f\u03c7f\u22650d \u02c6As\n\u0015\n= EQ\n\u0014Z 1\n0\nd(As + \u03f5 \u02c6As)\n\u0015\n+ EQ\n\u0014Z 1\n0\n(1 \u2212ef\u03c7f\u22650)dAs + f\u03c7f\u22650d \u02c6As\n\u0015\n\u2264C\u03b4 + \u03b4(h(0) + \u2225h\u2225L1)EQ[N[0, 1]]\n+\nsup\nf(\u03c9,s)\u2208B(F\u2212\u221e\ns\n)\u2229C(\u2126\u00d7R),0\u2264s\u22641\nEQ\n\u0014Z 1\n0\n\u03bb(1 \u2212ef)ds +\nZ 1\n0\nfdNs\n\u0015\n< \u221e.\n68\nTherefore,\n\u221e> lim inf\n\u03f5\u21930\nsup\nf(\u03c9,s)\u2208B(F\u2212\u221e\ns\n)\u2229C(\u2126\u00d7R),0\u2264s\u22641\nEQ\n\u0014Z 1\n0\n(1 \u2212ef)d(As + \u03f5 \u02c6As) + fd \u02c6As\n\u0015\n(3.39)\n= lim inf\n\u03f5\u21930\nsup\nf(\u03c9,s)\u2208B(F\u2212\u221e\ns\n)\u2229C(\u2126\u00d7R),0\u2264s\u22641\nEQ\n\"Z 1\n0\n \n1 \u2212ef + f \u00b7\nd \u02c6As\nd(As + \u03f5 \u02c6As)\n!\nd(As + \u03f5 \u02c6As)\n#\n= lim inf\n\u03f5\u21930\nEQ[h(Q\u03c9\u2212, P \u03c9\u2212\n\u03f5\n)F0\n1 ]\n= EQ[h(Q\u03c9\u2212, P \u03c9\u2212)F0\n1 ] = H(Q),\nby lower semicontinuity of the relative entropy h(\u00b7, \u00b7), Fatou\u2019s lemma, and the fact\nthat P \u03c9\u2212\n\u03f5\n\u2192P \u03c9\u2212weakly as \u03f5 \u21930. Hence H(Q) < \u221e.\nLemma 8. H(Q) is lower semicontinuous and convex in Q.\nProof. By Lemma 7, we can rewrite H(Q) as\nH(Q) =\nsup\nf(\u03c9,s)\u2208B(F\u2212\u221e\ns\n)\u2229C(\u2126\u00d7R),0\u2264s\u22641\nEQ\n\u0014Z 1\n0\n\u03bb(1 \u2212ef) + \u02c6\u03bbfds\n\u0015\n(3.40)\n=\nsup\nf(\u03c9,s)\u2208B(F\u2212\u221e\ns\n)\u2229C(\u2126\u00d7R),0\u2264s\u22641\nEQ\n\u0014Z 1\n0\n\u03bb(1 \u2212ef)ds +\nZ 1\n0\nfdNs\n\u0015\n.\nIf Qn \u2192Q, then EQn[N[0, 1]] \u2192EQ[N[0, 1]] and Qn \u2192Q weakly. Since f(\u03c9, s) \u2208\nC(\u2126\u00d7 R) \u2229B(F\u2212\u221e\ns\n),\nR 1\n0 f(\u03c9, s)dNs is continuous on \u2126, and since f is uniformly\nbounded,\nR 1\n0 f(\u03c9, s)dNs \u2264\u2225f\u2225L\u221eN[0, 1]. Hence,\n(3.41)\nEQn\n\u0014Z 1\n0\nf(\u03c9, s)dNs\n\u0015\n\u2192EQ\n\u0014Z 1\n0\nf(\u03c9, s)dNs\n\u0015\n.\n69\nLet \u03bbM = \u03bb\n\u0000P\n\u03c4<s hM(s \u2212\u03c4)\n\u0001\n, where hM(s) = h(s)\u03c7s\u2264M.\nThen, \u03bbM(\u03c9, s) \u2208\nC(\u2126\u00d7 R) and thus\nR 1\n0 \u03bbM(1 \u2212ef(\u03c9,s))ds \u2208C(\u2126). Also,\nR 1\n0 \u03bbM(1 \u2212ef(\u03c9,s))ds \u2264\nK(1 + e\u2225f\u2225L\u221e)N[\u2212M, 1], where K > 0 is some constant. Therefore,\n(3.42)\nEQn\n\u0014Z 1\n0\n\u03bbM(1 \u2212ef(\u03c9,s))ds\n\u0015\n\u2192EQ\n\u0014Z 1\n0\n\u03bbM(1 \u2212ef(\u03c9,s))ds\n\u0015\nas n \u2192\u221e. Next, notice that\n\f\f\f\fEQ\n\u0014Z 1\n0\n\u03bbM(1 \u2212ef(\u03c9,s))ds\n\u0015\n\u2212EQ\n\u0014Z 1\n0\n\u03bb(1 \u2212ef(\u03c9,s))ds\n\u0015\f\f\f\f\n(3.43)\n\u2264EQ(1 + e\u2225f\u2225L\u221e)\u03b1EQ[N[0, 1]]\nZ \u221e\nM\nh(s)ds \u21920\nas M \u2192\u221e. Similarly, we have\n(3.44)\nlim sup\nM\u2192\u221e\nlim sup\nn\u2192\u221e\n\f\f\f\fEQn\n\u0014Z 1\n0\n\u03bbM(1 \u2212ef(\u03c9,s))ds\n\u0015\n\u2212EQn\n\u0014Z 1\n0\n\u03bb(1 \u2212ef(\u03c9,s))ds\n\u0015\f\f\f\f = 0.\nHence,\n(3.45)\nEQn\n\u0014Z 1\n0\n\u03bb(\u03c9, s)(1 \u2212ef(\u03c9,s))ds\n\u0015\n\u2192EQ\n\u0014Z 1\n0\n\u03bb(\u03c9, s)(1 \u2212ef(\u03c9,s))ds\n\u0015\n.\nThe supremum is taken over a linear functional of Q, which is continuous in Q,\ntherefore the supremum over these linear functionals will be lower semicontinuous.\nSimilarly, since in the variational formula expression of H(Q) in Lemma 7, the\nsupremum is taken over a linear functional of Q, H(Q) is convex in Q.\nLemma 9. H(Q) is linear in Q.\nProof. It is in general true that the process-level entropy function H(Q) is linear\nin Q. Following the arguments in Donsker and Varadhan [31], there exists a subset\n70\n\u21260 \u2282\u2126which is F\u2212\u221e\n0\nmeasurable and a F\u2212\u221e\n0\nmeasurable map \u02c6Q : \u21260 \u2192ME(\u2126)\nsuch that Q(\u21260) = 1 for all Q \u2208MS(\u2126) and Q(\u03c9 : \u02c6Q = Q) = 1 for all Q \u2208ME(\u2126).\nTherefore, there exists a universal version, say \u02c6Q\u03c9\u2212independent of Q such that\nR \u02c6Q\u03c9\u2212Q(d\u03c9\u2212) = Q.\nSince that is true for all Q \u2208ME(\u2126), it also holds for\nQ \u2208MS(\u2126). Hence,\n(3.46)\nH(Q) = EQ h\nh(Q\u03c9\u2212, P \u03c9\u2212)F0\n1\ni\n= EQ h\nh( \u02c6Q\u03c9\u2212, P \u03c9\u2212)F0\n1\ni\n,\ni.e. H(Q) is linear in Q.\nIn this chapter, we are proving the large deviation principle for Hawkes pro-\ncesses started with empty history, i.e. with probability measure P \u2205. But when\ntime elapses, the Hawkes process generates points and that create a new history.\nWe need to understand how the history created a\ufb00ects the future. What we want\nto prove is some uniform estimates to the e\ufb00ect that if the past history is well\ncontrolled, then the new history will also be well controlled. This is essentially\nwhat the following Lemma 10 says. Consider the con\ufb01guration of points starting\nfrom time 0 up to time t. We shift it by t and denote that by wt such that wt \u2208\u2126\u2212,\nwhere \u2126\u2212is \u2126restricted to R\u2212. These notations will be used in Lemma 10.\nRemark 2. At the very beginning of the chapter, we de\ufb01ned \u03c9t. It should not be\nconfused with wt in this section.\nLemma 10. For any Q \u2208ME(\u2126) such that H(Q) < \u221eand any open neighborhood\nN of Q, there exists some K\u2212\n\u2113such that \u2205\u2208K\u2212\n\u2113and Q(K\u2212\n\u2113) \u21921 as \u2113\u2192\u221eand\n(3.47)\nlim inf\nt\u2192\u221e\n1\nt\ninf\nw0\u2208K\u2212\n\u2113\nlog P w0(Rt,\u03c9 \u2208N, wt \u2208K\u2212\n\u2113) \u2265\u2212H(Q).\n71\nProof. Let us abuse the notations a bit by de\ufb01ning\n(3.48)\n\u03bb(\u03c9\u2212) = \u03bb\n\uf8eb\n\uf8ed\nX\n\u03c4\u2208\u03c9\u2212,\u03c4\u2208\u03c9[0,s)\nh(s \u2212\u03c4)\n\uf8f6\n\uf8f8.\nFor any t > 0, since \u03bb(\u00b7) \u2265c > 0 and \u03bb(\u00b7) is Lipschitz with constant \u03b1, we have\nlog dP \u03c9\u2212\ndP w0\n\f\f\f\f\nF0\nt\n=\nZ t\n0\n\u03bb(w0) \u2212\u03bb(\u03c9\u2212)ds +\nZ t\n0\nlog\n\u0012\u03bb(\u03c9\u2212)\n\u03bb(w0)\n\u0013\ndNs\n(3.49)\n\u2264\nZ t\n0\n|\u03bb(w0) \u2212\u03bb(\u03c9\u2212)|ds +\nZ t\n0\nlog\n\u0012\n1 + |\u03bb(w0) \u2212\u03bb(\u03c9\u2212)|\n\u03bb(w0)\n\u0013\ndNs\n\u2264\nZ t\n0\n\u03b1\nX\n\u03c4\u2208\u03c9\u2212\u222aw0\nh(s \u2212\u03c4)ds +\nZ t\n0\n\u03b1\nc\nX\n\u03c4\u2208\u03c9\u2212\u222aw0\nh(s \u2212\u03c4)dNs.\nDe\ufb01ne\n(3.50)\nK\u2212\n\u2113= {\u03c9 : N[\u2212t, 0](\u03c9) \u2264\u2113(1 + t), \u2200t > 0} .\nBy the maximal ergodic theorem,\nQ((K\u2212\n\u2113)c) = Q\n\u0012\nsup\nt>0\nN[\u2212t, 0]\nt + 1\n> \u2113\n\u0013\n(3.51)\n\u2264Q\n\u0012\nsup\nt>0\nN[\u2212([t] + 1), 0]\n[t] + 1\n> \u2113\n\u0013\n= Q\n\u0012\nsup\nn\u22651,n\u2208N\nN[\u2212n, 0]\nn\n> \u2113\n\u0013\n\u2264EQ[N[0, 1]]\n\u2113\n\u21920\n(3.52)\nas \u2113\u2192\u221e. Thus Q(K\u2212\n\u2113) \u21921 as \u2113\u2192\u221e.\nFix any s > 0 and \u03c9\u2212\u2208K\u2212\n\u2113. Since h is decreasing, h\u2032 \u22640, integration by parts\n72\nshows that\nX\n\u03c4\u2208\u03c9\u2212\nh(s \u2212\u03c4) =\nZ \u221e\n0\nh(s + \u03c3)dN[\u2212\u03c3, 0]\n(3.53)\n= \u2212\nZ \u221e\n0\nN[\u2212\u03c3, 0]h\u2032(s + \u03c3)d\u03c3\n\u2264\u2212\nZ \u221e\n0\n\u2113(1 + \u03c3)h\u2032(s + \u03c3)d\u03c3\n= \u2113h(s) + \u2113\nZ \u221e\n0\nh(s + \u03c3)d\u03c3\n= \u2113h(s) + \u2113H(s),\nwhere H(t) =\nR \u221e\nt\nh(s)ds.\nTherefore, uniformly for \u03c9\u2212, w0 \u2208K\u2212\n\u2113,\n(3.54)\nZ t\n0\n\u03b1\nX\n\u03c4\u2208\u03c9\u2212\u222aw0\nh(s \u2212\u03c4)ds \u22642\u2113\u03b1\u2225h\u2225L1 + 2\u2113\u03b1u(t),\nwhere u(t) =\nR t\n0 H(s)ds and\n(3.55)\nZ t\n0\n\u03b1\nc\nX\n\u03c4\u2208\u03c9\u2212\u222aw0\nh(s \u2212\u03c4)dNs \u22642\u2113\u03b1\nc\nZ t\n0\n(h(s) + H(s))dNs.\nDe\ufb01ne\n(3.56)\nK+\n\u2113,t =\n\u001a\n\u03c9 : 2\u2113\u03b1\nc\nZ t\n0\n(h(s) + H(s))dNs \u2264\u21132(\u2225h\u2225L1 + u(t))\n\u001b\n.\nThen, uniformly in t > 0,\n(3.57)\nQ((K+\n\u2113,t)c) \u22642\u03b1EQ[N[0, 1]]\nc \u00b7 \u2113\n\u21920,\n73\nas \u2113\u2192\u221e. Thus inft>0 Q(K+\n\u2113,t) \u21921 as \u2113\u2192\u221e.\nHence, uniformly for \u03c9\u2212, w0 \u2208K\u2212\n\u2113and \u03c9 \u2208K+\n\u2113,t,\nlog dP \u03c9\u2212\ndP w0\n\f\f\f\f\nF0\nt\n\u22642\u2113\u03b1\u2225h\u2225L1 + 2\u2113\u03b1u(t) + \u21132(\u2225h\u2225L1 + u(t))\n(3.58)\n= C1(\u2113) + C2(\u2113)u(t),\n(3.59)\nwhere C1(\u2113) = 2\u2113\u03b1\u2225h\u2225L1 + \u21132\u2225h\u2225L1 and C2(\u2113) = 2\u2113\u03b1 + \u21132.\nObserve that\n(3.60)\nlim sup\nt\u2192\u221e\nu(t)\nt\n= lim sup\nt\u2192\u221e\n1\nt\nZ t\n0\nH(s)ds = 0.\nLet Dt = {Rt,\u03c9 \u2208N, wt \u2208K\u2212\n\u2113}.\nUniformly for w0 \u2208K\u2212\n\u2113,t,\nP w0(Dt)\n(3.61)\n\u2265e\u2212t(H(Q)+\u03f5)\u2212C1(\u2113)\u2212C2(\u2113)u(t)\n\u00b7 Q\n\"\nDt \u2229\n(\n1\nt log dP \u03c9\u2212\ndQ\u03c9\u2212\n\f\f\f\f\nF0\nt\n\u2264H(Q) + \u03f5\n)\n\u2229\n(\nlog dP \u03c9\u2212\ndP w0\n\f\f\f\f\nF0\nt\n\u2264C1(\u2113) + C2(\u2113)u(t)\n)#\n\u2265e\u2212t(H(Q)+\u03f5)\u2212C1(\u2113)\u2212C2(\u2113)u(t)\n\u00b7 Q\n\"\nDt \u2229\n(\n1\nt log dP \u03c9\u2212\ndQ\u03c9\u2212\n\f\f\f\f\nF0\nt\n\u2264H(Q) + \u03f5\n)\n\u2229{K+\n\u2113,t \u2229K\u2212\n\u2113}\n#\n.\nSince Q \u2208ME(\u2126), by ergodic theorem,\n(3.62)\nlim\nt\u2192\u221eQ(Rt,\u03c9 \u2208N) = 1,\n74\nand since \u03c8(\u03c9, t) = log dQ\u03c9\ndP \u03c9\n\f\f\nF0\nt satis\ufb01es,\n(3.63)\n\u03c8(\u03c9, t + s) = \u03c8(\u03c9, t) + \u03c8(\u03b8t\u03c9, s),\nEQ[\u03c8(\u03c9, t)] = tH(Q),\nfor almost every \u03c9\u2212under Q,\n(3.64)\nlim\nt\u2192\u221e\n1\nt log dP \u03c9\u2212\ndQ\u03c9\u2212\n\f\f\f\f\nF0\nt\n= H(Q).\nQ is stationary, so Q(wt \u2208K\u2212\n\u2113) \u2265Q(K\u2212\n\u2113) \u21921 as \u2113\u2192\u221e.\nAlso, Q(K+\n\u2113,t) \u2265\ninft>0 Q(K+\n\u2113,t) \u21921 as \u2113\u2192\u221e. Remember that lim supt\u2192\u221e\nu(t)\nt\n= 0. By choosing \u2113\nbig enough, we conclude that\n(3.65)\nlim inf\nt\u2192\u221e\n1\nt\ninf\nw0\u2208K\u2212\n\u2113\nlog P w0(Rt,\u03c9 \u2208N, wt \u2208K\u2212\n\u2113) \u2265\u2212H(Q) \u2212\u03f5.\nSince it holds for any \u03f5 > 0, we get the desired result.\nTheorem 12 (Lower Bound). For any open set G,\n(3.66)\nlim inf\nt\u2192\u221e\n1\nt log P(Rt,\u03c9 \u2208G) \u2265\u2212inf\nQ\u2208G H(Q).\nProof. It is su\ufb03cent to prove that for any Q \u2208MS(\u2126), H(Q) < \u221e, for any\nneighborhood N of Q, lim inft\u2192\u221e\n1\nt log P(Rt,\u03c9 \u2208N) \u2265\u2212H(Q). Since for every\ninvariant measure P \u2208MS, there exists a probability measure \u00b5P on the space ME\nof ergodic measures such that P =\nR\nME Q\u00b5P(dQ), for any Q \u2208MS(\u2126) such that\nH(Q) < \u221e, without loss of generality, we can assume that Q = P\u2113\nj=1 \u03b1jQj, where\n\u03b1j \u22650, 1 \u2264j \u2264\u2113and P\u2113\nj=1 \u03b1j = 1. By linearity of H(\u00b7), H(Q) = P\u2113\nj=1 \u03b1jH(Qj).\nDivide the interval [0, t] into subintervals of length \u03b1jt, let tj, 1 \u2264j \u2264\u2113be the\n75\nright hand endpoints of these subintervals, and let t0 = 0. For each Qj, take K\u2212\nM as\nin Lemma 10. We have min1\u2264j\u2264\u2113Qj(K\u2212\nM) \u21921, as M \u2192\u221e. Choose neighborhoods\nNj of Qj, 1 \u2264j \u2264\u2113such that S\u2113\nj=1 \u03b1jNj \u2286N. We have\nP \u2205(Rt,\u03c9 \u2208N) \u2265P \u2205(Rt1,\u03c9 \u2208N1, wt1 \u2208K\u2212\nM)\n(3.67)\n\u00b7\n\u2113Y\nj=2\ninf\nw0\u2208K\u2212\ntj\u22121\u2212tj\u22122\nP w0(Rtj\u2212tj\u22121,\u03c9 \u2208Nj, wtj\u2212tj\u22121 \u2208K\u2212\nM).\nNow, applying Lemma 10 and the linearity of H(\u00b7),\n(3.68)\nlim inf\nt\u2192\u221e\n1\nt log P \u2205(Rt,\u03c9 \u2208N) \u2265\u2212\n\u2113\nX\nj=1\n\u03b1jH(Qj) = \u2212H(Q).\n3.3\nUpper Bound\nRemark 3. By following the argument in Donsker and Varadhan [31], if \u03c9\u22127\u2192\nP \u03c9\u2212is weakly continuous, then\n(3.69)\nlim sup\nt\u2192\u221e\n1\nt log P(Rt,\u03c9 \u2208A) \u2264\u2212inf\nQ\u2208A H(Q),\nfor any compact A. If the Hawkes process has \ufb01nite range of memory, i.e. h(\u00b7)\nhas compact support, and if it is continuous, then, for any a < b, if \u03c9\u2212\nn \u2192\u03c9\u2212, we\n76\nhave\n\f\f\f\f\nZ b\na\n\u03bb(\u03c9\u2212\nn , \u03c9, s)ds \u2212\nZ b\na\n\u03bb(\u03c9\u2212, \u03c9, s)ds\n\f\f\f\f\n(3.70)\n\u2264\u03b1\nZ b\na\n\f\f\f\f\f\f\nX\n\u03c4\u2208\u03c9\u2212\nn\nh(s \u2212\u03c4) \u2212\nX\n\u03c4\u2208\u03c9\u2212\nh(s \u2212\u03c4)\n\f\f\f\f\f\f\nds \u2192\u221e,\nas n \u2192\u221e, which implies that P \u03c9\u2212\nn \u2192P \u03c9\u2212.\nIf the Hawkes process does not have \ufb01nite range of memory, then we should use\nthe speci\ufb01c features of the Hawkes process to obtain the upper bound.\nBefore we proceed, let us prove an easy but very useful lemma that we will use\nrepeatedly in the proofs of the estimates in this chapter.\nLemma 11. Let f(\u03c9, s) be F\u2212\u221e\ns\nprogressively measurable and predictable. Then,\n(3.71)\nE\nh\ne\nR t\n0 f(\u03c9,s)dNsi\n\u2264E\nh\ne\nR t\n0 (e2f(\u03c9,s)\u22121)\u03bb(\u03c9,s)dsi1/2\n.\nProof. Since exp\nnR t\n0 2f(\u03c9, s)dNs \u2212\nR t\n0(e2f(\u03c9,s) \u22121)\u03bb(\u03c9, s)ds\no\nis a martingale, by\nCauchy-Schwarz inequality,\nE\nh\ne\nR t\n0 f(\u03c9,s)dNsi\n= E\nh\ne\n1\n2\nR t\n0 2f(\u03c9,s)dNs\u22121\n2\nR t\n0 (e2f(\u03c9,s)\u22121)\u03bb(\u03c9,s)ds+ 1\n2\nR t\n0 (e2f(\u03c9,s)\u22121)\u03bb(\u03c9,s)dsi\n(3.72)\n\u2264E\nh\ne\nR t\n0 (e2f(\u03c9,s)\u22121)\u03bb(\u03c9,s)dsi1/2\n.\n77\nDe\ufb01ne CT\nCT =\n\u001a\nF(\u03c9) :=\nZ T\n0\nf(\u03c9, s)dNs \u2212\nZ T\n0\n(ef(\u03c9,s) \u22121)\u03bb(\u03c9, s)ds,\n(3.73)\nf(\u03c9, s) \u2208B(F0\ns ) \u2229C(\u2126\u00d7 R)\n\u001b\n.\nHere \u03bb(\u03c9, s) is F\u2212\u221e\ns\nprogressively measurable and predictable, and f(\u03c9, s) \u2208\nB(F0\ns ) \u2229C(\u2126\u00d7 R) means that f is F0\ns progressively measurable, predictable and\nalso bounded and continuous.\nLemma 12. For any T > 0 and F \u2208CT, we have, for any t > 0,\n(3.74)\nEP \u2205h\ne\n1\nT\nR t\n0 F(\u03b8s\u03c9)dsi\n\u22641.\nProof. For any t > 0, writing \u03c8(s) = P\nk:s+kT\u2264t F(\u03b8s+kT\u03c9),\nEP \u2205h\ne\n1\nT\nR t\n0 F(\u03b8s\u03c9)dsi\n= EP \u2205h\ne\n1\nT\nR T\n0 \u03c8(s)dsi\n(3.75)\n\u22641\nT\nZ T\n0\nEP \u2205\u0002\ne\u03c8(s)\u0003\nds = 1,\nby Jensen\u2019s inequality and the fact that EP \u2205\u0002\ne\u03c8(s)\u0003\n= 1 by iteratively conditioning\nsince EP \u03c9\u2212\u0002\neF(\u03c9)\u0003\n= 1 for any \u03c9\u2212.\nRemark 4. Under P \u2205, the F\u2212\u221e\ns\nprogressively measurable rate function \u03bb is well\nde\ufb01ned since it only creates a history between time 0 and time t. Similary, in\nthe proof in Lemma 12, EP \u03c9\u2212\u0002\neF(\u03c9)\u0003\n= 1 for any \u03c9\u2212should be interpreted as\nthe expectation is 1 given any history created between time 0 and t, which is well\nde\ufb01ned.\nNext, we need to compare 1\nT\nR t\n0 F(\u03b8s\u03c9t)ds and 1\nT\nR t\n0 F(\u03b8s\u03c9)ds.\n78\nLemma 13. For any q > 0, T > 0 and F \u2208CT,\n(3.76)\nlim sup\nt\u2192\u221e\n1\nt log EP \u2205\u0014\nexp\n\u001a\nq\n\f\f\f\f\n1\nT\nZ t\n0\nF(\u03b8s\u03c9t)ds \u22121\nT\nZ t\n0\nF(\u03b8s\u03c9)ds\n\f\f\f\f\n\u001b\u0015\n= 0.\nProof.\n\f\f\f\f\n1\nT\nZ t\n0\nF(\u03b8s\u03c9t)ds \u22121\nT\nZ t\n0\nF(\u03b8s\u03c9)ds\n\f\f\f\f\n(3.77)\n\u2264\n\f\f\f\f\n1\nT\nZ t\n0\nZ T\n0\nf(u, \u03b8s\u03c9)dNuds \u22121\nT\nZ t\n0\nZ T\n0\nf(u, \u03b8s\u03c9t)dNuds\n\f\f\f\f\n+\n\f\f\f\f\n1\nT\nZ t\n0\nZ T\n0\n(ef(u,\u03b8s\u03c9) \u22121)\u03bb(\u03b8s\u03c9, u)duds\n\u22121\nT\nZ t\n0\nZ T\n0\n(ef(u,\u03b8s\u03c9t) \u22121)\u03bb(\u03b8s\u03c9t, u)duds\n\f\f\f\f.\nIt is easy to see that\nR T\n0 f(u, \u03b8s\u03c9)dNuds is Fs\ns+T-measurable and\n(3.78)\nZ T\n0\nf(u, \u03b8s\u03c9)dNuds =\nZ T\n0\nf(u, \u03b8s\u03c9t)dNuds\nfor any 0 \u2264s \u2264t \u2212T. Hence,\n\f\f\f\f\n1\nT\nZ t\n0\nZ T\n0\nf(u, \u03b8s\u03c9)dNuds \u22121\nT\nZ t\n0\nZ T\n0\nf(u, \u03b8s\u03c9t)dNuds\n\f\f\f\f\n(3.79)\n\u22641\nT\nZ t\nt\u2212T\nZ T\n0\n|f(u, \u03b8s\u03c9)|dNuds + 1\nT\nZ t\nt\u2212T\nZ T\n0\n|f(u, \u03b8s\u03c9t)|dNuds\n\u2264\u2225f\u2225L\u221e\nT\nZ t\nt\u2212T\nN[s, s + T](\u03c9)ds + \u2225f\u2225L\u221e\nT\nZ t\nt\u2212T\nN[s, s + T](\u03c9t)ds\n\u2264\u2225f\u2225L\u221e\nT\n[N[t \u2212T, t + T](\u03c9) + N[t \u2212T, t + T](\u03c9t)]\n= \u2225f\u2225L\u221e\nT\n[N[t \u2212T, t + T](\u03c9) + N[t \u2212T, T](\u03c9) + N[0, T](\u03c9)] .\n79\nBy H\u00a8older\u2019s inequality and Lemma 17, we have\nlim sup\nt\u2192\u221e\n1\nt log EP \u2205h\ne| 1\nT\nR t\n0\nR T\n0 f(u,\u03b8s\u03c9)dNuds\u22121\nT\nR t\n0\nR T\n0 f(u,\u03b8s\u03c9t)dNuds|i\n(3.80)\n\u2264lim sup\nt\u2192\u221e\n1\nt log EP \u2205h\ne\n\u2225f\u2225L\u221e\nT\n[N[t\u2212T,t+T](\u03c9)+N[t\u2212T,T](\u03c9)+N[0,T](\u03c9)]i\n= 0.\nFurthermore,\n\f\f\f\f\n1\nT\nZ t\n0\nZ T\n0\n(ef(u,\u03b8s\u03c9) \u22121)\u03bb(\u03b8s\u03c9, u)duds \u22121\nT\nZ t\n0\nZ T\n0\n(ef(u,\u03b8s\u03c9t) \u22121)\u03bb(\u03b8s\u03c9t, u)duds\n\f\f\f\f\n(3.81)\n\u22641\nT\nZ t\n0\nZ T\n0\n\f\fef(u,\u03b8s\u03c9) \u2212ef(u,\u03b8s\u03c9t)\f\f \u03bb(\u03b8s\u03c9, u)duds\n+ 1\nT\nZ t\n0\nZ T\n0\n(ef(u,\u03b8s\u03c9t) \u22121) |\u03bb(\u03b8s\u03c9t, u) \u2212\u03bb(\u03b8s\u03c9, u)| duds.\n80\nFor the \ufb01rst term\n1\nT\nZ t\n0\nZ T\n0\n\f\fef(u,\u03b8s\u03c9) \u2212ef(u,\u03b8s\u03c9t)\f\f \u03bb(\u03b8s\u03c9, u)duds\n(3.82)\n= 1\nT\nZ t\nt\u2212T\nZ T\n0\n\f\fef(u,\u03b8s\u03c9) \u2212ef(u,\u03b8s\u03c9t)\f\f \u03bb(\u03b8s\u03c9, u)duds\n\u22642e\u2225f\u2225L\u221e\nT\nZ t\nt\u2212T\nZ T\n0\n\u03bb(\u03b8s\u03c9, u)duds\n= 2e\u2225f\u2225L\u221e\nT\nZ t\nt\u2212T\nZ T\n0\n\u03bb\n\uf8eb\n\uf8ed\nX\n\u03c4\u2208\u03c9[0,u+s)\nh(u + s \u2212\u03c4)\n\uf8f6\n\uf8f8duds\n\u22642e\u2225f\u2225L\u221eTC\u03f5 + 2e\u2225f\u2225L\u221e\nT\n\u03f5\nZ t\nt\u2212T\nZ T\n0\nX\n\u03c4\u2208\u03c9[0,u+s)\nh(u + s \u2212\u03c4)duds\n\u22642e\u2225f\u2225L\u221eTC\u03f5 + 2e\u2225f\u2225L\u221e\nT\n\u03f5\nZ t\nt\u2212T\nZ T\n0\nN[0, u + s]h(0)duds\n\u22642e\u2225f\u2225L\u221eTC\u03f5 + 2e\u2225f\u2225L\u221e\u03f5\nZ t\nt\u2212T\nN[0, s + T]h(0)ds\n\u22642e\u2225f\u2225L\u221eTC\u03f5 + 2e\u2225f\u2225L\u221e\u03f5T(N[0, t] + N[t, t + T])h(0).\nTherefore,\n(3.83)\nlim sup\nt\u2192\u221e\n1\nt log EP \u2205h\ne\n1\nT\nR t\n0\nR T\n0 |ef(u,\u03b8s\u03c9)\u2212ef(u,\u03b8s\u03c9t)|\u03bb(\u03b8s\u03c9,u)dudsi\n\u2264c(\u03f5),\nwhere c(\u03f5) \u21920 as \u03f5 \u21920; in other words, it vanishes.\n81\nFor the second term,\n1\nT\nZ t\n0\nZ T\n0\n(ef(u,\u03b8s\u03c9t) \u22121) |\u03bb(\u03b8s\u03c9t, u) \u2212\u03bb(\u03b8s\u03c9, u)| duds\n(3.84)\n\u2264e\u2225f\u2225L\u221e+ 1\nT\n\u00b7\nZ t\n0\nZ T\n0\n\u03b1\n\f\f\f\f\f\f\nX\n\u03c4\u2208\u03c9t[0,u+s)\u222a(\u03c9t)\u2212\nh(u + s \u2212\u03c4) \u2212\nX\n\u03c4\u2208\u03c9[0,u+s)\nh(u + s \u2212\u03c4)\n\f\f\f\f\f\f\nduds\n\u2264e\u2225f\u2225L\u221e+ 1\nT\nZ t\nt\u2212T\nZ T\n0\n\u03b1\nX\n\u03c4\u2208\u03c9t[0,u+s)\nh(u + s \u2212\u03c4)duds\n+ e\u2225f\u2225L\u221e+ 1\nT\nZ t\nt\u2212T\nZ T\n0\n\u03b1\nX\n\u03c4\u2208\u03c9[0,u+s)\nh(u + s \u2212\u03c4)duds\n+ e\u2225f\u2225L\u221e+ 1\nT\nZ t\n0\nZ T\n0\n\u03b1\nX\n\u03c4\u2208(\u03c9t)\u2212\nh(u + s \u2212\u03c4)duds\nAssume that h(\u00b7) is decreasing and limz\u2192\u221e\n\u03bb(z)\nz\n= 0. By applying Jensen\u2019s inequal-\nity twice, we can estimate the second term above,\nEP \u2205\u0014\ne\ne\u2225f\u2225L\u221e+1\nT\n\u03b1\nR t\nt\u2212T\nR T\n0\nR u+s\n0\nh(u+s\u2212v)dNvduds\n\u0015\n(3.85)\n\u22641\nT\nZ t\nt\u2212T\nEP \u2205h\ne(e\u2225f\u2225L\u221e+1)\u03b1\nR T\n0\nR u+s\n0\nh(u+s\u2212v)dNvdui\nds\n\u22641\nT 2\nZ t\nt\u2212T\nZ T\n0\nEP \u2205h\ne(e\u2225f\u2225L\u221e+1)\u03b1T\nR u+s\n0\nh(u+s\u2212v)dNvi\nduds\n\u22641\nT 2\nZ t\nt\u2212T\nZ T\n0\nEP \u2205h\ne\nR u+s\n0\nC(\u03b1,T,h)\u03bb(v)dvi1/2\nduds\n\u2264eC(\u03b1,T,h)C\u03f5\nT 2\nZ t\nt\u2212T\nZ T\n0\nEP \u2205\u0002\ne\u03f5C(\u03b1,T,h)N[0,u+s]h(0)\u00031/2 duds\n\u2264eC(\u03b1,T,h)C\u03f5EP \u2205\u0002\ne\u03f5C(\u03b1,T,h)N[0,t+T]h(0)\u00031/2 .\n82\nwhere C(\u03b1, T, h) = exp(2(e\u2225f\u2225L\u221e+ 1)\u03b1Th(0)) \u22121. Thus,\n(3.86)\nlim sup\nt\u2192\u221e\n1\nt log EP \u2205\u0014\ne\ne\u2225f\u2225L\u221e+1\nT\n\u03b1\nR t\nt\u2212T\nR T\n0\nR u+s\n0\nh(u+s\u2212v)dNvduds\n\u0015\n= 0.\nSimilarly, we can estimate the \ufb01rst term.\nFor the third term, by Jensen\u2019s inequality, we have\nEP \u2205\u0014\ne\ne\u2225f\u2225L\u221e+1\nT\nR t\n0\nR T\n0 \u03b1 P\n\u03c4\u2208(\u03c9t)\u2212h(u+s\u2212\u03c4)duds\n\u0015\n(3.87)\n\u22641\nT\nZ T\n0\nEP \u2205h\ne\u03b1(exp(\u2225f\u2225L\u221e)+1)\nR t\n0\nP\n\u03c4\u2208(\u03c9t)\u2212h(u+s\u2212\u03c4)dsi\ndu\n\u2264EP \u2205h\ne\u03b1(exp(\u2225f\u2225L\u221e)+1)\nR t\n0\nP\n\u03c4\u2208(\u03c9t)\u2212h(s\u2212\u03c4)dsi\n= EP \u2205h\ne\u03b1(exp(\u2225f\u2225L\u221e)+1)\nR t\n0\nR t\n0\nP\u221e\nk=0 h(s+kt+t\u2212u)dNudsi\nSince h(\u00b7) is decreasing,\nR (k+1)t\nkt\nh(s)ds \u2265th((k + 1)t). Thus\n(3.88)\n\u221e\nX\nk=0\nh(s + kt + t \u2212u) \u2264h(s + t \u2212u) + 1\nt\nZ \u221e\ns+t\u2212u\nh(v)dv.\nLet C(\u03b1, f) = \u03b1(exp(\u2225f\u2225L\u221e) + 1) and H(t) =\nR \u221e\nt\nh(s)ds. Then,\nEP \u2205h\ne\u03b1(exp(\u2225f\u2225L\u221e)+1)\nR t\n0\nR t\n0\nP\u221e\nk=0 h(s+kt+t\u2212u)dNudsi\n(3.89)\n\u2264EP \u2205h\neC(\u03b1,f)\nR t\n0\nR t\n0\n1\nt H(s+t\u2212u)dNuds+C(\u03b1,f)\nR t\n0[\nR t\n0 h(s+t\u2212u)ds]dNui\n= EP \u2205h\ne\nR t\n0[\nC(\u03b1,f)\nt\nR t\n0 H(s+t\u2212u)ds]dNu+\nR t\n0[\nR t\n0 C(\u03b1,f)h(s+t\u2212u)ds]dNui\n.\nNotice that\n(3.90)\nEP \u2205h\ne\nR t\n0[\nC(\u03b1,f)\nt\nR t\n0 H(s+t\u2212u)ds]dNui\n\u2264EP \u2205h\ne[\nC(\u03b1,f)\nt\nR t\n0 H(s)ds]Nti\n,\n83\nwhere C(\u03b1,f)\nt\nR t\n0 H(s)ds \u21920 as t \u2192\u221e, which implies that\n(3.91)\nlim sup\nt\u2192\u221e\n1\nt log EP \u2205h\ne\nR t\n0[\nC(\u03b1,f)\nt\nR t\n0 H(s+u)ds]dNui\n= 0.\nMoreover,\nEP \u2205h\ne\nR t\n0[\nR t\n0 C(\u03b1,f)h(s+t\u2212u)ds]dNui\n(3.92)\n\u2264EP \u2205h\ne\nR t\n0 (e2\nR t\n0 C(\u03b1,f)h(s+t\u2212u)ds\u22121)\u03bb(u)dui1/2\n\u2264e\n1\n2 C\u03f5\nR t\n0 (e2\nR t\n0 C(\u03b1,f)h(s+t\u2212u)ds\u22121)duEP \u2205h\ne\nR t\n0 (e\nR t\n0 2C(\u03b1,f)h(s+t\u2212u)ds\u22121)\u03f5 P\n\u03c4<u h(u\u2212\u03c4)dui1/2\n\u2264e\n1\n2 C\u03f5\nR t\n0 (e2C(\u03b1,f)H(t\u2212u)\u22121)duEP \u2205h\ne\nR t\n0 (e2C(\u03b1,f)\u2225h\u2225L1 \u22121)\u03f5 P\n\u03c4<u h(u\u2212\u03c4)dui1/2\n\u2264e\n1\n2 C\u03f5\nR t\n0 (e2C(\u03b1,f)H(u)\u22121)duEP \u2205h\ne\u03f5(e2C(\u03b1,f)\u2225h\u2225L1 \u22121)\u2225h\u2225L1Nti1/2\nNotice that it holds for any \u03f5 > 0 and that 1\nt\nR t\n0(e2C(\u03b1,f)H(u) \u22121)du \u21920 as t \u2192\u221e,\nwhich implies\n(3.93)\nlim sup\nt\u2192\u221e\n1\nt log EP \u2205h\ne\nR t\n0[\nR t\n0 C(\u03b1,f)h(s+t\u2212u)ds]dNui\n= 0.\nPutting all these things together and applying H\u00a8older\u2019s inequality several times,\nwe \ufb01nd that for any q > 0, T > 0 and F \u2208CT,\n(3.94)\nlim sup\nt\u2192\u221e\n1\nt log EP \u2205\u0014\nexp\n\u001a\nq\n\f\f\f\f\n1\nT\nZ t\n0\nF(\u03b8s\u03c9t)ds \u22121\nT\nZ t\n0\nF(\u03b8s\u03c9)ds\n\f\f\f\f\n\u001b\u0015\n= 0.\n84\nLemma 14.\n(3.95)\nlim\nT\u2192\u221e\n1\nT sup\nF\u2208CT\nZ\n\u2126\nF(\u03c9)Q(d\u03c9) \u2265H(Q).\nProof. Assume H(Q) < \u221e. For any \u03f5 > 0, there exists some f\u03f5 such that\n(3.96)\nEQ\n\u0014Z 1\n0\nf\u03f5dNs \u2212\nZ 1\n0\n(ef\u03f5 \u22121)\u03bbds\n\u0015\n\u2265H(Q) \u2212\u03f5.\nWe can \ufb01nd a sequence fT \u2208B\n\u0010\nF\u2212(T\u22121)\ns\n\u0011\n\u2229C(\u2126\u00d7R) \u2192f\u03f5 as T \u2192\u221e. By Fatou\u2019s\nlemma,\nlim inf\nT\u2192\u221e\n1\nT sup\nF\u2208CT\nZ\n\u2126\nF(\u03c9)Q(d\u03c9)\n(3.97)\n\u2265lim inf\nT\u2192\u221eEQ\n\u0014Z 1\n0\nfTdNs \u2212\nZ 1\n0\n(efT \u22121)\u03bbds\n\u0015\n\u2265H(Q) \u2212\u03f5.\nIf H(Q) = \u221e, then, for any M > 0, there exists some fM such that\n(3.98)\nEQ\n\u0014Z 1\n0\nfMdNs \u2212\nZ 1\n0\n(efM \u22121)\u03bbds\n\u0015\n\u2265M.\nRepeat the same argument as in the case that H(Q) < \u221e.\nLemma 15. For any compact set A,\n(3.99)\nlim sup\nt\u2192\u221e\n1\nt log P(Rt,\u03c9 \u2208A) \u2264\u2212inf\nQ\u2208A H(Q).\nProof. Notice that\n(3.100)\nEP \u2205\u0002\neN[0,t]\u0003\n\u2264EP \u2205h\ne(e2\u22121)\nR t\n0 \u03bb(s)dsi1/2\n\u2264EP \u2205h\ne(e2\u22121)\u03f5h(0)N[0,t]+C\u03f5(e2\u22121)i1/2\n.\n85\nBy choosing \u03f5 > 0 small enough, we have EP \u2205[eN[0,t]] \u2264eCt for some constant\nC > 0. Therefore\n(3.101)\nlim sup\n\u2113\u2192\u221e\nlim sup\nt\u2192\u221e\n1\nt log P \u2205(N[0, t] > \u2113t) = \u2212\u221e,\nwhich implies (by comparing\nR\n\u2126N[0, 1]dRt,\u03c9 and N[0, t]/t and the superexponential\nestimates in Lemma 17)\n(3.102)\nlim sup\n\u2113\u2192\u221e\nlim sup\nt\u2192\u221e\n1\nt log P \u2205\n\u0012Z\n\u2126\nN[0, 1]dRt,\u03c9 > \u2113\n\u0013\n= \u2212\u221e.\nTherefore, we need only to consider compact sets A such that for any Q \u2208A,\nEQ[N[0, 1]] < \u221e.\nNow for any A compact consisting of Q with EQ[N[0, 1]] < \u221eand for any\nF \u2208CT and for any p, q > 1,\n1\np + 1\nq = 1, by H\u00a8older\u2019s inequality, Chebychev\u2019s\ninequality, and Lemma 12,\nP \u2205(Rt,\u03c9 \u2208A)\n(3.103)\n\u2264EP \u2205h\ne\n1\npT\nR t\n0 F(\u03b8s\u03c9t)dsi\n\u00b7 exp\n\u001a\n\u2212t\npT inf\nQ\u2208A\nZ\n\u2126\nF(\u03c9)Q(d\u03c9)\n\u001b\n\u2264EP \u2205h\ne\n1\nT\nR t\n0 F(\u03b8s\u03c9)dsi1/p\nEP \u2205h\ne\nq\npT |\nR t\n0 F(\u03b8s\u03c9t)ds\u2212\nR t\n0 F(\u03b8s\u03c9)ds|i1/q\n\u00b7 exp\n\u001a\n\u2212t\npT inf\nQ\u2208A\nZ\n\u2126\nF(\u03c9)Q(d\u03c9)\n\u001b\n\u2264EP \u2205h\ne\nq\npT |\nR t\n0 F(\u03b8s\u03c9t)ds\u2212\nR t\n0 F(\u03b8s\u03c9)ds|i1/q\n\u00b7 exp\n\u001a\n\u2212t\npT inf\nQ\u2208A\nZ\n\u2126\nF(\u03c9)Q(d\u03c9)\n\u001b\nBy Lemma 13,\n(3.104)\nlim sup\nt\u2192\u221e\n1\nt log P \u2205(Rt,\u03c9 \u2208A) \u2264\u22121\np inf\nQ\u2208A\n1\nT\nZ\n\u2126\nF(\u03c9)Q(d\u03c9).\n86\nSince it holds for any p > 1, we get\n(3.105)\nlim sup\nt\u2192\u221e\n1\nt log P \u2205(Rt,\u03c9 \u2208A) \u2264\u2212inf\nQ\u2208A\n1\nT\nZ\n\u2126\nF(\u03c9)Q(d\u03c9).\nFor any compact A, given Q \u2208A and \u03f5 > 0, by Lemma 14, there exists TQ > 0\nand FQ \u2208CTQ such that\n1\nTQ\nR\n\u2126FQ(\u03c9)Q(d\u03c9) \u2265infA\u2208Q H(Q) \u22121\n2\u03f5. Since the linear\nintegral is a continuous functional of Q (see the proof of Lemma 8), there exists\na neighborhood GQ of Q such that\n1\nTQ\nR\n\u2126FQ(\u03c9)Q(d\u03c9) \u2265infA\u2208Q H(Q) \u2212\u03f5 for all\nQ \u2208GQ. Since A is compact, there exists GQ1, . . . , GQ\u2113such that A \u2282S\u2113\nj=1 GQj.\nHence\n(3.106)\ninf\n1\u2264j\u2264\u2113sup\nT>0\nsup\nF\u2208CT\ninf\nQ\u2208Gj\n1\nT\nZ\n\u2126\nF(\u03c9)Q(d\u03c9) \u2265inf\nQ\u2208A H(Q) \u2212\u03f5.\nNote that for any A and B,\nlim sup\nt\u2192\u221e\n1\nt log P(Rt,\u03c9 \u2208A \u222aB)\n(3.107)\n\u2264max\n\u001a\nlim sup\nt\u2192\u221e\n1\nt log P(Rt,\u03c9 \u2208A), lim sup\nt\u2192\u221e\n1\nt log P(Rt,\u03c9 \u2208B)\n\u001b\n.\nThus, for A \u2282S\u2113\nj=1 Gj,\n(3.108)\nlim sup\nt\u2192\u221e\n1\nt log P(Rt,\u03c9 \u2208A) \u2264\u2212inf\n1\u2264j\u2264\u2113sup\nT>0\nsup\nF\u2208CT\ninf\nQ\u2208Gj\n1\nT\nZ\nF(\u03c9)Q(d\u03c9),\nwhence lim supt\u2192\u221e\n1\nt log P(Rt,\u03c9 \u2208A) \u2264\u2212infQ\u2208A H(Q) for any compact A.\nTheorem 13 (Upper Bound). For any closed set C,\n(3.109)\nlim sup\nt\u2192\u221e\n1\nt log P(Rt,\u03c9 \u2208C) \u2264\u2212inf\nQ\u2208C H(Q).\n87\nProof. For any closed set C and compact An which is de\ufb01ned in Lemma 22, we\nhave\nlim sup\nt\u2192\u221e\n1\nt log P(Rt,\u03c9 \u2208C)\n(3.110)\n\u2264max\n\u001a\nlim sup\nt\u2192\u221e\n1\nt log P(Rt,\u03c9 \u2208C \u2229An), lim sup\nt\u2192\u221e\n1\nt log P(Rt,\u03c9 \u2208(An)c)\n\u001b\n.\nSince C \u2229An is compact, Lemma 15 implies\n(3.111)\nlim sup\nt\u2192\u221e\n1\nt log P(Rt,\u03c9 \u2208C \u2229An) \u2264\u2212\ninf\nQ\u2208C\u2229An H(Q) \u2264\u2212inf\nQ\u2208C H(Q).\nFurthermore, by Lemma 21,\nlim sup\nt\u2192\u221e\n1\nt log P(Rt,\u03c9 \u2208(An)c)\n(3.112)\n= lim sup\nt\u2192\u221e\n1\nt log P\n \nRt,\u03c9 \u2208\n\u221e\n[\nj=n\nAc\n1\nj ,j,j\n!\n\u2264max\nj\u2265n max\n\u001a\nlim sup\nt\u2192\u221e\n1\nt log P\n\u00121\nt\nZ t\n0\n\u03c7N[0,1]\u2265j(\u03b8s\u03c9t)ds \u2265\u03b5(j)\n\u0013\n,\nlim sup\nt\u2192\u221e\n1\nt log P\n\u00121\nt\nZ t\n0\n\u03c7N[0,1/j]\u22652(\u03b8s\u03c9t)ds \u2265(1/j)g(1/j)\n\u0013\n,\nlim sup\nt\u2192\u221e\n1\nt log P\n\u00121\nt\nZ t\n0\nN[0, 1]\u03c7N[0,1]\u2265\u2113(\u03b8s\u03c9t)ds \u2265m(\u2113)\n\u0013 \u001b\n\u2192\u2212\u221e\nas n \u2192\u221e. Hence,\n(3.113)\nlim sup\nt\u2192\u221e\n1\nt log P(Rt,\u03c9 \u2208C) \u2264\u2212inf\nQ\u2208C H(Q).\n88\n3.4\nSuperexponential Estimates\nIn order to get the full large deviation principle, we need the upper bound\ninequality valid for any closed set instead of for any compact set, which requires\nsome superexponential estimates.\nLemma 16. For any q > 0,\n(3.114)\nlim sup\nt\u2192\u221e\n1\nt log EP \u2205h\neq\nR t\n0 h(t\u2212s)dNsi\n= 0.\nProof.\nEP \u2205h\neq\nR t\n0 h(t\u2212s)dNsi\n\u2264EP \u2205h\ne\nR t\n0 (e2qh(t\u2212s)\u22121)\u03bb(P\n0<\u03c4<s h(s\u2212\u03c4))dsi1/2\n(3.115)\n\u2264EP \u2205h\ne(C\u03f5+h(0)\u03f5Nt)\nR t\n0 (e2qh(t\u2212s)\u22121)dsi1/2\n.\nNote that\nR t\n0(e2qh(t\u2212s) \u22121)ds =\nR t\n0(e2qh(s) \u22121)ds \u2208L1 since h \u2208L1. Therefore,\n(3.116)\nlim sup\nt\u2192\u221e\n1\nt log EP \u2205h\neq\nR t\n0 h(t\u2212s)dNsi\n\u2264c(\u03f5),\nwhere c(\u03f5) \u21920 as \u03f5 \u21920. Since it holds for any \u03f5, we get the desired result.\nLemma 17. For any q > 0 and T > 0,\n(3.117)\nlim sup\nt\u2192\u221e\n1\nt log EP \u2205\u0002\neqN[t,t+T]\u0003\n= 0.\nTherefore, for any \u03f5 > 0,\n(3.118)\nlim sup\nt\u2192\u221e\n1\nt log P \u2205(N[t, t + T] \u2265\u03f5t) = \u2212\u221e.\n89\nProof. By H\u00a8older\u2019s inequality,\nEP \u2205\u0002\neqN[t,t+T]\u0003\n\u2264EP \u2205h\ne(e2q\u22121)\nR t+T\nt\n\u03bb(P\n0<\u03c4<s h(s\u2212\u03c4))dsi1/2\n(3.119)\n\u2264e\n1\n2 (e2q\u22121)C\u03f5T \u00b7 EP \u2205h\ne\u03f5(e2q\u22121)h(0)N[t,t+T]+\u03f5(e2q\u22121)\nR t\n0 h(t\u2212s)dNsi1/2\n\u2264e\n1\n2 (e2q\u22121)C\u03f5T \u00b7 EP \u2205h\ne2\u03f5(e2q\u22121)h(0)N[t,t+T]i1/4\nEP \u2205\n\u00b7\nh\ne2\u03f5(e2q\u22121)\nR t\n0 h(t\u2212s)dNsi1/4\n.\nChoose \u03f5 < q[2(e2q \u22121)h(0)]\u22121. Then\n(3.120)\nEP \u2205\u0002\neqN[t,t+T]\u00033/4 \u2264e\n1\n2 (e2q\u22121)C\u03f5T \u00b7 EP \u2205h\ne2\u03f5(e2q\u22121)\nR t\n0 h(t\u2212s)dNsi1/4\n.\nLemma 16 completes the proof.\nLemma 18. We have the following superexponential estimates.\n(i) For any \u03f5 > 0,\n(3.121)\nlim sup\n\u03b4\u21920\nlim sup\nt\u2192\u221e\n1\nt log P\n\u0012 1\n\u03b4t\nZ t\n0\n\u03c7N[0,\u03b4]\u22652(\u03b8s\u03c9)ds \u2265\u03f5\n\u0013\n= \u2212\u221e.\n(ii) For any \u03f5 > 0,\n(3.122)\nlim sup\nM\u2192\u221e\nlim sup\nt\u2192\u221e\n1\nt log P\n\u00121\nt\nZ t\n0\n\u03c7N[0,1]\u2265M(\u03b8s\u03c9)ds \u2265\u03f5\n\u0013\n= \u2212\u221e.\n(iii) For any \u03f5 > 0,\n(3.123)\nlim sup\n\u2113\u2192\u221e\nlim sup\nt\u2192\u221e\n1\nt log P\n\u00121\nt\nZ t\n0\nN[0, 1]\u03c7N[0,1]\u2265\u2113(\u03b8s\u03c9)ds \u2265\u03f5\n\u0013\n= \u2212\u221e.\n90\nProof. (i) De\ufb01ne\n(3.124)\nN\u2113\u2032[0, t] =\nZ t\n0\n\u03c7\u03bb(s)<\u2113\u2032dNs,\n\u02c6N\u2113\u2032[0, t] =\nZ t\n0\n\u03c7\u03bb(s)\u2265\u2113\u2032dNs.\nThen N[0, t] = N\u2113\u2032[0, t] + \u02c6N\u2113\u2032[0, t] and N\u2113\u2032[0, t] has compensator\nR t\n0 \u03bb(s)\u03c7\u03bb(s)<\u2113\u2032ds\nand \u02c6N\u2113\u2032[0, t] has compensator\nR t\n0 \u03bb(s)\u03c7\u03bb(s)\u2265\u2113\u2032ds. Notice that\n(3.125)\n\u03c7N[0,\u03b4]\u22652 \u2264\u03c7N\u2113\u2032[0,\u03b4]\u22652 + \u03c7 \u02c6\nN\u2113\u2032[0,\u03b4]\u22651.\nIt is clear that N\u2113\u2032 is dominated by the usual Poisson process with rate \u2113\u2032. By\nLemma 19,\n(3.126)\nlim sup\n\u03b4\u21920\nlim sup\nt\u2192\u221e\n1\nt log P\n\u0012 1\n\u03b4t\nZ t\n0\n\u03c7N\u2113\u2032[0,\u03b4]\u22652(\u03b8s\u03c9)ds \u2265\u03f5\n2\n\u0013\n= \u2212\u221e.\nOn the other hand,\n1\n\u03b4\nZ t\n0\n\u03c7 \u02c6\nN\u2113\u2032[0,\u03b4]\u22651(\u03b8s\u03c9)ds = 1\n\u03b4\nZ t\n0\n\u03c7 \u02c6\nN\u2113\u2032[s,s+\u03b4]\u22651(\u03c9)ds\n(3.127)\n\u22641\n\u03b4\nZ t\n0\n\u02c6N\u2113\u2032[s, s + \u03b4]ds\n= 1\n\u03b4\nZ t+\u03b4\n\u03b4\n\u02c6N\u2113\u2032[0, s]ds \u22121\n\u03b4\nZ t\n0\n\u02c6N\u2113\u2032[0, s]ds\n\u2264\u02c6N\u2113\u2032[0, t] + N[t, t + \u03b4].\nBy Lemma 17, we have\n(3.128)\nlim sup\nt\u2192\u221e\n1\nt log P\n\u00121\nt N[t, t + \u03b4] \u2265\u03f5\n4\n\u0013\n= \u2212\u221e,\n91\nfor any \u03b4 > 0. Hence\n(3.129)\nlim sup\n\u03b4\u21920\nlim sup\nt\u2192\u221e\n1\nt log P\n\u00121\nt N[t, t + \u03b4] \u2265\u03f5\n4\n\u0013\n= \u2212\u221e.\nFinally, for some positive h(\u2113\u2032) to be chosen later,\nP\n\u00121\nt\n\u02c6N\u2113\u2032[0, t] \u2265\u03f5\n4\n\u0013\n\u2264E\nh\neh(\u2113\u2032) \u02c6\nN\u2113\u2032[0,t]i\ne\u2212th(\u2113\u2032)\u03f5/4\n(3.130)\n\u2264E\nh\ne(e2h(\u2113\u2032)\u22121)\nR t\n0 \u03bb(s)\u03c7\u03bb(s)\u2265\u2113\u2032dsi1/2\ne\u2212th(\u2113\u2032)\u03f5/4.\nLet f(z) =\nz\n\u03bb(z). Then f(z) \u2192\u221eas z \u2192\u221e. Let Zs = P\n\u03c4\u2208\u03c9[0,s] h(s\u2212\u03c4). Then, by\nthe de\ufb01nition of \u03bb(s) and abusing the notation a little bit, we see that \u03bb(s) = \u03bb(Zs).\nSince \u03bb(\u00b7) is increasing, its inverse function \u03bb\u22121 exists and \u03bb\u22121(\u2113\u2032) \u2192\u221eas \u2113\u2032 \u2192\u221e.\nWe have\nE\nh\ne(e2h(\u2113\u2032)\u22121)\nR t\n0 \u03bb(s)\u03c7\u03bb(s)\u2265\u2113\u2032dsi1/2\n\u2264E\nh\ne(e2h(\u2113\u2032)\u22121)\nR t\n0 \u03bb(Zs)\u03c7Zs\u2265\u03bb\u22121(\u2113\u2032)dsi1/2\n(3.131)\n\u2264E\n\u0014\ne\n(e2h(\u2113\u2032)\u22121)\nR t\n0 \u03bb(Zs)\nf(Zs)\ninfz\u2265\u2113\u2032 f(\u03bb\u22121(z)) ds\u00151/2\n.\nIt is clear that lim\u2113\u2032\u2192\u221einfz\u2265\u2113\u2032 f(\u03bb\u22121(z)) = \u221e. Choose\n(3.132)\nh(\u2113\u2032) = 1\n2 log\n\u0014\ninf\nz\u2265\u2113\u2032 f(\u03bb\u22121(z)) + 1\n\u0015\n.\n92\nThen h(\u2113\u2032) \u2192\u221eas \u2113\u2032 \u2192\u221eand\nE\n\u0014\ne\n(e2h(\u2113\u2032)\u22121)\nR t\n0 \u03bb(Zs)\nf(Zs)\ninfz\u2265\u2113\u2032 f(\u03bb\u22121(z))ds\u00151/2\n= E\nh\ne\nR t\n0 Zsdsi1/2\n(3.133)\n= E\nh\ne\nR t\n0\nP\n\u03c4\u2208\u03c9[0,s] h(s\u2212\u03c4)dsi1/2\n\u2264E\n\u0002\ne\u2225h\u2225L1Nt\u00031/2 .\nHence,\n(3.134)\nlim sup\n\u2113\u2032\u2192\u221e\nlim sup\n\u03b4\u21920\nlim sup\nt\u2192\u221e\n1\nt log P\n\u00121\nt\n\u02c6N\u2113\u2032[0, t] \u2265\u03f5\n4\n\u0013\n= \u2212\u221e.\n(ii) It is easy to see that (iii) implies (ii).\n(iii) Observe \ufb01rst that\nN[s, s + 1]\u03c7N[s,s+1]\u2265\u2113\u2264N\u2113\u2032[s, s + 1]\u03c7N\u2113\u2032[s,s+1]\u2265\u2113\n2 + \u02c6N\u2113\u2032[s, s + 1]\u03c7 \u02c6\nN\u2113\u2032[s,s+1]\u2265\u2113\n2\n(3.135)\n+ \u2113\n2\u03c7N\u2113\u2032[s,s+1]\u2265\u2113\n2 + \u2113\n2\u03c7 \u02c6\nN\u2113\u2032[s,s+1]\u2265\u2113\n2.\nFor the \ufb01rst term, notice that N\u2113\u2032 is dominated by a usual Poisson process with\nrate \u2113\u2032. Thus, by Lemma 20,\n(3.136)\nlim sup\n\u2113\u2192\u221e\nlim sup\nt\u2192\u221e\n1\nt log P\n\u00121\nt\nZ t\n0\nN\u2113\u2032[s, s + 1]\u03c7N\u2113\u2032[s,s+1]\u2265\u2113\n2(\u03c9)ds \u2265\u03f5\n4\n\u0013\n= \u2212\u221e.\nFor the second term, \u02c6N\u2113\u2032[s, s + 1]\u03c7 \u02c6\nN\u2113\u2032[s,s+1]\u2265\u2113\n2 \u2264\u02c6N\u2113\u2032[s, s + 1] and\n(3.137)\nZ t\n0\n\u02c6N\u2113\u2032[s, s + 1]ds \u2264\u02c6N\u2113\u2032[0, t] + N[t, t + 1].\n93\nBy Lemma 17,\n(3.138)\nlim sup\nt\u2192\u221e\n1\nt log P\n\u00121\nt N[t, t + 1] \u2265\u03f5\n8\n\u0013\n= \u2212\u221e,\nand by the same argument as in (i),\n(3.139)\nlim sup\n\u2113\u2032\u2192\u221e\nlim sup\n\u2113\u2192\u221e\nlim sup\nt\u2192\u221e\n1\nt log P\n\u00121\nt\n\u02c6N\u2113\u2032[0, t] \u2265\u03f5\n8\n\u0013\n= \u2212\u221e.\nFor the third term, notice that\n(3.140)\nZ t\n0\n\u2113\n2\u03c7N\u2113\u2032[s,s+1]\u2265\u2113\n2ds \u2264\nZ t\n0\nN\u2113\u2032[s, s + 1]\u03c7N\u2113\u2032[s,s+1]\u2265\u2113\n2(\u03c9)ds.\nSo we can get the same superexponential estimate as before. Finally, for the fourth\nterm,\n(3.141)\nZ t\n0\n\u2113\n2\u03c7 \u02c6\nN\u2113\u2032[s,s+1]\u2265\u2113\n2ds \u2264\nZ t\n0\n\u02c6N\u2113\u2032[s, s + 1](\u03c9)ds.\nWe can get the same superexponential estimate as before.\nLemma 19. Assume Nt is a Poisson process with constant rate \u03bb. Then for any\n\u03f5 > 0,\n(3.142)\nlim sup\n\u03b4\u21920\nlim sup\nt\u2192\u221e\n1\nt log P\n\u0012 1\n\u03b4t\nZ t\n0\n\u03c7N[s,s+\u03b4]\u22652(\u03c9)ds \u2265\u03f5\n\u0013\n= \u2212\u221e.\nProof. Let f(\u03b4, \u03c9) =\n1\nh(\u03b4)\u03c7N[0,\u03b4]\u22652(\u03c9), where h(\u03b4) is to be chosen later. By Jensen\u2019s\n94\ninequality and stationarity and independence of increments of the Poisson process,\nE\nh\ne\nR t\n0\n1\n\u03b4 f(\u03b4,\u03b8s\u03c9)dsi\n\u2264E\nh\ne\n1\n\u03b4\nR \u03b4\n0\nP[t/\u03b4]\nj=0 f(\u03b4,\u03b8s+j\u03b4\u03c9)dsi\n(3.143)\n\u2264E\n\u00141\n\u03b4\nZ \u03b4\n0\ne\nP[t/\u03b4]\nj=0 f(\u03b4,\u03b8s+j\u03b4\u03c9)ds\n\u0015\n= E\nh\ne\nP[t/\u03b4]\nj=0 f(\u03b4,\u03b8j\u03b4\u03c9)i\n= E\n\u0002\nef(\u03b4,\u03c9)\u0003[t/\u03b4]+1\n=\n\b\ne1/h(\u03b4)(1 \u2212e\u2212\u03bb\u03b4 \u2212\u03bb\u03b4e\u2212\u03bb\u03b4) + e\u2212\u03bb\u03b4 + \u03bb\u03b4e\u2212\u03bb\u03b4\t[t/\u03b4]+1\n\u2264(M \u2032e1/h(\u03b4)\u03bb2\u03b42 + 1)[t/\u03b4]+1,\nfor some M \u2032 > 0. Choose h(\u03b4) =\n1\nlog(1/\u03b4). Then,\n(3.144)\nE\nh\ne\nR t\n0\n1\n\u03b4 f(\u03b4,\u03b8s\u03c9)dsi\n\u2264(M \u2032\u03b4 + 1)[t/\u03b4]+1 \u2264eMt,\nfor some M > 0. Therefore, by Chebychev\u2019s inequality,\n(3.145)\nlim sup\nt\u2192\u221e\n1\nt log P\n\u0012\n1\n\u03b4h(\u03b4)t\nZ t\n0\n\u03c7N[s,s+\u03b4]\u22652(\u03c9)ds \u2265\n\u03f5\nh(\u03b4)\n\u0013\n\u2264M \u2212\n\u03f5\nh(\u03b4),\nwhich holds for any \u03b4 > 0. Letting \u03b4 \u21920, we get the desired result.\nLemma 20. Assume Nt is a Poisson process with constant rate \u03bb. Then for any\n\u03f5 > 0,\n(3.146)\nlim sup\n\u2113\u2192\u221e\nlim sup\nt\u2192\u221e\n1\nt log P\n\u00121\nt\nZ t\n0\nN[0, 1]\u03c7N[0,1]\u2265\u2113(\u03b8s\u03c9)ds \u2265\u03f5\n\u0013\n= \u2212\u221e.\nProof. Let h(\u2113) be some function of \u2113to be chosen later.\nFollowing the same\n95\nargument as in the proof of Lemma 19, we have\nP\n\u0012\nh(\u2113)\nZ t\n0\nN[0, 1]\u03c7N[0,1]\u2265\u2113(\u03b8s\u03c9)ds \u2265\u03f5h(\u2113)t\n\u0013\n(3.147)\n\u2264E\nh\neh(\u2113)\nR t\n0 N[0,1]\u03c7N[0,1]\u2265\u2113(\u03b8s\u03c9)dsi\ne\u2212\u03f5h(\u2113)t\n\u2264E\n\u0002\neh(\u2113)N[0,1]\u03c7N[0,1]\u2265\u2113\u0003[t]+1 e\u2212\u03f5h(\u2113)t\n=\n(\nP(N[0, 1] < \u2113) +\n\u221e\nX\nk=\u2113\neh(\u2113)ke\u2212\u03bb\u03bbk\nk!\n)[t]+1\ne\u2212\u03f5h(\u2113)t\n\u2264\n(\n1 + C1\n\u221e\nX\nk=\u2113\neh(\u2113)k+log(\u03bb)k\u2212log(k)k\n)[t]+1\ne\u2212\u03f5h(\u2113)t\n\u2264\n\b\n1 + C2eh(\u2113)\u2113+log(\u03bb)\u2113\u2212log(\u2113)\u2113\t[t]+1 e\u2212\u03f5h(\u2113)t.\nChoosing h(\u2113) = (log(\u2113))1/2 will do the work.\nThe following Lemma 21 provides us the superexponential estimates that we\nneed. These superexponential estimates have basically been done in Lemma 18.\nThe di\ufb00erence is that in the statement in Lemma 18, we used \u03c9 and in Lemma 21\nit is changed to \u03c9t which is what we needed. Lemma 21 has three statements. Part\n(i) says if you start with a sequence of simple point processes, the limiting point\nprocess may not be simple, but this has probability that is superexponentially\nsmall. Part (ii) is the usual superexponential we would expect if MS(\u2126) were\nequipped with weak topology. But since we are using a strengthened weak topology\nwith the convergence of \ufb01rst moment as well, we will also need Part (iii).\nLemma 21. We have the following superexponential estimates.\n(i) For some g(\u03b4) \u21920 as \u03b4 \u21920,\n(3.148)\nlim sup\n\u03b4\u21920\nlim sup\nt\u2192\u221e\n1\nt log P\n\u0012 1\n\u03b4t\nZ t\n0\n\u03c7N[0,\u03b4]\u22652(\u03b8s\u03c9t)ds \u2265g(\u03b4)\n\u0013\n= \u2212\u221e.\n96\n(ii) For some \u03b5(M) \u21920 as M \u2192\u221e,\n(3.149)\nlim sup\nM\u2192\u221e\nlim sup\nt\u2192\u221e\n1\nt log P\n\u00121\nt\nZ t\n0\n\u03c7N[0,1]\u2265M(\u03b8s\u03c9t)ds \u2265\u03b5(M)\n\u0013\n= \u2212\u221e.\n(iii) For some m(\u2113) \u21920 as \u2113\u2192\u221e,\n(3.150)\nlim sup\n\u2113\u2192\u221e\nlim sup\nt\u2192\u221e\n1\nt log P\n\u00121\nt\nZ t\n0\nN[0, 1]\u03c7N[0,1]\u2265\u2113(\u03b8s\u03c9t)ds \u2265m(\u2113)\n\u0013\n= \u2212\u221e.\nProof. We can replace the \u03f5 in the statement of Lemma 18 by g(\u03b4), \u03b5(M) and m(\u2113)\nby a standard analysis argument. Here, we can also replace the \u03c9 in Lemma 18\nby \u03c9t since\n(3.151)\n\f\f\f\f\nZ t\n0\n\u03c7N[0,\u03b4]\u22652(\u03b8s\u03c9t)ds \u2212\nZ t\n0\n\u03c7N[0,\u03b4]\u22652(\u03b8s\u03c9)ds\n\f\f\f\f \u22642\u03b4,\n(3.152)\n\f\f\f\f\nZ t\n0\n\u03c7N[0,1]\u2265M(\u03b8s\u03c9t)ds \u2212\nZ t\n0\n\u03c7N[0,1]\u2265M(\u03b8s\u03c9)ds\n\f\f\f\f \u22642,\nand\n\f\f\f\f\nZ t\n0\nN[0, 1]\u03c7N[0,1]\u2265\u2113(\u03b8s\u03c9t)ds \u2212\nZ t\n0\nN[0, 1]\u03c7N[0,1]\u2265\u2113(\u03b8s\u03c9)ds\n\f\f\f\f\n(3.153)\n\u2264\nZ t\nt\u22121\nN[s, s + 1](\u03c9)ds +\nZ t\nt\u22121\nN[s, s + 1](\u03c9t)ds\n\u2264N[t \u22121, t + 1](\u03c9) + N[t \u22121, t + 1](\u03c9t)\n= N[t \u22121, t + 1](\u03c9) + N[t \u22121, t](\u03c9) + N[0, 1](\u03c9).\n97\nBy Lemma 17, we have the superexponential estimate, for any \u03f5 > 0,\n(3.154)\nlim sup\nt\u2192\u221e\n1\nt log P\n\u00121\nt {N[t \u22121, t + 1](\u03c9) + N[t \u22121, t](\u03c9) + N[0, 1](\u03c9)} \u2265\u03f5\n\u0013\n= \u2212\u221e.\nLemma 22. For any \u03b4, M > 0, \u2113> 0, de\ufb01ne\nA\u03b4 = {Q \u2208MS(\u2126) : Q(N[0, \u03b4] \u22652) \u2264\u03b4g(\u03b4)} ,\n(3.155)\nAM = {Q \u2208MS(\u2126) : Q(N[0, 1] \u2265M) \u2264\u03b5(M)} ,\nA\u2113=\n\u001a\nQ \u2208MS(\u2126) :\nZ\nN[0,1]\u2265\u2113\nN[0, 1]dQ \u2264m(\u2113)\n\u001b\n,\nwhere \u03b5(M) \u21920 as M \u2192\u221e, m(\u2113) \u21920 as \u2113\u2192\u221eand g(\u03b4) \u21920 as \u03b4 \u21920. Let\nA\u03b4,M,\u2113= A\u03b4 \u2229AM \u2229A\u2113and\n(3.156)\nAn =\n\u221e\n\\\nj=n\nA 1\nj ,j,j.\nThen An is compact.\nProof. Observe that for \u03b2 > 0, the sets\n(3.157)\nK\u03b2 =\n\u221e\n\\\nk=1\n{\u03c9 : {N[\u2212k, \u2212(k \u22121)](\u03c9) \u2264\u03b2\u2113k} \u2229{N[k \u22121, k](\u03c9) \u2264\u03b2\u2113k}}\nare relatively compact in \u2126. Let K\u03b2 be the closure of K\u03b2, which is then compact.\nFor any Q \u2208An, Q(N[0, 1] \u2265M) \u2264\u03f5(M) for any M \u2265n. We can choose \u03b2 big\nenough and an increasing sequence \u2113k such that \u03b2\u21131 \u2265n and \u221e> P\u221e\nk=1 \u03f5(\u03b2\u2113k) \u21920\n98\nas \u03b2 \u2192\u221e, uniformly for Q \u2208An,\nQ\n\u0000K\u03b2\nc\u0001\n\u2264Q(Kc\n\u03b2)\n(3.158)\n= Q\n \u221e\n[\nk=1\n{N[\u2212k, \u2212(k \u22121)](\u03c9) > \u03b2\u2113k} \u2229{N[k \u22121, k](\u03c9) > \u03b2\u2113k}\n!\n\u2264\n\u221e\nX\nk=1\n{Q(N[\u2212(k \u22121), \u2212k] > \u03b2\u2113) + Q(N[k \u22121, k] > \u03b2\u2113k)}\n= 2\n\u221e\nX\nk=1\nQ(N[0, 1] > \u03b2\u2113k)\n\u22642\n\u221e\nX\nk=1\n\u03f5(\u03b2\u2113k) \u21920\nas \u03b2 \u2192\u221e. Therefore, An is tight in the weak topology and by Prokhorov theorem\nAn is precompact in the weak topology. In other words, for any sequence in An,\nthere exists a subsequence, say Qn such that Qn \u2192Q weakly as n \u2192\u221efor\nsome Q. By the de\ufb01nition of An, Qn are uniformly integrable, which implies that\nR\nN[0, 1]dQn \u2192\nR\nN[0, 1]dQ as n \u2192\u221e. It is also easy to see that An is closed by\nchecking that each A 1\nj ,j,j is closed. That implies that Q \u2208An. Finally, we need to\ncheck that Q is a simple point process. Let Ij,\u03b4 = [(j \u22121)\u03b4, j\u03b4]. We have for any\n99\nQ \u2208An,\nQ (\u2203t : N[t\u2212, t] \u22652) = Q\n \u221e\n[\nk=1\n{\u2203t \u2208[\u2212k, k] : N[t\u2212, t] \u22652}\n!\n(3.159)\n= Q\n\uf8eb\n\uf8ed\n\u221e\n[\nk=1\n\\\n\u03b4>0\n[k/\u03b4]\n[\nj=\u2212[k/\u03b4]+1\n{\u03c9 : #{\u03c9 \u222aIj,\u03b4} \u22652}\n\uf8f6\n\uf8f8\n\u2264\n\u221e\nX\nk=1\ninf\n\u03b4= 1\nm ,m\u2265n\n[k/\u03b4]\nX\nj=\u2212[k/\u03b4]+1\nQ(#{\u03c9 \u222aIj,\u03b4} \u22652)\n\u2264\n\u221e\nX\nk=1\ninf\n\u03b4= 1\nm ,m\u2265n{2[k/\u03b4]\u03b4g(\u03b4)}\n= 0.\nHence, An is precompact in our topology. Since An is closed, it is compact.\n3.5\nConcluding Remarks\nIn this chapter, we obtained a process-level large deviation principle for a wide\nclass of simple point processes, i.e. nonlinear Hawkes processes. Indeed, the meth-\nods and ideas should apply to other simple point processes as well and we should\nexpect to get the same expression for the rate function H(Q). For H(Q) < \u221e, it\nshould be of the form\n(3.160)\nH(Q) =\nZ\n\u2126\nZ 1\n0\n\u03bb(\u03c9, s) \u2212\u02c6\u03bb(\u03c9, s) + log\n \u02c6\u03bb(\u03c9, s)\n\u03bb(\u03c9, s)\n!\n\u02c6\u03bb(\u03c9, s)dsQ(d\u03c9),\nwhere \u03bb(\u03c9, s) is the intensity of the underlying simple point process. Now, it would\nbe interesting to ask for what conditions for a simple point process would guarantee\nthe process-level large deviation principle that we obtained in this chapter? First,\n100\nwe have to assume that \u03bb(\u03c9, t) is predictable and progressively measurable. Second,\nin our proof of the upper bound in this chapter, the key assumption we used about\nnonlinear Hawkes process was that limz\u2192\u221e\n\u03bb(z)\nz\n= 0. That is crucial to guarantee\nthe superexponential estimates we needed for the upper bound. If for a simple\npoint process, we have \u03bb(\u03c9, t) \u2264F(N(t, \u03c9)) for some sublinear function F(\u00b7), we\nwould expect the superexponential estimates still to work for the upper bound.\nThird, it is not enough to have \u03bb(\u03c9, t) \u2264F(N(t, \u03c9)) for sublinear F(\u00b7) to get\nthe full large deviation principle. The reason is that in the proof of lower bound,\nin particular, in Lemma 10, we need to use the fact that any memory in \u03bb(\u03c9, t)\nhas memory will decay to zero over time. For nonlinear Hawkes processes, this is\nguaranteed by the assumption that\nR \u221e\n0 h(t)dt < \u221e, which is crucial in the proof of\nLemma 10. Indeed for any simple point process P, if you want to de\ufb01ne P \u03c9\u2212, the\nprobability measure conditional on the past history \u03c9\u2212, to make sense of it, you\nhave to have some regularities to ensure that the memory of the history will decay\nto zero eventually over time. From this perspective, nonlinear Hawkes processes\nform a rich and ideal class for which the process-level large deviation principle\nholds.\n101\nChapter 4\nLarge Deviations for Markovian\nNonlinear Hawkes Processes\nIn Chapter 3, we studied the large deviations for (Nt/t \u2208\u00b7) by proving \ufb01rst\na process-level, i.e. level-3 large deviation principle and then applying the con-\ntraction principle. In this chapter, we will obtain an alternative expression for the\nrate function of the large deviation principle of (Nt/t \u2208\u00b7) when h(\u00b7) is exponential\nor sums of exponentials. The main idea is that when h(\u00b7) is exponential or sums\nof exponentials, the system is Markovian and we can use Feynman-Kac formula\nto obtain an upper bound and some tilting method to get a lower bound. The\nassumption limz\u2192\u221e\n\u03bb(z)\nz\n= 0 will provide us the compactness in order to apply a\nminmax theorem to match the lower bound and the upper bound.\n102\n4.1\nAn Ergodic Lemma\nIn this section, we prove an ergodic theorem for a class of Markovian processes\nwith jumps more general than the Markovian nonlinear Hawkes processes.\nLet Zi(t) := P\n\u03c4j<t aie\u2212bi(t\u2212\u03c4j), 1 \u2264i \u2264d, where bi > 0, ai \u0338= 0 (might be\nnegative), and \u03c4j\u2019s are the arrivals of the simple point process with intensity\n\u03bb(Z1(t), \u00b7 \u00b7 \u00b7 , Zd(t)) at time t, where \u03bb : Z \u2192R+ and Z := R\u03f51 \u00d7 \u00b7 \u00b7 \u00b7 R\u03f5d is the do-\nmain for (Z1(t), . . . , Zd(t)), where R\u03f5i := R+ or R\u2212depending on whether \u03f5i = +1\nor \u22121, where \u03f5i = +1 if ai > 0 and \u03f5i = \u22121 otherwise. If we assume the exciting\nfunction to be h(t) = Pd\ni=1 aie\u2212bit, then a Markovian nonlinear Hawkes process is\na simple point process with intensity of the form \u03bb(Pd\ni=1 Zi(t)).\nThe generator A for (Z1(t), . . . , Zd(t)) is given by\n(4.1) Af = \u2212\nd\nX\ni=1\nbizi\n\u2202f\n\u2202zi\n+ \u03bb(z1, . . . , zd)[f(z1 + a1, . . . , zd + ad) \u2212f(z1, . . . , zd)].\nWe want to prove the existence and uniqueness of the invariant probability\nmeasure for (Z1(t), . . . , Zd(t)). Here the invariance is in time.\nThe lecture notes [47] by Martin Hairer gives the criterion for the existence and\nuniqueness of the invariant probability measure for Markov processes.\nSuppose we have a jump di\ufb00usion process with generator L. If we can \ufb01nd u\nsuch that u \u22650, Lu \u2264C1 \u2212C2u for some constants C1, C2 > 0, then, there exists\nan invariant probability measure. We thereby have the following lemma.\nLemma 23. Consider h(t) = Pd\ni=1 aie\u2212bit > 0.\nLet \u03f5i = +1 if ai > 0 and\n\u03f5i = \u22121 if ai < 0. Assume \u03bb(z1, . . . , zn) \u2264Pd\ni=1 \u03b1i|zi| + \u03b2, where \u03b2 > 0 and\n\u03b1i > 0, 1 \u2264i \u2264d, satis\ufb01es Pd\ni=1\n|ai|\nbi \u03b1i < 1. Then, there exists a unique invariant\nprobability measure for (Z1(t), . . . , Zd(t)).\n103\nProof. The lecture notes [47] by Martin Hairer gives the criterion for the existence\nof an invariant probability measure for Markov processes. Suppose we have a jump\ndi\ufb00usion process with generator L. If we can \ufb01nd u such that u \u22650, Lu \u2264C1\u2212C2u\nfor some constants C1, C2 > 0, then, there exists an invariant probability measure.\nTry u(z1, . . . , zd) = Pd\ni=1 \u03f5icizi \u22650, where ci > 0, 1 \u2264i \u2264d. Then,\nAu = \u2212\nd\nX\ni=1\nbi\u03f5icizi + \u03bb(z1, . . . , zd)\nd\nX\ni=1\nai\u03f5ici\n(4.2)\n\u2264\u2212\nd\nX\ni=1\nbici|zi| +\nd\nX\ni=1\n\u03b1i|zi|\nd\nX\ni=1\n|ai|ci + \u03b2\nd\nX\ni=1\n|ai|ci.\nTaking ci = \u03b1i\nbi > 0, we get\nAu \u2264\u2212\n \n1 \u2212\nd\nX\ni=1\n|ai|\u03b1i\nbi\n!\nd\nX\ni=1\n\u03b1i|zi| + \u03b2\nd\nX\ni=1\n|ai|\u03b1i\nbi\n(4.3)\n\u2264\u2212min\n1\u2264i\u2264d bi \u00b7\n \n1 \u2212\nd\nX\ni=1\n|ai|\u03b1i\nbi\n!\nu + \u03b2\nd\nX\ni=1\n|ai|\u03b1i\nbi\n.\nNext, we will prove the uniqueness of the invariant probability measure. It is\nsu\ufb03cient to prove that for any x, y \u2208Zd, there exist times T1, T2 > 0 such that\nPx(T1, \u00b7) and Py(T2, \u00b7) are not mutually singular. Here Px(T, \u00b7) := P(Zx\nT \u2208\u00b7),\nwhere Zx\nT is ZT starting at Z0 = x, i.e. Zx\nT = xe\u2212bT + P\n\u03c4j<T ae\u2212b(T\u2212\u03c4j). To see\nthis, let us prove by contradiction. If there were two distinct invariant probability\nmeasures \u00b51 and \u00b52, then there exist two disjoints sets E1 and E2 such that \u00b51 :\nE1 \u2192E1 and \u00b52 : E2 \u2192E2, see for example Varadhan [108]. Now, we can choose\nx1 \u2208E1 and x2 \u2208E2. so that Px1(T1, \u00b7) and Px2(T2, \u00b7) are supported on E1 and\nE2 respectively for any T1, T2 > 0, which implies that Px1(T1, \u00b7) and Px2(T2, \u00b7) are\nmutually singular. This leads to a contradiction.\n104\nConsider the simplest case h(t) = ae\u2212bt.\nLet us assume that x > y > 0.\nConditioning on the event that Zx\nt and Zy\nt have exactly one jump during the time\ninterval (0, T) respectively, the laws of Px(T, \u00b7) and Py(T, \u00b7) have positive densities\non the sets\n(4.4)\n\u0000(a + x)e\u2212bT, xe\u2212bT + a\n\u0001\nand\n\u0000(a + y)e\u2212bT, ye\u2212bT + a\n\u0001\nrespectively. Choosing T > 1\nb log(x\u2212y+a\na\n), we have\n(4.5)\n\u0000(a + x)e\u2212bT, xe\u2212bT + a\n\u0001 \\ \u0000(a + y)e\u2212bT, ye\u2212bT + a\n\u0001\n\u0338= \u2205,\nwhich implies that Px(T, \u00b7) and Py(T, \u00b7) are not mutually singular.\nSimilarly, one can show the uniqueness of the invariant probability measure for\nthe multidimensional case. Indeed, it is easy to see that for any x, y \u2208Zd, Zx\nT1\nand Zy\nT2 hit a common point for some T1 and T2 after possibly di\ufb00erent number\nof jumps. Here Zx\nt := (Zx1\nt , . . . , Zxd\nt ) \u2208Zd and Zy\nt := (Zy1\nt , . . . , Zyd\nt ) \u2208Zd, where\nZxi\nt\n= xie\u2212bit + P\n\u03c4j<t aie\u2212bi(t\u2212\u03c4j), 1 \u2264i \u2264d. Since Px(T1, \u00b7) and Py(T2, \u00b7) have\nprobability densities, Px(T1, \u00b7) and Py(T2, \u00b7) are not mutually singular for some T1\nand T2.\n105\n4.2\nLarge Deviations for Markovian Nonlinear\nHawkes Processes with Exponential Exciting\nFunction\nWe assume \ufb01rst that h(t) = ae\u2212bt, where a, b > 0, i.e. the process Zt jumps\nupwards an amount a at each point and decays exponentially between points with\nrate b. In this case, Zt is Markovian.\nNotice \ufb01rst that Z0 = 0 and\n(4.6)\ndZt = \u2212bZtdt + adNt,\nwhich implies that Nt = 1\naZt + b\na\nR t\n0 Zsds.\nWe prove \ufb01rst the existence of the limit of the logarithmic moment generating\nfunction of Nt.\nTheorem 14. Assume that limz\u2192\u221e\n\u03bb(z)\nz\n= 0 and that \u03bb(\u00b7) is continuous and\nbounded below by some positive constant. Then,\n(4.7)\nlim\nt\u2192\u221e\n1\nt log E[e\u03b8Nt] = \u0393(\u03b8),\nwhere\n(4.8) \u0393(\u03b8) =\nsup\n(\u02c6\u03bb,\u02c6\u03c0)\u2208Qe\n\u001aZ \u03b8b\na z\u02c6\u03c0(dz) +\nZ\n(\u02c6\u03bb \u2212\u03bb)\u02c6\u03c0(dz) \u2212\nZ \u0010\nlog(\u02c6\u03bb/\u03bb)\n\u0011\n\u02c6\u03bb\u02c6\u03c0(dz)\n\u001b\n,\n106\nwhere Qe is de\ufb01ned as\n(4.9)\nQe =\nn\n(\u02c6\u03bb, \u02c6\u03c0) \u2208Q : \u02c6\nA has unique invariant probability measure \u02c6\u03c0\no\n,\nwhere\n(4.10)\nQ =\n\u001a\n(\u02c6\u03bb, \u02c6\u03c0) : \u02c6\u03c0 \u2208M(R+),\nZ\nz\u02c6\u03c0(dz) < \u221e, \u02c6\u03bb \u2208L1(\u02c6\u03c0), \u02c6\u03bb > 0\n\u001b\n,\nwhere M(R+) denotes the space of probability measures on R+ and for any \u02c6\u03bb such\nthat (\u02c6\u03bb, \u02c6\u03c0) \u2208Q, we de\ufb01ne the generator \u02c6\nA as\n(4.11)\n\u02c6\nAf(z) = \u2212bz\u2202f\n\u2202z + \u02c6\u03bb(z)[f(z + a) \u2212f(z)].\nfor any f : R+ \u2192R that is C1, i.e. continuously di\ufb00erentiable.\nProof. By Lemma 24,E[e\u03b8Nt] < \u221efor any \u03b8 \u2208R, also\n(4.12)\nE[e\u03b8Nt] = E\nh\ne\n\u03b8\na(Zt+b\nR t\n0 Zsds)i\n.\nDe\ufb01ne the set\n(4.13)\nU\u03b8 =\n\b\nu \u2208C1(R+, R+) : u(z) = ef(z), where f \u2208F\n\t\n,\nwhere\nF =\n\u001a\nf : f(z) = Kz + g(z) + L, K > \u03b8\na, K, L \u2208R,\n(4.14)\ng is C1 with compact support\n\u001b\n.\n107\nNow for any u \u2208U\u03b8, de\ufb01ne\n(4.15)\nM := sup\nz\u22650\nAu(z) + \u03b8b\na zu(z)\nu(z)\n.\nBy Dynkin\u2019s formula if M < \u221e, for V (z) := \u03b8b\na z, we have\nE\nh\nu(Zt)e\nR t\n0 V (Zs)dsi\n= u(Z0) +\nZ t\n0\nE\nh\n(Au(Zs) + V (Zs)u(Zs))e\nR s\n0 V (Zv)dvi\nds\n(4.16)\n\u2264u(Z0) + M\nZ t\n0\nE\nh\nu(Zs)e\nR s\n0 V (Zv)dvi\nds,\nwhich implies by Gronwall\u2019s lemma that\n(4.17)\nE\nh\nu(Zt)e\nR t\n0 V (Zs)dsi\n\u2264u(Z0)eMt = u(0)eMt.\nObserve that by the de\ufb01nition of U\u03b8, for any u \u2208U\u03b8, we have u(z) \u2265c1e\n\u03b8\na z for\nsome constant c1 > 0 and therefore by (4.12) and (4.17),\n(4.18)\nE\n\u0002\ne\u03b8Nt\u0003\n\u22641\nc1\nE\nh\nu(Zt)e\nR t\n0\n\u03b8b\na Zsdsi\n\u22641\nc1\nu(0)eMt.\nHence,\n(4.19)\nlim sup\nt\u2192\u221e\n1\nt log E\n\u0002\ne\u03b8Nt\u0003\n\u2264M = sup\nz\u22650\nAu(z) + \u03b8b\na zu(z)\nu(z)\n,\nwhich is still true even if M = \u221e. Since this holds for any u \u2208U\u03b8,\n(4.20)\nlim sup\nt\u2192\u221e\n1\nt log E\n\u0002\ne\u03b8Nt\u0003\n\u2264inf\nu\u2208U\u03b8 sup\nz\u22650\nAu(z) + \u03b8b\na zu(z)\nu(z)\n.\n108\nDe\ufb01ne the tilted probability measure \u02c6P by\n(4.21)\nd\u02c6P\ndP\n\f\f\f\f\nFt\n= exp\n(Z t\n0\n(\u03bb(Zs) \u2212\u02c6\u03bb(Zs))ds +\nZ t\n0\nlog\n \u02c6\u03bb(Zs)\n\u03bb(Zs)\n!\ndNs\n)\n.\nNotice that \u02c6P de\ufb01ned in (4.21) is indeed a probability measure by Girsanov for-\nmula. (For the theory of absolute continuity for point processes and their Girsanov\nformulas, we refer to Lipster and Shiryaev [72].)\nNow by Jensen\u2019s inequality,\nlim inf\nt\u2192\u221e\n1\nt log E[e\u03b8Nt]\n(4.22)\n= lim inf\nt\u2192\u221e\n1\nt log \u02c6E\n\"\nexp\n(\n\u03b8Nt \u2212log d\u02c6P\ndP\n\f\f\f\f\nFt\n)#\n\u2265lim inf\nt\u2192\u221e\n\u02c6E\n\"\n1\nt \u03b8Nt \u22121\nt log d\u02c6P\ndP\n\f\f\f\f\nFt\n#\n= lim inf\nt\u2192\u221e\n\u02c6E\n\"\n1\nt \u03b8Nt \u22121\nt\nZ t\n0\n(\u03bb(Zs) \u2212\u02c6\u03bb(Zs))ds \u2212\nZ t\n0\nlog\n \u02c6\u03bb(Zs)\n\u03bb(Zs)\n!\ndNs\n#\n.\nSince Nt \u2212\nR t\n0 \u02c6\u03bb(Zs)ds is a martingale under \u02c6P, we have\n(4.23)\n\u02c6E\n\"Z t\n0\nlog\n \u02c6\u03bb(Zs)\n\u03bb(Zs)\n!\n(dNs \u2212\u02c6\u03bb(Zs)ds)\n#\n= 0.\nTherefore, by the ergodic theorem, (for a reference, see Chapter 16.4 of Koralov\n109\nand Sinai [64]), for any (\u02c6\u03bb, \u02c6\u03c0) \u2208Qe,\nlim inf\nt\u2192\u221e\n1\nt log E[e\u03b8Nt]\n(4.24)\n\u2265lim inf\nt\u2192\u221e\n\u02c6E\n\"\n1\nt \u03b8Nt \u22121\nt\nZ t\n0\n(\u03bb(Zs) \u2212\u02c6\u03bb(Zs))ds \u2212\nZ t\n0\nlog\n \u02c6\u03bb(Zs)\n\u03bb(Zs)\n!\n\u02c6\u03bb(Zs)ds\n#\n=\nZ \u03b8b\na z\u02c6\u03c0(dz) +\nZ\n(\u02c6\u03bb \u2212\u03bb)\u02c6\u03c0(dz) \u2212\nZ \u0010\nlog(\u02c6\u03bb) \u2212log(\u03bb)\n\u0011\n\u02c6\u03bb\u02c6\u03c0(dz).\nHence,\nlim inf\nt\u2192\u221e\n1\nt log E[e\u03b8Nt]\n(4.25)\n\u2265\nsup\n(\u02c6\u03bb,\u02c6\u03c0)\u2208Qe\n\u001aZ \u03b8b\na z\u02c6\u03c0 +\nZ\n(\u02c6\u03bb \u2212\u03bb)\u02c6\u03c0 \u2212\nZ \u0010\nlog(\u02c6\u03bb) \u2212log(\u03bb)\n\u0011\n\u02c6\u03bb\u02c6\u03c0\n\u001b\n.\nRecall that\nF =\n\u001a\nf : f(z) = Kz + g(z) + L, K > \u03b8\na, K, L \u2208R,\n(4.26)\ng is C1 with compact support\n\u001b\n.\nWe claim that\n(4.27)\ninf\nf\u2208F\n\u001aZ\n\u02c6\nAf(z)\u02c6\u03c0(dz)\n\u001b\n=\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f3\n0\nif (\u02c6\u03bb, \u02c6\u03c0) \u2208Qe,\n\u2212\u221e\nif (\u02c6\u03bb, \u02c6\u03c0) \u2208Q\\Qe.\nIt is easy to see that for (\u02c6\u03bb, \u02c6\u03c0) \u2208Qe, and g being C1 with compact support,\nR\nAg\u02c6\u03c0 = 0. Next, we can \ufb01nd a sequence fn(z) \u2192z pointwise under the bound\n|fn(z)| \u2264\u03b1z + \u03b2, for some \u03b1, \u03b2 > 0, where fn(z) is C1 with compact support.\nBut by our de\ufb01nition of Q,\nR\nz\u02c6\u03c0 < \u221e. So by the dominated convergence theorem,\n110\nR \u02c6\nAz\u02c6\u03c0 = 0.\nThe nontrivial part is to prove that if for any g \u2208G = {g(z) +\nL, g is C1 with compact support} such that\nR \u02c6\nAg\u02c6\u03c0 = 0, then (\u02c6\u03bb, \u02c6\u03c0) \u2208Qe. We\ncan easily check the conditions in Echevrr\u00b4\u0131a [32].\n(For instance, G is dense in\nC(R+), the set of continuous and bounded functions on R+ with limit that exists\nat in\ufb01nity and \u02c6\nA satis\ufb01es the minimum principle, i.e. \u02c6\nAf(z0) \u22650 for any f(z0) =\ninfz\u2208R+ f(z). This is because at minimum, the \ufb01rst derivative of f vanishes and\n\u02c6\u03bb(z0)(f(z0 + a) \u2212f(z0)) \u22650. The other conditions in Echeverr\u00b4\u0131a [32] can also\nbe easily veri\ufb01ed.) Thus, Echevrr\u00b4\u0131a [32] implies that \u02c6\u03c0 is an invariant measure.\nNow, our proof in Lemma 23 shows that \u02c6\u03c0 has to be unique as well. Therefore,\n(\u02c6\u03bb, \u02c6\u03c0) \u2208Qe. This implies that if (\u02c6\u03bb, \u02c6\u03c0) \u2208Q\\Qe, there exists some g \u2208G, such\nthat\nR \u02c6\nAg\u02c6\u03c0 \u0338= 0. Now, any constant multiplier of g still belongs to G and thus\ninfg\u2208G\nR \u02c6\nAg\u02c6\u03c0 = \u2212\u221eand hence inff\u2208F\nR \u02c6\nAf \u02c6\u03c0 = \u2212\u221eif (\u02c6\u03bb, \u02c6\u03c0) \u2208Q\\Qe.\nTherefore,\nlim inf\nt\u2192\u221e\n1\nt log E[e\u03b8Nt] \u2265\nsup\n(\u02c6\u03bb,\u02c6\u03c0)\u2208Q\ninf\nf\u2208F\n\u001aZ \u03b8b\na z\u02c6\u03c0 \u2212\u02c6H(\u02c6\u03bb, \u02c6\u03c0) +\nZ\n\u02c6\nAf \u02c6\u03c0\n\u001b\n(4.28)\n\u2265\nsup\n(\u02c6\u03bb\u02c6\u03c0,\u02c6\u03c0)\u2208R\ninf\nf\u2208F\n\u001aZ \u03b8b\na z\u02c6\u03c0 \u2212\u02c6H(\u02c6\u03bb, \u02c6\u03c0) +\nZ\n\u02c6\nAf \u02c6\u03c0\n\u001b\n,\n(4.29)\nwhere R = {(\u02c6\u03bb\u02c6\u03c0, \u02c6\u03c0) : (\u02c6\u03bb, \u02c6\u03c0) \u2208Q} and\n(4.30)\n\u02c6H(\u02c6\u03bb, \u02c6\u03c0) =\nZ h\n(\u03bb \u2212\u02c6\u03bb) + log\n\u0010\n\u02c6\u03bb/\u03bb\n\u0011\n\u02c6\u03bb\ni\n\u02c6\u03c0.\n111\nDe\ufb01ne\nF(\u02c6\u03bb\u02c6\u03c0, \u02c6\u03c0, f) =\nZ \u03b8b\na z\u02c6\u03c0 \u2212\u02c6H(\u02c6\u03bb, \u02c6\u03c0) +\nZ\n\u02c6\nAf \u02c6\u03c0\n(4.31)\n=\nZ \u03b8b\na z\u02c6\u03c0 \u2212\u02c6H(\u02c6\u03bb, \u02c6\u03c0) \u2212\nZ\nbz\u2202f\n\u2202z \u02c6\u03c0 +\nZ\n(f(z + a) \u2212f(z))\u02c6\u03bb\u02c6\u03c0.\nNotice that F is linear in f and hence convex in f and also\n(4.32)\n\u02c6H(\u02c6\u03bb, \u02c6\u03c0) =\nsup\nf\u2208Cb(R+)\n\u001aZ h\n\u02c6\u03bbf + \u03bb(1 \u2212ef)\ni\n\u02c6\u03c0\n\u001b\n,\nwhere Cb(R+) denotes the set of bounded functions on R+. Inside the bracket\nabove, it is linear in both \u02c6\u03c0 and \u02c6\u03bb\u02c6\u03c0. Hence \u02c6H is weakly lower semicontinuous\nand convex in (\u02c6\u03bb\u02c6\u03c0, \u02c6\u03c0). Therefore, F is concave in (\u02c6\u03bb\u02c6\u03c0, \u02c6\u03c0). Furthermore, for any\nf = Kz + g + L \u2208F,\nF(\u02c6\u03bb\u02c6\u03c0, \u02c6\u03c0, f) =\nZ \u0012\u03b8\na \u2212K\n\u0013\nbz\u02c6\u03c0 \u2212\u02c6H(\u02c6\u03bb, \u02c6\u03c0) \u2212\nZ\nbz\u2202g\n\u2202z \u02c6\u03c0\n(4.33)\n+\nZ\n(g(z + a) \u2212g(z))\u02c6\u03bb\u02c6\u03c0 + Ka\nZ\n\u02c6\u03bb\u02c6\u03c0.\nIf \u03bbn\u03c0n \u2192\u03b3\u221eand \u03c0n \u2192\u03c0\u221eweakly, then, since g is C1 with compact support, we\nhave\n\u2212\nZ\nbz\u2202g\n\u2202z\u03c0n +\nZ\n(g(z + a) \u2212g(z))\u03bbn\u03c0n + Ka\nZ\n\u03bbn\u03c0n\n(4.34)\n\u2192\u2212\nZ\nbz\u2202g\n\u2202z\u03c0\u221e+\nZ\n(g(z + a) \u2212g(z))\u03b3\u221e+ Ka\nZ\n\u03b3\u221e,\nas n \u2192\u221e. Moreover, in general, if Pn \u2192P weakly, then, for any f which is upper\nsemicontinuous and bounded from above, we have lim supn\nR\nfdPn \u2264\nR\nfdP. Since\n112\n\u0000 \u03b8\na \u2212K\n\u0001\nbz is continuous and nonpositive on R+, we have\n(4.35)\nlim sup\nn\u2192\u221e\nZ \u0012\u03b8\na \u2212K\n\u0013\nbz\u03c0n \u2264\nZ \u0012\u03b8\na \u2212K\n\u0013\nbz\u03c0\u221e.\nHence, we conclude that F is upper semicontinuous in the weak topology.\nIn order to switch the supremum and in\ufb01mum in (4.29), since we have already\nproved that F is concave, upper semicontinuous in (\u02c6\u03bb\u02c6\u03c0, \u02c6\u03c0) and convex in f, it\nis su\ufb03cient to prove the compactness of R to apply Ky Fan\u2019s minmax theorem\n(see Fan [37]). Indeed, Jo\u00b4o developed some level set method and proved that it\nis su\ufb03cient to show the compactness of the level set (see Jo\u00b4o [60] and Frenk and\nKassay [40]). In other words, it su\ufb03ces to prove that, for any C \u2208R and f \u2208F,\nthe level set\n(4.36)\n\u001a\n(\u02c6\u03bb\u02c6\u03c0, \u02c6\u03c0) \u2208R : \u02c6H +\nZ\nbz\u2202f\n\u2202z \u02c6\u03c0 \u2212\u03b8b\na z\u02c6\u03c0 \u2212\u02c6\u03bb[f(z + a) \u2212f(z)]\u02c6\u03c0 \u2264C\n\u001b\nis compact.\nFix any f = Kz + g + L \u2208F, where K > \u03b8\na and g is C1 with compact support\nand L is some constant, uniformly for any pair (\u02c6\u03bb\u02c6\u03c0, \u02c6\u03c0) that is in the level set of\n113\n(4.36), there exists some C1, C2 > 0 such that\nC1 \u2265\u02c6H +\n\u0012\nK \u2212\u03b8\na\n\u0013\nb\nZ\nz\u02c6\u03c0 \u2212C2\nZ\n\u02c6\u03bb\u02c6\u03c0\n(4.37)\n\u2265\nZ\n\u02c6\u03bb\u2265cz+\u2113\nh\n\u03bb \u2212\u02c6\u03bb + \u02c6\u03bb log(\u02c6\u03bb/\u03bb)\ni\n\u02c6\u03c0 +\n\u0012\nK \u2212\u03b8\na\n\u0013\nb\nZ\nz\u02c6\u03c0\n\u2212C2\nZ\n\u02c6\u03bb\u2265cz+\u2113\n\u02c6\u03bb\u02c6\u03c0 \u2212C2\nZ\n\u02c6\u03bb<cz+\u2113\n\u02c6\u03bb\u02c6\u03c0\n\u2265\n\u0014\nmin\nz\u22650 log cz + \u2113\n\u03bb(z) \u22121 \u2212C2\n\u0015 Z\n\u02c6\u03bb\u2265cz+\u2113\n\u02c6\u03bb\u02c6\u03c0 +\n\u0014\n\u2212c \u00b7 C2 +\n\u0012\nK \u2212\u03b8\na\n\u0013\nb\n\u0015 Z\nz\u02c6\u03c0 \u2212\u2113C2.\nWe choose 0 < c <\n\u0000K \u2212\u03b8\na\n\u0001\nb\nC2 and \u2113large enough so that minz\u22650 log cz+\u2113\n\u03bb(z) \u22121\u2212C2 >\n0, where we used the fact that limz\u2192\u221e\n\u03bb(z)\nz\n= 0 and minz \u03bb(z) > 0. Hence,\n(4.38)\nZ\nz\u02c6\u03c0 \u2264C3,\nZ\n\u02c6\u03bb\u2265cz+\u2113\n\u02c6\u03bb\u02c6\u03c0 \u2264C4,\nwhere\n(4.39)\nC3 =\nC1 + \u2113C2\n\u2212c \u00b7 C2 +\n\u0000K \u2212\u03b8\na\n\u0001\nb,\nC4 =\nC1 + \u2113C2\nminz\u22650 log cz+\u2113\n\u03bb(z) \u22121 \u2212C2\n.\nTherefore, we have\n(4.40)\nZ\n\u02c6\u03bb\u02c6\u03c0 =\nZ\n\u02c6\u03bb\u2265cz+\u2113\n\u02c6\u03bb\u02c6\u03c0 +\nZ\n\u02c6\u03bb<cz+\u2113\n\u02c6\u03bb\u02c6\u03c0 \u2264C4 + c \u00b7 C3 + \u2113,\nand hence\n(4.41)\n\u02c6H(\u02c6\u03bb, \u02c6\u03c0) \u2264C1 + C2 [C4 + c \u00b7 C3 + \u2113] < \u221e.\n114\nTherefore, for any (\u03bbn\u03c0n, \u03c0n) \u2208R, we get\n(4.42)\nlim\n\u2113\u2192\u221esup\nn\nZ\nz\u2265\u2113\n\u03c0n \u2264lim\n\u2113\u2192\u221esup\nn\n1\n\u2113\nZ\nz\u03c0n \u2264lim\n\u2113\u2192\u221e\nC3\n\u2113= 0,\nwhich implies the tightness of \u03c0n. By Prokhorov\u2019s Theorem, there exists a subse-\nquence of \u03c0n which converges weakly to \u03c0\u221e. We also want to show that there exists\nsome \u03b3\u221esuch that \u03bbn\u03c0n \u2192\u03b3\u221eweakly (passing to a subsequence if necessary). It\nis enough to show that\n(i) supn\nR\n\u03bbn\u03c0n < \u221e.\n(ii) lim\u2113\u2192\u221esupn\nR\nz\u2265\u2113\u03bbn\u03c0n = 0.\n(i) and (ii) will give us tightness of \u03bbn\u03c0n and hence implies the weak convergence\nfor a subsequence.\nNow, let us prove statements (i) and (ii).\nTo prove (i), notice that\n(4.43)\nsup\nn\nZ\n\u03bbn\u03c0n = sup\nn\nZ b\naz\u03c0n \u2264b\na[C4 + c \u00b7 C3 + \u2113] < \u221e.\nTo prove (ii), notice that (\u03bb \u2212\u03bbn) + \u03bbn log(\u03bbn/\u03bb) \u22650. That is because x \u22121 \u2212\nlog x \u22650 for any x > 0 and hence\n(4.44)\n\u03bb \u2212\u02c6\u03bb + \u02c6\u03bb log(\u02c6\u03bb/\u03bb) = \u02c6\u03bb\nh\n(\u03bb/\u02c6\u03bb) \u22121 \u2212log(\u03bb/\u02c6\u03bb)\ni\n\u22650.\n115\nNotice that\nlim\n\u2113\u2192\u221esup\nn\nZ\nz\u2265\u2113\n\u03bbn\u03c0n \u2264lim\n\u2113\u2192\u221esup\nn\nZ\n\u03bbn<\n\u221a\n\u03bbz,z\u2265\u2113\n\u03bbn\u03c0n\n(4.45)\n+ lim\n\u2113\u2192\u221esup\nn\nZ\n\u03bbn\u2265\n\u221a\n\u03bbz,z\u2265\u2113\n\u03bbn\u03c0n.\nFor the \ufb01rst term, since supn\nR\nz\u03c0n < \u221eand limz\u2192\u221e\n\u03bb(z)\nz\n= 0,\n(4.46)\nlim\n\u2113\u2192\u221esup\nn\nZ\n\u03bbn<\n\u221a\n\u03bbz,z\u2265\u2113\n\u03bbn\u03c0n \u2264lim\n\u2113\u2192\u221esup\nn\nZ\nz\u2265\u2113\n\u221a\n\u03bbz\u03c0n = 0.\nFor the second term, since lim supz\u2192\u221e\n\u03bb(z)\nz\n= 0,\nlim\n\u2113\u2192\u221esup\nn\nZ\n\u03bbn\u2265\n\u221a\n\u03bbz,z\u2265\u2113\n\u03bbn\u03c0n\n(4.47)\n\u2264lim\n\u2113\u2192\u221esup\nn\n\u02c6H(\u03bbn, \u03c0n)\nsup\n\u03bbn\u2265\n\u221a\n\u03bbz,z\u2265\u2113\n\u03bbn\n\u03bb \u2212\u03bbn + \u03bbn log(\u03bbn/\u03bb) = 0.\nTherefore, passing to some subsequence if necessary, we have \u03bbn\u03c0n \u2192\u03b3\u221eand\n\u03c0n \u2192\u03c0\u221eweakly. Since we proved that F is upper semicontinuous in the weak\ntopology, the level set is compact in the weak topology. Therefore, we can switch\n116\nthe supremum and in\ufb01mum in (4.29) and get\nlim inf\nt\u2192\u221e\n1\nt log E\n\u0002\ne\u03b8Nt\u0003\n(4.48)\n\u2265inf\nf\u2208F\nsup\n\u02c6\u03c0:\nR\nz\u02c6\u03c0<\u221e\nsup\n\u02c6\u03bb\u2208L1(\u02c6\u03c0)\n\u001aZ \u03b8b\na z\u02c6\u03c0 + (\u02c6\u03bb \u2212\u03bb)\u02c6\u03c0 \u2212log(\u02c6\u03bb/\u03bb)\u02c6\u03bb\u02c6\u03c0 + \u02c6\nAf \u02c6\u03c0\n\u001b\n(4.49)\n= inf\nf\u2208F\nsup\n\u02c6\u03c0:\nR\nz\u02c6\u03c0<\u221e\nZ \u0014\u03b8bz\na + \u03bb(z)(ef(z+a)\u2212f(z) \u22121) \u2212bz\u2202f\n\u2202z\n\u0015\n\u02c6\u03c0(dz)\n(4.50)\n= inf\nf\u2208F sup\nz\u22650\n\u0014\u03b8bz\na + \u03bb(z)(ef(z+a)\u2212f(z) \u22121) \u2212bz\u2202f\n\u2202z\n\u0015\n(4.51)\n= inf\nf\u2208F sup\nz\u22650\n\u0014\u03b8bzef(z)\naef(z)\n+ \u03bb(z)\nef(z)(ef(z+a) \u2212ef(z)) \u2212bz\nef(z)\n\u2202ef(z)\n\u2202z\n\u0015\n(4.52)\n\u2265inf\nu\u2208U\u03b8 sup\nz\u22650\n\u001aAu\nu + \u03b8b\na z\n\u001b\n.\n(4.53)\nWe need some justi\ufb01cations. De\ufb01ne G(\u02c6\u03bb) = \u02c6\u03bb \u2212log(\u02c6\u03bb/\u03bb)\u02c6\u03bb + \u02c6\nAf. The supremum\nof G(\u02c6\u03bb) is achieved when \u2202G\n\u2202\u02c6\u03bb = 0 which implies \u02c6\u03bb = \u03bbef(z+a)\u2212f(z). Notice that\nfor f \u2208F, the optimal \u02c6\u03bb = \u03bbef(z+a)\u2212f(z) satis\ufb01es\nR \u02c6\u03bb\u02c6\u03c0 < \u221esince\nR\n\u03bb\u02c6\u03c0 < \u221eand\nR\nz\u02c6\u03c0 < \u221e. This gives us (4.50). Next, let us explain (4.51). For any probability\nmeasure \u02c6\u03c0,\nZ \u0014\u03b8bz\na + \u03bb(z)(ef(z+a)\u2212f(z) \u22121) \u2212bz\u2202f\n\u2202z\n\u0015\n\u02c6\u03c0(dz)\n(4.54)\n\u2264sup\nz\u22650\n\u0014\u03b8bz\na + \u03bb(z)(ef(z+a)\u2212f(z) \u22121) \u2212bz\u2202f\n\u2202z\n\u0015\n,\nwhich implies the right hand side of (4.50) is less or equal to the right hand side\nof (4.51). To prove the other direction. For any f = Kz + g + L \u2208F, we have\n\u03b8bz\na + \u03bb(z)(ef(z+a)\u2212f(z) \u22121) \u2212bz\u2202f\n\u2202z\n(4.55)\n=\n\u0012\u03b8b\na \u2212Kb\n\u0013\nz + \u03bb(z)(eKa+g(z+a)\u2212g(z) \u22121) \u2212bz\u2202g\n\u2202z,\n117\nwhich is continuous in z and also bounded on z \u2208[0, \u221e) since g is C1 with compact\nsupport and K > \u03b8\na and limz\u2192\u221e\n\u03bb(z)\nz\n= 0. Hence there exists some z\u2217\u22650 such\nthat\n\u03b8bz\na + \u03bb(z)(ef(z+a)\u2212f(z) \u22121) \u2212bz\u2202f\n\u2202z\n(4.56)\n= \u03b8bz\u2217\na\n+ \u03bb(z\u2217)(ef(z\u2217+a)\u2212f(z\u2217) \u22121) \u2212bz\u2217\u2202f\n\u2202z\n\f\f\f\f\nz=z\u2217\n.\nTake a sequence of probability measures \u02c6\u03c0n such that it has probability density\nfunction n if z \u2208[z\u2217\u22121\n2n, z\u2217+ 1\n2n] and 0 otherwise. Then, for every n,\nR\nz\u02c6\u03c0n(dz) <\n\u221e. Therefore, we have\nlim\nn\u2192\u221e\nZ \u0014\u03b8bz\na + \u03bb(z)(ef(z+a)\u2212f(z) \u22121) \u2212bz\u2202f\n\u2202z\n\u0015\n\u02c6\u03c0n(dz)\n(4.57)\n= lim\nn\u2192\u221en\nZ z\u2217+ 1\n2n\nz\u2217\u22121\n2n\n\u0014\u03b8bz\na + \u03bb(z)(ef(z+a)\u2212f(z) \u22121) \u2212bz\u2202f\n\u2202z\n\u0015\ndz\n= \u03b8bz\u2217\na\n+ \u03bb(z\u2217)(ef(z\u2217+a)\u2212f(z\u2217) \u22121) \u2212bz\u2217\u2202f\n\u2202z\n\f\f\f\f\nz=z\u2217\n= sup\nz\u22650\n\u0014\u03b8bz\na + \u03bb(z)(ef(z+a)\u2212f(z) \u22121) \u2212bz\u2202f\n\u2202z\n\u0015\n.\nWe conclude that the right hand side of (4.50) is greater or equal to the right hand\nside of (4.51).\nNotice that for any f = Kz + g + L \u2208F,\n\u03b8bz\na + \u03bb(z)(ef(z+a)\u2212f(z) \u22121) \u2212bz\u2202f\n\u2202z\n(4.58)\n= b(\u03b8 \u2212Ka)\na\nz + \u03bb(z)(eKa+g(z+a)\u2212g(z) \u22121) \u2212bz\u2202g\n\u2202z,\nwhose supremum is achieved at some \ufb01nite z\u2217> 0 since limz\u2192\u221e\n\u03bb(z)\nz\n= 0, K > \u03b8\na\n118\nand g \u2208C1 with compact support. Hence\nR\nz\u02c6\u03c0 < \u221eis satisi\ufb01ed for the optimal \u02c6\u03c0.\nThis gives us (4.51). Finally, for any f \u2208F, u = ef \u2208U\u03b8, which implies (4.53).\nLemma 24. Assume limz\u2192\u221e\n\u03bb(z)\nz\n= 0, we have E[e\u03b8Nt] < \u221efor any \u03b8 \u2208R.\nProof. Observe that for any \u03b3 \u2208R,\n(4.59)\nexp\n\u001a\n\u03b3Nt \u2212\nZ t\n0\n(e\u03b3 \u22121)\u03bb(Zs)ds\n\u001b\nis a martinagle. Since limz\u2192\u221e\n\u03bb(z)\nz\n= 0, for any \u03f5 > 0, there exists a constant\nC\u03f5 > 0 such that \u03bb(z) \u2264C\u03f5 + \u03f5z for any z \u22650. Also,\nZ t\n0\nZsds =\nZ t\n0\nZ s\n0\nh(s \u2212u)N(du)ds\n(4.60)\n=\nZ t\n0\n\u0014Z t\nu\nh(s \u2212u)ds\n\u0015\nN(du)\n\u2264\nZ t\n0\n\u0014Z \u221e\nu\nh(s \u2212u)ds\n\u0015\nN(du) = \u2225h\u2225L1Nt.\nTherefore, for any \u03b3 > 0,\n1 = E\nh\ne\u03b3Nt\u2212\nR t\n0 (e\u03b3\u22121)\u03bb(Zs)dsi\n(4.61)\n\u2265E\nh\ne\u03b3Nt\u2212(e\u03b3\u22121)\nR t\n0 (C\u03f5+\u03f5Zs)dsi\n\u2265E\n\u0002\ne\u03b3Nt\u2212(e\u03b3\u22121)C\u03f5t\u2212(e\u03b3\u22121)\u03f5\u2225h\u2225L1Nt\u0003\n.\nFor any \u03b8 > 0, choose \u03b3 > \u03b8 and \u03f5 small enough so that \u03b3 \u2212(e\u03b3 \u22121)\u03f5\u2225h\u2225L1 \u2265\u03b8.\nThen,\n(4.62)\nE\n\u0002\ne\u03b8Nt\u0003\n\u2264e(e\u03b3\u22121)C\u03f5t < \u221e.\n119\nNow, we are ready to prove the large deviations result.\nTheorem 15. Assume limz\u2192\u221e\n\u03bb(z)\nz\n= 0 and that \u03bb(\u00b7) is continuous and bounded\nbelow by some positive constant. Then, ( Nt\nt \u2208\u00b7) satis\ufb01es the large deviation prin-\nciple with the rate function I(\u00b7) as the Fenchel-Legendre transform of \u0393(\u00b7),\n(4.63)\nI(x) = sup\n\u03b8\u2208R\n{\u03b8x \u2212\u0393(\u03b8)} .\nProof. If lim supz\u2192\u221e\n\u03bb(z)\nz\n= 0, then the forthcoming Lemma 26 implies that \u0393(\u03b8) <\n\u221efor any \u03b8. Thus, by G\u00a8artner-Ellis Theorem, we have the upper bound. For\nG\u00a8artner-Ellis Theorem and a general theory of large deviations, see for example\n[30]. To prove the lower bound, it su\ufb03ces to show that for any x > 0, \u03f5 > 0, we\nhave\n(4.64)\nlim inf\nt\u2192\u221e\n1\nt log P\n\u0012Nt\nt \u2208B\u03f5(x)\n\u0013\n\u2265\u2212sup\n\u03b8\n{\u03b8x \u2212\u0393(\u03b8)},\nwhere B\u03f5(x) denotes the open ball centered at x with radius \u03f5.\nLet \u02c6P denote\nthe tilted probability measure with rate \u02c6\u03bb de\ufb01ned in Theorem 14. By Jensen\u2019s\n120\ninequality,\n1\nt log P\n\u0012Nt\nt \u2208B\u03f5(x)\n\u0013\n(4.65)\n= 1\nt log\nZ\nNt\nt \u2208B\u03f5(x)\ndP\nd\u02c6P\nd\u02c6P\n= 1\nt log \u02c6P\n\u0012Nt\nt \u2208B\u03f5(x)\n\u0013\n+ 1\nt log\n\"\n1\n\u02c6P\n\u0000 Nt\nt \u2208B\u03f5(x)\n\u0001\nZ\nNt\nt \u2208B\u03f5(x)\ndP\nd\u02c6P\nd\u02c6P\n#\n\u22651\nt log \u02c6P\n\u0012Nt\nt \u2208B\u03f5(x)\n\u0013\n\u2212\n1\n\u02c6P\n\u0000 Nt\nt \u2208B\u03f5(x)\n\u0001 \u00b7 1\nt\n\u02c6E\n\"\n1 Nt\nt \u2208B\u03f5(x) log d\u02c6P\ndP\n#\n.\nBy the ergodic theorem,\n(4.66)\nlim inf\nt\u2192\u221e\n1\nt log P\n\u0012Nt\nt \u2208B\u03f5(x)\n\u0013\n\u2265\u2212\u039b(x),\nwhere\n(4.67)\n\u039b(x) =\ninf\n(\u02c6\u03bb,\u02c6\u03c0)\u2208Qxe\n\u001aZ\n(\u03bb \u2212\u02c6\u03bb)\u02c6\u03c0 +\nZ\nlog(\u02c6\u03bb/\u03bb)\u02c6\u03bb\u02c6\u03c0\n\u001b\n,\nand\n(4.68)\nQx\ne =\n\u001a\n(\u02c6\u03bb, \u02c6\u03c0) \u2208Qe :\nZ\n\u02c6\u03bb(z)\u02c6\u03c0(dz) = x\n\u001b\n.\n121\nNotice that\n\u0393(\u03b8) =\nsup\n(\u02c6\u03bb,\u02c6\u03c0)\u2208Qe\n\u001aZ\n\u03b8\u02c6\u03bb\u02c6\u03c0 +\nZ\n(\u02c6\u03bb \u2212\u03bb)\u02c6\u03c0 \u2212\nZ\nlog(\u02c6\u03bb/\u03bb)\u02c6\u03bb\u02c6\u03c0\n\u001b\n(4.69)\n= sup\nx\nsup\n(\u02c6\u03bb,\u02c6\u03c0)\u2208Qxe\n\u001aZ\n\u03b8\u02c6\u03bb\u02c6\u03c0 +\nZ\n(\u02c6\u03bb \u2212\u03bb)\u02c6\u03c0 \u2212\nZ\nlog(\u02c6\u03bb/\u03bb)\u02c6\u03bb\u02c6\u03c0\n\u001b\n= sup\nx\nsup\n(\u02c6\u03bb,\u02c6\u03c0)\u2208Qxe\n\u001aZ \u03b8b\na z\u02c6\u03c0(dz) +\nZ\n(\u02c6\u03bb \u2212\u03bb)\u02c6\u03c0 \u2212\nZ\nlog(\u02c6\u03bb/\u03bb)\u02c6\u03bb\u02c6\u03c0\n\u001b\n= sup\nx {\u03b8x \u2212\u039b(x)}.\nWe prove in Lemma 25 that \u039b(x) is convex in x, identify it as the convex conjugate\nof \u0393(\u03b8) and thus conclude the proof.\nLemma 25. \u039b(x) in (4.67) is convex in x.\nProof. De\ufb01ne\n(4.70)\n\u02c6H(\u02c6\u03bb, \u02c6\u03c0) =\nZ\n(\u03bb \u2212\u02c6\u03bb)\u02c6\u03c0 +\nZ\nlog(\u02c6\u03bb/\u03bb)\u02c6\u03bb\u02c6\u03c0.\nThen,\n(4.71)\n\u039b(x) =\ninf\n(\u02c6\u03bb,\u02c6\u03c0)\u2208Qxe\n\u02c6H(\u02c6\u03bb, \u02c6\u03c0).\nWe want to prove that \u039b(\u03b1x1 + \u03b2x2) \u2264\u03b1\u039b(x1) + \u03b2\u039b(x2) for any \u03b1, \u03b2 \u22650 with\n\u03b1 + \u03b2 = 1. For any \u03f5 > 0, we can choose (\u02c6\u03bbk, \u02c6\u03c0k) \u2208Qxk\ne\nsuch that \u02c6H(\u02c6\u03bbk, \u02c6\u03c0k) \u2264\n\u039b(xk) + \u03f5/2, for k = 1, 2. Set\n(4.72)\n\u02c6\u03c03 = \u03b1\u02c6\u03c01 + \u03b2\u02c6\u03c02,\n\u02c6\u03bb3 =\nd(\u03b1\u02c6\u03c01)\nd(\u03b1\u02c6\u03c01 + \u03b2\u02c6\u03c02)\n\u02c6\u03bb1 +\nd(\u03b2\u02c6\u03c02)\nd(\u03b1\u02c6\u03c01 + \u03b2\u02c6\u03c02)\n\u02c6\u03bb2.\n122\nThen for any test function f,\n(4.73)\nZ\n\u02c6\nA3f \u02c6\u03c03 = \u03b1\nZ\n\u02c6\nA1f \u02c6\u03c01 + \u03b2\nZ\n\u02c6\nA2f \u02c6\u03c02 = 0,\nwhich implies (\u02c6\u03bb3, \u02c6\u03c03) \u2208Qe. Furthermore,\n(4.74)\nZ\n\u02c6\u03bb3\u02c6\u03c03 = \u03b1\nZ\n\u02c6\u03bb1\u02c6\u03c01 + \u03b2\nZ\n\u02c6\u03bb2\u02c6\u03c02 = \u03b1x1 + \u03b2x2.\nTherefore, (\u02c6\u03bb3, \u02c6\u03c03) \u2208Q\u03b1x1+\u03b2x2\ne\n. Finally, since x log x is a convex function and if we\napply Jensen\u2019s inequality, we get\n\u02c6H(\u02c6\u03bb3, \u02c6\u03c03) =\nZ h\n(\u03bb \u2212\u02c6\u03bb3 \u2212\u02c6\u03bb3 log \u03bb) + \u02c6\u03bb3 log \u02c6\u03bb3\ni\n\u02c6\u03c03\n(4.75)\n\u2264\nZ \u0014\n(\u03bb \u2212\u02c6\u03bb3 \u2212\u02c6\u03bb3 log \u03bb) + \u03b1d\u02c6\u03c01\nd\u02c6\u03c03\n\u02c6\u03bb1 log \u02c6\u03bb1 + \u03b2 d\u02c6\u03c02\nd\u02c6\u03c03\n\u02c6\u03bb2 log \u02c6\u03bb2\n\u0015\n\u02c6\u03c03\n= \u03b1 \u02c6H(\u02c6\u03bb1, \u02c6\u03c01) + \u03b2 \u02c6H(\u02c6\u03bb2, \u02c6\u03c02).\nTherefore,\n(4.76)\n\u039b(\u03b1x1 + \u03b2x2) \u2264\u02c6H(\u02c6\u03bb3, \u02c6\u03c03) \u2264\u03b1 \u02c6H(\u02c6\u03bb1, \u02c6\u03c01) + \u03b2 \u02c6H(\u02c6\u03bb2, \u02c6\u03c02) \u2264\u03b1\u039b(x1) + \u03b2\u039b(x2) + \u03f5.\nLemma 26. If lim supz\u2192\u221e\n\u03bb(z)\nbz < 1\na, then for any\n(4.77)\n\u03b8 < log\n \nb\na lim supz\u2192\u221e\n\u03bb(z)\nz\n!\n\u22121 + a\nb \u00b7 lim sup\nz\u2192\u221e\n\u03bb(z)\nz\n,\nwe have \u0393(\u03b8) < \u221e. If lim supz\u2192\u221e\n\u03bb(z)\nz\n= 0, then \u0393(\u03b8) < \u221efor any \u03b8 \u2208R.\n123\nProof. For K \u2265\u03b8\na, we have eKz \u2208U\u03b8 and\n\u0393(\u03b8) \u2264inf\ng\u2208U\u03b8 sup\nz\u22650\nAg(z) + \u03b8b\na zg(z)\ng(z)\n\u2264sup\nz\u22650\n\u001aAeKz\neKz + \u03b8b\na z\n\u001b\n(4.78)\n= sup\nz\u22650\n\u001a\n\u2212\n\u0012\nbK \u2212\u03b8b\na\n\u0013\nz + \u03bb(z)(eKa \u22121)\n\u001b\n.\nDe\ufb01ne the function\n(4.79)\nF(K) = \u2212K + lim sup\nz\u2192\u221e\n\u03bb(z)\nbz\n\u00b7 (eKa \u22121).\nThen F(0) = 0, F is convex and F(K) \u2192\u221eas K \u2192\u221eand its minimum is\nattained at\n(4.80)\nK\u2217= 1\na log\n \nb\na lim supz\u2192\u221e\n\u03bb(z)\nz\n!\n> 0,\nand F(K\u2217) < 0. Therefore, \u0393(\u03b8) < \u221efor any\n\u03b8 < \u2212a min\nK>0\n\u001a\n\u2212K + lim sup\nz\u2192\u221e\n\u03bb(z)\nbz\n\u00b7 (eKa \u22121)\n\u001b\n(4.81)\n= log\n \nb\na lim supz\u2192\u221e\n\u03bb(z)\nz\n!\n\u22121 + a\nb \u00b7 lim sup\nz\u2192\u221e\n\u03bb(z)\nz\n< K\u2217a.\nIf lim supz\u2192\u221e\n\u03bb(z)\nz\n= 0, trying eKz \u2208U\u03b8 for any K > \u03b8\na, we have \u0393(\u03b8) < \u221efor any\n\u03b8.\n124\n4.3\nLarge Deviations for Markovian Nonlinear\nHawkes Processes with Sum of Exponentials\nExciting Function\nIn this section, we consider the Markovian nonlinear Hawkes processes with\nsum of exponentials exciting functions, i.e. h(t) = Pd\ni=1 aie\u2212bit. Let\n(4.82)\nZi(t) =\nX\n\u03c4j<t\naie\u2212bi(t\u2212\u03c4j),\n1 \u2264i \u2264d,\nand Zt = Pd\ni=1 Zi(t) = P\n\u03c4j<t h(t \u2212\u03c4j), where \u03c4j\u2019s are the arrivals of the Hawkes\nprocess with intensity \u03bb(Zt) = \u03bb(Z1(t) + \u00b7 \u00b7 \u00b7 + Zd(t)) at time t.\nObserve that\nthis is a special case of the Markovian processes with jumps studied in Section 4.1\nwith \u03bb(Z1(t), Z2(t), \u00b7 \u00b7 \u00b7 , Zd(t)) taking the form \u03bb(Pd\ni=1 Zi(t)). It is easy to see that\n(Z1, . . . , Zd) is Markovian with generator\n(4.83) Af = \u2212\nd\nX\ni=1\nbizi\n\u2202f\n\u2202zi\n+ \u03bb\n \nd\nX\ni=1\nzi\n!\n\u00b7 [f(z1 + a1, . . . , zd + ad) \u2212f(z1, . . . , zd)].\nHere bi > 0 for any 1 \u2264i \u2264d and ai can be negative. But we restrict ourselves to\nthe set of bi\u2019s and ai\u2019s so that h(t) = Pd\ni=1 aie\u2212bit > 0 for any t \u22650 for the rest\nof this paper. In particular, h(0) = Pd\ni=1 ai > 0. If ai > 0, then Zi(t) \u22650 almost\nsurely; if ai < 0, then Zi(t) \u22640 almost surely.\nTheorem 16. Assume limz\u2192\u221e\n\u03bb(z)\nz\n= 0, \u03bb(\u00b7) is continuous and bounded below by\n125\na positive constant. Then,\n(4.84)\nlim\nt\u2192\u221e\n1\nt log E[e\u03b8Nt] = inf\nu\u2208U\u03b8\nsup\n(z1,...,zd)\u2208Z\n(\nAu\nu +\n\u03b8\nPd\ni=1 ai\nd\nX\ni=1\nbizi\n)\n,\nwhere Z = {(z1, . . . , zd) : aizi \u22650, 1 \u2264i \u2264d} and\n(4.85)\nU\u03b8 =\n\b\nu \u2208C1(Rd, R+), u = ef, f \u2208F\n\t\n,\nwhere\n(4.86)\nF =\n(\nf = g + \u03b8 Pd\ni=1 zi\nPd\ni=1 ai\n+ L, L \u2208R, g \u2208G\n)\n,\nwhere\n(4.87)\nG =\n(\nd\nX\ni=1\nK\u03f5izi + g, K > 0, g is C1 with compact support\n)\n.\nProof. Notice that\n(4.88)\ndZi(t) = \u2212biZi(t)dt + aidNt,\n1 \u2264i \u2264d.\nHence, aiNt = Zi(t) \u2212Zi(0) +\nR t\n0 biZi(s)ds and\n(4.89)\nE[e\u03b8Nt] = E\n\"\nexp\n(\n\u03b8 Pd\ni=1 Zi(t) \u2212Zi(0)\nPd\ni=1 ai\n+\n\u03b8\nPd\ni=1 ai\nZ t\n0\nd\nX\ni=1\nbiZi(s)ds\n)#\n.\nFollowing the same arguments in the proof of Theorem 14, we obtain the upper\n126\nbound\n(4.90)\nlim sup\nt\u2192\u221e\n1\nt log E[e\u03b8Nt] \u2264inf\nu\u2208U\u03b8\nsup\n(z1,...,zd)\u2208Z\n(\nAu\nu +\n\u03b8\nPd\ni=1 ai\nd\nX\ni=1\nbizi\n)\n.\nAs before, we can obtain the lower bound\nlim inf\nt\u2192\u221e\n1\nt log E[e\u03b8Nt]\n(4.91)\n\u2265\nsup\n(\u02c6\u03bb,\u02c6\u03c0)\u2208Qe\nZ h\n\u03b8\u02c6\u03bb \u2212\u03bb + \u02c6\u03bb \u2212\u02c6\u03bb log\n\u0010\n\u02c6\u03bb/\u03bb\n\u0011i\n\u02c6\u03c0(dz1, . . . , dzd)\n\u2265\nsup\n(\u02c6\u03bb,\u02c6\u03c0)\u2208Q\ninf\ng\u2208G\nZ h\n\u03b8\u02c6\u03bb \u2212\u03bb + \u02c6\u03bb \u2212\u02c6\u03bb log\n\u0010\n\u02c6\u03bb/\u03bb\n\u0011\n+ \u02c6\nAg\ni\n\u02c6\u03c0\n=\nsup\n(\u02c6\u03bb,\u02c6\u03c0)\u2208Q\ninf\nf\u2208F\nZ \"\n\u03b8 Pd\ni=1 bizi\nPd\ni=1 ai\n\u2212\u03bb + \u02c6\u03bb \u2212\u02c6\u03bb log\n\u0010\n\u02c6\u03bb/\u03bb\n\u0011\n+ \u02c6\nAf\n#\n\u02c6\u03c0.\nThe equality in the last line above holds by taking f = g + L + \u03b8 Pd\ni=1 zi\nPd\ni=1 ai \u2208F for\ng \u2208G, where\n(4.92)\nG =\n(\nd\nX\ni=1\nK\u03f5izi + g, K > 0, g is C1 with compact support\n)\n.\nHere, \u03f5i = ai/|ai|, 1 \u2264i \u2264d. De\ufb01ne\n(4.93)\nF(\u02c6\u03bb\u02c6\u03c0, \u02c6\u03c0, f) =\nZ \"\n\u03b8 Pd\ni=1 bizi\nPd\ni=1 ai\n+ \u02c6\nAf\n#\n\u02c6\u03c0 \u2212\u02c6H(\u02c6\u03bb, \u02c6\u03c0).\nF is linear in f and hence convex in f. Also \u02c6H is weakly lower semicontinuous\nand convex in (\u02c6\u03bb\u02c6\u03c0, \u02c6\u03c0). Therefore, F is concave in (\u02c6\u03bb\u02c6\u03c0, \u02c6\u03c0). Furthermore, for any\n127\nf = \u03b8 Pd\ni=1 zi\nPd\ni=1 ai + Pd\ni=1 K\u03f5izi + g + L \u2208F,\n(4.94) F(\u02c6\u03bb\u02c6\u03c0, \u02c6\u03c0, f) =\nZ \"\n\u03b8 +\nd\nX\ni=1\nK\u03f5iai\n#\n\u02c6\u03bb\u02c6\u03c0 \u2212\nZ\nd\nX\ni=1\nK\u03f5ibizi\u02c6\u03c0 \u2212\u02c6H(\u02c6\u03bb, \u02c6\u03c0)+\nZ\n\u02c6\nAg\u02c6\u03c0.\nIf \u03bbn\u03c0n \u2192\u03b3\u221eand \u03c0n \u2192\u03c0\u221eweakly, then, since g is C1 with compact support, we\nhave\n(4.95)\nZ \"\n\u03b8 +\nd\nX\ni=1\nK\u03f5iai\n#\n\u03bbn\u03c0n +\nZ\n\u02c6\nAg\u03c0n \u2192\nZ \"\n\u03b8 +\nd\nX\ni=1\nK\u03f5iai\n#\n\u03b3\u221e+\nZ\n\u02c6\nAg\u03c0\u221e.\nSince \u2212Pd\ni=1 K\u03f5ibizi is continuous and nonpositive on Z, we have\n(4.96)\nlim sup\nn\u2192\u221e\nZ \"\n\u2212\nd\nX\ni=1\nK\u03f5ibizi\n#\n\u03c0n \u2264\nZ \"\n\u2212\nd\nX\ni=1\nK\u03f5ibizi\n#\n\u03c0\u221e.\nHence, we conclude that F is upper semicontinuous in the weak topology.\nIn order to apply the minmax theorem, we want to prove the compactness in\nthe weak topology of the level set\n(4.97)\n(\n(\u02c6\u03bb\u02c6\u03c0, \u02c6\u03c0) :\nZ \"\n\u2212\u03b8 Pd\ni=1 bizi\nPd\ni=1 ai\n\u2212\u02c6\nAf\n#\n\u02c6\u03c0 + \u02c6H(\u02c6\u03bb, \u02c6\u03c0) \u2264C\n)\n.\nFor any f = \u03b8 Pd\ni=1 zi\nPd\ni=1 ai +Pd\ni=1 K\u03f5izi+g+L \u2208F, where g is C1 with compact support\n128\netc., there exist some C1, C2 > 0 such that\nC1 \u2265\u02c6H +\nd\nX\ni=1\nKbi\u03f5i\nZ\nzi\u02c6\u03c0 \u2212C2\nZ\n\u02c6\u03bb\u02c6\u03c0\n(4.98)\n\u2265\nZ\n\u02c6\u03bb\u2265Pd\ni=1 cizi+\u2113\nh\n\u03bb \u2212\u02c6\u03bb + \u02c6\u03bb log(\u02c6\u03bb/\u03bb)\ni\n\u02c6\u03c0 +\nd\nX\ni=1\nKbi\u03f5i\nZ\nzi\u02c6\u03c0\n\u2212C2\nZ\n\u02c6\u03bb\u2265Pd\ni=1 cizi+\u2113\n\u02c6\u03bb\u02c6\u03c0 \u2212C2\nZ\n\u02c6\u03bb<Pd\ni=1 cizi+\u2113\n\u02c6\u03bb\u02c6\u03c0\n\u2265\n\u0014\nmin\n(z1,...,zd)\u2208Z log c1z1 + \u00b7 \u00b7 \u00b7 + cdzd + \u2113\n\u03bb(z1 + \u00b7 \u00b7 \u00b7 + zd)\n\u22121 \u2212C2\n\u0015 Z\n\u02c6\u03bb\u2265Pd\ni=1 cizi+\u2113\n\u02c6\u03bb\u02c6\u03c0\n+\nd\nX\ni=1\n[\u2212ci \u00b7 C2 + Kbi\u03f5i]\nZ\nzi\u02c6\u03c0 \u2212\u2113C2.\nIf ai > 0, then \u03f5i > 0, pick up ci > 0 such that \u2212ci \u00b7 C2 + Kbi\u03f5i > 0. If ai < 0, then\n\u03f5i < 0, pick up ci such that \u2212ci \u00b7 C2 + Kbi\u03f5i < 0. Finally, choose \u2113big enough such\nthat the big bracket above is positive. Then\n(4.99)\nZ\n|zi|\u02c6\u03c0 \u2264C3,\nZ\n\u02c6\u03bb\u2265Pd\ni=1 cizi+\u2113\n\u02c6\u03bb\u02c6\u03c0 \u2264C4.\nHence,\nR \u02c6\u03bb\u02c6\u03c0 \u2264C5 and \u02c6H \u2264C6. We can use the similar method as in the proof of\nTheorem 14 to show that\n(4.100)\nlim\n\u2113\u2192\u221esup\nn\nZ\n|zi|>\u2113\n\u03bbn\u03c0n = 0,\n1 \u2264i \u2264d.\nFor any (\u03bbn\u03c0n, \u03c0n) \u2208R, we can \ufb01nd a subsequence that converges in the weak\n129\ntopology by Prokhorov\u2019s Theorem. Therefore,\nlim inf\nt\u2192\u221e\n1\nt log E[e\u03b8Nt]\n(4.101)\n\u2265\nsup\n(\u02c6\u03bb,\u02c6\u03c0)\u2208Q\ninf\nf\u2208F\nZ \"\n\u03b8 Pd\ni=1 bizi\nPd\ni=1 ai\n\u2212\u03bb + \u02c6\u03bb \u2212\u02c6\u03bb log\n\u0010\n\u02c6\u03bb/\u03bb\n\u0011\n+ \u02c6\nAf\n#\n\u02c6\u03c0\n= inf\nf\u2208F sup\n\u02c6\u03c0\nsup\n\u02c6\u03bb\nZ \"\n\u03b8 Pd\ni=1 bizi\nPd\ni=1 ai\n\u2212\u03bb + \u02c6\u03bb \u2212\u02c6\u03bb log\n\u0010\n\u02c6\u03bb/\u03bb\n\u0011\n+ \u02c6\nAf\n#\n\u02c6\u03c0\n= inf\nf\u2208F\nsup\n(z1,...,zd)\u2208Z\n\u03b8 Pd\ni=1 bizi\nPd\ni=1 ai\n+ \u03bb(ef(z1+a1,...,zd+ad)\u2212f(z1,...,zd) \u22121) \u2212\nd\nX\ni=1\nbizi\n\u2202f\n\u2202zi\n\u2265inf\nu\u2208U\u03b8\nsup\n(z1,...,zd)\u2208Z\n(\nAu\nu +\n\u03b8\nPd\ni=1 ai\nd\nX\ni=1\nbizi\n)\n.\nThat is because optimizing over \u02c6\u03bb, we get \u02c6\u03bb = \u03bbef(z1+a1,...,zd+ad)\u2212f(z1,...,zd) and \ufb01nally\nfor each f \u2208F, u = ef \u2208U\u03b8.\nTheorem 17. Assume limz\u2192\u221e\n\u03bb(z)\nz\n= 0, \u03bb(\u00b7) is continuous and bounded below by\nsome positive constant. Then, ( Nt\nt \u2208\u00b7) satis\ufb01es the large deviation principle with\nthe rate function I(\u00b7) as the Fenchel-Legendre transform of \u0393(\u00b7),\n(4.102)\nI(x) = sup\n\u03b8\u2208R\n{\u03b8x \u2212\u0393(\u03b8)} ,\nwhere\n(4.103)\n\u0393(\u03b8) =\nsup\n(\u02c6\u03bb,\u02c6\u03c0)\u2208Qe\nZ h\n\u03b8\u02c6\u03bb \u2212\u03bb + \u02c6\u03bb \u2212\u02c6\u03bb log\n\u0010\n\u02c6\u03bb/\u03bb\n\u0011i\n\u02c6\u03c0.\nProof. The proof is the same as in the case of exponential h(\u00b7).\n130\n4.4\nLarge Deviations for a Special Class of Non-\nlinear Hawkes Processes: An Approximation\nApproach\nWe already proved in Chapter 3 a large deviation principle of (Nt/t \u2208\u00b7) for\nnonlinear Hawkes process by proving a level-3 large deviation \ufb01rst and then ap-\nplying the contraction principle. In this section, we point out that there is an\nalternative approach, i.e. for general exciting function h(\u00b7), we can use sums of\nexponential functions to approximate h(\u00b7) and use the large deviations for the case\nwhen h(\u00b7) is a sum of exponentials to obtain the large deviations for general h(\u00b7).\nThe advantage of approximating the general case by the case when h is a sum\nof exponentials is that the rate function for the large deviations when h is a sum\nof exponentials can be evaluated by an optimization problem, which should be\ncomputable by some numerical scheme.\nBefore we proceed, let us \ufb01rst prove that h can be approximated by a sum of\nexponentials in both L1 and L\u221enorms.\nLemma 27. If h(t) > 0,\nR \u221e\n0 h(t)dt < \u221e, h(\u221e) = 0, and h is continuous, then h\ncan be approximated by a sum of exponentials both in L1 and L\u221enorms.\nProof. The Stone-Weierstrass theorem says that if X is a compact Hausdor\ufb00space\nand suppose A is a subspace of C(X) with the following properties. (i) If f, g \u2208A,\nthen f \u00d7 g \u2208A.\n(ii) 1 \u2208A.\n(iii) If x, y \u2208X then we can \ufb01nd an f \u2208A\nsuch that f(x) \u0338= f(y). Then A is dense in C(X) in L\u221enorm. Consider X =\nR+ \u222a{\u221e} = [0, \u221e] and C[0, \u221e] consists of continuous functions vanishing at \u221e\nand the constant function 1.\n131\nBy Stone-Weierstrass theorem, the linear combination of 1, e\u2212t, e\u22122t etc. is\ndense in C[0, \u221e]. In other words, for any continuous function h on C[0, \u221e], we\nhave\n(4.104)\nsup\nt\u22650\n\f\f\f\f\fh(t) \u2212\nn\nX\nj=0\naje\u2212jt\n\f\f\f\f\f \u2264\u03f5.\nIn fact, since h(\u221e) = 0, we get |a0| \u2264\u03f5. Thus\n(4.105)\nsup\nt\u22650\n\f\f\f\f\fh(t) \u2212\nn\nX\nj=1\naje\u2212jt\n\f\f\f\f\f \u22642\u03f5.\nHowever, Pn\nj=1 aje\u2212jt may not be positive. We can approximate\np\nh(t) \ufb01rst by\na sum of exponentials and then approximate h(t) by the square of that sum of\nexponentials, which is again a sum of exponentials but positive this time.\nIndeed, we can approximate h(t) by the sum of exponentials in L1 norm as well.\nSuppose \u2225h \u2212hn\u2225L\u221e\u21920, where hn is a sum of exponentials. Then, by dominated\nconvergence theorem, for any \u03b4 > 0,\nR\n|h \u2212hn|e\u2212\u03b4tdt \u21920 as n \u2192\u221e. Thus, we\ncan \ufb01nd a sequence \u03b4n > 0 such that \u03b4n \u21920 as n \u2192\u221eand\nR\n|h \u2212hn|e\u2212\u03b4ntdt \u21920.\nBy dominated convergence theorem again,\nR\nh(1 \u2212e\u2212\u03b4nt)dt \u21920. Hence, we have\nR\n|h \u2212hne\u2212\u03b4nt|dt \u21920 as n \u2192\u221e, where hne\u2212\u03b4nt is a sum of exponentials.\nWe will show that hne\u2212\u03b4nt converges to h in L\u221eas well.\n(4.106)\n\u2225h \u2212hne\u2212\u03b4nt\u2225L\u221e\u2264\u2225h \u2212hn\u2225L\u221e+ \u2225hn \u2212hne\u2212\u03b4nt\u2225L\u221e.\nNotice that (1\u2212e\u2212\u03b4nt)hn \u2264(1\u2212e\u2212\u03b4nt)(h(t)+\u03f5). Since h(\u221e) = 0, there exists some\nM > 0, such that for t > M, h(t) \u2264\u03f5 so that (1 \u2212e\u2212\u03b4nt)hn \u22642\u03f5 for t > M. For\nt \u2264M, (1 \u2212e\u2212\u03b4nt)hn \u2264(1 \u2212e\u2212\u03b4nM)(\u2225h\u2225L\u221e+ \u03f5) which is small if \u03b4n is small.\n132\nWe have the following results.\nTheorem 18. Assume that \u03bb(\u00b7) \u2265c for some c > 0, limz\u2192\u221e\n\u03bb(z)\nz\n= 0 and \u03bb(\u00b7)\u03b1 is\nLipschitz with constant L\u03b1 for any \u03b1 \u22651. We have (Nt/t \u2208\u00b7) satis\ufb01es the large\ndeviation principle with the rate function\n(4.107)\nI(x) = sup\n\u03b8\u2208R\n{\u03b8x \u2212\u0393(\u03b8)}.\nRemark 5. The class of nonlinear Hawkes process with general exciting function\nh for which we proved the large deviation principle here is unfortunately a bit too\nspecial. It works for the rate function like \u03bb(z) = [log(c+z)]\u03b2 for example but does\nnot work for \u03bb(\u00b7) that has sublinear power law growth.\nWe end this chapter with the proof of Theorem 18.\nLet Pn denote the probability measure under which Nt follows the Hawkes\nprocess with exciting function hn = Pn\ni=1 aie\u2212bit such that hn \u2192h as n \u2192\u221ein\nboth L1 and L\u221enorms. We can \ufb01nd such a sequence hn by Lemma 27. Let us\nde\ufb01ne\n(4.108)\n\u0393n(\u03b8) = lim\nt\u2192\u221e\n1\nt log EPn \u0002\ne\u03b8Nt\u0003\n.\nWe need the following lemmas to prove Theorem 18.\nLemma 28. For any K > 0 and \u03b81, \u03b82 \u2208[\u2212K, K], there exists some constant\nC(K) such that for any n,\n(4.109)\n|\u0393n(\u03b81) \u2212\u0393n(\u03b82)| \u2264C(K)|\u03b81 \u2212\u03b82|.\n133\nProof. Without loss of generality, take \u03b82 > \u03b81. Then\n\u0393n(\u03b81) \u2264\u0393n(\u03b82)\n(4.110)\n=\nsup\n(\u02c6\u03bb,\u02c6\u03c0)\u2208Q\u2217e\nZ\n(\u03b82 \u2212\u03b81)\u02c6\u03bb\u02c6\u03c0 + \u03b81\u02c6\u03bb\u02c6\u03c0 \u2212\u02c6H(\u02c6\u03bb, \u02c6\u03c0)\n\u2264\nsup\n(\u02c6\u03bb,\u02c6\u03c0)\u2208Q\u2217e\nZ\n(\u03b82 \u2212\u03b81)\u02c6\u03bb\u02c6\u03c0 + \u0393n(\u03b81),\nwhere\n(4.111)\nQ\u2217\ne =\n\u001a\n(\u02c6\u03bb, \u02c6\u03c0) \u2208Qe :\nZ\n\u03b81\u02c6\u03bb\u02c6\u03c0 \u2212\u02c6H(\u02c6\u03bb, \u02c6\u03c0) \u2265\u0393n(\u03b81) \u22121\n\u001b\n.\nThe key is to prove that sup(\u02c6\u03bb,\u02c6\u03c0)\u2208Q\u2217e\nR \u02c6\u03bb\u02c6\u03c0 \u2264C(K) for some positive constant C(K)\ndepending only on K. De\ufb01ne u = u(z1, . . . , zn) = e\nPn\ni=1 cizi where\n(4.112)\nci =\n3K\nPn\ni=1\nai\nbi\n\u00b7 1\nbi\n,\n1 \u2264i \u2264n.\nDe\ufb01ne V = \u2212Au\nu such that\n(4.113)\nV (z1, . . . , zn) =\n3K\nPn\ni=1\nai\nbi\nn\nX\ni=1\nzi \u2212\u03bb(z1 + \u00b7 \u00b7 \u00b7 + zn)(e3K \u22121).\nNotice that\nR \u02c6\nAf \u02c6\u03c0 = 0 for any test function f with certain regularities. If we try\nf = zi\nbi , 1 \u2264i \u2264n, we get\n(4.114)\n\u2212\nZ\nzi\u02c6\u03c0 + ai\nbi\nZ\n\u02c6\u03bb\u02c6\u03c0 = 0,\n1 \u2264i \u2264n.\n134\nSumming over 1 \u2264i \u2264n, we get\n(4.115)\nZ\n\u02c6\u03bb\u02c6\u03c0 =\n1\nPn\ni=1\nai\nbi\nZ\nn\nX\ni=1\nzi\u02c6\u03c0.\nNotice that Pn\ni=1\nai\nbi = \u2225hn\u2225L1 which is approximately \u2225h\u2225L1 when n is large. Since\nlim supz\u2192\u221e\n\u03bb(z)\nz\n= 0 and Pn\ni=1 zi \u22650, we have\n(4.116)\n\u03b81\nZ\n\u02c6\u03bb\u02c6\u03c0 \u2264K\nZ\n\u02c6\u03bb\u02c6\u03c0 =\nK\nPn\ni=1\nai\nbi\nZ\nn\nX\ni=1\nzi\u02c6\u03c0 \u22641\n2\nZ\nV \u02c6\u03c0 + C1/2(K),\nwhere C1/2(K) is some positive constant depending only on K.\nWe claim that\nR\nV (z)\u02c6\u03c0 \u2264\u02c6H(\u02c6\u03c0) for any \u02c6\u03c0 \u2208Q\u2217\ne. Let us prove it. By the ergodic\ntheorem and Jensen\u2019s inequality,\n(4.117)\nZ\nV (z)\u02c6\u03c0 = lim\nt\u2192\u221eE\u02c6\u03c0\n\u00141\nt\nZ t\n0\nV (Zs)ds\n\u0015\n\u2264lim sup\nt\u2192\u221e\n1\nt log E\u03c0 h\ne\nR t\n0 V (Zs)dsi\n+ \u02c6H(\u02c6\u03c0).\nNext, we will show that u \u22651. That is equivalent to proving Pn\ni=1\nzi\nbi \u22650. Consider\nthe process\n(4.118)\nYt =\nn\nX\ni=1\nZi(t)\nbi\n=\nX\n\u03c4j<t\nn\nX\ni=1\nai\nbi\ne\u2212bi(t\u2212\u03c4j) =\nX\n\u03c4j<t\ng(t \u2212\u03c4j),\nwhere g(t) = Pn\ni=1\nai\nbi e\u2212bit. Notice that g(t) =\nR \u221e\nt\nh(s)ds > 0. Therefore, Yt \u22650\nalmost surely and Pn\ni=1\nZi(t)\nbi\n\u22650. Since Au\nu + V = 0 and u \u22651, by Feynman-Kac\n135\nformula and Dynkin\u2019s formula,\nE\u03c0 h\ne\nR t\n0 V (Zs)dsi\n\u2264E\u03c0 h\nu(Zt)e\nR t\n0 V (Zs)dsi\n(4.119)\n= u(Z0) +\nZ t\n0\nE\u03c0 h\n(Au(Zs) + V (Zs)u(Zs))e\nR s\n0 V (Zu)dui\nds\n= u(Z0),\nand therefore\nR\nV (z)\u02c6\u03c0 \u2264\u02c6H(\u02c6\u03c0) for any \u02c6\u03c0 \u2208Q\u2217\ne. Hence,\n(4.120)\n\u03b81\nZ\n\u02c6\u03bb\u02c6\u03c0 \u22641\n2\nZ\nV (z) + C1/2(K) \u22641\n2\n\u02c6H + C1/2(K).\nNotice that\n(4.121)\n\u2212\u221e< \u0393n(\u03b81) \u22121 \u2264\u03b81\nZ\n\u02c6\u03bb\u02c6\u03c0 \u2212\u02c6H \u2264\u0393n(\u03b81) < \u221e.\nHence,\n(4.122)\n\u0393n(\u03b81) \u22121 + 1\n2\n\u02c6H \u2264\u03b81\nZ\n\u02c6\u03bb\u02c6\u03c0 \u22121\n2\n\u02c6H \u2264C1/2(K),\nwhich implies \u02c6H \u22642(C1/2(K) \u2212\u0393n(\u03b81) + 1) and so also,\n(4.123)\nZ\n\u02c6\u03bb\u02c6\u03c0 \u2264\n1\n2K\nZ\nV \u02c6\u03c0+ 1\nK C1/2(K) \u22641\nK (C1/2(K)\u2212\u0393n(\u03b81)+1)+ 1\nK C1/2(K).\nFinally, notice that since hn \u2192h in both L1 and L\u221enorms, we can \ufb01nd a function\ng such that supn hn \u2264g and \u2225g\u2225L1 < \u221e. and thus\n(4.124)\n\u0393n(\u03b81) \u2265\u0393n(\u2212K) \u2265\u0393g(\u2212K),\n136\nwhere \u0393g denotes the case when the rate function is still \u03bb(\u00b7) but the exciting\nfunction is g(\u00b7) instead of hn(\u00b7). Notice that here \u2225g\u2225L1 < \u221ebut may not be less\nthan 1. It is still well de\ufb01ned because of the assumption limz\u2192\u221e\n\u03bb(z)\nz\n= 0. Indeed,\nwe can \ufb01nd \u03bb(z) = \u03bd\u03f5 + \u03f5z that dominates the original \u03bb(\u00b7) for \u03bd\u03f5 > 0 big enough\nand \u03f5 > 0 small enough so that \u03f5\u2225g\u2225L1 < 1. Now, we have \u0393g(\u2212K) \u2265\u0393\u03bd\u03f5\n\u03f5g(\u2212K)\nwhich is \ufb01nite, where \u0393\u03bd\u03f5\n\u03f5g(\u2212K) corresponds to the case when \u03bb(z) = \u03bd\u03f5+\u03f5z. Hence,\n(4.125)\nsup\n(\u02c6\u03bb,\u02c6\u03c0)\u2208Q\u2217e\nZ\n\u02c6\u03bb\u02c6\u03c0 \u2264C(K),\nfor some C(K) > 0 depending only on K.\nLemma 29. Assume that \u03bb(\u00b7) \u2265c for some c > 0, limz\u2192\u221e\n\u03bb(z)\nz\n= 0 and \u03bb(\u00b7)\u03b1 is\nLipschitz with constant L\u03b1 for any \u03b1 \u22651. Then for any K > 0, \u0393n(\u03b8) is Cauchy\nwith \u03b8 uniformly in [\u2212K, K].\nProof. Let us write Hn(t) = P\n\u03c4j<t hn(t \u2212\u03c4j). Observe \ufb01rst, that for any q,\n(4.126)\nexp\n\u001a\nq\nZ t\n0\nlog\n\u0012\u03bb(Hm(s))\n\u03bb(Hn(s))\n\u0013\ndNs \u2212\nZ t\n0\n\u0012 \u03bb(Hm(s))q\n\u03bb(Hn(s))q\u22121 \u2212\u03bb(Hn(s))\n\u0013\nds\n\u001b\nis a martingale under Pn. By H\u00a8older\u2019s inequality, for any p, q > 1 with 1\np + 1\nq = 1,\nEPm[e\u03b8Nt] = EPn\n\u0014\ne\u03b8Nt dPm\ndPn\n\u0015\n(4.127)\n= EPn h\ne\u03b8Nt\u2212\nR t\n0 (\u03bb(Hm(s))\u2212\u03bb(Hn(s)))ds\u2212\nR t\n0 log(\n\u03bb(Hn(s))\n\u03bb(Hm(s)))dNsi\n\u2264EPn h\nep\u03b8Nt\u2212p\nR t\n0 (\u03bb(Hm(s))\u2212\u03bb(Hn(s)))dsi1/p\nEPn h\neq\nR t\n0 log(\n\u03bb(Hm(s))\n\u03bb(Hn(s)) )dNsi1/q\n.\n137\nBy the Cauchy-Schwarz inequality,\nEPn h\neq\nR t\n0 log(\n\u03bb(Hm(s))\n\u03bb(Hn(s)) )dNsi1/q\n\u2264EPn\n\"\ne\nR t\n0\n\u0012\n\u03bb(Hm(s))2q\n\u03bb(Hn(s))2q\u22121 \u2212\u03bb(Hn(s))\n\u0013\nds\n# 1\n2q\n(4.128)\n\u2264EPn h\ne\n1\nc2q\u22121 L2q\nR t\n0\nP\n\u03c4<s |hm(s\u2212\u03c4)\u2212hn(s\u2212\u03c4)|dsi 1\n2q\n\u2264EPn h\ne\n1\nc2q\u22121 L2q\u2225hm\u2212hn\u2225L1Nti 1\n2q .\nWe also have\n(4.129)\nEPn h\nep\u03b8Nt\u2212p\nR t\n0 (\u03bb(Hm(s))\u2212\u03bb(Hn(s)))dsi1/p\n\u2264EPn \u0002\nep\u03b8Nt+pL1\u2225hm\u2212hn\u2225L1Nt\u00031/p .\nTherefore, by Lemma 28 and the fact \u0393n(0) = 0 for any n, we have\n\u0393m(\u03b8) \u2212\u0393n(\u03b8)\n(4.130)\n\u22641\np\u0393n (p\u03b8 + pL1\u03f5m,n) + 1\n2q\u0393n\n\u0012L2q\u03f5m,n\nc2q\u22121\n\u0013\n\u2212\u0393n(\u03b8)\n\u2264C(K)L1\u03f5m,n + C(K)\n2q\n\u00b7 L2q\u03f5m,n\nc2q\u22121\n+ 1\np\u0393n(p\u03b8) \u22121\np\u0393n(\u03b8) +\n\u0012\n1 \u22121\np\n\u0013\n|\u0393n(\u03b8)|,\n\u2264C(K)L1\u03f5m,n + C(K)\n2q\n\u00b7 L2q\u03f5m,n\nc2q\u22121\n+ C(K)(p \u22121)K\np\n+\n\u0012\n1 \u22121\np\n\u0013\nC(K)K,\nwhere \u03f5m,n = \u2225hm \u2212hn\u2225L1. Hence,\n(4.131)\nlim sup\nm,n\u2192\u221e{\u0393m(\u03b8) \u2212\u0393n(\u03b8)} \u22642\n\u0012\n1 \u22121\np\n\u0013\nC(K)K,\nwhich is true for any p > 1. Letting p \u21931, we get the desired result.\nRemark 6. If \u03bb(\u00b7) \u2265c > 0 and limz\u2192\u221e\n\u03bb(z)\nz\u03b1\n= 0 for any \u03b1 > 0, then, \u03bb(\u00b7)\u03c3 is\n138\nLipschitz for any \u03c3 \u22651. For instance, \u03bb(z) = [log(z + c)]\u03b2 satis\ufb01es the conditions\nif \u03b2 > 0 and c > 1.\nTheorem 19. Assume that \u03bb(\u00b7) \u2265c for some c > 0, limz\u2192\u221e\n\u03bb(z)\nz\n= 0 and \u03bb(\u00b7)\u03b1 is\nLipschitz with constant L\u03b1 for any \u03b1 \u22651.\n(4.132)\nlim\nt\u2192\u221e\n1\nt log E[e\u03b8Nt] = \u0393(\u03b8) = lim\nn\u2192\u221e\u0393n(\u03b8),\nfor any \u03b8 \u2208R.\nProof. By Lemma 29, \u0393n(\u03b8) tends to \u0393(\u03b8) uniformly on any compact set [\u2212K, K].\nSince \u0393n(\u03b8) is Lipschitz by Lemma 28, it is continuous and the limit \u0393 is also\ncontinuous. Let \u03f5n = \u2225hn \u2212h\u2225L1 \u2264\u03f5. As in the proof of Lemma 29, for any\n\u03b8 \u2208[\u2212K, K], p, q > 1, 1\np + 1\nq = 1, we get\nlim sup\nt\u2192\u221e\n1\nt log E[e\u03b8Nt]\n(4.133)\n\u2264\u0393n(\u03b8) + C(K)L1\u03f5n + C(K)\n2q\n\u00b7 L2q\u03f5n\nc2q\u22121 + 2\n\u0012\n1 \u22121\np\n\u0013\nC(K)K.\nLetting n \u2192\u221e\ufb01rst and then p \u21931, we get lim supt\u2192\u221e\n1\nt log E[e\u03b8Nt] \u2264\u0393(\u03b8).\nSimilarly, for any p\u2032, q\u2032 > 1 with\n1\np\u2032 + 1\nq\u2032 = 1,\n\u0393n(\u03b8) \u2264lim inf\nt\u2192\u221e\n1\npt log E[e(p\u03b8+pL1\u03f5n)Nt] + lim inf\nt\u2192\u221e\n1\n2qt log E\n\u0014\ne\nL2q\u03f5n\nc2q\u22121 Nt\n\u0015\n(4.134)\n\u2264lim inf\nt\u2192\u221e\n1\npp\u2032t log E[epp\u2032\u03b8Nt] + lim inf\nt\u2192\u221e\n1\npq\u2032t log E[eq\u2032pL1\u03f5nNt]\n+ lim inf\nt\u2192\u221e\n1\n2qt log E\n\u0014\ne\nL2q\u03f5n\nc2q\u22121 Nt\n\u0015\n.\nSince we can dominate \u03bb(\u00b7) by the linear function \u03bb(z) = \u03bd + z in which case the\nlimit of logarithmic moment generating function \u0393\u03bd(\u03b8) is continuous in \u03b8, we may\n139\nlet n \u2192\u221eto obtain\n(4.135)\n\u0393(\u03b8) \u2264lim inf\nt\u2192\u221e\n1\npp\u2032t log E[epp\u2032\u03b8Nt].\nThis holds for any \u03b8 and thus\n(4.136)\nlim inf\nt\u2192\u221e\n1\nt log E[e\u03b8Nt] \u2265pp\u2032\u0393\n\u0012 \u03b8\npp\u2032\n\u0013\n.\nLetting p, p\u2032 \u21931 and using the continuity of \u0393(\u00b7), we get the desired result.\nFinally, let us prove Theorem 18.\nProof of Theorem 18. For the upper bound, apply the G\u00a8artner-Ellis Theorem. Let\nus prove the lower bound. Let B\u03f5(x) denote the open ball centered at x with radius\n\u03f5 > 0. By H\u00a8older\u2019s inequality, for any p, q > 1 with 1\np + 1\nq = 1,\n(4.137)\nPn\n\u0012Nt\nt \u2208B\u03f5(x)\n\u0013\n\u2264\n\r\r\r\r\ndPn\ndP\n\r\r\r\r\nLp(P)\nP\n\u0012Nt\nt \u2208B\u03f5(x)\n\u00131/q\n.\nTherefore, letting t \u2192\u221e, we have\nsup\n\u03b8\u2208R\n{\u03b8x \u2212\u0393n(\u03b8)} = lim\nt\u2192\u221e\n1\nt log Pn\n\u0012Nt\nt \u2208B\u03f5(x)\n\u0013\n(4.138)\n\u22641\npp\u2032\u0393(pp\u2032L1\u03f5n) +\n1\n2pq\u2032\u0393\n\u0012L2pq\u2032\u03f5n\nc2pq\u2032\u22121\n\u0013\n+ 1\nq lim inf\nt\u2192\u221e\n1\nt log P\n\u0012Nt\nt \u2208B\u03f5(x)\n\u0013\n,\nwhere \u03f5n = \u2225hn \u2212h\u2225L1. Hence, letting n \u2192\u221e, see that\n(4.139)\n1\nq lim inf\nt\u2192\u221e\n1\nt log P\n\u0012Nt\nt \u2208B\u03f5(x)\n\u0013\n\u2265lim sup\nn\u2192\u221esup\n\u03b8\u2208R\n{\u03b8x \u2212\u0393n(\u03b8)}.\n140\nSince \u0393n(\u03b8) \u2192\u0393(\u03b8) uniformly on any compact set K,\n(4.140)\nsup\n\u03b8\u2208K\n{\u03b8x \u2212\u0393n(\u03b8)} \u2192sup\n\u03b8\u2208K\n{\u03b8x \u2212\u0393(\u03b8)},\nas n \u2192\u221efor any such set K. Notice that \u03bb(\u00b7) \u2265c > 0 and recall that the limit\nfor the logarithmic moment generating function with parameter \u03b8 for a Poisson\nprocess with constant rate c is (e\u03b8 \u22121)c. Hence\n(4.141)\nlim inf\n\u03b8\u2192+\u221e\n\u0393n(\u03b8)\n\u03b8\n\u2265lim inf\n\u03b8\u2192+\u221e\n(e\u03b8 \u22121)c\n\u03b8\n= +\u221e,\nwhich implies that sup\u03b8\u2208R{\u03b8x \u2212\u0393n(\u03b8)} \u2192sup\u03b8\u2208R{\u03b8x \u2212\u0393(\u03b8)}. Therefore,\n(4.142)\n1\nq lim inf\nt\u2192\u221e\n1\nt log P\n\u0012Nt\nt \u2208B\u03f5(x)\n\u0013\n\u2265sup\n\u03b8\u2208R\n{\u03b8x \u2212\u0393(\u03b8)}.\nLetting q \u21931, we get the desired result.\n141\nChapter 5\nAsymptotics for Nonlinear\nHawkes Processes\nIn the existing literature of on nonlinear Hawkes processes, the usual assump-\ntion is that \u03bb(\u00b7) is \u03b1-Lipschitz, h(\u00b7) is integrable and \u03b1\u2225h\u2225L1 < 1. But how about\nother regimes? How do the asymptotics vary in di\ufb00erent regimes? This is the\nquestion we would try to answer in this chapter.\nWe divide the nonlinear Hawkes process into the following regimes.\n1. limz\u2192\u221e\n\u03bb(z)\nz\n= 0. This is the sublinear regime. In this regime, if we assume\nthat \u03bb(\u00b7) is \u03b1-Lipschitz, \u2225h\u2225L1 < \u221eand \u03b1\u2225h\u2225L1 < 1, then there exists a\nunique stationary version of the nonlinear Hawkes process. The central limit\ntheorem and large deviations for this regime are proved in Zhu [114], [112]\nand [113]. On the contrary, if we assume that \u2225h\u2225L1 = \u221e, then, there is no\nstationary version. Figure 5.1 illustrates \u03bbt in this case. We will obtain the\ntime asymptotics for \u03bbt in Section 5.1.\n2. limz\u2192\u221e\n\u03bb(z)\nz\n= 1 and \u2225h\u2225L1 < 1. This is the sub-critical regime. In this\n142\nregime, if we assume that \u03bb(\u00b7) is \u03b1-Lipschitz and \u03b1\u2225h\u2225L1 < 1, then there ex-\nists a unique stationary version of the nonlinear Hawkes process, see Br\u00b4emaud\nand Massouli\u00b4e [14]. The central limit theorem is proved in Zhu [114]. Figure\n5.3 illustrates \u03bbt in this case. We will summarize some known results about\nthe limit theorems in Section 5.2.\n3. limz\u2192\u221e\n\u03bb(z)\nz\n= 1 and \u2225h\u2225L1 = 1. This is the critical regime. This regime is\nvery subtle. We will show in Section 5.3 that in some cases, there exists a\nstationary version of the Hawkes process. In some other cases, it does not\nexist. In particular, when \u03bb(z) = \u03bd + z and\nR \u221e\n0 th(t)dt < \u221e, we will prove\nthat NtT\nT 2 \u2192\nR t\n0 \u03b7sds, where \u03b7s is a squared Bessel process. N[T, T + t\nT ] will\nconverge to a P\u00b4olya process as T \u2192\u221e. Figure 5.4 illustrates the behavior\nof \u03bbt in this case. When h(\u00b7) has heavy tails, i.e.\nR \u221e\n0 th(t)dt = \u221e, we will\nprove that the time asymptotic behavior is di\ufb00erent from the light tail case.\n4. limz\u2192\u221e\n\u03bb(z)\nz\n= 1 and \u2225h\u2225L1 > 1. This is the super-critical regime. We will\nprove in Section 5.4 that \u03bbt grows exponentially in t in this regime, which is\nconsistent with what we can see in Figure 5.5.\n5. P\u221e\nn=0\n1\n\u03bb(n) < \u221e. This is the explosive regime. In Section 5.5, we will \ufb01rst\nprovide a criterion for the explosion and non-explosion for nonlinear Hawkes\nprocess. Then, we will study the asymptotic behavior of the explosion time.\nFigure 5.6 illustrates the explosion of a \ufb01nite time.\nNotice that if \u2225h\u2225L1 = \u221eand limz\u2192\u221e\n\u03bb(z)\nz\n= \u03b1 > 0, then one is in the super-\ncritical regime and we will see that \u03bbt grows exponentially; this is discussed Section\n5.4. If \u2225h\u2225L1 = \u221eand P\u221e\nn=0\n1\n\u03bb(n) < \u221e, then one is in the explosive regime to be\ndiscussed in Section 5.5.\n143\nWe will launch a systematic study of the time asymptoics for Hawkes process in\ndi\ufb00erent regimes. We will study the sublinear regime, sub-critical regime, critical\nregime and super-critical regime in Sections 5.1, 5.2, 5.3, 5.4 respectively. Finally,\nin Section 5.5, we will provide a criterion for explosion and non-explosion for\nHawkes process and obtain some asymptotics for the explosion time.\n0\n5\n10\n15\n20\n25\n30\n0\n2\n4\n6\n8\n10\nFigure 5.1: Plot of intensity \u03bbt for a realization of Hawkes process. Here h(t) =\n(t + 1)\u22121\n2 and \u03bb(z) = (1 + z)\n1\n2.\n0\n5\n10\n15\n20\n25\n30\n0\n2\n4\n6\n8\n10\nFigure 5.2: Plot of intensity \u03bbt for a realization of Hawkes process. Here h(t) =\n4\n(t+1)3 and \u03bb(z) = (1 + z)\n1\n2. In this case, \u2225h\u2225L1 < \u221eand \u03bb(\u00b7) is sublinear and\nLipschitz. It will converge to the unique stationary version of the Hawkes process.\n144\n0\n5\n10\n15\n20\n25\n30\n0\n2\n4\n6\n8\n10\nFigure 5.3: Plot of intensity \u03bbt for a realization of Hawkes process. Here h(t) =\n1\n(t+1)3 and \u03bb(z) = 1+z. In this case, \u2225h\u2225L1 = 1\n2 < 1. It is in the sub-critical regime.\nThis is a classical Hawkes process and it will converge to the unique stationary\nversion of the Hawkes process.\n5.1\nSublinear Regime\nIn this section, we are interested in the sublinear case limz\u2192\u221e\n\u03bb(z)\nz\n= 0.\nIf\n\u2225h\u2225L1 < \u221eand \u03bb(\u00b7) is \u03b1-Lipschitz and \u03b1\u2225h\u2225L1 < 1, then, as Br\u00b4emaud and Mas-\nsouli\u00b4e [14] proved, there exists a unique stationary Hawkes process.\nRecently,\nKarabash [63] relaxed the Lipschitz condition and proved the stability result for a\nwider class of \u03bb(\u00b7). Let P and E denote the probability measure and expectation\nfor stationary Hawkes process. Then, by ergodic theorem, we have the law of large\nnumbers,\n(5.1)\nNt\nt \u2192\u00b5 = E[N[0, 1]],\nas t \u2192\u221e.\nThe central limit theorem and large deviations have already been discussed in\nChapter 2, Chapter 3 and Chapter 4.\nIf \u2225h\u2225L1 = \u221e, then there is no stationary version of Hawkes process and \u03bbt\ntends to \u221eas t \u2192\u221e. This is the case we are going to study for the rest of\n145\n0\n5\n10\n15\n20\n25\n30\n0\n10\n20\n30\n40\n50\nFigure 5.4: Plot of intensity \u03bbt for a realization of Hawkes process. Here h(t) =\n2\n(t+1)3 and \u03bb(z) = 1+z. In this case, \u2225h\u2225L1 = 1,\nR \u221e\n0 th(t)dt < \u221eand \u03bb(\u00b7) is linear.\nIt is therefore in the critical regime. From the graph, we can see that \u03bbt grows\nlinearly in t, which will be proved in this chapter. Indeed, we will prove that N\u00b7T\nT 2\nconverges to\nR \u00b7\n0 \u03b7sds as T \u2192\u221e, where \u03b7s is a squared Bessel process.\nthe subsection. We are interested the time asymptotic behavior of the nonlinear\nHawkes process in this regime.\nLet us \ufb01rst make a simple observation. Assume that \u03bb(z) \u2191\u221eas z \u2192\u221e.\nThen, assuming \u2225h\u2225L1 = \u221e, we have \u03bbt \u2192\u221eas t \u2192\u221ea.s. This can be seen by\nnoticing that\nR t\n0 h(t \u2212s)N(ds) \u2192\u221ea.s. if \u2225h\u2225L1 = \u221e, where Nt follows from a\nstandard Poisson process with constant rate \u03bb(0).\nLet us prove a special case \ufb01rst.\nProposition 1. Assume that h(\u00b7) \u22611 and \u03bb(z) = \u03b3(\u03bd + z)\u03b2, where \u03b3, \u03bd > 0 and\n0 < \u03b2 < 1. Then,\n(5.2)\n\u03bbt\nt\n\u03b2\n1\u2212\u03b2\n\u2192\u03b3\n1\n1\u2212\u03b2 (1 \u2212\u03b2)\n\u03b2\n1\u2212\u03b2 ,\nin probability as t \u2192\u221e.\n146\n0\n1\n2\n3\n4\n5\n6\n7\n0\n20\n40\n60\n80\n100\nFigure 5.5: Plot of intensity \u03bbt for a realization of Hawkes process. Here h(t) =\n3\n(t+1)3 and \u03bb(z) = 1 + z. In this case, \u2225h\u2225L1 = 3\n2 > 1 and it is in the super-critical\nregime. We expect that \u03bbt would grow exponentially in this case.\nProof. For \u03b1 > 0,\nd\u03bb\n1\n\u03b1\nt =\nh\n\u03bb(\u03bd + Nt + 1)\n1\n\u03b1 \u2212\u03bb(\u03bd + Nt)\n1\n\u03b1\ni\ndNt\n(5.3)\n=\nh\n\u03b3\n1\n\u03b1(\u03bd + Nt + 1)\n\u03b2\n\u03b1 \u2212\u03b3\n1\n\u03b1(\u03bd + Nt)\n\u03b2\n\u03b1\ni\ndNt\n=\n\"\u0012\n\u03bb\n1\n\u03b2\nt + \u03b3\n1\n\u03b2\n\u0013 \u03b2\n\u03b1\n\u2212\u03bb\n1\n\u03b1\nt\n#\ndNt.\nLet \u03b1 =\n\u03b2\n1\u2212\u03b2. We have\n(5.4)\n\u03bb\n1\u2212\u03b2\n\u03b2\nt\n=\nZ t\n0\n\u0014\n(\u03bb\n1\n\u03b2\ns + \u03b3\n1\n\u03b2 )1\u2212\u03b2 \u2212(\u03bb\n1\n\u03b2\ns )1\u2212\u03b2\n\u0015\n\u03bbsds +\nZ t\n0\n\u0014\n(\u03bb\n1\n\u03b2\ns + \u03b3\n1\n\u03b2 )1\u2212\u03b2 \u2212(\u03bb\n1\n\u03b2\ns )1\u2212\u03b2\n\u0015\ndMs.\nSince \u03bbt \u2192\u221ea.s. as t \u2192\u221e, by the bounded convergence theorem,\n(5.5)\n1\nt\nZ t\n0\nE\n\u001a\u0014\n(\u03bb\n1\n\u03b2\ns + \u03b3\n1\n\u03b2 )1\u2212\u03b2 \u2212(\u03bb\n1\n\u03b2\ns )1\u2212\u03b2\n\u0015\n\u03bbs\n\u001b\nds \u2192(1 \u2212\u03b2)\u03b3\n1\n\u03b2 ,\nas t \u2192\u221e. It is not di\ufb03cult to see that 1\nt\nR t\n0[(\u03bb\n1\n\u03b2\ns + \u03b3\n1\n\u03b2 )1\u2212\u03b2 \u2212(\u03bb\n1\n\u03b2\ns )1\u2212\u03b2]dMs \u21920 in\nprobability as t \u2192\u221e. Hence, \u03bb\n1\u2212\u03b2\n\u03b2\nt\nt\n\u2192(1 \u2212\u03b2)\u03b3\n1\n\u03b2 in probability as t \u2192\u221e.\n147\n0\n1\n2\n3\n4\n5\n6\n7\n0\n20\n40\n60\n80\n100\nFigure 5.6: Plot of intensity \u03bbt for a realization of Hawkes process. Here h(t) =\n1\n(t+1)3 and \u03bb(z) = (1 + z)\n3\n2. This is in the explosive regime. The plot is a little bit\ncheating because it is impossible to \u201cplot\u201d explosion. Nevertheless, you can think\nit as an illustration. It \u201cappears\u201d that the process explodes near time t = 6.\nRemark 7. Assume that h(t) = (t + 1)\u03b4, \u03b4 > \u22121 and \u03bb(z) = \u03b3(\u03bd + z)\u03b2, where\n\u03b3, \u03bd > 0 and 0 < \u03b2 < 1. We conjecture that\n(5.6)\n\u03bbt\nt\u03b1 \u2192\u03b3\n1\n1\u2212\u03b2 B(\u03b4, \u03b1)\n\u03b2\n1\u2212\u03b2 ,\nas t \u2192\u221ea.s., where \u03b1 = (1+\u03b4)\u03b2\n1\u2212\u03b2\nand B(\u03b4, \u03b1) =\nR 1\n0 u\u03b4(1 \u2212u)\u03b1du.\n5.2\nSub-Critical Regime\nIn this section, we review some known results about the limit theorems in the\nsub-critical regime.\nWe say the Hawkes process is in the sub-critical regime if\nlimz\u2192\u221e\n\u03bb(z)\nz\n= 1 and \u2225h\u2225L1 < 1. If we further assume that \u03bb(\u00b7) is \u03b1-Lipschitz and\n\u03b1\u2225h\u2225L1 < 1, then Br\u00b4emaud and Massouli\u00b4e [14] proved that there exists a unique\nstationary Hawkes process. In this regime, we also have the law of large numbers\nand the central limit theorem just as in Section 5.1. For the case when \u03bb(\u00b7) is\nnonlinear, we refer to the review in Section 5.1 for the law of large numbers and\n148\ncentral limit theorem.\nIn particular, when \u03bb(z) = \u03bd +z and \u03bd > 0, we have explict expressions for the\nlaw of large numbers, central limit theorem and large deviation principle. They\nare well known in the literature.\nThe ergodic theorem implies the following law of large numbers,\n(5.7)\nNt\nt \u2192\n\u03bd\n1 \u2212\u2225h\u2225L1 ,\nas t \u2192\u221ea.s.\nBordenave and Torrisi [11] proved a large deviation principle for (Nt\nt \u2208\u00b7) with the\nrate function\n(5.8)\nI(x) =\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f3\nx log\n\u0010\nx\n\u03bd+x\u2225h\u2225L1\n\u0011\n\u2212x + x\u2225h\u2225L1 + \u03bd\nif x \u2208[0, \u221e)\n+\u221e\notherwise\n.\nBacry et al. [2] proved a functional central limit theorem, stating that\n(5.9)\nN\u00b7t \u2212\u00b7\u00b5t\n\u221a\nt\n\u2192\u03c3B(\u00b7),\nas t \u2192\u221e,\non D[0, 1] with Skorokhod topology, where\n(5.10)\n\u00b5 =\n\u03bd\n1 \u2212\u2225h\u2225L1\nand\n\u03c32 =\n\u03bd\n(1 \u2212\u2225h\u2225L1)3.\nWhen \u03bb(\u00b7) is nonlinear and sub-critical, the central limit theorem has been\nobtained in Chapter 2.\n149\n5.3\nCritical Regime\nIn this section, we are interested in the critical regime, i.e. limz\u2192\u221e\n\u03bb(z)\nz\n= 1\nand \u2225h\u2225L1 = 1. This regime is very subtle. In some cases, there exists a stationary\nversion of Hawkes process whilst in some cases there does not.\nFor example,\nBr\u00b4emaud and Massouli\u00b4e [15] proved that\nProposition 2 (Br\u00b4emaud and Massouli\u00b4e). Assume \u03bb(z) = z, \u2225h\u2225L1 = 1 and\n(5.11)\nsup\nt\u22650\nt1+\u03b1h(t) \u2264R,\nlim\nt\u2192\u221et1+\u03b1h(t) = r,\nfor some \ufb01nite constants r, R > 0 and 0 < \u03b1 < 1\n2. Then, there exists a non-trivial\nstationary Hawkes process with \ufb01nite intensity.\nBr\u00b4emaud and Massouli\u00b4e considered only the linear Hawkes process in their\npaper [15]. If you allow nonlinear rate function, you get a much richer class of\nHawkes processes and in some cases, there still exists a stationary Hawkes process.\nIt is much easier to work with the exponential case, i.e. when h(t) = ae\u2212at and\n\u2225h\u2225L1 = 1.\nThe lecture notes by Hairer [47] provides a su\ufb03cient condition for which there\nexists an invariant probability measure.\nLet L be the generator of a Markov\nprocess. If there exists V \u22651, continuous, with precompact sublevel sets and\nsome function \u03c6 : R+ \u2192R+ strictly concave, increasing, with \u03c6(0) = 0, and\n\u03c6(x) \u2192\u221eas x \u2192\u221eand LV \u2264K \u2212\u03c6(V ) for some K > 0, then there exists an\ninvariant probability measure.\nProposition 3. Assume h(t) = ae\u2212at, a > 0 and \u03bb(z) = z \u2212\u03c8(z) + \u03bd, where \u03c8(z)\nis positive, increasing, strictly concave and \u03c8(z) \u2192\u221eand \u03c8(z)\nz\n\u21920 as z \u2192\u221e. If\n150\nalso \u03bb(z) is strictly positive. Then there exists an invarint probability measure.\nProof. Let V (z) = z + 1 and \u03c6(V ) = a(\u03c8(V ) \u2212\u03c8(0)).\nThen \u03c6 : R+ \u2192R+\nis increasing and strictly concave, \u03c6(z) \u2192\u221e, and \u03c6(0) = 0.\nRecall that the\ngenerator is given by\n(5.12)\nAf(z) = \u2212az\u2202f\n\u2202z + \u03bb(z)[f(z + a) \u2212f(z)].\nHence, we have\n(5.13) AV + \u03c6(V ) = \u2212\u03c8(z)a + a\u03c8(z + 1) \u2212a\u03c8(0) + a\u03bd \u2264a\u03c8(1) \u22122a\u03c8(0) + a\u03bd.\nWe can generalize our result to the much wider class of h(\u00b7) when h(\u00b7) is a\nsum of exponentials: h(t) = Pd\ni=1 aie\u2212bit, where bi > 0 and ai > 0, 1 \u2264i \u2264d.\nWrite Zi(t) = P\n\u03c4<t aie\u2212bi(t\u2212\u03c4). Then Zt = Pd\ni=1 Zi(t) and (Z1(t), . . . , Zd(t)) is\nMarkovian with the generator\n(5.14) Af = \u2212\nd\nX\ni=1\nbizi\n\u2202f\n\u2202zi\n+\u03bb\n \nd\nX\ni=1\nzi\n!\n\u00b7[f(z1 + a1, . . . , zd + ad) \u2212f(z1, . . . , zd)] .\nWe have the following result.\nProposition 4. Assume h(t) = Pd\ni=1 aie\u2212bit, bi > 0 and ai > 0, 1 \u2264i \u2264d and\n\u2225h\u2225L1 = Pd\ni=1\nai\nbi = 1. Also assume that \u03bb(z) = z\u2212\u03c8(z)+\u03bd, where \u03c8(z) is positive,\nincreasing, strictly concave and \u03c8(z) \u2192\u221eand \u03c8(z)\nz\n\u21920 as z \u2192\u221eand \u03bb(z) is\nstrictly positive. Then, there exists an invariant probability measure.\nProof. Let V = Pd\ni=1\nzi\nbi + 1 and \u03c6(V ) = \u03c8(min1\u2264i\u2264d biV ) \u2212\u03c8(0). Then \u03c6 : R+ \u2192\n151\nR+ is increasing and strictly concave, \u03c6(z) \u2192\u221eas z \u2192\u221eand \u03c6(0) = 0. Using\nthe concavity and monotonicity of \u03c8(\u00b7), we have\nAV + \u03c6(V )\n(5.15)\n= \u2212\u03c8\n \nd\nX\ni=1\nzi\n!\n+ \u03c8\n \nmin\n1\u2264i\u2264d bi\nd\nX\ni=1\nbi\nzi\nbi\n+ min\n1\u2264i\u2264d bi\n!\n\u2212\u03c8(0) + \u03bd\n\u2264\u2212\u03c8\n \nmin\n1\u2264i\u2264d bi\nd\nX\ni=1\nzi\nbi\n!\n+ \u03c8\n \nmin\n1\u2264i\u2264d bi\nd\nX\ni=1\nbi\nzi\nbi\n+ min\n1\u2264i\u2264d bi\n!\n\u2212\u03c8(0) + \u03bd\n\u2264\u03c8\n\u0012\nmin\n1\u2264i\u2264d bi\n\u0013\n\u22122\u03c8(0) + \u03bd.\nRemark 8. The following \u03c8(z) satis\ufb01es the assumptions in Proposition 4 for suf-\n\ufb01ciently large \u03bd > 0.\n(i) \u03c8(z) = (c1 + c2z)\u03b1, where c1, c2 > 0 and 0 < \u03b1 < 1.\n(ii) \u03c8(z) = log(c3 + z), where c3 > 1.\nRemark 9. Let \u00b5 be the invariant probability measure for (Z1(t), . . . , Zd(t)) in\nProsposition 4. Then, we have\nR\n\u03c8\n\u0010\nmin1\u2264i\u2264d bi\nPd\ni=1\nzi\nbi + 1\n\u0011\n\u00b5(dz) < \u221e.\nIndeed, when h(\u00b7) may not be exponential or a sum of exponentials, we have\nthe following result.\nTheorem 20. Assume \u03bb(z) = \u03bd + z \u2212\u03c8(z), where \u03c8(\u00b7) : R+ \u2192R+ satis\ufb01es\nlimz\u2192\u221e\u03c8(z) = \u221eand limz\u2192\u221e\n\u03c8(z)\nz\n= 0 and also \u03bb(z) is increasing. Also assume\nthat \u2225h\u2225L1 = 1.\nThen there exists a stationary Hawkes process satisfying the\ndynamics (1.2).\nProof. The proof uses Poisson embedding and follows the ideas in Br\u00b4emaud and\nMassouli\u00b4e [14]. Consider the canonical space of a point process on R2 in which\n152\nN is Poisson with intensity 1. Let \u03bb0\nt = Z0\nt = 0, t \u2208R and let N 0 be the point\nprocess counting the points of N below the curve t 7\u2192\u03bb0\nt, i.e. N 0 = \u2205. De\ufb01ne\nrecursively the processes \u03bbn\nt , Zn\nt and N n, n \u22650 as follows.\n\u03bbn+1\nt\n= \u03bb\n\u0012Z t\n\u2212\u221e\nh(t \u2212s)N n(ds)\n\u0013\n,\nZn+1\nt\n=\nZ t\n\u2212\u221e\nh(t \u2212s)N n(ds),\nt \u2208R,\n(5.16)\nN n+1(C) =\nZ\nC\nN(dt \u00d7 [0, \u03bbn+1\nt\n]),\nC \u2208B(R).\nBy our construction, \u03bbn\nt is an FN\nt -intensity of N n (see Br\u00b4emaud and Massouli\u00b4e\n[14]). Since \u03bb(\u00b7) is increasing, the processes \u03bbn\nt , Zn\nt and N n are increasing in n\nThus, the limit processes \u03bbt, Zt, N exist. Since \u03bbn\nt , Zn\nt are stationary in t and\nincreasing in n, we have\n(5.17)\nE\u03bbn+1\n0\n= \u03bd + E[\u03bbn\n0]\nZ \u221e\n0\nh(t)dt \u2212E\u03c8(Zn+1\n0\n) \u2264\u03bd + E\u03bbn+1\n0\n\u2212E\u03c8(Zn+1\n0\n).\nTherefore, by Fatou\u2019s lemma, E[\u03c8(Z0)] \u2264\u03bd < \u221e. Thus, \u03c8(Zt) is \ufb01nite a.s. Since\nlimz\u2192\u221e\u03c8(z) = \u221e, Zt is \ufb01nite a.s. and thus \u03bbt is \ufb01nite a.s. N, which counts the\nnumber of points of N below the curve t 7\u2192\u03bbt, admits \u03bbt as an FN\nt -intensity. The\nmonotonicity implies\n(5.18)\n\u03bbn\nt \u2264\u03bb\n\u0012Z t\n\u2212\u221e\nh(t \u2212s)N(ds)\n\u0013\n,\n\u03bbt \u2265\u03bb\n\u0012Z t\n\u2212\u221e\nh(t \u2212s)N n(ds)\n\u0013\n.\nLetting n \u2192\u221e, we complete the proof.\nRemark 10. The following \u03c8(z) satis\ufb01es the assumptions in Theorem 20.\n(i) \u03c8(z) = (c1 + c2z)\u03b1, where c1, c2 > 0, 0 < \u03b1 < 1, \u03bd > c\u03b1\n1 and \u03b1c\u03b1\u22121\n1\nc2 < 1.\n(ii) \u03c8(z) = log(c3 + z), where 1 < c3 < e\u03bd.\n153\nNext, let us consider the critical linear case, i.e. \u03bb(z) = \u03bd + z, \u03bd > 0 and\n\u2225h\u2225L1=1. We also assume that m :=\nR \u221e\n0 th(t)dt < \u221e. There is no stationary\nHawkes process in this regime and in the rest of this subsection, we will try to\nunderstand its time asymptotics.\nFirst, let us prove a lemma concerning the expectations of \u03bbt and Nt.\nLemma 30. Assume \u03bb(z) = \u03bd +z, \u03bd > 0 and \u2225h\u2225L1=1 and m =\nR \u221e\n0 th(t)dt < \u221e.\nWe have\n(5.19)\nlim\nt\u2192\u221e\nE[\u03bbt]\nt\n= \u03bd\nm,\nlim\nt\u2192\u221e\nE[Nt]\nt2\n= \u03bd\n2m.\nProof. Since\n(5.20)\n\u03bbt = \u03bd +\nZ t\n0\nh(t \u2212s)dNs,\ntaking f(t) = E[\u03bbt], we get\n(5.21)\nf(t) = \u03bd +\nZ t\n0\nh(t \u2212s)f(s)ds = \u03bd +\nZ t\n0\nh(s)f(t \u2212s)ds.\nTaking the Laplace transform on both sides of the equation, it is easy to see that\nthe Laplace transform \u02c6f of f is given by\n(5.22)\n\u02c6f(\u03c3) =\n\u03bd\n\u03c3(1 \u2212\u02c6h(\u03c3))\n\u223c\u03bd\nm\n1\n\u03c32,\nas \u03c3 \u21930,\nsince \u02c6h(0) = 1 by \u2225h\u2225L1 = 1 and 1\u2212\u02c6h(\u03c3)\n\u03c3\n\u223c\u2212\u02c6h\u2032(0) = m. By a Tauberian theorem,\n(see Chapter XIII of Feller [38]), we get f(t)\nt\n\u2192\u03bd\nm as t \u2192\u221e. Using the simple fact\nthat E[Nt] =\nR t\n0 f(s)ds, we complete the proof.\n154\nTheorem 21. Assume \u03bb(z) = \u03bd + z, \u03bd > 0 and \u2225h\u2225L1 = 1, m =\nR \u221e\n0 th(t)dt < \u221e\nand h(\u00b7) Lipschitz. We have the following asymptotics.\n(i) As T \u2192\u221e, on D[0, 1],\n(5.23)\nNtT\nT 2 \u2192\nZ t\n0\n\u03b7sds,\nwhere \u03b7t is a squared Bessel process, i.e.\n(5.24)\nd\u03b7t = \u03bd\nmdt + 1\nm\n\u221a\u03b7tdBt,\n\u03b70 = 0.\n(ii) limT\u2192\u221eN\n\u0002\nT, T + t\nT\n\u0003\n= P(t), where P(t) is a P\u00b4olya process with parame-\nters\n1\n2m2 and 2\u03bdm.\nRemark 11. The fact that a squared Bessel process arises in the limit of a critical\nlinear Hawkes process is not a surprise. It is well known that a critical branching\nprocess after certain scalings will converge to a squared Bessel process in the limit.\nThis was discovered by Wei and Winnicki [105].\nRemark 12. Before we proceed to the proof of Theorem 21, let us recall that a\nP\u00b4olya process with parameters \u03b1 and \u03b2 is a point process de\ufb01ned as the following.\nGenerate a positive random variable \u03be, with Gamma distribution of parameters \u03b1\n(shape) and \u03b2 (scale). Conditional on \u03be, P(t) is a Poisson process with intensity \u03be.\nThe marginal distribution of P(t) is negative binomial and unlike the usual Poisson\nprocess, P\u00b4olya process has dependent increments. The covariance of the increments\ncan be computed explicitly as Cov(P(t + \u03b4t) \u2212P(t), P(t)) = t \u00b7 \u03b4t \u00b7 \u03b1\u03b22. Peng and\nKou [92] used P\u00b4olya process to model clustering e\ufb00ects in the credit markets.\nProof of Theorem 21. (i) Let H(t) :=\nR \u221e\nt\nh(s)ds. Then, we have H(0) = 1 and\n155\nR \u221e\n0 H(t)dt =\nR \u221e\n0 th(t)dt = m. Let Mt := Nt \u2212\nR t\n0 \u03bbsds. Let us integrate \u03bbs =\nR s\n0 h(s \u2212u)N(du) + \u03bd over 0 \u2264s \u2264tT. We get\n(5.25)\nZ tT\n0\n\u03bbsds =\nZ tT\n0\nZ s\n0\nh(s \u2212u)dMuds +\nZ tT\n0\nZ s\n0\nh(s \u2212u)\u03bbududs + \u03bdtT.\nRearranging the equation and dividing by T, we get\n(5.26) 1\nT\n\u0014Z tT\n0\n\u03bbsds \u2212\nZ tT\n0\nZ s\n0\nh(s \u2212u)\u03bbududs\n\u0015\n= 1\nT\nZ tT\n0\nZ s\n0\nh(s\u2212u)dMuds+\u03bdt.\nFubini\u2019s theorem implies that\n(5.27)\n1\nT\n\u0014Z tT\n0\n\u03bbudu \u2212\nZ tT\n0\n\u0012Z tT\u2212u\n0\nh(s)ds\n\u0013\n\u03bbudu\n\u0015\n= 1\nT\nZ tT\n0\n\u0012Z tT\u2212u\n0\nh(s)ds\n\u0013\ndMu + \u03bdt.\nBy the de\ufb01nition of H(\u00b7), this is equivalent to\n(5.28)\nZ t\n0\nTH(tT \u2212uT)\u03bbuT\nT du = MtT\nT\n+ \u03bdt + 1\nT\nZ t\n0\nTH(tT \u2212uT)d\n\u0012MuT\nT\n\u0013\n.\nMtT\nT\nis a martingale and the tightness can be easily established. Furthermore, we\nhave\n(5.29)\nsup\nT>0\nE\n\"\u0012MtT\nT\n\u00132#\n= sup\nT>0\n1\nT 2E\n\u0014Z tT\n0\n\u03bbsds\n\u0015\n< \u221e,\nsince E[\u03bbt] \u2264Ct for some C > 0 by Lemma 30. This implies that the limit of MtT\nT\nis also a martinagle.\nMoreover, NtT\nT 2 and\nR t\n0\n\u03bbsT\nT ds are both tight. To see this, since Nt and \u03bbt are\nnonnegative, we can think of\n\u0000d\n\u0000 NtT\nT 2\n\u0001\n, 0 \u2264t \u22641\n\u0001\nand\n\u0000 \u03bbtT\nT dt, 0 \u2264t \u22641\n\u0001\nas two\n156\nmeasures.\nBut by Lemma 30, we know that there exist some positive contant\nC > 0, such that\n(5.30)\nE\n\u0014NT\nT 2\n\u0015\n\u2264C\nand\nE\n\u0014Z 1\n0\n\u03bbsT\nT ds\n\u0015\n\u2264C,\nuniformly in T > 0. Therefore,\n\u0000d\n\u0000 NtT\nT 2\n\u0001\n, 0 \u2264t \u22641\n\u0001\nand\n\u0000 \u03bbtT\nT dt, 0 \u2264t \u22641\n\u0001\nare\ntight in the weak topology. Hence, their distribution functions NtT\nT 2 and\nR t\n0\n\u03bbsT\nT ds\nare tight in D[0, 1] equipped with the Skorohod topology. Let us say that MtT\nT\n\u2192\u03b2t,\nNtT\nT 2 \u2192\u03c8t and\nR t\n0\n\u03bbsT\nT ds \u2192\u03c6t as T \u2192\u221e. Since the jumps of NtT\nT 2 are uniformly\nbounded by\n1\nT 2 which goes to zero as T \u2192\u221e, we conclude that \u03c8t is continuous.\nSimilarly, \u03b2t and \u03c6t are continuous. Moreover, the di\ufb00erence\n(5.31)\nNtT\nT 2 \u2212\nZ t\n0\n\u03bbsT\nT ds = MtT\nT 2 ,\nis a martingale and by Doob\u2019s martingale inequality, for any \u03f5 > 0,\n(5.32)\nP\n\u0012\nsup\n0\u2264t\u22641\n\f\f\f\f\nMtT\nT 2\n\f\f\f\f \u2265\u03f5\n\u0013\n\u22644\nT 4E\n\u0014Z tT\n0\n\u03bbsds\n\u0015\n\u21920,\nas T \u2192\u221e.\nTherefore, \u03c8t = \u03c6t.\nLet us denote TH(\u00b7T) by HT,\nM\u00b7T\nT\nby MT\nand\nR \u00b7\n0\n\u03bbsT\nT ds by \u039bT. For any smooth function K(\u00b7) supported on R+, taking the\nconvolutions of the both sides of (5.28), we get\n(5.33)\nK \u2217HT \u2217\u039bT = K \u2217MT + K \u2217(\u03bd\u00b7) + 1\nT K \u2217HT \u2217MT.\n157\nLetting T \u2192\u221e, using the fact that\nR \u221e\n0 H(t)dt =\nR \u221e\n0 th(t)dt = m, we get\n(5.34)\nm\nZ t\n0\nK(t \u2212s)d\u03c6s =\nZ t\n0\nK(t \u2212s)(\u03b2s + \u03bds)ds.\nSince this is true for any K, we get d\u03c6t\ndt = \u03b2t\nm + \u03bd\nt . Finally,\n(5.35)\n\u0012MtT\nT\n\u00132\n\u2212\nZ t\n0\n\u03bbsT\nT ds\nis a martingale and if we let T \u2192\u221e, we conclude that \u03b22\nt \u2212\u03c6t is a martingale. Let\n\u03b7t := d\u03c6t\ndt . We have proved that NtT\nT 2 \u2192\nR t\n0 \u03b7sds weakly on D[0, 1] equipped with\nSkorohod topology and \u03b7t is a squared Bessel process,\n(5.36)\nd\u03b7t = \u03bd\nmdt + 1\nm\n\u221a\u03b7tdBt,\n\u03b70 = 0.\n(ii) N[T, T + t\nT ] has the compensator\nR T+ t\nT\nT\n\u03bbsds. Observe that\nR T+ t\nT\nT\n\u03bbsds =\nT 2 R 1+ t\nT 2\n1\n\u03bbsT\nT ds \u2192\u03b71t as T \u2192\u221e, where \u03b71 has a Gamma distribution with shape\n1\n2m2 and scale 2\u03bdm.\nNow let us consider the case when h(\u00b7) has heavy tail, i.e.\nR \u221e\n0 th(t)dt = \u221e.\nLet us \ufb01rst prove the following lemma.\nLemma 31. Assume that\n(5.37)\n1 \u2212\nZ t\n0\nh(s)ds =\nZ \u221e\nt\nh(s)ds \u223ct\u2212\u03b1,\n0 < \u03b1 < 1.\nThen,\n(5.38)\nlim\nt\u2192\u221e\nE[\u03bbt]\nt\u03b1\n= \u03bd \u00b7 sin \u03c0\u03b1\n\u03c0\u03b1 ,\nlim\nt\u2192\u221e\nE[Nt]\nt1+\u03b1 =\n\u03bd\n\u0393(1 \u2212\u03b1)\u0393(2 + \u03b1).\n158\nProof. The Tauberian theorem of Chapter XIII of Feller [38] says that\n(5.39)\n1 \u2212\u02c6h(\u03c3) \u223c\u0393(1 \u2212\u03b1)\u03c3\u03b1,\n\u03c3 \u21920+.\nLet E[\u03bbt] = f(t). This implies that\n(5.40)\n\u02c6f(\u03c3) =\n\u03bd\n\u03c3(1 \u2212\u02c6h(\u03c3))\n\u223c\u03bd\u03c3\u22121\u2212\u03b1\n\u0393(1 \u2212\u03b1),\n\u03c3 \u21920+,\nwhich again by a Tauberian theorem (Theorem 2 of Chapter XIII.5 of Feller [38])\nimplies\n(5.41)\nZ t\n0\nf(s)ds \u223c\n\u03bd\n\u0393(1 \u2212\u03b1)\u0393(2 + \u03b1)t1+\u03b1,\nt \u2192\u221e.\nHence,\n(5.42)\nE[Nt] =\nZ t\n0\nE[\u03bbs]ds =\nZ t\n0\nf(s)ds \u223c\n\u03bd\n\u0393(1 \u2212\u03b1)\u0393(2 + \u03b1)t1+\u03b1,\nt \u2192\u221e.\nSince E[\u03bbt] = \u03bd +\nR t\n0 h(t \u2212s)dE[Ns], it is easy to check that\n(5.43)\nE[\u03bbt] = f(t) \u223c\n\u03bd\n\u0393(1 \u2212\u03b1)\u0393(1 + \u03b1)t\u03b1 = \u03bd \u00b7 sin \u03c0\u03b1\n\u03c0\u03b1\n\u00b7 t\u03b1,\nt \u2192\u221e.\nWe obtain the following law of large numbers.\nTheorem 22. Assume that\nR \u221e\nt\nh(s)ds \u223c\n1\nt\u03b1, 0 < \u03b1 < 1. Then,\n(5.44)\nNt\nt1+\u03b1 \u2192\n\u03bd\n\u0393(1 \u2212\u03b1)\u0393(2 + \u03b1)\nand\n\u03bbt\nt\u03b1 \u2192\u03bd \u00b7 sin \u03c0\u03b1\n\u03c0\u03b1 ,\na.s. as t \u2192\u221e.\n159\nProof. Let Xt = Nt \u2212E[Nt]. Then, Xt satis\ufb01es (see Bacry et al. [2])\n(5.45)\nXt = Mt +\nZ t\n0\n\u03c8(t \u2212s)Msds,\nwhere Mt = Nt \u2212\nR t\n0 \u03bbsds and \u03c8 = P\nn h\u2217n. Then, by Doob\u2019s maximal inequality,\nit is not hard to see that\nE\n\"\u0012Nt \u2212E[Nt]\nt1+\u03b1\n\u00132#\n\u2264\n1\nt2+2\u03b1E\n\u0014\nsup\ns\u2264t\nM 2\ns\n\u0015 \u0012\n1 +\nZ t\n0\n\u03c8(t \u2212s)ds\n\u00132\n(5.46)\n\u2264\nC\nt2+2\u03b1t1+\u03b1(t\u03b1)2 \u21920,\n(5.47)\nas t \u2192\u221esince 0 < \u03b1 < 1. Hence, as t \u2192\u221e,\n(5.48)\nNt\nt1+\u03b1 \u2192\n\u03bd\n\u0393(1 \u2212\u03b1)\u0393(2 + \u03b1),\nin L2 as t \u2192\u221e.\nTo show the almost sure convergence, we need only to show that 1\nt sups\u2264t Ms \u21920\na.s. as t \u2192\u221e. De\ufb01ne Yt =\nR t\n0\n1\n1+sdMs. Then by Lemma 31,\n(5.49)\nsup\nt>0 E[Y 2\nt ] =\nZ \u221e\n0\nE[\u03bbs]\n(1 + s)2ds < \u221e.\nBy the martingale convergence theorem, Yt \u2192Y\u221ea.s. as t \u2192\u221e. It follows that\n(5.50)\nMt\nt + 1 = Yt \u2212\n1\nt + 1\nZ t\n0\nYsds \u21920,\na.s. as t \u2192\u221e. From here, it is easy to show that 1\nt sups\u2264t Ms \u21920 a.s. Finally,\n160\nsince \u03bbt = \u03bd +\nR t\n0 h(t \u2212s)N(ds), we conclude that\n(5.51)\n\u03bbt\nt\u03b1 \u2192\u03bd \u00b7 sin \u03c0\u03b1\n\u03c0\u03b1 ,\na.s. as t \u2192\u221e.\n5.4\nSuper-Critical Regime\nIn this section, we are interested in the super-critical regime, i.e. limz\u2192\u221e\n\u03bb(z)\nz\n=\n1 and \u2225h\u2225L1 > 1. First, let us compute the asymptotics for the expectations. Let\n\u03b8 > 0 be the unique positive number such that\nR \u221e\n0 e\u2212\u03b8th(t)dt = 1. \u03b8 is sometimes\nreferred to as the Malthusian parameter in the literature. Let us also de\ufb01ne\n(5.52)\nh(t) = h(t)e\u2212\u03b8t,\nm =\nZ \u221e\n0\nth(t)e\u2212\u03b8tdt.\nClearly under our assumptions 0 < m < \u221eand \u2225h\u2225L1 = 1.\nLemma 32. (i) Assume \u03bb(z) = \u03bd + z, \u03bd > 0 being a constant. Then,\n(5.53)\nlim\nt\u2192\u221e\nE[\u03bbt]\ne\u03b8t\n= \u03bd\n\u03b8m.\n(ii) Assume limz\u2192\u221e\n\u03bb(z)\nz\n= 1 and let \u03bb(\u00b7) be bounded below by a positive con-\nstant. Then,\n(5.54)\nlim\nt\u2192\u221e\n1\nt log E[\u03bbt] = lim\nt\u2192\u221e\n1\nt log E[Nt] = \u03b8.\n161\nProof. (i) Let f(t) = E[\u03bbt]. We have\n(5.55)\nf(t)\ne\u03b8t = \u03bd\ne\u03b8t +\nZ t\n0\nh(t \u2212s)e\u2212\u03b8(t\u2212s)f(s)\ne\u03b8s ds = \u03bd\ne\u03b8t +\nZ t\n0\nh(t \u2212s)f(s)\ne\u03b8s ds\nTaking Laplace transform, we get\n(5.56)\n\\\nf(t)e\u2212\u03b8t(\u03c3) =\n\u03bd\n\u03b8(1 \u2212\u02c6h(\u03c3))\n\u223c\u03bd\n\u03b8m \u00b7 1\n\u03c3,\nas \u03c3 \u21930. By the Tauberian theorem, we have\n(5.57)\nlim\nt\u2192\u221e\nE[\u03bbt]\ne\u03b8t\n= \u03bd\n\u03b8m.\n(ii) is a direct consequence of (i).\nThis is consistent with the exponential case when h(t) = ae\u2212bt and a > b. We\nhave\n(5.58)\nE[\u03bbt] = \u2212\u03bdb\na \u2212b +\n\u03bda\na \u2212be(a\u2212b)t,\nE[Nt] = \u2212\u03bdbt\na \u2212b +\n\u03bda\n(a \u2212b)2(e(a\u2212b)t \u22121).\nIndeed, in the exponential case, \u03b8 = a \u2212b and\n(5.59) d(Zte\u2212(a\u2212b)t) = e\u2212(a\u2212b)tdZt + Ztde\u2212(a\u2212b)t = \u2212aZte\u2212(a\u2212b)tdt + ae\u2212(a\u2212b)tdNt.\nLet Yt = Zte\u2212(a\u2212b)t. We have\n(5.60)\ndYt = \u2212aYtdt + ae\u2212(a\u2212b)tdNt = \u03bdae\u2212(a\u2212b)tdt + ae\u2212(a\u2212b)tdMt.\n162\nIf we assume that N(\u2212\u221e, 0] = 0, then Z0 = 0 and\n(5.61)\nYt =\nZ t\n0\n\u03bdae\u2212(a\u2212b)sds + a\nZ t\n0\ne\u2212(a\u2212b)sdMs.\nClearly,\nR t\n0 \u03bde\u2212(a\u2212b)sds \u2192\n\u03bda\na\u2212b and\nR t\n0 ae\u2212(a\u2212b)sdMs is a martingale and\n(5.62)\nsup\nt>0 E\n\"\u0012Z t\n0\ne\u2212(a\u2212b)sdMs\n\u00132#\n=\nZ \u221e\n0\ne\u22122(a\u2212b)sE[\u03bbs]ds = \u03bd(2a \u2212b)\n2(a \u2212b)2 < \u221e.\nTherefore, by the martingale convergence theorem, there exists some W in L2(P)\nsuch that\n(5.63)\n\u03bbt\ne(a\u2212b)t \u2192\n\u03bda\na \u2212b + aW,\nas t \u2192\u221e. The convergence is a.s. and also in L2(P).\nFor the general h(\u00b7) such that \u2225h\u2225L1 > 1, we may even consider the case when\n\u2225h\u2225L1 = \u221e. For instance, if we assume that h(\u00b7) is decreasing and continuous and\nthen h(\u00b7) is bounded and all the arguments for the case 1 < \u2225h\u2225L1 < \u221ewould\nwork for the case \u2225h\u2225L1 = \u221eas well.\nTheorem 23. Assume \u03bb(z) = \u03bd + z, \u03bd > 0. We have,\n(5.64)\n\u03bbt\ne\u03b8t \u2192\u03bd\n\u03b8m + W\nm ,\na.s. as t \u2192\u221e,\nwhere W =\nR \u221e\n0 e\u2212\u03b8tdMt.\n163\nProof. It is not very hard to observe that\n\u03bbt\ne\u03b8t = \u03bd\ne\u03b8t +\nZ t\n0\nh(t \u2212s)\ne\u03b8t\ndMs +\nZ t\n0\nh(t \u2212s)\ne\u03b8t\n\u03bbsds\n(5.65)\n= \u03bd\ne\u03b8t +\nZ t\n0\nh(t \u2212s)e\u2212\u03b8(t\u2212s)\n\u0012dMs\ne\u03b8s\n\u0013\n+\nZ t\n0\nh(t \u2212s)e\u2212\u03b8(t\u2212s) \u03bbs\ne\u03b8sds\n= \u03bd\ne\u03b8t +\nZ t\n0\nh(t \u2212s)dM s +\nZ t\n0\nh(t \u2212s) \u03bbs\ne\u03b8sds.\nTaking Laplace transform, we get\n\\\n\u03bbte\u2212\u03b8t(\u03c3) =\n\u03bd\n\u03b8+\u03c3 + \u02c6h(\u03c3)\nR \u221e\n0 e\u2212\u03c3tdM t\n1 \u2212\u02c6h(\u03c3)\n=\n\u03bd\n\u03b8+\u03c3 + \u02c6h(\u03c3)\nR \u221e\n0 e\u2212(\u03c3+\u03b8)tdMt\n1 \u2212\u02c6h(\u03c3)\n(5.66)\n\u223c\n\u03bd\n\u03b8 + W\nm\n\u00b7 1\n\u03c3,\nas \u03c3 \u21930, where W =\nR \u221e\n0 e\u2212\u03b8tdMt. Notice that W is well de\ufb01ned a.s. because\nR t\n0 e\u2212\u03b8sdMs is a martingale and\n(5.67)\nsup\nt>0 E\n\"\u0012Z t\n0\ne\u2212\u03b8sdMs\n\u00132#\n=\nZ \u221e\n0\ne\u22122\u03b8sE[\u03bbs]ds < \u221e\nby Lemma 32. Hence, by the Tauberian theorem, we conclude that, as t \u2192\u221e,\n(5.68)\n\u03bbt\ne\u03b8t \u2192\u03bd\n\u03b8m + W\nm .\na.s.\nCorollary 1.\nNt\ne\u03b8t \u2192\n\u03bd\n\u03b82m + W\n\u03b8m a.s. as t \u2192\u221e.\nProof. Let Mt = Nt \u2212\nR t\n0 \u03bbsds.\nThen, since Mt is a martingale and E[M 2\nt ] =\nR t\n0 E\u03bbsds \u2264Ce\u03b8t for some C > 0, it is easy to see that Mt\ne\u03b8t \u21920 a.s. as t \u2192\u221e. On\n164\nthe other hand,\n(5.69)\n1\ne\u03b8t\nZ t\n0\n\u03bbsds =\nZ t\n0\ne\u2212\u03b8(t\u2212s) \u03bbs\ne\u03b8sds \u21921\n\u03b8\n\u0014 \u03bd\n\u03b8m + W\nm\n\u0015\n,\nby Theorem 23. Hence, we get the desired result.\nRemark 13. It would be interesting to study the properties of W de\ufb01ned in The-\norem 23. Observe that\nE\nh\ne\u2212\u03c3\nR t\n0 e\u2212\u03b8sdMsi\n= E\nh\ne\u2212\u03c3(\nR t\n0 e\u2212\u03b8sdNs\u2212\nR t\n0\nR s\n0 h(s\u2212u)N(du)e\u2212\u03b8sds)i\ne\n\u03c3\n\u03b8 \u03bd(1\u2212e\u2212\u03b8t)\n(5.70)\n= E\nh\ne\u2212\u03c3\nR t\n0(e\u2212\u03b8s\u2212\nR t\ns h(u\u2212s)e\u2212\u03b8udu)N(ds)i\ne\n\u03c3\n\u03b8 \u03bd(1\u2212e\u2212\u03b8t)\n= E\nh\ne\u2212\u03c3\nR t\n0 e\u2212\u03b8sH(t\u2212s)N(ds)i\ne\n\u03c3\n\u03b8 \u03bd(1\u2212e\u2212\u03b8t),\nwhere H(t) =\nR \u221e\nt\nh(s)ds. Hence,\n(5.71)\nE[e\u2212\u03c3W] = e\n\u03c3\u03bd\n\u03b8 lim\nt\u2192\u221ee\u03bd\nR t\n0 gt(s)ds,\nwhere gt(s) = exp\n\b\n\u2212\u03c3\ne\u03b8te\u03b8sH(s) +\nR s\n0 h(s \u2212u)gt(u)du\n\t\n\u22121.\n5.5\nExplosive Regime\nIn this section, we will provide an explosion, non-explosion criterion for non-\nlinear Hawkes processes, together with some asymptotics for the explosion time in\nthe explosive regime. Let \u03c4\u2113= inf{t > 0 : \u03bbt \u2265\u2113}. The quantity\n(5.72)\nlim\n\u2113\u2192\u221eP(\u03c4\u2113\u2264t) = F(t) = P(\u03c4 \u2264t),\n165\nis de\ufb01ned as the distribution function of the explosion time \u03c4. We say there is\nno explosion if F \u22610, otherwise there is explosion. For a short introduction to\nexplosion, non-explosion, we refer to Varadhan [109].\nNext, we provide an explosion, non-explosion criterion for nonlinear Hawkes\nprocesses.\nThe proof is based on a well known result for the explosion, non-\nexplosion criterion for a class of point processes which can be found in the book\nby Kallenberg [61].\nTheorem 24 (Explosion, Non-Explosion Criterion). Assume that \u03bb(\u00b7) is increas-\ning and that h(\u00b7) is integrable and decreasing, then there is explosion if and only\nif\n(5.73)\n\u221e\nX\nn=0\n1\n\u03bb(n) < \u221e.\nProof. Observe that, for any T > 0,\n(5.74)\nPh(T)(\u03c4 \u2264T) \u2264P(\u03c4 \u2264T) \u2264Ph(0)(\u03c4 \u2264T),\nwhere Ph(0) denotes the probability measure for the point process such that initially\nthe rate function is \u03bb(0) and after nth jumps, the rate function becomes \u03bb(nh(0));\nPh(T) is de\ufb01ned similarly. It is well known that the point process with intensity\n\u03bb(Nt\u2212) is explosive if and only if\n(5.75)\n\u221e\nX\nn=0\n1\n\u03bb(n) < \u221e.\nFor the details and proof of the above result, we refer to Kallenberg [61]. But it is\nclear under our assumptions that P\u221e\nn=0\n1\n\u03bb(n) < \u221eif and only if P\u221e\nn=0\n1\n\u03bb(cn) < \u221e,\n166\nwhere c > 0 is any positive constant. Therefore, there is explosion if and only if\n(5.76)\n\u221e\nX\nn=0\n1\n\u03bb(n) < \u221e.\nEvaluating the exact probability distribution of the explosion time \u03c4, i.e. P(\u03c4 \u2264\nt), is hard and almost impossible. Nevertheless, one can still study its asymptotic\nbehavior, i.e.\n(i) P(\u03c4 \u2265t) for large time t;\n(ii) P(\u03c4 \u2264\u03f5) for small time \u03f5.\nIn the rest of this section, we will use Proposition 5 to answer (i) and Proposi-\ntion 6 to answer (ii).\nProposition 5. Under the assumptions of Theorem 24 satisfying the explosion\ncriterion, we have\n(5.77)\nlim\nt\u2192\u221e\n1\nt log P\u2205(\u03c4 \u2265t) = inf\nt>0\n1\nt log P\u2205(\u03c4 \u2265t) = \u2212\u03c3,\nwhere 0 < \u03c3 < \u221e.\nProof. For a nonlinear Hawkes process with empty history, i.e. N(\u2212\u221e, 0] = 0, we\nhave\n(5.78)\nP\u2205(\u03c4 \u2265t + s) = P\u2205(\u03c4 \u2265t + s|\u03c4 \u2265s)P\u2205(\u03c4 \u2265s) \u2264P\u2205(\u03c4 \u2265t)P\u2205(\u03c4 \u2265s).\n167\nTherefore, log P\u2205(\u03c4 \u2265t) is sub-additive and we know that\n(5.79)\nlim\nt\u2192\u221e\n1\nt log P\u2205(\u03c4 \u2265t) = inf\nt>0\n1\nt log P\u2205(\u03c4 \u2265t) = \u2212\u03c3\nexists. And we also know that 0 < \u03c3 < \u221e. For example, it is easy to see that\n\u03c3 \u2264\u03bb(0). That is because P\u2205(\u03c4 \u2265t) \u2265P\u2205(N[0, t] = 0) = e\u2212\u03bb(0)t. To see that\n\u03c3 > 0, choose M large enough so that P(\u03c4 \u2265M) < 1 and then \u03c3 \u2265\u22121\nM log P(\u03c4 \u2265\nM) > 0.\nRemark 14. Indeed, in the Markovian case, we can say something more about \u03c3\nde\ufb01ned in Proposition 5. When h(t) = ae\u2212bt, Zt = P\n\u03c4<t ae\u2212b(t\u2212\u03c4) is Markovian\nand by noticing that\n(5.80)\nexp\n\u001a\nf(Zt) \u2212f(Z0) \u2212\nZ t\n0\nAef\nef (Zs)ds\n\u001b\nis a martingale and that Nt explodes if and only if Zt explodes, we have\n(5.81)\nlim\nt\u2192\u221e\n1\nt log P\u2205(\u03c4 \u2265t) = \u2212\u03c3,\nwhere \u03c3 is the principal eigenvalue for\n(5.82)\nAu = \u2212\u03c3u,\nu \u22651.\nNote that here you have to choose the test function u \u22651 rather than u \u22650.\nProposition 6. Assume that \u03bb(z) = \u03b3zk +\u03b4, where \u03b3, \u03b4 > 0 and k > 1. According\nto Theorem 24, it is in the explosive regime. We have the following asymptotics\n168\nfor small time \u03f5.\n(5.83)\nlim\n\u03f5\u21920 \u03f5\n1\nk\u22121 log P(\u03c4 \u2264\u03f5) = C\nk\nk\u22121\nk\n(k\u2212\n1\nk\u22121 \u2212k\u2212\nk\nk\u22121),\nwhere Ck =\nR \u221e\n0 log\n\u0010\n\u03b3ykh(0)k\n\u03b3ykh(0)k+1\n\u0011\ndy.\nBefore we proceed, let us \ufb01rst quote de Bruijn\u2019s Tauberian theorem from the\nbook by Bingham, Goldie and Teugels [9], which will be used in the proof of\nProposition 6.\nTheorem 25 (de Bruijn\u2019s Tauberian theorem). Let \u00b5 be a measure on (0, \u221e)\nwhose Laplace transform M(\u03bb) :=\nR \u221e\n0 e\u2212\u03bbxd\u00b5(x) converges for all \u03bb > 0. If \u03b1 < 0,\n\u03c6 \u2208R\u03b1(0+), i.e. \u03c6(\u03bbt)/\u03c6(t) \u223c\u03bb\u03b1 as t \u223c0+, put \u03c8(\u03bb) := \u03c6(\u03bb)/\u03bb \u2208R\u03b1\u22121(0+),\nthen, for B > 0,\n(5.84)\n\u2212log \u00b5(0, x] \u223c\nB\n\u00af\u03c6(1/x),\nx \u21920+,\nif and only if\n(5.85)\n\u2212log M(\u03bb) \u223c(1 \u2212\u03b1)\n\u0012 B\n\u2212\u03b1\n\u0013\n\u03b1\n\u03b1\u22121\n1\n\u00af\u03c8(\u03bb),\n\u03bb \u2192\u221e.\nHere, \u00af\u03c6(\u03bb) := sup{t : \u03c6(t) > \u03bb} and similarly for \u00af\u03c8.\nProof of Proposition 6. First, let us observe that since we are considering the event\n{\u03c4 \u2264\u03f5} for \u03f5 > 0 very small. It is su\ufb03cient to consider the point process with\nintensity \u03bb(h(0)Nt\u2212) at time t.\n169\nTo apply de Bruijin\u2019s Tauberian theorem, notice that\n(5.86)\n\u2212log M(\u03c3) = \u2212\n\u221e\nX\ni=0\nlog\n\u0012\n\u03bb(ih(0))\n\u03bb(ih(0)) + \u03c3\n\u0013\n.\nRecall that \u03bb(z) = \u03b3zk + \u03b4, where \u03b3, \u03b4 > 0 and k > 1. Then,\n\u2212log M(\u03c3) = \u2212\n\u221e\nX\ni=0\nlog\n\u0012\n\u03b3ikh(0)k + \u03b4\n\u03b3ikh(0)k + \u03b4 + \u03c3\n\u0013\n(5.87)\n\u2265\u2212\nZ \u221e\n1\nlog\n\u0012\n\u03b3xkh(0)k + \u03b4\n\u03b3xkh(0)k + \u03b4 + \u03c3\n\u0013\ndx\n= \u2212\u03c31/k\nZ \u221e\n1/\u03c31/k log\n\u0012\n\u03b3\u03c3ykh(0)k + \u03b4\n\u03b3\u03c3ykh(0)k + \u03b4 + \u03c3\n\u0013\ndy\n\u223c\u2212\u03c31/k\nZ \u221e\n0\nlog\n\u0012\n\u03b3ykh(0)k\n\u03b3ykh(0)k + 1\n\u0013\ndy,\nas \u03c3 \u2192\u221e.\nSimilarly,\n\u2212log M(\u03c3) \u2264\u2212\nZ \u221e\n0\nlog\n\u0012\n\u03b3xkh(0)k + \u03b4\n\u03b3xkh(0)k + \u03b4 + \u03c3\n\u0013\ndx\n(5.88)\n\u223c\u2212\u03c31/k\nZ \u221e\n0\nlog\n\u0012\n\u03b3ykh(0)k\n\u03b3ykh(0)k + 1\n\u0013\ndy\nas \u03c3 \u2192\u221e.\nNow let Ck =\nR \u221e\n0 log\n\u0010\n\u03b3ykh(0)k\n\u03b3ykh(0)k+1\n\u0011\ndy, \u03c6(t) = t1\u2212k, \u03c8(t) = t\u2212k and \u03b1 = 1 \u2212k < 0.\nThen \u00af\u03c6(1/\u03f5) = (1/\u03f5)\u2212\n1\nk\u22121 and \u00af\u03c8(\u03c3) = \u03c3\u22121\nk . To apply the theorem, we need to solve\nB such that\n(5.89)\n(1 \u2212\u03b1)\n\u0012 B\n\u2212\u03b1\n\u0013\n\u03b1\n\u03b1\u22121\n= k\n\u0012\nB\nk \u22121\n\u0013 k\u22121\nk\n= Ck,\n170\nfor B = (k \u22121)(Ck/k)\nk\nk\u22121. Therefore,\n(5.90)\nlim\n\u03f5\u21920 \u03f5\n1\nk\u22121 log P(\u03c4 \u2264\u03f5) = C\nk\nk\u22121\nk\n(k\u2212\n1\nk\u22121 \u2212k\u2212\nk\nk\u22121).\n171\nChapter 6\nLimit Theorems for Marked\nHawkes Processes\n6.1\nIntroduction and Main Results\n6.1.1\nIntroduction\nWe consider in this chapter a linear Hawkes process with random marks. Let\nNt be a simple point process. Nt denotes the number of points in the interval [0, t).\nLet Ft be the natural \ufb01ltration up to time t. We assume that N(\u2212\u221e, 0] = 0. At\ntime t, the point process has Ft-predictable intensity\n(6.1)\n\u03bbt := \u03bd + Zt,\nZt :=\nX\n\u03c4i<t\nh(t \u2212\u03c4i, ai),\nwhere \u03bd > 0, the (\u03c4i)i\u22651 are arrival times of the points, and the (ai)i\u22651 are i.i.d.\nrandom marks, ai being independent of previous arrival times \u03c4j, j \u2264i.\nLet\nus assume that ai has a common distribution q(da) on a metric space X. Here,\n172\nh(\u00b7, \u00b7) : R+ \u00d7 X \u2192R+ is integrable, i.e.\nR \u221e\n0\nR\nX h(t, a)q(da)dt < \u221e. Let H(a) :=\nR \u221e\n0 h(t, a)dt for any a \u2208X. We also assume that\n(6.2)\nZ\nX\nH(a)q(da) < 1.\nLet Pq denote the probability measure for the ai\u2019s with the common law q(da).\nUnder assumption (6.2), it is well known that there exists a unique stationary\nversion of the linear marked Hawkes process satisfying the dynamics (6.1) and\nthat by ergodic theorem, a law of large numbers holds,\n(6.3)\nlim\nt\u2192\u221e\nNt\nt =\n\u03bd\n1 \u2212Eq[H(a)].\nThis chapter is organized as follows. In Section 6.1.2, we will introduce the main\nresults of this paper, i.e. the central limit theorem and the large deviation principle\nfor linear marked Hawkes processes. The proof of the central limit theorem will be\ngiven in Section 6.2 and the proof of the large deviation principle will be given in\nSection 6.3. Finally, we will discuss an application of our results to a risk model\nin \ufb01nance in Section 6.4.\n6.1.2\nMain Results\nFor a linear marked Hawkes process satisfying the dynamics (6.1), we have the\nfollowing large deviation principle.\nTheorem 26 (Large Deviation Principle). Assume the conditions (6.2) and\n(6.4)\nlim\nx\u2192\u221e\n\u001aZ\nX\neH(a)xq(da) \u2212x\n\u001b\n= \u221e.\n173\nThen, (Nt/t \u2208\u00b7) satis\ufb01es a large deviation principle with rate function,\n\u039b(x) :=\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f3\ninf \u02c6q\nn\nxE\u02c6q[H(a)] + \u03bd \u2212x + x log\n\u0010\nx\nxE\u02c6q[H(a)]+\u03bd\n\u0011\n+ xE\u02c6q h\nlog d\u02c6q\ndq\nio\nx \u22650\n+\u221e\nx < 0\n=\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f3\n\u03b8\u2217x \u2212\u03bd(x\u2217\u22121)\nx \u22650\n+\u221e\nx < 0\n,\nwhere the in\ufb01mum of \u02c6q is taken over M(X), the space of probability measures on X\nsuch that \u02c6q is absolutely continuous w.r.t. q. Here, \u03b8\u2217and x\u2217satisfy the following\nequations\n(6.5)\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f3\nx\u2217= Eq \u0002\ne\u03b8\u2217+(x\u2217\u22121)H(a)\u0003\nx\n\u03bd = x\u2217+ x\n\u03bdEq \u0002\nH(a)e\u03b8\u2217+(x\u2217\u22121)H(a)\u0003\n.\nTheorem 27 (Central Limit Theorem). Assume limt\u2192\u221et1/2 R \u221e\nt\nEq[h(s, a)]ds = 0\nand that (6.2) holds. Then,\n(6.6)\nNt \u2212\n\u03bdt\n1\u2212Eq[H(a)]\n\u221a\nt\n\u2192N\n\u0012\n0, \u03bd(1 + Varq[H(a)])\n(1 \u2212Eq[H(a)])3\n\u0013\n,\nin distribution as t \u2192\u221e.\n174\n6.2\nProof of Central Limit Theorem\nProof of Theorem 27. First, let us observe that\nZ t\n0\n\u03bbsds = \u03bdt +\nX\n\u03c4i<t\nZ t\n\u03c4i\nh(s \u2212\u03c4i, ai)ds\n(6.7)\n= \u03bdt +\nX\n\u03c4i<t\nH(ai) \u2212Et,\nwhere the error term Et is given by\n(6.8)\nEt :=\nX\n\u03c4i<t\nZ \u221e\nt\nh(s \u2212\u03c4i, ai)ds.\nTherefore,\nNt \u2212\nR t\n0 \u03bbsds\n\u221a\nt\n= Nt \u2212\u03bdt \u2212P\n\u03c4i<t H(ai)\n\u221a\nt\n+ Et\n\u221a\nt\n(6.9)\n= (1 \u2212Eq[H(a)])Nt \u2212\u00b5t\n\u221a\nt\n+ Eq[H(a)]Nt \u2212P\n\u03c4i<t H(ai)\n\u221a\nt\n+ Et\n\u221a\nt,\nwhere \u00b5 :=\n\u03bd\n1\u2212Eq[H(a)]. Rearranging the terms in (6.9), we get\n(6.10)\nNt \u2212\u00b5t\n\u221a\nt\n=\n1\n1 \u2212Eq[H(a)]\n\"\nNt \u2212\nR t\n0 \u03bbsds\n\u221a\nt\n+\nP\n\u03c4i<t(H(ai) \u2212Eq[H(a)])\n\u221a\nt\n\u2212Et\n\u221a\nt\n#\n.\nIt is easy to check that\nEt\n\u221a\nt \u21920 in probability as t \u2192\u221e.\nTo see this, \ufb01rst\nnotice that E[\u03bbt] \u2264\n\u03bd\n1\u2212Eq[H(a)] uniformly in t. Let g(t, a) :=\nR \u221e\nt\nh(s, a)ds. We have\n175\nEt = P\n\u03c4i<t g(t \u2212\u03c4i, ai) and thus\nE[Et] =\nZ t\n0\nZ\nX\ng(t \u2212s, a)q(da)E[\u03bbs]ds\n(6.11)\n\u2264\n\u03bd\n1 \u2212Eq[H(a)]\nZ t\n0\nZ\nX\ng(t \u2212s, a)q(da)ds\n=\n\u03bd\n1 \u2212Eq[H(a)]\nZ t\n0\nEq[g(s, a)]ds.\nHence, by L\u2019H\u02c6opital\u2019s rule,\nlim\nt\u2192\u221e\n1\nt1/2\nZ t\n0\nEq[g(s, a)]ds = lim\nt\u2192\u221e\nEq[g(t, a)]\n1\n2t\u22121/2\n(6.12)\n= lim\nt\u2192\u221e2t1/2\nZ \u221e\nt\nEq[h(s, a)]ds = 0.\nHence, Et\n\u221a\nt \u21920 in probability as t \u2192\u221e.\nFurthermore, M1(t) := Nt \u2212\nR t\n0 \u03bbsds and M2(t) := P\n\u03c4i<t(H(ai)\u2212Eq[H(a)]) are\nboth martingales.\nMoreover, since\nR t\n0 \u03bbsds is of \ufb01nite variation, the quadratic variation of M1(t)+\nM2(t) is the same as the quadratic variation of Nt + M2(t).\nAnd notice that\nNt + M2(t) = P\n\u03c4i<t(1 + H(ai) \u2212Eq[H(a)]) which has quadratic variation\n(6.13)\nX\n\u03c4i<t\n(1 + H(ai) \u2212Eq[H(a)])2.\n176\nBy the standard law of large numbers, we have\n1\nt\nX\n\u03c4i<t\n(1 + H(ai) \u2212Eq[H(a)]) = Nt\nt \u00b7 1\nNt\nX\n\u03c4i<t\n(1 + H(ai) \u2212Eq[H(a)])2\n(6.14)\n\u2192\n\u03bd\n1 \u2212Eq[H(a)] \u00b7 Eq \u0002\n(1 + H(a) \u2212Eq[H(a)])2\u0003\n= \u03bd(1 + Varq[H(a)])\n1 \u2212Eq[H(a)]\n,\na.s. as t \u2192\u221e. By a standard martingale central limit theorem, we conclude that\n(6.15)\nNt \u2212\n\u03bdt\n1\u2212Eq[H(a)]\n\u221a\nt\n\u2192N\n\u0012\n0, \u03bd(1 + Varq[H(a)])\n(1 \u2212Eq[H(a)])3\n\u0013\n,\nin distribution as t \u2192\u221e.\n6.3\nProof of Large Deviation Principle\n6.3.1\nLimit of a Logarithmic Moment Generating Function\nIn this subsection, we prove the existence of the limit of the logarithmic moment\ngenerating function limt\u2192\u221e\n1\nt log E[e\u03b8Nt] and give a variational formula and a more\nexplicit formula for this limit.\nTheorem 28. The limit \u0393(\u03b8) of the logarithmic moment generating function is\n(6.16)\n\u0393(\u03b8) = lim\nt\u2192\u221e\n1\nt log E[e\u03b8Nt] =\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f3\n\u03bd(f(\u03b8) \u22121)\nif \u03b8 \u2208(\u2212\u221e, \u03b8c]\n+\u221e\notherwise\n,\n177\nwhere f(\u03b8) is the minimal solution to x =\nR\nX e\u03b8+H(a)(x\u22121)q(da) and\n(6.17)\n\u03b8c = \u2212log\nZ\nX\nH(a)eH(a)(xc\u22121)q(da) > 0,\nwhere xc > 1 satis\ufb01es the equation x\nR\nX H(a)eH(a)(x\u22121)q(da) =\nR\nX eH(a)(x\u22121)q(da).\nWe will break the proof of Theorem 28 into the proof of the lower bound, i.e.\nLemma 34 and the proof of the upper bound, i.e. Lemma 35.\nBefore we proceed, let us \ufb01rst prove Lemma 33, which will be repeatedly used.\nLemma 33. Consider a linear marked Hawkes process with intensity\n(6.18)\n\u03bbt := \u03b1 + \u03b2Zt := \u03b1 + \u03b2\nX\n\u03c4i<t\nh(t \u2212\u03c4i, ai),\nand \u03b2Eq[H(a)] < 1, where the ai are i.i.d. random marks with the common law\nq(da) independent of the previous arrival times, then there exists a unique invariant\nmeasure \u03c0 for Zt such that\n(6.19)\nZ\n\u03bb(z)\u03c0(dz) =\n\u03b1\n1 \u2212\u03b2Eq[H(a)].\nProof. The ergodicity of Zt is well known.\nLet \u03c0 be the invariant probability\nmeasure for Zt. Then\n(6.20)\nZ\n\u03bb(z)\u03c0(dz) = \u03b1 + \u03b2\nZ\nX\nZ \u221e\n0\nh(t, a)dtq(da)\nZ\n\u03bb(z)\u03c0(dz).\n178\nLemma 34 (Lower Bound).\n(6.21)\nlim inf\nt\u2192\u221e\n1\nt log E[e\u03b8Nt] \u2265\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f3\n\u03bd(f(\u03b8) \u22121)\nif \u03b8 \u2208(\u2212\u221e, \u03b8c]\n+\u221e\notherwise\n,\nwhere f(\u03b8) is the minimal solution to x =\nR\ne\u03b8+H(a)(x\u22121)q(da) and \u03b8c is de\ufb01ned in\n(6.17).\nProof. The intensity at time t is \u03bbt := \u03bb(Zt) where \u03bb(z) = \u03bd + z and Zt =\nP\n\u03c4i<t h(t \u2212\u03c4i, ai). We tilt \u03bb to \u02c6\u03bb and q to \u02c6q such that by Girsanov formula the\ntilted probability measure \u02c6P is given by\n(6.22)\nd\u02c6P\ndP\n\f\f\f\f\nFt\n= exp\n(Z t\n0\n(\u03bb(Zs) \u2212\u02c6\u03bb(Zs))ds +\nZ t\n0\nlog\n \u02c6\u03bb(Zs)\n\u03bb(Zs)\n!\n+ log\n\u0012d\u02c6q\ndq\n\u0013\ndNs\n)\n.\nLet Qe be the set of (\u02c6\u03bb, \u02c6q, \u02c6\u03c0) such that the marked Hawkes process with intensity\n\u02c6\u03bb(Zt) and random marks distributed as \u02c6q is ergodic with \u02c6\u03c0 as the invariant measure\nof Zt.\nBy the ergodic theorem and Jensen\u2019s inequality, for any (\u02c6\u03bb, \u02c6q, \u02c6\u03c0) \u2208Qe,\nlim inf\nt\u2192\u221e\n1\nt log E[e\u03b8Nt]\n(6.23)\n\u2265lim inf\nt\u2192\u221e\n\u02c6E\n\u00141\nt \u03b8Nt \u22121\nt\nZ t\n0\n(\u03bb \u2212\u02c6\u03bb)ds \u22121\nt\nZ t\n0\nh\nlog(\u02c6\u03bb/\u03bb) + log(d\u02c6q/dq)\ni\n\u02c6\u03bbds\n\u0015\n=\nZ\n\u03b8\u02c6\u03bb\u02c6\u03c0(dz) +\nZ\n(\u02c6\u03bb \u2212\u03bb)\u02c6\u03c0(dz) \u2212\nZZ h\nlog(\u02c6\u03bb/\u03bb) + log(d\u02c6q/dq)\ni\n\u02c6\u03bb\u02c6q\u02c6\u03c0(dz).\n179\nHence,\nlim inf\nt\u2192\u221e\n1\nt log E[e\u03b8Nt]\n(6.24)\n\u2265\nsup\n(\u02c6\u03bb,\u02c6q,\u02c6\u03c0)\u2208Qe\n\u001aZ\n\u03b8\u02c6\u03bb\u02c6\u03c0 +\nZ\n(\u02c6\u03bb \u2212\u03bb)\u02c6\u03c0 \u2212\nZZ h\nlog(\u02c6\u03bb/\u03bb) + log(d\u02c6q/dq)\ni\n\u02c6\u03bb\u02c6q\u02c6\u03c0\n\u001b\n.\n\u2265\nsup\n(K\u03bb,\u02c6q,\u02c6\u03c0)\u2208Qe\nZ h\u0000\u03b8 \u2212E\u02c6q[log(d\u02c6q/dq)]\n\u0001 \u02c6\u03bb + \u02c6\u03bb \u2212\u03bb \u2212\u02c6\u03bb log\n\u0010\n\u02c6\u03bb/\u03bb\n\u0011i\n\u02c6\u03c0\n\u2265\nsup\n0<K<E\u02c6q[H(a)]\u22121,(K\u03bb,\u02c6q,\u02c6\u03c0)\u2208Qe\nZ \u0014\u0000\u03b8 \u2212E\u02c6q[log(d\u02c6q/dq)]\n\u0001\n+ 1 \u22121\nK \u2212log K\n\u0015\n\u02c6\u03bb\u02c6\u03c0\n= sup\n\u02c6q\nsup\n0<K<E\u02c6q[H(a)]\u22121\n\u0014\u0000\u03b8 \u2212E\u02c6q[log(d\u02c6q/dq)]\n\u0001\n+ 1 \u22121\nK \u2212log K\n\u0015\n\u00b7\nK\u03bd\n1 \u2212KE\u02c6q[H(a)],\nwhere the last equality is obtained by applying Lemma 33. The supremum of \u02c6q is\ntaken over M(X) such that \u02c6q is absolutely continuous w.r.t. q. Optimizing over\nK > 0, we get\nlim inf\nt\u2192\u221e\n1\nt log E[e\u03b8Nt]\n(6.25)\n\u2265\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f3\nsup\u02c6q \u03bd( \u02c6f(\u03b8) \u22121)\nif \u03b8 \u2208\n\u0000\u2212\u221e, E\u02c6q h\nlog d\u02c6q\ndq\ni\n+ E\u02c6q[H(a)] \u22121 \u2212log E\u02c6q[H(a)]\n\u0003\n+\u221e\notherwise\n,\nwhere \u02c6f(\u03b8) is the minimal solution to the equation\nx = e\u03b8+E\u02c6q[log(dq/d\u02c6q)]+E\u02c6q[H(a)](x\u22121)\n(6.26)\n\u2264E\u02c6q\n\u0014\ne\u03b8+H(a)(x\u22121)dq\nd\u02c6q\n\u0015\n=\nZ\ne\u03b8+H(a)(x\u22121)q(da).\n180\nThe last inequality is satis\ufb01ed by Jensen\u2019s inequality; the equality holds if and only\nif\n(6.27)\nd\u02c6q\ndq =\neH(a)(x\u22121)\nEq[eH(a)(x\u22121)].\nOptimizing over \u02c6q, we get\n(6.28)\nlim inf\nt\u2192\u221e\n1\nt log E[e\u03b8Nt] \u2265\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f3\n\u03bd(f(\u03b8) \u22121)\nif \u03b8 \u2208(\u2212\u221e, \u03b8c]\n+\u221e\notherwise,\nwhere \u03b8c is some critical value to be determined. Let\n(6.29)\nG(x) = e\u03b8\nZ\neH(a)(x\u22121)q(da) \u2212x.\nIf \u03b8 = 0, then G(x) =\nR\neH(a)(x\u22121)q(da) \u2212x satis\ufb01es G(1) = 0, G(\u221e) = \u221e(by\n(6.4)) and G\u2032(1) = Eq[H(a)] \u22121 < 0 which implies minx>1 G(x) < 0. Hence, there\nexists some critical \u03b8c > 0 such that minx>1 G(x) = 0. The critical values xc and\n\u03b8c satisfy G(xc) = G\u2032(xc) = 0, which implies\n(6.30)\n\u03b8c = \u2212log\nZ\nH(a)eH(a)(xc\u22121)q(da),\nwhere xc > 1 satis\ufb01es the equation x\nR\nH(a)eH(a)(x\u22121)q(da) =\nR\neH(a)(x\u22121)q(da).\nIt is easy to check that indeed, for dq\u2217=\neH(a)(x\u2217\u22121)\nEq[eH(a)(x\u2217\u22121)]dq,\n(6.31)\nEq\u2217\n\u0014\nlog dq\u2217\ndq\n\u0015\n+ Eq\u2217[H(a)] \u22121 \u2212log Eq\u2217[H(a)]) = \u2212log\nZ\nH(a)eH(a)(x\u2217\u22121)q(da).\n181\nLemma 35 (Upper Bound).\n(6.32)\nlim sup\nt\u2192\u221e\n1\nt log E[e\u03b8Nt] \u2264\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f3\n\u03bd(f(\u03b8) \u22121)\nif \u03b8 \u2208(\u2212\u221e, \u03b8c]\n+\u221e\notherwise\n,\nwhere f(\u03b8) is the minimal solution to x =\nR\ne\u03b8+H(a)(x\u22121)q(da) and \u03b8c is de\ufb01ned in\n(6.17).\nProof. It is well known that a linear Hawkes process has an immigration-birth\nrepresentation.\nThe immigrants (roots) arrive via a standard Poisson process\nwith constant intensity \u03bd > 0. Each immigrant generates children according to\na Galton-Watson tree. (See for example Hawkes and Oakes [54] and Karabash\n[63].) Consider a random, rooted tree (with root, i.e. immigrant, at time 0) as-\nsociated to the Hawkes process via the Galton-Watson interpretation. Note the\nroot is unmarked at the start of the process so the marking goes into the expec-\ntation calculation later. Let K be the number of children of the root node, and\nlet S(1)\nt , S(2)\nt , . . . , S(K)\nt\nbe the number of descendants of root\u2019s k-th child that were\nborn before time t (including k-th child if an only if it was born before time t).\nLet St be the total number of children in tree before time t including root node.\n182\nThen\nFS(t) := E[exp(\u03b8St)]\n(6.33)\n=\n\u221e\nX\nk=0\nE[exp(\u03b8St)|K = k]P(K = k)\n= exp(\u03b8)\n\u221e\nX\nk=0\nP(K = k)\nk\nY\ni=1\nE\nh\nexp\n\u0010\n\u03b8S(i)\nt\n\u0011i\n= exp(\u03b8)\n\u221e\nX\nk=0\nE\nh\nexp\n\u0010\n\u03b8S(1)\nt\n\u0011ik\nP(K = k)\n= exp(\u03b8)\n\u221e\nX\nk=0\nZ\nX\n\"\u0012Z t\n0\nh(s, a)\nH(a) FS(t \u2212s)ds\n\u0013k\ne\u2212H(a)H(a)k\nk!\n#\nq(da)\n=\nZ\nX\nexp\n\u0012\n\u03b8 +\nZ t\n0\nh(s, a)(FS(t \u2212s) \u22121)ds\n\u0013\nq(da).\nNow observe that FS(t) is strictly increasing and hence must approach to the\nsmaller solution x\u2217of the following equation\n(6.34)\nx =\nZ\nX\nexp [\u03b8 + H(a)(x \u22121)] q(da).\nFinally, since random roots arrive according to a Poisson process with constant\nintensity \u03bd > 0, we have\n(6.35)\nFN(t) := E[exp(\u03b8Nt)] = exp\n\u0014\n\u03bd\nZ t\n0\n(FS(t \u2212s) \u22121)ds\n\u0015\n.\nBut since FS(s) \u2191x\u2217as s \u2192\u221ewe obtain the main result\n(6.36)\n1\nt log FN(t) = \u03bd 1\nt\n\u0014Z t\n0\n(FS(s) \u22121) ds\n\u0015\n\u2212\u2192\nt\u2192\u221e\u03bd(x\u2217\u22121),\nwhich proves the desired formula. Note that x\u2217= \u221ewhen there is no solution to\n183\n(6.34). The proof is complete.\n6.3.2\nLarge Deviation Principle\nIn this section, we prove the main result, i.e. Theorem 26 by using the G\u00a8artner-\nEllis theorem for the upper bound and tilting method for the lower bound.\nProof of Theorem 26. For the upper bound, since we have Theorem 28, we can\nsimply apply G\u00a8artner-Ellis theorem. To prove the lower bound, it su\ufb03ces to show\nthat for any x > 0, \u03f5 > 0, we have\n(6.37)\nlim inf\nt\u2192\u221e\n1\nt log P\n\u0012Nt\nt \u2208B\u03f5(x)\n\u0013\n\u2265\u2212sup\n\u03b8\u2208R\n{\u03b8x \u2212\u0393(\u03b8)},\nwhere B\u03f5(x) denotes the open ball centered at x with radius \u03f5. Let \u02c6P denote the\ntilted probability measure with rate \u02c6\u03bb and marks distributed by \u02c6q(da) as de\ufb01ned\nin Lemma 34. By Jensen\u2019s inequality,\n1\nt log P\n\u0012Nt\nt \u2208B\u03f5(x)\n\u0013\n(6.38)\n\u22651\nt log\nZ\nNt\nt \u2208B\u03f5(x)\ndP\nd\u02c6P\nd\u02c6P\n= 1\nt log \u02c6P\n\u0012Nt\nt \u2208B\u03f5(x)\n\u0013\n\u22121\nt log\n\"\n1\n\u02c6P\n\u0000 Nt\nt \u2208B\u03f5(x)\n\u0001\nZ\nNt\nt \u2208B\u03f5(x)\nd\u02c6P\ndPd\u02c6P\n#\n\u22651\nt log \u02c6P\n\u0012Nt\nt \u2208B\u03f5(x)\n\u0013\n\u2212\n1\n\u02c6P\n\u0000 Nt\nt \u2208B\u03f5(x)\n\u0001 \u00b7 1\nt \u00b7 \u02c6E\n\"\n1 Nt\nt \u2208B\u03f5(x) log d\u02c6P\ndP\n#\n.\nBy the ergodic theorem,\n(6.39)\nlim inf\nt\u2192\u221e\n1\nt log P\n\u0012Nt\nt \u2208B\u03f5(x)\n\u0013\n\u2265\u2212\ninf\n0<K<E\u02c6q[H(a)]\u22121\n(K\u03bb,\u02c6q,\u02c6\u03c0)\u2208Qxe\nH(\u02c6\u03bb, \u02c6q, \u02c6\u03c0).\n184\nwhere Qx\ne is de\ufb01ned by\n(6.40)\nQx\ne =\n\u001a\n(\u02c6\u03bb, \u02c6q, \u02c6\u03c0) \u2208Qe :\nZ\n\u02c6\u03bb(z)\u02c6\u03c0(dz) = x\n\u001b\n.\nand the relative entropy H is\n(6.41)\nH(\u02c6\u03bb, \u02c6q, \u02c6\u03c0) =\nZ\n(\u03bb \u2212\u02c6\u03bb)\u02c6\u03c0 +\nZ\nlog(\u02c6\u03bb/\u03bb)\u02c6\u03bb\u02c6\u03c0 +\nZZ\nlog(d\u02c6q/dq)\u02c6q\u02c6\u03bb\u02c6\u03c0.\nBy Lemma 33,\ninf\n0<K<E\u02c6q[H(a)]\u22121,x=\n\u03bdK\n1\u2212KE\u02c6q[H(a)] ,(K\u03bb,\u02c6q,\u02c6\u03c0)\u2208Qe\nH(\u02c6\u03bb, \u02c6q, \u02c6\u03c0)\n(6.42)\n=\ninf\nK=\nx\nxE\u02c6q[H(a)]+\u03bd ,(K\u03bb,\u02c6q,\u02c6\u03c0)\u2208Qe\n\u001a 1\nK \u22121 + log K + E\u02c6q\n\u0014\nlog d\u02c6q\ndq\n\u0015\u001b Z\n\u02c6\u03bb\u02c6\u03c0\n= inf\n\u02c6q\n\u001a\nE\u02c6q[H(a)] + \u03bd\nx \u22121 + log\n\u0012\nx\nxE\u02c6q[H(a)] + \u03bd\n\u0013\n+ E\u02c6q\n\u0014\nlog d\u02c6q\ndq\n\u0015\u001b\nx\n= inf\n\u02c6q\n\u001a\nxE\u02c6q[H(a)] + \u03bd \u2212x + x log\n\u0012\nx\nxE\u02c6q[H(a)] + \u03bd\n\u0013\n+ xE\u02c6q\n\u0014\nlog d\u02c6q\ndq\n\u0015\u001b\n.\nNext, let us \ufb01nd a more explict form for the Legendre-Fenchel transform of \u0393(\u03b8).\n(6.43)\nsup\n\u03b8\u2208R\n{\u03b8x \u2212\u0393(\u03b8)} = sup\n\u03b8\u2208R\n{\u03b8x \u2212\u03bd(f(\u03b8) \u22121)},\nwhere f(\u03b8) = Eq[e\u03b8+(f(\u03b8)\u22121)H(a)]. Here,\n(6.44)\nf \u2032(\u03b8) = Eq \u0002\n(1 + f \u2032(\u03b8)H(a))e\u03b8+(f(\u03b8)\u22121)H(a)\u0003\n.\nSo the optimal \u03b8\u2217for (6.43) would satisfy f \u2032(\u03b8\u2217) = x\n\u03bd and \u03b8\u2217and x\u2217= f(\u03b8\u2217) satisfy\n185\nthe following equations\n(6.45)\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f3\nx\u2217= Eq \u0002\ne\u03b8\u2217+(x\u2217\u22121)H(a)\u0003\nx\n\u03bd = x\u2217+ x\n\u03bdEq \u0002\nH(a)e\u03b8\u2217+(x\u2217\u22121)H(a)\u0003\n,\nand sup\u03b8\u2208R{\u03b8x \u2212\u0393(\u03b8)} = \u03b8\u2217x \u2212\u03bd(x\u2217\u22121).\nOn the other hand, letting dq\u2217=\ne(x\u2217\u22121)H(a)\nEq[e(x\u2217\u22121)H(a)]dq, we have\n(6.46)\nEq\u2217[H(a)] = Eq \u0002\ne\u03b8\u2217+(x\u2217\u22121)H(a)\u0003\nEq [e(x\u2217\u22121)H(a)]\n= 1\nx\u2217\n\u2212\u03bd\nx,\nand Eq\u2217[log dq\u2217\ndq ] = (x\u2217\u22121)Eq\u2217[H(a)] \u2212log Eq[e(x\u2217\u22121)H(a)], which imply\nlim inf\nt\u2192\u221e\n1\nt log P\n\u0012Nt\nt \u2208B\u03f5(x)\n\u0013\n(6.47)\n\u2265\u2212inf\n\u02c6q\n\u001a\nxE\u02c6q[H(a)] + \u03bd \u2212x + x log\n\u0012\nx\nxE\u02c6q[H(a)] + \u03bd\n\u0013\n+ xE\u02c6q\n\u0014\nlog d\u02c6q\ndq\n\u0015\u001b\n\u2265\u2212\n\u001a\nxEq\u2217[H(a)] + \u03bd \u2212x + x log\n\u0012\nx\nxEq\u2217[H(a)] + \u03bd\n\u0013\n+ xEq\u2217\n\u0014\nlog dq\u2217\ndq\n\u0015\u001b\n= \u03b8\u2217x \u2212\u03bd(x\u2217\u22121) = sup\n\u03b8\u2208R\n{\u03b8x \u2212\u0393(\u03b8)}.\n186\n6.4\nRisk Model with Marked Hawkes Claims Ar-\nrivals\nWe consider the following risk model for the surplus process Rt of an insurance\nportfolio,\n(6.48)\nRt = u + \u03c1t \u2212\nNt\nX\ni=1\nCi,\nwhere u > 0 is the initial reserve, \u03c1 > 0 is the constant premium and the Ci\u2019s\nare i.i.d.\npositive random variables with the common distribution \u00b5(dC).\nCi\nrepresents the claim size at the ith arrival time, these being independent of Nt, a\nmarked Hawkes process.\nFor u > 0, let\n(6.49)\n\u03c4u = inf{t > 0 : Rt \u22640},\nand denote the in\ufb01nite and \ufb01nite horizon ruin probabilities by\n(6.50)\n\u03c8(u) = P(\u03c4u < \u221e),\n\u03c8(u, uz) = P(\u03c4u \u2264uz),\nu, z > 0.\nBy the law of large numbers,\n(6.51)\nlim\nt\u2192\u221e\n1\nt\nNt\nX\ni=1\nCi =\nE\u00b5[C]\u03bd\n1 \u2212Eq[H(a)].\n187\nTherefore, to exclude the trivial case, we need to assume that\n(6.52)\nE\u00b5[C]\u03bd\n1 \u2212Eq[H(a)] < \u03c1 < \u03bd(xc) \u22121\n\u03b8c\n,\nwhere the critical values \u03b8c and xc = f(\u03b8c) satisfy\n(6.53)\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f3\nxc =\nR\nR+\nR\nX e\u03b8cC+H(a)(xc\u22121)q(da)\u00b5(dC)\n1 =\nR\nR+\nR\nX H(a)eH(a)(xc\u22121)+\u03b8cCq(da)\u00b5(dC)\n.\nLet us \ufb01rst assume that the claim sizes following light tails, i.e. there exists\nsome \u03b8 > 0 such that\nR\nR+ e\u03b8C\u00b5(dC) < \u221e.\nFollowing the proofs of large deviation results in Section 6.3, we have\n(6.54)\n\u0393C(\u03b8) := lim\nt\u2192\u221e\n1\nt log E\nh\ne\u03b8 PNt\ni=1 Cii\n=\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f3\n\u03bd(x \u22121)\nif \u03b8 \u2208(\u2212\u221e, \u03b8c]\n+\u221e\notherwise\n,\nwhere x is the minimal solution to the equation\n(6.55)\nx =\nZ\nR+\nZ\nX\ne\u03b8C+(x\u22121)H(a)q(da)\u00b5(dC).\nBefore we proceed, let us quote a result from Glynn and Whitt [43], which will\nbe used in our proof Theorem 30.\nTheorem 29 (Glynn and Whitt [43]). Let Sn be random variables. \u03c4u = inf{n :\nSn > u} and \u03c8(u) = P(\u03c4u < \u221e). Assume that there exist \u03b3, \u03f5 > 0 such that\n(i) \u03ban(\u03b8) = log E[e\u03b8Sn] is well de\ufb01ned and \ufb01nite for \u03b3 \u2212\u03f5 < \u03b8 < \u03b3 + \u03f5.\n(ii) lim supn\u2192\u221eE[e\u03b8(Sn\u2212Sn\u22121)] < \u221efor \u2212\u03f5 < \u03b8 < \u03f5.\n188\n(iii) \u03ba(\u03b8) = limn\u2192\u221e\n1\nn\u03ban(\u03b8) exists and is \ufb01nite for \u03b3 \u2212\u03f5 < \u03b8 < \u03b3 + \u03f5.\n(iv) \u03ba(\u03b3) = 0 and \u03ba is di\ufb00erentiable at \u03b3 with 0 < \u03ba\u2032(\u03b3) < \u221e.\nThen, limu\u2192\u221e\n1\nu log \u03c8(u) = \u2212\u03b3.\nRemark 15. We claim that \u0393C(\u03b8) = \u03c1\u03b8 has a unique positive solution \u03b8\u2020 < \u03b8c.\nLet G(\u03b8) = \u0393C(\u03b8) \u2212\u03c1\u03b8. Notice that G(0) = 0, G(\u221e) = \u221e, and that G is convex.\nWe also have G\u2032(0) =\nE\u00b5[C]\u03bd\n1\u2212Eq[H(a)] \u2212\u03c1 < 0 and \u0393C(\u03b8c)\u2212\u03c1\u03b8c > 0 since we assume that\n\u03c1 < \u03bd(f(\u03b8c)\u22121)\n\u03b8c\n. Therefore, there exists only one solution \u03b8\u2020 \u2208(0, \u03b8c) of \u0393C(\u03b8\u2020) = \u03c1\u03b8\u2020.\nTheorem 30 (In\ufb01nite Horizon). Assume all the assumptions in Theorem 26 and\nin addition (6.52), we have limu\u2192\u221e\n1\nu log \u03c8(u) = \u2212\u03b8\u2020, where \u03b8\u2020 \u2208(0, \u03b8c) is the\nunique positive solution of \u0393C(\u03b8) = \u03c1\u03b8.\nProof. Take St = PNt\ni=1 Ci \u2212\u03c1t and \u03bat(\u03b8) = log E[e\u03b8St]. Then limt\u2192\u221e\n1\nt\u03bat(\u03b8) =\n\u0393C(\u03b8)\u2212\u03c1\u03b8. Consider {Snh}n\u2208N. We have limn\u2192\u221e\n1\nn\u03banh(\u03b8) = h\u0393C(\u03b8)\u2212h\u03c1\u03b8. Check-\ning the conditions in Theorem 29 and applying it, we get\n(6.56)\nlim\nu\u2192\u221e\n1\nu log P\n\u0012\nsup\nn\u2208N\nSnh > u\n\u0013\n= \u2212\u03b8\u2020.\nFinally, notice that\n(6.57)\nsup\nt\u2208R+ St \u2265sup\nn\u2208N\nSnh \u2265sup\nt\u2208R+ St \u2212\u03c1h.\nHence, limu\u2192\u221e\n1\nu log \u03c8(u) = \u2212\u03b8\u2020.\nTheorem 31 (Finite Horizon). Under the same assumptions as in Theorem 30,\nwe have\n(6.58)\nlim\nu\u2192\u221e\n1\nu log \u03c8(u, uz) = \u2212w(z),\nfor any z > 0.\n189\nHere\n(6.59)\nw(z) =\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f3\nz\u039bC\n\u0000 1\nz + \u03c1\n\u0001\nif 0 < z <\n1\n\u0393\u2032(\u03b8\u2020)\u2212\u03c1\n\u03b8\u2020\nif z \u2265\n1\n\u0393\u2032(\u03b8\u2020)\u2212\u03c1\n,\n\u039bC(x) = sup\u03b8\u2208R{\u03b8x \u2212\u0393C(\u03b8)} and \u03b8\u2020 \u2208(0, \u03b8c) is the unique positive solution of\n\u0393C(\u03b8) = \u03c1\u03b8, as before.\nProof. The proof is similar to that in Stabile and Torrisi [102] and we omit it\nhere.\nNext, we are interested to study the case when the claim sizes have heavy tails,\ni.e.\nR\nR+ e\u03b8C\u00b5(dC) = +\u221efor any \u03b8 > 0.\nA distribution function B is subexponential, i.e. B \u2208S if\n(6.60)\nlim\nx\u2192\u221e\nP(C1 + C2 > x)\nP(C1 > x)\n= 2,\nwhere C1, C2 are i.i.d. random variables with distribution function B. Let us\ndenote B(x) := P(C1 \u2265x) and let us assume that E[C1] < \u221eand de\ufb01ne B0(x) :=\n1\nE[C]\nR x\n0 B(y)dy, where F(x) = 1 \u2212F(x) is the complement of any distribution\nfunction F(x).\nGoldie and Resnick [44] showed that if B \u2208S and satis\ufb01es some smoothness\nconditions, then B belongs to the maximum domain of attraction of either the\nFrechet distribution or the Gumbel distribution. In the former case, B is regularly\nvarying, i.e. B(x) = L(x)/x\u03b1+1, for some \u03b1 > 0 and we write it as B \u2208R(\u2212\u03b1\u22121),\n\u03b1 > 0.\nWe assume that B0 \u2208S and either B \u2208R(\u2212\u03b1\u22121) or B \u2208G, i.e. the maximum\n190\ndomain of attraction of Gumbel distribution. G includes Weibull and lognormal\ndistributions.\nWhen the arrival process Nt satis\ufb01es a large deviation result, the probability\nthat it deviates away from its mean is exponentially small, which is dominated\nby subexonential distributions. By using the techniques for the asymptotics of\nruin probabilities for risk processes with non-stationary, non-renewal arrivals and\nsubexponential claims from Zhu [118], we have the following in\ufb01nite-horizon and\n\ufb01nite-horizon ruin probability estimates when the claim sizes are subexponential.\nTheorem 32. Assume the net pro\ufb01t condition \u03c1 > E[C1]\n\u03bd\n1\u2212Eq[H(a)].\n(i) (In\ufb01nite-Horizon)\n(6.61)\nlim\nu\u2192\u221e\n\u03c8(u)\nB0(u) =\n\u03bdE[C1]\n\u03c1(1 \u2212Eq[H(a)]) \u2212\u03bdE[C1].\n(ii) (Finite-Horizon) For any T > 0,\nlim\nu\u2192\u221e\n\u03c8(u, uz)\nB0(u)\n(6.62)\n=\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n\u03bdE[C1]\n\u03c1(1\u2212Eq[H(a)])\u2212\u03bdE[C1]\n\u0014\n1 \u2212\n\u0010\n1 +\n\u0010\n\u03c1(1\u2212Eq[H(a)])\u2212\u03bdE[C1]\n\u03c1(1\u2212Eq[H(a)])\n\u0011\nT\n\u03b1\n\u0011\u2212\u03b1\u0015\nif B \u2208R(\u2212\u03b1 \u22121)\n\u03bdE[C1]\n\u03c1(1\u2212Eq[H(a)])\u2212\u03bdE[C1]\n\u0014\n1 \u2212e\u2212\u03c1(1\u2212Eq[H(a)])\u2212\u03bdE[C1]\n\u03c1(1\u2212Eq[H(a)])\nT\n\u0015\nif B \u2208G\n.\n6.5\nExamples with Explicit Formulas\nIn this section, we discuss two examples where an explicit formula exists.\nExample 1 is about the exponential asymptotics of the in\ufb01nite-horizon ruin\n191\nprobability when H(a) and the claim size C are exponentially distributed. Example\n2 gives an explicit expression for the rate function of the large deviation principle\nwhen H(a) is exponentially distributed.\nExample 1. Recall that x is the minimal solution of\n(6.63)\nx =\nZ\nR+\nZ\nX\ne\u03b8C+(x\u22121)H(a)q(da)\u00b5(dC).\nNow, assume that H(a) is exponentially distributed with parameter \u03bb > 0, then,\nwe have\n(6.64)\nx = E\u00b5[e\u03b8C]\n\u03bb\n\u03bb \u2212(x \u22121),\nwhich implies that\n(6.65)\nx = 1\n2\nn\n\u03bb + 1 \u2212\np\n(\u03bb + 1)2 \u22124\u03bbE\u00b5[e\u03b8C]\no\n.\nNow, assume that C is exponentially distributed with parameter \u03b3 > 0. Then,\n(6.66)\nx = 1\n2\n\u001a\n\u03bb + 1 \u2212\nr\n(\u03bb + 1)2 \u22124\u03bb\n\u03b3\n\u03b3 \u2212\u03b8\n\u001b\n.\nThe in\ufb01nite horizon probability satis\ufb01es limu\u2192\u221e\n1\nu log \u03c8(u) = \u2212\u03b8\u2020, where \u03b8\u2020 satis-\n\ufb01es\n(6.67)\n\u03c1\u03b8\u2020 = \u03bd\n\u00121\n2\n\u001a\n\u03bb + 1 \u2212\nr\n(\u03bb + 1)2 \u22124\u03bb\n\u03b3\n\u03b3 \u2212\u03b8\u2020\n\u001b\n\u22121\n\u0013\n,\n192\nwhich implies\n(6.68)\n2\u03c1\u03b8\u2020\n\u03bd\n+ 1 \u2212\u03bb = \u2212\ns\n(\u03bb + 1)2 \u2212\n4\u03bb\u03b3\n\u03b3 \u2212\u03b8\u2020,\nand thus\n(6.69)\n\u03c12\n\u03bd2(\u03b8\u2020)2 + \u03c1\u03b8\u2020\n\u03bd (1 \u2212\u03bb) = \u03bb \u2212\n\u03bb\u03b3\n\u03b3 \u2212\u03b8\u2020 = \u2212\u03bb\u03b8\u2020\n\u03b3 \u2212\u03b8\u2020.\nSince we are looking for positive \u03b8\u2020, we get the quadratic equation,\n(6.70)\n\u03c12(\u03b8\u2020)2 \u2212(\u03c12\u03b3 \u2212\u03c1\u03bd(1 \u2212\u03bb))\u03b8\u2020 \u2212(\u03c1\u03bd\u03b3(1 \u2212\u03bb) + \u03bb\u03bd2) = 0.\nSince \u03c1 >\nE\u00b5[C]\u03bd\n1\u2212Eq[H(a)] =\n\u03bd\u03bb\n\u03b3(\u03bb\u22121), we have \u03c1\u03bd\u03b3(1 \u2212\u03bb) + \u03bb\u03bd2 > 0. Therefore,\n(6.71) \u03b8\u2020 = (\u03c12\u03b3 \u2212\u03c1\u03bd(1 \u2212\u03bb)) +\np\n(\u03c12\u03b3 \u2212\u03c1\u03bd(1 \u2212\u03bb))2 + 4\u03c12(\u03c1\u03bd\u03b3(1 \u2212\u03bb) + \u03bb\u03bd2)\n2\u03c12\n.\nExample 2. Now, let H(a) be exponentially distributed with parameter \u03bb > 0. We\nwant an explicit expression for the rate function of the large deviation principle for\n(Nt/t \u2208\u00b7). Notice that,\n(6.72)\n\u0393(\u03b8) =\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f3\n\u03bd\n\u0010\n1\n2\nn\n\u03bb + 1 \u2212\np\n(\u03bb + 1)2 \u22124\u03bbe\u03b8\no\n\u22121\n\u0011\nfor \u03b8 \u2264log\n\u0010\n(\u03bb+1)2\n4\u03bb\n\u0011\n+\u221e\notherwise\n.\nTo get I(x) = sup\u03b8\u2208R{\u03b8x \u2212\u0393(\u03b8)}, we optimize over \u03b8 and consider x = \u0393\u2032(\u03b8).\n193\nEvidently,\n(6.73)\nx + 1\n2\u03bd(\u22124\u03bb)e\u03b8\n1\n2\np\n(\u03bb + 1)2 \u22124\u03bbe\u03b8 = 0,\nwhich gives us\n(6.74)\n\u03b8 = log\n \n\u22122x2 + x\np\n4x2 + \u03bd2(\u03bb + 1)2\n\u03bb\u03bd2\n!\n,\nwhence,\n(6.75)\nI(x) =\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\nx log\n\u0012\n\u22122x2+x\u221a\n4x2+\u03bd2(\u03bb+1)2\n\u03bb\u03bd2\n\u0013\n\u2212\u03bd\n\u0012\n1\n2\n\u001a\n\u03bb + 1 \u2212\n\u22122x+\u221a\n4x2+\u03bd2(\u03bb+1)2\n\u03bd\n\u001b\n\u22121\n\u0013\nif x \u22650\n+\u221e\notherwise\n.\n194\nBibliography\n[1] Bacry, E., Dayri, K. and J. F. Muzy. (2012). Non-parametric kernel estima-\ntion for symmetric Hawkes processes. Applications to high frequency \ufb01nancial\ndata. Eur. Phys. J. B 85:157\n[2] Bacry, E., Delattre, S., Ho\ufb00mann, M. and J. F. Muzy. Scaling lim-\nits for Hawkes processes and application to \ufb01nancial statistics. Preprint.\narXiv:1202.0842.\n[3] Bacry, E., Delattre, S., Ho\ufb00mann, M. and J. F. Muzy. Modeling microstruc-\nture noise with mutually exciting point processes. To appear in Quantitative\nFinance. arXiv:1101.3422.\n[4] Bacry, E. and J. F. Muzy. Hawkes model for price and trades high-frequency\ndynamics. Preprint. arXiv:1301.1135.\n[5] Bartlett, M. S. (1963). Statistical estimation of density functions. Sankhy\u00afa\nA. 25, 245-254.\n[6] Bartlett, M. S. (1963). The spectral analysis of point processes. J. R. Statist.\nSoc. B 25, 264-296.\n195\n[7] Bauwens, L. and N. Hautsch. Modelling \ufb01nancial high frequency data using\npoint processes. Handbook of Financial Time Series. 953-979, 2009.\n[8] Billingsley, P. Convergence of Probability Measures, 2nd edition. Wiley-\nInterscience, New York, 1999.\n[9] Bingham, N. H., Goldie, C. M. and J. L. Teugels, Regular Variation, Cam-\nbridge University Press, 1989\n[10] Blundell, C., Heller, K. A. and J. M. Beck. Modelling reciprocating relation-\nships with Hawkes processes. Preprint, 2012.\n[11] Bordenave, C. and G. L. Torrisi. (2007). Large deviations of Poisson cluster\nprocesses. Stochastic Models. 23, 593-625.\n[12] Bormetti, G., Calcagnile, L. M., Treccani, M., Corsi, F., Marmi, S. and\nF. Lillo. Modelling systemic cojumps with Hawkes factor models. Preprint.\narXiv:1301.6141.\n[13] Bowsher, C. G. (2007). Modelling security market events in continuous time:\nintensity based, multivariate point process models. Journal of Econometrics.\n141, 876-912.\n[14] Br\u00b4emaud, P. and L. Massouli\u00b4e. (1996). Stability of nonlinear Hawkes pro-\ncesses. Ann. Probab.. 24, 1563-1588.\n[15] Br\u00b4emaud, P., Nappo, G. and G. L. Torrisi. (2002). Rate of convergence to\nequilibrium of marked Hawkes processes. J. Appl. Prob. 39, 123-136.\n[16] Br\u00b4emaud, P. and L. Massouli\u00b4e. (2001). Hawkes branching point processes\nwithout ancestors. J. Appl. Prob. 38, 122-135.\n196\n[17] Br\u00b4emaud, P. and S. Foss. (2010). Ergodicity of a stress release point process\nseismic model with aftershocks. Markov Processes Relat. Fields. 16, 389-408.\n[18] Br\u00b4emaud, P. and L. Massouli\u00b4e. (2002). Power spectra of general shot noises\nand Hawkes point processes with a random excitation. Advances in Applied\nProbability. 34, 205-222.\n[19] Br\u00b4emaud, P., L. Massouli\u00b4e and A. Ridol\ufb01. (2005). Power spectra of random\nspike \ufb01elds and related processes. Advances in Applied Probability. 37, 1116-\n1146.\n[20] Brix, A. and W. S. Kendall. (2002). Simulation of cluster point processes\nwithout edge e\ufb00ects. Advances in Applied Probability. 34, 267-280.\n[21] Carstensen, L., Sandelin, A., Winther, O. and N. R. Hansen. (2010). Mul-\ntivariate Hawkes process models of the occurrence of regulatory elements.\nBMC Bioinformatics. 11:456.\n[22] Cartea, \u00b4A., Jaimungal, S. and J. Ricci. Buy low sell high: a high frequency\ntrading perspective. SSRN eLibrary, 2011.\n[23] Chavez-Demoulin, V., Davison, A. C. and A. J. McNeil. (2005). Estimating\nvalue-at-risk: a point process approach. Quantitative Finance. 5, 227-234.\n[24] Chavez-Demoulin, V. and J. A. McGill. (2012). High-frequency \ufb01nancial data\nmodeling using Hawkes processes. Journal of Banking & Finance. 36, 3415-\n3426.\n197\n[25] Chornoboy, E. S., Schramm, L. P. and A. F. Karr. (1988). Maximum likeli-\nhood identi\ufb01cation of neural point process systems. Biol. Cybern. 59, 265-\n275.\n[26] Crane, R. and D. Sornette. (2008). Robust dynamic classes revealed by mea-\nsuring the response function of a social system. Proc. Nat. Acad. Sci. USA\n105, 15649.\n[27] Daley, D. J. and D. Vere-Jones. An Introduction to the Theory of Point\nProcesses. Volume I and II, 2nd edition. Springer-Verlag, New York, 2003.\n[28] Dassios, A. and H. Zhao. (2011). A dynamic contagion process. Advances in\nApplied Probability. 43, 814-846.\n[29] Dassios, A. and H. Zhao. (2012). Ruin by dynamic contagion claims. Insur-\nance: Mathematics and Economics. 51, 93-106.\n[30] Dembo, A. and O. Zeitouni. Large Deviations Techniques and Applications,\n2nd edition. Springer, New York, 1998.\n[31] Donsker, M. D. and S. R. S. Varadhan. (1983). Asymptotic evaluation of\ncertain Markov process expectations for large time. IV. Communications of\nPure and Applied Mathematics. 36, 183-212.\n[32] Echeverr\u00b4\u0131a, P. (1982). A criterion for invariant measures of Markov processes.\nProbability Theory and Related Fields. 61, 1-16.\n[33] Egami, M., Kato, Y. and T. Sawaki. An analysis of CDS market liquidity by\nthe Hawkes process. SSRN eLibrary, 2013.\n198\n[34] Egesdal, M., Fathauer, C., Louie, K. and J. Neuman. Statistical and stochas-\ntic modelling of gang rivalries in Los Angeles. SIAM Undergraduate Research\nOnline. 2010.\n[35] Embrechts, P., Liniger, T. and L. Lin. (2011). Multivariate Hawkes processes:\nan application to \ufb01nancial data. J. Appl. Prob. Spec. Vol. 48A, 367-378.\n[36] Errais, E., Giesecke, K. and L. Goldberg. (2010). A\ufb03ne point processes and\nportfolio credit risk. SIAM J. Financial Math. 1, 642-665.\n[37] Fan, K. (1953). Minimax theorems. Proc. Natl. Acad. Sci. USA. 39, 42-47.\n[38] Feller, W., An Introduction to Probability Theory and Its Applications, Vol-\nume I and Volume II, 2nd edition, New York, 1971.\n[39] Filimonov, V. and D. Sornette. (2012). Quantifying re\ufb02exivity in \ufb01nancial\nmarkets: Toward a prediction of \ufb02ash crashes. Physical Review E 85 056108\n[40] Frenk, J. B. G. and G. Kassay. The Level Set Method of Jo\u00b4o and Its Use\nin Minimax Theory. Technical Report E.I 2003-03, Econometric Institute,\nErasmus University, Rotterdam, 2003.\n[41] Giesecke, K. and L. Goldberg, L. and X. Ding (2011). A top-down approach\nto multi-name credit. Operations Research. 59, 283-300.\n[42] Giesecke, K. and P. Tomecek. Dependent events and changes of time. Work-\ning paper, Cornell University, July 2005.\n[43] Glynn, P. W. and W. Whitt. (1994). Logarithmic asymptotics for steady-\nstate tail probabilities in a single-server queue. J. Appl. Probab. 31, 131-156.\n199\n[44] Goldie, C. M. and S. Resnick. (1988). Distributions that are both subex-\nponential and in the domain of attraction of an extreme value distribution.\nAdv. Appl. Probab. 20, 706-718.\n[45] Grandell, J. (1977) Point processes and random measures. Advances in Ap-\nplied Probability. 9, 502-526.\n[46] Gusto, G. and S. Schbath. (2005). F.A.D.O.: a statistical method to detect\nfavored or avoided distances between occurrences of motifs using the Hawkes\nmodel. Stat. Appl. Genet. Mol. Biol., 4, Article 24.\n[47] Hairer, M. Convergence of Markov Processes, Lecture Notes, University of\nWarwick, available at http://www.hairer.org/notes/Convergence.pdf,\n2010.\n[48] Halpin, P. F. and P. De Boeck. Modelling dyadic interaction with Hawkes\nprocess. Preprint, 2012. To appear in Psychometrika.\n[49] Hansen, N. R., Reynaud-Bouret, P. and V. Rivoirard. Lasso and probabilistic\ninequalities for multivariate point processes. Preprint. arXiv:1208.0570.\n[50] Hardiman, S. J., Bercot, N. and J-P. Bouchaud. Critical re\ufb02exivity in \ufb01nan-\ncial markets: a Hawkes process analysis. Preprint. arXiv:1302.1405.\n[51] Hawkes, A. G. (1971). Spectra of some self-exciting and mutually exciting\npoint processes. Biometrika. 58, 83-90.\n[52] Hawkes, A. G. (1971). Point spectra of some mutually exciting point pro-\ncesses. J. Roy. Statist. Soc. Ser. B 33, 438-443.\n200\n[53] Hawkes, A. G. and L. Adamopoulos. (1973). Cluster models for earthquakes-\nregional comparisons. Bull. Int. Statist. Inst. 45, 454-461.\n[54] Hawkes, A. G. and D. Oakes. (1974). A cluster process representation of a\nself-exciting process. J. Appl. Prob. 11, 93-503.\n[55] Hegemann, R. A., Lewis, E. A. and A. L. Bertozzi. (2013). An \u201cEstimate &\nScore Algorithm\u201d for simultaneous parameter estimation and reconstruction\nof incomplete data on social networks. Security Informatics. 2:1.\n[56] Hewlett, P. Clustering of order arrivals, price impact and trade path op-\ntimisation. Workshop on Financial Modeling with Jump processes, \u00b4Ecole\nPolytechnique, 2006.\n[57] Heyde, C. C. and Scott, D. J. (1973). Invariance principle for the law of the\niterated logarithm for martingales and processes with stationary increments.\nAnn. Probab. 1, 428-436.\n[58] Jagers, P. Branching Processes with Biological Applications. John Wiley, Lon-\ndon, 1975.\n[59] Johnson, D. H. (1996). Point process models of single-neuron discharges. J.\nComputational Neuroscience. 3, 275-299.\n[60] Jo\u00b4o, I. (1984). Note on my paper \u201cA simple proof for von Neumann\u2019s minmax\ntheorem\u201d. Acta. Math. Hung. 44, 363-365.\n[61] Kallenberg, O. Foundations of Modern Probability, Springer, 2nd edition,\n2002.\n201\n[62] Karabash, D. and L. Zhu. Limit theorems for marked Hawkes processes with\napplication to a risk model. Preprint. arXiv:1211.4039.\n[63] Karabash, D. On stability of Hawkes process. Preprint. arXiv:1201.1573.\n[64] Koralov, L. B. and Ya. G. Sinai. Theory of Probability and Random Processes.\nSpringer, 2nd edition, 2012.\n[65] Krumin, M., Reutsky I. and S. Shoham. (2010). Correlation-based analysis\nand generation of multiple spike trains using Hawkes models with an exoge-\nnous input. Frontiers in Computational Neuroscience. 4, article 147.\n[66] Kwieci\u00b4nski, A. and R. Szekli. (1996). Some monotonicity and dependence\nproperties of self-exciting point processes. Annals of Applied Probability. 6,\n1211-1231.\n[67] Large, J. (2007). Measuring the resiliency of an electronic limit order book.\nJournal of Financial Markets. 10, 1-25.\n[68] Lewis, E. and G. Mohler. A nonparametric EM algorithm for multiscale\nHawkes processes. Preprint, 2011.\n[69] Lewis, E., Mohler, Brantingham, P. J. and A. Bertozzi. (2011). Self-exciting\npoint process of insergency in Iraq. Security Journal. 25, 0955-1662.\n[70] Lewis, P. A. W. and G. S. Shedler. (1979). Simulation of nonhomogeneous\nPoisson processes by thinning. Naval Research Logistics Quarterly. 26, 403-\n413.\n[71] Liniger, T. Multivariate Hawkes Processes. PhD thesis, ETH, 2009.\n202\n[72] Lipster, R. S. and A. N. Shiryaev. Statistics of Random Processes II. Appli-\ncations, 2nd edition. Springer, 2001.\n[73] Marsan, D. and O. Lengline. (2008). Extending earthquakes\u2019 reach through\ncascading. Science. 319 (5866), 1076.\n[74] Massouli\u00b4e, L. (1998). Stability results for a general class of interacting point\nprocesses dynamics, and applications. Stochastic Processes and their Appli-\ncations. 75, 1-30.\n[75] Mehrdad, B., Sen, S. and L. Zhu. The speed of a biased walk on a Galton-\nWatson tree is monotonic with respect to progeny distributions for high\nvalues of bias. Preprint. arXiv:1212.3004.\n[76] Meyer, S., Elias J. and M. H\u00a8ohle. (2012). A space-time conditional intensity\nmodel for invasive meningococcal disease occurence. Biometrics. 68, 607-616.\n[77] Mitchell, L. and M. E. Cates. (2010). Hawkes process as a model of social\ninteractions: a view on video dynamics. Journal of Physics A: Mathematical\nand Theoretical. 43, 045101\n[78] Mohler, G. O., Short, M. B., Brantingham, P. J., Schoenberg F. P. and G. E.\nTita. (2011). Self-exciting point process modelling of crime. Journal of the\nAmerican Statistical Association. 106, 100-108.\n[79] M\u00f8ller, J. and J. G. Rasmussen. (2005). Perfect simulation of Hawkes pro-\ncesses. Advances in Applied Probability. 37, 629-646.\n[80] M\u00f8ller, J. and J. G. Rasmussen. (2006). Approximate simulation of Hawkes\nprocesses. Methodology and Computing in Applied Probability 8, 53-64.\n203\n[81] M\u00f8ller, J. and G. L. Torrisi. (2005). Second order analysis for spatial Hawkes\nprocesses. Technical Report R-2005-20, Department of Mathematical Sci-\nences, Aalborg University.\n[82] M\u00f8ller, J. and G. L. Torrisi. (2007). The pair correlation function of spatial\nHawkes processes. Statistics & Probability Letters. 77, 995-1003.\n[83] Muni Toke, I. and F. Pomponio. Modelling trades-through in a limited order\nbook using Hawkes processes. SSRN eLibrary, 2011.\n[84] Musmeci, F. and D. Vere-Jones. (1992). A space-time clustering model for\nhistorical earthquakes. Annals of the Institute of Statistical Mathematics. 44,\n1-11.\n[85] Oakes, D. (1975). The Markovian self-exciting process. J. Appl. Prob. 12,\n69-77.\n[86] Ogata, Y. (1978). The asymptotic behavior of maximum likelihood estimates\nfor stationary point processes. Ann. Inst. Statist. Math. 30, 243-261.\n[87] Ogata, Y. (1988). Statistical models for earthquake occurrences and residual\nanalysis for point processes. J. Amer. Statist. Assoc. 83, 9-27.\n[88] Ogata, Y. (1998). Space-time point-process models for earthquake occur-\nrences. Ann. Inst. Statist. Math. 50, 379-402.\n[89] Ogata, Y. (1981). On Lewis\u2019 simulation method for point processes. IEEE\nTransactions on Information Theory. 27, 23-31.\n[90] Ogata, Y., Akaike, H. and K. Katsura. (1982). The application of linear in-\ntensity models to the investigation of causal relations between a point process\n204\nand another stochastic process. Annals of the Institute of Statistical Mathe-\nmatics. 34, 373-387.\n[91] Ozaki, T. (1979). Maximum likelihood estimation of Hawkes\u2019 self-exciting\npoint processes. Ann. Inst. Statist. Math. 31, 145-155.\n[92] Peng, X. and S. Kou. Default Clustering and Valuation of Collateralized Debt\nObligations. Working Paper, Columbia University, January 2009.\n[93] Pernice, V., Staude B., Carndanobile, S. and S. Rotter. (2012). How structure\ndetermines correlations in neuronal networks. PLoS Computational Biology.\n85:031916.\n[94] Pernice, V., Staude B., Carndanobile, S. and S. Rotter. (2011). Recurrent\ninteractions in spiking networks with arbitrary topology. Physical Review E.\n7:e1002059.\n[95] Porter, M. and G. White. (2012). Self-exciting hurdle models for terrorist\nactivity. Annals of Applied Statistics 6, 106-124.\n[96] Reynaud-Bouret, P. and S. Schbath. (2010). Adaptive estimation for Hawkes\nprocesses; application to genome analysis. Ann. Statist. 38, 2781-2822.\n[97] Reynaud-Bouret, P. and E. Roy. (2007). Some non asymptotic tail estimates\nfor Hawkes processes. Bull. Belg. Math. Soc. Simon Stevin 13, 883-896.\n[98] Reynaud-Bouret, P., Tuleau-Malot, C., Rivoirard, V. and F. Grammont.\nSpike trains as (in)homogeneous Poisson processes or Hawkes processes: non-\nparametric adaptive estimation and goodness-of-\ufb01t tests. Preprint.\n205\n[99] Rubin, I. (1972). Regular point processes and their detection. IEEE Trans-\nactions on Information Theory. 18, 547-557.\n[100] Sen, S. and L. Zhu. Large deviations for self-correcting point processes. In\nPreparation.\n[101] Sornette, D. and S. Utkin. (2009). Limits of declustering methods for disen-\ntangling exogenous from endogenous events in time series with foreshocks,\nmain shocks, and aftershocks. Physical Review E. 79 (6), 61110.\n[102] Stabile, G. and G. L. Torrisi. (2010). Risk processes with non-stationary\nHawkes arrivals. Methodol. Comput. Appl. Prob. 12, 415-429.\n[103] Torrisi, G. L. (2002). A class of interacting marked point processes: Rate of\nconvergence to equilibrium. Journal of Applied Probability 39, 137-160.\n[104] Wang, T., Bebbington, M. and D. Harte. (2012). Markov-modulated Hawkes\nprocess with stepwise decay. Annals of the Institute of Statistical Mathemat-\nics 64 521-544.\n[105] Wei, C. Z. and J. Winnicki. (1989). Some asymptotic results for the branching\nprocess with immigration. Stoch. Proc. Appl. 31, 261-282.\n[106] Varadhan, S. R. S. (2008). Large deviations. Annals of Probability. 36, 397-\n419.\n[107] Varadhan, S. R. S. Large Deviations and Applications, SIAM, Philadelphia,\n1984.\n[108] Varadhan, S. R. S. Probability Theory, Courant Lecture Notes, American\nMathematical Society, 2007.\n206\n[109] Varadhan, S. R. S. Stochastic Processes, Courant Lecture Notes, American\nMathematical Society, 2007.\n[110] Vere-Jones, D. (1978). Earthquake prediction: A statistician\u2019s view. Journal\nof Physics of the Earth 26, 129-146.\n[111] Zheng, B., Roue\ufb00, F. and F. Abergel. Ergodicity and scaling limit of a con-\nstrained multivariate Hawkes process. SSRN eLibrary, 2013.\n[112] Zhu,\nL. Large deviations for Markovian nonlinear Hawkes processes.\nPreprint. arXiv:1108.2432.\n[113] Zhu, L. Process-level large deviations for nonlinear Hawkes point processes.\nTo appear in Annales de l\u2019Institut Henri Poincar\u00b4e. arXiv:1108.2431.\n[114] Zhu, L. Central limit theorem for nonlinear Hawkes processes. To appear in\nJournal of Applied Probability. arXiv:1204.1067.\n[115] Zhu, L. (2013). Moderate deviations for Hawkes processes. Statistics & Prob-\nability Letters 83, 885-890.\n[116] Zhu, L. Limit theorems for a Cox-Ingersoll-Ross process with Hawkes jumps.\nPreprint.\n[117] Zhu, L. Asymptotics for nonlinear Hawkes processes. In Preparation.\n[118] Zhu, L. Ruin probabilities for risk processes with non-stationary arrivals and\nsubexponential claims. Preprint. arXiv:1304.1940.\n207\n",
        "sentence": "",
        "context": "iii\nAcknowledgements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\niv\nAbstract . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nvi\nList of Figures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nxi\nterjee, Oliver Conway, S\u02c6\u0131nziana Datcu, Partha Dey, Thomas Fai, Max Fathi, Mert\nG\u00a8urb\u00a8uzbalaban, Matan Harel, Miranda Holmes-Cerfon, Arjun Krishnan, Shoshana\nLe\ufb04er, Sandra May, Jim Portegies, Alex Rozinov, Patrick Stewart, Adam Stinch-\nProfessor S. R. S. Varadhan\narXiv:1304.7531v3  [math.PR]  23 Jun 2013\nc\u20dd\nLingjiong Zhu\nAll Rights Reserved, 2013\nDedication\nTo the memory of my grandpa\nZhixuan Zhu (1923-2001)\niii\nAcknowledgements"
    }
]