[
    {
        "title": "The weighted majority algorithm",
        "author": [
            "Nick Littlestone",
            "Manfred K. Warmuth"
        ],
        "venue": "Inf. Comput.,",
        "citeRegEx": "1",
        "shortCiteRegEx": "1",
        "year": 1994,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Many classes of problems, including prediction with expert advice [1], the multi-armed bandit problem [2], dynamic pricing [3], the dark pool problem [4], label efficient prediction [5], and linear and convex optimization with full or bandit feedback [6, 7] can be modeled as an instance of partial monitoring. Let p\u0302i(t) \u2208 [0, 1] be the empirical distribution of the symbols under the selection of action i. References [1] Nick Littlestone and Manfred K. 2 Regret analysis of PM-DMED-Hinge Let p\u0302i,n \u2208 [0, 1] be the empirical distribution of the symbols from the action i when the action i is selected n times.",
        "context": null
    },
    {
        "title": "Asymptotically efficient adaptive allocation rules",
        "author": [
            "T.L. Lai",
            "Herbert Robbins"
        ],
        "venue": "Advances in Applied Mathematics,",
        "citeRegEx": "2",
        "shortCiteRegEx": "2",
        "year": 1985,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Many classes of problems, including prediction with expert advice [1], the multi-armed bandit problem [2], dynamic pricing [3], the dark pool problem [4], label efficient prediction [5], and linear and convex optimization with full or bandit feedback [6, 7] can be modeled as an instance of partial monitoring. Upper confidence bound (UCB), the most well-known algorithm for the multi-armed bandits, has a distribution-dependent regret bound [2, 14], and algorithms that minimize the distribution-dependent regret (e. In the context of the multi-armed bandit problem, Lai and Robbins [2] derived the regret lower bound of a strongly consistent algorithm: an algorithm must select each arm i until its number of draws Ni(t) satisfies log t . Based on the technique used in Lai and Robbins [2], the proof considers a modified game in which another action i 6= 1 is optimal. Moreover, the coefficient of the leading logarithmic term in the regret bound of the partial monitoring problem is equal to the bound given in Lai and Robbins [2]. [2] T. The technique here is mostly inspired from Theorem 1 in Lai and Robbins [2]. During the proof we also show that C 1 (p , {pi }) is equal to the optimal constant factor of Lai and Robbins [2]. LBTheory is the asymptotic regret lower bound of Lai and Robbins [2]. LB-Theory is the regret lower bound of Lai and Robbins [2], that is, \u2211 i6=1 \u2206i log t d(\u03bci\u2016\u03bc1) .",
        "context": null
    },
    {
        "title": "The value of knowing a demand curve: Bounds on regret for online posted-price auctions",
        "author": [
            "Robert D. Kleinberg",
            "Frank Thomson Leighton"
        ],
        "venue": "In FOCS,",
        "citeRegEx": "3",
        "shortCiteRegEx": "3",
        "year": 2003,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Many classes of problems, including prediction with expert advice [1], the multi-armed bandit problem [2], dynamic pricing [3], the dark pool problem [4], label efficient prediction [5], and linear and convex optimization with full or bandit feedback [6, 7] can be modeled as an instance of partial monitoring. [3] Robert D.",
        "context": null
    },
    {
        "title": "Optimal allocation strategies for the dark pool problem",
        "author": [
            "Alekh Agarwal",
            "Peter L. Bartlett",
            "Max Dama"
        ],
        "venue": "In AISTATS,",
        "citeRegEx": "4",
        "shortCiteRegEx": "4",
        "year": 2010,
        "abstract": "We study the problem of allocating stocks to dark pools. We propose and\nanalyze an optimal approach for allocations, if continuous-valued allocations\nare allowed. We also propose a modification for the case when only\ninteger-valued allocations are possible. We extend the previous work on this\nproblem to adversarial scenarios, while also improving on their results in the\niid setup. The resulting algorithms are efficient, and perform well in\nsimulations under stochastic and adversarial inputs.",
        "full_text": "Optimal Allocation Strategies for the Dark Pool Problem\nAlekh Agarwal\nUniversity of California, Berkeley\nalekh@cs.berkeley.edu\nPeter Bartlett\nUniversity of California, Berkeley\nbartlett@cs.berkeley.edu\nMax Dama\nUniversity of California, Berkeley\nmaxdama@berkeley.edu\nOctober 25, 2018\nAbstract\nWe study the problem of allocating stocks to dark pools. We propose and analyze an optimal approach\nfor allocations, if continuous-valued allocations are allowed. We also propose a modi\ufb01cation for the case\nwhen only integer-valued allocations are possible. We extend the previous work on this problem (Ganchev\net al., 2009) to adversarial scenarios, while also improving on their results in the iid setup. The resulting\nalgorithms are e\ufb03cient, and perform well in simulations under stochastic and adversarial inputs.\n1\nIntroduction\nIn this paper we consider the problem of allocating stocks to dark pools. As described by (Ganchev et al.,\n2009), dark pools are a recent type of stock exchange that are designed to facilitate large transactions. A\nkey aspect of dark pools is the censored feedback that the trader receives. At every round the trader has a\ncertain number V t of shares to allocate amongst K di\ufb00erent dark pools. The dark pool i trades as many of\nthe allocated shares vi as it can with the available liquidity. The trader only \ufb01nds out how many of these\nallocated shares were successfully traded at each dark pool, but not how many would have been traded if\nmore were allocated.\nIt is natural to assume that the actions of the trader a\ufb00ect the volume available at all dark pools at\nlater times. Similarly, it seems natural that at a given time, the liquidities available at di\ufb00erent venues\nshould be correlated: we would expect counterparties to distribute large trades across many dark pools,\nsimultaneously a\ufb00ecting their liquidity. Furthermore, in a realistic scenario, these variables are governed not\nonly by the trader\u2019s actions, but also by the actions of other competing traders, each trying to maximize\npro\ufb01ts. Since the gain of one trader is at the expense of another, this problem naturally lends itself to\nan adversarial analysis. Generalizing the setup of (Ganchev et al., 2009), we assume that the sequences of\nvolumes and available liquidity at each venue are chosen by an adversary who knows the previous allocations\nof our algorithm.\nWe propose an exponentiated gradient (henceforth EG) style algorithm that has an optimal regret guar-\nantee against the best allocation strategy in hindsight. Our algorithm uses a parametrization that allows it\nto handle the problem of changing constraint sets easily. Through a standard online to batch conversion,\nthis also yields a signi\ufb01cantly better algorithm in the iid setup studied in (Ganchev et al., 2009). However,\nthe EG algorithm has the drawback that it recommends continuous-valued allocations. We describe how the\nproblem of allocating an integral number of shares closely resembles a multi-armed bandit problem. As a\nresult, we use ideas from the Exp3 algorithm for adversarial bandit problems (Auer et al., 2003) to design an\nalgorithm that produces integer-valued allocations and enjoys a regret of order T 2/3 with high probability.\nWhile this regret bound holds in an adversarial setting, it also implies an improvement on (Ganchev et al.,\n1\narXiv:1003.2245v1  [stat.ML]  11 Mar 2010\n2009) in an iid setting. We also study an e\ufb03cient implementation of our algorithm using the idea of greedy\napproximations in Hilbert spaces (Jones, 1992), (Barron, 1993).\nIn the next section we will describe the problem setup in more detail and survey previous work. We\nwill describe the EG algorithm for continuous allocations and prove its regret bound and optimality in\nSection 3. In Section 4 we describe the algorithm for integer valued allocations. Section 4.4 describes an\ne\ufb03cient implementation. Finally we present experiments comparing our algorithms with that of (Ganchev\net al., 2009) using the data simulator described in their paper.\n2\nSetup and Related Work\nWe generalize the setup of (Ganchev et al., 2009). A learning algorithm receives a sequence of volumes\nV 1, . . . V T where V t \u2208{1, . . . , V }. It has K available venues, amongst which it can allocate up to V t units\nat time t. The learner chooses an allocation vt\ni for the ith venue at time t that satis\ufb01es PK\ni=1 vt\ni \u2264V t.\nEach venue has a maximum consumption level st\ni. The learner then receives the number of units rt\ni =\nmin(vt\ni, st\ni) consumed at venue i. We allow the sequence of volumes and maximum consumption levels to be\nchosen adversarially, i.e. Vt, st\ni can depend on {v1\ni , . . . , vt\u22121\ni\n}K\ni=1. We measure the performance of our learner\nin terms of its regret\nRT = max\nT\nX\nt=1\nK\nX\ni=1\nmin(ut\ni, st\ni) \u2212min(vt\ni, st\ni)\nwhere the outer maximization is over the vector opt \u2208{1, . . . , K}V and\nut\ni =\nV t\nX\nv=1\nI(optv = i),\ni.e., we compete against any strategy that chooses a \ufb01xed sequence of venues opt1, . . . , optV and always\nallocates the vth unit to venue optv.\nThe work most closely related to ours is (Ganchev et al., 2009). In that paper, the authors consider the\nsequence of volumes V 1, . . . , V T and allocation limits st\ni to be distributed in an iid fashion. They propose\nan algorithm based on Kaplan-Meier estimators. Their algorithm mimics an optimal allocation strategy by\nestimating the tail probabilities of st\ni being larger than a given value. They show that the allocations of their\nalgorithm are \u03f5-suboptimal with probability at most 1\u2212\u03f5 after seeing su\ufb03ciently many samples. Theorem 1\nin (Ganchev et al., 2009) shows that, if the st\ni is chosen iid, then the optimal strategy always allocates the\nith unit to a \ufb01xed venue. This justi\ufb01es our de\ufb01nition of regret in comparison to this class of strategies.\nThe ideas used in our paper draw on the rich literature on online adversarial learning. The algorithm of\nSection 3 is based on the classical EG algorithm (Littlestone and Warmuth, 1994). When playing integral\nallocations, we describe how the multi-armed bandits problem is a special case of our problem for V = 1. For\nthe general case, we describe an adaptation of the Exp3 algorithm (Auer et al., 2003) for adversarial multi-\narmed bandits. To provide regret bounds that hold with high probability, we use a variance correction similar\nto the Exp3.P algorithm (Auer et al., 2003). Our lower bounds use information theoretic techniques, building\non Fano\u2019s method (Yu, 1993). The e\ufb03cient implementation of our algorithm relies on greedy approximation\ntechniques in Hilbert space (Jones, 1992), (Barron, 1993).\n3\nOptimal algorithm for fractional allocations\nAlthough the dark pool problem requires us to allocate an integral number of shares at every venue, we start\nby studying the simpler case where we can allocate any positive value for every venue, so long as they satisfy\nPK\ni=1 vt\ni \u2264V t. We start by noting that the reward function rt\ni = min(vt\ni, st\ni) is concave in allocations vt\ni.\n2\nMaximization of concave functions is well understood, even in an adversarial scenario through approaches\nsuch as online gradient ascent. We note that in this problem, the algorithm has access to the subgradient of\nthe reward function. To see this, we de\ufb01ne\ngt\ni =\n\u001a 1\nif rt\ni = vt\ni\n0\nif rt\ni < vt\ni\n(1)\nThen it is easy to check that gt\ni can be constructed from the feedback we receive, and it lies in the\nsubgradient set \u2202rt\ni\n\u2202vt\ni . Hence, we can run a standard online (sub)gradient ascent algorithm on this sequence of\nreward functions. However, the allocations vt\ni are chosen from a di\ufb00erent set St = {\u20d7vt : PK\ni=1 vt\ni \u2264V t} at\nevery round. Using standard online gradient ascent analysis, we can demonstrate a low regret only against\na comparator that lies in the intersection of all these constraint sets \u2229T\nt=1St. However the regret guarantee\ncan be rather meaningless if V t is extremely small at even a single round. Ideally, we would like to compete\nwith an optimal allocation strategy like (Ganchev et al., 2009). A slightly di\ufb00erent parameterization allows\nus to do exactly that.\nLet us de\ufb01ne \u2206V\nK = {x1, . . . , xV\n:\nPK\ni=1 xv\ni = 1 \u2200v \u2264V } to be the Cartesian product of V simplices,\neach in RK. Then we can construct an algorithm for allocations as follows: for each unit v = {1, . . . , V }, we\nhave a distribution over the venues {1, . . . , K} where that unit is allocated. At time t, the algorithm plays\nvt\ni = PV t\nv=1 xv\nt,i. It is clear that this allocation satis\ufb01es the volume constraint.\nThe comparator is now de\ufb01ned as a \ufb01xed point u \u2208\u2206V\nK. We compete with the strategy that plays\naccording to vt\ni = PV t\nv=1 uv\ni . Then the best comparator u is equivalent to the best \ufb01xed allocation strategy\nopt \u2208{1, . . . , K}V . It is also clear that if we can compete with the best strategy in an adversarial setup,\nonline to batch conversion techniques (see Cesa-Bianchi et al (Cesa-Bianchi et al., 2001)) will give a small\nexpected error in the case where the volumes and maximum consumptions are drawn in an iid fashion.\n3.1\nAlgorithm and upper bound\nAn online gradient ascent algorithm for this setup is presented in Algorithm 1.\nAlgorithm 1 Exponentiated gradient algorithm for continuous-valued allocations to dark pools\nInput learning rate \u03b7, bound on volumes V .\nInitialize xv\n1,i = 1\nK for v \u2208{1, . . . , V }, i \u2208{1, . . . , K}.\nfor t = 1, . . . , T do\nSet vt\ni = PV t\nv=1 xv\nt,i.\nReceive rt\ni = min{vt\ni, st\ni}.\nSet gt\ni as de\ufb01ned in Equation (1).\nSet gv\nt,i = gt\ni if v \u2264V t, 0 otherwise.\nUpdate xv\nt+1,i \u221dxv\nt,i exp(\u03b7gv\nt,i).\nend for\nIt can be shown that the algorithm enjoys the following regret guaranteee.\nTheorem 1. For any choices of the volumes V t \u2208[0, V ] and of the maximum consumption levels st\ni, the\nregret of Algorithm 1 with \u03b7 =\nq\nln K\n(e\u22122)T over T rounds is O(V\n\u221a\nT ln K).\nProof. The regret is de\ufb01ned as\nRT = max\nu\u2208\u2206V\nK\nT\nX\nt=1\nK\nX\ni=1\nmin\n\uf8eb\n\uf8ed\nV t\nX\nv=1\nuv\ni , st\ni\n\uf8f6\n\uf8f8\u2212\nT\nX\nt=1\nK\nX\ni=1\nmin\n\u0000vt\ni, st\ni\n\u0001\n\u2264\nT\nX\nt=1\nV t\nX\nv=1\n(uv \u2212xv\nt )\u22a4gv\nt .\n3\nFollowing the proof of Theorem 11.3 from Cesa-Bianchi et al (Cesa-Bianchi and Lugosi, 2006), we de\ufb01ne\n\u03bdv\ni = \u03b7gv\nt,i \u2212\u03b7(gv\nt )\u22a4xv\nt . Also, we note that the gradient is zero for v > V t. So we can sum over v from 1 to\nV rather than V t. Then we bound the regret as\nT\nX\nt=1\nV\nX\nv=1\n\"\n(uv \u2212xv\nt )\u22a4gv\nt \u22121\n\u03b7 ln\n K\nX\ni=1\nxv\nt,i exp(\u03bdv\ni )\n!\n+1\n\u03b7 ln\n K\nX\ni=1\nxv\nt,i exp(\u03bdv\ni )\n!#\n.\nSome rewriting and simpli\ufb01cation gives the bound\n1\n\u03b7\nT\nX\nt=1\nV\nX\nv=1\n\" K\nX\ni=1\nuv\ni ln\n \nexp\n\u0000\u03b7gv\nt,i\n\u0001\nPK\ni=1 exp\n\u0000\u03b7gv\nt,i\n\u0001\n!\n+ ln\n K\nX\ni=1\nxv\nt,ie\u03bdv\ni\n!#\n= 1\n\u03b7\nT\nX\nt=1\nV\nX\nv=1\n\"\nuv\ni ln\n\u0012xv\nt+1,i\nxv\nt,i\n\u0013\n+ ln\n K\nX\ni=1\nxv\nt,i exp(\u03bdv\ni )\n!#\n\u22641\n\u03b7\nV\nX\nv=1\n\"\nKL(uv||xv\n1) +\nT\nX\nt=1\nln\n K\nX\ni=1\nxv\nt,i exp(\u03bdv\ni )\n!#\n.\nHere, the last line uses the de\ufb01nition of KL-divergence and the fact that the telescoping terms cancel out.\nNow gv\nt,i \u22641 so that \u03bdv\ni \u2264\u03b7. If \u03b7 \u22641, then it is easy to verify that exp(\u03bdv\ni ) \u22641 + \u03bdv\ni + (e \u22122) (\u03bdv\ni )2 . We also\nnote that PK\ni=1 xv\nt,i\u03bdv\ni = 0.\nAlso, each of the KL divergence terms in the above display is equal to ln K. This is because the optimal\ncomparator will have a 1 for exactly one venue for each unit v. As we choose xv\n1 to be uniform over all\nvenues, we get the KL divergence between a vertex of the K-simplex and the uniform distribution which, is\nln K.\nHence we bound the regret as\n1\n\u03b7 V ln K + 1\n\u03b7\nT\nX\nt=1\nV\nX\nv=1\nln\n K\nX\ni=1\nxv\nt,i\n\u0010\n1 + \u03bdv\ni + (e \u22122) (\u03bdv\ni )2\u0011!\n\u22641\n\u03b7 V ln K + 1\n\u03b7\nT\nX\nt=1\nV\nX\nv=1\n(e \u22122)\u03b72\n= 1\n\u03b7 V ln K + (e \u22122)\u03b7V T\n\u22643V\n\u221a\nT ln K,\nwhere the last step follows from setting \u03b7 =\nq\nln K\n(e\u22122)T .\n3.2\nLower bound and minimax optimality\nWe will now show that the online exponentiated gradient ascent algorithm in Algorithm 1 has the best regret\nguarantee possible. We start by noting that a a regret bound of O(\n\u221a\nT ln K) is known to be optimal for the\nexperts prediction problem (Haussler et al., 1998; Abernethy et al., 2009). Hence we can show the optimality\nof our algorithm for V = 1 by reducing experts prediction problem to the dark pools problem. Recall that\nin the experts prediction problem, the algorithm picks an expert from 1, . . . , K according to a probability\ndistribution pt at round t. Then it receives a vector of rewards \u03c1t with \u03c1t,i \u2208[0, 1],\ni = 1, . . . , K. In order\n4\nto describe a reduction, we need to map the allocations of an algorithm for the dark pools problem to the\nprobabilities for experts, and map the rewards of experts to the liquidities at each venue.\nWe consider a special setting where Vt = 1 at all times. Since Vt = 1, the allocations of any dark pools\nalgorithm are probabilities\u2013 they are non-negative and add to 1. Hence we set pt,i = vt\ni. We also set the\nliquidity st\ni = \u03c1t,ipt,i. Then the net reward of a dark pools algorithm at round t is:\nK\nX\ni=1\nmin(st\ni, vt\ni) =\nK\nX\ni=1\nmin(\u03c1t,ipt,i, pt,i) =\nK\nX\ni=1\n\u03c1t,ipt,i,\nwhere the last line follows from the observation that 0 \u2264\u03c1t,i \u22641. Hence the net reward of the dark pools\nproblem is same as that expected reward in the experts prediction problem. Using the known lower bounds\non the optimal regret in experts prediction problems, we get:\nmax\nu\u2208\u2206K\nT\nX\nt=1\nK\nX\ni=1\n\u0002\nmin\n\u0000ui, st\ni\n\u0001\n\u2212min(vt\ni, st\ni)\n\u0003\n= max\ni\nT\nX\nt=1\n\uf8ee\n\uf8f0\u03c1t,i \u2212\nK\nX\nj=1\n\u03c1t,jpt,j\n\uf8f9\n\uf8fb\n= \u2126(\n\u221a\nT ln K).\nWe also note that the regret in the experts prediction problem scales linearly with the scaling of the\nrewards. Hence, if the rewards take values in [0, V ], then the regret of any algorithm is guaranteed to be\n\u2126(V\n\u221a\nT ln K).\nFor arbitrary V , we again consider the special setting with Vt identically equal to V . We would now like\nto reduce the experts prediction problem where every expert\u2019s reward is a value in [0, V ]. At every round, we\nreceive a vector of allocations vt\ni. We set pt,i = vt\ni/V . We receive the rewards \u03c1t,i from the experts problem,\nand assign the liquidities st\ni = \u03c1t,ipt,i \u2208[0, V ]. Furthermore,\nmin(st\ni, vt\ni) = V min\n\u0012st\ni\nV , pt,i\n\u0013\n= \u03c1t,ipt,i.\nThe last step relies on observing that \u03c1t,i \u2264V so that \u03c1t,ipt,i/V \u2264pt,i. Now we can argue that the regrets\nof the two problems are identical as before. Hence the optimal regret on the dark pools problem is at least\n\u2126(V\n\u221a\nT ln K). As Algorithm 1 gets the same bound up to constant factors in a harder adversarial setting\nthan used in the lower bounds, we conclude that it attains the minimax optimal regret up to constant factors.\n4\nAlgorithm for integral allocations\nWhile the above algorithm is simple and optimal in theory, it is a bit unrealistic as it can recommend we\nallocate 1.5 units to a venue, for example. One might choose to naively round the recommendations of the\nalgorithm, but such a rounding would incur an additional approximation error which in general could be as\nlarge as O(T). In this section we describe a low regret algorithm that allocates an integral number of units\nto each venue.\nTo get some intuition about an algorithm for this scenario, consider the case when V = 1. Then the\nalgorithm has to allocate 1 unit to a venue at every round. It receives feedback about the maximum allocation\nlevel st\ni only at the venue where vt\ni = 1. This is clearly a reformulation of the classical K-armed bandits\n5\nproblem. An adaptation of Algorithm 1 that uses the Exp3 algorithm (Auer et al., 2003) would hence attain\na regret bound of O(\n\u221a\nTK ln K) for V = 1. Contrasting this with the bound of Theorem 1 for V = 1, we can\neasily see that the regret for playing integral allocations can be higher than that of continuous allocations\nby a factor of up to\n\u221a\nK. Indeed we will now show a modi\ufb01cation of the Exp3 approach that works for\narbitrary values of V . We will also show a lower bound. The upper bound shows that our algorithm incurs\nO(T 2/3) regret in expectation, which does not match the O(\n\u221a\nT) lower bound. However, it is still a signi\ufb01cant\nimprovement on Ganchev et al (Ganchev et al., 2009) as we will discusss later.\n4.1\nAlgorithm and upper bound\nWe need some new notation before describing the algorithm. For a fractional allocation vt\ni, we let f t\ni = \u230avt\ni\u230b\nand dt\ni = vt\ni \u2212\u230avt\ni\u230b.\nNow suppose we have a strategy that wants to allocate vt\ni units to venue i at time t. Suppose that we\ninstead allocate ut\ni = f t\ni units with probability 1 \u2212dt\ni and ut\ni = f t\ni + 1 units with probability dt\ni. Using the\nfact that the maximum consumption limits are integral too\nE min(ut\ni, st\ni) = dt\ni min(f t\ni + 1, st\ni) + (1 \u2212dt\ni) min(f t\ni , st\ni)\n=\n\u001a\nst\ni\nif st\ni \u2264f t\ni\nf t\ni + di\ni\nif st\ni \u2265f t\ni + 1\n= min(vt\ni, st\ni).\nThus, playing an integral allocation ut\ni according to such a scheme would be unbiased in expectation. Of\ncourse we need to ensure that we don\u2019t violate the constraint PK\ni=1 ut\ni \u2264V t in this process. To do so, we let\nPK\ni=1 dt\ni = Vt \u2212PK\ni=1 f t\ni = m. Then we will use a distribution over subsets of {1, . . . , K} of size m that has\nthe property that ith element gets sampled with probability dt\ni. It is clear that if there is such a distribution,\nthen we will have the unbiasedness needed above. It will also ensure feasibility of ut\ni if vt\ni was a feasible\nallocation. Our next result shows that such a distribution always exists.\nTheorem 2. Let 0 \u2264dt\ni < 1, PK\ni=1 dt\ni = m for m \u22651. Then there is always a distribution over subsets of\n{1, . . . , K} of size m such that the ith element is sampled with probability dt\ni.\nProof. Proof is by induction on K. For the case K = 2, m = 1, we sample the \ufb01rst element with probability\ndt\n1. If it is not picked, we pick element 2. It is clear that the marginals are correct establishing the base case.\nLet us assume the claim holds up to K \u22121 for all m \u2264K \u22121. Consider the inductive step for some K, m.\nWe are given a set of marginals, 0 \u2264dt\ni < 1, PK\ni=1 dt\ni = m. We would like a distribution p on subsets of\nsize m of {1, . . . , K} that matches these marginals. We partition these subsets into two groups; those that\ndo and do not contain the \ufb01rst element. We correspondingly partition p = (p1, p2). Let N1 =\n\u0000K\u22121\nm\u22121\n\u0001\nand\nN2 =\n\u0000K\u22121\nm\n\u0001\nbe the number of subsets in the two cases. Then we want PN\ni=1 p(i) = PN1\ni=1 p1(i) = dt\n1 in order\nto get the right marginal at element 1. Hence, we can write p1 = dt\n1q1, p2 = (1\u2212dt\n1)q2 for some distributions\nq1 and q2 on N1 and N2 subsets respectively. Now we write\ndt\ni =\n\u0012(m \u22121)dt\n1\nm \u2212dt\n1\n+ m(1 \u2212dt\n1)\nm \u2212dt\n1\n\u0013\ndt\ni\n(2)\nfor i > 1. Then\nK\nX\ni=2\n(m \u22121)\nm \u2212dt\n1\ndt\ni = m \u22121,\nK\nX\ni=2\nm\nm \u2212dt\n1\ndt\ni = m\n(3)\n6\nare marginals on subsets of size m \u22121 and m respectively of {1, . . . , K \u22121}, and are in [0, 1] as PK\ni=2 dt\ni =\nm\u2212dt\n1. Hence there exist distributions q1 and q2 that attain these marginals using the inductive hypothesis.\nWe set p1 = dt\n1q1, p2 = (1\u2212dt\n1)q2. Then Equations 2 and 3 together imply that we get the correct marginals\nfor every element.\nFor any allocation sequence vt, let p(dt) be the probability distribution over subsets of {1, . . . , K} guaran-\nteed by Theorem 2. For some constant \u03b3 \u2208(0, 1], let \u00afdt,i = (1 \u2212\u03b3)dt\ni + \u03b3m\nK . Then let p( \u00afdt,i) be a distribution\nover subsets that samples the ith venue with probability \u00afdt,i. We can construct this by mixing p(dt\ni) which\nexists by Theorem 2 and mixing uniform distribution over subsets of size m. Also, we let \u02dcVt,i \u2264Vt be the\nlargest index v0 such that Pv0\nv=1 xv\nt,i \u2264f t\ni . We de\ufb01ne a gradient estimator:\n\u02dcgv\nt,i =\n\uf8f1\n\uf8f2\n\uf8f3\nI(st\ni \u2265f t\ni ) \u2212I(st\ni=f t\ni )I(ut\ni=\u2308vt\ni\u2309)\n\u00afdt,i\nif v \u2264\u02dcVt,i\nI(st\ni\u2265vt\ni)I(ut\ni=\u2308vt\ni\u2309)\n\u00afdt,i\nif \u02dcVt,i + 1 \u2264v \u2264V t.\n(4)\nTo see why this gradient estimator is good, we \ufb01rst note that the gradient of the objective function at vt\ni\ncan be written as\ngv\nt,i = I(st\ni \u2265vt\ni) = I(st\ni \u2265f t\ni ) \u2212I(st\ni = f t\ni ),\nwhen v \u2264V t. Then we can easily show the following useful lemma.\nLemma 1. If an algorithm plays ut\ni = \u2308vt\ni\u2309with probability \u00afdt,i and ut\ni = f t\ni otherwise, then \u02dcgt as described\nin Equation (4) is an unbiased estimator of the gradient at (vt\n1, . . . , vt\nK).\nAn algorithm for playing integer-valued allocations at every round is shown in Algorithm 2.\nAlgorithm 2 An algorithm for playing integer-valued allocations to the dark pools\nInput learning rate \u03b7, threshold \u03b3, bound on volumes V .\nInitialize xv\n1,i = 1\nK for v = {1, . . . , V }.\nfor t = 1 . . . T do\nSet vt\ni = PV t\nv=1 xv\nt,i.\nLet p( \u00afdt,i) be the distribution over subsets from Theorem 2.\nSample a subset of size m = PK\ni=1 \u00afdt,i according to p( \u00afdt,i).\nPlay ut\ni = f t\ni + 1 if i is in the subset sampled, ut\ni = f t\ni otherwise.\nReceive rt\ni = min(ut\ni, st\ni).\nSet \u02dcgv\nt,i as de\ufb01ned in Equation (4).\nUpdate xv\nt+1,i \u221dxv\nt,i exp(\u03b7\u02dcgv\nt,i).\nend for\nWe can also demonstrate a guarantee on the expected regret of this algorithm.\nTheorem 3. Algorithm 2, with \u03b7 =\n\u0010\nV (ln K)2\nKT 2\n\u00111/3\n, has expected regret over T rounds of O((V TK)2/3(ln K)1/3),\nwhere V is the bound on volumes V t, and the volumes and maximum consumption levels st\ni are chosen by\nan oblivious adversary.\nAn oblivious adversary is one that chooses V t and st\ni without seeing the algorithm\u2019s (random) allocations\nut\ni. We note that the requirement that the adversary is oblivious can be removed by proving a high probability\nbound. We will describe a slight modi\ufb01cation of Algorithm 2 that enjoys such a guarantee.\nProof. Since the adversary is oblivious, we can \ufb01x a comparator u \u2208\u2206V\nK ahead of time. For the remainder,\nwe let Et denote conditional expectation at time t conditioned on the past moves of algorithm and adversary.\n7\nThen the expected regret is\nE\n\" T\nX\nt=1\nK\nX\ni=1\nmin\n V\nX\nv=1\nuv\ni , st\ni\n!\n\u2212\nT\nX\nt=1\nK\nX\ni=1\nmin\n\u0000ut\ni, st\ni\n\u0001\n#\n\u2264E\n\" T\nX\nt=1\nK\nX\ni=1\nmin\n V\nX\nv=1\nuv\ni , st\ni\n!\n\u2212\nT\nX\nt=1\nK\nX\ni=1\nmin(vt\ni, st\ni)\n#\n+ \u03b3TK.\nHere, the second step follows from the fact that ut\ni would be unbiased for vt\ni without for the \u03b3m\nK adjustment.\nHowever, this adjustment costs us at most \u03b3 PT\nt=1 mt \u2264\u03b3TK in terms of expected regret over T rounds. For\nthe \ufb01rst term, it is as if we had played the continuous valued allocation vt\ni itself. Again using the concavity\nof our reward function\nRT (u) \u2264E\n\" V\nX\nv=1\n(uv \u2212xv\nt )\u22a4gv\nt\n#\n+ \u03b3TK\n= E\n\" V\nX\nv=1\n(uv \u2212xv\nt )\u22a4(Et\u02dcgv\nt )\n#\n+ \u03b3TK.\nHere the last step follows from noting that \u02dcgt is unbiased estimator of gt by construction just like in\nExp3 (Auer et al., 2003).\nNow we note that the algorithm is doing exponentiated gradient descent on\nthe sequence \u02dcgt. Hence, we can proceed as in the proof of Theorem 1 to obtain\nRT (u) \u22641\n\u03b7 V ln K + 1\n\u03b7 E\nT\nX\nt=1\nV\nX\nv=1\nln\n K\nX\ni=1\nxv\nt,i exp(\u03bdv\ni )\n!\n+ \u03b3TK,\nwhere \u03bdv\ni = \u03b7\u02dcgv\nt,i \u2212\u03b7(\u02dcgv\nt )\u22a4xv\nt as before. Assuming a choice of \u03b7 such that \u03b7\u02dcgv\nt,i \u22641, we note again that\n\u03bdv\ni \u22641. So we can use the quadratic bound on exponential again and simplify as before to get\nRT (u) \u22641\n\u03b7 V ln K + 1\n\u03b7 E\nT\nX\nt=1\nV\nX\nv=1\nK\nX\ni=1\nxv\nt,i(\u03bdv\ni )2 + \u03b3TK\n= 1\n\u03b7 V ln K + \u03b7E\nT\nX\nt=1\nV\nX\nv=1\nK\nX\ni=1\nxv\nt,i(\u02dcgv\nt,i)2 + \u03b3TK.\nNow we can swap the sum over V and i to obtain\nRT (u) \u22641\n\u03b7 V ln K + \u03b7E\nT\nX\nt=1\nK\nX\ni=1\nV\nX\nv=1\nxv\nt,i(\u02dcgv\nt,i)2 + \u03b3TK\n= 1\n\u03b7 V ln K + \u03b7E\nT\nX\nt=1\nK\nX\ni=1\n\uf8ee\n\uf8f0\n\u02dcVt,i\nX\nv=1\nxv\nt,i(\u02dcgv\nt,i)2\n+\nV t\nX\nv= \u02dcVt,i+1\nxv\nt,i(\u02dcgv\nt,i)2\n\uf8f9\n\uf8fb+ \u03b3TK.\n8\nNow we look at the two gradient terms separately.\nEt\n\u02dcVt,i\nX\nv=1\nxv\nt,i(\u02dcgv\nt,i)2 =\n\u02dcVt,i\nX\nv=1\nxv\nt,i\n(\n\u00afdt,i\n\u0012\nI(st\ni \u2265f t\ni ) \u2212I(st\ni = f t\ni )\n\u00afdt,i\n\u00132\n+ (1 \u2212\u00afdt,i)I(st\ni \u2265vt\ni)\n\u001b\n\u22642vi\nt + 2vi\nt\nK\n\u03b3 .\nHere, we used the fact that \u00afdt,i \u2265\u03b3\nK as m \u22651 and indicator variables are bounded by 1. Hence\nE\nT\nX\nt=1\nK\nX\ni=1\n\u02dcVt,i\nX\nv=1\nxv\nt,i(\u02dcgv\nt,i)2 \u22642TV + 2TV K\n\u03b3\nusing PT\ni=1 vt\ni \u2264V . Next we examine the second gradient term\nEt\nV t\nX\nv= \u02dcVt,i+1\nxv\nt,i(\u02dcgv\nt,i)2 = Et\nV t\nX\nv= \u02dcVt,i+1\nxv\nt,i(\u02dcgV t\nt,i )2\n= Etdt\ni(\u02dcgV t\nt,i )2 \u2264\u00afdt,idt\ni\n1\n( \u00afdt,i)2\n\u22642\nif \u03b3 \u22641\n2.\nHence, E PT\nt=1\nPK\ni=1\nPV t\nv= \u02dcVt,i+1 xv\nt,i(\u02dcgv\nt,i)2 \u22642TK. Substituting the above terms in the bound, we get\nRt(u) \u22641\n\u03b7 V ln K + 2\u03b7\n\u0012\nTV + TV K\n\u03b3\n+ TK\n\u0013\n+ \u03b3TK.\nOptimizing for \u03b7, \u03b3 gives\nRT (u) \u22646(V TK)2/3(ln K)1/3.\nWe note that the term responsible for O(T 2/3) regret is I(st\ni=f t\ni )\n\u00afdt,i\n. While we assume that this can ac-\ncumulate at every round in the worst case, it seems unlikely that the liquidity st\ni will be equal to f t\ni very\nfrequently. In particular, if the st\ni\u2019s are generated by a stochastic process, one can control this probability\nusing the distribution of st\ni and obtain improved regret bounds.\n4.2\nVariance correction and High probability bound\nWe would like to show that the analysis of the previous section holds not just in expectation but also with\nhigh probability. This has two advantages. First, it tells us that on most random choices made by our\nalgorithm, it has a low regret. Further, the high probability guarantee can be easily combined with a union\nbound to give a regret bound for non-oblivious (adaptive) adversaries as well.\nHigh probability bounds in bandit problems are often tricky because even though the gradient estimator\nis unbiased, its variance is typically large. Hence, using standard martingale concentration on the estimator\n9\ndirectly gives a worse O(T 3/4) regret bound. To demonstrate a high probability guarantee of O(T 2/3), we\nneed to make a variance correction to our estimator \u02dcgt. We de\ufb01ne\n\u02c6gv\nt,i = \u02dcgv\nt,i + 10\u03b3\nK \u00afdt,i\nr\nln 1\n\u03b4 .\n(5)\nThe high probability analysis makes repeated use of the classical Hoe\ufb00ding-Azuma inequality as well as a\nversion of Freedman\u2019s inequality from Bartlett et al Bartlett et al. (2008). which we state for completeness.\ninequality.\nLemma 2 (Hoe\ufb00ding-Azuma inequality). Let X1, . . . , XT be a martingale di\ufb00erence sequence. Suppose\nthat |Yt| \u2264c almost surely for all t \u2208{1, . . . , T}. Then for all \u03b4 > 0,\nP\n T\nX\nt=1\nxt >\np\n2Tc2 ln(1/\u03b4)\n!\n\u2264\u03b4.\nLemma 3 (Bartlett et al. (2008)). Let X1, . . . , XT be a martingale di\ufb00erence sequence with |Xt| \u2264b.\nLet\nVartXt = Var(Xt|X1, . . . , Xt\u22121).\nLet V = PT\nt=1 VartXt be the sum of conditional variances of Xt\u2019s and \u03c3 =\n\u221a\nV . Then we have, for any\n\u03b4 \u22641/e and T \u22654,\nP\n T\nX\nt=1\nXt > 2 max{2\u03c3, b\np\nln(1/\u03b4)}\np\nln(1/\u03b4)\n!\n\u2264\u03b4 log2 T\nWe will now prove a series of concentration results which will immediately give the desired regret bound\nwhen put together. The steps in our analysis closely resemble the technique of Abernethy and Rakhlin\n(2009). The \ufb01rst concentration lemma shows that the regret of the integral allocations is close to their\ncontinuous valued counterparts.\nLemma 4.\nP\n \n\u2203i :\nT\nX\nt=1\nmin(ut\ni, st\ni) \u2212\nT\nX\nt=1\nmin(vt\ni, st\ni) > V\np\nT ln(K/\u03b4) + \u03b3T/K\n\u0011\n\u2264\u03b4.\nProof. We apply Lemma 2 to the martingale di\ufb00erence sequence Xt = min(ut\ni, st\ni) \u2212Et min(ut\ni, st\ni). Then\n|Xt| \u2264V . So\nP\n T\nX\nt=1\nmin(ut\ni, st\ni) \u2212\nT\nX\nt=1\nEt min(ut\ni, st\ni) > V\np\nT ln(1/\u03b4)\n!\n\u2264\u03b4.\nBut we note that by construction\nEt min(ut\ni, st\ni) = \u00afdt,i min(f t\ni + 1, st\ni) + (1 \u2212\u00afdt,i) min(f t\ni , st\ni)\n= min(f t\ni + \u00afdt,i, st\ni)\n\u2264min(f t\ni + dt\ni, st\ni) + \u03b3\nK .\nThe statement of lemma then follows from the above inequality and a union bound over all K venues.\n10\nThe next step is to show that the terms PV\nv=1(uv \u2212xv\nt )\u22a4\u02c6gv\nt and PV\nv=1(uv \u2212xv\nt )\u22a4gv\nt are close. We proceed\nindirectly by \ufb01rst bounding the conditional variances.\nLemma 5.\nVart\n\b\n(\u02dcgv\nt \u2212gv\nt )\u22a4(uv \u2212xv\nt )\n\t\n\u22645\n\" K\nX\ni=1\nui\n\u00afdt,i\n+\nK\nX\ni=1\n(xv\nt,i)2\n\u00afdt,i\n#\n.\nWe now combine this with Freedman\u2019s inequality to bound (\u02dcgv\nt \u2212gv\nt )\u22a4(uv \u2212xv\nt ).\nLemma 6.\nP\n T\nX\nt=1\nV\nX\nv=1\n(\u02c6gv\nt \u2212gv\nt )\u22a4(uv \u2212xv\nt ) > 30\u03b3TV\np\nln(1/\u03b4) + 2V\n\u0012K2\n\u03b32 + 1\n\u0013\nln(1/\u03b4) \u22642V \u03b4 log2 T\n!\n.\nProof. We de\ufb01ne the martingale Xt = PV\nv=1(\u02dcgv\nt \u2212gv\nt )\u22a4(uv \u2212xv\nt ). Then |Xt| \u2264V\n\u0010\nK\n\u03b3 + 1\n\u0011\nby H\u00a8older\u2019s\ninequality. Applying Hoe\ufb00ding-Azuma inequality gives the result.\nFinally, we also need to show that the size of the gradient estimator which is controlled in expectation is\nalso bounded with high probability.\nLemma 7.\nP\n T\nX\nt=1\nK\nX\ni=1\nvt\ni(\u02c6gV t\nt,i )2 > 2\n\u0012K2\n\u03b32 V + 8 ln 1\n\u03b4\n\u0013 p\n2T ln(1/\u03b4)\n!\n\u2264\u03b4.\nProof. We de\ufb01ne the martingale Xt = PT\nt=1\nPK\ni=1 vt\ni((\u02dcgV t\nt,i )2 \u2212Et\u02dcgV t\nt,i )2. Then using the bound on \u02dcgt, and the\nbound on expectation from proof of Theorem 3, Xt \u22642 K2\n\u03b32 V . Application of Hoe\ufb00ding-Azuma inequality\ngives the result.\nWe are now in a position to prove a high probability bound on the regret of Algorithm 2 when run with\nthe gradient estimator \u02c6gt instead of \u02dcgt.\nTheorem 4. With probability at least 1 - 1\nT , the regret of Algorithm 2 using the gradient estimator \u02c6gt against\noblivious adversaries is eO\n\u0000V (TK)2/3\u0001\n.\nThe proof essentially involves putting the lemmas together, along with the full information analysis of\nthe quantity (uv\nu \u2212xv\nt )\u22a4\u02c6gv\nt .\nProof. Using Lemma 4, with probability at least 1-\u03b4/3\nRT =\nT\nX\nt=1\nK\nX\ni=1\nmin(\nV t\nX\nv=1\nuv\ni , st\ni) \u2212min(ut\ni, st\ni)\n\u2264\nT\nX\nt=1\nK\nX\ni=1\nmin(\nV t\nX\nv=1\nuv\ni , st\ni) \u2212min(vt\ni, st\ni) +\nr\n2T ln 3K\n\u03b4\n+ \u03b3T\n\u2264\nT\nX\nt=1\nV t\nX\nv=1\n(uv \u2212xv\nt )\u22a4gv\nt + \u03b3T +\nr\n2T ln 3K\n\u03b4 .\n11\nInvoking Lemma 6, with probability at least 1-2\u03b4/3,\nRT \u2264\nT\nX\nt=1\nV t\nX\nv=1\n(uv \u2212xv\nt )\u22a4\u02dcgv\nt + \u03b3T +\nr\n2T ln 3K\n\u03b4\n+ 2V\n\u0012K2\n\u03b32 + 1\n\u0013 p\n2T ln(3/\u03b4) + 30\u03b3TV\np\nln(1/\u03b4).\nOnce again we note that we are doing exponentiated gradient descent on \u02c6gt so that we get from proof of\nTheorem 1\nT\nX\nt=1\nV t\nX\nv=1\n(uv \u2212xv\nt )\u22a4\u22641\n\u03b7 V ln K + \u03b7E\nT\nX\nt=1\nK\nX\ni=1\nV\nX\nv=1\nxv\nt,i(\u02dcgv\nt,i)2.\nUsing Lemma 7 and setting \u03b4 = 1\nT gives the statement of the theorem on optimizing for \u03b3, \u03b7.\nNote that our regret analysis so far has been against a \ufb01xed comparator. When the adversary adapts to\nplayer sequence, the comparator is random as well and depends on player\u2019s moves. However, the comparator\nconsists of delta vectors for every unit v. Hence, there are a total of KV possible comparators. Hence, we\ncan take a union bound over all the comparators as well, and this increases our regret bound by a factor of\nV ln K at most. This gives us the following corollary.\nCorollary 1. With probability at least 1 -\n1\nT , the regret of Algorithm 2 against adaptive adversaries is\neO\n\u0000V 2(TK)2/3\u0001\n.\nComparison with results of Ganchev et al. (2009): We note that although our results are in the\nadversarial setup, the same results also apply to iid problems. In particular, using online-to-batch conversion\ntechniques (Cesa-Bianchi et al., 2001), we can show that, after T rounds, with high probability the allocations\nof our algorithm on each round is within eO(V 2T \u22121/3K2/3) of the optimal allocation. This is a signi\ufb01cant\nimprovement on the result of Ganchev et al. (2009): it is straightforward to check that the proof they\nprovide gives a corresponding upper bound no better than O(T \u22121/4). As we shall see, the generalization to\nadversarial setups leads to improved performance in simulations.\n4.3\nLower bound on regret for integral allocations\nAs mentioned in the previous section, the problem of K-armed bandits is a special case of the dark pools\nproblem with integral allocations. Hence, we would like to leverage the proof techniques from existing lower\nbounds on the optimal regret in the K-armed bandits problem. As before we consider a special case with\nVt = V at every round. Following Auer et al. (2003), we construct K di\ufb00erent distributions for generating\nthe liquidities st\ni. At each round, the ith distribution samples st\ni = V with probability\n\u0000 1\n2 + \u03f5\n\u0001\nand si\nj = V\nwith probability 1\n2 for j \u0338= i. We now mimic the proof of Theorem 5.1 in Auer et al. (2003).\nWe start with a lemma analogous to Lemma A.1 of Auer et al (Auer et al., 2003). Let Vi = P\nt vt\ni. Let\nEi and Eunif denote expectations wrt the ith distribution and uniform reward distribution respectively.\nLemma 8. Let f be a function of the reward sequence r taking values in [0, M]. Then\nEif(r) \u2264Euniff(r) + M\ns\n2Eunif[Vi] ln\n\u0012\n1\n1 \u22124\u03f52\n\u0013\n.\nProof. It is clear from H\u00a8older\u2019s inequality and Pinsker\u2019s inequality that\nEi[f(r)] \u2212Eunif[f(r)] \u2264M\u2225Pi \u2212Punif\u22251 \u2264M\nq\n2KL(Punif||Pi).\n12\nNow we can proceed as in the proof of Auer et al. (2003)\nKL(Punif||Pi) =\nT\nX\nt=1\nKL(Punif(rt|rt\u22121)||Pi(rt||rt\u22121)\n=\nT\nX\nt=1\n\uf8ee\n\uf8f0\nK\nX\nj=1,j\u0338=i\nPunif(vt\nj > 0)KL\n\u00121\n2||1\n2\n\u0013\n+ Punif(vt\ni > 0)KL\n\u00121\n2 + \u03f5||1\n2\n\u0013\uf8f9\n\uf8fb\n=\nT\nX\nt=1\nPunif(vt\ni > 0)KL\n\u00121\n2 + \u03f5||1\n2\n\u0013\n.\nAs vt\ni is integer valued, Punif(vt\ni > 0) \u2264Eunif[vt\ni]. Hence\nKL(Punif||Pi) \u2264\nT\nX\nt=1\nEunif[vt\ni] ln\n\u0012\n1\n1 \u22124\u03f52\n\u0013\n= Eunif[Vi] ln\n\u0012\n1\n1 \u22124\u03f52\n\u0013\n.\nUsing this lemma, we can prove a lower bound on the regret of any algorithm that plays integer valued\nallocations.\nTheorem 5. Any algorithm that plays integer valued allocations has expected regret that is \u2126\n\u0010p\nTV (K + V ln K)\n\u0011\n.\nProof. The net reward of the algorithm when distribution i is picked is given by\nEi\nT\nX\nt=1\n\uf8ee\n\uf8f0\nK\nX\nj=1,j\u0338=i\n1\n2Eivt\nj +\n\u00121\n2 + \u03f5\n\u0013\nEivt\ni\n\uf8f9\n\uf8fb\n=\nT\nX\nt=1\n\u00141\n2(V \u2212Eivt\ni) +\n\u00121\n2 + \u03f5\n\u0013\nEivt\ni\n\u0015\n= TV\n2\n+ \u03f5\nT\nX\nt=1\nEivt\ni\n= TV\n2\n+ \u03f5Ei[Vi].\nAs in the proof of Theorem 5.1 of Auer et al. (2003), we now apply Lemma 8 to the function Vi of the reward\nsequence. As Vi \u2208[0, TV ], we get\nEi[Vi] \u2264Eunif[Vi] + TV\ns\n2Eunif[Vi] ln\n\u0012\n1\n1 \u22124\u03f52\n\u0013\n\u2264Eunif[Vi] + 2TV \u03f5\nq\nEunif[Vi].\n13\nThen\nK\nX\ni\u22121\nEi[Vi] \u2264\nK\nX\ni=1\nEunif[Vi] + 2TV \u03f5\nK\nX\ni=1\nq\nEunif[Vi].\nNow PK\ni=1 Eunif[Vi] = TV . Applying Jensen\u2019s inequality to the second term we get\nK\nX\ni=1\nEi[Vi] \u2264TV + 2TV \u03f5\n\u221a\nKTV .\nAs the index i was chosen uniformly at random, averaging over this choice gives an expected bound on\nthe reward of\n1\nK\nK\nX\ni=1\nEi[Vi] \u2264TV\nK + 2TV \u03f5\nr\nTV\nK .\nNoting again that the reward of optimal comparator is still\n\u0000 1\n2 + \u03f5\n\u0001\nTV , we get that the expected regret is\n\u2126\n \n\u03f5\n \nTV \u2212TV\nK + 2TV \u03f5\nr\nTV\nK\n!!\n.\nSetting \u03f5 optimally to c\nq\nK\nT V gives an \u2126(\n\u221a\nTV K) lower bound.\nWe also note that the lower bound of\n\u2126(V\n\u221a\nT ln K) shown for continuous-valued allocations applies to the integer-valued case as well. Combining\nthe two, we get that the regret is\n\u2126(max{\n\u221a\nTV K, V\n\u221a\nT ln K}) = \u2126\n\u0010\u221a\nT\n\u0010\u221a\nV K + V\n\u221a\nln K\n\u0011\u0011\n.\nThere is a gap between our lower and upper bounds in this case. We do not know which bound is loose.\n4.4\nE\ufb03cient sampling for integral allocations\nAll that remains to specify in Algorithm 2 is the construction of the distribution p over subsets at every\nround. Since we don\u2019t know what the distribution is, we cannot sample from it easily it would seem. If K is\nsmall, one can use non-negative least squares to \ufb01nd the distribution that has the given marginals. However,\nonce the number of venues K is large, p is a distribution over\n\u0000K\nm\n\u0001\nsubsets, for which the least squares solver\nmight be too slow. One way around is to use the idea of greedy approximations in Hilbert Spaces from\nthe classic paper of (Jones, 1992). We can greedily construct a distribution on subsets which matches the\nmarginals on every element approximately in an e\ufb03cient manner. Exact sampling from the distribution\nwithout ever constructing it explicitly is also possible. The explicit algorithms giving the implementations\ncan be found in the full version of the paper.\n5\nExperimental results\nWe compared four methods experimentally. We refer to Algorithms 1 and 2 as ExpGrad and Exp3 re-\nspectively. We also run the Optimistic Kaplan Meier estimator based algorithm of (Ganchev et al., 2009),\nwhich is called OptKM. Finally we implemented the parametric maximum likelihood estimation-allocation\nbased algorithm described in (Ganchev et al., 2009) as well, which we call ParML. As we did not have\naccess to real dark pool data, we decided to implement a data simulator similar to (Ganchev et al., 2009).\n14\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600\n1800\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nRound\nCumulative Reward\nCumulative Reward at Each Round\n \n \nExp3\nExpGrad\nOptKM\nParML\nFigure 1: Cumulative rewards for each algorithm as a function of the number of rounds when run on the parametric\nmodel of (Ganchev et al., 2009) averaged over 100 trials\n0\n0.5\n1\n1.5\n2\n2.5\nx 10\n4\n40\n60\n80\n100\n120\n140\n160\nRound\nAllocation\n2 Venue Allocations\n \n \nExp3\nExpGrad\nOptKM\nParML\n0\n0.5\n1\n1.5\n2\n2.5\nx 10\n4\n0\n0.5\n1\n1.5\n2\n2.5\n3 x 10\n6\nRound\nCumulative Reward\nCumulative Reward at Each Round\n \n \nExp3\nExpGrad\nOptKM\nParML\n(a)\n(b)\nFigure 2: Allocations to the 2 venues and cumulative rewards for the di\ufb00erent algorithms. Note the inability of\nParML and OptKM to e\ufb00ectively switch between venues when distributions switch. ExpGrad\nand Exp3 also\nachieve higher cumulative rewards.\nWe used a combination of a Zero Bin parameter and power law distribution to generate the st\ni\u2019s while the\nsequence V t was kept \ufb01xed. Parameters for the Zero Bin and power law were set to lie in the same regimes\nas the ones observed in the real data of (Ganchev et al., 2009).\nWe started by generating the data from the parametric model of (Ganchev et al., 2009). We used 48\nvenues, T = 2000 to match the experiments of (Ganchev et al., 2009). The values of si\nt\u2019s were sampled\niid from Zero Bin+Power law distributions with appropriately chosen parameters. A plot of the resulting\ncumulative rewards averaged over 100 trial runs can be seen in Figure 1.\nWe see that ParML has a slightly superior performance on this data, understandably as the data is being\ngenerated from the speci\ufb01c parametric model that the algorithm is designed for. However, ExpGrad gets\nnet allocations quite close to ParML. Furthermore, both Exp3\nand ExpGrad\nare far superior to the\nperformance OptKM which is our true competitor in some sense being a non-parametric approach just like\nours.\nNext, we study the performance of all four algorithms under a variety of adversarial scenarios. We start\nwith a simple setup of two venues. The parameters of the power law initially favor Venue 1 for 12500 rounds,\nand then we switch the power law parameters to favor Venue 2. We study both the cumulative rewards\nas well as the allocations to both venues for each algorithm. Clearly an algorithm will be more robust to\nadversarial perturbations if it can detect this change quickly and switch its allocations accordingly. We show\nthe results of this experiment in Figure 2.\nBecause of just 2 venues, rounding has a rather negligible e\ufb00ect in this case and both our methodshave\nan almost identical performance. Our algorithms ExpGrad\nand Exp3\nswitch much faster to the new\noptimal venue when distributions switch. Consequently, the cumulative reward of both our algorithms also\nturns out signi\ufb01cantly higher as shown in Figure 2(b).\nWe wanted to investigate how this behavior changes when the switching involves a larger number of\nvenues. We created another experiment where there are 5 venues, maximum volume V = 200. Venues 1 and\n15\n0\n0.5\n1\n1.5\n2\n2.5\nx 10\n4\n0\n20\n40\n60\n80\n100\n120\n140\nRound\nAllocation\nExp3\n \n \nVenue1\nVenue2\nVenue3\nVenue4\nVenue5\n0\n0.5\n1\n1.5\n2\n2.5\nx 10\n4\n0\n20\n40\n60\n80\n100\n120\n140\nRound\nAllocation\nExpGrad\n \n \nVenue1\nVenue2\nVenue3\nVenue4\nVenue5\n(a)\n(b)\n0\n0.5\n1\n1.5\n2\n2.5\nx 10\n4\n0\n20\n40\n60\n80\n100\n120\n140\n160\n180\n200\nRound\nAllocation\nOptKM\n \n \nVenue1\nVenue2\nVenue3\nVenue4\nVenue5\n0\n0.5\n1\n1.5\n2\n2.5\nx 10\n4\n0\n50\n100\n150\nRound\nAllocation\nParametric ML\n \n \nVenue1\nVenue2\nVenue3\nVenue4\nVenue5\n(c)\n(d)\nFigure 3: Allocations to the 5 venues for the di\ufb00erent algorithms. Note the poor switching of OptKM between\nvenues when distributions switch. ParML completely fails on this problem. Exp3 and ExpGrad correctly identify\nboth long and short range trends (see text).\n0\n0.5\n1\n1.5\n2\n2.5\nx 10\n4\n0\n0.5\n1\n1.5\n2\n2.5 x 10\n6\nRound\nCumulative Reward\nCumulative Reward at Each Round\n \n \nExp3\nExpGrad\nOptKM\nParML\n0\n0.5\n1\n1.5\n2\n2.5\nx 10\n4\n0\n1\n2\n3\n4\n5\n6 x 10\n6\nRound\nCumulative Reward\nCumulative Reward at Each Round\n \n \nExp3\nExpGrad\nOptKM\nParML\n(a)\n(b)\nFigure 4: Cumulative rewards for each algorithm when distributions switch between 5 venues, for V = 200(left) and\nV = 400. Note the superior performance of ExpGrad andExp3.\n5 oscillate between getting very favorable and unfavorable \u03b2 values (\u03b2 is the power law exponent). Other\nvenues also switch, but between less extreme values. Allocations to all 5 venues for each algorithm are shown\nin Figure 3.\nOnce again both Exp3 and ExpGrad identify both the long range trend (favorability of venues 1, 5\nover the others) and short range trend (favoring venue 1 over 5 in certain phases). There is a gap between\nExp3and ExpGrad this time, however, as rounding does start to play a role with 5 venues. OptKM adapts\nsomewhat, although it still doesn\u2019t reach as high an allocation level as Exp3 after switching to a new venue.\nParML completely fails to identify this switching. We also studied the behavior of algorithms as V is scaled\non the same problem. Figure 4 plots the cumulative reward of each algorithm for V = 200 and V = 400. It\nis clear that ExpGrad and Exp3 still comprehensively outperform others.\nIn summary, it seems that our algorithms are competitive with those of (Ganchev et al., 2009) when\nthe data is drawn from their parametric model. When their assumptions about iid data are not satis\ufb01ed,\nwe signi\ufb01cantly outperform those algorithms.\nWe note that we have only experimented with oblivious\nadversaries here. The gulf in performance may be even wider for adaptive adversaries.\nReferences\nAbernethy, J., Agarwal, A., Bartlett, P. L., and Rakhlin, A. (2009). A stochastic view of optimal regret through\nminimax duality. In Proceedings of the 22nd Annual Conference on Learning Theory.\n16\nAbernethy, J. and Rakhlin, A. (2009). Beating the adaptive bandit with high probability. In Proceedings of COLT\n2009.\nAuer, P., Cesa-Bianchi, N., Freund, Y., and Schapire, R. E. (2003). The nonstochastic multiarmed bandit problem.\nSIAM J. Comput., 32(1):48\u201377.\nBarron, A. (1993). Universal approximation bounds for superpositions of a sigmoidal function. Information Theory,\nIEEE Transactions on, 39(3):930\u2013945.\nBartlett, P. L., Dani, V., Hayes, T. P., Kakade, S. M., Rakhlin, A., and Tewari, A. (2008). High-probability regret\nbounds for bandit online linear optimization. In Proceedings of COLT 2008.\nCesa-Bianchi, N., Conconi, A., and Gentile, C. (2001). On the generalization ability of on-line learning algorithms.\nIEEE Transactions on Information Theory, 50:2050\u20132057.\nCesa-Bianchi, N. and Lugosi, G. (2006). Prediction, Learning and Games. Cambridge University Press.\nGanchev, K., Kearns, M., Nevmyvaka, Y., and Vaughan, J. W. (2009). Censored exploration and the dark pool\nproblem. In Proceedings of Uncertainity in Arti\ufb01cial Intelligence, UAI 2009.\nHaussler, D., Kivinen, J., and Warmuth, M. K. (1998). Sequential prediction of individual sequences under general\nloss functions. IEEE Transactions on Information Theory, 44(5):1906\u20131925.\nJones, L. K. (1992). A simple lemma on greedy approximation in Hilbert space and convergence rates for projection\npursuit regression and neural network training. The Annals of Statistics, 20(1).\nLittlestone, N. and Warmuth, M. K. (1994). The weighted majority algorithm. Inf. Comput., 108(2):212\u2013261.\nYu, B. (1993). Assouad, Fano and Le Cam. Festschrift in Honor of L. Le Cam on his 70th Birthday.\n17\n",
        "sentence": " Many classes of problems, including prediction with expert advice [1], the multi-armed bandit problem [2], dynamic pricing [3], the dark pool problem [4], label efficient prediction [5], and linear and convex optimization with full or bandit feedback [6, 7] can be modeled as an instance of partial monitoring. [4] Alekh Agarwal, Peter L.",
        "context": "adversarial setups leads to improved performance in simulations.\n4.3\nLower bound on regret for integral allocations\nAs mentioned in the previous section, the problem of K-armed bandits is a special case of the dark pools\nin the experts prediction problem, the algorithm picks an expert from 1, . . . , K according to a probability\ndistribution pt at round t. Then it receives a vector of rewards \u03c1t with \u03c1t,i \u2208[0, 1],\ni = 1, . . . , K. In order\n4\nCesa-Bianchi, N. and Lugosi, G. (2006). Prediction, Learning and Games. Cambridge University Press.\nGanchev, K., Kearns, M., Nevmyvaka, Y., and Vaughan, J. W. (2009). Censored exploration and the dark pool"
    },
    {
        "title": "Minimizing regret with label efficient prediction",
        "author": [
            "Nicol\u00f2 Cesa-Bianchi",
            "G\u00e1bor Lugosi",
            "Gilles Stoltz"
        ],
        "venue": "IEEE Transactions on Information Theory,",
        "citeRegEx": "5",
        "shortCiteRegEx": "5",
        "year": 2005,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Many classes of problems, including prediction with expert advice [1], the multi-armed bandit problem [2], dynamic pricing [3], the dark pool problem [4], label efficient prediction [5], and linear and convex optimization with full or bandit feedback [6, 7] can be modeled as an instance of partial monitoring. [5] Nicol\u00f2 Cesa-Bianchi, G\u00e1bor Lugosi, and Gilles Stoltz.",
        "context": null
    },
    {
        "title": "Online convex programming and generalized infinitesimal gradient ascent",
        "author": [
            "Martin Zinkevich"
        ],
        "venue": "In ICML, pages 928\u2013936,",
        "citeRegEx": "6",
        "shortCiteRegEx": "6",
        "year": 2003,
        "abstract": "",
        "full_text": "",
        "sentence": " Many classes of problems, including prediction with expert advice [1], the multi-armed bandit problem [2], dynamic pricing [3], the dark pool problem [4], label efficient prediction [5], and linear and convex optimization with full or bandit feedback [6, 7] can be modeled as an instance of partial monitoring. [6] Martin Zinkevich.",
        "context": null
    },
    {
        "title": "Stochastic linear optimization under bandit feedback",
        "author": [
            "Varsha Dani",
            "Thomas P. Hayes",
            "Sham M. Kakade"
        ],
        "venue": "In COLT,",
        "citeRegEx": "7",
        "shortCiteRegEx": "7",
        "year": 2008,
        "abstract": "",
        "full_text": "",
        "sentence": " Many classes of problems, including prediction with expert advice [1], the multi-armed bandit problem [2], dynamic pricing [3], the dark pool problem [4], label efficient prediction [5], and linear and convex optimization with full or bandit feedback [6, 7] can be modeled as an instance of partial monitoring. [7] Varsha Dani, Thomas P.",
        "context": null
    },
    {
        "title": "Discrete prediction games with arbitrary feedback and loss",
        "author": [
            "Antonio Piccolboni",
            "Christian Schindelhauer"
        ],
        "venue": "In COLT,",
        "citeRegEx": "8",
        "shortCiteRegEx": "8",
        "year": 2001,
        "abstract": "",
        "full_text": "",
        "sentence": " 1 Related work The paper by Piccolboni and Schindelhauer [8] is one of the first to study the regret of the finite partial monitoring problem. We compared Random, FeedExp3 [8], CBP [11] with \u03b1 = 1. [8] Antonio Piccolboni and Christian Schindelhauer.",
        "context": null
    },
    {
        "title": "Regret minimization under partial monitoring",
        "author": [
            "Nicol\u00f2 Cesa-Bianchi",
            "G\u00e1bor Lugosi",
            "Gilles Stoltz"
        ],
        "venue": "Math. Oper. Res.,",
        "citeRegEx": "9",
        "shortCiteRegEx": "9",
        "year": 2006,
        "abstract": " We consider repeated games in which the player, instead of observing the action chosen by the opponent in each game round, receives a feedback generated by the combined choice of the two players. We study Hannan-consistent players for these games, that is, randomized playing strategies whose per-round regret vanishes with probability one as the number n of game rounds goes to infinity. We prove a general lower bound of \u03a9(n\u22121/3) for the convergence rate of the regret, and exhibit a specific strategy that attains this rate for any game for which a Hannan-consistent player exists. ",
        "full_text": "",
        "sentence": " [9] to O(T ), who also showed an instance in which the bound is optimal. [9] Nicol\u00f2 Cesa-Bianchi, G\u00e1bor Lugosi, and Gilles Stoltz.",
        "context": null
    },
    {
        "title": "Minimax regret of finite partial-monitoring games in stochastic environments",
        "author": [
            "G\u00e1bor Bart\u00f3k",
            "D\u00e1vid P\u00e1l",
            "Csaba Szepesv\u00e1ri"
        ],
        "venue": "In COLT,",
        "citeRegEx": "10",
        "shortCiteRegEx": "10",
        "year": 2011,
        "abstract": "",
        "full_text": "",
        "sentence": " [10] classified the partial monitoring problems into four categories in terms of the minimax regret: a trivial problem with zero regret, an easy problem with \u0398\u0303( \u221a T ) regret1, a hard problem with \u0398(T ) regret, and a hopeless problem with \u0398(T ) regret. [10]) in which the number of rows of Si is the number of the different symbols in the i-th row of H . [10] G\u00e1bor Bart\u00f3k, D\u00e1vid P\u00e1l, and Csaba Szepesv\u00e1ri.",
        "context": null
    },
    {
        "title": "An adaptive algorithm for finite stochastic partial monitoring",
        "author": [
            "G\u00e1bor Bart\u00f3k",
            "Navid Zolghadr",
            "Csaba Szepesv\u00e1ri"
        ],
        "venue": "In ICML,",
        "citeRegEx": "11",
        "shortCiteRegEx": "11",
        "year": 2012,
        "abstract": "We present a new anytime algorithm that achieves near-optimal regret for any\ninstance of finite stochastic partial monitoring. In particular, the new\nalgorithm achieves the minimax regret, within logarithmic factors, for both\n\"easy\" and \"hard\" problems. For easy problems, it additionally achieves\nlogarithmic individual regret. Most importantly, the algorithm is adaptive in\nthe sense that if the opponent strategy is in an \"easy region\" of the strategy\nspace then the regret grows as if the problem was easy. As an implication, we\nshow that under some reasonable additional assumptions, the algorithm enjoys an\nO(\\sqrt{T}) regret in Dynamic Pricing, proven to be hard by Bartok et al.\n(2011).",
        "full_text": "An adaptive algorithm for \ufb01nite stochastic partial monitoring\nG\u00b4abor Bart\u00b4ok\nbartok@ualberta.ca\nNavid Zolghadr\nzolghadr@ualberta.ca\nCsaba Szepesv\u00b4ari\nszepesva@ualberta.ca\nDepartment of Computing Science, University of Alberta, AB, Canada, T6G 2E8\nAbstract\nWe present a new anytime algorithm that\nachieves near-optimal regret for any instance\nof \ufb01nite stochastic partial monitoring. In par-\nticular, the new algorithm achieves the min-\nimax regret, within logarithmic factors, for\nboth \u201ceasy\u201d and \u201chard\u201d problems. For easy\nproblems, it additionally achieves logarithmic\nindividual regret. Most importantly, the al-\ngorithm is adaptive in the sense that if the\nopponent strategy is in an \u201ceasy region\u201d of\nthe strategy space then the regret grows as if\nthe problem was easy. As an implication, we\nshow that under some reasonable additional\nassumptions, the algorithm enjoys an O(\n\u221a\nT)\nregret in Dynamic Pricing, proven to be hard\nby Bart\u00b4ok et al. (2011).\n1. Introduction\nPartial monitoring can be cast as a sequential game\nplayed by a learner and an opponent. In every time\nstep, the learner chooses an action and simultaneously\nthe opponent chooses an outcome. Then, based on the\naction and the outcome, the learner su\ufb00ers some loss\nand receives some feedback. Neither the outcome nor\nthe loss are revealed to the learner. Thus, a partial-\nmonitoring game with N actions and M outcomes is\nde\ufb01ned with the pair G = (L, H), where L \u2208RN\u00d7M\nis the loss matrix, and H \u2208\u03a3N\u00d7M is the feedback\nmatrix over some arbitrary set of symbols \u03a3. These\nmatrices are announced to both the learner and the\nopponent before the game starts. At time step t, if\nIt \u2208N = {1, 2, . . . , N} and Jt \u2208M = {1, 2, . . . , M}\ndenote the (possibly random) choices of the learner\nand the opponent, respectively then, the loss su\ufb00ered\nby the learner in that time step is L[It, Jt], while the\nAppearing in Proceedings of the 29 th International Confer-\nence on Machine Learning, Edinburgh, Scotland, UK, 2012.\nCopyright 2012 by the author(s)/owner(s).\nfeedback received is H[It, Jt].\nThe goal of the learner (or player) is to minimize his\ncumulative loss PT\nt=1 L[It, Jt]. The performance of the\nlearner is measured in terms of the regret, de\ufb01ned as\nthe excess cumulative loss he su\ufb00ers compared to that\nof the best \ufb01xed action in hindsight:\nRT =\nT\nX\nt=1\nL[It, Jt] \u2212min\ni\u2208N\nT\nX\nt=1\nL[i, Jt] .\nThe regret usually grows with the time horizon T.\nWhat distinguishes between a \u201csuccessful\u201d and an \u201cun-\nsuccessful\u201d learner is the growth rate of the regret. A\nregret linear in T means that the learner does not ap-\nproach the performance of the optimal action. On the\nother hand, if the growth rate is sublinear, it is said\nthat the learner can learn the game.\nIn this paper we restrict our attention to stochastic\ngames, adding the extra assumption that the oppo-\nnent generates the outcomes with a sequence of inde-\npendent and identically distributed random variables.\nThis distribution will be called the opponent strategy.\nAs for the player, a player strategy (or algorithm) is\na (possibly random) function from the set of feedback\nsequences (observation histories) to the set of actions.\nIn stochastic games, we use a slightly di\ufb00erent notion\nof regret: we compare the cumulative loss with that of\nthe action with the lowest expected loss.\nRT =\nT\nX\nt=1\nL[It, Jt] \u2212T min\ni\u2208N E[L[i, J1]] .\nThe \u201chardness\u201d of a game is de\ufb01ned in terms of the\nminimax expected regret (or minimax regret for short):\nRT (G) = min\nA\nmax\np\u2208\u2206M E[RT ] ,\nwhere \u2206M is the space of opponent strategies, and\nA is any strategy of the player. In other words, the\nAdaptive Stochastic Partial Monitoring\nminimax regret is the worst-case expected regret of the\nbest algorithm.\nA question of major importance is how the minimax\nregret scales with the parameters of the game, such as\nthe time horizon T, the number of actions N, the num-\nber of outcomes M. In the stochastic setting, another\nmeasure of \u201chardness\u201d is worth studying, namely the\nindividual or problem-dependent regret, de\ufb01ned as the\nexpected regret given a \ufb01xed opponent strategy.\n1.1. Related work\nTwo special cases of partial monitoring have been\nextensively studied for a long time: full-information\ngames, where the feedback carries enough informa-\ntion for the learner to infer the outcome for any\naction-outcome pair, and bandit games, where the\nlearner receives the loss of the chosen action as feed-\nback. Since Vovk (1990) and Littlestone & Warmuth\n(1994) we know that for full-information games, the\nminimax regret scales as \u0398(\u221aT log N).\nFor bandit\ngames, the minimax regret has been proven to scale as\n\u0398(\n\u221a\nNT) (Audibert & Bubeck, 2009).1 The individ-\nual regret of these kind of games has also been studied:\nAuer et al. (2002) showed that given any opponent\nstrategy, the expected regret can be upper bounded\nby c P\ni\u2208N:\u03b4i\u0338=0\n1\n\u03b4i log T, where \u03b4i is the expected dif-\nference between the loss of action i and an optimal\naction.\nFinite partial monitoring problems were introduced by\nPiccolboni & Schindelhauer (2001). They proved that\na game is either \u201chopeless\u201d (that is, its minimax re-\ngret scales linearly with T), or the regret can be upper\nbounded by O(T 3/4). They also give a characteriza-\ntion of hopeless games. Namely, a game is hopeless\nif it does not satisfy the global observability condition\n(see De\ufb01nition 5 in Section 2).\nTheir upper bound\nfor non-hopeless games was tightened to O(T 2/3) by\nCesa-Bianchi et al. (2006), who also showed that there\nexists a game with a matching lower bound.\nCesa-Bianchi et al. (2006) posted the problem of char-\nacterizing partial-monitoring games with minimax re-\ngret less than \u0398(T 2/3). This problem has been solved\nsince then. The \ufb01rst steps towards classifying partial-\nmonitoring games were made by Bart\u00b4ok et al. (2010),\nwho characterized almost all games with two outcomes.\nThey proved that there are only four categories: games\nwith minimax regret 0, e\u0398(\n\u221a\nT), \u0398(T 2/3), and \u0398(T),\nand named them trivial, easy, hard, and hopeless, re-\n1The Exp3 algorithm due to Auer et al. (2003) achieves\nalmost the same regret, with an extra logarithmic term.\nspectively.2\nThey also found that there exist games\nthat are easy, but can not easily be \u201ctransformed\u201d to\na bandit or full-information game. Later, Bart\u00b4ok et al.\n(2011) proved the same results for \ufb01nite stochastic par-\ntial monitoring, with any \ufb01nite number of outcomes.\nThe condition that separates easy games from hard\ngames is the local observability condition (see De\ufb01ni-\ntion 6).\nThe algorithm Balaton introduced there\nworks by eliminating actions that are thought to be\nsuboptimal with high con\ufb01dence. They conjectured in\ntheir paper that the same classi\ufb01cation holds for non-\nstochastic games, without changing the condition. Re-\ncently, Foster & Rakhlin (2011) designed the algorithm\nNeighborhoodWatch that proves this conjecture to\nbe true. Foster & Rakhlin prove an upper bound on a\nstronger notion of regret, called internal regret.\n1.2. Contributions\nIn this paper, we extend the results of Bart\u00b4ok et al.\n(2011). We introduce a new algorithm, called CBP\nfor \u201cCon\ufb01dence Bound Partial monitoring\u201d, with var-\nious desirable properties. First of all, while Balaton\nonly works on easy games, CBP can be run on any\nnon-hopeless game, and it achieves (up to logarithmic\nfactors) the minimax regret rates both for easy and\nhard games (see Corollaries 3 and 2). Furthermore, it\nalso achieves logarithmic problem-dependent regret for\neasy games (see Corollary 1). It is also an \u201canytime\u201d\nalgorithm, meaning that it does not have to know the\ntime horizon, nor does it have to use the doubling trick,\nto achieve the desired performance.\nThe \ufb01nal, and potentially most impactful, aspect of\nour algorithm is that through additional assumptions\non the set of opponent strategies, the minimax regret\nof even hard games can be brought down to e\u0398(\n\u221a\nT)!\nWhile this statement may seem to contradict the re-\nsult of Bart\u00b4ok et al. (2011), in fact it does not. For the\nprecise statement, see Theorem 2. We call this prop-\nerty \u201cadaptiveness\u201d to emphasize that the algorithm\ndoes not even have to know that the set of opponent\nstrategies is restricted.\n2. De\ufb01nitions and notations\nRecall from the introduction that an instance of partial\nmonitoring with N actions and M outcomes is de\ufb01ned\nby the pair of matrices L \u2208RN\u00d7M and H \u2208\u03a3N\u00d7M,\nwhere \u03a3 is an arbitrary set of symbols. In each round\nt, the opponent chooses an outcome Jt \u2208M and simul-\ntaneously the learner chooses an action It \u2208N. Then,\n2Note that these results do not concern the growth rate\nin terms of other parameters (like N).\nAdaptive Stochastic Partial Monitoring\nthe feedback H[It, Jt] is revealed and the learner suf-\nfers the loss L[It, Jt]. It is important to note that the\nloss is not revealed to the learner.\nAs it was previously mentioned, in this paper we deal\nwith stochastic opponents only. In this case, the choice\nof the opponent is governed by a sequence J1, J2, . . .\nof i.i.d. random variables. The distribution of these\nvariables p \u2208\u2206M is called an opponent strategy, where\n\u2206M, also called the probability simplex, is the set of\nall distributions over the M outcomes. It is easy to\nsee that, given opponent strategy p, the expected loss\nof action i can be expressed as \u2113\u22a4\ni p, where \u2113i is de\ufb01ned\nas the column vector consisting of the ith row of L.\nThe following de\ufb01nitions, taken from Bart\u00b4ok et al.\n(2011), are essential for understanding how the struc-\nture of L and H determines the \u201chardness\u201d of a game.\nAction i is called optimal under strategy p if its ex-\npected loss is not greater than that of any other ac-\ntion i\u2032 \u2208N. That is, \u2113\u22a4\ni p \u2264\u2113\u22a4\ni\u2032 p. Determining which\naction is optimal under opponent strategies yields the\ncell decomposition3 of the probability simplex \u2206M:\nDe\ufb01nition\n1\n(Cell\ndecomposition).\nFor\nevery\naction\ni\n\u2208\nN,\nlet\nCi\n=\n{p\n\u2208\n\u2206M\n:\naction i is optimal under p}.\nThe sets\nC1, . . . , CN constitute the cell decomposition of \u2206M.\nNow we can de\ufb01ne the following important properties\nof actions:\nDe\ufb01nition 2 (Properties of actions).\n\u2022 Action i is\ncalled dominated if Ci = \u2205. If an action is not\ndominated then it is called non-dominated.\n\u2022 Action\ni\nis\ncalled\ndegenerate\nif\nit\nis\nnon-\ndominated and there exists an action i\u2032 such that\nCi \u228aCi\u2032.\n\u2022 If an action is neither dominated nor degener-\nate then it is called Pareto-optimal. The set of\nPareto-optimal actions is denoted by P.\nFrom the de\ufb01nition of cells we see that a cell is either\nempty or it is a closed polytope. Furthermore, Pareto-\noptimal actions have (M \u22121)-dimensional cells. The\nfollowing de\ufb01nition, important for our algorithm, also\nuses the dimensionality of polytopes:\nDe\ufb01nition 3 (Neighbors). Two Pareto-optimal ac-\ntions i and j are neighbors if Ci \u2229Cj is an (M \u22122)-\ndimensional polytope. Let N be the set of unordered\npairs over N that contains neighboring action-pairs.\nThe neighborhood action set of two neighboring ac-\ntions i, j is de\ufb01ned as N +\ni,j = {k \u2208N : Ci \u2229Cj \u2286Ck}.\n3The concept of cell decomposition also appears in Pic-\ncolboni & Schindelhauer (2001).\nNote that the neighborhood action set N +\ni,j naturally\ncontains i and j. If N +\ni,j contains some other action k\nthen either Ck = Ci, Ck = Cj, or Ck = Ci \u2229Cj.\nIn general, the elements of the feedback matrix H can\nbe arbitrary symbols. Nevertheless, the nature of the\nsymbols themselves does not matter in terms of the\nstructure of the game. What determines the feedback\nstructure of a game is the occurrence of identical sym-\nbols in each row of H. To \u201cstandardize\u201d the feedback\nstructure, the signal matrix is de\ufb01ned for each action:\nDe\ufb01nition 4. Let si be the number of distinct sym-\nbols in the ith row of H and let \u03c31, . . . , \u03c3si\n\u2208\u03a3\nbe an enumeration of those symbols.\nThen the sig-\nnal matrix Si \u2208{0, 1}si\u00d7M of action i is de\ufb01ned as\nSi[k, l] = I{H[i,l]=\u03c3k}.\nThe idea of this de\ufb01nition is that if p \u2208\u2206M is the op-\nponent\u2019s strategy then Sip gives the distribution over\nthe symbols underlying action i. In fact, it is also true\nthat observing H[It, Jt] is equivalent to observing the\nvector SIteJt, where ek is the kth unit vector in the\nstandard basis of RM. From now on we assume with-\nout loss of generality that the learner\u2019s observation at\ntime step t is the random vector Yt = SIteJt. Note\nthat the dimensionality of this vector depends on the\naction chosen by the learner, namely Yt \u2208RsIt .\nThe following two de\ufb01nitions play a key role in classify-\ning partial-monitoring games based on their di\ufb03culty.\nDe\ufb01nition 5 (Global observability (Piccolboni &\nSchindelhauer, 2001)). A partial-monitoring game\n(L, H) admits the global observability condition, if for\nall pairs i, j of actions, \u2113i \u2212\u2113j \u2208\u2295k\u2208N Im S\u22a4\nk .\nDe\ufb01nition 6 (Local observability (Bart\u00b4ok et al.,\n2011)). A pair of neighboring actions i, j is said to\nbe locally observable if \u2113i \u2212\u2113j \u2208\u2295k\u2208N +\ni,j Im S\u22a4\nk . We\ndenote by L \u2282N the set of locally observable pairs of\nactions (the pairs are unordered). A game satis\ufb01es the\nlocal observability condition if every pair of neighbor-\ning actions is locally observable, i.e., if L = N.\nThe main result of Bart\u00b4ok et al. (2011) is that lo-\ncally observable games have eO(\n\u221a\nT) minimax regret.\nIt is easy to see that local observability implies global\nobservability. Also, from Piccolboni & Schindelhauer\n(2001) we know that if global observability does not\nhold then the game has linear minimax regret. From\nnow on, we only deal with games that admit the global\nobservability condition.\nA collection of the concepts and symbols introduced\nin this section is shown in Table 1.\nAdaptive Stochastic Partial Monitoring\nTable 1. List of basic symbols\nSymbol\nDe\ufb01nition\nFound in/at\nN, M \u2208N\nnumber of actions and outcomes\nN\n{1, . . . , N}, set of actions\n\u2206M \u2282RM\nM-dim. simplex, set of opponent strategies\np\u2217\u2208\u2206M\nopponent strategy\nL \u2208RN\u00d7M\nloss matrix\nH \u2208\u03a3N\u00d7M\nfeedback matrix\n\u2113i \u2208RM\n\u2113i = L[i, :], loss vector underlying action i\nCi \u2286\u2206M\ncell of action i\nDe\ufb01nition 1\nP \u2286N\nset of Pareto-optimal actions\nDe\ufb01nition 2\nN \u2286N 2\nset of unordered neighboring action-pairs\nDe\ufb01nition 3\nN +\ni,j \u2286N\nneighborhood action set of {i, j} \u2208N\nDe\ufb01nition 3\nSi \u2208{0, 1}si\u00d7M\nsignal matrix of action i\nDe\ufb01nition 4\nL \u2286N\nset of locally observable action pairs\nDe\ufb01nition 6\nVi,j \u2286N\nobserver actions underlying {i, j} \u2208N\nDe\ufb01nition 7\nvi,j,k \u2208Rsk, k \u2208Vi,j\nobserver vectors\nDe\ufb01nition 7\nWi \u2208R\ncon\ufb01dence width for action i \u2208N\nDe\ufb01nition 7\n3. The proposed algorithm\nOur algorithm builds on the core idea underlying al-\ngorithm Balaton of Bart\u00b4ok et al. (2011), so we start\nwith a brief review of Balaton.\nBalaton uses\nsweeps to successively eliminate suboptimal actions.\nThis is done by estimating the di\ufb00erences between\nthe expected losses of pairs of actions, i.e., \u03b4i,j =\n(\u2113i\u2212\u2113j)\u22a4p\u2217(i, j \u2208N). In fact, Balaton exploits that\nit su\ufb03ces to keep track of \u03b4i,j for neighboring pairs of\nactions (i.e., for action pairs i, j such that {i, j} \u2208N).\nThis is because if an action i is suboptimal, it will\nhave a neighbor j that has a smaller expected loss\nand so the action i will get eliminated when \u03b4i,j is\nchecked. Now, to estimate \u03b4i,j for some {i, j} \u2208N one\nobserves that under the local observability condition,\nit holds that \u2113i \u2212\u2113j = P\nk\u2208N +\ni,j S\u22a4\ni vi,j,k for some vec-\ntors vi,j,k \u2208R\u03c3k. This yields that \u03b4i,j = (\u2113i \u2212\u2113j)\u22a4p\u2217=\nP\nk\u2208N +\ni,j v\u22a4\ni,j,kSkp\u2217.\nSince \u03bdk\ndef\n= Skp\u2217is the vector\nof the distribution of symbols under action k, which\ncan be estimated by \u03bdk(t), the empirical frequencies of\nthe individual symbols observed under k up to time\nt, Balaton uses P\nk\u2208N +\ni,j v\u22a4\ni,j,k\u03bdk(t) to estimate \u03b4i,j.\nSince none of the actions in N +\ni,j can get eliminated\nbefore one of {i, j} gets eliminated, the estimate of\n\u03b4i,j gets re\ufb01ned until one of {i, j} is eliminated.\nThe essence of why Balaton achieves a low regret is\nas follows: When i is not a neighbor of the optimal\naction i\u2217one can show that it will be eliminated be-\nfore all neighbors j \u201cbetween i and i\u2217\u201d get eliminated.\nThus, the contribution of such \u201cfar\u201d actions to the re-\ngret is minimal. When i is a neighbor of i\u2217, it will\nbe eliminated in time proportional to \u03b4\u22122\ni,i\u2217. Thus the\ncontribution to the regret of such an action is propor-\ntional to \u03b4\u22121\ni\n, where \u03b4i\ndef\n= \u03b4i,i\u2217. It also holds that the\ncontribution to the regret of i cannot be larger than\n\u03b4iT.\nThus, the contribution of i to the regret is at\nmost min(\u03b4iT, \u03b4\u22121\ni\n) \u2264\n\u221a\nT.\nWhen some pairs {i, j} \u2208N are not locally observable,\none needs to use actions other than those in N +\ni,j to\nconstruct an estimate of \u03b4i,j. Under global observabil-\nity, \u2113i\u2212\u2113j = P\nk\u2208Vi,j S\u22a4\ni vi,j,k for an appropriate subset\nVi,j \u2282N and an appropriate set of vectors vi,j,\u00b7. Thus,\nif the actions in Vi,j are kept in play, one can estimate\nthe di\ufb00erence \u03b4i,j as before, using P\nk\u2208N +\ni,j v\u22a4\ni,j,k\u03bdk(t).\nThis motivates the following de\ufb01nition:\nDe\ufb01nition 7 (Observer sets and observer vectors).\nThe observer set Vi,j \u2282N underlying a pair of neigh-\nboring actions {i, j} \u2208N is a set of actions such that\n\u2113i \u2212\u2113j \u2208\u2295k\u2208Vi,j Im S\u22a4\nk .\nThe observer vectors (vi,j,k)k\u2208Vi,j are de\ufb01ned to sat-\nisfy the equation \u2113i \u2212\u2113j = P\nk\u2208Vi,j S\u22a4\nk vi,j,k. In par-\nticular, vi,j,k \u2208Rsk.\nIn what follows, the choice\nof the observer sets and vectors is restricted so that\nVi,j = Vj,i and vi,j,k = \u2212vj,i,k. Furthermore, the ob-\nserver set Vi,j is constrained to be a superset of N +\ni,j\nand in particular when a pair {i, j} is locally observ-\nable, Vi,j = N +\ni,j must hold. Finally, for any action\nk \u2208S\n{i,j}\u2208N N +\ni,j, let Wk = maxi,j:k\u2208Vi,j \u2225vi,j,k\u2225\u221ebe\nthe con\ufb01dence width of action k.\nThe reason of the particular choice Vi,j = N +\ni,j for lo-\nAdaptive Stochastic Partial Monitoring\ncally observable pairs {i, j} is that we plan to use Vi,j\n(and the vectors vi,j,\u00b7) in the case of locally observable\npairs, too. For not locally observable pairs, the whole\naction set N is always a valid observer set (thus, Vi,j\ncan be found). However, whenever possible, it is bet-\nter to use a smaller set. The actual choice of Vi,j (and\nvi,j,k) is postponed until the e\ufb00ect of this choice on the\nregret becomes clear.\nWith the observer sets, the basic idea of the algorithm\nbecomes as follows: (i) Eliminate the suboptimal ac-\ntions in successive sweeps; (ii) In each sweep, enrich\nthe set of remaining actions P(t) by adding the ob-\nserver actions underlying the remaining neighboring\npairs {i, j} \u2208N(t): V(t) = S\n{i,j}\u2208N (t) Vi,j; (iii) Ex-\nplore the actions in P(t) \u222aV(t) to update the symbol\nfrequency estimate vectors \u03bdk(t). Another re\ufb01nement\nis to eliminate the sweeps so as to make the algorithm\nenjoy an advantageous anytime property. This can be\nachieved by selecting in each step only one action. We\npropose the action to be chosen should be the one that\nmaximizes the reduction of the remaining uncertainty.\nThis algorithm could be shown to enjoy\n\u221a\nT regret for\nlocally observable games. However, if we run it on a\nnon-locally observable game and the opponent strat-\negy is on Ci \u2229Cj for {i, j} \u2208N \\ L, it will su\ufb00er linear\nregret! The reason is that if both actions i and j are\noptimal, and thus never get eliminated, the algorithm\nwill choose actions from Vi,j \\ N +\ni,j too often.\nFur-\nthermore, even if the opponent strategy is not on the\nboundary the regret can be too high: say action i is\noptimal but \u03b4j is small, while {i, j} \u2208N \\ L. Then\na third action k \u2208Vi,j with large \u03b4k will be chosen\nproportional to 1/\u03b42\nj times, causing high regret. To\ncombat this we restrict the frequency with which an\naction can be used for \u201cinformation seeking purposes\u201d.\nFor this, we introduce the set of rarely chosen actions,\nR(t) = {k \u2208N : nk(t) \u2264\u03b7kf(t)} ,\nwhere \u03b7k \u2208R, f : N \u2192R are tuning parameters to be\nchosen later. Then, the set of actions available at time\nt is restricted to P(t) \u222aN +(t) \u222a(V(t) \u2229R(t)), where\nN +(t) = S\n{i,j}\u2208N (t) N +\ni,j.\nWe will show that with\nthese modi\ufb01cations, the algorithm achieves O(T 2/3)\nregret in the general case, while it will also be shown\nto achieve an O(\n\u221a\nT) regret when the opponent uses\na benign strategy. A pseudocode for the algorithm is\ngiven in Algorithm 1.\nIt remains to specify the function getPolytope. It\ngets the array halfSpace as input.\nThe array half-\nSpace stores which neighboring action pairs have a\ncon\ufb01dent estimate on the di\ufb00erence of their expected\nlosses, along with the sign of the di\ufb00erence (if con\ufb01-\nAlgorithm 1 CBP\nInput: L, H, \u03b1, \u03b71, . . . , \u03b7N, f = f(\u00b7)\nCalculate P, N, Vi,j, vi,j,k, Wk\nfor t = 1 to N do\nChoose It = t and observe Yt\n{Initialization}\nnIt \u21901\n{# times the action is chosen}\n\u03bdIt \u2190Yt\n{Cumulative observations}\nend for\nfor t = N + 1, N + 2, . . . do\nfor each {i, j} \u2208N do\n\u02dc\u03b4i,j \u2190P\nk\u2208Vi,j v\u22a4\ni,j,k\n\u03bdk\nnk\n{Loss di\ufb00. estimate}\nci,j \u2190P\nk\u2208Vi,j \u2225vi,j,k\u2225\u221e\nq\n\u03b1 log t\nnk\n{Con\ufb01dence}\nif |\u02dc\u03b4i,j| \u2265ci,j then\nhalfSpace(i, j) \u2190sgn \u02dc\u03b4i,j\nelse\nhalfSpace(i, j) \u21900\nend if\nend for\n[P(t), N(t)] \u2190getPolytope(P, N, halfSpace)\nN +(t) = \u222a{i,j}\u2208N (t)N +\nij\nV(t) = \u222a{i,j}\u2208N (t)Vij\nR(t) = {k \u2208N : nk(t) \u2264\u03b7kf(t)}\nS(t) = P(t) \u222aN +(t) \u222a(V(t) \u2229R(t))\nChoose It = argmaxi\u2208S(t)\nW 2\ni\nni and observe Yt\n\u03bdIt \u2190\u03bdIt + Yt\nnIt \u2190nIt + 1\nend for\ndent). Each of these con\ufb01dent pairs de\ufb01ne an open\nhalfspace, namely\n\u2206{i,j} =\n\b\np \u2208\u2206M : halfSpace(i, j)(\u2113i \u2212\u2113j)\u22a4p > 0\n\t\n.\nThe function getPolytope calculates the open poly-\ntope de\ufb01ned as the intersection of the above halfspaces.\nThen for all i \u2208P it checks if Ci intersects with the\nopen polytope. If so, then i will be an element of P(t).\nSimilarly, for every {i, j} \u2208N, it checks if Ci \u2229Cj in-\ntersects with the open polytope and puts the pair in\nN(t) if it does.\nNote that it is not enough to compute P(t) and then\ndrop from N those pairs {k, l} where one of k or l is\nexcluded from P(t): it is possible that the boundary\nCk \u2229Cl between the cells of two actions k, l \u2208P(t) is\nincluded in the rejected region. For an illustration of\ncell decomposition and excluding cells, see Figure 1.\nComputational complexity\nThe computationally\nheavy parts of the algorithm are the initial calculation\nof the cell decomposition and the function getPoly-\ntope. All of these require linear programming. In the\npreprocessing phase we need to solve N + N 2 linear\nAdaptive Stochastic Partial Monitoring\n(1, 0, 0)\n(0, 1, 0)\n(0, 0, 1)\n(a) Cell decomposition.\n(1, 0, 0)\n(0, 1, 0)\n(0, 0, 1)\n(b) Gray\nindicates\nex-\ncluded area.\nFigure 1. An example of cell decomposition (M = 3).\nprograms to determine cells and neighboring pairs of\ncells.\nThen in every round, at most N 2 linear pro-\ngrams are needed. The algorithm can be sped up by\n\u201ccaching\u201d previously solved linear programs.\n4. Analysis of the algorithm\nThe \ufb01rst theorem in this section is an individual upper\nbound on the regret of CBP.\nTheorem 1. Let (L, H) be an N by M partial-\nmonitoring game. For a \ufb01xed opponent strategy p\u2217\u2208\n\u2206M, let \u03b4i denote the di\ufb00erence between the expected\nloss of action i and an optimal action. For any time\nhorizon T, algorithm CBP with parameters \u03b1 > 1,\n\u03bdk = W 2/3\nk\n, f(t) = \u03b11/3t2/3 log1/3 t has expected regret\nE[RT ] \u2264\nX\n{i,j}\u2208N\n2|Vi,j|\n\u0012\n1 +\n1\n2\u03b1 \u22122\n\u0013\n+\nN\nX\nk=1\n\u03b4k\n+\nN\nX\nk=1\n\u03b4k>0\n4W 2\nk\nd2\nk\n\u03b4k\n\u03b1 log T\n+\nX\nk\u2208V\\N +\n\u03b4k min\n \n4W 2\nk\nd2\nl(k)\n\u03b42\nl(k)\n\u03b1 log T,\n\u03b11/3W 2/3\nk\nT 2/3 log1/3 T\n!\n+\nX\nk\u2208V\\N +\n\u03b4k\u03b11/3W 2/3\nk\nT 2/3 log1/3 T\n+ 2dk\u03b11/3W 2/3T 2/3 log1/3 T ,\nwhere W = maxk\u2208N Wk, V = \u222a{i,j}\u2208N Vi,j, N + =\n\u222a{i,j}\u2208N N +\ni,j, and d1, . . . , dN are game-dependent con-\nstants.\nThe proof is omitted for lack of space.4 Here we give a\n4For complete proofs we refer the reader to the supple-\nmentary material.\nshort explanation of the di\ufb00erent terms in the bound.\nThe \ufb01rst term corresponds to the con\ufb01dence interval\nfailure event.\nThe second term comes from the ini-\ntialization phase of the algorithm. The remaining four\nterms come from categorizing the choices of the algo-\nrithm by two criteria: (1) Would It be di\ufb00erent if R(t)\nwas de\ufb01ned as R(t) = N?\n(2) Is It \u2208P(t) \u222aN +\nt ?\nThese two binary events lead to four di\ufb00erent cases in\nthe proof, resulting in the last four terms of the bound.\nAn implication of Theorem 1 is an upper bound on the\nindividual regret of locally observable games:\nCorollary 1. If G is locally observable then\nE[RT ] \u2264\nX\n{i,j}\u2208N\n2|Vi,j|\n\u0012\n1 +\n1\n2\u03b1 \u22122\n\u0013\n+\nN\nX\nk=1\n\u03b4k + 4W 2\nk\nd2\nk\n\u03b4k\n\u03b1 log T .\nProof. If a game is locally observable then V \\N + = \u2205,\nleaving the last two sums of the statement of Theo-\nrem 1 zero.\nThe following corollary is an upper bound on the min-\nimax regret of any globally observable game.\nCorollary 2. Let G be a globally observable game.\nThen there exists a constant c such that the expected\nregret can be upper bounded independently of the choice\nof p\u2217as\nE[RT ] \u2264cT 2/3 log1/3 T .\nThe following theorem is an upper bound on the min-\nimax regret of any globally observable game against\n\u201cbenign\u201d opponents. To state the theorem, we need a\nnew de\ufb01nition. Let A be some subset of actions in G.\nWe call A a point-local game in G if T\ni\u2208A Ci \u0338= \u2205.\nTheorem 2. Let G be a globally observable game. Let\n\u2206\u2032 \u2286\u2206M be some subset of the probability simplex\nsuch that its topological closure \u2206\u2032 has \u2206\u2032 \u2229Ci \u2229Cj = \u2205\nfor every {i, j} \u2208N \\ L. Then there exists a constant\nc such that for every p\u2217\u2208\u2206\u2032, algorithm CBP with\nparameters \u03b1 > 1, \u03bdk = W 2/3\nk\n, f(t) = \u03b11/3t2/3 log1/3 t\nachieves\nE[RT ] \u2264cdpmax\np\nbT log T ,\nwhere b is the size of the largest point-local game, and\ndpmax is a game-dependent constant.\nIn a nutshell, the proof revisits the four cases of the\nproof of Theorem 1, and shows that the terms which\nwould yield T 2/3 upper bound can be non-zero only\nfor a limited number of time steps.\nAdaptive Stochastic Partial Monitoring\nRemark 1. Note that the above theorem implies that\nCBP does not need to have any prior knowledge about\n\u2206\u2032 to achieve\n\u221a\nT regret. This is why we say our algo-\nrithm is \u201cadaptive\u201d.\nAn immediate implication of Theorem 2 is the follow-\ning minimax bound for locally observable games:\nCorollary 3. Let G be a locally observable \ufb01nite par-\ntial monitoring game. Then there exists a constant c\nsuch that for every p \u2208\u2206M,\nE[RT ] \u2264c\np\nT log T .\nRemark 2. The upper bounds in Corollaries 2 and 3\nboth have matching lower bounds up to logarith-\nmic factors (Bart\u00b4ok et al., 2011), proving that CBP\nachieves near optimal regret in both locally observable\nand non-locally observable games.\n5. Experiments\nWe demonstrate the results of the previous sections us-\ning instances of Dynamic Pricing, as well as a locally\nobservable game. We compare the results of CBP to\ntwo other algorithms: Balaton (Bart\u00b4ok et al., 2011)\nwhich is, as mentioned earlier in the paper, the \ufb01rst\nalgorithm that achieves eO(\n\u221a\nT) minimax regret for all\nlocally observable \ufb01nite stochastic partial-monitoring\ngames; and FeedExp3 (Piccolboni & Schindelhauer,\n2001), which achieves O(T 2/3) minimax regret on\nall non-hopeless \ufb01nite partial-monitoring games, even\nagainst adversarial opponents.\n5.1. A locally observable game\nThe game we use to compare CBP and Balaton has\n3 actions and 3 outcomes. The game is described with\nthe loss and feedback matrices:\nL =\n\uf8eb\n\uf8ed\n1\n1\n0\n0\n1\n1\n1\n0\n1\n\uf8f6\n\uf8f8;\nH =\n\uf8eb\n\uf8ed\na\nb\nb\nb\na\nb\nb\nb\na\n\uf8f6\n\uf8f8.\nWe ran the algorithms 10 times for 15 di\ufb00erent\nstochastic strategies. We averaged the results for each\nstrategy and then took pointwise maximum over the 15\nstrategies. Figure 2(a) shows the empirical minimax\nregret calculated the way described above. In addition,\nFigure 2(b) shows the regret of the algorithms against\none of the opponents, averaged over 100 runs.\nThe\nresults indicate that CBP outperforms both FeedExp\nand Balaton.\nWe also observe that, although the\nasymptotic performace of Balaton is proven to be\nbetter than that of FeedExp, a larger constant factor\nmakes Balaton lose against FeedExp even at time\nstep ten million.\n5.2. Dynamic Pricing\nIn Dynamic Pricing, at every time step a seller (player)\nsets a price for his product while a buyer (opponent)\nsecretly sets a maximum price he is willing to pay. The\nfeedback for the seller is \u201cbuy\u201d or \u201cno-buy\u201d, while his\nloss is either a preset constant (no-buy) or the di\ufb00er-\nence between the prices (buy). The \ufb01nite version of the\ngame can be described with the following matrices:\nL =\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\n0\n1\n\u00b7 \u00b7 \u00b7\nN \u22121\nc\n0\n\u00b7 \u00b7 \u00b7\nN \u22122\n...\n...\n...\n...\nc\n\u00b7 \u00b7 \u00b7\nc\n0\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\nH =\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\ny\ny\n\u00b7 \u00b7 \u00b7\ny\nn\ny\n\u00b7 \u00b7 \u00b7\ny\n...\n...\n...\n...\nn\n\u00b7 \u00b7 \u00b7\nn\ny\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\nThis game is not locally observable and thus it is\n\u201chard\u201d (Bart\u00b4ok et al., 2011).\nSimple linear algebra\ngives that the locally observable action pairs are the\n\u201cconsecutive\u201d actions (L = {{i, i + 1} : i \u2208N \u22121}),\nwhile quite surprisingly, all action pairs are neighbors.\nWe compare CBP with FeedExp on Dynamic Pric-\ning with N = M = 5 and c = 2.\nSince Balaton\nis unde\ufb01ned on not locally observable games, we can\nnot include it in the comparison. To demonstrate the\nadaptiveness of CBP, we use two sets of opponent\nstrategies. The \u201cbenign\u201d setting is a set of opponents\nwhich are far away from \u201cdangerous\u201d regions, that is,\nfrom boundaries between cells of non-locally observ-\nable neighboring action pairs. The \u201charsh\u201d settings,\nhowever, include opponent strategies that are close or\non the boundary between two such actions. For each\nsetting we maximize over 15 strategies and average\nover 10 runs. We also compare the individual regret of\nthe two algorithms against one benign and one harsh\nstrategy. We averaged over 100 runs and plotted the\n90 percent con\ufb01dence intervals.\nThe results (shown in Figures 3 and 4) indicate that\nCBP has a signi\ufb01cant advantage over FeedExp on\nbenign settings. Nevertheless, for the harsh settings\nFeedExp slightly outperforms CBP, which we think is\na reasonable price to pay for the bene\ufb01t of adaptivity.\nReferences\nAudibert, J-Y. and Bubeck, S.\nMinimax policies\nfor adversarial and stochastic bandits.\nIn COLT\n2009, Proceedings of the 22nd Annual Conference\non Learning Theory, Montr\u00b4eal, Canada, June 18\u2013\n21, 2009, pp. 217\u2013226. Omnipress, 2009.\nAuer, P., Cesa-Bianchi, N., and Fischer, P. Finite-time\nAnalysis of the Multiarmed Bandit Problem. Mach.\nLearn., 47(2-3):235\u2013256, 2002. ISSN 0885-6125.\nAuer, P., Cesa-Bianchi, N., Freund, Y., and Schapire,\nAdaptive Stochastic Partial Monitoring\n0\n2\n4\n6\n8\n10\nx 10\n5\n0\n1\n2\n3\n4\n5 x 10\n4\nTime Step\nMinimax Regret\n \n \nCBP\nFeedExp\nBalaton\n(a) Pointwise maximum over 15 settings.\n0\n250,000\n5,000,000\n7,500,000\n10,000,000\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5 x 10\n4\nTime Step\nRegret\n \n \nCBP\nFeedExp\nBalaton\n(b) Regret against one opponent strategy.\nFigure 2. Comparing CBP with Balaton and FeedExp on the easy game\n0\n2\n4\n6\n8\n10\nx 10\n5\n0\n2\n4\n6\n8\n10 x 10\n4\nTime Step\nMinimax Regret\n \n \nCBP\nFeedExp\n(a) Pointwise maximum over 15 settings.\n0\n250,000\n500,000\n750,000\n1,000,000\n0\n1\n2\n3\n4\n5\n6\n7\n8 x 10\n4\nTime Step\nRegret\n \n \nCBP\nFeedExp\n(b) Regret against one opponent strategy.\nFigure 3. Comparing CBP and FeedExp on \u201cbenign\u201d setting of the Dynamic Pricing game.\n0\n2\n4\n6\n8\n10\nx 10\n5\n0\n1\n2\n3\n4\n5\n6\n7 x 10\n4\nTime Step\nMinimax Regret\n \n \nCBP\nFeedExp\n(a) Pointwise maximum over 15 settings.\n0\n250,000\n500,000\n750,000\n1,000,000\n0\n1\n2\n3\n4\n5 x 10\n4\nTime Step\nRegret\n \n \nCBP\nFeedExp\n(b) Regret against one opponent strategy.\nFigure 4. Comparing CBP and FeedExp on \u201charsh\u201d setting of the Dynamic Pricing game.\nR.E. The nonstochastic multiarmed bandit problem.\nSIAM Journal on Computing, 32(1):48\u201377, 2003.\nBart\u00b4ok, G., P\u00b4al, D., and Szepesv\u00b4ari, Cs.\nToward a\nclassi\ufb01cation of \ufb01nite partial-monitoring games. In\nALT 2010, Canberra, Australia, pp. 224\u2013238, 2010.\nBart\u00b4ok, G., P\u00b4al, D., and Szepesv\u00b4ari, Cs. Minimax re-\ngret of \ufb01nite partial-monitoring games in stochastic\nenvironments. In COLT, July 2011.\nCesa-Bianchi, N., Lugosi, G., and Stoltz, G. Regret\nminimization under partial monitoring.\nIn Infor-\nmation Theory Workshop, 2006. ITW\u201906 Punta del\nEste. IEEE, pp. 72\u201376. IEEE, 2006.\nFoster, D.P. and Rakhlin, A. No internal regret via\nneighborhood watch. CoRR, abs/1108.6088, 2011.\nLittlestone, N. and Warmuth, M.K.\nThe weighted\nmajority algorithm. Information and Computation,\n108:212\u2013261, 1994.\nPiccolboni, A. and Schindelhauer, C.\nDiscrete pre-\ndiction games with arbitrary feedback and loss. In\nCOLT 2001, pp. 208\u2013223. Springer-Verlag, 2001.\nVovk, V.G. Aggregating strategies. In Annual Work-\nshop on Computational Learning Theory, 1990.\n",
        "sentence": " Since then, several algorithms with a \u00d5( \u221a T ) regret bound for easy problems have been proposed [11, 12, 13]. [11], which derivedO(logT ) distribution-dependent regret for easy problems. [11], we compared the performances of algorithms in three different games: the four-state game (Section 4), a three-state game and dynamic pricing. [11], we set N = 5,M = 5, and c = 2. We compared Random, FeedExp3 [8], CBP [11] with \u03b1 = 1. [11] G\u00e1bor Bart\u00f3k, Navid Zolghadr, and Csaba Szepesv\u00e1ri.",
        "context": "the strategy space then the regret grows as if\nthe problem was easy. As an implication, we\nshow that under some reasonable additional\nassumptions, the algorithm enjoys an O(\n\u221a\nT)\nregret in Dynamic Pricing, proven to be hard\nby Bart\u00b4ok et al. (2011).\nThey proved that there are only four categories: games\nwith minimax regret 0, e\u0398(\n\u221a\nT), \u0398(T 2/3), and \u0398(T),\nand named them trivial, easy, hard, and hopeless, re-\n1The Exp3 algorithm due to Auer et al. (2003) achieves\nboth \u201ceasy\u201d and \u201chard\u201d problems. For easy\nproblems, it additionally achieves logarithmic\nindividual regret. Most importantly, the al-\ngorithm is adaptive in the sense that if the\nopponent strategy is in an \u201ceasy region\u201d of"
    },
    {
        "title": "A near-optimal algorithm for finite partial-monitoring games against adversarial opponents",
        "author": [
            "G\u00e1bor Bart\u00f3k"
        ],
        "venue": "In COLT,",
        "citeRegEx": "12",
        "shortCiteRegEx": "12",
        "year": 2013,
        "abstract": "",
        "full_text": "",
        "sentence": " Since then, several algorithms with a \u00d5( \u221a T ) regret bound for easy problems have been proposed [11, 12, 13]. [12] G\u00e1bor Bart\u00f3k.",
        "context": null
    },
    {
        "title": "Efficient partial monitoring with prior information",
        "author": [
            "Hastagiri P. Vanchinathan",
            "G\u00e1bor Bart\u00f3k",
            "Andreas Krause"
        ],
        "venue": "In NIPS,",
        "citeRegEx": "13",
        "shortCiteRegEx": "13",
        "year": 2014,
        "abstract": "",
        "full_text": "",
        "sentence": " Since then, several algorithms with a \u00d5( \u221a T ) regret bound for easy problems have been proposed [11, 12, 13]. Among them, the Bayes-update Partial Monitoring (BPM) algorithm [13] is state-of-the-art in the sense of empirical performance. [13]) ambiguously define the harshness as the closeness to the boundary of the cells. 01, BPM-LEAST, BPM-TS [13], and PM-DMED with c = 1. Following the optimization of BPM-LEAST [13], we resorted to a finite sample approximation and used the Gurobi LP solver [19] in computing {r\u2217 i }: at each round, we sampled 1,000 points from PM , and relaxed the constraints on the samples. [13] Hastagiri P.",
        "context": null
    },
    {
        "title": "Finite-time Analysis of the Multiarmed Bandit Problem",
        "author": [
            "Peter Auer",
            "Nicol\u00f3 Cesa-bianchi",
            "Paul Fischer"
        ],
        "venue": "Machine Learning,",
        "citeRegEx": "14",
        "shortCiteRegEx": "14",
        "year": 2002,
        "abstract": "",
        "full_text": "",
        "sentence": " Upper confidence bound (UCB), the most well-known algorithm for the multi-armed bandits, has a distribution-dependent regret bound [2, 14], and algorithms that minimize the distribution-dependent regret (e. [14] Peter Auer, Nicol\u00f3 Cesa-bianchi, and Paul Fischer.",
        "context": null
    },
    {
        "title": "The KL-UCB algorithm for bounded stochastic bandits and beyond",
        "author": [
            "Aur\u00e9lien Garivier",
            "Olivier Capp\u00e9"
        ],
        "venue": "In COLT,",
        "citeRegEx": "15",
        "shortCiteRegEx": "15",
        "year": 2011,
        "abstract": "",
        "full_text": "",
        "sentence": " [15]). [15] Aur\u00e9lien Garivier and Olivier Capp\u00e9.",
        "context": null
    },
    {
        "title": "Large deviations techniques and applications",
        "author": [
            "Amir Dembo",
            "Ofer Zeitouni"
        ],
        "venue": "Applications of mathematics",
        "citeRegEx": "16",
        "shortCiteRegEx": "16",
        "year": 1998,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Large deviation principle [16] states that, the probability that an opponent with strategy q behaves like p is [16] Amir Dembo and Ofer Zeitouni.",
        "context": null
    },
    {
        "title": "An Asymptotically Optimal Bandit Algorithm for Bounded Support Models",
        "author": [
            "Junya Honda",
            "Akimichi Takemura"
        ],
        "venue": "In COLT,",
        "citeRegEx": "17",
        "shortCiteRegEx": "17",
        "year": 2010,
        "abstract": "",
        "full_text": "",
        "sentence": " 4 PM-DMED Algorithm In this section, we describe the partial monitoring deterministic minimum empirical divergence (PMDMED) algorithm, which is inspired by DMED [17] for solving the multi-armed bandit problem. [17] Junya Honda and Akimichi Takemura.",
        "context": null
    },
    {
        "title": "A dual parametrization method for convex semi-infinite programming",
        "author": [
            "S. Ito",
            "Y. Liu",
            "K.L. Teo"
        ],
        "venue": "Annals of Operations Research,",
        "citeRegEx": "20",
        "shortCiteRegEx": "20",
        "year": 2000,
        "abstract": "",
        "full_text": "",
        "sentence": " [20] and references therein). [20] S.",
        "context": null
    },
    {
        "title": "Introduction to sensitivity and stability analysis in nonlinear programming",
        "author": [
            "Anthony V. Fiacco"
        ],
        "venue": null,
        "citeRegEx": "21",
        "shortCiteRegEx": "21",
        "year": 1983,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " , Fiacco [21]). [21] Anthony V.",
        "context": null
    },
    {
        "title": "Asymptotically efficient adaptive choice of control laws in controlled Markov chains",
        "author": [
            "T.L. Graves",
            "T.L. Lai"
        ],
        "venue": "SIAM J. Contr. and Opt.,",
        "citeRegEx": "22",
        "shortCiteRegEx": "22",
        "year": 1997,
        "abstract": "",
        "full_text": "",
        "sentence": " [22] T. In fact, many regularity conditions are assumed in Graves and Lai [22], where another generalization of the bandit problem is considered and the regret lower bound is expressed in terms of LSIP.",
        "context": null
    },
    {
        "title": "On the complexity of A/B testing",
        "author": [
            "Emilie Kaufmann",
            "Olivier Capp\u00e9",
            "Aur\u00e9lien Garivier"
        ],
        "venue": "In COLT,",
        "citeRegEx": "23",
        "shortCiteRegEx": "23",
        "year": 2014,
        "abstract": "A/B testing refers to the task of determining the best option among two\nalternatives that yield random outcomes. We provide distribution-dependent\nlower bounds for the performance of A/B testing that improve over the results\ncurrently available both in the fixed-confidence (or delta-PAC) and\nfixed-budget settings. When the distribution of the outcomes are Gaussian, we\nprove that the complexity of the fixed-confidence and fixed-budget settings are\nequivalent, and that uniform sampling of both alternatives is optimal only in\nthe case of equal variances. In the common variance case, we also provide a\nstopping rule that terminates faster than existing fixed-confidence algorithms.\nIn the case of Bernoulli distributions, we show that the complexity of\nfixed-budget setting is smaller than that of fixed-confidence setting and that\nuniform sampling of both alternatives -though not optimal- is advisable in\npractice when combined with an appropriate stopping criterion.",
        "full_text": "JMLR: Workshop and Conference Proceedings vol 35 (2014) 1\u201323\nOn the Complexity of A/B Testing\nEmilie Kaufmann\nKAUFMANN@TELECOM-PARISTECH.FR\nLTCI, T\u00b4el\u00b4ecom ParisTech & CNRS\nOlivier Capp\u00b4e\nCAPPE@TELECOM-PARISTECH.FR\nLTCI, T\u00b4el\u00b4ecom ParisTech & CNRS\nAur\u00b4elien Garivier\nAURELIEN.GARIVIER@MATH.UNIV-TOULOUSE.FR\nInstitut de Math\u00b4ematiques de Toulouse, Universit\u00b4e Paul Sabatier\nAbstract\nA/B testing refers to the task of determining the best option among two alternatives that yield\nrandom outcomes. We provide distribution-dependent lower bounds for the performance of A/B\ntesting that improve over the results currently available both in the \ufb01xed-con\ufb01dence (or \u03b4-PAC) and\n\ufb01xed-budget settings. When the distribution of the outcomes are Gaussian, we prove that the com-\nplexity of the \ufb01xed-con\ufb01dence and \ufb01xed-budget settings are equivalent, and that uniform sampling\nof both alternatives is optimal only in the case of equal variances. In the common variance case,\nwe also provide a stopping rule that terminates faster than existing \ufb01xed-con\ufb01dence algorithms. In\nthe case of Bernoulli distributions, we show that the complexity of \ufb01xed-budget setting is smaller\nthan that of \ufb01xed-con\ufb01dence setting and that uniform sampling of both alternatives\u2014though not\noptimal\u2014is advisable in practice when combined with an appropriate stopping criterion.\nKeywords: Sequential testing. Best arm identi\ufb01cation. Bandit models. Sample complexity.\n1. Introduction\nA/B Testing is a popular procedure used, for instance, for website optimization: two versions of a\nwebpage, say A and B, are empirically compared by being presented to users. Each user only sees\none of the two versions, and the goal is to determine which version is preferable. We assume that\nthe users provide a real-valued index of the quality of the pages, which is modeled by probability\ndistributions \u03bdA and \u03bdB, with respective means \u00b5A and \u00b5B. For example, a standard objective is to\ndetermine which webpage has the highest conversion rate (probability that a user actually becomes\na customer) by receiving binary feedback from the users.\nMethods for A/B Testing are often viewed as statistical tests of the hypothesis H0 : (\u00b5A \u2264\n\u00b5B) against H1 : (\u00b5A > \u00b5B). One may consider either classical tests, based on a number of\nsamples nA and nB from each distribution \ufb01xed before the experiment, or sequential tests, based\non paired samples (Xs, Ys) of \u03bdA, \u03bdB and in which a randomized stopping rule determines when the\nexperiment is to be terminated. In both of these test settings, the sampling schedule is determined\nin advance, which is a possible source of sub-optimality as A/B Testing algorithms could take\nadvantage of past observations to provide a smarter choice of the page to be displayed to the next\nuser. In the sequel, we investigate whether A/B Testing could bene\ufb01t from an adaptive sampling\nschedule. Ignoring the possible long-term effects on users of presenting one or the other option, we\nconsider it as a particular instance of best arm identi\ufb01cation in a two-armed bandit model.\nc\u20dd2014 E. Kaufmann, O. Capp\u00b4e & A. Garivier.\narXiv:1405.3224v2  [math.ST]  24 Feb 2015\nKAUFMANN CAPP\u00b4E GARIVIER\nA two-armed bandit model consists of two unknown probability distributions on R, \u03bd1 and\n\u03bd2, sometimes referred to as arms or options (webpages in our motivating example). Arm a has\nexpectation \u00b5a. At each time t = 1, 2, . . . , an agent chooses an option At \u2208{1, 2} and receives\nan independent draw Zt of the corresponding arm \u03bdAt. We denote by P\u03bd (resp. E\u03bd) the probability\nlaw (resp. expectation) of the corresponding process (Zt). We assume that the bandit model \u03bd =\n(\u03bd1, \u03bd2) belongs to a class M such that for all \u03bd \u2208M, \u00b51 \u0338= \u00b52. In order to identify the best arm,\nthat is the arm a\u2217with highest expectation, the agent must use a strategy de\ufb01ning which arms to\nsample from, when to stop sampling, and which arm \u02c6a to choose. The sampling rule determines\nhow, at time t, the arm At is chosen based on the past observations; in other words, At is Ft\u22121\u2013\nmeasurable, with Ft = \u03c3(A1, Z1, . . . , At, Zt). The stopping rule \u03c4 is a stopping time with respect\nto (Ft)t\u2208N satisfying P\u03bd(\u03c4 < +\u221e) = 1. The recommendation rule is a F\u03c4-measurable random\narm \u02c6a \u2208{1, 2}. This triple ((At), \u03c4, \u02c6a) entirely determines the strategy, which we denote in the\nsequel by A. As discussed before, statistical tests correspond to strategies that sample the arms in a\nround-robin fashion, which we will refer to as uniform sampling.\nIn the bandit literature, two different settings have been considered. In the \ufb01xed-con\ufb01dence\nsetting, a risk parameter \u03b4 is \ufb01xed. A strategy A is called \u03b4-PAC if, for every choice of \u03bd \u2208M,\nP\u03bd(\u02c6a = a\u2217) \u22651 \u2212\u03b4. The goal is, among the \u03b4-PAC strategies, to minimize the expected number of\ndraws E\u03bd[\u03c4]. In the \ufb01xed-budget setting, the number of draws \u03c4 is \ufb01xed in advance (\u03c4 = t almost\nsurely) and the goal is to choose the recommendation rule so as to minimize pt(\u03bd) := P\u03bd(\u02c6a \u0338=\na\u2217). In the \ufb01xed-budget setting, a strategy A is called consistent if, for every choice of \u03bd \u2208M,\npt(\u03bd) \u2192\nt\u2192\u221e0.\nIn order to unify and compare these approaches, we de\ufb01ne the complexity \u03baC(\u03bd) (resp. \u03baB(\u03bd))\nof best arm identi\ufb01cation in the \ufb01xed-con\ufb01dence (resp. \ufb01xed-budget) setting, as follows:\n\u03baC(\u03bd) =\ninf\nA \u03b4\u2212PAC lim sup\n\u03b4\u21920\nE\u03bd[\u03c4]\nlog(1/\u03b4),\n\u03baB(\u03bd) =\ninf\nA consistent\n\u0012\nlim sup\nt\u2192\u221e\n\u22121\nt log pt(\u03bd)\n\u0013\u22121\n.\nHeuristically, for a given bandit model \u03bd and a given \u03b4 > 0, a \ufb01xed-con\ufb01dence optimal strategy uses\nan average number of samples of order \u03baC(\u03bd) log(1/\u03b4), whereas a \ufb01xed-budget optimal strategy\nuses approximately t = \u03baB(\u03bd) log(1/\u03b4) draws in order to ensure a probability of error at most\nequal to \u03b4. Most of the existing performance bounds for the \ufb01xed con\ufb01dence and \ufb01xed budget\nsettings can be expressed using these complexity measures.\nThe main goal of this paper is to determine \u03baC and \u03baB for important classes of parametric\nbandit models, allowing for a comparison between the \ufb01xed-con\ufb01dence and \ufb01xed-budget settings.\nClassical sequential testing theory provides a \ufb01rst element in that direction in the simpler case of\nfully speci\ufb01ed alternatives. Consider for instance the case where \u03bd1 and \u03bd2 are Gaussian laws with\nthe same known variance \u03c32, the means \u00b51 and \u00b52 being known up to a permutation. Denoting by\nP the joint distribution of the paired samples (Xs, Ys), one must choose between the hypotheses\nH0 : P = N\n\u0000\u00b51, \u03c32\u0001\n\u2297N\n\u0000\u00b52, \u03c32\u0001\nand H1 : P = N\n\u0000\u00b52, \u03c32\u0001\n\u2297N\n\u0000\u00b51, \u03c32\u0001\n. It is known since\nWald (1945) that among the sequential tests such that type I and type II probabilities of error are\nboth smaller than \u03b4, the Sequential Probability Ratio Test (SPRT) minimizes the expected number\nof required samples, and is such that E\u03bd[\u03c4] = 2\u03c32/(\u00b51 \u2212\u00b52)2 log(1/\u03b4). However, the batch test\nthat minimizes both probabilities of error is the Likelihood Ratio test; it can be shown to require a\nsample size of order 8\u03c32/(\u00b51 \u2212\u00b52)2 log(1/\u03b4) in order to ensure that both type I and type II error\nprobabilities are smaller than \u03b4. Thus, when the sampling strategy is uniform and the parameters are\n2\nON THE COMPLEXITY OF A/B TESTING\nknown, there is a clear gain in using randomized stopping strategies. We show in the following that\nthis conclusion is not valid anymore when the values of \u00b51 and \u00b52 are not assumed to be known.\nRelated works. Bandit models have received a considerable interest since their introduction\nby Thompson (1933). An important focus was set on a different perspective, in which each obser-\nvation is considered as a reward: the agent aims at maximizing the cumulative rewards obtained until\nsome horizon t. Equivalently, his goal is to minimize the regret Rt(\u03bd) = t\u00b5[1]\u2212E\u03bd\n\u0002Pt\ns=1 Zs\n\u0003\n. Re-\ngret minimization, which is paradigmatic of the so-called exploration versus exploitation dilemma,\nwas introduced by Robbins (1952) and its complexity is well understood for parametric bandits. In\ngeneric one-parameter models, Lai and Robbins (1985) prove that, with a proper notion of consis-\ntency adapted to regret minimization,\ninf\nA consistent lim inf\nt\u2192\u221e\nRt(\u03bd)\nlog t \u2265\nX\na:\u00b5a<\u00b5[1]\n(\u00b5[1] \u2212\u00b5a)\nKL(\u03bda, \u03bd[1]) ,\nwhere KL(\u03bdi, \u03bdj) denotes the Kullback-Leibler divergence between distributions \u03bdi and \u03bdj. Since\nthen, non-asymptotic analyses of ef\ufb01cient algorithms matching this bound have been proposed.\nOptimal algorithms include the KL-UCB algorithm of Capp\u00b4e et al. (2013)\u2014a variant of UCB1\n(Auer et al. (2002)) using informational upper bounds, Thompson Sampling (Kaufmann et al.\n(2012b); Agrawal and Goyal (2013)), the DMED algorithm (Honda and Takemura, 2011) and\nBayes-UCB Kaufmann et al. (2012a). This paper is a \ufb01rst step in the attempt to similarly char-\nacterize the complexity of pure exploration, where the goal is to determine the best arms without\ntrying to maximize the cumulated observations.\nThe problem of best arm identi\ufb01cation has received an important interest in the 1950s as a\nparticular case of \u2019ranking and identi\ufb01cation problems\u2019. The literature on the subject goes beyond\ntwo-armed bandit models to \ufb01nding the m > 1 best arms among K > 2 arms, and sometimes\nintroduces a relaxation parameter \u03f5 > 0, such that arms within \u03f5 of the best arm should be recom-\nmended. In the sequel, we always particularize the existing results to the two-armed bandit models\npresented above. The \ufb01rst advances on this topic are summarized in the monograph by Bechhofer\net al. (1968), who only consider the \ufb01xed-con\ufb01dence setting. In the same setting, algorithms in-\ntroduced more recently by Even-Dar et al. (2006); Kalyanakrishnan et al. (2012); Gabillon et al.\n(2012) can be used to \ufb01nd the best arm in a two-armed bounded bandit model, in which \u03bd1 and \u03bd2\nare probability distributions on [0, 1]. Combining the upper bound on E\u03bd[\u03c4] for the LUCB algo-\nrithm of Kalyanakrishnan et al. (2012) with the lower bound following from the work of Mannor\nand Tsitsiklis (2004), it can be shown that for bounded bandit models such that \u00b5a \u2208[0, 1 \u2212\u03b1] for\na \u2208{1, 2}, there exists a constant C\u03b1 for which\nC\u03b1/(\u00b51 \u2212\u00b52)2 \u2264\u03baC(\u03bd) \u2264584/(\u00b51 \u2212\u00b52)2.\nThe \ufb01xed-budget setting has been studied recently by Audibert et al. (2010); Bubeck et al.\n(2013). In two-armed bandit problems, the algorithms introduced in these papers boil down to\nsampling each arm t/2 times\u2014t denoting the total budget\u2014and recommending the empirical best\narm. A simple upper bound on the probability of error of this strategy can be derived, and this result\npaired with the lower bound of Audibert et al. (2010) yields, for bounded bandit models such that\n\u00b5a \u2208[\u03b1; 1 \u2212\u03b1] for a \u2208{1, 2}:\n(2/5)\u03b1(1 \u2212\u03b1)/(\u00b51 \u2212\u00b52)2 \u2264\u03baB(\u03bd) \u22642/(\u00b51 \u2212\u00b52)2.\n3\nKAUFMANN CAPP\u00b4E GARIVIER\nBubeck et al. (2011) show that in the \ufb01xed-budget setting any sampling strategy designed to\nminimize regret performs poorly with respect to the simple regret rt := \u00b5\u2217\u2212\u00b5 \u02c6S1, a quantity closely\nrelated to the probability pt(\u03bd) of recommending the wrong arm. Therefore, good strategies for\nbest arm identi\ufb01cation have to be quite different from UCB-like strategies. We will show below that\nthe complexities \u03baB(\u03bd) and \u03baC(\u03bd) of pure-exploration involve information terms that are different\nfrom the Kullback-Leibler divergence featured in Lai and Robbins\u2019 lower bound on regret.\nContents of the paper. Compared to existing results, we provide general lower bounds on\n\u03baB(\u03bd) and \u03baC(\u03bd) that: (i) are tighter, leading in speci\ufb01c parametric cases to a precise evaluation\nof these complexities; (ii) do not require unnecessary support assumptions; and (iii) are stated in\nterms of information divergences between the distributions \u03bd1 and \u03bd2 rather than in terms of the gap\n\u00b51 \u2212\u00b52. As can be expected, we will indeed con\ufb01rm that the inverse of the squared gap (\u00b51 \u2212\u00b52)2\nis the relevant measure of complexity only in the Gaussian case, and an approximation (in the spirit\nof Pinsker\u2019s inequality) for sub-Gaussian distributions.\nLower bounds on the sample complexity (resp. probability of error) of algorithms using the\nuniform sampling strategy in the \ufb01xed-con\ufb01dence (resp. \ufb01xed-budget) setting are also derived and\nwe show that for Gaussian bandit models with different variances, there is a signi\ufb01cant gain in using\na non-uniform sampling strategy. For Bernoulli bandits however, we show that little can be gained\nby departing from uniform sampling, and we therefore propose close-to-optimal tests both for the\nbatch and sequential settings. For Gaussian bandits with a known common variance the optimal\nalgorithm uses uniform sampling. In this speci\ufb01c case, we propose an improved \u03b4-PAC stopping\nrule, illustrating its performance through numerical experiments.\nOur contributions follow from two main mathematical results: Lemma 8 provides a general rela-\ntion between the expected number of draws and Kullback-Leibler divergences of the arms\u2019 distribu-\ntions, which is the key element to derive the lower bounds. Lemma 9 is a tight deviation inequality\nfor martingales with sub-Gaussian increments, in the spirit of the Law of Iterated Logarithm.\nThe paper is structured as follows. Section 2 presents a distribution-dependent lower bound\non both \u03baB(\u03bd) and \u03baC(\u03bd) under the some identi\ufb01ability assumption, as well as lower bounds for\nalgorithms using uniform sampling. Gaussian bandit models are then studied in details in Section\n3, and Bernoulli bandit models in Section 4. Section 5 includes a practical illustration of the per-\nformance of matching algorithms for Gaussian bandits, as well as a practical comparison of the\n\ufb01xed-con\ufb01dence and \ufb01xed-budget settings. The most important elements of proof are gathered in\nSection 6, with the rest of the proofs in the Appendix.\n2. Lower Bounding the Complexity\nIntroducing the Kullback-Leibler divergence of any two probability distributions p and q:\nKL(p, q) =\n( R\nlog\nh\ndp\ndq(x)\ni\ndp(x) if q \u226ap,\n+\u221eotherwise,\nwe make the assumption that there exists a set N such that for all \u03bd = (\u03bd1, \u03bd2) \u2208M, for a \u2208\n{1, 2}, \u03bda \u2208N and that N satis\ufb01es\n\u2200p, q \u2208N, p \u0338= q \u21d20 < KL(p, q) < +\u221e.\n4\nON THE COMPLEXITY OF A/B TESTING\nA class M of bandit models satisfying this property is called identi\ufb01able. For M an identi\ufb01able\nclass of bandit models, Theorem 1 provides lower bounds on \u03baB(\u03bd) and \u03baC(\u03bd) for every \u03bd \u2208M.\nThe proof of this theorem is based on changes of distribution and detailed in Section 6.\nTheorem 1 Let \u03bd = (\u03bd1, \u03bd2) be a two-armed bandit model such that \u00b51 > \u00b52. In the \ufb01xed-budget\nsetting, any consistent algorithm satis\ufb01es\nlim sup\nt\u2192\u221e\n\u22121\nt log pt(\u03bd) \u2264c\u2217(\u03bd),\nwhere c\u2217(\u03bd) :=\ninf\n(\u03bd\u2032\n1,\u03bd\u2032\n2)\u2208M:\u00b5\u2032\n1<\u00b5\u2032\n2\nmax\n\b\nKL(\u03bd\u2032\n1, \u03bd1), KL(\u03bd\u2032\n2, \u03bd2)\n\t\n.\nIn the \ufb01xed-con\ufb01dence setting any algorithm that is \u03b4-PAC on M satis\ufb01es, when \u03b4 \u22640.15,\nE\u03bd[\u03c4] \u2265\n1\nc\u2217(\u03bd) log\n\u0012 1\n2\u03b4\n\u0013\n,\nwhere c\u2217(\u03bd) :=\ninf\n(\u03bd\u2032\n1,\u03bd\u2032\n2)\u2208M:\u00b5\u2032\n1<\u00b5\u2032\n2\nmax\n\b\nKL(\u03bd1, \u03bd\u2032\n1), KL(\u03bd2, \u03bd\u2032\n2)\n\t\n.\nIn particular, Theorem 1 implies that \u03baB(\u03bd) \u22651/c\u2217(\u03bd) and \u03baC(\u03bd) \u22651/c\u2217(\u03bd). Proceeding\nsimilarly, one can obtain lower bounds for the algorithms that use uniform sampling of both arms.\nThe proof of the following result is easily adapted from that of Theorem 1 (cf. Section 6), using that\neach arm is drawn \u03c4/2 times.\nTheorem 2 Let \u03bd = (\u03bd1, \u03bd2) be a two-armed bandit model such that \u00b51 > \u00b52. In the \ufb01xed-budget\nsetting, any consistent algorithm using a uniform sampling strategy satis\ufb01es\nlim sup\nt\u2192\u221e\n\u22121\nt log pt(\u03bd) \u2264I\u2217(\u03bd) where I\u2217(\u03bd) :=\ninf\n(\u03bd\u2032\n1,\u03bd\u2032\n2)\u2208M:\u00b5\u2032\n1<\u00b5\u2032\n2\nKL (\u03bd\u2032\n1, \u03bd1) + KL (\u03bd\u2032\n2, \u03bd2)\n2\n.\nIn the \ufb01xed-con\ufb01dence setting, any algorithm that is \u03b4-PAC on M and uses a uniform sampling\nstrategy satis\ufb01es, for \u03b4 \u22640.15,\nE\u03bd[\u03c4] \u2265\n1\nI\u2217(\u03bd) log 1\n2\u03b4\nwhere I\u2217(\u03bd) :=\ninf\n(\u03bd\u2032\n1,\u03bd\u2032\n2)\u2208M:\u00b5\u2032\n1<\u00b5\u2032\n2\nKL (\u03bd1, \u03bd\u2032\n1) + KL (\u03bd2, \u03bd\u2032\n2)\n2\n.\nObviously, one always has I\u2217(\u03bd) \u2264c\u2217(\u03bd) and I\u2217(\u03bd) \u2264c\u2217(\u03bd) suggesting that uniform sampling\ncan be sub-optimal. It is possible to give explicit expressions for the quantities c\u2217(\u03bd), c\u2217(\u03bd) and\nI\u2217(\u03bd), I\u2217(\u03bd) for speci\ufb01c classes of parametric bandit models that will be considered in the rest of\nthe paper. In the case of Gaussian bandits with known variance (see Section 3):\nM = {\u03bd =\n\u0000N\n\u0000\u00b51, \u03c32\n1\n\u0001\n, N\n\u0000\u00b52, \u03c32\n2\n\u0001\u0001\n: (\u00b51, \u00b52) \u2208R2, \u00b51 \u0338= \u00b52},\n(1)\none obtains\nc\u2217(\u03bd) = c\u2217(\u03bd) = (\u00b51 \u2212\u00b52)2\n2(\u03c31 + \u03c32)2\nand I\u2217(\u03bd) = I\u2217(\u03bd) = (\u00b51 \u2212\u00b52)2\n4(\u03c32\n1 + \u03c32\n2).\nHence, the lower bounds of Theorem 1 are equal in this case, and we provide in Section 3 matching\nupper bounds con\ufb01rming that indeed \u03baB(\u03bd) = \u03baC(\u03bd). In addition, the observation that 2I\u2217(\u03bd) \u2265\nc\u2217(\u03bd) \u2265I\u2217(\u03bd) implies that, except when \u03c31 = \u03c32, strategies based on uniform sampling are sub-\noptimal.\n5\nKAUFMANN CAPP\u00b4E GARIVIER\nThe values of c\u2217(\u03bd) and c\u2217(\u03bd) can also be computed for canonical one-parameter exponential\nfamilies with density with respect to some reference measure given by\nf\u03b8(x) = A(x) exp(\u03b8x \u2212b(\u03b8)), for \u03b8 \u2208\u0398 \u2282R.\n(2)\nWe consider the class of bandit models\nM = {\u03bd = (\u03bd\u03b81, \u03bd\u03b82) : (\u03b81, \u03b82) \u2208\u03982, \u03b81 \u0338= \u03b82}\nwhere \u03bd\u03b8a has density f\u03b8a given by (2). Using the shorthand K(\u03b8, \u03b8\u2032) = KL(\u03bd\u03b8, \u03bd\u03b8\u2032) for (\u03b8, \u03b8\u2032) \u2208\n\u03982, one can show that, for \u03bd such that \u00b51 > \u00b52:\nc\u2217(\u03bd)\n=\ninf\u03b8\u2208\u0398 max (K(\u03b8, \u03b81), K(\u03b8, \u03b82))\n=\nK(\u03b8\u2217, \u03b81), where K(\u03b8\u2217, \u03b81) = K(\u03b8\u2217, \u03b82),\nc\u2217(\u03bd)\n=\ninf\u03b8\u2208\u0398 max (K(\u03b81, \u03b8), K(\u03b82, \u03b8))\n=\nK(\u03b81, \u03b8\u2217), where K(\u03b81, \u03b8\u2217) = K(\u03b82, \u03b8\u2217).\nThe coef\ufb01cient c\u2217(\u03bd) is known as the Chernoff information K\u2217(\u03b81, \u03b82) between the distributions\n\u03bd\u03b81 and \u03bd\u03b82 (see Cover and Thomas (2006) and Kaufmann and Kalyanakrishnan (2013) for earlier\nnotice of the relevance of this quantity in the best arm selection problem). By analogy, we will also\ndenote c\u2217(\u03bd) by K\u2217(\u03b81, \u03b82) = K(\u03b81, \u03b8\u2217).\nFor exponential family bandits the quantities c\u2217(\u03bd) and c\u2217(\u03bd) are not equal in general, although\nit can be shown that it is the case when the log-partition function b(\u03b8) is (Fenchel) self-conjugate\n(e.g., for Gaussian and exponential variables). In Section 4, we will focus on the case of Bernoulli\nmodels for which c\u2217(\u03bd) > c\u2217(\u03bd). By exhibiting a matching strategy in the \ufb01xed-budget case, we\nwill show that this implies that \u03baC(\u03bd) > \u03baB(\u03bd) in this case.\n3. The Gaussian Case\nWe study in this Section the class of two-armed Gaussian bandit models with known variances\nde\ufb01ned by (1), where \u03c31 and \u03c32 are \ufb01xed. In this case, we observed above that the lower bounds of\nTheorem 1 are similar, because c\u2217(\u03bd) = c\u2217(\u03bd). We prove in this section that indeed\n\u03baC(\u03bd) = \u03baB(\u03bd) = 2(\u03c31 + \u03c32)2\n(\u00b51 \u2212\u00b52)2\nby exhibiting strategies that reach these performance bounds. These strategies are based on the\nsimple recommendation of the empirical best arm but use non-uniform sampling in cases where \u03c31\nand \u03c32 differ. When \u03c31 = \u03c32 we provide in Theorem 3 an improved stopping rule that is \u03b4-PAC but\nresults in a signi\ufb01cant reduction of the running time of \ufb01xed-con\ufb01dence tests.\n3.1. Fixed-Budget Setting\nWe consider the simple family of static strategies that draw n1 samples from arm 1 followed by\nn2 = t \u2212n1 samples of arm 2, and then choose arm 1 if \u02c6\u00b51,n1 < \u02c6\u00b52,n2, where \u02c6\u00b5i,ni denotes\nthe empirical mean of the ni samples from arm i. Assume for instance that \u00b51 > \u00b52. Since\n\u02c6\u00b51,n1 \u2212\u02c6\u00b52,n2 \u2212\u00b51 + \u00b52 \u223cN\n\u00000, \u03c32\n1/n1 + \u03c32\n2/n2\n\u0001\n, the probability of error of such a strategy is\neasily upper bounded as:\nP (\u02c6\u00b51,n1 < \u02c6\u00b52,n2)\n\u2264\nexp\n \n\u2212\n\u0012\u03c32\n1\nn1\n+ \u03c32\n2\nn2\n\u0013\u22121 (\u00b51 \u2212\u00b52)2\n2\n!\n.\n6\nON THE COMPLEXITY OF A/B TESTING\nThe right hand side is minimized when n1/(n1 + n2) = \u03c31/(\u03c31 + \u03c32), and the static strategy\ndrawing n1 = \u2308\u03c31t/(\u03c31 + \u03c32)\u2309times arm 1 is such that\nlim inf\nt\u2192\u221e\u22121\nt log pt(\u03bd) \u2265(\u00b51 \u2212\u00b52)2\n2(\u03c31 + \u03c32)2 ,\nwhich matches the bound of Theorem 1 for Gaussian bandit models.\n3.2. Fixed-Con\ufb01dence Setting\n3.2.1. EQUAL VARIANCES\nWe start with the simpler case \u03c31 = \u03c32 = \u03c3, where the quantity I\u2217(\u03bd) introduced in Theorem 2\ncoincides with c\u2217(\u03bd), which suggests that uniform sampling could be optimal. A uniform sampling\nstrategy is equivalent to collecting paired samples (Xs, Ys) from both arms. The difference Xs \u2212Ys\nis Gaussian with mean \u00b5 = \u00b51 \u2212\u00b52 and a \u03b4-PAC algorithm is equivalent to a sequential test of\nH0 : \u00b5 < 0 versus H1 : \u00b5 > 0 such that the probability of error is uniformly bounded by \u03b4.\nRobbins (1970) proposes such a test that stops after a number of samples\n\u03c4 = inf\n\uf8f1\n\uf8f2\n\uf8f3t \u22082N\u2217:\n\f\f\f\f\f\f\nt/2\nX\ns=1\n(Xs \u2212Ys)\n\f\f\f\f\f\f\n>\np\n2\u03c32t\u03b2(t, \u03b4)\n\uf8fc\n\uf8fd\n\uf8fewith \u03b2(t, \u03b4) = t + 1\nt\nlog\n\u0012t + 1\n2\u03b4\n\u0013\n(3)\nand recommends the empirical best arm. This procedure belongs to the class of elimination strate-\ngies, introduced by Jennison et al. (1982) who derive a lower bound on the sample complexity of any\n\u03b4-PAC elimination strategy\u2014whereas our lower bound applies to any \u03b4-PAC algorithm\u2014matched\nby Robbins\u2019 algorithm, that is, lim\u03b4\u21920 E\u03bd[\u03c4]/log 1\n\u03b4 = 8\u03c32/(\u00b51 \u2212\u00b52)2. Therefore, Robbins\u2019 rule\n(3) yields an optimal strategy matching our general lower bound of Theorem 1 in the particular case\nof Gaussian distributions with common known variance.\nNote that any elimination strategy that is \u03b4-PAC and uses a threshold function smaller than Rob-\nbins\u2019 also matches our asymptotic lower bound, while being strictly more ef\ufb01cient than Robbins\u2019\nrule. For practical purpose, it is therefore interesting to exhibit smaller exploration rates \u03b2(t, \u03b4)\nleading to a \u03b4-PAC algorithm. The probability of error of such an algorithm is upper bounded, for\nexample for \u00b51 < \u00b52 by\nP\u03bd\n \n\u2203k \u2208N :\nk\nX\ns=1\nXs \u2212Ys \u2212(\u00b51 \u2212\u00b52)\n\u221a\n2\u03c32\n>\np\n2k\u03b2(2k, \u03b4)\n!\n= P\n\u0010\n\u2203k \u2208N : Sk >\np\n2k\u03b2(2k, \u03b4)\n\u0011\n(4)\nwhere Sk is a sum of k i.i.d. variables of distribution N (0, 1). Robbins (1970) obtains a non-\nexplicit con\ufb01dence region of risk at most \u03b4 by choosing \u03b2(2k, \u03b4) = log (log(k)/\u03b4) + o(log log(k)).\nThe dependency in k is in some sense optimal, because the Law of Iterated Logarithm (LIL) states\nthat lim supk\u2192\u221eSk/\np\n2k log log(k) = 1 almost surely. Recently, Jamieson et al. (2013) proposed\nan explicit con\ufb01dence region inspired by the LIL. However, Lemma 1 of (Jamieson et al., 2013)\ncannot be used to upper bound (4) by \u03b4 and we provide in Section 6 a result derived independently\n(Lemma 9) that achieves this goal and yields the following result.\nTheorem 3 For \u03b4 small enough, the elimination strategy (3) is \u03b4-PAC with\n\u03b2(t, \u03b4) = log 1\n\u03b4 + 3\n4 log log 1\n\u03b4 + 3\n2 log(1 + log(t/2)).\n(5)\n7\nKAUFMANN CAPP\u00b4E GARIVIER\nAlgorithm 1 \u03b1-Elimination\nRequire: Exploration function \u03b2(t, \u03b4), parameter \u03b1.\n1: Initialization: \u02c6\u00b51(0) = \u02c6\u00b52(0) = 0, \u03c32\n0(\u03b1) = 1. t = 0.\n2: while |\u02c6\u00b51(t) \u2212\u02c6\u00b52(t)| \u2264\np\n2\u03c32\nt (\u03b1)\u03b2(t, \u03b4) do\n3:\nt = t + 1.\n4:\nIf \u2308\u03b1t\u2309= \u2308\u03b1(t \u22121)\u2309, At = 2, else At = 1.\n5:\nObserve Zt \u223c\u03bdAt and compute the empirical means \u02c6\u00b51(t) and \u02c6\u00b52(t).\n6:\nCompute \u03c32\nt (\u03b1) = \u03c32\n1/\u2308\u03b1t\u2309+ \u03c32\n2/(t \u2212\u2308\u03b1t\u2309).\n7: end while\n8: return a = argmax\na=1,2\n\u02c6\u00b5a(t)\n3.2.2. MISMATCHED VARIANCES\nIn the case where \u03c31 \u0338= \u03c32, we rely on an \u03b1-elimination strategy, described in Algorithm 1. For\na = 1, 2, \u02c6\u00b5a(t) denotes the empirical mean of the samples gathered from arm a up to time t. The\nalgorithm is based on a non-uniform sampling strategy governed by the parameter \u03b1 \u2208(0, 1) which\nensures that, at the end of every round t, N1(t) = \u2308\u03b1t\u2309, N2(t) = t \u2212\u2308\u03b1t\u2309and \u02c6\u00b51(t) \u2212\u02c6\u00b52(t) \u223c\nN\n\u0000\u00b51 \u2212\u00b52, \u03c32\nt (\u03b1)\n\u0001\n. The sampling schedule used here is thus deterministic.\nTheorem 4 shows that the \u03c31/(\u03c31 + \u03c32)-elimination algorithm, with a suitable exploration rate,\nis \u03b4-PAC and matches the lower bound on E\u03bd[\u03c4], at least asymptotically when \u03b4 \u21920. Its proof can\nbe found in Appendix C.\nTheorem 4 If \u03b1 = \u03c31/(\u03c31 + \u03c32), the \u03b1-elimination strategy using the exploration rate \u03b2(t, \u03b4) =\nlog t\n\u03b4 + 2 log log(6t) is \u03b4-PAC on M and satis\ufb01es, for every \u03bd \u2208M, for every \u03f5 > 0,\nE\u03bd[\u03c4] \u2264(1 + \u03f5)2(\u03c31 + \u03c32)2\n(\u00b51 \u2212\u00b52)2 log\n\u00121\n\u03b4\n\u0013\n+ o\u03f5\n\u03b4\u21920\n\u0012\nlog\n\u00121\n\u03b4\n\u0013\u0013\n.\nRemark 5 When \u03c31 = \u03c32, 1/2-elimination reduces, up to rounding effects, to the elimination\nprocedure described in Section 3.2.1, for which Theorem 3 suggests an exploration rate of order\nlog(log(t)/\u03b4). As the feasibility of this exploration rate when \u03c31 \u0338= \u03c32 is yet to be established, we\nfocus on Gaussian bandits with equal variances in the numerical experiments of Section 5.\n4. The Bernoulli Case\nWe consider in this section the class of Bernoulli bandit models de\ufb01ned by\nM = {\u03bd = (B(\u00b51), B(\u00b52)) : (\u00b51, \u00b52) \u2208]0; 1[2, \u00b51 \u0338= \u00b52},\nwhere each arm can be equivalently parametrized by the natural parameter of the exponential\nfamily, \u03b8a = log(\u00b5a/(1 \u2212\u00b5a)). Following the notation of Section 2, the Kullback-Leibler di-\nvergence between two Bernoulli distributions can be either expressed as a function of the means,\nKL(B(\u00b51), B(\u00b52)), or of the natural parameters, K(\u03b81, \u03b82).\nIn this Section, we prove that \u03baC(\u03bd) > \u03baB(\u03bd) for Bernoulli bandit models (Proposition 7).\nTo do so, we \ufb01rst introduce a static strategy matching the lower bound of Theorem 1 in the \ufb01xed-\nbudget case (Proposition 6). This strategy is reminiscent of the algorithm exhibited for Gaussian\n8\nON THE COMPLEXITY OF A/B TESTING\nbandits in Section 3 and uses parameter-dependent non uniform sampling. This strategy is not\ndirectly helpful in practice but we show that it can be closely approximated by an algorithm using\nthe uniform sampling strategy. In the \ufb01xed-con\ufb01dence setting we similarly conjecture that little\ncan be gained from using a non-uniform sampling strategy and propose an algorithm based on a\nnon-trivial stopping strategy that is believed to match the bound of Theorem 2.\nProposition 6 Let \u03b1(\u03b81, \u03b82) be de\ufb01ned by\n\u03b1(\u03b81, \u03b82) = \u03b8\u2217\u2212\u03b81\n\u03b82 \u2212\u03b81\nwhere K(\u03b8\u2217, \u03b81) = K(\u03b8\u2217, \u03b82).\nFor all t, the static strategy that allocates \u2308\u03b1(\u03b81, \u03b82)t\u2309samples to arm 1 , and recommends the\nempirical best arm, satis\ufb01es pt(\u03bd) \u2264exp(\u2212K\u2217(\u03b81, \u03b82)t).\nThis result, proved in Appendix D, shows in particular that for every \u03bd \u2208M there exists a\nconsistent static strategy such that\nlim inf\nt\u2192\u221e\u22121\nt log pt \u2265K\u2217(\u03b81, \u03b82),\nand hence that \u03baB(\u03bd) =\n1\nK\u2217(\u03b81, \u03b82).\nHowever, as \u03b1(\u03b81, \u03b82) depends in the Bernoulli case on the unknown means of the arms, this optimal\nstatic strategy is not useful in practice. So far, it is not known whether there exists a universal\nstrategy such that pt(\u03bd) \u2264exp(\u2212K\u2217(\u03b81, \u03b82)t) for all bandit instance \u03bd \u2208M.\nFor Bernoulli bandit models it can be checked that for all \u03bd \u2208M, K\u2217(\u03b81, \u03b82) < K\u2217(\u03b81, \u03b82).\nThis fact together with Proposition 6 and Theorem 1 yields the following inequality.\nProposition 7 For all \u03bd \u2208M, \u03baC(\u03bd) > \u03baB(\u03bd).\nIn the speci\ufb01c case of Bernoulli distributions, there is a strong incentive to use uniform sam-\npling: the quantities I\u2217(\u03bd) and I\u2217(\u03bd) introduced in Theorem 2 appear to be very close to c\u2217(\u03bd)\nand c\u2217(\u03bd) respectively. This fact is illustrated in Figure 1, on which we represent these different\nquantities, that are functions of the means \u00b51, \u00b52 of the arms, as a function of \u00b51, for two \ufb01xed\nvalues of \u00b52. Therefore, algorithms matching the bounds of Theorem 2 provide upper bounds on\n\u03baB(\u03bd) (resp. \u03baC(\u03bd)) very close to 1/c\u2217(\u03bd) (resp. 1/c\u2217(\u03bd)). In the \ufb01xed-budget setting, Lemma\n15 shows that the strategy with uniform sampling that recommends the empirical best arm, sat-\nis\ufb01es pt(\u03bd) \u2264e\u2212tI\u2217(\u03bd), and matches the bound of Theorem 2 (see Remark 16 in Appendix D).\nHence, problem-dependent optimal strategy described above can be approximated by a very sim-\nple, universal algorithm.\nSimilarly, \ufb01nding an algorithm for the \ufb01xed-con\ufb01dence setting sam-\npling the arms uniformly and matching the bound of Theorem 2 is a crucial matter. This boils\ndown to \ufb01nding a good stopping rule. In all the algorithms studied so far, the stopping rule was\nbased on the difference of the empirical means of the arms. For Bernoulli arms, such a strat-\negy can be analyzed with the tools provided in this paper: the algorithm stopping for t such that\n\u02c6\u00b51,t/2 \u2212\u02c6\u00b52,t/2 >\np\n2\u03b2(t, \u03b4)/t with \u03b2(t, \u03b4) as in Theorem 3 is \u03b4-PAC and its expected running time\nbounded by 2/(\u00b51 \u2212\u00b52)2 log 1\n\u03b4 + o\n\u0000log 1\n\u03b4\n\u0001\n. Yet, Pinsker\u2019s inequality implies that I\u2217(\u00b51, \u00b52) >\n(\u00b51 \u2212\u00b52)2/2 and this algorithm is thus not optimal with respect to Theorem 2. The approximation\nI\u2217(\u00b51, \u00b52) = (\u00b51 \u2212\u00b52)2/(8\u00b51(1 \u2212\u00b51)) + o\n\u0000(\u00b51 \u2212\u00b52)2\u0001\nsuggests that the loss with respect to the\n9\nKAUFMANN CAPP\u00b4E GARIVIER\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n\u00b52=0.2\n \n \nc*( . ,\u00b52)\nI* ( . ,\u00b52)\nc*( . ,\u00b52)\n I*( . ,\u00b52)\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n\u00b52=0.5\n \n \nc*( . ,\u00b52)\nI* ( . ,\u00b52)\nc*( . ,\u00b52)\n I*( . ,\u00b52)\nFigure 1: Comparison of different information terms in Bernoulli bandit models.\noptimal error exponent is particularly signi\ufb01cant when both means are close to 0 or 1. The stopping\nrule we propose to circumvent this drawback is the following:\n\u03c4 = inf\n\b\nt \u22082N\u2217: tI\u2217(\u02c6\u00b51,t/2, \u02c6\u00b52,t/2) > log ((log(t) + 1)/\u03b4)\n\t\n.\n(6)\nThis algorithm is related to the KL-LUCB algorithm of Kaufmann and Kalyanakrishnan (2013). In-\ndeed, I\u2217(x, y) mostly coincides with K\u2217(B(x), B(y)) (Figure 1) and a closer examination shows that\nthe stopping criterion in KL-LUCB for two arms is exactly of the form tK\u2217(B(\u02c6\u00b51,t/2), B(\u02c6\u00b52,t/2)) >\n\u03b2(t, \u03b4). The results of Kaufmann and Kalyanakrishnan (2013) show in particular that the algorithm\nbased on (6) is provably \u03b4-PAC for appropriate choices of \u03b2(t, \u03b4). However, by analogy with the\nresult of Theorem 3 we believe that the analysis of Kaufmann and Kalyanakrishnan (2013) is too\nconservative and that the proposed approach should be \u03b4-PAC for exploration rates \u03b2(t, \u03b4) that grow\nas a function of t only at rate log log t.\n5. Numerical Experiments and Discussion\nThe goal of this Section is twofold: to compare results obtained in the \ufb01xed-budget and \ufb01xed-\ncon\ufb01dence settings and to illustrate the improvement resulting from the adoption of the reduced\nexploration rate of Theorem 3.\nIn Figure 2, we consider two bandit models: the \u2019easy\u2019 one is N (0.5, 0.25) \u2297N (0, 0.25),\n\u03ba = 8 (left) and the \u2019dif\ufb01cult\u2019 one is N (0.01, 0.25) \u2297N (0, 0.25), \u03ba = 20000 (right). In the\n\ufb01xed-budget setting, stars (\u2019*\u2019) report the probability of error pn(\u03bd) as a function of n. In the \ufb01xed-\ncon\ufb01dence setting, we plot both the empirical probability of error by circles (\u2019O\u2019) and the speci\ufb01ed\nmaximal error probability \u03b4 by crosses (\u2019X\u2019) as a function of the empirical average of the running\ntimes. Note the logarithmic scale used for the probabilities on the y-axis. All results are averaged on\nN = 106 independent Monte Carlo replications. For comparison purposes, a plain line represents\nthe theoretical rate x 7\u2192exp(\u2212x/\u03ba) which is a straight line on the log scale.\nIn the \ufb01xed-con\ufb01dence setting, we report results for algorithms of the form (3) with g(t, \u03b4) =\np\n2\u03c32t\u03b2(t, \u03b4) for three different exploration rates \u03b2(t, \u03b4). The exploration rate we consider are: the\nprovably-PAC rate of Robbins\u2019 algorithm log(t/\u03b4) (large blue symbols), the conjectured \u2019optimal\u2019\nexploration rate log((log(t) + 1)/\u03b4), almost provably \u03b4-PAC according to Theorem 3 (bold green\nsymbols), and the rate log(1/\u03b4), which would be appropriate if we were to perform the stopping test\n10\nON THE COMPLEXITY OF A/B TESTING\n0\n10\n20\n30\n40\n50\n60\n70\n10\n\u22125\n10\n\u22124\n10\n\u22123\n10\n\u22122\n10\n\u22121\n10\n0\n \n \nFixed\u2212Budget setting\nSPRT\nlog(1/delta)\nlog(log(t)/delta)\nlog(t/delta)\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\nx 10\n5\n10\n\u22124\n10\n\u22123\n10\n\u22122\n10\n\u22121\n10\n0\n \n \nFixed\u2212Budget setting\nSPRT\nlog(1/delta)\nlog(log(t)/delta)\nlog(t/delta)\nFigure 2: Experimental results (descriptions in text; plots best viewed in color).\nonly at a single pre-speci\ufb01ed time (orange symbols). For each algorithm, the log probability of error\nis approximately a linear function of the number of samples, with a slope close to \u22121/\u03ba, where \u03ba\nis the complexity. We can visualize the gain in sample complexity achieved by smaller exploration\nrates, but while the rate log((log(t) + 1)/\u03b4) appears to guarantee the desired probability of error\nacross all problems, the use of log(1/\u03b4) seems too risky, as one can see that the probability of error\nbecomes larger than \u03b4 on dif\ufb01cult problems. To illustrate the gain in sample complexity when the\nmeans of the arms are known, we add in red the SPRT algorithm mentioned in the introduction along\nwith the theoretical relation between the probability of error and the expected number of samples,\nmaterialized as a dashed line. The SPRT stops for t such that |(\u00b51\u2212\u00b52)(S1,t/2\u2212S2,t/2)| > log(1/\u03b4).\nRobbins\u2019 algorithm is \u03b4-PAC and matches the complexity (which is illustrated by the slope\nof the measures), though in practice the use of the exploration rate log((log(t) + 1)/\u03b4) leads to\nhuge gain in terms of number of samples used. It is important to keep in mind that running times\nplay the same role as error exponents and hence the threefold increase of average running times\nobserved on the rightmost plot of Figure 2 when using \u03b2(t, \u03b4) = log(t/\u03b4) is really prohibitive. This\nillustrates the asymptotic nature of our notion of complexity: the leading term in E\u03bd[\u03c4] is indeed\n\u03ba log(1/\u03b4) but there is a second-order constant term which is not negligible for \ufb01xed value of \u03b4.\nJamieson et al. (2013) implicitly consider an alternative complexity for Gaussian bandit models\nwith common known variance: for a \ufb01xed value of \u03b4, if \u2206= \u00b51 \u2212\u00b52, they show that when the gap\n\u2206goes to zeros, the sample complexity if of order some constant\u2014that depends on \u03b4\u2014multiplied\nby \u2206\u22122 log log \u2206\u22122.\nIf one compares on each problem the results for the \ufb01xed-budget setting to those for the best\n\u03b4-PAC algorithm (in green), one can see that to obtain the same probability of error, the \ufb01xed-\ncon\ufb01dence algorithm needs an average number of samples of order at least twice larger than the\ndeterministic number of samples required by the \ufb01xed-budget setting algorithm. This remark should\nbe related to the fact that a \u03b4-PAC algorithm is designed to be uniformly good across all problems,\nwhereas consistency is a weak requirement in the \ufb01xed-budget setting: any strategy that draws both\narm in\ufb01nitely often and recommends the empirical best is consistent. Figure 2 shows that when the\nvalues of \u00b51 and \u00b52 are unknown, the sequential version of the test is no more preferable to its batch\ncounterpart and can even become much worse if the exploration rate \u03b2(t, \u03b4) is chosen too conser-\nvatively. This observation should be mitigated by the fact that the sequential (or \ufb01xed-con\ufb01dence)\n11\nKAUFMANN CAPP\u00b4E GARIVIER\napproach is adaptive with respect to the dif\ufb01culty of the problem whereas it is impossible to predict\nthe ef\ufb01ciency of a batch (or \ufb01xed-budget) experiment without some prior knowledge regarding the\nproblem under consideration.\n6. Elements of Proof\n6.1. Proof of Theorem 1\nThe cornerstone of the proof of all the lower bounds given in this paper is Lemma 8 which relates\nthe probabilities of the same event under two different models to the expected number of draws of\neach arm. Its proof, which may be found in Appendix A, encapsulates the technical aspects of the\nchange of distributions. Na(t) denotes the number of draws of arm a up to round t and Na = Na(\u03c4)\nis the total number of draws of arm a by some algorithm A.\nLemma 8 Let \u03bd and \u03bd\u2032 be two bandit models. For any A \u2208F\u03c4 such that 0 < P\u03bd(A) < 1\nE\u03bd[N1]KL(\u03bd1, \u03bd\u2032\n1) + E\u03bd[N2]KL(\u03bd2, \u03bd\u2032\n2) \u2265d(P\u03bd(A), P\u03bd\u2032(A)),\n(7)\nwhere d(x, y) := KL(B(x), B(y)) = x log(x/y) + (1 \u2212x) log\n\u0000(1 \u2212x)/(1 \u2212y)\n\u0001\n.\nWithout loss of generality, assume that the bandit model \u03bd = (\u03bd1, \u03bd2) is such that a\u2217= 1.\nConsider any alternative bandit model \u03bd\u2032 = (\u03bd\u2032\n1, \u03bd\u2032\n2) in which a\u2217= 2 and the event A = (\u02c6a = 1)\nwhere \u02c6a is the arm chosen by algorithm A. Clearly A \u2208F\u03c4.\nFixed-Con\ufb01dence Setting.\nLet A be a \u03b4-PAC algorithm. Then it is correct on both \u03bd and \u03bd\u2032\nand satis\ufb01es P\u03bd(A) \u22651 \u2212\u03b4 and P\u03bd\u2032(A) \u2264\u03b4. Using monotonicity properties of d (for example\nx 7\u2192d(x, y) is increasing when x > y and decreasing when x < y) and inequality (7) in Lemma 8\nyields E\u03bd[N1]KL(\u03bd1, \u03bd\u2032\n1) + E\u03bd[N2]KL(\u03bd2, \u03bd\u2032\n2) \u2265d(\u03b4, 1 \u2212\u03b4), and hence\nE\u03bd[\u03c4] \u2265\nd(\u03b4, 1 \u2212\u03b4)\nmaxa=1,2 KL(\u03bda, \u03bd\u2032a),\nusing that \u03c4 = N1+N2. Optimizing over the possible model \u03bd\u2032 satisfying \u00b5\u2032\n1 < \u00b5\u2032\n2 to make the right\nhand side of the inequality as large as possible gives the result, using moreover that for \u03b4 \u22640.15, it\ncan be shown that d(1 \u2212\u03b4, \u03b4) \u2265log(1/(2\u03b4)).\nFixed-Budget Setting.\nInequality (7) in Lemma 8 applied to A yields\nE\u03bd\u2032[N1(t)]KL(\u03bd\u2032\n1, \u03bd1) + E\u03bd\u2032[N2(t)]KL(\u03bd\u2032\n2, \u03bd2) \u2265d(P\u03bd\u2032(A), P\u03bd(A)).\nNote that pt(\u03bd) = 1 \u2212P\u03bd(A) and pt(\u03bd\u2032) = P\u03bd\u2032(A). As algorithm A is correct on both \u03bd and \u03bd\u2032, for\nevery \u03f5 > 0 there exists t0(\u03f5) such that for all t \u2265t0(\u03f5), P\u03bd\u2032(A) \u2264\u03f5 \u2264P\u03bd(A). For t \u2265t0(\u03f5),\nE\u03bd\u2032[N1(t)]KL(\u03bd\u2032\n1, \u03bd1) + E\u03bd\u2032[N2(t)]KL(\u03bd\u2032\n2, \u03bd2) \u2265d(\u03f5, 1 \u2212pt(\u03bd)) \u2265(1 \u2212\u03f5) log 1 \u2212\u03f5\npt(\u03bd) + \u03f5 log \u03f5.\nTaking the limsup (denoted by lim) and letting \u03f5 go to zero, one can show that\nlim \u22121\nt log pt(\u03bd)\n\u2264\nlim\nhE\u03bd\u2032[N1(t)]\nt\nKL(\u03bd\u2032\n1, \u03bd1) + E\u03bd\u2032[N2(t)]\nt\nKL(\u03bd\u2032\n2, \u03bd2)\ni\n\u2264max\na=1,2 KL(\u03bd\u2032\na, \u03bda).\nOptimizing over the possible model \u03bd\u2032 satisfying \u00b5\u2032\n1 < \u00b5\u2032\n2 to make the right hand side of the\ninequality as small as possible gives the result.\n12\nON THE COMPLEXITY OF A/B TESTING\n6.2. Proof of Theorem 3\nAccording to (4), the proof of Theorem 3 boils down to \ufb01nding an exploration rate such that P(\u2203t \u2208\nN\u2217: St >\np\n2\u03c32t\u03b2(t, \u03b4)) \u2264\u03b4, where St = X1 +\u00b7 \u00b7 \u00b7+Xt is a sum of i.i.d. normal random variable.\nLemma 9 provides such a con\ufb01dence region. Its proof can be found in Appendix B.\nLemma 9 Let \u03b6(u) = P\nk\u22651 k\u2212u. Let X1, X2, . . . be independent random variables such that, for\nall \u03bb \u2208R, \u03c6(\u03bb) := log E[exp(\u03bbX1)] \u2264\u03bb2\u03c32/2. For every positive integer t let St = X1+\u00b7 \u00b7 \u00b7+Xt.\nThen, for all \u03b2 > 1 and x \u2265\n8\n(e\u22121)2 ,\nP\n\u0010\n\u2203t \u2208N\u2217: St >\np\n2\u03c32t(x + \u03b2 log log(et))\n\u0011\n\u2264\u221ae \u03b6\n\u0010\n\u03b2\n\u00001 \u22121\n2x\n\u0001\u0011\u0010 \u221ax\n2\n\u221a\n2 + 1\n\u0011\u03b2\nexp(\u2212x).\nLet \u03b2(t, \u03b4) be of the form \u03b2(t, \u03b4) = log 1\n\u03b4 +c log log 1\n\u03b4 +d log log(et), for some constants c > 0\nand d > 1. Lemma 9 yields\nP\n\u0010\n\u2203t \u2208N : St >\np\n2\u03c32t\u03b2(t, \u03b4)\n\u0011\n\u2264\u03b6\n\u0010\nd\n\u00001 \u2212\n1\n2(z + c log z)\n\u0001\u0011\n\u221ae\n(2\n\u221a\n2)d\n(\u221az + c log z +\n\u221a\n8)d\nzc\n\u03b4,\nwhere z := log 1\n\u03b4 > 0. To upper bound the above probability by \u03b4, at least for large values of z\n(which corresponds to small values of \u03b4), it suf\ufb01ces to choose the parameters c and d such that\n\u221ae \u03b6\n\u0010\nd\n\u00001 \u2212\n1\n2(z + c log z)\n\u0001\u0011\n1\n(2\n\u221a\n2)d\n(\u221az + c log z + 2\n\u221a\n2)d\nzc\n\u22641.\nFor c = d/2, the left hand side tends to \u221ae\u03b6 (d)/(2\n\u221a\n2)d when z goes to in\ufb01nity, which is smaller\nthan 1 for d \u22651.47. Thus, for \u03b4 small enough, the desired inequality holds for d = 3/2 and\nc = 3/4, which corresponds to the exploration rate of Theorem 3.\n7. Conclusion\nWe provide distribution-dependent lower bounds for best-arm identi\ufb01cation in the context of two-\narmed bandit models. These bounds involve information-theoretic quantities that re\ufb02ect the typical\ncauses of failure, which are different from those appearing in regret analysis. For Gaussian and\nBernoulli bandit models, we exhibit matching algorithms showing that these bounds are (mostly)\ntight, highlighting relationships between the complexities of the \ufb01xed-budget and \ufb01xed-con\ufb01dence\nsettings. Our numerical experiments illustrate the signi\ufb01cance of using appropriate exploration rates\nin the context of best arm(s) identi\ufb01cation and we believe that Lemma 8 can be adapted to deal with\nmore general K-armed bandit scenarios.\nThese results suggest three practical implications for A/B testing. First, for Binary and Gaussian-\nlike responses with matched variances it is reasonable to consider only tests\u2014i.e., strategies using\nuniform sampling\u2014rather than general sequential sampling strategies. Second, using a sequential\nstopping rule in this context is mostly of interest because it does not requires prior knowledge of\nthe complexity of the problem. It should however not be expected to reduce the (average) running\ntime of the experiment for a given probability of error. This leads to the third message regarding the\nutmost importance of using proper (i.e., provably \u03b4-PAC but not too conservative) exploration rates\nwhen using a sequential stopping rule.\n13\nKAUFMANN CAPP\u00b4E GARIVIER\nAcknowledgments\nWe thank S\u00b4ebastien Bubeck for fruitful discussions during the visit of the \ufb01rst author at Princeton\nUniversity. This work was supported by the ANR-2010-COSI-002 grant of the French National\nResearch Agency.\nReferences\nS. Agrawal and N. Goyal. Further Optimal Regret Bounds for Thompson Sampling. In Conference\non Arti\ufb01cial Intelligence and Statistics (AISTATS), 2013.\nJ-Y. Audibert, S. Bubeck, and R. Munos. Best Arm Identi\ufb01cation in Multi-armed Bandits. In\nConference on Learning Theory (COLT), 2010.\nP. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit problem.\nMachine Learning, 47(2):235\u2013256, 2002.\nRobert Bechhofer, Jack Kiefer, and Milton Sobel. Sequential identi\ufb01cation and ranking procedures.\nThe university of Chicago Press, 1968.\nS. Bubeck, R. Munos, and G. Stoltz. Pure Exploration in Finitely Armed and Continuous Armed\nBandits. Theoretical Computer Science, 412:1832\u20131852, 2011.\nS. Bubeck, T. Wang, and N. Viswanathan. Multiple Identi\ufb01cations in multi-armed bandits. In\nInternational Conference on Machine Learning (ICML), 2013.\nO. Capp\u00b4e, A. Garivier, O-A. Maillard, R. Munos, and G. Stoltz. Kullback-Leibler upper con\ufb01dence\nbounds for optimal sequential allocation. Annals of Statistics, 41(3):1516\u20131541, 2013.\nT. Cover and J. Thomas. Elements of Information Theory (2nd Edition). Wiley, 2006.\nE. Even-Dar, S. Mannor, and Y. Mansour. Action Elimination and Stopping Conditions for the\nMulti-Armed Bandit and Reinforcement Learning Problems. Journal of Machine Learning Re-\nsearch, 7:1079\u20131105, 2006.\nV. Gabillon, M. Ghavamzadeh, and A. Lazaric. Best Arm Identi\ufb01cation: A Uni\ufb01ed Approach to\nFixed Budget and Fixed Con\ufb01dence. In Neural Information and Signal Processing (NIPS), 2012.\nJ. Honda and A. Takemura.\nAn asymptotically optimal policy for \ufb01nite support models in the\nmultiarmed bandit problem. Machine Learning, 85(3):361\u2013391, 2011.\nK. Jamieson, M. Malloy, R. Nowak, and S. Bubeck. lil\u2019UCB: an optimal exploration algorithm for\nmulti-armed bandits. arXiv:1312.7308, 2013.\nC. Jennison, I.M. Johnstone, and B.W. Turnbull. Asymptotically optimal procedures for sequential\nadaptive selection of the best of several normal means. Statistical Decision Theory and Related\nTopics III, 2:55\u201386, 1982.\nS. Kalyanakrishnan, A. Tewari, P. Auer, and P. Stone. PAC subset selection in stochastic multi-\narmed bandits. In International Conference on Machine Learning (ICML), 2012.\n14\nON THE COMPLEXITY OF A/B TESTING\nE. Kaufmann and S. Kalyanakrishnan. Information complexity in bandit subset selection. In Con-\nference On Learning Theory (COLT), 2013.\nE. Kaufmann, A. Garivier, and O. Capp\u00b4e. On Bayesian Upper-Con\ufb01dence Bounds for Bandit Prob-\nlems. In AISTATS, 2012a.\nE. Kaufmann, N. Korda, and R. Munos. Thompson Sampling : an Asymptotically Optimal Finite-\nTime Analysis. In Algorithmic Learning Theory (ALT), 2012b.\nT.L. Lai and H. Robbins. Asymptotically ef\ufb01cient adaptive allocation rules. Advances in Applied\nMathematics, 6(1):4\u201322, 1985.\nS. Mannor and J. Tsitsiklis. The Sample Complexity of Exploration in the Multi-Armed Bandit\nProblem. Journal of Machine Learning Research, pages 623\u2013648, 2004.\nH. Robbins. Some aspects of the sequential design of experiments. Bulletin of the American Math-\nematical Society, 58(5):527\u2013535, 1952.\nH. Robbins. Statistical Methods Related to the law of the iterated logarithm. Annals of Mathematical\nStatistics, 41(5):1397\u20131409, 1970.\nD. Siegmund. Sequential Analysis. Springer-Verlag, 1985.\nW.R. Thompson. On the likelihood that one unknown probability exceeds another in view of the\nevidence of two samples. Biometrika, 25:285\u2013294, 1933.\nA. Wald. Sequential tests of statistical hypotheses. Annals of Mathematical Statistics, 16(2):117\u2013\n186, 1945.\nAppendix A. Proof of Lemma 8: Changes of Distributions\nUnder the identi\ufb01ability assumption, there exists a common measure \u03bb such that for all \u03bd = (\u03bd1, \u03bd2),\nfor all a \u2208{1, 2} \u03bda has a density fa with respect to \u03bb.\nLet \u03bd \u2208M be a bandit model, and consider an alternative bandit model \u03bd\u2032 \u2208M. fa, f\u2032\na are the\ndensities of \u03bda, \u03bd\u2032\na respectively and one can introduce the log-likelihood ratio of the observations up\nto time t under an algorithm A:\nLt :=\nt\nX\ns=1\n1(As=1) log\n\u0012f1(Zs)\nf\u2032\n1(Zs)\n\u0013\n+\nt\nX\ns=1\n1(As=2) log\n\u0012f2(Zs)\nf\u2032\n2(Zs)\n\u0013\n.\nThe key element in a change of distribution is the following classical lemma (whose proof is omit-\nted) that relates the probabilities of an event under P\u03bd and P\u03bd\u2032 through the log-likelihood ratio of\nthe observations. Such a result has often been used in the bandit literature for \u03bd and \u03bd\u2032 that differ\njust from one arm (either \u03bd1 = \u03bd\u2032\n1 or \u03bd2 = \u03bd\u2032\n2), for which the expression of the log-likelihood ratio\nis simpler. As we will see, here we consider more general changes of distributions.\nLemma 10 Let \u03c3 be any stopping time with respect to Ft. For every event A \u2208F\u03c3 (i.e. A such\nthat A \u2229(\u03c3 = t) \u2208Ft),\nP\u03bd\u2032(A) = E\u03bd[1A exp(\u2212L\u03c3)]\n15\nKAUFMANN CAPP\u00b4E GARIVIER\nLet \u03c4 be the stopping rule of algorithm A. We start by showing that for all A \u2208F\u03c4, P\u03bd(A) = 0\nif and only if P\u03bd\u2032(A) = 0. Thus, if 0 < P\u03bd(A) < 1 one also has 0 < P\u03bd\u2032(A) < 1 and the\nquantity d(P\u03bd(A), P\u03bd\u2032(A)) in the statement of Lemma 8 is well de\ufb01ned. Let A \u2208F\u03c4. Lemma 10\nyields P\u03bd\u2032(A) = E\u03bd[1A exp(\u2212L\u03c4)]. Thus P\u03bd\u2032(A) = 0 implies 1A exp(\u2212L\u03c4) = 0 P\u03bd \u2212a.s. As\nP\u03bd(\u03c4 < +\u221e) = 1, P\u03bd(exp(L\u03c4) > 0) = 1 and P\u03bd\u2032(A) = 0 \u21d2P\u03bd(A) = 0. A similar reasoning\nyields P\u03bd(A) = 0 \u21d2P\u03bd\u2032(A) = 0.\nWe now prove Lemma 8. Let A \u2208F\u03c4 be such that 0 < P\u03bd(A) < 1. Then 0 < P\u03bd\u2032(A) < 1.\nLemma 10 and the conditional Jensen inequality leads to\nP\u03bd\u2032(A) = E\u03bd[exp(\u2212L\u03c4)1A] = E\u03bd[E\u03bd[exp(\u2212L\u03c4)|1A]1A]\n\u2265E\u03bd[exp (\u2212E\u03bd[L\u03c4|1A]) 1A] = E\u03bd[exp (\u2212E\u03bd[L\u03c4|A]) 1A]\n= exp (\u2212E\u03bd[L\u03c4|A]) P\u03bd(A).\nWriting the same for the event A yields P\u03bd\u2032(A) \u2265exp\n\u0000\u2212E\u03bd[L\u03c4|A]\n\u0001\nP\u03bd(A) and \ufb01nally\nE\u03bd[L\u03c4|A] \u2265log P\u03bd(A)\nP\u03bd\u2032(A)\nand E\u03bd[L\u03c4|A] \u2265log P\u03bd(A)\nP\u03bd\u2032(A).\nTherefore one can write\nE\u03bd[L\u03c4]\n=\nE\u03bd[L\u03c4|A]P\u03bd(A) + E\u03bd[L\u03c4|A]P\u03bd(A)\n\u2265\nP\u03bd(A) log P\u03bd(A)\nP\u03bd\u2032(A) + P\u03bd(A) log P\u03bd(A)\nP\u03bd\u2032(A) = d(P\u03bd(A), P\u03bd\u2032(A)).\n(8)\nIntroducing (Ya,t), the sequence of i.i.d.\nsamples successively observed from arm a, the log-\nlikelihood ratio Lt can be rewritten\nLt =\n2\nX\na=1\nNa(t)\nX\nt=1\nlog\n\u0012fa(Ya,t)\nf\u2032a(Ya,t)\n\u0013\n;\nand E\u03bd\n\u0014\nlog\n\u0012fa(Ya,t)\nf\u2032a(Ya,t)\n\u0013\u0015\n= KL(\u03bda, \u03bd\u2032\na).\nApplying Wald\u2019s Lemma (see for example Siegmund (1985)) to L\u03c4 = P2\na=1\nPNa\nt=1 log\n\u0010\nfa(Ya,t)\nf\u2032a(Ya,t)\n\u0011\n,\nwhere Na = Na(\u03c4) is the total number of draws of arm a, yields\nE\u03bd[L\u03c4] = E\u03bd[N1]KL(\u03bd1, \u03bd\u2032\n1) + E\u03bd[N2]KL(\u03bd2, \u03bd\u2032\n2),\nwhich concludes the proof together with inequality (8).\nAppendix B. Proof of Lemma 9: An Optimal Con\ufb01dence Region\nWe start by stating three technical lemmas, whose proofs are partly omitted.\nLemma 11 For every \u03b7 > 0, every positive integer k, and every integer t such that (1 + \u03b7)k\u22121 \u2264\nt \u2264(1 + \u03b7)k,\nr\n(1 + \u03b7)k\u22121/2\nt\n+\ns\nt\n(1 + \u03b7)k\u22121/2 \u2264(1 + \u03b7)1/4 + (1 + \u03b7)\u22121/4 .\n16\nON THE COMPLEXITY OF A/B TESTING\nLemma 12 For every \u03b7 > 0,\nA(\u03b7) :=\n4\n\u0000(1 + \u03b7)1/4 + (1 + \u03b7)\u22121/4\u00012 \u22651 \u2212\u03b72\n16.\nLemma 13 Let t be such that (1+\u03b7)k\u22121 \u2264t \u2264(1+\u03b7)k. Then, if \u03bb = \u03c3\u22121p\n2zA(\u03b7)/(1 + \u03b7)k\u22121/2,\n\u03c3\n\u221a\n2z \u2265A(\u03b7)z\n\u03bb\n\u221a\nt + \u03bb\u03c32\u221a\nt\n2\n.\nProof:\nA(\u03b7)z\n\u03bb\n\u221a\nt + \u03bb\u03c32\u221a\nt\n2\n= \u03c3\np\n2zA(\u03b7)\n2\n r\n(1 + \u03b7)k\u22121/2\nt\n+\ns\nt\n(1 + \u03b7)k\u22121/2\n!\n\u2264\u03c3\n\u221a\n2z\naccording to Lemma 11.\n\u25a1\nAn important fact is that for every \u03bb \u2208R, because the Xi are \u03c3-subgaussian, Wt = exp(\u03bbSt \u2212\nt \u03bb2\u03c32\n2 )) is a super-martingale, and thus, for every positive u,\nP\n\uf8eb\n\uf8ed[\nt\u22651\n\u001a\n\u03bbSt \u2212t\u03bb2\u03c32\n2\n> u\n\u001b\uf8f6\n\uf8f8\u2264exp(\u2212u).\n(9)\nLet \u03b7 \u2208]0, e \u22121] to be de\ufb01ned later, and let Tk = N \u2229\n\u0002\n(1 + \u03b7)k\u22121, (1 + \u03b7)k\u0002\n.\nP\n\uf8eb\n\uf8ed[\nt\u22651\n\u001a St\n\u03c3\n\u221a\n2t >\np\nx + \u03b2 log log(et)\n\u001b\uf8f6\n\uf8f8\u2264\n\u221e\nX\nk=1\nP\n\uf8eb\n\uf8ed[\nt\u2208Tk\n\u001a St\n\u03c3\n\u221a\n2t >\np\nx + \u03b2 log log(et)\n\u001b\uf8f6\n\uf8f8\n\u2264\n\u221e\nX\nk=1\nP\n\uf8eb\n\uf8ed[\nt\u2208Tk\n\u001a St\n\u03c3\n\u221a\n2t >\np\nx + \u03b2 log (k log(1 + \u03b7))\n\u001b\uf8f6\n\uf8f8.\nWe use that \u03b7 \u2264e \u22121 to obtain the last inequality since this condition implies\nlog(log(e(1 + \u03b7)k\u22121) \u2265log(k log(1 + \u03b7)).\nFor a positive integer k, let zk = x+\u03b2 log (k log(1 + \u03b7)) and \u03bbk = \u03c3\u22121p\n2zkA(\u03b7)/(1 + \u03b7)k\u22121/2.\nLemma 13 shows that for every t \u2208Tk,\n\u001a St\n\u03c3\n\u221a\n2t > \u221azk\n\u001b\n\u2282\n\u001a St\n\u221a\nt > A(\u03b7)zk\n\u03bbk\n\u221a\nt + \u03c32\u03bbk\n\u221a\nt\n2\n\u001b\n.\n17\nKAUFMANN CAPP\u00b4E GARIVIER\nThus, by inequality (9),\nP\n\uf8eb\n\uf8ed[\nt\u2208Tk\n\u001a St\n\u03c3\n\u221a\n2t > \u221azk\n\u001b\uf8f6\n\uf8f8\u2264P\n\uf8eb\n\uf8ed[\nt\u2208Tk\n\u001a St\n\u221a\nt > A(\u03b7)zk\n\u03bbk\n\u221a\nt + \u03c32\u03bbk\n\u221a\nt\n2\n\u001b\uf8f6\n\uf8f8\n= P\n\uf8eb\n\uf8ed[\nt\u2208Tk\n\u001a\n\u03bbkSt \u2212\u03c32\u03bb2\nkt\n2\n> A(\u03b7)zk\n\u001b\uf8f6\n\uf8f8\n\u2264exp (\u2212A(\u03b7)zk) =\nexp(\u2212A(\u03b7)x)\n(k log(1 + \u03b7))\u03b2A(\u03b7) .\nOne chooses \u03b72 = 8/x for x such that x \u2265\n8\n(e\u22121)2 (which ensures \u03b7 \u2264e\u22121). Using Lemma 12,\none obtains that exp(\u2212A(\u03b7)x) \u2264\u221ae exp(\u2212x). Moreover,\n1\nlog(1 + \u03b7) \u22641 + \u03b7\n\u03b7\n=\n\u221ax\n2\n\u221a\n2 + 1 .\nThus,\nP\n\uf8eb\n\uf8ed[\nt\u2208Tk\n\u001a St\n\u03c3\n\u221a\n2t > \u221azk\n\u001b\uf8f6\n\uf8f8\u2264\n\u221ae\nk\u03b2A(\u03b7)\n\u0012 \u221ax\n2\n\u221a\n2 + 1\n\u0013\u03b2A(\u03b7)\nexp(\u2212x) \u2264\n\u221ae\nk\u03b2A(\u03b7)\n\u0012 \u221ax\n2\n\u221a\n2 + 1\n\u0013\u03b2\nexp(\u2212x)\nand hence,\nP\n\uf8eb\n\uf8ed[\nt\u22651\n\u001a St\n\u03c3\n\u221a\n2t >\np\nx + \u03b2 log log(et)\n\u001b\uf8f6\n\uf8f8\u2264\u221ae\u03b6 (\u03b2A(\u03b7))\n\u0012 \u221ax\n2\n\u221a\n2 + 1\n\u0013\u03b2A(\u03b7)\nexp (\u2212x)\n\u2264\u221ae\u03b6\n\u0012\n\u03b2\n\u0012\n1 \u22121\n2x\n\u0013\u0013 \u0012 \u221ax\n2\n\u221a\n2 + 1\n\u0013\u03b2\nexp (\u2212x) ,\nusing the lower bound on A(\u03b7) given in Lemma 12 and the fact that A(\u03b7) is upper bounded by 1.\nAppendix C. Proof of Theorem 4\nLet \u03b1 = \u03c31/(\u03c31 + \u03c32). We \ufb01rst prove that with the exploration rate \u03b2(t, \u03b4) = log(t/\u03b4)+2 log log(6t)\nthe algorithm is \u03b4-PAC. Assume that \u00b51 > \u00b52 and recall \u03c4 = inf{t \u2208N : |dt| >\np\n2\u03c32\nt (\u03b1)\u03b2(t, \u03b4)}.\nThe probability of error of the \u03b1-elimination strategy is upper bounded by\nP\u03bd\n\u0010\nd\u03c4 \u2264\u2212\np\n2\u03c32\u03c4(\u03b1)\u03b2(\u03c4, \u03b4)\n\u0011\n\u2264\nP\u03bd\n\u0010\nd\u03c4 \u2212(\u00b51 \u2212\u00b52) \u2264\u2212\np\n2\u03c32\u03c4(\u03b1)\u03b2(\u03c4, \u03b4)\n\u0011\n\u2264\nP\u03bd\n\u0012\n\u2203t \u2208N\u2217: dt \u2212(\u00b51 \u2212\u00b52) < \u2212\nq\n2\u03c32\nt (\u03b1)\u03b2(t, \u03b4)\n\u0013\n\u2264\n\u221e\nX\nt=1\nexp (\u2212\u03b2(t, \u03b4)) ,\n18\nON THE COMPLEXITY OF A/B TESTING\nby an union bound and Chernoff bound applied to dt \u2212(\u00b51 \u2212\u00b52) \u223cN\n\u00000, \u03c32\nt (\u03b1)\n\u0001\n. The choice of\n\u03b2(t, \u03b4) mentioned above ensures that the series in the right hand side is upper bounded by \u03b4, which\nshows the algorithm is \u03b4-PAC:\n\u221e\nX\nt=1\ne\u2212\u03b2(t,\u03b4) \u2264\u03b4\n\u221e\nX\nt=1\n1\nt(log(6t))2 \u2264\u03b4\n\u0012\n1\n(log 6)2 +\nZ \u221e\n1\ndt\nt(log(6t))2\n\u0013\n= \u03b4\n\u0012\n1\n(log 6)2 +\n1\nlog(6)\n\u0013\n\u2264\u03b4.\nTo upper bound the expected sample complexity, we start by upper bounding the probability\nthat \u03c4 exceeds some deterministic time T:\nP\u03bd(\u03c4 \u2265T)\n\u2264\nP\u03bd\n\u0012\n\u2200t = 1 . . . T, dt \u2264\nq\n2\u03c32\nt (\u03b1)\u03b2(t, \u03b4)\n\u0013\n\u2264P\u03bd\n\u0012\ndT \u2264\nq\n2\u03c32\nT (\u03b1)\u03b2(T, \u03b4)\n\u0013\n=\nP\u03bd\n\u0012\ndT \u2212(\u00b51 \u2212\u00b52) \u2264\u2212\n\u0014\n(\u00b51 \u2212\u00b52) \u2212\nq\n2\u03c32\nT (\u03b1)\u03b2(T, \u03b4)\n\u0015\u0013\n\u2264\nexp\n \n\u2212\n1\n2\u03c32\nT (\u03b1)\n\u0014\n(\u00b51 \u2212\u00b52) \u2212\nq\n2\u03c32\nT (\u03b1)\u03b2(T, \u03b4)\n\u00152!\n.\nThe last inequality follows from Chernoff bound and holds for T such that\n(\u00b51 \u2212\u00b52) >\nq\n2\u03c32\nT (\u03b1)\u03b2(T, \u03b4).\n(10)\nNow, for \u03b3 \u2208]0, 1[ we introduce\nT \u2217\n\u03b3\n:=\ninf\n\u001a\nt0 \u2208N : \u2200t \u2265t0, (\u00b51 \u2212\u00b52) \u2212\nq\n2\u03c32\nt (\u03b1)\u03b2(t, \u03b4) > \u03b3(\u00b51 \u2212\u00b52)\n\u001b\n.\nThis quantity is well de\ufb01ned as \u03c32\nt (\u03b1)\u03b2(t, \u03b4) \u2192\nt\u2192\u221e0. We then upper bound the expectation of \u03c4:\nE\u03bd[\u03c4]\n\u2264\nT \u2217\n\u03b3 +\nX\nT=T \u2217\n\u03b3 +1\nP (\u03c4 \u2265T)\n\u2264\nT \u2217\n\u03b3 +\nX\nT=T \u2217\n\u03b3 +1\nexp\n \n\u2212\n1\n2\u03c32\nT (\u03b1)\n\u0014\n(\u00b51 \u2212\u00b52) \u2212\nq\n2\u03c32\nT (\u03b1)\u03b2(T, \u03b4)\n\u00152!\n\u2264\nT \u2217\n\u03b3 +\n\u221e\nX\nT=T \u2217\n\u03b3 +1\nexp\n\u0012\n\u2212\n1\n2\u03c32\nT (\u03b1)\u03b32(\u00b51 \u2212\u00b52)2\n\u0013\n.\nFor all t \u2208N\u2217, it is easy to show that the following upper bound on \u03c32\nt (\u03b1) holds:\n\u2200t \u2208N, \u03c32\nt (\u03b1) \u2264(\u03c31 + \u03c32)2\nt\n\u00d7\nt \u2212\u03c31\n\u03c32\nt \u2212\u03c31\n\u03c32 \u22121.\n(11)\nUsing the bound (11), one has\nE\u03bd[\u03c4]\n\u2264\nT \u2217\n\u03b3 +\nZ \u221e\n0\nexp\n \n\u2212\nt\n2(\u03c31 + \u03c32)2\nt \u2212\u03c31\n\u03c32 \u22121\nt \u2212\u03c31\n\u03c32\n\u03b32(\u00b51 \u2212\u00b52)2\n!\ndt\n=\nT \u2217\n\u03b3 + 2(\u03c31 + \u03c32)2\n\u03b32(\u00b51 \u2212\u00b52)2 exp\n\u0012\u03b32(\u00b51 \u2212\u00b52)2\n2(\u03c31 + \u03c32)2\n\u0013\n.\n19\nKAUFMANN CAPP\u00b4E GARIVIER\nWe now give an upper bound on T \u2217\n\u03b3 . Let r \u2208[0, e/2 \u22121]. There exists N0(r) such that for\nt \u2265N0(r), \u03b2(t, \u03b4) \u2264log(t1+r/\u03b4). Using also (11), one gets T \u2217\n\u03b3 = max(N0(t), \u02dcT\u03b3), where\n\u02dcT\u03b3 = inf\n(\nt0 \u2208N : \u2200t \u2265t0, (\u00b51 \u2212\u00b52)2\n2(\u03c31 + \u03c32)2 (1 \u2212\u03b3)2t >\nt \u2212\u03c31\n\u03c32 \u22121\nt \u2212\u03c31\n\u03c32\nlog t1+r\n\u03b4\n)\n.\nIf t > (1+\u03b3 \u03c31\n\u03c32 )/\u03b3 one has (t\u2212\u03c31\n\u03c32 \u22121)/(t\u2212\u03c31\n\u03c32 ) \u2264(1 \u2212\u03b3)\u22121. Thus \u02dcT\u03b3 = max((1+\u03b3 \u03c31\n\u03c32 )/\u03b3, T \u2032\n\u03b3),\nwith\nT \u2032\n\u03b3 = inf\n\u001a\nt0 \u2208N : \u2200t \u2265t0, exp\n\u0012 (\u00b51 \u2212\u00b52)2\n2(\u03c31 + \u03c32)2 (1 \u2212\u03b3)3t\n\u0013\n\u2265t1+r\n\u03b4\n\u001b\n.\nThe following Lemma, whose proof can be found below, helps us bound this last quantity.\nLemma 14 For every \u03b2, \u03b7 > 0 and s \u2208[0, e/2], the following implication is true:\nx0 = s\n\u03b2 log\n\u0012e log (1/(\u03b2s\u03b7))\n\u03b2s\u03b7\n\u0013\n\u21d2\n\u2200x \u2265x0, e\u03b2x \u2265xs\n\u03b7 .\nApplying Lemma 14 with \u03b7 = \u03b4, s = 1 + r and\n\u03b2 = (1 \u2212\u03b3)3(\u00b51 \u2212\u00b52)2\n2(\u03c31 + \u03c32)2\nleads to\nT \u2032\n\u03b3 \u2264(1 + r)\n(1 \u2212\u03b3)3 \u00d7 2(\u03c31 + \u03c32)2\n(\u00b51 \u2212\u00b52)2\n\u0014\nlog 1\n\u03b4 + log log 1\n\u03b4\n\u0015\n+ R(\u00b51, \u00b52, \u03c31, \u03c32, \u03b3, r),\nwith\nR(\u00b51, \u00b52, \u03c31, \u03c32, \u03b3, r) =\n1 + r\n(1 \u2212\u03b3)3\n2(\u03c31 + \u03c32)2\n(\u00b51 \u2212\u00b52)2\n\u0014\n1 + (1 + r) log\n\u0012\n2(\u03c31 + \u03c32)2\n(1 \u2212\u03b3)3(\u00b51 \u2212\u00b52)2\n\u0013\u0015\n.\nNow for \u03f5 > 0 \ufb01xed, choosing r and \u03b3 small enough leads to\nE\u03bd[\u03c4] \u2264(1 + \u03f5)2(\u03c31 + \u03c32)2\n(\u00b51 \u2212\u00b52)2\n\u0014\nlog 1\n\u03b4 + log log 1\n\u03b4\n\u0015\n+ C(\u00b51, \u00b52, \u03c31, \u03c32, \u03f5),\nwhere C is a constant independent of \u03b4. This concludes the proof.\n\u25a1\nProof of Lemma 14\nLemma 14 easily follows from the fact that for any s, \u03b7 > 0,\nx0 = s log\n\uf8eb\n\uf8ed\ne log\n\u0010\n1\n\u03b7\n\u0011\n\u03b7\n\uf8f6\n\uf8f8\u21d2\n\u2200x \u2265x0, ex \u2265xs\n\u03b7\n20\nON THE COMPLEXITY OF A/B TESTING\nIndeed, it suf\ufb01ces to apply this statement to x = x\u03b2 and \u03b7 = \u03b7\u03b2s. The mapping x 7\u2192ex \u2212xs/\u03b7 is\nincreasing when x \u2265s. As x0 \u2265s, it suf\ufb01ces to prove that x0 de\ufb01ned above satis\ufb01es ex0 \u2265xs\n0/\u03b7.\nlog\n\u0012xs\n0\n\u03b7\n\u0013\n=\ns log\n \ns log\n \ne log 1\n\u03b7\n\u03b7\n!!\n+ log 1\n\u03b7\n=\ns\n\u0012\nlog(s) + log\n\u0014\nlog 1\n\u03b7 + log\n\u0012\ne log 1\n\u03b7\n\u0013\u0015\u0013\n+ log 1\n\u03b7\n\u2264\ns\n\u0012\nlog(s) + log\n\u0014\n2 log 1\n\u03b7\n\u0015\u0013\n+ log 1\n\u03b7\nwhere we use that for all y, log(y) \u22641\ney. Then\nlog\n\u0012xs\n0\n\u03b7\n\u0013\n\u2264\ns\n\u0012\nlog(s) + log(2) + log log 1\n\u03b7 + log 1\n\u03b7\n\u0013\n.\nFor s \u2264e\n2, log(s) + log(2) \u22641, hence\nlog\n\u0012xs\n0\n\u03b7\n\u0013\n\u2264\ns\n\u0012\n1 + log log 1\n\u03b7 + log 1\n\u03b7\n\u0013\n= s log\n\uf8eb\n\uf8ed\ne log\n\u0010\n1\n\u03b7\n\u0011\n\u03b7\n\uf8f6\n\uf8f8= x0,\nwhich is equivalent to ex0 \u2265xs\n0\n\u03b7 and concludes the proof.\nAppendix D. An Optimal Static Strategy for Bernoulli Bandit Models\nBounding the probability of error of a static strategy using n1 samples from arm 1 and n2 samples\nfrom arm 2 relies on the following Lemma, that applies more generally to exponential families.\nLemma 15 Let (X1,t)t\u2208N and (X2,t)t\u2208N be two independent i.i.d sequences, such that X1,1 \u223c\u03bd\u03b81\nand X2,1 \u223c\u03bd\u03b82 belong to an exponential family. Assume that \u00b5(\u03b81) > \u00b5(\u03b82). Then\nP\n \n1\nn1\nn1\nX\nt=1\nX1,t < 1\nn2\nn2\nX\nt=1\nX2,t\n!\n\u2264exp(\u2212(n1 + n2)g\u03b1(\u03b81, \u03b82)),\n(12)\nwhere \u03b1 =\nn1\nn1+n2 and g\u03b1(\u03b81, \u03b82) := \u03b1K(\u03b1\u03b81 + (1 \u2212\u03b1)\u03b82, \u03b81) + (1 \u2212\u03b1)K(\u03b1\u03b81 + (1 \u2212\u03b1)\u03b82, \u03b82).\nThe function \u03b1 7\u2192g\u03b1(\u03b81, \u03b82), can be maximized analytically, and the value \u03b1\u2217that realizes the\nmaximum is given by\nK(\u03b1\u2217\u03b81 + (1 \u2212\u03b1\u2217)\u03b82, \u03b81)\n=\nK(\u03b1\u2217\u03b81 + (1 \u2212\u03b1\u2217)\u03b82, \u03b82)\n\u03b1\u2217\u03b81 + (1 \u2212\u03b1\u2217)\u03b81\n=\n\u03b8\u2217\n\u03b1\u2217\n=\n\u03b8\u2217\u2212\u03b82\n\u03b81 \u2212\u03b82\nwhere \u03b8\u2217is de\ufb01ned by K(\u03b8\u2217, \u03b81) = K(\u03b8\u2217, \u03b82) = K\u2217(\u03b81, \u03b82). More interestingly, the associated rate\nis such that\ng\u03b1\u2217(\u03b81, \u03b82) = \u03b1\u2217K(\u03b8\u2217, \u03b81) + (1 \u2212\u03b1\u2217)K(\u03b8\u2217, \u03b82) = K\u2217(\u03b81, \u03b82),\nwhich leads to Proposition 6.\n21\nKAUFMANN CAPP\u00b4E GARIVIER\nRemark 16 When \u00b51 > \u00b52, applying Lemma 15 with n1 = n2 = t/2 yields\nP\n\u0000\u02c6\u00b51,t/2 < \u00b52,t/2\n\u0001\n\u2264exp\n\uf8eb\n\uf8ed\u2212\nK\n\u0010\n\u03b81, \u03b81+\u03b82\n2\n\u0011\n+ K\n\u0010\n\u03b82, \u03b81+\u03b82\n2\n\u0011\n2\nt\n\uf8f6\n\uf8f8= exp\n\u0000\u2212I\u2217(\u03bd)t\n\u0001\n,\nwhich shows that uniform sampling matches the lower bound of Theorem 2.\nProof of Lemma 15\nThe i.i.d. sequences (X1,t)t\u2208N and (X2,t)t\u2208N have respective densities f\u03b81\nand f\u03b82 where f\u03b8(x) = A(x) exp(\u03b8x \u2212b(\u03b8) and \u00b5(\u03b81) = \u00b51, \u00b5(\u03b82) = \u00b52. \u03b1 is such that n1 = \u03b1n\nand n2 = (1 \u2212\u03b1)n. One can write\nP\n \n1\nn1\nn1\nX\nt=1\nX1,t \u22121\nn2\nn2\nX\nt=1\nX2,t < 0\n!\n= P\n \n\u03b1\nn2\nX\nt=1\nX2,t \u2212(1 \u2212\u03b1)\nn1\nX\nt=1\nX1,t \u22650\n!\n.\nFor every \u03bb > 0, multiplying by \u03bb, taking the exponential of the two sides and using Markov\u2019s\ninequality (this technique is often referred to as Chernoff\u2019s method), one gets\nP\n \n1\nn1\nn1\nX\nt=1\nX1,t \u22121\nn2\nn2\nX\nt=1\nX2,t < 0\n!\n\u2264\n\u0010\nE\u03bd[e\u03bb\u03b1X2,1]\n\u0011(1\u2212\u03b1)n \u0010\nE\u03bd[e\u03bb(1\u2212\u03b1)X1,1]\n\u0011\u03b1n\n= exp\n\u0012\u0002\n(1 \u2212\u03b1)\u03c6X2,1(\u03bb\u03b1) + \u03b1\u03c6X1,1(\u2212(1 \u2212\u03b1)\u03bb)\n\u0003\n|\n{z\n}\nG\u03b1(\u03bb)\nn\n\u0013\nwith \u03c6X(\u03bb) = log E\u03bd[e\u03bbX] for any random variable X. If X \u223cf\u03b8 a direct computation gives\n\u03c6X(\u03bb) = b(\u03bb + \u03b8) \u2212b(\u03b8). Therefore the function G\u03b1(\u03bb) introduced above rewrites\nG\u03b1(\u03bb) = (1 \u2212\u03b1)(b(\u03bb\u03b1 + \u03b82) \u2212b(\u03b82)) + \u03b1(b(\u03b81 \u2212(1 \u2212\u03b1)\u03bb) \u2212b(\u03b81)).\nUsing that b\u2032(x) = \u00b5(x), we can compute the derivative of G and see that this function as a unique\nminimum in \u03bb\u2217given by\n\u00b5(\u03b81 \u2212(1 \u2212\u03b1)\u03bb\u2217)\n=\n\u00b5(\u03b82 + \u03b1\u03bb\u2217)\n\u03b81 \u2212(1 \u2212\u03b1)\u03bb\u2217\n=\n\u03b82 + \u03b1\u03bb\u2217\n\u03bb\u2217\n=\n\u03b81 \u2212\u03b82,\nusing that \u03b8 7\u2192\u00b5(\u03b8) is one-to-one. One can also show that\nG(\u03bb\u2217)\n=\n(1 \u2212\u03b1)[b(\u03b1\u03b81 + (1 \u2212\u03b1)\u03b82) \u2212b(\u03b82)] + \u03b1[b(\u03b1\u03b81 + (1 \u2212\u03b1)\u03b82) \u2212b(\u03b81)].\nUsing the expression of the KL-divergence between \u03bd\u03b81 and \u03bd\u03b82 as a function of the natural param-\neters: K(\u03b81, \u03b82) = \u00b5(\u03b81)(\u03b81 \u2212\u03b82) \u2212b(\u03b81) + b(\u03b82), one can also show that\n\u03b1K(\u03b1\u03b81 + (1 \u2212\u03b1)\u03b82, \u03b81)\n= \u2212\u03b1(1 \u2212\u03b1)\u00b5(\u03b1\u03b81 + (1 \u2212\u03b1)\u03b82)(\u03b81 \u2212\u03b82) + \u03b1[\u2212b(\u03b1\u03b81 + (1 \u2212\u03b1)\u03b82) + b(\u03b81)]\n(1 \u2212\u03b1)K(\u03b1\u03b81 + (1 \u2212\u03b1)\u03b82, \u03b82)\n= \u03b1(1 \u2212\u03b1)\u00b5(\u03b1\u03b81 + (1 \u2212\u03b1)\u03b82)(\u03b81 \u2212\u03b82) + (1 \u2212\u03b1)[\u2212b(\u03b1\u03b81 + (1 \u2212\u03b1)\u03b82) + b(\u03b82)]\n22\nON THE COMPLEXITY OF A/B TESTING\nSumming these two equalities leads to\nG(\u03bb\u2217) = \u2212[\u03b1K(\u03b1\u03b81 + (1 \u2212\u03b1)\u03b82, \u03b81) + (1 \u2212\u03b1)K(\u03b1\u03b81 + (1 \u2212\u03b1)\u03b82, \u03b82)] = \u2212g\u03b1(\u03b81, \u03b82).\nHence the inequality P\n\u0010\n1\nn1\nPn1\nt=1 X1,t <\n1\nn2\nPn2\nt=1 X2,t\n\u0011\n\u2264exp(G(\u03bb\u2217)n) is exactly (12).\n23\n",
        "sentence": " [23] Emilie Kaufmann, Olivier Capp\u00e9, and Aur\u00e9lien Garivier. [23].",
        "context": "13\nKAUFMANN CAPP\u00b4E GARIVIER\nAcknowledgments\nWe thank S\u00b4ebastien Bubeck for fruitful discussions during the visit of the \ufb01rst author at Princeton\nUniversity. This work was supported by the ANR-2010-COSI-002 grant of the French National\nResearch Agency.\nJMLR: Workshop and Conference Proceedings vol 35 (2014) 1\u201323\nOn the Complexity of A/B Testing\nEmilie Kaufmann\nKAUFMANN@TELECOM-PARISTECH.FR\nLTCI, T\u00b4el\u00b4ecom ParisTech & CNRS\nOlivier Capp\u00b4e\nCAPPE@TELECOM-PARISTECH.FR\nLTCI, T\u00b4el\u00b4ecom ParisTech & CNRS\n11\nKAUFMANN CAPP\u00b4E GARIVIER\napproach is adaptive with respect to the dif\ufb01culty of the problem whereas it is impossible to predict\nthe ef\ufb01ciency of a batch (or \ufb01xed-budget) experiment without some prior knowledge regarding the\nproblem under consideration."
    },
    {
        "title": "Bandits Games and Clustering Foundations",
        "author": [
            "S\u00e9bastien Bubeck"
        ],
        "venue": "Theses, Universite\u0301 des Sciences et Technologie de Lille - Lille I,",
        "citeRegEx": "24",
        "shortCiteRegEx": "24",
        "year": 2010,
        "abstract": "",
        "full_text": "",
        "sentence": " [24] S\u00e9bastien Bubeck. 5 in [24]).",
        "context": null
    },
    {
        "title": "Point-to-set maps in mathematical programming",
        "author": [
            "William W. Hogan"
        ],
        "venue": "SIAM Review,",
        "citeRegEx": "25",
        "shortCiteRegEx": "25",
        "year": 1973,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " [25] William W. See Hogan [25] for definitions of terms such as continuity of point-to-set maps.",
        "context": null
    }
]