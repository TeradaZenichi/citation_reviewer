[
    {
        "title": "Deep generative stochastic networks trainable by backprop",
        "author": [
            "Y. Bengio",
            "E. Laufer",
            "G. Alain",
            "J. Yosinski"
        ],
        "venue": "In ICML,",
        "citeRegEx": "Bengio et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Bengio et al\\.",
        "year": 2014,
        "abstract": "We introduce a novel training principle for probabilistic models that is an\nalternative to maximum likelihood. The proposed Generative Stochastic Networks\n(GSN) framework is based on learning the transition operator of a Markov chain\nwhose stationary distribution estimates the data distribution. The transition\ndistribution of the Markov chain is conditional on the previous state,\ngenerally involving a small move, so this conditional distribution has fewer\ndominant modes, being unimodal in the limit of small moves. Thus, it is easier\nto learn because it is easier to approximate its partition function, more like\nlearning to perform supervised function approximation, with gradients that can\nbe obtained by backprop. We provide theorems that generalize recent work on the\nprobabilistic interpretation of denoising autoencoders and obtain along the way\nan interesting justification for dependency networks and generalized\npseudolikelihood, along with a definition of an appropriate joint distribution\nand sampling mechanism even when the conditionals are not consistent. GSNs can\nbe used with missing inputs and can be used to sample subsets of variables\ngiven the rest. We validate these theoretical results with experiments on two\nimage datasets using an architecture that mimics the Deep Boltzmann Machine\nGibbs sampler but allows training to proceed with simple backprop, without the\nneed for layerwise pretraining.",
        "full_text": "Deep Generative Stochastic Networks Trainable by Backprop\nYoshua Bengio\u2217\nFIND.US@ON.THE.WEB\n\u00b4Eric Thibodeau-Laufer\nGuillaume Alain\nD\u00b4epartement d\u2019informatique et recherche op\u00b4erationnelle, Universit\u00b4e de Montr\u00b4eal,\u2217& Canadian Inst. for Advanced Research\nJason Yosinski\nDepartment of Computer Science, Cornell University\nAbstract\nWe introduce a novel training principle for prob-\nabilistic models that is an alternative to max-\nimum likelihood.\nThe proposed Generative\nStochastic Networks (GSN) framework is based\non learning the transition operator of a Markov\nchain whose stationary distribution estimates the\ndata distribution. The transition distribution of\nthe Markov chain is conditional on the previ-\nous state, generally involving a small move, so\nthis conditional distribution has fewer dominant\nmodes, being unimodal in the limit of small\nmoves.\nThus, it is easier to learn because it\nis easier to approximate its partition function,\nmore like learning to perform supervised func-\ntion approximation, with gradients that can be\nobtained by backprop. We provide theorems that\ngeneralize recent work on the probabilistic inter-\npretation of denoising autoencoders and obtain\nalong the way an interesting justi\ufb01cation for de-\npendency networks and generalized pseudolike-\nlihood, along with a de\ufb01nition of an appropri-\nate joint distribution and sampling mechanism\neven when the conditionals are not consistent.\nGSNs can be used with missing inputs and can\nbe used to sample subsets of variables given the\nrest. We validate these theoretical results with\nexperiments on two image datasets using an ar-\nchitecture that mimics the Deep Boltzmann Ma-\nchine Gibbs sampler but allows training to pro-\nceed with simple backprop, without the need for\nlayerwise pretraining.\nP(X)\nX\nC( \u02dcX|X)\n\u02dcX\nP(X| \u02dcX)\nP(X)\nX\nP(H|X)\nH\nP(X|H)\nFigure 1. Top: A denoising auto-encoder de\ufb01nes an estimated\nMarkov chain where the transition operator \ufb01rst samples a cor-\nrupted \u02dc\nX from C( \u02dc\nX|X) and then samples a reconstruction from\nP\u03b8(X| \u02dc\nX), which is trained to estimate the ground truth P(X| \u02dc\nX).\nNote how for any given \u02dc\nX, P(X| \u02dc\nX) is a much simpler (roughly\nunimodal) distribution than the ground truth P(X) and its parti-\ntion function is thus easier to approximate. Bottom: More gen-\nerally, a GSN allows the use of arbitrary latent variables H in\naddition to X, with the Markov chain state (and mixing) involv-\ning both X and H. Here H is the angle about the origin. The\nGSN inherits the bene\ufb01t of a simpler conditional and adds latent\nvariables, which allow far more powerful deep representations in\nwhich mixing is easier (Bengio et al., 2013b).\narXiv:1306.1091v5  [cs.LG]  24 May 2014\nDeep Generative Stochastic Networks Trainable by Backprop\n1\nIntroduction\nResearch in deep learning (see Bengio (2009) and Ben-\ngio et al. (2013a) for reviews) grew from breakthroughs in\nunsupervised learning of representations, based mostly on\nthe Restricted Boltzmann Machine (RBM) (Hinton et al.,\n2006), auto-encoder variants (Bengio et al., 2007; Vin-\ncent et al., 2008), and sparse coding variants (Lee et al.,\n2007; Ranzato et al., 2007). However, the most impres-\nsive recent results have been obtained with purely super-\nvised learning techniques for deep networks, in particular\nfor speech recognition (Dahl et al., 2010; Deng et al., 2010;\nSeide et al., 2011) and object recognition (Krizhevsky\net al., 2012).\nThe latest breakthrough in object recog-\nnition (Krizhevsky et al., 2012) was achieved with fairly\ndeep convolutional networks with a form of noise injec-\ntion in the input and hidden layers during training, called\ndropout (Hinton et al., 2012). In all of these cases, the\navailability of large quantities of labeled data was critical.\nOn the other hand, progress with deep unsupervised ar-\nchitectures has been slower, with the established options\nwith a probabilistic footing being the Deep Belief Network\n(DBN) (Hinton et al., 2006) and the Deep Boltzmann Ma-\nchine (DBM) (Salakhutdinov & Hinton, 2009). Although\nsingle-layer unsupervised learners are fairly well developed\nand used to pre-train these deep models, jointly training all\nthe layers with respect to a single unsupervised criterion\nremains a challenge, with a few techniques arising to re-\nduce that dif\ufb01culty (Montavon & Muller, 2012; Goodfel-\nlow et al., 2013). In contrast to recent progress toward joint\nsupervised training of models with many layers, joint un-\nsupervised training of deep models remains a dif\ufb01cult task.\nThough the goal of training large unsupervised networks\nhas turned out to be more elusive than its supervised coun-\nterpart, the vastly larger available volume of unlabeled data\nstill beckons for ef\ufb01cient methods to model it.\nRecent\nprogress in training supervised models raises the question:\ncan we take advantage of this progress to improve our abil-\nity to train deep, generative, unsupervised, semi-supervised\nor structured output models?\nThis paper lays theoretical foundations for a move in this\ndirection through the following main contributions:\n1 \u2013 Intuition: In Section 2 we discuss what we view as ba-\nsic motivation for studying alternate ways of training unsu-\npervised probabilistic models, i.e., avoiding the intractable\nsums or maximization involved in many approaches.\n2 \u2013 Training Framework:\nWe generalize recent work\non the generative view of denoising autoencoders (Bengio\net al., 2013c) by introducing latent variables in the frame-\nwork to de\ufb01ne Generative Stochastic Networks (GSNs)\n(Section 3).\nGSNs aim to estimate the data generating\ndistribution indirectly, by parametrizing the transition op-\nerator of a Markov chain rather than directly parametriz-\ning P(X). Most critically, this framework transforms the\nunsupervised density estimation problem into one which is\nmore similar to supervised function approximation. This\nenables training by (possibly regularized) maximum like-\nlihood and gradient descent computed via simple back-\npropagation, avoiding the need to compute intractable par-\ntition functions.\nDepending on the model, this may al-\nlow us to draw from any number of recently demonstrated\nsupervised training tricks. For example, one could use a\nconvolutional architecture with max-pooling for parametric\nparsimony and computational ef\ufb01ciency, or dropout (Hin-\nton et al., 2012) to prevent co-adaptation of hidden repre-\nsentations.\n3 \u2013 General theory: Training the generative (decoding /\ndenoising) component of a GSN P(X|h) with noisy repre-\nsentation h is often far easier than modeling P(X) explic-\nitly (compare the blue and red distributions in Figure 1).\nWe prove that if our estimated P(X|h) is consistent (e.g.\nthrough maximum likelihood), then the stationary distri-\nbution of the resulting chain is a consistent estimator of\nthe data generating density, P(X) (Section 3.2).\nWe\nstrengthen the consistency theorems introduced in Bengio\net al. (2013c) by showing that the corruption distribution\nmay be purely local, not requiring support over the whole\ndomain of the visible variables (Section 3.1).\n4 \u2013 Consequences of theory: We show that the model is\ngeneral and extends to a wide range of architectures, in-\ncluding sampling procedures whose computation can be\nunrolled as a Markov Chain, i.e., architectures that add\nnoise during intermediate computation in order to produce\nrandom samples of a desired distribution (Theorem 2). An\nexciting frontier in machine learning is the problem of\nmodeling so-called structured outputs, i.e., modeling a con-\nditional distribution where the output is high-dimensional\nand has a complex multimodal joint distribution (given the\ninput variable). We show how GSNs can be used to support\nsuch structured output and missing values (Section 3.4).\n5 \u2013 Example application:\nIn Section 4 we show an ex-\nample application of the GSN theory to create a deep GSN\nwhose computational graph resembles the one followed by\nGibbs sampling in deep Boltzmann machines (with con-\ntinuous latent variables), but that can be trained ef\ufb01ciently\nwith back-propagated gradients and without layerwise pre-\ntraining. Because the Markov Chain is de\ufb01ned over a state\n(X, h) that includes latent variables, we reap the dual ad-\nvantage of more powerful models for a given number of pa-\nrameters and better mixing in the chain as we add noise to\nvariables representing higher-level information, \ufb01rst sug-\ngested by the results obtained by Bengio et al. (2013b)\nand Luo et al. (2013). The experimental results show that\nsuch a model with latent states indeed mixes better than\nDeep Generative Stochastic Networks Trainable by Backprop\nshallower models without them (Table 1).\n6 \u2013 Dependency networks:\nFinally, an unexpected re-\nsult falls out of the GSN theory: it allows us to provide\na novel justi\ufb01cation for dependency networks (Heckerman\net al., 2000) and for the \ufb01rst time de\ufb01ne a proper joint dis-\ntribution between all the visible variables that is learned by\nsuch models (Section 3.5).\n2\nSumming over too many major modes\nMany of the computations involved in graphical models\n(inference, sampling, and learning) are made intractable\nand dif\ufb01cult to approximate because of the large number\nof non-negligible modes in the modeled distribution (either\ndirectly P(x) or a joint distribution P(x, h) involving la-\ntent variables h). In all of these cases, what is intractable is\nthe computation or approximation of a sum (often weighted\nby probabilities), such as a marginalization or the estima-\ntion of the gradient of the normalization constant. If only a\nfew terms in this sum dominate (corresponding to the dom-\ninant modes of the distribution), then many good approxi-\nmate methods can be found, such as Monte-Carlo Markov\nchains (MCMC) methods.\nSimilarly dif\ufb01cult tasks arise with structured output prob-\nlems where one wants to sample from P(y, h|x) and both\ny and h are high-dimensional and have a complex highly\nmultimodal joint distribution (given x).\nDeep Boltzmann machines (Salakhutdinov & Hinton,\n2009) combine the dif\ufb01culty of inference (for the positive\nphase where one tries to push the energies associated with\nthe observed x down) and also that of sampling (for the\nnegative phase where one tries to push up the energies as-\nsociated with x\u2019s sampled from P(x)). Unfortunately, us-\ning an MCMC method to sample from P(x, h) in order to\nestimate the gradient of the partition function may be seri-\nously hurt by the presence of a large number of important\nmodes, as argued below.\nTo evade the problem of highly multimodal joint or poste-\nrior distributions, the currently known approaches to deal-\ning with the above intractable sums make very strong ex-\nplicit assumptions (in the parametrization) or implicit as-\nsumptions (by the choice of approximation methods) on the\nform of the distribution of interest. In particular, MCMC\nmethods are more likely to produce a good estimator if the\nnumber of non-negligible modes is small: otherwise the\nchains would require at least as many MCMC steps as the\nnumber of such important modes, times a factor that ac-\ncounts for the mixing time between modes. Mixing time\nitself can be very problematic as a trained model becomes\nsharper, as it approaches a data generating distribution that\nmay have well-separated and sharp modes (i.e., manifolds).\nWe propose to make another assumption that might suf\ufb01ce\nto bypass this multimodality problem: the effectiveness of\nfunction approximation.\nIn particular, the GSN approach presented in the next sec-\ntion relies on estimating the transition operator of a Markov\nchain, e.g. P(xt|xt\u22121) or P(xt, ht|xt\u22121, ht\u22121). Because\neach step of the Markov chain is generally local, these tran-\nsition distributions will often include only a very small\nnumber of important modes (those in the neighbourhood\nof the previous state). Hence the gradient of their partition\nfunction will be easy to approximate. For example consider\nthe denoising transitions studied by Bengio et al. (2013c)\nand illustrated in Figure 1, where \u02dcxt\u22121 is a stochastically\ncorrupted version of xt\u22121 and we learn the denoising dis-\ntribution P(x|\u02dcx). In the extreme case (studied empirically\nhere) where P(x|\u02dcx) is approximated by a unimodal distri-\nbution, the only form of training that is required involves\nfunction approximation (predicting the clean x from the\ncorrupted \u02dcx).\nAlthough having the true P(x|\u02dcx) turn out to be unimodal\nmakes it easier to \ufb01nd an appropriate family of models for\nit, unimodality is by no means required by the GSN frame-\nwork itself. One may construct a GSN using any multi-\nmodal model for output (e.g. mixture of Gaussians, RBMs,\nNADE, etc.), provided that gradients for the parameters of\nthe model in question can be estimated (e.g. log-likelihood\ngradients).\nThe approach proposed here thus avoids the need for a poor\napproximation of the gradient of the partition function in\nthe inner loop of training, but still has the potential of cap-\nturing very rich distributions by relying mostly on \u201cfunc-\ntion approximation\u201d.\nBesides the approach discussed here, there may well be\nother very different ways of evading this problem of in-\ntractable marginalization, including approaches such as\nsum-product networks (Poon & Domingos, 2011), which\nare based on learning a probability function that has a\ntractable form by construction and yet is from a \ufb02exible\nenough family of distributions.\n3\nGenerative Stochastic Networks\nAssume the problem we face is to construct a model for\nsome unknown data-generating distribution P(X) given\nonly examples of X drawn from that distribution. In many\ncases, the unknown distribution P(X) is complicated, and\nmodeling it directly can be dif\ufb01cult.\nA recently proposed approach using denoising autoen-\ncoders transforms the dif\ufb01cult task of modeling P(X) into\na supervised learning problem that may be much easier to\nsolve.\nThe basic approach is as follows: given a clean\nexample data point X from P(X), we obtain a corrupted\nversion \u02dcX by sampling from some corruption distribution\nDeep Generative Stochastic Networks Trainable by Backprop\nC( \u02dcX|X). For example, we might take a clean image, X,\nand add random white noise to produce \u02dcX. We then use su-\npervised learning methods to train a function to reconstruct,\nas accurately as possible, any X from the data set given\nonly a noisy version \u02dcX. As shown in Figure 1, the recon-\nstruction distribution P(X| \u02dcX) may often be much easier\nto learn than the data distribution P(X), because P(X| \u02dcX)\ntends to be dominated by a single or few major modes (such\nas the roughly Gaussian shaped density in the \ufb01gure).\nBut how does learning the reconstruction distribution help\nus solve our original problem of modeling P(X)? The\ntwo problems are clearly related, because if we knew ev-\nerything about P(X), then our knowledge of the C( \u02dcX|X)\nthat we chose would allow us to precisely specify the opti-\nmal reconstruction function via Bayes rule: P(X| \u02dcX) =\n1\nzC( \u02dcX|X)P(X), where z is a normalizing constant that\ndoes not depend on X. As one might hope, the relation is\nalso true in the opposite direction: once we pick a method\nof adding noise, C( \u02dcX|X), knowledge of the corresponding\nreconstruction distribution P(X| \u02dcX) is suf\ufb01cient to recover\nthe density of the data P(X).\nThis intuition was borne out by proofs in two recent pa-\npers. Alain & Bengio (2013) showed that denoising auto-\nencoders with small Gaussian corruption and squared error\nloss estimated the score (derivative of the log-density with\nrespect to the input) of continuous observed random vari-\nables. More recently, Bengio et al. (2013c) generalized this\nto arbitrary variables (discrete, continuous or both), arbi-\ntrary corruption (not necessarily asymptotically small), and\narbitrary loss function (so long as they can be seen as a log-\nlikelihood).\nBeyond proving that P(X| \u02dcX) is suf\ufb01cient to reconstruct\nthe data density, Bengio et al. (2013c) also demonstrated\na method of sampling from a learned, parametrized model\nof the density, P\u03b8(X), by running a Markov chain that al-\nternately adds noise using C( \u02dcX|X) and denoises by sam-\npling from the learned P\u03b8(X| \u02dcX), which is trained to ap-\nproximate the true P(X| \u02dcX). The most important contri-\nbution of that paper was demonstrating that if a learned,\nparametrized reconstruction function P\u03b8(X| \u02dcX) converges\nto the true P(X| \u02dcX), then under some relatively benign\nconditions the stationary distribution \u03c0(X) of the result-\ning Markov chain will exist and will indeed converge to the\ndata distribution P(X).\nBefore moving on, we should pause to make an important\npoint clear. Alert readers may have noticed that P(X| \u02dcX)\nand P(X) can each be used to reconstruct the other given\nknowledge of C( \u02dcX|X). Further, if we assume that we have\nchosen a simple C( \u02dcX|X) (say, a uniform Gaussian with\na single width parameter), then P(X| \u02dcX) and P(X) must\nboth be of approximately the same complexity. Put another\nway, we can never hope to combine a simple C( \u02dcX|X) and a\nsimple P(X| \u02dcX) to model a complex P(X). Nonetheless,\nit may still be the case that P(X| \u02dcX) is easier to model than\nP(X) due to reduced computational complexity in com-\nputing or approximating the partition functions of the con-\nditional distribution mapping corrupted input \u02dcX to the dis-\ntribution of corresponding clean input X. Indeed, because\nthat conditional is going to be mostly assigning probabil-\nity to X locally around \u02dcX, P(X| \u02dcX) has only one or a few\nmodes, while P(X) can have a very large number.\nSo where did the complexity go?\nP(X| \u02dcX) has fewer\nmodes than P(X), but the location of these modes depends\non the value of \u02dcX. It is precisely this mapping from \u02dcX \u2192\nmode location that allows us to trade a dif\ufb01cult density\nmodeling problem for a supervised function approximation\nproblem that admits application of many of the usual su-\npervised learning tricks.\nIn the next four sections, we extend previous results in sev-\neral directions.\n3.1\nGenerative denoising autoencoders with local\nnoise\nThe main theorem in Bengio et al. (2013c), reproduced be-\nlow, requires that the Markov chain be ergodic. A set of\nconditions guaranteeing ergodicity is given in the afore-\nmentioned paper, but these conditions are restrictive in re-\nquiring that C( \u02dcX|X) > 0 everywhere that P(X) > 0.\nHere we show how to relax these conditions and still guar-\nantee ergodicity through other means.\nLet P\u03b8n(X| \u02dcX) be a denoising auto-encoder that has been\ntrained on n training examples. P\u03b8n(X| \u02dcX) assigns a prob-\nability to X, given \u02dcX, when \u02dcX \u223cC( \u02dcX|X). This estimator\nde\ufb01nes a Markov chain Tn obtained by sampling alterna-\ntively an \u02dcX from C( \u02dcX|X) and an X from P\u03b8(X| \u02dcX). Let\n\u03c0n be the asymptotic distribution of the chain de\ufb01ned by\nTn, if it exists. The following theorem is proven by Bengio\net al. (2013c).\nTheorem 1. If P\u03b8n(X| \u02dcX) is a consistent estimator of the\ntrue conditional distribution P(X| \u02dcX) and Tn de\ufb01nes an\nergodic Markov chain, then as n \u2192\u221e, the asymptotic\ndistribution \u03c0n(X) of the generated samples converges to\nthe data-generating distribution P(X).\nIn order for Theorem 1 to apply, the chain must be ergodic.\nOne set of conditions under which this occurs is given in\nthe aforementioned paper. We slightly restate them here:\nCorollary 1. If the support for both the data-generating\ndistribution and denoising model are contained in and\nnon-zero in a \ufb01nite-volume region V (i.e., \u2200\u02dcX, \u2200X\n/\u2208\nV, P(X) = 0, P\u03b8(X| \u02dcX) = 0 and \u2200\u02dcX, \u2200X \u2208V, P(X) >\n0, P\u03b8(X| \u02dcX) > 0, C( \u02dcX|X) > 0) and these statements re-\nmain true in the limit of n \u2192\u221e, then the chain de\ufb01ned by\nTn will be ergodic.\nDeep Generative Stochastic Networks Trainable by Backprop\n0.0\n0.1\n0.2\n0.3\n0.4\nprobability (linear)\nsampled X\nsampled \u02dcX\nleaky\nmodes\nP(X)\nC( \u02dcX|X)\nP(X| \u02dcX)\n10-4\n10-3\n10-2\n10-1\nprobability (log)\n4\n2\n0\n2\n4\nx (arbitrary units)\n4\n2\n0\n2\n4\nx (arbitrary units)\nFigure 2. If C( \u02dc\nX|X) is globally supported as required by Corol-\nlary 1 (Bengio et al., 2013c), then for P\u03b8n(X| \u02dc\nX) to converge to\nP(X| \u02dc\nX), it will eventually have to model all of the modes in\nP(X), even though the modes are damped (see \u201cleaky modes\u201d\non the left). However, if we guarantee ergodicity through other\nmeans, as in Corollary 2, we can choose a local C( \u02dc\nX|X) and al-\nlow P\u03b8n(X| \u02dc\nX) to model only the local structure of P(X) (see\nright).\nIf conditions in Corollary 1 apply, then the chain will be\nergodic and Theorem 1 will apply. However, these con-\nditions are suf\ufb01cient, not necessary, and in many cases\nthey may be arti\ufb01cially restrictive. In particular, Corol-\nlary 1 de\ufb01nes a large region V containing any possible X\nallowed by the model and requires that we maintain the\nprobability of jumping between any two points in a single\nmove to be greater than 0. While this generous condition\nhelps us easily guarantee the ergodicity of the chain, it also\nhas the unfortunate side effect of requiring that, in order\nfor P\u03b8n(X| \u02dcX) to converge to the conditional distribution\nP(X| \u02dcX), it must have the capacity to model every mode\nof P(X), exactly the dif\ufb01culty we were trying to avoid.\nThe left two plots in Figure 2 show this dif\ufb01culty: because\nC( \u02dcX|X) > 0 everywhere in V , every mode of P(X) will\nleak, perhaps attenuated, into P(X| \u02dcX).\nFortunately, we may seek ergodicity through other means.\nThe following corollary allows us to choose a C( \u02dcX|X)\nthat only makes small jumps, which in turn only requires\nP\u03b8(X| \u02dcX) to model a small part of the space V around each\n\u02dcX.\nLet P\u03b8n(X| \u02dcX) be a denoising auto-encoder that has been\ntrained on n training examples and C( \u02dcX|X) be some cor-\nruption distribution. P\u03b8n(X| \u02dcX) assigns a probability to\nX, given \u02dcX, when \u02dcX \u223cC( \u02dcX|X) and X \u223cP(X). De-\n\ufb01ne a Markov chain Tn by alternately sampling an \u02dcX from\nC( \u02dcX|X) and an X from P\u03b8(X| \u02dcX).\nCorollary 2. If the data-generating distribution is con-\ntained in and non-zero in a \ufb01nite-volume region V (i.e.,\n\u2200X /\u2208V, P(X) = 0, and \u2200X \u2208V, P(X) > 0) and\nall pairs of points in V can be connected by a \ufb01nite-length\npath through V and for some \u03f5 > 0, \u2200\u02dcX \u2208V, \u2200X \u2208V\nwithin \u03f5 of each other, C( \u02dcX|X) > 0 and P\u03b8(X| \u02dcX) > 0\nand these statements remain true in the limit of n \u2192\u221e,\nthen the chain de\ufb01ned by Tn will be ergodic.\nProof. Consider any two points Xa and Xb in V .\nBy\nthe assumptions of Corollary 2, there exists a \ufb01nite length\npath between Xa and Xb through V . Pick one such \ufb01-\nnite length path P. Chose a \ufb01nite series of points x =\n{x1, x2, . . . , xk} along P, with x1 = Xa and xk = Xb\nsuch that the distance between every pair of consecutive\npoints (xi, xi+1) is less than \u03f5 as de\ufb01ned in Corollary 2.\nThen the probability of sampling \u02dcX = xi+1 from C( \u02dcX|xi))\nwill be positive, because C( \u02dcX|X)) > 0 for all \u02dcX within\n\u03f5 of X by the assumptions of Corollary 2. Further, the\nprobability of sampling X = \u02dcX = xi+1 from P\u03b8(X| \u02dcX)\nwill be positive from the same assumption on P. Thus\nthe probability of jumping along the path from xi to xi+1,\nTn(Xt+1 = xi+1|Xt = xi), will be greater than zero\nfor all jumps on the path.\nBecause there is a positive\nprobability \ufb01nite length path between all pairs of points\nin V , all states commute, and the chain is irreducible. If\nwe consider Xa = Xb \u2208V , by the same arguments\nTn(Xt = Xa|Xt\u22121 = Xa) > 0. Because there is a pos-\nitive probability of remaining in the same state, the chain\nwill be aperiodic. Because the chain is irreducible and over\na \ufb01nite state space, it will be positive recurrent as well.\nThus, the chain de\ufb01ned by Tn is ergodic.\nAlthough this is a weaker condition that has the advantage\nof making the denoising distribution even easier to model\n(probably having less modes), we must be careful to choose\nthe ball size \u03f5 large enough to guarantee that one can jump\noften enough between the major modes of P(X) when\nthese are separated by zones of tiny probability. \u03f5 must be\nlarger than half the largest distance one would have to travel\nacross a desert of low probability separating two nearby\nmodes (which if not connected in this way would make V\nnot anymore have a single connected component). Practi-\ncally, there would be a trade-off between the dif\ufb01culty of\nestimating P(X| \u02dcX) and the ease of mixing between major\nmodes separated by a very low density zone.\nThe generalization of the above results presented in the\nnext section is meant to help deal with this mixing prob-\nlem.\nIt is inspired by the recent work (Bengio et al.,\n2013b) showing that mixing between modes can be a se-\nrious problem for RBMs and DBNs, and that well-trained\ndeeper models can greatly alleviate it by allowing the mix-\ning to happen at a more abstract level of representation\n(e.g., where some bits can actually represent which mode /\nDeep Generative Stochastic Networks Trainable by Backprop\nclass / manifold is considered).\n3.2\nGeneralizing the denoising autoencoder to GSNs\nThe denoising auto-encoder Markov chain is de\ufb01ned by\n\u02dcXt \u223cC( \u02dcX|Xt) and Xt+1 \u223cP\u03b8(X| \u02dcXt), where Xt alone\ncan serve as the state of the chain. The GSN framework\ngeneralizes this by de\ufb01ning a Markov chain with both a\nvisible Xt and a latent variable Ht as state variables, of the\nform\nHt+1\n\u223c\nP\u03b81(H|Ht, Xt)\nXt+1\n\u223c\nP\u03b82(X|Ht+1).\nX2\nX0\nX1\nH0\nH1\nH2\nDenoising auto-encoders are thus a special case of GSNs.\nNote that, given that the distribution of Ht+1 depends on\na previous value of Ht, we \ufb01nd ourselves with an extra\nH0 variable added at the beginning of the chain. This H0\ncomplicates things when it comes to training, but when we\nare in a sampling regime we can simply wait a suf\ufb01cient\nnumber of steps to burn in.\nThe next theoretical results give conditions for making the\nstationary distributions of the above Markov chain match a\ntarget data generating distribution.\nTheorem 2. Let (Ht, Xt)\u221e\nt=0 be the Markov chain de\ufb01ned\nby the following graphical model.\nX2\nX0\nX1\nH0\nH1\nH2\nIf we assume that the chain has a stationary distribution\n\u03c0X,H, and that for every value of (x, h) we have that\n\u2022 all the P(Xt = x|Ht = h) = g(x, h) share the same\ndensity for t \u22651\n\u2022 all the P(Ht+1 = h|Ht = h\u2032, Xt = x) = f(h, h\u2032, x)\nshare the same density for t \u22650\n\u2022 P(H0 = h|X0 = x) = P(H1 = h|X0 = x)\n\u2022 P(X1 = x|H1 = h) = P(X0 = x|H1 = h)\nthen for every value of (x, h) we get that\n\u2022 P(X0 = x|H0 = h) = g(x, h) holds, which is some-\nthing that was assumed only for t \u22651\n\u2022 P(Xt = x, Ht = h) = P(X0 = x, H0 = h) for all\nt \u22650\n\u2022 the stationary distribution \u03c0H,X has a marginal dis-\ntribution \u03c0X such that \u03c0 (x) = P (X0 = x).\nThose conclusions show that our Markov chain has the\nproperty that its samples in X are drawn from the same\ndistribution as X0.\nProof. The proof hinges on a few manipulations done with\nthe \ufb01rst variables to show that P(Xt = x|Ht = h) =\ng(x, h), which is assumed for t \u22651, also holds for t = 0.\nFor all h we have that\nP(H0 = h)\n=\nZ\nP(H0 = h|X0 = x)P(X0 = x)dx\n=\nZ\nP(H1 = h|X0 = x)P(X0 = x)dx\n=\nP(H1 = h).\nThe equality in distribution between (X1, H1) and\n(X0, H0) is obtained with\nP(X1 = x, H1 = h)\n=\nP(X1 = x|H1 = h)P(H1 = h)\n=\nP(X0 = x|H1 = h)P(H1 = h)\n(by hypothesis)\n=\nP(X0 = x, H1 = h)\n=\nP(H1 = h|X0 = x)P(X0 = x)\n=\nP(H0 = h|X0 = x)P(X0 = x)\n(by hypothesis)\n=\nP(X0 = x, H0 = h).\nThen we can use this to conclude that\nP(X0 = x, H0 = h) = P(X1 = x, H1 = h)\n=\u21d2\nP(X0 = x|H0 = h) = P(X1 = x|H1 = h) = g(x, h)\nso, despite the arrow in the graphical model being turned\nthe other way, we have that the density of P(X0 = x|H0 =\nh) is the same as for all other P(Xt = x|Ht = h) with\nt \u22651.\nNow, since the distribution of H1 is the same as the dis-\ntribution of H0, and the transition probability P(H1 =\nh|H0 = h\u2032) is entirely de\ufb01ned by the (f, g) densities which\nare found at every step for all t \u22650, then we know that\n(X2, H2) will have the same distribution as (X1, H1). To\nmake this point more explicitly,\nP(H1 = h|H0 = h\u2032)\n=\nZ\nP(H1 = h|H0 = h\u2032, X0 = x)P(X0 = x|H0 = h\u2032)dx\n=\nZ\nf(h, h\u2032, x)g(x, h\u2032)dx\n=\nZ\nP(H2 = h|H1 = h\u2032, X1 = x)P(X1 = x|H1 = h\u2032)dx\n=P(H2 = h|H1 = h\u2032)\nDeep Generative Stochastic Networks Trainable by Backprop\nThis also holds for P(H3|H2) and for all subsequent\nP(Ht+1|Ht).\nThis relies on the crucial step where we\ndemonstrate that P(X0 = x|H0 = h) = g(x, h). Once\nthis was shown, then we know that we are using the same\ntransitions expressed in terms of (f, g) at every step.\nSince the distribution of H0 was shown above to be the\nsame as the distribution of H1, this forms a recursive argu-\nment that shows that all the Ht are equal in distribution to\nH0. Because g(x, h) describes every P(Xt = x|Ht = h),\nwe have that all the joints (Xt, Ht) are equal in distribution\nto (X0, H0).\nThis implies that the stationary distribution \u03c0X,H is the\nsame as that of (X0, H0). Their marginals with respect to\nX are thus the same.\nTo apply Theorem 2 in a context where we use experimen-\ntal data to learn a model, we would like to have certain\nguarantees concerning the robustness of the stationary den-\nsity \u03c0X. When a model lacks capacity, or when it has seen\nonly a \ufb01nite number of training examples, that model can\nbe viewed as a perturbed version of the exact quantities\nfound in the statement of Theorem 2.\nA good overview of results from perturbation theory\ndiscussing stationary distributions in \ufb01nite state Markov\nchains can be found in (Cho et al., 2000). We reference\nhere only one of those results.\nTheorem 3. Adapted from (Schweitzer, 1968)\nLet K be the transition matrix of a \ufb01nite state, irreducible,\nhomogeneous Markov chain. Let \u03c0 be its stationary dis-\ntribution vector so that K\u03c0 = \u03c0. Let A = I \u2212K and\nZ = (A + C)\u22121 where C is the square matrix whose\ncolumns all contain \u03c0. Then, if \u02dcK is any transition ma-\ntrix (that also satis\ufb01es the irreducible and homogeneous\nconditions) with stationary distribution \u02dc\u03c0, we have that\n\u2225\u03c0 \u2212\u02dc\u03c0\u22251 \u2264\u2225Z\u2225\u221e\n\r\r\rK \u2212\u02dcK\n\r\r\r\n\u221e.\nThis theorem covers the case of discrete data by show-\ning how the stationary distribution is not disturbed by\na great amount when the transition probabilities that we\nlearn are close to their correct values.\nWe are talk-\ning here about the transition between steps of the chain\n(X0, H0), (X1, H1), . . . , (Xt, Ht), which are de\ufb01ned in\nTheorem 2 through the (f, g) densities.\nWe avoid discussing the training criterion for a GSN. Var-\nious alternatives exist, but this analysis is for future work.\nRight now Theorem 2 suggests the following rules :\n\u2022 Pick the transition distribution f(h, h\u2032, x) to be use-\nful. There is no bad f when g can be trained perfectly\nwith in\ufb01nite capacity. However, the choice of f can\nput a great burden on g, and using a simple f, such as\none that represents additive gaussian noise, will lead\nto less dif\ufb01culties in training g. In practice, we have\nalso found good results by training f at the same time\nby back-propagating the errors from g into f. In this\nway we simultaneously train g to model the distribu-\ntion implied by f and train f to make its implied dis-\ntribution easy to model by g.\n\u2022 Make sure that during training P(H0 = h|X0 =\nx) \u2192P(H1 = h|X0 = x). One interesting way\nto achieve this is, for each X0 in the training set, iter-\natively sample H1|(H0, X0) and substitute the value\nof H1 as the updated value of H0. Repeat until you\nhave achieved a kind of \u201cburn in\u201d. Note that, after\nthe training is completed, when we use the chain for\nsampling, the samples that we get from its stationary\ndistribution do not depend on H0. This technique of\nsubstituting the H1 into H0 does not apply beyond the\ntraining step.\n\u2022 De\ufb01ne g(x, h) to be your estimator for P(X0 =\nx|H1 = h), e.g. by training an estimator of this con-\nditional distribution from the samples (X0, H1).\n\u2022 The rest of the chain for t \u22651 is de\ufb01ned in terms of\n(f, g).\nAs much as we would like to simply learn g from pairs\n(H0, X0), the problem is that the training samples X(i)\n0\nare descendants of the corresponding values of H(i)\n0\nin the\noriginal graphical model that describes the GSN. Those\nH(i)\n0\nare hidden quantities in GSN and we have to \ufb01nd a\nway to deal with them. Setting them all to be some default\nvalue would not work because the relationship between H0\nand X0 would not be the same as the relationship later be-\ntween Ht and Xt in the chain.\n3.3\nAlternate\nparametrization\nwith\ndeterministic\nfunctions of random quantities\nThere are several equivalent ways of expressing a GSN.\nOne of the interesting formulations is to use determinis-\ntic functions of random variables to express the densities\n(f, g) used in Theorem 2.\nWith that approach, we de-\n\ufb01ne Ht+1 = f\u03b81(Xt, Zt, Ht) for some independent noise\nsource Zt, and we insist that Xt cannot be recovered ex-\nactly from Ht+1. The advantage of that formulation is that\none can directly back-propagated the reconstruction log-\nlikelihood log P(X1 = x0|H1 = f(X0, Z0, H0)) into all\nthe parameters of f and g (a similar idea was independently\nproposed in (Kingma, 2013) and also exploited in (Rezende\net al., 2014)).\nDeep Generative Stochastic Networks Trainable by Backprop\nFor the rest of this paper, we will use such a deterministic\nfunction f instead of having f refer to a probability density\nfunction. We apologize if it causes any confusion.\nIn the setting described at the beginning of section 3, the\nfunction playing the role of the \u201cencoder\u201d was \ufb01xed for\nthe purpose of the theorem, and we showed that learning\nonly the \u201cdecoder\u201d part (but a suf\ufb01ciently expressive one)\nsuf\ufb01ced. In this setting we are learning both, for which\nsome care is needed.\nOne problem would be if the created Markov chain failed\nto converge to a stationary distribution. Another such prob-\nlem could be that the function f(Xt, Zt, Ht) learned would\ntry to ignore the noise Zt, or not make the best use out of\nit. In that case, the reconstruction distribution would sim-\nply converge to a Dirac at the input X. This is the analogue\nof the constraint on auto-encoders that is needed to prevent\nthem from learning the identity function. Here, we must\ndesign the family from which f and g are learned such that\nwhen the noise Z is injected, there are always several pos-\nsible values of X that could have been the correct original\ninput.\nAnother extreme case to think about is when f(X, Z, H)\nis overwhelmed by the noise and has lost all informa-\ntion about X.\nIn that case the theorems are still appli-\ncable while giving uninteresting results: the learner must\ncapture the full distribution of X in P\u03b82(X|H) because\nthe latter is now equivalent to P\u03b82(X), since f(X, Z, H)\nno longer contains information about X. This illustrates\nthat when the noise is large, the reconstruction distribution\n(parametrized by \u03b82) will need to have the expressive power\nto represent multiple modes. Otherwise, the reconstruction\nwill tend to capture an average output, which would vi-\nsually look like a fuzzy combination of actual modes. In\nthe experiments performed here, we have only considered\nunimodal reconstruction distributions (with factorized out-\nputs), because we expect that even if P(X|H) is not uni-\nmodal, it would be dominated by a single mode when the\nnoise level is small. However, future work should investi-\ngate multimodal alternatives.\nA related element to keep in mind is that one should pick\nthe family of conditional distributions P\u03b82(X|H) so that\none can sample from them and one can easily train them\nwhen given (X, H) pairs, e.g., by maximum likelihood.\n3.4\nHandling missing inputs or structured output\nIn general, a simple way to deal with missing inputs is\nto clamp the observed inputs and then apply the Markov\nchain with the constraint that the observed inputs are \ufb01xed\nand not resampled at each time step, whereas the unob-\nserved inputs are resampled each time, conditioned on the\nclamped inputs.\nOne readily proves that this procedure gives rise to sam-\npling from the appropriate conditional distribution:\nProposition 1. If a subset x(s) of the elements of X is kept\n\ufb01xed (not resampled) while the remainder X(\u2212s) is updated\nstochastically during the Markov chain of Theorem 2, but\nusing P(Xt|Ht, X(s)\nt\n= x(s)), then the asymptotic distri-\nbution \u03c0n of the Markov chain produces samples of X(\u2212s)\nfrom the conditional distribution \u03c0n(X(\u2212s)|X(s) = x(s)).\nProof. Without constraint, we know that at convergence,\nthe chain produces samples of \u03c0n. A subset of these sam-\nples satis\ufb01es the condition X = x(s), and these constrained\nsamples could equally have been produced by sampling Xt\nfrom\nP\u03b82(Xt|f\u03b81(Xt\u22121, Zt\u22121, Ht\u22121), X(s)\nt\n= X(s)),\nby de\ufb01nition of conditional distribution.\nTherefore, at\nconvergence of the chain, we have that using the con-\nstrained distribution P(Xt|f(Xt\u22121, Zt\u22121, Ht\u22121), X(s)\nt\n=\nx(s)) produces a sample from \u03c0n under the condition\nX(s) = x(s).\nPractically, it means that we must choose an output (re-\nconstruction) distribution from which it is not only easy to\nsample from, but also from which it is easy to sample a sub-\nset of the variables in the vector X conditioned on the rest\nbeing known. In the experiments below, we used a factorial\ndistribution for the reconstruction, from which it is trivial\nto sample conditionally a subset of the input variables. In\ngeneral (with non-factorial output distributions) one must\nuse the proper conditional for the theorem to apply, i.e., it\nis not suf\ufb01cient to clamp the inputs, one must also sample\nthe reconstructions from the appropriate conditional distri-\nbution (conditioning on the clamped values).\nThis method of dealing with missing inputs can be imme-\ndiately applied to structured outputs. If X(s) is viewed as\nan \u201cinput\u201d and X(\u2212s) as an \u201coutput\u201d, then sampling from\nX(\u2212s)\nt+1\n\u223cP(X(\u2212s)|f((X(s), X(\u2212s)\nt\n), Zt, Ht), X(s)) will\nconverge to estimators of P(X(\u2212s)|X(s)). This still re-\nquires good choices of the parametrization (for f as well\nas for the conditional probability P), but the advantages\nof this approach are that there is no approximate infer-\nence of latent variables and the learner is trained with re-\nspect to simpler conditional probabilities: in the limit of\nsmall noise, we conjecture that these conditional probabil-\nities can be well approximated by unimodal distributions.\nTheoretical evidence comes from Alain & Bengio (2013):\nwhen the amount of corruption noise converges to 0 and the\ninput variables have a smooth continuous density, then a\nunimodal Gaussian reconstruction density suf\ufb01ces to fully\ncapture the joint distribution.\nDeep Generative Stochastic Networks Trainable by Backprop\nX0\nH1\nX1\nH2\nH3\nX2\nH0\nW1\nW1\nW1\nW2\nW2\nW2\nW3\nW3\nX1\nH1\nX2\nX3\nH2\nH3\nW3\nX0\ndata\ntarget\ntarget\ntarget\nFigure 3. Left: Generic GSN Markov chain with state variables Xt and Ht. Right: GSN Markov chain inspired by the unfolded\ncomputational graph of the Deep Boltzmann Machine Gibbs sampling process, but with backprop-able stochastic units at each layer.\nThe training example X = x0 starts the chain. Either odd or even layers are stochastically updated at each step, and all downward\nweight matrices are \ufb01xed to the transpose of the corresponding upward weight matrix. All xt\u2019s are corrupted by salt-and-pepper noise\nbefore entering the graph. Each xt for t > 0 is obtained by sampling from the reconstruction distribution for that step, P\u03b82(Xt|Ht). The\nwalkback training objective is the sum over all steps of log-likelihoods of target X = x0 under the reconstruction distribution. In the\nspecial case of a unimodal Gaussian reconstruction distribution, maximizing the likelihood is equivalent to minimizing reconstruction\nerror; in general one trains to maximum likelihood, not simply minimum reconstruction error.\n3.5\nDependency Networks as GSNs\nDependency networks (Heckerman et al., 2000) are mod-\nels in which one estimates conditionals Pi(xi|x\u2212i), where\nx\u2212i denotes x \\ xi, i.e., the set of variables other than the\ni-th one, xi. Note that each Pi may be parametrized sep-\narately, thus not guaranteeing that there exists a joint of\nwhich they are the conditionals.\nInstead of the ordered\npseudo-Gibbs sampler de\ufb01ned in Heckerman et al. (2000),\nwhich resamples each variable xi in the order x1, x2, . . .,\nwe can view dependency networks in the GSN framework\nby de\ufb01ning a proper Markov chain in which at each step\none randomly chooses which variable to resample. The cor-\nruption process therefore just consists of H = f(X, Z) =\nX\u2212s where X\u2212s is the complement of Xs, with s a ran-\ndomly chosen subset of elements of X (possibly con-\nstrained to be of size 1).\nFurthermore, we parametrize\nthe reconstruction distribution as P\u03b82(X\n=\nx|H)\n=\n\u03b4x\u2212s=X\u2212sP\u03b82,s(Xs = xs|x\u2212s) where the estimated con-\nditionals P\u03b82,s(Xs = xs|x\u2212s) are not constrained to be\nconsistent conditionals of some joint distribution over all\nof X.\nProposition 2. If the above GSN Markov chain has a sta-\ntionary distribution, then the dependency network de\ufb01nes\na joint distribution (which is that stationary distribution),\nwhich does not have to be known in closed form. Further-\nmore, if the conditionals are consistent estimators of the\nground truth conditionals, then that stationary distribution\nis a consistent estimator of the ground truth joint distribu-\ntion.\nThe proposition can be proven by immediate application of\nTheorem 1 from Bengio et al. (2013c) with the above def-\ninitions of the GSN. This joint stationary distribution can\nexist even if the conditionals are not consistent. To show\nthat, assume that some choice of (possibly inconsistent)\nconditionals gives rise to a stationary distribution \u03c0. Now\nlet us consider the set of all conditionals (not necessarily\nconsistent) that could have given rise to that \u03c0. Clearly, the\nconditionals derived from \u03c0 is part of that set, but there are\nin\ufb01nitely many others (a simple counting argument shows\nthat the \ufb01xed point equation of \u03c0 introduces fewer con-\nstraints than the number of degrees of freedom that de\ufb01ne\nthe conditionals). To better understand why the ordered\npseudo-Gibbs chain does not bene\ufb01t from the same proper-\nties, we can consider an extended case by adding an extra\ncomponent of the state X, being the index of the next vari-\nable to resample. In that case, the Markov chain associated\nwith the ordered pseudo-Gibbs procedure would be peri-\nodic, thus violating the ergodicity assumption of the the-\norem. However, by introducing randomness in the choice\nof which variable(s) to resample next, we obtain aperiod-\nicity and ergodicity, yielding as stationary distribution a\nmixture over all possible resampling orders. These results\nalso show in a novel way (see e.g.\nHyv\u00a8arinen (2006) for\nearlier results) that training by pseudolikelihood or gener-\nalized pseudolikelihood provides a consistent estimator of\nthe associated joint, so long as the GSN Markov chain de-\n\ufb01ned above is ergodic. This result can be applied to show\nthat the multi-prediction deep Boltzmann machine (MP-\nDBM) training procedure introduced by Goodfellow et al.\n(2013) also corresponds to a GSN. This has been exploited\nin order to obtain much better samples using the associated\nGSN Markov chain than by sampling from the correspond-\ning DBM (Goodfellow et al., 2013). Another interesting\nconclusion that one can draw from this paper and its GSN\ninterpretation is that state-of-the-art classi\ufb01cation error can\nthereby be obtained: 0.91% on MNIST without \ufb01ne-tuning\n(best comparable previous DBM results was well above\n1%) and 10.6% on permutation-invariant NORB (best pre-\nvious DBM results was 10.8%).\nDeep Generative Stochastic Networks Trainable by Backprop\n4\nExperimental Example of GSN\nThe theoretical results on Generative Stochastic Networks\n(GSNs) open for exploration a large class of possible\nparametrizations which will share the property that they\ncan capture the underlying data distribution through the\nGSN Markov chain.\nWhat parametrizations will work\nwell? Where and how should one inject noise? We present\nresults of preliminary experiments with speci\ufb01c selections\nfor each of these choices, but the reader should keep in\nmind that the space of possibilities is vast.\nAs a conservative starting point, we propose to explore\nfamilies of parametrizations which are similar to existing\ndeep stochastic architectures such as the Deep Boltzmann\nMachine (DBM) (Salakhutdinov & Hinton, 2009). Basi-\ncally, the idea is to construct a computational graph that is\nsimilar to the computational graph for Gibbs sampling or\nvariational inference in Deep Boltzmann Machines. How-\never, we have to diverge a bit from these architectures in\norder to accommodate the desirable property that it will\nbe possible to back-propagate the gradient of reconstruc-\ntion log-likelihood with respect to the parameters \u03b81 and\n\u03b82. Since the gradient of a binary stochastic unit is 0 al-\nmost everywhere, we have to consider related alternatives.\nAn interesting source of inspiration regarding this ques-\ntion is a recent paper on estimating or propagating gra-\ndients through stochastic neurons (Bengio, 2013).\nHere\nwe consider the following stochastic non-linearities: hi =\n\u03b7out + tanh(\u03b7in + ai) where ai is the linear activation for\nunit i (an af\ufb01ne transformation applied to the input of the\nunit, coming from the layer below, the layer above, or both)\nand \u03b7in and \u03b7out are zero-mean Gaussian noises.\nTo emulate a sampling procedure similar to Boltzmann\nmachines in which the \ufb01lled-in missing values can de-\npend on the representations at the top level, the computa-\ntional graph allows information to propagate both upwards\n(from input to higher levels) and downwards, giving rise\nto the computational graph structure illustrated in Figure 3,\nwhich is similar to that explored for deterministic recurrent\nauto-encoders (Seung, 1998; Behnke, 2001; Savard, 2011).\nDownward weight matrices have been \ufb01xed to the trans-\npose of corresponding upward weight matrices.\nThe walkback algorithm was proposed in Bengio et al.\n(2013c) to make training of generalized denoising auto-\nencoders (a special case of the models studied here) more\nef\ufb01cient. The basic idea is that the reconstruction is ob-\ntained after not one but several steps of the sampling\nMarkov chain.\nIn this context it simply means that the\ncomputational graph from X to a reconstruction probabil-\nity actually involves generating intermediate samples as if\nwe were running the Markov chain starting at X. In the\nexperiments, the graph was unfolded so that 2D sampled\nreconstructions would be produced, where D is the depth\n(number of hidden layers). The training loss is the sum\nof the reconstruction negative log-likelihoods (of target X)\nover all those reconstruction steps.\nExperiments evaluating the ability of the GSN models to\ngenerate good samples were performed on the MNIST and\nTFD datasets, following the setup in Bengio et al. (2013b).\nNetworks with 2 and 3 hidden layers were evaluated\nand compared to regular denoising auto-encoders (just 1\nhidden layer, i.e., the computational graph separates into\nseparate ones for each reconstruction step in the walkback\nalgorithm). They all have tanh hidden units and pre- and\npost-activation Gaussian noise of standard deviation 2,\napplied to all hidden layers except the \ufb01rst. In addition,\nat each step in the chain, the input (or the resampled\nXt) is corrupted with salt-and-pepper noise of 40% (i.e.,\n40% of the pixels are corrupted, and replaced with a 0\nor a 1 with probability 0.5). Training is over 100 to 600\nepochs at most, with good results obtained after around\n100 epochs. Hidden layer sizes vary between 1000 and\n1500 depending on the experiments, and a learning rate of\n0.25 and momentum of 0.5 were selected to approximately\nminimize the reconstruction negative log-likelihood. The\nlearning rate is reduced multiplicatively by 0.99 after each\nepoch. Following Breuleux et al. (2011), the quality of the\nsamples was also estimated quantitatively by measuring\nthe log-likelihood of the test set under a Parzen density\nestimator constructed from 10000 consecutively generated\nsamples (using the real-valued mean-\ufb01eld reconstructions\nas the training data for the Parzen density estimator). This\ncan be seen as an lower bound on the true log-likelihood,\nwith the bound converging to the true likelihood as we\nconsider more samples and appropriately set the smoothing\nparameter of the Parzen estimator1 Results are summarized\nin Table 1. The test set Parzen log-likelihood bound was\nnot used to select among model architectures, but visual\ninspection of samples generated did guide the preliminary\nsearch reported here.\nOptimization hyper-parameters\n(learning rate, momentum, and learning rate reduction\nschedule) were selected based on the reconstruction log-\nlikelihood training objective.\nThe Parzen log-likelihood\nbound obtained with a two-layer model on MNIST is\n214 (\u00b1 standard error of 1.1), while the log-likelihood\nbound obtained by a single-layer model (regular denoising\nauto-encoder, DAE in the table) is substantially worse, at\n-152\u00b12.2. In comparison, Bengio et al. (2013b) report a\nlog-likelihood bound of -244\u00b154 for RBMs and 138\u00b12\nfor a 2-hidden layer DBN, using the same setup. We have\nalso evaluated a 3-hidden layer DBM (Salakhutdinov &\n1However, in this paper, to be consistent with the numbers\ngiven in Bengio et al. (2013b) we used a Gaussian Parzen den-\nsity, which makes the numbers not comparable with the AIS log-\nlikelihood upper bounds for binarized images reported in other\npapers for the same data.\nDeep Generative Stochastic Networks Trainable by Backprop\nHinton, 2009), using the weights provided by the author,\nand obtained a Parzen log-likelihood bound of 32\u00b12. See\nhttp://www.mit.edu/\u02dcrsalakhu/DBM.html\nfor details.\nFigure 4. Top: two runs of consecutive samples (one row after the\nother) generated from 2-layer GSN model, showing fast mixing\nbetween classes, nice and sharp images. Note: only every fourth\nsample is shown; see the supplemental material for the samples\nin between. Bottom: conditional Markov chain, with the right\nhalf of the image clamped to one of the MNIST digit images and\nthe left half successively resampled, illustrating the power of the\ngenerative model to stochastically \ufb01ll-in missing inputs. See also\nFigure 6 for longer runs.\nInterestingly, the GSN and the DBN-2 actually perform\nslightly better than when using samples directly coming\nfrom the MNIST training set, maybe because they gener-\nate more \u201cprototypical\u201d samples (we are using mean-\ufb01eld\noutputs).\nFigure 4 shows a single run of consecutive samples from\nthis trained model (see Figure 6 for longer runs), illustrat-\ning that it mixes quite well (better than RBMs) and pro-\nduces rather sharp digit images. The \ufb01gure shows that it\ncan also stochastically complete missing values: the left\nhalf of the image was initialized to random pixels and the\nright side was clamped to an MNIST image. The Markov\nchain explores plausible variations of the completion ac-\ncording to the trained conditional distribution.\nA smaller set of experiments was also run on TFD, yield-\ning a test set Parzen log-likelihood bound of 1890 \u00b129.\nThe setup is exactly the same and was not tuned after\nthe MNIST experiments. A DBN-2 yields a Parzen log-\nlikelihood bound of 1908 \u00b166, which is indistinguishable\nstatistically, while an RBM yields 604 \u00b1 15. One out of\nevery 2 consecutive samples from the GSN-3 model are\nshown in Figure 5 (see Figure 8 for longer runs without\nskips).\nFigure 5. GSN samples from a 3-layer model trained on the TFD\ndataset. Every second sample is shown; see supplemental material\nfor every sample. At the end of each row, we show the nearest\nexample from the training set to the last sample on that row, to\nillustrate that the distribution is not merely copying the training\nset. See also Figure 8 for longer runs without skips.\n5\nConclusion\nWe have introduced a new approach to training generative\nmodels, called Generative Stochastic Networks (GSN), that\nis an alternative to maximum likelihood, with the objective\nof avoiding the intractable marginalizations and the dan-\nger of poor approximations of these marginalizations. The\ntraining procedure is more similar to function approxima-\ntion than to unsupervised learning because the reconstruc-\ntion distribution is simpler than the data distribution, of-\nten unimodal (provably so in the limit of very small noise).\nDeep Generative Stochastic Networks Trainable by Backprop\nTable 1. Test set log-likelihood lower bound (LL) obtained by a Parzen density estimator constructed using 10000 generated samples,\nfor different generative models trained on MNIST. The LL is not directly comparable to AIS likelihood estimates because we use a\nGaussian mixture rather than a Bernoulli mixture to compute the likelihood, but we can compare with Rifai et al. (2012); Bengio et al.\n(2013b;c) (from which we took the last three columns). A DBN-2 has 2 hidden layers, a CAE-1 has 1 hidden layer, and a CAE-2 has 2.\nThe DAE is basically a GSN-1, with no injection of noise inside the network. The last column uses 10000 MNIST training examples to\ntrain the Parzen density estimator.\nGSN-2\nDAE\nDBN-2\nCAE-1\nCAE-2\nMNIST\nLOG-LIKELIHOOD LOWER BOUND\n214\n144\n138\n68\n121\n24\nSTANDARD ERROR\n1.1\n1.6\n2.0\n2.9\n1.6\n1.6\nThis makes it possible to train unsupervised models that\ncapture the data-generating distribution simply using back-\nprop and gradient descent (in a computational graph that\nincludes noise injection). The proposed theoretical results\nstate that under mild conditions (in particular that the noise\ninjected in the networks prevents perfect reconstruction),\ntraining the model to denoise and reconstruct its observa-\ntions (through a powerful family of reconstruction distri-\nbutions) suf\ufb01ces to capture the data-generating distribution\nthrough a simple Markov chain. Another way to put it is\nthat we are training the transition operator of a Markov\nchain whose stationary distribution estimates the data dis-\ntribution, and it turns out that this is a much easier learning\nproblem because the normalization constant for this condi-\ntional distribution is generally dominated by fewer modes.\nThese theoretical results are extended to the case where the\ncorruption is local but still allows the chain to mix and to\nthe case where some inputs are missing or constrained (thus\nallowing to sample from a conditional distribution on a sub-\nset of the observed variables or to learned structured output\nmodels). The GSN framework is shown to lend to depen-\ndency networks a valid estimator of the joint distribution\nof the observed variables even when the learned condition-\nals are not consistent, also allowing to prove consistency of\ngeneralized pseudolikelihood training, associated with the\nstationary distribution of the corresponding GSN (that ran-\ndomly chooses a subset of variables and then resamples it).\nExperiments have been conducted to validate the theory, in\nthe case where the GSN architecture emulates the Gibbs\nsampling process of a Deep Boltzmann Machine, on two\ndatasets. A quantitative evaluation of the samples con\ufb01rms\nthat the training procedure works very well (in this case\nallowing us to train a deep generative model without lay-\nerwise pretraining) and can be used to perform conditional\nsampling of a subset of variables given the rest.\nAcknowledgements\nThe authors would like to acknowledge the stimulating dis-\ncussions and help from Vincent Dumoulin, Pascal Vincent,\nYao Li, Aaron Courville, Ian Goodfellow, and Hod Lipson,\nas well as funding from NSERC, CIFAR (YB is a CIFAR\nSenior Fellow), NASA (JY is a Space Technology Research\nFellow), and the Canada Research Chairs.\nA\nSupplemental Experimental Results\nExperiments evaluating the ability of the GSN models to\ngenerate good samples were performed on the MNIST and\nTFD datasets, following the setup in Bengio et al. (2013c).\nTheorem 2 requires H0 to have the same distribution as H1\n(given X0) during training, and the main paper suggests a\nway to achieve this by initializing each training chain with\nH0 set to the previous value of H1 when the same example\nX0 was shown. However, we did not implement that pro-\ncedure in the experiments below, so that is left for future\nwork to explore.\nNetworks with 2 and 3 hidden layers were evaluated\nand compared to regular denoising auto-encoders (just\n1 hidden layer, i.e., the computational graph separates\ninto separate ones for each reconstruction step in the\nwalkback algorithm).\nThey all have tanh hidden units\nand pre- and post-activation Gaussian noise of standard\ndeviation 2, applied to all hidden layers except the \ufb01rst.\nIn addition, at each step in the chain, the input (or the\nresampled Xt) is corrupted with salt-and-pepper noise of\n40% (i.e., 40% of the pixels are corrupted, and replaced\nwith a 0 or a 1 with probability 0.5).\nTraining is over\n100 to 600 epochs at most, with good results obtained\nafter around 100 epochs, using stochastic gradient descent\n(minibatch size = 1).\nHidden layer sizes vary between\n1000 and 1500 depending on the experiments, and a\nlearning rate of 0.25 and momentum of 0.5 were selected\nto approximately minimize the reconstruction negative\nlog-likelihood.\nThe learning rate is reduced multiplica-\ntively by 0.99 after each epoch.\nFollowing Breuleux et\nal. (2011), the quality of the samples was also estimated\nquantitatively by measuring the log-likelihood of the\ntest set under a Parzen density estimator constructed\nfrom 10000 consecutively generated samples (using the\nreal-valued mean-\ufb01eld reconstructions as the training data\nfor the Parzen density estimator). This can be seen as an\nlower bound on the true log-likelihood, with the bound\nconverging to the true likelihood as we consider more\nDeep Generative Stochastic Networks Trainable by Backprop\nsamples and appropriately set the smoothing parameter of\nthe Parzen estimator2. Results are summarized in Table 1.\nThe test set Parzen log-likelihood bound was not used to\nselect among model architectures, but visual inspection\nof samples generated did guide the preliminary search\nreported here.\nOptimization hyper-parameters (learning\nrate, momentum, and learning rate reduction schedule)\nwere selected based on the reconstruction log-likelihood\ntraining objective.\nThe Parzen log-likelihood bound\nobtained with a two-layer model on MNIST is 214 (\u00b1\nstandard error of 1.1), while the log-likelihood bound\nobtained by a single-layer model (regular denoising\nauto-encoder, DAE in the table) is substantially worse, at\n-152\u00b12.2. In comparison, Bengio et al. (2013c) report a\nlog-likelihood bound of -244\u00b154 for RBMs and 138\u00b12\nfor a 2-hidden layer DBN, using the same setup. We have\nalso evaluated a 3-hidden layer DBM (Salakhutdinov &\nHinton, 2009), using the weights provided by the author,\nand obtained a Parzen log-likelihood bound of 32\u00b12. See\nhttp://www.mit.edu/\u02dcrsalakhu/DBM.html\nfor details. Figure 6 shows two runs of consecutive sam-\nples from this trained model, illustrating that it mixes quite\nwell (better than RBMs) and produces rather sharp digit\nimages. The \ufb01gure shows that it can also stochastically\ncomplete missing values: the left half of the image was\ninitialized to random pixels and the right side was clamped\nto an MNIST image. The Markov chain explores plausible\nvariations of the completion according to the trained\nconditional distribution.\nA smaller set of experiments was also run on TFD, yielding\nfor a GSN a test set Parzen log-likelihood bound of 1890\n\u00b129. The setup is exactly the same and was not tuned after\nthe MNIST experiments. A DBN-2 yields a Parzen log-\nlikelihood bound of 1908 \u00b166, which is undistinguishable\nstatistically, while an RBM yields 604 \u00b1 15. A run of con-\nsecutive samples from the GSN-3 model are shown in Fig-\nure 8. Figure 7 shows consecutive samples obtained early\non during training, after only 5 and 25 epochs respectively,\nillustrating the fast convergence of the training procedure.\nReferences\nAlain, Guillaume and Bengio, Yoshua. What regularized\nauto-encoders learn from the data generating distribu-\ntion. In International Conference on Learning Repre-\nsentations (ICLR\u20192013), 2013.\nBehnke, Sven. Learning iterative image reconstruction in\n2However, in this paper, to be consistent with the numbers\ngiven in Bengio et al. (2013c) we used a Gaussian Parzen density,\nwhich (in addition to being lower rather than upper bounds) makes\nthe numbers not comparable with the AIS log-likelihood upper\nbounds for binarized images reported in some papers for the same\ndata.\nthe neural abstraction pyramid.\nInt. J. Computational\nIntelligence and Applications, 1(4):427\u2013438, 2001.\nBengio, Y., Lamblin, P., Popovici, D., and Larochelle,\nH.\nGreedy layer-wise training of deep networks.\nIn\nNIPS\u20192006, 2007.\nBengio, Yoshua. Learning deep architectures for AI. Now\nPublishers, 2009.\nBengio,\nYoshua.\nEstimating or propagating gradi-\nents through stochastic neurons.\nTechnical Report\narXiv:1305.2982, Universite de Montreal, 2013.\nBengio, Yoshua, Courville, Aaron, and Vincent, Pascal.\nUnsupervised feature learning and deep learning: A re-\nview and new perspectives. IEEE Trans. Pattern Analy-\nsis and Machine Intelligence (PAMI), 2013a.\nBengio, Yoshua, Mesnil, Gr\u00b4egoire, Dauphin, Yann, and Ri-\nfai, Salah. Better mixing via deep representations. In\nICML\u201913, 2013b.\nBengio, Yoshua, Yao, Li, Alain, Guillaume, and Vincent,\nPascal. Generalized denoising auto-encoders as genera-\ntive models. In NIPS26. Nips Foundation, 2013c.\nBreuleux, Olivier, Bengio, Yoshua, and Vincent, Pas-\ncal.\nQuickly generating representative samples from\nan RBM-derived process. Neural Computation, 23(8):\n2053\u20132073, 2011.\nCho, Grace E., Meyer, Carl D., Carl, and Meyer, D. Com-\nparison of perturbation bounds for the stationary distri-\nbution of a markov chain. Linear Algebra Appl, 335:\n137\u2013150, 2000.\nDahl, George E., Ranzato, Marc\u2019Aurelio, Mohamed,\nAbdel-rahman, and Hinton, Geoffrey E. Phone recogni-\ntion with the mean-covariance restricted Boltzmann ma-\nchine. In NIPS\u20192010, 2010.\nDeng, L., Seltzer, M., Yu, D., Acero, A., Mohamed, A., and\nHinton, G. Binary coding of speech spectrograms using\na deep auto-encoder.\nIn Interspeech 2010, Makuhari,\nChiba, Japan, 2010.\nGoodfellow, Ian J., Mirza, Mehdi, Courville, Aaron, and\nBengio, Yoshua. Multi-prediction deep Boltzmann ma-\nchines. In NIPS26. Nips Foundation, 2013.\nHeckerman, David, Chickering, David Maxwell, Meek,\nChristopher, Rounthwaite, Robert, and Kadie, Carl. De-\npendency networks for inference, collaborative \ufb01ltering,\nand data visualization. Journal of Machine Learning Re-\nsearch, 1:49\u201375, 2000.\nDeep Generative Stochastic Networks Trainable by Backprop\nHinton, Geoffrey E., Osindero, Simon, and Teh, Yee Whye.\nA fast learning algorithm for deep belief nets. Neural\nComputation, 18:1527\u20131554, 2006.\nHinton, Geoffrey E., Srivastava, Nitish, Krizhevsky, Alex,\nSutskever, Ilya, and Salakhutdinov, Ruslan. Improving\nneural networks by preventing co-adaptation of feature\ndetectors. Technical report, arXiv:1207.0580, 2012.\nHyv\u00a8arinen, Aapo. Consistency of pseudolikelihood estima-\ntion of fully visible boltzmann machines. Neural Com-\nputation, 2006.\nKingma, Diederik P.\nFast gradient-based inference with\ncontinuous latent variable models in auxiliary form.\nTechnical report, arXiv:1306.0733, 2013.\nKrizhevsky, A., Sutskever, I., and Hinton, G. ImageNet\nclassi\ufb01cation with deep convolutional neural networks.\nIn NIPS\u20192012. 2012.\nLee, Honglak, Battle, Alexis, Raina, Rajat, and Ng, An-\ndrew. Ef\ufb01cient sparse coding algorithms. In NIPS\u201906,\npp. 801\u2013808. MIT Press, 2007.\nLuo, Heng, Carrier, Pierre Luc, Courville, Aaron, and Ben-\ngio, Yoshua. Texture modeling with convolutional spike-\nand-slab RBMs and deep extensions. In AISTATS\u20192013,\n2013.\nMontavon, Gregoire and Muller, Klaus-Robert.\nDeep\nBoltzmann machines and the centering trick. In Mon-\ntavon, Gr\u00b4egoire, Orr, Genevieve, and M\u00a8uller, Klaus-\nRobert (eds.), Neural Networks: Tricks of the Trade,\nvolume 7700 of Lecture Notes in Computer Science, pp.\n621\u2013637. 2012.\nPoon, Hoifung and Domingos, Pedro.\nSum-product\nnetworks:\nA new deep architecture.\nIn UAI\u20192011,\nBarcelona, Spain, 2011.\nRanzato, M., Poultney, C., Chopra, S., and LeCun, Y. Ef\ufb01-\ncient learning of sparse representations with an energy-\nbased model. In NIPS\u20192006, 2007.\nRezende, Danilo J., Mohamed, Shakir, and Wierstra,\nDaan. Stochastic backpropagation and approximate in-\nference in deep generative models.\nTechnical report,\narXiv:1401.4082, 2014.\nRifai, Salah, Bengio, Yoshua, Dauphin, Yann, and Vincent,\nPascal. A generative process for sampling contractive\nauto-encoders. In ICML\u201912, 2012.\nSalakhutdinov, Ruslan and Hinton, Geoffrey E.\nDeep\nBoltzmann machines. In AISTATS\u20192009, pp. 448\u2013455,\n2009.\nSavard, Franc\u00b8ois. R\u00b4eseaux de neurones `a relaxation en-\ntra\u02c6\u0131n\u00b4es par crit`ere d\u2019autoencodeur d\u00b4ebruitant. Master\u2019s\nthesis, U. Montr\u00b4eal, 2011.\nSchweitzer, Paul J. Perturbation theory and \ufb01nite markov\nchains.\nJournal of Applied Probability, pp. 401\u2013413,\n1968.\nSeide, Frank, Li, Gang, and Yu, Dong.\nConversational\nspeech transcription using context-dependent deep neu-\nral networks. In Interspeech 2011, pp. 437\u2013440, 2011.\nSeung, Sebastian H. Learning continuous attractors in re-\ncurrent networks. In NIPS\u201997, pp. 654\u2013660. MIT Press,\n1998.\nVincent, Pascal, Larochelle, Hugo, Bengio, Yoshua, and\nManzagol, Pierre-Antoine.\nExtracting and composing\nrobust features with denoising autoencoders. In ICML\n2008, 2008.\nDeep Generative Stochastic Networks Trainable by Backprop\nFigure 6. These are expanded plots of those in Figure 4. Top: two runs of consecutive samples (one row after the other) generated from\na 2-layer GSN model, showing that it mixes well between classes and produces nice and sharp images. Figure 4 contained only one in\nevery four samples, whereas here we show every sample. Bottom: conditional Markov chain, with the right half of the image clamped\nto one of the MNIST digit images and the left half successively resampled, illustrating the power of the trained generative model to\nstochastically \ufb01ll-in missing inputs. Figure 4 showed only 13 samples in each chain; here we show 26.\nDeep Generative Stochastic Networks Trainable by Backprop\nFigure 7. Left: consecutive GSN samples obtained after 10 training epochs. Right: GSN samples obtained after 25 training epochs. This\nshows quick convergence to a model that samples well. The samples in Figure 6 are obtained after 600 training epochs.\nFigure 8. Consecutive GSN samples from a 3-layer model trained on the TFD dataset. At the end of each row, we show the nearest\nexample from the training set to the last sample on that row to illustrate that the distribution is not merely copying the training set.\n",
        "sentence": " Recently, stochastic neural networks (Bengio et al., 2014; Kingma et al., 2014; Gregor et al., 2015) have become popular, and deterministic networks are being used for image generation tasks (Dosovitskiy et al. Recently, stochastic neural networks (Bengio et al., 2014; Kingma et al., 2014; Gregor et al., 2015) have become popular, and deterministic networks are being used for image generation tasks (Dosovitskiy et al., 2015b). In all these models, loss is measured in the image space. By combining convolutions and unpooling (upsampling) layers (Lee et al., 2009; Goodfellow et al., 2014; Dosovitskiy et al., 2015b) these models can be applied to large images. There is a large body of work on assessing the perceptual similarity of images. Some prominent examples are the visible differences predictor (Daly, 1993), the spatio-temporal model for moving picture quality assessment (van den Branden Lambrecht & Verscheure, 1996), and the perceptual distortion metric of Winkler (1998). The most popular perceptual image similarity metric is the structural similarity metric (SSIM) (Wang et al. Recently, stochastic neural networks (Bengio et al., 2014; Kingma et al., 2014; Gregor et al., 2015) have become popular, and deterministic networks are being used for image generation tasks (Dosovitskiy et al., 2015b). In all these models, loss is measured in the image space. By combining convolutions and unpooling (upsampling) layers (Lee et al., 2009; Goodfellow et al., 2014; Dosovitskiy et al., 2015b) these models can be applied to large images. There is a large body of work on assessing the perceptual similarity of images. Some prominent examples are the visible differences predictor (Daly, 1993), the spatio-temporal model for moving picture quality assessment (van den Branden Lambrecht & Verscheure, 1996), and the perceptual distortion metric of Winkler (1998). The most popular perceptual image similarity metric is the structural similarity metric (SSIM) (Wang et al., 2004), which compares the local statistics of image patches. We are not aware of any work making use of similarity metrics for machine learning, except a recent pre-print of Ridgeway et al. (2015). They train autoencoders by directly maximizing the SSIM similarity of images.",
        "context": "tion than to unsupervised learning because the reconstruc-\ntion distribution is simpler than the data distribution, of-\nten unimodal (provably so in the limit of very small noise).\nDeep Generative Stochastic Networks Trainable by Backprop\nDeep Generative Stochastic Networks Trainable by Backprop\n1\nIntroduction\nResearch in deep learning (see Bengio (2009) and Ben-\ngio et al. (2013a) for reviews) grew from breakthroughs in\nunsupervised learning of representations, based mostly on\nsupervised training tricks. For example, one could use a\nconvolutional architecture with max-pooling for parametric\nparsimony and computational ef\ufb01ciency, or dropout (Hin-\nton et al., 2012) to prevent co-adaptation of hidden repre-\nsentations."
    },
    {
        "title": "Digital images and human vision. chapter The Visible Differences Predictor: An Algorithm for the Assessment of Image Fidelity, pp. 179\u2013206",
        "author": [
            "S. Daly"
        ],
        "venue": null,
        "citeRegEx": "Daly.,? \\Q1993\\E",
        "shortCiteRegEx": "Daly.",
        "year": 1993,
        "abstract": "",
        "full_text": "",
        "sentence": " Some prominent examples are the visible differences predictor (Daly, 1993), the spatio-temporal model for moving picture quality assessment (van den Branden Lambrecht & Verscheure, 1996), and the perceptual distortion metric of Winkler (1998).",
        "context": null
    },
    {
        "title": "ImageNet: A Large-Scale Hierarchical Image Database",
        "author": [
            "J. Deng",
            "W. Dong",
            "R. Socher",
            "L.-J. Li",
            "K. Li",
            "L. Fei-Fei"
        ],
        "venue": "In CVPR,",
        "citeRegEx": "Deng et al\\.,? \\Q2009\\E",
        "shortCiteRegEx": "Deng et al\\.",
        "year": 2009,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " First, when dealing with large ImageNet (Deng et al., 2009) images we increase the stride in the first layer from 2 to 4.",
        "context": null
    },
    {
        "title": "Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks",
        "author": [
            "E.L. Denton",
            "S. Chintala",
            "arthur Szlam",
            "R. Fergus"
        ],
        "venue": "In Advances in Neural Information Processing Systems",
        "citeRegEx": "Denton et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "Denton et al\\.",
        "year": 2015,
        "abstract": "In this paper we introduce a generative parametric model capable of producing\nhigh quality samples of natural images. Our approach uses a cascade of\nconvolutional networks within a Laplacian pyramid framework to generate images\nin a coarse-to-fine fashion. At each level of the pyramid, a separate\ngenerative convnet model is trained using the Generative Adversarial Nets (GAN)\napproach (Goodfellow et al.). Samples drawn from our model are of significantly\nhigher quality than alternate approaches. In a quantitative assessment by human\nevaluators, our CIFAR10 samples were mistaken for real images around 40% of the\ntime, compared to 10% for samples drawn from a GAN baseline model. We also show\nsamples from models trained on the higher resolution images of the LSUN scene\ndataset.",
        "full_text": "Deep Generative Image Models using a\nLaplacian Pyramid of Adversarial Networks\nEmily Denton\u2217\nDept. of Computer Science\nCourant Institute\nNew York University\nSoumith Chintala\u2217\nArthur Szlam\nRob Fergus\nFacebook AI Research\nNew York\nAbstract\nIn this paper we introduce a generative parametric model capable of producing\nhigh quality samples of natural images. Our approach uses a cascade of convo-\nlutional networks within a Laplacian pyramid framework to generate images in\na coarse-to-\ufb01ne fashion. At each level of the pyramid, a separate generative con-\nvnet model is trained using the Generative Adversarial Nets (GAN) approach [10].\nSamples drawn from our model are of signi\ufb01cantly higher quality than alternate\napproaches. In a quantitative assessment by human evaluators, our CIFAR10 sam-\nples were mistaken for real images around 40% of the time, compared to 10% for\nsamples drawn from a GAN baseline model. We also show samples from models\ntrained on the higher resolution images of the LSUN scene dataset.\n1\nIntroduction\nBuilding a good generative model of natural images has been a fundamental problem within com-\nputer vision. However, images are complex and high dimensional, making them hard to model\nwell, despite extensive efforts. Given the dif\ufb01culties of modeling entire scene at high-resolution,\nmost existing approaches instead generate image patches. In contrast, in this work, we propose\nan approach that is able to generate plausible looking scenes at 32 \u00d7 32 and 64 \u00d7 64. To do this,\nwe exploit the multi-scale structure of natural images, building a series of generative models, each\nof which captures image structure at a particular scale of a Laplacian pyramid [1]. This strategy\nbreaks the original problem into a sequence of more manageable stages. At each scale we train a\nconvolutional network-based generative model using the Generative Adversarial Networks (GAN)\napproach of Goodfellow et al. [10]. Samples are drawn in a coarse-to-\ufb01ne fashion, commencing\nwith a low-frequency residual image. The second stage samples the band-pass structure at the next\nlevel, conditioned on the sampled residual. Subsequent levels continue this process, always condi-\ntioning on the output from the previous scale, until the \ufb01nal level is reached. Thus drawing samples\nis an ef\ufb01cient and straightforward procedure: taking random vectors as input and running forward\nthrough a cascade of deep convolutional networks (convnets) to produce an image.\nDeep learning approaches have proven highly effective at discriminative tasks in vision, such as\nobject classi\ufb01cation [3]. However, the same level of success has not been obtained for generative\ntasks, despite numerous efforts [13, 24, 28]. Against this background, our proposed approach makes\na signi\ufb01cant advance in that it is straightforward to train and sample from, with the resulting samples\nshowing a surprising level of visual \ufb01delity, indicating a better density model than prior methods.\n1.1\nRelated Work\nGenerative image models are well studied, falling into two main approaches: non-parametric and\nparametric. The former copy patches from training images to perform, for example, texture synthesis\n[6] or super-resolution [8]. More ambitiously, entire portions of an image can be in-painted, given a\nsuf\ufb01ciently large training dataset [12]. Early parametric models addressed the easier problem of tex-\n\u2217denotes equal contribution.\n1\narXiv:1506.05751v1  [cs.CV]  18 Jun 2015\nture synthesis [2, 31, 20], with Portilla & Simoncelli [20] making use of a steerable pyramid wavelet\nrepresentation [25], similar to our use of a Laplacian pyramid. For image processing tasks, models\nbased on marginal distributions of image gradients are effective [18, 23], but are only designed for\nimage restoration rather than being true density models (so cannot sample an actual image). Very\nlarge Gaussian mixture models [32] and sparse coding models of image patches [29] can also be\nused but suffer the same problem.\nA wide variety of deep learning approaches involve generative parametric models. Restricted Boltz-\nmann machines [13, 16, 19, 21], Deep Boltzmann machines [24, 7], Denoising auto-encoders [28]\nall have a generative decoder that reconstructs the image from the latent representation. Variational\nauto-encoders [15, 22] provide probabilistic interpretation which facilitates sampling. However, for\nall these methods convincing samples have only been shown on simple datasets such as MNIST\nand NORB, possibly due to training complexities which limit their applicability to larger and more\nrealistic images.\nSeveral recent papers have proposed novel generative models. Dosovitskiy et al. [5] showed how a\nconvnet can draw chairs with different shapes and viewpoints. While our model also makes use of\nconvnets, it is able to sample general scenes and objects. The DRAW model of Gregor et al. [11]\nused an attentional mechanism with an RNN to generate images via a trajectory of patches, showing\nsamples of MNIST and CIFAR10 images. Sohl-Dickstein et al. [26] use a diffusion-based process\nfor deep unsupervised learning and the resulting model is able to produce reasonable CIFAR10 sam-\nples. Theis and Bethge [27] employ LSTMs to capture spatial dependencies and show convincing\ninpainting results of natural textures.\nOur work builds on the GAN approach of Goodfellow et al. [10] which works well for smaller\nimages (e.g. MNIST) but cannot directly handle large ones, unlike our method. Most relevant to our\napproach is the preliminary work of Mirza and Osindero [17] and Gauthier [9] who both propose\nconditional versions of the GAN model. The former shows MNIST samples, while the latter focuses\nsolely on frontal face images. Our approach also uses several forms of conditional GAN model but\nis much more ambitious in its scope.\n2\nApproach\nThe basic building block of our approach is the generative adversarial network (GAN) of Goodfellow\net al. [10]. After reviewing this, we introduce our LAPGAN model which integrates a conditional\nform of GAN model into the framework of a Laplacian pyramid.\n2.1\nGenerative Adversarial Networks\nThe GAN approach [10] is a framework for training generative models, which we brie\ufb02y explain in\nthe context of image data. The method pits two networks against one another: a generative model G\nthat captures the data distribution and a discriminative model D that distinguishes between samples\ndrawn from G and images drawn from the training data. In our approach, both G and D are convo-\nlutional networks. The former takes as input a noise vector z drawn from a distribution pNoise(z) and\noutputs an image \u02dch. The discriminative network D takes an image as input stochastically chosen\n(with equal probability) to be either \u02dch \u2013 as generated from G, or h \u2013 a real image drawn from the\ntraining data pData(h). D outputs a scalar probability, which is trained to be high if the input was\nreal and low if generated from G. A minimax objective is used to train both models together:\nmin\nG max\nD Eh\u223cpData(h)[log D(h)] + Ez\u223cpNoise(z)[log(1 \u2212D(G(z)))]\n(1)\nThis encourages G to \ufb01t pData(h) so as to fool D with its generated samples \u02dch. Both G and D\nare trained by backpropagating the loss in Eqn. 1 through their respective models to update the\nparameters.\nThe conditional generative adversarial net (CGAN) is an extension of the GAN where both networks\nG and D receive an additional vector of information l as input. This might contain, say, information\nabout the class of the training example h. The loss function thus becomes\nmin\nG max\nD Eh,l\u223cpData(h,l)[log D(h, l)] + Ez\u223cpNoise(z),l\u223cpl(l)[log(1 \u2212D(G(z, l), l))]\n(2)\nwhere pl(l) is, for example, the prior distribution over classes. This model allows the output of\nthe generative model to be controlled by the conditioning variable l. Mirza and Osindero [17] and\n2\nGauthier [9] both explore this model with experiments on MNIST and faces, using l as a class\nindicator. In our approach, l will be another image, generated from another CGAN model.\n2.2\nLaplacian Pyramid\nThe Laplacian pyramid [1] is a linear invertible image representation consisting of a set of band-pass\nimages, spaced an octave apart, plus a low-frequency residual. Formally, let d(.) be a downsampling\noperation which blurs and decimates a j \u00d7 j image I, so that d(I) is a new image of size j/2 \u00d7 j/2.\nAlso, let u(.) be an upsampling operator which smooths and expands I to be twice the size, so u(I)\nis a new image of size 2j \u00d7 2j. We \ufb01rst build a Gaussian pyramid G(I) = [I0, I1, . . . , IK], where\nI0 = I and Ik is k repeated applications\u2217of d(.) to I. K is the number of levels in the pyramid,\nselected so that the \ufb01nal level has very small spatial extent (\u22648 \u00d7 8 pixels).\nThe coef\ufb01cients hk at each level k of the Laplacian pyramid L(I) are constructed by taking the\ndifference between adjacent levels in the Gaussian pyramid, upsampling the smaller one with u(.)\nso that the sizes are compatible:\nhk = Lk(I) = Gk(I) \u2212u(Gk+1(I)) = Ik \u2212u(Ik+1)\n(3)\nIntuitively, each level captures image structure present at a particular scale. The \ufb01nal level of the\nLaplacian pyramid hK is not a difference image, but a low-frequency residual equal to the \ufb01nal\nGaussian pyramid level, i.e.\nhK = IK. Reconstruction from a Laplacian pyramid coef\ufb01cients\n[h1, . . . , hK] is performed using the backward recurrence:\nIk = u(Ik+1) + hk\n(4)\nwhich is started with IK = hK and the reconstructed image being I = Io. In other words, starting\nat the coarsest level, we repeatedly upsample and add the difference image h at the next \ufb01ner level\nuntil we get back to the full resolution image.\n2.3\nLaplacian Generative Adversarial Networks (LAPGAN)\nOur proposed approach combines the conditional GAN model with a Laplacian pyramid represen-\ntation. The model is best explained by \ufb01rst considering the sampling procedure. Following training\n(explained below), we have a set of generative convnet models {G0, . . . , GK}, each of which cap-\ntures the distribution of coef\ufb01cients hk for natural images at a different level of the Laplacian pyra-\nmid. Sampling an image is akin to the reconstruction procedure in Eqn. 4, except that the generative\nmodels are used to produce the hk\u2019s:\n\u02dcIk = u(\u02dcIk+1) + \u02dchk = u(\u02dcIk+1) + Gk(zk, u(\u02dcIk+1))\n(5)\nThe recurrence starts by setting \u02dcIK+1 = 0 and using the model at the \ufb01nal level GK to generate a\nresidual image \u02dcIK using noise vector zK: \u02dcIK = GK(zK). Note that models at all levels except the\n\ufb01nal are conditional generative models that take an upsampled version of the current image \u02dcIk+1 as\na conditioning variable, in addition to the noise vector zk. Fig. 1 shows this procedure in action for\na pyramid with K = 3 using 4 generative models to sample a 64 \u00d7 64 image.\nThe generative models {G0, . . . , GK} are trained using the CGAN approach at each level of the\npyramid. Speci\ufb01cally, we construct a Laplacian pyramid from each training image I. At each level\n\u2217i.e. I2 = d(d(I)).\nG2 \n~ I3 \nG3 \nz2 \n~ h2 \nz3 \nG1 \nz1 \nG0 \nz0 \n~ I2 \nl2 \n~ I0 \nh0 \n~ \nI1 \n~ \n~ h1 \nl1 \nl0 \nFigure 1: The sampling procedure for our LAPGAN model. We start with a noise sample z3 (right side) and\nuse a generative model G3 to generate \u02dcI3. This is upsampled (green arrow) and then used as the conditioning\nvariable (orange arrow) l2 for the generative model at the next level, G2. Together with another noise sample\nz2, G2 generates a difference image \u02dch2 which is added to l2 to create \u02dcI2. This process repeats across two\nsubsequent levels to yield a \ufb01nal full resolution sample I0.\n3\nG0 \nl2 \n~ I3 \nG3 \nD0 \nz0 \nD1 \nD2 \nh2 \n~ h2 \nz3 \nD3 \nI3 \nI2 \nI2 \nI3 \nReal/Generated? \nReal/ \nGenerated? \nG1 \nz1 \nG2 \nz2 \nReal/Generated? \nReal/ \nGenerated? \nl0 \nI = I0 \nh0 \nI1 \nI1 \nl1 \n~ h1 \nh1 \nh0 \n~ \nFigure 2: The training procedure for our LAPGAN model. Starting with a 64x64 input image I from our\ntraining set (top left): (i) we take I0 = I and blur and downsample it by a factor of two (red arrow) to produce\nI1; (ii) we upsample I1 by a factor of two (green arrow), giving a low-pass version l0 of I0; (iii) with equal\nprobability we use l0 to create either a real or a generated example for the discriminative model D0. In the real\ncase (blue arrows), we compute high-pass h0 = I0 \u2212l0 which is input to D0 that computes the probability of\nit being real vs generated. In the generated case (magenta arrows), the generative network G0 receives as input\na random noise vector z0 and l0. It outputs a generated high-pass image \u02dch0 = G0(z0, l0), which is input to\nD0. In both the real/generated cases, D0 also receives l0 (orange arrow). Optimizing Eqn. 2, G0 thus learns\nto generate realistic high-frequency structure \u02dch0 consistent with the low-pass image l0. The same procedure is\nrepeated at scales 1 and 2, using I1 and I2. Note that the models at each level are trained independently. At\nlevel 3, I3 is an 8\u00d78 image, simple enough to be modeled directly with a standard GANs G3 & D3.\nwe make a stochastic choice (with equal probability) to either (i) construct the coef\ufb01cients hk either\nusing the standard procedure from Eqn. 3, or (ii) generate them using Gk:\n\u02dchk = Gk(zk, u(Ik+1))\n(6)\nNote that Gk is a convnet which uses a coarse scale version of the image lk = u(Ik+1) as an input,\nas well as noise vector zk. Dk takes as input hk or \u02dchk, along with the low-pass image lk (which is\nexplicitly added to hk or \u02dchk before the \ufb01rst convolution layer), and predicts if the image was real or\ngenerated. At the \ufb01nal scale of the pyramid, the low frequency residual is suf\ufb01ciently small that it\ncan be directly modeled with a standard GAN: \u02dchK = GK(zK) and DK only has hK or \u02dchK as input.\nThe framework is illustrated in Fig. 2.\nBreaking the generation into successive re\ufb01nements is the key idea in this work. Note that we give\nup any \u201cglobal\u201d notion of \ufb01delity; we never make any attempt to train a network to discriminate\nbetween the output of a cascade and a real image and instead focus on making each step plausible.\nFurthermore, the independent training of each pyramid level has the advantage that it is far more\ndif\ufb01cult for the model to memorize training examples \u2013 a hazard when high capacity deep networks\nare used.\nAs described, our model is trained in an unsupervised manner. However, we also explore variants\nthat utilize class labels. This is done by add a 1-hot vector c, indicating class identity, as another\nconditioning variable for Gk and Dk.\n3\nModel Architecture & Training\nWe apply our approach to three datasets: (i) CIFAR10 \u2013 32\u00d732 pixel color images of 10 different\nclasses, 100k training samples with tight crops of objects; (ii) STL \u2013 96\u00d796 pixel color images of\n10 different classes, 100k training samples (we use the unlabeled portion of data); and (iii) LSUN\n[30] \u2013 \u223c10M images of 10 different natural scene types, downsampled to 64\u00d764 pixels.\nFor each dataset, we explored a variety of architectures for {Gk, Dk}. We now detail the best\nperforming models, selected using a combination of log-likelihood and visual appearance of the\nsamples. Complete Torch speci\ufb01cation \ufb01les for all models are provided in supplementary material\n[4]. For all models, the noise vector zk is drawn from a uniform [-1,1] distribution.\n4\n3.1\nCIFAR10 and STL\nInitial scale: This operates at 8 \u00d7 8 resolution, using densely connected nets for both GK & DK\nwith 2 hidden layers and ReLU non-linearities. DK uses Dropout and has 600 units/layer vs 1200\nfor GK. zK is a 100-d vector.\nSubsequent scales: For CIFAR10, we boost the training set size by taking four 28 \u00d7 28 crops from\nthe original images. Thus the two subsequent levels of the pyramid are 8 \u219214 and 14 \u219228. For\nSTL, we have 4 levels going from 8 \u219216 \u219232 \u219264 \u219296. For both datasets, Gk & Dk are\nconvnets with 3 and 2 layers, respectively (see [4]). The noise input zk to Gk is presented as a 4th\n\u201ccolor plane\u201d to low-pass lk, hence its dimensionality varies with the pyramid level. For CIFAR10,\nwe also explore a class conditional version of the model, where a vector c encodes the label. This is\nintegrated into Gk & Dk by passing it through a linear layer whose output is reshaped into a single\nplane feature map which is then concatenated with the 1st layer maps. The loss in Eqn. 2 is trained\nusing SGD with an initial learning rate of 0.02, decreased by a factor of (1 + 4 \u00d7 10\u22125) at each\nepoch. Momentum starts at 0.5, increasing by 0.0008 at epoch up to a maximum of 0.8. During\ntraining, we monitor log-likelihood using a Parzen-window estimator and retain the best performing\nmodel. Training time depends on the models size and pyramid level, with smaller models taking\nhours to train and larger models taking several days.\n3.2\nLSUN\nThe larger size of this dataset allows us to train a separate LAPGAN model for each the 10 different\nscene classes. During evaluation, so that we may understand the variation captured by our models,\nwe commence the sampling process with validation set images\u2020, downsampled to 4 \u00d7 4 resolution.\nThe four subsequent scales 4 \u21928 \u219216 \u219232 \u219264 use a common architecture for Gk & Dk at\neach level. Gk is a 5-layer convnet with {64, 368, 128, 224} feature maps and a linear output layer.\n7 \u00d7 7 \ufb01lters, ReLUs, batch normalization [14] and Dropout are used at each hidden layer. Dk has\n3 hidden layers with {48, 448, 416} maps plus a sigmoid output. See [4] for full details. Note that\nGk and Dk are substantially larger than those used for CIFAR10 and STL, as afforded by the larger\ntraining set.\n4\nExperiments\nWe evaluate our approach using 3 different methods: (i) computation of log-likelihood on a held\nout image set; (ii) drawing sample images from the model and (iii) a human subject experiment that\ncompares (a) our samples, (b) those of baseline methods and (c) real images.\n4.1\nEvaluation of Log-Likelihood\nA traditional method for evaluating generative models is to measure their log-likelihood on a held\nout set of images. But, like the original GAN method [10], our approach does not have a direct\nway of computing the probability of an image. Goodfellow et al. [10] propose using a Gaussian\nParzen window estimate to compute log-likelihoods. Despite showing poor performance in high\ndimensional spaces, this approach is the best one available for estimating likelihoods of models\nlacking an explicitly represented density function.\nOur LAPGAN model allows for an alternative method of estimating log-likelihood that exploits the\nmulti-scale structure of the model. This new approach uses a Gaussian Parzen window estimate to\ncompute a probability at each scale of the Laplacian pyramid. We use this procedure, described in\ndetail in Appendix A, to compute the log-likelihoods for CIFAR10 and STL images (both at 32\u00d732\nresolution). The parameter \u03c3 (controlling the Parzen window size) was chosen using the validation\nset. We also compute the Parzen window based log-likelihood estimates of the standard GAN [10]\nmodel, using 50k samples for both the CIFAR10 and STL estimates. Table 1 shows our model\nachieving a signi\ufb01cantly higher log-likelihood on both datasets. Comparisons to further approaches,\nnotably [26], are problematic due to different normalizations used on the data.\n4.2\nModel Samples\nWe show samples from models trained on CIFAR10, STL and LSUN datasets. Additional samples\ncan be found in the supplementary material [4].\n\u2020These were not used in any way during training.\n5\nModel\nCIFAR10\nSTL (@32\u00d732)\nGAN [10]\n-3617 \u00b1 353\n-3661 \u00b1 347\nLAPGAN\n-1799 \u00b1 826\n-2906 \u00b1 728\nTable 1: Parzen window based log-likelihood estimates for a standard GAN, our proposed LAPGAN\nmodel on CIFAR10 and STL datasets.\nFig. 3 shows samples from our models trained on CIFAR10. Samples from the class conditional\nLAPGAN are organized by class. Our reimplementation of the standard GAN model [10] produces\nslightly sharper images than those shown in the original paper. We attribute this improvement to\nthe introduction of data augmentation. The LAPGAN samples improve upon the standard GAN\nsamples. They appear more object-like and have more clearly de\ufb01ned edges. Conditioning on a\nclass label improves the generations as evidenced by the clear object structure in the conditional\nLAPGAN samples. The quality of these samples compares favorably with those from the DRAW\nmodel of Gregor et al. [11] and also Sohl-Dickstein et al. [26]. The rightmost column of each image\nshows the nearest training example to the neighboring sample (in L2 pixel-space). This demonstrates\nthat our model is not simply copying the input examples.\nFig. 4(a) shows samples from our LAPGAN model trained on STL. Here, we lose clear object shape\nbut the samples remain sharp. Fig. 4(b) shows the generation chain for random STL samples.\nFig. 5 shows samples from LAPGAN models trained on three LSUN categories (tower, bedroom,\nchurch front). The 4 \u00d7 4 validation image used to start the generation process is shown in the \ufb01rst\ncolumn, along with 10 different 64 \u00d7 64 samples, which illustrate the inherent variation captured\nby the model. Collectively, these show the models capturing long-range structure within the scenes,\nbeing able to recompose scene elements into credible looking images. To the best of our knowledge,\nno other generative model has been able to produce samples of this complexity. The substantial\ngain in quality over the CIFAR10 and STL samples is likely due to the much larger training LSUN\ntraining set which allowed us to train bigger and deeper models.\n4.3\nHuman Evaluation of Samples\nTo obtain a quantitative measure of quality of our samples, we asked 15 volunteers to participate\nin an experiment to see if they could distinguish our samples from real images. The subjects were\npresented with the user interface shown in Fig. 6(right) and shown at random four different types\nof image: samples drawn from three different GAN models trained on CIFAR10 ((i) LAPGAN, (ii)\nclass conditional LAPGAN and (iii) standard GAN [10]) and also real CIFAR10 images. After being\npresented with the image, the subject clicked the appropriate button to indicate if they believed the\nimage was real or generated. Since accuracy is a function of viewing time, we also randomly pick the\npresentation time from one of 11 durations ranging from 50ms to 2000ms, after which a gray mask\nimage is displayed. Before the experiment commenced, they were shown examples of real images\nfrom CIFAR10. After collecting \u223c10k samples from the volunteers, we plot in Fig. 6 the fraction of\nimages believed to be real for the four different data sources, as a function of presentation time. The\ncurves show our models produce samples that are far more realistic than those from standard GAN\n[10].\n5\nDiscussion\nBy modifying the approach in [10] to better respect the structure of images, we have proposed a\nconceptually simple generative model that is able to produce high-quality sample images that are\nboth qualitatively and quantitatively better than other deep generative modeling approaches. A key\npoint in our work is giving up any \u201cglobal\u201d notion of \ufb01delity, and instead breaking the generation\ninto plausible successive re\ufb01nements. We note that many other signal modalities have a multiscale\nstructure that may bene\ufb01t from a similar approach.\n6\nCC-LAPGAN: Airplane \nCC-LAPGAN: Automobile \nCC-LAPGAN: Bird \nCC-LAPGAN: Cat \nCC-LAPGAN: Deer \nCC-LAPGAN: Dog \nCC-LAPGAN: Frog \nCC-LAPGAN: Horse \nCC-LAPGAN: Ship \nCC-LAPGAN: Truck \nGAN [14] \nLAPGAN \nFigure 3: CIFAR10 samples: our class conditional CC-LAPGAN model, our LAPGAN model and\nthe standard GAN model of Goodfellow [10]. The yellow column shows the training set nearest\nneighbors of the samples in the adjacent column.\n(a)\n(b)\nFigure 4: STL samples: (a) Random 96x96 samples from our LAPGAN model. (b) Coarse-to-\ufb01ne\ngeneration chain.\n7\nFigure 5: 64 \u00d7 64 samples from three different LSUN LAPGAN models (top: tower, middle: bed-\nroom, bottom: church front). The \ufb01rst column shows the 4 \u00d7 4 validation set image used to start the\ngeneration process, with subsequent columns showing different draws from the model.\n8\n  50\n  75\n 100\n 150\n 200\n 300\n 400\n 650\n1000\n2000\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nPresentation time (ms)\n% classified real\n \n \nReal\nCC\u2212LAPGAN\nLAPGAN\nGAN\nFigure 6: Left: Human evaluation of real CIFAR10 images (red) and samples from Goodfellow\net al. [10] (magenta), our LAPGAN (blue) and a class conditional LAPGAN (green). The error\nbars show \u00b11\u03c3 of the inter-subject variability. Around 40% of the samples generated by our class\nconditional LAPGAN model are realistic enough to fool a human into thinking they are real images.\nThis compares with \u226410% of images from the standard GAN model [10], but is still a lot lower\nthan the > 90% rate for real images. Right: The user-interface presented to the subjects.\nAppendix A\nTo describe the log-likelihood computation in our model, let us consider a two scale pyramid for\nthe moment. Given a (vectorized) j \u00d7 j image I, denote by l = d(I) the coarsened image, and\nh = I \u2212u(d(I)) to be the high pass. In this section, to simplify the computations, we use a slightly\ndifferent u operator than the one used to generate the images displayed in Fig. 3. Namely, here we\ntake d(I) to be the mean over each disjoint block of 2 \u00d7 2 pixels, and take u to be the operator that\nremoves the mean from each 2 \u00d7 2 block. Since u has rank 3d2/4, in this section, we write h in an\northonormal basis of the range of u, then the (linear) mapping from I to (l, h) is unitary. We now\nbuild a probability density p on Rd2 by\np(I) = q0(l, h)q1(l) = q0(d(I), h(I))q1(d(I));\nin a moment we will carefully de\ufb01ne the functions qi. For now, suppose that qi \u22650,\nR\nq1(l) dl = 1,\nand for each \ufb01xed l,\nR\nq0(l, h) dh = 1. Then we can check that p has unit integral:\nZ\np dI =\nZ\nq0(d(I), h(I))q1(d(I))dI =\nZ Z\nq0(l, h)q1(l) dl dh = 1.\nNow we de\ufb01ne the qi with Parzen window approximations to the densities of each of the scales.\nFor q1, we take a set of training samples l1, ...., lN0, and construct the density function q1(l) \u223c\nPN1\ni=1 e||l\u2212li||2/\u03c31. We \ufb01x l = d(I) to de\ufb01ne q0(I) = q0(l, h) \u223cPN0\ni=1 e||h\u2212hi||2/\u03c30.For pyramids\nwith more levels, we continue in the same way for each of the \ufb01ner scales. Note we always use the\ntrue low pass at each scale, and measure the true high pass against the high pass samples generated\nfrom the model. Thus for a pyramid with K levels, the \ufb01nal log likelihood will be: log(qK(lK)) +\nPK\u22121\nk=0 log(qk(lk, hk)).\n9\nReferences\n[1] P. J. Burt, Edward, and E. H. Adelson. The laplacian pyramid as a compact image code. IEEE Transac-\ntions on Communications, 31:532\u2013540, 1983.\n[2] J. S. De Bonet. Multiresolution sampling procedure for analysis and synthesis of texture images. In\nProceedings of the 24th annual conference on Computer graphics and interactive techniques, pages 361\u2013\n368. ACM Press/Addison-Wesley Publishing Co., 1997.\n[3] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In CVPR, pages 248\u2013255. IEEE, 2009.\n[4] E. Denton, S. Chintala, A. Szlam, and R. Fergus. Deep generative image models using a laplacian pyramid\nof adversarial networks: Supplementary material. http://soumith.ch/eyescream.\n[5] A. Dosovitskiy, J. T. Springenberg, and T. Brox. Learning to generate chairs with convolutional neural\nnetworks. arXiv preprint arXiv:1411.5928, 2014.\n[6] A. A. Efros and T. K. Leung. Texture synthesis by non-parametric sampling. In ICCV, volume 2, pages\n1033\u20131038. IEEE, 1999.\n[7] S. A. Eslami, N. Heess, C. K. Williams, and J. Winn. The shape boltzmann machine: a strong model of\nobject shape. International Journal of Computer Vision, 107(2):155\u2013176, 2014.\n[8] W. T. Freeman, T. R. Jones, and E. C. Pasztor. Example-based super-resolution. Computer Graphics and\nApplications, IEEE, 22(2):56\u201365, 2002.\n[9] J. Gauthier. Conditional generative adversarial nets for convolutional face generation. Class Project for\nStanford CS231N: Convolutional Neural Networks for Visual Recognition, Winter semester 2014 2014.\n[10] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Ben-\ngio. Generative adversarial nets. In NIPS, pages 2672\u20132680. 2014.\n[11] K. Gregor, I. Danihelka, A. Graves, and D. Wierstra. DRAW: A recurrent neural network for image\ngeneration. CoRR, abs/1502.04623, 2015.\n[12] J. Hays and A. A. Efros. Scene completion using millions of photographs. ACM Transactions on Graphics\n(TOG), 26(3):4, 2007.\n[13] G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science,\n313(5786):504\u2013507, 2006.\n[14] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal\ncovariate shift. arXiv preprint arXiv:1502.03167v3, 2015.\n[15] D. P. Kingma and M. Welling. Auto-encoding variational bayes. ICLR, 2014.\n[16] A. Krizhevsky, G. E. Hinton, et al. Factored 3-way restricted boltzmann machines for modeling natural\nimages. In AISTATS, pages 621\u2013628, 2010.\n[17] M. Mirza and S. Osindero. Conditional generative adversarial nets. CoRR, abs/1411.1784, 2014.\n[18] B. A. Olshausen and D. J. Field. Sparse coding with an overcomplete basis set: A strategy employed by\nv1? Vision research, 37(23):3311\u20133325, 1997.\n[19] S. Osindero and G. E. Hinton. Modeling image patches with a directed hierarchy of markov random\n\ufb01elds. In J. Platt, D. Koller, Y. Singer, and S. Roweis, editors, NIPS, pages 1121\u20131128. 2008.\n[20] J. Portilla and E. P. Simoncelli. A parametric texture model based on joint statistics of complex wavelet\ncoef\ufb01cients. International Journal of Computer Vision, 40(1):49\u201370, 2000.\n[21] M. Ranzato, V. Mnih, J. M. Susskind, and G. E. Hinton. Modeling natural images using gated MRFs.\nIEEE Transactions on Pattern Analysis & Machine Intelligence, (9):2206\u20132222, 2013.\n[22] D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and variational inference in\ndeep latent gaussian models. arXiv preprint arXiv:1401.4082, 2014.\n[23] S. Roth and M. J. Black. Fields of experts: A framework for learning image priors. In In CVPR, pages\n860\u2013867, 2005.\n[24] R. Salakhutdinov and G. E. Hinton. Deep boltzmann machines. In AISTATS, pages 448\u2013455, 2009.\n[25] E. P. Simoncelli, W. T. Freeman, E. H. Adelson, and D. J. Heeger.\nShiftable multiscale transforms.\nInformation Theory, IEEE Transactions on, 38(2):587\u2013607, 1992.\n[26] J. Sohl-Dickstein, E. A. Weiss, N. Maheswaranathan, and S. Ganguli. Deep unsupervised learning using\nnonequilibrium thermodynamics. CoRR, abs/1503.03585, 2015.\n[27] L. Theis and M. Bethge. Generative image modeling using spatial LSTMs. Dec 2015.\n[28] P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol. Extracting and composing robust features with\ndenoising autoencoders. In ICML, pages 1096\u20131103, 2008.\n[29] J. Wright, Y. Ma, J. Mairal, G. Sapiro, T. S. Huang, and S. Yan. Sparse representation for computer vision\nand pattern recognition. Proceedings of the IEEE, 98(6):1031\u20131044, 2010.\n[30] Y. Zhang, F. Yu, S. Song, P. Xu, A. Seff, and J. Xiao. Large-scale scene understanding challenge. In\nCVPR Workshop, 2015.\n[31] S. C. Zhu, Y. Wu, and D. Mumford. Filters, random \ufb01elds and maximum entropy (frame): Towards a\nuni\ufb01ed theory for texture modeling. International Journal of Computer Vision, 27(2):107\u2013126, 1998.\n[32] D. Zoran and Y. Weiss. From learning models of natural image patches to whole image restoration. In\nICCV, 2011.\n10\n",
        "sentence": " Denton et al. (2015) employ a multi-scale approach, gradually generating higher resolution images. Denton et al. (2015) employ a multi-scale approach, gradually generating higher resolution images. Radford et al. (2015) make use of a convolutional-deconvolutional architecture and batch normalization.",
        "context": "we exploit the multi-scale structure of natural images, building a series of generative models, each\nof which captures image structure at a particular scale of a Laplacian pyramid [1]. This strategy\ninto plausible successive re\ufb01nements. We note that many other signal modalities have a multiscale\nstructure that may bene\ufb01t from a similar approach.\n6\nCC-LAPGAN: Airplane \nCC-LAPGAN: Automobile \nCC-LAPGAN: Bird \nCC-LAPGAN: Cat \nCC-LAPGAN: Deer\n[4] E. Denton, S. Chintala, A. Szlam, and R. Fergus. Deep generative image models using a laplacian pyramid\nof adversarial networks: Supplementary material. http://soumith.ch/eyescream."
    },
    {
        "title": "Discriminative unsupervised feature learning with exemplar convolutional neural networks",
        "author": [
            "A. Dosovitskiy",
            "P. Fischer",
            "J.T. Springenberg",
            "M. Riedmiller",
            "T. Brox"
        ],
        "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
        "citeRegEx": "Dosovitskiy et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "Dosovitskiy et al\\.",
        "year": 2015,
        "abstract": "Deep convolutional networks have proven to be very successful in learning\ntask specific features that allow for unprecedented performance on various\ncomputer vision tasks. Training of such networks follows mostly the supervised\nlearning paradigm, where sufficiently many input-output pairs are required for\ntraining. Acquisition of large training sets is one of the key challenges, when\napproaching a new task. In this paper, we aim for generic feature learning and\npresent an approach for training a convolutional network using only unlabeled\ndata. To this end, we train the network to discriminate between a set of\nsurrogate classes. Each surrogate class is formed by applying a variety of\ntransformations to a randomly sampled 'seed' image patch. In contrast to\nsupervised network training, the resulting feature representation is not class\nspecific. It rather provides robustness to the transformations that have been\napplied during training. This generic feature representation allows for\nclassification results that outperform the state of the art for unsupervised\nlearning on several popular datasets (STL-10, CIFAR-10, Caltech-101,\nCaltech-256). While such generic features cannot compete with class specific\nfeatures from supervised training on a classification task, we show that they\nare advantageous on geometric matching problems, where they also outperform the\nSIFT descriptor.",
        "full_text": "1\nDiscriminative Unsupervised Feature Learning\nwith Exemplar Convolutional Neural Networks\nAlexey Dosovitskiy, Philipp Fischer, Jost Tobias Springenberg, Martin Riedmiller, Thomas Brox\nAbstract\u2014Deep convolutional networks have proven to be very successful in learning task speci\ufb01c features that allow for\nunprecedented performance on various computer vision tasks. Training of such networks follows mostly the supervised learning\nparadigm, where suf\ufb01ciently many input-output pairs are required for training. Acquisition of large training sets is one of the key\nchallenges, when approaching a new task. In this paper, we aim for generic feature learning and present an approach for training a\nconvolutional network using only unlabeled data. To this end, we train the network to discriminate between a set of surrogate classes.\nEach surrogate class is formed by applying a variety of transformations to a randomly sampled \u2019seed\u2019 image patch. In contrast to\nsupervised network training, the resulting feature representation is not class speci\ufb01c. It rather provides robustness to the\ntransformations that have been applied during training. This generic feature representation allows for classi\ufb01cation results that\noutperform the state of the art for unsupervised learning on several popular datasets (STL-10, CIFAR-10, Caltech-101, Caltech-256).\nWhile such generic features cannot compete with class speci\ufb01c features from supervised training on a classi\ufb01cation task, we show that\nthey are advantageous on geometric matching problems, where they also outperform the SIFT descriptor.\n!\n1\nINTRODUCTION\nIn the recent two years Convolutional Neural Networks\n(CNNs) trained in a supervised manner via backpropaga-\ntion dramatically improved the state of the art performance\non a variety of Computer Vision tasks, such as image\nclassi\ufb01cation [1, 2, 3, 4], detection [5, 6], semantic seg-\nmentation [7, 8]. Interestingly, the features learned by such\nnetworks often generalize to new datasets: for example, the\nfeature representation of a network trained for classi\ufb01ca-\ntion on ImageNet [9] also performs well on PASCAL VOC\n[10]. Moreover, a network can be adapted to a new task\nby replacing the loss function and possibly the last few\nlayers of the network and \ufb01ne-tuning it to the new problem,\ni.e. adjusting the weights using backpropagation. With this\napproach, typically much smaller training sets are suf\ufb01cient.\nDespite the big success of this approach, it has at least\ntwo potential drawbacks. First, there is the need for huge\nlabeled datasets to be used for the initial supervised train-\ning. These are dif\ufb01cult to collect, and there are diminishing\nreturns of making the dataset larger and larger. Hence,\nunsupervised feature learning, which has quick access to\narbitrary amounts of data, is conceptually of large interest\ndespite its limited performance so far. Second, although the\nCNNs trained for classi\ufb01cation generalize well to similar\ntasks, such as object class detection, semantic segmentation,\nor image retrieval, the transfer becomes less ef\ufb01cient the\nmore the new task differs from the original training task.\nIn particular, object class annotation may not be bene\ufb01cial to\nlearn features for class-independent tasks, such as descriptor\nmatching.\nIn this work, we propose a procedure for training a\nCNN that does not rely on any labeled data but rather\nmakes use of a surrogate task automatically generated from\n\u2022\nAll authors are with the Computer Science Department\nat the University of Freiburg\nE-mail: {dosovits, \ufb01scher, springj, riedmiller, brox}@cs.uni-freiburg.de\nunlabeled images. The surrogate task is designed to yield\ngeneric features that are descriptive and robust to typical\nvariations in the data. The variation is simulated by ran-\ndomly applying transformations to a \u2019seed\u2019 image. This\nimage and its transformed versions constitute a surrogate\nclass. In contrast to previous data augmentation approaches,\nonly a single seeding sample is needed to build such a class.\nConsequently, we call thus trained networks Exemplar-CNN.\nBy construction, the representation learned by the\nExemplar-CNN is discriminative, while also invariant to\nsome typical transformations. These properties make it\nuseful for various vision tasks. We show that the feature\nrepresentation learned by the Exemplar-CNN performs well\non two very different tasks: object classi\ufb01cation and descrip-\ntor matching. The classi\ufb01cation accuracy obtained with the\nExemplar-CNN representation exceeds that of all previous\nunsupervised methods on four benchmark datasets: STL-10,\nCIFAR-10, Caltech-101, Caltech-256. On descriptor match-\ning, we show that the feature representation outperforms\nthe representation of the AlexNet [1], which was trained in\na supervised, class-speci\ufb01c manner on ImageNet. Moreover,\nit outperforms the popular SIFT descriptor.\n1.1\nRelated Work\nOur approach is related to a large body of work on un-\nsupervised learning of invariant features and training of\nconvolutional neural networks.\nConvolutional training is commonly used in both super-\nvised and unsupervised methods to utilize the invariance of\nimage statistics to translations [1, 11, 12]. Similar to our ap-\nproach, most successful methods employing convolutional\nneural networks for object recognition rely on data aug-\nmentation to generate additional training samples for their\nclassi\ufb01cation objective [1, 2]. While we share the architecture\n(a convolutional neural network) with these approaches, our\nmethod does not rely on any labeled training data.\narXiv:1406.6909v2  [cs.LG]  19 Jun 2015\n2\nFig. 1. Exemplary patches sampled from the STL unlabeled\ndataset which are later augmented by various transformations to\nobtain surrogate data for the CNN training.\nFig. 2. Several random transformations applied to one of the\npatches extracted from the STL unlabeled dataset. The original\n(\u2019seed\u2019) patch is in the top left corner.\nIn unsupervised learning, several studies on learning in-\nvariant representations exist. Denoising autoencoders [13],\nfor example, learn features that are robust to noise by\ntrying to reconstruct data from randomly perturbed input\nsamples. Zou et al. [14] learn invariant features from video\nby enforcing a temporal slowness constraint on the feature\nrepresentation learned by a linear autoencoder. Sohn et al.\n[15] and Hui et al. [16] learn features invariant to local\nimage transformations. In contrast to our discriminative\napproach, all these methods rely on directly modeling the\ninput distribution and are typically hard to use for jointly\ntraining multiple layers of a CNN.\nThe idea of learning features that are invariant to trans-\nformations has also been explored for supervised training of\nneural networks. The research most similar to ours is early\nwork on tangent propagation [17] (and the related double\nbackpropagation [18]) which aims to learn invariance to\nsmall prede\ufb01ned transformations in a neural network by\ndirectly penalizing the derivative of the output with respect\nto the magnitude of the transformations. In contrast, our\nalgorithm does not regularize the derivative explicitly. Thus\nit is less sensitive to the magnitude of the applied transfor-\nmation.\nThis work is also loosely related to the use of unlabeled\ndata for regularizing supervised algorithms, for example\nself-training [19] or entropy regularization [20]. In contrast\nto these semi-supervised methods, Exemplar-CNN training\ndoes not require any labeled data.\nFinally, the idea of creating an auxiliary task in order to\nlearn a good data representation was used in [21, 22].\n2\nCREATING SURROGATE TRAINING DATA\nThe input to the proposed training procedure is a set of\nunlabeled images, which come from roughly the same dis-\ntribution as the images in which we later aim to compute\nthe learned features. We randomly sample N patches of size\n32 \u00d7 32 pixels from different images at varying positions\nand scales forming the initial training set X = {x1, . . . xN}.\nWe are interested in patches containing objects or parts\nof objects, hence we sample only from regions containing\nconsiderable gradients. More precisely, we sample a patch\nwith probability proportional to mean squared gradient\nmagnitude within the patch. Exemplary patches sampled\nfrom the STL-10 unlabeled dataset are shown in Fig. 1.\nWe de\ufb01ne a family of transformations {T\u03b1| \u03b1 \u2208A}\nparameterized by vectors \u03b1 \u2208A, where A is the set of all\npossible parameter vectors. Each transformation T\u03b1 is a com-\nposition of elementary transformations. To learn features for\nthe purpose of object classi\ufb01cation, we used transformations\nfrom the following list:\n\u2022 translation: vertical and horizontal translation by a\ndistance within 0.2 of the patch size;\n\u2022 scaling: multiplication of the patch scale by a factor\nbetween 0.7 and 1.4;\n\u2022 rotation: rotation of the image by an angle up to 20\ndegrees;\n\u2022 contrast 1: multiply the projection of each patch pixel\nonto the principal components of the set of all pixels by\na factor between 0.5 and 2 (factors are independent for\neach principal component and the same for all pixels\nwithin a patch);\n\u2022 contrast 2: raise saturation and value (S and V compo-\nnents of the HSV color representation) of all pixels to a\npower between 0.25 and 4 (same for all pixels within\na patch), multiply these values by a factor between 0.7\nand 1.4, add to them a value between \u22120.1 and 0.1;\n\u2022 color: add a value between \u22120.1 and 0.1 to the hue\n(H component of the HSV color representation) of all\npixels in the patch (the same value is used for all pixels\nwithin a patch).\nThe approach is \ufb02exible with regard to extending this list by\nother transformations in order to serve other applications\nof the learned features better. For instance, in Section 5 we\nshow that descriptor matching bene\ufb01ts from adding a blur\ntransformation.\nAll numerical parameters of elementary transformations,\nwhen concatenated together, form a single parameter vec-\ntor \u03b1. For each initial patch xi \u2208X we sample K ran-\ndom parameter vectors {\u03b11\ni , . . . , \u03b1K\ni } and apply the cor-\nresponding transformations Ti = {T\u03b11\ni , . . . , T\u03b1K\ni } to the\npatch xi. This yields the set of its transformed versions\nSxi = Tixi = {Txi| T \u2208Ti}. An example of such a set is\nshown in Fig. 2 . Afterwards we subtract the mean of each\npixel over the whole resulting dataset. We do not apply any\nother preprocessing.\n3\nLEARNING ALGORITHM\nGiven the sets of transformed image patches, we declare\neach of these sets to be a class by assigning label i to the\nclass Sxi. We train a CNN to discriminate between these\nsurrogate classes. Formally, we minimize the following loss\nfunction:\nL(X) =\nX\nxi\u2208X\nX\nT \u2208Ti\nl(i, Txi),\n(1)\nwhere l(i, Txi) is the loss on the transformed sample Txi\nwith (surrogate) true label i. We use a CNN with a fully\n3\nconnected classi\ufb01cation layer and a softmax output layer\nand we optimize the multinomial negative log likelihood of\nthe network output, hence in our case\nl(i, Txi) = M(ei, f(Txi)),\nM(y, f) = \u2212\u27e8y, log f\u27e9= \u2212\nX\nk\nyk log fk,\n(2)\nwhere f(\u00b7) denotes the function computing the values of\nthe output layer of the CNN given the input data, and ei is\nthe ith standard basis vector. We note that in the limit of an\nin\ufb01nite number of transformations per surrogate class, the\nobjective function (1) takes the form\nbL(X) =\nX\nxi\u2208X\nE\u03b1[l(i, T\u03b1xi)],\n(3)\nwhich we shall analyze in the next section.\nIntuitively, the classi\ufb01cation problem described above\nserves to ensure that different input samples can be dis-\ntinguished. At the same time, it enforces invariance to\nthe speci\ufb01ed transformations. In the following sections we\nprovide a foundation for this intuition. We \ufb01rst present a\nformal analysis of the objective, separating it into a well de-\n\ufb01ned classi\ufb01cation problem and a regularizer that enforces\ninvariance (resembling the analysis in [23]). We then discuss\nthe derived properties of this classi\ufb01cation problem and\ncompare it to common practices for unsupervised feature\nlearning.\n3.1\nFormal Analysis\nWe denote by \u03b1 \u2208A the random vector of transformation\nparameters, by g(x) the vector of activations of the second-\nto-last layer of the network when presented the input patch\nx, by W the matrix of the weights of the last network layer,\nby h(x) = Wg(x) the last layer activations before applying\nthe softmax, and by f(x) = softmax (h(x)) the output of\nthe network. By plugging in the de\ufb01nition of the softmax\nactivation function\nsoftmax (z) = exp(z)/\u2225exp(z)\u22251\n(4)\nthe objective function (3) with loss (2) takes the form\nX\nxi\u2208X\nE\u03b1\n\u0002\u2212\u27e8ei, h(T\u03b1xi)\u27e9+ log \u2225exp(h(T\u03b1xi))\u22251\n\u0003.\n(5)\nWith bgi = E\u03b1 [g(T\u03b1xi)] being the average feature represen-\ntation of transformed versions of the image patch xi we can\nrewrite Eq. (5) as\nX\nxi\u2208X\n\u0002\u2212\u27e8ei, W bgi\u27e9+ log \u2225exp(W bgi)\u22251\n\u0003\n+\nX\nxi\u2208X\n\u0002\nE\u03b1 [log \u2225exp(h(T\u03b1xi))\u22251] \u2212log \u2225exp(W bgi)\u22251\n\u0003.\n(6)\nThe \ufb01rst sum is the objective function of a multinomial\nlogistic regression problem with input-target pairs ( bgi, ei).\nThis objective falls back to the transformation-free instance\nclassi\ufb01cation problem L(X) = P\nxi\u2208X l(i, xi) if g(xi) =\nE\u03b1[g(T\u03b1x)]. In general, this equality does not hold and thus\nthe \ufb01rst sum enforces correct classi\ufb01cation of the average\nrepresentation E\u03b1[g(T\u03b1xi)] for a given input sample. For\na truly invariant representation, however, the equality is\nachieved. Similarly, if we suppose that T\u03b1x = x for \u03b1 = 0,\nthat for small values of \u03b1 the feature representation g(T\u03b1xi)\nis approximately linear with respect to \u03b1 and that the\nrandom variable \u03b1 is centered, i.e. E\u03b1 [\u03b1] = 0, then bgi =\nE\u03b1 [g(T\u03b1xi)] \u2248E\u03b1 [g(xi) + \u2207\u03b1(g(T\u03b1xi))|\u03b1=0 \u03b1] = g(xi).\nThe second sum in Eq. (6) can be seen as a regularizer\nenforcing all h(T\u03b1xi) to be close to their average value, i.e.,\nthe feature representation is sought to be approximately\ninvariant to the transformations T\u03b1. To show this we use\nthe convexity of the function log \u2225exp(\u00b7)\u22251 and Jensen\u2019s\ninequality, which yields (proof in Appendix A):\nE\u03b1 [log \u2225exp(h(T\u03b1xi))\u22251] \u2212log \u2225exp(W bgi)\u22251 \u22650.\n(7)\nIf the feature representation is perfectly invariant, then\nh(T\u03b1xi) = W bgi and inequality (7) turns to equality, mean-\ning that the regularizer reaches its global minimum.\n3.2\nConceptual Comparison to Previous Unsupervised\nLearning Methods\nSuppose we want to unsupervisedly learn a feature rep-\nresentation useful for a recognition task, for example clas-\nsi\ufb01cation. The mapping from input images x to a feature\nrepresentation g(x) should then satisfy two requirements:\n(1) there must be at least one feature that is similar for\nimages of the same category y (invariance); (2) there must\nbe at least one feature that is suf\ufb01ciently different for images\nof different categories (ability to discriminate).\nMost unsupervised feature learning methods aim to\nlearn such a representation by modeling the input distribu-\ntion p(x). This is based on the assumption that a good model\nof p(x) contains information about the category distribution\np(y|x). That is, if a representation is learned, from which\na given sample can be reconstructed perfectly, then the\nrepresentation is expected to also encode information about\nthe category of the sample (ability to discriminate). Addi-\ntionally, the learned representation should be invariant to\nvariations in the samples that are irrelevant for the classi\ufb01-\ncation task, i.e., it should adhere to the manifold hypothesis\n(see e.g. [24] for a recent discussion). Invariance is classically\nachieved by regularization of the latent representation, e.g.,\nby enforcing sparsity [12] or robustness to noise [13].\nIn contrast, the discriminative objective in Eq. (1) does\nnot directly model the input distribution p(x) but learns\na representation that discriminates between input samples.\nThe representation is not required to reconstruct the input,\nwhich is unnecessary in a recognition or matching task.\nThis leaves more degrees of freedom to model the desired\nvariability of a sample. As shown in our analysis (see Eq.\n(7)), we enforce invariance to transformations applied dur-\ning surrogate data creation by requiring the representation\ng(T\u03b1xi) of the transformed image patch to be predictive of\nthe surrogate label assigned to the original image patch xi.\nIt should be noted that this approach assumes that the\ntransformations T\u03b1 do not change the identity of the image\ncontent. For example, if we use a color transformation we\nwill force the network to be invariant to this change and\ncannot expect the extracted features to perform well in a task\n4\nrelying on color information (such as differentiating black\npanthers from pumas)1.\n4\nEXPERIMENTS: CLASSIFICATION\nTo compare our discriminative approach to previous unsu-\npervised feature learning methods, we report classi\ufb01cation\nresults on the STL-10 [25], CIFAR-10 [26], Caltech-101 [27]\nand Caltech-256 [28] datasets.\n4.1\nExperimental Setup\nThe datasets we tested on differ in the number of classes (10\nfor CIFAR and STL, 101 for Caltech-101, 256 for Caltech-\n256) and the number of samples per class. STL is especially\nwell suited for unsupervised learning as it contains a large\nset of 100,000 unlabeled samples. In all experiments, except\nfor the dataset transfer experiment, we extracted surrogate\ntraining data from the unlabeled subset of STL-10. When\ntesting on CIFAR-10, we resized the images from 32 \u00d7 32\npixels to 64 \u00d7 64 pixels to make the scale of depicted ob-\njects more similar to the other datasets. Caltech-101 images\nwere resized to 150 \u00d7 150 pixels and Caltech-256 images to\n256\u00d7256 pixels (Caltech-256 images have on average higher\nresolution than Caltech-101 images, so not downsampling\nthem so much allows to preserve more \ufb01ne details).\nWe worked with three network architectures. A smaller\nnetwork was used to evaluate the in\ufb02uence of different\ncomponents of the augmentation procedure on classi\ufb01cation\nperformance. It consists of two convolutional layers with 64\n\ufb01lters each, followed by a fully connected layer with 128\nunits. This last layer is succeeded by a softmax layer, which\nserves as the network output. This network will be referred\nto as 64c5-64c5-128f as explained in Appendix B.1.\nTo compare our method to the state-of-the-art we trained\ntwo bigger networks: a network that consists of three con-\nvolutional layers with 64, 128 and 256 \ufb01lters respectively\nfollowed by a fully connected layer with 512 units (64c5-\n128c5-256c5-512f), and an even larger network, consisting\nof three convolutional layers with 92, 256 and 512 \ufb01lters\nrespectively and a fully connected layer with 1024 units\n(92c5-256c5-512c5-1024f).\nIn all these models all convolutional \ufb01lters are connected\nto a 5 \u00d7 5 region of their input. 2 \u00d7 2 max-pooling was\nperformed after the \ufb01rst and second convolutional layers.\nDropout [29, 30] was applied to the fully connected layers.\nWe trained the networks using an implementation based on\nCaffe [31]. Details on the training procedure and hyperpa-\nrameter settings are provided in Appendix B.2.\nAt test time we applied a network to arbitrarily sized\nimages by convolutionally computing the responses of all\nthe network layers except the top softmax (that is, we\ncomputed the responses of convolutional layers normally\nand then slided the fully connected layers on top of these).\nTo the feature maps of each layer we applied the pooling\nmethod that is commonly used for the respective dataset:\n1. Such cases could be covered either by careful selection of applied\ntransformations or by combining features from multiple networks\ntrained with different sets of transformations and letting the \ufb01nal\n(supervised) classi\ufb01er choose which features to use.\n1) 4-quadrant max-pooling, resulting in 4 values per fea-\nture map, which is the standard procedure for STL-10\nand CIFAR-10 [14, 16, 32, 34]\n2) 3-layer spatial pyramid, i.e. max-pooling over the\nwhole image as well as within 4 quadrants and within\nthe cells of a 4\u00d74 grid, resulting in 1+4+16 = 21 values\nper feature map, which is the standard for Caltech-101\nand Caltech-256 [14, 33, 35]\nFinally, we trained a one-vs-all linear support vector ma-\nchine (SVM) on the pooled features.\nOn all datasets we used the standard training and test\nprotocols. On STL-10 the SVM was trained on 10 pre-de\ufb01ned\nfolds of the training data. We report the mean and standard\ndeviation achieved on the \ufb01xed test set. For CIFAR-10 we\nreport two results:\n1) Training the SVM on the whole CIFAR-10 training set\n(called CIFAR-10)\n2) The average over 10 random selections of 400 training\nsamples per class (called CIFAR-10(400))\nFor Caltech-101 we follow the usual protocol of selecting 30\nrandom samples per class for training and not more than 50\nsamples per class for testing. For Caltech-256 we randomly\nselected 30 samples per class for training and used the\nrest for testing. Both for Caltech-101 and Caltech-256 we\nrepeated the testing procedure 10 times.\n4.2\nClassi\ufb01cation Results\nIn Table 1 we compare Exemplar-CNN to several unsu-\npervised feature learning methods, including the current\nstate of the art on each dataset. We also list the state of\nthe art for methods involving supervised feature learning\n(which is not directly comparable). Additionally we show\nthe dimensionality of the feature vectors produced by each\nmethod before \ufb01nal pooling. The smallest network was\ntrained on 8000 surrogate classes containing 150 samples\neach and the larger ones on 16000 classes with 100 samples\neach.\nThe features extracted from both larger networks out-\nperform the best prior result on all datasets. This is despite\nthe fact that the dimensionality of the feature vectors is\nsmaller than that of most other approaches and that the\nnetworks are trained on the STL-10 unlabeled dataset (i.e.\nthey are used in a transfer learning manner when applied\nto CIFAR-10 and Caltech). The increase in performance\nis especially pronounced when only few labeled samples\nare available for training the SVM, as is the case for all\nthe datasets except full CIFAR-10. This is in agreement\nwith previous evidence that with increasing feature vector\ndimensionality and number of labeled samples, training an\nSVM becomes less dependent on the quality of the features\n[16, 32]. Remarkably, on STL-10 we achieve an accuracy of\n74.2%, which is a large improvement over all previously\nreported results.\n1. On Caltech-101 one can either measure average accuracy over\nall samples (average overall accuracy) or calculate the accuracy for\neach class and then average these values (average per-class accuracy).\nThese differ, as some classes contain fewer than 50 test samples. Most\nresearchers in ML use average overall accuracy.\n5\nTABLE 1\nClassi\ufb01cation accuracies on several datasets (in percent). \u2217Average per-class accuracy1 78.0% \u00b1 0.4%. \u2020 Average per-class\naccuracy 85.0% \u00b1 0.7%. \u2021 Average per-class accuracy 85.8% \u00b1 0.7%.\nAlgorithm\nSTL-10\nCIFAR-10(400) CIFAR-10 Caltech-101 Caltech-256(30) #features\nConvolutional K-means Network [32]\n60.1 \u00b1 1\n70.7 \u00b1 0.7\n82.0\n\u2014\n\u2014\n8000\nMulti-way local pooling [33]\n\u2014\n\u2014\n\u2014\n77.3 \u00b1 0.6\n41.7\n1024 \u00d7 64\nSlowness on videos [14]\n61.0\n\u2014\n\u2014\n74.6\n\u2014\n556\nHierarchical Matching Pursuit (HMP) [34]\n64.5 \u00b1 1\n\u2014\n\u2014\n\u2014\n\u2014\n1000\nMultipath HMP [35]\n\u2014\n\u2014\n\u2014\n82.5 \u00b1 0.5\n50.7\n5000\nView-Invariant K-means [16]\n63.7\n72.6 \u00b1 0.7\n81.9\n\u2014\n\u2014\n6400\nExemplar-CNN (64c5-64c5-128f)\n67.1 \u00b1 0.2\n69.7 \u00b1 0.3\n76.5\n79.8 \u00b1 0.5\u2217\n42.4 \u00b1 0.3\n256\nExemplar-CNN (64c5-128c5-256c5-512f)\n72.8 \u00b1 0.4\n75.4 \u00b1 0.2\n82.2\n86.1 \u00b1 0.5\u2020\n51.2 \u00b1 0.2\n960\nExemplar-CNN (92c5-256c5-512c5-1024f)\n74.2 \u00b1 0.4\n76.6 \u00b1 0.2\n84.3\n87.1 \u00b1 0.7\u2021\n53.6 \u00b1 0.2\n1884\nSupervised state of the art\n70.1[36]\n\u2014\n92.0 [37]\n91.44 [38]\n70.6 [2]\n\u2014\n4.3\nDetailed Analysis\nWe performed additional experiments using the 64c5-64c5-\n128f network to study the effect of various design choices in\nExemplar-CNN training and validate the invariance proper-\nties of the learned features.\n4.3.1\nNumber of Surrogate Classes\nWe varied the number N of surrogate classes between 50\nand 32000. As a sanity check, we also tried classi\ufb01cation\nwith random \ufb01lters. The results are shown in Fig. 3.\nClearly, the classi\ufb01cation accuracy increases with the\nnumber of surrogate classes until it reaches an optimum at\nabout 8000 surrogate classes after which it did not change or\neven decreased. This is to be expected: the larger the number\nof surrogate classes, the more likely it is to draw very similar\nor even identical samples, which are hard or impossible\nto discriminate. Few such cases are not detrimental to the\nclassi\ufb01cation performance, but as soon as such collisions\ndominate the set of surrogate labels, the discriminative loss\nis no longer reasonable and training the network to the\nsurrogate task no longer succeeds. To check the validity\nof this explanation we also plot in Fig. 3 the validation\nerror on the surrogate data after training the network. It\nrapidly grows as the number of surrogate classes increases,\nshowing that the surrogate classi\ufb01cation task gets harder\nwith a growing number of classes. We observed that larger,\nmore powerful networks reach their peak performance for\nmore surrogate classes than smaller networks. However,\nthe performance that can be achieved with larger networks\nsaturates (not shown in the \ufb01gure).\nIt can be seen as a limitation that sampling too many,\ntoo similar images for training can even decrease the per-\nformance of the learned features. It makes the number and\nselection of samples a relevant parameter of the training\nprocedure. However, this drawback can be avoided for\nexample by clustering.\nTo demonstrate this, given the STL-10 unlabeled dataset\ncontaining 100,000 images, we \ufb01rst train a 64c5-128c5-256c5-\n512f Exemplar-CNN on a subset of 16,000 image patches.\nWe then use this Exemplar-CNN to extract descriptors of\nall images from the dataset and perform clustering similar\nto [39]. After discarding noisy and very similar clusters\nautomatically (see Appendix B.3 for details), this leaves\nus with 6510 clusters with approximately 10 images in\neach of them. To the images in each cluster we then apply\n50\n100\n250\n500\n1000 2000 4000 8000 1600032000\n54\n56\n58\n60\n62\n64\n66\n68\nNumber of classes (log scale)\nClassification accuracy on STL\u221210\n \n \nClassification\non STL (\u00b1 \u03c3)\nValidation error on\nsurrogate data\n0\n20\n40\n60\n80\n100\nError on validation data\nFig. 3. In\ufb02uence of the number of surrogate training classes. The val-\nidation error on the surrogate data is shown in red. Note the different\ny-axes for the two curves.\nthe same augmentation as in the original Exemplar-CNN.\nEach augmented cluster serves as a surrogate class for\ntraining. Table 2 shows the classi\ufb01cation performance of\nthe features learned by CNNs from this training data. Clus-\ntering increases the classi\ufb01cation accuracy on all datasets,\nin particular on STL by up to 2.4%, depending on the\nnetwork. This shows that the small modi\ufb01cation allows the\napproach to make use of large amounts of data. Potentially,\nusing even more data or performing clustering and network\ntraining within a uni\ufb01ed framework could further improve\nthe quality of the learned features.\n4.3.2\nNumber of Samples per Surrogate Class\nFig. 4 shows the classi\ufb01cation accuracy when the number\nK of training samples per surrogate class varies between\n1 and 300. The performance improves with more samples\nper surrogate class and saturates at around 100 samples.\nThis indicates that this amount is suf\ufb01cient to approximate\nthe formal objective from Eq. (3), hence further increasing\nthe number of samples does not signi\ufb01cantly change the\noptimization problem. On the other hand, if the number of\nsamples is too small, there is not enough data to learn the\ndesired invariance properties.\n4.3.3\nTypes of Transformations\nWe varied the transformations used for creating the surro-\ngate data to analyze their in\ufb02uence on the \ufb01nal classi\ufb01cation\n6\nTABLE 2\nClassi\ufb01cation accuracies with clustering (in percent).\nAlgorithm\nSTL-10\nCIFAR-10(400)\nCIFAR-10\nCaltech-101\nCaltech-256(30)\n64c5-64c5-128f\n69.5 \u00b1 0.4\n70.8 \u00b1 0.2\n76.8\n79.5 \u00b1 0.6\n42.9 \u00b1 0.3\n64c5-128c5-256c5-512f\n74.9 \u00b1 0.4\n75.7 \u00b1 0.2\n82.6\n85.7 \u00b1 0.6\n51.4 \u00b1 0.4\n92c5-256c5-512c5-1024f\n75.4 \u00b1 0.3\n77.4 \u00b1 0.2\n84.3\n87.2 \u00b1 0.6\n53.7 \u00b1 0.6\n1\n2\n4\n8\n16\n32\n64 100 150\n300\n45\n50\n55\n60\n65\n70\nNumber of samples per class (log scale)\nClassification accuracy on STL\u221210\n \n \n1000 classes\n2000 classes\n4000 classes\nrandom filters\nFig. 4. Classi\ufb01cation performance on STL for different numbers of sam-\nples per class. Random \ufb01lters can be seen as \u20190 samples per class\u2019.\nperformance. The set of \u2019seed\u2019 patches was \ufb01xed. The result\nis shown in Fig. 5. The value \u20190\u2019 corresponds to applying\nrandom compositions of all elementary transformations:\nscaling, rotation, translation, color variation, and contrast\nvariation. Different columns of the plot show the difference\nin classi\ufb01cation accuracy as we discarded some types of\nelementary transformations.\nSeveral tendencies can be observed. First, rotation and\nscaling have only a minor impact on the performance, while\ntranslations, color variations and contrast variations are\nsigni\ufb01cantly more important. Secondly, the results on STL-\n10 and CIFAR-10 consistently show that spatial invariance\nand color-contrast invariance are approximately of equal\nimportance for the classi\ufb01cation performance. This indicates\nthat variations in color and contrast, though often neglected,\nmay also improve performance in a supervised learning\nscenario. Thirdly, on Caltech-101 color and contrast trans-\nformations are much more important compared to spatial\ntransformations than on the two other datasets. This is not\nsurprising, since Caltech-101 images are often well aligned,\nand this dataset bias makes spatial invariance less useful.\nWe tried applying several other transformations (oc-\nclusion, af\ufb01ne transformation, additive Gaussian noise) in\naddition to the ones shown in Fig. 5, none of which seemed\nto improve the classi\ufb01cation accuracy. For the matching\ntask in Section 5, though, we found that using blur as an\nadditional transformation improves the performance.\n4.3.4\nIn\ufb02uence of the Dataset\nWe applied our feature learning algorithm to images sam-\npled from three datasets \u2013 STL-10 unlabeled dataset, CIFAR-\n10 and Caltech-101 \u2013 and evaluated the performance of the\nlearned feature representations on classi\ufb01cation tasks on\n\u221220\n\u221215\n\u221210\n\u22125\n0\nRemoved transformations\n \n \nrotation scaling   translation color contrast  rot+sc+tr  col+con\nall\n\u221220\n\u221215\n\u221210\n\u22125 \n0  \nDifference in classification accuracy\nSTL\u221210\nCIFAR\u221210\nCaltech\u2212101\nFig. 5. In\ufb02uence of removing groups of transformations during gen-\neration of the surrogate training data. Baseline (\u20190\u2019 value) is applying\nall transformations. Each group of three bars corresponds to removing\nsome of the transformations.\nFig. 7. Filters learned by \ufb01rst layers of 64c5-64c5-128f networks when\ntraining on surrogate data from various dataset. Top \u2013 from STL-10,\nmiddle \u2013 CIFAR-10, bottom \u2013 Caltech-101.\nthese datasets. We used the 64c5-64c5-128f network for this\nexperiment.\nWe show the \ufb01rst layer \ufb01lters learned from the three\ndatasets in Fig. 7. Note how \ufb01lters qualitatively differ de-\npending on the dataset they were trained on.\nClassi\ufb01cation results are shown in Table 3. The best\nclassi\ufb01cation results for each dataset are obtained when\ntraining on the patches extracted from the dataset itself.\nHowever, the difference is not drastic, indicating that the\nlearned features generalize well to other datasets.\n4.3.5\nIn\ufb02uence of the Network Architecture on Classi\ufb01ca-\ntion Performance\nWe perform an additional experiment to evaluate the in-\n\ufb02uence of the network architecture on classi\ufb01cation perfor-\nmance. The results of this experiment are shown in Table 4.\nAll networks were trained using a surrogate training set\ncontaining either 8000 classes with 150 samples each or\n7\n\u221220\n\u221210\n0\n10\n20\n0\n0.2\n0.4\n0.6\n0.8\n1\nTranslation (pixels)\nDistance between feature vectors\n \n \n(a)\n1st layer\n2nd layer\n3rd layer\n4\u2212quadrant\nHOG\n\u221250\n0\n50\n0\n0.2\n0.4\n0.6\n0.8\n1\nRotation angle (degrees)\nDistance between feature vectors\n(b)\n0.06 0.13 0.25\n0.5\n1\n2\n4\n8\n16\n0\n0.2\n0.4\n0.6\n0.8\n1\nSaturation multiplier\nDistance between feature vectors\n(c)\n\u221250\n0\n50\n10\n20\n30\n40\n50\n60\nRotation angle (degrees)\nClassification accuracy (in %)\n \n \n(d)\nNo movements in training data\nRotations up to 20 degrees\nRotations up to 40 degrees\n\u22120.2\n\u22120.1\n0\n0.1\n0.2\n0.3\n10\n20\n30\n40\n50\n60\nHue shift\nClassification accuracy (in %)\n \n \n(e)\nNo color transform\nHue change within \u00b1 0.1\nHue change within \u00b1 0.2\nHue change within \u00b1 0.3\n0.125\n0.25\n0.5\n1\n2\n4\n8\n10\n20\n30\n40\n50\n60\nContrast multiplier\nClassification accuracy (in %)\n \n \n(f)\nNo contrast transform\nContrast coefficients (2, 0.5, 0.1)\nContrast coefficients (4, 1, 0.2)\nContrast coefficients (6, 1.5, 0.3)\nFig. 6. Invariance properties of the feature representation learned by Exemplar-CNN. Top: transformations applied to an image patch (translation,\nrotation, contrast, saturation, color). Bottom: invariance of different feature representations. (a)-(c): Normalized Euclidean distance between feature\nvectors of the original and the translated image patches vs. the magnitude of the transformation, (d)-(f): classi\ufb01cation performance on transformed\nimage patches vs. the magnitude of the transformation for various magnitudes of transformations applied for creating the surrogate training data.\nTABLE 3\nDependence of classi\ufb01cation performance (in %) on the training and\ntesting datasets. Each column corresponds to different test data, each\nrow to different training data (i.e. source of seed patches). We used the\n64c5-64c5-128f network for this experiment.\nTESTING\nTRAINING\nSTL-10\nCIFAR-10(400)\nCALTECH-101\nSTL-10\n67.1 \u00b1 0.3\n69.7 \u00b1 0.3\n79.8 \u00b1 0.5\nCIFAR-10\n64.5 \u00b1 0.4\n70.3 \u00b1 0.4\n77.8 \u00b1 0.6\nCALTECH-101\n66.2 \u00b1 0.4\n69.5 \u00b1 0.2\n80.0 \u00b1 0.5\n16000 classes with 100 samples each (for larger networks).\nWe vary the number of layers, layer sizes and \ufb01lter sizes.\nClassi\ufb01cation accuracy generally improves with the network\nsize indicating that our classi\ufb01cation problem scales well to\nrelatively large networks without over\ufb01tting.\n4.3.6\nInvariance Properties of the Learned Representation\nWe analyzed to which extent the representation learned\nby the network is invariant to the transformations applied\nduring training. We randomly sampled 500 images from\nthe STL-10 test set and applied a range of transformations\n(translation, rotation, contrast, color) to each image. To\navoid empty regions beyond the image boundaries when\napplying spatial transformations, we cropped the central\n64 \u00d7 64 pixel sub-patch from each 96 \u00d7 96 pixel image. We\nthen applied two measures of invariance to these patches.\nFirst, as an explicit measure of invariance, we calculated\nthe normalized Euclidean distance between normalized fea-\nture vectors of the original image patch and the transformed\none [14] (see Appendix C for details). The downside of this\napproach is that the distance between extracted features\ndoes not take into account how informative and discrimi-\nnative they are. We therefore evaluated a second measure\n\u2013 classi\ufb01cation performance depending on the magnitude\nof the transformation applied to the classi\ufb01ed patches\n\u2013\nwhich does not come with this problem. To compute the\nclassi\ufb01cation accuracy, we trained an SVM on the central\n64\u00d764 pixel patches from one fold of the STL-10 training set\nand measured classi\ufb01cation performance on all transformed\nversions of 500 samples from the test set.\nThe results of both experiments are shown in Fig. 6.\nOverall the experiment empirically con\ufb01rms that the\nExemplar-CNN objective leads to learning invariant fea-\ntures. Features in the third layer and the \ufb01nal pooled feature\n8\nTABLE 4\nClassi\ufb01cation accuracy depending on the network architecture. The name coding is as follows: NcF stands for a convolutional layer with N \ufb01lters of\nsize F \u00d7 F pixels, Nf stands for a fully connected layer with N units. For example, 64c5-64c5-128f denotes a network with two convolutional layers\ncontaining 64 \ufb01lters spanning 5 \u00d7 5 pixels each, followed by a fully connected layer with 128 units. We also show the number of surrogate classes\nused for training each network.\nArchitecture\n#classes\nSTL-10\nCIFAR-10(400)\nCIFAR-10\nCaltech-101\n32c5-32c5-64f\n8000\n63.8 \u00b1 0.4\n66.1 \u00b1 0.4\n71.3\n78.2 \u00b1 0.6\n64c5-64c5-128f\n8000\n67.1 \u00b1 0.3\n69.7 \u00b1 0.3\n75.7\n79.8 \u00b1 0.5\n64c7-64c5-128f\n8000\n66.3 \u00b1 0.4\n69.5 \u00b1 0.3\n75.0\n79.4 \u00b1 0.7\n64c5-64c5-64c5-128f\n8000\n68.5 \u00b1 0.3\n70.9 \u00b1 0.3\n77.0\n82.2 \u00b1 0.7\n64c5-64c5-64c5-64c5-128f\n8000\n64.7 \u00b1 0.5\n67.5 \u00b1 0.3\n75.2\n75.7 \u00b1 0.4\n128c5-64c5-128f\n8000\n67.2 \u00b1 0.4\n69.9 \u00b1 0.2\n76.1\n80.1 \u00b1 0.5\n64c5-256c5-128f\n8000\n69.2 \u00b1 0.3\n71.7 \u00b1 0.3\n77.9\n81.6 \u00b1 0.5\n64c5-64c5-512f\n8000\n69.0 \u00b1 0.4\n71.7 \u00b1 0.2\n79.3\n82.9 \u00b1 0.4\n128c5-256c5-512f\n8000\n71.2 \u00b1 0.3\n73.9 \u00b1 0.3\n81.5\n84.3 \u00b1 0.6\n128c5-256c5-512f\n16000\n71.9 \u00b1 0.3\n74.3 \u00b1 0.3\n81.4\n84.6 \u00b1 0.6\n64c5-128c5-256c5-512f\n16000\n72.8 \u00b1 0.4\n75.3 \u00b1 0.3\n82.0\n85.5 \u00b1 0.4\n92c5-256c5-512c5-1024f\n16000\n73.9 \u00b1 0.4\n76.0 \u00b1 0.2\n83.6\n86.9 \u00b1 0.6\nrepresentation compare favorably to a HOG baseline (Fig. 6\n(a), (b)). This is consistent with the results we get in Section 5\nfor descriptor matching, where we compare the features to\nSIFT (which is similar to HOG).\nFig. 6(d)-(f) further show that stronger transformations\nin the surrogate training data lead to a more invariant\nclassi\ufb01cation with respect to these transformations. How-\never, adding too much contrast variation may deteriorate\nclassi\ufb01cation performance (Fig. 6 (f)). One possible reason is\nthat the contrast level can be a useful feature: for example,\nstrong edges in an image are usually more important than\nweak ones.\n5\nEXPERIMENTS: DESCRIPTOR MATCHING\nIn recognition tasks, such as image classi\ufb01cation and object\ndetection, the invariance requirements are largely de\ufb01ned\nby object class labels. Consequently, providing these class\nlabels already when learning the features should be advan-\ntageous. This can be seen in the comparison to the super-\nvised state-of-the-art in Table 1, where supervised feature\nlearning performs better than the presented approach.\nIn contrast, matching of interest points in two images\nshould be independent of object class labels. As a conse-\nquence, there is no apparent reason, why feature learning\nusing class annotation should outperform unsupervised fea-\nture learning. One could even imagine that the class anno-\ntation is confusing and yields inferior features for matching.\n5.1\nCompared Features\nWe compare the features learned by supervised and un-\nsupervised convolutional networks and SIFT [40] features.\nFor a long time SIFT has been the preferred descriptor in\nmatching tasks (see [41] for a comparison).\nAs supervised CNN we used the AlexNet model trained\non ImageNet available at [31]. The architecture of the net-\nwork follows Krizhevsky et al. [1] and contains 5 con-\nvolutional layers followed by 2 fully connected layers. In\nthe experiments, we extract features from one of the 5\nconvolutional layers of the network. For large input patch\nsizes, the output dimensionality is high, especially for lower\nlayers. For the descriptors to be more comparable to SIFT,\nwe decided to max-pool the extracted feature map down to\na \ufb01xed 4 \u00d7 4 spatial size which corresponds to the spatial\nresolution of SIFT pooling. Even though the spatial size is\nthe same, the number of features per cell is larger than for\nSIFT.\nAs unsupervised CNN we evaluated the matching per-\nformance of the 64c5-128c5-256c5-512f architecture, referred\nto as Exemplar-CNN-orig in the following. As the experi-\nments show, neural networks cannot handle blur very well.\nIncreasing image blur always leads to a matching per-\nformance drop. Hence we also trained another Exemplar-\nCNN to deal with this speci\ufb01c problem. First, we increased\nthe \ufb01lter size and introduced a stride of 2 in the \ufb01rst\nconvolutional layer, resulting in the following architecture:\n64c7s2-128c5-256c5-512f. This allows the network to identify\nedges in very blurry images more easily. Secondly, we used\nunlabeled images from Flickr for training, because these\nrepresent the general distribution of natural images better\nthan STL. Thirdly, we applied blur of variable strength to\nthe training data as an additional augmentation. We thus\ncall this network Exemplar-CNN-blur. As with AlexNet, we\nmax-pooled the feature maps produced by the Exemplar-\nCNNs to a 4 \u00d7 4 spatial size.\n5.2\nDatasets\nThe common matching dataset by Mikolajczyk et al. [42]\ncontains only 40 image pairs. This dataset size limits the\nreliability of conclusions drawn from the results, especially\nas we compare various design choices, such as the depth\nof the network layer from which we draw the features.\nWe set up an additional dataset that contains 384 image\npairs. It was generated by applying 6 different types of\ntransformations with varying strengths to 16 base images\nwe obtained from Flickr. These images were not contained\nin the set we used to train the unsupervised CNN.\nTo each base image we applied the geometric transfor-\nmations rotation, zoom, perspective, and nonlinear deformation.\nThese cover rigid and af\ufb01ne transformations as well as\nmore complex ones. Furthermore we applied changes to\nlighting and focus by adding blur. Each transformation was\n9\napplied in various magnitudes such that its effect on the\nperformance could be analyzed in depth. For each of the 16\nbase images we matched all the transformed versions of the\nimage to the original one, which resulted in 384 matching\npairs.\nThe dataset from Mikolajczyk et al. [42] was not gener-\nated synthetically but contains real photos taken from differ-\nent viewpoints or with different camera settings. While this\nre\ufb02ects reality better than a synthetic dataset, it also comes\nwith a drawback: the transformations are directly coupled\nwith the respective images. Hence, attributing performance\nchanges to either different image contents or to the applied\ntransformations becomes impossible. In contrast, the new\ndataset enables us to evaluate the effect of each type of\ntransformation independently of the image content.\n5.3\nPerformance Measure\nTo evaluate the matching performance for a pair of images,\nwe followed the procedure described in [41]. We \ufb01rst ex-\ntracted elliptic regions of interest and corresponding image\npatches from both images using the maximally stable extremal\nregions (MSER) detector [43]. We chose this detector because\nit was shown to perform consistently well in [42] and it\nis widely used. For each detected region we extracted a\npatch according to the region scale and rotated it according\nto its dominant orientation. The descriptors of all extracted\npatches were greedily matched based on the Euclidean dis-\ntance. This yielded a ranking of descriptor pairs. A pair was\nconsidered as a true positive if the ellipse of the descriptor\nin the target image and the ground truth ellipse in the target\nimage had an intersection over union (IOU) of at least 0.5.\nAll other pairs were considered false positives. Assuming\nthat a recall of 1 corresponds to the best achievable overall\nmatching given the detections, we computed a precision-\nrecall curve. The average precision, i.e., the area under this\ncurve, was used as performance measure.\n5.4\nPatch size and network layer\nThe MSER detector returns ellipses of varying sizes, de-\npending on the scale of the detected region. To compute\ndescriptors from these elliptic regions we normalized the\nimage patches to a \ufb01xed size. It is not immediately clear\nwhich patch size is best: larger patches provide a higher\nresolution, but enlarging them too much may introduce\ninterpolation artifacts and the effect of high-frequency noise\nmay be emphasized. Therefore, we optimized the patch size\non the Flickr dataset for each method.\nWhen using convolutional neural networks for region\ndescription, aside from the patch size there is another fun-\ndamental choice \u2013 the network layer from which the features\nare extracted. Features from higher layers are more abstract.\nFig. 8 shows the average performance of each method\nwhen varying the patch size between 69 and 157. We\nchose the maximum patch size value such that most el-\nlipses are smaller than that. We found that in case of SIFT,\nthe performance monotonously grows and saturates at the\nmaximum patch size. SIFT is based on normalized \ufb01nite\ndifferences, and thus very robust to blurred edges caused\nby interpolation. In contrast, for the networks, especially for\ntheir lower layers, there is an optimal patch size, after which\nperformance starts degrading. The lower network layers\ntypically learn Gabor-like \ufb01lters tuned to certain frequen-\ncies. Therefore, they suffer from over-smoothing caused by\ninterpolation. Features from higher layers have access to\nlarger receptive \ufb01elds and, thus, can again bene\ufb01t from\nlarger patch sizes.\nIn the following experiments we used the optimal pa-\nrameters given by Fig. 8: patch size 157 for SIFT and 113 for\nall other methods; layer 4 for AlexNet and Exemplar-CNN-\nblur and layer 3 for Exemplar-CNN-orig.\n5.5\nResults\nFig. 9 shows scatter plots that compare the performance of\npairs of methods in terms of average precision. Each dot\ncorresponds to an image pair. Points above the diagonal\nindicate better performance of the \ufb01rst method, and for\npoints below the diagonal the AP of the second method is\nhigher. The scatter plots also give an intuition of the variance\nin the performance difference.\nFig. 9a,b show that the features from both AlexNet and\nthe Exemplar-CNN outperform SIFT on the Flickr dataset.\nHowever, especially for features from AlexNet there are\nsome image pairs, for which SIFT performs clearly better.\nOn the Mikolayczyk dataset, SIFT even outperforms fea-\ntures from AlexNet. We will analyze this in more detail\nin the next paragraph. Fig. 9c,f compare AlexNet with the\nExemplar-CNN-blur and show that the loss function based\non surrogate classes is superior to the loss function based\non object class labels. In contrast to object classi\ufb01cation,\nclass-speci\ufb01c features are not advantageous for descriptor\nmatching. A loss function that focuses on the invariance\nproperties required for descriptor matching yields better\nresults.\nIn Fig. 10 and 11 we analyze the reason for the clearly\ninferior performance of AlexNet on some image pairs. The\n\ufb01gures show the mean average precision on the various\ntransformations of the datasets using the optimized param-\neters. On the Flickr dataset AlexNet performs better than\nSIFT for all transformations except blur, where there is a\nbig drop in performance. Also on the Mikolayczyk dataset,\nthe blur and zoomout transformations are the main reason\nfor SIFT performing better overall. Actually this effect is not\nsurprising. At the lower layers, the networks mostly contain\n\ufb01lters that are tuned to certain frequencies. Also the features\nat higher layers seem to expect a certain sharpness for\ncertain image structures. Consequently, a blurred version of\nthe same image activates very different features. In contrast,\nSIFT is very robust to image blur as it uses simple \ufb01nite\ndifferences that indicate edges at all frequencies, and the\nedge strength is normalized out.\nThe Exemplar-CNN-blur is much less affected by blur\nsince it has learned to be robust to it. To demonstrate the\nimportance of adding blur to the transformations, we also\nincluded the Exemplar-CNN which was used for the classi-\n\ufb01cation task, i.e., without blur among the transformations.\nLike AlexNet, it has problems with matching blurred images\nto the original image.\nComputation times per image are shown in Table 5.\nSIFT computation is clearly faster than feature computa-\ntion by neural networks, but the computation times of\n10\n69\n91\n113\n157\n0.3\n0.4\n0.5\n0.6\nPatch size\nAverage matching mAP\nSIFT\n69\n91\n113\n157\n0.3\n0.4\n0.5\n0.6\nPatch size\nAverage matching mAP\nAlexNet\n \n \nLayer 1\nLayer 2\nLayer 3\nLayer 4\nLayer 5\n69\n91\n113\n157\n0.3\n0.4\n0.5\n0.6\nPatch size\nAverage matching mAP\nExemplar\u2212CNN\u2212orig\n \n \nLayer 1\nLayer 2\nLayer 3\nLayer 4\n69\n91\n113\n157\n0.3\n0.4\n0.5\n0.6\nPatch size\nAverage matching mAP\nExemplar\u2212CNN\u2212blur\n \n \nLayer 1\nLayer 2\nLayer 3\nLayer 4\nFig. 8. Analysis of the matching performance depending on the patch size and the network layer at which features are computed.\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.2\n0.4\n0.6\n0.8\n1\nAP with SIFT\nAP with AlexNet\nAlexNet vs SIFT\n(a)\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.2\n0.4\n0.6\n0.8\n1\nAP with SIFT\nAP with Exemplar\u2212CNN\u2212blur\nExemplar\u2212CNN\u2212blur vs SIFT\n(b)\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.2\n0.4\n0.6\n0.8\n1\nAP with AlexNet\nAP with Exemplar\u2212CNN\u2212blur\nExemplar\u2212CNN\u2212blur vs AlexNet\n(c)\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.2\n0.4\n0.6\n0.8\n1\nAP with SIFT\nAP with AlexNet\nAlexNet vs SIFT\n(d)\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.2\n0.4\n0.6\n0.8\n1\nAP with SIFT\nAP with Exemplar\u2212CNN\u2212blur\nExemplar\u2212CNN\u2212blur vs SIFT\n(e)\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.2\n0.4\n0.6\n0.8\n1\nAP with AlexNet\nAP with Exemplar\u2212CNN\u2212blur\nExemplar\u2212CNN\u2212blur vs AlexNet\n(f)\nFig. 9. Scatter plots for different pairs of descriptors on the Flickr dataset (upper row) and the Mikolajczyk dataset (lower row). Each point in\na scatter plot corresponds to one image pair, and its coordinates are the AP values obtained with the compared descriptors. AlexNet (supervised\ntraining) and the Exemplar-CNN yield features that outperform SIFT on most images of the Flickr dataset (a,b), but AlexNet is inferior to SIFT on\nthe Mikolajczyk dataset. Features obtained with the unsupervised training procedure outperform the features from AlexNet on both datasets (c,f).\nthe neural networks are not prohibitively large, especially\nwhen extracting many descriptors per image using parallel\nhardware.\nMethod\nSIFT\nAlexNet\nEx-CNN-blur\nCPU\n4.5ms\n28.2ms\n103.9ms\nGPU\n-\n0.7ms\n1.8ms\nTABLE 5\nFeature computation times for a patch of 113 by 113 pixels.\n6\nCONCLUSIONS\nWe have proposed a discriminative objective for unsuper-\nvised feature learning by training a CNN without object\nclass labels. The core idea is to generate a set of surrogate\nlabels via data augmentation, where the applied transfor-\nmations de\ufb01ne the invariance properties that are to be\nlearned by the network. The learned features yield a large\nimprovement in classi\ufb01cation accuracy compared to fea-\ntures obtained with previous unsupervised methods. These\nresults strongly indicate that a discriminative objective is\nsuperior to objectives previously used for unsupervised\nfeature learning. The unsupervised training procedure also\nlends itself to learn features for geometric matching tasks. A\ncomparison to the long standing state-of-the-art descriptor\nfor this task, SIFT, revealed a problem when matching\nneural network features in case of blur. We showed that\nby adding blur to the set of transformations applied during\ntraining, the features obtained with such a network are not\nmuch affected by this problem anymore and outperform\n11\n1\n2\n3\n0\n0.2\n0.4\n0.6\n0.8\n1\nTransformation magnitude\nMatching mean AP\nNonlinear\n1\n2\n3\n4\n0\n0.2\n0.4\n0.6\n0.8\n1\nTransformation magnitude\nMatching mean AP\nLighting\n1\n2\n3\n0\n0.2\n0.4\n0.6\n0.8\n1\nTransformation magnitude\nMatching mean AP\nRotation\n \n \nSIFT\nAlexNet\nExemplar\u2212CNN\u2212orig\nExemplar\u2212CNN\u2212blur\n1\n2\n3\n4\n5\n0\n0.2\n0.4\n0.6\n0.8\n1\nTransformation magnitude\nMatching mean AP\nPerspective\n1\n2\n3\n4\n5\n6\n0\n0.2\n0.4\n0.6\n0.8\n1\nTransformation magnitude\nMatching mean AP\nZoom\n1\n2\n3\n0\n0.2\n0.4\n0.6\n0.8\n1\nTransformation magnitude\nMatching mean AP\nBlur\nFig. 10. Mean average precision on the Flickr dataset for various transformations. Except for the blur transformation, all networks perform\nconsistently better than SIFT. The network trained with blur transformations can keep up with SIFT even on blur.\n1\n2\n3\n4\n5\n0\n0.2\n0.4\n0.6\n0.8\nTransformation magnitude\nMatching mean AP\nZoom+rotation (bark)\n1\n2\n3\n4\n5\n0\n0.2\n0.4\n0.6\n0.8\nTransformation magnitude\nMatching mean AP\nBlur (bikes)\n1\n2\n3\n4\n5\n0\n0.2\n0.4\n0.6\n0.8\nTransformation magnitude\nMatching mean AP\nZoomout+rotation (boat)\n \n \nSIFT\nAlexNet\nExemplar\u2212CNN\u2212orig\nExemplar\u2212CNN\u2212blur\n1\n2\n3\n4\n5\n0\n0.2\n0.4\n0.6\n0.8\nTransformation magnitude\nMatching mean AP\nViewpoint (graf)\n1\n2\n3\n4\n5\n0\n0.2\n0.4\n0.6\n0.8\nTransformation magnitude\nMatching mean AP\nLighting (leuven)\n1\n2\n3\n4\n5\n0\n0.2\n0.4\n0.6\n0.8\nTransformation magnitude\nMatching mean AP\nBlur (trees)\n1\n2\n3\n4\n5\n0\n0.2\n0.4\n0.6\n0.8\nTransformation magnitude\nMatching mean AP\nCompression (ubc)\n1\n2\n3\n4\n5\n0\n0.2\n0.4\n0.6\n0.8\nTransformation magnitude\nMatching mean AP\nViewpoint (wall)\nFig. 11. Mean average precision on the Mikolajczyk dataset. The networks perform better on viewpoint transformations, while SIFT is more robust\nto strong blur and lighting transformations.\n12\nSIFT on most image pairs. This simple inclusion of blur\ndemonstrates the \ufb02exibility of the proposed unsupervised\nlearning strategy. The strong relationship of the approach to\ndata augmentation in supervised settings also emphasizes\nthe value of data augmentation in general and suggests the\nuse of more diverse transformations.\nAPPENDIX A\nFORMAL ANALYSIS\nProposition 1. The function\nZ(x) = log \u2225exp(x)\u22251, x \u2208Rn\nis convex. Moreover, for any x \u2208Rn the kernel of its\nHessian matrix \u22072Z(x) is given by span (1)\nProof Since\nZ(x) = log \u2225exp(x)\u22251 = log\nn\nX\ni=1\nexp(xi)\n(8)\nwe need to prove the convexity of the log-sum-exp function.\nThe Hessian \u22072 of this function is given as\n\u22072Z(x) =\n1\n(1T u)2 ((1T u) diag (u) \u2212uuT ),\n(9)\nwith u = exp(x) and 1 \u2208Rn being a vector of ones. To\nshow the convexity we must prove that zT \u22072Z(x)z \u22650 for\nall x, z \u2208Rn. From (9) we get\nzT \u22072Z(x) z =\n1\n(1T u)2 ((1T u) zT diag (u) z \u2212zT uuT z)\n= (Pn\nk=1 ukz2\nk)(Pn\nk=1 uk) \u2212(Pn\nk=1 ukzk)2\n(Pn\nk=1 uk)2\n\u22650\n(10)\nsince\n(Pn\nk=1 uk)2\n\u2265\n0\nand\n(Pn\nk=1 zkuk)2\n\u2264\n(Pn\nk=1 ukz2\nk)(Pn\nk=1 uk)\ndue\nto\nthe\nCauchy-Schwarz\ninequality.\nInequality (10) only turns to equality if\n\u221aukzk = c\u221auk,\n(11)\nwhere the constant c does not depend on k. This immedi-\nately gives z = c1, which proves the second statement of\nthe proposition.\nProposition 2. Let \u03b1 \u2208A be a random vector with values\nin a bounded set A \u2282Rk. Let x(\u00b7): A \u2192Rn be a\ncontinuous function. Then inequality (7)\nE\u03b1 [log \u2225exp(x(\u03b1))\u22251] \u2212log \u2225exp(E\u03b1[x(\u03b1)])\u22251 \u22650\nholds and only turns to equality if for all \u03b11, \u03b12 \u2208A:\n(x(\u03b11) \u2212x(\u03b12)) \u2208span (1) .\nProof Inequality (7) immediately follows from convexity of\nthe function log \u2225exp(\u00b7)\u22251 and Jensen\u2019s inequality.\nJensen\u2019s inequality only turns to equality if the function\nit is applied to is af\ufb01ne-linear on the convex hull of the\nintegration region. In particular this implies\n(x(\u03b11) \u2212x(\u03b12))T \u22072Z(x(\u03b11)) (x(\u03b11) \u2212x(\u03b12)) = 0 (12)\nfor all \u03b11, \u03b12 \u2208A. The second statement of Proposition 1\nthus immediately gives x(\u03b11) \u2212x(\u03b12) = c1, Q.E.D.\nAPPENDIX B\nMETHOD DETAILS\nWe describe here in detail the network architectures we\nevaluated and explain the network training procedure. We\nalso provide details of the clustering process we used to\nimprove Exemplar-CNN.\nB.1\nNetwork Architecture\nWe tested various network architectures in combination\nwith our training procedure. They are coded as follows:\nNcF stands for a convolutional layer with N \ufb01lters of size\nF \u00d7 F pixels, Nf stands for a fully connected layer with\nN units. For example, 64c5-64c5-128f denotes a network\nwith two convolutional layers containing 64 \ufb01lters spanning\n5 \u00d7 5 pixels each followed by a fully connected layer with\n128 units. The last speci\ufb01ed layer is always succeeded by\na softmax layer, which serves as the network output. We\napplied 2 \u00d7 2 max-pooling to the outputs of the \ufb01rst and\nsecond convolutional layers.\nAs stated in the paper we used a 64c5-64c5-128f architec-\nture in our experiments to evaluate the in\ufb02uence of different\ncomponents of the augmentation procedure (we refer to this\narchitecture as the \u2019small\u2019 network). A large network, coded\nas 64c5-128c5-256c5-512f, was then used to achieve better\nclassi\ufb01cation performance.\nAll considered networks contained recti\ufb01ed linear units\nin each layer but the softmax layer. Dropout was applied to\nthe fully connected layer.\nB.2\nTraining the Networks\nWe adopted the common practice of training the network\nwith stochastic gradient descent with a \ufb01xed momentum of\n0.9. We started with a learning rate of 0.01 and gradually de-\ncreased the learning rate during training. That is, we trained\nuntil there was no improvement in validation error, then\ndecreased the learning rate by a factor of 3, and repeated\nthis procedure until convergence. Training times on a Titan\nGPU were roughly 1.5 days for the 64c5-64c5-128f network,\n4 days for the 64c5-128c5-256c5-512f network and 9 days for\nthe 92c5-256c5-512c5-1024f network.\nB.3\nClustering\nTo judge about similarity of the clusters we use the follow-\ning simple heuristics. The method of [39] gives us a set\nof linear SVMs. We apply these SVMs to the whole STL-\n10 unlabeled dataset and select Npercluster = 10 top \ufb01ring\nimages per SVM, which gives us a set of initial clusters. We\nthen compute the overlap (number of common images) of\neach pair of these clusters. We set two thresholds Tmerge = 3\nand Tdiscard = 1 and perform a greedy procedure: starting\nfrom the most overlapping pair of clusters, we merge the\nclusters if their overlap exceeds Tmerge and discard one of\nthe clusters if the overlap is between Tdiscard and Tmerge.\nAPPENDIX C\nDETAILS OF COMPUTING THE MEASURE OF INVARI-\nANCE\nWe now explain in detail and motivate the computation of\nthe normalized Euclidean distance used as a measure of\ninvariance in the paper.\n13\nFirst we compute feature vectors of all image patches\nand their transformed versions. Then we normalize each\nfeature vector to unit Euclidean norm and compute the\nEuclidean distances between each original patch and all\nof its transformed versions. For each transformation and\nmagnitude we average these distances over all patches.\nFinally, we divide the resulting curves by their maximal\nvalues (typically it is the value for the maximum magnitude\nof the transformation).\nThe normalizations are performed to compensate for\npossibly different scales of different features. Normalizing\nfeature vectors to unit length ensures that the values are\nin the same range for different features. The \ufb01nal nor-\nmalization of the curves by the maximal value allows to\ncompensate for different variation of different features: as an\nextreme, a constant feature would be considered perfectly\ninvariant without this normalization, which is certainly not\ndesirable.\nThe resulting curves show how quickly the feature repre-\nsentation changes when an image is transformed more and\nmore. A representation for which the curve steeply goes up\nand then remains constant cannot be considered invariant\nto the transformation: the feature vector of the transformed\npatch becomes completely uncorrelated with the original\nfeature vector even for small magnitudes of the transfor-\nmation. On the other hand, if the curve grows gradually,\nthis indicates that the feature representation changes slowly\nwhen the transformation is applied, meaning invariance or,\nrather, covariance of the representation.\nACKNOWLEDGMENTS\nAD, PF, and TB acknowledge funding by the ERC Starting\nGrant VideoLearn (279401). JTS and MR are supported by\nthe BrainLinks-BrainTools Cluster of Excellence funded by\nthe German Research Foundation (EXC 1086). PF acknowl-\nedges a fellowship by the Deutsche Telekom Stifung.\nREFERENCES\n[1]\nA. Krizhevsky, I. Sutskever, and G. E. Hinton, \u201cImageNet\nclassi\ufb01cation with deep convolutional neural networks,\u201d\nin NIPS, 2012, pp. 1106\u20131114.\n[2]\nM. D. Zeiler and R. Fergus, \u201cVisualizing and understand-\ning convolutional networks,\u201d in ECCV, 2014.\n[3]\nJ. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang,\nE. Tzeng, and T. Darrell, \u201cDeCAF: A deep convolutional\nactivation feature for generic visual recognition,\u201d in ICML,\n2014.\n[4]\nA. S. Razavian, H. Azizpour, J. Sullivan, and S. Carlsson,\n\u201cCNN features off-the-shelf: An astounding baseline for\nrecognition,\u201d in CVPR Workshops 2014, 2014, pp. 512\u2013519.\n[5]\nR. Girshick, J. Donahue, T. Darrell, and J. Malik, \u201cRich fea-\nture hierarchies for accurate object detection and semantic\nsegmentation,\u201d in CVPR, 2014.\n[6]\nP. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,\nand Y. LeCun, \u201cOverFeat: Integrated recognition, localiza-\ntion and detection using convolutional networks.\u201d in ICLR,\n2014.\n[7]\nB. Hariharan, P. Arbelez, R. Girshick, and J. Malik, \u201cHy-\npercolumns for object segmentation and \ufb01ne-grained lo-\ncalization,\u201d CVPR, 2015.\n[8]\nJ. Long, E. Shelhamer, and T. Darrell, \u201cFully convolutional\nnetworks for semantic segmentation,\u201d in CVPR, 2015.\n[9]\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei,\n\u201cImageNet: A Large-Scale Hierarchical Image Database,\u201d\nin CVPR, 2009.\n[10] M. Everingham, L. Gool, C. K. Williams, J. Winn, and\nA. Zisserman, \u201cThe Pascal Visual Object Classes (VOC)\nChallenge,\u201d IJCV, vol. 88, no. 2, pp. 303\u2013338, 2010.\n[11] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E.\nHoward, W. Hubbard, and L. D. Jackel, \u201cBackpropagation\napplied to handwritten zip code recognition,\u201d Neural Com-\nputation, vol. 1, no. 4, pp. 541\u2013551, 1989.\n[12] K. Kavukcuoglu, P. Sermanet, Y. Boureau, K. Gregor,\nM. Mathieu, and Y. LeCun, \u201cLearning convolutional fea-\nture hierachies for visual recognition,\u201d in NIPS, 2010.\n[13] P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol,\n\u201cExtracting and composing robust features with denoising\nautoencoders,\u201d in ICML, 2008, pp. 1096\u20131103.\n[14] W. Y. Zou, A. Y. Ng, S. Zhu, and K. Yu, \u201cDeep learning\nof invariant features via simulated \ufb01xations in video,\u201d in\nNIPS, 2012, pp. 3212\u20133220.\n[15] K. Sohn and H. Lee, \u201cLearning invariant representations\nwith local transformations,\u201d in ICML, 2012.\n[16] K. Y. Hui, \u201cDirect modeling of complex invariances for\nvisual object features,\u201d in ICML, 2013.\n[17] P. Simard, B. Victorri, Y. LeCun, and J. S. Denker, \u201cTangent\nProp - A formalism for specifying selected invariances in\nan adaptive network,\u201d in NIPS, 1992.\n[18] H. Drucker and Y. LeCun, \u201cImproving generalization per-\nformance using double backpropagation,\u201d IEEE Transac-\ntions on Neural Networks, vol. 3, no. 6, pp. 991\u2013997, 1992.\n[19] M.-R. Amini and P. Gallinari, \u201cSemi supervised logistic\nregression,\u201d in ECAI, 2002, pp. 390\u2013394.\n[20] Y. Grandvalet and Y. Bengio, \u201cEntropy regularization,\u201d in\nSemi-Supervised Learning.\nMIT Press, 2006, pp. 151\u2013168.\n[21] A. Ahmed, K. Yu, W. Xu, Y. Gong, and E. Xing, \u201cTraining\nhierarchical feed-forward visual recognition models using\ntransfer learning from pseudo-tasks.\u201d in ECCV (3), 2008,\npp. 69\u201382.\n[22] R.\nCollobert,\nJ.\nWeston,\nL.\nBottou,\nM.\nKarlen,\nK.\nKavukcuoglu,\nand\nP.\nKuksa,\n\u201cNatural\nlanguage\nprocessing (almost) from scratch,\u201d Journal of Machine\nLearning Research, vol. 12, pp. 2493\u20132537, 2011.\n[23] S. Wager, S. Wang, and P. Liang, \u201cDropout training as\nadaptive regularization,\u201d in NIPS, 2013.\n[24] S. Rifai, Y. N. Dauphin, P. Vincent, Y. Bengio, and X. Muller,\n\u201cThe manifold tangent classi\ufb01er,\u201d in NIPS, 2011.\n[25] A. Coates, H. Lee, and A. Y. Ng, \u201cAn analysis of single-\nlayer networks in unsupervised feature learning,\u201d AIS-\nTATS, 2011.\n[26] A. Krizhevsky and G. Hinton, \u201cLearning multiple layers\nof features from tiny images,\u201d Master\u2019s thesis, Department\nof Computer Science, University of Toronto, 2009.\n[27] L. Fei-Fei, R. Fergus, and P. Perona, \u201cLearning generative\nvisual models from few training examples: An incremental\nbayesian approach tested on 101 object categories,\u201d in\nCVPR WGMBV, 2004.\n[28] G. Grif\ufb01n, A. Holub, and P. Perona, \u201cCaltech-256 object\ncategory dataset,\u201d California Institute of Technology, Tech.\nRep. 7694, 2007.\n[29] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever,\nand R. R. Salakhutdinov, \u201cImproving neural networks by\npreventing co-adaptation of feature detectors,\u201d 2012, pre-\nprint, arxiv:cs/1207.0580v3.\n[30] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and\nR. Salakhutdinov, \u201cDropout: A simple way to prevent neu-\nral networks from over\ufb01tting,\u201d Journal of Machine Learning\nResearch, vol. 15, pp. 1929\u20131958, 2014.\n[31] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long,\nR. Girshick, S. Guadarrama, and T. Darrell, \u201cCaffe: Con-\nvolutional architecture for fast feature embedding,\u201d arXiv\npreprint arXiv:1408.5093, 2014.\n14\n[32] A. Coates and A. Y. Ng, \u201cSelecting receptive \ufb01elds in deep\nnetworks,\u201d in NIPS, 2011, pp. 2528\u20132536.\n[33] Y. Boureau, N. Le Roux, F. Bach, J. Ponce, and Y. LeCun,\n\u201cAsk the locals: multi-way local pooling for image recog-\nnition,\u201d in ICCV\u201911.\nIEEE, 2011.\n[34] L. Bo, X. Ren, and D. Fox, \u201cUnsupervised feature learning\nfor RGB-D based object recognition,\u201d in ISER, June 2012.\n[35] \u2014\u2014, \u201cMultipath sparse coding using hierarchical match-\ning pursuit,\u201d in CVPR, 2013, pp. 660\u2013667.\n[36] K. Swersky, J. Snoek, and R. P. Adams, \u201cMulti-task\nbayesian optimization,\u201d in NIPS, 2013.\n[37] C.-Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu, \u201cDeeply\nsupervised nets,\u201d in Deep Learning and Representation Learn-\ning Workshop, NIPS, 2014.\n[38] K. He, X. Zhang, S. Ren, and J. Sun, \u201cSpatial pyramid\npooling in deep convolutional networks for visual recog-\nnition,\u201d in ECCV, 2014.\n[39] S. Singh, A. Gupta, and A. A. Efros, \u201cUnsupervised discov-\nery of mid-level discriminative patches,\u201d in ECCV, 2012.\n[40] D. G. Lowe, \u201cDistinctive image features from scale-\ninvariant keypoints,\u201d IJCV, vol. 60, no. 2, pp. 91\u2013110, Nov.\n2004.\n[41] K. Mikolajczyk and C. Schmid, \u201cA performance evaluation\nof local descriptors,\u201d IEEE Trans. Pattern Anal. Mach. Intell.,\nvol. 27, no. 10, pp. 1615\u20131630, 2005.\n[42] K. Mikolajczyk, T. Tuytelaars, C. Schmid, A. Zisserman,\nJ. Matas, F. Schaffalitzky, T. Kadir, and L. J. V. Gool, \u201cA\ncomparison of af\ufb01ne region detectors,\u201d IJCV, vol. 65, no.\n1-2, pp. 43\u201372, 2005.\n[43] J. Matas, O. Chum, M. Urban, and T. Pajdla, \u201cRobust wide\nbaseline stereo from maximally stable extremal regions,\u201d\nin Proc. BMVC, 2002, pp. 36.1\u201336.10, doi:10.5244/C.16.36.\n",
        "sentence": " All tested generators make use of up-convolutional (\u2019deconvolutional\u2019) layers, as in Dosovitskiy et al. (2015b). An up-convolutional layer consists of up-sampling and a subsequent convolution.",
        "context": "followed by a fully connected layer with 512 units (64c5-\n128c5-256c5-512f), and an even larger network, consisting\nof three convolutional layers with 92, 256 and 512 \ufb01lters\nrespectively and a fully connected layer with 1024 units\nwith two convolutional layers containing 64 \ufb01lters spanning\n5 \u00d7 5 pixels each followed by a fully connected layer with\n128 units. The last speci\ufb01ed layer is always succeeded by\na softmax layer, which serves as the network output. We\n(92c5-256c5-512c5-1024f).\nIn all these models all convolutional \ufb01lters are connected\nto a 5 \u00d7 5 region of their input. 2 \u00d7 2 max-pooling was\nperformed after the \ufb01rst and second convolutional layers."
    },
    {
        "title": "Inverting visual representations with convolutional networks",
        "author": [
            "A. Dosovitskiy",
            "T. Brox"
        ],
        "venue": "arxiv/1506.02753v2,",
        "citeRegEx": "Dosovitskiy and Brox.,? \\Q2015\\E",
        "shortCiteRegEx": "Dosovitskiy and Brox.",
        "year": 2015,
        "abstract": "Feature representations, both hand-designed and learned ones, are often hard\nto analyze and interpret, even when they are extracted from visual data. We\npropose a new approach to study image representations by inverting them with an\nup-convolutional neural network. We apply the method to shallow representations\n(HOG, SIFT, LBP), as well as to deep networks. For shallow representations our\napproach provides significantly better reconstructions than existing methods,\nrevealing that there is surprisingly rich information contained in these\nfeatures. Inverting a deep network trained on ImageNet provides several\ninsights into the properties of the feature representation learned by the\nnetwork. Most strikingly, the colors and the rough contours of an image can be\nreconstructed from activations in higher network layers and even from the\npredicted class probabilities.",
        "full_text": "Inverting Visual Representations with Convolutional Networks\nAlexey Dosovitskiy\nThomas Brox\nUniversity of Freiburg\nFreiburg im Breisgau, Germany\n{dosovits,brox}@cs.uni-freiburg.de\nAbstract\nFeature\nrepresentations,\nboth\nhand-designed\nand\nlearned ones, are often hard to analyze and interpret, even\nwhen they are extracted from visual data. We propose a\nnew approach to study image representations by inverting\nthem with an up-convolutional neural network. We apply\nthe method to shallow representations (HOG, SIFT, LBP),\nas well as to deep networks. For shallow representations\nour approach provides signi\ufb01cantly better reconstructions\nthan existing methods, revealing that there is surprisingly\nrich information contained in these features. Inverting a\ndeep network trained on ImageNet provides several insights\ninto the properties of the feature representation learned\nby the network. Most strikingly, the colors and the rough\ncontours of an image can be reconstructed from activations\nin higher network layers and even from the predicted class\nprobabilities.\n1. Introduction\nA feature representation useful for pattern recognition\ntasks is expected to concentrate on properties of the input\nimage which are important for the task and ignore the ir-\nrelevant properties of the input image. For example, hand-\ndesigned descriptors such as HOG [3] or SIFT [17], explic-\nitly discard the absolute brightness by only considering gra-\ndients, precise spatial information by binning the gradients\nand precise values of the gradients by normalizing the his-\ntograms. Convolutional neural networks (CNNs) trained in\na supervised manner [14, 13] are expected to discard infor-\nmation irrelevant for the task they are solving [28, 19, 22].\nIn this paper we propose a new approach to analyze\nwhich information is preserved by a feature representa-\ntion and which information is discarded. We train neural\nnetworks to invert feature representations in the following\nsense.\nGiven a feature vector, the network is trained to\npredict the expected pre-image, that is, the (weighted) av-\nerage of all natural images which could have produced the\nHOG\nSIFT\nAlexNet-CONV3\nAlexNet-FC8\nFigure 1: We train convolutional networks to reconstruct\nimages from different feature representations. Top row:\nInput features. Bottom row: Reconstructed image. Re-\nconstructions from HOG and SIFT are very realistic. Re-\nconstructions from AlexNet preserve color and rough object\npositions even when reconstructing from higher layers.\ngiven feature vector. The content of this expected pre-image\nshows image properties which can be con\ufb01dently inferred\nfrom the feature vector. The amount of blur corresponds to\nthe level of invariance of the feature representation. We ob-\ntain further insights into the structure of the feature space, as\nwe apply the networks to perturbed feature vectors, to inter-\npolations between two feature vectors, or to random feature\nvectors.\nWe apply our inversion method to AlexNet [13], a con-\nvolutional network trained for classi\ufb01cation on ImageNet,\nas well as to three widely used computer vision features:\nhistogram of oriented gradients (HOG) [3, 7], scale invari-\nant feature transform (SIFT) [17], and local binary pat-\nterns (LBP) [21]. The SIFT representation comes as a non-\nuniform, sparse set of oriented keypoints with their corre-\nsponding descriptors at various scales. This is an additional\nchallenge for the inversion task. LBP features are not dif-\nferentiable with respect to the input image. Thus, existing\nmethods based on gradients of representations [19] could\nnot be applied to them.\n1\narXiv:1506.02753v4  [cs.NE]  26 Apr 2016\n1.1. Related work\nOur approach is related to a large body of work on in-\nverting neural networks. These include works making use\nof backpropagation or sampling [15, 16, 18, 27, 9, 25] and,\nmost similar to our approach, other neural networks [2].\nHowever, only recent advances in neural network architec-\ntures allow us to invert a modern large convolutional net-\nwork with another network.\nOur approach is not to be confused with the Decon-\nvNet [28], which propagates high level activations back-\nward through a network to identify parts of the image re-\nsponsible for the activation. In addition to the high-level\nfeature activations, this reconstruction process uses extra\ninformation about maxima locations in intermediate max-\npooling layers. This information has been shown to be cru-\ncial for the approach to work [22]. A visualization method\nsimilar to DeconvNet is by Springenberg et al. [22], yet it\nalso makes use of intermediate layer activations.\nMahendran and Vedaldi [19] invert a differentiable im-\nage representation \u03a6 using gradient descent. Given a fea-\nture vector \u03a60, they seek for an image x\u2217which minimizes\na loss function \u2013 the squared Euclidean distance between\n\u03a60 and \u03a6(x) plus a regularizer enforcing a natural image\nprior. This method is fundamentally different from our ap-\nproach in that it optimizes the difference between the fea-\nture vectors, not the image reconstruction error. Addition-\nally, it includes a hand-designed natural image prior, while\nin our case the network implicitly learns such a prior. Tech-\nnically, it involves optimization at test time, which requires\ncomputing the gradient of the feature representation and\nmakes it relatively slow (the authors report 6s per image on\na GPU). In contrast, the presented approach is only costly\nwhen training the inversion network. Reconstruction from\na given feature vector just requires a single forward pass\nthrough the network, which takes roughly 5ms per image on\na GPU. The method of [19] requires gradients of the feature\nrepresentation, therefore it could not be directly applied to\nnon-differentiable representations such as LBP, or record-\nings from a real brain [20].\nThere has been research on inverting various tradi-\ntional computer vision representations: HOG and dense\nSIFT [24], keypoint-based SIFT [26], Local Binary De-\nscriptors [4], Bag-of-Visual-Words [11]. All these meth-\nods are either tailored for inverting a speci\ufb01c feature repre-\nsentation or restricted to shallow representations, while our\nmethod can be applied to any feature representation.\n2. Method\nDenote by (x, \u03c6) random variables representing a natu-\nral image and its feature vector, and denote their joint prob-\nability distribution by p(x, \u03c6) = p(x)p(\u03c6|x). Here p(x) is\nthe distribution of natural images and p(\u03c6|x) is the distribu-\ntion of feature vectors given an image. As a special case, \u03c6\nmay be a deterministic function of x. Ideally we would like\nto \ufb01nd p(x|\u03c6), but direct application of Bayes\u2019 theorem is\nnot feasible. Therefore in this paper we resort to a point es-\ntimate f(\u03c6) which minimizes the following mean squared\nerror objective:\nEx,\u03c6 ||x \u2212f(\u03c6)||2\n(1)\nThe minimizer of this loss is the conditional expectation:\n\u02c6f(\u03c60) = Ex [x | \u03c6 = \u03c60],\n(2)\nthat is, the expected pre-image.\nGiven a training set of images and their features\n{xi, \u03c6i}, we learn the weights w of an an up-convolutional\nnetwork f(\u03c6, w) to minimize a Monte-Carlo estimate of\nthe loss (1):\n\u02c6w = arg min\nw\nX\ni\n||xi \u2212f(\u03c6i, w)||2\n2.\n(3)\nThis means that simply training the network to predict im-\nages from their feature vectors results in estimating the ex-\npected pre-image.\n2.1. Feature representations to invert\nShallow features. We invert three traditional computer\nvision feature representations: histogram of oriented gradi-\nents (HOG), scale invariant feature transform (SIFT), and\nlocal binary patterns (LBP). We chose these features for a\nreason. There has been work on inverting HOG, so we can\ncompare to existing approaches. LBP is interesting because\nit is not differentiable, and hence gradient-based methods\ncannot invert it. SIFT is a keypoint-based representation,\nso the network has to stitch different keypoints into a single\nsmooth image.\nFor all three methods we use implementations from the\nVLFeat library [23] with the default settings. More pre-\ncisely, we use the HOG version from Felzenszwalb et al. [7]\nwith cell size 8, the version of SIFT which is very similar\nto the original implementation of Lowe [17] and the LBP\nversion similar to Ojala et al. [21] with cell size 16. Be-\nfore extracting the features we convert images to grayscale.\nMore details can be found in the supplementary material.\nAlexNet.\nWe also invert the representation of the\nAlexNet network [13] trained on ImageNet, available at\nthe Caffe [10] website. 1 It consists of 5 convolutional lay-\ners and 3 fully connected layers, with recti\ufb01ed linear units\n(ReLUs) after each layer, and local contrast normalization\nor max-pooling after some of them. Exact architecture is\nshown in the supplementary material.\nIn what follows,\n1More precisely, we used CaffeNet, which is almost identical to the\noriginal AlexNet.\nwhen we say \u2018output of the layer\u2019, we mean the output of the\nlast processing step of this layer. For example, the output of\nthe \ufb01rst convolutional layer CONV1 would be the result af-\nter ReLU, pooling and normalization, and the output of the\n\ufb01rst fully connected layer FC6 is after ReLU. FC8 denotes\nthe last layer, before the softmax.\n2.2. Network architectures and training\nAn up-convolutional layer, also often referred to as \u2018de-\nconvolutional\u2019, is a combination of upsampling and convo-\nlution [6]. We upsample a feature map by a factor 2 by re-\nplacing each value by a 2 \u00d7 2 block with the original value\nin the top left corner and all other entries equal to zero. Ar-\nchitecture of one of our up-convolutional networks is shown\nin Table 1. Architectures of other networks are shown in the\nsupplementary material.\nHOG and LBP. For an image of size W \u00d7 H, HOG\nand LBP features of an image form 3-dimensional arrays of\nsizes \u2308W/8\u2309\u00d7 \u2308H/8\u2309\u00d7 31 and \u2308W/16\u2309\u00d7 \u2308H/16\u2309\u00d7 58,\nrespectively. We use similar CNN architectures for invert-\ning both feature representations. The networks include a\ncontracting part, which processes the input features through\na series of convolutional layers with occasional stride of 2,\nresulting in a feature map 64 times smaller than the input\nimage. Then the expanding part of the network again up-\nsamples the feature map to the full image resolution by a se-\nries of up-convolutional layers. The contracting part allows\nthe network to aggregate information over large regions of\nthe input image. We found this is necessary to successfully\nestimate the absolute brightness.\nSparse SIFT. Running the SIFT detector and descrip-\ntor on an image gives a set of N keypoints, where the i-th\nkeypoint is described by its coordinates (xi, yi), scale si,\norientation \u03b1i, and a feature descriptor fi of dimensionality\nD. In order to apply a convolutional network, we arrange\nthe keypoints on a grid. We split the image into cells of\nsize d \u00d7 d (we used d = 4 in our experiments), this yields\n\u2308W/d\u2309\u00d7 \u2308H/d\u2309cells. In the rare cases when there are\nseveral keypoints in a cell, we randomly select one. We\nthen assign a vector to each of the cells: a zero vector to\na cell without a keypoint and a vector (fi, xi mod d, yi\nmod d, sin \u03b1i, cos \u03b1i, log si) to a cell with a keypoint. This\nresults in a feature map F of size \u2308W/d\u2309\u00d7\u2308H/d\u2309\u00d7(D+5).\nThen we apply a CNN to F, as described above.\nAlexNet. To reconstruct from each layer of AlexNet we\ntrained a separate network. We used two basic architectures:\none for reconstructing from convolutional layers and one for\nreconstructing from fully connected layers. The network for\nreconstructing from fully connected layers contains three\nfully connected layers and 5 up-convolutional layers, as\nshown in Table 1. The network for reconstructing from con-\nvolutional layers consists of three convolutional and several\nup-convolutional layers (the exact number depends on the\nLayer\nInput\nInSize\nK\nS\nOutSize\nfc1\nAlexNet-FC8\n1000\n\u2212\n\u2212\n4096\nfc2\nfc1\n4096\n\u2212\n\u2212\n4096\nfc3\nfc2\n4096\n\u2212\n\u2212\n4096\nreshape\nfc3\n4096\n\u2212\n\u2212\n4\u00d74\u00d7256\nupconv1\nreshape\n4\u00d74\u00d7256\n5\n2\n8\u00d78\u00d7256\nupconv2\nupconv1\n8\u00d78\u00d7256\n5\n2\n16\u00d716\u00d7128\nupconv3\nupconv2\n16\u00d716\u00d7128\n5\n2\n32\u00d732\u00d764\nupconv4\nupconv3\n32\u00d732\u00d764\n5\n2\n64\u00d764\u00d732\nupconv5\nupconv4\n64\u00d764\u00d732\n5\n2\n128\u00d7128\u00d73\nTable 1: Network for reconstructing from AlexNet FC8 fea-\ntures. K stands for kernel size, S for stride.\nlayer to reconstruct from). Filters in all (up-)convolutional\nlayers have 5 \u00d7 5 spatial size. After each layer we apply\nleaky ReLU nonlinearity with slope 0.2, that is, r(x) = x if\nx \u2a7e0 and r(x) = 0.2 \u00b7 x if x < 0.\nTraining details. We trained networks using a modi\ufb01ed\nversion of Caffe [10]. As training data we used the Ima-\ngeNet [5] training set. In some cases we predicted down-\nsampled images to speed up computations. We used the\nAdam [12] optimizer with \u03b21 = 0.9, \u03b22 = 0.999 and mini-\nbatch size 64. For most networks we found an initial learn-\ning rate \u03bb = 0.001 to work well. We gradually decreased\nthe learning rate towards the end of training. The duration of\ntraining depended on the network: from 15 epochs (passes\nthrough the dataset) for shallower networks to 60 epochs for\ndeeper ones.\nQuantitative evaluation. As a quantitative measure of\nperformance we used the average normalized reconstruc-\ntion error, that is the mean of ||xi \u2212f(\u03a6(xi))||2/N, where\nxi is an example from the test set, f is the function imple-\nmented by the inversion network and N is a normalization\ncoef\ufb01cient equal to the average Euclidean distance between\nimages in the test set. The test set we used for quantita-\ntive and qualitative evaluations is a subset of the ImageNet\nvalidation set.\n3. Experiments: shallow representations\nFigures 1 and 3 show reconstructions of several im-\nages from the ImageNet validation set. Normalized recon-\nstruction error of different approaches is shown in Table 2.\nClearly, our method signi\ufb01cantly outperforms existing ap-\nproaches. This is to be expected, since our method explic-\nitly aims to minimize the reconstruction error.\nHoggles [24]\nHOG\u22121 [19]\nHOG our\nSIFT our\nLBP our\n0.61\n0.63\n0.24\n0.28\n0.38\nTable 2: Normalized error of different methods when recon-\nstructing from HOG.\nImage\nHOG\nHoggles [24]\nHOG\u22121 [19]\nOur\nFigure 2: Reconstructing an image from its HOG descriptors with different methods.\nColorization. As mentioned above, we compute the fea-\ntures based on grayscale images, but the task of the net-\nworks is to reconstruct the color images. The features do\nnot contain any color information, so to predict colors the\nnetwork has to analyze the content of the image and make\nuse of a natural image prior it learned during training. It\ndoes successfully learn to do so, as can be seen in Figures 1\nand 3. Quite often the colors are predicted correctly, espe-\ncially for sky, sea, grass, trees. In other cases, the network\ncannot predict the color (for example, people in the top row\nof Figure 3) and leaves some areas gray. Occasionally the\nnetwork predicts the wrong color, such as in the bottom row\nof Figure 3.\nHOG. Figure 2 shows an example image, its HOG rep-\nresentation, the results of inversion with existing meth-\nods [24, 19] and with our approach. Most interestingly, the\nnetwork is able to reconstruct the overall brightness of the\nimage very well, for example the dark regions are recon-\nstructed dark. This is quite surprising, since the HOG de-\nscriptors are normalized and should not contain information\nabout absolute brightness.\nNormalization is always performed with a smoothing\n\u2019epsilon\u2019, so one might imagine that some information\nabout the brightness is present even in the normalized fea-\ntures. We checked that the network does not make use of\nthis information: multiplying the input image by 10 or 0.1\nhardly changes the reconstruction. Therefore, we hypothe-\nsize that the network reconstructs the overall brightness by\n1) analyzing the distribution of the HOG features (if in a\ncell there is similar amount of gradient in all directions, it is\nprobably noise; if there is one dominating gradient, it must\nactually be in the image), 2) accumulating gradients over\nspace: if there is much black-to-white gradient in one di-\nrection, then probably the brightness in that direction goes\nfrom dark to bright and 3) using semantic information.\nSIFT. Figure 4 shows an image, the detected SIFT key-\npoints and the resulting reconstruction. There are roughly\nImage\nHOG our\nSIFT our\nLBP our\nFigure 3: Inversion of shallow image representations. Note\nhow in the \ufb01rst row the color of grass and trees is predicted\ncorrectly in all cases, although it is not contained in the fea-\ntures.\nFigure 4: Reconstructing an image from SIFT descriptors\nwith different methods. (a) an image, (b) SIFT keypoints,\n(c) reconstruction of [26], (d) our reconstruction.\nImage\nCONV1\nCONV2\nCONV3\nCONV4\nCONV5\nFC6\nFC7\nFC8\nFigure 5: Reconstructions from different layers of AlexNet.\nImage\nCONV1\nCONV2\nCONV3\nCONV4\nCONV5\nFC6\nFC7\nFC8\nOur\n[19]\nAE\nFigure 6: Reconstructions from layers of AlexNet with our method (top), [19] (middle), and autoencoders (bottom).\n3000 keypoints detected in this image. Although made from\na sparse set of keypoints, the reconstruction looks very nat-\nural, just a little blurry. To achieve such a clear reconstruc-\ntion the network has to properly rotate and scale the descrip-\ntors and then stitch them together. Obviously it successfully\nlearns to do this.\nFor reference we also show a result of another existing\nmethod [26] for reconstructing images from sparse SIFT de-\nscriptors. The results are not directly comparable: while we\nuse the SIFT detector providing circular keypoints, Weinza-\nepfel et al. [26] use the Harris af\ufb01ne keypoint detector which\nyields elliptic keypoints, and the number and the locations\nof the keypoints may be different from our case. However,\nthe rough number of keypoints is the same, so a qualitative\ncomparison is still valid.\n4. Experiments: AlexNet\nWe applied our inversion method to different layers of\nAlexNet and performed several additional experiments to\nbetter understand the feature representations. More results\nare shown in the supplementary material.\n4.1. Reconstructions from different layers\nFigure 5 shows reconstructions from various layers of\nAlexNet. When using features from convolutional layers,\nthe reconstructed images look very similar to the input, but\nlose \ufb01ne details as we progress to higher layers. There is\nan obvious drop in reconstruction quality when going from\nCONV5 to FC6. However, the reconstructions from higher\nconvolutional layers and even fully connected layers pre-\nserve color and the approximate object location very well.\nReconstructions from FC7 and FC8 still look similar to the\ninput images, but blurry. This means that high level features\nconv1 conv2 conv3 conv4 conv5\nfc6\nfc7\nfc8\n0\n0.2\n0.4\n0.6\n0.8\n1\nLayer to reconstruct from\nNormalized reconstruction error\n \n \nOur\nMahendran et al.\nAutoencoder\nOur\u2212bin\nOur\u2212drop50\nAutoencoder\u2212bin\nOur\u2212bin\u2212drop50\nOur\u2212bin\u2212drop50least\nFigure 7: Average normalized reconstruction error depend-\ning on the network layer.\nare much less invariant to color and pose than one might ex-\npect: in principle fully connected layers need not preserve\nany information about colors and locations of objects in the\ninput image. This is somewhat in contrast with the results\nof [19], as shown in Figure 6. While their reconstructions\nare sharper, the color and position are completely lost in\nreconstructions from higher layers.\nFor quantitative evaluation before computing the error\nwe up-sample reconstructions to input image size with bi-\nlinear interpolation. Error curves shown in Figure 7 support\nthe conclusions made above.\nWhen reconstructing from\nFC6, the error is roughly twice as large as from CONV5.\nEven when reconstructing from FC8, the error is fairly low\nbecause the network manages to get the color and the rough\nplacement of large objects in images right. For lower lay-\ners, the reconstruction error of [19] is still much higher than\nof our method, even though visually the images look some-\nwhat sharper. The reason is that in their reconstructions the\ncolor and the precise placement of small details do not per-\nfectly match the input image, which results in a large overall\nerror.\n4.2. Autoencoder training\nOur inversion network can be interpreted as the decoder\nof the representation encoded by AlexNet. The difference to\nan autoencoder is that the encoder part stays \ufb01xed and only\nthe decoder is optimized. For comparison we also trained\nautoencoders with the same architecture as our reconstruc-\ntion nets, i.e., we also allowed the training to \ufb01ne-tune the\nparameters of the AlexNet part. This provides an upper\nbound on the quality of reconstructions we might expect\nfrom the inversion networks (with \ufb01xed AlexNet).\nAs shown in Figure 7, autoencoder training yields\nmuch lower reconstruction errors when reconstructing from\nhigher layers. Also the qualitative results in Figure 6 show\nImage\nall\ntop5\nnotop5\npomegranate (0.93)\nGranny Smith apple (0.99)\ncroquet ball (0.96)\nFigure 8: The effect of color on classi\ufb01cation and recon-\nstruction from layer FC8. Left to right: input image, recon-\nstruction from FC8, reconstruction from 5 largest activations\nin FC8, reconstruction from all FC8 activations except the 5\nlargest ones. Below each row the network prediction and its\ncon\ufb01dence are shown.\nmuch better reconstructions with autoencoders. Even from\nCONV5 features, the input image can be reconstructed al-\nmost perfectly. When reconstructing from fully connected\nlayers, the autoencoder results get blurred, too, due to the\ncompressed representation, but by far not as much as with\nthe \ufb01xed AlexNet weights. The gap between the autoen-\ncoder training and the training with \ufb01xed AlexNet gives an\nestimate of the amount of image information lost due to the\ntraining objective of the AlexNet, which is not based on re-\nconstruction quality.\nAn interesting observation with autoencoders is that the\nreconstruction error is quite high even when reconstructing\nfrom CONV1 features, and the best reconstructions were ac-\ntually obtained from CONV4. Our explanation is that the\nconvolution with stride 4 and consequent max-pooling in\nCONV1 loses much information about the image. To de-\ncrease the reconstruction error, it is bene\ufb01cial for the net-\nwork to slightly blur the image instead of guessing the de-\ntails. When reconstructing from deeper layers, deeper net-\nworks can learn a better prior resulting in slightly sharper\nimages and slightly lower reconstruction error. For even\ndeeper layers, the representation gets too compressed and\nthe error increases again. We observed (not shown in the\npaper) that without stride 4 in the \ufb01rst layer, the reconstruc-\ntion error of autoencoders got much lower.\n4.3. Case study: Colored apple\nWe performed a simple experiment illustrating how the\ncolor information in\ufb02uences classi\ufb01cation and how it is pre-\nserved in the high level features. We took an image of a\nred apple (Figure 8 top left) from Flickr and modi\ufb01ed its\nImage\nCONV3\nCONV4\nCONV5\nFC6\nFC7\nFC8\nCONV3\nCONV4\nCONV5\nFC6\nFC7\nFC8\nNo\nper-\nturb\nBin\nDrop\n50\nFixed AlexNet\nAutoencoder\nFigure 9: Reconstructions from different layers of AlexNet with disturbed features.\nhue to make it green or blue. Then we extracted AlexNet\nFC8 features of the resulting images. Remind that FC8 is\nthe last layer of the network, so the FC8 features, after ap-\nplication of softmax, give the network\u2019s prediction of class\nprobabilities. The largest activation, hence, corresponds to\nthe network\u2019s prediction of the image class. To check how\nclass-dependent the results of inversion are, we passed three\nversions of each feature vector through the inversion net-\nwork: 1) just the vector itself, 2) all activations except the\n5 largest ones set to zero, 3) the 5 largest activations set to\nzero.\nThis leads to several conclusions. First, color clearly can\nbe very important for classi\ufb01cation, so the feature represen-\ntation of the network has to be sensitive to it, at least in\nsome cases. Second, the color of the image can be precisely\nreconstructed even from FC8 or, equivalently, from the pre-\ndicted class probabilities. Third, the reconstruction quality\ndoes not depend much on the top predictions of the network\nbut rather on the small probabilities of all other classes. This\nis consistent with the \u2019dark knowledge\u2019 idea of [8]: small\nprobabilities of non-predicted classes carry more informa-\ntion than the prediction itself. More examples of this are\nshown in the supplementary material.\n4.4. Robustness of the feature representation\nWe have shown that high level feature maps preserve rich\ninformation about the image. How is this information rep-\nresented in the feature vector? It is dif\ufb01cult to answer this\nquestion precisely, but we can gain some insight by perturb-\ning the feature representations in certain ways and observ-\ning images reconstructed from these perturbed features. If\nperturbing the features in a certain way does not change the\nreconstruction much, then the perturbed property is not im-\nportant. For example, if setting a non-zero feature to zero\ndoes not change the reconstruction, then this feature does\nnot carry information useful for the reconstruction.\nWe applied binarization and dropout. To binarize the fea-\nture vector, we kept the signs of all entries and set their ab-\nsolute values to a \ufb01xed number, selected such that the Eu-\nclidean norm of the vector remained unchanged (we tried\nseveral other strategies, and this one led to the best result).\nFor all layers except FC8, feature vector entries are non-\nnegative, hence, binarization just sets all non-zero entries to\na \ufb01xed positive value. To perform dropout, we randomly set\n50% of the feature vector entries to zero and then normal-\nize the vector to keep its Euclidean norm unchanged (again,\nwe found this normalization to work best). Qualitative re-\nsults of these perturbations of features in different layers\nof AlexNet are shown in Figure 9. Quantitative results are\nshown in Figure 7. Surprisingly, dropout leads to larger de-\ncrease in reconstruction accuracy than binarization, even in\nthe layers where it had been applied during training. In lay-\ners FC7 and especially FC6, binarization hardly changes the\nreconstruction quality at all. Although it is known that bina-\nrized ConvNet features perform well in classi\ufb01cation [1], it\ncomes as a surprise that for reconstructing the input image\nthe exact values of the features are not important. In FC6\nvirtually all information about the image is contained in the\nbinary code given by the pattern of non-zero activations.\nFigures 7 and 9 show that this binary code only emerges\nwhen training with the classi\ufb01cation objective and dropout,\nwhile autoencoders are very sensitive to perturbations in the\nfeatures.\nTo test the robustness of this binary code, we applied\nbinarization and dropout together. We tried dropping out\n50% random activations or 50% least non-zero activations\nand then binarizing. Dropping out the 50% least activations\nreduces the error much less than dropping out 50% random\nactivations and is even better than not applying any dropout\nfor most layers. However, layers FC6 and FC7 are the most\ninteresting ones: here dropping out 50% random activations\ndecreases the performance substantially, while dropping out\n50% least activations only results in a small decrease. Pos-\nsibly the exact values of the features in FC6 and FC7 do not\naffect the reconstruction much, but they estimate the impor-\ntance of different features.\n4.5. Interpolation and random feature vectors\nAnother way to analyze the feature representation is by\ntraversing the feature manifold and by observing the corre-\nCONV5\nFC6\nFC7\nFC8\nFigure 10: Interpolation between the features of two\nimages.\nsponding images generated by the reconstruction networks.\nWe have seen the reconstructions from feature vectors of\nactual images, but what if a feature vector was not gener-\nated from a natural image? In Figure 10 we show recon-\nstructions obtained with our networks when interpolating\nbetween feature vectors of two images.\nIt is interesting\nto see that interpolating CONV5 features leads to a simple\noverlay of images, but the behavior of interpolations when\nreconstructing from FC6 is very different: images smoothly\nmorph into each other. More examples, together with the\nresults for autoencoders, are shown in the supplementary\nmaterial.\nAnother analysis method is by sampling feature vectors\nrandomly. Our networks were trained to reconstruct images\ngiven their feature representations, but the distribution of\nthe feature vectors is unknown. Hence, there is no simple\nprincipled way to sample from our model. However, by\nassuming independence of the features (a very strong and\nwrong assumption!), we can approximate the distribution\nof each dimension of the feature vector separately. To this\nend we simply computed a histogram of each feature over\na set of 4096 images and sampled from those. We ensured\nthat the sparsity of the random samples is the same as that\nof the actual feature vectors. This procedure led to low con-\ntrast images, perhaps because by independently sampling\neach dimension we did not introduce interactions between\nthe features. Multiplying the feature vectors by a constant\nfactor \u03b1 = 2 increases the contrast without affecting other\nproperties of the generated images.\nRandom samples obtained this way from four top layers\nof AlexNet are shown in Figure 11. No pre-selection was\nperformed. While samples from CONV5 look much like ab-\nstract art, the samples from fully convolutional layers are\nmuch more realistic. This shows that the networks learn\na natural image prior that allows them to produce some-\nwhat realistically looking images from random feature vec-\ntors. We found that a much simpler sampling procedure of\nCONV5\nFC6\nFC7\nFC8\nFigure 11: Images generated from random feature vectors\nof top layers of AlexNet.\n\ufb01tting a single shifted truncated Gaussian to all feature di-\nmensions produces qualitatively very similar images. These\nare shown in the supplementary material together with im-\nages generated from autoencoders, which look much less\nlike natural images.\n5. Conclusions\nWe have proposed to invert image representations with\nup-convolutional networks and have shown that this yields\nmore or less accurate reconstructions of the original images,\ndepending on the level of invariance of the feature represen-\ntation. The networks implicitly learn natural image priors\nwhich allow the retrieval of information that is obviously\nlost in the feature representation, such as color or bright-\nness in HOG or SIFT. The method is very fast at test time\nand does not require the gradient of the feature representa-\ntion to be inverted. Therefore, it can be applied to virtually\nany image representation.\nApplication of our method to the representations learned\nby the AlexNet convolutional network leads do several con-\nclusions: 1) Features from all layers of the network, includ-\ning the \ufb01nal FC8 layer, preserve the precise colors and the\nrough position of objects in the image; 2) In higher layers,\nalmost all information about the input image is contained in\nthe pattern of non-zero activations, not their precise values;\n3) In the layer FC8, most information about the input image\nis contained in small probabilities of those classes that are\nnot in top-5 network predictions.\nAcknowledgements\nWe acknowledge funding by the ERC Starting Grant\nVideoLearn (279401). We are grateful to Aravindh Mahen-\ndran for sharing with us the reconstructions achieved with\nthe method of Mahendran and Vedaldi [19]. We thank Jost\nTobias Springenberg for comments.\nReferences\n[1] P. Agrawal, R. Girshick, and J. Malik. Analyzing the perfor-\nmance of multilayer neural networks for object recognition.\nIn ECCV, 2014. 7\n[2] C. M. Bishop. Neural Networks for Pattern Recognition. Ox-\nford Uni. Press, New York, USA, 1995. 2\n[3] N. Dalal and B. Triggs. Histograms of oriented gradients for\nhuman detection. In CVPR, pages 886\u2013893, 2005. 1\n[4] E. d\u2019Angelo, L. Jacques, A. Alahi, and P. Vandergheynst.\nFrom bits to images: Inversion of local binary descriptors.\nIEEE Trans. Pattern Anal. Mach. Intell., 36(5):874\u2013887,\n2014. 2\n[5] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei.\nImageNet: A Large-Scale Hierarchical Image Database. In\nCVPR, 2009. 3\n[6] A. Dosovitskiy, J. T. Springenberg, and T. Brox. Learning\nto generate chairs with convolutional neural networks. In\nCVPR, 2015. 3\n[7] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ra-\nmanan. Object detection with discriminatively trained part\nbased models. TPAMI, 32(9):1627\u20131645, 2010. 1, 2\n[8] G. E. Hinton, O. Vinyals, and J. Dean. Distilling the knowl-\nedge in a neural network. arXiv:1503.02531, 2015. 7\n[9] C. Jensen, R. Reed, R. Marks, M. El-Sharkawi, J.-B. Jung,\nR. Miyamoto, G. Anderson, and C. Eggen. Inversion of feed-\nforward neural networks: Algorithms and applications. In\nProc. IEEE, pages 1536\u20131549, 1999. 2\n[10] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-\nshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional\narchitecture for fast feature embedding. arXiv:1408.5093,\n2014. 2, 3\n[11] H. Kato and T. Harada. Image reconstruction from bag-of-\nvisual-words. In CVPR, June 2014. 2\n[12] D. P. Kingma and J. Ba. Adam: A method for stochastic\noptimization. In ICLR, 2015. 3\n[13] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet\nclassi\ufb01cation with deep convolutional neural networks. In\nNIPS, pages 1106\u20131114, 2012. 1, 2\n[14] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E.\nHoward, W. Hubbard, and L. D. Jackel. Backpropagation\napplied to handwritten zip code recognition. Neural Compu-\ntation, 1(4):541\u2013551, 1989. 1\n[15] S. Lee and R. M. Kil. Inverse mapping of continuous func-\ntions using local and global information. IEEE Transactions\non Neural Networks, 5(3):409\u2013423, 1994. 2\n[16] A. Linden and J. Kindermann. Inversion of multilayer nets.\nIn Proc. Int. Conf. on Neural Networks, 1989. 2\n[17] D. G. Lowe. Distinctive image features from scale-invariant\nkeypoints.\nInternational Journal of Computer Vision,\n60(2):91\u2013110, 2004. 1, 2\n[18] B. Lu, H. Kita, and Y. Nishikawa. Inverting feedforward neu-\nral networks using linear and nonlinear programming. IEEE\nTransactions on Neural Networks, 10(6):1271\u20131290, 1999.\n2\n[19] A. Mahendran and A. Vedaldi. Understanding deep image\nrepresentations by inverting them. In CVPR, 2015. 1, 2, 3,\n4, 5, 6, 8, 11, 13\n[20] S. Nishimoto, A. Vu, T. Naselaris, Y. Benjamini, B. Yu,\nand J. Gallant.\nReconstructing visual experiences from\nbrain activity evoked by natural movies. Current Biology,\n21(19):1641\u20131646, 2011. 2\n[21] T. Ojala, M. Pietik\u00a8ainen, and T. M\u00a8aenp\u00a8a\u00a8a. Multiresolution\ngray-scale and rotation invariant texture classi\ufb01cation with\nlocal binary patterns. TPAMI, 24(7):971\u2013987, 2002. 1, 2\n[22] J. T. Springenberg, A. Dosovitskiy, T. Brox, and M. Ried-\nmiller. Striving for simplicity: The all convolutional net. In\nICLR Workshop Track, 2015. 1, 2\n[23] A. Vedaldi and B. Fulkerson. Vlfeat: an open and portable\nlibrary of computer vision algorithms. In International Con-\nference on Multimedia, pages 1469\u20131472, 2010. 2, 10\n[24] C. Vondrick, A. Khosla, T. Malisiewicz, and A. Torralba.\nHoggles: Visualizing object detection features. ICCV, 2013.\n2, 3, 4\n[25] A. R. Vrkonyi-Kczy. Observer-based iterative fuzzy and neu-\nral network model inversion for measurement and control\napplications. In I. J. Rudas, J. C. Fodor, and J. Kacprzyk,\neditors, Towards Intelligent Engineering and Information\nTechnology, volume 243 of Studies in Computational Intelli-\ngence, pages 681\u2013702. Springer, 2009. 2\n[26] P. Weinzaepfel, H. Jegou, and P. Prez. Reconstructing an\nimage from its local descriptors. In CVPR. IEEE Computer\nSociety, 2011. 2, 4, 5\n[27] R. J. Williams. Inverting a connectionist network mapping\nby back-propagation of error. In Eighth Annual Conference\nof the Cognitive Society, pages 859\u2013865, 1986. 2\n[28] M. D. Zeiler and R. Fergus. Visualizing and understanding\nconvolutional networks. In ECCV, 2014. 1, 2\nSupplementary material\nNetwork architectures\nTable 3 shows the architecture of\nAlexNet. Tables 4-8 show the architectures of networks\nwe used for inverting different features. After each fully\nconnected and convolutional layer there is always a leaky\nReLU nonlinearity. Networks for inverting HOG and LBP\nhave two streams. Stream A compresses the input features\nspatially and accumulates information over large regions.\nWe found this crucial to get good estimates of the overall\nbrightness of the image. Stream B does not compress spa-\ntially and hence can better preserve \ufb01ne local details. At\none points the outputs of the two streams are concatenated\nand processed jointly, denoted by \u201cJ\u201d. K stands for kernel\nsize, S for stride.\nShallow features details\nAs mentioned, in the paper, for\nall three methods we use implementations from the VLFeat\nlibrary [23] with the default settings. We use the Felzen-\nszwalb et al. version of HOG with cell size 8. For SIFT\nwe used 3 levels per octave, the \ufb01rst octave was 0 (corre-\nsponding to full resolution), the number of octaves was set\nautomatically, effectively searching keypoints of all possi-\nble sizes.\nLayer\nInput\nInSize\nK\nS\nOutSize\nconvA1\nHOG\n32\u00d732\u00d731\n5\n2\n16\u00d716\u00d7256\nconvA2\nconvA1\n16\u00d716\u00d7256\n5\n2\n8\u00d78\u00d7512\nconvA3\nconvA2\n8\u00d78\u00d7512\n3\n2\n4\u00d74\u00d71024\nupconvA1\nconvA3\n4\u00d74\u00d71024\n4\n2\n8\u00d78\u00d7512\nupconvA2\nupconvA1\n8\u00d78\u00d7512\n4\n2\n16\u00d716\u00d7256\nupconvA3\nupconvA2\n16\u00d716\u00d7256\n4\n2\n32\u00d732\u00d7128\nconvB1\nHOG\n32\u00d732\u00d731\n5\n1\n32\u00d732\u00d7128\nconvB2\nconvB1\n32\u00d732\u00d7128\n3\n1\n32\u00d732\u00d7128\nconvJ1\n{upconvA3, convB2}\n32\u00d732\u00d7256\n3\n1\n32\u00d732\u00d7256\nconvJ2\nconvJ1\n32\u00d732\u00d7256\n3\n1\n32\u00d732\u00d7128\nupconvJ4\nconvJ2\n32\u00d732\u00d7128\n4\n2\n64\u00d764\u00d764\nupconvJ5\nupconvJ4\n64\u00d764\u00d764\n4\n2\n128\u00d7128\u00d732\nupconvJ6\nupconvJ5\n128\u00d7128\u00d732\n4\n2\n256\u00d7256\u00d73\nTable 4: Network for reconstructing from HOG features.\nLayer\nInput\nInSize\nK\nS\nOutSize\nconv1\nSIFT\n64\u00d764\u00d7133\n5\n2\n32\u00d732\u00d7256\nconv2\nconv1\n32\u00d732\u00d7256\n3\n2\n16\u00d716\u00d7512\nconv3\nconv2\n16\u00d716\u00d7512\n3\n2\n8\u00d78\u00d71024\nconv4\nconv3\n8\u00d78\u00d71024\n3\n2\n4\u00d74\u00d72048\nconv5\nconv4\n4\u00d74\u00d72048\n3\n1\n4\u00d74\u00d72048\nconv6\nconv5\n4\u00d74\u00d72048\n3\n1\n4\u00d74\u00d71024\nupconv1\nconv6\n4\u00d74\u00d71024\n4\n2\n8\u00d78\u00d7512\nupconv2\nupconv1\n8\u00d78\u00d7512\n4\n2\n16\u00d716\u00d7256\nupconv3\nupconv2\n16\u00d716\u00d7256\n4\n2\n32\u00d732\u00d7128\nupconv4\nupconv3\n32\u00d732\u00d7128\n4\n2\n64\u00d764\u00d764\nupconv5\nupconv4\n64\u00d764\u00d764\n4\n2\n128\u00d7128\u00d732\nupconv6\nupconv5\n128\u00d7128\u00d732\n4\n2\n256\u00d7256\u00d73\nTable 5: Network for reconstructing from SIFT features.\nLayer\nInput\nInSize\nK\nS\nOutSize\nconvA1\nLBP\n16\u00d716\u00d758\n5\n2\n8\u00d78\u00d7256\nconvA2\nconvA1\n8\u00d78\u00d7256\n5\n2\n4\u00d74\u00d7512\nconvA3\nconvA2\n4\u00d74\u00d7512\n3\n1\n4\u00d74\u00d71024\nupconvA1\nconvA3\n4\u00d74\u00d71024\n4\n2\n8\u00d78\u00d7512\nupconvA2\nupconvA1\n8\u00d78\u00d7512\n4\n2\n16\u00d716\u00d7256\nconvB1\nLBP\n16\u00d716\u00d758\n5\n1\n16\u00d716\u00d7128\nconvB2\nconvB1\n16\u00d716\u00d7128\n3\n1\n16\u00d716\u00d7128\nconvJ1\n{upconvA2, convB2}\n16\u00d716\u00d7384\n3\n1\n16\u00d716\u00d7256\nconvJ2\nconvJ1\n16\u00d716\u00d7256\n3\n1\n16\u00d716\u00d7128\nupconvJ3\nconvJ2\n16\u00d716\u00d7128\n4\n2\n32\u00d732\u00d7128\nupconvJ4\nupconvJ3\n32\u00d732\u00d7128\n4\n2\n64\u00d764\u00d764\nupconvJ5\nupconvJ4\n64\u00d764\u00d764\n4\n2\n128\u00d7128\u00d732\nupconvJ6\nupconvJ5\n128\u00d7128\u00d732\n4\n2\n256\u00d7256\u00d73\nTable 6: Network for reconstructing from LBP features.\nLayer\nInput\nInSize\nK\nS\nOutSize\nconv1\nAlexNet-CONV5\n6\u00d76\u00d7256\n3\n1\n6\u00d76\u00d7256\nconv2\nconv1\n6\u00d76\u00d7256\n3\n1\n6\u00d76\u00d7256\nconv3\nconv2\n6\u00d76\u00d7256\n3\n1\n6\u00d76\u00d7256\nupconv1\nconv3\n6\u00d76\u00d7256\n5\n2\n12\u00d712\u00d7256\nupconv2\nupconv1\n12\u00d712\u00d7256\n5\n2\n24\u00d724\u00d7128\nupconv3\nupconv2\n24\u00d724\u00d7128\n5\n2\n48\u00d748\u00d764\nupconv4\nupconv3\n48\u00d748\u00d764\n5\n2\n96\u00d796\u00d732\nupconv5\nupconv4\n96\u00d796\u00d732\n5\n2\n192\u00d7192\u00d73\nTable 7: Network for reconstructing from AlexNet CONV5\nfeatures.\nThe LBP version we used works with 3 \u00d7 3 pixel neigh-\nborhoods. Each of the 8 non-central bits is equal to one if\nthe corresponding pixel is brighter than the central one. All\npossible 256 patterns are quantized into 58 patterns. These\ninclude 56 patterns with exactly one transition from 0 to 1\nwhen going around the central pixel, plus one quantized pat-\ntern comprising two uniform patterns, plus one quantized\npattern containing all other patterns. The quantized LBP\npatterns are then grouped into local histograms over cells of\n16 \u00d7 16 pixels.\nExperiments: shallow representations\nFigure 12 shows\nseveral images and their reconstructions from HOG, SIFT\nand LBP. HOG allows for the best reconstruction, SIFT\nslightly worse, LBP yet slightly worse. Colors are often\nLayer\nInput\nInSize\nK\nS\nOutSize\nfc1\nAlexNet-FC8\n1000\n\u2212\n\u2212\n4096\nfc2\nfc1\n4096\n\u2212\n\u2212\n4096\nfc3\nfc2\n4096\n\u2212\n\u2212\n4096\nreshape\nfc3\n4096\n\u2212\n\u2212\n4\u00d74\u00d7256\nupconv1\nreshape\n4\u00d74\u00d7256\n5\n2\n8\u00d78\u00d7256\nupconv2\nupconv1\n8\u00d78\u00d7256\n5\n2\n16\u00d716\u00d7128\nupconv3\nupconv2\n16\u00d716\u00d7128\n5\n2\n32\u00d732\u00d764\nupconv4\nupconv3\n32\u00d732\u00d764\n5\n2\n64\u00d764\u00d732\nupconv5\nupconv4\n64\u00d764\u00d732\n5\n2\n128\u00d7128\u00d73\nTable 8: Network for reconstructing from AlexNet FC8 fea-\ntures.\nlayer\nCONV1\nCONV2\nCONV3 CONV4\nCONV5\nFC6\nFC7\nFC8\nprocessing\nconv1 mpool1 conv2 mpool2 conv3\nconv4 conv5 mpool5\nfc6\ndrop6\nfc7\ndrop7\nfc8\nsteps\nrelu1\nnorm1\nrelu2\nnorm2\nrelu3\nrelu4\nrelu5\nrelu6\nrelu7\nout size\n55\n27\n27\n13\n13\n13\n13\n6\n1\n1\n1\n1\n1\nout channels\n96\n96\n256\n256\n384\n384\n256\n256\n4096 4096 4096 4096 1000\nTable 3: Summary of the AlexNet network. Input image size is 227 \u00d7 227.\nImage\nHOG our\nSIFT our\nLBP our\nFigure 12: Inversion of shallow image representations.\nreconstructed correctly, but sometimes are wrong, for ex-\nample in the last row. Interestingly, all network typically\nagree on estimated colors.\nExperiments: AlexNet\nWe show here several additional\n\ufb01gures similar to ones from the main paper. Reconstruc-\ntions from different layers of AlexNet are shown in Fig-\nure 13 . Figure 14 shows results illustrating the \u2019dark knowl-\nedge\u2019 hypothesis, similar to Figure 8 from the main paper.\nWe reconstruct from all FC8 features, as well as from only\n5 largest ones or all except the 5 largest ones. It turns out\nthat the top 5 activations are not very important.\nFigure 15 shows images generated by activating single\nneurons in different layers and setting all other neurons to\nzero. Particularly interpretable are images generated this\nway from FC8. Every FC8 neuron corresponds to a class.\nHence the image generated from the activation of, say, \u201cap-\nple\u201d neuron, could be expected to be a stereotypical apple.\nWhat we observe looks rather like it might be the average of\nall images of the class. For some classes the reconstructions\nare somewhat interpretable, for others \u2013 not so much.\nQualitative comparison of reconstructions with our\nmethod to the reconstructions of [19] and the results with\nAlexNet-based autoencoders is given in Figure 16 .\nReconstructions from feature vectors obtained by inter-\npolating between feature vectors of two images are shown in\nFigure 17 , both for \ufb01xed AlexNet and autoencoder training.\nMore examples of such interpolations with \ufb01xed AlexNet\nare shown in Figure 18 .\nAs described in section 5.5 of the main paper, we tried\ntwo different distributions for sampling random feature ac-\ntivations: a histogram-based and a truncated Gaussian. Fig-\nure 19 shows the results with \ufb01xed AlexNet network and\ntruncated Gaussian distribution. Figures 20 and 21 show\nimages generated with autoencoder-trained networks. Note\nthat images generated from autoencoders look much less\nrealistic than images generated with a network with \ufb01xed\nAlexNet weights. This indicates that reconstructing from\nAlexNet features requires a strong natural image prior.\nImage\nCONV1\nCONV2\nCONV3\nCONV4\nCONV5\nFC6\nFC7\nFC8\nFigure 13: Reconstructions from different layers of AlexNet.\nImage\nall\ntop5\nnotop5\nFigure 14: Left to right: input image,\nreconstruction from fc8, reconstruction\nfrom 5 largest activations in FC8, recon-\nstruction from all FC8 activations except\n5 largest ones.\nFC6\nFC7\nFC8\nFigure 15: Reconstructions from single neuron activations in the fully con-\nnected layers of AlexNet. The FC8 neurons correspond to classes, left to\nright: kite, convertible, desktop computer, school bus, street sign, soup\nbowl, bell pepper, soccer ball.\nImage\nCONV1\nCONV2\nCONV3\nCONV4\nCONV5\nFC6\nFC7\nFC8\nOur\n[19]\nAE\nOur\n[19]\nAE\nFigure 16: Reconstructions from different layers of AlexNet with our method and [19].\nCONV4\nCONV5\nFC6\nFC7\nFC8\nFigure 17: Interpolation between the features of two images. Left: AlexNet weights \ufb01xed, right: autoencoder.\nCONV4\nCONV5\nFC6\nFC7\nFC8\nFigure 18: More interpolations between the features of two images with \ufb01xed AlexNet weights.\nCONV5\nFC6\nFC7\nFC8\nFigure 19: Images generated from random feature vectors of top layers of AlexNet with the simpler truncated Gaussian\ndistribution (see section 5.5 of the main paper).\nCONV5\nFC6\nFC7\nFC8\nFigure 20: Images generated from random feature vectors of top layers of AlexNet-based autoencoders with the histogram-\nbased distribution (see section 5.5 of the main paper).\nCONV5\nFC6\nFC7\nFC8\nFigure 21: Images generated from random feature vectors of top layers of AlexNet-based autoencoders with the simpler\ntruncated Gaussian distribution (see section 5.5 of the main paper).\n",
        "sentence": "",
        "context": "right: kite, convertible, desktop computer, school bus, street sign, soup\nbowl, bell pepper, soccer ball.\nImage\nCONV1\nCONV2\nCONV3\nCONV4\nCONV5\nFC6\nFC7\nFC8\nOur\n[19]\nAE\nOur\n[19]\nAE\nImageNet: A Large-Scale Hierarchical Image Database. In\nCVPR, 2009. 3\n[6] A. Dosovitskiy, J. T. Springenberg, and T. Brox. Learning\nto generate chairs with convolutional neural networks. In\nCVPR, 2015. 3\n4, 5, 6, 8, 11, 13\n[20] S. Nishimoto, A. Vu, T. Naselaris, Y. Benjamini, B. Yu,\nand J. Gallant.\nReconstructing visual experiences from\nbrain activity evoked by natural movies. Current Biology,\n21(19):1641\u20131646, 2011. 2"
    },
    {
        "title": "Learning to generate chairs with convolutional neural networks",
        "author": [
            "A. Dosovitskiy",
            "J.T. Springenberg",
            "T. Brox"
        ],
        "venue": "In CVPR,",
        "citeRegEx": "Dosovitskiy et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "Dosovitskiy et al\\.",
        "year": 2015,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "A neural algorithm of artistic style",
        "author": [
            "L.A. Gatys",
            "A.S. Ecker",
            "M. Bethge"
        ],
        "venue": null,
        "citeRegEx": "Gatys et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "Gatys et al\\.",
        "year": 2015,
        "abstract": "In fine art, especially painting, humans have mastered the skill to create\nunique visual experiences through composing a complex interplay between the\ncontent and style of an image. Thus far the algorithmic basis of this process\nis unknown and there exists no artificial system with similar capabilities.\nHowever, in other key areas of visual perception such as object and face\nrecognition near-human performance was recently demonstrated by a class of\nbiologically inspired vision models called Deep Neural Networks. Here we\nintroduce an artificial system based on a Deep Neural Network that creates\nartistic images of high perceptual quality. The system uses neural\nrepresentations to separate and recombine content and style of arbitrary\nimages, providing a neural algorithm for the creation of artistic images.\nMoreover, in light of the striking similarities between performance-optimised\nartificial neural networks and biological vision, our work offers a path\nforward to an algorithmic understanding of how humans create and perceive\nartistic imagery.",
        "full_text": "A Neural Algorithm of Artistic Style\nLeon A. Gatys,1,2,3\u2217Alexander S. Ecker,1,2,4,5 Matthias Bethge1,2,4\n1Werner Reichardt Centre for Integrative Neuroscience\nand Institute of Theoretical Physics, University of T\u00a8ubingen, Germany\n2Bernstein Center for Computational Neuroscience, T\u00a8ubingen, Germany\n3Graduate School for Neural Information Processing, T\u00a8ubingen, Germany\n4Max Planck Institute for Biological Cybernetics, T\u00a8ubingen, Germany\n5Department of Neuroscience, Baylor College of Medicine, Houston, TX, USA\n\u2217To whom correspondence should be addressed; E-mail: leon.gatys@bethgelab.org\nIn \ufb01ne art, especially painting, humans have mastered the skill to create unique\nvisual experiences through composing a complex interplay between the con-\ntent and style of an image. Thus far the algorithmic basis of this process is\nunknown and there exists no arti\ufb01cial system with similar capabilities. How-\never, in other key areas of visual perception such as object and face recognition\nnear-human performance was recently demonstrated by a class of biologically\ninspired vision models called Deep Neural Networks.1,2 Here we introduce an\narti\ufb01cial system based on a Deep Neural Network that creates artistic images\nof high perceptual quality. The system uses neural representations to sepa-\nrate and recombine content and style of arbitrary images, providing a neural\nalgorithm for the creation of artistic images. Moreover, in light of the strik-\ning similarities between performance-optimised arti\ufb01cial neural networks and\nbiological vision,3\u20137 our work offers a path forward to an algorithmic under-\nstanding of how humans create and perceive artistic imagery.\n1\narXiv:1508.06576v2  [cs.CV]  2 Sep 2015\nThe class of Deep Neural Networks that are most powerful in image processing tasks are\ncalled Convolutional Neural Networks. Convolutional Neural Networks consist of layers of\nsmall computational units that process visual information hierarchically in a feed-forward man-\nner (Fig 1). Each layer of units can be understood as a collection of image \ufb01lters, each of which\nextracts a certain feature from the input image. Thus, the output of a given layer consists of\nso-called feature maps: differently \ufb01ltered versions of the input image.\nWhen Convolutional Neural Networks are trained on object recognition, they develop a\nrepresentation of the image that makes object information increasingly explicit along the pro-\ncessing hierarchy.8 Therefore, along the processing hierarchy of the network, the input image\nis transformed into representations that increasingly care about the actual content of the im-\nage compared to its detailed pixel values. We can directly visualise the information each layer\ncontains about the input image by reconstructing the image only from the feature maps in that\nlayer9 (Fig 1, content reconstructions, see Methods for details on how to reconstruct the im-\nage). Higher layers in the network capture the high-level content in terms of objects and their\narrangement in the input image but do not constrain the exact pixel values of the reconstruc-\ntion. (Fig 1, content reconstructions d,e). In contrast, reconstructions from the lower layers\nsimply reproduce the exact pixel values of the original image (Fig 1, content reconstructions\na,b,c). We therefore refer to the feature responses in higher layers of the network as the content\nrepresentation.\nTo obtain a representation of the style of an input image, we use a feature space originally\ndesigned to capture texture information.8 This feature space is built on top of the \ufb01lter responses\nin each layer of the network. It consists of the correlations between the different \ufb01lter responses\nover the spatial extent of the feature maps (see Methods for details). By including the feature\ncorrelations of multiple layers, we obtain a stationary, multi-scale representation of the input\nimage, which captures its texture information but not the global arrangement.\n2\nFigure 1: Convolutional Neural Network (CNN). A given input image is represented as a set\nof \ufb01ltered images at each processing stage in the CNN. While the number of different \ufb01lters\nincreases along the processing hierarchy, the size of the \ufb01ltered images is reduced by some\ndownsampling mechanism (e.g. max-pooling) leading to a decrease in the total number of\nunits per layer of the network. Content Reconstructions. We can visualise the information\nat different processing stages in the CNN by reconstructing the input image from only know-\ning the network\u2019s responses in a particular layer. We reconstruct the input image from from\nlayers \u2018conv1 1\u2019 (a), \u2018conv2 1\u2019 (b), \u2018conv3 1\u2019 (c), \u2018conv4 1\u2019 (d) and \u2018conv5 1\u2019 (e) of the orig-\ninal VGG-Network. We \ufb01nd that reconstruction from lower layers is almost perfect (a,b,c). In\nhigher layers of the network, detailed pixel information is lost while the high-level content of the\nimage is preserved (d,e). Style Reconstructions. On top of the original CNN representations\nwe built a new feature space that captures the style of an input image. The style representation\ncomputes correlations between the different features in different layers of the CNN. We recon-\nstruct the style of the input image from style representations built on different subsets of CNN\nlayers ( \u2018conv1 1\u2019 (a), \u2018conv1 1\u2019 and \u2018conv2 1\u2019 (b), \u2018conv1 1\u2019, \u2018conv2 1\u2019 and \u2018conv3 1\u2019 (c),\n\u2018conv1 1\u2019, \u2018conv2 1\u2019, \u2018conv3 1\u2019 and \u2018conv4 1\u2019 (d), \u2018conv1 1\u2019, \u2018conv2 1\u2019, \u2018conv3 1\u2019, \u2018conv4 1\u2019\nand \u2018conv5 1\u2019 (e)). This creates images that match the style of a given image on an increasing\nscale while discarding information of the global arrangement of the scene.\n3\nAgain, we can visualise the information captured by these style feature spaces built on\ndifferent layers of the network by constructing an image that matches the style representation\nof a given input image (Fig 1, style reconstructions).10,11 Indeed reconstructions from the style\nfeatures produce texturised versions of the input image that capture its general appearance in\nterms of colour and localised structures. Moreover, the size and complexity of local image\nstructures from the input image increases along the hierarchy, a result that can be explained\nby the increasing receptive \ufb01eld sizes and feature complexity. We refer to this multi-scale\nrepresentation as style representation.\nThe key \ufb01nding of this paper is that the representations of content and style in the Convo-\nlutional Neural Network are separable. That is, we can manipulate both representations inde-\npendently to produce new, perceptually meaningful images. To demonstrate this \ufb01nding, we\ngenerate images that mix the content and style representation from two different source images.\nIn particular, we match the content representation of a photograph depicting the \u201cNeckarfront\u201d\nin T\u00a8ubingen, Germany and the style representations of several well-known artworks taken from\ndifferent periods of art (Fig 2).\nThe images are synthesised by \ufb01nding an image that simultaneously matches the content\nrepresentation of the photograph and the style representation of the respective piece of art (see\nMethods for details). While the global arrangement of the original photograph is preserved,\nthe colours and local structures that compose the global scenery are provided by the artwork.\nEffectively, this renders the photograph in the style of the artwork, such that the appearance of\nthe synthesised image resembles the work of art, even though it shows the same content as the\nphotograph.\nAs outlined above, the style representation is a multi-scale representation that includes mul-\ntiple layers of the neural network. In the images we have shown in Fig 2, the style representation\nincluded layers from the whole network hierarchy. Style can also be de\ufb01ned more locally by\n4\nFigure 2: Images that combine the content of a photograph with the style of several well-known\nartworks. The images were created by \ufb01nding an image that simultaneously matches the content\nrepresentation of the photograph and the style representation of the artwork (see Methods). The\noriginal photograph depicting the Neckarfront in T\u00a8ubingen, Germany, is shown in A (Photo:\nAndreas Praefcke). The painting that provided the style for the respective generated image\nis shown in the bottom left corner of each panel. B The Shipwreck of the Minotaur by J.M.W.\nTurner, 1805. C The Starry Night by Vincent van Gogh, 1889. D Der Schrei by Edvard Munch,\n1893. E Femme nue assise by Pablo Picasso, 1910. F Composition VII by Wassily Kandinsky,\n1913.\n5\nincluding only a smaller number of lower layers, leading to different visual experiences (Fig 3,\nalong the rows). When matching the style representations up to higher layers in the network,\nlocal images structures are matched on an increasingly large scale, leading to a smoother and\nmore continuous visual experience. Thus, the visually most appealing images are usually cre-\nated by matching the style representation up to the highest layers in the network (Fig 3, last\nrow).\nOf course, image content and style cannot be completely disentangled. When synthesising\nan image that combines the content of one image with the style of another, there usually does\nnot exist an image that perfectly matches both constraints at the same time. However, the\nloss function we minimise during image synthesis contains two terms for content and style\nrespectively, that are well separated (see Methods). We can therefore smoothly regulate the\nemphasis on either reconstructing the content or the style (Fig 3, along the columns). A strong\nemphasis on style will result in images that match the appearance of the artwork, effectively\ngiving a texturised version of it, but hardly show any of the photograph\u2019s content (Fig 3, \ufb01rst\ncolumn). When placing strong emphasis on content, one can clearly identify the photograph,\nbut the style of the painting is not as well-matched (Fig 3, last column). For a speci\ufb01c pair of\nsource images one can adjust the trade-off between content and style to create visually appealing\nimages.\nHere we present an arti\ufb01cial neural system that achieves a separation of image content from\nstyle, thus allowing to recast the content of one image in the style of any other image. We\ndemonstrate this by creating new, artistic images that combine the style of several well-known\npaintings with the content of an arbitrarily chosen photograph. In particular, we derive the\nneural representations for the content and style of an image from the feature responses of high-\nperforming Deep Neural Networks trained on object recognition. To our knowledge this is the\n\ufb01rst demonstration of image features separating content from style in whole natural images.\n6\nFigure 3: Detailed results for the style of the painting Composition VII by Wassily Kandinsky.\nThe rows show the result of matching the style representation of increasing subsets of the CNN\nlayers (see Methods). We \ufb01nd that the local image structures captured by the style represen-\ntation increase in size and complexity when including style features from higher layers of the\nnetwork. This can be explained by the increasing receptive \ufb01eld sizes and feature complex-\nity along the network\u2019s processing hierarchy. The columns show different relative weightings\nbetween the content and style reconstruction. The number above each column indicates the\nratio \u03b1/\u03b2 between the emphasis on matching the content of the photograph and the style of the\nartwork (see Methods).\n7\nPrevious work on separating content from style was evaluated on sensory inputs of much lesser\ncomplexity, such as characters in different handwriting or images of faces or small \ufb01gures in\ndifferent poses.12,13\nIn our demonstration, we render a given photograph in the style of a range of well-known\nartworks. This problem is usually approached in a branch of computer vision called non-\nphotorealistic rendering (for recent review see14). Conceptually most closely related are meth-\nods using texture transfer to achieve artistic style transfer.15\u201319 However, these previous ap-\nproaches mainly rely on non-parametric techniques to directly manipulate the pixel representa-\ntion of an image. In contrast, by using Deep Neural Networks trained on object recognition, we\ncarry out manipulations in feature spaces that explicitly represent the high level content of an\nimage.\nFeatures from Deep Neural Networks trained on object recognition have been previously\nused for style recognition in order to classify artworks according to the period in which they\nwere created.20 There, classi\ufb01ers are trained on top of the raw network activations, which we\ncall content representations. We conjecture that a transformation into a stationary feature space\nsuch as our style representation might achieve even better performance in style classi\ufb01cation.\nIn general, our method of synthesising images that mix content and style from different\nsources, provides a new, fascinating tool to study the perception and neural representation of\nart, style and content-independent image appearance in general. We can design novel stimuli\nthat introduce two independent, perceptually meaningful sources of variation: the appearance\nand the content of an image. We envision that this will be useful for a wide range of experimen-\ntal studies concerning visual perception ranging from psychophysics over functional imaging\nto even electrophysiological neural recordings. In fact, our work offers an algorithmic under-\nstanding of how neural representations can independently capture the content of an image and\nthe style in which it is presented. Importantly, the mathematical form of our style representa-\n8\ntions generates a clear, testable hypothesis about the representation of image appearance down\nto the single neuron level. The style representations simply compute the correlations between\ndifferent types of neurons in the network. Extracting correlations between neurons is a bio-\nlogically plausible computation that is, for example, implemented by so-called complex cells\nin the primary visual system (V1).21 Our results suggest that performing a complex-cell like\ncomputation at different processing stages along the ventral stream would be a possible way to\nobtain a content-independent representation of the appearance of a visual input.\nAll in all it is truly fascinating that a neural system, which is trained to perform one of the\ncore computational tasks of biological vision, automatically learns image representations that\nallow the separation of image content from style. The explanation could be that when learning\nobject recognition, the network has to become invariant to all image variation that preserves\nobject identity. Representations that factorise the variation in the content of an image and the\nvariation in its appearance would be extremely practical for this task. Thus, our ability to\nabstract content from style and therefore our ability to create and enjoy art might be primarily a\npreeminent signature of the powerful inference capabilities of our visual system.\nMethods\nThe results presented in the main text were generated on the basis of the VGG-Network,22\na Convolutional Neural Network that rivals human performance on a common visual object\nrecognition benchmark task23 and was introduced and extensively described in.22 We used the\nfeature space provided by the 16 convolutional and 5 pooling layers of the 19 layer VGG-\nNetwork. We do not use any of the fully connected layers.The model is publicly available and\ncan be explored in the caffe-framework.24 For image synthesis we found that replacing the\nmax-pooling operation by average pooling improves the gradient \ufb02ow and one obtains slightly\nmore appealing results, which is why the images shown were generated with average pooling.\n9\nGenerally each layer in the network de\ufb01nes a non-linear \ufb01lter bank whose complexity in-\ncreases with the position of the layer in the network. Hence a given input image \u20d7x is encoded\nin each layer of the CNN by the \ufb01lter responses to that image. A layer with Nl distinct \ufb01lters\nhas Nl feature maps each of size Ml, where Ml is the height times the width of the feature map.\nSo the responses in a layer l can be stored in a matrix F l \u2208RNl\u00d7Ml where F l\nij is the activation\nof the ith \ufb01lter at position j in layer l. To visualise the image information that is encoded at\ndifferent layers of the hierarchy (Fig 1, content reconstructions) we perform gradient descent\non a white noise image to \ufb01nd another image that matches the feature responses of the original\nimage. So let \u20d7p and \u20d7x be the original image and the image that is generated and P l and F l their\nrespective feature representation in layer l. We then de\ufb01ne the squared-error loss between the\ntwo feature representations\nLcontent(\u20d7p, \u20d7x, l) = 1\n2\nX\ni,j\n\u0000F l\nij \u2212P l\nij\n\u00012 .\n(1)\nThe derivative of this loss with respect to the activations in layer l equals\n\u2202Lcontent\n\u2202F l\nij\n=\n(\u0000F l \u2212P l\u0001\nij\nif F l\nij > 0\n0\nif F l\nij < 0 .\n(2)\nfrom which the gradient with respect to the image \u20d7x can be computed using standard error\nback-propagation. Thus we can change the initially random image \u20d7x until it generates the same\nresponse in a certain layer of the CNN as the original image \u20d7p. The \ufb01ve content reconstructions\nin Fig 1 are from layers \u2018conv1 1\u2019 (a), \u2018conv2 1\u2019 (b), \u2018conv3 1\u2019 (c), \u2018conv4 1\u2019 (d) and \u2018conv5 1\u2019\n(e) of the original VGG-Network.\nOn top of the CNN responses in each layer of the network we built a style representation\nthat computes the correlations between the different \ufb01lter responses, where the expectation is\ntaken over the spatial extend of the input image. These feature correlations are given by the\nGram matrix Gl \u2208RNl\u00d7Nl, where Gl\nij is the inner product between the vectorised feature map\n10\ni and j in layer l:\nGl\nij =\nX\nk\nF l\nikF l\njk.\n(3)\nTo generate a texture that matches the style of a given image (Fig 1, style reconstructions),\nwe use gradient descent from a white noise image to \ufb01nd another image that matches the style\nrepresentation of the original image. This is done by minimising the mean-squared distance\nbetween the entries of the Gram matrix from the original image and the Gram matrix of the\nimage to be generated. So let \u20d7a and \u20d7x be the original image and the image that is generated and\nAl and Gl their respective style representations in layer l. The contribution of that layer to the\ntotal loss is then\nEl =\n1\n4N 2\nl M 2\nl\nX\ni,j\n\u0000Gl\nij \u2212Al\nij\n\u00012\n(4)\nand the total loss is\nLstyle(\u20d7a, \u20d7x) =\nL\nX\nl=0\nwlEl\n(5)\nwhere wl are weighting factors of the contribution of each layer to the total loss (see below for\nspeci\ufb01c values of wl in our results). The derivative of El with respect to the activations in layer\nl can be computed analytically:\n\u2202El\n\u2202F l\nij\n=\n(\n1\nN2\nl M2\nl\n\u0000(F l)T \u0000Gl \u2212Al\u0001\u0001\nji\nif F l\nij > 0\n0\nif F l\nij < 0 .\n(6)\nThe gradients of El with respect to the activations in lower layers of the network can be readily\ncomputed using standard error back-propagation. The \ufb01ve style reconstructions in Fig 1 were\ngenerated by matching the style representations on layer \u2018conv1 1\u2019 (a), \u2018conv1 1\u2019 and \u2018conv2 1\u2019\n(b), \u2018conv1 1\u2019, \u2018conv2 1\u2019 and \u2018conv3 1\u2019 (c), \u2018conv1 1\u2019, \u2018conv2 1\u2019, \u2018conv3 1\u2019 and \u2018conv4 1\u2019 (d),\n\u2018conv1 1\u2019, \u2018conv2 1\u2019, \u2018conv3 1\u2019, \u2018conv4 1\u2019 and \u2018conv5 1\u2019 (e).\nTo generate the images that mix the content of a photograph with the style of a painting\n(Fig 2) we jointly minimise the distance of a white noise image from the content representation\n11\nof the photograph in one layer of the network and the style representation of the painting in a\nnumber of layers of the CNN. So let \u20d7p be the photograph and\u20d7a be the artwork. The loss function\nwe minimise is\nLtotal(\u20d7p,\u20d7a, \u20d7x) = \u03b1Lcontent(\u20d7p, \u20d7x) + \u03b2Lstyle(\u20d7a, \u20d7x)\n(7)\nwhere \u03b1 and \u03b2 are the weighting factors for content and style reconstruction respectively. For\nthe images shown in Fig 2 we matched the content representation on layer \u2018conv4 2\u2019 and the\nstyle representations on layers \u2018conv1 1\u2019, \u2018conv2 1\u2019, \u2018conv3 1\u2019, \u2018conv4 1\u2019 and \u2018conv5 1\u2019 (wl =\n1/5 in those layers, wl = 0 in all other layers) . The ratio \u03b1/\u03b2 was either 1\u00d710\u22123 (Fig 2 B,C,D)\nor 1 \u00d7 10\u22124 (Fig 2 E,F). Fig 3 shows results for different relative weightings of the content and\nstyle reconstruction loss (along the columns) and for matching the style representations only\non layer \u2018conv1 1\u2019 (A), \u2018conv1 1\u2019 and \u2018conv2 1\u2019 (B), \u2018conv1 1\u2019, \u2018conv2 1\u2019 and \u2018conv3 1\u2019 (C),\n\u2018conv1 1\u2019, \u2018conv2 1\u2019, \u2018conv3 1\u2019 and \u2018conv4 1\u2019 (D), \u2018conv1 1\u2019, \u2018conv2 1\u2019, \u2018conv3 1\u2019, \u2018conv4 1\u2019\nand \u2018conv5 1\u2019 (E). The factor wl was always equal to one divided by the number of active layers\nwith a non-zero loss-weight wl.\nAcknowledgments\nThis work was funded by the German National Academic Foundation\n(L.A.G.), the Bernstein Center for Computational Neuroscience (FKZ 01GQ1002) and the Ger-\nman Excellency Initiative through the Centre for Integrative Neuroscience T\u00a8ubingen (EXC307)(M.B.,\nA.S.E, L.A.G.)\nReferences and Notes\n1. Krizhevsky, A., Sutskever, I. & Hinton, G. E. Imagenet classi\ufb01cation with deep convolu-\ntional neural networks. In Advances in neural information processing systems, 1097\u20131105\n(2012). URL http://papers.nips.cc/paper/4824-imagenet.\n12\n2. Taigman, Y., Yang, M., Ranzato, M. & Wolf, L. Deepface: Closing the gap to human-level\nperformance in face veri\ufb01cation. In Computer Vision and Pattern Recognition (CVPR),\n2014 IEEE Conference on, 1701\u20131708 (IEEE, 2014).\nURL http://ieeexplore.\nieee.org/xpls/abs_all.jsp?arnumber=6909616.\n3. G\u00a8uc\u00b8l\u00a8u, U. & Gerven, M. A. J. v. Deep Neural Networks Reveal a Gradient in the Com-\nplexity of Neural Representations across the Ventral Stream. The Journal of Neuroscience\n35, 10005\u201310014 (2015). URL http://www.jneurosci.org/content/35/27/\n10005.\n4. Yamins, D. L. K. et al.\nPerformance-optimized hierarchical models predict neural re-\nsponses in higher visual cortex.\nProceedings of the National Academy of Sciences\n201403112 (2014).\nURL http://www.pnas.org/content/early/2014/05/\n08/1403112111.\n5. Cadieu, C. F. et al. Deep Neural Networks Rival the Representation of Primate IT Cortex\nfor Core Visual Object Recognition. PLoS Comput Biol 10, e1003963 (2014). URL http:\n//dx.doi.org/10.1371/journal.pcbi.1003963.\n6. K\u00a8ummerer, M., Theis, L. & Bethge, M.\nDeep Gaze I: Boosting Saliency Prediction\nwith Feature Maps Trained on ImageNet. In ICLR Workshop (2015). URL /media/\npublications/1411.1045v4.pdf.\n7. Khaligh-Razavi, S.-M. & Kriegeskorte, N. Deep Supervised, but Not Unsupervised, Mod-\nels May Explain IT Cortical Representation. PLoS Comput Biol 10, e1003915 (2014). URL\nhttp://dx.doi.org/10.1371/journal.pcbi.1003915.\n13\n8. Gatys, L. A., Ecker, A. S. & Bethge, M. Texture synthesis and the controlled generation of\nnatural stimuli using convolutional neural networks. arXiv:1505.07376 [cs, q-bio] (2015).\nURL http://arxiv.org/abs/1505.07376. ArXiv: 1505.07376.\n9. Mahendran, A. & Vedaldi, A. Understanding Deep Image Representations by Inverting\nThem. arXiv:1412.0035 [cs] (2014). URL http://arxiv.org/abs/1412.0035.\nArXiv: 1412.0035.\n10. Heeger, D. J. & Bergen, J. R.\nPyramid-based Texture Analysis/Synthesis.\nIn Pro-\nceedings of the 22Nd Annual Conference on Computer Graphics and Interactive Tech-\nniques, SIGGRAPH \u201995, 229\u2013238 (ACM, New York, NY, USA, 1995).\nURL http:\n//doi.acm.org/10.1145/218380.218446.\n11. Portilla, J. & Simoncelli, E. P.\nA Parametric Texture Model Based on Joint Statis-\ntics of Complex Wavelet Coef\ufb01cients.\nInternational Journal of Computer Vision\n40, 49\u201370 (2000). URL http://link.springer.com/article/10.1023/A%\n3A1026553619983.\n12. Tenenbaum, J. B. & Freeman, W. T. Separating style and content with bilinear models. Neu-\nral computation 12, 1247\u20131283 (2000). URL http://www.mitpressjournals.\norg/doi/abs/10.1162/089976600300015349.\n13. Elgammal, A. & Lee, C.-S. Separating style and content on a nonlinear manifold. In\nComputer Vision and Pattern Recognition, 2004. CVPR 2004. Proceedings of the 2004\nIEEE Computer Society Conference on, vol. 1, I\u2013478 (IEEE, 2004).\nURL http://\nieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1315070.\n14. Kyprianidis, J. E., Collomosse, J., Wang, T. & Isenberg, T. State of the \u201dArt\u201d: A Taxon-\nomy of Artistic Stylization Techniques for Images and Video. Visualization and Computer\n14\nGraphics, IEEE Transactions on 19, 866\u2013885 (2013). URL http://ieeexplore.\nieee.org/xpls/abs_all.jsp?arnumber=6243138.\n15. Hertzmann, A., Jacobs, C. E., Oliver, N., Curless, B. & Salesin, D. H. Image analogies.\nIn Proceedings of the 28th annual conference on Computer graphics and interactive tech-\nniques, 327\u2013340 (ACM, 2001). URL http://dl.acm.org/citation.cfm?id=\n383295.\n16. Ashikhmin, N. Fast texture transfer. IEEE Computer Graphics and Applications 23, 38\u201343\n(2003).\n17. Efros, A. A. & Freeman, W. T. Image quilting for texture synthesis and transfer. In Pro-\nceedings of the 28th annual conference on Computer graphics and interactive techniques,\n341\u2013346 (ACM, 2001). URL http://dl.acm.org/citation.cfm?id=383296.\n18. Lee, H., Seo, S., Ryoo, S. & Yoon, K. Directional Texture Transfer. In Proceedings of the\n8th International Symposium on Non-Photorealistic Animation and Rendering, NPAR \u201910,\n43\u201348 (ACM, New York, NY, USA, 2010). URL http://doi.acm.org/10.1145/\n1809939.1809945.\n19. Xie, X., Tian, F. & Seah, H. S. Feature Guided Texture Synthesis (FGTS) for Artistic Style\nTransfer. In Proceedings of the 2Nd International Conference on Digital Interactive Media\nin Entertainment and Arts, DIMEA \u201907, 44\u201349 (ACM, New York, NY, USA, 2007). URL\nhttp://doi.acm.org/10.1145/1306813.1306830.\n20. Karayev, S. et al. Recognizing image style. arXiv preprint arXiv:1311.3715 (2013). URL\nhttp://arxiv.org/abs/1311.3715.\n15\n21. Adelson, E. H. & Bergen, J. R. Spatiotemporal energy models for the perception of motion.\nJOSA A 2, 284\u2013299 (1985). URL http://www.opticsinfobase.org/josaa/\nfulltext.cfm?uri=josaa-2-2-284.\n22. Simonyan, K. & Zisserman, A. Very Deep Convolutional Networks for Large-Scale Image\nRecognition. arXiv:1409.1556 [cs] (2014). URL http://arxiv.org/abs/1409.\n1556. ArXiv: 1409.1556.\n23. Russakovsky,\nO. et al.\nImageNet Large Scale Visual Recognition Challenge.\narXiv:1409.0575 [cs] (2014). URL http://arxiv.org/abs/1409.0575. ArXiv:\n1409.0575.\n24. Jia, Y. et al. Caffe: Convolutional architecture for fast feature embedding. In Proceedings\nof the ACM International Conference on Multimedia, 675\u2013678 (ACM, 2014). URL http:\n//dl.acm.org/citation.cfm?id=2654889.\n16\n",
        "sentence": " Generative adversarial networks (GANs) have been proposed by Goodfellow et al. (2014). In theory, this training procedure can lead to a generator that perfectly models the data distribution. To control the degree of realism in generated images, an alternative to adversarial training is an approach making use of feature statistics, similar to (Gatys et al., 2015).",
        "context": "13\n8. Gatys, L. A., Ecker, A. S. & Bethge, M. Texture synthesis and the controlled generation of\nnatural stimuli using convolutional neural networks. arXiv:1505.07376 [cs, q-bio] (2015).\nURL http://arxiv.org/abs/1505.07376. ArXiv: 1505.07376.\ntion of an image. In contrast, by using Deep Neural Networks trained on object recognition, we\ncarry out manipulations in feature spaces that explicitly represent the high level content of an\nimage.\narti\ufb01cial system based on a Deep Neural Network that creates artistic images\nof high perceptual quality. The system uses neural representations to sepa-\nrate and recombine content and style of arbitrary images, providing a neural"
    },
    {
        "title": "Generative adversarial nets",
        "author": [
            "I. Goodfellow",
            "J. Pouget-Abadie",
            "M. Mirza",
            "B. Xu",
            "D. Warde-Farley",
            "S. Ozair",
            "A. Courville",
            "Y. Bengio"
        ],
        "venue": "In NIPS,",
        "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Goodfellow et al\\.",
        "year": 2014,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " To this end, we build upon adversarial training as proposed by Goodfellow et al. (2014). We train a discriminator network to distinguish the output of the generator from real images. By combining convolutions and unpooling (upsampling) layers (Lee et al., 2009; Goodfellow et al., 2014; Dosovitskiy et al., 2015b) these models can be applied to large images. Instead of manually designing a prior, as in Mahendran & Vedaldi (2015), we learn it with an approach similar to Generative Adversarial Networks (GANs) of Goodfellow et al. (2014). Namely, we introduce a discriminator D\u03c6 which aims to discriminate the generated images from real ones, and which is trained concurrently with the generatorG\u03b8.",
        "context": null
    },
    {
        "title": "DRAW: A recurrent neural network for image generation",
        "author": [
            "K. Gregor",
            "I. Danihelka",
            "A. Graves",
            "D.J. Rezende",
            "D. Wierstra"
        ],
        "venue": "In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015,",
        "citeRegEx": "Gregor et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "Gregor et al\\.",
        "year": 2015,
        "abstract": "This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural\nnetwork architecture for image generation. DRAW networks combine a novel\nspatial attention mechanism that mimics the foveation of the human eye, with a\nsequential variational auto-encoding framework that allows for the iterative\nconstruction of complex images. The system substantially improves on the state\nof the art for generative models on MNIST, and, when trained on the Street View\nHouse Numbers dataset, it generates images that cannot be distinguished from\nreal data with the naked eye.",
        "full_text": "DRAW: A Recurrent Neural Network For Image Generation\nKarol Gregor\nKAROLG@GOOGLE.COM\nIvo Danihelka\nDANIHELKA@GOOGLE.COM\nAlex Graves\nGRAVESA@GOOGLE.COM\nDanilo Jimenez Rezende\nDANILOR@GOOGLE.COM\nDaan Wierstra\nWIERSTRA@GOOGLE.COM\nGoogle DeepMind\nAbstract\nThis paper introduces the Deep Recurrent Atten-\ntive Writer (DRAW) neural network architecture\nfor image generation. DRAW networks combine\na novel spatial attention mechanism that mimics\nthe foveation of the human eye, with a sequential\nvariational auto-encoding framework that allows\nfor the iterative construction of complex images.\nThe system substantially improves on the state\nof the art for generative models on MNIST, and,\nwhen trained on the Street View House Numbers\ndataset, it generates images that cannot be distin-\nguished from real data with the naked eye.\n1. Introduction\nA person asked to draw, paint or otherwise recreate a visual\nscene will naturally do so in a sequential, iterative fashion,\nreassessing their handiwork after each modi\ufb01cation. Rough\noutlines are gradually replaced by precise forms, lines are\nsharpened, darkened or erased, shapes are altered, and the\n\ufb01nal picture emerges. Most approaches to automatic im-\nage generation, however, aim to generate entire scenes at\nonce. In the context of generative neural networks, this typ-\nically means that all the pixels are conditioned on a single\nlatent distribution (Dayan et al., 1995; Hinton & Salakhut-\ndinov, 2006; Larochelle & Murray, 2011). As well as pre-\ncluding the possibility of iterative self-correction, the \u201cone\nshot\u201d approach is fundamentally dif\ufb01cult to scale to large\nimages. The Deep Recurrent Attentive Writer (DRAW) ar-\nchitecture represents a shift towards a more natural form of\nimage construction, in which parts of a scene are created\nindependently from others, and approximate sketches are\nsuccessively re\ufb01ned.\nProceedings of the 32 nd International Conference on Machine\nLearning, Lille, France, 2015. JMLR: W&CP volume 37. Copy-\nright 2015 by the author(s).\nTime\nFigure 1. A trained DRAW network generating MNIST dig-\nits. Each row shows successive stages in the generation of a sin-\ngle digit. Note how the lines composing the digits appear to be\n\u201cdrawn\u201d by the network. The red rectangle delimits the area at-\ntended to by the network at each time-step, with the focal preci-\nsion indicated by the width of the rectangle border.\nThe core of the DRAW architecture is a pair of recurrent\nneural networks: an encoder network that compresses the\nreal images presented during training, and a decoder that\nreconstitutes images after receiving codes. The combined\nsystem is trained end-to-end with stochastic gradient de-\nscent, where the loss function is a variational upper bound\non the log-likelihood of the data. It therefore belongs to the\nfamily of variational auto-encoders, a recently emerged\nhybrid of deep learning and variational inference that has\nled to signi\ufb01cant advances in generative modelling (Gre-\ngor et al., 2014; Kingma & Welling, 2014; Rezende et al.,\n2014; Mnih & Gregor, 2014; Salimans et al., 2014). Where\nDRAW differs from its siblings is that, rather than generat-\narXiv:1502.04623v2  [cs.CV]  20 May 2015\nDRAW: A Recurrent Neural Network For Image Generation\ning images in a single pass, it iteratively constructs scenes\nthrough an accumulation of modi\ufb01cations emitted by the\ndecoder, each of which is observed by the encoder.\nAn obvious correlate of generating images step by step is\nthe ability to selectively attend to parts of the scene while\nignoring others. A wealth of results in the past few years\nsuggest that visual structure can be better captured by a se-\nquence of partial glimpses, or foveations, than by a sin-\ngle sweep through the entire image (Larochelle & Hinton,\n2010; Denil et al., 2012; Tang et al., 2013; Ranzato, 2014;\nZheng et al., 2014; Mnih et al., 2014; Ba et al., 2014; Ser-\nmanet et al., 2014). The main challenge faced by sequential\nattention models is learning where to look, which can be\naddressed with reinforcement learning techniques such as\npolicy gradients (Mnih et al., 2014). The attention model in\nDRAW, however, is fully differentiable, making it possible\nto train with standard backpropagation. In this sense it re-\nsembles the selective read and write operations developed\nfor the Neural Turing Machine (Graves et al., 2014).\nThe following section de\ufb01nes the DRAW architecture,\nalong with the loss function used for training and the pro-\ncedure for image generation. Section 3 presents the selec-\ntive attention model and shows how it is applied to read-\ning and modifying images.\nSection 4 provides experi-\nmental results on the MNIST, Street View House Num-\nbers and CIFAR-10 datasets, with examples of generated\nimages; and concluding remarks are given in Section 5.\nLastly, we would like to direct the reader to the video\naccompanying this paper (https://www.youtube.\ncom/watch?v=Zt-7MI9eKEo) which contains exam-\nples of DRAW networks reading and generating images.\n2. The DRAW Network\nThe basic structure of a DRAW network is similar to that of\nother variational auto-encoders: an encoder network deter-\nmines a distribution over latent codes that capture salient\ninformation about the input data; a decoder network re-\nceives samples from the code distribuion and uses them to\ncondition its own distribution over images. However there\nare three key differences. Firstly, both the encoder and de-\ncoder are recurrent networks in DRAW, so that a sequence\nof code samples is exchanged between them; moreover the\nencoder is privy to the decoder\u2019s previous outputs, allow-\ning it to tailor the codes it sends according to the decoder\u2019s\nbehaviour so far. Secondly, the decoder\u2019s outputs are suc-\ncessively added to the distribution that will ultimately gen-\nerate the data, as opposed to emitting this distribution in\na single step. And thirdly, a dynamically updated atten-\ntion mechanism is used to restrict both the input region\nobserved by the encoder, and the output region modi\ufb01ed\nby the decoder. In simple terms, the network decides at\neach time-step \u201cwhere to read\u201d and \u201cwhere to write\u201d as well\nread\nx\nzt\nzt+1\nP(x|z1:T )\nwrite\nencoder\nRNN\nsample\ndecoder\nRNN\nread\nx\nwrite\nencoder\nRNN\nsample\ndecoder\nRNN\nct\u22121\nct\ncT\n\u03c3\nhenc\nt\u22121\nhdec\nt\u22121\nQ(zt|x, z1:t\u22121)\nQ(zt+1|x, z1:t)\n. . .\ndecoding\n(generative model)\nencoding\n(inference)\nx\nencoder\nFNN\nsample\ndecoder\nFNN\nz\nQ(z|x)\nP(x|z)\nFigure 2. Left: Conventional Variational Auto-Encoder. Dur-\ning generation, a sample z is drawn from a prior P(z) and passed\nthrough the feedforward decoder network to compute the proba-\nbility of the input P(x|z) given the sample. During inference the\ninput x is passed to the encoder network, producing an approx-\nimate posterior Q(z|x) over latent variables. During training, z\nis sampled from Q(z|x) and then used to compute the total de-\nscription length KL\n\u0000Q(Z|x)||P(Z)\n\u0001\n\u2212log(P(x|z)), which is\nminimised with stochastic gradient descent. Right: DRAW Net-\nwork. At each time-step a sample zt from the prior P(zt) is\npassed to the recurrent decoder network, which then modi\ufb01es part\nof the canvas matrix. The \ufb01nal canvas matrix cT is used to com-\npute P(x|z1:T ). During inference the input is read at every time-\nstep and the result is passed to the encoder RNN. The RNNs at\nthe previous time-step specify where to read. The output of the\nencoder RNN is used to compute the approximate posterior over\nthe latent variables at that time-step.\nas \u201cwhat to write\u201d. The architecture is sketched in Fig. 2,\nalongside a feedforward variational auto-encoder.\n2.1. Network Architecture\nLet RNN enc be the function enacted by the encoder net-\nwork at a single time-step. The output of RNN enc at time\nt is the encoder hidden vector henc\nt\n. Similarly the output of\nthe decoder RNN dec at t is the hidden vector hdec\nt\n. In gen-\neral the encoder and decoder may be implemented by any\nrecurrent neural network. In our experiments we use the\nLong Short-Term Memory architecture (LSTM; Hochreiter\n& Schmidhuber (1997)) for both, in the extended form with\nforget gates (Gers et al., 2000).\nWe favour LSTM due\nto its proven track record for handling long-range depen-\ndencies in real sequential data (Graves, 2013; Sutskever\net al., 2014). Throughout the paper, we use the notation\nb = W (a) to denote a linear weight matrix with bias from\nthe vector a to the vector b.\nAt each time-step t, the encoder receives input from both\nthe image x and from the previous decoder hidden vector\nhdec\nt\u22121. The precise form of the encoder input depends on a\nread operation, which will be de\ufb01ned in the next section.\nThe output henc\nt\nof the encoder is used to parameterise a\ndistribution Q(Zt|henc\nt\n) over the latent vector zt. In our\nDRAW: A Recurrent Neural Network For Image Generation\nexperiments the latent distribution is a diagonal Gaussian\nN(Zt|\u00b5t, \u03c3t):\n\u00b5t = W (henc\nt\n)\n(1)\n\u03c3t = exp (W (henc\nt\n))\n(2)\nBernoulli distributions are more common than Gaussians\nfor latent variables in auto-encoders (Dayan et al., 1995;\nGregor et al., 2014); however a great advantage of Gaus-\nsian latents is that the gradient of a function of the sam-\nples with respect to the distribution parameters can be eas-\nily obtained using the so-called reparameterization trick\n(Kingma & Welling, 2014; Rezende et al., 2014).\nThis\nmakes it straightforward to back-propagate unbiased, low\nvariance stochastic gradients of the loss function through\nthe latent distribution.\nAt each time-step a sample zt \u223cQ(Zt|henc\nt\n) drawn from\nthe latent distribution is passed as input to the decoder. The\noutput hdec\nt\nof the decoder is added (via a write opera-\ntion, de\ufb01ned in the sequel) to a cumulative canvas matrix\nct, which is ultimately used to reconstruct the image. The\ntotal number of time-steps T consumed by the network be-\nfore performing the reconstruction is a free parameter that\nmust be speci\ufb01ed in advance.\nFor each image x presented to the network, c0, henc\n0\n, hdec\n0\nare initialised to learned biases, and the DRAW net-\nwork iteratively computes the following equations for t =\n1 . . . , T:\n\u02c6xt = x \u2212\u03c3(ct\u22121)\n(3)\nrt = read(xt, \u02c6xt, hdec\nt\u22121)\n(4)\nhenc\nt\n= RNN enc(henc\nt\u22121, [rt, hdec\nt\u22121])\n(5)\nzt \u223cQ(Zt|henc\nt\n)\n(6)\nhdec\nt\n= RNN dec(hdec\nt\u22121, zt)\n(7)\nct = ct\u22121 + write(hdec\nt\n)\n(8)\nwhere \u02c6xt is the error image, [v, w] is the concatenation\nof vectors v and w into a single vector, and \u03c3 denotes\nthe logistic sigmoid function: \u03c3(x) =\n1\n1+exp(\u2212x). Note\nthat henc\nt\n, and hence Q(Zt|henc\nt\n), depends on both x\nand the history z1:t\u22121 of previous latent samples.\nWe\nwill sometimes make this dependency explicit by writing\nQ(Zt|x, z1:t\u22121), as shown in Fig. 2.\nhenc can also be\npassed as input to the read operation; however we did not\n\ufb01nd that this helped performance and therefore omitted it.\n2.2. Loss Function\nThe \ufb01nal canvas matrix cT is used to parameterise a model\nD(X|cT ) of the input data. If the input is binary, the natural\nchoice for D is a Bernoulli distribution with means given\nby \u03c3(cT ). The reconstruction loss Lx is de\ufb01ned as the\nnegative log probability of x under D:\nLx = \u2212log D(x|cT )\n(9)\nThe latent loss Lz for a sequence of latent distributions\nQ(Zt|henc\nt\n) is de\ufb01ned as the summed Kullback-Leibler di-\nvergence of some latent prior P(Zt) from Q(Zt|henc\nt\n):\nLz =\nT\nX\nt=1\nKL\n\u0000Q(Zt|henc\nt\n)||P(Zt)\n\u0001\n(10)\nNote that this loss depends upon the latent samples zt\ndrawn from Q(Zt|henc\nt\n), which depend in turn on the input\nx. If the latent distribution is a diagonal Gaussian with \u00b5t,\n\u03c3t as de\ufb01ned in Eqs 1 and 2, a simple choice for P(Zt) is\na standard Gaussian with mean zero and standard deviation\none, in which case Eq. 10 becomes\nLz = 1\n2\n T\nX\nt=1\n\u00b52\nt + \u03c32\nt \u2212log \u03c32\nt\n!\n\u2212T/2\n(11)\nThe total loss L for the network is the expectation of the\nsum of the reconstruction and latent losses:\nL = \u27e8Lx + Lz\u27e9z\u223cQ\n(12)\nwhich we optimise using a single sample of z for each\nstochastic gradient descent step.\nLz can be interpreted as the number of nats required to\ntransmit the latent sample sequence z1:T to the decoder\nfrom the prior, and (if x is discrete) Lx is the number of\nnats required for the decoder to reconstruct x given z1:T .\nThe total loss is therefore equivalent to the expected com-\npression of the data by the decoder and prior.\n2.3. Stochastic Data Generation\nAn image \u02dcx can be generated by a DRAW network by it-\neratively picking latent samples \u02dczt from the prior P, then\nrunning the decoder to update the canvas matrix \u02dcct. After T\nrepetitions of this process the generated image is a sample\nfrom D(X|\u02dccT ):\n\u02dczt \u223cP(Zt)\n(13)\n\u02dchdec\nt\n= RNN dec(\u02dchdec\nt\u22121, \u02dczt)\n(14)\n\u02dcct = \u02dcct\u22121 + write(\u02dchdec\nt\n)\n(15)\n\u02dcx \u223cD(X| \u02dccT )\n(16)\nNote that the encoder is not involved in image generation.\n3. Read and Write Operations\nThe DRAW network described in the previous section is\nnot complete until the read and write operations in Eqs. 4\nand 8 have been de\ufb01ned. This section describes two ways\nto do so, one with selective attention and one without.\nDRAW: A Recurrent Neural Network For Image Generation\n3.1. Reading and Writing Without Attention\nIn the simplest instantiation of DRAW the entire input im-\nage is passed to the encoder at every time-step, and the de-\ncoder modi\ufb01es the entire canvas matrix at every time-step.\nIn this case the read and write operations reduce to\nread(x, \u02c6xt, hdec\nt\u22121) = [x, \u02c6xt]\n(17)\nwrite(hdec\nt\n) = W (hdec\nt\n)\n(18)\nHowever this approach does not allow the encoder to fo-\ncus on only part of the input when creating the latent dis-\ntribution; nor does it allow the decoder to modify only a\npart of the canvas vector. In other words it does not pro-\nvide the network with an explicit selective attention mech-\nanism, which we believe to be crucial to large scale image\ngeneration. We refer to the above con\ufb01guration as \u201cDRAW\nwithout attention\u201d.\n3.2. Selective Attention Model\nTo endow the network with selective attention without sac-\nri\ufb01cing the bene\ufb01ts of gradient descent training, we take in-\nspiration from the differentiable attention mechanisms re-\ncently used in handwriting synthesis (Graves, 2013) and\nNeural Turing Machines (Graves et al., 2014).\nUnlike\nthe aforementioned works, we consider an explicitly two-\ndimensional form of attention, where an array of 2D Gaus-\nsian \ufb01lters is applied to the image, yielding an image\n\u2018patch\u2019 of smoothly varying location and zoom. This con-\n\ufb01guration, which we refer to simply as \u201cDRAW\u201d, some-\nwhat resembles the af\ufb01ne transformations used in computer\ngraphics-based autoencoders (Tieleman, 2014).\nAs illustrated in Fig. 3, the N \u00d7N grid of Gaussian \ufb01lters is\npositioned on the image by specifying the co-ordinates of\nthe grid centre and the stride distance between adjacent \ufb01l-\nters. The stride controls the \u2018zoom\u2019 of the patch; that is, the\nlarger the stride, the larger an area of the original image will\nbe visible in the attention patch, but the lower the effective\nresolution of the patch will be. The grid centre (gX, gY )\nand stride \u03b4 (both of which are real-valued) determine the\nmean location \u00b5i\nX, \u00b5j\nY of the \ufb01lter at row i, column j in the\npatch as follows:\n\u00b5i\nX = gX + (i \u2212N/2 \u22120.5) \u03b4\n(19)\n\u00b5j\nY = gY + (j \u2212N/2 \u22120.5) \u03b4\n(20)\nTwo more parameters are required to fully specify the at-\ntention model: the isotropic variance \u03c32 of the Gaussian\n\ufb01lters, and a scalar intensity \u03b3 that multiplies the \ufb01lter re-\nsponse. Given an A \u00d7 B input image x, all \ufb01ve attention\nparameters are dynamically determined at each time step\n\u03b4\ngY {\n{\ngX\n{\nFigure 3. Left: A 3 \u00d7 3 grid of \ufb01lters superimposed on an image.\nThe stride (\u03b4) and centre location (gX, gY ) are indicated. Right:\nThree N \u00d7 N patches extracted from the image (N = 12). The\ngreen rectangles on the left indicate the boundary and precision\n(\u03c3) of the patches, while the patches themselves are shown to the\nright. The top patch has a small \u03b4 and high \u03c3, giving a zoomed-in\nbut blurry view of the centre of the digit; the middle patch has\nlarge \u03b4 and low \u03c3, effectively downsampling the whole image;\nand the bottom patch has high \u03b4 and \u03c3.\nvia a linear transformation of the decoder output hdec:\n(\u02dcgX, \u02dcgY , log \u03c32, log \u02dc\u03b4, log \u03b3) = W (hdec)\n(21)\ngX = A + 1\n2\n(\u02dcgX + 1)\n(22)\ngY = B + 1\n2\n(\u02dcgY + 1)\n(23)\n\u03b4 = max(A, B) \u22121\nN \u22121\n\u02dc\u03b4\n(24)\nwhere the variance, stride and intensity are emitted in the\nlog-scale to ensure positivity. The scaling of gX, gY and \u03b4\nis chosen to ensure that the initial patch (with a randomly\ninitialised network) roughly covers the whole input image.\nGiven the attention parameters emitted by the decoder, the\nhorizontal and vertical \ufb01lterbank matrices FX and FY (di-\nmensions N \u00d7 A and N \u00d7 B respectively) are de\ufb01ned as\nfollows:\nFX[i, a] =\n1\nZX\nexp\n\u0012\n\u2212(a \u2212\u00b5i\nX)2\n2\u03c32\n\u0013\n(25)\nFY [j, b] =\n1\nZY\nexp\n \n\u2212(b \u2212\u00b5j\nY )2\n2\u03c32\n!\n(26)\nwhere (i, j) is a point in the attention patch, (a, b) is a point\nin the input image, and Zx, Zy are normalisation constants\nthat ensure that P\na FX[i, a] = 1 and P\nb FY [j, b] = 1.\nDRAW: A Recurrent Neural Network For Image Generation\nFigure 4. Zooming. Top Left: The original 100\u00d775 image. Top\nMiddle: A 12 \u00d7 12 patch extracted with 144 2D Gaussian \ufb01lters.\nTop Right: The reconstructed image when applying transposed\n\ufb01lters on the patch. Bottom: Only two 2D Gaussian \ufb01lters are\ndisplayed. The \ufb01rst one is used to produce the top-left patch fea-\nture. The last \ufb01lter is used to produce the bottom-right patch fea-\nture. By using different \ufb01lter weights, the attention can be moved\nto a different location.\n3.3. Reading and Writing With Attention\nGiven FX, FY and intensity \u03b3 determined by hdec\nt\u22121, along\nwith an input image x and error image \u02c6xt, the read opera-\ntion returns the concatenation of two N \u00d7 N patches from\nthe image and error image:\nread(x, \u02c6xt, hdec\nt\u22121) = \u03b3[FY xF T\nX, FY \u02c6xF T\nX]\n(27)\nNote that the same \ufb01lterbanks are used for both the image\nand error image. For the write operation, a distinct set of\nattention parameters \u02c6\u03b3, \u02c6FX and \u02c6FY are extracted from hdec\nt\n,\nthe order of transposition is reversed, and the intensity is\ninverted:\nwt = W (hdec\nt\n)\n(28)\nwrite(hdec\nt\n) = 1\n\u02c6\u03b3\n\u02c6F T\nY wt \u02c6FX\n(29)\nwhere wt is the N \u00d7 N writing patch emitted by hdec\nt\n. For\ncolour images each point in the input and error image (and\nhence in the reading and writing patches) is an RGB triple.\nIn this case the same reading and writing \ufb01lters are used for\nall three channels.\n4. Experimental Results\nWe assess the ability of DRAW to generate realistic-\nlooking images by training on three datasets of progres-\nsively increasing visual complexity: MNIST (LeCun et al.,\n1998), Street View House Numbers (SVHN) (Netzer et al.,\n2011) and CIFAR-10 (Krizhevsky, 2009).\nThe images\ngenerated by the network are always novel (not simply\ncopies of training examples), and are virtually indistin-\nguishable from real data for MNIST and SVHN; the gener-\nated CIFAR images are somewhat blurry, but still contain\nrecognisable structure from natural scenes. The binarized\nMNIST results substantially improve on the state of the art.\nAs a preliminary exercise, we also evaluate the 2D atten-\ntion module of the DRAW network on cluttered MNIST\nclassi\ufb01cation.\nFor all experiments, the model D(X|cT ) of the input data\nwas a Bernoulli distribution with means given by \u03c3(cT ).\nFor the MNIST experiments, the reconstruction loss from\nEq 9 was the usual binary cross-entropy term.\nFor the\nSVHN and CIFAR-10 experiments, the red, green and blue\npixel intensities were represented as numbers between 0\nand 1, which were then interpreted as independent colour\nemission probabilities. The reconstruction loss was there-\nfore the cross-entropy between the pixel intensities and the\nmodel probabilities. Although this approach worked well\nin practice, it means that the training loss did not corre-\nspond to the true compression cost of RGB images.\nNetwork hyper-parameters for all the experiments are\npresented in Table 3.\nThe Adam optimisation algo-\nrithm (Kingma & Ba, 2014) was used throughout.\nEx-\namples of generation sequences for MNIST and SVHN\nare provided in the accompanying video (https://www.\nyoutube.com/watch?v=Zt-7MI9eKEo).\n4.1. Cluttered MNIST Classi\ufb01cation\nTo test the classi\ufb01cation ef\ufb01cacy of the DRAW attention\nmechanism (as opposed to its ability to aid in image gener-\nation), we evaluate its performance on the 100 \u00d7 100 clut-\ntered translated MNIST task (Mnih et al., 2014). Each im-\nage in cluttered MNIST contains many digit-like fragments\nof visual clutter that the network must distinguish from the\ntrue digit to be classi\ufb01ed. As illustrated in Fig. 5, having\nan iterative attention model allows the network to progres-\nsively zoom in on the relevant region of the image, and\nignore the clutter outside it.\nOur model consists of an LSTM recurrent network that re-\nceives a 12 \u00d7 12 \u2018glimpse\u2019 from the input image at each\ntime-step, using the selective read operation de\ufb01ned in Sec-\ntion 3.2. After a \ufb01xed number of glimpses the network uses\na softmax layer to classify the MNIST digit. The network\nis similar to the recently introduced Recurrent Attention\nModel (RAM) (Mnih et al., 2014), except that our attention\nmethod is differentiable; we therefore refer to it as \u201cDiffer-\nentiable RAM\u201d.\nThe results in Table 1 demonstrate a signi\ufb01cant improve-\nment in test error over the original RAM network. More-\nover our model had only a single attention patch at each\nDRAW: A Recurrent Neural Network For Image Generation\nTime\nFigure 5. Cluttered MNIST classi\ufb01cation with attention. Each\nsequence shows a succession of four glimpses taken by the net-\nwork while classifying cluttered translated MNIST. The green\nrectangle indicates the size and location of the attention patch,\nwhile the line width represents the variance of the \ufb01lters.\nTable 1. Classi\ufb01cation test error on 100 \u00d7 100 Cluttered Trans-\nlated MNIST.\nModel\nError\nConvolutional, 2 layers\n14.35%\nRAM, 4 glimpses, 12 \u00d7 12, 4 scales\n9.41%\nRAM, 8 glimpses, 12 \u00d7 12, 4 scales\n8.11%\nDifferentiable RAM, 4 glimpses, 12 \u00d7 12\n4.18%\nDifferentiable RAM, 8 glimpses, 12 \u00d7 12\n3.36%\ntime-step, whereas RAM used four, at different zooms.\n4.2. MNIST Generation\nWe trained the full DRAW network as a generative model\non the binarized MNIST dataset (Salakhutdinov & Mur-\nray, 2008). This dataset has been widely studied in the\nliterature, allowing us to compare the numerical perfor-\nmance (measured in average nats per image on the test\nset) of DRAW with existing methods. Table 2 shows that\nDRAW without selective attention performs comparably to\nother recent generative models such as DARN, NADE and\nDBMs, and that DRAW with attention considerably im-\nproves on the state of the art.\nTable 2. Negative log-likelihood (in nats) per test-set example on\nthe binarised MNIST data set. The right hand column, where\npresent, gives an upper bound (Eq. 12) on the negative log-\nlikelihood. The previous results are from [1] (Salakhutdinov &\nHinton, 2009), [2] (Murray & Salakhutdinov, 2009), [3] (Uria\net al., 2014), [4] (Raiko et al., 2014), [5] (Rezende et al., 2014),\n[6] (Salimans et al., 2014), [7] (Gregor et al., 2014).\nModel\n\u2212log p\n\u2264\nDBM 2hl [1]\n\u224884.62\nDBN 2hl [2]\n\u224884.55\nNADE [3]\n88.33\nEoNADE 2hl (128 orderings) [3]\n85.10\nEoNADE-5 2hl (128 orderings) [4]\n84.68\nDLGM [5]\n\u224886.60\nDLGM 8 leapfrog steps [6]\n\u224885.51\n88.30\nDARN 1hl [7]\n\u224884.13\n88.30\nDARN 12hl [7]\n-\n87.72\nDRAW without attention\n-\n87.40\nDRAW\n-\n80.97\nFigure 6. Generated MNIST images. All digits were generated\nby DRAW except those in the rightmost column, which shows the\ntraining set images closest to those in the column second to the\nright (pixelwise L2 is the distance measure). Note that the net-\nwork was trained on binary samples, while the generated images\nare mean probabilities.\nOnce the DRAW network was trained, we generated\nMNIST digits following the method in Section 2.3, exam-\nples of which are presented in Fig. 6. Fig. 7 illustrates\nthe image generation sequence for a DRAW network with-\nout selective attention (see Section 3.1). It is interesting to\ncompare this with the generation sequence for DRAW with\nattention, as depicted in Fig. 1. Whereas without attention\nit progressively sharpens a blurred image in a global way,\nDRAW: A Recurrent Neural Network For Image Generation\nTime\nFigure 7. MNIST generation sequences for DRAW without at-\ntention. Notice how the network \ufb01rst generates a very blurry im-\nage that is subsequently re\ufb01ned.\nwith attention it constructs the digit by tracing the lines\u2014\nmuch like a person with a pen.\n4.3. MNIST Generation with Two Digits\nThe main motivation for using an attention-based genera-\ntive model is that large images can be built up iteratively,\nby adding to a small part of the image at a time. To test\nthis capability in a controlled fashion, we trained DRAW\nto generate images with two 28 \u00d7 28 MNIST images cho-\nsen at random and placed at random locations in a 60 \u00d7 60\nblack background. In cases where the two digits overlap,\nthe pixel intensities were added together at each point and\nclipped to be no greater than one. Examples of generated\ndata are shown in Fig. 8. The network typically generates\none digit and then the other, suggesting an ability to recre-\nate composite scenes from simple pieces.\n4.4. Street View House Number Generation\nMNIST digits are very simplistic in terms of visual struc-\nture, and we were keen to see how well DRAW performed\non natural images. Our \ufb01rst natural image generation ex-\nperiment used the multi-digit Street View House Numbers\ndataset (Netzer et al., 2011). We used the same preprocess-\ning as (Goodfellow et al., 2013), yielding a 64 \u00d7 64 house\nnumber image for each training example. The network was\nthen trained using 54 \u00d7 54 patches extracted at random lo-\ncations from the preprocessed images. The SVHN training\nset contains 231,053 images, and the validation set contains\n4,701 images.\nThe house number images generated by the network are\nFigure 8. Generated MNIST images with two digits.\nFigure 9. Generated SVHN images.\nThe rightmost column\nshows the training images closest (in L2 distance) to the gener-\nated images beside them. Note that the two columns are visually\nsimilar, but the numbers are generally different.\nhighly realistic, as shown in Figs. 9 and 10. Fig. 11 reveals\nthat, despite the long training time, the DRAW network un-\nder\ufb01t the SVHN training data.\n4.5. Generating CIFAR Images\nThe most challenging dataset we applied DRAW to was\nthe CIFAR-10 collection of natural images (Krizhevsky,\nDRAW: A Recurrent Neural Network For Image Generation\nTable 3. Experimental Hyper-Parameters.\nTask\n#glimpses\nLSTM #h\n#z\nRead Size\nWrite Size\n100 \u00d7 100 MNIST Classi\ufb01cation\n8\n256\n-\n12 \u00d7 12\n-\nMNIST Model\n64\n256\n100\n2 \u00d7 2\n5 \u00d7 5\nSVHN Model\n32\n800\n100\n12 \u00d7 12\n12 \u00d7 12\nCIFAR Model\n64\n400\n200\n5 \u00d7 5\n5 \u00d7 5\ns\nTime\nFigure 10. SVHN Generation Sequences. The red rectangle in-\ndicates the attention patch. Notice how the network draws the dig-\nits one at a time, and how it moves and scales the writing patch to\nproduce numbers with different slopes and sizes.\n 5060\n 5080\n 5100\n 5120\n 5140\n 5160\n 5180\n 5200\n 5220\n 0\n 50\n 100\n 150\n 200\n 250\n 300\n 350\ncost per example\nminibatch number (thousands)\ntraining\nvalidation\nFigure 11. Training and validation cost on SVHN. The valida-\ntion cost is consistently lower because the validation set patches\nwere extracted from the image centre (rather than from random\nlocations, as in the training set). The network was never able to\nover\ufb01t on the training data.\n2009). CIFAR-10 is very diverse, and with only 50,000\ntraining examples it is very dif\ufb01cult to generate realistic-\nFigure 12. Generated CIFAR images.\nThe rightmost column\nshows the nearest training examples to the column beside it.\nlooking objects without over\ufb01tting (in other words, without\ncopying from the training set). Nonetheless the images in\nFig. 12 demonstrate that DRAW is able to capture much of\nthe shape, colour and composition of real photographs.\n5. Conclusion\nThis paper introduced the Deep Recurrent Attentive Writer\n(DRAW) neural network architecture, and demonstrated its\nability to generate highly realistic natural images such as\nphotographs of house numbers, as well as improving on the\nbest known results for binarized MNIST generation. We\nalso established that the two-dimensional differentiable at-\ntention mechanism embedded in DRAW is bene\ufb01cial not\nonly to image generation, but also to image classi\ufb01cation.\nAcknowledgments\nOf the many who assisted in creating this paper, we are es-\npecially thankful to Koray Kavukcuoglu, Volodymyr Mnih,\nJimmy Ba, Yaroslav Bulatov, Greg Wayne, Andrei Rusu\nand Shakir Mohamed.\nDRAW: A Recurrent Neural Network For Image Generation\nReferences\nBa, Jimmy, Mnih, Volodymyr, and Kavukcuoglu, Koray.\nMultiple object recognition with visual attention. arXiv\npreprint arXiv:1412.7755, 2014.\nDayan, Peter, Hinton, Geoffrey E, Neal, Radford M, and\nZemel, Richard S. The helmholtz machine. Neural com-\nputation, 7(5):889\u2013904, 1995.\nDenil, Misha, Bazzani, Loris, Larochelle, Hugo, and\nde Freitas, Nando. Learning where to attend with deep\narchitectures for image tracking. Neural computation,\n24(8):2151\u20132184, 2012.\nGers, Felix A, Schmidhuber, J\u00a8urgen, and Cummins, Fred.\nLearning to forget: Continual prediction with lstm. Neu-\nral computation, 12(10):2451\u20132471, 2000.\nGoodfellow,\nIan J, Bulatov,\nYaroslav,\nIbarz,\nJulian,\nArnoud,\nSacha,\nand\nShet,\nVinay.\nMulti-digit\nnumber recognition from street view imagery using\ndeep convolutional neural networks.\narXiv preprint\narXiv:1312.6082, 2013.\nGraves, Alex. Generating sequences with recurrent neural\nnetworks. arXiv preprint arXiv:1308.0850, 2013.\nGraves, Alex, Wayne, Greg, and Danihelka, Ivo. Neural\nturing machines. arXiv preprint arXiv:1410.5401, 2014.\nGregor, Karol, Danihelka, Ivo, Mnih, Andriy, Blundell,\nCharles, and Wierstra, Daan. Deep autoregressive net-\nworks. In Proceedings of the 31st International Confer-\nence on Machine Learning, 2014.\nHinton, Geoffrey E and Salakhutdinov, Ruslan R. Reduc-\ning the dimensionality of data with neural networks. Sci-\nence, 313(5786):504\u2013507, 2006.\nHochreiter, Sepp and Schmidhuber, J\u00a8urgen. Long short-\nterm memory.\nNeural computation, 9(8):1735\u20131780,\n1997.\nKingma,\nDiederik\nand\nBa,\nJimmy.\nAdam:\nA\nmethod for stochastic optimization.\narXiv preprint\narXiv:1412.6980, 2014.\nKingma, Diederik P and Welling, Max.\nAuto-encoding\nvariational bayes. In Proceedings of the International\nConference on Learning Representations (ICLR), 2014.\nKrizhevsky, Alex.\nLearning multiple layers of features\nfrom tiny images. 2009.\nLarochelle, Hugo and Hinton, Geoffrey E.\nLearning to\ncombine foveal glimpses with a third-order boltzmann\nmachine. In Advances in Neural Information Processing\nSystems, pp. 1243\u20131251. 2010.\nLarochelle, Hugo and Murray, Iain. The neural autoregres-\nsive distribution estimator. Journal of Machine Learning\nResearch, 15:29\u201337, 2011.\nLeCun, Yann, Bottou, L\u00b4eon, Bengio, Yoshua, and Haffner,\nPatrick.\nGradient-based learning applied to document\nrecognition.\nProceedings of the IEEE, 86(11):2278\u2013\n2324, 1998.\nMnih, Andriy and Gregor, Karol. Neural variational infer-\nence and learning in belief networks. In Proceedings of\nthe 31st International Conference on Machine Learning,\n2014.\nMnih, Volodymyr, Heess, Nicolas, Graves, Alex, et al. Re-\ncurrent models of visual attention. In Advances in Neural\nInformation Processing Systems, pp. 2204\u20132212, 2014.\nMurray, Iain and Salakhutdinov, Ruslan. Evaluating prob-\nabilities under high-dimensional latent variable models.\nIn Advances in neural information processing systems,\npp. 1137\u20131144, 2009.\nNetzer, Yuval, Wang, Tao, Coates, Adam, Bissacco,\nAlessandro, Wu, Bo, and Ng, Andrew Y. Reading dig-\nits in natural images with unsupervised feature learning.\n2011.\nRaiko, Tapani, Li, Yao, Cho, Kyunghyun, and Bengio,\nYoshua. Iterative neural autoregressive distribution es-\ntimator nade-k. In Advances in Neural Information Pro-\ncessing Systems, pp. 325\u2013333. 2014.\nRanzato, Marc\u2019Aurelio. On learning where to look. arXiv\npreprint arXiv:1405.5488, 2014.\nRezende, Danilo J, Mohamed, Shakir, and Wierstra, Daan.\nStochastic backpropagation and approximate inference\nin deep generative models. In Proceedings of the 31st In-\nternational Conference on Machine Learning, pp. 1278\u2013\n1286, 2014.\nSalakhutdinov, Ruslan and Hinton, Geoffrey E. Deep boltz-\nmann machines. In International Conference on Arti\ufb01-\ncial Intelligence and Statistics, pp. 448\u2013455, 2009.\nSalakhutdinov, Ruslan and Murray, Iain. On the quantita-\ntive analysis of Deep Belief Networks. In Proceedings\nof the 25th Annual International Conference on Machine\nLearning, pp. 872\u2013879. Omnipress, 2008.\nSalimans, Tim, Kingma, Diederik P, and Welling, Max.\nMarkov chain monte carlo and variational inference:\nBridging the gap. arXiv preprint arXiv:1410.6460, 2014.\nSermanet, Pierre, Frome, Andrea, and Real, Esteban. At-\ntention for \ufb01ne-grained categorization. arXiv preprint\narXiv:1412.7054, 2014.\nDRAW: A Recurrent Neural Network For Image Generation\nSutskever, Ilya, Vinyals, Oriol, and Le, Quoc VV.\nSe-\nquence to sequence learning with neural networks. In\nAdvances in Neural Information Processing Systems, pp.\n3104\u20133112, 2014.\nTang, Yichuan, Srivastava, Nitish, and Salakhutdinov, Rus-\nlan. Learning generative models with visual attention.\narXiv preprint arXiv:1312.6110, 2013.\nTieleman, Tijmen. Optimizing Neural Networks that Gen-\nerate Images. PhD thesis, University of Toronto, 2014.\nUria, Benigno, Murray, Iain, and Larochelle, Hugo. A deep\nand tractable density estimator. In Proceedings of the\n31st International Conference on Machine Learning, pp.\n467\u2013475, 2014.\nZheng, Yin, Zemel, Richard S, Zhang, Yu-Jin, and\nLarochelle, Hugo.\nA neural autoregressive approach\nto attention-based recognition. International Journal of\nComputer Vision, pp. 1\u201313, 2014.\n",
        "sentence": " Recently, stochastic neural networks (Bengio et al., 2014; Kingma et al., 2014; Gregor et al., 2015) have become popular, and deterministic networks are being used for image generation tasks (Dosovitskiy et al.",
        "context": "arXiv preprint arXiv:1312.6110, 2013.\nTieleman, Tijmen. Optimizing Neural Networks that Gen-\nerate Images. PhD thesis, University of Toronto, 2014.\nUria, Benigno, Murray, Iain, and Larochelle, Hugo. A deep\nnetworks. arXiv preprint arXiv:1308.0850, 2013.\nGraves, Alex, Wayne, Greg, and Danihelka, Ivo. Neural\nturing machines. arXiv preprint arXiv:1410.5401, 2014.\nGregor, Karol, Danihelka, Ivo, Mnih, Andriy, Blundell,\nDRAW: A Recurrent Neural Network For Image Generation\nKarol Gregor\nKAROLG@GOOGLE.COM\nIvo Danihelka\nDANIHELKA@GOOGLE.COM\nAlex Graves\nGRAVESA@GOOGLE.COM\nDanilo Jimenez Rezende\nDANILOR@GOOGLE.COM\nDaan Wierstra\nWIERSTRA@GOOGLE.COM\nGoogle DeepMind\nAbstract"
    },
    {
        "title": "Reducing the dimensionality of data with neural",
        "author": [
            "G.E. Hinton",
            "R.R. Salakhutdinov"
        ],
        "venue": "networks. Science,",
        "citeRegEx": "Hinton and Salakhutdinov.,? \\Q2006\\E",
        "shortCiteRegEx": "Hinton and Salakhutdinov.",
        "year": 2006,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Learning and relearning in boltzmann machines. In Parallel Distributed Processing: Volume 1: Foundations, pp. 282\u2013317",
        "author": [
            "G.E. Hinton",
            "T.J. Sejnowski"
        ],
        "venue": null,
        "citeRegEx": "Hinton and Sejnowski.,? \\Q1986\\E",
        "shortCiteRegEx": "Hinton and Sejnowski.",
        "year": 1986,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "A fast learning algorithm for deep belief nets",
        "author": [
            "G.E. Hinton",
            "S. Osindero",
            "Y.-W. Teh"
        ],
        "venue": "Neural Comput.,",
        "citeRegEx": "Hinton et al\\.,? \\Q2006\\E",
        "shortCiteRegEx": "Hinton et al\\.",
        "year": 2006,
        "abstract": " We show how to use \u201ccomplementary priors\u201d to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind. ",
        "full_text": "",
        "sentence": " A prominent class of probabilistic models of images are restricted Boltzmann machines (Hinton & Sejnowski, 1986; Smolensky, 1987; Hinton & Salakhutdinov, 2006) and their deep variants (Hinton et al., 2006; Salakhutdinov & Hinton, 2009; Lee et al., 2009).",
        "context": null
    },
    {
        "title": "Caffe: Convolutional architecture for fast feature embedding",
        "author": [
            "Y. Jia",
            "E. Shelhamer",
            "J. Donahue",
            "S. Karayev",
            "J. Long",
            "R. Girshick",
            "S. Guadarrama",
            "T. Darrell"
        ],
        "venue": null,
        "citeRegEx": "Jia et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Jia et al\\.",
        "year": 2014,
        "abstract": "Caffe provides multimedia scientists and practitioners with a clean and\nmodifiable framework for state-of-the-art deep learning algorithms and a\ncollection of reference models. The framework is a BSD-licensed C++ library\nwith Python and MATLAB bindings for training and deploying general-purpose\nconvolutional neural networks and other deep models efficiently on commodity\narchitectures. Caffe fits industry and internet-scale media needs by CUDA GPU\ncomputation, processing over 40 million images a day on a single K40 or Titan\nGPU ($\\approx$ 2.5 ms per image). By separating model representation from\nactual implementation, Caffe allows experimentation and seamless switching\namong platforms for ease of development and deployment from prototyping\nmachines to cloud environments. Caffe is maintained and developed by the\nBerkeley Vision and Learning Center (BVLC) with the help of an active community\nof contributors on GitHub. It powers ongoing research projects, large-scale\nindustrial applications, and startup prototypes in vision, speech, and\nmultimedia.",
        "full_text": "Caffe: Convolutional Architecture\nfor Fast Feature Embedding\n\u2217\nYangqing Jia\u2217, Evan Shelhamer\u2217, Jeff Donahue, Sergey Karayev,\nJonathan Long, Ross Girshick, Sergio Guadarrama, Trevor Darrell\nUC Berkeley EECS, Berkeley, CA 94702\n{jiayq,shelhamer,jdonahue,sergeyk,jonlong,rbg,sguada,trevor}@eecs.berkeley.edu\nABSTRACT\nCa\ufb00e provides multimedia scientists and practitioners with\na clean and modi\ufb01able framework for state-of-the-art deep\nlearning algorithms and a collection of reference models.\nThe framework is a BSD-licensed C++ library with Python\nand MATLAB bindings for training and deploying general-\npurpose convolutional neural networks and other deep mod-\nels e\ufb03ciently on commodity architectures. Ca\ufb00e \ufb01ts indus-\ntry and internet-scale media needs by CUDA GPU computa-\ntion, processing over 40 million images a day on a single K40\nor Titan GPU (\u22482.5 ms per image). By separating model\nrepresentation from actual implementation, Ca\ufb00e allows ex-\nperimentation and seamless switching among platforms for\nease of development and deployment from prototyping ma-\nchines to cloud environments.\nCa\ufb00e is maintained and developed by the Berkeley Vi-\nsion and Learning Center (BVLC) with the help of an ac-\ntive community of contributors on GitHub. It powers on-\ngoing research projects, large-scale industrial applications,\nand startup prototypes in vision, speech, and multimedia.\nCategories and Subject Descriptors\nI.5.1 [Pattern Recognition]: [Applications\u2013Computer vi-\nsion]; D.2.2 [Software Engineering]: [Design Tools and\nTechniques\u2013Software libraries]; I.5.1 [Pattern Recognition]:\n[Models\u2013Neural Nets]\nGeneral Terms\nAlgorithms, Design, Experimentation\nKeywords\nOpen Source, Computer Vision, Neural Networks, Parallel\nComputation, Machine Learning\n\u2217Corresponding Authors.\nThe work was done while\nYangqing Jia was a graduate student at Berkeley.\nHe is\ncurrently a research scientist at Google, 1600 Amphitheater\nPkwy, Mountain View, CA 94043.\n.\n1.\nINTRODUCTION\nA key problem in multimedia data analysis is discovery of\ne\ufb00ective representations for sensory inputs\u2014images, sound-\nwaves, haptics, etc.\nWhile performance of conventional,\nhandcrafted features has plateaued in recent years, new de-\nvelopments in deep compositional architectures have kept\nperformance levels rising [8].\nDeep models have outper-\nformed hand-engineered feature representations in many do-\nmains, and made learning possible in domains where engi-\nneered features were lacking entirely.\nWe are particularly motivated by large-scale visual recog-\nnition, where a speci\ufb01c type of deep architecture has achieved\na commanding lead on the state-of-the-art.\nThese Con-\nvolutional Neural Networks, or CNNs, are discriminatively\ntrained via back-propagation through layers of convolutional\n\ufb01lters and other operations such as recti\ufb01cation and pooling.\nFollowing the early success of digit classi\ufb01cation in the 90\u2019s,\nthese models have recently surpassed all known methods for\nlarge-scale visual recognition, and have been adopted by in-\ndustry heavyweights such as Google, Facebook, and Baidu\nfor image understanding and search.\nWhile deep neural networks have attracted enthusiastic\ninterest within computer vision and beyond, replication of\npublished results can involve months of work by a researcher\nor engineer. Sometimes researchers deem it worthwhile to\nrelease trained models along with the paper advertising their\nperformance. But trained models alone are not su\ufb03cient for\nrapid research progress and emerging commercial applica-\ntions, and few toolboxes o\ufb00er truly o\ufb00-the-shelf deployment\nof state-of-the-art models\u2014and those that do are often not\ncomputationally e\ufb03cient and thus unsuitable for commercial\ndeployment.\nTo address such problems, we present Ca\ufb00e, a fully open-\nsource framework that a\ufb00ords clear access to deep architec-\ntures.\nThe code is written in clean, e\ufb03cient C++, with\nCUDA used for GPU computation, and nearly complete,\nwell-supported bindings to Python/Numpy and MATLAB.\nCa\ufb00e adheres to software engineering best practices, pro-\nviding unit tests for correctness and experimental rigor and\nspeed for deployment. It is also well-suited for research use,\ndue to the careful modularity of the code, and the clean sep-\naration of network de\ufb01nition (usually the novel part of deep\nlearning research) from actual implementation.\nIn Ca\ufb00e, multimedia scientists and practitioners have an\norderly and extensible toolkit for state-of-the-art deep learn-\ning algorithms, with reference models provided out of the\nbox. Fast CUDA code and GPU computation \ufb01t industry\nneeds by achieving processing speeds of more than 40 mil-\narXiv:1408.5093v1  [cs.CV]  20 Jun 2014\nCore\nOpen\nPretrained\nFramework\nLicense\nlanguage\nBinding(s)\nCPU\nGPU\nsource\nTraining\nmodels\nDevelopment\nCa\ufb00e\nBSD\nC++\nPython,\ndistributed\nMATLAB\ncuda-convnet [7]\nunspeci\ufb01ed\nC++\nPython\ndiscontinued\nDecaf [2]\nBSD\nPython\ndiscontinued\nOverFeat [9]\nunspeci\ufb01ed\nLua\nC++,Python\ncentralized\nTheano/Pylearn2 [4]\nBSD\nPython\ndistributed\nTorch7 [1]\nBSD\nLua\ndistributed\nTable 1: Comparison of popular deep learning frameworks. Core language is the main library language, while\nbindings have an o\ufb03cially supported library interface for feature extraction, training, etc. CPU indicates\navailability of host-only computation, no GPU usage (e.g., for cluster deployment); GPU indicates the GPU\ncomputation capability essential for training modern CNNs.\nlion images per day on a single K40 or Titan GPU. The\nsame models can be run in CPU or GPU mode on a vari-\nety of hardware: Ca\ufb00e separates the representation from the\nactual implementation, and seamless switching between het-\nerogeneous platforms furthers development and deployment\u2014\nCa\ufb00e can even be run in the cloud.\nWhile Ca\ufb00e was \ufb01rst designed for vision, it has been adopted\nand improved by users in speech recognition, robotics, neu-\nroscience, and astronomy. We hope to see this trend con-\ntinue so that further sciences and industries can take advan-\ntage of deep learning.\nCa\ufb00e is maintained and developed by the BVLC with the\nactive e\ufb00orts of several graduate students, and welcomes\nopen-source contributions at http://github.com/BVLC/caffe.\nWe thank all of our contributors for their work!\n2.\nHIGHLIGHTS OF CAFFE\nCa\ufb00e provides a complete toolkit for training, testing,\n\ufb01netuning, and deploying models, with well-documented ex-\namples for all of these tasks. As such, it\u2019s an ideal starting\npoint for researchers and other developers looking to jump\ninto state-of-the-art machine learning. At the same time,\nit\u2019s likely the fastest available implementation of these algo-\nrithms, making it immediately useful for industrial deploy-\nment.\nModularity. The software is designed from the begin-\nning to be as modular as possible, allowing easy extension to\nnew data formats, network layers, and loss functions. Lots\nof layers and loss functions are already implemented, and\nplentiful examples show how these are composed into train-\nable recognition systems for various tasks.\nSeparation of representation and implementation.\nCa\ufb00e model de\ufb01nitions are written as con\ufb01g \ufb01les using the\nProtocol Bu\ufb00er language.\nCa\ufb00e supports network archi-\ntectures in the form of arbitrary directed acyclic graphs.\nUpon instantiation, Ca\ufb00e reserves exactly as much memory\nas needed for the network, and abstracts from its underly-\ning location in host or GPU. Switching between a CPU and\nGPU implementation is exactly one function call.\nTest coverage. Every single module in Ca\ufb00e has a test,\nand no new code is accepted into the project without corre-\nsponding tests. This allows rapid improvements and refac-\ntoring of the codebase, and imparts a welcome feeling of\npeacefulness to the researchers using the code.\nPython and MATLAB bindings.\nFor rapid proto-\ntyping and interfacing with existing research code, Ca\ufb00e\nprovides Python and MATLAB bindings. Both languages\nmay be used to construct networks and classify inputs. The\nPython bindings also expose the solver module for easy pro-\ntotyping of new training procedures.\nPre-trained reference models. Ca\ufb00e provides (for aca-\ndemic and non-commercial use\u2014not BSD license) reference\nmodels for visual tasks, including the landmark \u201cAlexNet\u201d\nImageNet model [8] with variations and the R-CNN detec-\ntion model [3].\nMore are scheduled for release.\nWe are\nstrong proponents of reproducible research: we hope that\na common software substrate will foster quick progress in\nthe search over network architectures and applications.\n2.1\nComparison to related software\nWe summarize the landscape of convolutional neural net-\nwork software used in recent publications in Table 1. While\nour list is incomplete, we have included the toolkits that are\nmost notable to the best of our knowledge. Ca\ufb00e di\ufb00ers from\nother contemporary CNN frameworks in two major ways:\n(1) The implementation is completely C++ based, which\neases integration into existing C++ systems and interfaces\ncommon in industry. The CPU mode removes the barrier of\nspecialized hardware for deployment and experiments once\na model is trained.\n(2) Reference models are provided o\ufb00-the-shelf for quick\nexperimentation with state-of-the-art results, without the\nneed for costly re-learning. By \ufb01netuning for related tasks,\nsuch as those explored by [2], these models provide a warm-\nstart to new research and applications. Crucially, we publish\nnot only the trained models but also the recipes and code\nto reproduce them.\n3.\nARCHITECTURE\n3.1\nData Storage\nCa\ufb00e stores and communicates data in 4-dimensional ar-\nrays called blobs.\nBlobs provide a uni\ufb01ed memory interface, holding batches\nof images (or other data), parameters, or parameter updates.\nBlobs conceal the computational and mental overhead of\nmixed CPU/GPU operation by synchronizing from the CPU\nhost to the GPU device as needed. In practice, one loads\ndata from the disk to a blob in CPU code, calls a CUDA\nkernel to do GPU computation, and ferries the blob o\ufb00to\nthe next layer, ignoring low-level details while maintaining\na high level of performance. Memory on the host and device\nis allocated on demand (lazily) for e\ufb03cient memory usage.\n\u0001\u0002\u0003\u0004\u0005\u0001\u0006\u0006\u0007\b\u0002\b\t\n\u000b\f\r\u000e\n\u0001\u0002\u0003\n\f\t\u0006\u000f\u0003\u0004\u0005\f\t\u0006\u000f\u000e\n\f\t\u0006\u000f\u0003\n\u0002\t\t\u0010\u0011\n\n\u0012\r\u0012\n\f\t\u0006\u000f\u0011\u0004\u0005\f\t\u0006\u000f\u000e\n\u0002\t\t\u0010\u0003\n\u0001\u0002\u0011\u0004\u0005\u0001\u0006\u0006\u0007\b\u0002\b\t\n\u000b\f\r\u000e\n\u0010\t\u0013\u0013\u0004\u0005\u0013\t\u0014\r\u0015\u0012\u0016\u0017\u0010\t\u0013\u0013\u000e\n\u0001\u0002\u0011\n\u0015\u0006\u0001\u0013\r\u0018\u0001\u0006\u0002\u000b\r\u0004\u0005\n\u0012\r\u0012\u000e\n\u0010\u0012\u0019\u0007\u0010\n\f\t\u0006\u000f\u0011\n\u0002\t\t\u0010\u0011\u0004\u0005\u0002\t\t\u0010\u000e\n\u0002\t\t\u0010\u0003\u0004\u0005\u0002\t\t\u0010\u000e\n\b\u0007\u0010\u000b\u0011\u0004\u0005\b\u0007\u0010\u000b\u000e\n\b\u0007\u0010\u000b\u0011\nFigure 1: An MNIST digit classi\ufb01cation example of a Ca\ufb00e network, where blue boxes represent layers and\nyellow octagons represent data blobs produced by or fed into the layers.\nModels are saved to disk as Google Protocol Bu\ufb00ers1,\nwhich have several important features: minimal-size binary\nstrings when serialized, e\ufb03cient serialization, a human-readable\ntext format compatible with the binary version, and e\ufb03-\ncient interface implementations in multiple languages, most\nnotably C++ and Python.\nLarge-scale data is stored in LevelDB2 databases. In our\ntest program, LevelDB and Protocol Bu\ufb00ers provide a through-\nput of 150MB/s on commodity machines with minimal CPU\nimpact. Thanks to layer-wise design (discussed below) and\ncode modularity, we have recently added support for other\ndata sources, including some contributed by the open source\ncommunity.\n3.2\nLayers\nA Ca\ufb00e layer is the essence of a neural network layer: it\ntakes one or more blobs as input, and yields one or more\nblobs as output. Layers have two key responsibilities for the\noperation of the network as a whole: a forward pass that\ntakes the inputs and produces the outputs, and a backward\npass that takes the gradient with respect to the output, and\ncomputes the gradients with respect to the parameters and\nto the inputs, which are in turn back-propagated to earlier\nlayers.\nCa\ufb00e provides a complete set of layer types including: con-\nvolution, pooling, inner products, nonlinearities like recti\ufb01ed\nlinear and logistic, local response normalization, element-\nwise operations, and losses like softmax and hinge. These are\nall the types needed for state-of-the-art visual tasks. Coding\ncustom layers requires minimal e\ufb00ort due to the composi-\ntional construction of networks.\n3.3\nNetworks and Run Mode\nCa\ufb00e does all the bookkeeping for any directed acyclic\ngraph of layers, ensuring correctness of the forward and\nbackward passes. Ca\ufb00e models are end-to-end machine learn-\ning systems. A typical network begins with a data layer that\nloads from disk and ends with a loss layer that computes the\nobjective for a task such as classi\ufb01cation or reconstruction.\nThe network is run on CPU or GPU by setting a single\nswitch.\nLayers come with corresponding CPU and GPU\nroutines that produce identical results (with tests to prove\nit). The CPU/GPU switch is seamless and independent of\nthe model de\ufb01nition.\n3.4\nTraining A Network\nCa\ufb00e trains models by the fast and standard stochastic\ngradient descent algorithm.\nFigure 1 shows a typical ex-\nample of a Ca\ufb00e network (for MNIST digit classi\ufb01cation)\nduring training: a data layer fetches the images and labels\n1https://code.google.com/p/protobuf/\n2https://code.google.com/p/leveldb/\nFigure 2: An example of the Ca\ufb00e object classi\ufb01ca-\ntion demo. Try it out yourself online!\nfrom disk, passes it through multiple layers such as con-\nvolution, pooling and recti\ufb01ed linear transforms, and feeds\nthe \ufb01nal prediction into a classi\ufb01cation loss layer that pro-\nduces the loss and gradients which train the whole network.\nThis example is found in the Ca\ufb00e source code at exam-\nples/lenet/lenet_train.prototxt. Data are processed in\nmini-batches that pass through the network sequentially. Vi-\ntal to training are learning rate decay schedules, momentum,\nand snapshots for stopping and resuming, all of which are\nimplemented and documented.\nFinetuning, the adaptation of an existing model to new\narchitectures or data, is a standard method in Ca\ufb00e. From\na snapshot of an existing network and a model de\ufb01nition for\nthe new network, Ca\ufb00e \ufb01netunes the old model weights for\nthe new task and initializes new weights as needed. This\ncapability is essential for tasks such as knowledge transfer\n[2], object detection [3], and object retrieval [5].\n4.\nAPPLICATIONS AND EXAMPLES\nIn its \ufb01rst six months since public release, Ca\ufb00e has al-\nready been used in a large number of research projects at\nUC Berkeley and other universities, achieving state-of-the-\nart performance on a number of tasks. Members of Berkeley\nEECS have also collaborated with several industry partners\nsuch as Facebook [11] and Adobe [6], using Ca\ufb00e or its direct\nprecursor (Decaf) to obtain state-of-the-art results.\nObject Classi\ufb01cation Ca\ufb00e has an online demo3 show-\ning state-of-the-art object classi\ufb01cation on images provided\nby the users, including via mobile phone. The demo takes\nthe image and tries to categorize it into one of the 1,000 Im-\nageNet categories4. A typical classi\ufb01cation result is shown\nin Figure 2.\nFurthermore, we have successfully trained a model with\nall 10,000 categories of the full ImageNet dataset by \ufb01ne-\ntuning this network. The resulting network has been applied\nto open vocabulary object retrieval [5].\n3http://demo.caffe.berkeleyvision.org/\n4http://www.image-net.org/challenges/LSVRC/2013/\nFigure 3:\nFeatures extracted from a deep network,\nvisualized in a 2-dimensional space. Note the clear\nseparation between categories, indicative of a suc-\ncessful embedding.\nLearning Semantic Features In addition to end-to-end\ntraining, Ca\ufb00e can also be used to extract semantic features\nfrom images using a pre-trained network.\nThese features\ncan be used \u201cdownstream\u201d in other vision tasks with great\nsuccess [2]. Figure 3 shows a two-dimensional embedding\nof all the ImageNet validation images, colored by a coarse\ncategory that they come from. The nice separation testi\ufb01es\nto a successful semantic embedding.\nIntriguingly, this learned feature is useful for a lot more\nthan object categories. For example, Karayev et al. have\nshown promising results \ufb01nding images of di\ufb00erent styles\nsuch as \u201cVintage\u201d and \u201cRomantic\u201d using Ca\ufb00e features (Fig-\nure 4) [6].\nEthereal\nHDR\nMelancholy\nMinimal\nFigure 4:\nTop three most-con\ufb01dent positive pre-\ndictions on the Flickr Style dataset, using a Ca\ufb00e-\ntrained classi\ufb01er.\nObject Detection Most notably, Ca\ufb00e has enabled us\nto obtain by far the best performance on object detection,\nevaluated on the hardest academic datasets: the PASCAL\nVOC 2007-2012 and the ImageNet 2013 Detection challenge\n[3].\nGirshick et al. [3] have combined Ca\ufb00e together with tech-\nniques such as Selective Search [10] to e\ufb00ectively perform\nsimultaneous localization and recognition in natural images.\nFigure 5 shows a sketch of their approach.\nBeginners\u2019 Guides To help users get started with in-\nstalling, using, and modifying Ca\ufb00e, we have provided in-\nstructions and tutorials on the Ca\ufb00e webpage. The tuto-\nrials range from small demos (MNIST digit recognition) to\nserious deployments (end-to-end learning on ImageNet).\nAlthough these tutorials serve as e\ufb00ective documentation\nof the functionality of Ca\ufb00e, the Ca\ufb00e source code addition-\nally provides detailed inline documentation on all modules.\n1. Input \nimage\n2. Extract region \nproposals (~2k)\n3. Compute \nCNN features\naeroplane? no.\n...\nperson? yes.\ntvmonitor? no.\n4. Classify \nregions\nwarped region\n...\nCNN\nR-CNN: Regions with CNN features\nFigure 5: The R-CNN pipeline that uses Ca\ufb00e for\nobject detection.\nThis documentation will be exposed in a standalone web\ninterface in the near future.\n5.\nAVAILABILITY\nSource code is published BSD-licensed on GitHub.5 Project\ndetails, step-wise tutorials, and pre-trained models are on\nthe homepage.6 Development is done in Linux and OS X,\nand users have reported Windows builds.\nA public Ca\ufb00e\nAmazon EC2 instance is coming soon.\n6.\nACKNOWLEDGEMENTS\nWe would like to thank NVIDIA for GPU donation, the\nBVLC sponsors (http://bvlc.eecs.berkeley.edu/), and\nour open source community.\n7.\nREFERENCES\n[1] R. Collobert, K. Kavukcuoglu, and C. Farabet. Torch7: A\nMATLAB-like environment for machine learning. In\nBigLearn, NIPS Workshop, 2011.\n[2] J. Donahue, Y. Jia, O. Vinyals, J. Ho\ufb00man, N. Zhang,\nE. Tzeng, and T. Darrell. Decaf: A deep convolutional\nactivation feature for generic visual recognition. In ICML,\n2014.\n[3] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich\nfeature hierarchies for accurate object detection and\nsemantic segmentation. In CVPR, 2014.\n[4] I. Goodfellow, D. Warde-Farley, P. Lamblin, V. Dumoulin,\nM. Mirza, R. Pascanu, J. Bergstra, F. Bastien, and\nY. Bengio. Pylearn2: a machine learning research library.\narXiv preprint 1308.4214, 2013.\n[5] S. Guadarrama, E. Rodner, K. Saenko, N. Zhang,\nR. Farrell, J. Donahue, and T. Darrell. Open-vocabulary\nobject retrieval. In RSS, 2014.\n[6] S. Karayev, M. Trentacoste, H. Han, A. Agarwala,\nT. Darrell, A. Hertzmann, and H. Winnemoeller.\nRecognizing image style. arXiv preprint 1311.3715, 2013.\n[7] A. Krizhevsky. cuda-convnet.\nhttps://code.google.com/p/cuda-convnet/, 2012.\n[8] A. Krizhevsky, I. Sutskever, and G. Hinton. ImageNet\nclassi\ufb01cation with deep convolutional neural networks. In\nNIPS, 2012.\n[9] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,\nand Y. LeCun. Overfeat: Integrated recognition,\nlocalization and detection using convolutional networks. In\nICLR, 2014.\n[10] J. Uijlings, K. van de Sande, T. Gevers, and A. Smeulders.\nSelective search for object recognition. IJCV, 2013.\n[11] N. Zhang, M. Paluri, M. Ranzato, T. Darrell, and\nL. Bourdev. Panda: Pose aligned networks for deep\nattribute modeling. In CVPR, 2014.\n5https://github.com/BVLC/caffe/\n6http://caffe.berkeleyvision.org/\n",
        "sentence": " We modified the caffe (Jia et al., 2014) framework to train the networks.",
        "context": "Caffe: Convolutional Architecture\nfor Fast Feature Embedding\n\u2217\nYangqing Jia\u2217, Evan Shelhamer\u2217, Jeff Donahue, Sergey Karayev,\nJonathan Long, Ross Girshick, Sergio Guadarrama, Trevor Darrell\nUC Berkeley EECS, Berkeley, CA 94702\ndue to the careful modularity of the code, and the clean sep-\naration of network de\ufb01nition (usually the novel part of deep\nlearning research) from actual implementation.\nIn Ca\ufb00e, multimedia scientists and practitioners have an\nWe thank all of our contributors for their work!\n2.\nHIGHLIGHTS OF CAFFE\nCa\ufb00e provides a complete toolkit for training, testing,\n\ufb01netuning, and deploying models, with well-documented ex-\namples for all of these tasks. As such, it\u2019s an ideal starting"
    },
    {
        "title": "Adam: A method for stochastic optimization",
        "author": [
            "D.P. Kingma",
            "J. Ba"
        ],
        "venue": "In ICLR,",
        "citeRegEx": "Kingma and Ba.,? \\Q2015\\E",
        "shortCiteRegEx": "Kingma and Ba.",
        "year": 2015,
        "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization\nof stochastic objective functions, based on adaptive estimates of lower-order\nmoments. The method is straightforward to implement, is computationally\nefficient, has little memory requirements, is invariant to diagonal rescaling\nof the gradients, and is well suited for problems that are large in terms of\ndata and/or parameters. The method is also appropriate for non-stationary\nobjectives and problems with very noisy and/or sparse gradients. The\nhyper-parameters have intuitive interpretations and typically require little\ntuning. Some connections to related algorithms, on which Adam was inspired, are\ndiscussed. We also analyze the theoretical convergence properties of the\nalgorithm and provide a regret bound on the convergence rate that is comparable\nto the best known results under the online convex optimization framework.\nEmpirical results demonstrate that Adam works well in practice and compares\nfavorably to other stochastic optimization methods. Finally, we discuss AdaMax,\na variant of Adam based on the infinity norm.",
        "full_text": "Published as a conference paper at ICLR 2015\nADAM: A METHOD FOR STOCHASTIC OPTIMIZATION\nDiederik P. Kingma*\nUniversity of Amsterdam, OpenAI\ndpkingma@openai.com\nJimmy Lei Ba\u2217\nUniversity of Toronto\njimmy@psi.utoronto.ca\nABSTRACT\nWe introduce Adam, an algorithm for \ufb01rst-order gradient-based optimization of\nstochastic objective functions, based on adaptive estimates of lower-order mo-\nments. The method is straightforward to implement, is computationally ef\ufb01cient,\nhas little memory requirements, is invariant to diagonal rescaling of the gradients,\nand is well suited for problems that are large in terms of data and/or parameters.\nThe method is also appropriate for non-stationary objectives and problems with\nvery noisy and/or sparse gradients. The hyper-parameters have intuitive interpre-\ntations and typically require little tuning. Some connections to related algorithms,\non which Adam was inspired, are discussed. We also analyze the theoretical con-\nvergence properties of the algorithm and provide a regret bound on the conver-\ngence rate that is comparable to the best known results under the online convex\noptimization framework. Empirical results demonstrate that Adam works well in\npractice and compares favorably to other stochastic optimization methods. Finally,\nwe discuss AdaMax, a variant of Adam based on the in\ufb01nity norm.\n1\nINTRODUCTION\nStochastic gradient-based optimization is of core practical importance in many \ufb01elds of science and\nengineering. Many problems in these \ufb01elds can be cast as the optimization of some scalar parameter-\nized objective function requiring maximization or minimization with respect to its parameters. If the\nfunction is differentiable w.r.t. its parameters, gradient descent is a relatively ef\ufb01cient optimization\nmethod, since the computation of \ufb01rst-order partial derivatives w.r.t. all the parameters is of the same\ncomputational complexity as just evaluating the function. Often, objective functions are stochastic.\nFor example, many objective functions are composed of a sum of subfunctions evaluated at different\nsubsamples of data; in this case optimization can be made more ef\ufb01cient by taking gradient steps\nw.r.t. individual subfunctions, i.e. stochastic gradient descent (SGD) or ascent. SGD proved itself\nas an ef\ufb01cient and effective optimization method that was central in many machine learning success\nstories, such as recent advances in deep learning (Deng et al., 2013; Krizhevsky et al., 2012; Hinton\n& Salakhutdinov, 2006; Hinton et al., 2012a; Graves et al., 2013). Objectives may also have other\nsources of noise than data subsampling, such as dropout (Hinton et al., 2012b) regularization. For\nall such noisy objectives, ef\ufb01cient stochastic optimization techniques are required. The focus of this\npaper is on the optimization of stochastic objectives with high-dimensional parameters spaces. In\nthese cases, higher-order optimization methods are ill-suited, and discussion in this paper will be\nrestricted to \ufb01rst-order methods.\nWe propose Adam, a method for ef\ufb01cient stochastic optimization that only requires \ufb01rst-order gra-\ndients with little memory requirement. The method computes individual adaptive learning rates for\ndifferent parameters from estimates of \ufb01rst and second moments of the gradients; the name Adam\nis derived from adaptive moment estimation. Our method is designed to combine the advantages\nof two recently popular methods: AdaGrad (Duchi et al., 2011), which works well with sparse gra-\ndients, and RMSProp (Tieleman & Hinton, 2012), which works well in on-line and non-stationary\nsettings; important connections to these and other stochastic optimization methods are clari\ufb01ed in\nsection 5. Some of Adam\u2019s advantages are that the magnitudes of parameter updates are invariant to\nrescaling of the gradient, its stepsizes are approximately bounded by the stepsize hyperparameter,\nit does not require a stationary objective, it works with sparse gradients, and it naturally performs a\nform of step size annealing.\n\u2217Equal contribution. Author ordering determined by coin \ufb02ip over a Google Hangout.\n1\narXiv:1412.6980v9  [cs.LG]  30 Jan 2017\nPublished as a conference paper at ICLR 2015\nAlgorithm 1: Adam, our proposed algorithm for stochastic optimization. See section 2 for details,\nand for a slightly more ef\ufb01cient (but less clear) order of computation. g2\nt indicates the elementwise\nsquare gt \u2299gt. Good default settings for the tested machine learning problems are \u03b1 = 0.001,\n\u03b21 = 0.9, \u03b22 = 0.999 and \u03f5 = 10\u22128. All operations on vectors are element-wise. With \u03b2t\n1 and \u03b2t\n2\nwe denote \u03b21 and \u03b22 to the power t.\nRequire: \u03b1: Stepsize\nRequire: \u03b21, \u03b22 \u2208[0, 1): Exponential decay rates for the moment estimates\nRequire: f(\u03b8): Stochastic objective function with parameters \u03b8\nRequire: \u03b80: Initial parameter vector\nm0 \u21900 (Initialize 1st moment vector)\nv0 \u21900 (Initialize 2nd moment vector)\nt \u21900 (Initialize timestep)\nwhile \u03b8t not converged do\nt \u2190t + 1\ngt \u2190\u2207\u03b8ft(\u03b8t\u22121) (Get gradients w.r.t. stochastic objective at timestep t)\nmt \u2190\u03b21 \u00b7 mt\u22121 + (1 \u2212\u03b21) \u00b7 gt (Update biased \ufb01rst moment estimate)\nvt \u2190\u03b22 \u00b7 vt\u22121 + (1 \u2212\u03b22) \u00b7 g2\nt (Update biased second raw moment estimate)\nbmt \u2190mt/(1 \u2212\u03b2t\n1) (Compute bias-corrected \ufb01rst moment estimate)\nbvt \u2190vt/(1 \u2212\u03b2t\n2) (Compute bias-corrected second raw moment estimate)\n\u03b8t \u2190\u03b8t\u22121 \u2212\u03b1 \u00b7 bmt/(\u221abvt + \u03f5) (Update parameters)\nend while\nreturn \u03b8t (Resulting parameters)\nIn section 2 we describe the algorithm and the properties of its update rule. Section 3 explains\nour initialization bias correction technique, and section 4 provides a theoretical analysis of Adam\u2019s\nconvergence in online convex programming. Empirically, our method consistently outperforms other\nmethods for a variety of models and datasets, as shown in section 6. Overall, we show that Adam is\na versatile algorithm that scales to large-scale high-dimensional machine learning problems.\n2\nALGORITHM\nSee algorithm 1 for pseudo-code of our proposed algorithm Adam. Let f(\u03b8) be a noisy objec-\ntive function: a stochastic scalar function that is differentiable w.r.t. parameters \u03b8. We are in-\nterested in minimizing the expected value of this function, E[f(\u03b8)] w.r.t. its parameters \u03b8. With\nf1(\u03b8), ..., , fT (\u03b8) we denote the realisations of the stochastic function at subsequent timesteps\n1, ..., T. The stochasticity might come from the evaluation at random subsamples (minibatches)\nof datapoints, or arise from inherent function noise. With gt = \u2207\u03b8ft(\u03b8) we denote the gradient, i.e.\nthe vector of partial derivatives of ft, w.r.t \u03b8 evaluated at timestep t.\nThe algorithm updates exponential moving averages of the gradient (mt) and the squared gradient\n(vt) where the hyper-parameters \u03b21, \u03b22 \u2208[0, 1) control the exponential decay rates of these moving\naverages. The moving averages themselves are estimates of the 1st moment (the mean) and the\n2nd raw moment (the uncentered variance) of the gradient. However, these moving averages are\ninitialized as (vectors of) 0\u2019s, leading to moment estimates that are biased towards zero, especially\nduring the initial timesteps, and especially when the decay rates are small (i.e. the \u03b2s are close to 1).\nThe good news is that this initialization bias can be easily counteracted, resulting in bias-corrected\nestimates bmt and bvt. See section 3 for more details.\nNote that the ef\ufb01ciency of algorithm 1 can, at the expense of clarity, be improved upon by changing\nthe order of computation, e.g. by replacing the last three lines in the loop with the following lines:\n\u03b1t = \u03b1 \u00b7\np\n1 \u2212\u03b2t\n2/(1 \u2212\u03b2t\n1) and \u03b8t \u2190\u03b8t\u22121 \u2212\u03b1t \u00b7 mt/(\u221avt + \u02c6\u03f5).\n2.1\nADAM\u2019S UPDATE RULE\nAn important property of Adam\u2019s update rule is its careful choice of stepsizes. Assuming \u03f5 = 0, the\neffective step taken in parameter space at timestep t is \u2206t = \u03b1 \u00b7 bmt/\u221abvt. The effective stepsize has\ntwo upper bounds: |\u2206t| \u2264\u03b1 \u00b7 (1 \u2212\u03b21)/\u221a1 \u2212\u03b22 in the case (1 \u2212\u03b21) > \u221a1 \u2212\u03b22, and |\u2206t| \u2264\u03b1\n2\nPublished as a conference paper at ICLR 2015\notherwise. The \ufb01rst case only happens in the most severe case of sparsity: when a gradient has\nbeen zero at all timesteps except at the current timestep. For less sparse cases, the effective stepsize\nwill be smaller. When (1 \u2212\u03b21) = \u221a1 \u2212\u03b22 we have that | bmt/\u221abvt| < 1 therefore |\u2206t| < \u03b1. In\nmore common scenarios, we will have that bmt/\u221abvt \u2248\u00b11 since |E[g]/\np\nE[g2]| \u22641. The effective\nmagnitude of the steps taken in parameter space at each timestep are approximately bounded by\nthe stepsize setting \u03b1, i.e., |\u2206t| \u2a85\u03b1. This can be understood as establishing a trust region around\nthe current parameter value, beyond which the current gradient estimate does not provide suf\ufb01cient\ninformation. This typically makes it relatively easy to know the right scale of \u03b1 in advance. For\nmany machine learning models, for instance, we often know in advance that good optima are with\nhigh probability within some set region in parameter space; it is not uncommon, for example, to\nhave a prior distribution over the parameters. Since \u03b1 sets (an upper bound of) the magnitude of\nsteps in parameter space, we can often deduce the right order of magnitude of \u03b1 such that optima\ncan be reached from \u03b80 within some number of iterations. With a slight abuse of terminology,\nwe will call the ratio bmt/\u221abvt the signal-to-noise ratio (SNR). With a smaller SNR the effective\nstepsize \u2206t will be closer to zero. This is a desirable property, since a smaller SNR means that\nthere is greater uncertainty about whether the direction of bmt corresponds to the direction of the true\ngradient. For example, the SNR value typically becomes closer to 0 towards an optimum, leading\nto smaller effective steps in parameter space: a form of automatic annealing. The effective stepsize\n\u2206t is also invariant to the scale of the gradients; rescaling the gradients g with factor c will scale bmt\nwith a factor c and bvt with a factor c2, which cancel out: (c \u00b7 bmt)/(\u221a\nc2 \u00b7 bvt) = bmt/\u221abvt.\n3\nINITIALIZATION BIAS CORRECTION\nAs explained in section 2, Adam utilizes initialization bias correction terms. We will here derive\nthe term for the second moment estimate; the derivation for the \ufb01rst moment estimate is completely\nanalogous. Let g be the gradient of the stochastic objective f, and we wish to estimate its second\nraw moment (uncentered variance) using an exponential moving average of the squared gradient,\nwith decay rate \u03b22. Let g1, ..., gT be the gradients at subsequent timesteps, each a draw from an\nunderlying gradient distribution gt \u223cp(gt). Let us initialize the exponential moving average as\nv0 = 0 (a vector of zeros). First note that the update at timestep t of the exponential moving average\nvt = \u03b22 \u00b7 vt\u22121 + (1 \u2212\u03b22) \u00b7 g2\nt (where g2\nt indicates the elementwise square gt \u2299gt) can be written as\na function of the gradients at all previous timesteps:\nvt = (1 \u2212\u03b22)\nt\nX\ni=1\n\u03b2t\u2212i\n2\n\u00b7 g2\ni\n(1)\nWe wish to know how E[vt], the expected value of the exponential moving average at timestep t,\nrelates to the true second moment E[g2\nt ], so we can correct for the discrepancy between the two.\nTaking expectations of the left-hand and right-hand sides of eq. (1):\nE[vt] = E\n\"\n(1 \u2212\u03b22)\nt\nX\ni=1\n\u03b2t\u2212i\n2\n\u00b7 g2\ni\n#\n(2)\n= E[g2\nt ] \u00b7 (1 \u2212\u03b22)\nt\nX\ni=1\n\u03b2t\u2212i\n2\n+ \u03b6\n(3)\n= E[g2\nt ] \u00b7 (1 \u2212\u03b2t\n2) + \u03b6\n(4)\nwhere \u03b6 = 0 if the true second moment E[g2\ni ] is stationary; otherwise \u03b6 can be kept small since\nthe exponential decay rate \u03b21 can (and should) be chosen such that the exponential moving average\nassigns small weights to gradients too far in the past. What is left is the term (1 \u2212\u03b2t\n2) which is\ncaused by initializing the running average with zeros. In algorithm 1 we therefore divide by this\nterm to correct the initialization bias.\nIn case of sparse gradients, for a reliable estimate of the second moment one needs to average over\nmany gradients by chosing a small value of \u03b22; however it is exactly this case of small \u03b22 where a\nlack of initialisation bias correction would lead to initial steps that are much larger.\n3\nPublished as a conference paper at ICLR 2015\n4\nCONVERGENCE ANALYSIS\nWe analyze the convergence of Adam using the online learning framework proposed in (Zinkevich,\n2003). Given an arbitrary, unknown sequence of convex cost functions f1(\u03b8), f2(\u03b8),..., fT (\u03b8). At\neach time t, our goal is to predict the parameter \u03b8t and evaluate it on a previously unknown cost\nfunction ft. Since the nature of the sequence is unknown in advance, we evaluate our algorithm\nusing the regret, that is the sum of all the previous difference between the online prediction ft(\u03b8t)\nand the best \ufb01xed point parameter ft(\u03b8\u2217) from a feasible set X for all the previous steps. Concretely,\nthe regret is de\ufb01ned as:\nR(T) =\nT\nX\nt=1\n[ft(\u03b8t) \u2212ft(\u03b8\u2217)]\n(5)\nwhere \u03b8\u2217= arg min\u03b8\u2208X\nPT\nt=1 ft(\u03b8). We show Adam has O(\n\u221a\nT) regret bound and a proof is given\nin the appendix. Our result is comparable to the best known bound for this general convex online\nlearning problem. We also use some de\ufb01nitions simplify our notation, where gt \u225c\u2207ft(\u03b8t) and gt,i\nas the ith element. We de\ufb01ne g1:t,i \u2208Rt as a vector that contains the ith dimension of the gradients\nover all iterations till t, g1:t,i = [g1,i, g2,i, \u00b7 \u00b7 \u00b7 , gt,i]. Also, we de\ufb01ne \u03b3 \u225c\n\u03b22\n1\n\u221a\u03b22 . Our following\ntheorem holds when the learning rate \u03b1t is decaying at a rate of t\u22121\n2 and \ufb01rst moment running\naverage coef\ufb01cient \u03b21,t decay exponentially with \u03bb, that is typically close to 1, e.g. 1 \u221210\u22128.\nTheorem 4.1. Assume that the function ft has bounded gradients, \u2225\u2207ft(\u03b8)\u22252 \u2264G, \u2225\u2207ft(\u03b8)\u2225\u221e\u2264\nG\u221efor all \u03b8 \u2208Rd and distance between any \u03b8t generated by Adam is bounded, \u2225\u03b8n \u2212\u03b8m\u22252 \u2264D,\n\u2225\u03b8m \u2212\u03b8n\u2225\u221e\u2264D\u221efor any m, n \u2208{1, ..., T}, and \u03b21, \u03b22 \u2208[0, 1) satisfy\n\u03b22\n1\n\u221a\u03b22 < 1. Let \u03b1t =\n\u03b1\n\u221a\nt\nand \u03b21,t = \u03b21\u03bbt\u22121, \u03bb \u2208(0, 1). Adam achieves the following guarantee, for all T \u22651.\nR(T) \u2264\nD2\n2\u03b1(1 \u2212\u03b21)\nd\nX\ni=1\np\nTbvT,i+\n\u03b1(1 + \u03b21)G\u221e\n(1 \u2212\u03b21)\u221a1 \u2212\u03b22(1 \u2212\u03b3)2\nd\nX\ni=1\n\u2225g1:T,i\u22252+\nd\nX\ni=1\nD2\n\u221eG\u221e\n\u221a1 \u2212\u03b22\n2\u03b1(1 \u2212\u03b21)(1 \u2212\u03bb)2\nOur Theorem 4.1 implies when the data features are sparse and bounded gradients, the sum-\nmation term can be much smaller than its upper bound Pd\ni=1 \u2225g1:T,i\u22252\n<< dG\u221e\n\u221a\nT and\nPd\ni=1\np\nTbvT,i << dG\u221e\n\u221a\nT, in particular if the class of function and data features are in the form of\nsection 1.2 in (Duchi et al., 2011). Their results for the expected value E[Pd\ni=1 \u2225g1:T,i\u22252] also apply\nto Adam. In particular, the adaptive method, such as Adam and Adagrad, can achieve O(log d\n\u221a\nT),\nan improvement over O(\n\u221a\ndT) for the non-adaptive method. Decaying \u03b21,t towards zero is impor-\ntant in our theoretical analysis and also matches previous empirical \ufb01ndings, e.g. (Sutskever et al.,\n2013) suggests reducing the momentum coef\ufb01cient in the end of training can improve convergence.\nFinally, we can show the average regret of Adam converges,\nCorollary 4.2. Assume that the function ft has bounded gradients, \u2225\u2207ft(\u03b8)\u22252 \u2264G, \u2225\u2207ft(\u03b8)\u2225\u221e\u2264\nG\u221efor all \u03b8 \u2208Rd and distance between any \u03b8t generated by Adam is bounded, \u2225\u03b8n \u2212\u03b8m\u22252 \u2264D,\n\u2225\u03b8m \u2212\u03b8n\u2225\u221e\u2264D\u221efor any m, n \u2208{1, ..., T}. Adam achieves the following guarantee, for all\nT \u22651.\nR(T)\nT\n= O( 1\n\u221a\nT\n)\nThis result can be obtained by using Theorem 4.1 and Pd\ni=1 \u2225g1:T,i\u22252 \u2264dG\u221e\n\u221a\nT.\nThus,\nlimT \u2192\u221e\nR(T )\nT\n= 0.\n5\nRELATED WORK\nOptimization methods bearing a direct relation to Adam are RMSProp (Tieleman & Hinton, 2012;\nGraves, 2013) and AdaGrad (Duchi et al., 2011); these relationships are discussed below. Other\nstochastic optimization methods include vSGD (Schaul et al., 2012), AdaDelta (Zeiler, 2012) and the\nnatural Newton method from Roux & Fitzgibbon (2010), all setting stepsizes by estimating curvature\n4\nPublished as a conference paper at ICLR 2015\nfrom \ufb01rst-order information. The Sum-of-Functions Optimizer (SFO) (Sohl-Dickstein et al., 2014)\nis a quasi-Newton method based on minibatches, but (unlike Adam) has memory requirements linear\nin the number of minibatch partitions of a dataset, which is often infeasible on memory-constrained\nsystems such as a GPU. Like natural gradient descent (NGD) (Amari, 1998), Adam employs a\npreconditioner that adapts to the geometry of the data, since bvt is an approximation to the diagonal\nof the Fisher information matrix (Pascanu & Bengio, 2013); however, Adam\u2019s preconditioner (like\nAdaGrad\u2019s) is more conservative in its adaption than vanilla NGD by preconditioning with the square\nroot of the inverse of the diagonal Fisher information matrix approximation.\nRMSProp:\nAn optimization method closely related to Adam is RMSProp (Tieleman & Hinton,\n2012). A version with momentum has sometimes been used (Graves, 2013). There are a few impor-\ntant differences between RMSProp with momentum and Adam: RMSProp with momentum gener-\nates its parameter updates using a momentum on the rescaled gradient, whereas Adam updates are\ndirectly estimated using a running average of \ufb01rst and second moment of the gradient. RMSProp\nalso lacks a bias-correction term; this matters most in case of a value of \u03b22 close to 1 (required in\ncase of sparse gradients), since in that case not correcting the bias leads to very large stepsizes and\noften divergence, as we also empirically demonstrate in section 6.4.\nAdaGrad:\nAn algorithm that works well for sparse gradients is AdaGrad (Duchi et al., 2011). Its\nbasic version updates parameters as \u03b8t+1 = \u03b8t \u2212\u03b1 \u00b7 gt/\nqPt\ni=1 g2\nt . Note that if we choose \u03b22 to be\nin\ufb01nitesimally close to 1 from below, then lim\u03b22\u21921 bvt = t\u22121 \u00b7 Pt\ni=1 g2\nt . AdaGrad corresponds to a\nversion of Adam with \u03b21 = 0, in\ufb01nitesimal (1 \u2212\u03b22) and a replacement of \u03b1 by an annealed version\n\u03b1t = \u03b1 \u00b7 t\u22121/2, namely \u03b8t \u2212\u03b1 \u00b7 t\u22121/2 \u00b7 bmt/\np\nlim\u03b22\u21921 bvt = \u03b8t \u2212\u03b1 \u00b7 t\u22121/2 \u00b7 gt/\nq\nt\u22121 \u00b7 Pt\ni=1 g2\nt =\n\u03b8t \u2212\u03b1 \u00b7 gt/\nqPt\ni=1 g2\nt . Note that this direct correspondence between Adam and Adagrad does\nnot hold when removing the bias-correction terms; without bias correction, like in RMSProp, a \u03b22\nin\ufb01nitesimally close to 1 would lead to in\ufb01nitely large bias, and in\ufb01nitely large parameter updates.\n6\nEXPERIMENTS\nTo empirically evaluate the proposed method, we investigated different popular machine learning\nmodels, including logistic regression, multilayer fully connected neural networks and deep convolu-\ntional neural networks. Using large models and datasets, we demonstrate Adam can ef\ufb01ciently solve\npractical deep learning problems.\nWe use the same parameter initialization when comparing different optimization algorithms. The\nhyper-parameters, such as learning rate and momentum, are searched over a dense grid and the\nresults are reported using the best hyper-parameter setting.\n6.1\nEXPERIMENT: LOGISTIC REGRESSION\nWe evaluate our proposed method on L2-regularized multi-class logistic regression using the MNIST\ndataset. Logistic regression has a well-studied convex objective, making it suitable for comparison\nof different optimizers without worrying about local minimum issues. The stepsize \u03b1 in our logistic\nregression experiments is adjusted by 1/\n\u221a\nt decay, namely \u03b1t =\n\u03b1\n\u221a\nt that matches with our theorat-\nical prediction from section 4. The logistic regression classi\ufb01es the class label directly on the 784\ndimension image vectors. We compare Adam to accelerated SGD with Nesterov momentum and\nAdagrad using minibatch size of 128. According to Figure 1, we found that the Adam yields similar\nconvergence as SGD with momentum and both converge faster than Adagrad.\nAs discussed in (Duchi et al., 2011), Adagrad can ef\ufb01ciently deal with sparse features and gradi-\nents as one of its main theoretical results whereas SGD is low at learning rare features. Adam with\n1/\n\u221a\nt decay on its stepsize should theoratically match the performance of Adagrad. We examine the\nsparse feature problem using IMDB movie review dataset from (Maas et al., 2011). We pre-process\nthe IMDB movie reviews into bag-of-words (BoW) feature vectors including the \ufb01rst 10,000 most\nfrequent words. The 10,000 dimension BoW feature vector for each review is highly sparse. As sug-\ngested in (Wang & Manning, 2013), 50% dropout noise can be applied to the BoW features during\n5\nPublished as a conference paper at ICLR 2015\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\niterations over entire dataset\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\ntraining cost\nMNIST Logistic Regression\nAdaGrad\nSGDNesterov\nAdam\n0\n20\n40\n60\n80\n100\n120\n140\n160\niterations over entire dataset\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\ntraining cost\nIMDB BoW feature Logistic Regression\nAdagrad+dropout\nRMSProp+dropout\nSGDNesterov+dropout\nAdam+dropout\nFigure 1: Logistic regression training negative log likelihood on MNIST images and IMDB movie\nreviews with 10,000 bag-of-words (BoW) feature vectors.\ntraining to prevent over-\ufb01tting. In \ufb01gure 1, Adagrad outperforms SGD with Nesterov momentum\nby a large margin both with and without dropout noise. Adam converges as fast as Adagrad. The\nempirical performance of Adam is consistent with our theoretical \ufb01ndings in sections 2 and 4. Sim-\nilar to Adagrad, Adam can take advantage of sparse features and obtain faster convergence rate than\nnormal SGD with momentum.\n6.2\nEXPERIMENT: MULTI-LAYER NEURAL NETWORKS\nMulti-layer neural network are powerful models with non-convex objective functions. Although\nour convergence analysis does not apply to non-convex problems, we empirically found that Adam\noften outperforms other methods in such cases. In our experiments, we made model choices that are\nconsistent with previous publications in the area; a neural network model with two fully connected\nhidden layers with 1000 hidden units each and ReLU activation are used for this experiment with\nminibatch size of 128.\nFirst, we study different optimizers using the standard deterministic cross-entropy objective func-\ntion with L2 weight decay on the parameters to prevent over-\ufb01tting. The sum-of-functions (SFO)\nmethod (Sohl-Dickstein et al., 2014) is a recently proposed quasi-Newton method that works with\nminibatches of data and has shown good performance on optimization of multi-layer neural net-\nworks. We used their implementation and compared with Adam to train such models. Figure 2\nshows that Adam makes faster progress in terms of both the number of iterations and wall-clock\ntime. Due to the cost of updating curvature information, SFO is 5-10x slower per iteration com-\npared to Adam, and has a memory requirement that is linear in the number minibatches.\nStochastic regularization methods, such as dropout, are an effective way to prevent over-\ufb01tting and\noften used in practice due to their simplicity. SFO assumes deterministic subfunctions, and indeed\nfailed to converge on cost functions with stochastic regularization. We compare the effectiveness of\nAdam to other stochastic \ufb01rst order methods on multi-layer neural networks trained with dropout\nnoise. Figure 2 shows our results; Adam shows better convergence than other methods.\n6.3\nEXPERIMENT: CONVOLUTIONAL NEURAL NETWORKS\nConvolutional neural networks (CNNs) with several layers of convolution, pooling and non-linear\nunits have shown considerable success in computer vision tasks. Unlike most fully connected neural\nnets, weight sharing in CNNs results in vastly different gradients in different layers. A smaller\nlearning rate for the convolution layers is often used in practice when applying SGD. We show the\neffectiveness of Adam in deep CNNs. Our CNN architecture has three alternating stages of 5x5\nconvolution \ufb01lters and 3x3 max pooling with stride of 2 that are followed by a fully connected layer\nof 1000 recti\ufb01ed linear hidden units (ReLU\u2019s). The input image are pre-processed by whitening, and\n6\nPublished as a conference paper at ICLR 2015\n0\n50\n100\n150\n200\niterations over entire dataset\n10\n-2\n10\n-1\ntraining cost\nMNIST Multilayer Neural Network + dropout\nAdaGrad\nRMSProp\nSGDNesterov\nAdaDelta\nAdam\n(a)\n(b)\nFigure 2: Training of multilayer neural networks on MNIST images. (a) Neural networks using\ndropout stochastic regularization. (b) Neural networks with deterministic cost function. We compare\nwith the sum-of-functions (SFO) optimizer (Sohl-Dickstein et al., 2014)\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\niterations over entire dataset\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\ntraining cost\nCIFAR10 ConvNet First 3 Epoches\nAdaGrad\nAdaGrad+dropout\nSGDNesterov\nSGDNesterov+dropout\nAdam\nAdam+dropout\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\niterations over entire dataset\n10-4\n10-3\n10-2\n10-1\n100\n101\n102\ntraining cost\nCIFAR10 ConvNet\nAdaGrad\nAdaGrad+dropout\nSGDNesterov\nSGDNesterov+dropout\nAdam\nAdam+dropout\nFigure 3: Convolutional neural networks training cost. (left) Training cost for the \ufb01rst three epochs.\n(right) Training cost over 45 epochs. CIFAR-10 with c64-c64-c128-1000 architecture.\ndropout noise is applied to the input layer and fully connected layer. The minibatch size is also set\nto 128 similar to previous experiments.\nInterestingly, although both Adam and Adagrad make rapid progress lowering the cost in the initial\nstage of the training, shown in Figure 3 (left), Adam and SGD eventually converge considerably\nfaster than Adagrad for CNNs shown in Figure 3 (right). We notice the second moment estimate bvt\nvanishes to zeros after a few epochs and is dominated by the \u03f5 in algorithm 1. The second moment\nestimate is therefore a poor approximation to the geometry of the cost function in CNNs comparing\nto fully connected network from Section 6.2. Whereas, reducing the minibatch variance through\nthe \ufb01rst moment is more important in CNNs and contributes to the speed-up. As a result, Adagrad\nconverges much slower than others in this particular experiment. Though Adam shows marginal\nimprovement over SGD with momentum, it adapts learning rate scale for different layers instead of\nhand picking manually as in SGD.\n7\nPublished as a conference paper at ICLR 2015\n\u03b21=0\n\u03b21=0.9\n\u03b22=0.99\n\u03b22=0.999\n\u03b22=0.9999\n\u03b22=0.99\n\u03b22=0.999\n\u03b22=0.9999\n(a) after 10 epochs\n(b) after 100 epochs\nlog10(\u03b1)\nLoss\nFigure 4: Effect of bias-correction terms (red line) versus no bias correction terms (green line)\nafter 10 epochs (left) and 100 epochs (right) on the loss (y-axes) when learning a Variational Auto-\nEncoder (VAE) (Kingma & Welling, 2013), for different settings of stepsize \u03b1 (x-axes) and hyper-\nparameters \u03b21 and \u03b22.\n6.4\nEXPERIMENT: BIAS-CORRECTION TERM\nWe also empirically evaluate the effect of the bias correction terms explained in sections 2 and 3.\nDiscussed in section 5, removal of the bias correction terms results in a version of RMSProp (Tiele-\nman & Hinton, 2012) with momentum. We vary the \u03b21 and \u03b22 when training a variational auto-\nencoder (VAE) with the same architecture as in (Kingma & Welling, 2013) with a single hidden\nlayer with 500 hidden units with softplus nonlinearities and a 50-dimensional spherical Gaussian\nlatent variable. We iterated over a broad range of hyper-parameter choices, i.e. \u03b21 \u2208[0, 0.9] and\n\u03b22 \u2208[0.99, 0.999, 0.9999], and log10(\u03b1) \u2208[\u22125, ..., \u22121]. Values of \u03b22 close to 1, required for robust-\nness to sparse gradients, results in larger initialization bias; therefore we expect the bias correction\nterm is important in such cases of slow decay, preventing an adverse effect on optimization.\nIn Figure 4, values \u03b22 close to 1 indeed lead to instabilities in training when no bias correction term\nwas present, especially at \ufb01rst few epochs of the training. The best results were achieved with small\nvalues of (1\u2212\u03b22) and bias correction; this was more apparent towards the end of optimization when\ngradients tends to become sparser as hidden units specialize to speci\ufb01c patterns. In summary, Adam\nperformed equal or better than RMSProp, regardless of hyper-parameter setting.\n7\nEXTENSIONS\n7.1\nADAMAX\nIn Adam, the update rule for individual weights is to scale their gradients inversely proportional to a\n(scaled) L2 norm of their individual current and past gradients. We can generalize the L2 norm based\nupdate rule to a Lp norm based update rule. Such variants become numerically unstable for large\np. However, in the special case where we let p \u2192\u221e, a surprisingly simple and stable algorithm\nemerges; see algorithm 2. We\u2019ll now derive the algorithm. Let, in case of the Lp norm, the stepsize\nat time t be inversely proportional to v1/p\nt\n, where:\nvt = \u03b2p\n2vt\u22121 + (1 \u2212\u03b2p\n2)|gt|p\n(6)\n= (1 \u2212\u03b2p\n2)\nt\nX\ni=1\n\u03b2p(t\u2212i)\n2\n\u00b7 |gi|p\n(7)\n8\nPublished as a conference paper at ICLR 2015\nAlgorithm 2: AdaMax, a variant of Adam based on the in\ufb01nity norm. See section 7.1 for details.\nGood default settings for the tested machine learning problems are \u03b1 = 0.002, \u03b21 = 0.9 and\n\u03b22 = 0.999. With \u03b2t\n1 we denote \u03b21 to the power t. Here, (\u03b1/(1 \u2212\u03b2t\n1)) is the learning rate with the\nbias-correction term for the \ufb01rst moment. All operations on vectors are element-wise.\nRequire: \u03b1: Stepsize\nRequire: \u03b21, \u03b22 \u2208[0, 1): Exponential decay rates\nRequire: f(\u03b8): Stochastic objective function with parameters \u03b8\nRequire: \u03b80: Initial parameter vector\nm0 \u21900 (Initialize 1st moment vector)\nu0 \u21900 (Initialize the exponentially weighted in\ufb01nity norm)\nt \u21900 (Initialize timestep)\nwhile \u03b8t not converged do\nt \u2190t + 1\ngt \u2190\u2207\u03b8ft(\u03b8t\u22121) (Get gradients w.r.t. stochastic objective at timestep t)\nmt \u2190\u03b21 \u00b7 mt\u22121 + (1 \u2212\u03b21) \u00b7 gt (Update biased \ufb01rst moment estimate)\nut \u2190max(\u03b22 \u00b7 ut\u22121, |gt|) (Update the exponentially weighted in\ufb01nity norm)\n\u03b8t \u2190\u03b8t\u22121 \u2212(\u03b1/(1 \u2212\u03b2t\n1)) \u00b7 mt/ut (Update parameters)\nend while\nreturn \u03b8t (Resulting parameters)\nNote that the decay term is here equivalently parameterised as \u03b2p\n2 instead of \u03b22. Now let p \u2192\u221e,\nand de\ufb01ne ut = limp\u2192\u221e(vt)1/p, then:\nut = lim\np\u2192\u221e(vt)1/p = lim\np\u2192\u221e\n \n(1 \u2212\u03b2p\n2)\nt\nX\ni=1\n\u03b2p(t\u2212i)\n2\n\u00b7 |gi|p\n!1/p\n(8)\n= lim\np\u2192\u221e(1 \u2212\u03b2p\n2)1/p\n \nt\nX\ni=1\n\u03b2p(t\u2212i)\n2\n\u00b7 |gi|p\n!1/p\n(9)\n= lim\np\u2192\u221e\n \nt\nX\ni=1\n\u0010\n\u03b2(t\u2212i)\n2\n\u00b7 |gi|\n\u0011p\n!1/p\n(10)\n= max\n\u0000\u03b2t\u22121\n2\n|g1|, \u03b2t\u22122\n2\n|g2|, . . . , \u03b22|gt\u22121|, |gt|\n\u0001\n(11)\nWhich corresponds to the remarkably simple recursive formula:\nut = max(\u03b22 \u00b7 ut\u22121, |gt|)\n(12)\nwith initial value u0 = 0. Note that, conveniently enough, we don\u2019t need to correct for initialization\nbias in this case. Also note that the magnitude of parameter updates has a simpler bound with\nAdaMax than Adam, namely: |\u2206t| \u2264\u03b1.\n7.2\nTEMPORAL AVERAGING\nSince the last iterate is noisy due to stochastic approximation, better generalization performance is\noften achieved by averaging. Previously in Moulines & Bach (2011), Polyak-Ruppert averaging\n(Polyak & Juditsky, 1992; Ruppert, 1988) has been shown to improve the convergence of standard\nSGD, where \u00af\u03b8t = 1\nt\nPn\nk=1 \u03b8k. Alternatively, an exponential moving average over the parameters can\nbe used, giving higher weight to more recent parameter values. This can be trivially implemented\nby adding one line to the inner loop of algorithms 1 and 2: \u00af\u03b8t \u2190\u03b22 \u00b7 \u00af\u03b8t\u22121 +(1\u2212\u03b22)\u03b8t, with \u00af\u03b80 = 0.\nInitalization bias can again be corrected by the estimator b\u03b8t = \u00af\u03b8t/(1 \u2212\u03b2t\n2).\n8\nCONCLUSION\nWe have introduced a simple and computationally ef\ufb01cient algorithm for gradient-based optimiza-\ntion of stochastic objective functions. Our method is aimed towards machine learning problems with\n9\nPublished as a conference paper at ICLR 2015\nlarge datasets and/or high-dimensional parameter spaces. The method combines the advantages of\ntwo recently popular optimization methods: the ability of AdaGrad to deal with sparse gradients,\nand the ability of RMSProp to deal with non-stationary objectives. The method is straightforward\nto implement and requires little memory. The experiments con\ufb01rm the analysis on the rate of con-\nvergence in convex problems. Overall, we found Adam to be robust and well-suited to a wide range\nof non-convex optimization problems in the \ufb01eld machine learning.\n9\nACKNOWLEDGMENTS\nThis paper would probably not have existed without the support of Google Deepmind. We would\nlike to give special thanks to Ivo Danihelka, and Tom Schaul for coining the name Adam. Thanks to\nKai Fan from Duke University for spotting an error in the original AdaMax derivation. Experiments\nin this work were partly carried out on the Dutch national e-infrastructure with the support of SURF\nFoundation. Diederik Kingma is supported by the Google European Doctorate Fellowship in Deep\nLearning.\nREFERENCES\nAmari, Shun-Ichi. Natural gradient works ef\ufb01ciently in learning. Neural computation, 10(2):251\u2013276, 1998.\nDeng, Li, Li, Jinyu, Huang, Jui-Ting, Yao, Kaisheng, Yu, Dong, Seide, Frank, Seltzer, Michael, Zweig, Geoff,\nHe, Xiaodong, Williams, Jason, et al. Recent advances in deep learning for speech research at microsoft.\nICASSP 2013, 2013.\nDuchi, John, Hazan, Elad, and Singer, Yoram. Adaptive subgradient methods for online learning and stochastic\noptimization. The Journal of Machine Learning Research, 12:2121\u20132159, 2011.\nGraves, Alex. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.\nGraves, Alex, Mohamed, Abdel-rahman, and Hinton, Geoffrey. Speech recognition with deep recurrent neural\nnetworks. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on,\npp. 6645\u20136649. IEEE, 2013.\nHinton, G.E. and Salakhutdinov, R.R. Reducing the dimensionality of data with neural networks. Science, 313\n(5786):504\u2013507, 2006.\nHinton, Geoffrey, Deng, Li, Yu, Dong, Dahl, George E, Mohamed, Abdel-rahman, Jaitly, Navdeep, Senior,\nAndrew, Vanhoucke, Vincent, Nguyen, Patrick, Sainath, Tara N, et al. Deep neural networks for acoustic\nmodeling in speech recognition: The shared views of four research groups. Signal Processing Magazine,\nIEEE, 29(6):82\u201397, 2012a.\nHinton, Geoffrey E, Srivastava, Nitish, Krizhevsky, Alex, Sutskever, Ilya, and Salakhutdinov, Ruslan R. Im-\nproving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580,\n2012b.\nKingma, Diederik P and Welling, Max. Auto-Encoding Variational Bayes. In The 2nd International Conference\non Learning Representations (ICLR), 2013.\nKrizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classi\ufb01cation with deep convolutional\nneural networks. In Advances in neural information processing systems, pp. 1097\u20131105, 2012.\nMaas, Andrew L, Daly, Raymond E, Pham, Peter T, Huang, Dan, Ng, Andrew Y, and Potts, Christopher.\nLearning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association\nfor Computational Linguistics: Human Language Technologies-Volume 1, pp. 142\u2013150. Association for\nComputational Linguistics, 2011.\nMoulines, Eric and Bach, Francis R.\nNon-asymptotic analysis of stochastic approximation algorithms for\nmachine learning. In Advances in Neural Information Processing Systems, pp. 451\u2013459, 2011.\nPascanu, Razvan and Bengio, Yoshua.\nRevisiting natural gradient for deep networks.\narXiv preprint\narXiv:1301.3584, 2013.\nPolyak, Boris T and Juditsky, Anatoli B. Acceleration of stochastic approximation by averaging. SIAM Journal\non Control and Optimization, 30(4):838\u2013855, 1992.\n10\nPublished as a conference paper at ICLR 2015\nRoux, Nicolas L and Fitzgibbon, Andrew W. A fast natural newton method. In Proceedings of the 27th\nInternational Conference on Machine Learning (ICML-10), pp. 623\u2013630, 2010.\nRuppert, David. Ef\ufb01cient estimations from a slowly convergent robbins-monro process. Technical report,\nCornell University Operations Research and Industrial Engineering, 1988.\nSchaul, Tom, Zhang, Sixin, and LeCun, Yann. No more pesky learning rates. arXiv preprint arXiv:1206.1106,\n2012.\nSohl-Dickstein, Jascha, Poole, Ben, and Ganguli, Surya. Fast large-scale optimization by unifying stochas-\ntic gradient and quasi-newton methods. In Proceedings of the 31st International Conference on Machine\nLearning (ICML-14), pp. 604\u2013612, 2014.\nSutskever, Ilya, Martens, James, Dahl, George, and Hinton, Geoffrey. On the importance of initialization and\nmomentum in deep learning. In Proceedings of the 30th International Conference on Machine Learning\n(ICML-13), pp. 1139\u20131147, 2013.\nTieleman, T. and Hinton, G. Lecture 6.5 - RMSProp, COURSERA: Neural Networks for Machine Learning.\nTechnical report, 2012.\nWang, Sida and Manning, Christopher. Fast dropout training. In Proceedings of the 30th International Confer-\nence on Machine Learning (ICML-13), pp. 118\u2013126, 2013.\nZeiler, Matthew D. Adadelta: An adaptive learning rate method. arXiv preprint arXiv:1212.5701, 2012.\nZinkevich, Martin. Online convex programming and generalized in\ufb01nitesimal gradient ascent. 2003.\n11\nPublished as a conference paper at ICLR 2015\n10\nAPPENDIX\n10.1\nCONVERGENCE PROOF\nDe\ufb01nition 10.1. A function f : Rd \u2192R is convex if for all x, y \u2208Rd, for all \u03bb \u2208[0, 1],\n\u03bbf(x) + (1 \u2212\u03bb)f(y) \u2265f(\u03bbx + (1 \u2212\u03bb)y)\nAlso, notice that a convex function can be lower bounded by a hyperplane at its tangent.\nLemma 10.2. If a function f : Rd \u2192R is convex, then for all x, y \u2208Rd,\nf(y) \u2265f(x) + \u2207f(x)T (y \u2212x)\nThe above lemma can be used to upper bound the regret and our proof for the main theorem is\nconstructed by substituting the hyperplane with the Adam update rules.\nThe following two lemmas are used to support our main theorem. We also use some de\ufb01nitions sim-\nplify our notation, where gt \u225c\u2207ft(\u03b8t) and gt,i as the ith element. We de\ufb01ne g1:t,i \u2208Rt as a vector\nthat contains the ith dimension of the gradients over all iterations till t, g1:t,i = [g1,i, g2,i, \u00b7 \u00b7 \u00b7 , gt,i]\nLemma 10.3. Let gt = \u2207ft(\u03b8t) and g1:t be de\ufb01ned as above and bounded, \u2225gt\u22252 \u2264G, \u2225gt\u2225\u221e\u2264\nG\u221e. Then,\nT\nX\nt=1\ns\ng2\nt,i\nt\n\u22642G\u221e\u2225g1:T,i\u22252\nProof. We will prove the inequality using induction over T.\nThe base case for T = 1, we have\nq\ng2\n1,i \u22642G\u221e\u2225g1,i\u22252.\nFor the inductive step,\nT\nX\nt=1\ns\ng2\nt,i\nt\n=\nT \u22121\nX\nt=1\ns\ng2\nt,i\nt\n+\ns\ng2\nT,i\nT\n\u22642G\u221e\u2225g1:T \u22121,i\u22252 +\ns\ng2\nT,i\nT\n= 2G\u221e\nq\n\u2225g1:T,i\u22252\n2 \u2212g2\nT +\ns\ng2\nT,i\nT\nFrom, \u2225g1:T,i\u22252\n2 \u2212g2\nT,i +\ng4\nT,i\n4\u2225g1:T,i\u22252\n2 \u2265\u2225g1:T,i\u22252\n2 \u2212g2\nT,i, we can take square root of both side and\nhave,\nq\n\u2225g1:T,i\u22252\n2 \u2212g2\nT,i \u2264\u2225g1:T,i\u22252 \u2212\ng2\nT,i\n2\u2225g1:T,i\u22252\n\u2264\u2225g1:T,i\u22252 \u2212\ng2\nT,i\n2\np\nTG2\u221e\nRearrange the inequality and substitute the\nq\n\u2225g1:T,i\u22252\n2 \u2212g2\nT,i term,\nG\u221e\nq\n\u2225g1:T,i\u22252\n2 \u2212g2\nT +\ns\ng2\nT,i\nT\n\u22642G\u221e\u2225g1:T,i\u22252\n12\nPublished as a conference paper at ICLR 2015\nLemma 10.4. Let \u03b3 \u225c\n\u03b22\n1\n\u221a\u03b22 . For \u03b21, \u03b22 \u2208[0, 1) that satisfy\n\u03b22\n1\n\u221a\u03b22 < 1 and bounded gt, \u2225gt\u22252 \u2264G,\n\u2225gt\u2225\u221e\u2264G\u221e, the following inequality holds\nT\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n\u2264\n2\n1 \u2212\u03b3\n1\n\u221a1 \u2212\u03b22\n\u2225g1:T,i\u22252\nProof. Under the assumption,\n\u221a\n1\u2212\u03b2t\n2\n(1\u2212\u03b2t\n1)2 \u2264\n1\n(1\u2212\u03b21)2 . We can expand the last term in the summation\nusing the update rules in Algorithm 1,\nT\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n=\nT \u22121\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n+\np\n1 \u2212\u03b2T\n2\n(1 \u2212\u03b2T\n1 )2\n(PT\nk=1(1 \u2212\u03b21)\u03b2T \u2212k\n1\ngk,i)2\nq\nT PT\nj=1(1 \u2212\u03b22)\u03b2T \u2212j\n2\ng2\nj,i\n\u2264\nT \u22121\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n+\np\n1 \u2212\u03b2T\n2\n(1 \u2212\u03b2T\n1 )2\nT\nX\nk=1\nT((1 \u2212\u03b21)\u03b2T \u2212k\n1\ngk,i)2\nq\nT PT\nj=1(1 \u2212\u03b22)\u03b2T \u2212j\n2\ng2\nj,i\n\u2264\nT \u22121\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n+\np\n1 \u2212\u03b2T\n2\n(1 \u2212\u03b2T\n1 )2\nT\nX\nk=1\nT((1 \u2212\u03b21)\u03b2T \u2212k\n1\ngk,i)2\nq\nT(1 \u2212\u03b22)\u03b2T \u2212k\n2\ng2\nk,i\n\u2264\nT \u22121\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n+\np\n1 \u2212\u03b2T\n2\n(1 \u2212\u03b2T\n1 )2\n(1 \u2212\u03b21)2\np\nT(1 \u2212\u03b22)\nT\nX\nk=1\nT\n\u0012 \u03b22\n1\n\u221a\u03b22\n\u0013T \u2212k\n\u2225gk,i\u22252\n\u2264\nT \u22121\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n+\nT\np\nT(1 \u2212\u03b22)\nT\nX\nk=1\n\u03b3T \u2212k\u2225gk,i\u22252\nSimilarly, we can upper bound the rest of the terms in the summation.\nT\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n\u2264\nT\nX\nt=1\n\u2225gt,i\u22252\np\nt(1 \u2212\u03b22)\nT \u2212t\nX\nj=0\nt\u03b3j\n\u2264\nT\nX\nt=1\n\u2225gt,i\u22252\np\nt(1 \u2212\u03b22)\nT\nX\nj=0\nt\u03b3j\nFor \u03b3 < 1, using the upper bound on the arithmetic-geometric series, P\nt t\u03b3t <\n1\n(1\u2212\u03b3)2 :\nT\nX\nt=1\n\u2225gt,i\u22252\np\nt(1 \u2212\u03b22)\nT\nX\nj=0\nt\u03b3j \u2264\n1\n(1 \u2212\u03b3)2\u221a1 \u2212\u03b22\nT\nX\nt=1\n\u2225gt,i\u22252\n\u221a\nt\nApply Lemma 10.3,\nT\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n\u2264\n2G\u221e\n(1 \u2212\u03b3)2\u221a1 \u2212\u03b22\n\u2225g1:T,i\u22252\nTo simplify the notation, we de\ufb01ne \u03b3 \u225c\n\u03b22\n1\n\u221a\u03b22 . Intuitively, our following theorem holds when the\nlearning rate \u03b1t is decaying at a rate of t\u22121\n2 and \ufb01rst moment running average coef\ufb01cient \u03b21,t decay\nexponentially with \u03bb, that is typically close to 1, e.g. 1 \u221210\u22128.\nTheorem 10.5. Assume that the function ft has bounded gradients, \u2225\u2207ft(\u03b8)\u22252 \u2264G, \u2225\u2207ft(\u03b8)\u2225\u221e\u2264\nG\u221efor all \u03b8 \u2208Rd and distance between any \u03b8t generated by Adam is bounded, \u2225\u03b8n \u2212\u03b8m\u22252 \u2264D,\n13\nPublished as a conference paper at ICLR 2015\n\u2225\u03b8m \u2212\u03b8n\u2225\u221e\u2264D\u221efor any m, n \u2208{1, ..., T}, and \u03b21, \u03b22 \u2208[0, 1) satisfy\n\u03b22\n1\n\u221a\u03b22 < 1. Let \u03b1t =\n\u03b1\n\u221a\nt\nand \u03b21,t = \u03b21\u03bbt\u22121, \u03bb \u2208(0, 1). Adam achieves the following guarantee, for all T \u22651.\nR(T) \u2264\nD2\n2\u03b1(1 \u2212\u03b21)\nd\nX\ni=1\np\nTbvT,i+\n\u03b1(\u03b21 + 1)G\u221e\n(1 \u2212\u03b21)\u221a1 \u2212\u03b22(1 \u2212\u03b3)2\nd\nX\ni=1\n\u2225g1:T,i\u22252+\nd\nX\ni=1\nD2\n\u221eG\u221e\n\u221a1 \u2212\u03b22\n2\u03b1(1 \u2212\u03b21)(1 \u2212\u03bb)2\nProof. Using Lemma 10.2, we have,\nft(\u03b8t) \u2212ft(\u03b8\u2217) \u2264gT\nt (\u03b8t \u2212\u03b8\u2217) =\nd\nX\ni=1\ngt,i(\u03b8t,i \u2212\u03b8\u2217\n,i)\nFrom the update rules presented in algorithm 1,\n\u03b8t+1 = \u03b8t \u2212\u03b1t bmt/\np\nbvt\n= \u03b8t \u2212\n\u03b1t\n1 \u2212\u03b2t\n1\n\u0012 \u03b21,t\n\u221abvt\nmt\u22121 + (1 \u2212\u03b21,t)\n\u221abvt\ngt\n\u0013\nWe focus on the ith dimension of the parameter vector \u03b8t \u2208Rd. Subtract the scalar \u03b8\u2217\n,i and square\nboth sides of the above update rule, we have,\n(\u03b8t+1,i \u2212\u03b8\u2217\n,i)2 =(\u03b8t,i \u2212\u03b8\u2217\n,i)2 \u2212\n2\u03b1t\n1 \u2212\u03b2t\n1\n( \u03b21,t\np\nbvt,i\nmt\u22121,i + (1 \u2212\u03b21,t)\np\nbvt,i\ngt,i)(\u03b8t,i \u2212\u03b8\u2217\n,i) + \u03b12\nt ( bmt,i\np\nbvt,i\n)2\nWe can rearrange the above equation and use Young\u2019s inequality, ab \u2264a2/2 + b2/2. Also, it can be\nshown that\np\nbvt,i =\nqPt\nj=1(1 \u2212\u03b22)\u03b2t\u2212j\n2\ng2\nj,i/\np\n1 \u2212\u03b2t\n2 \u2264\u2225g1:t,i\u22252 and \u03b21,t \u2264\u03b21. Then\ngt,i(\u03b8t,i \u2212\u03b8\u2217\n,i) =(1 \u2212\u03b2t\n1)\np\nbvt,i\n2\u03b1t(1 \u2212\u03b21,t)\n\u0012\n(\u03b8t,i \u2212\u03b8\u2217\n,t)2 \u2212(\u03b8t+1,i \u2212\u03b8\u2217\n,i)2\n\u0013\n+\n\u03b21,t\n(1 \u2212\u03b21,t)\nbv\n1\n4\nt\u22121,i\n\u221a\u03b1t\u22121\n(\u03b8\u2217\n,i \u2212\u03b8t,i)\u221a\u03b1t\u22121\nmt\u22121,i\nbv\n1\n4\nt\u22121,i\n+ \u03b1t(1 \u2212\u03b2t\n1)\np\nbvt,i\n2(1 \u2212\u03b21,t)\n( bmt,i\np\nbvt,i\n)2\n\u2264\n1\n2\u03b1t(1 \u2212\u03b21)\n\u0012\n(\u03b8t,i \u2212\u03b8\u2217\n,t)2 \u2212(\u03b8t+1,i \u2212\u03b8\u2217\n,i)2\n\u0013p\nbvt,i +\n\u03b21,t\n2\u03b1t\u22121(1 \u2212\u03b21,t)(\u03b8\u2217\n,i \u2212\u03b8t,i)2p\nbvt\u22121,i\n+\n\u03b21\u03b1t\u22121\n2(1 \u2212\u03b21)\nm2\nt\u22121,i\np\nbvt\u22121,i\n+\n\u03b1t\n2(1 \u2212\u03b21)\nbm2\nt,i\np\nbvt,i\nWe apply Lemma 10.4 to the above inequality and derive the regret bound by summing across all\nthe dimensions for i \u22081, ..., d in the upper bound of ft(\u03b8t) \u2212ft(\u03b8\u2217) and the sequence of convex\nfunctions for t \u22081, ..., T:\nR(T) \u2264\nd\nX\ni=1\n1\n2\u03b11(1 \u2212\u03b21)(\u03b81,i \u2212\u03b8\u2217\n,i)2p\nbv1,i +\nd\nX\ni=1\nT\nX\nt=2\n1\n2(1 \u2212\u03b21)(\u03b8t,i \u2212\u03b8\u2217\n,i)2(\np\nbvt,i\n\u03b1t\n\u2212\np\nbvt\u22121,i\n\u03b1t\u22121\n)\n+\n\u03b21\u03b1G\u221e\n(1 \u2212\u03b21)\u221a1 \u2212\u03b22(1 \u2212\u03b3)2\nd\nX\ni=1\n\u2225g1:T,i\u22252 +\n\u03b1G\u221e\n(1 \u2212\u03b21)\u221a1 \u2212\u03b22(1 \u2212\u03b3)2\nd\nX\ni=1\n\u2225g1:T,i\u22252\n+\nd\nX\ni=1\nT\nX\nt=1\n\u03b21,t\n2\u03b1t(1 \u2212\u03b21,t)(\u03b8\u2217\n,i \u2212\u03b8t,i)2p\nbvt,i\n14\nPublished as a conference paper at ICLR 2015\nFrom the assumption, \u2225\u03b8t \u2212\u03b8\u2217\u22252 \u2264D, \u2225\u03b8m \u2212\u03b8n\u2225\u221e\u2264D\u221e, we have:\nR(T) \u2264\nD2\n2\u03b1(1 \u2212\u03b21)\nd\nX\ni=1\np\nTbvT,i +\n\u03b1(1 + \u03b21)G\u221e\n(1 \u2212\u03b21)\u221a1 \u2212\u03b22(1 \u2212\u03b3)2\nd\nX\ni=1\n\u2225g1:T,i\u22252 + D2\n\u221e\n2\u03b1\nd\nX\ni=1\nt\nX\nt=1\n\u03b21,t\n(1 \u2212\u03b21,t)\np\ntbvt,i\n\u2264\nD2\n2\u03b1(1 \u2212\u03b21)\nd\nX\ni=1\np\nTbvT,i +\n\u03b1(1 + \u03b21)G\u221e\n(1 \u2212\u03b21)\u221a1 \u2212\u03b22(1 \u2212\u03b3)2\nd\nX\ni=1\n\u2225g1:T,i\u22252\n+ D2\n\u221eG\u221e\n\u221a1 \u2212\u03b22\n2\u03b1\nd\nX\ni=1\nt\nX\nt=1\n\u03b21,t\n(1 \u2212\u03b21,t)\n\u221a\nt\nWe can use arithmetic geometric series upper bound for the last term:\nt\nX\nt=1\n\u03b21,t\n(1 \u2212\u03b21,t)\n\u221a\nt \u2264\nt\nX\nt=1\n1\n(1 \u2212\u03b21)\u03bbt\u22121\u221a\nt\n\u2264\nt\nX\nt=1\n1\n(1 \u2212\u03b21)\u03bbt\u22121t\n\u2264\n1\n(1 \u2212\u03b21)(1 \u2212\u03bb)2\nTherefore, we have the following regret bound:\nR(T) \u2264\nD2\n2\u03b1(1 \u2212\u03b21)\nd\nX\ni=1\np\nTbvT,i +\n\u03b1(1 + \u03b21)G\u221e\n(1 \u2212\u03b21)\u221a1 \u2212\u03b22(1 \u2212\u03b3)2\nd\nX\ni=1\n\u2225g1:T,i\u22252 +\nd\nX\ni=1\nD2\n\u221eG\u221e\n\u221a1 \u2212\u03b22\n2\u03b1\u03b21(1 \u2212\u03bb)2\n15\n",
        "sentence": "",
        "context": "modeling in speech recognition: The shared views of four research groups. Signal Processing Magazine,\nIEEE, 29(6):82\u201397, 2012a.\nHinton, Geoffrey E, Srivastava, Nitish, Krizhevsky, Alex, Sutskever, Ilya, and Salakhutdinov, Ruslan R. Im-\nDeng, Li, Li, Jinyu, Huang, Jui-Ting, Yao, Kaisheng, Yu, Dong, Seide, Frank, Seltzer, Michael, Zweig, Geoff,\nHe, Xiaodong, Williams, Jason, et al. Recent advances in deep learning for speech research at microsoft.\nICASSP 2013, 2013.\n(5786):504\u2013507, 2006.\nHinton, Geoffrey, Deng, Li, Yu, Dong, Dahl, George E, Mohamed, Abdel-rahman, Jaitly, Navdeep, Senior,\nAndrew, Vanhoucke, Vincent, Nguyen, Patrick, Sainath, Tara N, et al. Deep neural networks for acoustic"
    },
    {
        "title": "Semi-supervised learning with deep generative models",
        "author": [
            "D. Kingma",
            "D. Rezende",
            "S. Mohamed",
            "M. Welling"
        ],
        "venue": "In NIPS,",
        "citeRegEx": "Kingma et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Kingma et al\\.",
        "year": 2014,
        "abstract": "",
        "full_text": "",
        "sentence": " Recently, stochastic neural networks (Bengio et al., 2014; Kingma et al., 2014; Gregor et al., 2015) have become popular, and deterministic networks are being used for image generation tasks (Dosovitskiy et al. Please refer to Kingma et al. (2014) for details.",
        "context": null
    },
    {
        "title": "ImageNet classification with deep convolutional neural networks",
        "author": [
            "A. Krizhevsky",
            "I. Sutskever",
            "G.E. Hinton"
        ],
        "venue": "In NIPS, pp",
        "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E",
        "shortCiteRegEx": "Krizhevsky et al\\.",
        "year": 2012,
        "abstract": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called \"dropout\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.",
        "full_text": "",
        "sentence": " AlexNet (Krizhevsky et al., 2012) is a network with 5 convolutional and 2 fully connected layers trained on image classification.",
        "context": null
    },
    {
        "title": "Autoencoding beyond pixels using a learned similarity",
        "author": [
            "A.B.L. Larsen",
            "S.K. S\u00f8nderby",
            "O. Winther"
        ],
        "venue": "metric. arxiv:1512.09300,",
        "citeRegEx": "Larsen et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "Larsen et al\\.",
        "year": 2015,
        "abstract": "",
        "full_text": "",
        "sentence": " Most related is concurrent work of Larsen et al. (2015). The general idea is the same \u2014 to measure the similarity not in the image space, but rather in a feature space. Most related is concurrent work of Larsen et al. (2015). The general idea is the same \u2014 to measure the similarity not in the image space, but rather in a feature space. They also use adversarial training to improve the realism of the generated images. However, Larsen et al. (2015) only apply this approach to a variational autoencoder trained on images of faces, and measure the similarity between features extracted from the discriminator. This is similar to (Larsen et al., 2015), but the comparator does not have to be a part of the discriminator.",
        "context": null
    },
    {
        "title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations",
        "author": [
            "H. Lee",
            "R. Grosse",
            "R. Ranganath",
            "A.Y. Ng"
        ],
        "venue": "In ICML, pp",
        "citeRegEx": "Lee et al\\.,? \\Q2009\\E",
        "shortCiteRegEx": "Lee et al\\.",
        "year": 2009,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " A prominent class of probabilistic models of images are restricted Boltzmann machines (Hinton & Sejnowski, 1986; Smolensky, 1987; Hinton & Salakhutdinov, 2006) and their deep variants (Hinton et al., 2006; Salakhutdinov & Hinton, 2009; Lee et al., 2009). By combining convolutions and unpooling (upsampling) layers (Lee et al., 2009; Goodfellow et al., 2014; Dosovitskiy et al., 2015b) these models can be applied to large images.",
        "context": null
    },
    {
        "title": "Understanding deep image representations by inverting them",
        "author": [
            "A. Mahendran",
            "A. Vedaldi"
        ],
        "venue": "In CVPR,",
        "citeRegEx": "Mahendran and Vedaldi.,? \\Q2015\\E",
        "shortCiteRegEx": "Mahendran and Vedaldi.",
        "year": 2015,
        "abstract": "Image representations, from SIFT and Bag of Visual Words to Convolutional\nNeural Networks (CNNs), are a crucial component of almost any image\nunderstanding system. Nevertheless, our understanding of them remains limited.\nIn this paper we conduct a direct analysis of the visual information contained\nin representations by asking the following question: given an encoding of an\nimage, to which extent is it possible to reconstruct the image itself? To\nanswer this question we contribute a general framework to invert\nrepresentations. We show that this method can invert representations such as\nHOG and SIFT more accurately than recent alternatives while being applicable to\nCNNs too. We then use this technique to study the inverse of recent\nstate-of-the-art CNN image representations for the first time. Among our\nfindings, we show that several layers in CNNs retain photographically accurate\ninformation about the image, with different degrees of geometric and\nphotometric invariance.",
        "full_text": "Understanding Deep Image Representations by Inverting Them\nAravindh Mahendran\nUniversity of Oxford\nAndrea Vedaldi\nUniversity of Oxford\nAbstract\nImage representations, from SIFT and Bag of Visual\nWords to Convolutional Neural Networks (CNNs), are a\ncrucial component of almost any image understanding sys-\ntem. Nevertheless, our understanding of them remains lim-\nited. In this paper we conduct a direct analysis of the visual\ninformation contained in representations by asking the fol-\nlowing question: given an encoding of an image, to which\nextent is it possible to reconstruct the image itself? To an-\nswer this question we contribute a general framework to in-\nvert representations. We show that this method can invert\nrepresentations such as HOG and SIFT more accurately\nthan recent alternatives while being applicable to CNNs\ntoo. We then use this technique to study the inverse of re-\ncent state-of-the-art CNN image representations for the \ufb01rst\ntime. Among our \ufb01ndings, we show that several layers in\nCNNs retain photographically accurate information about\nthe image, with different degrees of geometric and photo-\nmetric invariance.\n1. Introduction\nMost image understanding and computer vision methods\nbuild on image representations such as textons [15], his-\ntogram of oriented gradients (SIFT [18] and HOG [4]), bag\nof visual words [3][25], sparse [35] and local coding [32],\nsuper vector coding [37], VLAD [9], Fisher Vectors [21],\nand, lately, deep neural networks, particularly of the convo-\nlutional variety [13, 23, 36]. However, despite the progress\nin the development of visual representations, their design is\nstill driven empirically and a good understanding of their\nproperties is lacking. While this is true of shallower hand-\ncrafted features, it is even more so for the latest generation\nof deep representations, where millions of parameters are\nlearned from data.\nIn this paper we conduct a direct analysis of representa-\ntions by characterising the image information that they re-\ntain (Fig. 1). We do so by modeling a representation as a\nfunction \u03a6(x) of the image x and then computing an ap-\nproximated inverse \u03c6\u22121, reconstructing x from the code\n\u03a6(x). A common hypothesis is that representations col-\nlapse irrelevant differences in images (e.g. illumination or\nFigure 1. What is encoded by a CNN? The \ufb01gure shows \ufb01ve\npossible reconstructions of the reference image obtained from the\n1,000-dimensional code extracted at the penultimate layer of a ref-\nerence CNN[13] (before the softmax is applied) trained on the Im-\nageNet data. From the viewpoint of the model, all these images are\npractically equivalent. This image is best viewed in color/screen.\nviewpoint), so that \u03a6 should not be uniquely invertible.\nHence, we pose this as a reconstruction problem and \ufb01nd\na number of possible reconstructions rather than a single\none. By doing so, we obtain insights into the invariances\ncaptured by the representation.\nOur contributions are as follows. First, we propose a\ngeneral method to invert representations, including SIFT,\nHOG, and CNNs (Sect. 2). Crucially, this method uses only\ninformation from the image representation and a generic\nnatural image prior, starting from random noise as initial\nsolution, and hence captures only the information contained\nin the representation itself. We discuss and evaluate differ-\nent regularization penalties as natural image priors. Sec-\nond, we show that, despite its simplicity and generality, this\nmethod recovers signi\ufb01cantly better reconstructions from\nDSIFT and HOG compared to recent alternatives [31]. As\nwe do so, we emphasise a number of subtle differences be-\ntween these representations and their effect on invertibility.\nThird, we apply the inversion technique to the analysis of\nrecent deep CNNs, exploring their invariance by sampling\npossible approximate reconstructions. We relate this to the\ndepth of the representation, showing that the CNN gradually\nbuilds an increasing amount of invariance, layer after layer.\nFourth, we study the locality of the information stored in\n1\narXiv:1412.0035v1  [cs.CV]  26 Nov 2014\nthe representations by reconstructing images from selected\ngroups of neurons, either spatially or by channel.\nThe rest of the paper is organised as follows. Sect. 2 in-\ntroduces the inversion method, posing this as a regularised\nregression problem and proposing a number of image priors\nto aid the reconstruction. Sect. 3 introduces various repre-\nsentations: HOG and DSIFT as examples of shallow repre-\nsentations, and state-of-the-art CNNs as an example of deep\nrepresentations. It also shows how HOG and DSIFT can be\nimplemented as CNNs, simplifying the computation of their\nderivatives. Sect. 4 and 5 apply the inversion technique to\nthe analysis of respectively shallow (HOG and DSIFT) and\ndeep (CNNs) representations. Finally, Sect. 6 summarises\nour \ufb01ndings.\nWe use the matconvnet toolbox [30] for implementing\nconvolutional neural networks.\nRelated work. There is a signi\ufb01cant amount of work in un-\nderstanding representations by means of visualisations. The\nworks most related to ours are Weinzaepfel et al. [33] and\nVondrick et al. [31] which invert sparse DSIFT and HOG\nfeatures respectively. While our goal is similar to theirs,\nour method is substantially different from a technical view-\npoint, being based on the direct solution of a regularised\nregression problem. The bene\ufb01t is that our technique ap-\nplies equally to shallow (SIFT, HOG) and deep (CNN) rep-\nresentations.\nCompared to existing inversion techniques\nfor dense shallow representations [31], it is also shown to\nachieve superior results, both quantitatively and qualita-\ntively.\nAn interesting conclusion of [31, 33] is that, while HOG\nand SIFT may not be exactly invertible, they capture a sig-\nni\ufb01cant amount of information about the image. This is in\napparent contradiction with the results of Tatu et al. [27]\nwho show that it is possible to make any two images\nlook nearly identical in SIFT space up to the injection of\nadversarial noise. A symmetric effect was demonstrated\nfor CNNs by Szegedy et al. [26], where an imperceptible\namount of adversarial noise suf\ufb01ces to change the predicted\nclass of an image. The apparent inconsistency is easily re-\nsolved, however, as the methods of [26, 27] require the in-\njection of high-pass structured noise which is very unlikely\nto occur in natural images.\nOur work is also related to the DeConvNet method of\nZeiler and Fergus [36], who backtrack the network com-\nputations to identify which image patches are responsible\nfor certain neural activations. Simonyan et al. [24], how-\never, demonstrated that DeConvNets can be interpreted as a\nsensitivity analysis of the network input/output relation. A\nconsequence is that DeConvNets do not study the problem\nof representation inversion in the sense adopted here, which\nhas signi\ufb01cant methodological consequences; for example,\nDeConvNets require auxiliary information about the acti-\nvations in several intermediate layers, while our inversion\nuses only the \ufb01nal image code. In other words, DeConvNets\nlook at how certain network outputs are obtained, whereas\nwe look for what information is preserved by the network\noutput.\nThe problem of inverting representations, particularly\nCNN-based ones, is related to the problem of inverting\nneural networks, which received signi\ufb01cant attention in the\npast. Algorithms similar to the back-propagation technique\ndeveloped here were proposed by [14, 16, 19, 34], along\nwith alternative optimisation strategies based on sampling.\nHowever, these methods did not use natural image priors as\nwe do, nor were applied to the current generation of deep\nnetworks. Other works [10, 28] specialised on inverting\nnetworks in the context of dynamical systems and will not\nbe discussed further here. Others [1] proposed to learn a\nsecond neural network to act as the inverse of the original\none, but this is complicated by the fact that the inverse is\nusually not unique. Finally, auto-encoder architectures [8]\ntrain networks together with their inverses as a form of su-\npervision; here we are interested instead in visualising feed-\nforward and discriminatively-trained CNNs now popular in\ncomputer vision.\n2. Inverting representations\nThis section introduces our method to compute an ap-\nproximate inverse of an image representation. This is for-\nmulated as the problem of \ufb01nding an image whose repre-\nsentation best matches the one given [34]. Formally, given\na representation function \u03a6 : RH\u00d7W \u00d7C \u2192Rd and a rep-\nresentation \u03a60 = \u03a6(x0) to be inverted, reconstruction \ufb01nds\nthe image x \u2208RH\u00d7W \u00d7C that minimizes the objective:\nx\u2217=\nargmin\nx\u2208RH\u00d7W \u00d7C \u2113(\u03a6(x), \u03a60) + \u03bbR(x)\n(1)\nwhere the loss \u2113compares the image representation \u03a6(x) to\nthe target one \u03a60 and R : RH\u00d7W \u00d7C \u2192R is a regulariser\ncapturing a natural image prior.\nMinimising (1) results in an image x\u2217that \u201cresembles\u201d\nx0 from the viewpoint of the representation. While there\nmay be no unique solution to this problem, sampling the\nspace of possible reconstructions can be used to charac-\nterise the space of images that the representation deems to\nbe equivalent, revealing its invariances.\nWe next discusses the choice of loss and regularizer.\nLoss function. There are many possible choices of the loss\nfunction \u2113. While we use the Euclidean distance:\n\u2113(\u03a6(x), \u03a60) = \u2225\u03a6(x) \u2212\u03a60\u22252,\n(2)\nit is possible to change the nature of the loss entirely, for ex-\nample to optimize selected neural responses. The latter was\nused in [5, 24] to generate images representative of given\nneurons.\nRegularisers.\nDiscriminatively-trained representations\nmay discard a signi\ufb01cant amount of low-level image statis-\n2\ntics as these are usually not interesting for high-level tasks.\nAs this information is nonetheless useful for visualization, it\ncan be partially recovered by restricting the inversion to the\nsubset of natural images X \u2282RH\u00d7W \u00d7C. However, min-\nimising over X requires addressing the challenge of mod-\neling this set. As a proxy one can incorporate in the re-\nconstruction an appropriate image prior. Here we experi-\nment with two such priors. The \ufb01rst one is simply the \u03b1-\nnorm R\u03b1(x) = \u2225x\u2225\u03b1\n\u03b1, where x is the vectorised and mean-\nsubtracted image. By choosing a relatively large exponent\n(\u03b1 = 6 is used in the experiments) the range of the image\nis encouraged to stay within a target interval instead of di-\nverging.\nA second richer regulariser is total variation (TV)\nRV \u03b2(x), encouraging images to consist of piece-wise con-\nstant patches. For continuous functions (or distributions)\nf : RH\u00d7W \u2283\u2126\u2192R, the TV norm is given by:\nRV \u03b2(f) =\nZ\n\u2126\n \u0012\u2202f\n\u2202u(u, v)\n\u00132\n+\n\u0012\u2202f\n\u2202v (u, v)\n\u00132! \u03b2\n2\ndu dv\nwhere \u03b2 = 1. Here images are discrete (x \u2208RH\u00d7W ) and\nthe TV norm is replaced by the \ufb01nite-difference approxima-\ntion:\nRV \u03b2(x) =\nX\ni,j\n\u0010\n(xi,j+1 \u2212xij)2 + (xi+1,j \u2212xij)2\u0011 \u03b2\n2 .\nIt was observed empirically that the TV regularizer (\u03b2 = 1)\nin the presence of subsampling, also caused by max pooling\nin CNNs, leads to \u201cspikes\u201d in the reconstruction. This is a\nknown problem in TV-based image interpolation (see e.g.\nFig. 3 in [2]) and is illustrated in Fig. 2.left when inverting\na layer in a CNN. The \u201cspikes\u201d occur at the locations of\nthe samples because: (1) the TV norm along any path be-\ntween two samples depends only on the overall amount of\nintensity change (not on the sharpness of the changes) and\n(2) integrated on the 2D image, it is optimal to concentrate\nsharp changes around a boundary with a small perimeter.\nHyper-Laplacian priors with \u03b2 < 1 are often used as a better\nmatch of the gradient statistics of natural images [12], but\nthey only exacerbate this issue. Instead, we trade-off the\nsharpness of the image with the removal of such artifacts\nby choosing \u03b2 > 1 which, by penalising large gradients,\ndistributes changes across regions rather than concentrating\nthem at a point or curve. We refer to this as the V \u03b2 regular-\nizer. As seen in Fig. 2 (right), the spikes are removed with\n\u03b2 = 2 but the image is washed out as edges are penalized\nmore than with \u03b2 = 1.\nWhen the target of the reconstruction is a colour image,\nboth regularisers are summed for each colour channel.\nBalancing the different terms. Balancing loss and regu-\nlariser(s) requires some attention. While an optimal tuning\ncan be achieved by cross-validation, it is important to start\nFigure 2. Left: Spikes in a inverse of norm1 features - detail\nshown. Right: Spikes removed by a V \u03b2 regularizer with \u03b2 = 2.\nfrom reasonable settings of the parameters. First, the loss is\nreplaced by the normalized version \u2225\u03a6(x) \u2212\u03a60\u22252\n2/\u2225\u03a60\u22252\n2.\nThis \ufb01xes its dynamic range, as after normalisation the loss\nnear the optimum can be expected to be contained in the\n[0, 1) interval, touching zero at the optimum. In order to\nmake the dynamic range of the regulariser(s) comparable\none can aim for a solution x\u2217which has roughly unitary\nEuclidean norm. While representations are largely insensi-\ntive to the scaling of the image range, this is not exactly true\nfor the \ufb01rst few layers of CNNs, where biases are tuned to a\n\u201cnatural\u201d working range. This can be addressed by consid-\nering the objective \u2225\u03a6(\u03c3x) \u2212\u03a60\u22252\n2/\u2225\u03a60\u22252\n2 + R(x) where\nthe scaling \u03c3 is the average Euclidean norm of natural im-\nages in a training set.\nSecond, the multiplier \u03bb\u03b1 of the \u03b1-norm regularizer\nshould be selected to encourage the reconstructed image\n\u03c3x to be contained in a natural range [\u2212B, B] (e.g. in\nmost CNN implementations B\n= 128).\nIf most pix-\nels in \u03c3x have a magnitude similar to B, then R\u03b1(x) \u2248\nHWB\u03b1/\u03c3\u03b1, and \u03bb\u03b1 \u2248\u03c3\u03b1/(HWB\u03b1). A similar argu-\nment suggests to pick the V \u03b2-norm regulariser coef\ufb01cient\nas \u03bbV \u03b2 \u2248\u03c3\u03b2/(HW(aB)\u03b2), where a is a small fraction\n(e.g. a = 1%) relating the dynamic range of the image to\nthat of its gradient.\nThe \ufb01nal form of the objective function is\n\u2225\u03a6(\u03c3x) \u2212\u03a60\u22252\n2/\u2225\u03a60\u22252\n2 + \u03bb\u03b1R\u03b1(x) + \u03bbV \u03b2RV \u03b2(x) (3)\nIt is in general non convex because of the nature of \u03a6. We\nnext discuss how to optimize it.\n2.1. Optimisation\nFinding an optimizer of the objective (1) may seem a\nhopeless task as most representations \u03a6 involve strong non-\nlinearities; in particular, deep representations are a chain\nof several non-linear layers. Nevertheless, simple gradient\ndescent (GD) procedures have been shown to be very effec-\ntive in learning such models from data, which is arguably\nan even harder task. Hence, it is not unreasonable to use\nGD to solve (1) too. We extend GD to incorporate a few ex-\ntensions that proved useful in learning deep networks [13],\nas discussed below.\nMomentum. GD is extended to use momentum:\n\u00b5t+1 \u2190m\u00b5t \u2212\u03b7t\u2207E(x),\nxt+1 \u2190xt + \u00b5t\n3\nwhere E(x) = \u2113(\u03a6(x), \u03a60) + \u03bbR(x) is the objective func-\ntion. The vector \u00b5t is a weighed average of the last several\ngradients, with decaying factor m = 0.9. Learning pro-\nceeds a few hundred iterations with a \ufb01xed learning rate \u03b7t\nand is reduced tenfold, until convergence.\nComputing derivatives. Applying GD requires comput-\ning the derivatives of the loss function composed with the\nrepresentation \u03a6(x). While the squared Euclidean loss is\nsmooth, this is not the case for the representation. A key\nfeature of CNNs is the ability of computing the deriva-\ntives of each computational layer, composing the latter in\nan overall derivative of the whole function using back-\npropagation. Our translation of HOG and DSIFT into CNN\nallows us to apply the same technique to these computer\nvision representations too.\n3. Representations\nThis section describes the image representations stud-\nied in the paper: DSIFT (Dense-SIFT), HOG, and refer-\nence deep CNNs. Furthermore, it shows how to implement\nDSIFT and HOG in a standard CNN framework in order to\ncompute their derivatives. Being able to compute deriva-\ntives is the only requirement imposed by the algorithm of\nSect. 2.1. Implementing DSIFT and HOG in a standard\nCNN framework makes derivative computation convenient.\nCNN-A: deep networks.\nAs a reference deep network\nwe consider the Caffe-Alex [11] model (CNN-A), which\nclosely reproduces the network by Krizhevsky et al. [13].\nThis and many other similar networks alternate the fol-\nlowing computational building blocks: linear convolution,\nReLU gating, spatial max-pooling, and group normalisa-\ntion. Each such block takes as input a d-dimensional image\nand produces as output a k-dimensional one. Blocks can\nadditionally pad the image (with zeros for the convolutional\nblocks and with \u2212\u221efor max pooling) or subsample the\ndata. The last several layers are deemed \u201cfully connected\u201d\nas the support of the linear \ufb01lters coincides with the size of\nthe image; however, they are equivalent to \ufb01ltering layers in\nall other respects. Table 2 details the structure of CNN-A.\nCNN-DSIFT and CNN-HOG. This section shows how\nDSIFT [17, 20] and HOG [4] can be implemented as CNNs.\nThis formalises the relation between CNNs and these stan-\ndard representations.\nIt also makes derivative computa-\ntion for these representations simple; for the inversion al-\ngorithm of Sect. 2. The DSIFT and HOG implementations\nin the VLFeat library [29] are used as numerical references.\nThese are equivalent to Lowe\u2019s [17] SIFT and the DPM\nV5 HOG [6, 7].\nSIFT and HOG involve: computing and binning image\ngradients, pooling binned gradients into cell histograms,\ngrouping cells into blocks, and normalising the blocks. De-\nnote by g the gradient at a given pixel and consider binning\nthis into one of K orientations (where K = 8 for SIFT and\nK = 18 for HOG). This can be obtained in two steps: di-\nrectional \ufb01ltering and gating. The k-th directional \ufb01lter is\nGk = u1kGx + u2kGy where\nuk =\n\u0014\ncos 2\u03c0k\nK\nsin 2\u03c0k\nK\n\u0015\n,\nGx =\n\uf8ee\n\uf8f0\n0\n0\n0\n\u22121\n0\n1\n0\n0\n0\n\uf8f9\n\uf8fb,\nGy = G\u22a4\nx .\nThe output of a directional \ufb01lter is the projection \u27e8g, uk\u27e9of\nthe gradient along direction uk. A suitable gating function\nimplements binning into a histogram element hk. DSIFT\nuses bilinear orientation binning, given by\nhk = \u2225g\u2225max\n\u001a\n0, 1 \u2212K\n2\u03c0 cos\u22121 \u27e8g, uk\u27e9\n\u2225g\u2225\n\u001b\n,\nwhereas HOG (in the DPM V5 variant) uses hard assign-\nments hk = \u2225g\u22251 [\u27e8g, uk\u27e9> \u2225g\u2225cos \u03c0/K]. Filtering is\na standard CNN operation but these binning functions are\nnot. While their implementation is simple, an interesting\nalternative is the approximated bilinear binning:\nhk \u2248\u2225g\u2225max\n\u001a\n0,\n1\n1 \u2212a\n\u27e8g, uk\u27e9\n\u2225g\u2225\n\u2212\na\n1 \u2212a\n\u001b\n\u221dmax {0, \u27e8g, uk\u27e9\u2212a\u2225g\u2225} ,\na = cos 2\u03c0/K.\nThe norm-dependent offset \u2225g\u2225is still non-standard, but the\nReLU operator is, which shows to which extent approxi-\nmate binning can be achieved in typical CNNs.\nThe next step is to pool the binned gradients into cell\nhistograms using bilinear spatial pooling, followed by ex-\ntracting blocks of 2 \u00d7 2 (HOG) or 4 \u00d7 4 (SIFT) cells. Both\nsuch operations can be implemented by banks of linear \ufb01l-\nters. Cell blocks are then l2 normalised, which is a special\ncase of the standard local response normalisation layer. For\nHOG, blocks are further decomposed back into cells, which\nrequires another \ufb01lter bank. Finally, the descriptor values\nare clamped from above by applying y = min{x, 0.2} to\neach component, which can be reduced to a combination of\nlinear and ReLU layers.\nThe conclusion is that approximations to DSIFT and\nHOG can be implemented with conventional CNN compo-\nnents plus the non-conventional gradient norm offset. How-\never, all the \ufb01lters involved are much sparser and simpler\nthan the generic 3D \ufb01lters in learned CNNs. Nonetheless,\nin the rest of the paper we will use exact CNN equivalents of\nDSIFT and HOG, using modi\ufb01ed or additional CNN com-\nponents as needed.\n1 These CNNs are numerically indis-\n1This requires addressing a few more subtleties. In DSIFT gradient\ncontributions are usually weighted by a Gaussian centered at each descrip-\ntor (a 4 \u00d7 4 cell block); here we use the VLFeat approximation (fast op-\ntion) of weighting cells rather than gradients, which can be incorporated in\nthe block-forming \ufb01lters. In UoCTTI HOG, cells contain both oriented and\nunoriented gradients (27 components in total) as well as 4 texture compo-\nnents. The latter are ignored for simplicity, while the unoriented gradients\nare obtained as average of the oriented ones in the block-forming \ufb01lters.\n4\ndescriptors\nHOG\nHOG\nHOGb\nDSIFT\nmethod\nHOGgle\nour\nour\nour\nerror (%)\n66.20\n28.10\n10.67\n10.89\n\u00b113.7\n\u00b17.9\n\u00b15.2\n\u00b17.5\nTable 1. Average reconstruction error of different representation\ninversion methods, applied to HOG and DSIFT. HOGb denotes\nHOG with bilinear orientation assignments. The standard devia-\ntion shown is the standard deviation of the error and not the stan-\ndard deviation of the mean error.\nFigure 4. Effect of V \u03b2 regularization. The same inversion algo-\nrithm visualized in Fig. 3(d) is used with a smaller (\u03bbV \u03b2 = 0.5),\ncomparable (\u03bbV \u03b2 = 5.0), and larger (\u03bbV \u03b2 = 50) regularisation\ncoef\ufb01cient.\ntinguishable from the VLFeat reference implementations,\nbut, true to their CNN nature, allow computing the feature\nderivatives as required by the algorithm of Sect. 2.\nNext we apply the algorithm from Sect. 2 on CNN-A,\nCNN-DSIFT and CNN-HOG to analyze our method.\n4. Experiments with shallow representations\nThis section evaluates the representation inversion\nmethod of Sect. 2 by applying it to HOG and DSIFT. The\nanalysis includes both a qualitative (Fig. 3) and quantitative\n(Table 1) comparison with existing technique. The quanti-\ntative evaluation reports a normalized reconstruction error\n\u2225\u03a6(x\u2217) \u2212\u03a6(xi)\u22252/N\u03a6 averaged over 100 images xi from\nthe ILSVRC 2012 challenge [22] validation data (images 1\nto 100). A normalization is essential to place the Euclidean\ndistance in the context of the volume occupied by the fea-\ntures: if the features are close together, then even an Eu-\nclidean distance of 0.1 is very large, but if the features are\nspread out, then even an Euclidean distance of 105 may be\nvery small. We use N\u03a6 to be the average pairwise euclidean\ndistance between \u03a6(xi)\u2019s across the 100 test images.\nWe \ufb01x the parameters in equation 3 to \u03bb\u03b1 = 2.16 \u00d7 108,\n\u03bbV \u03b2 = 5, and \u03b2 = 2.\nThe closest alternative to our method is HOGgle, a tech-\nnique introduced by Vondrick et al. [31] for the visual-\nisation of HOG features.\nThe HOGgle code is publicly\navailable from the authors\u2019 website and is used through-\nout these experiments. Crucially, HOGgle is pre-trained to\ninvert the UoCTTI implementation of HOG, which is nu-\nmerically equivalent to CNN-HOG (Sect. 3), allowing for a\ndirect comparison between algorithms.\nCuriously, in UoCTTI HOG the l2 normalisation factor is computed con-\nsidering only the unoriented gradient components in a block, but applied\nto all, which requires modifying the normalization operator. Finally, when\nblocks are decomposed back to cells, they are averaged rather than stacked\nas in the original Dalal-Triggs HOG, which can be implemented in the\nblock-decomposition \ufb01lters.\na\nb\nc\nd\nFigure 5. Test images for qualitative results.\nCompared to our method, HOGgle is fast (2-3s vs 60s\non the same CPU) but not very accurate, as it is apparent\nboth qualitatively (Fig. 3.c vs d) and quantitatively (66%\nvs 28% reconstruction error, see Table. 1). Interestingly,\n[31] propose a direct optimisation method similar to (1),\nbut show that it does not perform better than HOGgle. This\ndemonstrates the importance of the choice of regulariser\nand the ability of computing the derivative of the represen-\ntation. The effect of the regularizer \u03bbV \u03b2 is further analysed\nin Fig. 4 (and later in Table 3): without this prior infor-\nmation, the reconstructions present a signi\ufb01cant amount of\ndiscretization artifacts.\nIn terms of speed, an advantage of optimizing (1) is that\nit can be switched to use GPU code immediately given the\nunderlying CNN framework; doing so results in a ten-fold\nspeedup. Furthermore the CNN-based implementation of\nHOG and DSIFT wastes signi\ufb01cant resources using generic\n\ufb01ltering code despite the particular nature of the \ufb01lters in\nthese two representations.\nHence we expect that an op-\ntimized implementation could be several times faster than\nthis.\nIt is also apparent that different representations can be\neasier or harder to invert. In particular, modifying HOG\nto use bilinear gradient orientation assignments as SIFT\n(Sect. 3) signi\ufb01cantly reduces the reconstruction error (from\n28% down to 11%) and improves the reconstruction quality\n(Fig. 3.e). More impressive is DSIFT: it is quantitatively\nsimilar to HOG with bilinear orientations, but produces sig-\nni\ufb01cantly more detailed images (Fig. 3.f). Since HOG uses\na \ufb01ner quantisation of the gradient compared to SIFT but\notherwise the same cell size and sampling, this result can\nbe imputed to the heavier block-normalisation of HOG that\nevidently discards more image information than SIFT.\n5. Experiments with deep representations\nFigure 8. Effect of V \u03b2 regularization on CNNs. Inversions of the\nlast layers of CNN-A for Fig. 5.d with a progressively larger regu-\nlariser \u03bbV \u03b2. This image is best viewed in color/screen.\nThis section evaluates the inversion method applied to\nCNN-A described in Sect. 3.\nCompared to CNN-HOG\n5\n(a) Orig.\n(b) HOG\n(c) HOGgle [31]\n(d) HOG\u22121\n(e) HOGb\u22121\n(f) DSIFT\u22121\nFigure 3. Reconstruction quality of different representation inversion methods, applied to HOG and DSIFT. HOGb denotes HOG with\nbilinear orientation assignments. This image is best viewed on screen.\nlayer\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\nname\nconv1 relu1 mpool1 norm1 conv2 relu2 mpool2 norm2 conv3 relu3 conv4 relu4 conv5 relu5 mpool5\nfc6\nrelu6\nfc7\nrelu7\nfc8\ntype\ncnv\nrelu\nmpool\nnrm\ncnv\nrelu\nmpool\nnrm\ncnv\nrelu\ncnv\nrelu\ncnv\nrelu\nmpool\ncnv\nrelu\ncnv\nrelu\ncnv\nchannels\n96\n96\n96\n96\n256\n256\n256\n256\n384\n384\n384\n384\n256\n256\n256\n4096 4096 4096 4096 1000\nrec. \ufb01eld\n11\n11\n19\n19\n51\n51\n67\n67\n99\n99\n131\n131\n163\n163\n195\n355\n355\n355\n355\n355\nTable 2. CNN-A structure. The table speci\ufb01es the structure of CNN-A along with receptive \ufb01eld size of each neuron. The \ufb01lters in layers\nfrom 16 to 20 operate as \u201cfully connected\u201d: given the standard image input size of 227 \u00d7 227 pixels, their support covers the whole image.\nNote also that their receptive \ufb01eld is larger than 227 pixels, but can be contained in the image domain due to padding.\nand CNN-DSIFT, this network is signi\ufb01cantly larger and\ndeeper. It seems therefore that the inversion problem should\nbe considerably harder. Also, CNN-A is not handcrafted but\nlearned from 1.2M images of the ImageNet ILSVRC 2012\ndata [22].\nThe algorithm of Sect. 2.1 is used to invert the code ob-\ntained from each individual CNN layer for 100 ILSVRC\nvalidation images (these were not used to train the CNN-A\nmodel [13]). Similar to Sect. 4, the normalized inversion er-\nror is computed and reported in Table 3. The experiment is\nrepeated by \ufb01xing \u03bb\u03b1 to a \ufb01xed value of 2.16\u00d7108 and grad-\nually increasing \u03bbV \u03b2 ten-folds, starting from a relatively\nsmall value \u03bb1 = 0.5. The ImageNet ILSVRC mean im-\nage is added back to the reconstruction before visualisation\nas this is subtracted when training the network. Somewhat\nsurprisingly, the quantitative results show that CNNs are, in\nfact, not much harder to invert than HOG. The error rarely\nexceeds 20%, which is comparable to the accuracy of HOG\n(Sect. 4). The last layer is in particular easy to invert with\nan average error of 8.5%.\nWe choose the regularizer coef\ufb01cients for each represen-\ntation/layer based on a quantitative and qualitative study\nof the reconstruction. We pick \u03bb1 = 0.5 for layers 1-6,\n\u03bb2 = 5.0 for layers 7-12 and \u03bb3 = 50 for layers 13-20. The\nerror value corresponding to these parameters is marked in\nbold face in table 3. Increasing \u03bbV \u03b2 causes a deterioration\nfor the \ufb01rst layers, but for the latter layers it helps recover a\nmore visually interpretable reconstruction. Though this pa-\nrameter can be tuned by cross validation on the normalized\nreconstruction error, a selection based on qualitative analy-\nsis is preferred because the method should yield images that\nare visually meaningful.\nQualitatively, Fig. 6 illustrates the reconstruction for a\ntest image from each layer of CNN-A. The progression is\nremarkable. The \ufb01rst few layers are essentially an invert-\nible code of the image. All the convolutional layers main-\ntain a photographically faithful representation of the image,\nalthough with increasing fuzziness. The 4,096-dimensional\nfully connected layers are perhaps more interesting, as they\ninvert back to a composition of parts similar but not iden-\ntical to the ones found in the original image. Going from\nrelu7 to fc8 reduces the dimensionality further to just 1,000;\nnevertheless some of these visual elements can still be iden-\nti\ufb01ed. Similar effects can be observed in the reconstructions\nin Fig. 7. This \ufb01gure includes also the reconstruction of an\nabstract pattern, which is not included in any of the Ima-\ngeNet classes; still, all CNN codes capture distinctive visual\nfeatures of the original pattern, clearly indicating that even\nvery deep layers capture visual information.\nNext, Fig. 7 examines the invariance captured by the\nCNN model by considering multiple reconstructions out of\neach deep layer. A careful examination of these images re-\n6\nconv1\nrelu1\nmpool1\nnorm1\nconv2\nrelu2\nmpool2\nnorm2\nconv3\nrelu3\nconv4\nrelu4\nconv5\nrelu5\nmpool5\nfc6\nrelu6\nfc7\nrelu7\nfc8\nFigure 6. CNN reconstruction. Reconstruction of the image of Fig. 5.a from each layer of CNN-A. To generate these results, the regular-\nization coef\ufb01cient for each layer is chosen to match the highlighted rows in table 3. This \ufb01gure is best viewed in color/screen.\n\u03bbV \u03b2\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\nconv1 relu1 pool1 norm1 conv2 relu2 pool2 norm2 conv3 relu3 conv4 relu4 conv5 relu5 pool5 fc6 relu6 fc7 relu7 fc8\n\u03bb1\n10.0 11.3 21.9 20.3 12.4 12.9 15.5\n15.9\n14.5 16.5 14.9 13.8 12.6 15.6 16.6 12.4 15.8 12.8 10.5 5.3\n\u00b15.0\n\u00b15.5\n\u00b19.2\n\u00b15.0\n\u00b13.1\n\u00b15.3\n\u00b14.7\n\u00b14.6\n\u00b14.7\n\u00b15.3\n\u00b13.8\n\u00b13.8\n\u00b12.8\n\u00b15.1\n\u00b14.6\n\u00b13.5\n\u00b14.5\n\u00b16.4\n\u00b11.9\n\u00b11.1\n\u03bb2\n20.2 22.4 30.3\n28.2\n20.0 17.4 18.2 18.4 14.4 15.1 13.3 14.0 15.4 13.9 15.5 14.2 13.7 15.4 10.8 5.9\n\u00b19.3\n\u00b110.3\n\u00b113.6\n\u00b17.6\n\u00b14.9\n\u00b15.0\n\u00b15.5\n\u00b15.0\n\u00b13.6\n\u00b13.3\n\u00b12.6\n\u00b12.8\n\u00b12.7\n\u00b13.2\n\u00b13.5\n\u00b13.7\n\u00b13.1\n\u00b110.3\n\u00b11.6\n\u00b10.9\n\u03bb3\n40.8 45.2 54.1\n48.1\n39.7 32.8 32.7\n32.4\n25.6 26.9 23.3 23.9 25.7 20.1 19.0 18.6 18.7 17.1 15.5 8.5\n\u00b117.0\n\u00b118.7\n\u00b122.7\n\u00b111.8\n\u00b19.1\n\u00b17.7\n\u00b18.0\n\u00b17.0\n\u00b15.6\n\u00b15.2\n\u00b14.1\n\u00b14.6\n\u00b14.3\n\u00b14.3\n\u00b14.3\n\u00b14.9\n\u00b13.8\n\u00b13.4\n\u00b12.1\n\u00b11.3\nTable 3. Inversion error for CNN-A. Average inversion percentage error (normalized) for all the layers of CNN-A and various amounts\nof V \u03b2 regularisation: \u03bb1 = 0.5, \u03bb2 = 10\u03bb1 and \u03bb3 = 100\u03bb1. In bold face are the error values corresponding to the regularizer that works\nbest both qualitatively and quantitatively. The deviations speci\ufb01ed in this table are the standard deviations of the errors and not the standard\ndeviations of the mean error value.\npool5\nrelu6\nrelu7\nfc8\npool5\nrelu6\nrelu7\nfc8\nFigure 7. CNN invariances. Multiple reconstructions of the images of Fig. 5.c\u2013d from different deep codes obtained from CNN-A. This\n\ufb01gure is best seen in colour/screen.\nveals that the codes capture progressively larger deforma-\ntions of the object. In the \u201c\ufb02amingo\u201d reconstruction, in par-\nticular, relu7 and fc8 invert back to multiple copies of the\nobject/parts at different positions and scales.\nNote that all these and the original images are nearly in-\ndistinguishable from the viewpoint of the CNN model; it is\ntherefore interesting to note the lack of detail in the deep-\nest reconstructions, showing that the network captures just a\nsketch of the objects, which evidently suf\ufb01ces for classi\ufb01ca-\ntion. Considerably lowering the regulariser parameter still\nyields very accurate inversions, but this time with barely any\nresemblance to a natural image. This con\ufb01rms that CNNs\nhave strong non-natural confounders.\nWe now examine reconstructions obtained from subset\nof neural responses in different CNN layers. Fig. 9 explores\nthe locality of the codes by reconstructing a central 5 \u00d7 5\npatch of features in each layer. The regulariser encourages\nportions of the image that do not contribute to the neural\n7\nconv1\nrelu1\nmpool1\nnorm1\nconv2\nrelu2\nmpool2\nnorm2\nconv3\nrelu3\nconv4\nrelu4\nconv5\nrelu5\nFigure 9. CNN receptive \ufb01eld. Reconstructions of the image of Fig. 5.a from the central 5 \u00d7 5 neuron \ufb01elds at different depths of CNN-A.\nThe white box marks the \ufb01eld of view of the 5 \u00d7 5 neuron \ufb01eld. The \ufb01eld of view is the entire image for conv5 and relu5.\nconv1-grp1\nnorm1-grp1\nnorm2-grp1\nconv1-grp1\nnorm1-grp1\nnorm2-grp1\nconv1-grp2\nnorm1-grp2\nnorm2-grp2\nconv1-grp2\nnorm1-grp2\nnorm2-grp2\nFigure 10. CNN neural streams. Reconstructions of the images of Fig. 5.c-b from either of the two neural streams of CNN-A. This \ufb01gure\nis best seen in colour/screen.\nresponses to be switched off. The locality of the features is\nobvious in the \ufb01gure; what is less obvious is that the effec-\ntive receptive \ufb01eld of the neurons is in some cases signi\ufb01-\ncantly smaller than the theoretical one - shown as a white\nbox in the image.\nFinally, Fig. 10 reconstructs images from a subset of fea-\nture channels. CNN-A contains in fact two subsets of fea-\nture channels which are independent for the \ufb01rst several lay-\ners (up to norm2) [13]. Reconstructing from each subset\nindividually, clearly shows that one group is tuned towards\nlow-frequency colour information whereas the second one\nis tuned to towards high-frequency luminance components.\nRemarkably, this behaviour emerges naturally in the learned\nnetwork without any mechanism directly encouraging this\npattern.\n6. Summary\nThis paper proposed an optimisation method to invert\nshallow and deep representations based on optimizing an\nobjective function with gradient descent. Compared to al-\nternatives, a key difference is the use of image priors such as\nthe V \u03b2 norm that can recover the low-level image statistics\nremoved by the representation. This tool performs better\nFigure 11. Diversity in the CNN model. mpool5 reconstructions\nshow that the network retains rich information even at such deep\nlevels. This \ufb01gure is best viewed in color/screen (zoom in).\nthan alternative reconstruction methods for HOG. Applied\nto CNNs, the visualisations shed light on the information\nrepresented at each layer. In particular, it is clear that a pro-\ngressively more invariant and abstract notion of the image\ncontent is formed in the network.\nIn the future, we shall experiment with more expres-\nsive natural image priors and analyze the effect of network\nhyper-parameters on the reconstructions. We shall extract\nsubsets of neurons that encode object parts and try to estab-\nlish sub-networks that capture different details of the image.\n8\nReferences\n[1] C. M. Bishop. Neural Networks for Pattern Recognition. Clarendon\nPress, Oxford, 1995.\n[2] Y. Chen, R. Ranftl, and T. Pock. A bi-level view of inpainting-based\nimage compression. In Proc. of Computer Vision Winter Workshop,\n2014.\n[3] G. Csurka, C. R. Dance, L. Dan, J. Willamowski, and C. Bray. Visual\ncategorization with bags of keypoints. In Proc. ECCV Workshop on\nStat. Learn. in Comp. Vision, 2004.\n[4] N. Dalal and B. Triggs. Histograms of oriented gradients for human\ndetection. In CVPR, 2005.\n[5] D. Erhan, Y. Bengio, A. Courville, and P. Vincent.\nVisualizing\nhigher-layer features of a deep network. Technical Report 1341, Uni-\nversity of Montreal, 2009.\n[6] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan.\nObject detection with discriminatively trained part based models.\nIEEE Transactions on Pattern Analysis and Machine Intelligence,\n32(9):1627\u20131645, 2010.\n[7] R. B. Girshick, P. F. Felzenszwalb, and D. McAllester.\nDiscrim-\ninatively trained deformable part models, release 5.\nhttp://\npeople.cs.uchicago.edu/\u02dcrbg/latent-release5/.\n[8] G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality\nof data with neural networks. Science, 313(5786), 2006.\n[9] H. J\u00b4egou, M. Douze, C. Schmid, and P. P\u00b4erez. Aggregating local\ndescriptors into a compact image representation. In CVPR, 2010.\n[10] C. A. Jensen, R. D. Reed, R. J. Marks, M. El-Sharkawi, J.-B. Jung,\nR. Miyamoto, G. Anderson, and C. Eggen. Inversion of feedforward\nneural networks: algorithms and applications. Proc. of the IEEE,\n87(9), 1999.\n[11] Y. Jia.\nCaffe: An open source convolutional architecture for fast\nfeature embedding. http://caffe.berkeleyvision.org/,\n2013.\n[12] D. Krishnan and R. Fergus. Fast image deconvolution using hyper-\nlaplacian priors. In NIPS, 2009.\n[13] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classi\ufb01ca-\ntion with deep convolutional neural networks. In NIPS, 2012.\n[14] S. Lee and R. M. Kil. Inverse mapping of continuous functions using\nlocal and global information. IEEE Trans. on Neural Networks, 5(3),\n1994.\n[15] T. Leung and J. Malik. Representing and recognizing the visual ap-\npearance of materials using three-dimensional textons. IJCV, 43(1),\n2001.\n[16] A. Linden and J. Kindermann. Inversion of multilayer nets. In Proc.\nInt. Conf. on Neural Networks, 1989.\n[17] D. G. Lowe. Object recognition from local scale-invariant features.\nIn ICCV, 1999.\n[18] D. G. Lowe. Distinctive image features from scale-invariant key-\npoints. IJCV, 2(60):91\u2013110, 2004.\n[19] B.-L. Lu, H. Kita, and Y. Nishikawa. Inverting feedforward neural\nnetworks using linear and nonlinear programming. IEEE Trans. on\nNeural Networks, 10(6), 1999.\n[20] E. Nowak, F. Jurie, and B. Triggs. Sampling strategies for bag-of-\nfeatures image classi\ufb01cation. In ECCV, 2006.\n[21] F. Perronnin and C. Dance. Fisher kernels on visual vocabularies for\nimage categorizaton. In CVPR, 2006.\n[22] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,\nZ. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and\nL. Fei-Fei.\nImageNet Large Scale Visual Recognition Challenge,\n2014.\n[23] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. Le-\nCun. Overfeat: Integrated recognition, localization and detection us-\ning convolutional networks. In CoRR, volume abs/1312.6229, 2014.\n[24] K. Simonyan, A. Vedaldi, and A. Zisserman.\nDeep inside con-\nvolutional networks: Visualising image classi\ufb01cation models and\nsaliency maps. In Proc. ICLR, 2014.\n[25] J. Sivic and A. Zisserman. Video Google: A text retrieval approach\nto object matching in videos. In ICCV, 2003.\n[26] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. J. Good-\nfellow, and R. Fergus.\nIntriguing properties of neural networks.\nCoRR, abs/1312.6199, 2013.\n[27] A. Tatu, F. Lauze, M. Nielsen, and B. Kimia. Exploring the rep-\nresentation capabilities of the HOG descriptor. In ICCV Workshop,\n2011.\n[28] A. R. V\u00b4arkonyi-K\u00b4oczy and A. R\u00a8ovid. Observer based iterative neu-\nral network model inversion. In IEEE Int. Conf. on Fuzzy Systems,\n2005.\n[29] A. Vedaldi. An open implementation of the SIFT detector and de-\nscriptor. Technical Report 070012, UCLA CSD, 2007.\n[30] A. Vedaldi and K. Lenc. MatConvNet: CNNs for MATLAB. http:\n//www.vlfeat.org/matconvnet/, 2014.\n[31] C. Vondrick, A. Khosla, T. Malisiewicz, and A. Torralba. HOGgles:\nVisualizing object detection features. In ICCV, 2013.\n[32] J. Wang, J. Yang, K. Yu, F. Lv, T. Huang, and Y. Gong. Locality-\nconstrained linear coding for image classi\ufb01cation. CVPR, 2010.\n[33] P. Weinzaepfel, H. J\u00b4egou, and P. P\u00b4erez. Reconstructing an image\nfrom its local descriptors. In CVPR, 2011.\n[34] R. J. Williams. Inverting a connectionist network mapping by back-\npropagation of error. In Proc. CogSci, 1986.\n[35] J. Yang, K. Yu, and T. Huang. Supervised translation-invariant sparse\ncoding. In CVPR, 2010.\n[36] M. D. Zeiler and R. Fergus. Visualizing and understanding convolu-\ntional networks. In ECCV, 2014.\n[37] X. Zhou, K. Yu, T. Zhang, and T. S. Huang. Image classi\ufb01cation us-\ning super-vector coding of local image descriptors. In ECCV, 2010.\n9\n",
        "sentence": "",
        "context": "from 16 to 20 operate as \u201cfully connected\u201d: given the standard image input size of 227 \u00d7 227 pixels, their support covers the whole image.\nZ. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and\nL. Fei-Fei.\nImageNet Large Scale Visual Recognition Challenge,\n2014.\n[23] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. Le-\ncrucial component of almost any image understanding sys-\ntem. Nevertheless, our understanding of them remains lim-\nited. In this paper we conduct a direct analysis of the visual\ninformation contained in representations by asking the fol-"
    },
    {
        "title": "Deep multiscale video prediction beyond mean square error",
        "author": [
            "M. Mathieu",
            "C. Couprie",
            "Y. LeCun"
        ],
        "venue": "URL http://arxiv. org/abs/1511.05440",
        "citeRegEx": "Mathieu et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "Mathieu et al\\.",
        "year": 2015,
        "abstract": "",
        "full_text": "",
        "sentence": " Recently Mathieu et al. (2015) used GANs for predicting future frames in videos by conditioning on previous frames.",
        "context": null
    },
    {
        "title": "Conditional generative adversarial nets",
        "author": [
            "M. Mirza",
            "S. Osindero"
        ],
        "venue": null,
        "citeRegEx": "Mirza and Osindero.,? \\Q2014\\E",
        "shortCiteRegEx": "Mirza and Osindero.",
        "year": 2014,
        "abstract": "Generative Adversarial Nets [8] were recently introduced as a novel way to\ntrain generative models. In this work we introduce the conditional version of\ngenerative adversarial nets, which can be constructed by simply feeding the\ndata, y, we wish to condition on to both the generator and discriminator. We\nshow that this model can generate MNIST digits conditioned on class labels. We\nalso illustrate how this model could be used to learn a multi-modal model, and\nprovide preliminary examples of an application to image tagging in which we\ndemonstrate how this approach can generate descriptive tags which are not part\nof training labels.",
        "full_text": "Conditional Generative Adversarial Nets\nMehdi Mirza\nD\u00b4epartement d\u2019informatique et de recherche op\u00b4erationnelle\nUniversit\u00b4e de Montr\u00b4eal\nMontr\u00b4eal, QC H3C 3J7\nmirzamom@iro.umontreal.ca\nSimon Osindero\nFlickr / Yahoo Inc.\nSan Francisco, CA 94103\nosindero@yahoo-inc.com\nAbstract\nGenerative Adversarial Nets [8] were recently introduced as a novel way to train\ngenerative models. In this work we introduce the conditional version of generative\nadversarial nets, which can be constructed by simply feeding the data, y, we wish\nto condition on to both the generator and discriminator. We show that this model\ncan generate MNIST digits conditioned on class labels. We also illustrate how\nthis model could be used to learn a multi-modal model, and provide preliminary\nexamples of an application to image tagging in which we demonstrate how this\napproach can generate descriptive tags which are not part of training labels.\n1\nIntroduction\nGenerative adversarial nets were recently introduced as an alternative framework for training gen-\nerative models in order to sidestep the dif\ufb01culty of approximating many intractable probabilistic\ncomputations.\nAdversarial nets have the advantages that Markov chains are never needed, only backpropagation is\nused to obtain gradients, no inference is required during learning, and a wide variety of factors and\ninteractions can easily be incorporated into the model.\nFurthermore, as demonstrated in [8], it can produce state of the art log-likelihood estimates and\nrealistic samples.\nIn an unconditioned generative model, there is no control on modes of the data being generated.\nHowever, by conditioning the model on additional information it is possible to direct the data gener-\nation process. Such conditioning could be based on class labels, on some part of data for inpainting\nlike [5], or even on data from different modality.\nIn this work we show how can we construct the conditional adversarial net. And for empirical results\nwe demonstrate two set of experiment. One on MNIST digit data set conditioned on class labels and\none on MIR Flickr 25,000 dataset [10] for multi-modal learning.\n1\narXiv:1411.1784v1  [cs.LG]  6 Nov 2014\n2\nRelated Work\n2.1\nMulti-modal Learning For Image Labelling\nDespite the many recent successes of supervised neural networks (and convolutional networks in\nparticular) [13, 17], it remains challenging to scale such models to accommodate an extremely large\nnumber of predicted output categories. A second issue is that much of the work to date has focused\non learning one-to-one mappings from input to output. However, many interesting problems are\nmore naturally thought of as a probabilistic one-to-many mapping. For instance in the case of\nimage labeling there may be many different tags that could appropriately applied to a given image,\nand different (human) annotators may use different (but typically synonymous or related) terms to\ndescribe the same image.\nOne way to help address the \ufb01rst issue is to leverage additional information from other modalities:\nfor instance, by using natural language corpora to learn a vector representation for labels in which\ngeometric relations are semantically meaningful. When making predictions in such spaces, we ben-\ne\ufb01t from the fact that when prediction errors we are still often \u2018close\u2019 to the truth (e.g. predicting\n\u2019table\u2019 instead of \u2019chair\u2019), and also from the fact that we can naturally make predictive generaliza-\ntions to labels that were not seen during training time. Works such as [3] have shown that even a\nsimple linear mapping from image feature-space to word-representation-space can yield improved\nclassi\ufb01cation performance.\nOne way to address the second problem is to use a conditional probabilistic generative model, the\ninput is taken to be the conditioning variable and the one-to-many mapping is instantiated as a\nconditional predictive distribution.\n[16] take a similar approach to this problem, and train a multi-modal Deep Boltzmann Machine on\nthe MIR Flickr 25,000 dataset as we do in this work.\nAdditionally, in [12] the authors show how to train a supervised multi-modal neural language model,\nand they are able to generate descriptive sentence for images.\n3\nConditional Adversarial Nets\n3.1\nGenerative Adversarial Nets\nGenerative adversarial nets were recently introduced as a novel way to train a generative model.\nThey consists of two \u2018adversarial\u2019 models: a generative model G that captures the data distribution,\nand a discriminative model D that estimates the probability that a sample came from the training\ndata rather than G. Both G and D could be a non-linear mapping function, such as a multi-layer\nperceptron.\nTo learn a generator distribution pg over data data x, the generator builds a mapping function from\na prior noise distribution pz(z) to data space as G(z; \u03b8g). And the discriminator, D(x; \u03b8d), outputs\na single scalar representing the probability that x came form training data rather than pg.\nG and D are both trained simultaneously: we adjust parameters for G to minimize log(1 \u2212D(G(z))\nand adjust parameters for D to minimize logD(X), as if they are following the two-player min-max\ngame with value function V (G, D):\nmin\nG max\nD V (D, G) = Ex\u223cpdata(x)[log D(x)] + Ez\u223cpz(z)[log(1 \u2212D(G(z)))].\n(1)\n3.2\nConditional Adversarial Nets\nGenerative adversarial nets can be extended to a conditional model if both the generator and discrim-\ninator are conditioned on some extra information y. y could be any kind of auxiliary information,\nsuch as class labels or data from other modalities. We can perform the conditioning by feeding y\ninto the both the discriminator and generator as additional input layer.\n2\nIn the generator the prior input noise pz(z), and y are combined in joint hidden representation, and\nthe adversarial training framework allows for considerable \ufb02exibility in how this hidden representa-\ntion is composed. 1\nIn the discriminator x and y are presented as inputs and to a discriminative function (embodied\nagain by a MLP in this case).\nThe objective function of a two-player minimax game would be as Eq 2\nmin\nG max\nD V (D, G) = Ex\u223cpdata(x)[log D(x|y)] + Ez\u223cpz(z)[log(1 \u2212D(G(z|y)))].\n(2)\nFig 1 illustrates the structure of a simple conditional adversarial net.\nFigure 1: Conditional adversarial net\n4\nExperimental Results\n4.1\nUnimodal\nWe trained a conditional adversarial net on MNIST images conditioned on their class labels, encoded\nas one-hot vectors.\nIn the generator net, a noise prior z with dimensionality 100 was drawn from a uniform distribution\nwithin the unit hypercube. Both z and y are mapped to hidden layers with Recti\ufb01ed Linear Unit\n(ReLu) activation [4, 11], with layer sizes 200 and 1000 respectively, before both being mapped to\nsecond, combined hidden ReLu layer of dimensionality 1200. We then have a \ufb01nal sigmoid unit\nlayer as our output for generating the 784-dimensional MNIST samples.\n1For now we simply have the conditioning input and prior noise as inputs to a single hidden layer of a MLP,\nbut one could imagine using higher order interactions allowing for complex generation mechanisms that would\nbe extremely dif\ufb01cult to work with in a traditional generative framework.\n3\nModel\nMNIST\nDBN [1]\n138 \u00b1 2\nStacked CAE [1]\n121 \u00b1 1.6\nDeep GSN [2]\n214 \u00b1 1.1\nAdversarial nets\n225 \u00b1 2\nConditional adversarial nets\n132 \u00b1 1.8\nTable 1: Parzen window-based log-likelihood estimates for MNIST. We followed the same procedure as [8]\nfor computing these values.\nThe discriminator maps x to a maxout [6] layer with 240 units and 5 pieces, and y to a maxout layer\nwith 50 units and 5 pieces. Both of the hidden layers mapped to a joint maxout layer with 240 units\nand 4 pieces before being fed to the sigmoid layer. (The precise architecture of the discriminator\nis not critical as long as it has suf\ufb01cient power; we have found that maxout units are typically well\nsuited to the task.)\nThe model was trained using stochastic gradient decent with mini-batches of size 100 and ini-\ntial learning rate of 0.1 which was exponentially decreased down to .000001 with decay factor of\n1.00004. Also momentum was used with initial value of .5 which was increased up to 0.7. Dropout\n[9] with probability of 0.5 was applied to both the generator and discriminator. And best estimate of\nlog-likelihood on the validation set was used as stopping point.\nTable 1 shows Gaussian Parzen window log-likelihood estimate for the MNIST dataset test data.\n1000 samples were drawn from each 10 class and a Gaussian Parzen window was \ufb01tted to these\nsamples. We then estimate the log-likelihood of the test set using the Parzen window distribution.\n(See [8] for more details of how this estimate is constructed.)\nThe conditional adversarial net results that we present are comparable with some other network\nbased, but are outperformed by several other approaches \u2013 including non-conditional adversarial\nnets. We present these results more as a proof-of-concept than as demonstration of ef\ufb01cacy, and\nbelieve that with further exploration of hyper-parameter space and architecture that the conditional\nmodel should match or exceed the non-conditional results.\nFig 2 shows some of the generated samples. Each row is conditioned on one label and each column\nis a different generated sample.\nFigure 2: Generated MNIST digits, each row conditioned on one label\n4.2\nMultimodal\nPhoto sites such as Flickr are a rich source of labeled data in the form of images and their associated\nuser-generated metadata (UGM) \u2014 in particular user-tags.\n4\nUser-generated metadata differ from more \u2018canonical\u2019 image labelling schems in that they are typ-\nically more descriptive, and are semantically much closer to how humans describe images with\nnatural language rather than just identifying the objects present in an image. Another aspect of\nUGM is that synoymy is prevalent and different users may use different vocabulary to describe the\nsame concepts \u2014 consequently, having an ef\ufb01cient way to normalize these labels becomes impor-\ntant. Conceptual word embeddings [14] can be very useful here since related concepts end up being\nrepresented by similar vectors.\nIn this section we demonstrate automated tagging of images, with multi-label predictions, using con-\nditional adversarial nets to generate a (possibly multi-modal) distribution of tag-vectors conditional\non image features.\nFor image features we pre-train a convolutional model similar to the one from [13] on the full\nImageNet dataset with 21,000 labels [15]. We use the output of the last fully connected layer with\n4096 units as image representations.\nFor the world representation we \ufb01rst gather a corpus of text from concatenation of user-tags, titles\nand descriptions from YFCC100M 2 dataset metadata. After pre-processing and cleaning of the\ntext we trained a skip-gram model [14] with word vector size of 200. And we omitted any word\nappearing less than 200 times from the vocabulary, thereby ending up with a dictionary of size\n247465.\nWe keep the convolutional model and the language model \ufb01xed during training of the adversarial\nnet. And leave the experiments when we even backpropagate through these models as future work.\nFor our experiments we use MIR Flickr 25,000 dataset [10], and extract the image and tags features\nusing the convolutional model and language model we described above. Images without any tag\nwere omitted from our experiments and annotations were treated as extra tags. The \ufb01rst 150,000\nexamples were used as training set. Images with multiple tags were repeated inside the training set\nonce for each associated tag.\nFor evaluation, we generate 100 samples for each image and \ufb01nd top 20 closest words using cosine\nsimilarity of vector representation of the words in the vocabulary to each sample. Then we select\nthe top 10 most common words among all 100 samples. Table 4.2 shows some samples of the user\nassigned tags and annotations along with the generated tags.\nThe best working model\u2019s generator receives Gaussian noise of size 100 as noise prior and maps it\nto 500 dimension ReLu layer. And maps 4096 dimension image feature vector to 2000 dimension\nReLu hidden layer. Both of these layers are mapped to a joint representation of 200 dimension linear\nlayer which would output the generated word vectors.\nThe discriminator is consisted of 500 and 1200 dimension ReLu hidden layers for word vectors and\nimage features respectively and maxout layer with 1000 units and 3 pieces as the join layer which is\n\ufb01nally fed to the one single sigmoid unit.\nThe model was trained using stochastic gradient decent with mini-batches of size 100 and ini-\ntial learning rate of 0.1 which was exponentially decreased down to .000001 with decay factor of\n1.00004. Also momentum was used with initial value of .5 which was increased up to 0.7. Dropout\nwith probability of 0.5 was applied to both the generator and discriminator.\nThe hyper-parameters and architectural choices were obtained by cross-validation and a mix of\nrandom grid search and manual selection (albeit over a somewhat limited search space.)\n5\nFuture Work\nThe results shown in this paper are extremely preliminary, but they demonstrate the potential of\nconditional adversarial nets and show promise for interesting and useful applications.\nIn future explorations between now and the workshop we expect to present more sophisticated mod-\nels, as well as a more detailed and thorough analysis of their performance and characteristics.\n2Yahoo Flickr Creative Common 100M http://webscope.sandbox.yahoo.com/catalog.\nphp?datatype=i&did=67.\n5\nUser tags + annotations\nGenerated tags\nmontanha, trem, inverno,\nfrio, people, male, plant\nlife, tree, structures, trans-\nport, car\ntaxi,\npassenger,\nline,\ntransportation,\nrailway\nstation,\npassengers,\nrailways,\nsignals,\nrail,\nrails\nfood, raspberry, delicious,\nhomemade\nchicken,\nfattening,\ncooked,\npeanut,\ncream,\ncookie,\nhouse\nmade,\nbread, biscuit, bakes\nwater, river\ncreek, lake, along, near,\nriver, rocky, treeline, val-\nley, woods, waters\npeople, portrait, female,\nbaby, indoor\nlove, people, posing, girl,\nyoung, strangers, pretty,\nwomen, happy, life\nTable 2: Samples of generated tags\nAlso, in the current experiments we only use each tag individually. But by using multiple tags at\nthe same time (effectively posing generative problem as one of \u2018set generation\u2019) we hope to achieve\nbetter results.\nAnother obvious direction left for future work is to construct a joint training scheme to learn the\nlanguage model. Works such as [12] has shown that we can learn a language model for suited for\nthe speci\ufb01c task.\nAcknowledgments\nThis project was developed in Pylearn2 [7] framework, and we would like to thank Pylearn2 devel-\nopers. We also like to thank Ian Goodfellow for helpful discussion during his af\ufb01liation at University\nof Montreal. The authors gratefully acknowledge the support from the Vision & Machine Learning,\nand Production Engineering teams at Flickr (in alphabetical order: Andrew Stadlen, Arel Cordero,\nClayton Mellina, Cyprien Noel, Frank Liu, Gerry Pesavento, Huy Nguyen, Jack Culpepper, John\nKo, Pierre Garrigues, Rob Hess, Stacey Svetlichnaya, Tobi Baumgartner, and Ye Lu).\nReferences\n[1] Bengio, Y., Mesnil, G., Dauphin, Y., and Rifai, S. (2013). Better mixing via deep representations. In\nICML\u20192013.\n[2] Bengio, Y., Thibodeau-Laufer, E., Alain, G., and Yosinski, J. (2014). Deep generative stochastic net-\nworks trainable by backprop. In Proceedings of the 30th International Conference on Machine Learning\n(ICML\u201914).\n6\n[3] Frome, A., Corrado, G. S., Shlens, J., Bengio, S., Dean, J., Mikolov, T., et al. (2013). Devise: A deep\nvisual-semantic embedding model. In Advances in Neural Information Processing Systems, pages 2121\u2013\n2129.\n[4] Glorot, X., Bordes, A., and Bengio, Y. (2011). Deep sparse recti\ufb01er neural networks. In International\nConference on Arti\ufb01cial Intelligence and Statistics, pages 315\u2013323.\n[5] Goodfellow, I., Mirza, M., Courville, A., and Bengio, Y. (2013a). Multi-prediction deep boltzmann ma-\nchines. In Advances in Neural Information Processing Systems, pages 548\u2013556.\n[6] Goodfellow, I. J., Warde-Farley, D., Mirza, M., Courville, A., and Bengio, Y. (2013b). Maxout networks.\nIn ICML\u20192013.\n[7] Goodfellow, I. J., Warde-Farley, D., Lamblin, P., Dumoulin, V., Mirza, M., Pascanu, R., Bergstra, J.,\nBastien, F., and Bengio, Y. (2013c).\nPylearn2: a machine learning research library.\narXiv preprint\narXiv:1308.4214.\n[8] Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and\nBengio, Y. (2014). Generative adversarial nets. In NIPS\u20192014.\n[9] Hinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2012). Improving\nneural networks by preventing co-adaptation of feature detectors. Technical report, arXiv:1207.0580.\n[10] Huiskes, M. J. and Lew, M. S. (2008). The mir \ufb02ickr retrieval evaluation. In MIR \u201908: Proceedings of the\n2008 ACM International Conference on Multimedia Information Retrieval, New York, NY, USA. ACM.\n[11] Jarrett, K., Kavukcuoglu, K., Ranzato, M., and LeCun, Y. (2009). What is the best multi-stage architecture\nfor object recognition? In ICCV\u201909.\n[12] Kiros, R., Zemel, R., and Salakhutdinov, R. (2013). Multimodal neural language models. In Proc. NIPS\nDeep Learning Workshop.\n[13] Krizhevsky, A., Sutskever, I., and Hinton, G. (2012). ImageNet classi\ufb01cation with deep convolutional\nneural networks. In Advances in Neural Information Processing Systems 25 (NIPS\u20192012).\n[14] Mikolov, T., Chen, K., Corrado, G., and Dean, J. (2013). Ef\ufb01cient estimation of word representations in\nvector space. In International Conference on Learning Representations: Workshops Track.\n[15] Russakovsky, O. and Fei-Fei, L. (2010). Attribute learning in large-scale datasets. In European Confer-\nence of Computer Vision (ECCV), International Workshop on Parts and Attributes, Crete, Greece.\n[16] Srivastava, N. and Salakhutdinov, R. (2012). Multimodal learning with deep boltzmann machines. In\nNIPS\u20192012.\n[17] Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., and Rabi-\nnovich, A. (2014). Going deeper with convolutions. arXiv preprint arXiv:1409.4842.\n7\n",
        "sentence": "",
        "context": "of Montreal. The authors gratefully acknowledge the support from the Vision & Machine Learning,\nand Production Engineering teams at Flickr (in alphabetical order: Andrew Stadlen, Arel Cordero,\nClayton Mellina, Cyprien Noel, Frank Liu, Gerry Pesavento, Huy Nguyen, Jack Culpepper, John\nKo, Pierre Garrigues, Rob Hess, Stacey Svetlichnaya, Tobi Baumgartner, and Ye Lu).\nReferences\nConditional Generative Adversarial Nets\nMehdi Mirza\nD\u00b4epartement d\u2019informatique et de recherche op\u00b4erationnelle\nUniversit\u00b4e de Montr\u00b4eal\nMontr\u00b4eal, QC H3C 3J7\nmirzamom@iro.umontreal.ca\nSimon Osindero\nFlickr / Yahoo Inc.\nSan Francisco, CA 94103"
    },
    {
        "title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks",
        "author": [
            "A. Radford",
            "L. Metz",
            "S. Chintala"
        ],
        "venue": null,
        "citeRegEx": "Radford et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "Radford et al\\.",
        "year": 2015,
        "abstract": "In recent years, supervised learning with convolutional networks (CNNs) has\nseen huge adoption in computer vision applications. Comparatively, unsupervised\nlearning with CNNs has received less attention. In this work we hope to help\nbridge the gap between the success of CNNs for supervised learning and\nunsupervised learning. We introduce a class of CNNs called deep convolutional\ngenerative adversarial networks (DCGANs), that have certain architectural\nconstraints, and demonstrate that they are a strong candidate for unsupervised\nlearning. Training on various image datasets, we show convincing evidence that\nour deep convolutional adversarial pair learns a hierarchy of representations\nfrom object parts to scenes in both the generator and discriminator.\nAdditionally, we use the learned features for novel tasks - demonstrating their\napplicability as general image representations.",
        "full_text": "Under review as a conference paper at ICLR 2016\nUNSUPERVISED REPRESENTATION LEARNING\nWITH DEEP CONVOLUTIONAL\nGENERATIVE ADVERSARIAL NETWORKS\nAlec Radford & Luke Metz\nindico Research\nBoston, MA\n{alec,luke}@indico.io\nSoumith Chintala\nFacebook AI Research\nNew York, NY\nsoumith@fb.com\nABSTRACT\nIn recent years, supervised learning with convolutional networks (CNNs) has\nseen huge adoption in computer vision applications. Comparatively, unsupervised\nlearning with CNNs has received less attention. In this work we hope to help\nbridge the gap between the success of CNNs for supervised learning and unsuper-\nvised learning. We introduce a class of CNNs called deep convolutional generative\nadversarial networks (DCGANs), that have certain architectural constraints, and\ndemonstrate that they are a strong candidate for unsupervised learning. Training\non various image datasets, we show convincing evidence that our deep convolu-\ntional adversarial pair learns a hierarchy of representations from object parts to\nscenes in both the generator and discriminator. Additionally, we use the learned\nfeatures for novel tasks - demonstrating their applicability as general image repre-\nsentations.\n1\nINTRODUCTION\nLearning reusable feature representations from large unlabeled datasets has been an area of active\nresearch. In the context of computer vision, one can leverage the practically unlimited amount of\nunlabeled images and videos to learn good intermediate representations, which can then be used on\na variety of supervised learning tasks such as image classi\ufb01cation. We propose that one way to build\ngood image representations is by training Generative Adversarial Networks (GANs) (Goodfellow\net al., 2014), and later reusing parts of the generator and discriminator networks as feature extractors\nfor supervised tasks. GANs provide an attractive alternative to maximum likelihood techniques.\nOne can additionally argue that their learning process and the lack of a heuristic cost function (such\nas pixel-wise independent mean-square error) are attractive to representation learning. GANs have\nbeen known to be unstable to train, often resulting in generators that produce nonsensical outputs.\nThere has been very limited published research in trying to understand and visualize what GANs\nlearn, and the intermediate representations of multi-layer GANs.\nIn this paper, we make the following contributions\n\u2022 We propose and evaluate a set of constraints on the architectural topology of Convolutional\nGANs that make them stable to train in most settings. We name this class of architectures\nDeep Convolutional GANs (DCGAN)\n\u2022 We use the trained discriminators for image classi\ufb01cation tasks, showing competitive per-\nformance with other unsupervised algorithms.\n\u2022 We visualize the \ufb01lters learnt by GANs and empirically show that speci\ufb01c \ufb01lters have\nlearned to draw speci\ufb01c objects.\n1\narXiv:1511.06434v2  [cs.LG]  7 Jan 2016\nUnder review as a conference paper at ICLR 2016\n\u2022 We show that the generators have interesting vector arithmetic properties allowing for easy\nmanipulation of many semantic qualities of generated samples.\n2\nRELATED WORK\n2.1\nREPRESENTATION LEARNING FROM UNLABELED DATA\nUnsupervised representation learning is a fairly well studied problem in general computer vision\nresearch, as well as in the context of images. A classic approach to unsupervised representation\nlearning is to do clustering on the data (for example using K-means), and leverage the clusters for\nimproved classi\ufb01cation scores. In the context of images, one can do hierarchical clustering of image\npatches (Coates & Ng, 2012) to learn powerful image representations. Another popular method\nis to train auto-encoders (convolutionally, stacked (Vincent et al., 2010), separating the what and\nwhere components of the code (Zhao et al., 2015), ladder structures (Rasmus et al., 2015)) that\nencode an image into a compact code, and decode the code to reconstruct the image as accurately\nas possible. These methods have also been shown to learn good feature representations from image\npixels. Deep belief networks (Lee et al., 2009) have also been shown to work well in learning\nhierarchical representations.\n2.2\nGENERATING NATURAL IMAGES\nGenerative image models are well studied and fall into two categories: parametric and non-\nparametric.\nThe non-parametric models often do matching from a database of existing images, often matching\npatches of images, and have been used in texture synthesis (Efros et al., 1999), super-resolution\n(Freeman et al., 2002) and in-painting (Hays & Efros, 2007).\nParametric models for generating images has been explored extensively (for example on MNIST\ndigits or for texture synthesis (Portilla & Simoncelli, 2000)). However, generating natural images\nof the real world have had not much success until recently. A variational sampling approach to\ngenerating images (Kingma & Welling, 2013) has had some success, but the samples often suffer\nfrom being blurry. Another approach generates images using an iterative forward diffusion process\n(Sohl-Dickstein et al., 2015). Generative Adversarial Networks (Goodfellow et al., 2014) generated\nimages suffering from being noisy and incomprehensible. A laplacian pyramid extension to this\napproach (Denton et al., 2015) showed higher quality images, but they still suffered from the objects\nlooking wobbly because of noise introduced in chaining multiple models. A recurrent network\napproach (Gregor et al., 2015) and a deconvolution network approach (Dosovitskiy et al., 2014) have\nalso recently had some success with generating natural images. However, they have not leveraged\nthe generators for supervised tasks.\n2.3\nVISUALIZING THE INTERNALS OF CNNS\nOne constant criticism of using neural networks has been that they are black-box methods, with little\nunderstanding of what the networks do in the form of a simple human-consumable algorithm. In the\ncontext of CNNs, Zeiler et. al. (Zeiler & Fergus, 2014) showed that by using deconvolutions and\n\ufb01ltering the maximal activations, one can \ufb01nd the approximate purpose of each convolution \ufb01lter in\nthe network. Similarly, using a gradient descent on the inputs lets us inspect the ideal image that\nactivates certain subsets of \ufb01lters (Mordvintsev et al.).\n3\nAPPROACH AND MODEL ARCHITECTURE\nHistorical attempts to scale up GANs using CNNs to model images have been unsuccessful. This\nmotivated the authors of LAPGAN (Denton et al., 2015) to develop an alternative approach to it-\neratively upscale low resolution generated images which can be modeled more reliably. We also\nencountered dif\ufb01culties attempting to scale GANs using CNN architectures commonly used in the\nsupervised literature. However, after extensive model exploration we identi\ufb01ed a family of archi-\n2\nUnder review as a conference paper at ICLR 2016\ntectures that resulted in stable training across a range of datasets and allowed for training higher\nresolution and deeper generative models.\nCore to our approach is adopting and modifying three recently demonstrated changes to CNN archi-\ntectures.\nThe \ufb01rst is the all convolutional net (Springenberg et al., 2014) which replaces deterministic spatial\npooling functions (such as maxpooling) with strided convolutions, allowing the network to learn\nits own spatial downsampling. We use this approach in our generator, allowing it to learn its own\nspatial upsampling, and discriminator.\nSecond is the trend towards eliminating fully connected layers on top of convolutional features.\nThe strongest example of this is global average pooling which has been utilized in state of the\nart image classi\ufb01cation models (Mordvintsev et al.). We found global average pooling increased\nmodel stability but hurt convergence speed. A middle ground of directly connecting the highest\nconvolutional features to the input and output respectively of the generator and discriminator worked\nwell. The \ufb01rst layer of the GAN, which takes a uniform noise distribution Z as input, could be called\nfully connected as it is just a matrix multiplication, but the result is reshaped into a 4-dimensional\ntensor and used as the start of the convolution stack. For the discriminator, the last convolution layer\nis \ufb02attened and then fed into a single sigmoid output. See Fig. 1 for a visualization of an example\nmodel architecture.\nThird is Batch Normalization (Ioffe & Szegedy, 2015) which stabilizes learning by normalizing the\ninput to each unit to have zero mean and unit variance. This helps deal with training problems that\narise due to poor initialization and helps gradient \ufb02ow in deeper models. This proved critical to get\ndeep generators to begin learning, preventing the generator from collapsing all samples to a single\npoint which is a common failure mode observed in GANs. Directly applying batchnorm to all layers\nhowever, resulted in sample oscillation and model instability. This was avoided by not applying\nbatchnorm to the generator output layer and the discriminator input layer.\nThe ReLU activation (Nair & Hinton, 2010) is used in the generator with the exception of the output\nlayer which uses the Tanh function. We observed that using a bounded activation allowed the model\nto learn more quickly to saturate and cover the color space of the training distribution. Within the\ndiscriminator we found the leaky recti\ufb01ed activation (Maas et al., 2013) (Xu et al., 2015) to work\nwell, especially for higher resolution modeling. This is in contrast to the original GAN paper, which\nused the maxout activation (Goodfellow et al., 2013).\nArchitecture guidelines for stable Deep Convolutional GANs\n\u2022 Replace any pooling layers with strided convolutions (discriminator) and fractional-strided\nconvolutions (generator).\n\u2022 Use batchnorm in both the generator and the discriminator.\n\u2022 Remove fully connected hidden layers for deeper architectures.\n\u2022 Use ReLU activation in generator for all layers except for the output, which uses Tanh.\n\u2022 Use LeakyReLU activation in the discriminator for all layers.\n4\nDETAILS OF ADVERSARIAL TRAINING\nWe trained DCGANs on three datasets, Large-scale Scene Understanding (LSUN) (Yu et al., 2015),\nImagenet-1k and a newly assembled Faces dataset. Details on the usage of each of these datasets\nare given below.\nNo pre-processing was applied to training images besides scaling to the range of the tanh activation\nfunction [-1, 1]. All models were trained with mini-batch stochastic gradient descent (SGD) with\na mini-batch size of 128. All weights were initialized from a zero-centered Normal distribution\nwith standard deviation 0.02. In the LeakyReLU, the slope of the leak was set to 0.2 in all models.\nWhile previous GAN work has used momentum to accelerate training, we used the Adam optimizer\n(Kingma & Ba, 2014) with tuned hyperparameters. We found the suggested learning rate of 0.001,\nto be too high, using 0.0002 instead. Additionally, we found leaving the momentum term \u03b21 at the\n3\nUnder review as a conference paper at ICLR 2016\nFigure 1: DCGAN generator used for LSUN scene modeling. A 100 dimensional uniform distribu-\ntion Z is projected to a small spatial extent convolutional representation with many feature maps.\nA series of four fractionally-strided convolutions (in some recent papers, these are wrongly called\ndeconvolutions) then convert this high level representation into a 64 \u00d7 64 pixel image. Notably, no\nfully connected or pooling layers are used.\nsuggested value of 0.9 resulted in training oscillation and instability while reducing it to 0.5 helped\nstabilize training.\n4.1\nLSUN\nAs visual quality of samples from generative image models has improved, concerns of over-\ufb01tting\nand memorization of training samples have risen. To demonstrate how our model scales with more\ndata and higher resolution generation, we train a model on the LSUN bedrooms dataset containing\na little over 3 million training examples. Recent analysis has shown that there is a direct link be-\ntween how fast models learn and their generalization performance (Hardt et al., 2015). We show\nsamples from one epoch of training (Fig.2), mimicking online learning, in addition to samples after\nconvergence (Fig.3), as an opportunity to demonstrate that our model is not producing high quality\nsamples via simply over\ufb01tting/memorizing training examples. No data augmentation was applied to\nthe images.\n4.1.1\nDEDUPLICATION\nTo further decrease the likelihood of the generator memorizing input examples (Fig.2) we perform a\nsimple image de-duplication process. We \ufb01t a 3072-128-3072 de-noising dropout regularized RELU\nautoencoder on 32x32 downsampled center-crops of training examples. The resulting code layer\nactivations are then binarized via thresholding the ReLU activation which has been shown to be an\neffective information preserving technique (Srivastava et al., 2014) and provides a convenient form\nof semantic-hashing, allowing for linear time de-duplication . Visual inspection of hash collisions\nshowed high precision with an estimated false positive rate of less than 1 in 100. Additionally, the\ntechnique detected and removed approximately 275,000 near duplicates, suggesting a high recall.\n4.2\nFACES\nWe scraped images containing human faces from random web image queries of peoples names. The\npeople names were acquired from dbpedia, with a criterion that they were born in the modern era.\nThis dataset has 3M images from 10K people. We run an OpenCV face detector on these images,\nkeeping the detections that are suf\ufb01ciently high resolution, which gives us approximately 350,000\nface boxes. We use these face boxes for training. No data augmentation was applied to the images.\n4\nUnder review as a conference paper at ICLR 2016\nFigure 2: Generated bedrooms after one training pass through the dataset. Theoretically, the model\ncould learn to memorize training examples, but this is experimentally unlikely as we train with a\nsmall learning rate and minibatch SGD. We are aware of no prior empirical evidence demonstrating\nmemorization with SGD and a small learning rate.\nFigure 3: Generated bedrooms after \ufb01ve epochs of training. There appears to be evidence of visual\nunder-\ufb01tting via repeated noise textures across multiple samples such as the base boards of some of\nthe beds.\n4.3\nIMAGENET-1K\nWe use Imagenet-1k (Deng et al., 2009) as a source of natural images for unsupervised training. We\ntrain on 32 \u00d7 32 min-resized center crops. No data augmentation was applied to the images.\n5\nUnder review as a conference paper at ICLR 2016\n5\nEMPIRICAL VALIDATION OF DCGANS CAPABILITIES\n5.1\nCLASSIFYING CIFAR-10 USING GANS AS A FEATURE EXTRACTOR\nOne common technique for evaluating the quality of unsupervised representation learning algo-\nrithms is to apply them as a feature extractor on supervised datasets and evaluate the performance\nof linear models \ufb01tted on top of these features.\nOn the CIFAR-10 dataset, a very strong baseline performance has been demonstrated from a well\ntuned single layer feature extraction pipeline utilizing K-means as a feature learning algorithm.\nWhen using a very large amount of feature maps (4800) this technique achieves 80.6% accuracy.\nAn unsupervised multi-layered extension of the base algorithm reaches 82.0% accuracy (Coates &\nNg, 2011). To evaluate the quality of the representations learned by DCGANs for supervised tasks,\nwe train on Imagenet-1k and then use the discriminator\u2019s convolutional features from all layers,\nmaxpooling each layers representation to produce a 4 \u00d7 4 spatial grid. These features are then\n\ufb02attened and concatenated to form a 28672 dimensional vector and a regularized linear L2-SVM\nclassi\ufb01er is trained on top of them. This achieves 82.8% accuracy, out performing all K-means\nbased approaches. Notably, the discriminator has many less feature maps (512 in the highest layer)\ncompared to K-means based techniques, but does result in a larger total feature vector size due to\nthe many layers of 4 \u00d7 4 spatial locations. The performance of DCGANs is still less than that of\nExemplar CNNs (Dosovitskiy et al., 2015), a technique which trains normal discriminative CNNs\nin an unsupervised fashion to differentiate between speci\ufb01cally chosen, aggressively augmented,\nexemplar samples from the source dataset. Further improvements could be made by \ufb01netuning the\ndiscriminator\u2019s representations, but we leave this for future work. Additionally, since our DCGAN\nwas never trained on CIFAR-10 this experiment also demonstrates the domain robustness of the\nlearned features.\nTable 1: CIFAR-10 classi\ufb01cation results using our pre-trained model. Our DCGAN is not pre-\ntrained on CIFAR-10, but on Imagenet-1k, and the features are used to classify CIFAR-10 images.\nModel\nAccuracy\nAccuracy (400 per class)\nmax # of features units\n1 Layer K-means\n80.6%\n63.7% (\u00b10.7%)\n4800\n3 Layer K-means Learned RF\n82.0%\n70.7% (\u00b10.7%)\n3200\nView Invariant K-means\n81.9%\n72.6% (\u00b10.7%)\n6400\nExemplar CNN\n84.3%\n77.4% (\u00b10.2%)\n1024\nDCGAN (ours) + L2-SVM\n82.8%\n73.8% (\u00b10.4%)\n512\n5.2\nCLASSIFYING SVHN DIGITS USING GANS AS A FEATURE EXTRACTOR\nOn the StreetView House Numbers dataset (SVHN)(Netzer et al., 2011), we use the features of\nthe discriminator of a DCGAN for supervised purposes when labeled data is scarce. Following\nsimilar dataset preparation rules as in the CIFAR-10 experiments, we split off a validation set of\n10,000 examples from the non-extra set and use it for all hyperparameter and model selection. 1000\nuniformly class distributed training examples are randomly selected and used to train a regularized\nlinear L2-SVM classi\ufb01er on top of the same feature extraction pipeline used for CIFAR-10. This\nachieves state of the art (for classi\ufb01cation using 1000 labels) at 22.48% test error, improving upon\nanother modifcation of CNNs designed to leverage unlabled data (Zhao et al., 2015). Additionally,\nwe validate that the CNN architecture used in DCGAN is not the key contributing factor of the\nmodel\u2019s performance by training a purely supervised CNN with the same architecture on the same\ndata and optimizing this model via random search over 64 hyperparameter trials (Bergstra & Bengio,\n2012). It achieves a sign\ufb01cantly higher 28.87% validation error.\n6\nINVESTIGATING AND VISUALIZING THE INTERNALS OF THE NETWORKS\nWe investigate the trained generators and discriminators in a variety of ways. We do not do any\nkind of nearest neighbor search on the training set. Nearest neighbors in pixel or feature space are\n6\nUnder review as a conference paper at ICLR 2016\nTable 2: SVHN classi\ufb01cation with 1000 labels\nModel\nerror rate\nKNN\n77.93%\nTSVM\n66.55%\nM1+KNN\n65.63%\nM1+TSVM\n54.33%\nM1+M2\n36.02%\nSWWAE without dropout\n27.83%\nSWWAE with dropout\n23.56%\nDCGAN (ours) + L2-SVM\n22.48%\nSupervised CNN with the same architecture\n28.87% (validation)\ntrivially fooled (Theis et al., 2015) by small image transforms. We also do not use log-likelihood\nmetrics to quantitatively assess the model, as it is a poor (Theis et al., 2015) metric.\n6.1\nWALKING IN THE LATENT SPACE\nThe \ufb01rst experiment we did was to understand the landscape of the latent space. Walking on the\nmanifold that is learnt can usually tell us about signs of memorization (if there are sharp transitions)\nand about the way in which the space is hierarchically collapsed. If walking in this latent space\nresults in semantic changes to the image generations (such as objects being added and removed), we\ncan reason that the model has learned relevant and interesting representations. The results are shown\nin Fig.4.\n6.2\nVISUALIZING THE DISCRIMINATOR FEATURES\nPrevious work has demonstrated that supervised training of CNNs on large image datasets results in\nvery powerful learned features (Zeiler & Fergus, 2014). Additionally, supervised CNNs trained on\nscene classi\ufb01cation learn object detectors (Oquab et al., 2014). We demonstrate that an unsupervised\nDCGAN trained on a large image dataset can also learn a hierarchy of features that are interesting.\nUsing guided backpropagation as proposed by (Springenberg et al., 2014), we show in Fig.5 that the\nfeatures learnt by the discriminator activate on typical parts of a bedroom, like beds and windows.\nFor comparison, in the same \ufb01gure, we give a baseline for randomly initialized features that are not\nactivated on anything that is semantically relevant or interesting.\n6.3\nMANIPULATING THE GENERATOR REPRESENTATION\n6.3.1\nFORGETTING TO DRAW CERTAIN OBJECTS\nIn addition to the representations learnt by a discriminator, there is the question of what representa-\ntions the generator learns. The quality of samples suggest that the generator learns speci\ufb01c object\nrepresentations for major scene components such as beds, windows, lamps, doors, and miscellaneous\nfurniture. In order to explore the form that these representations take, we conducted an experiment\nto attempt to remove windows from the generator completely.\nOn 150 samples, 52 window bounding boxes were drawn manually. On the second highest con-\nvolution layer features, logistic regression was \ufb01t to predict whether a feature activation was on a\nwindow (or not), by using the criterion that activations inside the drawn bounding boxes are posi-\ntives and random samples from the same images are negatives. Using this simple model, all feature\nmaps with weights greater than zero ( 200 in total) were dropped from all spatial locations. Then,\nrandom new samples were generated with and without the feature map removal.\nThe generated images with and without the window dropout are shown in Fig.6, and interestingly,\nthe network mostly forgets to draw windows in the bedrooms, replacing them with other objects.\n7\nUnder review as a conference paper at ICLR 2016\nFigure 4: Top rows: Interpolation between a series of 9 random points in Z show that the space\nlearned has smooth transitions, with every image in the space plausibly looking like a bedroom. In\nthe 6th row, you see a room without a window slowly transforming into a room with a giant window.\nIn the 10th row, you see what appears to be a TV slowly being transformed into a window.\n6.3.2\nVECTOR ARITHMETIC ON FACE SAMPLES\nIn the context of evaluating learned representations of words (Mikolov et al., 2013) demonstrated\nthat simple arithmetic operations revealed rich linear structure in representation space. One canoni-\ncal example demonstrated that the vector(\u201dKing\u201d) - vector(\u201dMan\u201d) + vector(\u201dWoman\u201d) resulted in a\nvector whose nearest neighbor was the vector for Queen. We investigated whether similar structure\nemerges in the Z representation of our generators. We performed similar arithmetic on the Z vectors\nof sets of exemplar samples for visual concepts. Experiments working on only single samples per\nconcept were unstable, but averaging the Z vector for three examplars showed consistent and stable\ngenerations that semantically obeyed the arithmetic. In addition to the object manipulation shown\nin (Fig. 7), we demonstrate that face pose is also modeled linearly in Z space (Fig. 8).\nThese demonstrations suggest interesting applications can be developed using Z representations\nlearned by our models. It has been previously demonstrated that conditional generative models can\nlearn to convincingly model object attributes like scale, rotation, and position (Dosovitskiy et al.,\n2014). This is to our knowledge the \ufb01rst demonstration of this occurring in purely unsupervised\n8\nUnder review as a conference paper at ICLR 2016\nFigure 5: On the right, guided backpropagation visualizations of maximal axis-aligned responses\nfor the \ufb01rst 6 learned convolutional features from the last convolution layer in the discriminator.\nNotice a signi\ufb01cant minority of features respond to beds - the central object in the LSUN bedrooms\ndataset. On the left is a random \ufb01lter baseline. Comparing to the previous responses there is little to\nno discrimination and random structure.\nFigure 6: Top row: un-modi\ufb01ed samples from model. Bottom row: the same samples generated\nwith dropping out \u201dwindow\u201d \ufb01lters. Some windows are removed, others are transformed into objects\nwith similar visual appearance such as doors and mirrors. Although visual quality decreased, overall\nscene composition stayed similar, suggesting the generator has done a good job disentangling scene\nrepresentation from object representation. Extended experiments could be done to remove other\nobjects from the image and modify the objects the generator draws.\nmodels. Further exploring and developing the above mentioned vector arithmetic could dramat-\nically reduce the amount of data needed for conditional generative modeling of complex image\ndistributions.\n7\nCONCLUSION AND FUTURE WORK\nWe propose a more stable set of architectures for training generative adversarial networks and we\ngive evidence that adversarial networks learn good representations of images for supervised learning\nand generative modeling. There are still some forms of model instability remaining - we noticed as\nmodels are trained longer they sometimes collapse a subset of \ufb01lters to a single oscillating mode.\n9\nUnder review as a conference paper at ICLR 2016\nFigure 7: Vector arithmetic for visual concepts. For each column, the Z vectors of samples are\naveraged. Arithmetic was then performed on the mean vectors creating a new vector Y . The center\nsample on the right hand side is produce by feeding Y as input to the generator. To demonstrate\nthe interpolation capabilities of the generator, uniform noise sampled with scale +-0.25 was added\nto Y to produce the 8 other samples. Applying arithmetic in the input space (bottom two examples)\nresults in noisy overlap due to misalignment.\nFurther work is needed to tackle this from of instability. We think that extending this framework\n10\nUnder review as a conference paper at ICLR 2016\nFigure 8: A \u201dturn\u201d vector was created from four averaged samples of faces looking left vs looking\nright. By adding interpolations along this axis to random samples we were able to reliably transform\ntheir pose.\nto other domains such as video (for frame prediction) and audio (pre-trained features for speech\nsynthesis) should be very interesting. Further investigations into the properties of the learnt latent\nspace would be interesting as well.\nACKNOWLEDGMENTS\nWe are fortunate and thankful for all the advice and guidance we have received during this work,\nespecially that of Ian Goodfellow, Tobias Springenberg, Arthur Szlam and Durk Kingma. Addition-\nally we\u2019d like to thank all of the folks at indico for providing support, resources, and conversations,\nespecially the two other members of the indico research team, Dan Kuster and Nathan Lintz. Finally,\nwe\u2019d like to thank Nvidia for donating a Titan-X GPU used in this work.\nREFERENCES\nBergstra, James and Bengio, Yoshua. Random search for hyper-parameter optimization. JMLR,\n2012.\nCoates, Adam and Ng, Andrew. Selecting receptive \ufb01elds in deep networks. NIPS, 2011.\nCoates, Adam and Ng, Andrew Y. Learning feature representations with k-means. In Neural Net-\nworks: Tricks of the Trade, pp. 561\u2013580. Springer, 2012.\nDeng, Jia, Dong, Wei, Socher, Richard, Li, Li-Jia, Li, Kai, and Fei-Fei, Li. Imagenet: A large-scale\nhierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009.\nIEEE Conference on, pp. 248\u2013255. IEEE, 2009.\nDenton, Emily, Chintala, Soumith, Szlam, Arthur, and Fergus, Rob. Deep generative image models\nusing a laplacian pyramid of adversarial networks. arXiv preprint arXiv:1506.05751, 2015.\nDosovitskiy, Alexey, Springenberg, Jost Tobias, and Brox, Thomas. Learning to generate chairs\nwith convolutional neural networks. arXiv preprint arXiv:1411.5928, 2014.\n11\nUnder review as a conference paper at ICLR 2016\nDosovitskiy, Alexey, Fischer, Philipp, Springenberg, Jost Tobias, Riedmiller, Martin, and Brox,\nThomas. Discriminative unsupervised feature learning with exemplar convolutional neural net-\nworks. In Pattern Analysis and Machine Intelligence, IEEE Transactions on, volume 99. IEEE,\n2015.\nEfros, Alexei, Leung, Thomas K, et al. Texture synthesis by non-parametric sampling. In Computer\nVision, 1999. The Proceedings of the Seventh IEEE International Conference on, volume 2, pp.\n1033\u20131038. IEEE, 1999.\nFreeman, William T, Jones, Thouis R, and Pasztor, Egon C. Example-based super-resolution. Com-\nputer Graphics and Applications, IEEE, 22(2):56\u201365, 2002.\nGoodfellow, Ian J, Warde-Farley, David, Mirza, Mehdi, Courville, Aaron, and Bengio, Yoshua.\nMaxout networks. arXiv preprint arXiv:1302.4389, 2013.\nGoodfellow, Ian J., Pouget-Abadie, Jean, Mirza, Mehdi, Xu, Bing, Warde-Farley, David, Ozair,\nSherjil, Courville, Aaron C., and Bengio, Yoshua. Generative adversarial nets. NIPS, 2014.\nGregor, Karol, Danihelka, Ivo, Graves, Alex, and Wierstra, Daan. Draw: A recurrent neural network\nfor image generation. arXiv preprint arXiv:1502.04623, 2015.\nHardt, Moritz, Recht, Benjamin, and Singer, Yoram. Train faster, generalize better: Stability of\nstochastic gradient descent. arXiv preprint arXiv:1509.01240, 2015.\nHauberg, Sren, Freifeld, Oren, Larsen, Anders Boesen Lindbo, Fisher III, John W., and Hansen,\nLars Kair. Dreaming more data: Class-dependent distributions over diffeomorphisms for learned\ndata augmentation. arXiv preprint arXiv:1510.02795, 2015.\nHays, James and Efros, Alexei A. Scene completion using millions of photographs. ACM Transac-\ntions on Graphics (TOG), 26(3):4, 2007.\nIoffe, Sergey and Szegedy, Christian. Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.\nKingma, Diederik P and Ba, Jimmy Lei. Adam: A method for stochastic optimization. arXiv\npreprint arXiv:1412.6980, 2014.\nKingma, Diederik P and Welling, Max.\nAuto-encoding variational bayes.\narXiv preprint\narXiv:1312.6114, 2013.\nLee, Honglak, Grosse, Roger, Ranganath, Rajesh, and Ng, Andrew Y. Convolutional deep belief\nnetworks for scalable unsupervised learning of hierarchical representations. In Proceedings of the\n26th Annual International Conference on Machine Learning, pp. 609\u2013616. ACM, 2009.\nLoosli, Ga\u00a8elle, Canu, St\u00b4ephane, and Bottou, L\u00b4eon. Training invariant support vector machines using\nselective sampling. In Bottou, L\u00b4eon, Chapelle, Olivier, DeCoste, Dennis, and Weston, Jason\n(eds.), Large Scale Kernel Machines, pp. 301\u2013320. MIT Press, Cambridge, MA., 2007. URL\nhttp://leon.bottou.org/papers/loosli-canu-bottou-2006.\nMaas, Andrew L, Hannun, Awni Y, and Ng, Andrew Y. Recti\ufb01er nonlinearities improve neural\nnetwork acoustic models. In Proc. ICML, volume 30, 2013.\nMikolov, Tomas, Sutskever, Ilya, Chen, Kai, Corrado, Greg S, and Dean, Jeff. Distributed repre-\nsentations of words and phrases and their compositionality. In Advances in neural information\nprocessing systems, pp. 3111\u20133119, 2013.\nMordvintsev,\nAlexander,\nOlah,\nChristopher,\nand Tyka,\nMike.\nInceptionism :\nGoing\ndeeper into neural networks.\nhttp://googleresearch.blogspot.com/2015/06/\ninceptionism-going-deeper-into-neural.html. Accessed: 2015-06-17.\nNair, Vinod and Hinton, Geoffrey E. Recti\ufb01ed linear units improve restricted boltzmann machines.\nIn Proceedings of the 27th International Conference on Machine Learning (ICML-10), pp. 807\u2013\n814, 2010.\n12\nUnder review as a conference paper at ICLR 2016\nNetzer, Yuval, Wang, Tao, Coates, Adam, Bissacco, Alessandro, Wu, Bo, and Ng, Andrew Y. Read-\ning digits in natural images with unsupervised feature learning. In NIPS workshop on deep learn-\ning and unsupervised feature learning, volume 2011, pp. 5. Granada, Spain, 2011.\nOquab, M., Bottou, L., Laptev, I., and Sivic, J. Learning and transferring mid-level image represen-\ntations using convolutional neural networks. In CVPR, 2014.\nPortilla, Javier and Simoncelli, Eero P.\nA parametric texture model based on joint statistics of\ncomplex wavelet coef\ufb01cients. International Journal of Computer Vision, 40(1):49\u201370, 2000.\nRasmus, Antti, Valpola, Harri, Honkala, Mikko, Berglund, Mathias, and Raiko, Tapani.\nSemi-\nsupervised learning with ladder network. arXiv preprint arXiv:1507.02672, 2015.\nSohl-Dickstein, Jascha, Weiss, Eric A, Maheswaranathan, Niru, and Ganguli, Surya. Deep unsuper-\nvised learning using nonequilibrium thermodynamics. arXiv preprint arXiv:1503.03585, 2015.\nSpringenberg, Jost Tobias, Dosovitskiy, Alexey, Brox, Thomas, and Riedmiller, Martin. Striving for\nsimplicity: The all convolutional net. arXiv preprint arXiv:1412.6806, 2014.\nSrivastava, Rupesh Kumar, Masci, Jonathan, Gomez, Faustino, and Schmidhuber, J\u00a8urgen. Under-\nstanding locally competitive networks. arXiv preprint arXiv:1410.1165, 2014.\nTheis, L., van den Oord, A., and Bethge, M.\nA note on the evaluation of generative models.\narXiv:1511.01844, Nov 2015. URL http://arxiv.org/abs/1511.01844.\nVincent, Pascal, Larochelle, Hugo, Lajoie, Isabelle, Bengio, Yoshua, and Manzagol, Pierre-Antoine.\nStacked denoising autoencoders: Learning useful representations in a deep network with a local\ndenoising criterion. The Journal of Machine Learning Research, 11:3371\u20133408, 2010.\nXu, Bing, Wang, Naiyan, Chen, Tianqi, and Li, Mu. Empirical evaluation of recti\ufb01ed activations in\nconvolutional network. arXiv preprint arXiv:1505.00853, 2015.\nYu, Fisher, Zhang, Yinda, Song, Shuran, Seff, Ari, and Xiao, Jianxiong. Construction of a large-scale\nimage dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365,\n2015.\nZeiler, Matthew D and Fergus, Rob. Visualizing and understanding convolutional networks. In\nComputer Vision\u2013ECCV 2014, pp. 818\u2013833. Springer, 2014.\nZhao, Junbo, Mathieu, Michael, Goroshin, Ross, and Lecun, Yann.\nStacked what-where auto-\nencoders. arXiv preprint arXiv:1506.02351, 2015.\n13\nUnder review as a conference paper at ICLR 2016\n8\nSUPPLEMENTARY MATERIAL\n8.1\nEVALUATING DCGANS CAPABILITY TO CAPTURE DATA DISTRIBUTIONS\nWe propose to apply standard classi\ufb01cation metrics to a conditional version of our model, evaluating\nthe conditional distributions learned. We trained a DCGAN on MNIST (splitting off a 10K validation\nset) as well as a permutation invariant GAN baseline and evaluated the models using a nearest\nneighbor classi\ufb01er comparing real data to a set of generated conditional samples. We found that\nremoving the scale and bias parameters from batchnorm produced better results for both models. We\nspeculate that the noise introduced by batchnorm helps the generative models to better explore and\ngenerate from the underlying data distribution. The results are shown in Table 3 which compares\nour models with other techniques. The DCGAN model achieves the same test error as a nearest\nneighbor classi\ufb01er \ufb01tted on the training dataset - suggesting the DCGAN model has done a superb\njob at modeling the conditional distributions of this dataset. At one million samples per class, the\nDCGAN model outperforms In\ufb01MNIST (Loosli et al., 2007), a hand developed data augmentation\npipeline which uses translations and elastic deformations of training examples. The DCGAN is\ncompetitive with a probabilistic generative data augmentation technique utilizing learned per class\ntransformations (Hauberg et al., 2015) while being more general as it directly models the data instead\nof transformations of the data.\nTable 3: Nearest neighbor classi\ufb01cation results.\nModel\nTest Error @50K samples\nTest Error @10M samples\nAlignMNIST\n-\n1.4%\nIn\ufb01MNIST\n-\n2.6%\nReal Data\n3.1%\n-\nGAN\n6.28%\n5.65%\nDCGAN (ours)\n2.98%\n1.48%\nFigure 9:\nSide-by-side illustration of (from left-to-right) the MNIST dataset, generations from a\nbaseline GAN, and generations from our DCGAN .\n14\nUnder review as a conference paper at ICLR 2016\nFigure 10: More face generations from our Face DCGAN.\n15\nUnder review as a conference paper at ICLR 2016\nFigure 11: Generations of a DCGAN that was trained on the Imagenet-1k dataset.\n16\n",
        "sentence": "",
        "context": "ACKNOWLEDGMENTS\nWe are fortunate and thankful for all the advice and guidance we have received during this work,\nespecially that of Ian Goodfellow, Tobias Springenberg, Arthur Szlam and Durk Kingma. Addition-\n4.2\nFACES\nWe scraped images containing human faces from random web image queries of peoples names. The\npeople names were acquired from dbpedia, with a criterion that they were born in the modern era.\ning digits in natural images with unsupervised feature learning. In NIPS workshop on deep learn-\ning and unsupervised feature learning, volume 2011, pp. 5. Granada, Spain, 2011."
    },
    {
        "title": "Learning to generate images with perceptual similarity",
        "author": [
            "K. Ridgeway",
            "J. Snell",
            "B. Roads",
            "R.S. Zemel",
            "M.C. Mozer"
        ],
        "venue": "metrics. arxiv:1511.06409,",
        "citeRegEx": "Ridgeway et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "Ridgeway et al\\.",
        "year": 2015,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Deep boltzmann machines",
        "author": [
            "R. Salakhutdinov",
            "G.E. Hinton"
        ],
        "venue": "In AISTATS,",
        "citeRegEx": "Salakhutdinov and Hinton.,? \\Q2009\\E",
        "shortCiteRegEx": "Salakhutdinov and Hinton.",
        "year": 2009,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Deep inside convolutional networks: Visualising image classification models and saliency maps",
        "author": [
            "K. Simonyan",
            "A. Vedaldi",
            "A. Zisserman"
        ],
        "venue": "In ICLR workshop track,",
        "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Simonyan et al\\.",
        "year": 2014,
        "abstract": "This paper addresses the visualisation of image classification models, learnt\nusing deep Convolutional Networks (ConvNets). We consider two visualisation\ntechniques, based on computing the gradient of the class score with respect to\nthe input image. The first one generates an image, which maximises the class\nscore [Erhan et al., 2009], thus visualising the notion of the class, captured\nby a ConvNet. The second technique computes a class saliency map, specific to a\ngiven image and class. We show that such maps can be employed for weakly\nsupervised object segmentation using classification ConvNets. Finally, we\nestablish the connection between the gradient-based ConvNet visualisation\nmethods and deconvolutional networks [Zeiler et al., 2013].",
        "full_text": "Deep Inside Convolutional Networks: Visualising\nImage Classi\ufb01cation Models and Saliency Maps\nKaren Simonyan\nAndrea Vedaldi\nAndrew Zisserman\nVisual Geometry Group, University of Oxford\n{karen,vedaldi,az}@robots.ox.ac.uk\nAbstract\nThis paper addresses the visualisation of image classi\ufb01cation models, learnt us-\ning deep Convolutional Networks (ConvNets).\nWe consider two visualisation\ntechniques, based on computing the gradient of the class score with respect to\nthe input image. The \ufb01rst one generates an image, which maximises the class\nscore [5], thus visualising the notion of the class, captured by a ConvNet. The\nsecond technique computes a class saliency map, speci\ufb01c to a given image and\nclass. We show that such maps can be employed for weakly supervised object\nsegmentation using classi\ufb01cation ConvNets. Finally, we establish the connection\nbetween the gradient-based ConvNet visualisation methods and deconvolutional\nnetworks [13].\n1\nIntroduction\nWith the deep Convolutional Networks (ConvNets) [10] now being the architecture of choice for\nlarge-scale image recognition [4, 8], the problem of understanding the aspects of visual appearance,\ncaptured inside a deep model, has become particularly relevant and is the subject of this paper.\nIn previous work, Erhan et al. [5] visualised deep models by \ufb01nding an input image which max-\nimises the neuron activity of interest by carrying out an optimisation using gradient ascent in the\nimage space. The method was used to visualise the hidden feature layers of unsupervised deep ar-\nchitectures, such as the Deep Belief Network (DBN) [7], and it was later employed by Le et al.[9]\nto visualise the class models, captured by a deep unsupervised auto-encoder. Recently, the problem\nof ConvNet visualisation was addressed by Zeiler et al.[13]. For convolutional layer visualisation,\nthey proposed the Deconvolutional Network (DeconvNet) architecture, which aims to approximately\nreconstruct the input of each layer from its output.\nIn this paper, we address the visualisation of deep image classi\ufb01cation ConvNets, trained on the\nlarge-scale ImageNet challenge dataset [2]. To this end, we make the following three contributions.\nFirst, we demonstrate that understandable visualisations of ConvNet classi\ufb01cation models can be ob-\ntained using the numerical optimisation of the input image [5] (Sect. 2). Note, in our case, unlike [5],\nthe net is trained in a supervised manner, so we know which neuron in the \ufb01nal fully-connected clas-\nsi\ufb01cation layer should be maximised to visualise the class of interest (in the unsupervised case, [9]\nhad to use a separate annotated image set to \ufb01nd out the neuron responsible for a particular class). To\nthe best of our knowledge, we are the \ufb01rst to apply the method of [5] to the visualisation of ImageNet\nclassi\ufb01cation ConvNets [8]. Second, we propose a method for computing the spatial support of a\ngiven class in a given image (image-speci\ufb01c class saliency map) using a single back-propagation\npass through a classi\ufb01cation ConvNet (Sect. 3). As discussed in Sect. 3.2, such saliency maps can\nbe used for weakly supervised object localisation. Finally, we show in Sect. 4 that the gradient-based\nvisualisation methods generalise the deconvolutional network reconstruction procedure [13].\nConvNet implementation details.\nOur visualisation experiments were carried out using a single\ndeep ConvNet, trained on the ILSVRC-2013 dataset [2], which includes 1.2M training images,\nlabelled into 1000 classes. Our ConvNet is similar to that of [8] and is implemented using their\n1\narXiv:1312.6034v2  [cs.CV]  19 Apr 2014\ncuda-convnet toolbox1, although our net is less wide, and we used additional image jittering,\nbased on zeroing-out random parts of an image. Our weight layer con\ufb01guration is: conv64-conv256-\nconv256-conv256-conv256-full4096-full4096-full1000, where convN denotes a convolutional layer\nwith N \ufb01lters, fullM \u2013 a fully-connected layer with M outputs. On ILSVRC-2013 validation set, the\nnetwork achieves the top-1/top-5 classi\ufb01cation error of 39.7%/17.7%, which is slightly better than\n40.7%/18.2%, reported in [8] for a single ConvNet.\n2\nClass Model Visualisation\nIn this section we describe a technique for visualising the class models, learnt by the image clas-\nsi\ufb01cation ConvNets. Given a learnt classi\ufb01cation ConvNet and a class of interest, the visualisation\nmethod consists in numerically generating an image [5], which is representative of the class in terms\nof the ConvNet class scoring model.\nMore formally, let Sc(I) be the score of the class c, computed by the classi\ufb01cation layer of the\nConvNet for an image I. We would like to \ufb01nd an L2-regularised image, such that the score Sc is\nhigh:\narg max\nI\nSc(I) \u2212\u03bb\u2225I\u22252\n2,\n(1)\nwhere \u03bb is the regularisation parameter. A locally-optimal I can be found by the back-propagation\nmethod. The procedure is related to the ConvNet training procedure, where the back-propagation is\nused to optimise the layer weights. The difference is that in our case the optimisation is performed\nwith respect to the input image, while the weights are \ufb01xed to those found during the training stage.\nWe initialised the optimisation with the zero image (in our case, the ConvNet was trained on the\nzero-centred image data), and then added the training set mean image to the result. The class model\nvisualisations for several classes are shown in Fig. 1.\nIt should be noted that we used the (unnormalised) class scores Sc, rather than the class posteriors,\nreturned by the soft-max layer: Pc =\nexp Sc\nP\nc exp Sc . The reason is that the maximisation of the class\nposterior can be achieved by minimising the scores of other classes. Therefore, we optimise Sc to\nensure that the optimisation concentrates only on the class in question c. We also experimented\nwith optimising the posterior Pc, but the results were not visually prominent, thus con\ufb01rming our\nintuition.\n3\nImage-Speci\ufb01c Class Saliency Visualisation\nIn this section we describe how a classi\ufb01cation ConvNet can be queried about the spatial support of\na particular class in a given image. Given an image I0, a class c, and a classi\ufb01cation ConvNet with\nthe class score function Sc(I), we would like to rank the pixels of I0 based on their in\ufb02uence on the\nscore Sc(I0).\nWe start with a motivational example. Consider the linear score model for the class c:\nSc(I) = wT\nc I + bc,\n(2)\nwhere the image I is represented in the vectorised (one-dimensional) form, and wc and bc are respec-\ntively the weight vector and the bias of the model. In this case, it is easy to see that the magnitude\nof elements of w de\ufb01nes the importance of the corresponding pixels of I for the class c.\nIn the case of deep ConvNets, the class score Sc(I) is a highly non-linear function of I, so the\nreasoning of the previous paragraph can not be immediately applied. However, given an image\nI0, we can approximate Sc(I) with a linear function in the neighbourhood of I0 by computing the\n\ufb01rst-order Taylor expansion:\nSc(I) \u2248wT I + b,\n(3)\nwhere w is the derivative of Sc with respect to the image I at the point (image) I0:\nw = \u2202Sc\n\u2202I\n\f\f\f\f\nI0\n.\n(4)\nAnother interpretation of computing the image-speci\ufb01c class saliency using the class score deriva-\ntive (4) is that the magnitude of the derivative indicates which pixels need to be changed the least\n1http://code.google.com/p/cuda-convnet/\n2\ndumbbell \ncup \ndalmatian \nbell pepper \nlemon \nhusky \nwashing machine \ncomputer keyboard \nkit fox \ngoose \nlimousine \nostrich \nFigure 1: Numerically computed images, illustrating the class appearance models, learnt by a\nConvNet, trained on ILSVRC-2013. Note how different aspects of class appearance are captured\nin a single image. Better viewed in colour.\n3\nto affect the class score the most. One can expect that such pixels correspond to the object location\nin the image. We note that a similar technique has been previously applied by [1] in the context of\nBayesian classi\ufb01cation.\n3.1\nClass Saliency Extraction\nGiven an image I0 (with m rows and n columns) and a class c, the class saliency map M \u2208Rm\u00d7n\nis computed as follows. First, the derivative w (4) is found by back-propagation. After that, the\nsaliency map is obtained by rearranging the elements of the vector w. In the case of a grey-scale\nimage, the number of elements in w is equal to the number of pixels in I0, so the map can be\ncomputed as Mij = |wh(i,j)|, where h(i, j) is the index of the element of w, corresponding to the\nimage pixel in the i-th row and j-th column. In the case of the multi-channel (e.g. RGB) image, let\nus assume that the colour channel c of the pixel (i, j) of image I corresponds to the element of w\nwith the index h(i, j, c). To derive a single class saliency value for each pixel (i, j), we took the\nmaximum magnitude of w across all colour channels: Mij = maxc |wh(i,j,c)|.\nIt is important to note that the saliency maps are extracted using a classi\ufb01cation ConvNet trained\non the image labels, so no additional annotation is required (such as object bounding boxes or\nsegmentation masks). The computation of the image-speci\ufb01c saliency map for a single class is\nextremely quick, since it only requires a single back-propagation pass.\nWe visualise the saliency maps for the highest-scoring class (top-1 class prediction) on randomly se-\nlected ILSVRC-2013 test set images in Fig. 2. Similarly to the ConvNet classi\ufb01cation procedure [8],\nwhere the class predictions are computed on 10 cropped and re\ufb02ected sub-images, we computed 10\nsaliency maps on the 10 sub-images, and then averaged them.\n3.2\nWeakly Supervised Object Localisation\nThe weakly supervised class saliency maps (Sect. 3.1) encode the location of the object of the given\nclass in the given image, and thus can be used for object localisation (in spite of being trained on\nimage labels only). Here we brie\ufb02y describe a simple object localisation procedure, which we used\nfor the localisation task of the ILSVRC-2013 challenge [12].\nGiven an image and the corresponding class saliency map, we compute the object segmentation mask\nusing the GraphCut colour segmentation [3]. The use of the colour segmentation is motivated by the\nfact that the saliency map might capture only the most discriminative part of an object, so saliency\nthresholding might not be able to highlight the whole object. Therefore, it is important to be able\nto propagate the thresholded map to other parts of the object, which we aim to achieve here using\nthe colour continuity cues. Foreground and background colour models were set to be the Gaussian\nMixture Models. The foreground model was estimated from the pixels with the saliency higher than\na threshold, set to the 95% quantile of the saliency distribution in the image; the background model\nwas estimated from the pixels with the saliency smaller than the 30% quantile (Fig. 3, right-middle).\nThe GraphCut segmentation [3] was then performed using the publicly available implementation2.\nOnce the image pixel labelling into foreground and background is computed, the object segmentation\nmask is set to the largest connected component of the foreground pixels (Fig. 3, right).\nWe entered our object localisation method into the ILSVRC-2013 localisation challenge. Consid-\nering that the challenge requires the object bounding boxes to be reported, we computed them as\nthe bounding boxes of the object segmentation masks. The procedure was repeated for each of the\ntop-5 predicted classes. The method achieved 46.4% top-5 error on the test set of ILSVRC-2013.\nIt should be noted that the method is weakly supervised (unlike the challenge winner with 29.9%\nerror), and the object localisation task was not taken into account during training. In spite of its\nsimplicity, the method still outperformed our submission to ILSVRC-2012 challenge (which used\nthe same dataset), which achieved 50.0% localisation error using a fully-supervised algorithm based\non the part-based models [6] and Fisher vector feature encoding [11].\n4\nRelation to Deconvolutional Networks\nIn this section we establish the connection between the gradient-based visualisation and the\nDeconvNet architecture of [13]. As we show below, DeconvNet-based reconstruction of the n-th\nlayer input Xn is either equivalent or similar to computing the gradient of the visualised neuron ac-\n2http://www.robots.ox.ac.uk/\u02dcvgg/software/iseg/\n4\nFigure 2:\nImage-speci\ufb01c class saliency maps for the top-1 predicted class in ILSVRC-2013\ntest images. The maps were extracted using a single back-propagation pass through a classi\ufb01cation\nConvNet. No additional annotation (except for the image labels) was used in training.\n5\nFigure 3:\nWeakly supervised object segmentation using ConvNets (Sect. 3.2). Left: images\nfrom the test set of ILSVRC-2013. Left-middle: the corresponding saliency maps for the top-1\npredicted class. Right-middle: thresholded saliency maps: blue shows the areas used to compute\nthe foreground colour model, cyan \u2013 background colour model, pixels shown in red are not used for\ncolour model estimation. Right: the resulting foreground segmentation masks.\n6\ntivity f with respect to Xn, so DeconvNet effectively corresponds to the gradient back-propagation\nthrough a ConvNet.\nFor the convolutional layer Xn+1 = Xn\u22c6Kn, the gradient is computed as \u2202f/\u2202Xn = \u2202f/\u2202Xn+1\u22c6\nc\nKn, where Kn and c\nKn are the convolution kernel and its \ufb02ipped version, respectively. The convo-\nlution with the \ufb02ipped kernel exactly corresponds to computing the n-th layer reconstruction Rn in\na DeconvNet: Rn = Rn+1 \u22c6c\nKn.\nFor the RELU recti\ufb01cation layer Xn+1 = max(Xn, 0), the sub-gradient takes the form: \u2202f/\u2202Xn =\n\u2202f/\u2202Xn+1 1 (Xn > 0), where 1 is the element-wise indicator function. This is slightly different\nfrom the DeconvNet RELU reconstruction: Rn = Rn+1 1 (Rn+1 > 0), where the sign indicator is\ncomputed on the output reconstruction Rn+1 instead of the layer input Xn.\nFinally, consider a max-pooling layer Xn+1(p) = maxq\u2208\u2126(p) Xn(q), where the element p of\nthe output feature map is computed by pooling over the corresponding spatial neighbourhood\n\u2126(p) of the input.\nThe sub-gradient is computed as \u2202f/\u2202Xn(s) = \u2202f/\u2202Xn+1(p) 1(s =\narg maxq\u2208\u2126(p) Xn(q)). Here, arg max corresponds to the max-pooling \u201cswitch\u201d in a DeconvNet.\nWe can conclude that apart from the RELU layer, computing the approximate feature map recon-\nstruction Rn using a DeconvNet is equivalent to computing the derivative \u2202f/\u2202Xn using back-\npropagation, which is a part of our visualisation algorithms. Thus, gradient-based visualisation can\nbe seen as the generalisation of that of [13], since the gradient-based techniques can be applied to\nthe visualisation of activities in any layer, not just a convolutional one. In particular, in this paper\nwe visualised the class score neurons in the \ufb01nal fully-connected layer.\nIt should be noted that our class model visualisation (Sect. 2) depicts the notion of a class, memo-\nrised by a ConvNet, and is not speci\ufb01c to any particular image. At the same time, the class saliency\nvisualisation (Sect. 3) is image-speci\ufb01c, and in this sense is related to the image-speci\ufb01c convolu-\ntional layer visualisation of [13] (the main difference being that we visualise a neuron in a fully\nconnected layer rather than a convolutional layer).\n5\nConclusion\nIn this paper, we presented two visualisation techniques for deep classi\ufb01cation ConvNets. The \ufb01rst\ngenerates an arti\ufb01cial image, which is representative of a class of interest. The second computes\nan image-speci\ufb01c class saliency map, highlighting the areas of the given image, discriminative with\nrespect to the given class. We showed that such saliency map can be used to initialise GraphCut-\nbased object segmentation without the need to train dedicated segmentation or detection models.\nFinally, we demonstrated that gradient-based visualisation techniques generalise the DeconvNet\nreconstruction procedure [13]. In our future research, we are planning to incorporate the image-\nspeci\ufb01c saliency maps into learning formulations in a more principled manner.\nAcknowledgements\nThis work was supported by ERC grant VisRec no. 228180. We gratefully acknowledge the support\nof NVIDIA Corporation with the donation of the Tesla K40 GPU used for this research.\nReferences\n[1] D. Baehrens, T. Schroeter, S. Harmeling, M. Kawanabe, K. Hansen, and K.-R. M\u00a8uller. How to explain\nindividual classi\ufb01cation decisions. JMLR, 11:1803\u20131831, 2010.\n[2] A. Berg, J. Deng, and L. Fei-Fei.\nLarge scale visual recognition challenge (ILSVRC), 2010.\nURL\nhttp://www.image-net.org/challenges/LSVRC/2010/.\n[3] Y. Boykov and M. P. Jolly. Interactive graph cuts for optimal boundary and region segmentation of objects\nin N-D images. In Proc. ICCV, volume 2, pages 105\u2013112, 2001.\n[4] D. C. Ciresan, U. Meier, and J. Schmidhuber. Multi-column deep neural networks for image classi\ufb01cation.\nIn Proc. CVPR, pages 3642\u20133649, 2012.\n[5] D. Erhan, Y. Bengio, A. Courville, and P. Vincent. Visualizing higher-layer features of a deep network.\nTechnical Report 1341, University of Montreal, Jun 2009.\n[6] P. Felzenszwalb, D. Mcallester, and D. Ramanan. A discriminatively trained, multiscale, deformable part\nmodel. In Proc. CVPR, 2008.\n7\n[7] G. E. Hinton, S. Osindero, and Y. W. Teh. A fast learning algorithm for deep belief nets. Neural Compu-\ntation, 18(7):1527\u20131554, 2006.\n[8] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet classi\ufb01cation with deep convolutional neural\nnetworks. In NIPS, pages 1106\u20131114, 2012.\n[9] Q. Le, M. Ranzato, R. Monga, M. Devin, K. Chen, G. Corrado, J. Dean, and A. Ng. Building high-level\nfeatures using large scale unsupervised learning. In Proc. ICML, 2012.\n[10] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition.\nProceedings of the IEEE, 86(11):2278\u20132324, 1998.\n[11] F. Perronnin, J. S\u00b4anchez, and T. Mensink. Improving the Fisher kernel for large-scale image classi\ufb01cation.\nIn Proc. ECCV, 2010.\n[12] K. Simonyan, A. Vedaldi, and A. Zisserman. Deep Fisher networks and class saliency maps for ob-\nject classi\ufb01cation and localisation. In ILSVRC workshop, 2013. URL http://image-net.org/\nchallenges/LSVRC/2013/slides/ILSVRC_az.pdf.\n[13] M. D. Zeiler and R. Fergus.\nVisualizing and understanding convolutional networks.\nCoRR,\nabs/1311.2901v3, 2013.\n8\n",
        "sentence": " Mahendran & Vedaldi (2015), as well as Simonyan et al. (2014) and Yosinski et al. Mahendran & Vedaldi (2015), as well as Simonyan et al. (2014) and Yosinski et al. (2015), apply gradient-based optimization to find an image \u0128 which minimizes the loss",
        "context": "[10] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition.\nProceedings of the IEEE, 86(11):2278\u20132324, 1998.\nimises the neuron activity of interest by carrying out an optimisation using gradient ascent in the\nimage space. The method was used to visualise the hidden feature layers of unsupervised deep ar-\nAbstract\nThis paper addresses the visualisation of image classi\ufb01cation models, learnt us-\ning deep Convolutional Networks (ConvNets).\nWe consider two visualisation\ntechniques, based on computing the gradient of the class score with respect to"
    },
    {
        "title": "Information processing in dynamical systems: Foundations of harmony theory",
        "author": [
            "P. Smolensky"
        ],
        "venue": "In Parallel Distributed Processing: Volume 1: Foundations,",
        "citeRegEx": "Smolensky.,? \\Q1987\\E",
        "shortCiteRegEx": "Smolensky.",
        "year": 1987,
        "abstract": "",
        "full_text": "",
        "sentence": " A prominent class of probabilistic models of images are restricted Boltzmann machines (Hinton & Sejnowski, 1986; Smolensky, 1987; Hinton & Salakhutdinov, 2006) and their deep variants (Hinton et al.",
        "context": null
    },
    {
        "title": "Perceptual quality measure using a spatio-temporal model of the human visual system",
        "author": [
            "C.J. van den Branden Lambrecht",
            "O. Verscheure"
        ],
        "venue": "Electronic Imaging: Science & Technology,",
        "citeRegEx": "Lambrecht and Verscheure.,? \\Q1996\\E",
        "shortCiteRegEx": "Lambrecht and Verscheure.",
        "year": 1996,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Extracting and composing robust features with denoising autoencoders",
        "author": [
            "P. Vincent",
            "H. Larochelle",
            "Y. Bengio",
            "P.-A. Manzagol"
        ],
        "venue": "In ICML, pp",
        "citeRegEx": "Vincent et al\\.,? \\Q2008\\E",
        "shortCiteRegEx": "Vincent et al\\.",
        "year": 2008,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Autoencoders (Hinton & Salakhutdinov, 2006; Vincent et al., 2008) have been widely used for unsupervised learning and generative modeling, too.",
        "context": null
    },
    {
        "title": "Unsupervised learning of visual representations using videos",
        "author": [
            "X. Wang",
            "A. Gupta"
        ],
        "venue": "In ICCV,",
        "citeRegEx": "Wang and Gupta.,? \\Q2015\\E",
        "shortCiteRegEx": "Wang and Gupta.",
        "year": 2015,
        "abstract": "Is strong supervision necessary for learning a good visual representation? Do\nwe really need millions of semantically-labeled images to train a Convolutional\nNeural Network (CNN)? In this paper, we present a simple yet surprisingly\npowerful approach for unsupervised learning of CNN. Specifically, we use\nhundreds of thousands of unlabeled videos from the web to learn visual\nrepresentations. Our key idea is that visual tracking provides the supervision.\nThat is, two patches connected by a track should have similar visual\nrepresentation in deep feature space since they probably belong to the same\nobject or object part. We design a Siamese-triplet network with a ranking loss\nfunction to train this CNN representation. Without using a single image from\nImageNet, just using 100K unlabeled videos and the VOC 2012 dataset, we train\nan ensemble of unsupervised networks that achieves 52% mAP (no bounding box\nregression). This performance comes tantalizingly close to its\nImageNet-supervised counterpart, an ensemble which achieves a mAP of 54.4%. We\nalso show that our unsupervised network can perform competitively in other\ntasks such as surface-normal estimation.",
        "full_text": "Unsupervised Learning of Visual Representations using Videos\nXiaolong Wang, Abhinav Gupta\nRobotics Institute, Carnegie Mellon University\nAbstract\nIs strong supervision necessary for learning a good\nvisual representation?\nDo we really need millions of\nsemantically-labeled images to train a Convolutional Neu-\nral Network (CNN)? In this paper, we present a simple yet\nsurprisingly powerful approach for unsupervised learning\nof CNN. Speci\ufb01cally, we use hundreds of thousands of un-\nlabeled videos from the web to learn visual representations.\nOur key idea is that visual tracking provides the supervi-\nsion. That is, two patches connected by a track should have\nsimilar visual representation in deep feature space since\nthey probably belong to the same object or object part. We\ndesign a Siamese-triplet network with a ranking loss func-\ntion to train this CNN representation. Without using a sin-\ngle image from ImageNet, just using 100K unlabeled videos\nand the VOC 2012 dataset, we train an ensemble of un-\nsupervised networks that achieves 52% mAP (no bound-\ning box regression). This performance comes tantalizingly\nclose to its ImageNet-supervised counterpart, an ensemble\nwhich achieves a mAP of 54.4%. We also show that our\nunsupervised network can perform competitively in other\ntasks such as surface-normal estimation.\n1. Introduction\nWhat is a good visual representation and how can we\nlearn it? At the start of this decade, most computer vision\nresearch focused on \u201cwhat\u201d and used hand-de\ufb01ned features\nsuch as SIFT [32] and HOG [5] as the underlying visual\nrepresentation. Learning was often the last step where these\nlow-level feature representations were mapped to seman-\ntic/3D/functional categories. However, the last three years\nhave seen the resurgence of learning visual representations\ndirectly from pixels themselves using the deep learning\nand Convolutional Neural Networks (CNNs) [28, 24, 23].\nAt the heart of CNNs is a completely supervised learning\nparadigm. Often millions of examples are \ufb01rst labeled us-\ning Mechanical Turk followed by data augmentation to cre-\nate tens of millions of training instances. CNNs are then\ntrained using gradient descent and back propagation. But\none question still remains: is strong-supervision necessary\nfor training these CNNs? Do we really need millions of\nsemantically-labeled images to learn a good representation?\n\u2026 \n\u2026 \n\u2026 \n\u2026 \nLearning to Rank \nConv \nNet \nConv \nNet \nConv \nNet \nQuery \n(First Frame) \nTracked \n(Last Frame) \nNegative  \n(Random) \n(a) Unsupervised Tracking in Videos  \n\ud835\udc37 \n, \n\ud835\udc37 \n, \n\ud835\udc37 \n, \n\ud835\udc37 \n, \n\ud835\udc37: Distance in deep feature space \n(b) Siamese-triplet Network \n(c) Ranking Objective  \nFigure 1. Overview of our approach. (a) Given unlabeled videos,\nwe perform unsupervised tracking on the patches in them. (b)\nTriplets of patches including query patch in the initial frame of\ntracking, tracked patch in the last frame, and random patch from\nother videos are fed into our siamese-triplet network for train-\ning. (c) The learning objective: Distance between the query and\ntracked patch in feature space should be smaller than the distance\nbetween query and random patches.\nIt seems humans can learn visual representations using little\nor no semantic supervision but our approaches still remain\ncompletely supervised.\nIn this paper, we explore the alternative: how we can ex-\nploit the unlabeled visual data on the web to train CNNs\n(e.g. AlexNet [24])? In the past, there have been several at-\ntempts at unsupervised learning using millions of static im-\nages [26, 44] or frames extracted from videos [56, 48, 34].\nThe most common architecture used is an auto-encoder\nwhich learns representations based on its ability to recon-\nstruct the input images [35, 3, 49, 37]. While these ap-\nproaches have been able to automatically learn V1-like \ufb01l-\nters given unlabeled data, they are still far away from su-\npervised approaches on tasks such as object detection. So,\nwhat is the missing link? We argue that static images them-\nselves might not have enough information to learn a good\n1\narXiv:1505.00687v2  [cs.CV]  6 Oct 2015\nvisual representation. But what about videos? Do they have\nenough information to learn visual representations? In fact,\nhumans also learn their visual representations not from mil-\nlions of static images but years of dynamic sensory inputs.\nCan we have similar learning capabilities for CNNs?\nWe present a simple yet surprisingly powerful approach\nfor unsupervised learning of CNNs using hundreds of thou-\nsands of unlabeled videos from the web. Visual tracking is\none of the \ufb01rst capabilities that develops in infants and often\nbefore semantic representations are learned1. Taking a leaf\nfrom this observation, we propose to exploit visual track-\ning for learning CNNs in an unsupervised manner. Speci\ufb01-\ncally, we track millions of \u201cmoving\u201d patches in hundreds of\nthousands of videos. Our key idea is that two patches con-\nnected by a track should have similar visual representation\nin deep feature space since they probably belong to the same\nobject. We design a Siamese-triplet network with ranking\nloss function to train the CNN representation. This ranking\nloss function enforces that in the \ufb01nal deep feature space\nthe \ufb01rst frame patch should be much closer to the tracked\npatch than any other randomly sampled patch. We demon-\nstrate the strength of our learning algorithm using exten-\nsive experimental evaluation. Without using a single image\nfrom ImageNet, just 100K unlabeled videos and VOC 2012\ndataset, we train an ensemble of AlexNet networks that\nachieves 52% mAP (no bounding box regression). This per-\nformance is similar to its ImageNet-supervised counterpart,\nan ensemble which achieves 54.4% mAP. We also show that\nour network trained using unlabeled videos achieves simi-\nlar performance to its completely supervised counterpart on\nother tasks such as surface normal estimation. We believe\nthis is the \ufb01rst time an unsupervised-pretrained CNN has\nbeen shown so competitive; that too on varied datasets and\ntasks. Speci\ufb01cally for VOC, we would like to put our re-\nsults in context: this is the best results till-date by using\nonly PASCAL-provided annotations (next best is scratch at\n44%).\n2. Related Work\nUnsupervised learning of visual representations has a\nrich history starting from original auto-encoders work of\nOlhausen and Field [35]. Most of the work in this area\ncan be broadly divided into three categories.\nThe \ufb01rst\nclass of algorithms focus on learning generative models\nwith strong priors [20, 46]. These algorithms essentially\ncapture co-occurrence statistics of features.\nThe second\nclass of algorithms use manually de\ufb01ned features such as\nSIFT or HOG and perform clustering over training data\nto discover semantic classes [42, 38]. Some of these re-\ncent algorithms also focus on learning mid-level repre-\nsentations rather than discovering semantic classes them-\n1http://www.aoa.org/patients-and-public/good-vision-throughout-\nlife/childrens-vision/infant-vision-birth-to-24-months-of-age\nselves [41, 6, 7]. The third class of algorithms and more\nrelated to our paper is unsupervised learning of visual rep-\nresentations from the pixels themselves using deep learning\napproaches [21, 26, 44, 39, 29, 47, 9, 33, 2, 49, 8]. Starting\nfrom the seminal work of Olhausen and Field [35], the goal\nis to learn visual representations which are (a) sparse and\n(b) reconstructive. Olhausen and Field [35] showed that us-\ning this criteria they can learn V1-like \ufb01lters directly from\nthe data. However, this work only focused on learning a sin-\ngle layer. This idea was extended by Hinton and Salakhut-\ndinov [21] to train a deep belief network in an unsuper-\nvised manner via stacking layer-by-layer RBMs. Similar to\nthis, Bengio et al. [3] investigated stacking of both RBMs\nand autoencoders. As a next step, Le et al. [26] scaled up\nthe learning of multi-layer autoencoder on large-scale unla-\nbeled data. They demonstrated that although the network is\ntrained in an unsupervised manner, the neurons in high lay-\ners can still have high responses on semantic objects such\nas human heads and cat faces. Sermanet et al. [39] applied\nconvolutional sparse coding to pre-train the model layer-by-\nlayer in unsupervised manner. The model is then \ufb01ne-tuned\nfor pedestrian detection. In a contemporary work, Doersch\net al. [8] explored to use spatial context as a cue to perform\nunsupervised learning for CNNs.\nHowever, it is not clear if static images is the right way\nto learn visual representations. Therefore, researchers have\nstarted focusing on learning feature representations using\nvideos [11, 53, 27, 43, 56, 16, 48, 34, 45].\nEarly work\nsuch as [56] focused on inclusion of constraints via video\nto autoencoder framework. The most common constraint is\nenforcing learned representations to be temporally smooth.\nSimilar to this, Goroshin et al. [16] proposed to learn auto-\nencoders based on the slowness prior. Other approaches\nsuch as Taylor et al. [48] trained convolutional gated RBMs\nto learn latent representations from pairs of successive im-\nages. This was extended in a recent work by Srivastava et\nal. [43] where they proposed to learn a LSTM model in an\nunsupervised manner to predict future frames.\nFinally, our work is also related to metric learning via\ndeep networks [51, 31, 4, 17, 15, 22, 54]. For example,\nChopra et al. [4] proposed to learn convolutional networks\nin a siamese architecture for face veri\ufb01cation.\nWang et\nal. [51] introduced a deep triplet ranking network to learn\n\ufb01ne-grained image similarity. Zhang et al. [55] optimized\nthe max-margin loss on triplet units to learn deep hashing\nfunction for image retrieval. However, all these methods\nrequired labeled data.\nOur work is also related to [30],\nwhich used CNN pre-trained on ImageNet classi\ufb01cation\nand detection dataset as initialization, and performed semi-\nsupervised learning in videos to tackle object detection in\ntarget domain. However, in our work, we propose an unsu-\npervised approach instead of semi-supervised algorithm.\n3. Overview\nOur goal is to train convolutional neural networks using\nhundreds of thousands of unlabeled videos from the Inter-\nnet. We follow the AlexNet architecture to design our base\nnetwork. However, since we do not have labels, it is not\nclear what should be the loss function and how we should\noptimize it. But in case of videos, we have another supervi-\nsory information: time. For example, we all know that the\nscene does not change drastically within a short time in a\nvideo and same object instances appear in multiple frames\nof the video. So, how do we exploit this information to train\na CNN-based representation?\nWe sample millions of patches in these videos and track\nthem over time. Since we are tracking these patches, we\nknow that the \ufb01rst and last tracked frames correspond to the\nsame instance of the moving object or object part. There-\nfore, any visual representation that we learn should keep\nthese two data points close in the feature space. But just us-\ning this constraint is not suf\ufb01cient: all points can be mapped\nto a single point in feature space. Therefore, for training our\nCNN, we sample a third patch which creates a triplet. For\ntraining, we use a loss function [51] that enforces that the\n\ufb01rst two patches connected by tracking are closer in feature\nspace than the \ufb01rst one and a random one.\nTraining a network with such triplets converges fast since\nthe task is easy to over\ufb01t to. One way is to increase the\nnumber of training triplets. However, after initial conver-\ngence most triplets satisfy the loss function and therefore\nback-propagating gradients using such triplets is inef\ufb01cient.\nInstead, analogous to hard-negative mining, we select the\nthird patch from multiple patches that violates the constraint\n(loss is maximum).\nSelecting this patch leads to more\nmeaningful gradients for faster learning.\n4. Patch Mining in Videos\nGiven a video, we want to extract patches of interest\n(patches with motion in our case) and track these patches to\ncreate training instances. One obvious way to \ufb01nd patches\nof interest is to compute optical \ufb02ow and use the high mag-\nnitude \ufb02ow regions. However, since YouTube videos are\nnoisy with a lot of camera motion, it is hard to localize\nmoving objects using simple optical \ufb02ow magnitude vec-\ntors. Thus we follow a two-step approach: in the \ufb01rst step,\nwe obtain SURF [1] interest points and use Improved Dense\nTrajectories (IDT) [50] to obtain motion of each SURF\npoint. Note that since IDT applies a homography estimation\n(video stabilization) method, it reduces the problem caused\nby camera motion. Given the trajectories of SURF inter-\nest points, we classify these points as moving if the \ufb02ow\nmagnitude is more than 0.5 pixels. We also reject frames\nif (a) very few (< 25%) SURF interest points are classi\ufb01ed\nas moving because it might be just noise; (b) majority of\nSURF interest points (> 75%) are classi\ufb01ed as moving as\n\u2026 \n\u2026 \nQuery \n(First Frame) \nTracked \n(Last Frame) \nSliding Window Searching \nTracking \nSmall Motion \nCamera Motion \nFigure 2.\nGiven the video about buses (the \u201cbus\u201d label are not\nutilized), we perform IDT on it. red points represents the SURF\nfeature points, green represents the trajectories for the points. We\nreject the frames with small and large camera motions (top pairs).\nGiven the selected frame, we \ufb01nd the bounding box containing\nmost of the moving SURF points. We then perform tracking. The\n\ufb01rst and last frame of the track provide pair of patches for training\nCNN.\nit corresponds to moving camera. Once we have extracted\nmoving SURF interest points, in the second step, we \ufb01nd the\nbest bounding box such that it contains most of the moving\nSURF points. The size of the bounding box is set as h \u00d7 w,\nand we perform sliding window with it in the frame. We\ntake the bounding box which contains the most number of\nmoving SURF interest points as the interest bounding box.\nIn the experiment, we set h = 227, w = 227 in the frame\nwith size 448 \u00d7 600. Note that these patches might contain\nobjects or part of an object as shown in Figure 2.\nTracking.\nGiven the initial bounding box, we perform\ntracking using the KCF tracker [19]. After tracking along 30\nframes in the video, we obtain the second patch. This patch\nacts as the similar patch to the query patch in the triplet.\nNote that the KCF tracker does not use any supervised in-\nformation except for the initial bounding box.\n5. Learning Via Videos\nIn the previous section, we discussed how we can use\ntracking to generate pairs of patches. We use this procedure\nto generate millions of such pairs (See Figure 3 for exam-\nples of pairs of patches mined). We now describe how we\nuse these as training instances for our visual representation\nlearning.\n5.1. Siamese Triplet Network\nOur goal is to learn a feature space such that the query\npatch is closer to the tracked patch as compared to any other\nrandomly sampled patch. To learn this feature space we de-\nsign a Siamese-triplet network. A Siamese-triplet network\nconsist of three base networks which share the same param-\nQuery \n(First Frame) \nTracked \n(Last Frame) \nQuery \n(First Frame) \nTracked \n(Last Frame) \nPatch \nPairs \nPatch \nPairs \nFigure 3. Examples of patch pairs we obtain via patch mining in the videos.\neters (see Figure 4). For our experiments, we take the image\nwith size 227 \u00d7 227 as input. The base network is based\non the AlexNet architecture [24] for the convolutional lay-\ners. Then we stack two fully connected layers on the pool5\noutputs, whose neuron numbers are 4096 and 1024 respec-\ntively. Thus the \ufb01nal output of each single network is 1024\ndimensional feature space f(\u00b7). We de\ufb01ne the loss function\non this feature space.\n5.2. Ranking Loss Function\nGiven the set of patch pairs S sampled from the video,\nwe propose to learn an image similarity model in the form\nof CNN. Speci\ufb01cally, given an image X as an input for the\nnetwork, we can obtain its feature in the \ufb01nal layer as f(X).\nThen, we de\ufb01ne the distance of two image patches X1, X2\nbased on the cosine distance in the feature space as,\nD(X1, X2) = 1 \u2212\nf(X1) \u00b7 f(X2)\n\u2225f(X1)\u2225\u2225f(X2)\u2225.\n(1)\nWe want to train a CNN to obtain feature representation\nf(\u00b7), so that the distance between query image patch and the\ntracked patch is small and the distance between query patch\nand other random patches is encouraged to be larger. For-\nmally, given the patch set S, where Xi is the original query\npatch (\ufb01rst patch in tracked frames), X+\ni is the tracked patch\nand X\u2212\ni is a random patch from a different video, we want\nto enforce D(Xi, X\u2212\ni ) > D(Xi, X+\ni ). Therefore, the loss\nof our ranking model is de\ufb01ned by hinge loss as,\nL(Xi, X+\ni , X\u2212\ni ) = max{0, D(Xi, X+\ni ) \u2212D(Xi, X\u2212\ni ) + M},\n(2)\nwhere M represents the gap parameters between two dis-\ntances. We set M = 0.5 in the experiment. Then our objec-\ntive function for training can be represented as,\nmin\nW\n\u03bb\n2 \u2225W \u22252\n2 +\nN\nX\ni=1\nmax{0, D(Xi, X+\ni ) \u2212D(Xi, X\u2212\ni ) + M},\n(3)\nwhere W is the parameter weights of the network, i.e., pa-\nrameters for function f(\u00b7). N is the number of the triplets of\nsamples. \u03bb is a constant representing weight decay, which\nis set to \u03bb = 0.0005.\n5.3. Hard Negative Mining for Triplet Sampling\nOne non-trivial part for learning to rank is the process of\nselecting negative samples. Given a pair of similar images\nXi, X+\ni , how can we select the patch X\u2212\ni , which is a nega-\ntive match to Xi, from the large pool of patches? Here we\n\ufb01rst select the negative patches randomly, and then \ufb01nd hard\nexamples (in a process analogous to hard negative mining).\nRandom Selection:\nDuring learning, we perform\nmini-batch Stochastic Gradient Descent (SGD). For each\nXi, X+\ni , we randomly sample K negative matches in the\nsame batch B, thus we have K sets of triplet of samples.\nFor every triplet of samples, we calculate the gradients over\nthree of them respectively and perform back propagation.\nNote that we shuf\ufb02e all the images randomly after each\nepoch of training, thus the pair of patches Xi, X+\ni can look\nat different negative matches each time.\nHard Negative Mining: While one can continue to sam-\nple random patches for creating the triplets, it is more ef\ufb01-\ncient to search the negative patches smartly. After 10 epochs\nof training using negative data selected randomly, we want\nto make the problem harder to get more robust feature rep-\nresentations. Analogous to hard-negative mining procedure\nin SVM, where gradient descent learning is only performed\non hard-negatives (not all possible negative), we search for\n\ud835\udc4b\ud835\udc56\n+ \n\ud835\udc4b\ud835\udc56\n\u2212 \n\ud835\udc4b\ud835\udc56 \n\ud835\udc53(\ud835\udc4b\ud835\udc56\n+) \n\ud835\udc53(\ud835\udc4b\ud835\udc56\n\u2212) \n\ud835\udc53(\ud835\udc4b\ud835\udc56) \nRanking  \nLoss \nLayer \nShared Weights \nShared Weights \n96 256 384 \n384 \n256 \n4096 \n1024 \nFigure 4.\nSiamese-triplet network. Each base network in the\nSiamese-triplet network share the same architecture and parameter\nweights. The architecture is recti\ufb01ed from AlexNet by using only\ntwo fully connected layers. Given a triplet of training samples,\nwe obtain their features from the last layer by forward propagation\nand compute the ranking loss.\nFigure 5.\nTop response regions for the pool5 neurons of our\nunsupervised-CNN. Each row shows top response of one neuron.\nthe negative patch such that the loss is maximum and use\nthat patch to compute and back propagate gradients.\nSpeci\ufb01cally, the sampling of negative matches is similar\nas random selection before, except that this time we select\naccording to the loss(Eq. 2). For each pair Xi, X+\ni , we cal-\nculate the loss of all other negative matches in batch B, and\nselect the top K ones with highest losses. We apply the loss\non these K negative matches as our \ufb01nal loss and calculate\nthe gradients over them. Since the feature of each sample\nis already computed after the forward propagation, we only\nneed to calculate the loss over these features, thus the extra\ncomputation for hard negative mining is very small. For the\nexperiments, we use K = 4. Note that while some of the\nnegatives might be semantically similar patches, our em-\nbedding constraint only requires same instance examples to\nbe closer than category examples (which can be closer than\nother negatives in the space).\n5.4. Adapting for Supervised Tasks\nGiven the CNN learned by using unsupervised data, we\nwant to transfer the learned representations to the tasks with\nsupervised data. In our experiments, we apply our model\nto two different tasks including object detection and sur-\nface normal estimation. In both tasks we take the base net-\nwork from our Siamese-triplet network and adjust the fully\nconnected layers and outputs accordingly.\nWe introduce\ntwo ways to \ufb01ne-tune and transfer the information obtained\nfrom unsupervised data to supervised learning.\nOne straight forward approach is directly applying our\nranking model as a pre-trained network for the target task.\nMore speci\ufb01cally, we use the parameters of the convolu-\ntional layers in the base network of our triplet architecture\nas initialization for the target task. For the fully connected\nlayers, we initialize them randomly. This method of trans-\nferring feature representation is very similar to the approach\napplied in RCNN [14]. However, RCNN uses the network\npre-trained with ImageNet Classi\ufb01cation data. In our case,\nthe unsupervised ranking task is quite different from object\ndetection and surface normal estimation. Thus, we need\nto adapt the learning rate to the \ufb01ne-tuning procedure in-\ntroduced in RCNN. We start with the learning rate with\n\u03f5 = 0.01 instead of 0.001 and set the same learning rate\nfor all layers. This setting is crucial since we want the pre-\ntrained features to be used as initialization of supervised\nlearning, and adapting the features to the new task.\nIn this paper,\nwe explore one more approach to\ntransfer/\ufb01ne-tune the network. Speci\ufb01cally, we note that\nthere might be more juice left in the millions of unsuper-\nvised training data (which could not be captured in the ini-\ntial learning stage).\nTherefore, we use an iterative \ufb01ne-\ntuning scheme. Given the initial unsupervised network, we\n\ufb01rst \ufb01ne-tune using the PASCAL VOC data. Given the new\n\ufb01ne-tuned network, we use this network to re-adapt to rank-\ning triplet task. Here we again transfer convolutional pa-\nrameters for re-adapting. Finally, this re-adapted network is\n\ufb01ne-tuned on the VOC data yielding a better trained model.\nWe show in the experiment that this circular approach gives\nimprovement in performance. We also notice that after two\niterations of this approach the network converges.\n5.5. Model Ensemble\nWe proposed an approach to learn CNNs using unlabeled\nvideos. However, there is absolutely no limit to generating\ntraining instances and pairs of tracked patches (YouTube\nhas more than billions of videos). This opens up the possi-\nbility of training multiple CNNs using different sets of data.\nOnce we have trained these CNNs, we append the fc7 fea-\ntures from each of these CNNs to train the \ufb01nal SVM. Note\nthat the ImageNet trained models also provide initial boost\nfor adding more networks (See Table 1).\n5.6. Implementation Details\nWe apply mini-batch SGD in training. As the 3 networks\nshare the same parameters, instead of inputting 3 samples\nto the triplet network, we perform the forward propagation\nfor the whole batch by a single network and calculate the\nloss based on the output feature. Given a pair of patches\nXi, X+\ni , we randomly select another patch X\u2212\ni \u2208B which\nis extracted in a different video from Xi, X+\ni . Given their\nfeatures from forward propagation f(Xi), f(X+\ni ), f(X\u2212\ni ),\nwe can compute the loss as Eq. 2.\nFor unsupervised learning, we download 100K videos\nfrom YouTube using the URLs provided by [30]. [30] used\nthousands of keywords to retrieve videos from YouTube.\nNote we drop the labels associated with each video. By per-\nforming our patch mining method on the videos, we obtain\n8 million image patches. We train three different networks\nseparately using 1.5M, 5M and 8M training samples. We\nreport numbers based on these three networks. To train our\nsiamese-triplet networks, we set the batch size as |B| = 100,\n(a) Unsupervised Pre-trained \n(b) Fine-tuned  \nFigure 6.\nConv1 \ufb01lters visualization. (a) The \ufb01lters of the \ufb01rst\nconvolutional layer of the siamese-triplet network trained in unsu-\npervised manner. (b) By \ufb01ne-tuning the unsupervised pre-trained\nnetwork on PASCAL VOC 2012, we obtain sharper \ufb01lters.\nthe learning rate starting with \u03f50 = 0.001. We \ufb01rst train our\nnetwork with random negative samples at this learning rate\nfor 150K iterations, and then we apply hard negative min-\ning based on it. For training on 1.5M patches, we reduce\nthe learning rate by a factor of 10 at every 80K iterations\nand train for 240K iterations. For training on 5M and 8M\npatches, we reduce the learning rate by a factor of 10 at ev-\nery 120K iterations and train for 350K iterations.\n6. Experiments\nWe demonstrate the quality of our learned visual rep-\nresentations with qualitative and quantitative experiments.\nQualitatively, we show the convolutional \ufb01lters learned in\nlayer 1 (See Figure 6). Our learned \ufb01lters are similar to V1\nthough not as strong. However, after \ufb01ne-tuning on PAS-\nCAL VOC 2012, these \ufb01lters become quite strong. We also\nshow that the underlying representation learns a reasonable\nnearness metric by showing what the units in Pool5 layers\nrepresent (See Figure 5). Ignoring boundary effects, each\npool5 unit has a receptive \ufb01eld of 195 \u00d7 195 pixels in the\noriginal 227 \u00d7 227 pixel input. A central pool5 unit has a\nnearly global view, while one near the edge has a smaller,\nclipped support. Each row displays top 6 activations for a\npool5 unit. We have chosen 5 pool5 units for visualization.\nFor example, the \ufb01rst neuron represents animal heads, sec-\nond represents potted plant, etc. This visualization indicates\nthe nearness metric learned by the network since each row\ncorresponds to similar \ufb01ring patterns inside the CNN. Our\nunsupervised networks are available for download.\n6.1. Unsupervised CNNs without Fine-tuning\nFirst, we demonstrate that the unsupervised-CNN rep-\nresentation learned using videos (without \ufb01ne-tuning) is\nreasonable.\nWe perform Nearest Neighbors (NN) using\nground-truth (GT) windows in VOC 2012 val set as query.\nThe retrieval-database consists of all selective search win-\ndows (more than 0.5 overlap with GT windows) in VOC\n2012 train set. See Figure 7 for qualitative results. Our\nunsupervised-CNN is far superior to a random AlexNet ar-\nchitecture and the results are quite comparable to AlexNet\ntrained on ImageNet.\nQuantitatively, we measure the retrieval rate by counting\nnumber of correct retrievals in top-K (K=20) retrievals. A\nretrieval is correct if the semantic class for retrieved patch\nand query patch are the same. Using our unsupervised-CNN\n(Pool5 features) without \ufb01ne-tuning and cosine distance, we\nobtain 40% retrieval rate. Our performance is signi\ufb01cantly\nbetter as compared to 24% by ELDA [18] on HOG and\n19% by AlexNet with random parameters (our initializa-\ntion). This clearly demonstrates our unsupervised network\nlearns a good visual representation compared to a random\nparameter CNN. As a baseline, ImageNet CNN performs\n62% (but note it already learns on semantics).\nWe also evaluate our unsupervised-CNN without \ufb01ne-\ntuning for scene classi\ufb01cation task on MIT Indoor 67 [36].\nWe train a linear classi\ufb01er using softmax loss.\nUsing\npool5 features from unsupervised-CNN without \ufb01ne-tuning\ngives 41% classi\ufb01cation accuracy compared to 21% for\nGIST+SVM and 16% for random AlexNet.\nImageNet-\ntrained AlexNet has 54% accuracy. We also provide object\ndetection results without \ufb01ne-tuning in the next section.\n6.2. Unsupervised CNNs with Fine-tuning\nNext, we evaluate our approach by transferring the fea-\nture representation learned in unsupervised manner to the\ntasks with labeled data. We focus on two challenging prob-\nlems: object detection and surface normal estimation.\n6.2.1\nObject Detection\nFor object detection, we perform our experiments on PAS-\nCAL VOC 2012 dataset [10].\nWe follow the detection\npipeline introduced in RCNN [14], which borrowed the\nCNNs pre-trained on other datasets and \ufb01ne-tuned on it with\nthe VOC data. The \ufb01ne-tuned CNN was then used to extract\nfeatures followed by training SVMs for each object class.\nHowever, instead of using ImageNet pre-trained network as\ninitialization in RCNN, we use our unsupervised-CNN. We\n\ufb01ne-tune our network with the trainval set (11540 images)\nand train SVMs with them. Evaluation is performed in the\nstandard test set (10991 images).\nAt the \ufb01ne-tuning stage, we change the output to 21\nclasses and initialize the convolutional layers with our unsu-\npervised pre-trained network. To \ufb01ne-tune the network, we\nstart with learning rate as \u03f5 = 0.01 and reduce the learning\nrate by a factor of 10 at every 80K iterations. The network\nis \ufb01ne-tuned for 200K iterations. Note that for all the exper-\niments, no bounding box regression is performed.\nQuery \n(a) Random AlexNet \n(b) Imagenet AlexNet \n(c) Unsupervised AlexNet \nFigure 7. Nearest neighbors results. Given the query object from VOC 2012 val, we retrieve the NN from VOC 2012 train via calculating\nthe cosine distance on pool5 feature space. We compare the results of 3 different models: (a) AlexNet with random parameters; (b) AlexNet\ntrained with Imagenet data; (c) AlexNet trained using our unsupervised method on 8M data.\nTable 1. mean Average Precision (mAP) on VOC 2012. \u201cexternal\u201d column shows the number of patches used to pre-train unsupervised-CNN.\nVOC 2012 test\nexternal aero bike bird boat bottle\nbus\ncar\ncat\nchair cow table\ndog\nhorse mbike person plant sheep sofa train\ntv\nmAP\nscratch\n0\n66.1 58.1 32.7 23.0\n21.8\n54.5 56.4 50.8\n21.6\n42.2 31.8 49.2\n49.8\n61.6\n52.1\n25.1\n52.6\n31.3 50.0\n49.1\n44.0\nscratch (3 ensemble)\n0\n68.7 61.2 36.1 25.7\n24.3\n58.9 58.8 55.3\n24.4\n43.5 36.7 53.0\n53.8\n65.6\n54.3\n27.3\n53.5\n38.3 54.6\n51.8\n47.3\nunsup + ft\n1.5M\n68.8 62.1 34.7 25.3\n26.6\n57.7 59.6 56.3\n22.0\n42.6 33.8 52.3\n50.3\n65.6\n53.9\n25.8\n51.5\n32.3 51.7\n51.8\n46.2\nunsup + ft\n5M\n69.0 64.0 37.1 23.6\n24.6\n58.7 58.9 59.6\n22.3\n46.0 35.1 53.3\n53.7\n66.9\n54.1\n25.4\n52.9\n31.2 51.9\n51.8\n47.0\nunsup + ft\n8M\n67.6 63.4 37.3 27.6\n24.0\n58.7 59.9 59.5\n23.7\n46.3 37.6 54.8\n54.7\n66.4\n54.8\n25.8\n52.5\n31.2 52.6\n52.6\n47.5\nunsup + ft (2 ensemble)\n6.5M\n72.4 66.2 41.3 26.4\n26.8\n61.0 61.9 63.1\n25.3\n51.0 38.7 58.1\n58.3\n70.0\n56.2\n28.6\n56.1\n38.5 55.9\n54.3\n50.5\nunsup + ft (3 ensemble)\n8M\n73.4 67.3 44.1 30.4\n27.8\n63.3 62.6 64.2\n27.7\n51.1 40.6 60.8\n59.2\n71.2\n58.5\n28.2\n55.6\n39.4 58.0\n56.1\n52.0\nunsup + iterative ft\n5M\n67.7 64.0 41.3 25.3\n27.3\n58.8 60.3 60.2\n24.3\n46.7 34.4 53.6\n53.8\n68.2\n55.7\n26.4\n51.1\n34.3 53.4\n52.3\n48.0\nRCNN 70K\n72.7 62.9 49.3 31.1\n25.9\n56.2 53.0 70.0\n23.3\n49.0 38.0 69.5\n60.1\n68.2\n46.4\n17.5\n57.2\n46.2 50.8\n54.1\n50.1\nRCNN 70K (2 ensemble)\n75.3 68.3 53.1 35.2\n27.7\n59.6 54.7 73.4\n26.5\n53.0 42.2 73.1\n66.1\n71.0\n48.5\n21.7\n59.2\n50.8 55.2\n58.0\n53.6\nRCNN 70K (3 ensemble)\n74.6 68.7 54.9 35.7\n29.4\n61.0 54.4 74.0\n28.4\n53.6 43.0 74.0\n66.1\n72.8\n50.3\n20.5\n60.0\n51.2 57.9\n58.0\n54.4\nRCNN 200K (big stepsize)\n73.3 67.1 46.3 31.7\n30.6\n59.4 61.0 67.9\n27.3\n53.1 39.1 64.1\n60.5\n70.9\n57.2\n26.1\n59.0\n40.1 56.2\n54.9\n52.3\nWe compare our method with the model trained from\nscratch as well as using ImagNet pre-trained network. No-\ntice that the results for VOC 2012 reported in RCNN [14]\nare obtained by only \ufb01ne-tuning on the train set without\nusing the val set. For fair comparison, we \ufb01ne-tuned the\nImageNet pre-trained network with VOC 2012 trainval set.\nMoreover, as the step size of reducing learning rate in\nRCNN [14] is set to 20K and iterations for \ufb01ne-tuning is\n70K, we also try to enlarge the step size to 50K and \ufb01ne-\ntune the network for 200K iterations. We report the results\nfor both of these settings.\nSingle Model. We show the results in Table 1. As a\nbaseline, we train the network from scratch on VOC 2012\ndataset and obtain 44% mAP. Using our unsupervised net-\nwork pre-trained with 1.5M pair of patches and then \ufb01ne-\ntuned on VOC 2012, we obtain mAP of 46.2% (unsup+ft,\nexternal data = 1.5M). However, using more data, 5M\nand 8M patches in pre-training and then \ufb01ne-tune, we can\nachieve 47% and 47.5% mAP. These results indicate that\nour unsupervised network provides a signi\ufb01cant boost as\ncompared to the scratch network. More importantly, when\nmore unlabeled data is applied, we can get better perfor-\nmance ( 3.5% boost compared to training from scratch).\nModel Ensemble. We also try combining different mod-\nels using different sets of unlabeled data in pre-training. By\nensembling two \ufb01ne-tuned networks which are pre-trained\nusing 1.5M and 5M patches, we obtained a boost of 3.5%\ncomparing to the single model, which is 50.5%(unsup+ft\n(2 ensemble)). Finally, we ensemble all three different net-\nworks pre-trained with different sets of data, whose size are\n1.5M, 5M and 8M respectively. We get another boost and\nreach 52% mAP (unsup+ft (3 ensemble)).\nBaselines. We compare our approach with RCNN [14]\nwhich uses ImageNet pre-trained models. Following the\nprocedure in [14], we obtain 50.1% mAP (RCNN 70K) by\nsetting the step size to 20K and \ufb01ne-tuning for 70K itera-\ntions. To generate a model ensemble, the CNNs are \ufb01rst\ntrained on the ImageNet dataset separately, and then they\nare \ufb01ne-tuned with the VOC 2012 dataset. The result of\nensembling two of these networks is 53.6% mAP (RCNN\n70K (2 ensemble)). If we ensemble three networks, we get\na mAP of 54.4%. For fair of comparison, we also \ufb01ne-\ntuned the ImageNet pre-trained model with larger step size\n(50K) and more iterations (200K). The result is 52.3% mAP\n(RCNN 200K (big stepsize)). Note that while ImageNet\nnetwork shows diminishing returns with ensembling since\nthe training data remains similar, in our case since every\nnetwork in the ensemble looks at different sets of data, we\nget huge performance boosts.\nExploring a better way to transfer learned represen-\ntation. Given our \ufb01ne-tuned model using 5M patches in\npre-training (unsup+ft, external = 5M), we use it to re-learn\nand re-adapt to the unsupervised triplet task. After that, the\nnetwork is re-applied to \ufb01ne-tune on VOC 2012. The \ufb01nal\nTable 2. Results on NYU v2 for per-pixel surface normal estimation, eval-\nuated over valid pixels.\n(Lower Better)\n(Higher Better)\nMean\nMedian 11.25\u25e622.5\u25e630\u25e6\nscratch\n38.6\n26.5\n33.1\n46.8 52.5\nunsup + ft\n34.2\n21.9\n35.7\n50.6 57.0\nImageNet + ft\n33.3\n20.8\n36.7\n51.7 58.1\nUNFOLD [13]\n35.1\n19.2\n37.6\n53.3 58.9\nDiscr. [25]\n32.5\n22.4\n27.4\n50.2 60.2\n3DP (MW) [12]\n36.0\n20.5\n35.9\n52.0 57.8\nresult for this single model is 48% mAP (unsup + iterative\nft), which is 1% better than the initial \ufb01ne-tuned network.\nUnsupervised network without \ufb01ne-tuning: We also\nperform object detection without \ufb01ne-tuning on VOC 2012.\nWe extract pool5 features using our unsupervised-CNN and\ntrain SVM on top of it. We obtain mAP of 26.1% using our\nunsupervised network (training with 8M data). The ensem-\nble of two unsupervised-network (training with 5M and 8M\ndata) gets mAP of 28.2%. As a comparison, Imagenet pre-\ntrained network without \ufb01ne-tuning gets mAP of 40.4%.\n6.2.2\nSurface Normal Estimation\nTo illustrate that our unsupervised representation can be\ngeneralized to different tasks, we adapt the unsupervised\nCNN to the task of surface normal estimation from a RGB\nimage.\nIn this task, we want to estimate the orienta-\ntion of the pixels.\nWe perform our experiments on the\nNYUv2 dataset [40], which includes 795 images for train-\ning and 654 images for testing. Each image is has corre-\nsponding depth information which can be used to generate\ngroundtruth surface normals. For evaluation and generating\nthe groundtruth, we adopt the protocols introduced in [12]\nwhich is used by different methods [12, 25, 13] on this task.\nTo apply deep learning to this task, we followed the same\nform of outputs and loss function as the coarse network\nmentioned in [52]. Speci\ufb01cally, we \ufb01rst learn a codebook\nby performing k-means on surface normals and generate 20\ncodewords. Each codeword represents one class and thus\nwe transform the problem to 20-class classi\ufb01cation for each\npixel. Given a 227 \u00d7 227 image as input, our network gen-\nerates surface normals for the whole scene. The output of\nour network is 20 \u00d7 20 pixels, each of which is represented\nby a distribution over 20 codewords. Thus the dimension of\noutput is 20 \u00d7 20 \u00d7 20 = 8000.\nThe network architecture for this task is also based on\nthe AlexNet. To relieve over-\ufb01tting, we only stack two fully\nconnected layers with 4096 and 8000 neurons on the pool5\nlayer. During training, we initialize the network with the\nunsupervised pre-trained network (single network using 8M\nexternal data). We use the same learning rate 1.0 \u00d7 10\u22126\nas [52] and \ufb01ne-tune with 10K iterations given the small\nnumber of training data. Note that unlike [52], we do not\nutilize any data from the videos in NYU dataset for training.\nFigure 8. Surface normal estimation results on NYU dataset. For\nvisualization, we use green for horizontal surface, blue for facing\nright and red for facing left, i.e., blue \u2192X; green \u2192Y; red \u2192Z.\nFor comparison, we also trained networks from scratch\nas well as using ImageNet pre-trained. For evaluation, we\nreport mean and median error (in degrees). We also report\npercentage of pixels with less than 11.25, 22.5 and 30 de-\ngree errors. We show our qualitative results in in Figure 8.\nand quantitative results in Table 2. Our approach (unsup +\nft) is signi\ufb01cantly better than network trained from scratch\nand comes very close to Imagenet-pretrained CNN (\u223c1%).\n7. Discussion and Conclusion\nWe have presented an approach to train CNNs in an un-\nsupervised manner using videos. Speci\ufb01cally, we track mil-\nlions of patches and learn an embedding using CNN that\nkeeps patches from same track closer in the embedding\nspace as compared to any random third patch. Our unsuper-\nvised pre-trained CNN \ufb01ne-tuned using VOC training data\noutperforms CNN trained from scratch by 3.5%. An ensem-\nble version of our approach outperforms scratch by 4.7%\nand comes tantalizingly close to an Imagenet-pretrained\nCNN (within 2.5%). We believe this is an extremely sur-\nprising result since until recently semantic supervision was\nconsidered a strong requirement for training CNNs. We be-\nlieve our successful implementation opens up a new space\nfor designing unsupervised learning algorithms for CNN\ntraining.\nAcknowledgement: This work was partially supported by ONR MURI\nN000141010934 and NSF IIS 1320083. This material was also based on\nresearch partially sponsored by DARPA under agreement number FA8750-\n14-2-0244. The U.S. Government is authorized to reproduce and distribute\nreprints for Governmental purposes notwithstanding any copyright nota-\ntion thereon. The views and conclusions contained herein are those of the\nauthors and should not be interpreted as necessarily representing the of\ufb01-\ncial policies or endorsements, either expressed or implied, of DARPA or\nthe U.S. Government. The authors would like to thank Yahoo! and Nvidia\nfor the compute cluster and GPU donations respectively.\nReferences\n[1] H. Bay, T. Tuytelaars, and L. V. Gool.\nSurf: Speeded up robust\nfeatures. In ECCV, 2006. 3\n[2] Y. Bengio, A. Courville, and P. Vincent. Representation learning: A\nreview and new perspectives. TPAMI, 35(8):1798\u20131828, 2013. 2\n[3] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. Greedy layer-\nwise training of deep networks. In NIPS, 2007. 1, 2\n[4] S. Chopra, R. Hadsell, and Y. LeCun. Learning a similarity met-\nric discriminatively, with application to face veri\ufb01cation. In CVPR,\n2005. 2\n[5] N. Dalal and B. Triggs. Histograms of oriented gradients for human\ndetection. In CVPR, 2005. 1\n[6] C. Doersch, A. Gupta, and A. A. Efros. Mid-level visual element\ndiscovery as discriminative mode seeking. In NIPS, 2013. 2\n[7] C. Doersch, A. Gupta, and A. A. Efros. Context as supervisory sig-\nnal: Discovering objects with predictable context. In ECCV, 2014.\n2\n[8] C. Doersch, A. Gupta, and A. A. Efros. Unsupervised visual repre-\nsentation learning by context prediction. In ICCV, 2015. 2\n[9] S. M. A. Eslami, N. Heess, and J. Winn. The shape boltzmann ma-\nchine: a strong model of object shape. In CVPR, 2012. 2\n[10] M. Everingham, L. V. Gool, C. K. Williams, J. Winn, , and A. Zis-\nserman.\nThe pascal visual object classes (voc) challenge.\nIJCV,\n88(2):303\u2013338, 2010. 6\n[11] P. Foldiak. Learning invariance from transformation sequences. Neu-\nral Computation, 3(2):194\u2013200, 1991. 2\n[12] D. F. Fouhey, A. Gupta, and M. Hebert. Data-driven 3D primitives\nfor single image understanding. In ICCV, 2013. 8\n[13] D. F. Fouhey, A. Gupta, and M. Hebert. Unfolding an indoor origami\nworld. In ECCV, 2014. 8\n[14] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hier-\narchies for accurate object detection and semantic segmentation. In\nCVPR, 2014. 5, 6, 7\n[15] Y. Gong, Y. Jia, T. K. Leung, A. Toshev, and S. Ioffe. Deep con-\nvolutional ranking for multilabel image annotation. In ICLR, 2007.\n2\n[16] R. Goroshin, J. Bruna, J. Tompson, D. Eigen, and Y. LeCun. Un-\nsupervised learning of spatiotemporally coherent metrics.\nCoRR,\nabs/1412.6056, 2015. 2\n[17] R. Hadsell, S. Chopra, and Y. LeCun. Dimensionality reduction by\nlearning an invariant mapping. In CVPR, 2006. 2\n[18] B. Hariharan, J. Malik, and D. Ramanan. Discriminative decorrela-\ntion for clustering and classi\ufb01cation. In ECCV, 2012. 6\n[19] J. F. Henriques, R. Caseiro, P. Martins, and J. Batista. High-speed\ntracking with kernelized correlation \ufb01lters. TPAMI, 2015. 3\n[20] G. E. Hinton, P. Dayan, B. J. Frey, and R. M. Neal.\nThe\u201d\nwake-sleep\u201d algorithm for unsupervised neural networks. Science,\n268(5214):1158\u20131161, 1995. 2\n[21] G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality\nof data with neural networks. Science, 313:504\u2013507, 2006. 2\n[22] E. Hoffer and N. Ailon. Deep metric learning using triplet network.\nCoRR, /abs/1412.6622, 2015. 2\n[23] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,\nS. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for\nfast feature embedding. CoRR, /abs/1408.5093, 2014. 1\n[24] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classi\ufb01ca-\ntion with deep convolutional neural networks. In NIPS, 2012. 1,\n4\n[25] L. Ladick\u00b4y, B. Zeisl, and M. Pollefeys.\nDiscriminatively trained\ndense surface normal estimation. In ECCV, 2014. 8\n[26] Q. V. Le, M. A. Ranzato, R. Monga, M. Devin, K. Chen, G. S. Cor-\nrado, J. Dean, and A. Y. Ng. Building high-level features using large\nscale unsupervised learning. In ICML, 2012. 1, 2\n[27] Q. V. Le, W. Y. Zou, S. Y. Yeung, and A. Y. Ng. Learning hierar-\nchical invariant spatio-temporal features for action recognition with\nindependent subspace analysis. In CVPR, 2011. 2\n[28] Y. LeCun, B. Boser, J. Denker, D. Henderson, R. E. Howard, W. Hub-\nbard, and L. D. Jackel. Handwritten digit recognition with a back-\npropagation network. In NIPS, 1990. 1\n[29] H. Lee, R. Grosse, R. Ranganath, and A. Y. Ng. Convolutional deep\nbelief networks for scalable unsupervised learning of hierarchical\nrepresentations. In ICML, 2009. 2\n[30] X. Liang, S. Liu, Y. Wei, L. Liu, L. Lin, and S. Yan. Computational\nbaby learning. CoRR, abs/1411.2861, 2014. 2, 5\n[31] S. Liu, X. Liang, L. Liu, X. Shen, J. Yang, C. Xu, X. Cao, and\nS. Yan. Matching-cnn meets knn: Quasi-parametric human parsing.\nIn CVPR, 2015. 2\n[32] D. Lowe.\nDistinctive Image Features from Scale-Invariant Key-\npoints. IJCV, 60(2):91\u2013110, 2004. 1\n[33] P. Luo, X. Wang, and X. Tang. Hierarchical face parsing via deep\nlearning. In CVPR, 2012. 2\n[34] H. Mobahi, R. Collobert, and J. Weston. Deep learning from tempo-\nral coherence in video. In ICML, 2009. 1, 2\n[35] B. A. Olshausen and D. J. Field. Sparse coding with an overcomplete\nbasis set: A strategy employed by v1? Vision research, 1997. 1, 2\n[36] A. Quattoni and A.Torralba. Recognizing indoor scenes. In CVPR,\n2009. 6\n[37] M. A. Ranzato, F. J. Huang, Y.-L. Boureau, and Y. LeCun. Unsu-\npervised learning of invariant feature hierarchies with applications to\nobject recognition. In CVPR, 2007. 1\n[38] B. C. Russell, A. A. Efros, J. Sivic, W. T. Freeman, and A. Zisserman.\nUsing multiple segmentations to discover objects and their extent in\nimage collections. In CVPR, 2006. 2\n[39] P. Sermanet, K. Kavukcuoglu, S. Chintala, and Y. LeCun. Pedestrian\ndetection with unsupervised multi-stage feature learning. In CVPR,\n2013. 2\n[40] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus. Indoor segmen-\ntation and support inference from RGBD images. In ECCV, 2012.\n8\n[41] S. Singh, A. Gupta, and A. A. Efros. Unsupervised discovery of\nmid-level discriminative patches. In ECCV, 2012. 2\n[42] J. Sivic, B. C. Russell, A. A. Efros, A. Zisserman, and W. T. Freeman.\nDiscovering objects and their location in images. In ICCV, 2005. 2\n[43] N. Srivastava, E. Mansimov, and R. Salakhutdinov.\nUnsuper-\nvised learning of video representations using lstms.\nCoRR,\nabs/1502.04681, 2015. 2\n[44] N. Srivastava and R. R. Salakhutdinov. Multimodal learning with\ndeep boltzmann machines. In NIPS, 2012. 1, 2\n[45] D. Stavens and S. Thrun. Unsupervised learning of invariant features\nusing video. In CVPR, 2010. 2\n[46] E. B. Sudderth, A. Torralba, W. T. Freeman, and A. S. Willsky.\nDescribing visual scenes using transformed dirichlet processes. In\nNIPS, 2005. 2\n[47] Y. Tang, R. Salakhutdinov, and G. Hinton. Robust boltzmann ma-\nchines for recognition and denoising. In CVPR, 2012. 2\n[48] G. W. Taylor, R. Fergus, Y. LeCun, and C. Bregler. Convolutional\nlearning of spatio-temporal features. In ECCV, 2010. 1, 2\n[49] P. Vincent, H. Larochelle, Y. Bengio, and P. Manzagol.\nExtract-\ning and composing robust features with denoising autoencoders. In\nICML, 2008. 1, 2\n[50] H. Wang and C. Schmid. Action recognition with improved trajecto-\nries. In ICCV, 2013. 3\n[51] J. Wang, Y. Song, T. Leung, C. Rosenberg, J. Wang, J. Philbin,\nB. Chen, and Y. Wu. Learning \ufb01ne-grained image similarity with\ndeep ranking. In CVPR, 2014. 2, 3\n[52] X. Wang, D. F. Fouhey, and A. Gupta. Designing deep networks for\nsurface normal estimation. In CVPR, 2015. 8\n[53] L. Wiskott and T. J. Sejnowski. Slow feature analysis:unsupervised\nlearning of invariances. Neural Computation, 14:715\u2013770, 2002. 2\n[54] P. Wohlhart and V. Lepetit. Learning descriptors for object recogni-\ntion and 3d pose estimation. In CVPR, 2015. 2\n[55] R. Zhang, L. Lin, R. Zhang, W. Zuo, and L. Zhang. Bit-scalable deep\nhashing with regularized similarity learning for image retrieval and\nperson re-identi\ufb01cation. TIP, 24(12):4766\u20134779, 2015. 2\n[56] W. Y. Zou, S. Zhu, A. Y. Ng, and K. Yu. Deep learning of invariant\nfeatures via simulated \ufb01xations in video. In NIPS, 2012. 1, 2\n",
        "sentence": "",
        "context": "research partially sponsored by DARPA under agreement number FA8750-\n14-2-0244. The U.S. Government is authorized to reproduce and distribute\nreprints for Governmental purposes notwithstanding any copyright nota-\ntion thereon. The views and conclusions contained herein are those of the\nauthors and should not be interpreted as necessarily representing the of\ufb01-\ncial policies or endorsements, either expressed or implied, of DARPA or\ndirectly from pixels themselves using the deep learning\nand Convolutional Neural Networks (CNNs) [28, 24, 23].\nAt the heart of CNNs is a completely supervised learning\nparadigm. Often millions of examples are \ufb01rst labeled us-"
    },
    {
        "title": "Image quality assessment: From error visibility to structural similarity",
        "author": [
            "Z. Wang",
            "A.C. Bovik",
            "H.R. Sheikh",
            "E.P. Simoncelli"
        ],
        "venue": "IEEE Transactions on Image Processing,",
        "citeRegEx": "Wang et al\\.,? \\Q2004\\E",
        "shortCiteRegEx": "Wang et al\\.",
        "year": 2004,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " The most popular perceptual image similarity metric is the structural similarity metric (SSIM) (Wang et al., 2004), which compares the local statistics of image patches.",
        "context": null
    },
    {
        "title": "A perceptual distortion metric for digital color images",
        "author": [
            "S. Winkler"
        ],
        "venue": "In in Proc. SPIE,",
        "citeRegEx": "Winkler.,? \\Q1998\\E",
        "shortCiteRegEx": "Winkler.",
        "year": 1998,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Understanding neural networks through deep visualization",
        "author": [
            "J. Yosinski",
            "J. Clune",
            "A. Nguyen",
            "T. Fuchs",
            "H. Lipson"
        ],
        "venue": "In Deep Learning Workshop,",
        "citeRegEx": "Yosinski et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "Yosinski et al\\.",
        "year": 2015,
        "abstract": "Recent years have produced great advances in training large, deep neural\nnetworks (DNNs), including notable successes in training convolutional neural\nnetworks (convnets) to recognize natural images. However, our understanding of\nhow these models work, especially what computations they perform at\nintermediate layers, has lagged behind. Progress in the field will be further\naccelerated by the development of better tools for visualizing and interpreting\nneural nets. We introduce two such tools here. The first is a tool that\nvisualizes the activations produced on each layer of a trained convnet as it\nprocesses an image or video (e.g. a live webcam stream). We have found that\nlooking at live activations that change in response to user input helps build\nvaluable intuitions about how convnets work. The second tool enables\nvisualizing features at each layer of a DNN via regularized optimization in\nimage space. Because previous versions of this idea produced less recognizable\nimages, here we introduce several new regularization methods that combine to\nproduce qualitatively clearer, more interpretable visualizations. Both tools\nare open source and work on a pre-trained convnet with minimal setup.",
        "full_text": "Understanding Neural Networks Through Deep Visualization\nJason Yosinski\nYOSINSKI@CS.CORNELL.EDU\nCornell University\nJeff Clune\nJEFFCLUNE@UWYO.EDU\nAnh Nguyen\nANGUYEN8@UWYO.EDU\nUniversity of Wyoming\nThomas Fuchs\nFUCHS@CALTECH.EDU\nJet Propulsion Laboratory, California Institute of Technology\nHod Lipson\nHOD.LIPSON@CORNELL.EDU\nCornell University\nAbstract\nRecent years have produced great advances in\ntraining large, deep neural networks (DNNs), in-\ncluding notable successes in training convolu-\ntional neural networks (convnets) to recognize\nnatural images. However, our understanding of\nhow these models work, especially what compu-\ntations they perform at intermediate layers, has\nlagged behind.\nProgress in the \ufb01eld will be\nfurther accelerated by the development of bet-\nter tools for visualizing and interpreting neural\nnets.\nWe introduce two such tools here.\nThe\n\ufb01rst is a tool that visualizes the activations pro-\nduced on each layer of a trained convnet as it\nprocesses an image or video (e.g. a live web-\ncam stream). We have found that looking at live\nactivations that change in response to user input\nhelps build valuable intuitions about how con-\nvnets work. The second tool enables visualizing\nfeatures at each layer of a DNN via regularized\noptimization in image space. Because previous\nversions of this idea produced less recognizable\nimages, here we introduce several new regular-\nization methods that combine to produce qualita-\ntively clearer, more interpretable visualizations.\nBoth tools are open source and work on a pre-\ntrained convnet with minimal setup.\nPublished in the Deep Learning Workshop, 31 st International\nConference on Machine Learning, Lille, France, 2015. Copyright\n2015 by the author(s).\n1. Introduction\nThe last several years have produced tremendous progress\nin training powerful, deep neural network models that are\napproaching and even surpassing human abilities on a vari-\nety of challenging machine learning tasks (Taigman et al.,\n2014; Schroff et al., 2015; Hannun et al., 2014). A \ufb02agship\nexample is training deep, convolutional neural networks\n(CNNs) with supervised learning to classify natural images\n(Krizhevsky et al., 2012). That area has bene\ufb01tted from the\ncombined effects of faster computing (e.g. GPUs), better\ntraining techniques (e.g. dropout (Hinton et al., 2012)), bet-\nter activation units (e.g. recti\ufb01ed linear units (Glorot et al.,\n2011)), and larger labeled datasets (Deng et al., 2009; Lin\net al., 2014).\nWhile there has thus been considerable improvements in\nour knowledge of how to create high-performing architec-\ntures and learning algorithms, our understanding of how\nthese large neural models operate has lagged behind. Neu-\nral networks have long been known as \u201cblack boxes\u201d be-\ncause it is dif\ufb01cult to understand exactly how any particu-\nlar, trained neural network functions due to the large num-\nber of interacting, non-linear parts. Large modern neural\nnetworks are even harder to study because of their size;\nfor example, understanding the widely-used AlexNet DNN\ninvolves making sense of the values taken by the 60 mil-\nlion trained network parameters. Understanding what is\nlearned is interesting in its own right, but it is also one\nkey way of further improving models: the intuitions pro-\nvided by understanding the current generation of models\nshould suggest ways to make them better. For example,\nthe deconvolutional technique for visualizing the features\nlearned by the hidden units of DNNs suggested an archi-\ntectural change of smaller convolutional \ufb01lters that led to\narXiv:1506.06579v1  [cs.CV]  22 Jun 2015\nstate of the art performance on the ImageNet benchmark in\n2013 (Zeiler & Fergus, 2013).\nWe also note that tools that enable understanding will es-\npecially bene\ufb01t the vast numbers of newcomers to deep\nlearning, who would like to take advantage of off-the-shelf\nsoftware packages \u2014 like Theano (Bergstra et al., 2010),\nPylearn2 (Goodfellow et al., 2013), Caffe (Jia et al., 2014),\nand Torch (Collobert et al., 2011) \u2014 in new domains, but\nwho may not have any intuition for why their models work\n(or do not). Experts can also bene\ufb01t as they iterate ideas for\nnew models or when they are searching for good hyperpa-\nrameters. We thus believe that both experts and newcomers\nwill bene\ufb01t from tools that provide intuitions about the in-\nner workings of DNNs. This paper provides two such tools,\nboth of which are open source so that scientists and prac-\ntitioners can integrate them with their own DNNs to better\nunderstand them.\nThe \ufb01rst tool is software that interactively plots the activa-\ntions produced on each layer of a trained DNN for user-\nprovided images or video. Static images afford a slow, de-\ntailed investigation of a particular input, whereas video in-\nput highlights the DNNs changing responses to dynamic in-\nput. At present, the videos are processed live from a user\u2019s\ncomputer camera, which is especially helpful because users\ncan move different items around the \ufb01eld of view, occlude\nand combine them, and perform other manipulations to ac-\ntively learn how different features in the network respond.\nThe second tool we introduce enables better visualization\nof the learned features computed by individual neurons at\nevery layer of a DNN. Seeing what features have been\nlearned is important both to understand how current DNNs\nwork and to fuel intuitions for how to improve them.\nAttempting to understand what computations are per-\nformed at each layer in DNNs is an increasingly popular di-\nrection of research. One approach is to study each layer as\na group and investigate the type of computation performed\nby the set of neurons on a layer as a whole (Yosinski et al.,\n2014; Mahendran & Vedaldi, 2014). This approach is in-\nformative because the neurons in a layer interact with each\nother to pass information to higher layers, and thus each\nneuron\u2019s contribution to the entire function performed by\nthe DNN depends on that neuron\u2019s context in the layer.\nAnother approach is to try to interpret the function com-\nputed by each individual neuron. Past studies in this vein\nroughly divide into two different camps: dataset-centric\nand network-centric. The former requires both a trained\nDNN and running data through that network; the latter re-\nquires only the trained network itself. One dataset-centric\napproach is to display images from the training or test\nset that cause high or low activations for individual units.\nAnother is the deconvolution method of Zeiler & Fer-\ngus (2013), which highlights the portions of a particular\nimage that are responsible for the \ufb01ring of each neural unit.\nNetwork-centric approaches investigate a network directly\nwithout any data from a dataset.\nFor example, Erhan\net al. (2009) synthesized images that cause high activa-\ntions for particular units. Starting with some initial input\nx = x0, the activation ai(x) caused at some unit i by\nthis input is computed, and then steps are taken in input\nspace along the gradient \u2202ai(x)/\u2202x to synthesize inputs\nthat cause higher and higher activations of unit i, eventu-\nally terminating at some x\u2217which is deemed to be a pre-\nferred input stimulus for the unit in question. In the case\nwhere the input space is an image, x\u2217can be displayed\ndirectly for interpretation. Others have followed suit, us-\ning the gradient to \ufb01nd images that cause higher activations\n(Simonyan et al., 2013; Nguyen et al., 2014) or lower acti-\nvations (Szegedy et al., 2013) for output units.\nThese gradient-based approaches are attractive in their sim-\nplicity, but the optimization process tends to produce im-\nages that do not greatly resemble natural images.\nIn-\nstead, they are composed of a collection of \u201chacks\u201d that\nhappen to cause high (or low) activations: extreme pixel\nvalues, structured high frequency patterns, and copies of\ncommon motifs without global structure (Simonyan et al.,\n2013; Nguyen et al., 2014; Szegedy et al., 2013; Good-\nfellow et al., 2014). The fact that activations may be ef-\nfected by such hacks is better understood thanks to sev-\neral recent studies.\nSpeci\ufb01cally, it has been shown that\nsuch hacks may be applied to correctly classi\ufb01ed images\nto cause them to be misclassi\ufb01ed even via imperceptibly\nsmall changes (Szegedy et al., 2013), that such hacks can\nbe found even without the gradient information to produce\nunrecognizable \u201cfooling examples\u201d (Nguyen et al., 2014),\nand that the abundance of non-natural looking images that\ncause extreme activations can be explained by the locally\nlinear behavior of neural nets (Goodfellow et al., 2014).\nWith such strong evidence that optimizing images to cause\nhigh activations produces unrecognizable images, is there\nany hope of using such methods to obtain useful visualiza-\ntions? It turns out there is, if one is able to appropriately\nregularize the optimization. Simonyan et al. (2013) showed\nthat slightly discernible images for the \ufb01nal layers of a con-\nvnet could be produced with L2-regularization. Mahendran\nand Vedaldi (2014) also showed the importance of incor-\nporating natural-image priors in the optimization process\nwhen producing images that mimic an entire-layer\u2019s \ufb01ring\npattern produced by a speci\ufb01c input image. We build on\nthese works and contribute three additional forms of reg-\nularization that, when combined, produce more recogniz-\nable, optimization-based samples than previous methods.\nBecause the optimization is stochastic, by starting at dif-\nferent random initial images, we can produce a set of opti-\n2\nFigure 1. The bottom shows a screenshot from the interactive visualization software. The webcam input is shown, along with the whole\nlayer of conv5 activations. The selected channel pane shows an enlarged version of the 13x13 conv5151 channel activations. Below it,\nthe deconv starting at the selected channel is shown. On the right, three selections of nine images are shown: synthetic images produced\nusing the regularized gradient ascent methods described in Section 3, the top 9 image patches from the training set (the images from the\ntraining set that caused the highest activations for the selected channel), and the deconv of the those top 9 images. All areas highlighted\nwith a green star relate to the particular selected channel, here conv5151; when the selection changes, these panels update. The top\ndepicts enlarged numerical optimization results for this and other channels. conv52 is a channel that responds most strongly to dog faces\n(as evidenced by the top nine images, which are not shown due to space constraints), but it also responds to \ufb02owers on the blanket on the\nbottom and half way up the right side of the image (as seen in the inset red highlight). This response to \ufb02owers can be partially seen in\nthe optimized images but would be missed in an analysis focusing only on the top nine images and their deconv versions, which contain\nno \ufb02owers. conv5151 detects different types of faces. The top nine images are all of human faces, but here we see it responds also to the\ncat\u2019s face (and in Figure 2 a lion\u2019s face). Finally, conv5111 activates strongly for the cat\u2019s face, the optimized images show catlike fur\nand ears, and the top nine images (not shown here) are also all of cats. For this image, the softmax output layer top two predictions are\n\u201cEgyptian Cat\u201d and \u201cComputer Keyboard.\u201d All \ufb01gures in this paper are best viewed digitally, in color, signi\ufb01cantly zoomed in.\n3\nmized images whose variance provides information about\nthe invariances learned by the unit.\nTo summarize, this paper makes the following two contri-\nbutions:\n1. We describe and release a software tool that provides\na live, interactive visualization of every neuron in a\ntrained convnet as it responds to a user-provided im-\nage or video. The tool displays forward activation val-\nues, preferred stimuli via gradient ascent, top images\nfor each unit from the training set, deconv highlighting\n(Zeiler & Fergus, 2013) of top images, and backward\ndiffs computed via backprop or deconv starting from\narbitrary units. The combined effect of these comple-\nmentary visualizations promotes a greater understand-\ning of what a neuron computes than any single method\non its own. We also describe a few insights we have\ngained from using this tool. (Section 2).\n2. We extend past efforts to visualize preferred activation\npatterns in input space by adding several new types\nof regularization, which produce what we believe are\nthe most interpretable images for large convnets so far\n(Section 3).\nBoth of our tools are released as open source and are avail-\nable at http://yosinski.com/deepvis. While the\ntools could be adapted to integrate with any DNN soft-\nware framework, they work out of the box with the popular\nCaffe DNN software package (Jia et al., 2014). Users may\nrun visualizations with their own Caffe DNN or our pre-\ntrained DNN, which comes with pre-computed images op-\ntimized to activate each neuron in this trained network. Our\npre-trained network is nearly identical to the \u201cAlexNet\u201d\narchitecture (Krizhevsky et al., 2012), but with local re-\nponse normalization layers after pooling layers following\n(Jia et al., 2014). It was trained with the Caffe framework\non the ImageNet 2012 dataset (Deng et al., 2009).\n2. Visualizing Live Convnet Activations\nOur \ufb01rst visualization method is straightforward: plotting\nthe activation values for the neurons in each layer of a con-\nvnet in response to an image or video. In fully connected\nneural networks, the order of the units is irrelevant, so plots\nof these vectors are not spatially informative. However, in\nconvolutional networks, \ufb01lters are applied in a way that re-\nspects the underlying geometry of the input; in the case\nof 2D images, \ufb01lters are applied in a 2D convolution over\nthe two spatial dimensions of the image. This convolution\nproduces activations on subsequent layers that are, for each\nchannel, also arranged spatially.\nFigure 1 shows examples of this type of plot for the conv5\nlayer. The conv5 layer has size 256\u00d713\u00d713, which we de-\npict as 256 separate 13\u00d713 grayscale images. Each of the\n256 small images contains activations in the same spatial\nx-y spatial layout as the input data, and the 256 images are\nsimply and arbitrarily tiled into a 16\u00d716 grid in row-major\norder. Figure 2 shows a zoomed in view of one particu-\nlar channel, conv5151, that responds to human and animal\nfaces. All layers can be viewed in the software tool, includ-\ning pooling and normalization layers.\nVisualizing these\nlayers provides intuitions about their effects and functions.\nAlthough this visualization is simple to implement, we \ufb01nd\nit informative because all data \ufb02owing through the network\ncan be visualized. There is nothing mysterious happening\nbehind the scenes. Because this convnet contains only a\nsingle path from input to output, every layer is a bottleneck\nthrough which all information must pass en-route to a clas-\nsi\ufb01cation decision. The layer sizes are all small enough that\nany one layer can easily \ufb01t on a computer screen.1 So far,\nwe have gleaned several surprising intuitions from using\nthe tool:\n\u2022 One of the most interesting conclusions so far has\nbeen that representations on some layers seem to be\nsurprisingly local. Instead of \ufb01nding distributed repre-\nsentations on all layers, we see, for example, detectors\nfor text, \ufb02owers, fruit, and faces on conv4 and conv5.\nThese conclusions can be drawn either from the live\nvisualization or the optimized images (or, best, by us-\ning both in concert) and suggest several directions for\nfuture research (discussed in Section 4).\n\u2022 When using direct \ufb01le input to classify photos from\nFlickr or Google Images, classi\ufb01cations are often cor-\nrect and highly con\ufb01dent (softmax probability for cor-\nrect class near 1). On the other hand, when using in-\nput from a webcam, predictions often cannot be cor-\nrect because no items from the training set are shown\nin the image. The training set\u2019s 1000 classes, though\nnumerous, do not cover most common household ob-\njects. Thus, when shown a typical webcam view of a\nperson with no ImageNet classes present, the output\nhas no single high probability, as is expected. Sur-\nprisingly, however, this probability vector is noisy and\nvaries signi\ufb01cantly in response to tiny changes in the\ninput, often changing merely in response to the noise\nfrom the webcam. We might have instead expected\nunchanging and low con\ufb01dence predictions for a given\nscene when no object the network has been trained to\nclassify is present. Plotting the fully connected layers\n(fc6 and fc7) also reveals a similar sensitivity to small\ninput changes.\n1The layer with the most activations is conv1 which, when\ntiled, is only 550x550 before adding padding.\n4\nFigure 2. A view of the 13\u00d713 activations of the 151st channel on\nthe conv5 layer of a deep neural network trained on ImageNet, a\ndataset that does not contain a face class, but does contain many\nimages with faces. The channel responds to human and animal\nfaces and is robust to changes in scale, pose, lighting, and context,\nwhich can be discerned by a user by actively changing the scene\nin front of a webcam or by loading static images (e.g. of the lions)\nand seeing the corresponding response of the unit. Photo of lions\nvia Flickr user arnolouise, licensed under CC BY-NC-SA 2.0.\n\u2022 Although the last three layers are sensitive to small\ninput changes, much of the lower layer computation\nis more robust. For example, when visualizing the\nconv5 layer, one can \ufb01nd many invariant detectors\nfor faces, shoulders, text, etc.\nby moving oneself\nor objects in front of the camera. Even though the\n1000 classes contain no explicitly labeled faces or\ntext, the network learns to identify these concepts sim-\nply because they represent useful partial information\nfor making a later classi\ufb01cation decision. One face\ndetector, denoted conv5151 (channel number 151 on\nconv5), is shown in Figure 2 activating for human\nand lion faces and in Figure 1 activating for a cat\nface. Zhou et al. (2014) recently observed a similar\neffect where convnets trained only to recognize dif-\nferent scene types \u2014 playgrounds, restaurant patios,\nliving rooms, etc. \u2014 learn object detectors (e.g. for\nchairs, books, and sofas) on intermediate layers.\nThe reader is encouraged to try this visualization tool out\nfor him or herself.\nThe code, together with pre-trained\nmodels and images synthesized by gradient ascent, can be\ndownloaded at http://yosinski.com/deepvis.\n3. Visualizing via Regularized Optimization\nThe second contribution of this work is introducing several\nregularization methods to bias images found via optimiza-\ntion toward more visually interpretable examples. While\neach of these regularization methods helps on its own, in\ncombination they are even more effective. We found use-\nful combinations via a random hyperparameter search, as\ndiscussed below.\nFormally, consider an image x \u2208RC\u00d7H\u00d7W , where C = 3\ncolor channels and the height (H) and width (W) are both\n227 pixels. When this image is presented to a neural net-\nwork, it causes an activation ai(x) for some unit i, where\nfor simplicity i is an index that runs over all units on all lay-\ners. We also de\ufb01ne a parameterized regularization function\nR\u03b8(x) that penalizes images in various ways.\nOur network was trained on ImageNet by \ufb01rst subtract-\ning the per-pixel mean of examples in ImageNet before in-\nputting training examples to the network. Thus, the direct\ninput to the network, x, can be thought of as a zero-centered\ninput. We may pose the optimization problem as \ufb01nding an\nimage x\u2217where\nx\u2217= arg max\nx\n(ai(x) \u2212R\u03b8(x))\n(1)\nIn practice, we use a slightly different formulation. Be-\ncause we search for x\u2217by starting at some x0 and taking\ngradient steps, we instead de\ufb01ne the regularization via an\noperator r\u03b8(\u00b7) that maps x to a slightly more regularized\nversion of itself. This latter de\ufb01nition is strictly more ex-\npressive, allowing regularization operators r\u03b8 that are not\n5\nthe gradient of any R\u03b8. This method is easy to implement\nwithin a gradient descent framework by simply alternating\nbetween taking a step toward the gradient of ai(x) and tak-\ning a step in the direction given by r\u03b8. With a gradient\ndescent step size of \u03b7, a single step in this process applies\nthe update:\nx \u2190r\u03b8\n\u0012\nx + \u03b7 \u2202ai\n\u2202x\n\u0013\n(2)\nWe investigated the following four regularizations. All are\ndesigned to overcome different pathologies commonly en-\ncountered by gradient descent without regularization.\nL2 decay: A common regularization, L2 decay penalizes\nlarge values and is implemented as r\u03b8(x) = (1\u2212\u03b8decay)\u00b7x.\nL2 decay tends to prevent a small number of extreme pixel\nvalues from dominating the example image. Such extreme\nsingle-pixel values neither occur naturally with great fre-\nquency nor are useful for visualization. L2 decay was also\nused by Simonyan et al. (2013).\nGaussian blur:\nProducing images via gradient ascent\ntends to produce examples with high frequency informa-\ntion (see Supplementary Section S1 for a possible reason).\nWhile these images cause high activations, they are neither\nrealistic nor interpretable (Nguyen et al., 2014). A useful\nregularization is thus to penalize high frequency informa-\ntion. We implement this as a Gaussian blur step r\u03b8(x) =\nGaussianBlur(x, \u03b8b width). Convolving with a blur ker-\nnel is more computationally expensive than the other reg-\nularization methods, so we added another hyperparameter\n\u03b8b every to allow, for example, blurring every several op-\ntimization steps instead of every step. Blurring an image\nmultiple times with a small width Gaussian kernel is equiv-\nalent to blurring once with a larger width kernel, and the\neffect will be similar even if the image changes slightly\nduring the optimization process. This technique thus low-\ners computational costs without limiting the expressiveness\nof the regularization. Mahendran & Vedaldi (2014) used a\npenalty with a similar effect to blurring, called total varia-\ntion, in their work reconstructing images from layer codes.\nClipping pixels with small norm: The \ufb01rst two regulariza-\ntions suppress high amplitude and high frequency informa-\ntion, so after applying both, we are left with an x\u2217that con-\ntains somewhat small, somewhat smooth values. However,\nx\u2217will still tend to contain non-zero pixel values every-\nwhere. Even if some pixels in x\u2217show the primary object\nor type of input causing the unit under consideration to ac-\ntivate, the gradient with respect to all other pixels in x\u2217will\nstill generally be non-zero, so these pixels will also shift to\nshow some pattern as well, contributing in whatever small\nway they can to ultimately raise the chosen unit\u2019s activa-\ntion. We wish to bias the search away from such behavior\nand instead show only the main object, letting other regions\nbe exactly zero if they are not needed. We implement this\nbias using an r\u03b8(x) that computes the norm of each pixel\n(over red, green, and blue channels) and then sets any pix-\nels with small norm to zero. The threshold for the norm,\n\u03b8n pct, is speci\ufb01ed as a percentile of all pixel norms in x.\nClipping pixels with small contribution: Instead of clip-\nping pixels with small norms, we can try something slightly\nsmarter and clip pixels with small contributions to the acti-\nvation. One way of computing a pixel\u2019s contribution to an\nactivation is to measure how much the activation increases\nor decreases when the pixel is set to zero; that is, to com-\npute the contribution as |ai(x) \u2212ai(x\u2212j)|, where x\u2212j is x\nbut with the jth pixel set to zero. This approach is straight-\nforward but prohibitively slow, requiring a forward pass\nfor every pixel. Instead, we approximate this process by\nlinearizing ai(x) around x, in which case the contribution\nof each dimension of x can be estimated as the elemen-\ntwise product of x and the gradient. We then sum over\nall three channels and take the absolute value, computing\n|P\nc x \u25e6\u2207xai(x)|. We use the absolute value to \ufb01nd pix-\nels with small contribution in either direction, positive or\nnegative. While we could choose to keep the pixel transi-\ntions where setting the pixel to zero would result in a large\nactivation increase, these shifts are already handled by gra-\ndient ascent, and here we prefer to clip only the pixels that\nare deemed not to matter, not to take large gradient steps\noutside the region where the linear approximation is most\nvalid. We de\ufb01ne this r\u03b8(x) as the operation that sets pixels\nwith contribution under the \u03b8c pct percentile to zero.\nIf the above regularization methods are applied individu-\nally, they are somewhat effective at producing more inter-\npretable images; Figure 3 shows the effects of each indi-\nvidual hyperparameter. However, preliminary experiments\nuncovered that their combined effect produces better vi-\nsualizations. To pick a reasonable set of hyperparameters\nfor all methods at once, we ran a random hyperparameter\nsearch of 300 possible combinations and settled on four that\ncomplement each other well. The four selected combina-\ntions are listed in Table 1 and optimized images using each\nare shown for the \u201cGorilla\u201d class output unit in Figure 4.\nOf the four, some show high frequency information, oth-\ners low frequency; some contain dense pixel data, and oth-\ners contain only sparse outlines of important regions. We\nfound the version in the lower-left quadrant to be the best\nsingle set of hyperparameters, but often greater intuition\ncan be gleaned by considering all four at once. Figure 5\nshows the optimization results computed for a selection of\nunits on all layers. A single image for every \ufb01lter of all\n\ufb01ve convolutional layers is shown in Supplementary Fig-\nure S1. Nine images for each \ufb01lter of all layers, including\neach of the 1000 ImageNet output classes, can be viewed\nat http://yosinski.com/deepvis.\n6\nFigure 4. Visualizations of the preferred inputs for different class units on layer fc8, the 1000-dimensional output of the network just\nbefore the \ufb01nal softmax. In the lower left are 9 visualizations each (in 3\u00d73 grids) for four different sets of regularization hyperparameters\nfor the Gorilla class (Table 1). For all other classes, we have selected four interpretable visualizations produced by our regularized\noptimization method. We chose the four combinations of regularization hyperparameters by performing a random hyperparameter search\nand selecting combinations that complement each other. For example, the lower left quadrant tends to show lower frequency patterns,\nthe upper right shows high frequency patterns, and the upper left shows a sparse set of important regions. Often greater intuition can\nbe gleaned by considering all four at once. In nearly every case, we have found that one can guess what class a neuron represents by\nviewing sets of these optimized, preferred images. Best viewed electronically, zoomed in.\n7\nFigure 5. Visualization of example features of eight layers of a deep, convolutional neural network. The images re\ufb02ect the true sizes\nof the features at different layers. In each layer, we show visualizations from 4 random gradient descent runs for each channel. While\nthese images are hand picked to showcase the diversity and interpretability of the visualizations, one image for each \ufb01lter of all \ufb01ve\nconvolutional layers is shown in Figure S1 in supplementary information. One can recognize important features of objects at different\nscales, such as edges, corners, wheels, eyes, shoulders, faces, handles, bottles, etc. The visualizations show the increase in complexity\nand variation on higher layers, comprised of simpler components from lower layers. The variation of patterns increases with increasing\nlayer number, indicating that increasingly invariant representations are learned. In particular, the jump from Layer 5 (the last convolution\nlayer) to Layer 6 (the \ufb01rst fully-connected layer) brings about a large increase in variation. Best viewed electronically, zoomed in.\n8\n\u2713decay = 0.5\n\u2713n pct = 95\n\u2713c pct = 95\n\u2713b width = 1.0\n\u2713b every = 4\n\u2713decay = 0.0\n\u2713n pct = 0\n\u2713c pct = 0\n\u2713b every = 10\n\u2713b width = 0.0\nFigure 3. The effects of each regularization method from Sec-\ntion 3 when used individually. Each of the four rows shows a\nlinear sweep in hyperparameter space from no regularization (left)\nto strong regularization (right). When applied too strongly, some\nregularizations cause the optimization to fail (e.g. L2 decay, top\nrow) or the images to be less interpretable (small norm and small\ncontribution clipping, bottom two rows). For this reason, a ran-\ndom hyperparameter search was useful for \ufb01nding joint hyperpa-\nrameter settings that worked well together (see Figure 4). Best\nviewed electronically, zoomed in.\n4. Discussion and Conclusion\nWe have introduced two visual tools for aiding in the inter-\npretation of trained neural nets. Intuition gained from these\ntools may prompt ideas for improved methods and future\nresearch. Here we discuss several such ideas.\nThe interactive tool reveals that representations on later\nconvolutional layers tend to be somewhat local, where\nchannels correspond to speci\ufb01c, natural parts (e.g. wheels,\nfaces) instead of being dimensions in a completely dis-\ntributed code.\nThat said, not all features correspond to\nnatural parts, raising the possibility of a different decom-\nposition of the world than humans might expect. These\nvisualizations suggest that further study into the exact na-\nture of learned representations \u2014 whether they are local to\na single channel or distributed across several \u2014 is likely to\nbe interesting (see Zhou et al. (2014) for work in this direc-\ntion). The locality of the representation also suggests that\nduring transfer learning, when new models are trained atop\nthe conv4 or conv5 representations, a bias toward sparse\nconnectivity could be helpful because it may be necessary\nto combine only a few features from these layers to create\nimportant features at higher layers.\nThe second tool \u2014 new regularizations that enable im-\nproved, interpretable, optimized visualizations of learned\nfeatures \u2014 will help researchers and practitioners under-\nstand, debug, and improve their models. The visualizations\nalso reveal a new twist in an ongoing story. Previous stud-\nTable 1. Four hyperparameter combinations that produce different\nstyles of recognizable images. We identi\ufb01ed these four after re-\nviewing images produced by 300 randomly selected hyperparam-\neter combinations. From top to bottom, they are the hyperparam-\neter combinations that produced the top-left, top-right, bottom-\nleft, and bottom-right Gorilla class visualizations, respectively, in\nFigure 4. The third row hyperparameters produced most of the\nvisualizations for the other classes in Figure 4, and all of those in\nFigure 5.\n\u03b8decay\n\u03b8b width\n\u03b8b every\n\u03b8n pct\n\u03b8c pct\n0\n0.5\n4\n50\n0\n0.3\n0\n0\n20\n0\n0.0001\n1.0\n4\n0\n0\n0\n0.5\n4\n0\n90\nies have shown that discriminative networks can easily be\nfooled or hacked by the addition of certain structured noise\nin image space (Szegedy et al., 2013; Nguyen et al., 2014).\nAn oft-cited reason for this property is that discriminative\ntraining leads networks to ignore non-discriminative infor-\nmation in their input, e.g. learning to detect jaguars by\nmatching the unique spots on their fur while ignoring the\nfact that they have four legs. For this reason it has been\nseen as a hopeless endeavor to create a generative model\nin which one randomly samples an x from a broad distri-\nbution on the space of all possible images and then itera-\ntively transforms x into a recognizable image by moving\nit to a region that satis\ufb01es both a prior p(x) and posterior\np(y|x) for some class label y. Past attempts have largely\nsupported this view by producing unrealistic images using\nthis method (Nguyen et al., 2014; Simonyan et al., 2013).\nHowever, the results presented here suggest an alternate\npossibility: the previously used priors may simply have\nbeen too weak (see Section S1 for one hypothesis of why\na strong p(x) model is needed). With the careful design or\nlearning of a p(x) model that biases toward realism, one\nmay be able to harness the large number of parameters\npresent in a discriminately learned p(y|x) model to gen-\nerate realistic images by enforcing probability under both\nmodels simultaneously. Even with the simple, hand-coded\np(x) models we use in this paper as regularizers, com-\nplex dependencies between distant pixels already arise (cf.\nthe beetles with structure spanning over 100 pixels in Fig-\nure 4). This implies that the discriminative parameters also\ncontain signi\ufb01cant \u201cgenerative\u201d structure from the training\ndataset; that is, the parameters encode not only the jaguar\u2019s\nspots, but to some extent also its four legs. With better,\nlearned probabilistic models over the input and activations\nof higher layers, much more structure may be apparent.\nWork by Dai et al. (2015) shows some interesting results\nin this direction. While the images generated in this pa-\nper are far from being photo-realistic, they do suggest that\n9\ntransferring discriminatively trained parameters to gener-\native models \u2014 opposite the direction of the usual unsu-\npervised pretraining approach \u2014 may be a fruitful area for\nfurther investigation.\nAcknowledgments\nThe authors would like to thank the NASA Space Technol-\nogy Research Fellowship (JY) for funding, Wendy Shang,\nYoshua Bengio, Brian Cheung, and Andrej Karpathy for\nhelpful discussions, and Freckles the cat for her feline\ncountenance.\nReferences\nBergstra, James, Breuleux, Olivier, Bastien, Fr\u00b4ed\u00b4eric, Lam-\nblin, Pascal, Pascanu, Razvan, Desjardins, Guillaume, Turian,\nJoseph, Warde-Farley, David, and Bengio, Yoshua. Theano: a\nCPU and GPU math expression compiler. In Proceedings of\nthe Python for Scienti\ufb01c Computing Conference (SciPy), June\n2010. Oral Presentation.\nCollobert, Ronan, Kavukcuoglu, Koray, and Farabet, Cl\u00b4ement.\nTorch7:\nA matlab-like environment for machine learning.\nIn BigLearn, NIPS Workshop, number EPFL-CONF-192376,\n2011.\nDai, Jifeng, Lu, Yang, and Wu, Ying Nian. Generative modeling\nof convolutional neural networks. In International Conference\non Learning Representations (ICLR), 2015.\nDeng, Jia, Dong, Wei, Socher, Richard, Li, Li-Jia, Li, Kai,\nand Fei-Fei, Li. Imagenet: A large-scale hierarchical image\ndatabase. In Computer Vision and Pattern Recognition, 2009.\nCVPR 2009. IEEE Conference on, pp. 248\u2013255. IEEE, 2009.\nErhan, Dumitru, Bengio, Yoshua, Courville, Aaron, and Vin-\ncent, Pascal. Visualizing higher-layer features of a deep net-\nwork. Technical report, Technical report, University of Mon-\ntreal, 2009.\nGlorot, Xavier, Bordes, Antoine, and Bengio, Yoshua.\nDeep\nsparse recti\ufb01er networks.\nIn Proceedings of the 14th Inter-\nnational Conference on Arti\ufb01cial Intelligence and Statistics.\nJMLR W&CP Volume, volume 15, pp. 315\u2013323, 2011.\nGoodfellow, Ian J, Warde-Farley, David, Lamblin, Pascal, Du-\nmoulin, Vincent, Mirza, Mehdi, Pascanu, Razvan, Bergstra,\nJames, Bastien, Fr\u00b4ed\u00b4eric, and Bengio, Yoshua.\nPylearn2:\na\nmachine\nlearning\nresearch\nlibrary.\narXiv\npreprint\narXiv:1308.4214, 2013.\nGoodfellow, Ian J, Shlens, Jonathon, and Szegedy, Christian. Ex-\nplaining and Harnessing Adversarial Examples. ArXiv e-prints,\nDecember 2014.\nHannun, A., Case, C., Casper, J., Catanzaro, B., Diamos, G.,\nElsen, E., Prenger, R., Satheesh, S., Sengupta, S., Coates, A.,\nand Ng, A. Y. Deep Speech: Scaling up end-to-end speech\nrecognition. ArXiv e-prints, December 2014.\nHinton,\nGeoffrey E, Srivastava,\nNitish,\nKrizhevsky,\nAlex,\nSutskever, Ilya, and Salakhutdinov, Ruslan R. Improving neu-\nral networks by preventing co-adaptation of feature detectors.\narXiv preprint arXiv:1207.0580, 2012.\nJia, Yangqing, Shelhamer, Evan, Donahue, Jeff, Karayev, Sergey,\nLong, Jonathan, Girshick, Ross, Guadarrama, Sergio, and Dar-\nrell, Trevor. Caffe: Convolutional architecture for fast feature\nembedding. arXiv preprint arXiv:1408.5093, 2014.\nKrizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoff. Imagenet\nclassi\ufb01cation with deep convolutional neural networks.\nIn\nAdvances in Neural Information Processing Systems 25, pp.\n1106\u20131114, 2012.\nLin, Tsung-Yi, Maire, Michael, Belongie, Serge, Hays, James,\nPerona, Pietro, Ramanan, Deva, Doll\u00b4ar, Piotr, and Zitnick,\nC. Lawrence. Microsoft COCO: common objects in context.\nCoRR, abs/1405.0312, 2014. URL http://arxiv.org/\nabs/1405.0312.\nMahendran, A. and Vedaldi, A. Understanding Deep Image Rep-\nresentations by Inverting Them.\nArXiv e-prints, November\n2014.\nNguyen, Anh, Yosinski, Jason, and Clune, Jeff. Deep Neural Net-\nworks are Easily Fooled: High Con\ufb01dence Predictions for Un-\nrecognizable Images. ArXiv e-prints, December 2014.\nSchroff, F., Kalenichenko, D., and Philbin, J. FaceNet: A Uni-\n\ufb01ed Embedding for Face Recognition and Clustering. ArXiv\ne-prints, March 2015.\nSimonyan, Karen, Vedaldi, Andrea, and Zisserman, Andrew.\nDeep inside convolutional networks:\nVisualising image\nclassi\ufb01cation models and saliency maps.\narXiv preprint\narXiv:1312.6034, presented at ICLR Workshop 2014, 2013.\nSzegedy, Christian, Zaremba, Wojciech, Sutskever, Ilya, Bruna,\nJoan, Erhan, Dumitru, Goodfellow, Ian J., and Fergus, Rob. In-\ntriguing properties of neural networks. CoRR, abs/1312.6199,\n2013.\nTaigman, Yaniv, Yang, Ming, Ranzato, Marc\u2019Aurelio, and Wolf,\nLior. Deepface: Closing the gap to human-level performance\nin face veri\ufb01cation. In Computer Vision and Pattern Recogni-\ntion (CVPR), 2014 IEEE Conference on, pp. 1701\u20131708. IEEE,\n2014.\nTorralba, Antonio and Oliva, Aude. Statistics of natural image\ncategories. Network: computation in neural systems, 14(3):\n391\u2013412, 2003.\nYosinski, J., Clune, J., Bengio, Y., and Lipson, H. How trans-\nferable are features in deep neural networks? In Ghahramani,\nZ., Welling, M., Cortes, C., Lawrence, N.D., and Weinberger,\nK.Q. (eds.), Advances in Neural Information Processing Sys-\ntems 27, pp. 3320\u20133328. Curran Associates, Inc., December\n2014.\nZeiler, Matthew D and Fergus, Rob.\nVisualizing and un-\nderstanding convolutional neural networks.\narXiv preprint\narXiv:1311.2901, 2013.\nZhou, Bolei, Khosla, Aditya, Lapedriza, `Agata, Oliva, Aude, and\nTorralba, Antonio. Object detectors emerge in deep scene cnns.\nCoRR, abs/1412.6856, 2014. URL http://arxiv.org/\nabs/1412.6856.\n10\nSupplementary Information for:\nUnderstanding Neural Networks Through Deep Visualization\nJason Yosinski\nYOSINSKI@CS.CORNELL.EDU\nJeff Clune\nJEFFCLUNE@UWYO.EDU\nAnh Nguyen\nANGUYEN8@UWYO.EDU\nThomas Fuchs\nFUCHS@CALTECH.EDU\nHod Lipson\nHOD.LIPSON@CORNELL.EDU\nS1. Why are gradient optimized images\ndominated by high frequencies?\nIn the main text we mentioned that images produced by\ngradient ascent to maximize the activations of neurons in\nconvolutional networks tend to be dominated by high fre-\nquency information (cf. the left column of Figure 3). One\nhypothesis for why this occurs centers around the differing\nstatistics of the activations of channels in a convnet. The\nconv1 layer consists of blobs of color and oriented Gabor\nedge \ufb01lters of varying frequencies. The average activation\nvalues (after the recti\ufb01er) of the edge \ufb01lters vary across\n\ufb01lters, with low frequency \ufb01lters generally having much\nhigher average activation values than high frequency \ufb01lters.\nIn one experiment we observed that the average activation\nvalues of the \ufb01ve lowest frequency edge \ufb01lters was 90 ver-\nsus an average for the \ufb01ve highest frequency \ufb01lters of 5.4, a\ndifference of a factor of 17 (manuscript in preparation)2,3.\nThe activation values for blobs of color generally fall in the\nmiddle of the range. This phenomenon likely arises for rea-\nsons related to the 1/f power spectrum of natural images in\nwhich low spatial frequencies tend to contain higher energy\nthan high spatial frequencies (Torralba & Oliva, 2003).\nNow consider the connections from the conv1 \ufb01lters to a\nsingle unit on conv2. In order to merge information from\nboth low frequency and high frequency conv1 \ufb01lters, the\nconnection weights from high frequency conv1 units may\ngenerally have to be larger than connections from low fre-\nquency conv1 units in order to allow both signals to affect\nthe conv2 unit\u2019s activation similarly. If this is the case, then\ndue to the larger multipliers, the activation of this particular\nconv2 unit is affected more by small changes in the activa-\ntions of high frequency \ufb01lters than low frequency \ufb01lters.\n2Li, Yosinski, Clune, Song, Hopcroft, Lipson. 2015. How\nsimilar are features learned by different deep neural networks? In\npreparation.\n3Activation values are averaged over the ImageNet validation\nset, over all spatial positions, over the channels with the \ufb01ve\n{highest, lowest} frequencies, and over four separately trained\nnetworks.\nSeen in the other direction: when gradient information is\npassed from higher layers to lower layers during backprop,\nthe partial derivative arriving at this conv2 unit (a scalar)\nwill be passed backward and multiplied by larger values\nwhen destined for high frequency conv1 \ufb01lters than low fre-\nquency \ufb01lters. Thus, following the gradient in pixel space\nmay tend to produce an overabundance of high frequency\nchanges instead of low frequency changes.\nThe above discussion focuses on the differing statistics of\nedge \ufb01lters in conv1, but note that activation statistics on\nsubsequent layers also vary across each layer.4 This may\nproduce a similar (though more subtle to observe) effect in\nwhich rare higher layer features are also overrepresented\ncompared to more common higher layer features.\nOf course, this hypothesis is only one tentative explanation\nfor why high frequency information dominates the gradi-\nent. It relies on the assumption that the average activation\nof a unit is a representative statistic of the whole distribu-\ntion of activations for that unit. In our observation this\nhas been the case, with most units having similar, albeit\nscaled, distributions. However, more study is needed be-\nfore a de\ufb01nitive conclusion can be reached.\nS2. Conv Layer Montages\nOne example optimized image using the hyperparameter\nsettings from the third row of Table 1 for every \ufb01lter of all\n\ufb01ve convolutional layers is shown in Figure S1.\n4We have observed that statistics vary on higher layers, but in\na different manner: most channels on these layers have similar av-\nerage activations, with most of the variance across channels being\ndominated by a small number of channels with unusually small\nor unusually large averages (Li, Yosinski, Clune, Song, Hopcroft,\nLipson. 2015. How similar are features learned by different deep\nneural networks? In preparation.)\nconv5\nconv3\nconv4\nconv2\nconv1\nFigure S1. One optimized, preferred image for every channel of all \ufb01ve convolutional layers. These images were produced with the\nhyperparameter combinations from the third row of Table 1. Best viewed electronically, zoomed in.\n12\n",
        "sentence": "",
        "context": "these images are hand picked to showcase the diversity and interpretability of the visualizations, one image for each \ufb01lter of all \ufb01ve\nogy Research Fellowship (JY) for funding, Wendy Shang,\nYoshua Bengio, Brian Cheung, and Andrej Karpathy for\nhelpful discussions, and Freckles the cat for her feline\ncountenance.\nReferences\nBergstra, James, Breuleux, Olivier, Bastien, Fr\u00b4ed\u00b4eric, Lam-\nThese conclusions can be drawn either from the live\nvisualization or the optimized images (or, best, by us-\ning both in concert) and suggest several directions for\nfuture research (discussed in Section 4)."
    }
]