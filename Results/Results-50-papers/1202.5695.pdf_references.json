[
    {
        "title": "Information Processing in Dynamical Systems: Foundations of Harmony Theory. In Parallel Distributed Processing: Explorations in the Microstructure of Cognition, pages 194\u2013281",
        "author": [
            "P. Smolensky"
        ],
        "venue": null,
        "citeRegEx": "Smolensky.,? \\Q1986\\E",
        "shortCiteRegEx": "Smolensky.",
        "year": 1986,
        "abstract": "",
        "full_text": "",
        "sentence": " The breadth of applications for the restricted Boltzmann machine (RBM) (Smolensky, 1986; Freund and Haussler, 1991) has expanded rapidly in recent years.",
        "context": null
    },
    {
        "title": "Unsupervised learning of distributions of binary vectors using 2-layer networks",
        "author": [
            "Y. Freund",
            "D. Haussler"
        ],
        "venue": "In NIPS",
        "citeRegEx": "Freund and Haussler.,? \\Q1991\\E",
        "shortCiteRegEx": "Freund and Haussler.",
        "year": 1991,
        "abstract": "",
        "full_text": "",
        "sentence": " The breadth of applications for the restricted Boltzmann machine (RBM) (Smolensky, 1986; Freund and Haussler, 1991) has expanded rapidly in recent years.",
        "context": null
    },
    {
        "title": "Factored 3-way restricted Boltzmann machines for modeling natural images",
        "author": [
            "M. Ranzato",
            "A. Krizhevsky",
            "G.E. Hinton"
        ],
        "venue": "In AISTATS,",
        "citeRegEx": "Ranzato et al\\.,? \\Q2010\\E",
        "shortCiteRegEx": "Ranzato et al\\.",
        "year": 2010,
        "abstract": "",
        "full_text": "",
        "sentence": " For example, RBMs have been used to model image patches (Ranzato et al., 2010), text documents as bags of words (Salakhutdinov and Hinton, 2009), and movie ratings (Salakhutdinov et al.",
        "context": null
    },
    {
        "title": "Replicated Softmax: an Undirected Topic Model",
        "author": [
            "R. Salakhutdinov",
            "G.E. Hinton"
        ],
        "venue": "In NIPS",
        "citeRegEx": "Salakhutdinov and Hinton.,? \\Q2009\\E",
        "shortCiteRegEx": "Salakhutdinov and Hinton.",
        "year": 2009,
        "abstract": "",
        "full_text": "",
        "sentence": " , 2010), text documents as bags of words (Salakhutdinov and Hinton, 2009), and movie ratings (Salakhutdinov et al. The issue is not one of representing such observations in the RBM framework: socalled softmax units (Salakhutdinov and Hinton, 2009) are the natural choice for modeling words. Because the multinomial probabilities are given by a softmax nonlinearity, the group of units v are referred to as softmax units (Salakhutdinov and Hinton, 2009).",
        "context": null
    },
    {
        "title": "Restricted Boltzmann machines for collaborative filtering",
        "author": [
            "R. Salakhutdinov",
            "A. Mnih",
            "G.E. Hinton"
        ],
        "venue": "In ICML,",
        "citeRegEx": "Salakhutdinov et al\\.,? \\Q2007\\E",
        "shortCiteRegEx": "Salakhutdinov et al\\.",
        "year": 2007,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " , 2010), text documents as bags of words (Salakhutdinov and Hinton, 2009), and movie ratings (Salakhutdinov et al., 2007), among other data.",
        "context": null
    },
    {
        "title": "Exponential family harmoniums with an application to information retrieval",
        "author": [
            "M. Welling",
            "M. Rosen-Zvi",
            "G.E. Hinton"
        ],
        "venue": "In NIPS",
        "citeRegEx": "Welling et al\\.,? \\Q2005\\E",
        "shortCiteRegEx": "Welling et al\\.",
        "year": 2005,
        "abstract": "",
        "full_text": "",
        "sentence": " Although RBMs were originally developed for binary observations, they have been generalized to several other types of data, including integer- and real-valued observations (Welling et al., 2005).",
        "context": null
    },
    {
        "title": "Parameter inference for imperfectly observed Gibbsian fields",
        "author": [
            "L. Younes"
        ],
        "venue": "Probability Theory Related Fields,",
        "citeRegEx": "Younes.,? \\Q1989\\E",
        "shortCiteRegEx": "Younes.",
        "year": 1989,
        "abstract": "",
        "full_text": "",
        "sentence": " This procedure belongs to the general class of Robbins-Monro stochastic approximation algorithms (Younes, 1989).",
        "context": null
    },
    {
        "title": "A neural probabilistic language model",
        "author": [
            "Y. Bengio",
            "R. Ducharme",
            "P. Vincent"
        ],
        "venue": "In NIPS",
        "citeRegEx": "Bengio et al\\.,? \\Q2001\\E",
        "shortCiteRegEx": "Bengio et al\\.",
        "year": 2001,
        "abstract": "",
        "full_text": "",
        "sentence": " Dealing with large multinomial distributions is an issue that has come up previously in work on neural network language models (Bengio et al., 2001). Dealing with large multinomial distributions is an issue that has come up previously in work on neural network language models (Bengio et al., 2001). For example, Morin and Bengio (2005) addressed this problem by introducing a fixed factorization of the (conditional) multinomial using a binary tree in which each leaf is associated with a single word. Dealing with large multinomial distributions is an issue that has come up previously in work on neural network language models (Bengio et al., 2001). For example, Morin and Bengio (2005) addressed this problem by introducing a fixed factorization of the (conditional) multinomial using a binary tree in which each leaf is associated with a single word. The tree was determined using an external knowledge base, although Mnih and Hinton (2009) investigated ways of extending this approach by learning the word tree from data. Therefore, we use an RBM parameterization very similar to that of Mnih and Hinton (2007), which itself is inspired by previous work on neural language models (Bengio et al., 2001).",
        "context": null
    },
    {
        "title": "Hierarchical probabilistic neural network language model",
        "author": [
            "F. Morin",
            "Y. Bengio"
        ],
        "venue": "In AISTATS,",
        "citeRegEx": "Morin and Bengio.,? \\Q2005\\E",
        "shortCiteRegEx": "Morin and Bengio.",
        "year": 2005,
        "abstract": "",
        "full_text": "",
        "sentence": " Therefore, we use an RBM parameterization very similar to that of Mnih and Hinton (2007), which itself is inspired by previous work on neural language models (Bengio et al.",
        "context": null
    },
    {
        "title": "A scalable hierarchical distributed language model",
        "author": [
            "A. Mnih",
            "G.E. Hinton"
        ],
        "venue": "In NIPS",
        "citeRegEx": "Mnih and Hinton.,? \\Q2009\\E",
        "shortCiteRegEx": "Mnih and Hinton.",
        "year": 2009,
        "abstract": "",
        "full_text": "",
        "sentence": " While one strategy might be to use a conditional RBM to model the tree-based factorizations, similar to Mnih and Hinton (2009), the end result would not be an RBM model of n-gram word windows, nor would it even be a conditional RBM over the next word given the n\u2212 1 previous ones. (2010) provide an overview and evaluation of these different methods, including those of Mnih and Hinton (2009) and of Collobert and Weston (2008). (2010) provide an overview and evaluation of these different methods, including those of Mnih and Hinton (2009) and of Collobert and Weston (2008). We have already mentioned the work of Mnih and Hinton (2009), who model the conditional distribution of the last word in n-gram windows. (2010) provide an overview and evaluation of these different methods, including those of Mnih and Hinton (2009) and of Collobert and Weston (2008). We have already mentioned the work of Mnih and Hinton (2009), who model the conditional distribution of the last word in n-gram windows. (2010) provide an overview and evaluation of these different methods, including those of Mnih and Hinton (2009) and of Collobert and Weston (2008). We have already mentioned the work of Mnih and Hinton (2009), who model the conditional distribution of the last word in n-gram windows. Collobert and Weston (2008) follows a similar approach, by training a neural network to fill-in the middle word of an n-gram window, using a marginbased learning objective. In contrast to Mnih and Hinton (2007), rather than training the WRRBM conditionally to model p(wn+t\u22121|wt, . 79 HLBL (Mnih and Hinton, 2009) 94.",
        "context": null
    },
    {
        "title": "On the alias method for generating random variables from a discrete distribution",
        "author": [
            "R.A. Kronmal",
            "A.V. Perterson"
        ],
        "venue": "The American Statistician,",
        "citeRegEx": "Kronmal and Perterson.,? \\Q1979\\E",
        "shortCiteRegEx": "Kronmal and Perterson.",
        "year": 1979,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " edu/~gdahl) of Kronmal and Perterson (1979) can be used to generate samples in constant time with linear setup time.",
        "context": null
    },
    {
        "title": "Training products of experts by minimizing contrastive divergence",
        "author": [
            "G.E. Hinton"
        ],
        "venue": "Neural Computation,",
        "citeRegEx": "Hinton.,? \\Q2002\\E",
        "shortCiteRegEx": "Hinton.",
        "year": 2002,
        "abstract": " It is possible to combine multiple latent-variable models of the same data by multiplying their probability distributions together and then renormalizing. This way of combining individual \u201cexpert\u201d models makes it hard to generate samples from the combined model but easy to infer the values of the latent variables of each expert, because the combination rule ensures that the latent variables of different experts are conditionally independent when given the data. A product of experts (PoE) is therefore an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary. Training a PoE by maximizing the likelihood of the data is difficult because it is hard even to approximate the derivatives of the renormalization term in the combination rule. Fortunately, a PoE can be trained using a different objective function called \u201ccontrastive divergence\u201d whose derivatives with regard to the parameters can be approximated accurately and efficiently. Examples are presented of contrastive divergence learning using several types of expert on several types of data. ",
        "full_text": "",
        "sentence": " Although there is evidence (Hinton, 2002) that poorly-mixing Markov chains can yield good learning signals, when this will occur is not as well understood. Turian et al. (2010) provide an overview and evaluation of these different methods, including those of Mnih and Hinton (2009) and of Collobert and Weston (2008).",
        "context": null
    },
    {
        "title": "S\u00e9n\u00e9cal. Quick training of probabilistic neural nets by importance sampling",
        "author": [
            "Y. Bengio",
            "J.-S"
        ],
        "venue": "In AISTATS,",
        "citeRegEx": "Bengio and J..S.,? \\Q2003\\E",
        "shortCiteRegEx": "Bengio and J..S.",
        "year": 2003,
        "abstract": "",
        "full_text": "",
        "sentence": " tations that capture syntactic information (as do the representations studied in Turian et al. (2010)), as it only models short windows of text and must enforce local agreement.",
        "context": null
    },
    {
        "title": "Word representations: A simple and general method for semisupervised learning",
        "author": [
            "J. Turian",
            "L. Ratinov",
            "Y. Bengio"
        ],
        "venue": "In ACL,",
        "citeRegEx": "Turian et al\\.,? \\Q2010\\E",
        "shortCiteRegEx": "Turian et al\\.",
        "year": 2010,
        "abstract": "",
        "full_text": "",
        "sentence": " As described by Turian et al. (2010), learning realvalued word representations can be used as a simple way of performing semi-supervised learning for a given method, by first learning word representations on unlabeled text and then feeding these representations as additional features to a supervised learning model. We evaluated the learned WRRBM word representations on a chunking task, following the setup described in Turian et al. (2010) and using the associated publicly-available code, as well as CRFSuite. We evaluated the learned WRRBM word representations on a chunking task, following the setup described in Turian et al. (2010) and using the associated publicly-available code, as well as CRFSuite. As in Turian et al. (2010), we used data from the CoNLL2000 shared task. The baseline results were taken from Turian et al. (2010). The performance measure is F1. representation features (as Turian et al. (2010) recommend) and for each WRRBM model, tried `2 penalties \u03bb \u2208 {0. The results are reported in Table 1, where we observe that word representations learned by our model achieved higher validation and test scores than the baseline of not using word representation features, and are comparable to the best of the three word representations tried in Turian et al. (2010). 44, a test set result superior to all word embedding results on chunking reported in Turian et al. (2010). (2011) used different (less stringent) preprocessing and a vocabulary of 300,000 words and obtained higher F1 scores than the methods evaluated in Turian et al. (2010). Unfortunately, the vocabulary and preprocessing differences mean that neither our result nor the one in Turian et al. (2011) used different (less stringent) preprocessing and a vocabulary of 300,000 words and obtained higher F1 scores than the methods evaluated in Turian et al. (2010). Unfortunately, the vocabulary and preprocessing differences mean that neither our result nor the one in Turian et al. (2010) are directly comparable to Dhillon et al. (2011) used different (less stringent) preprocessing and a vocabulary of 300,000 words and obtained higher F1 scores than the methods evaluated in Turian et al. (2010). Unfortunately, the vocabulary and preprocessing differences mean that neither our result nor the one in Turian et al. (2010) are directly comparable to Dhillon et al. (2011).",
        "context": null
    },
    {
        "title": "A unified architecture for natural language processing: Deep neural networks with multitask learning",
        "author": [
            "R. Collobert",
            "J. Weston"
        ],
        "venue": "In ICML,",
        "citeRegEx": "Collobert and Weston.,? \\Q2008\\E",
        "shortCiteRegEx": "Collobert and Weston.",
        "year": 2008,
        "abstract": "",
        "full_text": "",
        "sentence": " 00 C&W (Collobert and Weston, 2008) 94.",
        "context": null
    },
    {
        "title": "Three new graphical models for statistical language modelling",
        "author": [
            "A. Mnih",
            "G.E. Hinton"
        ],
        "venue": "In ICML,",
        "citeRegEx": "Mnih and Hinton.,? \\Q2007\\E",
        "shortCiteRegEx": "Mnih and Hinton.",
        "year": 2007,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit",
        "author": [
            "S. Bird",
            "E. Klein",
            "E. Loper"
        ],
        "venue": null,
        "citeRegEx": "Bird et al\\.,? \\Q2009\\E",
        "shortCiteRegEx": "Bird et al\\.",
        "year": 2009,
        "abstract": "",
        "full_text": "",
        "sentence": " We used NLTK (Bird et al., 2009) to tokenize the words and sentences, and also corrected a few common punctuation-related tokenization errors. We used NLTK (Bird et al., 2009) to tokenize the words and sentences, and also corrected a few common punctuation-related tokenization errors. As in Collobert et al. (2011), we lowercased all words and delexicalized numbers (replacing consecutive occurrences of one or more digits inside a word with just a single # character). We used NLTK (Bird et al., 2009) to tokenize the words and sentences, and also corrected a few common punctuation-related tokenization errors. As in Collobert et al. (2011), we lowercased all words and delexicalized numbers (replacing consecutive occurrences of one or more digits inside a word with just a single # character). Unlike Collobert et al. (2011), we did not include additional capitalization features, but discarded all capitalization information.",
        "context": null
    },
    {
        "title": "Natural language processing (almost) from scratch",
        "author": [
            "R. Collobert",
            "J. Weston",
            "L. Bottou",
            "M. Karlen",
            "K. Kavukcuoglu",
            "P. Kuksa"
        ],
        "venue": "Journal of Machine Learning Research,",
        "citeRegEx": "Collobert et al\\.,? \\Q2011\\E",
        "shortCiteRegEx": "Collobert et al\\.",
        "year": 2011,
        "abstract": "We propose a unified neural network architecture and learning algorithm that\ncan be applied to various natural language processing tasks including:\npart-of-speech tagging, chunking, named entity recognition, and semantic role\nlabeling. This versatility is achieved by trying to avoid task-specific\nengineering and therefore disregarding a lot of prior knowledge. Instead of\nexploiting man-made input features carefully optimized for each task, our\nsystem learns internal representations on the basis of vast amounts of mostly\nunlabeled training data. This work is then used as a basis for building a\nfreely available tagging system with good performance and minimal computational\nrequirements.",
        "full_text": "arXiv\narXiv\nNatural Language Processing (almost) from Scratch\nRonan Collobert\nronan@collobert.com\nNEC Labs America, Princeton NJ.\nJason Weston\njweston@google.com\nGoogle, New York, NY.\nL\u00b4eon Bottou\nleon@bottou.org\nMichael Karlen\nmichael.karlen@gmail.com\nKoray Kavukcuoglu\u2020\nkoray@cs.nyu.edu\nPavel Kuksa\u2021\npkuksa@cs.rutgers.edu\nNEC Labs America, Princeton NJ.\nAbstract\nWe propose a uni\ufb01ed neural network architecture and learning algorithm that can be applied\nto various natural language processing tasks including: part-of-speech tagging, chunking,\nnamed entity recognition, and semantic role labeling. This versatility is achieved by trying\nto avoid task-speci\ufb01c engineering and therefore disregarding a lot of prior knowledge.\nInstead of exploiting man-made input features carefully optimized for each task, our system\nlearns internal representations on the basis of vast amounts of mostly unlabeled training\ndata. This work is then used as a basis for building a freely available tagging system with\ngood performance and minimal computational requirements.\nKeywords:\nNatural Language Processing, Neural Networks\n1. Introduction\nWill a computer program ever be able to convert a piece of English text into a data structure\nthat unambiguously and completely describes the meaning of the natural language text?\nAmong numerous problems, no consensus has emerged about the form of such a data\nstructure. Until such fundamental Arti\ufb01cial Intelligence problems are resolved, computer\nscientists must settle for reduced objectives: extracting simpler representations describing\nrestricted aspects of the textual information.\nThese simpler representations are often motivated by speci\ufb01c applications, for instance,\nbag-of-words variants for information retrieval. These representations can also be motivated\nby our belief that they capture something more general about natural language.\nThey\ncan describe syntactic information (e.g. part-of-speech tagging, chunking, and parsing) or\nsemantic information (e.g. word-sense disambiguation, semantic role labeling, named entity\nextraction, and anaphora resolution). Text corpora have been manually annotated with such\ndata structures in order to compare the performance of various systems. The availability of\nstandard benchmarks has stimulated research in Natural Language Processing (NLP) and\n\u2020. Koray Kavukcuoglu is also with New York University, New York, NY.\n\u2021. Pavel Kuksa is also with Rutgers University, New Brunswick, NJ.\nc\u20dd2009 Ronan Collobert, Jason Weston, L\u00b4eon Bottou, Michael Karlen, Koray Kavukcuoglu and Pavel Kuksa.\narXiv:1103.0398v1  [cs.LG]  2 Mar 2011\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\ne\ufb00ective systems have been designed for all these tasks. Such systems are often viewed as\nsoftware components for constructing real-world NLP solutions.\nThe overwhelming majority of these state-of-the-art systems address a benchmark\ntask by applying linear statistical models to ad-hoc features.\nIn other words, the\nresearchers themselves discover intermediate representations by engineering task-speci\ufb01c\nfeatures. These features are often derived from the output of preexisting systems, leading\nto complex runtime dependencies. This approach is e\ufb00ective because researchers leverage\na large body of linguistic knowledge. On the other hand, there is a great temptation to\noptimize the performance of a system for a speci\ufb01c benchmark. Although such performance\nimprovements can be very useful in practice, they teach us little about the means to progress\ntoward the broader goals of natural language understanding and the elusive goals of Arti\ufb01cial\nIntelligence.\nIn this contribution, we try to excel on multiple benchmarks while avoiding task-speci\ufb01c\nenginering.\nInstead we use a single learning system able to discover adequate internal\nrepresentations. In fact we view the benchmarks as indirect measurements of the relevance\nof the internal representations discovered by the learning procedure, and we posit that these\nintermediate representations are more general than any of the benchmarks. Our desire to\navoid task-speci\ufb01c engineered features led us to ignore a large body of linguistic knowledge.\nInstead we reach good performance levels in most of the tasks by transferring intermediate\nrepresentations discovered on large unlabeled datasets. We call this approach \u201calmost from\nscratch\u201d to emphasize the reduced (but still important) reliance on a priori NLP knowledge.\nThe paper is organized as follows.\nSection 2 describes the benchmark tasks of\ninterest. Section 3 describes the uni\ufb01ed model and reports benchmark results obtained with\nsupervised training. Section 4 leverages large unlabeled datasets (\u223c852 million words)\nto train the model on a language modeling task.\nPerformance improvements are then\ndemonstrated by transferring the unsupervised internal representations into the supervised\nbenchmark models. Section 5 investigates multitask supervised training. Section 6 then\nevaluates how much further improvement can be achieved by incorporating standard NLP\ntask-speci\ufb01c engineering into our systems. Drifting away from our initial goals gives us the\nopportunity to construct an all-purpose tagger that is simultaneously accurate, practical,\nand fast. We then conclude with a short discussion section.\n2. The Benchmark Tasks\nIn this section, we brie\ufb02y introduce four standard NLP tasks on which we will benchmark\nour architectures within this paper: Part-Of-Speech tagging (POS), chunking (CHUNK),\nNamed Entity Recognition (NER) and Semantic Role Labeling (SRL). For each of them,\nwe consider a standard experimental setup and give an overview of state-of-the-art systems\non this setup. The experimental setups are summarized in Table 1, while state-of-the-art\nsystems are reported in Table 2.\n2.1 Part-Of-Speech Tagging\nPOS aims at labeling each word with a unique tag that indicates its syntactic role, e.g.\nplural noun, adverb, . . . A standard benchmark setup is described in detail by Toutanova\n2\narXiv\nNatural Language Processing (almost) from Scratch\nTask\nBenchmark\nDataset\nTraining set\nTest set\n(#tokens)\n(#tokens)\n(#tags)\nPOS\nToutanova et al. (2003)\nWSJ\nsections 0\u201318\nsections 22\u201324\n( 45 )\n( 912,344 )\n( 129,654 )\nChunking\nCoNLL 2000\nWSJ\nsections 15\u201318\nsection 20\n( 42 )\n( 211,727 )\n( 47,377 )\n(IOBES)\nNER\nCoNLL 2003\nReuters\n\u201ceng.train\u201d\n\u201ceng.testb\u201d\n( 17 )\n( 203,621 )\n( 46,435 )\n(IOBES)\nSRL\nCoNLL 2005\nWSJ\nsections 2\u201321\nsection 23\n( 186 )\n( 950,028 )\n+ 3 Brown sections\n(IOBES)\n( 63,843 )\nTable 1:\nExperimental setup: for each task, we report the standard benchmark we used,\nthe dataset it relates to, as well as training and test information.\nSystem\nAccuracy\nShen et al. (2007)\n97.33%\nToutanova et al. (2003)\n97.24%\nGim\u00b4enez and M`arquez (2004)\n97.16%\n(a) POS\nSystem\nF1\nShen and Sarkar (2005)\n95.23%\nSha and Pereira (2003)\n94.29%\nKudo and Matsumoto (2001)\n93.91%\n(b) CHUNK\nSystem\nF1\nAndo and Zhang (2005)\n89.31%\nFlorian et al. (2003)\n88.76%\nKudo and Matsumoto (2001)\n88.31%\n(c) NER\nSystem\nF1\nKoomen et al. (2005)\n77.92%\nPradhan et al. (2005)\n77.30%\nHaghighi et al. (2005)\n77.04%\n(d) SRL\nTable 2: State-of-the-art systems on four NLP tasks. Performance is reported in per-word\naccuracy for POS, and F1 score for CHUNK, NER and SRL. Systems in bold will be referred\nas benchmark systems in the rest of the paper (see text).\net al. (2003). Sections 0\u201318 of Wall Street Journal (WSJ) data are used for training, while\nsections 19\u201321 are for validation and sections 22\u201324 for testing.\nThe best POS classi\ufb01ers are based on classi\ufb01ers trained on windows of text, which are\nthen fed to a bidirectional decoding algorithm during inference. Features include preceding\nand following tag context as well as multiple words (bigrams, trigrams. . . ) context, and\nhandcrafted features to deal with unknown words.\nToutanova et al. (2003), who use\nmaximum entropy classi\ufb01ers, and a bidirectional dependency network (Heckerman et al.,\n2001) at inference, reach 97.24% per-word accuracy. Gim\u00b4enez and M`arquez (2004) proposed\na SVM approach also trained on text windows, with bidirectional inference achieved with\ntwo Viterbi decoders (left-to-right and right-to-left).\nThey obtained 97.16% per-word\naccuracy.\nMore recently, Shen et al. (2007) pushed the state-of-the-art up to 97.33%,\nwith a new learning algorithm they call guided learning, also for bidirectional sequence\nclassi\ufb01cation.\n3\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\n2.2 Chunking\nAlso called shallow parsing, chunking aims at labeling segments of a sentence with syntactic\nconstituents such as noun or verb phrases (NP or VP). Each word is assigned only one unique\ntag, often encoded as a begin-chunk (e.g. B-NP) or inside-chunk tag (e.g. I-NP). Chunking\nis often evaluated using the CoNLL 2000 shared task1. Sections 15\u201318 of WSJ data are\nused for training and section 20 for testing. Validation is achieved by splitting the training\nset.\nKudoh and Matsumoto (2000) won the CoNLL 2000 challenge on chunking with a F1-\nscore of 93.48%.\nTheir system was based on Support Vector Machines (SVMs).\nEach\nSVM was trained in a pairwise classi\ufb01cation manner, and fed with a window around the\nword of interest containing POS and words as features, as well as surrounding tags. They\nperform dynamic programming at test time.\nLater, they improved their results up to\n93.91% (Kudo and Matsumoto, 2001) using an ensemble of classi\ufb01ers trained with di\ufb00erent\ntagging conventions (see Section 3.2.3).\nSince then, a certain number of systems based on second-order random \ufb01elds were\nreported (Sha and Pereira, 2003; McDonald et al., 2005; Sun et al., 2008), all reporting\naround 94.3% F1 score. These systems use features composed of words, POS tags, and\ntags.\nMore recently, Shen and Sarkar (2005) obtained 95.23% using a voting classi\ufb01er scheme,\nwhere each classi\ufb01er is trained on di\ufb00erent tag representations2 (IOB, IOE, . . . ). They use\nPOS features coming from an external tagger, as well carefully hand-crafted specialization\nfeatures which again change the data representation by concatenating some (carefully\nchosen) chunk tags or some words with their POS representation. They then build trigrams\nover these features, which are \ufb01nally passed through a Viterbi decoder a test time.\n2.3 Named Entity Recognition\nNER labels atomic elements in the sentence into categories such as \u201cPERSON\u201d or\n\u201cLOCATION\u201d. As in the chunking task, each word is assigned a tag pre\ufb01xed by an indicator\nof the beginning or the inside of an entity. The CoNLL 2003 setup3 is a NER benchmark\ndataset based on Reuters data. The contest provides training, validation and testing sets.\nFlorian et al. (2003) presented the best system at the NER CoNLL 2003 challenge, with\n88.76% F1 score. They used a combination of various machine-learning classi\ufb01ers. Features\nthey picked included words, POS tags, CHUNK tags, pre\ufb01xes and su\ufb03xes, a large gazetteer\n(not provided by the challenge), as well as the output of two other NER classi\ufb01ers trained\non richer datasets. Chieu (2003), the second best performer of CoNLL 2003 (88.31% F1),\nalso used an external gazetteer (their performance goes down to 86.84% with no gazetteer)\nand several hand-chosen features.\nLater, Ando and Zhang (2005) reached 89.31% F1 with a semi-supervised approach.\nThey trained jointly a linear model on NER with a linear model on two auxiliary\nunsupervised tasks. They also performed Viterbi decoding at test time. The unlabeled\n1. See http://www.cnts.ua.ac.be/conll2000/chunking.\n2. See Table 3 for tagging scheme details.\n3. See http://www.cnts.ua.ac.be/conll2003/ner.\n4\narXiv\nNatural Language Processing (almost) from Scratch\ncorpus was 27M words taken from Reuters. Features included words, POS tags, su\ufb03xes\nand pre\ufb01xes or CHUNK tags, but overall were less specialized than CoNLL 2003 challengers.\n2.4 Semantic Role Labeling\nSRL aims at giving a semantic role to a syntactic constituent of a sentence.\nIn the\nPropBank (Palmer et al., 2005) formalism one assigns roles ARG0-5 to words that are\narguments of a verb (or more technically, a predicate) in the sentence, e.g. the following\nsentence might be tagged \u201c[John]ARG0 [ate]REL [the apple]ARG1 \u201d, where \u201cate\u201d is the\npredicate. The precise arguments depend on a verb\u2019s frame and if there are multiple verbs\nin a sentence some words might have multiple tags.\nIn addition to the ARG0-5 tags,\nthere there are several modi\ufb01er tags such as ARGM-LOC (locational) and ARGM-TMP\n(temporal) that operate in a similar way for all verbs. We picked CoNLL 20054 as our SRL\nbenchmark. It takes sections 2\u201321 of WSJ data as training set, and section 24 as validation\nset. A test set composed of section 23 of WSJ concatenated with 3 sections from the Brown\ncorpus is also provided by the challenge.\nState-of-the-art SRL systems consist of several stages: producing a parse tree, identifying\nwhich parse tree nodes represent the arguments of a given verb, and \ufb01nally classifying these\nnodes to compute the corresponding SRL tags.\nThis entails extracting numerous base\nfeatures from the parse tree and feeding them into statistical models. Feature categories\ncommonly used by these system include (Gildea and Jurafsky, 2002; Pradhan et al., 2004):\n\u2022 the parts of speech and syntactic labels of words and nodes in the tree;\n\u2022 the node\u2019s position (left or right) in relation to the verb;\n\u2022 the syntactic path to the verb in the parse tree;\n\u2022 whether a node in the parse tree is part of a noun or verb phrase;\n\u2022 the voice of the sentence: active or passive;\n\u2022 the node\u2019s head word; and\n\u2022 the verb sub-categorization.\nPradhan et al. (2004) take these base features and de\ufb01ne additional features, notably\nthe part-of-speech tag of the head word, the predicted named entity class of the argument,\nfeatures providing word sense disambiguation for the verb (they add 25 variants of 12 new\nfeature types overall). This system is close to the state-of-the-art in performance. Pradhan\net al. (2005) obtain 77.30% F1 with a system based on SVM classi\ufb01ers and simultaneously\nusing the two parse trees provided for the SRL task. In the same spirit, Haghighi et al.\n(2005) use log-linear models on each tree node, re-ranked globally with a dynamic algorithm.\nTheir system reaches 77.04% using the \ufb01ve top Charniak parse trees.\nKoomen et al. (2005) hold the state-of-the-art with Winnow-like (Littlestone, 1988)\nclassi\ufb01ers, followed by a decoding stage based on an integer program that enforces speci\ufb01c\nconstraints on SRL tags. They reach 77.92% F1 on CoNLL 2005, thanks to the \ufb01ve top\nparse trees produced by the Charniak (2000) parser (only the \ufb01rst one was provided by the\ncontest) as well as the Collins (1999) parse tree.\n4. See http://www.lsi.upc.edu/~srlconll.\n5\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\n2.5 Evaluation\nIn our experiments, we strictly followed the standard evaluation procedure of each CoNLL\nchallenges for NER, CHUNK and SRL. All these three tasks are evaluated by computing the\nF1 scores over chunks produced by our models. The POS task is evaluated by computing\nthe per-word accuracy, as it is the case for the standard benchmark we refer to (Toutanova\net al., 2003). We picked the conlleval script5 for evaluating POS6, NER and CHUNK.\nFor SRL, we used the srl-eval.pl script included in the srlconll package7.\n2.6 Discussion\nWhen participating in an (open) challenge, it is legitimate to increase generalization by all\nmeans. It is thus not surprising to see many top CoNLL systems using external labeled data,\nlike additional NER classi\ufb01ers for the NER architecture of Florian et al. (2003) or additional\nparse trees for SRL systems (Koomen et al., 2005). Combining multiple systems or tweaking\ncarefully features is also a common approach, like in the chunking top system (Shen and\nSarkar, 2005).\nHowever, when comparing systems, we do not learn anything of the quality of each\nsystem if they were trained with di\ufb00erent labeled data. For that reason, we will refer to\nbenchmark systems, that is, top existing systems which avoid usage of external data and\nhave been well-established in the NLP \ufb01eld: (Toutanova et al., 2003) for POS and (Sha and\nPereira, 2003) for chunking. For NER we consider (Ando and Zhang, 2005) as they were\nusing additional unlabeled data only. We picked (Koomen et al., 2005) for SRL, keeping in\nmind they use 4 additional parse trees not provided by the challenge. These benchmark\nsystems will serve as baseline references in our experiments.\nWe marked them in bold\nin Table 2.\nWe note that for the four tasks we are considering in this work, it can be seen that for the\nmore complex tasks (with corresponding lower accuracies), the best systems proposed have\nmore engineered features relative to the best systems on the simpler tasks. That is, the POS\ntask is one of the simplest of our four tasks, and only has relatively few engineered features,\nwhereas SRL is the most complex, and many kinds of features have been designed for it.\nThis clearly has implications for as yet unsolved NLP tasks requiring more sophisticated\nsemantic understanding than the ones considered here.\n3. The Networks\nAll the NLP tasks above can be seen as tasks assigning labels to words. The traditional NLP\napproach is: extract from the sentence a rich set of hand-designed features which are then\nfed to a standard classi\ufb01cation algorithm, e.g. a Support Vector Machine (SVM), often with\na linear kernel. The choice of features is a completely empirical process, mainly based \ufb01rst\non linguistic intuition, and then trial and error, and the feature selection is task dependent,\nimplying additional research for each new NLP task. Complex tasks like SRL then require\na large number of possibly complex features (e.g., extracted from a parse tree) which can\n5. Available at http://www.cnts.ua.ac.be/conll2000/chunking/conlleval.txt.\n6. We used the \u201c-r\u201d option of the conlleval script to get the per-word accuracy, for POS only.\n7. Available at http://www.lsi.upc.es/~srlconll/srlconll-1.1.tgz.\n6\narXiv\nNatural Language Processing (almost) from Scratch\nInput Window\nLookup Table\nLinear\nHardTanh\nLinear\nText\ncat\nsat\non the mat\nFeature 1\nw1\n1\nw1\n2\n. . .\nw1\nN\n...\nFeature K\nwK\n1\nwK\n2\n. . .\nwK\nN\nLTW 1\n...\nLTW K\nM 1 \u00d7 \u00b7\nM 2 \u00d7 \u00b7\nword of interest\nd\nconcat\nn1\nhu\nn2\nhu = #tags\nFigure 1: Window approach network.\nimpact the computational cost which might be important for large-scale applications or\napplications requiring real-time response.\nInstead, we advocate a radically di\ufb00erent approach: as input we will try to pre-process\nour features as little as possible and then use a multilayer neural network (NN) architecture,\ntrained in an end-to-end fashion.\nThe architecture takes the input sentence and learns\nseveral layers of feature extraction that process the inputs. The features computed by the\ndeep layers of the network are automatically trained by backpropagation to be relevant to\nthe task. We describe in this section a general multilayer architecture suitable for all our\nNLP tasks, which is generalizable to other NLP tasks as well.\nOur architecture is summarized in Figure 1 and Figure 2. The \ufb01rst layer extracts features\nfor each word. The second layer extracts features from a window of words or from the whole\nsentence, treating it as a sequence with local and global structure (i.e., it is not treated like\na bag of words). The following layers are standard NN layers.\nNotations\nWe consider a neural network f\u03b8(\u00b7), with parameters \u03b8.\nAny feed-forward\nneural network with L layers, can be seen as a composition of functions fl\n\u03b8(\u00b7), corresponding\nto each layer l:\nf\u03b8(\u00b7) = fL\n\u03b8 (fL\u22121\n\u03b8\n(. . . f1\n\u03b8 (\u00b7) . . .)) .\nIn the following, we will describe each layer we use in our networks shown in Figure 1\nand Figure 2. We adopt few notations. Given a matrix A we denote [A]i, j the coe\ufb03cient\n7\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\nInput Sentence\nLookup Table\nConvolution\nMax Over Time\nLinear\nHardTanh\nLinear\nText\nThe cat\nsat\non\nthe mat\nFeature 1\nw1\n1\nw1\n2\n. . .\nw1\nN\n...\nFeature K\nwK\n1\nwK\n2\n. . .\nwK\nN\nLTW 1\n...\nLTW K\nmax(\u00b7)\nM 2 \u00d7 \u00b7\nM 3 \u00d7 \u00b7\nd\nPadding\nPadding\nn1\nhu\nM 1 \u00d7 \u00b7\nn1\nhu\nn2\nhu\nn3\nhu = #tags\nFigure 2: Sentence approach network.\nat row i and column j in the matrix.\nWe also denote \u27e8A\u27e9dwin\ni\nthe vector obtained by\nconcatenating the dwin column vectors around the ith column vector of matrix A \u2208Rd1\u00d7d2:\nh\n\u27e8A\u27e9dwin\ni\niT\n=\n\u0010\n[A]1, i\u2212dwin/2 . . . [A]d1, i\u2212dwin/2 , . . . , [A]1, i+dwin/2 . . . [A]d1, i+dwin/2\n\u0011\n.\nAs a special case, \u27e8A\u27e91\ni represents the ith column of matrix A. For a vector v, we denote\n[v]i the scalar at index i in the vector. Finally, a sequence of element {x1, x2, . . . , xT } is\nwritten [x]T\n1 . The ith element of the sequence is [x]i.\n8\narXiv\nNatural Language Processing (almost) from Scratch\n3.1 Transforming Words into Feature Vectors\nOne of the essential key points of our architecture is its ability to perform well with the\nuse of (almost8) raw words. The ability for our method to learn good word representations\nis thus crucial to our approach. For e\ufb03ciency, words are fed to our architecture as indices\ntaken from a \ufb01nite dictionary D. Obviously, a simple index does not carry much useful\ninformation about the word. However, the \ufb01rst layer of our network maps each of these\nword indices into a feature vector, by a lookup table operation. Given a task of interest, a\nrelevant representation of each word is then given by the corresponding lookup table feature\nvector, which is trained by backpropagation.\nMore formally, for each word w \u2208D, an internal dwrd-dimensional feature vector\nrepresentation is given by the lookup table layer LTW (\u00b7):\nLTW (w) = \u27e8W\u27e91\nw ,\nwhere W \u2208Rdwrd\u00d7|D| is a matrix of parameters to be learnt, \u27e8W\u27e91\nw \u2208Rdwrd is the wth\ncolumn of W and dwrd is the word vector size (a hyper-parameter to be chosen by the user).\nGiven a sentence or any sequence of T words [w]T\n1 in D, the lookup table layer applies the\nsame operation for each word in the sequence, producing the following output matrix:\nLTW ([w]T\n1 ) =\n\u0010\n\u27e8W\u27e91\n[w]1\n\u27e8W\u27e91\n[w]2\n. . .\n\u27e8W\u27e91\n[w]T\n\u0011\n.\n(1)\nThis matrix can then be fed to further neural network layers, as we will see below.\n3.1.1 Extending to Any Discrete Features\nOne might want to provide features other than words if one suspects that these features are\nhelpful for the task of interest. For example, for the NER task, one could provide a feature\nwhich says if a word is in a gazetteer or not. Another common practice is to introduce some\nbasic pre-processing, such as word-stemming or dealing with upper and lower case. In this\nlatter option, the word would be then represented by three discrete features: its lower case\nstemmed root, its lower case ending, and a capitalization feature.\nGenerally speaking, we can consider a word as represented by K discrete features w \u2208\nD1\u00d7\u00b7 \u00b7 \u00b7\u00d7DK, where Dk is the dictionary for the kth feature. We associate to each feature a\nlookup table LTW k(\u00b7), with parameters W k \u2208Rdk\nwrd\u00d7|Dk| where dk\nwrd \u2208N is a user-speci\ufb01ed\nvector size. Given a word w, a feature vector of dimension dwrd = P\nk dk\nwrd is then obtained\nby concatenating all lookup table outputs:\nLTW 1,...,W K(w) =\n\uf8eb\n\uf8ec\n\uf8ed\nLTW 1(w1)\n...\nLTW K(wK)\n\uf8f6\n\uf8f7\n\uf8f8=\n\uf8eb\n\uf8ec\n\uf8ed\n\u27e8W 1\u27e91\nw1\n...\n\u27e8W K\u27e91\nwK\n\uf8f6\n\uf8f7\n\uf8f8.\n8. We did some pre-processing, namely lowercasing and encoding capitalization as another feature. With\nenough (unlabeled) training data, presumably we could learn a model without this processing. Ideally,\nan even more raw input would be to learn from letter sequences rather than words, however we felt that\nthis was beyond the scope of this work.\n9\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\nThe matrix output of the lookup table layer for a sequence of words [w]T\n1 is then similar\nto (1), but where extra rows have been added for each discrete feature:\nLTW 1,...,W K([w]T\n1 ) =\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ed\n\u27e8W 1\u27e91\n[w1]1\n. . .\n\u27e8W 1\u27e91\n[w1]T\n...\n...\n\u27e8W K\u27e91\n[wK]1\n. . .\n\u27e8W K\u27e91\n[wK]T\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f8.\n(2)\nThese vector features in the lookup table e\ufb00ectively learn features for words in the dictionary.\nNow, we want to use these trainable features as input to further layers of trainable feature\nextractors, that can represent groups of words and then \ufb01nally sentences.\n3.2 Extracting Higher Level Features from Word Feature Vectors\nFeature vectors produced by the lookup table layer need to be combined in subsequent layers\nof the neural network to produce a tag decision for each word in the sentence. Producing\ntags for each element in variable length sequences (here, a sentence is a sequence of words)\nis a standard problem in machine-learning. We consider two common approaches which tag\none word at the time: a window approach, and a (convolutional) sentence approach.\n3.2.1 Window Approach\nA window approach assumes the tag of a word depends mainly on its neighboring words.\nGiven a word to tag, we consider a \ufb01xed size ksz (a hyper-parameter) window of words\naround this word. Each word in the window is \ufb01rst passed through the lookup table layer (1)\nor (2), producing a matrix of word features of \ufb01xed size dwrd \u00d7 ksz. This matrix can be\nviewed as a dwrd ksz-dimensional vector by concatenating each column vector, which can be\nfed to further neural network layers. More formally, the word feature window given by the\n\ufb01rst network layer can be written as:\nf1\n\u03b8 = \u27e8LTW ([w]T\n1 )\u27e9dwin\nt\n=\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\n\u27e8W\u27e91\n[w]t\u2212dwin/2\n...\n\u27e8W\u27e91\n[w]t\n...\n\u27e8W\u27e91\n[w]t+dwin/2\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\n.\n(3)\nLinear Layer\nThe \ufb01xed size vector f1\n\u03b8 can be fed to one or several standard neural\nnetwork layers which perform a\ufb03ne transformations over their inputs:\nfl\n\u03b8 = W l fl\u22121\n\u03b8\n+ bl ,\n(4)\nwhere W l \u2208Rnl\nhu\u00d7nl\u22121\nhu and bl \u2208Rnl\nhu are the parameters to be trained. The hyper-parameter\nnl\nhu is usually called the number of hidden units of the lth layer.\nHardTanh Layer\nSeveral linear layers are often stacked, interleaved with a non-linearity\nfunction, to extract highly non-linear features. If no non-linearity is introduced, our network\n10\narXiv\nNatural Language Processing (almost) from Scratch\nwould be a simple linear model. We chose a \u201chard\u201d version of the hyperbolic tangent as non-\nlinearity. It has the advantage of being slightly cheaper to compute (compared to the exact\nhyperbolic tangent), while leaving the generalization performance unchanged (Collobert,\n2004). The corresponding layer l applies a HardTanh over its input vector:\nh\nfl\n\u03b8\ni\ni = HardTanh(\nh\nfl\u22121\n\u03b8\ni\ni) ,\nwhere\nHardTanh(x) =\n\uf8f1\n\uf8f2\n\uf8f3\n\u22121\nif x < \u22121\nx\nif \u22121 <= x <= 1\n1\nif x > 1\n.\n(5)\nScoring\nFinally, the output size of the last layer L of our network is equal to the number\nof possible tags for the task of interest. Each output can be then interpreted as a score of\nthe corresponding tag (given the input of the network), thanks to a carefully chosen cost\nfunction that we will describe later in this section.\nRemark 1 (Border E\ufb00ects) The feature window (3) is not well de\ufb01ned for words near\nthe beginning or the end of a sentence. To circumvent this problem, we augment the sentence\nwith a special \u201cPADDING\u201d word replicated dwin/2 times at the beginning and the end. This\nis akin to the use of \u201cstart\u201d and \u201cstop\u201d symbols in sequence models.\n3.2.2 Sentence Approach\nWe will see in the experimental section that a window approach performs well for most\nnatural language processing tasks we are interested in. However this approach fails with\nSRL, where the tag of a word depends on a verb (or, more correctly, predicate) chosen\nbeforehand in the sentence. If the verb falls outside the window, one cannot expect this word\nto be tagged correctly. In this particular case, tagging a word requires the consideration of\nthe whole sentence. When using neural networks, the natural choice to tackle this problem\nbecomes a convolutional approach, \ufb01rst introduced by Waibel et al. (1989) and also called\nTime Delay Neural Networks (TDNNs) in the literature.\nWe describe in detail our convolutional network below. It successively takes the complete\nsentence, passes it through the lookup table layer (1), produces local features around each\nword of the sentence thanks to convolutional layers, combines these feature into a global\nfeature vector which can then be fed to standard a\ufb03ne layers (4). In the semantic role\nlabeling case, this operation is performed for each word in the sentence, and for each verb\nin the sentence. It is thus necessary to encode in the network architecture which verb we\nare considering in the sentence, and which word we want to tag. For that purpose, each\nword at position i in the sentence is augmented with two features in the way described\nin Section 3.1.1. These features encode the relative distances i \u2212posv and i \u2212posw with\nrespect to the chosen verb at position posv, and the word to tag at position posw respectively.\nConvolutional Layer\nA convolutional layer can be seen as a generalization of a window\napproach: given a sequence represented by columns in a matrix fl\u22121\n\u03b8\n(in our lookup table\nmatrix (1)), a matrix-vector operation as in (4) is applied to each window of successive\n11\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\n 0\n 10\n 20\n 30\n 40\n 50\n 60\n 70\nThe\nproposed\nchanges\nalso\nwould\nallow\nexecutives\nto\nreport\nexercises\nof\noptions\nlater\nand\nless\noften\n.\n 0\n 10\n 20\n 30\n 40\n 50\n 60\n 70\nThe\nproposed\nchanges\nalso\nwould\nallow\nexecutives\nto\nreport\nexercises\nof\noptions\nlater\nand\nless\noften\n.\nFigure 3: Number of features chosen at each word position by the Max layer. We consider\na sentence approach network (Figure 2) trained for SRL. The number of \u201clocal\u201d features\noutput by the convolution layer is 300 per word. By applying a Max over the sentence,\nwe obtain 300 features for the whole sentence. It is interesting to see that the network\ncatches features mostly around the verb of interest (here \u201creport\u201d) and word of interest\n(\u201cproposed\u201d (left) or \u201coften\u201d (right)).\nwindows in the sequence. Using previous notations, the tth output column of the lth layer\ncan be computed as:\n\u27e8fl\n\u03b8\u27e91\nt = W l \u27e8fl\u22121\n\u03b8\n\u27e9dwin\nt\n+ bl\n\u2200t ,\n(6)\nwhere the weight matrix W l is the same across all windows t in the sequence. Convolutional\nlayers extract local features around each window of the given sequence. As for standard\na\ufb03ne layers (4), convolutional layers are often stacked to extract higher level features.\nIn this case, each layer must be followed by a non-linearity (5) or the network would be\nequivalent to one convolutional layer.\nMax Layer\nThe size of the output (6) depends on the number of words in the sentence\nfed to the network. Local feature vectors extracted by the convolutional layers have to be\ncombined to obtain a global feature vector, with a \ufb01xed size independent of the sentence\nlength, in order to apply subsequent standard a\ufb03ne layers.\nTraditional convolutional\nnetworks often apply an average (possibly weighted) or a max operation over the \u201ctime\u201d t\nof the sequence (6). (Here, \u201ctime\u201d just means the position in the sentence, this term stems\nfrom the use of convolutional layers in e.g. speech data where the sequence occurs over\ntime.) The average operation does not make much sense in our case, as in general most\nwords in the sentence do not have any in\ufb02uence on the semantic role of a given word to tag.\nInstead, we used a max approach, which forces the network to capture the most useful local\nfeatures produced by the convolutional layers (see Figure 3), for the task at hand. Given a\nmatrix fl\u22121\n\u03b8\noutput by a convolutional layer l \u22121, the Max layer l outputs a vector fl\n\u03b8:\nh\nfl\n\u03b8\ni\ni = max\nt\nh\nfl\u22121\n\u03b8\ni\ni, t\n1 \u2264i \u2264nl\u22121\nhu .\n(7)\nThis \ufb01xed sized global feature vector can be then fed to standard a\ufb03ne network layers (4).\nAs in the window approach, we then \ufb01nally produce one score per possible tag for the given\ntask.\n12\narXiv\nNatural Language Processing (almost) from Scratch\nScheme\nBegin\nInside\nEnd\nSingle\nOther\nIOB\nB-X\nI-X\nI-X\nB-X\nO\nIOE\nI-X\nI-X\nE-X\nE-X\nO\nIOBES\nB-X\nI-X\nE-X\nS-X\nO\nTable 3:\nVarious tagging schemes. Each word in a segment labeled \u201cX\u201d is tagged with a\npre\ufb01xed label, depending of the word position in the segment (begin, inside, end). Single\nword segment labeling is also output. Words not in a labeled segment are labeled \u201cO\u201d.\nVariants of the IOB (and IOE) scheme exist, where the pre\ufb01x B (or E) is replaced by I for\nall segments not contiguous with another segment having the same label \u201cX\u201d.\nRemark 2 The same border e\ufb00ects arise in the convolution operation (6) as in the window\napproach (3). We again work around this problem by padding the sentences with a special\nword.\n3.2.3 Tagging Schemes\nAs explained earlier, the network output layers compute scores for all the possible tags for\nthe task of interest. In the window approach, these tags apply to the word located in the\ncenter of the window. In the (convolutional) sentence approach, these tags apply to the\nword designated by additional markers in the network input.\nThe POS task indeed consists of marking the syntactic role of each word. However, the\nremaining three tasks associate labels with segments of a sentence. This is usually achieved\nby using special tagging schemes to identify the segment boundaries, as shown in Table 3.\nSeveral such schemes have been de\ufb01ned (IOB, IOE, IOBES, . . . ) without clear conclusion\nas to which scheme is better in general. State-of-the-art performance is sometimes obtained\nby combining classi\ufb01ers trained with di\ufb00erent tagging schemes (e.g. Kudo and Matsumoto,\n2001).\nThe ground truth for the NER, CHUNK, and SRL tasks is provided using two di\ufb00erent\ntagging schemes. In order to eliminate this additional source of variations, we have decided\nto use the most expressive IOBES tagging scheme for all tasks. For instance, in the CHUNK\ntask, we describe noun phrases using four di\ufb00erent tags. Tag \u201cS-NP\u201d is used to mark a noun\nphrase containing a single word. Otherwise tags \u201cB-NP\u201d, \u201cI-NP\u201d, and \u201cE-NP\u201d are used\nto mark the \ufb01rst, intermediate and last words of the noun phrase. An additional tag \u201cO\u201d\nmarks words that are not members of a chunk. During testing, these tags are then converted\nto the original IOB tagging scheme and fed to the standard performance evaluation scripts\nmentioned in Section 2.5.\n3.3 Training\nAll our neural networks are trained by maximizing a likelihood over the training data, using\nstochastic gradient ascent. If we denote \u03b8 to be all the trainable parameters of the network,\nwhich are trained using a training set T we want to maximize the following log-likelihood\n13\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\nwith respect to \u03b8:\n\u03b8 7\u2192\nX\n(x, y)\u2208T\nlog p(y | x, \u03b8) ,\n(8)\nwhere x corresponds to either a training word window or a sentence and its associated\nfeatures, and y represents the corresponding tag. The probability p(\u00b7) is computed from the\noutputs of the neural network. We will see in this section two ways of interpreting neural\nnetwork outputs as probabilities.\n3.3.1 Word-Level Log-Likelihood\nIn this approach, each word in a sentence is considered independently.\nGiven an input\nexample x, the network with parameters \u03b8 outputs a score [f\u03b8(x)]i, for the ith tag with\nrespect to the task of interest. To simplify the notation, we drop x from now, and we write\ninstead [f\u03b8]i. This score can be interpreted as a conditional tag probability p(i | x, \u03b8) by\napplying a softmax (Bridle, 1990) operation over all the tags:\np(i | x, \u03b8) =\ne[f\u03b8]i\nP\nj e[f\u03b8]j\n.\n(9)\nDe\ufb01ning the log-add operation as\nlogadd\ni\nzi = log(\nX\ni\nezi) ,\n(10)\nwe can express the log-likelihood for one training example (x, y) as follows:\nlog p(y | x, \u03b8) = [f\u03b8]y \u2212logadd\nj\n[f\u03b8]j .\n(11)\nWhile this training criterion, often referred as cross-entropy is widely used for classi\ufb01cation\nproblems, it might not be ideal in our case, where there is often a correlation between the\ntag of a word in a sentence and its neighboring tags. We now describe another common\napproach for neural networks which enforces dependencies between the predicted tags in a\nsentence.\n3.3.2 Sentence-Level Log-Likelihood\nIn tasks like chunking, NER or SRL we know that there are dependencies between word\ntags in a sentence: not only are tags organized in chunks, but some tags cannot follow\nother tags. Training using a word-level approach discards this kind of labeling information.\nWe consider a training scheme which takes into account the sentence structure: given the\npredictions of all tags by our network for all words in a sentence, and given a score for going\nfrom one tag to another tag, we want to encourage valid paths of tags during training, while\ndiscouraging all other paths.\nWe consider the matrix of scores f\u03b8([x]T\n1 ) output by the network. As before, we drop the\ninput [x]T\n1 for notation simpli\ufb01cation. The element [f\u03b8]i, t of the matrix is the score output\nby the network with parameters \u03b8, for the sentence [x]T\n1 and for the ith tag, at the tth word.\n14\narXiv\nNatural Language Processing (almost) from Scratch\nWe introduce a transition score [A]i, j for jumping from i to j tags in successive words, and\nan initial score [A]i, 0 for starting from the ith tag. As the transition scores are going to be\ntrained (as are all network parameters \u03b8), we de\ufb01ne \u02dc\u03b8 = \u03b8 \u222a{[A]i, j \u2200i, j}. The score of\na sentence [x]T\n1 along a path of tags [i]T\n1 is then given by the sum of transition scores and\nnetwork scores:\ns([x]T\n1 , [i]T\n1 , \u02dc\u03b8) =\nT\nX\nt=1\n\u0010\n[A][i]t\u22121, [i]t + [f\u03b8][i]t, t\n\u0011\n.\n(12)\nExactly as for the word-level likelihood (11), where we were normalizing with respect to all\ntags using a softmax (9), we normalize this score over all possible tag paths [j]T\n1 using a\nsoftmax, and we interpret the resulting ratio as a conditional tag path probability. Taking\nthe log, the conditional probability of the true path [y]T\n1 is therefore given by:\nlog p([y]T\n1 | [x]T\n1 , \u02dc\u03b8) = s([x]T\n1 , [y]T\n1 , \u02dc\u03b8) \u2212logadd\n\u2200[j]T\n1\ns([x]T\n1 , [j]T\n1 , \u02dc\u03b8) .\n(13)\nWhile the number of terms in the logadd operation (11) was equal to the number of tags, it\ngrows exponentially with the length of the sentence in (13). Fortunately, one can compute\nit in linear time with the following standard recursion over t, taking advantage of the\nassociativity and distributivity on the semi-ring9 (R \u222a{\u2212\u221e}, logadd, +):\n\u03b4t(k) \u2206=\nlogadd\n{[j]t\n1 \u2229[j]t=k}\ns([x]t\n1, [j]t\n1, \u02dc\u03b8)\n= logadd\ni\nlogadd\n{[j]t\n1 \u2229[j]t\u22121=i \u2229[j]t=k}\ns([x]t\n1, [j]t\u22121\n1\n, \u02dc\u03b8) + [A][j]t\u22121, k + [f\u03b8]k, t\n= logadd\ni\n\u03b4t\u22121(i) + [A]i, k + [f\u03b8]k, t\n= [f\u03b8]k, t + logadd\ni\n\u0010\n\u03b4t\u22121(i) + [A]i, k\n\u0011\n\u2200k ,\n(14)\nfollowed by the termination\nlogadd\n\u2200[j]T\n1\ns([x]T\n1 , [j]T\n1 , \u02dc\u03b8) = logadd\ni\n\u03b4T (i) .\n(15)\nWe can now maximize in (8) the log-likelihood (13) over all the training pairs ([x]T\n1 , [y]T\n1 ).\nAt inference time, given a sentence [x]T\n1 to tag, we have to \ufb01nd the best tag path which\nminimizes the sentence score (12). In other words, we must \ufb01nd\nargmax\n[j]T\n1\ns([x]T\n1 , [j]T\n1 , \u02dc\u03b8) .\n(16)\nThe Viterbi algorithm is the natural choice for this inference. It corresponds to performing\nthe recursion (14) and (15), but where the logadd is replaced by a max, and then tracking\nback the optimal path through each max.\n9. In other words, read logadd as \u2295and + as \u2297.\n15\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\nRemark 3 (Graph Transformer Networks) Our approach is a particular case of the\ndiscriminative forward training for graph transformer networks (GTNs) (Bottou et al., 1997;\nLe Cun et al., 1998). The log-likelihood (13) can be viewed as the di\ufb00erence between the\nforward score constrained over the valid paths (in our case there is only the labeled path)\nand the unconstrained forward score (15).\nRemark 4 (Conditional Random Fields) An important feature of equation (12) is the\nabsence of normalization.\nSumming the exponentials e [f\u03b8]i, t over all possible tags does\nnot necessarily yield the unity.\nIf this was the case, the scores could be viewed as the\nlogarithms of conditional transition probabilities, and our model would be subject to the\nlabel-bias problem that motivates Conditional Random Fields (CRFs) (La\ufb00erty et al., 2001).\nThe denormalized scores should instead be likened to the potential functions of a CRF.\nIn fact, a CRF maximizes the same likelihood (13) using a linear model instead of a\nnonlinear neural network. CRFs have been widely used in the NLP world, such as for POS\ntagging (La\ufb00erty et al., 2001), chunking (Sha and Pereira, 2003), NER (McCallum and Li,\n2003) or SRL (Cohn and Blunsom, 2005). Compared to such CRFs, we take advantage of\nthe nonlinear network to learn appropriate features for each task of interest.\n3.3.3 Stochastic Gradient\nMaximizing (8) with stochastic gradient (Bottou, 1991) is achieved by iteratively selecting\na random example (x, y) and making a gradient step:\n\u03b8 \u2190\u2212\u03b8 + \u03bb \u2202log p(y | x, \u03b8)\n\u2202\u03b8\n,\n(17)\nwhere \u03bb is a chosen learning rate. Our neural networks described in Figure 1 and Figure 2\nare a succession of layers that correspond to successive composition of functions. The neural\nnetwork is \ufb01nally composed with the word-level log-likelihood (11), or successively composed\nin the recursion (14) if using the sentence-level log-likelihood (13).\nThus, an analytical\nformulation of the derivative (17) can be computed, by applying the di\ufb00erentiation chain\nrule through the network, and through the word-level log-likelihood (11) or through the\nrecurrence (14).\nRemark 5 (Di\ufb00erentiability) Our cost functions are di\ufb00erentiable almost everywhere.\nNon-di\ufb00erentiable points arise because we use a \u201chard\u201d transfer function (5) and because\nwe use a \u201cmax\u201d layer (7) in the sentence approach network.\nFortunately, stochastic\ngradient still converges to a meaningful local minimum despite such minor di\ufb00erentiability\nproblems (Bottou, 1991, 1998). Stochastic gradient iterations that hit a non-di\ufb00erentiability\nare simply skipped.\nRemark 6 (Modular Approach) The well known \u201cback-propagation\u201d algorithm (LeCun,\n1985; Rumelhart et al., 1986) computes gradients using the chain rule. The chain rule can\nalso be used in a modular implementation.10 Our modules correspond to the boxes in Figure 1\nand Figure 2. Given derivatives with respect to its outputs, each module can independently\n10. See http://torch5.sf.net.\n16\narXiv\nNatural Language Processing (almost) from Scratch\nApproach\nPOS\nChunking\nNER\nSRL\n(PWA)\n(F1)\n(F1)\n(F1)\nBenchmark Systems\n97.24\n94.29\n89.31\n77.92\nNN+WLL\n96.31\n89.13\n79.53\n55.40\nNN+SLL\n96.37\n90.33\n81.47\n70.99\nTable 4:\nComparison in generalization performance of benchmark NLP systems with a\nvanilla neural network (NN) approach, on POS, chunking, NER and SRL tasks. We report\nresults with both the word-level log-likelihood (WLL) and the sentence-level log-likelihood\n(SLL). Generalization performance is reported in per-word accuracy rate (PWA) for POS\nand F1 score for other tasks. The NN results are behind the benchmark results, in Section 4\nwe show how to improve these models using unlabeled data.\nTask\nWindow/Conv. size\nWord dim.\nCaps dim.\nHidden units\nLearning rate\nPOS\ndwin = 5\nd0 = 50\nd1 = 5\nn1\nhu = 300\n\u03bb = 0.01\nCHUNK\n\u201d\n\u201d\n\u201d\n\u201d\n\u201d\nNER\n\u201d\n\u201d\n\u201d\n\u201d\n\u201d\nSRL\n\u201d\n\u201d\n\u201d\nn1\nhu = 300\nn2\nhu = 500\n\u201d\nTable 5:\nHyper-parameters of our networks. We report for each task the window size\n(or convolution size), word feature dimension, capital feature dimension, number of hidden\nunits and learning rate.\ncompute derivatives with respect to its inputs and with respect to its trainable parameters,\nas proposed by Bottou and Gallinari (1991). This allows us to easily build variants of our\nnetworks. For details about gradient computations, see Appendix A.\nRemark 7 (Tricks) Many tricks have been reported for training neural networks (LeCun\net al., 1998). Which ones to choose is often confusing. We employed only two of them: the\ninitialization and update of the parameters of each network layer were done according to\nthe \u201cfan-in\u201d of the layer, that is the number of inputs used to compute each output of this\nlayer (Plaut and Hinton, 1987). The fan-in for the lookup table (1), the lth linear layer (4)\nand the convolution layer (6) are respectively 1, nl\u22121\nhu and dwin\u00d7nl\u22121\nhu . The initial parameters\nof the network were drawn from a centered uniform distribution, with a variance equal to\nthe inverse of the square-root of the fan-in. The learning rate in (17) was divided by the\nfan-in, but stays \ufb01xed during the training.\n3.4 Supervised Benchmark Results\nFor POS, chunking and NER tasks, we report results with the window architecture described\nin Section 3.2.1. The SRL task was trained using the sentence approach (Section 3.2.2).\nResults are reported in Table 4, in per-word accuracy (PWA) for POS, and F1 score for all\n17\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\nfrance\njesus\nxbox\nreddish\nscratched\nmegabits\n454\n1973\n6909\n11724\n29869\n87025\npersuade\nthickets\ndecadent\nwidescreen\nodd\nppa\nfaw\nsavary\ndivo\nantica\nanchieta\nuddin\nblackstock\nsympathetic\nverus\nshabby\nemigration\nbiologically\ngiorgi\njfk\noxide\nawe\nmarking\nkayak\nshaheed\nkhwarazm\nurbina\nthud\nheuer\nmclarens\nrumelia\nstationery\nepos\noccupant\nsambhaji\ngladwin\nplanum\nilias\neglinton\nrevised\nworshippers\ncentrally\ngoa\u2019uld\ngsNUMBER\nedging\nleavened\nritsuko\nindonesia\ncollation\noperator\nfrg\npandionidae\nlifeless\nmoneo\nbacha\nw.j.\nnamsos\nshirt\nmahan\nnilgiris\nTable 6: Word embeddings in the word lookup table of a SRL neural network trained from\nscratch, with a dictionary of size 100, 000. For each column the queried word is followed by\nits index in the dictionary (higher means more rare) and its 10 nearest neighbors (arbitrary\nusing the Euclidean metric).\nthe other tasks. We performed experiments both with the word-level log-likelihood (WLL)\nand with the sentence-level log-likelihood (SLL). The hyper-parameters of our networks are\nreported in Table 5. All our networks were fed with two raw text features: lower case words,\nand a capital letter feature. We chose to consider lower case words to limit the number\nof words in the dictionary.\nHowever, to keep some upper case information lost by this\ntransformation, we added a \u201ccaps\u201d feature which tells if each word was in low caps, was all\ncaps, had \ufb01rst letter capital, or had one capital. Additionally, all occurrences of sequences\nof numbers within a word are replaced with the string \u201cNUMBER\u201d, so for example both the\nwords \u201cPS1\u201d and \u201cPS2\u201d would map to the single word \u201cpsNUMBER\u201d. We used a dictionary\ncontaining the 100,000 most common words in WSJ (case insensitive). Words outside this\ndictionary were replaced by a single special \u201cRARE\u201d word.\nResults show that neural networks \u201cout-of-the-box\u201d are behind baseline benchmark\nsystems.\nLooking at all submitted systems reported on each CoNLL challenge website\nshowed us our networks performance are nevertheless in the performance ballpark of existing\napproaches. The training criterion which takes into account the sentence structure (SLL)\nseems to boost the performance for the Chunking, NER and SRL tasks, with little advantage\nfor POS. This result is in line with existing NLP studies comparing sentence-level and word-\nlevel likelihoods (Liang et al., 2008). The capacity of our network architectures lies mainly\nin the word lookup table, which contains 50\u00d7100, 000 parameters to train. In the WSJ data,\n15% of the most common words appear about 90% of the time. Many words appear only\na few times. It is thus very di\ufb03cult to train properly their corresponding 50 dimensional\nfeature vectors in the lookup table. Ideally, we would like semantically similar words to be\nclose in the embedding space represented by the word lookup table: by continuity of the\nneural network function, tags produced on semantically similar sentences would be similar.\nWe show in Table 6 that it is not the case: neighboring words in the embedding space do\nnot seem to be semantically related.\n18\narXiv\nNatural Language Processing (almost) from Scratch\n 95.5\n 96\n 96.5\n 100\n 300\n 500\n 700\n 900\n(a) POS\n 90\n 90.5\n 91\n 91.5\n 100\n 300\n 500\n 700\n 900\n(b) CHUNK\n 85\n 85.5\n 86\n 86.5\n 100\n 300\n 500\n 700\n 900\n(c) NER\n 67\n 67.5\n 68\n 68.5\n 69\n 100\n 300\n 500\n 700\n 900\n(d) SRL\nFigure 4:\nF1 score on the validation set (y-axis) versus number of hidden units (x-axis)\nfor di\ufb00erent tasks trained with the sentence-level likelihood (SLL), as in Table 4. For SRL,\nwe vary in this graph only the number of hidden units in the second layer. The scale is\nadapted for each task. We show the standard deviation (obtained over 5 runs with di\ufb00erent\nrandom initialization), for the architecture we picked (300 hidden units for POS, CHUNK\nand NER, 500 for SRL).\nWe will focus in the next section on improving these word embeddings by leveraging\nunlabeled data. We will see our approach results in a performance boost for all tasks.\nRemark 8 (Architectures) In all our experiments in this paper, we tuned the hyper-\nparameters by trying only a few di\ufb00erent architectures by validation. In practice, the choice\nof hyperparameters such as the number of hidden units, provided they are large enough, has\na limited impact on the generalization performance. In Figure 4, we report the F1 score\nfor each task on the validation set, with respect to the number of hidden units. Considering\nthe variance related to the network initialization, we chose the smallest network achieving\n\u201creasonable\u201d performance, rather than picking the network achieving the top performance\nobtained on a single run.\nRemark 9 (Training Time) Training our network is quite computationally expensive.\nChunking and NER take about one hour to train, POS takes few hours, and SRL takes\nabout three days. Training could be faster with a larger learning rate, but we prefered to\nstick to a small one which works, rather than \ufb01nding the optimal one for speed. Second\norder methods (LeCun et al., 1998) could be another speedup technique.\n4. Lots of Unlabeled Data\nWe would like to obtain word embeddings carrying more syntactic and semantic information\nthan shown in Table 6. Since most of the trainable parameters of our system are associated\nwith the word embeddings, these poor results suggest that we should use considerably\nmore training data.\nFollowing our NLP from scratch philosophy, we now describe how\nto dramatically improve these embeddings using large unlabeled datasets. We then use\nthese improved embeddings to initialize the word lookup tables of the networks described\nin Section 3.4.\n19\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\n4.1 Datasets\nOur \ufb01rst English corpus is the entire English Wikipedia.11 We have removed all paragraphs\ncontaining non-roman characters and all MediaWiki markups.\nThe resulting text was\ntokenized using the Penn Treebank tokenizer script.12 The resulting dataset contains about\n631 million words.\nAs in our previous experiments, we use a dictionary containing the\n100,000 most common words in WSJ, with the same processing of capitals and numbers.\nAgain, words outside the dictionary were replaced by the special \u201cRARE\u201d word.\nOur second English corpus is composed by adding an extra 221 million words extracted\nfrom the Reuters RCV1 (Lewis et al., 2004) dataset.13 We also extended the dictionary to\n130, 000 words by adding the 30, 000 most common words in Reuters. This is useful in order\nto determine whether improvements can be achieved by further increasing the unlabeled\ndataset size.\n4.2 Ranking Criterion versus Entropy Criterion\nWe used these unlabeled datasets to train language models that compute scores describing\nthe acceptability of a piece of text. These language models are again large neural networks\nusing the window approach described in Section 3.2.1 and in Figure 1. As in the previous\nsection, most of the trainable parameters are located in the lookup tables.\nSimilar language models were already proposed by Bengio and Ducharme (2001) and\nSchwenk and Gauvain (2002). Their goal was to estimate the probability of a word given\nthe previous words in a sentence. Estimating conditional probabilities suggests a cross-\nentropy criterion similar to those described in Section 3.3.1. Because the dictionary size is\nlarge, computing the normalization term can be extremely demanding, and sophisticated\napproximations are required. More importantly for us, neither work leads to signi\ufb01cant\nword embeddings being reported.\nShannon (1951) has estimated the entropy of the English language between 0.6 and 1.3\nbits per character by asking human subjects to guess upcoming characters. Cover and King\n(1978) give a lower bound of 1.25 bits per character using a subtle gambling approach.\nMeanwhile, using a simple word trigram model, Brown et al. (1992b) reach 1.75 bits per\ncharacter. Teahan and Cleary (1996) obtain entropies as low as 1.46 bits per character\nusing variable length character n-grams. The human subjects rely of course on all their\nknowledge of the language and of the world. Can we learn the grammatical structure of the\nEnglish language and the nature of the world by leveraging the 0.2 bits per character that\nseparate human subjects from simple n-gram models? Since such tasks certainly require\nhigh capacity models, obtaining su\ufb03ciently small con\ufb01dence intervals on the test set entropy\nmay require prohibitively large training sets.14 The entropy criterion lacks dynamical range\nbecause its numerical value is largely determined by the most frequent phrases. In order to\nlearn syntax, rare but legal phrases are no less signi\ufb01cant than common phrases.\n11. Available at http://download.wikimedia.org. We took the November 2007 version.\n12. Available at http://www.cis.upenn.edu/~treebank/tokenization.html.\n13. Now available at http://trec.nist.gov/data/reuters/reuters.html.\n14. However, Klein and Manning (2002) describe a rare example of realistic unsupervised grammar induction\nusing a cross-entropy approach on binary-branching parsing trees, that is, by forcing the system to\ngenerate a hierarchical representation.\n20\narXiv\nNatural Language Processing (almost) from Scratch\nIt is therefore desirable to de\ufb01ne alternative training criteria. We propose here to use a\npairwise ranking approach (Cohen et al., 1998). We seek a network that computes a higher\nscore when given a legal phrase than when given an incorrect phrase. Because the ranking\nliterature often deals with information retrieval applications, many authors de\ufb01ne complex\nranking criteria that give more weight to the ordering of the best ranking instances (see\nBurges et al., 2007; Cl\u00b4emen\u00b8con and Vayatis, 2007). However, in our case, we do not want\nto emphasize the most common phrase over the rare but legal phrases. Therefore we use a\nsimple pairwise criterion.\nWe consider a window approach network, as described in Section 3.2.1 and Figure 1,\nwith parameters \u03b8 which outputs a score f\u03b8(x) given a window of text x = [w]dwin\n1\n. We\nminimize the ranking criterion with respect to \u03b8:\n\u03b8 7\u2192\nX\nx\u2208X\nX\nw\u2208D\nmax\nn\n0 , 1 \u2212f\u03b8(x) + f\u03b8(x(w))\no\n,\n(18)\nwhere X is the set of all possible text windows with dwin words coming from our training\ncorpus, D is the dictionary of words, and x(w) denotes the text window obtained by replacing\nthe central word of text window [w]dwin\n1\nby the word w.\nOkanohara and Tsujii (2007) use a related approach to avoiding the entropy criteria\nusing a binary classi\ufb01cation approach (correct/incorrect phrase). Their work focuses on\nusing a kernel classi\ufb01er, and not on learning word embeddings as we do here. Smith and\nEisner (2005) also propose a contrastive criterion which estimates the likelihood of the data\nconditioned to a \u201cnegative\u201d neighborhood.\nThey consider various data neighborhoods,\nincluding sentences of length dwin drawn from Ddwin. Their goal was however to perform\nwell on some tagging task on fully unsupervised data, rather than obtaining generic word\nembeddings useful for other tasks.\n4.3 Training Language Models\nThe language model network was trained by stochastic gradient minimization of the ranking\ncriterion (18), sampling a sentence-word pair (s, w) at each iteration.\nSince training times for such large scale systems are counted in weeks, it is not feasible\nto try many combinations of hyperparameters. It also makes sense to speed up the training\ntime by initializing new networks with the embeddings computed by earlier networks. In\nparticular, we found it expedient to train a succession of networks using increasingly large\ndictionaries, each network being initialized with the embeddings of the previous network.\nSuccessive dictionary sizes and switching times are chosen arbitrarily. (Bengio et al., 2009)\nprovides a more detailed discussion of this, the (as yet, poorly understood) \u201ccurriculum\u201d\nprocess.\nFor the purposes of model selection we use the process of \u201cbreeding\u201d.\nThe idea of\nbreeding is instead of trying a full grid search of possible values (which we did not have\nenough computing power for) to search for the parameters in anology to breeding biological\ncell lines. Within each line, child networks are initialized with the embeddings of their\nparents and trained on increasingly rich datasets with sometimes di\ufb00erent parameters. That\nis, suppose we have k processors, which is much less than the possible set of parameters\none would like to try. One chooses k initial parameter choices from the large set, and trains\n21\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\nthese on the k processors. In our case, possible parameters to adjust are: the learning rate\n\u03bb, the word embedding dimensions d, number of hidden units n1\nhu and input window size\ndwin. One then trains each of these models in an online fashion for a certain amount of\ntime (i.e. a few days), and then selects the best ones using the validation set error rate.\nThat is, breeding decisions were made on the basis of the value of the ranking criterion (18)\nestimated on a validation set composed of one million words held out from the Wikipedia\ncorpus. In the next breeding iteration, one then chooses another set of k parameters from\nthe possible grid of values that permute slightly the most successful candidates from the\nprevious round. As many of these parameter choices can share weights, we can e\ufb00ectively\ncontinue online training retaining some of the learning from the previous iterations.\nVery long training times make such strategies necessary for the foreseeable future: if we\nhad been given computers ten times faster, we probably would have found uses for datasets\nten times bigger. However, we should say we believe that although we ended up with a\nparticular choice of parameters, many other choices are almost equally as good, although\nperhaps there are others that are better as we could not do a full grid search.\nIn the following subsections, we report results obtained with two trained language\nmodels.\nThe results achieved by these two models are representative of those achieved\nby networks trained on the full corpuses.\n\u2022 Language model LM1 has a window size dwin = 11 and a hidden layer with n1\nhu = 100\nunits. The embedding layers were dimensioned like those of the supervised networks\n(Table 5).\nModel LM1 was trained on our \ufb01rst English corpus (Wikipedia) using\nsuccessive dictionaries composed of the 5000, 10, 000, 30, 000, 50, 000 and \ufb01nally\n100, 000 most common WSJ words. The total training time was about four weeks.\n\u2022 Language model LM2 has the same dimensions. It was initialized with the embeddings\nof LM1, and trained for an additional three weeks on our second English corpus\n(Wikipedia+Reuters) using a dictionary size of 130,000 words.\n4.4 Embeddings\nBoth networks produce much more appealing word embeddings than in Section 3.4. Table 7\nshows the ten nearest neighbors of a few randomly chosen query words for the LM1 model.\nThe syntactic and semantic properties of the neighbors are clearly related to those of the\nquery word.\nThese results are far more satisfactory than those reported in Table 7 for\nembeddings obtained using purely supervised training of the benchmark NLP tasks.\n4.5 Semi-supervised Benchmark Results\nSemi-supervised learning has been the object of much attention during the last few years (see\nChapelle et al., 2006).\nPrevious semi-supervised approaches for NLP can be roughly\ncategorized as follows:\n\u2022 Ad-hoc approaches such as (Rosenfeld and Feldman, 2007) for relation extraction.\n\u2022 Self-training approaches, such as (Ue\ufb03ng et al., 2007) for machine translation,\nand (McClosky et al., 2006) for parsing. These methods augment the labeled training\n22\narXiv\nNatural Language Processing (almost) from Scratch\nfrance\njesus\nxbox\nreddish\nscratched\nmegabits\n454\n1973\n6909\n11724\n29869\n87025\naustria\ngod\namiga\ngreenish\nnailed\noctets\nbelgium\nsati\nplaystation\nbluish\nsmashed\nmb/s\ngermany\nchrist\nmsx\npinkish\npunched\nbit/s\nitaly\nsatan\nipod\npurplish\npopped\nbaud\ngreece\nkali\nsega\nbrownish\ncrimped\ncarats\nsweden\nindra\npsNUMBER\ngreyish\nscraped\nkbit/s\nnorway\nvishnu\nhd\ngrayish\nscrewed\nmegahertz\neurope\nananda\ndreamcast\nwhitish\nsectioned\nmegapixels\nhungary\nparvati\ngeforce\nsilvery\nslashed\ngbit/s\nswitzerland\ngrace\ncapcom\nyellowish\nripped\namperes\nTable 7: Word embeddings in the word lookup table of the language model neural network\nLM1 trained with a dictionary of size 100, 000. For each column the queried word is followed\nby its index in the dictionary (higher means more rare) and its 10 nearest neighbors (using\nthe Euclidean metric, which was chosen arbitrarily).\nset with examples from the unlabeled dataset using the labels predicted by the model\nitself. Transductive approaches, such as (Joachims, 1999) for text classi\ufb01cation can\nbe viewed as a re\ufb01ned form of self-training.\n\u2022 Parameter sharing approaches such as (Ando and Zhang, 2005; Suzuki and Isozaki,\n2008).\nAndo and Zhang propose a multi-task approach where they jointly train\nmodels sharing certain parameters. They train POS and NER models together with a\nlanguage model (trained on 15 million words) consisting of predicting words given the\nsurrounding tokens. Suzuki and Isozaki embed a generative model (Hidden Markov\nModel) inside a CRF for POS, Chunking and NER. The generative model is trained\non one billion words. These approaches should be seen as a linear counterpart of our\nwork. Using multilayer models vastly expands the parameter sharing opportunities\n(see Section 5).\nOur approach simply consists of initializing the word lookup tables of the supervised\nnetworks with the embeddings computed by the language models. Supervised training is\nthen performed as in Section 3.4.\nIn particular the supervised training stage is free to\nmodify the lookup tables. This sequential approach is computationally convenient because\nit separates the lengthy training of the language models from the relatively fast training of\nthe supervised networks. Once the language models are trained, we can perform multiple\nexperiments on the supervised networks in a relatively short time. Note that our procedure\nis clearly linked to the (semi-supervised) deep learning procedures of (Hinton et al., 2006;\nBengio et al., 2007; Weston et al., 2008).\nTable 8 clearly shows that this simple initialization signi\ufb01cantly boosts the generalization\nperformance of the supervised networks for each task. It is worth mentioning the larger\nlanguage model led to even better performance.\nThis suggests that we could still take\nadvantage of even bigger unlabeled datasets.\n23\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\nApproach\nPOS\nCHUNK\nNER\nSRL\n(PWA)\n(F1)\n(F1)\n(F1)\nBenchmark Systems\n97.24\n94.29\n89.31\n77.92\nNN+WLL\n96.31\n89.13\n79.53\n55.40\nNN+SLL\n96.37\n90.33\n81.47\n70.99\nNN+WLL+LM1\n97.05\n91.91\n85.68\n58.18\nNN+SLL+LM1\n97.10\n93.65\n87.58\n73.84\nNN+WLL+LM2\n97.14\n92.04\n86.96\n58.34\nNN+SLL+LM2\n97.20\n93.63\n88.67\n74.15\nTable 8:\nComparison in generalization performance of benchmark NLP systems with our\n(NN) approach on POS, chunking, NER and SRL tasks. We report results with both the\nword-level log-likelihood (WLL) and the sentence-level log-likelihood (SLL). We report with\n(LMn) performance of the networks trained from the language model embeddings (Table 7).\nGeneralization performance is reported in per-word accuracy (PWA) for POS and F1 score\nfor other tasks.\n4.6 Ranking and Language\nThere is a large agreement in the NLP community that syntax is a necessary prerequisite for\nsemantic role labeling (Gildea and Palmer, 2002). This is why state-of-the-art semantic role\nlabeling systems thoroughly exploit multiple parse trees. The parsers themselves (Charniak,\n2000; Collins, 1999) contain considerable prior information about syntax (one can think of\nthis as a kind of informed pre-processing).\nOur system does not use such parse trees because we attempt to learn this information\nfrom the unlabeled data set. It is therefore legitimate to question whether our ranking\ncriterion (18) has the conceptual capability to capture such a rich hierarchical information.\nAt \ufb01rst glance, the ranking task appears unrelated to the induction of probabilistic\ngrammars that underly standard parsing algorithms. The lack of hierarchical representation\nseems a fatal \ufb02aw (Chomsky, 1956).\nHowever, ranking is closely related to an alternative description of the language\nstructure: operator grammars (Harris, 1968). Instead of directly studying the structure\nof a sentence, Harris de\ufb01nes an algebraic structure on the space of all sentences. Starting\nfrom a couple of elementary sentence forms, sentences are described by the successive\napplication of sentence transformation operators.\nThe sentence structure is revealed as\na side e\ufb00ect of the successive transformations. Sentence transformations can also have a\nsemantic interpretation.\nIn the spirit of structural linguistics, Harris describes procedures to discover sentence\ntransformation operators by leveraging the statistical regularities of the language. Such\nprocedures are obviously useful for machine learning approaches. In particular, he proposes\na test to decide whether two sentences forms are semantically related by a transformation\noperator. He \ufb01rst de\ufb01nes a ranking criterion (Harris, 1968, section 4.1):\n\u201cStarting for convenience with very short sentence forms, say ABC, we\nchoose a particular word choice for all the classes, say BqCq, except one, in\n24\narXiv\nNatural Language Processing (almost) from Scratch\nthis case A; for every pair of members Ai, Aj of that word class we ask how\nthe sentence formed with one of the members, i.e. AiBqCq compares as to\nacceptability with the sentence formed with the other member, i.e. AjBqCq.\u201d\nThese gradings are then used to compare sentence forms:\n\u201cIt now turns out that, given the graded n-tuples of words for a particular\nsentence form, we can \ufb01nd other sentences forms of the same word classes in\nwhich the same n-tuples of words produce the same grading of sentences.\u201d\nThis is an indication that these two sentence forms exploit common words with the same\nsyntactic function and possibly the same meaning. This observation forms the empirical\nbasis for the construction of operator grammars that describe real-world natural languages\nsuch as English.\nTherefore there are solid reasons to believe that the ranking criterion (18) has the\nconceptual potential to capture strong syntactic and semantic information. On the other\nhand, the structure of our language models is probably too restrictive for such goals, and\nour current approach only exploits the word embeddings discovered during training.\n5. Multi-Task Learning\nIt is generally accepted that features trained for one task can be useful for related tasks. This\nidea was already exploited in the previous section when certain language model features,\nnamely the word embeddings, were used to initialize the supervised networks.\nMulti-task learning (MTL) leverages this idea in a more systematic way. Models for\nall tasks of interests are jointly trained with an additional linkage between their trainable\nparameters in the hope of improving the generalization error. This linkage can take the form\nof a regularization term in the joint cost function that biases the models towards common\nrepresentations.\nA much simpler approach consists in having the models share certain\nparameters de\ufb01ned a priori. Multi-task learning has a long history in machine learning and\nneural networks. Caruana (1997) gives a good overview of these past e\ufb00orts.\n5.1 Joint Decoding versus Joint Training\nMultitask approaches do not necessarily involve joint training. For instance, modern speech\nrecognition systems use Bayes rule to combine the outputs of an acoustic model trained on\nspeech data and a language model trained on phonetic or textual corpora (Jelinek, 1976).\nThis joint decoding approach has been successfully applied to structurally more complex\nNLP tasks.\nSutton and McCallum (2005b) obtains improved results by combining the\npredictions of independently trained CRF models using a joint decoding process at test\ntime that requires more sophisticated probabilistic inference techniques.\nOn the other\nhand, Sutton and McCallum (2005a) obtain results somewhat below the state-of-the-art\nusing joint decoding for SRL and syntactic parsing. Musillo and Merlo (2006) also describe\na negative result at the same joint task.\nJoint decoding invariably works by considering additional probabilistic dependency\npaths between the models.\nTherefore it de\ufb01nes an implicit supermodel that describes\nall the tasks in the same probabilistic framework. Separately training a submodel only\n25\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\nApproach\nPOS\nCHUNK\nNER\nSRL\n(PWA)\n(F1)\n(F1)\n(F1)\nBenchmark Systems\n97.24\n94.29\n89.31\n\u2013\nWindow Approach\nNN+SLL+LM2\n97.20\n93.63\n88.67\n\u2013\nNN+SLL+LM2+MTL\n97.22\n94.10\n88.62\n\u2013\nSentence Approach\nNN+SLL+LM2\n97.12\n93.37\n88.78\n74.15\nNN+SLL+LM2+MTL\n97.22\n93.75\n88.27\n74.29\nTable 9:\nE\ufb00ect of multi-tasking on our neural architectures. We trained POS, CHUNK\nNER in a MTL way, both for the window and sentence network approaches. SRL was only\nincluded in the sentence approach joint training. As a baseline, we show previous results\nof our window approach system, as well as additional results for our sentence approach\nsystem, when trained separately on each task. Benchmark system performance is also given\nfor comparison.\nmakes sense when the training data blocks these additional dependency paths (in the sense\nof d-separation, Pearl, 1988).\nThis implies that, without joint training, the additional\ndependency paths cannot directly involve unobserved variables. Therefore, the natural idea\nof discovering common internal representations across tasks requires joint training.\nJoint training is relatively straightforward when the training sets for the individual\ntasks contain the same patterns with di\ufb00erent labels. It is then su\ufb03cient to train a model\nthat computes multiple outputs for each pattern (Suddarth and Holden, 1991).\nUsing\nthis scheme, Sutton et al. (2007) demonstrates improvements on POS tagging and noun-\nphrase chunking using jointly trained CRFs. However the joint labeling requirement is a\nlimitation because such data is not often available. Miller et al. (2000) achieves performance\nimprovements by jointly training NER, parsing, and relation extraction in a statistical\nparsing model. The joint labeling requirement problem was weakened using a predictor to\n\ufb01ll in the missing annotations.\nAndo and Zhang (2005) propose a setup that works around the joint labeling\nrequirements. They de\ufb01ne linear models of the form fi(x) = w\u22a4\ni \u03a6(x) + v\u22a4\ni \u0398\u03a8(x) where\nfi is the classi\ufb01er for the i-th task with parameters wi and vi. Notations \u03a6(x) and \u03a8(x)\nrepresent engineered features for the pattern x. Matrix \u0398 maps the \u03a8(x) features into a low\ndimensional subspace common across all tasks. Each task is trained using its own examples\nwithout a joint labeling requirement. The learning procedure alternates the optimization\nof wi and vi for each task, and the optimization of \u0398 to minimize the average loss for all\nexamples in all tasks. The authors also consider auxiliary unsupervised tasks for predicting\nsubstructures. They report excellent results on several tasks, including POS and NER.\n26\narXiv\nNatural Language Processing (almost) from Scratch\nLookup Table\nLinear\nLookup Table\nLinear\nHardTanh\nHardTanh\nLinear\nTask 1\nLinear\nTask 2\nM 2\n(t1) \u00d7 \u00b7\nM 2\n(t2) \u00d7 \u00b7\nLTW 1\n...\nLTW K\nM 1 \u00d7 \u00b7\nn1\nhu\nn1\nhu\nn2\nhu,(t1) = #tags\nn2\nhu,(t2) = #tags\nFigure 5: Example of multitasking with NN. Task 1 and Task 2 are two tasks trained with\nthe window approach architecture presented in Figure 1. Lookup tables as well as the \ufb01rst\nhidden layer are shared. The last layer is task speci\ufb01c. The principle is the same with more\nthan two tasks.\n5.2 Multi-Task Benchmark Results\nTable 9 reports results obtained by jointly trained models for the POS, CHUNK, NER and\nSRL tasks using the same setup as Section 4.5. We trained jointly POS, CHUNK and NER\nusing the window approach network. As we mentioned earlier, SRL can be trained only\nwith the sentence approach network, due to long-range dependencies related to the verb\npredicate. We thus also trained all four tasks using the sentence approach network. In\nboth cases, all models share the lookup table parameters (2). The parameters of the \ufb01rst\nlinear layers (4) were shared in the window approach case (see Figure 5), and the \ufb01rst the\nconvolution layer parameters (6) were shared in the sentence approach networks.\nFor the window approach, best results were obtained by enlarging the \ufb01rst hidden layer\nsize to n1\nhu = 500 (chosen by validation) in order to account for its shared responsibilities.\nWe used the same architecture than SRL for the sentence approach network. The word\nembedding dimension was kept constant d0 = 50 in order to reuse the language models\nof Section 4.5.\nTraining was achieved by minimizing the loss averaged across all tasks. This is easily\nachieved with stochastic gradient by alternatively picking examples for each task and\napplying (17) to all the parameters of the corresponding model, including the shared\nparameters. Note that this gives each task equal weight. Since each task uses the training\nsets described in Table 1, it is worth noticing that examples can come from quite di\ufb00erent\n27\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\nApproach\nPOS\nCHUNK\nNER\nSRL\n(PWA)\n(F1)\n(F1)\nBenchmark Systems\n97.24\n94.29\n89.31\n77.92\nNN+SLL+LM2\n97.20\n93.63\n88.67\n74.15\nNN+SLL+LM2+Su\ufb03x2\n97.29\n\u2013\n\u2013\n\u2013\nNN+SLL+LM2+Gazetteer\n\u2013\n\u2013\n89.59\n\u2013\nNN+SLL+LM2+POS\n\u2013\n94.32\n88.67\n\u2013\nNN+SLL+LM2+CHUNK\n\u2013\n\u2013\n\u2013\n74.72\nTable 10:\nComparison in generalization performance of benchmark NLP systems with\nour neural networks (NNs) using increasing task-speci\ufb01c engineering. We report results\nobtained with a network trained without the extra task-speci\ufb01c features (Section 5) and\nwith the extra task-speci\ufb01c features described in Section 6. The POS network was trained\nwith two character word su\ufb03xes; the NER network was trained using the small CoNLL\n2003 gazetteer; the CHUNK and NER networks were trained with additional POS features;\nand \ufb01nally, the SRL network was trained with additional CHUNK features.\ndatasets. The generalization performance for each task was measured using the traditional\ntesting data speci\ufb01ed in Table 1. Fortunately, none of the training and test sets overlap\nacross tasks.\nWhile we \ufb01nd worth mentioning that MTL can produce a single uni\ufb01ed architecture that\nperforms well for all these tasks, no (or only marginal) improvements were obtained with\nthis approach compared to training separate architectures per task (which still use semi-\nsupervised learning, which is somehow the most important MTL task). The next section\nshows we can leverage known correlations between tasks in more direct manner.\n6. The Temptation\nResults so far have been obtained by staying (almost15) true to our from scratch philosophy.\nWe have so far avoided specializing our architecture for any task, disregarding a lot of useful\na priori NLP knowledge. We have shown that, thanks to large unlabeled datasets, our\ngeneric neural networks can still achieve close to state-of-the-art performance by discovering\nuseful features. This section explores what happens when we increase the level of task-\nspeci\ufb01c engineering in our systems by incorporating some common techniques from the\nNLP literature. We often obtain further improvements. These \ufb01gures are useful to quantify\nhow far we went by leveraging large datasets instead of relying on a priori knowledge.\n6.1 Su\ufb03x Features\nWord su\ufb03xes in many western languages are strong predictors of the syntactic function\nof the word and therefore can bene\ufb01t the POS system. For instance, Ratnaparkhi (1996)\n15. We did some basic preprocessing of the raw input words as described in Section 3.4, hence the \u201calmost\u201d\nin the title of this article. A completely from scratch approach would presumably not know anything\nabout words at all and would work from letters only (or, taken to a further extreme, from speech or\noptical character recognition, as humans do).\n28\narXiv\nNatural Language Processing (almost) from Scratch\nuses inputs representing word su\ufb03xes and pre\ufb01xes up to four characters. We achieve this\nin the POS task by adding discrete word features (Section 3.1.1) representing the last two\ncharacters of every word. The size of the su\ufb03x dictionary was 455. This led to a small\nimprovement of the POS performance (Table 10, row NN+SLL+LM2+Su\ufb03x2). We also tried\nsu\ufb03xes obtained with the Porter (1980) stemmer and obtained the same performance as\nwhen using two character su\ufb03xes.\n6.2 Gazetteers\nState-of-the-art NER systems often use a large dictionary containing well known named\nentities (e.g. Florian et al., 2003).\nWe restricted ourselves to the gazetteer provided\nby the CoNLL challenge, containing 8, 000 locations, person names, organizations, and\nmiscellaneous entities. We trained a NER network with 4 additional word features indicating\n(feature \u201con\u201d or \u201co\ufb00\u201d) whether the word is found in the gazetteer under one of these four\ncategories. The gazetteer includes not only words, but also chunks of words. If a sentence\nchunk is found in the gazetteer, then all words in the chunk have their corresponding\ngazetteer feature turned to \u201con\u201d.\nThe resulting system displays a clear performance\nimprovement (Table 10, row NN+SLL+LM2+Gazetteer), slightly outperforming the baseline.\nA plausible explanation of this large boost over the network using only the language model\nis that gazeetters include word chunks, while we use only the word representation of our\nlanguage model. For example, \u201cunited\u201d and \u201cbicycle\u201d seen separately are likely to be non-\nentities, while \u201cunited bicycle\u201d might be an entity, but catching it would require higher\nlevel representations of our language model.\n6.3 Cascading\nWhen one considers related tasks, it is reasonable to assume that tags obtained for one task\ncan be useful for taking decisions in the other tasks. Conventional NLP systems often use\nfeatures obtained from the output of other preexisting NLP systems. For instance, Shen\nand Sarkar (2005) describe a chunking system that uses POS tags as input; Florian et al.\n(2003) describes a NER system whose inputs include POS and CHUNK tags, as well as\nthe output of two other NER classi\ufb01ers. State-of-the-art SRL systems exploit parse trees\n(Gildea and Palmer, 2002; Punyakanok et al., 2005), related to CHUNK tags, and built\nusing POS tags (Charniak, 2000; Collins, 1999).\nTable 10 reports results obtained for the CHUNK and NER tasks by adding discrete\nword features (Section 3.1.1) representing the POS tags. In order to facilitate comparisons,\ninstead of using the more accurate tags from our POS network, we use for each task the\nPOS tags provided by the corresponding CoNLL challenge. We also report results obtained\nfor the SRL task by adding word features representing the CHUNK tags (also provided by\nthe CoNLL challenge). We consistently obtain moderate improvements.\n6.4 Ensembles\nConstructing ensembles of classi\ufb01ers is a proven way to trade computational e\ufb03ciency for\ngeneralization performance (Bell et al., 2007). Therefore it is not surprising that many\nNLP systems achieve state-of-the-art performance by combining the outputs of multiple\n29\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\nApproach\nPOS\nCHUNK\nNER\n(PWA)\n(F1)\n(F1)\nBenchmark Systems\n97.24\n94.29\n89.31\nNN+SLL+LM2+POS\nworst\n97.29\n93.99\n89.35\nNN+SLL+LM2+POS\nmean\n97.31\n94.17\n89.65\nNN+SLL+LM2+POS\nbest\n97.35\n94.32\n89.86\nNN+SLL+LM2+POS\nvoting ensemble\n97.37\n94.34\n89.70\nNN+SLL+LM2+POS\njoined ensemble\n97.30\n94.35\n89.67\nTable 11: Comparison in generalization performance for POS, CHUNK and NER tasks of\nthe networks obtained using by combining ten training runs with di\ufb00erent initialization.\nclassi\ufb01ers. For instance, Kudo and Matsumoto (2001) use an ensemble of classi\ufb01ers trained\nwith di\ufb00erent tagging conventions (see Section 3.2.3). Winning a challenge is of course a\nlegitimate objective. Yet it is often di\ufb03cult to \ufb01gure out which ideas are most responsible\nfor the state-of-the-art performance of a large ensemble.\nBecause neural networks are nonconvex, training runs with di\ufb00erent initial parameters\nusually give di\ufb00erent solutions.\nTable 11 reports results obtained for the CHUNK and\nNER task after ten training runs with random initial parameters. Voting the ten network\noutputs on a per tag basis (\u201cvoting ensemble\u201d) leads to a small improvement over the average\nnetwork performance. We have also tried a more sophisticated ensemble approach: the ten\nnetwork output scores (before sentence-level likelihood) were combined with an additional\nlinear layer (4) and then fed to a new sentence-level likelihood (13). The parameters of\nthe combining layers were then trained on the existing training set, while keeping the ten\nnetworks \ufb01xed (\u201cjoined ensemble\u201d). This approach did not improve on simple voting.\nThese ensembles come of course at the expense of a ten fold increase of the running\ntime. On the other hand, multiple training times could be improved using smart sampling\nstrategies (Neal, 1996).\nWe can also observe that the performance variability among the ten networks is not very\nlarge. The local minima found by the training algorithm are usually good local minima,\nthanks to the oversized parameter space and to the noise induced by the stochastic gradient\nprocedure (LeCun et al., 1998). In order to reduce the variance in our experimental results,\nwe always use the same initial parameters for networks trained on the same task (except of\ncourse for the results reported in Table 11.)\n6.5 Parsing\nGildea and Palmer (2002) o\ufb00er several arguments suggesting that syntactic parsing is a\nnecessary prerequisite for the SRL task. The CoNLL 2005 SRL benchmark task provides\nparse trees computed using both the Charniak (2000) and Collins (1999) parsers. State-of-\nthe-art systems often exploit additional parse trees such as the k top ranking parse trees\n(Koomen et al., 2005; Haghighi et al., 2005).\nIn contrast our SRL networks so far do not use parse trees at all. They rely instead\non internal representations transferred from a language model trained with an objective\n30\narXiv\nNatural Language Processing (almost) from Scratch\nlevel 0\nS\nNP\nThe luxury auto maker\nb-np\ni-np\ni-np\ne-np\nNP\nlast year\nb-np e-np\nVP\nsold\ns-vp\nNP\n1,214 cars\nb-np e-np\nPP\nin\ns-pp\nNP\nthe U.S.\nb-np e-np\nlevel 1\nS\nThe luxury auto maker last year\no\no\no\no\no\no\nVP\nsold 1,214 cars\nb-vp i-vp e-vp\nPP\nin\nthe U.S.\nb-pp i-pp e-pp\nlevel 2\nS\nThe luxury auto maker last year\no\no\no\no\no\no\nVP\nsold 1,214 cars in\nthe U.S.\nb-vp i-vp i-vp i-vp i-vp e-vp\nFigure 6: Charniak parse tree for the sentence \u201cThe luxury auto maker last year sold 1,214\ncars in the U.S.\u201d. Level 0 is the original tree. Levels 1 to 4 are obtained by successively\ncollapsing terminal tree branches. For each level, words receive tags describing the segment\nassociated with the corresponding leaf. All words receive tag \u201cO\u201d at level 3 in this example.\nfunction that captures a lot of syntactic information (see Section 4.6).\nIt is therefore\nlegitimate to question whether this approach is an acceptable lightweight replacement for\nparse trees.\nWe answer this question by providing parse tree information as additional input features\nto our system. We have limited ourselves to the Charniak parse tree provided with the\nCoNLL 2005 data.\nConsidering that a node in a syntactic parse tree assigns a label\nto a segment of the parsed sentence, we propose a way to feed (partially) this labeled\nsegmentation to our network, through additional lookup tables.\nEach of these lookup\ntables encode labeled segments of each parse tree level (up to a certain depth). The labeled\nsegments are fed to the network following a IOBES tagging scheme (see Sections 3.2.3\nand 3.1.1). As there are 40 di\ufb00erent phrase labels in WSJ, each additional tree-related\nlookup tables has 161 entries (40 \u00d7 4 + 1) corresponding to the IBES segment tags, plus the\nextra O tag.\nWe call level 0 the information associated with the leaves of the original Charniak parse\ntree. The lookup table for level 0 encodes the corresponding IOBES phrase tags for each\nwords. We obtain levels 1 to 4 by repeatedly trimming the leaves as shown in Figure 6. We\nlabeled \u201cO\u201d words belonging to the root node \u201cS\u201d, or all words of the sentence if the root\nitself has been trimmed.\nExperiments were performed using the LM2 language model using the same network\narchitectures (see Table 5) and using additional lookup tables of dimension 5 for each\n31\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\nApproach\nSRL\n(valid)\n(test)\nBenchmark System (six parse trees)\n77.35\n77.92\nBenchmark System (top Charniak parse tree only)\n74.76\n\u2013\nNN+SLL+LM2\n72.29\n74.15\nNN+SLL+LM2+Charniak (level 0 only)\n74.44\n75.65\nNN+SLL+LM2+Charniak (levels 0 & 1)\n74.50\n75.81\nNN+SLL+LM2+Charniak (levels 0 to 2)\n75.09\n76.05\nNN+SLL+LM2+Charniak (levels 0 to 3)\n75.12\n75.89\nNN+SLL+LM2+Charniak (levels 0 to 4)\n75.42\n76.06\nNN+SLL+LM2+CHUNK\n\u2013\n74.72\nNN+SLL+LM2+PT0\n\u2013\n75.49\nTable 12:\nGeneralization performance on the SRL task of our NN architecture compared\nwith the benchmark system. We show performance of our system fed with di\ufb00erent levels\nof depth of the Charniak parse tree. We report previous results of our architecture with no\nparse tree as a baseline. Koomen et al. (2005) report test and validation performance using\nsix parse trees, as well as validation performance using only the top Charniak parse tree.\nFor comparison purposes, we hence also report validation performance. Finally, we report\nour performance with the CHUNK feature, and compare it against a level 0 feature PT0\nobtained by our network.\nparse tree level. Table 12 reports the performance improvements obtained by providing\nincreasing levels of parse tree information. Level 0 alone increases the F1 score by almost\n1.5%. Additional levels yield diminishing returns. The top performance reaches 76.06% F1\nscore. This is not too far from the state-of-the-art system which we note uses six parse\ntrees instead of one. Koomen et al. (2005) also report a 74.76% F1 score on the validation\nset using only the Charniak parse tree. Using the \ufb01rst three parse tree levels, we reach this\nperformance on the validation set.\nWe also reported in Table 12 our previous performance obtained with the CHUNK\nfeature (see Table 10). It is surprising to observe that adding chunking features into the\nsemantic role labeling network performs signi\ufb01cantly worse than adding features describing\nthe level 0 of the Charniak parse tree (Table 12). Indeed, if we ignore the label pre\ufb01xes\n\u201cBIES\u201d de\ufb01ning the segmentation, the parse tree leaves (at level 0) and the chunking\nhave identical labeling. However, the parse trees identify leaf sentence segments that are\noften smaller than those identi\ufb01ed by the chunking tags, as shown by Hollingshead et al.\n(2005).16\nInstead of relying on Charniak parser, we chose to train a second chunking\nnetwork to identify the segments delimited by the leaves of the Penn Treebank parse trees\n(level 0). Our network achieved 92.25% F1 score on this task (we call it PT0), while we\n16. As in (Hollingshead et al., 2005), consider the sentence and chunk labels \u201c(NP They) (VP are starting\nto buy) (NP growth stocks)\u201d. The parse tree can be written as \u201c(S (NP They) (VP are (VP starting (S\n(VP to (VP buy (NP growth stocks)))))))\u201d. The tree leaves segmentation is thus given by \u201c(NP They)\n(VP are) (VP starting) (VP to) (VP buy) (NP growth stocks)\u201d.\n32\narXiv\nNatural Language Processing (almost) from Scratch\nevaluated Charniak performance as 91.94% on the same task. As shown in Table 12, feeding\nour own PT0 prediction into the SRL system gives similar performance to using Charniak\npredictions, and is consistently better than the CHUNK feature.\n6.6 Word Representations\nIn Section 4, we adapted our neural network architecture for training a language model task.\nBy leveraging a large amount of unlabeled text data, we induced word embeddings which\nwere shown to boost generalization performance on all tasks. While we chose to stick with\none single architecture, other ways to induce word representations exist. Mnih and Hinton\n(2007) proposed a related language model approach inspired from Restricted Boltzmann\nMachines. However, word representations are perhaps more commonly infered from n-gram\nlanguage modelling rather than smoothed language models. One popular approach is the\nBrown clustering algorithm (Brown et al., 1992a), which builds hierachical word clusters\nby maximizing the bigram\u2019s mutual information.\nThe induced word representation has\nbeen used with success in a wide variety of NLP tasks, including POS (Sch\u00a8utze, 1995),\nNER (Miller et al., 2004; Ratinov and Roth, 2009), or parsing (Koo et al., 2008). Other\nrelated approaches exist, like phrase clustering (Lin and Wu, 2009) which has been shown\nto work well for NER. Finally, Huang and Yates (2009) have recently proposed a smoothed\nlanguage modelling approach based on a Hidden Markov Model, with success on POS and\nChunking tasks.\nWhile a comparison of all these word representations is beyond the scope of this paper,\nit is rather fair to question the quality of our word embeddings compared to a popular NLP\napproach. In this section, we report a comparison of our word embeddings against Brown\nclusters, when used as features into our neural network architecture. We report as baseline\nprevious results where our word embeddings are \ufb01ne-tuned for each task. We also report\nperformance when our embeddings are kept \ufb01xed during task-speci\ufb01c training. Since convex\nmachine learning algorithms are common practice in NLP, we \ufb01nally report performances\nfor the convex version of our architecture.\nFor the convex experiments, we considered the linear version of our neural networks\n(instead of having several linear layers interleaved with a non-linearity). While we always\npicked the sentence approach for SRL, we had to consider the window approach in this\nparticular convex setup, as the sentence approach network (see Figure 2) includes a Max\nlayer.\nHaving only one linear layer in our neural network is not enough to make our\narchitecture convex: all lookup-tables (for each discrete feature) must also be \ufb01xed. The\nword-lookup table is simply \ufb01xed to the embeddings obtained from our language model\nLM2. All other discrete feature lookup-tables (caps, POS, Brown Clusters...) are \ufb01xed to a\nstandard sparse representation. Using the notation introduced in Section 3.1.1, if LTW k is\nthe lookup-table of the kth discrete feature, we have W k \u2208R|Dk|\u00d7|Dk| and the representation\nof the discrete input w is obtained with:\nLTW k(w) = \u27e8W k\u27e91\nw =\n\u0012\n0, \u00b7 \u00b7 \u00b7 0,\n1\nat index w, 0, \u00b7 \u00b7 \u00b7 0\n\u0013T\n.\n(19)\nTraining our architecture in this convex setup with the sentence-level likelihood (13)\ncorresponds to training a CRF. In that respect, these convex experiments show the\nperformance of our word embeddings in a classical NLP framework.\n33\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\nApproach\nPOS\nCHUNK\nNER\nSRL\n(PWA)\n(F1)\n(F1)\n(F1)\nNon-convex Approach\nLM2 (non-linear NN)\n97.29\n94.32\n89.59\n76.06\nLM2 (non-linear NN, \ufb01xed embeddings)\n97.10\n94.45\n88.79\n72.24\nBrown Clusters (non-linear NN, 130K words)\n96.92\n94.35\n87.15\n72.09\nBrown Clusters (non-linear NN, all words)\n96.81\n94.21\n86.68\n71.44\nConvex Approach\nLM2 (linear NN, \ufb01xed embeddings)\n96.69\n93.51\n86.64\n59.11\nBrown Clusters (linear NN, 130K words)\n96.56\n94.20\n86.46\n51.54\nBrown Clusters (linear NN, all words)\n96.28\n94.22\n86.63\n56.42\nTable 13:\nGeneralization performance of our neural network architecture trained with\nour language model (LM2) word embeddings, and with the word representations derived\nfrom the Brown Clusters. As before, all networks are fed with a capitalization feature.\nAdditionally, POS is using a word su\ufb03x of size 2 feature, CHUNK is fed with POS, NER\nuses the CoNLL 2003 gazetteer, and SRL is fed with levels 1\u20135 of the Charniak parse tree,\nas well as a verb position feature. We report performance with both convex and non-convex\narchitectures (300 hidden units for all tasks, with an additional 500 hidden units layer for\nSRL). We also provide results for Brown Clusters induced with a 130K word dictionary, as\nwell as Brown Clusters induced with all words of the given tasks.\nFollowing the Ratinov and Roth (2009) and Koo et al. (2008) setups, we generated 1, 000\nBrown clusters using the implementation17 from Liang (2005). To make the comparison\nfair, the clusters were \ufb01rst induced on the concatenation of Wikipedia and Reuters datasets,\nas we did in Section 4 for training our largest language model LM2, using a 130K word\ndictionary. This dictionary covers about 99% of the words in the training set of each task.\nTo cover the last 1%, we augmented the dictionary with the missing words (reaching about\n140K words) and induced Brown Clusters using the concatenation of WSJ, Wikipedia, and\nReuters.\nThe Brown clustering approach is hierarchical and generates a binary tree of clusters.\nEach word in the vocabulary is assigned to a node in the tree. Features are extracted from\nthis tree by considering the path from the root to the node containing the word of interest.\nFollowing Ratinov & Roth, we picked as features the path pre\ufb01xes of size 4, 6, 10 and 20. In\nthe non-convex experiments, we fed these four Brown Cluster features to our architecture\nusing four di\ufb00erent lookup tables, replacing our word lookup table. The size of the lookup\ntables was chosen to be 12 by validation. In the convex case, we used the classical sparse\nrepresentation (19), as for any other discrete feature.\nWe \ufb01rst report in Table 13 generalization performance of our best non-convex networks\ntrained with our LM2 language model and with Brown Cluster features. Our embeddings\nperform at least as well as Brown Clusters. Results are more mitigated in a convex setup.\nFor most task, going non-convex is better for both word representation types. In general,\n17. Available at http://www.eecs.berkeley.edu/~pliang/software.\n34\narXiv\nNatural Language Processing (almost) from Scratch\nTask\nFeatures\nPOS\nSu\ufb03x of size 2\nCHUNK\nPOS\nNER\nCoNLL 2003 gazetteer\nPT0\nPOS\nSRL\nPT0, verb position\nTable 14:\nFeatures used by SENNA implementation, for each task. In addition, all tasks\nuse \u201clow caps word\u201d and \u201ccaps\u201d features.\n\u201c\ufb01ne-tuning\u201d our embeddings for each task also gives an extra boost. Finally, using a better\nword coverage with Brown Clusters (\u201call words\u201d instead of \u201c130K words\u201d in Table 13) did\nnot help.\nMore complex features could be possibly combined instead of using a non-linear\nmodel. For instance, Turian et al. (2010) performed a comparison of Brown Clusters and\nembeddings trained in the same spirit as ours18, with additional features combining labels\nand tokens. We believe this type of comparison should be taken with care, as combining\na given feature with di\ufb00erent word representations might not have the same e\ufb00ect on each\nword representation.\n6.7 Engineering a Sweet Spot\nWe implemented a standalone version of our architecture, written in the C language.\nWe gave the name \u201cSENNA\u201d (Semantic/syntactic Extraction using a Neural Network\nArchitecture) to the resulting system. The parameters of each architecture are the ones\ndescribed in Table 5.\nAll the networks were trained separately on each task using the\nsentence-level likelihood (SLL). The word embeddings were initialized to LM2 embeddings,\nand then \ufb01ne-tuned for each task. We summarize features used by our implementation\nin Table 14, and we report performance achieved on each task in Table 15.\nThe runtime\nversion19 contains about 2500 lines of C code, runs in less than 150MB of memory, and needs\nless than a millisecond per word to compute all the tags. Table 16 compares the tagging\nspeeds for our system and for the few available state-of-the-art systems: the Toutanova et al.\n(2003) POS tagger20, the Shen et al. (2007) POS tagger21 and the Koomen et al. (2005) SRL\nsystem.22 All programs were run on a single 3GHz Intel core. The POS taggers were run\nwith Sun Java 1.6 with a large enough memory allocation to reach their top tagging speed.\n18. However they did not reach our embedding performance.\nThere are several di\ufb00erences in how they\ntrained their models that might explain this. Firstly, they may have experienced di\ufb03culties because\nthey train 50-dimensional embeddings for 269K distinct words using a comparatively small training set\n(RCV1, 37M words), unlikely to contain enough instances of the rare words. Secondly, they predict the\ncorrectness of the \ufb01nal word of each window instead of the center word (Turian et al., 2010), e\ufb00ectively\nrestricting the model to unidirectional prediction. Finally, they do not \ufb01ne tune their embeddings after\nunsupervised training.\n19. Available at http://ml.nec-labs.com/senna.\n20. Available at http://nlp.stanford.edu/software/tagger.shtml. We picked the 3.0 version (May 2010).\n21. Available at http://www.cis.upenn.edu/~xtag/spinal.\n22. Available at http://l2r.cs.uiuc.edu/~cogcomp/asoftware.php?skey=SRL.\n35\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\nTask\nBenchmark\nSENNA\nPart of Speech (POS)\n(Accuracy)\n97.24 %\n97.29 %\nChunking (CHUNK)\n(F1)\n94.29 %\n94.32 %\nNamed Entity Recognition (NER)\n(F1)\n89.31 %\n89.59 %\nParse Tree level 0 (PT0)\n(F1)\n91.94 %\n92.25 %\nSemantic Role Labeling (SRL)\n(F1)\n77.92 %\n75.49 %\nTable 15: Performance of the engineered sweet spot (SENNA) on various tagging tasks. The\nPT0 task replicates the sentence segmentation of the parse tree leaves. The corresponding\nbenchmark score measures the quality of the Charniak parse tree leaves relative to the Penn\nTreebank gold parse trees.\nPOS System\nRAM (MB)\nTime (s)\nToutanova et al. (2003)\n800\n64\nShen et al. (2007)\n2200\n833\nSENNA\n32\n4\nSRL System\nRAM (MB)\nTime (s)\nKoomen et al. (2005)\n3400\n6253\nSENNA\n124\n51\nTable 16:\nRuntime speed and memory consumption comparison between state-of-the-art\nsystems and our approach (SENNA). We give the runtime in seconds for running both\nthe POS and SRL taggers on their respective testing sets. Memory usage is reported in\nmegabytes.\nThe beam size of the Shen tagger was set to 3 as recommended in the paper. Regardless\nof implementation di\ufb00erences, it is clear that our neural networks run considerably faster.\nThey also require much less memory. Our POS and SRL tagger runs in 32MB and 120MB\nof RAM respectively. The Shen and Toutanova taggers slow down signi\ufb01cantly when the\nJava machine is given less than 2.2GB and 800MB of RAM respectively, while the Koomen\ntagger requires at least 3GB of RAM.\nWe believe that a number of reasons explain the speed advantage of our system. First,\nour system only uses rather simple input features and therefore avoids the nonnegligible\ncomputation time associated with complex handcrafted features. Secondly, most network\ncomputations are dense matrix-vector operations. In contrast, systems that rely on a great\nnumber of sparse features experience memory latencies when traversing the sparse data\nstructures. Finally, our compact implementation is self-contained. Since it does not rely on\nthe outputs of disparate NLP system, it does not su\ufb00er from communication latency issues.\n7. Critical Discussion\nAlthough we believe that this contribution represents a step towards the \u201cNLP from scratch\u201d\nobjective, we are keenly aware that both our goal and our means can be criticized.\n36\narXiv\nNatural Language Processing (almost) from Scratch\nThe main criticism of our goal can be summarized as follows. Over the years, the NLP\ncommunity has developed a considerable expertise in engineering e\ufb00ective NLP features.\nWhy should they forget this painfully acquired expertise and instead painfully acquire\nthe skills required to train large neural networks? As mentioned in our introduction, we\nobserve that no single NLP task really covers the goals of NLP. Therefore we believe that\ntask-speci\ufb01c engineering (i.e. that does not generalize to other tasks) is not desirable. But\nwe also recognize how much our neural networks owe to previous NLP task-speci\ufb01c research.\nThe main criticism of our means is easier to address. Why did we choose to rely on a\ntwenty year old technology, namely multilayer neural networks? We were simply attracted\nby their ability to discover hidden representations using a stochastic learning algorithm\nthat scales linearly with the number of examples. Most of the neural network technology\nnecessary for our work has been described ten years ago (e.g. Le Cun et al., 1998). However,\nif we had decided ten years ago to train the language model network LM2 using a vintage\ncomputer, training would only be nearing completion today. Training algorithms that scale\nlinearly are most able to bene\ufb01t from such tremendous progress in computer hardware.\n8. Conclusion\nWe have presented a multilayer neural network architecture that can handle a number of\nNLP tasks with both speed and accuracy. The design of this system was determined by\nour desire to avoid task-speci\ufb01c engineering as much as possible. Instead we rely on large\nunlabeled datasets and let the training algorithm discover internal representations that\nprove useful for all the tasks of interest. Using this strong basis, we have engineered a fast\nand e\ufb03cient \u201call purpose\u201d NLP tagger that we hope will prove useful to the community.\nAcknowledgments\nWe acknowledge the persistent support of NEC for this research e\ufb00ort. We thank Yoshua\nBengio, Samy Bengio, Eric Cosatto, Vincent Etter, Hans-Peter Graf, Ralph Grishman, and\nVladimir Vapnik for their useful feedback and comments.\n37\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\nAppendix A. Neural Network Gradients\nWe consider a neural network f\u03b8(\u00b7), with parameters \u03b8. We maximize the likelihood (8), or\nminimize ranking criterion (18), with respect to the parameters \u03b8, using stochastic gradient.\nBy negating the likelihood, we now assume it all corresponds to minimize a cost C(f\u03b8(\u00b7)),\nwith respect to \u03b8.\nFollowing the classical \u201cback-propagation\u201d derivations (LeCun, 1985; Rumelhart et al.,\n1986) and the modular approach shown in (Bottou, 1991), any feed-forward neural network\nwith L layers, like the ones shown in Figure 1 and Figure 2, can be seen as a composition\nof functions fl\n\u03b8(\u00b7), corresponding to each layer l:\nf\u03b8(\u00b7) = fL\n\u03b8 (fL\u22121\n\u03b8\n(. . . f1\n\u03b8 (\u00b7) . . .))\nPartionning the parameters of the network with respect to each layers 1 \u2264l \u2264L, we write:\n\u03b8 = (\u03b81, . . . , \u03b8l, . . . , \u03b8L) .\nWe are now interested in computing the gradients of the cost with respect to each \u03b8l.\nApplying the chain rule (generalized to vectors) we obtain the classical backpropagation\nrecursion:\n\u2202C\n\u2202\u03b8l\n=\n\u2202fl\n\u03b8\n\u2202\u03b8l\n\u2202C\n\u2202fl\n\u03b8\n(20)\n\u2202C\n\u2202fl\u22121\n\u03b8\n=\n\u2202fl\n\u03b8\n\u2202fl\u22121\n\u03b8\n\u2202C\n\u2202fl\n\u03b8\n.\n(21)\nIn other words, we \ufb01rst initialize the recursion by computing the gradient of the cost with\nrespect to the last layer output \u2202C/\u2202fL\n\u03b8 . Then each layer l computes the gradient respect\nto its own parameters with (20), given the gradient coming from its output \u2202C/\u2202fl\n\u03b8. To\nperform the backpropagation, it also computes the gradient with respect to its own inputs,\nas shown in (21). We now derive the gradients for each layer we used in this paper.\nLookup Table Layer\nGiven a matrix of parameters \u03b81 = W 1 and word (or discrete\nfeature) indices [w]T\n1 , the layer outputs the matrix:\nfl\n\u03b8([w]T\nl ) =\n\u0010\n\u27e8W\u27e91\n[w]1\n\u27e8W\u27e91\n[w]2\n. . .\n\u27e8W\u27e91\n[w]T\n\u0011\n.\nThe gradients of the weights \u27e8W\u27e91\ni are given by:\n\u2202C\n\u2202\u27e8W\u27e91\ni\n=\nX\n{1\u2264t\u2264T / [w]t=i}\n\u27e8\u2202C\n\u2202fl\n\u03b8\n\u27e91\ni\nThis sum equals zero if the index i in the lookup table does not corresponds to a word in\nthe sequence. In this case, the ith column of W does not need to be updated. As a Lookup\nTable Layer is always the \ufb01rst layer, we do not need to compute its gradients with respect\nto the inputs.\n38\narXiv\nNatural Language Processing (almost) from Scratch\nLinear Layer\nGiven parameters \u03b8l = (W l, bl), and an input vector fl\u22121\n\u03b8\nthe output is\ngiven by:\nfl\n\u03b8 = W lfl\u22121\n\u03b8\n+ bl .\n(22)\nThe gradients with respect to the parameters are then obtained with:\n\u2202C\n\u2202W l =\n\u0014 \u2202C\n\u2202fl\n\u03b8\n\u0015 h\nfl\u22121\n\u03b8\niT\nand \u2202C\n\u2202bl = \u2202C\n\u2202fl\n\u03b8\n,\n(23)\nand the gradients with respect to the inputs are computed with:\n\u2202C\n\u2202fl\u22121\n\u03b8\n=\nh\nW liT \u2202C\n\u2202fl\n\u03b8\n.\n(24)\nConvolution Layer\nGiven a input matrix fl\u22121\n\u03b8\n, a Convolution Layer fl\n\u03b8(\u00b7) applies a\nLinear Layer operation (22) successively on each window \u27e8fl\u22121\n\u03b8\n\u27e9dwin\nt\n(1 \u2264t \u2264T) of size\ndwin.\nUsing (23), the gradients of the parameters are thus given by summing over all\nwindows:\n\u2202C\n\u2202W l =\nT\nX\nt=1\n\u0014\n\u27e8\u2202C\n\u2202fl\n\u03b8\n\u27e91\nt\n\u0015 h\n\u27e8fl\u22121\n\u03b8\n\u27e9dwin\nt\niT\nand \u2202C\n\u2202bl =\nT\nX\nt=1\n\u27e8\u2202C\n\u2202fl\n\u03b8\n\u27e91\nt .\nAfter initializing the input gradients \u2202C/\u2202fl\u22121\n\u03b8\nto zero, we iterate (24) over all windows for\n1 \u2264t \u2264T, leading the accumulation23:\n\u27e8\u2202C\n\u2202fl\u22121\n\u03b8\n\u27e9dwin\nt\n+=\nh\nW liT\n\u27e8\u2202C\n\u2202fl\n\u03b8\n\u27e91\nt .\nMax Layer\nGiven a matrix fl\u22121\n\u03b8\n, the Max Layer computes\nh\nfl\n\u03b8\ni\ni = max\nt\nh\n\u27e8fl\u22121\n\u03b8\n\u27e91\nt\ni\ni and ai = argmax\nt\nh\n\u27e8fl\u22121\n\u03b8\n\u27e91\nt\ni\ni \u2200i ,\nwhere ai stores the index of the largest value. We only need to compute the gradient with\nrespect to the inputs, as this layer has no parameters. The gradient is given by\n\"\n\u27e8\u2202C\n\u2202fl\u22121\n\u03b8\n\u27e91\nt\n#\ni\n=\n( h\n\u27e8\u2202C\n\u2202fl\n\u03b8 \u27e91\nt\ni\ni\nif t = ai\n0\notherwise\n.\nHardTanh Layer\nGiven a vector fl\u22121\n\u03b8\n, and the de\ufb01nition of the HardTanh (5) we get\n\"\n\u2202C\n\u2202fl\u22121\n\u03b8\n#\ni\n=\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n0\nif\nh\nfl\u22121\n\u03b8\ni\ni < \u22121\nh\n\u2202C\n\u2202fl\n\u03b8\ni\ni\nif \u22121 <=\nh\nfl\u22121\n\u03b8\ni\ni <= 1\n0\nif\nh\nfl\u22121\n\u03b8\ni\ni > 1\n,\nif we ignore non-di\ufb00erentiability points.\n23. We denote \u201c+=\u201d any accumulation operation.\n39\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\nWord-Level Log-Likelihood\nThe network outputs a score [f\u03b8]i for each tag indexed by\ni. Following (11), if y is the true tag for a given example, the stochastic score to minimize\ncan be written as\nC(f\u03b8) = logadd\nj\n[f\u03b8]j \u2212[f\u03b8]y\nConsidering the de\ufb01nition of the logadd (10), the gradient with respect to f\u03b8 is given by\n\u2202C\n\u2202[f\u03b8]i\n=\ne[f\u03b8]i\nP\nk e[f\u03b8]k \u22121i=y\n\u2200i.\nSentence-Level Log-Likelihood\nThe network outputs a matrix where each element\n[f\u03b8]i, t gives a score for tag i at word t. Given a tag sequence [y]T\n1 and a input sequence [x]T\n1 ,\nwe maximize the likelihood (13), which corresponds to minimizing the score\nC(f\u03b8, A) = logadd\n\u2200[j]T\n1\ns([x]T\n1 , [j]T\n1 , \u02dc\u03b8)\n|\n{z\n}\nClogadd\n\u2212s([x]T\n1 , [y]T\n1 , \u02dc\u03b8) ,\nwith\ns([x]T\n1 , [y]T\n1 , \u02dc\u03b8) =\nT\nX\nt=1\n\u0010\n[A][y]t\u22121, [y]t + [f\u03b8][y]t, t\n\u0011\n.\nWe \ufb01rst initialize all gradients to zero\n\u2202C\n\u2202\n\u0002\nf\u03b8\n\u0003\ni, t\n= 0 \u2200i, t and\n\u2202C\n\u2202[A]i, j\n= 0\n\u2200i, j .\nWe then accumulate gradients over the second part of the cost \u2212s([x]T\n1 , [y]T\n1 , \u02dc\u03b8), which\ngives:\n\u2202C\n\u2202\n\u0002\nf\u03b8\n\u0003\n[y]t, t\n+= 1\n\u2202C\n\u2202[A][y]t\u22121, [y]t\n+= 1\n\u2200t .\nWe now need to accumulate the gradients over the \ufb01rst part of the cost, that is Clogadd.\nWe di\ufb00erentiate Clogadd by applying the chain rule through the recursion (14). First we\ninitialize our recursion with\n\u2202Clogadd\n\u2202\u03b4T (i) =\ne\u03b4T (i)\nP\nk e\u03b4T (k)\n\u2200i .\nWe then compute iteratively:\n\u2202Clogadd\n\u2202\u03b4t\u22121(i) =\nX\nj\n\u2202Clogadd\n\u2202\u03b4t(j)\ne\u03b4t\u22121(i)+[A]i, j\nP\nk e\u03b4t\u22121(k)+[A]k, j ,\n(25)\n40\narXiv\nNatural Language Processing (almost) from Scratch\nwhere at each step t of the recursion we accumulate of the gradients with respect to the\ninputs f\u03b8, and the transition scores [A]i, j:\n\u2202C\n\u2202\n\u0002\nf\u03b8\n\u0003\ni, t\n+=\u2202Clogadd\n\u2202\u03b4t(i)\n\u2202\u03b4t(i)\n\u2202\n\u0002\nf\u03b8\n\u0003\ni, t\n= \u2202Clogadd\n\u2202\u03b4t(i)\n\u2202C\n\u2202[A]i, j\n+=\u2202Clogadd\n\u2202\u03b4t(j)\n\u2202\u03b4t(j)\n\u2202[A]i, j\n= \u2202Clogadd\n\u2202\u03b4t(j)\ne\u03b4t\u22121(i)+[A]i, j\nP\nk e\u03b4t\u22121(k)+[A]k, j .\nRanking Criterion\nWe use the ranking criterion (18) for training our language model.\nIn this case, given a \u201cpositive\u201d example x and a \u201cnegative\u201d example x(w), we want to\nminimize:\nC(f\u03b8(x), f\u03b8(xw)) = max\nn\n0 , 1 \u2212f\u03b8(x) + f\u03b8(x(w))\no\n.\n(26)\nIgnoring the non-di\ufb00erentiability of max(0, \u00b7) in zero, the gradient is simply given by:\n \n\u2202C\n\u2202f\u03b8(x)\n\u2202C\n\u2202f\u03b8(xw)\n!\n=\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f3\n\u0012\n\u22121\n1\n\u0013\nif 1 \u2212f\u03b8(x) + f\u03b8(x(w)) > 0\n\u0012\n0\n0\n\u0013\notherwise\n.\nReferences\nR. K. Ando and T. Zhang. A framework for learning predictive structures from multiple\ntasks and unlabeled data. JMLR, 6:1817\u20131953, 11 2005.\nR. M. Bell, Y. Koren, and C. Volinsky. The BellKor solution to the Net\ufb02ix Prize. Technical\nreport, AT&T Labs, 2007. http://www.research.att.com/~volinsky/netflix.\nY. Bengio and R. Ducharme. A neural probabilistic language model. In NIPS 13, 2001.\nY. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. Greedy layer-wise training of deep\nnetworks. In Advances in Neural Information Processing Systems, NIPS 19, 2007.\nY. Bengio, J. Louradour, R. Collobert, and J. Weston. Curriculum learning. In International\nConference on Machine Learning, ICML, 2009.\nL. Bottou. Stochastic gradient learning in neural networks. In Proceedings of Neuro-N\u02c6\u0131mes\n91, Nimes, France, 1991. EC2.\nL. Bottou. Online algorithms and stochastic approximations. In David Saad, editor, Online\nLearning and Neural Networks. Cambridge University Press, Cambridge, UK, 1998.\nL. Bottou and P. Gallinari. A framework for the cooperation of learning algorithms. In\nD. Touretzky and R. Lippmann, editors, Advances in Neural Information Processing\nSystems, volume 3. Morgan Kaufmann, Denver, 1991.\nL. Bottou, Y. LeCun, and Yoshua Bengio. Global training of document processing systems\nusing graph transformer networks. In Proc. of Computer Vision and Pattern Recognition,\npages 489\u2013493, Puerto-Rico, 1997. IEEE.\n41\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\nJ. S. Bridle. Probabilistic interpretation of feedforward classi\ufb01cation network outputs, with\nrelationships to statistical pattern recognition. In F. Fogelman Souli\u00b4e and J. H\u00b4erault,\neditors, Neurocomputing: Algorithms, Architectures and Applications, pages 227\u2013236.\nNATO ASI Series, 1990.\nP. F. Brown, P. V. deSouza, R. L. Mercer, V. J. D. Pietra, and J C. Lai. Class-based n-gram\nmodels of natural language. Computational Linguistics, 18(4):467\u2013479, 1992a.\nP. F. Brown, V. J. Della Pietra, R. L. Mercer, S. A. Della Pietra, and J. C. Lai. An estimate\nof an upper bound for the entropy of english. Computational Linguistics, 18(1):31\u201341,\n1992b.\nC. J. C. Burges, R. Ragno, and Quoc Viet Le. Learning to rank with nonsmooth cost\nfunctions.\nIn B. Sch\u00a8olkopf, J. Platt, and T. Ho\ufb00man, editors, Advances in Neural\nInformation Processing Systems 19, pages 193\u2013200. MIT Press, Cambridge, MA, 2007.\nR. Caruana. Multitask Learning. Machine Learning, 28(1):41\u201375, 1997.\nO. Chapelle, B. Schlkopf, and A. Zien. Semi-Supervised Learning. Adaptive computation\nand machine learning. MIT Press, Cambridge, Mass., USA, 09 2006.\nE. Charniak. A maximum-entropy-inspired parser. Proceedings of the \ufb01rst conference on\nNorth American chapter of the Association for Computational Linguistics, pages 132\u2013139,\n2000.\nH. L. Chieu. Named entity recognition with a maximum entropy approach. In In Proceedings\nof the Seventh Conference on Natural Language Learning (CoNLL-2003, pages 160\u2013163,\n2003.\nN. Chomsky.\nThree models for the description of language.\nIRE Transactions on\nInformation Theory, 2(3):113\u2013124, September 1956.\nS. Cl\u00b4emen\u00b8con and N. Vayatis. Ranking the best instances. Journal of Machine Learning\nResearch, 8:2671\u20132699, December 2007.\nW. W. Cohen, R. E. Schapire, and Y. Singer. Learning to order things. Journal of Arti\ufb01cial\nIntelligence Research, 10:243\u2013270, 1998.\nT. Cohn and P. Blunsom. Semantic role labelling with tree conditional random \ufb01elds. In\nNinth Conference on Computational Natural Language (CoNLL), 2005.\nM. Collins.\nHead-Driven Statistical Models for Natural Language Parsing.\nPhD thesis,\nUniversity of Pennsylvania, 1999.\nR. Collobert. Large Scale Machine Learning. PhD thesis, Universit\u00b4e Paris VI, 2004.\nT. Cover and R. King. A convergent gambling estimate of the entropy of english. IEEE\nTransactions on Information Theory, 24(4):413\u2013421, July 1978.\n42\narXiv\nNatural Language Processing (almost) from Scratch\nR. Florian, A. Ittycheriah, H. Jing, and T. Zhang.\nNamed entity recognition through\nclassi\ufb01er combination.\nIn Proceedings of the seventh conference on Natural language\nlearning at HLT-NAACL 2003, pages 168\u2013171. Association for Computational Linguistics,\n2003.\nD. Gildea and D. Jurafsky. Automatic labeling of semantic roles. Computational Linguistics,\n28(3):245\u2013288, 2002.\nD. Gildea and M. Palmer. The necessity of parsing for predicate argument recognition.\nProceedings of the 40th Annual Meeting of the ACL, pages 239\u2013246, 2002.\nJ. Gim\u00b4enez and L. M`arquez.\nSVMTool:\nA general POS tagger generator based on\nsupport vector machines. In Proceedings of the 4th International Conference on Language\nResources and Evaluation (LREC\u201904), 2004.\nA. Haghighi, K. Toutanova, and C. D. Manning. A joint model for semantic role labeling.\nIn Proceedings of the Ninth Conference on Computational Natural Language Learning\n(CoNLL-2005). Association for Computational Linguistics, June 2005.\nZ. S. Harris. Mathematical Structures of Language. John Wiley & Sons Inc., 1968.\nD. Heckerman, D. M. Chickering, C. Meek, R. Rounthwaite, and C. Kadie. Dependency\nnetworks for inference, collaborative \ufb01ltering, and data visualization. Journal of Machine\nLearning Research, 1:49\u201375, 2001. ISSN 1532-4435.\nG. E. Hinton, S. Osindero, and Y.-W. Teh. A fast learning algorithm for deep belief nets.\nNeural Comp., 18(7):1527\u20131554, July 2006.\nK. Hollingshead, S. Fisher, and B. Roark.\nComparing and combining \ufb01nite-state and\ncontext-free parsers. In HLT \u201905: Proceedings of the conference on Human Language\nTechnology and Empirical Methods in Natural Language Processing, pages 787\u2013794.\nAssociation for Computational Linguistics, 2005.\nF. Huang and A. Yates. Distributional representations for handling sparsity in supervised\nsequence-labeling.\nIn Proceedings of the Association for Computational Linguistics\n(ACL), pages 495\u2013503. Association for Computational Linguistics, 2009.\nF. Jelinek. Continuous speech recognition by statistical methods. Proceedings of the IEEE,\n64(4):532\u2013556, 1976.\nT. Joachims. Transductive inference for text classi\ufb01cation using support vector machines.\nIn ICML, 1999.\nD. Klein and C. D. Manning. Natural language grammar induction using a constituent-\ncontext model.\nIn Thomas G. Dietterich, Suzanna Becker, and Zoubin Ghahramani,\neditors, Advances in Neural Information Processing Systems 14, pages 35\u201342. MIT Press,\nCambridge, MA, 2002.\nT. Koo, X. Carreras, and M. Collins.\nSimple semi-supervised dependency parsing.\nIn\nProceedings of the Association for Computational Linguistics (ACL), pages 595\u2013603, 2008.\n43\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\nP. Koomen, V. Punyakanok, D. Roth, and W. Yih. Generalized inference with multiple\nsemantic role labeling systems (shared task paper). In Ido Dagan and Dan Gildea, editors,\nProc. of the Annual Conference on Computational Natural Language Learning (CoNLL),\npages 181\u2013184, 2005.\nT. Kudo and Y. Matsumoto. Chunking with support vector machines. In In Proceedings\nof the 2nd Meeting of the North American Association for Computational Linguistics:\nNAACL 2001, pages 1\u20138. Association for Computational Linguistics, 2001.\nT. Kudoh and Y. Matsumoto. Use of support vector learning for chunk identi\ufb01cation. In\nProceedings of CoNLL-2000 and LLL-2000, pages 142\u2013144, 2000.\nJ. La\ufb00erty, A. McCallum, and F. Pereira. Conditional random \ufb01elds: Probabilistic models\nfor segmenting and labeling sequence data. In Eighteenth International Conference on\nMachine Learning, ICML, 2001.\nY. Le Cun, L. Bottou, Y. Bengio, and P. Ha\ufb00ner.\nGradient based learning applied to\ndocument recognition. Proceedings of IEEE, 86(11):2278\u20132324, 1998.\nY. LeCun.\nA learning scheme for asymmetric threshold networks.\nIn Proceedings of\nCognitiva 85, pages 599\u2013604, Paris, France, 1985.\nY. LeCun, L. Bottou, G. B. Orr, and K.-R. M\u00a8uller. E\ufb03cient backprop. In G.B. Orr and\nK.-R. M\u00a8uller, editors, Neural Networks: Tricks of the Trade, pages 9\u201350. Springer, 1998.\nD. D. Lewis, Y. Yang, T. G. Rose, and F. Li. Rcv1: A new benchmark collection for text\ncategorization research. Journal of Machine Learning Research, 5:361\u2013397, 2004.\nP. Liang. Semi-supervised learning for natural language. Master\u2019s thesis, Massachusetts\nInstitute of Technology, 2005.\nP. Liang, H. Daum\u00b4e, III, and D. Klein. Structure compilation: trading structure for features.\nIn International conference on Machine learning (ICML), pages 592\u2013599. ACM, 2008.\nD. Lin and X. Wu.\nPhrase clustering for discriminative learning.\nIn Proceedings of\nthe Association for Computational Linguistics (ACL), pages 1030\u20131038. Association for\nComputational Linguistics, 2009.\nN. Littlestone. Learning quickly when irrelevant attributes abound: A new linear-threshold\nalgorithm. In Machine Learning, pages 285\u2013318, 1988.\nA. McCallum and Wei Li.\nEarly results for named entity recognition with conditional\nrandom \ufb01elds, feature induction and web-enhanced lexicons.\nIn Proceedings of the\nseventh conference on Natural language learning at HLT-NAACL 2003, pages 188\u2013191.\nAssociation for Computational Linguistics, 2003.\nD. McClosky, E. Charniak, and M. Johnson. E\ufb00ective self-training for parsing. Proceedings\nof HLT-NAACL 2006, 2006.\n44\narXiv\nNatural Language Processing (almost) from Scratch\nR. McDonald, K. Crammer, and F. Pereira. Flexible text segmentation with structured\nmultilabel classi\ufb01cation. In HLT \u201905: Proceedings of the conference on Human Language\nTechnology and Empirical Methods in Natural Language Processing, pages 987\u2013994.\nAssociation for Computational Linguistics, 2005.\nS. Miller, H. Fox, L. Ramshaw, and R. Weischedel. A novel use of statistical parsing to\nextract information from text.\n6th Applied Natural Language Processing Conference,\n2000.\nS. Miller, J. Guinness, and A. Zamanian.\nName tagging with word clusters and\ndiscriminative training. In Proceedings of HLT-NAACL, pages 337\u2013342, 2004.\nA Mnih and G. E. Hinton. Three new graphical models for statistical language modelling.\nIn International Conference on Machine Learning, ICML, pages 641\u2013648, 2007.\nG. Musillo and P. Merlo. Robust Parsing of the Proposition Bank. ROMAND 2006: Robust\nMethods in Analysis of Natural language Data, 2006.\nR. M. Neal. Bayesian Learning for Neural Networks. Number 118 in Lecture Notes in\nStatistics. Springer-Verlag, New York, 1996.\nD. Okanohara and J. Tsujii. A discriminative language model with pseudo-negative samples.\nProceedings of the 45th Annual Meeting of the ACL, pages 73\u201380, 2007.\nM. Palmer, D. Gildea, and P. Kingsbury. The proposition bank: An annotated corpus of\nsemantic roles. Comput. Linguist., 31(1):71\u2013106, 2005. ISSN 0891-2017.\nJ. Pearl. Probabilistic Reasoning in Intelligent Systems. Morgan Kaufman, San Mateo,\n1988.\nD. C. Plaut and G. E. Hinton. Learning sets of \ufb01lters using back-propagation. Computer\nSpeech and Language, 2:35\u201361, 1987.\nM. F. Porter. An algorithm for su\ufb03x stripping. Program, 14(3):130\u2013137, 1980.\nS. Pradhan, W. Ward, K. Hacioglu, J. Martin, and D. Jurafsky. Shallow semantic parsing\nusing support vector machines. Proceedings of HLT/NAACL-2004, 2004.\nS. Pradhan, K. Hacioglu, W. Ward, J. H. Martin, and D. Jurafsky. Semantic role chunking\ncombining complementary syntactic views. In Proceedings of the Ninth Conference on\nComputational Natural Language Learning (CoNLL-2005), pages 217\u2013220. Association\nfor Computational Linguistics, June 2005.\nV. Punyakanok, D. Roth, and W. Yih. The necessity of syntactic parsing for semantic role\nlabeling. In IJCAI, pages 1117\u20131123, 2005.\nL. Ratinov and D. Roth. Design challenges and misconceptions in named entity recognition.\nIn Proceedings of the Thirteenth Conference on Computational Natural Language Learning\n(CoNLL), pages 147\u2013155. Association for Computational Linguistics, 2009.\n45\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\nA. Ratnaparkhi. A maximum entropy model for part-of-speech tagging. In Eric Brill and\nKenneth Church, editors, Proceedings of the Conference on Empirical Methods in Natural\nLanguage Processing, pages 133\u2013142. Association for Computational Linguistics, 1996.\nB. Rosenfeld and R. Feldman.\nUsing Corpus Statistics on Entities to Improve Semi-\nsupervised Relation Extraction from the Web. Proceedings of the 45th Annual Meeting\nof the ACL, pages 600\u2013607, 2007.\nD. E. Rumelhart, G. E. Hinton, and R. J. Williams.\nLearning internal representations\nby back-propagating errors. In D.E. Rumelhart and J. L. McClelland, editors, Parallel\nDistributed Processing: Explorations in the Microstructure of Cognition, volume 1, pages\n318\u2013362. MIT Press, 1986.\nH. Sch\u00a8utze. Distributional part-of-speech tagging. In Proceedings of the Association for\nComputational Linguistics (ACL), pages 141\u2013148. Morgan Kaufmann Publishers Inc.,\n1995.\nH. Schwenk and J. L. Gauvain.\nConnectionist language modeling for large vocabulary\ncontinuous speech recognition. In IEEE International Conference on Acoustics, Speech,\nand Signal Processing, pages 765\u2013768, 2002.\nF. Sha and F. Pereira. Shallow parsing with conditional random \ufb01elds. In NAACL \u201903:\nProceedings of the 2003 Conference of the North American Chapter of the Association for\nComputational Linguistics on Human Language Technology, pages 134\u2013141. Association\nfor Computational Linguistics, 2003.\nC. E. Shannon. Prediction and entropy of printed english. Bell Systems Technical Journal,\n30:50\u201364, 1951.\nH. Shen and A. Sarkar. Voting between multiple data representations for text chunking.\nAdvances in Arti\ufb01cial Intelligence, pages 389\u2013400, 2005.\nL. Shen, G. Satta, and A. K. Joshi. Guided learning for bidirectional sequence classi\ufb01cation.\nIn Proceedings of the 45th Annual Meeting of the Association for Computational\nLinguistics (ACL), 2007.\nN. A. Smith and J. Eisner. Contrastive estimation: Training log-linear models on unlabeled\ndata. In Proceedings of the 43rd Annual Meeting of the Association for Computational\nLinguistics (ACL), pages 354\u2013362. Association for Computational Linguistics, 2005.\nS. C. Suddarth and A. D. C. Holden. Symbolic-neural systems and the use of hints for\ndeveloping complex systems. International Journal of Man-Machine Studies, 35(3):291\u2013\n311, 1991.\nX. Sun, L.-P. Morency, D. Okanohara, and J. Tsujii. Modeling latent-dynamic in shallow\nparsing: a latent conditional model with improved inference. In COLING \u201908: Proceedings\nof the 22nd International Conference on Computational Linguistics, pages 841\u2013848.\nAssociation for Computational Linguistics, 2008.\n46\narXiv\nNatural Language Processing (almost) from Scratch\nC. Sutton and A. McCallum. Joint parsing and semantic role labeling. In Proceedings of\nCoNLL-2005, pages 225\u2013228, 2005a.\nC. Sutton and A. McCallum. Composition of conditional random \ufb01elds for transfer learning.\nProceedings of the conference on Human Language Technology and Empirical Methods in\nNatural Language Processing, pages 748\u2013754, 2005b.\nC. Sutton, A. McCallum, and K. Rohanimanesh. Dynamic Conditional Random Fields:\nFactorized Probabilistic Models for Labeling and Segmenting Sequence Data. JMLR, 8:\n693\u2013723, 2007.\nJ. Suzuki and H. Isozaki. Semi-supervised sequential labeling and segmentation using giga-\nword scale unlabeled data. In Proceedings of ACL-08: HLT, pages 665\u2013673, Columbus,\nOhio, June 2008. Association for Computational Linguistics.\nW. J. Teahan and J. G. Cleary. The entropy of english using ppm-based models. In In Data\nCompression Conference (DCC\u201996), pages 53\u201362. IEEE Computer Society Press, 1996.\nK. Toutanova, D. Klein, C. D. Manning, and Y. Singer. Feature-rich part-of-speech tagging\nwith a cyclic dependency network. In HLT-NAACL, 2003.\nJ. Turian, L. Ratinov, and Y. Bengio.\nWord representations:\nA simple and general\nmethod for semi-supervised learning. In Proceedings of the Association for Computational\nLinguistics (ACL), pages 384\u2013392. Association for Computational Linguistics, 2010.\nN. Ue\ufb03ng, G. Ha\ufb00ari, and A. Sarkar.\nTransductive learning for statistical machine\ntranslation. Proceedings of the 45th Annual Meeting of the ACL, pages 25\u201332, 2007.\nA. Waibel, T. Hanazawa, G. Hinton, K. Shikano, and K.J. Lang. Phoneme recognition\nusing time-delay neural networks. IEEE Transactions on Acoustics, Speech, and Signal\nProcessing, 37(3):328\u2013339, 1989.\nJ. Weston, F. Ratle, and R. Collobert. Deep learning via semi-supervised embedding. In\nProceedings of the 25th international conference on Machine learning, pages 1168\u20131175.\nACM, 2008.\n47\n",
        "sentence": " Better results have been reported by others for this dataset: the spectral approach of Dhillon et al. (2011) used different (less stringent) preprocessing and a vocabulary of 300,000 words and obtained higher F1 scores than the methods evaluated in Turian et al.",
        "context": "reported (Sha and Pereira, 2003; McDonald et al., 2005; Sun et al., 2008), all reporting\naround 94.3% F1 score. These systems use features composed of words, POS tags, and\ntags.\na SVM approach also trained on text windows, with bidirectional inference achieved with\ntwo Viterbi decoders (left-to-right and right-to-left).\nThey obtained 97.16% per-word\naccuracy.\nscore. This is not too far from the state-of-the-art system which we note uses six parse\ntrees instead of one. Koomen et al. (2005) also report a 74.76% F1 score on the validation"
    },
    {
        "title": "Multi-view learning of word embeddings via CCA",
        "author": [
            "P. Dhillon",
            "D.P. Foster",
            "L. Ungar"
        ],
        "venue": "In NIPS",
        "citeRegEx": "Dhillon et al\\.,? \\Q2011\\E",
        "shortCiteRegEx": "Dhillon et al\\.",
        "year": 2011,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Visualizing data using t-SNE",
        "author": [
            "L. van der Maaten",
            "G.E. Hinton"
        ],
        "venue": "Journal of Machine Learning Research,",
        "citeRegEx": "Maaten and Hinton.,? \\Q2008\\E",
        "shortCiteRegEx": "Maaten and Hinton.",
        "year": 2008,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Learning word vectors for sentiment analysis",
        "author": [
            "A.L. Maas",
            "R.E. Daly",
            "P.T. Pham",
            "D. Huang",
            "A.Y. Ng",
            "C. Potts"
        ],
        "venue": "In ACL,",
        "citeRegEx": "Maas et al\\.,? \\Q2011\\E",
        "shortCiteRegEx": "Maas et al\\.",
        "year": 2011,
        "abstract": "",
        "full_text": "",
        "sentence": " We trained class-specific, 5-gram WRRBMs on the labeled documents of the Large Movie Review dataset introduced by Maas et al. (2011), independently parameterizing words that occurred at least 235 times in the training set (giving us approximately the same vocabulary size as the model used in Maas et al. We trained class-specific, 5-gram WRRBMs on the labeled documents of the Large Movie Review dataset introduced by Maas et al. (2011), independently parameterizing words that occurred at least 235 times in the training set (giving us approximately the same vocabulary size as the model used in Maas et al. (2011)). We trained class-specific, 5-gram WRRBMs on the labeled documents of the Large Movie Review dataset introduced by Maas et al. (2011), independently parameterizing words that occurred at least 235 times in the training set (giving us approximately the same vocabulary size as the model used in Maas et al. (2011)). Table 2. Experimental results on the sentiment classification task. The baseline results were taken from Maas et al. (2011). The performance measure is accuracy (%). 96 Maas et al. (2011)\u2019s \u201cfull\u201d method 87. 96 Maas et al. (2011)\u2019s \u201cfull\u201d method 87.44 Bag of words \u201cbnc\u201d 87.80 Maas et al. (2011)\u2019s \u201cfull\u201d method 88. 96 Maas et al. (2011)\u2019s \u201cfull\u201d method 87.44 Bag of words \u201cbnc\u201d 87.80 Maas et al. (2011)\u2019s \u201cfull\u201d method 88.33 + bag of words \u201cbnc\u201d Maas et al. (2011)\u2019s \u201cfull\u201d method 88. The bag-of-words feature vector was weighted and normalized as in Maas et al. (2011) and the average free energies were scaled to lie on [0, 1].",
        "context": null
    }
]