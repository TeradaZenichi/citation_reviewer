[
    {
        "title": "Greedy layer-wise training of deep networks",
        "author": [
            "Bengio",
            "Yoshua",
            "Lamblin",
            "Pascal",
            "Popovici",
            "Dan",
            "Larochelle",
            "Hugo"
        ],
        "venue": "Advances in neural information processing systems,",
        "citeRegEx": "Bengio et al\\.,? \\Q2007\\E",
        "shortCiteRegEx": "Bengio et al\\.",
        "year": 2007,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Bengio et al. (2007) showed that greedy layerwise pre-training can speed up the convergence and improve generalization capabilities of deep networks.",
        "context": null
    },
    {
        "title": "Restoring an image taken through a window covered with dirt or rain",
        "author": [
            "Eigen",
            "David",
            "Krishnan",
            "Dilip",
            "Fergus",
            "Rob"
        ],
        "venue": "In Computer Vision (ICCV),",
        "citeRegEx": "Eigen et al\\.,? \\Q2013\\E",
        "shortCiteRegEx": "Eigen et al\\.",
        "year": 2013,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " , 2012) or image restoration (Eigen et al., 2013).",
        "context": null
    },
    {
        "title": "Pylearn2: a machine learning research",
        "author": [
            "Goodfellow",
            "Ian J",
            "Warde-Farley",
            "David",
            "Lamblin",
            "Pascal",
            "Dumoulin",
            "Vincent",
            "Mirza",
            "Mehdi",
            "Pascanu",
            "Razvan",
            "Bergstra",
            "James",
            "Bastien",
            "Fr\u00e9d\u00e9ric",
            "Bengio",
            "Yoshua"
        ],
        "venue": "library. arXiv preprint arXiv:1308.4214,",
        "citeRegEx": "Goodfellow et al\\.,? \\Q2013\\E",
        "shortCiteRegEx": "Goodfellow et al\\.",
        "year": 2013,
        "abstract": "",
        "full_text": "",
        "sentence": " Our methods are amenable to different nonlinearities such as maxout (Goodfellow et al., 2013b) and different training algorithms such as SGD with momentum, SFO (Sohl-Dickstein et al., 2014), or Dropout (Srivastava et al., 2014). Performance of a fully connectedl maxout network used for the encoder similar to Goodfellow et al. (2013b) is also shown.",
        "context": null
    },
    {
        "title": "Bilinear sparse coding for invariant vision",
        "author": [
            "Grimes",
            "David B",
            "Rao",
            "Rajesh PN"
        ],
        "venue": "Neural computation,",
        "citeRegEx": "Grimes et al\\.,? \\Q2005\\E",
        "shortCiteRegEx": "Grimes et al\\.",
        "year": 2005,
        "abstract": " Recent algorithms for sparse coding and independent component analysis (ICA) have demonstrated how localized features can be learned from natural images. However, these approaches do not take image transformations into account. We describe an unsupervised algorithm for learning both localized features and their transformations directly from images using a sparse bilinear generative model. We show that from an arbitrary set of natural images, the algorithm produces oriented basis filters that can simultaneously represent features in an image and their transformations. The learned generative model can be used to translate features to different locations, thereby reducing the need to learn the same feature at multiple locations, a limitation of previous approaches to sparse coding and ICA. Our results suggest that by explicitly modeling the interaction between local image features and their transformations, the sparse bilinear approach can provide a basis for achieving transformation-invariant vision. ",
        "full_text": "",
        "sentence": " For example, Kingma et al. (2014) utilized a variational autoencoder in a semi-supervised learning paradigm which learned to separate content and style in data.",
        "context": null
    },
    {
        "title": "Semisupervised learning with deep generative models",
        "author": [
            "Kingma",
            "Diederik P",
            "Mohamed",
            "Shakir",
            "Rezende",
            "Danilo Jimenez",
            "Welling",
            "Max"
        ],
        "venue": "In Advances in Neural Information Processing Systems,",
        "citeRegEx": "Kingma et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Kingma et al\\.",
        "year": 2014,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing",
        "author": [
            "Krizhevsky",
            "Alex",
            "Sutskever",
            "Ilya",
            "Hinton",
            "Geoffrey E"
        ],
        "venue": null,
        "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E",
        "shortCiteRegEx": "Krizhevsky et al\\.",
        "year": 2012,
        "abstract": "",
        "full_text": "",
        "sentence": " One of the goals of representation learning is to find an efficient representation of input data that simplifies tasks such as object classification (Krizhevsky et al., 2012) or image restoration (Eigen et al.",
        "context": null
    },
    {
        "title": "Building high-level features using large scale unsupervised learning",
        "author": [
            "Le",
            "Quoc V"
        ],
        "venue": "In Acoustics, Speech and Signal Processing (ICASSP),",
        "citeRegEx": "Le and V.,? \\Q2013\\E",
        "shortCiteRegEx": "Le and V.",
        "year": 2013,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "The mnist database of handwritten digits",
        "author": [
            "LeCun",
            "Yann",
            "Cortes",
            "Corinna"
        ],
        "venue": null,
        "citeRegEx": "LeCun et al\\.,? \\Q1998\\E",
        "shortCiteRegEx": "LeCun et al\\.",
        "year": 1998,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Unsupervised feature learning for audio classification using convolutional deep belief networks",
        "author": [
            "Lee",
            "Honglak",
            "Pham",
            "Peter",
            "Largman",
            "Yan",
            "Ng",
            "Andrew Y"
        ],
        "venue": "Advances in Neural Information Processing Systems",
        "citeRegEx": "Lee et al\\.,? \\Q2009\\E",
        "shortCiteRegEx": "Lee et al\\.",
        "year": 2009,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Bilinear models of natural images",
        "author": [
            "Olshausen",
            "Bruno A",
            "Cadieu",
            "Charles",
            "Culpepper",
            "Jack",
            "Warland",
            "David K"
        ],
        "venue": "In Electronic Imaging",
        "citeRegEx": "Olshausen et al\\.,? \\Q2007\\E",
        "shortCiteRegEx": "Olshausen et al\\.",
        "year": 2007,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Learning to disentangle factors of variation with manifold interaction",
        "author": [
            "Reed",
            "Scott",
            "Sohn",
            "Kihyuk",
            "Zhang",
            "Yuting",
            "Lee",
            "Honglak"
        ],
        "venue": "In Proceedings of The 31st International Conference on Machine Learning,",
        "citeRegEx": "Reed et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Reed et al\\.",
        "year": 2014,
        "abstract": "",
        "full_text": "",
        "sentence": " identity using higher-order restricted Boltzmann machines (Reed et al., 2014).",
        "context": null
    },
    {
        "title": "Contractive autoencoders: Explicit invariance during feature extraction",
        "author": [
            "Rifai",
            "Salah",
            "Vincent",
            "Pascal",
            "Muller",
            "Xavier",
            "Glorot",
            "Bengio",
            "Yoshua"
        ],
        "venue": "In Proceedings of the 28th International Conference on Machine Learning",
        "citeRegEx": "Rifai et al\\.,? \\Q2011\\E",
        "shortCiteRegEx": "Rifai et al\\.",
        "year": 2011,
        "abstract": "",
        "full_text": "",
        "sentence": " Autoencoder models have been shown to be useful for a variety of machine learning tasks (Rifai et al., 2011; Vincent et al., 2010; Le, 2013). Rifai et al. (2012) proposed a similar penalty in Contractive Discriminant Analysis method which penalized the cross-derivatives between sets of supervised and unsupervised latent variables with respect to the input. Inspired by Rifai et al. (2011), for the next two layers, we plot the singular value spectrum.",
        "context": null
    },
    {
        "title": "Disentangling factors of variation for facial expression recognition",
        "author": [
            "Rifai",
            "Salah",
            "Bengio",
            "Yoshua",
            "Courville",
            "Aaron",
            "Vincent",
            "Pascal",
            "Mirza",
            "Mehdi"
        ],
        "venue": "In Computer Vision\u2013ECCV",
        "citeRegEx": "Rifai et al\\.,? \\Q2012\\E",
        "shortCiteRegEx": "Rifai et al\\.",
        "year": 2012,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Fast large-scale optimization by unifying stochastic gradient and quasi-newton methods",
        "author": [
            "Sohl-Dickstein",
            "Jascha",
            "Poole",
            "Ben",
            "Ganguli",
            "Surya"
        ],
        "venue": "In Proceedings of the 31st International Conference on Machine Learning",
        "citeRegEx": "Sohl.Dickstein et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Sohl.Dickstein et al\\.",
        "year": 2014,
        "abstract": "",
        "full_text": "",
        "sentence": " , 2013b) and different training algorithms such as SGD with momentum, SFO (Sohl-Dickstein et al., 2014), or Dropout (Srivastava et al.",
        "context": null
    },
    {
        "title": "Dropout: A simple way to prevent neural networks from overfitting",
        "author": [
            "Srivastava",
            "Nitish",
            "Hinton",
            "Geoffrey",
            "Krizhevsky",
            "Alex",
            "Sutskever",
            "Ilya",
            "Salakhutdinov",
            "Ruslan"
        ],
        "venue": "The Journal of Machine Learning Research,",
        "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E",
        "shortCiteRegEx": "Srivastava et al\\.",
        "year": 1929,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "The toronto face database",
        "author": [
            "J. Susskind",
            "A. Anderson",
            "G.E. Hinton"
        ],
        "venue": "Technical report, University of Toronto,",
        "citeRegEx": "Susskind et al\\.,? \\Q2010\\E",
        "shortCiteRegEx": "Susskind et al\\.",
        "year": 2010,
        "abstract": "",
        "full_text": "",
        "sentence": " 2 TORONTO FACES DATABASE The Toronto Faces Database (Susskind et al., 2010) consists of 102,236 grayscale face images of size 48x48.",
        "context": null
    },
    {
        "title": "Going deeper with convolutions",
        "author": [
            "Szegedy",
            "Christian",
            "Liu",
            "Wei",
            "Jia",
            "Yangqing",
            "Sermanet",
            "Pierre",
            "Reed",
            "Scott",
            "Anguelov",
            "Dragomir",
            "Erhan",
            "Dumitru",
            "Vanhoucke",
            "Vincent",
            "Rabinovich",
            "Andrew"
        ],
        "venue": "arXiv preprint arXiv:1409.4842,",
        "citeRegEx": "Szegedy et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Szegedy et al\\.",
        "year": 2014,
        "abstract": "We propose a deep convolutional neural network architecture codenamed\n\"Inception\", which was responsible for setting the new state of the art for\nclassification and detection in the ImageNet Large-Scale Visual Recognition\nChallenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the\nimproved utilization of the computing resources inside the network. This was\nachieved by a carefully crafted design that allows for increasing the depth and\nwidth of the network while keeping the computational budget constant. To\noptimize quality, the architectural decisions were based on the Hebbian\nprinciple and the intuition of multi-scale processing. One particular\nincarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22\nlayers deep network, the quality of which is assessed in the context of\nclassification and detection.",
        "full_text": "Going deeper with convolutions\nChristian Szegedy\nGoogle Inc.\nWei Liu\nUniversity of North Carolina, Chapel Hill\nYangqing Jia\nGoogle Inc.\nPierre Sermanet\nGoogle Inc.\nScott Reed\nUniversity of Michigan\nDragomir Anguelov\nGoogle Inc.\nDumitru Erhan\nGoogle Inc.\nVincent Vanhoucke\nGoogle Inc.\nAndrew Rabinovich\nGoogle Inc.\nAbstract\nWe propose a deep convolutional neural network architecture codenamed Incep-\ntion, which was responsible for setting the new state of the art for classi\ufb01cation\nand detection in the ImageNet Large-Scale Visual Recognition Challenge 2014\n(ILSVRC14). The main hallmark of this architecture is the improved utilization\nof the computing resources inside the network. This was achieved by a carefully\ncrafted design that allows for increasing the depth and width of the network while\nkeeping the computational budget constant. To optimize quality, the architectural\ndecisions were based on the Hebbian principle and the intuition of multi-scale\nprocessing. One particular incarnation used in our submission for ILSVRC14 is\ncalled GoogLeNet, a 22 layers deep network, the quality of which is assessed in\nthe context of classi\ufb01cation and detection.\n1\nIntroduction\nIn the last three years, mainly due to the advances of deep learning, more concretely convolutional\nnetworks [10], the quality of image recognition and object detection has been progressing at a dra-\nmatic pace. One encouraging news is that most of this progress is not just the result of more powerful\nhardware, larger datasets and bigger models, but mainly a consequence of new ideas, algorithms and\nimproved network architectures. No new data sources were used, for example, by the top entries in\nthe ILSVRC 2014 competition besides the classi\ufb01cation dataset of the same competition for detec-\ntion purposes. Our GoogLeNet submission to ILSVRC 2014 actually uses 12\u00d7 fewer parameters\nthan the winning architecture of Krizhevsky et al [9] from two years ago, while being signi\ufb01cantly\nmore accurate. The biggest gains in object-detection have not come from the utilization of deep\nnetworks alone or bigger models, but from the synergy of deep architectures and classical computer\nvision, like the R-CNN algorithm by Girshick et al [6].\nAnother notable factor is that with the ongoing traction of mobile and embedded computing, the\nef\ufb01ciency of our algorithms \u2013 especially their power and memory use \u2013 gains importance. It is\nnoteworthy that the considerations leading to the design of the deep architecture presented in this\npaper included this factor rather than having a sheer \ufb01xation on accuracy numbers. For most of the\nexperiments, the models were designed to keep a computational budget of 1.5 billion multiply-adds\nat inference time, so that the they do not end up to be a purely academic curiosity, but could be put\nto real world use, even on large datasets, at a reasonable cost.\n1\narXiv:1409.4842v1  [cs.CV]  17 Sep 2014\nIn this paper, we will focus on an ef\ufb01cient deep neural network architecture for computer vision,\ncodenamed Inception, which derives its name from the Network in network paper by Lin et al [12]\nin conjunction with the famous \u201cwe need to go deeper\u201d internet meme [1]. In our case, the word\n\u201cdeep\u201d is used in two different meanings: \ufb01rst of all, in the sense that we introduce a new level of\norganization in the form of the \u201cInception module\u201d and also in the more direct sense of increased\nnetwork depth. In general, one can view the Inception model as a logical culmination of [12]\nwhile taking inspiration and guidance from the theoretical work by Arora et al [2]. The bene\ufb01ts\nof the architecture are experimentally veri\ufb01ed on the ILSVRC 2014 classi\ufb01cation and detection\nchallenges, on which it signi\ufb01cantly outperforms the current state of the art.\n2\nRelated Work\nStarting with LeNet-5 [10], convolutional neural networks (CNN) have typically had a standard\nstructure \u2013 stacked convolutional layers (optionally followed by contrast normalization and max-\npooling) are followed by one or more fully-connected layers. Variants of this basic design are\nprevalent in the image classi\ufb01cation literature and have yielded the best results to-date on MNIST,\nCIFAR and most notably on the ImageNet classi\ufb01cation challenge [9, 21]. For larger datasets such\nas Imagenet, the recent trend has been to increase the number of layers [12] and layer size [21, 14],\nwhile using dropout [7] to address the problem of over\ufb01tting.\nDespite concerns that max-pooling layers result in loss of accurate spatial information, the same\nconvolutional network architecture as [9] has also been successfully employed for localization [9,\n14], object detection [6, 14, 18, 5] and human pose estimation [19]. Inspired by a neuroscience\nmodel of the primate visual cortex, Serre et al. [15] use a series of \ufb01xed Gabor \ufb01lters of different sizes\nin order to handle multiple scales, similarly to the Inception model. However, contrary to the \ufb01xed\n2-layer deep model of [15], all \ufb01lters in the Inception model are learned. Furthermore, Inception\nlayers are repeated many times, leading to a 22-layer deep model in the case of the GoogLeNet\nmodel.\nNetwork-in-Network is an approach proposed by Lin et al. [12] in order to increase the representa-\ntional power of neural networks. When applied to convolutional layers, the method could be viewed\nas additional 1\u00d71 convolutional layers followed typically by the recti\ufb01ed linear activation [9]. This\nenables it to be easily integrated in the current CNN pipelines. We use this approach heavily in our\narchitecture. However, in our setting, 1 \u00d7 1 convolutions have dual purpose: most critically, they\nare used mainly as dimension reduction modules to remove computational bottlenecks, that would\notherwise limit the size of our networks. This allows for not just increasing the depth, but also the\nwidth of our networks without signi\ufb01cant performance penalty.\nThe current leading approach for object detection is the Regions with Convolutional Neural Net-\nworks (R-CNN) proposed by Girshick et al. [6]. R-CNN decomposes the overall detection problem\ninto two subproblems: to \ufb01rst utilize low-level cues such as color and superpixel consistency for\npotential object proposals in a category-agnostic fashion, and to then use CNN classi\ufb01ers to identify\nobject categories at those locations. Such a two stage approach leverages the accuracy of bound-\ning box segmentation with low-level cues, as well as the highly powerful classi\ufb01cation power of\nstate-of-the-art CNNs. We adopted a similar pipeline in our detection submissions, but have ex-\nplored enhancements in both stages, such as multi-box [5] prediction for higher object bounding\nbox recall, and ensemble approaches for better categorization of bounding box proposals.\n3\nMotivation and High Level Considerations\nThe most straightforward way of improving the performance of deep neural networks is by increas-\ning their size. This includes both increasing the depth \u2013 the number of levels \u2013 of the network and its\nwidth: the number of units at each level. This is as an easy and safe way of training higher quality\nmodels, especially given the availability of a large amount of labeled training data. However this\nsimple solution comes with two major drawbacks.\nBigger size typically means a larger number of parameters, which makes the enlarged network more\nprone to over\ufb01tting, especially if the number of labeled examples in the training set is limited.\nThis can become a major bottleneck, since the creation of high quality training sets can be tricky\n2\n(a) Siberian husky\n(b) Eskimo dog\nFigure 1: Two distinct classes from the 1000 classes of the ILSVRC 2014 classi\ufb01cation challenge.\nand expensive, especially if expert human raters are necessary to distinguish between \ufb01ne-grained\nvisual categories like those in ImageNet (even in the 1000-class ILSVRC subset) as demonstrated\nby Figure 1.\nAnother drawback of uniformly increased network size is the dramatically increased use of compu-\ntational resources. For example, in a deep vision network, if two convolutional layers are chained,\nany uniform increase in the number of their \ufb01lters results in a quadratic increase of computation. If\nthe added capacity is used inef\ufb01ciently (for example, if most weights end up to be close to zero),\nthen a lot of computation is wasted. Since in practice the computational budget is always \ufb01nite, an\nef\ufb01cient distribution of computing resources is preferred to an indiscriminate increase of size, even\nwhen the main objective is to increase the quality of results.\nThe fundamental way of solving both issues would be by ultimately moving from fully connected\nto sparsely connected architectures, even inside the convolutions. Besides mimicking biological\nsystems, this would also have the advantage of \ufb01rmer theoretical underpinnings due to the ground-\nbreaking work of Arora et al. [2]. Their main result states that if the probability distribution of\nthe data-set is representable by a large, very sparse deep neural network, then the optimal network\ntopology can be constructed layer by layer by analyzing the correlation statistics of the activations\nof the last layer and clustering neurons with highly correlated outputs. Although the strict math-\nematical proof requires very strong conditions, the fact that this statement resonates with the well\nknown Hebbian principle \u2013 neurons that \ufb01re together, wire together \u2013 suggests that the underlying\nidea is applicable even under less strict conditions, in practice.\nOn the downside, todays computing infrastructures are very inef\ufb01cient when it comes to numerical\ncalculation on non-uniform sparse data structures. Even if the number of arithmetic operations is\nreduced by 100\u00d7, the overhead of lookups and cache misses is so dominant that switching to sparse\nmatrices would not pay off. The gap is widened even further by the use of steadily improving,\nhighly tuned, numerical libraries that allow for extremely fast dense matrix multiplication, exploit-\ning the minute details of the underlying CPU or GPU hardware [16, 9]. Also, non-uniform sparse\nmodels require more sophisticated engineering and computing infrastructure. Most current vision\noriented machine learning systems utilize sparsity in the spatial domain just by the virtue of em-\nploying convolutions. However, convolutions are implemented as collections of dense connections\nto the patches in the earlier layer. ConvNets have traditionally used random and sparse connection\ntables in the feature dimensions since [11] in order to break the symmetry and improve learning, the\ntrend changed back to full connections with [9] in order to better optimize parallel computing. The\nuniformity of the structure and a large number of \ufb01lters and greater batch size allow for utilizing\nef\ufb01cient dense computation.\nThis raises the question whether there is any hope for a next, intermediate step: an architecture\nthat makes use of the extra sparsity, even at \ufb01lter level, as suggested by the theory, but exploits our\n3\ncurrent hardware by utilizing computations on dense matrices. The vast literature on sparse matrix\ncomputations (e.g. [3]) suggests that clustering sparse matrices into relatively dense submatrices\ntends to give state of the art practical performance for sparse matrix multiplication. It does not\nseem far-fetched to think that similar methods would be utilized for the automated construction of\nnon-uniform deep-learning architectures in the near future.\nThe Inception architecture started out as a case study of the \ufb01rst author for assessing the hypothetical\noutput of a sophisticated network topology construction algorithm that tries to approximate a sparse\nstructure implied by [2] for vision networks and covering the hypothesized outcome by dense, read-\nily available components. Despite being a highly speculative undertaking, only after two iterations\non the exact choice of topology, we could already see modest gains against the reference architec-\nture based on [12]. After further tuning of learning rate, hyperparameters and improved training\nmethodology, we established that the resulting Inception architecture was especially useful in the\ncontext of localization and object detection as the base network for [6] and [5]. Interestingly, while\nmost of the original architectural choices have been questioned and tested thoroughly, they turned\nout to be at least locally optimal.\nOne must be cautious though: although the proposed architecture has become a success for computer\nvision, it is still questionable whether its quality can be attributed to the guiding principles that have\nlead to its construction. Making sure would require much more thorough analysis and veri\ufb01cation:\nfor example, if automated tools based on the principles described below would \ufb01nd similar, but\nbetter topology for the vision networks. The most convincing proof would be if an automated\nsystem would create network topologies resulting in similar gains in other domains using the same\nalgorithm but with very differently looking global architecture. At very least, the initial success of\nthe Inception architecture yields \ufb01rm motivation for exciting future work in this direction.\n4\nArchitectural Details\nThe main idea of the Inception architecture is based on \ufb01nding out how an optimal local sparse\nstructure in a convolutional vision network can be approximated and covered by readily available\ndense components. Note that assuming translation invariance means that our network will be built\nfrom convolutional building blocks. All we need is to \ufb01nd the optimal local construction and to\nrepeat it spatially. Arora et al. [2] suggests a layer-by layer construction in which one should analyze\nthe correlation statistics of the last layer and cluster them into groups of units with high correlation.\nThese clusters form the units of the next layer and are connected to the units in the previous layer. We\nassume that each unit from the earlier layer corresponds to some region of the input image and these\nunits are grouped into \ufb01lter banks. In the lower layers (the ones close to the input) correlated units\nwould concentrate in local regions. This means, we would end up with a lot of clusters concentrated\nin a single region and they can be covered by a layer of 1\u00d71 convolutions in the next layer, as\nsuggested in [12]. However, one can also expect that there will be a smaller number of more\nspatially spread out clusters that can be covered by convolutions over larger patches, and there\nwill be a decreasing number of patches over larger and larger regions. In order to avoid patch-\nalignment issues, current incarnations of the Inception architecture are restricted to \ufb01lter sizes 1\u00d71,\n3\u00d73 and 5\u00d75, however this decision was based more on convenience rather than necessity. It also\nmeans that the suggested architecture is a combination of all those layers with their output \ufb01lter\nbanks concatenated into a single output vector forming the input of the next stage. Additionally,\nsince pooling operations have been essential for the success in current state of the art convolutional\nnetworks, it suggests that adding an alternative parallel pooling path in each such stage should have\nadditional bene\ufb01cial effect, too (see Figure 2(a)).\nAs these \u201cInception modules\u201d are stacked on top of each other, their output correlation statistics\nare bound to vary: as features of higher abstraction are captured by higher layers, their spatial\nconcentration is expected to decrease suggesting that the ratio of 3\u00d73 and 5\u00d75 convolutions should\nincrease as we move to higher layers.\nOne big problem with the above modules, at least in this na\u00a8\u0131ve form, is that even a modest number of\n5\u00d75 convolutions can be prohibitively expensive on top of a convolutional layer with a large number\nof \ufb01lters. This problem becomes even more pronounced once pooling units are added to the mix:\ntheir number of output \ufb01lters equals to the number of \ufb01lters in the previous stage. The merging of\nthe output of the pooling layer with the outputs of convolutional layers would lead to an inevitable\n4\n1x1 convolutions\n3x3 convolutions\n5x5 convolutions\nFilter \nconcatenation\nPrevious layer\n3x3 max pooling\n(a) Inception module, na\u00a8\u0131ve version\n1x1 convolutions\n3x3 convolutions\n5x5 convolutions\nFilter \nconcatenation\nPrevious layer\n3x3 max pooling\n1x1 convolutions\n1x1 convolutions\n1x1 convolutions\n(b) Inception module with dimension reductions\nFigure 2: Inception module\nincrease in the number of outputs from stage to stage. Even while this architecture might cover the\noptimal sparse structure, it would do it very inef\ufb01ciently, leading to a computational blow up within\na few stages.\nThis leads to the second idea of the proposed architecture: judiciously applying dimension reduc-\ntions and projections wherever the computational requirements would increase too much otherwise.\nThis is based on the success of embeddings: even low dimensional embeddings might contain a lot\nof information about a relatively large image patch. However, embeddings represent information in\na dense, compressed form and compressed information is harder to model. We would like to keep\nour representation sparse at most places (as required by the conditions of [2]) and compress the\nsignals only whenever they have to be aggregated en masse. That is, 1\u00d71 convolutions are used to\ncompute reductions before the expensive 3\u00d73 and 5\u00d75 convolutions. Besides being used as reduc-\ntions, they also include the use of recti\ufb01ed linear activation which makes them dual-purpose. The\n\ufb01nal result is depicted in Figure 2(b).\nIn general, an Inception network is a network consisting of modules of the above type stacked upon\neach other, with occasional max-pooling layers with stride 2 to halve the resolution of the grid. For\ntechnical reasons (memory ef\ufb01ciency during training), it seemed bene\ufb01cial to start using Inception\nmodules only at higher layers while keeping the lower layers in traditional convolutional fashion.\nThis is not strictly necessary, simply re\ufb02ecting some infrastructural inef\ufb01ciencies in our current\nimplementation.\nOne of the main bene\ufb01cial aspects of this architecture is that it allows for increasing the number of\nunits at each stage signi\ufb01cantly without an uncontrolled blow-up in computational complexity. The\nubiquitous use of dimension reduction allows for shielding the large number of input \ufb01lters of the\nlast stage to the next layer, \ufb01rst reducing their dimension before convolving over them with a large\npatch size. Another practically useful aspect of this design is that it aligns with the intuition that\nvisual information should be processed at various scales and then aggregated so that the next stage\ncan abstract features from different scales simultaneously.\nThe improved use of computational resources allows for increasing both the width of each stage\nas well as the number of stages without getting into computational dif\ufb01culties. Another way to\nutilize the inception architecture is to create slightly inferior, but computationally cheaper versions\nof it. We have found that all the included the knobs and levers allow for a controlled balancing of\ncomputational resources that can result in networks that are 2 \u22123\u00d7 faster than similarly performing\nnetworks with non-Inception architecture, however this requires careful manual design at this point.\n5\nGoogLeNet\nWe chose GoogLeNet as our team-name in the ILSVRC14 competition. This name is an homage to\nYann LeCuns pioneering LeNet 5 network [10]. We also use GoogLeNet to refer to the particular\nincarnation of the Inception architecture used in our submission for the competition. We have also\nused a deeper and wider Inception network, the quality of which was slightly inferior, but adding it\nto the ensemble seemed to improve the results marginally. We omit the details of that network, since\nour experiments have shown that the in\ufb02uence of the exact architectural parameters is relatively\n5\ntype\npatch size/\nstride\noutput\nsize\ndepth\n#1\u00d71\n#3\u00d73\nreduce\n#3\u00d73\n#5\u00d75\nreduce\n#5\u00d75\npool\nproj\nparams\nops\nconvolution\n7\u00d77/2\n112\u00d7112\u00d764\n1\n2.7K\n34M\nmax pool\n3\u00d73/2\n56\u00d756\u00d764\n0\nconvolution\n3\u00d73/1\n56\u00d756\u00d7192\n2\n64\n192\n112K\n360M\nmax pool\n3\u00d73/2\n28\u00d728\u00d7192\n0\ninception (3a)\n28\u00d728\u00d7256\n2\n64\n96\n128\n16\n32\n32\n159K\n128M\ninception (3b)\n28\u00d728\u00d7480\n2\n128\n128\n192\n32\n96\n64\n380K\n304M\nmax pool\n3\u00d73/2\n14\u00d714\u00d7480\n0\ninception (4a)\n14\u00d714\u00d7512\n2\n192\n96\n208\n16\n48\n64\n364K\n73M\ninception (4b)\n14\u00d714\u00d7512\n2\n160\n112\n224\n24\n64\n64\n437K\n88M\ninception (4c)\n14\u00d714\u00d7512\n2\n128\n128\n256\n24\n64\n64\n463K\n100M\ninception (4d)\n14\u00d714\u00d7528\n2\n112\n144\n288\n32\n64\n64\n580K\n119M\ninception (4e)\n14\u00d714\u00d7832\n2\n256\n160\n320\n32\n128\n128\n840K\n170M\nmax pool\n3\u00d73/2\n7\u00d77\u00d7832\n0\ninception (5a)\n7\u00d77\u00d7832\n2\n256\n160\n320\n32\n128\n128\n1072K\n54M\ninception (5b)\n7\u00d77\u00d71024\n2\n384\n192\n384\n48\n128\n128\n1388K\n71M\navg pool\n7\u00d77/1\n1\u00d71\u00d71024\n0\ndropout (40%)\n1\u00d71\u00d71024\n0\nlinear\n1\u00d71\u00d71000\n1\n1000K\n1M\nsoftmax\n1\u00d71\u00d71000\n0\nTable 1: GoogLeNet incarnation of the Inception architecture\nminor. Here, the most successful particular instance (named GoogLeNet) is described in Table 1 for\ndemonstrational purposes. The exact same topology (trained with different sampling methods) was\nused for 6 out of the 7 models in our ensemble.\nAll the convolutions, including those inside the Inception modules, use recti\ufb01ed linear activation.\nThe size of the receptive \ufb01eld in our network is 224\u00d7224 taking RGB color channels with mean sub-\ntraction. \u201c#3\u00d73 reduce\u201d and \u201c#5\u00d75 reduce\u201d stands for the number of 1\u00d71 \ufb01lters in the reduction\nlayer used before the 3\u00d73 and 5\u00d75 convolutions. One can see the number of 1\u00d71 \ufb01lters in the pro-\njection layer after the built-in max-pooling in the pool proj column. All these reduction/projection\nlayers use recti\ufb01ed linear activation as well.\nThe network was designed with computational ef\ufb01ciency and practicality in mind, so that inference\ncan be run on individual devices including even those with limited computational resources, espe-\ncially with low-memory footprint. The network is 22 layers deep when counting only layers with\nparameters (or 27 layers if we also count pooling). The overall number of layers (independent build-\ning blocks) used for the construction of the network is about 100. However this number depends on\nthe machine learning infrastructure system used. The use of average pooling before the classi\ufb01er is\nbased on [12], although our implementation differs in that we use an extra linear layer. This enables\nadapting and \ufb01ne-tuning our networks for other label sets easily, but it is mostly convenience and\nwe do not expect it to have a major effect. It was found that a move from fully connected layers to\naverage pooling improved the top-1 accuracy by about 0.6%, however the use of dropout remained\nessential even after removing the fully connected layers.\nGiven the relatively large depth of the network, the ability to propagate gradients back through all the\nlayers in an effective manner was a concern. One interesting insight is that the strong performance\nof relatively shallower networks on this task suggests that the features produced by the layers in the\nmiddle of the network should be very discriminative. By adding auxiliary classi\ufb01ers connected to\nthese intermediate layers, we would expect to encourage discrimination in the lower stages in the\nclassi\ufb01er, increase the gradient signal that gets propagated back, and provide additional regulariza-\ntion. These classi\ufb01ers take the form of smaller convolutional networks put on top of the output of\nthe Inception (4a) and (4d) modules. During training, their loss gets added to the total loss of the\nnetwork with a discount weight (the losses of the auxiliary classi\ufb01ers were weighted by 0.3). At\ninference time, these auxiliary networks are discarded.\nThe exact structure of the extra network on the side, including the auxiliary classi\ufb01er, is as follows:\n\u2022 An average pooling layer with 5\u00d75 \ufb01lter size and stride 3, resulting in an 4\u00d74\u00d7512 output\nfor the (4a), and 4\u00d74\u00d7528 for the (4d) stage.\n6\ninput\nConv\n7x7+2(S)\nMaxPool\n3x3+2(S)\nLocalRespNorm\nConv\n1x1+1(V)\nConv\n3x3+1(S)\nLocalRespNorm\nMaxPool\n3x3+2(S)\nConv\n1x1+1(S)\nConv\n1x1+1(S)\nConv\n1x1+1(S)\nMaxPool\n3x3+1(S)\nDepthConcat\nConv\n3x3+1(S)\nConv\n5x5+1(S)\nConv\n1x1+1(S)\nConv\n1x1+1(S)\nConv\n1x1+1(S)\nConv\n1x1+1(S)\nMaxPool\n3x3+1(S)\nDepthConcat\nConv\n3x3+1(S)\nConv\n5x5+1(S)\nConv\n1x1+1(S)\nMaxPool\n3x3+2(S)\nConv\n1x1+1(S)\nConv\n1x1+1(S)\nConv\n1x1+1(S)\nMaxPool\n3x3+1(S)\nDepthConcat\nConv\n3x3+1(S)\nConv\n5x5+1(S)\nConv\n1x1+1(S)\nConv\n1x1+1(S)\nConv\n1x1+1(S)\nConv\n1x1+1(S)\nMaxPool\n3x3+1(S)\nAveragePool\n5x5+3(V)\nDepthConcat\nConv\n3x3+1(S)\nConv\n5x5+1(S)\nConv\n1x1+1(S)\nConv\n1x1+1(S)\nConv\n1x1+1(S)\nConv\n1x1+1(S)\nMaxPool\n3x3+1(S)\nDepthConcat\nConv\n3x3+1(S)\nConv\n5x5+1(S)\nConv\n1x1+1(S)\nConv\n1x1+1(S)\nConv\n1x1+1(S)\nConv\n1x1+1(S)\nMaxPool\n3x3+1(S)\nDepthConcat\nConv\n3x3+1(S)\nConv\n5x5+1(S)\nConv\n1x1+1(S)\nConv\n1x1+1(S)\nConv\n1x1+1(S)\nConv\n1x1+1(S)\nMaxPool\n3x3+1(S)\nAveragePool\n5x5+3(V)\nDepthConcat\nConv\n3x3+1(S)\nConv\n5x5+1(S)\nConv\n1x1+1(S)\nMaxPool\n3x3+2(S)\nConv\n1x1+1(S)\nConv\n1x1+1(S)\nConv\n1x1+1(S)\nMaxPool\n3x3+1(S)\nDepthConcat\nConv\n3x3+1(S)\nConv\n5x5+1(S)\nConv\n1x1+1(S)\nConv\n1x1+1(S)\nConv\n1x1+1(S)\nConv\n1x1+1(S)\nMaxPool\n3x3+1(S)\nDepthConcat\nConv\n3x3+1(S)\nConv\n5x5+1(S)\nConv\n1x1+1(S)\nAveragePool\n7x7+1(V)\nFC\nConv\n1x1+1(S)\nFC\nFC\nSoftmaxActivation\nsoftmax0\nConv\n1x1+1(S)\nFC\nFC\nSoftmaxActivation\nsoftmax1\nSoftmaxActivation\nsoftmax2\nFigure 3: GoogLeNet network with all the bells and whistles\n7\n\u2022 A 1\u00d71 convolution with 128 \ufb01lters for dimension reduction and recti\ufb01ed linear activation.\n\u2022 A fully connected layer with 1024 units and recti\ufb01ed linear activation.\n\u2022 A dropout layer with 70% ratio of dropped outputs.\n\u2022 A linear layer with softmax loss as the classi\ufb01er (predicting the same 1000 classes as the\nmain classi\ufb01er, but removed at inference time).\nA schematic view of the resulting network is depicted in Figure 3.\n6\nTraining Methodology\nOur networks were trained using the DistBelief [4] distributed machine learning system using mod-\nest amount of model and data-parallelism. Although we used CPU based implementation only, a\nrough estimate suggests that the GoogLeNet network could be trained to convergence using few\nhigh-end GPUs within a week, the main limitation being the memory usage. Our training used\nasynchronous stochastic gradient descent with 0.9 momentum [17], \ufb01xed learning rate schedule (de-\ncreasing the learning rate by 4% every 8 epochs). Polyak averaging [13] was used to create the \ufb01nal\nmodel used at inference time.\nOur image sampling methods have changed substantially over the months leading to the competition,\nand already converged models were trained on with other options, sometimes in conjunction with\nchanged hyperparameters, like dropout and learning rate, so it is hard to give a de\ufb01nitive guidance\nto the most effective single way to train these networks. To complicate matters further, some of\nthe models were mainly trained on smaller relative crops, others on larger ones, inspired by [8].\nStill, one prescription that was veri\ufb01ed to work very well after the competition includes sampling\nof various sized patches of the image whose size is distributed evenly between 8% and 100% of the\nimage area and whose aspect ratio is chosen randomly between 3/4 and 4/3. Also, we found that the\nphotometric distortions by Andrew Howard [8] were useful to combat over\ufb01tting to some extent. In\naddition, we started to use random interpolation methods (bilinear, area, nearest neighbor and cubic,\nwith equal probability) for resizing relatively late and in conjunction with other hyperparameter\nchanges, so we could not tell de\ufb01nitely whether the \ufb01nal results were affected positively by their\nuse.\n7\nILSVRC 2014 Classi\ufb01cation Challenge Setup and Results\nThe ILSVRC 2014 classi\ufb01cation challenge involves the task of classifying the image into one of\n1000 leaf-node categories in the Imagenet hierarchy. There are about 1.2 million images for training,\n50,000 for validation and 100,000 images for testing. Each image is associated with one ground\ntruth category, and performance is measured based on the highest scoring classi\ufb01er predictions.\nTwo numbers are usually reported: the top-1 accuracy rate, which compares the ground truth against\nthe \ufb01rst predicted class, and the top-5 error rate, which compares the ground truth against the \ufb01rst\n5 predicted classes: an image is deemed correctly classi\ufb01ed if the ground truth is among the top-5,\nregardless of its rank in them. The challenge uses the top-5 error rate for ranking purposes.\nWe participated in the challenge with no external data used for training. In addition to the training\ntechniques aforementioned in this paper, we adopted a set of techniques during testing to obtain a\nhigher performance, which we elaborate below.\n1. We independently trained 7 versions of the same GoogLeNet model (including one wider\nversion), and performed ensemble prediction with them. These models were trained with\nthe same initialization (even with the same initial weights, mainly because of an oversight)\nand learning rate policies, and they only differ in sampling methodologies and the random\norder in which they see input images.\n2. During testing, we adopted a more aggressive cropping approach than that of Krizhevsky et\nal. [9]. Speci\ufb01cally, we resize the image to 4 scales where the shorter dimension (height or\nwidth) is 256, 288, 320 and 352 respectively, take the left, center and right square of these\nresized images (in the case of portrait images, we take the top, center and bottom squares).\nFor each square, we then take the 4 corners and the center 224\u00d7224 crop as well as the\n8\nTeam\nYear\nPlace\nError (top-5)\nUses external data\nSuperVision\n2012\n1st\n16.4%\nno\nSuperVision\n2012\n1st\n15.3%\nImagenet 22k\nClarifai\n2013\n1st\n11.7%\nno\nClarifai\n2013\n1st\n11.2%\nImagenet 22k\nMSRA\n2014\n3rd\n7.35%\nno\nVGG\n2014\n2nd\n7.32%\nno\nGoogLeNet\n2014\n1st\n6.67%\nno\nTable 2: Classi\ufb01cation performance\nNumber of models\nNumber of Crops\nCost\nTop-5 error\ncompared to base\n1\n1\n1\n10.07%\nbase\n1\n10\n10\n9.15%\n-0.92%\n1\n144\n144\n7.89%\n-2.18%\n7\n1\n7\n8.09%\n-1.98%\n7\n10\n70\n7.62%\n-2.45%\n7\n144\n1008\n6.67%\n-3.45%\nTable 3: GoogLeNet classi\ufb01cation performance break down\nsquare resized to 224\u00d7224, and their mirrored versions. This results in 4\u00d73\u00d76\u00d72 = 144\ncrops per image. A similar approach was used by Andrew Howard [8] in the previous year\u2019s\nentry, which we empirically veri\ufb01ed to perform slightly worse than the proposed scheme.\nWe note that such aggressive cropping may not be necessary in real applications, as the\nbene\ufb01t of more crops becomes marginal after a reasonable number of crops are present (as\nwe will show later on).\n3. The softmax probabilities are averaged over multiple crops and over all the individual clas-\nsi\ufb01ers to obtain the \ufb01nal prediction. In our experiments we analyzed alternative approaches\non the validation data, such as max pooling over crops and averaging over classi\ufb01ers, but\nthey lead to inferior performance than the simple averaging.\nIn the remainder of this paper, we analyze the multiple factors that contribute to the overall perfor-\nmance of the \ufb01nal submission.\nOur \ufb01nal submission in the challenge obtains a top-5 error of 6.67% on both the validation and\ntesting data, ranking the \ufb01rst among other participants. This is a 56.5% relative reduction compared\nto the SuperVision approach in 2012, and about 40% relative reduction compared to the previous\nyear\u2019s best approach (Clarifai), both of which used external data for training the classi\ufb01ers. The\nfollowing table shows the statistics of some of the top-performing approaches.\nWe also analyze and report the performance of multiple testing choices, by varying the number of\nmodels and the number of crops used when predicting an image in the following table. When we\nuse one model, we chose the one with the lowest top-1 error rate on the validation data. All numbers\nare reported on the validation dataset in order to not over\ufb01t to the testing data statistics.\n8\nILSVRC 2014 Detection Challenge Setup and Results\nThe ILSVRC detection task is to produce bounding boxes around objects in images among 200\npossible classes. Detected objects count as correct if they match the class of the groundtruth and\ntheir bounding boxes overlap by at least 50% (using the Jaccard index). Extraneous detections count\nas false positives and are penalized. Contrary to the classi\ufb01cation task, each image may contain\n9\nTeam\nYear\nPlace\nmAP\nexternal data\nensemble\napproach\nUvA-Euvision\n2013\n1st\n22.6%\nnone\n?\nFisher vectors\nDeep Insight\n2014\n3rd\n40.5%\nImageNet 1k\n3\nCNN\nCUHK DeepID-Net\n2014\n2nd\n40.7%\nImageNet 1k\n?\nCNN\nGoogLeNet\n2014\n1st\n43.9%\nImageNet 1k\n6\nCNN\nTable 4: Detection performance\nTeam\nmAP\nContextual model\nBounding box regression\nTrimps-Soushen\n31.6%\nno\n?\nBerkeley Vision\n34.5%\nno\nyes\nUvA-Euvision\n35.4%\n?\n?\nCUHK DeepID-Net2\n37.7%\nno\n?\nGoogLeNet\n38.02%\nno\nno\nDeep Insight\n40.2%\nyes\nyes\nTable 5: Single model performance for detection\nmany objects or none, and their scale may vary from large to tiny. Results are reported using the\nmean average precision (mAP).\nThe approach taken by GoogLeNet for detection is similar to the R-CNN by [6], but is augmented\nwith the Inception model as the region classi\ufb01er. Additionally, the region proposal step is improved\nby combining the Selective Search [20] approach with multi-box [5] predictions for higher object\nbounding box recall. In order to cut down the number of false positives, the superpixel size was\nincreased by 2\u00d7. This halves the proposals coming from the selective search algorithm. We added\nback 200 region proposals coming from multi-box [5] resulting, in total, in about 60% of the pro-\nposals used by [6], while increasing the coverage from 92% to 93%. The overall effect of cutting the\nnumber of proposals with increased coverage is a 1% improvement of the mean average precision\nfor the single model case. Finally, we use an ensemble of 6 ConvNets when classifying each region\nwhich improves results from 40% to 43.9% accuracy. Note that contrary to R-CNN, we did not use\nbounding box regression due to lack of time.\nWe \ufb01rst report the top detection results and show the progress since the \ufb01rst edition of the detection\ntask. Compared to the 2013 result, the accuracy has almost doubled. The top performing teams all\nuse Convolutional Networks. We report the of\ufb01cial scores in Table 4 and common strategies for each\nteam: the use of external data, ensemble models or contextual models. The external data is typically\nthe ILSVRC12 classi\ufb01cation data for pre-training a model that is later re\ufb01ned on the detection data.\nSome teams also mention the use of the localization data. Since a good portion of the localization\ntask bounding boxes are not included in the detection dataset, one can pre-train a general bounding\nbox regressor with this data the same way classi\ufb01cation is used for pre-training. The GoogLeNet\nentry did not use the localization data for pretraining.\nIn Table 5, we compare results using a single model only. The top performing model is by Deep\nInsight and surprisingly only improves by 0.3 points with an ensemble of 3 models while the\nGoogLeNet obtains signi\ufb01cantly stronger results with the ensemble.\n9\nConclusions\nOur results seem to yield a solid evidence that approximating the expected optimal sparse structure\nby readily available dense building blocks is a viable method for improving neural networks for\ncomputer vision. The main advantage of this method is a signi\ufb01cant quality gain at a modest in-\ncrease of computational requirements compared to shallower and less wide networks. Also note that\nour detection work was competitive despite of neither utilizing context nor performing bounding box\n10\nregression and this fact provides further evidence of the strength of the Inception architecture. Al-\nthough it is expected that similar quality of result can be achieved by much more expensive networks\nof similar depth and width, our approach yields solid evidence that moving to sparser architectures\nis feasible and useful idea in general. This suggest promising future work towards creating sparser\nand more re\ufb01ned structures in automated ways on the basis of [2].\n10\nAcknowledgements\nWe would like to thank Sanjeev Arora and Aditya Bhaskara for fruitful discussions on [2]. Also\nwe are indebted to the DistBelief [4] team for their support especially to Rajat Monga, Jon Shlens,\nAlex Krizhevsky, Jeff Dean, Ilya Sutskever and Andrea Frome. We would also like to thank to Tom\nDuerig and Ning Ye for their help on photometric distortions. Also our work would not have been\npossible without the support of Chuck Rosenberg and Hartwig Adam.\nReferences\n[1] Know your meme: We need to go deeper.\nhttp://knowyourmeme.com/memes/\nwe-need-to-go-deeper. Accessed: 2014-09-15.\n[2] Sanjeev Arora, Aditya Bhaskara, Rong Ge, and Tengyu Ma. Provable bounds for learning\nsome deep representations. CoRR, abs/1310.6343, 2013.\n[3] \u00a8Umit V. C\u00b8 ataly\u00a8urek, Cevdet Aykanat, and Bora Uc\u00b8ar. On two-dimensional sparse matrix par-\ntitioning: Models, methods, and a recipe. SIAM J. Sci. Comput., 32(2):656\u2013683, February\n2010.\n[4] Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao,\nMarc\u2019aurelio Ranzato, Andrew Senior, Paul Tucker, Ke Yang, Quoc V. Le, and Andrew Y.\nNg. Large scale distributed deep networks. In P. Bartlett, F.c.n. Pereira, C.j.c. Burges, L. Bot-\ntou, and K.q. Weinberger, editors, Advances in Neural Information Processing Systems 25,\npages 1232\u20131240. 2012.\n[5] Dumitru Erhan, Christian Szegedy, Alexander Toshev, and Dragomir Anguelov. Scalable ob-\nject detection using deep neural networks. In Computer Vision and Pattern Recognition, 2014.\nCVPR 2014. IEEE Conference on, 2014.\n[6] Ross B. Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies\nfor accurate object detection and semantic segmentation.\nIn Computer Vision and Pattern\nRecognition, 2014. CVPR 2014. IEEE Conference on, 2014.\n[7] Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhut-\ndinov. Improving neural networks by preventing co-adaptation of feature detectors. CoRR,\nabs/1207.0580, 2012.\n[8] Andrew G. Howard. Some improvements on deep convolutional neural network based image\nclassi\ufb01cation. CoRR, abs/1312.5402, 2013.\n[9] Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton. Imagenet classi\ufb01cation with deep con-\nvolutional neural networks. In Advances in Neural Information Processing Systems 25, pages\n1106\u20131114, 2012.\n[10] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel.\nBackpropagation applied to handwritten zip code recognition. Neural Comput., 1(4):541\u2013551,\nDecember 1989.\n[11] Yann LeCun, L\u00b4eon Bottou, Yoshua Bengio, and Patrick Haffner.\nGradient-based learning\napplied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.\n[12] Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. CoRR, abs/1312.4400, 2013.\n[13] B. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM\nJ. Control Optim., 30(4):838\u2013855, July 1992.\n[14] Pierre Sermanet, David Eigen, Xiang Zhang, Micha\u00a8el Mathieu, Rob Fergus, and Yann Le-\nCun. Overfeat: Integrated recognition, localization and detection using convolutional net-\nworks. CoRR, abs/1312.6229, 2013.\n11\n[15] Thomas Serre, Lior Wolf, Stanley M. Bileschi, Maximilian Riesenhuber, and Tomaso Poggio.\nRobust object recognition with cortex-like mechanisms. IEEE Trans. Pattern Anal. Mach.\nIntell., 29(3):411\u2013426, 2007.\n[16] Fengguang Song and Jack Dongarra.\nScaling up matrix computations on shared-memory\nmanycore systems with 1000 cpu cores. In Proceedings of the 28th ACM International Con-\nference on Supercomputing, ICS \u201914, pages 333\u2013342, New York, NY, USA, 2014. ACM.\n[17] Ilya Sutskever, James Martens, George E. Dahl, and Geoffrey E. Hinton. On the importance\nof initialization and momentum in deep learning. In Proceedings of the 30th International\nConference on Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013, volume 28\nof JMLR Proceedings, pages 1139\u20131147. JMLR.org, 2013.\n[18] Christian Szegedy, Alexander Toshev, and Dumitru Erhan. Deep neural networks for object\ndetection.\nIn Christopher J. C. Burges, L\u00b4eon Bottou, Zoubin Ghahramani, and Kilian Q.\nWeinberger, editors, Advances in Neural Information Processing Systems 26: 27th Annual\nConference on Neural Information Processing Systems 2013. Proceedings of a meeting held\nDecember 5-8, 2013, Lake Tahoe, Nevada, United States., pages 2553\u20132561, 2013.\n[19] Alexander Toshev and Christian Szegedy. Deeppose: Human pose estimation via deep neural\nnetworks. CoRR, abs/1312.4659, 2013.\n[20] Koen E. A. van de Sande, Jasper R. R. Uijlings, Theo Gevers, and Arnold W. M. Smeulders.\nSegmentation as selective search for object recognition. In Proceedings of the 2011 Interna-\ntional Conference on Computer Vision, ICCV \u201911, pages 1879\u20131886, Washington, DC, USA,\n2011. IEEE Computer Society.\n[21] Matthew D. Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In\nDavid J. Fleet, Tom\u00b4as Pajdla, Bernt Schiele, and Tinne Tuytelaars, editors, Computer Vision\n- ECCV 2014 - 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Pro-\nceedings, Part I, volume 8689 of Lecture Notes in Computer Science, pages 818\u2013833. Springer,\n2014.\n12\n",
        "sentence": " It was possible to train this autoencoder without pre-training because of the additional gradient information injected by the supervised and covariance cost at the end of the encoder similar to Szegedy et al. (2014). In Figure 8, we show the images generated by the decoder while iterating through each camera pose.",
        "context": "creasing the learning rate by 4% every 8 epochs). Polyak averaging [13] was used to create the \ufb01nal\nmodel used at inference time.\nOur image sampling methods have changed substantially over the months leading to the competition,\nand already converged models were trained on with other options, sometimes in conjunction with\nchanged hyperparameters, like dropout and learning rate, so it is hard to give a de\ufb01nitive guidance\nfor accurate object detection and semantic segmentation.\nIn Computer Vision and Pattern\nRecognition, 2014. CVPR 2014. IEEE Conference on, 2014.\n[7] Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhut-"
    },
    {
        "title": "Separating style and content with bilinear models",
        "author": [
            "Tenenbaum",
            "Joshua B",
            "Freeman",
            "William T"
        ],
        "venue": "Neural computation,",
        "citeRegEx": "Tenenbaum et al\\.,? \\Q2000\\E",
        "shortCiteRegEx": "Tenenbaum et al\\.",
        "year": 2000,
        "abstract": " Perceptual systems routinely separate \u201ccontent\u201d from \u201cstyle,\u201d classifying familiar words spoken in an unfamiliar accent, identifying a font or handwriting style across letters, or recognizing a familiar face or object seen under unfamiliar viewing conditions. Yet a general and tractable computational model of this ability to untangle the underlying factors of perceptual observations remains elusive (Hofstadter, 1985). Existing factor models (Mardia, Kent, &amp; Bibby, 1979; Hinton &amp; Zemel, 1994; Ghahramani, 1995; Bell &amp; Sejnowski, 1995; Hinton, Dayan, Frey, &amp; Neal, 1995; Dayan, Hinton, Neal, &amp; Zemel, 1995; Hinton &amp; Ghahramani, 1997) are either insufficiently rich to capture the complex interactions of perceptually meaningful factors such as phoneme and speaker accent or letter and font, or do not allow efficient learning algorithms. We present a general framework for learning to solve two-factor tasks using bilinear models, which provide sufficiently expressive representations of factor interactions but can nonetheless be fit to data using efficient algorithms based on the singular value decomposition and expectation-maximization. We report promising results on three different tasks in three different perceptual domains: spoken vowel classification with a benchmark multi-speaker database, extrapolation of fonts to unseen letters, and translation of faces to novel illuminants. ",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion",
        "author": [
            "Vincent",
            "Pascal",
            "Larochelle",
            "Hugo",
            "Lajoie",
            "Isabelle",
            "Bengio",
            "Yoshua",
            "Manzagol",
            "Pierre-Antoine"
        ],
        "venue": "The Journal of Machine Learning Research,",
        "citeRegEx": "Vincent et al\\.,? \\Q2010\\E",
        "shortCiteRegEx": "Vincent et al\\.",
        "year": 2010,
        "abstract": "",
        "full_text": "",
        "sentence": " Autoencoder models have been shown to be useful for a variety of machine learning tasks (Rifai et al., 2011; Vincent et al., 2010; Le, 2013).",
        "context": null
    },
    {
        "title": "Adadelta: An adaptive learning rate method",
        "author": [
            "Zeiler",
            "Matthew D"
        ],
        "venue": "arXiv preprint arXiv:1212.5701,",
        "citeRegEx": "Zeiler and D.,? \\Q2012\\E",
        "shortCiteRegEx": "Zeiler and D.",
        "year": 2012,
        "abstract": "We present a novel per-dimension learning rate method for gradient descent\ncalled ADADELTA. The method dynamically adapts over time using only first order\ninformation and has minimal computational overhead beyond vanilla stochastic\ngradient descent. The method requires no manual tuning of a learning rate and\nappears robust to noisy gradient information, different model architecture\nchoices, various data modalities and selection of hyperparameters. We show\npromising results compared to other methods on the MNIST digit classification\ntask using a single machine and on a large scale voice dataset in a distributed\ncluster environment.",
        "full_text": "ADADELTA: AN ADAPTIVE LEARNING RATE METHOD\nMatthew D. Zeiler1,2\u2217\n1Google Inc., USA\n2New York University, USA\nABSTRACT\nWe present a novel per-dimension learning rate method for\ngradient descent called ADADELTA. The method dynami-\ncally adapts over time using only \ufb01rst order information and\nhas minimal computational overhead beyond vanilla stochas-\ntic gradient descent. The method requires no manual tuning of\na learning rate and appears robust to noisy gradient informa-\ntion, different model architecture choices, various data modal-\nities and selection of hyperparameters. We show promising\nresults compared to other methods on the MNIST digit clas-\nsi\ufb01cation task using a single machine and on a large scale\nvoice dataset in a distributed cluster environment.\nIndex Terms\u2014 Adaptive Learning Rates, Machine Learn-\ning, Neural Networks, Gradient Descent\n1. INTRODUCTION\nThe aim of many machine learning methods is to update a\nset of parameters x in order to optimize an objective function\nf(x). This often involves some iterative procedure which ap-\nplies changes to the parameters, \u2206x at each iteration of the\nalgorithm. Denoting the parameters at the t-th iteration as xt,\nthis simple update rule becomes:\nxt+1 = xt + \u2206xt\n(1)\nIn this paper we consider gradient descent algorithms which\nattempt to optimize the objective function by following the\nsteepest descent direction given by the negative of the gradi-\nent gt. This general approach can be applied to update any\nparameters for which a derivative can be obtained:\n\u2206xt = \u2212\u03b7gt\n(2)\nwhere gt is the gradient of the parameters at the t-th iteration\n\u2202f(xt)\n\u2202xt\nand \u03b7 is a learning rate which controls how large of\na step to take in the direction of the negative gradient. Fol-\nlowing this negative gradient for each new sample or batch\nof samples chosen from the dataset gives a local estimate\nof which direction minimizes the cost and is referred to as\nstochastic gradient descent (SGD) [1]. While often simple to\nderive the gradients for each parameter analytically, the gradi-\nent descent algorithm requires the learning rate hyperparam-\neter to be chosen.\n\u2217This work was done while Matthew D. Zeiler was an intern at Google.\nSetting the learning rate typically involves a tuning pro-\ncedure in which the highest possible learning rate is chosen\nby hand. Choosing higher than this rate can cause the system\nto diverge in terms of the objective function, and choosing\nthis rate too low results in slow learning. Determining a good\nlearning rate becomes more of an art than science for many\nproblems.\nThis work attempts to alleviate the task of choosing a\nlearning rate by introducing a new dynamic learning rate that\nis computed on a per-dimension basis using only \ufb01rst order\ninformation. This requires a trivial amount of extra compu-\ntation per iteration over gradient descent. Additionally, while\nthere are some hyper parameters in this method, we has found\ntheir selection to not drastically alter the results. The bene\ufb01ts\nof this approach are as follows:\n\u2022 no manual setting of a learning rate.\n\u2022 insensitive to hyperparameters.\n\u2022 separate dynamic learning rate per-dimension.\n\u2022 minimal computation over gradient descent.\n\u2022 robust to large gradients, noise and architecture choice.\n\u2022 applicable in both local or distributed environments.\n2. RELATED WORK\nThere are many modi\ufb01cations to the gradient descent algo-\nrithm.\nThe most powerful such modi\ufb01cation is Newton\u2019s\nmethod which requires second order derivatives of the cost\nfunction:\n\u2206xt = H\u22121\nt\ngt\n(3)\nwhere H\u22121\nt\nis the inverse of the Hessian matrix of second\nderivatives computed at iteration t. This determines the op-\ntimal step size to take for quadratic problems, but unfortu-\nnately is prohibitive to compute in practice for large models.\nTherefore, many additional approaches have been proposed\nto either improve the use of \ufb01rst order information or to ap-\nproximate the second order information.\n2.1. Learning Rate Annealing\nThere have been several attempts to use heuristics for estimat-\ning a good learning rate at each iteration of gradient descent.\nThese either attempt to speed up learning when suitable or to\nslow down learning near a local minima. Here we consider\nthe latter.\narXiv:1212.5701v1  [cs.LG]  22 Dec 2012\nWhen gradient descent nears a minima in the cost sur-\nface, the parameter values can oscillate back and forth around\nthe minima. One method to prevent this is to slow down the\nparameter updates by decreasing the learning rate. This can\nbe done manually when the validation accuracy appears to\nplateau. Alternatively, learning rate schedules have been pro-\nposed [1] to automatically anneal the learning rate based on\nhow many epochs through the data have been done. These ap-\nproaches typically add additional hyperparameters to control\nhow quickly the learning rate decays.\n2.2. Per-Dimension First Order Methods\nThe heuristic annealing procedure discussed above modi\ufb01es\na single global learning rate that applies to all dimensions of\nthe parameters. Since each dimension of the parameter vector\ncan relate to the overall cost in completely different ways,\na per-dimension learning rate that can compensate for these\ndifferences is often advantageous.\n2.2.1. Momentum\nOne method of speeding up training per-dimension is the mo-\nmentum method [2]. This is perhaps the simplest extension to\nSGD that has been successfully used for decades. The main\nidea behind momentum is to accelerate progress along dimen-\nsions in which gradient consistently point in the same direc-\ntion and to slow progress along dimensions where the sign\nof the gradient continues to change. This is done by keeping\ntrack of past parameter updates with an exponential decay:\n\u2206xt = \u03c1\u2206xt\u22121 \u2212\u03b7gt\n(4)\nwhere \u03c1 is a constant controlling the decay of the previous\nparameter updates. This gives a nice intuitive improvement\nover SGD when optimizing dif\ufb01cult cost surfaces such as a\nlong narrow valley. The gradients along the valley, despite\nbeing much smaller than the gradients across the valley, are\ntypically in the same direction and thus the momentum term\naccumulates to speed up progress. In SGD the progress along\nthe valley would be slow since the gradient magnitude is small\nand the \ufb01xed global learning rate shared by all dimensions\ncannot speed up progress. Choosing a higher learning rate\nfor SGD may help but the dimension across the valley would\nthen also make larger parameter updates which could lead\nto oscillations back as forth across the valley. These oscil-\nlations are mitigated when using momentum because the sign\nof the gradient changes and thus the momentum term damps\ndown these updates to slow progress across the valley. Again,\nthis occurs per-dimension and therefore the progress along the\nvalley is unaffected.\n2.2.2. ADAGRAD\nA recent \ufb01rst order method called ADAGRAD [3] has shown\nremarkably good results on large scale learning tasks in a dis-\ntributed environment [4]. This method relies on only \ufb01rst\norder information but has some properties of second order\nmethods and annealing. The update rule for ADAGRAD is\nas follows:\n\u2206xt = \u2212\n\u03b7\nqPt\n\u03c4=1 g2\u03c4\ngt\n(5)\nHere the denominator computes the \u21132 norm of all previous\ngradients on a per-dimension basis and \u03b7 is a global learning\nrate shared by all dimensions.\nWhile there is the hand tuned global learning rate, each\ndimension has its own dynamic rate. Since this dynamic rate\ngrows with the inverse of the gradient magnitudes, large gra-\ndients have smaller learning rates and small gradients have\nlarge learning rates. This has the nice property, as in second\norder methods, that the progress along each dimension evens\nout over time. This is very bene\ufb01cial for training deep neu-\nral networks since the scale of the gradients in each layer is\noften different by several orders of magnitude, so the optimal\nlearning rate should take that into account. Additionally, this\naccumulation of gradient in the denominator has the same ef-\nfects as annealing, reducing the learning rate over time.\nSince the magnitudes of gradients are factored out in\nADAGRAD, this method can be sensitive to initial conditions\nof the parameters and the corresponding gradients. If the ini-\ntial gradients are large, the learning rates will be low for the\nremainder of training. This can be combatted by increasing\nthe global learning rate, making the ADAGRAD method sen-\nsitive to the choice of learning rate. Also, due to the continual\naccumulation of squared gradients in the denominator, the\nlearning rate will continue to decrease throughout training,\neventually decreasing to zero and stopping training com-\npletely. We created our ADADELTA method to overcome the\nsensitivity to the hyperparameter selection as well as to avoid\nthe continual decay of the learning rates.\n2.3. Methods Using Second Order Information\nWhereas the above methods only utilized gradient and func-\ntion evaluations in order to optimize the objective, second\norder methods such as Newton\u2019s method or quasi-Newtons\nmethods make use of the Hessian matrix or approximations\nto it. While this provides additional curvature information\nuseful for optimization, computing accurate second order in-\nformation is often expensive.\nSince computing the entire Hessian matrix of second\nderivatives is too computationally expensive for large models,\nBecker and LecCun [5] proposed a diagonal approximation to\nthe Hessian. This diagonal approximation can be computed\nwith one additional forward and back-propagation through\nthe model, effectively doubling the computation over SGD.\nOnce the diagonal of the Hessian is computed, diag(H), the\nupdate rule becomes:\n\u2206xt = \u2212\n1\n|diag(Ht)| + \u00b5 gt\n(6)\nwhere the absolute value of this diagonal Hessian is used to\nensure the negative gradient direction is always followed and\n\u00b5 is a small constant to improve the conditioning of the Hes-\nsian for regions of small curvature.\nA recent method by Schaul et al. [6] incorporating the\ndiagonal Hessian with ADAGRAD-like terms has been intro-\nduced to alleviate the need for hand speci\ufb01ed learning rates.\nThis method uses the following update rule:\n\u2206xt = \u2212\n1\n|diag(Ht)|\nE[gt\u2212w:t]2\nE[g2\nt\u2212w:t] gt\n(7)\nwhere E[gt\u2212w:t] is the expected value of the previous w gra-\ndients and E[g2\nt\u2212w:t] is the expected value of squared gradi-\nents over the same window w. Schaul et al. also introduce a\nheuristic for this window size w (see [6] for more details).\n3. ADADELTA METHOD\nThe idea presented in this paper was derived from ADA-\nGRAD [3] in order to improve upon the two main draw-\nbacks of the method: 1) the continual decay of learning rates\nthroughout training, and 2) the need for a manually selected\nglobal learning rate. After deriving our method we noticed\nseveral similarities to Schaul et al. [6], which will be com-\npared to below.\nIn the ADAGRAD method the denominator accumulates\nthe squared gradients from each iteration starting at the be-\nginning of training. Since each term is positive, this accumu-\nlated sum continues to grow throughout training, effectively\nshrinking the learning rate on each dimension. After many it-\nerations, this learning rate will become in\ufb01nitesimally small.\n3.1. Idea 1: Accumulate Over Window\nInstead of accumulating the sum of squared gradients over all\ntime, we restricted the window of past gradients that are ac-\ncumulated to be some \ufb01xed size w (instead of size t where\nt is the current iteration as in ADAGRAD). With this win-\ndowed accumulation the denominator of ADAGRAD cannot\naccumulate to in\ufb01nity and instead becomes a local estimate\nusing recent gradients. This ensures that learning continues\nto make progress even after many iterations of updates have\nbeen done.\nSince storing w previous squared gradients is inef\ufb01cient,\nour methods implements this accumulation as an exponen-\ntially decaying average of the squared gradients. Assume at\ntime t this running average is E[g2]t then we compute:\nE[g2]t = \u03c1 E[g2]t\u22121 + (1 \u2212\u03c1) g2\nt\n(8)\nwhere \u03c1 is a decay constant similar to that used in the momen-\ntum method. Since we require the square root of this quantity\nin the parameter updates, this effectively becomes the RMS\nof previous squared gradients up to time t:\nRMS[g]t =\np\nE[g2]t + \u03f5\n(9)\nwhere a constant \u03f5 is added to better condition the denomina-\ntor as in [5]. The resulting parameter update is then:\n\u2206xt = \u2212\n\u03b7\nRMS[g]t\ngt\n(10)\nAlgorithm 1 Computing ADADELTA update at time t\nRequire: Decay rate \u03c1, Constant \u03f5\nRequire: Initial parameter x1\n1: Initialize accumulation variables E[g2]0 = 0, E[\u2206x2]0 = 0\n2: for t = 1 : T do %% Loop over # of updates\n3:\nCompute Gradient: gt\n4:\nAccumulate Gradient: E[g2]t = \u03c1E[g2]t\u22121 + (1 \u2212\u03c1)g2\nt\n5:\nCompute Update: \u2206xt = \u2212\nRMS[\u2206x]t\u22121\nRMS[g]t\ngt\n6:\nAccumulate Updates: E[\u2206x2]t = \u03c1E[\u2206x2]t\u22121+(1\u2212\u03c1)\u2206x2\nt\n7:\nApply Update: xt+1 = xt + \u2206xt\n8: end for\n3.2. Idea 2: Correct Units with Hessian Approximation\nWhen considering the parameter updates, \u2206x, being applied\nto x, the units should match. That is, if the parameter had\nsome hypothetical units, the changes to the parameter should\nbe changes in those units as well. When considering SGD,\nMomentum, or ADAGRAD, we can see that this is not the\ncase. The units in SGD and Momentum relate to the gradient,\nnot the parameter:\nunits of \u2206x \u221dunits of g \u221d\u2202f\n\u2202x \u221d\n1\nunits of x\n(11)\nassuming the cost function, f, is unitless. ADAGRAD also\ndoes not have correct units since the update involves ratios of\ngradient quantities, hence the update is unitless.\nIn contrast, second order methods such as Newton\u2019s\nmethod that use Hessian information or an approximation\nto the Hessian do have the correct units for the parameter\nupdates:\n\u2206x \u221dH\u22121g \u221d\n\u2202f\n\u2202x\n\u22022f\n\u2202x2\n\u221dunits of x\n(12)\nNoticing this mismatch of units we considered terms to\nadd to Eqn. 10 in order for the units of the update to match\nthe units of the parameters. Since second order methods are\ncorrect, we rearrange Newton\u2019s method (assuming a diagonal\nHessian) for the inverse of the second derivative to determine\nthe quantities involved:\n\u2206x =\n\u2202f\n\u2202x\n\u22022f\n\u2202x2\n\u21d2\n1\n\u22022f\n\u2202x2\n= \u2206x\n\u2202f\n\u2202x\n(13)\nSince the RMS of the previous gradients is already repre-\nsented in the denominator in Eqn. 10 we considered a mea-\nsure of the \u2206x quantity in the numerator. \u2206xt for the current\ntime step is not known, so we assume the curvature is locally\nsmooth and approximate \u2206xt by compute the exponentially\ndecaying RMS over a window of size w of previous \u2206x to\ngive the ADADELTA method:\n\u2206xt = \u2212RMS[\u2206x]t\u22121\nRMS[g]t\ngt\n(14)\nwhere the same constant \u03f5 is added to the numerator RMS as\nwell. This constant serves the purpose both to start off the \ufb01rst\niteration where \u2206x0 = 0 and to ensure progress continues to\nbe made even if previous updates become small.\nThis derivation made the assumption of diagonal curva-\nture so that the second derivatives could easily be rearranged.\nFurthermore, this is an approximation to the diagonal Hessian\nusing only RMS measures of g and \u2206x. This approximation\nis always positive as in Becker and LeCun [5], ensuring the\nupdate direction follows the negative gradient at each step.\nIn Eqn. 14 the RMS[\u2206x]t\u22121 quantity lags behind the de-\nnominator by 1 time step, due to the recurrence relationship\nfor \u2206xt. An interesting side effect of this is that the system is\nrobust to large sudden gradients which act to increase the de-\nnominator, reducing the effective learning rate at the current\ntime step, before the numerator can react.\nThe method in Eqn. 14 uses only \ufb01rst order information\nand has some properties from each of the discussed meth-\nods. The negative gradient direction for the current iteration\n\u2212gt is always followed as in SGD. The numerator acts as\nan acceleration term, accumulating previous gradients over a\nwindow of time as in momentum. The denominator is re-\nlated to ADAGRAD in that the squared gradient information\nper-dimension helps to even out the progress made in each di-\nmension, but is computed over a window to ensure progress\nis made later in training. Finally, the method relates to Schaul\net al. \u2019s in that some approximation to the Hessian is made,\nbut instead costs only one gradient computation per iteration\nby leveraging information from past updates. For the com-\nplete algorithm details see Algorithm 1.\n4. EXPERIMENTS\nWe evaluate our method on two tasks using several different\nneural network architectures. We train the neural networks\nusing SGD, Momentum, ADAGRAD, and ADADELTA in a\nsupervised fashion to minimize the cross entropy objective\nbetween the network output and ground truth labels. Compar-\nisons are done both on a local computer and in a distributed\ncompute cluster.\n4.1. Handwritten Digit Classi\ufb01cation\nIn our \ufb01rst set of experiments we train a neural network on the\nMNIST handwritten digit classi\ufb01cation task. For comparison\nwith Schaul et al. \u2019s method we trained with tanh nonlinear-\nities and 500 hidden units in the \ufb01rst layer followed by 300\nhidden units in the second layer, with the \ufb01nal softmax out-\nput layer on top. Our method was trained on mini-batches of\n100 images per batch for 6 epochs through the training set.\nSetting the hyperparameters to \u03f5 = 1e \u22126 and \u03c1 = 0.95 we\nachieve 2.00% test set error compared to the 2.10% of Schaul\net al. While this is nowhere near convergence it gives a sense\nof how quickly the algorithms can optimize the classi\ufb01cation\nobjective.\n0\n10\n20\n30\n40\n50\n  1\n1.5\n  2\n2.5\n  3\n3.5\n  4\n4.5\n  5\n5.5\n  6\nEpoch\nTest Error %\n \n \nSGD\nMOMENTUM\nADAGRAD\nADADELTA\nFig. 1. Comparison of learning rate methods on MNIST digit\nclassi\ufb01cation for 50 epochs.\nTo further analyze various methods to convergence, we\ntrain the same neural network with 500 hidden units in the \ufb01rst\nlayer, 300 hidden units in the second layer and recti\ufb01ed linear\nactivation functions in both layers for 50 epochs. We notice\nthat recti\ufb01ed linear units work better in practice than tanh, and\ntheir non-saturating nature further tests each of the methods\nat coping with large variations of activations and gradients.\nIn Fig. 1 we compare SGD, Momentum, ADAGRAD,\nand ADADELTA in optimizing the test set errors. The unal-\ntered SGD method does the worst in this case, whereas adding\nthe momentum term to it signi\ufb01cantly improves performance.\nADAGRAD performs well for the \ufb01rst 10 epochs of training,\nafter which it slows down due to the accumulations in the de-\nnominator which continually increase. ADADELTA matches\nthe fast initial convergence of ADAGRAD while continuing\nto reduce the test error, converging near the best performance\nwhich occurs with momentum.\nSGD\nMOMENTUM\nADAGRAD\n\u03f5 = 1e0\n2.26%\n89.68%\n43.76%\n\u03f5 = 1e\u22121\n2.51%\n2.03%\n2.82%\n\u03f5 = 1e\u22122\n7.02%\n2.68%\n1.79%\n\u03f5 = 1e\u22123\n17.01%\n6.98%\n5.21%\n\u03f5 = 1e\u22124\n58.10%\n16.98%\n12.59%\nTable 1. MNIST test error rates after 6 epochs of training for\nvarious hyperparameter settings using SGD, MOMENTUM,\nand ADAGRAD.\n\u03c1 = 0.9\n\u03c1 = 0.95\n\u03c1 = 0.99\n\u03f5 = 1e\u22122\n2.59%\n2.58%\n2.32%\n\u03f5 = 1e\u22124\n2.05%\n1.99%\n2.28%\n\u03f5 = 1e\u22126\n1.90%\n1.83%\n2.05%\n\u03f5 = 1e\u22128\n2.29%\n2.13%\n2.00%\nTable 2. MNIST test error rate after 6 epochs for various\nhyperparameter settings using ADADELTA.\n4.2. Sensitivity to Hyperparameters\nWhile momentum converged to a better \ufb01nal solution than\nADADELTA after many epochs of training, it was very sen-\nsitive to the learning rate selection, as was SGD and ADA-\nGRAD. In Table 1 we vary the learning rates for each method\nand show the test set errors after 6 epochs of training using\nrecti\ufb01ed linear units as the activation function. The optimal\nsettings from each column were used to generate Fig. 1. With\nSGD, Momentum, or ADAGRAD the learning rate needs to\nbe set to the correct order of magnitude, above which the so-\nlutions typically diverge and below which the optimization\nproceeds slowly. We can see that these results are highly vari-\nable for each method, compared to ADADELTA in Table 2\nin which the two hyperparameters do not signi\ufb01cantly alter\nperformance.\n4.3. Effective Learning Rates\nTo investigate some of the properties of ADADELTA we plot\nin Fig. 2 the step sizes and parameter updates of 10 randomly\nselected dimensions in each of the 3 weight matrices through-\nout training. There are several interesting things evident in\nthis \ufb01gure. First, the step sizes, or effective learning rates (all\nterms except gt from Eqn. 14) shown in the left portion of the\n\ufb01gure are larger for the lower layers of the network and much\nsmaller for the top layer at the beginning of training. This\nproperty of ADADELTA helps to balance the fact that lower\nlayers have smaller gradients due to the diminishing gradi-\n0\n100\n200\n0\n0.5\n1\nd x1\n0\n100\n200\n0\n0.5\n1\nd x2\n0\n100\n200\n0\n0.5\n1\nd x3\n0\n100\n200\n\u22120.01\n0\n0.01\n6 x1\n0\n100\n200\n\u22120.01\n0\n0.01\n6 x2\n0\n100\n200\n\u22120.01\n0\n0.01\n6 x3\nFig. 2.\nStep sizes and parameter updates shown every 60\nbatches during training the MNIST network with tanh non-\nlinearities for 25 epochs. Left: Step sizes for 10 randomly\nselected dimensions of each of the 3 weight matrices of the\nnetwork. Right: Parameters changes for the same 10 dimen-\nsions for each of the 3 weight matrices. Note the large step\nsizes in lower layers that help compensate for vanishing gra-\ndients that occur with backpropagation.\nent problem in neural networks and thus should have larger\nlearning rates.\nSecondly, near the end of training these step sizes con-\nverge to 1. This is typically a high learning rate that would\nlead to divergence in most methods, however this conver-\ngence towards 1 only occurs near the end of training when the\ngradients and parameter updates are small. In this scenario,\nthe \u03f5 constants in the numerator and denominator dominate\nthe past gradients and parameter updates, converging to the\nlearning rate of 1.\nThis leads to the last interesting property of ADADELTA\nwhich is that when the step sizes become 1, the parameter\nupdates (shown on the right of Fig. 2) tend towards zero. This\noccurs smoothly for each of the weight matrices effectively\noperating as if an annealing schedule was present.\nHowever, having no explicit annealing schedule imposed\non the learning rate could be why momentum with the proper\nhyperparameters outperforms ADADELTA later in training as\nseen in Fig. 1. With momentum, oscillations that can occur\nnear a minima are smoothed out, whereas with ADADELTA\nthese can accumulate in the numerator. An annealing sched-\nule could possibly be added to the ADADELTA method to\ncounteract this in future work.\n4.4. Speech Data\nIn the next set of experiments we trained a large-scale neu-\nral network with 4 hidden layers on several hundred hours\nof US English data collected using Voice Search, Voice IME,\nand read data. The network was trained using the distributed\nsystem of [4] in which a centralized parameter server accu-\nmulates the gradient information reported back from several\nreplicas of the neural network. In our experiments we used ei-\nther 100 or 200 such replica networks to test the performance\nof ADADELTA in a highly distributed environment.\nThe neural network is setup as in [7] where the inputs\nare 26 frames of audio, each consisting of 40 log-energy \ufb01l-\nter bank outputs.\nThe outputs of the network were 8,000\nsenone labels produced from a GMM-HMM system using\nforced alignment with the input frames. Each hidden layer\nof the neural network had 2560 hidden units and was trained\nwith either logistic or recti\ufb01ed linear nonlinearities.\nFig. 3 shows the performance of the ADADELTA method\nwhen using 100 network replicas. Notice our method ini-\ntially converges faster and outperforms ADAGRAD through-\nout training in terms of frame classi\ufb01cation accuracy on the\ntest set. The same settings of \u03f5 = 1e\u22126 and \u03c1 = 0.95 from\nthe MNIST experiments were used for this setup.\nWhen training with recti\ufb01ed linear units and using 200\nmodel replicas we also used the same settings of hyperpa-\nrameters (see Fig. 4). Despite having 200 replicates which\ninherently introduces signi\ufb01cants amount of noise to the gra-\ndient accumulations, the ADADELTA method performs well,\nquickly converging to the same frame accuracy as the other\nmethods.\n0\n20\n40\n60\n80\n100\n18\n20\n22\n24\n26\n28\n30\n32\n34\n36\n38\nTime (hours)\nFame Acc %uracy\n \n \nADAGRAD log\nADADELTA log\nFig. 3. Comparison of ADAGRAD and ADADELTA on the\nSpeech Dataset with 100 replicas using logistic nonlinearities.\n0\n20\n40\n60\n80\n100\n120\n140\n160\n15\n20\n25\n30\n35\nTime (hours)\nFrame Accuracy %\n \n \nADAGRAD relu\nADADELTA relu\nMOMENTUM relu\nFig. 4.\nComparison of ADAGRAD, Momentum, and\nADADELTA on the Speech Dataset with 200 replicas using\nrecti\ufb01ed linear nonlinearities.\n5. CONCLUSION\nIn this tech report we introduced a new learning rate method\nbased on only \ufb01rst order information which shows promis-\ning result on MNIST and a large scale Speech recognition\ndataset. This method has trivial computational overhead com-\npared to SGD while providing a per-dimension learning rate.\nDespite the wide variation of input data types, number of hid-\nden units, nonlinearities and number of distributed replicas,\nthe hyperparameters did not need to be tuned, showing that\nADADELTA is a robust learning rate method that can be ap-\nplied in a variety of situations.\nAcknowledgements We thank Geoff Hinton, Yoram\nSinger, Ke Yang, Marc\u2019Aurelio Ranzato and Jeff Dean for\nthe helpful comments and discussions regarding this work.\n6. REFERENCES\n[1] H. Robinds and S. Monro, \u201cA stochastic approximation\nmethod,\u201d Annals of Mathematical Statistics, vol. 22, pp.\n400\u2013407, 1951.\n[2] D.E. Rumelhart, G.E. Hinton, and R.J. Williams, \u201cLearn-\ning representations by back-propagating errors,\u201d Nature,\nvol. 323, pp. 533\u2013536, 1986.\n[3] J. Duchi, E. Hazan, and Y. Singer, \u201cAdaptive subgradient\nmethods for online leaning and stochastic optimization,\u201d\nin COLT, 2010.\n[4] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin,\nQ. Le, M. Mao, M. Ranzato, A. Senior, P. Tucker,\nK. Yang, and A. Ng, \u201cLarge scale distributed deep net-\nworks,\u201d in NIPS, 2012.\n[5] S. Becker and Y. LeCun, \u201cImproving the convergence of\nback-propagation learning with second order methods,\u201d\nTech. Rep., Department of Computer Science, University\nof Toronto, Toronto, ON, Canada, 1988.\n[6] T. Schaul, S. Zhang, and Y. LeCun,\n\u201cNo more pesky\nlearning rates,\u201d arXiv:1206.1106, 2012.\n[7] N. Jaitly, P. Nguyen, A. Senior, and V. Vanhoucke, \u201cAp-\nplication of pretrained deep neural networks to large vo-\ncabulary speech recognition,\u201d in Interspeech, 2012.\n",
        "sentence": "",
        "context": "plication of pretrained deep neural networks to large vo-\ncabulary speech recognition,\u201d in Interspeech, 2012.\ntion, different model architecture choices, various data modal-\nities and selection of hyperparameters. We show promising\nresults compared to other methods on the MNIST digit clas-\nsi\ufb01cation task using a single machine and on a large scale\nvol. 323, pp. 533\u2013536, 1986.\n[3] J. Duchi, E. Hazan, and Y. Singer, \u201cAdaptive subgradient\nmethods for online leaning and stochastic optimization,\u201d\nin COLT, 2010.\n[4] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin,"
    }
]