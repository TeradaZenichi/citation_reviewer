[
    {
        "title": "Online linear optimization via smoothing",
        "author": [
            "Jacob Abernethy",
            "Chansoo Lee",
            "Abhinav Sinha",
            "Ambuj Tewari"
        ],
        "venue": "In COLT, pages 807\u2013823,",
        "citeRegEx": "1",
        "shortCiteRegEx": "1",
        "year": 2014,
        "abstract": "We present a new optimization-theoretic approach to analyzing\nFollow-the-Leader style algorithms, particularly in the setting where\nperturbations are used as a tool for regularization. We show that adding a\nstrongly convex penalty function to the decision rule and adding stochastic\nperturbations to data correspond to deterministic and stochastic smoothing\noperations, respectively. We establish an equivalence between \"Follow the\nRegularized Leader\" and \"Follow the Perturbed Leader\" up to the smoothness\nproperties. This intuition leads to a new generic analysis framework that\nrecovers and improves the previous known regret bounds of the class of\nalgorithms commonly known as Follow the Perturbed Leader.",
        "full_text": "arXiv:1405.6076v1  [cs.LG]  23 May 2014\nOnline Linear Optimization via Smoothing\nJacob Abernethy\nJABERNET@UMICH.EDU\nChansoo Lee\nCHANSOOL@UMICH.EDU\nComputer Science and Engineering Division, University of Michigan, Ann Arbor\nAbhinav Sinha\nABSI@UMICH.EDU\nElectrical and Computer Engineering Division, University of Michigan, Ann Arbor\nAmbuj Tewari\nTEWARIA@UMICH.EDU\nDepartment of Statistics, University of Michigan, Ann Arbor\nAbstract\nWe present a new optimization-theoreticapproach to analyzing Follow-the-Leader style algorithms,\nparticularly in the setting where perturbations are used as a tool for regularization. We show that\nadding a strongly convex penalty function to the decision rule and adding stochastic perturbations\nto data correspond to deterministic and stochastic smoothing operations, respectively. We establish\nan equivalence between \u201cFollow the Regularized Leader\u201d and \u201cFollow the Perturbed Leader\u201d up to\nthe smoothness properties. This intuition leads to a new generic analysis framework that recovers\nand improves the previous known regret bounds of the class of algorithms commonly known as\nFollow the Perturbed Leader.\n1. Introduction\nIn this paper, we study online learning (other names include adversarial learning or no-regret learn-\ning) in which the learner iteratively plays actions based on the data received up to the previous\niteration. The data sequence is chosen by an adversary and the learner\u2019s goal is to minimize the\nworst-case regret. The key to developing optimal algorithms is regularization, interpreted as hedg-\ning against an adversarial future input and avoiding over\ufb01tting to the observed data. In this paper,\nwe focus on regularization techniques for online linear optimization problems where the learner\u2019s\naction is evaluated on a linear reward function.\nFollow the Regularized Leader (FTRL) is an algorithm that uses explicit regularization via\npenalty function, which directly changes the optimization objective. At every iteration, FTRL se-\nlects an action by optimizing arg maxw f(w, \u0398) \u2212R(w) where f is the true objective, \u0398 is the\nobserved data, and R is a strongly convex penalty function such as the well-known \u21132-regularizer\n\u2225\u00b7 \u22252. The regret analysis of FTRL reduces to the analysis of the second-order behavior of the\npenalty function (Shalev-Shwartz, 2012), which is well-studied due to the powerful convex anal-\nysis tools. In fact, regularization via penalty methods for online learning in general are very well\nunderstood. Srebro et al. (2011) proved that Mirror Descent, a regularization via penalty method,\nachieves a nearly optimal regret guarantee for a general class of online learning problems, and\nMcMahan (2011) showed that FTRL is equivalent to Mirror Descent under some assumptions.\nFollow the Perturbed Leader (FTPL), on the other hand, uses implicit regularization via per-\nturbations. At every iteration, FTPL selects an action by optimizing arg maxw f(w, \u0398 + u) where\n1\nABERNETHY LEE SINHA TEWARI\n\u0398 is the observed data and u is some random noise vector, often referred to as a \u201cperturbation\u201d of\nthe input. Unfortunately, the analysis of FTPL lacks a generic framework and relies substantially\non clever algebra tricks and heavy probabilistic analysis (Kalai and Vempala, 2005; Devroye et al.,\n2013; van Erven et al., 2014). Convex analysis techniques, which led to our current thorough under-\nstanding of FTRL, have not been applied to FTPL, partly because the decision rule of FTPL does\nnot explicitly contain a convex function.\nIn this paper, we present a new analysis framework that makes it possible to analyze FTPL in\nthe same way that FTRL has been analyzed, particularly with regards to second-order properties\nof convex functions. We show that both FTPL and FTRL naturally arise as smoothing operations\nof a non-smooth potential function and the regret analysis boils down to controlling the smoothing\nparameters as de\ufb01ned in Section 3. This new uni\ufb01ed analysis framework not only recovers the\nknown optimal regret bounds, but also gives a new type of generic regret bounds.\nPrior to our work, Rakhlin et al. (2012) showed that both FTPL and FTRL naturally arise as\nadmissible relaxations of the minimax value of the game between the learner and adversary. In\nshort, adding a random perturbation and adding a regularization penalty function are both optimal\nways to simulate the worst-case future input sequence. We establish a stronger connection between\nFTRL and FTPL; both algorithms are derived from smoothing operations and they are equivalent\nup to the smoothing parameters. This equivalence is in fact a very strong result, considering the fact\nthat Harsanyi (1973) showed that there is no general bijection between FTPL and FTRL.\nThis paper also aligns itself with the previous work that studied the connection between explicit\nregularization via penalty and implicit regularization via perturbations. Bishop (1995) showed that\nadding Gaussian noise to features of the training examples is equivalent to Tikhonov regularization,\nand more recently Wager et al. (2013) showed that for online learning, dropout training (Hinton\net al., 2012) is similar to AdagGrad (Duchi et al., 2010) in that both methods scale features by the\nFisher information. These results are derived from Taylor approximations, but our FTPL-FTRL\nconnection is derived from the convex conjugate duality.\nAn interesting feature of our analysis framework is that we can directly apply existing techniques\nfrom the optimization literature, and conversely, our new \ufb01ndings in online linear optimization may\napply to optimization theory. In Section 4.3, a straightforward application of the results on Gaussian\nsmoothing by Nesterov (2011) and Duchi et al. (2012) gives a generic regret bound for an arbitrary\nonline linear optimization problem. In Section 4.1 and 4.2, we improve this bound for the special\ncases that correspond to canonical online linear optimization problems, and these results may be of\ninterest to the optimization community.\n2. Preliminaries\n2.1. Convex Analysis\nLet f be a differentiable, closed, and proper convex function whose domain is domf \u2286RN. We\nsay that f is L-Lipschitz with respect to a norm \u2225\u00b7 \u2225when f satis\ufb01es |f(x) \u2212f(y)| \u2264L\u2225x \u2212y\u2225for\nall x, y \u2208dom(f).\nThe Bregman divergence, denoted Df(y, x), is the gap between f(y) and the linear approxi-\nmation of f(y) around x. Formally, Df(y, x) = f(y) \u2212f(x) \u2212\u27e8\u2207f(x), y \u2212x\u27e9. We say that\nf is \u03b2-strongly convex with respect to a norm \u2225\u00b7 \u2225\nif we have Df(y, x) \u2265\u03b2\n2 \u2225y \u2212x\u22252 for all\nx, y \u2208domf. Similarly, f is said to be \u03b2-strongly smooth with respect to a norm \u2225\u00b7 \u2225if we have\n2\nONLINE LINEAR OPTIMIZATION VIA SMOOTHING\nDf(y, x) \u2264\u03b2\n2 \u2225y \u2212x\u22252 for all x, y \u2208domf. The Bregman divergence measures how fast the gradi-\nent changes, or equivalently, how large the second derivative is. In fact, we can bound the Bregman\ndivergence by analyzing the local behavior of Hessian, as the following adaptation of Abernethy\net al. (2013, Lemma 4.6) shows.\nLemma 1\nLet f be a twice-differentiable convex function with domf \u2286RN. Let x \u2208domf, such\nthat vT \u22072f(x+\u03b1v)v \u2208[a, b] (a \u2264b) for all \u03b1 \u2208[0, 1]. Then, a\u2225v\u22252/2 \u2264Df(x+v, x) \u2264b\u2225v\u22252/2.\nThe Fenchel conjugate of f is f \u22c6(\u03b8) = supw\u2208dom(f){\u27e8w, \u03b8\u27e9\u2212f(w)}, and it is a dual mapping\nthat satis\ufb01es f = (f \u22c6)\u22c6and \u2207f \u22c6\u2208dom(f). By the strong convexity-strong smoothness duality, f\nis \u03b2-strongly smooth with respect to a norm \u2225\u00b7 \u2225if and only if f \u22c6is 1\n\u03b2-strongly smooth with respect\nto the dual norm \u2225\u00b7 \u2225\u22c6. For more details and proofs, readers are referred to an excellent survey by\nShalev-Shwartz (2012).\n2.2. Online Linear Optimization\nLet X and Y be convex and closed subsets of RN. The online linear optimization is de\ufb01ned to be\nthe following iterative process:\nOn round t = 1, . . . , T,\n\u2022 the learner plays wt \u2208X.\n\u2022 the adversary reveals \u03b8t \u2208Y.\n\u2022 the learner receives a reward1 \u27e8wt, \u03b8t\u27e9.\nWe say X is the decision set and Y is the reward set. Let \u0398t = Pt\ns=1 \u03b8s be the cumulative reward.\nThe learner\u2019s goal is to minimize the (external) regret, de\ufb01ned as:\nRegret = max\nw\u2208X \u27e8w, \u0398T \u27e9\n|\n{z\n}\nbaseline potential\n\u2212\nT\nX\nt=1\n\u27e8wt, \u03b8t\u27e9.\n(1)\nThe baseline potential function \u03a6(\u0398) := maxw\u2208X \u27e8w, \u0398\u27e9is the comparator term against which we\nde\ufb01ne the regret, and it coincides with the support function of X. For a bounded compact set X,\nthe support function of X is sublinear2 and Lipschitz continuous with respect to any norm \u2225\u00b7 \u2225with\nthe Lipschitz constant supx\u2208X \u2225x\u2225. For more details and proofs, readers are referred to Rockafellar\n(1997, Section 13) or Molchanov (2005, Appendix F).\n3. Online Linear Optimization Algorithms via Smoothing\n3.1. Gradient-Based Prediction Algorithm\nFollow-the-Leader style algorithms solve an optimization objective every round and play an action\nof the form wt = arg maxw\u2208X f(w, \u0398t\u22121) given a \ufb01xed \u0398t\u22121. For example, Follow the Regular-\nized Leader maximizes f(w, \u0398) = \u27e8w, \u0398\u27e9\u2212R(w) where R is a strongly convex regularizer, and\nFollow the Perturbed Leader maximizes f = \u27e8w, \u0398 + u\u27e9where u is a random noise. A surprising\n1. Our somewhat less conventional choice of maximizing the reward instead of minimizing the loss was made so that\nwe directly analyze the convex function max(\u00b7) without cumbersome sign changes.\n2. A function f is sublinear if it is positive homogeneous (i.e., f(ax) = af(x) for all a > 0) and subadditive (i.e.,\nf(x) + f(y) \u2265f(x + y)).\n3\nABERNETHY LEE SINHA TEWARI\nfact about these algorithms is that there are many scenarios in which the action wt is exactly the\ngradient of some scalar potential function \u03a6t evaluated at \u0398t\u22121. This perspective gives rise to what\nwe call the Gradient-based Prediction Algorithm (GBPA), presented below. Note that Cesa-Bianchi\nand Lugosi (2006, Theorem 11.6) presented a similar algorithm, but our formulation eliminates all\ndual mappings.\nAlgorithm 1: Gradient-Based Prediction Algorithm (GBPA)\nInput: X, Y \u2286RN\nInitialize \u03980 = 0\nfor t = 1 to T do\nThe learner chooses differentiable \u03a6t : RN \u2192R whose gradient satis\ufb01es Image(\u2207\u03a6t) \u2286X\nThe learner plays wt = \u2207\u03a6t(\u0398t\u22121)\nThe adversary reveals \u03b8t \u2208Y and the learner gets a reward of \u27e8wt, \u03b8t\u27e9\nUpdate \u0398t = \u0398t\u22121 + \u03b8t\nend\nLemma 2 (GBPA Regret) Let \u03a6 be the baseline potential function for an online linear optimiza-\ntion problem. The regret of the GBPA can be written as:\nRegret = \u03a6(\u0398T ) \u2212\u03a6T (\u0398T )\n|\n{z\n}\nunderestimation penalty\n+\nT\nX\nt=1\n\u0012\n(\u03a6t(\u0398t\u22121) \u2212\u03a6t\u22121(\u0398t\u22121))\n|\n{z\n}\noverestimation penalty\n+ D\u03a6t(\u0398t, \u0398t\u22121)\n|\n{z\n}\ndivergence penalty\n\u0013\n,\n(2)\nwhere \u03a60 \u2261\u03a6.\nProof See Appendix A.1.\nIn the existing FTPL analysis, the counterpart of the divergence penalty is \u27e8wt+1\u2212wt, \u03b8t\u27e9, which\nis controlled by analyzing the probability that the noise would cause the two random variables wt+1\nand wt to differ. In our framework, wt is the gradient of a function \u03a6t of \u0398, which means that if\n\u03a6t is twice-differentiable, we can take the derivative of wt with respect to \u0398. This derivative is the\nHessian matrix of \u03a6t, which essentially controls \u27e8wt \u2212wt\u22121\u27e9with the help of Lemma 1. Since\nwe focus on the curvature property of functions as opposed to random vectors, our FTPL analysis\ninvolves less probabilistic analysis than Devroye et al. (2013) or van Erven et al. (2014) does.\nWe point out a couple of important facts about Lemma 2:\n(a) If \u03a61 \u2261\u00b7 \u00b7 \u00b7 \u2261\u03a6T , then the overestimation penalty sums up to \u03a61(0) \u2212\u03a6(0) = \u03a6T(0) \u2212\u03a6(0).\n(b) If \u03a6t is \u03b2-strongly smooth with respect to \u2225\u00b7 \u2225, the divergence penalty at t is at most \u03b2\n2 \u2225\u03b8t\u22252.\n3.2. Smoothability of the Baseline Potential\nEquation 2 shows that the regret of the GBPA can be broken into two parts. One source of regret\nis the Bregman divergence of \u03a6t; since \u03b8t is not known until playing wt, the GBPA always ascends\nalong the gradient that is one step behind. The adversary can exploit this and play \u03b8t to induce\na large gap between \u03a6t(xt) and the linear approximation of \u03a6t(\u0398t) around \u0398t\u22121. Of course, the\nlearner can reduce this gap by choosing a smooth \u03a6t whose gradient changes slowly. The learner,\n4\nONLINE LINEAR OPTIMIZATION VIA SMOOTHING\nhowever, cannot achieve low regret by choosing an arbitrarily smooth \u03a6t, because the other source\nof regret is the difference between \u03a6t and \u03a6. In short, the GBPA achieves low regret if the potential\nfunction \u03a6t gives a favorable tradeoff between the two sources of regret. This tradeoff is captured\nby the following de\ufb01nition of smoothability.\nDe\ufb01nition 3 (Beck and Teboulle, 2012, De\ufb01nition 2.1) Let \u03a6 be a closed proper convex function.\nA collection of functions {\u02c6\u03a6\u03b7 : \u03b7 \u2208R} is said to be an \u03b7-smoothing of a smoothable function \u03a6\nwith smoothing parameters (\u03b1, \u03b2, \u2225\u00b7 \u2225), if for every \u03b7 > 0\n(i) There exists \u03b11 (underestimation bound) and \u03b12 (overestimation bound) such that\nsup\n\u0398\u2208dom(\u03a6)\n\u03a6(\u0398) \u2212\u02c6\u03a6\u03b7(\u0398) \u2264\u03b11\u03b7 and\nsup\n\u0398\u2208dom(\u03a6)\n\u02c6\u03a6\u03b7(\u0398) \u2212\u03a6(\u0398) \u2264\u03b12\u03b7\nwith \u03b11 + \u03b12 = \u03b1.\n(ii) \u02c6\u03a6\u03b7 is \u03b2\n\u03b7 -strongly smooth with respect to \u2225\u00b7 \u2225.\nWe say \u03b1 is the deviation parameter, and \u03b2 is the smoothness parameter.\nA straightforward application of Lemma 2 gives the following statement:\nCorollary 4\nLet \u03a6 be the baseline potential for an online linear optimization problem. Suppose\n{\u02c6\u03a6\u03b7} is an \u03b7-smoothing of \u03a6 with parameters (\u03b1, \u03b2, \u2225\u00b7 \u2225). Then, the GBPA run with \u03a61 \u2261\u00b7 \u00b7 \u00b7 \u2261\n\u03a6T \u2261\u02c6\u03a6\u03b7 has regret at most\nRegret \u2264\u03b1\u03b7 + \u03b2\n2\u03b7\nT\nX\nt=1\n\u2225\u03b8t\u22252\nIn online linear optimization, we often consider the settings where the marginal reward vectors\n\u03b81, . . . , \u03b8t are constrained by a norm, i.e., \u2225\u03b8t\u2225\u2264r for all t. In such settings, the regret grows in\nO(\u221ar\u03b1\u03b2T) for the optimal choice of \u03b1. The product \u03b1\u03b2, therefore, is at the core of the GBPA\nregret analysis.\n3.3. Algorithms\nFollow the Leader (FTL)\nConsider the GBPA run with a \ufb01xed potential function \u03a6t \u2261\u03a6 for\nt = 1, . . . , T, i.e., the learner chooses the baseline potential function every iteration. At iteration t,\nthis algorithm plays \u2207\u03a6t(\u0398t\u22121) = arg maxw\u27e8w, \u0398t\u22121\u27e9, which is equivalent to FTL (Cesa-Bianchi\nand Lugosi, 2006, Section 3.2). FTL suffers zero regret from the over- or underestimation penalty,\nbut the divergence penalty grows linearly in T in the worst case, resulting in an \u2126(T) regret.\nFollow the Regularized Leader (FTRL)\nConsider the GBPA run with a regularized potential:\n\u2200t, \u03a6t(\u0398) = R\u22c6(\u0398) = max\nw\u2208X {\u27e8w, \u0398\u27e9\u2212R(w)}\n(3)\nwhere R : X \u2192R is a \u03b2-strongly convex function. At time t, this algorithm plays \u2207\u03a6t(\u0398t\u22121) =\narg maxw{\u27e8w, \u0398t\u22121\u27e9\u2212R(w)}, which is equivalent to FTRL. By the strong convexity-strong smooth-\nness duality, \u03a6t is 1\n\u03b2-strongly smooth with respect to the dual norm \u2225\u00b7 \u2225\u22c6. In Section 5, we give an\nalternative interpretation of FTRL as a deterministic smoothing technique called inf-conv smooth-\ning.\n5\nABERNETHY LEE SINHA TEWARI\nFollow the Perturbed Leader (FTPL)\nConsider the GBPA run with a stochastically smoothed\npotential:\n\u2200t, \u03a6t(\u0398) = \u02dc\u03a6(\u0398; \u03b7, D) def\n= Eu\u223cD[\u03a6(\u0398 + \u03b7u)] = Eu\u223cD\nh\nmax\nw\u2208X {\u27e8w, \u0398 + \u03b7u\u27e9}\ni\n(4)\nwhere D is a smoothing distribution with the support RN and \u03b7 > 0 is a scaling parameter. This\ntechnique of stochastic smoothing has been well-studied in the optimization literature for gradient-\nfree optimization algorithms (Glasserman, 1991; Yousean et al., 2010) and accelerated gradient\nmethods for non-smooth optimizations (Duchi et al., 2012). If the max expression inside the ex-\npectation has a unique maximizer with probability one, we can swap the expectation and gradient\n(Bertsekas, 1973, Proposition 2.2) to obtain\n\u2207\u03a6t(\u0398t\u22121) = Eu\u223cD\nh\narg max\nw\u2208X\n{\u27e8w, \u0398t\u22121 + \u03b7u\u27e9}\ni\n.\n(5)\nEach arg max expression is equivalent to the decision rule of FTPL (Hannan, 1957; Kalai and\nVempala, 2005); the GBPA on a stochastically smoothed potential can thus be seen as playing the\nexpected action of FTPL. Since the learner gets a linear reward in online linear optimization, the\nregret of the GBPA on a stochastically smoothed potential is equal to the expected regret of FTPL.\nFTPL-FTRL Duality\nOur potential-based formulation of FTRL and FTPL reveals that a strongly\nconvex regularizer de\ufb01nes a smooth potential function via duality, while adding perturbations is\na direct smoothing operation on the baseline potential function. By the strong convexity-strong\nsmoothness duality, if the stochastically smoothed potential function is (1/\u03b2)-strongly smooth with\nrespect to \u2225\u00b7 \u2225\u22c6, then its Fenchel conjugate implicitly de\ufb01nes a regularizer that is \u03b2-strongly convex\nwith respect to \u2225\u00b7 \u2225.\nThis connection via duality is a bijection in the special case where the decision set is one-\ndimensional. Previously it had been observed3 that the Hedge Algorithm (Freund and Schapire,\n1997), which can be cast as FTRL with an entropic regularization R(w) = P\ni wi log wi, is equiv-\nalent to FTPL with Gumbel-distributed noise. Hofbauer and Sandholm (2002, Section 2) gave a\ngeneralization of this fact to a much larger class of perturbations, although they focused on repeated\ngame playing where the learner\u2019s decision set X is the probability simplex. The inverse mapping\nfrom FTPL to FTRL, however, does not appear to have been previously published.\nTheorem 5\nConsider the one-dimensional online linear optimization problem with X = Y =\n[0, 1]. Let R : X \u2192R be a strongly convex regularizer. Its Fenchel conjugate R\u22c6de\ufb01nes a valid\nCDF of a continuous distribution D such that Equation 3 and Equation 4 are equal. Conversely, let\nFD be a CDF of a continuous distribution D with a \ufb01nite expectation. If we de\ufb01ne R to be such that\nR(w) \u2212R(0) = \u2212\nR w\n0 F \u22121\nD (1 \u2212z)dz, then Equation 3 and Equation 4 are equal.\nProof In Appendix B.1.\n3. Adam Kalai \ufb01rst described this result in personal communication and Warmuth (2009) expanded it into a short note\navailable online. However, the result appears to be folklore in the area of probabilistic choice models, and it is\nmentioned brie\ufb02y in Hofbauer and Sandholm (2002).\n6\nONLINE LINEAR OPTIMIZATION VIA SMOOTHING\n4. Online Linear Optimization via Gaussian Smoothing\nGaussian smoothing is a standard technique for smoothing a function. In computer vision applica-\ntions, for example, image pixels are viewed as a function of the (x, y)-coordinates, and Gaussian\nsmoothing is used to blur noises in the image. We \ufb01rst present basic results on Gaussian smoothing\nfrom the optimization literature.\nDe\ufb01nition 6 (Gaussian smoothing) Let \u03a6 : RN \u2192R be a function. Then, we de\ufb01ne its Gaussian\nsmoothing, with a scaling parameter \u03b7 > 0 and a covariance matrix \u03a3, as\n\u02dc\u03a6(\u0398; \u03b7, N(0, \u03a3)) = Eu\u223cN (0,\u03a3)\u03a6(\u0398 + \u03b7u) = (2\u03c0)\u2212N\n2 det(\u03a3)\u22121\n2\nZ\nRN \u03a6(\u0398 + \u03b7u)e\u22121\n2uT \u03a3\u22121u du\nIn this section, when the smoothing parameters are clear from the context, we use a shorthand nota-\ntion \u02dc\u03a6. An extremely useful property of Gaussian smoothing is that \u02dc\u03a6 is always twice-differentiable,\neven when \u03a6 is not. The trick is to introduce a new variable \u02dc\u0398 = \u0398 + \u03b7u. After substitutions, the\nvariable \u0398 only appears in the exponent, which can be safely differentiated.\nLemma 7\n(Nesterov 2011, Lemma 2, and Bhatnagar 2007, Section 3) Let \u03a6 : RN \u2192R be a\nfunction. For any positive \u03b7, \u02dc\u03a6(\u00b7 ; \u03b7, N(0, \u03a3)) is twice-differentiable and\n\u2207\u02dc\u03a6(\u0398; \u03b7, N(0, \u03a3)) = 1\n\u03b7 Eu[\u03a6(\u0398 + \u03b7u)\u03a3\u22121u]\n(6)\n\u22072 \u02dc\u03a6(\u0398; \u03b7, N(0, \u03a3)) = 1\n\u03b72 Eu\nh\n\u03a6(\u0398 + \u03b7u)\n\u0010\n(\u03a3\u22121u)(\u03a3\u22121u)T \u2212\u03a3\u22121\u0011i\n(7)\nIf \u03a6(\u0398 + \u03b7u) is differentiable almost everywhere, then we can directly differentiate Equation 6\nby swapping the expectation and gradient (Bertsekas, 1973, Proposition 2.2) and obtain an alterna-\ntive expression for Hessian:\n\u22072 \u02dc\u03a6(\u0398; \u03b7, N(0, \u03a3)) = 1\n\u03b7 Eu[\u2207\u03a6\n\u0000\u0398 + \u03b7u)(\u03a3\u22121u)T ].\n(8)\n4.1. Experts Setting (\u21131-\u2113\u221ecase)\nThe experts setting is where X = \u2206N def\n= {w \u2208RN : P\ni wi = 1, wi \u22650 \u2200i}, and Y = {\u03b8 \u2208\nRN : \u2225\u03b8\u2225\u221e\u22641}. The baseline potential function is \u03a6(\u0398) = maxw\u2208X\u27e8w, \u0398\u27e9= \u0398i\u2217(\u0398), where we\nde\ufb01ne i\u2217(z) := min{i : i \u2208arg maxj zj}.\nOur regret bound in Theorem 8 is data-dependent, and it is stronger than the previously known\nO(\u221aT log N) regret bounds of the algorithms that use similar perturbations. In the game theoretic\nanalysis of Gaussian perturbations by Rakhlin et al. (2012), the algorithm uses the scaling parameter\n\u03b7t =\n\u221a\nT \u2212t, which requires the knowledge of T and does not adapt to data. Devroye et al. (2013)\nproposed the Prediction by Random Walk (PRW) algorithm, which \ufb02ips a fair coin every round and\ndecides whether to add 1 to each coordinate. Due to the discrete nature of the algorithm, the analysis\nmust assume the worst case where \u2225\u03b8t\u2225\u22c6= 1 for all t.\n7\nABERNETHY LEE SINHA TEWARI\nTheorem 8 Let \u03a6 be the baseline potential for the experts setting. The GBPA run with the Gaussian\nsmoothing of \u03a6, i.e., \u03a6t(\u00b7) = \u02dc\u03a6(\u00b7; \u03b7t, N(0, I)) for all t has regret at most\nRegret \u2264\np\n2 log N\n\u0010\n\u03b7T + PT\nt=1\n1\n\u03b7t \u2225\u03b8t\u22252\n\u221e\n\u0011\n.\n(9)\nIf the algorithm selects \u03b7t =\nqPT\nt=1 \u2225\u03b8t\u22252\u221efor all t (with the help of hindsight), we have\nRegret \u22642\nq\n2 PT\nt=1 \u2225\u03b8t\u22252\n\u221elog N.\nIf the algorithm selects \u03b7t adaptively according to \u03b7t =\nq\n2(1 + Pt\u22121\ns=1 \u2225\u03b8s\u22252\u221e), we have\nRegret \u22644\nq\n(1 + PT\nt=1 \u2225\u03b8t\u22252\n\u221e) log N.\nProof In order to apply Lemma 2, we need to upper bound (i) the overestimation and underesti-\nmation penalty, and (ii) the Bregman divergence. To bound (i), \ufb01rst note that due to convexity of\n\u03a6, the smoothed potential \u02dc\u03a6 is also convex and upper bounds the baseline potential. Hence, the\nunderestimation penalty is at most 0, and when \u03b7t is \ufb01xed for all t, it is straightforward to bound the\noverestimation penalty:\n\u03a6T (0) \u2212\u03a6(0) \u2264Eu\u223cN (0,I)[\u03a6(\u03b7T u)] \u2264\u03b7T\np\n2 log N.\n(10)\nThe \ufb01rst inequality is the triangle inequality. The second inequality is a well-known result and we\nincluded the proof in Appendix C.1 for completeness. For the adaptive \u03b7t, we apply Lemma 10,\nwhich we prove at the end of this section, to get the same bound.\nIt now remains to bound the Bregman divergence. This is achieved in Lemma 9 where we upper\nbound P\ni,j |\u22072\nij\u03a6|, which is an upper bound on max\u03b8:\u2225\u03b8\u2225\u221e=1 \u03b8T(\u22072\u03a6)\u03b8. The \ufb01nal step is to apply\nLemma 1.\nThe proof of Theorem 8 shows that for the experts setting, the Gaussian smoothing is an \u03b7-\nsmoothing with parameters (O(\u221alog N), O(\u221alog N), \u2225\u00b7 \u2225). This is in contrast to the Hedge Al-\ngorithm (Freund and Schapire, 1997), which is an \u03b7-smoothing with parameters (log N, 1, \u2225\u00b7 \u2225)\n(See Section 5 for details). Interestingly, the two algorithms obtain the same optimal regret (up to\nconstant factors) although they have different smoothing parameters.\nLemma 9\nLet \u03a6 be the baseline potential for the experts setting. Let the Hessian matrix of the\nGaussian-smoothed baseline potential be denoted H, i.e., H = \u22072 \u02dc\u03a6(\u0398; \u03b7, N(0, I)). Then,\nX\ni,j\n|Hij| \u22642\u221a2 log N\n\u03b7\n.\nProof With probability one, \u03a6(\u0398 + \u03b7u) is differentiable and from Lemma 7, we can write\nH = 1\n\u03b7 E[\u2207\u03a6(\u0398 + \u03b7u)uT ] = E[ei\u2217(\u03b7u+\u0398)uT ],\nwhere ei \u2208RN is the i-th standard basis vector.\n8\nONLINE LINEAR OPTIMIZATION VIA SMOOTHING\nFirst, we note that all off-diagonals of H are negative and all diagonal entries in H are positive.\nThis is because the Hessian matrix is the covariance matrix between the probability that i-th coor-\ndinate is the maximum and the extra random Gaussian noise added to the j-th coordinate; for any\npositive number \u03b1, uj = \u03b1 and uj = \u2212\u03b1 have the same probability, but the indicator for i = i\u2217has\na higher probability to be 1 when ui is positive (hence Hii > 0) and uj is negative for i \u0338= j (hence\nHij < 0 for i \u0338= j).\nSecond, the entries of H sum up to 0, as\nX\ni,j\nHij = 1\n\u03b7 E\nhP\nj uj\nP\ni 1{i = i\u2217(\u0398 + u)}\ni\n= 1\n\u03b7 E\nhP\nj uj\ni\n= 0.\nCombining the two observations, we have\nX\ni,j\n|Hij| =\nX\ni,j:Hij>0\nHij +\nX\ni,j:Hij<0\n\u2212Hij = 2\nX\ni,j:Hij>0\nHij = 2Tr(H)\n.\nFinally, the trace is bounded as follows:\nTr(H) = 1\n\u03b7 E\nh X\ni\nui1{i = i\u2217(\u0398 + u)}\ni\n\u22641\n\u03b7E\nh\n(max\nk\nuk)\nX\ni\n1{i = i\u2217(\u0398 + u)}\ni\n= 1\n\u03b7E[max\nk\nuk] \u22641\n\u03b7\np\n2 log N,\nwhere the \ufb01nal inequality is shown in Appendix C.1. Multiplying both sides by 2 completes the\nproof.\nTime-Varying Scaling Parameters\nWhen the scaling parameter \u03b7t changes every iteration, the\noverestimation penalty becomes a sum of T terms. The following lemma shows that using the\nsublinearity of the baseline potential, we can collapse them into one.\nLemma 10\nLet \u03a6 : RN \u2192R be a sublinear function, and D be a continuous distribution with\nthe support RN. Let \u03a6t(\u0398) = \u02dc\u03a6(\u0398; \u03b7t, D) for t = 0, . . . , T and choose \u03b7t to be a non-decreasing\nsequence of non-negative numbers (\u03b70 = 0 so that \u03a60 = \u03a6). Then, the overestimation penalty in\nEquation 2 has the following upper bound:\nT\nX\nt=1\n\u03a6t(\u0398t\u22121) \u2212\u03a6t\u22121(\u0398t\u22121) \u2264\u03b7T Eu\u223cD[\u03a6(u)].\nProof See Appendix C.2\n4.2. Online Linear Optimization over Euclidean Balls (\u21132-\u21132 case)\nThe Euclidean balls setting is where X = Y = {x \u2208RN : \u2225x\u22252 \u22641}. The baseline potential\nfunction is \u03a6(\u0398) = maxw\u2208X\u27e8w, \u0398\u27e9= \u2225\u0398\u22252. We show that the GBPA with Gaussian smoothing\nachieves a minimax optimal regret (Abernethy et al., 2008) up to a constant factor.\n9\nABERNETHY LEE SINHA TEWARI\nTheorem 11 Let \u03a6 be the baseline potential for the Euclidean balls setting. The GBPA run with\n\u03a6t(\u00b7) = \u02dc\u03a6(\u00b7; \u03b7, N(0, I)) for all t has regret at most\nRegret \u2264\u03b7T\n\u221a\nN +\n1\n2\n\u221a\nN\nPT\nt=1\n1\n\u03b7t \u2225\u03b8t\u22252\n2.\n(11)\nIf the algorithm selects \u03b7t =\nqPT\ns=1 \u2225\u03b8s\u22252\n2/(2N) for all t (with the help of hindsight), we have\nRegret \u2264\nq\n2 PT\nt=1 \u2225\u03b8t\u22252\n2.\nIf the algorithm selects \u03b7t adaptively according to \u03b7t =\nq\n(1 + Pt\u22121\ns=1 \u2225\u03b8s\u22252\n2))/N, we have\nRegret \u22642\nq\n1 + PT\nt=1 \u2225\u03b8t\u22252\n2\nProof The proof is mostly similar to that of Theorem 8. In order to apply Lemma 2, we need to\nupper bound (i) the overestimation and underestimation penalty, and (ii) the Bregman divergence.\nThe Gaussian smoothing always overestimates a convex function, so it suf\ufb01ces to bound the\noverestimation penalty. Furthermore, it suf\ufb01ces to consider the \ufb01xed \u03b7t case due to Lemma 1. The\noverestimation penalty can be upper-bounded as follows:\n\u03a6T (0) \u2212\u03a6(0) = Eu\u223cN (0,I)\u2225\u0398 + \u03b7T u\u22252 \u2212\u2225\u0398\u22252 \u2264\u03b7T Eu\u223cN (0,I)\u2225u\u22252\n\u2264\u03b7T\nq\nEu\u223cN (0,I)\u2225u\u22252\n2 = \u03b7T\n\u221a\nN\nThe \ufb01rst inequality is from the triangle inequality, and the second inequality is from the concavity\nof the square root.\nFor the divergence penalty, note that the upper bound on maxv:\u2225\u03b8\u22252=1 \u03b8T (\u22072 \u02dc\u03a6)\u03b8 is exactly the\nmaximum eigenvalue of the Hessian, which we bound in Lemma 12. The \ufb01nal step is to apply\nLemma 1.\nLemma 12 Let \u03a6 be the baseline potential for the Euclidean balls setting. Then, for all \u0398 \u2208RN\nand \u03b7 > 0, the Hessian matrix of the Gaussian smoothed potential satis\ufb01es\n\u22072 \u02dc\u03a6(\u0398; \u03b7, N(0, I)) \u2aaf\n1\n\u03b7\n\u221a\nN I.\nProof The Hessian of the Euclidean norm \u22072\u03a6(\u0398) = \u2225\u0398\u2225\u22121\n2 I \u2212\u2225\u0398\u2225\u22123\n2 \u0398\u0398T diverges near \u0398 = 0.\nExpectedly, the maximum curvature is at origin even after Gaussian smoothing (See Appendix C.3).\nSo, it suf\ufb01ces to prove\n\u22072\u03a6(0) = Eu\u223cN (0,I)[\u2225u\u22252(uuT \u2212I)] \u2aaf\nq\n1\nN I,\nwhere the Hessian expression is from Equation 8.\nBy symmetry, all off-diagonal elements of the Hessian are 0. Let Y = \u2225u\u22252, which is Chi-\nsquared with N degrees of freedom. So,\nTr(E[\u2225u\u22252(uuT \u2212I)]) = E[Tr(\u2225u\u22252(uuT \u2212I))] = E[\u2225u\u22253\n2 \u2212N\u2225u\u22252] = E[Y\n3\n2] \u2212NE[Y\n1\n2]\n10\nONLINE LINEAR OPTIMIZATION VIA SMOOTHING\nUsing the Chi-Squared moment formula (Harvey, 1965, p. 20), the above becomes:\n2\n3\n2 \u0393(3\n2 + N\n2 )\n\u0393(N\n2 )\n\u2212N2\n1\n2 \u0393(1\n2 + N\n2 )\n\u0393(N\n2 )\n=\n\u221a\n2\u0393(1\n2 + N\n2 )\n\u0393(N\n2 )\n.\n(12)\nFrom the log-convexity of the Gamma function,\nlog \u0393\n\u00001\n2 + N\n2\n\u0001\n\u22641\n2\n\u0000log \u0393\n\u0000 N\n2\n\u0001\n+ log \u0393\n\u0000 N\n2 + 1\n\u0001\u0001\n= log \u0393\n\u0000 N\n2\n\u0001 q\nN\n2 .\nExponentiating both sides, we obtain\n\u0393\n\u00001\n2 + N\n2\n\u0001\n\u2264\u0393\n\u0000N\n2\n\u0001 q\nN\n2 ,\nwhich we apply to Equation 12 and get Tr(\u22072\u03a6(0)) \u2264\n\u221a\nN. To complete the proof, note that by\nsymmetry, each entry must have the same expected value, and hence it is bounded by\np\n1/N.\n4.3. General Bound\nIn this section, we will use a generic property of Gaussian smoothing to derive a regret bound that\nholds for any arbitrary online linear optimization problem.\nLemma 13 (Duchi et al., 2012, Lemma E.2) Let \u03a6 be a real-valued convex function on a closed\ndomain which is a subset of RN. Suppose \u03a6 is L-Lipschitz with respect to \u2225\u00b7 \u22252, and let \u02c6\u03a6\u03b7 be the\nGaussian smoothing of \u03a6 with the scaling parameter \u03b7 and identity covariance. Then, {\u02c6\u03a6\u03b7} is an\n\u03b7-smoothing of \u03a6 with parameters (L\n\u221a\nN, L, \u2225\u00b7 \u22252).\nConsider an instance of online linear optimization with decision set X and reward set Y. The\nbaseline potential function \u03a6 is \u2225X\u22252-Lipschitz with respect to \u2225\u00b7\u22252, where \u2225X\u22252 = supx\u2208X \u2225x\u22252.\nFrom Lemma 13 and Corollary 4, it follows that\nRegret \u2264\u03b7\n\u221a\nN\u2225X\u22252 + \u2225X\u22252\n2\nT\nX\nt=1\n\u2225\u03b8t\u22252\n2,\nwhich is O(N\n1\n4 \u2225X\u22252\u2225Y\u22252\n\u221a\nT) after tuning \u03b7. This regret bound, however, often gives a suboptimal\ndependence on the dimension N. For example, it gives O(N\n3\n4 T\n1\n2 ) regret bound for the experts\nsetting where \u2225X\u22252 = 1 and \u2225Y\u22252 =\n\u221a\nN, and O(N\n1\n4T\n1\n2) regret bound for the Euclidean balls\nsetting where \u2225X\u22252 = \u2225Y\u22252 = 1.\n4.4. Online Convex Optimization\nIn online convex optimization, the learner receives a sequence of convex functions ft whose domain\nis X and its subgradients are in the set Y (Zinkevich, 2003). After the learner plays wt \u2208X, the\nreward function ft is revealed. The learner gains ft(wt) and observes \u2207ft(wt), a subgradient of ft\nat wt.\nA simple linearization argument shows that our regret bounds for online linear optimization\ngeneralize to online convex optimization. Let w\u2217be the optimal \ufb01xed point in hindsight. The\n11\nABERNETHY LEE SINHA TEWARI\ntrue regret is upper bounded by the linearized regret, as ft(w\u2217) \u2212ft(wt) \u2264\u27e8w\u2217\u2212wt, \u2207ft(wt)\u27e9\nfor any subgradient \u2207ft(\u00b7), and our analysis bounds the linearized regret. Unlike in the online\nlinear optimization settings, however, the regret bound is valid only for the GBPA with smoothed\npotentials, which plays the expected action of FTPL.\n5. Online Linear Optimization via Inf-conv Smoothing\nBeck and Teboulle (2012) proposed inf-conv smoothing, which is an in\ufb01mal convolution with a\nstrongly smooth function. In this section, we will show that FTRL is equivalent to the GBPA run\nwith the inf-conv smoothing of the baseline potential function.\nLet (X, \u2225\u00b7 \u2225) be a normed vector space, and (X \u22c6, \u2225\u00b7 \u2225\u22c6) be its dual. Let \u03a6 : X \u22c6\u2192R be a\nclosed proper convex function, and let S be a \u03b2-strongly smooth function on X \u22c6with respect to\n\u2225\u00b7 \u2225\u22c6. Then, the inf-conv smoothing of \u03a6 with S is de\ufb01ned as:\n\u03a6ic(\u0398; \u03b7, S) def\n=\ninf\n\u0398\u2217\u2208X \u22c6\nn\n\u03a6(\u0398\u2217) + \u03b7S\n\u0012\u0398 \u2212\u0398\u2217\n\u03b7\n\u0013 o\n= max\nw\u2208X\nn\n\u27e8w, \u0398\u27e9\u2212\u03a6\u22c6(w) \u2212\u03b7S\u22c6(w)\no\n. (13)\nThe \ufb01rst expression with in\ufb01mum is precisely the in\ufb01mal convolution of \u03a6(\u00b7) and \u03b7S( \u00b7\n\u03b7), and\nthe second expression with supremum is an equivalent dual formulation. The inf-conv smoothing\n\u03a6ic(\u0398; \u03b7, S) is \ufb01nite, and it is an \u03b7-smoothing of \u03a6 (De\ufb01nition 3) with smoothing parameters\n\u0012\nmax\n\u0398\u2208X \u22c6\nmax\nw\u2208\u2202\u03a6(\u0398) S\u22c6(w), \u03b2, \u2225\u00b7 \u2225\n\u0013\n.\n(14)\nwhere \u2202\u03a6(\u0398) is a set of subgradients of \u03a6 at \u0398.\nConnection to FTRL\nConsider an online linear optimization problem with decision set X \u2286RN.\nThen, the dual space X \u22c6is simply RN. Let R be a \u03b2-strongly convex function on X with respect to\na norm \u2225\u00b7\u2225. By the strong convexity-strong smoothness duality, R\u22c6is 1\n\u03b2-strongly smooth. Consider\nthe inf-conv smoothing of the baseline potential function \u03a6 with R\u22c6, denoted \u03a6ic(\u0398; \u03b7, R\u22c6). We\nwill that the GBPA run with \u03a6ic(\u0398; \u03b7, R\u22c6) is equivalent to FTRL with R as the regularizer.\nFirst, note that the baseline potential is the convex conjugate of the null regularizer, i.e., \u03a6\u22c6(w) =\n0 for all w \u2208X. The dual formulation of inf-conv smoothing (Equation 13) thus becomes\n\u03a6ic(\u0398; \u03b7, S) = max\nw\u2208X\nn\n\u27e8w, \u0398\u27e9\u2212\u03b7R(w)\no\n,\nwhich is identical to Equation 3 except that the above expression has an extra parameter \u03b7 that\ncontrols the degree of smoothing. To simplify the deviation parameter in Equation 14, note that\nthe subgradients of \u03a6 always lie in X because of duality. Hence, the two supremum expressions\ncollapse to one supremum: maxw\u2208X S\u22c6(w). Plugging the smoothing parameters into Corollary 4\ngives the well-known FTRL regret bound as in Theorem 2.11 or 2.21 of Shalev-Shwartz (2012):\nRegret \u2264\u03b7 max\nw\u2208X S\u22c6(w) + \u03b2\n2\u03b7\nT\nX\nt=1\n\u2225\u03b8t\u22252.\n12\nONLINE LINEAR OPTIMIZATION VIA SMOOTHING\nAcknowledgments\nCL and AT gratefully acknowledge the support of NSF under grant IIS-1319810. We thank the\nanonymous reviewers for their helpful suggestions. We would also like to thank Andre Wibisono\nfor several very useful discussions and his help improving the manuscript. Finally we thank Elad\nHazan for early support in developing the ideas herein.\n13\nABERNETHY LEE SINHA TEWARI\nReferences\nJacob Abernethy, Peter L. Bartlett, Alexander Rakhlin, and Ambuj Tewari. Optimal stragies and\nminimax lower bounds for online convex games. In Proceedings of Conference on Learning\nTheory (COLT), 2008.\nJacob Abernethy, Yiling Chen, and Jennifer Wortman Vaughan. Ef\ufb01cient market making via con-\nvex optimization, and a connection to online learning. ACM Transactions on Economics and\nComputation, 1(2):12, 2013.\nAmir Beck and Marc Teboulle. Smoothing and \ufb01rst order methods: A uni\ufb01ed framework. SIAM\nJournal on Optimization, 22(2):557\u2013580, 2012.\nDimitri P. Bertsekas.\nStochastic optimization problems with nondifferentiable cost functionals.\nJournal of Optimization Theory and Applications, 12(2):218\u2013231, 1973. ISSN 0022-3239. doi:\n10.1007/BF00934819.\nShalabh Bhatnagar. Adaptive newton-based multivariate smoothed functional algorithms for simu-\nlation optimization. ACM Transactions on Modeling and Computer Simulation, 2007.\nChris M. Bishop. Training with noise is equivalent to tikhonov regularization. Neural Computation,\n7(1):108\u2013116, January 1995. ISSN 0899-7667.\nNicol`o Cesa-Bianchi and G\u00b4abor Lugosi. Prediction, learning, and games. Cambridge University\nPress, 2006. ISBN 978-0-521-84108-5.\nLuc Devroye, G\u00b4abor Lugosi, and Gergely Neu. Prediction by random-walk perturbation. In Pro-\nceedings of Conference on Learning Theory (COLT), 2013.\nJohn Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and\nstochastic optimization. In Proceedings of Conference on Learning Theory (COLT), 2010.\nJohn Duchi, Peter L. Bartlett, and Martin J. Wainwright. Randomized smoothing for stochastic\noptimization. SIAM Journal on Optimization, 22(2):674\u2013701, 2012. doi: 10.1137/110831659.\nYoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning and an\napplication to boosting. Journal of Computer and System Sciences, 55(1):119 \u2013 139, 1997. ISSN\n0022-0000. doi: http://dx.doi.org/10.1006/jcss.1997.1504.\nPaul Glasserman.\nGradient Estimation Via Perturbation Analysis.\nKluwer international series\nin engineering and computer science: Discrete event dynamic systems. Springer, 1991. ISBN\n9780792390954.\nJames Hannan. Approximation to bayes risk in repeated play. Contributions to the Theory of Games,\n3:97\u2013139, 1957.\nJohn C. Harsanyi. Oddness of the number of equilibrium points: a new proof. International Journal\nof Game Theory, 2(1):235\u2013250, 1973.\nJames R. Harvey. Fractional moments of a quadratic form in noncentral normal random variables,\nApril 1965.\n14\nONLINE LINEAR OPTIMIZATION VIA SMOOTHING\nGeoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.\nImproving neural networks by preventing co-adaptation of feature detectors.\nArXiv preprint,\narXiv:1207.0580, 2012.\nJosef Hofbauer and William H. Sandholm. On the global convergence of stochastic \ufb01ctitious play.\nEconometrica, 70(6):2265\u20132294, 2002.\nAdam T. Kalai and Santosh Vempala. Ef\ufb01cient algorithms for online decision problems. Journal of\nComputer and System Sciences, 71(3):291\u2013307, 2005.\nH. Brendan McMahan. Follow-the-regularized-leader and mirror descent: Equivalence theorems\nand l1 regularization. In AISTATS, pages 525\u2013533, 2011.\nIlya S. Molchanov. Theory of random sets. Probability and its applications. Springer, New York,\n2005. ISBN 1-85233-892-X.\nYurii Nesterov. Random gradient-free minimization of convex functions. ECORE Discussion Paper,\n2011.\nAlexander Rakhlin, Ohad Shamir, and Karthik Sridharan. Relax and randomize : From value to\nalgorithms. In Proceedings of Neural Information Processing Systems (NIPS), 2012.\nR.T. Rockafellar. Convex Analysis. Convex Analysis. Princeton University Press, 1997. ISBN\n9780691015866.\nShai Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends in\nMachine Learning, 4(2):107\u2013194, February 2012. ISSN 1935-8237.\nNati Srebro, Karthik Sridharan, and Ambuj Tewari. On the universality of online mirror descent. In\nProceedings of Neural Information Processing Systems (NIPS), pages 2645\u20132653, 2011.\nTim van Erven, Wojciech Kotlowski, and Manfred K. Warmuth. Follow the leader with dropout\nperturbations. In Proceedings of Conference on Learning Theory (COLT), 2014.\nStefan Wager, Sida Wang, and Percy Liang. Dropout training as adaptive regularization. In Pro-\nceedings of Neural Information Processing Systems (NIPS), 2013.\nManfred\nWarmuth.\nA\nperturbation\nthat\nmakes\n\u201cFollow\nthe\nleader\u201d\nequivalent\nto\n\u201cRandomized\nweighted\nmajority\u201d.\nhttp://classes.soe.ucsc.edu/cmps290c/Spring09/lect/10/wmkalai-rewrite.pdf,\n2009.\nFarzad Yousean, Angelia Nedi\u00b4c, and Uday V. Shanbhag. Convex nondifferentiable stochastic opti-\nmization: A local randomized smoothing technique. In Proceedings of American Control Con-\nference (ACC), 2010, pages 4875\u20134880, June 2010.\nMartin Zinkevich. Online convex programming and generalized in\ufb01nitesimal gradient ascent. In\nInternational Conference on Machine Learning (ICML), 2003.\n15\nABERNETHY LEE SINHA TEWARI\nAppendix A. Gradient-Based Prediction Algorithm\nA.1. Proof of Lemma 2\nProof We note that since \u03a60(0) = 0,\n\u03a6T (\u0398T ) =\nT\nX\nt=1\n\u03a6t(\u0398t) \u2212\u03a6t\u22121(\u0398t\u22121)\n=\nT\nX\nt=1\n\u0010\u0000\u03a6t(\u0398t) \u2212\u03a6t(\u0398t\u22121)\n\u0001\n+\n\u0000\u03a6t(\u0398t\u22121) \u2212\u03a6t\u22121(\u0398t\u22121)\n\u0001\u0011\nThe \ufb01rst difference can be rewritten as:\n\u03a6t(\u0398t) \u2212\u03a6t(\u0398t\u22121) = \u27e8\u2207\u03a6t(\u0398t\u22121), \u0398t)\u27e9+ D\u03a6t(\u0398t, \u0398t\u22121)\nBy combining the above two,\nRegret = \u03a6(\u0398T ) \u2212\nT\nX\nt=1\n\u27e8\u2207\u03a6t(\u0398t\u22121), \u0398t\u27e9\n= \u03a6(\u0398T ) \u2212\u03a6T (\u0398T ) +\nT\nX\nt=1\nD\u03a6t(\u0398t, \u0398t\u22121) + \u03a6t(\u0398t\u22121) \u2212\u03a6t\u22121(\u0398t\u22121)\nwhich completes the proof.\nAppendix B. FTPL-FTRL Duality\nB.1. Proof of Theorem 5\nProof Consider a one-dimensional online linear optimization prediction problem where the player\nchooses an action wt from X = [0, 1] and the adversary chooses a reward \u03b8t from Y = [0, 1]. This\ncan be interpreted as a two-expert setting; the player\u2019s action wt \u2208X is the probability of following\nthe \ufb01rst expert and \u03b8t is the net excess reward of the \ufb01rst expert over the second. The baseline\npotential for this setting is \u03a6(\u0398) = maxw\u2208[0,1] w\u0398.\nLet us consider an instance of FTPL with a continuous distribution D whose cumulative density\nfunction (cdf) is FD. Let \u02dc\u03a6 be the smoothed potential function (Equation 4) with distribution D. Its\nderivative is\n\u02dc\u03a6\u2032(\u0398) = E[arg max\nw\u2208K\nw(\u0398 + u)] = P[u > \u2212\u0398]\n(15)\nbecause the maximizer is unique with probability 1. Notice, crucially, that the derivative \u02dc\u03a6\u2032(\u0398) is\nexactly the expected solution of our FTPL instance. Moreover, by differentiating it again, we see\nthat the second derivative of \u02dc\u03a6 at \u0398 is exactly the pdf of D evaluated at (\u2212\u0398).\nWe can now precisely de\ufb01ne the mapping from FTPL to FTRL. Our goal is to \ufb01nd a convex\nregularization function R such that P(u > \u2212\u0398) = arg maxw\u2208X (w\u0398 \u2212R(w)). Since this is a\n16\nONLINE LINEAR OPTIMIZATION VIA SMOOTHING\none-dimensional convex optimization problem, we can differentiate for the solution. The character-\nization of R is:\nR(w) \u2212R(0) = \u2212\nZ w\n0\nF \u22121\nD (1 \u2212z)dz.\n(16)\nNote that the cdf FD(\u00b7) is indeed invertible since it is a strictly increasing function.\nThe inverse mapping is just as straightforward. Given a regularization function R well-de\ufb01ned\nover [0, 1], we can always construct its Fenchel conjugate R\u22c6(\u0398) = supw\u2208X\u27e8w, \u0398\u27e9\u2212R(w). The\nderivative of R\u22c6is an increasing convex function, whose in\ufb01mum is 0 at \u0398 = \u2212\u221eand supremum\nis 1 at \u0398 = +\u221e. Hence, R\u22c6de\ufb01nes a cdf, and an easy calculation shows that this perturbation\ndistribution exactly reproduces FTRL corresponding to R.\nAppendix C. Gaussian smoothing\nC.1. Proof of Equation 10\nLet X1, . . . , XN be independent standard Gaussian random variables, and let Z = maxi=1,...,N Xi.\nFor any real number a, we have\nexp(aE[Z]) \u2264E exp(aZ) = E max\ni=1,...,N exp(aXi) \u2264\nN\nX\ni=1\nE[exp(aXi)] = N exp(a2/2).\nThe \ufb01rst inequality is from the convexity of the exponential function, and the last equality is by\nthe de\ufb01nition of the moment generating function of Gaussian random variables. Taking the natural\nlogarithm of both sides and dividing by a gives\nE[Z] \u2264log N\na\n+ a\n2.\nIn particular, by choosing a = \u221a2 log N, we have E[Z] \u2264\u221a2 log N.\nC.2. Proof of Lemma 10\nProof By the subadditivity (triangle inequality) of \u03a6,\n\u02dc\u03a6(\u0398; \u03b7, N(0, I)) \u2212\u02dc\u03a6(\u0398; \u03b7\u2032, N(0, I)) = Eu\u223cN (0,I)[\u03a6(\u0398 + \u03b7u) \u2212\u03a6(\u0398 + \u03b7\u2032u)]\n(17)\n\u2264Eu\u223cN (0,I)[\u03a6((\u03b7 \u2212\u03b7\u2032)u)]\n(18)\nand the statement follows from the positive homogeneity of \u03a6.\nC.3. Proof that the origin is the worst case (Lemma 12)\nProof Let \u03a6(\u0398) = \u2225\u0398\u22252 and \u03b7 be a positive number. By continuity of eigenvectors, it suf\ufb01ces\nto show that the maximum eigenvalue of the Hessian matrix of the Gaussian smoothed potential\n\u02dc\u03a6(\u0398; \u03b7, N(0, I)) is decreasing in \u2225\u0398\u2225for \u2225\u0398\u2225> 0.\nBy Lemma 7, the gradient can be written as follows:\n\u2207\u02dc\u03a6(\u0398; \u03b7, N(0, I)) = 1\n\u03b7Eu\u223cN (0,I)[u\u2225\u0398 + \u03b7u\u2225]\n(19)\n17\nABERNETHY LEE SINHA TEWARI\nLet ui be the i-th coordinate of the vector u. Since the standard normal distribution is spherically\nsymmetric, we can rotate the random variable u such that its \ufb01rst coordinate u1 is along the direction\nof \u0398. After rotation, the gradient can be written as\n1\n\u03b7 Eu\u223cN (0,I)\n\uf8ee\n\uf8f0u\nv\nu\nu\nt(\u2225\u0398\u2225+ \u03b7u1)2 +\nN\nX\nk=2\n\u03b72u2\nk\n\uf8f9\n\uf8fb\nwhich is clearly independent of the coordinates of \u0398. The pdf of standard Gaussian distribution\nhas the same value at (u1, u2, . . . , un) and its sign-\ufb02ipped pair (u1, \u2212u2, . . . , \u2212un). Hence, in\nexpectation, the two vectors cancel out every coordinate but the \ufb01rst, which is along the direction of\n\u0398. Therefore, there exists a function \u03b1 such that Eu\u223cN (0,I)[u\u2225\u0398 + \u03b7u\u2225] = \u03b1(\u2225\u0398\u2225)\u0398.\nNow, we will show that \u03b1 is decreasing in \u2225\u0398\u2225. Due to symmetry, it suf\ufb01ces to consider \u0398 = te1\nfor t \u2208R+, without loss of generality. For any t > 0,\n\u03b1(t) = E[u1\nq\n(t + \u03b7u1)2 + u2\nrest)]/t\n= Eurest[Eu1[u1\np\n(t + \u03b7u1)2 + b2|urest = b]]/t\n= Eurest[Ea=\u03b7|u1|[a\n\u0010p\n(t + a)2 + b2 \u2212\np\n(t \u2212a)2 + B\n\u0011\n|urest = b]]/t\nLet g(t) =\n\u0010p\n(t + a)2 + B \u2212\np\n(t \u2212a)2 + B\n\u0011\n/t. Take the \ufb01rst derivative with respect to t,\nand we have:\ng\u2032(t) = 1\nt2\n \np\n(t \u2212a)2 + b2 \u2212\nt(t \u2212a)\np\n(t + a)2 + b2 \u2212\np\n(t + a)2 + b2 +\nt(t \u2212a)\np\n(t + a)2 + b2\n!\n= 1\nt2\n \na2 + b2 \u2212at\np\n(t \u2212a)2 + b2 \u2212\na2 + b2 + at\np\n(t + a)2 + b2\n!\n\u0010\n(a2 + b2) \u2212at\n\u00112\u0010\n(t + a)2 + b2\u0011\n\u2212\n\u0010\n(a2 + b2) + at\n\u00112\u0010\n(t \u2212a)2 + b2\u0011\n= \u22124ab2t3 < 0\nbecause t, \u03b7, u\u2032, B are all positive. So, g(t) < 0, which proves that \u03b1 is decreasing in \u0398.\nThe \ufb01nal setp is to write the gradient as \u2207(\u02dc\u03a6; \u03b7, N(0, I))(\u0398) = \u03b1(\u2225\u0398\u2225)\u0398 and differentiate it:\n\u22072f\u03b7(\u0398) = \u03b1\u2032(\u2225\u0398\u2225)\n\u2225\u0398\u2225\n\u0398\u0398T + \u03b1(\u2225\u0398\u2225)I\nThe Hessian has two distinct eigenvalues \u03b1(\u2225\u0398\u2225) and \u03b1(\u2225\u0398\u2225) + \u03b1\u2032(\u2225\u0398\u2225)\u2225\u0398\u2225, which correspond\nto the eigenspace orthogonal to \u0398 and parallel to \u0398, respectively. Since \u03b1\u2032 is negative, \u03b1 is always\nthe maximum eigenvalue and it decreases in \u2225\u0398\u2225.\n18\n",
        "sentence": " The player receives a gain Ak \u2022Wk def = Tr(AkWk) \u2208 [0, 1]. We denote by \u03a3 = A1 + \u00b7 \u00b7 \u00b7+ AT , by nnz(A) = maxk\u2208[T ] { nnz(Ak) } , and by \u03bb = 1 T \u03bbmax(\u03a3) \u2208 [0, 1]. showed that FTPL strategies can also be analyzed using a FTRL framework [1]. Let {Zt}t=1 be a random process with respect to a filter {0,\u03a9} = F0 \u2282 F1 \u2282 \u00b7 \u00b7 \u00b7 \u2282 FT and each Zt \u2208 [0, 1] is Ft-measurable. (The first inequality has used (a\u2212 b)2 \u2264 a2 + b2 when a, b \u2265 0, and the second has used Zt \u2208 [0, 1]. Above, \u00ac uses Tr(AT\u03a6T\u22121AT ) \u2264 Tr(AT\u03a6T\u22121) as well as wkw> k = \u03a6k\u22121/Tr(\u03a6k\u22121), \u00ad uses 1+x \u2264 ex, \u00ae uses 1+2x \u2265 e2x\u22122x for x \u2208 [0, 1], \u00ad uses E[\u03bd> 1 \u03a60\u03bd1] = 1, \u00b0 uses the Lieb-Thirring inequality Tr(ABAB) \u2264 Tr(A2B2),22 \u00b1 uses (I + \u03b7A1) I + (4\u03b7 + 11\u03b7)A1.",
        "context": "this algorithm plays \u2207\u03a6t(\u0398t\u22121) = arg maxw\u27e8w, \u0398t\u22121\u27e9, which is equivalent to FTL (Cesa-Bianchi\nand Lugosi, 2006, Section 3.2). FTL suffers zero regret from the over- or underestimation penalty,\nw\u2208X {\u27e8w, \u0398\u27e9\u2212R(w)}\n(3)\nwhere R : X \u2192R is a \u03b2-strongly convex function. At time t, this algorithm plays \u2207\u03a6t(\u0398t\u22121) =\narg maxw{\u27e8w, \u0398t\u22121\u27e9\u2212R(w)}, which is equivalent to FTRL. By the strong convexity-strong smooth-\nness duality, \u03a6t is 1\nw\u2208K\nw(\u0398 + u)] = P[u > \u2212\u0398]\n(15)\nbecause the maximizer is unique with probability 1. Notice, crucially, that the derivative \u02dc\u03a6\u2032(\u0398) is\nexactly the expected solution of our FTPL instance. Moreover, by differentiating it again, we see"
    },
    {
        "title": "Spectral smoothing via random matrix perturbations",
        "author": [
            "Jacob Abernethy",
            "Chansoo Lee",
            "Ambuj Tewari"
        ],
        "venue": "ArXiv e-prints,",
        "citeRegEx": "2",
        "shortCiteRegEx": "2",
        "year": 2015,
        "abstract": "We consider stochastic smoothing of spectral functions of matrices using\nperturbations commonly studied in random matrix theory. We show that a spectral\nfunction remains spectral when smoothed using a unitarily invariant\nperturbation distribution. We then derive state-of-the-art smoothing bounds for\nthe maximum eigenvalue function using the Gaussian Orthogonal Ensemble (GOE).\nSmoothing the maximum eigenvalue function is important for applications in\nsemidefinite optimization and online learning. As a direct consequence of our\nGOE smoothing results, we obtain an $O((N \\log N)^{1/4} \\sqrt{T})$ expected\nregret bound for the online variance minimization problem using an algorithm\nthat performs only a single maximum eigenvector computation per time step. Here\n$T$ is the number of rounds and $N$ is the matrix dimension. Our algorithm and\nits analysis also extend to the more general online PCA problem where the\nlearner has to output a rank $k$ subspace. The algorithm just requires\ncomputing $k$ maximum eigenvectors per step and enjoys an $O(k (N \\log N)^{1/4}\n\\sqrt{T})$ expected regret bound.",
        "full_text": "",
        "sentence": " If instead of playing an arbitrary matrix in \u2206d, the player is only allowed to play a rank-1 matrix Wk = wkw > k , then this online matrix optimization becomes the well-known online eigenvector problem [2, 13, 16, 21, 25]: Many researchers also analyzed the so-called followthe-perturbed-leader (FTPL) strategy for this problem [2, 13, 16, 21].",
        "context": null
    },
    {
        "title": "Using optimization to obtain a widthindependent, parallel, simpler, and faster positive SDP solver",
        "author": [
            "Zeyuan Allen-Zhu",
            "Yin Tat Lee",
            "Lorenzo Orecchia"
        ],
        "venue": "In Proceedings of the 27th ACM-SIAM Symposium on Discrete Algorithms,",
        "citeRegEx": "3",
        "shortCiteRegEx": "3",
        "year": 2016,
        "abstract": "",
        "full_text": "",
        "sentence": " Its natural matrix extension, commonly known as matrix multiplicative weight update (MMWU) [26], has been used towards efficient algorithms for solving semidefinite programs [3, 10, 28], balanced separators [27], Ramanujan sparsifiers [7, 22], and even in the proof of QIP = PSPACE [19]. 4 Some researchers [3, 7, 22, 28] use the Johnson-Lindenstrauss (JL) compression to reduce the dimension of Wk to make it more efficiently computable. If this is the case, then one can compute the 3\u00d7 3 matrix ( ui Xkuj ) i,j\u2208[3] explicitly, and then we can obtain its rank-3 eigendecomposition X 1/2 k UX 1/2 k = \u22113 j=1 pj \u00b7 yjy> j in O(d) time. from N (0, 1) for every i \u2208 [d], j \u2208 [3]. Notice that TrU = 13 \u2211 i\u2208[d],j\u2208[3](uj,i) 2 so 3TrU is distributed according to chisquared distribution \u03c72(3d) whose PDF is p(x) = 2 \u2212 3d 2 e\u2212 x 2 x 3d 2 \u22121 \u0393(3d/2) . Moreover, this can be done in time O(d) as long as we can compute the three vectors { Xuj } j\u2208[3] to an additive \u03b5\u0303/poly(d, T ) error in Euclidean norm.",
        "context": null
    },
    {
        "title": "Doubly Accelerated Methods for Faster CCA and Generalized Eigendecomposition",
        "author": [
            "Zeyuan Allen-Zhu",
            "Yuanzhi Li"
        ],
        "venue": "ArXiv e-prints,",
        "citeRegEx": "4",
        "shortCiteRegEx": "4",
        "year": 2016,
        "abstract": "We study $k$-GenEV, the problem of finding the top $k$ generalized\neigenvectors, and $k$-CCA, the problem of finding the top $k$ vectors in\ncanonical-correlation analysis. We propose algorithms $\\mathtt{LazyEV}$ and\n$\\mathtt{LazyCCA}$ to solve the two problems with running times linearly\ndependent on the input size and on $k$.\n  Furthermore, our algorithms are DOUBLY-ACCELERATED: our running times depend\nonly on the square root of the matrix condition number, and on the square root\nof the eigengap. This is the first such result for both $k$-GenEV or $k$-CCA.\nWe also provide the first gap-free results, which provide running times that\ndepend on $1/\\sqrt{\\varepsilon}$ rather than the eigengap.",
        "full_text": "Doubly Accelerated Methods for\nFaster CCA and Generalized Eigendecomposition\nZeyuan Allen-Zhu\nzeyuan@csail.mit.edu\nInstitute for Advanced Study\nYuanzhi Li\nyuanzhil@cs.princeton.edu\nPrinceton University\nJuly 20, 2016\u2217\nAbstract\nWe study k-GenEV, the problem of \ufb01nding the top k generalized eigenvectors, and k-CCA,\nthe problem of \ufb01nding the top k vectors in canonical-correlation analysis. We propose algorithms\nLazyEV and LazyCCA to solve the two problems with running times linearly dependent on the\ninput size and on k.\nFurthermore, our algorithms are doubly-accelerated: our running times depend only on the\nsquare root of the matrix condition number, and on the square root of the eigengap. This is the\n\ufb01rst such result for both k-GenEV or k-CCA. We also provide the \ufb01rst gap-free results, which\nprovide running times that depend on 1/\u221a\u03b5 rather than the eigengap.\n1\nIntroduction\nThe Generalized Eigenvector (GenEV) problem and the Canonical Correlation Analysis (CCA)\nare two fundamental problems in scienti\ufb01c computing, machine learning, operations research, and\nstatistics. Algorithms solving these problems are often used to extract features to compare large-\nscale datasets, as well as used for problems in regression [17], clustering [10], classi\ufb01cation [18],\nword embeddings [11], and many others.\nGiven two symmetric matrices A, B \u2208Rd\u00d7d where B is positive de\ufb01nite. The GenEV problem\nis to \ufb01nd generalized eigenvectors v1, . . . , vd where each vi satis\ufb01es\nvi \u2208arg max\nv\u2208Rd\n\f\fv\u22a4Av\n\f\f\nsuch that\n\u001a v\u22a4Bv = 1\nv\u22a4Bvj = 0 \u2200j \u2208[i \u22121]\nThe values \u03bbi\ndef\n= v\u22a4\ni Avi are known as the generalized eigenvalues, and it satis\ufb01es |\u03bb1| \u2265\u00b7 \u00b7 \u00b7 |\u03bbd|.\nFollowing the tradition of [13, 29], we assume without loss of generality that \u03bbi \u2208[\u22121, 1].\nGiven matrices X \u2208Rn\u00d7dx, Y\n\u2208Rn\u00d7dy and denoting by Sxx =\n1\nnX\u22a4X, Sxy =\n1\nnX\u22a4Y ,\nSyy = 1\nnY \u22a4Y , the CCA problem is to \ufb01nd canonical-correlation vectors {(\u03c6i, \u03c8i)}r\ni=1 where r =\nmin{dx, dy} and each pair\n(\u03c6i, \u03c8i) \u2208\narg max\n\u03c6\u2208Rdx,\u03c8\u2208Rdy\n\b\n\u03c6\u22a4Sxy\u03c8\n\t\nsuch that\n\u001a \u03c6\u22a4Sxx\u03c6 = 1 \u2227\u03c6\u22a4Sxx\u03c6j = 0 \u2200j \u2208[i \u22121]\n\u03c8\u22a4Syy\u03c8 = 1 \u2227\u03c8\u22a4Syy\u03c8j = 0 \u2200j \u2208[i \u22121]\nThe values \u03c3i\ndef\n= \u03c6\u22a4\ni Sxy\u03c8i \u22650 are known as the canonical-correlation coe\ufb03cients, and it always\nsatis\ufb01es 1 \u2265\u03c31 \u2265\u00b7 \u00b7 \u00b7 \u2265\u03c3r \u22650.\n\u2217First appeared on arXiv on this date.\nIn this new version, we have stated more clearly why this paper has\noutperformed relevant previous results, and included discussions for doubly-stochastic methods.\n1\narXiv:1607.06017v2  [math.OC]  26 Nov 2016\nIt is a folklore that solving CCA exactly can be reduced to solving GenEV exactly, if one de\ufb01nes\nB = diag{Sxx, Syy} \u2208Rd\u00d7d and A = [[0, Sxy]; [S\u22a4\nxy, 0]] \u2208Rd\u00d7d for d\ndef\n= dx + dy, see Lemma 2.3.\nDespite the fundamental importance and the frequent necessity in applications, there are few\nresults on obtaining provably e\ufb03cient algorithms for GenEV and CCA until very recently. In the\nbreakthrough result of Ma, Lu and Foster [20], they proposed to study algorithms to \ufb01nd top k\ngeneralized eigenvectors (k-GenEV) or top k canonical-correlation vectors (k-CCA). They designed\nan alternating minimization algorithm whose running time is only linear in terms of the number of\nnon-zero elements of the matrix (that we denote by nnz(A) for a matrix A in this paper), and also\nnearly-linear in k. Such algorithms are very appealing because in real-life applications, it is often\nonly relevant to obtain top correlation vectors, as opposed to the less meaningful vectors in the\ndirections where the datasets do not correlate. Unfortunately, the method of Ma, Lu and Foster\nhas a running time that linearly scales with \u03ba and 1/gap, where\n\u2022 \u03ba \u22651 is the condition number of matrix B in GenEV, or of matrices X\u22a4X, Y \u22a4Y in CCA; and\n\u2022 gap \u2208[0, 1) is the (relative) eigengap \u03bbk\u2212\u03bbk+1\n\u03bbk\nin GenEV, or \u03c3k\u2212\u03c3k+1\n\u03c3k\nin CCA.\nThese parameters are usually not constants and scale with the problem size.\nChallenge 1: Acceleration\nFor many easier scienti\ufb01c computing problems, we are able to design algorithms that have acceler-\nated dependencies on \u03ba and 1/gap. As two concrete examples, k-PCA can be solved with a running\ntime linearly in 1/\u221agap as opposed to 1/gap [16]; computing B\u22121w for a vector w can be solved in\ntime linearly in \u221a\u03ba as opposed to \u03ba, where \u03ba is the condition number of matrix B [9, 23, 27].\nTherefore, can we obtain doubly-accelerated methods for k-GenEV and k-CCA, meaning\nthat the running times linearly scale with both \u221a\u03ba and 1/\u221agap? Before this paper, for the general\ncase k > 1, the method of Ge et al. [15] made acceleration possible for parameter \u03ba, but not for\nparameter 1/gap (see Table 2).\nChallenge 2: Gap-Freeness\nSince gap can be even zero in the extreme case, can we design algorithms that do not scale with\n1/gap? Recall that this is possible for the easier task of k-PCA. The block Krylov method [22] runs\nin time linear in 1/\u221a\u03b5 as opposed to 1/\u221agap, where \u03b5 is the approximation ratio.\nThere is no gap-free result previously known for k-GenEV or k-CCA even for k = 1.\nChallenge 3: Stochasticity\nFor matrix-related problems, one can often obtain a stochastic running time which requires some\nnotations to describe. Consider the simple task of computing B\u22121w for some vector w, where recall\naccelerated methods solve this problem with a running time linearly in \u221a\u03ba for \u03ba being the condition\nnumber of B. If B = 1\nnX\u22a4X is given as the covariance matrix of X \u2208Rn\u00d7d, then (accelerated)\nstochastic gradient methods can be used to compute B\u22121w in a time linearly in (1+\np\n\u03ba\u2032/n) instead\nof \u221a\u03ba, where \u03ba\u2032 =\nmaxi\u2208[n]{\u2225Xi\u22252}\n\u03bbmin(B)\n\u2208\n\u0002\n\u03ba, n\u03ba\n\u0003\nand Xi is the i-th row of X. (See Lemma 2.6.) Since\n1 +\np\n\u03ba\u2032/n \u2264O(\u221a\u03ba), stochastic methods are no slower than non-stochastic ones.\nTherefore, can we obtain a similar (but doubly-accelerated!) stochastic method for k-CCA?1\nNote that, if the doubly-accelerated requirement is dropped, this task is easier and indeed possible,\nsee Ge et al. [15]. However, since their stochastic method is not doubly-accelerated, in certain\nparameter regimes, it can run slower than non-stochastic ones (even for k = 1, see Table 1).\n1 Note that a similar problem can be also asked for k-GenEV when A and B are both given in their covariance\nmatrix forms. We refrain from doing it in this paper for notational simplicity.\n2\nRemark.\nIn general, if designed properly,\n\u2022 Accelerated results are better because they are never slower than non-accelerated ones.\n\u2022 Gap-free results are better because they imply gap-dependent ones.2\n\u2022 Stochastic results are better because they are never slower than non-stochastic ones.\n1.1\nOur Main Results\nWe provide algorithms LazyEV and LazyCCA that are doubly-accelerated, gap-free, and stochastic.3\nFor the general k-GenEV problem, our LazyEV can be implemented to run in time\neO\n\u0010knnz(B)\u221a\u03ba\n\u221agap\n+ knnz(A) + k2d\n\u221agap\n\u0011\nor\neO\n\u0010knnz(B)\u221a\u03ba\n\u221a\u03b5\n+ knnz(A) + k2d\n\u221a\u03b5\n\u0011\nin the gap-dependent and gap-free cases respectively. Since our running time only linearly depends\non \u221a\u03ba and \u221agap (resp. \u221a\u03b5), our algorithm LazyEV is doubly-accelerated.\nFor the general k-CCA problem, our LazyCCA can be implemented to run in time\neO\n\u0010knnz(X, Y ) \u00b7\n\u00001 +\np\n\u03ba\u2032/n\n\u0001\n+ k2d\n\u221agap\n\u0011\nor\neO\n\u0010knnz(X, Y ) \u00b7\n\u00001 +\np\n\u03ba\u2032/n\n\u0001\n+ k2d\n\u221a\u03b5\n\u0011\nin the gap-dependent and gap-free cases respectively. Here, nnz(X, Y ) = nnz(X) + nnz(Y ) and\n\u03ba\u2032 = 2 maxi{\u2225Xi\u22252,\u2225Yi\u22252}\n\u03bbmin(diag{Sxx,Syy}) where Xi or Yi is the i-th row vector of X or Y . Therefore, our algorithm\nLazyCCA is doubly-accelerated and stochastic.\nWe fully compare our running time with prior work in Table 1 and Table 2, and summarize our\nmain contributions below.\n\u2022 For k > 1, we have outperformed all relevant prior works (see Table 2). Moreover, no known\nmethod was doubly-accelerated even in the non-stochastic setting.\n\u2022 For k \u22651, we have obtained the \ufb01rst gap-free running times.\n\u2022 Even for k = 1, we have outperformed most of the state-of-the-arts (see Table 1).\nOther Contributions.\nBesides the aforementioned running time improvements, we summarize\nsome other virtues of our algorithms as follows:\n\u2022 For GenEV, our LazyEV distinguishes positive generalized eigenvalues from negative ones. For\ninstance, if A has two generalized eigenvectors v1, v2 with respect to B, one with eigenvalue\n\u03bb and the other with \u2212\u03bb. Then, previous result GenELin only \ufb01nds the subspace spanned by\nv1, v2 but cannot distinguish v1 from v2.\n\u2022 For CCA with k > 1, previous result CCALin only outputs the subspace spanned by the top k\ncorrelation vectors but not identify which vector gives the highest correlation and so on. Our\nLazyCCA provides per-vector guarantees on all the top k correlation vectors, see Theorem 6.3.\n\u2022 Our LazyEV and LazyCCA reduce the underlying non-convex problem to multiple calls of\nquadratic minimization. Since quadratic minimization is a well-studied convex optimization\nproblem, many e\ufb03cient and robust algorithms can be found. In contrast, previous results for\nthe k > 1 case rely on more sophisticated nonconvex optimization; and the previous work\nof [29] \u2014although uses convex optimization to solve 1-CCA\u2014 requires one to work with a\nsum-of-non-convex function which is usually less e\ufb03cient to minimize.\n2If a method depends on 1/\u03b5 then one can choose \u03b5 = gap and this translates to a gap-dependent running time.\n3Recalling Footnote 1, for notational simplicity, we only state our k-GenEV result in non-stochastic running time.\n3\nProblem\nPaper\nRunning time\n(\u00d7 for beaten)\ngap-free?\nnegative EV?\n1-GenEV\nGenELin [15]\neO\n\u0000 nnz(B)\u221a\u03baB\ngap\n+ nnz(A)\ngap\n\u0001\n\u00d7\nno\nno\nLazyEV Theorem 4.3\neO\n\u0000 nnz(B)\u221a\u03baB\n\u221agap\n+ nnz(A)\n\u221agap\n\u0001\nno\nyes\nLazyEV Theorem 4.4\neO\n\u0000 nnz(B)\u221a\u03baB\n\u221a\u03b5\n+ nnz(A)\n\u221a\u03b5\n\u0001\nyes\nyes\nProblem\nPaper\nRunning time\n(\u00d7 for beaten)\ngap-free?\nstochastic?\n1-CCA\nAppGrad [20]\nnnz(X, Y ) \u00b7 eO\n\u0000 \u03ba\ngap\n\u0001\n\u00d7\nno\nno\nCCALin [15]\nnnz(X, Y ) \u00b7 eO\n\u0000 \u221a\u03ba\ngap\n\u0001\n\u00d7\nno\nno\nALS [29]\nnnz(X, Y ) \u00b7 eO\n\u0000 \u221a\u03ba\ngap2\n\u0001\n\u00d7\nno\nno\nSI [29]\nnnz(X, Y ) \u00b7 eO\n\u0000\u221a\u03ba\n\u221agap\u00b7\u03c31\n\u0001\n\u00d7\nno\nno\nCCALin [15]\nnnz(X, Y ) \u00b7 eO\n\u0000 1+\u221a\n\u03ba\u2032/n\ngap\n\u0001\n\u00d7\nno\nyes\nALS [29]\nnnz(X, Y ) \u00b7 eO\n\u0000 1+\u221a\n\u03ba\u2032/n\ngap2\n\u0001\n\u00d7\nno\nyes\nLazyCCA Theorem 6.2\nnnz(X, Y ) \u00b7 eO\n\u0000 1+\u221a\n\u03ba\u2032/n\n\u221agap\n\u0001\nno\nyes\nLazyCCA Theorem 6.3\nnnz(X, Y ) \u00b7 eO\n\u0000 1+\u221a\n\u03ba\u2032/n\n\u221a\u03b5\n\u0001\nyes\nyes\nSI [29]\nnnz(X, Y ) \u00b7 eO\n\u0010\n1 +\n\u221a\n\u03ba\u2032/n1/4\n\u221agap\u00b7\u03c31\n\u0011\n(see Remark 3)\nno\ndoubly\nLazyCCA Theorem 6.2\nnnz(X, Y ) \u00b7 eO\n\u00001 +\n\u221a\n\u03ba\u2032/n1/4\n\u221agap\u00b7\u03c31\n\u0001\nno\ndoubly\nLazyCCA Theorem 6.3\nnnz(X, Y ) \u00b7 eO\n\u00001 +\n\u221a\n\u03ba\u2032/n1/4\n\u221a\u03b5\u00b7\u03c31\n\u0001\nyes\ndoubly\nTable 1: Performance comparison on 1-GenEV and 1-CCA.\nIn GenEV, gap = \u03bb1\u2212\u03bb2\n\u03bb1\n\u2208[0, 1] and \u03baB = \u03bbmax(B)\n\u03bbmin(B) > 1.\nIn CCA, gap = \u03c31\u2212\u03c32\n\u03c31\n\u2208[0, 1], \u03ba = \u03bbmax(diag{Sxx,Syy})\n\u03bbmin(diag{Sxx,Syy}) > 1, \u03ba\u2032 = 2 maxi{\u2225Xi\u22252,\u2225Yi\u22252}\n\u03bbmin(diag{Sxx,Syy}) \u2208[\u03ba, 2n\u03ba], and \u03c31 \u2208[0, 1].\nRemark 1. Stochastic methods depend on modi\ufb01ed condition number \u03ba\u2032; the reason \u03ba\u2032 \u2208[\u03ba, 2n\u03ba] is in Def. 2.4.\nRemark 2. All non-stochastic CCA methods in this table have been outperformed because 1 +\np\n\u03ba\u2032/n \u2264O(\u03ba).\nRemark 3. Doubly-stochastic methods are not necessarily interesting. We discuss them in Section 1.2.\nRemark 4. Some CCA methods have a running time dependency on \u03c31 \u2208[0, 1], and this is intrinsic and cannot be\nremoved. In particular, if we scale the data matrix X and Y , the value \u03c31 stays the same.\nRemark 5.\nThe only (non-doubly-stochastic) doubly-accelerated method before our work is SI [29] (for 1-CCA\nonly). Our LazyEV is faster than theirs by a factor \u2126(\np\nn\u03ba/\u03ba\u2032 \u00d7\np\n1/\u03c31). Here, n\u03ba/\u03ba\u2032 \u22651/2 and 1/\u03c31 \u22651 are two\nscaling-invariant quantities usually much greater than 1.\n4\nProblem\nPaper\nRunning time\n(\u00d7 for beaten)\ngap-free?\nnegative EV?\nk-GenEV\nGenELin [15]\neO\n\u0000 knnz(B)\u221a\u03baB\ngap\n+ knnz(A)+k2d\ngap\n\u0001\n\u00d7\nno\nno\nLazyEV Theorem 4.3\neO\n\u0000 knnz(B)\u221a\u03baB\n\u221agap\n+ knnz(A)+k2d\n\u221agap\n\u0001\nno\nyes\nLazyEV Theorem 4.4\neO\n\u0000 knnz(B)\u221a\u03baB\n\u221a\u03b5\n+ knnz(A)+k2d\n\u221a\u03b5\n\u0001\nyes\nyes\nProblem\nPaper\nRunning time\n(\u00d7 for beaten)\ngap-free?\nstochastic?\nk-CCA\nAppGrad [20]\neO\n\u0000 knnz(X,Y )\u00b7\u03ba+k2d\ngap\n\u0001\n(local conv.)\n\u00d7\nno\nno\nCCALin [15]\neO\n\u0000 knnz(X,Y )\u00b7\u221a\u03ba+k2d\ngap\n\u0001\n\u00d7\nno\nno\nCCALin [15]\neO\n\u0000 knnz(X,Y )\u00b7\n\u00001+\u221a\n\u03ba\u2032/n\n\u0001\n+k2d\ngap\n\u0001\n\u00d7\nno\nyes\nLazyCCA Theorem 6.2\neO\n\u0000 knnz(X,Y )\u00b7\n\u00001+\u221a\n\u03ba\u2032/n\n\u0001\n+k2d\n\u221agap\n\u0001\nno\nyes\nLazyCCA Theorem 6.3\neO\n\u0000 knnz(X,Y )\u00b7\n\u00001+\u221a\n\u03ba\u2032/n\n\u0001\n+k2d\n\u221a\u03b5\n\u0001\nyes\nyes\nLazyCCA Theorem 6.2\neO\n\u0000knnz(X, Y )\u00b7\n\u00001+\n\u221a\n\u03ba\u2032\n\u221agap\u00b7\u03c3k\u00b7(nnz(X,Y )/kd)1/4\n\u0001\u0001\nno\ndoubly\nLazyCCA Theorem 6.3\neO\n\u0000knnz(X, Y ) \u00b7\n\u00001 +\n\u221a\n\u03ba\u2032\n\u221a\u03b5\u00b7\u03c3k\u00b7(nnz(X,Y )/kd)1/4\n\u0001\u0001\nyes\ndoubly\nTable 2: Performance comparison on k-GenEV and k-CCA.\nIn GenEV, gap =\n\u03bbk\u2212\u03bbk+1\n\u03bbk\n\u2208[0, 1] and \u03baB = \u03bbmax(B)\n\u03bbmin(B) > 1.\nIn CCA, gap =\n\u03c3k\u2212\u03c3k+1\n\u03c3k\n\u2208[0, 1], \u03ba = \u03bbmax(diag{Sxx,Syy})\n\u03bbmin(diag{Sxx,Syy}) > 1, \u03ba\u2032 = 2 maxi{\u2225Xi\u22252,\u2225Yi\u22252}\n\u03bbmin(diag{Sxx,Syy}) \u2208[\u03ba, 2n\u03ba], and \u03c3k \u2208[0, 1].\nRemark 1. Stochastic methods depend on a modi\ufb01ed condition number \u03ba\u2032. The reason \u03ba\u2032 \u2208[\u03ba, 2n\u03ba] is in Def. 2.4.\nRemark 2. All non-stochastic CCA methods in this table have been outperformed because 1 +\np\n\u03ba\u2032/n \u2264O(\u03ba).\nRemark 3. Doubly-stochastic methods are not necessarily interesting. We discuss them in Section 1.2.\n1.2\nOur Side Results on Doubly-Stocahstic Methods\nRecall that when considering acceleration, there are two parameters \u03ba and 1/gap. One can also\ndesign stochastic methods with respect to both parameters \u03ba and 1/gap, meaning that\nwith a running time proportional to\n1 +\n\u221a\n\u03ba\u2032/nc\n\u221agap\n(constant c is usually 1/2)\ninstead of\n1\n\u221agap +\n\u221a\n\u03ba\u2032/n\n\u221agap\n(stochastic) or\n\u221a\u03ba\n\u221agap (non-stochastic).\nWe call such methods doubly-\nstochastic.\nUnfortunately, doubly-stochastic methods are usually slower than stochastic ones.\nTake 1-\nCCA as an example. The best stochastic running time (obtained exclusively by us) for 1-CCA is\nnnz(X, Y ) \u00b7 eO\n\u0000 1+\u221a\n\u03ba\u2032/n\n\u221agap\n\u0001\n. In contrast, if one uses a doubly-stochastic method \u2014either [29] or our\nLazyCCA\u2014 the running time becomes nnz(X, Y ) \u00b7 eO\n\u00001 +\n\u221a\n\u03ba\u2032/n1/4\n\u221agap\u00b7\u03c31\n\u0001\n. Therefore, for 1-CCA,\ndoubly-stochastic methods are faster than stochastic ones only when\n\u03ba\u2032\n\u03c31 \u2264o(n1/2) .\nThe above condition is usually not satis\ufb01ed. For instance, (1) \u03ba\u2032 is usually around n for most\ninteresting data-sets, cf. the experiments of [25]; (2) \u03ba\u2032 is between n1/2 and 100n in all the CCA\nexperiments of [29]; and (3) by Def. 2.4 it satis\ufb01es \u03ba\u2032 \u2265d so \u03ba\u2032 cannot be smaller than o(n1/2)\nunless d \u226an1/2.4 Even worse, parameter \u03c31 \u2208[0, 1] is usually much smaller than 1. Note that \u03c31\nis scaling invariant: even if one scales X and Y up by the same factor, \u03c31 remains unchanged.\n4Note that item (3) \u03ba\u2032 \u2265d may not hold in the more general setting of CCA, see Remark 2.5.\n5\nNevertheless, in order to compare our LazyCCA framework with all relevant prior works, we\nalso obtain doubly-stochastic running times for k-CCA. Our running time matches that of [29]\nwhen k = 1, and no doubly-stochastic running time for k > 1 was known before our work.\n1.3\nOther Related Works\nFor the easier task of PCA and SVD, the \ufb01rst gap-free result was obtained by Musco and Musco [22]\n(or in the online setting by [3]), the \ufb01rst stochastic result was obtained by Shamir [26], and the\n\ufb01rst accelerated stochastic result was obtained by Garber et al. [13, 14].\nThe shift-and-invert\npreconditioning technique of Garber et al. is also used in this paper.\nFor another related problem PCR (principle component regression), we recently obtained an\naccelerated method [4] as opposed the previously non-accelerated one [12]; however, the acceleration\ntechniques in [4] are not relevant to this paper.\nFor GenEV and CCA, many scalable algorithms have been designed recently [19\u201321, 28, 30].\nHowever, as summarized by the authors of CCALin, these cited methods are more or less heuristics\nand do not have provable guarantees. Furthermore, for k > 1, the AppGrad result of [20] only pro-\nvides local convergence guarantees and thus requires a warm-start whose computational complexity\nis not discussed in their paper.\nFinally, our algorithms on GenEV and CCA are based on \ufb01nding vectors one-by-one, which is\nadvantageous in practice because one does not need k to be known and can stop the algorithm\nwhenever the eigenvalues (or correlation values) are too small. Known approaches for k > 1 cases\n(such as GenELin, CCALin, AppGrad) \ufb01nd all k vectors at once, therefore requiring k to be known\nbeforehand. As a separate note, these known approaches do not need the user to know the desired\naccuracy a priori but our LazyEV and LazyCCA algorithms do.\n2\nPreliminaries\nFor a vector x we denote by \u2225x\u2225or \u2225x\u22252 the Euclidean norm of x. Given a matrix A we denote\nby \u2225A\u22252 and \u2225A\u2225F respectively the spectral and Frobenius norms of A. For q \u22651, we denote by\n\u2225A\u2225Sq the Schatten q-norm of A. We write A \u2ab0B if A, B are symmetric and A \u2212B is positive\nsemi-de\ufb01nite (PSD), and write A \u227bB if A, B are symmetric but A \u2212B is positive de\ufb01nite (PD).\nWe denote by \u03bbmax(M) and \u03bbmin(M) the largest and smallest eigenvalue of a symmetric matrix\nM, and by \u03baM the condition number \u03bbmax(M)/\u03bbmin(M) of a PSD matrix M.\nThroughout this paper, for a matrix A \u2208Rn\u00d7d, we de\ufb01ne nnz(A)\ndef\n= max{n, d, N} where N is the\nnumber of non-zero entries of A. For two matrices X, Y , we denote by nnz(X, Y ) = nnz(X)+nnz(Y ),\nand by Xi or Yi the i-th row vector of X or Y .\nWe also use poly(x1, x2, . . . , xt) to represent\na quantity that is asymptotically at most polynomial in terms of variables x1, . . . , xt. Given a\ncolumn orthonormal matrix U \u2208Rn\u00d7k, we denote by U \u22a5\u2208Rn\u00d7(n\u2212k) the column orthonormal\nmatrix consisting of an arbitrary basis in the space orthogonal to the span of U\u2019s columns.\nGiven a PSD matrix B and a vector v, the value v\u22a4Bv is the B-inner product. Two vectors\nv, w satisfying v\u22a4Bw = 0 are known as B-orthogonal. Given a PSD matrix B, we denote by B\u22121\nthe Moore-Penrose pseudoinverse of B which is also PSD, and denote by B1/2 the matrix square\nroot of B (satisfying B1/2 \u2ab00). All occurrences of B\u22121, B1/2 and B\u22121/2 are for analysis purpose\nonly. When implementing our algorithms, it only requires one to multiply B to a vector.\n6\nDe\ufb01nition 2.1 (GenEV). Given symmetric matrices A, B \u2208Rd\u00d7d where B is positive de\ufb01nite.\nThe generalized eigenvectors of A with respect to B are v1, . . . , vd, where each vi is\nvi \u2208arg max\nv\u2208Rd\n(\n\f\fv\u22a4Av\n\f\f\nsuch that\nn v\u22a4Bv = 1\nv\u22a4Bvj = 0 \u2200j \u2208[i \u22121]\n)\nThe corresponding generalized eigenvalues \u03bb1, . . . , \u03bbd satisfy \u03bbi = v\u22a4\ni Avi which is possibly negative.\nFollowing the tradition of [13, 29], we assume without loss of generality that \u03bbi \u2208[\u22121, 1].\nDe\ufb01nition 2.2 (CCA).\nGiven X \u2208Rn\u00d7dx, Y \u2208Rn\u00d7dy, letting Sxx = 1\nnX\u22a4X, Sxy = 1\nnX\u22a4Y ,\nSyy = 1\nnY \u22a4Y , the canonical-correlation vectors are {(\u03c6i, \u03c8i)}r\ni=1 where r = min{dx, dy} and \u2200i:\n(\u03c6i, \u03c8i) \u2208\narg max\n\u03c6\u2208Rdx,\u03c8\u2208Rdy\n(\n\u03c6\u22a4Sxy\u03c8\nsuch that\nn \u03c6\u22a4Sxx\u03c6 = 1 \u2227\u03c6\u22a4Sxx\u03c6j = 0 \u2200j \u2208[i \u22121]\n\u03c8\u22a4Syy\u03c8 = 1 \u2227\u03c8\u22a4Syy\u03c8j = 0 \u2200j \u2208[i \u22121]\n)\nThe corresponding canonical-correlation coe\ufb03cients \u03c31, . . . , \u03c3r satisfy \u03c3i = \u03c6\u22a4\ni Sxy\u03c8i \u2208[0, 1].\nWe emphasize here that \u03c3i always lies in [0, 1] and is scaling-invariant. When dealing with a CCA\nproblem, we also denote by d = dx + dy.\nLemma 2.3 (CCA to GenEV). Given a CCA problem with matrices X \u2208Rn\u00d7dx, Y \u2208Rn\u00d7dy, and\nsuppose the canonical-correlation vectors and coe\ufb03cients are {(\u03c6i, \u03c8i, \u03c3i)}r\ni=1 where r = min{dx, dy}.\nDe\ufb01ne A =\n\u0010\n0\nSxy\nS\u22a4\nxy\n0\n\u0011\nand B =\n\u0010\nSxx\n0\n0\nSyy\n\u0011\n. Then, the GenEV problem of A with respect to B has\n2r eigenvalues {\u00b1\u03c3i}r\ni=1 and corresponding generalized eigenvectors\nn\u0010\n\u03c6i\n\u03c8i\n\u0011\n,\n\u0010\n\u2212\u03c6i\n\u03c8i\n\u0011on\ni=1. The re-\nmaining dx + dy \u22122r eigenvalues are zeros.\nDe\ufb01nition 2.4.\nIn CCA, let A and B be as de\ufb01ned in Lemma 2.3. We de\ufb01ne condition numbers\n\u03ba\ndef\n= \u03bbmax(B)\n\u03bbmin(B) and \u03ba\u2032 def\n= 2 maxi{\u2225Xi\u22252, \u2225Yi\u22252}\n\u03bbmin(B)\n(it must satisfy \u03ba\u2032 \u2208[\u03ba, 2n\u03ba] and \u03ba\u2032 \u2265d.)\nProof. We have \u03bbmax(B) \u2264Tr(B) = 1\nn\nP\ni \u2225Xi\u22252 + \u2225Yi\u22252 \u22642 maxi{\u2225Xi\u22252, \u2225Yi\u22252} and thus \u03ba\u2032 \u2265\u03ba.\nSuppose without loss of generality that \u2225X1\u22252 = maxi{\u2225Xi\u22252, \u2225Yi\u22252}; then, we have \u03bbmax(B) \u2265\n1\nn\u2225X1\u22252 = 1\nn maxi{\u2225Xi\u22252, \u2225Yi\u22252}. This implies \u03ba\u2032 \u22642n\u03ba. Finally, 2 maxi{\u2225Xi\u22252, \u2225Yi\u22252} \u2265Tr(B) \u2265\nd\u03bbmin(B) and therefore \u03ba\u2032 \u2265d.\n\u25a1\nRemark 2.5. We have followed the very original de\ufb01nition of CCA [15, 20] by letting Sxx = 1\nnX\u22a4X\nand Syy = 1\nnY \u22a4Y . In contrast, one prior work [29] considered the slightly more general version\nSxx = \u03b3xI + 1\nnX\u22a4X and Syy = \u03b3yI + 1\nnY \u22a4Y for some \u03b3x, \u03b3y \u22650. All of the results in this paper\ncontinue to hold in this more general setting, but we refrain from doing so for notational simplicity.\n(The only di\ufb00erence is that the parameter \u03ba\u2032 will no longer satisfy \u03ba\u2032 \u2265d in Def. 2.4.)\nLemma 2.6.\nGiven matrices X \u2208Rn\u00d7dx, Y \u2208Rn\u00d7dy, let A and B be as de\ufb01ned in Lemma 2.3.\nFor every w \u2208Rd, Katyusha method [1] \ufb01nds a vector w\u2032 \u2208Rd satisfying \u2225w\u2032 \u2212B\u22121Aw\u2225\u2264\u03b5\nin time\nO\n\u0010\nnnz(X, Y ) \u00b7\n\u00001 +\np\n\u03ba\u2032/n\n\u0001\n\u00b7 log \u03ba\u2225w\u22252\n\u03b5\n\u0011\n.\n7\nAlgorithm 1 AppxPCA\u00b1(A, M, \u03b4\u00d7, \u03b5, p)\nInput: A, an approximate matrix inversion method; M \u2208Rd\u00d7d, a symmetric matrix satisfying\n\u2212I \u2aafM \u2aafI; \u03b4\u00d7 \u2208(0, 0.5], a multiplicative error; \u03b5 \u2208(0, 1), a numerical accuracy parameter;\nand p \u2208(0, 1), the con\ufb01dence parameter.\n1:\nbw0 \u2190RanInit(d); s \u21900; \u03bb(0) \u21901 + \u03b4\u00d7;\n\u22c4\nbw0 is a random unit vector, see Def. 3.2\n2: m1 \u2190\n\u0006\n4 log\n\u0000 288d\u03b8\np2\n\u0001\u0007\n, m2 \u2190\n\u0006\nlog\n\u0000 36d\u03b8\np2\u03b5\n\u0001\u0007\n;\n\u22c4\n\u03b8 is the parameter of RanInit, see Def. 3.2\n3: e\u03b51 \u2190\n1\n64m1\n\u0000 \u03b4\u00d7\n48\n\u0001m1 and e\u03b52 \u2190\n\u03b5\n8m2\n\u0000 \u03b4\u00d7\n48\n\u0001m2\n4: repeat\n\u22c4\nit satis\ufb01es m1 = T PM(8, 1/32, p) and m2 = T PM(2, \u03b5/4, p), see Lemma B.1\n5:\ns \u2190s + 1;\n6:\nfor t = 1 to m1 do\n7:\nApply A to \ufb01nd bwt satisfying\n\r\r bwt \u2212(\u03bb(s\u22121)I \u2212M)\u22121 bwt\u22121\n\r\r \u2264e\u03b51;\n8:\nwa \u2190bwm1/\u2225bwm1\u2225;\n9:\nApply A to \ufb01nd va satisfying\n\r\rva \u2212(\u03bb(s\u22121)I \u2212M)\u22121wa\n\r\r \u2264e\u03b51;\n10:\nfor t = 1 to m1 do\n11:\nApply A to \ufb01nd bwt satisfying\n\r\r bwt \u2212(\u03bb(s\u22121)I + M)\u22121 bwt\u22121\n\r\r \u2264e\u03b51;\n12:\nwb \u2190bwm1/\u2225bwm1\u2225;\n13:\nApply A to \ufb01nd vb satisfying\n\r\rvb \u2212(\u03bb(s\u22121)I + M)\u22121wb\n\r\r \u2264e\u03b51;\n14:\n\u2206(s) \u21901\n2 \u00b7\n1\nmax{w\u22a4\na va,w\u22a4\nb vb}\u2212e\u03b51 and \u03bb(s) \u2190\u03bb(s\u22121) \u2212\u2206(s)\n2 ;\n15: until \u2206(s) \u2264\u03b4\u00d7\u03bb(s)\n12\n16: f \u2190s;\n17: if the last w\u22a4\na va \u2265w\u22a4\nb vb then\n18:\nfor t = 1 to m2 do\n19:\nApply A to \ufb01nd bwt satisfying\n\r\r bwt \u2212(\u03bb(f)I \u2212M)\u22121 bwt\u22121\n\r\r \u2264e\u03b52;\n20:\nreturn (+, w) where w\ndef\n= bwm2/\u2225bwm2\u2225.\n21: else\n22:\nfor t = 1 to m2 do\n23:\nApply A to \ufb01nd bwt satisfying\n\r\r bwt \u2212(\u03bb(f)I + M)\u22121 bwt\u22121\n\r\r \u2264e\u03b52;\n24:\nreturn (\u2212, w) where w\ndef\n= bwm2/\u2225bwm2\u2225.\n25: end if\n3\nLeading Eigenvector via Two-Sided Shift-and-Invert\nIn this section we de\ufb01ne AppxPCA\u00b1, the multiplicative approximation algorithm for computing\nthe two-sided leading eigenvector of a symmetric matrix using the shift-and-invert preconditioning\nframework [13, 14]. Our pseudo-code Algorithm 1 is a modi\ufb01cation of Algorithm 5 in [13].\nThe main di\ufb00erences between AppxPCA\u00b1 and Algorithm 5 of [13] are two-fold. First, given a\nsymmetric matrix M, AppxPCA\u00b1 simultaneously considers an upper-bounding shift together with\na lower-bounding shift, and try to invert both \u03bbI \u2212M and \u03bbI + M. This allows us to determine\napproximately how close \u03bb is to the largest and the smallest eigenvalues of M, and decrease \u03bb\naccordingly; in the end, it outputs an approximate eigenvector of M that corresponds to a negative\neigenvalue if needed. Second, we provide a multiplicative-error guarantee rather than additive as\noriginally appeared in [13]. Without this multiplicative-error guarantee, our \ufb01nal running time will\ndepend on\n1\ngap\u00b7\u03bbmax(M) rather than\n1\ngap.5 Of course, we believe the bulk of the credit for conceiving\n5This is why the SI method [29] depends on\n1\ngap\u00b7\u03c31 in Table 1.\n8\nAppxPCA\u00b1 belongs to the original authors of [13, 14].\nTheorem 3.1 (AppxPCA\u00b1).\nLet M \u2208Rd\u00d7d be a symmetric matrix with eigenvalues 1 \u2265\u03bb1 \u2265\n\u00b7 \u00b7 \u00b7 \u2265\u03bbd \u2265\u22121 and corresponding eigenvectors u1, . . . , ud. Let \u03bb\u2217= \u2225M\u22252 = max{\u03bb1, \u2212\u03bbd}. With\nprobability at least 1 \u2212p, AppxPCA\u00b1 produces a pair (sgn, w) satisfying\nif sgn = +, then\nX\ni\u2208[d],\u03bbi\u2264(1\u2212\u03b4\u00d7/2)\u03bb\u2217\n(w\u22a4ui)2 \u2264\u03b5\nand\nw\u22a4Mw \u2265(1 \u2212\u03b4\u00d7/2)(1 \u22123\u03b5)\u03bb\u2217, and\nif sgn = \u2212, then\nX\ni\u2208[d],\u03bbi\u2265\u2212(1\u2212\u03b4\u00d7/2)\u03bb\u2217\n(w\u22a4ui)2 \u2264\u03b5\nand\nw\u22a4Mw \u2264\u2212(1 \u2212\u03b4\u00d7/2)(1 \u22123\u03b5)\u03bb\u2217.\nFurthermore, the total number of oracle calls to A is O(log(1/\u03b4\u00d7)m1 + m2), and each time we call\nA it satis\ufb01es that \u03bbmax(\u03bb(s)I\u2212M)\n\u03bbmin(\u03bb(s)I\u2212M) , \u03bbmax(\u03bb(s)I+M)\n\u03bbmin(\u03bb(s)I+M) \u2208[1, 96\n\u03b4\u00d7 ] and\n1\n\u03bbmin(\u03bb(s)I\u2212M),\n1\n\u03bbmin(\u03bb(s)I+M) \u2264\n48\n\u03b4\u00d7\u03bb\u2217.\nWe remark here that, unlike the original shift-and-invert method which chooses a random\n(Gaussian) unit vector in Line 1 of AppxPCA\u00b1, we allow this initial vector to be generated from an\narbitrary \u03b8-conditioned random vector generator, de\ufb01ned as follows:\nDe\ufb01nition 3.2.\nAn algorithm RanInit(d) is a \u03b8-conditioned random vector generator if w =\nRanInit(d) is a d-dimensional unit vector and, for every p \u2208(0, 1), every unit vector u \u2208Rd, with\nprobability at least 1 \u2212p, it satis\ufb01es (u\u22a4w)2 \u2264p2\u03b8\n9d .\nThis modi\ufb01cation is needed in order to obtain our e\ufb03cient implementations of GenEV and CCA\nalgorithms. One can construct \u03b8-conditioned random vector generator as follows:\nProposition 3.3.\nGiven PSD matrix B \u2208Rd\u00d7d, if we set RanInit(d)\ndef\n=\nB1/2v\n(v\u22a4Bv)0.5 where v is a\nrandom Gaussian vector, then RanInit(d) is a \u03b8-conditioned random vector generator for \u03b8 = \u03baB.\nProof of Proposition 3.3. We have\n(u\u22a4w)2 = Tr(uu\u22a4vBv\u22a4)\nv\u22a4Bv\nx\n\u2265Tr(uu\u22a4vBv\u22a4)\n\u03bbmax(B)\ny\n\u2265\u03bbmin(B) \u00b7 Tr(uu\u22a4vv\u22a4)\n\u03bbmax(B)\n= \u03b8(u\u22a4v)2 .\nAbove, x is because v\u22a4Bv \u2264\u03bbmax(B) \u00b7 \u2225v\u22252\n2 = \u03bbmax(B), and y follows from the fact that vBv\u22a4\u2ab0\nv\n\u0000\u03bbmin(B)I\n\u0001\nv\u22a4= \u03bbmin(B)vv\u22a4. Finally, using for instance [8, Lemma 5], it holds with probability\nat least 1 \u2212p that (u\u22a4v)2 \u2265p2\n9d.\n\u25a1\n4\nLazyEV: Our Algorithm for Generalized Eigendecomposition\nIn this section, we propose LazyEV (see Algorithm 2) to compute approximately the k \u201cleading\u201d\neigenvectors corresponding to the k largest absolute eigenvalues of some symmetric matrix M \u2208\nRd\u00d7d. Later, we shall solve the k-GenEV problem by setting M = B\u22121/2AB\u22121/2 and using LazyEV\nto \ufb01nd the top k leading eigenvectors of M, which correspond to the top k leading generalized\neigenvectors of A with respect to B.\nOur algorithm LazyEV is formally stated in Algorithm 2. It applies k times AppxPCA\u00b1, each\ntime with a multiplicative error \u03b4\u00d7/2, and projects the matrix M into the orthogonal space with\nrespect to the obtained leading eigenvector. We state our main approximation theorem below.\n9\nAlgorithm 2 LazyEV(A, M, k, \u03b4\u00d7, \u03b5pca, p)\nInput: A, an approximate matrix inversion method; M \u2208Rd\u00d7d, a matrix satisfying \u2212I \u2aafM \u2aafI;\nk \u2208[d], the desired rank; \u03b4\u00d7 \u2208(0, 1), a multiplicative error; \u03b5pca \u2208(0, 1), a numerical accuracy\nparameter; and p \u2208(0, 1), a con\ufb01dence parameter.\n1: M0 \u2190M; V0 = [];\n2: for s = 1 to k do\n3:\nv\u2032\ns \u2190AppxPCA\u00b1(A, Ms\u22121, \u03b4\u00d7/2, \u03b5pca, p/k);\n4:\nvs \u2190\n\u0000(I \u2212Vs\u22121V \u22a4\ns\u22121)v\u2032\ns\n\u0001\n/\n\r\r(I \u2212Vs\u22121V \u22a4\ns\u22121)v\u2032\ns\n\r\r;\n\u22c4\nproject v\u2032\ns to V \u22a5\ns\u22121\n5:\nVs \u2190[Vs\u22121, vs];\n6:\nMs \u2190(I \u2212vsv\u22a4\ns )Ms\u22121(I \u2212vsv\u22a4\ns )\n\u22c4\nwe also have Ms = (I \u2212VsV \u22a4\ns )M(I \u2212VsV \u22a4\ns )\n7: end for\n8: return Vk.\nTheorem 4.1 (approximation of LazyEV).\nLet M \u2208Rd\u00d7d be a symmetric matrix with eigenvalues\n\u03bb1, . . . , \u03bbd \u2208[\u22121, 1] and corresponding eigenvectors u1, . . . , ud, and assume |\u03bb1| \u2265\u00b7 \u00b7 \u00b7 \u2265|\u03bbd|.\nFor every k \u2208[d], \u03b4\u00d7, p \u2208(0, 1), there exists some \u03b5pca \u2264O\n\u0000poly(\u03b4\u00d7,\n|\u03bb1|\n|\u03bbk+1|, 1\nd)\n\u0001\nsuch that6\nLazyEV outputs a (column) orthonormal matrix Vk = (v1, . . . , vk) \u2208Rd\u00d7k which, with probability at\nleast 1 \u2212p, satis\ufb01es all of the following properties. (Denote by Ms = (I \u2212VsV \u22a4\ns )M(I \u2212VsV \u22a4\ns ).)\n(a) Correlation guarantee: \u2225V \u22a4\nk U\u22252 \u2264\u03b5,\nwhere U = (uj, . . . , ud) and j is the smallest index satisfying |\u03bbj| \u2264(1 \u2212\u03b4\u00d7)\u2225Mk\u22121\u22252.\n(b) Spectral norm guarantee: |\u03bbk+1| \u2264\u2225Mk\u22252 \u2264|\u03bbk+1|\n1\u2212\u03b4\u00d7 .\n(c) Rayleigh quotient guarantee: (1 \u2212\u03b4\u00d7)|\u03bbk| \u2264|v\u22a4\nk Mvk| \u2264\n1\n1\u2212\u03b4\u00d7 |\u03bbk|.\n(d) Schatten-q norm guarantee: for every q \u22651,\n\u2225Mk\u2225Sq \u2264(1 + \u03b4\u00d7)2\n(1 \u2212\u03b4\u00d7)2\n\u0010\nd\nX\ni=k+1\n\u03bbq\ni\n\u00111/q\n= (1 + \u03b4\u00d7)2\n(1 \u2212\u03b4\u00d7)2\nmin\nV \u2208Rd\u00d7k,V \u22a4V =I\n\b\n\u2225(I \u2212V V \u22a4)M(I \u2212V V \u22a4)\u2225Sq\n\t\n.\nThe next theorem states that, if M = B\u22121/2AB\u22121/2, then LazyEV can be implemented without\never needing to compute B1/2 or B\u22121/2.\nTheorem 4.2 (running time of LazyEV).\nLet A, B \u2208Rd\u00d7d be two symmetric matrices sat-\nisfying B \u227b0 and \u2212B \u2aafA \u2aafB.\nSuppose M = B\u22121/2AB\u22121/2 and RanInit(d) is the ran-\ndom vector generator de\ufb01ned in Proposition 3.3 with respect to B.\nThen, the computation of\nV \u2190B\u22121/2LazyEV(A, M, k, \u03b4\u00d7, \u03b5pca, p) can be implemented to run in time\n\u2022 eO\n\u0010\nknnz(B)+k2d+k\u03a5\n\u221a\n\u03b4\u00d7\n\u0011\nwhere \u03a5 is the time to multiply B\u22121A to a vector,7 or\n\u2022 eO\n\u0010 k\u221a\u03baBnnz(B)+knnz(A)+k2d\n\u221a\n\u03b4\u00d7\n\u0011\nif we use Conjugate gradient to multiply B\u22121A to a vector.\nAbove, the eO notation hides polylogarithmic factors with respect to 1/\u03b5pca, 1/\u03b4\u00d7, 1/p, \u03baB, d.\nOur main theorems immediately imply the following corollaries (proved in Appendix E.2):\n6The complete speci\ufb01cations of \u03b5pca is included in Appendix E. Since our \ufb01nal running time only depends on\nlog(1/\u03b5pca), we have not attempted to improve the constants in this polynomial dependency.\n7More precisely, to compute (B\u22121A)w for some vector w with error \u03b5 where log(1/\u03b5) = eO(1).\n10\nTheorem 4.3 (gap-dependent k-GenEV). Let A, B \u2208Rd\u00d7d be two symmetric matrices satisfying\nB \u227b0 and \u2212B \u2aafA \u2aafB. Suppose the generalized eigenvalue and eigenvector pairs of A with\nrespect to B are {(\u03bbi, ui)}d\ni=1, and it satis\ufb01es 1 \u2265|\u03bb1| \u2265\u00b7 \u00b7 \u00b7 \u2265|\u03bbd|. Let gap = |\u03bbk|\u2212|\u03bbk+1|\n|\u03bbk|\n\u2208[0, 1]\nbe the relative eigengap. For \ufb01xed \u03b5, p > 0, consider the output\nVk \u2190B\u22121/2LazyEV\n\u0010\nA, B\u22121/2AB\u22121/2, k, gap, O\n\u0000\u03b54\u00b7gap\nk3(\u03c31/\u03c3k)4\n\u0001\n, p\n\u0011\n\u2208Rd\u00d7k .\nThen, de\ufb01ning W = (uk+1, . . . , ud), we have with probability at least 1 \u2212p:\nV\u22a4\nk BVk = I\nand\n\u2225V\u22a4\nk BW\u22252 \u2264\u03b5 .\nMoreover, our running time is eO\n\u0010 k\u221a\u03baBnnz(B)+knnz(A)+k2d\n\u221agap\n\u0011\n.\nTheorem 4.4 (gap-free k-GenEV). In the same setting as Theorem 4.3, for \u03b5, p > 0, consider\n(v1, . . . , vk)\ndef\n= Vk \u2190B\u22121/2LazyEV\n\u0010\nA, B\u22121/2AB\u22121/2, k, \u03b5, O\n\u0010\n\u03b55\nk3d4(\u03c31/\u03c3k+1)12\n\u0011\n, p\n\u0011\n.\nThen, with probability at least 1 \u2212p: it satis\ufb01es V\u22a4\nk BVk = I and\n\u2200s \u2208[k]:\n\f\fv\u22a4\ns Avs\n\f\f \u2208\nh\n(1 \u2212\u03b5)|\u03bbs|, |\u03bbs|\n1 \u2212\u03b5\ni\nand\nmax\nw\u2208Rd\u2227w\u22a4BVk=0\n\f\f\fw\u22a4Aw\nw\u22a4Bw\n\f\f\f \u2264|\u03bbk+1|\n1 \u2212\u03b5\n.\nMoreover, our running time is eO\n\u0010 k\u221a\u03baBnnz(B)+knnz(A)+k2d\n\u221a\u03b5\n\u0011\n.\n5\nIdeas Behind Theorems 4.1 and 4.2\nOur LazyEV algorithm reduces the problem of \ufb01nding generalized eigenvectors to \ufb01nding regular\neigenvectors of M = B\u22121/2AB\u22121/2. In Section 5.1 we discuss how to ensure accuracy: that is, why\ndoes LazyEV guarantee to \ufb01nd approximately the top absolute eigenvectors of M; and in Section 5.2\nwe discuss how to implement LazyEV without ever needing to compute B1/2 or B\u22121/2.\n5.1\nIdeas Behind Theorem 4.1: Approximation Guarantee of GenEV\nOur approximation guarantee in Theorem 4.1 is a natural generalization of the recent work on\nfast iterative methods to \ufb01nd the top k eigenvectors of a PSD matrix M [2]. That method is\ncalled LazySVD. At a high level, LazySVD \ufb01nds the top k eigenvectors of M one by one but only\napproximately. Starting with M0 = M, in the s-th iteration where s \u2208[k], LazySVD computes\napproximately the leading eigenvector of matrix Ms\u22121 (using shift-and-invert [13]) and call it vs.\nThen, LazySVD projects Ms \u2190(I \u2212vsv\u22a4\ns )Ms\u22121(I \u2212vsv\u22a4\ns ) and proceeds to the next iteration.\nWhile the algorithmic idea of LazySVD is simple, the analysis requires some careful linear alge-\nbraic lemmas. Most notably, if vs is an approximate leading eigenvector of Ms\u22121, then one needs to\nprove that the small eigenvectors of Ms\u22121 somehow still \u201cembed\u201d into that of Ms after projection.\nThis is achieved by a gap-free variant of the Wedin theorem plus a few other technical lemmas, and\nwe recommend interested readers to see the high-level overview section of [2].\nIn this paper, to relax the assumption that M is PSD, and to \ufb01nd leading eigenvectors whose\nabsolute eigenvalues are large, we have to make some non-trivial changes in the algorithm and\nthe analysis. On the algorithm side, LazyEV replaces the use of the shift-and-invert protocol in\nLazySVD with our two-sided variant developed in Section 3. On the analysis side, we have to make\nsure all lemmas properly deal with negative eigenvalues: for instance, if we perform a projection\nM\u2032 \u2190(I\u2212vv\u22a4)M(I\u2212vv\u22a4) where v correlates by at most \u03b5 with all eigenvectors of M whose absolute\neigenvalues are smaller than a threshold \u00b5, then, after the projection, we need to prove that these\neigenvectors can be approximately \u201cembedded\u201d into the eigenspace spanned by all eigenvectors of\n11\nM\u2032 whose absolute eigenvalues are smaller than \u00b5+\u03c4. The approximation of this embedding should\ndepend on \u03b5, \u00b5 and \u03c4. See Lemma C.4 in the appendix.\nThe proof of Theorem 4.1 is in Appendix E, and the matrix algebraic lemmas are in Appendix C.\n5.2\nProof of Theorem 4.2: Fast Implementation of GenEV\nWe can implement LazyEV e\ufb03ciently without the necessity of computing B1/2 or B\u22121/2. In each\niteration of LazyEV, we call AppxPCA\u00b1 and compute a vector v\u2032\ns. We do not explicitly store v\u2032\ns,\nbut rather write it as v\u2032\ns = B1/2v\u2032\ns and store only v\u2032\ns \u2208Rd. We shall later ensure that AppxPCA\u00b1\noutputs v\u2032\ns directly. Similarly, we also write vs = B1/2vs and only store vs. All together, we do\nnot explicitly compute Vs, but instead write Vs = B1/2Vs and only keep track of Vs \u2208Rd\u00d7s.\nNow, the computation of vs becomes the B-projection into the Vs\u22121 space:\n\u2225(I \u2212Vs\u22121V \u22a4\ns\u22121)v\u2032\ns\u2225= \u2225B1/2v\u2032\ns \u2212B1/2Vs\u22121V\u22a4\ns\u22121Bv\u2032\ns\u2225=\n\u0010\u0000v\u2032\ns \u2212Vs\u22121V\u22a4\ns\u22121Bv\u2032\ns\n\u0001\u22a4B\n\u0000v\u2032\ns \u2212Vs\u22121V\u22a4\ns\u22121Bv\u2032\ns\n\u0001\u00111/2\n\u2225(I \u2212Vs\u22121V \u22a4\ns\u22121)v\u2032\ns\u2225\u00b7 vs = B\u22121/2(I \u2212Vs\u22121V \u22a4\ns\u22121)v\u2032\ns = B\u22121/2(I \u2212Vs\u22121V \u22a4\ns\u22121)B1/2v\u2032\ns = v\u2032\ns \u2212Vs\u22121V\u22a4\ns\u22121Bv\u2032\ns\nand this can be implemented to run in O(kd + nnz(B)) time. Finally, we write\nMs = (I \u2212VsV \u22a4\ns )B\u22121/2AB\u22121/2(I \u2212VsV \u22a4\ns ) = B\u22121/2(I \u2212BVsV\u22a4\ns )A(I \u2212VsV\u22a4\ns B)B\u22121/2\nand only pass it implicity to AppxPCA\u00b1 (without directly computing this matrix).\nTo implement AppxPCA\u00b1, we again write all vectors bwt = B1/2wt and only store wt. Thus,\nthe normalization wa \u2190bwm1/\u2225bwm1\u22252 becomes the B-normalization wa \u2190wm1/(w\u22a4\nm1Bwm1)1/2\nwhich runs in O(nnz(B)) time. Recall that AppxPCA\u00b1 makes a polylogarithmic number of calls to\nthe matrix inversion subroutine A, each time requesting to approximately invert either \u03bbI \u2212Ms or\n\u03bbI + Ms. Let us only focus on inverting \u03bbI \u2212Ms and the other case is similar. We write\nN\ndef\n= B\u22121/2\u0000\u03bbI \u2212Ms\n\u0001\nB1/2 = \u03bbI \u2212(I \u2212VsV\u22a4\ns B)B\u22121A(I \u2212VsV\u22a4\ns B) .\nNow, the accuracy requirement in AppxPCA\u00b1 becomes\n\ufb01nd wt satisfying \u2225B1/2wt \u2212(\u03bbI \u2212Ms)\u22121B1/2wt\u22121\u2225\u2264e\u03b5\n\u21d0\u21d2\n\ufb01nd wt satisfying \u2225B1/2wt \u2212B1/2N\u22121wt\u22121\u2225\u2264e\u03b5\n\u21d0=\n\ufb01nd wt satisfying \u2225wt \u2212N\u22121wt\u22121\u2225\u2264e\u03b5/\np\n\u03bbmax(B)\nUsing accelerated gradient descent (see Theorem D.1), we can reduce this approximate inversion\nwt \u2190N\u22121wt\u22121 to T times of approximate matrix-vector multiplication (i.e., w\u2032 \u2190Nw) for T =\neO(\u221a\u03ba(\u03bbI\u2212Ms)).8 We can further derive that T = eO(1/\np\n\u03b4\u00d7) owing to Theorem 3.1. Notice that\neach time we compute w\u2032 \u2190Nw it su\ufb03ces to compute it to an additive accuracy \u2225w\u2032 \u2212Nw\u2225\u2264\u03b5\nwhere the error satis\ufb01es log(1/\u03b5) = eO(1).\nFinally, the matrix-vector multiplication Nw = \u03bbw\u2212(I\u2212VsV\u22a4\ns B)B\u22121A(I\u2212VsV\u22a4\ns B)w consists\nof two rank-s B-projections which run in time O(nnz(B) + kd), plus the time needed to multiply\nB\u22121A to a vector. This proves that LazyEV can be implemented so that\n\u2022 It computes matrix-vector multiplication of the form w\u2032 \u2190B\u22121Aw a total of eO(k/\np\n\u03b4\u00d7) times,\neach time to an accuracy \u03b5 where log(1/\u03b5) = eO(1);\n\u2022 The rest of the computation costs a total of eO\n\u0000(knnz(B) + k2d)/\np\n\u03b4\u00d7\n\u0001\ntime.\nThis \ufb01nishes the proof of the \ufb01rst item of the theorem.\n8This reduction would be obvious if we required the matrix-vector multiplication to be exact, and for instance\nChebyshev method serves for exactly this purpose. However, in order to relax the multiplication to be approximate,\nand without incurring an error that blows up exponentially with T, we build our own inexact variant of the accelerate\ngradient descent method AGDinexact in Appendix D that could be of independent interest.\n12\nAlgorithm 3 LazyCCA(A, M, k, \u03b4\u00d7, \u03b5pca, p)\nInput: A, an approximate matrix inversion method;\nM \u2208Rd\u00d7d, a matrix satisfying \u2212I \u2aafM \u2aafI;\nk \u2208[d], the desired rank;\n\u03b4\u00d7 \u2208(0, 1), a multiplicative error;\n\u03b5pca \u2208(0, 1), a numerical accuracy parameter; and\np \u2208(0, 1), a con\ufb01dence parameter.\n1: M0 \u2190M;\n2: V0 = [];\n3: for s = 1 to k do\n4:\nv\u2032\ns \u2190AppxPCA\u00b1(A, Ms\u22121, \u03b4\u00d7/2, \u03b5pca, p/k);\n5:\nvs \u2190\n\u0000(I \u2212Vs\u22121V \u22a4\ns\u22121)v\u2032\ns\n\u0001\n/\n\r\r(I \u2212Vs\u22121V \u22a4\ns\u22121)v\u2032\ns\n\r\r;\n\u22c4\nproject v\u2032\ns to V \u22a5\ns\u22121\n6:\nwrite vs = (\u03be\u2032\ns, \u03b6\u2032\ns) where \u03be\u2032\ns \u2208Rdx and \u03b6\u2032\ns \u2208Rdy;\n7:\n\u03bes \u2190\u03be\u2032\ns/(\n\u221a\n2\u2225\u03be\u2032\ns\u22252) and \u03b6s \u2190\u03b6\u2032\ns/(\n\u221a\n2\u2225\u03b6\u2032\ns\u22252);\n8:\nVs \u2190\nh\nVs\u22121,\n\u0010 \u03bes\n\u2212\u03bes\n\u03b6s\n\u03b6s\n\u0011i\n;\n9:\nMs \u2190\n\u0000I \u22122diag(\u03bes\u03be\u22a4\ns , \u03b6s\u03b6\u22a4\ns )\n\u0001\nMs\u22121\n\u0000I \u22122diag(\u03bes\u03be\u22a4\ns , \u03b6s\u03b6\u22a4\ns )\n\u0001\n\u22c4\nor equivalently, Ms = (I \u2212VsV \u22a4\ns )M(I \u2212VsV \u22a4\ns )\n10: end for\n11: return Vk.\nAs for the second item, we simply notice that whenever we want to compute w\u2032 \u2190B\u22121Aw, we\ncan \ufb01rst compute Aw in time O(nnz(A)), and then use Conjugate gradient [27] to compute B\u22121\napplied to this vector. The running time of Conjugate gradient is at most eO\n\u0000\u221a\u03baB \u00b7 nnz(B)\n\u0001\nwhere\nthe eO factor hides a logarithmic factor on the accuracy.\n\u25a1\n6\nLazyCCA: Our Algorithm for Canonical Correlation Analysis\nWe propose LazyCCA (see Algorithm 3), a variant of LazyEV that is specially designed for matrices\nM of the form M = B\u22121/2AB\u22121/2, where A and B come from a CCA instance following Lemma 2.3.\nMore speci\ufb01cally, recall from Lemma 2.3 that the eigenvectors of matrices M arising from CCA\ninstances are symmetric: if (\u03be, \u03b6) is a normalized eigenvector of M with eigenvalue \u03c3 where \u03be \u2208\nRdx, \u03b6 \u2208Rdy, then (\u2212\u03be, \u03b6) is also a normalized eigenvector but with eigenvalue \u2212\u03c3. Furthermore,\nsince (\u03be, \u03b6) is orthogonal to (\u2212\u03be, \u03b6), we must have \u2225\u03be\u2225= \u2225\u03b6\u2225= 1/\n\u221a\n2. Our LazyCCA method is\ndesigned to ensure such symmetry and orthogonality as well. When an approximate eigenvector\nvs = (\u03be\u2032\ns, \u03b6\u2032\ns) is obtained, we re-scale the pair to \u03bes and \u03b6s where both of them have norm exactly 1/\n\u221a\n2\n(see Line 7 of LazyCCA). Then, we simultaneously add two (orthogonal) approximate eigenvectors\n(\u03bes, \u03b6s) and (\u2212\u03bes, \u03b6s) to the column orthonormal matrix Vs.\nRemark 6.1. This re-scaling step, together with the fact that we \ufb01nd vector pairs one by one,\nallows us to provide per-vector guarantee on the obtained approximate correlation vectors (see\nTheorem 6.3). This is in contrast to CCALin which is based on subspace power method so can\nonly \ufb01nd the subspace spanned by the top k correlation vectors but not distinguish them.\nWe prove three main theorems in the appendix:\n\u2022 A main theorem for the approximation guarantee (see Theorem F.1).\nThe main \u201cdelta\u201d between the proofs of Theorem F.1 and Theorem 4.1 is to show that, after\n13\nre-scaling, the vector (\u03bes, \u03b6s) is also an approximate leading eigenvector of Ms\u22121. In particular,\nits Rayleigh quotient can only become better after scaling (see (F.3) in the appendix).\n\u2022 A main theorem for the stochastic running time (see Theorem F.2).\nThe main \u201cdelta\u201d between the proofs of Theorem F.2 and Theorem 4.2 is to replace the use of\nconjugate gradient with the Katyusha method to multiply B\u22121A to a vector (see Lemma 2.6).\n\u2022 A main theorem for the doubly-stochastic running time guarantee (see Theorem F.3).\nThe main \u201cdelta\u201d between the proofs of Theorem F.3 and Theorem F.2 is to use accelerated\nSVRG as opposed to Katyusha to compute matrix inverse.\nWe state below the \ufb01nal statements on LazyCCA.\nTheorem 6.2 (gap-dependent k-CCA).\nLet X \u2208Rn\u00d7dx, Y\n\u2208Rn\u00d7dy be two matrices with\ncanonical-correlation coe\ufb03cients 1 \u2265\u03c31 \u2265\u00b7 \u00b7 \u00b7 \u03c3r \u22650 and the corresponding correlation vec-\ntors {(\u03c6i, \u03c8i)}r\ni=1. Let gap = \u03c3k\u2212\u03c3k+1\n\u03c3k\n\u2208[0, 1] be the relative gap, and de\ufb01ne A = [[0, Sxy]; [S\u22a4\nxy, 0]]\nand B = diag(Sxx, Syy) following Def. 2.2. For every \u03b5, p > 0, consider the output\n\u0010 \u00b1\u03c6\u2032\n1\n. . .\n\u00b1\u03c6\u2032\nk\n\u03c8\u2032\n1\n. . .\n\u03c8\u2032\nk\n\u0011\ndef\n= Vk \u2190\n\u221a\n2B\u22121/2LazyCCA\n\u0010\nA, B\u22121/2AB\u22121/2, \u03b5, gap, O\n\u0000\u03b54\u00b7gap\nk3(\u03c31/\u03c3k)4\n\u0001\n, p\n\u0011\n.\nThen, letting\nV\u03c6 = (\u03c6\u2032\n1, . . . , \u03c6\u2032\nk), V\u03c8 = (\u03c8\u2032\n1, . . . , \u03c8\u2032\nk), W\u03c6 = (\u03c6k+1, \u03c6k+2, . . . ), and W\u03c8 = (\u03c8k+1, \u03c8k+2, . . . ) ,\nwe have with probability at least 1 \u2212p:\nV\u03c6 \u2208Rdx\u00d7k satis\ufb01es V\u22a4\n\u03c6 SxxV\u03c6 = I and \u2225V\u22a4\n\u03c6 SxxW\u03c6\u22252 \u2264\u03b5 ,\nV\u03c8 \u2208Rdy\u00d7k satis\ufb01es V\u22a4\n\u03c8 SyyV\u03c8 = I and \u2225V\u22a4\n\u03c8 SyyW\u03c8\u22252 \u2264\u03b5 .\nThe (stochastic) running time is eO\n\u0010 knnz(X,Y )\u00b7(1+\u221a\n\u03ba\u2032/n)+k2d\n\u221agap\n\u0011\n. The doubly-stochastic running time\nis eO\n\u0010\nnnz(X, Y )\u00b7\n\u00001+\n\u221a\n\u03ba\u2032/n1/4\n\u221agap\u00b7\u03c31\n\u0001\u0011\nfor 1-CCA, and eO\n\u0010\nknnz(X, Y )\u00b7\n\u00001+\n\u221a\n\u03ba\u2032\n\u221agap\u00b7\u03c3k\u00b7(nnz(X,Y )/kd)1/4\n\u0001\u0011\nfor\nk-CCA as long as nnz(X, Y ) \u2265kd.\nTheorem 6.3 (gap-free k-CCA). In the same setting as Theorem 6.2, for \u03b5, p > 0, consider\n\u0010 \u00b1\u03c6\u2032\n1\n. . .\n\u00b1\u03c6\u2032\nk\n\u03c8\u2032\n1\n. . .\n\u03c8\u2032\nk\n\u0011\n= Vk \u2190\n\u221a\n2B\u22121/2LazyCCA\n\u0010\nA, B\u22121/2AB\u22121/2, \u03b5, gap, O\n\u0000\u03b54\u00b7gap\nk3(\u03c31/\u03c3k)4\n\u0001\n, p\n\u0011\n.\nLetting V\u03c6 = (\u03c6\u2032\n1, . . . , \u03c6\u2032\nk) \u2208Rdx\u00d7k and V\u03c8 = (\u03c8\u2032\n1, . . . , \u03c8\u2032\nk) \u2208Rdy\u00d7k, with probability at least 1\u2212p,\n\u2022 V\u22a4\n\u03c6 SxxV\u03c6 = I, V\u22a4\n\u03c8 SyyV\u03c8 = I;\n\u2022 (1 \u2212\u03b5)\u03c3i \u2264|\u03c6\u2032\niSxy\u03c8i| \u2264(1 + \u03b5)\u03c3i for every i \u2208[k]; and\n\u2022 max\u03c6\u2208Rdx,\u03c8\u2208Rdy\nn\n\u03c6\u22a4Sxy\u03c8\n\f\f\f \u03c6\u22a4SxxV\u03c6 = 0 \u2227\u03c8\u22a4SyyV\u03c8 = 0\no\n\u2264(1 + \u03b5)\u03c3k+1 .\nThe (stochastic) running time is eO\n\u0010 knnz(X,Y )\u00b7(1+\u221a\n\u03ba\u2032/n)+k2d\n\u221a\u03b5\n\u0011\n. The doubly-stochastic running time\nis eO\n\u0010\nnnz(X, Y ) \u00b7\n\u00001 +\n\u221a\n\u03ba\u2032/n1/4\n\u221a\u03b5\u00b7\u03c31\n\u0001\u0011\nfor 1-CCA, and eO\n\u0010\nknnz(X, Y ) \u00b7\n\u00001 +\n\u221a\n\u03ba\u2032\n\u221a\u03b5\u00b7\u03c3k\u00b7(nnz(X,Y )/kd)1/4\n\u0001\u0011\nfor\nk-CCA as long as nnz(X, Y ) \u2265kd.\nRemark 6.4. The doubly-stochastic running times for the general k > 1 setting can be slightly\nfaster (but notationally more involved) than the ones stated above. For instance, we are aware of\na proof that gives running time\neO\n\u0012\nknnz(X, Y ) \u00b7\n\u0010\n1 +\n\u221a\n\u03ba\u2032\n\u221agap \u00b7 \u03c3k\n\u00b7 max\n\b\nk min{\u03c31, \u03c31\u03ba/d}, 1\n\t1/4d1/4\n(nnz(X, Y ))1/4\n\u0011\u0013\n14\nfor the gap-dependent case of k-CCA, or the same formula but replacing gap with \u03b5 for the gap-\nfree case. We refrain from proving it in full because as discussed in Section 1.2, doubly-stochastic\nrunning times are not necessarily interesting.\nAppendix\nA\nProof of Lemma 2.6\nProof of Lemma 2.6. First of all, computing B\u22121Aw is equivalent to minimizing f(x)\ndef\n= 1\n2x\u22a4Bx \u2212\nx\u22a4Aw. Suppose we write x = (x1, x2) and w = (w1, w2) where x1, w1 \u2208Rdx, x2, w2 \u2208Rdy, then\none can rewrite f as f(x) =\n1\n2n\n\u0000\u2225Xx1 \u2212Y w2\u22252\n2 + \u2225Xw1 \u2212Y x2\u22252\n2\n\u0001\n+ C where C is a \ufb01xed constant.\nTherefore, one can also write\nf(x) = 1\n2n\nn\nX\ni=1\n(\u27e8Xi, x1\u27e9\u2212\u27e8Yi, w2\u27e9)2 + (\u27e8Yi, x2\u27e9\u2212\u27e8Xi, w1\u27e9)2\nwhere each Xi is a row vector of X and Yi is a row vector of Y . In such a case, we observe that\n\u2022 f(x) is an average of 2n smooth functions, where function (\u27e8Xi, x1\u27e9\u2212\u27e8Yi, w2\u27e9)2 is smooth\nwith parameter 2\u2225Xi\u22252 (meaning Hessian bounded by \u2225Xi\u22252 in spectral norm) and (\u27e8Yi, x2\u27e9\u2212\n\u27e8Xi, w1\u27e9)2 is smooth with parameter 2\u2225Yi\u22252. In other words, each function has a smoothness\nparameter at most 2 maxi{\u2225Xi\u22252, \u2225Yi\u22252}.\n\u2022 f(x) is at least \u03bbmin(B) = min{\u03bbmin(Sxx), \u03bbmin(Syy)} strongly convex, meaning the Hessian\n\u22072f(x) has no eigenvalue less than this quantity.\nFor such reason, one can apply the convergence theorem of Katyusha [1] to \ufb01nd some x such that\nf(x) \u2212miny f(y) \u2264e\u03b5 in time O\n\u0000nnz(X, Y ) \u00b7\n\u00001 +\np\n\u03ba\u2032/n\n\u0001\nlog f(x0)\u2212f(x\u2217)\ne\u03b5\n\u0001\nwhere x0 is an arbitrary\nstarting vector fed into Katyusha and x\u2217is the exact minimizer. If we choose x0 to be the zero\nvector, it is easy to verify that f(x0) \u2212f(x\u2217) \u2264O(\u03bbmax(B) \u00b7 \u2225w\u22252).\nIt is not hard to see that an additive e\u03b5 minimizer of f(x) implies an \u03b5-approximate solution for\nthe inverse \u2225w\u2032 \u2212B\u22121Aw\u2225\u2264\u03b5 where \u03b52 = 2e\u03b5/\u03bbmin(B). This \ufb01nishes the proof of the running time\nin Lemma 2.6.\n\u25a1\nB\nProof for Section 3: Two-Sided Shift-and-Invert\nB.1\nInexact Power Method\nIn this subsection we review some classical convergence lemmas regarding power method and its\ninexact variant. These lemmas almost directly follow from previous results such as [13, 15], and\nare more similar to [2]. We skip the proofs in this paper.\nConsider power method that starts with a random unit vector w0 \u2190RanInit(d) and apply\nwt \u2190Mwt\u22121/\u2225Mwt\u22121\u2225iteratively.\nLemma B.1 (Exact Power Method).\nLet M be a PSD matrix with eigenvalues \u03bb1 \u2265\u00b7 \u00b7 \u00b7 \u2265\u03bbd\nand the correpsonding eigenvectors u1, . . . , ud. Fix an error tolerance \u03b5 > 0, parameter \u03ba \u22651, and\nfailure probability p > 0, de\ufb01ne\nT PM(\u03ba, \u03b5, p) =\nl\u03ba\n2 log\n\u00109d\u03b8\np2\u03b5\n\u0011m\n15\nThen, with probability at least 1 \u2212p it holds that \u2200t \u2265T PM(\u03ba, \u03b5, p):\nX\ni\u2208[d],\u03bbi\u2264(1\u22121/\u03ba)\u03bb1\n(w\u22a4\nt ui)2 \u2264\u03b5\nand\nw\u22a4\nt Mwt \u2265(1 \u22121/\u03ba \u2212\u03b5)\u03bb1 .\nLemma B.2 (Lemma 4.1 of [13]).\nLet M be a PSD matrix with eigenvalues \u03bb1 \u2265\u00b7 \u00b7 \u00b7 \u03bbd. Fix an\naccuracy parameter e\u03b5 > 0, and consider two update sequences\nbw\u2217\n0 = w0,\n\u2200t \u22651: bw\u2217\nt \u2190M bw\u2217\nt\u22121\nbw0 = w0,\n\u2200t \u22651: bwt satis\ufb01es \u2225bwt \u2212M bwt\u22121\u2225\u2264e\u03b5,\nThen, de\ufb01ning wt = bwt/\u2225bwt\u2225and w\u2217\nt = bw\u2217\nt /\u2225bw\u2217\nt \u2225, it satis\ufb01es\n\u2225wt \u2212w\u2217\nt \u2225\u2264e\u03b5 \u00b7 \u0393(M, t),\nwhere\n\u0393(M, t)\ndef\n= 2\n\u03bbt\nd\n\u001a t,\nif \u03bb1 = 1;\n(\u03bbt\n1 \u22121)/(\u03bb1 \u22121),\nif \u03bb1 \u0338= 1.\nand we have \u0393(M, t) \u22642t \u00b7 max{1, \u03bbt\n1}\n\u03bbt\nd\nTheorem B.3 (Inexact Power Method).\nLet M be a PSD matrix with eigenvalues \u03bb1 \u2265\u00b7 \u00b7 \u00b7 \u2265\u03bbd\nand the corresponding eigenvectors u1, . . . , ud. With probability at least 1\u2212p it holds that, for every\n\u03b5 \u2208(0, 1) and every t \u2265T PM(\u03ba, \u03b5/4, p), if wt is generated by the power method with per-iteration\nerror e\u03b5 =\n\u03b5\n4\u0393(M,t), then\nX\ni\u2208[d],\u03bbi\u2264(1\u22121/\u03ba)\u03bb1\n(w\u22a4\nt ui)2 \u2264\u03b5\nand\nw\u22a4\nt Mwt \u2265(1 \u22121/\u03ba \u2212\u03b5)\u03bb1 .\nB.2\nProof of Theorem 3.1\nWe prove Theorem 3.1 by \ufb01rst showing the following lemma. Most of these properties are analogous\nto their original variants in [13, 14], but here we take extra care also on negative eigenvalues and\nthus allowing M to be non-PSD.\nLemma B.4 (useful properties of AppxPCA\u00b1).\nWith probability at least 1 \u2212p, it holds that (by\nletting \u03bb\u2217= \u2225M\u22252):\n(a) e\u03b51 \u2264\n1\n32\u0393((\u03bb(s\u22121)I\u2212M)\u22121,m1) and e\u03b51 \u2264\n1\n32\u0393((\u03bb(s\u22121)I+M)\u22121,m1) for each iteration s \u22651;\n(b) e\u03b52 \u2264\n\u03b5\n4\u0393((\u03bb(f)I\u2212M)\u22121,m2) and e\u03b52 \u2264\n\u03b5\n4\u0393((\u03bb(f)I+M)\u22121,m2) when the repeat-until loop is over;\n(c) 0 \u22643\n4(\u03bb(s\u22121) \u2212\u03bb\u2217) \u2264\u2206(s) \u2264\u03bb(s\u22121) \u2212\u03bb\u2217and 1\n2(\u03bb(s\u22121) \u2212\u03bb\u2217) \u2264\u03bb(s) \u2212\u03bb\u2217for each iteration s \u22651;\nand\n(d) \u03bb(f) \u2212\u03bb\u2217\u2208[ \u03b4\u00d7\n48 \u03bb(f), \u03b4\u00d7\n13 \u03bb\u2217] when the repeat-until loop is over.\n(e) when the repeat-until loop is over,\nif w\u22a4\na va \u2265w\u22a4\nb vb then \u03bb(f) \u2212\u03bbmax(M) \u226410\n3 (\u03bb(f) \u2212\u03bb\u2217); or\nif w\u22a4\na va \u2264w\u22a4\nb vb then \u03bb(f) + \u03bbmin(M) \u226410\n3 (\u03bb(f) \u2212\u03bb\u2217) .\nProof. We denote by C(s) def\n= (\u03bb(s)I \u2212M)\u22121 and by D(s) def\n= (\u03bb(s)I + M)\u22121 for notational simplicity.\nBelow we prove all the items by induction for a speci\ufb01c iteration s \u22652 assuming that the items of\nthe previous s \u22121 iterations are true. The base case of s = 1 can be veri\ufb01ed similar to the general\narguments after some notational changes. We omitted the proofs of the base case s = 1.\n(a) Recall that\n\u0393(C(s\u22121), t) \u22642t \u00b7 max{1, \u03bbmax(C(s\u22121))t}\n\u03bbmin(C(s\u22121))t\nand\n\u0393(D(s\u22121), t) \u22642t \u00b7 max{1, \u03bbmax(D(s\u22121))t}\n\u03bbmin(D(s\u22121))t\n16\nOn one hand, we have \u03bbmax(C(s\u22121)) =\n1\n\u03bb(s\u22121)\u2212\u03bb\u2217\u2264\n2\n\u03bb(s\u22122)\u2212\u03bb\u2217\u2264\n2\n\u2206(s\u22121) using Lemma B.4.c of\nthe previous iteration. Combining this with the termination criterion \u2206(s\u22121) \u2265\u03b4\u00d7\n12 \u03bb(s\u22121), we\nhave \u03bbmax(C(s\u22121)) \u2264\n24\n\u03b4\u00d7\u03bb(s\u22121) . On the other hand, we have \u03bbmin(C(s\u22121)) =\n1\n\u03bb(s\u22121)\u2212\u03bbmin(M) \u2265\n1\n\u03bb(s\u22121)+\u03bb\u2217\u2265\n1\n2\u03bb(s\u22121) . Combining the two bounds we conclude that \u0393(C(s\u22121), t) \u22642t(48/\u03b4\u00d7)t. It\nis now obvious that e\u03b51 \u2264\n1\n32\u0393(C(s\u22121),m1) is satis\ufb01ed because e\u03b51 =\n1\n64m1\n\u0000 \u03b4\u00d7\n48\n\u0001m1.\nSimilarly, on one hand, we have \u03bbmax(D(s\u22121)) =\n1\n\u03bb(s\u22121)+\u03bbmin(M) \u2264\n1\n\u03bb(s\u22121)\u2212\u03bb\u2217\u2264\n2\n\u03bb(s\u22122)\u2212\u03bb\u2217\u2264\n2\n\u2206(s\u22121) using Lemma B.4.c of the previous iteration.\nCombining this with the termination\ncriterion \u2206(s\u22121) \u2265\u03b4\u00d7\n12 \u03bb(s\u22121), we have \u03bbmax(D(s\u22121)) \u2264\n24\n\u03b4\u00d7\u03bb(s\u22121) . On the other hand, we have\n\u03bbmin(D(s\u22121)) =\n1\n\u03bb(s\u22121)+\u03bbmax(M) \u2265\n1\n\u03bb(s\u22121)+\u03bb\u2217\u2265\n1\n2\u03bb(s\u22121) . Combining the two bounds we conclude\nthat \u0393(D(s\u22121), t) \u22642t(48/\u03b4\u00d7)t. It is now obvious that e\u03b51 \u2264\n1\n32\u0393(D(s\u22121),m1) is satis\ufb01ed.\n(b) The same analysis as in the proof of Lemma B.4.a suggests that \u0393(C(f), t) \u22642t(48/\u03b4\u00d7)t and\n\u0393(D(f), t) \u22642t(48/\u03b4\u00d7)t.\nThese immediately imply e\u03b52 \u2264\n\u03b5\n4\u0393(C(f),m2) and e\u03b52 \u2264\n\u03b5\n4\u0393(D(f),m2)\nbecause e\u03b52 =\n\u03b5\n8m2\n\u0000 \u03b4\u00d7\n48\n\u0001m2\n(c) Because Lemma B.4.a holds for the current iteration s we can apply Theorem B.3 (with \u03b5 =\n1/16 and \u03ba = 16) and get\nw\u22a4\na C(s\u22121)wa \u22657\n8\u03bbmax(C(s\u22121))\nand\nw\u22a4\nb D(s\u22121)wb \u22657\n8\u03bbmax(D(s\u22121)) .\nBy the de\ufb01nition of v in AppxPCA\u00b1 and the Cauchy-Schwartz inequality it holds that\nw\u22a4\na va = w\u22a4\na C(s\u22121)wa + w\u22a4\na\n\u0000va \u2212C(s\u22121)wa\n\u0001\n\u2208\n\u0002\nw\u22a4\na C(s\u22121)wa \u2212e\u03b51, w\u22a4\na C(s\u22121)wa + e\u03b51\n\u0003\n,\nand\nw\u22a4\nb vb = w\u22a4\nb D(s\u22121)wb + w\u22a4\nb\n\u0000vb \u2212D(s\u22121)wb\n\u0001\n\u2208\n\u0002\nw\u22a4\nb D(s\u22121)wb \u2212e\u03b51, w\u22a4\nb D(s\u22121)wb + e\u03b51\n\u0003\n.\nCombining the above equations we have\nw\u22a4\na va \u2212e\u03b51 \u2208\nh7\n8\u03bbmax(C(s\u22121)) \u22122e\u03b51, \u03bbmax(C(s\u22121))\ni\n\u2286\nh3\n4\u03bbmax(C(s\u22121)), \u03bbmax(C(s\u22121))\ni\n=\n\u00023\n4, 1\n\u0003\n\u00b7\n1\n\u03bb(s\u22121) \u2212\u03bbmax(M) ,\nand\nw\u22a4\nb vb \u2212e\u03b51 \u2286\nh3\n4\u03bbmax(D(s\u22121)), \u03bbmax(D(s\u22121))\ni\n=\n\u00023\n4, 1\n\u0003\n\u00b7\n1\n\u03bb(s\u22121) + \u03bbmin(M) .\n(B.1)\nIn other words, \u2206(s)\ndef\n=\n3\n4 \u00b7\n1\nmax{w\u22a4\na va,w\u22a4\nb wb}\u2212e\u03b51 \u2208\n\u0002 3\n4(\u03bb(s\u22121) \u2212\u03bb\u2217), \u03bb(s\u22121) \u2212\u03bb\u2217\u0003\nbecause \u03bb\u2217=\nmax{\u03bbmax(M), \u2212\u03bbmin(M)}.\nAt the same time, our update rule \u03bb(s) = \u03bb(s\u22121) \u2212\u2206(s)/2 ensures that \u03bb(s) \u2212\u03bb\u2217= \u03bb(s\u22121) \u2212\n\u03bb\u2217\u2212\u2206(s)/2 \u2265\u03bb(s\u22121) \u2212\u03bb\u2217\u2212\u03bb(s\u22121)\u2212\u03bb\u2217\n2\n= 1\n2(\u03bb(s\u22121) \u2212\u03bb\u2217).\n(d) The upper bound holds because \u03bb(f) \u2212\u03bb\u2217= \u03bb(f\u22121) \u2212\u2206(f)\n2\n\u2212\u03bb\u2217\u2264\n\u0000 4\n3 \u22121\n2\n\u0001\n\u2206(f) \u22645\u03b4\u00d7\u03bb(f)\n72\nwhere\nthe \ufb01rst inequality follows from Lemma B.4.c of this last iteration, and the second inequality\nfollows from our termination criterion \u2206(f) \u2264\u03b4\u00d7\u03bb(f)\n12\n. Simply rewriting this inequality we have\n\u03bb(f) \u2212\u03bb\u2217\u2264\n5\u03b4\u00d7/72\n1\u22125\u03b4\u00d7/72\u03bb\u2217< \u03b4\u00d7\n13 \u03bb\u2217.\nThe lower bound is because using Lemma B.4.c (of this and the previous iteration) we have\n\u03bb(f) \u2212\u03bb\u2217\u2265\n1\n4\n\u0000\u03bb(f\u22122) \u2212\u03bb\u2217\u0001\n\u2265\n\u2206(f\u22121)\n4\nx\n\u2265\n\u03b4\u00d7\u03bb(f\u22121)\n48\n\u2265\n\u03b4\u00d7\u03bb(f)\n48\n.\nHere, inequality x is because\n\u2206(f\u22121) > \u03b4\u00d7\u03bb(f\u22121)\n12\ndue to the termination criterion.\n17\n(e) We only prove the case when w\u22a4\na va \u2265w\u22a4\nb vb and the other case is similar. We compute that\n\u03bb(f) \u2212\u03bbmax(M) = \u03bb(f\u22121) \u2212\u03bbmax(M) \u2212\u2206(f)\n2\nx\n\u22644\n3(\u03bb(f\u22121) + \u03bbmin(M)) \u2212\u2206(f)\n2\n= 4\n3(\u03bb(f) + \u03bbmin(M)) + \u2206(f)\n2\ny\n\u22644\n3(\u03bb(f) + \u03bbmin(M)) + \u03b4\u00d7\u03bb(f)\n24\nz\n\u22644\n3(\u03bb(f) + \u03bbmin(M)) + 2(\u03bb(f) \u2212\u03bb\u2217)\n{\n\u226410\n3 (\u03bb(f) \u2212\u03bb\u2217) .\nAbove, x is from (B.1) together with the fact that w\u22a4\na va \u2265w\u22a4\nb vb; y is using the termination\ncriterion \u2206(f) \u2264\u03b4\u00d7\u03bb(f)\n12\n; z is from Lemma B.4.d\nFinally, since the success of Theorem B.3 only depends on the randomness of bw0, we have that with\nprobability at least 1 \u2212p all the above items are satis\ufb01ed.\n\u25a1\nWe are now ready to prove Theorem 3.1.\nProof of Theorem 3.1. We only focus on the case when sgn = + and the other case is similar. It\nfollows from Theorem B.3 (with \u03ba = 2) that, letting \u00b5i = 1/(\u03bb(f)\u2212\u03bbi) be the i-th largest eigenvalue\nof the matrix (\u03bb(f)I \u2212M)\u22121, then\nX\ni\u2208[d],\u00b5i\u2264\u00b51/2\n(w\u22a4ui)2 \u2264\u03b5 .\nNote that if an index i \u2208[d] satis\ufb01es \u03bb\u2217\u2212\u03bbi \u2265\u03b4\u00d7\n2 \u03bb\u2217, then we must have \u03bb\u2217\u2212\u03bbi \u226513\n2 (\u03bb(f) \u2212\u03bb\u2217)\nowing to \u03bb(f) \u2212\u03bb\u2217\u2264\u03b4\u00d7\n13 \u03bb\u2217from Lemma B.4.d. This further implies that \u03bb(f) \u2212\u03bbi \u226515\n2 (\u03bb(f) \u2212\u03bb\u2217).\nPlugging in Lemma B.4.e we further have \u03bb(f) \u2212\u03bbi \u226515\n2 \u00b7 3\n10(\u03bb(f) \u2212\u03bb1) > 2(\u03bb(f) \u2212\u03bb1). Using the\nde\ufb01nition of \u00b5i, we must have \u00b51/2 > \u00b5i. In sum, we also have\nX\ni\u2208[d],\u03bbi\u2264(1\u2212\u03b4\u00d7/2)\u03bb\u2217\n(w\u22a4ui)2 \u2264\u03b5 .\nOn the other hand,\nw\u22a4Mw =\nd\nX\ni=1\n\u03bbi(w\u22a4ui)2 \u2265\u2212\u03b5\u03bb\u2217+\nX\ni\u2208[d],\u03bbi>(1\u2212\u03b4\u00d7/2)\u03bb\u2217\n\u03bbi(w\u22a4ui)2\n\u2265\u2212\u03b5\u03bb\u2217+ (1 \u2212\u03b4\u00d7/2)\u03bb\u2217\u00b7\nX\ni\u2208[d],\u03bbi>(1\u2212\u03b4\u00d7/2)\u03bb\u2217\n(w\u22a4ui)2\n\u2265\u2212\u03b5\u03bb\u2217+ (1 \u2212\u03b4\u00d7/2)(1 \u2212\u03b5)\u03bb\u2217\u2265(1 \u2212\u03b4\u00d7/2)(1 \u22123\u03b5)\u03bb\u2217.\nThe number of oracle calls to A is determined by the number of iterations in the repeat-until\nloop. It is easy to verify that there are at most O(log(1/\u03b4\u00d7)) such iteartions, so the total number\nof oracle calls to A is only O(log(1/\u03b4\u00d7)m1 + m2).\nAs for the condition number, each time we call A we have\n\u03bbmax(\u03bb(s)I \u2212M)\n\u03bbmin(\u03bb(s)I \u2212M) \u2264\u03bb(s) \u2212\u03bbd\n\u03bb(s) \u2212\u03bb1\n\u2264\u03bb(s) + \u03bb\u2217\n\u03bb(s) \u2212\u03bb\u2217\u2264\n2\u03bb(s)\n\u03bb(s) \u2212\u03bb\u2217\nand\n\u03bbmax(\u03bb(s)I + M)\n\u03bbmin(\u03bb(s)I + M) \u2264\n2\u03bb(s)\n\u03bb(s) \u2212\u03bb\u2217\nIf s = 0 then we have\n\u03bb(0)\n\u03bb(0)\u2212\u03bb\u2217\u22641+\u03b4\u00d7\n\u03b4\u00d7\nbecause \u03bb\u2217\u22641. If s \u2264f \u22122 then we have\n\u03bb(s)\n\u03bb(s)\u2212\u03bb\u2217\u2264\n\u03bb(s)\n\u2206(s+1) \u2264\n\u03bb(s)\n\u03b4\u00d7\u03bb(s+1)/12 \u226412\n\u03b4\u00d7 where the \ufb01rst inequality follows from Lemma B.4.c, the second inequality follows\nfrom the stopping criterion, and the third inequality follows from the monotonicity of \u03bb(s).\nIf\ns = f \u22121 then we have\n\u03bb(s)\n\u03bb(s)\u2212\u03bb\u2217\u2264\n2\u03bb(s)\n\u03bb(s\u22121)\u2212\u03bb\u2217\u22642\u03bb(s)\n\u2206(s) \u2264\n2\u03bb(s)\n\u03b4\u00d7\u03bb(s)/12 = 24\n\u03b4\u00d7 where the \ufb01rst two inequalities\n18\nfollow from Lemma B.4.c and the third inequality follows from our stopping criterion. If s = f\nthen we have\n\u03bb(s)\n\u03bb(s)\u2212\u03bb\u2217\u226448\n\u03b4\u00d7 owing to Lemma B.4.d. In all cases we have \u03bbmax(\u03bb(s)I\u2212M)\n\u03bbmin(\u03bb(s)I\u2212M) \u226496\n\u03b4\u00d7 and\n\u03bbmax(\u03bb(s)I+M)\n\u03bbmin(\u03bb(s)I+M) \u226496\n\u03b4\u00d7 .\nFinally, we have\n1\n\u03bbmin(\u03bb(s)I\u2212M) =\n\u03bb(s)\n\u03bbmin(\u03bb(s)I\u2212M) \u00b7\n1\n\u03bb(s) \u2264\n\u03bb(s)\n\u03bb(s)\u2212\u03bb\u2217\u00b7\n1\n\u03bb\u2217\u2264\n48\n\u03b4\u00d7\u03bb\u2217where the \ufb01rst\ninequality follows from \u03bb(s) \u2265\u03bb\u2217. Similarly, we also have\n1\n\u03bbmin(\u03bb(s)I+M) \u2264\n48\n\u03b4\u00d7\u03bb\u2217.\n\u25a1\nC\nMain Matrix-Algebra Lemmas\nIn this section we provide some necessary lemmas on matrix algebra that shall become essential\nfor our proof of Theorem 4.1. Many of these lemmas are analogous to those ones used in the SVD\nalgorithm by the same authors of this paper [2], however, we need some extra care in this paper\nbecause the underlying matrix M is no longer PSD.\nProposition C.1.\nLet A, B be two (column) orthonormal matrix such that for \u03b7 \u22650,\nA\u22a4BB\u22a4A \u2ab0(1 \u2212\u03b7)I\nThen we have: there exists a matrix Q, \u2225Q\u22252 \u22641 such that\n\u2225A \u2212BQ\u22252 \u2264\u221a\u03b7\nProof. Since A\u22a4A = I and A\u22a4BB\u22a4A \u2ab0(1 \u2212\u03b7)I, we know that A\u22a4B\u22a5(B\u22a5)\u22a4A \u2aaf\u03b7I. By the fact\nthat\nA = (BB\u22a4+ B\u22a5(B\u22a5)\u22a4)A = BB\u22a4A + B\u22a5(B\u22a5)\u22a4A\nwe can let Q = B\u22a4A and obtain\n\u2225A \u2212BQ\u22252 \u2264\u2225B\u22a5(B\u22a5)\u22a4A\u22252 \u2264\u221a\u03b7 .\n\u25a1\nC.1\nApproximate Projection Lemma\nThe next lemma states that, projecting a symmetric matrix M into the orthogonal space of Vs \u2208\nRd\u00d7s is almost equivalent to projecting it into the orthogonal space of Qs \u2208Rd\u00d7s, if Qs is the\nprojection of Vs into the orthogonal space of U but \u2225V \u22a4\ns U\u2225is small. This lemma is obvious if\n\u201csmall\u201d means zero correlation: if Vs were completely orthogonal to U then Qs would equal to Vs,\nso projecting M into the orthogonal space of Vs would be equivalent to that of Qs. However, even\nin the inexact scenario, this argument is true.\nLemma C.2.\nLet M be a symmetric matrix with (not necessarily sorted) eigenvalues \u03bb1, . . . , \u03bbd\nand the corresponding (normalized) eigenvectors u1, . . . , ud \u2208Rd. For every k \u22651, de\ufb01ne U \u22a5=\n(u1, . . . , uk) \u2208Rd\u00d7k and U = (uk+1, . . . , ud) \u2208Rd\u00d7(d\u2212k). For every \u03b5 \u2208(0, 1\n2), let Vs \u2208Rd\u00d7s be a\ncolumn orthogonal matrix such that \u2225V \u22a4\ns U\u22252 \u2264\u03b5, de\ufb01ne Qs \u2208Rd\u00d7s to be an arbitrary orthogonal\nbasis of the column span of U \u22a5(U \u22a5)\u22a4Vs, then we have:\n\r\r\r\n\u0010\nI \u2212QsQ\u22a4\ns\n\u0011\nM\n\u0010\nI \u2212QsQ\u22a4\ns\n\u0011\n\u2212\n\u0010\nI \u2212VsV \u22a4\ns\n\u0011\nM\n\u0010\nI \u2212VsV \u22a4\ns\n\u0011\r\r\r\n2 \u226413\u03b5\u2225M\u22252 .\nProof of Lemma C.2. Since Qs is an orthogonal basis of the column span of U \u22a5(U \u22a5)\u22a4Vs, there is\na matrix R \u2208Rs\u00d7s such that\nQs = U \u22a5(U \u22a5)\u22a4VsR\nUsing the fact that Q\u22a4\ns Qs = I, we have:\n(U \u22a5(U \u22a5)\u22a4VsR)\u22a4(U \u22a5(U \u22a5)\u22a4VsR) = I =\u21d2R\u22a4V \u22a4\ns U \u22a5(U \u22a5)\u22a4VsR = I .\n19\nBy the fact that V \u22a4\ns Vs = I and U \u22a5(U \u22a5)\u22a4+ UU\u22a4= I, we can rewrite the above equality as:\nR\u22a4\u0010\nI \u2212V \u22a4\ns UU\u22a4Vs\n\u0011\nR = I\n(C.1)\nFrom our lemma assumption, we have: \u2225V \u22a4\ns U\u22252 \u2264\u03b5, which implies 0 \u2aafV \u22a4\ns UU \u22a4Vs \u2aaf\u03b52I.\nPutting this into (C.1), we obtain:\nI \u2aafR\u22a4R \u2aaf\n1\n1 \u2212\u03b52 I \u2aaf\n\u00001 + 4\n3\u03b52\u0001\nI\nThe above inequality directly implies that I \u2aafRR\u22a4\u2aaf\n\u00001 + 4\n3\u03b52\u0001\nI. Therefore,\n\r\r\rQsQ\u22a4\ns \u2212VsV \u22a4\ns\n\r\r\r\n2\n=\n\r\r\rU \u22a5(U \u22a5)\u22a4VsRR\u22a4V \u22a4\ns U \u22a5(U \u22a5)\u22a4\u2212VsV \u22a4\ns\n\r\r\r\n2\n=\n\r\r\rU \u22a5(U \u22a5)\u22a4VsRR\u22a4V \u22a4\ns U \u22a5(U \u22a5)\u22a4\u2212(U \u22a5(U \u22a5)\u22a4+ UU\u22a4)VsV \u22a4\ns (U \u22a5(U \u22a5)\u22a4+ UU \u22a4)\n\r\r\r\n2\n\u2264\n\r\r\rU \u22a5(U \u22a5)\u22a4Vs(RR\u22a4\u2212I)V \u22a4\ns U \u22a5(U \u22a5)\u22a4\r\r\r\n2 +\n\r\r\rUU\u22a4VsV \u22a4\ns UU \u22a4\r\r\r\n2 + 2\n\r\r\rU \u22a5(U \u22a5)\u22a4VsV \u22a4\ns UU \u22a4\r\r\r\n2\n\u2264\n\r\r\rRR\u22a4\u2212I\n\r\r\r\n2 +\n\r\r\rU \u22a4VsV \u22a4\ns U\n\r\r\r\n2 + 2\n\r\r\rV \u22a4\ns UU \u22a4Vs\n\r\r\r\n1/2\n2\n\u2264\n4\n3\u03b52 + \u03b52 + 2\u03b5 < 19\n6 \u03b5 .\nFinally, we have\n\r\r\r\n\u0010\nI \u2212QsQ\u22a4\ns\n\u0011\nM\n\u0010\nI \u2212QsQ\u22a4\ns\n\u0011\n\u2212\n\u0010\nI \u2212VsV \u22a4\ns\n\u0011\nM\n\u0010\nI \u2212VsV \u22a4\ns\n\u0011\r\r\r\n2\n\u22642\n\r\r\r\n\u0010\nQsQ\u22a4\ns \u2212VsV \u22a4\ns\n\u0011\nM\n\r\r\r\n2 +\n\r\r\r\n\u0010\nQsQ\u22a4\ns \u2212VsV \u22a4\ns\n\u0011\nMQsQ\u22a4\ns\n\r\r\r\n2 +\n\r\r\r\n\u0010\nQsQ\u22a4\ns \u2212VsV \u22a4\ns\n\u0011\nMVsV \u22a4\ns\n\r\r\r\n2\n\u226419 \u00d7 4\n6\n\u03b5\u2225M\u22252 < 13\u03b5\u2225M\u22252 .\n\u25a1\nC.2\nGap-Free Wedin Theorem\nLemma C.3 (two-sided gap-free Wedin theorem).\nFor \u03b5 \u22650, let A, B be two symmetric matrices\nsuch that \u2225A \u2212B\u22252 \u2264\u03b5. For every \u00b5 \u22650, \u03c4 > 0, let U be column orthonormal matrix consisting\nof eigenvectors of A with absolute eigenvalues \u2264\u00b5, let V be column orthonormal matrix consisting\nof eigenvectors of B with absolute eigenvalues \u2265\u00b5 + \u03c4, then we have:\n\u2225U \u22a4V \u2225\u2264\u03b5\n\u03c4 .\nProof of Lemma C.3. We write A and B in terms of eigenvalue decomposition:\nA = U\u03a3U \u22a4+ U \u2032\u03a3\u2032U \u2032\u22a4\nand\nB = V e\u03a3V \u22a4+ V \u2032 e\u03a3\u2032V \u2032\u22a4,\nwhere U \u2032 is orthogonal to U and V \u2032 is orthogonal to V . Letting R = A \u2212B, we obtain:\n\u03a3U \u22a4= U \u22a4A = U \u22a4(B + R)\n=\u21d2\u03a3U \u22a4V = U \u22a4BV + U \u22a4RV = U \u22a4V e\u03a3 + U \u22a4RV\n=\u21d2\u03a3U \u22a4V e\u03a3\u22121 = U \u22a4V + U \u22a4RV e\u03a3\u22121 .\nTaking spectral norm on both sides, we obtain:\n\u2225\u03a3\u22252\u2225U \u22a4V \u22252\u2225e\u03a3\u22121\u22252 \u2265\u2225\u03a3U \u22a4V e\u03a3\u22121\u22252 \u2265\u2225U \u22a4V \u22252 \u2212\u2225U \u22a4RV e\u03a3\u22121\u22252 .\nThis can be simpli\ufb01ed to\n\u00b5\n\u00b5 + \u03c4 \u2225U \u22a4V \u22252 \u2265\u2225U \u22a4V \u22252 \u2212\n\u03b5\n\u00b5 + \u03c4 ,\n20\nand therefore we have \u2225U \u22a4V \u22252 \u2264\u03b5\n\u03c4 as desired.\n\u25a1\nC.3\nEigenvector Projection Lemma\nOur next technical lemma studies the projection of a matrix M into the orthogonal direction of\na vector v, where v has little correlation with M\u2019s leading eigenvectors below some threshold \u00b5\n(denoted by U). The conclusion of the lemma says that, after the projection, if we study the leading\neigenvectors of M\u2032 = (I \u2212vv\u22a4)M(I \u2212vv\u22a4) below some threshold \u00b5 + \u03c4 and denote it by V1, then\nU approximately embeds into V1, meaning that although V1 could be of a larger dimension of U,\nhowever, there exists a matrix Q with spectral norm no more than 1 such that \u2225U \u2212V1Q\u22252 is small.\nLemma C.4.\nLet M \u2208Rd\u00d7d be a symmetric matrix with eigenvalues \u03bb1, . . . , \u03bbd and corresponding\neigenvectors u1, . . . , ud. Suppose |\u03bb1| \u2265\u00b7 \u00b7 \u00b7 \u2265|\u03bbd|. De\ufb01ne U = (uj+1, . . . , ud) \u2208Rd\u00d7(d\u2212j) to be the\nmatrix consisting of all eigenvectors with absolute eigenvalues \u2264\u00b5. Let v \u2208Rd be a unit vector\nsuch that \u2225v\u22a4U\u22252 \u2264\u03b5 \u22641/2, and de\ufb01ne\nM\u2032 =\n\u0010\nI \u2212vv\u22a4\u0011\nM\n\u0010\nI \u2212vv\u22a4\u0011\n.\nThen, denoting by [V2, V1, v] \u2208Rd\u00d7d the unitary matrix consisting of (column) eigenvectors of\nM\u2032, where V1 consists of eigenvectors with absolute eigenvalue \u2264\u00b5 + \u03c4, then there exists a matrix\nQ with spectral norm \u2225Q\u22252 \u22641 such that\n\u2225U \u2212V1Q\u22252 \u2264\nr\n169\u03b52\u2225M\u22252\n2\n\u03c4 2\n+ \u03b52 .\nProof of Lemma C.4. Using Lemma C.2, let q =\nU\u22a5(U\u22a5)\u22a4v\n\u2225U\u22a5(U\u22a5)\u22a4v\u22252 be the projection of v to U \u22a5, we\nknow that\n\r\r\r\n\u0010\nI \u2212qq\u22a4\u0011\nM\n\u0010\nI \u2212qq\u22a4\u0011\n\u2212\n\u0010\nI \u2212vv\u22a4\u0011\nM\n\u0010\nI \u2212vv\u22a4\u0011\r\r\r\n2 \u226413\u03b5\u2225M\u22252 .\nDenote\n\u0000I \u2212qq\u22a4\u0001\nM\n\u0000I \u2212qq\u22a4\u0001\nas M\u2032\u2032. We know that uj+1, . . . , ud are still eigenvectors of M\u2032\u2032\nwith eigenvalue \u03bbj+1, . . . , \u03bbd.\nApply Lemma C.3 on A = M\u2032\u2032, U and B = M\u2032, V = V2, we obtain:\n\u2225U \u22a4V2\u22252 \u226413\u03b5\u2225M\u22252\n\u03c4\n.\nThis implies that\nU \u22a4V1V \u22a4\n1 U = I \u2212U \u22a4V2V \u22a4\n2 U \u2212U \u22a4vv\u22a4U \u2ab0\n\u0012\n1 \u2212169\u03b52\u2225M\u22252\n2\n\u03c4 2\n\u2212\u03b52\n\u0013\nI ,\nwhere the inequality uses the assumption \u2225v\u22a4U\u22252 \u2264\u03b5.\nApply Proposition C.1 to A = U and B = V1, we conclude that there exists a matrix Q,\n\u2225Q\u22252 \u22641 such that\n\u2225U \u2212V1Q\u22252 \u2264\nr\n169\u03b52\u2225M\u22252\n2\n\u03c4 2\n+ \u03b52 .\n\u25a1\nD\nMatrix Inversion via Approx Accelerated Gradient Descent\nGiven a positive de\ufb01nite matrix N, it is well-known that one can reduce the (approximate) matrix\ninversion problem N\u22121\u03c7 to multiple computations of the matrix-vector multiplication (i.e., of the\nform w\u2032 \u2190Nw). In particular, Chebyshev method [9] uses the so-called Chebyshev polynomial for\n21\nAlgorithm 4 AGDinexact(f, x0, T)\nInput: f an L-smooth and \u03c3-strongly convex function;\nx0 some initial point; and\nT the number of iterations.\nOutput: yT .\n1: \u03c4 \u2190\n2\n1+\u221a\n8L/\u03c3+1, \u03b7 \u2190\n1\n\u03c4L.\n\u22c4\n\u03c4 = O(\n\u221a\u03c3\n\u221a\nL) and \u03b7 = O(\n1\n\u221a\n\u03c3L)\n2: y0 \u2190x0,\nz0 \u2190x0.\n3: for k \u21900 to T \u22121 do\n4:\nxk+1 \u2190\u03c4zk + (1 \u2212\u03c4)yk.\n5:\nCompute approximate gradient e\u2207f(xk+1) satisfying \u2225e\u2207f(xk+1) \u2212\u2207f(xk+1)\u22252 \u2264e\u03b5.\n6:\nyk+1 \u2190xk+1 \u22121\nL e\u2207f(xk+1)\n7:\nzk+1 \u2190\n1\n1+\u03b7\u03c3\n\u0000zk + \u03b7\u03c3xk+1 \u2212\u03b7 e\u2207f(xk+1)\n\u0001\n8: end for\n9: return yT .\nthis purpose, and the number of matrix-vector multiplications is determined by the degree of that\npolynomial.\nIn this section, we revisit this problem by allowing matrix-vector multiplications to be computed\nonly approximately. We emphasize that this is not a simple task in general. If matrix inversion\nis reduced to T matrix-vector multiplications, then a standard analysis implies that each of these\nmultiplications must be computed up to a very small error 2\u2212\u2126(T). If the actual matrix-vector\nmultiplication subroutine has a logarithmic dependency on the error in its running time, then we\nwill have a total running time at least quadratically dependent on T.9\nTo avoid such an exponentially accuracy loss, we abandon known results (such as Chebyshev\nmethod) and design our own method. We prove the following theorem in this section:\nTheorem D.1.\nGiven a matrix N, we reduce the problem of computing \u03be \u2190N\u22121\u03c7 to multiple\ncomputations w\u2032 \u2190Nw as follows.\nIf N satis\ufb01es N = B\u22121/2NB1/2 where N and B are both d \u00d7 d positive de\ufb01nite matrices, for\nevery e\u03b5 > 0 and \u03c7 \u2208Rd, in order to obtain \u03be satisfying \u2225\u03be \u2212N\u22121\u03c7\u2225\u2264e\u03b5,\n\u2022 it su\ufb03ces to compute w\u2032 \u2190Nw only eO\n\u0000\u221a\u03baN\n\u0001\ntimes, and\n\u2022 each time of accuracy \u2225w\u2032 \u2212Nw\u2225\u2264O(1/poly(\u03baB, e\u03b5, \u03bbmin(N))).\nOur reduction is based on an inexact variant of the accelerated gradient descent (AGD) method\noriginally put forward by Nesterov [23], which relies on some convex optimization techniques and\ncan be proved using the linear-coupling framework [5].\nWe prove this inexact AGD result in\nAppendix D.1. Our \ufb01nal proof of Theorem D.1 is included in Appendix D.2.\nD.1\nInexact Accelerated Gradient Descent\nWe study an inexact version of the classical accelerated gradient descent (AGD) method, and our\npseudocode is presented in Algorithm 4.\nThe di\ufb00erence between our method and known AGD\nmethods is that we only require the algorithm to know an approximate gradient e\u2207f(xk+1) in each\niteration k, as opposed to the exact full gradient \u2207f(xk+1). We require \u2225e\u2207f(xk+1) \u2212\u2207f(xk+1)\u22252\nto be upper bounded by some parameter e\u03b5 in each iteration. Our next convergence theorem states\n9Indeed, for instance in the ALS algorithm of [29] for solving CCA, the authors obtained a running time propor-\ntional to 1/gap2 although there are only 1/gap iterations.\n22\nthat this inexact AGD method only incurs an additive loss proportional to O(e\u03b52).\nTheorem D.2 (inexact AGD). If f(x) is L-smooth and \u03c3-strongly convex, then AGDinexact(f, x0, T)\nproduces an output yT satisfying\nf(yT ) \u2212f(x\u2217) \u2264O(1) \u00b7 (1 \u2212\u03c4)T (f(x0) \u2212f(x\u2217)) + O\n\u0000e\u03b52\n\u03c3\n\u0001\n,\nwhere \u03c4 = \u2126(\np\n\u03c3/L). In other words, if the approximate gradient oracle satis\ufb01es e\u03b5 \u2264O(\u221a\u03b5\u03c3) and\nT = O(\np\nL/\u03c3 \u00b7 log(1/\u03b5)), then we have f(yT ) \u2212f(x\u2217) \u2264\u03b5.\nTheorem D.2 can be proved using the linear-coupling framework of [5].\nIn this framework,\naccelerated methods are analyzed by a gradient descent lemma (Lemma D.3 below), a mirror\ndescent lemma (Lemma D.4 below), and a coupling step (Lemma D.5 and D.6 below).\nLemma D.3 (gradient descent).\nf(yk+1) \u2264f(xk+1) \u2212\n1\n2L\u2225\u2207f(xk+1)\u22252\n2 + e\u03b52\n2L.\nProof. Abbreviating xk+1 by x and yk+1 by y, the smoothness property of function f(\u00b7) tells us\nf(y) \u2212f(x) \u2264\u27e8\u2207f(x), y \u2212x\u27e9+ L\n2 \u2225y \u2212x\u22252 .\nNow, since y \u2212x = \u2212\u2207f(x)+\u03c7\nL\nwhere \u2225\u03c7\u22252 \u2264e\u03b5, we have\n\u27e8\u2207f(x), y \u2212x\u27e9+ L\n2 \u2225y \u2212x\u22252\n2 = \u22121\nL \u27e8\u2207f(x), \u2207f(x) + \u03c7\u27e9+ 1\n2L\u27e8\u2207f(x) + \u03c7, \u2207f(x) + \u03c7\u27e9\n\u2264\u22121\n2L\u2225\u2207f(x)\u22252\n2 + e\u03b52\n2L .\n\u25a1\nSince our update on z can be written in the following minimization form, known as mirror-\ndescent form in optimization literatures:\nz(i)\nk+1 = min\nz\nn1\n2\u2225z \u2212zk\u22252\n2 + \u03b7\u27e8e\u2207f(xk+1), z\u27e9+ \u03b7\u03c3\n2 \u2225z \u2212xk+1\u22252\n2\no\n.\n(D.1)\nIt implies the following classical lemma (see for instance [6, Lemma 5.4]):\nLemma D.4 (mirror descent).\nFor every u \u2208Rn,\n\u03b7\u27e8e\u2207f(xk+1), zk+1 \u2212u\u27e9\u2212\u03b7\u03c3\n2 \u2225xk+1 \u2212u\u22252\n2 \u2264\u22121\n2\u2225zk \u2212zk+1\u22252\n2 + 1\n2\u2225zk \u2212u\u22252\n2 \u22121 + \u03b7\u03c3\n2\n\u2225zk+1 \u2212u\u22252\n2 .\nThe following inequality is a nature linear combination of the two lemmas above:\nLemma D.5 (coupling 1).\nFor every u \u2208Rn,\n\u03b7\u27e8\u2207f(xk+1), zk \u2212u\u27e9\u2212\u03b7\u03c3\n2 \u2225u \u2212xk+1\u22252\n2\n\u2264\u03b72L\n\u0000f(xk+1) \u2212f(yk+1)\n\u0001\n+ 1\n2\u2225zk \u2212u\u22252\n2 \u22121 + \u03b7\u03c3/2\n2\n\u2225zk+1 \u2212u\u22252\n2 + e\u03b52(\u03b7\n\u03c3 + \u03b72\n2 ) .\nProof. Combining Lemma D.3 and Lemma D.4 we deduce that for each i \u2208[n],\n\u27e8\u03b7\u2207f(xk+1), zk \u2212u\u27e9\u2212\u03b7\u03c3\n2 \u2225xk+1 \u2212u\u22252\n2\n\u2264\u27e8\u03b7\u2207f(xk+1), zk \u2212zk+1\u27e9+ \u27e8\u03b7 e\u2207f(xk+1), zk+1 \u2212u\u27e9+ e\u03b5\u03b7\u2225zk+1 \u2212u\u22252 \u2212\u03b7\u03c3\n2 \u2225xk+1 \u2212u\u22252\n2\nx\n\u2264\u27e8\u03b7\u2207f(xk+1), zk \u2212zk+1\u27e9\u22121\n2\u2225zk \u2212zk+1\u22252\n2 + 1\n2\u2225zk \u2212u\u22252\n2 \u22121 + \u03b7\u03c3\n2\n\u2225zk+1 \u2212u\u22252\n2 + e\u03b5\u03b7\u2225zk+1 \u2212u\u22252\ny\n\u2264\u03b72\n2 \u2225\u2207f(xk+1)\u22252\n2 + 1\n2\u2225zk \u2212u\u22252\n2 \u22121 + \u03b7\u03c3/2\n2\n\u2225zk+1 \u2212u\u22252\n2 + e\u03b52\u03b7\n\u03c3\n23\nz\n\u2264\u03b72L\n\u0000f(xk+1) \u2212f(yk+1)\n\u0001\n+ 1\n2\u2225zk \u2212u\u22252\n2 \u22121 + \u03b7\u03c3/2\n2\n\u2225zk+1 \u2212u\u22252\n2 + e\u03b52(\u03b7\n\u03c3 + \u03b72\n2 ) .\nAbove, x uses Lemma D.4, y uses the Young\u2019s inequality which states 2\u27e8a, b\u27e9\u2264\u2225a\u22252 + \u2225b\u22252, z\nuses Lemma D.3.\n\u25a1\nTaking into account xk+1 = \u03c4zk + (1 \u2212\u03c4)yk and the convexity of f(\u00b7), we can rewrite some\nterms of Lemma D.5 and obtain\nLemma D.6 (coupling 2).\n0 \u2264(1 \u2212\u03c4)\u03b7\n\u03c4\n(f(yk)\u2212f(x\u2217))\u2212\u03b7\n\u03c4 (f(yk+1)\u2212f(x\u2217))+ 1\n2\u2225zk\u2212x\u2217\u22252\n2\u22121 + \u03b7\u03c3/2\n2\n\u2225zk+1\u2212x\u2217\u22252\n2+e\u03b52(\u03b7\n\u03c3 + \u03b72\n2 )\nProof.\n\u03b7(f(xk+1) \u2212f(x\u2217))\nx\n\u2264\u03b7\u27e8\u2207f(xk+1), xk+1 \u2212x\u2217\u27e9\u2212\u03b7\u03c3\n2 \u2225x\u2217\u2212xk+1\u22252\n2\n= \u03b7\u27e8\u2207f(xk+1), xk+1 \u2212zk\u27e9+ \u03b7\u27e8\u2207f(xk+1), zk \u2212x\u2217\u27e9\u2212\u03b7\u03c3\n2 \u2225x\u2217\u2212xk+1\u22252\n2\ny= (1 \u2212\u03c4)\u03b7\n\u03c4\n\u27e8\u2207f(xk+1), yk \u2212xk+1\u27e9+ \u03b7\u27e8\u2207f(xk+1), zk \u2212x\u2217\u27e9\u2212\u03b7\u03c3\n2 \u2225x\u2217\u2212xk+1\u22252\n2\nz\n\u2264(1 \u2212\u03c4)\u03b7\n\u03c4\n(f(yk) \u2212f(xk+1)) + \u03b72L\n\u0000f(xk+1) \u2212f(yk+1)\n\u0001\n+ 1\n2\u2225zk \u2212u\u22252\n2 \u22121 + \u03b7\u03c3/2\n2\n\u2225zk+1 \u2212u\u22252\n2 + e\u03b52\u0000\u03b7\n\u03c3 + \u03b72\n2\n\u0001\n.\nAbove, x is owing to the strong convexity of f(\u00b7), y uses the fact that xk+1 = \u03c4zk + (1 \u2212\u03c4)yk, and\nz uses the convexity of f(\u00b7) as well as Lemma D.5 with the choice of u = x\u2217. Recall \u03b7 =\n1\n\u03c4L, we\narrive at the desired inequality.\n\u25a1\nWe are now ready to prove Theorem D.2.\nProof of Theorem D.2. We choose \u03c4 =\n2\n1+\u221a\n8L/\u03c3+1 \u2208[0, 1), and this choice ensures that 1+\u03b7\u03c3/2 =\n1\n1\u2212\u03c4 . Under these parameter choices, Lemma D.6 becomes\n\u0000f(yk+1)\u2212f(x\u2217)\n\u0001\n+\n\u03c4\n2\u03b7(1 \u2212\u03c4)\u2225zk+1\u2212x\u2217\u22252\n2 \u2264(1\u2212\u03c4)\n\u0010\n(f(yk)\u2212f(x\u2217))+\n\u03c4\n2\u03b7(1 \u2212\u03c4)\u2225zk\u2212x\u2217\u22252\n2\n\u0011\n+e\u03b52\u03c4\n\u0000 1\n\u03c3+\u03b7\n2\n\u0001\nTelescoping it for all iterations k = 0, 1, . . . , T \u22121, we conclude that\nf(yT )\u2212f(x\u2217) \u2264(1\u2212\u03c4)T \u0010\nf(y0)\u2212f(x\u2217)+ \u03c4\n2\u03b7\u2225z0\u2212x\u2217\u22252\n2\n\u0011\n+e\u03b52( 1\n\u03c3+\u03b7\n2) \u2264O(1)\u00b7(1\u2212\u03c4)T (f(x0)\u2212f(x\u2217))+O\n\u0000e\u03b52\n\u03c3\n\u0001\n.\nwhere the last inequality is because (i) x0 = y0 = z0, (ii) \u03c4/\u03b7 = O(\u03c3) and (iii) the strong convexity\nof f(\u00b7) which implies f(x0) \u2212f(x\u2217) \u2265\u03c3\n2 \u2225x0 \u2212x\u2217\u22252\n2.\n\u25a1\nD.2\nProof of Theorem D.1\nProof of Theorem D.1. We \ufb01rst verify accuracy. Since\n\u2225\u03be \u2212N\u22121\u03c7\u2225\u2264e\u03b5 \u21d0= \u2225B1/2\u03be \u2212B1/2N\u22121\u03c7\u2225\u2264e\u03b5 \u00b7\np\n\u03bbmin(B)\n\u21d0= \u2225B1/2\u03be \u2212N\u22121B1/2\u03c7\u2225\u2264e\u03b5 \u00b7\np\n\u03bbmin(B) ,\n(D.2)\nit su\ufb03ces to \ufb01nd \u03be to satisfy (D.2) in order to satisfy the accuracy requirement \u2225\u03be \u2212N\u22121\u03c7\u2225\u2264e\u03b5.\nDe\ufb01ne f(x)\ndef\n= 1\n2x\u22a4Nx \u2212\n\u0000B1/2\u03c7\n\u0001\u22a4x and let x\u2217be its minimizer. Then it satis\ufb01es x\u2217= N\u22121B1/2\u03c7\n24\nand f(x) \u2212f(x\u2217) =\n1\n2(x \u2212x\u2217)\u22a4N(x \u2212x\u2217).\nFor this reason, it su\ufb03ces to \ufb01nd an approximate\nminimizer of f(x) satisfying\n1\n2(x \u2212x\u2217)\u22a4N(x \u2212x\u2217) = f(x) \u2212f(x\u2217) \u2264e\u03b52\n2 \u03bbmin(B)\u03bbmin(N) =: e\u03b5\u2032\n(D.3)\nbecause if we let \u03be = B\u22121/2x then the above inequality implies 1\n2\u2225x \u2212x\u2217\u22252 \u2264e\u03b52\n2 \u00b7 \u03bbmin(B) which is\nthe same as (D.2). In sum, we can call AGDinexact to \ufb01nd an approximate minimizer x with additive\nerror no more than e\u03b5\u2032, and then de\ufb01ning \u03be = B\u22121/2x gives a solution of \u03be satisfying \u2225\u03be\u2212N\u22121\u03c7\u2225\u2264e\u03b5.\nWe now focus on the actual implementation of AGDinexact. If we choose x0 = 0 as the initial\nvector, we can write xk, yk, zk implicitly as xk = B1/2xk, yk = B1/2yk, zk = B1/2yk (thus only keep\ntrack of xk, yk, zk) throughout the algorithm. Under these notations, we claim that it su\ufb03ces to\nperform matrix vector multiplication on N (i.e., of the form w\u2032 \u2190Nw) for at most O(T) times on\nthose implicit vectors where T = O\n\u0000p\n\u03bbmax(N)/\u03bbmin(N) log(1/e\u03b5\u2032)\n\u0001\nis the number of iterations of\nAGDinexact according to Theorem D.2.\nThis is so because \u2207f(xk) = Nxk \u2212B1/2\u03c7 = B1/2\u0000Nxk \u2212\u03c7\n\u0001\nand therefore for instance yk+1 \u2190\nxk+1 \u22121\nL e\u2207f(xk+1) can be implemented as yk+1 \u2190xk+1 \u22121\nL(Nxk+1 \u2212\u03c7) so only matrix-vector\nmultiplication on N is needed. In addition, as long as each w\u2032 \u2190Nw is computed to an additive\nerror \u2225w\u2032 \u2212Nw\u2225\u2264O\n\u0000e\u03b5\u00b7\u03bbmin(N)\np\n\u03bbmin(B)/\u03bbmax(B)\n\u0001\n, we can use B1/2(w\u2032 \u2212\u03c7) as the approximate\ngradient which is di\ufb00erent from the true gradient \u2207f(xk) by an additive amount O(\np\n\u03bbmin(N)e\u03b5\u2032).\nThis satis\ufb01es the approximation require of Theorem D.2, and thus the accuracy guarantee provided\nby Theorem D.2 is satis\ufb01ed.\n\u25a1\nE\nProof for Section 4: GenEV Theorems\nE.1\nProof of Theorem 4.1\nIn this section we prove Theorem 4.1 formally.\nTheorem 4.1 (restated).\nLet M \u2208Rd\u00d7d be a symmetric matrix with eigenvalues \u03bb1, . . . , \u03bbd \u2208\n[\u22121, 1] and corresponding eigenvectors u1, . . . , ud. Suppose without loss of generality that |\u03bb1| \u2265\n\u00b7 \u00b7 \u00b7 \u2265|\u03bbd|.\nSuppose k \u2208[d], \u03b4\u00d7, p \u2208(0, 1). Then, LazyEV outputs a (column) orthonormal matrix Vk =\n(v1, . . . , vk) \u2208Rd\u00d7k which, with probability at least 1 \u2212p, satis\ufb01es all of the following properties.\n(Denote by Mk = (I \u2212VkV \u22a4\nk )M(I \u2212VkV \u22a4\nk ).)\n(a) Core lemma: if \u03b5pca \u2264\n\u03b54\u03b4\u00d7\n212k3(|\u03bb1|/|\u03bbk|)2 , then \u2225V \u22a4\nk U\u22252 \u2264\u03b5, where U = (uj, . . . , ud) is the\n(column) orthonormal matrix and j is the smallest index satisfying |\u03bbj| \u2264(1 \u2212\u03b4\u00d7)\u2225Mk\u22121\u22252.\n(b) Spectral norm guarantee: if \u03b5pca \u2264\n\u03b45\n\u00d7\n228k3(|\u03bb1|/|\u03bbk+1|)6 , then |\u03bbk+1| \u2264\u2225Mk\u22252 \u2264|\u03bbk+1|\n1\u2212\u03b4\u00d7 .\n(c) Rayleigh quotient guarantee: if \u03b5pca \u2264\n\u03b45\n\u00d7\n228k3(|\u03bb1|/|\u03bbk+1|)6 , then (1 \u2212\u03b4\u00d7)|\u03bbk| \u2264|v\u22a4\nk Mvk| \u2264\n1\n1\u2212\u03b4\u00d7 |\u03bbk|.\n(d) Schatten-q norm guarantee: for every q \u22651, if \u03b5pca \u2264\n\u03b45\n\u00d7\n228k3d4/q(|\u03bb1|/|\u03bbk+1|)6 , then\n\u2225Mk\u2225Sq \u2264(1 + \u03b4\u00d7)2\n(1 \u2212\u03b4\u00d7)2\n\u0010\nd\nX\ni=k+1\n\u03bbq\ni\n\u00111/q\n= (1 + \u03b4\u00d7)2\n(1 \u2212\u03b4\u00d7)2\nmin\nV \u2208Rd\u00d7k,V \u22a4V =I\n\b\n\u2225(I\u2212V V \u22a4)M(I\u2212V V \u22a4)\u2225Sq\n\t\n.\nProof of Theorem 4.1. Let Vs = (v1, . . . , vs), so we can write\nMs = (I \u2212VsV \u22a4\ns )M(I \u2212VsV \u22a4\ns ) = (I \u2212vsv\u22a4\ns )Ms\u22121(I \u2212vsv\u22a4\ns ) .\n25\nWe \ufb01rst claim that \u2225Ms\u22121\u22252 \u2265|\u03bbs| for every s = 1, . . . , k. This can be proved by the Cauchy\ninterlacing theorem. Indeed, M2\ns\u22121 = (I \u2212Vs\u22121V \u22a4\ns\u22121)M2(I \u2212Vs\u22121V \u22a4\ns\u22121) is a projection of M2 into\na d \u2212s + 1 dimensional space, and therefore its largest eigenvalue \u2225M2\ns\u22121\u22252 should be at least as\nlarge as |\u03bbs|2, the s-th largest eigenvalue of M2. In other words, we have shown \u2225Ms\u22121\u22252 \u2265|\u03bbs|.\n(a) De\ufb01ne b\u03bb = \u2225Mk\u22121\u22252 \u2265|\u03bbk|.\nNote that all column vectors in Vs are automatically eigenvectors of Ms with eigenvalues zero.\nFor analysis purpose only, let Ws be the column matrix of eigenvectors in V \u22a5\ns\nof Ms that have\nabsolute eigenvalues in the range [0, (1 \u2212\u03b4\u00d7 + \u03c4s)b\u03bb], where \u03c4s\ndef\n=\ns\n2k\u03b4\u00d7. We now show that for\nevery s = 0, . . . , k, there exists a matrix Qs such that \u2225U \u2212WsQs\u22252 is small and \u2225Qs\u22252 \u22641.\nWe will do this by induction.\nIn the base case: since \u03c40 = 0, we have W0 = U by the de\ufb01nition of U. We can therefore de\ufb01ne\nQ0 to be the identity matrix.\nFor every s = 0, 1, . . . , k \u22121, suppose there exists a matrix Qs with \u2225Qs\u22252 \u22641 that satis\ufb01es\n\u2225U \u2212WsQs\u22252 \u2264\u03b7s for some \u03b7s > 0, we construct Qs+1 as follows.\nFirst we observe that AppxPCA\u00b1 outputs a vector v\u2032\ns+1 satisfying \u2225v\u2032\u22a4\ns+1Ws\u22252\n2 \u2264\u03b5pca and\n\u2225v\u2032\u22a4\ns+1Vs\u22252\n2 \u2264\u03b5pca with probability at least 1 \u2212p/k.\nThis follows from Theorem 3.1 (us-\ning M = Ms) because [0, (1 \u2212\u03b4\u00d7 + \u03c4s)b\u03bb] \u2286[0, (1 \u2212\u03b4\u00d7/2)b\u03bb], together with the fact that\n\u2225Ms\u22252 \u2265\u2225Mk\u22121\u22252 \u2265b\u03bb. Now, since vs+1 is the projection of v\u2032\ns+1 into V \u22a5\ns , we have\n\u2225v\u22a4\ns+1Ws\u22252\n2 \u2264\n\u2225v\u2032\u22a4\ns+1Ws\u22252\n2\n\u2225(I \u2212VsV \u22a4\ns )v\u2032\ns+1\u22252\n2\n=\n\u2225v\u2032\u22a4\ns+1Ws\u22252\n2\n1 \u2212\u2225V \u22a4\ns v\u2032\ns+1\u22252\n2\n\u2264\n\u03b5pca\n1 \u2212\u03b5pca\n< 1.5\u03b5pca .\n(E.1)\nNext we apply Lemma C.4 with M = Ms, M\u2032 = Ms+1, U = Ws, V = Ws+1, v = vs+1,\n\u00b5 = (1 \u2212\u03b4\u00d7 + \u03c4s)b\u03bb, and \u03c4 = (\u03c4s+1 \u2212\u03c4s)b\u03bb. We obtain a matrix eQs, \u2225eQs\u22252 \u22641 such that10\n\u2225Ws \u2212Ws+1f\nQs\u22252 \u2264\ns\n169(\u03bb1/b\u03bb)2 \u00b7 1.5\u03b5pca\n(\u03c4s+1 \u2212\u03c4s)2\n+ \u03b5pca < 32\u03bb1k\u221a\u03b5pca\n\u03bbk\u03b4\u00d7\n,\nand this implies that\n\u2225Ws+1f\nQsQs \u2212U\u22252 \u2264\u2225Ws+1f\nQsQs \u2212WsQs\u22252 + \u2225WsQs \u2212U\u22252 \u2264\u03b7s + 32\u03bb1k\u221a\u03b5pca\n\u03bbk\u03b4\u00d7\n.\nLet Qs+1 = f\nQsQs we know that \u2225Qs+1\u22252 \u22641 and\n\u2225Ws+1Qs+1 \u2212U\u22252 \u2264\u03b7s+1\ndef\n= \u03b7s + 32\u03bb1k\u221a\u03b5pca\n\u03bbk\u03b4\u00d7\n.\nTherefore, after k-iterations of LazyEV, we obtain:\n\u2225WkQk \u2212U\u22252 \u2264\u03b7k = 32\u03bb1k2\u221a\u03b5pca\n\u03bbk\u03b4\u00d7\nMultiply U \u22a4from the left, we obtain \u2225U \u22a4WkQk \u2212I\u22252 \u2264\u03b7k. Since \u2225Qk\u22252 \u22641, we must have\n\u03c3min(U \u22a4Wk) \u22651 \u2212\u03b7k (here \u03c3min denotes the smallest singular value). Therefore,\nU \u22a4WkW \u22a4\nk U \u2ab0(1 \u2212\u03b7k)2I .\nSince Vk and Wk are orthogonal of each other, we have\nU \u22a4VkV \u22a4\nk U \u2aafU \u22a4(I \u2212WkW \u22a4\nk )U \u2aafI \u2212(1 \u2212\u03b7k)2I \u2aaf2\u03b7kI\n10Technically speaking, to apply Lemma C.4 we need U = Ws to consist of all eigenvectors of Ms with abso-\nlute eigenvalues \u2264\u00b5. However, we only de\ufb01ned Ws to be such eigenvectors that are also orthogonal to Vs. It is\nstraightforward to verify that the same result of Lemma C.4 remains true because vs+1 is orthogonal to Vs.\n26\nTherefore,\n\u2225V \u22a4\nk U\u22252 \u22648(|\u03bb1|/|\u03bbk|)1/2k\u03b51/4\npca\n\u03b41/2\n\u00d7\n\u2264\u03b5 .\n(b) The statement is obvious when k = 0. For every k \u22651, the lower bound is obvious. We prove\nthe upper bound by contradiction. Suppose that \u2225Mk\u22252 > |\u03bbk+1|\n1\u2212\u03b4\u00d7 . Then, since \u2225Mk\u22121\u22252 \u2265\n\u2225Mk\u22252 and therefore |\u03bbk+1|, . . . , |\u03bbd| < (1 \u2212\u03b4\u00d7)\u2225Mk\u22121\u22252, we can apply Theorem 4.1.a of\nthe current k to deduce that \u2225V \u22a4\nk U>k\u22252 \u2264\u03b5 where U>k\ndef\n= (uk+1, . . . , ud). We now apply\nLemma C.2 with Vs = Vk and U = U>k, we obtain a matrix Qk \u2208Rd\u00d7k whose columns are\nspanned by u1, . . . , uk and satisfy\n\r\r\r\n\u0010\nI \u2212QkQ\u22a4\nk\n\u0011\nM\n\u0010\nI \u2212QkQ\u22a4\nk\n\u0011\n\u2212\n\u0010\nI \u2212VkV \u22a4\nk\n\u0011\nM\n\u0010\nI \u2212VkV \u22a4\nk\n\u0011\r\r\r\n2 < 16|\u03bb1|\u03b5 .\nHowever, our assumption says that the second matrix\n\u0000I \u2212VkV \u22a4\nk\n\u0001\nM\n\u0000I \u2212VkV \u22a4\nk\n\u0001\nhas spectral\nnorm at least |\u03bbk+1|/(1\u2212\u03b4\u00d7), but we know that\n\u0000I \u2212QkQ\u22a4\nk\n\u0001\nM\n\u0000I \u2212QkQ\u22a4\nk\n\u0001\nhas spectral norm\nexactly |\u03bbk+1| due to the de\ufb01nition of Qk. Therefore, we must have |\u03bbk+1|\n1\u2212\u03b4\u00d7 \u2212|\u03bbk+1| \u226416|\u03bb1|\u03b5\ndue to triangle inequality.\nIn other words, by selecting \u03b5 in Theorem 4.1.a to satisfy \u03b5 \u2264\n\u03b4\u00d7\n16|\u03bb1|/|\u03bbk+1| (which is satis\ufb01ed\nby our assumption on \u03b5pca), we get a contradiction so can conclude that \u2225Mk\u22252 \u2264|\u03bbk+1|\n1\u2212\u03b4\u00d7 .\n(c) We compute that\n|v\u22a4\nk Mvk| = |v\u22a4\nk Mk\u22121vk|\nx\n\u2265\n|v\u2032\u22a4\nk Mk\u22121v\u2032\nk|\n\u2225(I \u2212Vk\u22121V \u22a4\nk\u22121)v\u2032\nk\u22252\n2\ny\n\u2265|v\u2032\u22a4\nk Mk\u22121v\u2032\nk|\n(1 \u2212\u221a\u03b5pca)2\nz\n\u2265\n1 \u2212\u03b5pca\n(1 \u2212\u221a\u03b5pca)2 (1 \u2212\u03b4\u00d7/2)\u2225Mk\u22121\u22252 \u2265(1 \u2212\u03b4\u00d7)\u2225Mk\u22121\u22252 .\n(E.2)\nAbove, x is because vk is the projection of v\u2032\nk into V \u22a5\nk\u22121, y is because \u2225V \u22a4\nk\u22121v\u2032\nk\u22252\n2 \u2264\u03b5pca following\nthe same reason as (E.1), and z is owing to Theorem 3.1. Next, since \u2225Mk\u22121\u22252 \u2265|\u03bbk|, we\nautomatically have |v\u22a4\nk Mvk| \u2265(1 \u2212\u03b4\u00d7)|\u03bbk|. On the other hand, |v\u22a4\nk Mvk| = |v\u22a4\nk Mk\u22121vk| \u2264\n\u2225Mk\u22121\u22252 \u2264\n|\u03bbk|\n1\u2212\u03b4\u00d7 where the last inequality is owing to Theorem 4.1.b.\n(d) Since \u2225V \u22a4\nk U\u22252 \u2264\u03b5c\ndef\n= 8(|\u03bb1|/|\u03bbk|)1/2k\u03b51/4\npca\n\u03b41/2\n\u00d7\nfrom the analysis of Theorem 4.1.a, we can apply\nLemma C.2 to obtain a (column) orthogonal matrix Qk \u2208Rd\u00d7k such that\n\u2225M\u2032\nk \u2212Mk\u22252 \u226416|\u03bb1|\u03b5c,\nwhere M\u2032\nk\ndef\n= (I \u2212QkQ\u22a4\nk )M(I \u2212QkQ\u22a4\nk )\n(E.3)\nSuppose U = (ud\u2212p+1, . . . , ud) is of dimension d \u00d7 p, that is, there are exactly p eigenvalues of\nM whose absolute value is \u2264(1 \u2212\u03b4\u00d7)\u2225Mk\u22121\u22252. Then, the de\ufb01nition of Qk in Lemma C.2 tells\nus U \u22a4Qk = 0 so M\u2032\nk agrees with M on all the eigenvalues and eigenvectors {(\u03bbj, uj)}d\nj=d\u2212p+1\nbecause an index j satis\ufb01es |\u03bbj| \u2264(1\u2212\u03b4\u00d7)\u2225Mk\u22121\u22252 if and only if j \u2208{d\u2212p+1, d\u2212p+2, . . . , d}.\nDenote by \u00b51, . . . , \u00b5d\u2212k the eigenvalues of M\u2032\nk excluding the k zero eigenvalues in subspace Qk,\nand assume without loss of generality that {\u00b51, . . . , \u00b5p} = {\u03bbd\u2212p+1, . . . , \u03bbd}. Then,\n\u2225M\u2032\nk\u2225q\nSq =\nd\u2212k\nX\ni=1\n|\u00b5i|q =\np\nX\ni=1\n|\u00b5i|q +\nd\u2212k\nX\ni=p+1\n|\u00b5i|q =\nd\nX\ni=d\u2212p+1\n|\u03bbi|q +\nd\u2212k\nX\ni=p+1\n|\u00b5i|q\nx\n\u2264\nd\nX\ni=d\u2212p+1\n|\u03bbi|q + (d \u2212k \u2212p)\u2225M\u2032\nk\u2225q\n2\ny\n\u2264\nd\nX\ni=d\u2212p+1\n|\u03bbi|q + (d \u2212k \u2212p)(\u2225Mk\u22252 + 16|\u03bb1|\u03b5c)q\n27\nz\n\u2264\nd\nX\ni=d\u2212p+1\n|\u03bbi|q + (d \u2212k \u2212p)\n\u0010 |\u03bbk+1|\n(1 \u2212\u03b4\u00d7) + 16|\u03bb1|\u03b5c\n\u0011q\nAbove, x is because each |\u00b5i| is no greater than \u2225M\u2032\nk\u22252, and y is owing to (E.3), and z is\nbecause of Theorem 4.1.b. Suppose we choose \u03b5c so that \u03b5c \u2264|\u03bbk+1|\u03b4\u00d7\n16\u03bb1\n(and this is indeed\nsatis\ufb01ed by our assumption on \u03b5pca), then we can continue and write\n\u2225M\u2032\nk\u2225q\nSq \u2264\nd\nX\ni=d\u2212p+1\n|\u03bbi|q + (d \u2212k \u2212p)(1 + \u03b4\u00d7)q\n(1 \u2212\u03b4\u00d7)q |\u03bbk+1|q\n{\n\u2264\nd\nX\ni=d\u2212p+1\n|\u03bbi|q + (1 + \u03b4\u00d7)q\n(1 \u2212\u03b4\u00d7)2q\nd\u2212p\nX\ni=k+1\n|\u03bbi|q \u2264(1 + \u03b4\u00d7)q\n(1 \u2212\u03b4\u00d7)2q\nd\nX\ni=k+1\n|\u03bbi|q .\nAbove, { is because for each eigenvalue \u03bbi where i \u2208{k + 1, k + 2, . . . , d \u2212p}, we have\n|\u03bbi| > (1 \u2212\u03b4\u00d7)\u2225Mk\u22121\u22252 \u2265(1 \u2212\u03b4\u00d7)|\u03bbk| \u2265(1 \u2212\u03b4\u00d7)|\u03bbk+1|. Finally, using (E.3) again we have\n\u2225Mk\u2225Sq \u2264\u2225M\u2032\nk\u2225Sq + \u2225Mk \u2212M\u2032\nk\u2225Sq \u2264\u2225M\u2032\nk\u2225Sq + d1/p\u2225Mk \u2212M\u2032\nk\u22252\n\u2264\n1 + \u03b4\u00d7\n(1 \u2212\u03b4\u00d7)2\n\u0010\nd\nX\ni=k+1\n|\u03bbi|q\u00111/q\n+ 16d1/p|\u03bb1|\u03b5c\nAs long as \u03b5c \u2264\u03b4\u00d7|\u03bbk+1|\n16d1/p\u03bb1 , we have\n\u2225Mk\u2225Sq \u2264(1 + \u03b4\u00d7)2\n(1 \u2212\u03b4\u00d7)2\n\u0010\nd\nX\ni=k+1\n|\u03bbi|q\u00111/q\nas desired. Finally, we note that \u03b5c \u2264\n\u03b4\u00d7\u03bbk+1\n16d1/p\u03bb1 is satis\ufb01ed with our assumption on \u03b5pca, and\nnote that minV \u2208Rd\u00d7k,V \u22a4V =I\n\b\n\u2225(I \u2212V V \u22a4)M(I \u2212V V \u22a4)\u2225Sq\n\t\n=\n\u0000 Pd\ni=k+1 |\u03bbi|q\u00011/q which follows\neasily from Cauchy interlacing theorem.\n\u25a1\nE.2\nProofs of Theorems 4.3 and 4.4\nProof of Theorem 4.3. De\ufb01ne Vk = B1/2Vk = LazyEV(\u00b7 \u00b7 \u00b7 ) to be the direct output of LazyEV. The\ncolumn orthogonality of Vk implies V\u22a4\nk BVk = I.\nIt is clear from the de\ufb01nition of generalized eigenvectors that B1/2u1, . . . , B1/2ud are eigenvectors\nof M\ndef\n= B\u22121/2AB\u22121/2 with eigenvalues \u03bb1, . . . , \u03bbd. Applying Theorem 4.1.a, we have: \u2225V \u22a4\nk U\u22252 \u2264\u03b5\nwhere U = (B1/2uj, . . . , B1/2ud) is a (column) orthonormal matrix and j is the smallest index\nsatisfying |\u03bbj| \u2264(1 \u2212\u03b4\u00d7)\u2225Mk\u22121\u22252. Since it satis\ufb01es \u2225Mk\u22121\u22252 \u2265|\u03bbk|, we have\n|\u03bbk+1| = |\u03bbk|(1 \u2212gap) = |\u03bbk|(1 \u2212\u03b4\u00d7) \u2264(1 \u2212\u03b4\u00d7)\u2225Mk\u22121\u22252 .\nTherefore, j must be equal to k+1 according to its de\ufb01nition, so we have U = B1/2W. This implies\n\u2225V\u22a4\nk BW\u22252 = \u2225V \u22a4\nk U\u22252 \u2264\u03b5.\nThe running time statement comes directly from Theorem 4.2 by putting in the parameters. \u25a1\nProof of Theorem 4.4. De\ufb01ne Vk = B1/2Vk = LazyEV(\u00b7 \u00b7 \u00b7 ) to be the direct output of LazyEV. The\ncolumn orthogonality of Vk implies V\u22a4\nk BVk = I.\nIt is clear from the de\ufb01nition of generalized eigenvectors that B1/2u1, . . . , B1/2ud are eigenvectors\nof M\ndef\n= B\u22121/2AB\u22121/2 with eigenvalues \u03bb1, . . . , \u03bbd.\nApplying Theorem 4.1.b, we have:\n\r\r(I \u2212\nVkV \u22a4\nk )B\u22121/2AB\u22121/2(I \u2212VkV \u22a4\nk )\n\r\r\n2 \u2264|\u03bbk+1|\n1\u2212\u03b5 . Next, for every vector w \u2208Rd that is B-orthogonal to\n28\nVk, that is, w\u22a4BVk = 0, we can de\ufb01ne w\ndef\n= B1/2w and we know w is orthogonal to Vk. We can\napply the above spectral upper bound and get\nw\u22a4Aw = w\u22a4B\u22121/2AB\u22121/2w = w\u22a4(I \u2212VkV \u22a4\nk )B\u22121/2AB\u22121/2(I \u2212VkV \u22a4\nk )w\n\u2264\u2225w\u22252\n2 \u00b7 |\u03bbk+1|\n1 \u2212\u03b5 = w\u22a4Bw \u00b7 |\u03bbk+1|\n1 \u2212\u03b5\nas desired. At the same time, denoting by vs = B1/2vs, Theorem 4.1.c implies that\n\u2200s \u2208[k]:\n\f\fv\u22a4\ns Avs\n\f\f =\n\f\fv\u22a4\ns Mvs\n\f\f \u2208\nh\n(1 \u2212\u03b5)|\u03bbs|, |\u03bbs|\n1 \u2212\u03b5\ni\n.\nThe running time statement comes directly from Theorem 4.2 by putting in the parameters. \u25a1\nF\nProof for Section 6: CCA Theorems\nF.1\nThe Main Convergence Theorem\nSince LazyCCA only admits minor changes on top of LazyEV, the next theorem is an almost identical\ncopy of Theorem 4.1. To make this paper concise, instead of reproving Theorem F.1 line by line,\nwe here only sketch the main changes needed in the new proof.\nTheorem F.1.\nLet M = B\u22121/2AB\u22121/2 \u2208Rd\u00d7d be a symmetric matrix where A and B are\nmatrices coming from a CCA instance using Lemma 2.3. Suppose M has eigenvalues \u03bb1, . . . , \u03bbd \u2208\n[\u22121, 1] and corresponding eigenvectors u1, . . . , ud. Suppose without loss of generality that |\u03bb1| \u2265\n\u00b7 \u00b7 \u00b7 \u2265|\u03bbd|.\nFor every k \u2208[d], \u03b4\u00d7, p \u2208(0, 1), there exists some \u03b5pca \u2264O\n\u0000poly(\u03b4\u00d7,\n|\u03bb1|\n|\u03bbk+1 , 1\nd)\n\u0001\nsuch that\nLazyCCA outputs a (column) orthonormal matrix Vk = (v1, . . . , v2k) \u2208Rd\u00d72k which, with probabil-\nity at least 1\u2212p, satis\ufb01es all of the following properties. (Denote by Ms = (I\u2212VsV \u22a4\ns )M(I\u2212VsV \u22a4\ns ).)\n(a) Correlation guarantee: \u2225V \u22a4\nk U\u22252 \u2264\u03b5,\nwhere U = (uj, . . . , ud) and j is the smallest index satisfying |\u03bbj| \u2264(1 \u2212\u03b4\u00d7)\u2225Mk\u22121\u22252.\n(b) Spectral norm guarantee: |\u03bb2k+1| \u2264\u2225Mk\u22252 \u2264|\u03bb2k+1|\n1\u2212\u03b4\u00d7 .\n(c) Rayleigh quotient guarantee: (1 \u2212\u03b4\u00d7)|\u03bb2k| \u2264|v\u22a4\n2k\u22121Mv2k\u22121| = |v\u22a4\n2kMv2k| \u2264\n1\n1\u2212\u03b4\u00d7 |\u03bb2k|.\n(d) Schatten-q norm guarantee: for every q \u22651,\n\u2225Mk\u2225Sq \u2264(1 + \u03b4\u00d7)2\n(1 \u2212\u03b4\u00d7)2\n\u0010\nd\nX\ni=2k+1\n\u03bbq\ni\n\u00111/q\n= (1 + \u03b4\u00d7)2\n(1 \u2212\u03b4\u00d7)2\nmin\nV \u2208Rd\u00d72k,V \u22a4V =I\n\b\n\u2225(I\u2212V V \u22a4)M(I\u2212V V \u22a4)\u2225Sq\n\t\n.\nProof sketch of Theorem F.1. Recall that when a vector vs \u2208Rd is obtained in iteration s of LazyEV,\nthe proof of Theorem 4.1 suggest that the following two properties hold\n\f\fv\u22a4\ns Ms\u22121vs\n\f\f\f \u2265(1 \u2212\u03b4\u00d7)\u2225Ms\u22121\u22252\nand\n\r\rv\u22a4\ns Ws\u22121\n\r\r\r\n2\n2 \u22641.5\u03b5pca .\n(F.1)\n(The \ufb01rst property is shown in (E.2), and the second property is shown in (E.1). Recall that Ws\u22121\nis the column orthonormal matrix containing all eigenvectors of Ms\u22121 whose absolute eigenvalues\nare below some threshold.) Then, the proof of Theorem 4.1 proceeds by heavily relying on (F.1).\nIn our LazyCCA, after obtaining this same vector vs, we write it as vs = (\u03be\u2032\ns, \u03b6\u2032\nv) and perform\nblock-scaling \u03bes = \u03be\u2032\ns/(\n\u221a\n2\u2225\u03be\u2032\ns\u2225) and \u03b6s = \u03b6\u2032\ns/(\n\u221a\n2\u2225\u03b6\u2032\ns\u2225), see Line 7 of LazyCCA. Therefore, in order\nfor the same proof of Theorem 4.1 to hold, we need to show that this new vector (\u03bes, \u03b6s) satis\ufb01es\n29\nthe same properties up to constants:\n\f\f\f\n\u0010 \u03bes\n\u03b6s\n\u0011\u22a4\nMs\u22121\n\u0010 \u03bes\n\u03b6s\n\u0011\f\f\f \u2265(1 \u2212\u03b4\u00d7)\u2225Ms\u22121\u22252\nand\n\r\r\r\n\u0010 \u03bes\n\u03b6s\n\u0011\u22a4\nWs\u22121\n\r\r\r\n2\n2 \u226412.5\u03b5pca .\n(F.2)\nSuppose Vs\u22121 =\n\u0010 \u00b1\u03be1\n\u00b7 \u00b7 \u00b7\n\u00b1\u03bes\u22121\n\u03b61\n\u00b7 \u00b7 \u00b7\n\u03b6s\u22121\n\u0011\n. Since vs is orthogonal to all vectors in the column span of\nVs\u22121 according to Line 5 of LazyCCA, we automatically have \u03be\u22a4\ns \u03bei = 0 and \u03b6\u22a4\ns \u03b6i for all i \u2208[s \u22121].\nWe also have \u2225\u03bes\u22252 + \u2225\u03b6s\u22252 = 1/2 + 1/2 = 1 so the new vector (\u03bes, \u03b6s) has Euclidean norm 1.\nAs for the \ufb01rst property in (F.2), we observe that the new vector (\u03bes, \u03b6s) enjoys an (absolute)\nRayleigh quotient value that is no worse than the original vs = (\u03be\u2032\ns, \u03b6\u2032\ns). This is so because (without\nloss of generality we consider v\u22a4\ns Ms\u22121vs > 0):\n\u0010 \u03bes\n\u03b6s\n\u0011\u22a4\nMs\u22121\n\u0010 \u03bes\n\u03b6s\n\u0011 x=\n\u0010 \u03bes\n\u03b6s\n\u0011\u22a4\nM\n\u0010 \u03bes\n\u03b6s\n\u0011 y= 2\u03be\u22a4\ns\n\u0000S\u22121/2\nxx\nSxyS\u22121/2\nyy\n\u0001\n\u03b6s\nz=\n1\n\u2225\u03be\u2032s\u2225\u2225\u03b6\u2032s\u2225\u00b7\u03be\u2032\u22a4\ns\n\u0000S\u22121/2\nxx\nSxyS\u22121/2\nyy\n\u0001\n\u03b6\u2032\ns\n{=\n1\n2\u2225\u03be\u2032s\u2225\u2225\u03b6\u2032s\u2225\u00b7v\u22a4\ns Mvs\n|=\n1\n2\u2225\u03be\u2032s\u2225\u2225\u03b6\u2032s\u2225\u00b7v\u22a4\ns Ms\u22121vs\n}\n\u2265v\u22a4\ns Ms\u22121vs .\n(F.3)\nAbove, x is because (\u03bes, \u03b6s) is orthogonal to Vs\u22121; y is by the de\ufb01nition of M = B\u22121/2AB\u22121/2 as\nwell as the de\ufb01nition of A and B; z is by the de\ufb01nitions of \u03bes and \u03b6s; { is by vs = (\u03be\u2032\ns, \u03b6\u2032\ns) and\nagain by the de\ufb01nition of M; | follows from the fact that vs is orthogonal to Vs\u22121; and } follows\nfrom AM-GM together with the fact that \u2225\u03be\u2032\ns\u22252 + \u2225\u03b6\u2032\ns\u22252 = \u2225vs\u22252 = 1. This \ufb01nishes proving the \ufb01rst\nproperty in (F.2) because the original vector vs satis\ufb01es |v\u22a4\ns Ms\u22121vs| \u2265(1 \u2212\u03b4\u00d7)\u2225Ms\u22121\u22252 according\nto (F.1).\nWe make an additional observation here: \u2225\u03be\u2032\ns\u22252 and \u2225\u03b6\u2032\ns\u22252 must be in the range [0.06, 0.94] before\nscaling. Indeed, suppose for instance \u2225\u03be\u2032\ns\u22252 = c for some c \u2208[0, 1]. Then, it satis\ufb01es 2\u2225\u03be\u2032\ns\u2225\u2225\u03b6\u2032\ns\u2225=\n2\np\nc(1 \u2212c) and therefore (F.3) becomes\n\u0010 \u03bes\n\u03b6s\n\u0011\u22a4\nMs\u22121\n\u0010 \u03bes\n\u03b6s\n\u0011\n\u2265\n1\u2212\u03b4\u00d7\n2\u221a\nc(1\u2212c)\u2225Ms\u22121\u22252, meaning that\n1\u2212\u03b4\u00d7\n2\u221a\nc(1\u2212c) \u22641. If \u03b4\u00d7 \u22641/2, this implies c \u22121/2 \u2208[\u2212\n\u221a\n3/4,\n\u221a\n3, 4] and thus c \u2208[0.06, 0.94].\nAs for the second property in (F.2), for every matrix Ws\u22121 that is in the proof of Theorem 4.1.a,\nits columns are all eigenvectors of Ms\u22121 whose absolute eigenvalues are below some threshold, so\nmust consist of only symmetric vectors in this CCA setting: that is, Ws\u22121 =\n\u0010 \u00b1a1\n. . .\n\u00b1at\nb1\n. . .\nbt\n\u0011\n.11\nAccording to (F.1) we already know \u2225v\u22a4\ns Ws\u22121\u22252 \u22641.5\u03b5pca, which implies\n\u2225v\u22a4\ns Ws\u22121\u22252 =\nt\nX\ni=1\n(\u03be\u2032\u22a4\ns ai+\u03b6\u2032\u22a4\ns bi)2+(\u03be\u2032\u22a4\ns ai\u2212\u03b6\u2032\u22a4\ns bi)2 = 2\u2225\u03be\u2032\u22a4\ns (a1, . . . , at)\u22252+2\u2225\u03b6\u2032\u22a4\ns (b1, . . . , bt)\u22252 \u22641.5\u03b5pca .\nNow we can compute\n\r\r\r\n\u0010 \u03bes\n\u03b6s\n\u0011\u22a4\nWs\u22121\n\r\r\r\n2\n2 = 2\u2225\u03be\u22a4\ns (a1, . . . , at)\u22252 + 2\u2225\u03b6\u22a4\ns (b1, . . . , bt)\u22252\n\u22640.5\n0.06 \u00b7\n\u00002\u2225\u03be\u2032\u22a4\ns (a1, . . . , at)\u22252 + 2\u2225\u03b6\u2032\u22a4\ns (b1, . . . , bt)\u22252\u0001\n\u226412.5\u03b5pca ,\n11This is because matrix Ms is always of the form D\u22121/2CD1/2 where D = diag{D1, D2} is block diagonal and\npositive de\ufb01nite, while C = [[0, C1]; [C\u22a4\n1 , 0]] has only zero on its two block diagonal locations. The same proof of\nLemma 2.3 shows that the eigenvectors of D\u22121/2CD1/2 must be symmetric. In fact, to be precise, Ws may also\ncontain some eigenvectors corresponding to zero eigenvalues. However, adding them will make our notations heavier,\nso we refrain from doing that in this sketched proof.\n30\nwhere the \ufb01rst inequality is because \u2225\u03be\u2032\ns\u22252, \u2225\u03b6\u2032\ns\u22252 \u22650.06.\nThis \ufb01nishes proving the two properties in (F.2), so Theorem F.1 holds after plugging the rest\nof the proof of Theorem 4.1 in but changing constants slightly.\n\u25a1\nF.2\nFast Implementation of LazyCCA: Stochastic\nTheorem F.2 (stochastic running time of LazyCCA).\nLet X \u2208Rn\u00d7dx, Y \u2208Rn\u00d7dy be two matrices,\nand de\ufb01ne A and B according to Lemma 2.3. Suppose M = B\u22121/2AB\u22121/2, and RanInit(d) is\nthe random vector generator de\ufb01ned in Proposition 3.3, and we want to compute matrix V \u2190\nB\u22121/2LazyCCA(A, M, k, \u03b4\u00d7, \u03b5pca, p). Then, this procedure can be implemented to run in time\n\u2022 eO\n\u0010\nknnz(B)+k2d+k\u03a5\n\u221a\n\u03b4\u00d7\n\u0011\nwhere \u03a5 is the time needed to compute B\u22121Aw for a vector w to an\naccuracy \u03b5 where log(1/\u03b5) = eO(1), or\n\u2022 eO\n\u0010 knnz(X,Y )\n\u00001+\u221a\n\u03ba\u2032/n\n\u0001\n+k2d\n\u221a\n\u03b4\u00d7\n\u0011\nif we simply use Katyusha to compute B\u22121Aw.\nAbove, \u03ba = \u03bbmax(B)\n\u03bbmin(B) = max{\u03bbmax(Sxx),\u03bbmax(Syy)}\nmin{\u03bbmin(Sxx),\u03bbmin(Syy)} , and \u03ba\u2032 =\n2 maxi\u2208[n]{\u2225Xi\u22252,\u2225Yi\u22252}\n\u03bbmin(B)\n\u2208[\u03ba, 2n\u03ba].\nProof. The proof of the \ufb01rst item is almost identical to the proof of Theorem 4.2 so ignored here.\nAs for the second item, it follows from the \ufb01rst item together with the running time of Katyusha\nin Lemma 2.6.\n\u25a1\nF.3\nFast Implementation of LazyCCA: Doubly Stochastic\nTheorem F.3 (doubly-stochastic running time of LazyCCA).\nLet X \u2208Rn\u00d7dx, Y \u2208Rn\u00d7dy be\ntwo matrices, and de\ufb01ne A and B according to Lemma 2.3. Suppose M = B\u22121/2AB\u22121/2, and\nRanInit(d) is the random vector generator de\ufb01ned in Proposition 3.3, and we want to compute\nV \u2190B\u22121/2LazyCCA(A, M, k, \u03b4\u00d7, \u03b5pca, p). Then, this procedure can be implemented to run in time\n\u2022 eO\n\u0010\nnnz(X, Y ) \u00b7\n\u00001 +\n\u221a\n\u03ba\u2032/n1/4\n\u221a\n\u03b4\u00d7\u03c31\n\u0001\u0011\nif k = 1 and we use accelerated SVRG as the method A;\n\u2022 eO\n\u0010\nknnz(X, Y ) \u00b7\n\u00001 +\n\u221a\n\u03ba\u2032\n\u221a\n\u03b4\u00d7\u03c3k(nnz(X,Y )/kd)1/4\n\u0001\u0011\nif nnz(X, Y ) \u2265kd and we use accelerated SVRG\nas the method A.\nAbove, \u03ba = \u03bbmax(B)\n\u03bbmin(B) = max{\u03bbmax(Sxx),\u03bbmax(Syy)}\nmin{\u03bbmin(Sxx),\u03bbmin(Syy)} , and \u03ba\u2032 =\n2 maxi\u2208[n]{\u2225Xi\u22252,\u2225Yi\u22252}\n\u03bbmin(B)\n\u2208[\u03ba, 2n\u03ba].\nTo prove Theorem F.3, let us recall from the proof of Theorem 4.2 (see Section 5.2) that it\nsu\ufb03ces to bound the time needed to compute N\u22121w where\nN\ndef\n= B\u22121/2\u0000\u03bbI \u2212Ms\n\u0001\nB1/2 = \u03bbI \u2212(I \u2212VsV\u22a4\ns B)B\u22121A(I \u2212VsV\u22a4\ns B) ,\nand we only need to compute it poly-logarithmic number of times for each s = 1, . . . , k.\nObserve now that N\u22121w = (BN)\u22121Bw, so it su\ufb03ces to bound the time needed to compute\n(BN)\u22121 applied to a vector, and we have (dropping the subscript of V for cleanness)\nBN = \u03bbB \u2212(I \u2212BVV\u22a4)A(I \u2212VV\u22a4B) .\nF.3.1\nSpecial Case of 1-CCA\nWhen k = 1, the matrix V is empty and we want to compute (\u03bbB \u2212A)\u22121w for some vector w.\nThis is equivalent to minimizing f(z)\ndef\n= 1\n2z\u22a4(\u03bbB \u2212A)z \u2212w\u22a4z. We analyze its running time as\nfollows, and a similar analysis has also appeared in [29, Section 3.2.3].\n31\nUsing the de\ufb01nition of A and B, we can write f(z) = 1\nn\nPn\ni=1 fi(z) where\nfi(z)\ndef\n= 1\n2\u03bb(\u27e8Xi, z1\u27e92 + \u27e8Yi, z2\u27e92) \u22122\u27e8Xi, z1\u27e9\u27e8Yi, z2\u27e9+ w\u22a4z ,\nwhere we have denoted by z = (z1, z2) for z1 \u2208Rdx and z2 \u2208Rdy, and by Xi, Yi the i-th row vector\nof matrix X and Y respectively. The smoothness of fi(z) is given by (using the fact that \u03bb \u22642\nwhich comes from the de\ufb01nition of AppxPCA\u00b1)\n\u2225\u22072fi(z)\u22252 \u2264O(1) \u00b7 max{\u2225Xi\u22252, \u2225Yi\u22252} .\nThe strong convexity of f(z) is given by\n\u22072f(z) = B1/2(\u03bbI \u2212B\u22121/2AB\u22121/2)B1/2 \u2ab0B1/2\u0000\u03b4\u00d7\u03c31\n48 I)B1/2 \u2ab0\u03c31\u03b4\u00d7\u03bbmin(B)\n48\n\u00b7 I ,\nwhere the \ufb01rst inequality is because \u03bbmin(\u03bbI \u2212B\u22121/2AB\u22121/2) \u2265\u03b4\u00d7\u03bb\u2217\n48\nfrom Theorem 3.1, as well as\n\u03bb\u2217\u2265\u03c31 = \u03bbmax(B\u22121/2AB\u22121/2) from the de\ufb01nition of the algorithm.\nTherefore, by applying the accelerated SVRG method on minimizing the sum-of-non-convex\nobjective f(z) [7, 24], we can \ufb01nd an e\u03b5 minimizer of f(z) in time\neO\n\u0010\nnnz(X, Y )\u00b7\n\u0010\n1+\np\nmaxi{\u2225Xi\u22252, \u2225Yi\u22252}\np\n\u03c31\u03b4\u00d7\u03bbmin(B)n1/4\n\u0011\n\u00b7log2 \u2225w\u2225\ne\u03b5\n\u0011\n= eO\n\u0010\nnnz(X, Y )\u00b7\n\u00001+\n\u221a\n\u03ba\u2032\np\n\u03c31\u03b4\u00d7n1/4\n\u0001\n\u00b7log2 \u2225w\u2225\ne\u03b5\n\u0011\n.\nFinally, similar to the proof of Lemma 2.6, an e\u03b5 minimizer of f(z) implies\n\r\r(\u03bbB \u2212A)\u22121w \u2212z\n\r\r \u2264\u03b5\nwhen \u03b52 = 96e\u03b5/(\u03c31\u03b4\u00d7\u03bbmin(B)). Putting this back we obtain a \ufb01nal running time of eO\n\u0010\nnnz(X, Y ) \u00b7\n\u00001 +\n\u221a\n\u03ba\u2032\n\u221a\n\u03c31\u03b4\u00d7n1/4\n\u0001\u0011\n, ignoring log factors.\nF.3.2\nGenearl Case of k-CCA\nFor the general case when V is not empty, one can carefully check that to compute\n(\u03bbB \u2212(I \u2212BVV\u22a4)A(I \u2212VV\u22a4B))\u22121w = (\u03bbB \u2212A + (BVV\u22a4A + AVV\u22a4B \u2212BVV\u22a4AVV\u22a4B)\u22121w\nit su\ufb03ces to minimize f\u2032(z) = 1\nn\nPn\ni=1 f\u2032\ni(z) for\nf\u2032\ni(z)\ndef\n= fi(z) + 1\n2z\u22a4Qz\nwhere\nQ\ndef\n= BVV\u22a4A + AVV\u22a4B \u2212BVV\u22a4AVV\u22a4B ,\nand fi(z) is the same as de\ufb01ned in the k = 1 case of the previous subsection.\nClaim F.4.\n\u2225\u22072f\u2032\ni(z)\u22252 \u2264O(1) \u00b7 max{\u2225Xi\u22252, \u2225Yi\u22252}\nand\n\u22072f\u2032(z) \u2ab0\u03c3k\u03b4\u00d7\u03bbmin(B)\n48\n\u00b7 I .\nProof. Since\n\u2225BV\u22252 \u2264\u2225B1/2\u22252\u2225B\u22121/2V \u22252 \u2264\u2225B\u22251/2\n2\n\u2225AV\u22252 \u2264\u2225B1/2\u22252\u2225B\u22121/2AB\u22121/2\u22252\u2225B1/2V\u22252 \u2264\u2225B\u22251/2\n2\n\u03c31\n\u2225V\u22a4AV\u22a4\u22252 \u2264\u2225V\u22a4B1/2\u22252\u2225B\u22121/2AB\u22121/2\u22252\u2225B1/2V\u22252 \u2264\u03c31\nwe can easily compute that \u2225Q\u22252 \u2264O(1) \u00b7 \u2225B\u22252 \u2264O(1) \u00b7 Tr(B) \u2264O(1) \u00b7 maxi{\u2225Xi\u22252, \u2225Yi\u22252}.\nTherefore, we have the same smoothness property as in the previous subsection:\n\u2225\u22072f\u2032\ni(z)\u22252 \u2264O(1) \u00b7 max{\u2225Xi\u22252, \u2225Yi\u22252} .\nThe strong convexity of f\u2032(z) is given by\n\u22072f\u2032(z) = B1/2(\u03bbI \u2212Ms)B1/2 \u2ab0B1/2\u0000\u03b4\u00d7\u03c3k\n48 I)B1/2 \u2ab0\u03c3k\u03b4\u00d7\u03bbmin(B)\n48\n\u00b7 I ,\n32\nwhere the \ufb01rst inequality is because (1) \u03bbmin(\u03bbI\u2212Ms) \u2265\u03b4\u00d7\u03bb\u2217\n48\nfrom Theorem 3.1, (2) \u03bb\u2217\u2265\u03bbmax(Ms)\nfrom the de\ufb01nition of the algorithm, as well as (3) \u03bbmax(Ms) \u2265\u03c3k from the fact that Ms is exactly\nB\u22121/2AB\u22121/2 but projecting out at most k \u22121 dimensions.\n\u25a1\nClaim F.5.\nWith a preprocessing time O(nnz(X, Y )), we can compute (\u2207f\u2032\ni(z))w for an arbitrary\nvector w and an arbitrary index i \u2208[n] in time O(kd), and (\u2207f\u2032(z))w in time O(nnz(X, Y ) + kd).\nProof. Note that (\u2207fi(z))w only costs running time O(d) and thus it su\ufb03ces to focus on the\ncomputation of Qw. We \ufb01rst note that one can pre-compute AV, BV in time O(nnz(X, Y )) with\nthe help from the previous iteration. This is so because, suppose V = [V\u2032, v] where v is the new\ncolumn vector added in the current iteration. Then, AV = [AV\u2032, Av] and this additional column\nAv can be computed in time O(nnz(X, Y )). Next, we write\nQ =\n\u0000BV\n\u0001\u0000V\u22a4A\n\u0001\n+\n\u0000AV\n\u0001\u0000V\u22a4B\n\u0001\n\u2212\n\u0000BV\n\u0001\u0000V\u22a4A\n\u0001\nV\n\u0000V\u22a4B\n\u0001\nand every matrix between parenthesis of the above formulation has either only O(k) rows or O(k)\ncolumns. Therefore, computing Qw costs a total running time at most O(kd) and so is (\u2207f\u2032\ni(z))w.\nFinally, we have (\u2207f\u2032(z))w = (\u03bbB \u2212A)w + Qw and therefore computing (\u2207f\u2032(z))w costs time\nO(nnz(X, Y ) + kd).\n\u25a1\nSince computing (\u2207f\u2032\ni(z))w requires time O(kd) and computing (\u2207f\u2032(z))w requires time O(nnz(X, Y )+\nkd), we can apply SVRG to minimize this sum-of-non-convex function f\u2032(z) [7, 24], with a running\ntime\neO\n\u0010\u0000nnz(X, Y ) + kd\n\u0001\n+ (max{\u2225Xi\u22252, \u2225Yi\u22252})2\n\u03b42\n\u00d7\u03c32\nk\u03bbmin(B)2\nkd\n\u0011\u0011\n= eO\n\u0010\nnnz(X, Y ) + (\u03ba\u2032)2\n\u03b42\n\u00d7\u03c32\nk\nkd\n\u0011\n.\nNow, as long as nnz(X, Y ) \u2265kd, we can apply acceleration on top of SVRG [7, 24] to given an\naccelerated running time\neO\n\u0010\nnnz(X, Y ) \u00b7\n\u0010\n1 +\n\u221a\n\u03ba\u2032\n\u03b41/2\n\u00d7 \u03c31/2\nk\n(nnz(X, Y )/(kd))1/4\n\u0011\u0011\n.\nTaking into account all the iterations s = 1, . . . , k concludes the proof of Theorem F.3.\nF.4\nProofs of Theorems 6.2 and 6.3\nThe two corollaries follow from Theorem F.1 for the similar reason as Theorem 4.3 and Theorem 4.4\nfollowing from Theorem 4.1.\nProof Theorem 6.2. The approximation guarantees \u2225V\u22a4\n\u03c6 SxxW\u03c6\u22252 \u2264\u03b5 and \u2225V\u22a4\n\u03c8 SyyW\u03c8\u22252 \u2264\u03b5 fol-\nlow from Theorem F.1.a, and the running time follows from Theorem F.2 and Theorem F.3.\n\u25a1\nProof of Theorem 6.3. The approximation guarantees max\u03c6\u2208Rdx,\u03c8\u2208Rdy\nn\n\u03c6\u22a4Sxy\u03c8\n\f\f\f \u03c6\u22a4SxxV\u03c6 = 0 \u2227\n\u03c8\u22a4SyyV\u03c8 = 0\no\n\u2264(1 + \u03b5)\u03c3k+1 follow from Theorem F.1.b and the de\ufb01nition of M, and the approx-\nimation guarantee (1 \u2212\u03b5)\u03c3i \u2264|\u03c6\u2032\niSxy\u03c8i| \u2264(1 + \u03b5)\u03c3i follows from Theorem F.1.c and the fact that\n|\u03bb2i\u22121| = |\u03bb2i| = \u03c3i. The running time follows from Theorem F.2 and Theorem F.3.\n\u25a1\nReferences\n[1] Zeyuan Allen-Zhu. Katyusha: The First Direct Acceleration of Stochastic Gradient Methods.\nArXiv e-prints, abs/1603.05953, March 2016.\n33\n[2] Zeyuan Allen-Zhu and Yuanzhi Li. Even Faster SVD Decomposition Yet Without Agonizing\nPain. In NIPS, 2016.\n[3] Zeyuan Allen-Zhu and Yuanzhi Li. Fast Global Convergence of Online PCA. ArXiv e-prints,\nabs/1607.07837, July 2016.\n[4] Zeyuan Allen-Zhu and Yuanzhi Li. Faster Principal Component Regression via Optimal Poly-\nnomial Approximation to sgn(x). ArXiv e-prints, abs/1608.04773, August 2016.\n[5] Zeyuan Allen-Zhu and Lorenzo Orecchia. Linear Coupling: An Ultimate Uni\ufb01cation of Gra-\ndient and Mirror Descent.\nIn Proceedings of the 8th Innovations in Theoretical Computer\nScience, ITCS \u201917, 2017. Full version available at http://arxiv.org/abs/1407.1537.\n[6] Zeyuan Allen-Zhu, Peter Richt\u00b4arik, Zheng Qu, and Yang Yuan. Even faster accelerated coor-\ndinate descent using non-uniform sampling. In ICML, 2016.\n[7] Zeyuan Allen-Zhu and Yang Yuan. Improved SVRG for Non-Strongly-Convex or Sum-of-Non-\nConvex Objectives. In ICML, 2016.\n[8] Sanjeev Arora, Satish Rao, and Umesh V. Vazirani. Expander \ufb02ows, geometric embeddings\nand graph partitioning. Journal of the ACM, 56(2), 2009.\n[9] Owe Axelsson. A survey of preconditioned iterative methods for linear systems of algebraic\nequations. BIT Numerical Mathematics, 25(1):165\u2013187, 1985.\n[10] Kamalika Chaudhuri, Sham M Kakade, Karen Livescu, and Karthik Sridharan. Multi-view\nclustering via canonical correlation analysis. In ICML, pages 129\u2013136, 2009.\n[11] Paramveer Dhillon, Dean P Foster, and Lyle H Ungar. Multi-view learning of word embeddings\nvia cca. In NIPS, pages 199\u2013207, 2011.\n[12] Roy Frostig, Cameron Musco, Christopher Musco, and Aaron Sidford. Principal Component\nProjection Without Principal Component Analysis. In ICML, 2016.\n[13] Dan Garber and Elad Hazan. Fast and simple PCA via convex optimization. ArXiv e-prints,\nSeptember 2015.\n[14] Dan Garber, Elad Hazan, Chi Jin, Sham M. Kakade, Cameron Musco, Praneeth Netrapalli,\nand Aaron Sidford. Robust shift-and-invert preconditioning: Faster and more sample e\ufb03cient\nalgorithms for eigenvector computation. In ICML, 2016.\n[15] Rong Ge, Chi Jin, Sham M. Kakade, Praneeth Netrapalli, and Aaron Sidford. E\ufb03cient Algo-\nrithms for Large-scale Generalized Eigenvector Computation and Canonical Correlation Anal-\nysis. In ICML, 2016.\n[16] Gene H. Golub and Charles F. Van Loan. Matrix Computations. The JHU Press, 4th edition,\n2012.\n[17] Sham M Kakade and Dean P Foster. Multi-view regression via canonical correlation analysis.\nIn Learning theory, pages 82\u201396. Springer, 2007.\n[18] Nikos Karampatziakis and Paul Mineiro. Discriminative features via generalized eigenvectors.\nIn ICML, pages 494\u2013502, 2014.\n34\n[19] Yichao Lu and Dean P Foster. Large scale canonical correlation analysis with iterative least\nsquares. In NIPS, pages 91\u201399, 2014.\n[20] Zhuang Ma, Yichao Lu, and Dean Foster.\nFinding linear structure in large datasets with\nscalable canonical correlation analysis. In ICML, pages 169\u2013178, 2015.\n[21] Tomer Michaeli, Weiran Wang, and Karen Livescu. Nonparametric canonical correlation anal-\nysis. arXiv preprint, abs/1511.04839, 2015.\n[22] Cameron Musco and Christopher Musco. Randomized block krylov methods for stronger and\nfaster approximate singular value decomposition. In NIPS, pages 1396\u20131404, 2015.\n[23] Yurii Nesterov. A method of solving a convex programming problem with convergence rate\nO(1/k2). In Doklady AN SSSR (translated as Soviet Mathematics Doklady), volume 269, pages\n543\u2013547, 1983.\n[24] Shai Shalev-Shwartz. SDCA without Duality, Regularization, and Individual Convexity. In\nICML, 2016.\n[25] Shai Shalev-Shwartz and Tong Zhang. Accelerated Proximal Stochastic Dual Coordinate As-\ncent for Regularized Loss Minimization. In Proceedings of the 31st International Conference\non Machine Learning, ICML 2014, pages 64\u201372, 2014.\n[26] Ohad Shamir. A Stochastic PCA and SVD Algorithm with an Exponential Convergence Rate.\nIn ICML, pages 144\u2014-153, 2015.\n[27] Jonathan Richard Shewchuk. An introduction to the conjugate gradient method without the\nagonizing pain, 1994.\n[28] Weiran Wang and Karen Livescu. Large-scale approximate kernel canonical correlation anal-\nysis. arXiv preprint, abs/1511.04773, 2015.\n[29] Weiran Wang, Jialei Wang, Dan Garber, and Nathan Srebro. E\ufb03cient Globally Convergent\nStochastic Optimization for Canonical Correlation Analysis. In NIPS, 2016.\n[30] Daniela M Witten, Robert Tibshirani, and Trevor Hastie. A penalized matrix decomposition,\nwith applications to sparse principal components and canonical correlation analysis. Biostatis-\ntics, page kxp008, 2009.\n35\n",
        "sentence": " For the most efficient offline eigenvectors algorithms, we refer interested readers to our paper [5] (for PCA / SVD) and [4] (for CCA and generalized eigendecomposition).",
        "context": "[15] Rong Ge, Chi Jin, Sham M. Kakade, Praneeth Netrapalli, and Aaron Sidford. E\ufb03cient Algo-\nrithms for Large-scale Generalized Eigenvector Computation and Canonical Correlation Anal-\nysis. In ICML, 2016.\ngeneralized eigenvectors (k-GenEV) or top k canonical-correlation vectors (k-CCA). They designed\nan alternating minimization algorithm whose running time is only linear in terms of the number of\nDoubly Accelerated Methods for\nFaster CCA and Generalized Eigendecomposition\nZeyuan Allen-Zhu\nzeyuan@csail.mit.edu\nInstitute for Advanced Study\nYuanzhi Li\nyuanzhil@cs.princeton.edu\nPrinceton University\nJuly 20, 2016\u2217\nAbstract"
    },
    {
        "title": "Even Faster SVD Decomposition Yet Without Agonizing Pain",
        "author": [
            "Zeyuan Allen-Zhu",
            "Yuanzhi Li"
        ],
        "venue": "In NIPS,",
        "citeRegEx": "5",
        "shortCiteRegEx": "5",
        "year": 2016,
        "abstract": "",
        "full_text": "",
        "sentence": " For the most efficient offline eigenvectors algorithms, we refer interested readers to our paper [5] (for PCA / SVD) and [4] (for CCA and generalized eigendecomposition).",
        "context": null
    },
    {
        "title": "First Efficient Convergence for Streaming k-PCA: a Global, Gap- Free, and Near-Optimal Rate",
        "author": [
            "Zeyuan Allen-Zhu",
            "Yuanzhi Li"
        ],
        "venue": "ArXiv e-prints,",
        "citeRegEx": "6",
        "shortCiteRegEx": "6",
        "year": 2016,
        "abstract": "",
        "full_text": "",
        "sentence": " In the special case of Ak being rank-1, the \u00d5( \u221a T ) regret for Oja\u2019s algorithm was recently shown by [6], using different techniques from us. The stochastic online eigenvector problem is almost equivalent to the streaming PCA problem [6, 17]. The two papers [6, 17] use different techniques from ours and do not imply our result on stochastic online eigenvector. We prove this lower bound by reducing the problem to an information-theoretic lower bound that has appeared in our separate paper [6]. The lower bound in [6] states that, for every 1 \u2265 \u03bb \u2265 \u03bb2 \u2265 0, there exists a PSD matrix B with the largest two eigenvalues being \u03bb and \u03bb2, and a distribution D of rank-1 matrices with spectral norm at most 1 and expectation equal to D.",
        "context": null
    },
    {
        "title": "Spectral Sparsification and Regret Minimization Beyond Multiplicative Updates",
        "author": [
            "Zeyuan Allen-Zhu",
            "Zhenyu Liao",
            "Lorenzo Orecchia"
        ],
        "venue": "In Proceedings of the 47th Annual ACM Symposium on Theory of Computing,",
        "citeRegEx": "7",
        "shortCiteRegEx": "7",
        "year": 2015,
        "abstract": "",
        "full_text": "",
        "sentence": " Its natural matrix extension, commonly known as matrix multiplicative weight update (MMWU) [26], has been used towards efficient algorithms for solving semidefinite programs [3, 10, 28], balanced separators [27], Ramanujan sparsifiers [7, 22], and even in the proof of QIP = PSPACE [19]. Some authors also refer to MMWU as the follow-the-regularized-leader strategy or FTRL for short, because MMWU can be analyzed from a mirror-descent view with the matrix entropy function as its regularizer [7]. This more challenging setting is very desirable for multiple reasons: \u2022 in many applications \u2014such as graph problems [7, 22]\u2014 Ak does not depend on wk; \u2022 vector-based strategies wk can be cheaper to compute and more efficient to communicate. MMWU [7, 9] \u00d5( \u221a T ) O(d) \u00d5 ( d \u03b52 ) MMWU with JL [7, 28] \u00d5( \u221a T ) \u00d5 ( T 5 4 nnz(\u03a3) ) \u00d5 ( 1 \u03b54. 4 Some researchers [3, 7, 22, 28] use the Johnson-Lindenstrauss (JL) compression to reduce the dimension of Wk to make it more efficiently computable. MMWU [7, 9] \u00d5( \u221a \u03bbT ) O(d) \u00d5 ( \u03bbd \u03b52 ) MMWU with JL [7, 28] \u00d5( \u221a \u03bbT ) \u00d5 ( T 5 4 \u03bb\u2212 3 4 nnz(\u03a3) ) \u00d5 ( \u03bb \u03b54. In a recent result, the authors of [7] generalized MMWU to `1\u22121/q regularized strategies. We encourage interested readers to see the introduction of [7] for more background information, but we shall make this present paper self-contained. Prior work on MMWU and its extensions relies heavily on one of the following trace inequalities [7]: Golden-Thompson inequality : Tr(e) \u2264 Tr ( ee ) At a high level, issue (a) is not a big deal because if v\u2032 j satisfies \u2016vj \u2212 v\u2032 j\u20162 \u2264 \u03b5\u0303/poly(d, T ) and we use v\u2032 j instead of vj , then the final regret is affected by less than \u03b5\u0303; issue (b) can be dealt as long as we perform a careful binary search to find ck, similar to prior work [7]; issue (c) can be done as long as we have a good control on the condition number of the matrix ( ckI\u2212 \u03b7\u03a3k\u22121 ) . 1) like it was used in [7]. The authors of [7] have shown that the potential Tr(X 1\u22121/q k ) is robust against noise and a completely analogous (but lengthy) proof of theirs applies to this paper.",
        "context": null
    },
    {
        "title": "Improved SVRG for Non-Strongly-Convex or Sum-of-Non-Convex Objectives",
        "author": [
            "Zeyuan Allen-Zhu",
            "Yang Yuan"
        ],
        "venue": "In ICML,",
        "citeRegEx": "8",
        "shortCiteRegEx": "8",
        "year": 2016,
        "abstract": "",
        "full_text": "",
        "sentence": " Since \u2016\u2207fi(x)\u20162 \u2264 \u03b7k for each i, one can apply the SVRG method [8, 29] to minimize f(x) which gives running time \u00d5 ( nnz(\u03a3k\u22121) + (\u03b7k)2 maxi\u2208[k\u22121]{nnz(Ai)} ) .",
        "context": null
    },
    {
        "title": "The Multiplicative Weights Update Method: a Meta- Algorithm and Applications",
        "author": [
            "Sanjeev Arora",
            "Elad Hazan",
            "Satyen Kale"
        ],
        "venue": "Theory of Computing,",
        "citeRegEx": "9",
        "shortCiteRegEx": "9",
        "year": 2012,
        "abstract": "",
        "full_text": "",
        "sentence": " The multiplicative weight update (MWU) method is a simple but extremely powerful algorithmic tool that has been repeatedly discovered in theory of computation, machine learning, optimization, and game theory (see for instance the survey [9] and the book [12]). The best choice \u03b7 = \u221a log d/ \u221a T yields a total regret at most O( \u221a T log d) [26], and this is optimal up to constant [9]. MMWU [7, 9] \u00d5( \u221a T ) O(d) \u00d5 ( d \u03b52 ) MMWU [7, 9] \u00d5( \u221a \u03bbT ) O(d) \u00d5 ( \u03bbd \u03b52 )",
        "context": null
    },
    {
        "title": "A combinatorial, primal-dual approach to semidefinite programs",
        "author": [
            "Sanjeev Arora",
            "Satyen Kale"
        ],
        "venue": "In Proceedings of the thirty-ninth annual ACM symposium on Theory of computing - STOC",
        "citeRegEx": "10",
        "shortCiteRegEx": "10",
        "year": 2007,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Its natural matrix extension, commonly known as matrix multiplicative weight update (MMWU) [26], has been used towards efficient algorithms for solving semidefinite programs [3, 10, 28], balanced separators [27], Ramanujan sparsifiers [7, 22], and even in the proof of QIP = PSPACE [19].",
        "context": null
    },
    {
        "title": "Online principal components analysis",
        "author": [
            "Christos Boutsidis",
            "Dan Garber",
            "Zohar Karnin",
            "Edo Liberty"
        ],
        "venue": "In Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms,",
        "citeRegEx": "11",
        "shortCiteRegEx": "11",
        "year": 2015,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Some authors also study a very different online model for computing the top k eigenvectors[11, 20]: they are interested in outputting O(k \u00b7 poly(1/\u03b5)) vectors instead of k but with a good PCA reconstruction error.",
        "context": null
    },
    {
        "title": "Prediction, Learning, and Games",
        "author": [
            "Nicolo Cesa-Bianchi",
            "Gabor Lugosi"
        ],
        "venue": null,
        "citeRegEx": "12",
        "shortCiteRegEx": "12",
        "year": 2006,
        "abstract": "This important text and reference for researchers and students in machine learning, game theory, statistics and information theory offers a comprehensive treatment of the problem of predicting individual sequences. Unlike standard statistical approaches to forecasting, prediction of individual sequences does not impose any probabilistic assumption on the data-generating mechanism. Yet, prediction algorithms can be constructed that work well for all possible sequences, in the sense that their performance is always nearly as good as the best forecasting strategy in a given reference class. The central theme is the model of prediction using expert advice, a general framework within which many related problems can be cast and discussed. Repeated game playing, adaptive data compression, sequential investment in the stock market, sequential pattern analysis, and several other problems are viewed as instances of the experts' framework and analyzed from a common nonstochastic standpoint that often reveals new and intriguing connections.",
        "full_text": "",
        "sentence": " The multiplicative weight update (MWU) method is a simple but extremely powerful algorithmic tool that has been repeatedly discovered in theory of computation, machine learning, optimization, and game theory (see for instance the survey [9] and the book [12]).",
        "context": null
    },
    {
        "title": "Analyze gauss: optimal bounds for privacy-preserving principal component analysis",
        "author": [
            "Cynthia Dwork",
            "Kunal Talwar",
            "Abhradeep Thakurta",
            "Li Zhang"
        ],
        "venue": "In STOC,",
        "citeRegEx": "13",
        "shortCiteRegEx": "13",
        "year": 2014,
        "abstract": "",
        "full_text": "",
        "sentence": " If instead of playing an arbitrary matrix in \u2206d, the player is only allowed to play a rank-1 matrix Wk = wkw > k , then this online matrix optimization becomes the well-known online eigenvector problem [2, 13, 16, 21, 25]: Many researchers also analyzed the so-called followthe-perturbed-leader (FTPL) strategy for this problem [2, 13, 16, 21]. [13] and independently shown by Kot lowski and Warmuth [21].",
        "context": null
    },
    {
        "title": "Un-regularizing: approximate proximal point and faster stochastic algorithms for empirical risk minimization",
        "author": [
            "Roy Frostig",
            "Rong Ge",
            "Sham M. Kakade",
            "Aaron Sidford"
        ],
        "venue": "In ICML,",
        "citeRegEx": "14",
        "shortCiteRegEx": "14",
        "year": 2015,
        "abstract": "We develop a family of accelerated stochastic algorithms that minimize sums\nof convex functions. Our algorithms improve upon the fastest running time for\nempirical risk minimization (ERM), and in particular linear least-squares\nregression, across a wide range of problem settings. To achieve this, we\nestablish a framework based on the classical proximal point algorithm. Namely,\nwe provide several algorithms that reduce the minimization of a strongly convex\nfunction to approximate minimizations of regularizations of the function. Using\nthese results, we accelerate recent fast stochastic algorithms in a black-box\nfashion. Empirically, we demonstrate that the resulting algorithms exhibit\nnotions of stability that are advantageous in practice. Both in theory and in\npractice, the provided algorithms reap the computational benefits of adding a\nlarge strongly convex regularization term, without incurring a corresponding\nbias to the original problem.",
        "full_text": "Un-regularizing: approximate proximal point and faster stochastic\nalgorithms for empirical risk minimization\nRoy Frostig\u22171, Rong Ge2, Sham M. Kakade2, and Aaron Sidford3\n1Stanford University\n2Microsoft Research, New England\n3MIT\nAbstract\nWe develop a family of accelerated stochastic algorithms that minimize sums of convex\nfunctions. Our algorithms improve upon the fastest running time for empirical risk minimization\n(ERM), and in particular linear least-squares regression, across a wide range of problem settings.\nTo achieve this, we establish a framework based on the classical proximal point algorithm.\nNamely, we provide several algorithms that reduce the minimization of a strongly convex func-\ntion to approximate minimizations of regularizations of the function. Using these results, we\naccelerate recent fast stochastic algorithms in a black-box fashion.\nEmpirically, we demonstrate that the resulting algorithms exhibit notions of stability that\nare advantageous in practice. Both in theory and in practice, the provided algorithms reap the\ncomputational bene\ufb01ts of adding a large strongly convex regularization term, without incurring\na corresponding bias to the original problem.\n1\nIntroduction\nA general optimization problem central to machine learning is that of empirical risk minimization\n(ERM): \ufb01nding a predictor or regressor that minimizes a sum of loss functions de\ufb01ned by a data\nsample. We focus in part on the problem of empirical risk minimization of linear predictors: given\na set of n data points ai, . . . , an \u2208Rd and convex loss functions \u03c6i : R \u2192R for i = 1, . . . , n, solve\nmin\nx\u2208Rn F(x),\nwhere\nF(x) def\n=\nn\nX\ni=1\n\u03c6i(aT\ni x).\n(1)\nThis problem underlies supervised learning (e.g. the training of logistic regressors when \u03c6i(z) =\nlog(1 + e\u2212zbi), or their regularized form when \u03c6i(z) = log(1 + e\u2212zbi) + \u03b3\n2n\u2225x\u22252\n2 for a scalar \u03b3 > 0)\nand captures the widely-studied problem of linear least-squares regression when \u03c6i(z) = 1\n2(z \u2212bi)2.\nOver the past \ufb01ve years, problems such as (1) have received increased attention, with a recent\nburst of activity in the design of fast randomized algorithms. Iterative methods that randomly\n\u2217This is an extended and updated version of our conference paper that appeared in Proceedings of the 32nd\nInternational Conference on Machine Learning, Lille, France, 2015. JMLR: W&CP Volume 37.\nEmail: rf@cs.stanford.edu, rongge@microsoft.com, skakade@microsoft.com, sidford@mit.edu.\n1\narXiv:1506.07512v1  [stat.ML]  24 Jun 2015\nsample the \u03c6i have been shown to outperform standard \ufb01rst-order methods under mild assumptions\n(Bottou and Bousquet, 2008; Johnson and Zhang, 2013; Xiao and Zhang, 2014; Defazio et al., 2014;\nShalev-Shwartz and Zhang, 2014).\nDespite the breadth of these recent results, their running time guarantees when solving the\nERM problem (1) are sub-optimal in terms of their dependence on a natural notion of the prob-\nlem\u2019s condition number (See Section 1.1). This dependence can, however, signi\ufb01cantly impact their\nguarantees on running time. High-dimensional problems encountered in practice are often poorly\nconditioned. In large-scale machine learning applications, the condition number of the ERM prob-\nlem (1) captures notions of data complexity arising from variable correlation in high dimensions\nand is hence prone to be very large.\nMore speci\ufb01cally, among the recent randomized algorithms, each one either:\n1. Solves the ERM problem (1), under an assumption of strong convexity, with convergence\nthat depends linearly on the problem\u2019s condition number (Johnson and Zhang, 2013; Defazio\net al., 2014).\n2. Solves only an explicitly regularized ERM problem, minx{F(x) + \u03bbr(x)} where the regularizer\nr is a known 1-strongly convex function and \u03bb must be strictly positive, even when F is itself\nstrongly convex.\nOne such result is due to Shalev-Shwartz and Zhang (2014) and is the\n\ufb01rst to achieve acceleration for this problem, i.e. dependence only on the square root of the\nregularized problem\u2019s condition number, which scales inversely with \u03bb. Hence, taking small\n\u03bb to solve the ERM problem (where \u03bb = 0 in e\ufb00ect) is not a viable option.\nIn this paper we show how to bridge this gap via black-box reductions. Namely, we develop\nalgorithms to solve the ERM problem (1) \u2013 under a standard assumption of strong convexity \u2013\nthrough repeated, approximate minimizations of the regularized ERM problem minx{F(x) + \u03bbr(x)}\nfor fairly large \u03bb. Instantiating our framework with known randomized algorithms that solve the\nregularized ERM problem, we achieve accelerated running time guarantees for solving the original\nERM problem.\nThe key to our reductions are approximate variants of the classical proximal point algorithm\n(PPA) (Rockafellar, 1976; Parikh and Boyd, 2014). We show how both PPA and the inner minimiza-\ntion procedure can then be accelerated and our analysis gives precise approximation requirements\nfor either option. Furthermore, we show further practical improvements when the inner minimizer\noperates by a dual ascent method. In total, this provides at least three di\ufb00erent algorithms for\nachieving an improved accelerated running time for solving the ERM problem (1) under the stan-\ndard assumption of strongly convex F and smooth \u03c6i. (Table 1 summarizes our improvements in\ncomparison to existing minimization procedures.)\nPerhaps the strongest and most general theoretical reduction we provide in this paper is en-\ncompassed by the following theorem which we prove in Section 3.\nTheorem 1.1 (Accelerated Approximate Proximal Point Algorithm). Let f : Rn \u2192R be a \u00b5-\nstrongly convex function and suppose that, for all x0 \u2208Rn, c > 0, \u03bb > 0, we can compute a point\nxc (possibly random) such that\nEf(xc) \u2212min\nx\n\u001a\nf(x) + \u03bb\n2 \u2225x \u2212x0\u22252\n2\n\u001b\n\u22641\nc\n\u0014\nf(x0) \u2212min\nx\n\u001a\nf(x) + \u03bb\n2 \u2225x \u2212x0\u22252\n2\n\u001b\u0015\n2\nin time Tc. Then given any x0, c > 0, \u03bb \u22652\u00b5, we can compute x1 such that\nEf(x1) \u2212min\nx f(x) \u22641\nc\nh\nf(x0) \u2212min\nx f(x)\ni\nin time O\n\u0012\nT4( 2\u03bb+\u00b5\n\u00b5\n)3/2\np\n\u2308\u03bb/\u00b5\u2309log c\n\u0013\n.\nThis theorem essentially states that we can use a linearly convergent algorithm for minimiz-\ning f(x) + \u03bb\u2225x \u2212x0\u22252\n2 in order to minimize f, while incurring a multiplicative overhead of only\nO(\np\n\u2308\u03bb/\u00b5\u2309polylog(\u03bb/\u00b5)). Applying this theorem to previous state-of-the-art algorithms improves\nboth the running time for solving (1), as well as the following more general ERM problem:\nmin\nx\u2208Rd\nn\nX\ni=1\n\u03c8i(x),\nwhere\n\u03c8i : Rd \u2192R.\n(2)\nProblem (2) is fundamental in the theory of convex optimization and covers ERM problems for\nmulticlass and structured prediction.\nThere are a variety of additional extensions to the ERM problem to which some of our analysis\neasily applies.\nFor instance, we could work in more general normed spaces, allow non-uniform\nsmoothness of the \u03c6, add an explicit regularizer, etc. However, to simplify exposition and compari-\nson to related work, we focus on (1) and make clear the extensions to (2) in Section 3. These cases\ncapture the core of the arguments presented and illustrate the generality of this approach.\nSeveral of the algorithmic tools and analysis techniques in this paper are similar in principle to\n(and sometimes appear indirectly in) work scattered throughout the machine learning and optimiza-\ntion literature \u2013 from classical treatments of error-tolerant PPA (Rockafellar, 1976; Guler, 1992) to\nthe e\ufb00ective proximal term used by Accelerated Proximal SDCA Shalev-Shwartz and Zhang (2014)\nin enabling its acceleration.\nBy analyzing these as separate tools, and by bookkeeping the error requirements that they\nimpose, we are able to assemble them into algorithms with improved guarantees. We believe that\nthe presentation of Accelerated APPA (Algorithm 2) arising from this view simpli\ufb01es, and clari\ufb01es\nin terms of broader convex optimization theory, the \u201couter loop\u201d steps employed by Accelerated\nProximal SDCA. More generally, we hope that disentangling the relevant algorithmic components\ninto this general reduction framework will lead to further applications both in theory and in practice.\n1.1\nFormal setup\nWe consider the ERM problem (1) in the following common setting:\nAssumption 1.2 (Regularity). Each loss function \u03c6i is L-smooth, i.e. for all x, y \u2208R,\n\u03c6(y) \u2264\u03c6(x) + \u03c6\u2032(x)(y \u2212x) + L\n2 (y \u2212x)2,\nand the sum F is \u00b5-strongly convex, i.e. for all x, y \u2208Rd,\nF(x) \u2265F(x) + \u2207F(x)T(y \u2212x) + \u00b5\n2 \u2225y \u2212x\u22252\n2.\n3\nWe let R def\n= maxi \u2225ai\u22252 and let A \u2208Rn\u00d7d be the matrix whose i\u2019th row is aT\ni . We refer to\n\u03ba def\n= \u2308LR2/\u00b5\u2309\nas the condition number of (1).\nAlthough many algorithms are designed for special cases of the ERM objective F where there is\nsome known, exploitable structure to the problem, our aim is to study the most general case subject\nto Assumption 1.2. To standardize the comparison among algorithms, we consider the following\ngeneric model of interaction with F:\nAssumption 1.3 (Computational model). For any i \u2208[n] and x \u2208Rd, we consider two primitive\noperations:\n\u2022 For b \u2208R, compute the gradient of x 7\u2192\u03c6i(aT\ni x \u2212b).\n\u2022 For b \u2208R, c \u2208Rd, minimize \u03c6i(aT\ni x) + b\u2225x \u2212c\u22252\n2.\nWe refer to these operations, as well as to the evaluation of \u03c6i(aT\ni x), as single accesses to \u03c6i, and\nassume that these operations can be performed in O(d) time.\nNotation\nDenote [n] def\n= {1, . . . , n}. Denote the optimal value of a convex function by fopt =\nminx f(x), and, when f is clear from context, let xopt denote a minimizer.\nA point x\u2032 is an\n\u03f5-approximate minimizer of f if f(x\u2032) \u2212fopt \u2264\u03f5.\nThe Fenchel dual of a convex function f :\nRk \u2192R is f\u2217: Rk \u2192R de\ufb01ned by f\u2217(y) = supx\u2208Rk{\u27e8y, x\u27e9\u2212f(x)}. We use eO(\u00b7) to hide factors\npolylogarithmic in n, L, \u00b5, \u03bb, and R, i.e. eO(f) = O(fpolylog(n, L, \u00b5, \u03bb, R)).\nRegularization and duality\nThroughout the paper we let F : Rd \u2192R denote a \u00b5-strongly\nconvex function. For certain results presented, F must in particular be the ERM problem (1),\nwhile other statements hold more generally. We make it clear on a case-by-case basis when F must\nhave the ERM structure as in (1).\nBeginning in Section 1.3 and throughout the remainder of the paper, we frequently consider the\nfunction fs,\u03bb(x), de\ufb01ned for all x, s \u2208Rd and \u03bb > 0 by\nfs,\u03bb(x) def\n= F(x) + \u03bb\n2\u2225x \u2212s\u22252\n2\n(3)\nIn such context, we let xopt\ns,\u03bb\ndef\n= argminx fs,\u03bb(x) and we call\n\u03ba\u03bb\ndef\n= \u2308LR2/\u03bb\u2309\nthe regularized condition number.\nWhen F is indeed the ERM objective (1), certain algorithms for minimizing fs,\u03bb operate in the\nregularized ERM dual. Namely, they proceed by decreasing the negative dual objective gs,\u03bb : Rn \u2192\nR, given by\ngs,\u03bb(y) def\n= G(y) + 1\n2\u03bb\u2225ATy\u22252\n2 \u2212sTATy,\n(4)\nwhere G(y) def\n= Pn\ni=1 \u03c6\u2217\ni (yi). Similar to the above, we let yopt\ns,\u03bb\ndef\n= argminy gs,\u03bb(y).\n4\nEmpirical risk minimization\nAlgorithm\nRunning time\nProblem\nGD\ndn2\u03ba log(\u03f50/\u03f5)\nF\nAccel. GD\ndn3/2\u221a\u03ba log(\u03f50/\u03f5)\nF\nSAG, SVRG\ndn\u03ba log(\u03f50/\u03f5)\nF\nSDCA\ndn\u03ba\u03bb log(\u03f50/\u03f5)\nF + \u03bbr\nAP-SDCA\ndn\u221a\u03ba\u03bb log(\u03f50/\u03f5)\nF + \u03bbr\nAPCG\ndn\u221a\u03ba\u03bb log(\u03f50/\u03f5)\nF + \u03bbr\nThis work\ndn\u221a\u03ba log(\u03f50/\u03f5)\nF\nLinear least-squares regression\nAlgorithm\nRunning time\nProblem\nNaive mult.\nnd2\n\u2225Ax \u2212b\u22252\n2\nFast mult.\nnd\u03c9\u22121\n\u2225Ax \u2212b\u22252\n2\nRow sampling\n(nd + d\u03c9) log(\u03f50/\u03f5)\n\u2225Ax \u2212b\u22252\n2\nOSNAP\n(nd + d\u03c9) log(\u03f50/\u03f5)\n\u2225Ax \u2212b\u22252\n2\nR. Kaczmarz\ndn\u03ba log(\u03f50/\u03f5)\nAx = b\nAcc. coord.\ndn\u221a\u03ba log(\u03f50/\u03f5)\nAx = b\nThis work\ndn\u221a\u03ba log(\u03f50/\u03f5)\n\u2225Ax \u2212b\u22252\n2\nTable 1: Theoretical performance comparison on ERM and linear regression. Running times hold\nin expectation for randomized algorithms. In the \u201cproblem\u201d column for ERM, F marks algorithms\nthat can optimize the ERM objective (1), while F + \u03bbr marks those that only solve the explicitly\nregularized problem. For linear regression, Ax = b marks algorithms that only solve consistent\nlinear systems, whereas \u2225Ax \u2212b\u22252\n2 marks those that more generally minimize the squared loss. The\nconstant \u03c9 denotes the exponent of the matrix multiplication running time (currently below 2.373\n(Williams, 2012)). See Section 1.2 for more detail on these algorithms and their running times.\nTo make corresponding primal progress, dual-based algorithms make use of the dual-to-primal\nmapping, given by\nbxs,\u03bb(y) def\n= s \u22121\n\u03bbATy,\n(5)\nand the primal-to-dual mapping, given entrywise by\n[by(x)]i\ndef\n=\n\u0014\u2202\u03c6i(z)\n\u2202z\n\u0015\f\f\f\f\nz=aT\ni x\n(6)\nfor i = 1, . . . , n. (See Appendix B for a derivation of these facts and further properties of the dual.)\n1.2\nRunning times and related work\nIn Table 1 we compare our results with the running time of both classical and recent algorithms\nfor solving the ERM problem (1) and linear least-squares regression. Here we brie\ufb02y explain these\nrunning times and related work.\nEmpirical risk minimization\nIn the context of the ERM problem, GD refers to canonical gra-\ndient descent on F, Accel. GD is Nesterov\u2019s accelerated gradient decent (Nesterov, 1983, 2004),\nSVRG is the stochastic variance-reduced gradient of Johnson and Zhang (2013), SAG is the stochas-\ntic average gradient of Roux et al. (2012) and Defazio et al. (2014), SDCA is the stochastic dual\ncoordinate ascent of Shalev-Shwartz and Zhang (2013), AP-SDCA is the Accelerated Proximal\nSDCA of Shalev-Shwartz and Zhang (2014) and APCG is the accelerated coordinate algorithm\nof Lin et al. (2014). The latter three algorithms are more restrictive in that they only solve the\nexplicitly regularized problem F + \u03bbr, even if F is itself strongly convex (such algorithms run in\ntime inversely proportional to \u03bb).\nThe running times of the algorithms are presented based on the setting considered in this paper,\ni.e. under Assumptions 1.2 and 1.3. Many of the algorithms can be applied in more general settings\n5\n(e.g. even if the function F is not strongly convex) and have di\ufb00erent convergence guarantees in\nthose cases. The running times are characterized by four parameters: d is the data dimension,\nn is the number of samples, \u03ba = \u2308LR2/\u00b5\u2309is the condition number (for F + \u03bbr minimizers, the\ncondition number \u03ba\u03bb = \u2308LR2/\u03bb\u2309is used) and \u03f50/\u03f5 is the ratio between the initial and desired\naccuracy. Running times are stated per eO-notation; factors that depend polylogarithmically on n,\n\u03ba, and \u03ba\u03bb are ignored.\nLinear least-squares regression\nFor the linear least-squares regression problem, there is greater\nvariety in the algorithms that apply. For comparison, Table 1 includes Moore-Penrose pseudoin-\nversion \u2013 computed via naive matrix multiplication and inversion routines, as well as by their\nasymptotically fastest counterparts \u2013 in order to compute a closed-form solution via the standard\nnormal equations.\nThe table also lists algorithms based on the randomized Kaczmarz method\n(Strohmer and Vershynin, 2009; Needell et al., 2014) and their accelerated variant (Lee and Sid-\nford, 2013), as well as algorithms based on subspace embedding (OSNAP) or row sampling (Nelson\nand Nguyen, 2013; Li et al., 2013; Cohen et al., 2015). Some Kaczmarz-based methods only apply\nto the more restrictive problem of solving a consistent system (\ufb01nding x satisfying Ax = b) rather\nthan minimize the squared loss \u2225Ax\u2212b\u22252\n2. The running times depend on the same four parameters\nn, d, \u03ba, \u03f50/\u03f5 as before, except for computing the closed-form pseudoinverse, which for simplicity we\nconsider \u201cexact,\u201d independent of initial and target errors \u03f50/\u03f5.\nApproximate proximal point\nThe key to our improved running times is a suite of approximate\nproximal point algorithms that we propose and analyze. We remark that notions of error-tolerance\nin the typical proximal point algorithm \u2013 for both its plain and accelerated variants \u2013 have been\nde\ufb01ned and studied in prior work (Rockafellar, 1976; Guler, 1992). However, these mainly consider\nthe cumulative absolute error of iterates produced by inner minimizers, assuming that such a\nsequence is somehow produced. Since essentially any procedure of interest begins at some initial\npoint \u2013 and has runtime that depends on the relative error ratio between its start and end \u2013 such\na view does not yield fully concrete algorithms, nor does it yield end-to-end runtime upper bounds\nsuch as those presented in this paper.\nAdditional related work\nThere is an immense body of literature on proximal point methods\nand alternating direction method of multipliers (ADMM) that are relevant to the approach in this\npaper; see Boyd et al. (2011); Parikh and Boyd (2014) for modern surveys. We also note that the\nindependent work of Lin et al. (2015) contains results similar to some of those in this paper.\n1.3\nMain results\nAll formal results in this paper are obtained through a framework that we develop for iteratively\napplying and accelerating various minimization algorithms.\nWhen instantiated with recently-\ndeveloped fast minimizers we obtain, under Assumptions 1.2 and 1.3, algorithms guaranteed to\nsolve the ERM problem in time eO(nd\u221a\u03ba log(1/\u03f5)).\nOur framework stems from a critical insight of the classical proximal point algorithm (PPA) or\nproximal iteration: to minimize F (or more generally, any convex function) it su\ufb03ces to iteratively\nminimize\nfs,\u03bb(x) def\n= F(x) + \u03bb\n2\u2225x \u2212s\u22252\n2\n6\nfor \u03bb > 0 and proper choice of center s \u2208Rd. PPA iteratively applies the update\nx(t+1) \u2190argmin\nx\nfx(t),\u03bb(x)\nand converges to the minimizer of F. The minimization in the update is known as the proximal\noperator (Parikh and Boyd, 2014), and we refer to it in the sequel as the inner minimization\nproblem.\nIn this paper we provide three distinct approximate proximal point algorithms, i.e. algorithms\nthat do not require full inner minimization. Each enables the use of existing fast algorithm as its\ninner minimizer, in turn yielding several ways to obtain our improved ERM running time:\n\u2022 In Section 2 we develop a basic approximate proximal point algorithm (APPA). The algo-\nrithm is essentially PPA with a relaxed requirement of inner minimization by only a \ufb01xed\nmultiplicative constant in each iteration. Instantiating this algorithm with an accelerated,\nregularized ERM solver \u2013 such as APCG (Lin et al., 2014) \u2013 as its inner minimizer yields the\nimproved accelerated running time for the ERM problem (1).\n\u2022 In Section 3 we develop Accelerated APPA. Instantiating this algorithm with SVRG (Johnson\nand Zhang, 2013) as its inner minimizer yields the improved accelerated running time for both\nthe ERM problem (1) as well as the general ERM problem (2).\n\u2022 In Section 4 we develop Dual APPA: an algorithm whose approximate inner minimizers\noperate on the dual fs,\u03bb, with warm starts between iterations. Dual APPA enables several\ninner minimizers that are a priori incompatible with APPA. Instantiating this algorithm\nwith an accelerate, regularized ERM solver \u2013 such as APCG (Lin et al., 2014) \u2013 as its inner\nminimizer yields the improved accelerated running time for the ERM problem (1).\nEach of the three algorithms exhibits a slight advantage over the others in di\ufb00erent regimes.\nAPPA has by far the simplest and most straightforward analysis, and applies directly to any \u00b5-\nstrongly convex function F (not only F given by (1)). Accelerated APPA is more complicated, but\nin many regimes is a more e\ufb03cient reduction than APPA; it too applies to any \u00b5-strongly convex\nfunction F and in turn proves Theorem 1.1.\nOur third algorithm, Dual APPA, is the least general in terms of the assumptions on which it\nrelies. It is the only reduction we develop that requires the ERM structure of F. However, this\nalgorithm is a natural choice in conjunction with inner minimizers that operate on a popular dual\nobjective. In Section 5 we demonstrate moreover that this algorithm has properties that make it\ndesirable in practice.\n1.4\nPaper organization\nThe remainder of this paper is organized as follows. In Section 2, Section 3, and Section 4 we state\nand analyze the approximate proximal point algorithms described above. In Section 5 we discuss\npractical concerns and cover numerical experiments involving Dual APPA and related stochastic\nalgorithms. In Appendix A we prove general technical lemmas used throughout the paper and in\nAppendix B we provide a derivation of regularized ERM duality and related technical lemmas.\n7\n2\nApproximate proximal point algorithm (APPA)\nIn this section we describe our approximate proximal point algorithm (APPA). This algorithm\nis perhaps the simplest, both in its description and in its analysis, in comparison to the others\ndescribed in this paper. This section also introduces technical machinery that is used throughout\nthe sequel.\nWe \ufb01rst present our formal abstraction of inner minimizers (Section 2.1), then we present our\nalgorithm (Section 2.2), and \ufb01nally we step through its analysis (Section 2.3).\n2.1\nApproximate primal oracles\nTo design APPA, we \ufb01rst quantify the error that can be tolerated of an inner minimizer, while\naccounting for the computational cost of ensuring such error. The abstraction we use is the following\nnotion of inner approximation:\nDe\ufb01nition 2.1. An algorithm P is a primal (c, \u03bb)-oracle if, given x \u2208Rd, it outputs P(x) that is\na ([fx,\u03bb(x) \u2212fopt\nx,\u03bb]/c)-approximate minimizer of fx,\u03bb in time TP.1\nIn other words, a primal oracle is an algorithm initialized at x that reduces the error of fx,\u03bb by\na 1/c fraction, in time that depends on \u03bb, and c, and regularity properties of F. Typical iterative\n\ufb01rst-order algorithms, such as those in Table 1, yield primal (c, \u03bb)-oracles with runtimes TP that\nscale inversely in \u03bb or\n\u221a\n\u03bb, and logarithmically in c. For instance:\nTheorem 2.2 (SVRG as a primal oracle). SVRG (Johnson and Zhang, 2013) is a primal (c, \u03bb)-\noracle with runtime complexity TP = O(nd min{\u03ba, \u03ba\u03bb} log c) for both the ERM problem (1) and the\ngeneral ERM problem (2).\nTheorem 2.3 (APCG as an accelerated primal oracle). Using APCG (Lin et al., 2014) we can ob-\ntain a primal (c, \u03bb)-oracle with runtime complexity TP = eO(nd\u221a\u03ba\u03bb log c) for the ERM problem (1).2\nProof. Corollary B.3 implies that, given a primal point x, we can obtain, in O(nd) time, a corre-\nsponding dual point y such that the duality gap fx,\u03bb(x) + gx,\u03bb(y) (and thus the dual error) is at\nmost O(poly(\u03ba\u03bb)) times the primal error. Lemma B.1 implies that decreasing the dual error by\na factor O(poly(\u03ba\u03bb)c) decreases the induced primal error by c. Therefore, applying APCG to the\ndual and performing the primal and dual mappings yield the theorem.\n2.2\nAlgorithm\nOur Approximate Proximal Point Algorithm (APPA) is given by the following Algorithm 1.\n1When the oracle is a randomized algorithm, we require that expected error is the same, i.e. that the solution be\n\u03f5-approximate in expectation.\n2AP-SDCA could likely also serve as a primal oracle with the same guarantees. However, the results in Shalev-\nShwartz and Zhang (2014) are stated assuming initial primal and dual variables are zero. It is not directly clear how\none can provide a generic relative decrease in error from this speci\ufb01c initial primal-dual pair.\n8\nAlgorithm 1 Approximate PPA (APPA)\ninput x(0) \u2208Rd, \u03bb > 0\ninput primal ( 2(\u03bb+\u00b5)\n\u00b5\n, \u03bb)-oracle P\nfor t = 1, . . . , T\ndo\nx(t) \u2190P(x(t\u22121))\nend for\noutput x(T)\nThe central goal of this section is to prove the following lemma, which guarantees a geometric\nconvergence rate for the iterates produced in this manner\nLemma 2.4 (Contraction in APPA). For any c\u2032 \u2208(0, 1), x \u2208Rd, and possibly randomized primal\n( \u03bb+\u00b5\nc\u2032\u00b5 , \u03bb)-oracle P (possibly randomized) we have\nE[F(P(x))] \u2212F opt \u2264\u03bb + c\u2032\u00b5\n\u03bb + \u00b5\n\u0000F(x) \u2212F opt\u0001\n.\n(7)\nThis lemma immediately implies the following running-time bounds for APPA.\nTheorem 2.5 (Un-regularizing in APPA). Given a primal ( 2(\u00b5+\u03bb)\n\u00b5\n, \u03bb)-oracle P, Algorithm 1 min-\nimizes the general ERM problem (2) to within accuracy \u03f5 in time O(TP\u2308\u03bb/\u00b5\u2309log(\u03f50/\u03f5)).3\nCombining Theorem 2.5 and Theorem 2.3 immediately yields our desired running time for\nsolving (1).\nCorollary 2.6. Instantiating Algorithm 1 with the Theorem 2.3 as the primal oracle and taking\n\u03bb = \u00b5 yields the running time of eO(nd\u221a\u03ba log(\u03f50/\u03f5)) for solving (1).\n2.3\nAnalysis\nThis section gives a proof of Lemma 2.4. Throughout, no assumption is made on F aside from\n\u00b5-strong convexity. Namely, we need not have F be smooth or at all di\ufb00erentiable.\nFirst, we consider the e\ufb00ect of an exact inner minimizer. Namely, we prove the following lemma\nrelating the minimum of the inner problem fs,\u03bb to F opt.\nLemma 2.7 (Relationship between minima). For all s \u2208Rd and \u03bb \u22650\nfopt\ns,\u03bb \u2212F opt \u2264\n\u03bb\n\u00b5 + \u03bb\n\u0000F(s) \u2212F opt\u0001\n.\nProof. Let xopt = argminx F(x) and for all \u03b1 \u2208[0, 1] let x\u03b1 = (1 \u2212\u03b1)s + \u03b1xopt. The \u00b5-strong\nconvexity of F implies that, for all \u03b1 \u2208[0, 1],\nF(x\u03b1) \u2264(1 \u2212\u03b1)F(s) + \u03b1F(xopt) \u2212\u03b1(1 \u2212\u03b1)\u00b5\n2\n\u2225s \u2212xopt\u22252\n2.\nConsequently, by the de\ufb01nition of fopt\ns,\u03bb ,\nfopt\ns,\u03bb \u2264F(x\u03b1) + \u03bb\n2 \u2225x\u03b1 \u2212s\u22252\n2 \u2264(1 \u2212\u03b1)F(s) + \u03b1F(xopt) \u2212\u03b1(1 \u2212\u03b1)\u00b5\n2\n\u2225s \u2212xopt\u22252\n2 + \u03bb\u03b12\n2 \u2225s \u2212xopt\u22252\n2\nChoosing \u03b1 =\n\u00b5\n\u00b5+\u03bb yields the result.\n3When the oracle is a randomized algorithm, the expected accuracy is at most \u03f5.\n9\nThis immediately implies contraction for the exact PPA, as it implies that in every iteration of\nPPA the error in F decreases by a multiplicative \u03bb/(\u03bb + \u00b5). Using this we prove Lemma 2.4.\nProof of Lemma 2.4. Let x\u2032 = P(x). By de\ufb01nition of primal oracle P we have\nfx,\u03bb(x\u2032) \u2212fopt\nx,\u03bb \u2264\nc\u2032\u00b5\n\u03bb + \u00b5\n\u0010\nfx,\u03bb(x) \u2212fopt\nx,\u03bb\n\u0011\n.\nCombining this and Lemma 2.7 we have\nfx,\u03bb(x\u2032) \u2212F opt \u2264\nc\u2032\u00b5\n\u03bb + \u00b5\n\u0010\nfx,\u03bb(x) \u2212fopt\nx,\u03bb\n\u0011\n+\n\u03bb\n\u00b5 + \u03bb\n\u0000F(x) \u2212F opt\u0001\nUsing that clearly for all z we have F(z) \u2264fx,\u03bb(z) we see that F(x\u2032) \u2264fx,\u03bb(x\u2032) and F opt \u2264fopt\nx,\u03bb .\nCombining with the fact that fx,\u03bb(x) = F(x) yields the result.\n3\nAccelerated APPA\nIn this section we show how generically accelerate the APPA algorithm of Section 2. Accelerated\nAPPA (Algorithm 2) uses inner minimizers more e\ufb03ciently, but requires a smaller minimization\nfactor when compared to APPA. The algorithm and its analysis immediately prove Theorem 1.1\nand in turn yield another means by which we achieve the accelerated running time guarantees for\nsolving (1).\nWe \ufb01rst present the algorithm and state its running time guarantees (Section 3.1), then prove\nthe guarantees as part of analysis (Section 3.2).\n3.1\nAlgorithm\nOur accelerated APPA algorithm is given by Algorithm 2. In every iteration it still makes a single\ncall to a primal oracle, but rather than requiring a \ufb01xed constant minimization the minimization\nfactor depends polynomial on the ratio of \u03bb and \u00b5.\nAlgorithm 2 Accelerated APPA\ninput x(0) \u2208Rd, \u00b5 > 0, \u03bb > 2\u00b5\ninput primal (4\u03c13/2, \u03bb)-oracle P, where \u03c1 = \u00b5+2\u03bb\n\u00b5\nDe\ufb01ne \u03b6 = 2\n\u00b5 + 1\n\u03bb\nv(0) \u2190x(0)\nfor t = 0, . . . , T \u22121 do\ny(t) \u2190\n\u0010\n1\n1+\u03c1\u22121/2\n\u0011\nx(t) +\n\u0010\n\u03c1\u22121/2\n1+\u03c1\u22121/2\n\u0011\nv(t)\nx(t+1) \u2190P(y(t))\ng(t) \u2190\u03bb(y(t) \u2212x(t+1))\nv(t+1) \u2190(1 \u2212\u03c1\u22121/2)v(t) + \u03c1\u22121/2 \u0002\ny(t) \u2212\u03b6g(t)\u0003\nend for\noutput x(T)\nThe central goal is to prove the following theorem regarding the running time of APPA.\n10\nTheorem 3.1 (Un-regularizing in Accelerated APPA). Given a primal (4( 2\u03bb+\u00b5\n\u00b5\n)3/2, \u03bb)-oracle P\nfor \u03bb \u22652\u00b5, Algorithm 2 minimizes the general ERM problem (2) to within accuracy \u03f5 in time\nO(TP\np\n\u2308\u03bb/\u00b5\u2309log(\u03f50/\u03f5)).\nThis theorem is essentially a restatement of Theorem 1.1 and by instantiating it with Theo-\nrem 2.2 we obtain the following.\nCorollary 3.2. Instantiating Theorem 3.1 with SVRG (Johnson and Zhang, 2013) as the primal\noracle and taking \u03bb = 2\u00b5 + LR2 yields the running time bound eO(nd\u221a\u03ba log(\u03f50/\u03f5)) for the general\nERM problem (2).\n3.2\nAnalysis\nHere we establish the convergence rate of Algorithm 2, Accelerated APPA, and prove Theorem 3.1.\nNote that as in Section 2 the results in this section use nothing about the structure of F other than\nstrong convexity and thus they apply to the general ERM problem (2).\nWe remark that aspects of the proofs in this section bear resemblance to the analysis in Shalev-\nShwartz and Zhang (2014), which achieves similar results in a more specialized setting.\nOur proof is split into the following parts.\n\u2022 In Lemma 3.3 we show that applying a primal oracle to the inner minimization problem gives\nus a quadratic lower bound on F(x).\n\u2022 In Lemma 3.4 we use this lower bound to construct a series of lower bounds for the main\nobjective function f, and accelerate the APPA algorithm, comprising the bulk of the analysis.\n\u2022 In Lemma 3.5 we show that the requirements of Lemma 3.4 can be met by using a primal\noracle that decreases the error by a constant factor.\n\u2022 In Lemma 3.6 we analyze the initial error requirements of Lemma 3.4.\nThe proof of Theorem 3.1 follows immediately from these lemmas.\nLemma 3.3. For x0 \u2208Rn and \u03f5 > 0 suppose that x+ is an \u03f5-approximate solution to fx0,\u03bb. Then\nfor \u00b5\u2032 def\n= \u00b5/2, g def\n= \u03bb(x0 \u2212x+), and all x \u2208Rn we have\nF(x) \u2265F(x+) \u22121\n2\u00b5\u2032 \u2225g\u22252 + \u00b5\u2032\n2\n\r\r\r\rx \u2212\n\u0012\nx0 \u2212\n\u0012 1\n\u00b5\u2032 + 1\n\u03bb\n\u0013\ng\n\u0013\r\r\r\r\n2\n2\n\u2212\u03bb + 2\u00b5\u2032\n\u00b5\u2032\n\u03f5.\nNote that as \u00b5\u2032 = \u00b5/2 we are only losing a factor of 2 in the strong convexity parameter for our\nlower bound. This allows us to account for errors without sacri\ufb01cing in our ultimate asymptotic\nconvergence rates.\nProof. Since F is \u00b5-strongly convex clearly fx0,\u03bb is \u00b5 + \u03bb strongly convex, by Lemma A.1\nfx0,\u03bb(x) \u2212fx0,\u03bb(xopt\nx0,\u03bb) \u2265\u00b5 + \u03bb\n2\n\u2225x \u2212xopt\u22252\n2.\n(8)\nBy Cauchy-Schwartz and Young\u2019s Inequality we know that\n\u03bb + \u00b5\u2032\n2\n\u2225x \u2212x+\u22252\n2 \u2264\u03bb + \u00b5\u2032\n2\n\u0010\n\u2225x \u2212xopt\nx0,\u03bb\u22252\n2 + \u2225xopt\nx0,\u03bb \u2212x+\u22252\n2\n\u0011\n+ \u00b5\u2032\n2 \u2225x \u2212xopt\nx0,\u03bb\u22252\n2 + (\u03bb + \u00b5\u2032)2\n2\u00b5\u2032\n\u2225xopt\nx0,\u03bb \u2212x+\u22252\n2,\n11\nwhich implies\n\u00b5 + \u03bb\n2\n\u2225x \u2212xopt\nx0,\u03bb\u22252\n2 \u2265\u03bb + \u00b5\u2032\n2\n\u2225x \u2212x+\u22252\n2 \u2212\u03bb + \u00b5\u2032\n\u00b5\u2032\n\u00b7 \u03bb + \u00b5\n2\n\u2225xopt\nx0,\u03bb \u2212x+\u22252\n2.\nOn the other hand, since fx0,\u03bb(x+) \u2264fx0,\u03bb(xopt\nx0,\u03bb)+\u03f5 by assumption we have \u03bb+\u00b5\n2 \u2225x+\u2212xopt\u22252\n2 \u2264\u03f5\nand therefore\nfx0,\u03bb(x) \u2212fx0,\u03bb(x+) \u2265fx0,\u03bb(x) \u2212fx0,\u03bb(xopt\nx0,\u03bb) \u2212\u03f5 \u2265\u00b5 + \u03bb\n2\n\u2225x \u2212xopt\nx0,\u03bb\u22252\n2 \u2212\u03f5\n\u2265\u03bb + \u00b5\u2032\n2\n\u2225x \u2212x+\u22252\n2 \u2212\u03bb + \u00b5\u2032\n\u00b5\u2032\n\u00b7 \u03bb + \u00b5\n2\n\u2225xopt\nx0,\u03bb \u2212x+\u22252\n2 \u2212\u03f5\n\u2265\u03bb + \u00b5\u2032\n2\n\u2225x \u2212x+\u22252\n2 \u2212\u03bb + 2\u00b5\u2032\n\u00b5\u2032\n\u03f5.\nNow since\n\u2225x \u2212x+\u22252\n2 = \u2225x \u2212x0 + 1\n\u03bbg\u22252\n2 = \u2225x \u2212x0\u22252 + 2\n\u03bb \u27e8g, x \u2212x0\u27e9+ 1\n\u03bb2 \u2225g\u22252\n2,\nand using the fact that fx0,\u03bb(x) = F(x) + \u03bb\n2\u2225x \u2212x0\u22252\n2, we have\nF(x) \u2265F(x+) +\n\u0014 1\n\u03bb + \u00b5\u2032\n2\u03bb2\n\u0015\n\u2225g\u22252\n2 +\n\u0012\n1 + \u00b5\u2032\n\u03bb\n\u0013\n\u27e8g, x \u2212x0\u27e9+ \u00b5\u2032\n2 \u2225x \u2212x0\u22252 \u2212\u03bb + 2\u00b5\u2032\n\u00b5\u2032\n\u03f5.\nThe right hand side of the above equation is a quadratic function. Looking at its gradient with\nrespect to x we see that it obtains its minimum when x = x0 \u2212( 1\n\u00b5\u2032 + 1\n\u03bb)g and has a minimum value\nof F(x+) \u2212\n1\n2\u00b5\u2032 \u2225g\u22252\n2 \u2212\u03bb+2\u00b5\u2032\n\u00b5\u2032\n\u03f5.\nLemma 3.4. Suppose that in each iteration t we have \u03c8t\ndef\n= \u03c8opt\nt\n+ \u00b5\u2032\n2 \u2225x \u2212v(t)\u22252\n2 such that F(x) \u2265\n\u03c8t(x) for all x. Let \u03c1 def\n= \u00b5\u2032+\u03bb\n\u00b5\u2032\nfor \u03bb \u22653\u00b5\u2032, and let\n\u2022 y(t) def\n=\n\u0010\n1\n1+\u03c1\u22121/2\n\u0011\nx(t) +\n\u0010\n\u03c1\u22121/2\n1+\u03c1\u22121/2\n\u0011\nv(t),\n\u2022 E[fy(t),\u03bb(x(t+1))] \u2212fopt\ny(t),\u03bb \u2264\u03c1\u22123/2\n4\n(F(x(t)) \u2212\u03c8opt\nt\n),\n\u2022 g(t) def\n= \u03bb(y(t) \u2212x(t+1)),\n\u2022 v(t+1) def\n= (1 \u2212\u03c1\u22121/2)v(t) + \u03c1\u22121/2 h\ny(t) \u2212\n\u0010\n1\n\u00b5\u2032 + 1\n\u03bb\n\u0011\ng(t)i\n.\nWe have\nE[F(x(t)) \u2212\u03c8opt\nt\n] \u2264\n \n1 \u2212\u03c1\u22121/2\n2\n!t\n(F(x0) \u2212\u03c8opt\n0 ).\nProof. Regardless of how y(t) is chosen we know by Lemma 3.3 that for \u03b3 = 1 + \u00b5\u2032\n\u03bb and all x \u2208Rn\nF(x) \u2265F(x(t+1))\u22121\n2\u00b5\u2032 \u2225g(t)\u22252\n2+ \u00b5\u2032\n2\n\r\r\r\rx \u2212\n\u0012\ny(t) \u2212\u03b3\n\u00b5\u2032 g(t)\n\u0013\r\r\r\r\n2\n2\n\u2212\u03bb + 2\u00b5\u2032\n\u00b5\u2032\n\u0010\nfy(t),\u03bb(x(t+1)) \u2212fopt\ny(t),\u03bb\n\u0011\n. (9)\n12\nThus, for \u03b2 = 1 \u2212\u03c1\u22121/2 we can let\n\u03c8t+1(x) def\n= \u03b2\u03c8t(x) + (1 \u2212\u03b2)\n\u0014\nF(x(t+1)) \u22121\n2\u00b5\u2032 \u2225g(t)\u22252\n2 + \u00b5\u2032\n2 \u2225x \u2212\n\u0012\ny(t) \u2212\u03b3\n\u00b5\u2032 g(t)\n\u0013\n\u22252\n2\n\u2212\u03bb + 2\u00b5\u2032\n\u00b5\u2032\n(fy(t),\u03bb(x(t+1)) \u2212fopt\ny(t),\u03bb)\n\u0015\n= \u03b2\n\u0014\n\u03c8opt\nt\n+ \u00b5\u2032\n2 \u2225x \u2212v(t)\u22252\n2\n\u0015\n+ (1 \u2212\u03b2)\n\u0014\nF(x(t+1)) \u22121\n2\u00b5\u2032 \u2225g(t)\u22252\n2 + \u00b5\u2032\n2 \u2225x \u2212\n\u0012\ny(t) \u2212\u03b3\n\u00b5\u2032 g(t)\n\u0013\n\u22252\n2\n\u2212\u03bb + 2\u00b5\u2032\n\u00b5\u2032\n(fy(t),\u03bb(x(t+1)) \u2212fopt\ny(t),\u03bb)\n\u0015\n= \u03c8opt\nt+1 + \u00b5\u2032\n2 \u2225x \u2212v(t+1)\u22252\n2.\nwhere in the last line we used Lemma A.3. Again, by Lemma A.3 we know that\n\u03c8opt\nt+1 = \u03b2\u03c8t + (1 \u2212\u03b2)\n\u0012\nF(x(t+1)) \u22121\n2\u00b5\u2032 \u2225g(t)\u22252\n2 \u2212\u03bb + 2\u00b5\u2032\n\u00b5\u2032\n(fy(t),\u03bb(x(t+1)) \u2212fopt\ny(t),\u03bb)\n\u0013\n+ \u03b2(1 \u2212\u03b2)\u00b5\u2032\n2 \u2225v(t) \u2212\n\u0012\ny(t) \u2212\u03b3\n\u00b5\u2032 g(t)\n\u0013\n\u22252\n2\n\u2265\u03b2\u03c8t + (1 \u2212\u03b2)F(x(t+1)) \u2212(1 \u2212\u03b2)2\n2\u00b5\u2032\n\u2225g(t)\u22252\n2 + \u03b2(1 \u2212\u03b2)\u03b3\nD\ng(t), v(t) \u2212y(t)E\n\u2212(1 \u2212\u03b2)(\u03bb + 2\u00b5\u2032)\n\u00b5\u2032\n(fy(t),\u03bb(x(t+1)) \u2212fopt\ny(t),\u03bb).\nIn the second step we used the following fact:\n\u22121 \u2212\u03b2\n2\u00b5\u2032\n+ \u03b2(1 \u2212\u03b2)\u00b5\u2032\n2 \u00b7 \u03b32\n\u00b5\u2032 = 1 \u2212\u03b2\n2\u00b5\u2032 (\u22121 + \u03b2\u03b32) \u2265\u2212(1 \u2212\u03b2)2\n2\u00b5\u2032\n.\nFurthermore, expanding the term \u00b5\n2\u2225(x \u2212y(t)) + \u03b3\n\u00b5g(t)\u22252\n2 and instantiating x with x(t) in (9) yields\nF(x(t+1)) \u2264F(x(t)) \u22121\n\u03bb\u2225g(t)\u22252\n2 + \u03b3\nD\ng(t), y(t) \u2212x(t)E\n+ \u03bb + 2\u00b5\u2032\n\u00b5\u2032\n(fy(t),\u03bb(x(t+1)) \u2212fopt\ny(t),\u03bb).\nConsequently we know\nF(x(t+1)) \u2212\u03c8opt\nt+1 \u2264\u03b2[f(x(t)) \u2212\u03c8opt\nt\n] +\n\u0014(1 \u2212\u03b2)2\n2\u00b5\u2032\n\u2212\u03b2\n\u03bb\n\u0015\n\u2225g(t)\u22252\n2 + \u03b3\u03b2\nD\ng(t), y(t) \u2212x(t) \u2212(1 \u2212\u03b2)(v(t) \u2212y(t))\nE\n+ (\u03bb + 2\u00b5\u2032)\n\u00b5\u2032\n(fy(t),\u03bb(x(t+1)) \u2212fopt\ny(t),\u03bb)\nNote that we have chosen y(t) so that the inner product term equals 0, and we choose \u03b2 = 1\u2212\u03c1\u22121/2 \u2265\n1\n2 which ensures\n(1 \u2212\u03b2)2\n2\u00b5\u2032\n\u2212\u03b2\n\u03bb \u2264\n1\n2(\u00b5\u2032 + \u03bb) \u22121\n2\u03bb \u22640.\nAlso, by assumption we know E[fy(t),\u03bb(x(t+1)) \u2212fopt\ny(t),\u03bb] \u2264\u03c1\u22123/2\n4\n(f(x(t)) \u2212\u03c8opt\nt\n), which implies\nE[F(x(t+1)) \u2212\u03c8opt\nt+1] \u2264\n \n\u03b2 + (\u03bb + 2\u00b5\u2032)\n\u00b5\u2032\n\u00b7 \u03c1\u22123/2\n4\n!\n(F(x(t)) \u2212\u03c8opt\nt\n) \u2264(1 \u2212\u03c1\u22121/2/2)(F(x(t)) \u2212\u03c8opt\nt\n).\n13\nIn the \ufb01nal step we are using the fact that \u03bb+2\u00b5\u2032\n\u00b5\u2032\n\u22642\u03c1 and \u03c1 \u22651.\nLemma 3.5. Under the setting of Lemma 3.4, we have fy(t),\u03bb(x(t)) \u2212fopt\ny(t),\u03bb \u2264F(x(t)) \u2212\u03c8opt\nt\n. In\nparticular, in order to achieve E[fy(t),\u03bb(x(t+1))] \u2264\u03c1\u22123/2\n8\n(F(x(t)) \u2212\u03c8opt\nt\n) we only need an oracle that\nshrinks the function error by a factor of \u03c1\u22123/2\n8\n(in expectation).\nProof. We know\nfy(t),\u03bb(x(t)) \u2212f(x(t)) = \u03bb\n2 \u2225x(t) \u2212y(t)\u22252\n2 = \u03bb\n2 \u00b7\n\u03c1\u22121\n(1 + \u03c1\u22121/2)2 \u2225x(t) \u2212v(t)\u22252\n2.\nWe will try to show the lower bound fopt\ny(t),\u03bb is larger than \u03c8opt\nt\nby the same amount. This is because\nfor all x we have\nfy(t),\u03bb(x) = F(x) + \u03bb\n2 \u2225x \u2212y(t)\u22252\n2 \u2265\u03c8opt\nt\n+ \u00b5\u2032\n2 \u2225x \u2212v(t)\u22252\n2 + \u03bb\n2 \u2225x \u2212y(t)\u22252\n2.\nThe right hand side is a quadratic function, whose optimal point is at x = \u00b5\u2032v(t)+\u03bby(t)\n\u00b5\u2032+\u03bb\nand whose\noptimal value is equal to\n\u03c8opt\nt\n+\u03bb\n2\n\u0012\n\u00b5\u2032\n\u00b5\u2032 + \u03bb\n\u00132\n\u2225v(t)\u2212y(t)\u22252\n2+\u00b5\u2032\n2\n\u0012\n\u03bb\n\u00b5 + \u03bb\n\u00132\n\u2225v(t)\u2212y(t)\u22252\n2 = \u03c8opt\nt\n+\n\u00b5\u2032\u03bb\n2(\u00b5\u2032 + \u03bb)\u00b7\n1\n(1 + \u03c1\u22121/2)2 \u2225x(t)\u2212v(t)\u22252\n2.\nBy de\ufb01nition of \u03c1\u22121, we know\n\u00b5\u2032\u03bb\n2(\u00b5\u2032+\u03bb) \u00b7\n1\n(1+\u03c1\u22121/2)2 \u2225x(t) \u2212v(t)\u22252\n2 is exactly equal to \u03bb\n2 \u00b7\n\u03c1\u22121\n(1+\u03c1\u22121/2)2 \u2225x(t) \u2212\nv(t)\u22252\n2, therefore fy(t),\u03bb(x(t)) \u2212fopt\ny(t),\u03bb \u2264F(x(t)) \u2212\u03c8opt\nt\n.\nRemark\nIn the next lemma we show that moving to the regularized problem has the same e\ufb00ect\non the primal function value and the lower bound. This is a result of the choice of \u03b2 in the proof\nof Lemma 3.4. However, this does not mean that the choice of \u03b2 is very fragile. We can choose any\n\u03b2\u2032 that is between the current \u03b2 and 1; the e\ufb00ect on this lemma will be that the increase in primal\nfunction becomes smaller than the increase in the lower bound (so the lemma continues to hold).\nLemma 3.6. Let \u03c8opt\n0\n= F(x(0)) \u2212\u03bb+2\u00b5\u2032\n\u00b5\u2032\n(F(x(0)) \u2212fopt), and v(0) = x(0), then \u03c80\ndef\n= \u03c8opt\n0\n+\n\u00b5\u2032\n2 \u2225x \u2212v0\u22252 is a valid lower bound for F. In particular when \u03bb = LR2 then F(x(0)) \u2212\u03c8opt\n0\n\u2264\n2\u03ba(F(x(0)) \u2212fopt).\nProof. This lemma is a direct corollary of Lemma 3.3 with x+ = x(0).\n4\nDual APPA\nIn this section we develop Dual APPA (Algorithm 3), a natural approximate proximal point al-\ngorithm that operates entirely in the regularized ERM dual.\nOur focus here is on theoretical\nproperties of Dual APPA; Section 5 later explores aspects of Dual APPA more in practice.\nWe \ufb01rst present an abstraction for dual-based inner minimizers (Section 4.1), then present the\nalgorithm (Section 4.2), and \ufb01nally step through its runtime analysis (Section 4.3).\n14\n4.1\nApproximate dual oracles\nOur primary goal in this section is to quantify how much objective function progress an algorithm\nneeds to make in the dual problem, gs,\u03bb (See Section 1.1) in order to ensure primal progress at a\nrate similar to that in APPA (Algorithm 1).\nHere, similar to Section 2.1, we formally de\ufb01ne our requirements for an approximate dual-based\ninner dual minimize. In particular, we use the following notion of dual oracle.\nDe\ufb01nition 4.1. An algorithm D is a dual (c, \u03bb)-oracle if, given s \u2208Rd and y \u2208Rn, it outputs\nD(s, y) that is a ([gs,\u03bb(y) \u2212gopt\ns,\u03bb]/c)-approximate minimizer of gs,\u03bb in time TD.4\nDual based algorithms for regularized ERM and variants of coordinate descent typically can be\nused as such a dual oracle. In particular we note that APCG is such a dual oracle.\nTheorem 4.2 (APCG as a dual oracle). APCG (Lin et al., 2014) is a dual (c, \u03bb)-oracle with\nruntime complexity TD = eO(nd\u221a\u03ba\u03bb log c).5\n4.2\nAlgorithm\nOur dual APPA is given by the following Algorithm 3.\nAlgorithm 3 Dual APPA\ninput x(0) \u2208Rd, \u03bb > 0\ninput dual (\u03c3, \u03bb)-oracle D\n(see Theorem 4.3 for \u03c3)\ny(0) \u2190by(x(0))\nfor t = 1, . . . , T\ndo\ny(t) \u2190D(x(t\u22121), y(t\u22121))\nx(t) \u2190bxx(t\u22121),\u03bb(y(t))\nend for\noutput x(T)\nDual APPA (Algorithm 3) repeatedly queries a dual oracle while producing primal iterates via\nthe dual-to-primal mapping (5) along the way. We show that it obtains the following running time\nbound:\nTheorem 4.3 (Un-regularizing in Dual APPA). Given a dual (\u03c3, \u03bb)-oracle D, where\n\u03c3 \u226580n2\u03ba2\n\u03bb max{\u03ba, \u03ba\u03bb}\u2308\u03bb/\u00b5\u2309\nAlgorithm 3 minimizes the ERM problem (1) to within accuracy \u03f5 in time eO(TD\u2308\u03bb/\u00b5\u2309log(\u03f50/\u03f5)).6\nCombining Theorem 4.3 and Theorem 4.2 immediately yields another way to achieve our desired\nrunning time for solving (1).\n4As in the primal oracle de\ufb01nition, when the oracle is a randomized algorithm, we require that its output be an\nexpected \u03f5-approximate solution.\n5As in Theorem 2.3, AP-SDCA could likely also serve as a dual oracle with the same guarantees, provided it is\nmodi\ufb01ed to allow for the more general primal-dual initialization.\n6As in Theorem 2.5, when the oracle is a randomized algorithm, the expected accuracy is at most \u03f5.\n15\nCorollary 4.4. Instantiating Theorem 4.3 with Theorem 4.2 as the dual oracle and taking \u03bb = \u00b5\nyields the running time bound eO(nd\u221a\u03ba log(\u03f50/\u03f5)).\nWhile both this result and the results in Section 2 show that APCG can be used to achieve our\nfastest running times for solving (1), note that the algorithms they suggest are in fact di\ufb00erent. In\nevery invocation of APCG in Algorithm 1, we need to explicitly compute both the primal-to-dual\nand dual-to-primal mappings (in O(nd) time). However, here we only need to compute the primal-\nto-dual mapping once upfront, in order to initialize the algorithm. Every subsequent invocation\nof APCG then only requires a single dual-to-primal mapping computation, which can often be\nstreamlined. From a practical viewpoint, this can be seen as a natural \u201cwarm start\u201d scheme for\nthe dual-based inner minimizer.\n4.3\nAnalysis\nHere we proves Theorem 4.3. We begin by bounding the error of the dual regularized ERM problem\nwhen the center of regularization changes. This characterizes the initial error at the beginning of\neach Dual APPA iteration.\nLemma 4.5 (Dual error after re-centering.). For all y \u2208Rn, x \u2208Rd, and x\u2032 = bxx(y) we have\ngx\u2032,\u03bb(y) \u2212gopt\nx\u2032,\u03bb \u22642(gx,\u03bb(y) \u2212gopt\nx,\u03bb) + 4n\u03ba\n\u0002\nF(x\u2032) \u2212F opt + F(x) \u2212F opt\u0003\nIn other words, the dual error gs,\u03bb(y) \u2212gopt\ns,\u03bb is bounded across a re-centering step by multiples\nof previous sub-optimality measurements (namely, dual error and gradient norm).\nProof. By the de\ufb01nition of gx,\u03bb and x\u2032 we have, for all z,\ngx\u2032,\u03bb(z) = G(z) + 1\n2\u03bb\u2225ATz\u22252 \u2212x\u2032TATz = gx,\u03bb(z) \u2212(x\u2032 \u2212x)\u22a4ATz = gx,\u03bb(z) + 1\n\u03bbyTAATz .\nFurthermore, since g is 1\nL-strongly convex we can invoke Lemma A.2 obtaining\ngx\u2032,\u03bb(y) \u2212gopt\nx\u2032,\u03bb \u22642\nh\ngx,\u03bb(y) \u2212gopt\nx\u2032,\u03bb\ni\n+ L\n\r\r\r\r\n1\n\u03bbAATy\n\r\r\r\r\n2\n2\n.\nSince each row of A has \u21132 norm at most R we know that \u2225Az\u22252\n2 \u2264nR2\u2225z\u22252\n2 and we know that by\nde\ufb01nition ATy = \u03bb(x \u2212x\u2032). Combining these yields\ngx\u2032,\u03bb(y) \u2212gopt\nx\u2032,\u03bb \u22642\nh\ngx,\u03bb(y) \u2212gopt\nx\u2032,\u03bb\ni\n+ nLR2\u2225x \u2212x\u2032\u22252\n2.\nFinally, since F is \u00b5-strongly convex, by Lemma A.1, we have\n1\n2\u2225x \u2212x\u2032\u22252\n2 \u2264\u2225x\u2032 \u2212xopt\u22252\n2 + \u2225x \u2212xopt\u22252\n2 \u22642\n\u00b5\n\u0002\nF(x\u2032) \u2212F opt + F(x) \u2212F opt\u0003\n.\nCombining and recalling the de\ufb01nition of \u03ba yields the result.\nThe following lemma establishes the rate of convergence of the primal iterates {x(t)} produced\nover the course of Dual APPA, and in turn implies Theorem 4.3.\n16\nLemma 4.6 (Convergence rate of Dual APPA). Let c\u2032 \u2208(0, 1) be arbitrary and suppose that\n\u03c3 \u2265(40/c\u2032)n2\u03ba2\n\u03bb max{\u03ba, \u03ba\u03bb}\u2308\u03bb/\u00b5\u2309in Dual APPA (Algorithm 3). Then in every iteration t \u22651 of\nDual APPA (Algorithm 3) the following invariants hold:\nF(x(t\u22121)) \u2212F opt \u2264\n\u0012\u03bb + c\u2032\u00b5\n\u03bb + \u00b5\n\u0013t\u22121 \u0010\nF(x(0)) \u2212F opt\u0011\n,\nand\n(10)\ngx(t\u22121),\u03bb(y(t)) \u2212gopt\nx(t\u22121),\u03bb \u2264\n\u0012\u03bb + c\u2032\u00b5\n\u03bb + \u00b5\n\u0013t\u22121 \u0010\nF(x(0)) \u2212F opt\u0011\n.\n(11)\nProof. For notational convenience we let r def\n= ( \u03bb+c\u2032\u00b5\n\u03bb+\u00b5 ), gt\ndef\n= gx(t),\u03bb, ft\ndef\n= fx(t),\u03bb, and \u03f5t\ndef\n= F(x(t))\u2212\nF opt for all t \u22650. Thus, we wish to show that \u03f5t\u22121 \u2264rt\u22121\u03f50 (equivalent to (11)) and we wish to\nshow that gt\u22121(y(t)) \u2212gopt\nt\u22121 \u2264rt\u22121\u03f50 (equivalent to (10)) for all t \u22651.\nBy de\ufb01nition of a dual oracle we have, for all t \u22651,\ngt\u22121(y(t)) \u2212gopt\nt\u22121 \u22641\n\u03c3\nh\ngt\u22121(yt\u22121) \u2212gopt\nt\u22121\ni\n,\n(12)\nby Lemma B.1 we have, for all t \u22651,\nft\u22121(x(t)) \u2212fopt\nt\u22121 \u22642n2\u03ba2\n\u03bb\nh\ngt\u22121(y(t)) \u2212gopt\nt\u22121\ni\n,\n(13)\nby Lemma 4.5 we know\ngt(y(t)) \u2212gopt\nt\n\u22642\nh\ngt\u22121(y(t)) \u2212gopt\nt\ni\n+ 4n\u03ba(\u03f5t + \u03f5t\u22121),\n(14)\nand by Lemma 2.7 we know that for all t \u22651\nfopt\nt\u22121 \u2212F opt \u2264\n\u03bb\n\u00b5 + \u03bb\u03f5t\u22121\n(15)\nFurthermore, by Corollary B.3, the de\ufb01nition of y(0), and the facts that f0(x(0)) = F(x(0)) and\nft(z) \u2265F(z) we have\ng0(y(0)) \u2212gopt\n0\n\u22642\u03ba\u03bb\n\u0010\nf0(x(0)) \u2212fopt\n0\n\u0011\n\u22642\u03ba\u03bb\n\u0010\nF(x(0)) \u2212F opt\u0011\n= 2\u03ba\u03bb\u03f50\n(16)\nWe show that combining these and applying strong induction on t yields the desired result.\nWe begin with our base cases. When t = 1 the invariant (11) holds immediately by de\ufb01nition.\nFurthermore, when t = 1 we see that the invariant (10) holds, since \u03c3 \u22652\u03ba\u03bb and\ng0(y(1)) \u2212gopt\n0\n\u22641\n\u03c3(g0(y(0)) \u2212gopt\n0\n) \u22642\u03ba\u03bb\n\u03c3\n\u0010\nf0(x(0)) \u2212fopt\n0\n\u0011\n\u22642\u03ba\u03bb\n\u03c3 \u03f50,\n(17)\nwere we used (12) and (16) respectively. Finally we show that invariant (11) holds for t = 2:\nF(x(1)) \u2212F opt \u2264f0(x(1)) \u2212fopt\n0\n+ fopt\n0\n\u2212F opt\n(Since F(z) \u2264ft(z) for all t, z)\n\u22642n2\u03ba2\n\u03bb(g0(y(1)) \u2212gopt\n0\n) +\n\u03bb\n\u00b5 + \u03bb\u03f50\n(Equations (13) and (15))\n\u2264\n\u00124n2\u03ba3\n\u03bb\n\u03c3\n+\n\u03bb\n\u00b5 + \u03bb\n\u0013\n\u03f50\n(Equation (17))\n\u2264r\u03f50\n(Since \u03c3 \u22654n\u03ba3\n\u03bb/(c\u2032\u03bb/(\u00b5 + \u03bb)))\n17\nNow consider t \u22653 for the second invariant (11). We show this holds assuming the invariants\nhold for all smaller t.\nF(x(t\u22121)) \u2212F opt \u2264ft\u22122(x(t\u22121)) \u2212fopt\nt\u22122 + fopt\nt\u22122 \u2212F opt\n(Since F(z) \u2264ft(z) for all t, z)\n\u22642n2\u03ba2\n\u03bb(gt\u22122(yt\u22121) \u2212gopt\nt\u22122) +\n\u03bb\n\u00b5 + \u03bb\u03f5t\u22122\n(Equations (13) and (15))\n\u22642n2\u03ba2\n\u03bb\n\u03c3\n\u0010\ngt\u22122(yt\u22122) \u2212gopt\nt\u22122\n\u0011\n+\n\u03bb\n\u00b5 + \u03bb\u03f5t\u22122\n(Equation (12))\nFurthermore,\ngt\u22122(yt\u22122) \u2212gopt\nt\u22122 \u22642(gt\u22123(yt\u22122) \u2212gopt\nt\u22123) + 4n\u03ba [\u03f5t\u22122 + \u03f5t\u22123]\n(Equation (17))\n\u2264\n\u00002rt\u22122 + 4n\u03ba(rt\u22121 + rt\u22122)\n\u0001\n\u03f50\n(Inductive hypothesis)\n\u226410n\u03bart\u22121\u03f50\n(r \u22641 and \u03ba \u22651)\nSince \u03c3 \u226520n2\u03ba2\n\u03bb\u03ba/(c\u2032\u03bb/(\u00b5 + \u03bb)) combining yields that\n2n2\u03ba2\n\u03bb\n\u03c3\n\u0010\ngt\u22122(yt\u22122) \u2212gopt\nt\u22122\n\u0011\n\u2264\nc\u2032\u00b5\n\u00b5 + \u03bbrt\u22121\u03f50\nand the result follows by the inductive hypothesis on \u03f5t\u22122.\nFinally we show that invariant (10) holds for any t \u22652 given that it holds for all smaller t and\ninvariant (11) holds for that t and all smaller t.\ngt\u22121(y(t)) \u2212gopt\nt\u22121 \u22641\n\u03c3(gt\u22121(yt\u22121) \u2212gopt\nt\u22121)\n(De\ufb01nition dual oracle.)\n\u22641\n\u03c3\nh\n2(gx(t\u22122)(yt\u22121) \u2212gopt\nx(t\u22122)) + 4n\u03ba [\u03f5t\u22121 + \u03f5t\u22122]\ni\n(Equation (14))\n\u22641\n\u03c3\n\u0002\n2rt\u22121 + 4n\u03ba\n\u0002\nrt + rt\u22121\u0003\u0003\n\u03f50\n(Inductive hypothesis)\n\u2264rt\u22121\u03f50\n(\u03c3 \u22658n\u03ba)\nThe result then follows by induction.\n5\nImplementation\nIn the following two subsections, respectively, we discuss implementation details and report on an\nempirical evaluation of the APPA framework.\n5.1\nPractical concerns\nWhile theoretical convergence rates lay out a broad-view comparison of the algorithms in the\nliterature, we brie\ufb02y remark on some of the \ufb01ner-grained di\ufb00erences between algorithms, which\ninform their implementation or empirical behavior. To match the terminology used for SVRG in\nJohnson and Zhang (2013), we refer to a \u201cstage\u201d as a single step of APPA, i.e. the time spent\nexecuting the inner minimization of fx(t),\u03bb or gx(t),\u03bb (as in (3) and (4)).\n18\nRe-centering overhead of Dual APPA vs. SVRG\nAt the end of every one of its stages,\nSVRG pauses to compute an exact gradient by a complete pass over the dataset (costing \u0398(nd)\ntime during which n gradients are computed). Although an amortized runtime analysis hides this\ncost, this operation cannot be carried out in-step with the iterative updates of the previous stage,\nsince the exact gradient is computed at a point that is only selected at the stage\u2019s end.\nMeanwhile, if each stage in Dual APPA is initialized with a valid primal-dual pair for the inner\nproblem, Dual APPA can update the current primal point together with every dual coordinate\nupdate, in time O(d), i.e. with negligible increase in the overhead of the update. When doing so,\nthe corresponding data row remains fresh in cache and, unlike SVRG, no additional gradient need\nbe computed.\nMoreover, initializing each stage with a valid such primal-dual pair can be done in only O(d)\ntime. At the end of a stage where s was the center point, Dual APPA holds a primal-dual pair\n(x, y) where x = bxs(y). The next stage is centered at x and the dual variables initialized at y, so\nit remains to set up a corresponding primal point x\u2032 = bxx(y) = x \u22121\n\u03bbATy. This can be done by\ncomputing x\u2032 \u21902x \u2212s, since we know that x \u2212s = \u22121\n\u03bbATy.\nDecreasing \u03bb\nAPPA and Dual APPA enjoy the nice property that, as long as the inner problems\nare solved with enough accuracy, the algorithm does not diverge even for large choice of \u03bb. In\npractice this allows us to start with a large \u03bb and make faster inner minimizations. If we heuristically\nobserve that the function error is not decreasing rapidly enough, we can switch to a smaller \u03bb.\nFigure 3 (Section 5.2) demonstrates this empirically. This contrasts with algorithm parameters\nsuch as step size choices in stochastic optimizers (that may still appear in inner minimization).\nSuch parameters are typically more sensitive, and can suddenly lead to divergence when taken too\nlarge, making them less amenable to mid-run parameter tuning.\nStable update steps\nWhen used as inner minimizers, dual coordinate-wise methods such as\nSDCA typically provide a convenient framework in which to derive parameter updates with data-\ndependent step sizes, or sometimes enables closed-form updates altogether (i.e. optimal solutions to\neach single-coordinate maximization sub-problem). For example, when Dual APPA is used together\nwith SDCA to solve a problem of least-squares or ridge regression, the locally optimal SDCA\nupdates can be performed e\ufb03ciently in closed form. This decreases the number of algorithmic\nparameters requiring tuning, improves the overall the stability of the end-to-end optimizer and, in\nturn, makes it easier to use out of the box.\n5.2\nEmpirical analysis\nWe experiment with Dual APPA in comparison with SDCA, SVRG, and SGD on several binary\nclassi\ufb01cation tasks.\nBeyond general benchmarking, the experiments also demonstrate the advantages of the unordi-\nnary \u201cbias-variance tradeo\ufb00\u201d presented by approximate proximal iteration: the vanishing proximal\nterm empirically provides advantages of regularization (added strong convexity, lower variance)\nat a bias cost that is less severe than with typical \u21132 regularization. Even if some amount of \u21132\nshrinkage is desired, Dual APPA can place yet higher weight on its \u21132 term, enjoy improved speed\nand stability, and after a few stages achieve roughly the desired bias.\n19\nDatasets\nIn this section we show results for three binary classi\ufb01cation tasks, derived from\nMNIST,7 CIFAR-10,8 and Protein:9 in MNIST we classify the digits {1, 2, 4, 5, 7} vs. the rest,\nand in CIFAR we classify the animal categories vs. the automotive ones. MNIST and CIFAR are\ntaken under non-linear feature transformations that increase the problem scale signi\ufb01cantly: we\nnormalize the rows by scaling the data matrix by the inverse average \u21132 row norm. We then take\ntake n/5 random Fourier features per the randomized scheme of Rahimi and Recht (2007). This\nyields 12K features for MNIST (60K training examples, 10K test) and 10K for CIFAR (50K train-\ning examples, 10K test). Meanwhile, Protein is a standard pre-featurized benchmark (75 features,\n\u223c117K training examples, \u223c30K test) that we preprocess minimally by row normalization and an\nappended a\ufb03ne feature, and whose train/test split we obtain by randomly holding out 20% of the\noriginal labeled data.\nAlgorithms\nEach algorithm is parameterized by a scalar value \u03bb analogous to the \u03bb used in\nproximal iteration: \u03bb is the step size for SVRG, \u03bbt\u22121/2 is the decaying step size for SGD, and\n\u03bb\n2\u2225x\u22252\n2 is the ridge penalty for SDCA. (See Johnson and Zhang (2013) for a comparison of SVRG to\na more thoroughly tuned SGD under di\ufb00erent decay schemes.) We use Dual APPA (Algorithm 3)\nwith SDCA as the inner minimizer. For the algorithms with a notion of a stage \u2013 i.e. Dual APPA\u2019s\ntime spent invoking the inner minimizer, SVRG\u2019s period between computing exact gradients \u2013 we\nset the stage size equal to the dataset size for simplicity.10 SVRG is given an advantage in that we\nchoose not to count its gradient computations when it computes the exact gradient between stages.\nAll algorithms are initialized at x = 0. Each algorithm was run under \u03bb = 10i for i = \u22128, \u22127, . . . , 8,\nand plots report the trial that best minimized the original ERM objective.\nConvergence and bias\nThe proximal term in APPA introduces a vanishing bias for the problem\n(towards the initial point of x = 0) that provides a speedup by adding strong convexity to the prob-\nlem. We investigate a natural baseline: for the purpose of minimizing the original ERM problem,\nhow does APPA compare to solving one instance of a regularized ERM problem (using a single run\nof its inner optimizer)? In other words, to what extent does re-centering the regularizer over time\nhelp in solving the un-regularized problem? Intuitively, even if SDCA is run to convergence, some\nof the minimization is of the regularization term rather than the ERM term, hence one cannot\nweigh the regularization too heavily. Meanwhile, APPA can enjoy more ample strong convexity\nby placing a larger weight on its \u21132 term. This advantage is evident for MNIST and CIFAR in\nFigures 1 and 2: recalling that \u03bb is the same strong convexity added both by APPA and by SDCA,\nwe see that APPA takes \u03bb at least an order of magnitude larger than SDCA does, to achieve faster\nand more stable convergence towards an ultimately lower \ufb01nal value.\nFigure 1 also shows dashed lines corresponding to the ERM performance of the least-squares\n\ufb01t and of fully-optimized ridge regression, using \u03bb as that of the best APPA and SDCA runs.\nThese appear in the legend as \u201cls(\u03bb).\u201d They indicate lower bounds on the ERM value attainable\nby any algorithm that minimizes the corresponding regularized ERM objective. Lastly, test set\nclassi\ufb01cation accuracy demonstrates the extent to which a shrinkage bias is statistically desirable.\n7http://yann.lecun.com/exdb/mnist/\n8http://www.cs.toronto.edu/\u223ckriz/cifar.html\n9http://osmot.cs.cornell.edu/kddcup/datasets.html\n10Such a choice is justi\ufb01ed by the observation that doubling the stage size does not have noticeable e\ufb00ect on the\nresults discussed.\n20\n0\n5\n10\n15\n20\n# gradients / n\n1e-3\n1e-2\n1e-1\n5e-4\n2e-3\n5e-3\n2e-2\n5e-2\n0.1\nsdca(0.1)\nsvrg(1.0)\nappa(1.0)\nsgd(1.0)\nls(0)\nls(0.1)\nls(1.0)\n0\n5\n10\n15\n20\n# gradients / n\n1e-1\n2e-2\n5e-2\n2e-1\n0.02\n0.22\n(a) MNIST. Left: excess train loss F(x) \u2212F opt. Right: test error rate.\n0\n5\n10\n15\n20\n# gradients / n\n1e-2\n5e-3\n2e-2\n5e-2\n0.06\nsdca(0.1)\nsvrg(1.0)\nappa(1.0)\nsgd(1.0)\nls(0)\nls(0.1)\nls(1.0)\n0\n5\n10\n15\n20\n# gradients / n\n2e-1\n0.11\n0.28\n(b) CIFAR. Left: excess train loss F(x) \u2212F opt. Right: test error rate.\n0\n5\n10\n15\n20\n# gradients / n\n1e-4\n1e-3\n5e-5\n2e-4\n5e-4\n2e-3\n0.002\nsdca(0.1)\nsvrg(0.01)\nappa(0.1)\nsgd(1.0)\nls(0)\nls(0.1)\nls(0.1)\n0\n5\n10\n15\n20\n# gradients / n\n5e-3\n0.005\n0.009\n(c) Protein. Left: excess train loss F(x) \u2212F opt. Right: test error rate.\nFigure 1: Sub-optimality curves when optimizing under squared loss \u03c6i(z) =\n1\n2n(z \u2212bi)2.\n21\n0\n5\n10\n15\n20\n# gradients / n\n1e-2\n1e-1\n2e-2\n5e-2\n0.01\n0.15\nsdca(1e \u221208)\nsvrg(10.0)\nappa(0.1)\nsgd(1000.0)\n0\n5\n10\n15\n20\n# gradients / n\n2e-2\n5e-2\n0.02\n0.05\n(a) MNIST. Left: train loss F(x). Right: test error rate.\n0\n5\n10\n15\n20\n# gradients / n\n2e-1\n5e-1\n0.17\n0.58\nsdca(0.01)\nsvrg(10.0)\nappa(0.1)\nsgd(1000.0)\n0\n5\n10\n15\n20\n# gradients / n\n2e-1\n0.12\n0.22\n(b) CIFAR. Left: train loss F(x). Right: test error rate.\nFigure 2: Objective curves when optimizing under logistic loss \u03c6i(z) = 1\nn log(1 + e\u2212zbi).\nIn the MNIST and CIFAR holdout, we want only the small bias taken explicitly by SDCA (and\ne\ufb00ectively achieved by APPA). In the Protein holdout, we want no bias at all (again e\ufb00ectively\nachieved by APPA).\nParameter sensitivity\nBy solving only regularized ERM inner problems, SDCA and APPA\nenjoy a stable response to poor speci\ufb01cation of the biasing parameter \u03bb. Figure 3 plots the al-\ngorithms\u2019 \ufb01nal value after 20 stages, against di\ufb00erent choices of \u03bb. Overestimating the step size\nin SGD or SVRG incurs a sharp transition into a regime of divergence. Meanwhile, APPA and\nSDCA always converge, with solution quality degrading more smoothly. APPA then exhibits an\neven better degradation as it overcomes an overaggressive biasing by the 20th stage.\n22\n10\u2212210\u22121100 101 102 103 104 105 106 107 108\nparameter setting \u03bb\n1e-1\n1\n2e-2\n5e-2\n2e-1\n5e-1\n2\n\ufb01nal value\nsdca\nsvrg\nappa\nsgd\n10\u2212210\u22121100 101 102 103 104 105 106 107 108\nparameter setting \u03bb\n1e-1\n1\n5e-2\n2e-1\n5e-1\n2\n\ufb01nal value\n(a) Squared loss. Left: MNIST. Right: CIFAR.\n10\u2212210\u22121100 101 102 103 104 105 106 107 108\nparameter setting \u03bb\n1e-1\n1\n2e-2\n5e-2\n2e-1\n5e-1\n2\n5\n\ufb01nal value\nsdca\nsvrg\nappa\nsgd\n10\u2212210\u22121100 101 102 103 104 105 106 107 108\nparameter setting \u03bb\n1\n2e-1\n5e-1\n2\n5\n\ufb01nal value\n(b) Logistic loss. Left: MNIST. Right: CIFAR.\nFigure 3: Sensitivity to \u03bb: the \ufb01nal objective values attained by each algorithm, after 20 stages (or\nthe equivalent), with \u03bb chosen at di\ufb00erent orders of magnitude. SGD and SVRG exhibit a sharp\nthreshold past which they easily diverge, whereas SDCA degrades more gracefully, and Dual APPA\nyet more so.\nAcknowledgments\nPart of this work took place while RF and AS were at Microsoft Research, New England, and\nanother part while AS was visiting the Simons Institute for the Theory of Computing, UC Berkeley.\nThis work was partially supported by NSF awards 0843915 and 1111109, NSF Graduate Research\nFellowship (grant no. 1122374).\nReferences\nL. Bottou and O. Bousquet. The tradeo\ufb00s of large scale learning. In Advances in Neural Information\nProcessing Systems (NIPS), 2008.\n23\nS. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimization and statistical\nlearning via the alternating direction method of multipliers. Foundations and Trends in Machine\nLearning, 3(1):1\u2013122, 2011.\nM. B. Cohen, Y. T. Lee, C. Musco, C. Musco, R. Peng, and A. Sidford. Uniform sampling for\nmatrix approximation. In Innovations in Theoretical Computer Science (ITCS), 2015.\nA. Defazio, F. Bach, and S. Lacoste-Julien. Saga: A fast incremental gradient method with support\nfor non-strongly convex composite objectives. In Advances in Neural Information Processing\nSystems (NIPS), 2014.\nO. Guler. New proximal point algorithms for convex minimization. SIAM Journal on Optimization,\n2(4):649\u2013664, 1992.\nR. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance re-\nduction. In Advances in Neural Information Processing Systems (NIPS), 2013.\nY. T. Lee and A. Sidford. E\ufb03cient accelerated coordinate descent methods and faster algorithms\nfor solving linear systems. In Foundations of Computer Science (FOCS), 2013.\nM. Li, G. L. Miller, and R. Peng. Iterative row sampling. In Foundations of Computer Science\n(FOCS), 2013.\nH. Lin, J. Mairal, and Z. Harchaoui. A universal catalyst for \ufb01rst-order optimization. arXiv, 2015.\nQ. Lin, Z. Lu, and L. Xiao. An accelerated proximal coordinate gradient method. In Advances in\nNeural Information Processing Systems (NIPS), 2014.\nD. Needell, N. Srebro, and R. Ward.\nStochastic gradient descent, weighted sampling, and the\nrandomized kaczmarz algorithm. In Advances in Neural Information Processing Systems (NIPS),\n2014.\nJ. Nelson and H. L. Nguyen.\nOSNAP: Faster numerical linear algebra algorithms via sparser\nsubspace embeddings. In Foundations of Computer Science (FOCS), 2013.\nY. Nesterov. A method of solving a convex programming problem with convergence rate O(1/k2).\nSoviet Mathematics Doklady, 27(2):372\u2013376, 1983.\nY. Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Springer, 2004.\nN. Parikh and S. Boyd.\nProximal algorithms.\nFoundations and Trends in Optimization, 1(3):\n123\u2013231, 2014.\nA. Rahimi and B. Recht. Random features for large-scale kernel machines. In Advances in Neural\nInformation Processing Systems (NIPS), 2007.\nR. T. Rockafellar. Monotone operators and the proximal point algorithm. SIAM Journal on Control\nand Optimization, 14(5):877\u2013898, 1976.\nN. L. Roux, M. Schmidt, and F. Bach. A stochastic gradient method with an exponential conver-\ngence rate for \ufb01nite training sets. In Advances in Neural Information Processing Systems (NIPS),\n2012.\n24\nS. Shalev-Shwartz and T. Zhang. Stochastic dual coordinate ascent methods for regularized loss\nminimization. Journal of Machine Learning Research (JMLR), 14:567\u2013599, 2013.\nS. Shalev-Shwartz and T. Zhang. Accelerated proximal stochastic dual coordinate ascent for regu-\nlarized loss minimization. Mathematical Programming, pages 1\u201341, 2014.\nT. Strohmer and R. Vershynin. A randomized kaczmarz algorithm with exponential convergence.\nJournal of Fourier Analysis and Applications, 15:262\u2013278, 2009.\nV. V. Williams. Multiplying matrices faster than Coppersmith-Winograd. In Symposium on Theory\nof Computing (STOC), 2012.\nL. Xiao and T. Zhang. A proximal stochastic gradient method with progressive variance reduction.\nSIAM Journal on Optimization, 24(4):2057\u20132075, 2014.\nA\nTechnical lemmas\nIn this section we provide several stand-alone technical lemmas we use throughout the paper. First\nwe provide Lemma A.1 some common inequalities regarding smooth or strongly convex functions,\nthen Lemma A.2 which shows the e\ufb00ect of adding a linear term to a convex function, and then\nLemma A.3 a small technical lemma regarding convex combinations of quadratic functions.\nLemma A.1 (Standard bounds for smooth, strongly convex functions). Let f : Rk \u2192R be di\ufb00er-\nentiable function that obtains its minimal value at xopt.\nIf f is L-smooth then for all x \u2208Rk\n1\n2L\u2225\u2207f(x)\u22252\n2 \u2264f(x) \u2212f(xopt) \u2264L\n2 \u2225x \u2212xopt\u22252\n2 .\nIf f is \u00b5-strongly convex the for all x \u2208Rk\n\u00b5\n2 \u2225x \u2212xopt\u22252\n2 \u2264f(x) \u2212f(xopt) \u22641\n2\u00b5\u2225\u2207f(x)\u22252\n2 .\nProof. Apply the de\ufb01nition of smoothness and strong convexity at the points x and xopt and\nminimize the resulting quadratic form.\nLemma A.2. Let f : Rn \u2192R be a \u00b5-strongly convex function and for all a, x \u2208Rn let fa(x) =\nf(x) + a\u22a4x. Then\nfa(x) \u2212fopt\na\n\u22642(f(x) \u2212fopt) + 1\n\u00b5\u2225a\u22252\n2\nProof. 11 Let xopt = argminx f(x). Since f is \u00b5-strongly convex by Lemma A.1 we have f(x) \u2265\nf(xopt) + \u00b5\n2\u2225x \u2212xopt\u22252\n2 for all x. Consequently, for all x\nfopt\na\n\u2265f(x) + a\u22a4x \u2265f(xopt) + \u00b5\n2 \u2225x \u2212xopt\u22252\n2 + a\u22a4x \u2265fa(xopt) + a\u22a4(x \u2212xopt) + \u00b5\n2 \u2225x \u2212xopt\u22252\n2\n11Note we could have also proved this by appealing to the gradient of f and Lemma A.1, however the proof here\nholds even if f is not di\ufb00erentiable.\n25\nMinimizing with respect to x yields that fopt\na\n\u2265fa(xopt) \u2212\n1\n2\u00b5\u2225a\u22252\n2.\nConsequently, by Cauchy\nSchwarz, and Young\u2019s Inequality we have\nfa(x) \u2212fopt\na\n\u2264f(x) \u2212fopt + a\u22a4(x \u2212xopt) + 1\n2\u00b5\u2225a\u22252\n2\n(18)\n\u2264f(x) \u2212fopt + 1\n2\u00b5\u2225a\u22252\n2 + \u00b5\n2 \u2225x \u2212xopt\u22252\n2 + 1\n2\u00b5\u2225a\u22252\n2\n(19)\nApplying A.1 again yields the result.\nLemma A.3. Suppose that for all x we have\nf1(x) def\n= \u03c81 + \u00b5\n2 \u2225x \u2212v1\u22252\n2 and f2(x) = \u03c82 + \u00b5\n2 \u2225x \u2212v2\u22252\n2\nthen\n\u03b1f1(x) + (1 \u2212\u03b1)f2(x) = \u03c8\u03b1 + \u00b5\n2 \u2225x \u2212v\u03b1\u22252\n2\nwhere\nv\u03b1 = \u03b1v1 + (1 \u2212\u03b1)v2\nand\n\u03c8\u03b1 = \u03b1\u03c81 + (1 \u2212\u03b1)\u03c82 + \u00b5\n2 \u03b1(1 \u2212\u03b1)\u2225v1 \u2212v2\u22252\n2\nProof. Setting the gradient of \u03b1f1(x) + (1 \u2212\u03b1)f2(x) to 0 we know that v\u03b1 must satisfy\n\u03b1\u00b5 (v\u03b1 \u2212v1) + (1 \u2212\u03b1)\u00b5 (v\u03b1 \u2212v2) = 0\nand thus v\u03b1 = \u03b1v1 + (1 \u2212\u03b1)v2. Finally,\n\u03c8\u03b1 = \u03b1\nh\n\u03c81 + \u00b5\n2 \u2225v\u03b1 \u2212v1\u22252\n2\ni\n+ (1 \u2212\u03b1)\nh\n\u03c82 + \u00b5\n2 \u2225v\u03b1 \u2212v2\u22252\n2\ni\n= \u03b1\u03c81 + (1 \u2212\u03b1)\u03c82 + \u00b5\n2\n\u0002\n\u03b1(1 \u2212\u03b1)2\u2225v2 \u2212v1\u22252\n2 + (1 \u2212\u03b1)\u03b12\u2225v2 \u2212v1\u22252\n2\n\u0003\n= \u03b1\u03c81 + (1 \u2212\u03b1)\u03c82 + \u00b5\n2 \u03b1(1 \u2212\u03b1)\u2225v1 \u2212v2\u22252\n2.\nB\nRegularized ERM duality\nIn this section we derive the dual (4) to the problem of computing proximal operator for the ERM\nobjective (3) (Section B.1) and prove several bounds on primal and dual errors (Section B.2).\nThroughout this section we assume F is given by the ERM problem (1) and we make extensive use\nof the notation and assumptions in Section 1.1.\nB.1\nDual derivation\nWe can rewrite the primal problem, minx fs,\u03bb(x), as\nmin\nx\u2208Rd,z\u2208Rn\nPn\ni=1 \u03c6i(zi) + \u03bb\n2\u2225x \u2212s\u22252\n2\nsubject to\nzi = aT\ni x,\nfor i = 1, . . . , n\n.\n26\nBy convex duality, this is equivalent to\nmin\nx,{zi} max\ny\u2208Rn\nn\nX\ni=1\n\u03c6i(zi) + \u03bb\n2 \u2225x \u2212s\u22252\n2 + yT(Ax \u2212z) = max\ny\nmin\nx,{zi}\nn\nX\ni=1\n\u03c6i(zi) + \u03bb\n2 \u2225x \u2212s\u22252\n2 + yT(Ax \u2212z)\nSince\nmin\nzi {\u03c6i(zi) \u2212yizi} = \u2212max\nzi\n{yizi \u2212\u03c6i(zi)} = \u2212\u03c6\u2217\ni (yi)\nand\nmin\nx\n\u001a\u03bb\n2 \u2225x \u2212s\u22252\n2 + yTAx\n\u001b\n= yTAs + min\nx\n\u001a\u03bb\n2 \u2225x \u2212s\u22252\n2 + yTA(x \u2212s)\n\u001b\n= yTAs \u22121\n2\u03bb\u2225ATy\u22252\n2,\nit follows that the optimization problem is in turn equivalent to\n\u2212min\ny\nn\nX\ni=1\n\u03c6\u2217\ni (yi) + 1\n2\u03bb\u2225ATy\u22252\n2 \u2212sTATy.\nThis negated problem is precisely the dual formulation.\nThe \ufb01rst problem is a Lagrangian saddle-point problem, where the Lagrangian is de\ufb01ned as\nL(x, y, z) =\nn\nX\ni=1\n\u03c6i(zi) + \u03bb\n2 \u2225x \u2212s\u22252\n2 + yT(Ax \u2212z).\nThe dual-to-primal mapping (5) and primal-to-dual mapping (6) are implied by the KKT conditions\nunder L, and can be derived by solving for x, y, and z in the system \u2207L(x, y, z) = 0.\nThe duality gap in this context is de\ufb01ned as\ngaps,\u03bb(x, y) def\n= fs,\u03bb(x) + gs,\u03bb(y).\n(20)\nStrong duality dictates that gaps,\u03bb(x, y) \u22650 for all x \u2208Rd, y \u2208Rn, with equality attained when x\nis primal-optimal and y is dual-optimal.\nB.2\nError bounds\nLemma B.1 (Dual error bounds primal error). For all s \u2208Rd, y \u2208Rn, and \u03bb > 0 we have\nfs,\u03bb(bxs,\u03bb(y)) \u2212fopt\ns,\u03bb \u22642(n\u03ba\u03bb)2(gs,\u03bb(y) \u2212gopt\ns,\u03bb).\nProof. Because F is nR2L smooth, fs,\u03bb is nR2L + \u03bb smooth. Consequently, for all x \u2208Rd we have\nfs,\u03bb(x) \u2212fopt\ns,\u03bb \u2264nR2L + \u03bb\n2\n\u2225x \u2212xopt\ns,\u03bb\u22252\n2\nSince we know that xopt\ns,\u03bb = s \u22121\n\u03bbATyopt\ns,\u03bb and \u2225ATz\u22252\n2 \u2264nR2\u2225z\u22252\n2 for all z \u2208Rn we have\nfs,\u03bb(bxx,\u03bb(y)) \u2212fs,\u03bb(xopt\ns,\u03bb) \u2264nR2L + \u03bb\n2\n\u2225s \u22121\n\u03bbATy \u2212(s \u22121\n\u03bbATyopt\ns,\u03bb )\u22252\n2\n= nR2L + \u03bb\n2\u03bb2\n\u2225y \u2212yopt\ns,\u03bb \u22252\nAAT\n\u2264nR2(nR2L + \u03bb)\n2\u03bb2\n\u2225y \u2212yopt\ns,\u03bb \u22252\n2.\n(21)\n27\nFinally, since each \u03c6\u2217\ni is 1/L-strongly convex, G is 1/L-strongly convex and hence so is gs,\u03bb. There-\nfore by Lemma A.1 we have\n1\n2L\u2225y \u2212yopt\ns,\u03bb \u22252\n2 \u2264gs,\u03bb(y) \u2212gs,\u03bb(yopt\ns,\u03bb ).\n(22)\nSubstituting (22) in (21) and recalling that \u03ba\u03bb \u22651 yields the result.\nLemma B.2 (Gap for primal-dual pairs). For all s, x \u2208Rd and \u03bb > 0 we have\ngaps,\u03bb(x, by(x)) = 1\n2\u03bb\u2225\u2207F(x)\u22252\n2 + \u03bb\n2 \u2225x \u2212s\u22252\n2.\n(23)\nProof. To prove the \ufb01rst identity (23), let by = by(x) for brevity. Recall that\nbyi = \u03c6\u2032\ni(aT\ni x) \u2208argmax\nyi\n{xTaiyi \u2212\u03c6\u2217\ni (yi)}\n(24)\nby de\ufb01nition, and hence xTaibyi \u2212\u03c6\u2217\ni (byi) = \u03c6i(aT\ni x). Observe that\ngaps,\u03bb(x, by) =\nn\nX\ni=1\n\u0010\n\u03c6i(aT\ni x) + \u03c6\u2217\ni (byi)\n\u0011\n\u2212xTATby + 1\n2\u03bb\u2225ATby\u22252 + \u03bb\n2\u2225x \u2212s\u22252\n=\nn\nX\ni=1\n\uf8eb\n\uf8ec\n\uf8ed\u03c6i(aT\ni x) + \u03c6\u2217\ni (byi) \u2212xTaibyi\n|\n{z\n}\n=0 (by (24))\n\uf8f6\n\uf8f7\n\uf8f8+ 1\n2\u03bb\u2225ATby\u22252 + \u03bb\n2\u2225x \u2212s\u22252\n=\n1\n2\u03bb\u2225ATby\u22252 + \u03bb\n2\u2225x \u2212s\u22252\n=\n1\n2\u03bb\u2225\nn\nX\ni=1\nai\u03c6\u2032\ni(aT\ni x)\u22252 + \u03bb\n2\u2225x \u2212s\u22252\n=\n1\n2\u03bb\u2225\u2207F(x)\u22252 + \u03bb\n2\u2225x \u2212s\u22252.\nCorollary B.3 (Initial dual error). For all s, x \u2208Rd and \u03bb > 0 we have\ngx,\u03bb(by(x)) \u2212gopt\nx,\u03bb \u22642\u03ba\u03bb\n\u0010\nfx,\u03bb(x) \u2212fopt\nx,\u03bb\n\u0011\nProof. By Lemma B.2 we have\ngapx,\u03bb(x, by(x)) = 1\n2\u03bb\u2225\u2207F(x)\u22252\n2 + \u03bb\n2 \u2225x \u2212x\u22252\n2 = 1\n2\u03bb\u2225\u2207F(x)\u22252\n2\nNow clearly \u2207F(x) = \u2207fx,\u03bb(x). Furthermore, since fx,\u03bb(x) is (nLR2 + \u03bb)-smooth by Lemma A.1\nwe have \u2225\u2207fx,\u03bb(x)\u2225\u22642(nLR2 + \u03bb)(fx,\u03bb(x) \u2212fopt\nx,\u03bb ). Consequently,\ngx,\u03bb(by(x)) \u2212gopt\nx,\u03bb \u2264gapx,\u03bb(x, by(x)) \u22642(nLR2 + \u03bb)\n2\u03bb\n\u0010\nfx,\u03bb(x) \u2212fopt\nx,\u03bb\n\u0011\n.\nRecalling the de\ufb01nition of \u03ba\u03bb and the fact that 1 \u2264\u03ba\u03bb yields the result.\n28\n",
        "sentence": " Then, using the Catalyst/APPA acceleration scheme [14, 23], the above running time can be improved to \u00d5 ( nnz(\u03a3k\u22121) + \u221a \u03b7k \u00b7maxi\u2208[k\u22121]{nnz(\u03a3k\u22121)nnz(Ai)} ) .",
        "context": "oracle and taking \u03bb = 2\u00b5 + LR2 yields the running time bound eO(nd\u221a\u03ba log(\u03f50/\u03f5)) for the general\nERM problem (2).\n3.2\nAnalysis\nHere we establish the convergence rate of Algorithm 2, Accelerated APPA, and prove Theorem 3.1.\nfactor depends polynomial on the ratio of \u03bb and \u00b5.\nAlgorithm 2 Accelerated APPA\ninput x(0) \u2208Rd, \u00b5 > 0, \u03bb > 2\u00b5\ninput primal (4\u03c13/2, \u03bb)-oracle P, where \u03c1 = \u00b5+2\u03bb\n\u00b5\nDe\ufb01ne \u03b6 = 2\n\u00b5 + 1\n\u03bb\nv(0) \u2190x(0)\nfor t = 0, . . . , T \u22121 do\ny(t) \u2190\n\u0010\n1\n1+\u03c1\u22121/2\n\u0011\nx(t) +\n\u0010\n\u03c1\u22121/2\nobjective function f, and accelerate the APPA algorithm, comprising the bulk of the analysis.\n\u2022 In Lemma 3.5 we show that the requirements of Lemma 3.4 can be met by using a primal\noracle that decreases the error by a constant factor."
    },
    {
        "title": "Fast and simple PCA via convex optimization",
        "author": [
            "Dan Garber",
            "Elad Hazan"
        ],
        "venue": "ArXiv e-prints,",
        "citeRegEx": "15",
        "shortCiteRegEx": "15",
        "year": 2015,
        "abstract": "The problem of principle component analysis (PCA) is traditionally solved by\nspectral or algebraic methods. We show how computing the leading principal\ncomponent could be reduced to solving a \\textit{small} number of\nwell-conditioned {\\it convex} optimization problems. This gives rise to a new\nefficient method for PCA based on recent advances in stochastic methods for\nconvex optimization.\n  In particular we show that given a $d\\times d$ matrix $\\X =\n\\frac{1}{n}\\sum_{i=1}^n\\x_i\\x_i^{\\top}$ with top eigenvector $\\u$ and top\neigenvalue $\\lambda_1$ it is possible to: \\begin{itemize} \\item compute a unit\nvector $\\w$ such that $(\\w^{\\top}\\u)^2 \\geq 1-\\epsilon$ in\n$\\tilde{O}\\left({\\frac{d}{\\delta^2}+N}\\right)$ time, where $\\delta = \\lambda_1\n- \\lambda_2$ and $N$ is the total number of non-zero entries in\n$\\x_1,...,\\x_n$,\n  \\item compute a unit vector $\\w$ such that $\\w^{\\top}\\X\\w \\geq\n\\lambda_1-\\epsilon$ in $\\tilde{O}(d/\\epsilon^2)$ time. \\end{itemize} To the\nbest of our knowledge, these bounds are the fastest to date for a wide regime\nof parameters. These results could be further accelerated when $\\delta$ (in the\nfirst case) and $\\epsilon$ (in the second case) are smaller than $\\sqrt{d/N}$.",
        "full_text": "arXiv:1509.05647v4  [math.OC]  25 Nov 2015\nFast and Simple PCA via Convex Optimization\nDan Garber\nTechnion - Israel Institute of Technology\ndangar@tx.technion.ac.il\nElad Hazan\nPrinceton University\nehazan@cs.princeton.edu\nAbstract\nThe problem of principle component analysis (PCA) is traditionally solved by spectral\nor algebraic methods. We show how computing the leading principal component could be\nreduced to solving a small number of well-conditioned convex optimization problems. This\ngives rise to a new e\ufb03cient method for PCA based on recent advances in stochastic methods\nfor convex optimization.\nIn particular we show that given a d \u00d7 d matrix X = 1\nn\nPn\ni=1 xix\u22a4\ni with top eigenvector\nu and top eigenvalue \u03bb1 it is possible to:\n\u2022 compute a unit vector w such that (w\u22a4u)2 \u22651 \u2212\u01eb in \u02dcO\n\u0000 d\n\u03b42 + N\n\u0001\ntime, where \u03b4 =\n\u03bb1 \u2212\u03bb2 and N is the total number of non-zero entries in x1, ..., xn,\n\u2022 compute a unit vector w such that w\u22a4Xw \u2265\u03bb1 \u2212\u01eb in \u02dcO(d/\u01eb2) time.\nTo the best of our knowledge, these bounds are the fastest to date for a wide regime of\nparameters. These results could be further accelerated when \u03b4 (in the \ufb01rst case) and \u01eb (in\nthe second case) are smaller than\np\nd/N.\n1\nIntroduction\nSince its introduction by Pearson [20] and Hotelling [13], the principle component analysis\ntechnique of \ufb01nding the subspace of largest variance in the data has become ubiquitous in\nunsupervised learning, feature generation and data visualization.\nFor data given as a set of n vectors in Rd, x1, ..., xn, denote by X the normalized covariance\nmatrix X = 1\nn\nPn\ni=1 xix\u22a4\ni . The PCA method \ufb01nds the k-dimensional subspace such that the\nprojection of the data onto the subspace has largest variance. Formally, let W \u2208Rd\u00d7k be an\northogonal projection matrix, PCA could be formalized as the following optimization problem.\nmax\nW\u2208Rd\u00d7k ,WT W=I \u2225XW\u22252\nF ,\n(PCA)\nwhere \u2225\u00b7\u2225F is the Frobenius norm. Note that the above optimization problem is an inherently\nnon-convex optimization problem even for k = 1. Henceforth we focus on the problem of \ufb01nding\nwith high precision only the leading principal component, i.e. on the case k = 1.\nFinding the leading principal component could be carried out using matrix factorization\ntechniques in time O(nd2 + d3), by explicitly computing the matrix X and then computing its\nsingular value decomposition (SVD). However this requires super-linear time and potentially\nO(d2) space.\nSince super-linear times are prohibitive for large scale machine learning problems, approx-\nimation methods have been the focus of recent literature. Iterative eigenvector methods, such\nas the Lanczos method or Power method [10], can be applied without explicitly computing the\nmatrix X. These latter methods require the data to be well-conditioned, and the spectral gap,\n1\ni.e., the distance between largest and second largest eigenvalues of X, to be bounded away from\nzero.\nIf we let \u03b4 > 0 denote this spectral gap, then the Power and Lanczos methods requires\nroughly (\u03bb1/\u03b4)\u2212\u03b3 passes over the entire data to compute the leading PC which amounts to\n\u02dcO((\u03bb1/\u03b4)\u03b3N) total time, where \u03b3 = 1 for the Power method and \u03b3 = 1/2 for the Lanczos\nmethod, and N it total number of non-zero entries in the vectors x1, ..., xn.\nThus, iterative methods replace the expensive super-linear operations of matrix factorization\nby linear-time operations, but require several passes over the data. Depending on the spectral\ngap of X, the latter methods may also be prohibitively expensive. This motivates the study of\nmethods that retain the best from both worlds: linear time per iteration, as well as doing only\na small number (i.e., at most logarithmic in the natural parameters of the problem) of passes\nover the entire data.\nIn the convex optimization world, such hybrid results with simple iterations coupled with\nonly a few passes over the data were obtained in recent years [14, 18, 15]. Recently Shamir\n[24, 25] made headway in the non-convex spectral world by suggesting a stochastic optimiza-\ntion method for the problem, that is based Oja\u2019s randomized Power method [19, 4] with an\napplication of the variance reduction principle demonstrated in [14].\nShamir\u2019s algorithm runs in total time of \u02dcO(\u03b4\u22122d + N), assuming the availability of a unit\nvector w such that (w\u22a4u)2 = \u2126(1), called a \u201cwarm start\u201d. Finding such a warm start vector w\ncould take as much as \u02dcO(\np\n\u03bb1/\u03b4\u03b4\u22122d) time using existing methods 1, hence the total running\ntime becomes \u02dcO(\np\n\u03bb1/\u03b4\u03b4\u22122d+N). Quite remarkably, Shamir\u2019s result separates the dependency\non the gap \u03b4 and the data size N.\nAnalyzing the Power method in case of stochastic updates, as done in [4, 24], is much more\nintricate than analyzing stochastic gradient algorithms for convex optimization, and indeed both\nthe analysis of Oja\u2019s algorithm in [19] and Shamir\u2019s algorithm in [24, 25] are quite elaborate.\nIn this paper we continue the search for e\ufb03cient algorithms for PCA. Our main contribu-\ntion, at a high-level, is in showing that the problem of computing the largest eignevector of\na positive semide\ufb01nite matrix could be reduced to solving only a poly-logarithmic number of\nwell conditioned unconstrained smooth and strongly convex optimization problems. These well-\nconditioned convex optimization problems could then be solved by a variety of algorithms that\nare available in the convex and stochastic optimization literature, and in particular, the recent\nalgorithmic advances in stochastic convex optimization, such as the results in [14, 18, 15, 12, 7].\nAs a result, we derive algorithms that in terms of running time, their worst case running time\nstarting from a \u201ccold-start\u201d is equivalent or better than that of the algorthm of Shamir ini-\ntialized with a \u201cwarm-start\u201d .\n1.1\nProblem setting and main results\nAssume we are given a set of n vectors in Rd, x1, ..., xn, where n > d and let us denote by X\nthe normalized covariance matrix X = 1\nn\nPn\ni=1 xix\u22a4\ni . Assume further that X has an eigengap\nof \u03b4, i.e. \u03bb1(X) \u2212\u03bb2(X) = \u03b4, and w.l.o.g. that \u2225xi\u2225\u22641 for all i \u2208[n]. Our goal is to \ufb01nd a\nunit vector w such that\n(w\u22a4u)2 \u22651 \u2212\u01eb,\nwhere u is the leading eigenvector of X and \u01eb is the desired accuracy parameter.\n1Finding a \u201cwarm start\u201d could be carried out either by applying iterative methods such as Power and Lanczos\nmethods to the entire data or by applying them only to a small random sample of the data. Since we want to be\noverall competitive with these methods, we focus on the second option.\n2\nIn the rest of the paper we denote the eigenvalues of X in descending order by \u03bb1 \u2265\u03bb2 \u2265\n... \u2265\u03bbd and by u = u1, u2, ..., ud the corresponding eigenvectors. We denote by N the total\nnumber of non-zero entries in the vectors x1, ..., xn.\nWe assume we are given an estimate \u02c6\u03b4 such that\nc1\u03b4 \u2264\u02c6\u03b4 \u2264c2\u03b4,\nfor some universal constants c1, c2 which satisfy c2 \u2212c1 = \u0398(1). Note that \ufb01nding such a \u02c6\u03b4\ncould be done in time that is logarithmic in 1/\u03b4.\nThe main result of this paper is the proof of the following high-level proposition.\nProposition 1. There exists an algorithm that after solving a poly-logarithmic number of well-\nconditioned unconstrained convex optimization problems, returns a unit vector w, such that\n(w\u22a4u)2 \u22651 \u2212\u01eb.\nBased on this proposition we prove the following theorems.\nTheorem 1.1. Fix \u01eb > 0, p > 0. There exists an algorithm that \ufb01nds with probability at least\n1 \u2212p a unit vector w such that (w\u22a4u)2 \u22651 \u2212\u01eb, in total time \u02dcO\n\u0000 d\n\u03b42 + N\n\u0001\n.\nTheorem 1.2. Fix \u01eb > 0, p > 0. Assume that \u03b4 = o(\np\nd/N). There exists an algorithm that\n\ufb01nds with probability at least 1 \u2212p a unit vector w such that (w\u22a4u)2 \u22651 \u2212\u01eb, in total time\n\u02dcO\n\u0010\nN3/4d1/4\n\u221a\n\u03b4\n\u0011\n.\nThroughout this work we use the notation \u02dcO(\u00b7) to hide poly-logarithmic dependencies on\nd, \u01eb\u22121, p\u22121, \u03b4\u22121.\nMethod\nComplexity\nComments\nPower method [10]\n\u03bb1\n\u03b4 N\nLanczos [10]\nq\n\u03bb1\n\u03b4 N\nVR-PCA [24, 25]\nq\n\u03bb1\n\u03b4\nd\n\u03b42 + N\nTheorem 1.1\nd\n\u03b42 + N\nfastest when \u03b4 = \u2126(\np\nd/N)\nTheorem 1.2 (\u03b4 = o(\np\nd/N))\nN3/4d1/4\n\u221a\n\u03b4\nfastest when \u03bb1 = \u03c9(\np\nd/N), \u03b4 = o(\np\nd/N)\nTable 1: Comparison with previous eigengap-based results. Note that the result of [24, 25]\napply in general only from a \u201cwarm-start\u201d, i.e. when initialized with a unit vector w0 such that\n(w\u22a4\n0 u)2 = \u2126(1). Finding such a warm-start could be carried out e\ufb03ciently using a sample of\nroughly 1/\u03b42 matrices xix\u22a4\ni and applying the Lanczos method to this sample.\n1.1.1\nGap-free results\nIt makes since to not only consider the problem of approximating the leading eigenvector of\nthe matrix X, but also the problem of approximating the corresponding eignevalue, that is the\nproblem of \ufb01nding a unit vector w such that\nw\u22a4Xw \u2265\u03bb1 \u2212\u01eb.\nFor this purpose we also prove the following theorems which are analogous to Theorems 1.1,\n1.2.\n3\nTheorem 1.3. Fix \u01eb > 0, p > 0. There exists an algorithm that \ufb01nds with probability at least\n1 \u2212p a unit vector w such that w\u22a4Xw \u2265\u03bb1 \u2212\u01eb, in total time \u02dcO\n\u0000 d\n\u01eb2\n\u0001\n.\nTheorem 1.4. Fix \u01eb = o(\np\nd/N), p > 0. There exists an algorithm that \ufb01nds with probability\nat least 1 \u2212p a unit vector w such that w\u22a4Xw \u2265\u03bb1 \u2212\u01eb, in total time \u02dcO\n\u0010\nN3/4d1/4\n\u221a\u01eb\n\u0011\n.\nWe note that Theorem 1.3, in which the bound is independent of n, applies also in the case\nwhen X is not given explicitly, but only through a stochastic oracle that when queried, returns\na rank-one matrix xix\u22a4\ni\nsampled at random from an unknown distribution D that satis\ufb01es\nX = Ex\u223cD[xx\u22a4]. See section 6 for more details.\nMethod\nComplexity\nComments\nPower method [17]\n\u03bb1\n\u01eb N\nLanczos [17]\nq\n\u03bb1\n\u01eb N\nSample PM\n\u03bb1\n\u01eb\nd\n\u01eb2\nSample Lanczos\nq\n\u03bb1\n\u01eb\nd\n\u01eb2\nTheorem 1.3\nd/\u01eb2\nfastest when \u01eb = \u2126\n\u0012\nmax{\nd2/3\n\u03bb1/3\n1\nN1/3 ,\np\nd/N}\n\u0013\nTheorem 1.4 (\u01eb = o(\np\nd/N))\nN3/4d1/4\n\u221a\u01eb\nfastest when \u03bb1 = \u03c9(\np\nd/N), \u01eb = o(\np\nd/N)\nTable 2: Comparison with previous eigengap-free results. Sample PM and Sample Lanczos refer\nto the application of the standard Power Method and Lanczos algorithms to a random sample\nof size roughly \u01eb\u22122, chosen uniformly at random from the matrices {xix\u22a4\ni }n\ni=1. Such a sample\nsu\ufb03ces to approximate the top eigenvalue to precision \u01eb (using standard concentration results\nfor matrices such as the Matrix Hoe\ufb00ding inequality [26]).\nThe rest of this paper is organized as follows. In Section 2 we give relevant preliminaries\non classical methods for the computation of the largest eigenvector and related tools, and also\npreliminaries on convex optimization. In Section 3 we present the core idea for our algorithms:\na shrinking inverse power method algorithm that computes the leading eigenvector after com-\nputing only a poly-logarithmic number of matrix-vector products. Based on this algorithm,\nin Section 4 we present our convex optimization-based eigenvector algorithm that requires to\nsolve only a poly-logarithmic number of well-conditioned convex unconstrained optimization\nproblems in order to compute the largest eigenvector. In Section 5 we combine the result of\nSection 4 with recent fast stochastic methods for convex optimization to prove Theorems 1.1,\n1.2. In Section 6 we prove Theorems 1.3, 1.4 .\n2\nPreliminaries\n2.1\nClassical algorithms for the leading eigenvector problem\n2.1.1\nThe Power Method\nOur main algorithmic tool for proving the convergence of our method is the classical Power\nMethod for approximating the leading eigenvalue and eigenvector of a positive de\ufb01nite matrix.\n4\nAlgorithm 1 Power Method\n1: Input: a positive de\ufb01nite matrix M\n2: Let w0 to be a random unit vector\n3: for t = 1, 2, ... do\n4:\nwt \u2190\nMwt\u22121\n\u2225Mwt\u22121\u2225\n5: end for\nIn our analysis we will require the following theorem that gives guarantees on the approx-\nimation error of Algorithm 1.\nBoth parts of the theorem follows essentially form the same\nanalysis. Part one upper bounds the number of iterations of the algorithm required to achieve\na crude approximation to the leading eigenvalue, and part two upper bounds the number of\niterations required to achieve a high-precision approximation for the leading eigenvector.\nTheorem 2.1. Let M be a positive de\ufb01nite matrix and denote its eigenvalues in descending\norder by \u03bb1, \u03bb2, ..., \u03bbd, and let u1, u2, ..., ud denote the corresponding eigenvectors. Denote \u03b4 =\n\u03bb1 \u2212\u03bb2 and \u03ba = \u03bb1\n\u03b4 . Fix an error tolerance \u01eb > 0 and failure probability p > 0. De\ufb01ne:\nT PM\ncrude(\u01eb, p) = \u23081\n\u01eb ln\n\u001218d\np2\u01eb\n\u0013\n\u2309,\nT PM\nacc (\u03ba, \u01eb, p) = \u2308\u03ba\n2 ln\n\u0012 9d\np2\u01eb\n\u0013\n\u2309.\nThen, with probability 1 \u2212p it holds that\n1. (crude regime) \u2200t \u2265T PM\ncrude(\u01eb, p): w\u22a4\nt Mwt \u2265(1 \u2212\u01eb)\u03bb1.\n2. (accurate regime) \u2200t \u2265T PM\nacc (\u03ba, \u01eb, p): (w\u22a4\nt u1)2 \u22651 \u2212\u01eb.\nIn both cases, the success probability depends only on the random variable (w\u22a4\n0 u1)2.\nA proof is given in the appendix for completeness.\n2.1.2\nThe Inverse Power Method and Conditioning\nAs seen in Theorem 2.1, for a given matrix X, the convergence rate of the Power Method\nalgorithm is strongly connected with the condition number \u03ba(X) = \u03bb1(X)\n\u03b4(X) which can be quite\nlarge. Consider now the following matrix\nM\u22121 := (\u03bbI \u2212X)\u22121 ,\nwhere \u03bb is a parameter.\nNote that if X is positive semide\ufb01nite and \u03bb > \u03bb1(X), then M\u22121 is positive de\ufb01nite. Further-\nmore, the eigenvectors of M\u22121 are the same as those of X and its eigenvalue are (in descending\norder) \u03bbi(M\u22121) =\n1\n\u03bb\u2212\u03bbi(X). Thus, if our goal is to compute the leading eigenvector of X we\nmight as well compute the leading eigenvector of M\u22121. This is also known as the Inverse Method\n[10].\nThe following lemma shows that with a careful choice for the parameter \u03bb, we can make the\ncondition number \u03ba(M\u22121) = \u03bb1(M\u22121)\n\u03b4(M\u22121) to be much smaller than that of the original matrix X.\nLemma 2.1 (Inverse Conditioning). Fix a scalar a > 0. Let M\u22121 = (\u03bbI \u2212X)\u22121 such that\n\u03bb1(X) + a\u03b4(X) \u2265\u03bb > \u03bb1(X). It holds that\n\u03bb1(M\u22121)\n\u03b4(M\u22121) \u22641 + a.\n5\nProof. We denote by \u03bb1, \u03bb2, \u03b4 the values \u03bb1(X), \u03bb2(X) and \u03b4(X) respectively.\nIt holds that\n\u03bb1(M\u22121) =\n1\n\u03bb \u2212\u03bb1\n,\n\u03bb2(M\u22121) =\n1\n\u03bb \u2212\u03bb2\n.\nThus we have that\n\u03bb1(M\u22121)\n\u03b4(M\u22121)\n=\n1\n\u03bb \u2212\u03bb1\n\u0012\n1\n\u03bb \u2212\u03bb1\n\u2212\n1\n\u03bb \u2212\u03bb2\n\u0013\u22121\n=\n1\n\u03bb \u2212\u03bb1\n\u0012\n1\n\u03bb \u2212\u03bb1\n\u2212\n1\n\u03bb \u2212\u03bb1 + \u03b4\n\u0013\u22121\n=\n1\n\u03bb \u2212\u03bb1\n\u0012\n\u03b4\n(\u03bb \u2212\u03bb1)(\u03bb \u2212\u03bb1 + \u03b4)\n\u0013\u22121\n=\n\u03bb \u2212\u03bb1 + \u03b4\n\u03b4\n\u22641 + a\u03b4\n\u03b4 = 1 + a,\nand the lemma follows.\nOf course, a problem with the above suggested approach is that we don\u2019t know how to\nset the parameter \u03bb to be close enough to \u03bb1(X). The following simple lemma shows that by\napproximating the largest eignevalue of (\u03bbI\u2212X)\u22121, we can derive bounds on the suboptimality\ngap \u03bb \u2212\u03bb1(X), which in turn can be used to better tune \u03bb.\nLemma 2.2. Fix a positive semide\ufb01nite matrix X and denote the largest eigenvalue by \u03bb1. Let\n\u03bb > \u03bb1 and consider a unit vector w such that\nw\u22a4(\u03bbI \u2212X)\u22121 w \u2265(1 \u2212\u01eb)\u03bb1\n\u0010\n(\u03bbI \u2212X)\u22121\u0011\n,\nfor some \u01eb \u2208(0, 1].\nDenote \u2206=\n1\u2212\u01eb\nw\u22a4(\u03bbI\u2212X)\u22121w. Then it holds that\n(1 \u2212\u01eb)(\u03bb \u2212\u03bb1) \u2264\u2206\u2264\u03bb \u2212\u03bb1.\nProof. According to our assumption on w it holds that\n\u03bb1\n\u0000(\u03bbI \u2212X)\u22121\u0001\n\u2265w\u22a4(\u03bbI \u2212X)\u22121 w \u2265(1 \u2212\u01eb)\u03bb1\n\u0010\n(\u03bbI \u2212X)\u22121\u0011\n.\nThus by our de\ufb01nition of \u2206and the fact that \u03bb1\n\u0000\u03bbI \u2212X)\u22121\u0001\n=\n1\n\u03bb\u2212\u03bb1 , it holds that\n(1 \u2212\u01eb)(\u03bb \u2212\u03bb1) \u2264\u2206\u2264\u03bb \u2212\u03bb1,\nand the lemma follows.\nThus, if we set for instance \u01eb = 1/2, and then de\ufb01ne \u03bb\u2032 = \u03bb \u2212\u2206, then by Lemma 2.2 we\nhave that \u03bb\u2032 \u2212\u03bb1 \u2264\u03bb\u2212\u03bb1\n2\n.\n2.2\nMatrix Inversion via Convex Optimization\n2.2.1\nSmoothness and strong convexity of continuous functions\nDe\ufb01nition 2.1 (smooth function). We say that a function f : Rd \u2192R is \u03b2 smooth if for all\nx, y \u2208Rd it holds that\n\u2225\u2207f(x) \u2212\u2207f(y)\u2225\u2264\u03b2\u2225x \u2212y\u2225.\n6\nDe\ufb01nition 2.2 (strongly convex function). We say that a function f : Rd \u2192R is \u03b1-strongly\nconvex if for all x, y \u2208Rd it holds that\nf(y) \u2265f(x) + \u2207f(x)\u22a4(y \u2212x) + \u03b1\n2 \u2225x \u2212y\u22252.\nThe above de\ufb01nition combined with \ufb01rst order optimality conditions imply that for an \u03b1-\nstrongly convex function f, if x\u2217= arg minx\u2208Rd f(x), then for any x \u2208Rd it holds that\nf(x) \u2212f(x\u2217) \u2265\u03b1\n2 \u2225x \u2212x\u2217\u22252.\n(1)\nLemma 2.3 (smoothness and strong convexity of quadratic functions). Consider the function\nf(x) = 1\n2x\u22a4Mx + b\u22a4x,\nwhere M \u2208Rd\u00d7d is symmetric and b \u2208Rd.\nIf M \u2ab00 then f(x) is \u03bb1(M)-smooth and\n\u03bbd(M)-strongly convex, where \u03bb1, \u03bbd denote the largest and smallest eigenvalues of M respec-\ntively.Otherwise, f(x) is \u2225M\u2225-smooth.\n2.2.2\nMatrix inversion via convex optimization\nIn order to apply the Inverse Method discussed in the previous subsection, we need to apply\nPower Method steps (Algorithm 1) to the matrix (\u03bbI \u2212X)\u22121. Denote M = \u03bbI \u2212X. Thus on\neach iteration of the Inverse Method we need to compute a matrix-vector product of the form\nM\u22121w. This requires in general to solve a linear system of equations. However, it could be also\napproximated arbitrarily well using convex optimization. Consider the following optimization\nproblem:\nmin\nz\u2208Rd{F(z) := 1\n2z\u22a4Mz \u2212w\u22a4z}.\n(2)\nBy the \ufb01rst order optimality condition, we have that an optimal solution for Problem (2) -\nz\u2217satis\ufb01es that \u2207F(z\u2217) = Mz\u2217\u2212w = 0, meaning,\nz\u2217= M\u22121w.\nNote that under the assumption that \u03bb > \u03bb(X) (as stated Subsection 2.1.2) it holds that\nM is positive de\ufb01nite and hence invertible.\nMost importantly, note that under this assumption on \u03bb it further follows from Lemma 2.3\nthat F(z) is \u03bbd(M) = (\u03bb \u2212\u03bb1(X))-strongly convex and \u03bb1(M) = (\u03bb \u2212\u03bbd(X))-smooth and thus\ncould be minimized very e\ufb03ciently via algorithms for convex minimization.\nSince by using algorithms for convex minimization we can only \ufb01nd an approximated-\nminimizer of F(z), we must discuss the e\ufb00ect of the approximation error on the convergence of\nthe proposed algorithms. As it will turn out, the approximation error that we will care about it\nthe distance \u2225z \u2212z\u2217\u2225where z is an approximated minimizer of F. The following lemma, which\nfollows directly from the strong convexity of F and Eq. (1), ties between the approximation\nerror of a point z with respect to the function F and the distance to the optimal solution z\u2217.\nLemma 2.4. Given a positive semide\ufb01nite matrix X, a vector w, a scalar \u03bb such that \u03bb > \u03bb1(X)\nand an error tolerance \u01eb, let M = \u03bbI \u2212X, and denote by z\u2217the minimizer of F(z) - as de\ufb01ned\nin Eq. (2). Then, for any z it holds that\n\u2225z \u2212z\u2217\u2225\u2264\ns\n2(F(z) \u2212F(z\u2217))\n\u03bb \u2212\u03bb1(X)\n.\n7\n2.2.3\nFast stochastic gradient methods for smooth and strongly convex optimiza-\ntion\nIn this subsection we brie\ufb02y survey recent developments in stochastic optimization algorithms\nfor convex optimization which we leverage in our analysis in order to get the fast rates for PCA.\nConsider an optimization problem of the following form\nmin\nz {F(z) := 1\nn\nn\nX\ni=1\nfi(z)}\n(P)\nwhere we assume that each fi is convex and \u03b2-smooth and that the sum F(z) is \u03c3-strongly\nconvex.\nWe are going to show that the PCA problem could be reduced to solving a series of convex\noptimization problems that takes the form of Problem (P) (this is actually not a precise state-\nment since in our case each function fi won\u2019t be convex on its own and we will need to address\nthis issue). Thus, we are interested in fast algorithms for solving Problem (P).\nThe standard gradient descent method can solve Problem (P) to \u01eb precision in O ((\u03b2/\u03c3) log(1/\u01eb))\niterations were each iteration requires to compute the gradient of F(z). Thus the overall time\nbecomes \u02dcO\n\u0010\n\u03b2\n\u03c3TG\n\u0011\nwhere we denote by TG the time to evaluate the gradient direction of F.\nof any of the functions fi. The dependence on the condition number \u03b2\n\u03c3 could be dramatically\nimproved without increasing signi\ufb01cantly the per-iteration complexity, by using Nesterov\u2019s ac-\ncelerated method that requires O\n\u0010p\n\u03b2/\u03c3 log(1/\u01eb)\n\u0011\niterations, and overall time of \u02dcO\n\u0010p\n\u03b2/\u03c3TG\n\u0011\nHowever, both methods could be quite computationally expensive when both \u03b2\n\u03c3 and Tg are\nlarge.\nAnother alternative is to use stochastic gradient descent, which on each iteration t, performs\na gradient improvement step based on a single function fit where it is chosen uniformly at\nrandom from [n]. This single random gradient serves as an estimator for the full gradient. The\nbene\ufb01t of this method is that each iteration is extremely cheap - only requires to evaluate the\ngradient of a single function. However the convergence rate of this method is roughly 1/(\u03c3\u01eb)\nwhich is ine\ufb00ective when \u01eb is very small. The intuitive reason for the slow convergence is the\nlarge variance of the gradient estimator.\nEach of the methods mentioned above, the deterministic gradient descent, and the stochastic\none has its own caveats.\nThe deterministic gradient method does not consider the special\nstructure of Problem (P) which is given by a sum of functions, and on the other hand, the\nstochastic gradient method does not exploit the fact that the sum of functions is \ufb01nite.\nRecently, a new family of stochastic gradient descent-based methods was devised, which is\ntailored to tackling Problem (P) [21, 23, 14, 18, 15, 12, 7]. Roughly speaking, these methods\napply cheap stochastic gradient descent update steps, but use the fact the the objective function\nis given in the form a \ufb01nite sum, to construct a gradient estimator with reduced variance.\nFor instance, the SVRG algorithm presented in [14], requires O(log(1/\u01eb)) iterations to\nreach \u01eb accuracy, where each iteration requires computing a single full gradient of F(z) and\nroughly O(\u03b2/\u03c3) cheap stochastic gradient updates.\nThus the total running time becomes\nO\n\u0010\u0010\n\u03b2\n\u03c3Tg + TG\n\u0011\nlog(1/\u01eb)\n\u0011\n, where Tg denotes the worst case time to evaluate the gradient of\na single function fi.\nThe following theorem summarizes the application of SVRG to solving Problem (P). For\ndetails see [14].\n8\nTheorem 2.2 (Convergnec of SVRG). Fix \u01eb > 0. Assume each fi(z) is \u03b2-smooth and F(z) is\n\u03c3-strongly convex. Then the SVRG Algorithm detailed in [14] \ufb01nds in total time\nO\n\u0012\u0012\u03b2\n\u03c3Tg + TG\n\u0013\nlog(1/\u01eb)\n\u0013\na vector \u02c6z \u2208Rd such that\nE[F(\u02c6z)] \u2212min\nz\u2208Rd F(z) \u2264\u01eb.\nIn recent works [7, 12], it was demonstrated how methods such as the SVRG algorithm could\nbe further accelerated by improving the dependence on the condition number \u03b2/\u03c3.\n3\nThe Basic Approach: a Shrinking Inverse Power Algorithm\nThe goal of this section is to present a Power Method-based algorithm that requires to compute\nan overall poly-logarithmic number of matrix-vector products in order to \ufb01nd an \u01eb approxi-\nmation for the leading eigenvector of a given positive semide\ufb01nite matrix X.\nAlgorithm 2 Shrinking Inverse Power Method\n1: Input: matrix X \u2208Rn\u00d7n such that X \u2ab00, \u03bb1(X) \u22641, an estimate \u02c6\u03b4 for \u03b4(X), accuracy\nparameter \u01eb \u2208(0, 1)\n2: \u03bb(0) \u21901 + \u02c6\u03b4\n3: s \u21900\n4: repeat\n5:\ns \u2190s + 1\n6:\nLet Ms = (\u03bb(s\u22121)I \u2212X)\n7:\nApply the Power Method (Algorithm 1) to the matrix M\u22121\ns\nto \ufb01nd a unit vector ws\nsuch that\nw\u22a4\ns M\u22121\ns ws \u22651\n2\u03bb1(M\u22121\ns )\n8:\n\u2206s \u21901\n2 \u00b7\n1\nw\u22a4\ns M\u22121\ns\nws\n9:\n\u03bb(s) \u2190\u03bb(s\u22121) \u2212\u2206s\n2\n10: until \u2206s \u2264\u02c6\u03b4\n11: \u03bb(f) \u2190\u03bb(s)\n12: Let Mf = (\u03bb(f)I \u2212X)\n13: Apply the Power Method (Algorithm 1) to the matrix M\u22121\nf\nto \ufb01nd a unit vector wf\nsuch that\n(w\u22a4\nf u)2 \u22651 \u2212\u01eb\n14: return wf\nWe prove the following theorem.\nTheorem 3.1. Assume \u02c6\u03b4 satis\ufb01es that \u02c6\u03b4 \u2208\n\u0002 \u03b4\n2, 2\u03b4\n\u0003\n. There exists an implementation for Algo-\nrithm 2 that requires computing at most\nO\n\u0012\nlog(d/p) log(\u03b4\u22121) + log\n\u0012 d\np\u01eb\n\u0013\u0013\n= \u02dcO(1)\n9\nmatrix-vector products of the form M\u22121w, where M is one of the matrices computed during the\nrun of the algorithm (Ms or Mf) and w is some vector, such that with probability at least 1\u2212p\nit holds that the output of the algorithm, the vector wf, satis\ufb01es:\n(w\u22a4\nf u)2 \u22651 \u2212\u01eb.\nIn order to prove Theorem 3.1 we need a few simple Lemmas.\nFirst, it is important that throughout the run of Algorithm 2, the matrices Ms and the\nmatrix Mf will be positive de\ufb01nite (and as a results so are their inverses). The following lemma\nshows that this is indeed the case.\nLemma 3.1. For all s \u22650 it holds that \u03bb(s) > \u03bb1.\nProof. The proof is by a simple induction.\nThe claim clearly holds for s = 0 since by our\nassumption \u03bb1 \u22641.\nSuppose now that the claim holds for some iteration s. According to\nLemma 2.2 it holds that the value \u2206s+1 computed on iteration s + 1 satis\ufb01es that\n\u2206s+1 \u2264\u03bb(s) \u2212\u03bb1.\nHence, according to the algorithm, it holds that\n\u03bb(s+1) = \u03bb(s) \u2212\u2206s+1\n2\n\u2265\u03bb(s) \u2212\u03bb(s) \u2212\u03bb1\n2\n= \u03bb(s) + \u03bb1\n2\n> \u03bb1,\nwhere the last inequality follows from the induction hypothesis.\nThe following lemma bounds the number of iterations of the loop in Algorithm 2.\nLemma 3.2. The repeat-until loop is executed at most O(log(\u02c6\u03b4\u22121)) times.\nProof. Fix an iteration s of the loop. By applying Lemma 2.2 with respect to the unit vector\nws we have that\n\u2206s \u22651\n2(\u03bb(s\u22121) \u2212\u03bb1).\nBy the update rule of the algorithm it follows that\n\u03bb(s) \u2212\u03bb1\n=\n\u03bb(s\u22121) \u2212\u2206s\n2 \u2212\u03bb1 \u2264(\u03bb(s\u22121) \u2212\u03bb1) \u22121\n4(\u03bb(s\u22121) \u2212\u03bb1)\n=\n3\n4(\u03bb(s\u22121) \u2212\u03bb1).\n(3)\nThus, after at most T = \u2308log3/4\n\u0010\n\u02c6\u03b4\n\u03bb(0)\u2212\u03bb1\n\u0011\n\u2309= O(log(\u02c6\u03b4\u22121)) (using our choice of \u03bb(0)) itera-\ntions, we arrive at a value \u03bb(T) which satis\ufb01es \u03bb(T) \u2212\u03bb1 \u2264\u02c6\u03b4. By Lemma 2.2 it follows that\nin the following iteration it will hold that \u2206T+1 \u2264\u02c6\u03b4 and the loop will end. Hence, the overall\nnumber of iterations is at most T + 1 = O(log(\u02c6\u03b4\u22121)).\nFinally, the following lemma gives approximation guarantees on the estimate \u03bb(f).\nLemma 3.3. Suppose that all executions of the Power Method in Algorithm 2 are successful.\nThen it holds that\n\u03bb1 + 3\u02c6\u03b4\n2 \u2265\u03bb(f) \u2265\u03bb1 +\n\u02c6\u03b4\n4.\n(4)\n10\nProof. Denote by sf the last iteration of the loop in Algorithm 2, and note that using this\nnotation we have that \u03bb(f) = \u03bb(sf) and that \u2206sf \u2264\u02c6\u03b4. Using Lemma 2.2, we thus have that\n\u03bb(f) \u2212\u03bb1 = \u03bb(sf \u22121) \u2212\u2206sf\n2\n\u2212\u03bb1 \u22642\u2206sf \u2212\u2206sf\n2\n= 3\n2\u2206sf \u22643\n2\n\u02c6\u03b4,\nwhich gives the \ufb01rst part of the lemma.\nFor the second part, using Lemma 2.2 again, we have that\n\u03bb(f) \u2212\u03bb1\n=\n\u03bb(sf \u22121) \u2212\u2206sf\n2\n\u2212\u03bb1 \u2265\u03bb(sf \u22121) \u2212\u03bb1 \u22121\n2(\u03bb(sf \u22121) \u2212\u03bb1)\n=\n1\n2(\u03bb(sf \u22121) \u2212\u03bb1).\n(5)\nIn case sf = 1, then by our choice of \u03bb(0) we have that \u03bb(0) \u2212\u03bb1 \u2265\u02c6\u03b4, and the lemma follows.\nOtherwise, by unfolding Eq. (5) one more time, we have that\n\u03bb(f) \u2212\u03bb1 \u22651\n4(\u03bb(sf \u22122) \u2212\u03bb1) \u2265\n\u2206(sf \u22121)\n4\n>\n\u02c6\u03b4\n4,\nwhere the second inequality follows from Lemma 2.2 and the last inequality follows from the\nstopping condition of the loop.\nWe can now prove Theorem 3.1.\nProof. First a note regarding the success probability of the invocations of the Power Method\nalgorithm in Algorithm 2: since, as stated in Theorem 2.1, the success of the PM algorithm\ndepends only on the magnitude of (w\u22a4\n0 u)2, and all matrices Ms, Mf in Algorithm 2 have the\nsame leading eigenvector, as long as all invocations use the same random initial vector and\nnumber of steps that guarantees success with probability 1 \u2212p, they all succeed together with\nprobability at least 1 \u2212p.\nLet us now assume that all executions of the Power Method algorithm in Algorithm 1 were\nsuccessful.\nAccording to Lemma 3.2, the loop is executed at most O(log(\u02c6\u03b4\u22121)) = O(log(\u03b4\u22121)) times\n(following our assumption on \u02c6\u03b4). Each iteration s of the loop requires to invoke the Power\nMethod to approximate \u03bb1(M) up to a factor of 1/2 which according to Theorem 2.1, requires\ncomputing T PM\ncrude(1/2, p) = O(log(d/p)) matrix-vector products, in order to succeed with prob-\nability at least 1 \u2212p. Thus the overall number of matrix-vector products computed during the\nloop is O\n\u0000log(d/p) log(\u03b4\u22121)\n\u0001\n.\nAccording to lemma 3.3 it holds that \u03bb(f) \u2212\u03bb1 \u22643\n2 \u02c6\u03b4 \u22643\u03b4. Thus, according to Lemma 2.1,\nwe have that\n\u03ba(M\u22121\nf ) =\n\u03bb1(M\u22121\nf )\n\u03b4(M\u22121\nf ) \u22644 = O(1).\nThus, in the \ufb01nal invocation of the PM algorithm, it requires at most T PM\nacc (4, \u01eb, p) =\nO\n\u0010\nlog\n\u0010\nd\np\u01eb\n\u0011\u0011\nmatrix-vector products to compute wf as desired, with probability at least 1 \u2212p.\nThus the overall number of matrix-vector products is\nO\n\u0012\nlog(d/p) log(\u03b4\u22121) + log\n\u0012 d\np\u01eb\n\u0013\u0013\n.\n11\n4\nA Convex Optimization-based Eigenvector Algorithm\nIn this section we present our algorithm for approximating the largest eigenvector of a given\nmatrix based on convex optimization. The algorithm is based on Algorithm 2 from the previous\nsection, but replaces explicit computation of products between vectors and inverted matrices,\nwith solving convex optimization problems, as detailed in Subsection 2.2.\nTowards this end, we assume that we are given access to an algorithm - A for solving\nproblems of the following structure:\nmin\nz\u2208Rd{Fw,\u03bb(z) := 1\n2z\u22a4(\u03bbI \u2212X)z \u2212w\u22a4z},\n(6)\nwhere X is positive de\ufb01nite, \u03bb > \u03bb1(X) and w is some vector. Note that under these conditions,\nthe function Fw,\u03bb(z) is strongly convex. Note also that the minimizer of Fw,\u03bb(z) - z\u2217is given by\nz\u2217= (\u03bbI \u2212X)\u22121w, and thus solving Problem (6) is equivalent to computing a product between\na vector and an inverse matrix.\nThere are a few issues with our approach that require delicate care however: 1) we need to\npay close attention that the convex optimization problems are well-conditioned and 2) since now\nwe use a numerical procedure to compute the matrix-vector products, we have approximation\nerrors that we need to consider.\nAlgorithm 3 Leading Eigenvector via Convex Optimization\n1: Input: matrix X \u2208Rn\u00d7n such that X \u2ab00, \u03bb1(X) \u22641, an estimate \u02c6\u03b4 for \u03b4(X), accuracy\nparameter \u01eb \u2208(0, 1), failure probability parameters p\n2: \u03bb(0) \u21901 + \u02c6\u03b4\n3: Set: m1 \u2190T PM\ncrude(1/8, p),\nm2 \u2190T PM\nacc (3, \u01eb/2, p)\n4: Set: \u02dc\u01eb \u2190min{ 1\n16\n\u0010 \u02c6\u03b4\n8\n\u0011m1+1\n, \u01eb\n4\n\u0010 \u02c6\u03b4\n8\n\u0011m2+1\n}\n5: Let \u02c6w0 be a random unit vector\n6: s \u21900\n7: repeat\n8:\ns \u2190s + 1\n9:\nLet Ms = (\u03bb(s\u22121)I \u2212X)\n10:\nfor t = 1...m1 do\n11:\nApply Algorithm A to \ufb01nd a vector \u02c6wt such that \u2225\u02c6wt \u2212M\u22121\ns\n\u02c6wt\u22121\u2225\u2264\u02dc\u01eb\n12:\nend for\n13:\nws \u2190\n\u02c6wm1\n\u2225\u02c6wm1\u2225\n14:\nApply Algorithm A to \ufb01nd a vector vs such that \u2225vs \u2212M\u22121\ns ws\u2225\u2264\u02dc\u01eb\n15:\n\u2206s \u21901\n2 \u00b7\n1\nw\u22a4\ns vs\u2212\u02dc\u01eb\n16:\n\u03bb(s) \u2190\u03bb(s\u22121) \u2212\u2206s\n2\n17: until \u2206s \u2264\u02c6\u03b4\n18: \u03bb(f) \u2190\u03bb(s)\n19: Let Mf = (\u03bb(f)I \u2212X)\n20: for t = 1...m2 do\n21:\nApply Algorithm A to \ufb01nd a vector \u02c6wt such that \u2225\u02c6wt \u2212M\u22121\nf\n\u02c6wt\u22121\u2225\u2264\u02dc\u01eb\n22: end for\n23: return wf \u2190\n\u02c6wm2\n\u2225\u02c6wm2\u2225\n12\n4.1\nPower Method with inaccurate updates\nIn this section we analyze the potential convergence of the Power Method with inaccurate\nmatrix-vector products.\nLemma 4.1 (Power Method with inaccurate matrix-vector products). Let M be a positive\nde\ufb01nite matrix with largest eigenvalue \u03bb1 and smallest eigenvalue \u03bbd. Fix an accuracy param-\neter \u01eb > 0. Let w be an arbitrary unit vector and consider the following sequences of vectors\n{w\u2217\nt }\u221e\nt=0, { \u02c6w\u2217\nt }\u221e\nt=0 and {wt}\u221e\nt=0, { \u02c6wt}\u221e\nt=0 de\ufb01ned as follows:\nw\u2217\n0 = \u02c6w\u2217\n0 = w,\n\u2200t \u22651 : \u02c6w\u2217\nt \u2190M \u02c6w\u2217\nt\u22121,\nw\u2217\nt \u2190\n\u02c6w\u2217\nt\n\u2225\u02c6w\u2217\nt \u2225,\nw0 = \u02c6w0 = w,\n\u2200t \u22651 : \u02c6wt satis\ufb01es that \u2225\u02c6wt \u2212M \u02c6wt\u22121\u2225\u2264\u01eb,\nwt \u2190\n\u02c6wt\n\u2225\u02c6wt\u2225.\nDenote:\n\u0393(M, t) := 2\n\u03bbt\nd\n(\nt\nif \u03bb1 = 1\n\u03bbt\n1\u22121\n\u03bb1\u22121\nif \u03bb1 \u0338= 1 ,\n\u02c6\u0393(M, t) :=\n(\nt\nif \u03bb1 = 1\n\u03bbt\n1\u22121\n\u03bb1\u22121\nif \u03bb1 \u0338= 1 .\nThen it holds that for all t \u22650,\n\u2225\u02c6wt \u2212\u02c6w\u2217\nt \u2225\u2264\u01eb \u00b7 \u02c6\u0393(M, t)\nand\n\u2225wt \u2212w\u2217\nt \u2225\u2264\u01eb \u00b7 \u0393(M, t)\nProof. First, observe that:\n\u2225\u02c6wt+1 \u2212\u02c6w\u2217\nt+1\u2225\n=\n\u2225\u02c6wt+1 \u2212M \u02c6wt + M \u02c6wt \u2212\u02c6w\u2217\nt+1\u2225\u2264\u2225\u02c6wt+1 \u2212M \u02c6wt\u2225+ \u2225M \u02c6wt \u2212M \u02c6w\u2217\nt \u2225\n\u2264\n\u01eb + \u03bb1 \u00b7 \u2225\u02c6wt \u2212\u02c6w\u2217\nt \u2225.\nIn case \u03bb1 = 1, we clearly have that\n\u2225\u02c6wt+1 \u2212\u02c6w\u2217\nt+1\u2225\u2264(t + 1)\u01eb + \u2225\u02c6w0 \u2212\u02c6w\u2217\n0\u2225= (t + 1)\u01eb.\n(7)\nOtherwise, in case \u03bb1 \u0338= 1, by a simple algebraic manipulation we have that\n\u2225\u02c6wt+1 \u2212\u02c6w\u2217\nt+1\u2225+\n\u01eb\n\u03bb1 \u22121\n\u2264\n\u03bb1 \u00b7 \u2225\u02c6wt \u2212\u02c6w\u2217\nt \u2225+\n\u01eb\u03bb1\n\u03bb1 \u22121 = \u03bb1\n\u0012\n\u2225\u02c6wt \u2212\u02c6w\u2217\nt \u2225+\n\u01eb\n\u03bb1 \u22121\n\u0013\n.\nIt thus follows that for all t \u22650,\n\u2225\u02c6wt+1 \u2212\u02c6w\u2217\nt+1\u2225\u2264\u03bbt+1\n1\n\u0012\n\u2225\u02c6w0 \u2212\u02c6w\u2217\n0\u2225+\n\u01eb\n\u03bb1 \u22121\n\u0013\n\u2212\n\u01eb\n\u03bb1 \u22121 =\n\u01eb\n\u03bb1 \u22121(\u03bbt+1\n1\n\u22121).\n(8)\nWe now have that for all t \u22651 it holds that\n\u2225wt \u2212w\u2217\nt \u2225\n=\n\u2225\u02c6wt\n\u2225\u02c6wt\u2225\u2212\n\u02c6w\u2217\nt\n\u2225\u02c6w\u2217\nt \u2225\u2225= \u2225\u02c6wt\n\u2225\u02c6wt\u2225\u2212\n\u02c6wt\n\u2225\u02c6w\u2217\nt \u2225+\n\u02c6wt\n\u2225\u02c6w\u2217\nt \u2225\u2212\n\u02c6w\u2217\nt\n\u2225\u02c6w\u2217\nt \u2225\u2225\n\u2264\n\u2225\n\u02c6wt\n\u2225\u02c6w\u2217\nt \u2225\u2212\n\u02c6w\u2217\nt\n\u2225\u02c6w\u2217\nt \u2225\u2225+ \u2225\u02c6wt\n\u2225\u02c6wt\u2225\u2212\n\u02c6wt\n\u2225\u02c6w\u2217\nt \u2225\u2225\n=\n1\n\u2225\u02c6w\u2217\nt \u2225\u00b7 \u2225\u02c6wt \u2212\u02c6w\u2217\nt \u2225+ \u2225\u02c6wt\u2225\u00b7 |\n1\n\u2225\u02c6wt\u2225\u2212\n1\n\u2225\u02c6w\u2217\nt \u2225|\n=\n1\n\u2225\u02c6w\u2217\nt \u2225\u00b7 \u2225\u02c6wt \u2212\u02c6w\u2217\nt \u2225+ |\u2225\u02c6w\u2217\nt \u2225\u2212\u2225\u02c6wt\u2225|\n\u2225\u02c6w\u2217\nt \u2225\n\u22642\u2225\u02c6wt \u2212\u02c6w\u2217\nt \u2225\n\u2225\u02c6w\u2217\nt \u2225\n,\n13\nwhere the last inequality follows from the triangle inequality.\nNow, since \u02c6w\u2217\nt = Mtw and M \u2ab0\u03bbdI, it follows that\n\u2225\u02c6w\u2217\nt \u2225\u2265\u03bbt\nd \u00b7 \u2225w\u2225= \u03bbt\nd.\nCombining this with Eq. (8) and (7), we have that for all t \u22651,\n\u2225wt \u2212w\u2217\nt \u2225\u22642\u01eb\n\u03bbt\nd\n(\nt\nif \u03bb1 = 1;\n\u03bbt\n1\u22121\n\u03bb1\u22121\nif \u03bb1 \u0338= 1.\nThus the lemma follows.\nThe following corollary is a consequence of the convergence result for the Power Method\n(Theorem 2.1) and Lemma 4.1.\nTheorem 4.1 (Convergence of the Power Method with inaccurate updates). Fix \u01eb > 0 and\np > 0.\nLet M be a positive de\ufb01nite matrix with largest eigevalue \u03bb1 and eigengap \u03b4 > 0.\nConsider a sequence of vectors {wt}\u03c4\nt=0 where w0 is a random unit vector, and for all t \u2208[\u03c4] it\nholds that \u2225wt \u2212Mwt\u22121\u2225\u2264\u02dc\u01eb :=\n\u01eb\n4\u0393(M,\u03c4) (see Lemma 4.1 for de\ufb01nition of \u0393(M, \u03c4)). Then the\nfollowing two guarantees hold:\n1. If \u03c4 \u2265T PM\ncrude(\u01eb/2, p) then with probability at least 1 \u2212p it holds that\nw\u22a4\n\u03c4 Mw\u03c4 \u2265(1 \u2212\u01eb)\u03bb1.\n2. If \u03c4 \u2265T PM\nacc (\u03ba(M), \u01eb/2, p)then with probability at least 1 \u2212p it holds that\n(w\u22a4\n\u03c4 u1)2 \u22651 \u2212\u01eb,\nwhere u1 is the largest eigenvector of M.\nIn both cases, the probability of success depends only on the random variable (w\u22a4\n0 u1)2.\nProof. Let {w\u2217\nt }\u221e\nt=0 be a sequence of unit vectors as de\ufb01ned in Lemma 4.1. For the \ufb01rst item,\nnote that\nw\u22a4\n\u03c4 Mw\u03c4\n=\nw\u2217\u22a4\n\u03c4 Mw\u2217\n\u03c4 +\n\u0010\nw\u22a4\n\u03c4 Mw\u03c4 \u2212w\u2217\u22a4\n\u03c4 Mw\u2217\n\u03c4\n\u0011\n.\nSince M is positive de\ufb01nite, the function f(w) = w\u22a4Mw is convex and we have that\n|w\u22a4\n\u03c4 Mw\u03c4 \u2212w\u2217\u22a4\n\u03c4 Mw\u2217\n\u03c4|\n\u2264\n2 max{(Mw\u03c4)\u22a4(w\u03c4 \u2212w\u2217\n\u03c4), (Mw\u2217\n\u03c4)\u22a4(w\u2217\n\u03c4 \u2212w\u03c4)}\n\u2264\n2\u03bb1(M) \u00b7 \u2225w\u03c4 \u2212w\u2217\n\u03c4\u2225.\nThus we have that\nw\u22a4\n\u03c4 Mw\u03c4\n\u2265\nw\u2217\u22a4\n\u03c4 Mw\u2217\n\u03c4 \u22122\u03bb1(M) \u00b7 \u2225w\u03c4 \u2212w\u2217\n\u03c4\u2225\n\u2265\nw\u2217\u22a4\n\u03c4 Mw\u2217\n\u03c4 \u2212\u01eb\n2\u03bb1(M),\nwhere the last inequality follows from Lemma 4.1 and our choice of \u02dc\u01eb.\nNow, by our choice of \u03c4 and Theorem 2.1, we have that with probability at least 1 \u2212p it\nholds that\nw\u22a4\n\u03c4 Mw\u03c4\n\u2265\n(1 \u2212\u01eb/2)\u03bb1(M) \u2212\u01eb\n2\u03bb1(M) = (1 \u2212\u01eb)\u03bb1(M).\n14\nFor the second item, note that\n(w\u22a4\n\u03c4 u1)2\n=\n(w\u2217\u22a4\n\u03c4 u1 + (w\u03c4 \u2212w\u2217\u22a4\n\u03c4 )\u22a4u1)2 \u2265(w\u2217\u22a4\n\u03c4 u1)2 \u22122\u2225w\u03c4 \u2212w\u2217\n\u03c4\u2225\n\u2265\n(w\u2217\u22a4\n\u03c4 u1)2 \u2212\u01eb\n2,\nwhere the last inequality follows again from Lemma 4.1 and our choice of \u02dc\u01eb.\nAgain, by our de\ufb01nition of \u03c4 and Theorem 2.1, we have that with probability at least 1 \u2212p\nit holds that\n(w\u22a4\n\u03c4 u1)2 \u22651 \u2212\u01eb/2 \u2212\u01eb/2 = 1 \u2212\u01eb.\n4.2\nConvergence of Algorithm 3\nThe key step in the analysis of Algorithm 3 is to show that if all numerical computations are\ncarried out with su\ufb03ciently small error, then Algorithm 3 successfully simulates the Power\nMethod-based Algorithm 2. A main ingredient in Algorithm 2 is the computations of the values\n\u2206s which are used in turn to update the estimates \u03bb(s). Our use of the values \u2206s in Algorithm\n2 was through the approximation guarantees they provided for the gap \u03bb(s\u22121) \u2212\u03bb1 (recall we\nhave seen that 1\n2(\u03bb(s\u22121) \u2212\u03bb1) \u2264\u2206s \u2264\u03bb(s\u22121) \u2212\u03bb1). The following lemma shows that with a\ncorrect choice for the numerical error parameter \u02dc\u01eb, these guarantees also hold for the values \u2206s\ncomputed in Algorithm 3.\nLemma 4.2. Suppose that \u02c6\u03b4 \u2208\n\u0002 \u03b4\n2, 3\u03b4\n4\n\u0003\nand that \u02dc\u01eb \u2264\n1\n16\n\u0010 \u02c6\u03b4\n8\n\u0011m1+1\n, where m1 is as de\ufb01ned in\nAlgorithm 3. Then with probability at least 1 \u2212p it holds that for any s, the update of the\nvariables \u2206s, \u03bb(s) in Algorithm 3 is equivalent to that in Algorithm 2. In particular, for all\ns \u22651 it holds that\n1\n2(\u03bb(s\u22121) \u2212\u03bb1) \u2264\u2206s \u2264\u03bb(s\u22121) \u2212\u03bb1.\nProof. For clarity we refer by \u2206s, \u03bb(s) to the values computed in Algorithm 3 and by \u02dc\u2206s, \u02dc\u03bb(s) to\nthe corresponding values computed in Algorithm 2 from Section 3. The proof of the lemma is\nby induction on s. For the base case s = 0, clearly \u03bb(0) = \u02dc\u03bb(0) and both \u22060, \u02dc\u22060 are unde\ufb01ned.\nConsider now some s \u22651.\nSuppose for now that\n\u02dc\u01eb \u2264min{\n1\n16\u0393(M\u22121\ns , m1), \u03bb1(M\u22121\ns )\n8\n}.\n(9)\nThen it follows from Theorem 4.1 and our choice of m1 that with probability at least 1 \u2212p,\nw\u22a4\ns M\u22121\ns ws \u22653\n4\u03bb1(M\u22121\ns ).\n(10)\nBy the de\ufb01nition of the vector vs in Algorithm 3 and the Cauchy-Schwartz inequality it\nholds that\nw\u22a4\ns vs = w\u22a4\ns M\u22121\ns ws + w\u22a4\ns (vs \u2212M\u22121\ns ws) \u2208[w\u22a4\ns M\u22121\ns ws \u2212\u02dc\u01eb, w\u22a4\ns M\u22121\ns ws + \u02dc\u01eb].\n(11)\nThus, combining Eq. (10) and (11), we have that\nw\u22a4\ns vs \u2212\u02dc\u01eb \u2208[w\u22a4\ns M\u22121\ns ws \u22122\u02dc\u01eb, w\u22a4\ns M\u22121\ns ws] \u2286[3\u03bb1(M\u22121\ns )/4 \u22122\u02dc\u01eb, \u03bb1(M\u22121\ns )].\n15\nBy our choice of \u02dc\u01eb it follows that\nw\u22a4\ns vs \u2212\u02dc\u01eb \u2208[\u03bb1(M\u22121\ns )/2, \u03bb1(M\u22121\ns )].\nThus, the computation of the value of \u2206s, and as a result the computation of \u03bb(s), is identical\nto that of \u02dc\u2206s, \u02dc\u03bb(s), and the claim follows.\nIt only remains to give an explicit bound on \u02dc\u01eb, as de\ufb01ned in Eq. (9). For this we need to\nupper bound \u0393(M\u22121\ns , m1) for all values of s. Recall that from the results of Section 3 it follows\nthat the values \u03bb(s) are monotonically non-increasing (see Eq. (3) in proof of Lemma 3.2) and\nlower-bounded by \u03bb(f) \u2265\u03bb1 + \u02c6\u03b4/4 (Lemma 3.3). By our assumption of \u02c6\u03b4 we have that\n\u03bb1(M\u22121\ns )\n=\n1\n\u03bb(s\u22121) \u2212\u03bb1\n\u2265\n1\n\u03bb(0) \u2212\u03bb1\n=\n1\n1 + \u02c6\u03b4 \u2212\u03bb1\n\u2265\n1\n1 + 3\u03b4\n4 \u2212\u03b4\n>\n1 + \u03b4\n4 \u22651 +\n\u02c6\u03b4\n3,\n(12)\nwhere the second inequality holds since by de\ufb01nition of \u03b4 it follows that \u03bb1 = \u03bb2 + \u03b4 \u2265\u03b4.\nFix a natural number t. By the de\ufb01nition of \u0393(M, t) (see Lemma 4.1) we have that\n\u0393(M\u22121\ns , t)\n=\n2\n\u03bbd(M\u22121\ns )t \u00b7 \u03bb1(M\u22121\ns )t \u22121\n\u03bb1(M\u22121\ns ) \u22121 =\n2\n\u03bb1(M\u22121\ns ) \u22121\n\u0012\u03bb1(M\u22121\ns )\n\u03bbd(M\u22121\ns )\n\u0013t\n\u2264\n6\n\u02c6\u03b4\n\u0012\u03bb(s\u22121) \u2212\u03bbd\n\u03bb(s\u22121) \u2212\u03bb1\n\u0013t\n\u22646\n\u02c6\u03b4\n\u0012\n\u03bb(0)\n\u03bb(f) \u2212\u03bb1\n\u0013t\n\u2264\n6\n\u02c6\u03b4\n \n1 + \u02c6\u03b4\n\u02c6\u03b4/4\n!t\n= 6\n\u02c6\u03b4\n\u0012\n4 + 4\n\u02c6\u03b4\n\u0013t\n<\n\u00128\n\u02c6\u03b4\n\u0013t+1\n,\n(13)\nwhere the \ufb01rst inequality follows from Eq. (12), the second inequality follows from the bounds\n\u03bbd \u22650 and \u03bb(f) \u2264\u03bb(s\u22121) \u2264\u03bb(0), and the third inequality follows from plugging the value of \u03bb(0)\nand from Lemma 3.3.\nThus, it follows that we can take \u02dc\u01eb \u2264min{ 1\n16\n\u0010 \u02c6\u03b4\n8\n\u0011m1+1\n, 1+\u02c6\u03b4/3\n8\n} =\n1\n16\n\u0010 \u02c6\u03b4\n8\n\u0011m1+1\n.\nTheorem 4.2 (Convergence of Algorithm 3). Suppose that \u02c6\u03b4 \u2208[\u03b4/2, 3\u03b4/4]. Fix \u01eb > 0 . Let\nm1 \u2265T PM\ncrude(1/8, p) and m2 \u2265T PM\nacc (3, \u01eb/2, p). Suppose that \u02dc\u01eb, satis\ufb01es that\n\u02dc\u01eb \u2264min{ 1\n16\n \u02c6\u03b4\n8\n!m1+1\n, \u01eb\n4\n \u02c6\u03b4\n8\n!m2+1\n}.\nThen, with probability at least 1 \u2212p it holds that the output of Algorithm 3, the unit vector wf,\nsatis\ufb01es that\n(w\u22a4\nf u1)2 \u22651 \u2212\u01eb,\nand the total number of calls to the convex optimization oracle A is\nO\n\u0012\nlog(d/p) log(\u03b4\u22121) + log( d\np\u01eb)\n\u0013\n.\n16\nProof. First, note that as in the proof of Theorem 3.1, since all the noisy simulations of the\nPower Method in Algorithm 3 are initialized with the same random unit vector \u02c6w0, they all\nsucceed together with probability at least 1 \u2212p (provided that the other parameters m1, m2, \u02dc\u01eb\nare set correctly).\nBy our choice of \u02dc\u01eb and Lemma 4.2 it follows that we can invoke the results of Section 3.\nBy Lemma 3.2, we can upper bound the number of iterations made by the repeat-until loop by\nO(log(\u03b4\u22121). Since each iteration of the loop requires m1 + 1 calls to the optimization oracle A,\nthe overall number of calls to A during the loop is O(m1 log(\u03b4\u22121)).\nBy Lemma 3.3 we have that the \ufb01nal estimate \u03bb(f) satis\ufb01es that \u03bb(f) \u2212\u03bb1 \u22643\u02c6\u03b4\n2 \u22649\u03b4\n8 < 2\u03b4.\nThus, by Lemma 2.1 we have that\n\u03ba(M\u22121\nf ) =\n\u03bb1(M\u22121\nf )\n\u03b4(M\u22121\nf ) \u22643.\n(14)\nSuppose now that \u02dc\u01eb \u2264\n\u01eb\n4\u0393(M\u22121\nf\n,m2). By our choice of m2 and Eq. (14), it follows from Theorem\n4.1 that with probability at least 1 \u2212p indeed (w\u22a4\nf u)2 \u22651 \u2212\u01eb.\nThe number of calls to the oracle A in this \ufb01nal part is m2. Thus overall number of calls to\nA in Algorithm 3 is O(m1 log(\u03b4\u22121) + m2).\nIt remains to lower-bound the term\n\u01eb\n4\u0393(M\u22121\nf ,m2).\nFollowing the analysis in the proof of\nLemma 4.2 (Eq. (13)), we can upper bound \u0393(M\u22121\nf , m2) \u2264\n\u0010\n8\n\u02c6\u03b4\n\u0011m2+1\n, which agrees with the\nbound on \u02dc\u01eb stated in the theorem.\nIn order to analyze the arithmetic complexity of Algorithm 3 using a speci\ufb01c implementation\nfor the optimization oracle A, it is not only important to bound the number of calls to A (as\ndone in Theorem 4.2), but to also bound important parameters of the optimization problem (6)\nthat naturally arise when considering the arithmetic complexity of di\ufb00erent implementations\nfor A. For this issue we have the following lemma which follows directly from the discussion in\nSection 3 and the assumptions stated in Theorem 4.2.\nLemma 4.3. Let \u03bb, w be such that during the run of Algorithm 3, the optimization oracle A is\napplied to the minimization of the function\nFw,\u03bb(z) = 1\n2z\u22a4(\u03bbI \u2212X)z \u2212w\u22a4z.\nThen, under the conditions stated in Theorem 4.2 it holds that\n1. Fw,\u03bb(z) is (\u03bb \u2212\u03bb1(X)) = \u2126(\u03b4)-strongly convex.\n2. for all i \u2208[n] it holds that the function fi(z) = 1\n2z\u22a4(\u03bbI \u2212xix\u22a4\ni )z \u2212w\u22a4z is 1 + \u02c6\u03b4 = O(1)-\nsmooth.\n3. log(\u2225z\u2217\u2225) = \u02dcO(1), where z\u2217is the global minimizer of Fw,\u03bb(z).\n5\nPutting it all together: Fast PCA via SVRG\nIn this section we prove Theorems 1.1, 1.2.\nFollowing the convex optimization-based eigenvector algorithm presented in the previous\nsection - Algorithm 3, we consider the implementation of the convex optimization oracle A,\n17\ninvoked in Algorithm 3, using the SVRG algorithm [14] discussed in subsection 2.2.3. Recall\nthat the oracle A is used to solve Problem (6) for some parameters \u03bb, w. Indeed the objective\nFw,\u03bb(z) in (6) could be written as a \ufb01nite some of functions in the following way:\nFw,\u03bb(z) = 1\nn\nn\nX\ni=1\n\u00121\n2z\u22a4(\u03bbI \u2212xix\u22a4\ni )z \u2212w\u22a4z\n\u0013\n.\n(15)\nFurther, recall that for \u03bb > \u03bb1(X), Fw,\u03bb(z) is always (\u03bb \u2212\u03bb1(X))-strongly-convex and that for\nevery i \u2208[n], the function\nfi(z) := 1\n2z\u22a4(\u03bbI \u2212xix\u22a4\ni )z \u2212w\u22a4z,\nis max{\u03bb, \u2225xi\u22252 \u2212\u03bb} smooth. However, fi(z) need not be convex. Hence the SVRG theorem\nfrom [14] could not be directly applied to minimizing (15). However, we prove in the appendix\nthat the SVRG method still converges but with a slightly worse dependence on the condition\nnumber 2.\nBelow we give an explicit implementation of the the SVRG algorithm for minimizing (15).\nAlgorithm 4 SVRG for PCA\n1: Input: \u03bb \u2208R, X = 1\nn\nPn\ni=1 xix\u22a4\ni , w, \u03b7, m, T.\n2: \u02dcz0 \u2190\u20d70\n3: for s = 1, ...T do\n4:\n\u02dcz \u2190\u02dczs\u22121\n5:\n\u02dc\u00b5 \u2190(\u03bbI \u2212X)\u02dcz \u2212wt\u22121\n6:\nz0 \u2190\u02dcz\n7:\nfor t = 1, 2, ..., m do\n8:\nRandomly pick it \u2208[n]\n9:\nzt \u2190zt\u22121 \u2212\u03b7\n\u0000(\u03bbI \u2212xitx\u22a4\nit)(zt\u22121 \u2212\u02dcz) + \u02dc\u00b5\n\u0001\n10:\nend for\n11:\n\u02dczs \u21901\nm\nPm\u22121\nt=0 zt\n12: end for\n13: return \u02dczT\nThe following theorems are proven in the appendix.\nTheorem 5.1. Fix \u01eb > 0, p > 0. There exists a choice of \u03b7, m such that Algorithm 4 \ufb01nds with\nprobability at least 1 \u2212p an \u01eb-approxiated minimizer of (15) in overall time\n\u02dcO\n\u0012\nN +\nd\n(\u03bb \u2212\u03bb1(X))2\n\u0013\n.\nBased on the recent acceleration framework of [12] we also have the following result (the\nproof is given in the appendix).\nTheorem 5.2. Fix \u01eb > 0, p > 0. Assume that \u03bb \u2212\u03bb1 = O(\np\nd/N). There exists an accelerated\nversion of Algorithm 4 that \ufb01nds with probability at least 1 \u2212p an \u01eb-approximated minimizer of\n(15) in overall time\n\u02dcO\n \nN 3/4d1/4\np\n\u03bb \u2212\u03bb1(X)\n!\n.\n2We note that in [22] it was shown that the same kind of result holds also for the SDCA method that was\noriginally introduced in [23].\n18\n5.1\nProving Theorems 1.1, 1.2\nThe proof of Theorem 1.1 follows from the bounds in Theorem 4.2, Lemma 4.3 and Theorem\n5.1.\nThe proof of Theorem 1.2 follows from the bounds in Theorem 4.2, Lemma 4.3 and Theorem\n5.2.\n6\nProof of Theorems 1.3, 1.4\nIn this section we prove Theorems 1.3 and 1.4. Since the algorithm and corresponding proofs\nbasically follow from the results of the previous sections, we give the algorithm and a sketch of\nthe proofs.\nThe algorithm is the same as Algorithm 3, but intuitively, it replaces the estimate for the\neigengap \u02c6\u03b4 with the desired approximation error for the top eigenvalue, \u01eb. This is intuitive since\nnow, instead of \ufb01nding a unit vector that is approximately entirely contained in the span of\nvectors {ui | \u03bbi > \u03bb1 \u2212\u03b4} = {u1}, we wish to \ufb01nd a unit vector that is approximately entirely\ncontained in the span of vectors {ui | \u03bbi > \u03bb1 \u2212\u01eb}.\nAlgorithm 5 Leading Eigenvalue Approximation via Convex Optimization\n1: Input: matrix X \u2208Rn\u00d7n such that X \u2ab00, \u03bb1(X) \u22641, accuracy parameter \u01eb \u2208(0, 1), failure\nprobability parameters p, positive integers m1, m2, numerical accuracy parameter \u02dc\u01eb\n2: \u03bb(0) \u21901 + \u01eb\n3: Let \u02c6w0 be a random unit vector\n4: s \u21900\n5: repeat\n6:\ns \u2190s + 1\n7:\nLet Ms = (\u03bb(s\u22121)I \u2212X)\n8:\nfor t = 1...m1 do\n9:\nApply Algorithm A to \ufb01nd a vector \u02c6wt such that \u2225\u02c6wt \u2212M\u22121\ns\n\u02c6wt\u22121\u2225\u2264\u02dc\u01eb\n10:\nend for\n11:\nws \u2190\n\u02c6wm1\n\u2225\u02c6wm1\u2225\n12:\nApply Algorithm A to \ufb01nd a vector vs such that \u2225vs \u2212M\u22121\ns ws\u2225\u2264\u02dc\u01eb\n13:\n\u2206s \u21901\n2 \u00b7\n1\nw\u22a4\ns vs\u2212\u02dc\u01eb\n14:\n\u03bb(s) \u2190\u03bb(s\u22121) \u2212\u2206s\n2\n15: until \u2206s \u2264\u01eb\n16: \u03bb(f) \u2190\u03bb(s)\n17: Let Mf = (\u03bb(f)I \u2212X)\n18: for t = 1...m2 do\n19:\nApply Algorithm A to \ufb01nd a vector \u02c6wt such that \u2225\u02c6wt \u2212M\u22121\nf\n\u02c6wt\u22121\u2225\u2264\u02dc\u01eb\n20: end for\n21: return wf \u2190\n\u02c6wm2\n\u2225\u02c6wm2\u2225\nThe following simple lemma is of the same \ufb02avor as Lemma 2.1 and shows that we can\nbene\ufb01t from conditioning the inverse matrix (\u03bbI\u2212X)\u22121, even if the goal is only to approximate\nthe leading eigenvalue and not the leading eigenvector.\nLemma 6.1. Fix \u01eb > 0 and a scalar a > 0. Let M\u22121 = (\u03bbI \u2212X)\u22121 such that \u03bb1(X) + a \u00b7 \u01eb \u2265\n19\n\u03bb > \u03bb1(X). It holds for all i \u2208[d] such that \u03bbi(X) \u2264\u03bb1(X) \u2212\u01eb, that\n\u03bb1(M\u22121)\n\u03bbi(M\u22121) \u22651 + a\u22121.\nProof. It holds that\n\u03bb1(M\u22121)\n\u03bbi(M\u22121) =\n1\n\u03bb \u2212\u03bb1\n\u00b7 (\u03bb \u2212\u03bbi) = 1 + \u03bb1 \u2212\u03bbi\n\u03bb \u2212\u03bbi\n\u22651 + \u03bb1 \u2212\u03bbi\na\u01eb\n.\nThus, for any i \u2208[d] such that \u03bb1 \u2212\u03bbi \u2265\u01eb it follows that \u03bb1(M\u22121)\n\u03bbi(M\u22121) \u22651 + a\u22121.\nIn order to prove the convergence of Algorithm 5 we are going to need a slightly di\ufb00erent\nresult for the Power Method (Algorithm 1) than that of Theorem 2.1. This result follows from\nthe same analysis as Theorem 2.1. For a proof see the proof of Theorem 2.1.\nLemma 6.2 (Convergence of Power Method to span of top eigenvectors). Let M be a pos-\nitive de\ufb01nite matrix and denote its eigenvalues in descending order by \u03bb1, \u03bb2, ..., \u03bbd, and let\nu1, u2, ..., ud denote the corresponding eigenvectors. Fix \u01eb1, \u01eb2 \u2208(0, 1) and failure probability\np > 0. De\ufb01ne:\nT PM(\u01eb1, \u01eb2, p) := \u23081\n2\u01eb1\nln\n\u0012 9d\np2\u01eb2\n\u0013\n\u2309.\nThen, with probability 1 \u2212p it holds for any t \u2265T PM(\u01eb1, \u01eb2, p) that\nX\ni\u2208[d]: \u03bbi\u2264(1\u2212\u01eb1)\u03bb1\n(w\u22a4\nt ui)2 \u2264\u01eb2.\nThe probability of success depends only on the random variable (w\u22a4\n0 u1)2.\nTheorem 6.1. There exists a choice for the parameters m1, m2, \u02dc\u01eb such that Algorithm 5, when\nimplemented with SVRG as the optimization oracle A, \ufb01nds in time \u02dcO\n\u0000 d\n\u01eb2 + N\n\u0001\na unit vector\nwf that with probability at least 1 \u2212p satis\ufb01es,\nw\u22a4\nf Xwf \u2265\u03bb1 \u2212\u01eb.\nProof sketch. Following the same lines of the analysis presented in Theorem 4.2, there exists\nm1 = \u02dcO(1) and \u02dc\u01eb satisfying log(1/\u02dc\u01eb) = \u02dcO(1), such that the repeat-until loop terminates after\n\u02dcO(1) iterations with a value \u03bb(f) such that 3\n2\u01eb \u2265\u03bb(f) \u2212\u03bb1 \u22651\n4\u01eb.\nDenote S(\u01eb) = {i \u2208[d] | \u03bbi > \u03bb1 \u2212\u01eb/2}. By Lemma 6.1 we have that for all i /\u2208S(\u01eb) it holds\nthat \u03bbi(M\u22121\nf ) \u22643\n4\u03bb1(M\u22121\nf ).\nThus, by applying Lemma 6.2 with respect to the matrix M\u22121\nf\nwith \u01eb1 = 1\n4, \u01eb2 = \u01eb/2, we\nget that for m2 = \u02dcO(1), it follows that P\ni\u2208S(\u01eb)(w\u22a4\nf ui)2 \u22651 \u2212\u01eb/2. Thus it follows that\nw\u22a4\nf Xwf\n=\nd\nX\ni=1\n\u03bbi(w\u22a4\nf ui)2 \u2265\nX\ni\u2208S(\u01eb)\n\u03bbi(w\u22a4\nf ui)2 \u2265(\u03bb1 \u2212\u01eb/2)\nX\ni\u2208S(\u01eb)\n(w\u22a4\nf ui)2\n\u2265\n(\u03bb1 \u2212\u01eb/2)(1 \u2212\u01eb/2) > \u03bb1 \u2212\u01eb.\nSince on every iteration s of the repeat-until loop it holds that \u03bb(s) \u2212\u03bb1 = \u2126(\u01eb), similarly\nto Lemma 4.3, we have that the optimization oracle A is applied to solving O(1)-smooth and\n\u2126(\u01eb)-strongly convex problems. Hence, by the SVRG theorem, Theorem 5.1, each invocation of\nA when implemented using the SVRG algorithm, requires \u02dcO\n\u0000 d\n\u01eb2 + N\n\u0001\ntime. Hence, the theorem\nfollows.\n20\nTheorem 1.3 follows from Theorem 6.1 and the observation that by using a standard con-\ncentration argument for matrices 3, instead of applying the algorithm directly to the matrix\nX = 1\nn\nPn\ni=1 xixi, it su\ufb03ces to apply it to the matrix \u02dcX = 1\nn\u2032\nPn\u2032\ni=1 \u02dcxi\u02dcxi, where n\u2032 = \u02dcO(\u01eb\u22122) and\nthe vectors {\u02dcxi}n\u2032\ni=1 are sampled uniformly from {xi}n\ni=1. Hence, we can basically substitute N\nwith d/\u01eb2 in the running time stated in Theorem 6.1. From this observation it also follows that\nTheorem 1.3 holds also when X is not given explicitly as a \ufb01nite sum of rank one matrices, but\nX = Ex\u223cD[xx\u22a4] for some unknown distribution D from which we can sample in O(d) time.\nTheorem 1.4 follows if we replace the use of the SVRG algorithm in the proof of Theorem\n6.1 with its accelerated version, i.e. use Theorem 5.2 instead of Theorem 5.1.\n7\nDense PCA and Acceleration of Algorithms for Semide\ufb01nite\nOptimization\nSo far we have considered the problem of computing an approximation for the largest eigenvector\nof a matrix X given as a sum of rank-one matrices, i.e.\nX =\n1\nn\nPn\ni=1 xix\u22a4\ni .\nHowever, our\ntechniques extend well beyond this case. In fact, the only place in which we have used our\nstructural assumption on X is in our implementation of SVRG for PCA given in Algorithm 4\nand the corresponding runtime analysis.\nIn this section we consider the following natural generalization of the PCA problem which\nwe refer to as dense PCA: we assume X is of the form\nX =\nn\nX\ni=1\npiAi,\nwhere Pn\ni=1 pi = 1, \u2200i \u2208[n]pi \u22650 and \u2200i \u2208[n] Ai is a symmetric real d \u00d7 d matrix. We further\nassume that \u2200i \u2208[n] \u2225Ai\u2225\u22641 and X \u2ab00. We focus here on the problem of fast approximation\nof \u03bb1(X), i.e. \ufb01nding a unit vector w such that\nw\u22a4Xw \u2265\u03bb1(X) \u2212\u01eb.\nAttaining such an approximated eignevector for X, as considered in this section ,is an\nimportant subroutine in several algorithms for Semide\ufb01nite Programming [2, 11, 5, 8] and\nOnline Learning [9, 16, 1, 6].\nFor the purposes of this section we denote by N the total number of non-zeros in all matrices\nA1, ..., An, and in addition we denote by S the maximal number of non-zero entries in any of\nthe matrices A1, ..., An.\n7.1\nSVRG for Dense PCA\nAs discussed above, our fast PCA algorithms and corresponding analysis could be directly\napplied to the dense case under consideration in this section. The only thing that needs to be\nchanged is the application of the SVRG algorithm. A modi\ufb01ed SVRG algorithm for the dense\nPCA problem is detailed below. Speci\ufb01cally, we apply SVRG to the following optimization\nproblem:\nmin\nz\u2208Rd{Fw,\u03bb(z) :=\nn\nX\ni=1\npi\n\u0010\nz\u22a4(\u03bbI \u2212Ai)z \u2212w\u22a4z\n\u0011\n}.\n(16)\n3See for instance the Matrix Hoe\ufb00ding concentration inequality in [26].\n21\nAlgorithm 6 SVRG for Dense PCA\n1: Input: \u03bb \u2208R, X = Pn\ni=1 piAi, w, \u03b7, m, T.\n2: \u02dcz0 \u2190\u20d70\n3: for s = 1, ...T do\n4:\n\u02dcz \u2190\u02dczs\u22121\n5:\n\u02dc\u00b5 \u2190(\u03bbI \u2212X)\u02dcz \u2212wt\u22121\n6:\nz0 \u2190\u02dcz\n7:\nfor t = 1, 2, ..., m do\n8:\nPick it \u2208[n] according to probability distribution (p1, p2, ..., pn)\n9:\nzt \u2190zt\u22121 \u2212\u03b7 ((\u03bbI \u2212Ait)(zt\u22121 \u2212\u02dcz) + \u02dc\u00b5)\n10:\nend for\n11:\n\u02dczs \u21901\nm\nPm\u22121\nt=0 zt\n12: end for\n13: return \u02dczT\nNote that here, as opposed to the standard PCA problem, we assume that X is given by a\nweighted average of matrices and not necessarily a uniform average. This di\ufb00erence comes into\nplay in Algorithm 6 when we sample a random gradient index it according to the weights {pi}n\ni=1,\nand not uniformly as in Algorithm 4. This change however does not change the analysis of the\nalgorithm given in Theorem B.1, and thus we have the following theorem which is analogues to\nTheorem 5.1.\nTheorem 7.1. Fix \u01eb > 0, p > 0. There exists a choice of \u03b7, m such that Algorithm 6 \ufb01nds with\nprobability at least 1 \u2212p an \u01eb-approxiated minimizer of (16) in overall time\n\u02dcO\n\u0012\nN +\nd + S\n(\u03bb \u2212\u03bb1(X))2\n\u0013\n.\nBy Plugging Theorem 7.1 into the proof of Theorem 1.3(instead of Theorem 5.1) we arrive\nat the following theorem (analogues to Theorem 1.3).\nTheorem 7.2. Fix \u01eb > 0, p > 0. There exists an algorithm that \ufb01nds with probability at least\n1 \u2212p a unit vector w such that w\u22a4Xw \u2265\u03bb1 \u2212\u01eb, in total time \u02dcO\n\u0000 d+S\n\u01eb2\n\u0001\n.\n7.2\nFaster sublinear-time SDP algorithm\nHere we detail a speci\ufb01c application of Theorem 7.2 to accelerate the sublinear-time approx-\nimation algorithm for Semide\ufb01nite Programming recently proposed by Garber and Hazan [8].\nTowards this end we consider the following semide\ufb01nite optimization problem:\nmax\nW: W\u2ab00, Trace(W)=1 min\ni\u2208[n] W \u2022 Ai \u2212bi,\n(17)\nwhere we assume that \u2200i \u2208[n]: Ai is symmetric, \u2225Ai\u2225\u22641, \u2225Ai\u2225F \u2264F for some F, and\n|bi| \u22641. Note that under the assumption \u2225Ai\u2225\u22641 it holds that F \u2264\n\u221a\nd. We use A \u2022 B to\ndenote the dot product P\ni,j Ai,jBi,j.\nAs before we denote by N the total number of non-zero entries the matrices A1, ..., An and\nby S the maximal number of non-zero in any single matrix.\nTo the best of our knowledge the algorithm in [8] is the current state-of-the-art for approx-\nimating Problem (17) for a wide regime of the input parameters d, n, \u01eb.\n22\nTheorem 7.3 (Theorem 1 in [8]). There exists an algorithm that \ufb01nds in total time\n\u02dcO\n\u0012 1\n\u01eb2\n\u0012\nmF 2 + min{ S\n\u01eb2.5 , N\n\u221a\u01eb}\n\u0013\u0013\na solution Wf such that with probability at least 1/2 it holds that\nmin\ni\u2208[n] Wf \u2022 Ai \u2212bi \u2265\nmax\nW: W\u2ab00, Trace(W)=1 min\ni\u2208[n] W \u2022 Ai \u2212bi \u2212\u01eb.\nThe algorithm performs roughly \u01eb\u22122 iterations where each iteration is comprised of two\nmain parts: i) low-variance estimation of the products Ai \u2022 ww\u22a4\u2200i \u2208[n] for some unit vector\nw \u2208Rd, which are used to obtain a probability distribution p \u2208Rn over the functions {fi(W) :=\nAi \u2022 W \u2212bi}n\ni=1, and ii) an O(\u01eb) - approximated leading eigenvalue computation of the matrix\nPn\ni=1 piAi where p is a distribution as discussed above. The term on the right in the running\ntime stated in Theorem 7.3 comes from this approximated eigenvalue computation which is\ndone according to either the standard Lanczos method or the Sample Lanczos method detailed\nin Table 1.1.1. By replacing the Sample Lanczos procedure with Theorem 7.2 we arrive at the\nfollowing improved Theorem.\nTheorem 7.4 (Accelerated sublinear SDP solver). There exists an algorithm that \ufb01nds in total\ntime\n\u02dcO\n\u0012 1\n\u01eb2\n\u0012\nmF 2 + min{ S\n\u01eb2 , N\n\u221a\u01eb}\n\u0013\u0013\na solution Wf such that with probability at least 1/2 it holds that\nmin\ni\u2208[n] Wf \u2022 Ai \u2212bi \u2265\nmax\nW: W\u2ab00, Trace(W)=1 min\ni\u2208[n] W \u2022 Ai \u2212bi \u2212\u01eb.\nA slight technical issue is that the queried matrix X = Pn\ni=1 piAi need not be positive\nsemide\ufb01nite as we assumed in our results, however under our assumptions we can easily apply\nour results to the matrix \u02dcX = Pn\ni=1 piAi + I which is positive semide\ufb01nite, which only slightly\na\ufb00ects the leading constants in our theorems.\nReferences\n[1] Jacob D. Abernethy, Chansoo Lee, and Ambuj Tewari. Spectral smoothing via random\nmatrix perturbations. CoRR, abs/1507.03032, 2015.\n[2] Sanjeev Arora, Elad Hazan, and Satyen Kale. Fast algorithms for approximate semide.nite\nprogramming using the multiplicative weights update method. In 46th Annual IEEE Sym-\nposium on Foundations of Computer Science (FOCS 2005), 23-25 October 2005, Pittsburgh,\nPA, USA, Proceedings, pages 339\u2013348, 2005.\n[3] Sanjeev Arora, Satish Rao, and Umesh V. Vazirani. Expander \ufb02ows, geometric embeddings\nand graph partitioning. Journal of the ACM, 56(2), 2009.\n[4] Akshay Balsubramani, Sanjoy Dasgupta, and Yoav Freund. The fast convergence of incre-\nmental pca. In Advances in Neural Information Processing Systems 26, pages 3174\u20133182.\n2013.\n[5] Alexandre d\u2019Aspremont and Noureddine El Karoui. A stochastic smoothing algorithm for\nsemide\ufb01nite programming. SIAM Journal on Optimization, 24(3):1138\u20131177, 2014.\n23\n[6] Cynthia Dwork, Kunal Talwar, Abhradeep Thakurta, and Li Zhang. Analyze gauss: opti-\nmal bounds for privacy-preserving principal component analysis. In Symposium on Theory\nof Computing, STOC 2014, New York, NY, USA, May 31 - June 03, 2014, pages 11\u201320,\n2014.\n[7] Roy Frostig, Rong Ge, Sham Kakade, and Aaron Sidford. Un-regularizing: approximate\nproximal point and faster stochastic algorithms for empirical risk minimization. In Pro-\nceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille,\nFrance, 6-11 July 2015, pages 2540\u20132548, 2015.\n[8] Dan Garber and Elad Hazan.\nSublinear time algorithms for approximate semide\ufb01nite\nprogramming. Mathematical Programming, pages 1\u201333, 2015.\n[9] Dan Garber, Elad Hazan, and Tengyu Ma. Online learning of eigenvectors. In Proceedings\nof the 32nd International Conference on Machine Learning, ICML 2015, Lille, France,\n6-11 July 2015, pages 560\u2013568, 2015.\n[10] Gene H. Golub and Charles F. Van Loan. Matrix Computations (3rd Ed.). Johns Hopkins\nUniversity Press, Baltimore, MD, USA, 1996.\n[11] Elad Hazan. Sparse approximate solutions to semide\ufb01nite programs. In LATIN 2008: The-\noretical Informatics, 8th Latin American Symposium, B\u00b4uzios, Brazil, April 7-11, 2008,Pro-\nceedings, pages 306\u2013316, 2008.\n[12] Julien Mairal Hongzhou Lin and Zaid Harchaoui. A universal catalyst for \ufb01rst-order opti-\nmization. CoRR, abs/1506.02186, 2015.\n[13] H. Hotelling. Analysis of a complex of statistical variables into principal components. J.\nEduc. Psych., 24, 1933.\n[14] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive\nvariance reduction. In Advances in Neural Information Processing Systems 26: 27th Annual\nConference on Neural Information Processing Systems., pages 315\u2013323, 2013.\n[15] Jakub Konecn\u00b4y and Peter Richt\u00b4arik. Semi-stochastic gradient descent methods. CoRR,\nabs/1312.1666, 2013.\n[16] Wojciech Kotlowski and Manfred K. Warmuth. PCA with gaussian perturbations. CoRR,\nabs/1506.04855, 2015.\n[17] J. Kuczy\u00b4nski and H. Wo\u00b4zniakowski. Estimating the largest eigenvalues by the power and\nlanczos algorithms with a random start. SIAM J. Matrix Anal. Appl., 13:1094\u20131122, Oc-\ntober 1992.\n[18] Mehrdad Mahdavi, Lijun Zhang, and Rong Jin. Mixed optimization for smooth functions.\nIn Advances in Neural Information Processing Systems 26, pages 674\u2013682. 2013.\n[19] Erkki Oja. Simpli\ufb01ed neuron model as a principal component analyzer. Journal of math-\nematical biology, 15(3):267\u2013273, 1982.\n[20] K. Pearson. On lines and planes of closest \ufb01t to systems of points in space. Philosophical\nMagazine, 2(6):559\u2013572, 1901.\n24\n[21] Nicolas Le Roux, Mark W. Schmidt, and Francis R. Bach. A stochastic gradient method\nwith an exponential convergence rate for \ufb01nite training sets. In Advances in Neural Infor-\nmation Processing Systems 25: 26th Annual Conference on Neural Information Processing\nSystems., pages 2672\u20132680, 2012.\n[22] Shai Shalev-Shwartz. SDCA without duality. CoRR, abs/1502.06177, 2015.\n[23] Shai Shalev-Shwartz and Tong Zhang.\nStochastic dual coordinate ascent methods for\nregularized loss minimization. CoRR, abs/1209.1873, 2012.\n[24] Ohad Shamir. A stochastic PCA algorithm with an exponential convergence rate. CoRR,\nabs/1409.2848, 2014.\n[25] Ohad Shamir. Fast stochastic algorithms for svd and pca: Convergence properties and\nconvexity. CoRR, abs/1507.08788, 2015.\n[26] Joel A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of\nComputational Mathematics, 12(4):389\u2013434, 2012.\nA\nConvergence of the Power Method\nWe \ufb01rst restate the theorem and then prove it.\nTheorem A.1. Let M be a positive de\ufb01nite matrix and denote its eigenvalues in descending\norder by \u03bb1, \u03bb2, ..., \u03bbd, and let u1, u2, ..., ud denote the corresponding eigenvectors. Denote \u03b4 =\n\u03bb1 \u2212\u03bb2 and \u03ba = \u03bb1\n\u03b4 . Fix an error tolerance \u01eb > 0 and failure probability p > 0. De\ufb01ne:\nT PM\ncrude(\u01eb, p) = \u23081\n\u01eb ln\n\u001218d\np2\u01eb\n\u0013\n\u2309,\nT PM\nacc (\u03ba, \u01eb, p) = \u2308\u03ba\n2 ln\n\u0012 9d\np2\u01eb\n\u0013\n\u2309.\nThen, with probability 1 \u2212p it holds that\n1. (crude regime) \u2200t \u2265T PM\ncrude(\u01eb, p): w\u22a4\nt Mwt \u2265(1 \u2212\u01eb)\u03bb1.\n2. (accurate regime) \u2200t \u2265T PM\nacc (\u03ba, \u01eb, p): (w\u22a4\nt u1)2 \u22651 \u2212\u01eb.\nIn both cases, the success probability depends only on the random variable (w\u22a4\n0 u1)2.\nProof. By the update rule of Algorithm 1, it holds for all i \u2208[d] that\n(w\u22a4\nt ui)2\n=\n\u0000(Mtw0)\u22a4ui\n\u00012\n\u2225Mtw0\u22252\n= (w\u22a4\n0 Mtui)2\nw\u22a4\n0 M2tw0\n=\n(\u03bbt\niw\u22a4\n0 ui)2\nPd\nj=1 \u03bb2t\nj (w\u22a4\n0 uj)2 =\n(w\u22a4\n0 ui)2\nPd\nj=1\n\u0010\u03bbj\n\u03bbi\n\u00112t\n(w\u22a4\n0 uj)2\n\u2264\n(w\u22a4\n0 ui)2\n\u0010\n\u03bb1\n\u03bbi\n\u00112t\n(w\u22a4\n0 u1)2\n= (w\u22a4\n0 ui)2\n(w\u22a4\n0 u1)2\n\u0012 \u03bbi\n\u03bb1\n\u00132t\n.\n(18)\nSince w0 is a random unit vector, according to Lemma 5 in [3], it holds that with probability\nat least 1 \u2212p, (w\u22a4\n0 u1)2 \u2265p2\n9d. Thus we have that with probability at least 1 \u2212p it holds for all\ni \u2208[d] that\n(w\u22a4\nt ui)2 \u2264(w\u22a4\n0 ui)2 9d\np2\n\u0012\n1 \u2212\u03bb1 \u2212\u03bbi\n\u03bb1\n\u00132t\n\u2264(w\u22a4\n0 ui)2 9d\np2 \u00b7 exp\n\u0012\n\u22122\u03bb1 \u2212\u03bbi\n\u03bb1\nt\n\u0013\n.\n(19)\n25\nGiven \u01eb \u2208(0, 1), de\ufb01ne S(\u01eb) = {i \u2208[d] | \u03bbi > (1 \u2212\u01eb)\u03bb1}. Fix now \u01eb1, \u01eb2 \u2208(0, 1) and de\ufb01ne\nT(\u01eb1, \u01eb2, p) := \u23081\n2\u01eb1\nln\n\u00129d\np2 \u00b7 1\n\u01eb2\n\u0013\n\u2309.\nAccording to Eq. (19), with probability at least 1 \u2212p we have that for t \u2265T(\u01eb1, \u01eb2, p),\nfor all i /\u2208S(\u01eb1) it holds that (w\u22a4\nt ui)2 \u2264\u01eb2(w\u22a4\n0 ui)2, and thus in particular it holds that\nP\ni\u2208S(\u01eb1)(w\u22a4\nt ui)2 \u22651 \u2212\u01eb2.\nPart one of the theorem now follows by noticing that according to the above, by setting\n\u01eb1 = \u01eb2 = \u01eb/2 we have that with probability at least 1 \u2212p, for t \u2265T PM\ncrude(\u01eb, p) = T(\u01eb/2, \u01eb/2, p),\nit holds that\nw\u22a4\nt Mwt =\nd\nX\ni=1\n\u03bbi(w\u22a4\nt ui)2 \u2265\nX\ni\u2208S(\u01eb/2)\n(1 \u2212\u01eb/2)\u03bb1(w\u22a4\nt ui)2 \u2265(1 \u2212\u01eb/2)2\u03bb1 > (1 \u2212\u01eb)\u03bb1.\nFor the second part of the theorem, note that S(\u03b4/\u03bb1) = {1}. Thus we have that with\nprobability at least 1\u2212p, for all t \u2265T PM\nacc (\u03bb1/\u03b4, \u01eb, p) = T(\u03b4/\u03bb1, \u01eb, p), it holds that (w\u22a4\nt u1) \u22651\u2212\u01eb.\nB\nSVRG for Convex Functions given by Sums of Non-convex\nFunctions and its Acceleration\nSuppose we want to minimize a function F(x) that admits the following structure\nF(x) = 1\nn\nn\nX\ni=1\nfi(x),\n(20)\nwhere each fi is \u03b2 smooth, i.e.\n\u2225\u2207fi(x) \u2212\u2207fi(y)\u2225\u2264\u03b2\u2225x \u2212y\u2225\n\u2200x, y \u2208Rd,\nand F(x) is \u03c3-strongly convex, i.e.\nF(y) \u2264F(x) + (y \u2212x)\u22a4\u2207F(x) + \u03c3\n2 \u2225x \u2212y\u22252\n\u2200x, y \u2208Rd.\nAlgorithm 7 SVRG\n1: Input: \u02dcx0, \u03b7, m\n2: for s = 1, 2, ... do\n3:\n\u02dcx \u2190\u02dcxs\u22121\n4:\n\u02dc\u00b5 \u2190\u2207F(\u02dcx)\n5:\nx0 \u2190\u02dcx\n6:\nfor t = 1, 2, ..., m do\n7:\nRandomly pick it \u2208[n]\n8:\nxt \u2190xt\u22121 \u2212\u03b7 (\u2207fit(xt\u22121) \u2212\u2207fit(\u02dcx) + \u02dc\u00b5)\n9:\nend for\n10:\n\u02dcxs \u21901\nm\nPm\u22121\nt=0 xt\n11: end for\n26\nTheorem B.1. Suppose that each function fi(x) in the objective (20) is \u03b2-smooth and that\nF(x) is \u03c3-strongly convex. Then for \u03b7 =\n\u03c3\n7\u03b22 and m \u2265\n1\n2\u03b72\u03b22 it holds that\nE[\u2225\u02dcxs \u2212x\u2217\u22252] \u22642\u2212s\u2225\u02dcx0 \u2212x\u2217\u22252.\nProof. We begin by analyzing the reduction in error on a single epoch s and then apply the\nresult recursively. Let us \ufb01x an iteration t \u2208[m] of the inner loop in epoch s. In the sequel we\ndenote by Et[\u00b7] the expectation with respect to the random choice of it (i.e., the expectation is\nconditioned on all randomness introduced up to the tth iteration of the inner loop during epoch\ns). De\ufb01ne\nvt = \u2207fit(xt\u22121) \u2212\u2207fit(\u02dcx) + \u02dc\u00b5.\nNote that Et[vt] = \u2207F(xt\u22121) and thus vt is an unbiased estimator for \u2207F(xt\u22121). We continue\nto upper bound the variance of vt in terms of the distance of xt\u22121 and \u02dcx from x\u2217.\nEt[\u2225vt\u22252]\n\u2264\n2Et[\u2225\u2207fit(xt\u22121) \u2212\u2207fit(x\u2217)\u22252] + 2Et[\u2225\u2207fit(\u02dcx) \u2212\u2207fit(x\u2217) \u2212\u2207F(\u02dcx)\u22252]\n=\n2Et[\u2225\u2207fit(xt\u22121) \u2212\u2207fit(x\u2217)\u22252] + 2Et[\u2225\u2207fit(\u02dcx) \u2212\u2207fit(x\u2217)\u22252]\n\u2212\n4\u2207F(\u02dcx)\u22a4(\u2207F(\u02dcx) \u2212\u2207F(x\u2217)) + 2\u2225\u2207F(\u02dcx)\u22252\n\u2264\n2Et[\u2225\u2207fit(xt\u22121) \u2212\u2207fit(x\u2217)\u22252] + 2Et[\u2225\u2207fit(\u02dcx) \u2212\u2207fit(x\u2217)\u22252]\n\u2264\n2\u03b22(\u2225xt\u22121 \u2212x\u2217\u22252 + \u2225\u02dcx \u2212x\u2217\u22252),\nwhere the \ufb01rst inequality follows from (a + b)2 \u22642a2 + 2b2, the \ufb01rst equality follows since\nEt[fit(\u02dcx)] = \u2207F(\u02dcx) (same goes for x\u2217), the second inequality follows since \u2207F(x\u2217) = 0, and\nthe third inequality follows from smoothness of fit.\nWe now have that,\nEt[\u2225xt \u2212x\u2217\u22252]\n=\n\u2225xt\u22121 \u2212x\u2217\u22252 \u22122\u03b7(xt\u22121 \u2212x\u2217)\u22a4Et[vt] + \u03b72Et[\u2225vt\u22252]\n\u2264\n\u2225xt\u22121 \u2212x\u2217\u22252 \u22122\u03b7(xt\u22121 \u2212x\u2217)\u22a4\u2207F(xt\u22121) + 2\u03b72\u03b22(\u2225xt\u22121 \u2212x\u2217\u22252 + \u2225\u02dcx \u2212x\u2217\u22252)\n\u2264\n\u2225xt\u22121 \u2212x\u2217\u22252 \u22122\u03b7\u03c3\u2225xt\u22121 \u2212x\u2217\u22252 + 2\u03b72\u03b22(\u2225xt\u22121 \u2212x\u2217\u22252 + \u2225\u02dcx \u2212x\u2217\u22252),\nwhere the second inequality follow from convexity and strong-convexity of F.\nThus we have that,\nE[\u2225xt \u2212x\u2217\u22252] \u2212E[\u2225xt\u22121 \u2212x\u2217\u22252] \u22642\u03b7(\u03b7\u03b22 \u2212\u03c3)E[\u2225xt\u22121 \u2212x\u2217\u22252] + 2\u03b72\u03b22E[\u2225\u02dcx \u2212x\u2217\u22252].\nSumming over all iterations of the inner loop on epoch s we have\nE[\u2225xm \u2212x\u2217\u22252] \u2212E[\u2225x0 \u2212x\u2217\u22252] \u22642\u03b7(\u03b7\u03b22 \u2212\u03c3)\nm\nX\nt=1\nE[\u2225xt\u22121 \u2212x\u2217\u22252] + 2m\u03b72\u03b22E[\u2225\u02dcx \u2212x\u2217\u22252].\nRearranging and using x0 = \u02dcx we have that,\n2\u03b7(\u03c3 \u2212\u03b7\u03b22)\nm\nX\nt=1\nE[\u2225xt \u2212x\u2217\u22252] \u2264(1 + 2m\u03b72\u03b22)E[\u2225\u02dcx \u2212x\u2217\u22252].\nUsing \u02dcx = \u02dcxs\u22121 and \u02dcxs = 1\nm\nPm\u22121\nt=0 xt we have that,\nE[\u2225\u02dcxs \u2212x\u2217\u22252] \u2264\n1 + 2m\u03b72\u03b22\n2\u03b7m(\u03c3 \u2212\u03b7\u03b22)E[\u2225\u02dcxs\u22121 \u2212x\u2217\u22252].\nPlugging the values of \u03b7, m gives the theorem.\n27\nB.1\nAcceleration of Algorithm 7\nWe now discuss how using the recent generic acceleration framework of [12], we can further accel-\nerate Algorithm 7. Note that Algorithm 7 requires to compute overall \u02dcO(n + m) = \u02dcO\n\u0010\n\u03b22\n\u03c32 + n\n\u0011\ngradients of functions from the set {fi(x)}n\ni=1. Using the framework of [12], this quantity could\nbe dramatically reduced.\nOn a very high-level, the framework of [12] applies a convex optimization algorithm in an\nalmost black-box fashion in order to simulate an algorithm known as the Accelerated Proximal-\npoint algorithm. That is, it uses the convex optimization algorithm to \ufb01nd an approximated\nglobal minimizer of the modi\ufb01ed function:\n\u02dcF(x) = 1\nn\nn\nX\ni=1\nfi(x) + \u03ba\n2\u2225x \u2212y\u22252,\n(21)\nwhere \u03ba, y are parameters.\nNote that the SVRG algorithm (7) could be directly applied to minimize (21) by considering\nthe set of functions \u02dcfi(x) = fi(x) + \u03ba\n2\u2225x \u2212y\u22252 for all i \u2208[n]. It clearly holds that \u02dcF(x) =\nPn\ni=1 \u02dcfi(x). Not also that now \u02dcF(x) is \u03c3 + \u03ba strongly convex and for each i \u2208[n] it holds that\n\u02dcfi(x) is \u03b2 + \u03ba smooth.\nThe following Theorem (rephrased for our needs) is proven in [12] (Theorem 3.1).\nTheorem B.2. Fix the parameter \u03ba.\nThere exists an acceleration scheme for Algorithm 7\nthat \ufb01nds an \u01eb-approximated minimizer of (20), after approximately minimizing \u02dcO\n\u0010q\n\u03c3+\u03ba\n\u03c3\n\u0011\ninstances of (21).\nPlugging Theorems B.1, B.2 and the proprieties of \u02dcF(x) we have that Algorithm 7 could be\napplied to \ufb01nding an \u01eb minimizer of (20) in total time:\n\u02dcO\n r\n\u03c3 + \u03ba\n\u03c3\n \nTG +\n\u0012\u03b2 + \u03ba\n\u03c3 + \u03ba\n\u00132\nTg\n!!\n,\nwhere TG denotes the time to evaluate the gradient of F and Tg denotes the worst case time to\nevaluate the gradient of a single function fi.\nBy optimizing the above bound with respect to \u03ba we arrive at the following theorem.\nTheorem B.3. Assume that the gradient vector of each function fi(x) could be computed in\nO(d) time. Assume further that \u03b2 = \u2126(\np\nTG/Tg\u03c3). Algorithm 7 combined with the acceleration\nframework of [12], \ufb01nds an \u01eb-approximated minimizer of (20) in total time \u02dcO\n\u0012q\n\u03b2\n\u03c3T 3/4\nG T 1/4\ng\n\u0013\n.\n28\n",
        "sentence": " This can be done via a \u201cbinary search\u201d procedure which was used widely for shift-and-invert based methods [15]: (This requires O(1) iterations of power method applied to (cI\u2212 \u03b7\u03a3k\u22121)\u22121 [15]. It is a simple exercise (with details given in [15]) to show that when the procedure ends, it satisfies 1 2e \u2264 c \u2212 \u03b7\u03a3k\u22121 \u2264 1e so c is a lower bound on ck.",
        "context": "We assume we are given an estimate \u02c6\u03b4 such that\nc1\u03b4 \u2264\u02c6\u03b4 \u2264c2\u03b4,\nfor some universal constants c1, c2 which satisfy c2 \u2212c1 = \u0398(1). Note that \ufb01nding such a \u02c6\u03b4\ncould be done in time that is logarithmic in 1/\u03b4.\nprobability at least 1 \u2212p.\nLet us now assume that all executions of the Power Method algorithm in Algorithm 1 were\nsuccessful.\nAccording to Lemma 3.2, the loop is executed at most O(log(\u02c6\u03b4\u22121)) = O(log(\u03b4\u22121)) times\nnumber of iterations is at most T + 1 = O(log(\u02c6\u03b4\u22121)).\nFinally, the following lemma gives approximation guarantees on the estimate \u03bb(f).\nLemma 3.3. Suppose that all executions of the Power Method in Algorithm 2 are successful.\nThen it holds that\n\u03bb1 + 3\u02c6\u03b4"
    },
    {
        "title": "Online learning of eigenvectors",
        "author": [
            "Dan Garber",
            "Elad Hazan",
            "Tengyu Ma"
        ],
        "venue": "In Proceedings of the 32nd International Conference on Machine Learning",
        "citeRegEx": "16",
        "shortCiteRegEx": "16",
        "year": 2015,
        "abstract": "",
        "full_text": "",
        "sentence": " This result resolves an open question regarding how to obtain both (nearly) optimal and efficient algorithms for the online eigenvector problem [16]. If instead of playing an arbitrary matrix in \u2206d, the player is only allowed to play a rank-1 matrix Wk = wkw > k , then this online matrix optimization becomes the well-known online eigenvector problem [2, 13, 16, 21, 25]: FTPL (T \u2265 d only) [16] \u00d5( \u221a dT ) \u00d5 ( T 3 4 d\u2212 1 4 nnz(\u03a3) ) \u00d5 ( d \u03b53. block power method [16] \u00d5( \u221a T ) O ( nnz(\u03a3) ) \u00d5 ( 1 \u03b52 nnz(\u03a3) ) Many researchers also analyzed the so-called followthe-perturbed-leader (FTPL) strategy for this problem [2, 13, 16, 21]. Most notably, Garber, Hazan and Ma [16] proposed to compute an (approximate) leading eigenvector of the matrix \u03a3k\u22121 +rr> at iteration k, where r is a random vector whose norm is carefully chosen. For this problem, Garber, Hazan, and Ma [16] showed that a block power method matches the optimum regret and enjoys an efficient O(nnz(\u03a3T ))-time implementation per iteration. If nnz(\u03a3T ) = d 2 and nnz(A) = O(d), our running time is O(d) times faster than [16]. block power method [16] \u00d5( \u221a \u03bbT ) O ( nnz(\u03a3) ) \u00d5 ( 1 \u03b52 nnz(\u03a3) ) 7 The block power method (for the stochastic online eigenvector problem) can also be analyzed in this \u03bb-refined language, for instance by modifying the proof in [16]. We thank Dan Garber and Tengyu Ma for clarifying some results of prior work [16]. [16]",
        "context": null
    },
    {
        "title": "The noisy power method: A meta algorithm with applications",
        "author": [
            "Moritz Hardt",
            "Eric Price"
        ],
        "venue": "In NIPS,",
        "citeRegEx": "17",
        "shortCiteRegEx": "17",
        "year": 2014,
        "abstract": "We provide a new robust convergence analysis of the well-known power method\nfor computing the dominant singular vectors of a matrix that we call the noisy\npower method. Our result characterizes the convergence behavior of the\nalgorithm when a significant amount noise is introduced after each\nmatrix-vector multiplication. The noisy power method can be seen as a\nmeta-algorithm that has recently found a number of important applications in a\nbroad range of machine learning problems including alternating minimization for\nmatrix completion, streaming principal component analysis (PCA), and\nprivacy-preserving spectral analysis. Our general analysis subsumes several\nexisting ad-hoc convergence bounds and resolves a number of open problems in\nmultiple applications including streaming PCA and privacy-preserving singular\nvector computation.",
        "full_text": "The Noisy Power Method:\nA Meta Algorithm with Applications\nMoritz Hardt\u2217\nEric Price\u2020\nFebruary 5, 2015\nAbstract\nWe provide a new robust convergence analysis of the well-known power method for\ncomputing the dominant singular vectors of a matrix that we call the noisy power method.\nOur result characterizes the convergence behavior of the algorithm when a signi\ufb01cant\namount noise is introduced after each matrix-vector multiplication. The noisy power\nmethod can be seen as a meta-algorithm that has recently found a number of important\napplications in a broad range of machine learning problems including alternating min-\nimization for matrix completion, streaming principal component analysis (PCA), and\nprivacy-preserving spectral analysis. Our general analysis subsumes several existing ad-\nhoc convergence bounds and resolves a number of open problems in multiple applications:\nStreaming PCA. A recent work of Mitliagkas et al. (NIPS 2013) gives a space-e\ufb03cient\nalgorithm for PCA in a streaming model where samples are drawn from a gaussian spiked\ncovariance model. We give a simpler and more general analysis that applies to arbitrary\ndistributions con\ufb01rming experimental evidence of Mitliagkas et al. Moreover, even in the\nspiked covariance model our result gives quantitative improvements in a natural parameter\nregime. It is also notably simpler and follows easily from our general convergence analysis\nof the noisy power method together with a matrix Cherno\ufb00bound.\nPrivate PCA. We provide the \ufb01rst nearly-linear time algorithm for the problem of dif-\nferentially private principal component analysis that achieves nearly tight worst-case error\nbounds. Complementing our worst-case bounds, we show that the error dependence of our\nalgorithm on the matrix dimension can be replaced by an essentially tight dependence on\nthe coherence of the matrix. This result resolves the main problem left open by Hardt and\nRoth (STOC 2013). The coherence is always bounded by the matrix dimension but often\nsubstantially smaller thus leading to strong average-case improvements over the optimal\nworst-case bound.\n1\nIntroduction\nComputing the dominant singular vectors of a matrix is one of the most important algorith-\nmic tasks underlying many applications including low-rank approximation, PCA, spectral\nclustering, dimensionality reduction, matrix completion and topic modeling. The classical\nproblem is well-understood, but many recent applications in machine learning face the fun-\ndamental problem of approximately \ufb01nding singular vectors in the presence of noise. Noise\n\u2217IBM Research Almaden. Email: mhardt@us.ibm.com\n\u2020IBM Research Almaden. Email: ecprice@mit.edu\n1\narXiv:1311.2495v4  [cs.DS]  3 Feb 2015\ncan enter the computation through a variety of sources including sampling error, missing\nentries, adversarial corruptions and privacy constraints. It is desirable to have one robust\nmethod for handling a variety of cases without the need for ad-hoc analyses. In this paper we\nconsider the noisy power method, a fast general purpose method for computing the dominant\nsingular vectors of a matrix when the target matrix can only be accessed through inaccurate\nmatrix-vector products.\nFigure 1 describes the method when the target matrix A is a symmetric d \u00d7 d matrix\u2014a\ngeneralization to asymmetric matrices is straightforward. The algorithm starts from an initial\nmatrix X0 \u2208Rd\u00d7p and iteratively attempts to perform the update rule X\u2113\u2192AX\u2113. However,\neach such matrix product is followed by a possibly adversarially and adaptively chosen\nperturbation G\u2113leading to the update rule X\u2113\u2192AX\u2113+ G\u2113. It will be convenient though not\nnecessary to maintain that X\u2113has orthonormal columns which can be achieved through a\nQR-factorization after each update.\nInput: Symmetric matrix A \u2208Rd\u00d7d, number of iterations L, dimension p\n1. Choose X0 \u2208Rd\u00d7p.\n2. For \u2113= 1 to L:\n(a) Y\u2113\u2190AX\u2113\u22121 + G\u2113where G\u2113\u2208Rd\u00d7p is some perturbation\n(b) Let Y\u2113= X\u2113R\u2113be a QR-factorization of Y\u2113\nOutput: Matrix XL\nFigure 1: Noisy Power Method (NPM)\nThe noisy power method is a meta algorithm that when instantiated with di\ufb00erent settings\nof G\u2113and X0 adapts to a variety of applications. In fact, there have been a number of recent\nsurprising applications of the noisy power method:\n1. Jain et al. [JNS13, Har14] observe that the update rule of the well-known alternating\nleast squares heuristic for matrix completion can be considered as an instance of NPM.\nThis lead to the \ufb01rst provable convergence bounds for this important heuristic.\n2. Mitgliakas et al. [MCJ13] observe that NPM applies to a streaming model of principal\ncomponent analysis (PCA) where it leads to a space-e\ufb03cient and practical algorithm for\nPCA in settings where the covariance matrix is too large to process directly.\n3. Hardt and Roth [HR13] consider the power method in the context of privacy-preserving\nPCA where noise is added to achieve di\ufb00erential privacy.\nIn each setting there has so far only been an ad-hoc analysis of the noisy power method. In\nthe \ufb01rst setting, only local convergence is argued, that is, X0 has to be cleverly chosen. In the\nsecond setting, the analysis only holds for the spiked covariance model of PCA. In the third\napplication, only the case p = 1 was considered.\nIn this work we give a completely general analysis of the noisy power method that over-\ncomes limitations of previous analyses. Our result characterizes the global convergence\nproperties of the algorithm in terms of the noise G\u2113and the initial subspace X0. We then\nconsider the important case where X0 is a randomly chosen orthonormal basis. This case\nis rather delicate since the initial correlation between a random matrix X0 and the target\nsubspace is vanishing in the dimension d for small p. Another important feature of the analysis\n2\nis that it shows how X\u2113converges towards the \ufb01rst k \u2a7dp singular vectors. Choosing p to\nbe larger than the target dimension leads to a quantitatively stronger result. Theorem 2.4\nformally states our convergence bound. Here we highlight one useful corollary to illustrate\nour more general result.\nCorollary 1.1. Let k \u2a7dp. Let U \u2208Rd\u00d7k represent the top k singular vectors of A and let \u03c31 \u2a7e\u00b7\u00b7\u00b7 \u2a7e\n\u03c3n \u2a7e0 denote its singular values. Suppose X0 is an orthonormal basis of a random p-dimensional\nsubspace. Further suppose that at every step of NPM we have\n5\u2225G\u2113\u2225\u2a7d\u03b5(\u03c3k \u2212\u03c3k+1)\nand\n5\u2225U\u22a4G\u2113\u2225\u2a7d(\u03c3k \u2212\u03c3k+1)\n\u221ap\u2212\n\u221a\nk\u22121\n\u03c4\n\u221a\nd\nfor some \ufb01xed parameter \u03c4 and \u03b5 < 1/2. Then with all but \u03c4\u2212\u2126(p+1\u2212k) + e\u2212\u2126(d) probability, there\nexists an L = O(\n\u03c3k\n\u03c3k\u2212\u03c3k+1 log(d\u03c4/\u03b5)) so that after L steps we have that\n\r\r\r(I \u2212XLX\u22a4\nL )U\n\r\r\r \u2a7d\u03b5.\nThe corollary shows that the algorithm converges in the strong sense that the entire spectral\nnorm of U up to an \u03b5 error is contained in the space spanned by XL. To achieve this the result\nplaces two assumptions on the magnitude of the noise. The total spectral norm of G\u2113must be\nbounded by \u03b5 times the separation between \u03c3k and \u03c3k+1. This dependence on the singular value\nseparation arises even in the classical perturbation theory of Davis-Kahan [DK70]. The second\ncondition is speci\ufb01c to the power method and requires that the noise term is proportionally\nsmaller when projected onto the space spanned by the top k singular vectors. This condition\nensures that the correlation between X\u2113and U that is initially very small is not destroyed\nby the noise addition step. If the noise term has some spherical properties (e.g. a Gaussian\nmatrix), we expect the projection onto U to be smaller by a factor of\n\u221a\nk/d, since the space U\nis k-dimensional. In the case where p = k + \u2126(k) this is precisely what the condition requires.\nWhen p = k the requirement is stronger by a factor of k. This phenomenon stems from the fact\nthat the smallest singular value of a random p \u00d7 k gaussian matrix behaves di\ufb00erently in the\nsquare and the rectangular case.\nWe demonstrate the usefulness of our convergence bound with several novel results in\nsome of the aforementioned applications.\n1.1\nApplication to memory-e\ufb03cient streaming PCA\nIn the streaming PCA setting we receive a stream of samples z1,z2,...zn \u2208Rd drawn i.i.d. from\nan unknown distribution D over Rd. Our goal is to compute the dominant k eigenvectors of\nthe covariance matrix A = Ez\u223cD zz\u22a4. The challenge is to do this in space linear in the output\nsize, namely O(kd). Recently, Mitgliakas et al. [MCJ13] gave an algorithm for this problem\nbased on the noisy power method. We analyze the same algorithm, which we restate here and\ncall SPM:\nThe algorithm can be executed in space O(pd) since the update step can compute the\nd \u00d7 p matrix A\u2113X\u2113\u22121 incrementally without explicitly computing A\u2113. The algorithm maps to\nour setting by de\ufb01ning G\u2113= (A\u2113\u2212A)X\u2113\u22121. With this notation Y\u2113= AX\u2113\u22121 + G\u2113. We can apply\nCorollary 1.1 directly once we have suitable bounds on \u2225G\u2113\u2225and \u2225U\u22a4G\u2113\u2225.\nThe result of [MCJ13] is speci\ufb01c to the spiked covariance model. The spiked covariance\nmodel is de\ufb01ned by an orthonormal basis U \u2208Rd\u00d7k and a diagonal matrix \u039b \u2208Rk\u00d7k with\ndiagonal entries \u03bb1 \u2a7e\u03bb2 \u2a7e\u00b7\u00b7\u00b7 \u2a7e\u03bbk > 0. The distribution D(U,\u039b) is de\ufb01ned as the normal\ndistribution N(0,(U\u039b2U\u22a4+ \u03c32Idd\u00d7d)). Without loss of generality we can scale the examples\n3\nInput: Stream of samples z1,z2,...,zn \u2208Rd, iterations L, dimension p\n1. Let X0 \u2208Rd\u00d7p be a random orthonormal basis. Let T = \u230am/L\u230b\n2. For \u2113= 1 to L:\n(a) Compute Y\u2113= A\u2113X\u2113\u22121 where A\u2113= P\u2113T\ni=(\u2113\u22121)T +1 ziz\u22a4\ni\n(b) Let Y\u2113= X\u2113R\u2113be a QR-factorization of Y\u2113\nOutput: Matrix XL\nFigure 2: Streaming Power Method (SPM)\nsuch that \u03bb1 = 1. One corollary of our result shows that the algorithm outputs XL such that\n\r\r\r(I \u2212XLX\u22a4\nL )U\n\r\r\r \u2a7d\u03b5 with probability 9/10 provided p = k + \u2126(k) and the number of samples\nsatis\ufb01es\nn = \u0398\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\u03c36 + 1\n\u03b52\u03bb6\nk\nkd\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8.\nPreviously, the same bound1 was known with a quadratic dependence on k in the case where\np = k. Here we can strengthen the bound by increasing p slightly.\nWhile we can get some improvements even in the spiked covariance model, our result is\nsubstantially more general and applies to any distribution. The sample complexity bound we\nget varies according to a technical parameter of the distribution. Roughly speaking, we get a\nnear linear sample complexity if the distribution is either \u201cround\u201d (as in the spiked covariance\nsetting) or is very well approximated by a k dimensional subspace. To illustrate the latter\ncondition, we have the following result without making any assumptions other than scaling\nthe distribution:\nCorollary 1.2. Let D be any distribution scaled so that Pr{\u2225z\u2225> t} \u2a7dexp(\u2212t) for every t \u2a7e1. Let U\nrepresent the top k eigenvectors of the covariance matrix Ezz\u22a4and \u03c31 \u2a7e\u00b7\u00b7\u00b7 \u2a7e\u03c3d \u2a7e0 its eigenvalues.\nThen, SPM invoked with p = k + \u2126(k) outputs a matrix XL such with probability 9/10 we have\n\r\r\r(I \u2212XLX\u22a4\nL )U\n\r\r\r \u2a7d\u03b5 provided SPM receives n samples where n satis\ufb01es n = \u02dcO\n\u0010\n\u03c3k\n\u03b52k(\u03c3k\u2212\u03c3k+1)3 \u00b7 d\n\u0011\n.\nThe corollary establishes a sample complexity that\u2019s linear in d provided that the spectrum\ndecays quickly, as is common in applications. For example, if the spectrum follows a power\nlaw so that \u03c3j \u2248j\u2212c for a constant c > 1/2, the bound becomes n = \u02dcO(k2c+2d/\u03b52).\n1.2\nApplication to privacy-preserving spectral analysis\nMany applications of singular vector computation are plagued by the fact that the underlying\nmatrix contains sensitive information about individuals. A successful paradigm in privacy-\npreserving data analysis rests on the notion of di\ufb00erential privacy which requires all access\nto the data set to be randomized in such a way that the presence or absence of a single data\nitem is hidden. The notion of data item varies and could either refer to a single entry, a single\nrow, or a rank-1 matrix of bounded norm. More formally, Di\ufb00erential Privacy requires that\nthe output distribution of the algorithm changes only slightly with the addition or deletion of\n1That the bound stated in [MCJ13] has a \u03c36 dependence is not completely obvious. There is a O(\u03c34) in the\nnumerator and log((\u03c32 + 0.75\u03bb2\nk)/(\u03c32 + 0.5\u03bb2\nk)) in the denominator which simpli\ufb01es to O(1/\u03c32) for constant \u03bbk and\n\u03c32 \u2a7e1.\n4\na single data item. This requirement often necessitates the introduction of signi\ufb01cant levels\nof noise that make the computation of various objectives challenging. Di\ufb00erentially private\nsingular vector computation has been studied actively in recent years [BDMN05, MM09,\nBBDS12, CSS12, KT13, HR12, HR13, DTTZ14]. There are two main objectives. The \ufb01rst is\ncomputational e\ufb03ciency. The second objective is to minimize the amount of error that the\nalgorithm introduces.\nIn this work, we give a fast algorithm for di\ufb00erentially private singular vector computation\nbased on the noisy power method that leads to nearly optimal bounds in a number of settings\nthat were considered in previous work. The algorithm is described in Figure 3. It\u2019s a simple\ninstance of NPM in which each noise matrix G\u2113is a gaussian random matrix scaled so that the\nalgorithm achieves (\u03b5,\u03b4)-di\ufb00erential privacy (as formally de\ufb01ned in De\ufb01nition 4.1). It is easy\nto see that the algorithm can be implemented in time nearly linear in the number of nonzero\nentries of the input matrix (input sparsity). This will later lead to strong improvements in\nrunning time compared with several previous works.\nInput: Symmetric A \u2208Rd\u00d7d, L, p, privacy parameters \u03b5,\u03b4 > 0\n1. Let X0 be a random orthonormal basis and put \u03c3 = \u03b5\u22121p\n4pLlog(1/\u03b4)\n2. For \u2113= 1 to L:\n(a) Y\u2113\u2190AX\u2113\u22121 + G\u2113where G\u2113\u223cN(0,\u2225X\u2113\u22121\u22252\n\u221e\u03c32)d\u00d7p.\n(b) Compute the QR-factorization Y\u2113= X\u2113R\u2113\nOutput: Matrix XL\nFigure 3: Private Power Method (PPM). Here \u2225X\u2225\u221e= maxij |Xij|.\nWe \ufb01rst state a general purpose analysis of PPM that follows from Corollary 1.1.\nTheorem 1.3. Let k \u2a7dp. Let U \u2208Rd\u00d7k represent the top k singular vectors of A and let \u03c31 \u2a7e\n\u00b7\u00b7\u00b7 \u2a7e\u03c3d \u2a7e0 denote its singular values. Then, PPM satis\ufb01es (\u03b5,\u03b4)-di\ufb00erential privacy and after\nL = O(\n\u03c3k\n\u03c3k\u2212\u03c3k+1 log(d)) iterations we have with probability 9/10 that\n\r\r\r(I \u2212XLX\u22a4\nL )U\n\r\r\r \u2a7dO\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\u03c3 max\u2225X\u2113\u2225\u221e\np\nd logL\n\u03c3k \u2212\u03c3k+1\n\u00b7\n\u221ap\n\u221ap \u2212\n\u221a\nk \u22121\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8.\nWhen p = k + \u2126(k) the trailing factor becomes a constant. If p = k it creates a factor k\noverhead. In the worst-case we can always bound \u2225X\u2113\u2225\u221eby 1 since X\u2113is an orthonormal\nbasis. However, in principle we could hope that a much better bound holds provided that\nthe target subspace U has small coordinates. Hardt and Roth [HR12, HR13] suggested a\nway to accomplish a stronger bound by considering a notion of coherence of A, denoted\nas \u00b5(A). Informally, the coherence is a well-studied parameter that varies between 1 and\nn, but is often observed to be small. Intuitively, the coherence measures the correlation\nbetween the singular vectors of the matrix with the standard basis. Low coherence means\nthat the singular vectors have small coordinates in the standard basis. Many results on matrix\ncompletion and robust PCA crucially rely on the assumption that the underlying matrix has\nlow coherence [CR09, CT10, CLMW11] (though the notion of coherence here will be somewhat\ndi\ufb00erent).\n5\nTheorem 1.4. Under the assumptions of Theorem 1.3, we have the conclusion\n\r\r\r(I \u2212XLX\u22a4\nL )U\n\r\r\r \u2a7dO\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\u03c3\np\n\u00b5(A)logd logL\n\u03c3k \u2212\u03c3k+1\n\u00b7\n\u221ap\n\u221ap \u2212\n\u221a\nk \u22121\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8.\nHardt and Roth proved this result for the case where p = 1. The extension to p > 1 lost\na factor of\n\u221a\nd in general and therefore gave no improvement over Theorem 1.3. Our result\nresolves the main problem left open in their work. The strength of Theorem 1.4 is that the\nbound is essentially dimension-free under a natural assumption on the matrix and never worse\nthan our worst-case result. It is also known that in general the dependence on d achieved in\nTheorem 1.3 is best possible in the worst case (see discussion in [HR13]) so that further progress\nrequires making stronger assumptions. Coherence is a natural such assumption. The proof of\nTheorem 1.4 proceeds by showing that each iterate X\u2113satis\ufb01es \u2225X\u2113\u2225\u221e\u2a7dO(\np\n\u00b5(A)log(d)/d) and\napplying Theorem 1.3. To do this we exploit a non-trivial symmetry of the algorithm that we\ndiscuss in Section 4.3.\nOther objective functions and variants di\ufb00erential privacy.\nAn important recent work\nby Dwork, Talwar, Thakurta and Zhang analyzes the mechanism of adding Gaussian noise\nto the covariance matrix and computing a truncated singular value decomposition of the\nnoisy covariance matrix [DTTZ14]. Their objective function is a natural measure of how\nmuch variance of the data is captured by the resulting subspace. Our results are formally\nincomparable due to a di\ufb00erent choice of objective function. We also do not know how to\nanalyze the performance of the power method under their objective function. Indeed, this is\nan interesting question related to the content of Conjecture 1.6 that we will state shortly.\nOur discussion above applied to (\u03b5,\u03b4)-di\ufb00erential privacy under changing a single entry of\nthe matrix. Several works consider other variants of di\ufb00erential privacy. It is generally easy\nto adapt the power method to these settings by changing the noise distribution or its scaling.\nTo illustrate this aspect, we consider the problem of privacy-preserving principal component\nanalysis as recently studied by [CSS12, KT13]. Both works consider an algorithm called\nexponential mechanism. The \ufb01rst work gives a heuristic implementation that may not converge,\nwhile the second work gives a provably polynomial time algorithm though the running time\nis more than cubic. Our algorithm gives strong improvements in running time while giving\nnearly optimal accuracy guarantees as it matches a lower bound of [KT13] up to a \u02dcO(\n\u221a\nk) factor.\nWe also improve the error dependence on k by polynomial factors compared to previous work.\nMoreover, we get an accuracy improvement of O(\n\u221a\nd) for the case of (\u03b5,\u03b4)-di\ufb00erential privacy,\nwhile these previous works only apply to (\u03b5,0)-di\ufb00erential privacy. Section 4.2 provides formal\nstatements.\n1.3\nRelated Work\nNumerical Analysis.\nOne might expect that a suitable analysis of the noisy power method\nwould have appeared in the numerical analysis literature. However, we are not aware of a\nreference and there are a number of points to consider. First, our noise model is adaptive\nthus setting it apart from the classical perturbation theory of the singular vector decomposi-\ntion [DK70]. Second, we think of the perturbation at each step as large making it conceptually\ndi\ufb00erent from \ufb02oating point errors. Third, research in numerical analysis over the past decades\n6\nhas largely focused on faster Krylov subspace methods. There is some theory of inexact Krylov\nmethods, e.g., [SS07] that captures the e\ufb00ect of noisy matrix-vector products in this context.\nRelated to our work are also results on the perturbation stability of the QR-factorization\nsince those could be used to obtain convergence bounds for subspace iteration. Such bounds,\nhowever, must depend on the condition number of the matrix that the QR-factorization is\napplied to. See Chapter 19.9 in [Hig02] and the references therein for background. Our proof\nstrategy avoids this particular dependence on the condition number.\nStreaming PCA.\nPCA in the streaming model is related to a host of well-studied problems\nthat we cannot survey completely here. We refer to [ACLS12, MCJ13] for a thorough discussion\nof prior work. Not mentioned therein is a recent work on incremental PCA [BDF13] that leads\nto space e\ufb03cient algorithms computing the top singular vector; however, it\u2019s not clear how to\nextend their results to computing multiple singular vectors.\nPrivacy.\nThere has been much work on di\ufb00erentially private spectral analysis starting with\nBlum et al. [BDMN05] who used an algorithm sometimes called Randomized Response, which\nadds a single noise matrix N either to the input matrix A or the covariance matrix AA\u22a4. This\napproach was used by McSherry and Mironov [MM09] for the purpose of a di\ufb00erentially\nprivate recommender system. Most recently, as discussed earlier, Dwork, Talwar, Thakurta\nand Zhang [DTTZ14] revisit (a variant of) the this algorithm and give matching upper and\nlower bounds under a natural objective function. While often suitable when AA\u22a4\ufb01ts into\nmemory, the approach can be di\ufb03cult to apply when the dimension of AA\u22a4is huge as it\nrequires computing a dense noise matrix N. The power method can be applied more easily to\nlarge sparse matrices, as well as in a streaming setting as shown by [MCJ13].\nChaudhuri et al. [CSS12] and Kapralov-Talwar [KT13] use the so-called exponential mecha-\nnism to sample approximate eigenvectors of the matrix. The sampling is done using a heuristic\napproach without convergence polynomial time convergence guarantees in the \ufb01rst case and\nusing a polynomial time algorithm in the second. Both papers achieve a tight dependence on\nthe matrix dimension d (though the dependence on k is suboptimal in general). Most closely\nrelated to our work are the results of Hardt and Roth [HR13, HR12] that introduced matrix\ncoherence as a way to circumvent existing worst-case lower bounds on the error. They also\nanalyzed a natural noisy variant of power iteration for the case of computing the dominant\neigenvector of A. When multiple eigenvectors are needed, their algorithm uses the well-known\nde\ufb02ation technique. However, this step loses control of the coherence of the original matrix\nand hence results in suboptimal bounds. In fact, a\np\nrank(A) factor is lost.\n1.4\nOpen Questions\nWe believe Corollary 1.1 to be a fairly precise characterization of the convergence of the\nnoisy power method to the top k singular vectors when p = k. The main \ufb02aw is that the\nnoise tolerance depends on the eigengap \u03c3k \u2212\u03c3k+1, which could be very small. We have some\nconjectures for results that do not depend on this eigengap.\nFirst, when p > k, we think that Corollary 1.1 might hold using the gap \u03c3k \u2212\u03c3p+1 instead of\n\u03c3k \u2212\u03c3k+1. Unfortunately, our proof technique relies on the principal angle decreasing at each\nstep, which does not necessarily hold with the larger level of noise. Nevertheless we expect the\n7\nprincipal angle to decrease fairly fast on average, so that XL will contain a subspace very close\nto U. We are actually unaware of this sort of result even in the noiseless setting.\nConjecture 1.5. Let X0 be a random p-dimensional basis for p > k. Suppose at every step we have\n100\u2225G\u2113\u2225\u2a7d\u03b5(\u03c3k \u2212\u03c3p+1)\nand\n100\u2225UT G\u2113\u2225\u2a7d\n\u221ap \u2212\n\u221a\nk \u22121\n\u221a\nd\nThen with high probability, after L = O(\n\u03c3k\n\u03c3k\u2212\u03c3p+1 log(d/\u03b5)) iterations we have\n\u2225(I \u2212XLX\u22a4\nL )U\u2225\u2a7d\u03b5.\nThe second way of dealing with a small eigengap would be to relax our goal. Corollary 1.1\nis quite stringent in that it requires XL to approximate the top k singular vectors U, which gets\nharder when the eigengap approaches zero and the kth through p + 1st singular vectors are\nnearly indistinguishable. A relaxed goal would be for XL to spectrally approximate A, that is\n\u2225(I \u2212XLX\u22a4\nL )A\u2225\u2a7d\u03c3k+1 + \u03b5.\n(1)\nThis weaker goal is known to be achievable in the noiseless setting without any eigengap at\nall. In particular, [HMT11] shows that (1) happens after L = O(\u03c3k+1\n\u03b5 logn) steps in the noiseless\nsetting. A plausible extension to the noisy setting would be:\nConjecture 1.6. Let X0 be a random 2k-dimensional basis. Suppose at every step we have\n\u2225G\u2113\u2225\u2a7d\u03b5\nand\n\u2225UT G\u2113\u2225\u2a7d\u03b5\n\u221a\nk/d\nThen with high probability, after L = O(\u03c3k+1\n\u03b5 logd) iterations we have that\n\u2225(I \u2212XLX\u22a4\nL )A\u2225\u2a7d\u03c3k+1 + O(\u03b5).\n2\nConvergence of the noisy power method\nFigure 1 presents our basic algorithm that we analyze in this section. An important tool in\nour analysis are principal angles, which are useful in analyzing the convergence behavior\nof numerical eigenvalue methods. Roughly speaking, we will show that the tangent of the\nk-th principal angle between X and the top k eigenvectors of A decreases as \u03c3k+1/\u03c3k in each\niteration of the noisy power method.\nDe\ufb01nition 2.1 (Principal angles). Let X and Y be subspaces of Rd of dimension at least k. The\nprincipal angles 0 \u2a7d\u03b81 \u2a7d\u00b7\u00b7\u00b7 \u2a7d\u03b8k between X and Y and associated principal vectors x1,...,xk and\ny1,...,yk are de\ufb01ned recursively via\n\u03b8i(X ,Y) = min\n(\narccos\n \n\u27e8x,y\u27e9\n\u2225x\u22252\u2225y\u22252\n!\n: x \u2208X ,y \u2208Y,x \u22a5xj,y \u22a5yj for all j < i\n)\nand xi,yi are the x and y that give this value. For matrices X and Y, we use \u03b8k(X,Y) to denote\nthe kth principal angle between their ranges.\n8\n2.1\nConvergence argument\nWe will make use of a non-recursive expression for the principal angles, de\ufb01ned in terms of\nthe set Pk of p \u00d7 p projection matrices \u03a0 from p dimensions to k dimensional subspaces:\nClaim 2.2. Let U \u2208Rd\u00d7k have orthonormal columns and X \u2208Rd\u00d7p have independent columns, for\np \u2a7ek. Then\ncos\u03b8k(U,X) = max\n\u03a0\u2208Pk\nmin\nx\u2208range(X\u03a0)\n\u2225x\u22252=1\n\u2225U\u22a4x\u2225= max\n\u03a0\u2208Pk\nmin\n\u2225w\u22252=1\n\u03a0w=w\n\u2225U\u22a4Xw\u2225\n\u2225Xw\u2225\n.\nFor V = U\u22a5, we have\ntan\u03b8k(U,X) = min\n\u03a0\u2208Pk\nmax\nx\u2208range(X\u03a0)\n\u2225V \u22a4x\u2225\n\u2225U\u22a4x\u2225= min\n\u03a0\u2208Pk\nmax\n\u2225w\u22252=1\n\u03a0w=w\n\u2225V \u22a4Xw\u2225\n\u2225U\u22a4Xw\u2225.\nFix parameters 1 \u2a7dk \u2a7dp \u2a7dd. In this section we consider a symmetric d \u00d7 d matrix A with\nsingular values \u03c31 \u2a7e\u03c32 \u2a7e\u00b7\u00b7\u00b7 \u2a7e\u03c3d. We let U \u2208Rd\u00d7k contain the \ufb01rst k eigenvectors of A. Our\nmain lemma shows that tan\u03b8k(U,X) decreases multiplicatively in each step.\nLemma 2.3. Let U contain the largest k eigenvectors of a symmetric matrix A \u2208Rd\u00d7d, and let\nX \u2208Rd\u00d7p with XtransX = Id for some p \u2a7ek. Let G \u2208Rd\u00d7p satisfy\n4\u2225U\u22a4G\u2225\u2a7d(\u03c3k \u2212\u03c3k+1)cos\u03b8k(U,X)\n4\u2225G\u2225\u2a7d(\u03c3k \u2212\u03c3k+1)\u03b5.\nfor some \u03b5 < 1. Then\ntan\u03b8k(U,AX + G) \u2a7dmax\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\u03b5,max\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\u03b5,\n \u03c3k+1\n\u03c3k\n!1/4\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8tan\u03b8k(U,X)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8.\nProof. Let \u03a0\u2217be the matrix projecting onto the smallest k principal angles of X, so that\ntan\u03b8k(U,X) = max\n\u2225w\u22252=1\n\u03a0\u2217w=w\n\u2225V \u22a4Xw\u2225\n\u2225U\u22a4Xw\u2225.\nWe have that\ntan\u03b8k(U,AX + G) = min\n\u03a0\u2208Pk\nmax\n\u2225w\u22252=1\n\u03a0w=w\n\u2225V \u22a4(AX + G)w\u2225\n\u2225U\u22a4(AX + G)w\u2225\n\u2a7dmax\n\u2225w\u22252=1\n\u03a0\u2217w=w\n\u2225V \u22a4AXw\u2225+ \u2225V \u22a4Gw\u2225\n\u2225U\u22a4AXw\u2225\u2212\u2225U\u22a4Gw\u2225\n\u2a7dmax\n\u2225w\u22252=1\n\u03a0\u2217w=w\n1\n\u2225U\u22a4Xw\u2225\u00b7 \u03c3k+1\u2225V \u22a4Xw\u2225+ \u2225V \u22a4Gw\u2225\n\u03c3k \u2212\u2225U\u22a4Gw\u2225/\u2225U\u22a4Xw\u2225\n(2)\nDe\ufb01ne \u2206= (\u03c3k \u2212\u03c3k+1)/4. By the assumption on G,\nmax\n\u2225w\u22252=1\n\u03a0\u2217w=w\n\u2225U\u22a4Gw\u2225\n\u2225U\u22a4Xw\u2225\u2a7d\u2225U\u22a4G\u2225/ cos\u03b8k(U,X) \u2a7d(\u03c3k \u2212\u03c3k+1)/4 = \u2206.\n9\nSimilarly, and using that 1/ cos\u03b8 \u2a7d1 + tan\u03b8 for any angle \u03b8,\nmax\n\u2225w\u22252=1\n\u03a0\u2217w=w\n\u2225V \u22a4Gw\u2225\n\u2225U\u22a4Xw\u2225\u2a7d\u2225G\u2225/ cos\u03b8k(U,X) \u2a7d\u03b5\u2206(1 + tan\u03b8k(U,X)).\nPlugging back into (2) and using \u03c3k = \u03c3k+1 + 4\u2206,\ntan\u03b8k(U,AX + G) \u2a7dmax\n\u2225w\u22252=1\n\u03a0\u2217w=w\n\u2225V \u22a4Xw\u2225\n\u2225U\u22a4Xw\u2225\u00b7\n\u03c3k+1\n\u03c3k+1 + 3\u2206+ \u03b5\u2206(1 + tan\u03b8k(U,X))\n\u03c3k+1 + 3\u2206\n.\n= \u03c3k+1 + \u03b5\u2206\n\u03c3k+1 + 3\u2206tan\u03b8k(U,X) +\n\u03b5\u2206\n\u03c3k+1 + 3\u2206\n= (1 \u2212\n\u2206\n\u03c3k+1 + 3\u2206)\u03c3k+1 + \u03b5\u2206\n\u03c3k+1 + 2\u2206tan\u03b8k(U,X) +\n\u2206\n\u03c3k+1 + 3\u2206\u03b5\n\u2a7dmax(\u03b5, \u03c3k+1 + \u03b5\u2206\n\u03c3k+1 + 2\u2206tan\u03b8k(U,X))\nwhere the last inequality uses that the weighted mean of two terms is less than their maximum.\nFinally, we have that\n\u03c3k+1 + \u03b5\u2206\n\u03c3k+1 + 2\u2206\u2a7dmax(\n\u03c3k+1\n\u03c3k+1 + \u2206,\u03b5)\nbecause the left hand side is a weighted mean of the components on the right. Since\n\u03c3k+1\n\u03c3k+1+\u2206\u2a7d\n(\n\u03c3k+1\n\u03c3k+1+4\u2206)1/4 = (\u03c3k+1/\u03c3k)1/4, this gives the result.\n\u25a0\nWe can inductively apply the previous lemma to get the following general convergence\nresult.\nTheorem 2.4. Let U represent the top k eigenvectors of the matrix A and \u03b3 = 1 \u2212\u03c3k+1/\u03c3k. Suppose\nthat the initial subspace X0 and noise G\u2113is such that\n5\u2225U\u22a4G\u2113\u2225\u2a7d(\u03c3k \u2212\u03c3k+1)cos\u03b8k(U,X0)\n5\u2225G\u2113\u2225\u2a7d\u03b5(\u03c3k \u2212\u03c3k+1)\nat every stage \u2113, for some \u03b5 < 1/2. Then there exists an L \u22721\n\u03b3 log\n\u0010tan\u03b8k(U,X0)\n\u03b5\n\u0011\nsuch that for all \u2113\u2a7eL\nwe have tan\u03b8(U,XL) \u2a7d\u03b5.\nProof of Theorem 2.4. We will see that at every stage \u2113of the algorithm,\ntan\u03b8k(U,X\u2113) \u2a7dmax(\u03b5,tan\u03b8k(U,X0))\nwhich implies for \u03b5 \u2a7d1/2 that\ncos\u03b8k(U,X\u2113) \u2a7emin(1 \u2212\u03b52/2,cos\u03b8k(U,X0)) \u2a7e7\n8 cos\u03b8k(U,X0)\nso Lemma 2.3 applies at every stage. This means that\ntan\u03b8k(U,X\u2113+1) = tan\u03b8k(U,AX\u2113+ G) \u2a7dmax(\u03b5,\u03b4tan\u03b8k(U,X\u2113))\n10\nfor \u03b4 = max(\u03b5,(\u03c3k+1/\u03c3k)1/4). After\nL = log1/\u03b4\ntan\u03b8k(U,X0)\n\u03b5\niterations the tangent will reach \u03b5 and remain there. Observing that\nlog(1/\u03b4) \u2273min(log(1/\u03b5),log(\u03c3k/\u03c3k+1)) \u2a7emin(1,log\n1\n1 \u2212\u03b3 ) \u2a7emin(1,\u03b3) = \u03b3\ngives the result.\n\u25a0\n2.2\nRandom initialization\nThe next lemma essentially follows from bounds on the smallest singular value of gaussian\nrandom matrices [RV09].\nLemma 2.5. For an arbitrary orthonormal U and random subspace X, we have\ntan\u03b8k(U,X) \u2a7d\u03c4\n\u221a\nd\n\u221ap \u2212\n\u221a\nk \u22121\nwith all but \u03c4\u2212\u2126(p+1\u2212k) + e\u2212\u2126(d) probability.\nProof. Consider the singular value decomposition U\u22a4X = A\u03a3B\u22a4of U\u22a4X. Setting \u03a0 to be\nmatrix projecting onto the \ufb01rst k columns of B, we have that\ntan\u03b8k(U,X) \u2a7dmax\n\u2225w\u22252=1\n\u03a0w=w\n\u2225V \u22a4Xw\u2225\n\u2225U\u22a4Xw\u2225\u2a7d\u2225V \u22a4X\u2225max\n\u2225w\u22252=1\n\u03a0w=w\n1\n\u2225\u03a3B\u22a4w\u2225= \u2225V \u22a4X\u2225\nmax\n\u2225w\u22252=1\nsupp(w)\u2208[k]\n1\n\u2225\u03a3w\u2225= \u2225V \u22a4X\u2225\n\u03c3k(U\u22a4X).\nLet X \u223cN(0,Id\u00d7p) represent the random subspace. Then Y := U\u22a4X \u223cN(0,Ik\u00d7p). By [RV09],\nfor any \u03b5, the smallest singular value of Y is at least (\u221ap \u2212\n\u221a\nk \u22121)/\u03c4 with all but \u03c4\u2212\u2126(p+1\u2212k) +\ne\u2212\u2126(p) probability. On the other hand, \u2225X\u2225\u2272\n\u221a\nd with all but e\u2212\u2126(d) probability. Hence\ntan\u03b8k(U,X) \u2272\u03c4\n\u221a\nd\n\u221ap \u2212\n\u221a\nk \u22121\nwith the desired probability. Rescaling \u03c4 gets the result.\n\u25a0\nWith this lemma we can prove the corollary that we stated in the introduction.\nProof of Corollary 1.1. By Claim 2.5, with the desired probability we have tan\u03b8k(U,X0) \u2a7d\n\u03c4\n\u221a\nd\n\u221ap\u2212\n\u221a\nk\u22121. Hence cos\u03b8k(U,X0) \u2a7e1/(1 + tan\u03b8k(U,X0)) \u2a7e\n\u221ap\u2212\n\u221a\nk\u22121\n2\u00b7\u03c4\n\u221a\nd . Rescale \u03c4 and apply Theo-\nrem 2.4 to get that tan\u03b8k(U,XL) \u2a7d\u03b5. Then \u2225(I \u2212XLX\u22a4\nL )U\u2225= sin\u03b8k(U,XL) \u2a7dtan\u03b8k(U,XL) \u2a7d\n\u03b5.\n\u25a0\n11\n3\nMemory e\ufb03cient streaming PCA\nIn the streaming PCA setting we receive a stream of samples z1,z2,\u00b7\u00b7\u00b7 \u2208Rd. Each sample is\ndrawn i.i.d. from an unknown distribution D over Rd. Our goal is to compute the dominant\nk eigenvectors of the covariance matrix A = Ez\u223cD zz\u22a4. The challenge is to do this with small\nspace, so we cannot store the d2 entries of the sample covariance matrix. We would like to use\nO(dk) space, which is necessary even to store the output.\nThe streaming power method (Figure 2, introduced by [MCJ13]) is a natural algorithm that\nperforms streaming PCA with O(dk) space. The question that arises is how many samples it\nrequires to achieve a given level of accuracy, for various distributions D. Using our general\nanalysis of the noisy power method, we show that the streaming power method requires fewer\nsamples and applies to more distributions than was previously known.\nWe analyze a broad class of distributions:\nDe\ufb01nition 3.1. A distribution D over Rd is (B,p)-round if for every p-dimensional projection\nP and all t \u2a7e1 we have Prz\u223cD {\u2225z\u2225> t} \u2a7dexp(\u2212t) and Prz\u223cD\nn\n\u2225Pz\u2225> t \u00b7\np\nBp/d\no\n\u2a7dexp(\u2212t).\nThe \ufb01rst condition just corresponds to a normalization of the samples drawn from D.\nAssuming the \ufb01rst condition holds, the second condition always holds with B = d/p. For this\nreason our analysis in principle applies to any distribution, but the sample complexity will\ndepend quadratically on B.\nLet us illustrate this de\ufb01nition through the example of the spiked covariance model studied\nby [MCJ13]. The spiked covariance model is de\ufb01ned by an orthonormal basis U \u2208Rd\u00d7k and a\ndiagonal matrix \u039b \u2208Rk\u00d7k with diagonal entries \u03bb1 \u2a7e\u03bb2 \u2a7e\u00b7\u00b7\u00b7 \u2a7e\u03bbk > 0. The distribution D(U,\u039b)\nis de\ufb01ned as the normal distribution N(0,(U\u039b2U\u22a4+ \u03c32Idd\u00d7d)/D) where D = \u0398(d\u03c32 + P\ni \u03bb2\ni ) is\na normalization factor chosen so that the distribution satis\ufb01es the norm bound. Note that the\nthe i-th eigenvalue of the covariance matrix is \u03c3i = (\u03bb2\ni + \u03c32)/D for 1 \u2a7di \u2a7dk and \u03c3i = \u03c32/D for\ni > k. We show in Lemma 3.6 that the spiked covariance model D(U,\u039b) is indeed (B,p)-round\nfor B = O(\n\u03bb2\n1+\u03c32\ntr(\u039b)/d+\u03c32 ), which is constant for \u03c3 \u2273\u03bb1.\nWe have the following main theorem.\nTheorem 3.2. Let D be a (B,p)-round distribution over Rd with covariance matrix A whose\neigenvalues are \u03c31 \u2a7e\u03c32 \u2a7e\u00b7\u00b7\u00b7 \u2a7e\u03c3d \u2a7e0. Let U \u2208Rd\u00d7k be an orthonormal basis for the eigenvectors\ncorresponding to the \ufb01rst k eigenvalues of A. Then, the streaming power method SPM returns an\northonormal basis X \u2208Rd\u00d7p such that tan\u03b8(U,X) \u2a7d\u03b5 with probability 9/10 provided that SPM\nreceives n samples from D for some n satisfying\nn \u2a7d\u02dcO\n B2\u03c3kk log2 d\n\u03b52(\u03c3k \u2212\u03c3k+1)3d\n!\nif p = k + \u0398(k). More generally, for all p \u2a7ek one can get the slightly stronger result\nn \u2a7d\u02dcO\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nBp\u03c3k max{1/\u03b52,Bp/(\u221ap \u2212\n\u221a\nk \u22121)2}log2 d\n(\u03c3k \u2212\u03c3k+1)3d\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8.\nInstantiating with the spiked covariance model gives the following:\n12\nCorollary 3.3. In the spiked covariance model D(U,\u039b) the conclusion of Theorem 3.2 holds for\np = 2k with\nn = \u02dcO\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n(\u03bb2\n1 + \u03c32)2(\u03bb2\nk + \u03c32)\n\u03b52\u03bb6\nk\ndk\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8.\nWhen \u03bb1 = O(1) and \u03bbk = \u2126(1) this becomes n = \u02dcO\n\u0010\u03c36+1\n\u03b52\n\u00b7 dk\n\u0011\n.\nWe can apply Theorem 3.2 to all distributions that have exponentially concentrated norm\nby setting B = d/p. This gives the following result.\nCorollary 3.4. Let D be any distribution scaled such that Prz\u223cD[\u2225z\u2225> t] \u2a7dexp(\u2212t) for all t \u2a7e1.\nThen the conclusion of Theorem 3.2 holds for p = 2k with\nn = \u02dcO\n \n\u03c3k\n\u03b52k(\u03c3k \u2212\u03c3k+1)3 \u00b7 d\n!\n.\nIf the eigenvalues follow a power law, \u03c3j \u2248j\u2212c for a constant c > 1/2, this gives an n =\n\u02dcO(k2c+2d/\u03b52) bound on the sample complexity.\n3.1\nError term analysis\nFix an orthonormal basis X \u2208Rd\u00d7k. Let z1,...,zn \u223cD be samples from a distribution D with\ncovariance matrix A and consider the matrix\nG =\n\u0010\nA \u2212b\nA\n\u0011\nX ,\nwhere b\nA = 1\nn\nPn\ni=1 ziz\u22a4\ni is the empirical covariance matrix on n samples. Then, we have that\nb\nAX = AX + G. In other words, one update step of the power method executed on b\nA can be\nexpressed as an update step on A with noise matrix G. This simple observation allows us to\napply our analysis of the noisy power method to this setting after obtaining suitable bounds\non \u2225G\u2225and \u2225U\u22a4G\u2225.\nLemma 3.5. Let D be a (B,p)-round distribution with covariance matrix M. Then with all but\nO(1/n2) probability,\n\u2225G\u2225\u2272\ns\nBplog4 nlogd\ndn\n+ 1\nn2\nand\n\u2225U\u22a4G\u2225\u2272\ns\nB2p2 log4 nlogd\nd2n\n+ 1\nn2\nProof. We will use a matrix Cherno\ufb00bound to show that\n1. Pr\nn\n\u2225G\u2225> Ct log(n)2p\nBp/d + O(1/n2)\no\n\u2a7dd exp(\u2212t2n) + 1/n2\n2. Pr\nn\n\u2225U\u22a4G\u2225> Ct log(n)2Bp/d + O(1/n2)\no\n\u2a7dd exp(\u2212t2n) + 1/n2\nsetting t =\nq\n2\nn logd gives the result. However, matrix Cherno\ufb00inequality requires the distri-\nbution to satisfy a norm bound with probability 1. We will therefore create a closely related\ndistribution \u02dcD that satis\ufb01es such a norm constraint and is statistically indistinguishable up\nto small error on n samples. We can then work with \u02dcD instead of D. This truncation step is\nstandard and works because of the concentration properties of D.\n13\nIndeed, let \u02dcD be the distribution obtained from D be replacing a sample z with 0 if\n\u2225z\u2225> C log(n)\nor\n\u2225U\u22a4z\u2225\u2a7eC log(n)\np\nBp/d\nor\n\u2225z\u22a4X\u2225> C log(n)\np\nBp/d .\nFor su\ufb03ciently large constant C, it follows from the de\ufb01nition of (B,p)-round that the probabil-\nity that one or more of n samples from D get zeroed out is at most 1/n2. In particular, the two\nproduct distributions D(n) and \u02dcD(n) have total variation distance at most 1/n2. Furthermore,\nwe claim that the covariance matrices of the two distributions are at most O(1/n2) apart in\nspectral norm. Formally,\n\r\r\r\r\r E\nz\u223cDzz\u22a4\u2212E\n\u02dcz\u223c\u02dcD\n\u02dcz\u02dcz\u22a4\n\r\r\r\r\r \u2a7d1\nn2 \u00b7 O\n Z\nt\u2a7e1\nC2t2 log2(n)exp(\u2212t)dt\n!\n\u2a7dO(1/n2).\nIn the \ufb01rst inequality we use the fact that z only gets zeroed out with probability 1/n2.\nConditional on this event, the norm of z is larger than tC log(n) with probability at most\nn2 exp(\u22121\n2tC logn) \u2a7dexp(\u2212t). Assuming the norm is at most tC log(n) we have \u2225zz\u22a4\u2225\u2a7dt2C2 log2(n)\nand this bounds the contribution to the spectral norm of the di\ufb00erence.\nNow let \u02dcG be the error matrix de\ufb01ned as G except that we replace the samples z1,...,zn\nby n samples \u02dcz1,..., \u02dczn from the truncated distribution \u02dcD. By our preceding discussion, it now\nsu\ufb03ces to show that\n1. Pr\nn\n\u2225\u02dcG\u2225> Ct log2(n)\np\nBp/d\no\n\u2a7dd exp(\u2212t2n)\n2. Pr\nn\n\u2225U\u22a4\u02dcG\u2225> Ct log2(n)Bp/d\no\n\u2a7dd exp(\u2212t2n)\nTo see this, let Si = \u02dczi \u02dcz\u22a4\ni X. We have\n\u2225Si\u2225\u2a7d\u2225\u02dczi\u2225\u00b7\n\r\r\r\u02dcz\u22a4\ni X\n\r\r\r \u2a7dC2 log2(n) \u00b7\np\nBp/d\nSimilarly,\n\r\r\rU\u22a4Si\n\r\r\r \u2a7d\u2225U\u22a4\u02dczi\u2225\u00b7\n\r\r\r\u02dcz\u22a4\ni X\n\r\r\r \u2a7dC2 log2(n) \u00b7 Bp\nd .\nThe claims now follow directly from the matrix Cherno\ufb00bound stated in Lemma A.4.\n\u25a0\n3.2\nProof of Theorem 3.2\nGiven Lemma 3.5 we will choose n such that the error term in each iteration satis\ufb01es the\nassumptions of Theorem 2.4. Let G\u2113denote the instance of the error term G arising in the \u2113-th\niteration of the algorithm. We can \ufb01nd an n satisfying\nn\nlog(n)4 = O\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nBpmax\nn\n1/\u03b52,Bp/(\u221ap \u2212\n\u221a\nk \u22121)2o\nlogd\n(\u03c3k \u2212\u03c3k+1)2d\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\nsuch that by Lemma 3.5 we have that with probability 1 \u2212O(1/n2),\n\u2225G\u2113\u2225\u2a7d\u03b5(\u03c3k \u2212\u03c3k+1)\n5\nand\n\u2225U\u22a4G\u2113\u2225\u2a7d\u03c3k \u2212\u03c3k+1\n5\n\u221ap \u2212\n\u221a\nk \u22121\n\u221a\nd\n.\n14\nHere we used that by de\ufb01nition 1/n \u226a\u03b5 and 1/n \u226a\u03c3k \u2212\u03c3k+1 and so the 1/n2 term in Lemma 3.5\nis of lower order.\nWith this bound, it follows from Theorem 2.4 that after L = O(log(d/\u03b5)/(1 \u2212\u03c3k+1/\u03c3k))\niterations we have with probability 1 \u2212max{1,L/n2} that tan\u03b8(U,XL) \u2a7d\u03b5. The over all sample\ncomplexity is therefore\nLn = \u02dcO\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nBp\u03c3k max\nn\n1/\u03b52,Bp/(\u221ap \u2212\n\u221a\nk \u22121)2o\nlog2 d\n(\u03c3k \u2212\u03c3k+1)3d\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8.\nHere we used that 1 \u2212\u03c3k+1/\u03c3k = (\u03c3k \u2212\u03c3k+1)/\u03c3k. This concludes the proof of Theorem 3.2.\n3.3\nProof of Lemma 3.6 and Corollary 3.4\nLemma 3.6. The spiked covariance model D(U,\u039b) is (B,k)-round for B = O(\n\u03bb2\n1+\u03c32\ntr(\u039b)/d+\u03c32 ).\nProof. Note that an example z \u223cD(U,\u039b) is distributed as U\u039bg + g\u2032 where g \u223cN(0,1/D)k is a\nstandard gaussian and g\u2032 \u223cN(0,\u03c32/D)d. is a noise term. Recall, that D is the normalization\nterm. Let P be any projection operator onto a k-dimensional space. Then,\n\u2225Pz\u2225= \u2225PU\u039bg + Pg\u2032\u2225\u2a7d\u2225PU\u039bg\u2225+ \u2225Pg\u2032\u2225\u2a7d\u2225\u039bg\u2225+ \u2225Pg\u2032\u2225\u2a7d\u03bb1\u2225g\u2225+ \u2225Pg\u2032\u2225.\nBy rotational invariance of g\u2032, we may assume that P is the projection onto the \ufb01rst k coordi-\nnates. Hence, \u2225Pg\u2032\u2225is distributed like the norm of N(0,\u03c32/D)k. Using standard tail bounds\nfor the norm of a gaussian random variables, we can see that \u2225Pz\u22252 = O(t(k\u03bb2\n1 + k\u03c32)/D) with\nprobability 1 \u2212exp(\u2212t). On the other hand, D = \u0398(Pk\ni=1 \u03bb2\ni + d\u03c32). We can now solve for B by\nsetting\n\u0398(\nk\u03bb2\n1 + k\u03c32\nPk\ni=1 \u03bb2\ni + d\u03c32 ) = Bk\nd\n\u21d4\nB = \u0398(\n\u03bb2\n1 + \u03c32\n1\nd\nPk\ni=1 \u03bb2\ni + \u03c32 ).\n\u25a0\nCorollary 3.4 follows by plugging in the bound on B and the eigenvalues of the covariance\nmatrix into our main theorem.\nProof of Corollary 3.4. In the spiked covariance model D(U,\u039b) we have\nB = \u03bb2\n1 + \u03c32\nD\n,\n\u03c3k = \u03bb2\nk + \u03c32\nD\n,\n\u03c3k+1 = \u03c32\nD ,\nD = O(tr(\u039b2) + d\u03c32).\nHence,\nB2\u03c3k\n(\u03c3k \u2212\u03c3k+1)3d = (\u03bb2\n1 + \u03c32)2(\u03bb2\nk + \u03c32)\n\u03bb6\nkd\n\u2a7d(\u03bb2\n1 + \u03c32)3\n\u03bb6\nkd\nPlugging this bound into Theorem 3.2 gives Corollary 3.4.\n\u25a0\n15\n4\nPrivacy-preserving singular vector computation\nIn this section we prove our results about privacy-preserving singular vector computation. We\nbegin with a standard de\ufb01nition of di\ufb00erential privacy, sometimes referred to as entry-level\ndi\ufb00erential privacy, as it hides the presence or absence of a single entry.\nDe\ufb01nition 4.1 (Di\ufb00erential Privacy). A randomized algorithm M : Rd\u00d7d\u2032 \u2192R (where R is some\narbitrary abstract range) is (\u03b5,\u03b4)-di\ufb00erentially private if for all pairs of matrices A,A\u2032 \u2208Rd\u00d7d\u2032\ndi\ufb00ering in only one entry by at most 1 in absolute value, we have that for all subsets of the\nrange S \u2286R, the algorithm satis\ufb01es: Pr{M(A) \u2208S} \u2a7dexp(\u03b5)Pr{M(A\u2032) \u2208S} + \u03b4.\nThe de\ufb01nition is most meaningful when A has entries in [0,1] so that the above de\ufb01nition\nallows for a single entry to change arbitrarily within this range. However, this is not a\nrequirement for us. The privacy guarantee can be strengthened by decreasing \u03b5 > 0.\nFor our choice of \u03c3 in Figure 3 the algorithm satis\ufb01es (\u03b5,\u03b4)-di\ufb00erential privacy as follows\neasily from properties of the Gaussian distribution. See, for example, [HR13] for a proof.\nClaim 4.2. PPM satis\ufb01es (\u03b5,\u03b4)-di\ufb00erential privacy.\nIt is straightforward to prove Theorem 1.3 by invoking our convergence analysis of the noisy\npower method together with suitable error bounds. The error bounds are readily available as\nthe noise term is just gaussian.\nProof of Theorem 1.3. Let m = max\u2225X\u2113\u2225\u221e. By Lemma A.2 the following bounds hold with\nprobability 99/100:\n1. maxL\n\u2113=1 \u2225G\u2113\u2225\u2272\u03c3m\np\nd logL\n2. maxL\n\u2113=1 \u2225U\u22a4G\u2113\u2225\u2272\u03c3m\np\nk logL\nLet\n\u03b5\u2032 = \u03c3m\np\nd logL\n\u03c3k \u2212\u03c3k+1\n\u22735maxL\n\u2113=1 \u2225G\u2113\u2225\n\u03c3k \u2212\u03c3k+1\n.\nBy Corollary 1.1, if we also have that maxL\n\u2113=1 \u2225U\u22a4G\u2113\u2225\u2a7d(\u03c3k \u2212\u03c3k+1)\n\u221ap\u2212\n\u221a\nk\u22121\n\u03c4\n\u221a\nd\nfor a su\ufb03ciently\nlarge constant \u03c4, then we will have that\n\u2225(I \u2212XLX\u22a4\nL )U\u2225\u2a7d\u03b5\u2032 \u2a7d\u03c3m\np\nd logL\n\u03c3k \u2212\u03c3k+1\nafter the desired number of iterations, giving the theorem. Otherwise,\n(\u03c3k \u2212\u03c3k+1)\n\u221ap \u2212\n\u221a\nk \u22121\n\u03c4\n\u221a\nd\n\u2a7d\nL\nmax\n\u2113=1 \u2225U\u22a4G\u2113\u2225\u2272\u03b5\u2032(\u03c3k \u2212\u03c3k+1)\n\u221a\nk/d,\nso it is trivially true that\n\u03c3m\np\nd logL\n\u03c3k \u2212\u03c3k+1\n\u221ap\n\u221ap \u2212\n\u221a\nk \u22121\n\u2a7e\u03b5\u2032\n\u221a\nk\n\u221ap \u2212\n\u221a\nk \u22121\n\u22731 \u2a7e\u2225(I \u2212XLX\u22a4\nL )U\u2225.\n\u25a0\n16\n4.1\nLow-rank approximation\nOur results readily imply that we can compute accurate di\ufb00erentially private low-rank ap-\nproximations. The main observation is that, assuming XL and U have the same dimension,\ntan\u03b8(U,XL) \u2a7d\u03b1 implies that the matrix XL also leads to a good low-rank approximation for A\nin the spectral norm. In particular\n\u2225(I \u2212XLX\u22a4\nL )A\u2225\u2a7d\u03c3k+1 + \u03b1\u03c31 .\n(3)\nMoreover the projection step of computing XLX\u22a4\nL A can be carried out easily in a privacy-\npreserving manner. It is again the \u2113\u221e-norm of the columns of XL that determine the magnitude\nof noise that is needed. Since A is symmetric, we have X\u22a4A = (AX)\u22a4. Hence, to obtain a good\nlow-rank approximation it su\ufb03ces to compute the product AXL privately as AXL + GL. This\nleads to the following corollary.\nCorollary 4.3. Let A \u2208Rd\u00d7d be a symmetric matrix with singular values \u03c31 \u2a7e... \u2a7e\u03c3d and let\n\u03b3 = 1\u2212\u03c3k+1/\u03c3k. There is an (\u03b5,\u03b4)-di\ufb00erentially private algorithm that given A and k, outputs a rank\n2k matrix B such that with probability 9/10,\n\u2225A \u2212B\u2225\u2a7d\u03c3k+1 + \u02dcO\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\u03c31\np\n(k/\u03b3)d logd log(1/\u03b4)\n\u03b5(\u03c3k \u2212\u03c3k+1)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8.\nThe \u02dcO-notation hides the factor O\n\u0010p\nlog(log(d)/\u03b3)\n\u0011\n.\nProof. Apply Theorem 1.3 with p = 2k and run the algorithm for L + 1 steps with L =\nO(\u03b3\u22121 logd). This gives the bound\n\u03b1 = \u2225(I \u2212XLX\u22a4\nL )A\u2225\u2a7dO\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\np\n(k/\u03b3)d logd log(log(d)/\u03b3)log(1/\u03b4)\n\u03b5(\u03c3k \u2212\u03c3k+1)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8.\nMoreover, the algorithm has computed YL+1 = AXL + GL and we have B = XLY \u22a4\nL+1 = XLX\u22a4\nL A +\nXLG\u22a4\nL . Therefore\n\u2225A \u2212B\u2225\u2a7d\u03c3k+1 + \u03b1\u03c31 +\n\r\r\rXLG\u22a4\nL\n\r\r\r\nwhere\n\r\r\rXLG\u22a4\nL\n\r\r\r \u2a7d\u2225GL\u2225. By de\ufb01nition of the algorithm and Lemma A.2, we have\n\u2225GL\u2225\u2a7dO\n\u0010\u221a\n\u03c32d\n\u0011\n= O\n\u00121\n\u03b5\np\n(k/\u03b3)d log(d)log(1/\u03b4)\n\u0013\n.\nGiven that the \u03b1-term gets multiplied by \u03c31, this bound on \u2225GL\u2225is of lower order and the\ncorollary follows.\n\u25a0\n4.2\nPrincipal Component Analysis\nHere we illustrate that our bounds directly imply results for the privacy notion studied by\nKapralov and Talwar [KT13]. The notion is particularly relevant in a setting where we think\nof A as a sum of rank 1 matrices each of bounded spectral norm.\nDe\ufb01nition 4.4. A randomized algorithm M : Rd\u00d7d\u2032 \u2192R (where R is some arbitrary abstract\nrange) is (\u03b5,\u03b4)-di\ufb00erentially private under unit spectral norm changes if for all pairs of matrices\nA,A\u2032 \u2208Rd\u00d7d\u2032 satisfying \u2225A \u2212A\u2032\u22252 \u2a7d1, we have that for all subsets of the range S \u2286R, the\nalgorithm satis\ufb01es: Pr{M(A) \u2208S} \u2a7dexp(\u03b5)Pr{M(A\u2032) \u2208S} + \u03b4.\n17\nLemma 4.5. If PPM is executed with each G\u2113sampled independently as G\u2113\u223cN(0,\u03c32)d\u00d7p with\n\u03c3 = \u03b5\u22121p\n4pLlog(1/\u03b4), then PPM satis\ufb01es (\u03b5,\u03b4)-di\ufb00erential privacy under unit spectral norm\nchanges.\nIf G\u2113is sampled with i.i.d. Laplacian entries G\u2113\u223cLap(0,\u03bb)n\u00d7k where \u03bb = 10\u03b5\u22121pL\n\u221a\nd, then\nPPM satis\ufb01es (\u03b5,0)-di\ufb00erential privacy under unit spectral norm changes.\nProof. The \ufb01rst claim follows from the privacy proof in [HR12]. We sketch the argument here\nfor completeness. Let D be any matrix with \u2225D\u22252 \u2a7d1 (thought of as A\u2212A\u2032 in De\ufb01nition 4.4) and\nlet \u2225x\u2225= 1 be any unit vector which we think of as one of the columns of X = X\u2113\u22121. Then, we\nhave \u2225Dx\u2225\u2a7d\u2225D\u2225\u00b7\u2225x\u2225\u2a7d1, by de\ufb01nition of the spectral norm. This shows that the \u201c\u21132-sensitivity\u201d\nof one matrix-vector multiplication in our algorithm is bounded by 1. It is well-known that\nit su\ufb03ces to add Gaussian noise scaled to the \u21132-sensitivity of the matrix-vector product in\norder to achieve di\ufb00erential privacy. Since there are kL matrix-vector multiplications in total\nwe need to scale the noise by a factor of\n\u221a\nkL.\nThe second claim follows analogously. Here however we need to scale the noise magnitude\nto the \u201c\u21131-sensitivity\u201d of the matrix-vector product which be bound by \u221an using Cauchy-\nSchwarz. The claim then follows using standard properties of the Laplacian mechanism.\n\u25a0\nGiven the previous lemma it is straightforward to derive the following corollaries.\nCorollary 4.6. Let A \u2208Rd\u00d7d be a symmetric matrix with singular values \u03c31 \u2a7e... \u2a7e\u03c3d and let\n\u03b3 = 1 \u2212\u03c3k+1/\u03c3k. There is an algorithm that given a A and parameter k, preserves (\u03b5,\u03b4)-di\ufb00erentially\nprivacy under unit spectral norm changes and outputs a rank 2k matrix B such that with probability\n9/10,\n\u2225A \u2212B\u2225\u2a7d\u03c3k+1 + \u02dcO\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\u03c31\np\n(k/\u03b3)d logd log(1/\u03b4)\n\u03b5(\u03c3k \u2212\u03c3k+1)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8.\nThe \u02dcO-notation hides the factor O\n\u0010p\nlog(log(d)/\u03b3)\n\u0011\n.\nProof. The proof is analogous to the proof of Corollary 4.3.\n\u25a0\nA similar corollary applies to (\u03b5,0)-di\ufb00erential privacy.\nCorollary 4.7. Let A \u2208Rd\u00d7d be a symmetric matrix with singular values \u03c31 \u2a7e... \u2a7e\u03c3d and let\n\u03b3 = 1 \u2212\u03c3k+1/\u03c3k. There is an algorithm that given a A and parameter k, preserves (\u03b5,\u03b4)-di\ufb00erentially\nprivacy under unit spectral norm changes and outputs a rank 2k matrix B such that with probability\n9/10,\n\u2225A \u2212B\u2225\u2a7d\u03c3k+1 + \u02dcO\n \u03c31k1.5d log(d)log(d/\u03b3)\n\u03b5\u03b3(\u03c3k \u2212\u03c3k+1)\n!\n.\nProof. We invoke PPM with p = 2k and Laplacian noise with the scaling given by Lemma 4.5\nso that the algorithm satis\ufb01es (\u03b5,0)-di\ufb00erential privacy. Speci\ufb01cally, G\u2113\u223cLap(0,\u03bb)d\u00d7p where\n\u03bb = 10\u03b5\u22121pL\n\u221a\nd. Lemma A.3. Indeed, with probability 99/100, we have\n1. maxL\n\u2113=1 \u2225G\u2113\u2225\u2a7dO\n\u0010\n\u03bb\n\u221a\nkd log(kdL)\n\u0011\n= O\n\u0010\n(1/\u03b5\u03b3)k1.5d log(d)log(kdL)\n\u0011\n2. maxL\n\u2113=1 \u2225U\u22a4G\u2113\u2225\u2a7dO(\u03bbk log(kL)) = O\n\u0010\n(1/\u03b5\u03b3)k2\u221a\nd log(d)log(kL)\n\u0011\n18\nWe can now plug these error bounds into Corollary 1.1 to obtain the bound\n\r\r\r(I \u2212XLX\u22a4\nL )U\n\r\r\r \u2a7dO\n k1.5d log(d)log(d/\u03b3)\n\u03b5\u03b3(\u03c3k \u2212\u03c3k+1)\n!\nRepeating the argument from the proof of Corollary 4.3 gives the stated guarantee for low-rank\napproximation.\n\u25a0\nThe bound above matches a lower bound shown by Kapralov and Talwar [KT13] up to\na factor of \u02dcO(\n\u221a\nk). We believe that this factor can be eliminated from our bounds by using a\nquantitatively stronger version of Lemma A.3. Compared to the upper bound of [KT13] our\nalgorithm is faster by a more than a quadratic factor in d. Moreover, previously only bounds\nfor (\u03b5,0)-di\ufb00erential privacy were known for the spectral norm privacy notion, whereas our\nbounds strongly improve when going to (\u03b5,\u03b4)-di\ufb00erential privacy.\n4.3\nDimension-free bounds for incoherent matrices\nThe guarantee in Theorem 1.3 depends on the quantity \u2225X\u2113\u2225\u221ewhich could in principle be\nas small as\n\u221a\n1/d. Yet, in the above theorems, we use the trivial upper bound 1. This in turn\nresulted in a dependence on the dimensions of A in our theorems. Here, we show that the\ndependence on the dimension can be replaced by an essentially tight dependence on the\ncoherence of the input matrix. In doing so, we resolve the main open problem left open by\nHardt and Roth [HR13]. The de\ufb01nition of coherence that we will use is formally de\ufb01ned as\nfollows.\nDe\ufb01nition 4.8 (Matrix Coherence). We say that a matrix A \u2208Rd\u00d7d\u2032 with singular value decom-\nposition A = U\u03a3V \u22a4has coherence\n\u00b5(A) def\n=\nn\nd\u2225U\u22252\n\u221e,d\u2032\u2225V \u22252\n\u221e\no\n.\nHere \u2225U\u2225\u221e= maxij |Uij| denotes the largest entry of U in absolute value.\nOur goal is to show that the \u2113\u221e-norm of the vectors arising in PPM is closely related to the\ncoherence of the input matrix. We obtain a nearly tight connection between the coherence of\nthe matrix and the \u2113\u221e-norm of the vectors that PPM computes.\nTheorem 4.9. Let A \u2208Rd\u00d7d be symmetric. Suppose NPM is invoked on A, and L \u2a7dn, with each G\u2113\nsampled from N(0,\u03c32\n\u2113)d\u00d7p for some \u03c3\u2113> 0. Then, with probability 1 \u22121/n,\nL\nmax\n\u2113=1 \u2225X\u2113\u22252\n\u221e\u2a7dO\n \u00b5(A)log(d)\nd\n!\n.\nProof. Fix \u2113\u2208[L]. Let A = Pn\ni=1 \u03c3iuiu\u22a4\ni be given in its eigendecomposition. Note that\nB =\nd\nmax\ni=1 \u2225ui\u2225\u221e\u2a7d\nr\n\u00b5(A)\nd\n.\nWe may write any column x of X\u2113as x = Pd\ni=1 si\u03b1iui where \u03b1i are non-negative scalars such\nthat Pd\ni=1 \u03b12\ni = 1, and si \u2208{\u22121,1} where si = sign(\u27e8x,ui\u27e9). Hence, by Lemma 4.13 (shown below),\n19\nthe signs (s1,...,sd) are distributed uniformly at random in {\u22121,1}d. Hence, by Lemma 4.14\n(shown below), it follows that Pr\nn\n\u2225x\u2225\u221e> 4B\np\nlogd\no\n\u2a7d1/n3 . By a union bound over all p \u2a7dd\ncolumns it follows that Pr\nn\n\u2225X\u2113\u2225\u221e> 4B\np\nlogd\no\n\u2a7d1/d2 . Another union bound over all L \u2a7dd\nsteps completes the proof.\n\u25a0\nThe previous theorem states that no matter what the scaling of the Gaussian noise is in\neach step of the algorithm, so long as it is Gaussian the algorithm will maintain that X\u2113has\nsmall coordinates. We cannot hope to have coordinates smaller than\np\n\u00b5(A)/d, since eventually\nthe algorithm will ideally converge to U. This result directly implies the theorem we stated in\nthe introduction.\nProof of Theorem 1.4. The claim follows directly from Theorem 1.3 after applying Theorem 4.9\nwhich shows that with probability 1 \u22121/n,\nL\nmax\n\u2113=1 \u2225X\u2113\u22252\n\u221e\u2a7dO\n \u00b5(A)log(d)\nd\n!\n.\n\u25a0\n4.4\nProofs of supporting lemmas\nWe will now establish Lemma 4.13 and Lemma 4.14 that were needed in the proof of the previ-\nous theorem. For that purpose we need some basic symmetry properties of the QR-factorization.\nTo establish these properties we recall the Gram-Schmidt algorithm for computing the QR-\nfactorization.\nDe\ufb01nition 4.10 (Gram-Schmidt). The Gram-Schmidt orthonormalization algorithm, denoted\nGS, is given an input matrix V \u2208Rd\u00d7p with columns v1,...,vp and outputs an orthonormal\nmatrix Q \u2208Rd\u00d7p with the same range as V . The columns q1,...,qp of Q are computed as follows:\nFor i = 1 to p do:\n\u2013 rii \u2190\u2225vi\u2225\n\u2013 qi \u2190vi/rii\n\u2013 For j = i + 1 to p do:\n\u2013 rij \u2190\u27e8qi,vj\u27e9\n\u2013 vj \u2190vj \u2212rijqi\nThe \ufb01rst states that the Gram-Schmidt operation commutes with an orthonormal transfor-\nmation of the input.\nLemma 4.11. Let V \u2208Rd\u00d7p and let O \u2208Rd\u00d7d be an orthonormal matrix. Then, GS(OV ) =\nO \u00d7 GS(V ).\nProof. Let {rij}ij\u2208[p] denote the scalars computed by the Gram-Schmidt algorithm as speci\ufb01ed\nin De\ufb01nition 4.10. Notice that each of the numbers {rij}ij\u2208[p] is invariant under an orthonormal\ntransformation of the vectors v1,...,vp. This is because \u2225Ovi\u2225= \u2225vi\u2225and \u27e8Ovi,Ovj\u27e9= \u27e8vi,vj\u27e9.\nMoreover, The output Q of Gram-Schmidt on input of V satis\ufb01es Q = V R, where R is an upper\nright triangular matrix which only depends on the numbers {rij}i,j\u2208[p]. Hence, the matrix R is\nidentical when the input is OV . Thus, GS(OV ) = OV R = O \u00d7 GS(V ).\n\u25a0\n20\nGiven i.i.d. Gaussian matrices G0,G1,...,GL \u223cN(0,1)d\u00d7p, we can describe the behavior of\nour algorithm by a deterministic function f (G0,G1,...,GL) which executes subspace iteration\nstarting with G0 and then suitably scales G\u2113in each step. The next lemma shows that this\nfunction is distributive with respect to orthonormal transformations.\nLemma 4.12. Let f : (Rd\u00d7p)L \u2192Rn\u00d7p denote the output of PPM on input of a matrix A \u2208Rn\u00d7n as\na function of the noise matrices used by the algorithm as described above. Let O be an orthonormal\nmatrix with the same eigenbasis as A. Then,\nf (OG0,OG1,...,OGL) = O \u00d7 f (G0,...,GL).\n(4)\nProof. For ease of notation we will denote by X0,...,XL the iterates of the algorithm when the\nnoise matrices are G0,...,GL, and we denote by Y0,...,YL the iterates of the algorithm when\nthe noise matrices are OG0,...,OGL. In this notation, our goal is to show that YL = OXL.\nWe will prove the claim by induction on L. For L = 0, the base case follows from Lemma 4.11.\nIndeed,\nY0 = GS(OG0) = O \u00d7 GS(G0) = OX0 .\nLet \u2113\u2a7e1. We assume the claim holds for \u2113\u22121 and show that it holds for \u2113. We have,\nY\u2113= GS(AY\u2113\u22121 + OG\u2113)\n= GS(AOX\u2113\u22121 + OG\u2113)\n(by induction hypothesis)\n= GS(O(AX\u2113\u22121 + G\u2113))\n(A and O commute)\n= O \u00d7 GS(AX\u2113\u22121 + G\u2113)\n(Lemma 4.11)\n= OX\u2113.\nNote that A and O commute, since they share the same eigenbasis by the assumption of the\nlemma. This is what we needed to prove.\n\u25a0\nThe previous lemmas lead to the following result characterizing the distribution of signs\nof inner products between the columns of X\u2113and the eigenvectors of A.\nLemma 4.13 (Sign Symmetry). Let A be a symmetric matrix given in its eigendecomposition as\nA = Pd\ni=1 \u03bbiuiu\u22a4\ni . Let \u2113\u2a7e0 and let x be any column of X\u2113, where X\u2113is the iterate of PPM on input\nof A. Put Si = sign(\u27e8ui,x\u27e9) for i \u2208[d]. Then (S1,...,Sd) is uniformly distributed in {\u22121,1}d.\nProof. Let (z1,...,zd) \u2208{\u22121,1}d be a uniformly random sign vector. Let O = Pd\ni=1 ziuiu\u22a4\ni .\nNote that O is an orthonormal transformation. Clearly, any column Ox of OX\u2113satis\ufb01es\nthe conclusion of the lemma, since \u27e8ui,Ox\u27e9= zi\u27e8ui,x\u27e9. Since the Gaussian distribution is\nrotationally invariant, we have that OG\u2113and G\u2113follow the same distribution. In particular,\ndenoting by Y\u2113the matrix computed by the algorithm if OG0,...,OG\u2113were chosen, we have\nthat Y\u2113and X\u2113are identically distributed. Finally, by Lemma 4.12, we have that Y\u2113= OX\u2113. By\nour previous observation this means that Y\u2113satis\ufb01es the conclusion of the lemma. As Y\u2113and\nX\u2113are identically distributed, the claim also holds for X\u2113.\n\u25a0\nWe will use the previous lemma to bound the \u2113\u221e-norm of the intermediate matrices X\u2113\narising in power iteration in terms of the coherence of the input matrix. We need the following\nlarge deviation bound.\n21\nLemma 4.14. Let \u03b11,...,\u03b1d be scalars such that Pd\ni=1 \u03b12\ni = 1 and u1,...,ud are unit vectors in Rn.\nPut B = maxd\ni=1 \u2225ui\u2225\u221e. Further let (s1,...,sd) be chosen uniformly at random in {\u22121,1}d. Then,\nPr\n\uf8f1\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f3\n\r\r\r\r\r\r\r\nd\nX\ni=1\nsi\u03b1iui\n\r\r\r\r\r\r\r\u221e\n> 4B\np\nlogd\n\uf8fc\uf8f4\uf8f4\uf8fd\uf8f4\uf8f4\uf8fe\u2a7d1/d3 .\nProof. Let X = Pd\ni=1 Xi where Xi = si\u03b1iui. We will bound the deviation of X in each entry and\nthen take a union bound over all entries. Consider Z = Pd\ni=1 Zi where Zi is the \ufb01rst entry of Xi.\nThe argument is identical for all other entries of X. We have EZ = 0 and EZ2 = Pd\ni=1 EZ2\ni \u2a7d\nB2 Pd\ni=1 \u03b12\ni = B2. Hence, by Theorem A.1 (Cherno\ufb00bound),\nPr\nn\n|Z| > 4B\np\nlog(d)\no\n\u2a7dexp\n\u0012\n\u221216B2 log(d)\n4B2\n\u0013\n\u2a7dexp(\u22124log(d)) = 1\nd4 .\nThe claim follows by taking a union bound over all d entries of X.\n\u25a0\nReferences\n[ACLS12]\nRaman Arora, Andrew Cotter, Karen Livescu, and Nathan Srebro. Stochastic\noptimization for pca and pls. In Communication, Control, and Computing (Allerton),\n2012 50th Annual Allerton Conference on, pages 861\u2013868. IEEE, 2012.\n[BBDS12]\nJeremiah Blocki, Avrim Blum, Anupam Datta, and Or She\ufb00et. The Johnson-\nLindenstrauss transform itself preserves di\ufb00erential privacy. In Proc. 53rd Foun-\ndations of Computer Science (FOCS), pages 410\u2013419. IEEE, 2012.\n[BDF13]\nAkshay Balsubramani, Sanjoy Dasgupta, and Yoav Freund. The fast convergence\nof incremental PCA. In Proc. 27th Neural Information Processing Systems (NIPS),\npages 3174\u20133182, 2013.\n[BDMN05] Avrim Blum, Cynthia Dwork, Frank McSherry, and Kobbi Nissim. Practical\nprivacy: the SuLQ framework. In Proc. 24th PODS, pages 128\u2013138. ACM, 2005.\n[CLMW11] Emmanuel J. Cand\u00e8s, Xiaodong Li, Yi Ma, and John Wright. Robust principal\ncomponent analysis? J. ACM, 58(3):11, 2011.\n[CR09]\nEmmanuel J. Cand\u00e8s and Benjamin Recht. Exact matrix completion via convex\noptimization. Foundations of Computional Mathematics, 9:717\u2013772, December\n2009.\n[CSS12]\nKamalika Chaudhuri, Anand Sarwate, and Kaushik Sinha. Near-optimal di\ufb00eren-\ntially private principal components. In Proc. 26th Neural Information Processing\nSystems (NIPS), 2012.\n[CT10]\nEmmanuel J. Cand\u00e8s and Terence Tao. The power of convex relaxation: near-\noptimal matrix completion. IEEE Transactions on Information Theory, 56(5):2053\u2013\n2080, 2010.\n22\n[DK70]\nChandler Davis and W. M. Kahan. The rotation of eigenvectors by a perturbation.\niii. SIAM J. Numer. Anal., 7:1\u201346, 1970.\n[DTTZ14]\nCynthia Dwork, Kunal Talwar, Abhradeep Thakurta, and Li Zhang. Analyze\nGauss: optimal bounds for privacy-preserving principal component analysis. In\nProc. 46th Symposium on Theory of Computing (STOC), pages 11\u201320. ACM, 2014.\n[Har14]\nMoritz Hardt. Understanding alternating minimization for matrix completion. In\nProc. 55th Foundations of Computer Science (FOCS). IEEE, 2014.\n[Hig02]\nNicholas J. Higham. Accuracy and Stability of Numerical Algorithms. Society for\nIndustrial and Applied Mathematics, 2002.\n[HMT11]\nNathan Halko, Per-Gunnar Martinsson, and Joel A. Tropp. Finding structure\nwith randomness: Probabilistic algorithms for constructing approximate matrix\ndecompositions. SIAM Review, 53(2):217\u2013288, 2011.\n[HR12]\nMoritz Hardt and Aaron Roth. Beating randomized response on incoherent\nmatrices. In Proc. 44th Symposium on Theory of Computing (STOC), pages 1255\u2013\n1268. ACM, 2012.\n[HR13]\nMoritz Hardt and Aaron Roth. Beyond worst-case analysis in private singular\nvector computation. In Proc. 45th Symposium on Theory of Computing (STOC).\nACM, 2013.\n[JNS13]\nPrateek Jain, Praneeth Netrapalli, and Sujay Sanghavi. Low-rank matrix com-\npletion using alternating minimization. In Proc. 45th Symposium on Theory of\nComputing (STOC), pages 665\u2013674. ACM, 2013.\n[KT13]\nMichael Kapralov and Kunal Talwar. On di\ufb00erentially private low rank approx-\nimation. In Proc. 24rd Symposium on Discrete Algorithms (SODA). ACM-SIAM,\n2013.\n[MCJ13]\nIoannis Mitliagkas, Constantine Caramanis, and Prateek Jain. Memory limited,\nstreaming PCA. In Proc. 27th Neural Information Processing Systems (NIPS), pages\n2886\u20132894, 2013.\n[MM09]\nFrank McSherry and Ilya Mironov. Di\ufb00erentially private recommender systems:\nbuilding privacy into the net. In Proc. 15th KDD, pages 627\u2013636. ACM, 2009.\n[RV09]\nMark Rudelson and Roman Vershynin. Smallest singular value of a random\nrectangular matrix. Communications on Pure and Applied Mathematics, 62(12):1707\u2013\n1739, 2009.\n[SS07]\nValeria Simoncini and Daniel B. Szyld. Recent computational developments\nin krylov subspace methods for linear systems. Numerical Linear Algebra With\nApplications, 14:1\u201359, 2007.\n23\nA\nDeferred Concentration Inequalities\nTheorem A.1 (Cherno\ufb00bound). Let the random variables X1,...,Xm be independent random\nvariables such that for every i, Xi \u2208[\u22121,1] almost surely. Let X = Pm\ni=1 Xi and let \u03c32 = VX. Then,\nfor any t > 0, Pr{|X \u2212EX| > t} \u2a7dexp\n\u0010\n\u2212t2\n4\u03c32\n\u0011\n.\nThe next lemma follows from standard concentration properties of the Gaussian distribu-\ntion.\nLemma A.2. Let U \u2208Rd\u00d7k be a matrix with orthonormal columns. Let G1,...,GL \u223cN(0,\u03c32)d\u00d7p\nwith k \u2a7dp \u2a7dd and assume that L \u2a7dd. Then, with probability 1 \u221210\u22124,\nmax\n\u2113\u2208[L] \u2225U\u22a4G\u2113\u2225\u2a7dO\n\u0010\n\u03c3\np\np + logL\n\u0011\n.\nProof. By rotational invariance of G\u2113the spectral norm \u2225U\u22a4G\u2113\u2225is distributed like largest\nsingular value of a random draw from k \u00d7p gaussian matrix N(0,\u03c32)k\u00d7p. Since p \u2a7ek, the largest\nsingular value strongly concentrates around O(\u03c3\u221ap) with a gaussian tail. By the gaussian\nconcentration of Lipschitz functions of gaussians, taking the maximum over L gaussian\nmatrices introduces an additive O(\u03c3\np\nlogL) term.\n\u25a0\nWe also have an analogue of the previous lemma for the Laplacian distribution.\nLemma A.3. Let U \u2208Rn\u00d7k be a matrix with orthonormal columns. Let G1,...,GL \u223cLap(0,\u03bb)d\u00d7p\nwith k \u2a7dp \u2a7dd and assume that L \u2a7dd. Then, with probability 1 \u221210\u22124,\nmax\n\u2113\u2208[L] \u2225U\u22a4G\u2113\u2225\u2a7dO\n\u0010\n\u03bb\np\npk log(Lpk)\n\u0011\n.\nProof. We claim that with probability 1\u221210\u22124 for every \u2113\u2208[L], every entry of U\u22a4G\u2113is bounded\nby O(\u03bblog(Lpk)) in absolute value. This follows because each entry has variance \u03bb2 and is a\nweighted sum of n independent Laplacian random variables Lap(0,\u03bb). Assuming this event\noccurs, we have\nmax\n\u2113\u2208[L] \u2225U\u22a4G\u2113\u2225\u2a7dmax\n\u2113\u2208[L] \u2225U\u22a4G\u2113\u2225F \u2a7dO\n\u0010\n\u03bb\np\npk log(Lpk)\n\u0011\n.\n\u25a0\nLemma A.4 (Matrix Cherno\ufb00). Let X1,...,Xn \u223cX be i.i.d. random matrices of maximum dimen-\nsion d and mean \u00b5, uniformly bounded by \u2225X\u2225\u2a7dR. Then for all t \u2a7d1,\nPr\nn\r\r\r 1\nn\nP\ni Xi \u2212EX1\n\r\r\r \u2a7etR\no\n\u2a7dde\u2212\u2126(mt2)\nB\nReduction to symmetric matrices\nFor all our purposes it su\ufb03ces to consider symmetric n \u00d7 n matrices. Given a non-symmetric\nm \u00d7 n matrix B we may always consider the (m + n) \u00d7 (m + n) matrix A = [0B|B\u22a40]. This\ntransformation preserves all the parameters that we are interested in as was argued in [HR13]\nmore formally. This allows us to discuss symmetric eigendecompositions rather than singular\nvector decompositions and therefore simplify our presentation below.\n24\n",
        "sentence": " The stochastic online eigenvector problem is almost equivalent to the streaming PCA problem [6, 17]. The two papers [6, 17] use different techniques from ours and do not imply our result on stochastic online eigenvector.",
        "context": "applied to. See Chapter 19.9 in [Hig02] and the references therein for background. Our proof\nstrategy avoids this particular dependence on the condition number.\nStreaming PCA.\nPCA in the streaming model is related to a host of well-studied problems\n1.1\nApplication to memory-e\ufb03cient streaming PCA\nIn the streaming PCA setting we receive a stream of samples z1,z2,...zn \u2208Rd drawn i.i.d. from\nan unknown distribution D over Rd. Our goal is to compute the dominant k eigenvectors of\nimization for matrix completion, streaming principal component analysis (PCA), and\nprivacy-preserving spectral analysis. Our general analysis subsumes several existing ad-"
    },
    {
        "title": "QIP = PSPACE",
        "author": [
            "Rahul Jain",
            "Zhengfeng Ji",
            "Sarvagya Upadhyay",
            "John Watrous"
        ],
        "venue": "Journal of the ACM (JACM),",
        "citeRegEx": "19",
        "shortCiteRegEx": "19",
        "year": 2011,
        "abstract": "We prove that the complexity class QIP, which consists of all problems having\nquantum interactive proof systems, is contained in PSPACE. This containment is\nproved by applying a parallelized form of the matrix multiplicative weights\nupdate method to a class of semidefinite programs that captures the\ncomputational power of quantum interactive proofs. As the containment of PSPACE\nin QIP follows immediately from the well-known equality IP = PSPACE, the\nequality QIP = PSPACE follows.",
        "full_text": "arXiv:0907.4737v2  [quant-ph]  3 Aug 2009\nQIP = PSPACE\nRahul Jain\u2217\nZhengfeng Ji\u2020\nSarvagya Upadhyay\u2021\nJohn Watrous\u2021\n\u2217Department of Computer Science and Centre for Quantum Technologies\nNational University of Singapore\nRepublic of Singapore\n\u2020Perimeter Institute for Theoretical Physics\nWaterloo, Ontario, Canada\n\u2021Institute for Quantum Computing and School of Computer Science\nUniversity of Waterloo\nWaterloo, Ontario, Canada\nAugust 3, 2009\nAbstract\nWe prove that the complexity class QIP, which consists of all problems having quantum\ninteractive proof systems, is contained in PSPACE. This containment is proved by applying a\nparallelized form of the matrix multiplicative weights update method to a class of semide\ufb01nite\nprograms that captures the computational power of quantum interactive proofs. As the con-\ntainment of PSPACE in QIP follows immediately from the well-known equality IP = PSPACE,\nthe equality QIP = PSPACE follows.\n1\nIntroduction\nEf\ufb01cient proof veri\ufb01cation is a fundamental notion in computational complexity theory. The most\ndirect complexity-theoretic abstraction of ef\ufb01cient proof veri\ufb01cation is represented by the com-\nplexity class NP, wherein a deterministic polynomial-time veri\ufb01cation procedure decides whether\na given polynomial-length proof string is valid for a given input. One cannot overstate the im-\nportance of this class and its presently unknown relationship to P, the class of problems solvable\ndeterministically in polynomial time. This problem, which is known as the P versus NP problem,\nis one of the greatest of all unsolved problems in mathematics.\nIn the early to mid 1980\u2019s, Babai [Bab85] and Goldwasser, Micali, and Rackoff [GMR85] intro-\nduced a computational model that extends the notion of ef\ufb01cient proof veri\ufb01cation to interactive\nsettings. (Journal versions of these papers appeared later as [BM88] and [GMR89].) In this model,\nwhich is known as the interactive proof system model, a computationally bounded veri\ufb01er interacts\nwith a prover of unlimited computation power. The interaction comprises one or more rounds\nof communication between the prover and veri\ufb01er, and the veri\ufb01er may make use of randomly\ngenerated bits during the interaction. After the rounds of communication are \ufb01nished, the veri\ufb01er\nmakes a decision to accept or reject based on the interaction.\nA decision problem A is said to have an interactive proof system if there exists a veri\ufb01er,\nalways assumed to run in polynomial time, that meets two conditions: the completeness condition\n1\nand the soundness condition. The completeness condition formalizes the requirement that true\nstatements can be proved, which in the present setting means that if an input string x is a yes-\ninstance of A, then there exists a course of action for the prover that causes the veri\ufb01er to accept\nwith high probability. The soundness condition formalizes the requirement that false statements\ncannot be proved, meaning in this case that if an input string x is a no-instance of A, then the\nveri\ufb01er will reject with high probability no matter what course of action the prover takes. One\ndenotes by IP the collection of decision problems having interactive proof systems. (Here, and\nthroughout the rest of the paper, we take the term problem to mean promise problem, and consider\nthat all complexity classes to be discussed are classes of promise problems. Promise problems\nwere de\ufb01ned by Even, Selman and Yacobi [ESY84], and readers unfamiliar with them are referred\nto the survey of Goldreich [Gol05].)\nThe expressive power of interactive proof systems was not initially known when they were\n\ufb01rst de\ufb01ned, but it was soon determined to coincide with PSPACE, the class of problems solvable\ndeterministically in polynomial space. The containment IP \u2286PSPACE, which is generally at-\ntributed to Feldman [Fel86], is fairly straightforward\u2014and readers not interested in proving this\nfact for themselves can \ufb01nd a proof in [HO02]. Known proofs [LFKN92, Sha92, She92] of the re-\nverse containment PSPACE \u2286IP, on the other hand, are not straightforward, and make essential\nuse of a technique commonly known as arithmetization. This technique involves the extension of\nBoolean formulas to multivariate polynomials over large \ufb01nite \ufb01elds whose 0 and 1 elements are\ntaken to represent Boolean values. Through the use of randomness and polynomial interpolation,\nveri\ufb01ers may be constructed for arbitrary PSPACE problems.\nMany variants of interactive proof systems have been studied, including public-coin interac-\ntive proofs [Bab85, BM88, GS89], multi-prover interactive proofs [BOGKW88], zero-knowledge in-\nteractive proofs [GMR89, GMW91], and competing-prover interactive proofs [FK97]. The present\npaper is concerned with quantum interactive proof systems, which were \ufb01rst studied a decade after\nIP = PSPACE was proved [Wat99, KW00]. The fundamental notions of this model are the same as\nthose of classical interactive proof systems, except that the prover and veri\ufb01er may now process\nand exchange quantum information. Similar to the classical case, several variants of quantum\ninteractive proof systems have been studied (including those considered in [HKSZ08, KKMV09,\nKM03, Kob08, MW05, Wat09]).\nOne of the most interesting aspects of quantum interactive proof systems, which distinguishes\nthem from classical interactive proof systems (at least to the best of our current knowledge), is that\nthey can be parallelized to three messages. That is, quantum interactive proof systems consisting\nof just three messages exchanged between the prover and veri\ufb01er already have the full power of\nquantum interactive proofs having a polynomial number of messages [KW00]. Classical inter-\nactive proofs are not known to hold this property, and if they do the polynomial-time hierarchy\ncollapses to the second level [BM88].\nThe complexity class QIP is de\ufb01ned as the class of decision problems having quantum inter-\nactive proof systems. QIP trivially contains IP, as the ability of a veri\ufb01er to process quantum\ninformation is never a hindrance\u2014a quantum veri\ufb01er can simulate a classical veri\ufb01er, and a com-\nputationally unbounded prover can never use quantum information to an advantage against a\nveri\ufb01er behaving classically. The inclusion PSPACE \u2286QIP is therefore immediate. The best upper\nbound on QIP known prior to the present paper was QIP \u2286EXP, which was proved in [KW00]\nthrough the use of semide\ufb01nite programming. The optimal probability with which a given veri\ufb01er\ncan be made to accept in a quantum interactive proof system can be represented as an exponential-\nsize semide\ufb01nite program, and known polynomial-time algorithms for semide\ufb01nite programming\n2\nprovide the required tool to prove the containment. It has been an open problem for the last decade\nto establish more precise bounds on the class QIP.\nIt was recently shown in the paper [JUW09] that QIP(2), the class of problem having 2-message\nquantum interactive proof systems, is contained in PSPACE. That paper made use of a parallel\nalgorithm, based on a method known as the matrix multiplicative weights update method, to ap-\nproximate optimal solutions for a class of semide\ufb01nite programs that represent the maximum\nacceptance probabilities for veri\ufb01ers in two-message quantum interactive proofs. In this paper we\nextend this result to all of QIP, establishing the relationship QIP = PSPACE. Similar to [JUW09],\nwe use the matrix multiplicative weights update method, together with parallel methods for ma-\ntrix computations.\nThe multiplicative weights method is a framework for algorithm design having its origins in\nvarious \ufb01elds, including learning theory, game theory, and optimization. Its matrix variant, as\ndiscussed in the survey paper [AHK05] and the PhD thesis of Kale [Kal07], gives an iterative\nway to approximate the optimal value of semide\ufb01nite programs [AK07, WK06]. In addition to its\napplication in [JUW09], it was applied to quantum complexity in [JW09] to prove the containment\nof the complexity class QRG(1) in PSPACE. The key strength of this method for these applications\nis that it can be parallelized for some special classes of semide\ufb01nite programs.\nA key result that allows our technique to work for the entire class QIP is the characterization\nQIP = QMAM proved in [MW05]. This characterization, which is described in greater detail in\nthe next section, concerns a restricted notion of interactive proof systems known as Arthur\u2013Merlin\ngames. An Arthur\u2013Merlin game is an interactive proof system wherein the veri\ufb01er can only send\nuniformly generated random bits to the prover. Following Babai [Bab85], one refers to the veri\ufb01er\nas Arthur and to the prover as Merlin in this setting. It is also typical to refer to the individual bits of\nArthur\u2019s messages as coins, given that they are each uniformly generated like the \ufb02ip of a fair coin.\nThe restriction that Arthur sends only uniformly generated bits to Merlin, and therefore does not\nhave the option to base his messages on private information unknown to Merlin, would seem to\nlimit the power of Arthur\u2013Merlin games in comparison to ordinary interactive proof systems. But\nin fact this is known not to be the case, both for classical [GS89] and quantum [MW05] interactive\nproof systems. In the quantum setting, this characterization admits a signi\ufb01cant simpli\ufb01cation in\nthe semide\ufb01nite programs that capture the complexity of the class QIP.\nThe remainder of this paper has the following organization. Section 2 includes background\ninformation, notation, and other preliminary discussions that are relevant to the remainder of the\npaper. Section 3 describes a semide\ufb01nite programming problem that captures the complexity of\nthe class QIP based on quantum Arthur\u2013Merlin games, and Section 4 presents the main algorithm\nthat solves this problem. Finally, Section 5 discusses a parallel approximation to the algorithm\nfrom Section 4 and explains how its properties lead to the containment QIP \u2286PSPACE.\n2\nPreliminaries\nThis section contains a summary of the notation and terminology on linear algebra, quantum in-\nformation, semide\ufb01nite programming, quantum Arthur\u2013Merlin games, and bounded-depth cir-\ncuits that is used later in the paper. For the most part, these discussions are intended only to\nmake clear the notation and terminology that we use, and not to provide introductions to these\ntopics. We assume that the reader already has familiarity with complexity theory and quantum\ncomputing, and refer readers who are not to [AB09] and [NC00].\n3\n2.1\nLinear algebra and quantum information\nA quantum register refers to a collection of qubits, or more generally a \ufb01nite-size component in a\nquantum computer. Every quantum register V has associated with it a \ufb01nite, non-empty set \u03a3\nof classical states and a complex vector space of the form V = C\u03a3. We use the Dirac notation\n{|a\u27e9: a \u2208\u03a3} to refer to the standard basis (or elementary unit vectors) in V, and de\ufb01ne the inner\nproduct and Euclidean norm on V in the standard way. The set {\u27e8a| : a \u2208\u03a3} consists of the\nelements in the dual space of V that are in correspondence with the standard basis vectors.\nFor such a space V, we write L (V) to denote the space of linear mappings, or operators, from\nV to itself, which is identi\ufb01ed with the set of square complex matrices indexed by \u03a3 in usual way.\nAn inner product on L (V) is de\ufb01ned as\n\u27e8A, B\u27e9= Tr(A\u2217B),\nwhere A\u2217denotes the adjoint (or conjugate transpose) of A. The identity operator on V is denoted\n1V (or just 1 when V is understood).\nThe following special types of operators are relevant to the paper:\n1. An operator A \u2208L (V) is Hermitian if A = A\u2217. The eigenvalues of a Hermitian operator are\nalways real, and for m = dim(V) we write\n\u03bb1(A) \u2265\u03bb2(A) \u2265\u00b7 \u00b7 \u00b7 \u2265\u03bbm(A)\nto denote the eigenvalues of A sorted from largest to smallest.\n2. An operator P \u2208L (V) is positive semide\ufb01nite if it is Hermitian and all of its eigenvalues are\nnonnegative. The set of such operators is denoted Pos (V). The notation P \u22650 also indicates\nthat P is positive semide\ufb01nite, and more generally the notations A \u2264B and B \u2265A indicate\nthat B \u2212A \u22650 for Hermitian operators A and B.\nEvery Hermitian operator A can be expressed uniquely as A = P \u2212Q for positive semide\ufb01nite\noperators P and Q satisfying \u27e8P, Q\u27e9= 0. The operator P is said to be the positive part of A,\nwhile Q is the negative part.\n3. A positive semide\ufb01nite operator P \u2208Pos (V) is also said to be positive de\ufb01nite if all of its eigen-\nvalues are positive (which implies that P must be invertible). The notation P > 0 also indicates\nthat P is positive de\ufb01nite, and the notations A < B and B > A indicate that B \u2212A > 0 for\nHermitian operators A and B.\n4. An operator \u03c1 \u2208Pos (V) is a density operator if it is both positive semide\ufb01nite and has trace\nequal to 1. The set of such operators is denoted D (V).\n5. An operator \u03a0 \u2208Pos (V) is a projection if all of its eigenvalues are either 0 or 1.\nA quantum state of a register V is a density operator \u03c1 \u2208D (V), and a measurement on V is a\ncollection {Pb : b \u2208\u0393} \u2286Pos (V) satisfying\n\u2211\nb\u2208\u0393\nPb = 1V.\nThe set \u0393 is the set of measurement outcomes, and when such a measurement is performed on V\nwhile it is in the state \u03c1, each outcome b \u2208\u0393 occurs with probability \u27e8Pb, \u03c1\u27e9.\n4\nThe spectral norm of an operator A \u2208L (V) is de\ufb01ned as\n\u2225A\u2225= max{\u2225Av\u2225: v \u2208V, \u2225v\u2225= 1}.\nThe spectral norm is sub-multiplicative, meaning that \u2225AB\u2225\u2264\u2225A\u2225\u2225B\u2225for all operators A, B \u2208\nL (V), and it holds that \u2225P\u2225= \u03bb1(P) for every positive semide\ufb01nite operator P. For any operator\nA \u2208L (V), the exponential of A is de\ufb01ned as\nexp(A) = 1 + A + A2/2 + A3/6 + \u00b7 \u00b7 \u00b7\nThe Golden-Thompson Inequality (see Section IX.3 of [Bha97]) states that, for any two Hermitian\noperators A and B on V, we have\nTr [exp(A + B)] \u2264Tr [exp(A) exp(B)] .\nThe tensor product V \u2297W of vector spaces V = C\u03a3 and W = C\u0393 may be associated with the\nspace C\u03a3\u00d7\u0393, and the tensor product of operators A \u2208L (V) and B \u2208L (W) is then taken to be\nthe unique operator A \u2297B \u2208L (V \u2297W) satisfying (A \u2297B)(v \u2297w) = (Av) \u2297(Bw) for all v \u2208V\nand w \u2208W. These notions may be associated with the usual Kronecker product of vectors and\nmatrices. For quantum registers V and W, the space V \u2297W is associated with the pair (V, W),\nviewed as a single register. Tensor products involving three or more spaces are handled similarly.\nFor a given linear mapping of the form \u03a6 : L (V) \u2192L (W), one de\ufb01nes the adjoint mapping\n\u03a6\u2217: L (W) \u2192L (V) to be the unique linear mapping that satis\ufb01es\n\u27e8B, \u03a6(A)\u27e9= \u27e8\u03a6\u2217(B), A\u27e9\nfor all operators A \u2208L (V) and B \u2208L (W).\nFinally, for spaces V and W, one de\ufb01nes the partial trace TrV : L (V \u2297W) \u2192L (W) to be the\nunique linear mapping that satis\ufb01es TrV(A \u2297B) = (Tr A)B for all A \u2208L (V) and B \u2208L (W). A\nsimilar notation is used for the partial trace TrW, or partial traces de\ufb01ned on three or more tensor\nfactors. When this notation is used, the spaces on which the trace is not taken are determined by\ncontext. When a pair of registers (V, W) is viewed as a single register and has the quantum state\n\u03c1 \u2208D (V \u2297W), one de\ufb01nes the state of W to be TrV(\u03c1). In other words, the partial trace describes\nthe action of destroying, or simply ignoring, a given quantum register.\n2.2\nSemide\ufb01nite programming\nA semide\ufb01nite program over complex vector spaces V and W is a pair of optimization problems as\nfollows.\nPrimal problem\nmaximize:\n\u27e8C, X\u27e9\nsubject to:\n\u03a8(X) \u2264D,\nX \u2208Pos (V) .\nDual problem\nminimize:\n\u27e8D, Y\u27e9\nsubject to:\n\u03a8\u2217(Y) \u2265C,\nY \u2208Pos (W) .\nHere, the operators C \u2208L (V) and D \u2208L (W) are Hermitian and \u03a8 : L (V) \u2192L (W) must be\na linear mapping that maps Hermitian operators to Hermitian operators. Readers familiar with\nsemide\ufb01nite programming will note that the above form of a semide\ufb01nite program is different\n5\nfrom the well-known standard form, but it is equivalent and better suited for this paper\u2019s needs.\nThe form given above is, in essence, the one that is typically followed for general conic program-\nming [BV04].\nIt is typical that semide\ufb01nite programs are stated in forms that do not explicitly describe \u03a8, C\nand D, and the same is true for the semide\ufb01nite programs we will consider. It is, however, routine\nto put them into the above form.\nWith the above optimization problems in mind, one de\ufb01nes the primal feasible set P and the\ndual feasible set D as\nP = {X \u2208Pos (V) : \u03a8(X) \u2264D} ,\nD = {Y \u2208Pos (W) : \u03a8\u2217(Y) \u2265C} .\nOperators X \u2208P and Y \u2208D are also said to be primal feasible and dual feasible, respectively. The\nfunctions X 7\u2192\u27e8C, X\u27e9and Y 7\u2192\u27e8D, Y\u27e9are called the primal and dual objective functions, and the\noptimal values associated with the primal and dual problems are de\ufb01ned as\n\u03b1 = sup\nX\u2208P\n\u27e8C, X\u27e9\nand\n\u03b2 = inf\nY\u2208D \u27e8D, Y\u27e9.\nSemide\ufb01nite programs have associated with them a powerful theory of duality, which refers\nto the special relationship between the primal and dual problems. The property of weak duality,\nwhich holds for all semide\ufb01nite programs, states that \u03b1 \u2264\u03b2. This property implies that every dual\nfeasible operator Y \u2208D provides an upper bound of \u27e8D, Y\u27e9on the value \u27e8C, X\u27e9that is achievable\nover all choices of a primal feasible X \u2208P, and likewise every primal feasible operator X \u2208P\nprovides a lower bound of \u27e8C, X\u27e9on the value \u27e8D, Y\u27e9that is achievable over all choices of a dual\nfeasible Y \u2208D.\nIt is not always the case that \u03b1 = \u03b2 for a given semide\ufb01nite program, but in most natural cases\nit does hold. The situation in which \u03b1 = \u03b2 is known as strong duality, and several conditions have\nbeen identi\ufb01ed that imply strong duality. One such condition is strict dual feasibility: if \u03b1 is \ufb01nite\nand there exists an operator Y > 0 such that \u03a8\u2217(Y) > C, then \u03b1 = \u03b2. The symmetric condition of\nstrict primal feasibility also implies strong duality.\n2.3\nSingle-coin quantum Arthur\u2013Merlin games\nQuantum Arthur\u2013Merlin games were proposed in [MW05] as a natural quantum variant of clas-\nsical Arthur\u2013Merlin games. Here, one simply mimics the classical de\ufb01nition in requiring that\nArthur\u2019s messages to Merlin consist of uniformly generated random bits. Merlin\u2019s messages to\nArthur, however, may be quantum; and after all of the messages have been exchanged Arthur is\nfree to perform a quantum computation when deciding to accept or reject.\nOf particular interest to us are quantum Arthur\u2013Merlin games in which three messages are\nexchanged, and where Arthur\u2019s only message consists of a single bit. In more precise terms, such\nan interaction takes the following form:\n1. Merlin sends a quantum register W to Arthur. Merlin is free to initialize this register to any\nquantum state of his choice, and may entangle it with a register of his own if he chooses.\n2. After receiving W from Merlin, Arthur chooses a bit a \u2208{0, 1} uniformly at random. Merlin\nlearns the value of a.\n6\n3. Merlin sends Arthur a second quantum register Y. He does this after step 2, so he has the\noption to condition the state of Y upon the value of a. The register Y could, of course, be\nentangled with W in any way that quantum information theory permits.\n4. After receiving Y, Arthur performs one of two binary-valued measurements, determined by\nthe value of the random bit a, on the pair (W, Y). The measurement outcome 1 is interpreted\nas acceptance, while 0 is interpreted as rejection.\nArthur\u2019s measurements must of course be ef\ufb01ciently implementable. This notion is formalized\nby requiring that the measurements are implementable by polynomial-time generated families of\nquantum circuits, which naturally requires the registers W and Y to consist of a number of qubits\nthat is polynomial in the length of the input. Further details may be found in [MW05].\nThe result of [MW05] that we make use of is that every problem A \u2208QIP has a single-coin\nArthur\u2013Merlin game as just described. The game is such that if x is a yes-instance of the problem\nA, then Arthur accepts with probability 1, whereas if the input x is a no-instance of the prob-\nlem then Arthur accepts with probability at most 1/2 + \u03b5, for any desired constant \u03b5 > 0. (In\nthe construction given in [MW05], Arthur\u2019s measurements are always nontrivial projective mea-\nsurements. This implies that even for no-instance inputs, Merlin can cause Arthur to accept with\nprobability at least 1/2 by simply guessing in advance Arthur\u2019s random bit.)\n2.4\nBounded-depth circuit complexity\nIn the last section of the paper, we will require the de\ufb01nitions of two complexity classes based\non bounded-depth circuit families: NC and NC(poly). It is convenient for us to de\ufb01ne these as\nclasses of functions rather than decision problems, and when we wish to view them as classes of\ndecision problems we simply restrict our attention to binary-valued functions. The class NC con-\ntains all functions computable by logarithmic-space uniform Boolean circuits of polylogarthmic\ndepth, and NC(poly) contains all functions that can be computed by polynomial-space uniform\nfamilies of Boolean circuits having polynomial-depth. For decision problems it is known [Bor77]\nthat NC(poly) = PSPACE, and the proof of our main result will make use of this fact.\nThere are two fundamental properties of NC(poly) that we will take advantage of. The \ufb01rst\nis that functions in NC and NC(poly) compose well, and the second is that many computational\nproblems involving matrices are in NC. In more precise terms, the \ufb01rst property is as follows. If\nF : {0, 1}\u2217\u2192{0, 1}\u2217is a function in NC(poly) and G : {0, 1}\u2217\u2192{0, 1}\u2217is a function in NC,\nthen the composition G \u25e6F is also in NC(poly). This follows from the most straightforward way\nof composing the families of circuits that compute F and G.\nTo discuss the second property, it will be helpful to make clear our assumptions concerning\nmatrix computations. We will always assume that the matrices on which computations are per-\nformed have entries with rational real and imaginary parts, and that the rational numbers are\nrepresented as pairs of integers in binary notation. Unless it is explicitly noted otherwise, any\nother rational numbers involved in our computations will be represented in a similar way.\nWith these assumptions in place, we \ufb01rst note that elementary matrix operations, including\ninverses and iterated sums and products of matrices, are known to be in NC. There is an extensive\nliterature on this topic, and we refer the reader to von zur Gathen\u2019s survey [Gat93] for more details.\nWe also note that matrix exponentials and spectral decompositions can be approximated to high\naccuracy in NC. In more precise terms, the following two problems are in NC.\n7\nMatrix exponentials\nInput:\nAn n \u00d7 n matrix M, a positive rational number \u03b7, and an integer k expressed in\nunary notation (i.e., 1k).\nPromise:\n\u2225M\u2225\u2264k.\nOutput:\nAn n \u00d7 n matrix X such that \u2225exp(M) \u2212X\u2225< \u03b7.\nSpectral decompositions\nInput:\nAn n \u00d7 n Hermitian matrix H and a positive rational number \u03b7.\nOutput:\nAn n \u00d7 n unitary matrix U and an n \u00d7 n real diagonal matrix \u039b such that\n\u2225M \u2212U\u039bU\u2217\u2225< \u03b7.\nThe reader will note that in these problems, the description of the error parameter \u03b7 could require\nas few as O(log(1/\u03b7)) bits. This implies that highly accurate approximations, for instance where\n\u03b7 = 2\u2212n, are possible in NC. The fact that matrix exponentials can be approximated in NC follows\nby truncating the series\nexp(M) = 1 + M + M2/2 + M3/6 + \u00b7 \u00b7 \u00b7\nto a number of terms linear in k + log(1/\u03b7). (From a numerical point of view this is not a very\ngood way to compute matrix exponentials [ML03], but it is arguably the simplest way to prove that\nthe stated problem is in NC.) The fact that spectral decompositions can be approximated in NC\nfollows from a composition of known facts: in NC one can compute characteristic polynomials and\nnull spaces of matrices, perform orthogonalizations of vectors, and approximate roots of integer\npolynomials to high precision [Csa76, BGH82, BCP83, BOFKT86, Gat93, Nef94].\n3\nA semide\ufb01nite programming formulation of the problem\nConsider Arthur\u2019s veri\ufb01cation procedure for a given single-coin QMAM protocol on a \ufb01xed input\nstring x. Arthur \ufb01rst receives a register W, then generates a random bit a \u2208{0, 1}, and then re-\nceives a second register Y. He then measures (W, Y) with respect to a binary-valued measurement\n{Pa, 1 \u2212Pa} \u2282Pos (W \u2297Y) ,\nwhere we take each of the operators P0 and P1 to represent acceptance and 1 \u2212P0 and 1 \u2212P1 to\nrepresent rejection. If the quantum state of (W, Y) is given by a density operator \u03c1 \u2208D (W \u2297Y)\nwhen Arthur measures, he will therefore accept with probability \u27e8Pa, \u03c1\u27e9.\nNow de\ufb01ne\nQ = 1\n2 |0\u27e9\u27e80| \u2297P0 + 1\n2 |1\u27e9\u27e81| \u2297P1 \u2208Pos (X \u2297W \u2297Y) ,\nwhere we take X = C{0,1} to be the vector space corresponding to Arthur\u2019s random choice of\na \u2208{0, 1}, and consider the optimal probability that Merlin can cause Arthur to accept. If, for\neach of the values a \u2208{0, 1}, Merlin is able to leave the state \u03c1a in the registers (W, Y) right before\nArthur measures, he will convince Arthur to accept with probability\n1\n2 \u27e8P0, \u03c10\u27e9+ 1\n2 \u27e8P1, \u03c11\u27e9= \u27e8Q, X\u27e9\n(1)\n8\nfor\nX = |0\u27e9\u27e80| \u2297\u03c10 + |1\u27e9\u27e81| \u2297\u03c11.\nThere is, of course, a constraint on Merlin\u2019s choice of \u03c10 and \u03c11, which is that they must agree on\nW, as Merlin cannot touch the register W at any point after Arthur chooses the random bit a. In\nmore precise terms, it must hold that\nTrY(\u03c10) = \u03c3 = TrY(\u03c11)\n(2)\nfor some density operator \u03c3 \u2208D (W). This, in fact, is Merlin\u2019s only constraint\u2014for if he holds a\npuri\ufb01cation of the state \u03c3, he is free to set the state of (W, Y) to any choice of \u03c10 and \u03c11 satisfying\n(2) without needing access to W.\nNow, we note that the condition (2) implies that\nTrY(X) = 1X \u2297\u03c3.\n(3)\nMoreover, for an arbitrary operator X \u2208Pos (X \u2297W \u2297Y) satisfying the constraint (3), one has\nthat the operators \u03c10 and \u03c11 de\ufb01ned as\n\u03c1a = (\u27e8a| \u22971W\u2297Y) X (|a\u27e9\u22971W\u2297Y)\nfor a \u2208{0, 1} satisfy the conditions (1) and (2). It follows that the following semide\ufb01nite program\nrepresents the optimal probability with which Merlin can convince Arthur to accept.\nPrimal problem\nmaximize:\n\u27e8Q, X\u27e9\nsubject to:\nTrY(X) \u22641X \u2297\u03c3,\nX \u2208Pos (X \u2297W \u2297Y) ,\n\u03c3 \u2208D (W) .\nDual problem\nminimize:\n\u2225TrX (Y)\u2225\nsubject to:\nY \u22971Y \u2265Q,\nY \u2208Pos (X \u2297W) .\nNote that the inequality in the primal problem can be exchanged for an equality without changing\nthe optimal value. This is because any primal feasible X can be in\ufb02ated to achieve the equality\nTrY(X) = 1X \u2297\u03c3 for some choice of \u03c3, and this can only increase the value of the objective function\nby virtue of the fact that Q is positive semide\ufb01nite. It is immediate that the optimal solution to the\nprimal problem is bounded and the dual problem is strictly feasible, from which strong duality\nfollows; the primal and dual problems have the same optimal values.\nNow, under the assumption that Q is invertible, one may perform a change of variables to put\nthe above semide\ufb01nite program into a form that more closely resembles the one in [JUW09]. To\ndo this we de\ufb01ne a linear mapping \u03a6 : L (X \u2297W \u2297Y) \u2192L (X \u2297W) as\n\u03a6(X) = TrY\n\u0010\nQ\u22121/2XQ\u22121/2\u0011\n,\n(4)\nwhose adjoint mapping \u03a6\u2217: L (X \u2297W) \u2192L (X \u2297W \u2297Y) is given by\n\u03a6\u2217(Y) = Q\u22121/2(Y \u22971Y)Q\u22121/2,\nand consider the following semide\ufb01nite program.\n9\nPrimal problem\nmaximize:\nTr(X)\nsubject to:\n\u03a6(X) \u22641X \u2297\u03c3,\nX \u2208Pos (X \u2297W \u2297Y) ,\n\u03c3 \u2208D (W) .\nDual problem\nminimize:\n\u2225TrX (Y)\u2225\nsubject to:\n\u03a6\u2217(Y) \u22651X \u2297W\u2297Y,\nY \u2208Pos (X \u2297W) .\nIt is clear that this semide\ufb01nite program has the same optimal value as the previous one.\nWe will be interested in the optimal value of this semide\ufb01nite program in the case that \u2225Q\u22121\u2225\nis upper-bounded by a \ufb01xed constant and where there is a promise on the optimal value. The\npromise, which will come from the properties of the quantum Arthur\u2013Merlin games under con-\nsideration, is that the optimal value does not lie in the interval (5/8, 7/8), and the goal is to\ndetermine whether the optimal value is larger than 7/8 or smaller than 5/8.\nFor readers familiar with the semide\ufb01nite program for QIP(2) presented in [JUW09], we note\nthat there are two essential differences between it and the one above. The \ufb01rst difference is that\nthe semide\ufb01nite program in [JUW09] effectively replaces the density operator \u03c3 with the scalar\nvalue 1, which would seem to suggest added dif\ufb01culty for the case at hand. The second difference\nis that X is two-dimensional for the semide\ufb01nite program above, whereas it has arbitrary size in\n[JUW09]. This second difference more than compensates for the dif\ufb01culty induced by the \ufb01rst,\nand we \ufb01nd that the above semide\ufb01nite program is actually much easier to solve than the one for\nQIP(2).\n4\nThe main algorithm and its analysis\nWe now present the main algorithm for the semide\ufb01nite programming problem from the previous\nsection. The algorithm, which is described in Figure 1, takes as input an operator\nQ \u2208Pos (X \u2297W \u2297Y) .\nIt is assumed that Q is invertible and satis\ufb01es \u2225Q\u22121\u2225\u226464. (The algorithm could easily be adapted\nto handle any other \ufb01xed constant in place of 64, but this choice is suf\ufb01cient for our needs.) More-\nover, it is assumed that the optimal value of the semide\ufb01nite program in Section 3 that is de\ufb01ned\nby Q does not lie in the interval (5/8, 7/8). Our goal is to prove that the algorithm accepts when\nthe optimal value is at least 7/8 and rejects when the optimal value is at most 5/8.\nHere we present the correctness of the algorithm under the assumption that all computations\nare performed exactly. Issues that arise due to inaccuracies in the computation are discussed in\nthe next section.\nAssume \ufb01rst that the algorithm accepts, and write\n\u03c1 = \u03c1t,\n\u03a0 = \u03a0t,\n\u03be = \u03bet\nand\n\u03b2 = \u03b2t\nfor t \u2208{0, . . . , T \u22121} corresponding to the iteration in which acceptance occurs. For the sake of\nclarity, let us note explicitly that\n\u03c1 \u2208D (X \u2297W \u2297Y) ,\n\u03a0 \u2208Pos (X \u2297W)\nand\n\u03be \u2208D (W) .\nWe wish to prove that the optimal value of our semide\ufb01nite program is at least 7/8, and we will\ndo this by constructing a primal feasible solution that achieves an objective value strictly larger\nthan 5/8.\n10\n1. Let N = dim(X \u2297W \u2297Y) and M = dim(W), and de\ufb01ne\nW0 = 1X \u2297W\u2297Y,\n\u03c10 = W0/N,\nZ0 = 1W\nand\n\u03be0 = Z0/M.\nAlso let\n\u03b3 = 4\n3,\n\u03b5 = 1\n64,\n\u03b4 =\n\u03b5\n2 \u2225Q\u22121\u2225\nand\nT =\n\u00184 log(N)\n\u03b53\u03b4\n\u0019\n.\n2. Repeat for each t = 0, . . . , T \u22121:\n(a) Let \u03a0t be the projection onto the positive eigenspaces of the operator\n\u03a6(\u03c1t) \u2212\u03b3 1X \u2297\u03bet,\nwhere \u03a6 is de\ufb01ned from Q as in (4), and set \u03b2t = \u27e8\u03a0t, \u03a6(\u03c1t)\u27e9.\n(b) If \u03b2t \u2264\u03b5 then accept, else let\nWt+1 = exp\n \n\u2212\u01eb\u03b4\nt\n\u2211\nj=0\n\u03a6\u2217(\u03a0j/\u03b2j)\n!\n,\n\u03c1t+1 = Wt+1/ Tr(Wt+1),\nand\nZt+1 = exp\n \n\u03b5\u03b4\nt\n\u2211\nj=0\nTrX (\u03a0j/\u03b2j)\n!\n,\n\u03bet+1 = Zt+1/ Tr(Zt+1).\n3. If acceptance did not occur in step 2, then reject.\nFigure 1: An algorithm that accepts if the optimal value of the semide\ufb01nite program in Section 3 is\nlarger than 7/8, and rejects if the optimal value is smaller than 5/8.\nBy the de\ufb01nition of \u03a0, it holds that\n\u03a0\u03a6(\u03c1)\u03a0 \u2265\u03a0(\u03a6(\u03c1) \u2212\u03b3 1X \u2297\u03be)\u03a0 \u2265\u03a6(\u03c1) \u2212\u03b3 1X \u2297\u03be,\n(5)\nand by Lemma 1 (which is stated and proved below) it holds that\n21X \u2297TrX (\u03a0\u03a6(\u03c1)\u03a0) \u2265\u03a0\u03a6(\u03c1)\u03a0.\n(6)\nCombining the equations (5) and (6) one has\n\u03a6(\u03c1) \u22641X \u2297(\u03b3 \u03be + 2 TrX (\u03a0\u03a6(\u03c1)\u03a0)) .\n(7)\nIt therefore holds that\nX =\n\u03c1\n\u03b3 + 2 \u27e8\u03a0, \u03a6(\u03c1)\u27e9\nand\n\u03c3 = \u03b3\u03be + 2 TrX (\u03a0\u03a6(\u03c1)\u03a0)\n\u03b3 + 2 \u27e8\u03a0, \u03a6(\u03c1)\u27e9\nrepresent a feasible solution to the primal problem under consideration, achieving the objective\nvalue\n1\n\u03b3 + 2 \u27e8\u03a0, \u03a6(\u03c1)\u27e9=\n1\n\u03b3 + 2\u03b2 \u2265\n1\n\u03b3 + 2\u03b5 > 5\n8\n11\nas required.\nNow assume that the algorithm rejects, and consider the operator\nY = (1 + 2\u03b5)\nT\nT\u22121\n\u2211\nt=0\n\u03a0t/\u03b2t.\nWe claim that Y is dual feasible and achieves an objective value that is strictly smaller than 7/8.\nThis will imply that the optimal value of the semide\ufb01nite program is at most 5/8.\nLet us \ufb01rst prove that Y is dual feasible. It is clear that Y is positive semide\ufb01nite, so it suf\ufb01ces\nto prove that \u03a6\u2217(Y) \u22651X \u2297W\u2297Y, or equivalently that \u03bbN(\u03a6\u2217(Y)) \u22651. Observe, for each t =\n0, . . . , T \u22121, that\nTr(Wt+1) = Tr [exp (\u2212\u03b5\u03b4\u03a6\u2217(\u03a00/\u03b20 + \u00b7 \u00b7 \u00b7 + \u03a0t/\u03b2t))]\n\u2264Tr [exp (\u2212\u03b5\u03b4\u03a6\u2217(\u03a00/\u03b20 + \u00b7 \u00b7 \u00b7 + \u03a0t\u22121/\u03b2t\u22121)) exp (\u2212\u03b5\u03b4\u03a6\u2217(\u03a0t/\u03b2t))]\n= Tr [Wt exp (\u2212\u03b5\u03b4\u03a6\u2217(\u03a0t/\u03b2t))]\nby the Golden\u2013Thompson inequality. As each \u03a0t is a projection operator, we have\n\u2225\u03a6\u2217(\u03a0t)\u2225=\n\r\r\rQ\u22121/2(\u03a0t \u22971Y)Q\u22121/2\r\r\r \u2264\n\r\r\rQ\u22121/2\r\r\r\n2\n=\n\r\r\rQ\u22121\r\r\r ,\nwhere we have used the sub-multiplicativity of the spectral norm to obtain the inequality. Given\nthat \u03b2t > \u03b5 in the case at hand, it follows that \u2225\u03b4\u03a6\u2217(\u03a0t/\u03b2t)\u2225< 1. By Lemma 2 (also presented\nbelow) it therefore follows that\nexp (\u2212\u03b5\u03b4\u03a6\u2217(\u03a0t/\u03b2t)) \u22641 \u2212\u03b5\u03b4 exp(\u2212\u03b5)\u03a6\u2217(\u03a0t/\u03b2t).\nAs each Wt is positive semide\ufb01nite, we obtain\nTr(Wt+1) \u2264Tr(Wt)\n\u0012\n1 \u2212\u03b5\u03b4 exp(\u2212\u03b5)\n\u001c\nWt\nTr(Wt), \u03a6\u2217(\u03a0t/\u03b2t)\n\u001d\u0013\n.\n(8)\nSubstituting \u03c1t = Wt/ Tr(Wt) yields\nTr(Wt+1) \u2264Tr(Wt) (1 \u2212\u03b5\u03b4 exp(\u2212\u03b5) \u27e8\u03c1t, \u03a6\u2217(\u03a0t/\u03b2t)\u27e9)\n= Tr(Wt) (1 \u2212\u03b5\u03b4 exp(\u2212\u03b5))\n\u2264Tr(Wt) exp(\u2212\u03b5\u03b4 exp(\u2212\u03b5)),\nwhere the equality follows from \u27e8\u03c1t, \u03a6\u2217(\u03a0t)\u27e9= \u27e8\u03a6(\u03c1t), \u03a0t\u27e9= \u03b2t and the last inequality follows\nfrom the fact that 1 + z \u2264exp(z) for all real numbers z. As Tr(W0) = N, it follows that\nTr(WT) \u2264Tr(W0) exp(\u2212T\u03b5\u03b4 exp(\u2212\u03b5)) = exp(\u2212T\u03b5\u03b4 exp(\u2212\u03b5) + log(N)).\n(9)\nOn the other hand, we have\nTr(WT) = Tr\n\"\nexp\n \n\u2212\u03b5\u03b4\nT\u22121\n\u2211\nt=0\n\u03a6\u2217(\u03a0t/\u03b2t)\n!#\n\u2265exp\n \n\u2212\u03b5\u03b4\u03bbN\n \n\u03a6\u2217\n \nT\u22121\n\u2211\nt=0\n\u03a0t/\u03b2t\n!!!\n.\n(10)\nCombining (9) and (10), we have\n\u03bbN\n \n\u03a6\u2217\n \nT\u22121\n\u2211\nt=0\n\u03a0t/\u03b2t\n!!\n\u2265T exp(\u2212\u03b5) \u2212log(N)\n\u03b5\u03b4\n.\n12\nUsing the inequality exp(\u2212\u03b5) \u2212\u03b52/4 > 1 \u2212\u03b5, and substituting the value of T speci\ufb01ed by the\nalgorithm, we have\n\u03bbN(\u03a6\u2217(Y)) \u2265(1 + 2\u03b5)\n\u0012\nexp(\u2212\u03b5) \u2212log(N)\nT\u03b5\u03b4\n\u0013\n> (1 + 2\u03b5)(1 \u2212\u03b5) > 1\nas required.\nNow it remains to establish an upper bound on the dual objective value achieved by Y. A\nsimilar method to the one used to prove the feasibility of Y above will provide a suitable bound.\nWe begin by observing, for each t = 0, . . . , T \u22121, that\nTr(Zt+1) = Tr [exp (\u03b5\u03b4 TrX (\u03a00/\u03b20 + \u00b7 \u00b7 \u00b7 + \u03a0t/\u03b2t))]\n\u2264Tr [exp (\u03b5\u03b4 TrX (\u03a00/\u03b20 + \u00b7 \u00b7 \u00b7 + \u03a0t\u22121/\u03b2t\u22121)) exp (\u03b5\u03b4 TrX (\u03a0t/\u03b2t))]\n= Tr [Zt exp (\u03b5\u03b4 TrX (\u03a0t/\u03b2t))] .\nGiven that\n\u2225TrX (\u03a0t)\u2225\u2264\u2225(\u27e80| \u22971W) \u03a0t (|0\u27e9\u22971W)\u2225+ \u2225(\u27e81| \u22971W) \u03a0t (|1\u27e9\u22971W)\u2225\u22642,\nand using the fact that \u03b2t > \u01eb in the case at hand, it follows that \u2225\u03b4 TrX (\u03a0t/\u03b2t)\u2225< 1. We now\napply Lemma 2 to obtain\nexp (\u03b5\u03b4 TrX (\u03a0t/\u03b2t)) \u22641 + \u03b5\u03b4 exp(\u03b5) TrX (\u03a0t/\u03b2t).\nAs each Zt is positive semide\ufb01nite it follows that\nTr(Zt+1) \u2264Tr(Zt)\n\u0012\n1 + \u03b5\u03b4 exp(\u03b5)\n\u001c\nZt\nTr(Zt), TrX (\u03a0t/\u03b2t)\n\u001d\u0013\n.\n(11)\nSubstituting \u03bet = Zt/ Tr(Zt) gives\nTr(Zt+1) \u2264Tr(Zt) (1 + \u03b5\u03b4 exp(\u03b5) \u27e8\u03bet, TrX (\u03a0t/\u03b2t)\u27e9) = Tr(Zt) (1 + \u03b5\u03b4 exp(\u03b5) \u27e81X \u2297\u03bet, \u03a0t/\u03b2t\u27e9) .\nNow, as \u27e8\u03a6(\u03c1t) \u2212\u03b31X \u2297\u03bet, \u03a0t\u27e9\u22650, we may again use the fact that 1 + z \u2264exp(z) for all real\nnumbers z to obtain\nTr(Zt+1) \u2264Tr(Zt)\n\u0012\n1 + \u03b5\u03b4 exp(\u03b5)\n\u03b3\n\u27e8\u03a6(\u03c1t), \u03a0t/\u03b2t\u27e9\n\u0013\n\u2264Tr(Zt) exp\n\u0012\u03b5\u03b4 exp(\u03b5)\n\u03b3\n\u0013\n.\n(12)\nConsequently\nTr(ZT) \u2264Tr(Z0) exp\n\u0012 T\u03b5\u03b4 exp(\u03b5)\n\u03b3\n\u0013\n= exp\n\u0012T\u03b5\u03b4 exp(\u03b5)\n\u03b3\n+ log(M)\n\u0013\n.\nOn the other hand we have\nTr(ZT) = Tr\n\"\nexp\n \n\u03b5\u03b4\nT\u22121\n\u2211\nt=0\nTrX (\u03a0t/\u03b2t)\n!#\n\u2265exp\n \n\u03b5\u03b4\u03bb1\n \nTrX\n \nT\u22121\n\u2211\nt=0\n\u03a0t/\u03b2t\n!!!\n,\nand therefore\n\u03bb1\n \nTrX\n \nT\u22121\n\u2211\nt=0\n\u03a0t/\u03b2t\n!!\n\u2264T exp(\u03b5)\n\u03b3\n+ log(M)\n\u03b5\u03b4\n.\n13\nGiven that M < N it follows that\n\u2225TrX (Y)\u2225= \u03bb1(TrX (Y)) \u2264(1 + 2\u03b5)\n\u0012exp(\u03b5)\n\u03b3\n+ log(M)\nT\u03b5\u03b4\n\u0013\n< 7\n8.\nThus, Y is a dual feasible solution whose objective value is smaller than 7/8, and we conclude that\nthe optimal value of our semide\ufb01nite program is at most 5/8 as required.\nIt remains to state and prove the two lemmas that were required in the analysis above. They\nare as follows.\nLemma 1. Let P \u2208Pos (X \u2297Z) be any positive semide\ufb01nite operator, and assume that dim(X ) = 2.\nThen P \u226421X \u2297TrX (P).\nProof. Let \u03c3x, \u03c3y and \u03c3z denote the Pauli operators on X . In matrix form they are\n\u03c3x =\n\u00120\n1\n1\n0\n\u0013\n,\n\u03c3y =\n\u00120\n\u2212i\ni\n0\n\u0013\nand\n\u03c3z =\n\u00121\n0\n0\n\u22121\n\u0013\n.\nAs each of these operators is Hermitian, we have that (\u03c3x \u22971Z)P(\u03c3x \u22971Z), (\u03c3y \u22971Z)P(\u03c3y \u22971Z)\nand (\u03c3z \u22971Z)P(\u03c3z \u22971Z) are positive semide\ufb01nite. It therefore holds that\n21X \u2297TrX (P) = P + (\u03c3x \u22971Z)P(\u03c3x \u22971Z) + (\u03c3y \u22971Z)P(\u03c3y \u22971Z) + (\u03c3z \u22971Z)P(\u03c3z \u22971Z) \u2265P\nas required.\nLemma 2. Let P be an operator satisfying 0 \u2264P \u22641. Then for every real number \u03b7 > 0, the following\ntwo inequalities hold:\nexp(\u03b7P) \u22641 + \u03b7 exp(\u03b7)P,\nexp(\u2212\u03b7P) \u22641 \u2212\u03b7 exp(\u2212\u03b7)P.\nProof. It is suf\ufb01cient to prove the inequalities for P replaced by a scalar \u03bb \u2208[0, 1], for then the op-\nerator inequalities follow by considering a spectral decomposition of P. If \u03bb = 0 both inequalities\nare immediate, so let us assume \u03bb > 0. By the Mean Value Theorem there exists a value \u03bb0 \u2208(0, \u03bb)\nsuch that\nexp(\u03b7\u03bb) \u22121\n\u03bb\n= \u03b7 exp(\u03b7\u03bb0) \u2264\u03b7 exp(\u03b7),\nfrom which the \ufb01rst inequality follows. Similarly, there exists a value \u03bb0 \u2208(0, \u03bb) such that\nexp(\u2212\u03b7\u03bb) \u22121\n\u03bb\n= \u2212\u03b7 exp(\u2212\u03b7\u03bb0) \u2264\u2212\u03b7 exp(\u2212\u03b7),\nwhich yields the second inequality.\n5\nProof that QIP is contained in PSPACE\nWith the algorithm from the previous section in hand, the proof that QIP \u2286PSPACE follows the\nsame approach used in [JUW09] to prove QIP(2) \u2286PSPACE. The proof is described in the two\nsubsections that follow.\n14\n5.1\nSimulation by bounded-depth Boolean circuits\nLet A = (Ayes, Ano) be a promise problem in QIP. Our goal is to prove that A \u2208PSPACE. Given\nthat PSPACE = NC(poly), as was mentioned in Section 2.4, it suf\ufb01ces to prove A \u2208NC(poly).\nUsing Theorem 5.4 of [MW05] we have that there exists a single-coin QMAM-protocol for A\nwith perfect completeness and soundness probability 1/2 + \u03b5, for \u03b5 = 1/64. (Of course any other\nsuf\ufb01ciently small positive constant would do, and in fact one can replace \u03b5 with an exponentially\nsmall value\u2014but this choice is suf\ufb01cient for our needs.) We will make a small modi\ufb01cation in\nArthur\u2019s speci\ufb01cation so that he always accepts outright with probability 4\u03b5, and otherwise mea-\nsures the registers sent by Merlin according to his original speci\ufb01cation. With this modi\ufb01cation in\nplace, we have that if x \u2208Ayes, then Arthur can be made to accept with certainty, while if x \u2208Ano\nthen the maximum probability with which Arthur can be made to accept is smaller than 1/2 + 3\u03b5.\nIt also holds that every strategy of Merlin causes Arthur to accept with probability at least 4\u03b5.\nNow, for any \ufb01xed choice of an input string x \u2208Ayes \u222aAno, let Q be the operator de\ufb01ned from\nthis modi\ufb01ed speci\ufb01cation of Arthur on the input x as was described in Section 3. Give that Arthur\nalways accepts with probability at least 4\u03b5, it follows that the smallest eigenvalue of Q is at least\n2\u01eb. Therefore, Q is invertible and satis\ufb01es \u2225Q\u22121\u2225\u22641/(2\u03b5). Moreover, the semide\ufb01nite program\nde\ufb01ned by Q, as described in Section 3, has an optimal value that is equal to 1 when x \u2208Ayes and\nsmaller than 1/2 + 3\u03b5 when x \u2208Ano.\nNext, consider a two-step computation as follows:\n1. Compute from a given input string x an explicit description of the operator Q speci\ufb01ed above.\n2. Run an NC implementation of the algorithm from Section 4 on Q.\nThe \ufb01rst step of this computation can be performed in NC(poly) using an exact computation. This\nfollows from the fact that in NC(poly) one can \ufb01rst compute explicit matrix representations of\nall of the gates in the quantum circuit specifying Arthur\u2019s measurements, and then process these\nmatrices using elementary matrix operations to obtain Q. Note that, without loss of generality, the\ndescription of Q has length polynomial in N, which (as de\ufb01ned in the algorithm) is the dimension\nof the space on which it acts.\nThe second step of the computation, which is an NC implementation of the algorithm from\nSection 4, is not quite as straightforward as the \ufb01rst step. In fact, it is only possible for us to\napproximate this algorithm in NC, as we only know how to approximate the operator Q\u22121/2, the\nmatrix exponentials, and the spectral decompositions needed to obtain the projection operators\n\u03a00, . . . , \u03a0T\u22121. Nevertheless, we claim that such an approximation is possible in NC, with suf\ufb01cient\naccuracy to distinguish the two cases x \u2208Ayes and x \u2208Ano. This fact is argued in the subsection\nfollowing this one.\nUnder the assumption that the second step is performed in NC, we have that the composition\nof the two steps is an NC(poly) computation. We therefore obtain that A \u2208NC(poly) as required.\n5.2\nA high precision NC implementation of the algorithm\nIt remains to argue that the algorithm from Section 4 can be approximated by an NC computation\nwith suf\ufb01cient accuracy to distinguish the cases x \u2208Ayes and x \u2208Ano as described above. It\nwill be evident from the discussion that follows that obtaining suf\ufb01cient accuracy in NC is not\na signi\ufb01cant challenge; and one could, in fact, demand much greater accuracy (by an order of\nmagnitude) and still be able to perform the computation in NC.\n15\nThe \ufb01rst step in the implementation of the algorithm is to approximate Q\u22121/2. In more pre-\ncise terms, we \ufb01rst compute an operator R such that R2 is a close approximation to Q, and then\ncompute R\u22121 in NC using an exact computation. To compute R, we may compute a spectral de-\ncomposition of Q, and then take R to be the operator that results by replacing each eigenvalue\nin this decomposition with its square root. It is straightforward to perform high-precision ap-\nproximations of these computations in NC with suf\ufb01cient accuracy so that\n\r\rQ \u2212R2\r\r \u2264\u03b5 and\n\r\rR\u22121\r\r \u22641/\u03b5. Now, if we compare two semide\ufb01nite programs, one de\ufb01ned by Q as speci\ufb01ed in\nSection 3 and the other de\ufb01ned similarly with Q replaced by R2, we \ufb01nd that the optimal values\nare close. More speci\ufb01cally, given that\n\r\rQ \u2212R2\r\r \u2264\u03b5, the optimal values of the two semide\ufb01nite\nprograms can differ by at most 2\u03b5. Thus, the optimal value of the semide\ufb01nite program for R2 is\nat least 1 \u22122\u03b5 > 7/8 in case x \u2208Ayes and at most 1/2 + 5\u03b5 < 5/8 in case x \u2208Ano.\nIn the interest of clarity, to avoid introducing a new variable R into the analysis that follows,\nlet us simply rede\ufb01ne Q at this point to be R2. Thus, Q\u22121/2 = R\u22121 is known exactly by our\nimplementation of the algorithm and all of the requirements on Q are in place\u2014which are that\n\r\rQ\u22121/2\r\r \u22641/\u03b5 = 64 and the optimal value of the semide\ufb01nite program in Section 3 de\ufb01ned by Q\nis at least 7/8 if x \u2208Ayes and at most 5/8 if x \u2208Ano.\nNext, let us focus on the projection operators\n\u03a00, . . . , \u03a0T\u22121 \u2208Pos (X \u2297W)\n(13)\nand the density operators\n\u03c10, . . . , \u03c1T \u2208D (X \u2297W \u2297Y)\nand\n\u03be0, . . . , \u03beT \u2208D (W)\n(14)\nthat are to be computed in the course of the algorithm. We will choose an integer K that we\ntake to represent the number of bits of accuracy with which these operators are stored. In more\nprecise terms, the algorithm will store the real and imaginary parts of each of the entries of the\nabove operators (13) and (14) as integers divided by 2K. It will suf\ufb01ce to take K = c\u2308log(N)\u2309,\nfor a suitable choice of a constant c, although one could in fact afford to take K to be polynomial\nin N rather than logarithmic. As each entry of these operators has absolute value at most 1, the\ntotal number of bits needed to represent the entire collection of operators is O(TKN2), which is\npolynomial in N.\nIn addition to the above operators, the algorithm will store the scalar values \u03b20, . . . , \u03b2T\u22121.\nThese values do not need to be approximated; each value \u03b2t is computed exactly as the ratio-\nnal number de\ufb01ned by the operators \u03c1t and \u03a0t stored by the algorithm. We will not consider that\nthe operators W1, . . . , WT and Z1, . . . , ZT are stored by the algorithm at all, as their only purpose\nin the computation is to specify the density operators \u03c11, . . . , \u03c1T and \u03be1, . . . , \u03beT.\nWe will also take \u00b5 to be a small constant, say \u00b5 = 2\u221210, that will represent an error parameter\nfor the computation. Similar to the choice of K, we could afford to take \u00b5 to be signi\ufb01cantly smaller\nthan this and still be able to perform the computation in NC.\nNow, consider the two steps (a) and (b) that are performed within each iteration of the loop\nin step 2 of the algorithm. We must approximate these steps, and we demand the following accu-\nracy requirements when doing this. For step (a), we will require that the projection operator \u03a0t\ncomputed by the algorithm satis\ufb01es the condition\n\u03a0t(\u03a6(\u03c1t) \u2212\u03b31X \u2297\u03bet)\u03a0t \u2265Pt \u2212\u00b5\nM1X \u2297W,\n(15)\nwhere Pt is de\ufb01ned as the positive part of \u03a6(\u03c1t) \u2212\u03b31X \u2297\u03bet. It is possible to perform such a\ncomputation in NC by setting the error parameter \u03b7 in an approximate spectral decomposition\n16\ncomputation of \u03a6(\u03c1t) \u2212\u03b31X \u2297\u03bet as \u03b7 = \u00b5/(2M), for instance. Then, \u03a0t is taken to be the appro-\npriately de\ufb01ned projection operator rounded to K bits of accuracy. For step (b), we will require\nthat\n\u2225\u03c1t+1 \u2212Wt+1/ Tr(Wt+1)\u2225< \u00b5\u03b4\nN\nand\n\u2225\u03bet+1 \u2212Zt+1/ Tr(Zt+1)\u2225< \u00b5\u03b4\nM .\n(16)\nIn these inequalities we do not consider that Wt+1 and Zt+1 are stored by the algorithm, but rather\nwe consider that they are operators de\ufb01ned by the equations\nWt+1 = exp\n \n\u2212\u01eb\u03b4\nt\n\u2211\nj=0\n\u03a6\u2217(\u03a0j/\u03b2j)\n!\nand\nZt+1 = exp\n \n\u03b5\u03b4\nt\n\u2211\nj=0\nTrX (\u03a0j/\u03b2j)\n!\n,\nfor the particular operators \u03a00/\u03b20, . . . , \u03a0t/\u03b2t that are stored by the algorithm. The algorithm\u2019s\napproximations of Wt+1 and Zt+1 determine the density operators \u03c1t+1 and \u03bet+1. As the matrix\nexponentials are to be computed for operators having norm bounded by T = O(log N), it is clear\nthat \u03c1t+1 and \u03bet+1 with the required properties can be computed in NC.\nFinally, we have that the total number of iterations in the algorithm is T = O(log N). Given\nthat each of the iterations of the algorithm can be performed in NC, and that the total number\nof bits that must be stored from one iteration to the next is polynomial in N, we have that the\ncomposition of these T iterations can be performed in NC as well.\nIt remains only to show that the approximations (15) and (16) are suf\ufb01cient to guarantee that\nthe algorithm accepts or rejects correctly. This analysis is done in almost exactly the same way as\nwas presented in Section 4. Even though the operators\n\u03c10, . . . , \u03c1T\u22121,\n\u03be0, . . . , \u03beT\u22121,\nand\n\u03a00/\u03b20, . . . , \u03a0T\u22121/\u03b2T\u22121\ndo not necessarily satisfy the precise equations that were assumed in Section 4, they may never-\ntheless be used to construct primal and dual solutions to the semide\ufb01nite program that satisfy the\nrequired bounds.\nIn the case that the algorithm accepts, a consideration of the operators \u03c1 = \u03c1t, \u03a0 = \u03a0t, and\n\u03be = \u03bet as before allows for the construction of a primal feasible solution with a large objective\nvalue. In place of (7), we have\n\u03a6(\u03c1) \u22641X \u2297\n\u0010\n\u03b3\u03be + 2 TrX (\u03a0\u03a6(\u03c1)\u03a0) + \u00b5\nM1W\n\u0011\n,\nwhich allows for a lower bound of 1/(\u03b3 + 2\u03b5 + \u00b5) for the primal objective function. For our choice\n\u00b5 = 2\u221210 of an error bound, this quantity is still lower-bounded by 5/8, which implies that the\nalgorithm has operated correctly in this case.\nA similar analysis to the one before holds for the case of rejection as well. We consider the\noperators\n\u03a00/\u03b20, . . . , \u03a0T\u22121/\u03b2T\u22121\nproduced by the algorithm, and take\nY = (1 + 2\u03b5)(1 + 2\u00b5)\nT\nT\u22121\n\u2211\nt=0\n\u03a0t/\u03b2t.\nWhen proving the dual feasibility of Y we are no longer free to substitute \u03c1t = Wt/ Tr(Wt), but\ninstead we must introduce a small error term due to the fact that \u03c1t is just an approximation to\n17\nWt/ Tr(Wt). By the \ufb01rst inequality of (16) above we may conclude that\n\u001c\nWt\nTr(Wt), \u03a6\u2217(\u03a0t/\u03b2t)\n\u001d\n\u22651 \u2212\u00b5;\nand by substituting this into (11) and following a similar argument to the one from before we\nobtain\n\u03bbN(\u03a6\u2217(Y)) \u2265(1 + 2\u03b5)(1 + 2\u00b5)\n\u0012\n(1 \u2212\u00b5) exp(\u2212\u03b5) \u2212\u03b52\n4\n\u0013\n> 1.\nThus, dual feasibility holds for Y. Along similar lines, by using (15) and (16), one \ufb01nds again\nthat the dual objective value achieved by Y less than 7/8, and therefore the algorithm operates\ncorrectly in this case as well.\nAcknowledgments\nWe thank Xiaodi Wu for helpful discussions. Rahul Jain\u2019s research is supported by the inter-\nnal grants of the Centre for Quantum Technologies, which is funded by the Singapore Ministry\nof Education and the Singapore National Research Foundation. Zhengfeng Ji\u2019s research at the\nPerimeter Institute is supported by the Government of Canada through Industry Canada and by\nthe Province of Ontario through the Ministry of Research & Innovation. Sarvagya Upadhyay\u2019s\nresearch is supported in part by Canada\u2019s NSERC, CIFAR, MITACS, QuantumWorks, Industry\nCanada, Ontario\u2019s Ministry of Research and Innovation, and the U.S. ARO. John Watrous\u2019s re-\nsearch is supported by Canada\u2019s NSERC and CIFAR.\nReferences\n[AB09]\nS. Arora and B. Barak. Computational Complexity: A Modern Approach. Cambridge\nUniversity Press, 2009.\n[AHK05]\nS. Arora, E. Hazan, and S. Kale. Fast algorithms for approximate semide\ufb01nite pro-\ngramming using the multiplicative weights update method. In Proceedings of the 46th\nAnnual IEEE Symposium on Foundations of Computer Science, pages 339\u2013348, 2005.\n[AK07]\nS. Arora and S. Kale. A combinatorial, primal-dual approach to semide\ufb01nite pro-\ngrams. In Proceedings of the Thirty-Ninth Annual ACM Symposium on Theory of Com-\nputing, pages 227\u2013236, 2007.\n[Bab85]\nL. Babai. Trading group theory for randomness. In Proceedings of the 17th Annual\nACM Symposium on Theory of Computing, pages 421\u2013429, 1985.\n[BCP83]\nA. Borodin, S. Cook, and N. Pippenger. Parallel computation for well-endowed\nrings and space-bounded probabilistic machines. Information and Control, 58:113\u2013\n136, 1983.\n[BGH82]\nA. Borodin, J. von zur Gathen, and J. Hopcroft. Fast parallel matrix and GCD compu-\ntations. In Proceedings of the 23rd Annual IEEE Symposium on Foundations of Computer\nScience, pages 65\u201371, 1982.\n[Bha97]\nR. Bhatia. Matrix Analysis. Springer, 1997.\n18\n[BM88]\nL. Babai and S. Moran. Arthur-Merlin games: a randomized proof system, and a\nhierarchy of complexity classes. Journal of Computer and System Sciences, 36(2):254\u2013\n276, 1988.\n[BOFKT86]\nM. Ben-Or, E. Feig, D. Kozen, and P. Tiwari. A fast parallel algorithm for determining\nall roots of a polynomial with real roots.\nIn Proceedings of the 18th Annual ACM\nSymposium on Theory of Computing, pages 340\u2013349, 1986.\n[BOGKW88] M. Ben-Or, S. Goldwasser, J. Kilian, and A. Wigderson. Multi-prover interactive\nproofs: how to remove intractability assumptions. In Proceedings of the 20th Annual\nACM Symposium on Theory of Computing, pages 113\u2013131, 1988.\n[Bor77]\nA. Borodin. On relating time and space to size and depth. SIAM Journal on Comput-\ning, 6:733\u2013744, 1977.\n[BV04]\nS. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press,\n2004.\n[Csa76]\nL. Csanky. Fast parallel matrix inversion algorithms. SIAM Journal on Computing,\n5(4):618\u2013623, 1976.\n[ESY84]\nS. Even, A. Selman, and Y. Yacobi. The complexity of promise problems with appli-\ncations to public-key cryptography. Information and Control, 61(2):159\u2013173, 1984.\n[Fel86]\nP. Feldman. The optimum prover lies in PSPACE. Manuscript, 1986.\n[FK97]\nU. Feige and J. Kilian. Making games short. In Proceedings of the 29th Annual ACM\nSymposium on Theory of Computing, pages 506\u2013516, 1997.\n[Gat93]\nJ. von zur Gathen.\nParallel linear algebra.\nIn J. Reif, editor, Synthesis of Parallel\nAlgorithms, chapter 13. Morgan Kaufmann Publishers, Inc., 1993.\n[GMR85]\nS. Goldwasser, S. Micali, and C. Rackoff. The knowledge complexity of interactive\nproof systems. In Proceedings of the 17th Annual ACM Symposium on Theory of Com-\nputing, pages 291\u2013304, 1985.\n[GMR89]\nS. Goldwasser, S. Micali, and C. Rackoff. The knowledge complexity of interactive\nproof systems. SIAM Journal on Computing, 18(1):186\u2013208, 1989.\n[GMW91]\nO. Goldreich, S. Micali, and A. Wigderson. Proofs that yield nothing but their valid-\nity or all languages in NP have zero-knowledge proof systems. Journal of the ACM,\n38(1):691\u2013729, 1991.\n[Gol05]\nO. Goldreich. On promise problems (a survey in memory of Shimon Even [1935\u2013\n2004]).\nElectronic Colloquium on Computational Complexity, Report TR05-018,\n2005.\n[GS89]\nS. Goldwasser and M. Sipser. Private coins versus public coins in interactive proof\nsystems. In S. Micali, editor, Randomness and Computation, volume 5 of Advances in\nComputing Research, pages 73\u201390. JAI Press, 1989.\n19\n[HKSZ08]\nS. Hallgren, A. Kolla, P. Sen, and S. Zhang. Making classical honest veri\ufb01er zero\nknowledge protocols secure against quantum attacks. In Proceedings of the 35th Inter-\nnational Colloquium on Automata, Languages and Programming, volume 5126 of Lecture\nNotes in Computer Science, pages 592\u2013603. Springer, 2008.\n[HO02]\nL. Hemaspaandra and M. Ogihara. The Complexity Theory Companion. Springer, 2002.\n[JUW09]\nR. Jain, S. Upadhyay, and J. Watrous. Two-message quantum interactive proofs are in\nPSPACE. In Proceedings of the 50th Annual IEEE Symposium on Foundations of Computer\nScience, 2009. To appear.\n[JW09]\nR. Jain and J. Watrous. Parallel approximation of non-interactive zero-sum quantum\ngames. In Proceedings of the 24th IEEE Conference on Computational Complexity, pages\n243\u2013253, 2009.\n[Kal07]\nS. Kale. Ef\ufb01cient algorithms using the multiplicative weights update method. PhD thesis,\nPrinceton University, 2007.\n[KKMV09]\nJ. Kempe, H. Kobayashi, K. Matsumoto, and T. Vidick. Using entanglement in quan-\ntum multi-prover interactive proofs. Computational Complexity, 18(2):273\u2013307, 2009.\n[KM03]\nH. Kobayashi and K. Matsumoto. Quantum multi-prover interactive proof systems\nwith limited prior entanglement. Journal of Computer and System Sciences, 66(3):429\u2013\n450, 2003.\n[Kob08]\nH. Kobayashi. General properties of quantum zero-knowledge proofs. In Proceedings\nof the Fifth IACR Theory of Cryptography Conference, volume 4948 of Lecture Notes in\nComputer Science, pages 107\u2013124. Springer, 2008.\n[KW00]\nA. Kitaev and J. Watrous. Parallelization, ampli\ufb01cation, and exponential time simu-\nlation of quantum interactive proof system. In Proceedings of the 32nd Annual ACM\nSymposium on Theory of Computing, pages 608\u2013617, 2000.\n[LFKN92]\nC. Lund, L. Fortnow, H. Karloff, and N. Nisan. Algebraic methods for interactive\nproof systems. Journal of the ACM, 39(4):859\u2013868, 1992.\n[ML03]\nC. Moler and C. Van Loan. Nineteen dubious ways to compute the exponential of a\nmatrix, twenty-\ufb01ve years later. SIAM Review, 45(1):3\u201349, 2003.\n[MW05]\nC. Marriott and J. Watrous. Quantum Arthur-Merlin games. Computational Complex-\nity, 14(2):122\u2013152, 2005.\n[NC00]\nM. A. Nielsen and I. L. Chuang. Quantum Computation and Quantum Information.\nCambridge University Press, 2000.\n[Nef94]\nC. A. Neff. Speci\ufb01ed precision polynomial root isolation is in NC. Journal of Computer\nand System Sciences, 48(3):429\u2013463, 1994.\n[Sha92]\nA. Shamir. IP = PSPACE. Journal of the ACM, 39(4):869\u2013877, 1992.\n[She92]\nA. Shen. IP = PSPACE: simpli\ufb01ed proof. Journal of the ACM, 39(4):878\u2013880, 1992.\n20\n[Wat99]\nJ. Watrous.\nPSPACE has constant-round quantum interactive proof systems.\nIn\nProceedings of the 40th Annual IEEE Symposium on Foundations of Computer Science,\npages 112\u2013119, 1999.\n[Wat09]\nJ. Watrous. Zero-knowledge against quantum attacks. SIAM Journal on Computing,\n39(1):25\u201358, 2009.\n[WK06]\nM. Warmuth and D. Kuzmin. Online variance minimization. In Proceedings of the\n19th Annual Conference on Learning Theory, volume 4005 of Lecture Notes in Computer\nScience, pages 514\u2013528. Springer, 2006.\n21\n",
        "sentence": " Its natural matrix extension, commonly known as matrix multiplicative weight update (MMWU) [26], has been used towards efficient algorithms for solving semidefinite programs [3, 10, 28], balanced separators [27], Ramanujan sparsifiers [7, 22], and even in the proof of QIP = PSPACE [19].",
        "context": "extend this result to all of QIP, establishing the relationship QIP = PSPACE. Similar to [JUW09],\nwe use the matrix multiplicative weights update method, together with parallel methods for ma-\ntrix computations.\nquantum interactive proof systems, is contained in PSPACE. That paper made use of a parallel\nalgorithm, based on a method known as the matrix multiplicative weights update method, to ap-\nparallelized form of the matrix multiplicative weights update method to a class of semide\ufb01nite\nprograms that captures the computational power of quantum interactive proofs. As the con-"
    },
    {
        "title": "Online pca with spectral bounds",
        "author": [
            "Zohar Karnin",
            "Edo Liberty"
        ],
        "venue": "In Proceedings of the 28th Annual Conference on Computational Learning Theory (COLT),",
        "citeRegEx": "20",
        "shortCiteRegEx": "20",
        "year": 2015,
        "abstract": "",
        "full_text": "",
        "sentence": " Some authors also study a very different online model for computing the top k eigenvectors[11, 20]: they are interested in outputting O(k \u00b7 poly(1/\u03b5)) vectors instead of k but with a good PCA reconstruction error.",
        "context": null
    },
    {
        "title": "Pca with gaussian perturbations",
        "author": [
            "Wojciech Kot  lowski",
            "Manfred K. Warmuth"
        ],
        "venue": "ArXiv e-prints,",
        "citeRegEx": "21",
        "shortCiteRegEx": "21",
        "year": 2015,
        "abstract": "Most of machine learning deals with vector parameters. Ideally we would like\nto take higher order information into account and make use of matrix or even\ntensor parameters. However the resulting algorithms are usually inefficient.\nHere we address on-line learning with matrix parameters. It is often easy to\nobtain online algorithm with good generalization performance if you\neigendecompose the current parameter matrix in each trial (at a cost of\n$O(n^3)$ per trial). Ideally we want to avoid the decompositions and spend\n$O(n^2)$ per trial, i.e. linear time in the size of the matrix data. There is a\ncore trade-off between the running time and the generalization performance,\nhere measured by the regret of the on-line algorithm (total gain of the best\noff-line predictor minus the total gain of the on-line algorithm). We focus on\nthe key matrix problem of rank $k$ Principal Component Analysis in\n$\\mathbb{R}^n$ where $k \\ll n$. There are $O(n^3)$ algorithms that achieve the\noptimum regret but require eigendecompositions. We develop a simple algorithm\nthat needs $O(kn^2)$ per trial whose regret is off by a small factor of\n$O(n^{1/4})$. The algorithm is based on the Follow the Perturbed Leader\nparadigm. It replaces full eigendecompositions at each trial by the problem\nfinding $k$ principal components of the current covariance matrix that is\nperturbed by Gaussian noise.",
        "full_text": "arXiv:1506.04855v2  [cs.LG]  23 Jul 2015\nPCA with Gaussian perturbations\nWojciech Kot lowski\nwkotlowski@cs.put.poznan.pl\nPozna\u00b4n University of Technology, Poland\nManfred K. Warmuth\nmanfred@cse.ucsc.edu\nUniversity of California, Santa Cruz\nAbstract\nMost of machine learning deals with vector parameters.\nIdeally we would like to take\nhigher order information into account and make use of matrix or even tensor parameters.\nHowever the resulting algorithms are usually ine\ufb03cient. Here we address on-line learning\nwith matrix parameters. It is often easy to obtain online algorithm with good generalization\nperformance if you eigendecompose the current parameter matrix in each trial (at a cost\nof O(n3) per trial). Ideally we want to avoid the decompositions and spend O(n2) per\ntrial, i.e. linear time in the size of the matrix data. There is a core trade-o\ufb00between\nthe running time and the generalization performance, here measured by the regret of the\non-line algorithm (total gain of the best o\ufb00-line predictor minus the total gain of the on-line\nalgorithm).\nWe focus on the key matrix problem of rank k Principal Component Analysis in Rn\nwhere k \u226an. There are O(n3) algorithms that achieve the optimum regret but require\neigendecompositions. We develop a simple algorithm that needs O(kn2) per trial whose\nregret is o\ufb00by a small factor of O(n1/4). The algorithm is based on the Follow the Perturbed\nLeader paradigm. It replaces full eigendecompositions at each trial by the problem \ufb01nding\nk principal components of the current covariance matrix that is perturbed by Gaussian\nnoise.\n1. Introduction\nIn Principal Component Analysis (PCA), the data points xt \u2208Rn are projected onto\na k-dimensional subspace (represented by a rank k projection matrix P ).\nThe goal is\nto maximize the total squared norm of the projected data points, P\nt \u2225P xt\u22252.\nThis is\nequivalent to \ufb01nding the principal eigenvectors u1, . . . , uk i.e.\nthose belonging to the k\nlargest eigenvalues of the data \u201ccovariance matrix\u201d P\nt xtx\u22a4\nt , and setting the projection\nmatrix to P = Pk\ni=1 uiu\u22a4\ni . In this paper we choose the online version of PCA (Warmuth\nand Kuzmin, 2008) as our paradigmatic matrix parameter problem and we explore the core\ntrade-o\ufb00between generalization performance and time e\ufb03ciency per trial for this problem.\nIn each trial t = 1, . . . , T, the online PCA algorithm chooses a projection matrix Pt of rank\nk based on the previously observed points x1, . . . , xt\u22121. Then a next point xt is revealed\nand the algorithm receives gain \u2225Ptxt\u22252. The goal here is to obtain an online algorithm\nwhose cumulative gain over trials t = 1, . . . , T is close to the cumulative gain of the best\nrank k projection matrix chosen in hindsight after seeing all T instances. The maximum\ndi\ufb00erence between the cumulative gain of the best o\ufb00-line comparator and the cumulative\ngain of the algorithm and is called the (worst-case) regret.\nc\u20dd.\nKot lowski and Warmuth\nIf you use the principal eigenvectors ui (1 \u2264i \u2264k) as the parameters, then the gain\nis formidably non-convex. However the key insight of (Warmuth and Kuzmin, 2006, 2008)\nis the observation that the seemingly quadratic gain \u2225P xt\u22252 is a linear function of the\nprojection matrix P when the data is expressed in terms of the matrix instance xtx\u22a4\nt\nrather than vector instance xt:\n\u2225P xt\u22252 = xtP 2xt\nP 2=P\n=\nxtP xt = tr(P xtx\u22a4\nt ).\nGood algorithms hedge their bets by predicting with a random projection matrix. In that\ncase E [\u2225P xt\u22252] becomes tr(E[P ] xtx\u22a4\ni ). Thus it is natural to use mixtures E[P ] of rank k\nprojection matrices as the parameter matrix of the algorithm. Such mixture are positive\nde\ufb01nite matrices of trace k with eigenvalues capped at 1. The gist is that the gain is now\nlinear in this alternate parameter matrix and the non-convexity has been circumvented.\nThis observation is the starting point for lifting known online learning algorithms for linear\ngain/loss on vector instances to the matrix domain, which resulted in the Matrix Expo-\nnentiated Gradient (MEG) algorithm (Tsuda et al., 2005; Arora and Kale, 2007; Warmuth\nand Kuzmin, 2008), as well as the (matrix) Gradient Descent (GD) algorithm (Arora et al.,\n2013, 2012; Jiazhong et al., 2013). Both algorithms are motivated by trading o\ufb00a Breg-\nman divergence against the gain, followed by a Bregman projection onto the convex hull\nof rank k projection matrices which is our parameter space. The worst-case regret of these\nalgorithms for online PCA is optimal (within constant factors). Furthermore MEG remains\noptimal for a generalization of the PCA problem to the dense instance case in which the\n\u201csparse\u201d rank one outer products xtx\u22a4\nt of vanilla PCA are generalized to positive de\ufb01nite\nmatrices Xt with bounded eigenvalues. (Jiazhong et al., 2013).\nUnfortunately, both algorithms require full eigendecomposition of the parameter matrix\nat a cost of O(n3) per trial.1 It was posed as an open problem (Hazan et al., 2010) whether\nthere exists an algorithm with good regret guarantees requiring time comparable with that\nof \ufb01nding the top k eigenvectors. The latter operation operation can be done e\ufb03ciently\nby means of e.g. power iteration based methods (Arnoldi, 1951; Cullum and Willoughby,\n1985): It essentially requires time O(kn2), which is much less than the cost of a full eigen-\ndecomposition in the natural case when k \u226an. This operation is also used by the simple\nFollow the Leader algorithm which predicts with the k principal components of the current\ncovariance matrix. This algorithm performs well when the data is i.i.d. but can be forced\nto have large regret on worst-case data.\nIn this paper, we provide an algorithm based on the Follow the Perturbed Leader (FPL)\napproach, which perturbs the cumulative data matrix by adding a random symmetric noise\nmatrix, and then predicts with the k principal components of the current perturbed co-\nvariance matrix. The key question is what perturbation to use and whether that exists a\nperturbation for which FPL achieves close to optimal regret. In the vanilla vector parameter\nbased FPL algorithm (Kalai and Vempala, 2005), exponentially distributed perturbation\nlead to optimal algorithms when the perturbations is properly scaled. We could apply the\nsame perturbations to the eigenvalues of the current parameter matrix and achieve optimal\nregret. However this approach requires us to eigendecompose the current parameter matrix\n1. For the rank one instances xtx\u22a4\nt of PCA the update of the eigenvalues takes O(n2) Bunch et al. (1978/79)\nper trial. However the update of the eigensystem remains O(n3).\n2\nPCA with Gaussian perturbations\nand this defeats the purpose. We need to \ufb01nd a perturbation that requires O(n2) time\nto compute instead of O(n3). We use a random symmetric Gaussian matrix (a so called\nGaussian orthogonal ensemble), which consists of entries generated i.i.d.\nfrom a Gaus-\nsian distribution. Our approach is more similar to the recent algorithm based on Random\nWalk Perturbation Devroye et al. (2013) and can be considered as a matrix generalization\nthereof, drawing connections to Random Matrix Theory Tao (2012). Calculation of our\nrandom noise matrix requires O(n2) and hence the total computational time is dominated\nby \ufb01nding the k principal components of the perturbed matrix. At the same time, our\nalgorithm achieves O(n1/4\u221a\nkT) worst-case regret for online PCA (sparse instances) and\nO(k\n\u221a\nnT) worst-case regret for dense instance case. Comparing to the minimax regrets\n\u0398(\n\u221a\nkT) and \u0398(k\u221aT log n) in the sparse and dense cases, respectively, we are only a factor\nof O(n1/4) and O(n1/2) o\ufb00from the optimum, respectively.\nOur approach can be considered a generalization of the Random Walk Perturbation\n(RWP) algorithm Devroye et al. (2013) to the matrix domain. In RWP, an independent\nBernoulli coin \ufb02ip is added to each component of the loss/gain vector, the process which\ncan be closely approximated (through Central Limit Theorem) by Gaussian perturbations\nwith variance growing linearly in t. This is also the case of our algorithm, where we use a\nsymmetric matrix with i.i.d. Gaussian-distributed entries with variance also growing lin-\nearly in t. Our analysis, however, resorts to properties of random matrices, e.g. expected\nmaximum eigenvalue, which leads to worse regret bounds than in the vector case. Inter-\nestingly, comparing to Devroye et al. (2013) we get rid of additional O(log T) factor in the\nregret.\nRelated work.\nOnline PCA, in the framework considered here, was introduced in Tsuda\net al. (2005) and independently in Arora and Kale (2007), along with Matrix Exponentiated\nGradient algorithm. The problem of \ufb01nding e\ufb03cient algorithms which avoid full eigende-\ncomposition was posed as an open problem by Hazan et al. (2010). An e\ufb03cient algorithm\nfor PCA based on Online Gradient Descent was proposed in Arora et al. (2013, 2012), but\nthe main version of the algorithm (Matrix Stochastic Gradient, MSG) still requires O(n3)\nin the worst case, while a faster version (Capped MSG) operates on low-rank deterministic\nparameter matrix, which can be shown to have regret linear in T in the adversarial setting.\nThe most closely related to our work is Garber et al. (2015), in which several algorithms\nare proposed for learning the top eigenvector (i.e., the simplest case k = 1 of online PCA):\nbased on online Franke-Wolfe method Hazan and Kale (2012), and based on FPL approach\nwith entry-wise uniform perturbation, exponentially-distributed perturbation, and a per-\nturbation based on a sparse rank-one random matrix vv\u22a4composed of a random Gaussian\nvector v. Except for the last algorithm, all the other approaches have regret guarantees\nwhich are inferior comparing to our method, either in terms of dependence on T or depen-\ndence on n, or both. The method based on rank-one perturbation achieves regret bound\nO(\n\u221a\nnT) which is the same as ours in the dense instance case. It is not clear whether this\nmethod would bene\ufb01t anyhow from sparsity of instance matrices (as in the standard online\nPCA), and whether it would easily generalize to k > 1 case.\nThere are other formulations of online PCA problem. For instance, Boutsidis et al.\n(2015) aims at \ufb01nding low dimensional data representation in a single pass, with the goal\nof good reconstruction guarantees using only a small number of dimensions. On the other\n3\nKot lowski and Warmuth\nhand, Balsubramani et al. (2013); Shamir (2015) consider online PCA in the stochastic\noptimization setting. However, the algorithm considered therein are not directly applicable\nin the adversarial setting studied in this work.\nNew conjecture.\nOur algorithm is e\ufb03cient, but its suboptimal regret is due to the fact\nthat the noise matrix does not adapt to the eigensystem of cumulative covariance matrix.\nWhat O(n2) perturbation can we use that does adapt to the eigensystem of the current\ncovariance matrix? A clear candidate is to use Dropout Perturbation. In the vector case\nthis perturbation independently at random zeros out each component of the gain/loss vector\nin each trial (van Erven et al., 2014) and achieves optimal regret without having to tune the\nmagnitude of the perturbations. In the matrix case it would be natural to independently\nzero out components of the instance matrix when expressed in the eigensystem of the current\nloss matrix. However these approaches again require eigendecompositions.\nA new variant is to skip at trial t the entire instance matrix with probability half, i.e. at\ntrial t predict with the k principal components of the following perturbed current covariance\nmatrix\nt\u22121\nX\nq=1\n\u03b1tXt,\nwhere the \u03b1t are Bernoilli coin \ufb02ips with probability half.\nWe call this the Follow the\nSkipping Leader algorithm because it skips entire instances Xt with probability half. It\nis easy to maintain this perturbed covariance matrix in O(n2) time per trial. However,\nunfortunately already in the vector parameter case this algorithm can be forced to have a\ngravely suboptimal linear regret in n (Neu and Lugosi, 2014). The counter example requires\ndense loss vectors. When lifting this counter example to the matrix setting then the regret\ncan still be forced to be linear with sparse instance xtx\u22a4\nt . However we conjecture that the\ntime e\ufb03cient Follow the Skipping Leader algorithm achieves the optimal regret for standard\nPCA with sparse instance. This is because in PCA regret is naturally measured w.r.t. the\nmaximum gain of the best rank k subspace and not the loss. Note that that this type of\nproblem is decidedly not symmetric w.r.t. gain and loss (See Jiazhong et al. (2013) for an\nextended discussion).\nFinally we also conjecture the regret bounds achieved by the algorithm of this paper\n(Gaussian perturbations) is the best you can achieve with rotation invariant noise and\nknowing this fact would be interesting in its own right.\n2. Problem setting\nIn the online PCA, in each trial t = 1, . . . , T, the algorithm probabilistically chooses a\nprojection matrix Pt \u2208Rn\u00d7n of rank k. Then a point xt \u2208Rn is revealed and the algorithm\nreceives gain \u2225Ptxt\u22252 = tr(Ptxtx\u22a4\nt ).\nNote again that the gain is linear in Pt and in\ninstance matrix xtx\u22a4\nt . This observation calls for generalization of the online PCA in which\nthe instance matrix is any positive de\ufb01nite matrix Xt (with bounded eigenvalues) and the\ngain becomes tr(PtXt). We call this the dense instance case as opposed to standard online\nPCA, which we call sparse instance case.\nIn the above protocol, the algorithm is allowed to choose its k dimensional subspace Pt\nprobabilistically. Therefore we use expected gain E[tr(PtXt)] as the evaluation of the algo-\n4\nPCA with Gaussian perturbations\nrithm\u2019s performance, where the expectation is with respect to the internal randomization\nof the algorithm. The regret of the algorithm is then the di\ufb00erence between the cumula-\ntive gain of the best o\ufb00-line rank k projector and the the cumulative gain of the algorithm\n(due to linearity of gain, no randomization is necessary when considering the best o\ufb00-line\ncomparator):\nR = max\nP \u2208P\n\uf8f1\n\uf8f2\n\uf8f3\nT\nX\nt=1\ntr(P Xt)\n\uf8fc\n\uf8fd\n\uf8fe\u2212\nT\nX\nt=1\nE[tr(PtXt)] = \u03bb1:k\n\u0000X\u2264T\n\u0001\n\u2212\nT\nX\nt=1\nE[tr(PtXt)],\n(1)\nwhere P denotes the set of all rank k projectors, X\u2264t = Pt\nq=1 Xq is the cumulative data\nmatrix, and \u03bb1:k(X) = Pk\ni=1 \u03bbi(X) denotes the sum of top k eigenvalues of X. The goal\nof the algorithm is to have small regret for any sequence of instance matrices. Since the\nregret naturally scales with the eigenvalues of Xt, we assume for the sake of simplicity that\nall eigenvalues of Xt are bounded by 1 (i.e. for each t = 1, . . . , T, the spectral norm of Xt,\n\u2225Xt\u2225\u221e\u22641).\nWe note that due to linearly of the gain, E[tr(PtXt)] = tr(E[Pt]Xt), so that the algo-\nrithm\u2019s gain is fully determined by E[Pt], a convex combinations of rank k projection matri-\nces. Hence, the parameter set of the algorithm can be equivalently taken as W = conv(P),\na convex hull of P, which is a set of positive de\ufb01nite matrices with trace k and all eigenvalues\nnot larger than 1 Warmuth and Kuzmin (2008). This is the key idea behind MEG and GD\nalgorithms, which maintain the uncertainty about projection matrix by means of a param-\neter Wt \u2208W, update their parameter by minimizing a trade-o\ufb00between a divergence of\nthe new and old parameter and the gain/loss of the new parameter on the current instance,\nwhile constraining the new parameter to lie in the parameter set W. While predicting, the\nalgorithm chooses its projection matrix Pt by sampling from this mixture Wt Warmuth\nand Kuzmin (2008).\n3. The algorithm\nOur algorithm belongs to a class of Follow the Perturbed Leader (FPL) algorithms, which\nare de\ufb01ned by the choice:\nPt = argmax\nP \u2208P\n\b\ntr(P (X<t + Nt))\n\t\n,\nwhere X<t = P\nq<t Xq is the cumulative data matrix observed so far, while Nt is the\nsymmetric noise matrix generated randomly by the algorithm2 and w.l.o.g.\nwe assume\nE[Nt] = 0. Perturbing the cumulative data matrix is necessary as one can easily show that\nany deterministic strategy (including Follow the Leader obtained by taking Nt = 0) can be\nforced to have regret linear in T.\nDe\ufb01ne a \u201cfake\u201d prediction strategy:\ne\nPt = argmax\nP \u2208P\n\b\ntr(P (X\u2264t + Nt))\n\t\n,\n2. Note that if the algorithm plays against oblivious adversary, it is allowed to generate the noise matrix\nonce in the \ufb01rst trial and then reuse it throughout the game.\n5\nKot lowski and Warmuth\nwhich acts as FPL, but adds the current instance Xt to the cumulative data matrix, and\nhence does not corresponds to any valid online algorithm. What follows is a standard lemma\nfor bounding the FPL regret, adapted to the matrix case:\nLemma 1 We have:\nR \u2264\nX\nt\nE\nh\ntr(( e\nPt \u2212Pt)Xt)\ni\n+\nX\nt\nE\n\u0002\n\u03bb1:k(Nt \u2212Nt\u22121)\n\u0003\n,\nA vector version of Lemma 1 can be \ufb01nd in standard textbooks on online learning (see, e.g.,\nCesa-Bianchi and Lugosi (2006)). Since adaptation to the matrix case is rather straightfor-\nward, we defer the proof to the Appendix.\nWe now specify the noise matrix our algorithm employs. Let G be an n\u00d7n matrix such\nthat each entry is generated i.i.d. from a Gaussian distribution, i.e. Gij \u223cN(0, \u03c32). We\nde\ufb01ne the noise matrix of our algorithm as:\nNt =\n\u221a\ntN,\nwhere N = 1\n2\n\u0010\nG + G\u22a4\u0011\n.\nNote that N is a symmetrized version of G, and Nt multiplies N by\n\u221a\nt so that the variance\nof each entry in Nt grows linearly in t. Interestingly, distribution of N is known as Gaussian\northogonal ensemble in the Random Matrix Theory Tao (2012). The algorithm uses the\nvariable noise rate and hence does not require any tuning for the time horizon T. We still\nhave a single parameter \u03c32, but that parameter is only chosen based on the sparseness of\nthe instance matrix.\nNote that according to rules for summing Gaussian variables, we can also express Nt as a\nsum of t independent copies of N, Nt = Pt\nq=1 N(q). We thus get an equivalent picture of our\nalgorithm in which in each trial, an independent noise variable N(t) generated from a \ufb01xed\ndistribution is added to the current data instance Xt, and then the action of the algorithm\nis based on the sum of perturbed data instances. This pictures makes our approach similar\nto RWP algorithm and let us relate our algorithm to dropout perturbation in the next\nsection.\nWe now show the main result of this paper, the regret bound of the algorithm based on\nGaussian perturbation.\nTheorem 2 Given the choice of the noise matrix Nt described above,\n\u2022 For dense instance, setting \u03c32 = 1 gives:\nR \u22642k\n\u221a\nnT.\n\u2022 For sparse instance, setting \u03c32 =\n1\nk\u221an gives:\nR \u22642n1/4\u221a\nkT.\nProof\nWe apply Lemma 1 and bound both sums on the right hand side separately. We\nstart with the second sum. We have:\nX\nt\nE\n\u0002\n\u03bb1:k(Nt \u2212Nt\u22121)\n\u0003\n=\nX\nt\n(\n\u221a\nt \u2212\n\u221a\nt \u22121)E\n\u0002\n\u03bb1:k(N)\n\u0003\n=\n\u221a\nTE\n\u0002\n\u03bb1:k(N)\n\u0003\n6\nPCA with Gaussian perturbations\nIt follows from Random Matrix Theory (see, e.g., Davidson and Szarek (2001)) that the\nlargest eigenvalue of a matrix generated from a Gaussian orthogonal ensemble is of order\nO(\u221an), speci\ufb01cally:\nE\n\u0002\n\u03bbmax(N)\n\u0003\n\u2264\n\u221a\nn\u03c32.\nTherefore,\nE\n\u0002\n\u03bb1:k(N)\n\u0003\n\u2264kE\n\u0002\n\u03bbmax(N)\n\u0003\n\u2264k\n\u221a\nn\u03c32,\nso that the second sum is bounded by k\n\u221a\nnT\u03c32.\nLet us now bound the \ufb01rst sum.\nFirst, note that Nij \u223cN(0, \u03c32/2) for i \u0338= j and\nNii \u223cN(0, \u03c32). This means that the joint density p(N) = p(N11, . . . , Nnn) is proportional\nto:\np(N) \u221dexp\n\u001a\n\u2212\n1\n2\u03c32\n\u0012 X\ni>j\n2N2\nij +\nX\ni\nN2\nii\n\u0013\u001b\n= exp\n\u001a\n\u22121\n2\u03c32 tr(N2)\n\u001b\n.\nSimilarly, the joint density of Nt is proportional to:\npt(Nt) \u221dexp\n\u001a\n1\n2t\u03c32 tr(N2\nt )\n\u001b\n.\nFor any symmetric matrix A, de\ufb01ne:\nP (A) = argmax\nP \u2208P\n\b\ntr(P A)\n\t\n,\nso that Pt = P (X<t + Nt) and e\nPt = P (X<t + Nt + Xt). Furthermore, de\ufb01ne a function:\nft(s) = E\n\u0002\ntr(P (X<t + Nt + sXt) Xt)\n\u0003\n.\nNote that ft(0) = E\n\u0002\ntr(PtXt)\n\u0003\nand ft(1) = E\nh\ntr( e\nPtXt)\ni\n. In this notation,\nX\nt\nE\nh\ntr(( e\nPt \u2212Pt)Xt)\ni\n=\nX\nt\n(ft(1) \u2212ft(0)),\nso it remains to bound ft(1) \u2212ft(0) for all t. We have:\nft(s) =\nZ\ntr(P (X<t + Nt + sXt) Xt)pt(Nt) dNt\n=\nZ\ntr(P (X<t + Nt) Xt)pt(Nt \u2212sXt) dNt,\nwhich follows from changing the integration variable from Nt to Nt\u2212sXt. Since by H\u00a8older\u2019s\ninequality:\ntr(P (X<t + Nt) Xt) \u2264tr(P (X<t + Nt)) \u00b7 \u2225Xt\u2225\u221e= k\u2225Xt\u2225\u221e\u2264k,\n(2)\nand since pt is the density of Gaussian distribution, it can easily be shown by using standard\nargument based on Dominated Convergence Theorem that one can replace the order of\n7\nKot lowski and Warmuth\ndi\ufb00erentiation w.r.t. s and integration w.r.t. Nt. This means that ft(s) is di\ufb00erentiable\nand:\nf \u2032\nt(s) =\nZ\ntr(P (X<t + Nt) Xt)dpt(Nt \u2212sXt)\nds\ndNt\n=\n1\nt\u03c32\nZ\ntr(P (X<t + Nt) Xt) tr((Nt \u2212sXt)Xt)pt(Nt \u2212sXt) dNt\n=\n1\nt\u03c32\nZ\ntr(P (X<t + Nt + sXt) Xt) tr(NtXt)pt(Nt) dNt\n\u2264\nr\nt\u03c32\nZ \u0000tr(NtXt)\n\u0001\n+ pt(Nt) dNt,\nwhere (c)+ = max{c, 0}, and r = k in the dense instance case, while r = 1 in the sparse\ninstance case.\nThe last inequality follows from the same argument as in (2) when the\ninstances are dense, and from the opposite application of H\u00a8older\u2019s inequality when the\ninstances are sparse, i.e. for sparse instance Xt = xtx\u22a4\nt with \u2225xt\u2225= 1, and any P :\ntr(P xtx\u22a4\nt ) \u2264tr(xtx\u22a4\nt ) \u00b7 \u2225P \u2225\u221e= \u2225xt\u22252 \u00b7 1 = 1.\nDenote:\nz = tr(NtXt) = 2\nX\ni>j\n(Nt)ij(Xt)ij +\nX\ni\n(Nt)ii(Xt)ii.\nUsing summation rules for Gaussian variables:\nz \u223cN\n\uf8eb\n\uf8ed0, 2t\u03c32 X\ni>j\n(Xt)2\nij + t\u03c32 X\ni\n(Xt)2\nii\n\uf8f6\n\uf8f8= N\n\u0010\n0, t\u03c32 tr(X2)\n\u0011\n,\nso that:\nf \u2032\nt(s) \u2264\nr\nt\u03c32\nZ\n(tr(NtXt))+pt(Nt) dNt\n=\nr\nt\u03c32 Ez\u223cN (0,t\u03c32 tr(X2))[(z)+]\n=\nr\n\u221a\nt\u03c32\np\ntr(X2) Ez\u223cN (0,1)[(z)+]\n=\nr\n\u221a\n2\u03c0t\u03c32\np\ntr(X2).\nBy the mean value theorem,\nft(1) \u2212ft(0) = f \u2032\nt(s),\nfor some s \u2208[0, 1],\nwhich implies:\nE\nh\ntr(( e\nPt \u2212Pt)Xt)\ni\n\u2264\nr\n\u221a\n2\u03c0t\u03c32\nq\ntr(X2\nt ).\nSumming over trials and using PT\nt=1 1/\n\u221a\nt \u22642\n\u221a\nT, we bound the \ufb01rst sum in Lemma 1 by\nr\nq\n2T maxt tr(X2\nt )\n\u03c0\u03c32\n. Using the bound on the second sum, we get that:\nR \u2264r\nr\n2T maxt tr(X2\nt )\n\u03c0\u03c32\n+ k\n\u221a\nnT\u03c32.\n8\nPCA with Gaussian perturbations\nThe proof is \ufb01nished by noticing that tr(X2) \u2264n and r = k for dense instances, while\ntr(X2) = 1 and k = 1 for sparse instances.\nComparing the regret with values of the minimax regret \u0398(\n\u221a\nkT) in the standard online\nPCA setting (sparse instance case), and \u0398(k\u221aT log n) in the dense instance case Jiazhong\net al. (2013), we see that the algorithm presented here is suboptimal by a factor of O(n1/4)\nin the online PCA setting, and by a factor of O(\u221an/ log n) in the dense instances setting.\n4. Conclusions\nIn this paper, we studied the online PCA problem and its generalization to the case of\ndense instance matrices. While there are algorithms which essentially achieve the minimax\nregret, such as Matrix Exponentiated Gradient or (Matrix) Gradient Descent, all these\nmethods take O(n3) per trial, because they require full eigendecomposition of the data\nmatrix. We proposed an algorithm based on Follow the Perturbed Leader approach, which\nuses as a perturbation a random symmetric Gaussian matrix. The algorithm avoids full\neigendecomposition and only requires calculating the top k eigenvectors. Hence, prediction\ntakes O(kn2), while the algorithm achieves the worst-case regret which is only O(n1/4) close\nto the minimax regret for standard online PCA setting, and O(\u221an) close to minimax regret\nfor generalization of online PCA to a dense instance matrices. Finally, we raised an open\nquestion, whether a more adaptive version of our algorithm, based on dropout perturbation,\nwould achieve the minimax regret.\nAcknowledgments\nWojciech Kot lowski was supported by the Polish National Science Cente grant\n2013/11/D/ST6/03050.\nAppendix A. Proof of Lemma 1\nWe have:\n\u03bb1:k(X\u2264t + Nt) = max\nP \u2208P\n\b\ntr(P (X\u2264t + Nt))\n\t\n= tr( e\nPt(X\u2264t + Nt))\n= tr( e\nPt(Xt + Nt \u2212Nt\u22121)) + tr( e\nPt(X<t + Nt\u22121))\n\u2264tr( e\nPt(Xt + Nt \u2212Nt\u22121)) + \u03bb1:k(X<t + Nt\u22121),\nso that:\n\u03bb1:k(X\u2264t + Nt) \u2212\u03bb1:k(X<t + Nt\u22121) \u2264tr( e\nPt(Xt + Nt \u2212Nt\u22121)).\nSumming over trials t = 1, . . . , T, the terms on the left-hand side telescope and, de\ufb01ning\nN0 = 0, we get:\n\u03bb1:k(X\u2264T + NT ) \u2264\nT\nX\nt=1\ntr( e\nPt(Xt + Nt \u2212Nt\u22121)).\n9\nKot lowski and Warmuth\nSince \u03bb1:k(\u00b7) is convex as a maximum over linear functions, Jensen\u2019s inequality implies\n\u03bb1:k(X\u2264T ) \u2264E\n\u0002\n\u03bb1:k(X\u2264T + NT )\n\u0003\nand hence:\n\u03bb1:k(X\u2264T ) \u2264\nT\nX\nt=1\nE\nh\ntr( e\nPt(Xt + Nt \u2212Nt\u22121))\ni\n\u2264\nT\nX\nt=1\nE\nh\ntr( e\nPtXt)\ni\n+\nT\nX\nt=1\nE\n\u0014\nmax\nP \u2208P {Nt \u2212Nt\u22121}\n\u0015\n=\nT\nX\nt=1\nE\nh\ntr( e\nPtXt)\ni\n+\nT\nX\nt=1\nE\n\u0002\n\u03bb1:k(Nt \u2212Nt\u22121)\n\u0003\n.\nThe lemma follows by plugging the inequality above into the de\ufb01nition of the regret (1).\nReferences\nWalter E. Arnoldi.\nThe principle of minimized iterations in the solution of the matrix\neigenvalue problem. Quarterly of Applied Mathematics, 9:17\u201329, 1951.\nRaman Arora, Andrew Cotter, Karen Livescu, and Nathan Srebro. Stochastic optimization\nfor PCA and PLS. In 2012 50th Annual Allerton Conference on Communication, Control,\nand Computing, pages 861\u2013868, 2012.\nRaman Arora, Andrew Cotter, and Nati Srebro.\nStochastic optimization of PCA with\ncapped MSG. In NIPS, pages 1815\u20131823, 2013.\nSanjeev Arora and Satyen Kale.\nA combinatorial, primal-dual approach to semide\ufb01nite\nprograms. In STOC, pages 227\u2013236. ACM, 2007.\nAkshay Balsubramani, Sanjoy Dasgupta, and Yoav Freund. The fast convergence of incre-\nmental PCA. In NIPS, pages 3174\u20133182, 2013.\nChristos Boutsidis, Dan Garber, Zohar Shay Karnin, and Edo Liberty. Online Principal\nComponents Analysis. In SODA, pages 887\u2013901, 2015.\nJames R. Bunch, Christopher P. Nielsen, and Danny C. Sorensen. Numerische Mathematik,\n31(1):31\u201348, 1978/79.\nNicol`o Cesa-Bianchi and G\u00b4abor Lugosi. Prediction, learning, and games. Cambridge Uni-\nversity Press, 2006. ISBN 978-0-521-84108-5.\nJane K. Cullum and Ralph A. Willoughby. Lanczos Algorithms for Large Symmetric Eigen-\nvalue Computations. Cambridge University Press, 1985.\nKenneth R. Davidson and Stanislaw J. Szarek. Local operator theory, random matrices and\nBanach spaces. In Handbook of the geometry of Banach spaces. Volume 1, pages 317\u2013366.\nElsevier, North-Holland, Amsterdam, 2001.\nLuc Devroye, G\u00b4abor Lugosi, and Gergely Neu. Prediction by random-walk perturbation.\nIn COLT, pages 460\u2013473, 2013.\n10\nPCA with Gaussian perturbations\nDan Garber, Elad Hazan, and Tengyu Ma. Online learning of eigenvectors. In ICML, 2015.\nElad Hazan and Satyen Kale. Projection-free online learning. In ICML, 2012.\nElad Hazan, Satyen Kale, and Manfred K. Warmuth. On-line variance minimization in\nO(n2) per trial? In COLT, pages 314\u2013315. Omnipress, 2010. open problem.\nNie Jiazhong, Wojciech Kot lowski, and Manfred K. Warmuth. Online PCA with optimal\nregrets. In ALT, volume 8139 of LNCS, pages 98\u2013112. Springer, 2013.\nAdam Tauman Kalai and Santosh Vempala. E\ufb03cient algorithms for online decision prob-\nlems. J. Comput. Syst. Sci., 71(3):291\u2013307, 2005.\nGergely Neu and G\u00b4abor Lugosi. Private communication, 2014.\nOhad Shamir. A stochastic PCA and SVD algorithm with an exponential convergence rate.\nIn ICML, 2015.\nTerence Tao. Topics in Random Matrix Theory. American Mathematical Society, 2012.\nKoji Tsuda, Gunnar R\u00a8atsch, and Manfred K. Warmuth. Matrix exponentiated gradient\nupdates for on-line learning and Bregman projections.\nJournal of Machine Learning\nResearch, 6:995\u20131018, 2005.\nTim van Erven, Wojciech Kot lowski, and Manfred K. Warmuth. Follow the leader with\ndropout perturbations. In COLT, pages 949\u2013974, 2014.\nManfred K. Warmuth and Dima Kuzmin. Online variance minimization. In COLT, pages\n514\u2013528, 2006.\nManfred K. Warmuth and Dima Kuzmin. Randomized online PCA algorithms with regret\nbounds that are logarithmic in the dimension. Journal of Machine Learning Research, 9:\n2287\u20132320, 2008.\n11\n",
        "sentence": " If instead of playing an arbitrary matrix in \u2206d, the player is only allowed to play a rank-1 matrix Wk = wkw > k , then this online matrix optimization becomes the well-known online eigenvector problem [2, 13, 16, 21, 25]: Many researchers also analyzed the so-called followthe-perturbed-leader (FTPL) strategy for this problem [2, 13, 16, 21]. [13] and independently shown by Kot lowski and Warmuth [21].",
        "context": "algorithm chooses its projection matrix Pt by sampling from this mixture Wt Warmuth\nand Kuzmin (2008).\n3. The algorithm\nOur algorithm belongs to a class of Follow the Perturbed Leader (FPL) algorithms, which\nare de\ufb01ned by the choice:\nPt = argmax\nP \u2208P\n\b\n5\nKot lowski and Warmuth\nwhich acts as FPL, but adds the current instance Xt to the cumulative data matrix, and\nhence does not corresponds to any valid online algorithm. What follows is a standard lemma\nwe assume\nE[Nt] = 0. Perturbing the cumulative data matrix is necessary as one can easily show that\nany deterministic strategy (including Follow the Leader obtained by taking Nt = 0) can be\nforced to have regret linear in T."
    },
    {
        "title": "Constructing linear-sized spectral sparsification in almost-linear time",
        "author": [
            "Yin Tat Lee",
            "He Sun"
        ],
        "venue": "In FOCS,",
        "citeRegEx": "22",
        "shortCiteRegEx": "22",
        "year": 2015,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Its natural matrix extension, commonly known as matrix multiplicative weight update (MMWU) [26], has been used towards efficient algorithms for solving semidefinite programs [3, 10, 28], balanced separators [27], Ramanujan sparsifiers [7, 22], and even in the proof of QIP = PSPACE [19]. This more challenging setting is very desirable for multiple reasons: \u2022 in many applications \u2014such as graph problems [7, 22]\u2014 Ak does not depend on wk; \u2022 vector-based strategies wk can be cheaper to compute and more efficient to communicate. 4 Some researchers [3, 7, 22, 28] use the Johnson-Lindenstrauss (JL) compression to reduce the dimension of Wk to make it more efficiently computable.",
        "context": null
    },
    {
        "title": "A Universal Catalyst for First-Order Optimization",
        "author": [
            "Hongzhou Lin",
            "Julien Mairal",
            "Zaid Harchaoui"
        ],
        "venue": null,
        "citeRegEx": "23",
        "shortCiteRegEx": "23",
        "year": 2015,
        "abstract": "",
        "full_text": "",
        "sentence": " Then, using the Catalyst/APPA acceleration scheme [14, 23], the above running time can be improved to \u00d5 ( nnz(\u03a3k\u22121) + \u221a \u03b7k \u00b7maxi\u2208[k\u22121]{nnz(\u03a3k\u22121)nnz(Ai)} ) .",
        "context": null
    },
    {
        "title": "Introductory Lectures on Convex Programming Volume: A Basic course, volume I",
        "author": [
            "Yurii Nesterov"
        ],
        "venue": "Kluwer Academic Publishers,",
        "citeRegEx": "24",
        "shortCiteRegEx": "24",
        "year": 2004,
        "abstract": "",
        "full_text": "",
        "sentence": " 1), so one can apply conjugate gradient [31] or Nesterov\u2019s accelerated gradient descent [24] to minimize this objective.",
        "context": null
    },
    {
        "title": "Online pca with optimal regrets",
        "author": [
            "Jiazhong Nie",
            "Wojciech Kot  lowski",
            "Manfred K Warmuth"
        ],
        "venue": "In International Conference on Algorithmic Learning Theory,",
        "citeRegEx": "25",
        "shortCiteRegEx": "25",
        "year": 2013,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " If instead of playing an arbitrary matrix in \u2206d, the player is only allowed to play a rank-1 matrix Wk = wkw > k , then this online matrix optimization becomes the well-known online eigenvector problem [2, 13, 16, 21, 25]: studied the high-rank variant using MMWU [25], but their per-iteration running time is still O(d3) due to eigendecomposition.",
        "context": null
    },
    {
        "title": "Fast Approximation Algorithms for Graph Partitioning using Spectral and Semidefinite-Programming Techniques",
        "author": [
            "Lorenzo Orecchia"
        ],
        "venue": "PhD thesis, EECS Department,",
        "citeRegEx": "26",
        "shortCiteRegEx": "26",
        "year": 2011,
        "abstract": "",
        "full_text": "",
        "sentence": " Matrix multiplicative weight update (MMWU) [26] is an extremely powerful algorithmic tool for computer science and related fields. Its natural matrix extension, commonly known as matrix multiplicative weight update (MMWU) [26], has been used towards efficient algorithms for solving semidefinite programs [3, 10, 28], balanced separators [27], Ramanujan sparsifiers [7, 22], and even in the proof of QIP = PSPACE [19]. The best choice \u03b7 = \u221a log d/ \u221a T yields a total regret at most O( \u221a T log d) [26], and this is optimal up to constant [9].",
        "context": null
    },
    {
        "title": "Approximating the exponential, the lanczos method and an \u00d5(m)-time spectral algorithm for balanced separator",
        "author": [
            "Lorenzo Orecchia",
            "Sushant Sachdeva",
            "Nisheeth K. Vishnoi"
        ],
        "venue": null,
        "citeRegEx": "27",
        "shortCiteRegEx": "27",
        "year": 2012,
        "abstract": "",
        "full_text": "",
        "sentence": " Its natural matrix extension, commonly known as matrix multiplicative weight update (MMWU) [26], has been used towards efficient algorithms for solving semidefinite programs [3, 10, 28], balanced separators [27], Ramanujan sparsifiers [7, 22], and even in the proof of QIP = PSPACE [19].",
        "context": null
    },
    {
        "title": "Faster and simpler width-independent parallel algorithms for positive semidefinite programming",
        "author": [
            "Richard Peng",
            "Kanat Tangwongsan"
        ],
        "venue": null,
        "citeRegEx": "28",
        "shortCiteRegEx": "28",
        "year": 2012,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Its natural matrix extension, commonly known as matrix multiplicative weight update (MMWU) [26], has been used towards efficient algorithms for solving semidefinite programs [3, 10, 28], balanced separators [27], Ramanujan sparsifiers [7, 22], and even in the proof of QIP = PSPACE [19]. MMWU with JL [7, 28] \u00d5( \u221a T ) \u00d5 ( T 5 4 nnz(\u03a3) ) \u00d5 ( 1 \u03b54. 4 Some researchers [3, 7, 22, 28] use the Johnson-Lindenstrauss (JL) compression to reduce the dimension of Wk to make it more efficiently computable. MMWU with JL [7, 28] \u00d5( \u221a \u03bbT ) \u00d5 ( T 5 4 \u03bb\u2212 3 4 nnz(\u03a3) ) \u00d5 ( \u03bb \u03b54.",
        "context": null
    },
    {
        "title": "SDCA without Duality, Regularization, and Individual Convexity",
        "author": [
            "Shai Shalev-Shwartz"
        ],
        "venue": "In ICML,",
        "citeRegEx": "29",
        "shortCiteRegEx": "29",
        "year": 2016,
        "abstract": "Stochastic Dual Coordinate Ascent is a popular method for solving regularized\nloss minimization for the case of convex losses. We describe variants of SDCA\nthat do not require explicit regularization and do not rely on duality. We\nprove linear convergence rates even if individual loss functions are\nnon-convex, as long as the expected loss is strongly convex.",
        "full_text": "arXiv:1602.01582v2  [cs.LG]  21 May 2016\nSDCA without Duality, Regularization, and Individual Convexity\nShai Shalev-Shwartz\nSHAIS@CS.HUJI.AC.IL\nSchool of Computer Science and Engineering, The Hebrew University of Jerusalem, Israel\nAbstract\nStochastic Dual Coordinate Ascent is a popular\nmethod for solving regularized loss minimization\nfor the case of convex losses. We describe vari-\nants of SDCA that do not require explicit regu-\nlarization and do not rely on duality. We prove\nlinear convergence rates even if individual loss\nfunctions are non-convex, as long as the expected\nloss is strongly convex.\n1. Introduction\nWe consider the following loss minimization problem:\nmin\nw\u2208Rd F(w) := 1\nn\nn\nX\ni=1\nfi(w) .\nAn important sub-class of problems is when each fi can\nbe written as fi(w) = \u03c6i(w) + \u03bb\n2 \u2225w\u22252, where \u03c6i is Li-\nsmooth and convex. A popular method for solving this\nsub-class of problems is Stochastic Dual Coordinate As-\ncent (SDCA), and (Shalev-Shwartz & Zhang, 2013) estab-\nlished the convergence rate of \u02dcO((Lmax/\u03bb + n) log(1/\u01eb)),\nwhere Lmax = maxi Li.\nAs its name indicates, SDCA is derived by considering a\ndual problem. In this paper, we consider the possibility\nof applying SDCA for problems in which individual fi do\nnot necessarily have the form \u03c6i(w) + \u03bb\n2 \u2225w\u22252, and can\neven be non-convex (e.g., deep learning optimization prob-\nlems, or problems arising in fast calculation of the top sin-\ngular vectors (Jin et al., 2015)). In many such cases, the\nProceedings of the 33 rd International Conference on Machine\nLearning, New York, NY, USA, 2016. JMLR: W&CP volume\n48. Copyright 2016 by the author(s).\ndual problem is meaningless. Instead of directly using the\ndual problem, we describe and analyze a variant of SDCA\nin which only gradients of fi are being used. Following\n(Johnson & Zhang, 2013), we show that SDCA is a mem-\nber of the Stochastic Gradient Descent (SGD) family of al-\ngorithms, that is, its update is based on an unbiased esti-\nmate of the gradient, but unlike the vanilla SGD, for SDCA\nthe variance of the estimation of the gradient tends to zero\nas we converge to a minimum.\nOur analysis assumes that F is \u03bb-strongly convex and each\nfi is Li-smooth.\nWhen each fi is also convex we es-\ntablish the convergence rate of \u02dcO(\u00afL/\u03bb + n), where \u00afL is\nthe average of Li and the \u02dcO notation hides logarithmic\nterms, including the factor log(1/\u01eb). This matches the best\nknown bound for SVRG given in (Xiao & Zhang, 2014).\nLower bounds have been derived in (Arjevani et al., 2015;\nAgarwal & Bottou, 2014). Applying an acceleration tech-\nnique ((Shalev-Shwartz & Zhang, 2015; Lin et al., 2015))\nwe obtain the convergence rate \u02dcO(n1/2 p\u00afL/\u03bb + n). If fi\nare non-convex we \ufb01rst prove that SDCA enjoys the rate\n\u02dcO(\u00afL2/\u03bb2+n). Finally, we show how the acceleration tech-\nnique yields the bound \u02dcO\n\u0010\nn3/4p\u00afL/\u03bb + n\n\u0011\n. That is, we\nhave the same dependency on the square root of the con-\ndition number,\np\u00afL/\u03bb, but this term is multiplied by n3/4\nrather than by n1/2. Understanding if this factor can be\neliminated is left to future work.\nRelated work:\nIn recent years,\nmany randomized\nmethods\nfor\noptimizing\naverage\nof\nfunctions\nhave\nbeen proposed.\nFor example, SAG (Le Roux et al.,\n2012),\nSVRG\n(Johnson & Zhang,\n2013),\nFinito\n(Defazio et al.,\n2014b),\nSAGA\n(Defazio et al.,\n2014a),\nS2GD\n(Kone\u02c7cn`y & Richt\u00b4arik,\n2013),\nand\nUniVr (Allen-Zhu & Yuan, 2015).\nAll of these meth-\nods have similar convergence rates for strongly convex\nSDCA without Duality, Regularization, and Individual Convexity\nand smooth problems. Here we show that SDCA achieves\nthe best known convergence rate for the case in which\nindividual loss functions are convex, and a slightly worse\nrate for the case in which individual loss functions are\nnon-convex. A systematic study of the convergence rate\nof the different methods under non-convex losses is left to\nfuture work.\nThis version of the paper improves upon a previous un-\npublished version of the paper (Shalev-Shwartz, 2015)\nin three aspects.\nFirst, the convergence rate here de-\npends on \u00afL as opposed to Lmax in (Shalev-Shwartz,\n2015).\nSecond, the version in (Shalev-Shwartz, 2015)\nonly deals with the regularized case, while here we show\nthat the same rate can be obtained for unregularized ob-\njectives.\nLast, for the non-convex case, here we derive\nthe bound \u02dcO\n\u0010\nn3/4p\u00afL/\u03bb + n\n\u0011\nwhile in (Shalev-Shwartz,\n2015) only the bound of \u02dcO(L2\nmax/\u03bb2 + n) has been given.\n(Csiba & Richt\u00b4arik,\n2015)\nextended\nthe\nwork\nof\n(Shalev-Shwartz, 2015) to support arbitrary mini-batching\nschemes, and (He & Tak\u00b4a\u02c7c, 2015) extended the work\nof (Shalev-Shwartz, 2015) to support adaptive sampling\nprobabilities.\nA primal form of SDCA has been also\ngiven in (Defazio, 2014).\nUsing SVRG for non-convex\nindividual functions has been recently studied in (Shamir,\n2015; Jin et al., 2015), in the context of fast computation\nof the top singular vectors of a matrix.\n2. SDCA without Duality\nWe start the section by describing a variant of SDCA that\ndo not rely on duality. To simplify the presentation, we start\nin Section 2.1 with regularized loss minimization problems.\nIn Section 2.2 we tackle the non-regularized case and in\nSection 2.3 we tackle the non-convex case.\nWe recall the following basic de\ufb01nitions: A (differentiable)\nfunction f is \u03bb-strongly convex if for every u, w we have\nf(w) \u2212f(u) \u2265\u2207f(u)\u22a4(w \u2212u) + \u03bb\n2 \u2225w \u2212u\u22252. We say\nthat f is convex if it is 0-strongly convex. We say that\nf is L-smooth if \u2225\u2207f(w) \u2212\u2207f(u)\u2225\u2264L\u2225w \u2212u\u2225. It\nis well known that smoothness and convexity also implies\nthat f(w) \u2212f(u) \u2264\u2207f(u)\u22a4(w \u2212u) + L\n2 \u2225w \u2212u\u22252.\n2.1. Regularized problems\nIn regularized problems, each fi can be written as fi(w) =\n\u03c6i(w)+ \u03bb\n2 \u2225w\u22252. Similarly to the original SDCA algorithm,\nwe maintain vectors \u03b11, . . . , \u03b1n, where each \u03b1i \u2208Rd. We\ncall these vectors pseudo-dual vectors. The algorithm is\ndescribed below.\nAlgorithm 1:\nDual-Free SDCA\nfor Regularized Objectives\nGoal: Minimize F(w) = 1\nn\nPn\ni=1 \u03c6i(w) + \u03bb\n2 \u2225w\u22252\nInput: Objective F, number of iterations T ,\nstep size \u03b7,\nSmoothness parameters L1, . . . , Ln\nInitialize: w(0) =\n1\n\u03bb n\nPn\ni=1 \u03b1(0)\ni\nfor some \u03b1(0) = (\u03b1(0)\n1 , . . . , \u03b1(0)\nn )\n\u2200i \u2208[n], qi = (Li + \u00afL)/(2n\u00afL)\nwhere \u00afL = 1\nn\nPn\ni=1 Li\nFor t = 1, . . . , T\nPick i \u223cq, denote \u03b7i =\n\u03b7\nqin\nUpdate:\n\u03b1(t)\ni\n= \u03b1(t\u22121)\ni\n\u2212\u03b7i\u03bbn\n\u0010\n\u2207\u03c6i(w(t\u22121)) + \u03b1(t\u22121)\ni\n\u0011\nw(t) = w(t\u22121) \u2212\u03b7i\n\u0010\n\u2207\u03c6i(w(t\u22121)) + \u03b1(t\u22121)\ni\n\u0011\nObserve that SDCA keeps the primal-dual relation\nw(t\u22121) = 1\n\u03bbn\nn\nX\ni=1\n\u03b1(t\u22121)\ni\nObserve also that the update of \u03b1 can be rewritten as\n\u03b1(t)\ni\n= (1 \u2212\u03b2i)\u03b1(t\u22121)\ni\n+ \u03b2i\n\u0010\n\u2212\u2207\u03c6i(w(t\u22121))\n\u0011\n,\nwhere \u03b2i = \u03b7i\u03bbn. Namely, the new value of \u03b1i is a convex\ncombination of its old value and the negative gradient. Fi-\nnally, observe that, conditioned on the value of w(t\u22121) and\n\u03b1(t\u22121), we have that\nEi\u223cq[w(t)] = w(t\u22121) \u2212\u03b7\nX\ni\nqi\nqin\n\u0010\n(\u2207\u03c6i(w(t\u22121)) + \u03b1(t\u22121)\ni\n)\n\u0011\n= w(t\u22121) \u2212\u03b7\n \n\u22071\nn\nn\nX\ni=1\n\u03c6i(w(t\u22121)) + \u03bbw(t\u22121)\n!\n= w(t\u22121) \u2212\u03b7\u2207P(w(t\u22121)) .\nThat is, SDCA is in fact an instance of Stochastic Gradient\nDescent (SGD). As we will see shortly, the advantage of\nSDCA without Duality, Regularization, and Individual Convexity\nSDCA over a vanilla SGD algorithm is because the vari-\nance of the update goes to zero as we converge to an opti-\nmum.\nOur convergence analysis relies on bounding the following\npotential function, de\ufb01ned for every t \u22650,\nCt = \u03bb\n2 \u2225w(t) \u2212w\u2217\u22252 + \u03b7\nn2\nn\nX\ni=1\n[ 1\nqi\n\u2225\u03b1(t)\ni\n\u2212\u03b1\u2217\ni \u22252] ,\n(1)\nwhere\nw\u2217= argmin\nw\nF(w), and \u2200i, \u03b1\u2217\ni = \u2212\u2207\u03c6i(w\u2217) .\n(2)\nIntuitively, Ct measures the distance to the optimum both\nin primal and pseudo-dual variables. Observe that if F is\nLF -smooth and convex then\nF(w(t)) \u2212F(w\u2217) \u2264LF\n2 \u2225w(t) \u2212w\u2217\u22252 \u2264LF\n\u03bb Ct ,\nand therefore a bound on Ct immediately implies a bound\non the sub-optimality of w(t).\nThe following theorem establishes the convergence rate of\nSDCA for the case in which each \u03c6i is convex.\nTheorem 1 Assume that each \u03c6i is Li-smooth and convex,\nand Algorithm 1 is run with \u03b7 \u2264min\n\b 1\n4\u00afL ,\n1\n4 \u03bbn\n\t\n. Then,\nfor every t \u22651,\nE[Ct] \u2264(1 \u2212\u03b7\u03bb)t C0 ,\nwhere Ct is as de\ufb01ned in (1). In particular, to achieve\nE[F(w(T )) \u2212F(w\u2217)]\n\u2264\n\u01eb it suf\ufb01ces to set \u03b7\n=\nmin\n\b 1\n4\u00afL ,\n1\n4 \u03bbn\n\t\nand\nT \u2265\u02dc\u2126\n\u0012 \u00afL\n\u03bb + n\n\u0013\n.\nVariance Reduction:\nThe lemma below tells us that the\nvariance of the SDCA update decreases as we get closer to\nthe optimum.\nLemma 1 Under the same conditions of Theorem 1, the\nexpected value of \u2225w(t) \u2212w(t\u22121)\u22252 conditioned on w(t\u22121)\nsatis\ufb01es:\nE[\u2225w(t)\u2212w(t\u22121)\u22252] \u22643 \u03b7\n\u0010\n1\n2\u2225w(t\u22121) \u2212w\u2217\u22252 + Ct\u22121\n\u0011\n.\n2.2. SDCA without regularization\nWe now turn to the case in which the objective is not explic-\nitly regularized. The algorithm below tackles this problem\nby a reduction to the regularized case. In particular, we\narti\ufb01cially add regularization to the objective and compen-\nsate for it by adding one more loss function that cancels\nout the regularization term. While the added function is\nnot convex (in fact, it is concave), we prove that the same\nconvergence rate holds due to the special structure of the\nadded loss function.\nAlgorithm 2:\nDual-Free SDCA\nfor Non-Regularized Objectives\nGoal: Minimize F(w) = 1\nn\nPn\ni=1 fi(w)\nInput: Objective F, number of iterations T ,\nstep size \u03b7, Strong convexity parameter \u03bb,\nSmoothness parameters L1, . . . , Ln\nDe\ufb01ne:\nFor all i \u2208[n], \u03c6i(w) = n+1\nn fi(w), \u02dcLi = n+1\nn Li\nFor i = n + 1, \u03c6i(w) = \u2212\u03bb i\n2 \u2225w\u22252, \u02dcLi = \u03bb i\nSolve:\nRewrite F as F(w) =\n1\nn+1\nPn+1\ni=1 \u03c6i(w) + \u03bb\n2 \u2225w\u22252\nCall Algorithm 1 with F above and with {\u02dcLi}\nTheorem 2 Assume that F is \u03bb-strongly convex, that each\nfi is Li-smooth and convex, and that Algorithm 2 is run\nwith \u03b7 \u2264min\nn\n1\n8(\u00afL+\u03bb) ,\n1\n4 \u03bb(n+1)\no\n. Then, for every t \u22651,\nE[Ct] \u2264(1 \u2212\u03b7\u03bb)t C0 ,\nwhere Ct is as de\ufb01ned in (1). In particular, to achieve\nE[F(w(T )) \u2212F(w\u2217)]\n\u2264\n\u01eb it suf\ufb01ces to set \u03b7\n=\nmin\nn\n1\n8(\u00afL+\u03bb) ,\n1\n4 \u03bb(n+1)\no\nand\nT \u2265\u02dc\u2126\n\u0012 \u00afL\n\u03bb + n\n\u0013\n.\n2.3. The non-convex case\nWe now consider the non-convex case. For simplicity, we\nfocus on the regularized setting. In the non-regularized\nsetting we can simply replace every fi with \u03c6i(w) =\nfi(w)\u2212\u03bb\n2 \u2225w\u22252 and apply the regularized setting. Note that\nthis does not change signi\ufb01cantly the smoothness (because\n\u03bb is typically much smaller than the average smoothness of\nthe fi).\nSDCA without Duality, Regularization, and Individual Convexity\nWe can apply Algorithm 1 for the non-convex case, and the\nonly change is the choice of \u03b7, as re\ufb02ected in the theorem\nbelow.\nTheorem 3 Consider running algorithm 1 on F which is\n\u03bb-strongly convex, assume that each \u03c6i is Li-smooth, and\n\u03b7 \u2264min\n\b \u03bb\n4\u00afL2 ,\n1\n4 \u03bbn\n\t\n. Then, for every t \u22651,\nE[Ct] \u2264(1 \u2212\u03b7\u03bb)t C0 ,\nwhere Ct is as de\ufb01ned in (1). In particular, to achieve\nE[F(w(T )) \u2212F(w\u2217)]\n\u2264\n\u01eb it suf\ufb01ces to set \u03b7\n=\nmin\n\b \u03bb\n4\u00afL2 ,\n1\n4 \u03bbn\n\t\nand\nT \u2265\u02dc\u2126\n\u0012 \u00afL2\n\u03bb2 + n\n\u0013\n.\nAs can be seen, the dependence of T on the condition num-\nber,\n\u00afL\n\u03bb , is quadratic for the non-convex case, as opposed to\na linear dependency for the convex case. We next show\nhow to improve the bound using acceleration.\n2.4. Acceleration\nAccelerated SDCA (Shalev-Shwartz & Zhang, 2015) is ob-\ntained by solving (using SDCA) a sequence of problems,\nwhere at each iteration, we add an arti\ufb01cial regularization\nof the form \u03ba\n2 \u2225w \u2212y(t\u22121)\u22252, where y(t\u22121) is a function of\nw(t\u22121) and w(t\u22122). The algorithm has been generalized in\n(Lin et al., 2015) to allow the inner solver to be any algo-\nrithm. For completeness, we provide the pseudo-code of\nthe \u201cCatalyst\u201d algorithm of (Lin et al., 2015) and its analy-\nsis.\nAlgorithm 3:\nAcceleration\nGoal: Minimize a \u03bb-strongly convex function F(w)\nParameters: \u03ba, T\nInitialize:\nInitial solution w(0)\n\u01eb0 s.t. \u01eb0 \u2265F(w(0)) \u2212F(w\u2217)\ny(0) = w(0), q =\n\u03bb\n\u03bb+\u03ba\nFor: t = 1, . . . , T\nDe\ufb01ne Gt(w) = F(w) + \u03ba\n2 \u2225w \u2212y(t\u22121)\u22252\nSet \u01ebt = (1 \u22120.9\u221aq) \u01ebt\u22121\nFind w(t) s.t. Gt(w(t)) \u2212minw Gt(w) \u2264\u01ebt\nSet y(t) = w(t) +\n\u221aq\u2212q\n\u221aq+q (w(t) \u2212w(t\u22121))\nOutput: w(T )\nLemma 2 Fix \u01eb > 0 and suppose we run the Acceleration\nalgorithm (Algorithm 3) for\nT = \u2126\n r\n\u03bb + \u03ba\n\u03bb\nlog\n\u0012\u03bb + \u03ba\n\u03bb \u01eb\n\u0013!\niterations. Then, F(w(T )) \u2212F(w\u2217) \u2264\u01eb.\nProof\nThe lemma follows directly from Theorem 3.1\nof (Lin et al., 2015) by observing that Algorithm 3 is a\nspeci\ufb01cation of Algorithm 1 in (Lin et al., 2015) with\n\u03b10 = \u221aq (which implies that \u03b1t = \u03b10 for every t), with\n\u01ebt = \u01eb0(1 \u2212\u03c1)t, and with \u03c1 = 0.9\u221aq.\nTheorem 4 Let F =\n1\nn\nPn\ni=1 \u03c6i(w) + \u03bb\n2 \u2225w\u22252, assume\nthat each \u03c6i is Li smooth and that F is \u03bb-strongly con-\nvex. Assume also that (\u00afL/\u03bb)2 \u22653n (otherwise we can\nsimply apply \u02dcO(n) iterations of Algorithm 1). Then, run-\nning Algorithm 3 with parameters \u03ba =\n\u00afL/\u221an, T\n=\n\u02dc\u2126\n\u0010\n1 + n\u22121/4p\u00afL/\u03bb\n\u0011\n, and while at each iteration of Al-\ngorithm 3 using \u02dc\u2126(n) iterations of Algorithm 1 to mini-\nmize Gt, guarantees that F(w(T ))\u2212F(w\u2217) \u2264\u01eb (with high\nprobability). The total required number of iterations of Al-\ngorithm 1 is therefore bounded by \u02dcO\n\u0010\nn + n3/4p\u00afL/\u03bb\n\u0011\n.\nObserve that for the case of convex individual func-\ntions, accelerating Algorithm 1 yields the upper bound\n\u02dcO\n\u0010\nn + n1/2p\u00afL/\u03bb\n\u0011\n. Therefore, the convex and non-\nconvex cases have the same dependency on the condition\nnumber, but the non-convex case has a worse dependence\non n.\n3. Proofs\n3.1. Proof of Theorem 1\nObserve that 0 = \u2207F(w\u2217) =\n1\nn\nP\ni \u2207\u03c6i(w\u2217) + \u03bbw\u2217,\nwhich implies that w\u2217\n=\n1\n\u03bbn\nP\ni \u03b1\u2217\ni , where \u03b1\u2217\ni\n=\n\u2212\u2207\u03c6i(w\u2217).\nDe\ufb01ne ui = \u2212\u2207\u03c6i(w(t\u22121)) and vi = \u2212ui + \u03b1(t\u22121)\ni\n. We\nalso denote two potentials:\nAt =\nn\nX\nj=1\n1\nqj\n\u2225\u03b1(t)\nj\n\u2212\u03b1\u2217\nj\u22252 ,\nBt = \u2225w(t) \u2212w\u2217\u22252 .\nSDCA without Duality, Regularization, and Individual Convexity\nWe will \ufb01rst analyze the evolution of At and Bt. If on\nround t we update using element i then \u03b1(t)\ni\n= (1 \u2212\n\u03b2i)\u03b1(t\u22121)\ni\n+ \u03b2iui. It follows that,\nAt\u22121 \u2212At = \u22121\nqi\n\u2225\u03b1(t)\ni\n\u2212\u03b1\u2217\ni \u22252 + 1\nqi\n\u2225\u03b1(t\u22121)\ni\n\u2212\u03b1\u2217\ni \u22252\n(3)\n= \u22121\nqi\n\u2225(1 \u2212\u03b2i)(\u03b1(t\u22121)\ni\n\u2212\u03b1\u2217\ni ) + \u03b2i(ui \u2212\u03b1\u2217\ni )\u22252\n+ 1\nqi\n\u2225\u03b1(t\u22121)\ni\n\u2212\u03b1\u2217\ni \u22252\n= 1\nqi\n(\u2212(1 \u2212\u03b2i)\u2225\u03b1(t\u22121)\ni\n\u2212\u03b1\u2217\ni \u22252 \u2212\u03b2i\u2225ui \u2212\u03b1\u2217\ni \u22252\n+ \u03b2i(1 \u2212\u03b2i)\u2225\u03b1(t\u22121)\ni\n\u2212ui\u22252 + \u2225\u03b1(t\u22121)\ni\n\u2212\u03b1\u2217\ni \u22252 )\n= \u03b2i\nqi\n\u0010\n\u2225\u03b1(t\u22121)\ni\n\u2212\u03b1\u2217\ni \u22252 \u2212\u2225ui \u2212\u03b1\u2217\ni \u22252 + (1 \u2212\u03b2i)\u2225vi\u22252\u0011\n= \u03b7 \u03bb\nq2\ni\n\u0010\n\u2225\u03b1(t\u22121)\ni\n\u2212\u03b1\u2217\ni \u22252 \u2212\u2225ui \u2212\u03b1\u2217\ni \u22252 + (1 \u2212\u03b2i)\u2225vi\u22252\u0011\n.\n(4)\nTaking expectation w.r.t. i \u223cq we obtain\nE[At\u22121 \u2212At] =\n\u03b7\u03bb\nn\nX\ni=1\n1\nqi\n\u0010\n\u2225\u03b1(t\u22121)\ni\n\u2212\u03b1\u2217\ni \u22252 \u2212\u2225ui \u2212\u03b1\u2217\ni \u22252 + (1 \u2212\u03b2i)\u2225vi\u22252\u0011\n(5)\n= \u03b7\u03bb\n \nAt\u22121 +\nn\nX\ni=1\n1\nqi\n\u0000\u2212\u2225ui \u2212\u03b1\u2217\ni \u22252 + (1 \u2212\u03b2i)\u2225vi\u22252\u0001\n!\n.\n(6)\nAs to the second potential, we have\nBt\u22121 \u2212Bt = \u2212\u2225w(t) \u2212w\u2217\u22252 + \u2225w(t\u22121) \u2212w\u2217\u22252\n(7)\n= 2 (w(t\u22121) \u2212w\u2217)\u22a4(\u03b7 vi) \u2212\u03b72\ni \u2225vi\u22252 .\nTaking expectation w.r.t.\ni\n\u223c\nq and noting that\nEi\u223cq(\u03b7ivi) = \u03b7\u2207F(w(t\u22121)) we obtain\nE[Bt\u22121 \u2212Bt] =2\u03b7 (w(t\u22121) \u2212w\u2217)\u22a4\u2207F(w(t\u22121))\n(8)\n\u2212\u03b72\nn2\nX\ni\n1\nqi\n\u2225vi\u22252 .\nWe now take a potential of the form Ct = caAt + cbBt.\nCombining (6) and (8) we obtain\nE[Ct\u22121 \u2212Ct] = ca\u03b7\u03bbAt\u22121 \u2212ca\u03b7\u03bb\nX\ni\n1\nqi\n\u2225ui \u2212\u03b1\u2217\ni \u22252\n+ 2cb\u03b7(w(t\u22121) \u2212w\u2217)\u22a4\u2207F(w(t\u22121))\n+\nX\ni\n1\nqi\n\u2225vi\u22252\n\u0012\nca\u03b7\u03bb(1 \u2212\u03b2i) \u2212cb\u03b72\nn2\n\u0013\n(9)\nWe will choose the parameters \u03b7, ca, cb such that\n\u03b7 \u2264min\n\u001a qi\n2\u03bb ,\n1\n4\u00afL\n\u001b\nand cb\nca\n= \u03bbn2\n2\u03b7\n(10)\nThis implies that \u03b2i = \u03b7i\u03bbn = \u03b7\u03bb\nqi \u22641/2, and therefore the\nterm in (9) is non-negative. Next, due to strong convexity\nof F we have that\n(w(t\u22121) \u2212w\u2217)\u22a4\u2207F(w(t\u22121))\n\u2265F(w(t\u22121)) \u2212F(w\u2217) + \u03bb\n2 \u2225w(t\u22121) \u2212w\u2217\u22252 .\nTherefore,\nE[Ct\u22121 \u2212Ct] = ca\u03b7\u03bbAt\u22121 \u2212ca\u03b7\u03bb\nX\ni\n1\nqi\n\u2225ui \u2212\u03b1\u2217\ni \u22252\n+ 2cb\u03b7(F(w(t\u22121)) \u2212F(w\u2217)) + cb\u03b7\u03bbBt\u22121\n= \u03b7 \u03bb Ct\u22121+\n\u03b7\n \n2cb(F(w(t\u22121)) \u2212F(w\u2217)) \u2212ca\u03bb\nX\ni\n1\nqi\n\u2225ui \u2212\u03b1\u2217\ni \u22252\n!\n.\n(11)\nNote that ui\u2212\u03b1\u2217\ni = \u2207\u03c6i(w(t\u22121))\u2212\u2207\u03c6i(w\u2217). In Lemma 3\nwe show that when \u03c6i is Li smooth and convex then\n\u2225\u2207\u03c6i(w(t\u22121)) \u2212\u2207\u03c6i(w\u2217)\u22252\n(12)\n\u22642 Li (\u03c6i(w(t\u22121)) \u2212\u03c6i(w\u2217) \u2212\u2207\u03c6i(w\u2217)\u22a4(w(t\u22121) \u2212w\u2217))\nSDCA without Duality, Regularization, and Individual Convexity\nTherefore, denoting \u03c4 =\n\u0010\n2 maxi Li\nqi\n\u0011\nwe obtain that\nX\ni\n1\nqi\n\u2225ui \u2212\u03b1\u2217\ni \u22252 =\nX\ni\n1\nqi\n\u2225\u2207\u03c6i(w(t\u22121)) \u2212\u2207\u03c6i(w\u2217)\u22252\n(13)\n\u2264\u03c4\nX\ni\n(\u03c6i(w(t\u22121)) \u2212\u03c6i(w\u2217) \u2212\u2207\u03c6i(w\u2217)\u22a4(w(t\u22121) \u2212w\u2217))\n= \u03c4 n\n\u0012\nF(w(t\u22121)) \u2212F(w\u2217) \u2212\u03bb\n2 \u2225w(t\u22121) \u2212w\u2217\u22252\n\u0013\n\u2264\u03c4 n\n\u0010\nF(w(t\u22121)) \u2212F(w\u2217)\n\u0011\n.\n(14)\nThe de\ufb01nition of qi implies that for every i,\nLi\nqi\n= 2n\u00afL\nLi\nLi + \u00afL \u22642n\u00afL .\n(15)\nCombining this with (13) and (11) we obtain\nE[Ct\u22121 \u2212Ct] \u2265\n\u03b7 \u03bb Ct\u22121 + \u03b7\n\u00002cb \u22124n2 \u00afL\u03bbca\n\u0001\n(F(w(t\u22121)) \u2212F(w\u2217))\nPlugging the value of cb = ca\u03bbn2\n2\u03b7\nyields that the coef\ufb01cient\nin the last term is\n2ca\u03bbn2\n2\u03b7\n\u22124n2 \u00afL\u03bbca = ca\u03bbn2\n\u00121\n\u03b7 \u22124\u00afL\n\u0013\n\u22650 ,\nwhere we used the choice of \u03b7 \u2264\n1\n4\u00afL. In summary, we have\nshown that E[Ct\u22121 \u2212Ct] \u2265\u03b7 \u03bb Ct\u22121, which implies that\nE[Ct] \u2264(1 \u2212\u03b7 \u03bb) Ct\u22121 .\nTaking expectation over Ct\u22121 and continue recursively, we\nobtain that E[Ct] \u2264(1 \u2212\u03b7 \u03bb)t C0 \u2264e\u2212\u03b7 \u03bb t C0.\nFinally, since qi \u22651/(2n) for every i, we can choose\n\u03b7 = min\n\u001a 1\n4\u00afL ,\n1\n4 \u03bbn\n\u001b\nand therefore\n1\n\u03b7\u03bb \u22644\n\u0012\nn +\n\u00afL\n\u03bb\n\u0013\n.\nThe proof is concluded by choosing cb = \u03bb/2 and ca =\n\u03b7/n2.\n3.2. Proof of Lemma 1\nWe have:\nE[\u2225w(t) \u2212w(t\u22121)\u22252] =\nX\ni\nqi\u03b72\ni \u2225\u2207\u03c6i(w(t\u22121)) + \u03b1(t\u22121)\ni\n\u22252\n\u22643\u03b72\nn2\nX\ni\n1\nqi\n(\u2225\u2207\u03c6i(w(t\u22121)) + \u03b1\u2217\ni \u22252\n+ \u2225\u03b1(t\u22121)\ni\n\u2212\u03b1\u2217\ni \u22252)\n(triangle inequality)\n= 3\u03b72\nn2\nX\ni\n( 1\nqi \u2225\u2207\u03c6i(w(t\u22121)) \u2212\u2207\u03c6i(w\u2217)\u22252\n+ 1\nqi \u2225\u03b1(t\u22121)\ni\n\u2212\u03b1\u2217\ni \u22252)\n\u22643\u03b72\nn2\nX\ni\n\u0010\n2n\u00afL \u2225w(t\u22121) \u2212w\u2217\u22252 + 1\nqi \u2225\u03b1(t\u22121)\ni\n\u2212\u03b1\u2217\ni \u22252\u0011\n(smoothness and (15))\n\u22643 \u03b7\n\u0010\n1\n2\u2225w(t\u22121) \u2212w\u2217\u22252 + Ct\u22121\n\u0011\n(because \u03b7 \u2264\n1\n4\u00afL) .\n3.3. Proof of Theorem 2\nThe beginning of the proof is identical to the proof of The-\norem 1. The change starts in (13), where we cannot apply\n(12) to \u03c6n+1 because it is not convex. To overcome this,\nwe \ufb01rst apply (12) to \u03c61, . . . , \u03c6n, and obtain that\nn\nX\ni=1\n1\nqi\n\u2225ui \u2212\u03b1\u2217\ni \u22252 =\nn\nX\ni=1\n1\nqi\n\u2225\u2207\u03c6i(w(t\u22121)) \u2212\u2207\u03c6i(w\u2217)\u22252\n\u2264\n \n2 max\ni\n\u02dcLi\nqi\n!\n\u00b7\nn\nX\ni=1\n(\u03c6i(w(t\u22121)) \u2212\u03c6i(w\u2217) \u2212\u2207\u03c6i(w\u2217)\u22a4(w(t\u22121) \u2212w\u2217))\n= 2 (n + 1)\n \nmax\ni\n\u02dcLi\nqi\n!\n(F(w(t\u22121)) \u2212F(w\u2217)) ,\nwhere the last equality follows from the fact that\nPn\ni=1 \u03c6i(w) = (n + 1)F(w), which also implies that\nP\ni \u2207\u03c6i(w\u2217)\n=\n0.\nIn addition, since \u03c6n+1(w)\n=\nSDCA without Duality, Regularization, and Individual Convexity\n\u2212\u03bb(n+1)\n2\n\u2225w\u22252, we have\n1\nqn+1\n\u2225\u2207\u03c6n+1(w) \u2212\u2207\u03c6n+1(w\u2217)\u22252\n= \u03bb2(n + 1)2\nqn+1\n\u2225w \u2212w\u2217\u22252\n= 2 (n + 1)\n\u02dcLn+1\nqn+1\n\u00b7 \u03bb\n2 \u2225w \u2212w\u2217\u22252\n\u22642 (n + 1)\n\u02dcLn+1\nqn+1\n(F(w) \u2212F(w\u2217)) ,\nwhere the last inequality is because of the \u03bb-strong con-\nvexity of F. Combining the two inequalities, we obtain an\nanalogue of (13),\nn+1\nX\ni=1\n1\nqi\n\u2225ui \u2212\u03b1\u2217\ni \u22252\n\u22644 (n + 1)\n \nmax\ni\u2208[n+1]\n\u02dcLi\nqi\n!\n(F(w(t\u22121)) \u2212F(w\u2217)) .\nThe rest of the proof is almost identical, except that we have\nn replaced by n+1 and \u00afL replaced by \u02dcL :=\n1\nn+1\nPn\ni=1 \u02dcLi.\nWe now need to choose\n\u03b7 = min\n\u001a 1\n8\u02dcL\n,\n1\n4 \u03bb(n + 1)\n\u001b\n.\nObserve that,\n(n+1)\u02dcL = n + 1\nn\n n\nX\ni=1\nLi\n!\n+\u03bb(n+1) = (n+1)(\u00afL+\u03bb) ,\nso we can rewrite\n\u03b7 = min\n\u001a\n1\n8(\u00afL + \u03bb) ,\n1\n4 \u03bb(n + 1)\n\u001b\n.\nThis yields\n1\n\u03b7\u03bb \u22644\n\u0012\nn + 3 + 2\u00afL\n\u03bb\n\u0013\n.\n3.4. Proof of Theorem 3\nThe beginning of the proof is identical to the proof of The-\norem 1 up to (9).\nWe will choose the parameters \u03b7, ca, cb such that\n\u03b7 \u2264min\n\u001a qi\n2\u03bb ,\n1\n4\u00afL\n\u001b\nand cb\nca\n= \u03bbn2\n2\u03b7\n(16)\nThis implies that \u03b2i = \u03b7i\u03bbn = \u03b7\u03bb\nqi \u22641/2, and therefore the\nterm in (9) is non-negative. Next, due to strong convexity\nof F we have that\n(w(t\u22121) \u2212w\u2217)\u22a4\u2207F(w(t\u22121))\n\u2265F(w(t\u22121)) \u2212F(w\u2217) + \u03bb\n2 \u2225w(t\u22121) \u2212w\u2217\u22252\n\u2265\u03bb\u2225w(t\u22121) \u2212w\u2217\u22252 .\nTherefore,\nE[Ct\u22121 \u2212Ct]\n= ca\u03b7\u03bbAt\u22121 \u2212ca\u03b7\u03bb\nX\ni\n1\nqi\n\u2225ui \u2212\u03b1\u2217\ni \u22252 + 2cb\u03b7\u03bbBt\u22121\n= \u03b7 \u03bb Ct\u22121 + \u03b7 \u03bb\n \ncbBt\u22121 \u2212ca\nX\ni\n1\nqi\n\u2225ui \u2212\u03b1\u2217\ni \u22252\n!\n.\n(17)\nNext, we use the smoothness of the \u03c6i to get\nX\ni\n1\nqi\n\u2225ui \u2212\u03b1\u2217\ni \u22252 =\nX\ni\n1\nqi\n\u2225\u2207\u03c6i(w(t\u22121)) \u2212\u2207\u03c6i(w\u2217)\u22252\n\u2264\nX\ni\nL2\ni\nqi\n\u2225w(t\u22121) \u2212w\u2217\u22252 = Bt\u22121\nX\ni\nL2\ni\nqi\n.\nThe de\ufb01nition of qi implies that for every i,\nLi\nqi\n= 2n\u00afL\nLi\nLi + \u00afL \u22642n\u00afL ,\nso by combining with (17) we obtain\nE[Ct\u22121 \u2212Ct] \u2265\u03b7 \u03bb Ct\u22121 + \u03b7\u03bb\n\u0000cb \u22122n2 \u00afL2ca\n\u0001\nBt\u22121\nThe last term will be non-negative if cb\nca \u22652n2 \u00afL2. Since\nwe chose cb\nca = \u03bbn2\n2\u03b7 we obtain the requirement\n\u03bbn2\n2\u03b7 \u22652n2 \u00afL2 \u21d2\u03b7 \u2264\n\u03bb\n4\u00afL2 .\nIn summary, we have shown that E[Ct\u22121\u2212Ct] \u2265\u03b7 \u03bb Ct\u22121.\nThe rest of the proof is identical, but the requirement on \u03b7\nis\n\u03b7 \u2264min\n\u001a \u03bb\n4\u00afL2 ,\n1\n4 \u03bbn\n\u001b\n,\nand therefore\n1\n\u03b7\u03bb \u22644\n\u0012\nn +\n\u00afL2\n\u03bb2\n\u0013\n.\nSDCA without Duality, Regularization, and Individual Convexity\n4. Proof of Theorem 4\nProof Each iteration of Algorithm 3 requires to minimize\nGt to accuracy \u01ebt \u2264O(1) (1 \u2212\u03c1)t, where \u03c1 = 0.9 \u221aq. If\nt \u2264T where T is as de\ufb01ned in Lemma 2, then we have\nthat,\n\u2212t log(1\u2212\u03c1) \u2264\u2212T log(1\u2212\u03c1) = \u2212log(1 \u2212\u03c1)\n\u03c1\nlog\n\u0012800\nq \u01eb\n\u0013\nUsing Lemma 4, \u2212log(1\u2212\u03c1)\n\u03c1\n\u22642 for every \u03c1 \u2208(0, 1/2). In\nour case, \u03c1 is indeed in (0, 1/2) because of the de\ufb01nition of\n\u03ba and our assumption that (\u00afL/\u03bb)2 \u22653n. Hence,\nlog( 1\n\u01ebt ) = O(log((\u03bb + \u03ba)/(\u03bb\u01eb))) .\nCombining this with Theorem 3, and using the de\ufb01nition\nof Gt, we obtain that the number of iterations required1 by\neach application of Algorithm 3 is\n\u02dcO\n\u0012(\u00afL + \u03ba)2\n(\u03bb + \u03ba)2 + n\n\u0013\n= \u02dcO(n) ,\nwhere in the equality we used the de\ufb01nition of \u03ba. Finally,\nmultiplying this by the value of T as given in Lemma 2 we\nobtain (ignoring log-terms):\nr\n1 + \u03ba\n\u03bb n \u2264(1 +\nr\u03ba\n\u03bb) n = n + n3/4\nr \u00afL\n\u03bb .\n4.1. Technical Lemmas\nLemma 3 Assume that \u03c6 is L-smooth and convex. Then,\nfor every w and u,\n\u2225\u2207\u03c6(w)\u2212\u2207\u03c6(u)\u22252 \u22642L\n\u0002\n\u03c6(w) \u2212\u03c6(u) \u2212\u2207\u03c6(u)\u22a4(w \u2212u)\n\u0003\n.\nProof For every i, de\ufb01ne\ng(w) = \u03c6(w) \u2212\u03c6(u) \u2212\u2207\u03c6(u)\u22a4(w \u2212u) .\nClearly, since \u03c6 is L-smooth so is g. In addition, by convex-\nity of \u03c6 we have g(w) \u22650 for all w. It follows that g is non-\nnegative and smooth, and therefore, it is self-bounded (see\n1While Theorem 3 bounds the expected sub-optimality, by\ntechniques similar to (Shalev-Shwartz & Zhang, 2015) it can be\nconverted to a bound that holds with high probability.\nSection 12.1.3 in (Shalev-Shwartz & Ben-David, 2014)):\n\u2225\u2207g(w)\u22252 \u22642Lg(w) .\nUsing the de\ufb01nition of g, we obtain\n\u2225\u2207\u03c6(w) \u2212\u2207\u03c6(u)\u22252\n= \u2225\u2207g(w)\u22252 \u22642Lg(w)\n= 2L\n\u0002\n\u03c6(w) \u2212\u03c6(u) \u2212\u2207\u03c6(u)\u22a4(w \u2212u)\n\u0003\n.\nLemma 4 For a \u2208(0, 1/2) we have \u2212log(1\u2212a)/a \u22641.4.\nProof Denote g(a) = \u2212log(1 \u2212a)/a. It is easy to verify\nthat the derivative of g in (0, 1/2) is positive and that\ng(0.5) \u22641.4. The proof follows.\n5. Summary\nWe have described and analyzed a dual free version of\nSDCA that supports non-regularized objectives and non-\nconvex individual loss functions. Our analysis shows a lin-\near rate of convergence for all of these cases. Two imme-\ndiate open questions are whether the worse dependence on\nthe condition number for the non-accelerated result for the\nnon-convex case is necessary, and whether the factor n3/4\nin Theorem 4 can be reduced to n1/2.\nAcknowledgements:\nIn a previous draft of this paper, the\nbound for the non-convex case was n5/4 +n3/4p\u00afL/\u03bb. We\nthank Ohad Shamir for showing us how to derive the im-\nproved bound of n + n3/4p\u00afL/\u03bb. The work is supported\nby ICRI-CI and by the European Research Council (Theo-\nryDL project).\nReferences\nAgarwal, Alekh and Bottou, Leon. A lower bound for the\noptimization of \ufb01nite sums. In ICML, 2014.\nAllen-Zhu, Zeyuan and Yuan, Yang.\nUnivr: A univer-\nsal variance reduction framework for proximal stochas-\ntic gradient method. arXiv preprint arXiv:1506.01972,\n2015.\nSDCA without Duality, Regularization, and Individual Convexity\nArjevani,\nYossi,\nShalev-Shwartz,\nShai,\nand Shamir,\nOhad.\nOn lower and upper bounds for smooth and\nstrongly convex optimization problems. arXiv preprint\narXiv:1503.06833, 2015.\nCsiba, Dominik and Richt\u00b4arik, Peter.\nPrimal method\nfor erm with \ufb02exible mini-batching schemes and non-\nconvex losses. arXiv preprint arXiv:1506.02227, 2015.\nDefazio, Aaron. New Optimisation Methods for Machine\nLearning. PhD thesis, Australian National Univer- sity,\n2014.\nDefazio, Aaron, Bach, Francis, and Lacoste-Julien, Simon.\nSaga: A fast incremental gradient method with support\nfor non-strongly convex composite objectives. In Ad-\nvances in Neural Information Processing Systems, pp.\n1646\u20131654, 2014a.\nDefazio, Aaron J, Caetano, Tib\u00b4erio S, and Domke,\nJustin.\nFinito: A faster, permutable incremental gra-\ndient method for big data problems.\narXiv preprint\narXiv:1407.2710, 2014b.\nHe, Xi and Tak\u00b4a\u02c7c, Martin. Dual free sdca for empirical risk\nminimization with adaptive probabilities. arXiv preprint\narXiv:1510.06684, 2015.\nJin, Chi, Kakade, Sham M, Musco, Cameron, Netrapalli,\nPraneeth, and Sidford, Aaron. Robust shift-and-invert\npreconditioning: Faster and more sample ef\ufb01cient al-\ngorithms for eigenvector computation. arXiv preprint\narXiv:1510.08896, 2015.\nJohnson, Rie and Zhang, Tong.\nAccelerating stochastic\ngradient descent using predictive variance reduction. In\nAdvances in Neural Information Processing Systems, pp.\n315\u2013323, 2013.\nKone\u02c7cn`y, Jakub and Richt\u00b4arik, Peter. Semi-stochastic gra-\ndient descent methods. arXiv preprint arXiv:1312.1666,\n2013.\nLe Roux, Nicolas, Schmidt, Mark, and Bach, Francis. A\nstochastic gradient method with an exponential conver-\ngence rate for \ufb01nite training sets. In Advances in Neural\nInformation Processing Systems, pp. 2663\u20132671, 2012.\nLin, Hongzhou, Mairal, Julien, and Harchaoui, Zaid. A\nuniversal catalyst for \ufb01rst-order optimization.\nIn Ad-\nvances in Neural Information Processing Systems, pp.\n3366\u20133374, 2015.\nShalev-Shwartz, S. and Zhang, T. Accelerated proximal\nstochastic dual coordinate ascent for regularized loss\nminimization.\nMathematical Programming SERIES A\nand B (to appear), 2015.\nShalev-Shwartz, Shai. Sdca without duality. arXiv preprint\narXiv:1502.06177, 2015.\nShalev-Shwartz, Shai and Ben-David, Shai. Understanding\nMachine Learning: From Theory to Algorithms. Cam-\nbridge university press, 2014.\nShalev-Shwartz, Shai and Zhang, Tong. Stochastic dual co-\nordinate ascent methods for regularized loss minimiza-\ntion. Journal of Machine Learning Research, 14:567\u2013\n599, Feb 2013.\nShamir, Ohad. A stochastic pca and svd algorithm with an\nexponential convergence rate. In ICML, 2015.\nXiao, Lin and Zhang, Tong. A proximal stochastic gradi-\nent method with progressive variance reduction. SIAM\nJournal on Optimization, 24(4):2057\u20132075, 2014.\n",
        "sentence": " Since \u2016\u2207fi(x)\u20162 \u2264 \u03b7k for each i, one can apply the SVRG method [8, 29] to minimize f(x) which gives running time \u00d5 ( nnz(\u03a3k\u22121) + (\u03b7k)2 maxi\u2208[k\u22121]{nnz(Ai)} ) .",
        "context": "E[F(w(T )) \u2212F(w\u2217)]\n\u2264\n\u01eb it suf\ufb01ces to set \u03b7\n=\nmin\n\b 1\n4\u00afL ,\n1\n4 \u03bbn\n\t\nand\nT \u2265\u02dc\u2126\n\u0012 \u00afL\n\u03bb + n\n\u0013\n.\nVariance Reduction:\nThe lemma below tells us that the\nvariance of the SDCA update decreases as we get closer to\nthe optimum.\nas we converge to a minimum.\nOur analysis assumes that F is \u03bb-strongly convex and each\nfi is Li-smooth.\nWhen each fi is also convex we es-\ntablish the convergence rate of \u02dcO(\u00afL/\u03bb + n), where \u00afL is\nthe average of Li and the \u02dcO notation hides logarithmic\nTheorem 2 Assume that F is \u03bb-strongly convex, that each\nfi is Li-smooth and convex, and that Algorithm 2 is run\nwith \u03b7 \u2264min\nn\n1\n8(\u00afL+\u03bb) ,\n1\n4 \u03bb(n+1)\no\n. Then, for every t \u22651,\nE[Ct] \u2264(1 \u2212\u03b7\u03bb)t C0 ,\nwhere Ct is as de\ufb01ned in (1). In particular, to achieve"
    },
    {
        "title": "Convergence of stochastic gradient descent for pca",
        "author": [
            "Ohad Shamir"
        ],
        "venue": "In ICML,",
        "citeRegEx": "30",
        "shortCiteRegEx": "30",
        "year": 2016,
        "abstract": "We consider the problem of principal component analysis (PCA) in a streaming\nstochastic setting, where our goal is to find a direction of approximate\nmaximal variance, based on a stream of i.i.d. data points in $\\reals^d$. A\nsimple and computationally cheap algorithm for this is stochastic gradient\ndescent (SGD), which incrementally updates its estimate based on each new data\npoint. However, due to the non-convex nature of the problem, analyzing its\nperformance has been a challenge. In particular, existing guarantees rely on a\nnon-trivial eigengap assumption on the covariance matrix, which is intuitively\nunnecessary. In this paper, we provide (to the best of our knowledge) the first\neigengap-free convergence guarantees for SGD in the context of PCA. This also\npartially resolves an open problem posed in \\cite{hardt2014noisy}. Moreover,\nunder an eigengap assumption, we show that the same techniques lead to new SGD\nconvergence guarantees with better dependence on the eigengap.",
        "full_text": "arXiv:1509.09002v2  [cs.LG]  4 Jan 2016\nConvergence of Stochastic Gradient Descent for PCA\nOhad Shamir\nWeizmann Institute of Science\nohad.shamir@weizmann.ac.il\nAbstract\nWe consider the problem of principal component analysis (PCA) in a streaming stochastic setting,\nwhere our goal is to \ufb01nd a direction of approximate maximal variance, based on a stream of i.i.d. data\npoints in Rd. A simple and computationally cheap algorithm for this is stochastic gradient descent\n(SGD), which incrementally updates its estimate based on each new data point. However, due to the\nnon-convex nature of the problem, analyzing its performance has been a challenge. In particular, exist-\ning guarantees rely on a non-trivial eigengap assumption on the covariance matrix, which is intuitively\nunnecessary. In this paper, we provide (to the best of our knowledge) the \ufb01rst eigengap-free conver-\ngence guarantees for SGD in the context of PCA. This also partially resolves an open problem posed\nin [10]. Moreover, under an eigengap assumption, we show that the same techniques lead to new SGD\nconvergence guarantees with better dependence on the eigengap.\n1\nIntroduction\nPrincipal component analysis (PCA) [20, 11] is a fundamental tool in data analysis and visualization, de-\nsigned to \ufb01nd the subspace of largest variance in a given dataset (a set of points in Euclidean space). We\nfocus on a simple stochastic setting, where the data x1, x2, . . . \u2208Rd is assumed to be drawn i.i.d. from\nan unknown underlying distribution, and our goal is to \ufb01nd a direction of approximately maximal variance.\nThis can be written as the optimization problem\nmin\nw:\u2225w\u2225=1 \u2212w\u22a4E[xx\u22a4]w,\n(1)\nor equivalently, \ufb01nding an approximate leading eigenvector of the covariance matrix E[xx\u22a4].\nThe conceptually simplest method for this task, given m sampled points x1, . . . , xm, is to construct\nthe empirical covariance matrix 1\nm\nPm\ni=1 xix\u22a4\ni , and compute its leading eigenvector by an eigendecompo-\nsition. Based on concentration of measure arguments, it is not dif\ufb01cult to show that this would result in\nan O(\np\n1/m)-optimal solution to Eq. (1). Unfortunately, the runtime of this method is O(md2 + d3).\nIn large-scale applications, both m and d might be huge, and even forming the d \u00d7 d covariance matrix,\nlet alone performing an eigendecomposition, can be computationally prohibitive. A standard alternative to\nexact eigendecomposition is iterative methods, such as power iterations or the Lanczos method, which re-\nquire performing multiple products of a vector with the empirical covariance matrix. Although this doesn\u2019t\nrequire computing and storing the matrix explicitly, it still requires multiple passes over the data, whose\nnumber may scale with eigengap parameters of the matrix or the target accuracy [14, 16]. Recently, new\nrandomized algorithms for this problem were able to signi\ufb01cantly reduce the required number of passes,\nwhile maintaining the ability to compute high-accuracy solutions [25, 24, 8, 12].\n1\nIn this work, we consider the ef\ufb01cacy of algorithms which perform a single pass over the data, and in\nparticular, stochastic gradient descent (SGD). For solving Eq. (1), SGD corresponds to initializing at some\nunit vector w0, and then at each iteration t perform a stochastic gradient step with respect to xtx\u22a4\nt (which\nis an unbiased estimate of E[xx\u22a4]), followed by a projection to the unit sphere:\nwt := (I + \u03b7xtx\u22a4\nt )wt\u22121 , wt := wt/\u2225wt\u2225.\nHere, \u03b7 is a step size parameter. In the context of PCA, this is also known as Oja\u2019s method [18, 19]. The\nalgorithm is highly ef\ufb01cient in terms of memory and runtime per iteration, requiring storage of a single\nd-dimensional vector, and performing only vector-vector and a vector-scalar products in each iteration.\nIn the world of convex stochastic optimization and learning, SGD has another remarkable property:\nDespite it being a simple, one-pass algorithm, it is essentially (worst-case) statistically optimal, attaining\nthe same statistical estimation error rate as exact empirical risk minimization [5, 23, 22]. Thus, it is quite\nnatural to ask whether SGD also performs well for the PCA problem in Eq. (1), compared to statistically\noptimal but computationally heavier methods.\nThe study of SGD (or variants thereof) for PCA has gained interest in recent years, with some notable\nexamples including [1, 3, 2, 15, 10, 7, 12]. While experimentally SGD appears to perform reasonably\nwell, its theoretical analysis has proven dif\ufb01cult, due to the non-convex nature of the objective function in\nEq. (1). Remarkably, despite this non-convexity, \ufb01nite-time convergence guarantees have been obtained\nunder an eigengap assumption \u2013 namely, that the difference between the largest and 2nd-largest eigenvalues\nof E[xx\u22a4] are separated by some \ufb01xed value \u03bb > 0. For example, [7] require O(d/\u03bb2\u01eb) iterations to ensure\nwith high probability that one of the iterates is \u01eb-optimal. [12] require O(1/\u03bb2 + 1/\u03bb\u01eb) iterations, provided\nwe begin close enough to an optimal solution.\nNevertheless, one may ask whether the eigengap assumption is indeed necessary, if our goal is simply\nto \ufb01nd an approximately optimal solution of Eq. (1). Intuitively, if E[xx\u22a4] has two equal (or near equal)\ntop eigenvalues, then we may still expect to get a solution which lies close to the subspace of these two top\neigenvalues, and approximately minimizes Eq. (1), with the runtime not dependent on any eigengap. Unfor-\ntunately, existing results tell us nothing about this regime, and not just for minor technical reasons: These\nresults are based on tracking the geometric convergence of the SGD iterates wt to a leading eigenvector of\nthe covariance matrix. When there is no eigengap, there is also no single eigenvector to converge to, and\nsuch a geometric approach does not seem to work. Getting an eigengap-free analysis has also been posed as\nan open problem in [10]. We note that while there are quite a few other single-pass, eigengap-free methods\nfor this problem, such as [28, 29, 17, 6, 9, 13], their memory and runtime-per iteration requirements are\nmuch higher than SGD, often O(d2) or worse.\nIn this work, we study the convergence of SGD for PCA, using a different technique that those employed\nin previous works, with the following main results:\n\u2022 We provide the \ufb01rst (to the best of our knowledge) SGD convergence guarantee which does not pose\nan eigengap assumption. Roughly speaking, we prove that if the step size is chosen appropriately,\nthen after T iterations starting from random initialization, with positive probability, SGD returns an\n\u02dcO(\np\np/T)-optimal1 solution of Eq. (1), where p is a parameter depending on how the algorithm is\ninitialized:\n\u2013 If the algorithm is initialized from a warm-start point w0 such that\n1\n\u27e8v,w0\u27e92 \u2264O(1) for some\nleading eigenvector v of the covariance matrix, then p = O(1).\n1Throughout, we use O, \u2126to hide constants, and \u02dcO, \u02dc\u2126to hide constants and logarithmic factors.\n2\n\u2013 Under uniform random initialization on the unit Euclidean sphere, p = O(d), where d is the\ndimension.\n\u2013 Using a more sophisticated initialization (requiring the usage of the \ufb01rst O(d) iterations, but no\nwarm-start point), p = \u02dcO(nA), where nA is the numerical rank of the covariance matrix. The\nnumerical rank is a relaxation of the standard notion of rank, is always at most d and can be\nconsidered a constant under some mild assumptions.\n\u2022 In the scenario of a positive eigengap \u03bb > 0, and using a similar proof technique, we prove an SGD\nconvergence guarantee of O(p/\u03bbT) (where p is as above) with positive probability. This guarantee is\noptimal in terms of dependence on T, \u03bb, and in particular, has better dependence on \u03bb compared to all\nprevious works on SGD-like methods we are aware of (1/\u03bb as opposed to 1/\u03bb2).\nUnfortunately, a drawback of our guarantees is that they only hold with rather low probability: \u2126(1/p),\nwhich can be small if p is large. Formally, this can be overcome by repeating the algorithm \u02dcO(p) times,\nwhich ensures that with high probability, at least one of the outputs will be close to optimal. However, we\nsuspect that these low probabilities are an artifact of our proof technique, and resolving it is left to future\nwork.\n2\nSetting\nWe use bold-faced letters to denote vectors, and capital letters to denote matrices. Given a matrix M, we let\n\u2225M\u2225denote its spectral norm, and \u2225M\u2225F its Frobenius norm.\nWe now present the formal problem setting, in a somewhat more general way than the PCA problem\nconsidered earlier. Speci\ufb01cally, we study the problem of solving\nmin\nw\u2208Rd:\u2225w\u2225=1 \u2212w\u22a4Aw,\n(2)\nwhere d > 1 and A is a positive semide\ufb01nite matrix, given access to a stream of i.i.d. positive semide\ufb01nite\nmatrices \u02dcAt where E[ \u02dcAt] = A (e.g. xtx\u22a4\nt in the PCA case). Notice that the gradient of Eq. (2) at a point\nw equals 2Aw, with an unbiased stochastic estimate being 2 \u02dcAtw. Therefore, applying SGD to Eq. (2)\nreduces to the following: Initialize at some unit-norm vector w0, and for t = 1, . . . , T, perform wt =\n(I + \u03b7 \u02dcAt)wt\u22121, wt = wt/\u2225wt\u2225, returning wT . In fact, for the purpose of the analysis, it is suf\ufb01cient to\nconsider a formally equivalent algorithm, which only performs the projection to the unit sphere at the end:\n\u2022 Initialize by picking a unit norm vector w0\n\u2022 For t = 1, . . . , T, perform wt = (I + \u03b7 \u02dcAt)wt\u22121\n\u2022 Return\nwT\n\u2225wT \u2225\nIt is easy to verify that the output of this algorithm is mathematically equivalent to the original SGD algo-\nrithm, since the stochastic gradient step amounts to multiplying wt\u22121 by a matrix independent of wt\u22121, and\nthe projection just amounts to re-scaling. In both cases, we can write the algorithm\u2019s output in closed form\nas\n\u0010Q1\nt=T (I + \u03b7 \u02dcAt)\n\u0011\nw0\n\r\r\r\n\u0010Q1\nt=T (I + \u03b7 \u02dcAt)\n\u0011\nw0\n\r\r\r\n.\n3\n3\nConvergence Without an Eigengap Assumption\nOur main result is the following theorem, which analyzes the performance of SGD for solving Eq. (2).\nTheorem 1. Suppose that\n\u2022 For some leading eigenvector v of A,\n1\n\u27e8v,w0\u27e92 \u2264p for some p (assumed to be \u22658 for simplicity).\n\u2022 For some b \u22651, both \u2225\u02dc\nAt\u2225\n\u2225A\u2225and \u2225\u02dc\nAt\u2212A\u2225\n\u2225A\u2225\nare at most b with probability 1.\nIf we run the algorithm above for T iterations with \u03b7 =\n1\nb\u221apT (assumed to be \u22641), then with probability at\nleast 1\ncp, the returned w satis\ufb01es\n1 \u2212w\u22a4Aw\n\u2225A\u2225\n\u2264c\u2032 log(T)b\u221ap\n\u221a\nT\n,\nwhere c, c\u2032 are positive numerical constants.\nThe proof and an outline of its main ideas appears in Subsection 5.1 below. Note that this is a multiplica-\ntive guarantee on the suboptimality of Eq. (2), since we normalize by \u2225A\u2225, which is the largest magnitude\nEq. (2) can attain. By multiplying both sides by \u2225A\u2225, we can convert this to an additive bound of the form\n\u2225A\u2225\u2212w\u22a4Aw \u2264c\u2032 log(T)b\u2032\u221ap\n\u221a\nT\n,\nwhere b\u2032 is a bound on max\nn\n\u2225\u02dcAt\u2225, \u2225\u02dcAt \u2212A\u2225\no\n. Also, note that the choice of \u03b7 in the theorem is not crucial,\nand similar bounds (with different c, c\u2032) can be shown for other \u03b7 = \u0398(1/b\u221apT).\nThe value of p in the theorem depends on how the initial point w0 is chosen. One possibility, of course,\nis if we can initialize the algorithm from a \u201cwarm-start\u201d point w0 such that\n1\n\u27e8v,w0\u27e92 \u2264O(1), in which case\nthe bound in the theorem becomes O(log(T)/\n\u221a\nT) with probability \u2126(1). Such a w0 may be given by some\nother algorithm, or alternatively, if we are interested in analyzing SGD in the regime where it is close to one\nof the leading eigenvectors.\nOf course, such an assumption is not always relevant, so let us turn to consider the performance without\nsuch a \u201cwarm-start\u201d. For example, the simplest and most common way to initialize w0 is by picking it\nuniformly at random from the unit sphere. In that case, for any v, \u27e8v, w0\u27e92 = \u0398(1/d) with high constant\nprobability2, so the theorem above applies with p = O(d):\nCorollary 1. If w0 is chosen uniformly at random from the unit sphere in Rd, then Thm. 1 applies with\np = O(d), and the returned w satis\ufb01es, with probability at least \u2126(1/d),\n1 \u2212w\u22a4Aw\n\u2225A\u2225\n\u2264O\n \nlog(T)b\n\u221a\nd\n\u221a\nT\n!\n,\n2One way to see this is by assuming w.l.o.g. that v = e1 and noting that the distribution of w0 is the same as w/\u2225w\u2225where\nw has a standard Gaussian distribution, hence \u27e8v, w0\u27e92 = w2\n1/ P\nj w2\nj , and by using standard concentration tools it can be shown\nthat the numerator is \u0398(1) and the denominator is \u0398(d) with high probability.\n4\nWhile providing some convergence guarantee, note that the probability of success is low, scaling down\nlinearly with d. One way to formally solve this is to repeat the algorithm \u2126(d) times, which ensures that\nwith high probability, at least one output will succeed (and \ufb01nding it can be done empirically by testing the\noutputs on a validation set). However, it turns out that by picking w0 in a smarter way, we can get a bound\nwhere the d factors are substantially improved.\nSpeci\ufb01cally, we consider the following method, parameterized by number of iterations T0, which are\nimplemented before the main algorithm above:\n\u2022 Sample w from a standard Gaussian distribution on Rd\n\u2022 Let w0 = 0.\n\u2022 For t = 1, . . . , T0, let w0 := w0 + 1\nT0 \u02dcAtw\n\u2022 Return w0 :=\nw0\n\u2225w0\u2225.\nEssentially, instead of initializing from a random point w, we initialize from\n\u02dcAw\n\u2225\u02dcAw\u2225\n, where \u02dcA = 1\nT0\nT0\nX\nt=1\n\u02dcAt.\nSince \u02dcA is a mean of T0 random matrices with mean A, this amounts to performing a single approximate\npower iteration. Recently, it was shown that a single exact power iteration can improve the starting point of\nstochastic methods for PCA [24]. The method above extends this idea to a purely streaming setting, where\nwe only have access to stochastic approximations of A.\nThe improved properties of w0 with this initialization is formalized in the following lemma (where\n\u2225A\u2225F denotes the Frobenius norm of A):\nLemma 1. The following holds for some numerical constants c, c\u2032 > 0: For w0 as de\ufb01ned above, if T0 \u2265\ncdb2 log(d), then with probability at least 7\n10 \u22122\nd \u2212exp(\u2212d/8),\n1\n\u27e8v, w0\u27e92 \u2264c\u2032 log(d)nA,\nwhere nA = \u2225A\u22252\nF\n\u2225A\u22252 is the numerical rank of A.\nThe proof is provided in Subsection 5.2. Combining this with Thm. 1, we immediately get the following\ncorollary:\nCorollary 2. If w0 is initialized as described above, then Thm. 1 applies with p = O(log(d)nA), and the\nreturned w satis\ufb01es, with probability at least \u2126(1/nA log(d)),\n1 \u2212w\u22a4Aw\n\u2225A\u2225\n\u2264O\n \nlog(T)b\np\nlog(d)nA\n\u221a\nT\n!\n,\nThe improvement of Corollary 2 compared to Corollary 1 depends on how much smaller is nA, the\nnumerical rank of A, compared to d. We argue that in most cases, nA is much smaller, and often can\nbe thought of as a moderate constant, in which case Corollary 2 provides an \u02dcO\n\u0010\nb\n\u221a\nT\n\u0011\nerror bound with\nprobability \u02dc\u2126(1), at the cost of \u02dcO(db2) additional iterations at the beginning. Speci\ufb01cally:\n5\n\u2022 nA is always in [1, d], and in particular, can never be larger than d.\n\u2022 nA is always upper bounded by the rank of A, and is small even if A is only approximately low rank.\nFor example, if the spectrum of A has polynomial decay i\u2212\u03b1 where \u03b1 > 1, then nA will be a constant\nindependent of d. Moreover, to begin with, PCA is usually applied in situations where we hope A is\nclose to being low rank.\n\u2022 When \u02dcAt is of rank 1 (which is the case, for instance, in PCA, where \u02dcAt equals the outer product\nof the t-th datapoint xt), we have nA \u2264b2, where we recall that b upper bounds the scaled spectral\nnorm of \u02dcAt. In machine learning application, the data norm is often assumed to be bounded, hence b\nis not too large. To see why this holds, note that for rank 1 matrices, the spectral and Frobenius norms\ncoincide, hence\nnA =\n\u0012\u2225A\u2225F\n\u2225A\u2225\n\u00132\n=\n \n\u2225E[ \u02dcA1]\u2225F\n\u2225A\u2225\n!2\n\u2264\n \nE\n\"\n\u2225\u02dcA1\u2225F\n\u2225A\u2225\n#!2\n=\n \nE\n\"\n\u2225\u02dcA1\u2225\n\u2225A\u2225\n#!2\n\u2264b2,\nwhere we used Jensen\u2019s inequality.\nSimilar to Corollary 1, we can also convert the bound of Corollary 2 into a high-probability bound, by\nrepeating the algorithm \u02dcO(nA) times.\n4\nConvergence under an Eigengap Assumption\nAlthough our main interest so far has been the convergence of SGD without any eigengap assumptions, we\nshow in this section that our techniques also imply new bounds for PCA with an eigengap assumptions,\nwhich in certain aspects are stronger than what was previously known.\nSpeci\ufb01cally, we consider the same setting as before, but where the ratio s1\u2212s2\ns1\n, where s1, s2 are the\nleading singular values of the covariance matrix A is assumed to be strictly positive and lower bounded by\nsome \ufb01xed \u03bb > 0. Using this assumption and a proof largely similar to that of Thm. 1, we have the following\ntheorem:\nTheorem 2. Under the same conditions as Thm. 1, suppose furthermore that\n\u2022 The top two eigenvalues of A have a gap \u03bb\u2225A\u2225> 0\n\u2022\nlog2(T)b2p\n\u03bbT\n\u2264log(T)b\u221ap\n\u221a\nT\nIf we run the algorithm above for T > 1 iterations with \u03b7 =\nlog(T)\n\u03bbT\n(assumed to be \u22641), then with\nprobability at least 1\ncp, the returned w satis\ufb01es\n1 \u2212w\u22a4Aw\n\u2225A\u2225\n\u2264c\u2032 log2(T)b2p\n\u03bbT\n,\nwhere c, c\u2032 are positive numerical constants.\nThe proof appears in Subsection 5.3. Considering \ufb01rst the technical conditions of the theorem, we\nnote that assuming log2(T)b2p\n\u03bbT\n\u2264log(T)b\u221ap\n\u221a\nT\nsimply amounts to saying that T is suf\ufb01ciently large so that the\n6\nO\n\u0010\nlog2(T)b2p\n\u03bbT\n\u0011\nbound provided by Thm. 2 is better than the O\n\u0010log(T)b\u221ap\n\u221a\nT\n\u0011\nbound provided by Thm. 1, by\nmore than a constant. This is the interesting regime, since otherwise we might as well choose \u03b7 as in Thm. 1\nand get a better bound without any eigengap assumptions. Moreover, as in Thm. 1, a similar proof would\nhold if the step size is replaced by c log(T)/\u03bbT for some constant c \u22651.\nAs in Thm. 1, we note that p can be as large as d under random initialization, but this can be improved to\nthe numerical rank of A using an approximate power iteration, or by analyzing the algorithm starting from\na warm-start point w0 for which\n1\n\u27e8v,w0\u27e92 \u2264O(1) for a leading eigenvector v of A. Also, note that under\nan eigengap assumption, if 1 \u2212w\u22a4Aw\n\u2225A\u2225\ngoes to 0 with the number of iterations T, it must hold that \u27e8v, w\u27e92\ngoes to 1 for a leading eigenvector of A, so the analysis with p = O(1) is also relevant for analyzing SGD\nfor suf\ufb01ciently large T, once we\u2019re suf\ufb01ciently close to the optimum.\nComparing the bound to previous bounds in the literature for SGD-like methods (which all assume an\neigengap, e.g. [3, 10, 7, 12]), an interesting difference is that the dependence on the eigengap \u03bb is only\n1/\u03bb, as opposed to 1/\u03bb2 or worse. Intuitively, we are able to improve this dependence since we track the\nsuboptimality directly, as opposed to tracking how wT converges to a leading eigenvector, say in terms of\nthe Euclidean norm. This has an interesting parallel in the analysis of SGD for \u03bb-strongly convex functions,\nwhere the suboptimality of wT decays as \u02dcO(1/\u03bbT), although E[\u2225wT \u2212w\u2217\u22252] can only be bounded by\nO(1/\u03bb2T) (compare for instance Lemma 1 in [21] and Theorem 1 in [26]). Quite recently, Jin et al. ([12])\nproposed another streaming algorithm which does have only 1/\u03bb dependence (at least for suf\ufb01ciently large\nT), and a high probability convergence rate which is even asymptotically optimal in some cases. However,\ntheir formal analysis is from a warm-start point (which implies p = O(1) in our notation), whereas the\nanalysis here applies to any starting point. Moreover, the algorithm in [12] is different and more complex,\nwhereas our focus here is on the simple and practical SGD algorithm. Finally, we remark that although\nan O(1/\u03bbT) convergence rate is generally optimal (using any algorithm), we do not know whether the\ndependence on b and p in the convergence bound of Thm. 2 for SGD is optimal, or whether it can be\nimproved.\n5\nProofs\n5.1\nProof of Thm. 1\nTo simplify things, we will assume that we work in a coordinate system where A is diagonal, A =\ndiag(s1, . . . , sd), where s1 \u2265s2 \u2265. . . \u2265sd \u22650, and s1 is the eigenvalue corresponding to v. This is\nwithout loss of generality, since the algorithm and the theorem conditions are invariant to the choice of co-\nordinate system. Moreover, since the objective function in the theorem is invariant to \u2225A\u2225, we shall assume\nthat \u2225A\u2225= s1 = 1. Under these assumptions, the theorem\u2019s conditions reduce to:\n\u2022\n1\nw2\n0,1 \u2264p, for some p \u22658\n\u2022 b \u22651 is an upper bound on \u2225\u02dcAt\u2225, \u2225\u02dcAt \u2212A\u2225\nLet \u01eb \u2208(0, 1) be a parameter to be determined later. The proof works by lower bounding the probability\nof the objective function (which under the assumption \u2225A\u2225= 1, equals 1 \u2212w\u22a4Aw) being suboptimal by\nat most \u01eb. This can be written as\nPr\n\u0012w\u22a4\nT (I \u2212A)wT\n\u2225wT \u22252\n\u2264\u01eb\n\u0013\n,\n7\nor equivalently,\nPr\n\u0010\nw\u22a4\nT ((1 \u2212\u01eb)I \u2212A)wT \u22640\n\u0011\n.\nLetting\nVT = w\u22a4\nT ((1 \u2212\u01eb)I \u2212A)wT ,\nwe need to lower bound Pr(VT \u22640).\nIn analyzing the convergence of stochastic gradient descent, a standard technique to bound such prob-\nabilities is via a martingale analysis, showing that after every iteration, the objective function decreases by\na certain amount. Unfortunately, due to the non-convexity of the objective function here, the amount of\ndecrease at iteration t critically depends on the current iterate wt, and in the worst case may even be 0 (e.g.\nif wt is orthogonal to the leading eigenvector, and there is no noise). Moreover, analyzing the evolution of\nwt is dif\ufb01cult, especially without eigengap assumptions, where there isn\u2019t necessarily some \ufb01xed direction\nwhich wt converges to. Hence, we are forced to take a more circuitous route.\nIn a nutshell, the proof is composed of three parts. First, we prove that if \u01eb and the step size \u03b7 are\nchosen appropriately, then E[VT ] \u2264\u2212\u02dc\u2126\n\u0010\n(1 + \u03b7)2T \u01eb\np\n\u0011\n. If we could also prove a concentration result,\nnamely that VT is not much larger than its expectation, this would imply that Pr(VT \u22640) is indeed large.\nUnfortunately, we do not know how to prove such concentration. However, it turns out that it is possible\nto prove that VT is not much smaller than its expected value: More precisely, that VT \u2265\u2212\u02dcO\n\u0000(1 + \u03b7)2T \u01eb\n\u0001\nwith high probability. We then show that given such a high-probability lower bound on VT , and a bound on\nits expectation, we can produce an upper bound on VT which holds with probability \u02dc\u2126(1/p), hence leading\nto the result stated in the theorem.\nWe begin with a preliminary technical lemma:\nLemma 2. For any \u01eb, \u03b7 \u2208(0, 1), and integer k \u22650,\nmax\ns\u2208[0,1](1 + \u03b7s)k(1 \u2212\u01eb \u2212s) \u22641 + 2(1 + \u03b7(1 \u2212\u01eb))k\n\u03b7(k + 1)\n.\nProof. The result trivially holds for k = 0, so we will assume k > 0 from now. Let\nf(s) = (1 + \u03b7s)k(1 \u2212\u01eb \u2212s).\nDifferentiating f and setting to zero, we have\nk\u03b7(1 + \u03b7s)k\u22121(1 \u2212\u01eb \u2212s) \u2212(1 + \u03b7s)k = 0\n\u21d4k\u03b7(1 \u2212\u01eb \u2212s) = 1 + \u03b7s\n\u21d4\nk\u03b7(1 \u2212\u01eb) \u22121\nk\u03b7 + \u03b7\n= s\n\u21d4s = k(1 \u2212\u01eb) \u22121/\u03b7\nk + 1\n.\nLet sc = k(1\u2212\u01eb)\u22121/\u03b7\nk+1\ndenote this critical point, and consider two cases:\n\u2022 sc /\u2208[0, 1]: In that case, f has no critical points in the domain, hence is maximized at one of the\ndomain endpoints, with a value of at most\nmax{f(0), f(1)} = max{1 \u2212\u01eb, \u2212\u01eb(1 + \u03b7)k} \u22641.\n8\n\u2022 sc \u2208[0, 1]: In that case, we must have k(1 \u2212\u01eb) \u22121\n\u03b7 \u22650, and the value of f at sc is\n\u0012\n1 + \u03b7k(1 \u2212\u01eb) \u22121\nk + 1\n\u0013k \u0012\n1 \u2212\u01eb \u2212k(1 \u2212\u01eb) \u22121/\u03b7\nk + 1\n\u0013\n=\n\u0012\n1 + \u03b7k(1 \u2212\u01eb) \u22121\nk + 1\n\u0013k  \n1 \u2212\u01eb + 1\n\u03b7\nk + 1\n!\n\u2264(1 + \u03b7(1 \u2212\u01eb))k\n \n1 + 1\n\u03b7\nk + 1\n!\n\u22642 (1 + \u03b7(1 \u2212\u01eb))k\n\u03b7(k + 1)\n.\nThe maximal value of f is either the value above, or the maximal value of f at the domain endpoints,\nwhich we already showed to be most 1. Overall, the maximal value f can attain is at most\nmax\n(\n1, 2 (1 + \u03b7(1 \u2212\u01eb))k\n\u03b7(k + 1)\n)\n\u22641 + 2 (1 + \u03b7(1 \u2212\u01eb))k\n\u03b7(k + 1)\n.\nCombining the two cases, the result follows.\nUsing this lemma, we now prove that VT = w\u22a4\nT ((1 \u2212\u01eb)I \u2212A)wT has a large negative expected value.\nTo explain the intuition, note that if we could have used the exact A instead of the stochastic approximations\n\u02dcAt in deriving wT , then we would have\nw\u22a4\nT ((1 \u2212\u01eb)I \u2212A)wT = w\u22a4\n0 (I + \u03b7A)T ((1 \u2212\u01eb)I \u2212A)(I + \u03b7A)T w0\n=\nd\nX\nj=1\n(1 + \u03b7sj)2T (1 \u2212\u01eb \u2212sj)w2\n0,j\n\u22641\np(1 + \u03b7s1)2T (1 \u2212\u01eb \u2212s1) +\nd\nX\nj=2\n(1 + \u03b7sj)2T (1 \u2212\u01eb \u2212sj)w2\n0,j\n\u22641\np(1 + \u03b7s1)2T (1 \u2212\u01eb \u2212s1) +\n\uf8eb\n\uf8ed\nd\nX\nj=2\nw2\n0,j\n\uf8f6\n\uf8f8max\ns\u2208[0,1](1 + \u03b7s)2T (1 \u2212\u01eb \u2212s),\nwhich by the assumptions s1 = 1 and 1 = \u2225w0\u22252 = Pd\nj=1 w2\n0,j is at most\n\u2212\u01eb\np(1 + \u03b7)2T + max\ns\u2208[0,1](1 + \u03b7s)2T (1 \u2212\u01eb \u2212s).\nApplying Lemma 2 and picking \u03b7, \u01eb appropriately, it can be shown that the above is at most \u2212\u2126\n\u0010\n\u01eb\np (1 + \u03b7)2T \u0011\n.\nUnfortunately, this calculation doesn\u2019t apply in practice, since we use the stochastic approximations \u02dcAt\ninstead of A. However, using more involved calculations, we prove in the lemma below that the expectation\nis still essentially the same, provided \u01eb, \u03b7 are chosen appropriately.\n9\nLemma 3. If \u03b7 = 1\nb\nq\n1\npT \u22641 and \u01eb = clog(T)b\u221ap\n\u221a\nT\n\u22641 for some suf\ufb01ciently large constant c, then it holds\nthat\nE[VT ] \u2264\u2212(1 + \u03b7)2T \u01eb\n4p.\nProof. To simplify notation, de\ufb01ne for all t = 1, . . . , T the matrices\nCt\n0 = I + \u03b7A ,\nCt\n1 = \u03b7( \u02dcAt \u2212A).\nNote that Ct\n0 is deterministic whereas Ct\n1 is random and zero-mean. Moreover, \u2225Ct\n0\u2225\u22641+\u03b7 and \u2225Ct\n1\u2225\u2264\u03b7b.\nBy de\ufb01nition of the algorithm, we have the following:\nVT = w\u22a4\nT ((1 \u2212\u01eb)I \u2212A)wT\n= w\u22a4\n0\n T\nY\nt=1\n\u0010\nI + \u03b7 \u02dcAt\n\u0011!\n((1 \u2212\u01eb)I \u2212A)\n 1\nY\nt=T\n\u0010\nI + \u03b7 \u02dcAt\n\u0011!\nw0\n= w\u22a4\n0\n T\nY\nt=1\n\u0000Ct\n0 + Ct\n1\n\u0001\n!\n((1 \u2212\u01eb)I \u2212A)\n 1\nY\nt=T\n\u0000Ct\n0 + Ct\n1\n\u0001\n!\nw0\n=\nX\n(i1,...,iT )\u2208{0,1}T\nX\n(j1,...,jT )\u2208{0,1}T\nw\u22a4\n0\n T\nY\nt=1\nCt\nit\n!\n((1 \u2212\u01eb)I \u2212A)\n 1\nY\nt=T\nCt\njt\n!\nw0.\nSince C1\n1, . . . , CT\n1 are independent and zero-mean, the expectation of each summand in the expression above\nis non-zero only if it = jt for all t. Therefore,\nE\nh\nw\u22a4\nT ((1 \u2212\u01eb)I \u2212A)wT\ni\n=\nX\n(i1,...,iT )\u2208{0,1}T\nE\n\"\nw\u22a4\n0\n T\nY\nt=1\nCt\nit\n!\n((1 \u2212\u01eb)I \u2212A)\n 1\nY\nt=T\nCt\nit\n!\nw0\n#\n.\nWe now decompose this sum according to what is the largest value of t for which it = 1 (hence Ct\nit = Ct\n1).\nThe intuition for this, as will be seen shortly, is that Lemma 2 allows us to attain tighter bounds on the\nsummands when t is much smaller than T. Formally, we can rewrite the expression above as\nE\n\"\nw0\n T\nY\nt=1\nCt\n0\n!\n((1 \u2212\u01eb)I \u2212A)\n 1\nY\nt=T\nCt\n0\n!\nw0\n#\n+\nT\u22121\nX\nk=0\nX\n(i1,...,ik)\u2208{0,1}k\nE\n\"\nw0\n k\nY\nt=1\nCt\nit\n!\nCk+1\n1\n \nT\nY\nt=k+2\nCt\n0\n!\n((1 \u2212\u01eb)I \u2212A)\n k+2\nY\nt=T\nCt\n0\n!\nCk+1\n1\n 1\nY\nt=k\nCt\nit\n!\nw0\n#\n.\nSince Ct\n0 = I +\u03b7A is diagonal and the same for all t, and ((1\u2212\u01eb)I \u2212A) is diagonal as well, we can simplify\nthe above to\nw0(C1\n0)2T ((1 \u2212\u01eb)I \u2212A)w0\n+\nT\u22121\nX\nk=0\nX\n(i1,...,ik)\u2208{0,1}k\nE\n\"\nw0\n k\nY\nt=1\nCt\nit\n!\nCk+1\n1\n(C1\n0)2(T\u2212k\u22121)((1 \u2212\u01eb)I \u2212A)Ck+1\n1\n 1\nY\nt=k\nCt\nit\n!\nw0\n#\n.\n10\nUsing the fact that the spectral norm is sub-multiplicative, and that for any symmetric matrix B, v\u22a4Bv \u2264\n\u2225v2\u2225\u03bbmax(B), where \u03bbmax(B) denotes the largest eigenvalue of B, we can upper bound the above by\n\u2264w0(C1\n0)2T ((1 \u2212\u01eb)I \u2212A)w0\n+\nT\u22121\nX\nk=0\nX\n(i1,...,ik)\u2208{0,1}k\nE\n\"\n\u2225w0\u22252\n k\nY\nt=1\n\u2225Ct\nit\u22252\n!\n\u2225Ck+1\n1\n\u22252\u03bbmax\n\u0010\n(C1\n0)2(T\u2212k\u22121)((1 \u2212\u01eb)I \u2212A)\n\u0011#\n.\nSince \u2225w0\u2225= 1, and \u2225Ct\n0\u2225\u2264(1 + \u03b7), \u2225Ct\n1\u2225\u2264\u03b7b, this is at most\nw0(C1\n0)2T ((1 \u2212\u01eb)I \u2212A)w0\n+\nT\u22121\nX\nk=0\nX\n(i1,...,ik)\u2208{0,1}k\n\u0010\n(1 + \u03b7)2(k\u2212Pk\nt=1 it)(\u03b7b)2 Pk\nt=1 it\u0011\n(\u03b7b)2\u03bbmax\n\u0010\n(C1\n0)2(T\u2212k\u22121)((1 \u2212\u01eb)I \u2212A)\n\u0011\n= w0(C1\n0)2T ((1 \u2212\u01eb)I \u2212A)w0\n+\nT\u22121\nX\nk=0\n\u0000(1 + \u03b7)2 + (\u03b7b)2\u0001k (\u03b7b)2\u03bbmax\n\u0010\n(C1\n0)2(T\u2212k\u22121)((1 \u2212\u01eb)I \u2212A)\n\u0011\n= w0(I + \u03b7A)2T ((1 \u2212\u01eb)I \u2212A)w0\n+ (\u03b7b)2\nT\u22121\nX\nk=0\n\u0000(1 + \u03b7)2 + (\u03b7b)2\u0001k \u03bbmax\n\u0010\n(I + \u03b7A)2(T\u2212k\u22121)((1 \u2212\u01eb)I \u2212A)\n\u0011\n(3)\nRecalling that A = diag(s1, . . . , sd) with s1 = 1, that \u2225w0\u22252 = Pd\nj=1 w2\n0,j = 1, and that w2\n0,1 \u22651\np, the \ufb01rst\nterm in Eq. (3) equals\nw0(I + \u03b7A)2T ((1 \u2212\u01eb)I \u2212A)w0 =\nd\nX\nj=1\n(1 + \u03b7sj)2T (1 \u2212\u01eb \u2212sj)w0,j\n= (1 + \u03b7)(\u2212\u01eb)w2\n0,1 +\nd\nX\nj=2\n(1 + \u03b7sj)2T (1 \u2212\u01eb \u2212sj)w2\n0,j\n\u2264\u2212(1 + \u03b7)2T \u01eb\np + max\ns\u2208[0,1](1 + \u03b7s)2T (1 \u2212\u01eb \u2212s).\nApplying Lemma 2, and recalling that \u03b7 \u22641, we can upper bound the above by\n\u2212(1 + \u03b7)2T \u01eb\np + 1 + 2(1 + \u03b7(1 \u2212\u01eb))2T\n\u03b7(2T + 1)\n= (1 + \u03b7)2T\n\uf8eb\n\uf8ec\n\uf8ed\u2212\u01eb\np + (1 + \u03b7)\u22122T + 2\n\u0010\n1+\u03b7(1\u2212\u01eb)\n1+\u03b7\n\u00112T\n\u03b7(2T + 1)\n\uf8f6\n\uf8f7\n\uf8f8\n\u2264(1 + \u03b7)2T\n \n\u2212\u01eb\np + (1 + \u03b7)\u22122T +\n\u00001 \u22121\n2\u03b7\u01eb\n\u0001\n)2T\n\u03b7T\n!\n.\n(4)\nAs to the second term in Eq. (3), again using the fact that A = diag(s1, . . . , sd), we can upper bound it by\n(\u03b7b)2\nT\u22121\nX\nk=0\n\u0000(1 + \u03b7)2 + (\u03b7b)2\u0001k max\ns\u2208[0,1](1 + \u03b7s)2(T\u2212k\u22121)(1 \u2212\u01eb \u2212s).\n11\nApplying Lemma 2, and recalling that \u03b7 \u22641, this is at most\n(\u03b7b)2\nT\u22121\nX\nk=0\n\u0000(1 + \u03b7)2 + (\u03b7b)2\u0001k\n \n1 + 2(1 + \u03b7(1 \u2212\u01eb))2(T\u2212k\u22121)\n\u03b7(2(T \u2212k) \u22121)\n!\n= (\u03b7b)2(1 + \u03b7)2T\nT\u22121\nX\nk=0\n \n1 +\n\u0012 \u03b7b\n1 + \u03b7\n\u00132!k\n\uf8eb\n\uf8ec\n\uf8ed(1 + \u03b7)\u22122(T\u2212k) + 2\n\u0010\n1+\u03b7(1\u2212\u01eb)\n1+\u03b7\n\u00112(T\u2212k)\n\u03b7(2(T \u2212k) \u22121)\n\uf8f6\n\uf8f7\n\uf8f8\n\u2264(\u03b7b)2(1 + \u03b7)2T\nT\u22121\nX\nk=0\n\u00001 + (\u03b7b)2\u0001k\n \n(1 + \u03b7)\u22122(T\u2212k) + 2\n\u00001 \u22121\n2\u03b7\u01eb\n\u00012(T\u2212k)\n\u03b7(2(T \u2212k) \u22121)\n!\n.\nUpper bounding\n\u00001 + (\u03b7b)2\u0001k by\n\u00001 + (\u03b7b)2\u0001T , and rewriting the sum in terms of k instead of T \u2212k, we\nget\n(\u03b7b)2(1 + \u03b7)2T \u00001 + (\u03b7b)2\u0001T\nT\nX\nk=1\n \n(1 + \u03b7)\u22122k + 2\n\u00001 \u22121\n2\u03b7\u01eb\n\u00012k\n\u03b7(2k \u22121)\n!\n.\nSince k \u22651, we have\n1\n2k\u22121 =\n2k\n2k\u22121\n1\n2k \u22642 1\n2k, so the above is at most\n(\u03b7b)2(1 + \u03b7)2T \u00001 + (\u03b7b)2\u0001T\nT\nX\nk=1\n \n(1 + \u03b7)\u22122k + 4\n\u03b7\n\u00001 \u22121\n2\u03b7\u01eb\n\u00012k\n2k\n!\n\u2264(\u03b7b)2(1 + \u03b7)2T \u00001 + (\u03b7b)2\u0001T\n \u221e\nX\nk=1\n(1 + \u03b7)\u22122k + 4\n\u03b7\n\u221e\nX\nk=1\n\u00001 \u22121\n2\u03b7\u01eb\n\u0001k\nk\n!\n= (\u03b7b)2(1 + \u03b7)2T \u00001 + (\u03b7b)2\u0001T\n\u0012\n1\n(1 + \u03b7)2 \u22121 \u22124\n\u03b7 log\n\u00121\n2\u03b7\u01eb\n\u0013\u0013\n\u2264(\u03b7b)2(1 + \u03b7)2T \u00001 + (\u03b7b)2\u0001T\n\u0012 1\n2\u03b7 + 4\n\u03b7 log\n\u0012 2\n\u03b7\u01eb\n\u0013\u0013\n= \u03b7b2(1 + \u03b7)2T \u00001 + (\u03b7b)2\u0001T\n\u00121\n2 + 4 log\n\u0012 2\n\u03b7\u01eb\n\u0013\u0013\n.\nRecalling that this is an upper bound on the second term in Eq. (3), and combining with the upper bound in\nEq. (4) on the \ufb01rst term, we get overall a bound of\n(1 + \u03b7)2T\n \n\u2212\u01eb\np + (1 + \u03b7)\u22122T +\n\u00001 \u22121\n2\u03b7\u01eb\n\u00012T\n\u03b7T\n+ \u03b7b2 \u00001 + (\u03b7b)2\u0001T\n\u00121\n2 + 4 log\n\u0012 2\n\u03b7\u01eb\n\u0013\u0013!\n.\n(5)\nWe now argue that under suitable choices of \u03b7, \u01eb, the expression above is \u2212\u2126((1 + \u03b7)2T (\u01eb/p). For example,\nthis is satis\ufb01ed if \u03b7 =\n1\nb\u221apT , and we pick \u01eb = c log(T)b\u221ap\n\u221a\nT\nfor some suf\ufb01ciently large constant c. Under these\nchoices, the expression inside the main parentheses above becomes\n\u2212clog(T)b\n\u221apT\n+\n\u0012\n1 +\n1\nb\u221apT\n\u0013\u22122T\n+b\nr p\nT\n\u0012\n1 \u2212c log(T)\n2T\n\u00132T\n+\nb\n\u221apT\n\u0012\n1 + 1\npT\n\u0013T \u00121\n2 + 4 log\n\u0012\n2T\nc log(T)\n\u0013\u0013\n.\n12\nUsing the facts that (1 \u2212a/t)t \u2264exp(\u2212a) for all positive t, a such that a/t < 1, and that c log(T)/2T < 1\nby the assumption that \u01eb \u22641, the above is at most\n\u2212clog(T)b\n\u221apT\n+\nb\n\u221apT\n\u0012\np exp(\u2212c log(T)) + exp(1/p)\n\u00121\n2 + 4 log\n\u0012\n2T\nc log(T)\n\u0013\u0013\u0013\n+\n\u0012\n1 +\n1\nb\u221apT\n\u0013\u22122T\n= clog(T)b\n\u221apT\n\u0012\n\u22121 +\np\nc log(T)T c + exp(1/p)\nc log(T)\n\u00121\n2 + 4 log\n\u0012\n2T\nc log(T)\n\u0013\u0013\u0013\n+\n\u0012\n1 +\n1\nb\u221apT\n\u0013\u22122T\n.\nNote that p, b \u22651 by assumption, and that we can assume T \u2265p (by the assumption that \u01eb \u22641). Therefore,\npicking c suf\ufb01ciently large ensures that the above is at most\nclog(T)b\n\u221apT\n\u0012\n\u22121\n2\n\u0013\n+\n\u0012\n1 +\n1\nb\u221apT\n\u0013\u22122T\n.\nThe second term is exponentially small in T, and in particular can be veri\ufb01ed to be less than 1\n4clog(T)b\n\u221apT\nin\nthe regime where \u01eb = clog(T)b\u221ap\n\u221a\nT\nis at most 1 (assuming c is large enough). Overall, we get a bound of\n\u2212clog(T)b\n\u221apT\n\u00b7 1\n4 = \u2212\u01eb\n4p. Plugging this back into Eq. (5), the result follows.\nHaving proved an upper bound on E[VT], we now turn to prove a high-probability lower bound on\nVT . The proof is based on relating VT to \u2225wT \u22252, and then performing a rather straightforward martingale\nanalysis of log(\u2225wT \u22252).\nLemma 4. Suppose that \u02dcAt is positive semide\ufb01nite for all t, and Pr(\u2225\u02dcAt\u2225\u2264b) = 1. Then for any \u03b4 \u2208(0, 1),\nwe have with probability at least 1 \u2212\u03b4 that\nVT > \u2212exp\n\u0010\n\u03b7b\np\nT log(1/\u03b4) + (b2 + 3)T\u03b72\u0011\n(1 + \u03b7)2T \u01eb.\nProof. Since I \u2212A is a positive semide\ufb01nite matrix, we have\nVT = w\u22a4\nT ((1 \u2212\u01eb)I \u2212A)wT \u2265\u2212\u01eb\u2225wT \u22252.\nThus, it is suf\ufb01cient to prove that\n\u2225wT \u22252 < exp\n\u0010\n\u03b7b\np\nT log(1/\u03b4) + (b2 + 3)T\u03b72\u0011\n(1 + \u03b7)2T .\n(6)\nThe proof goes through a martingale argument. We have\nlog(\u2225wT \u22252) = log\n T\u22121\nY\nt=0\n\u2225wt+1\u22252\n\u2225wt\u22252\n!\n=\nT\u22121\nX\nt=0\nlog\n\u0012\u2225wt+1\u22252\n\u2225wt\u22252\n\u0013\n=\nT\u22121\nX\nt=0\nlog\n \n\u2225(I + \u03b7 \u02dcAt)wt\u22252\n\u2225wt\u22252\n!\n=\nT\u22121\nX\nt=0\nlog\n \n1 +\n \n\u2225(I + \u03b7 \u02dcAt)wt\u22252\n\u2225wt\u22252\n\u22121\n!!\n.\n13\nNote that since \u02dcAt is positive semide\ufb01nite, we always have (1 + \u03b7b)\u2225wt\u22252 \u2265\u2225(I + \u03b7 \u02dcAt)wt\u22252 \u2265\u2225wt\u22252,\nand therefore each summand is of the form log(1+at) where at \u2208[0, \u03b7b]. Using the identity log(1+a) \u2264a\nfor any non-negative a, we can upper bound the above by\nT\u22121\nX\nt=0\n \n\u2225(I + \u03b7 \u02dcAt)wt\u22252\n\u2225wt\u22252\n\u22121\n!\n.\n(7)\nBased on the preceding discussion, this is a sum of random variables bounded in [0, \u03b7b], and the expectation\nof the t-th summand over \u02dcAt, conditioned on \u02dcA1, . . . , \u02dcAt\u22121, equals\nw\u22a4\nt E\nh\n(I + \u03b7 \u02dcAt)\u22a4(I + \u03b7 \u02dcAt)\ni\nwt\n\u2225wt\u22252\n\u22121\n=\nw\u22a4\nt\n\u0010\n(I + \u03b7A)2 + \u03b72 \u0010\n\u02dcA\u22a4\nt \u02dcAt \u2212A2\u0011\u0011\nwt\n\u2225wt\u22252\n\u22121\n\u2264w\u22a4\nt (I + \u03b7A)2wt\n\u2225wt\u22252\n+ \u03b72 w\u22a4\nt \u02dcA\u22a4\nt \u02dcAtwt\n\u2225wt\u22252\n\u22121\n\u2264\u2225(I + \u03b7A)2\u2225+ \u03b72\u2225\u02dcA\u22a4\nt \u02dcAt\u2225\u22121\n\u2264(1 + \u03b7)2 + \u03b72\u2225\u02dcAt\u22252 \u22121\n\u22642\u03b7 + (b2 + 1)\u03b72.\nUsing Azuma\u2019s inequality, it follows that with probability at least 1 \u2212\u03b4, Eq. (7) is at most\nT\n\u00002\u03b7 + (b2 + 1)\u03b72\u0001\n+ \u03b7b\np\nT log(1/\u03b4).\nCombining the observations above, and the fact that log(1+ z) \u2265z \u2212z2 for any z \u22650, we get that with\nprobability at least 1 \u2212\u03b4,\nlog(\u2225wT \u22252) < 2T\u03b7 + (b2 + 1)T\u03b72 + \u03b7b\np\nT log(1/\u03b4)\n= \u03b7b\np\nT log(1/\u03b4) + (b2 + 3)T\u03b72 + 2T(\u03b7 \u2212\u03b72)\n\u2264\u03b7b\np\nT log(1/\u03b4) + (b2 + 3)T\u03b72 + 2T log(1 + \u03b7),\nand therefore\n\u2225wT \u22252 < exp\n\u0010\n\u03b7b\np\nT log(1/\u03b4) + (b2 + 3)T\u03b72\u0011\n(1 + \u03b7)2T ,\nwhich establishes Eq. (6) and proves the lemma.\nWe now have most of the required components to prove Thm. 1. First, we showed in Lemma 3 that if\n\u03b7 = 1\nb\nq\n1\npT , then\nE[VT ] \u2264\u2212(1 + \u03b7)2T \u01eb\n4p.\n(8)\nfor \u01eb = O(b log(T)\np\np/T). Using the same step size \u03b7, Lemma 4 implies that\nPr\n \nVT \u2264\u2212exp\n s\nlog(1/\u03b4)\np\n+ 1 + 3/b2\np\n!\n(1 + \u03b7)2T \u01eb\n!\n\u2264\u03b4,\n14\nand since we assume b \u22651 (hence 1 + 3/b2 \u22644), this implies that\nPr\n \n\u2212\nVT\nexp(4/p)(1 + \u03b7)2T \u01eb \u2265exp\n s\nlog(1/\u03b4)\np\n!!\n\u2264\u03b4.\n(9)\nNow, de\ufb01ne the non-negative random variable\nRT = max\n\u001a\n0, \u2212\nVT\nexp(4/p)(1 + \u03b7)2T \u01eb\n\u001b\n,\nand note that by its de\ufb01nition, E[RT ] \u2265E\nh\n\u2212\nVT\nexp(4/p)(1+\u03b7)2T \u01eb\ni\nand Pr\n\u0012\nRT \u2265exp\n\u0012q\nlog(1/\u03b4)\np\n\u0013\u0013\nequals\nPr\n\u0012\n\u2212\nVT\nexp(4/p)(1+\u03b7)2T \u01eb \u2265exp\n\u0012q\nlog(1/\u03b4)\np\n\u0013\u0013\n. Using Eq. (8) and Eq. (9), this implies that\nE[RT ] \u2265\n1\n4p exp(4/p)\n,\nPr\n \nRT \u2265exp\n s\nlog(1/\u03b4)\np\n!!\n\u2264\u03b4.\nTo summarize the development so far, we de\ufb01ned a non-negative random variable RT , which is bounded\nwith high probability, yet its expectation is at least \u2126(1/p). The following lemma shows that for a bounded\nnon-negative random variable with \u201clarge\u201d expectation, the probability of it being on the same order as its\nexpectation cannot be too small:\nLemma 5. Let X be a non-negative random variable such that for some \u03b1, \u03b2 \u2208[0, 1], we have E[X] \u2265\u03b1,\nand for any \u03b4 \u2208(0, 1],\nPr\n\u0010\nX \u2265exp\n\u0010\n\u03b2\np\nlog(1/\u03b4)\n\u0011\u0011\n\u2264\u03b4.\nThen\nPr\n\u0010\nX > \u03b1\n2\n\u0011\n\u2265\n\u03b1 \u2212exp\n\u0010\n\u22122\n\u03b22\n\u0011\n15\n.\nBefore proving the lemma, let us show to use it to prove Thm. 1. Applying it on the random variable\nRT , which satis\ufb01es the lemma conditions with \u03b1 =\n1\n4p exp(4/p), \u03b2 =\nq\n1\np, we have\n1\n15\n\u0012\n1\n4p exp(4/p) \u2212exp (\u22122p)\n\u0013\n\u2264Pr\n\u0012\nRT >\n1\n8p exp(4/p)\n\u0013\n= Pr\n\u0012\nmax\n\u001a\n0, \u2212\nVT\nexp(4/p)(1 + \u03b7)2T \u01eb\n\u001b\n>\n1\n8p exp(4/p)\n\u0013\n= Pr\n\u0012\n\u2212\nVT\nexp(4/p)(1 + \u03b7)2T \u01eb >\n1\n8p exp(4/p)\n\u0013\n= Pr\n\u0012\nVT \u2264\u2212(1 + \u03b7)2T \u01eb\n8p\n\u0013\n\u2264Pr (VT \u22640)\n15\n1\n15\n\u0010\n1\n4p exp(4/p) \u2212exp (\u22122p)\n\u0011\ncan be veri\ufb01ed to be at least\n1\n100p for any p \u22658, hence we obtained\nPr(VT \u22640) \u2265\n1\n100p.\nAs discussed at the beginning of the proof, VT \u22640 implies that\nwT (I \u2212A)wT\n\u2225wT \u22252\n\u2264\u01eb,\nwhere \u01eb = clog(T)b\u221ap\n\u221a\nT\nis the value chosen in Lemma 3, and the theorem is established.\nAll that remains now is to prove Lemma 5. To explain the intuition, suppose that X in the lemma was\nactually at most 1 with probability 1, rather than just bounded with high probability. Then we would have\n\u03b1 \u2264E[X] = Pr\n\u0010\nX \u2265\u03b1\n2\n\u0011\nE\nh\nX|X \u2265\u03b1\n2\ni\n+ Pr\n\u0010\nX < \u03b1\n2\n\u0011\nE\nh\nX|X \u2264\u03b1\n2\ni\n\u2264Pr\n\u0010\nX \u2265\u03b1\n2\n\u0011\n\u00b7 1 + Pr\n\u0010\nX < \u03b1\n2\n\u0011\n\u00b7 \u03b1\n2\n= Pr\n\u0010\nX \u2265\u03b1\n2\n\u0011\n+\n\u0010\n1 \u2212Pr\n\u0010\nX \u2265\u03b1\n2\n\u0011\u0011 \u03b1\n2 ,\nwhich implies that\n\u03b1 \u2264\n\u0010\n1 \u2212\u03b1\n2\n\u0011\nPr\n\u0010\nX \u2265\u03b1\n2\n\u0011\n+ \u03b1\n2\n=\u21d2\nPr\n\u0010\nX \u2265\u03b1\n2\n\u0011\n\u2265\n\u03b1/2\n1 \u2212\u03b1/2 \u2265\u03b1\n2 .\nTherefore, X is at least one-half its expectation lower bound (\u03b1) with probability at least \u03b1/2. The proof of\nLemma 5, presented below, follows the same intuition, but uses a more delicate analysis since X is actually\nonly upper bounded with high probability.\nProof of Lemma 5. Inverting the bound in the lemma, we have that for any z \u2208[1, \u221e),\nPr(X \u2265z) \u2264exp(\u2212(log(z)/\u03b2)2).\nNow, let r2 > r1 > 0, be parameters to be chosen later. We have\nE[X] =\nZ \u221e\nz=0\nPr(X > z)dz =\nZ r1\nz=0\nPr(X > z)dz +\nZ r2\nz=r1\nPr(X > z)dz +\nZ \u221e\nz=r2\nPr(X > z)dz\n\u2264r1 + (r2 \u2212r1) Pr(X > r1) +\nZ \u221e\nz=r2\nexp(\u2212(log(z)/\u03b2)2)dz\n(10)\nPerforming the variable change y = (log(z)/\u03b2)2 (which implies z = exp(\u03b2\u221ay) and dy =\n2\u221ay\nexp(\u03b2\u221ay)dz), we\nget\nZ \u221e\nz=r2\nexp(\u2212(log(z)/\u03b2)2)dz =\nZ \u221e\ny=\n\u0010 log(r2)\n\u03b2\n\u00112\n1\n2\u221ay exp(\u03b2\u221ay \u2212y)dy\n\u2264\n\u03b2\n2 log(r2)\nZ \u221e\ny=\n\u0010 log(r2)\n\u03b2\n\u00112 exp(\u03b2\u221ay \u2212y)dy.\n16\nSuppose that we choose r2 \u2265exp(2\u03b22). Then log(r2)\n2\u03b2\n\u2265\u03b2, which implies that for any y in the integral\nabove, 1\n2\n\u221ay \u2265\u03b2, and therefore \u03b2\u221ay \u2212y \u22641\n2y \u2212y = \u22121\n2y. As a result, we can upper bound the above by\n\u03b2\n2 log(r2)\nZ \u221e\ny=\n\u0010 log(r2)\n\u03b2\n\u00112 exp\n\u0012\n\u22121\n2y\n\u0013\ndy =\n\u03b2\nlog(r2) exp\n\u0012\n\u2212log2(r2)\n2\u03b22\n\u0013\n.\nPlugging this upper bound back into Eq. (10), extracting Pr(X > r1), and using the assumption E[X] \u2265\u03b1,\nwe get that\nPr(X > r1) \u2265\n\u03b1 \u2212r1 \u2212\n\u03b2\nlog(r2) exp\n\u0010\n\u2212log2(r2)\n2\u03b22\n\u0011\nr2 \u2212r1\n.\nChoosing r1 = \u03b1/2 and r2 = exp(2) (which ensures r2 \u2265exp(2\u03b22) as assumed earlier, since \u03b2 \u22641), we\nget\nPr\n\u0010\nX > \u03b1\n2\n\u0011\n\u2265\n\u03b1 \u2212\u03b2 exp\n\u0010\n\u22122\n\u03b22\n\u0011\n2 exp(2) \u2212\u03b1\n.\nSince \u03b2, \u03b1 \u22641, and 2 exp(2) < 15, this can be simpli\ufb01ed to\nPr\n\u0010\nX > \u03b1\n2\n\u0011\n\u2265\n\u03b1 \u2212exp\n\u0010\n\u22122\n\u03b22\n\u0011\n15\n.\n5.2\nProof of Lemma 1\nDe\ufb01ne \u2206= \u2225\u02dcA \u2212A\u2225. Also, let s1 \u2265s2 \u2265. . . \u2265sd \u22650 be the d eigenvalues of A, with eigenvectors\nv1, . . . , vd, where we assume that v = v1. Using the facts (x + y)2 \u22642x2 + 2y2 and \u2225v1\u2225= 1, we have\n1\n\u27e8v1, w0\u27e92 =\n\u2225\u02dcAw\u22252\n\u27e8v1, \u02dcAw\u27e92 =\n\u2225Aw + ( \u02dcA \u2212A)w\u22252\n\u0010\n\u27e8v1, Aw\u27e9+ \u27e8v1, ( \u02dcA \u2212A)w\u27e9\n\u00112\n\u2264\n2\u2225Aw\u22252 + 2\u2225( \u02dcA \u2212A)w\u22252\n\u27e8v1, Aw\u27e92 + 2\u27e8v1, Aw\u27e9\u27e8v1, ( \u02dcA \u2212A)w\u27e9\n\u2264\n2\u2225Aw\u22252 + 2\u2225w\u22252\u22062\n\u27e8v1, Aw\u27e92 \u22122|\u27e8v1, Aw\u27e9|\u2225w\u2225\u2206,\nwhere we implicitly assume that \u2206is suf\ufb01ciently small for the denominator to be positive (eventually, we\nwill pick T0 large enough to ensure this).\nRecall that v1, . . . , vd forms an orthonormal basis for Rd, so w = Pd\ni=1 vi\u27e8vi, w\u27e9. Therefore, we can\nwrite the above as\n2\n\u0010Pd\ni=1 sivi\u27e8vi, w\u27e9\n\u00112\n+ 2\u2225w\u22252\u22062\n(s1\u27e8v1, w\u27e9)2 \u22122|s1\u27e8v1, w\u27e9|\u2225w\u2225\u2206\n=\n2 Pd\ni=1 s2\ni \u27e8vi, w\u27e92 + 2\u2225w\u22252\u22062\ns2\n1\u27e8v1, w\u27e92 \u22122|s1\u27e8v1, w\u27e9|\u2225w\u2225\u2206\n\u2264\n2\n\u0010Pd\ni=1 s2\ni\n\u0011 \u0000maxi\u27e8vi, w\u27e92\u0001\n+ 2\u2225w\u22252\u22062\ns2\n1\u27e8v1, w\u27e92 \u22122|s1\u27e8v1, w\u27e9|\u2225w\u2225\u2206\n.\n17\nTo simplify notation, since w is drawn from a standard Gaussian distribution, which is rotationally invariant,\nwe can assume without loss of generality that (v1, . . . , vd) = (e1, . . . , ed), the standard basis, so the above\nreduces to\n2\n\u0010Pd\ni=1 s2\ni\n\u0011\nmaxi w2\ni + 2\u2225w\u22252\u22062\ns2\n1w2\n1 \u22122|s1w1|\u2225w\u2225\u2206\n.\nRecall that \u2206= \u2225\u02dcA \u2212A\u2225, where \u02dcA is the average of T0 independent random matrices with mean A, and\nspectral norm at most \u2225A\u2225b. Using a Hoeffding matrix bound (e.g. [27]), and the fact that \u2225A\u2225= s1, it\nfollows that with probability at least 1 \u2212\u03b4,\n\u2206\u2264\u2225A\u2225b\ns\n8 log(d/\u03b4)\nT0\n= s1\ns\n8b2 log(d/\u03b4)\nT0\n.\nPlugging into the above, we get an upper bound of\n2\n\u0010Pd\ni=1 s2\ni\n\u0011\nmaxi w2\ni + \u2225w\u22252s2\n1\n16b2 log(d/\u03b4)\nT0\ns2\n1w2\n1 \u22122s2\n1|w1|\u2225w\u2225\nq\n8b2 log(d/\u03b4)\nT0\n,\nholding with probability at least 1 \u2212\u03b4. Dividing both numerator and denominator by s2\n1, and recalling that\nnA = \u2225A\u22252\nF\n\u2225A\u22252 =\nPd\ni=1 s2\ni\ns2\n1\n, the above equals\n2nA maxi w2\ni + \u2225w\u22252 16b2 log(d/\u03b4)\nT0\nw2\n1 \u22122|w1|\u2225w\u2225\nq\n8b2 log(d/\u03b4)\nT0\n=\n2nA maxi w2\ni + \u2225w\u22252 16b2 log(d/\u03b4)\nT0\n|w1|\n\u0012\n|w1| \u22122\u2225w\u2225\nq\n8b2 log(d/\u03b4)\nT0\n\u0013.\n(11)\nBased on standard Gaussian concentration arguments, it holds that\nPr\n\u0012\nw2\n1 \u22641\n8\n\u0013\n\u22643\n10 ,\nPr\n\u0012\nmax\ni\nw2\ni \u226518 log(d)\n\u0013\n\u22641\nd ,\nPr\n\u0010\n\u2225w\u2225\u2265\n\u221a\n2d\n\u0011\n\u2264exp\n\u0012\n\u2212d\n8\n\u0013\n.\n(see for instance the proof of Lemma 1 in [24], and Corollary 2.3 in [4]). Combining the above with a union\nbound, it holds that with probability at least 1 \u2212\u03b4 \u22123\n10 \u22121\nd \u2212exp(\u2212d/8), Eq. (11) is at most\n36 log(d)nA + 32db2 log(d/\u03b4)\nT0\n1\n8\n\u0012\n1\n8 \u22122\n\u221a\n2\nq\n8db2 log(d/\u03b4)\nT0\n\u0013.\nRecalling that this is an upper bound on\n1\n\u27e8v1,w0\u27e92 , picking \u03b4 = 1/d for simplicity, and slightly simplifying,\nwe showed that with probability at least 7\n10 \u22122\nd \u2212exp(\u2212d/8),\n1\n\u27e8v1, w0\u27e92 \u2264\n36 log(d)nA + 64b2 log(d)\nT0\n1\n8\n\u0012\n1\n8 \u22128\n\u221a\n2\nq\ndb2 log(d)\nT0\n\u0013.\nSince nA \u22651, then by picking T0 \u2265cdb2 log(d) for a suf\ufb01ciently large constant c, we get that\n1\n\u27e8v1,w0\u27e92 \u2264\nc\u2032 log(d)nA for a numerical constant c\u2032, as required.\n18\n5.3\nProof of Thm. 2\nThe proof is very similar to that of Thm. 1, using some of the same lemmas, and other lemmas having slight\ndifferences to take advantage of the eigengap assumption. Below, we focus on the differences, referring to\nparts of the proof of Thm. 1 where necessary.\nFirst, as in the proof of Thm. 1, we assume that we work in a coordinate system where A is diagonal,\nA = diag(s1, . . . , sd), where s1 \u2265s2 \u2265. . . \u2265sd \u22650, and s1 is the eigenvalue corresponding to v. By\nthe eigengap assumption, we can assume that s2, . . . , sd are all at most 1 \u2212\u03bb for some strictly positive\n\u03bb \u2208(0, 1]. Under these assumptions, the theorem\u2019s conditions reduce to:\n\u2022\n1\nw2\n0,1 \u2264p, for some p \u22658\n\u2022 b \u22651 is an upper bound on \u2225\u02dcAt\u2225, \u2225\u02dcAt \u2212A\u2225,\nand as in the proof of Thm. 1, it is enough to lower bound Pr(VT \u22640) where\nVT = w\u22a4\nT ((1 \u2212\u01eb)I \u2212A)wT .\nWe begin by a technical lemma, which bounds a certain quantity appearing later in the proofs:\nLemma 6. Under the conditions of Thm. 2,\nlog2(T)b2\n\u03bb2T\n\u22641\np \u22641.\nProof. By the assumption log2(T)b2p\n\u03bbT\n\u2264log(T)b\u221ap\n\u221a\nT\n, it follows that log(T)b\n\u03bb\n\u221a\nT\n\u2264\n1\n\u221ap, and the result follows by\nsquaring both sides.\nWe now continue by presenting the following variant of Lemma 3:\nLemma 7. Under the conditions of Thm. 2, if we pick \u03b7 =\nlog(T)\n\u03bbT\n\u22641 and \u01eb = clog2(T)b2p\n\u03bbT\nfor some\nsuf\ufb01ciently large numerical constant c, then\nE[VT ] \u2264\u2212(1 + \u03b7)2T \u01eb\n4p.\nProof. By the exact same proof as in Lemma 3 (up till Eq. (3)), we have\nE[VT ] = E[w\u22a4\nT ((1 \u2212\u01eb)I \u2212A)wT ]\n\u2264w0(I + \u03b7A)2T ((1 \u2212\u01eb)I \u2212A)w0\n+ (\u03b7b)2\nT\u22121\nX\nk=0\n\u0000(1 + \u03b7)2 + (\u03b7b)2\u0001k \u03bbmax\n\u0010\n(I + \u03b7A)2(T\u2212k\u22121)((1 \u2212\u01eb)I \u2212A)\n\u0011\n(12)\nRecalling that A = diag(s1, . . . , sd) with s1 = 1, that \u2225w0\u22252 = Pd\nj=1 w2\n0,j = 1, and that w2\n0,1 \u22651\np, the \ufb01rst\n19\nterm in Eq. (3) equals\nw0(I + \u03b7A)2T ((1 \u2212\u01eb)I \u2212A)w0 =\nd\nX\nj=1\n(1 + \u03b7sj)2T (1 \u2212\u01eb \u2212sj)w2\n0,j\n= (1 + \u03b7)(\u2212\u01eb)w2\n0,1 +\nd\nX\nj=2\n(1 + \u03b7sj)2T (1 \u2212\u01eb \u2212sj)w2\n0,j\n\u2264\u2212(1 + \u03b7)2T \u01eb\np +\nmax\ns\u2208[0,1\u2212\u03bb](1 + \u03b7s)2T (1 \u2212\u01eb \u2212s)\n\u2264\u2212(1 + \u03b7)2T \u01eb\np + (1 + \u03b7(1 \u2212\u03bb))2T\n\u2264(1 + \u03b7)2T\n \n\u2212\u01eb\np +\n\u0012\n1 \u2212\n\u03b7\u03bb\n1 + \u03b7\n\u00132T !\n\u2264(1 + \u03b7)2T\n \n\u2212\u01eb\np +\n\u0012\n1 \u2212\u03b7\u03bb\n2\n\u00132T !\n,\n(13)\nwhere we used the assumption that \u03b7 \u22641. As to the second term in Eq. (12), upper bounding it in exactly\nthe same way as in the proof of Lemma 3 (without using the eigengap assumption), we get an upper bound\nof\n\u03b7b2(1 + \u03b7)2T \u00001 + (\u03b7b)2\u0001T\n\u00121\n2 + 4 log\n\u0012 2\n\u03b7\u01eb\n\u0013\u0013\n.\nCombining this with Eq. (13), and plugging back to Eq. (12), we get that\nE[VT ] \u2264(1 + \u03b7)2T\n \n\u2212\u01eb\np +\n\u0012\n1 \u2212\u03b7\u03bb\n2\n\u00132T\n+ \u03b7b2 \u00001 + (\u03b7b)2\u0001T\n\u00121\n2 + 4 log\n\u0012 2\n\u03b7\u01eb\n\u0013\u0013!\n.\n(14)\nPicking \u03b7 = log(T)\n\u03bbT , and \u01eb = c log2(T)b2p\n\u03bbT\nfor some constant c \u22652, the above equals\n(1+\u03b7)2T\n \n\u2212c log2(T)b2\n\u03bbT\n+\n\u0012\n1 \u2212log(T)\n2T\n\u00132T\n+ b2 log(T)\n\u03bbT\n\u0012\n1 + b2 log2(T)\n\u03bb2T 2\n\u0013T \u00121\n2 + 4 log\n\u0012\n2\u03bb2T 2\nc log3(T)b2p\n\u0013\u0013!\n.\nUsing the facts that (1 + a/t)t \u2264exp(a) for all positive t, a, that c log3(T)b2p \u22652, and that \u03bb \u22641, the\nabove is at most\n(1 + \u03b7)2T\n\u0012\n\u2212c log2(T)b2\n\u03bbT\n+ 1\nT + b2 log(T)\n\u03bbT\nexp\n\u0012b2 log2(T)\n\u03bb2T\n\u0013 \u00121\n2 + 4 log\n\u0000T 2\u0001\u0013\u0013\n.\nBy Lemma 6, b2 log2(T)\n\u03bb2T\n\u22641, so the above is at most\n(1 + \u03b7)2T\n\u0012\n\u2212c log2(T)b2\n\u03bbT\n+ 1\nT + b2 log(T)\n\u03bbT\nexp(1)\n\u00121\n2 + 8 log (T)\n\u0013\u0013\n\u2264(1 + \u03b7)2T b2 log2(T)\n\u03bbT\n\u0012\n\u2212c +\n\u03bb\nb2 log2(T) + exp(1)\n\u0012\n1\n2 log(T) + 8\n\u0013\u0013\n.\n20\nClearly, for large enough c, the expression in the main parenthesis above is at most \u2212c/4, so we get an upper\nbound of\n\u2212(1 + \u03b7)2T cb2 log2(T)\n4\u03bbT\n= \u2212(1 + \u03b7)2T \u01eb\n4p,\nfrom which the result follows.\nRather similar to the proof of Thm. 1, we now de\ufb01ne the non-negative random variable\nRT = max\n\u001a\n0, \u2212\nVT\nexp((b2 + 3)T\u03b72)(1 + \u03b7)2T \u01eb\n\u001b\n.\nBy Lemma 7,\nE[RT ] \u2265E\n\u0014\n\u2212\nVT\nexp((b2 + 3)T\u03b72)(1 + \u03b7)2T \u01eb\n\u0015\n\u2265\n1\n4p exp((b2 + 3)T\u03b72),\nand by Lemma 4,\nPr\n\u0010\nRT \u2265exp\n\u0010\n\u03b7b\np\nT log(1/\u03b4)\n\u0011\u0011\n\u2264\u03b4.\nTherefore, applying Lemma 5 on RT , with \u03b1 =\n1\n4p exp((b2+3)T\u03b72) (which is in [0, 1]) and with \u03b2 = \u03b7b\n\u221a\nT\n(which can be veri\ufb01ed to be in [0, 1] by the fact that \u03b7 = log(T)\n\u03bbT\nand Lemma 6), we get that\nPr\n\u0012\nRT >\n1\n8p exp((b2 + 3)T\u03b72)\n\u0013\n\u2265\n1\n15\n\u0012\n1\n4p exp((b2 + 3)T\u03b72) \u2212exp\n\u0012\n\u2212\n2\n\u03b72b2T\n\u0013\u0013\n.\n(15)\nBy de\ufb01nition of RT , the left hand side of this inequality is at most\n= Pr\n\u0012\nmax\n\u001a\n0, \u2212\nVT\nexp((b2 + 3)T\u03b72)(1 + \u03b7)2T \u01eb\n\u001b\n>\n1\n8p exp((b2 + 3)T\u03b72)\n\u0013\n= Pr\n\u0012\n\u2212\nVT\nexp((b2 + 3)T\u03b72)(1 + \u03b7)2T \u01eb >\n1\n8p exp((b2 + 3)T\u03b72)\n\u0013\n= Pr\n\u0012\nVT \u2264\u2212(1 + \u03b7)2T \u01eb\n8p\n\u0013\n\u2264Pr (VT \u22640) ,\nand the right hand side of Eq. (15) (by de\ufb01nition of \u03b7, the assumption b \u22651, and Lemma 6) equals\n1\n15\n\uf8eb\n\uf8ed\n1\n4p exp\n\u0010\n(b2+3) log2(T)\n\u03bb2T\n\u0011 \u2212exp\n\u0012\n\u2212\n2\u03bb2T\nb2 log2(T)\n\u0013\uf8f6\n\uf8f8\n\u2265\n1\n15\n\uf8eb\n\uf8ed\n1\n4p exp\n\u0010\n4b2 log2(T)\n\u03bb2T\n\u0011 \u2212\n1\nexp\n\u0010\n2\n\u03bb2T\nb2 log2(T)\n\u0011\n\uf8f6\n\uf8f8\n\u2265\n1\n15\n\uf8eb\n\uf8ed\n1\n4p exp\n\u0010\n4\np\n\u0011 \u2212\n1\nexp (2p))\n\uf8f6\n\uf8f8,\n21\nwhich can be veri\ufb01ed to be at least\n1\n100p for any p \u22658. Plugging these bounds back to Eq. (15), we obtained\nPr(VT \u22640) \u2265\n1\n100p.\nBy de\ufb01nition of VT , VT \u22640 implies that\nwT (I \u2212A)wT\n\u2225wT \u22252\n\u2264\u01eb,\nwhere \u01eb = clog2(T)b2p\n\u03bbT\nis the value chosen in Lemma 7, and the theorem is established.\nAcknowledgments\nThis research is supported in part by an FP7 Marie Curie CIG grant, the Intel ICRI-CI Institute, and Israel\nScience Foundation grant 425/13. We thank Ofer Zeitouni for several illuminating discussions.\nReferences\n[1] R. Arora, A. Cotter, K. Livescu, and N. Srebro. Stochastic optimization for PCA and PLS. In 2012\n50th Annual Allerton Conference on Communication, Control, and Computing, 2012.\n[2] R. Arora, A. Cotter, and N. Srebro. Stochastic optimization of PCA with capped MSG. In NIPS, 2013.\n[3] A. Balsubramani, S. Dasgupta, and Y. Freund. The fast convergence of incremental PCA. In NIPS,\n2013.\n[4] A. Barvinok. Measure concentration lecture notes. http://www.math.lsa.umich.edu/\u02dcbarvinok/total7\n2005.\n[5] Olivier Bousquet and L\u00b4eon Bottou.\nThe tradeoffs of large scale learning.\nIn Advances in neural\ninformation processing systems, pages 161\u2013168, 2008.\n[6] C. Boutsidis, D. Garber, Z. Karnin, and E. Liberty. Online principal components analysis. In SODA,\n2015.\n[7] C. De Sa, K. Olukotun, and C. R\u00b4e. Global convergence of stochastic gradient descent for some non-\nconvex matrix problems. In ICML, 2015.\n[8] D. Garber and E. Hazan.\nFast and simple pca via convex optimization.\narXiv preprint\narXiv:1509.05647, 2015.\n[9] D. Garber, E. Hazan, and T. Ma. Online learning of eigenvectors. In ICML, 2015.\n[10] M. Hardt and E. Price. The noisy power method: A meta algorithm with applications. In NIPS, 2014.\n[11] H. Hotelling. Analysis of a complex of statistical variables into principal components. Journal of\neducational psychology, 24(6):417, 1933.\n22\n[12] C. Jin, S. Kakade, C. Musco, P. Netrapalli, and A. Sidford.\nRobust shift-and-invert precondi-\ntioning: Faster and more sample ef\ufb01cient algorithms for eigenvector computation.\narXiv preprint\narXiv:1510.08896, 2015.\n[13] W. Kot\u0142owski and M. Warmuth. Pca with gaussian perturbations. arXiv preprint arXiv:1506.04855,\n2015.\n[14] J. Kuczynski and H. Wozniakowski. Estimating the largest eigenvalue by the power and lanczos al-\ngorithms with a random start. SIAM journal on matrix analysis and applications, 13(4):1094\u20131122,\n1992.\n[15] I. Mitliagkas, C. Caramanis, and P. Jain. Memory limited, streaming PCA. In NIPS, 2013.\n[16] C. Musco and C. Musco. Stronger approximate singular value decomposition via the block lanczos\nand power methods. arXiv preprint arXiv:1504.05477, 2015.\n[17] J. Nie, W. Kot\u0142owski, and M. Warmuth. Online pca with optimal regrets. In Algorithmic Learning\nTheory, pages 98\u2013112. Springer, 2013.\n[18] E. Oja. Simpli\ufb01ed neuron model as a principal component analyzer. Journal of mathematical biology,\n15(3):267\u2013273, 1982.\n[19] E. Oja and J. Karhunen.\nOn stochastic approximation of the eigenvectors and eigenvalues of the\nexpectation of a random matrix. Journal of mathematical analysis and applications, 106(1):69\u201384,\n1985.\n[20] K. Pearson. Liii. on lines and planes of closest \ufb01t to systems of points in space. The London, Edinburgh,\nand Dublin Philosophical Magazine and Journal of Science, 2(11):559\u2013572, 1901.\n[21] A. Rakhlin, O. Shamir, and K. Sridharan. Making gradient descent optimal for strongly convex stochas-\ntic optimization. In ICML, 2012.\n[22] S. Shalev-Shwartz and S. Ben-David. Understanding Machine Learning: From Theory to Algorithms.\nCambridge University Press, 2014.\n[23] S. Shalev-Shwartz, O. Shamir, N. Srebro, and K. Sridharan. Stochastic convex optimization. In COLT,\n2009.\n[24] O. Shamir. Fast stochastic algorithms for svd and pca: Convergence properties and convexity. arXiv\npreprint arXiv:1507.08788, 2015.\n[25] O. Shamir. A stochastic PCA and SVD algorithm with an exponential convergence rate. In ICML,\n2015.\n[26] O. Shamir and T. Zhang.\nStochastic gradient descent for non-smooth optimization: Convergence\nresults and optimal averaging schemes. In ICML, 2013.\n[27] J. Tropp.\nUser-friendly tail bounds for sums of random matrices.\nFoundations of Computational\nMathematics, 12(4):389\u2013434, 2012.\n[28] M. Warmuth and D. Kuzmin. Online variance minimization. In Learning theory, pages 514\u2013528.\nSpringer, 2006.\n23\n[29] M. Warmuth and D. Kuzmin. Randomized online pca algorithms with regret bounds that are logarith-\nmic in the dimension. Journal of Machine Learning Research, 9(10):2287\u20132320, 2008.\n24\n",
        "sentence": " Shamir [30] analyzed the so-called Oja\u2019s algorithm but his total regret is O( \u221a dT log(T )) which is a factor \u221a d worse than optimum.",
        "context": "\u0010\nb\n\u221a\nT\n\u0011\nerror bound with\nprobability \u02dc\u2126(1), at the cost of \u02dcO(db2) additional iterations at the beginning. Speci\ufb01cally:\n5\n\u2022 nA is always in [1, d], and in particular, can never be larger than d.\n6\nO\n\u0010\nlog2(T)b2p\n\u03bbT\n\u0011\nbound provided by Thm. 2 is better than the O\n\u0010log(T)b\u221ap\n\u221a\nT\n\u0011\nbound provided by Thm. 1, by\nmore than a constant. This is the interesting regime, since otherwise we might as well choose \u03b7 as in Thm. 1\nO(1/\u03bb2T) (compare for instance Lemma 1 in [21] and Theorem 1 in [26]). Quite recently, Jin et al. ([12])\nproposed another streaming algorithm which does have only 1/\u03bb dependence (at least for suf\ufb01ciently large"
    },
    {
        "title": "An introduction to the conjugate gradient method without the agonizing pain",
        "author": [
            "Jonathan Richard Shewchuk"
        ],
        "venue": null,
        "citeRegEx": "31",
        "shortCiteRegEx": "31",
        "year": 1994,
        "abstract": "",
        "full_text": "",
        "sentence": " 1), so one can apply conjugate gradient [31] or Nesterov\u2019s accelerated gradient descent [24] to minimize this objective.",
        "context": null
    },
    {
        "title": "Note on the gamma function",
        "author": [
            "J.G. Wendel"
        ],
        "venue": "The American Mathematical Monthly,",
        "citeRegEx": "32",
        "shortCiteRegEx": "32",
        "year": 1948,
        "abstract": "The gamma function \u0393(z + 1) = \u041f(z) has been defined in different ways:(1)(Weierstrass)(2)(Kuler)(3)(Gauss)(4)(Euler)(5)(Lerch)",
        "full_text": "",
        "sentence": " Wendell [32]), and the second inequality uses our assumption on q.",
        "context": null
    }
]