[
    {
        "title": "Model selection through sparse maximum likelihood estimation",
        "author": [
            "O. Banerjee",
            "L. El Ghaoui",
            "A. d\u2019Aspremont"
        ],
        "venue": "Machine Learning Research",
        "citeRegEx": "Banerjee et al\\.,? \\Q2007\\E",
        "shortCiteRegEx": "Banerjee et al\\.",
        "year": 2007,
        "abstract": "We consider the problem of estimating the parameters of a Gaussian or binary\ndistribution in such a way that the resulting undirected graphical model is\nsparse. Our approach is to solve a maximum likelihood problem with an added\nl_1-norm penalty term. The problem as formulated is convex but the memory\nrequirements and complexity of existing interior point methods are prohibitive\nfor problems with more than tens of nodes. We present two new algorithms for\nsolving problems with at least a thousand nodes in the Gaussian case. Our first\nalgorithm uses block coordinate descent, and can be interpreted as recursive\nl_1-norm penalized regression. Our second algorithm, based on Nesterov's first\norder method, yields a complexity estimate with a better dependence on problem\nsize than existing interior point methods. Using a log determinant relaxation\nof the log partition function (Wainwright & Jordan (2006)), we show that these\nsame algorithms can be used to solve an approximate sparse maximum likelihood\nproblem for the binary case. We test our algorithms on synthetic data, as well\nas on gene expression and senate voting records data.",
        "full_text": "arXiv:0707.0704v1  [cs.AI]  4 Jul 2007\nModel Selection Through Sparse Max Likelihood Estimation\nModel Selection Through\nSparse Maximum Likelihood Estimation\nfor Multivariate Gaussian or Binary Data\nOnureena Banerjee\nonureena@eecs.berkeley.edu\nLaurent El Ghaoui\nelghaoui@eecs.berkeley.edu\nEECS Department\nUniversity of California, Berkeley\nBerkeley, CA 94720 USA\nAlexandre d\u2019Aspremont\naspremon@princeton.edu\nORFE Department\nPrinceton University\nPrinceton, NJ 08544 USA\nEditor: Leslie Pack Kaelbling\nAbstract\nWe consider the problem of estimating the parameters of a Gaussian or binary distribution\nin such a way that the resulting undirected graphical model is sparse.\nOur approach\nis to solve a maximum likelihood problem with an added \u21131-norm penalty term.\nThe\nproblem as formulated is convex but the memory requirements and complexity of existing\ninterior point methods are prohibitive for problems with more than tens of nodes. We\npresent two new algorithms for solving problems with at least a thousand nodes in the\nGaussian case. Our \ufb01rst algorithm uses block coordinate descent, and can be interpreted\nas recursive \u21131-norm penalized regression. Our second algorithm, based on Nesterov\u2019s \ufb01rst\norder method, yields a complexity estimate with a better dependence on problem size than\nexisting interior point methods. Using a log determinant relaxation of the log partition\nfunction (Wainwright and Jordan [2006]), we show that these same algorithms can be used\nto solve an approximate sparse maximum likelihood problem for the binary case. We test\nour algorithms on synthetic data, as well as on gene expression and senate voting records\ndata.\nKeywords:\nModel Selection, Maximum Likelihood Estimation, Convex Optimization,\nGaussian Graphical Model, Binary Data\n1\nBanerjee, El Ghaoui, and d\u2019Aspremont\n1. Introduction\nUndirected graphical models o\ufb00er a way to describe and explain the relationships among a\nset of variables, a central element of multivariate data analysis. The principle of parsimony\ndictates that we should select the simplest graphical model that adequately explains the\ndata. In this paper weconsider practical ways of implementing the following approach to\n\ufb01nding such a model: given a set of data, we solve a maximum likelihood problem with an\nadded \u21131-norm penalty to make the resulting graph as sparse as possible.\nMany authors have studied a variety of related ideas. In the Gaussian case, model selection\ninvolves \ufb01nding the pattern of zeros in the inverse covariance matrix, since these zeros\ncorrespond to conditional independencies among the variables.\nTraditionally, a greedy\nforward-backward search algorithm is used to determine the zero pattern [e.g., Lauritzen,\n1996]. However, this is computationally infeasible for data with even a moderate number of\nvariables. Li and Gui [2005] introduce a gradient descent algorithm in which they account\nfor the sparsity of the inverse covariance matrix by de\ufb01ning a loss function that is the\nnegative of the log likelihood function. Recently, Huang et al. [2005] considered penalized\nmaximum likelihood estimation, and Dahl et al. [2006] proposed a set of large scale methods\nfor problems where a sparsity pattern for the inverse covariance is given and one must\nestimate the nonzero elements of the matrix.\nAnother way to estimate the graphical model is to \ufb01nd the set of neighbors of each node\nin the graph by regressing that variable against the remaining variables.\nIn this vein,\nDobra and West [2004] employ a stochastic algorithm to manage tens of thousands of vari-\nables. There has also been a great deal of interest in using \u21131-norm penalties in statistical\napplications. d\u2019Aspremont et al. [2004] apply an \u21131 norm penalty to sparse principle compo-\nnent analysis. Directly related to our problem is the use of the Lasso of Tibshirani [1996] to\nobtain a very short list of neighbors for each node in the graph. Meinshausen and B\u00a8uhlmann\n[2006] study this approach in detail, and show that the resulting estimator is consistent,\neven for high-dimensional graphs.\nThe problem formulation for Gaussian data, therefore, is simple. The di\ufb03culty lies in its\ncomputation. Although the problem is convex, it is non-smooth and has an unbounded\nconstraint set. As we shall see, the resulting complexity for existing interior point methods\nis O(p6), where p is the number of variables in the distribution. In addition, interior point\nmethods require that at each step we compute and store a Hessian of size O(p2).\nThe\nmemory requirements and complexity are thus prohibitive for O(p) higher than the tens.\nSpecialized algorithms are needed to handle larger problems.\nThe remainder of the paper is organized as follows.\nWe begin by considering Gaussian\ndata. In Section 2 we set up the problem, derive its dual, discuss properties of the solution\nand how heavily to weight the \u21131-norm penalty in our problem. In Section 3 we present a\nprovably convergent block coordinate descent algorithm that can be interpreted as recursive\n\u21131-norm penalized regression. In Section ?? we present a second, alternative algorithm based\n2\nModel Selection Through Sparse Max Likelihood Estimation\non Nesterov\u2019s recent work on non-smooth optimization, and give a rigorous complexity\nanalysis with better dependence on problem size than interior point methods. In Section ??\nwe show that the algorithms we developed for the Gaussian case can also be used to solve\nan approximate sparse maximum likelihood problem for multivariate binary data, using a\nlog determinant relaxation for the log partition function given by Wainwright and Jordan\n[2006]. In Section 6, we test our methods on synthetic as well as gene expression and senate\nvoting records data.\n2. Problem Formulation\nIn this section we set up the sparse maximum likelihood problem for Gaussian data, derive\nits dual, and discuss some of its properties.\n2.1 Problem setup.\nSuppose we are given n samples independently drawn from a p-variate Gaussian distribution:\ny(1), . . . , y(n) \u223cN(\u03a3p, \u00b5), where the covariance matrix \u03a3 is to be estimated. Let S denote\nthe second moment matrix about the mean:\nS := 1\nn\nn\nX\nk=1\n(y(k) \u2212\u00b5)(y(k) \u2212\u00b5)T .\nLet \u02c6\u03a3\u22121 denote our estimate of the inverse covariance matrix. Our estimator takes the\nform:\n\u02c6\u03a3\u22121 = arg max\nX\u227b0 log det X \u2212trace(SX) \u2212\u03bb\u2225X\u22251.\n(1)\nHere, \u2225X\u22251 denotes the sum of the absolute values of the elements of the positive de\ufb01nite\nmatrix X.\nThe scalar parameter \u03bb controls the size of the penalty. The penalty term is a proxy for\nthe number of nonzero elements in X, and is often used \u2013 albiet with vector, not matrix,\nvariables \u2013 in regression techniques, such as the Lasso.\nIn the case where S \u227b0, the classical maximum likelihood estimate is recovered for \u03bb = 0.\nHowever, when the number of samples n is small compared to the number of variables p,\nthe second moment matrix may not be invertible. In such cases, for \u03bb > 0, our estimator\nperforms some regularization so that our estimate \u02c6\u03a3 is always invertible, no matter how\nsmall the ratio of samples to variables is.\n3\nBanerjee, El Ghaoui, and d\u2019Aspremont\nEven in cases where we have enough samples so that S \u227b0, the inverse S\u22121 may not\nbe sparse, even if there are many conditional independencies among the variables in the\ndistribution. By trading o\ufb00maximality of the log likelihood for sparsity, we hope to \ufb01nd a\nvery sparse solution that still adequately explains the data. A larger value of \u03bb corresponds\nto a sparser solution that \ufb01ts the data less well. A smaller \u03bb corresponds to a solution that\n\ufb01ts the data well but is less sparse. The choice of \u03bb is therefore an important issue that will\nbe examined in detail in Section 2.3.\n2.2 The dual problem and bounds on the solution.\nWe can write (1) as\nmax\nX\u227b0\nmin\n\u2225U\u2225\u221e\u2264\u03bb log det X + trace(X, S + U)\nwhere \u2225U\u2225\u221edenotes the maximum absolute value element of the symmetric matrix U. This\ncorresponds to seeking an estimate with the maximum worst-case log likelihood, over all\nadditive perturbations of the second moment matrix S. A similar robustness interpretation\ncan be made for a number of estimation problems, such as support vector machines for\nclassi\ufb01cation.\nWe can obtain the dual problem by exchanging the max and the min. The resulting inner\nproblem in X can be solved analytically by setting the gradient of the objective to zero and\nsolving for X. The result is\nmin\n\u2225U\u2225\u221e\u2264\u03bb \u2212log det(S + U) \u2212p\nwhere the primal and dual variables are related as: X = (S + U)\u22121. Note that the log\ndeterminant function acts a log barrier, creating an implicit constraint that S + U \u227b0.\nTo write things neatly, let W = S + U. Then the dual of our sparse maximum likelihood\nproblem is\n\u02c6\u03a3 := max{log det W : \u2225W \u2212S\u2225\u221e\u2264\u03bb}.\n(2)\nObserve that the dual problem (1) estimates the covariance matrix while the primal problem\nestimates its inverse. We also observe that the diagonal elements of the solution are \u03a3kk =\nSkk + \u03bb for all k.\nThe following theorem shows that adding the \u21131-norm penalty regularlizes the solution.\nTheorem 1 For every \u03bb > 0, the optimal solution to (1) is unique, with bounded eigenval-\nues:\np\n\u03bb \u2265\u2225\u02c6\u03a3\u22121\u22252 \u2265(\u2225S\u22252 + \u03bbp)\u22121.\nHere, \u2225A\u22252 denotes the maximum eigenvalue of a symmetric matrix A.\n4\nModel Selection Through Sparse Max Likelihood Estimation\nThe dual problem (2) is smooth and convex.\nWhen p(p + 1)/2 is in the low hundreds,\nthe problem can be solved by existing software that uses an interior point method [e.g.,\nVandenberghe et al., 1998].\nThe complexity to compute an \u01eb-suboptimal solution using\nsuch second-order methods, however, is O(p6 log(1/\u01eb)), making them infeasible when p is\nlarger than the tens.\nA related problem, solved by Dahl et al. [2006], is to compute a maximum likelihood es-\ntimate of the covariance matrix when the sparsity structure of the inverse is known in\nadvance. This is accomplished by adding constraints to (1) of the form: Xij = 0 for all\npairs (i, j) in some speci\ufb01ed set. Our constraint set is unbounded as we hope to uncover\nthe sparsity structure automatically, starting with a dense second moment matrix S.\n2.3 Choice of penalty parameter.\nConsider the true, unknown graphical model for a given distribution. This graph has p\nnodes, and an edge between nodes k and j is missing if variables k and j are independent\nconditional on the rest of the variables. For a given node k, let Ck denote its connectivity\ncomponent: the set of all nodes that are connected to node k through some chain of edges.\nIn particular, if node j \u0338\u2208Ck, then variables j and k are independent.\nWe would like to choose the penalty parameter \u03bb so that, for \ufb01nite samples, the probability\nof error in estimating the graphical model is controlled. To this end, we can adapt the work\nof Meinshausen and B\u00a8uhlmann [2006] as follows. Let \u02c6C\u03bb\nk denote our estimate of the connec-\ntivity component of node k. In the context of our optimization problem, this corresponds\nto the entries of row k in \u02c6\u03a3 that are nonzero.\nLet \u03b1 be a given level in [0, 1]. Consider the following choice for the penalty parameter in\n(1):\n\u03bb(\u03b1) := (max\ni>j \u02c6\u03c3i\u02c6\u03c3j)\ntn\u22122(\u03b1/2p2)\nq\nn \u22122 + t2\nn\u22122(\u03b1/2p2)\n(3)\nwhere tn\u22122(\u03b1) denotes the (100\u2212\u03b1)% point of the Student\u2019s t-distribution for n\u22122 degrees\nof freedom, and \u02c6\u03c3i is the empirical variance of variable i. Then we can prove the following\ntheorem:\nTheorem 2 Using \u03bb(\u03b1) the penalty parameter in (1), for any \ufb01xed level \u03b1,\nP(\u2203k \u2208{1, . . . , p} : \u02c6C\u03bb\nk \u0338\u2286Ck) \u2264\u03b1.\nObserve that, for a \ufb01xed problem size p, as the number of samples n increases to in\ufb01nity,\nthe penalty parameter \u03bb(\u03b1) decreases to zero. Thus, asymptotically we recover the clas-\n5\nBanerjee, El Ghaoui, and d\u2019Aspremont\nsical maximum likelihood estimate, S, which in turn converges in probability to the true\ncovariance \u03a3.\n3. Block Coordinate Descent Algorithm\nIn this section we present an algorithm for solving (2) that uses block coordinate descent.\n3.1 Algorithm description.\nWe begin by detailing the algorithm. For any symmetric matrix A, let A\\j\\k denote the\nmatrix produced by removing row k and column j.\nLet Aj denote column j with the\ndiagonal element Ajj removed. The plan is to optimize over one row and column of the\nvariable matrix W at a time, and to repeatedly sweep through all columns until we achieve\nconvergence.\nInitialize: W (0) := S + \u03bbI\nFor k \u22650\n1. For j = 1, . . . , p\n(a) Let W (j\u22121) denote the current iterate. Solve the quadratic program\n\u02c6y := arg min\ny {yT (W (j\u22121)\n\\j\\j )\u22121y : \u2225y \u2212Sj\u2225\u221e\u2264\u03bb}\n(4)\n(b) Update rule: W (j) is W (j\u22121) with column/row Wj replaced by \u02c6y.\n2. Let \u02c6W (0) := W (p).\n3. After each sweep through all columns, check the convergence condition. Convergence\noccurs when\ntrace(( \u02c6W (0))\u22121S) \u2212p + \u03bb\u2225( \u02c6W (0))\u22121\u22251 \u2264\u01eb.\n(5)\n3.2 Convergence and property of solution.\nUsing Schur complements, we can prove convergence:\nTheorem 3 The block coordinate descent algorithm described above converges, acheiving\nan \u01eb-suboptimal solution to (2). In particular, the iterates produced by the algorithm are\nstrictly positive de\ufb01nite: each time we sweep through the columns, W (j) \u227b0 for all j.\n6\nModel Selection Through Sparse Max Likelihood Estimation\nThe proof of Theorem 3 sheds some interesting light on the solution to problem (1). In\nparticular, we can use this method to show that the solution has the following property:\nTheorem 4 Fix any k \u2208{1, . . . , p}. If \u03bb \u2265|Skj| for all j \u0338= k, then column and row k of\nthe solution \u02c6\u03a3 to (2) are zero, excluding the diagonal element.\nThis means that, for a given second moment matrix S, if \u03bb is chosen such that the condition\nin Theorem 4 is met for some column k, then the sparse maximum likelihood method esti-\nmates variable k to be independent of all other variables in the distribution. In particular,\nTheorem 4 implies that if \u03bb \u2265|Skj| for all k > j, then (1) estimates all variables in the\ndistribution to be pairwise independent.\nUsing the work of Luo and Tseng [1992], it may be possible to show that the local conver-\ngence rate of this method is at least linear. In practice we have found that a small number\nof sweeps through all columns, independent of problem size p, is su\ufb03cient to achieve con-\nvergence. For a \ufb01xed number of K sweeps, the cost of the method is O(Kp4), since each\niteration costs O(p3).\n3.3 Interpretation as recursive penalized regression.\nThe dual of (4) is\nmin\nx xT W (j\u22121)\n\\j\\j x \u2212ST\nj x + \u03bb\u2225x\u22251.\n(6)\nStrong duality obtains so that problems (6) and (4) are equivalent. If we let Q denote the\nsquare root of W (j\u22121)\n\\j\\j , and b := 1\n2Q\u22121Sj, then we can write (6) as\nmin\nx \u2225Qx \u2212b\u22252\n2 + \u03bb\u2225x\u22251.\n(7)\nThe problem (7) is a penalized least-squares problems, known as the Lasso. If W (j\u22121)\n\\j\\j\nwere\nthe j-th principal minor of the sample covariance S, then (7) would be equivalent to a\npenalized regression of variable j against all others. Thus, the approach is reminiscent of\nthe approach explored by Meinshausen and B\u00a8uhlmann [2006], but there are two di\ufb00erences.\nFirst, we begin with some regularization and, as a consequence, each penalized regression\nproblem has a unique solution. Second, and more importantly, we update the problem data\nafter each regression: except at the very \ufb01rst update, W (j\u22121)\n\\j\\j\nis never a minor of S. In this\nsense, the coordinate descent method can be interpreted as a recursive Lasso.\n7\nBanerjee, El Ghaoui, and d\u2019Aspremont\n4. Nesterov\u2019s First Order Method\nIn this section we apply the recent results due to Nesterov [2005] to obtain a \ufb01rst order algo-\nrithm for solving (1) with lower memory requirements and a rigorous complexity estimate\nwith a better dependence on problem size than those o\ufb00ered by interior point methods.\nOur purpose is not to obtain another algorithm, as we have found that the block coordinate\ndescent is fairly e\ufb03cient; rather, we seek to use Nesterov\u2019s formalism to derive a rigorous\ncomplexity estimate for the problem, improved over that o\ufb00ered by interior-point methods.\nAs we will see, Nesterov\u2019s framework allows us to obtain an algorithm that has a complexity\nof O(p4.5/\u01eb), where \u01eb > 0 is the desired accuracy on the objective of problem (1). This is\nin contrast to the complexity of interior-point methods, O(p6 log(1/\u01eb)). Thus, Nesterov\u2019s\nmethod provides a much better dependence on problem size and lower memory requirements\nat the expense of a degraded dependence on accuracy.\n4.1 Idea of Nesterov\u2019s method.\nNesterov\u2019s method applies to a class of non-smooth, convex optimization problems of the\nform\nmin\nx {f(x) : x \u2208Q1}\n(8)\nwhere the objective function can be written as\nf(x) = \u02c6f(x) + max\nu {\u27e8Ax, u\u27e92 : u \u2208Q2}.\nHere, Q1 and Q2 are bounded, closed, convex sets, \u02c6f(x) is di\ufb00erentiable (with a Lipschitz-\ncontinuous gradient) and convex on Q1, and A is a linear operator. The challenge is to\nwrite our problem in the appropriate form and choose associated functions and parameters\nin such a way as to obtain the best possible complexity estimate, by applying general results\nobtained by Nesterov [2005].\nObserve that we can write (1) in the form (8) if we impose bounds on the eigenvalues of\nthe solution, X. To this end, we let\nQ1 := {x : aI \u2aafX \u2aafbI}\nQ2 := {u : \u2225u\u2225\u221e\u2264\u03bb}\n(9)\nwhere the constants a, b are given such that b > a > 0. By Theorem 1, we know that such\nbounds always exist. We also de\ufb01ne \u02c6f(x) := \u2212log det x + \u27e8S, x\u27e9, and A := \u03bbI.\nTo Q1 and Q2, we associate norms and continuous, strongly convex functions, called prox-\nfunctions, d1(x) and d2(u). For Q1 we choose the Frobenius norm, and a prox-function\n8\nModel Selection Through Sparse Max Likelihood Estimation\nd1(x) = \u2212log det x + log b.\nFor Q2, we choose the Frobenius norm again, and a prox-\nfunction d2(x) = \u2225u\u22252\nF /2.\nThe method applies a smoothing technique to the non-smooth problem (8), which replaces\nthe objective of the original problem, f(x), by a penalized function involving the prox-\nfunction d2(u):\n\u02dcf(x) = \u02c6f(x) + max\nu\u2208Q2{\u27e8Ax, u\u27e9\u2212\u00b5d2(u)}.\n(10)\nThe above function turns out to be a smooth uniform approximation to f everywhere. It\nis di\ufb00erentiable, convex on Q1, and a has a Lipschitz-continuous gradient, with a constant\nL that can be computed as detailed below. A speci\ufb01c gradient scheme is then applied to\nthis smooth approximation, with convergence rate O(L/\u01eb).\n4.2 Algorithm and complexity estimate.\nTo detail the algorithm and compute the complexity, we must \ufb01rst calculate some pa-\nrameters corresponding to our de\ufb01nitions and choices above. First, the strong convexity\nparameter for d1(x) on Q1 is \u03c31 = 1/b2, in the sense that\n\u22072d1(X)[H, H] = trace(X\u22121HX\u22121H) \u2265b\u22122\u2225H\u22252\nF\nfor every symmetric H. Furthermore, the center of the set Q1 is x0 := arg minx\u2208Q1 d1(x) =\nbI, and satis\ufb01es d1(x0) = 0. With our choice, we have D1 := maxx\u2208Q1 d1(x) = p log(b/a).\nSimilarly, the strong convexity parameter for d2(u) on Q2 is \u03c32 := 1, and we have\nD2 := max\nu\u2208Q2 d2(U) = p2/2.\nWith this choice, the center of the set Q2 is u0 := arg minu\u2208Q2 d2(u) = 0.\nFor a desired accuracy \u01eb, we set the smoothness parameter \u00b5 := \u01eb/2D2, and set x0 = bI.\nThe algorithm proceeds as follows:\nFor k \u22650 do\n1. Compute \u2207\u02dcf(xk) = \u2212x\u22121 + S + u\u2217(xk), where u\u2217(x) solves (10).\n2. Find yk = arg miny {\u27e8\u2207\u02dcf(xk), y \u2212xk\u27e9+ 1\n2L(\u01eb)\u2225y \u2212xk\u22252\nF : y \u2208Q1}.\n3. Find zk = arg minx {L(\u01eb)\n\u03c31 d1(X) + Pk\ni=0\ni+1\n2 \u27e8\u2207\u02dcf(xi), x \u2212xi\u27e9: x \u2208Q1}.\n9\nBanerjee, El Ghaoui, and d\u2019Aspremont\n4. Update xk =\n2\nk+3zk + k+1\nk+3yk.\nIn our case, the Lipschitz constant for the gradient of our smooth approximation to the\nobjective function is\nL(\u01eb) := M + D2\u2225A\u22252/(2\u03c32\u01eb)\nwhere M := 1/a2 is the Lipschitz constant for the gradient of \u02dcf, and the norm \u2225A\u2225is\ninduced by the Frobenius norm, and is equal to \u03bb.\nThe algorithm is guaranteed to produce an \u01eb-suboptimal solution after a number of steps\nnot exceeding\nN(\u01eb) :=\n4\u2225A\u2225\nrD1D2\n\u03c31\u03c32\n\u00b7 1\n\u01eb +\nq\nMD1\n\u03c31\u01eb\n= (\u03ba\np\n(log \u03ba))(4p1.5a\u03bb/\n\u221a\n2 + \u221a\u01ebp)/\u01eb.\n(11)\nwhere \u03ba = b/a is a bound on the condition number of the solution.\nNow we are ready to estimate the complexity of the algorithm. For Step 1, the gradient of\nthe smooth approximation is computed in closed form by taking the inverse of x. Step 2\nessentially amounts to projecting on Q1, and requires that we solve an eigenvalue problem.\nThe same is true for Step 3. In fact, each iteration costs O(p3). The number of iterations\nnecessary to achieve an objective with absolute accuracy less than \u01eb is given in (11) by\nN(\u01eb) = O(p1.5/\u01eb). Thus, if the condition number \u03ba is \ufb01xed in advance, the complexity of\nthe algorithm is O(p4.5/\u01eb).\n5. Binary Variables: Approximate Sparse Maximum Likelihood\nEstimation\nIn this section, we consider the problem of estimating an undirected graphical model for\nmultivariate binary data. Recently, Wainwright et al. [2006] applied an \u21131-norm penalty to\nthe logistic regression problem to obtain a binary version of the high-dimensional consistency\nresults of Meinshausen and B\u00a8uhlmann [2006]. We apply the log determinant relaxation of\nWainwright and Jordan [2006] to formulate an approximate sparse maximum likelihood\n(ASML) problem for estimating the parameters in a multivariate binary distribution. We\nshow that the resulting problem is the same as the Gaussian sparse maximum likelihood\n(SML) problem, and that we can therefore apply our previously-developed algorithms to\nsparse model selection in a binary setting.\n10\nModel Selection Through Sparse Max Likelihood Estimation\nConsider a distribution made up of p binary random variables. Using n data samples, we\nwish to estimate the structure of the distribution. The logistic model of this distribution is\np(x; \u03b8) = exp{\np\nX\ni=1\n\u03b8ixi +\np\u22121\nX\ni=1\np\nX\nj=i+1\n\u03b8ijxixj \u2212A(\u03b8)}\n(12)\nwhere\nA(\u03b8) = log\nX\nx\u2208X p\nexp{\np\nX\ni=1\n\u03b8ixi +\np\u22121\nX\ni=1\np\nX\nj=i+1\n\u03b8ijxixj}\n(13)\nis the log partition function.\nThe sparse maximum likelihood problem in this case is to maximize (12) with an added\n\u21131-norm penalty on terms \u03b8kj.\nSpeci\ufb01cally, in the undirected graphical model, an edge\nbetween nodes k and j is missing if \u03b8kj = 0.\nA well-known di\ufb03culty is that the log partition function has too many terms in its outer\nsum to compute. However, if we use the log determinant relaxation for the log partition\nfunction developed by Wainwright and Jordan [2006], we can obtain an approximate sparse\nmaximum likelihood (ASML) estimate. We shall set up the problem in the next section.\n5.1 Problem formulation.\nLet\u2019s begin with some notation. Letting d := p(p + 1)/2, de\ufb01ne the map R : Rd \u2192Sp+1 as\nfollows:\nR(\u03b8) =\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\n0\n\u03b81\n\u03b82\n. . .\n\u03b8p\n\u03b81\n0\n\u03b812\n. . .\n\u03b81p\n...\n\u03b8p\n\u03b81p\n\u03b82p\n. . .\n0\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\nSuppose that our n samples are z(1), . . . , z(n) \u2208{\u22121, +1}p. Let \u00afzi and \u00afzij denote sample\nmean and second moments. The sparse maximum likelihood problem is\n\u02c6\u03b8exact := arg max\n\u03b8\n1\n2\u27e8R(\u03b8), R(\u00afz)\u27e9\u2212A(\u03b8) \u2212\u03bb\u2225\u03b8\u22251.\n(14)\nFinally de\ufb01ne the constant vector m = (1, 4\n3, . . . , 4\n3) \u2208Rp+1. Wainwright and Jordan [2006]\ngive an upper bound on the log partition function as the solution to the following variational\nproblem:\nA(\u03b8) \u2264max\u00b5 1\n2 log det(R(\u00b5) + diag(m)) + \u27e8\u03b8, \u00b5\u27e9\n= 1\n2 \u00b7 max\u00b5 log det(R(\u00b5) + diag(m)) + \u27e8R(\u03b8), R(\u00b5)\u27e9.\n(15)\n11\nBanerjee, El Ghaoui, and d\u2019Aspremont\nIf we use the bound (15) in our sparse maximum likelihood problem (14), we won\u2019t be able\nto extract an optimizing argument \u02c6\u03b8. Our \ufb01rst step, therefore, will be to rewrite the bound\nin a form that will allow this.\nLemma 5 We can rewrite the bound (15) as\nA(\u03b8) \u2264p\n2 log(e\u03c0\n2 ) \u22121\n2(p + 1) \u22121\n2 \u00b7 {max\n\u03bd\n\u03bdT m + log det(\u2212(R(\u03b8) + diag(\u03bd))).\n(16)\nUsing this version of the bound (15), we have the following theorem.\nTheorem 6 Using the upper bound on the log partition function given in (16), the approx-\nimate sparse maximum likelihood problem has the following solution:\n\u02c6\u03b8k = \u00af\u00b5k\n\u02c6\u03b8kj = \u2212(\u02c6\u0393)\u22121\nkj\n(17)\nwhere the matrix \u02c6\u0393 is the solution to the following problem, related to (2):\n\u02c6\u0393 := arg max{log det W : Wkk = Skk + 1\n3, |Wkj \u2212Skj| \u2264\u03bb}.\n(18)\nHere, S is de\ufb01ned as before:\nS = 1\nn\nn\nX\nk=1\n(z(k) \u2212\u00af\u00b5)(z(k) \u2212\u00af\u00b5)T\nwhere \u00af\u00b5 is the vector of sample means \u00afzi.\nIn particular, this means that we can reuse the algorithms developed in Sections 3 and\n?? for problems with binary variables. The relaxation (15) is the simplest one o\ufb00ered by\nWainwright and Jordan [2006]. The relaxation can be tightened by adding linear constraints\non the variable \u00b5.\n5.2 Penalty parameter choice for binary variables.\nFor the choice of the penalty parameter \u03bb, we can derive a formula analogous to (3).\nConsider the choice\n\u03bb(\u03b1)bin := (\u03c72(\u03b1/2p2, 1))\n1\n2\n(mini>j \u02c6\u03c3i\u02c6\u03c3j)\u221an\n(19)\n12\nModel Selection Through Sparse Max Likelihood Estimation\nwhere \u03c72(\u03b1, 1) is the (100 \u2212\u03b1)% point of the chi-square distribution for one degree of\nfreedom. Since our variables take on values in {\u22121, 1}, the empirical variances are of the\nform:\n\u02c6\u03c32\ni = 1 \u2212\u00af\u00b52\ni .\nUsing (19), we have the following binary version of Theorem 2:\nTheorem 7 With (19) chosen as the penalty parameter in the approximate sparse maxi-\nmum likelihood problem, for a \ufb01xed level \u03b1,\nP(\u2203k \u2208{1, . . . , p} : \u02c6C\u03bb\nk \u0338\u2286Ck) \u2264\u03b1.\n6. Numerical Results\nIn this section we present the results of some numerical experiments, both on synthetic and\nreal data.\n6.1 Synthetic experiments.\nSynthetic experiments require that we generate underlying sparse inverse covariance matri-\nces. To this end, we \ufb01rst randomly choose a diagonal matrix with positive diagonal entries.\nA given number of nonzeros are inserted in the matrix at random locations symmetrically.\nPositive de\ufb01niteness is ensured by adding a multiple of the identity to the matrix if needed.\nThe multiple is chosen to be only as large as necessary for inversion with no errors.\n6.1.1 Sparsity and thresholding.\nA very simple approach to obtaining a sparse estimate of the inverse covariance matrix\nwould be to apply a threshold to the inverse empirical covariance matrix, S\u22121. However,\neven when S is easily invertible, it can be di\ufb03cult to select a threshold level. We solved a\nsynthetic problem of size p = 100 where the true concentration matrix density was set to\n\u03b4 = 0.1. Drawing n = 200 samples, we plot in Figure (1) the sorted absolute value elements\nof S\u22121 on the left and \u02c6\u03a3\u22121 on the right.\nIt is clearly easier to choose a threshold level for the SML estimate. Applying a threshold to\neither S\u22121 or \u02c6\u03a3\u22121 would decrease the log likelihood of the estimate by an unknown amount.\nWe only observe that to preserve positive de\ufb01niteness, the threshold level t must satisfy the\nbound\nt \u2264min\n\u2225v\u22251\u22641 vT S\u22121v.\n13\nBanerjee, El Ghaoui, and d\u2019Aspremont\n0\n2000\n4000\n6000\n8000\n10000\n0\n2\n4\n6\n8\n10\n(A)\n0\n2000\n4000\n6000\n8000\n10000\n0\n2\n4\n6\n8\n10\n(B)\nFigure 1: Sorted absolute value of elements of (A) S\u22121 and (B) \u02c6\u03a3\u22121. The solution \u02c6\u03a3\u22121 to\n(1) is un-thresholded.\n6.1.2 Recovering structure.\nWe begin with a small experiment to test the ability of the method to recover the sparse\nstructure of an underlying covariance matrix. Figure 2 (A) shows a sparse inverse covariance\nmatrix of size p = 30. Figure 2 (B) displays a corresponding S\u22121, using n = 60 samples.\nFigure 2 (C) displays the solution to (1) for \u03bb = 0.1. The value of the penalty parameter\nhere is chosen arbitrarily, and the solution is not thresholded. Nevertheless, we can still\npick out features that were present in the true underlying inverse covariance matrix.\nA\n10\n20\n30\n5\n10\n15\n20\n25\n30\nB\n10\n20\n30\n5\n10\n15\n20\n25\n30\nC\n10\n20\n30\n5\n10\n15\n20\n25\n30\nFigure 2: Recovering the sparsity pattern.\nWe plot (A) the original inverse covariance\nmatrix \u03a3\u22121, (B) the noisy sample inverse S\u22121, and (C) the solution to problem\n(1) for \u03bb = 0.1.\nUsing the same underlying inverse covariance matrix, we repeat the experiment using\nsmaller sample sizes.\nWe solve (1) for n = 30 and n = 20 using the same arbitrarily\nchosen penalty parameter value \u03bb = 0.1, and display the solutions in Figure (3). As ex-\npected, our ability to pick out features of the true inverse covariance matrix diminishes with\nthe number of samples. This is an added reason to choose a larger value of \u03bb when we have\nfewer samples, as in (3).\n14\nModel Selection Through Sparse Max Likelihood Estimation\nA\n10\n20\n30\n5\n10\n15\n20\n25\n30\nB\n10\n20\n30\n5\n10\n15\n20\n25\n30\nC\n10\n20\n30\n5\n10\n15\n20\n25\n30\nFigure 3: Recovering the sparsity pattern for small sample size. We plot (A) the original\ninverse covariance matrix \u03a3\u22121, (B) the solution to problem (1) for n = 30 and\n(C) the solution for n = 20. A penalty parameter of \u03bb = 0.1 is used for (B) and\n(C).\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n\u22120.4\n\u22120.2\n0\n0.2\n0.4\n0.6\n0.8\n1\nPSfrag replacements\n\u02c6\u03a3\u22121\nij\n\u03bb\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\n\u22120.7\n\u22120.6\n\u22120.5\n\u22120.4\n\u22120.3\n\u22120.2\n\u22120.1\n0\n0.1\n0.2\n0.3\nPSfrag replacements\n\u02c6\u03a3\u22121\nij\n\u03bb\n\u03bb\n\u02c6\u03a3\u22121\nij\nFigure 4: Path following: elements of solution to (1) as \u03bb increases. Red lines correspond to\nelements that are zero in the true inverse covariance matrix; blue lines correspond\nto true nonzeros. Vertical lines mark a range of \u03bb values using which we recover\nthe sparsity pattern exactly.\n6.1.3 Path following experiments.\nFigure (4) shows two path following examples. We solve two randomly generated problems\nof size p = 5 and n = 100 samples. The red lines correspond to elements of the solution\nthat are zero in the true underlying inverse covariance matrix. The blue lines correspond\nto true nonzeros.\nThe vertical lines mark ranges of \u03bb for which we recover the correct\nsparsity pattern exactly. Note that, by Theorem 4, for \u03bb values greater than those shown,\nthe solution will be diagonal.\n15\nBanerjee, El Ghaoui, and d\u2019Aspremont\n\u22120.5\n0\n0.5\n1\n0\n1\n2\n3\n4\n5\n6\n7\nPSfrag replacements\nError (in %)\nlog(\u03bb/\u03c3)\nFigure 5: Recovering sparsity pattern in a matrix with added uniform noise of size \u03c3 = 0.1.\nWe plot the average percentage or misclassi\ufb01ed entries as a function of log(\u03bb/\u03c3).\nOn a related note, we observe that (1) also works well in recovering the sparsity pattern of a\nmatrix masked by noise. The following experiment illustrates this observation. We generate\na sparse inverse covariance matrix of size p = 50 as described above. Then, instead of using\nan empirical covariance S as input to (1), we use S = (\u03a3\u22121 + V )\u22121, where V is a randomly\ngenerated uniform noise of size \u03c3 = 0.1. We then solve (1) for various values of the penalty\nparameter \u03bb.\nIn \ufb01gure 5, for a each of value of \u03bb shown, we randomly selected 10 sample covariance ma-\ntrices S of size p = 50 and computed the number of misclassi\ufb01ed zeros and nonzero elements\nin the solution to (1). We plot the average percentage of errors (number of misclassi\ufb01ed\nzeros plus misclassi\ufb01ed nonzeros divided by p2), as well as error bars corresponding to one\nstandard deviation. As shown, the error rate is nearly zero on average when the penalty is\nset to equal the noise level \u03c3.\n6.1.4 CPU times versus problem size.\nFor a sense of the practical performance of the Nesterov method and the block coordinate\ndescent method, we randomly selected 10 sample covariance matrices S for problem sizes p\nranging from 400 to 1000.In each case, the number of samples n was chosen to be about a\nthird of p. In \ufb01gure 6 we plot the average CPU time to achieve a duality gap of \u01eb = 1. CPU\ntimes were computed using an AMD Athlon 64 2.20Ghz processor with 1.96GB of RAM.\n16\nModel Selection Through Sparse Max Likelihood Estimation\n400\n500\n600\n700\n800\n900\n1000\n0\n1000\n2000\n3000\n4000\n5000\n6000\n7000\n8000\n9000\nProblem size p\nAverage CPU times in seconds\nFigure 6: Average CPU times vs. problem size using block coordinate descent. We plot the\naverage CPU time (in seconds) to reach a gap of \u01eb = 0.1 versus problem size p.\nAs shown, we are typically able to solve a problem of size p = 1000 in about two and half\nhours.\n6.1.5 Performance as a binary classifier.\nIn this section we numerically examine the ability of the sparse maximum likelihood (SML)\nmethod to correctly classify elements of the inverse covariance matrix as zero or nonzero. For\ncomparision, we will use the Lasso estimate of Meinshausen and B\u00a8uhlmann [2006], which\nhas been shown to perform extremely well. The Lasso regresses each variable against all\nothers one at a time. Upon obtaining a solution \u03b8(k) for each variable k, one can estimate\nsparsity in one of two ways: either by declaring an element \u02c6\u03a3ij nonzero if both \u03b8(k)\ni\n\u0338= 0\nand \u03b8(k)\nj\n\u0338= 0 (Lasso-AND) or, less conservatively, if either of those quantities is nonzero\n(Lasso-OR).\nAs noted previously, Meinshausen and B\u00a8uhlmann [2006] have also derived a formula for\nchoosing their penalty parameter. Both the SML and Lasso penalty parameter formulas\ndepend on a chosen level \u03b1, which is a bound on the same error probability for each method.\nFor these experiments, we set \u03b1 = 0.05.\nIn the following experiments, we \ufb01xed the problem size p at 30 and generated sparse un-\nderlying inverse covariance matrices as described above. We varied the number of samples\nn from 10 to 310. For each value of n shown, we ran 30 trials in which we estimated the\nsparsity pattern of the inverse covariance matrix using the SML, Lasso-OR, and Lasso-AND\n17\nBanerjee, El Ghaoui, and d\u2019Aspremont\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nPositive predictive value\nRatio of samples to variables: n/p\nSML\nLASSO\u2212V\nLASSO\u2212&\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\nPower\nRatio of samples to variables: n/p\nSML\nLASSO\u2212V\nLASSO\u2212&\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n0\n0.005\n0.01\n0.015\n0.02\n0.025\n0.03\n0.035\n0.04\n0.045\n0.05\nDensity of solution\nRatio of samples to variables: n/p\nSML\nLASSO\u2212V\nLASSO\u2212&\nTrue\nFigure 7: Classifying zeros and nonzeros for a true density of \u03b4 = 0.05. We plot the positive\npredictive value, the power, and the estimated density using SML, Lasso-OR and\nLasso-AND.\nmethods. We then recorded the average number of nonzeros estimated by each method,\nand the average number of entries correctly identi\ufb01ed as nonzero (true positives).\nWe show two sets of plots. Figure (7) corresponds to experiments where the true density\nwas set to be low, \u03b4 = 0.05. We plot the power (proportion of correctly identi\ufb01ed nonzeros),\npositive predictive value (proportion of estimated nonzeros that are correct), and the density\nestimated by each method. Figure (8) corresponds to experiments where the true density\nwas set to be high, \u03b4 = 0.40, and we plot the same three quantities.\n18\nModel Selection Through Sparse Max Likelihood Estimation\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\n0.95\n1\nPositive predictive value\nRatio of samples to variables: n/p\nSML\nLASSO\u2212V\nLASSO\u2212&\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n0\n0.02\n0.04\n0.06\n0.08\n0.1\n0.12\n0.14\nPower\nRatio of samples to variables: n/p\nSML\nLASSO\u2212V\nLASSO\u2212&\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\u22120.05\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\nDensity of solution\nRatio of samples to variables: n/p\nSML\nLASSO\u2212V\nLASSO\u2212&\nTrue\nFigure 8: Classifying zeros and nonzeros for a true density of \u03b4 = 0.40. We plot the positive\npredictive value, the power, and the estimated density using SML, Lasso-OR and\nLasso-AND.\nMeinshausen and B\u00a8uhlmann [2006] report that, asymptotically, Lasso-AND and Lasso-OR\nyield the same estimate of the sparsity pattern of the inverse covariance matrix. At a \ufb01nite\nnumber of samples, the SML method seems to fall in in between the two methods in terms\nof power, positive predictive value, and the density of the estimate. It typically o\ufb00ers, on\naverage, the lowest total number of errors, tied with either Lasso-AND or Lasso-OR. Among\nthe two Lasso methods, it would seem that if the true density is very low, it is slightly better\nto use the more conservative Lasso-AND. If the density is higher, it may be better to use\nLasso-OR. When the true density is unknown, we can achieve an accuracy comparable to\n19\nBanerjee, El Ghaoui, and d\u2019Aspremont\nthe better choice among the Lasso methods by computing the SML estimate. Figure (9)\nshows one example of sparsity pattern recovery when the true density is low.\nA\nB\nC\nD\nFigure 9: Comparing sparsity pattern recovery to the Lasso. (A) true covariance (B) Lasso-\nOR (C) Lasso-AND (D) SML.\nThe Lasso and SML methods have a comparable computational complexity. However, unlike\nthe Lasso, the SML method is not parallelizable. Parallelization would render the Lasso a\nmore computationally attractive choice, since each variable can regressed against all other\nseparately, at an individual cost of O(p3). In exchange, SML can o\ufb00er a more accurate\nestimate of the sparsity pattern, as well as a well-conditioned estimate of the covariance\nmatrix itself.\n6.2 Gene expression and U.S. Senate voting records data.\nWe tested our algorithms on three sets of data: two gene expression data sets, as well as\nUS Senate voting records. In this section we brie\ufb02y explore the resulting graphical models.\n20\nModel Selection Through Sparse Max Likelihood Estimation\n6.2.1 Rosetta Inpharmatics compendium.\nWe applied our algorithms to the Rosetta Inpharmatics Compendium of gene expression\npro\ufb01les described by Hughes et al. [2000]. The 300 experiment compendium contains n =\n253 samples with p = 6136 variables. With a view towards obtaining a very sparse graph,\nwe replaced \u03b1/2p2 in (3) by \u03b1, and set \u03b1 = 0.05.\nThe resulting penalty parameter is\n\u03bb = 0.0313.\nThis is a large penalty for this data set, and by applying Theorem 4 we \ufb01nd that all but 270\nof the variables are estimated to be independent from all the rest, clearly a very conservative\nestimate. Figure (10) displays the resulting graph.\nFigure 10: Application to Hughes compendium. The above graph results from solving (1)\nfor this data set with a penalty parameter of \u03bb = 0.0313.\nFigure (11) closes in on a region of Figure (10), a cluster of genes that is unconnected to\nthe remaining genes in this estimate. According to Gene Ontology [see Consortium, 2000],\nthese genes are associated with iron homeostasis. The probability that a gene has been false\nincluded in this cluster is at most 0.05.\nAs a second example, in Figure (12), we show a subgraph of genes associated with cellular\nmembrane fusion. All three graphs were rendered using Cytoscape.\n21\nBanerjee, El Ghaoui, and d\u2019Aspremont\nFigure 11: Application to Hughes dataset (closeup of Figure (10). These genes are associ-\nated with iron homeostasis.\nFigure 12: Application to Hughes dataset (subgraph of Figure (10). These genes are asso-\nciated with cellular membrane fusion.\n22\nModel Selection Through Sparse Max Likelihood Estimation\nTable 1: Predictor genes for LDL receptor.\nAccession\nGene\nBF553500\nCbp/p300-interacting transactivator\nBF387347\nEST\nBF405996\ncalcium channel, voltage dependent\nNM 017158\ncytochrome P450, 2c39\nK03249\nenoyl-CoA, hydratase/3-hydroxyacyl Co A dehydrog.\nBE100965\nEST\nAI411979\nCarnitine O-acetyltransferase\nAI410548\n3-hydroxyisobutyryl-Co A hydrolase\nNM 017288\nsodium channel, voltage-gated\nY00102\nestrogen receptor 1\nNM 013200\ncarnitine palmitoyltransferase 1b\n6.2.2 Iconix microarray data.\nNext we analyzed a subset of a 10, 000 gene microarray dataset from 160 drug treated rat\nlivers Natsoulis et al. [2005]. In this study, rats were treated with a variety of \ufb01brate, statin,\nor estrogen receptor agonist compounds. Taking the 500 genes with the highest variance, we\nonce again replaced \u03b1/2p2 in (3) by \u03b1, and set \u03b1 = 0.05. The resulting penalty parameter\nis \u03bb = 0.0853.\nBy applying Theorem 4 we \ufb01nd that all but 339 of the variables are estimated to be inde-\npendent from the rest. This estimate is less conservative than that obtained in the Hughes\ncase since the ratio of samples to variables is 160 to 500 instead of 253 to 6136.\nThe \ufb01rst order neighbors of any node in a Gaussian graphical model form the set of pre-\ndictors for that variable. In the estimated obtained by solving (1), we found that LDL\nreceptor had one of the largest number of \ufb01rst-order neighbors in the Gaussian graphical\nmodel. The LDL receptor is believed to be one of the key mediators of the e\ufb00ect of both\nstatins and estrogenic compounds on LDL cholesterol. Table 1 lists some of the \ufb01rst order\nneighbors of LDL receptor.\nIt is perhaps not surprising that several of these genes are directly involved in either lipid\nor steroid metabolism (K03249, AI411979, AI410548, NM 013200, Y00102). Other genes\nsuch as Cbp/p300 are known to be global transcriptional regulators. Finally, some are un-\nannotated ESTs. Their connection to the LDL receptor in this analysis may provide clues\nto their function.\n23\nBanerjee, El Ghaoui, and d\u2019Aspremont\n6.2.3 Senate voting records data.\nWe conclude our numerical experiments by testing our approximate sparse maximum likeli-\nhood estimation method on binary data. The data set consists of US senate voting records\ndata from the 109th congress (2004 - 2006). There are one hundred variables, correspond-\ning to 100 senators. Each of the 542 samples is bill that was put to a vote. The votes are\nrecorded as -1 for no and 1 for yes.\nThere are many missing values in this dataset, corresponding to missed votes. Since our\nanalysis depends on data values taken solely from {\u22121, 1}, it was necessary to impute values\nto these. For this experiment, we replaced all missing votes with noes (-1). We chose the\npenalty parameter \u03bb(\u03b1) according to (19), using a signi\ufb01cance level of \u03b1 = 0.05. Figure\n(13) shows the resulting graphical model, rendered using Cytoscape. Red nodes correspond\nto Republican senators, and blue nodes correspond to Democratic senators.\nWe can make some tentative observations by browsing the network of senators. As neighbors\nmost Democrats have only other Democrats and Republicans have only other Republicans.\nSenator Chafee (R, RI) has only democrats as his neighbors, an observation that supports\nmedia statements made by and about Chafee during those years. Senator Allen (R, VA)\nunites two otherwise separate groups of Republicans and also provides a connection to\nthe large cluster of Democrats through Ben Nelson (D, NE). Senator Lieberman (D, CT)\nis connected to other Democrats only through Kerry (D, MA), his running mate in the\n2004 presidential election. These observations also match media statements made by both\npundits and politicians. Thus, although we obtained this graphical model via a relaxation\nof the log partition function, the resulting picture is largely supported by conventional\nwisdom. Figure (14) shows a subgraph consisting of neighbors of degree three or lower of\nSenator Allen.\nAcknowledgments\nWe are indebted to Georges Natsoulis for his interpretation of the Iconix dataset analysis\nand for gathering the senate voting records. We also thank Martin Wainwright, Bin Yu,\nPeter Bartlett, and Michael Jordan for many enlightening conversations.\nReferences\nD. Bertsekas. Nonlinear Programming. Athena Scienti\ufb01c, 1998.\nGene Ontology Consortium. Gene ontology: tool for the uni\ufb01cation of biology. Nature\nGenet., 25:25\u201329, 2000.\n24\nModel Selection Through Sparse Max Likelihood Estimation\nFigure 13: US Senate, 109th Congress (2004-2006).\nThe graph displays the solution to\n(14) obtained using the log determinant relaxation to the log partition function\nof Wainwright and Jordan [2006].\nDemocratic senators are colored blue and\nRepublican senators are colored red.\n25\nBanerjee, El Ghaoui, and d\u2019Aspremont\nFigure 14: US Senate, 109th Congress. Neighbors of Senator Allen (degree three or lower).\nJ. Dahl, V. Roychowdhury, and L. Vandenberghe.\nCovariance selection for non-chordal\ngraphs via chordal embedding. Submitted to Optimization Methods and Software, 2006.\nAlexandre d\u2019Aspremont, Laurent El Ghaoui, M.I. Jordan, and G. R. G. Lanckriet.\nA\ndirect formulation for sparse PCA using semide\ufb01nite programming. Advances in Neural\nInformation Processing Systems, 17, 2004.\nA. Dobra and M. West. Bayesian covariance selection. Working paper, ISDS, Duke Uni-\nversity, 2004.\nJ. Z. Huang, N. Liu, and M. Pourahmadi. Covariance selection and estimattion via penalized\nnormal likelihood. Wharton Preprint, 2005.\nT. R. Hughes, M. J. Marton, A. R. Jones, C. J. Roberts, R. Stoughton, C. D. Armour, H. A.\nBennett, E. Co\ufb00ey, H. Dai, Y. D. He, M. J. Kidd, A. M. King, M. R. Meyer, D. Slade,\nP. Y. Lum, S. B. Stepaniants, D. D. Shoemaker, D. Gachotte, K. Chakraburtty, J. Simon,\nM. Bard, and S. H. Friend. Functional discovery via a compendium of expression pro\ufb01les.\nCell, 102(1):109\u2013126, 2000.\nS. Lauritzen. Graphical Models. Springer Verlag, 1996.\nH. Li and J. Gui. Gradient directed regularization for sparse gaussian concentration graphs,\nwith applications to inference of genetic networks. University of Pennsylvania Technical\nReport, 2005.\nZ. Q. Luo and P. Tseng. On the convergence of the coordinate descent method for convex\ndi\ufb00erentiable minimization.\nJournal of Optimization Theory and Applications, 72(1):\n7\u201335, 1992.\n26\nModel Selection Through Sparse Max Likelihood Estimation\nN. Meinshausen and P. B\u00a8uhlmann. High dimensional graphs and variable selection with the\nlasso. Annals of statistics, 34:1436\u20131462, 2006.\nG. Natsoulis, L. El Ghaoui, G. Lanckriet, A. Tolley, F. Leroy, S. Dunlea, B. Eynon, C. Pear-\nson, S. Tugendreich, and K. Jarnagin. Classi\ufb01cation of a large microarray data set: al-\ngorithm comparison and analysis of drug signatures.\nGenome Research, 15:724 \u2013736,\n2005.\nY. Nesterov. Smooth minimization of non-smooth functions. Math. Prog., 103(1):127\u2013152,\n2005.\nR. Tibshirani.\nRegression shrinkage and selection via the lasso.\nJournal of the Royal\nstatistical society, series B, 58(267-288), 1996.\nLieven Vandenberghe, Stephen Boyd, and Shao-Po Wu. Determinant maximization with\nlinear matrix inequality constraints. SIAM Journal on Matrix Analysis and Applications,\n19(4):499 \u2013 533, 1998.\nM. Wainwright and M. Jordan. Log-determinant relaxation for approximate inference in\ndiscrete markov random \ufb01elds. IEEE Transactions on Signal Processing, 2006.\nM. J. Wainwright, P. Ravikumar, and J. D. La\ufb00erty. High-dimensional graphical model\nselection using \u21131-regularized logistic regression. Proceedings of Advances in Neural In-\nformation Processing Systems, 2006.\nAppendix A. Proof of solution properties and block coordinate descent\nconvergence\nIn this section, we give short proofs of the two theorems on properties of the solution to\n(1), as well as the convergence of the block coordinate descent method.\nProof of Theorem 1:\nSince \u02c6\u03a3 satis\ufb01es \u02c6\u03a3 = S + \u02c6U, where \u2225U\u2225\u221e\u2264\u03bb, we have:\n\u2225\u02c6\u03a3\u22252 = \u2225S + \u02c6U\u22252\n\u2264\u2225S\u22252 + \u2225U\u22252 \u2264\u2225S\u22252 + \u2225U\u2225\u221e\u2264\u2225S\u22252 + \u03bbp\nwhich yields the lower bound on \u2225\u02c6\u03a3\u22121\u22252. Likewise, we can show that \u2225\u02c6\u03a3\u22121\u22252 is bounded\nabove. At the optimum, the primal dual gap is zero:\n\u2212log det \u02c6\u03a3\u22121 + trace(S \u02c6\u03a3\u22121) + \u03bb\u2225\u02c6\u03a3\u22121\u22251 \u2212log det \u02c6\u03a3 \u2212p\n= trace(S \u02c6\u03a3\u22121) + \u03bb\u2225\u02c6\u03a3\u22121\u22251 \u2212p = 0\n27\nBanerjee, El Ghaoui, and d\u2019Aspremont\nWe therefore have\n\u2225\u02c6\u03a3\u22121\u22252 \u2264\u2225\u02c6\u03a3\u22121\u2225F \u2264\u2225\u02c6\u03a3\u22121\u22251\n= p/\u03bb \u2212trace(S \u02c6\u03a3\u22121)/\u03bb \u2264p/\u03bb\nwhere the last inequality follows from trace(S \u02c6\u03a3\u22121) \u22650, since S \u2ab00 and \u02c6\u03a3\u22121 \u227b0.\nNext we prove the convergence of block coordinate descent:\nProof of Theorem 3:\nTo see that optimizing over one row and column of W in (2) yields the quadratic program\n(4), let all but the last row and column of W be \ufb01xed. Since we know the diagonal entries\nof the solution, we can \ufb01x the remaining diagonal entry as well:\nW =\n\u0012W\\p\\p\nwp\nwT\np\nWpp\n\u0013\nThen, using Schur complements, we have that\ndet W = det W\\p\\p \u00b7 (Wpp \u2212wT\np (W\\p\\p)\u22121wp)\nwhich gives rise to (4).\nBy general results on block coordinate descent algorithms [e.g., Bertsekas, 1998], the algo-\nrithms converges if and only if (4) has a unique solution at each iteration. Thus it su\ufb03ces\nto show that, at every sweep, W (j) \u227b0 for all columns j. Prior to the \ufb01rst sweep, the initial\nvalue of the variable is positive de\ufb01nite: W (0) \u227b0 since W (0) := S + \u03bbI, and we have S \u2ab00\nand \u03bb > 0 by assumption.\nNow suppose that W (j) \u227b0. This implies that the following Schur complement is positive:\nwjj \u2212W T\nj (W (j)\n\\j\\j)\u22121Wj > 0\nBy the update rule we have that the corresponding Schur complement for W (j+1) is even\ngreater:\nwjj \u2212W T\nj (W (j+1)\n\\j\\j )\u22121Wj > wjj \u2212W T\nj (W (j)\n\\j\\j)\u22121Wj > 0\nso that W (j+1) \u227b0.\nFinally, we apply Theorem 3 to prove the second property of the solution.\nProof of Theorem 4:\n28\nModel Selection Through Sparse Max Likelihood Estimation\nSuppose that column j of the second moment matrix satis\ufb01es |Sij| \u2264\u03bb for all i \u0338= j. This\nmeans that the zero vector is in the constraint set of (4) for that column. Each time we\nreturn to column j, the objective function will be di\ufb00erent, but always of the form yT Ay\nfor A \u227b0. Since the constraint set will not change, the solution for column j will always be\nzero. By Theorem 3, the block coordinate descent algorithm converges to a solution, and\nso therefore the solution must have \u02c6\u03a3j = 0.\nAppendix B. Proof of error bounds.\nNext we shall show that the penalty parameter choice given in (3) yields the error probability\nbound of Theorem 2. The proof is nearly identical to that of [Meinshausen and B\u00a8uhlmann,\n2006,Theorem\n3]. The di\ufb00erences stem from a di\ufb00erent objective function, and the fact\nthat our variable is a matrix of size p rather than a vector of size p. Our proof is only an\nadaptation of their proof to our problem.\nB.1 Preliminaries\nBefore we begin, consider problem (1), for a matrix S of any size:\n\u02c6X = arg min \u2212log det X + trace(SX) + \u03bb\u2225X\u22251\nwhere we have dropped the constraint X \u227b0 since it is implicit, due to the log determinant\nfunction. Since the problem is unconstrained, the solution \u02c6X must correspond to setting\nthe subgradient of the objective to zero:\nSij \u2212X\u22121\nij\n= \u2212\u03bb\nfor Xij > 0\nSij \u2212X\u22121\nij\n= \u03bb\nfor Xij < 0\n|Sij \u2212X\u22121\nij | \u2264\u03bb\nfor Xij = 0\n(20)\nRecall that by Theorem 1, the solution is unique for \u03bb positive.\nB.2 Proof of error bound for Gaussian data.\nNow we are ready to prove Theorem 2.\nProof of Theorem 2:\nSort columns of the covariance matrix so that variables in the same connectivity component\nare grouped together.\nThe correct zero pattern for the covariance matrix is then block\ndiagonal. De\ufb01ne\n\u03a3correct := blk diag(C1, . . . , C\u2113)\n(21)\n29\nBanerjee, El Ghaoui, and d\u2019Aspremont\nThe inverse (\u03a3correct)\u22121 must also be block diagonal, with possible additional zeros inside\nthe blocks. If we constrain the solution to (1) to have this structure, then by the form of\nthe objective, we can optimize over each block separately. For each block, the solution is\ncharacterized by (20).\nNow, suppose that\n\u03bb >\nmax\ni\u2208N,j\u2208N\\Ci\n|Sij \u2212\u03a3correct\nij\n|\n(22)\nThen, by the subgradient characterization of the solution noted above, and the fact that\nthe solution is unique for \u03bb > 0, it must be the case that \u02c6\u03a3 = \u03a3correct. By the de\ufb01nition\nof \u03a3correct, this implies that, for \u02c6\u03a3, we have \u02c6Ck = Ck for all k \u2208N.\nTaking the contrapositive of this statement, we can write:\nP(\u2203k \u2208N : \u02c6Ck \u0338\u2286Ck)\n\u2264P(maxi\u2208N,j\u2208N\\Ci |Sij \u2212\u03a3correct\nij\n| \u2265\u03bb)\n\u2264p2(n) \u00b7 maxi\u2208N,j\u2208N\\Ci P(|Sij \u2212\u03a3correct\nij\n| \u2265\u03bb)\n= p2(n) \u00b7 maxi\u2208N,j\u2208N\\Ci P(|Sij| \u2265\u03bb)\n(23)\nThe equality at the end follows since, by de\ufb01nition, \u03a3correct\nij\n= 0 for j \u2208N\\Ci. It remains\nto bound P(|Sij| \u2265\u03bb).\nThe statement |Skj| \u2265\u03bb can be written as:\n|Rkj|(1 \u2212R2\nkj)\u22121\n2 \u2265\u03bb(skksjj \u2212\u03bb2)\u22121\n2\nwhere Rkj is the correlation between variables k and j, since\n|Rkj|(1 \u2212R2\nkj)\u22121\n2 = |Skj|(SkkSjj \u2212S2\nkj)\u22121\n2\nFurthermore, the condition j \u2208N\\Ck is equivalent to saying that variables k and j are\nindependent: \u03a3kj = 0. Conditional on this, the statistic\nRkj(1 \u2212R2\nkj)\u22121\n2(n \u22122)\n1\n2\nhas a Student\u2019s t-distribution for n \u22122 degrees of freedom. Therefore, for all j \u2208N\\Ck,\nP(|Skj| \u2265\u03bb|Skk = skk, Sjj = sjj)\n= 2P(Tn\u22122 \u2265\u03bb(skksjj \u2212\u03bb2)\u22121\n2(n \u22122)\n1\n2 |Skk = skk, Sjj = sjj)\n\u22642 \u02dcFn\u22122(\u03bb(\u02c6\u03c32\nk\u02c6\u03c32\nj \u2212\u03bb2)\u22121\n2(n \u22122)\n1\n2 )\n(24)\n30\nModel Selection Through Sparse Max Likelihood Estimation\nwhere \u02c6\u03c32\nk is the sample variance of variable k, and \u02dcFn\u22122 = 1 \u2212Fn\u22122 is the CDF of the\nStudent\u2019s t-distribution with n \u22122 degree of freedom. This implies that, for all j \u2208N\\Ck,\nP(|Skj| \u2265\u03bb) \u22642 \u02dcFn\u22122(\u03bb(\u02c6\u03c32\nk\u02c6\u03c32\nj \u2212\u03bb2)\u22121\n2 (n \u22122)\n1\n2 )\nsince P(A) =\nR\nP(A|B)P(B)dB \u2264K\nR\nP(B)dB = K. Putting the inequalities together,\nwe have that:\nP(\u2203k : \u02c6C\u03bb\nk \u0338\u2286Ck)\n\u2264p2 \u00b7 maxk,j\u2208N\\Ck 2 \u02dcFn\u22122(\u03bb(\u02c6\u03c32\nk\u02c6\u03c32\nj \u2212\u03bb2)\u22121\n2(n \u22122)\n1\n2 )\n= 2p2 \u02dcFn\u22122(\u03bb((n \u22122)/((maxi>j \u02c6\u03c3k\u02c6\u03c3j)2 \u2212\u03bb2))\n1\n2 )\nFor any \ufb01xed \u03b1, our required condition on \u03bb is therefore\n\u02dcFn\u22122(\u03bb((n \u22122)/((max\ni>j \u02c6\u03c3k\u02c6\u03c3j)2 \u2212\u03bb2))\n1\n2 ) = \u03b1/2p2\nwhich is satis\ufb01ed by choosing \u03bb according to (3).\nB.3 Proof of bound for binary data.\nWe can reuse much of the previous proof to derive a corresponding formula for the binary\ncase.\nProof of Theorem 7:\nThe proof of Theorem 7 is identical to the proof of Theorem 2, except that we have a\ndi\ufb00erent null distribution for |Skj|. The null distribution of\nnR2\nkj\nis chi-squared with one degree of freedom. Analogous to (24), we have:\nP(|Skj| \u2265\u03bb|Skk = skk, Sjj = sjj)\n= 2P(nR2\nkj \u2265n\u03bb2skksjj|Skk = skk, Sjj = sjj)\n\u22642 \u02dcG(n\u03bb2\u02c6\u03c32\nk\u02c6\u03c32\nj)\nwhere \u02c6\u03c32\nk is the sample variance of variable k, and \u02dcG = 1\u2212G is the CDF of the chi-squared\ndistribution with one degree of freedom. This implies that, for all j \u2208N\\Ck,\nP(|Skj| \u2265\u03bb) \u22642 \u02dcG((\u03bb\u02c6\u03c3k\u02c6\u03c3j\n\u221an)2)\n31\nBanerjee, El Ghaoui, and d\u2019Aspremont\nPutting the inequalities together, we have that:\nP(\u2203k : \u02c6C\u03bb\nk \u0338\u2286Ck)\n\u2264p2 \u00b7 maxk,j\u2208N\\Ck 2 \u02dcG((\u03bb\u02c6\u03c3k\u02c6\u03c3j\n\u221an)2)\n= 2p2 \u02dcG((mini>j \u02c6\u03c3k\u02c6\u03c3j)2n\u03bb2)\nso that, for any \ufb01xed \u03b1, we can achieve our desired bound by choosing \u03bb(\u03b1) according to\n(19).\nAppendix C. Proof of connection between Gaussian SML and binary\nASML\nWe end with a proof of Theorem 6, which connects the exact Gaussian sparse maximum\nlikelihood problem with the approximate sparse maximum likelihood problem obtained by\nusing the log determinant relaxation of Wainwright and Jordan [2006]. First we must prove\nLemma 5.\nProof of Lemma 5:\nThe conjugate function for the convex normalization A(\u03b8) is de\ufb01ned as\nA\u2217(\u00b5) := sup\n\u03b8\n{\u27e8\u00b5, \u03b8\u27e9\u2212A(\u03b8)}\n(25)\nWainwright and Jordan derive a lower bound on this conjugate function using an entropy\nbound:\nA\u2217(\u00b5) \u2265B\u2217(\u00b5)\n(26)\nSince our original variables are spin variables x {\u22121, +1}, the bound given in the paper is\nB\u2217(\u00b5) := \u22121\n2 log det(R(\u00b5) + diag(m)) \u2212p\n2 log(e\u03c0\n2 )\n(27)\nwhere m := (1, 4\n3, . . . , 4\n3).\nThe dual of this lower bound is B(\u03b8):\nB\u2217(\u00b5) := max\u03b8\u27e8\u03b8, \u00b5\u27e9\u2212B(\u03b8)\n\u2264max\u03b8\u27e8\u03b8, \u00b5\u27e9\u2212A(\u03b8) =: A\u2217(\u00b5)\n(28)\n32\nModel Selection Through Sparse Max Likelihood Estimation\nThis means that, for all \u00b5, \u03b8,\n\u27e8\u03b8, \u00b5\u27e9\u2212B(\u03b8) \u2264A\u2217(\u00b5)\n(29)\nor\nB(\u03b8) \u2265\u27e8\u03b8, \u00b5\u27e9\u2212A\u2217(\u00b5)\n(30)\nso that in particular\nB(\u03b8) \u2265max\n\u00b5 \u27e8\u03b8, \u00b5\u27e9\u2212A\u2217(\u00b5) =: A(\u03b8)\n(31)\nUsing the de\ufb01nition of B(\u03b8) and its dual B\u2217(\u00b5), we can write\nB(\u03b8) := max\u00b5\u27e8\u03b8, \u00b5\u27e9\u2212B\u2217(\u00b5)\n= p\n2 log(e\u03c0\n2 ) + max\u00b5 1\n2\u27e8R(\u03b8), R(\u00b5)\u27e9+ 1\n2 log det(R(\u00b5) + diag(m))\n= p\n2 log(e\u03c0\n2 ) + 1\n2 \u00b7 max{\u27e8R(\u03b8), X \u2212diag(m)\u27e9+ log det(X) : X \u227b0, diag(X) = m}\n= p\n2 log(e\u03c0\n2 ) + 1\n2 \u00b7 {maxX\u227b0 min\u03bd\u27e8R(\u03b8), X \u2212diag(m)\u27e9+ log det(X) + \u03bdT(diag(X) \u2212m)}\n= p\n2 log(e\u03c0\n2 ) + 1\n2 \u00b7 {maxX\u227b0 min\u03bd\u27e8R(\u03b8) + diag(\u03bd), X\u27e9+ log det(X) \u2212\u03bdTm}\n= p\n2 log(e\u03c0\n2 ) + 1\n2 \u00b7 {min\u03bd \u2212\u03bdT m + maxX\u227b0\u27e8R(\u03b8) + diag(\u03bd), X\u27e9+ log det(X)}\n= p\n2 log(e\u03c0\n2 ) + 1\n2 \u00b7 {min\u03bd \u2212\u03bdT m \u2212log det(\u2212(R(\u03b8) + diag(\u03bd))) \u2212(p + 1)}\n= p\n2 log(e\u03c0\n2 ) \u22121\n2(p + 1) + 1\n2 \u00b7 {min\u03bd \u2212\u03bdTm \u2212log det(\u2212(R(\u03b8) + diag(\u03bd)))}\n= p\n2 log(e\u03c0\n2 ) \u22121\n2(p + 1) \u22121\n2 \u00b7 {max\u03bd \u03bdT m + log det(\u2212(R(\u03b8) + diag(\u03bd\u03bb)}\n(32)\nNow we use lemma 5 to prove the main result of section 5.1. Having expressed the upper\nbound on the log partition function as a constant minus a maximization problem will help\nwhen we formulate the sparse approximate maximum likelihood problem.\nProof of Theorem 6:\n33\nBanerjee, El Ghaoui, and d\u2019Aspremont\nThe approximate sparse maximum likelihood problem is obtained by replacing the log par-\ntition function A(\u03b8) with its upper bound B(\u03b8), as derived in lemma 5:\nn \u00b7 {max\u03b8 1\n2\u27e8R(\u03b8), R(\u00afz)\u27e9\u2212B(\u03b8) \u2212\u03bb\u2225\u03b8\u22251}\n= n \u00b7 {max\u03b8 1\n2\u27e8R(\u03b8), R(\u00afz)\u27e9\u2212\u03bb\u2225\u03b8\u22251 + 1\n2(p + 1) \u2212p\n2 log(e\u03c0\n2 )\n+ 1\n2 \u00b7 {max\u03bd \u03bdTm + log det(\u2212(R(\u03b8) + diag(\u03bd)))}}\n= n\n2 (p + 1) \u2212np\n2 log(e\u03c0\n2 ) + n\n2 \u00b7 max\u03b8,\u03bd{\u03bdT m + \u27e8R(\u03b8), R(\u00afz)\u27e9\n+ log det(\u2212(R(\u03b8) + diag(\u03bd))) \u22122\u03bb\u2225\u03b8\u22251}\n(33)\nWe can collect the variables \u03b8 and \u03bd into an unconstrained symmetric matrix variable\nY := \u2212(R(\u03b8) + diag(\u03bd)).\nObserve that\n\u27e8R(\u03b8), R(\u00afz)\u27e9= \u27e8\u2212Y \u2212diag(\u03bd), R(\u00afz)\u27e9\n= \u2212\u27e8Y, R(\u00afz)\u27e9\u2212\u27e8diag(\u03bd), R(\u00afz)\u27e9= \u2212\u27e8Y, R(\u00afz)\u27e9\n(34)\nand that\n\u03bdT m = \u27e8diag(\u03bd), diag(\u03bd)\u27e9= \u27e8\u2212Y \u2212R(\u03b8), diag(m)\u27e9\n= \u2212\u27e8Y, diag(m)\u27e9\u2212\u27e8R(\u03b8), diag(m)\u27e9= \u2212\u27e8Y, diag(m)\u27e9\n(35)\nThe approximate sparse maximum likelihood problem can then be written in terms of Y :\nn\n2 (p + 1) \u2212np\n2 log(e\u03c0\n2 ) + n\n2 \u00b7 max\u03b8,\u03bd{\u03bdT m + \u27e8R(\u03b8), R(\u00afz)\u27e9\n+ log det(\u2212(R(\u03b8) + diag(\u03bd))) \u22122\u03bb\u2225\u03b8\u22251}\n= n\n2 (p + 1) \u2212np\n2 log(e\u03c0\n2 ) + n\n2 \u00b7 max{log det Y \u2212\u27e8Y, R(\u00afz) + diag(m)\u27e9\n\u22122\u03bb Pp\ni=2\nPp+1\nj=i+1 |Yij|}\n(36)\nIf we let M := R(\u00afz) + diag(m), then:\nM =\n\u00121\n\u00af\u00b5T\n\u00af\u00b5\nZ + 1\n3I\n\u0013\nwhere \u00af\u00b5 is the sample mean and\nZ = 1\nn\nn\nX\nk=1\nz(k)(z(k))T\n34\nModel Selection Through Sparse Max Likelihood Estimation\nDue to the added 1\n3I term, we have that M \u227b0 for any data set.\nThe problem can now be written as:\n\u02c6Y := arg max{log det Y \u2212\u27e8Y, M\u27e9\u22122\u03bb\np\nX\ni=2\np+1\nX\nj=i+1\n|Yij| : Y \u227b0}\n(37)\nSince we are only penalizing certain elements of the variable Y , the solution \u02c6X of the dual\nproblem to (37) will be of the form:\n\u02c6X =\n\u00121\n\u00af\u00b5T\n\u00af\u00b5\n\u02dcX\n\u0013\nwhere\n\u02dcX := arg max{log det V : Vkk = Zkk + 1\n3, |Vkj \u2212Zkj| \u2264\u03bb}.\nWe can write an equivalent problem for estimating the covariance matrix. De\ufb01ne a new\nvariable:\n\u0393 = V \u2212\u00af\u00b5\u00af\u00b5T\nUsing this variable, and the fact that the second moment matrix about the mean, de\ufb01ned\nas before, can be written\nS = 1\nn\nn\nX\nk=1\nz(k)(z(k))T \u2212\u00af\u00b5\u00af\u00b5T = Z \u2212\u00af\u00b5\u00af\u00b5T\nwe obtain the formulation (18). Using Schur complements, we see that our primal variable\nis of the form:\nY =\n\u0012\u2217\n\u2217\n\u2217\n\u02c6\u0393\u22121\n\u0013\nFrom our de\ufb01nition of the variable Y , we see that the parameters we are estimating, \u02c6\u03b8kj,\nare the negatives of the o\ufb00-diagonal elements of \u02c6\u0393\u22121, which gives us (17).\n35\n",
        "sentence": " These methods are readily adapted to matrix valued data and have been applied to covariance estimation (El Karoui, 2009; Bien & Tibshirani, 2010) and graphical model structure learning (Banerjee et al., 2007; Friedman et al., 2008).",
        "context": "additive perturbations of the second moment matrix S. A similar robustness interpretation\ncan be made for a number of estimation problems, such as support vector machines for\nclassi\ufb01cation.\nInformation Processing Systems, 17, 2004.\nA. Dobra and M. West. Bayesian covariance selection. Working paper, ISDS, Duke Uni-\nversity, 2004.\nJ. Z. Huang, N. Liu, and M. Pourahmadi. Covariance selection and estimattion via penalized\nWe tested our algorithms on three sets of data: two gene expression data sets, as well as\nUS Senate voting records. In this section we brie\ufb02y explore the resulting graphical models.\n20\nModel Selection Through Sparse Max Likelihood Estimation"
    },
    {
        "title": "A fast iterative shrinkagethresholding algorithm for linear inverse problems",
        "author": [
            "A. Beck",
            "M. Teboulle"
        ],
        "venue": "SIAM Journal of Imaging Sciences,",
        "citeRegEx": "Beck and Teboulle,? \\Q2009\\E",
        "shortCiteRegEx": "Beck and Teboulle",
        "year": 2009,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Incremental gradient, subgradient, and proximal methods for convex optimization: a survey",
        "author": [
            "D.P. Bertsekas"
        ],
        "venue": "Optimization for Machine Learning, pp",
        "citeRegEx": "Bertsekas,? \\Q2011\\E",
        "shortCiteRegEx": "Bertsekas",
        "year": 2011,
        "abstract": "We survey incremental methods for minimizing a sum $\\sum_{i=1}^mf_i(x)$\nconsisting of a large number of convex component functions $f_i$. Our methods\nconsist of iterations applied to single components, and have proved very\neffective in practice. We introduce a unified algorithmic framework for a\nvariety of such methods, some involving gradient and subgradient iterations,\nwhich are known, and some involving combinations of subgradient and proximal\nmethods, which are new and offer greater flexibility in exploiting the special\nstructure of $f_i$. We provide an analysis of the convergence and rate of\nconvergence properties of these methods, including the advantages offered by\nrandomization in the selection of components. We also survey applications in\ninference/machine learning, signal processing, and large-scale and distributed\noptimization.",
        "full_text": "arXiv:1507.01030v2  [cs.SY]  19 Dec 2017\nAugust 2010 (revised December 2010)\nReport LIDS - 2848\nIncremental Gradient, Subgradient, and Proximal Methods for\nConvex Optimization: A Survey\n1\nDimitri P. Bertsekas 2\nAbstract\nWe survey incremental methods for minimizing a sum Pm\ni=1 fi(x) consisting of a large number of convex\ncomponent functions fi. Our methods consist of iterations applied to single components, and have proved\nvery e\ufb00ective in practice. We introduce a uni\ufb01ed algorithmic framework for a variety of such methods,\nsome involving gradient and subgradient iterations, which are known, and some involving combinations\nof subgradient and proximal methods, which are new and o\ufb00er greater \ufb02exibility in exploiting the special\nstructure of fi.\nWe provide an analysis of the convergence and rate of convergence properties of these\nmethods, including the advantages o\ufb00ered by randomization in the selection of components. We also survey\napplications in inference/machine learning, signal processing, and large-scale and distributed optimization.\n1.\nINTRODUCTION\nWe consider optimization problems with a cost function consisting of a large number of component functions,\nsuch as\nminimize\nm\nX\ni=1\nfi(x)\nsubject to x \u2208X,\n(1.1)\nwhere fi : \u211cn 7\u2192\u211c, i = 1, . . . , m, are real-valued functions, and X is a closed convex set.\u2020 We focus on\nthe case where the number of components m is very large, and there is an incentive to use incremental\n1\nThis is an extended version of similarly titled papers that appear in Math. Programming Journal, 2011, Vol.\n129, pp. 163-195, and the edited volume Optimization for Machine Learning (S. Sra, S. Nowozin, and S. Wright, Eds.),\nMIT Press, 2012. This version corrects two \ufb02aws of the Dec. 2010 original survey: in the statements and proofs of\nProps. 3.1 and 5.2. Both corrections are described by footnotes preceding the propositions. A supplementary survey,\ndealing with aggregated incremental gradient, proximal, and augmented Lagrangian methods is: Bertsekas, D. P.,\n2015. \u201cIncremental Aggregated Proximal and Augmented Lagrangian Algorithms,\u201d Lab. for Information and Decision\nSystems Report LIDS-P-3176, MIT, September 2015.\n2 The author is with the Dept. of Electr. Engineering and Comp. Science, M.I.T., Cambridge, Mass., 02139. His\nresearch was supported by the AFOSR under Grant FA9550-10-1-0412. Thanks are due to Huizhen (Janey) Yu for\nextensive helpful discussions and suggestions. Comments by Angelia Nedi\u00b4c and Ben Recht are also appreciated.\n\u2020 Throughout the paper, we will operate within the n-dimensional space \u211cn with the standard Euclidean norm,\n1\nmethods that operate on a single component fi at each iteration, rather than on the entire cost function. If\neach incremental iteration tends to make reasonable progress in some \u201caverage\u201d sense, then depending on the\nvalue of m, an incremental method may signi\ufb01cantly outperform (by orders of magnitude) its nonincremental\ncounterpart, as experience has shown.\nIn this paper, we survey the algorithmic properties of incremental methods in a uni\ufb01ed framework,\nbased on the author\u2019s recent work on incremental proximal methods [Ber10] (an early version appears in\nthe supplementary algorithms chapter of the book [Ber09]). In this section, we \ufb01rst provide an overview of\nrepresentative applications, and then we discuss three types of incremental methods: gradient, subgradient,\nand proximal. We unify these methods, into a combined method, which we use as a vehicle for analysis later.\n1.1\nSome Examples of Additive Cost Problems\nAdditive cost problems of the form (1.1) arise in a variety of contexts. Let us provide a few examples where\nthe incremental approach may have an advantage over alternatives.\nExample 1.1: (Least Squares and Related Inference Problems)\nAn important context where cost functions of the form Pm\ni=1 fi(x) arise is inference/machine learning, where\neach term fi(x) corresponds to error between some data and the output of a parametric model, with x being\nthe vector of parameters. An example is linear least squares problems, where fi has quadratic structure, except\nfor a regularization function. The latter function may be di\ufb00erentiable/quadratic, as in the classical regression\nproblem\nminimize\nm\nX\ni=1\n(a\u2032\nix \u2212bi)2 + \u03b3\u2225x \u2212\u00afx\u22252\nsubject to x \u2208\u211cn,\nwhere \u00afx is given, or nondi\ufb00erentiable, as in the \u21131-regularization problem\nminimize\nm\nX\ni=1\n(a\u2032\nix \u2212bi)2 + \u03b3\nn\nX\nj=1\n|xj|\nsubject to x = (x1, . . . , xn) \u2208\u211cn,\nwhich will be discussed further in Section 5.\nA more general class of additive cost problems is nonlinear least squares. Here\nfi(x) = \u0000hi(x)\u00012,\ndenoted \u2225\u00b7\u2225. All vectors are considered column vectors and a prime denotes transposition, so x\u2032x = \u2225x\u22252. We will be\nusing standard terminology of convex optimization, as given for example in textbooks such as Rockafellar\u2019s [Roc70],\nor the author\u2019s recent book [Ber09].\n2\nwhere hi(x) represents the di\ufb00erence between the ith of m measurements from a physical system and the output\nof a parametric model whose parameter vector is x. Problems of nonlinear curve \ufb01tting and regression, as well\nas problems of training neural networks fall in this category, and they are typically nonconvex.\nAnother possibility is to use a nonquadratic function to penalize the error between some data and the\noutput of the parametric model. For example in place of the squared error (a\u2032\nix \u2212bi)2, we may use\nfi(x) = \u2113i(a\u2032\nix \u2212bi),\nwhere \u2113i is a convex function.\nThis is a common approach in robust estimation and some support vector\nmachine formulations.\nStill another example is maximum likelihood estimation, where fi is of the form\nfi(x) = \u2212log PY (yi; x),\nand y1, . . . , ym represent values of independent samples of a random vector whose distribution PY (\u00b7; x) depends\non an unknown parameter vector x \u2208\u211cn that we wish to estimate. Related contexts include \u201cincomplete\u201d data\ncases, where the expectation-maximization (EM) approach is used.\nThe following four examples deal with broadly applicable problem structures that give rise to additive\ncost functions.\nExample 1.2: (Dual Optimization in Separable Problems)\nConsider the problem\nmaximize\nm\nX\ni=1\nci(yi)\nsubject to\nm\nX\ni=1\ngi(yi) \u22650,\nyi \u2208Yi,\ni = 1, . . . , m,\nwhere ci : \u211c\u21137\u2192\u211cand gi : \u211c\u21137\u2192\u211cn are functions of a vector yi \u2208\u211c\u2113, and Yi are given sets of \u211c\u2113. Then by\nassigning a dual vector/multiplier x \u2208\u211cn to the n-dimensional constraint function, we obtain the dual problem\nminimize\nn\nX\ni=1\nfi(x)\nsubject to x \u22650,\nwhere\nfi(x) = sup\nyi\u2208Yi\n\b\nci(yi) + x\u2032gi(yi)\t\n,\nwhich has the additive form (1.1). Here Yi is not assumed convex, so integer programming and other discrete\noptimization problems are included. However, the dual cost function components fi are always convex, and\ntheir values and subgradients can often be conveniently computed, particularly when yi is a scalar or Yi is a\n\ufb01nite set.\n3\nExample 1.3: (Problems with Many Constraints)\nProblems of the form\nminimize\nf(x)\nsubject to gj(x) \u22640,\nj = 1, . . . , r,\nx \u2208X,\n(1.2)\nwhere the number r of constraints is very large often arise in practice, either directly or via reformulation from\nother problems. They can be handled in a variety of ways. One possibility is to adopt a penalty function\napproach, and replace problem (1.2) with\nminimize\nf(x) + c\nr\nX\nj=1\nP\u0000gj(x)\u0001\nsubject to x \u2208X,\n(1.3)\nwhere P(\u00b7) is a scalar penalty function satisfying P(t) = 0 if t \u22640, and P(t) > 0 if t > 0, and c is a\npositive penalty parameter.\nFor example, one may use the quadratic penalty P(t) = \u0000max{0, t}\u00012, or the\nnondi\ufb00erentiable penalty P(t) = max{0, t}. In the latter case, it can be shown that the optimal solutions of\nproblems (1.2) and (1.3) coincide when c is su\ufb03ciently large (see for example [BNO03], Section 7.3, for the case\nwhere f is convex). The cost function of the penalized problem (1.3) is of the additive form (1.1).\nSet constraints of the form x \u2208\u2229m\ni=1Xi, where Xi are closed sets, can also be handled by penalties in a\nway that gives rise to additive cost functions (a simpler but important special case where such constraints arise\nis the problem of \ufb01nding a common point within the sets Xi, i = 1, . . . , m; see Section 5.2). In particular, under\nrelatively mild conditions, problem (1.2) with X = \u2229m\ni=1Xi is equivalent to the unconstrained minimization of\nf(x) + c\nr\nX\nj=1\nP\u0000gj(x)\u0001\n+ \u03b3\nm\nX\ni=1\ndist(x; Xi),\nwhere dist(x; Xi) = miny\u2208Xi \u2225y \u2212x\u2225and \u03b3 is a su\ufb03ciently large penalty parameter. We discuss this possibility\nin Section 5.2.\nExample 1.4: (Minimization of an Expected Value - Stochastic Programming)\nConsider the minimization of an expected value\nminimize\nE\b\nH(x, w)\t\nsubject to x \u2208X,\n(1.4)\nwhere H is a function of x and a random variable w taking a \ufb01nite but very large number of values wi,\ni = 1, . . . , m, with corresponding probabilities \u03c0i. Here the cost function can be written as the sum of the m\nfunctions \u03c0iH(x, wi).\nAn example is stochastic programming, a classical model of two-stage optimization under uncertainty,\nwhere a vector x \u2208X is selected at cost C(x), a random event occurs that has m possible outcomes w1, . . . , wm,\nand then another vector y is selected from some set Y with knowledge of the outcome that occurred. Then the\n4\noptimal decision problem is to specify a vector yi \u2208Y for each outcome wi, and to minimize over x and yi the\nexpected cost\nC(x) +\nm\nX\ni=1\n\u03c0iGi(yi),\nwhere Gi(yi) is the cost associated with the occurrence of wi and \u03c0i is the corresponding probability. This is\na problem with an additive cost function.\nAdditive cost function problems also arise from problem (1.4) in a di\ufb00erent way, when the expected value\nE\b\nH(x, w)\t\nis approximated by an m-sample average\nF(x) = 1\nm\nm\nX\ni=1\nH(x, wi),\nwhere wi are independent samples of the random variable w. The minimum of the sample average f(x) is then\ntaken as an approximation of the minimum of E\b\nH(x, w)\t\n.\nExample 1.5: (Weber Problem in Location Theory)\nA basic problem in location theory is to \ufb01nd a point x in the plane whose sum of weighted distances from a\ngiven set of points y1, . . . , ym is minimized. Mathematically, the problem is\nminimize\nm\nX\ni=1\nwi\u2225x \u2212yi\u2225\nsubject to x \u2208\u211cn,\nwhere w1, . . . , wm are given positive scalars. This problem descends from the famous Fermat-Torricelli-Viviani\nproblem (see [BMS99] for an account of the history). The algorithmic approaches of the present paper would\nbe of potential interest when the number of points m is large. We refer to Drezner and Hamacher [DrH04] for\na survey of recent research, and to Beck and Teboulle [BeT10] for a discussion that is relevant to our context.\nThe structure of the additive cost function (1.1) often facilitates the use of a distributed computing\nsystem that is well-suited for the incremental approach. The following is an illustrative example.\nExample 1.6: (Distributed Incremental Optimization \u2013 Sensor Networks)\nConsider a network of m sensors where data are collected and are used to solve some inference problem involving\na parameter vector x. If fi(x) represents an error penalty for the data collected by the ith sensor, the inference\nproblem is of the form (1.1). While it is possible to collect all the data at a fusion center where the problem\nwill be solved in centralized manner, it may be preferable to adopt a distributed approach in order to save\nin data communication overhead and/or take advantage of parallelism in computation. In such an approach\nthe current iterate xk is passed on from one sensor to another, with each sensor i performing an incremental\niteration involving just its local component function fi, and the entire cost function need not be known at any\none location. We refer to Blatt, Hero, and Gauchman [BHG08], and Rabbat and Nowak [RaN04], [RaN05] for\nfurther discussion.\n5\nThe approach of computing incrementally the values and subgradients of the components fi in a dis-\ntributed manner can be substantially extended to apply to general systems of asynchronous distributed compu-\ntation, where the components are processed at the nodes of a computing network, and the results are suitably\ncombined, as discussed by Nedi\u00b4c, Bertsekas, and Borkar [NBB01]. The analysis here relies on ideas from dis-\ntributed asynchronous gradient methods (both deterministic and stochastic), which were developed in the early\n80s by the author and his coworkers [Ber83], [TBA86], [BeT89]), and have been experiencing a resurgence\nrecently (see e.g., Nedi\u00b4c and Ozdaglar [NeO09]).\n1.2\nIncremental Gradient Methods - Di\ufb00erentiable Problems\nLet us consider \ufb01rst the case where the components fi are di\ufb00erentiable (not necessarily convex). Then, we\nmay use incremental gradient methods, which have the form\nxk+1 = PX\n\u0000xk \u2212\u03b1k\u2207fik(xk)\n\u0001\n,\n(1.5)\nwhere \u03b1k is a positive stepsize, PX(\u00b7) denotes projection on X, and ik is the index of the cost component\nthat is iterated on. Such methods have a long history, particularly for the unconstrained case (X = \u211cn),\nstarting with the Widrow-Ho\ufb00least mean squares (LMS) method [WiH60] for positive semide\ufb01nite quadratic\ncomponent functions (see e.g., [Luo91], [BeT96], Section 3.2.5, [Ber99], Section 1.5.2). They have also been\nused extensively for the training of neural networks, a case of nonquadratic/nonconvex cost components,\nunder the generic name \u201cbackpropagation methods.\u201d There are several variants of these methods, which\ndi\ufb00er in the stepsize selection scheme, and the order in which components are taken up for iteration (it could\nbe deterministic or randomized). They are supported by convergence analyses under various conditions; see\nLuo [Luo91], Grippo [Gri93], [Gri00], Luo and Tseng [LuT94], Mangasarian and Solodov [MaS94], Bertsekas\n[Ber97], Solodov [Sol98], Tseng [Tse98].\nWhen comparing the incremental gradient method with its classical nonincremental gradient counter-\npart [m = 1 and all components lumped into a single function F(x) = Pm\ni=1 fi(x)], there are two comple-\nmentary performance issues to consider:\n(a) Progress when far from convergence. Here the incremental method can be much faster. For an extreme\ncase let X = \u211cn (no constraints), and take m very large and all components fi identical to each other.\nThen an incremental iteration requires m times less computation than a classical gradient iteration,\nbut gives exactly the same result, when the stepsize is appropriately scaled to be m times larger. While\nthis is an extreme example, it re\ufb02ects the essential mechanism by which incremental methods can be\nfar superior: when the components fi are not too dissimilar, far from the minimum a single component\ngradient will point to \u201cmore or less\u201d the right direction [see also the discussion of [Ber97], and [Ber99]\n(Example 1.5.5 and Exercise 1.5.5)].\n6\n(b) Progress when close to convergence. Here the incremental method is generally inferior. As we will\ndiscuss shortly, it converges at a sublinear rate because it requires a diminishing stepsize \u03b1k, compared\nwith the typically linear rate achieved with the classical gradient method when a small constant stepsize\nis used (\u03b1k \u2261\u03b1). One may use a constant stepsize with the incremental method, and indeed this may\nbe the preferred mode of implementation, but then the method typically oscillates in the neighborhood\nof a solution, with size of oscillation roughly proportional to \u03b1, as examples and theoretical analysis\nshow.\nTo understand the convergence mechanism of incremental gradient methods, let us consider the case\nX = \u211cn, and assume that the component functions fi are selected for iteration according to a cyclic order\n[i.e., ik = (k modulo m) + 1], and let us assume that \u03b1k is constant within a cycle (i.e., for all \u2113= 0, 1, . . .,\n\u03b1\u2113m = \u03b1\u2113m+1 = \u00b7 \u00b7 \u00b7 = \u03b1\u2113m+m\u22121). Then, viewing the iteration (1.5) in terms of cycles, we have for every k\nthat marks the beginning of a cycle (ik = 1),\nxk+m = xk \u2212\u03b1k\nm\nX\ni=1\n\u2207fi(xk+i\u22121) = xk \u2212\u03b1k\n\u0000\u2207F(xk) + ek\n\u0001\n,\n(1.6)\nwhere F is the cost function/sum of components, F(x) = Pm\ni=1 fi(x), and ek is given by\nek =\nm\nX\ni=1\n\u0000\u2207fi(xk) \u2212\u2207fi(xk+i\u22121)\n\u0001\n,\nand may be viewed as an error in the calculation of the gradient \u2207f(xk). For Lipschitz continuous gradient\nfunctions \u2207fi, the error ek is proportional to \u03b1k, and this shows two fundamental properties of incremental\ngradient methods, which hold generally for the other incremental methods of this paper as well:\n(a) A constant stepsize (\u03b1k \u2261\u03b1) typically cannot guarantee convergence, since then the size of the gradient\nerror \u2225ek\u2225is typically bounded away from 0. Instead (in the case of di\ufb00erentiable components fi) a\npeculiar form of convergence takes place for constant but su\ufb03ciently small \u03b1, whereby the iterates\nwithin cycles converge but to di\ufb00erent points within a sequence of m points (i.e., the sequence of \ufb01rst\npoints in the cycles converges to a di\ufb00erent limit than the sequence of second points in the cycles, etc).\nThis is true even in the most favorable case of a linear least squares problem (see Luo [Luo91], or the\ntextbook analysis of [Ber99], Section 1.5.1).\n(b) A diminishing stepsize [such as \u03b1k = O(1/k)] leads to diminishing error ek, so (under the appropriate\nLipschitz condition) it can result in convergence to a stationary point of f.\nA corollary of these properties is that the price for achieving convergence is the slow (sublinear)\nasymptotic rate of convergence associated with a diminishing stepsize, which compares unfavorably with the\noften linear rate of convergence associated with a constant stepsize and the nonincremental gradient method.\nHowever, in practical terms this argument does not tell the entire story, since the incremental gradient method\n7\noften achieves in the early iterations a much faster convergence rate than its nonincremental counterpart. In\npractice, the incremental method is usually operated with a stepsize that is either constant or is gradually\nreduced up to a positive value, which is small enough so that the resulting asymptotic oscillation is of no\nessential concern. An alternative, is to use a constant stepsize throughout, but reduce over time the degree\nof incrementalism, so that ultimately the method becomes nonincremental and achieves a linear convergence\nrate (see [Ber97], [Sol98]).\nAside from extensions to nonidi\ufb00erentiable cost problems, for X = \u211cn, there is an important variant\nof the incremental gradient method that involves extrapolation along the direction of the di\ufb00erence of the\npreceding two iterates:\nxk+1 = xk \u2212\u03b1k\u2207fik(xk) + \u03b2(xk \u2212xk\u22121),\n(1.7)\nwhere \u03b2 is a scalar in [0, 1) and x\u22121 = x0 (see e.g., [MaS94], [Tse98], [Ber96], Section 3.2). This is sometimes\ncalled incremental gradient method with momentum. The nonincremental version of this method is the heavy\nball method of Polyak [Pol64], which can be shown to have faster convergence rate than the corresponding\ngradient method (see [Pol87], Section 3.2.1).\nA nonincremental method of this type, but with variable\nand suitably chosen value of \u03b2, has been proposed by Nesterov [Nes83], and has received a lot of attention\nrecently because it has optimal iteration complexity properties under certain conditions (see Nesterov [Nes04],\n[Nes05], Lu, Monteiro, and Yuan [LMY08], Tseng [Tse08], Beck and Teboulle [BeT09], [BeT10]). However,\nno incremental analogs of this method with favorable complexity properties are currently known.\nAnother variant of the incremental gradient method for the case X = \u211cn has been proposed by Blatt,\nHero, and Gauchman [BHG08], which (after the \ufb01rst m iterates are computed) has the form\nxk+1 = xk \u2212\u03b1\nm\u22121\nX\n\u2113=0\n\u2207fik\u2212\u2113(xk\u2212\u2113)\n(1.8)\n[for k < m, the summation should go up to \u2113= k, and \u03b1 should be replaced by a corresponding larger\nvalue, such as \u03b1k = m\u03b1/(k + 1)]. This method also computes the gradient incrementally, one component\nper iteration, but in place of the single component gradient \u2207fik(xk) in Eq. (1.5), it uses an approximation\nto the total cost gradient \u2207f(xk), which is an aggregate of the component gradients computed in the\npast m iterations. A cyclic order of component function selection [ik = (k modulo m) + 1] is assumed in\n[BHG08], and a convergence analysis is given, including a linear convergence rate result for a su\ufb03ciently\nsmall constant stepsize \u03b1 and quadratic component functions fi. It is not clear how iterations (1.5) and\n(1.8) compare in terms of rate of convergence, although the latter seems likely to make faster progress when\nclose to convergence. Note that iteration (1.8) bears similarity to the incremental gradient iteration with\nmomentum (1.7) where \u03b2 \u22481. In particular, when \u03b1k \u2261\u03b1, the sequence generated by Eq. (1.7) satis\ufb01es\nxk+1 = xk \u2212\u03b1\nk\nX\n\u2113=0\n\u03b2\u2113\u2207fik\u2212\u2113(xk\u2212\u2113)\n(1.9)\n8\n[both iterations (1.8) and (1.9) involve di\ufb00erent types of diminishing dependence on past gradient compo-\nnents]. There are no known analogs of iterations (1.7) and (1.8) for nondi\ufb00erentiable cost problems.\nAmong alternative incremental methods for di\ufb00erentiable cost problems, let us also mention versions\nof the Gauss-Newton method for nonlinear least squares problems, based on the extended Kalman \ufb01lter\n(Davidon [Dav76], Bertsekas [Ber96], and Moriyama, Yamashita, and Fukushima [MYF03]).\nThey are\nmathematically equivalent to the ordinary Gauss-Newton method for linear least squares, which they solve\nexactly after a single pass through the component functions fi, but they often perform much faster than the\nlatter in the nonlinear case, particularly when m is large.\nLet us \ufb01nally note that incremental gradient methods are also related to stochastic gradient methods,\nwhich aim to minimize an expected value E\n\b\nH(x, w)\n\t\n(cf. Example 1.2) by using the iteration\nxk+1 = xk \u2212\u03b1k\u2207H(xk, wk),\nwhere wk is a sample of the random variable w. These methods also have a long history (see Polyak and\nTsypkin [PoT73], Ljung [Lju77], Kushner and Clark [KuC78], Tsitsiklis, Bertsekas, and Athans [TBA86],\nPolyak [Pol87], Bertsekas and Tsitsiklis [BeT89], [BeT96], [BeT00], Gaivoronskii [Gai93], P\ufb02ug [P\ufb0296],\nKushner and Yin [KuY97], Bottou [Bot05], Meyn [Mey07], Borkar [Bor08], Nemirovski et. al [NJL09], Lee\nand Wright [LeW10]), and are strongly connected with stochastic approximation algorithms. The main\ndi\ufb00erence between stochastic and deterministic formulations is that the former involve sequential sampling\nof cost components from an in\ufb01nite population under some statistical assumptions, while in the latter the\nset of cost components is predetermined and \ufb01nite. However, it is possible to view the incremental gradient\nmethod (1.5), with a randomized selection of the component function fi (i.e., with ik chosen to be any one\nof the indexes 1, . . . , m, with equal probability 1/m), as a stochastic gradient method (see [BeT96], Example\n4.4, [BeT00], Section 5).\nThe stochastic formulation of incremental methods just discussed highlights an important application\ncontext where the component functions fi are not given a priori, but rather become known sequentially\nthrough some observation process. Then it often makes sense to use an incremental method to process the\ncomponent functions as they become available, and to obtain approximate solutions as early as possible.\nIn fact this may be essential in time-sensitive and possibly time-varying environments, where solutions are\nneeded \u201con-line.\u201d In such cases, one may hope than an adequate estimate of the optimal solution will be\nobtained, before all the functions fi are processed for the \ufb01rst time.\n1.3\nIncremental Subgradient Methods - Nondi\ufb00erentiable Problems\nWe now discuss the case where the component functions fi are convex and nondi\ufb00erentiable at some points,\nand consider incremental subgradient methods. These are similar to their gradient counterparts (1.5) except\n9\nthat an arbitrary subgradient \u02dc\u2207fik(xk) of the cost component fik is used in place of the gradient:\u2020\nxk+1 = PX\n\u0000xk \u2212\u03b1k \u02dc\u2207fik(xk)\n\u0001\n.\n(1.10)\nSuch methods were \ufb01rst proposed in the general form (1.10) in the Soviet Union by Kibardin [Kib80],\nfollowing the earlier paper by Litvakov [Lit66] (which considered convex/nondi\ufb00erentiable extensions of linear\nleast squares problems) and other related subsequent proposals.\u2021 These works remained unnoticed in the\nWestern literature, where incremental methods were reinvented often in di\ufb00erent contexts and with di\ufb00erent\nlines of analysis; see Solodov and Zavriev [SoZ98], Bertsekas [Ber99] (Section 6.3.2), Ben-Tal, Margalit,\nand Nemirovski [BMN01], Nedi\u00b4c and Bertsekas [NeB00], [NeB01], [NeB10], Nedi\u00b4c, Bertsekas, and Borkar\n[NBB01], Kiwiel [Kiw04], Rabbat and Nowak [RaN04], [RaN05], Gaudioso, Giallombardo, and Miglionico\n[GGM06], Shalev-Shwartz et. al. [SSS07], Helou and De Pierro [HeD09], Johansson, Rabi, and Johansson\n[JRJ09], Predd, Kulkarni, and Poor [PKP09], and Ram, Nedi\u00b4c, Veeravalli [RNV09], [RNV09], and Duchi,\nHazan, and Singer [DHS10].\nIncremental subgradient methods have convergence characteristics that are similar in many ways to\ntheir gradient counterparts, the most important similarity being the necessity for a diminishing stepsize \u03b1k\nfor convergence. The lines of analysis, however, tend to be di\ufb00erent, since incremental gradient methods rely\nfor convergence on arguments based on decrease of the cost function value, while incremental subgradient\nmethods rely on arguments based on decrease of the iterates\u2019 distance to the optimal solution set. The line of\nanalysis of the present paper is of the latter type, similar to earlier works of the author and his collaborators\n(see [NeB00], [NeB01], [NBB01], and the textbook presentations in [Ber99], [BNO03]).\nNote two important rami\ufb01cations of the lack of di\ufb00erentiability of the component functions fi:\n(1) Convexity of fi becomes essential, since the notion of subgradient is connected with convexity (subgra-\ndient-like algorithms for nondi\ufb00erentiable/nonconvex problems have been suggested in the literature,\nbut tend to be complicated and have not found much application thus far).\n(2) There is more reason to favor the incremental over the nonincremental methods, since (contrary to\n\u2020 In this paper, we use \u02dc\u2207f(x) to denote a subgradient of a convex function f at a vector x, i.e, a vector such that\nf(z) \u2265f(x) + \u02dc\u2207f(x)\u2032(z \u2212x) for all x \u2208\u211cn. The choice of \u02dc\u2207f(x) from within the set of all subgradients at x [the\nsubdi\ufb00erential at x, denoted \u2202f(x)] will be clear from the context. Note that if f is real-valued, \u2202f(x) is nonempty\nand compact for all x. If f is di\ufb00erentiable at x, \u2202f(x) consists of a single element, the gradient \u2207f(x).\n\u2021 Generally, in the 60s and 70s, algorithmic ideas relating to simple gradient methods with and without determin-\nistic and stochastic errors were popular in the Soviet scienti\ufb01c community, partly due to an emphasis on stochastic\niterative algorithms, such as pseudogradient and stochastic approximation; the works of Ermoliev, Polyak, and Tsyp-\nkin, to name a few of the principal contributors, are representative [Erm69], [PoT73], [Erm76], [Pol78], [Pol87]. By\ncontrast the emphasis in the Western literature at the time was in more complex Newton-like and conjugate direction\nmethods.\n10\nthe di\ufb00erentiable case) nonincremental subgradient methods also require a diminishing stepsize for\nconvergence, and typically achieve a sublinear rate of convergence. Thus the one theoretical advantage\nof the nonincremental gradient method discussed earlier is not shared by its subgradient counterpart.\nLet us \ufb01nally mention that just as in the di\ufb00erentiable case, there is a substantial literature for stochastic\nversions of subgradient methods. In fact, as we will discuss in this paper, there is a potentially signi\ufb01cant\nadvantage in turning the method into a stochastic one by randomizing the order of selection of the components\nfi for iteration.\n1.4\nIncremental Proximal Methods\nWe now consider an extension of the incremental approach to proximal algorithms. The simplest one for\nproblem (1.1) is of the form\nxk+1 = arg min\nx\u2208X\n\u001a\nfik(x) +\n1\n2\u03b1k\n\u2225x \u2212xk\u22252\n\u001b\n,\n(1.11)\nwhich relates to the proximal minimization algorithm (Martinet [Mar70], Rockafellar [Roc76]) in the same\nway that the incremental subgradient method (1.10) relates to the classical nonincremental subgradient\nmethod.\u2020 Here {\u03b1k} is a positive scalar sequence, and we will assume that each fi : \u211cn 7\u2192\u211cis a convex\nfunction, and X is a nonempty closed convex set. The motivation for this type of method, which was proposed\nonly recently in [Ber10], is that with a favorable structure of the components, the proximal iteration (1.10)\nmay be obtained in closed form or be relatively simple, in which case it may be preferable to a gradient or\nsubgradient iteration. In this connection, we note that generally, proximal iterations are considered more\nstable than gradient iterations; for example in the nonincremental case, they converge essentially for any\nchoice of \u03b1k, while this is not so for gradient methods.\nUnfortunately, while some cost function components may be well suited for a proximal iteration, others\nmay not be because the minimization (1.11) is inconvenient, and this leads us to consider combinations of\ngradient/subgradient and proximal iterations. In fact this has motivated in the past nonincremental combi-\nnations of gradient and proximal methods for minimizing the sum of two functions (or more generally, \ufb01nding\na zero of the sum of two nonlinear operators). These methods have a long history, dating to the splitting\nalgorithms of Lions and Mercier [LiM79], Passty [Pas79], and Spingarn [Spi85], and have become popular\nrecently (see Beck and Teboulle [BeT09], [BeT10], and the references they give to specialized algorithms,\nsuch as shrinkage/thresholding, cf. Section 5.1). Let us also note that splitting methods are related to alter-\nnating direction methods of multipliers (see Gabay and Mercier [GaM76], [Gab83], Bertsekas and Tsitsiklis\n\u2020 In this paper we restrict attention to proximal methods with the quadratic regularization term \u2225x \u2212xk\u22252. Our\napproach is applicable in principle when a nonquadratic term is used instead in order to match the structure of the\ngiven problem. The discussion of such alternative algorithms is beyond our scope.\n11\n[BeT89], Eckstein and Bertsekas [EcB92]), which are presently experiencing a revival as viable (nonincre-\nmental) methods for minimizing sums of component functions (see the survey by Boyd et. al. [BPC10], which\ncontains extensive references to recent work and applications, and the complexity-oriented work of Goldfarb,\nMa, and Scheinberg [GoM09], [GMS10]).\nWith similar motivation in mind, we adopt in this paper a uni\ufb01ed algorithmic framework that includes\nincremental gradient, subgradient, and proximal methods, and their combinations, and serves to highlight\ntheir common structure and behavior. We focus on problems of the form\nminimize\nF(x)\ndef\n=\nm\nX\ni=1\nFi(x)\nsubject to x \u2208X,\n(1.12)\nwhere for all i,\nFi(x) = fi(x) + hi(x),\n(1.13)\nfi : \u211cn 7\u2192\u211cand hi : \u211cn 7\u2192\u211care real-valued convex functions, and X is a nonempty closed convex set.\nIn Section 2, we consider several incremental algorithms that iterate on the components fi with a\nproximal iteration, and on the components hi with a subgradient iteration. By choosing all the fi or all\nthe hi to be identically zero, we obtain as special cases the subgradient and proximal iterations (1.10) and\n(1.11), respectively. However, our methods o\ufb00er greater \ufb02exibility, and may exploit the special structure of\nproblems where the functions fi are suitable for a proximal iteration, while the components hi are not and\nthus may be preferably treated with a subgradient iteration.\nIn Section 3, we discuss the convergence and rate of convergence properties of methods that use a cyclic\nrule for component selection, while in Section 4, we discuss the case of a randomized component selection\nrule. In summary, the convergence behavior of our incremental methods is similar to the one outlined earlier\nfor the incremental subgradient method (1.10). This includes convergence within a certain error bound for\na constant stepsize, exact convergence to an optimal solution for an appropriately diminishing stepsize, and\nimproved convergence rate/iteration complexity when randomization is used to select the cost component\nfor iteration. In Section 5 we illustrate our methods for some example applications.\n2.\nINCREMENTAL SUBGRADIENT-PROXIMAL METHODS\nIn this section, we consider problem (1.12)-(1.13), and introduce several incremental algorithms that involve\na combination of a proximal and a subgradient iteration. One of our algorithms has the form\nzk = arg min\nx\u2208X\n\u001a\nfik(x) +\n1\n2\u03b1k\n\u2225x \u2212xk\u22252\n\u001b\n,\n(2.1)\nxk+1 = PX\n\u0000zk \u2212\u03b1k \u02dc\u2207hik(zk)\n\u0001\n,\n(2.2)\n12\nwhere \u02dc\u2207hik(zk) is an arbitrary subgradient of hik at zk. Note that the iteration is well-de\ufb01ned because\nthe minimum in Eq. (2.1) is uniquely attained since fi is continuous and \u2225x \u2212xk\u22252 is real-valued, strictly\nconvex, and coercive, while the subdi\ufb00erential \u2202hi(zk) is nonempty since hi is real-valued. Note also that\nby choosing all the fi or all the hi to be identically zero, we obtain as special cases the subgradient and\nproximal iterations (1.10) and (1.11), respectively.\nThe iterations (2.1) and (2.2) maintain both sequences {zk} and {xk} within the constraint set X, but\nit may be convenient to relax this constraint for either the proximal or the subgradient iteration, thereby\nrequiring a potentially simpler computation. This leads to the algorithm\nzk = arg min\nx\u2208\u211cn\n\u001a\nfik(x) +\n1\n2\u03b1k\n\u2225x \u2212xk\u22252\n\u001b\n,\n(2.3)\nxk+1 = PX\n\u0000zk \u2212\u03b1k \u02dc\u2207hik(zk)\n\u0001\n,\n(2.4)\nwhere the restriction x \u2208X has been omitted from the proximal iteration, and the algorithm\nzk = xk \u2212\u03b1k \u02dc\u2207hik(xk),\n(2.5)\nxk+1 = arg min\nx\u2208X\n\u001a\nfik(x) +\n1\n2\u03b1k\n\u2225x \u2212zk\u22252\n\u001b\n,\n(2.6)\nwhere the projection onto X has been omitted from the subgradient iteration. It is also possible to use\ndi\ufb00erent stepsize sequences in the proximal and subgradient iterations, but for notational simplicity we will\nnot discuss this type of algorithm.\nAll of the incremental proximal algorithms given above are new to our knowledge, having \ufb01rst been\nproposed in the author\u2019s recent paper [Ber10] and the on-line chapter of the book [Ber09]. The closest\nconnection to the existing proximal methods is the \u201cproximal gradient\u201d method, which has been analyzed\nand discussed recently in the context of several machine learning applications by Beck and Teboulle [BeT09],\n[BeT10] (it can also be interpreted in terms of splitting algorithms [LiM79], [Pas79]).\nThis method is\nnonincremental, applies to di\ufb00erentiable hi, and contrary to subgradient and incremental methods, it does\nnot require a diminishing stepsize for convergence to the optimum. In fact, the line of convergence analysis\nof Beck and Teboulle relies on the di\ufb00erentiability of hi and the nonincremental character of the proximal\ngradient method, and is thus di\ufb00erent from ours.\nPart (a) of the following proposition is a key fact about incremental proximal iterations. It shows\nthat they are closely related to incremental subgradient iterations, with the only di\ufb00erence being that the\nsubgradient is evaluated at the end point of the iteration rather than at the start point. Part (b) of the\nproposition provides an inequality that is well-known in the theory of proximal methods, and will be useful\nfor our convergence analysis. In the following, we denote by ri(S) the relative interior of a convex set S, and\nby dom(f) the e\ufb00ective domain\n\b\nx | f(x) < \u221e\n\t\nof a function f : \u211cn 7\u2192(\u2212\u221e, \u221e].\n13\nProposition 2.1:\nLet X be a nonempty closed convex set, and let f : \u211cn 7\u2192(\u2212\u221e, \u221e] be a closed\nproper convex function such that ri(X) \u2229ri\n\u0000dom(f)\n\u0001\n\u0338= \u2205. For any xk \u2208\u211cn and \u03b1k > 0, consider the\nproximal iteration\nxk+1 = arg min\nx\u2208X\n\u001a\nf(x) +\n1\n2\u03b1k\n\u2225x \u2212xk\u22252\n\u001b\n.\n(2.7)\n(a) The iteration can be written as\nxk+1 = PX\n\u0000xk \u2212\u03b1k \u02dc\u2207f(xk+1)\n\u0001\n,\ni = 1, . . . , m,\n(2.8)\nwhere \u02dc\u2207f(xk+1) is some subgradient of f at xk+1.\n(b) For all y \u2208X, we have\n\u2225xk+1 \u2212y\u22252 \u2264\u2225xk \u2212y\u22252 \u22122\u03b1k\n\u0000f(xk+1) \u2212f(y)\n\u0001\n\u2212\u2225xk \u2212xk+1\u22252\n\u2264\u2225xk \u2212y\u22252 \u22122\u03b1k\n\u0000f(xk+1) \u2212f(y)\n\u0001\n.\n(2.9)\nProof:\n(a) We use the formula for the subdi\ufb00erential of the sum of the three functions f, (1/2\u03b1k)\u2225x\u2212xk\u22252,\nand the indicator function of X (cf. Prop. 5.4.6 of [Ber09]), together with the condition that 0 should belong\nto this subdi\ufb00erential at the optimum xk+1. We obtain that Eq. (2.7) holds if and only if\n1\n\u03b1k\n(xk \u2212xk+1) \u2208\u2202f(xk+1) + NX(xk+1),\n(2.10)\nwhere NX(xk+1) is the normal cone of X at xk+1 [the set of vectors y such that y\u2032(x \u2212xk+1) \u22640 for all\nx \u2208X, and also the subdi\ufb00erential of the indicator function of X at xk+1; see [Ber09], p. 185]. This is true\nif and only if\nxk \u2212xk+1 \u2212\u03b1k \u02dc\u2207f(xk+1) \u2208NX(xk+1),\nfor some \u02dc\u2207f(xk+1) \u2208\u2202f(xk+1), which in turn is true if and only if Eq. (2.8) holds, by the projection theorem.\n(b) We have\n\u2225xk \u2212y\u22252 = \u2225xk \u2212xk+1 + xk+1 \u2212y\u22252 = \u2225xk \u2212xk+1\u22252 \u22122(xk \u2212xk+1)\u2032(y \u2212xk+1) + \u2225xk+1 \u2212y\u22252.\n(2.11)\nAlso since from Eq. (2.10),\n1\n\u03b1k (xk \u2212xk+1) is a subgradient at xk+1 of the sum of f and the indicator function\nof X, we have (using also the assumption y \u2208X)\nf(xk+1) + 1\n\u03b1k\n(xk \u2212xk+1)\u2032(y \u2212xk+1) \u2264f(y).\n14\nCombining this relation with Eq. (2.11), the result follows.\nQ.E.D.\nBased on Prop. 2.1(a), we see that all the preceding iterations can be written in an incremental\nsubgradient format:\n(a) Iteration (2.1)-(2.2) can be written as\nzk = PX\n\u0000xk \u2212\u03b1k \u02dc\u2207fik(zk)\n\u0001\n,\nxk+1 = PX\n\u0000zk \u2212\u03b1k \u02dc\u2207hik(zk)\n\u0001\n.\n(2.12)\n(b) Iteration (2.3)-(2.4) can be written as\nzk = xk \u2212\u03b1k \u02dc\u2207fik(zk),\nxk+1 = PX\n\u0000zk \u2212\u03b1k \u02dc\u2207hik(zk)\n\u0001\n.\n(2.13)\n(c) Iteration (2.5)-(2.6) can be written as\nzk = xk \u2212\u03b1k \u02dc\u2207hik(xk),\nxk+1 = PX\n\u0000zk \u2212\u03b1k \u02dc\u2207fik(xk+1)\n\u0001\n.\n(2.14)\nNote that in all the preceding updates, the subgradient \u02dc\u2207hik can be any vector in the subdi\ufb00erential of hik,\nwhile the subgradient \u02dc\u2207fik must be a speci\ufb01c vector in the subdi\ufb00erential of fik, speci\ufb01ed according to Prop.\n2.1(a). Note also that iteration (2.13) can be written as\nxk+1 = PX\n\u0000xk \u2212\u03b1k \u02dc\u2207Fik(zk)\n\u0001\n,\nand resembles the incremental subgradient method for minimizing over X the cost F(x) = Pm\ni=1 Fi(x) [cf.\nEq. (1.12)], the only di\ufb00erence being that the subgradient of Fik is taken at zk rather than xk.\nAn important issue which a\ufb00ects the methods\u2019 e\ufb00ectiveness is the order in which the components\n{fi, hi} are chosen for iteration. In this paper, we consider two possibilities:\n(1) A cyclic order, whereby {fi, hi} are taken up in the \ufb01xed deterministic order 1, . . . , m, so that ik is\nequal to (k modulo m) plus 1. A contiguous block of iterations involving {f1, h1}, . . . , {fm, hm} in this\norder and exactly once is called a cycle. We assume that the stepsize \u03b1k is constant within a cycle (for\nall k with ik = 1 we have \u03b1k = \u03b1k+1 . . . = \u03b1k+m\u22121).\n(2) A randomized order based on uniform sampling, whereby at each iteration a component pair {fi, hi}\nis chosen randomly by sampling over all component pairs with a uniform distribution, independently\nof the past history of the algorithm.\nIt is essential to include all components in a cycle in the cyclic case, and to sample according to the uniform\ndistribution in the randomized case, for otherwise some components will be sampled more often than others,\nleading to a bias in the convergence process.\n15\nAnother technique for incremental methods, popular in neural network training practice, is to reshu\ufb04e\nrandomly the order of the component functions after each cycle. This alternative order selection scheme leads\nto convergence, like the preceding two. Moreover, this scheme has the nice property of allocating exactly one\ncomputation slot to each component in an m-slot cycle (m incremental iterations). By comparison, choosing\ncomponents by uniform sampling allocates one computation slot to each component on the average, but\nsome components may not get a slot while others may get more than one. A nonzero variance in the number\nof slots that any \ufb01xed component gets within a cycle, may be detrimental to performance, and indicates that\nreshu\ufb04ing randomly the order of the component functions after each cycle works better; this is consistent\nwith experimental observations shared with us by B. Recht (private communication). While it seems di\ufb03cult\nto establish this fact analytically, a justi\ufb01cation is suggested by the view of the incremental method as a\ngradient-like method that uses as descent direction the true gradient at the start of the cycle plus an \u201cerror\u201d\n[due to the calculation of the component gradients at points intermediate within a cycle; cf. Eq. (1.6)]. The\nerror has apparently greater variance in the uniform sampling method than in the randomly shu\ufb04ed order\nmethod (in fact the variance of the error would seem relatively larger as m increases, although other factors\nsuch as variance of size of component gradients would also play a role). Heuristically, if the variance of the\nerror is larger, the direction of descent deteriorates, suggesting slower convergence. In this paper, we will\nfocus on the easier-to-analyze uniform sampling method, and show by analysis that it is superior to the\ncyclic order.\nFor the remainder of the paper, we denote by F \u2217the optimal value of problem (1.12):\nF \u2217= inf\nx\u2208X F(x),\nand by X\u2217the set of optimal solutions (which could be empty):\nX\u2217=\n\b\nx\u2217| x\u2217\u2208X, F(x\u2217) = F \u2217\t\n.\nAlso, for a nonempty closed convex set X, we denote by dist(\u00b7; X) the distance function given by\ndist(x; X) = min\nz\u2208X \u2225x \u2212z\u2225,\nx \u2208\u211cn.\nIn our convergence analysis of Section 4, we will use the following well-known theorem (see Neveu\n[Nev75], p. 33). We will use a much simpler deterministic version of the theorem in Section 3.\n16\nProposition 2.2: (Supermartingale Convergence Theorem)\nLet Yk, Zk, and Wk, k = 0, 1, . . .,\nbe three sequences of random variables and let Fk, k = 0, 1, . . ., be sets of random variables such that\nFk \u2282Fk+1 for all k. Suppose that:\n(1) The random variables Yk, Zk, and Wk are nonnegative, and are functions of the random variables\nin Fk.\n(2) For each k, we have\nE\n\b\nYk+1 | Fk\n\t\n\u2264Yk \u2212Zk + Wk.\n(3) There holds, with probability 1, P\u221e\nk=0 Wk < \u221e.\nThen we have P\u221e\nk=0 Zk < \u221e, and the sequence Yk converges to a nonnegative random variable Y ,\nwith probability 1.\n3.\nCONVERGENCE FOR METHODS WITH CYCLIC ORDER\nIn this section, we discuss convergence under the cyclic order. We consider a randomized order in the next\nsection. We focus on the sequence {xk} rather than {zk}, which need not lie within X in the case of iterations\n(2.13) and (2.14) when X \u0338= \u211cn. In summary, the idea is to show that the e\ufb00ect of taking subgradients of fi\nor hi at points near xk (e.g., at zk rather than at xk) is inconsequential, and diminishes as the stepsize \u03b1k\nbecomes smaller, as long as some subgradients relevant to the algorithms are uniformly bounded in norm by\nsome constant. This is similar to the convergence mechanism of incremental gradient methods described in\nSection 1.2. We use the following assumptions throughout the present section.\nAssumption 3.1: [For iterations (2.12) and (2.13)]\nThere is a constant c \u2208\u211csuch that for\nall k\nmax\n\b\n\u2225\u02dc\u2207fik(zk)\u2225, \u2225\u02dc\u2207hik(zk)\u2225\n\t\n\u2264c.\n(3.1)\nFurthermore, for all k that mark the beginning of a cycle (i.e., all k > 0 with ik = 1), we have\nmax\n\b\nfj(xk) \u2212fj(zk+j\u22121), hj(xk) \u2212hj(zk+j\u22121)\n\t\n\u2264c \u2225xk \u2212zk+j\u22121\u2225,\n\u2200j = 1, . . . , m.\n(3.2)\n17\nAssumption 3.2: [For iteration (2.14)]\nThere is a constant c \u2208\u211csuch that for all k\nmax\n\b\n\u2225\u02dc\u2207fik(xk+1)\u2225, \u2225\u02dc\u2207hik(xk)\u2225\n\t\n\u2264c.\n(3.3)\nFurthermore, for all k that mark the beginning of a cycle (i.e., all k > 0 with ik = 1), we have\nmax\n\b\nfj(xk) \u2212fj(xk+j\u22121), hj(xk) \u2212hj(xk+j\u22121)\n\t\n\u2264c \u2225xk \u2212xk+j\u22121\u2225,\n\u2200j = 1, . . . , m,\n(3.4)\nfj(xk+j\u22121) \u2212fj(xk+j) \u2264c \u2225xk+j\u22121 \u2212xk+j\u2225,\n\u2200j = 1, . . . , m.\n(3.5)\nNote that the condition (3.2) is satis\ufb01ed if for each i and k, there is a subgradient of fi at xk and a\nsubgradient of hi at xk, whose norms are bounded by c. Conditions that imply the preceding assumptions\nare:\n(a) For algorithm (2.12): fi and hi are Lipschitz continuous over the set X.\n(b) For algorithms (2.13) and (2.14): fi and hi are Lipschitz continuous over the entire space \u211cn.\n(c) For all algorithms (2.12), (2.13), and (2.14): fi and hi are polyhedral [this is a special case of (a) and\n(b)].\n(d) For all algorithms (2.12), (2.13), and (2.14): The sequences {xk} and {zk} are bounded [since then\nfi and hi, being real-valued and convex, are Lipschitz continuous over any bounded set that contains\n{xk} and {zk}].\nThe following proposition provides a key estimate that reveals the convergence mechanism of our\nmethods.\u2020\n\u2020 The original version of this report gave \u03b2 =\n1\nm + 4 for the case of algorithms (2.12) and (2.13), and \u03b2 =\n5\nm + 4\nfor the case of algorithm (2.14), because a loose bound was used in the following calculation. The tighter version for\nalgorithm (2.14) given here was prompted by an observation by M. Andersen and P. C. Hansen in Oct. 2013.\n18\nProposition 3.1:\nLet {xk} be the sequence generated by any one of the algorithms (2.12)-(2.14),\nwith a cyclic order of component selection. Then for all y \u2208X and all k that mark the beginning of\na cycle (i.e., all k with ik = 1), we have\n\u2225xk+m \u2212y\u22252 \u2264\u2225xk \u2212y\u22252 \u22122\u03b1k\n\u0000F(xk) \u2212F(y)\n\u0001\n+ \u03b12\nk\u03b2m2c2,\n(3.6)\nwhere \u03b2 = 1\nm + 4.\nProof:\nWe \ufb01rst prove the result for algorithms (2.12) and (2.13), and then indicate the modi\ufb01cations\nnecessary for algorithm (2.14). Using Prop. 2.1(b), we have for all y \u2208X and k,\n\u2225zk \u2212y\u22252 \u2264\u2225xk \u2212y\u22252 \u22122\u03b1k\n\u0000fik(zk) \u2212fik(y)\n\u0001\n.\n(3.7)\nAlso, using the nonexpansion property of the projection [i.e.,\n\r\rPX(u) \u2212PX(v)\n\r\r \u2264\u2225u \u2212v\u2225for all u, v \u2208\u211cn],\nthe de\ufb01nition of subgradient, and Eq. (3.1), we obtain for all y \u2208X and k,\n\u2225xk+1 \u2212y\u22252 =\n\r\rPX\n\u0000zk \u2212\u03b1k \u02dc\u2207hik(zk)\n\u0001\n\u2212y\n\r\r2\n\u2264\u2225zk \u2212\u03b1k \u02dc\u2207hik(zk) \u2212y\u22252\n= \u2225zk \u2212y\u22252 \u22122\u03b1k \u02dc\u2207hik(zk)\u2032(zk \u2212y) + \u03b12\nk\n\r\r \u02dc\u2207hik(zk)\n\r\r2\n\u2264\u2225zk \u2212y\u22252 \u22122\u03b1k\n\u0000hik(zk) \u2212hik(y)\n\u0001\n+ \u03b12\nkc2.\n(3.8)\nCombining Eqs. (3.7) and (3.8), and using the de\ufb01nition Fj = fj + hj, we have\n\u2225xk+1 \u2212y\u22252 \u2264\u2225xk \u2212y\u22252 \u22122\u03b1k\n\u0000fik(zk) + hik(zk) \u2212fik(y) \u2212hik(y)\n\u0001\n+ \u03b12\nkc2\n= \u2225xk \u2212y\u22252 \u22122\u03b1k\n\u0000Fik(zk) \u2212Fik(y)\n\u0001\n+ \u03b12\nkc2.\n(3.9)\nLet now k mark the beginning of a cycle (i.e., ik = 1). Then at iteration k + j \u22121, j = 1, . . . , m, the\nselected components are {fj, hj}, in view of the assumed cyclic order. We may thus replicate the preceding\ninequality with k replaced by k + 1, . . . , k + m \u22121, and add to obtain\n\u2225xk+m \u2212y\u22252 \u2264\u2225xk \u2212y\u22252 \u22122\u03b1k\nm\nX\nj=1\n\u0000Fj(zk+j\u22121) \u2212Fj(y)\n\u0001\n+ m\u03b12\nkc2,\nor equivalently, since F = Pm\nj=1 Fj,\n\u2225xk+m \u2212y\u22252 \u2264\u2225xk \u2212y\u22252 \u22122\u03b1k\n\u0000F(xk) \u2212F(y)\n\u0001\n+ m\u03b12\nkc2 + 2\u03b1k\nm\nX\nj=1\n\u0000Fj(xk) \u2212Fj(zk+j\u22121)\n\u0001\n.\n(3.10)\n19\nThe remainder of the proof deals with appropriately bounding the last term above.\nFrom Eq. (3.2), we have for j = 1, . . . , m,\nFj(xk) \u2212Fj(zk+j\u22121) \u22642c \u2225xk \u2212zk+j\u22121\u2225.\n(3.11)\nWe also have\n\u2225xk \u2212zk+j\u22121\u2225\u2264\u2225xk \u2212xk+1\u2225+ \u00b7 \u00b7 \u00b7 + \u2225xk+j\u22122 \u2212xk+j\u22121\u2225+ \u2225xk+j\u22121 \u2212zk+j\u22121\u2225,\n(3.12)\nand by the de\ufb01nition of the algorithms (2.12) and (2.13), the nonexpansion property of the projection, and\nEq. (3.1), each of the terms in the right-hand side above is bounded by 2\u03b1kc, except for the last, which is\nbounded by \u03b1kc. Thus Eq. (3.12) yields \u2225xk \u2212zk+j\u22121\u2225\u2264\u03b1k(2j \u22121)c, which together with Eq. (3.11), shows\nthat\nFj(xk) \u2212Fj(zk+j\u22121) \u22642\u03b1kc2(2j \u22121).\n(3.13)\nCombining Eqs. (3.10) and (3.13), we have\n\u2225xk+m \u2212y\u22252 \u2264\u2225xk \u2212y\u22252 \u22122\u03b1k\n\u0000F(xk) \u2212F(y)\n\u0001\n+ m\u03b12\nkc2 + 4\u03b12\nkc2\nm\nX\nj=1\n(2j \u22121),\nand \ufb01nally\n\u2225xk+m \u2212y\u22252 \u2264\u2225xk \u2212y\u22252 \u22122\u03b1k\n\u0000F(xk) \u2212F(y)\n\u0001\n+ m\u03b12\nkc2 + 4\u03b12\nkc2m2,\nwhich is of the form (3.6) with \u03b2 = 1\nm + 4.\nFor the algorithm (2.14), a similar argument goes through using Assumption 3.2. In place of Eq. (3.7),\nusing the nonexpansion property of the projection, the de\ufb01nition of subgradient, and Eq. (3.3), we obtain\nfor all y \u2208X and k \u22650,\n\u2225zk \u2212y\u22252 \u2264\u2225xk \u2212y\u22252 \u22122\u03b1k\n\u0000hik(xk) \u2212hik(y)\n\u0001\n+ \u03b12\nkc2,\n(3.14)\nwhile in place of Eq. (3.8), using Prop. 2.1(b), we have\n\u2225xk+1 \u2212y\u22252 \u2264\u2225zk \u2212y\u22252 \u22122\u03b1k\n\u0000fik(xk+1) \u2212fik(y)\n\u0001\n.\n(3.15)\nCombining these equations, in analogy with Eq. (3.9), we obtain\n\u2225xk+1 \u2212y\u22252 \u2264\u2225xk \u2212y\u22252 \u22122\u03b1k\n\u0000fik(xk+1) + hik(xk) \u2212fik(y) \u2212hik(y)\n\u0001\n+ \u03b12\nkc2\n= \u2225xk \u2212y\u22252 \u22122\u03b1k\n\u0000Fik(xk) \u2212Fik(y)\n\u0001\n+ \u03b12\nkc2 + 2\u03b1k\n\u0000fik(xk) \u2212fik(xk+1)\n\u0001\n.\n(3.16)\nAs earlier, we let k mark the beginning of a cycle (i.e., ik = 1). We replicate the preceding inequality\nwith k replaced by k + 1, . . . , k + m \u22121, and add to obtain [in analogy with Eq. (3.10)]\n\u2225xk+m \u2212y\u22252 \u2264\u2225xk \u2212y\u22252 \u22122\u03b1k\n\u0000F(xk) \u2212F(y)\n\u0001\n+ m\u03b12\nkc2\n+ 2\u03b1k\nm\nX\nj=1\n\u0000Fj(xk) \u2212Fj(xk+j\u22121)\n\u0001\n+ 2\u03b1k\nm\nX\nj=1\n\u0000fj(xk+j\u22121) \u2212fj(xk+j)\n\u0001\n.\n(3.17)\n20\nWe now bound the two sums in Eq. (3.17), using Assumption 3.2. From Eq. (3.4), we have\nFj(xk) \u2212Fj(xk+j\u22121) \u22642c\u2225xk \u2212xk+j\u22121\u2225\u22642c\n\u0000\u2225xk \u2212xk+1\u2225+ \u00b7 \u00b7 \u00b7 + \u2225xk+j\u22122 \u2212xk+j\u22121\u2225\n\u0001\n,\nand since by Eq. (3.3) and the de\ufb01nition of the algorithm, each of the norm terms in the right-hand side\nabove is bounded by 2\u03b1kc,\nFj(xk) \u2212Fj(xk+j\u22121) \u22644\u03b1kc2(j \u22121).\nAlso from Eqs. (3.3) and (3.5), and the nonexpansion property of the projection, we have\nfj(xk+j\u22121) \u2212fj(xk+j) \u2264c \u2225xk+j\u22121 \u2212xk+j\u2225\u22642\u03b1kc2.\nCombining the preceding relations and adding, we obtain\n2\u03b1k\nm\nX\nj=1\n\u0000Fj(xk) \u2212Fj(xk+j\u22121)\n\u0001\n+ 2\u03b1k\nm\nX\nj=1\n\u0000fj(xk+j\u22121) \u2212fj(xk+j)\n\u0001\n\u22648\u03b12\nkc2\nm\nX\nj=1\n(j \u22121) + 4\u03b12\nkc2m\n= 4\u03b12\nkc2(m2 \u2212m) + 4\u03b12\nkc2m =\n\u0012\n4 + 1\nm\n\u0013\n\u03b12\nkc2m2,\nwhich together with Eq. (3.17), yields Eq. (3.6).\nQ.E.D.\nAmong other things, Prop. 3.1 guarantees that with a cyclic order, given the iterate xk at the start of\na cycle and any point y \u2208X having lower cost than xk (for example an optimal point), the algorithm yields\na point xk+m at the end of the cycle that will be closer to y than xk, provided the stepsize \u03b1k is less than\n2\n\u0000F(xk) \u2212F(y)\n\u0001\n\u03b2m2c2\n.\nIn particular, for any \u01eb > 0 and assuming that there exists an optimal solution x\u2217, either we are within\n\u03b1k\u03b2m2c2\n2\n+ \u01eb of the optimal value,\nF(xk) \u2264F(x\u2217) + \u03b1k\u03b2m2c2\n2\n+ \u01eb,\nor else the squared distance to x\u2217will be strictly decreased by at least 2\u03b1k\u01eb,\n\u2225xk+m \u2212x\u2217\u22252 < \u2225xk \u2212x\u2217\u22252 \u22122\u03b1k\u01eb.\nThus, using Prop. 3.1, we can provide various types of convergence results. As an example, for a constant\nstepsize (\u03b1k \u2261\u03b1), convergence can be established to a neighborhood of the optimum, which shrinks to 0 as\n\u03b1 \u21920, as stated in the following proposition.\n21\nProposition 3.2:\nLet {xk} be the sequence generated by any one of the algorithms (2.12)-(2.14),\nwith a cyclic order of component selection, and let the stepsize \u03b1k be \ufb01xed at some positive constant\n\u03b1.\n(a) If F \u2217= \u2212\u221e, then\nlim inf\nk\u2192\u221eF(xk) = F \u2217.\n(b) If F \u2217> \u2212\u221e, then\nlim inf\nk\u2192\u221eF(xk) \u2264F \u2217+ \u03b1\u03b2m2c2\n2\n,\nwhere c and \u03b2 are the constants of Prop. 3.1.\nProof:\nWe prove (a) and (b) simultaneously. If the result does not hold, there must exist an \u01eb > 0 such\nthat\nlim inf\nk\u2192\u221eF(xkm) \u2212\u03b1\u03b2m2c2\n2\n\u22122\u01eb > F \u2217.\nLet \u02c6y \u2208X be such that\nlim inf\nk\u2192\u221eF(xkm) \u2212\u03b1\u03b2m2c2\n2\n\u22122\u01eb \u2265F(\u02c6y),\nand let k0 be large enough so that for all k \u2265k0, we have\nF(xkm) \u2265lim inf\nk\u2192\u221eF(xkm) \u2212\u01eb.\nBy combining the preceding two relations, we obtain for all k \u2265k0,\nF(xkm) \u2212F(\u02c6y) \u2265\u03b1\u03b2m2c2\n2\n+ \u01eb.\nUsing Prop. 3.1 for the case where y = \u02c6y together with the above relation, we obtain for all k \u2265k0,\n\u2225x(k+1)m \u2212\u02c6y\u22252 \u2264\u2225xkm \u2212\u02c6y\u22252 \u22122\u03b1\n\u0000F(xkm) \u2212F(\u02c6y)\n\u0001\n+ \u03b2\u03b12m2c2 \u2264\u2225xkm \u2212\u02c6y\u22252 \u22122\u03b1\u01eb.\nThis relation implies that for all k \u2265k0,\n\u2225x(k+1)m \u2212\u02c6y\u22252 \u2264\u2225x(k\u22121)m \u2212\u02c6y\u22252 \u22124\u03b1\u01eb \u2264\u00b7 \u00b7 \u00b7 \u2264\u2225xk0 \u2212\u02c6y\u22252 \u22122(k + 1 \u2212k0)\u03b1\u01eb,\nwhich cannot hold for k su\ufb03ciently large \u2013 a contradiction.\nQ.E.D.\nThe next proposition gives an estimate of the number of iterations needed to guarantee a given level\nof optimality up to the threshold tolerance \u03b1\u03b2m2c2/2 of the preceding proposition.\n22\nProposition 3.3:\nAssume that X\u2217is nonempty. Let {xk} be a sequence generated as in Prop. 3.2.\nThen for \u01eb > 0, we have\nmin\n0\u2264k\u2264N F(xk) \u2264F \u2217+ \u03b1\u03b2m2c2 + \u01eb\n2\n,\n(3.18)\nwhere N is given by\nN = m\n\u0016dist(x0; X\u2217)2\n\u03b1\u01eb\n\u0017\n.\n(3.19)\nProof:\nAssume, to arrive at a contradiction, that Eq. (3.18) does not hold, so that for all k with 0 \u2264km \u2264\nN, we have\nF(xkm) > F \u2217+ \u03b1\u03b2m2c2 + \u01eb\n2\n.\nBy using this relation in Prop. 3.1 with \u03b1k replaced by \u03b1 and y equal to the vector of X\u2217that is at minimum\ndistance from xkm, we obtain for all k with 0 \u2264km \u2264N,\ndist(x(k+1)m; X\u2217)2 \u2264dist(xkm; X\u2217)2 \u22122\u03b1\n\u0000F(xkm) \u2212F \u2217\u0001\n+\u03b12\u03b2m2c2\n\u2264dist(xkm; X\u2217)2 \u2212(\u03b12\u03b2m2c2 + \u03b1\u01eb) + \u03b12\u03b2m2c2\n= dist(xkm; X\u2217)2 \u2212\u03b1\u01eb.\nAdding the above inequalities for k = 0, . . . , N\nm, we obtain\ndist(xN+m; X\u2217)2 \u2264dist(x0; X\u2217)2 \u2212\n\u0012N\nm + 1\n\u0013\n\u03b1\u01eb,\nso that\n\u0012N\nm + 1\n\u0013\n\u03b1\u01eb \u2264dist(x0; X\u2217)2,\nwhich contradicts the de\ufb01nition of N.\nQ.E.D.\nAccording to Prop. 3.3, to achieve a cost function value within O(\u01eb) of the optimal, the term \u03b1\u03b2m2c2\nmust also be of order O(\u01eb), so \u03b1 must be of order O(\u01eb/m2c2), and from Eq. (3.19), the number of necessary\niterations N is O(m3c2/\u01eb2), and the number of necessary cycles is O\n\u0000(mc)2/\u01eb2)\n\u0001\n. This is the same type of\nestimate as for the nonincremental subgradient method [i.e., O(1/\u01eb2), counting a cycle as one iteration of\nthe nonincremental method, and viewing mc as a Lipschitz constant for the entire cost function F], and\ndoes not reveal any advantage for the incremental methods given here. However, in the next section, we\ndemonstrate a much more favorable iteration complexity estimate for the incremental methods that use a\nrandomized order of component selection.\n23\nExact Convergence for a Diminishing Stepsize\nWe can also obtain an exact convergence result for the case where the stepsize \u03b1k diminishes to zero. The\nidea is that with a constant stepsize \u03b1 we can get to within an O(\u03b1)-neighborhood of the optimum, as shown\nabove, so with a diminishing stepsize \u03b1k, we should be able to reach an arbitrarily small neighborhood of the\noptimum. However, for this to happen, \u03b1k should not be reduced too fast, and should satisfy P\u221e\nk=0 \u03b1k = \u221e\n(so that the method can \u201ctravel\u201d in\ufb01nitely far if necessary).\nProposition 3.4:\nLet {xk} be the sequence generated by any one of the algorithms (2.12)-(2.14),\nwith a cyclic order of component selection, and let the stepsize \u03b1k satisfy\nlim\nk\u2192\u221e\u03b1k = 0,\n\u221e\nX\nk=0\n\u03b1k = \u221e.\nThen,\nlim inf\nk\u2192\u221eF(xk) = F \u2217.\nFurthermore, if X\u2217is nonempty and\n\u221e\nX\nk=0\n\u03b12\nk < \u221e,\nthen {xk} converges to some x\u2217\u2208X\u2217.\nProof:\nFor the \ufb01rst part, it will be su\ufb03cient to show that lim infk\u2192\u221eF(xkm) = F \u2217. Assume, to arrive at\na contradiction, that there exists an \u01eb > 0 such that\nlim inf\nk\u2192\u221eF(xkm) \u22122\u01eb > F \u2217.\nThen there exists a point \u02c6y \u2208X such that\nlim inf\nk\u2192\u221eF(xkm) \u22122\u01eb > F(\u02c6y).\nLet k0 be large enough so that for all k \u2265k0, we have\nF(xkm) \u2265lim inf\nk\u2192\u221eF(xkm) \u2212\u01eb.\nBy combining the preceding two relations, we obtain for all k \u2265k0,\nF(xkm) \u2212F(\u02c6y) > \u01eb.\n24\nBy setting y = \u02c6y in Prop. 3.1, and by using the above relation, we have for all k \u2265k0,\n\u2225x(k+1)m \u2212\u02c6y\u22252 \u2264\u2225xkm \u2212\u02c6y\u22252 \u22122\u03b1km\u01eb + \u03b2\u03b12\nkmm2c2 = \u2225xkm \u2212\u02c6y\u22252 \u2212\u03b1km (2\u01eb \u2212\u03b2\u03b1kmm2c2) .\nSince \u03b1k \u21920, without loss of generality, we may assume that k0 is large enough so that\n2\u01eb \u2212\u03b2\u03b1km2c2 \u2265\u01eb,\n\u2200k \u2265k0.\nTherefore for all k \u2265k0, we have\n\u2225x(k+1)m \u2212\u02c6y\u22252 \u2264\u2225xkm \u2212\u02c6y\u22252 \u2212\u03b1km\u01eb \u2264\u00b7 \u00b7 \u00b7 \u2264\u2225xk0m \u2212\u02c6y\u22252 \u2212\u01eb\nk\nX\n\u2113=k0\n\u03b1\u2113m,\nwhich cannot hold for k su\ufb03ciently large. Hence lim infk\u2192\u221eF(xkm) = F \u2217.\nTo prove the second part of the proposition, note that from Prop. 3.1, for every x\u2217\u2208X\u2217and k \u22650 we\nhave\n\u2225x(k+1)m \u2212x\u2217\u22252 \u2264\u2225xkm \u2212x\u2217\u22252 \u22122\u03b1km\n\u0000F(xkm) \u2212F(x\u2217)\n\u0001\n+ \u03b12\nkm\u03b2m2c2.\n(3.20)\nThe Supermartingale Convergence Theorem (Prop. 2.2)\u2020 and the hypothesis P\u221e\nk=0 \u03b12\nk < \u221e, imply that\n\b\n\u2225xkm \u2212x\u2217\u2225\n\t\nconverges for every x\u2217\u2208X\u2217. Since then {xkm} is bounded, it has a limit point \u00afx \u2208X that\nsatis\ufb01es\nF(\u00afx) = lim inf\nk\u2192\u221eF(xkm) = F \u2217.\nThis implies that \u00afx \u2208X\u2217, so it follows that\n\b\n\u2225xkm \u2212\u00afx\u2225\n\t\nconverges, and that the entire sequence {xkm}\nconverges to \u00afx (since \u00afx is a limit point of {xkm}).\nFinally, to show that the entire sequence {xk} also converges to \u00afx, note that from Eqs. (3.1) and (3.3),\nand the form of the iterations (2.12)-(2.14), we have \u2225xk+1 \u2212xk\u2225\u22642\u03b1kc \u21920. Since {xkm} converges to \u00afx,\nit follows that {xk} also converges to \u00afx.\nQ.E.D.\n4.\nCONVERGENCE FOR METHODS WITH RANDOMIZED ORDER\nIn this section, we discuss convergence for the randomized component selection order and a constant stepsize\n\u03b1. The randomized versions of iterations (2.12), (2.13), and (2.14), are\nzk = PX\n\u0000xk \u2212\u03b1 \u02dc\u2207f\u03c9k(zk)\n\u0001\n,\nxk+1 = PX\n\u0000zk \u2212\u03b1 \u02dc\u2207h\u03c9k(zk)\n\u0001\n,\n(4.1)\nzk = xk \u2212\u03b1 \u02dc\u2207f\u03c9k(zk),\nxk+1 = PX\n\u0000zk \u2212\u03b1 \u02dc\u2207h\u03c9k(zk)\n\u0001\n,\n(4.2)\n\u2020 Actually we use here a deterministic version/special case of the theorem, where Yk, Zk, and Wk are nonnegative\nscalar sequences satisfying Yk+1 \u2264Yk \u2212Zk + Wk with P\u221e\nk=0 Wk < \u221e. Then the sequence Yk must converge. This\nversion is given with proof in many sources, including [BeT96] (Lemma 3.4), and [BeT00] (Lemma 1).\n25\nzk = PX\n\u0000xk \u2212\u03b1 \u02dc\u2207h\u03c9k(zk)\n\u0001\n,\nxk+1 = zk \u2212\u03b1 \u02dc\u2207f\u03c9k(xk+1),\n(4.3)\nrespectively, where {\u03c9k} is a sequence of random variables, taking values from the index set {1, . . . , m}.\nWe assume the following throughout the present section.\nAssumption 4.1: [For iterations (4.1) and (4.2)]\n(a) {\u03c9k} is a sequence of random variables, each uniformly distributed over {1, . . . , m}, and such\nthat for each k, \u03c9k is independent of the past history {xk, zk\u22121, xk\u22121, . . . , z0, x0}.\n(b) There is a constant c \u2208\u211csuch that for all k, we have with probability 1\nmax\n\b\n\u2225\u02dc\u2207fi(zi\nk)\u2225, \u2225\u02dc\u2207hi(zi\nk)\u2225\n\t\n\u2264c,\n\u2200i = 1, . . . , m,\n(4.4)\nmax\n\b\nfi(xk) \u2212fi(zi\nk), hi(xk) \u2212hi(zi\nk)\n\t\n\u2264c\u2225xk \u2212zi\nk\u2225,\n\u2200i = 1, . . . , m,\n(4.5)\nwhere zi\nk is the result of the proximal iteration, starting at xk if \u03c9k would be i, i.e.,\nzi\nk = arg min\nx\u2208X\n\u001a\nfi(x) +\n1\n2\u03b1k\n\u2225x \u2212xk\u22252\n\u001b\n,\nin the case of iteration (4.1), and\nzi\nk = arg min\nx\u2208\u211cn\n\u001a\nfi(x) +\n1\n2\u03b1k\n\u2225x \u2212xk\u22252\n\u001b\n,\nin the case of iteration (4.2).\nAssumption 4.2: [For iteration (4.3)]\n(a) {\u03c9k} is a sequence of random variables, each uniformly distributed over {1, . . . , m}, and such\nthat for each k, \u03c9k is independent of the past history {xk, zk\u22121, xk\u22121, . . . , z0, x0}.\n(b) There is a constant c \u2208\u211csuch that for all k, we have with probability 1\nmax\n\b\n\u2225\u02dc\u2207fi(xi\nk+1)\u2225, \u2225\u02dc\u2207hi(xk)\u2225\n\t\n\u2264c,\n\u2200i = 1, . . . , m,\n(4.6)\nfi(xk) \u2212fi(xi\nk+1) \u2264c\u2225xk \u2212xi\nk+1\u2225,\n\u2200i = 1, . . . , m,\n(4.7)\n26\nwhere xi\nk+1 is the result of the iteration, starting at xk if \u03c9k would be i, i.e.,\nxi\nk+1 = PX\n\u0000zi\nk \u2212\u03b1k \u02dc\u2207fi(xi\nk+1)\n\u0001\n,\nwith\nzi\nk = xk \u2212\u03b1k \u02dc\u2207hi(xk).\nNote that condition (4.5) is satis\ufb01ed if there exist subgradients of fi and hi at xk with norms less or\nequal to c. Thus the conditions (4.4) and (4.5) are similar, the main di\ufb00erence being that the \ufb01rst applies\nto \u201cslopes\u201d of fi and hi at zi\nk while the second applies to the \u201cslopes\u201d of fi and hi at xk. As in the case\nof Assumption 3.1, these conditions are guaranteed by Lipschitz continuity assumptions on fi and hi. The\nconvergence analysis of the randomized algorithms of this section is somewhat more complicated than the\none of the cyclic order counterparts, and relies on the Supermartingale Convergence Theorem. The following\nproposition deals with the case of a constant stepsize, and parallels Prop. 3.2 for the cyclic order case.\nProposition 4.1:\nLet {xk} be the sequence generated by one of the randomized incremental\nmethods (4.1)-(4.3), and let the stepsize \u03b1k be \ufb01xed at some positive constant \u03b1.\n(a) If F \u2217= \u2212\u221e, then with probability 1\ninf\nk\u22650 F(xk) = F \u2217.\n(b) If F \u2217> \u2212\u221e, then with probability 1\ninf\nk\u22650 F(xk) \u2264F \u2217+ \u03b1\u03b2mc2\n2\n,\nwhere \u03b2 = 5.\nProof:\nConsider \ufb01rst algorithms (4.1) and (4.2). By adapting the proof argument of Prop. 3.1 with Fik\nreplaced by F\u03c9k [cf. Eq. (3.9)], we have\n\u2225xk+1 \u2212y\u22252 \u2264\u2225xk \u2212y\u22252 \u22122\u03b1\n\u0000F\u03c9k(zk) \u2212F\u03c9k(y)\n\u0001\n+ \u03b12c2,\n\u2200y \u2208X,\nk \u22650.\nBy taking the conditional expectation with respect to Fk = {xk, zk\u22121, . . . , z0, x0}, and using the fact that\n27\n\u03c9k takes the values i = 1, . . . , m with equal probability 1/m, we obtain for all y \u2208X and k,\nE\n\b\n\u2225xk+1 \u2212y\u22252 | Fk\n\t\n\u2264\u2225xk \u2212y\u22252 \u22122\u03b1E\n\b\nF\u03c9k(zk) \u2212F\u03c9k(y) | Fk\n\t\n+ \u03b12c2\n= \u2225xk \u2212y\u22252 \u22122\u03b1\nm\nm\nX\ni=1\n\u0000Fi(zi\nk) \u2212Fi(y)\n\u0001\n+ \u03b12c2\n= \u2225xk \u2212y\u22252 \u22122\u03b1\nm\n\u0000F(xk) \u2212F(y)\n\u0001\n+ 2\u03b1\nm\nm\nX\ni=1\n\u0000Fi(xk) \u2212Fi(zi\nk)\n\u0001\n+ \u03b12c2.\n(4.8)\nBy using Eqs. (4.4) and (4.5),\nm\nX\ni=1\n\u0000Fi(xk) \u2212Fi(zi\nk)\n\u0001\n\u22642c\nm\nX\ni=1\n\u2225xk \u2212zi\nk\u2225= 2c\u03b1\nm\nX\ni=1\n\u2225\u02dc\u2207fi(zi\nk)\u2225\u22642m\u03b1c2.\nBy combining the preceding two relations, we obtain\nE\n\b\n\u2225xk+1 \u2212y\u22252 | Fk\n\t\n\u2264\u2225xk \u2212y\u22252 \u22122\u03b1\nm\n\u0000F(xk) \u2212F(y)\n\u0001\n+ 4\u03b12c2 + \u03b12c2\n= \u2225xk \u2212y\u22252 \u22122\u03b1\nm\n\u0000F(xk) \u2212F(y)\n\u0001\n+ \u03b2\u03b12c2,\n(4.9)\nwhere \u03b2 = 5.\nThe preceding equation holds also for algorithm (4.3). To see this note that Eq. (3.16) yields for all\ny \u2208X\n\u2225xk+1 \u2212y\u22252 \u2264\u2225xk \u2212y\u22252 \u22122\u03b1\n\u0000F\u03c9k(xk) \u2212F\u03c9k(y)\n\u0001\n+ \u03b12c2 + 2\u03b1\n\u0000f\u03c9k(xk) \u2212f\u03c9k(xk+1)\n\u0001\n,\n(4.10)\nand similar to Eq. (4.8), we obtain\nE\n\b\n\u2225xk+1 \u2212y\u22252 | Fk\n\t\n\u2264\u2225xk \u2212y\u22252 \u22122\u03b1\nm\n\u0000F(xk) \u2212F(y)\n\u0001\n+ 2\u03b1\nm\nm\nX\ni=1\n\u0000fi(xk) \u2212fi(xi\nk+1)\n\u0001\n+ \u03b12c2.\n(4.11)\nFrom Eq. (4.7), we have\nfi(xk) \u2212fi(xi\nk+1) \u2264c\u2225xk \u2212xi\nk+1\u2225,\nand from Eq. (4.6) and the nonexpansion property of the projection,\n\u2225xk \u2212xi\nk+1\u2225\u2264\n\r\rxk \u2212zi\nk + \u03b1 \u02dc\u2207fi(xi\nk+1)\n\r\r =\n\r\rxk \u2212xk + \u03b1 \u02dc\u2207hi(xk) + \u03b1 \u02dc\u2207fi(xi\nk+1)\n\r\r \u22642\u03b1c.\nCombining the preceding inequalities, we obtain Eq. (4.9) with \u03b2 = 5.\nLet us \ufb01x a positive scalar \u03b3, consider the level set L\u03b3 de\ufb01ned by\nL\u03b3 =\n\uf8f1\n\uf8f2\n\uf8f3\nn\nx \u2208X | F(x) < \u2212\u03b3 + 1 + \u03b1\u03b2mc2\n2\no\nif F \u2217= \u2212\u221e,\nn\nx \u2208X | F(x) < F \u2217+ 2\n\u03b3 + \u03b1\u03b2mc2\n2\no\nif F \u2217> \u2212\u221e,\nand let y\u03b3 \u2208X be such that\nF(y\u03b3) =\n( \u2212\u03b3\nif F \u2217= \u2212\u221e,\nF \u2217+ 1\n\u03b3\nif F \u2217> \u2212\u221e.\n28\nNote that y\u03b3 \u2208L\u03b3 by construction. De\ufb01ne a new process {\u02c6xk} that is identical to {xk}, except that once xk\nenters the level set L\u03b3, the process terminates with \u02c6xk = y\u03b3. We will now argue that for any \ufb01xed \u03b3, {\u02c6xk}\n(and hence also {xk}) will eventually enter L\u03b3, which will prove both parts (a) and (b).\nUsing Eq. (4.9) with y = y\u03b3, we have\nE\n\b\n\u2225\u02c6xk+1 \u2212y\u03b3\u22252 | Fk\n\t\n\u2264\u2225\u02c6xk \u2212y\u03b3\u22252 \u22122\u03b1\nm\n\u0000F(\u02c6xk) \u2212F(y\u03b3)\n\u0001\n+ \u03b2\u03b12c2,\nfrom which\nE\n\b\n\u2225\u02c6xk+1 \u2212y\u03b3\u22252 | Fk\n\t\n\u2264\u2225\u02c6xk \u2212y\u03b3\u22252 \u2212vk,\n(4.12)\nwhere\nvk =\n( 2\u03b1\nm\n\u0000F(\u02c6xk) \u2212F(y\u03b3)\n\u0001\n\u2212\u03b2\u03b12c2\nif \u02c6xk /\u2208L\u03b3,\n0\nif \u02c6xk = y\u03b3.\nThe idea of the subsequent argument is to show that as long as \u02c6xk /\u2208L\u03b3, the scalar vk (which is a measure\nof progress) is strictly positive and bounded away from 0.\n(a) Let F \u2217= \u2212\u221e. Then if \u02c6xk /\u2208L\u03b3, we have\nvk = 2\u03b1\nm\n\u0000F(\u02c6xk) \u2212F(y\u03b3)\n\u0001\n\u2212\u03b2\u03b12c2\n\u22652\u03b1\nm\n\u0012\n\u2212\u03b3 + 1 + \u03b1\u03b2mc2\n2\n+ \u03b3\n\u0013\n\u2212\u03b2\u03b12c2\n= 2\u03b1\nm .\nSince vk = 0 for \u02c6xk \u2208L\u03b3, we have vk \u22650 for all k, and by Eq. (4.12) and the Supermartingale Convergence\nTheorem (cf. Prop. 2.2), P\u221e\nk=0 vk < \u221eimplying that \u02c6xk \u2208L\u03b3 for su\ufb03ciently large k, with probability 1.\nTherefore, in the original process we have\ninf\nk\u22650 F(xk) \u2264\u2212\u03b3 + 1 + \u03b1\u03b2mc2\n2\nwith probability 1. Letting \u03b3 \u2192\u221e, we obtain infk\u22650 F(xk) = \u2212\u221ewith probability 1.\n(b) Let F \u2217> \u2212\u221e. Then if \u02c6xk /\u2208L\u03b3, we have\nvk = 2\u03b1\nm\n\u0000F(\u02c6xk) \u2212F(y\u03b3)\n\u0001\n\u2212\u03b2\u03b12c2\n\u22652\u03b1\nm\n\u0012\nF \u2217+ 2\n\u03b3 + \u03b1\u03b2mc2\n2\n\u2212F \u2217\u22121\n\u03b3\n\u0013\n\u2212\u03b2\u03b12c2\n= 2\u03b1\nm\u03b3 .\nHence, vk \u22650 for all k, and by the Supermartingale Convergence Theorem, we have P\u221e\nk=0 vk < \u221eimplying\nthat \u02c6xk \u2208L\u03b3 for su\ufb03ciently large k, so that in the original process,\ninf\nk\u22650 F(xk) \u2264F \u2217+ 2\n\u03b3 + \u03b1\u03b2mc2\n2\n29\nwith probability 1. Letting \u03b3 \u2192\u221e, we obtain infk\u22650 F(xk) \u2264F \u2217+ \u03b1\u03b2mc2/2.\nQ.E.D.\nBy comparing Prop. 4.1(b) with Prop. 3.2(b), we see that when F \u2217> \u2212\u221eand the stepsize \u03b1 is\nconstant, the randomized methods (4.1), (4.2), and (4.3), have a better error bound (by a factor m) than\ntheir nonrandomized counterparts.\nIn fact an example given in p. 514 of [BNO03] for the incremental\nsubgradient method can be adapted to show that the bound of Prop. 3.2(b) is tight in the sense that for a\nbad problem/cyclic order we have lim infk\u2192\u221eF(xk)\u2212F \u2217= O(\u03b1m2c2). By contrast the randomized method\nwill get to within O(\u03b1mc2) with probability 1 for any problem, according to Prop. 4.1(b). Thus with the\nrandomized algorithm we do not run the risk of choosing by accident a bad cyclic order. A related result\nis provided by the following proposition, which should be compared with Prop. 3.3 for the nonrandomized\nmethods.\nProposition 4.2:\nAssume that X\u2217is nonempty. Let {xk} be a sequence generated as in Prop. 4.1.\nThen for any positive scalar \u01eb, we have with probability 1\nmin\n0\u2264k\u2264N F(xk) \u2264F \u2217+ \u03b1\u03b2mc2 + \u01eb\n2\n,\n(4.13)\nwhere N is a random variable with\nE\n\b\nN\n\t\n\u2264m dist(x0; X\u2217)2\n\u03b1\u01eb\n.\n(4.14)\nProof:\nLet \u02c6y be some \ufb01xed vector in X\u2217. De\ufb01ne a new process {\u02c6xk} which is identical to {xk} except that\nonce xk enters the level set\nL =\n\u001a\nx \u2208X\n\f\f\f F(x) < F \u2217+ \u03b1\u03b2mc2 + \u01eb\n2\n\u001b\n,\nthe process {\u02c6xk} terminates at \u02c6y. Similar to the proof of Prop. 4.1 [cf. Eq. (4.9) with y being the closest\npoint of \u02c6xk in X\u2217], for the process {\u02c6xk} we obtain for all k,\nE\n\b\ndist(\u02c6xk+1; X\u2217)2 | Fk\n\t\n\u2264E\n\b\n\u2225\u02c6xk+1 \u2212y\u22252 | Fk\n\t\n\u2264dist(\u02c6xk; X\u2217)2 \u22122\u03b1\nm\n\u0000F(\u02c6xk) \u2212F \u2217\u0001\n+ \u03b2\u03b12c2\n= dist(\u02c6xk; X\u2217)2 \u2212vk,\n(4.15)\nwhere Fk = {xk, zk\u22121, . . . , z0, x0} and\nvk =\n\u001a 2\u03b1\nm\n\u0000F(\u02c6xk) \u2212F \u2217\u0001\n\u2212\u03b2\u03b12c2\nif \u02c6xk \u0338\u2208L,\n0\notherwise.\n30\nIn the case where \u02c6xk \u0338\u2208L, we have\nvk \u22652\u03b1\nm\n\u0012\nF \u2217+ \u03b1\u03b2mc2 + \u01eb\n2\n\u2212F \u2217\n\u0013\n\u2212\u03b2\u03b12c2 = \u03b1\u01eb\nm .\n(4.16)\nBy the Supermartingale Convergence Theorem (cf. Prop. 2.2), from Eq. (4.15) we have\n\u221e\nX\nk=0\nvk < \u221e\nwith probability 1, so that vk = 0 for all k \u2265N, where N is a random variable. Hence \u02c6xN \u2208L with\nprobability 1, implying that in the original process we have\nmin\n0\u2264k\u2264N F(xk) \u2264F \u2217+ \u03b1\u03b2mc2 + \u01eb\n2\nwith probability 1. Furthermore, by taking the total expectation in Eq. (4.15), we obtain for all k,\nE\n\b\ndist(\u02c6xk+1; X\u2217)2\t\n\u2264E\n\b\ndist(\u02c6xk; X\u2217)2\t\n\u2212E{vk} \u2264dist(\u02c6x0; X\u2217)2 \u2212E\n\uf8f1\n\uf8f2\n\uf8f3\nk\nX\nj=0\nvj\n\uf8fc\n\uf8fd\n\uf8fe,\nwhere in the last inequality we use the facts \u02c6x0 = x0 and E\n\b\ndist(\u02c6x0; X\u2217)2\t\n= dist(\u02c6x0; X\u2217)2. Therefore,\nletting k \u2192\u221e, and using the de\ufb01nition of vk and Eq. (4.16),\ndist(\u02c6x0; X\u2217)2 \u2265E\n( \u221e\nX\nk=0\nvk\n)\n= E\n(N\u22121\nX\nk=0\nvk\n)\n\u2265E\n\u001aN\u03b1\u01eb\nm\n\u001b\n= \u03b1\u01eb\nm E\n\b\nN\n\t\n.\nQ.E.D.\nLike Prop. 4.1, a comparison of Props. 3.3 and 4.2 again suggests an advantage for the randomized\nmethods: compared to their deterministic counterparts, they achieve a much smaller error tolerance (a\nfactor of m), in the same expected number of iterations. Note, however, that the preceding assessment is\nbased on upper bound estimates, which may not be sharp on a given problem [although the bound of Prop.\n3.2(b) is tight with a worst-case problem selection as mentioned earlier; see [BNO03], p. 514]. Moreover, the\ncomparison based on worst-case values versus expected values may not be strictly valid. In particular, while\nProp. 3.3 provides an upper bound estimate on N, Prop. 4.2 provides an upper bound estimate on E{N},\nwhich is not quite the same.\nFinally for the case of a diminishing stepsize, let us give the following proposition, which parallels Prop.\n3.4 for the cyclic order.\n31\nProposition 4.3:\nLet {xk} be the sequence generated by one of the randomized incremental\nmethods (4.1)-(4.3), and let the stepsize \u03b1k satisfy\nlim\nk\u2192\u221e\u03b1k = 0,\n\u221e\nX\nk=0\n\u03b1k = \u221e.\nThen, with probability 1,\nlim inf\nk\u2192\u221eF(xk) = F \u2217.\nFurthermore, if X\u2217is nonempty and\n\u221e\nX\nk=0\n\u03b12\nk < \u221e,\nthen {xk} converges to some x\u2217\u2208X\u2217with probability 1.\nProof:\nThe proof of the \ufb01rst part is nearly identical to the corresponding part of Prop. 3.4. To prove the\nsecond part, similar to the proof of Prop. 4.1, we obtain for all k and all x\u2217\u2208X\u2217,\nE\n\b\n\u2225xk+1 \u2212x\u2217\u22252 | Fk\n\t\n\u2264\u2225xk \u2212x\u2217\u22252 \u22122\u03b1k\nm\n\u0000F(xk) \u2212F \u2217\u0001\n+ \u03b2\u03b12\nkc2\n(4.17)\n[cf. Eq. (4.9) with \u03b1 and y replaced with \u03b1k and x\u2217, respectively], where Fk = {xk, zk\u22121, . . . , z0, x0}. By the\nSupermartingale Convergence Theorem (Prop. 2.2), for each x\u2217\u2208X\u2217, there is a set \u2126x\u2217of sample paths of\nprobability 1 such that for each sample path in \u2126x\u2217\n\u221e\nX\nk=0\n2\u03b1k\nm\n\u0000F(xk) \u2212F \u2217\u0001\n< \u221e,\n(4.18)\nand the sequence {\u2225xk \u2212x\u2217\u2225} converges.\nLet {vi} be a countable subset of the relative interior ri(X\u2217) that is dense in X\u2217[such a set exists since\nri(X\u2217) is a relatively open subset of the a\ufb03ne hull of X\u2217; an example of such a set is the intersection of X\u2217\nwith the set of vectors of the form x\u2217+ Pp\ni=1 ri\u03bei, where \u03be1, . . . , \u03bep are basis vectors for the a\ufb03ne hull of X\u2217\nand ri are rational numbers]. Let also \u2126vi be the set of sample paths de\ufb01ned earlier that corresponds to vi.\nThe intersection\n\u00af\u2126= \u2229\u221e\ni=1\u2126vi\nhas probability 1, since its complement \u00af\u2126c is equal to \u222a\u221e\ni=1\u2126cvi and\nProb(\u222a\u221e\ni=1\u2126cvi) \u2264\n\u221e\nX\ni=1\nProb (\u2126cvi) = 0.\n32\nFor each sample path in \u00af\u2126, all the sequences {\u2225xk \u2212vi\u2225} converge so that {xk} is bounded, while by\nthe \ufb01rst part of the proposition [or Eq. (4.18)] lim infk\u2192\u221eF(xk) = F \u2217. Therefore, {xk} has a limit point\n\u00afx in X\u2217. Since {vi} is dense in X\u2217, for every \u01eb > 0 there exists vi(\u01eb) such that \u2225\u00afx \u2212vi(\u01eb)\u2225< \u01eb. Since the\nsequence {\u2225xk \u2212vi(\u01eb)\u2225} converges and \u00afx is a limit point of {xk}, we have limk\u2192\u221e\u2225xk \u2212vi(\u01eb)\u2225< \u01eb, so that\nlim sup\nk\u2192\u221e\n\u2225xk \u2212\u00afx\u2225\u2264lim\nk\u2192\u221e\u2225xk \u2212vi(\u01eb)\u2225+ \u2225vi(\u01eb) \u2212\u00afx\u2225< 2\u01eb.\nBy taking \u01eb \u21920, it follows that xk \u2192\u00afx.\nQ.E.D.\n5.\nSOME APPLICATIONS\nIn this section we illustrate our methods in the context of two types of practical applications, and discuss\nrelations with known algorithms.\n5.1\nRegularized Least Squares\nLet us consider least squares problems, involving minimization of a sum of quadratic component functions\nfi(x) that correspond to errors between data and the output of a model that is parameterized by a vector\nx. Often a convex regularization function R(x) is added to the least squares objective, to induce desirable\nproperties of the solution. This gives rise to problems of the form\nminimize\nR(x) + 1\n2\nm\nX\ni=1\n(c\u2032\nix \u2212di)2\nsubject to x \u2208\u211cn,\n(5.1)\nwhere ci and di are given vectors and scalars, respectively, and \u03b3 is a positive scalar. When R is di\ufb00erentiable\n(e.g., quadratic), and either m is very large or the data (ci, di) become available sequentially over time, it\nmakes sense to consider incremental gradient methods, which have a long history of applications over the\nlast 50 years, starting with the Widrow-Ho\ufb00least mean squares (LMS) method [WiH60].\nThe classical type of regularization involves a quadratic function R (as in classical regression and the\nLMS method), but nondi\ufb00erentiable regularization functions have become increasingly important recently.\nOn the other hand, to apply our incremental methods, a quadratic R is not essential. What is important\nis that R has a simple form that facilitates the use of proximal algorithms, such as for example a separable\nform, so that the proximal iteration on R is simpli\ufb01ed through decomposition. As an example, consider the\n\u21131-regularization problem, where\nR(x) = \u03b3\u2225x\u22251 = \u03b3\nn\nX\nj=1\n|xj|,\n(5.2)\n33\n\u03b3 is a positive scalar and xj is the jth coordinate of x. Then the proximal iteration\nzk = arg min\nx\u2208\u211cn\n\u001a\n\u03b3 \u2225x\u22251 +\n1\n2\u03b1k\n\u2225x \u2212xk\u22252\n\u001b\ndecomposes into the n one-dimensional minimizations\nzj\nk = arg min\nxj\u2208\u211c\n\u001a\n\u03b3 |xj| +\n1\n2\u03b1k\n|xj \u2212xj\nk|2\n\u001b\n,\nj = 1, . . . , n,\nand can be done in closed form\nzj\nk =\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f3\nxj\nk \u2212\u03b3\u03b1k\nif \u03b3\u03b1k \u2264xj\nk,\n0\nif \u2212\u03b3\u03b1k < xj\nk < \u03b3\u03b1k,\nxj\nk + \u03b3\u03b1k\nif xj\nk \u2264\u2212\u03b3\u03b1k,\nj = 1, . . . , n.\n(5.3)\nWe refer to Figueiredo, Nowak, and Wright [FNW07], Wright, Nowak, and Figueiredo [WNF08], Beck\nand Teboulle [BeT10], and the references given there, for a discussion of a broad variety of applications in\nestimation and signal processing problems, where nondi\ufb00erentiable regularization functions play an important\nrole.\nWe now note that the incremental algorithms of this paper are well-suited for solution of \u21131-regularization\nproblems of the form (5.1)-(5.2). For example, the kth incremental iteration may consist of selecting a data\npair (cik, dik) and performing a proximal iteration of the form (5.3) to obtain zk, followed by a gradient\niteration on the component 1\n2(c\u2032\nikx \u2212dik)2, starting at zk:\nxk+1 = zk \u2212\u03b1kcik(c\u2032\nikzk \u2212dik).\nThis algorithm is the special case of the algorithms (2.12)-(2.14) (here X = \u211cn, and all three algorithms\ncoincide), with fi(x) being \u03b3\u2225x\u22251 (we use m copies of this function) and hi(x) =\n1\n2(c\u2032\nix \u2212di)2. It can be\nviewed as an incremental version of a popular class of algorithms in signal processing, known as iterative\nshrinkage/thresholding (see Chambolle et. al. [CDL98], Figueiredo and Nowak [FiN03], Daubechies, Defrise,\nand Mol [DDM04], Combettes and Wajs [CoW05], Bioucas-Dias and Figueiredo [BiF07], Elad, Matalon, and\nZibulevsky [EMZ07], Beck and Teboulle [BeT09], [BeT10]). Our methods bear the same relation to this class\nof algorithms as the LMS method bears to gradient algorithms for the classical linear least squares problem\nwith quadratic regularization function.\nFinally, let us note that as an alternative, the proximal iteration (5.3) could be replaced by a proximal\niteration on \u03b3 |xj| for some selected index j, with all indexes selected cyclically in incremental iterations.\nRandomized selection of the data pair (cik, dik) would also be interesting, particularly in contexts where the\ndata has a natural stochastic interpretation.\n34\n5.2\nIterated Projection Algorithms\nA feasibility problem that arises in many contexts involves \ufb01nding a point with certain properties within a\nset intersection \u2229m\ni=1Xi, where each Xi is a closed convex set. For the case where m is large and each of the\nsets Xi has a simple form, incremental methods that make successive projections on the component sets Xi\nhave a long history (see e.g., Gubin, Polyak, and Raik [GPR67], and recent papers such as Bauschke [Bau01],\nBauschke, Combettes, and Kruk [BCL06], and Cegielski and Suchocka [CeS08], and their bibliographies).\nWe may consider the following generalized version of the classical feasibility problem,\nminimize\nf(x)\nsubject to x \u2208\u2229m\ni=1Xi,\n(5.4)\nwhere f : \u211cn 7\u2192\u211cis a convex cost function, and the method\nxk+1 = PXik\n\u0000xk \u2212\u03b1k \u02dc\u2207f(xk)\n\u0001\n,\n(5.5)\nwhere the index ik is chosen from {1, . . . , m} according to a randomized rule. Incremental algorithms for\nproblem (5.4), which bear some relation with ours have been recently proposed by Nedi\u00b4c [Ned10]. Actually,\nthe algorithm of [Ned10] involves an additional projection on a special set X0 at each iteration, but for\nsimplicity we will take X0 = \u211cn. The incremental approach is particularly well-suited for problems of the\nform (5.4) where the sets Xi are not known in advance, but are revealed as the algorithm progresses.\nWhile the problem (5.4) does not involve a sum of component functions, it may be converted into one\nthat does by using an exact penalty function. In particular, consider the problem\nminimize\nf(x) + \u03b3\nm\nX\ni=1\ndist(x; Xi)\nsubject to x \u2208\u211cn,\n(5.6)\nwhere \u03b3 is a positive penalty parameter. Then for f Lipschitz continuous and \u03b3 su\ufb03ciently large, problems\n(5.4) and (5.6) are equivalent. We show this for the case where m = 1 and then we generalize.\nProposition 5.1:\nLet f : Y 7\u2192\u211cbe a function de\ufb01ned on a subset Y of \u211cn, and let X be a\nnonempty closed subset of Y . Assume that f is Lipschitz continuous over Y with constant L, i.e.,\n\f\ff(x) \u2212f(y)\n\f\f \u2264L\u2225x \u2212y\u2225,\n\u2200x, y \u2208Y,\nand let \u03b3 be a scalar with \u03b3 > L. Then the set of minima of f over X coincides with the set of minima\nof\nf(x) + \u03b3 dist(x; X)\nover Y .\n35\nProof:\nDenote F(x) = f(x) + \u03b3 dist(x; X). For a vector x \u2208Y , let \u02c6x denote a vector of X that is at\nminimum distance from X. If \u03b3 > L, we have\nF(x) = f(x) + \u03b3\u2225x \u2212\u02c6x\u2225= f(\u02c6x) +\n\u0000f(x) \u2212f(\u02c6x)\n\u0001\n+ \u03b3\u2225x \u2212\u02c6x\u2225\u2265f(\u02c6x) + (\u03b3 \u2212L)\u2225x \u2212\u02c6x\u2225\u2265F(\u02c6x),\n\u2200x \u2208Y,\nwith strict inequality if x \u0338= \u02c6x; here the \ufb01rst inequality follows using the Lipschitz property of f to write\nf(x) \u2212f(\u02c6x) \u2265\u2212L\u2225x \u2212\u02c6x\u2225,\nwhile the second inequality follows from the fact f(\u02c6x) = F(\u02c6x). In words, the value of F(x) is strictly reduced\nwhen we project an x \u2208Y with x /\u2208X onto X. Hence the minima of F over Y can only lie within X, while\nF = f within X. Thus all minima of F over Y must lie in X and also minimize f over X (since F = f\non X). Conversely, all minima of f over X are also minima of F over X (since F = f on X), and by the\npreceding inequality, they are also minima of F over Y .\nQ.E.D.\nWe now provide a generalization for m > 1.\u2020\nProposition 5.2:\nLet f : Y 7\u2192\u211cbe a function de\ufb01ned on a subset Y of \u211cn, and let Xi, i = 1, . . . , m,\nbe closed subsets of Y with nonempty intersection. Assume that f is Lipschitz continuous over Y with\nconstant L, and that for some scalar \u03b2 > 0, we have\ndist(x; X1 \u2229\u00b7 \u00b7 \u00b7 \u2229Xm) \u2264\u03b2\nm\nX\ni=1\ndist(x; Xi),\n\u2200x \u2208Y.\n(5.7)\nLet \u03b3 be a scalar with \u03b3 > \u03b2L. Then the set of minima of f over \u2229m\ni=1Xi coincides with the set of\nminima of\nf(x) + \u03b3\nm\nX\ni=1\ndist(x; Xi)\nover Y .\nProof:\nThe proof is similar to the proof of Prop. 5.1, using Eq. (5.7) to modify the main inequality. Denote\nF(x) = f(x) + \u03b3 Pm\ni=1 dist(x; Xi) and X = X1 \u2229\u00b7 \u00b7 \u00b7 \u2229Xm. For a vector x \u2208Y , let \u02c6xi denote a vector of Xi\nthat is at minimum distance from x, and let \u02c6x denote a vector of X that is at minimum distance from x. If\n\u2020 In the original version of this report the assumption on existence of the scalar \u03b2 in the proposition below was\nneglected, due to a faulty application of Prop. 5.1 in its proof. This was noted in a paper by Kundu, Bach, and\nBhattacharrya in Oct. 2017. If the sets Xi are polyhedral this assumption is not necessary; this is Ho\ufb00man\u2019s lemma.\n36\n\u03b3 > \u03b2L, we have\nF(x) = f(x)+\u03b3\nm\nX\ni=1\n\u2225x\u2212\u02c6xi\u2225\u2265f(\u02c6x)+\n\u0000f(x)\u2212f(\u02c6x)\n\u0001\n+ \u03b3\n\u03b2 \u2225x\u2212\u02c6x\u2225\u2265f(\u02c6x)+\n\u0012 \u03b3\n\u03b2 \u2212L\n\u0013\n\u2225x\u2212\u02c6x\u2225\u2265F(\u02c6x),\n\u2200x \u2208Y,\nwith strict inequality if x \u0338= \u02c6x. The proof now proceeds as in the proof of Prop. 5.1.\nQ.E.D.\nRegarding algorithmic solution, from Prop. 5.2, it follows that we may consider in place of the original\nproblem (5.4) the additive cost problem (5.6) for which our algorithms apply. In particular, let us consider\nthe algorithms (2.12)-(2.14), with X = \u211cn, which involve a proximal iteration on one of the functions\n\u03b3 dist(x; Xi) followed by a subgradient iteration on f. A key fact here is that the proximal iteration\nzk = arg min\nx\u2208\u211cn\n\u001a\n\u03b3 dist(x; Xik) +\n1\n2\u03b1k\n\u2225x \u2212xk\u22252\n\u001b\n(5.8)\ninvolves a projection on Xik of xk, followed by an interpolation. This is shown in the following proposition.\nProposition 5.3:\nLet zk be the vector produced by the proximal iteration (5.8). If xk \u2208Xik then\nzk = xk, while if xk /\u2208Xik,\nzk =\n( (1 \u2212\u03b2k)xk + \u03b2kPXik (xk)\nif \u03b2k < 1,\nPXik (xk)\nif \u03b2k \u22651,\n(5.9)\nwhere\n\u03b2k =\n\u03b1k\u03b3\ndist(xk; Xik).\nProof:\nThe case xk \u2208Xik is evident, so assume that xk /\u2208Xik. From the nature of the cost function in\nEq. (5.8) we see that zk is a vector that lies in the line segment between xk and PXik (xk). Hence there are\ntwo possibilities: either\nzk = PXik (xk),\n(5.10)\nor zk /\u2208Xik in which case by setting to 0 the gradient at zk of the cost function in Eq. (5.8) yields\n\u03b3\nzk \u2212PXik (zk)\n\r\r\rzk \u2212PXik (zk)\n\r\r\r\n= 1\n\u03b1k\n(xk \u2212zk).\nThis equation implies that xk, zk, and PXik (zk) lie on the same line, so that PXik (zk) = PXik (xk) and\nzk = xk \u2212\n\u03b1k\u03b3\ndist(xk; Xik)\n\u0000xk \u2212PXik (xk)\n\u0001\n= (1 \u2212\u03b2k)xk + \u03b2kPXik (xk).\n(5.11)\n37\nBy calculating and comparing the value of the cost function in Eq. (5.8) for each of the possibilities (5.10)\nand (5.11), we can verify that (5.11) gives a lower cost if and only if \u03b2k < 1.\nQ.E.D.\nLet us now consider the problem\nminimize\nm\nX\ni=1\n\u0000fi(x) + hi(x)\n\u0001\nsubject to x \u2208\u2229m\ni=1Xi.\nBased on the preceding analysis, we can convert this problem to the unconstrained minimization problem\nminimize\nm\nX\ni=1\n\u0000fi(x) + hi(x) + \u03b3dist(x; Xi)\n\u0001\nsubject to x \u2208\u211cn,\nwhere \u03b3 is su\ufb03ciently large. The algorithm (2.14), applied to this problem, yields the iteration\nyk = xk \u2212\u03b1k \u02dc\u2207hik(xk),\nzk = yk \u2212\u03b1k \u02dc\u2207fik(zk),\nxk+1 =\n( (1 \u2212\u03b2k)zk + \u03b2kPXik (zk)\nif \u03b2k < 1,\nPXik (zk)\nif \u03b2k \u22651,\nwhere\n\u03b2k =\n\u03b1k\u03b3\ndist(zk; Xik),\n[cf. Eq. (5.9)]. The index ik may be chosen either randomly or according to a cyclic rule.\nLet us \ufb01nally note another problem where our incremental methods apply:\nminimize\nf(x) + c\nr\nX\nj=1\nmax\n\b\n0, gj(x)\n\t\nsubject to x \u2208\u2229m\ni=1Xi.\nThis type of problem is obtained by replacing convex inequality constraints of the form gj(x) \u22640 with\nthe nondi\ufb00erentiable penalty terms c max\n\b\n0, gj(x)\n\t\n, where c > 0 is a penalty parameter. Then a possible\nincremental method at each iteration, would either do a subgradient or proximal iteration on f, or select one\nof the violated constraints (if any) and perform a subgradient iteration on the corresponding function gj, or\nselect one of the sets Xi and do an interpolated projection on it. Related methods may also be obtained\nwhen f is replaced by a cost function of the form\nm\nX\ni=1\n\u0000fi(x) + hi(x)\n\u0001\n,\nand the components fi are dealt with a proximal iteration while the components hi are dealt with a subgra-\ndient iteration.\n38\nReferences\n6.\nCONCLUSIONS\nWe have surveyed incremental algorithms, which can deal with many of the challenges posed by large data\nsets in machine learning applications, as well as with the additive structure of many interesting problems,\nincluding those arising in the context of duality. We have used a uni\ufb01ed analytical framework that includes\nincremental proximal algorithms and their combinations with the more established incremental gradient\nand subgradient methods. This allows the \ufb02exibility to separate the cost function into the parts that are\nconveniently handled by proximal iterations (e.g., in essentially closed form), and the remaining parts to\nbe handled by subgradient iterations. We have outlined the convergence properties of these methods, and\nwe have shown that our algorithms apply to some important problems that have been the focus of recent\nresearch.\nMuch work remains to be done to apply and evaluate our methods within the broad context of potential\napplications. Let us mention some possibilities that may extend the range of applications of our approach, and\nare interesting subjects for further investigation: alternative proximal and projected subgradient iterations,\ninvolving nonquadratic proximal terms and/or subgradient projections, alternative stepsize rules, distributed\nasynchronous implementations along the lines of [NBB01], polyhedral approximation (bundle) variants of\nthe proximal iterations in the spirit of [BeY09], and variants for methods with errors in the calculation of\nthe subgradients along the lines of [NeB10].\n7.\nREFERENCES\n[BCL03] Bauschke, H. H., Combettes, P. L., and Luke, D. R., 2003. \u201cHybrid Projection-Re\ufb02ection Method for Phase\nRetrieval,\u201d Journal of the Optical Society of America, Vol. 20, pp. 1025-1034.\n[BCK06] Bauschke, H. H., Combettes, P. L., and Kruk, S. G., 2006. \u201cExtrapolation Algorithm for A\ufb03ne-Convex\nFeasibility Problems,\u201d Numer. Algorithms, Vol. 41, pp. 239-274.\n[BHG08] Blatt, D., Hero, A. O., Gauchman, H., 2008. \u201cA Convergent Incremental Gradient Method with a Constant\nStep Size,\u201d SIAM J. Optimization, Vol. 18, pp. 29-51.\n[BMN01] Ben-Tal, A., Margalit, T., and Nemirovski, A., 2001. \u201cThe Ordered Subsets Mirror Descent Optimization\nMethod and its Use for the Positron Emission Tomography Reconstruction,\u201d in Inherently Parallel Algorithms in Fea-\nsibility and Optimization and their Applications (D. Butnariu, Y. Censor, and S. Reich, eds.), Elsevier, Amsterdam,\nNetherlands.\n[BMS99] Boltyanski, V., Martini, H., and Soltan, V., 1999. Geometric Methods and Optimization Problems, Kluwer,\nBoston.\n[BNO03] Bertsekas, D. P., Nedi\u00b4c, A., and Ozdaglar, A. E., 2003. Convex Analysis and Optimization, Athena Scienti\ufb01c,\nBelmont, MA.\n39\nReferences\n[BPC10] Boyd, S., Parikh, N., Chu, E., Peleato, B., and Eckstein, J., 2010. \u201cDistributed Optimization and Statistical\nLearning via the Alternating Direction Method of Multipliers,\u201d working paper on line, Stanford, Univ.\n[Bau01] Bauschke, H. H., 2001. \u201cProjection Algorithms: Results and Open Problems,\u201d in Inherently Parallel Algo-\nrithms in Feasibility and Optimization and their Applications (D. Butnariu, Y. Censor, and S. Reich, eds.), Elsevier,\nAmsterdam, Netherlands.\n[BeT89] Bertsekas, D. P., and Tsitsiklis, J. N., 1989. Parallel and Distributed Computation: Numerical Methods,\nPrentice-Hall, Englewood Cli\ufb00s, N. J.\n[BeT96] Bertsekas, D. P., and Tsitsiklis, J. N., 1996. Neuro-Dynamic Programming, Athena Scienti\ufb01c, Belmont, MA.\n[BeT00] Bertsekas, D. P., and Tsitsiklis, J. N., 2000. \u201cGradient Convergence in Gradient Methods,\u201d SIAM J. Opti-\nmization, Vol. 10, pp. 627-642.\n[BeT09] Beck, A., and Teboulle, M., 2009. \u201cA Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse\nProblems,\u201d SIAM J. on Imaging Sciences, Vol. 2, pp. 183-202.\n[BeT10] Beck, A., and Teboulle, M., 2010. \u201cGradient-Based Algorithms with Applications to Signal-Recovery Prob-\nlems,\u201d in Convex Optimization in Signal Processing and Communications (Y. Eldar and D. Palomar, eds.), Cambridge\nUniversity Press, pp. 42-88.\n[BeY09] Bertsekas, D. P., and Yu, H., 2009. \u201cA Unifying Polyhedral Approximation Framework for Convex Optimiza-\ntion,\u201d Lab. for Information and Decision Systems Report LIDS-P-2820, MIT; to appear in SIAM J. on Optimization.\n[Ber83] Bertsekas, D. P., 1983. \u201cDistributed Asynchronous Computation of Fixed Points,\u201d Mathematical Program-\nming, Vol. 27, pp. 107-120.\n[Ber96] Bertsekas, D. P., 1996. \u201cIncremental Least Squares Methods and the Extended Kalman Filter,\u201d SIAM J. on\nOptimization, Vol. 6, pp. 807-822.\n[Ber97] Bertsekas, D. P., 1997. \u201cA Hybrid Incremental Gradient Method for Least Squares,\u201d SIAM J. on Optimization,\nVol. 7, pp. 913-926.\n[Ber99] Bertsekas, D. P., 1999. Nonlinear Programming, 2nd edition, Athena Scienti\ufb01c, Belmont, MA.\n[Ber09] Bertsekas, D. P., 2009. Convex Optimization Theory, Athena Scienti\ufb01c, Belmont, MA; also, this book\u2019s on-line\nsupplementary chapter on algorithms.\n[Ber10] Bertsekas, D. P., 2010. \u201cIncremental Proximal Methods for Large Scale Convex Optimization,\u201d Lab. for Info.\nand Decision Systems Report LIDS-P-2847, MIT, Cambridge, MA; to appear in Math. Programming J.\n[BiF07] Bioucas-Dias, J., and Figueiredo, M. A. T., 2007. \u201cA New TwIST: Two-Step Iterative Shrinkage/Thresholding\nAlgorithms for Image Restoration,\u201d IEEE Trans. Image Processing, Vol. 16, pp. 2992-3004.\n[Bor08] Borkar, V. S., 2008. Stochastic Approximation: A Dynamical Systems Viewpoint, Cambridge Univ. Press.\n40\nReferences\n[Bot05] Bottou, L., 2005. \u201cSGD: Stochastic Gradient Descent,\u201d http://leon.bottou.org/projects/sgd.\n[CDL98] Chambolle, A., DeVore, R. A., Lee, N. Y., and Lucier, B. J., 1998. \u201cNonlinear Wavelet Image Processing:\nVariational Problems, Compression, and Noise Removal Through Wavelet Shrinkage,\u201d IEEE Trans. Image Processing,\nVol. 7, pp. 319-335.\n[CeS08] Cegielski, A., and Suchocka, A., 2008. \u201cRelaxed Alternating Projection Methods,\u201d SIAM J. Optimization,\nVol. 19, pp. 1093-1106.\n[CoW05] Combettes, P. L., and Wajs, V. R., 2005. \u201cSignal Recovery by Proximal Forward-Backward Splitting,\u201d\nMultiscale Modeling and Simulation, Vol. 4, pp. 1168-1200.\n[DDM04] Daubechies, I., Defrise, M., and Mol, C. D., 2004. \u201cAn Iterative Thresholding Algorithm for Linear Inverse\nProblems with a Sparsity Constraint,\u201d Comm. Pure Appl. Math., Vol. 57, pp. 1413-1457.\n[DrH04] Drezner, Z., and Hamacher, H. W., 2004. Facility Location: Applications and Theory, Springer, N. Y.\n[DHS10] Duchi, J., Hazan, E., and Singer, Y., 2010. \u201cAdaptive Subgradient Methods for Online Learning and Stochas-\ntic Optimization,\u201d UC Berkeley EECS Technical Report 2010-24, to appear in J. of Machine Learning Research.\n[Dav76] Davidon, W. C., 1976. \u201cNew Least Squares Algorithms,\u201d J. Optimization Theory and Applications, Vol. 18,\npp. 187-197.\n[EMZ07] Elad, M., Matalon, B., and Zibulevsky, M., 2007. \u201cCoordinate and Subspace Optimization Methods for\nLinear Least Squares with Non-Quadratic Regularization,\u201d J. on Applied and Computational Harmonic Analysis,\nVol. 23, pp. 346-367.\n[EcB92] Eckstein, J., and Bertsekas, D. P., 1992. \u201cOn the Douglas-Rachford Splitting Method and the Proximal Point\nAlgorithm for Maximal Monotone Operators,\u201d Math. Programming, Vol. 55, pp. 293-318.\n[Erm69] Ermoliev, Yu. M., \u201cOn the Stochastic Quasi-Gradient Method and Stochastic Quasi-Feyer Sequences,\u201d\nKibernetika, No. 2, 1969, pp. 73\u201383.\n[Erm76] Ermoliev, Yu. M., Stochastic Programming Methods, Nauka, Moscow, 1976.\n[FiN03] Figueiredo, M. A. T., and Nowak, R. D., 2003. \u201cAn EM Algorithm for Wavelet-Based Image Restoration,\u201d\nIEEE Trans. Image Processing, Vol. 12, pp. 906-916.\n[FNW07] Figueiredo, M. A. T., Nowak, R. D., and Wright, S. J., 2007. \u201cGradient Projection for Sparse Reconstruction:\nApplication to Compressed Sensing and Other Inverse Problems,\u201d IEEE J. Sel. Topics in Signal Processing, Vol. 1,\npp. 586-597.\n[GGM06] Gaudioso, M., Giallombardo, G., and Miglionico, G., 2006. \u201cAn Incremental Method for Solving Convex\nFinite Min-Max Problems,\u201d Math. of Operations Research, Vol. 31, pp. 173-187.\n[GMS10] Goldfarb, D., Ma, S., and Scheinberg, K., 2010. \u201cFast Alternating Linearization Methods for Minimizing\nthe Sum of Two Convex Functions\u201d, Columbia Univ. report, on line.\n41\nReferences\n[GPR67] Gubin, L. G., Polyak, B. T., and Raik, E. V., 1967. \u201cThe Method of Projection for Finding the Common\nPoint in Convex Sets,\u201d U.S.S.R. Comput. Math. Phys., Vol. 7, pp. 124 (English Translation).\n[GaM76] Gabay, D., and Mercier, B., 1979. \u201cA Dual Algorithm for the Solution of Nonlinear Variational Problems\nvia Finite-Element Approximations,\u201d Comp. Math. Appl., Vol. 2, pp. 17-40.\n[Gab83] Gabay, D., 1983. \u201cApplications of the Method of Multipliers to Variational Inequalities,\u201d in M. Fortin and\nR. Glowinski, eds., Augmented Lagrangian Methods: Applications to the Solution of Boundary-Value Problems,\nNorth-Holland, Amsterdam.\n[GoM09] Goldfarb, D., and Ma, S., 2009. \u201cFast Multiple Splitting Algorithms for Convex Optimization,\u201d Columbia\nUniv. report, on line.\n[Gri94] Grippo, L., 1994. \u201cA Class of Unconstrained Minimization Methods for Neural Network Training,\u201d Optim.\nMethods and Software, Vol. 4, pp. 135-150.\n[Gri00] Grippo, L., 2000. \u201cConvergent On-Line Algorithms for Supervised Learning in Neural Networks,\u201d IEEE Trans.\nNeural Networks, Vol. 11, pp. 1284-1299.\n[HeD09] Helou, E. S., and De Pierro, A. R., 2009. \u201cIncremental Subgradients for Constrained Convex Optimization:\nA Uni\ufb01ed Framework and New Methods,\u201d SIAM J. on Optimization, Vol. 20, pp. 1547-1572.\n[JRJ09] Johansson, B., Rabi, M., and Johansson, M., 2009. \u201cA Randomized Incremental Subgradient Method for\nDistributed Optimization in Networked Systems,\u201d SIAM J. on Optimization, Vol. 20, pp. 1157-1170.\n[Kib80] Kibardin, V. M., 1980. \u201cDecomposition into Functions in the Minimization Problem,\u201d Automation and\nRemote Control, Vol. 40, pp. 1311-1323.\n[Kiw04] Kiwiel, K. C., 2004. \u201cConvergence of Approximate and Incremental Subgradient Methods for Convex Opti-\nmization,\u201d SIAM J. on Optimization, Vol. 14, pp. 807-840.\n[KuC78] Kushner, H. J., and Clark, D. S., 1978. Stochastic Approximation Methods for Constrained and Uncon-\nstrained Systems, Springer-Verlag, N. Y.\n[KuY97] Kushner, H. J., and Yin, G., 1997. Stochastic Approximation Methods, Springer-Verlag, N. Y.\n[LMY08] Lu, Z., Monteiro, R. D. C., and Yuan, M., 2008. \u201cConvex Optimization Methods for Dimension Reduction\nand Coe\ufb03cient Estimation in Multivariate Linear Regression,\u201d Report, School of Industrial and Systems Engineering,\nGeorgia Institute of Technology, Atlanta; appeared on line in Math. Programming J., 2010.\n[LeW10] Lee, S., and Wright, S. J., 2010. \u201cSparse Nonlinear Support Vector Machines via Stochastic Approximation,\u201d\nUniv. of Wisconsin Report, submitted.\n[LiM79] Lions, P. L., and Mercier, B., 1979. \u201cSplitting Algorithms for the Sum of Two Nonlinear Operators,\u201d SIAM\nJ. on Numerical Analysis, Vol. 16, pp. 964-979.\n[Lit66] Litvakov, B. M., 1966. \u201cOn an Iteration Method in the Problem of Approximating a Function from a Finite\n42\nReferences\nNumber of Observations,\u201d Avtom. Telemech., No. 4, pp. 104-113.\n[Lju77] Ljung, L., 1977. \u201cAnalysis of Recursive Stochastic Algorithms,\u201d IEEE Trans. on Automatic Control, Vol. 22,\npp. 551-575.\n[LuT94] Luo, Z. Q., and Tseng, P., 1994. \u201cAnalysis of an Approximate Gradient Projection Method with Applications\nto the Backpropagation Algorithm,\u201d Optimization Methods and Software, Vol. 4, pp. 85-101.\n[Luo91] Luo, Z. Q., 1991. \u201cOn the Convergence of the LMS Algorithm with Adaptive Learning Rate for Linear\nFeedforward Networks,\u201d Neural Computation, Vol. 3, pp. 226-245.\n[MYF03] Moriyama, H., Yamashita N., and Fukushima, M., 2003. \u201cThe Incremental Gauss-Newton Algorithm with\nAdaptive Stepsize Rule,\u201d Computational Optimization and Applications, Vol. 26, pp. 107-141.\n[MaS94] Mangasarian, O. L., and Solodov, M. V., 1994. \u201cSerial and Parallel Backpropagation Convergence Via\nNonmonotone Perturbed Minimization,\u201d Opt. Methods and Software, Vol. 4, pp. 103-116.\n[Mar70] Martinet, B., 1970. \u201cRegularisation d\u2019 In\u00b4equations Variationelles par Approximations Successives,\u201d Revue\nFran. d\u2019Automatique et Infomatique Rech. Op\u00b4erationelle, Vol. 4, pp. 154-159.\n[Mey07] Meyn, S., 2007. Control Techniques for Complex Networks, Cambridge University Press, N. Y.\n[NBB01] Nedi\u00b4c, A., Bertsekas, D. P., and Borkar, V., 2001. \u201cDistributed Asynchronous Incremental Subgradient\nMethods,\u201d in Inherently Parallel Algorithms in Feasibility and Optimization and their Applications (D. Butnariu, Y.\nCensor, and S. Reich, eds.), Elsevier, Amsterdam, Netherlands.\n[NJL09] Nemirovski, A., Juditsky, A., Lan, G., and Shapiro, A., 2009. \u201cRobust Stochastic Approximation Approach\nto Stochastic Programming,\u201d SIAM Journal on Optimization, Vol. 19, pp. 1574-1609.\n[NeB00] Nedi\u00b4c, A., and Bertsekas, D. P., 2000. \u201cConvergence Rate of the Incremental Subgradient Algorithm,\u201d in\nStochastic Optimization: Algorithms and Applications, Eds., S. Uryasev and P. M. Pardalos, Kluwer Academic\nPublishers, pp. 263-304.\n[NeB01] Nedi\u00b4c, A., and Bertsekas, D. P., 2001. \u201cIncremental Subgradient Methods for Nondi\ufb00erentiable Optimiza-\ntion,\u201d SIAM J. on Optimization, Vol. 12, 2001, pp. 109-138.\n[NeB10] Nedi\u00b4c, A., and Bertsekas, D. P., 2010. \u201cThe E\ufb00ect of Deterministic Noise in Subgradient Methods,\u201d Math.\nProgramming, Ser. A, Vol. 125, pp. 75-99.\n[NeO09] Nedi\u00b4c, A., and Ozdaglar, A., 2009. \u201cDistributed Subgradient Methods for Multi-Agent Optimization,\u201d IEEE\nTrans. on Aut. Control, Vol. 54, pp. 48-61.\n[Ned10] Nedi\u00b4c, A., 2010. \u201cRandom Projection Algorithms for Convex Minimization Problems,\u201d Univ. of Illinois\nReport; appear in Math. Programming Journal.\n[Nes83] Nesterov, Y., 1983. \u201cA Method for Unconstrained Convex Minimization Problem with the Rate of Convergence\nO(1/k2),\u201d Doklady AN SSSR 269, pp. 543-547; translated as Soviet Math. Dokl.\n43\nReferences\n[Nes04] Nesterov, Y., 2004. Introductory Lectures on Convex Optimization, Kluwer Academic Publisher, Dordrecht,\nThe Netherlands.\n[Nes05] Nesterov, Y., 2005. \u201cSmooth Minimization of Nonsmooth Functions,\u201d Math. Programming, Vol. 103 pp.\n127-152.\n[Nev75] Neveu, J., 1975. Discrete Parameter Martingales, North-Holland, Amsterdam, The Netherlands.\n[PKP09] Predd, J. B., Kulkarni, S. R., and Poor, H. V., 2009. \u201cA Collaborative Training Algorithm for Distributed\nLearning,\u201d IEEE Transactions on Information Theory, Vol. 55, pp. 1856-1871.\n[Pas79] Passty, G. B., 1979. \u201cErgodic Convergence to a Zero of the Sum of Monotone Operators in Hilbert Space,\u201d\nJ. Math. Anal. Appl., Vol. 72, pp. 383-390.\n[P\ufb0296] P\ufb02ug, G., 1996. Optimization of Stochastic Models. The Interface Between Simulation and Optimization,\nKluwer, Boston.\n[PoT73] Polyak, B. T., and Tsypkin, Y. Z., 1973. \u201cPseudogradient Adaptation and Training Algorithms,\u201d Automation\nand Remote Control, Vol. 12, pp. 83-94.\n[Pol64] Poljak, B. T., 1964. \u201cSome Methods of Speeding up the Convergence of Iteration Methods,\u201d Z. Vy\u02d8Cisl. Mat.\ni Mat. Fiz., Vol. 4, pp. 1-17.\n[Pol87] Polyak, B. T., 1987. Introduction to Optimization, Optimization Software Inc., N. Y.\n[Pol78] Polyak, B. T., 1978. \u201cNonlinear Programming Methods in the Presence of Noise,\u201d Math. Programming, Vol.\n14, pp. 87\u201397.\n[Pol87] Polyak, B. T., 1987. Introduction to Optimization, Optimization Software Inc., N. Y.\n[RNV09] Ram, S. S., Nedi\u00b4c, A., and Veeravalli, V. V., 2009. \u201cIncremental Stochastic Subgradient Algorithms for\nConvex Optimization,\u201d SIAM Journal on Optimization, Vol. 20, pp. 691-717.\n[RNV10] Ram, S. S., Nedi\u00b4c, A., and Veeravalli, V. V., 2010. \u201cDistributed Stochastic Subgradient Projection Algo-\nrithms for Convex Optimization,\u201d Journal of Optimization Theory and Applications, Vol. 147, pp. 516-545.\n[RaN04] Rabbat, M. G., and Nowak, R. D., 2004. \u201cDistributed Optimization in Sensor Networks,\u201d in Proc. Inf.\nProcessing Sensor Networks, Berkeley, CA, pp. 20-27.\n[RaN05] Rabbat M. G., and Nowak R. D., 2005. \u201cQuantized Incremental Algorithms for Distributed Optimization,\u201d\nIEEE Journal on Select Areas in Communications, Vol. 23, pp. 798-808.\n[Roc70] Rockafellar, R. T., 1970. Convex Analysis, Princeton University Press, Princeton, NJ.\n[Roc76] Rockafellar, R. T., 1976. \u201cMonotone Operators and the Proximal Point Algorithm,\u201d SIAM Journal on Control\nand Optimization, Vol. 14, pp. 877-898.\n[SSS07] Shalev-Shwartz, S., Singer, Y., Srebro, N., and Cotter, A., 2007. \u201cPegasos: Primal Estimated Subgradient\n44\nReferences\nSolver for SVM,\u201d in ICML 07, New York, N. Y., pp. 807-814.\n[SoZ98] Solodov, M. V., and Zavriev, S. K., 1998. \u201cError Stability Properties of Generalized Gradient-Type Algo-\nrithms,\u201d J. Opt. Theory and Appl., Vol. 98, pp. 663-680.\n[Sol98] Solodov, M. V., 1998. \u201cIncremental Gradient Algorithms with Stepsizes Bounded Away from Zero,\u201d Comput.\nOpt. Appl., Vol. 11, pp. 28-35.\n[Spi85] Spingarn, J. E., 1985. \u201cApplications of the Method of Partial Inverses to Convex Programming: Decomposi-\ntion,\u201d Math. Programming, Vol. 32, pp. 199-223.\n[TBA86] Tsitsiklis, J. N., Bertsekas, D. P., and Athans, M., 1986. \u201cDistributed Asynchronous Deterministic and\nStochastic Gradient Optimization Algorithms,\u201d IEEE Trans. Automatic Control, Vol. AC-31, pp. 803-812.\n[Tse98] Tseng, P., 1998. \u201cAn Incremental Gradient(-Projection) Method with Momentum Term and Adaptive Stepsize\nRule,\u201d SIAM J. on Optimization, Vol. 8, pp. 506-531.\n[Tse08] Tseng, P., 2008. \u201cOn Accelerated Proximal Gradient Methods for Convex-Concave Optimization,\u201d Report,\nMath. Dept., Univ. of Washington.\n[VoU07] Vonesch, C., and Unser, M., 2007. \u201cFast Iterative Thresholding Algorithm for Wavelet-Regularized Decon-\nvolution,\u201d in Proc. SPIE Optics and Photonics 2007 Conference on Mathematical Methods: Wavelet XII, Vol. 6701,\nSan Diego, CA, pp. 1-5.\n[WNF08] Wright, S. J., Nowak, R. D., and Figueiredo, M. A. T., 2008. \u201cSparse Reconstruction by Separable Approx-\nimation,\u201d in Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP\n2008), pp. 3373-3376.\n[WiH60] Widrow, B., and Ho\ufb00, M. E., 1960. \u201cAdaptive Switching Circuits,\u201d Institute of Radio Engineers, Western\nElectronic Show and Convention, Convention Record, Part 4, pp. 96-104.\n45\n",
        "sentence": " As a consequence, we mention a matching serial algorithm (Algorithm 2) introduced in (Bertsekas, 2011) that has a flavor similar to multi-pass stochastic gradient descent.",
        "context": "combined, as discussed by Nedi\u00b4c, Bertsekas, and Borkar [NBB01]. The analysis here relies on ideas from dis-\ntributed asynchronous gradient methods (both deterministic and stochastic), which were developed in the early\nsupplementary chapter on algorithms.\n[Ber10] Bertsekas, D. P., 2010. \u201cIncremental Proximal Methods for Large Scale Convex Optimization,\u201d Lab. for Info.\nand Decision Systems Report LIDS-P-2847, MIT, Cambridge, MA; to appear in Math. Programming J.\n\u2021 Generally, in the 60s and 70s, algorithmic ideas relating to simple gradient methods with and without determin-\nistic and stochastic errors were popular in the Soviet scienti\ufb01c community, partly due to an emphasis on stochastic"
    },
    {
        "title": "Sparse estimation of a covariance matrix",
        "author": [
            "J. Bien",
            "R. Tibshirani"
        ],
        "venue": null,
        "citeRegEx": "Bien and Tibshirani,? \\Q2010\\E",
        "shortCiteRegEx": "Bien and Tibshirani",
        "year": 2010,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Predicting protein\u2013 protein interactions from primary",
        "author": [
            "J.R. Bock",
            "D.A. Gough"
        ],
        "venue": "structure. Bioinformatics,",
        "citeRegEx": "Bock and Gough,? \\Q2001\\E",
        "shortCiteRegEx": "Bock and Gough",
        "year": 2001,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "A singular value thresholding algorithm for matrix completion",
        "author": [
            "J.F. Cai",
            "E.J. Candes",
            "Z. Shen"
        ],
        "venue": "Arxiv preprint Arxiv:0810.3286,",
        "citeRegEx": "Cai et al\\.,? \\Q2008\\E",
        "shortCiteRegEx": "Cai et al\\.",
        "year": 2008,
        "abstract": "This paper introduces a novel algorithm to approximate the matrix with\nminimum nuclear norm among all matrices obeying a set of convex constraints.\nThis problem may be understood as the convex relaxation of a rank minimization\nproblem, and arises in many important applications as in the task of recovering\na large matrix from a small subset of its entries (the famous Netflix problem).\nOff-the-shelf algorithms such as interior point methods are not directly\namenable to large problems of this kind with over a million unknown entries.\n  This paper develops a simple first-order and easy-to-implement algorithm that\nis extremely efficient at addressing problems in which the optimal solution has\nlow rank. The algorithm is iterative and produces a sequence of matrices (X^k,\nY^k) and at each step, mainly performs a soft-thresholding operation on the\nsingular values of the matrix Y^k. There are two remarkable features making\nthis attractive for low-rank matrix completion problems. The first is that the\nsoft-thresholding operation is applied to a sparse matrix; the second is that\nthe rank of the iterates X^k is empirically nondecreasing. Both these facts\nallow the algorithm to make use of very minimal storage space and keep the\ncomputational cost of each iteration low. We provide numerical examples in\nwhich 1,000 by 1,000 matrices are recovered in less than a minute on a modest\ndesktop computer. We also demonstrate that our approach is amenable to very\nlarge scale problems by recovering matrices of rank about 10 with nearly a\nbillion unknowns from just about 0.4% of their sampled entries. Our methods are\nconnected with linearized Bregman iterations for l1 minimization, and we\ndevelop a framework in which one can understand these algorithms in terms of\nwell-known Lagrange multiplier algorithms.",
        "full_text": "arXiv:0810.3286v1  [math.OC]  18 Oct 2008\nA Singular Value Thresholding Algorithm for Matrix Completion\nJian-Feng Cai\u2020\nEmmanuel J. Cand`es\u266f\nZuowei Shen\u00a7\n\u2020 Temasek Laboratories, National University of Singapore, Singapore 117543\n\u266fApplied and Computational Mathematics, Caltech, Pasadena, CA 91125\n\u00a7 Department of Mathematics, National University of Singapore, Singapore 117543\nSeptember 2008\nAbstract\nThis paper introduces a novel algorithm to approximate the matrix with minimum nuclear\nnorm among all matrices obeying a set of convex constraints. This problem may be understood as\nthe convex relaxation of a rank minimization problem, and arises in many important applications\nas in the task of recovering a large matrix from a small subset of its entries (the famous Net\ufb02ix\nproblem). O\ufb00-the-shelf algorithms such as interior point methods are not directly amenable to\nlarge problems of this kind with over a million unknown entries.\nThis paper develops a simple \ufb01rst-order and easy-to-implement algorithm that is extremely\ne\ufb03cient at addressing problems in which the optimal solution has low rank. The algorithm is\niterative and produces a sequence of matrices {Xk, Y k} and at each step, mainly performs a\nsoft-thresholding operation on the singular values of the matrix Y k. There are two remarkable\nfeatures making this attractive for low-rank matrix completion problems.\nThe \ufb01rst is that\nthe soft-thresholding operation is applied to a sparse matrix; the second is that the rank of\nthe iterates {Xk} is empirically nondecreasing. Both these facts allow the algorithm to make\nuse of very minimal storage space and keep the computational cost of each iteration low. On\nthe theoretical side, we provide a convergence analysis showing that the sequence of iterates\nconverges. On the practical side, we provide numerical examples in which 1, 000\u00d71, 000 matrices\nare recovered in less than a minute on a modest desktop computer. We also demonstrate that\nour approach is amenable to very large scale problems by recovering matrices of rank about 10\nwith nearly a billion unknowns from just about 0.4% of their sampled entries. Our methods are\nconnected with the recent literature on linearized Bregman iterations for \u21131 minimization, and\nwe develop a framework in which one can understand these algorithms in terms of well-known\nLagrange multiplier algorithms.\nKeywords. Nuclear norm minimization, matrix completion, singular value thresholding, La-\ngrange dual function, Uzawa\u2019s algorithm.\n1\nIntroduction\n1.1\nMotivation\nThere is a rapidly growing interest in the recovery of an unknown low-rank or approximately low-\nrank matrix from very limited information. This problem occurs in many areas of engineering and\n1\napplied science such as machine learning [1, 3, 4], control [42] and computer vision, see [48]. As\na motivating example, consider the problem of recovering a data matrix from a sampling of its\nentries. This routinely comes up whenever one collects partially \ufb01lled out surveys, and one would\nlike to infer the many missing entries. In the area of recommender systems, users submit ratings\non a subset of entries in a database, and the vendor provides recommendations based on the user\u2019s\npreferences. Because users only rate a few items, one would like to infer their preference for unrated\nitems; this is the famous Net\ufb02ix problem [2]. Recovering a rectangular matrix from a sampling of\nits entries is known as the matrix completion problem. The issue is of course that this problem is\nextraordinarily ill posed since with fewer samples than entries, we have in\ufb01nitely many completions.\nTherefore, it is apparently impossible to identify which of these candidate solutions is indeed the\n\u201ccorrect\u201d one without some additional information.\nIn many instances, however, the matrix we wish to recover has low rank or approximately low\nrank.\nFor instance, the Net\ufb02ix data matrix of all user-ratings may be approximately low-rank\nbecause it is commonly believed that only a few factors contribute to anyone\u2019s taste or preference.\nIn computer vision, inferring scene geometry and camera motion from a sequence of images is a well-\nstudied problem known as the structure-from-motion problem. This is an ill-conditioned problem\nfor objects may be distant with respect to their size, or especially for \u201cmissing data\u201d which occur\nbecause of occlusion or tracking failures. However, when properly stacked and indexed, these images\nform a matrix which has very low rank (e.g. rank 3 under orthography) [21, 48]. Other examples\nof low-rank matrix \ufb01tting abound; e.g. in control (system identi\ufb01cation), machine learning (multi-\nclass learning) and so on. Having said this, the premise that the unknown has (approximately) low\nrank radically changes the problem, making the search for solutions feasible since the lowest-rank\nsolution now tends to be the right one.\nIn a recent paper [13], Cand`es and Recht showed that matrix completion is not as ill-posed as\npeople thought. Indeed, they proved that most low-rank matrices can be recovered exactly from\nmost sets of sampled entries even though these sets have surprisingly small cardinality, and more\nimportantly, they proved that this can be done by solving a simple convex optimization problem.\nTo state their results, suppose to simplify that the unknown matrix M \u2208Rn\u00d7n is square, and that\none has available m sampled entries {Mij : (i, j) \u2208\u2126} where \u2126is a random subset of cardinality\nm. Then [13] proves that most matrices M of rank r can be perfectly recovered by solving the\noptimization problem\nminimize\n\u2225X\u2225\u2217\nsubject to\nXij = Mij,\n(i, j) \u2208\u2126,\n(1.1)\nprovided that the number of samples obeys\nm \u2265Cn6/5r log n\n(1.2)\nfor some positive numerical constant C.1 In (1.1), the functional \u2225X\u2225\u2217is the nuclear norm of the\nmatrix M, which is the sum of its singular values. The optimization problem (1.1) is convex and\ncan be recast as a semide\ufb01nite program [29,30]. In some sense, this is the tightest convex relaxation\nof the NP-hard rank minimization problem\nminimize\nrank(X)\nsubject to\nXij = Mij,\n(i, j) \u2208\u2126,\n(1.3)\n1Note that an n \u00d7 n matrix of rank r depends upon r(2n \u2212r) degrees of freedom.\n2\nsince the nuclear ball {X : \u2225X\u2225\u2217\u22641} is the convex hull of the set of rank-one matrices with\nspectral norm bounded by one. Another interpretation of Cand`es and Recht\u2019s result is that under\nsuitable conditions, the rank minimization program (1.3) and the convex program (1.1) are formally\nequivalent in the sense that they have exactly the same unique solution.\n1.2\nAlgorithm outline\nBecause minimizing the nuclear norm both provably recovers the lowest-rank matrix subject to\nconstraints (see [45] for related results) and gives generally good empirical results in a variety of\nsituations, it is understandably of great interest to develop numerical methods for solving (1.1). In\n[13], this optimization problem was solved using one of the most advanced semide\ufb01nite programming\nsolvers, namely, SDPT3 [47].\nThis solver and others like SeDuMi are based on interior-point\nmethods, and are problematic when the size of the matrix is large because they need to solve huge\nsystems of linear equations to compute the Newton direction. In fact, SDPT3 can only handle\nn \u00d7 n matrices with n \u2264100. Presumably, one could resort to iterative solvers such as the method\nof conjugate gradients to solve for the Newton step but this is problematic as well since it is well\nknown that the condition number of the Newton system increases rapidly as one gets closer to the\nsolution. In addition, none of these general purpose solvers use the fact that the solution may have\nlow rank. We refer the reader to [40] for some recent progress on interior-point methods concerning\nsome special nuclear norm-minimization problems.\nThis paper develops the singular value thresholding algorithm for approximately solving the\nnuclear norm minimization problem (1.1) and by extension, problems of the form\nminimize\n\u2225X\u2225\u2217\nsubject to\nA(X) = b,\n(1.4)\nwhere A is a linear operator acting on the space of n1 \u00d7 n2 matrices and b \u2208Rm. This algorithm is\na simple \ufb01rst-order method, and is especially well suited for problems of very large sizes in which\nthe solution has low rank. We sketch this algorithm in the special matrix completion setting and\nlet P\u2126be the orthogonal projector onto the span of matrices vanishing outside of \u2126so that the\n(i, j)th component of P\u2126(X) is equal to Xij if (i, j) \u2208\u2126and zero otherwise. Our problem may be\nexpressed as\nminimize\n\u2225X\u2225\u2217\nsubject to\nP\u2126(X) = P\u2126(M),\n(1.5)\nwith optimization variable X \u2208Rn1\u00d7n2. Fix \u03c4 > 0 and a sequence {\u03b4k}k\u22651 of scalar step sizes.\nThen starting with Y 0 = 0 \u2208Rn1\u00d7n2, the algorithm inductively de\ufb01nes\n(\nXk = shrink(Y k\u22121, \u03c4),\nY k = Y k\u22121 + \u03b4kP\u2126(M \u2212Xk)\n(1.6)\nuntil a stopping criterion is reached. In (1.6), shrink(Y , \u03c4) is a nonlinear function which applies a\nsoft-thresholding rule at level \u03c4 to the singular values of the input matrix, see Section 2 for details.\nThe key property here is that for large values of \u03c4, the sequence {Xk} converges to a solution which\nvery nearly minimizes (1.5). Hence, at each step, one only needs to compute at most one singular\nvalue decomposition and perform a few elementary matrix additions. Two important remarks are\nin order:\n3\n1. Sparsity. For each k \u22650, Y k vanishes outside of \u2126and is, therefore, sparse, a fact which can\nbe used to evaluate the shrink function rapidly.\n2. Low-rank property. The matrices Xk turn out to have low rank, and hence the algorithm has\nminimum storage requirement since we only need to keep principal factors in memory.\nOur numerical experiments demonstrate that the proposed algorithm can solve problems, in\nMatlab, involving matrices of size 30, 000\u00d730, 000 having close to a billion unknowns in 17 minutes\non a standard desktop computer with a 1.86 GHz CPU (dual core with Matlab\u2019s multithreading\noption enabled) and 3 GB of memory. As a consequence, the singular value thresholding algorithm\nmay become a rather powerful computational tool for large scale matrix completion.\n1.3\nGeneral formulation\nThe singular value thresholding algorithm can be adapted to deal with other types of convex\nconstraints. For instance, it may address problems of the form\nminimize\n\u2225X\u2225\u2217\nsubject to\nfi(X) \u22640,\ni = 1, . . . , m,\n(1.7)\nwhere each fi is a Lipschitz convex function (note that one can handle linear equality constraints by\nconsidering pairs of a\ufb03ne functionals). In the simpler case where the fi\u2019s are a\ufb03ne functionals, the\ngeneral algorithm goes through a sequence of iterations which greatly resemble (1.6). This is useful\nbecause this enables the development of numerical algorithms which are e\ufb00ective for recovering\nmatrices from a small subset of sampled entries possibly contaminated with noise.\n1.4\nContents and notations\nThe rest of the paper is organized as follows. In Section 2, we derive the singular value threshold-\ning (SVT) algorithm for the matrix completion problem, and recasts it in terms of a well-known\nLagrange multiplier algorithm. In Section 3, we extend the SVT algorithm and formulate a gen-\neral iteration which is applicable to general convex constraints.\nIn Section 4, we establish the\nconvergence results for the iterations given in Sections 2 and 3. We demonstrate the performance\nand e\ufb00ectiveness of the algorithm through numerical examples in Section 5, and review additional\nimplementation details. Finally, we conclude the paper with a short discussion in Section 6.\nBefore continuing, we provide here a brief summary of the notations used throughout the\npaper. Matrices are bold capital, vectors are bold lowercase and scalars or entries are not bold.\nFor instance, X is a matrix and Xij its (i, j)th entry.\nLikewise, x is a vector and xi its ith\ncomponent. The nuclear norm of a matrix is denoted by \u2225X\u2225\u2217, the Frobenius norm by \u2225X\u2225F\nand the spectral norm by \u2225X\u22252; note that these are respectively the 1-norm, the 2-norm and the\nsup-norm of the vector of singular values.\nThe adjoint of a matrix X is X\u2217and similarly for\nvectors. The notation diag(x), where x is a vector, stands for the diagonal matrix with {xi} as\ndiagonal elements. We denote by \u27e8X, Y \u27e9= trace(X\u2217Y ) the standard inner product between two\nmatrices (\u2225X\u22252\nF = \u27e8X, X\u27e9). The Cauchy-Schwarz inequality gives \u27e8X, Y \u27e9\u2264\u2225X\u2225F \u2225Y \u2225F and it is\nwell known that we also have \u27e8X, Y \u27e9\u2264\u2225X\u2225\u2217\u2225Y \u22252 (the spectral and nuclear norms are dual from\none another), see e.g. [13,45].\n4\n2\nThe Singular Value Thresholding Algorithm\nThis section introduces the singular value thresholding algorithm and discusses some of its ba-\nsic properties. We begin with the de\ufb01nition of a key building block, namely, the singular value\nthresholding operator.\n2.1\nThe singular value shrinkage operator\nConsider the singular value decomposition (SVD) of a matrix X \u2208Rn1\u00d7n2 of rank r\nX = U\u03a3V \u2217,\n\u03a3 = diag({\u03c3i}1\u2264i\u2264r),\n(2.1)\nwhere U and V are respectively n1 \u00d7 r and n2 \u00d7 r matrices with orthonormal columns, and the\nsingular values \u03c3i are positive (unless speci\ufb01ed otherwise, we will always assume that the SVD of\na matrix is given in the reduced form above). For each \u03c4 \u22650, we introduce the soft-thresholding\noperator D\u03c4 de\ufb01ned as follows:\nD\u03c4(X) := UD\u03c4(\u03a3)V \u2217,\nD\u03c4(\u03a3) = diag({\u03c3i \u2212\u03c4)+}),\n(2.2)\nwhere t+ is the positive part of t, namely, t+ = max(0, t). In words, this operator simply applies a\nsoft-thresholding rule to the singular values of X, e\ufb00ectively shrinking these towards zero. This is\nthe reason why we will also refer to this transformation as the singular value shrinkage operator.\nEven though the SVD may not be unique, it is easy to see that the singular value shrinkage operator\nis well de\ufb01ned and we do not elaborate further on this issue. In some sense, this shrinkage operator\nis a straightforward extension of the soft-thresholding rule for scalars and vectors. In particular,\nnote that if many of the singular values of X are below the threshold \u03c4, the rank of D\u03c4(X) may\nbe considerably lower than that of X, just like the soft-thresholding rule applied to vectors leads\nto sparser outputs whenever some entries of the input are below threshold.\nThe singular value thresholding operator is the proximity operator associated with the nuclear\nnorm. Details about the proximity operator can be found in e.g. [35].\nTheorem 2.1 For each \u03c4 \u22650 and Y \u2208Rn1\u00d7n2, the singular value shrinkage operator (2.2) obeys\nD\u03c4(Y ) = arg min\nX\n\u001a1\n2\u2225X \u2212Y \u22252\nF + \u03c4\u2225X\u2225\u2217\n\u001b\n.\n(2.3)\nProof. Since the function h0(X) := \u03c4\u2225X\u2225\u2217+ 1\n2\u2225X \u2212Y \u22252\nF is strictly convex, it is easy to see that\nthere exists a unique minimizer, and we thus need to prove that it is equal to D\u03c4(Y ). To do this,\nrecall the de\ufb01nition of a subgradient of a convex function f : Rn1\u00d7n2 \u2192R. We say that Z is a\nsubgradient of f at X0, denoted Z \u2208\u2202f(X0), if\nf(X) \u2265f(X0) + \u27e8Z, X \u2212X0\u27e9\n(2.4)\nfor all X. Now \u02c6\nX minimizes h0 if and only if 0 is a subgradient of the functional h0 at the point\n\u02c6\nX, i.e.\n0 \u2208\u02c6\nX \u2212Y + \u03c4\u2202\u2225\u02c6\nX\u2225\u2217,\n(2.5)\nwhere \u2202\u2225\u02c6\nX\u2225\u2217is the set of subgradients of the nuclear norm. Let X \u2208Rn1\u00d7n2 be an arbitrary\nmatrix and U\u03a3V \u2217be its SVD. It is known [13,37,49] that\n\u2202\u2225X\u2225\u2217=\n\b\nUV \u2217+ W : W \u2208Rn1\u00d7n2,\nU \u2217W = 0,\nW V = 0,\n\u2225W \u22252 \u22641\n\t\n.\n(2.6)\n5\nSet \u02c6\nX := D\u03c4(Y ) for short. In order to show that \u02c6\nX obeys (2.5), decompose the SVD of Y as\nY = U0\u03a30V \u2217\n0 + U1\u03a31V \u2217\n1 ,\nwhere U0, V0 (resp. U1, V1) are the singular vectors associated with singular values greater than \u03c4\n(resp. smaller than or equal to \u03c4). With these notations, we have\n\u02c6\nX = U0(\u03a30 \u2212\u03c4I)V \u2217\n0\nand, therefore,\nY \u2212\u02c6\nX = \u03c4(U0V \u2217\n0 + W ),\nW = \u03c4 \u22121U1\u03a31V \u2217\n1 .\nBy de\ufb01nition, U \u2217\n0 W = 0, W V0 = 0 and since the diagonal elements of \u03a31 have magnitudes\nbounded by \u03c4, we also have \u2225W \u22252 \u22641. Hence Y \u2212\u02c6\nX \u2208\u03c4\u2202\u2225\u02c6\nX\u2225\u2217, which concludes the proof.\n2.2\nShrinkage iterations\nWe are now in the position to introduce the singular value thresholding algorithm. Fix \u03c4 > 0 and\na sequence {\u03b4k} of positive step sizes. Starting with Y0, inductively de\ufb01ne for k = 1, 2, . . .,\n(\nXk = D\u03c4(Y k\u22121),\nY k = Y k\u22121 + \u03b4kP\u2126(M \u2212Xk)\n(2.7)\nuntil a stopping criterion is reached (we postpone the discussion this stopping criterion and of the\nchoice of step sizes). This shrinkage iteration is very simple to implement. At each step, we only\nneed to compute an SVD and perform elementary matrix operations. With the help of a standard\nnumerical linear algebra package, the whole algorithm can be coded in just a few lines.\nBefore addressing further computational issues, we would like to make explicit the relationship\nbetween this iteration and the original problem (1.1). In Section 4, we will show that the sequence\n{Xk} converges to the unique solution of an optimization problem closely related to (1.1), namely,\nminimize\n\u03c4\u2225X\u2225\u2217+ 1\n2\u2225X\u22252\nF\nsubject to\nP\u2126(X) = P\u2126(M).\n(2.8)\nFurthermore, it is intuitive that the solution to this modi\ufb01ed problem converges to that of (1.5) as\n\u03c4 \u2192\u221eas shown in Section 3. Thus by selecting a large value of the parameter \u03c4, the sequence of\niterates converges to a matrix which nearly minimizes (1.1).\nAs mentioned earlier, there are two crucial properties which make this algorithm ideally suited\nfor matrix completion.\n\u2022 Low-rank property. A remarkable empirical fact is that the matrices in the sequence {Xk}\nhave low rank (provided, of course, that the solution to (2.8) has low rank). We use the word\n\u201cempirical\u201d because all of our numerical experiments have produced low-rank sequences but\nwe cannot rigorously prove that this is true in general. The reason for this phenomenon is,\nhowever, simple: because we are interested in large values of \u03c4 (as to better approximate the\nsolution to (1.1)), the thresholding step happens to \u2018kill\u2019 most of the small singular values\nand produces a low-rank output. In fact, our numerical results show that the rank of Xk is\nnondecreasing with k, and the maximum rank is reached in the last steps of the algorithm,\nsee Section 5.\n6\nThus, when the rank of the solution is substantially smaller than either dimension of the\nmatrix, the storage requirement is low since we could store each Xk in its SVD form (note\nthat we only need to keep the current iterate and may discard earlier values).\n\u2022 Sparsity. Another important property of the SVT algorithm is that the iteration matrix Y k\nis sparse. Since Y 0 = 0, we have by induction that Y k vanishes outside of \u2126. The fewer\nentries available, the sparser Y k. Because the sparsity pattern \u2126is \ufb01xed throughout, one can\nthen apply sparse matrix techniques to save storage. Also, if |\u2126| = m, the computational cost\nof updating Y k is of order m. Moreover, we can call subroutines supporting sparse matrix\ncomputations, which can further reduce computational costs.\nOne such subroutine is the SVD. However, note that we do not need to compute the entire\nSVD of Y k to apply the singular value thresholding operator. Only the part corresponding\nto singular values greater than \u03c4 is needed. Hence, a good strategy is to apply the iterative\nLanczos algorithm to compute the \ufb01rst few singular values and singular vectors. Because\nY k is sparse, Y k can be applied to arbitrary vectors rapidly, and this procedure o\ufb00ers a\nconsiderable speedup over naive methods.\n2.3\nRelation with other works\nOur algorithm is inspired by recent work in the area of \u21131 minimization, and especially by the work\non linearized Bregman iterations for compressed sensing, see [9\u201311,23,44,51] for linearized Bregman\niterations and [14\u201317,26] for some information about the \ufb01eld of compressed sensing. In this line\nof work, linearized Bregman iterations are used to \ufb01nd the solution to an underdetermined system\nof linear equations with minimum \u21131 norm. In fact, Theorem 2.1 asserts that the singular value\nthresholding algorithm can be formulated as a linearized Bregman iteration. Bregman iterations\nwere \ufb01rst introduced in [43] as a convenient tool for solving computational problems in the imaging\nsciences, and a later paper [51] showed that they were useful for solving \u21131-norm minimization\nproblems in the area of compressed sensing. Linearized Bregman iterations were proposed in [23]\nto improve performance of plain Bregman iterations, see also [51].\nAdditional details together\nwith a technique for improving the speed of convergence called kicking are described in [44]. On\nthe practical side, the paper [11] applied Bregman iterations to solve a deblurring problem while\non the theoretical side, the references [9, 10] gave a rigorous analysis of the convergence of such\niterations. New developments keep on coming out at a rapid pace and recently, [32] introduced a\nnew iteration, the split Bregman iteration, to extend Bregman-type iterations (such as linearized\nBregman iterations) to problems involving the minimization of \u21131-like functionals such as total-\nvariation norms, Besov norms, and so forth.\nWhen applied to \u21131-minimization problems, linearized Bregman iterations are sequences of\nsoft-thresholding rules operating on vectors. Iterative soft-thresholding algorithms in connection\nwith \u21131 or total-variation minimization have quite a bit of history in signal and image processing\nand we would like to mention the works [12, 39] for total-variation minimization, [24, 25, 31] for\n\u21131 minimization, and [5, 7, 8, 19, 20, 27, 28, 46] for some recent applications in the area of image\ninpainting and image restoration. Just as iterative soft-thresholding methods are designed to \ufb01nd\nsparse solutions, our iterative singular value thresholding scheme is designed to \ufb01nd a sparse vector\nof singular values. In classical problems arising in the areas of compressed sensing, and signal or\nimage processing, the sparsity is expressed in a known transformed domain and soft-thresholding is\napplied to transformed coe\ufb03cients. In contrast, the shrinkage operator D\u03c4 is adaptive. The SVT not\n7\nonly discovers a sparse singular vector but also the bases in which we have a sparse representation.\nIn this sense, the SVT algorithm is an extension of earlier iterative soft-thresholding schemes.\nFinally, we would like to contrast the SVT iteration (2.7) with the popular iterative soft-\nthresholding algorithm used in many papers in imaging processing and perhaps best known under\nthe name of Proximal Forward-Backward Splitting method (PFBS), see [8,22,24,31,33] for example.\nThe constrained minimization problem (1.5) may be relaxed into\nminimize\n\u03bb\u2225X\u2225\u2217+ 1\n2\u2225P\u2126(X) \u2212P\u2126(M)\u22252\nF\n(2.9)\nfor some \u03bb > 0. Theorem 2.1 asserts that D\u03bb is the proximity operator of \u03bb\u2225X\u2225\u2217and Proposition\n3.1(iii) in [22] gives that the solution to this unconstrained problem is characterized by the \ufb01xed\npoint equation X = D\u03bb\u03b4(X + \u03b4P\u2126(M \u2212X)) for each \u03b4 > 0. One can then apply a simpli\ufb01ed\nversion of the PFBS method (see (3.6) in [22]) to obtain iterations of the form\nXk = D\u03bb\u03b4k\u22121(Xk\u22121 + \u03b4k\u22121P\u2126(M \u2212Xk\u22121)).\nIntroducing an intermediate matrix Y k, this algorithm may be expressed as\n(\nXk = D\u03bb\u03b4k\u22121(Y k\u22121),\nY k = Xk + \u03b4kP\u2126(M \u2212Xk).\n(2.10)\nThe di\ufb00erence with (2.7) may seem subtle at \ufb01rst\u2014replacing Xk in (2.10) with Y k\u22121 and setting\n\u03b4k = \u03b4 gives (2.7) with \u03c4 = \u03bb\u03b4\u2014but has enormous consequences as this gives entirely di\ufb00erent\nalgorithms. First, they have di\ufb00erent limits: while (2.7) converges to the solution of the constrained\nminimization (2.8), (2.10) converges to the solution of (2.9) provided that the sequence of step sizes\nis appropriately selected. Second, selecting a large \u03bb (or a large value of \u03c4 = \u03bb\u03b4) in (2.10) gives\na low-rank sequence of iterates and a limit with small nuclear norm. The limit, however, does\nnot \ufb01t the data and this is why one has to choose a small or moderate value of \u03bb (or of \u03c4 = \u03bb\u03b4).\nHowever, when \u03bb is not su\ufb03ciently large, the Xk may not have low rank even though the solution\nhas low rank (and one may need to compute many singular vectors), and Y k is not su\ufb03ciently\nsparse to make the algorithm computationally attractive. Moreover, the limit does not necessary\nhave a small nuclear norm. These are reasons why (2.10) is not suitable for matrix completion.\n2.4\nInterpretation as a Lagrange multiplier method\nIn this section, we recast the SVT algorithm as a type of Lagrange multiplier algorithm known as\nUzawa\u2019s algorithm. An important consequence is that this will allow us to extend the SVT algorithm\nto other problems involving the minimization of the nuclear norm under convex constraints, see\nSection 3. Further, another contribution of this paper is that this framework actually recasts linear\nBregman iterations as a very special form of Uzawa\u2019s algorithm, hence providing fresh and clear\ninsights about these iterations.\nIn what follows, we set f\u03c4(X) = \u03c4\u2225X\u2225\u2217+ 1\n2\u2225X\u22252\nF for some \ufb01xed \u03c4 > 0 and recall that we wish\nto solve (2.8)\nminimize\nf\u03c4(X)\nsubject to\nP\u2126(X) = P\u2126(M).\nThe Lagrangian for this problem is given by\nL(X, Y ) = f\u03c4(X) + \u27e8Y , P\u2126(M \u2212X)\u27e9,\n8\nwhere Y \u2208Rn1\u00d7n2. Strong duality holds and X\u22c6and Y \u22c6are primal-dual optimal if (X\u22c6, Y \u22c6) is a\nsaddlepoint of the Lagrangian L(X, Y ), i.e. a pair obeying\nsup\nY\ninf\nX L(X, Y ) = L(X\u22c6, Y \u22c6) = inf\nX sup\nY\nL(X, Y ).\n(2.11)\n(The function g0(Y ) = infX L(X, Y ) is called the dual function.) Uzawa\u2019s algorithm approaches\nthe problem of \ufb01nding a saddlepoint with an iterative procedure. From Y0 = 0, say, inductively\nde\ufb01ne\n(\nL(Xk, Y k\u22121) = minX L(X, Y k\u22121)\nY k = Y k\u22121 + \u03b4kP\u2126(M \u2212Xk),\n(2.12)\nwhere {\u03b4k}k\u22651 is a sequence of positive step sizes. Uzawa\u2019s algorithm is, in fact, a subgradient\nmethod applied to the dual problem, where each step moves the current iterate in the direction of\nthe gradient or of a subgradient. Indeed, observe that\n\u2202Y g0(Y ) = \u2202Y L( \u02dc\nX, Y ) = P\u2126(M \u2212\u02dc\nX),\n(2.13)\nwhere \u02dc\nX is the minimizer of the Lagrangian for that value of Y so that a gradient descent update\nfor Y is of the form\nY k = Y k\u22121 + \u03b4k\u2202Y g0(Y k\u22121) = Y k\u22121 + \u03b4kP\u2126(M \u2212Xk).\nIt remains to compute the minimizer of the Lagrangian (2.12), and note that\narg min f\u03c4(X) + \u27e8Y , P\u2126(M \u2212X)\u27e9= arg min \u03c4\u2225X\u2225\u2217+ 1\n2\u2225X \u2212P\u2126Y \u22252\nF .\n(2.14)\nHowever, we know that the minimizer is given by D\u03c4(P\u2126(Y )) and since Y k = P\u2126(Y k) for all k \u22650,\nUzawa\u2019s algorithm takes the form\n(\nXk = D\u03c4(Y k\u22121)\nY k = Y k\u22121 + \u03b4kP\u2126(M \u2212Xk),\nwhich is exactly the update (2.7). This point of view brings to bear many di\ufb00erent mathemat-\nical tools for proving the convergence of the singular value thresholding iterations. For an early\nuse of Uzawa\u2019s algorithm minimizing an \u21131-like functional, the total-variation norm, under linear\ninequality constraints, see [12].\n3\nGeneral Formulation\nThis section presents a general formulation of the SVT algorithm for approximately minimizing the\nnuclear norm of a matrix under convex constraints.\n3.1\nLinear equality constraints\nSet the objective functional f\u03c4(X) = \u03c4\u2225X\u2225\u2217+ 1\n2\u2225X\u22252\nF for some \ufb01xed \u03c4 > 0, and consider the\nfollowing optimization problem:\nminimize\nf\u03c4(X)\nsubject to\nA(X) = b,\n(3.1)\n9\nwhere A is a linear transformation mapping n1 \u00d7n2 matrices into Rm (A\u2217is the adjoint of A). This\nmore general formulation is considered in [13] and [45] as an extension of the matrix completion\nproblem. Then the Lagrangian for this problem is of the form\nL(X, y) = f\u03c4(X) + \u27e8y, b \u2212A(X)\u27e9,\n(3.2)\nwhere X \u2208Rn1\u00d7n2 and y \u2208Rm, and starting with y0 = 0, Uzawa\u2019s iteration is given by\n(\nXk = D\u03c4(A\u2217(yk\u22121)),\nyk = yk\u22121 + \u03b4k(b \u2212A(Xk)).\n(3.3)\nThe iteration (3.3) is of course the same as (2.7) in the case where A is a sampling operator\nextracting m entries with indices in \u2126out of an n1 \u00d7 n2 matrix. To verify this claim, observe\nthat in this situation, A\u2217A = P\u2126, and let M be any matrix obeying A(M) = b. Then de\ufb01ning\nY k = A\u2217(yk) and substituting this expression in (3.3) gives (2.7).\n3.2\nGeneral convex constraints\nOne can also adapt the algorithm to handle general convex constraints.\nSuppose we wish to\nminimize f\u03c4(X) de\ufb01ned as before over a convex set X \u2208C. To simplify, we will assume that this\nconvex set is given by\nC = {X : fi(X) \u22640, \u2200i = 1, . . . , m},\nwhere the fi\u2019s are convex functionals (note that one can handle linear equality constraints by\nconsidering pairs of a\ufb03ne functionals). The problem of interest is then of the form\nminimize\nf\u03c4(X)\nsubject to\nfi(X) \u22640,\ni = 1, . . . , m.\n(3.4)\nJust as before, it is intuitive that as \u03c4 \u2192\u221e, the solution to this problem converges to a minimizer\nof the nuclear norm under the same constraints (1.7) as shown in Theorem 3.1 at the end of this\nsection.\nPut F(X) := (f1(X), . . . , fm(X)) for short. Then the Lagrangian for (3.4) is equal to\nL(X, y) = f\u03c4(X) + \u27e8y, F(X)\u27e9,\nwhere X \u2208Rn1\u00d7n2 and y \u2208Rm is now a vector with nonnegative components denoted, as usual,\nby y \u22650. One can apply Uzawa\u2019s method just as before with the only modi\ufb01cation that we will\nuse a subgradient method with projection to maximize the dual function since we need to make\nsure that the successive updates yk belong to the nonnegative orthant. This gives\n(\nXk = arg min {f\u03c4(X) + \u27e8yk\u22121, F(X)\u27e9},\nyk = [yk\u22121 + \u03b4kF(Xk)]+.\n(3.5)\nAbove, x+ is of course the vector with entries equal to max(xi, 0). When F is an a\ufb03ne mapping\nof the form b \u2212A(X) so that one solves\nminimize\nf\u03c4(X)\nsubject to\nA(X) \u2265b,\n10\nthis simpli\ufb01es to\n(\nXk = D\u03c4(A\u2217(yk\u22121)),\nyk = [yk\u22121 + \u03b4k(b \u2212A(Xk))]+,\n(3.6)\nand thus the extension to linear inequality constraints is straightforward.\n3.3\nExample\nAn interesting example concerns the extension of the Dantzig selector [18] to matrix problems.\nSuppose we have available linear measurements about a matrix M of interest\nb = A(M) + z,\n(3.7)\nwhere z \u2208Rm is a noise vector. Then under these circumstances, one might want to \ufb01nd the\nmatrix which minimizes the nuclear norm among all matrices which are consistent with the data b.\nInspired by the work on the Dantzig selector which was originally developed for estimating sparse\nparameter vectors from noisy data, one could approach this problem by solving\nminimize\n\u2225X\u2225\u2217\nsubject to\n|vec(A\u2217(r))| \u2264vec(E),\nr := b \u2212A(X),\n(3.8)\nwhere E is an array of tolerances, which is adjusted to \ufb01t the noise statistics [18]. Above, vec(A) \u2264\nvec(B), for any two matrices A and B, means componentwise inequalities; that is, Aij \u2264Bij for all\nindices i, j. We use this notation as not to confuse the reader with the positive semide\ufb01nite ordering.\nIn the case of the matrix completion problem where A extracts sampled entries indexed by \u2126, one\ncan always see the data vector as the sampled entries of some matrix B obeying P\u2126(B) = A\u2217(b);\nthe constraint is then natural for it may be expressed as\n|Bij \u2212Xij| \u2264Eij,\n(i, j) \u2208\u2126,\nIf z is white noise with standard deviation \u03c3, one may want to use a multiple of \u03c3 for Eij. In\nwords, we are looking for a matrix with minimum nuclear norm under the constraint that all of its\nsampled entries do not deviate too much from what has been observed.\nLet Y+ \u2208Rn1\u00d7n2 (resp. Y\u2212\u2208Rn1\u00d7n2) be the Lagrange multiplier associated with the compo-\nnentwise linear inequality constraints vec(A\u2217(r)) \u2264vec(E) (resp. \u2212vec(A\u2217(r)) \u2264vec(E)). Then\nstarting with Y 0\n\u00b1 = 0, the SVT iteration for this problem is of the form\n(\nXk = D\u03c4(A\u2217A(Y k\u22121\n+\n\u2212Y k\u22121\n\u2212\n)),\nY k\n\u00b1 = [Y k\u22121\n\u00b1\n+ \u03b4k(\u00b1A\u2217(rk) \u2212E)]+,\nrk = bk \u2212A(Xk),\n(3.9)\nwhere again [\u00b7]+ is applied componentwise.\nWe conclude by noting that in the matrix completion problem where A\u2217A = P\u2126and one\nobserves P\u2126(B), one can check that this iteration simpli\ufb01es to\n(\nXk = D\u03c4(Y k\u22121\n+\n\u2212Y k\u22121\n\u2212\n),\nY k\n\u00b1 = [Y k\u22121\n\u00b1\n+ \u03b4kP\u2126(\u00b1(B \u2212Xk) \u2212E)]+.\n(3.10)\nAgain, this is easy to implement and whenever the solution has low rank, the iterates Xk have low\nrank as well.\n11\n3.4\nWhen the proximal problem gets close\nWe now show that minimizing the proximal objective f\u03c4(X) = \u03c4\u2225X\u2225\u2217+ 1\n2\u2225X\u22252\nF is the same as\nminimizing the nuclear norm in the limit of large \u03c4\u2019s. The theorem below is general and covers the\nspecial case of linear equality constraints as in (2.8).\nTheorem 3.1 Let X\u22c6\n\u03c4 be the solution to (3.4) and X\u221ebe the minimum Frobenius-norm solution\nto (1.7) de\ufb01ned as\nX\u221e:= arg min\nX {\u2225X\u22252\nF\n: X is a solution of (1.7)}.\n(3.11)\nAssume that the fi(X)\u2019s, 1 \u2264i \u2264m, are convex and lower semi-continuous. Then\nlim\n\u03c4\u2192\u221e\u2225X\u22c6\n\u03c4 \u2212X\u221e\u2225F = 0.\n(3.12)\nProof. It follows from the de\ufb01nition of X\u22c6\n\u03c4 and X\u221ethat\n\u2225X\u22c6\n\u03c4 \u2225\u2217+ 1\n2\u03c4 \u2225X\u22c6\n\u03c4 \u22252\nF \u2264\u2225X\u221e\u2225\u2217+ 1\n2\u03c4 \u2225X\u221e\u22252\nF ,\nand\n\u2225X\u221e\u2225\u2217\u2264\u2225X\u22c6\n\u03c4 \u2225\u2217.\n(3.13)\nSumming these two inequalities gives\n\u2225X\u22c6\n\u03c4 \u22252\nF \u2264\u2225X\u221e\u22252\nF ,\n(3.14)\nwhich implies that \u2225X\u22c6\n\u03c4 \u22252\nF is bounded uniformly in \u03c4. Thus, we would prove the theorem if we\ncould establish that any convergent subsequence {X\u22c6\n\u03c4k}k\u22651 must converge to X\u221e.\nConsider an arbitrary converging subsequence {X\u22c6\n\u03c4k} and set Xc := limk\u2192\u221eX\u22c6\n\u03c4k. Since for\neach 1 \u2264i \u2264m, fi(X\u22c6\n\u03c4k) \u22640 and fi is lower semi-continuous, Xc obeys\nfi(Xc) \u22640,\ni = 1, . . . , m.\n(3.15)\nFurthermore, since \u2225X\u22c6\n\u03c4 \u22252\nF is bounded, (3.13) yields\nlim sup\n\u03c4\u2192\u221e\n\u2225X\u22c6\n\u03c4 \u2225\u2217\u2264\u2225X\u221e\u2225\u2217,\n\u2225X\u221e\u2225\u2217\u2264lim inf\n\u03c4\u2192\u221e\u2225X\u22c6\n\u03c4 \u2225\u2217.\nAn immediate consequence is lim\u03c4\u2192\u221e\u2225X\u22c6\n\u03c4 \u2225\u2217= \u2225X\u221e\u2225\u2217and, therefore, \u2225Xc\u2225\u2217= \u2225X\u221e\u2225\u2217. This\nshows that Xc is a solution to (1.1). Now it follows from the de\ufb01nition of X\u221ethat \u2225Xc\u2225F \u2265\n\u2225X\u221e\u2225F , while we also have \u2225Xc\u2225F \u2264\u2225X\u221e\u2225F because of (3.14). We conclude that \u2225Xc\u2225F =\n\u2225X\u221e\u2225F and thus Xc = X\u221esince X\u221eis unique.\n4\nConvergence Analysis\nThis section establishes the convergence of the SVT iterations. We begin with the simpler proof\nof the convergence of (2.7) in the special case of the matrix completion problem, and then present\nthe argument for the more general constraints (3.5). We hope that this progression will make the\nsecond and more general proof more transparent.\n12\n4.1\nConvergence for matrix completion\nWe begin by recording a lemma which establishes the strong convexity of the objective f\u03c4.\nLemma 4.1 Let Z \u2208\u2202f\u03c4(X) and Z\u2032 \u2208\u2202f\u03c4(X\u2032). Then\n\u27e8Z \u2212Z\u2032, X \u2212X\u2032\u27e9\u2265\u2225X \u2212X\u2032\u22252\nF .\n(4.1)\nProof. An element Z of \u2202f\u03c4(X) is of the form Z = \u03c4Z0 + X, where Z0 \u2208\u2202\u2225X\u2225\u2217, and similarly\nfor Z\u2032. This gives\n\u27e8Z \u2212Z\u2032, X \u2212X\u2032\u27e9= \u03c4 \u27e8Z0 \u2212Z\u2032\n0, X \u2212X\u2032\u27e9+ \u2225X \u2212X\u2032\u22252\nF\nand it thus su\ufb03ces to show that the \ufb01rst term of the right-hand side is nonnegative. From (2.6),\nwe have that any subgradient of the nuclear norm at X obeys \u2225Z0\u22252 \u22641 and \u27e8Z0, X\u27e9= \u2225X\u2225\u2217. In\nparticular, this gives\n|\u27e8Z0, X\u2032\u27e9| \u2264\u2225Z0\u22252\u2225X\u2032\u2225\u2217\u2264\u2225X\u2032\u2225\u2217,\n|\u27e8Z\u2032\n0, X\u27e9| \u2264\u2225Z\u2032\n0\u22252\u2225X\u2225\u2217\u2264\u2225X\u2225\u2217.\nWhence,\n\u27e8Z0 \u2212Z\u2032\n0, X \u2212X\u2032\u27e9= \u27e8Z0, X\u27e9+ \u27e8Z\u2032\n0, X\u2032\u27e9\u2212\u27e8Z0, X\u2032\u27e9\u2212\u27e8Z\u2032\n0, X\u27e9\n= \u2225X\u2225\u2217+ \u2225X\u2032\u2225\u2217\u2212\u27e8Z0, X\u2032\u27e9\u2212\u27e8Z\u2032\n0, X\u27e9\u22650,\nwhich proves the lemma.\nThis lemma is key in showing that the SVT algorithm (2.7) converges.\nTheorem 4.2 Suppose that the sequence of step sizes obeys 0 < inf \u03b4k \u2264sup \u03b4k < 2. Then the\nsequence {Xk} obtained via (2.7) converges to the unique solution of (2.8).\nProof. Let (X\u22c6, Y \u22c6) be primal-dual optimal for the problem (2.8). The optimality conditions give\n0 = Zk \u2212P\u2126(Y k\u22121)\n0 = Z\u22c6\u2212P\u2126(Y \u22c6),\nfor some Zk \u2208\u2202f\u03c4(Xk) and some Z\u22c6\u2208\u2202f\u03c4(X\u22c6). We then deduce that\n(Zk \u2212Z\u22c6) \u2212P\u2126(Y k\u22121 \u2212Y \u22c6) = 0\nand, therefore, it follows from Lemma 4.1 that\n\u27e8Xk \u2212X\u22c6, P\u2126(Y k\u22121 \u2212Y \u22c6)\u27e9= \u27e8Zk \u2212Z\u22c6, Xk \u2212X\u22c6\u27e9\u2265\u2225Xk \u2212X\u22c6\u22252\nF .\n(4.2)\nWe continue and observe that because P\u2126X\u22c6= P\u2126M,\n\u2225P\u2126(Y k \u2212Y \u22c6)\u2225F = \u2225P\u2126(Y k\u22121 \u2212Y \u22c6) + \u03b4kP\u2126(X\u22c6\u2212Xk)\u2225F .\nTherefore, setting rk = \u2225P\u2126(Y k \u2212Y \u22c6)\u2225F ,\nr2\nk = r2\nk\u22121 \u22122\u03b4k\u27e8P\u2126(Y k\u22121 \u2212Y \u22c6), Xk \u2212X\u22c6\u27e9+ \u03b42\nk\u2225P\u2126(X\u22c6\u2212Xk)\u22252\nF\n\u2264r2\nk\u22121 \u22122\u03b4k\u2225Xk \u2212X\u22c6\u22252\nF + \u03b42\nk\u2225Xk \u2212X\u22c6\u22252\nF\n(4.3)\nsince for any matrix X, \u2225P\u2126(X)\u2225F \u2264\u2225X\u2225F . Under our assumptions about the size of \u03b4k, we have\n2\u03b4k \u2212\u03b42\nk \u2265\u03b2 for all k \u22651 and some \u03b2 > 0 and thus\nr2\nk \u2264r2\nk\u22121 \u2212\u03b2\u2225Xk \u2212X\u22c6\u22252\nF .\n(4.4)\nTwo properties follow from this:\n13\n1. The sequence {\u2225P\u2126(Y k \u2212Y \u22c6)\u2225F } is nonincreasing and, therefore, converges to a limit.\n2. As a consequence, \u2225Xk \u2212X\u22c6\u22252\nF \u21920 as k \u2192\u221e.\nThe theorem is established.\n4.2\nGeneral convergence theorem\nOur second result is more general and establishes the convergence of the SVT iterations to the\nsolution of (3.4) under general convex constraints. From now now, we will only assume that the\nfunction F(X) is Lipschitz in the sense that\n\u2225F(X) \u2212F(Y \u2225\u2264L(F)\u2225X \u2212Y \u2225F ,\n(4.5)\nfor some nonnegative constant L(F).\nNote that if F is a\ufb03ne, F(X) = b \u2212A(X), we have\nL(F) = \u2225A\u22252 where \u2225A\u22252 is the spectrum norm of the linear transformation A de\ufb01ned as \u2225A\u22252 :=\nsup{\u2225A(X)\u2225\u21132 : \u2225X\u2225F = 1}. We also recall that F(X) = (f1(X), . . . , fm(X)) where each fi is\nconvex, and that the Lagrangian for the problem (3.4) is given by\nL(X, y) = f\u03c4(X) + \u27e8y, F(X)\u27e9,\ny \u22650.\nWe will assume to simplify that strong duality holds which is automatically true if the constraints\nobey constraint quali\ufb01cations such as Slater\u2019s condition [6].\nWe \ufb01rst establish the following preparatory lemma.\nLemma 4.3 Let (X\u22c6, y\u22c6) be a primal-dual optimal pair for (3.4). Then for each \u03b4 > 0, y\u22c6obeys\ny\u22c6= [y\u22c6+ \u03b4F(X\u22c6)]+.\n(4.6)\nProof. Recall that the projection x0 of a point x onto a convex set C is characterized by\n(\nx0 \u2208C,\n\u27e8y \u2212x0, x \u2212x0\u27e9\u22640, \u2200y \u2208C.\nIn the case where C = Rm\n+ = {x \u2208Rm : x \u22650}, this condition becomes x0 \u22650 and\n\u27e8y \u2212x0, x \u2212x0\u27e9\u22640, \u2200y \u22650.\nNow because y\u22c6is dual optimal we have\nL(X\u22c6, y\u22c6) \u2265L(X\u22c6, y),\n\u2200y \u22650.\nSubstituting the expression for the Lagrangian, this is equivalent to\n\u27e8y \u2212y\u22c6, F(X\u22c6)\u27e9\u22640,\n\u2200y \u22650,\nwhich is the same as\n\u27e8y \u2212y\u22c6, y\u22c6+ \u03c1F(X\u22c6) \u2212y\u22c6\u27e9\u22640,\n\u2200y \u22650, \u2200\u03c1 \u22650.\nHence it follows that y\u22c6must be the projection of y\u22c6+ \u03c1F(X\u22c6) onto the nonnegative orthant Rm\n+.\nSince the projection of an arbitrary vector x onto Rm\n+ is given by x+, our claim follows.\nWe are now in the position to state our general convergence result.\n14\nTheorem 4.4 Suppose that the sequence of step sizes obeys 0 < inf \u03b4k \u2264sup \u03b4k < 2/\u2225L(F)\u22252,\nwhere L(F) is the Lipschitz constant in (4.5). Then assuming strong duality, the sequence {Xk}\nobtained via (3.5) converges to the unique solution of (3.4).\nProof. Let (X\u22c6, y\u22c6) be primal-dual optimal for the problem (3.4). We claim that the optimality\nconditions give that for all X\n\u27e8Zk, X \u2212Xk\u27e9+ \u27e8yk\u22121, F(X) \u2212F(Xk)\u27e9\u22650,\n\u27e8Z\u22c6, X \u2212X\u22c6\u27e9+ \u27e8y\u22c6, F(X) \u2212F(X\u22c6)\u27e9\u22650,\n(4.7)\nfor some Zk \u2208\u2202f\u03c4(Xk) and some Z\u22c6\u2208\u2202f\u03c4(X\u22c6). We justify this assertion by proving one of the\ntwo inequalities since the other is exactly similar. For the \ufb01rst, Xk minimizes L(X, yk\u22121) over all\nX and, therefore, there exist Zk \u2208\u2202f\u03c4(Xk) and Zk\ni \u2208\u2202fi(Xk), 1 \u2264i \u2264m, such that\nZk +\nm\nX\ni=1\nyk\u22121\ni\nZk\ni = 0.\nNow because each fi is convex,\nfi(X) \u2212fi(Xk) \u2265\u27e8Zk\ni , X \u2212Xk\u27e9\nand, therefore,\n\u27e8Zk, X \u2212Xk\u27e9+\nm\nX\ni=1\nyk\u22121\ni\n(fi(X) \u2212fi(Xk)) \u2265\u27e8Zk +\nm\nX\ni=1\nyk\u22121\ni\nZk\ni , X \u2212Xk\u27e9= 0.\nThis is (4.7).\nNow write the \ufb01rst inequality in (4.7) for X\u22c6, the second for Xk and sum the two inequalities.\nThis gives\n\u27e8Zk \u2212Z\u22c6, Xk \u2212X\u22c6\u27e9+ \u27e8yk\u22121 \u2212y\u22c6, F(Xk) \u2212F(X\u22c6)\u27e9\u22640.\nThe rest of the proof is essentially the same as that of Theorem 4.5. It follows from Lemma 4.1\nthat\n\u27e8yk\u22121 \u2212y\u22c6, F(Xk) \u2212F(X\u22c6)\u27e9\u2264\u2212\u27e8Zk \u2212Z\u22c6, Xk \u2212X\u22c6\u27e9\u2264\u2212\u2225Xk \u2212X\u22c6\u22252\nF .\n(4.8)\nWe continue and observe that because y\u22c6= [y\u22c6+ \u03b4kF(X)]+ by Lemma 4.3, we have\n\u2225yk \u2212y\u22c6\u2225= \u2225[yk\u22121 + \u03b4kF(Xk)]+ \u2212[y\u22c6+ \u03b4kF(X\u22c6)]+\u2225\n\u2264\u2225yk\u22121 \u2212y\u22c6+ \u03b4k(F(Xk) \u2212F(X\u22c6))\u2225\nsince the projection onto the convex set Rm\n+ is a contraction. Therefore,\n\u2225yk \u2212y\u22c6\u22252 = \u2225yk\u22121 \u2212y\u22c6\u22252 + 2\u03b4k \u27e8yk\u22121 \u2212y\u22c6, F(Xk) \u2212F(X\u22c6)\u27e9+ \u03b42\nk\u2225F(Xk) \u2212F(X\u22c6)\u22252\n\u2264\u2225yk\u22121 \u2212y\u22c6\u22252 \u22122\u03b4k\u2225Xk \u2212X\u22c6\u22252\nF + \u03b42\nkL2 \u2225Xk \u2212X\u22c6\u22252\nF ,\nwhere we have put L instead of L(F) for short. Under our assumptions about the size of \u03b4k, we\nhave 2\u03b4k \u2212\u03b42\nkL2 \u2265\u03b2 for all k \u22651 and some \u03b2 > 0. Then\n\u2225yk \u2212y\u22c6\u22252 \u2264\u2225yk\u22121 \u2212y\u22c6\u22252 \u2212\u03b2\u2225Xk \u2212X\u22c6\u22252\nF ,\n(4.9)\n15\nand the conclusion is as before.\nThe problem (3.1) with linear constraints can be reduced to (3.4) by choosing\nF(X) =\n\u0014 b\n\u2212b\n\u0015\n\u2212\n\u0014 A\n\u2212A\n\u0015\nX,\nand we have the following corollary:\nCorollary 4.5 Suppose that the sequence of step sizes obeys 0 < inf \u03b4k \u2264sup \u03b4k < 2/\u2225A\u22252\n2. Then\nthe sequence {Xk} obtained via (3.3) converges to the unique solution of (3.1).\nLet \u2225A\u22252 := sup{\u2225A(X)\u2225F : \u2225X\u2225F = 1}. With F(X) given as above, we have |L(F)|2 = 2\u2225A\u22252\n2\nand thus, Theorem 4.4 guarantees convergence as long as 0 < inf \u03b4k \u2264sup \u03b4k < 1/\u2225A\u22252\n2. However,\nan argument identical to the proof of Theorem 4.2 would remove the extra factor of two. We omit\nthe details.\n5\nImplementation and Numerical Results\nThis section provides implementation details of the SVT algorithm\u2014as to make it practically\ne\ufb00ective for matrix completion\u2014such as the numerical evaluation of the singular value thresholding\noperator, the selection of the step size \u03b4k, the selection of a stopping criterion, and so on. This\nsection also introduces several numerical simulation results which demonstrate the performance\nand e\ufb00ectiveness of the SVT algorithm. We show that 30, 000 \u00d7 30, 000 matrices of rank 10 are\nrecovered from just about 0.4% of their sampled entries in a matter of a few minutes on a modest\ndesktop computer with a 1.86 GHz CPU (dual core with Matlab\u2019s multithreading option enabled)\nand 3 GB of memory.\n5.1\nImplementation details\n5.1.1\nEvaluation of the singular value thresholding operator\nTo apply the singular value tresholding operator at level \u03c4 to an input matrix, it su\ufb03ces to know\nthose singular values and corresponding singular vectors above the threshold \u03c4.\nIn the matrix\ncompletion problem, the singular value thresholding operator is applied to sparse matrices {Y k}\nsince the number of sampled entries is typically much lower than the number of entries in the\nunknown matrix M, and we are hence interested in numerical methods for computing the dominant\nsingular values and singular vectors of large sparse matrices. The development of such methods is\na relatively mature area in scienti\ufb01c computing and numerical linear algebra in particular. In fact,\nmany high-quality packages are readily available. Our implementation uses PROPACK, see [36]\nfor documentation and availability. One reason for this choice is convenience: PROPACK comes\nin a Matlab and a Fortran version, and we \ufb01nd it convenient to use the well-documented Matlab\nversion. More importantly, PROPACK uses the iterative Lanczos algorithm to compute the singular\nvalues and singular vectors directly, by using the Lanczos bidiagonalization algorithm with partial\nreorthogonalization. In particular, PROPACK does not compute the eigenvalues and eigenvectors\nof (Y k)\u2217Y k and Y k(Y k)\u2217, or of an augmented matrix as in the Matlab built-in function \u2018svds\u2019 for\nexample. Consequently, PROPACK is an e\ufb03cient\u2014both in terms of number of \ufb02ops and storage\nrequirement\u2014and stable package for computing the dominant singular values and singular vectors\n16\nof a large sparse matrix.\nFor information, the available documentation [36] reports a speedup\nfactor of about ten over Matlab\u2019s \u2018svds\u2019. Furthermore, the Fortran version of PROPACK is about\n3\u20134 times faster than the Matlab version. Despite this signi\ufb01cant speedup, we have only used the\nMatlab version but since the singular value shrinkage operator is by-and-large the dominant cost in\nthe SVT algorithm, we expect that a Fortran implementation would run about 3 to 4 times faster.\nAs for most SVD packages, though one can specify the number of singular values to compute,\nPROPACK can not automatically compute only those singular values exceeding the threshold \u03c4.\nOne must instead specify the number s of singular values ahead of time, and the software will\ncompute the s largest singular values and corresponding singular vectors. To use this package, we\nmust then determine the number sk of singular values of Y k\u22121 to be computed at the kth iteration.\nWe use the following simple method. Let rk\u22121 = rank(Xk\u22121) be the number of nonzero singular\nvalues of Xk\u22121 at the previous iteration. Set sk = rk\u22121 +1 and compute the \ufb01rst sk singular values\nof Y k\u22121. If some of the computed singular values are already smaller than \u03c4, then sk is a right\nchoice. Otherwise, increment sk by a prede\ufb01ned integer \u2113repeatedly until some of the singular\nvalues fall below \u03c4. In the experiments, we choose \u2113= 5. Another rule might be to repeatedly\nmultiply sk by a positive number\u2014e.g. 2\u2014until our criterion is met. Incrementing sk by a \ufb01xed\ninteger works very well in practice; in our experiments, we very rarely need more than one update.\nWe note that it is not necessary to rerun the Lanczos iterations for the \ufb01rst sk vectors since they\nhave been already computed; only a few new singular values (\u2113of them) need to be numerically\nevaluated. This can be done by modifying the PROPACK routines. We have not yet modi\ufb01ed\nPROPACK, however. Had we done so, our run times would be decreased.\n5.1.2\nStep sizes\nThere is a large literature on ways of selecting a step size but for simplicity, we shall use step sizes\nthat are independent of the iteration count; that is \u03b4k = \u03b4 for k = 1, 2, . . .. From Theorem 4.2,\nconvergence for the completion problem is guaranteed (2.7) provided that 0 < \u03b4 < 2. This choice\nis, however, too conservative and the convergence is typically slow. In our experiments, we use\ninstead\n\u03b4 = 1.2 n1n2\nm ,\n(5.1)\ni.e. 1.2 times the undersampling ratio. We give a heuristic justi\ufb01cation below.\nConsider a \ufb01xed matrix A \u2208Rn1\u00d7n2. Under the assumption that the column and row spaces of\nA are not well aligned with the vectors taken from the canonical basis of Rn1 and Rn2 respectively\u2014\nthe incoherence assumption in [13]\u2014then with very large probability over the choices of \u2126, we have\n(1 \u2212\u01eb)p \u2225A\u22252\nF \u2264\u2225P\u2126(A)\u22252\nF \u2264(1 + \u01eb)p \u2225A\u22252\nF ,\np := m/(n1n2),\n(5.2)\nprovided that the rank of A is not too large. The probability model is that \u2126is a set of sampled\nentries of cardinality m sampled uniformly at random so that all the choices are equally likely. In\n(5.2), we want to think of \u01eb as a small constant, e.g. smaller than 1/2. In other words, the \u2018energy\u2019\nof A on \u2126(the set of sampled entries) is just about proportional to the size of \u2126. The near isometry\n(5.2) is a consequence of Theorem 4.1 in [13], and we omit the details.\nNow returning to the proof of Theorem 4.2, we see that a su\ufb03cient condition for the convergence\nof (2.7) is\n\u2203\u03b2 > 0,\n\u22122\u03b4\u2225X\u22c6\u2212Xk\u22252\nF + \u03b42\u2225P\u2126(X\u22c6\u2212Xk)\u22252\nF \u2264\u2212\u03b2\u2225X\u22c6\u2212Xk\u22252\nF ,\n17\ncompare (4.4), which is equivalent to\n0 < \u03b4 < 2\n\u2225X\u22c6\u2212Xk\u22252\nF\n\u2225P\u2126(X\u22c6\u2212Xk)\u22252\nF\n.\nSince \u2225P\u2126(X)\u2225F \u2264\u2225X\u2225F for any matrix X \u2208Rn1\u00d7n2, it is safe to select \u03b4 < 2. But suppose that\nwe could apply (5.2) to the matrix A = X\u22c6\u2212Xk. Then we could take \u03b4 inversely proportional\nto p; e.g. with \u01eb = 1/4, we could take \u03b4 \u22641.6p\u22121. Below, we shall use the value \u03b4 = 1.2p\u22121 which\nallows us to take large steps and still provides convergence, at least empirically.\nThe reason why this is not a rigorous argument is that (5.2) cannot be applied to A = X\u22c6\u2212Xk\neven though this matrix di\ufb00erence may obey the incoherence assumption. The issue here is that\nX\u22c6\u2212Xk is not a \ufb01xed matrix, but rather depends on \u2126since the iterates {Xk} are computed\nwith the knowledge of the sampled set.\n5.1.3\nInitial steps\nThe SVT algorithm starts with Y 0 = 0, and we want to choose a large \u03c4 to make sure that the\nsolution of (2.8) is close enough to a solution of (1.1). De\ufb01ne k0 as that integer obeying\n\u03c4\n\u03b4\u2225P\u2126(M)\u22252\n\u2208(k0 \u22121, k0].\n(5.3)\nSince Y 0 = 0, it is not di\ufb03cult to see that\nXk = 0,\nY k = k\u03b4 P\u2126(M),\nk = 1, . . . , k0.\nTo save work, we may simply skip the computations of X1, . . . , Xk0, and start the iteration by\ncomputing Xk0+1 from Y k0.\nThis strategy is a special case of a kicking device introduced in [44]; the main idea of such\na kicking scheme is that one can \u2018jump over\u2019 a few steps whenever possible.\nJust like in the\naforementioned reference, we can develop similar kicking strategies here as well. Because in our\nnumerical experiments the kicking is rarely triggered, we forgo the description of such strategies.\n5.1.4\nStopping criteria\nHere, we discuss stopping criteria for the sequence of SVT iterations (2.7), and present two possi-\nbilities.\nThe \ufb01rst is motivated by the \ufb01rst-order optimality conditions or KKT conditions tailored to the\nminimization problem (2.8). By (2.14) and letting \u2202Y g0(Y ) = 0 in (2.13), we see that the solution\nX\u22c6\n\u03c4 to (2.8) must also verify\n(\nX = D\u03c4(Y ),\nP\u2126(X \u2212M) = 0,\n(5.4)\nwhere Y is a matrix vanishing outside of \u2126c. Therefore, to make sure that Xk is close to X\u22c6\n\u03c4 , it\nis su\ufb03cient to check how close (Xk, Y k\u22121) is to obeying (5.4). By de\ufb01nition, the \ufb01rst equation in\n(5.4) is always true. Therefore, it is natural to stop (2.7) when the error in the second equation is\nbelow a speci\ufb01ed tolerance. We suggest stopping the algorithm when\n\u2225P\u2126(Xk \u2212M)\u2225F\n\u2225P\u2126(M)\u2225F\n\u2264\u01eb,\n(5.5)\n18\nwhere \u01eb is a \ufb01xed tolerance, e.g. 10\u22124. We provide a short heuristic argument justifying this choice\nbelow.\nIn the matrix completion problem, we know that under suitable assumptions\n\u2225P\u2126(M)\u22252\nF \u224dp \u2225M\u22252\nF ,\nwhich is just (5.2) applied to the \ufb01xed matrix M (the symbol \u224dhere means that there is a constant\n\u01eb as in (5.2)). Suppose we could also apply (5.2) to the matrix Xk \u2212M (which we rigorously cannot\nsince Xk depends on \u2126), then we would have\n\u2225P\u2126(Xk \u2212M)\u22252\nF \u224dp \u2225Xk \u2212M\u22252\nF ,\n(5.6)\nand thus\n\u2225P\u2126(Xk \u2212M)\u2225F\n\u2225P\u2126(M)\u2225F\n\u224d\u2225Xk \u2212M\u2225F\n\u2225M\u2225F\n.\nIn words, one would control the relative reconstruction error by controlling the relative error on\nthe set of sampled locations.\nA second stopping criterion comes from duality theory. Firstly, the iterates Xk are generally\nnot feasible for (2.8) although they become asymptotically feasible. One can construct a feasible\npoint from Xk by projecting it onto the a\ufb03ne space {X : P\u2126(X) = P\u2126(M)} as follows:\n\u02dc\nXk = Xk + P\u2126(M \u2212Xk).\nAs usual let f\u03c4(X) = \u03c4\u2225X\u2225\u2217+ 1\n2\u2225X\u22252\nF and denote by p\u22c6the optimal value of (2.8). Since \u02dc\nXk is\nfeasible, we have\np\u22c6\u2264f\u03c4( \u02dc\nXk) := bk.\nSecondly, using the notations of Section 2.4, duality theory gives that\nak := g0(Y k\u22121) = L(Xk, Y k\u22121) \u2264p\u22c6.\nTherefore, bk \u2212ak is an upper bound on the duality gap and one can stop the algorithm when this\nquantity falls below a given tolerance.\nFor very large problems in which one holds Xk in reduced SVD form, one may not want to\ncompute the projection \u02dc\nXk since this matrix would not have low rank and would require signi\ufb01-\ncant storage space (presumably, one would not want to spend much time computing this projection\neither). Hence, the second method only makes practical sense when the dimensions are not pro-\nhibitively large, or when the iterates do not have low rank.\n5.1.5\nAlgorithm\nWe conclude this section by summarizing the implementation details and give the SVT algorithm\nfor matrix completion below (Algorithm 1). Of course, one would obtain a very similar structure\nfor the more general problems of the form (3.1) and (3.4) with linear inequality constraints. For\nconvenience, de\ufb01ne for each nonnegative integer s \u2264min{n1, n2},\n[U k, \u03a3k, V k]s,\nk = 1, 2, . . . ,\nwhere U k = [uk\n1, . . . , uk\ns] and V k = [vk\n1, . . . , vk\ns] are the \ufb01rst s singular vectors of the matrix Y k,\nand \u03a3k is a diagonal matrix with the \ufb01rst s singular values \u03c3k\n1, . . . , \u03c3k\ns on the diagonal.\n19\nAlgorithm 1: Singular Value Thresholding (SVT) Algorithm\nInput: sampled set \u2126and sampled entries P\u2126(M), step size \u03b4, tolerance \u01eb, parameter\n\u03c4, increment \u2113, and maximum iteration count kmax\nOutput: Xopt\nDescription: Recover a low-rank matrix M from a subset of sampled entries\n1\nSet Y 0 = k0\u03b4 P\u2126(M) (k0 is de\ufb01ned in (5.3))\n2\nSet r0 = 0\n3\nfor k = 1 to kmax\n4\nSet sk = rk\u22121 + 1\n5\nrepeat\n6\nCompute [U k\u22121, \u03a3k\u22121, V k\u22121]sk\n7\nSet sk = sk + \u2113\n8\nuntil \u03c3k\u22121\nsk\u2212\u2113\u2264\u03c4\n9\nSet rk = max{j : \u03c3k\u22121\nj\n> \u03c4}\n10\nSet Xk = Prk\nj=1(\u03c3k\u22121\nj\n\u2212\u03c4)uk\u22121\nj\nvk\u22121\nj\n11\nif \u2225P\u2126(Xk \u2212M)\u2225F /\u2225P\u2126M\u2225F \u2264\u01eb then break\n12\nSet Y k\nij =\n(\n0\nif (i, j) \u0338\u2208\u2126,\nY k\u22121\nij\n+ \u03b4(Mij \u2212Xk\nij)\nif (i, j) \u2208\u2126\n13\nend for k\n14\nSet Xopt = Xk\n5.2\nNumerical results\n5.2.1\nLinear equality constraints\nOur implementation is in Matlab and all the computational results we are about to report were\nobtained on a desktop computer with a 1.86 GHz CPU (dual core with Matlab\u2019s multithreading\noption enabled) and 3 GB of memory. In our simulations, we generate n \u00d7 n matrices of rank r\nby sampling two n \u00d7 r factors ML and MR independently, each having i.i.d. Gaussian entries, and\nsetting M = MLM \u2217\nR as it is suggested in [13]. The set of observed entries \u2126is sampled uniformly\nat random among all sets of cardinality m.\nThe recovery is performed via the SVT algorithm (Algorithm 1), and we use\n\u2225P\u2126(Xk \u2212M)\u2225F /\u2225P\u2126M\u2225F < 10\u22124\n(5.7)\nas a stopping criterion. As discussed earlier, the step sizes are constant and we set \u03b4 = 1.2p\u22121.\nThroughout this section, we denote the output of the SVT algorithm by Xopt. The parameter \u03c4\nis chosen empirically and set to \u03c4 = 5n. A heuristic argument is as follows. Clearly, we would like\nthe term \u03c4\u2225M\u2225\u2217to dominate the other, namely, 1\n2\u2225M\u22252\nF . For products of Gaussian matrices as\nabove, standard random matrix theory asserts that the Frobenius norm of M concentrates around\nn\u221ar, and that the nuclear norm concentrates around about nr (this should be clear in the simple\ncase where r = 1 and is generally valid). The value \u03c4 = 5n makes sure that on the average, the\n20\nUnknown M\nComputational results\nsize (n \u00d7 n)\nrank (r)\nm/dr\nm/n2\ntime(s)\n# iters\nrelative error\n10\n6\n0.12\n23\n117\n1.64 \u00d7 10\u22124\n1, 000 \u00d7 1, 000\n50\n4\n0.39\n196\n114\n1.59 \u00d7 10\u22124\n100\n3\n0.57\n501\n129\n1.68 \u00d7 10\u22124\n10\n6\n0.024\n147\n123\n1.73 \u00d7 10\u22124\n5, 000 \u00d7 5, 000\n50\n5\n0.10\n950\n108\n1.61 \u00d7 10\u22124\n100\n4\n0.158\n3,339\n123\n1.72 \u00d7 10\u22124\n10\n6\n0.012\n281\n123\n1.73 \u00d7 10\u22124\n10, 000 \u00d7 10, 000\n50\n5\n0.050\n2,096\n110\n1.65 \u00d7 10\u22124\n100\n4\n0.080\n7,059\n127\n1.79 \u00d7 10\u22124\n10\n6\n0.006\n588\n124\n1.73 \u00d7 10\u22124\n20, 000 \u00d7 20, 000\n50\n5\n0.025\n4,581\n111\n1.66 \u00d7 10\u22124\n30, 000 \u00d7 30, 000\n10\n6\n0.004\n1,030\n125\n1.73 \u00d7 10\u22124\nTable 1: Experimental results for matrix completion. The rank r is the rank of the unknown\nmatrix M, m/dr is the ratio between the number of sampled entries and the number of\ndegrees of freedom in an n \u00d7 n matrix of rank r (oversampling ratio), and m/n2 is the fraction\nof observed entries. All the computational results on the right are averaged over \ufb01ve runs.\nvalue of \u03c4\u2225M\u2225\u2217is about 10 times that of 1\n2\u2225M\u22252\nF as long as the rank is bounded away from the\ndimension n.\nOur computational results are displayed in Table 1. There, we report the run time in seconds, the\nnumber of iterations it takes to reach convergence (5.7), and the relative error of the reconstruction\nrelative error = \u2225Xopt \u2212M\u2225F /\u2225M\u2225F ,\n(5.8)\nwhere M is the real unknown matrix. All of these quantities are averaged over \ufb01ve runs. The table\nalso gives the percentage of entries that are observed, namely, m/n2 together with a quantity that\nwe may want to think as the information oversampling ratio. Recall that an n \u00d7 n matrix of rank\nr depends upon dr := r(2n \u2212r) degrees of freedom. Then m/dr is the ratio between the number of\nsampled entries and the \u2018true dimensionality\u2019 of an n \u00d7 n matrix of rank r.\nThe \ufb01rst observation is that the SVT algorithm performs extremely well in these experiments.\nIn all of our experiments, it takes fewer than 200 SVT iterations to reach convergence.\nAs a\nconsequence, the run times are short.\nAs indicated in the table, we note that one recovers a\n1, 000\u00d71, 000 matrix of rank 10 in less than a minute. The algorithm also recovers 30, 000\u00d730, 000\nmatrices of rank 10 from about 0.4% of their sampled entries in just about 17 minutes. In addition,\nhigher-rank matrices are also e\ufb03ciently completed: for example, it takes between one and two\nhours to recover 10, 000\u00d710, 000 matrices of rank 100 and 20, 000\u00d720, 000 matrices of rank 50. We\nwould like to stress that these numbers were obtained on a modest CPU (1.86GHz). Furthermore,\na Fortran implementation is likely to cut down on these numbers by a multiplicative factor typically\nbetween three and four.\nWe also check the validity of the stopping criterion (5.7) by inspecting the relative error de\ufb01ned\nin (5.8). The table shows that the heuristic and nonrigorous analysis of Section 5.1 holds in practice\nsince the relative reconstruction error is of the same order as \u2225P\u2126(Xopt \u2212M)\u2225F /\u2225P\u2126M\u2225F \u223c10\u22124.\nIndeed, the overall relative errors reported in Table 1 are all less than 2 \u00d7 10\u22124.\n21\nWe emphasized all along an important feature of the SVT algorithm, which is that the matrices\nXk have low rank.\nWe demonstrate this fact empirically in Figure 1, which plots the rank of\nXk versus the iteration count k, and does this for unknown matrices of size 5, 000 \u00d7 5, 000 with\ndi\ufb00erent ranks. The plots reveal an interesting phenomenon: in our experiments, the rank of Xk\nis nondecreasing so that the maximum rank is reached in the \ufb01nal steps of the algorithm. In fact,\nthe rank of the iterates quickly reaches the value r of the true rank. After these few initial steps,\nthe SVT iterations search for that matrix with rank r minimizing the objective functional. As\nmentioned earlier, the low-rank property is crucial for making the algorithm run fast.\n0\n50\n100\n150\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nItertion Step k\nRank of Xk\n0\n20\n40\n60\n80\n100\n120\n140\n0\n10\n20\n30\n40\n50\n60\nIteration step k\nRank of Xk\n0\n50\n100\n150\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n110\nIteration step k\nRank of Xk\nr = 10\nr = 50\nr = 100\nFigure 1: Rank of Xk as a function k when the unknown matrix M is of size 5, 000 \u00d7 5, 000\nand of rank r.\nFinally, we demonstrate the results of the SVT algorithm for matrix completion from noisy\nsampled entries. Suppose we observe data from the model\nBij = Mij + Zij,\n(i, j) \u2208\u2126,\n(5.9)\nwhere Z is a zero-mean Gaussian white noise with standard deviation \u03c3. We run the SVT algorithm\nbut stop early, as soon as Xk is consistent with the data and obeys\n\u2225P\u2126(Xk \u2212B)\u22252\nF \u2264(1 + \u01eb) m\u03c32,\n(5.10)\nwhere \u01eb is a small parameter. Our reconstruction\n\u02c6\nM is the \ufb01rst Xk obeying (5.10). The results\nare shown in Table 2 (the quantities are averages of 5 runs). De\ufb01ne the noise ratio as\n\u2225P\u2126(Z)\u2225F /\u2225P\u2126(M)\u2225F ,\nand the relative error by (5.8). From Table 2, we see that the SVT algorithm works well as the\nrelative error between the recovered and the true data matrix is just about equal to the noise ratio.\nThe theory of low-rank matrix recovery from noisy data is nonexistent at the moment, and is\nobviously beyond the scope of this paper. Having said this, we would like to conclude this section\nwith an intuitive and nonrigorous discussion, which may explain why the observed recovery error\nis within the noise level. Suppose again that\n\u02c6\nM obeys (5.6), namely,\n\u2225P\u2126( \u02c6\nM \u2212M)\u22252\nF \u224dp\u2225\u02c6\nM \u2212M\u22252\nF .\n(5.11)\n22\nUnknown matrix M\nComputational results\nnoise ratio\nsize (n \u00d7 n)\nrank (r)\nm/dr\nm/n2\ntime(s)\n# iters\nrelative error\n10\n6\n0.12\n10.8\n51\n0.78 \u00d7 10\u22122\n10\u22122\n1, 000 \u00d7 1, 000\n50\n4\n0.39\n87.7\n48\n0.95 \u00d7 10\u22122\n100\n3\n0.57\n216\n50\n1.13 \u00d7 10\u22122\n10\n6\n0.12\n4.0\n19\n0.72 \u00d7 10\u22121\n10\u22121\n1, 000 \u00d7 1, 000\n50\n4\n0.39\n33.2\n17\n0.89 \u00d7 10\u22121\n100\n3\n0.57\n85.2\n17\n1.01 \u00d7 10\u22121\n10\n6\n0.12\n0.9\n3\n0.52\n1\n1, 000 \u00d7 1, 000\n50\n4\n0.39\n7.8\n3\n0.63\n100\n3\n0.57\n34.8\n3\n0.69\nTable 2: Simulation results for noisy data. The computational results are averaged over \ufb01ve\nruns.\nAs mentioned earlier, one condition for this to happen is that M and\n\u02c6\nM have low rank. This is\nthe reason why it is important to stop the algorithm early as we hope to obtain a solution which\nis both consistent with the data and has low rank (the limit of the SVT iterations, limk\u2192\u221eXk,\nwill not generally have low rank since there may be no low-rank matrix matching the noisy data).\nFrom\n\u2225P\u2126( \u02c6\nM \u2212M)\u2225F \u2264\u2225P\u2126( \u02c6\nM \u2212B)\u2225F + \u2225P\u2126(B \u2212M)\u2225F ,\nand the fact that both terms on the right-hand side are on the order of\n\u221a\nm\u03c32, we would have\np\u2225\u02c6\nM \u2212M\u22252\nF = O(m\u03c32) by (5.11). In particular, this would give that the relative reconstruction\nerror is on the order of the noise ratio since \u2225P\u2126(M)\u22252\nF \u224dp\u2225M\u22252\nF \u2014as observed experimentally.\n5.2.2\nLinear inequality constraints\nWe now examine the speed at which one can solve similar problems with linear inequality constraints\ninstead of linear equality constraints. We assume the model (5.9), where the matrix M of rank\nr is sampled as before, and solve the problem (3.8) by using (3.10). We formulate the inequality\nconstraints in (3.8) with Eij = \u03c3 so that one searches for a solution\n\u02c6\nM with minimum nuclear\nnorm among all those matrices whose sampled entries deviate from the observed ones by at most\nthe noise level \u03c3.2 In this experiment, we adjust \u03c3 to be one tenth of a typical absolute entry of\nM, i.e. \u03c3 = 0.1 P\nij\u2208\u2126|Mij|/m, and the noise ratio as de\ufb01ned earlier is 0.780. We set n = 1, 000,\nr = 10, and the number m of sampled entries is \ufb01ve times the number of degrees of freedom,\ni.e. m = 5dr. Just as before, we set \u03c4 = 5n, and choose a constant step size \u03b4 = 1.2p\u22121.\nThe results, reported in Figure 2, show that the algorithm behaves just as well with linear\ninequality constraints.\nTo make this point, we compare our results with those obtained from\nnoiseless data (same unknown matrix and sampled locations). In the noiseless case, it takes about\n150 iterations to reach the tolerance \u01eb = 10\u22124 whereas in the noisy case, convergence occurs in\nabout 200 iterations (Figure 2(a)). In addition, just as in the noiseless problem, the rank of the\niterates is nondecreasing and quickly reaches the true value r of the rank of the unknown matrix\n2This may not be conservative enough from a statistical viewpoint but this works well in this case, and our\nemphasis here is on computational rather than statistical issues.\n23\n0\n20\n40\n60\n80\n100\n120\n140\n160\n180\n200\n10\n\u22125\n10\n\u22124\n10\n\u22123\n10\n\u22122\n10\n\u22121\n10\n0\nRelative errors\n \n \nError from noisy data\nResidual error from noisy data\nError from noiseless data\nResidual error from noiseless data\n0\n20\n40\n60\n80\n100\n120\n140\n160\n180\n200\n0\n5\n10\n15\nRank\n \n \nNoisy data\nNoiseless data\n(a)\n(b)\n0\n20\n40\n60\n80\n100\n120\n140\n160\n180\n200\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\nsvd time\n \n \nNoisy data\nNoiseless data\n(c)\nFigure 2: Computational results of the algorithm applied to noisy (linear inequality con-\nstraints as in (3.8)) and noiseless data (equality constraints). The blue (resp. red) color is\nused for the noisy (resp. noiseless) experiment.\n(a) Plot of the reconstruction errors from\nnoisy and noiseless data as a function of the iteration count. The thin line is the residual\nrelative error \u2225P\u2126(Xk \u2212M)\u2225F /\u2225P\u2126(M)\u2225F and the thick line is the overall relative error\n\u2225Xk \u2212M\u2225F /\u2225M\u2225F. (b) Rank of the iterates as a function of the iteration count. (c) Time it\ntakes to compute the singular value thresholding operation as a function of the iteration count.\nThe computer here is a single-core 3.00GHz Pentium 4 running Matlab 7.2.0.\nM we wish to recover (Figure 2(b)). As a consequence the SVT iterations take about the same\namount of time as in the noiseless case (Figure 2(c)) so that the total running time of the algorithm\ndoes not appear to be substantially di\ufb00erent from that in the noiseless case.\nWe close by pointing out that from a statistical point of view, the recovery of the matrix M\nfrom undersampled and noisy entries by the matrix equivalent of the Dantzig selector appears to\nbe accurate since the relative error obeys \u2225\u02c6\nM \u2212M\u2225F /\u2225M\u2225F = 0.0769 (recall that the noise ratio\nis about 0.08).\n6\nDiscussion\nThis paper introduced a novel algorithm, namely, the singular value thresholding algorithm for\nmatrix completion and related nuclear norm minimization problems. This algorithm is easy to\nimplement and surprisingly e\ufb00ective both in terms of computational cost and storage requirement\n24\nwhen the minimum nuclear-norm solution is also the lowest-rank solution. We would like to close\nthis paper by discussing a few open problems and research directions related to this work.\nOur algorithm exploits the fact that the sequence of iterates {Xk} have low rank when the\nminimum nuclear solution has low rank. An interesting question is whether one can prove (or\ndisprove) that in a majority of the cases, this is indeed the case.\nIt would be interesting to explore other ways of computing D\u03c4(Y )\u2014in words, the action of\nthe singular value shrinkage operator. Our approach uses the Lanczos bidiagonalization algorithm\nwith partial reorthogonalization which takes advantages of sparse inputs but other approaches are\npossible. We mention two of them.\n1. A series of papers have proposed the use of randomized procedures for the approximation\nof a matrix Y with a matrix Z of rank r [38, 41]. When this approximation consists of the\ntruncated SVD retaining the part of the expansion corresponding to singular values greater\nthan \u03c4, this can be used to evaluate D\u03c4(Y ). Some of these algorithms are e\ufb03cient when the\ninput Y is sparse [41], and it would be interesting to know whether these methods are fast\nand accurate enough to be used in the SVT iteration (2.7).\n2. A wide range of iterative methods for computing matrix functions of the general form f(Y )\nare available today, see [34] for a survey.\nA valuable research direction is to investigate\nwhether some of these iterative methods, or other to be developed, would provide powerful\nways for computing D\u03c4(Y ).\nIn practice, one would like to solve (2.8) for large values of \u03c4. However, a larger value of \u03c4\ngenerally means a slower rate of convergence. A good strategy might be to start with a value of\n\u03c4, which is large enough so that (2.8) admits a low-rank solution, and at the same time for which\nthe algorithm converges rapidly. One could then use a continuation method as in [50] to increase\nthe value of \u03c4 sequentially according to a schedule \u03c40, \u03c41, . . ., and use the solution to the previous\nproblem with \u03c4 = \u03c4i\u22121 as an initial guess for the solution to the current problem with \u03c4 = \u03c4i (warm\nstarting). We hope to report on this in a separate paper.\nAcknowledgments\nJ-F. C. is supported by the Wavelets and Information Processing Programme under a grant from DSTA,\nSingapore. E. C. is partially supported by the Waterman Award from the National Science Foundation\nand by an ONR grant N00014-08-1-0749. Z. S. is supported in part by Grant R-146-000-113-112 from the\nNational University of Singapore. E. C. would like to thank Benjamin Recht and Joel Tropp for fruitful\nconversations related to this project, and Stephen Becker for his help in preparing the computational results\nof Section 5.2.2.\nReferences\n[1] J. Abernethy, F. Bach, T. Evgeniou, and J.-P. Vert. Low-rank matrix factorization with attributes.\nTechnical Report N24/06/MM, Ecole des Mines de Paris, 2006.\n[2] ACM SIGKDD and Net\ufb02ix. Proceedings of KDD Cup and Workshop, 2007. Proceedings available online\nat http://www.cs.uic.edu/\u223cliub/KDD-cup-2007/proceedings.html.\n[3] Y. Amit, M. Fink, N. Srebro, and S. Ullman. Uncovering shared structures in multiclass classi\ufb01cation.\nIn Proceedings of the Twenty-fourth International Conference on Machine Learning, 2007.\n25\n[4] A. Argyriou, T. Evgeniou, and M. Pontil. Multi-task feature learning. In Neural Information Processing\nSystems, 2007.\n[5] J. Bect, L. Blanc-F\u00b4eraud, G. Aubert, and A. Chambolle, A \u21131 uni\ufb01ed variational framework for image\nrestoration, in Proc. Eighth Europ. Conf. Comput. Vision, 2004.\n[6] S. Boyd, and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.\n[7] J.-F. Cai, R. Chan, L. Shen, and Z. Shen. Restoration of chopped and nodded images by framelets.\nSIAM J. Sci. Comput., 30(3):1205\u20131227, 2008.\n[8] J.-F. Cai, R. H. Chan, and Z. Shen.\nA framelet-based image inpainting algorithm.\nAppl. Comput.\nHarmon. Anal., 24(2):131\u2013149, 2008.\n[9] J.-F. Cai, S. Osher, and Z. Shen. Convergence of the Linearized Bregman Iteration for \u21131-norm Mini-\nmization, 2008. UCLA CAM Report (08-52).\n[10] J.-F. Cai, S. Osher, and Z. Shen. Linearized Bregman Iterations for Compressed Sensing, 2008. Math.\nComp., to appear; see also UCLA CAM Report (08-06).\n[11] J.-F. Cai, S. Osher, and Z. Shen. Linearized Bregman Iterations for Frame-Based Image Deblurring,\n2008. preprint.\n[12] E. J. Cand`es, and F. Guo. New multiscale transforms, minimum total variation synthesis: Applications\nto edge-preserving image reconstruction. Signal Processing, 82:1519\u20131543, 2002.\n[13] E. J. Cand`es and B. Recht. Exact Matrix Completion via Convex Optimization, 2008.\n[14] E. J. Cand`es and J. Romberg. Sparsity and incoherence in compressive sampling. Inverse Problems,\n23(3):969\u2013985, 2007.\n[15] E. J. Cand`es, J. Romberg, and T. Tao. Robust uncertainty principles: exact signal reconstruction from\nhighly incomplete frequency information. IEEE Trans. Inform. Theory, 52(2):489\u2013509, 2006.\n[16] E. J. Cand`es and T. Tao. Decoding by linear programming. IEEE Trans. Inform. Theory, 51(12):4203\u2013\n4215, 2005.\n[17] E. J. Cand`es and T. Tao. Near-optimal signal recovery from random projections: universal encoding\nstrategies? IEEE Trans. Inform. Theory, 52(12):5406\u20135425, 2006.\n[18] E. J. Cand`es and T. Tao. The Dantzig selector: statistical estimation when p is much larger than n.\nAnnals of Statistics 35:2313\u20132351, 2007.\n[19] A. Chai and Z. Shen. Deconvolution: A wavelet frame approach. Numer. Math., 106(4):529\u2013587, 2007.\n[20] R. H. Chan, T. F. Chan, L. Shen, and Z. Shen. Wavelet algorithms for high-resolution image recon-\nstruction. SIAM J. Sci. Comput., 24(4):1408\u20131432 (electronic), 2003.\n[21] P. Chen, and D. Suter. Recovering the missing components in a large noisy low-rank matrix: application\nto SFM source. IEEE Transactions on Pattern Analysis and Machine Intelligence, 26(8):1051-1063, 2004.\n[22] P. L. Combettes and V. R. Wajs. Signal recovery by proximal forward-backward splitting. Multiscale\nModel. Simul., 4(4):1168\u20131200 (electronic), 2005.\n[23] J. Darbon and S. Osher. Fast discrete optimization for sparse approximations and deconvolutions, 2007.\npreprint.\n[24] I. Daubechies, M. Defrise, and C. De Mol. An iterative thresholding algorithm for linear inverse problems\nwith a sparsity constraint. Comm. Pure Appl. Math., 57(11):1413\u20131457, 2004.\n[25] I. Daubechies, G. Teschke, and L. Vese. Iteratively solving linear inverse problems under general convex\nconstraints. Inverse Probl. Imaging, 1(1):29\u201346, 2007.\n26\n[26] D. L. Donoho. Compressed sensing. IEEE Trans. Inform. Theory, 52(4):1289\u20131306, 2006.\n[27] M. Elad, J.-L. Starck, P. Querre, and D. L. Donoho. Simultaneous cartoon and texture image inpainting\nusing morphological component analysis (MCA). Appl. Comput. Harmon. Anal., 19(3):340\u2013358, 2005.\n[28] M. J. Fadili, J.-L. Starck, and F. Murtagh. Inpainting and zooming using sparse representations. The\nComputer Journal, to appear.\n[29] M. Fazel. Matrix Rank Minimization with Applications. PhD thesis, Stanford University, 2002.\n[30] M. Fazel, H. Hindi, and S. Boyd, Log-det heuristic for matrix rank minimization with applications to\nHankel and Euclidean distance matrices. in Proc. Am. Control Conf., June 2003.\n[31] M. Figueiredo, and R. Nowak, An EM algorithm for wavelet-based image restoration. IEEE Transactions\non Image Processing, 12(8):906\u2013916, 2003.\n[32] T. Goldstein and S. Osher. The Split Bregman Algorithm for L1 Regularized Problems, 2008. UCLA\nCAM Reprots (08-29).\n[33] E. T. Hale, W. Yin, and Y. Zhang. Fixed-point continuation for l1-minimization: methodology and\nconvergence. 2008. preprint.\n[34] N. J. Higham. Functions of Matrices: Theory and Computation. Society for Industrial and Applied\nMathematics, Philadelphia, PA, USA, 2008.\n[35] J.-B. Hiriart-Urruty and C. Lemar\u00b4echal. Convex analysis and minimization algorithms. I, volume 305\nof Grundlehren der Mathematischen Wissenschaften [Fundamental Principles of Mathematical Sciences].\nSpringer-Verlag, Berlin, 1993. Fundamentals.\n[36] R. M. Larsen, PROPACK \u2013 Software for large and sparse SVD calculations, Available from http:\n//sun.stanford.edu/\u223crmunk/PROPACK/.\n[37] A. S. Lewis. The mathematics of eigenvalue optimization. Math. Program., 97(1-2, Ser. B):155\u2013176,\n2003. ISMP, 2003 (Copenhagen).\n[38] E. Liberty, F. Woolfe, P.-G. Martinsson, V. Rokhlin, and M. Tygert. Randomized algorithms for the\nlow-rank approximation of matrices. Proc. Natl. Acad. Sci. USA, 104(51): 20167\u201320172, 2007.\n[39] S. Lintner, and F. Malgouyres. Solving a variational image restoration model which involves \u2113\u221econ-\nstraints. Inverse Problems, 20:815\u2013831, 2004.\n[40] Z. Liu, and L. Vandenberghe. Interior-point method for nuclear norm approximation with application\nto system identi\ufb01cation. submitted to Mathematical Programming, 2008.\n[41] P.-G. Martinsson, V. Rokhlin, and M. Tygert.\nA randomized algorithm for the approximation of\nmatrices Department of Computer Science, Yale University, New Haven, CT, Technical Report 1361,\n2006.\n[42] M. Mesbahi and G. P. Papavassilopoulos. On the rank minimization problem over a positive semide\ufb01nite\nlinear matrix inequality. IEEE Transactions on Automatic Control, 42(2):239\u2013243, 1997.\n[43] S. Osher, M. Burger, D. Goldfarb, J. Xu, and W. Yin. An iterative regularization method for total\nvariation-based image restoration. Multiscale Model. Simul., 4(2):460\u2013489 (electronic), 2005.\n[44] S. Osher, Y. Mao, B. Dong, and W. Yin. Fast Linearized Bregman Iteration for Compressed Sensing\nand Sparse Denoising, 2008. UCLA CAM Reprots (08-37).\n[45] B. Recht, M. Fazel, and P. Parrilo. Guaranteed minimum rank solutions of matrix equations via nuclear\nnorm minimization. 2007. Submitted to SIAM Review.\n[46] J.-L. Starck, D. L. Donoho, and E. J. Cand`es,\nAstronomical image representation by the curvelet\ntransform. Astronom. and Astrophys., 398:785\u2013800, 2003.\n27\n[47] K. C. Toh, M. J. Todd, and R. H. T\u00a8ut\u00a8unc\u00a8u. SDPT3 \u2013 a MATLAB software package for semide\ufb01nite-\nquadratic-linear programming, Available from http://www.math.nus.edu.sg/\u223cmattohkc/sdpt3.html.\n[48] C. Tomasi and T. Kanade. Shape and motion from image streams under orthography: a factorization\nmethod. International Journal of Computer Vision, 9(2):137\u2013154, 1992.\n[49] G. A. Watson. Characterization of the subdi\ufb00erential of some matrix norms. Linear Algebra Appl.,\n170:33\u201345, 1992.\n[50] S. J. Wright, R. Nowak, and M. Figueiredo. Sparse reconstruction by separable approximation. Sub-\nmitted for publication, 2007.\n[51] W. Yin, S. Osher, D. Goldfarb, and J. Darbon. Bregman iterative algorithms for \u21131-minimization with\napplications to compressed sensing. SIAM J. Imaging Sci., 1(1):143\u2013168, 2008.\n28\n",
        "sentence": " Indeed, the notion of sparsity assumption has been transposed into the concept of low-rank matrices and opened the way to numerous achievements (see for instance (Srebro, 2004; Cai et al., 2008)).",
        "context": "1\nIntroduction\n1.1\nMotivation\nThere is a rapidly growing interest in the recovery of an unknown low-rank or approximately low-\nrank matrix from very limited information. This problem occurs in many areas of engineering and\n1\npeople thought. Indeed, they proved that most low-rank matrices can be recovered exactly from\nmost sets of sampled entries even though these sets have surprisingly small cardinality, and more\nIn many instances, however, the matrix we wish to recover has low rank or approximately low\nrank.\nFor instance, the Net\ufb02ix data matrix of all user-ratings may be approximately low-rank"
    },
    {
        "title": "Robust principal component analysis",
        "author": [
            "E.J. Candes",
            "X. Li",
            "Y. Ma",
            "J. Wright"
        ],
        "venue": "Arxiv preprint ArXiv:0912.3599,",
        "citeRegEx": "Candes et al\\.,? \\Q2009\\E",
        "shortCiteRegEx": "Candes et al\\.",
        "year": 2009,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " In Robust PCA (Candes et al., 2009) and related literature, the signal S is assumed to have an additive decomposition S = X + Y where X is sparse and Y low-rank.",
        "context": null
    },
    {
        "title": "Rank-sparsity incoherence for matrix decomposition",
        "author": [
            "V. Chandrasekaran",
            "S. Sanghavi",
            "P.A. Parrilo",
            "A.S. Willsky"
        ],
        "venue": "SIAM J. Opt.,",
        "citeRegEx": "Chandrasekaran et al\\.,? \\Q2011\\E",
        "shortCiteRegEx": "Chandrasekaran et al\\.",
        "year": 2011,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " , in (Chandrasekaran et al., 2011).",
        "context": null
    },
    {
        "title": "Proximal splitting methods in signal processing. Fixed-Point Algorithms for Inverse Problems",
        "author": [
            "P.L. Combettes",
            "J.C. Pesquet"
        ],
        "venue": "Science and Engineering,",
        "citeRegEx": "Combettes and Pesquet,? \\Q2011\\E",
        "shortCiteRegEx": "Combettes and Pesquet",
        "year": 2011,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "A direct formulation for sparse pca using semidefinite programming",
        "author": [
            "A. D\u2019Aspremont",
            "L. El Ghaoui",
            "M.I. Jordan",
            "G.R.G. Lanckriet"
        ],
        "venue": "SIAM review,",
        "citeRegEx": "D.Aspremont et al\\.,? \\Q2007\\E",
        "shortCiteRegEx": "D.Aspremont et al\\.",
        "year": 2007,
        "abstract": "We examine the problem of approximating, in the Frobenius-norm sense, a\npositive, semidefinite symmetric matrix by a rank-one matrix, with an upper\nbound on the cardinality of its eigenvector. The problem arises in the\ndecomposition of a covariance matrix into sparse factors, and has wide\napplications ranging from biology to finance. We use a modification of the\nclassical variational representation of the largest eigenvalue of a symmetric\nmatrix, where cardinality is constrained, and derive a semidefinite programming\nbased relaxation for our problem. We also discuss Nesterov's smooth\nminimization technique applied to the SDP arising in the direct sparse PCA\nmethod.",
        "full_text": "",
        "sentence": " A different formulation using SDP programming is introduced in (D\u2019Aspremont et al., 2007) with good empirical results.",
        "context": null
    },
    {
        "title": "Operator norm consistent estimation of large-dimensional sparse covariance matrices",
        "author": [
            "N. El Karoui"
        ],
        "venue": "Annals of Statistics,",
        "citeRegEx": "Karoui,? \\Q2009\\E",
        "shortCiteRegEx": "Karoui",
        "year": 2009,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Sparse inverse covariance estimation with the graphical lasso",
        "author": [
            "J. Friedman",
            "T. Hastie",
            "R. Tibshirani"
        ],
        "venue": null,
        "citeRegEx": "Friedman et al\\.,? \\Q2008\\E",
        "shortCiteRegEx": "Friedman et al\\.",
        "year": 2008,
        "abstract": "Abstract\n               We consider the problem of estimating sparse graphs by a lasso penalty applied to the inverse covariance matrix. Using a coordinate descent procedure for the lasso, we develop a simple algorithm\u2014the graphical lasso\u2014that is remarkably fast: It solves a 1000-node problem (\u223c500000 parameters) in at most a minute and is 30\u20134000 times faster than competing methods. It also provides a conceptual link between the exact problem and the approximation suggested by Meinshausen and B\u00fchlmann (2006). We illustrate the method on some cell-signaling data from proteomics.",
        "full_text": "",
        "sentence": " These methods are readily adapted to matrix valued data and have been applied to covariance estimation (El Karoui, 2009; Bien & Tibshirani, 2010) and graphical model structure learning (Banerjee et al., 2007; Friedman et al., 2008). Due to the low-rank assumption, our method does not directly apply to the estimation of precision matrices often used for gaussian graphical model structure learning (Friedman et al., 2008), and the applications of conditional independence structures generated by low-rank and possibly sparse models is to be discussed.",
        "context": null
    },
    {
        "title": "Sparse approximate solutions to semidefinite programs",
        "author": [
            "E. Hazan"
        ],
        "venue": "In Proceedings of the 8th Latin American conference on Theoretical informatics,",
        "citeRegEx": "Hazan,? \\Q2008\\E",
        "shortCiteRegEx": "Hazan",
        "year": 2008,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " The special form of this SDP can be leveraged to use the efficient resolution technique from (Hazan, 2008).",
        "context": null
    },
    {
        "title": "Non-negative Matrix Factorization with Sparseness Constraints",
        "author": [
            "P.O. Hoyer"
        ],
        "venue": "Journal of Machine Learning Research,",
        "citeRegEx": "Hoyer,? \\Q2004\\E",
        "shortCiteRegEx": "Hoyer",
        "year": 2004,
        "abstract": "",
        "full_text": "",
        "sentence": " There is no strong guarantee on the sparsity achieved by NMF nor is it easy to set the target sparsity and different methods for sparse NMF have been proposed in (Hoyer, 2004; Kim & Park, 2008).",
        "context": null
    },
    {
        "title": "Global functional atlas of escherichia coli encompassing previously uncharacterized proteins",
        "author": [
            "P. Hu",
            "S.C. Janga",
            "M. Babu",
            "J.J. D\u0131\u0301az-Mej\u0301\u0131a",
            "G. Butland",
            "W. Yang",
            "O. Pogoutse",
            "X. Guo",
            "S. Phanse",
            "P Wong"
        ],
        "venue": "PLoS biology,",
        "citeRegEx": "Hu et al\\.,? \\Q2009\\E",
        "shortCiteRegEx": "Hu et al\\.",
        "year": 2009,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " We use data from (Hu et al., 2009), in which protein interactions in Escherichia coli bacteria are scored by strength in [0, 2].",
        "context": null
    },
    {
        "title": "Convex optimization without projection steps",
        "author": [
            "M. Jaggi"
        ],
        "venue": "Arxiv preprint arXiv:1108.1170,",
        "citeRegEx": "Jaggi,? \\Q2011\\E",
        "shortCiteRegEx": "Jaggi",
        "year": 2011,
        "abstract": "For the general problem of minimizing a convex function over a compact convex\ndomain, we will investigate a simple iterative approximation algorithm based on\nthe method by Frank & Wolfe 1956, that does not need projection steps in order\nto stay inside the optimization domain. Instead of a projection step, the\nlinearized problem defined by a current subgradient is solved, which gives a\nstep direction that will naturally stay in the domain. Our framework\ngeneralizes the sparse greedy algorithm of Frank & Wolfe and its primal-dual\nanalysis by Clarkson 2010 (and the low-rank SDP approach by Hazan 2008) to\narbitrary convex domains. We give a convergence proof guaranteeing\n{\\epsilon}-small duality gap after O(1/{\\epsilon}) iterations.\n  The method allows us to understand the sparsity of approximate solutions for\nany l1-regularized convex optimization problem (and for optimization over the\nsimplex), expressed as a function of the approximation quality. We obtain\nmatching upper and lower bounds of {\\Theta}(1/{\\epsilon}) for the sparsity for\nl1-problems. The same bounds apply to low-rank semidefinite optimization with\nbounded trace, showing that rank O(1/{\\epsilon}) is best possible here as well.\nAs another application, we obtain sparse matrices of O(1/{\\epsilon}) non-zero\nentries as {\\epsilon}-approximate solutions when optimizing any convex function\nover a class of diagonally dominant symmetric matrices.\n  We show that our proposed first-order method also applies to nuclear norm and\nmax-norm matrix optimization problems. For nuclear norm regularized\noptimization, such as matrix completion and low-rank recovery, we demonstrate\nthe practical efficiency and scalability of our algorithm for large matrix\nproblems, as e.g. the Netflix dataset. For general convex optimization over\nbounded matrix max-norm, our algorithm is the first with a convergence\nguarantee, to the best of our knowledge.",
        "full_text": "Convex Optimization without Projection Steps\nwith Applications to Sparse and Low Rank Approximation\nMartin Jaggi\nETH Z\u00a8urich, Switzerland\njaggi@inf.ethz.ch\nAbstract.\nWe study the general problem of minimizing a convex function over a compact\nconvex domain. We will investigate a simple iterative approximation algorithm based on the\nmethod by Frank & Wolfe [FW56], that does not need projection steps in order to stay inside\nthe optimization domain.\nInstead of a projection step, the linearized problem de\ufb01ned by a\ncurrent subgradient is solved, which gives a step direction that will naturally stay in the do-\nmain. Our framework generalizes the sparse greedy algorithm of [FW56] and its primal-dual\nanalysis by [Cla10] (and the low-rank SDP approach by [Haz08]) to arbitrary convex domains.\nAnalogously, we give a convergence proof guaranteeing \u03b5-small duality gap after O( 1\n\u03b5) iterations.\nThe method allows us to understand the sparsity of approximate solutions for any \u21131-regularized\nconvex optimization problem (and for optimization over the simplex), expressed as a function of\nthe approximation quality. We obtain matching upper and lower bounds of \u0398(1\n\u03b5) for the sparsity\nfor \u21131-problems. The same bounds apply to low-rank semide\ufb01nite optimization with bounded\ntrace, showing that rank O( 1\n\u03b5) is best possible here as well. As another application, we obtain\nsparse matrices of O( 1\n\u03b5) non-zero entries as \u03b5-approximate solutions when optimizing any convex\nfunction over a class of diagonally dominant symmetric matrices.\nWe show that our proposed \ufb01rst-order method also applies to nuclear norm and max-norm\nmatrix optimization problems. For nuclear norm regularized optimization, such as matrix com-\npletion and low-rank recovery, we demonstrate the practical e\ufb03ciency and scalability of our\nalgorithm for large matrix problems, as e.g. the Net\ufb02ix dataset. For general convex optimiza-\ntion over bounded matrix max-norm, our algorithm is the \ufb01rst with a convergence guarantee,\nto the best of our knowledge.\n(This article consists of the \ufb01rst two chapters of the author\u2019s PhD thesis [Jag11].)\n1\narXiv:1108.1170v6  [math.OC]  27 Dec 2011\nContents\n1\nIntroduction\n3\n2\nThe Poor Man\u2019s Approach to Convex Optimization and Duality\n5\n2.1\nSubgradients of a Convex Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n2.2\nA Duality for Convex Optimization over Compact Domain . . . . . . . . . . . . . . . . . .\n6\n3\nA Projection-Free First-Order Method for Convex Optimization\n7\n3.1\nThe Algorithm\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n3.2\nObtaining a Guaranteed Small Duality Gap . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n3.3\nChoosing the Optimal Step-Size by Line-Search . . . . . . . . . . . . . . . . . . . . . . . .\n13\n3.4\nThe Curvature Measure of a Convex Function . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n3.5\nOptimizing over Convex Hulls . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n3.6\nRandomized Variants, and Stochastic Optimization . . . . . . . . . . . . . . . . . . . . . .\n16\n3.7\nRelation to Classical Convex Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n4\nSparse Approximation over the Simplex\n18\n4.1\nUpper Bound: Sparse Greedy on the Simplex . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n4.2\n\u2126( 1\n\u03b5) Lower Bound on the Sparsity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\n5\nSparse Approximation with Bounded \u21131-Norm\n22\n5.1\nRelation to Matching Pursuit and Basis Pursuit in Compressed Sensing\n. . . . . . . . . .\n25\n6\nOptimization with Bounded \u2113\u221e-Norm\n26\n7\nSemide\ufb01nite Optimization with Bounded Trace\n27\n7.1\nLow-Rank Semide\ufb01nite Optimization with Bounded Trace: The O( 1\n\u03b5) Algorithm by Hazan\n28\n7.2\nSolving Arbitrary SDPs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n32\n7.3\nTwo Improved Variants of Algorithm 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\n7.4\n\u2126( 1\n\u03b5) Lower Bound on the Rank . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\n8\nSemide\ufb01nite Optimization with \u2113\u221e-Bounded Diagonal\n35\n9\nSparse Semide\ufb01nite Optimization\n37\n10 Submodular Optimization\n39\n11 Optimization with the Nuclear and Max-Norm\n39\n11.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n39\n11.2 The Nuclear Norm for Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n42\n11.2.1 Weighted Nuclear Norm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n44\n11.3 The Max-Norm for Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n45\n11.4 Optimizing with Bounded Nuclear Norm and Max-Norm . . . . . . . . . . . . . . . . . . .\n46\n11.4.1 Optimization with a Nuclear Norm Regularization . . . . . . . . . . . . . . . . . .\n47\n11.4.2 Optimization with a Max-Norm Regularization . . . . . . . . . . . . . . . . . . . .\n48\n11.5 Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n49\n11.5.1 Robust Principal Component Analysis . . . . . . . . . . . . . . . . . . . . . . . . .\n49\n11.5.2 Matrix Completion and Low Norm Matrix Factorizations\n. . . . . . . . . . . . . .\n50\n11.5.3 The Structure of the Resulting Eigenvalue Problems . . . . . . . . . . . . . . . . .\n52\n11.5.4 Relation to Simon Funk\u2019s SVD Method\n. . . . . . . . . . . . . . . . . . . . . . . .\n52\n11.6 Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n53\n11.7 Conclusion\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n55\nBibliography\n55\n2\n1 Introduction\nMotivation.\nFor the performance of large scale approximation algorithms for convex optimiza-\ntion, the trade-o\ufb00between the number of iterations on one hand, and the computational cost\nper iteration on the other hand, is of crucial importance. The lower complexity per iteration\nis among the main reasons why \ufb01rst-order methods (i.e., methods using only information from\nthe \ufb01rst derivative of the objective function), as for example stochastic gradient descent, are\ncurrently used much more widely and successfully in many machine learning applications \u2014 de-\nspite the fact that they often need a larger number of iterations than for example second-order\nmethods.\nClassical gradient descent optimization techniques usually require a projection step in each\niteration, in order to get back to the feasible region. For a variety of applications, this is a\nnon-trivial and costly step.\nOne prominent example is semide\ufb01nite optimization, where the\nprojection of an arbitrary symmetric matrix back to the PSD matrices requires the computation\nof a complete eigenvalue-decomposition.\nHere we study a simple \ufb01rst-order approach that does not need any projection steps, and is\napplicable to any convex optimization problem over a compact convex domain. The algorithm\nis a generalization of an existing method originally proposed by Frank & Wolfe [FW56], which\nwas recently extended and analyzed in the seminal paper of Clarkson [Cla10] for optimization\nover the unit simplex.\nInstead of a projection, the primitive operation of the optimizer here is to minimize a linear ap-\nproximation to the function over the same (compact) optimization domain. Any (approximate)\nminimizer of this simpler linearized problem is then chosen as the next step-direction. Because\nall such candidates are always feasible for the original problem, the algorithm will automatically\nstay in our convex feasible region. The analysis will show that the number of steps needed is\nroughly identical to classical gradient descent schemes, meaning that O\n\u0000 1\n\u03b5\n\u0001\nsteps su\ufb03ce in order\nto obtain an approximation quality of \u03b5 > 0.\nThe main question about the e\ufb03ciency per iteration of our algorithm, compared to a classical\ngradient descent step, can not be answered generally in favor of one or the other. Whether a\nprojection or a linearized problem is computationally cheaper will crucially depend on the shape\nand the representation of the feasible region. Interestingly, if we consider convex optimization\nover the Euclidean \u2225.\u22252-ball, the two approaches fully coincide, i.e., we exactly recover classical\ngradient descent.\nHowever there are several classes of optimization problems where the lin-\nearization approach we present here is de\ufb01nitely very attractive, and leads to faster and simpler\nalgorithms. This includes for example \u21131-regularized problems, which we discuss in Sections 4\nand 5, as well as semide\ufb01nite optimization under bounded trace, as studied by [Haz08], see\nSection 7.\nSparsity and Low-Rank.\nFor these mentioned speci\ufb01c classes of convex optimization problems,\nwe will additionally demonstrate that our studied algorithm leads to (optimally) sparse or low-\nrank solutions. This property is a crucial side-e\ufb00ect that can usually not be achieved by classical\noptimization techniques, and corresponds to the coreset concept known from computational\ngeometry, see also [GJ09]. More precisely, we show matching upper and lower bounds of \u0398\n\u0000 1\n\u03b5\n\u0001\nfor the sparsity of solutions to general \u21131-regularized problems, and also for optimizing over\nthe simplex, if the required approximation quality is \u03b5. For matrix optimization, an analogous\nstatement will hold for the rank in case of nuclear norm regularized problems.\nApplications.\nApplications of the \ufb01rst mentioned class of \u21131-regularized problems do include\nmany machine learning algorithms ranging from support vector machines (SVMs) to boosting\n3\nand multiple kernel learning, as well as \u21132-support vector regression (SVR), mean-variance anal-\nysis in portfolio selection [Mar52], the smallest enclosing ball problem [BC07], \u21131-regularized\nleast squares (also known as basis pursuit de-noising in compressed sensing), the Lasso [Tib96],\nand \u21131-regularized logistic regression [KKB07] as well as walking of arti\ufb01cial dogs over rough\nterrain [KBP+10].\nThe second mentioned class of matrix problems, that is, optimizing over semide\ufb01nite matrices\nwith bounded trace, has applications in low-rank recovery [FHB01, CR09, CT10], dimension-\nality reduction, matrix factorization and completion problems, as well as general semide\ufb01nite\nprograms (SDPs).\nFurther applications to nuclear norm and max-norm optimization, such as sparse/robust PCA\nwill be discussed in Section 11.\nHistory and Related Work.\nThe class of \ufb01rst-order optimization methods in the spirit of Frank\nand Wolfe [FW56] has a rich history in the literature. Although the focus of the original paper\nwas on quadratic programming, its last section [FW56, Section 6] already introduces the general\nalgorithm for minimizing convex functions using the above mentioned linearization idea, when\nthe optimization domain is given by linear inequality constraints. In this case, each intermediate\nstep consists of solving a linear program. The given convergence guarantee bounds the primal\nerror, and assumes that all internal problems are solved exactly.\nLater [DH78] has generalized the same method to arbitrary convex domains, and improved\nthe analysis to also work when the internal subproblems are only solved approximately, see\nalso [Dun80]. Patriksson in [Pat93, Pat98] then revisited the general optimization paradigm,\ninvestigated several interesting classes of convex domains, and coined the term \u201ccost approxi-\nmation\u201d for this type of algorithms. More recently, [Zha03] considered optimization over convex\nhulls, and studies the crucial concept of sparsity of the resulting approximate solutions. However,\nthis proposed algorithm does not use linear subproblems.\nThe most recent work of Clarkson [Cla10] provides a good overview of the existing lines of re-\nsearch, and investigates the sparsity solutions when the optimization domain is the unit simplex,\nand establishing the connection to coreset methods from computational geometry. Furthermore,\n[Cla10] was the \ufb01rst to introduce the stronger notion of convergence in primal-dual error for this\nclass of problems, and relating this notion of duality gap to Wolfe duality.\nOur Contributions.\nThe character of this article mostly lies in reviewing, re-interpreting and\ngeneralizing the existing approach given by [Cla10], [Haz08] and the earlier papers by [Zha03,\nDH78, FW56], who do deserve credit for the analysis techniques. Our contribution here is to\ntransfer these methods to the more general case of convex optimization over arbitrary bounded\nconvex subsets of a vector space, while providing stronger primal-dual convergence guarantees.\nTo do so, we propose a very simple alternative concept of optimization duality, which will allow\nus to generalize the stronger primal-dual convergence analysis which [Cla10] has provided for\nthe the simplex case, to optimization over arbitrary convex domains. So far, no such guarantees\non the duality gap were known in the literature for the Frank-Wolfe-type algorithms [FW56],\nexcept when optimizing over the simplex. Furthermore, we generalize Clarkson\u2019s analysis [Cla10]\nto work when only approximate linear internal optimizers are used, and to arbitrary starting\npoints. Also, we study the sparsity of solutions in more detail, obtaining upper and lower bounds\nfor the sparsity of approximate solutions for a wider class of domains.\nOur proposed notion of duality gives simple certi\ufb01cates for the current approximation quality,\nwhich can be used for any optimization algorithm for convex optimization problems over bounded\ndomain, even in the case of non-di\ufb00erentiable objective functions.\n4\nWe demonstrate the broad applicability of our general technique to several important classes\nof optimization problems, such as \u21131- and \u2113\u221e-regularized problems, as well as semide\ufb01nite opti-\nmization with uniformly bounded diagonal, and sparse semide\ufb01nite optimization.\nLater in Section 11 we will give a simple transformation in order to apply the \ufb01rst-order\noptimization techniques we review here also to nuclear norm and max-norm matrix optimization\nproblems.\nAcknowledgments.\nCredit for the important geometric interpretation of the duality gap over\nthe spectahedron as the distance to the linearization goes to Soeren Laue. Furthermore, the\nauthor would like to thank Marek Sulovsk\u00b4y, Bernd G\u00a8artner, Arkadi Nemirovski, Elad Hazan,\nJoachim Giesen, Sebastian Stich, Michel Baes, Michael B\u00a8urgisser and Christian Lorenz M\u00a8uller\nfor helpful discussions and comments.\n2 The Poor Man\u2019s Approach to Convex Optimization and Duality\nThe Idea of a Duality given by Supporting Hyperplanes.\nSuppose we are given the task of\nminimizing a convex function f over a bounded convex set D \u2282Rn, and let us assume for the\nmoment that f is continuously di\ufb00erentiable.\nThen for any point x \u2208D, it seems natural to consider the tangential \u201csupporting\u201d hyperplane\nto the graph of the function f at the point (x, f(x)). Since the function f is convex, any such\nlinear approximation must lie below the graph of the function.\nUsing this linear approximation for each point x \u2208D, we de\ufb01ne a dual function value \u03c9(x)\nas the minimum of the linear approximation to f at point x, where the minimum is taken over\nthe domain D. We note that the point attaining this linear minimum also seems to be good\ndirection of improvement for our original minimization problem given by f, as seen from the\ncurrent point x. This idea will lead to the optimization algorithm that we will discuss below.\nAs the entire graph of f lies above any such linear approximation, it is easy to see that\n\u03c9(x) \u2264f(y) holds for each pair x, y \u2208D. This fact is called weak duality in the optimization\nliterature.\nThis rather simple de\ufb01nition already completes the duality concept that we will need in this\npaper. We will provide a slightly more formal and concise de\ufb01nition in the next subsection,\nwhich is useful also for the case of non-di\ufb00erentiable convex functions. The reason we call this\nconcept a poor man\u2019s duality is that we think it is considerably more direct and intuitive for the\nsetting here, when compared to classical Lagrange duality or Wolfe duality, see e.g. [BV04].\n2.1 Subgradients of a Convex Function\nIn the following, we will work over a general vector space X equipped with an inner product\n\u27e8., .\u27e9. As the most prominent example in our investigations, the reader might always think of\nthe case X = Rn with \u27e8x, y\u27e9= xT y being the standard Euclidean scalar product.\nWe consider general convex optimization problems given by a convex function f : X \u2192R over\na compact1 convex domain D \u2286X, or formally\nminimize\nx\u2208D\nf(x) .\n(1)\nIn order to develop both our algorithm and the notion of duality for such convex optimization\nproblems in the following, we need to formally de\ufb01ne the supporting hyperplanes at a given\n1Here we call a set D \u2286X compact if it is closed and bounded. See [KZ05] for more details.\n5\npoint x \u2208D. These planes coincide exactly with the well-studied concept of subgradients of a\nconvex function.\nFor each point x \u2208D, the subdi\ufb00erential at x is de\ufb01ned as the set of normal vectors of the\na\ufb03ne linear functions through (x, f(x)) that lie below the function f. Formally\n\u2202f(x) := {dx \u2208X | f(y) \u2265f(x) + \u27e8y \u2212x, dx\u27e9\n\u2200y \u2208D} .\n(2)\nAny element dx \u2208\u2202f(x) is called a subgradient to f at x. Note that for each x, \u2202f(x) is a\nclosed convex set. Furthermore, if f is di\ufb00erentiable, then the subdi\ufb00erential consists of exactly\none element for each x \u2208D, namely \u2202f(x) = {\u2207f(x)}, as explained e.g. in [Nem05, KZ05].\nIf we assume that f is convex and lower semicontinuous2 on D, then it is known that \u2202f(x)\nis non-empty, meaning that there exists at least one subgradient dx for every point x \u2208D.\nFor a more detailed investigation of subgradients, we refer the reader to one of the works of\ne.g. [Roc97, BV04, Nem05, KZ05, BL06].\n2.2 A Duality for Convex Optimization over Compact Domain\nFor a given point x \u2208D, and any choice of a subgradient dx \u2208\u2202f(x), we de\ufb01ne a dual function\nvalue\n\u03c9(x, dx) := min\ny\u2208D f(x) + \u27e8y \u2212x, dx\u27e9.\n(3)\nIn other words \u03c9(x, dx) \u2208R is the minimum of the linear approximation to f de\ufb01ned by the\nsubgradient dx at the supporting point x, where the minimum is taken over the domain D. This\nminimum is always attained, since D is compact, and the linear function is continuous in y.\nBy the de\ufb01nition of the subgradient \u2014 as lying below the graph of the function f \u2014 we readily\nattain the property of weak-duality, which is at the core of the optimization approach we will\nstudy below.\nLemma 1 (Weak duality). For all pairs x, y \u2208D, it holds that\n\u03c9(x, dx) \u2264f(y)\nProof. Immediately from the de\ufb01nition of the dual \u03c9(., .):\n\u03c9(x, dx) =\nminz\u2208D f(x) + \u27e8z \u2212x, dx\u27e9\n\u2264\nf(x) + \u27e8y \u2212x, dx\u27e9\n\u2264\nf(y) .\nHere the last inequality is by the de\ufb01nition (2) of a subgradient.\nGeometrically, this fact can be understood as that any function value f(y), which is \u201cpart of\u201d\nthe graph of f, always lies higher than the minimum over any linear approximation (given by\ndx) to f.\nIn the case that f is di\ufb00erentiable, there is only one possible choice for a subgradient, namely\ndx = \u2207f(x), and so we will then denote the (unique) dual value for each x by\n\u03c9(x) := \u03c9(x, \u2207f(x)) = min\ny\u2208D f(x) + \u27e8y \u2212x, \u2207f(x)\u27e9.\n(4)\n2The assumption that our objective function f is lower semicontinuous on D, is equivalent to the fact that its\nepigraph \u2014 i.e. the set {(x, t) \u2208D \u00d7 R | t \u2265f(x)} of all points lying on or above the graph of the function\n\u2014 is closed, see also [KZ05, Theorem 7.1.2].\n6\nThe Duality Gap as a Measure of Approximation Quality.\nThe above duality concept allows\nus to compute a very simple measure of approximation quality, for any candidate solution x \u2208D\nto problem (1). This measure will be easy to compute even if the true optimum value f(x\u2217) is\nunknown, which will very often be the case in practical applications. The duality gap g(x, dx)\nat any point x \u2208D and any subgradient subgradient dx \u2208\u2202f(x) is de\ufb01ned as\ng(x, dx) := f(x) \u2212\u03c9(x, dx) = max\ny\u2208D \u27e8x \u2212y, dx\u27e9,\n(5)\nor in other words as the di\ufb00erence of the current function value f(x) to the minimum value\nof the corresponding linearization at x, taken over the domain D.\nThe quantity g(x, dx) =\nf(x) \u2212\u03c9(x, dx) will be called the duality gap at x, for the chosen dx.\nBy the weak duality Lemma 1, we obtain that for any optimal solution x\u2217to problem (1), it\nholds that\ng(x, dx) \u2265f(x) \u2212f(x\u2217) \u22650\n\u2200x \u2208D, \u2200dx \u2208\u2202f(x) .\n(6)\nHere the quantity f(x) \u2212f(x\u2217) is what we call the primal error at point x, which is usually\nimpossible to compute due to x\u2217being unknown. The above inequality (6) now gives us that\nthe duality gap \u2014 which is easy to compute, given dx \u2014 is always an upper bound on the\nprimal error. This property makes the duality gap an extremely useful measure for example as\na stopping criterion in practical optimizers or heuristics.\nWe call a point x \u2208X an \u03b5-approximation if g(x, dx) \u2264\u03b5 for some choice of subgradient\ndx \u2208\u2202f(x).\nFor the special case that f is di\ufb00erentiable, we will again use the simpler notation g(x) for\nthe (unique) duality gap for each x, i.e.\ng(x) := g(x, \u2207f(x)) = max\ny\u2208D \u27e8x \u2212y, \u2207f(x)\u27e9.\nRelation to Duality of Norms.\nIn the special case when the optimization domain D is given\nby the unit ball of some norm on the space X, we observe the following:\nObservation 2. For optimization over any domain D = {x \u2208X | \u2225x\u2225\u22641} being the unit ball\nof some norm \u2225.\u2225, the duality gap for the optimization problem min\nx\u2208D f(x) is given by\ng(x, dx) = \u2225dx\u2225\u2217+ \u27e8x, dx\u27e9,\nwhere \u2225.\u2225\u2217is the dual norm of \u2225.\u2225.\nProof. Directly by the de\ufb01nitions of the dual norm \u2225x\u2225\u2217= sup\u2225y\u2225\u22641\u27e8y, x\u27e9, and the duality gap\ng(x, dx) = maxy\u2208D \u27e8y, \u2212dx\u27e9+ \u27e8x, dx\u27e9as in (5).\n3 A Projection-Free First-Order Method for Convex Optimization\n3.1 The Algorithm\nIn the following we will generalize the sparse greedy algorithm of [FW56] and its analysis\nby [Cla10] to convex optimization over arbitrary compact convex sets D \u2286X of a vector space.\nMore formally, we assume that the space X is a Hilbert space, and consider problems of the\nform (1), i.e.,\nminimize\nx\u2208D\nf(x) .\nHere we suppose that the objective function f is di\ufb00erentiable over the domain D, and that for\nany x \u2208D, we are given the gradient \u2207f(x) via an oracle.\n7\nThe existing algorithms so far did only apply to convex optimization over the simplex (or\nconvex hulls in some cases) [Cla10], or over the spectahedron of PSD matrices [Haz08], or then\ndid not provide guarantees on the duality gap.\nInspired by the work of Hazan [Haz08], we\ncan also relax the requirement of exactly solving the linearized problem in each step, to just\ncomputing approximations thereof, while keeping the same convergence results. This allows for\nmore e\ufb03cient steps in many applications.\nAlso, our algorithm variant works for arbitrary starting points, without needing to compute\nthe best initial \u201cstarting vertex\u201d of D as in [Cla10].\nThe Primal-Dual Idea.\nWe are motivated by the geometric interpretation of the \u201cpoor man\u2019s\u201d\nduality gap, as explained in the previous Section 2. This duality gap is the maximum di\ufb00erence\nof the current value f(x), to the linear approximation to f at the currently \ufb01xed point x, where\nthe linear maximum is taken over the domain D. This observation leads to the algorithmic idea\nof directly using the current linear approximation (over the convex domain D) to obtain a good\ndirection for the next step, automatically staying in the feasible region.\nThe general optimization method with its two precision variants is given in Algorithm 1.\nFor the approximate variant, the constant Cf > 0 is an upper bound on the curvature of the\nobjective function f, which we will explain below in more details.\nAlgorithm 1 Greedy on a Convex Set\nInput: Convex function f, convex set D, target accuracy \u03b5\nOutput: \u03b5-approximate solution for problem (1)\nPick an arbitrary starting point x(0) \u2208D\nfor k = 0 . . . \u221edo\nLet \u03b1 :=\n2\nk+2\nCompute s := ExactLinear\n\u0000\u2207f(x(k)), D\n\u0001\n{Solve the linearized primitive task exactly}\n\u2014or\u2014\nCompute s := ApproxLinear\n\u0000\u2207f(x(k)), D, \u03b1Cf\n\u0001\n{Approximate the linearized problem}\nUpdate x(k+1) := x(k) + \u03b1(s \u2212x(k))\nend for\nThe Linearized Primitive.\nThe internal \u201cstep direction\u201d procedure ExactLinear(c, D) used\nin Algorithm 1 is a method that minimizes the linear function \u27e8x, c\u27e9over the compact convex\ndomain D. Formally it must return a point s \u2208D such that \u27e8s, c\u27e9= min\ny\u2208D \u27e8y, c\u27e9. In terms of\nthe smooth convex optimization literature, the vectors y that have negative scalar product with\nthe gradient, i.e. \u27e8y, \u2207f(x)\u27e9< 0, are called descent directions, see e.g. [BV04, Chapter 9]. The\nmain di\ufb00erence to classical convex optimization is that we always choose descent steps staying\nin the domain D, where traditional gradient descend techniques usually use arbitrary directions\nand need to project back onto D after each step. We will comment more on this analogy in\nSection 3.7.\nIn the alternative interpretation of our duality concept from Section 2, the linearized sub-task\nmeans that we search for a point s that \u201crealizes\u201d the current duality gap g(x), that is the\ndistance to the linear approximation, as given in (5).\nIn the special case that the set D is given by an intersection of linear constraints, this sub-task\nis exactly equivalent to linear programming, as already observed by [FW56, Section 6]. However,\nfor many other representations of important speci\ufb01c feasible domains D, this internal primitive\noperation is signi\ufb01cantly easier to solve, as we will see in the later sections.\n8\nx\ns\nD\nf(x)\nFigure 1: Visualization of a step of Algorithm 1, moving from the current point x = x(k) towards a linear\nminimizer s \u2208D. Here the two-dimensional domain D is part of the ground plane, and we plot\nthe function values vertically. Visualization by Robert Carnecky.\nThe alternative approximate variant of Algorithm 1 uses the procedure ApproxLinear (c, D, \u03b5\u2032)\nas the internal \u201cstep-direction generator\u201d. Analogously to the exact primitive, this procedure\napproximates the minimum of the linear function \u27e8x, c\u27e9over the convex domain D, with ad-\nditive error \u03b5\u2032 > 0. Formally ApproxLinear (c, D, \u03b5\u2032) must return a point s \u2208D such that\n\u27e8s, c\u27e9\u2264min\ny\u2208D \u27e8y, c\u27e9+ \u03b5\u2032. For several applications, this can be done signi\ufb01cantly more e\ufb03ciently\nthan the exact variant, see e.g. the applications for semide\ufb01nite programming in Section 7.\nThe Curvature.\nEverything we need for the analysis of Algorithm 1 is that the linear approx-\nimation to our (convex) function f at any point x does not deviate from f by too much, when\ntaken over the whole optimization domain D.\nThe curvature constant Cf of a convex and di\ufb00erentiable function f : Rn \u2192R, with respect\nto the compact domain D is de\ufb01ned as.\nCf :=\nsup\nx,s\u2208D,\n\u03b1\u2208[0,1],\ny=x+\u03b1(s\u2212x)\n1\n\u03b12 (f(y) \u2212f(x) \u2212\u27e8y \u2212x, \u2207f(x)\u27e9) .\n(7)\nA motivation to consider this quantity follows if we imagine our optimization procedure at the\ncurrent point x = x(k), and choosing the next iterate as y = x(k+1) := x + \u03b1(s \u2212x). Bounded\nCf then means that the deviation of f at y from the \u201cbest\u201d linear prediction given by \u2207f(x) is\nbounded, where the acceptable deviation is weighted by the inverse of the squared step-size \u03b1.\nFor linear functions f for example, it holds that Cf = 0.\nThe de\ufb01ning term f(y) \u2212f(x) \u2212\u27e8y \u2212x, dx\u27e9is also widely known as the Bregman divergence\nde\ufb01ned by f. The quantity Cf turns out to be small for many relevant applications, some of\nwhich we will discuss later, or see also [Cla10].\nThe assumption of bounded curvature Cf is indeed very similar to a Lipschitz assumption on\nthe gradient of f, see also the discussion in Sections 3.4 and 3.7. In the optimization literature,\nthis property is sometimes also called Cf-strong smoothness.\nIt will not always be possible to compute the constant Cf exactly. However, for all algorithms\nin the following, and also their analysis, it is su\ufb03cient to just use some upper bound on Cf. We\nwill comment in some more details on the curvature measure in Section 3.4.\n9\nConvergence.\nThe following theorem shows that after O\n\u0000 1\n\u03b5\n\u0001\nmany iterations, Algorithm 1\nobtains an \u03b5-approximate solution.\nThe analysis essentially follows the approach of [Cla10],\ninspired by the earlier work of [FW56, DH78, Pat93] and [Zha03]. Later, in Section 4.2, we\nwill show that this convergence rate is indeed best possible for this type of algorithm, when\nconsidering optimization over the unit-simplex. More precisely, we will show that the dependence\nof the sparsity on the approximation quality, as given by the algorithm here, is best possible up\nto a constant factor. Analogously, for the case of semide\ufb01nite optimization with bounded trace,\nwe will prove in Section 7.4 that the obtained (low) rank of the approximations given by this\nalgorithm is optimal, for the given approximation quality.\nTheorem 3 (Primal Convergence). For each k \u22651, the iterate x(k) of the exact variant of Algo-\nrithm 1 satis\ufb01es\nf(x(k)) \u2212f(x\u2217) \u22644Cf\nk + 2 ,\nwhere x\u2217\u2208D is an optimal solution to problem (1). For the approximate variant of Algorithm 1,\nit holds that\nf(x(k)) \u2212f(x\u2217) \u22648Cf\nk + 2 .\n(In other words both algorithm variants deliver a solution of primal error at most \u03b5 after O( 1\n\u03b5)\nmany iterations.)\nThe proof of the above theorem on the convergence-rate of the primal error crucially depends\non the following Lemma 4 on the improvement in each iteration.\nWe recall from Section 2\nthat the duality gap for the general convex problem (1) over the domain D is given by g(x) =\nmax\ns\u2208D \u27e8x \u2212s, \u2207f(x)\u27e9.\nLemma 4. For any step x(k+1) := x(k) + \u03b1(s \u2212x(k)) with arbitrary step-size \u03b1 \u2208[0, 1], it holds\nthat\nf(x(k+1)) \u2264f(x(k)) \u2212\u03b1g(x(k)) + \u03b12Cf\nif s is given by s := ExactLinear (\u2207f(x), D).\nIf the approximate primitive s := ApproxLinear (\u2207f(x), D, \u03b1Cf) is used instead, then it\nholds that\nf(x(k+1)) \u2264f(x(k)) \u2212\u03b1g(x(k)) + 2\u03b12Cf .\nProof. We write x := x(k), y := x(k+1) = x+\u03b1(s\u2212x), and dx := \u2207f(x) to simplify the notation,\nand \ufb01rst prove the second part of the lemma. We use the de\ufb01nition of the curvature constant\nCf of our convex function f, to obtain\nf(y) =\nf(x + \u03b1(s \u2212x))\n\u2264\nf(x) + \u03b1\u27e8s \u2212x, dx\u27e9+ \u03b12Cf .\nNow we use that the choice of s := ApproxLinear (dx, D, \u03b5\u2032) is a good \u201cdescent direction\u201d on\nthe linear approximation to f at x. Formally, we are given a point s that satis\ufb01es \u27e8s, dx\u27e9\u2264\nmin\ny\u2208D\u27e8y, dx\u27e9+ \u03b5\u2032, or in other words we have\n\u27e8s \u2212x, dx\u27e9\u2264\nminy\u2208D\u27e8y, dx\u27e9\u2212\u27e8x, dx\u27e9+ \u03b5\u2032\n=\n\u2212g(x, dx) + \u03b5\u2032 ,\nfrom the de\ufb01nition (5) of the duality gap g(x) = g(x, dx). Altogether, we obtain\nf(y) \u2264\nf(x) + \u03b1(\u2212g(x) + \u03b5\u2032) + \u03b12Cf\n=\nf(x) \u2212\u03b1g(x) + 2\u03b12Cf ,\n10\nthe last equality following by our choice of \u03b5\u2032 = \u03b1Cf. This proves the lemma for the approximate\ncase. The \ufb01rst claim for the exact linear primitive ExactLinear() follows by the same proof\nfor \u03b5\u2032 = 0.\nHaving Lemma 4 at hand, the proof of our above primal convergence Theorem 3 now follows\nalong the same idea as in [Cla10, Theorem 2.3] or [Haz08, Theorem 1]. Note that a weaker\nvariant of Lemma 4 was already proven by [FW56].\nProof of Theorem 3. From Lemma 4 we know that for every step of the exact variant of Algo-\nrithm 1, it holds that f(x(k+1)) \u2264f(x(k)) \u2212\u03b1g(x(k)) + \u03b12Cf.\nWriting h(x) := f(x) \u2212f(x\u2217) for the (unknown) primal error at any point x, this implies that\nh(x(k+1)) \u2264\nh(x(k)) \u2212\u03b1g(x(k)) + \u03b12Cf\n\u2264\nh(x(k)) \u2212\u03b1h(x(k)) + \u03b12Cf\n=\n(1 \u2212\u03b1)h(x(k)) + \u03b12Cf ,\n(8)\nwhere we have used weak duality h(x) \u2264g(x) as given by in (6). We will now use induction\nover k in order to prove our claimed bound, i.e.,\nh(x(k+1)) \u2264\n4Cf\nk + 1 + 2\nk = 0 . . . \u221e.\nThe base-case k = 0 follows from (8) applied for the \ufb01rst step of the algorithm, using \u03b1 = \u03b1(0) =\n2\n0+2 = 1.\nNow considering k \u22651, the bound (8) gives us\nh(x(k+1)) \u2264\n(1 \u2212\u03b1(k))h(x(k)) + \u03b1(k)2Cf\n=\n(1 \u2212\n2\nk+2)h(x(k)) + (\n2\nk+2)2Cf\n\u2264\n(1 \u2212\n2\nk+2) 4Cf\nk+2 + (\n2\nk+2)2Cf ,\nwhere in the last inequality we have used the induction hypothesis for x(k). Simply rearranging\nthe terms gives\nh(x(k+1)) \u2264\n4Cf\nk+2 \u2212\n8Cf\n(k+2)2 +\n4Cf\n(k+2)2\n=\n4Cf\n\u0010\n1\nk+2 \u2212\n1\n(k+2)2\n\u0011\n=\n4Cf\nk+2\nk+2\u22121\nk+2\n\u2264\n4Cf\nk+2\nk+2\nk+3\n=\n4Cf\nk+3 ,\nwhich is our claimed bound for k \u22651.\nThe analogous claim for Algorithm 1 using the approximate linear primitive ApproxLinear()\nfollows from the exactly same argument, by replacing every occurrence of Cf in the proof here\nby 2Cf instead (compare to Lemma 4 also).\n3.2 Obtaining a Guaranteed Small Duality Gap\nFrom the above Theorem 3 on the convergence of Algorithm 1, we have obtained small primal\nerror. However, the optimum value f(x\u2217) is unknown in most practical applications, where we\nwould prefer to have an easily computable measure of the current approximation quality, for\nexample as a stopping criterion of our optimizer in the case that Cf is unknown. The duality\ngap g(x) that we de\ufb01ned in Section 2 satis\ufb01es these requirements, and always upper bounds the\nprimal error f(x) \u2212f(x\u2217).\n11\nBy a nice argument of Clarkson [Cla10, Theorem 2.3], the convergence on the simplex opti-\nmization domain can be extended to obtain the stronger property of guaranteed small duality\ngap g(x(k)) \u2264\u03b5, after at most O( 1\n\u03b5) many iterations. This stronger convergence result was not\nyet known in earlier papers of [FW56, DH78, Jon92, Pat93] and [Zha03]. Here we will generalize\nthe primal-dual convergence to arbitrary compact convex domains. The proof of our theorem\nbelow again relies on Lemma 4.\nTheorem 5 (Primal-Dual Convergence). Let K :=\nl 4Cf\n\u03b5\nm\n. We run the exact variant of Algo-\nrithm 1 for K iterations (recall that the step-sizes are given by \u03b1(k) :=\n2\nk+2, 0 \u2264k \u2264K),\nand then continue for another K + 1 iterations, now with the \ufb01xed step-size \u03b1(k) :=\n2\nK+2 for\nK \u2264k \u22642K + 1.\nThen the algorithm has an iterate x(\u02c6k), K \u2264\u02c6k \u22642K + 1, with duality gap bounded by\ng(x(\u02c6k)) \u2264\u03b5 .\nThe same statement holds for the approximate variant of Algorithm 1, when setting K :=\nl 8Cf\n\u03b5\nm\ninstead.\nProof. The proof follows the idea of [Cla10, Section 7].\nBy our previous Theorem 3 we already know that the primal error satis\ufb01es h(x(K)) = f(x(K))\u2212\nf(x\u2217) \u22644Cf\nK+2 after K iterations. In the subsequent K + 1 iterations, we will now suppose that\ng(x(k)) always stays larger than\n4Cf\nK+2. We will try to derive a contradiction to this assumption.\nPutting the assumption g(x(k)) > 4Cf\nK+2 into the step improvement bound given by Lemma 4,\nwe get that\nf(x(k+1)) \u2212f(x(k)) \u2264\n\u2212\u03b1(k)g(x(k)) + \u03b1(k)2Cf\n<\n\u2212\u03b1(k) 4Cf\nK+2 + \u03b1(k)2Cf\nholds for any step size \u03b1(k) \u2208(0, 1]. Now using the \ufb01xed step-size \u03b1(k) =\n2\nK+2 in the iterations\nk \u2265K of Algorithm 1, this reads as\nf(x(k+1)) \u2212f(x(k)) <\n\u2212\n2\nK+2\n4Cf\nK+2 +\n4\n(K+2)2 Cf\n=\n\u2212\n4Cf\n(K+2)2\nSumming up over the additional steps, we obtain\nf(x(2K+2)) \u2212f(x(K)) =\n2K+1\nX\nk=K\nf(x(k+1)) \u2212f(x(k))\n<\n\u2212(K + 2)\n4Cf\n(K+2)2 = \u22124Cf\nK+2 ,\nwhich together with our known primal approximation error f(x(K))\u2212f(x\u2217) \u22644Cf\nK+2 would result\nin f(x(2K+2)) \u2212f(x\u2217) < 0, a contradiction. Therefore there must exist \u02c6k, K \u2264\u02c6k \u22642K + 1, with\ng(x(\u02c6k)) \u22644Cf\nK+2 \u2264\u03b5.\nThe analysis for the approximate variant of Algorithm 1 follows using the analogous second\nbound from Lemma 4.\nFollowing [Cla10, Theorem 2.3], one can also prove a similar primal-dual convergence theorem\nfor the line-search algorithm variant that uses the optimal step-size in each iteration, as we will\ndiscuss in the next Section 3.3. This is somewhat expected as the line-search algorithm in each\nstep is at least as good as the \ufb01xed step-size variant we consider here.\n12\n3.3 Choosing the Optimal Step-Size by Line-Search\nAlternatively, instead of the \ufb01xed step-size \u03b1 =\n2\nk+2 in Algorithm 1, one can also \ufb01nd the optimal\n\u03b1 \u2208[0, 1] by line-search. This will not improve the theoretical convergence guarantee, but might\nstill be considered in practical applications if the best \u03b1 is easy to compute. Experimentally, we\nobserved that line-search can improve the numerical stability in particular if approximate step\ndirections are used, which we will discuss e.g. for semide\ufb01nite matrix completion problems in\nSection 11.5.\nFormally, given the chosen direction s, we then search for the \u03b1 of best improvement in the\nobjective function f, that is\n\u03b1 := arg min\n\u03b1\u2208[0,1]\nf\n\u0010\nx(k) + \u03b1(s \u2212x(k))\n\u0011\n.\n(9)\nThe resulting modi\ufb01ed version of Algorithm 1 is depicted again in Algorithm 2, and was precisely\nanalyzed in [Cla10] for the case of optimizing over the simplex.\nAlgorithm 2 Greedy on a Convex Set, using Line-Search\nInput: Convex function f, convex set D, target accuracy \u03b5\nOutput: \u03b5-approximate solution for problem (16)\nPick an arbitrary starting point x(0) \u2208D\nfor k = 0 . . . \u221edo\nCompute s := ExactLinear\n\u0000\u2207f(x(k)), D\n\u0001\n\u2014or\u2014\nCompute s := ApproxLinear\n\u0010\n\u2207f(x(k)), D, 2Cf\nk+2\n\u0011\nFind the optimal step-size \u03b1 := arg min\n\u03b1\u2208[0,1]\nf\n\u0010\nx(k) + \u03b1(s \u2212x(k))\n\u0011\nUpdate x(k+1) := x(k) + \u03b1(s \u2212x(k))\nend for\nIn many cases, we can solve this line-search analytically in a straightforward manner, by di\ufb00er-\nentiating the above expression with respect to \u03b1: Consider f\u03b1 := f\n\u0010\nx(k+1)\n(\u03b1)\n\u0011\n= f\n\u0000x(k) + \u03b1\n\u0000s \u2212x(k)\u0001\u0001\nand compute\n0 != \u2202\n\u2202\u03b1f\u03b1 =\nD\ns \u2212x(k), \u2207f\n\u0000x(k+1)\n(\u03b1)\n\u0001E\n.\n(10)\nIf this equation can be solved for \u03b1, then the optimal such \u03b1 can directly be used as the step-size\nin Algorithm 1, and the convergence guarantee of Theorem 3 still holds. This is because the\nimprovement in each step will be at least as large as if we were using the older (potentially\nsub-optimal) \ufb01xed choice of \u03b1 =\n2\nk+2. Here we have assumed that \u03b1(k) \u2208[0, 1] always holds,\nwhich can be done when using some caution, see also [Cla10].\nNote that the line-search can also be used for the approximate variant of Algorithm 1, where\nwe keep the accuracy for the internal primitive method ApproxLinear\n\u0000\u2207f(x(k)), D, \u03b5\u2032\u0001\n\ufb01xed\nto \u03b5\u2032 = \u03b1\ufb01xedCf = 2Cf\nk+2. Theorem 3 then holds as as in the original case.\n3.4 The Curvature Measure of a Convex Function\nFor the case of di\ufb00erentiable f over the space X = Rn, we recall the de\ufb01nition of the curvature\nconstant Cf with respect to the domain D \u2282Rn, as stated in (7),\nCf :=\nsup\nx,s\u2208D,\n\u03b1\u2208[0,1],\ny=x+\u03b1(s\u2212x)\n1\n\u03b12\n\u0000f(y) \u2212f(x) \u2212(y \u2212x)T \u2207f(x)\n\u0001\n.\n13\nAn overview of values of Cf for several classes of functions f over domains that are related to\nthe unit simplex can be found in [Cla10].\nAsymptotic Curvature.\nAs Algorithm 1 converges towards some optimal solution x\u2217, it also\nmakes sense to consider the asymptotic curvature C\u2217\nf, de\ufb01ned as\nC\u2217\nf :=\nsup\ns\u2208D,\n\u03b1\u2208[0,1],\ny=x\u2217+\u03b1(s\u2212x\u2217)\n1\n\u03b12\n\u0000f(y) \u2212f(x\u2217) \u2212(y \u2212x\u2217)T \u2207f(x\u2217)\n\u0001\n.\n(11)\nClearly C\u2217\nf \u2264Cf. As described in [Cla10, Section 4.4], we expect that as the algorithm converges\ntowards x\u2217, also the improvement bound as given by Lemma 4 should be determined by C\u2217\nf +o(1)\ninstead of Cf, resulting in a better convergence speed constant than given Theorem 3, at least\nfor large k. The class of strongly convex functions is an example for which the convergence of\nthe relevant constant towards C\u2217\nf is easy to see, since for these functions, convergence in the\nfunction value also imlies convergence in the domain, towards a unique point x\u2217, see e.g. [BV04,\nSection 9.1.2].\nRelating the Curvature to the Hessian Matrix.\nBefore we can compare the assumption of\nbounded curvature Cf to a Lipschitz assumption on the gradient of f, we will need to relate Cf\nto the Hessian matrix (matrix of all second derivatives) of f.\nHere the idea described in [Cla10, Section 4.1] is to make use of the degree-2 Taylor-expansion\nof our function f at the \ufb01xed point x, as a function of \u03b1, which is\nf(x + \u03b1(s \u2212x)) = f(x) + \u03b1(s \u2212x)T \u2207f(x) + \u03b12\n2 (s \u2212x)T \u22072f(z)(s \u2212x) ,\nwhere z is a point on the line-segment [x, y] \u2286D \u2282Rd between the two points x \u2208Rn and\ny = x + \u03b1(s \u2212x) \u2208Rn. To upper bound the curvature measure, we can now directly plug in\nthis expression for f(y) into the above de\ufb01nition of Cf, obtaining\nCf \u2264\nsup\nx,y\u2208D,\nz\u2208[x,y]\u2286D\n1\n2(y \u2212x)T \u22072f(z)(y \u2212x) .\n(12)\nFrom this bound, it follows that Cf is upper bounded by the largest eigenvalue of the Hessian\nmatrix of f, scaled with the domain\u2019s Euclidean diameter, or formally\nLemma 6. For any twice di\ufb00erentiable convex function f over a compact convex domain D, it\nholds that\nCf \u22641\n2 diam(D)2 \u00b7 sup\nz\u2208D\n\u03bbmax\n\u0000\u22072f(z)\n\u0001\n.\nNote that as f is convex, the Hessian matrix \u22072f(z) is positive semide\ufb01nite for all z, see\ne.g. [KZ05, Theorem 7.3.6].\nProof. Applying the Cauchy-Schwarz inequality to (12) for any x, y \u2208D (as in the de\ufb01nition of\nCf), we get\n(y \u2212x)T \u22072f(z)(y \u2212x) \u2264\n\u2225y \u2212x\u22252\n\r\r\u22072f(z)(y \u2212x)\n\r\r\n2\n\u2264\n\u2225y \u2212x\u22252\n2\n\r\r\u22072f(z)\n\r\r\nspec\n\u2264\ndiam(D)2 \u00b7 sup\nz\u2208D\n\u03bbmax\n\u0000\u22072f(z)\n\u0001\n.\nThe middle inequality follows from the variational characterization of the matrix spectral norm,\ni.e. \u2225A\u2225spec := supx\u0338=0\n\u2225Ax\u22252\n\u2225x\u22252 . Finally, in the last inequality we have used that by convexity of\nf, the Hessian matrix \u22072f(z) is PSD, so that its spectral norm is its largest eigenvalue.\n14\nNote that in the case of D being the unit simplex (see also the following Section 4), we have\nthat diam(\u2206n) =\n\u221a\n2, meaning the scaling factor disappears, i.e. Cf \u2264sup\nz\u2208\u2206n\n\u03bbmax\n\u0000\u22072f(z)\n\u0001\n.\nBounded Curvature vs. Lipschitz-Continuous Gradient.\nOur core assumption on the given\noptimization problems is that that the curvature Cf of the function is bounded over the domain.\nEquivalently, this means that the function does not deviate from its linear approximation by too\nmuch. Here we will explain that this assumption is in fact very close to the natural assumption\nthat the gradient \u2207f is Lipschitz-continuous, which is often assumed in classical convex opti-\nmization literature, where it is sometimes called Cf-strong smoothness, see e.g. [Nem05, KSST09]\n(or [d\u2019A08] if the gradient information is only approximate).\nLemma 7. Let f be a convex and twice di\ufb00erentiable function, and assume that the gradient \u2207f\nis Lipschitz-continuous over the domain D with Lipschitz-constant L > 0. Then\nCf \u22641\n2 diam(D)2L .\nProof. Having \u2225\u2207f(y) \u2212\u2207f(x)\u22252 \u2264L \u2225y \u2212x\u22252 \u2200x, y \u2208D by the Cauchy-Schwarz inequality\nimplies that (y \u2212x)T (\u2207f(y) \u2212\u2207f(x)) \u2264L \u2225y \u2212x\u22252\n2, so that\nf(y) \u2264f(x) + (y \u2212x)T \u2207f(x) + L\n2 \u2225y \u2212x\u22252\n2 .\n(13)\nIf f is twice di\ufb00erentiable, it can directly be seen that the above condition implies that\nL \u00b7 I \u2ab0\u22072f(z) holds for the Hessian of f, that is \u03bbmax\n\u0000\u22072f(z)\n\u0001\n\u2264L.\nTogether with our result from Lemma 6, the claim follows.\nThe above bound (13) which is implied by Lipschitz-continuous gradient means that the\nfunction is not \u201ccurved\u201d by more than L in some sense, which is an interesting property. In fact\nthis is exactly the opposite inequality compared to the property of strong convexity, which is\nan assumption on the function f that we do not impose here. Strong convexity on a compact\ndomain means that the function is always curved at least by some constant (as our L). We just\nnote that for strongly convex functions, \u201caccelerated\u201d algorithms with an even faster convergence\nof\n1\nk2 (meaning O( 1\n\u221a\u03b5) steps) do exist [Nes04, Nes07a].\n3.5 Optimizing over Convex Hulls\nIn the case that the optimization domain D is given as the convex hull of a (\ufb01nite or in\ufb01nite)\nsubset V \u2282X, i.e.\nD = conv(V ) ,\nthen it is particularly easy to solve the linear optimization subproblems as needed in our Algo-\nrithm 1. Recall that conv(V ) is de\ufb01ned as the set of all \ufb01nite convex combinations P\ni \u03b1ivi for\na \ufb01nite subset {v1, . . . , vk} \u2286V , while V can also be an in\ufb01nite set.\nLemma 8 (Linear Optimization over Convex Hulls). Let D = conv(V ) for any subset V \u2282X, and\nD compact. Then any linear function y 7\u2192\u27e8y, c\u27e9will attain its minimum and maximum over D\nat some \u201cvertex\u201d v \u2208V .\nProof. W.l.g. we will only show the case for the maximum. Let s \u2208D be a point attaining the\nlinear optimum \u27e8s, c\u27e9= maxy\u2208D\u27e8y, c\u27e9. Then by de\ufb01nition of D, we have that s = Pk\ni=1 \u03b1ivi,\nmeaning that s is the weighted average of some \ufb01nite set of \u201cvertices\u201d v1, . . . , vk \u2208V , with\n\u03b1i \u22650, P\ni \u03b1i = 1. By linearity of the inner product,\n15\n\u27e8s, c\u27e9=\n* k\nX\ni=1\n\u03b1ivi, c\n+\n=\nk\nX\ni=1\n\u03b1i\u27e8vi, c\u27e9,\nand therefore we must have that \u27e8vi, c\u27e9\u2265\u27e8s, c\u27e9for at least one of the indices i, meaning that\nvi \u2208V is also attaining the linear maximum.\nIn the following we will discuss several application where this simple fact will be useful to\nsolve the linearized subproblems ExactLinear() more e\ufb03ciently, as the set V is often much\neasier to describe than the full compact domain D.\nThe setting of convex optimization over a convex hull in a vector space was already studied\nby [Zha03]. There, each iteration of the optimizer consists of greedily selecting the point (or\n\u201cvertex\u201d) of V which promises best improvement. [Zha03] then gave a similar primal convergence\nguarantee as in our Theorem 3 (but no primal-dual convergence result on general convex hulls was\nknown so far, to the best of our knowledge). The above Lemma 8 in a sense explains the relation\nto our linearized internal problem. The main di\ufb00erence is that the algorithm of [Zha03] always\nevaluates the original non-linear function f at all vertices V , while our slightly more general\nframework only relies on the linear subproblem, and allows for arbitrary means to approximately\nsolve the subproblem.\n3.6 Randomized Variants, and Stochastic Optimization\nFor a variety of classes of our convex optimization problems, randomization can help to solve\nthe linearized subproblem more e\ufb03ciently. This idea is strongly related to online and stochastic\noptimization, see e.g. [Nes11], and also the popular stochastic gradient descent (SGD) tech-\nniques [Bot10].\nWe can also apply such cheaper randomized steps in our described framework, instead of\ndeterministically solving the internal linear problem in each iteration. Assumed that the user of\nour method is able to decompose the linearized problem in some arbitrary way using random-\nization, and if the randomization is such that the linearized problem will be solved \u201caccurately\nenough\u201d with some probability p > 0 in each iteration, then our convergence analysis still holds\nalso in this probabilistic setting as follows:\nFormally, we assume that we are given access to a randomized procedure RandomLinear (c, D, \u03b5\u2032),\nwhich returns a point s \u2208D such that \u27e8s, c\u27e9\u2264min\ny\u2208D \u27e8y, c\u27e9+ \u03b5\u2032 with probability at least p > 0. In\nother words, with a positive probability, RandomLinear() will behave like ApproxLinear().\nIn each iteration of the line-search variant of our algorithm (see Algorithm 2), we will now use\nthat randomized internal procedure instead. The expected improvement given by a step towards\ns = RandomLinear() is at least p times the amount given in Lemma 4. (Here we have used\nthat in the events of \u201cfailure\u201d of RandomLinear(), the objective function value will at least\nnot become worse, due to the use of line-search).\nIn other words if we perform 1\np times more iterations than required for the deterministic\nAlgorithm 1, then we have that the convergence by Theorem 3 also holds for the randomized\nvariant described here.\nStochastic Gradient Descent (SGD).\nA classical example is when the linearized problem is\ngiven by simply \ufb01nding the maximum over say n coordinates, as we will e.g. see in the following\nSections 4 and 5 for optimizing over the simplex, or over bounded \u21131-norm. In this case, by\nsampling uniformly at random, with probability 1\nn we will pick the correct coordinate, for which\nthe step improvement is as in the deterministic Algorithm 1. Therefore we have obtained the\nsame convergence guarantee as for the deterministic algorithm, but the necessary number of\nsteps is multiplied by n.\n16\nFor unconstrained convex optimization, the convergence of SGD and other related methods\nwas analyzed e.g.\nin [Nes11] and also the earlier paper [Nes07b, Section 6].\nAlso here, a\ncomparable slow-down was observed when using the cheaper randomized steps.\n3.7 Relation to Classical Convex Optimization\nRelation to Gradient Descent and Steepest Descent.\nThe internal linear optimizer in our\nAlgorithm 1 can also be interpreted in terms of descent directions. Recall that all vectors y\nthat have negative scalar product with the current gradient, i.e. \u27e8y, \u2207f(x)\u27e9< 0, are called\ndescent directions, see e.g. [BV04, Chapter 9.4]. Also observe that \u27e8y, \u2207f(x)\u27e9is the directional\nderivative of f in direction of y if y is of unit length. Our method therefore chooses the best\ndescent direction over the entire domain D, where the quality is measured as the best possible\nabsolute improvement as suggested by the linearization at the current point x. In any iteration,\nthis will crucially depend on the global shape of the domain D, even if the actual step-size \u03b1(k)\nmight be very small.\nThis crucially contrasts classical gradient descent techniques, which only use local information\nto determine the step-directions, facing the risk of walking out of the domain D and therefore\nrequiring projection steps after each iteration.\nRelation to Inaccurate and Missing Gradient Information.\nThe ability of our Algorithm 1\nto deal with only approximate internal linear optimizers as in ApproxLinear() is also related\nto existing methods that assume that gradient information is only available with noise, or in a\nstochastic or sampling setting.\nFor the case of optimizing smooth convex functions, [d\u2019A08] has used a similar measure of\nerror, namely that the linearization given by the \u201cnoisy\u201d version \u02dcdx of the gradient \u2207f(x) does\nnot di\ufb00er by more than say \u03b5\u2032 when measured over the entire domain D, or formally\n\f\f\f\u27e8y \u2212z, \u02dcdx\u27e9\u2212\u27e8y \u2212z, \u2207f(x)\u27e9\n\f\f\f \u2264\u03b5\u2032 ,\n\u2200x, y, z \u2208D .\nThis assumption is similar, but stronger than the additive approximation quality that we require\nin our above setting (we only need that the linearized optimal values are within \u03b5\u2032). Also, the\nalgorithm as well as the analysis in [d\u2019A08] are more complicated than the method proposed\nhere, due to the need of projections and proximal operators.\nWe have discussed the case where gradient information is available only in a stochastic oracle\n(e.g. such that the gradient is obtained in expectation) in the above Subsection 3.6. For an\noverview of related randomized methods in unconstrained convex optimization, we refer the\nreader to the recent work by [Nes11], which also applies when the gradient itself is not available\nand has to be estimated by oracle calls to the function alone.\nIf gradient information can be constructed in any way such that the linearized problem\nApproxLinear() can be solved to the desired additive error, then our above analysis of Algo-\nrithm 1 will still hold.\nRelation to Mirror Descent, Proximal Methods and Conjugate Functions.\nOur proposed\nmethod is related to mirror descent as well as proximal methods in convex optimization, but our\napproach is usually simpler. The mirror descent technique originates from e.g. [BT03, BTN05].\nFor a brief overview of proximal methods with applications to some of the classes of sparse\nconvex optimization problems as studied here, we refer to [BJMO11, Section 3].\nTo investigate the connection, we write flin|x(y) := f(x) + \u27e8y \u2212x, dx\u27e9for the linearization\ngiven by the (sub)gradient dx = \u2207f(x) at a \ufb01xed point x \u2208D. A variant of mirror descent,\n17\nsee e.g. [BTN05, Haz11] is to \ufb01nd the next iterate y as the point maximizing the Bregman\ndivergence\nf(y) \u2212f(x) \u2212\u27e8y \u2212x, dx\u27e9= f(y) \u2212flin|x(y)\n(14)\nrelative to the currently \ufb01xed old iterate x. This is the same task as maximizing the gap between\nthe function f(y) and its linear approximation at x, or equivalently we evaluate the conjugate\nfunction f\u2217(z) := sup\ny\u2208D\n\u27e8y, z\u27e9\u2212f(y) at z = dx. The de\ufb01nition of the conjugate dual is also known\nas Fenchel duality, see e.g. [BL06]. In [Nem05], the conjugate function is also called the Legendre\ntransformation.\nHowever in our approach, the inner task ExactLinear(dx, D) as well as ApproxLinear(dx, D, \u03b5\u2032)\nis a simpler linear problem. Namely, we directly minimize the linearization at the current point\nx, i.e. we maximize\n\u2212f(x) \u2212\u27e8y \u2212x, dx\u27e9= \u2212flin|x(y)\n(15)\nand then move towards an approximate maximizer y. In terms of Fenchel duality, this simpler\nlinear problem is the evaluation of the conjugate dual of the characteristic function of our domain\nD, i.e.\n1\u2217\nD(z) := sup\ny\u2208X\n\u27e8y, z\u27e9\u22121D(y) ,\nwhere this function is evaluated at the current subgradient z = dx. The characteristic function\n1D : X \u2192R of a set D \u2286X is de\ufb01ned as 1D(y) = 0 for y \u2208D and 1D(y) = \u221eotherwise.\nCompared to our algorithm, mirror descent schemes require a \u201cprojection\u201d step in each it-\neration, sometimes also called the proximal or mirror operator. This refers to minimizing the\nlinearization plus a strongly convex prox-function that punishes the distance to the starting\npoint. If the squared Euclidean norm is used, the mirror operator corresponds to the standard\nprojection back onto the domain D. Our method uses no such prox-function, and neither is\nthe zero-function a strongly convex one, as would be required for mirror descent to work. It is\nexpected that the computational cost per iteration of our method will in most application cases\nbe lower compared to mirror descent schemes.\nFor convex optimization over the simplex, which we will study in more details in the following\nSection 4, [BT03] have proposed a mirror descent algorithm, obtaining a convergence of f(x(k))\u2212\nf(x\u2217) \u2264\n\u221a\n2 ln n L\n\u221a\nk. This however is worse than the convergence of our methods as given by\nTheorem 3. Our convergence is independent of the dimension n, and goes with 1\nk instead of\n1\n\u221a\nk. Also the earlier paper by [BTMN01] only obtained a convergence of O\n\u0000 1\n\u221a\nk\n\u0001\nfor the case of\nLipschitz-continuous convex functions over the simplex.\nThe NERML optimizer by [BTN05] is a variant of mirror descent that memorizes several past\nlinearizations of the objective function, to improve the available knowledge about the current\nfunction landscape. It is an open research question if this idea could also help in our setting\nhere, or for stochastic gradient descent schemes [Bot10].\n4 Sparse Approximation over the Simplex\nAs a \ufb01rst application of the above general scheme, we will now consider optimization problems\nde\ufb01ned over the unit simplex, or in other words the non-negative vectors of \u21131-norm equal to\none. This will serve as a warm-up case before considering \u21131-norm regularized problems in the\nnext Section 5.\nOur approach here will allow us to understand the best achievable sparsity of approximate\nsolutions, as a function of the approximation quality, as already shown by [Cla10].\nIn particular, we will show that our main Algorithm 1 on page 8 and its analysis do lead to\nClarkson\u2019s approach [Cla10] for optimizing over the simplex. In this case, it was already known\n18\nthat sparsity O\n\u0000 1\n\u03b5\n\u0001\ncan always be achieved by applying Algorithm 1 to the simplex domain,\nsee [Cla10].\nWe will also show that this is indeed optimal, by providing an asymptotically\nmatching lower bound in Section 4.2. Also, our analysis holds even if the linear subproblems\nare only solved approximately, and allows arbitrary starting points, in contrast to [Cla10].\nHaving this e\ufb03cient algorithm giving sparsity O\n\u0000 1\n\u03b5\n\u0001\nis in particularly attractive in view of the\ncomputational complexity of vector cardinality minimization, which is known to be NP-hard, by\na reduction to Exact-Cover, see [Nat95]. Vector cardinality minimization here refers to \ufb01nding\nthe sparsest vector that is an \u03b5-approximation to some given convex minimization problem.\nFormally, \ufb01nding the sparsest x that satis\ufb01es \u2225Ax \u2212b\u22252 \u2264\u03b5 for given A, b and \u03b5.\nSet-Up.\nWe suppose that a basis has been chosen in the space X, so that we can assume\nX = Rn with the standard inner product \u27e8x, y\u27e9= xT y. Here we consider one special class of the\ngeneral optimization problems (1), namely we optimize over non-negative vectors that sum up\nto one, that is\nminimize\nx\u2208Rn\nf(x)\ns.t.\n\u2225x\u22251 = 1 ,\nx \u22650 .\n(16)\nIn the following we write \u2206n := {x \u2208Rn | x \u22650, \u2225x\u22251 = 1} for the unit simplex in Rn. As\nthe objective function f is now de\ufb01ned over Rn, all subgradients or gradients of f will also be\nrepresented by vectors in Rn in the following.\nNote that the alternative case of optimizing under an inequality constraint \u2225x\u22251 \u22641 instead\nof \u2225x\u22251 = 1 can easily be reduced to the above form (16) by introducing a new \u201cslack\u201d variable.\nFormally, one uses vectors \u02c6x = (x1, . . . , xn, xn+1) \u2208Rn+1 instead and optimizes the function\n\u02c6f(\u02c6x) := f(x1, . . . , xn) over the simplex domain \u2225\u02c6x\u22251 = 1, \u02c6x \u22650.\nCoresets.\nThe coreset concept was originally introduced in computational geometry by [BHPI02]\nand [APV02]. For point-set problems, the coreset idea refers to identifying a very small subset\n(coreset) of the points, such that the solution just on the coreset is guaranteed to be a good\napproximation to original problem. Here for general convex optimization, the role of the coreset\npoints is taken by the non-zero coordinates of our sought vector x instead. The coreset size then\ncorresponds to the sparsity of x.\nFormally if there exists an \u03b5-approximate solution x \u2208D \u2286Rn to the convex optimiza-\ntion problem (1), using only k many non-zero coordinates, then we say that the corresponding\ncoordinates of x form an \u03b5-coreset of size k for problem (1).\nIn other words, the following upper and lower bounds of O\n\u0000 1\n\u03b5\n\u0001\non the sparsity of approxi-\nmations for problem (16) are indeed matching upper and lower bounds on the coreset size for\nconvex optimization over the simplex, analogous to what we have found in the geometric problem\nsetting in [GJ09].\n4.1 Upper Bound: Sparse Greedy on the Simplex\nHere we will show how the general algorithm and its analysis from the previous Section 3 do\nin particular lead to Clarkson\u2019s approach [Cla10] for minimizing any convex function over the\nunit simplex. The algorithm follows directly from Algorithm 1, and will have a running time\nof O\n\u0000 1\n\u03b5\n\u0001\nmany gradient evaluations. We will crucially make use of the fact that every linear\nfunction attains its minimum at a vertex of the simplex \u2206n. Formally, for any vector c \u2208Rn,\nit holds that min\ns\u2208\u2206n sT c = min\ni\nci . This property is easy to verify in the special case here, but is\nalso a direct consequence of the small Lemma 8 which we have proven for general convex hulls,\n19\nif we accept that the unit simplex is the convex hull of the unit basis vectors. We have obtained\nthat the internal linearized primitive can be solved exactly by choosing\nExactLinear (c, \u2206n) := ei\nwith i = arg min\ni\nci .\nAlgorithm 3 Sparse Greedy on the Simplex\nInput: Convex function f, target accuracy \u03b5\nOutput: \u03b5-approximate solution for problem (16)\nSet x(0) := e1\nfor k = 0 . . . \u221edo\nCompute i := arg mini\n\u0000\u2207f(x(k))\n\u0001\ni\nLet \u03b1 :=\n2\nk+2\nUpdate x(k+1) := x(k) + \u03b1(ei \u2212x(k))\nend for\nObserve that in each iteration, this algorithm only introduces at most one new non-zero\ncoordinate, so that the sparsity of x(k) is always upper bounded by the number of steps k, plus\none, given that we start at a vertex. Since Algorithm 3 only moves in coordinate directions, it\ncan be seen as a variant of coordinate descent. The convergence result directly follows from the\ngeneral analysis we gave in the previous Section 3.\nTheorem 9 ([Cla10, Theorem 2.3], Convergence of Sparse Greedy on the Simplex). For each k \u22651,\nthe iterate x(k) of Algorithm 3 satis\ufb01es\nf(x(k)) \u2212f(x\u2217) \u22644Cf\nk + 2 .\nwhere x\u2217\u2208\u2206n is an optimal solution to problem (16).\nFurthermore, for any \u03b5 > 0, after at most 2\nl 4Cf\n\u03b5\nm\n+ 1 = O\n\u0000 1\n\u03b5\n\u0001\nmany steps3, it has an iterate\nx(k) of sparsity O\n\u0000 1\n\u03b5\n\u0001\n, satisfying g(x(k)) \u2264\u03b5.\nProof. This is a corollary of Theorem 3 and Theorem 5.\nDuality Gap.\nWe recall from Section 2 that the duality gap (5) at any point x \u2208\u2206n is easily\ncomputable from any subgradient, and in our case becomes\ng(x, dx) = xT dx \u2212min\ni\n(dx)i ,\nand\ng(x) = xT \u2207f(x) \u2212min\ni (\u2207f(x))i .\n(17)\nHere we have again used the observation that linear functions attain their minimum at a vertex\nof the domain, i.e, min\ns\u2208\u2206n sT c = min\ni\nci.\nApplications.\nMany practically relevant optimization problems do \ufb01t into our setting (16)\nhere, allowing the application of Algorithm 3. This includes linear classi\ufb01ers such as support\nvector machines (SVMs), see also [GJ09], as well as kernel learning (\ufb01nding the best convex\ncombination among a set of base kernels) [BLJ04]. Some other applications that directly \ufb01t\ninto our framework are \u21132-support vector regression (SVR), AdaBoost [Zha03], mean-variance\nanalysis in portfolio selection [Mar52], the smallest enclosing ball problem [BC07], and estimating\nmixtures of probability densities [Cla10]. For more applications we refer to [Cla10].\n3Note that in order for our Theorem 5 on the bounded duality gap to apply, the step-size in the second half\nof the iterations needs to be \ufb01xed to \u03b1(k) :=\n2\nK+2, see Section 3.2. This remark also applies to the later\napplications of our general Algorithm 1 that we discuss in the following. We already mentioned above that if\nline-search is used instead, then no such technicality is necessary, see also [Cla10].\n20\nLine-Search for the Best Step-Size.\nIn most applications it will be a straight-forward task to\n\ufb01nd the optimal step-size \u03b1 \u2208[0, 1] in each step instead, as described in Section 3.3.\nFor the special case of polytope distance and SVM problems, the resulting method then exactly\ncorresponds to Gilbert\u2019s geometric algorithm [Gil66], as shown in [GJ09]. Here the wording of\n\u201cline-search\u201d makes geometric sense in that we need to \ufb01nd the point s on a given line, such\nthat s is closest to the origin.\nAway Steps.\nBy performing more work with the currently non-zero coordinates, one can get\nthe sparsity even smaller. More precisely the number of non-zeros can be improved close to\n2Cf\n\u03b5\ninstead of 2\nl 4Cf\n\u03b5\nm\nas given by the above Theorem 9. The idea of away-steps introduced\nby [TY07] is to keep the total number of non-zero coordinates (i.e. the coreset size) \ufb01xed over\nall iterations, by removing the smallest non-zero coordinate from x after each adding step. For\nmore background we refer to [Cla10, Algorithm 9.1].\n4.2 \u2126( 1\n\u03b5) Lower Bound on the Sparsity\nWe will now show that sparsity O\n\u0000 1\n\u03b5\n\u0001\n, as obtained by the greedy algorithm we analyzed in the\nprevious section is indeed best possible, by providing a lower bound of \u2126\n\u0000 1\n\u03b5\n\u0001\n. In the language of\ncoresets, this means we will provide a matching lower bound on the size of coresets for convex\noptimization over the simplex. Together with the upper bound, this therefore completely char-\nacterizes the trade-o\ufb00between sparsity and approximation quality for the family of optimization\nproblems of the form (16). The same matching sparsity upper and lower bounds will also hold\nfor optimizing over the \u21131-ball instead, see Section 5.\nFor the following lower bound construction we consider the di\ufb00erentiable function f(x) :=\n\u2225x\u22252\n2 = xT x. This function has gradient \u2207f(x) = 2x. Its curvature constant is Cf = 2, which\nfollows directly from the de\ufb01nition (7), and the fact that here f(y) \u2212f(x) \u2212(y \u2212x)T \u2207f(x) =\nyT y \u2212xT x \u2212(y \u2212x)T 2x = \u2225x \u2212y\u22252\n2, so that Cf = supx,s\u2208\u2206n \u2225x \u2212s\u22252\n2 = diam(\u2206n)2 = 2.\nThe following lemmata show that the sparse greedy algorithm of [Cla10] from Section 4.1 is\nindeed optimal for the approximation quality (primal as well as dual error respectively), giving\nbest possible sparsity, up to a small multiplicative constant.\nLemma 10. For f(x) := \u2225x\u22252\n2, and 1 \u2264k \u2264n, it holds that\nmin\nx\u2208\u2206n\ncard(x)\u2264k\nf(x) = 1\nk.\nProof. We prove the inequality min\nx.. f(x) \u22651\nk by induction on k.\nCase k = 1 For any unit length vector x \u2208\u2206n having just a single non-zero entry, f(x) =\n\u2225x\u22252 = \u2225x\u22251 = 1.\nCase k > 1 For every x \u2208\u2206n of sparsity card(x) \u2264k, we can pick a coordinate i with xi \u0338= 0,\nand write x = (1 \u2212\u03b1)v + \u03b1ei as the sum of two orthogonal vectors v and a unit basis vector ei,\nwhere v \u2208\u2206n of sparsity \u2264k \u22121, vi = 0, and \u03b1 = xi. So for every x \u2208\u2206n of sparsity \u2264k, we\ntherefore get that\nf(x) = \u2225x\u22252\n2\n=\nxT x\n=\n((1 \u2212\u03b1)v + \u03b1ei)T ((1 \u2212\u03b1)v + \u03b1ei)\n=\n(1 \u2212\u03b1)2vT v + \u03b12\n\u2265\n(1 \u2212\u03b1)2\n1\nk\u22121 + \u03b12\n\u2265\nmin0\u2264\u03b2\u22641(1 \u2212\u03b2)2\n1\nk\u22121 + \u03b22\n=\n1\nk.\n21\nIn the \ufb01rst inequality we have applied the induction hypothesis for v \u2208\u2206n of sparsity \u2264k \u22121.\nEquality: The function value f(x) = 1\nk is obtained by setting k of the coordinates of x to 1\nk\neach.\nIn other words for any vector x of sparsity card(x) = k, the primal error f(x)\u2212f(x\u2217) is always\nlower bounded by 1\nk \u22121\nn. For the duality gap g(x), the lower bound is even slightly higher:\nLemma 11. For f(x) := \u2225x\u22252\n2, and any k \u2208N, k < n, it holds that\ng(x) \u22652\nk\n\u2200x \u2208\u2206n s.t. card(x) \u2264k.\nProof. g(x) = xT \u2207f(x) \u2212mini(\u2207f(x))i = 2(xT x \u2212mini xi). We now use mini xi = 0 because\ncard(x) < n, and that by Lemma 10 we have xT x = f(x) \u22651\nk.\nNote:\nWe could also consider the function f(x) := \u03b3 \u2225x\u22252\n2 instead, for some \u03b3 > 0. This f has\ncurvature constant Cf = 2\u03b3, and for this scaling, our above lower bound on the duality gap will\nalso scale linearly, giving Cf\nk .\n5 Sparse Approximation with Bounded \u21131-Norm\nIn this second application case, will apply the general greedy approach from Section 3 in order\nto understand the best achievable sparsity for convex optimization under bounded \u21131-norm, as\na function of the approximation quality. Here the situation is indeed extremely similar to the\nabove Section 4 of optimizing over the simplex, and the resulting algorithm will again have a\nrunning time of O\n\u0000 1\n\u03b5\n\u0001\nmany gradient evaluations.\nIt is known that the vector \u2225.\u22251-norm is the best convex approximation to the sparsity (car-\ndinality) of a vector, that is card(.). More precisely, the function \u2225.\u22251 is the convex envelope\nof the sparsity, meaning that it is the \u201clargest\u201d convex function that is upper bounded by the\nsparsity on the convex domain of vectors {x | \u2225x\u2225\u221e\u22641}. This can be seen by observing that\ncard(x) \u2265\n\u2225x\u22251\n\u2225x\u2225\u221e, see e.g. [RFP10]. We will discuss the analogous generalization to matrices in\nthe second part of this article, see Section 11, namely using the matrix nuclear norm as the\n\u201cbest\u201d convex approximation of the matrix rank.\nSet-Up.\nHere we consider one special class of the general optimization problem (1), namely\nproblems over vectors in Rn with bounded \u2225.\u22251-norm, that is\nminimize\nx\u2208Rn\nf(x)\ns.t.\n\u2225x\u22251 \u22641 .\n(18)\nWe write \u2666n := {x \u2208Rn | \u2225x\u22251 \u22641} for the \u21131-ball in Rn. Note that one can simply rescale\nthe function argument to allow for more general constraints \u2225x\u22251 \u2264t for t > 0. Again we have\nX = Rn with the standard inner product \u27e8x, y\u27e9= xT y, so that also the subgradients or gradients\nof f are represented as vectors in Rn.\nThe Linearized Problem.\nAs already in the simplex case, the subproblem of optimizing a linear\nfunction over the \u21131-ball is particularly easy to solve, allowing us to provide a fast implementation\nof the internal primitive procedure ExactLinear (c, \u2666n).\nNamely, it is again easy to see that every linear function attains its minimum/maximum at a\nvertex of the ball \u2666n, as we have already seen for general convex hulls in our earlier Lemma 8,\n22\nand here \u2666n = conv({\u00b1ei | i \u2208[n]}). Here this crucial observation can also alternatively be\ninterpreted as the known fact that the dual norm to the \u21131-norm is in fact the \u2113\u221e-norm, see also\nour earlier Observation 2.\nObservation 12. For any vector c \u2208Rn, it holds that\nei \u00b7 sign(ci) \u2208arg max\ny\u2208\u2666n\nyT c\nwhere i \u2208[n] is an index of a maximal coordinate of c measured in absolute value, or formally\ni \u2208arg maxj |cj|.\nUsing this observation for c = \u2212\u2207f(x) in our general Algorithm 1, we therefore directly\nobtain the following simple method for \u21131-regularized convex optimization, as depicted in the\nAlgorithm 4.\nAlgorithm 4 Sparse Greedy on the \u21131-Ball\nInput: Convex function f, target accuracy \u03b5\nOutput: \u03b5-approximate solution for problem (18)\nSet x(0) := 0\nfor k = 0 . . . \u221edo\nCompute i := arg maxi\n\f\f\u0000\u2207f(x(k))\n\u0001\ni\n\f\f,\nand let s := ei \u00b7 sign\n\u0000\u0000\u2212\u2207f(x(k))\n\u0001\ni\n\u0001\nLet \u03b1 :=\n2\nk+2\nUpdate x(k+1) := x(k) + \u03b1(s \u2212x(k))\nend for\nObserve that in each iteration, this algorithm only introduces at most one new non-zero\ncoordinate, so that the sparsity of x(k) is always upper bounded by the number of steps k.\nThis means that the method is again of coordinate-descent-type, as in the simplex case of the\nprevious Section 4.1. Its convergence analysis again directly follows from the general analysis\nfrom Section 3.\nTheorem 13 (Convergence of Sparse Greedy on the \u21131-Ball). For each k \u22651, the iterate x(k) of\nAlgorithm 4 satis\ufb01es\nf(x(k)) \u2212f(x\u2217) \u22644Cf\nk + 2 .\nwhere x\u2217\u2208\u2666n is an optimal solution to problem (18).\nFurthermore, for any \u03b5 > 0, after at most 2\nl 4Cf\n\u03b5\nm\n+ 1 = O\n\u0000 1\n\u03b5\n\u0001\nmany steps, it has an iterate\nx(k) of sparsity O\n\u0000 1\n\u03b5\n\u0001\n, satisfying g(x(k)) \u2264\u03b5.\nProof. This is a corollary of Theorem 3 and Theorem 5.\nThe Duality Gap, and Duality of the Norms.\nWe recall the de\ufb01nition of the duality gap (5)\ngiven by the linearization at any point x \u2208\u2666n, see Section 2. Thanks to our Observation 2, the\ncomputation of the duality gap in the case of the \u21131-ball here becomes extremely simple, and is\ngiven by the norm that is dual to the \u21131-norm, namely the \u2113\u221e-norm of the used subgradient,\ni.e.,\ng(x, dx) = \u2225dx\u2225\u221e+ xT dx,\nand\ng(x) = \u2225\u2207f(x)\u2225\u221e+ xT \u2207f(x) .\nAlternatively, the same expression can also be derived directly (without explicitly using duality\nof norms) by applying the Observation 12.\n23\nA Lower Bound on the Sparsity.\nThe lower bound of \u2126\n\u0000 1\n\u03b5\n\u0001\non the sparsity as proved in\nSection 4.2 for the simplex case in fact directly translates to the \u21131-ball as well. Instead of\nchoosing the objective function f as the distance to the origin (which is part of the \u21131-ball),\nwe consider the optimization problem\nmin\n\u2225x\u22251\u22641 f(x) := \u2225x \u2212r\u22252\n2 with respect to the \ufb01xed point\nr := ( 2\nn, . . . , 2\nn) \u2208Rn. This problem is of the form (18), and corresponds to optimizing the\nEuclidean distance to the point r given by mirroring the origin at the positive facet of the \u21131-\nball. Here by the \u201cpositive facet\u201d, we mean the hyperplane de\ufb01ned by the intersection of the\nboundary of the \u21131-ball with the positive orthant, which is exactly the unit simplex. Therefore,\nthe proof for the simplex case from Section 4.2 holds analogously for our setting here.\nWe have thus obtained that sparsity O\n\u0000 1\n\u03b5\n\u0001\nas obtained by the greedy Algorithm 4 is indeed\nbest possible for \u21131-regularized optimization problems of the form (18).\nUsing Barycentric Coordinates Instead.\nClarkson [Cla10, Theorem 4.2] already observed that\nAlgorithm 3 over the simplex \u2206n can be used to optimize a convex function f(y) over arbitrary\nconvex hulls, by just using barycentric coordinates y = Ax, x \u2208\u2206n, for A \u2208Rn\u00d7m being the\nmatrix containing all m vertices of the convex domain as its columns. Here however we saw that\nfor the \u21131-ball, the steps of the algorithm are even slightly simpler, as well as that the duality\ngap can be computed instantly from the \u2113\u221e-norm of the gradient.\nApplications.\nOur Algorithm 4 applies to arbitrary convex vector optimization problems with\nan \u2225.\u22251-norm regularization term, giving a guaranteed sparsity of O\n\u0000 1\n\u03b5\n\u0001\nfor all these applications.\nA classical example for problems of this class is given by the important \u2225.\u22251-regularized least\nsquares regression approach, i.e.\nmin\nx\u2208Rn \u2225Ax \u2212b\u22252\n2 + \u00b5 \u2225x\u22251\nfor a \ufb01xed matrix A \u2208Rm\u00d7n, a vector b \u2208Rm and a \ufb01xed regularization parameter \u00b5 >\n0.\nThe same problem is also known as basis pursuit de-noising in the compressed sensing\nliterature, which we will discuss more precisely in Section 5.1. The above formulation is in fact\nthe Lagrangian formulation of the corresponding constrained problem for \u2225x\u22251 \u2264t for some\n\ufb01xed parameter t corresponding to \u00b5. This equivalent formulation is also known as the Lasso\nproblem [Tib96] which is\nmin\nx\u2208Rn\n\u2225Ax \u2212b\u22252\n2\ns.t.\n\u2225x\u22251 \u2264t .\nThe above formulation is exactly a problem of our above form (18), namely\nmin\n\u02c6x\u2208\u2666n \u2225tA\u02c6x \u2212b\u22252\n2 ,\nif we rescale the argument x =: t\u02c6x so that \u2225\u02c6x\u22251 \u22641.\nAnother important application for our result is logistic regression with \u2225.\u22251-norm regulariza-\ntion, see e.g. [KKB07], which is also a convex optimization problem [Ren05]. The reduction to\nan \u21131-problem of our form (18) works exactly the same way as described here.\nRelated Work.\nAs we mentioned above, the optimization problem (18) \u2014 if f is the squared\nerror of a linear function \u2014 is very well studied as the Lasso approach, see e.g. [Tib96] and the\nreferences therein. For general objective functions f of bounded curvature, the above interesting\ntrade-o\ufb00between sparsity and the approximation quality was already investigated by [SSSZ10],\nand also by our earlier paper [GJ09] for the analogous case of optimizing over the simplex.\n24\n[SSSZ10, Theorem 2.4] shows a sparse convergence analogous to our above Theorem 13, for\nthe \u201cforward greedy selection\u201d algorithm on problem (18), but only for the case that f is\ndi\ufb00erentiable.\n5.1 Relation to Matching Pursuit and Basis Pursuit in Compressed Sensing\nBoth our sparse greedy Algorithm 3 for optimizing over the simplex and also Algorithm 4 for\ngeneral \u21131-problems are very similar to the technique of matching pursuit, which is one of the\nmost popular techniques in sparse recovery in the vector case [Tro04].\nSuppose we want to recover a sparse signal vector x \u2208Rn from a noisy measurement vec-\ntor Ax = y \u2208Rm.\nFor a given dictionary matrix A \u2208Rm\u00d7n, matching pursuit iteratively\nchooses the dictionary element Ai \u2208Rm that has the highest inner product with the cur-\nrent residual, and therefore reduces the representation error f(x) = \u2225Ax \u2212y\u22252\n2 by the largest\namount. This choice of coordinate i = arg maxj AT\nj (Ax \u2212y) exactly corresponds4 to the choice\nof i := arg minj\n\u0000\u2207f(x(k))\n\u0001\nj in Algorithm 3.\nAnother variant of matching pursuit, called orthogonal matching pursuit (OMP) [Tro04,\nTG07], includes an extra orthogonalization step, and is closer related to the coreset algorithms\nthat optimize over the all existing set of non-zero coordinates before adding a new one, see\ne.g. [Cla10, Algorithm 8.2], or the analogous \u201cfully corrective\u201d variant of [SSSZ10]. If y = Ax,\nwith x sparse and the columns of A su\ufb03ciently incoherent, then OMP recovers the sparsest\nrepresentation for the given y [Tro04].\nThe paper [Zha11] recently proposed another algorithm that generalizes OMP, comes with a\nguarantee on correct sparse recovery, and also corresponds to \u201ccompletely optimize within each\ncoreset\u201d. The method uses the same choice of the new coordinate i := arg maxj\n\f\f\f\n\u0000\u2207f(x(k))\n\u0001\nj\n\f\f\f as\nin our Algorithm 4. However the analysis of [Zha11] requires the not only bounded curvature as\nin our case, but also needs strong convexity of the objective function (which then also appears\nas a multiplicative factor in the number of iterations needed). Our Algorithm 4 as well as the\nearlier method by [Zha03] are simpler to implement, and have a lower complexity per iteration,\nas we do not need to optimize over several currently non-zero coordinates, but only change one\ncoordinate by a \ufb01xed amount in each iteration.\nOur Algorithm 4 for general \u21131-regularized problems also applies to solving the so called basis\npursuit problem [CDS98, FNW07] and [BV04, Section 6.5.4], which is minx\u2208Rn \u2225x\u22251 s.t. Ax = y.\nNote that this is in fact just the constrained variant of the corresponding \u201crobust\u201d \u21131-regularized\nleast squares regression problem\nmin\nx\u2208Rn \u2225Ax \u2212y\u22252\n2 + \u00b5 \u2225x\u22251 ,\nwhich is the equivalent trade-o\ufb00variant of our problem of the form (18). [FNW07] propose a\ntraditional gradient descent technique for solving the above least squares problem, but do not\ngive a convergence analysis.\nSolution path algorithms with approximation guarantees for related problems (obtaining solu-\ntions for all values of the tradeo\ufb00parameter \u00b5) have been studied in [GJL10, GJL12a, GJL12b],\nand the author\u2019s PhD thesis [Jag11], building on the same duality gap concept we introduced\nin Section 2.\n4The objective function f(x) := \u2225Ax \u2212y\u22252\n2 can be written as f(x) = (Ax\u2212y)T (Ax\u2212y) = xT AT Ax\u22122yT Ax\u2212yT y,\nso its gradient is \u2207f(x) = 2AT Ax \u22122AT y = 2AT (Ax \u2212y) \u2208Rn.\n25\n6 Optimization with Bounded \u2113\u221e-Norm\nApplying our above general optimization framework for the special case of the domain being the\n\u2225.\u2225\u221e-norm unit ball, we again obtain a very simple greedy algorithm. The running time will\nagain correspond to O\n\u0000 1\n\u03b5\n\u0001\nmany gradient evaluations. Formally, we consider problems of the\nform\nminimize\nx\u2208Rn\nf(x)\ns.t.\n\u2225x\u2225\u221e\u22641 .\n(19)\nWe denote the feasible set, i.e. the \u2225.\u2225\u221e-norm unit ball, by \u25a1n := {x \u2208Rn | \u2225x\u2225\u221e\u22641}. For\nthis set, it will again be very simple to implement the internal primitive operation of optimizing a\nlinear function over the same domain. The following crucial observation allows us to implement\nExactLinear (c, \u25a1n) in a very simple way. This can also alternatively be interpreted as the\nknown fact that the dual-norm to the \u2113\u221e-norm is the \u21131-norm, which also explains why the greedy\nalgorithm we will obtain here is very similar to the \u21131-version from the previous Section 5.\nObservation 14. For any vector c \u2208Rn, it holds that\nsc \u2208arg max\ny\u2208\u25a1n\nyT c\nwhere sc \u2208Rn is the sign-vector of c, de\ufb01ned by the sign of each individual coordinate, i.e.\n(sc)i = sign(ci) \u2208{\u22121, 1}.\nUsing this observation for c = \u2212dx in our general Algorithm 1, we directly obtain the following\nsimple method for optimization over a box-domain \u25a1n, as depicted in Algorithm 5.\nAlgorithm 5 Sparse Greedy on the Cube\nInput: Convex function f, target accuracy \u03b5\nOutput: \u03b5-approximate solution for problem (19)\nSet x(0) := 0\nfor k = 0 . . . \u221edo\nCompute the sign-vector s of \u2207f(x(k)), such that\nsi = sign\n\u0000\u0000\u2212\u2207f(x(k))\n\u0001\ni\n\u0001\n,\ni = 1..n\nLet \u03b1 :=\n2\nk+2\nUpdate x(k+1) := x(k) + \u03b1(s \u2212x(k))\nend for\nThe convergence analysis again directly follows from the general analysis from Section 3.\nTheorem 15. For each k \u22651, the iterate x(k) of Algorithm 5 satis\ufb01es\nf(x(k)) \u2212f(x\u2217) \u22644Cf\nk + 2 .\nwhere x\u2217\u2208\u25a1n is an optimal solution to problem (19).\nFurthermore, for any \u03b5 > 0, after at most 2\nl 4Cf\n\u03b5\nm\n+ 1 = O\n\u0000 1\n\u03b5\n\u0001\nmany steps, it has an iterate\nx(k) with g(x(k)) \u2264\u03b5.\nProof. This is a corollary of Theorem 3 and Theorem 5.\n26\nThe Duality Gap, and Duality of the Norms.\nWe recall the de\ufb01nition of the duality gap (5)\ngiven by the linearization at any point x \u2208\u25a1n, see Section 2. Thanks to our Observation 2, the\ncomputation of the duality gap in the case of the \u2113\u221e-ball here becomes extremely simple, and\nis given by the norm that is dual to the \u2113\u221e-norm, namely the \u21131-norm of the used subgradient,\ni.e.,\ng(x, dx) = \u2225dx\u22251 + xT dx,\nand\ng(x) = \u2225\u2207f(x)\u22251 + xT \u2207f(x) .\nAlternatively, the same expression can also be derived directly (without explicitly using duality\nof norms) by applying the Observation 14.\nSparsity and Compact Representations.\nThe analogue of \u201csparsity\u201d as in Sections 4 and 5 in\nthe context of our Algorithm 5 means that we can describe the obtained approximate solution x\nas a convex combination of few (i.e. O( 1\n\u03b5) many) cube vertices. This does not imply that x has\nfew non-zero coordinates, but that we have a compact representation given by only O( 1\n\u03b5) many\nbinary n-vectors indicating the corresponding cube vertices, of which x is a convex combination.\nApplications.\nAny convex problem under coordinate-wise upper and lower constraints can be\ntransformed to the form (19) by re-scaling the optimization argument. A speci\ufb01c interesting\napplication was given by [MR11], who have demonstrated that integer linear programs can\nbe relaxed to convex problems of the above form, such that the solutions coincide with high\nprobability under some mild additional assumptions.\nUsing Barycentric Coordinates Instead.\nClarkson [Cla10, Theorem 4.2] already observed that\nAlgorithm 3 over the simplex \u2206n can be used to optimize a convex function f(y) over arbitrary\nconvex hulls, by just using barycentric coordinates y = Ax, x \u2208\u2206n, for A \u2208Rn\u00d7m being the\nmatrix containing all m vertices of the convex domain as its columns. Here however we saw\nthat for the unit box, the steps of the algorithm are much simpler, as well as that the duality\ngap can be computed instantly, without having to explicitly deal with the exponentially many\nvertices (here m = 2n) of the cube.\n7 Semide\ufb01nite Optimization with Bounded Trace\nWe will now apply the greedy approach from the previous Section 3 to semide\ufb01nite optimiza-\ntion problems, for the case of bounded trace. The main paradigm in this section will be to\nunderstand the best achievable low-rank property of approximate solutions as a function of the\napproximation quality.\nIn particular, we will show that our general Algorithm 1 and its analysis do lead to Hazan\u2019s\nmethod for convex semide\ufb01nite optimization with bounded trace, as given by [Haz08]. Hazan\u2019s\nalgorithm can also be used as a simple solver for general SDPs. [Haz08] has already shown that\nguaranteed \u03b5-approximations of rank O\n\u0000 1\n\u03b5\n\u0001\ncan always be found. Here we will also show that\nthis is indeed optimal, by providing an asymptotically matching lower bound in Section 7.4.\nFurthermore, we \ufb01x some problems in the original analysis of [Haz08], and require only a weaker\napproximation quality for the internal linearized primitive problems.\nWe also propose two\nimprovement variants for the method in Section 7.3.\nLater in Section 11, we will discuss the application of these algorithms for nuclear norm and\nmax-norm optimization problems, which have many important applications in practice, such as\ndimensionality reduction, low-rank recovery as well as matrix completion and factorizations.\n27\nWe now consider convex optimization problems of the form (1) over the space X = Sn\u00d7n of\nsymmetric matrices, equipped with the standard Frobenius inner product \u27e8X, Y \u27e9= X \u2022 Y . It is\nleft to the choice of the reader to identify the symmetric matrices either with Rn2 and consider\nfunctions with f(X) = f(XT ), or only \u201cusing\u201d the variables in the upper right (or lower left)\ntriangle, corresponding to Rn(n+1)/2. In any case, the subgradients or gradients of our objective\nfunction f need to be available in the same representation (same choice of basis for the vector\nspace X).\nFormally, we consider the following special case of the general optimization problems (1), i.e.,\nminimize\nX\u2208Sn\u00d7n\nf(X)\ns.t.\nTr(X) = 1 ,\nX \u2ab00\n(20)\nWe will write S := {X \u2208Sn\u00d7n | X \u2ab00, Tr(X) = 1} for the feasible set, that is the PSD matrices\nof unit trace. The set S is sometimes called the Spectahedron, and can be seen as a natural\ngeneralization of the unit simplex to symmetric matrices. By the Cholesky factorization, it can\nbe seen that the Spectahedron is the convex hull of all rank-1 matrices of unit trace (i.e. the\nmatrices of the form vvT for a unit vector v \u2208Rn, \u2225v\u22252 = 1).\n7.1 Low-Rank Semide\ufb01nite Optimization with Bounded Trace: The O( 1\n\u03b5)\nAlgorithm by Hazan\nApplying our general greedy Algorithm 1 that we studied in Section 3 to the above semidef-\ninite optimization problem, we directly obtain the following Algorithm 6, which is Hazan\u2019s\nmethod [Haz08, GM11].\nNote that this is now a \ufb01rst application of Algorithm 1 where the internal linearized problem\nApproxLinear() is not trivial to solve, contrasting the applications for vector optimization\nproblems we studied above.\nThe algorithm here obtains low-rank solutions (sum of rank-1\nmatrices) to any convex optimization problem of the form (20). More precisely, it guarantees \u03b5-\nsmall duality gap after at most O\n\u0000 1\n\u03b5\n\u0001\niterations, where each iteration only involves the calculation\nof a single approximate eigenvector of a matrix M \u2208Sn\u00d7n. We will see that in practice for\nexample Lanczos\u2019 or the power method can be used as the internal optimizer ApproxLinear().\nAlgorithm 6 Hazan\u2019s Algorithm / Sparse Greedy for Bounded Trace\nInput: Convex function f with curvature constant Cf, target accuracy \u03b5\nOutput: \u03b5-approximate solution for problem (20)\nSet X(0) := vvT for an arbitrary unit length vector v \u2208Rn.\nfor k = 0 . . . \u221edo\nLet \u03b1 :=\n2\nk+2\nCompute v := v(k) = ApproxEV\n\u0000\u2207f(X(k)), \u03b1Cf\n\u0001\nUpdate X(k+1) := X(k) + \u03b1(vvT \u2212X(k))\nend for\nHere ApproxEV(A, \u03b5\u2032) is a subroutine that delivers an approximate smallest eigenvector (the\neigenvector corresponding to the smallest eigenvalue) to a matrix A with the desired accuracy\n\u03b5\u2032 > 0. More precisely, it must return a unit length vector v such that vT Av \u2264\u03bbmin(A) + \u03b5\u2032.\nNote that as our convex function f takes a symmetric matrix X as an argument, its gradients\n\u2207f(X) are given as symmetric matrices as well.\nIf we want to understand this proposed Algorithm 6 as an instance of the general convex\noptimization Algorithm 1, we just need to explain why the largest eigenvector should indeed\n28\nbe a solution to the internal linearized problem ApproxLinear(), as required in Algorithm 1.\nFormally, we have to show that v := ApproxEV(A, \u03b5\u2032) does approximate the linearized problem,\nthat is\nvvT \u2022 A \u2264min\nY \u2208S Y \u2022 A + \u03b5\u2032\nfor the choice of v := ApproxEV(A, \u03b5\u2032), and any matrix A \u2208Sn\u00d7n.\nThis fact is formalized in Lemma 16 below, and will be the crucial property enabling the fast\nimplementation of Algorithm 6.\nAlternatively, if exact eigenvector computations are available, we can also implement the exact\nvariant of Algorithm 1 using ExactLinear(), thereby halving the total number of iterations.\nObserve that an approximate eigenvector here is signi\ufb01cantly easier to compute than a pro-\njection onto the feasible set S. If we were to \ufb01nd the \u2225.\u2225Fro-closest PSD matrix to a given\nsymmetric matrix A, we would have to compute a complete eigenvector decomposition of A, and\nonly keeping those corresponding to positive eigenvalues, which is computationally expensive.\nBy contrast, a single approximate smallest eigenvector computation as in ApproxEV(A, \u03b5\u2032)\ncan be done in near linear time in the number of non-zero entries of A. We will discuss the\nimplementation of ApproxEV(A, \u03b5\u2032) in more detail further below.\nSparsity becomes Low Rank.\nAs the rank-1 matrices are indeed the \u201cvertices\u201d of the domain S\nas shown in Lemma 16 below, our Algorithm 6 can be therefore seen as a matrix generalization\nof the sparse greedy approximation algorithm of [Cla10] for vectors in the unit simplex, see\nSection 4, which has seen many successful applications. Here sparsity just gets replaced by\nlow rank. By the analysis of the general algorithm in Theorem 3, we already know that we\nobtain \u03b5-approximate solutions for any convex optimization problem (20) over the spectahedron\nS. Because each iterate X(k) is represented as a sum (convex combination) of k many rank-1\nmatrices vvT , it follows that X(k) is of rank at most k. Therefore, the resulting \u03b5-approximations\nare of low rank, i.e. rank O\n\u0000 1\n\u03b5\n\u0001\n.\nFor large-scale applications where 1\n\u03b5 \u226an, the representation of X(k) as a sum of rank-1\nmatrices is much more e\ufb03cient than storing an entire matrix X(k) \u2208Sn\u00d7n. Later in Section 11.5\n(or see also [JS10]) we will demonstrate that Algorithm 6 can readily be applied to practical\nproblems for n \u2265106 on an ordinary computer, well exceeding the possibilities of interior point\nmethods.\n[Haz08] already observed that the same Algorithm 6 with a well-crafted function f can also be\nused to approximately solve arbitrary SDPs with bounded trace, which we will brie\ufb02y explain\nin Section 7.2.\nLinearization, the Duality Gap, and Duality of the Norms.\nHere we will prove that the general\nduality gap (5) can be calculated very e\ufb03ciently for the domain being the spectahedron S. From\nthe following Lemma 16, we obtain that\ng(X) =X \u2022 \u2207f(X) + \u03bbmax(\u2212\u2207f(X))\n=X \u2022 \u2207f(X) \u2212\u03bbmin(\u2207f(X)) .\n(21)\nAs predicted by our Observation 2 on formulating the duality gap, we have again obtained the\ndual norm to the norm that determines the domain D. It can be seen that over the space of\nsymmetric matrices, the dual norm of the matrix trace-norm (also known as the nuclear norm)\nis given by the spectral norm, i.e. the largest eigenvalue. To see this, we refer the reader to the\nlater Section 11.2 on the properties of the nuclear norm and its dual characterization.\nThe following Lemma 16 shows that any linear function attains its minimum and maximum\nat a \u201cvertex\u201d of the Spectahedron S, as we have already proved for the case of general convex\nhulls in Lemma 8.\n29\nLemma 16. The spectahedron is the convex hull of the rank-1 matrices,\nS = conv(\n\b\nvvT \f\f v \u2208Rn, \u2225v\u22252 = 1\n\t\n) .\nFurthermore, for any symmetric matrix A \u2208Sn\u00d7n, it holds that\nmax\nX\u2208S A \u2022 X = \u03bbmax(A) .\nProof. Clearly, it holds that vvT \u2208S for any unit length vector v \u2208Rn, as Tr(vvT ) = \u2225v\u22252\n2.\nTo prove the other inclusion, we consider an arbitrary matrix X \u2208S, and let X = U T U be its\nCholesky factorization. We let \u03b1i be the squared norms of the rows of U, and let ui be the row\nvectors of U, scaled to unit length. From the observation 1 = Tr(X) = Tr(U T U) = Tr(UUT ) =\nP\ni \u03b1i it follows that any X \u2208S can be written as a convex combination of at most n many\nrank-1 matrices X = Pn\ni=1 \u03b1iuiuT\ni with unit vectors ui \u2208Rn, proving the \ufb01rst part of the claim.\nFurthermore, this implies that we can write\nmax\nX\u2208S A \u2022 X = max\nui,\u03b1i A \u2022\nn\nX\ni=1\n\u03b1iuiuT\ni = max\nui,\u03b1i\nn\nX\ni=1\n\u03b1i(A \u2022 uiuT\ni ),\nwhere the maximization maxui,\u03b1i is taken over unit vectors ui \u2208Rn, \u2225ui\u2225= 1, for 1 \u2264i \u2264n,\nand real coe\ufb03cients \u03b1i \u22650, with Pn\ni=1 \u03b1i = 1. Therefore\nmax\nX\u2208S A \u2022 X\n= max\nui,\u03b1i\nn\nX\ni=1\n\u03b1i(A \u2022 uiuT\ni )\n=\nmax\nv\u2208Rn,\u2225v\u2225=1 A \u2022 vvT\n=\nmax\nv\u2208Rn,\u2225v\u2225=1 vT Av\n= \u03bbmax (A) ,\nwhere the last equality is the variational characterization of the largest eigenvalue.\nCurvature.\nWe know that the constant in the actual running time for a given convex function\nf : Sd\u00d7d \u2192R is given by the curvature constant Cf as given in (7), which for the domain S\nbecomes\nCf :=\nsup\nX,V \u2208S, \u03b1\u2208[0,1],\nY =X+\u03b1(V \u2212X)\n1\n\u03b12\n\u0000f(Y ) \u2212f(X) + (Y \u2212X) \u2022 \u2207f(X)\n\u0001\n.\n(22)\nConvergence.\nWe can now see the convergence analysis for Algorithm 6 following directly as\na corollary of our simple analysis of the general framework in Section 3. The following theorem\nproves that O\n\u0000 1\n\u03b5\n\u0001\nmany iterations are su\ufb03cient to obtain primal error \u2264\u03b5. This result was\nalready known in [Haz08, Theorem 1], or [GM11, Chapter 5] where some corrections to the\noriginal paper were made.\nTheorem 17. For each k \u22651, the iterate X(k) of Algorithm 6 satis\ufb01es\nf(X(k)) \u2212f(X\u2217) \u22648Cf\nk + 2 .\nwhere X\u2217\u2208S is an optimal solution to problem (20).\nFurthermore, for any \u03b5 > 0, after at most 2\nl 8Cf\n\u03b5\nm\n+ 1 = O\n\u0000 1\n\u03b5\n\u0001\nmany steps, it has an iterate\nX(k) of rank O\n\u0000 1\n\u03b5\n\u0001\n, satisfying g(X(k)) \u2264\u03b5.\nProof. This is a corollary of Theorem 3 and Theorem 5.\n30\nApproximating the Largest Eigenvector.\nApproximating the smallest eigenvector of a sym-\nmetric matrix \u2207f(X) (which is the largest eigenvector of \u2212\u2207f(X)) is a well-studied problem in\nthe literature. We will see in the following that the internal procedure ApproxEV(M, \u03b5\u2032), can be\nperformed in near-linear time, when measured in the number of non-zero entries of the gradient\nmatrix \u2207f(X). This will follow from the analysis of [KW92] for the power method or Lanczos\u2019\nalgorithm, both with a random start vector. A similar statement has been used in [AHK05,\nLemma 2].\nTheorem 18. Let M \u2208Sn\u00d7n be a positive semide\ufb01nite matrix. Then with high probability, both\ni) O\n\u0010\nlog(n)\n\u03b3\n\u0011\niterations of the power method, or\nii) O\n\u0010\nlog(n)\n\u221a\u03b3\n\u0011\niterations of Lanczos\u2019 algorithm\nwill produce a unit vector x such that xT Mx\n\u03bb1(M) \u22651 \u2212\u03b3.\nProof. The statement for the power method follows from [KW92, Theorem 3.1(a)], and for\nLanczos\u2019 algorithm by [KW92, Theorem 3.2(a)].\nThe only remaining obstacle to use this result for our internal procedure ApproxEV(M, \u03b5\u2032) is\nthat our gradient matrix M = \u2212\u2207f(X) is usually not PSD. However, this can easily be \ufb01xed by\nadding a large enough constant t to the diagonal, i.e.\n\u02c6\nM := M + tI, or in other words shifting\nthe spectrum of M so that the eigenvalues satisfy \u03bbi( \u02c6\nM) = \u03bbi(M) + t \u22650 \u2200i. The choice of\nt = \u2212\u03bbmin(M) is good enough for this to hold.\nNow by setting \u03b3 := \u03b5\u2032\nL \u2264\n\u03b5\u2032\n\u03bbmax( \u02c6\nM) for some upper bound L \u2265\u03bbmax( \u02c6\nM) = \u03bbmax(M)\u2212\u03bbmin(M),\nthis implies that our internal procedure ApproxEV(M, \u03b5\u2032) can be implemented by performing\nO\n\u0010\nlog(n)\n\u221a\nL\n\u221a\n\u03b5\u2032\n\u0011\nmany Lanczos steps (that is matrix-vector multiplications). Note that a simple\nchoice for L is given by the spectral norm of M, since 2 \u2225M\u2225spec = 2 maxi \u03bbi(M) \u2265\u03bbmax(M) \u2212\n\u03bbmin(M). We state the implication for our algorithm in the following corollary.\nTheorem 19. For M \u2208Sn\u00d7n, and \u03b5\u2032 > 0, the procedure ApproxEV(M, \u03b5\u2032) requires a total of\nO\n\u0010\nNf\nlog(n)\n\u221a\nL\n\u221a\n\u03b5\u2032\n\u0011\nmany arithmetic operations, with high probability, by using Lanczos\u2019 algorithm.\nHere Nf is the number of non-zero entries in M, which in the setting of Algorithm 6 is the\ngradient matrix \u2212\u2207f(X). We have also assumed that the spectral norm of M is bounded by L.\nSince we already know the number of necessary \u201couter\u201d iterations of Algorithm 6, by Theo-\nrem 17, we conclude with the following analysis of the total running time. Here we again use\nthat the required internal accuracy is given by \u03b5\u2032 = \u03b1Cf \u2264\u03b5Cf.\nCorollary 20. When using Lanczos\u2019 algorithm for the approximate eigenvector procedure ApproxEV(., .),\nthen Algorithm 6 provides an \u03b5-approximate solution in O\n\u0000 1\n\u03b5\n\u0001\niterations, requiring a total of\n\u02dcO\n\u0010 Nf\n\u03b51.5\n\u0011\narithmetic operations (with high probability).\nHere the notation \u02dcO(.) suppresses the logarithmic factor in n. This corollary improves the\noriginal analysis of [Haz08] by a factor of\n1\n\u221a\u03b5, since [Haz08, Algorithm 1] as well as the proof\nof [Haz08, Theorem 1] used an internal accuracy bound of \u03b5\u2032 = O\n\u0000 1\nk2\n\u0001\ninstead of the su\ufb03cient\nchoice of \u03b5\u2032 = O\n\u0000 1\nk\n\u0001\nas in our general analysis here.\n31\nRepresentation of the Estimate X in the Algorithm.\nThe above result on the total running\ntime assumes the following: After having obtained an approximate eigenvector v, the rank-1\nupdate X(k+1) := (1 \u2212\u03b1)X(k) + \u03b1vvT can be performed e\ufb03ciently, or more precisely in time\nNf. In the worst case, when a fully dense matrix X is needed, this update cost is Nf = n2.\nHowever, there are many interesting applications where the function f depends only on a small\nfraction of the entries of X, so that Nf \u226an2. Here, a prominent example is matrix completion\nfor recommender systems. In this case, only those Nf many entries of X will be stored and\na\ufb00ected by the rank-1 update, see also our Section 11.5.\nAn alternative representation of X consists of the low-rank factorization, given by the v-\nvectors of each of the O\n\u0000 1\n\u03b5\n\u0001\nmany update steps, using a smaller memory of size O\n\u0000 n\n\u03b5\n\u0001\n. However,\ncomputing the gradient \u2207f(X) from this representation of X might require more time then.\n7.2 Solving Arbitrary SDPs\nIn [Haz08] it was established that Algorithm 6 can also be used to approximately solve arbitrary\nsemide\ufb01nite programs (SDPs) in feasibility form, i.e.,\n\ufb01nd X s.t.\nAi \u2022 X \u2264bi\ni = 1..m\nX \u2ab00 .\n(23)\nAlso every classical SDP with a linear objective function\nmaximize\nX\nC \u2022 X\ns.t.\nAi \u2022 X \u2264bi\ni = 1..m\u2032\nX \u2ab00 .\n(24)\ncan be turned into a feasibility SDP (23) by \u201cguessing\u201d the optimal value C \u2022 X by binary\nsearch [AHK05, Haz08].\nHere we will therefore assume that we are given a feasibility SDP of the form\n(23) by its\nconstraints Ai \u2022 X \u2264bi, which we want to solve for X. We can represent the constraints of (23)\nin a smooth optimization objective instead, using the soft-max function\nf(X) := 1\n\u03c3 log\n m\nX\ni=1\ne\u03c3(Ai\u2022X\u2212bi)\n!\n.\n(25)\nSuppose that the original SDP was feasible, then after O\n\u0000 1\n\u03b5\n\u0001\nmany iterations of Algorithm 6,\nfor a suitable choice of \u03c3, we have obtained X such that f(X) \u2264\u03b5, which implies that all\nconstraints are violated by at most \u03b5. This means that Ai \u2022 X \u2264bi + \u03b5, or in other words we\nsay that X is \u03b5-feasible [Haz08, GM11]. It turns out the best choice for the parameter \u03c3 is\nlog m\n\u03b5\n, and the curvature constant Cf(\u03c3) for this function is bounded by \u03c3 \u00b7 maxi \u03bbmax(Ai)2. The\ntotal number of necessary approximate eigenvector computations is therefore in O\n\u0010\nlog m\n\u03b52\n\u0011\n. In\nfact, Algorithm 6 when applied to the function (25) is very similar to the multiplicative weights\nmethod [AHK05]. Note that the soft-max function (25) is convex in X, see also [Ren05]. For a\nslightly more detailed exhibition of this approach of using Algorithm 6 to approximately solving\nSDPs, we refer the reader to the book of [GM11].\nNote that this technique of introducing the soft-max function is closely related to smoothing\ntechniques in the optimization literature [Nem04, BB09], where the soft-max function is intro-\nduced to get a smooth approximation to the largest eigenvalue function. The transformation to\na smooth saddle-point problem suggested by [BB09] is more complicated than the simple notion\nof \u03b5-feasibility suggested here, and will lead to a comparable computational complexity in total.\n32\n7.3 Two Improved Variants of Algorithm 6\nChoosing the Optimal \u03b1 by Line-Search.\nAs we mentioned already for the general algorithm\nfor convex optimization in Section 3, the optimal \u03b1 in Algorithm 6, i.e. the \u03b1 \u2208[0, 1] of best\nimprovement in the objective function f can be found by line-search.\nIn particular for matrix completion problems, which we will discuss in more details in Sec-\ntion 11.5, the widely used squared error is easy to analyze in this respect: If the optimization\nfunction is given by f(X) = 1\n2\nP\nij\u2208P (Xij \u2212Yij)2, where P is the set of observed positions of the\nmatrix Y , then the optimality condition (10) from Section 3.3 is equivalent to\n\u03b1 =\nP\nij\u2208P (Xij \u2212yij)(Xij \u2212vivj)\nP\nij\u2208P (Xij \u2212vivj)2\n.\n(26)\nHere X = X(k), and v is the approximate eigenvector v(k) used in step k of Algorithm 6. The\nabove expression is computable very e\ufb03ciently compared to the eigenvector approximation task.\nImmediate Feedback in the Power Method.\nAs a second improvement, we propose a heuristic\nto speed up the eigenvector computation, i.e. the internal procedure ApproxEV (\u2207f(X), \u03b5\u2032).\nInstead of multiplying the current candidate vector vk with the gradient matrix \u2207f(X) in each\npower iteration, we multiply with 1\n2\n\u0000\u2207f(X) + \u2207f(X)\n\u0001\n, or in other words the average between\nthe current gradient and the gradient at the new candidate location X =\n\u00001\u22121\nk\n\u0001\nX(k)+ 1\nkv(k)v(k)T .\nTherefore, we immediately take into account the e\ufb00ect of the new feature vector v(k). This\nheuristic (which unfortunately does not fall into our current theoretical guarantee) is inspired by\nstochastic gradient descent as in Simon Funk\u2019s method, which we will describe in Section 11.5.4.\nIn practical experiments, this proposed slight modi\ufb01cation will result in a signi\ufb01cant speed-up\nof Algorithm 6, as we will observe e.g. for matrix completion problems in Section 11.5.\n7.4 \u2126( 1\n\u03b5) Lower Bound on the Rank\nAnalogous to the vector case discussed in Section 4.2, we can also show that the rank of O\n\u0000 1\n\u03b5\n\u0001\n,\nas obtained by the greedy Algorithm 6 is indeed optimal, by providing a lower bound of \u2126\n\u0000 1\n\u03b5\n\u0001\n.\nIn other words we can now exactly characterize the trade-o\ufb00between rank and approximation\nquality, for convex optimization over the spectahedron.\nFor the lower bound construction, we consider the convex function f(X) := \u2225X\u22252\nFro = X \u2022 X\nover the symmetric matrices Sn\u00d7n. This function has gradient \u2207f(X) = 2X. We will later see\nthat its curvature constant is Cf = 2.\nThe following lemmata show that the above sparse SDP Algorithm 6 is optimal for the ap-\nproximation quality (primal as well as dual error respectively), giving lowest possible rank, up\nto a small multiplicative constant.\nLemma 21. For f(X) := \u2225X\u22252\nFro, and 1 \u2264k \u2264n, it holds that\nmin\nX\u2208S\nRk(X)\u2264k\nf(X) = 1\nk.\nWe will see that this claim can be reduced to the analogous Lemma 10 for the vector case,\nby the standard technique of diagonalizing a symmetric matrix. (This idea was suggested by\nElad Hazan). Alternatively, an explicit (but slightly longer) proof without requiring the spectral\ntheorem can be obtained by using the Cholesky-decomposition together with induction on k.\n33\nProof. We observe that the objective function \u2225.\u22252\nFro, the trace, as well as the property of being\npositive semide\ufb01nite, are all invariant under orthogonal transformations (or in other words under\nthe choice of basis).\nBy the standard spectral theorem, for any symmetric matrix X of Rk(X) \u2264k, there exists an\northogonal transformation mapping X to a diagonal matrix X\u2032 with at most k non-zero entries\non the diagonal (being eigenvalues of X by the way). For diagonal matrices, the \u2225.\u2225Fro matrix\nnorm coincides with the \u2225.\u22252 vector norm of the diagonal of the matrix. Finally by applying the\nvector case Lemma 10 for the diagonal of X\u2032, we obtain that f(X) = f(X\u2032) \u22651\nk.\nTo see that the minimum can indeed be attained, one again chooses the \u201cuniform\u201d example\nX := 1\nkIk \u2208S, being the matrix consisting of k non-zero entries (of 1\nk each) on the diagonal.\nThis gives f(X) = 1\nk.\nRecall from Section 7.1 that for convex problems of the form (20) over the Spectahedron, the\nduality gap is the non-negative value g(X) := f(X)\u2212\u03c9(X) = X \u2022\u2207f(X)\u2212\u03bbmin(\u2207f(X)). Also,\nby weak duality as given in Lemma 1, this value is always an upper bound for the primal error,\nthat is f(X) \u2212f(X\u2217) \u2264g(X) \u2200X.\nLemma 22. For f(X) := \u2225X\u22252\nFro, and any k \u2208N, k < n, it holds that\ng(X) \u22651\nk\n\u2200X \u2208S s.t. Rk(X) \u2264k.\nProof. g(X) = \u03bbmax(\u2212\u2207f(X))+X\u2022\u2207f(X) = \u2212\u03bbmin(X)+X\u20222X. We now use that \u03bbmin(X) = 0\nfor all symmetric PSD matrices X that are not of full rank n, and that by Lemma 21, we have\nX \u2022 X = Tr(XT X) = f(X) \u22651\nk.\nThe Curvature.\nWe will compute the curvature Cf of our function f(X) := X \u2022 X, showing\nthat Cf = 2 in this case. Using the de\ufb01nition (7), and the fact that here\nf(Y ) \u2212f(X) \u2212(Y \u2212X) \u2022 \u2207f(X)\n=\nY \u2022 Y \u2212X \u2022 X \u2212(Y \u2212X) \u2022 2X\n=\n\u2225X \u2212Y \u22252\nFro ,\none obtains that Cf = supX,Y \u2208S \u2225X \u2212Y \u22252\nFro = diamFro(S)2 = 2.\nFinally the following\nLemma 23 shows that the diameter is indeed 2.\nLemma 23 (Diameter of the Spectahedron).\ndiamFro(S)2 = 2 .\nProof. Using the fact that the spectahedron S is the convex hull of the rank-1 matrices of unit\ntrace, see Lemma 16, we know that the diameter must be attained at two vertices of S, i.e.\nu, v \u2208Rn with \u2225u\u22252 = \u2225v\u22252 = 1, and\n\r\rvvT \u2212uuT \r\r2\nFro\n=\nvvT \u2022 vvT + uuT \u2022 uuT \u22122vvT \u2022 uuT\n=\nvT vvT v + uT uuT u \u22122uT vvT u\n=\n\u2225v\u22254 + \u2225u\u22254 \u22122(uT v)2 .\nClearly, this quantity is maximized if u and v are orthogonal.\nNote:\nWe could also study f(X) := \u03b3 \u2225X\u22252\nFro instead, for some \u03b3 > 0. This function has\ncurvature constant Cf = 2\u03b3, and for this scaling our above lower bounds will also just scale\nlinearly, giving Cf\nk instead of 1\nk.\n34\n8 Semide\ufb01nite Optimization with \u2113\u221e-Bounded Diagonal\nHere we specialize our general Algorithm 1 to semide\ufb01nite optimization problems where all\ndiagonal entries are individually constrained. This will result in a new optimization method\nthat can also be applied to max-norm optimization problems, which we will discuss in more\ndetail in Section 11. As in the previous Section 7, here we also consider matrix optimization\nproblems over the space X = Sn\u00d7n of symmetric matrices, equipped with the standard Frobenius\ninner product \u27e8X, Y \u27e9= X \u2022 Y .\nFormally, we consider the following special case of the general optimization problems (1), i.e.\nminimize\nX\u2208Sn\u00d7n\nf(X)\ns.t.\nXii \u22641\n\u2200i,\nX \u2ab00 .\n(27)\nWe will write \u229e:= {X \u2208Sn\u00d7n | X \u2ab00, Xii \u22641 \u2200i} for the feasible set in this case, that is the\nPSD matrices whose diagonal entries are all upper bounded by one. This class of optimization\nproblems has become widely known for the linear objective case when f(X) = A \u2022 X, if A being\nthe Laplacian matrix of a graph. In this case, one obtains the standard SDP relaxation of the\nMax-Cut problem [GW95], which we will brie\ufb02y discuss below. Also, this optimization domain\nis strongly related to the matrix max-norm, which we study in more detail in Section 11.3.\nOur general optimization Algorithm 1 directly applies to this specialized class of optimization\nproblems as well, in which case it becomes the method depicted in the following Algorithm 7.\nAlgorithm 7 Sparse Greedy for Max-Norm Bounded Semide\ufb01nite Optimization\nInput: Convex function f with curvature Cf, target accuracy \u03b5\nOutput: \u03b5-approximate solution for problem (27)\nSet X(0) := vvT for an arbitrary unit length vector v \u2208Rn.\nfor k = 0 . . . \u221edo\nLet \u03b1 :=\n2\nk+2\nCompute S := ApproxLinear\n\u0000\u2207f(X(k)), \u229e, \u03b1Cf\n\u0001\nUpdate X(k+1) := X(k) + \u03b1(S \u2212X(k))\nend for\nThe Linearized Problem.\nHere, the internal subproblem ApproxLinear() of approximately\nminimizing a linear function over the domain \u229eof PSD matrices is a non-trivial task. Every\ncall of ApproxLinear(A, \u229e, \u03b5\u2032) in fact means that we have to solve a semide\ufb01nite program\nminY \u2208\u229eY \u2022 A for a given matrix A, or in other words\nminimize\nY\nY \u2022 A\ns.t.\nYii \u22641\n\u2200i,\nY \u2ab00\n(28)\nup to an additive approximation error of \u03b5\u2032 = \u03b1Cf.\nRelation to Max-Cut.\nIn [AHK05, Kal07], the same linear problem is denoted by (MaxQP).\nIn the special case that A is chosen as the Laplacian matrix of a graph, then the above SDP\nis widely known as the standard SDP relaxation of the Max-Cut problem [GW95] (not to be\nconfused with the combinatorial Max-Cut problem itself, which is known to be NP-hard). In\nfact the original relaxation uses equality constraints Yii = 1 on the diagonal instead, but for any\n35\nmatrix A of positive diagonal entries (such as e.g. a graph Laplacian), this condition follows\nautomatically in the maximization variant of (28), see [KL96], or also [GM11, Kal07] for more\nbackground.\nDuality and Duality of Norms.\nIn Section 11.3 we will see that the above quantity (28) that\ndetermines both the step in our greedy Algorithm 7, but also the duality gap, is in fact the norm\nof A that is dual to the matrix max-norm.\nFor optimization problems of the form (27), it can again be shown that the poor-man\u2019s duality\ngiven by the linearization (see also Section 2) indeed coincides with classical Wolfe-duality from\nthe optimization literature.\nFortunately, it was shown by [AHK05] that also this linearized convex optimization prob-\nlem (28) \u2014 and therefore also our internal procedure ApproxLinear(.) \u2014 can be solved rela-\ntively e\ufb03ciently, if the matrix A (i.e. \u2207f(X) in our case) is sparse.5\nTheorem 24. The algorithm of [AHK05] delivers an additive \u03b5\u2032-approximation to the linearized\nproblem (28) in time\n\u02dcO\n\u0012n1.5L2.5\n\u03b5\u20322.5\nNA\n\u0013\nwhere the constant L > 0 is an upper bound on the maximum value of Y \u2022 A over Y \u2208\u229e, and\nNA is the number of non-zeros in A.\nProof. The results of [AHK05, Theorem 3] and [Kal07, Theorem 33] give a running time of order\n\u02dcO\n\u0010\nn1.5\n\u03b52.5 \u00b7 min\nn\nN, n1.5\n\u03b5\u03b1\u2217\no\u0011\nto obtain a multiplicative (1 \u2212\u03b5)-approximation, where \u03b1\u2217is the value\nof an optimal solution. Formally we obtain S \u2208\u229ewith S \u2022 A \u2265(1 \u2212\u03b5)\u03b1\u2217. In other words by\nusing an accuracy of \u03b5 := \u03b5\u2032\n\u03b1\u2217, we obtain an additive \u03b5\u2032-approximation to (28).\nHere the notation \u02dcO(.) again suppresses poly-logarithmic factors in n, and N is the number\nof non-zero entries of the matrix A. Note that analogous to the approximate eigenvector com-\nputation for Hazan\u2019s Algorithm 6, we need the assumption that the linear function given by\nY \u2022 \u2207f(X) is bounded over the domain Y \u2208\u229e. However this is a reasonable assumption, as our\nfunction has bounded curvature Cf (corresponding to \u2207f(X) being Lipschitz-continuous over\nthe domain \u229e), and the diameter of \u229eis bounded.\nThe reason we need an absolute approximation quality lies in the analysis of Algorithm 1,\neven if it would feel much more natural to work with relative approximation quality in many\ncases.\nConvergence.\nThe convergence result for the general Algorithm 1 directly gives us the analysis\nfor the specialized algorithm here. Note that the curvature over the domain \u229ehere is given by\nCf :=\nsup\nX,V \u2208\u229e, \u03b1\u2208[0,1],\nY =X+\u03b1(V \u2212X)\n1\n\u03b12\n\u0000f(Y ) \u2212f(X) + (Y \u2212X) \u2022 \u2207f(X)\n\u0001\n.\n(29)\nTheorem 25. For each k \u22651, the iterate X(k) of Algorithm 7 satis\ufb01es\nf(X(k)) \u2212f(X\u2217) \u22648Cf\nk + 2 .\n5Also, Kale in [Kal07, Theorem 14] has shown that this problem can be solved very e\ufb03ciently if the matrix\nA = \u2212\u2207f(X) is sparse.\nNamely if A is the Laplacian matrix of a weighted graph, then a multiplicative\n\u03b5-approximation to (28) can be computed in time \u02dcO( \u22062\nd2 NA) time, where NA is the number of non-zero entries\nof the matrix A. Here \u2206is the maximum entry on the diagonal of A, and d is the average value on the\ndiagonal.\n36\nwhere X\u2217\u2208S is an optimal solution to problem (27).\nFurthermore, after at most 2\nl 8Cf\n\u03b5\nm\n+ 1 = O( 1\n\u03b5) many steps, it has an iterate X(k) with\ng(X(k)) \u2264\u03b5.\nProof. This is a corollary of Theorem 3 and Theorem 5.\nApplications.\nThe new algorithm can be used to solve arbitrary max-norm constrained convex\noptimization problems, such as max-norm regularized matrix completion problems, which we\nwill study in Section 11.\n9 Sparse Semide\ufb01nite Optimization\nAnother interesting optimization domain among the semide\ufb01nite matrices is given by the ma-\ntrices with only one non-zero o\ufb00-diagonal entry. Here we specialize our general Algorithm 1 to\nconvex optimization over the convex hull given by such matrices. Our algorithm will therefore\nobtain \u03b5-approximate solutions given by only O\n\u0000 1\n\u03b5\n\u0001\nsuch sparse matrices, or in other words\nsolutions of sparsity O\n\u0000 1\n\u03b5\n\u0001\n.\nWhy bother?\nThe same sparse matrices are also used in the graph sparsi\ufb01cation approach\nby [BSS09]6. Furthermore, sparse solutions to convex matrix optimization problems have gained\ninterest in dimensionality reduction, as in sparse PCA, see [ZdG10] for an overview.\nSetup.\nFormally, here we again use the standard Frobenius inner product \u27e8X, Y \u27e9= X \u2022 Y\nover the symmetric matrices Sn\u00d7n, and consider the sparse PSD matrices given by P (ij) :=\n(ei + ej)(ei + ej)T =\n \u00b7 \u00b7 \u00b7 \u00b7 \u00b7\n\u00b7 1 \u00b7 1 \u00b7\n\u00b7 \u00b7 \u00b7 \u00b7 \u00b7\n\u00b7 1 \u00b7 1 \u00b7\n\u00b7 \u00b7 \u00b7 \u00b7 \u00b7\n!\n, for any \ufb01xed pair of indices i, j \u2208[n], i \u0338= j. In other words\nP (ij)\nuv\n= 1 for u \u2208{i, j}, v \u2208{i, j}, and zero everywhere else. We also consider the analogous\n\u201cnegative\u201d counterparts of such matrices, namely N(kl) := (ei \u2212ej)(ei \u2212ej)T =\n \u00b7\n\u00b7\n\u00b7\n\u00b7\n\u00b7\n\u00b7 1 \u00b7 \u22121 \u00b7\n\u00b7\n\u00b7\n\u00b7\n\u00b7\n\u00b7\n\u00b7 \u22121 \u00b7 1 \u00b7\n\u00b7\n\u00b7\n\u00b7\n\u00b7\n\u00b7\n!\n,\ni.e. N(ij)\nuv\n= \u22121 for the two o\ufb00-diagonal entries (u, v) \u2208{(i, j), (j, i)}, and N(ij)\nuv\n= 1 for the two\ndiagonal entries (u, v) \u2208{(i, i), (j, j)}, and zero everywhere else.\nAnalogously to the two previous applications of our method to semide\ufb01nite optimization, we\nnow optimize a convex function, i.e.\nminimize\nX\u2208S4sparse\nf(X)\n(30)\nover the domain\nD = S4\nsparse := conv\n\uf8eb\n\uf8ed[\nij\nP (ij) \u222a\n[\nij\nN(ij)\n\uf8f6\n\uf8f8.\nOptimizing over Sparse Matrices, and Solving the Linearization.\nApplying our general Al-\ngorithm 1 to this class of problems (30) becomes very simple, as the linear primitive problem\nExactLinear\n\u0000DX, S4\nsparse\n\u0001\nfor any \ufb01xed matrix DX \u2208Sn\u00d7n is easily solved over S4\nsparse. From\nour simple Lemma 8 on linear functions over convex hulls, we know that this linear minimum\nis attained by the single sparse matrix P (ij) or N(ij) that maximizes the inner product with\n6The theoretical result of [BSS09] guarantees that all eigenvalues of the resulting sparse matrix (corresponding\nto the Laplacian of a sparse graph) do not di\ufb00er too much from their counterparts in the original graph.\n37\n\u2212DX. The optimal pair of indices (k, l) can be found by a linear pass through the gradient\nDX = \u2207f(X). This means that the linearized problem is much easier to solve than in the above\ntwo Sections 7 and 8. Altogether, Algorithm 1 will build approximate solutions X(k), each of\nwhich is a convex combination of k of the atomic matrices P (ij) or N(ij), as formalized in the\nfollowing theorem:\nTheorem 26. Let n \u22652 and let X(0) := P (12) be the starting point. Then for each k \u22651, the\niterate X(k) of Algorithm 1 has at most 4(k + 1) non-zero entries, and satis\ufb01es\nf(X(k)) \u2212f(X\u2217) \u22648Cf\nk + 2 .\nwhere X\u2217\u2208S4\nsparse is an optimal solution to problem (30).\nFurthermore, for any \u03b5 > 0, after at most 2\nl 8Cf\n\u03b5\nm\n+ 1 = O\n\u0000 1\n\u03b5\n\u0001\nmany steps, it has an iterate\nX(k) of only O\n\u0000 1\n\u03b5\n\u0001\nmany non-zero entries, satisfying g(X(k)) \u2264\u03b5.\nProof. This is a corollary of Theorem 3 and Theorem 5. The sparsity claim follows from our\nobservation that the step directions given by ExactLinear\n\u0000\u2207f(X), S4\nsparse\n\u0001\nare always given\nby one of the sparse matrices P (ij) or N(ij).\nOptimizing over Non-Negative Matrix Factorizations.\nWe also consider the slight variant\nof (30), namely optimizing only over one of the two types of matrices as the domain D, i.e. only\ncombinations of positive P (ij) or only of negative N(ij). This means that the domain is given\nby D = S4+\nsparse := conv\n\u0010S\nij P (ij)\u0011\nor D = S4\u2212\nsparse := conv\n\u0010S\nij N(ij)\u0011\n. The above analysis for\nAlgorithm 1 holds in exactly the same way. Now for S4+\nsparse, each step direction s = s(k) used\nby the algorithm is given by s = P (ij) = (ei + ej)(ei + ej)T for some i, j, and so we have that\neach of the approximations X(k) is a sum of k many positive rank-1 factors of this form. In\nother words in each step k, X(k) = LRT is a product of two (entry-wise) non-negative matrices\nof at most k columns each, i.e. L \u2208Rn\u00d7k and Rn\u00d7k. Consequently, our algorithm provides\nsolutions that are non-negative matrix factorizations, which is a successful technique in matrix\ncompletion problems from recommender systems, see e.g. [Wu07].\nRelation to Bounded Trace and Diagonally Dominant Matrices.\nObserve the matrices in\nS4\nsparse form a subset of the bounded trace PSD matrices S that we studied in the previous\nSection 7, since every matrix P (ij) or N(ij) is PSD and has trace equal two. Furthermore, we\nobserve that all matrices X \u2208S4\nsparse are diagonally dominant, meaning that\n|Xii| \u2265\nX\nj\u0338=i\n|Xij|\n\u2200i \u2208[n]\nIn the case that we restrict to using only one of the two types of matrices S4+\nsparse or S4\u2212\nsparse as\nthe domain, then we have that equality |Xii| = P\nj\u0338=i |Xij| always holds, since this equality is\npreserved under taking convex combinations, and holds for the atomic matrices P (ij) and N(ij).\nThe Curvature.\nThe above reasoning also implies that the curvature Cf for problems of the\nform (30) is upper bounded by the curvature in the spectahedron-case as given in (22), since\nS4+\nsparse \u2286S4\nsparse \u22862 \u00b7 S.\n38\nApplications and Future Research.\nComputationally, the approach here looks very attractive,\nas the cost of a \u201csparse\u201d step here is much cheaper than an approximate eigenvector computation\nwhich is needed in the bounded trace case as explained in Section 7.\nAlso, it will be interesting to see how a regularization by constraining to a scaled domain\nS4\nsparse or S4+\nsparse will perform in practical machine learning applications as for example dimen-\nsionality reduction, compared to nuclear norm regularization that we will discuss in the following\nChapter 11.\nIt also remains to investigate further on whether we can approximate general bounded trace\nsemide\ufb01nite problems of the form (20) by using only sparse matrices.\n10 Submodular Optimization\nFor a \ufb01nite ground set S, a real valued function de\ufb01ned on all subsets of S, is called submodular,\nif\ng(X \u2229Y ) + g(X \u222aY ) \u2264g(X) + g(Y )\n\u2200X, Y \u2286S\nFor any given submodular function g with g(\u2205) = 0, the paper [Lov83, Section 3] introduces a\ncorresponding convex set in R|S|, called the submodular polyhedron (or also Lovasz polyhedron),\nPg :=\n(\nx \u2208R|S|\n\f\f\f\f\f\nX\ni\u2208T\nxi \u2264g(T) \u2200T \u2286S\n)\n.\nWe would now like to study convex optimization problems over such domains, which become\ncompact convex sets if we intersect with the positive orthant, i.e. D := Pg \u2229R|S|\n\u22650.\nNicely for our optimization framework, [Lov83, Section 3] already showed that there is a\nsimple greedy algorithm which optimizes any linear function over the domain Pg, i.e. it solves\nmax\nx\u2208Pg cT x, or in other words it exactly solves our internal problem ExactLinear (c, Pg).\nLovasz [Lov83] already demonstrated how to use this kind of linear optimization over Pg to\nsolve submodular minimization problems. It remains to investigate if there are interesting ap-\nplications for the wider class of more general convex (non-linear) functions f over such domains,\nas addressed by our Algorithm 1.\n11 Optimization with the Nuclear and Max-Norm\nMatrix optimization problems with a nuclear norm or max-norm regularization, such as e.g.\nlow norm matrix factorizations, have seen many applications recently, ranging from low-rank\nrecovery, dimensionality reduction, to recommender systems. We propose two new \ufb01rst-order\napproximation methods building upon two of the simple semide\ufb01nite optimizers we have studied\nabove, that is the approximate SDP solver of [Haz08] from Section 7 on one hand, and our\nbounded diagonal optimizer from Section 8 on the other hand.\nThe algorithms come with\nstrong convergence guarantees.\nIn contrast to existing methods, our nuclear norm optimizer does not need any Cholesky\nor singular value decompositions internally, and provides guaranteed approximations that are\nsimultaneously of low rank. The method is free of tuning parameters, and easy to parallelize.\n11.1 Introduction\nHere we consider convex optimization problems over matrices, which come with a regularization\non either the nuclear norm or the max-norm of the optimization variable.\n39\nConvex optimization with the nuclear norm has become a very successful technique in vari-\nous machine learning, computer vision and compressed sensing areas such as low-rank recovery\n[FHB01, CR09, CT10], dimensionality reduction (such as robust principal component analy-\nsis [CLMW11]), and also recommender systems and matrix completion. Here matrix factor-\nizations [SRJ04, KBV09] \u2014 regularized by the nuclear or max-norm \u2014 have gained a lot of\nattention with the recently ended Net\ufb02ix Prize competition. Many more applications of simi-\nlar optimization problems can be found among dimensionality reduction, matrix classi\ufb01cation,\nmulti-task learning, spectral clustering and others. The success of these methods is fueled by\nthe property of the nuclear norm being a natural convex relaxation of the rank, allowing the use\nof scalable convex optimization techniques.\nBased on the semide\ufb01nite optimization methods that we have presented in the above Sections 7\nand 8, we propose two new, yet simple, \ufb01rst-order algorithms for nuclear norm as well as max-\nnorm regularized convex optimization.\nFor the nuclear norm case, our proposed method builds upon the \ufb01rst-order scheme for semidef-\ninite optimization by [Haz08], which we have investigated in Section 7.1. This approach allows us\nto signi\ufb01cantly reduce the computational complexity per iteration, and therefore scale to much\nlarger datasets: While existing methods need an entire and exact singular value decomposition\n(SVD) in each step, our method only uses a single approximate eigenvector computation per\niteration, which can be done by e.g. the power method. A conference version of our work for\nnuclear norm regularized problems has appeared in [JS10].\nIn the same spirit, we will also give a new algorithm with a convergence guarantee for optimiz-\ning with a max-norm regularization. For matrix completion problems, experiments show that\nthe max-norm can result in an improved generalization performance compared to the nuclear\nnorm in some cases [SRJ04, LRS+10].\nNuclear Norm Regularized Convex Optimization.\nWe consider the following convex optimiza-\ntion problems over matrices:\nmin\nZ\u2208Rm\u00d7n f(Z) + \u00b5 \u2225Z\u2225\u2217\n(31)\nand the corresponding constrained variant\nmin\nZ\u2208Rm\u00d7n, \u2225Z\u2225\u2217\u2264t\n2\nf(Z)\n(32)\nwhere f(Z) is any di\ufb00erentiable convex function (usually called the loss function), \u2225.\u2225\u2217is the\nnuclear norm of a matrix, also known as the trace norm (sum of the singular values, or \u21131-norm\nof the spectrum). Here \u00b5 > 0 and t > 0 respectively are given parameters, usually called the\nregularization parameter.\nThe nuclear norm is know as the natural generalization of the (sparsity inducing) \u21131-norm\nfor vectors, to the case of semide\ufb01nite matrices. When choosing f(X) := \u2225A(X) \u2212b\u22252\n2 for some\nlinear map A : Rn\u00d7m \u2192Rp, the above formulation (31) is the matrix generalization of the\nproblem minx\u2208Rn \u2225Ax \u2212b\u22252\n2 +\u00b5 \u2225x\u22251, for a \ufb01xed matrix A, which is the important \u21131-regularized\nleast squares problem, also known as the basis pursuit de-noising problem in the compressed\nsensing literature, see also Section 5.1.\nThe analoguous vector variant of (32) is the Lasso\nproblem [Tib96] which is minx\u2208Rn\nn\n\u2225Ax \u2212b\u22252\n2\n\f\f\f \u2225x\u22251 \u2264t\no\n.\nMax-Norm Regularized Convex Optimization.\nIntuitively, one can think of the matrix max-\nnorm as the generalization of the vector \u2113\u221e-norm to PSD matrices. Here we consider optimiza-\n40\ntion problems with a max-norm regularization, which are given by\nmin\nZ\u2208Rm\u00d7n f(Z) + \u00b5 \u2225Z\u2225max\n(33)\nand the corresponding constrained variant being\nmin\nZ\u2208Rm\u00d7n, \u2225Z\u2225max\u2264t f(Z) .\n(34)\nOur Contribution.\nUsing the optimization methods from the previous Sections 7 and 27, we\npresent a much simpler algorithm to solve problems of the form (32), which does not need\nany internal SVD computations. The same approach will also solve the max-norm regularized\nproblems (34). We achieve this by transforming the problems to the convex optimization setting\nover positive semide\ufb01nite matrices which we have studied in the above Sections 7.1 and 8.\nOur new approach has several advantages for nuclear norm optimization when compared\nto the existing algorithms such as \u201cproximal gradient\u201d methods (APG) and \u201csingular value\nthresholding\u201d (SVT), see e.g. [GLW+09, CCS10, TY10, JY09], and also in comparison to the\nalternating-gradient-descent-type methods (as e.g. [RS05, Lin07]).\ni) By employing the approximate SDP solver by [Haz08], see Algorithm 6, we obtain a\nguaranteed \u03b5-approximate solution Z after O\n\u0000 1\n\u03b5\n\u0001\niterations. Crucially, the resulting solu-\ntion Z is simultaneously of low rank, namely rank O\n\u0000 1\n\u03b5\n\u0001\n. Also the algorithm maintains a\ncompact representation of Z in terms of a low-rank matrix factorization Z = LRT (with\nthe desired bounded nuclear norm), and can therefore even be applied if the full matrix\nZ would be far too large to even be stored.\nii) Compared to the alternating-gradient-descent-type methods from machine learning, we\novercome the problem of working with non-convex formulations of the form f(LRT ),\nwhich is NP-hard, and instead solve the original convex problem in f(Z).\niii) The total running time of our algorithm for nuclear norm problems grows linear in the\nproblem size, allows to take full advantage of sparse problems such as e.g. for matrix\ncompletion. More precisely, the algorithm runs in time O\n\u0010 Nf\n\u03b51.5\n\u0011\n, where Nf is the number\nof matrix entries on which the objective function f depends. Per iteration, our method\nconsists of only a single approximate (largest) eigenvector computation, allowing it to\nscale to any problem size where the power method (or Lanczos\u2019 algorithm) can still be\napplied. This also makes the method easy to implement and to parallelize. Existing\nAPG/SVT methods by contrast need an entire SVD in each step, which is signi\ufb01cantly\nmore expensive.\niv) On the theory side, our simple convergence guarantee of O\n\u0000 1\n\u03b5\n\u0001\nsteps holds even if the\nused eigenvectors are only approximate.\nIn comparison, those existing methods that\ncome with a convergence guarantee do require an exact SVD in each iteration, which\nmight not always be a realistic assumption in practice.\nWe demonstrate that our new algorithm on standard datasets improves over the state of the\nart methods, and scales to large problems such as matrix factorizations on the Net\ufb02ix dataset.\nHazan\u2019s Algorithm 6 can be interpreted as the generalization of the coreset approach to\nproblems on symmetric matrices, which we have explained in the previous Section 7.1. Compared\nto the O(1/\u221a\u03b5) convergence methods in the spirit of [Nes83, Nes07a], our number of steps is\nlarger, which is however more than compensated by the improved step complexity, being lower\nby a factor of roughly (n + m).\nOur new method for the nuclear norm case can also be interpreted as a modi\ufb01ed, theoret-\nically justi\ufb01ed variant of Simon Funk\u2019s popular SVD heuristic [Web06] for regularized matrix\n41\nfactorization. To our knowledge this is the \ufb01rst guaranteed convergence result for this class of\nalternating-gradient-descent-type algorithms.\nRelated Work.\nFor nuclear norm optimization, there are two lines of existing methods. On\nthe one hand, in the optimization community, [TY10, LST09], [GLW+09] and [JY09] indepen-\ndently proposed algorithms that obtain an \u03b5-accurate solution to (31) in O(1/\u221a\u03b5) steps, by\nimproving the algorithm of [CCS10]. These methods are known under the names \u201caccelerated\nproximal gradient\u201d (APG) and \u201csingular value thresholding\u201d (SVT). More recently also [MHT10]\nand [MGC09] proposed algorithms along the same idea. Each step of all those algorithms re-\nquires the computation of the singular value decomposition (SVD) of a matrix of the same size\nas the solution matrix, which is expensive even with the currently available fast methods such\nas PROPACK. [TY10] and [JY09] and also [GLW+09] show that the primal error of their algo-\nrithm is smaller than \u03b5 after O(1/\u221a\u03b5) steps, using an analysis inspired by [Nes83] and [BT09].\nFor an overview of related algorithms, we also refer the reader to [CLMW11]. As mentioned\nabove, the method presented here has a signi\ufb01cantly lower computational cost per iteration (one\napproximate eigenvector compared to a full exact SVD), and is also faster in practice on large\nmatrix completion problems.\nOn the other hand, in the machine learning community, research originated from matrix\ncompletion and factorization [SRJ04], later motivated by the Net\ufb02ix prize challenge, getting sig-\nni\ufb01cant momentum from the famous blog post by [Web06]. Only very recently an understanding\nhas formed that many of these methods can indeed by seen as optimizing with regularization\nterm closely related to the nuclear norm, see Section 11.5.4 and [JS10, SS10]. The majority of\nthe currently existing machine learning methods such as for example [RS05, Lin07] and later\nalso [Pat07, ZWSP08, KBV09, TPNT09, IR10, GNHS11] are of the type of \u201calternating\u201d gra-\ndient descent applied to f(LRT ), where at each step one of the factors L and R is kept \ufb01xed,\nand the other factor is updated by a gradient or stochastic gradient step. Therefore, despite\nworking well in many practical applications, all these mentioned methods can get stuck in local\nminima \u2014 and so are theoretically not well justi\ufb01ed, see also the discussion in [DeC06] and our\nSection 11.4.\nThe same issue also comes up for max-norm optimization, where for example [LRS+10] op-\ntimize over the non-convex factorization (38) for bounded max-norm. To our knowledge, no\nalgorithm with a convergence guarantee was known so far.\nFurthermore, optimizing with a rank constraint was recently shown to be NP-hard [GG10].\nIn practical applications, nearly all approaches for large scale problems are working over a\nfactorization Z = LRT of bounded rank, therefore ruling out their ability to obtain a solution\nin polynomial time in the worst-case, unless P = NP.\nOur new method for both nuclear and max-norm avoids all the above described problems by\nsolving an equivalent convex optimization problem, and provably runs in near linear time in the\nnuclear norm case.\n11.2 The Nuclear Norm for Matrices\nThe nuclear norm \u2225Z\u2225\u2217of a rectangular matrix Z \u2208Rm\u00d7n, also known as the trace norm or Ky\nFan norm, is given by the sum of the singular values of Z, which is equal to the \u21131-norm of the\nsingular values of Z (because singular values are always non-negative). Therefore, the nuclear\nnorm is often called the Schatten \u21131-norm. In this sense, it is a natural generalization of the\n\u21131-norm for vectors which we have studied earlier.\nThe nuclear norm has a nice equivalent characterization in terms of matrix factorizations of Z,\n42\ni.e.\n\u2225Z\u2225\u2217:=\nmin\nLRT =Z\n1\n2\n\u0000\u2225L\u22252\nFro + \u2225R\u22252\nFro\n\u0001\n,\n(35)\nwhere the number of columns of the factors L \u2208Rm\u00d7k and Rn\u00d7k is not constrained [FHB01,\nSRJ04]. In other words, the nuclear norm constrains the average Euclidean row or column norms\nof any factorization of the original matrix Z.\nFurthermore, the nuclear norm is dual to the standard spectral matrix norm (i.e. the matrix\noperator norm), meaning that\n\u2225Z\u2225\u2217=\nmax\nB,\u2225B\u2225spec\u22641 B \u2022 Z ,\nsee also [RFP10]. Recall that \u2225B\u2225spec is de\ufb01ned as the \ufb01rst singular value \u03c31(B) of the matrix B.\nSimilarly to the property of the vector \u2225.\u22251-norm being the best convex approximation to the\nsparsity of a vector, as we discussed in Section 5 the nuclear norm is the best convex approx-\nimation of the matrix rank. More precisely, \u2225.\u2225\u2217is the convex envelope of the rank [FHB01],\nmeaning that it is the largest convex function that is upper bounded by the rank on the convex\ndomain of matrices\nn\nZ\n\f\f\f \u2225Z\u2225spec \u22641\no\n. This motivates why the nuclear norm is widely used\nas a proxy function (or convex relaxation) for rank minimization, which otherwise is a hard\ncombinatorial problem.\nIts relation to semide\ufb01nite optimization \u2014 which explains why the nuclear norm is often called\nthe trace norm \u2014 is that\n\u2225Z\u2225\u2217= minimize\nV,W\nt\ns.t.\n\u0012 V\nZ\nZT\nW\n\u0013\n\u2ab00 and\nTr(V ) + Tr(W) \u22642t .\n(36)\nHere the two optimization variables range over the symmetric matrices V \u2208Sm\u00d7m and W \u2208\nSn\u00d7n.\nThis semide\ufb01nite characterization will in fact be the central tool for our algorithmic\napproach for nuclear norm regularized problems in the following. The equivalence of the above\ncharacterization to the earlier \u201cfactorization\u201d formulation (35) is a consequence of the following\nsimple Lemma 27.\nThe Lemma gives a correspondence between the (rectangular) matrices\nZ \u2208Rm\u00d7n of bounded nuclear norm on one hand, and the (symmetric) PSD matrices X \u2208\nS(m+n)\u00d7(m+n) of bounded trace on the other hand.\nLemma 27 ([FHB01, Lemma 1]). For any non-zero matrix Z \u2208Rm\u00d7n and t \u2208R, it holds that\n\u2225Z\u2225\u2217\u2264t\n2\nif and only if\n\u2203symmetric matrices V \u2208Sm\u00d7m, W \u2208Sn\u00d7n\ns.t.\n\u0012 V\nZ\nZT\nW\n\u0013\n\u2ab00 and\nTr(V ) + Tr(W) \u2264t .\nProof.\n\u21d2Using the characterization (35) of the nuclear norm \u2225Z\u2225\u2217= minLRT =Z\n1\n2(\u2225L\u22252\nFro +\n\u2225R\u22252\nFro) we get that \u2203L, R, LRT = Z s.t. \u2225L\u22252\nFro + \u2225R\u22252\nFro = Tr(LLT ) + Tr(RRT ) \u2264t, or in\nother words we have found a matrix\n\u0000 LLT\nZ\nZT\nRRT\n\u0001\n= (L\nR)(L\nR)T \u2ab00 of trace \u2264t.\n\u21d0As the matrix\n\u0000 V\nZ\nZT W\n\u0001\nis symmetric and PSD, it can be (Cholesky) factorized to (L; R)(L; R)T\ns.t. LRT = Z and t \u2265Tr(LLT ) + Tr(RRT ) = \u2225L\u22252\nFro + \u2225R\u22252\nFro, therefore \u2225Z\u2225\u2217\u2264t\n2.\n43\nInterestingly, for characterizing bounded nuclear norm matrices, it does not make any di\ufb00er-\nence whether we enforce an equality or inequality constraint on the trace. This fact will turn\nout to be useful in order to apply our Algorithm 6 later on.\nCorollary 28. For any non-zero matrix Z \u2208Rm\u00d7n and t \u2208R, it holds that\n\u2225Z\u2225\u2217\u2264t\n2\nif and only if\n\u2203symmetric matrices V \u2208Sm\u00d7m, W \u2208Sn\u00d7n\ns.t.\n\u0012 V\nZ\nZT\nW\n\u0013\n\u2ab00 and\nTr(V ) + Tr(W) = t .\nProof.\n\u21d2From Lemma 27 we obtain a matrix\n\u0000 V\nZ\nZT W\n\u0001\n=: X \u2ab00 of trace say s \u2264t. If s < t,\nwe add (t \u2212s) to the top-left entry of V , i.e. we add to X the PSD rank-1 matrix (t \u2212s)e1eT\n1\n(which again gives a PSD matrix). \u21d0follows directly from Lemma 27.\n11.2.1 Weighted Nuclear Norm\nA promising weighted nuclear norm regularization for matrix completion was recently proposed\nby [SS10]. For \ufb01xed weight vectors p \u2208Rm, q \u2208Rn, the weighted nuclear norm \u2225Z\u2225nuc(p,q) of\nZ \u2208Rm\u00d7n is de\ufb01ned as\n\u2225Z\u2225nuc(p,q) := \u2225PZQ\u2225\u2217,\nwhere P = diag(\u221ap) \u2208Rm\u00d7m denotes the diagonal matrix whose i-th diagonal entry is \u221api,\nand analogously for Q = diag(\u221aq) \u2208Rn\u00d7n. Here p \u2208Rm is the vector whose entries are the\nprobabilities p(i) > 0 that the i-th row is observed in the sampling \u2126. Analogously, q \u2208Rn\ncontains the probability q(j) > 0 for each column j. The opposite weighting (using\n1\np(i) and\n1\nq(j)\ninstead of p(i),q(j)) has also been suggested by [WKS08].\nAny optimization problem with a weighted nuclear norm regularization\nmin\nZ\u2208Rm\u00d7n, \u2225Z\u2225nuc(p,q) \u2264t/2 f(Z)\n(37)\nand arbitrary loss function f can therefore be formulated equivalently over the domain \u2225PZQ\u2225\u2217\u2264\nt/2, such that it reads as (if we substitute \u00afZ := PZQ),\nmin\n\u00afZ\u2208Rm\u00d7n,\u2225\u00afZ\u2225\u2217\u2264t/2\nf(P \u22121 \u00afZQ\u22121).\nHence, we have reduced the task to our standard convex problem (32) for \u02c6f that here is de\ufb01ned\nas\n\u02c6f(X) := f(P \u22121 \u00afZQ\u22121),\nwhere X =:\n\u0000 V\n\u00afZ\n\u00afZT W\n\u0001\n. This equivalence implies that any algorithm solving (32) also serves as an\nalgorithm for weighted nuclear norm regularization. In particular, Hazan\u2019s Algorithm 6 does\nimply a guaranteed approximation quality of \u03b5 for problem (37) after O\n\u0000 1\n\u03b5\n\u0001\nmany rank-1 updates,\nas we discussed in Section 7. So far, to the best of our knowledge, no approximation guarantees\nwere known for the weighted nuclear norm.\nSolution path algorithms (maintaining approximation guarantees when the regularization\nparameter t changes) as proposed by [GJL10, GJL12a, GJL12b], and the author\u2019s PhD the-\nsis [Jag11], can also be extended to the case of the weighted nuclear norm.\n44\n11.3 The Max-Norm for Matrices\nWe think of the matrix max-norm as a generalization of the vector \u2113\u221e-norm to the case of\npositive semide\ufb01nite matrices, which we have studied before.\nIn some matrix completion applications, the max-norm has been observed to provide solutions\nof better generalization performance than the nuclear norm [SRJ04]. Both matrix norms can be\nseen as a convex surrogate of the rank [SS05].\nThe max-norm \u2225Z\u2225max of a rectangular matrix Z \u2208Rm\u00d7n has a nice characterization in terms\nof matrix factorizations of Z, i.e.\n\u2225Z\u2225max :=\nmin\nLRT =Z max{\u2225L\u22252\n2,\u221e, \u2225R\u22252\n2,\u221e} ,\n(38)\nwhere the number of columns of the factors L \u2208Rm\u00d7k and Rn\u00d7k is not constrained [LRS+10].\nHere \u2225L\u22252,\u221eis the maximum \u21132-norm of any row Li: of L, that is \u2225L\u22252,\u221e:= maxi \u2225Li:\u22252 =\nmaxi\nqP\nk L2\nik. Compared to the nuclear norm, we therefore observe that the max-norm con-\nstrains the maximal Euclidean row-norms of any factorization of the original matrix Z, see\nalso [SS05]. 7\nAn alternative formulation of the max-norm was given by [LMSS07] and [SS05], stating that\n\u2225Z\u2225max =\nmin\nLRT =Z(max\ni\n||Li:||2)(max\ni\n||Ri:||2) .\nThe dual norm to the max-norm, as given in [SS05], is\n\u2225Z\u2225\u2217\nmax =\nmax\n\u2225Y \u2225max\u22641 Z \u2022 Y\n=\nmax\nk,\nli\u2208Rk,\u2225li\u22252\u22641\nrj\u2208Rk,\u2225rj\u22252\u22641\nX\ni,j\nZij lT\ni rj ,\nwhere the last equality follows from the characterization (38).\nThe relation of the max-norm to semide\ufb01nite optimization \u2014 which also explains the naming\nof the max-norm \u2014 is that\n\u2225Z\u2225max = minimize\nV,W\nt\ns.t.\n\u0012 V\nZ\nZT\nW\n\u0013\n\u2ab00 and\nVii \u2264t \u2200i \u2208[m],\nWii \u2264t \u2200i \u2208[n]\n(39)\nHere the two optimization variables range over the symmetric matrices V\n\u2208Sm\u00d7m and\nW \u2208Sn\u00d7n, see for example [LRS+10].\nAs already in the nuclear norm case, this semide\ufb01-\nnite characterization will again be the central tool for our algorithmic approach for max-norm\nregularized problems in the following.\nThe equivalence of the above characterization to the\nearlier \u201cfactorization\u201d formulation (38) is a consequence of the following simple Lemma 29. The\nLemma gives a correspondence between the (rectangular) matrices Z \u2208Rm\u00d7n of bounded max-\nnorm on one hand, and the (symmetric) PSD matrices X \u2208S(m+n)\u00d7(m+n) of uniformly bounded\ndiagonal on the other hand.\nLemma 29. For any non-zero matrix Z \u2208Rn\u00d7m and t \u2208R:\n\u2225Z\u2225max \u2264t\n7Note that the max-norm does not coincide with the matrix norm induced by the vector \u2225.\u2225\u221e-norm, that is\n\u2225Z\u2225\u221e:= supx\u0338=0\n\u2225Zx\u2225\u221e\n\u2225x\u2225\u221e. The latter matrix norm by contrast is known to be the maximum of the row sums\nof Z (i.e. the \u21131-norms of the rows).\n45\nif and only if\n\u2203symmetric matrices V \u2208Sm\u00d7m, W \u2208Sn\u00d7n\ns.t.\n\u0012 V\nZ\nZT\nW\n\u0013\n\u2ab00 and\nVii \u2264t \u2200i \u2208[m],\nWii \u2264t \u2200i \u2208[n]\nProof.\n\u21d2Using the above characterization (38) of the max-norm, or namely that \u2225Z\u2225max =\nminLRT =Z max{\u2225L\u22252\n2,\u221e, \u2225R\u22252\n2,\u221e}, we get that \u2203L, R with LRT = Z, s.t. max{\u2225L\u22252\n2,\u221e, \u2225R\u22252\n2,\u221e} =\nmax{maxi \u2225Li:\u22252\n2 , maxi \u2225Ri:\u22252\n2} \u2264t, or in other words we have found a matrix\n\u0000 LLT\nZ\nZT\nRRT\n\u0001\n=\n(L; R)(L; R)T \u2ab00 where every diagonal element is at most t, that is \u2225Li:\u22252\n2 = (LLT )ii \u2264t \u2200i \u2208\n[m], and \u2225Ri:\u22252\n2 = (RRT )ii \u2264t \u2200i \u2208[n].\n\u21d0As the matrix\n\u0000 V\nZ\nZT W\n\u0001\nis symmetric and PSD, it can be (Cholesky) factorized to (L; R)(L; R)T\ns.t. LRT = Z and \u2225Li:\u22252\n2 = (LLT )ii \u2264t \u2200i \u2208[m] and \u2225Ri:\u22252\n2 = (RRT )ii \u2264t \u2200i \u2208[n], which\nimplies \u2225Z\u2225max \u2264t.\n11.4 Optimizing with Bounded Nuclear Norm and Max-Norm\nMost of the currently known algorithms for matrix factorizations as well as nuclear norm or\nmax-norm regularized optimization problems, such as (31), (32), (33) or (34), do su\ufb00er from the\nfollowing problem:\nIn order to optimize the convex objective function f(Z) while controlling the norm \u2225Z\u2225\u2217or\n\u2225Z\u2225max, the methods instead try to optimize f(LRT ), with respect to both factors L \u2208Rm\u00d7k\nand R \u2208Rn\u00d7k, with the corresponding regularization constraint imposed on L and R. This\napproach is of course very tempting, as the constraints on the factors \u2014 which originate from\nthe matrix factorization characterizations (35) and (38) \u2014 are simple and in some sense easier\nto enforce.\nUnhealthy Side-E\ufb00ects of Factorizing.\nHowever, there is a signi\ufb01cant price to pay: Even if the\nobjective function f(Z) is convex in Z, the very same function expressed as a function f(LRT ) of\nboth the factor variables L and R becomes a severely non-convex problem, naturally consisting\nof a large number of saddle-points (consider for example just the smallest case L, R \u2208R1\u00d71\ntogether with the identity function f(Z) = Z \u2208R).\nThe majority of the currently existing methods such as for example [RS05, Lin07] and later also\n[Pat07, ZWSP08, KBV09, TPNT09, IR10, GNHS11] is of this \u201calternating\u201d gradient descent\ntype, where at each step one of the factors L and R is kept \ufb01xed, and the other factor is\nupdated by e.g. a gradient or stochastic gradient step. Therefore, despite working well in many\npractical applications, all these mentioned methods can get stuck in local minima \u2014 and so are\ntheoretically not well justi\ufb01ed, see also the discussion in [DeC06].\nThe same issue also comes up for max-norm optimization, where for example [LRS+10] opti-\nmize over the non-convex factorization (38) for bounded max-norm.\nConcerning the \ufb01xed rank of the factorization, [GG10] have shown that \ufb01nding the optimum\nunder a rank constraint (even if the rank is one) is NP-hard (here the used function f was the\nstandard squared error on an incomplete matrix). On the positive side, [BM03] have shown\nthat if the rank k of the factors L and R exceeds the rank of the optimum solution X\u2217, then \u2014\nin some cases \u2014 it can be guaranteed that the local minima (or saddle points) are also global\nminima. However, in nearly all practical applications it is computationally infeasible for the\nabove mentioned methods to optimize with the rank k being in the same order of magnitude as\nthe original matrix size m and n (as e.g. in the Net\ufb02ix problem, such factors L, R could possibly\nnot even be stored on a single machine8).\n8Algorithm 6 in contrast does never need to store a full estimate matrix X, but instead just keeps the rank-1\nfactors v obtained in each step, maintaining a factorized representation of X.\n46\nRelief:\nOptimizing Over an Equivalent Convex Problem.\nHere we simply overcome this\nproblem by using the transformation to semide\ufb01nite matrices, which we have outlined in the\nabove Corollary 28 and Lemma 29. These bijections of bounded nuclear and max-norm matrices\nto the PSD matrices over the corresponding natural convex domains do allow us to directly\noptimize a convex problem, avoiding the factorization problems explained above. We describe\nthis simple trick formally in the next two Subsections 11.4.1 and 11.4.2.\nBut what if you really need a Matrix Factorization?\nIn some applications (such as for exam-\nple embeddings or certain collaborative \ufb01ltering problems) of the above mentioned regularized\noptimization problems over f(Z), one would still want to obtain the solution (or approximation)\nZ in a factorized representation, that is Z = LRT . We note that this is also straight-forward to\nachieve when using our transformation: An explicit factorization of any feasible solution to the\ntransformed problem (20) or (27) \u2014 if needed \u2014 can always be directly obtained since X \u2ab00.\nAlternatively, algorithms for solving the transformed problem (20) can directly maintain the\napproximate solution X in a factorized representation (as a sum of rank-1 matrices), as achieved\nfor example by Algorithms 6 and 7.\n11.4.1 Optimization with a Nuclear Norm Regularization\nHaving Lemma 27 at hand, we immediately get to the crucial observation of this section, allowing\nus to apply Algorithm 6:\nAny optimization problem over bounded nuclear norm matrices (32) is in fact equivalent to\na standard bounded trace semide\ufb01nite problem (20). The same transformation also holds for\nproblems with a bound on the weighted nuclear norm, as given in (37).\nCorollary 30. Any nuclear norm regularized problem of the form (32) is equivalent to a bounded\ntrace convex problem of the form (20), namely\nminimize\nX\u2208S(m+n)\u00d7(m+n)\n\u02c6f(X)\ns.t.\nTr(X) = t ,\nX \u2ab00\n(40)\nwhere \u02c6f is de\ufb01ned by \u02c6f(X) := f(Z) for Z \u2208Rm\u00d7n being the upper right part of the symmetric\nmatrix X. Formally we again think of X \u2208S(n+m)\u00d7(n+m) as consisting of the four parts X =:\n\u0012\nV\nZ\nZT\nW\n\u0013\nwith V \u2208Sm\u00d7m, W \u2208Sn\u00d7n and Z \u2208Rm\u00d7n.\nHere \u201cequivalent\u201d means that for any feasible point of one problem, we have a feasible point\nof the other problem, attaining the same objective value. The only di\ufb00erence to the original\nformulation (20) is that the function argument X needs to be rescaled by 1\nt in order to have\nunit trace, which however is a very simple operation in practical applications. Therefore, we can\ndirectly apply Hazan\u2019s Algorithm 6 for any max-norm regularized problem as follows:\nUsing our analysis of Algorithm 6 from Section 7.1, we see that Algorithm 8 runs in time near\nlinear in the number Nf of non-zero entries of the gradient \u2207f. This makes it very attractive in\nparticular for recommender systems applications and matrix completion, where \u2207f is a sparse\nmatrix (same sparsity pattern as the observed entries), which we will discuss in more detail in\nSection 11.5.\nCorollary 31. After at most O\n\u0000 1\n\u03b5\n\u0001\nmany iterations (i.e. approximate eigenvalue computations),\nAlgorithm 8 obtains a solution that is \u03b5 close to the optimum of (32). The algorithm requires a\ntotal of \u02dcO\n\u0010 Nf\n\u03b51.5\n\u0011\narithmetic operations (with high probability).\n47\nAlgorithm 8 Nuclear Norm Regularized Solver\nInput: A convex nuclear norm regularized problem (32),\ntarget accuracy \u03b5\nOutput: \u03b5-approximate solution for problem (32)\n1. Consider the transformed symmetric problem for \u02c6f,\nas given by Corollary 30\n2. Adjust the function \u02c6f so that it \ufb01rst rescales its argument by t\n3. Run Hazan\u2019s Algorithm 6 for \u02c6f(X) over the domain X \u2208S.\nProof. We use the transformation from Corollary 30 and then rescale all matrix entries by\n1\nt .\nThen result then follows from Corollary 20 on page 31 on the running time of Hazan\u2019s\nalgorithm.\nThe fact that each iteration of our algorithm is computationally very cheap \u2014 consisting only\nof the computation of an approximate eigenvector \u2014 strongly contrasts the existing \u201cproximal\ngradient\u201d and \u201csingular value thresholding\u201d methods [GLW+09, JY09, MGC09, LST09, CCS10,\nTY10], which in each step need to compute an entire SVD. Such a single incomplete SVD\ncomputation (\ufb01rst k singular vectors) amounts to the same computational cost as an entire\nrun of our algorithm (for k steps). Furthermore, those existing methods which come with a\ntheoretical guarantee, in their analysis assume that all SVDs used during the algorithm are\nexact, which is not feasible in practice. By contrast, our analysis is rigorous even if the used\neigenvectors are only \u03b5\u2032-approximate.\nAnother nice property of Hazan\u2019s method is that the returned solution is guaranteed to be\nsimultaneously of low rank (k after k steps), and that by incrementally adding the rank-1\nmatrices vkvT\nk , the algorithm automatically maintains a matrix factorization of the approximate\nsolution.\nAlso, Hazan\u2019s algorithm, as being an instance of our presented general framework, is designed\nto automatically stay within the feasible region S, where most of the existing methods do need\na projection step to get back to the feasible region (as e.g. [Lin07, LST09]), making both the\ntheoretical analysis and implementation more complicated.\n11.4.2 Optimization with a Max-Norm Regularization\nThe same approach works analogously for the max-norm, by using Lemma 29 in order to apply\nAlgorithm 7:\nAny optimization problem over bounded max-norm matrices (34) is in fact equivalent to a\nsemide\ufb01nite problem (27) over the \u201cbox\u201d of matrices where each element on the diagonal is\nbounded above by t. We think of this domain as generalizing the positive cube of vectors, to\nthe PSD matrices.\nCorollary 32. Any max-norm regularized problem of the form (34) is equivalent to a bounded\ndiagonal convex problem of the form (27), i.e.,\nminimize\nX\u2208S(m+n)\u00d7(m+n)\n\u02c6f(X)\ns.t.\nXii \u22641\n\u2200i,\nX \u2ab00\n(41)\nwhere \u02c6f is de\ufb01ned by \u02c6f(X) := f(Z) for Z \u2208Rm\u00d7n being the upper right part of the symmetric\nmatrix X. Formally we again think of any X \u2208S(n+m)\u00d7(n+m) as consisting of the four parts\nX =:\n\u0012\nV\nZ\nZT\nW\n\u0013\nwith V \u2208Sm\u00d7m, W \u2208Sn\u00d7n and Z \u2208Rm\u00d7n.\n48\nAgain the only di\ufb00erence to the original formulation (27) is that the function argument X\nneeds to be rescaled by 1\nt in order to have the diagonal bounded by one, which however is a\nvery simple operation in practical applications. This means we can directly apply Algorithm 7\nfor any max-norm regularized problem as follows:\nAlgorithm 9 Max-Norm Regularized Solver\nInput: A convex max-norm regularized problem (34),\ntarget accuracy \u03b5\nOutput: \u03b5-approximate solution for problem (34)\n1. Consider the transformed symmetric problem for \u02c6f,\nas given by Corollary 32\n2. Adjust the function \u02c6f so that it \ufb01rst rescales its argument by t\n3. Run Algorithm 7 for \u02c6f(X) over the domain X \u2208\u229e.\nUsing the analysis of our new Algorithm 7 from Section 7.1, we obtain the following guarantee:\nCorollary 33. After\nl 8Cf\n\u03b5\nm\nmany iterations, Algorithm 9 obtains a solution that is \u03b5 close to the\noptimum of (34).\nProof. We use the transformation from Corollary 32 and then rescale all matrix entries by 1\nt .\nThen the running time of the algorithm follows from Theorem 25.\nMaximum Margin Matrix Factorizations.\nIn the case of matrix completion, the \u201closs\u201d func-\ntion f is de\ufb01ned as measuring the error from X to some \ufb01xed observed matrix, but just at a\nsmall \ufb01xed set of \u201cobserved\u201d positions of the matrices. As we already mentioned, semide\ufb01nite\noptimization over X as above can always be interpreted as \ufb01nding a matrix factorization, as a\nsymmetric PSD matrix X always has a (unique) Cholesky factorization.\nNow for the setting of matrix completion, it is known that the above described optimization\ntask under bounded max-norm, can be geometrically interpreted as learning a maximum margin\nseparating hyperplane for each user/movie. In other words the factorization problem decomposes\ninto a collection of SVMs, one for each user or movie, if we think of the corresponding other\nfactor to be \ufb01xed for a moment [SRJ04]. We will discuss matrix completion in more detail in\nSection 11.5.\nOther Applications of Max-Norm Optimization.\nApart from matrix completion, optimiza-\ntion problems employing the max-norm have other prominent applications in spectral methods,\nspectral graph properties, low-rank recovery, and combinatorial problems such as Max-Cut.\n11.5 Applications\nOur Algorithm 8 directly applies to arbitrary nuclear norm regularized problems of the form (32).\nSince the nuclear norm is in a sense the most natural generalization of the sparsity-inducing \u21131-\nnorm to the case of low rank matrices (see also the discussion in the previous chapters) there\nare many applications of this class of optimization problems.\n11.5.1 Robust Principal Component Analysis\nOne prominent example of a nuclear norm regularized problem in the area of dimensionality\nreduction is given by the technique of robust PCA as introduced by [CLMW11], also called\nprincipal component pursuit, which is the optimization task\nmin\nZ\u2208Rm\u00d7n \u2225Z\u2225\u2217+ \u00b5 \u2225M \u2212Z\u22251 .\n(42)\n49\nHere M \u2208Rm\u00d7n is the given data matrix, and \u2225.\u22251 denotes the entry-wise \u21131-norm. By consid-\nering the equivalent constrained variant \u2225Z\u2225\u2217\u2264t\n2 instead, we obtain a problem the form (32),\nsuitable for our Algorithm 8. However, since the original objective function f(Z) = \u2225M \u2212Z\u22251\nis not di\ufb00erentiable, a smoothed version of the \u21131-norm has to be used instead. This situation\nis analogous to the hinge-loss objective in maximum margin matrix factorization [SRJ04].\nExisting algorithms for robust PCA do usually require a complete (and exact) SVD in each\niteration, as e.g. [TY10, AGI11], and are often harder to analyze compared to our approach. The\n\ufb01rst algorithm with a convergence guarantee of O\n\u0000 1\n\u03b5\n\u0001\nwas given by [AGI11], requiring a SVD\ncomputation per step. Our Algorithm 8 obtains the same guarantee in the same order of steps,\nbut only requires a single approximate eigenvector computation per step, which is signi\ufb01cantly\ncheaper.\nLast but not least, the fact that our algorithm delivers approximate solutions to (42) of rank\nO\n\u0000 1\n\u03b5\n\u0001\nwill be interesting for practical dimensionality reduction applications, as it re-introduces\nthe important concept of low-rank factorizations as in classical PCA. In other words our algo-\nrithm produces an embedding into at most O\n\u0000 1\n\u03b5\n\u0001\nmany new dimensions, which is much easier\nto deal with in practice compared to the full rank n solutions resulting from the existing solvers\nfor robust PCA, see e.g. [CLMW11] and the references therein.\nWe did not yet perform practical experiments for robust PCA, but chose to demonstrate the\npractical performance of Algorithm 6 for matrix completion problems \ufb01rst.\n11.5.2 Matrix Completion and Low Norm Matrix Factorizations\nFor matrix completion problems as for example in collaborative \ufb01ltering and recommender sys-\ntems [KBV09], our algorithm is particularly suitable as it retains the sparsity of the observations,\nand constructs the solution in a factorized way. In the setting of a partially observed matrix\nsuch as in the Net\ufb02ix case, the loss function f(X) only depends on the observed positions, which\nare very sparse, so \u2207f(X) \u2014 which is all we need for our algorithm \u2014 is also sparse.\nWe want to approximate a partially given matrix Y (let \u2126be the set of known training entries\nof the matrix) by a product Z = LRT such that some convex loss function f(Z) is minimized.\nBy \u2126test we denote the unknown test entries of the matrix we want to predict.\nComplexity.\nJust recently it has been shown that the standard low-rank matrix completion\nproblem \u2014 that is \ufb01nding the best approximation to an incomplete matrix by the standard \u21132-\nnorm \u2014 is an NP-hard problem, if the rank of the approximation is constrained. The hardness\nis claimed to hold even for the rank 1 case [GG10]. In the light of this hardness result, the\nadvantage of relaxing the rank by replacing it by the nuclear norm (or max-norm) is even\nmore evident. Our near linear time Algorithm 8 relies on a convex optimization formulation\nand does indeed deliver an guaranteed \u03b5-accurate solution for the nuclear norm regularization,\nfor arbitrary \u03b5 > 0. Such a guarantee is lacking for the \u201calternating\u201d descent heuristics such\nas [RS05, Lin07, Pat07, ZWSP08, KBV09, TPNT09, IR10, GNHS11, SS10, LRS+10, RR11]\n(which build upon the non-convex factorized versions (35) and (38) while constraining the rank\nof the used factors L and R).\nDi\ufb00erent Regularizations.\nRegularization by the weighted nuclear norm is observed by [SS10]\nto provide better generalization performance than the classical nuclear norm. As it can be simply\nreduced to the nuclear norm, see Section 11.2.1, our Algorithm 8 can directly be applied in the\nweighted case as well. On the other hand, experimental evidence also shows that the max-norm\nsometimes provides better generalization performance than the nuclear norm [SRJ04, LRS+10].\n50\nFor any convex loss function, our Algorithm 9 solves the corresponding max-norm regularized\nmatrix completion task.\nDi\ufb00erent Loss Functions.\nOur method applies to any convex loss function on a low norm\nmatrix factorization problem, and we will only mention two loss functions in particular:\nMaximum Margin Matrix Factorization (MMMF) [SRJ04] can directly be solved by our Al-\ngorithm 8. Here the original (soft margin) formulation is the trade-o\ufb00formulation (31) with\nf(Z) := P\nij\u2208\u2126|Zij \u2212yij| being the hinge or \u21131-loss. Because this function is not di\ufb00erentiable,\nthe authors recommend using the di\ufb00erentiable smoothed hinge loss instead.\nWhen using the standard squared loss function f(Z) := P\nij\u2208\u2126(Zij \u2212yij)2, the problem is\nknown as Regularized Matrix Factorization [Wu07], and both our algorithms directly apply.\nThis loss function is widely used in practice, has a very simple gradient, and is the natural\nmatrix generalization of the \u21132-loss (notice the analogous Lasso and regularized least squares\nformulation). The same function is known as the rooted mean squared error, which was the\nquality measure used in the Net\ufb02ix competition. We write RMSEtrain and RMSEtest for the\nrooted error on the training ratings \u2126and test ratings \u2126test respectively.\nRunning time and memory.\nFrom Corollary 31 we have that the running time of our nuclear\nnorm optimization Algorithm 8 is linear in the size of the input: Each matrix-vector multipli-\ncation in Lanczos\u2019 or the power method exactly costs |\u2126| (the number of observed positions\nof the matrix) operations, and we know that in total we need at most O\n\u00001/\u03b51.5\u0001\nmany such\nmatrix-vector multiplications.\nAlso the memory requirements are very small: Either we store the entire factorization of X(k)\n(meaning the O\n\u0000 1\n\u03b5\n\u0001\nmany vectors v(k)) \u2014 which is still much smaller than the full matrix X\n\u2014 or then instead we can only update and store the prediction values X(k)\nij\nfor ij \u2208\u2126\u222a\u2126test\nin each step. This, together with the known ratings yij determines the sparse gradient matrix\n\u2207f(X(k)) during the algorithm. Therefore, the total memory requirement is only |\u2126\u222a\u2126test|\n(the size of the output) plus the size (n + m) of a single feature vector v.\nThe constant Cf in the running time of Algorithm 6.\nOne might ask if the constant hidden\nin the O( 1\n\u03b5) number of iterations is indeed controllable. Here we show that for the standard\nsquared error on any \ufb01xed set of observed entries \u2126, this is indeed the case. For more details on\nthe constant Cf, we refer the reader to Sections 3.4 and 7.1.\nLemma 34. For the squared error f(Z) = 1\n2\nP\nij\u2208\u2126(Zij \u2212yij)2 over the spectahedron S, it holds\nthat C \u02c6f \u22641.\nProof. In Lemma 6, we have seen that the constant C \u02c6f is upper bounded by half the diameter\nof the domain, times the largest eigenvalue of the Hessian \u22072 \u02c6f( \u20d7X). Here we consider \u02c6f as a\nfunction on vectors \u20d7X \u2208Rn2 corresponding to the matrices X \u2208Sn\u00d7n. However for the squared\nerror as in our case here, the Hessian will be a diagonal matrix. One can directly compute that\nthe diagonal entries of \u22072 \u02c6f( \u20d7X) are 1 at the entries corresponding to \u2126, and zero everywhere\nelse. Furthermore, the squared diameter of the spectahedron is upper bounded by 2, as we have\nshown in Lemma 23. Therefore C \u02c6f \u22641 for the domain S.\nIf the domain is the scaled spectahedron t \u00b7 S as used in our Algorithm 8, then the squared\ndiameter of the domain is 2t2, compare to Lemma 23. This means that the curvature is upper\nbounded by C \u02c6f \u2264t2 in this case. Alternatively, the same bound for the curvature of \u02dcf(X) :=\n\u02c6f(tX) can be obtained along the same lines as for the spectahedron domain in the previous\nlemma, and the same factor of t2 will be the scaling factor of the Hessian, resulting from the\nchain-rule for taking derivatives.\n51\n11.5.3 The Structure of the Resulting Eigenvalue Problems\nFor the actual computation of the approximate largest eigenvector in Algorithm 6, i.e.\nthe\ninternal procedure ApproxEV\n\u0010\n\u2212\u2207\u02c6f(X(k)),\n2C \u02c6\nf\nk+2\n\u0011\n, either Lanczos\u2019 method or the power method\n(as in PageRank, see e.g. [Ber05]) can be used. In our Theorem 18 of Section 7.1, we stated\nthat both the power method as well as Lanczos\u2019 algorithm do provably obtain the required\napproximation quality in a bounded number of steps if the matrix is PSD, with high probability,\nsee also [KW92, AHK05].\nBoth methods are known to scale well to very large problems and can be parallelized easily,\nas each iteration consists of just one matrix-vector multiplication.\nHowever, we have to be\ncareful that we obtain the eigenvector for the largest eigenvalue which is not necessarily the\nprincipal one (largest in absolute value). In that case the spectrum can be shifted by adding an\nappropriate constant to the diagonal of the matrix.\nFor arbitrary loss function f, the gradient \u2212\u2207\u02c6f(X), which is the matrix whose largest eigen-\nvector we have to compute in the algorithm, is always a symmetric matrix of the block form\n\u2207\u02c6f(X) =\n\u0012 0\nG\nGT\n0\n\u0013\nfor G = \u2207f(Z), when X =:\n\u0012\nV\nZ\nZT\nW\n\u0013\n. In other words \u2207\u02c6f(X) is the\nadjacency matrix of a weighted bipartite graph. One vertex class corresponds to the n rows of\nthe original matrix X2 (users in recommender systems), the other class corresponds to the m\ncolumns (items or movies). It is easy to see that the spectrum of \u2207\u02c6f is always symmetric:\nWhenever ( vw ) is an eigenvector for some eigenvalue \u03bb, then ( v\n\u2212w ) is an eigenvector for \u2212\u03bb.\nHence, we have exactly the same setting as in the established Hubs and Authorities (HITS)\nmodel [Kle99]. The \ufb01rst part of any eigenvector is always an eigenvector of the hub matrix GT G,\nand the second part is an eigenvector of the authority matrix GGT .\nRepeated squaring.\nIn the special case that the matrix G is very rectangular (n \u226am or\nn \u226bm), one of the two square matrices GT G or GGT is very small. Then it is known that one\ncan obtain an exponential speed-up in the power method by repeatedly squaring the smaller\none of the matrices, analogously to the \u201csquare and multiply\u201d-approach for computing large\ninteger powers of real numbers. In other words we can perform O(log 1\n\u03b5) many matrix-matrix\nmultiplications instead of O( 1\n\u03b5) matrix-vector multiplications.\n11.5.4 Relation to Simon Funk\u2019s SVD Method\nInterestingly, our proposed framework can also be seen as a theoretically justi\ufb01ed variant of\nSimon Funk\u2019s [Web06] and related approximate SVD methods, which were used as a building\nblock by most of the teams participating in the Net\ufb02ix competition (including the winner team).\nThose methods have been further investigated by [Pat07, TPNT09] and also [KBC07], which\nalready proposed a heuristic using the HITS formulation. These approaches are algorithmically\nextremely similar to our method, although they are aimed at a slightly di\ufb00erent optimization\nproblem, and do not directly guarantee bounded nuclear norm. Recently, [SS10] observed that\nFunk\u2019s algorithm can be seen as stochastic gradient descent to optimize (31) when the regular-\nization term is replaced by a weighted variant of the nuclear norm.\nSimon Funk\u2019s method considers the standard squared loss function \u02c6f(X) = 1\n2\nP\nij\u2208S(Xij \u2212\nyij)2, and \ufb01nds the new rank-1 estimate (or feature) v by iterating v := v + \u03bb(\u2212\u2207\u02c6f(X)v \u2212Kv),\nor equivalently\nv := \u03bb\n\u0012\n\u2212\u2207\u02c6f(X) +\n\u0012 1\n\u03bb \u2212K\n\u0013\nI\n\u0013\nv ,\n(43)\na \ufb01xed number of times. Here \u03bb is a small \ufb01xed constant called the learning rate. Additionally\na decay rate K > 0 is used for regularization, i.e. to penalize the magnitude of the resulting\n52\nfeature v. This matrix-vector multiplication formulation (43) is equivalent to a step of the power\nmethod applied within our framework9, and for small enough learning rates \u03bb the resulting\nfeature vector will converge to the largest eigenvector of \u2212\u2207\u02c6f(Z).\nHowever in Funk\u2019s method, the magnitude of each new feature strongly depends on the starting\nvector v0, the number of iterations, the learning rate \u03bb as well as the decay K, making the\nconvergence very sensitive to these parameters. This might be one of the reasons that so far no\nresults on the convergence speed could be obtained. Our method is free of these parameters, the\nk-th new feature vector is always a unit vector scaled by\n1\n\u221a\nk. Also, we keep the Frobenius norm\n\u2225U\u22252\nFro + \u2225V \u22252\nFro of the obtained factorization exactly \ufb01xed during the algorithm, whereas in\nFunk\u2019s method \u2014 which has a di\ufb00erent optimization objective \u2014 this norm strictly increases\nwith every newly added feature.\nOur described framework therefore gives theoretically justi\ufb01ed variant of the experimentally\nsuccessful method [Web06] and its related variants such as [KBC07, Pat07, TPNT09].\n11.6 Experimental Results\nWe run our algorithm for the following standard datasets10 for matrix completion problems,\nusing the squared error function.\ndataset\n#ratings\nn\nm\nMovieLens 100k\n105\n943\n1682\nMovieLens 1M\n106\n6040\n3706\nMovieLens 10M\n107\n69878\n10677\nNet\ufb02ix\n108\n480189\n17770\nAny eigenvector method can be used as a black-box in our algorithm. To keep the experiments\nsimple, we used the power method11, and performed 0.2 \u00b7 k power iterations in step k. If not\nstated otherwise, the only optimization we used is the improvement by averaging the old and\nnew gradient as explained in Section 7.3.\nAll results were obtained by our (single-thread)\nimplementation in Java 6 on a 2.4 GHz Intel C2D laptop.\nSensitivity.\nThe generalization performance of our method is relatively stable under changes\nof the regularization parameter, see Figure 2:\nMovielens.\nTable 1 reports the running times of our algorithm on the three MovieLens datasets.\nOur algorithm gives an about 5.6 fold speed increase over the reported timings by [TY10], which\nis a very similar method to [JY09]. [TY10] already improves the \u201csingular value thresholding\u201d\nmethods [CCS10] and [MGC09]. For MMMF, [RS05] report an optimization time of about 5\nhours on the 1M dataset, but use the di\ufb00erent smoothed hinge loss function so that the results\ncannot be directly compared. [MGC09], [SJ03] and [JY09] only obtained results on much smaller\ndatasets.\nIn the following experiments on the MovieLens and Net\ufb02ix datasets we have pre-normalized\nall training ratings to the simple average \u00b5i+\u00b5j\n2\nof the user and movie mean values, for the sake\nof being consistent with comparable literature.\n9Another di\ufb00erence of our method to Simon Funk\u2019s lies in the stochastic gradient descent type of the latter,\ni.e. \u201cimmediate feedback\u201d: During each matrix multiplication, it already takes the modi\ufb01ed current feature\nv into account when calculating the loss \u02c6f(Z), whereas our Algorithm 6 alters Z only after the eigenvector\ncomputation is \ufb01nished.\n10See www.grouplens.org and archive.ics.uci.edu/ml.\n11We used the power method starting with the uniform unit vector.\n1\n2 of the approximate eigenvalue corresponding\nto the previously obtained feature vk\u22121 was added to the matrix diagonal to ensure good convergence.\n53\n0.89\n0.91\n0.93\n0.95\n0\n15000\n30000\n45000\n60000\nTrace regularization t\nRMSE test\nk=1000\nFigure 2: Sensitivity of the method on the choice of the regularization parameter t in (32), on MovieLens\n1M.\nTable 1: Running times tour (in seconds) of our algorithm on the three MovieLens datasets compared to\nthe reported timings tTY of [TY10]. The ratings {1, . . . , 5} were used as-is and not normalized\nto any user and/or movie means. In accordance with [TY10], 50% of the ratings were used\nfor training, the others were used as the test set. Here NMAE is the mean absolute error,\ntimes\n1\n5\u22121, over the total set of ratings. k is the number of iterations of our algorithm, #mm\nis the total number of sparse matrix-vector multiplications performed, and tr is the used trace\nparameter t in (32). They used Matlab/PROPACK on an Intel Xeon 3.20 GHz processor.\nNMAE\ntTY\ntour\nk\n#mm\ntr\n100k\n0.205\n7.39\n0.156\n15\n33\n9975\n1M\n0.176\n24.5\n1.376\n35\n147\n36060\n10M\n0.164\n202\n36.10\n65\n468\n281942\nFor MovieLens 10M, we used partition rb provided with the dataset (10 test ratings per user).\nThe regularization parameter t was set to 48333. We obtained a RMSEtest of 0.8617 after k = 400\nsteps, in a total running time of 52 minutes (16291 matrix multiplications). Our best RMSEtest\nvalue was 0.8573, compared to 0.8543 obtained by [LU09] using their non-linear improvement\nof MMMF.\nAlgorithm Variants.\nComparing the proposed algorithm variants from Section 7.3, Figure 3\ndemonstrates moderate improvements compared to our original Algorithm 8.\nNet\ufb02ix.\nTable 2 compares our method to the two \u201chard impute\u201d and \u201csoft impute\u201d sin-\ngular value thresholding methods of [MHT10] on the Net\ufb02ix dataset, where they used Mat-\nlab/PROPACK on an Intel Xeon 3 GHz processor. The \u201csoft impute\u201d variant uses a constrained\nrank heuristic in each update step, and an \u201cun-shrinking\u201d or \ufb01tting heuristic as post-processing.\nBoth are advantages for their method, and were not used for our implementation. Nevertheless,\nour algorithm seems to perform competitive compared to the reported timings of [MHT10].\nNote that the primary goal of this experimental section is not to compete with the prediction\nquality of the best engineered recommender systems (which are usually ensemble methods, i.e.\ncombinations of many di\ufb00erent individual methods). We just demonstrate that our method\nsolves nuclear norm regularized problems of the form (32) on large sample datasets, obtaining\nstrong performance improvements.\n54\n0.63\n0.708\n0.785\n0.863\n0.94\n0\n100\n200\n300\n400\nk\nRMSE\nMovieLens 10M rb\n1/k, test\nbest on line segm., test\ngradient interp., test\n1/k, train\nbest on line segm., train\ngradient interp., train\nFigure 3: Improvements for the two algorithm variants described in Section 7.3, when running on Movie-\nLens 10M. The thick lines above indicate the error on the test set, while the thinner lines\nindicate the training error.\nTable 2: Running times tour (in hours) of our algorithm on the Net\ufb02ix dataset compared to the reported\ntimings tM,hard for \u201chard impute\u201d by [MHT09] and tM,soft for \u201csoft impute\u201d by [MHT10].\nRMSEtest\ntM,hard tM,soft\ntour\nk\n#mm\ntr\n0.986\n3.3\nn.a.\n0.144\n20\n50\n99592\n0.977\n5.8\nn.a.\n0.306\n30\n109\n99592\n0.965\n6.6\nn.a.\n0.504\n40\n185\n99592\n0.962\nn.a.\n1.36\n1.08\n45\n243\n174285\n0.957\nn.a.\n2.21\n1.69\n60\n416\n174285\n0.954\nn.a.\n2.83\n2.68\n80\n715\n174285\n0.9497\nn.a.\n3.27\n6.73\n135\n1942\n174285\n0.9478\nn.a.\nn.a.\n13.6\n200\n4165\n174285\n11.7 Conclusion\nWe have introduced a new method to solve arbitrary convex problems with a nuclear norm\nregularization, which is simple to implement and to parallelize.\nThe method is parameter-\nfree and comes with a convergence guarantee. This guarantee is, to our knowledge, the \ufb01rst\nguaranteed convergence result for the class of Simon-Funk-type algorithms, as well as the \ufb01rst\nalgorithm with a guarantee for max-norm regularized problems.\nIt remains to investigate if our algorithm can be applied to other matrix factorization problems\nsuch as (potentially only partially observed) low rank approximations to kernel matrices as used\ne.g. by the PSVM technique [CZW+07], regularized versions of latent semantic analysis (LSA),\nor non-negative matrix factorization [Wu07].\n55\nReferences\n[AGI11]\nNecdet Serhat Aybat, Donald Goldfarb, and Garud Iyengar. Fast First-Order Methods for\nStable Principal Component Pursuit. arXiv math.OC, May 2011.\n[AHK05]\nSanjeev Arora, Elad Hazan, and Satyen Kale. Fast algorithms for approximate semide\ufb01nite\nprogramming using the multiplicative weights update method. FOCS 2005 - 46th Annual\nIEEE Symposium on Foundations of Computer Science, pages 339\u2013348, 2005.\n[APV02]\nPankaj Agarwal, Cecilia Procopiuc, and Kasturi Varadarajan. Approximation Algorithms\nfor k-Line Center. In Algorithms \u2014 ESA 2002, pages 425\u2013432. 2002.\n[BB09]\nMichel Baes and Michael Buergisser. Smoothing techniques for solving semide\ufb01nite programs\nwith many constraints. Optimization Online, 2009.\n[BC07]\nMihai B\u02d8adoiu and Kenneth L Clarkson. Optimal core-sets for balls. Computational Geometry:\nTheory and Applications, 40(1):14\u201322, 2007.\n[Ber05]\nPavel Berkhin. A survey on PageRank computing. Internet mathematics, 2(1):73, 2005.\n[BHPI02]\nMihai B\u02d8adoiu, Sariel Har-Peled, and Piotr Indyk.\nApproximate clustering via core-sets.\nSTOC \u201902: Proceedings of the thiry-fourth annual ACM Symposium on Theory of Computing,\n2002.\n[BJMO11]\nFrancis Bach, Rodolphe Jenatton, Julien Mairal, and Guillaume Obozinski. Optimization\nwith Sparsity-Inducing Penalties. Technical report, August 2011.\n[BL06]\nJonathan M Borwein and Adrian S Lewis.\nConvex analysis and nonlinear optimization:\ntheory and examples. CMS books in mathematics. Springer, 2006.\n[BLJ04]\nFrancis Bach, Gert R.G. Lanckriet, and Michael I Jordan. Multiple kernel learning, conic\nduality, and the SMO algorithm. ICML \u201904: Proceedings of the twenty-\ufb01rst international\nconference on Machine learning, 2004.\n[BM03]\nSamuel Burer and Renato D C Monteiro. A nonlinear programming algorithm for solving\nsemide\ufb01nite programs via low-rank factorization. Mathematical Programming, 95(2):329\u2013357,\n2003.\n[Bot10]\nL\u00b4eon Bottou. Large-Scale Machine Learning with Stochastic Gradient Descent. In COMP-\nSTAT\u20192010 - Proceedings of the 19th International Conference on Computational Statistics,\npages 177\u2013187, 2010.\n[BSS09]\nJoshua Batson, Daniel Spielman, and Nikhil Srivastava. Twice-ramanujan sparsi\ufb01ers. STOC\n\u201909: Proceedings of the 41st annual ACM Symposium on Theory of Computing, 2009.\n[BT03]\nAmir Beck and Marc Teboulle. Mirror descent and nonlinear projected subgradient methods\nfor convex optimization. Operations Research Letters, 31(3):167\u2013175, 2003.\n[BT09]\nAmir Beck and Marc Teboulle. A Fast Iterative Shrinkage-Thresholding Algorithm for Linear\nInverse Problems. SIAM Journal on Imaging Sciences, 2(1):183, 2009.\n[BTMN01] Aharon Ben-Tal, Tamar Margalit, and Arkadi Nemirovski.\nThe Ordered Subsets Mirror\nDescent Optimization Method with Applications to Tomography. SIAM Journal on Opti-\nmization, 12(1):79, 2001.\n[BTN05]\nAharon Ben-Tal and Arkadi Nemirovski. Non-euclidean restricted memory level method for\nlarge-scale convex optimization. Mathematical Programming, 102(3):407\u2013456, 2005.\n[BV04]\nStephen P Boyd and Lieven Vandenberghe. Convex optimization. 2004.\n[CCS10]\nJian-Feng Cai, Emmanuel J Candes, and Zuowei Shen.\nA Singular Value Thresholding\nAlgorithm for Matrix Completion. SIAM Journal on Optimization, 20(4):1956\u20131982, 2010.\n[CDS98]\nScott Shaobing Chen, David L Donoho, and Michael A Saunders. Atomic Decomposition by\nBasis Pursuit. SIAM Journal on Scienti\ufb01c Computing, 20(1):33, 1998.\n56\n[Cla10]\nKenneth L Clarkson. Coresets, Sparse Greedy Approximation, and the Frank-Wolfe Algo-\nrithm. ACM Transactions on Algorithms, 6(4):1\u201330, 2010.\n[CLMW11] Emmanuel J Candes, Xiaodong Li, Yi Ma, and John Wright. Robust principal component\nanalysis? Journal of the ACM, 58(3), May 2011.\n[CR09]\nEmmanuel J Candes and Benjamin Recht. Exact Matrix Completion via Convex Optimiza-\ntion. Foundations of Computational Mathematics, 9(6):717\u2013772, 2009.\n[CT10]\nEmmanuel J Candes and Terence Tao. The Power of Convex Relaxation: Near-Optimal\nMatrix Completion. IEEE Transactions on Information Theory, 56(5):2053\u20132080, 2010.\n[CZW+07]\nEdward Chang, Kaihua Zhu, Hao Wang, Hongjie Bai, Jian Li, Zhihuan Qiu, and Hang Cui.\nPSVM: Parallelizing Support Vector Machines on Distributed Computers.\nIn NIPS \u201907:\nAdvances in Neural Information Processing Systems 20, pages 257\u2013264, 2007.\n[d\u2019A08]\nAlexandre d\u2019Aspremont. Smooth Optimization with Approximate Gradient. SIAM Journal\non Optimization, 19(3):1171, 2008.\n[DeC06]\nDennis DeCoste. Collaborative prediction using ensembles of Maximum Margin Matrix Fac-\ntorizations. ICML \u201906: Proceedings of the 23rd International Conference on Machine Learn-\ning, 2006.\n[DH78]\nJoseph C Dunn and S Harshbarger. Conditional gradient algorithms with open loop step\nsize rules. Journal of Mathematical Analysis and Applications, 62(2):432\u2013444, 1978.\n[Dun80]\nJoseph C Dunn. Convergence Rates for Conditional Gradient Sequences Generated by Im-\nplicit Step Length Rules. SIAM Journal on Control and Optimization, 18(5):473, 1980.\n[FHB01]\nMaryam Fazel, Haitham Hindi, and Stephen P Boyd. A Rank Minimization Heuristic with\nApplication to Minimum Order System Approximation. Proceedings American Control Con-\nference, 6:4734\u20134739, 2001.\n[FNW07]\nMario A T Figueiredo, Robert D Nowak, and Stephen J Wright. Gradient Projection for\nSparse Reconstruction: Application to Compressed Sensing and Other Inverse Problems.\nIEEE Journal of Selected Topics in Signal Processing, 1(4):586\u2013597, 2007.\n[FW56]\nMarguerite Frank and Philip Wolfe. An algorithm for quadratic programming. Naval Re-\nsearch Logistics Quarterly, 3:95\u2013110, 1956.\n[GG10]\nNicolas Gillis and Fran\u00b8cois Glineur. Low-Rank Matrix Approximation with Weights or Miss-\ning Data is NP-hard. arXiv math.OC, 2010.\n[Gil66]\nElmer G Gilbert. An Iterative Procedure for Computing the Minimum of a Quadratic Form\non a Convex Set. SIAM Journal on Control, 4(1):61\u201380, 1966.\n[GJ09]\nBernd G\u00a8artner and Martin Jaggi. Coresets for polytope distance. SCG \u201909: Proceedings of\nthe 25th Annual Symposium on Computational Geometry, 2009.\n[GJL10]\nJoachim Giesen, Martin Jaggi, and S\u00a8oren Laue. Approximating Parameterized Convex Op-\ntimization Problems. In ESA 2010 - Proceedings of the 18th annual European Conference on\nAlgorithms: Part I, pages 524\u2013535. LNCS, 2010.\n[GJL12a]\nJoachim Giesen, Martin Jaggi, and S\u00a8oren Laue. Approximating Parameterized Convex Op-\ntimization Problems. To appear in ACM Transactions on Algorithms, 2012.\n[GJL12b]\nJoachim Giesen, Martin Jaggi, and S\u00a8oren Laue. Regularization Paths with Guarantees for\nConvex Semide\ufb01nite Optimization. To appear in AISTATS - Fifteenth International Con-\nference on Arti\ufb01cial Intelligence and Statistics, 2012.\n[GLW+09]\nArvind Ganesh, Zhouchen Lin, John Wright, Leqin Wu, Minming Chen, and Yi Ma. Fast\nAlgorithms for Recovering a Corrupted Low-Rank Matrix. In CAMSAP - 3rd IEEE Inter-\nnational Workshop on Computational Advances in Multi-Sensor Adaptive Processing, pages\n213\u2013216, 2009.\n57\n[GM11]\nBernd G\u00a8artner and Ji\u02c7r\u00b4\u0131 Matou\u02c7sek. Approximation Algorithms and Semide\ufb01nite Program-\nming. Springer, December 2011.\n[GNHS11]\nRainer Gemulla, Erik Nijkamp, Peter J Haas, and Yannis Sismanis.\nLarge-Scale Matrix\nFactorization with Distributed Stochastic Gradient Descent. In KDD \u201911 - Proceedings of\nthe 17th ACM SIGKDD international conference on Knowledge Discovery and Data Mining,\n2011.\n[GW95]\nMichel Goemans and David Williamson. Improved approximation algorithms for maximum\ncut and satis\ufb01ability problems using semide\ufb01nite programming. Journal of the ACM, 42(6),\n1995.\n[Haz08]\nElad Hazan. Sparse Approximate Solutions to Semide\ufb01nite Programs. In LATIN 2008, pages\n306\u2013316. Springer, 2008.\n[Haz11]\nElad Hazan. The convex optimization approach to regret minimization. In Optimization for\nMachine Learning. ie.technion.ac.il, 2011.\n[IR10]\nAlexander Ilin and Tapani Raiko. Practical Approaches to Principal Component Analysis in\nthe Presence of Missing Values. Journal of Machine Learning Research, pages 1\u201344, 2010.\n[Jag11]\nMartin Jaggi. Sparse Convex Optimization Methods for Machine Learning. PhD thesis, ETH\nZ\u00a8urich, 2011.\n[Jon92]\nLee K Jones. A Simple Lemma on Greedy Approximation in Hilbert Space and Conver-\ngence Rates for Projection Pursuit Regression and Neural Network Training. The Annals of\nStatistics, 20(1):608\u2013613, 1992.\n[JS10]\nMartin Jaggi and Marek Sulovsk\u00b4y. A Simple Algorithm for Nuclear Norm Regularized Prob-\nlems. ICML 2010: Proceedings of the 27th International Conference on Machine Learning,\n2010.\n[JY09]\nShuiwang Ji and Jieping Ye. An accelerated gradient method for trace norm minimization.\nICML \u201909: Proceedings of the 26th Annual International Conference on Machine Learning,\n2009.\n[Kal07]\nSatyen Kale. E\ufb03cient Algorithms using the Multiplicative Weights Update Method. PhD\nthesis, cs.princeton.edu, 2007.\n[KBC07]\nMikl\u00b4os Kurucz, Andras A Benczur, and Karoly Csalogany. Methods for large scale SVD with\nmissing values. KDD Cup and Workshop at the 13th ACM SIGKDD Conference, 2007.\n[KBP+10]\nMrinal Kalakrishnan, Jonas Buchli, Peter Pastor, Michael Mistry, and Stefan Schaal. Fast,\nrobust quadruped locomotion over challenging terrain. In ICRA 2010 - IEEE International\nConference on Robotics and Automation, pages 2665\u20132670, 2010.\n[KBV09]\nYehuda Koren, Robert Bell, and Chris Volinsky. Matrix Factorization Techniques for Rec-\nommender Systems. IEEE Computer, 42(8):30\u201337, 2009.\n[KKB07]\nKwangmoo Koh, Seung-Jean Kim, and Stephen P Boyd. An interior-point method for large-\nscale l1-regularized logistic regression. Journal of Machine Learning Research, 8:1519\u20131555,\n2007.\n[KL96]\nPhilip Klein and Hsueh-I Lu. E\ufb03cient approximation algorithms for semide\ufb01nite programs\narising from MAX CUT and COLORING. In STOC \u201996: Proceedings of the twenty-eighth\nannual ACM Symposium on Theory of Computing, 1996.\n[Kle99]\nJon M Kleinberg. Authoritative sources in a hyperlinked environment. Journal of the ACM,\n46(5), 1999.\n[KSST09]\nSham M Kakade, Shai Shalev-Shwartz, and Ambuj Tewari. On the duality of strong convexity\nand strong smoothness: Learning applications and matrix regularization. Technical report,\nToyota Technological Institute - Chicago, USA, 2009.\n58\n[KW92]\nJacek Kuczy\u00b4nski and Henryk Wo\u00b4zniakowski.\nEstimating the Largest Eigenvalue by the\nPower and Lanczos Algorithms with a Random Start. SIAM Journal on Matrix Analysis\nand Applications, 13(4):1094\u20131122, 1992.\n[KZ05]\nAndrew Kurdila and Michael Zabarankin. Convex functional analysis. Birkh\u00a8auser Verlag,\n2005.\n[Lin07]\nChih-Jen Lin. Projected Gradient Methods for Nonnegative Matrix Factorization. Neural\nComput., 19(10):2756\u20132779, 2007.\n[LMSS07]\nNathan Linial, Shahar Mendelson, Gideon Schechtman, and Adi Shraibman. Complexity\nmeasures of sign matrices. Combinatorica, 27(4):439\u2013463, 2007.\n[Lov83]\nL\u00b4aszl\u00b4o Lov\u00b4asz. Submodular functions and convexity. Mathematical programming: The state\nof the art, 1983.\n[LRS+10]\nJason Lee, Benjamin Recht, Ruslan Salakhutdinov, Nathan Srebro, and Joel A Tropp. Practi-\ncal Large-Scale Optimization for Max-Norm Regularization. NIPS 2010: Advances in Neural\nInformation Processing Systems 23, 2010.\n[LST09]\nYong-Jin Liu, Defeng Sun, and Kim-Chuan Toh. An Implementable Proximal Point Algo-\nrithmic Framework for Nuclear Norm Minimization. Optimization Online, 2009.\n[LU09]\nNeil Lawrence and Raquel Urtasun. Non-linear matrix factorization with Gaussian processes.\nICML \u201909: Proceedings of the 26th Annual International Conference on Machine Learning,\n2009.\n[Mar52]\nHarry Markowitz. Portfolio Selection. The Journal of Finance, 7(1):77\u201391, 1952.\n[MGC09]\nShiqian Ma, Donald Goldfarb, and Lifeng Chen. Fixed point and Bregman iterative methods\nfor matrix rank minimization. Mathematical Programming, 128(1):321\u2013353, 2009.\n[MHT09]\nRahul Mazumder, Trevor Hastie, and Robert Tibshirani. Spectral Regularization Algorithms\nfor Learning Large Incomplete Matrices. Submitted to JMLR, 2009.\n[MHT10]\nRahul Mazumder, Trevor Hastie, and Robert Tibshirani. Spectral Regularization Algorithms\nfor Learning Large Incomplete Matrices. Journal of Machine Learning Research, 11:1\u201336,\n2010.\n[MR11]\nOlvi L Mangasarian and Benjamin Recht. Probability of unique integer solution to a system\nof linear equations. European Journal of Operational Research, In Press, 2011.\n[Nat95]\nBalas Kausik Natarajan. Sparse Approximate Solutions to Linear Systems. SIAM Journal\non Computing, 24(2):227\u2013234, 1995.\n[Nem04]\nArkadi Nemirovski. Prox-Method with Rate of Convergence O(1/t) for Variational Inequal-\nities with Lipschitz Continuous Monotone Operators and Smooth Convex-Concave Saddle\nPoint Problems. SIAM Journal on Optimization, 15(1):229, 2004.\n[Nem05]\nArkadi Nemirovski. Lectures on modern convex optimization. Georgia Institute of Technol-\nogy, 2005.\n[Nes83]\nYurii Nesterov. A method of solving a convex programming problem with convergence rate\nO(1/sqr(k)). Soviet Mathematics Doklady, 27:372\u2013376, 1983.\n[Nes04]\nYurii Nesterov. Smooth minimization of non-smooth functions. Mathematical Programming,\n103(1):127\u2013152, 2004.\n[Nes07a]\nYurii Nesterov. Gradient methods for minimizing composite objective function. Technical\nreport, CORE and INMA, Universit\u00b4e catholique de Louvain, Belgium, 2007.\n[Nes07b]\nYurii Nesterov. Primal-dual subgradient methods for convex problems. Mathematical Pro-\ngramming, 120(1):221\u2013259, 2007.\n[Nes11]\nYurii Nesterov. Random gradient-free minimization of convex functions. CORE Tech Report,\nFebruary 2011.\n59\n[Pat93]\nMichael Patriksson. Partial linearization methods in nonlinear programming. Journal of\nOptimization Theory and Applications, 78(2):227\u2013246, 1993.\n[Pat98]\nMichael Patriksson. Cost Approximation: A Uni\ufb01ed Framework of Descent Algorithms for\nNonlinear Programs. SIAM Journal on Optimization, 8(2):561, 1998.\n[Pat07]\nArkadiusz Paterek.\nImproving regularized singular value decomposition for collaborative\n\ufb01ltering. KDD Cup and Workshop at the 13th ACM SIGKDD Conference, 2007.\n[Ren05]\nJason D M Rennie. Regularized Logistic Regression is Strictly Convex. people.csail.mit.edu,\n2005.\n[RFP10]\nBenjamin Recht, Maryam Fazel, and Pablo A Parrilo. Guaranteed Minimum-Rank Solutions\nof Linear Matrix Equations via Nuclear Norm Minimization. SIAM Review, 52(3):471\u2013501,\n2010.\n[Roc97]\nR Tyrrell Rockafellar. Convex analysis. Princeton University Press, 1997.\n[RR11]\nBenjamin Recht and Christopher R\u00b4e. Parallel Stochastic Gradient Algorithms for Large-Scale\nMatrix Completion. submitted, 2011.\n[RS05]\nJason D M Rennie and Nathan Srebro.\nFast maximum margin matrix factorization for\ncollaborative prediction. ICML \u201905: Proceedings of the 22nd International Conference on\nMachine Learning, 2005.\n[SJ03]\nNathan Srebro and Tommi Jaakkola.\nWeighted Low-Rank Approximations.\nICML \u201903:\nProceedings of the 20th International Conference on Machine Learning, 2003.\n[SRJ04]\nNathan Srebro, Jason D M Rennie, and Tommi Jaakkola. Maximum-margin matrix factor-\nization. NIPS \u201904: Advances in Neural Information Processing Systems 17, 17:1329\u20131336,\n2004.\n[SS05]\nNathan Srebro and Adi Shraibman. Rank, Trace-Norm and Max-Norm. COLT \u201905: Pro-\nceedings of the 18st annual Workshop on Computational Learning Theory, 3559:545\u2013560,\n2005.\n[SS10]\nRuslan Salakhutdinov and Nathan Srebro. Collaborative Filtering in a Non-Uniform World:\nLearning with the Weighted Trace Norm.\nNIPS 2010: Advances in Neural Information\nProcessing Systems 23, 2010.\n[SSSZ10]\nShai Shalev-Shwartz, Nathan Srebro, and Tong Zhang. Trading Accuracy for Sparsity in\nOptimization Problems with Sparsity Constraints. SIAM Journal on Optimization, 20:2807,\n2010.\n[TG07]\nJoel A Tropp and Anna Gilbert. Signal Recovery From Random Measurements Via Or-\nthogonal Matching Pursuit. IEEE Transactions on Information Theory, 53(12):4655\u20134666,\n2007.\n[Tib96]\nRobert Tibshirani. Regression Shrinkage and Selection via the Lasso. Journal of the Royal\nStatistical Society. Series B (Methodological), pages 267\u2013288, 1996.\n[TPNT09]\nG\u00b4abor Tak\u00b4acs, Istv\u00b4an Pil\u00b4aszy, Botty\u00b4an N\u00b4emeth, and Domonkos Tikk. Scalable Collabora-\ntive Filtering Approaches for Large Recommender Systems. Journal of Machine Learning\nResearch, 10, 2009.\n[Tro04]\nJoel A Tropp. Greed is good: algorithmic results for sparse approximation. IEEE Transac-\ntions on Information Theory, 50(10):2231\u20132242, 2004.\n[TY07]\nMichael Todd and E Alper Yildirim.\nOn Khachiyan\u2019s algorithm for the computation of\nminimum-volume enclosing ellipsoids.\nDiscrete Applied Mathematics, 155(13):1731\u20131744,\n2007.\n[TY10]\nKim-Chuan Toh and Sangwoon Yun. An accelerated proximal gradient algorithm for nuclear\nnorm regularized linear least squares problems. Paci\ufb01c Journal of Optimization, 2010.\n60\n[Web06]\nBrandyn\nWebb.\nNet\ufb02ix\nUpdate:\nTry\nThis\nat\nHome.\nBlog\npost\nsifter.org/\u02dc simon/journal/20061211.html, 2006.\n[WKS08]\nMarkus Weimer, Alexandros Karatzoglou, and Alex J Smola. Improving maximum margin\nmatrix factorization. Machine Learning, 72(3):263\u2013276, 2008.\n[Wu07]\nMingrui Wu. Collaborative \ufb01ltering via ensembles of matrix factorizations. KDD Cup and\nWorkshop at the 13th ACM SIGKDD Conference, 2007.\n[ZdG10]\nYouwei Zhang, Alexandre d\u2019Aspremont, and Laurent El Ghaoui. Sparse PCA: Convex Re-\nlaxations, Algorithms and Applications. arXiv math.OC, 2010.\n[Zha03]\nTong Zhang. Sequential greedy approximation for certain convex optimization problems.\nIEEE Transactions on Information Theory, 49(3):682\u2013691, 2003.\n[Zha11]\nTong Zhang. Sparse Recovery with Orthogonal Matching Pursuit under RIP. IEEE Trans-\nactions on Information Theory, 57(9):6215\u20136221, September 2011.\n[ZWSP08]\nYunhong Zhou, Dennis Wilkinson, Robert Schreiber, and Rong Pan. Large-Scale Parallel\nCollaborative Filtering for the Net\ufb02ix Prize.\nIn Algorithmic Aspects in Information and\nManagement, pages 337\u2013348. 2008.\n61\n",
        "sentence": " A trace norm constraint alone can be taken into account without projection or relaxation into a penalized form by casting the problem as a SDP as proposed in (Jaggi, 2011).",
        "context": "used to approximately solve arbitrary SDPs with bounded trace, which we will brie\ufb02y explain\nin Section 7.2.\nLinearization, the Duality Gap, and Duality of the Norms.\nHere we will prove that the general\nsionality reduction, compared to nuclear norm regularization that we will discuss in the following\nChapter 11.\nIt also remains to investigate further on whether we can approximate general bounded trace\n2010.\n[JY09]\nShuiwang Ji and Jieping Ye. An accelerated gradient method for trace norm minimization.\nICML \u201909: Proceedings of the 26th Annual International Conference on Machine Learning,\n2009.\n[Kal07]"
    },
    {
        "title": "Clustering partially observed graphs via convex optimization",
        "author": [
            "A. Jalali",
            "Y. Chen",
            "S. Sanghavi",
            "H. Xu"
        ],
        "venue": "ICML \u201911,",
        "citeRegEx": "Jalali et al\\.,? \\Q2011\\E",
        "shortCiteRegEx": "Jalali et al\\.",
        "year": 2011,
        "abstract": "This paper considers the problem of clustering a partially observed\nunweighted graph---i.e., one where for some node pairs we know there is an edge\nbetween them, for some others we know there is no edge, and for the remaining\nwe do not know whether or not there is an edge. We want to organize the nodes\ninto disjoint clusters so that there is relatively dense (observed)\nconnectivity within clusters, and sparse across clusters.\n  We take a novel yet natural approach to this problem, by focusing on finding\nthe clustering that minimizes the number of \"disagreements\"---i.e., the sum of\nthe number of (observed) missing edges within clusters, and (observed) present\nedges across clusters. Our algorithm uses convex optimization; its basis is a\nreduction of disagreement minimization to the problem of recovering an\n(unknown) low-rank matrix and an (unknown) sparse matrix from their partially\nobserved sum. We evaluate the performance of our algorithm on the classical\nPlanted Partition/Stochastic Block Model. Our main theorem provides sufficient\nconditions for the success of our algorithm as a function of the minimum\ncluster size, edge density and observation probability; in particular, the\nresults characterize the tradeoff between the observation probability and the\nedge density gap. When there are a constant number of clusters of equal size,\nour results are optimal up to logarithmic factors.",
        "full_text": "Journal of Machine Learning Research 15 (2014) 2213-2238\nSubmitted 12/11; Revised 2/13; Published 6/14\nClustering Partially Observed Graphs via Convex Optimization\nYudong Chen\nYDCHEN@UTEXAS.EDU\nAli Jalali\nALIJ@MAIL.UTEXAS.EDU\nSujay Sanghavi\nSANGHAVI@MAIL.UTEXAS.EDU\nDepartment of Electrical and Computer Engineering\nThe University of Texas at Austin\nAustin, TX 78712, USA\nHuan Xu\nMPEXUH@NUS.EDU.SG\nDepartment of Mechanical Engineering\nNational University of Singapore\nSingapore 117575, SINGAPORE\nEditor: Marina Meila\nAbstract\nThis paper considers the problem of clustering a partially observed unweighted graph\u2014i.e., one\nwhere for some node pairs we know there is an edge between them, for some others we know there\nis no edge, and for the remaining we do not know whether or not there is an edge. We want to\norganize the nodes into disjoint clusters so that there is relatively dense (observed) connectivity\nwithin clusters, and sparse across clusters.\nWe take a novel yet natural approach to this problem, by focusing on \ufb01nding the clustering\nthat minimizes the number of \u201cdisagreements\u201d\u2014i.e., the sum of the number of (observed) missing\nedges within clusters, and (observed) present edges across clusters. Our algorithm uses convex\noptimization; its basis is a reduction of disagreement minimization to the problem of recovering\nan (unknown) low-rank matrix and an (unknown) sparse matrix from their partially observed sum.\nWe evaluate the performance of our algorithm on the classical Planted Partition/Stochastic Block\nModel. Our main theorem provides suf\ufb01cient conditions for the success of our algorithm as a func-\ntion of the minimum cluster size, edge density and observation probability; in particular, the results\ncharacterize the tradeoff between the observation probability and the edge density gap. When there\nare a constant number of clusters of equal size, our results are optimal up to logarithmic factors.\nKeywords: graph clustering, convex optimization, sparse and low-rank decomposition\n1. Introduction\nThis paper is about the following task: given partial observation of an undirected unweighted graph,\npartition the nodes into disjoint clusters so that there are dense connections within clusters, and\nsparse connections across clusters. By partial observation, we mean that for some node pairs we\nknow if there is an edge or not, and for the other node pairs we do not know\u2014these pairs are un-\nobserved. This problem arises in several \ufb01elds across science and engineering. For example, in\nsponsored search, each cluster is a submarket that represents a speci\ufb01c group of advertisers that do\nmost of their spending on a group of query phrases\u2014see e.g., Yahoo!-Inc (2009) for such a project\nat Yahoo. In VLSI and design automation, it is useful in minimizing signaling between compo-\nnents (Kernighan and Lin, 1970). In social networks, clusters may represent groups of people with\nc\u20dd2014 Yudong Chen, Ali Jalali, Sujay Sanghavi and Huan Xu.\narXiv:1104.4803v4  [cs.LG]  24 Jul 2014\nCHEN, JALALI, SANGHAVI AND XU\nsimilar interest or background; \ufb01nding clusters enables better recommendations and link predic-\ntion (Mishra et al., 2007). In the analysis of document databases, clustering the citation graph is\noften an essential and informative \ufb01rst step (Ester et al., 1995). In this paper, we will focus not on\nspeci\ufb01c application domains, but rather on the basic graph clustering problem itself.\nPartially observed graphs appear in many applications. For example, in online social networks\nlike Facebook, we observe an edge/no edge between two users when they accept each other as a\nfriend or explicitly decline a friendship suggestion. For the other user pairs, however, we simply\nhave no friendship information between them, which are thus unobserved. More generally, we have\npartial observations whenever obtaining similarity data is dif\ufb01cult or expensive (e.g., because it re-\nquires human participation). In these applications, it is often the case that most pairs are unobserved,\nwhich is the regime we are particularly interested in.\nAs with any clustering problem, we need a precise mathematical de\ufb01nition of the clustering\ncriterion with potentially a guaranteed performance. There is relatively few existing results with\nprovable performance guarantees for graph clustering with partially observed node pairs. Many\nexisting approaches to clustering fully observed graphs either require an additional input (e.g., the\nnumber of clusters k required for spectral or k-means clustering methods), or do not guarantee the\nperformance of the clustering. We review existing related work in Section 1.2.\n1.1 Our Approach\nWe focus on a natural formulation, one that does not require any other extraneous input besides\nthe graph itself. It is based on minimizing disagreements, which we now de\ufb01ne. Consider any\ncandidate clustering; this will have (a) observed node pairs that are in different clusters, but have\nan edge between them, and (b) observed node pairs that are in the same cluster, but do not have an\nedge between them. The total number of node pairs of types (a) and (b) is the number of disagree-\nments between the clustering and the given graph. We focus on the problem of \ufb01nding the optimal\nclustering\u2014one that minimizes the number of disagreements. Note that we do not pre-specify the\nnumber of clusters. For the special case of fully observed graphs, this formulation is exactly the\nsame as the problem of correlation clustering, \ufb01rst proposed by Bansal et al. (2002). They show\nthat exact minimization of the above objective is NP-hard in the worst case\u2014we survey and com-\npare with this and other related work in Section 1.2. As we will see, our approach and results are\ndifferent.\nWe aim to achieve the combinatorial disagreement minimization objective using matrix split-\nting via convex optimization. In particular, as we show in Section 2 below, one can represent the\nadjacency matrix of the given graph as the sum of an unknown low-rank matrix (corresponding to\n\u201cideal\u201d clusters) and a sparse matrix (corresponding to disagreements from this \u201cideal\u201d in the given\ngraph). Our algorithm either returns a clustering, which is guaranteed to be disagreement minimiz-\ning, or returns a \u201cfailure\u201d\u2014it never returns a sub-optimal clustering. For our main analytical result,\nwe evaluate our algorithm\u2019s performance on the natural and classical planted partition/stochastic\nblock model with partial observations. Our analysis provides stronger guarantees than are current\nresults on general matrix splitting (Cand`es et al., 2011; Hsu et al., 2011; Li, 2013; Chen et al.,\n2013). The algorithm, model and results are given in Section 2. We prove our theoretical results in\nSection 3 and provide empirical results in Section 4.\n2214\nCLUSTERING PARTIALLY OBSERVED GRAPHS VIA CONVEX OPTIMIZATION\n1.2 Related Work\nOur problem can be interpreted in the general clustering context as one in which the presence of an\nedge between two points indicates a \u201csimilarity\u201d, and the lack of an edge means \u201cno similarity\u201d. The\ngeneral \ufb01eld of clustering is of course vast, and a detailed survey of all methods therein is beyond\nour scope here. We focus instead on the three sets of papers most relevant to the problem here:\nthe work on correlation clustering, on the planted partition/stochastic block model, and on graph\nclustering with partial observations.\n1.2.1 CORRELATION CLUSTERING\nAs mentioned, for a completely observed graph, our problem is mathematically precisely the same\nas correlation clustering formulated in Bansal et al. (2002); in particular a \u201c+\u201d in correlation cluster-\ning corresponds to an edge in the graph, a \u201c-\u201d to the lack of an edge, and disagreements are de\ufb01ned in\nthe same way. Thus, this paper can equivalently be considered as an algorithm, and guarantees, for\ncorrelation clustering under partial observations. Since correlation clustering is NP-hard, there has\nbeen much work on devising alternative approximation algorithms (Bansal et al., 2002; Emmanuel\nand Fiat, 2003). Approximations using convex optimization, including LP relaxation (Charikar\net al., 2003; Demaine and Immorlica, 2003; Demaine et al., 2006) and SDP relaxation (Swamy,\n2004; Mathieu and Schudy, 2010), possibly followed by rounding, have also been developed. We\nemphasize that we use a different convex relaxation, and we focus on understanding when our con-\nvex program yields an optimal clustering without further rounding.\nWe note that Mathieu and Schudy (2010) use a convex formulation with constraints enforcing\npositive semi-de\ufb01niteness, triangle inequalities and \ufb01xed diagonal entries. For the fully observed\ncase, their relaxation is at least as tight as ours, and since they add more constraints, it is possible\nthat there are instances where their convex program works and ours does not. However, this seems\nhard to prove/disprove. Indeed, in the full observation setting they consider, their exact recovery\nguarantee is no better than ours. Moreover, as we argue in the next section, our guarantees are\norder-wise optimal in some important regimes and thus cannot be improved even with a tighter\nrelaxation. Practically, our method is faster since, to the best of our knowledge, there is no low-\ncomplexity algorithm to deal with the \u0398(n3) triangle inequality constraints required by Mathieu\nand Schudy (2010). This means that our method can handle large graphs while their result is practi-\ncally restricted to small ones (\u223c100 nodes). In summary, their approach has higher computational\ncomplexity, and does not provide signi\ufb01cant and characterizable performance gain in terms of exact\ncluster recovery.\n1.2.2 PLANTED PARTITION MODEL\nThe planted partition model, also known as the stochastic block-model (Condon and Karp, 2001;\nHolland et al., 1983), assumes that the graph is generated with in-cluster edge probability p and\ninter-cluster edge probability q (where p > q) and fully observed. The goal is to recover the latent\ncluster structure. A class of this model with \u03c4 \u225cmax{1 \u2212p, q} < 1\n2 is often used as benchmark\nfor average case performance for correlation clustering (see, e.g., Mathieu and Schudy, 2010). Our\ntheoretical results are applicable to this model and thus directly comparable with existing work\nin this area. A detailed comparison is provided in Table 1. For fully observed graphs, our result\nmatches the previous best bounds in both the minimum cluster size and the difference between\nin-cluster/inter-cluster densities. We would like to point out that nuclear norm minimization has\n2215\nCHEN, JALALI, SANGHAVI AND XU\nbeen used to solve the closely related planted clique problem (Alon et al., 1998; Ames and Vavasis,\n2011).\nPaper\nCluster size K\nDensity difference (1 \u22122\u03c4)\nBoppana (1987)\nn/2\n\u02dc\u2126( 1\n\u221an)\nJerrum and Sorkin (1998)\nn/2\n\u02dc\u2126(\n1\nn1/6\u2212\u03f5 )\nCondon and Karp (2001)\n\u02dc\u2126(n)\n\u02dc\u2126(\n1\nn1/2\u2212\u03f5 )\nCarson and Impagliazzo (2001)\nn/2\n\u02dc\u2126( 1\n\u221an)\nFeige and Kilian (2001)\nn/2\n\u02dc\u2126( 1\n\u221an)\nMcSherry (2001)\n\u02dc\u2126(n2/3)\n\u02dc\u2126(\nq\nn2\nK3 )\nBollob\u00b4as and Scott (2004)\n\u02dc\u2126(n)\n\u02dc\u2126(\nq\n1\nn)\nGiesen and Mitsche (2005)\n\u02dc\u2126(\u221an)\n\u02dc\u2126(\n\u221an\nK )\nShamir and Tsur (2007)\n\u02dc\u2126(\u221an)\n\u02dc\u2126(\n\u221an\nK )\nMathieu and Schudy (2010)\n\u02dc\u2126(\u221an)\n\u02dc\u2126(1)\nRohe et al. (2011)\n\u02dc\u2126(n3/4)\n\u02dc\u2126( n3/4\nK )\nOymak and Hassibi (2011)\n\u02dc\u2126(\u221an)\n\u02dc\u2126(\n\u221an\nK )\nChaudhuri et al. (2012)\n\u02dc\u2126(\u221an)\n\u02dc\u2126(\n\u221an\nK )\nThis paper\n\u02dc\u2126(\u221an)\n\u02dc\u2126(\n\u221an\nK )\nTable 1: Comparison with literature. This table shows the lower-bound requirements on the minimum clus-\nter size K and the density difference p\u2212q = 1\u22122\u03c4 that existing literature needs for exact recovery\nof the planted partitions, when the graph is fully observed and \u03c4 \u225cmax{1 \u2212p, q} = \u0398(1). Some\nof the results in the table only guarantee recovering the membership of most, instead of all, nodes.\nTo compare with these results, we use the soft-\u2126notation \u02dc\u2126(\u00b7), which hides the logarithmic factors\nthat are necessary for recovering all nodes, which is the goal of this paper.\n1.2.3 PARTIALLY OBSERVED GRAPHS\nThe previous work listed in Table 1, except Oymak and Hassibi (2011), does not handle partial\nobservations directly. One natural way to proceed is to impute the missing observations with no-\nedge, or random edges with symmetric probabilities, and then apply any of the results in Table 1.\nThis approach, however, leads to sub-optimal results. Indeed, this is done explicitly by Oymak and\nHassibi (2011). They require the probability of observation p0 to satisfy p0 \u2273\n\u221aKmin\nn\n, where n is\nthe number of nodes and Kmin is the minimum cluster size; in contrast, our approach only needs\np0 \u2273\nn\nK2\nmin (both right hand sides have to be less than 1, requiring Kmin \u2273\u221an, so the right hand\nside of our condition is order-wise smaller and thus less restrictive.) Shamir and Tishby (2011) deal\nwith partial observations directly and shows that p0 \u22731\nn suf\ufb01ces for recovering two clusters of size\n\u2126(n). Our result is applicable to much smaller clusters of size \u02dc\u2126(\u221an). In addition, a nice feature of\n2216\nCLUSTERING PARTIALLY OBSERVED GRAPHS VIA CONVEX OPTIMIZATION\nour result is that it explicitly characterizes the tradeoffs between the three relevant parameters: p0,\n\u03c4, and Kmin; theoretical result like this is not available in previous work.\nThere exists other work that considers partial observations, but under rather different settings.\nFor example, Balcan and Gupta (2010), Voevodski et al. (2010) and Krishnamurthy et al. (2012)\nconsider the clustering problem where one samples the rows/columns of the adjacency matrix rather\nthan its entries. Hunter and Strohmer (2010) consider partial observations in the features rather than\nin the similarity graph. Eriksson et al. (2011) show that \u02dc\u2126(n) actively selected pairwise similarities\nare suf\ufb01cient for recovering a hierarchical clustering structure. Their results seem to rely on the\nhierarchical structure. When disagreements are present, the \ufb01rst split of the cluster tree can recovers\nclusters of size \u2126(n); our results allow smaller clusters. Moreover, they require active control over\nthe observation process, while we assume random observations.\n2. Main Results\nOur setup for the graph clustering problem is as follows. We are given a partially observed graph\nof n nodes, whose adjacency matrix is A \u2208Rn\u00d7n, which has ai,j = 1 if there is an edge between\nnodes i and j, ai,j = 0 if there is no edge, and ai,j =\u201c?\u201d if we do not know. (Here we follow the\nconvention that ai,i = 0 for all i.) Let \u2126obs \u225c{(i, j) : ai,j \u0338=?} be the set of observed node pairs.\nThe goal to \ufb01nd the optimal clustering, i.e., the one that has the minimum number of disagreements\n(de\ufb01ned in Section 1.1) in \u2126obs.\nIn the rest of this section, we present our algorithm for the above task and analyze its perfor-\nmance under the planted partition model with partial observations. We also study the optimality of\nthe performance of our algorithm by deriving a necessary condition for any algorithm to succeed.\n2.1 Algorithm\nOur algorithm is based on convex optimization, and either (a) outputs a clustering that is guaranteed\nto be the one that minimizes the number of observed disagreements, or (b) declares \u201cfailure\u201d. In\nparticular, it never produces a suboptimal clustering.1 We now brie\ufb02y present the main idea and\nthen describe the algorithm.\nConsider \ufb01rst the fully observed case, i.e., every ai,j = 0 or 1. Suppose also that the graph is\nalready ideally clustered\u2014i.e., there is a partition of the nodes such that there is no edge between\nclusters, and each cluster is a clique. In this case, the matrix A + I is now a low-rank matrix, with\nthe rank equal to the number of clusters. This can be seen by noticing that if we re-order the rows\nand columns so that clusters appear together, the result would be a block-diagonal matrix, with each\nblock being an all-ones submatrix\u2014and thus rank one. Of course, this re-ordering does not change\nthe rank of the matrix, and hence A + I is exactly low-rank.\nConsider now any given graph, still fully observed. In light of the above, we are looking for a\ndecomposition of its A + I into a low-rank part K\u2217(of block-diagonal all-ones, one block for each\ncluster) and a remaining B\u2217(the disagreements), such that the number of non-zero entries in B\u2217is\nas small as possible; i.e., B\u2217is sparse. Finally, the problem we look at is recovery of the best K\u2217\nwhen we do not observe all entries of A + I. The idea is depicted in Figure 1.\n1. In practice, one might be able to use the \u201cfailed\u201d output with rounding as an approximate solution. In this paper, we\nfocus on the performance of the unrounded algorithm.\n2217\nCHEN, JALALI, SANGHAVI AND XU\nFigure 1: The adjacency matrix of a graph drawn from the planted partition model before and after proper\nreordering (i.e., clustering) of the nodes. The \ufb01gure on the right is indicative of the matrix as a\nsuperposition of a sparse matrix and a low-rank block diagonal one.\nWe propose to perform the matrix splitting using convex optimization (Chandrasekaran et al.,\n2011; Cand`es et al., 2011). Our approach consists of dropping any additional structural require-\nments, and just looking for a decomposition of the given A + I as the sum of a sparse matrix B and\na low-rank matrix K. Recall that \u2126obs is the set of observed entries, i.e., the set of elements of A\nthat are known to be 0 or 1; we use the following convex program:\nmin\nB,K\n\u03bb \u2225B\u22251 + \u2225K\u2225\u2217\ns.t.\nP\u2126obs(B + K) = P\u2126obs(A + I).\n(1)\nHere, for any matrix M, the term P\u2126obs(M) keeps all elements of M in \u2126obs unchanged, and sets\nall other elements to 0; the constraints thus state that the sparse and low-rank matrix should in sum\nbe consistent with the observed entries of A + I. The term \u2225B\u22251 = P\ni,j |bi,j| is the \u21131 norm of\nthe entries of the matrix B, which is well-known to be a convex surrogate for the number of non-\nzero entries \u2225B\u22250. The second term \u2225K\u2225\u2217= P\ns \u03c3s(K) is the nuclear norm (also known as the\ntrace norm), de\ufb01ned as the sum of the singular values of K. This has been shown to be the tightest\nconvex surrogate for the rank function for matrices with unit spectral norm (Fazel, 2002). Thus our\nobjective function is a convex surrogate for the (natural) combinatorial objective \u03bb\u2225B\u22250 +rank(K).\nThe optimization problem (1) is, in fact, a semide\ufb01nite program (SDP) (Chandrasekaran et al.,\n2011).\nWe remark on the above formulation. (a) This formulation does not require specifying the\nnumber of clusters; this parameter is effectively learned from the data. The tradeoff parameter \u03bb is\narti\ufb01cial and can be easily determined: since any desired K\u2217has trace exactly equal to n, we simply\nchoose the smallest \u03bb such that the trace of the optimal solution is at least n. This can be done\nby, e.g., bisection, which is described below. (b) It is possible to obtain tighter convex relaxations\nby adding more constraints, such as the diagonal entry constraints ki,i = 1, \u2200i, the positive semi-\nde\ufb01nite constraint K \u2ab00, or even the triangular inequalities ki,j + kj,k \u2212ki,k \u22641. Indeed, this is\ndone by Mathieu and Schudy (2010). Note that the guarantees for our formulation (to be presented\nin the next subsection) automatically imply guarantees for any other tighter relaxations. We choose\nto focus on our (looser) formulation for two reasons. First, and most importantly, even with the extra\n2218\nCLUSTERING PARTIALLY OBSERVED GRAPHS VIA CONVEX OPTIMIZATION\nconstraints, Mathieu and Schudy (2010) do not deliver better exact recovery guarantees (cf. Table 1).\nIn fact, we show in Section 2.3 that our results are near optimal in some important regimes, so tighter\nrelaxations do not seem to provide additional bene\ufb01ts in exact recovery. Second, our formulation can\nbe solved ef\ufb01ciently using existing Augmented Lagrangian Multiplier methods (Lin et al., 2009).\nThis is no longer the case with the \u0398(n3) triangle inequality constraints enforced by Mathieu and\nSchudy (2010), and solving it as a standard SDP is only feasible for small graphs.\nWe are interested in the case when the convex program (1) produces an optimal solution K that\nis a block-diagonal matrix and corresponds to an ideal clustering.\nDe\ufb01nition 1 (Validity) The convex program (1) is said to produce a valid output if the low-rank\nmatrix part K of the optimal solution corresponds to a graph of disjoint cliques; i.e., its rows and\ncolumns can be re-ordered to yield a block-diagonal matrix with all ones for each block.\nValidity of a given K can be easily checked via elementary re-ordering operations.2 Our \ufb01rst simple\nbut useful insight is that whenever the convex program (1) yields a valid solution, it is the disagree-\nment minimizer.\nTheorem 2 For any \u03bb > 0, if the solution of (1) is valid, then it is the clustering that minimizes the\nnumber of observed disagreements.\nOur complete clustering procedure is given as Algorithm 1. It takes the adjacency matrix of the\ngraph A and outputs either the optimal clustering or declares failure. Setting the parameter \u03bb is\ndone via binary search. The initial value of \u03bb is not crucial; we use \u03bb =\n1\n32\u221a\u00afp0n based on our\ntheoretical analysis in the next sub-section, where \u00afp0 is the empirical fraction of observed pairs.\nTo solve the optimization problem (1), we use the fast algorithm developed by Lin et al. (2009),\nwhich is tailored for matrix splitting and takes advantage of the sparsity of the observations. By\nTheorem 2, whenever the algorithm results in a valid K, we have found the optimal clustering.\nAlgorithm 1 Optimal-Cluster(A)\n\u03bb \u2190\n1\n32\u221a\u00afp0n\nwhile not terminated do\nSolve (1) to obtain the solution (B, K)\nif K is valid then\nOutput the clustering in K and EXIT.\nelse if trace(K) > n then\n\u03bb \u2190\u03bb/2\nelse if trace(K) < n then\n\u03bb \u21902\u03bb\nend if\nend while\nDeclare Failure.\n2. If we re-order a valid K such that identical rows and columns appear together, it will become block-diagonal.\n2219\nCHEN, JALALI, SANGHAVI AND XU\n2.2 Performance Analysis\nFor the main analytical contribution of this paper, we provide conditions under which the above\nalgorithm will \ufb01nd the clustering that minimizes the number of disagreements among the observed\nentries. In particular, we characterize its performance under the standard and classical planted\npartition/stochastic block model with partial observations, which we now describe.\nDe\ufb01nition 3 (Planted Partition Model with Partial Observations) Suppose that n nodes are par-\ntitioned into r clusters, each of size at least Kmin. Let K\u2217be the low-rank matrix corresponding to\nthis clustering (as described above). The adjacency matrix A of the graph is generated as follows:\nfor each pair of nodes (i, j) in the same cluster, ai,j =? with probability 1 \u2212p0, ai,j = 1 with\nprobability p0p, or aij = 0 otherwise, independent of all others; similarly, for (i, j) in different\nclusters, ai,j =? with probability 1 \u2212p0, ai,j = 1 with probability p0q, or ai,j = 0 otherwise.\nUnder the above model, the graph is observed at locations chosen at random with probability p0.\nIn expectation a fraction of 1 \u2212p of the in-cluster observations are disagreements; similarly, the\nfraction of disagreements in the across-cluster observations is q. Let B\u2217= P\u2126obs(A + I \u2212K\u2217)\nbe the matrix of observed disagreements for the original clustering; note that the support of B\u2217\nis contained in \u2126obs. The following theorem provides a suf\ufb01cient condition for our algorithm to\nrecover the original clustering (B\u2217, K\u2217) with high probability. Combined with Theorem 2, it also\nshows that under the same condition, the original clustering is disagreement minimizing with high\nprobability.\nTheorem 4 Let \u03c4 = max{1 \u2212p, q}. There exist universal positive constants c and C such that,\nwith probability at least 1 \u2212cn\u221210, the original clustering (B\u2217, K\u2217) is the unique optimal solution\nof (1) with \u03bb =\n1\n32\u221anp0 provided that\np0 (1 \u22122\u03c4)2 \u2265C n log2 n\nK2\nmin\n.\n(2)\nNote that the quantity \u03c4 is (an upper bound of) the probability of having a disagreement, and 1 \u22122\u03c4\nis (a lower bound of) the density gap p\u2212q. The suf\ufb01cient condition in the theorem is given in terms\nof the three parameters that de\ufb01ne problem: the minimum cluster size Kmin, the density gap 1\u22122\u03c4,\nand the observation probability p0. We remark on these parameters.\n\u2022 Minimum cluster size Kmin. Since the left hand side of the condition (2) in Theorem 4 is no\nmore than 1, it imposes a lower-bound Kmin = \u02dc\u2126(\u221an) on the cluster sizes. This means that\nour method can handle a growing number \u02dcO(\u221an) of clusters. The lower-bound on Kmin is\nattained when 1 \u22122\u03c4 and p0 are both \u0398(1), i.e., not decreasing as n grows. Note that all\nrelevant works require a lower-bound at least as strong as ours (cf. Table 1).\n\u2022 Density gap 1 \u22122\u03c4. When p0 = \u0398(1), our result allows this gap to be vanishingly small, i.e.,\n\u02dc\u2126\n\u0010 \u221an\nKmin\n\u0011\n, where a larger Kmin allows for a smaller gap. As we mentioned in Section 1.2,\nthis matches the best available results (cf. Table 1), including those in Mathieu and Schudy\n(2010) and Oymak and Hassibi (2011), which use tighter convex relaxations that are more\ncomputationally demanding. We note that directly applying existing results in the low-rank-\nplus-sparse literature (Cand`es et al., 2011; Li, 2013) leads to weaker results, where the gap be\nbounded below by a constant.\n2220\nCLUSTERING PARTIALLY OBSERVED GRAPHS VIA CONVEX OPTIMIZATION\n\u2022 Observation probability p0. When 1 \u22122\u03c4 = \u0398(1), our result only requires a vanishing\nfraction of observations, i.e., p0 can be as small as \u02dc\u0398\n\u0010\nn\nK2\nmin\n\u0011\n; a larger Kmin allows for a\nsmaller p0. As mentioned in Section 1.2, this scaling is better than prior results we know of.\n\u2022 Tradeoffs. A novel aspect of our result is that it shows an explicit tradeoff between the ob-\nservation probability p0 and the density gap 1 \u22122\u03c4. The left hand side of (2) is linear in p0\nand quadratic in 1 \u22122\u03c4. This means if the number of observations is four times larger, then\nwe can handle a 50% smaller density gap. Moreover, p0 can go to zero quadratically faster\nthen 1 \u22122\u03c4. Consequently, treating missing observations as disagreements would lead to\nquadratically weaker results. This agrees with the intuition that handling missing entries with\nknown locations is easier than correcting disagreements whose locations are unknown.\nWe would like to point out that our algorithm has the capability to handle outliers. Suppose there\nare some isolated nodes which do not belong to any cluster, and they connect to each other and each\nnode in the clusters with probability at most \u03c4, with \u03c4 obeying the condition (2) in Theorem 4. Our\nalgorithm will classify all these edges as disagreements, and hence automatically reveal the identity\nof each outlier. In the output of our algorithm, the low rank part K will have all zeros in the columns\nand rows corresponding to outliers\u2014all their edges will appear in the disagreement matrix B.\n2.3 Lower Bounds\nWe now discuss the tightness of Theorem 4. Consider \ufb01rst the case where Kmin = \u0398(n), which\nmeans there are a constant number of clusters. We establish a fundamental lower bound on the\ndensity gap 1\u22122\u03c4 and the observation probability p0 that are required for any algorithm to correctly\nrecover the clusters.\nTheorem 5 Under the planted partition model with partial observations, suppose the true clus-\ntering is chosen uniformly at random from all possible clusterings with equal cluster size K. If\nK = \u0398(n) and \u03c4 = 1 \u2212p = q > 1/100, then for any algorithm to correctly identify the clusters\nwith probability at least 3\n4, we need\np0(1 \u22122\u03c4)2 \u2265C 1\nn,\nwhere C > 0 is an absolute constant.\nTheorem 5 generalizes a similar result in Chaudhuri et al. (2012), which does not consider partial\nobservations. The theorem applies to any algorithm regardless of its computational complexity, and\ncharacterizes the fundamental tradeoff between p0 and 1 \u22122\u03c4. It shows that when Kmin = \u0398(n),\nthe requirement for 1 \u22122\u03c4 and p0 in Theorem 4 is optimal up to logarithmic factors, and cannot be\nsigni\ufb01cantly improved by using more complicated methods.\nFor the general case with Kmin = O(n), only part of the picture is known. Using non-rigorous\narguments, Decelle et al. (2011) show that 1 \u22122\u03c4 \u2273\n\u221an\nKmin is necessary when \u03c4 = \u0398(1) and the\ngraph is fully observed; otherwise recovery is impossible or computationally hard. According to\nthis lower-bound, our requirement on the density gap 1 \u22122\u03c4 is probably tight (up to log factors)\nfor all Kmin. However, a rigorous proof of this claim is still lacking, and seems to be a dif\ufb01cult\nproblem. Similarly, the tightness of our condition on p0 and the tradeoff between p0 and \u03c4 is also\nunclear in this regime.\n2221\nCHEN, JALALI, SANGHAVI AND XU\n3. Proofs\nIn this section, we prove Theorems 2 and 4. The proof of Theorem 5 is deferred to Appendix B.\n3.1 Proof of Theorem 2\nWe \ufb01rst prove Theorem 2, which says that if the optimization problem (1) produces a valid matrix,\ni.e., one that corresponds to a clustering of the nodes, then this is the disagreement minimizing\nclustering. Consider the following non-convex optimization problem\nmin\nB,K\n\u03bb \u2225B\u22251 + \u2225K\u2225\u2217\ns.t.\nP\u2126obs(B + K) = P\u2126obs(I + A),\nK is valid,\n(3)\nand let (B, K) be any feasible solution. Since K represents a valid clustering, it is positive semidef-\ninite and has all ones along its diagonal. Therefore, it obeys \u2225K\u2225\u2217= trace(K) = n. On the other\nhand, because both K\u2212I and A are adjacency matrices, the entries of B = I+A\u2212K in \u2126obs must\nbe equal to \u22121, 1 or 0 (i.e., it is a disagreement matrix). Clearly any optimal B must have zeros at\nthe entries in \u2126c\nobs. Hence \u2225B\u22251 = \u2225P\u2126obs(B)\u22250 when K is valid. We thus conclude that the above\noptimization problem (3) is equivalent to minimizing \u2225P\u2126obs(B)\u22250 subject to the same constraints.\nThis is exactly the minimization of the number of disagreements on the observed edges. Now notice\nthat (1) is a relaxed version of (3). Therefore, if the optimal solution of (1) is valid and thus feasible\nto (3), then it is also optimal to (3), the disagreement minimization problem.\n3.2 Proof of Theorem 4\nWe now turn to the proof of Theorem 4, which provides guarantees for when the convex program (1)\nrecovers the true clustering (B\u2217, K\u2217).\n3.2.1 PROOF OUTLINE AND PRELIMINARIES\nWe overview the main steps in the proof of Theorem 4; details are provided in Sections 3.2.2\u20133.2.4\nto follow. We would like to show that the pair (B\u2217, K\u2217) corresponding to the true clustering is the\nunique optimal solution to our convex program (1). This involves the following three steps.\nStep 1: We show that it suf\ufb01ces to consider an equivalent model for the observation and dis-\nagreements. This model is easier to handle, especially when the observation probability and density\ngap are vanishingly small, which is the regime of interest in this paper.\nStep 2: We write down the sub-gradient based \ufb01rst-order suf\ufb01cient conditions that need to\nbe satis\ufb01ed for (B\u2217, K\u2217) to be the unique optimum of (1). In our case, this involves showing\nthe existence of a matrix W\u2014the dual certi\ufb01cate\u2014that satis\ufb01es certain properties. This step is\ntechnical\u2014requiring us to deal with the intricacies of sub-gradients since our convex function is not\nsmooth\u2014but otherwise standard. Luckily for us, this has been done previously (Chandrasekaran\net al., 2011; Cand`es et al., 2011; Li, 2013).\nStep 3: Using the assumptions made on the true clustering K\u2217and its disagreements B\u2217, we\nconstruct a candidate dual certi\ufb01cate W that meets the requirements in step 2, and thus certify\n(B\u2217, K\u2217) as being the unique optimum.\n2222\nCLUSTERING PARTIALLY OBSERVED GRAPHS VIA CONVEX OPTIMIZATION\nThe crucial Step 3 is where we go beyond the existing literature on matrix splitting (Chan-\ndrasekaran et al., 2011; Cand`es et al., 2011; Li, 2013). These results assume the observation prob-\nability and/or density gap is at least a constant, and hence do not apply to our setting. Here we\nprovide a re\ufb01ned analysis, which leads to better performance guarantees than those that could be\nobtained via a direct application of existing sparse and low-rank matrix splitting results.\nNext, we introduce some notations used in the rest of the proof of the theorem. The following\nde\ufb01nitions related to K\u2217are standard. By symmetry, the SVD of K\u2217has the form U\u03a3UT , where\nU \u2208Rn\u00d7r contains the singular vectors of K\u2217. We de\ufb01ne the subspace\nT \u225c\n\b\nUXT + YUT : X, Y \u2208Rn\u00d7r\t\n,\nwhich is spanned of all matrices that share either the same column space or the same row space as\nK\u2217. For any matrix M \u2208Rn\u00d7n, its orthogonal projection to the space T is given by PT (M) =\nUUT M + MUUT \u2212UUT MUUT . The projection onto T \u22a5, the complement orthogonal space\nof T , is given by PT \u22a5(M) = M \u2212PT (M).\nThe following de\ufb01nitions are related to B\u2217and partial observations. Let \u2126\u2217= {(i, j) : b\u2217\ni,j \u0338=\n0} be the set of matrix entries corresponding to the disagreements. Recall that \u2126obs is the set of\nobserved entries. For any matrix M and entry set \u21260, we let P\u21260 (M) \u2208Rn\u00d7n be the matrix\nobtained from M by setting all entries not in the set \u21260 to zero. We write \u21260 \u223cBer0(p) if the\nentry set \u21260 does not contain the diagonal entries, and each pair (i, j) and (j, i) (i \u0338= j) is contained\nin \u21260 with probability p, independent all others; \u21260 \u223cBer1(p) is de\ufb01ned similarly except that \u21260\ncontains all the diagonal entries. Under our partially observed planted partition model, we have\n\u2126obs \u223cBer1(p0) and \u2126\u2217\u223cBer0(\u03c4).\nSeveral matrix norms are used. \u2225M\u2225and \u2225M\u2225F represent the spectral and Frobenius norms of\nthe matrix M, respectively, and \u2225M\u2225\u221e\u225cmaxi,j |mi,j| is the matrix in\ufb01nity norm.\n3.2.2 STEP 1: EQUIVALENT MODEL FOR OBSERVATIONS AND DISAGREEMENTS\nIt is easy show that increasing p or decreasing q can only make the probability of success higher,\nso without loss of generality we assume 1 \u2212p = q = \u03c4. Observe that the probability of success\nis completely determined by the distribution of (\u2126obs, B\u2217) under the planted partition model with\npartial observations. The \ufb01rst step is to show that it suf\ufb01ces to consider an equivalent model for\ngenerating (\u2126obs, B\u2217), which results in the same distribution but is easier to handle. This is in the\nsame spirit as Cand`es et al. (2011, Theorems 2.2 and 2.3) and Li (2013, Section 4.1). In particular,\nwe consider the following procedure:\n1. Let \u0393 \u223cBer1 (p0(1 \u22122\u03c4)), and \u2126\u223cBer0\n\u0010\n2p0\u03c4\n1\u2212p0+2p0\u03c4\n\u0011\n. Let \u2126obs = \u0393 \u222a\u2126.\n2. Let S be a symmetric random matrix whose upper-triangular entries are independent and\nsatisfy P(si,j = 1) = P(si,j = \u22121) = 1\n2.\n3. De\ufb01ne \u2126\u2032 \u2286\u2126as \u2126\u2032 =\nn\n(i, j) : (i, j) \u2208\u2126, si,j = 1 \u22122k\u2217\ni,j\no\n. In other words, \u2126\u2032 is the\nentries of S whose signs are consistent with a disagreement matrix.\n4. De\ufb01ne \u2126\u2217= \u2126\u2032\\\u0393, and \u02dc\u0393 = \u2126obs\\\u2126\u2217.\n5. Let B\u2217= P\u2126\u2217(S).\n2223\nCHEN, JALALI, SANGHAVI AND XU\nIt is easy to verify that (\u2126obs, B\u2217) has the same distribution as in the original model. In particular,\nwe have P[(i, j) \u2208\u2126obs] = p0, P[(i, j) \u2208\u2126\u2217, (i, j) \u2208\u2126obs] = p0\u03c4 and P[(i, j) \u2208\u2126\u2217, (i, j) /\u2208\n\u2126obs] = 0, and observe that given K\u2217, B\u2217is completely determined by its support \u2126\u2217.\nThe advantage of the above model is that \u0393 and \u2126are independent of each other, and S has\nrandom signed entries. This facilitates the construction of the dual certi\ufb01cate, especially in the\nregime of vanishing p0 and\n\u0000 1\n2 \u2212\u03c4\n\u0001\nconsidered in this paper. We use this equivalent model in the\nrest of the proof.\n3.2.3 STEP 2: SUFFICIENT CONDITIONS FOR OPTIMALITY\nWe state the \ufb01rst-order conditions that guarantee (B\u2217, K\u2217) to be the unique optimum of (1) with\nhigh probability. Here and henceforth, by with high probability we mean with probability at least\n1\u2212cn\u221210 for some universal constant c > 0. The following lemma follows from Theorem 4.4 in Li\n(2013) and the discussion thereafter.\nLemma 6 (Optimality Condition) Suppose\n\r\r\r\n1\n(1\u22122\u03c4)p0 PT P\u0393PT \u2212PT\n\r\r\r \u22641\n2. Then (B\u2217, K\u2217) is\nthe unique optimal solution to (1) with high probability if there exists W \u2208Rn\u00d7n such that\n1.\n\r\rPT (W + \u03bbP\u2126S \u2212UU\u22a4)\n\r\r\nF \u2264\n\u03bb\nn2 ,\n2. \u2225PT \u22a5(W + \u03bbP\u2126S)\u2225\u22641\n4,\n3. P\u0393c(W) = 0,\n4. \u2225P\u0393(W)\u2225\u221e\u2264\u03bb\n4.\nLemma 9 in the appendix guarantees that the condition\n\r\r\r\n1\n(1\u22122\u03c4)p0 PT P\u0393PT \u2212PT\n\r\r\r \u22641\n2 is satis\ufb01ed\nwith high probability under the assumption of Theorem 4. It remains to show the existence of a\ndesired dual certi\ufb01cate W which satis\ufb01es the four conditions in Lemma 6 with high probability.\n3.2.4 STEP 3: DUAL CERTIFICATE CONSTRUCTION\nWe use a variant of the so-called gol\ufb01ng scheme (Cand`es et al., 2011; Gross, 2011) to construct\nW. Our application of gol\ufb01ng scheme, as well as its analysis, is different from previous work and\nleads to stronger guarantees. In particular, we go beyond existing results by allowing the fraction of\nobserved entries and the density gap to be vanishing.\nBy de\ufb01nition in Section 3.2.2, \u0393 obeys \u0393 \u223cBer1(p0(1\u22122\u03c4)). Observe that \u0393 may be considered\nto be generated by \u0393 = S\n1\u2264k\u2264k0 \u0393k, where the sets \u0393k \u223cBer1(t) are independent; here the\nparameter t obeys p0(1 \u22122\u03c4) = 1 \u2212(1 \u2212t)k0, and k0 is chosen to be \u23085 log n\u2309. This implies t \u2265\np0(1\u22122\u03c4)/k0 \u2265C0\nn log n\nK2\nmin for some constant C0, with the last inequality holds under the assumption\nof Theorem 4. For any random entry set \u21260 \u223cBer1(\u03c1), de\ufb01ne the operator R\u21260 : Rn\u00d7n 7\u2192Rn\u00d7n\nby\nR\u21260(M) =\nn\nX\ni=1\nmi,ieieT\ni + \u03c1\u22121\nX\n1\u2264i<j\u2264n\n\u03b4ijmi,j\n\u0000eieT\nj + ejeT\ni\n\u0001\n,\nwhere \u03b4ij is the indicator random variable with \u03b4ij = 1 if (i, j) \u2208\u21260 and 0 otherwise, and ei is the\ni-th standard basis in Rn\u00d7n, i.e., the column vector with 1 in its i-th entry and 0 elsewhere.\n2224\nCLUSTERING PARTIALLY OBSERVED GRAPHS VIA CONVEX OPTIMIZATION\nWe now de\ufb01ne our dual certi\ufb01cate. Let W = Wk0, where Wk0 is de\ufb01ned recursively by setting\nW0 = 0 and for all k = 1, 2, . . . , k0,\nWk = Wk\u22121 + R\u0393kPT\n\u0000UUT \u2212\u03bbPT (P\u2126(S)) \u2212Wk\u22121\n\u0001\n.\nClearly the equality condition in Lemma 6 is satis\ufb01ed. It remains to show that W also satis\ufb01es\nthe inequality conditions with high probability. The proof makes use of the technical Lemmas 9\u2013\n12 given in the appendix. For convenience of notation, we de\ufb01ne the quantity \u2206k = UUT \u2212\n\u03bbPT (P\u2126(S)) \u2212PT (Wk), and use the notation\nk\nY\ni=1\n(PT \u2212PT R\u0393iPT ) = (PT \u2212PT R\u0393kPT ) \u00b7 \u00b7 \u00b7 (PT \u2212PT R\u03931PT ),\nwhere the order of multiplication is important. Observe that by construction of W, we have\n\u2206k =\nk\nY\ni=1\n(PT \u2212PT R\u0393iPT )(UUT \u2212\u03bbPT P\u2126(S)), k = 1, . . . , k0,\n(4)\nWk0 =\nk0\nX\nk=1\nR\u0393k\u2206k\u22121.\n(5)\nWe are ready to prove that W satis\ufb01es inequalities 1, 2 and 4 in Lemma 6.\nInequality 1: Thanks to (4), we have the following geometric convergence :\n\r\r\rPT (W + \u03bbP\u2126S \u2212UU\u22a4)\n\r\r\r\nF = \u2225\u2206k0\u2225F\n\u2264\n k0\nY\nk=1\n\u2225PT \u2212PT R\u0393kPT \u2225\n!\n\r\rUUT \u2212\u03bbPT P\u2126(S)\n\r\r\nF\n(a)\n\u2264e\u2212k0(\n\r\rUUT \r\r\nF + \u03bb \u2225PT P\u2126(S)\u2225F )\n(b)\n\u2264n\u22125(n + \u03bb \u00b7 n) \u2264(1 + \u03bb)n\u22124\n(c)\n\u2264\n1\n2n2 \u03bb.\nHere, the inequality (a) follows Lemma 9 with \u03f51 = e\u22121, (b) follows from our choices of \u03bb and k0\nand the fact that \u2225PT P\u2126(S)\u2225F \u2264\u2225P\u2126(S)\u2225F \u2264n, and (c) holds under the assumption \u03bb \u2265\n1\n32\u221an\nin the theorem.\nInequality 4: We have\n\u2225P\u0393(W)\u2225\u221e= \u2225P\u0393(Wk0)\u2225\u221e\u2264\nk0\nX\nk=1\n\u2225R\u0393i\u2206i\u22121\u2225\u221e\u2264t\u22121\nk0\nX\nk=1\n\u2225\u2206k\u22121\u2225\u221e,\n2225\nCHEN, JALALI, SANGHAVI AND XU\nwhere the \ufb01rst inequality follows from (5) and the triangle inequality. We proceed to obtain\nk0\nX\nk=1\n\u2225\u2206k\u22121\u2225\u221e\n(a)\n=\nk0\nX\nk=1\n\r\r\r\r\r\nk\u22121\nY\ni=1\n(PT \u2212PT R\u0393iPT )(UUT \u2212\u03bbPT P\u2126(S))\n\r\r\r\r\r\n\u221e\n(b)\n\u2264\nk0\nX\nk=1\n\u00121\n2\n\u0013k \r\rUUT \u2212\u03bbPT P\u2126(S)\n\r\r\n\u221e\n(c)\n\u2264\n1\nKmin\n+ \u03bb\ns\np0n log n\nK2\nmin\n,\n(6)\nwhere (a) follows from (4), (b) follows from Lemma 11 and (c) follows from Lemma 12. It follows\nthat\n\u2225P\u0393(W)\u2225\u221e\u22641\nt\n\u0012\n1\nKmin\n+ n log n\nK2\nmin\n\u03bb\n\u0013\n\u2264\nk0\np0(1 \u22122\u03c4)\n \n1\nKmin\n+ \u03bb\ns\np0n log n\nK2\nmin\n!\n\u22641\n4\u03bb,\nwhere the last inequality holds under the assumption of Theorem 4\nInequality 2: Observe that by the triangle inequality, we have\n\u2225PT \u22a5(W + \u03bbP\u2126(S))\u2225\u2264\u03bb \u2225PT \u22a5(P\u2126(S))\u2225+ \u2225PT \u22a5(Wk0)\u2225.\nFor the \ufb01rst term, standard results on the norm of a matrix with i.i.d. entries (e.g., see Vershynin\n2010) give\n\u03bb \u2225PT \u22a5(P\u2126(S))\u2225\u2264\u03bb \u2225P\u2126(S)\u2225\u2264\n1\n32\u221ap0n \u00b7 4\nr\n2p0\u03c4n\n1 \u2212p0 + 2p0\u03c4 \u22641\n8\nIt remains to show that the second term is bounded by 1\n8. To this end, we observe that\n\u2225PT \u22a5(Wk0)\u2225\n(a)\n=\nk0\nX\nk=1\n\u2225PT \u22a5(R\u0393k\u2206k\u22121 \u2212\u2206k\u22121)\u2225\n\u2264\nk0\nX\nk=1\n\u2225(R\u0393k \u2212I) \u2206k\u22121\u2225\n(b)\n\u2264C\nr\nn log n\nt\nk0\nX\nk=1\n\u2225\u2206k\u22121\u2225\u221e\n(c)\n\u2264C\ns\nk0n log n\np0(1 \u22122\u03c4)\n \n1\nKmin\n+ \u03bb\ns\np0n log n\nK2\nmin\n!\n\u22641\n8,\nwhere in (a) we use (5) and the fact that \u2206k \u2208T , (b) follows from Lemma 10, and (c) follows\nfrom (6). This completes the proof of Theorem 4.\n2226\nCLUSTERING PARTIALLY OBSERVED GRAPHS VIA CONVEX OPTIMIZATION\n0\n0.2\n0.4\n0.6\n0.8\n0\n0.2\n0.4\n0.6\n0.8\n1\np 0\nProb. of Success\n \n \nn = 200\nn = 400\nn = 1000\nn = 2000\n0\n50\n100\n150\n200\n250\n0\n0.2\n0.4\n0.6\n0.8\n1\np 0n / log(n )\nProb. of Success\n \n \nn = 200\nn = 400\nn = 1000\nn = 2000\nFigure 2: Simulation results verifying the performance of our algorithm as a function of the observation\nprobability p0 and the graph size n. The left pane shows the probability of successful recovery\nunder different p0 and n with \ufb01xed \u03c4 = 0.2 and Kmin = n/4; each point is an average over 5\ntrials. After proper rescaling of the x-axis, the curves align as shown in the right pane, indicating\na good match with our theoretical results.\n4. Experimental Results\nWe explore via simulation the performance of our algorithm as a function of the values of the model\nparameters (n, Kmin, p0, \u03c4). We see that the performance matches well with the theory.\nIn the experiment, each test case is constructed by generating a graph with n nodes divided into\nclusters of equal size Kmin, and then placing a disagreement on each pair of node with probability \u03c4\nindependently. Each node pair is then observed with probability p0. We then run Algorithm 1, where\nthe optimization problem (1) is solved using the fast algorithm in Lin et al. (2009).. We check if the\nalgorithm successfully outputs a solution that equals to the underlying true clusters. In the \ufb01rst set\nof experiments, we \ufb01x \u03c4 = 0.2 and Kmin = n/4 and vary p0 and n. For each (p0, n), we repeat the\nexperiment for 5 times and plot the probability of success in the left pane of Figure 2.\nOne observes that our algorithm has better performance with larger p0 and n, and the success\nprobability exhibits a phase transition. Theorem 4 predicts that, with \u03c4 \ufb01xed and Kmin = n/4, the\ntransition occurs at p0 \u221dn log2 n\nK2\nmin\n\u221dlog2 n\nn\n; in particular, if we plot the success probability against\nthe control parameter\np0n\nlog2 n, all curves should align with each other. Indeed, this is precisely what\nwe see in the right pane of Figure 2 where we use\np0n\nlog n as the control parameter. This shows that\nTheorem 4 gives the correct scaling between p0 and n up to an extra log factor.\nIn a similar fashion, we run another three sets of experiments with the following settings: (1)\nn = 1000 and \u03c4 = 0.2 with varying (p0, Kmin); (2) Kmin = n/4 and p0 = 0.2 with varying (\u03c4, n);\n(3) n = 1000 and p0 = 0.6 with varying (\u03c4, Kmin). The results are shown in Figures 3, 4 and 5;\nnote that each x-axis corresponds to a control parameter chosen according to the scaling predicted\nby Theorem 4. Again we observe that all the curves roughly align, indicating a good match with the\ntheory. In particular, by comparing Figures 2 and 4 (or Figures 3 and 5), one veri\ufb01es the quadratic\ntradeoff between observations and disagreements (i.e., p0 vs. 1 \u22122\u03c4) as predicted by Theorem 4.\nFinally, we compare the performance of our method with spectral clustering, a popular method\nfor graph clustering. For spectral clustering, we \ufb01rst impute the missing entries of the adjacency\n2227\nCHEN, JALALI, SANGHAVI AND XU\n0\n0.005\n0.01\n0.015\n0\n0.2\n0.4\n0.6\n0.8\n1\np 0K 2\nm in\nProb. of Success\n \n \nKmin = 125\nKmin = 100\nKmin = 50\nFigure 3: Simulation results verifying the performance of our algorithm as a function of the observation\nprobability p0 and the cluster size Kmin, with n = 1000 and \u03c4 = 0.2 \ufb01xed.\n0\n5\n10\n15\n20\n0\n0.2\n0.4\n0.6\n0.8\n1\n(1 \u22122\u03c4 )\np\nn / log(n )\nProb. of Success\n \n \nn = 200\nn = 400\nn = 1000\nn = 2000\nFigure 4: Simulation results verifying the performance of our algorithm as a function of the disagreement\nprobability \u03c4 and the graph size n, with p0 = 0.2 and Kmin = n/4 \ufb01xed.\nmatrix with either zeros or random 1/0\u2019s. We then compute the \ufb01rst k principal components of the\nadjacency matrix, and run k-means clustering on the principal components (von Luxburg, 2007);\nhere we set k equal to the number of clusters. The adjacency matrix is generated in the same fashion\nas before using the parameters n = 2000, Kmin = 200 and \u03c4 = 0.1. We vary the observation proba-\nbility p0 and plot the success probability in Figure 6. It can be observed that our method outperforms\nspectral clustering with both imputation schemes; in particular, it requires fewer observations.\n5. Conclusion\nWe proposed a convex optimization formulation, based on a reduction to decomposing low-rank\nand sparse matrices, to address the problem of clustering partially observed graphs. We showed\nthat under a wide range of parameters of the planted partition model with partial observations, our\nmethod is guaranteed to \ufb01nd the optimal (disagreement-minimizing) clustering. In particular, our\nmethod succeeds under higher levels of noise and/or missing observations than existing methods in\n2228\nCLUSTERING PARTIALLY OBSERVED GRAPHS VIA CONVEX OPTIMIZATION\n0\n0.05\n0.1\n0.15\n0.2\n0\n0.2\n0.4\n0.6\n0.8\n1\n(1 \u22122\u03c4 )K m in\nProb. of Success\n \n \nKmin = 125\nKmin = 100\nKmin = 50\nFigure 5: Simulation results verifying the performance of our algorithm as a function of the disagreement\nprobability \u03c4 and the cluster size Kmin, with n = 1000 and p0 = 0.6 \ufb01xed.\n0.05\n0.1\n0.15\n0.2\n0.25\n0\n0.2\n0.4\n0.6\n0.8\n1\np 0\nProb. of Success\n \n \nOur Method\nSpectral (Zero)\nSpectral (Rand)\nFigure 6: Comparison of our method and spectral clustering under different observation probabilities p0,\nwith n = 2000, Kmin = 200 and \u03c4 = 0.1. For spectral clustering, two imputation schemes\nare considered: (a) Spectral (Zero), where the missing entries are imputed with zeros, and (b)\nSpectral (Rand), where they are imputed with 0/1 random variables with symmetric probabilities.\nThe result shows that our method recovers the underlying clusters with fewer observations.\nthis setting. The effectiveness of the proposed method and the scaling of the theoretical results are\nvalidated by simulation studies.\nThis work is motivated by graph clustering applications where obtaining similarity data is ex-\npensive and it is desirable to use as few observations as possible. As such, potential directions for\nfuture work include considering different sampling schemes such as active sampling, as well as\ndealing with sparse graphs with very few connections.\nAcknowledgments\nS. Sanghavi would like to acknowledge DTRA grant HDTRA1-13-1-0024 and NSF grants 1302435,\n1320175 and 0954059. H. Xu is partially supported by the Ministry of Education of Singapore\n2229\nCHEN, JALALI, SANGHAVI AND XU\nthrough AcRF Tier Two grant R-265-000-443-112. The authors are grateful to the anonymous re-\nviewers for their thorough reviews of this work and valuable suggestions on improving the manuscript.\nAppendix A. Technical Lemmas\nIn this section, we provide several auxiliary lemmas required in the proof of Theorem 4. We will\nmake use of the non-commutative Bernstein inequality. The following version is given by Tropp\n(2012).\nLemma 7 (Tropp, 2012) Consider a \ufb01nite sequence {Mi} of independent, random n\u00d7n matrices\nthat satisfy the assumption EMi = 0 and \u2225Mi\u2225\u2264D almost surely. Let\n\u03c32 = max\n(\r\r\r\r\r\nX\ni\nE\nh\nMiM\u22a4\ni\ni\r\r\r\r\r ,\n\r\r\r\r\r\nX\ni\nE\nh\nM\u22a4\ni Mi\ni\r\r\r\r\r\n)\n.\nThen for all \u03b8 > 0 we have\nP\nh\r\r\r\nX\nMi\n\r\r\r \u2265\u03b8\ni\n\u22642n exp\n\u0012\n\u2212\n\u03b82\n2\u03c32 + 2D\u03b8/3\n\u0013\n.\n\u2264\n(\n2n exp\n\u0010\n\u22123\u03b82\n8\u03c32\n\u0011\n,\nfor \u03b8 \u2264\u03c32\nD ;\n2n exp\n\u0000\u22123\u03b8\n8D\n\u0001\n,\nfor \u03b8 \u2265\u03c32\nD .\n(7)\nRemark 8 When n = 1, this becomes the standard two-sided Bernstein inequality.\nWe will also make use of the following estimate, which follows from the structure of U.\n\r\r\rPT (eie\u22a4\nj )\n\r\r\r\n2\nF =\n\r\rUUT ei\n\r\r2 +\n\r\rUUT ej\n\r\r2 \u2212\n\r\rUUT ei\n\r\r2 \r\rUUT ej\n\r\r2 \u2264\n2n\nK2\nmin\n,\n\u22001 \u2264i, j \u2264n.\nThe \ufb01rst auxiliary lemma controls the operator norm of certain random operators. A similar\nresult was given in Cand`es et al. (2011, Theorem 4.1). Our proof is different from theirs.\nLemma 9 Suppose \u21260 is a set of entries obeying \u21260 \u223cBer1(\u03c1). Consider the operator PT \u2212\nPT R\u21260PT . For some constant C0 > 0, we have\n\u2225PT \u2212PT R\u21260PT \u2225< \u03f51\nwith high probability provided that \u03c1 \u2265C0\nn log n\n\u03f52\n1K2\nmin and \u03f51 \u22641.\nProof For each (i, j), de\ufb01ne the indicator random variable \u03b4ij = 1{(i,j)\u2208\u21260}. We observe that for\nany matrix M \u2208T ,\n(PT R\u21260PT \u2212PT ) M =\nX\n1\u2264i<j\u2264n\nSij(M)\n\u225c\nX\n1\u2264i<j\u2264n\n\u0000\u03c1\u22121\u03b4ij \u22121\n\u0001 D\nPT (eie\u22a4\nj ), M\nE\nPT (eie\u22a4\nj + eje\u22a4\ni ).\n2230\nCLUSTERING PARTIALLY OBSERVED GRAPHS VIA CONVEX OPTIMIZATION\nHere Sij : Rn\u00d7n 7\u2192Rn\u00d7n is a linear self-adjoint operator with E [Sij] = 0. Using the fact that\nPT (eie\u22a4\nj ) =\n\u0000PT (eje\u22a4\ni )\n\u0001\u22a4and M is symmetric, we obtain the bounds\n\u2225Sij\u2225\u2264\u03c1\u22121 \r\r\rPT (eie\u22a4\nj )\n\r\r\r\nF\n\r\r\rPT (eie\u22a4\nj + eje\u22a4\ni )\n\r\r\r\nF\n\u2264\u03c1\u22121 \u00b7 2\n\r\r\rPT (eie\u22a4\nj )\n\r\r\r\n2\nF \u2264\n4n\nK2\nmin\u03c1,\nand\n\r\r\r\r\r\r\nE\n\uf8ee\n\uf8f0\nX\n1\u2264i<j\u2264n\nS2\nij(M)\n\uf8f9\n\uf8fb\n\r\r\r\r\r\r\nF\n=\n\r\r\r\r\r\r\nX\n1\u2264i<j\u2264n\nE\nh\n(\u03c1\u22121\u03b4(k)\nij \u22121)2i D\nPT (eie\u22a4\nj ), M\nE D\nPT (eie\u22a4\nj + eje\u22a4\ni ), eie\u22a4\nj\nE\nPT (eie\u22a4\nj + eje\u22a4\ni )\n\r\r\r\r\r\r\nF\n=\n\u0000\u03c1\u22121 \u22121\n\u0001\n\r\r\r\r\r\r\nX\n1\u2264i<j\u2264n\n2\n\r\r\rPT (eie\u22a4\nj )\n\r\r\r\n2\nF mi,jPT (eie\u22a4\nj + eje\u22a4\ni )\n\r\r\r\r\r\r\nF\n\u2264\n\u0000\u03c1\u22121 \u22121\n\u0001\n\r\r\r\r\r\r\nX\n1\u2264i<j\u2264n\n2\n\r\r\rPT (eie\u22a4\nj )\n\r\r\r\n2\nF mi,j(eie\u22a4\nj + eje\u22a4\ni )\n\r\r\r\r\r\r\nF\n\u2264\n\u0000\u03c1\u22121 \u22121\n\u0001\n4n\nK2\nmin\n\r\r\r\r\r\r\nX\n1\u2264i<j\u2264n\nmi,j(eie\u22a4\nj + eje\u22a4\ni )\n\r\r\r\r\r\r\nF\n=\n\u0000\u03c1\u22121 \u22121\n\u0001\n4n\nK2\nmin\n\u2225M\u2225F ,\nwhich means\n\r\r\rE\nhP\n1\u2264i<j\u2264n S2\nij\ni\r\r\r \u2264\n4n\nK2\nmin\u03c1. Applying the \ufb01rst inequality in the Bernstein inequal-\nity (7) gives\nP\nh\r\r\r\nP\n1\u2264i<j\u2264n Sij\n\r\r\r \u2265\u03f51\ni\n\u22642n2\u22122\u03b2\nprovided \u03c1 \u226564\u03b2n log n\n3K2\nmin\u03f52\n1 and \u03f51 < 1.\nThe next lemma bounds the spectral norm of certain symmetric random matrices. A related\nresult for non-symmetric matrices appeared in Cand`es and Recht (2009, Theorem 6.3).\nLemma 10 Suppose \u21260 is a set of entries obeying \u21260 \u223cBer1(\u03c1), and M is a \ufb01xed n\u00d7n symmetric\nmatrix. Then for some constant C0 > 0, we have\n\u2225(I \u2212R\u21260)M\u2225<\ns\nC0\nn log n\n\u03c1\n\u2225M\u2225\u221e,\nwith high probability provided that \u03c1 \u2265C0\nlog n\nn .\n2231\nCHEN, JALALI, SANGHAVI AND XU\nProof De\ufb01ne \u03b4ij as before. Notice that\nR\u21260(M) \u2212M =\nX\ni<j\nSij \u225c\nX\ni<j\n(\u03c1\u22121\u03b4ij \u22121)mi,j\n\u0010\neie\u22a4\nj + eje\u22a4\ni\n\u0011\n.\nHere the symmetric matrix Sij \u2208Rn\u00d7n satis\ufb01es E [Sij] = 0, \u2225Sij\u2225\u22642\u03c1\u22121 \u2225M\u2225\u221eand the bound\n\r\r\rE\nhP\ni<j S2\nij\ni\r\r\r =\n\u0000\u03c1\u22121 \u22121\n\u0001\n\r\r\r\r\r\r\nX\ni<j\nm2\ni,j\n\u0010\neie\u22a4\ni + eje\u22a4\nj\n\u0011\n\r\r\r\r\r\r\n\u2264\n\u0000\u03c1\u22121 \u22121\n\u0001\n\r\r\r\r\r\r\ndiag\n\uf8eb\n\uf8edX\nj\nm2\n1,j, . . . ,\nX\nj\nm2\nn,j\n\uf8f6\n\uf8f8\n\r\r\r\r\r\r\n\u2264\n\u0000\u03c1\u22121 \u22121\n\u0001\nn \u2225M\u22252\n\u221e\u22642\u03c1\u22121n \u2225M\u22252\n\u221e.\nWhen \u03c1 \u226516\u03b2 log n\n3n\n, we apply the \ufb01rst inequality in the Bernstein inequality (7) to obtain\nP\n\"\r\r\rP\ni<j Sij\n\r\r\r \u2265\ns\n16\u03b2n log n\n3\u03c1\n\u2225M\u2225\u221e\n#\n\u22642n exp\n \n\u2212\n3 \u00b7 16\u03b2n log n\n3\u03c1\n\u2225M\u22252\n\u221e\n8 \u00b7 2n\n\u03c1 \u2225M\u22252\n\u221e\n!\n\u22642n1\u2212\u03b2.\nThe conclusion follows by choosing a suf\ufb01ciently large constant \u03b2.\nThe third lemma bounds the in\ufb01nity norm of certain random symmetric matrices. A related\nresult is given in Cand`es et al. (2011, Lemma 3.1).\nLemma 11 Suppose \u21260 is a set of entries obeying \u21260 \u223cBer1(\u03c1), and M \u2208T is a \ufb01xed symmetric\nn \u00d7 n matrix. Then for some constant C0 > 0, we have\n\u2225(PT \u2212PT R\u21260PT )M\u2225\u221e< \u03f53\u2225M\u2225\u221e,\nwith high probability provided that \u03c1 \u2265C0\nn log n\n\u03f52\n3K2\nmin and \u03f53 \u22641.\nProof De\ufb01ne \u03b4ij as before. Fix an entry index (a, b). Notice that\n(PT R\u21260PT M \u2212PT M)a,b =\nX\ni<j\n\u03beij \u225c\nX\ni<j\nD\n(\u03c1\u22121\u03b4(k)\nij \u22121)mi,jPT\n\u0010\neie\u22a4\nj + eje\u22a4\ni\n\u0011\n, eae\u22a4\nb\nE\n.\nThe random variable \u03beij satis\ufb01es E [\u03beij] = 0 and obeys the bounds\n|\u03beij| \u22642p\u22121 \r\r\rPT (eie\u22a4\nj )\n\r\r\r\nF\n\r\r\rPT (eae\u22a4\nb )\n\r\r\r\nF |mi,j| \u2264\n4n\nK2\nmin\u03c1 \u2225M\u2225\u221e\n2232\nCLUSTERING PARTIALLY OBSERVED GRAPHS VIA CONVEX OPTIMIZATION\nand\n\f\f\f\f\f\f\nE\n\uf8ee\n\uf8f0X\ni<j\n\u03be2\nij\n\uf8f9\n\uf8fb\n\f\f\f\f\f\f\n=\n\f\f\f\f\f\f\nX\ni<j\nE\nh\n(\u03c1\u22121\u03b4(k)\nij \u22121)2i\nm2\ni,j\nD\nPT\n\u0010\neie\u22a4\nj + eje\u22a4\ni\n\u0011\n, eae\u22a4\nb\nE2\n\f\f\f\f\f\f\n\u2264\n\u0000\u03c1\u22121 \u22121\n\u0001\n\u2225M\u22252\n\u221e\nX\ni<j\nD\neie\u22a4\nj + eje\u22a4\ni , PT (eae\u22a4\nb )\nE2\n\u22642\n\u0000\u03c1\u22121 \u22121\n\u0001\n\u2225M\u22252\n\u221e\n\r\r\rPT (eae\u22a4\nb )\n\r\r\r\n2\nF\n\u22642\n\u0000\u03c1\u22121 \u22121\n\u0001\n2n\nK2\nmin\n\u2225M\u22252\n\u221e\n\u2264\n4n\nK2\nmin\u03c1 \u2225M\u22252\n\u221e.\nWhen \u03c1 \u226564\u03b2n log n\n3K2\nmin\u03f52\n3 and \u03f53 \u22641, we apply the \ufb01rst inequality in the Bernstein inequality (7) with\nn = 1 to obtain\nP\nh\f\f\f(PT R\u21260PT M \u2212PT M)a,b\n\f\f\f \u2265\u03f53 \u2225M\u2225\u221e\ni\n\u22642 exp\n\uf8eb\n\uf8ed\u2212\n3\u03f52\n3 \u2225M\u22252\n\u221e\n8\n4n\nK2\nmin\u03c1 \u2225M\u22252\n\u221e\n\uf8f6\n\uf8f8\u22642n\u22122\u03b2.\nApplying the union bound then yields\nP [\u2225PT R\u21260PT M \u2212PT M\u2225\u221e\u2265\u03f53 \u2225M\u2225\u221e] \u22642n2\u22122\u03b2.\nThe last lemma bounds the matrix in\ufb01nity norm of PT P\u2126(S) for a \u00b11 random matrix S.\nLemma 12 Suppose \u2126\u223cBer0\n\u0010\n2p0\u03c4\n1\u2212p0+2p0\u03c4\n\u0011\nand S \u2208Rn\u00d7n has i.i.d. symmetric \u00b11 entries .\nUnder the assumption of Theorem 4, for some constant C0, we have with high probability\n\u2225PT P\u2126(S)\u2225\u221e\u2264C0\ns\np0n log n\nK2\nmin\n.\nProof By triangle inequality, we have\n\u2225PT P\u2126(S)\u2225\u221e\u2264\n\r\rUUT P\u2126(S)\n\r\r\n\u221e+\n\r\rP\u2126(S)UUT \r\r\n\u221e+\n\r\rUUT P\u2126(S)UUT \r\r\n\u221e,\nso it suf\ufb01ces to show that each of these three terms are bounded by C\nq\np0n log n\nK2\nmin\nw.h.p. for some\nconstant C. Under the assumption on \u2126and S in the lemma statement, each pair of symmetric\nentries of P\u2126(S) equals \u00b11 with probability \u03c1 \u225c\np0\u03c4\n1\u2212p0+2p0\u03c4 and 0 otherwise; notice that \u03c1 \u2264p0\n2\nsince \u03c4 \u22641\n2. Let\n\u0000s(i)\u0001T be the ith row of UUT . From the structure of U, we know that for all i\nand j,\n\f\f\fs(i)\nj\n\f\f\f \u2264\n1\nKmin\n,\n2233\nCHEN, JALALI, SANGHAVI AND XU\nand for all i,\nn\nX\nj=1\n\u0010\ns(i)\nj\n\u00112\n\u2264\n1\nKmin\n.\nWe now bound\n\r\rUUT P\u2126(S)\n\r\r\n\u221e. For simplicity, we focus on the (1, 1) entry of UUT P\u2126(S) and\ndenote this random variable as X. We may write X as X = P\ni s(1)\ni\n(P\u2126(S))i,1 , for which we have\nE\nh\ns(1)\ni\n(P\u2126(S))i,1\ni\n= 0,\n\f\f\fs(1)\ni\n(P\u2126(S))i,1\n\f\f\f \u2264\n\f\f\fs(1)\ni\n\f\f\f \u2264\n1\nKmin\n,\na.s.\nVar (X) =\nX\ni:(i,1)\u2208\u2126\n(s(1)\ni )2 \u00b7 2\u03c1 \u2264\np0\nKmin\n.\nApplying the standard Bernstein inequality then gives\nP\n\"\n|X| > C\ns\np0n log n\nK2\nmin\n#\n\u22642 exp\n\u0014\n\u2212\n\u0012\nC2 p0n log n\nK2\nmin\n\u0013\n/\n\u0012\n2 p0\nKmin\n+ 2C\u221ap0n log n\n3K2\nmin\n\u0013\u0015\n.\nUnder the assumption of Theorem 4, the right hand side above is bounded by 2n\u221212. It follows\nfrom the union bound that\n\r\rUUT P\u2126(S)\n\r\r\n\u221e\u2264C\nq\np0n log n\nK2\nmin\nw.h.p. Clearly, the same bound holds\nfor\n\r\rP\u2126(S)UUT \r\r\n\u221e. Finally, let K be the size of the cluster that node j is in. Observe that due to\nthe structure of UU\u22a4, we have\n\u0000UUT P\u2126(S)UUT \u0001\ni,j =\nX\nl\n\u0010\nUU\u22a4P\u2126(S)\n\u0011\ni,l\n\u0010\nUU\u22a4\u0011\nl,j \u22641\nK \u00b7 K \u00b7\n\r\r\rUU\u22a4P\u2126(S)\n\r\r\r\n\u221e,\nwhich implies\n\r\rUUT P\u2126(S)UUT \r\r\n\u221e\u2264\n\r\rUU\u22a4P\u2126(S)\n\r\r\n\u221e. This completes the proof of the lemma.\nAppendix B. Proof of Theorem 5\nWe use a standard information theoretical argument, which improves upon a related proof by Chaud-\nhuri et al. (2012). Let K be the size of the clusters (which are assumed to have equal size). For\nsimplicity we assume n/K is an integer. Let F be the set of all possible partition of n nodes into\nn/K clusters of equal size K. Using Stirling\u2019s approximation, we have\nM \u225c|F| =\n1\n(n/K)!\n\u0012 n\nK\n\u0013\u0012n \u2212K\nK\n\u0013\n\u00b7 \u00b7 \u00b7\n\u0012K\nK\n\u0013\n\u2265\n\u0010 n\n3K\n\u0011n(1\u22121\nK )\n\u2265c\n1\n2 n\n1 ,\nwhich holds for K = \u0398(n).\nSuppose the clustering Y is chosen uniformly at random from F, and the graph A is generated\nfrom Y according to the planted partition model with partial observations, where we use aij =? for\n2234\nCLUSTERING PARTIALLY OBSERVED GRAPHS VIA CONVEX OPTIMIZATION\nunobserved pairs. We use PA|Y to denote the distribution of A given Y. Let \u02c6Y be any measurable\nfunction of the observation A. A standard application of Fano\u2019s inequality and the convexity of the\nmutual information (Yang and Barron, 1999) gives\nsup\nY \u2208F\nP\nh\n\u02c6Y \u0338= Y|Y\ni\n\u22651 \u2212\nM\u22122 P\nY(1),Y(2)\u2208F D\n\u0010\nPA|Y(1)\u2225PA|Y(2)\n\u0011\n+ log 2\nlog M\n,\n(8)\nwhere D(\u00b7\u2225\u00b7) denotes the KL-divergence. We now upper bound this divergence. Given Y(l), l =\n1, 2, the ai,j\u2019s are independent of each other, so we have\nD\n\u0010\nPA|Y(1)\u2225PA|Y(2)\n\u0011\n=\nX\ni,j\nD\n\u0010\nPai,j|Y(1)\u2225Pai,j|Y(2)\n\u0011\n.\nFor each pair (i, j), the KL-divergence is zero if y(1)\ni,j = y(2)\ni,j , and otherwise satis\ufb01es\nD\n\u0010\nPai,j|Y(1)\u2225Pai,j|Y(2)\n\u0011\n\u2264p0(1 \u2212\u03c4) log p0(1 \u2212\u03c4)\np0\u03c4\n+ p0\u03c4 log\np0\u03c4\np0(1 \u2212\u03c4) + (1 \u2212p0) log 1 \u2212p0\n1 \u2212p0\n= p0(1 \u22122\u03c4) log 1 \u2212\u03c4\n\u03c4\n\u2264p0(1 \u22122\u03c4)\n\u00121 \u2212\u03c4\n\u03c4\n\u22121\n\u0013\n\u2264c2p0(1 \u22122\u03c4)2,\nwhere c2 > 0 is a universal constant and the last inequality holds under the assumption \u03c4 > 1/100.\nLet N be the number of pairs (i, j) such that y(1)\ni,j \u0338= y(2)\ni,j . When K = \u0398(n), we have\nN \u2264|{(i, j) : y(1)\ni,j = 1} \u222a{(i, j) : y(2)\ni,j = 1}| \u2264n2.\nIt follows that D\n\u0010\nPA|Y(1)\u2225PA|Y(2)\n\u0011\n\u2264N \u00b7 c2p0(1 \u22122\u03c4)2 \u2264c2n2p0(1 \u22122\u03c4)2. Combining pieces,\nfor the left hand side of (8) to be less than 1/4, we must have p0(1 \u22122\u03c4)2 \u2265C 1\nn.\nReferences\nN. Alon, M. Krivelevich, and B. Sudakov. Finding a large hidden clique in a random graph. In\nProceedings of the 9th annual ACM-SIAM Symposium on Discrete Algorithms, pages 457\u2013466,\n1998.\nB. Ames and S. Vavasis. Nuclear norm minimization for the planted clique and biclique problems.\nMathematical Programming, 129(1):69\u201389, 2011.\nM. F. Balcan and P. Gupta. Robust hierarchical clustering. In Proceedings of the Conference on\nLearning Theory (COLT), 2010.\nN. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Proceedings of the 43rd Symposium\non Foundations of Computer Science, 2002.\n2235\nCHEN, JALALI, SANGHAVI AND XU\nB. Bollob\u00b4as and A. D. Scott. Max cut for random graphs with a planted partition. Combinatorics,\nProbability and Computing, 13(4-5):451\u2013474, 2004.\nR. B. Boppana. Eigenvalues and graph bisection: An average-case analysis. In Proceedings of the\n28th Annual Symposium on Foundations of Computer Science, pages 280\u2013285, 1987.\nE. Cand`es and B. Recht. Exact matrix completion via convex optimization. Foundations of Com-\nputational mathematics, 9(6):717\u2013772, 2009.\nE. Cand`es, X. Li, Y. Ma, and J. Wright. Robust principal component analysis? Journal of the ACM,\n58(3):11, 2011.\nT. Carson and R. Impagliazzo. Hill-climbing \ufb01nds random planted bisections. In Proceedings of\nthe 12th annual ACM-SIAM Symposium on Discrete Algorithms, pages 903\u2013909, 2001.\nV. Chandrasekaran, S. Sanghavi, P. Parrilo, and A. Willsky. Rank-sparsity incoherence for matrix\ndecomposition. SIAM Journal on Optimization, 21(2):572\u2013596, 2011.\nM. Charikar, V. Guruswami, and A. Wirth. Clustering with qualitative information. In Proceedings\nof the 44th Annual IEEE Symposium on Foundations of Computer Science, pages 524\u2013533, 2003.\nK. Chaudhuri, F. Chung, and A. Tsiatas. Spectral clustering of graphs with general degrees in the\nextended planted partition model. Journal of Machine Learning Research, 2012:1\u201323, 2012.\nY. Chen, A. Jalali, S. Sanghavi, and C. Caramanis. Low-rank matrix recovery from errors and\nerasures. IEEE Transactions on Information Theory, 59(7):4324\u20134337, 2013.\nA. Condon and R. M. Karp. Algorithms for graph partitioning on the planted partition model.\nRandom Structures and Algorithms, 18(2):116\u2013140, 2001.\nA. Decelle, F. Krzakala, C. Moore, and L. Zdeborov\u00b4a. Asymptotic analysis of the stochastic block\nmodel for modular networks and its algorithmic applications. Physical Review E, 84(6):066106,\n2011.\nE. D. Demaine and N. Immorlica. Correlation clustering with partial information. Approxima-\ntion, Randomization, and Combinatorial Optimization: Algorithms and Techniques, pages 71\u201380,\n2003.\nE. D. Demaine, D. Emanuel, A. Fiat, and N. Immorlica. Correlation clustering in general weighted\ngraphs. Theoretical Computer Science, 361(2):172\u2013187, 2006.\nD. Emmanuel and A. Fiat. Correlation clustering minimizing disagreements on arbitrary weighted\ngraphs. In Proceedings of the 11th Annual European Symposium on Algorithms, pages 208\u2013220,\n2003.\nB. Eriksson, G. Dasarathy, A. Singh, and R. Nowak. Active clustering: Robust and ef\ufb01cient hierar-\nchical clustering using adaptively selected similarities. Arxiv preprint arXiv:1102.3887, 2011.\nM. Ester, H. Kriegel, and X. Xu. A database interface for clustering in large spatial databases. In\nProceedings of the International Conference on Knowledge Discovery and Data Mining, pages\n94\u201399, 1995.\n2236\nCLUSTERING PARTIALLY OBSERVED GRAPHS VIA CONVEX OPTIMIZATION\nM. Fazel. Matrix Rank Minimization with Applications. PhD thesis, Stanford University, 2002.\nU. Feige and J. Kilian. Heuristics for semirandom graph problems. Journal of Computer and System\nSciences, 63(4):639\u2013671, 2001.\nJ. Giesen and D. Mitsche. Reconstructing many partitions using spectral techniques. In Fundamen-\ntals of Computation Theory, pages 433\u2013444. Springer, 2005.\nD. Gross. Recovering low-rank matrices from few coef\ufb01cients in any basis. IEEE Transactions on\nInformation Theory, 57(3):1548\u20131566, 2011.\nP. W. Holland, K. B. Laskey, and S. Leinhardt. Stochastic blockmodels: Some \ufb01rst steps. Social\nnetworks, 5(2):109\u2013137, 1983.\nD. Hsu, S. M. Kakade, and T. Zhang. Robust matrix decomposition with sparse corruptions. IEEE\nTransactions on Information Theory, 57(11):7221\u20137234, 2011.\nB. Hunter and T. Strohmer. Spectral clustering with compressed, incomplete and inaccurate mea-\nsurements.\nAvailable at https://www.math.ucdavis.edu/\u02dcstrohmer/papers/\n2010/SpectralClustering.pdf, 2010.\nM. Jerrum and G. B. Sorkin.\nThe metropolis algorithm for graph bisection.\nDiscrete Applied\nMathematics, 82(1-3):155\u2013175, 1998.\nB. W. Kernighan and S. Lin. An ef\ufb01cient heuristic procedure for partitioning graphs. Bell System\nTechnical Journal, 49(2):291\u2013307, 1970.\nA. Krishnamurthy, S. Balakrishnan, M. Xu, and A. Singh. Ef\ufb01cient active algorithms for hierarchi-\ncal clustering. In Proceedings of the 29th International Conference on Machine Learning, pages\n887\u2013894, 2012.\nX. Li. Compressed sensing and matrix completion with constant proportion of corruptions. Con-\nstructive Approximation, 37(1):73\u201399, 2013.\nZ. Lin, M. Chen, L. Wu, and Y. Ma. The Augmented Lagrange Multiplier Method for Exact Recov-\nery of Corrupted Low-Rank Matrices. UIUC Technical Report UILU-ENG-09-2215, 2009.\nC. Mathieu and W. Schudy. Correlation clustering with noisy input. In Proceedings of the 21st\nAnnual ACM-SIAM Symposium on Discrete Algorithms, pages 712\u2013728, 2010.\nF. McSherry. Spectral partitioning of random graphs. In Proceedings of the 42nd IEEE Symposium\non Foundations of Computer Science, pages 529\u2013537, 2001.\nN. Mishra, I. Stanton R. Schreiber, and R. E. Tarjan. Clustering social networks. In Algorithms and\nModels for Web-Graph, pages 56\u201367. Springer, 2007.\nS. Oymak and B. Hassibi. Finding dense clusters via low rank + sparse decomposition. Available\non arXiv:1104.5186v1, 2011.\nK. Rohe, S. Chatterjee, and B. Yu. Spectral clustering and the high-dimensional stochastic block-\nmodel. The Annals of Statistics, 39(4):1878\u20131915, 2011.\n2237\nCHEN, JALALI, SANGHAVI AND XU\nO. Shamir and N. Tishby. Spectral clustering on a budget. In Proceedings of the 14th International\nConference on Arti\ufb01cial Intelligence and Statistics, pages 661\u2013669, 2011.\nR. Shamir and D. Tsur. Improved algorithms for the random cluster graph model. Random Struc-\ntures & Algorithms, 31(4):418\u2013449, 2007.\nC. Swamy. Correlation clustering: maximizing agreements via semide\ufb01nite programming. In Pro-\nceedings of the 15th Annual ACM-SIAM Symposium on Discrete Algorithms, 2004.\nJ. A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computational\nMathematics, 12(4):389\u2013434, 2012.\nR. Vershynin. Introduction to the non-asymptotic analysis of random matrices. Arxiv preprint\narxiv:1011.3027, 2010.\nK. Voevodski, M. F. Balcan, H. Roglin, S. H. Teng, and Y. Xia. Ef\ufb01cient clustering with limited\ndistance information. arXiv preprint arXiv:1009.5168, 2010.\nU. von Luxburg. A tutorial on spectral clustering. Statistics and Computing, 17(4):395\u2013416, 2007.\nYahoo!-Inc. Graph partitioning. Available at http://research.yahoo.com/project/2368, 2009.\nY. Yang and A. Barron. Information-theoretic determination of minimax rates of convergence. The\nAnnals of Statistics, 27(5):1564\u20131599, 1999.\n2238\n",
        "sentence": " This technique has been successfully applied to background substraction in image sequences, to graph clustering (Jalali et al., 2011) and covariance estimation (Luo, 2011).",
        "context": "this setting. The effectiveness of the proposed method and the scaling of the theoretical results are\nvalidated by simulation studies.\nThis work is motivated by graph clustering applications where obtaining similarity data is ex-\nspeci\ufb01c application domains, but rather on the basic graph clustering problem itself.\nPartially observed graphs appear in many applications. For example, in online social networks\nFinally, we compare the performance of our method with spectral clustering, a popular method\nfor graph clustering. For spectral clustering, we \ufb01rst impute the missing entries of the adjacency\n2227\nCHEN, JALALI, SANGHAVI AND XU\n0\n0.005\n0.01\n0.015\n0\n0.2\n0.4"
    },
    {
        "title": "Sparse nonnegative matrix factorization for clustering",
        "author": [
            "J. Kim",
            "H. Park"
        ],
        "venue": "Technical report, Georgia Institute of Technology,",
        "citeRegEx": "Kim and Park,? \\Q2008\\E",
        "shortCiteRegEx": "Kim and Park",
        "year": 2008,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Nuclear norm penalization and optimal rates for noisy matrix completion",
        "author": [
            "V. Koltchinskii",
            "K. Lounici",
            "A. Tsybakov"
        ],
        "venue": "Annals of Statistics,",
        "citeRegEx": "Koltchinskii et al\\.,? \\Q2011\\E",
        "shortCiteRegEx": "Koltchinskii et al\\.",
        "year": 2011,
        "abstract": "",
        "full_text": "",
        "sentence": " In the low-rank matrix completion problem, the standard relaxation approach leads to the use of the trace norm as the main regularizer within the optimization procedures (Srebro et al., 2005; Koltchinskii et al., 2011) and their resolution can either be obtained in closed form (loss measured in terms of Frobenius norm) or through iterative proximal solutions (Combettes & Pesquet, 2011; Beck & Teboulle, 2009) (for general classes of losses). The techniques used in the proof (see the Appendix) are very similar to those introduced in (Koltchinskii et al., 2011). In fact, for \u03b1 = 0, \u03c4 can be set to zero, and we get a sharp bound for Lasso, while the tracenorm regression bounds of (Koltchinskii et al., 2011) are obtained for \u03b1 = 1.",
        "context": null
    },
    {
        "title": "Learning the parts of objects by non-negative matrix factorization",
        "author": [
            "D.D. Lee",
            "Seung",
            "H.S"
        ],
        "venue": null,
        "citeRegEx": "Lee et al\\.,? \\Q1999\\E",
        "shortCiteRegEx": "Lee et al\\.",
        "year": 1999,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Nonnegative Matrix Factorization (NMF) (Lee et al., 1999) imposes non negativity constraints on the coefficients of U and V to enhance interpretability by allowing only for additive effects and tends to produce sparse factor matrices U, V , although this a rather indirect effect.",
        "context": null
    },
    {
        "title": "The link-prediction problem for social networks",
        "author": [
            "D. Liben-Nowell",
            "J. Kleinberg"
        ],
        "venue": "Journal of the American society for information science and technology,",
        "citeRegEx": "Liben.Nowell and Kleinberg,? \\Q2007\\E",
        "shortCiteRegEx": "Liben.Nowell and Kleinberg",
        "year": 2007,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "High dimensional low rank and sparse covariance matrix estimation via convex minimization",
        "author": [
            "X. Luo"
        ],
        "venue": "Arxiv preprint arXiv:1111.1133,",
        "citeRegEx": "Luo,? \\Q2011\\E",
        "shortCiteRegEx": "Luo",
        "year": 2011,
        "abstract": "",
        "full_text": "",
        "sentence": " , 2011) and covariance estimation (Luo, 2011).",
        "context": null
    },
    {
        "title": "A multivariate tchebycheff inequality",
        "author": [
            "I. Olkin",
            "J.W. Pratt"
        ],
        "venue": "The Annals of Mathematical Statistics,",
        "citeRegEx": "Olkin and Pratt,? \\Q1958\\E",
        "shortCiteRegEx": "Olkin and Pratt",
        "year": 1958,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Generalized forward-backward splitting",
        "author": [
            "H. Raguet",
            "J. Fadili",
            "G. Peyr\u00e9"
        ],
        "venue": "Arxiv preprint arXiv:1108.4404,",
        "citeRegEx": "Raguet et al\\.,? \\Q2011\\E",
        "shortCiteRegEx": "Raguet et al\\.",
        "year": 2011,
        "abstract": "",
        "full_text": "",
        "sentence": " A generalization of these two setups has been recently proposed in (Raguet et al., 2011) under the name of Generalized ForwardBackward, which we specialize to our problem in Algorithm 1.",
        "context": null
    },
    {
        "title": "Link discovery using graph feature tracking",
        "author": [
            "E. Richard",
            "N. Baskiotis",
            "Evgeniou",
            "Th",
            "N. Vayatis"
        ],
        "venue": "Proceedings of Neural Information Processing Systems (NIPS),",
        "citeRegEx": "Richard et al\\.,? \\Q2010\\E",
        "shortCiteRegEx": "Richard et al\\.",
        "year": 2010,
        "abstract": "",
        "full_text": "",
        "sentence": " A useful example that links our work to the matrix completion framework is when linear measurements of the target matrix or graph are available, or can be predicted as in (Richard et al., 2010).",
        "context": null
    },
    {
        "title": "Learning with matrix factorizations",
        "author": [
            "N. Srebro"
        ],
        "venue": "PhD thesis,",
        "citeRegEx": "Srebro,? \\Q2004\\E",
        "shortCiteRegEx": "Srebro",
        "year": 2004,
        "abstract": "",
        "full_text": "",
        "sentence": " Indeed, the notion of sparsity assumption has been transposed into the concept of low-rank matrices and opened the way to numerous achievements (see for instance (Srebro, 2004; Cai et al., 2008)). In the case of the sole rank constraint, (Srebro, 2004) remarked that all low-rank matrices with the same sign pattern are equivalent in terms of loss and applied a standard argument for generalization in classes of finite cardinality. By upper bounding the number of sign configurations for a fixed sparsity pattern in (U, V ) using an argument similar to (Srebro, 2004), a union bound gives , (Srebro et al., 2005; Srebro, 2004)), thus jointly optimizing in U, V \u2208 Rn\u00d7r loss functions of the form `((U, V ), A) = ||UV T \u2212A||F for some target maximum rank r.",
        "context": null
    },
    {
        "title": "Maximummargin matrix factorization",
        "author": [
            "N. Srebro",
            "J. Rennie",
            "T. Jaakkola"
        ],
        "venue": "Advances in Neural Information Processing Systems,",
        "citeRegEx": "Srebro et al\\.,? \\Q2005\\E",
        "shortCiteRegEx": "Srebro et al\\.",
        "year": 2005,
        "abstract": "",
        "full_text": "",
        "sentence": " In the low-rank matrix completion problem, the standard relaxation approach leads to the use of the trace norm as the main regularizer within the optimization procedures (Srebro et al., 2005; Koltchinskii et al., 2011) and their resolution can either be obtained in closed form (loss measured in terms of Frobenius norm) or through iterative proximal solutions (Combettes & Pesquet, 2011; Beck & Teboulle, 2009) (for general classes of losses). , (Srebro et al., 2005; Srebro, 2004)), thus jointly optimizing in U, V \u2208 Rn\u00d7r loss functions of the form `((U, V ), A) = ||UV T \u2212A||F for some target maximum rank r.",
        "context": null
    },
    {
        "title": "Regression shrinkage and selection via the lasso",
        "author": [
            "R. Tibshirani"
        ],
        "venue": "Journal of the Royal Statistical Society,",
        "citeRegEx": "Tibshirani,? \\Q1996\\E",
        "shortCiteRegEx": "Tibshirani",
        "year": 1996,
        "abstract": "SUMMARY\n               We propose a new method for estimation in linear models. The \u2018lasso\u2019 minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.",
        "full_text": "",
        "sentence": " Efficient procedures developed in the context of sparse model estimation mostly rely on the use of `1-norm regularization (Tibshirani, 1996).",
        "context": null
    },
    {
        "title": "Model selection and estimation in regression with grouped variables",
        "author": [
            "M. Yuan",
            "Y. Lin"
        ],
        "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
        "citeRegEx": "Yuan and Lin,? \\Q2006\\E",
        "shortCiteRegEx": "Yuan and Lin",
        "year": 2006,
        "abstract": "SummaryWe consider the problem of selecting grouped variables (factors) for accurate prediction in regression. Such a problem arises naturally in many practical situations with the multifactor analysis-of-variance problem as the most important and well-known example. Instead of selecting factors by stepwise backward elimination, we focus on the accuracy of estimation and consider extensions of the lasso, the LARS algorithm and the non-negative garrotte for factor selection. The lasso, the LARS algorithm and the non-negative garrotte are recently proposed regression methods that can be used to select individual variables. We study and propose efficient algorithms for the extensions of these methods for factor selection and show that these extensions give superior performance to the traditional stepwise backward elimination method in factor selection problems. We study the similarities and the differences between these methods. Simulations and real examples are used to illustrate the methods.",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Sparse principal component analysis",
        "author": [
            "H. Zou",
            "T. Hastie",
            "R. Tibshirani"
        ],
        "venue": "Journal of Computational and Graphical Statistics, pp",
        "citeRegEx": "Zou et al\\.,? \\Q2004\\E",
        "shortCiteRegEx": "Zou et al\\.",
        "year": 2004,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " SPCA proposed in (Zou et al., 2004) penalizes the `1 norm of the principal components and can be reduced to solving independent elastic-nets.",
        "context": null
    },
    {
        "title": "Regularization and variable selection via the elastic net",
        "author": [
            "Zou",
            "Hui",
            "Hastie",
            "Trevor"
        ],
        "venue": "Journal of the Royal Statistical Society, Series B,",
        "citeRegEx": "Zou et al\\.,? \\Q2005\\E",
        "shortCiteRegEx": "Zou et al\\.",
        "year": 2005,
        "abstract": "SummaryWe propose the elastic net, a new regularization and variable selection method. Real world data and a simulation study show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation. In addition, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together. The elastic net is particularly useful when the number of predictors (p) is much bigger than the number of observations (n). By contrast, the lasso is not a very satisfactory variable selection method in the p\u226bn case. An algorithm called LARS-EN is proposed for computing elastic net regularization paths efficiently, much like algorithm LARS does for the lasso.",
        "full_text": "",
        "sentence": "",
        "context": null
    }
]