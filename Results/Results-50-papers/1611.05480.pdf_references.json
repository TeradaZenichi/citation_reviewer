[
    {
        "title": "Recommender systems survey",
        "author": [
            "Jes\u00fas Bobadilla",
            "Fernando Ortega",
            "Antonio Hernando",
            "Abraham Guti\u00e9rrez"
        ],
        "venue": "Knowledge-Based Systems,",
        "citeRegEx": "1",
        "shortCiteRegEx": "1",
        "year": 2013,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " of interest to individual users and subsequently suggest these items to them as recommendations [1], [2]. Depending on the application domain and the deployment platform, dozens of techniques and methods have been proposed to address cross-domain and cross-platform scalability and quality challenges [1], [3], [4]. The main objective for the Recommendation Systems (RSs) is providing a user with content he/she would like by estimating the relevancy or the rating of these contents based on the information about users and the items [1], [9].",
        "context": null
    },
    {
        "title": "Recommender system application developments: a survey",
        "author": [
            "Jie Lu",
            "Dianshuang Wu",
            "Mingsong Mao",
            "Wei Wang",
            "Guangquan Zhang"
        ],
        "venue": "Decision Support Systems,",
        "citeRegEx": "2",
        "shortCiteRegEx": "2",
        "year": 2015,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " of interest to individual users and subsequently suggest these items to them as recommendations [1], [2]. Applications domains of RSs include: \u201ce-government, e-business, e-commerce/eshopping, e-library, e-learning, e-tourism, e-resource services and e-group activities\u201d [2]. deployment platforms emerged over years to include, besides the classical Web-based platform, other modern platforms like mobile, TV, and radio [2].",
        "context": null
    },
    {
        "title": "Content-based recommender systems: State of the art and trends",
        "author": [
            "Pasquale Lops",
            "Marco De Gemmis",
            "Giovanni Semeraro"
        ],
        "venue": "In Recommender systems handbook,",
        "citeRegEx": "3",
        "shortCiteRegEx": "3",
        "year": 2011,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Depending on the application domain and the deployment platform, dozens of techniques and methods have been proposed to address cross-domain and cross-platform scalability and quality challenges [1], [3], [4].",
        "context": null
    },
    {
        "title": "A survey of collaborative filtering techniques",
        "author": [
            "Xiaoyuan Su",
            "Taghi M Khoshgoftaar"
        ],
        "venue": "Advances in artificial intelligence,",
        "citeRegEx": "4",
        "shortCiteRegEx": "4",
        "year": 2009,
        "abstract": "As one of the most successful approaches to building recommender systems, collaborative filtering (CF) uses the known preferences of a group of users to make recommendations or predictions of the unknown preferences for other users. In this paper, we first introduce CF tasks and their main challenges, such as data sparsity, scalability, synonymy, gray sheep, shilling attacks, privacy protection, etc., and their possible solutions. We then present three main categories of CF techniques: memory-based, model-based, and hybrid CF algorithms (that combine CF with other recommendation techniques), with examples for representative algorithms of each category, and analysis of their predictive performance and their ability to address the challenges. From basic techniques to the state-of-the-art, we attempt to present a comprehensive survey for CF techniques, which can be served as a roadmap for research and practice in this area.",
        "full_text": "",
        "sentence": " Depending on the application domain and the deployment platform, dozens of techniques and methods have been proposed to address cross-domain and cross-platform scalability and quality challenges [1], [3], [4]. Unlike CB methods which recommend items that are similar to what target user liked in the past, CF methods leverage preferences of other similar users in order to make recommendations to the target user [4], [5]. Collaborative Filtering is one of the most successful techniques to building RSs due to their independence from the content of items being recommended, which make them easy to create and use across many application domains [4].",
        "context": null
    },
    {
        "title": "Content-based recommender systems",
        "author": [
            "Charu C Aggarwal"
        ],
        "venue": "In Recommender Systems,",
        "citeRegEx": "5",
        "shortCiteRegEx": "5",
        "year": 2016,
        "abstract": "",
        "full_text": "",
        "sentence": " CB techniques work by measuring or predicting the similarity between profiles of the items (attributes/descriptions) and profiles of the users\u2019 (attributes/descriptions of past preferred items) [5], [6]. Unlike CB methods which recommend items that are similar to what target user liked in the past, CF methods leverage preferences of other similar users in order to make recommendations to the target user [4], [5].",
        "context": null
    },
    {
        "title": "Content-based recommendation systems. In The adaptive web, pages 325\u2013341",
        "author": [
            "Michael J Pazzani",
            "Daniel Billsus"
        ],
        "venue": null,
        "citeRegEx": "6",
        "shortCiteRegEx": "6",
        "year": 2007,
        "abstract": "",
        "full_text": "",
        "sentence": " CB techniques work by measuring or predicting the similarity between profiles of the items (attributes/descriptions) and profiles of the users\u2019 (attributes/descriptions of past preferred items) [5], [6].",
        "context": null
    },
    {
        "title": "Semantics-aware contentbased recommender systems",
        "author": [
            "Marco de Gemmis",
            "Pasquale Lops",
            "Cataldo Musto",
            "Fedelucio Narducci",
            "Giovanni Semeraro"
        ],
        "venue": "In Recommender Systems Handbook,",
        "citeRegEx": "7",
        "shortCiteRegEx": "7",
        "year": 2015,
        "abstract": "",
        "full_text": "",
        "sentence": " Item description, on the other hand, is typically textual which makes the similarity scoring more challenging due to language ambiguity raising the need for semanticaware CB systems [7].",
        "context": null
    },
    {
        "title": "Distributed representations of sentences and documents",
        "author": [
            "Quoc V Le",
            "Tomas Mikolov"
        ],
        "venue": "In ICML,",
        "citeRegEx": "8",
        "shortCiteRegEx": "8",
        "year": 2014,
        "abstract": "Many machine learning algorithms require the input to be represented as a\nfixed-length feature vector. When it comes to texts, one of the most common\nfixed-length features is bag-of-words. Despite their popularity, bag-of-words\nfeatures have two major weaknesses: they lose the ordering of the words and\nthey also ignore semantics of the words. For example, \"powerful,\" \"strong\" and\n\"Paris\" are equally distant. In this paper, we propose Paragraph Vector, an\nunsupervised algorithm that learns fixed-length feature representations from\nvariable-length pieces of texts, such as sentences, paragraphs, and documents.\nOur algorithm represents each document by a dense vector which is trained to\npredict words in the document. Its construction gives our algorithm the\npotential to overcome the weaknesses of bag-of-words models. Empirical results\nshow that Paragraph Vectors outperform bag-of-words models as well as other\ntechniques for text representations. Finally, we achieve new state-of-the-art\nresults on several text classification and sentiment analysis tasks.",
        "full_text": "arXiv:1405.4053v2  [cs.CL]  22 May 2014\nDistributed Representations of Sentences and Documents\nQuoc Le\nQVL@GOOGLE.COM\nTomas Mikolov\nTMIKOLOV@GOOGLE.COM\nGoogle Inc, 1600 Amphitheatre Parkway, Mountain View, CA 94043\nAbstract\nMany machine learning algorithms require the\ninput to be represented as a \ufb01xed-length feature\nvector. When it comes to texts, one of the most\ncommon \ufb01xed-length features is bag-of-words.\nDespite their popularity, bag-of-words features\nhave two major weaknesses: they lose the order-\ning of the words and they also ignore semantics\nof the words. For example, \u201cpowerful,\u201d \u201cstrong\u201d\nand \u201cParis\u201d are equally distant. In this paper, we\npropose Paragraph Vector, an unsupervised algo-\nrithm that learns \ufb01xed-length feature representa-\ntions from variable-length pieces of texts, such as\nsentences, paragraphs, and documents. Our algo-\nrithm represents each document by a dense vec-\ntor which is trained to predict words in the doc-\nument. Its construction gives our algorithm the\npotential to overcome the weaknesses of bag-of-\nwords models. Empirical results show that Para-\ngraph Vectors outperform bag-of-words models\nas well as other techniques for text representa-\ntions. Finally, we achieve new state-of-the-art re-\nsults on several text classi\ufb01cation and sentiment\nanalysis tasks.\n1. Introduction\nText classi\ufb01cation and clustering play an important role\nin many applications, e.g, document retrieval, web search,\nspam \ufb01ltering. At the heart of these applications is ma-\nchine learning algorithms such as logistic regression or K-\nmeans. These algorithms typically require the text input to\nbe represented as a \ufb01xed-length vector. Perhaps the most\ncommon \ufb01xed-length vector representation for texts is the\nbag-of-words or bag-of-n-grams (Harris, 1954) due to its\nsimplicity, ef\ufb01ciency and often surprising accuracy.\nHowever, the bag-of-words (BOW) has many disadvan-\nProceedings of the 31 st International Conference on Machine\nLearning, Beijing, China, 2014. JMLR: W&CP volume 32. Copy-\nright 2014 by the author(s).\ntages. The word order is lost, and thus different sentences\ncan have exactly the same representation, as long as the\nsame words are used. Even though bag-of-n-grams con-\nsiders the word order in short context, it suffers from data\nsparsity and high dimensionality. Bag-of-words and bag-\nof-n-grams have very little sense about the semantics of the\nwords or more formally the distances between the words.\nThis means that words \u201cpowerful,\u201d \u201cstrong\u201d and \u201cParis\u201d are\nequally distant despite the fact that semantically, \u201cpower-\nful\u201d should be closer to \u201cstrong\u201d than \u201cParis.\u201d\nIn this paper, we propose Paragraph Vector, an unsuper-\nvised framework that learns continuous distributed vector\nrepresentations for pieces of texts.\nThe texts can be of\nvariable-length, ranging from sentences to documents. The\nname Paragraph Vector is to emphasize the fact that the\nmethod can be applied to variable-length pieces of texts,\nanything from a phrase or sentence to a large document.\nIn our model, the vector representation is trained to be use-\nful for predicting words in a paragraph. More precisely, we\nconcatenate the paragraph vector with several word vec-\ntors from a paragraph and predict the following word in the\ngiven context. Both word vectors and paragraph vectors are\ntrained by the stochastic gradient descent and backpropaga-\ntion (Rumelhart et al., 1986). While paragraph vectors are\nunique among paragraphs, the word vectors are shared. At\nprediction time, the paragraph vectors are inferred by \ufb01x-\ning the word vectors and training the new paragraph vector\nuntil convergence.\nOur technique is inspired by the recent work in learn-\ning vector representations of words using neural net-\nworks (Bengio et al., 2006; Collobert & Weston, 2008;\nMnih & Hinton, 2008; Turian et al., 2010; Mikolov et al.,\n2013a;c). In their formulation, each word is represented by\na vector which is concatenated or averaged with other word\nvectors in a context, and the resulting vector is used to pre-\ndict other words in the context. For example, the neural\nnetwork language model proposed in (Bengio et al., 2006)\nuses the concatenation of several previous word vectors to\nform the input of a neural network, and tries to predict the\nnext word. The outcome is that after the model is trained,\nthe word vectors are mapped into a vector space such that\nDistributed Representations of Sentences and Documents\nsemantically similar words have similar vector representa-\ntions (e.g., \u201cstrong\u201d is close to \u201cpowerful\u201d).\nFollowing these successful techniques, researchers have\ntried to extend the models to go beyond word level\nto achieve phrase-level or sentence-level representa-\ntions (Mitchell & Lapata, 2010; Zanzotto et al., 2010;\nYessenalina & Cardie, 2011; Grefenstette et al., 2013;\nMikolov et al., 2013c). For instance, a simple approach is\nusing a weighted average of all the words in the document.\nA more sophisticated approach is combining the word vec-\ntors in an order given by a parse tree of a sentence, using\nmatrix-vector operations (Socher et al., 2011b). Both ap-\nproaches have weaknesses. The \ufb01rst approach, weighted\naveraging of word vectors, loses the word order in the same\nway as the standard bag-of-words models do. The second\napproach, using a parse tree to combine word vectors, has\nbeen shown to work for only sentences because it relies on\nparsing.\nParagraph Vector is capable of constructing representations\nof input sequences of variable length. Unlike some of the\nprevious approaches, it is general and applicable to texts of\nany length: sentences, paragraphs, and documents. It does\nnot require task-speci\ufb01c tuning of the word weighting func-\ntion nor does it rely on the parse trees. Further in the paper,\nwe will present experiments on several benchmark datasets\nthat demonstrate the advantages of Paragraph Vector. For\nexample, on sentiment analysis task, we achieve new state-\nof-the-art results, better than complex methods, yielding a\nrelative improvement of more than 16% in terms of error\nrate. On a text classi\ufb01cation task, our method convincingly\nbeats bag-of-words models, giving a relative improvement\nof about 30%.\n2. Algorithms\nWe start by discussing previous methods for learning word\nvectors. These methods are the inspiration for our Para-\ngraph Vector methods.\n2.1. Learning Vector Representation of Words\nThis section introduces the concept of distributed vector\nrepresentation of words.\nA well known framework for\nlearning the word vectors is shown in Figure 1. The task\nis to predict a word given the other words in a context.\nIn this framework, every word is mapped to a unique vec-\ntor, represented by a column in a matrix W. The column\nis indexed by position of the word in the vocabulary. The\nconcatenation or sum of the vectors is then used as features\nfor prediction of the next word in a sentence.\nMore formally, given a sequence of training words\nw1, w2, w3, ..., wT , the objective of the word vector model\nFigure 1. A framework for learning word vectors.\nContext of\nthree words (\u201cthe,\u201d \u201ccat,\u201d and \u201csat\u201d) is used to predict the fourth\nword (\u201con\u201d). The input words are mapped to columns of the ma-\ntrix W to predict the output word.\nis to maximize the average log probability\n1\nT\nT \u2212k\nX\nt=k\nlog p(wt|wt\u2212k, ..., wt+k)\nThe prediction task is typically done via a multiclass clas-\nsi\ufb01er, such as softmax. There, we have\np(wt|wt\u2212k, ..., wt+k) =\neywt\nP\ni eyi\nEach of yi is un-normalized log-probability for each output\nword i, computed as\ny = b + Uh(wt\u2212k, ..., wt+k; W)\n(1)\nwhere U, b are the softmax parameters. h is constructed by\na concatenation or average of word vectors extracted from\nW.\nIn practice, hierarchical softmax (Morin & Bengio, 2005;\nMnih & Hinton, 2008; Mikolov et al., 2013c) is preferred\nto softmax for fast training.\nIn our work, the structure\nof the hierarical softmax is a binary Huffman tree, where\nshort codes are assigned to frequent words. This is a good\nspeedup trick because common words are accessed quickly.\nThis use of binary Huffman code for the hierarchy is the\nsame with (Mikolov et al., 2013c).\nThe neural network based word vectors are usually\ntrained using stochastic gradient descent where the gra-\ndient is obtained via backpropagation (Rumelhart et al.,\n1986).\nThis type of models is commonly known as\nneural language models (Bengio et al., 2006).\nA\nparticular implementation of neural network based al-\ngorithm for training the word vectors is available at\ncode.google.com/p/word2vec/ (Mikolov et al.,\n2013a).\nAfter the training converges, words with similar meaning\nare mapped to a similar position in the vector space. For\nDistributed Representations of Sentences and Documents\nexample, \u201cpowerful\u201d and \u201cstrong\u201d are close to each other,\nwhereas \u201cpowerful\u201d and \u201cParis\u201d are more distant. The dif-\nference between word vectors also carry meaning. For ex-\nample, the word vectors can be used to answer analogy\nquestions using simple vector algebra: \u201cKing\u201d - \u201cman\u201d +\n\u201cwoman\u201d = \u201cQueen\u201d (Mikolov et al., 2013d). It is also pos-\nsible to learn a linear matrix to translate words and phrases\nbetween languages (Mikolov et al., 2013b).\nThese properties make word vectors attractive for many\nnatural language processing tasks such as language mod-\neling (Bengio et al., 2006; Mikolov, 2012), natural lan-\nguage understanding (Collobert & Weston, 2008; Zhila\net al., 2013), statistical machine translation (Mikolov et al.,\n2013b; Zou et al., 2013), image understanding (Frome\net al., 2013) and relational extraction (Socher et al., 2013a).\n2.2. Paragraph Vector: A distributed memory model\nOur approach for learning paragraph vectors is inspired by\nthe methods for learning the word vectors. The inspiration\nis that the word vectors are asked to contribute to a predic-\ntion task about the next word in the sentence. So despite\nthe fact that the word vectors are initialized randomly, they\ncan eventually capture semantics as an indirect result of the\nprediction task. We will use this idea in our paragraph vec-\ntors in a similar manner. The paragraph vectors are also\nasked to contribute to the prediction task of the next word\ngiven many contexts sampled from the paragraph.\nIn our Paragraph Vector framework (see Figure 2), every\nparagraph is mapped to a unique vector, represented by a\ncolumn in matrix D and every word is also mapped to a\nunique vector, represented by a column in matrix W. The\nparagraph vector and word vectors are averaged or concate-\nnated to predict the next word in a context. In the experi-\nments, we use concatenation as the method to combine the\nvectors.\nMore formally, the only change in this model compared\nto the word vector framework is in equation 1, where h is\nconstructed from W and D.\nThe paragraph token can be thought of as another word. It\nacts as a memory that remembers what is missing from the\ncurrent context \u2013 or the topic of the paragraph. For this\nreason, we often call this model the Distributed Memory\nModel of Paragraph Vectors (PV-DM).\nThe contexts are \ufb01xed-length and sampled from a sliding\nwindow over the paragraph. The paragraph vector is shared\nacross all contexts generated from the same paragraph but\nnot across paragraphs. The word vector matrix W, how-\never, is shared across paragraphs. I.e., the vector for \u201cpow-\nerful\u201d is the same for all paragraphs.\nThe paragraph vectors and word vectors are trained using\nstochastic gradient descent and the gradient is obtained via\nbackpropagation. At every step of stochastic gradient de-\nscent, one can sample a \ufb01xed-length context from a random\nparagraph, compute the error gradient from the network in\nFigure 2 and use the gradient to update the parameters in\nour model.\nAt prediction time, one needs to perform an inference step\nto compute the paragraph vector for a new paragraph. This\nis also obtained by gradient descent. In this step, the pa-\nrameters for the rest of the model, the word vectors W and\nthe softmax weights, are \ufb01xed.\nSuppose that there are N paragraphs in the corpus, M\nwords in the vocabulary, and we want to learn paragraph\nvectors such that each paragraph is mapped to p dimen-\nsions and each word is mapped to q dimensions, then the\nmodel has the total of N \u00d7 p + M \u00d7 q parameters (ex-\ncluding the softmax parameters). Even though the number\nof parameters can be large when N is large, the updates\nduring training are typically sparse and thus ef\ufb01cient.\nFigure 2. A framework for learning paragraph vector. This frame-\nwork is similar to the framework presented in Figure 1; the only\nchange is the additional paragraph token that is mapped to a vec-\ntor via matrix D. In this model, the concatenation or average of\nthis vector with a context of three words is used to predict the\nfourth word. The paragraph vector represents the missing infor-\nmation from the current context and can act as a memory of the\ntopic of the paragraph.\nAfter being trained, the paragraph vectors can be used as\nfeatures for the paragraph (e.g., in lieu of or in addition\nto bag-of-words). We can feed these features directly to\nconventional machine learning techniques such as logistic\nregression, support vector machines or K-means.\nIn summary, the algorithm itself has two key stages: 1)\ntraining to get word vectors W, softmax weights U, b and\nparagraph vectors D on already seen paragraphs; and 2)\n\u201cthe inference stage\u201d to get paragraph vectors D for new\nparagraphs (never seen before) by adding more columns\nin D and gradient descending on D while holding W, U, b\n\ufb01xed. We use D to make a prediction about some particular\nlabels using a standard classi\ufb01er, e.g., logistic regression.\nDistributed Representations of Sentences and Documents\nAdvantages of paragraph vectors:\nAn important ad-\nvantage of paragraph vectors is that they are learned from\nunlabeled data and thus can work well for tasks that do not\nhave enough labeled data.\nParagraph vectors also address some of the key weaknesses\nof bag-of-words models. First, they inherit an important\nproperty of the word vectors: the semantics of the words. In\nthis space, \u201cpowerful\u201d is closer to \u201cstrong\u201d than to \u201cParis.\u201d\nThe second advantage of the paragraph vectors is that they\ntake into consideration the word order, at least in a small\ncontext, in the same way that an n-gram model with a large\nn would do. This is important, because the n-gram model\npreserves a lot of information of the paragraph, including\nthe word order. That said, our model is perhaps better than\na bag-of-n-grams model because a bag of n-grams model\nwould create a very high-dimensional representation that\ntends to generalize poorly.\n2.3. Paragraph Vector without word ordering:\nDistributed bag of words\nThe above method considers the concatenation of the para-\ngraph vector with the word vectors to predict the next word\nin a text window. Another way is to ignore the context\nwords in the input, but force the model to predict words\nrandomly sampled from the paragraph in the output. In re-\nality, what this means is that at each iteration of stochastic\ngradient descent, we sample a text window, then sample\na random word from the text window and form a classi\ufb01-\ncation task given the Paragraph Vector. This technique is\nshown in Figure 3. We name this version the Distributed\nBag of Words version of Paragraph Vector (PV-DBOW), as\nopposed to Distributed Memory version of Paragraph Vec-\ntor (PV-DM) in previous section.\nFigure 3. Distributed Bag of Words version of paragraph vectors.\nIn this version, the paragraph vector is trained to predict the words\nin a small window.\nIn addition to being conceptually simple, this model re-\nquires to store less data. We only need to store the softmax\nweights as opposed to both softmax weights and word vec-\ntors in the previous model. This model is also similar to the\nSkip-gram model in word vectors (Mikolov et al., 2013c).\nIn our experiments, each paragraph vector is a combina-\ntion of two vectors: one learned by the standard paragraph\nvector with distributed memory (PV-DM) and one learned\nby the paragraph vector with distributed bag of words (PV-\nDBOW). PV-DM alone usually works well for most tasks\n(with state-of-art performances), but its combination with\nPV-DBOW is usually more consistent across many tasks\nthat we try and therefore strongly recommended.\n3. Experiments\nWe perform experiments to better understand the behavior\nof the paragraph vectors. To achieve this, we benchmark\nParagraph Vector on two text understanding problems that\nrequire \ufb01xed-length vector representations of paragraphs:\nsentiment analysis and information retrieval.\nFor sentiment analysis, we use two datasets: Stanford sen-\ntiment treebank dataset (Socher et al., 2013b) and IMDB\ndataset (Maas et al., 2011). Documents in these datasets\ndiffer signi\ufb01cantly in lengths: every example in Socher et\nal. (Socher et al., 2013b)\u2019s dataset is a single sentence while\nevery example in Maas et al. (Maas et al., 2011)\u2019s dataset\nconsists of several sentences.\nWe also test our method on an information retrieval task,\nwhere the goal is to decide if a document should be re-\ntrieved given a query.\n3.1. Sentiment Analysis with the Stanford Sentiment\nTreebank Dataset\nDataset:\nThis dataset was \ufb01rst proposed by (Pang & Lee,\n2005) and subsequently extended by (Socher et al., 2013b)\nas a benchmark for sentiment analysis. It has 11855 sen-\ntences taken from the movie review site Rotten Tomatoes.\nThe dataset consists of three sets: 8544 sentences for train-\ning, 2210 sentences for test and 1101 sentences for valida-\ntion (or development).\nEvery sentence in the dataset has a label which goes from\nvery negative to very positive in the scale from 0.0 to 1.0.\nThe labels are generated by human annotators using Ama-\nzon Mechanical Turk.\nThe dataset comes with detailed labels for sentences,\nand subphrases in the same scale.\nTo achieve this,\nSocher et al. (Socher et al., 2013b) used the Stanford\nParser (Klein & Manning, 2003) to parse each sentence\nto subphrases. The subphrases were then labeled by hu-\nman annotators in the same way as the sentences were\nlabeled.\nIn total, there are 239,232 labeled phrases\nin the dataset.\nThe dataset can be downloaded at:\nhttp://nlp.Stanford.edu/sentiment/\nDistributed Representations of Sentences and Documents\nTasks and Baselines:\nIn (Socher et al., 2013b), the au-\nthors propose two ways of benchmarking. First, one could\nconsider a 5-way \ufb01ne-grained classi\ufb01cation task where\nthe labels are {Very Negative, Negative, Neutral, Posi-\ntive, Very Positive} or a 2-way coarse-grained classi\ufb01ca-\ntion task where the labels are {Negative, Positive}. The\nother axis of variation is in terms of whether we should la-\nbel the entire sentence or all phrases in the sentence. In this\nwork we only consider labeling the full sentences.\nSocher et al. (Socher et al., 2013b) apply several methods\nto this dataset and \ufb01nd that their Recursive Neural Tensor\nNetwork works much better than bag-of-words model. It\ncan be argued that this is because movie reviews are often\nshort and compositionality plays an important role in de-\nciding whether the review is positive or negative, as well as\nsimilarity between words does given the rather tiny size of\nthe training set.\nExperimental protocols:\nWe follow the experimental\nprotocols as described in (Socher et al., 2013b). To make\nuse of the available labeled data, in our model, each sub-\nphrase is treated as an independent sentence and we learn\nthe representations for all the subphrases in the training set.\nAfter learning the vector representations for training sen-\ntences and their subphrases, we feed them to a logistic re-\ngression to learn a predictor of the movie rating.\nAt test time, we freeze the vector representation for each\nword, and learn the representations for the sentences using\ngradient descent. Once the vector representations for the\ntest sentences are learned, we feed them through the logis-\ntic regression to predict the movie rating.\nIn our experiments, we cross validate the window size us-\ning the validation set, and the optimal window size is 8.\nThe vector presented to the classi\ufb01er is a concatenation of\ntwo vectors, one from PV-DBOW and one from PV-DM.\nIn PV-DBOW, the learned vector representations have 400\ndimensions. In PV-DM, the learned vector representations\nhave 400 dimensions for both words and paragraphs. To\npredict the 8-th word, we concatenate the paragraph vec-\ntors and 7 word vectors. Special characters such as ,.!? are\ntreated as a normal word. If the paragraph has less than 9\nwords, we pre-pad with a special NULL word symbol.\nResults:\nWe report the error rates of different methods in\nTable 1. The \ufb01rst highlight for this Table is that bag-of-\nwords or bag-of-n-grams models (NB, SVM, BiNB) per-\nform poorly. Simply averaging the word vectors (in a bag-\nof-words fashion) does not improve the results. This is\nbecause bag-of-words models do not consider how each\nsentence is composed (e.g., word ordering) and therefore\nfail to recognize many sophisticated linguistic phenom-\nena, for instance sarcasm.\nThe results also show that\nTable 1. The performance of our method compared to other ap-\nproaches on the Stanford Sentiment Treebank dataset. The error\nrates of other methods are reported in (Socher et al., 2013b).\nModel\nError rate\nError rate\n(Positive/\n(Fine-\nNegative)\ngrained)\nNa\u00a8\u0131ve Bayes\n18.2 %\n59.0%\n(Socher et al., 2013b)\nSVMs (Socher et al., 2013b)\n20.6%\n59.3%\nBigram Na\u00a8\u0131ve Bayes\n16.9%\n58.1%\n(Socher et al., 2013b)\nWord Vector Averaging\n19.9%\n67.3%\n(Socher et al., 2013b)\nRecursive Neural Network\n17.6%\n56.8%\n(Socher et al., 2013b)\nMatrix Vector-RNN\n17.1%\n55.6%\n(Socher et al., 2013b)\nRecursive Neural Tensor Network\n14.6%\n54.3%\n(Socher et al., 2013b)\nParagraph Vector\n12.2%\n51.3%\nmore advanced methods (such as Recursive Neural Net-\nwork (Socher et al., 2013b)), which require parsing and\ntake into account the compositionality, perform much bet-\nter.\nOur method performs better than all these baselines, e.g.,\nrecursive networks, despite the fact that it does not re-\nquire parsing. On the coarse-grained classi\ufb01cation task, our\nmethod has an absolute improvement of 2.4% in terms of\nerror rates. This translates to 16% relative improvement.\n3.2. Beyond One Sentence: Sentiment Analysis with\nIMDB dataset\nSome of the previous techniques only work on sentences,\nbut not paragraphs/documents with several sentences. For\ninstance, Recursive Neural Tensor Network (Socher et al.,\n2013b) is based on the parsing over each sentence and it\nis unclear how to combine the representations over many\nsentences. Such techniques therefore are restricted to work\non sentences but not paragraphs or documents.\nOur method does not require parsing, thus it can produce\na representation for a long document consisting of many\nsentences. This advantage makes our method more general\nthan some of the other approaches. The following experi-\nment on IMDB dataset demonstrates this advantage.\nDataset:\nThe IMDB dataset was \ufb01rst proposed by Maas\net al. (Maas et al., 2011) as a benchmark for sentiment anal-\nysis. The dataset consists of 100,000 movie reviews taken\nfrom IMDB. One key aspect of this dataset is that each\nmovie review has several sentences.\nThe 100,000 movie reviews are divided into three datasets:\nDistributed Representations of Sentences and Documents\n25,000 labeled training instances, 25,000 labeled test in-\nstances and 50,000 unlabeled training instances. There are\ntwo types of labels: Positive and Negative. These labels are\nbalanced in both the training and the test set. The dataset\ncan be downloaded at http://ai.Stanford.edu/\namaas/data/sentiment/index.html\nExperimental protocols:\nWe learn the word vectors and\nparagraph vectors using 75,000 training documents (25,000\nlabeled and 50,000 unlabeled instances). The paragraph\nvectors for the 25,000 labeled instances are then fed\nthrough a neural network with one hidden layer with 50\nunits and a logistic classi\ufb01er to learn to predict the senti-\nment.1\nAt test time, given a test sentence, we again freeze the rest\nof the network and learn the paragraph vectors for the test\nreviews by gradient descent. Once the vectors are learned,\nwe feed them through the neural network to predict the sen-\ntiment of the reviews.\nThe hyperparameters of our paragraph vector model are se-\nlected in the same manner as in the previous task. In par-\nticular, we cross validate the window size, and the opti-\nmal window size is 10 words. The vector presented to the\nclassi\ufb01er is a concatenation of two vectors, one from PV-\nDBOW and one from PV-DM. In PV-DBOW, the learned\nvector representations have 400 dimensions. In PV-DM,\nthe learned vector representations have 400 dimensions for\nboth words and documents. To predict the 10-th word, we\nconcatenate the paragraph vectors and word vectors. Spe-\ncial characters such as ,.!? are treated as a normal word.\nIf the document has less than 9 words, we pre-pad with a\nspecial NULL word symbol.\nResults:\nThe results of Paragraph Vector and other base-\nlines are reported in Table 2. As can be seen from the\nTable, for long documents, bag-of-words models perform\nquite well and it is dif\ufb01cult to improve upon them using\nword vectors. The most signi\ufb01cant improvement happened\nin 2012 in the work of (Dahl et al., 2012) where they com-\nbine a Restricted Boltzmann Machines model with bag-of-\nwords. The combination of two models yields an improve-\nment approximately 1.5% in terms of error rates.\nAnother signi\ufb01cant improvement comes from the work\nof (Wang & Manning, 2012). Among many variations they\ntried, NBSVM on bigram features works the best and yields\na considerable improvement of 2% in terms of the error\nrate.\nThe method described in this paper is the only approach\nthat goes signi\ufb01cantly beyond the barrier of 10% error rate.\n1In our experiments, the neural network did perform better\nthan a linear logistic classi\ufb01er in this task.\nIt achieves 7.42% which is another 1.3% absolute improve-\nment (or 15% relative improvement) over the best previous\nresult of (Wang & Manning, 2012).\nTable 2. The performance of Paragraph Vector compared to other\napproaches on the IMDB dataset. The error rates of other methods\nare reported in (Wang & Manning, 2012).\nModel\nError rate\nBoW (bnc) (Maas et al., 2011)\n12.20 %\nBoW (b\u2206t\u2019c) (Maas et al., 2011)\n11.77%\nLDA (Maas et al., 2011)\n32.58%\nFull+BoW (Maas et al., 2011)\n11.67%\nFull+Unlabeled+BoW (Maas et al., 2011)\n11.11%\nWRRBM (Dahl et al., 2012)\n12.58%\nWRRBM + BoW (bnc) (Dahl et al., 2012)\n10.77%\nMNB-uni (Wang & Manning, 2012)\n16.45%\nMNB-bi (Wang & Manning, 2012)\n13.41%\nSVM-uni (Wang & Manning, 2012)\n13.05%\nSVM-bi (Wang & Manning, 2012)\n10.84%\nNBSVM-uni (Wang & Manning, 2012)\n11.71%\nNBSVM-bi (Wang & Manning, 2012)\n8.78%\nParagraph Vector\n7.42%\n3.3. Information Retrieval with Paragraph Vectors\nWe turn our attention to an information retrieval task which\nrequires \ufb01xed-length representations of paragraphs.\nHere, we have a dataset of paragraphs in the \ufb01rst 10 results\nreturned by a search engine given each of 1,000,000 most\npopular queries. Each of these paragraphs is also known as\na \u201csnippet\u201d which summarizes the content of a web page\nand how a web page matches the query.\nFrom such collection, we derive a new dataset to test vector\nrepresentations of paragraphs. For each query, we create\na triplet of paragraphs: the two paragraphs are results of\nthe same query, whereas the third paragraph is a randomly\nsampled paragraph from the rest of the collection (returned\nas the result of a different query). Our goal is to identify\nwhich of the three paragraphs are results of the same query.\nTo achieve this, we will use paragraph vectors and compute\nthe distances the paragraphs. A better representation is one\nthat achieves a small distance for pairs of paragraphs of the\nsame query and a larg distance for pairs of paragraphs of\ndifferent queries.\nHere is a sample of three paragraphs, where the \ufb01rst para-\ngraph should be closer to the second paragraph than the\nthird paragraph:\n\u2022 Paragraph 1: calls from ( 000 ) 000 - 0000 . 3913\ncalls reported from this number . according to 4 re-\nports the identity of this caller is american airlines .\nDistributed Representations of Sentences and Documents\n\u2022 Paragraph 2: do you want to \ufb01nd out who called you\nfrom +1 000 - 000 - 0000 , +1 0000000000 or ( 000\n) 000 - 0000 ? see reports and share information you\nhave about this caller\n\u2022 Paragraph 3: allina health clinic patients for your\nconvenience , you can pay your allina health clinic\nbill online . pay your clinic bill now , question and\nanswers...\nThe triplets are split into three sets: 80% for training, 10%\nfor validation, and 10% for testing. Any method that re-\nquires learning will be trained on the training set, while its\nhyperparameters will be selected on the validation set.\nWe benchmark four methods to compute features for para-\ngraphs:\nbag-of-words, bag-of-bigrams, averaging word\nvectors and Paragraph Vector. To improve bag-of-bigrams,\nwe also learn a weighting matrix such that the distance be-\ntween the \ufb01rst two paragraphs is minimized whereas the\ndistance between the \ufb01rst and the third paragraph is max-\nimized (the weighting factor between the two losses is a\nhyperparameter).\nWe record the number of times when each method produces\nsmaller distance for the \ufb01rst two paragraphs than the \ufb01rst\nand the third paragraph. An error is made if a method does\nnot produce that desirable distance metric on a triplet of\nparagraphs.\nThe results of Paragraph Vector and other baselines are re-\nported in Table 3. In this task, we \ufb01nd that TF-IDF weight-\ning performs better than raw counts, and therefore we only\nreport the results of methods with TF-IDF weighting.\nThe results show that Paragraph Vector works well and\ngives a 32% relative improvement in terms of error rate.\nThe fact that the paragraph vector method signi\ufb01cantly out-\nperforms bag of words and bigrams suggests that our pro-\nposed method is useful for capturing the semantics of the\ninput text.\nTable 3. The performance of Paragraph Vector and bag-of-words\nmodels on the information retrieval task.\n\u201cWeighted Bag-of-\nbigrams\u201d is the method where we learn a linear matrix W on TF-\nIDF bigram features that maximizes the distance between the \ufb01rst\nand the third paragraph and minimizes the distance between the\n\ufb01rst and the second paragraph.\nModel\nError rate\nVector Averaging\n10.25%\nBag-of-words\n8.10 %\nBag-of-bigrams\n7.28 %\nWeighted Bag-of-bigrams\n5.67%\nParagraph Vector\n3.82%\n3.4. Some further observations\nWe perform further experiments to understand various as-\npects of the models. Here\u2019s some observations\n\u2022 PV-DM is consistently better than PV-DBOW. PV-\nDM alone can achieve results close to many results\nin this paper (see Table 2). For example, in IMDB,\nPV-DM only achieves 7.63%.\nThe combination of\nPV-DM and PV-DBOW often work consistently bet-\nter (7.42% in IMDB) and therefore recommended.\n\u2022 Using concatenation in PV-DM is often better than\nsum. In IMDB, PV-DM with sum can only achieve\n8.06%. Perhaps, this is because the model loses the\nordering information.\n\u2022 It\u2019s better to cross validate the window size. A good\nguess of window size in many applications is between\n5 and 12. In IMDB, varying the window sizes between\n5 and 12 causes the error rate to \ufb02uctuate 0.7%.\n\u2022 Paragraph Vector can be expensive, but it can be done\nin parallel at test time. On average, our implementa-\ntion takes 30 minutes to compute the paragraph vec-\ntors of the IMDB test set, using a 16 core machine\n(25,000 documents, each document on average has\n230 words).\n4. Related Work\nDistributed representations for words were \ufb01rst proposed\nin (Rumelhart et al., 1986) and have become a successful\nparadigm, especially for statistical language modeling (El-\nman, 1990; Bengio et al., 2006; Mikolov, 2012). Word vec-\ntors have been used in NLP applications such as word rep-\nresentation, named entity recognition, word sense disam-\nbiguation, parsing, tagging and machine translation (Col-\nlobert & Weston, 2008; Turney & Pantel, 2010; Turian\net al., 2010; Collobert et al., 2011; Socher et al., 2011b;\nHuang et al., 2012; Zou et al., 2013).\nRepresenting phrases is a recent trend and received much\nattention (Mitchell & Lapata, 2010; Zanzotto et al., 2010;\nYessenalina & Cardie, 2011; Grefenstette et al., 2013;\nMikolov et al., 2013c). In this direction, autoencoder-style\nmodels have also been used to model paragraphs (Maas\net al., 2011; Larochelle & Lauly, 2012; Srivastava et al.,\n2013).\nDistributed representations of phrases and sentences are\nalso the focus of Socher et al. (Socher et al., 2011a;c;\n2013b).\nTheir methods typically require parsing and is\nshown to work for sentence-level representations. And it\nis not obvious how to extend their methods beyond single\nsentences. Their methods are also supervised and thus re-\nquire more labeled data to work well. Paragraph Vector,\nDistributed Representations of Sentences and Documents\nin contrast, is mostly unsupervised and thus can work well\nwith less labeled data.\nOur approach of computing the paragraph vectors via gra-\ndient descent bears resemblance to a successful paradigm\nin computer vision (Perronnin & Dance, 2007; Perronnin\net al., 2010) known as Fisher kernels (Jaakkola & Haus-\nsler, 1999). The basic construction of Fisher kernels is the\ngradient vector over an unsupervised generative model.\n5. Discussion\nWe described Paragraph Vector, an unsupervised learning\nalgorithm that learns vector representations for variable-\nlength pieces of texts such as sentences and documents.\nThe vector representations are learned to predict the sur-\nrounding words in contexts sampled from the paragraph.\nOur experiments on several text classi\ufb01cation tasks such as\nStanford Treebank and IMDB sentiment analysis datasets\nshow that the method is competitive with state-of-the-art\nmethods. The good performance demonstrates the merits\nof Paragraph Vector in capturing the semantics of para-\ngraphs. In fact, paragraph vectors have the potential to\novercome many weaknesses of bag-of-words models.\nAlthough the focus of this work is to represent texts, our\nmethod can be applied to learn representations for sequen-\ntial data. In non-text domains where parsing is not avail-\nable, we expect Paragraph Vector to be a strong alternative\nto bag-of-words and bag-of-n-grams models.\nReferences\nBengio,\nYoshua,\nSchwenk,\nHolger,\nSen\u00b4ecal,\nJean-\nS\u00b4ebastien, Morin, Fr\u00b4ederic, and Gauvain, Jean-Luc.\nNeural probabilistic language models. In Innovations in\nMachine Learning, pp. 137\u2013186. Springer, 2006.\nCollobert, Ronan and Weston, Jason. A uni\ufb01ed architecture\nfor natural language processing: Deep neural networks\nwith multitask learning. In Proceedings of the 25th In-\nternational Conference on Machine Learning, pp. 160\u2013\n167. ACM, 2008.\nCollobert, Ronan, Weston, Jason, Bottou, L\u00b4eon, Karlen,\nMichael, Kavukcuoglu, Koray, and Kuksa, Pavel. Nat-\nural language processing (almost) from scratch.\nThe\nJournal of Machine Learning Research, 12:2493\u20132537,\n2011.\nDahl, George E., Adams, Ryan P., and Larochelle, Hugo.\nTraining Restricted Boltzmann Machines on word obser-\nvations. In International Conference on Machine Learn-\ning, 2012.\nElman, Jeff. Finding structure in time. In Cognitive Sci-\nence, pp. 179\u2013211, 1990.\nFrome, Andrea, Corrado, Greg S., Shlens, Jonathon, Ben-\ngio, Samy, Dean, Jeffrey, Ranzato, Marc\u2019Aurelio, and\nMikolov, Tomas. DeViSE: A deep visual-semantic em-\nbedding model. In Advances in Neural Information Pro-\ncessing Systems, 2013.\nGrefenstette, E., Dinu, G., Zhang, Y., Sadrzadeh, M., and\nBaroni, M. Multi-step regression learning for composi-\ntional distributional semantics. In Conference on Empir-\nical Methods in Natural Language Processing, 2013.\nHarris, Zellig. Distributional structure. Word, 1954.\nHuang, Eric, Socher, Richard, Manning, Christopher, and\nNg, Andrew Y.\nImproving word representations via\nglobal context and multiple word prototypes. In Pro-\nceedings of the 50th Annual Meeting of the Association\nfor Computational Linguistics: Long Papers-Volume 1,\npp. 873\u2013882.Association for Computational Linguistics,\n2012.\nJaakkola, Tommi and Haussler, David. Exploiting gener-\native models in discriminative classi\ufb01ers. In Advances\nin Neural Information Processing Systems 11, pp. 487\u2013\n493, 1999.\nKlein, Dan and Manning, Chris D.\nAccurate unlexical-\nized parsing. In Proceedings of Association for Compu-\ntational Linguistics, 2003.\nLarochelle, Hugo and Lauly, Stanislas. A neural autore-\ngressive topic model. In Advances in Neural Information\nProcessing Systems, 2012.\nMaas, Andrew L., Daly, Raymond E., Pham, Peter T.,\nHuang, Dan, Ng, Andrew Y., and Potts, Christopher.\nLearning word vectors for sentiment analysis. In Pro-\nceedings of the 49th Annual Meeting of the Association\nfor Computational Linguistics, 2011.\nMikolov, Tomas. Statistical Language Models based on\nNeural Networks. PhD thesis, Brno University of Tech-\nnology, 2012.\nMikolov, Tomas, Chen, Kai, Corrado, Greg, and Dean, Jef-\nfrey. Ef\ufb01cient estimation of word representations in vec-\ntor space. arXiv preprint arXiv:1301.3781, 2013a.\nMikolov, Tomas, Le, Quoc V., and Sutskever, Ilya. Ex-\nploiting similarities among languages for machine trans-\nlation. CoRR, abs/1309.4168, 2013b.\nMikolov, Tomas, Sutskever, Ilya, Chen, Kai, Corrado,\nGreg, and Dean, Jeffrey. Distributed representations of\nphrases and their compositionality. In Advances on Neu-\nral Information Processing Systems, 2013c.\nDistributed Representations of Sentences and Documents\nMikolov, Tomas, Yih, Scott Wen-tau, and Zweig, Geoffrey.\nLinguistic regularities in continuous space word repre-\nsentations. In NAACL HLT, 2013d.\nMitchell, Jeff and Lapata, Mirella. Composition in distri-\nbutional models of semantics. Cognitive Science, 2010.\nMnih, Andriy and Hinton, Geoffrey E.\nA scalable hi-\nerarchical distributed language model. In Advances in\nNeural Information Processing Systems, pp. 1081\u20131088,\n2008.\nMorin, Frederic and Bengio, Yoshua. Hierarchical proba-\nbilistic neural network language model. In Proceedings\nof the International Workshop on Arti\ufb01cial Intelligence\nand Statistics, pp. 246\u2013252, 2005.\nPang, Bo and Lee, Lillian. Seeing stars: Exploiting class\nrelationships for sentiment categorization with respect to\nrating scales. In Proceedings of Association for Compu-\ntational Linguistics, pp. 115\u2013124, 2005.\nPerronnin, Florent and Dance, Christopher. Fisher kernels\non visual vocabularies for image categorization. In IEEE\nConference on Computer Vision and Pattern Recogni-\ntion, 2007.\nPerronnin, Florent, Liu, Yan, Sanchez, Jorge, and Poirier,\nHerve.\nLarge-scale image retrieval with compressed\n\ufb01sher vectors. In IEEE Conference on Computer Vision\nand Pattern Recognition, 2010.\nRumelhart, David E, Hinton, Geoffrey E, and Williams,\nRonald J. Learning representations by back-propagating\nerrors. Nature, 323(6088):533\u2013536, 1986.\nSocher, Richard, Huang, Eric H., Pennington, Jeffrey,\nManning, Chris D., and Ng, Andrew Y. Dynamic pool-\ning and unfolding recursive autoencoders for paraphrase\ndetection. In Advances in Neural Information Process-\ning Systems, 2011a.\nSocher, Richard, Lin, Cliff C, Ng, Andrew, and Manning,\nChris. Parsing natural scenes and natural language with\nrecursive neural networks. In Proceedings of the 28th\nInternational Conference on Machine Learning (ICML-\n11), pp. 129\u2013136, 2011b.\nSocher, Richard, Pennington, Jeffrey, Huang, Eric H,\nNg, Andrew Y, and Manning, Christopher D.\nSemi-\nsupervised recursive autoencoders for predicting senti-\nment distributions.\nIn Proceedings of the Conference\non Empirical Methods in Natural Language Processing,\n2011c.\nSocher, Richard, Chen, Danqi, Manning, Christopher D.,\nand Ng, Andrew Y. Reasoning with neural tensor net-\nworks for knowledge base completion. In Advances in\nNeural Information Processing Systems, 2013a.\nSocher, Richard, Perelygin, Alex, Wu, Jean Y., Chuang, Ja-\nson, Manning, Christopher D., Ng, Andrew Y., and Potts,\nChristopher. Recursive deep models for semantic com-\npositionality over a sentiment treebank. In Conference\non Empirical Methods in Natural Language Processing,\n2013b.\nSrivastava, Nitish, Salakhutdinov, Ruslan, and Hinton, Ge-\noffrey. Modeling documents with deep boltzmann ma-\nchines. In Uncertainty in Arti\ufb01cial Intelligence, 2013.\nTurian, Joseph, Ratinov, Lev, and Bengio, Yoshua. Word\nrepresentations: a simple and general method for semi-\nsupervised learning. In Proceedings of the 48th Annual\nMeeting of the Association for Computational Linguis-\ntics, pp. 384\u2013394. Association for Computational Lin-\nguistics, 2010.\nTurney, Peter D. and Pantel, Patrick. From frequency to\nmeaning: Vector space models of semantics. Journal of\nArti\ufb01cial Intelligence Research, 2010.\nWang, Sida and Manning, Chris D. Baselines and bigrams:\nSimple, good sentiment and text classi\ufb01cation. In Pro-\nceedings of the 50th Annual Meeting of the Association\nfor Computational Linguistics, 2012.\nYessenalina, Ainur and Cardie, Claire.\nCompositional\nmatrix-space models for sentiment analysis. In Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, 2011.\nZanzotto,\nFabio,\nKorkontzelos,\nIoannis,\nFallucchi,\nFrancesca, and Manandhar, Suresh.\nEstimating linear\nmodels for compositional distributional semantics.\nIn\nCOLING, 2010.\nZhila, A., Yih, W.T., Meek, C., Zweig, G., and Mikolov,\nT. Combining heterogeneous models for measuring re-\nlational similarity. In NAACL HLT, 2013.\nZou, Will, Socher, Richard, Cer, Daniel, and Manning,\nChristopher.\nBilingual word embeddings for phrase-\nbased machine translation. In Conference on Empirical\nMethods in Natural Language Processing, 2013.\n",
        "sentence": " Our solution utilizes a state-of-the-art deep learning document embedding algorithms (also known as doc2vec) [8]. each word is distributed all along a word window in distributed representations (as known as word2vec and doc2vec feature) as shown in Figure 3 [8].",
        "context": "(25,000 documents, each document on average has\n230 words).\n4. Related Work\nDistributed representations for words were \ufb01rst proposed\nin (Rumelhart et al., 1986) and have become a successful\nparadigm, especially for statistical language modeling (El-\nman, 1990; Bengio et al., 2006; Mikolov, 2012). Word vec-\ntors have been used in NLP applications such as word rep-\nresentation, named entity recognition, word sense disam-\nbiguation, parsing, tagging and machine translation (Col-\nusing a weighted average of all the words in the document.\nA more sophisticated approach is combining the word vec-\ntors in an order given by a parse tree of a sentence, using\nmatrix-vector operations (Socher et al., 2011b). Both ap-"
    },
    {
        "title": "Toward the next generation of recommender systems: A survey of the state-of-the-art and possible extensions. Knowledge and Data Engineering",
        "author": [
            "Gediminas Adomavicius",
            "Alexander Tuzhilin"
        ],
        "venue": "IEEE Transactions on,",
        "citeRegEx": "9",
        "shortCiteRegEx": "9",
        "year": 2005,
        "abstract": "",
        "full_text": "",
        "sentence": " The main objective for the Recommendation Systems (RSs) is providing a user with content he/she would like by estimating the relevancy or the rating of these contents based on the information about users and the items [1], [9].",
        "context": null
    },
    {
        "title": "Item cold-start recommendations: learning local collective embeddings",
        "author": [
            "Martin Saveski",
            "Amin Mantrach"
        ],
        "venue": "In Proceedings of the 8th ACM Conference on Recommender systems,",
        "citeRegEx": "10",
        "shortCiteRegEx": "10",
        "year": 2014,
        "abstract": "",
        "full_text": "",
        "sentence": " Addressing cold-start is inevitable in modern RSs for two reasons [10]: Several methods have been introduced to address the coldstart and the data sparsity problems [10]\u2013[14]. Saveski and Mantrach [10] proposed Local Collective Embeddings",
        "context": null
    },
    {
        "title": "Methods and metrics for cold-start recommendations",
        "author": [
            "Andrew I Schein",
            "Alexandrin Popescul",
            "Lyle H Ungar",
            "David M Pennock"
        ],
        "venue": "In Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval,",
        "citeRegEx": "11",
        "shortCiteRegEx": "11",
        "year": 2002,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " [11] proposed an approach that creates a joint distribution of",
        "context": null
    },
    {
        "title": "Integrating trust and similarity to ameliorate the data sparsity and cold start for recommender systems",
        "author": [
            "Guibing Guo"
        ],
        "venue": "In Proceedings of the 7th ACM conference on Recommender systems,",
        "citeRegEx": "12",
        "shortCiteRegEx": "12",
        "year": 2013,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "A collaborative filtering approach to mitigate the new user cold start problem",
        "author": [
            "Jes\u00faS Bobadilla",
            "Fernando Ortega",
            "Antonio Hernando",
            "Jes\u00faS Bernal"
        ],
        "venue": "Knowledge-Based Systems,",
        "citeRegEx": "13",
        "shortCiteRegEx": "13",
        "year": 2012,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Combining content and collaboration in text filtering",
        "author": [
            "Ian Soboroff",
            "Charles Nicholas"
        ],
        "venue": "In Proceedings of the IJCAI,",
        "citeRegEx": "14",
        "shortCiteRegEx": "14",
        "year": 1999,
        "abstract": "",
        "full_text": "",
        "sentence": " Several methods have been introduced to address the coldstart and the data sparsity problems [10]\u2013[14]. Soboroff and Nicholas [14] utilized Similar to [14], Schein et al.",
        "context": null
    },
    {
        "title": "Indexing by latent semantic analysis",
        "author": [
            "Scott Deerwester",
            "Susan T Dumais",
            "George W Furnas",
            "Thomas K Landauer",
            "Richard Harshman"
        ],
        "venue": "Journal of the American society for information science,",
        "citeRegEx": "15",
        "shortCiteRegEx": "15",
        "year": 1990,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Latent Semantic Indexing (LSI) [15] in order to create topical representations of user profiles in the latent space.",
        "context": null
    },
    {
        "title": "Grouplens: an open architecture for collaborative filtering of netnews",
        "author": [
            "Paul Resnick",
            "Neophytos Iacovou",
            "Mitesh Suchak",
            "Peter Bergstrom",
            "John Riedl"
        ],
        "venue": "In Proceedings of the 1994 ACM conference on Computer supported cooperative work,",
        "citeRegEx": "16",
        "shortCiteRegEx": "16",
        "year": 1994,
        "abstract": "",
        "full_text": "",
        "sentence": " a neighborhood-based) techniques [16], [17].",
        "context": null
    },
    {
        "title": "Grouplens: applying collaborative filtering to usenet news",
        "author": [
            "Joseph A Konstan",
            "Bradley N Miller",
            "David Maltz",
            "Jonathan L Herlocker",
            "Lee R Gordon",
            "John Riedl"
        ],
        "venue": "Communications of the ACM,",
        "citeRegEx": "17",
        "shortCiteRegEx": "17",
        "year": 1997,
        "abstract": "",
        "full_text": "",
        "sentence": " a neighborhood-based) techniques [16], [17].",
        "context": null
    },
    {
        "title": "Item-based collaborative filtering recommendation algorithms",
        "author": [
            "Badrul Sarwar",
            "George Karypis",
            "Joseph Konstan",
            "John Riedl"
        ],
        "venue": "In Proceedings of the 10th international conference on World Wide Web,",
        "citeRegEx": "18",
        "shortCiteRegEx": "18",
        "year": 2001,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " After finding these similar neighbors, recommendations are generated by choosing the top-K items similar to a given item in case of itembased recommendations, or by aggregating the correlation scores of items liked by similar users in case of user-based recommendations [18]\u2013[20]. The choice of the similarity or correlation metric has a major contribution to the quality of recommendations [18].",
        "context": null
    },
    {
        "title": "Item-based top-n recommendation algorithms",
        "author": [
            "Mukund Deshpande",
            "George Karypis"
        ],
        "venue": "ACM Transactions on Information Systems (TOIS),",
        "citeRegEx": "19",
        "shortCiteRegEx": "19",
        "year": 2004,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Amazon. com recommendations: Item-to-item collaborative filtering",
        "author": [
            "Greg Linden",
            "Brent Smith",
            "Jeremy York"
        ],
        "venue": "IEEE Internet computing,",
        "citeRegEx": "20",
        "shortCiteRegEx": "20",
        "year": 2003,
        "abstract": "",
        "full_text": "",
        "sentence": " After finding these similar neighbors, recommendations are generated by choosing the top-K items similar to a given item in case of itembased recommendations, or by aggregating the correlation scores of items liked by similar users in case of user-based recommendations [18]\u2013[20].",
        "context": null
    },
    {
        "title": "Empirical analysis of predictive algorithms for collaborative filtering",
        "author": [
            "John S Breese",
            "David Heckerman",
            "Carl Kadie"
        ],
        "venue": "In Proceedings of the Fourteenth conference on Uncertainty in artificial intelligence,",
        "citeRegEx": "21",
        "shortCiteRegEx": "21",
        "year": 1998,
        "abstract": "Collaborative filtering or recommender systems use a database about user\npreferences to predict additional topics or products a new user might like. In\nthis paper we describe several algorithms designed for this task, including\ntechniques based on correlation coefficients, vector-based similarity\ncalculations, and statistical Bayesian methods. We compare the predictive\naccuracy of the various methods in a set of representative problem domains. We\nuse two basic classes of evaluation metrics. The first characterizes accuracy\nover a set of individual predictions in terms of average absolute deviation.\nThe second estimates the utility of a ranked list of suggested items. This\nmetric uses an estimate of the probability that a user will see a\nrecommendation in an ordered list. Experiments were run for datasets associated\nwith 3 application areas, 4 experimental protocols, and the 2 evaluation\nmetrics for the various algorithms. Results indicate that for a wide range of\nconditions, Bayesian networks with decision trees at each node and correlation\nmethods outperform Bayesian-clustering and vector-similarity methods. Between\ncorrelation and Bayesian networks, the preferred method depends on the nature\nof the dataset, nature of the application (ranked versus one-by-one\npresentation), and the availability of votes with which to make predictions.\nOther considerations include the size of database, speed of predictions, and\nlearning time.",
        "full_text": "43 \nEmpirical Analysis of Predictive Algorithms for Collaborative \nFiltering \nJohn S. Breese \nDavid Heckerman \nCarl Kadie \nMicrosoft Research \nRedmond, WA 98052-6399 \n{ breese,heckerma, carlk} @microsoft. com \nAbstract \nCollaborative filtering or recommender sys\u00ad\ntems use a database about user preferences to \npredict additional topics or products a new \nuser might like. In this paper we describe \nseveral algorithms designed for this task, in\u00ad\ncluding techniques based on correlation coef\u00ad\nficients, vector-based similarity calculations, \nand statistical Bayesian methods. We com\u00ad\npare the predictive accuracy of the various \nmethods in a set of representative problem \ndomains. We use two basic classes of evalua\u00ad\ntion metrics. The first characterizes accuracy \nover a set of individual predictions in terms of \naverage absolute deviation. The second esti\u00ad\nmates the utility of a ranked list of suggested \nitems. This metric uses an estimate of the \nprobability that a user will see a recommen\u00ad\ndation in an ordered list. \nExperiments were run for datasets associ\u00ad\nated with 3 application areas, 4 experimen\u00ad\ntal protocols, and the 2 evaluation met\u00ad\nrics for the various algorithms. \nResults \nindicate that for a wide range of con\u00ad\nditions, Bayesian networks with decision \ntrees at each node and correlation methods \noutperform Bayesian-clustering and vector\u00ad\nsimilarity methods. Between correlation and \nBayesian networks, the preferred method de\u00ad\npends on the nature of the dataset, nature \nof the application (ranked versus one-by-one \npresentation), and the availability of votes \nwith which to make predictions. Other con\u00ad\nsiderations include the size of database, speed \nof predictions, and learning time. \n1 \nIntroduction \nTypically, automated search over a corpus of items \nis based on a query identifying intrinsic features of \nthe items sought. Search for textual documents (e.g. \nWeb pages) uses queries containing words or describ\u00ad\ning concepts that are desired in the returned docu\u00ad\nments. Search for titles of compact discs, for example, \nrequires identification of desired artist, genre, or time \nperiod. Most content retrieval methodologies use some \ntype of similarity score to match a query describing the \ncontent with the individual titles or items, and then \npresent the user with a ranked list of suggestions. \nA complementary method of identifying potentially in\u00ad\nteresting content uses data on the preferences of a set \nof users. Typically, these systems do not use any infor\u00ad\nmation regarding the actual content (e.g. words, au\u00ad\nthor, description) of the items, but are rather based \non usage or preference patterns of other users. \nSo \ncalled collaborative filtering or recommender systems \n[Resnick and Varian, 1997] are built on the assump\u00ad\ntion that a good way to find interesting content is to \nfind other people who have similar interests, and then \nrecommend titles that those similar users like. \nThough there is increasing commercial interest in col\u00ad\nlaborative filtering technology, there has been little \npublished research on the relative performance of var\u00ad\nious algorithms used in collaborative filtering systems. \nIn this paper we describe various collaborative filtering \nprediction methodologies, including previously pub\u00ad\nlished algorithms based on correlation coefficients, as \nwell as algorithms based on learning Bayesian mod\u00ad\nels. We present empirical data regarding the relative \npredictive performance of the various algorithms and \nextensions. Although we present some results address\u00ad\ning the computational and scalability issues involved in \napplying the various algorithms, our primary empha\u00ad\nsis is the accuracy and the quality of recommendations \nof the predictive component. \n44 \nBreese, Heckerman, and Kadie \n2 \nCollaborative Filtering Algorithms \nThe task in collaborative filtering is to predict the util\u00ad\nity of items to a particular user (the active user) based \non a database of user votes from a sample or popula\u00ad\ntion of other users (the user database). In this paper \nwe will examine two general classes of collaborative \nfiltering algorithms. Memory-based algorithms oper\u00ad\nate over the entire user database to make predictions. \nIn Model-based collaborative filtering, in contrast, uses \nthe user database to estimate or learn a model, which \nis then used for predictions. \nCollaborative filtering systems are often distinguished \nby whether they operate over implicit versus explicit \nvotes. Explicit voting refers to a user consciously ex\u00ad\npressing his or her preference for a title, usually on a \ndiscrete numerical scale. For example, GroupLens sys\u00ad\ntem of Resnick et al. [1994) uses a scale of one (bad) \nto five (good) for users to rate Netnews articles, and \nusers explicitly rate each article after reading it. Im\u00ad\nplicit voting refers to interpreting user behavior or se\u00ad\nlections to impute a vote or preference. Implicit votes \ncan based on browsing data (for example in Web ap\u00ad\nplications), purchase history (for example in online or \ntraditional stores), or other types of information access \npatterns. \nRegardless of the type of vote data available, collab\u00ad\norative filtering algorithms must address the issue of \nmissing data- we typically do not have a complete \nset of votes across all titles. We cannot assume that \nitems are missing at random. In most applications, \nusers will vote on items they have accessed, and are \nmore likely to access (and vote) on items they like. \nMany of the applications of interest to us involve im\u00ad\nplicit voting, and some of the algorithms described in \nthe next section rely on an interpretation that any vote \nappearing in the database indicates a positive pref\u00ad\nerence. \nWe also show that by making different as\u00ad\nsumptions about the nature of missing data, the per\u00ad\nformance of collaborative filtering algorithms can be \nimproved. \n2.1 \nMemory-Based Algorithms \nGenerally, the task in collaborative filtering is to pre\u00ad\ndict the votes of a particular user (we will refer to this \nuser as the active user) from a database of user votes \nfrom a sample or population of other users. The user \ndatabase therefore consists of a set of votes vi,j corre\u00ad\nsponding to the vote for user i on item j. If Ii is the \nset of items on which user i has voted, then we can \ndefine the mean vote for user i as: \n1 \n'ih \n= -IJ.I L Vi,j \n2 jEI; \nIn memory-based collaborative filtering algorithms, we \npredict the votes of the active user (indicated with a \nsubscript a) based on some partial information regard\u00ad\ning the active user and a set of weights calculated from \nthe user database. We assume that the predicted vote \nof the active user for item j, Pa,j, is a weighted sum of \nthe votes of the other users: \nn \nPa,j = Va + ;; L w(a, i)(vi,j- 'ih) \n(1) \ni=l \nwhere n is the number of users in the collaborative \nfiltering database with nonzero weights. The weights \nw(i, a) can reflect distance, correlation, or similarity \nbetween each user i and the active user. D; is a normal\u00ad\nizing factor such that the absolute values of the weights \nsum to unity. In the following, we distinguish between \nthe various collaborative filtering algorithms in terms \nof the details of the \"weight\" calculation. There are \nother possible characterizations for memory-based col\u00ad\nlaborative filtering, however in this paper we restrict \nourselves to the formulation described above. \n2.1.1 \nCorrelation \nThis general formulation of statistical collaborative \nfiltering (as opposed to verbal or qualitative anno\u00ad\ntations) first appeared in the published literature in \nthe context of the GroupLens project, where the Pear\u00ad\nson correlation coefficient was defined as the basis for \nthe weights [Resnick et al., 1994). The correlation be\u00ad\ntween users a and i is: \n( \n\") \n\"L-j(va,j- Va)(vi,j- Vi) \nw a, 2 \n= 1============== \nJ\"L-j ( Va,j - Va)2 \"L-j ( Vi,j - Vi)2 \n(2) \nwhere the summations over j are over the items for \nwhich both users a and i have recorded votes. \n2.1.2 \nVector Similarity \nIn the field of information retrieval, the similarity be\u00ad\ntween two documents is often measured by treating \neach document as a vector of word frequencies and \ncomputing the cosine of the angle formed by the two \nfrequency vectors [Salton and McGill, 1983). We can \nadopt this formalism to collaborative filtering, where \nusers take the role of documents, titles take the role \nof words, and votes take the role of word frequencies. \nNote that under this algorithm, observed votes indi\u00ad\ncate a positive preference, there is no role for negative \nAnalysis of Algorithms for Collaborative Filtering \n45 \nvotes, and unobserved items receive a zero vote. The \nrelevant weights are now \n(3) \nwhere the squared terms in the denominator serve to \nnormalize votes so that users that vote on more ti\u00ad\ntles will not a priori be more similar to other users. \nOther normalization schemes, including absolute sum \nand number of votes, are possible. \n2.2 \nExtensions to Memory-Based Algorithms \nWe have investigated a number of modifications to the \nstandard algorithms that can improve performance. \nWe describe these extensions here and the effective\u00ad\nness of each is discussed in Section 4. \n2.2.1 \nDefault Voting \nDefault voting is an extension to the correlation algo\u00ad\nrithm described in Section 2.1.1. It arose out of the \nobservation that when there are relatively few votes, \nfor either the active user or the matching user, the cor\u00ad\nrelation algorithm will not do well because it uses only \nvotes in the intersection of the items both individuals \nhave voted on (Ian Ij)\u00b7 If we assume some default \nvalue as a vote for titles for which we do not have \nexplicit votes, then we can form the match over the \nunion of voted items,(Ia U Ij), where the default vote \nvalue is inserted into the formula for the appropriate \nunobserved items. \nIn addition, we can assume the same default vote value \nd for some number of additional items k that neither \nuser has voted on. This has the effect of assuming \nthere are some additional number of unspecified items \nthat neither user voted on, but they would nonetheless \nagree on.1 In most cases, the value for d will reflect \na neutral or somewhat negative preference for these \nunobserved items. \nIn applications with implicit voting, an observed vote \nis typically an indication of a positive preference (e.g. a \nvisit to the Web page is assigned a vote value of 1). In \nthis case the default vote can take on the value associ\u00ad\nated with \"did not visit\" or 0. In this instance, default \nvoting takes on the role of extending the data for each \nuser with the true value for missing data. Note, how\u00ad\never, we only calculate weights for users who match \nthe active user on at least one item. \n1In our experiments, we have used a value of 10,000 or \nk. \n2.2.2 \nInverse User Frequency \nIn applications of vector similarity in information re\u00ad\ntrieval, word frequencies are typically modified by the \ninverse document frequency [Salton and McGill, 1983]. \nThe idea is to reduce weights for commonly occurring \nwords, capturing the intuition that they are not as use\u00ad\nful in identifying the topic of a document, while words \nthat occur less frequently are more indicative of topic. \nWe can apply an analogous transformation to votes \nin a collaborative filtering database, which we term \ninverse user frequency. The idea is that universally \nliked items are not as useful in capturing similarity as \nless common items. We define the fi as log ;:. where \n1 \nnj is the number of users who have voted for item j \nand n is the total number of users in the database. \nNote that if everyone has voted on a item j, then the \nfi is zero. \nTo apply inverse user frequency while using the vec\u00ad\ntor similarity algorithm, we use a transformed vote in \nEquation 3. The transformed vote is simply the orig\u00ad\ninal vote multiplied by the fi factor. In the case of \ncorrelation, we modify Equation 2 so that the fi is \ntreated as a frequency and an item with a higher fi \nis assigned more weight in the correlation calculation. \nThe relevant correlation weight with inverse frequency \nis: \nw(a,i) = \nwhere \n2.2.3 \nLj /j Lj fiva,jVi,j- (Lj fiva,j)(Lj fivi,j)) \nVfJV \nj \nj \nj \nj \nj \nj \nCase Amplification \nCase amplification refers to a transform applied to the \nweights used in the basic collaborative filtering pre\u00ad\ndiction formula as in Equation 1. We transform the \nestimated weights as follows \nI \n{ Wp. \nW \n_ \na,t \na,i-\n-( \n-wP .) \na,t \nif Wa,i 2:: 0 \nif Wa,i < 0 \nThe transform emphasizes weights that are closer to \none, and punishes low weights. A typical value for p \nfor our experiments is 2.5. \n46 \nBreese, Heckerman, and Kadie \n2.3 \nModel-Based Methods \nFrom a probabilistic perspective, the collaborative fil\u00ad\ntering task can be viewed as calculating the expected \nvalue of a vote, given what we know about the user. \nFor the active user, we wish to predict votes on as\u00ad\nyet unobserved items. If we assume that the votes are \ninteger valued with a range for 0 to m we have: \nm \nPa,j = E(va,j) = '2:: Pr (va,j = ilva,k, k E Ia) i (4) \ni=O \nwhere the probability expression is the probability that \nthe active user will have a particular vote value for \nitem j given the previously observed votes. In this \npaper we examine two alternative probabilistic models \nfor collaborative filtering, cluster models and Bayesian \nnetworks. \n2.3.1 \nCluster Models \nOne plausible probabilistic model for collaborative fil\u00ad\ntering is a Bayesian classifier where the probability of \nvotes are conditionally independent given membership \nin an unobserved class variable C taking on some rel\u00ad\natively small number of discrete values. The idea is \nthat there are certain groups or types of users cap\u00ad\nturing a common set of preferences and tastes. Given \nthe class, the preferences regarding the various items \n(expressed as votes) are independent. The probability \nmodel relating joint probability of class and votes to a \ntractable set of conditional and marginal distributions \nis the standard \"naive\" Bayes formulation: \nn \nPr (C = c,v1, ... ,vn) = Pr(C = c) Il Pr (viiC =c) \ni=l \nThe left-hand side of this expression is the probability \nof observing an individual of a particular cl_(l.Ss and a \ncomplete set of vote values. It is straightforWard to cal\u00ad\nculate the needed probability expressions for Equation \n4 within this framework. This model is also known as \na multinomial mixture model. \nThe parameters of the model, the probabilities of class \nmembership Pr(C \n= c) , and the conditional prob\u00ad\nabilities of votes given class Pr (v;IC =c) are esti\u00ad\nmated from a training set of user votes, the user \ndatabase. Since we never observe the class variables in \nthe database of users, we must employ methods that \ncan learn parameters for models with hidden variables. \nWe use the EM algorithm [Dempster et al., 1977) to \nlearn the parameters for a model structure with a fixed \nnumber of classes. We choose the number of classes by \nselecting the model structure that yields the largest \n(approximate) marginal likelihood of the data. \nWe \n\u0002 \nBeverly Hills, 90210 \nBeverly Hills, 90210 \n\u0001 \nNot Watched \nFriends \nWatched \nI \nWatched fipiiiiiiii! \nNot Watched lill \nNot \nFriends \nNot Watched \nI \nWatched I \nNot WatchedN\nC lli!iii!IIJII \nFigure 1: A decision tree for whether an individual \nwatched \"Melrose Place\", with parents \"Friend's\", and \n\"Beverly Hills, 90201\". The bar charts at the bot\u00ad\ntom of the tree indicate the probabilities of watched \nand not watched for \"Melrose Place\", conditioned on \nviewing the parent programs. \nuse the method of Cheeseman and Stutz (1995) to ap\u00ad\nproximate the marginal likelihood (see also Chicker\u00ad\ning and Beckerman, 1997). In our experiments, we \nassume each model structure (every possible number \nof classes) is equally likely, and use a uniform prior \nfor model parameters. We initialize the EM algorithm \nusing the marginal-plus-noise technique described in \n[Thiesson et al., 1 997). \n2.3.2 \nBayesian Network Model \nAn alternative model formulation for probabilistic col\u00ad\nlaborative filtering is a Bayesian network with a node \ncorresponding to each item in the domain. The states \nof each node correspond to the possible vote values for \neach item. We also include a state corresponding to \n\"no vote\" for those domains where there is no natural \ninterpretation for missing data. \nWe then apply an algorithm for learning Bayesian net\u00ad\nworks to the training data, where missing votes in the \ntraining data are indicated by the \"no vote\" value. \nThe learning algorithm searches over various model \nstructures in terms of dependencies for each item. In \nthe resulting network, each item will have a set of par\u00ad\nent items that are the best predictors of its votes. Each \nconditional probability table is represented by a deci\u00ad\nsion tree encoding the conditional probabilities for that \nnode. An example of such a tree, for television view\u00ad\ning data (see Section 3.2) is shown in Figure 1. Details \nof the learning algorithm are discussed in Chickering \net al.(1997). In the remainder of the paper the term \nBayesian network will refer to these networks with a \ndecision tree for each title. \nIn the experiments that follow, we use a structure \nprior that penalizes each additional free parameter \nwith probability 0.1, and derive parameter priors from \na prior network as described in Chickering et al., 1997. \nAnalysis of Algorithms for Collaborative Filtering \n47 \nIn particular, we use a prior network that encodes a \nuniform distribution over all possible outcomes and an \nequivalent sample size of 10. Experiments on subsets \nof the training data showed these parameters to pro\u00ad\nduce accurate results, although there was little sensi\u00ad\ntivity. \n3 \nEmpirical Analysis \nThe purpose of this paper is to evaluate the predictive \naccuracy of the various algorithms for collaborative fil\u00ad\ntering. In this section we will describe the evaluation \ncriteria, the various protocols, and the datasets used \nin the analysis. We then present and discuss the re\u00ad\nsults regarding predictive accuracy, as well as several \ncomputational considerations. \n3.1 \nEvaluation Criteria \nThe effectiveness of a collaborative filtering algorithm \ndepends on manner in which recommendations will be \npresented to the user. To evaluate these algorithms, we \nhave defined metrics based on the type of collaborative \nfiltering application and interface one is providing. \nThere are two basic classes of collaborative filtering ap\u00ad\nplications. In the first class, individual items are pre\u00ad\nsented one-at-a-time to the users along with a rating \nindicating potential interest in the topic. The original \nGroupLens system was in this category- each article \nin a GNUs-like Netnews interface has an ASCII bar\u00ad\nchart indicating the system's prediction regarding the \nuser's possible interest in that article. Thus, each piece \nof content has an associated estimated rating, and the \nuser interface displays this estimate along with a link \nto the content or as a part of the display or presenta\u00ad\ntion of the item. \nA second class of collaborative filtering applications \npresent the user with an ordered list of recommended \nitems. Examples of systems that present recommen\u00ad\ndation lists include PHOAKS (L.Terveen et al., 1997] \nand SiteSeer (Rucker and Polanco, 1997]. In the spirit \nof the Internet search engines, these systems provide \na ranked list of items (Web sites, music recordings) \nwhere highest ranked items are predicted to be most \npreferred. In these types of systems, the user presum\u00ad\nably will investigate items in the ordered list starting \nat the top hoping to find interesting items. \nWe \nhave \napplied \ntwo \nscoring \nmetrics \nin \nour \nevaluations-one appropriate for individual item-by\u00ad\nitem recommendations and the other appropriate for \nranked lists. In both cases, the basic evaluation se\u00ad\nquence proceeds as follows. A dataset of users (and \ntheir votes) is divided into a training set and a test \nset. The data for the training set is used as the col-\nlaborative filtering database or to build a probabilistic \nmodel. We then cycle through the users in the test \nset, treating each user as the active user. We divide \nthe votes for each test user into a set of votes that we \ntreat as observed, Ia, and a set that we will attempt \nto predict, Pa. We use the votes in Ia to predict the \nvotes in Pa as shown in Equations 1 and 4. \nFor individual scoring, we look at the average absolute \ndeviation of the predicted vote to the actual vote on \nitems the users in the test set have actually voted on. \nThat is, if the number of predicted votes in the test \nset for the active case is ma, then the average absolute \ndeviation for a user is: \n1 \nSa=- L IPa,j- Va,jl \nma jEPa \nThese scores are then averaged over all the users in \nthe test set of users. This metric was also used in \nevaluating the GroupLens project (Miller et al., 1997]. \nFor ranked scoring, the story is a bit more complex. \nIn information retrieval research, ranked lists of re\u00ad\nturned items are evaluated in terms of recall and pre\u00ad\ncision. For a given number of returned items, recall \nis the percentage of relevant items that were returned \nand precision is the percentage of returned items that \nare relevant. In a collaborative filtering framework, if \nvotes were binary (like and dislike) and we had com\u00ad\nplete preference judgments for a set of users we could \ndevelop a similar metric. However, more generally, we \nwish to estimate the expected utility of a particular \nranked list to a user. The expected utility of a list is \nsimply the probability of viewing a recommended item \ntimes its utility. In this analysis, we will equate the \nutility of an item with the difference between the vote \nand the default or neutral vote in the domain. \nFurthermore, we make an estimate of how likely it is \nthat the user will visit an item on a ranked list. We \nposit that each successive item in a list is less likely to \nbe viewed by the user with an exponential decay. Then \nthe expected utility of a ranked list of items (sorted by \nindex j in order of declining Va,j) is: \nR \n= '\"\"\" max( Va,j - d, 0) \na \nL.; \n2(j-1)/(<>-1) \nj \n(5) \nwhere d is the neutral vote and a: is the viewing halflife. \nThe halflife is the number of the item on the list such \nthat there is a 50-50 chance the user will review that \nitem. For these experiments, we used a halflife of 5 \nitems. 2 \n2We ran a set of experiments using a halflife of 10 items \nand found little sensitivity of results. \n48 \nBreese, Heckennan, and Kadie \nIn scoring a ranked list generated for a user, we ap\u00ad\nply Equation 5 using observed votes where available. \nFor items that are not available, we apply the neutral \nvote, d, which effectively removes those items from the \nscoring. The final score for an experiment over a set \nof active users in the test set is \nR = 100 L:a Ra \n: Rmax \n6a a \nwhere R;::ax is the maximum achievable utility if all \nobserved items had been at the top of the ranked list, \nordered by vote value. This transformation allows us \nto consider results independent of the size of the test \nset and number of items predicted in a given experi\u00ad\nment. \n3.2 \nDatasets \nWe evaluated the algorithm for three separate \ndatasets, as follows: \n\u2022 MS Web: This dataset captures individual visits \nto various areas ( vroots) of the Microsoft corpo\u00ad\nrate web site. This is an example of an implicit \nvoting database and application. Each vroot was \ncharacterized as being visited (vote of one) or not \n(no vote). \n\u2022 Television: \nThis dataset uses Neilsen network \ntelevision viewing data for individuals for a two \nweek period in the summer of 1996. The data was \ntransformed into binary data indicating whether \neach show was watched, or not, as above.3 \n\u2022 EachMovie: This is an explicit voting example us\u00ad\ning data from the EachMovie collaborative filter\u00ad\ning site deployed by Digital Equipment Research \nCenter from 1995 through 1997. 4 Votes ranged \nin value from 0 to 5. \nTable 3.2 provides additional information about each \ndataset. \n3.3 \nProtocols \nWe did two classes of experiments reflecting differing \nnumbers of votes available to the recommenders. In \nthe first protocol, we withhold a single randomly se\u00ad\nlected vote for each user in the test set, and try to \npredict its value given all the other votes the user has \nvoted on. We term this protocol All but 1. In the sec\u00ad\nond set of experiments, we randomly select 2, 5, or 10 \n3This dataset was made available for this study courtesy \nof Nielsen Media Research. \n4For \nmore \ninformation \nsee \nhttp:/ jwww. research. digital. com/SRC /EachMoviej. \nDataset \nMSWEB \nNeilsen \nEachmovie \nTotal users \n3453 \n1463 \n4119 \nTotal titles \n294 \n203 \n1623 \nMean votes \nper user \n3.95 \n9.55 \n46.4 \nMedian votes \nper user \n3 \n8 \n26 \nTable 1: Number of users, titles, and votes for the \ndatasets used in testing the algorithms. Only users \nwith 2 or more votes are considered. \nvotes from each test user as the observed votes, and \nthen attempt to predict the remaining votes. We call \nthese protocols Given 2, Given 5, and Given 10. \nThe All but 1 experiments measure the algorithms' \nperformance when given as much data as possible from \neach test user. The various Given experiments look at \nusers with less data available, and examine the perfor\u00ad\nmance of the algorithms when there is relatively little \nknown about an active user. In running the tests, if \na prospective test did not have adequate votes for a \ntrial it was eliminated from the evaluation. Thus the \nnumber of trials evaluated under each protocol vary. \n4 \nResults \nIn the following sections, we compare algorithms and \nanalyze the effects of individual algorithmic exten\u00ad\nsions. We use randomized block design where each \nalgorithm is run on the same test cases and observed \nvotes. We will refer to one of these comparisons as an \nexperiment. Our analyses uses ANOVA with the Bon\u00ad\nferroni procedure for multiple comparisons statistics \n[McClave and Dietrich, 1988). In the tables that fol\u00ad\nlow, the value in the last row is labeled RD for Required \nDifference. The difference between any two scores in \na column must be at least as big as the value in the \nRD row in order to be considered statistically signif\u00ad\nicant at the 90% confidence level for the experiment \nas a whole. As a visual aid, a score in boldface is \nsignificantly different from the score directly below it \nin the table. \n4.1 \nOverall Performance \nThe following tables show the performance of the vari\u00ad\nous major classes of algorithms on the various datasets \nand experiments. We compared the best performing \nvariation of each algorithm on each dataset, for the \ndifferent protocols. We also present the scores that \nresult from presenting the user with the most popular \nitems, regardless of the known votes of the individ-\nAnalysis of Algorithms for Collaborative Filtering \n49 \nMS Web, Rank Scoring \nAlgorithm \nGiven2 \nGiven5 \nGiven10 \nAllBut1 \nBN \n59.95 \n59.84 \n53.92 \n66.69 \nCR+ \n60.64 \n57.89 \n51.47 \n63.59 \nVSIM \n59.22 \n56.13 \n49.33 \n61.70 \nBC \n57.03 \n54.83 \n47.83 \n59.42 \nPOP \n49.14 \n46.91 \n41.14 \n49.77 \nRD \n0.91 \n1.82 \n4-49 \n0.93 \nTable 2: \nRanked scoring results for the MS Web \ndataset. Higher scores indicate better performance. \nual. This results in a baseline performance of a \"zero\u00ad\norder\" collaborative filtering system, and is labeled as \nPOP in the tables. The algorithm labeled CR+ refers \nto use of the correlation technique with inverse user \nfrequency, default voting, and case amplification ex\u00ad\ntensions. VSIM refers to using the vector similarity \nmethod with the inverse user frequency transforma\u00ad\ntion. BN and BC refer to the Bayesian network and \nclustering models respectively. \nOur results show that Bayesian networks with deci\u00ad\nsion trees at each node and correlation methods are \nthe best performing algorithms over the experiments \nwe have run. We ran 16 combinations of dataset, pro\u00ad\ntocol, and scoring criteria. The Bayesian network and \ncorrelation-based were each either best, or statistically \nequivalent, in 10 cases. Bayesian clustering was best \nperforming in 2 cases and vector similarity was best in \n3 cases. \nWe see that the Bayesian network performs best un\u00ad\nder the All but 1 protocol. Generally, all the methods \nperform less well in the Given 2 and Given 5 protocols \nas might be expected. However the vector similarity \nand clustering methods are competitive for some of \nthese limited-data scenarios, since these methods can \nuse partial information effectively. \nTable 2 shows data for rank scoring for the Microsoft \nweb site dataset. \nFor ranked scoring, higher scores \nindicate better performance. We see the Bayesian net\u00ad\nwork model results in the best, or statistically equiv\u00ad\nalent to the best, score for all protocols. Correlation, \nwith the appropriate enhancements designed to im\u00ad\nprove ranked scoring, is fairly close in performance. \nNote that correlation without default voting cannot \noperate on binary data with implicit voting, since all \nobserved votes will have the same value. The vector \nsimilarity algorithm is slightly worse than correlation. \nAll these algorithms outperform using popularity as a \nrecommender. \nFor the Neilsen dataset (Table 3), the Bayesian net\u00ad\nwork outperforms the other algorithms except for the \nNeilsen, Rank Scoring \nAlgorithm \nGiven2 \nGiven5 \nGiven10 \nAllButl \nBN \n34.90 \n42.24 \n47.39 \n44.92 \nCR+ \n39.44 \n43.23 \n43.47 \n39.49 \nVSIM \n39.20 \n40.89 \n39.12 \n36.23 \nBC \n19.55 \n18.85 \n22.51 \n16.48 \nPOP \n20.17 \n19.53 \n19.04 \n13.91 \nRD \n1.53 \n1.78 \n2.42 \n2.40 \nTable 3: \nRanked scoring results for the Neilsen \ndataset. Higher scores indicate better performance. \nEachMovie, Rank Scoring \nAlgorithm \nGiven2 \nGiven5 \nGiven10 \nAllBut1 \nCR+ \n41.60 \n42.33 \n41.46 \n23.16 \nVSIM \n42.45 \n42.12 \n40.15 \n22.07 \nBC \n38.06 \n36.68 \n34.98 \n21.38 \nBN \n28.64 \n30.50 \n33.16 \n23.49 \nPOP \n30.80 \n28.90 \n28.01 \n13.94 \nRD \n0.75 \n0.75 \n0.78 \n0.78 \nTable 4: Ranked scoring results for the EachMovie \ndataset. Higher scores indicate better performance. \nGiven 2 protocol. Correlation, with extensions, and \nvector similarity are fairly close in performance, while \nBayesian clustering performs relatively poorly. We see \nthat the Bayesian network drops off in performance \nquite significantly for the Given 2 protocol, relative to \ncorrelation and vector similarity. We will discuss this \nobservation below. \nWe see a somewhat different pattern for EachMovie \nunder ranked scoring, shown in Table 4. Here the cor\u00ad\nrelation algorithm is the top performer overall, with \nvector similarity performing well with less data. For \nthis dataset and score, the Bayesian network performs \nworse than any of the other algorithms on all the Given \nexperiments, but is the top performer and is competi\u00ad\ntive with correlation for the All but 1 protocol. \nThe Bayesian networks using decision trees suffer in \nthe Given scenarios because they are provided with \nrelatively little data. \nIf a title that is held out for \ntesting appears near the top of a tree, then it's value \nis set to \"no vote\" in evaluating the probability of a \npossibly related title. This may result in a title that is \nprovided being ignored or having little impact, simply \ndue to the ordering of the various predicting titles in \nthe tree. The various All But 1 experiments are able \nto utilize trees to a fuller extent, and therefore perform \nwell relative to the other methods that can use partial \ndata. \n50 \nBreese, Heckerman, and Kadie \nEachMovie, Absolute Deviation \nAlgorithm \nGiven2 \nGiven5 \nGiven10 \nAllBut1 \nCR \n1.257 \n1.139 \n1.069 \n0.994 \nBC \n1.127 \n1.144 \n1.138 \n1.103 \nBN \n1.143 \n1.154 \n1.139 \n1.066 \nVSIM \n2.113 \n2.177 \n2.235 \n2.136 \nRD \n0.022 \n0.023 \n0.025 \n0.043 \nTable 5: Absolute Deviation scoring results for the \nEachMovie dataset. Lower scores are better. \nFor absolute deviation, we examined the EachMovie \ndataset and results are shown in Table 5. This dataset \nhas a vote range of 0 to 5, making vote prediction \na relevant task. We examine the same algorithms as \nin the previous table, except now we use a correla\u00ad\ntion algorithm without applying any of the extensions \nexcept for inverse user frequency. The other exten\u00ad\nsions are not effective for absolute deviation scoring. \nThis basic correlation algorithm performs best in all \nbut the Given 2 experiments, indicating that this al\u00ad\ngorithm performs well when given adequate data re\u00ad\ngarding the active case. The Bayesian clustered model \ndoes slightly better than the Bayesian network, and \noutperforms correlation in the Given 2 and Given 5 \ncases. \n4.2 \nInverse User Frequency \nIn Section 2.2.2 we describe using inverse user fre\u00ad\nquency to modify vote values in applying memory\u00ad\nbased algorithms. We performed a set of 12 experi\u00ad\nments (3 datasets, 4 protocols) each for vector sim\u00ad\nilarity and correlation judging the effect of applying \ninverse user frequency under ranked scoring. In all \nexperiments, application of IUF improved the ranked \nscore, and in 23 of 24 cases results were statistically \nsignificant. The average improvement was 1.9%, with \nan improvement of 2.2% for the vector similarity algo\u00ad\nrithm, and 1.5% for the correlation algorithm. \nIn 8 experiments run on the EachMovie dataset using \nabsolute deviation scoring, the improvement averaged \na more impressive 11%. Results were significant in 6 \nof the 8 experiments. The average improvement was of \n15.5% for vector similarity, and 6.5% for correlation. \n4.3 \nCase Amplification \nCase amplification (Section 2.2.3) modifies weights \nused in an memory-based algorithm to emphasize \nhigher weights. We performed a set of 12 experiments \n(3 datasets, 4 protocols) applying case amplification to \ncorrelation. The average improvement in the ranked \nscore was 4.8%, and results were significant in 11 of 12 \nexperiments. There is no significant effect of case am\u00ad\nplification on absolute deviation scoring. We also ran \nexperiments combining case amplification and inverse \nuser frequency, and found the benefits to be additive. \n4.4 \nProbabilistic Methods \nWe used a training set to build probabilistic models \nfor each dataset. Each title was encoded with an addi\u00ad\ntional explicit vote value of \"no vote\" to complete the \ndataset for probabilistic learning. When scoring with \nBayesian networks and cluster models, the \"no vote\" \nvalues were explicitly entered into the network when \nmissing, for both ranked and absolute deviation scor\u00ad\ning. For the trees, the \"no vote\" values were entered \nin each tree independently in order to generate a prob\u00ad\nability for that title. For absolute deviation scoring, \nthe expected vote was calculated by renormalizing the \noutput probabilities, clamping the \"no vote\" probabil\u00ad\nity to zero. \nThere are roughly 1600 movies in the EachMovie \ndataset, too many to estimate a full model in a reason\u00ad\nable amount of time. Therefore the Bayesian methods \nwere trained from EachMovie for the top 300 movies \nin terms of overall popularity. For testing, all 1600 \nmovies were used. In the other datasets, all items were \nused for training and testing. \nFor the Bayesian networks, we applied alternate prior \nspecifications which resulted in trees of varying com\u00ad\nplexity. Priors that strongly penalized splits generated \nBayesian networks with nodes with approximately 2 to \n4 parents and 4 to 6 distributions in the decision tree \nrepresentation. The model with the larger trees had \nsomewhere between 4 and 6 predecessors and 6 to 8 \ndistributions per variable. In all our experiments the \nlarger trees outperformed the smaller tree so we re\u00ad\nstrict our results to those models. Additional details \nare available in Breese et al. (1998). \nApplying clustering to the datasets identified 3 classes \nfor the Neilsen dataset, 7 classes for the MS Web \ndataset, and 9 classes for the EachMovie dataset. The \nclasses found by clustering for the MS Web dataset are \nshown below. Each entry is a page area or virtual root \nthat distinguishes this class from the others. The class \nnames on the left were manually generated based on \ninspecting the resulting classes. \nSupport Support Desktop, Knowledge Base, Win\u00ad\ndows95 Support, Search, NT Server Support \nWindows Products, Free Downloads, Windows95, \nWindows95 Support, Windows Family of Prod\u00ad\nucts \nOffice Products, MS Office Info, Free Downloads, MS \nAnalysis of Algorithms for Collaborative Filtering \n51 \nWord News, Office Free Stuff, MS Office \nDevelopers Search, Training, Games, Developer \nNetwork, Job Openings \nInternet Explore r Internet Explorer, Free Down\u00ad\nloads, IE support, Net Meeting, International IE \nContent \nInternet Explorer Technical Search, Free Down\u00ad\nloads, Products, Internet Explorer, Internet Site \nConstruction for Dev. \nIE Site Builder Internet Site Construction for Dev., \nWeb Site Builders Gallery, Developer Workshop, \nSitebuilder Network Membership, Jakarta, Ac\u00ad\ntiveX Technology Dev. \nAmong probabilistic methods, the Bayesian network \nwith a decision tree at each item outperformed the \ncluster models for ranked scoring. In 12 comparisons, \nthere was an average 41% improvemep.t in ranked \nscores, all differences being statistically significant. \nFor absolute deviation experiments run with the Each\u00ad\nMovie data, we found that the cluster model performed \nslightly better than the trees. \n5 \nAdditional Issues \nAlthough predictive accuracy is probably the most im\u00ad\nportant aspect in gauging the efficacy of a collabora\u00ad\ntive filtering algorithm, there are other considerations, \nincluding size of model, sampling, and runtime perfor\u00ad\nmance. \nIf one considers the size of the overall collaborative fil\u00ad\ntering prediction representation, memory-based meth\u00ad\nods require a relatively small algorithm code base, plus \na user database consisting of a sample of user votes. \nThe model-based methods require the representation \nof the Bayesian network model, typically having much \nsmaller memory requirements. For example, the user \ndatabases required for the memory-based methods for \nthe EachMovie and MS Web datasets were approxi\u00ad\nmately 314 and 318 Kilobytes compressed, while the \nBayesian network model sizes were 27 and 55 Kilobytes \ncompressed respectively. \nThe number of items in the usage database used for \nthe memory-based methods was determined by exper\u00ad\nimenting with the scoring for various sizes of training \nset. Figure 2 shows the increase in ranked scoring ac\u00ad\ncuracy as a function of size of training set. We used \ntraining set sizes (number of users) of 1637 for Neilsen, \n5000 for EachMovie, and 32711 for MS Web. Identi\u00ad\ncal training sets were used as the user database for \nmodel-based methods, and as the database for learn\u00ad\ning probabilistic models. Our experiments have found \n65.0 \nQJ \n63.0 \nH \n0 \n61.0 \nu \nUl \nB 59.0 \n@ \nrrJ \nA \n57.0 \n55.0 \n0 \n10000 \n20000 \n30000 \nTraining Set Size \nFigure 2: A learning curve showing the effect of sample \nsize on ranked scoring for the correlation method, All \nbut 1 protocol, MSWeb dataset. \nthat sample sizes on this order are adequate for pur\u00ad\nposes of generating recommendations. \nIn terms of runtime performance, the probabilistic, \nmodel-based methods were approximately 4 times as \nfast as the memory-based methods in generating rec\u00ad\nommendations, with correlation generating 3.2 recom\u00ad\nmendations per second and the Bayes net generating \n12.9 recommendations per second on 266 MHz Pen\u00ad\ntium II processor (Eachmovie dataset). Of course, the \nprobabilistic models must be learned. Learning times \nfor the models used in these experiments ranged from \nless than an hour for Neilsen and up to 8 hours for \nEachMovie and MS Web. \n6 \nConclusions \nThis paper presents an extensive set of experiments re\u00ad\ngarding the predictive performance of statistical algo\u00ad\nrithms for collaborative filtering or recommender sys\u00ad\ntems. Results indicate that for a wide range of con\u00ad\nditions, Bayesian networks with decision trees at each \nnode and correlation methods outperform Bayesian\u00ad\nclustering and vector similarity methods. Between cor\u00ad\nrelation and Bayesian networks, the preferred method \ndepends on the nature of the dataset, nature of the ap\u00ad\nplication (ranked or one-by-one presentation), and the \navailability of votes with which to make predictions. \nWe see that when there are relatively few votes, corre\u00ad\nlation and Bayesian networks have less of an advantage \nover the other techniques. \nOther considerations include the size of database, \nspeed of predictions, and learning time. Bayesian net\u00ad\nworks are typically have smaller memory requirements \nand allow for faster predictions than a memory-based \n52 \nBreese, Heckerman, and Kadie \ntechnique such as correlation. However, the Bayesian \nmethods examined here require a learning phase that \ncan take up to several hours and results in a lag before \nchanged behavior is reflected in recommendations. \nWe plan to make the MS Web data used in this study \navailable to learning community through the Irvine \nrepository. As noted, the EachMovie data is currently \navailable. We hope that the availability of this data \ncoupled with discussion spurred by this paper will re\u00ad\nsult in additional examination and improvement of col\u00ad\nlaborative filtering algorithms. \nAcknowledgements \nDatasets for this paper were generously provided by \nDigital Equipment Corporation (EachMovie), Neilsen \nMedia Research (Neilsen), and Microsoft Corporation \n(MS Web). Max Chickering, David Hovel, and Robert \nRounthwaite contributed to the programming of the \nalgorithms that were used in this study. We also thank \nMax Chickering, Eric Horvitz, and Chris Meek for use\u00ad\nful discussions. John Riedl also provided useful com\u00ad\nments. \nReferences \n[Breese et al., 1998] Breese, J., Heckerman, D., and \nKadie, C. (May, 1998). An experimental compar\u00ad\nison of collaborative filtering methods. Technical \nReport MSR-TR-98-12, Microsoft Research, Red\u00ad\nmond, WA. \n[Cheeseman and Stutz, 1995] Cheeseman, \nP. \nand Stutz, J. (1995). Bayesian classification (Auto\u00ad\nClass): Theory and results. In Fayyad, U., Piatesky\u00ad\nShapiro, G., Smyth, P., and Uthurusamy, R., ed\u00ad\nitors, Advances in Know ledge Discovery and Data \nMining, pages 153-180. AAAI Press, Menlo Park, \nCA. \n[Chickering and Heckerman, 1997] Chickering, D. and \nHeckerman, D. (1997). Efficient approximations for \nthe marginal likelihood of Bayesian networks with \nhidden variables. Machine Learning, 29:181-212. \n[Chickering et al., 1997] Chickering, D., Heckerman, \nD., and Meek, C. (1997). \nA Bayesian approach \nto learning Bayesian networks with local structure. \nIn Proceedings of Thirteenth Conference on Un\u00ad\ncertainty in Artificial Intelligence, Providence, RI. \nMorgan Kaufmann. \n[Dempster et al., 1977] Dempster, A., Laird, N., and \nRubin, D. (1977). Maximum likelihood from incom\u00ad\nplete data via the EM algorithm. Journal of the \nRoyal Statistical Society, B 39:1-38. \n[L.Terveen et al., 1997] L.Terveen, Hill, W., Amenta, \nB., \nMcDconald, \nD., \nand Creter, \nJ. \n(1997). \nPHOAKS: A system for sharing recommendations. \nCommunications of the ACM, 40(3):59-62. \n[McClave and Dietrich, 1988] McClave, J. T. and Di\u00ad\netrich, F. H. (1988). Statistics. Dellen Publishing \nCompany, San Francisco, fourth edition. \n[Miller et al., 1997] Miller, B., Riedl, J., and Konstan, \nJ. (1997). Experiences with GroupLens: Making \nUsenet useful again. In Proceeding of the USENIX \n1997 Annual Technical Conference, pages 219-231, \nAnaheim, CA. \n[Resnick et al., 1994] \nResnick, P., Iacovou, N., Suchak, M., Bergstrom, \nP., and Riedl, J. (1994). Grouplens: An open ar\u00ad\nchitecture for collaborative filtering of netnews. In \nProceedings of the ACM 1994 Conference on Com\u00ad\nputer Supported Cooperative Work, pages 175-186, \nNew York. ACM. \n[Resnick and Varian, 1997] Resnick, P. and Varian, H. \n(1997). Recommender systems. Communications of \nthe ACM, 40(3):56-58. \n[Rucker and Polanco, 1997] Rucker, J. and Polanco, \nM. J. (1997). Siteseer: Personalized navigation of \nthe web. Communications of the ACM, 40(3):56-58. \n[Salton and McGill, 1983] Salton, G. and McGill, M. \n(1983). \nIntroduction to Modern Information Re\u00ad\ntrieval. McGraw-Hill, New York. \n[Thiesson et al., 1997] Thiesson, B., Meek, C., Chick\u00ad\nering, D., and Heckerman, D. (December, 1997). \nLearning mixtures of DAG models. \nTechnical \nReport MSR-TR-97-30, Microsoft Research, Red\u00ad\nmond, WA. \n",
        "sentence": " Several algorithms were proposed to generate such models including Bayesian networks [21]\u2013[23], clustering [24], [25], latent semantic analysis [26], Singular Value Decomposition (SVD) [27], Alternating Least Squares (ALS) [28], [29] regression models [30], Markov Decision Processes (MDPs) [31], and others [32].",
        "context": "rics for the various algorithms. \nResults \nindicate that for a wide range of con\u00ad\nditions, Bayesian networks with decision \ntrees at each node and correlation methods \noutperform Bayesian-clustering and vector\u00ad\nsimilarity methods. Between correlation and\n2.3.1 \nCluster Models \nOne plausible probabilistic model for collaborative fil\u00ad\ntering is a Bayesian classifier where the probability of \nvotes are conditionally independent given membership \nin an unobserved class variable C taking on some rel\u00ad\nditions, Bayesian networks with decision trees at each \nnode and correlation methods outperform Bayesian\u00ad\nclustering and vector similarity methods. Between cor\u00ad\nrelation and Bayesian networks, the preferred method"
    },
    {
        "title": "Collaborative filtering with the simple bayesian classifier",
        "author": [
            "Koji Miyahara",
            "Michael J Pazzani"
        ],
        "venue": "In Pacific Rim International conference on artificial intelligence,",
        "citeRegEx": "22",
        "shortCiteRegEx": "22",
        "year": 2000,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Collaborative filtering for multi-class data using belief nets algorithms",
        "author": [
            "Xiaoyuan Su",
            "Taghi M Khoshgoftaar"
        ],
        "venue": "In 2006 18th IEEE International Conference on Tools with Artificial Intelligence",
        "citeRegEx": "23",
        "shortCiteRegEx": "23",
        "year": 2006,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Several algorithms were proposed to generate such models including Bayesian networks [21]\u2013[23], clustering [24], [25], latent semantic analysis [26], Singular Value Decomposition (SVD) [27], Alternating Least Squares (ALS) [28], [29] regression models [30], Markov Decision Processes (MDPs) [31], and others [32].",
        "context": null
    },
    {
        "title": "Clustering methods for collaborative filtering",
        "author": [
            "Lyle H Ungar",
            "Dean P Foster"
        ],
        "venue": "In AAAI workshop on recommendation systems,",
        "citeRegEx": "24",
        "shortCiteRegEx": "24",
        "year": 1998,
        "abstract": "",
        "full_text": "",
        "sentence": " Several algorithms were proposed to generate such models including Bayesian networks [21]\u2013[23], clustering [24], [25], latent semantic analysis [26], Singular Value Decomposition (SVD) [27], Alternating Least Squares (ALS) [28], [29] regression models [30], Markov Decision Processes (MDPs) [31], and others [32].",
        "context": null
    },
    {
        "title": "Rectree: An efficient collaborative filtering method",
        "author": [
            "Sonny Han Seng Chee",
            "Jiawei Han",
            "Ke Wang"
        ],
        "venue": "In International Conference on Data Warehousing and Knowledge Discovery,",
        "citeRegEx": "25",
        "shortCiteRegEx": "25",
        "year": 2001,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Several algorithms were proposed to generate such models including Bayesian networks [21]\u2013[23], clustering [24], [25], latent semantic analysis [26], Singular Value Decomposition (SVD) [27], Alternating Least Squares (ALS) [28], [29] regression models [30], Markov Decision Processes (MDPs) [31], and others [32].",
        "context": null
    },
    {
        "title": "Latent semantic models for collaborative filtering",
        "author": [
            "Thomas Hofmann"
        ],
        "venue": "ACM Transactions on Information Systems (TOIS),",
        "citeRegEx": "26",
        "shortCiteRegEx": "26",
        "year": 2004,
        "abstract": "Collaborative filtering aims at learning predictive models of user preferences, interests or behavior from community data, that is, a database of available user preferences. In this article, we describe a new family of model-based algorithms designed for this task. These algorithms rely on a statistical modelling technique that introduces latent class variables in a mixture model setting to discover user communities and prototypical interest profiles. We investigate several variations to deal with discrete and continuous response variables as well as with different objective functions. The main advantages of this technique over standard memory-based methods are higher accuracy, constant time prediction, and an explicit and compact model representation. The latter can also be used to mine for user communitites. The experimental evaluation shows that substantial improvements in accucracy over existing methods and published results can be obtained.",
        "full_text": "",
        "sentence": " Several algorithms were proposed to generate such models including Bayesian networks [21]\u2013[23], clustering [24], [25], latent semantic analysis [26], Singular Value Decomposition (SVD) [27], Alternating Least Squares (ALS) [28], [29] regression models [30], Markov Decision Processes (MDPs) [31], and others [32].",
        "context": null
    },
    {
        "title": "Learning collaborative information filters",
        "author": [
            "Daniel Billsus",
            "Michael J Pazzani"
        ],
        "venue": "In Icml,",
        "citeRegEx": "27",
        "shortCiteRegEx": "27",
        "year": 1998,
        "abstract": "",
        "full_text": "",
        "sentence": " Several algorithms were proposed to generate such models including Bayesian networks [21]\u2013[23], clustering [24], [25], latent semantic analysis [26], Singular Value Decomposition (SVD) [27], Alternating Least Squares (ALS) [28], [29] regression models [30], Markov Decision Processes (MDPs) [31], and others [32].",
        "context": null
    },
    {
        "title": "Large-scale parallel collaborative filtering for the netflix prize",
        "author": [
            "Yunhong Zhou",
            "Dennis Wilkinson",
            "Robert Schreiber",
            "Rong Pan"
        ],
        "venue": "In International Conference on Algorithmic Applications in Management,",
        "citeRegEx": "28",
        "shortCiteRegEx": "28",
        "year": 2008,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Several algorithms were proposed to generate such models including Bayesian networks [21]\u2013[23], clustering [24], [25], latent semantic analysis [26], Singular Value Decomposition (SVD) [27], Alternating Least Squares (ALS) [28], [29] regression models [30], Markov Decision Processes (MDPs) [31], and others [32].",
        "context": null
    },
    {
        "title": "Matrix factorization techniques for recommender systems",
        "author": [
            "Yehuda Koren",
            "Robert Bell",
            "Chris Volinsky"
        ],
        "venue": null,
        "citeRegEx": "29",
        "shortCiteRegEx": "29",
        "year": 2009,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Several algorithms were proposed to generate such models including Bayesian networks [21]\u2013[23], clustering [24], [25], latent semantic analysis [26], Singular Value Decomposition (SVD) [27], Alternating Least Squares (ALS) [28], [29] regression models [30], Markov Decision Processes (MDPs) [31], and others [32].",
        "context": null
    },
    {
        "title": "Collaborative filtering using a regression-based approach",
        "author": [
            "Slobodan Vucetic",
            "Zoran Obradovic"
        ],
        "venue": "Knowledge and Information Systems,",
        "citeRegEx": "30",
        "shortCiteRegEx": "30",
        "year": 2005,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Several algorithms were proposed to generate such models including Bayesian networks [21]\u2013[23], clustering [24], [25], latent semantic analysis [26], Singular Value Decomposition (SVD) [27], Alternating Least Squares (ALS) [28], [29] regression models [30], Markov Decision Processes (MDPs) [31], and others [32].",
        "context": null
    },
    {
        "title": "An mdp-based recommender system",
        "author": [
            "Guy Shani",
            "David Heckerman",
            "Ronen I Brafman"
        ],
        "venue": "Journal of Machine Learning Research,",
        "citeRegEx": "31",
        "shortCiteRegEx": "31",
        "year": 2005,
        "abstract": "",
        "full_text": "",
        "sentence": " Several algorithms were proposed to generate such models including Bayesian networks [21]\u2013[23], clustering [24], [25], latent semantic analysis [26], Singular Value Decomposition (SVD) [27], Alternating Least Squares (ALS) [28], [29] regression models [30], Markov Decision Processes (MDPs) [31], and others [32].",
        "context": null
    },
    {
        "title": "Model-based collaborative filtering",
        "author": [
            "Charu C Aggarwal"
        ],
        "venue": "In Recommender Systems,",
        "citeRegEx": "32",
        "shortCiteRegEx": "32",
        "year": 2016,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Several algorithms were proposed to generate such models including Bayesian networks [21]\u2013[23], clustering [24], [25], latent semantic analysis [26], Singular Value Decomposition (SVD) [27], Alternating Least Squares (ALS) [28], [29] regression models [30], Markov Decision Processes (MDPs) [31], and others [32].",
        "context": null
    },
    {
        "title": "Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th annual meeting of the association for computational linguistics, pages 384\u2013394",
        "author": [
            "Joseph Turian",
            "Lev Ratinov",
            "Yoshua Bengio"
        ],
        "venue": "Association for Computational Linguistics,",
        "citeRegEx": "33",
        "shortCiteRegEx": "33",
        "year": 2010,
        "abstract": "",
        "full_text": "",
        "sentence": " Another important aspect to understand this work is how to represent documents in the vector space to measure similarity [33].",
        "context": null
    },
    {
        "title": "Latent dirichlet allocation",
        "author": [
            "David M Blei",
            "Andrew Y Ng",
            "Michael I Jordan"
        ],
        "venue": "Journal of machine Learning research,",
        "citeRegEx": "34",
        "shortCiteRegEx": "34",
        "year": 2003,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Topic models such as Latent Dirichlet Allocation (LDA) are also used as features in document classification problems such as sentiment analysis and have shown promising results [34], [35]. Latent Dirichlet Allocation: LDA is a probabilistic model which learns P (z|w), distribution of a latent topic variable z given a word w [34].",
        "context": null
    },
    {
        "title": "Learning word vectors for sentiment analysis",
        "author": [
            "Andrew L Maas",
            "Raymond E Daly",
            "Peter T Pham",
            "Dan Huang",
            "Andrew Y Ng",
            "Christopher Potts"
        ],
        "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume",
        "citeRegEx": "35",
        "shortCiteRegEx": "35",
        "year": 2011,
        "abstract": "",
        "full_text": "",
        "sentence": " Topic models such as Latent Dirichlet Allocation (LDA) are also used as features in document classification problems such as sentiment analysis and have shown promising results [34], [35].",
        "context": null
    },
    {
        "title": "A neural knowledge language model",
        "author": [
            "Sunging Ahn",
            "Heeyoul Choi",
            "Tanel P\u00e4rnamaa",
            "Yoshua Bengio"
        ],
        "venue": "arXiv preprint arXiv:1608.00318,",
        "citeRegEx": "36",
        "shortCiteRegEx": "36",
        "year": 2016,
        "abstract": "Current language models have a significant limitation in the ability to\nencode and decode factual knowledge. This is mainly because they acquire such\nknowledge from statistical co-occurrences although most of the knowledge words\nare rarely observed. In this paper, we propose a Neural Knowledge Language\nModel (NKLM) which combines symbolic knowledge provided by the knowledge graph\nwith the RNN language model. By predicting whether the word to generate has an\nunderlying fact or not, the model can generate such knowledge-related words by\ncopying from the description of the predicted fact. In experiments, we show\nthat the NKLM significantly improves the performance while generating a much\nsmaller number of unknown words.",
        "full_text": "A Neural Knowledge Language Model\nSungjin Ahn 1 Heeyoul Choi 2 Tanel P\u00a8arnamaa 3 Yoshua Bengio 1 4\nAbstract\nCurrent language models have signi\ufb01cant limita-\ntion in the ability to encode and decode factual\nknowledge. This is mainly because they acquire\nsuch knowledge from statistical co-occurrences al-\nthough most of the knowledge words are rarely ob-\nserved. In this paper, we propose a Neural Knowl-\nedge Language Model (NKLM) which combines\nsymbolic knowledge provided by the knowledge\ngraph with the RNN language model. By predict-\ning whether the word to generate has an under-\nlying fact or not, the model can generate such\nknowledge-related words by copying from the\ndescription of the predicted fact. In experiments,\nwe show that the NKLM signi\ufb01cantly improves\nthe performance while generating a much smaller\nnumber of unknown words.\n1. Introduction\nKanye West, a famous <unknown> and the husband of\n<unknown>, released his latest album <unknown> in\n<unknown>.\nA core purpose of language is to communicate knowl-\nedge. For human-level language understanding, it is thus of\nprimary importance for a language model to take advantage\nof knowledge. Traditional language models are good at cap-\nturing statistical co-occurrences of entities as long as they\nare observed frequently in the corpus (e.g., words like verbs,\npronouns, and prepositions). However, they are in general\nlimited in their ability in dealing with factual knowledge be-\ncause these are usually represented by named entities such\nas person names, place names, years, etc. (as shown in the\nabove example sentence of Kanye West.)\nTraditional language models have demonstrated to some\nextent the ability to encode and decode fatual knowl-\nedge (Vinyals & Le, 2015; Serban et al., 2015) when trained\n1Universit\u00b4e de Montr\u00b4eal, Canada 2Handong Global University,\nSouth Korea 3Work done during internship at the Universit\u00b4e de\nMontr\u00b4eal, Canada 4CIFAR Senior Fellow. Correspondence to:\nSungjin Ahn <sjn.ahn@gmail.com>.\nwith a very large corpus. However, we claim that simply\nfeeding a larger corpus into a bigger model hardly results in\na good knowledge language model.\nThe primary reason for this is the dif\ufb01culty in learning\ngood representations for rare and unknown words. This is a\nsigni\ufb01cant problem because these words are of our primary\ninterest in knowledge-related applications such as question\nanswering (Iyyer et al., 2014; Weston et al., 2016; Bordes\net al., 2015) and dialogue modeling (Vinyals & Le, 2015;\nSerban et al., 2015). Speci\ufb01cally, in the recurrent neural\nnetwork language model (RNNLM) (Mikolov et al., 2010),\nthe computational complexity is linearly dependent on the\nnumber of vocabulary words. Thus, including all words of\na language is computationally prohibitive. Even if we can\ninclude a very large number of words in the vocabulary,\naccording to Zipf\u2019s law, a large portion of the words will\nstill be rarely observed in the corpus.\nThe fact that languages and knowledge can change over time\nalso makes it dif\ufb01cult to simply rely on a large corpus. Me-\ndia produce an endless stream of new knowledge every day\n(e.g., the results of baseball games played yesterday) that\nis even changing over time. Furthermore, a good language\nmodel should exercise some level of reasoning. For ex-\nample, it may be possible to observe many occurrences of\nBarack Obama\u2019s year of birth and thus able to predict it in a\ncorrelated context. However, one would not expect current\nlanguage models to predict, with a proper reasoning, the\nblank in \u201cBarack Obama\u2019s age is\n\u201d even if it is only a\nsimple reformulation of the knowledge on the year of birth1.\nIn this paper, we propose a Neural Knowledge Language\nModel (NKLM) as a step towards addressing the limitations\nof traditional language modeling when it comes to exploit-\ning factual knowledge. In particular, we incorporate sym-\nbolic knowledge provided by the knowledge graph (Nickel\net al., 2015) into the RNNLM. This connection makes sense\nparticularly by observing that facts in knowledge graphs\ncome along with textual representations which are mostly\nabout rare words in text corpora.\nIn NKLM, we assume that each word generation is either\n1We do not investigate the reasoning ability in this paper but\nhighlight this example because the explicit representation of facts\nwould help to handle such examples.\narXiv:1608.00318v2  [cs.CL]  2 Mar 2017\nA Neural Knowledge Language Model\nbased on a fact or not. Thus, at each time step, before gen-\nerating a word, we predict whether the word to generate\nhas an underlying fact or not. As a result, our model pro-\nvides predictions over facts in addition to predictions over\nwords. Hence, the previous context information on both\nfacts and words \ufb02ow through an RNN and provide a richer\ncontext. The NKLM has two ways to generate a word. One\noption is to generate a \u201cvocabulary word\u201d using the vocab-\nulary softmax as is in the RNNLM. The other option is to\ngenerate a \u201cknowledge word\u201d by predicting the position of\na word within the textual representation of the predicted\nfact. This makes it possible to generate words which are\nnot in the prede\ufb01ned vocabulary and consequently resolves\nthe rare and unknown word problem. The NKLM can also\nimmediately adapt to adding or modifying knowledge be-\ncause the model learns to predict facts, which can easily be\nmodi\ufb01ed without having to retrain the model.\nThe contributions of the paper are:\n\u2022 To propose the NKLM model to resolve limitations\nof traditional language models in dealing with factual\nknowledge by using the knowledge graph.\n\u2022 To develop a new dataset called WikiFact which can be\nused in knowledge-related language models by provid-\ning text aligned with facts.\n\u2022 To show that the proposed model signi\ufb01cantly im-\nproves the performance and can generate named en-\ntities which in traditional models were treated as un-\nknown words.\n\u2022 To propose new evaluation metrics that resolve the\nproblem of the traditional perplexity metric in dealing\nwith unknown words.\n2. Related Work\nThere have been remarkable recent advances in language\nmodeling research based on neural networks (Bengio\net al., 2003; Mikolov et al., 2010). In particular, the\nRNNLMs are interesting for their ability to take advantage\nof longer-term temporal dependencies without a strong con-\nditional independence assumption. It is especially notewor-\nthy that the RNNLM using the Long Short-Term Memory\n(LSTM) (Hochreiter & Schmidhuber, 1997) has recently\nadvanced to the level of outperforming carefully-tuned tra-\nditional n-gram based language models (Jozefowicz et al.,\n2016).\nThere have been many efforts to speed up the language\nmodels so that they can cover a larger vocabulary. These\nmethods approximate the softmax output using hierarchical\nsoftmax (Morin & Bengio, 2005; Mnih & Hinton, 2009),\nimportance sampling (Jean et al., 2015), noise contrastive\nestimation (Mnih & Teh, 2012), etc. Although helpful to\nmitigate the computational problem, these approaches still\nsuffer from the rare or unknown words problem.\nTo help deal with the rare/unknown word problem, the\npointer networks (Vinyals et al., 2015) have been adopted\nto implement the copy mechanism (Gulcehre et al., 2016;\nGu et al., 2016) and applied to machine translation and text\nsummarization. With this approach, the (unknown) word\nto copy from the context sentence is inferred from neigh-\nboring words. Similarly, Merity et al. (2016) proposed to\ncopy from the context sentences and Lebret et al. (2016)\nfrom Wikipedia infobox. However, because in our case the\ncontext can be very short and often contains no known rele-\nvant words (e.g., person names), we cannot use the existing\napproach directly.\nOur knowledge memory is also related to the recent litera-\nture on neural networks with external memory (Bahdanau\net al., 2014; Weston et al., 2015; Graves et al., 2014). In\nWeston et al. (2015), given simple sentences as facts which\nare stored in the external memory, the question answering\ntask is studied. In fact, the tasks that the knowledge-based\nlanguage model aims to solve (i.e., predict the next word)\ncan be considered as a \ufb01ll-in-the-blank type of question an-\nswering. The idea of jointly using Wikipedia and knowledge\ngraphs has also been used in the context of enriching word\nembedding (Celikyilmaz et al., 2015; Long et al., 2016).\nContext-dependent (or topic-based) language models have\nbeen studied to better capture long-term dependencies,\nby learning some context representation from the history.\n(Gildea & Hofmann, 1999) modeled the topic as a latent\nvariable and proposed an EM-based approach. In (Mikolov\n& Zweig, 2012), the topic features are learned by latent\nDirichlet allocation (LDA) (Blei et al., 2003).\n3. Model\n3.1. Preliminary\nA topic is associated to topic knowledge and topic descrip-\ntion. Topic knowledge F is a set of facts {a1, a2, . . . , a|F|}\non the topic and topic description W is a sequence of words\n(w1, w2, . . . , w|W |) describing the topic. We can obtain the\ntopic knowledge from a knowledge graph such as Freebase\nand the topic description from Wikipedia. In the corpus, we\nare given pairs of topic knowledge and topic description for\nK topics, i.e., {(Fk, Wk)}K\nk=1. In the following, we omit\nindex k when we indicate an arbitrary topic.\nA fact is represented as a triple of subject, relationship, and\nobject which is associated with a textual representation, e.g.,\n(Barack Obama, Married-To, Michelle Obama). Note that\nall facts in a topic knowledge have the same subject entity\nwhich is the topic entity itself.\nA Neural Knowledge Language Model\nWe de\ufb01ne knowledge words Oa of a fact a as a sequence of\nwords (oa\n1, oa\n2, . . . , oa\nN) from which we can copy a word to\ngenerate output. We also maintain a global vocabulary V\ncontaining frequent words. Because the words describing\nrelationships (e.g., \u201cmarried to\u201d) are common and thus can\nbe generated via the vocabulary V not via copy, we limit\nthe knowledge words of a fact to be the words for the ob-\nject entity (e.g., Oa = (oa\n1=\u201cMichelle\u201d, oa\n2=\u201cObama\u201d). In\naddition, to make it possible to access the subject words\nfrom the knowledge words, we add a special fact, (Topic,\nTopic Itself, Topic), to all topic knowledge.\nWe train the model in a supervised way with labels on facts\nand words. This requires aligning words in the topic descrip-\ntion with their corresponding facts in the topic knowledge.\nSpeci\ufb01cally, given F and W for a topic, we perform simple\nstring matching between the words in W and all the knowl-\nedge words OF = \u222aa\u2208FOa in such a way to associate fact\na to word w if w appears in knowledge words Oa. As a re-\nsult, from F and W, we construct a sequence of augmented\nobservations Y = {yt = (wt, at, zt)}t=1:|W |. Here, zt is\na binary variable indicating whether wt is observed in the\nknowledge words or not:\nzt =\n(\n1,\nif wt \u2208Oat,\n0,\notherwise.\n(1)\nIn addition, because not all words are associated to a fact\n(e.g., words like, is, a, the, have), we introduce a special\nfact type, called Not-a-Fact (NaF), and to which assign such\nwords. The following is an example of an augmented obser-\nvation induced from a topic description and knowledge.\nExample. Given a topic on Fred Rogers with topic descrip-\ntion\nW=\u201cRogers was born in Latrobe, Pennsylvania in 1928\u201d\nand topic knowledge F = {a42, a83, a0} where\na42 = (Fred Rogers, Place of Birth, Latrobe Pennsylvania)\na83 = (Fred Rogers, Year of Birth, 1928)\na0 = (Fred Rogers, Topic Itself, Fred Rogers),\nthe augmented observation Y is\nY = {(w=\u201cRogers\u201d, a=0, z=1), (\u201cwas\u201d, NaF, 0),\n(\u201cborn\u201d, NaF, 0), (\u201cin\u201d, NaF, 0), (\u201cLatrobe\u201d, 42, 1),\n(\u201cPennsylvania\u201d, 42, 1), (\u201cin\u201d, NaF, 0), (\u201c1928\u201d, 83, 1)}.\nDuring inference and training of a topic, we assume that\nthe topic knowledge F is loaded in the knowledge mem-\nory in a form of a matrix F \u2208RDa\u00d7|F| where the i-th\ncolumn is a fact embedding ai \u2208RDa. The fact embed-\nding is the concatenation of subject, relationship, and object\nembeddings. We obtain these entity embeddings from a pre-\nliminary run of a knowledge graph embedding method such\nas TransE (Bordes et al., 2013). Note that we \ufb01x the fact\nembedding during the training. Thus, there is no drift of\n\ud835\udc99\"\na1\na2\na3\na4\n\u2026\naN\nNaF\n\ud835\udc98\"\n,\nO1\nO2\nO3\nO4\n\u2026\nON\nTopic Knowledge\n\ud835\udc98\"\n.\n\ud835\udc67\"\n\ud835\udc89\"\n\ud835\udc89\"\n\ud835\udc82\"\n\ud835\udc8c\"\n\ud835\udc82\"34\n\ud835\udc98\"34\n.\n\ud835\udc98\"34\n,\n\ud835\udc89\"34\n\ud835\udc89\"\n\ud835\udc86\nLSTM\ncopy\nfact search\nFigure 1. The NKLM model. The input consisting of a word (either\nwo\nt\u22121 or wv\nt\u22121) and a fact (at\u22121) goes into LSTM. The LSTM\u2019s\noutput ht together with the knowledge context e generates the\nfact key kt. Using the fact key, the fact embedding at is retrieved\nfrom the topic knowledge memory. Using at and ht, the word\ngeneration source zt is determined, which in turn determines the\nnext word generation source wv\nt or wo\nt . The copied word wo\nt is a\nsymbol taken from the fact description Oat.\nfact embeddings after training and thus the model can deal\nwith new facts at test time; we learn the embedding of the\nTopic Itself.\nFor notation, to denote the vector representation of an object\nof our interest, we use bold lowercase. For example, the\nembedding of a word w is represented by w = W[w]\nwhere WDw\u00d7|V| is the word embedding matrix, and W[w]\ndenotes the w-th column of W.\n3.2. Inference\nAt each time step, the NKLM performs the following four\nsub-steps:\n1. Using both the word and fact predictions of the previ-\nous time step, make an input to the current time step\nand update the LSTM controller.\n2. Given the output of the LSTM controller, predict a fact\nand extract its corresponding embedding.\n3. With the extracted fact embedding and the state of the\nLSTM controller, make a binary decision to determine\nthe source of word generation.\n4. According to the chosen source, generate a word either\nfrom the global vocabulary or by copying a word from\nthe knowledge words of the selected fact.\nA model diagram is depicted in Fig. 1. In the following, we\ndescribe these steps in more detail.\nA Neural Knowledge Language Model\n3.2.1. INPUT REPRESENTATION AND LSTM\nCONTROLLER\nAs shown in Fig. 1, the input at time step t is the con-\ncatenation of three embedding vectors corresponding to\na fact at\u22121, a (global) vocabulary word wv\nt\u22121 \u2208V, and\na knowledge word wo\nt\u22121 \u2208Oat\u22121, respectively.\nHow-\never, because the predicted word comes at a time step\nonly either from the vocabulary or by copying from the\nknowledge words, i.e., wt\u22121 \u2208{wv\nt\u22121, wo\nt\u22121} , we set\neither wv\nt\u22121 or wo\nt\u22121 to a zero vector when it is not the\ngeneration source at the previous step. The resulting in-\nput representation xt = fconcat(at\u22121, wv\nt\u22121, wo\nt\u22121) is then\nfed into the LSTM controller, and obtain the output states\nht = fLSTM(xt, ht\u22121).\n3.2.2. FACT EXTRACTION\nWe then predict a relevant fact at on which the word wt will\nbe based. Predicting a fact is done in two steps.\nFirst, a fact-key kfact \u2208RDa is generated by a function\nffactkey(ht, ek) which is in our experiments a multilayer per-\nceptron (MLP) with one hidden layer of ReLU nonlinearity\nand linear outputs. Here, ek \u2208RDa is the embedding of\nthe topic knowledge which provides information about what\nfacts are currently available in the topic knowledge. This\nwould help the key generator adapt, without retraining, to\nchanges in the topic knowledge such as removal or modi\ufb01-\ncation of some facts. Our experiments use mean-pooling to\nobtain ek, but one can also consider using a more sophis-\nticated method such as the soft-attention mechanism (Bah-\ndanau et al., 2014).\nThen, using the generated fact-key kfact, we select a fact by\nkey-value lookup across the knowledge memory F and then\nretrieve its embedding at as follows:\nP(a|ht) =\nexp(k\u22a4\nfactF[a])\nP\na\u2032\u2208F exp(k\u22a4\nfactF[a\u2032]),\n(2)\nat = argmax\na\u2208F\nP(a|ht),\n(3)\nat = F[at].\n(4)\n3.2.3. SELECTING WORD GENERATION SOURCE\nGiven the context ht and the extracted fact at, the model\ndecides the source for the next word generation: either from\nthe vocabulary V or from the knowledge words Oat. We\nde\ufb01ne the probability of selecting generation-by-copy as:\n\u02c6zt = p(zt|ht, at) = sigmoid(fcopy(ht, at)).\n(5)\nHere, fcopy is an MLP with one ReLU hidden layer and a\nsingle linear output unit.\nWord wt is generated from the source indicated by \u02c6zt as\nAlgorithm 1 NKLM inference at time step t\n1: ## Make input\n2: xt = fconcat(at\u22121, wv\nt\u22121, wo\nt\u22121)\n3: ## Update LSTM controller\n4: ht = LSTM(ht\u22121, xt)\n5: ## Fact prediction and extract embedding\n6: at = argmaxa\u2208F P(a|ht, ek)\n7: at = F[at]\n8: ## Decide word generation source\n9: zt = I[p(zt|ht, at) > 0.5]\n10: if zt == 0 then\n11:\n## Word generation from vocabulary\n12:\nwt = wv\nt = argmaxw\u2208V P(w|ht, at)\n13:\nwo\nt = 0\n14: else\n15:\n## Word generation by copy\n16:\nnt = argmaxn=0:|Oat|\u22121 P(n|ht, at)\n17:\nwt = wo\nt = Oat[nt]\n18:\nwv\nt = 0\n19: end if\nfollows:\nwt =\n(\nwv\nt \u2208V,\nif \u02c6zt < 0.5,\nwo\nt \u2208Oat,\notherwise.\n3.2.4. WORD GENERATION\nGeneration from Vocabulary Softmax: For vocabulary\nword wv\nt \u2208V, we follow the usual way of selecting a word\nusing the softmax function:\nP(wv\nt = w|ht, at) =\nexp(k\u22a4\nvocaW[w])\nP\nw\u2032\u2208V exp(k\u22a4\nvocaW[w\u2032]),\n(6)\nwhere kvoca \u2208RDw is obtained by fvoca(ht, at) which is an\nMLP with a ReLU hidden layer and Dw linear output units.\nGeneration by Copy from Knowledge Words: To copy\na knowledge word wo\nt \u2208Oat, we \ufb01rst predict the position\nof the word within the knowledge words and then copy\nthe word on the predicted position. This copy-by-position\nallows us not to rely on the word embeddings by instead\nlearning position embeddings.\nOne reason to use position prediction is that the traditional\ncopy mechanism (Gulcehre et al., 2016; Gu et al., 2016)\nis dif\ufb01cult to apply to our context because the knowledge\nwords usually consist of only unknown words and/or are\nshort in length. Furthermore, it makes sense when consid-\nering the fact that we mostly need to copy the knowledge\nwords in increasing order from the \ufb01rst word. For example,\ngiven that the \ufb01rst symbol o1 = \u201cMichelle\u201d was used in\nthe previous time step and prior to that other words such\nA Neural Knowledge Language Model\n# topics\n# toks\n# uniq toks\n# facts\n# entities\n# relations\nmaxk |Fk|\navgk|Fk|\nmaxa |Oa|\navga|Oa|\n10K\n1.5M\n78k\n813k\n560K\n1.5K\n1K\n79\n19\n2.15\nTable 1. Statistics of the WikiFacts-FilmActor-v0.1 dataset.\nas \u201cPresident\u201d and \u201cUS\u201d were also observed, the model\ncan easily predict that it is time to select the second symbol,\ni.e., o2 = \u201cObama\u201d.\nMore speci\ufb01cally, we \ufb01rst generate the position key kpos \u2208\nRDo by a function fposkey(ht, at) which is again an MLP\nwith one hidden layer and linear outputs whose dimension\nis the maximum number of positions, e.g., the maximum\nlength of the knowledge words (e.g., N o\nmax = maxa\u2208\u00af\nF |Oa|\nwhere \u00afF = \u222akFk). Then, the word to copy is chosen by\nP(n|ht, at) =\nexp(k\u22a4\nposP[n])\nP\nn\u2032 exp(k\u22a4\nposP[n\u2032]),\n(7)\nnt =\nargmax\nn=0:|Oat|\u22121\nP(n|ht, at),\n(8)\nwo\nt = Oat[nt],\n(9)\nwith position n\u2032 running from 0 to |Oat| \u22121.\nHere,\nPDo\u00d7N o\nmax is the matrix of position embeddings of dimen-\nsion Do. Note that N o\nmax is typically a much smaller number\n(e.g., 20 in our experiments) than the size of vocabulary,\nand thus the computation for copy is ef\ufb01cient. The position\nembedding matrix P is learned during training.\nAlthough in our experiments we \ufb01nd that the simple posi-\ntion prediction performs well, we note that one could also\nconsider a more advanced encoding such as one based on a\nconvolutional network (Kim, 2014) to model the knowledge\nwords.\nTo compute p(wt|w<t, F), we \ufb01rst obtain {z<t, a<t} from\n{w<t} and F using the augmentation procedure, and per-\nform the above inference process with hard decisions taken\nabout zt and at based on the model\u2019s predictions. The infer-\nence procedure is summarized in Algorithm 1.\n3.3. Learning\nWe perform supervised learning on the augmented observa-\ntion Y , similarly to Reed & de Freitas (2016). That is, given\nword observations {Yk}K\nk=1 and knowledge {Fk}K\nk=1, our\nobjective is to maximize the log-likelihood of the augmented\nobservation w.r.t the model parameter \u03b8,\n\u03b8\u2217= argmax\n\u03b8\nX\nk\nlog P\u03b8(Yk|Fk).\n(10)\nBy the chain rule, we can decompose the probability of the\nobservation Yk as\nlog P\u03b8(Yk|Fk) =\n|Yk|\nX\nt=1\nlog P\u03b8(yk\nt |yk\n1:t\u22121, Fk).\n(11)\nThen, after omitting Fk and k for simplicity, we can rewrite\nthe single step conditional probability as\nP\u03b8(yt|y1:t\u22121) = P\u03b8(wt, at, zt|ht)\n(12)\n= P\u03b8(wt|at, zt, ht)P\u03b8(at|ht)P\u03b8(zt|ht).\nWe maximize the above objective using stochastic gradient\noptimization.\n4. Evaluation\nAn obstacle in developing the proposed model is the lack\nof datasets where the text is aligned with facts at the word\nlevel. While the Penn Treebank (PTB) dataset (Marcus\net al., 1993) has been frequently used in language mod-\neling, as pointed by Merity et al. (2016), its limited vo-\ncabulary containing a relatively small amount of named\nentities makes it dif\ufb01cult to use them for knowledge-related\ntasks where rare words are of primary interest; we would\nhave only a very small amount of words to be associated\nwith facts.\nAs other larger datasets such as in Chelba\net al. (2013) also have problems in licensing or in the for-\nmat of the dataset, we produce the WikiFacts dataset for\nevaluation of the proposed model and the baseline model.\nThe dataset is freely available in https://bitbucket.\norg/skaasj/wikifact_filmactor.\n4.1. The WikiFacts Dataset\nIn WikiFacts, we align Wikipedia descriptions with corre-\nsponding Freebase2 facts. Because many Freebase topics\nprovide a link to its corresponding topic in Wikipedia, we\nchoose a set of topics for which both a Freebase entity and\na Wikipedia description exist. In the experiments, we used\na version called WikiFacts-FilmActor-v0.1 where\nthe domain is restricted to the /Film/Actor in Freebase.\nWe used the summary part (\ufb01rst few paragraphs) of the\nWikipedia page as the text to be modeled, but discarded\ntopics for which the number of facts is too large (> 1000) or\nthe Wikipedia description is too short (< 3 sentences). For\nthe string matching, we also used synonyms and alias infor-\nmation provided by WordNet (Miller, 1995) and Freebase.\nWe augmented the fact set F with the anchor facts A whose\nrelationship is all set to UnknownRelation. That is, ob-\nserving that an anchor (a word under a hyperlink) in a\nWikipedia description has a corresponding Freebase entity\nas well as being semantically closely related to the topic\n2Freebase has migrated to Wikidata. www.wikidata.org\nA Neural Knowledge Language Model\nValidation\nTest\nModel\nPPL\nUPP\nUPP-f\nPPL\nUPP\nUPP-f\n# UNK\nRNNLM\n39.4\n97.9\n56.8\n39.4\n107.0\n58.4\n23247\nNKLM\n27.5\n45.4\n33.5\n28.0\n48.7\n34.6\n12523\nno-copy\n38.4\n93.5\n54.9\n38.3\n102.1\n56.4\n29756\nno-fact-no-copy\n40.5\n98.8\n58.0\n40.3\n107.4\n59.3\n32671\nno-TransE\n48.9\n80.7\n59.6\n49.3\n85.8\n61.0\n13903\nTable 2. We compare four different versions of the NKLM to the RNNLM on three different perplexity metrics. We used 10K\nvocabulary. In no-copy, we disabled the generation-by-copy functionality, and in no-fact-no-copy, using topic knowledge is also\nadditionally disabled by setting all facts as NaF. Thus, no-fact-no-copy is very similar to RNNLM. In no-TransE, we used random\nvectors instead of the TransE embeddings to initialize the knowledge graph entities. As shown, the NKLM shows best performance in all\ncases. The no-fact-no-copy performs similar to the RNNLM as expected (slightly worse partly because it has a smaller number of model\nparameters than that of the RNNLM). As expected, no-copy performs better than no-fact-no-copy by using additional information from\nthe fact embedding, but without the copy mechanism. In the comparison of the NKLM and no-copy, we can see the signi\ufb01cant gain of\nusing the copy mechanism to predict named entities. In the last column, we can also see that, with the copy mechanism, the number of\npredicting unknown decreases signi\ufb01cantly. Lastly, we can see that the TransE embedding is important.\nValidation\nTest\nModel\nPPL\nUPP\nUPP-f\nPPL\nUPP\nUPP-f\n# UNK\nNKLM 5k\n22.8\n48.5\n30.7\n23.2\n52.0\n31.7\n19557\nRNNLM 5k\n27.4\n108.5\n47.6\n27.5\n118.3\n48.9\n34994\nNKLM 10k\n27.5\n45.4\n33.5\n28.0\n48.7\n34.6\n12523\nRNNLM 10k\n39.4\n97.9\n56.8\n39.4\n107.0\n58.4\n23247\nNKLM 20k\n33.4\n45.9\n37.9\n34.7\n49.2\n39.7\n9677\nRNNLM 20k\n57.9\n99.5\n72.1\n59.3\n108.3\n75.5\n13773\nNKLM 40k\n41.4\n49.0\n44.4\n43.6\n52.7\n47.1\n5809\nRNNLM 40k\n82.4\n107.9\n92.3\n86.4\n116.9\n97.9\n9009\nTable 3. The NKLM and the RNNLM are compared for vocabularies of four different sizes [5K, 10K, 20K, 40K]. As shown, in all\ncases the NKLM signi\ufb01cantly outperforms the RNNLM. Interestingly, for the standard perplexity (PPL), the gap between the two models\nincreases as the vocabulary size increases while for UPP the gap stays at a similar level regardless of the vocabulary size. This tells us that\nthe standard perplexity is signi\ufb01cantly affected by the UNK predictions, because with UPP the contribution of UNK predictions to the\ntotal perplexity is very small. Also, from the UPP value for the RNNLM, we can see that it initially improves when vocabulary size is\nincreased as it can cover more words, but decreases back when the vocabulary size is largest (40K) because the rare words are added last\nto the vocabulary.\nin which the anchor is found, we make a synthetic fact of\nthe form (Topic, UnknownRelation, Anchor). This po-\ntentially compensates for some missing facts in Freebase.\nBecause we extract the anchor facts from the full Wikipedia\npage and they all share the same relation, it is more chal-\nlenging for the model to use these anchor facts than using\nthe Freebase facts.\nAs a result, for each word w in the description, we obtain a\ntuple (w, z, a, n, k). Here, w is word id, z the copy indicator,\na fact id, n the position to copy from Oa if z = 1, and k\ntopic id. We provide a summary of the dataset statistics in\nTable 1.\n4.2. Experiments\n4.2.1. SETUP\nWe split the dataset into 80/10/10 for train, validation, and\ntest. As a baseline model, we use the RNNLM. For both the\nNKLM and the RNNLM, two-layer LSTMs with dropout\nregularization (Zaremba et al., 2014) are used. We tested\nmodels with different numbers of LSTM hidden units [200,\n500, 1000], and report results from the 1000 hidden-unit\nmodel. For the NKLM, we set the symbol embedding di-\nmension to 40 and word embedding dimension to 400. Un-\nder this setting, the number of parameters in the NKLM is\nslightly smaller than that of the RNNLM.\nWe used 100-dimension TransE embeddings for Freebase\nentities and relations, and concatenate the relation and ob-\nject embeddings to obtain fact embeddings. We averaged\nall fact embeddings in Fk to obtain the topic knowledge em-\nA Neural Knowledge Language Model\nWarm-up\nLouise Allbritton ( 3 july <unk>february 1979 ) was\nRNNLM\na <unk><unk>who was born in <unk>, <unk>, <unk>, <unk>, <unk>, <unk>, <unk>\nNKLM\nan english [Actor]. he was born in [Oklahoma] , and died in [Oklahoma]. he was married to [Charles] [Collingwood]\nWarm-up\nIssa Serge Coelo ( born 1967 ) is a <unk>\nRNNLM\nactor . he is best known for his role as <unk><unk>in the television series <unk>. he also\nNKLM\n[Film] director . he is best known for his role as the <unk><unk>in the \ufb01lm [Un] [taxi] [pour] [Aouzou]\nWarm-up\nAdam wade Gontier is a canadian Musician and Songwriter .\nRNNLM\nshe is best known for her role as <unk><unk>on the television series <unk>. she has also appeared\nNKLM\nhe is best known for his work with the band [Three] [Days] [Grace] . he is the founder of the\nWarm-up\nRory Calhoun ( august 8 , 1922 april 28\nRNNLM\n, 2010 ) was a <unk>actress . she was born in <unk>, <unk>, <unk>. she was\nNKLM\n, 2008 ) was an american [Actor] . he was born in [Los] [Angeles] california . he was born in\nTable 4. Sampled Descriptions. Given the warm-up phrases, we generate samples from the NKLM and the RNNLM. We denote the\ncopied knowledge words by [word] and the UNK words by <unk>. Overall, the RNNLM generates many UNKs (we used 10K\nvocabulary) while the NKLM is capable to generate named entities even if the model has not seen some of the words at all during\ntraining. In the \ufb01rst case, we found that the generated symbols (words in []) conform to the facts of the topic (Louise Allbritton) except\nthat she actually died in Mexico, not in Oklahoma. (We found that the place of death fact was missing.) While she is an actress, the\nmodel generated a word [Actor]. This is because in Freebase, there exists only /profession/actor but no /profession/actress. It is also\nnoteworthy that the NKLM fails to use the gender information provided by facts; the NKLM uses \u201che\u201d instead of \u201cshe\u201d although the fact\n/gender/female is available. From this, we see that if a fact is not detected (i.e., NaF), the statistical co-occurrence governs the information\n\ufb02ow. Similarly, in other samples, the NKLM generates movie titles (Un Taxi Pour Aouzou), band name (Three Days Grace), and place\nof birth (Los Angeles). In addition, to see the NKLM\u2019s ability to adapt to knowledge updates without retraining, we changed the fact\n/place of birth/Oklahoma to /place of birth/Chicago and found that the NKLM replaces \u201cOklahoma\u201d by \u201cChicago\u201d while keeping other\nwords the same.\nbedding ek. We unrolled the LSTMs for 30 steps and used\nminibatch size 20. We trained the models using stochastic\ngradient ascent with gradient clipping range [-5,5]. The\ninitial learning rate was set to 0.5 for the NKLM and 1.5 for\nthe RNNLM, and decayed after every epoch by a factor of\n0.98. We trained for 50 epochs and report the results chosen\nby the best validation set results.\n4.2.2. THE UNKNOWN PENALIZED PERPLEXITY\nThe perplexity exp(\u22121\nN\nPN\ni=1 log p(wi)) is the standard\nperformance metric for language modeling. This, however,\nhas a problem in evaluating language models for a corpus\ncontaining many named entities: a model can get good\nperplexity by accurately predicting UNK words as the UNK\nclass. As an extreme example, when all words in a sentence\nare unknown words, a model predicting everything as UNK\nwill get a good perplexity. Considering that unknown words\nprovide virtually no useful information, this is clearly a\nproblem in tasks where named entities are important such\nas question answering, dialogue modeling, and knowledge\nlanguage modeling.\nTo this end, we propose a new evaluation metric, called\nthe Unknown-Penalized Perplexity (UPP), and evaluate the\nmodels on this metric as well as the standard perplexity\n(PPL). Because the actual word underlying the UNK should\nbe one of the out-of-vocabulary (OOV) words, in UPP we\npenalize the likelihood of unknown words as follows:\nPUPP(wunk) =\nP(wunk)\n|Vtotal \\ Vvoca|.\nHere, Vtotal is a set of all unique words in the corpus, and\nVvoca \u2282Vtotal is the global vocabulary used for word genera-\ntion. In other words, in UPP we assume that the OOV set is\nequal to Vtotal \\ Vvoca and thus assign a uniform probability\nto OOV words. In another version, UPP-fact, we consider\nthe fact that the RNNLM can also use the knowledge given\nto the NKLM to some extent, but with limited capability\n(because the model is not designed for it). For this, we\nassume that the OOV set is equal to the total knowledge\nwords of a topic k, i.e.,\nPUPP-fact(wunk) = P(wunk)\n|OFk| ,\nwhere OFk = \u222aa\u2208FkOa. In other words, by using UPP-\nfact, we assume that, for an unknown word, the RNNLM can\npick one of the knowledge words with uniform probability.\n4.2.3. OBSERVATIONS FROM EXPERIMENT RESULTS\nWe describe the detail results and analysis on the experi-\nments in detail in the captions of Table 2, 3, and 4. Our\nobservations from the experiment results are as follows.\n\u2022 The NKLM outperforms the RNNLM in all three per-\nplexity measures.\nA Neural Knowledge Language Model\n\u2022 The copy mechanism is the key of the signi\ufb01cant per-\nformance improvement. Without the copy mechanism,\nthe NKLM still performs better than the RNNLM due\nto its usage of the fact information, but the improve-\nment is not so signi\ufb01cant.\n\u2022 The NKLM results in a much smaller number of UNKs\n(roughly, a half of the RNNLM).\n\u2022 When no knowledge is available, the NKLM performs\nas well as the RNNLM.\n\u2022 Knowledge graph embedding using TransE is an ef\ufb01-\ncient way of representing facts in our model.\n\u2022 The NKLM generates named entities in the provided\nfacts whereas the RNNLM generates many more\nUNKs.\n\u2022 The NKLM shows its ability to adapt immediately to\nthe change of the knowledge.\n\u2022 The standard perplexity is signi\ufb01cantly affected by the\nprediction accuracy on the unknown words. Thus, one\nneed carefully consider when using it as a metric for\nknowledge-related language models.\n5. Conclusion\nIn this paper, we presented a novel Neural Knowledge Lan-\nguage Model (NKLM) that brings the symbolic knowledge\nfrom a knowledge graph into the expressive power of RNN\nlanguage models. The NKLM signi\ufb01cantly outperforms the\nRNNLM in terms of perplexity and generates named entities\nwhich are not observed during training, as well as immedi-\nately adapting to changes in knowledge. We believe that the\nWikiFact dataset introduced in this paper, can be useful in\nother knowledge-related language tasks as well. In addition,\nthe Unknown-Penalized Perplexity introduced in order to\nresolve the limitation of the standard perplexity, can also be\nuseful in evaluating other language tasks.\nThe task that we investigated in this paper is limited in\nthe sense that we assume that the true topic of a given\ndescription is known. Relaxing this assumption by making\nthe model search for a proper topic on-the-\ufb02y will make\nthe model more practical and scalable. We believe that\nthere are many more open research challenges related to the\nknowledge language models.\nAcknowledgments\nThe authors would like to thank Alberto Garc\u00b4\u0131a-Dur\u00b4an,\nCaglar Gulcehre, Chinnadhurai Sankar, Iulian Serban,\nSarath Chandar, and Peter Clark for helpful feedbacks and\ndiscussions as well as the developers of Theano (Bastien\net al., 2012), NSERC, CIFAR, Facebook, Google, IBM, Mi-\ncrosoft, Samsung, and Canada Research Chairs for funding,\nand Compute Canada for computing resources.\nReferences\nBahdanau, Dzmitry, Cho, Kyunghyun, and Bengio, Yoshua.\nNeural machine translation by jointly learning to align\nand translate. arXiv preprint arXiv:1409.0473, 2014.\nBastien, Fr\u00b4ed\u00b4eric, Lamblin, Pascal, Pascanu, Razvan,\nBergstra, James, Goodfellow, Ian, Bergeron, Arnaud,\nBouchard, Nicolas, Warde-Farley, David, and Bengio,\nYoshua. Theano: new features and speed improvements.\narXiv preprint arXiv:1211.5590, 2012.\nBengio, Yoshua, Ducharme, R\u00b4ejean, Vincent, Pascal, and\nJauvin, Christian. A neural probabilistic language model.\nIn Journal of Machine Learning Research, 2003.\nBlei, David M, Ng, Andrew Y, and Jordan, Michael I. Latent\ndirichlet allocation. the Journal of machine Learning\nresearch, 3:993\u20131022, 2003.\nBordes, Antoine, Usunier, Nicolas, Garcia-Duran, Alberto,\nWeston, Jason, and Yakhnenko, Oksana.\nTranslating\nembeddings for modeling multi-relational data. In Ad-\nvances in Neural Information Processing Systems, pp.\n2787\u20132795, 2013.\nBordes, Antoine, Usunier, Nicolas, Chopra, Sumit, and\nWeston, Jason. Large-scale simple question answering\nwith memory networks. arXiv preprint arXiv:1506.02075,\n2015.\nCelikyilmaz, Asli, Hakkani-Tur, Dilek, Pasupat, Panupong,\nand Sarikaya, Ruhi. Enriching word embeddings using\nknowledge graph for semantic tagging in conversational\ndialog systems. In 2015 AAAI Spring Symposium Series,\n2015.\nChelba, Ciprian, Mikolov, Tomas, Schuster, Mike, Ge,\nQi, Brants, Thorsten, Koehn, Phillipp, and Robinson,\nTony.\nOne billion word benchmark for measuring\nprogress in statistical language modeling. arXiv preprint\narXiv:1312.3005, 2013.\nGildea, Daniel and Hofmann, Thomas. Topic-based lan-\nguage models using em. EuroSpeech 1999, 1999.\nGraves, Alex, Wayne, Greg, and Danihelka, Ivo. Neural\nturing machines. arXiv preprint arXiv:1410.5401, 2014.\nGu, Jiatao, Lu, Zhengdong, Li, Hang, and Li, Victor\nO. K. Incorporating copying mechanism in sequence-\nto-sequence learning. CoRR, abs/1603.06393, 2016.\nGulcehre, Caglar, Ahn, Sungjin, Nallapati, Ramesh, Zhou,\nBowen, and Bengio, Yoshua.\nPointing the unknown\nwords. ACL 2016, 2016.\nHochreiter, Sepp and Schmidhuber, J\u00a8urgen. Long short-term\nmemory. Neural computation, 9(8):1735\u20131780, 1997.\nA Neural Knowledge Language Model\nIyyer, Mohit, Boyd-Graber, Jordan L, Claudino, Leonardo\nMax Batista, Socher, Richard, and Daum\u00b4e III, Hal. A\nneural network for factoid question answering over para-\ngraphs. In EMNLP 2014, pp. 633\u2013644, 2014.\nJean, Sebastien, Cho, Kyunghyun, Memisevic, Roland, and\nBengio, Yoshua. On using very large target vocabulary\nfor neural machine translation. ACL 2015, 2015.\nJozefowicz, Rafal, Vinyals, Oriol, Schuster, Mike, Shazeer,\nNoam, and Wu, Yonghui. Exploring the limits of lan-\nguage modeling. arXiv preprint arXiv:1602.02410, 2016.\nKim, Yoon. Convolutional neural networks for sentence\nclassi\ufb01cation. EMNLP 2014, 2014.\nLebret, R\u00b4emi, Grangier, David, and Auli, Michael. Neural\ntext generation from structured data with application to\nthe biography domain. arXiv preprint arXiv:1603.07771,\n2016.\nLong, Teng, Lowe, Ryan, Cheung, Jackie Chi Kit, and\nPrecup, Doina. Leveraging lexical resources for learning\nentity embeddings in multi-relational data. 2016.\nMarcus, Mitchell P, Marcinkiewicz, Mary Ann, and San-\ntorini, Beatrice. Building a large annotated corpus of\nenglish: The penn treebank. Computational linguistics,\n19(2):313\u2013330, 1993.\nMerity, Stephen, Xiong, Caiming, Bradbury, James, and\nSocher, Richard. Pointer sentinel mixture models. arXiv\npreprint arXiv:1609.07843, 2016.\nMikolov, Tomas and Zweig, Geoffrey. Context dependent\nrecurrent neural network language model. In Spoken\nLanguage Technology Workshop (SLT), 2012 IEEE, pp.\n234\u2013239. IEEE, 2012.\nMikolov, Tomas, Kara\ufb01\u00b4at, Martin, Burget, Lukas, Cernock`y,\nJan, and Khudanpur, Sanjeev. Recurrent neural network\nbased language model. In INTERSPEECH 2010, vol-\nume 2, pp. 3, 2010.\nMiller, George A. Wordnet: a lexical database for english.\nCommunications of the ACM, 38(11):39\u201341, 1995.\nMnih, Andriy and Hinton, Geoffrey E. A scalable hierar-\nchical distributed language model. In Advances in neural\ninformation processing systems, pp. 1081\u20131088, 2009.\nMnih, Andriy and Teh, Yee Whye. A fast and simple algo-\nrithm for training neural probabilistic language models.\nICML 2012, 2012.\nMorin, Frederic and Bengio, Yoshua. Hierarchical proba-\nbilistic neural network language model. AISTATS 2005,\npp. 246, 2005.\nNickel, Maximilian, Murphy, Kevin, Tresp, Volker, and\nGabrilovich, Evgeniy. A review of relational machine\nlearning for knowledge graphs: From multi-relational link\nprediction to automated knowledge graph construction.\narXiv preprint arXiv:1503.00759, 2015.\nReed, Scott and de Freitas, Nando. Neural programmer-\ninterpreters. ICLR 2016, 2016.\nSerban, Iulian V, Sordoni, Alessandro, Bengio, Yoshua,\nCourville, Aaron, and Pineau, Joelle. Building end-to-\nend dialogue systems using generative hierarchical neural\nnetworks. 30th AAAI Conference on Arti\ufb01cial Intelli-\ngence, 2015.\nVinyals, Oriol and Le, Quoc. A neural conversational model.\narXiv preprint arXiv:1506.05869, 2015.\nVinyals, Oriol, Fortunato, Meire, and Jaitly, Navdeep.\nPointer networks. NIPS 2015, 2015.\nWeston, Jason, Chopra, Sumit, and Bordes, Antoine. Mem-\nory networks. ICLR 2015, 2015.\nWeston, Jason, Bordes, Antoine, Chopra, Sumit, and\nMikolov, Tomas. Towards ai-complete question answer-\ning: A set of prerequisite toy tasks. ICLR 2016, 2016.\nZaremba, Wojciech, Sutskever, Ilya, and Vinyals, Oriol.\nRecurrent neural network regularization. arXiv preprint\narXiv:1409.2329, 2014.\nA Neural Knowledge Language Model\nAPPENDIX: HEATMAPS\np_copy_fact\n,\n2008\n)\nwas\nan\namerican Actor\n.\nhe\nwas\nborn\nin\nLos\nAngeles california\n.\n<naf>\nprofession-Screenwriter\nprofession-Actor\nplace_of_birth-Los Angeles\nprofession-Film Producer\ntopic_itself-Rory Calhoun\nunk_rel-Spellbound\nunk_rel-California\nunk_rel-Santa Cruz\nFigure 2. This is a heatmap of an example sentence generated by the NKLM having a warmup \u201cRory Calhoun ( august 8 , 1922 april\n28\u201d. The \ufb01rst row shows the probability of selecting copy (Equation 5 in Section 3.1). The bottom heat map shows the state of the\ntopic-memory at each time step (Equation 2 in Section 3.1). In particular, this topic has 8 facts and an additional <NaF> fact. For the\n\ufb01rst six time steps, the model retrieves <NaF>from the knowledge memory, copy-switch is off and the words are generated from the\ngeneral vocabulary. For the next time step, the model gives higher probability to three different profession facts: \u201cScreenwriter\u201d, \u201cActor\u201d\nand \u201cFilm Producer.\u201d The fact \u201cActor\u201d has the highest probability, copy-switch is higher than 0.5, and therefore \u201cActor\u201d is copied as the\nnext word. Moreover, we see that the model correctly retrieves the place of birth fact and outputs \u201cLos Angeles.\u201d After that, the model\nstill predicts the place of birth fact, but copy-switch decides that the next word should come from the general vocabulary, and outputs\n\u201cCalifornia.\u201d\np_copy_fact\nan\nenglish Actor\n.\nhe\nwas\nborn\nin\nOklahoma\n,\nand\ndied\nin\nOklahoma\n.\nhe\nwas\nmarried\nto\nCharles Collingwood .\n<naf>\neducation.institution-University of Oklahoma\nperformance.film-Son of Dracula\nlocation.people_born_here-Oklahoma City\nperformance.film-The Egg and I\nmarriage.type_of_union-Marriage\nmarriage.spouse-Charles Collingwood\nprofession-Actor\ntopic_itself-Louise Allbritton\nunk_rel-Universal Studios\nunk_rel-Pasadena Playhouse\nunk_rel-Pittsburgh\nunk_rel-Sitting Pretty\nunk_rel-Hollywood\nunk_rel-World War II\nunk_rel-United Service Organizations\nFigure 3. This is an example sentence generated by the NKLM having a warmup \u201cLouise Allbritton ( 3 july <unk>february 1979 )\nwas\u201d. We see that the model correctly retrieves and outputs the profession (\u201cActor\u201d), place of birth (\u201cOklahoma\u201d), and spouse (\u201cCharles\nCollingwood\u201d) facts. However, the model makes a mistake by retrieving the place of birth fact in a place where the place of death fact is\nsupposed to be used. This is probably because the place of death fact is missing in this topic memory and then the model searches for a\nfact about location, which is somewhat encoded in the place of birth fact. In addition, Louise Allbritton was a woman, but the model\ngenerates a male profession \u201cActor\u201d and male pronoun \u201che\u201d. The \u201cActor\u201d is generated because there is no \u201cActress\u201d representation in\nFreebase.\n",
        "sentence": " Simply using documents that are in a large scale into the models is hardly good enough for learning a good representation of the documents [36].",
        "context": "Montr\u00b4eal, Canada 4CIFAR Senior Fellow. Correspondence to:\nSungjin Ahn <sjn.ahn@gmail.com>.\nwith a very large corpus. However, we claim that simply\nfeeding a larger corpus into a bigger model hardly results in\na good knowledge language model.\nThe primary reason for this is the dif\ufb01culty in learning\ngood representations for rare and unknown words. This is a\nsigni\ufb01cant problem because these words are of our primary\ninterest in knowledge-related applications such as question\naccording to Zipf\u2019s law, a large portion of the words will\nstill be rarely observed in the corpus.\nThe fact that languages and knowledge can change over time\nalso makes it dif\ufb01cult to simply rely on a large corpus. Me-"
    },
    {
        "title": "Query sense disambiguation leveraging large scale user behavioral data",
        "author": [
            "Mohammed Korayem",
            "Camilo Ortiz",
            "Khalifeh AlJadda",
            "Trey Grainger"
        ],
        "venue": "In IEEE International Conference on Big Data (Big Data",
        "citeRegEx": "37",
        "shortCiteRegEx": "37",
        "year": 2015,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " All experiments used a sample of 1,147,725 classified jobs from our dataset [37].",
        "context": null
    },
    {
        "title": "Software Framework for Topic Modelling with Large Corpora",
        "author": [
            "Radim \u0158eh\u016f\u0159ek",
            "Petr Sojka"
        ],
        "venue": "In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks,",
        "citeRegEx": "38",
        "shortCiteRegEx": "38",
        "year": 2010,
        "abstract": "",
        "full_text": "",
        "sentence": " [38] to generate tf-idf model and train LDA and doc2vec models.",
        "context": null
    }
]