[
    {
        "title": "Struck: Structured output tracking with kernels",
        "author": [
            "S. Hare",
            "A. Saffari",
            "P. Torr"
        ],
        "venue": "Proc. IEEE Int. Conf. Comp. Vis., 2011.",
        "citeRegEx": "1",
        "shortCiteRegEx": null,
        "year": 2011,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " 1 INTRODUCTION Structured learning has attracted extensive attention recently in machine learning and computer vision [1]\u2013[4]. The work in [1] introduced structured learning to real-time object detection and tracking, which also optimizes the Pascal box overlap score. 5 Optimization of the Pascal image overlap criterion Object detection/localization has used the image area overlap as the loss function [1]\u2013[3], e. This simple search strategy has been used in [1]. 5 Visual tracking by optimizing the image area overlap criterion In [1], a visual tracking method, termed Struck, was introduced based on SSVM. Struck50 is structured SVM tracking with a buffer size of 50 [1]. Structured SVM of [1] is the second best, which confirms the usefulness of structured training. Struck50 is structured SVM tracking with a buffer size of 50 [1]. We observe similar results as in Table 4: Our StructBoost outperforms other methods on all the sequences, and structured SVM of [1] is the second best. We also compare our trackers with a few state-of-the-art tracking methods, including Struck [1] (with a buffer size (a) Testing images The test video sequences \u201ccoke, tiger1, tiger2, david, girl and sylv\u201d were used in [1]. When Struck uses a Gaussian kernel defined on raw pixels, the performance is slightly different [1], and ours still outperforms Struck in most cases. This might be due to the fact that our StructBoost selects relevant features (300 features selected here), and the SSVM of [1] uses all the image patch information which may contain noises.",
        "context": null
    },
    {
        "title": "Structured learning and prediction in computer vision",
        "author": [
            "S. Nowozin",
            "C.H. Lampert"
        ],
        "venue": "Foundations & Trends in Computer Graphics & Vision, 2011.",
        "citeRegEx": "2",
        "shortCiteRegEx": null,
        "year": 2011,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " The survey of [2] has provided a comprehensive review of structured learning and its application in computer vision.",
        "context": null
    },
    {
        "title": "Learning to localize objects with structured output regression",
        "author": [
            "M.B. Blaschko",
            "C.H. Lampert"
        ],
        "venue": "Proc. Eur. Conf. Comp. Vis., 2008, pp. 2\u201315.",
        "citeRegEx": "3",
        "shortCiteRegEx": null,
        "year": 2008,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Blaschko and Lampert [3] trained SSVM models to predict the bounding box of objects in a given image, by optimizing the Pascal bounding box overlap score. 5 Optimization of the Pascal image overlap criterion Object detection/localization has used the image area overlap as the loss function [1]\u2013[3], e. In [3], a branch-and-bound search has been employed to find the global optimum.",
        "context": null
    },
    {
        "title": "Support vector machine learning for interdependent and structured output spaces",
        "author": [
            "I. Tsochantaridis",
            "T. Hofmann",
            "T. Joachims",
            "Y. Altun"
        ],
        "venue": "Proc. Int. Conf. Mach. Learn., 2004, pp. 104\u2013111.  15",
        "citeRegEx": "4",
        "shortCiteRegEx": null,
        "year": 2004,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " 1 INTRODUCTION Structured learning has attracted extensive attention recently in machine learning and computer vision [1]\u2013[4]. Structured support vector machines (SSVM) [4] generalize the multiclass SVM of [5] and [6] to the much broader problem of learning for interdependent and structured outputs. 2 Related work The two state-of-the-art structured learning methods are CRF [10] and SSVM [4], which captures the interdependency among output variables. Similar to [4], here we consider the tree loss: \u2206(y, y\u2032). 5 CONCLUSION We have presented a boosting method for structural learning, as an alternative to SSVM [4] and CRF [10].",
        "context": null
    },
    {
        "title": "Multi-class support vector machines",
        "author": [
            "J. Weston",
            "C. Watkins"
        ],
        "venue": "Proc. Euro. Symp. Artificial Neural Networks, 1999.",
        "citeRegEx": "5",
        "shortCiteRegEx": null,
        "year": 1999,
        "abstract": "",
        "full_text": "",
        "sentence": " Structured support vector machines (SSVM) [4] generalize the multiclass SVM of [5] and [6] to the much broader problem of learning for interdependent and structured outputs. Recently, Shen and Hao proposed a direct formulation for multi-class boosting using the loss functions of multi-class SVM [5], [6].",
        "context": null
    },
    {
        "title": "On the algorithmic implementation of multiclass kernel-based vector mchines",
        "author": [
            "K. Crammer",
            "Y. Singer"
        ],
        "venue": "J. Mach. Learn. Res., vol. 2, pp. 265\u2013292, 2001.",
        "citeRegEx": "6",
        "shortCiteRegEx": null,
        "year": 2001,
        "abstract": "",
        "full_text": "",
        "sentence": " Structured support vector machines (SSVM) [4] generalize the multiclass SVM of [5] and [6] to the much broader problem of learning for interdependent and structured outputs. Recently, Shen and Hao proposed a direct formulation for multi-class boosting using the loss functions of multi-class SVM [5], [6].",
        "context": null
    },
    {
        "title": "On the dual formulation of boosting algorithms",
        "author": [
            "C. Shen",
            "H. Li"
        ],
        "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 32, no. 12, pp. 2216\u20132231, 2010.",
        "citeRegEx": "7",
        "shortCiteRegEx": null,
        "year": 2010,
        "abstract": "We study boosting algorithms from a new perspective. We show that the\nLagrange dual problems of AdaBoost, LogitBoost and soft-margin LPBoost with\ngeneralized hinge loss are all entropy maximization problems. By looking at the\ndual problems of these boosting algorithms, we show that the success of\nboosting algorithms can be understood in terms of maintaining a better margin\ndistribution by maximizing margins and at the same time controlling the margin\nvariance.We also theoretically prove that, approximately, AdaBoost maximizes\nthe average margin, instead of the minimum margin. The duality formulation also\nenables us to develop column generation based optimization algorithms, which\nare totally corrective. We show that they exhibit almost identical\nclassification results to that of standard stage-wise additive boosting\nalgorithms but with much faster convergence rates. Therefore fewer weak\nclassifiers are needed to build the ensemble using our proposed optimization\ntechnique.",
        "full_text": "arXiv:0901.3590v7  [cs.LG]  27 May 2023\nOn the Dual Formulation of Boosting Algorithms\nChunhua Shen,\nHanxi Li\nAbstract\u2014We study boosting algorithms from a new perspective. We show that the Lagrange dual problems of the \u21131 norm regularized\nAdaBoost, LogitBoost and soft-margin LPBoost with generalized hinge loss are all entropy maximization problems. By looking at the\ndual problems of these boosting algorithms, we show that the success of boosting algorithms can be understood in terms of maintaining\na better margin distribution by maximizing margins and at the same time controlling the margin variance. We also theoretically prove\nthat, approximately, the \u21131 norm regularized AdaBoost maximizes the average margin, instead of the minimum margin. The duality\nformulation also enables us to develop column generation based optimization algorithms, which are totally corrective. We show that\nthey exhibit almost identical classi\ufb01cation results to that of standard stage-wise additive boosting algorithms but with much faster\nconvergence rates. Therefore fewer weak classi\ufb01ers are needed to build the ensemble using our proposed optimization technique.\nIndex Terms\u2014AdaBoost, LogitBoost, LPBoost, Lagrange duality, linear programming, entropy maximization.\n\u2726\n1\nINTRODUCTION\nB\nOOSTING has attracted a lot of research interests\nsince the \ufb01rst practical boosting algorithm, Ad-\naBoost, was introduced by Freund and Schapire [1]. The\nmachine learning community has spent much effort on\nunderstanding how the algorithm works [2], [3], [4].\nHowever, up to date there are still questions about\nthe success of boosting that are left unanswered [5].\nIn boosting, one is given a set of training examples\nxi \u2208X, i = 1 \u00b7 \u00b7 \u00b7 M, with binary labels yi being either +1\nor \u22121. A boosting algorithm \ufb01nds a convex linear com-\nbination of weak classi\ufb01ers (a.k.a. base learners, weak\nhypotheses) that can achieve much better classi\ufb01cation\naccuracy than an individual base classi\ufb01er. To do so,\nthere are two unknown variables to be optimized. The\n\ufb01rst one is the base classi\ufb01ers. An oracle is needed to\nproduce base classi\ufb01ers. The second one is the positive\nweights associated with each base classi\ufb01er.\nAdaBoost is one of the \ufb01rst and the most popu-\nlar boosting algorithms for classi\ufb01cation. Later, various\nboosting algorithms have been advocated. For example,\nLogitBoost by Friedman et al. [6] replaces AdaBoost\u2019s\nexponential cost function with the function of logistic\nregression. MadaBoost [7] instead uses a modi\ufb01ed ex-\nponential loss. The authors of [6] consider boosting alg-\norithms with a generalized additive model framework.\nSchapire et al. [2] showed that AdaBoost converges to\na large margin solution. However, recently it is pointed\nout that AdaBoost does not converge to the maximum\nmargin solution [4], [8]. Motivated by the success of the\nmargin theory associated with support vector machines\n(SVMs), LPBoost was invented by [9], [10] with the\nintuition of maximizing the minimum margin of all\nC. Shen and H. Li are with NICTA (National ICT Australia), Canberra Re-\nsearch Laboratory, Canberra, ACT 2601, Australia; and Australian National\nUniversity, Canberra, ACT 0200, Australia. E-mail: chunhua@me.com\nPublished 2010 Dec.; IEEE Transactions on Pattern Analysis and Machine\nIntellgience 32(12): 2216-31.\nDigital Object Identi\ufb01er: 10.1109/TPAMI.2010.47\ntraining examples. The \ufb01nal optimization problem can\nbe formulated as a linear program (LP). It is observed\nthat the hard-margin LPBoost does not perform well in\nmost cases although it usually produces larger minimum\nmargins. More often LPBoost has worse generalization\nperformance. In other words, a higher minimum margin\nwould not necessarily imply a lower test error. Breiman\n[11] also noticed the same phenomenon: his Arc-Gv\nalgorithm has a minimum margin that provably con-\nverges to the optimal but Arc-Gv is inferior in terms of\ngeneralization capability. Experiments on LPBoost and\nArc-Gv have put the margin theory into serious doubt.\nUntil recently, Reyzin and Schapire [12] re-ran Breiman\u2019s\nexperiments by controlling weak classi\ufb01ers\u2019 complex-\nity. They found that the minimum margin is indeed\nlarger for Arc-Gv, but the overall margin distribution\nis typically better for AdaBoost. The conclusion is that\nthe minimum margin is important, but not always at\nthe expense of other factors. They also conjectured that\nmaximizing the average margin, instead of the minimum\nmargin, may result in better boosting algorithms. Recent\ntheoretical work [13] has shown the important role of the\nmargin distribution on bounding the generalization error\nof combined classi\ufb01ers such as boosting and bagging.\nAs the soft-margin SVM usually has a better classi-\n\ufb01cation accuracy than the hard-margin SVM, the soft-\nmargin LPBoost also performs better by relaxing the\nconstraints that all training examples must be correctly\nclassi\ufb01ed. Cross-validation is required to determine an\noptimal value for the soft-margin trade-off parameter.\nR\u00a8atsch et al. [14] showed the equivalence between SVMs\nand boosting-like algorithms. Comprehensive overviews\non boosting are given by [15] and [16].\nWe show in this work that the Lagrange duals of\n\u21131 norm regularized AdaBoost, LogitBoost and LPBoost\nwith generalized hinge loss are all entropy maximization\nproblems. Previous work like [17], [18], [19] noticed the\nconnection between boosting techniques and entropy\nmaximization based on Bregman distances. They did\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 32, NO. 12, DEC. 2010\n2\nnot show that the duals of boosting algorithms are\nactually entropy regularized LPBoost as we show in\n(10), (28) and (31). By knowing this duality equivalence,\nwe derive a general column generation (CG) based\noptimization framework that can be used to optimize\narbitrary convex loss functions. In other words, we\ncan easily design totally-corrective AdaBoost, LogitBoost\nand boosting with generalized hinge loss, etc.\nOur major contributions are the following:\n1) We derive the Lagrangian duals of boosting alg-\norithms and show that most of them are entropy\nmaximization problems.\n2) The authors of [12] conjectured that \u201cit may be\nfruitful to consider boosting algorithms that greed-\nily maximize the average or median margin rather\nthan the minimum one\u201d. We theoretically prove\nthat, actually, \u21131 norm regularized AdaBoost ap-\nproximately maximizes the average margin, in-\nstead of the minimum margin. This is an important\nresult in the sense that it provides an alternative\ntheoretical explanation that is consistent with the\nmargins theory and agrees with the empirical ob-\nservations made by [12].\n3) We propose AdaBoost-QP that directly optimizes\nthe asymptotic cost function of AdaBoost. The ex-\nperiments con\ufb01rm our theoretical analysis.\n4) Furthermore, based on the duals we derive, we\ndesign column generation based optimization tech-\nniques for boosting learning. We show that the new\nalgorithms have almost identical results to that of\nstandard stage-wise additive boosting algorithms\nbut with much faster convergence rates. Therefore\nfewer weak classi\ufb01ers are needed to build the\nensemble.\nThe following notation is used. Typically, we use bold\nletters u, v to denote vectors, as opposed to scalars u, v\nin lower case letters. We use capital letters U, V to denote\nmatrices. All vectors are column vectors unless otherwise\nspeci\ufb01ed. The inner product of two column vectors u\nand v are u\u22a4v = P\ni uivi. Component-wise inequalities\nare expressed using symbols \u227d, \u227b, \u227c, \u227a; e.g., u \u227dv\nmeans for all the entries ui \u2265vi. 0 and 1 are column\nvectors with each entry being 0 and 1 respectively. The\nlength will be clear from the context. The abbreviation\ns.t. means \u201csubject to\u201d. We denote the domain of a\nfunction f(\u00b7) as dom f.\nThe paper is organized as follows. Section 2 brie\ufb02y re-\nviews several boosting algorithms for self-completeness.\nTheir corresponding duals are derived in Section 3. Our\nmain results are also presented in Section 3. In Section 4,\nwe then present numerical experiments to illustrate var-\nious aspects of our new algorithms obtained in Section\n3. We conclude the paper in the last section.\n2\nBOOSTING ALGORITHMS\nWe \ufb01rst review some basic ideas and the corresponding\noptimization problems of AdaBoost, LPBoost and Logit-\nBoost, which are of interest in this present work.\nLet H be a class of base classi\ufb01er H = {hj(\u00b7) : X \u2192\nR, j = 1 \u00b7 \u00b7 \u00b7 N}. A boosting algorithm seeks for a convex\nlinear combination\nF(x) = PN\nj=1 wjhj(x),\n(1)\nwhere w is the weak classi\ufb01er weights to be optimized.\nAdaBoost calls an oracle that selects a weak classi\ufb01er\nhj(\u00b7) at each iteration j and then calculates the weight\nwj associated with hj(\u00b7). It is shown in [6], [20] that\nAdaBoost (and many others like LogitBoost) performs\ncoordinate gradient descent in function space, at each\niteration choosing a weak classi\ufb01er to include in the\ncombination such that the cost function is maximally\nreduced. It is well known that coordinate descent has a\nslow convergence in many cases. From an optimization\npoint of view, there is no particular reason to keep the\nweights w1, \u00b7 \u00b7 \u00b7 , wj\u22121 \ufb01xed at iteration j. Here we focus\non the underlying mathematical programs that boosting\nalgorithms minimize.\nAdaBoost has proved to minimize the exponential loss\nfunction [17]:\nmin\nw\nM\nX\ni=1\nexp(\u2212yiF(xi)), s.t. w \u227d0.\n(2)\nBecause the logarithmic function log(\u00b7) is a strictly\nmonotonically increasing function, AdaBoost equiva-\nlently solves\nmin\nw\nlog\n M\nX\ni=1\nexp(\u2212yiF(xi))\n!\n, s.t. w \u227d0, 1\u22a4w = 1\nT .\n(3)\nNote that in the AdaBoost algorithm, the constraint\n1\u22a4w =\n1\nT is not explicitly enforced. However, without\nthis regularization constraint, in the case of separable\ntraining data, one can always make the cost function\napproach to zero via enlarging the solution w by an\narbitrarily large factor. Here what matters is the sign\nof the classi\ufb01cation evaluation function. Standard Ad-\naBoost seems to select the value of T by selecting how\nmany iterations it runs. Note that the relaxed version\n1\u22a4w \u2264\n1\nT is actually equivalent to 1\u22a4w = 1\nT .1 With the\nconstraint 1\u22a4w \u22641\nT , if the \ufb01nal solution has 1\u22a4w < 1\nT ,\none can scale w such that 1\u22a4w =\n1\nT and clearly the\nscaled w achieves a smaller loss. So the optimum must\nbe achieved at the boundary.\nThe boosting algorithm introduced in (3) is a \u21131 norm\nregularized version of the original AdaBoost because it\nis equivalent to\nmin\nw\nlog\n M\nX\ni=1\nexp(\u2212yiF(xi))\n!\n+ 1\nT \u20321\u22a4w, s.t. w \u227d0. (4)\nFor a certain T , one can always \ufb01nd a T \u2032 such that (3)\nand (4) have exactly the same solution. Hereafter, we\nrefer to this algorithm as AdaBoost\u21131.\n1. The reason why we do not write this constraint as 1\u22a4w = T will\nbecome clear later.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 32, NO. 12, DEC. 2010\n3\nWe will show that it is very important to introduce this\nnew cost function. All of our main results on AdaBoost\u21131\nare obtained by analyzing this logarithmic cost function,\nnot the original cost function. Let us de\ufb01ne the matrix\nH \u2208ZM\u00d7N, which contains all the possible predictions\nof the training data using weak classi\ufb01ers from the pool\nH. Explicitly Hij = hj(xi) is the label ({+1, \u22121}) given\nby weak classi\ufb01er hj(\u00b7) on the training example xi. We\nuse Hi = [Hi1 Hi2 \u00b7 \u00b7 \u00b7 HiN] to denote i-th row of H, which\nconstitutes the output of all the weak classi\ufb01ers on the\ntraining example xi. The cost function of AdaBoost\u21131\nwrites:\nmin\nw\nlog\n M\nX\ni=1\nexp(\u2212yiHiw)\n!\n, s.t. w \u227d0, 1\u22a4w = 1\nT .\n(5)\nWe can also write the above program into\nmin\nw\nlog\n M\nX\ni=1\nexp\n\u0012\n\u2212yiHiw\nT\n\u0013!\n, s.t. w \u227d0, 1\u22a4w = 1,\n(6)\nwhich is exactly the same as (5). In [4] the smooth\nmargin that is similar but different to the logarithmic\ncost function, is used to analyze AdaBoost\u2019s convergence\nbehavior.2\nProblem (5) (or (6)) is a convex problem in w. We know\nthat the log-sum-exp function lse(x) = log(PM\ni=1 exp xi)\nis convex [21]. Composition with an af\ufb01ne mapping\npreserves convexity. Therefore, the cost function is con-\nvex. The constraints are linear hence convex too. For\ncompleteness, we include the description of the standard\nstage-wise AdaBoost and Arc-Gv in Algorithm 1. The\nonly difference of these two algorithms is the way to\ncalculate wj (step (2) of Algorithm 1). For AdaBoost:\nwj = 1\n2 log 1 + rj\n1 \u2212rj\n,\n(7)\nwhere rj is the edge of the weak classi\ufb01er hj(\u00b7) de\ufb01ned\nas rj = PM\ni=1 uiyihj(xi) = PM\ni=1 uiyiHij. Arc-Gv modi-\n\ufb01es (7) in order to maximize the minimum margin:\nwj = 1\n2 log 1 + rj\n1 \u2212rj\n\u22121\n2 log 1 + \u033aj\n1 \u2212\u033aj\n,\n(8)\nwhere \u033aj is the minimum margin over all training exam-\nples of the combined classi\ufb01er up to the current round:\n\u033aj = min\ni {yi\nPj\u22121\ns=1 wshs(xi)/Pj\u22121\ns=1 ws}, with \u033a1 = 0. Arc-\nGv clips wj into [0, 1] by setting wj = 1 if wj > 1 and\nwj = 0 if wj < 0 [11]. Other work such as [8], [22] has\nused different approaches to determine \u033aj in (8).\n3\nLAGRANGE DUAL OF BOOSTING ALGORIT-\nHMS\nOur main derivations are based on a form of duality\ntermed convex conjugate or Fenchel duality.\n2. The smooth margin in [4] is de\ufb01ned as\n\u2212log\u0000PM\ni=1 exp (\u2212yiHiw)\u0001\n1\u22a4w\n.\nAlgorithm 1 Stage-wise AdaBoost, and Arc-Gv.\nInput: Training set\n(xi, yi), yi = {+1, \u22121}, i = 1 \u00b7 \u00b7 \u00b7 M; maximum\niteration Nmax.\nInitialization: u0\ni =\n1\nM , \u2200i = 1 \u00b7 \u00b7 \u00b7 M.\n1\nfor j = 1, \u00b7 \u00b7 \u00b7 , Nmax do\n2\n1) Find a new base hj(\u00b7) using the distribution uj;\n2) Choose wj;\n3) Update u: uj+1\ni\n\u221duj\ni exp (\u2212yiwjhj(xi)), \u2200i; and\nnormalize uj+1.\nOutput: The learned classi\ufb01er\nF(x) = PN\nj=1 wjhj(x).\nDe\ufb01nition 3.1. (Convex conjugate) Let f : Rn \u2192R. The\nfunction f \u2217: Rn \u2192R, de\ufb01ned as\nf \u2217(u) =\nsup\nx\u2208dom f\n\u0000u\u22a4x \u2212f(x)\n\u0001\n,\n(9)\nis called the convex conjugate (or Fenchel duality) of\nthe function f(\u00b7). The domain of the conjugate function\nconsists of u \u2208Rn for which the supremum is \ufb01nite.\nf \u2217(\u00b7) is always a convex function because it is the\npointwise supremum of a family of af\ufb01ne functions of\nu. This is true even if f(\u00b7) is non-convex [21].\nProposition 3.1. (Conjugate of log-sum-exp) The conju-\ngate of the log-sum-exp function is the negative entropy\nfunction, restricted to the probability simplex. Formally,\nfor lse(x) = log(PM\ni=1 exp xi), its conjugate is:\nlse\u2217(u) =\n(PM\ni=1 ui log ui,\nif u \u227d0 and 1\u22a4u = 1;\n\u221e\notherwise.\nWe interpret 0 log 0 as 0.\nChapter 3.3 of [21] gives this result.\nTheorem 3.1. The dual of AdaBoost\u21131 is a Shannon en-\ntropy maximization problem, which writes,\nmax\nr,u\nr\nT \u2212PM\ni=1ui log ui\ns.t. PM\ni=1uiyiHi \u227c\u2212r1\u22a4,\n(10)\nu \u227d0, 1\u22a4u = 1.\nProof: To derive a Lagrange dual of AdaBoost\u21131, we\n\ufb01rst introduce a new variable z \u2208RM such that its i-th\nentry zi = \u2212yiHiw, to obtain the equivalent problem\nmin\nw\nlog\n\u0010PM\ni=1 exp zi\n\u0011\ns.t. zi = \u2212yiHiw (\u2200i = 1, \u00b7 \u00b7 \u00b7 , M),\n(11)\nw \u227d0, 1\u22a4w = 1\nT .\nThe Lagrangian L(\u00b7) associated with the problem (5) is\nL(w, z, u, q, r) = log\n M\nX\ni=1\nexp zi\n!\n\u2212\nM\nX\ni=1\nui(zi + yiHiw)\n\u2212q\u22a4w \u2212r(1\u22a4w \u22121\nT ),\n(12)\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 32, NO. 12, DEC. 2010\n4\nwith q \u227d0. The dual function is\ninf\nz,w L = inf\nz,w log\n M\nX\ni=1\nexp zi\n!\n\u2212\nM\nX\ni=1\nuizi + r\nT\n\u2212\nmust be 0\nz\n}|\n{\n M\nX\ni=1\nuiyiHi + q\u22a4+ r1\u22a4\n!\nw\n= inf\nz log\n M\nX\ni=1\nexp zi\n!\n\u2212u\u22a4z + r\nT\n=\n\u2212lse\u2217(u) (see Proposition 3.1)\nz\n}|\n{\n\u2212sup\nz\n\"\nu\u22a4z \u2212log\n M\nX\ni=1\nexp zi\n!#\n+ r\nT\n= \u2212\nM\nX\ni=1\nui log ui + r\nT .\n(13)\nBy collecting all the constraints and eliminating q, the\ndual of Problem (5) is (10).\nKeeping two variables w and z, and introducing new\nequality constraints zi = \u2212yiHiw, \u2200i, is essential to\nderive the above simple and elegant Lagrange dual.\nSimple equivalent reformulations of a problem can lead\nto very different dual problems. Without introducing\nnew variables and equality constraints, one would not be\nable to obtain (10). Here we have considered the negative\nmargin zi to be the central objects of study. In [23], a\nsimilar idea has been used to derive different duals\nof kernel methods, which leads to the so-called value\nregularization. We focus on boosting algorithms instead\nof kernel methods in this work. Also note that we would\nhave the following dual if we work directly on the cost\nfunction in (3):\nmax\nr,u\nr\nT \u2212PM\ni=1ui log ui + 1\u22a4u\ns.t. PM\ni=1uiyiHi \u227c\u2212r1\u22a4, u \u227d0.\n(14)\nNo normalization requirement 1\u22a4u = 1 is imposed.\nInstead, 1\u22a4u works as a regularization term. The con-\nnection between AdaBoost and LPBoost is not clear with\nthis dual.\nLagrange duality between problems (5) and (10) as-\nsures that weak duality and strong duality hold. Weak\nduality says that any feasible solution of (10) produces a\nlower bound of the original problem (5). Strong duality\ntells us the optimal value of (10) is the same as the\noptimal value of (5). The weak duality is guaranteed by\nthe Lagrange duality theory. The strong duality holds\nsince the primal problem (5) is a convex problem that\nsatis\ufb01es Slater\u2019s condition [21].\nTo show the connection with LPBoost, we equivalently\nrewrite the above formulation by reversing the sign of r\nand multiplying the cost function with T , (T > 0):\nmin\nr,u r + T PM\ni=1ui log ui\ns.t. PM\ni=1uiyiHij \u2264r (\u2200j = 1, \u00b7 \u00b7 \u00b7 , N),\n(15)\nu \u227d0, 1\u22a4u = 1.\nNote that the constraint u \u227d0 is implicitly enforced\nby the logarithmic function and thus it can be dropped\nwhen one solves (15).\n3.1\nConnection between AdaBoost\u21131 and Gibbs free\nenergy\nGibbs free energy is the chemical potential that is min-\nimized when a system reaches equilibrium at constant\npressure and temperature.\nLet us consider a system that has M states at tem-\nperature T . Each state has energy vi and probability ui\nof likelihood of occurring. The Gibbs free energy of this\nsystem is related with its average energy and entropy,\nnamely:\nG(v, u) = u\u22a4v + T PM\ni=1 ui log ui.\n(16)\nWhen the system reaches equilibrium, G(v, u) is mini-\nmized. So we have\nmin\nu\nG(v, u), s.t. u \u227d0, 1\u22a4u = 1.\n(17)\nThe constraints ensure that u is a probability distribu-\ntion.\nNow let us de\ufb01ne vector vj with its entries being vij =\nyiHij. vij is the energy associated with state i for case\nj. vij can only take discrete binary values +1 or \u22121. We\nrewrite our dual optimization problem (15) into\nmin\nu\nworst case energy vector vj\nz\n}|\n{\nmax\nj {u\u22a4vj}\n+T\nM\nX\ni=1\nui log ui,\ns.t. u \u227d0, 1\u22a4u = 1.\n(18)\nThis can be interpreted as \ufb01nding the minimum Gibbs\nfree energy for the worst case energy vector.\n3.2\nConnection between AdaBoost\u21131 and LPBoost\nFirst let us recall the basic concepts of LPBoost. The idea\nof LPBoost is to maximize the minimum margin because\nit is believed that the minimum margin plays a critically\nimportant role in terms of generalization error [2]. The\nhard-margin LPBoost [9] can be formulated as\nmax\nw\nminimum margin\nz\n}|\n{\nmin\ni {yiHiw}, s.t. w \u227d0, 1\u22a4w = 1.\n(19)\nThis problem can be solved as an LP. Its dual is also an\nLP:\nmin\nr,u\nr\ns.t. PM\ni=1uiyiHij \u2264r (\u2200j = 1, \u00b7 \u00b7 \u00b7 , N),\nu \u227d0, 1\u22a4u = 1.\n(20)\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 32, NO. 12, DEC. 2010\n5\nArc-Gv has been shown asymptotically to a solution of\nthe above LPs [11].\nThe performance deteriorates when no linear combi-\nnation of weak classi\ufb01ers can be found that separates the\ntraining examples. By introducing slack variables, we get\nthe soft-margin LPBoost algorithm\nmax\nw,\u033a,\u03be \u033a \u2212D1\u22a4\u03be\ns.t. yiHiw \u2265\u033a \u2212\u03bei, (\u2200i = 1, \u00b7 \u00b7 \u00b7 , M),\n(21)\nw \u227d0, 1\u22a4w = 1, \u03be \u227d0.\nHere D is a trade-off parameter that controls the balance\nbetween training error and margin maximization. The\ndual of (21) is similar to the hard-margin case except\nthat the dual variable u is capped:\nmin\nr,u\nr\ns.t. PM\ni=1uiyiHij \u2264r (\u2200j = 1, \u00b7 \u00b7 \u00b7 , N),\nD1 \u227du \u227d0, 1\u22a4u = 1.\n(22)\nComparing (15) with hard-margin LPBoost\u2019s dual, it\nis easy to see that the only difference is the entropy\nterm in the cost function. If we set T = 0, (15) re-\nduces to the hard-margin LPBoost. In this sense, we\ncan view AdaBoost\u21131\u2019s dual as entropy regularized hard-\nmargin LPBoost. Since the regularization coef\ufb01cient T is\nalways positive, the effects of the entropy regularization\nterm is to encourage the distribution u as uniform\nas possible (the negative entropy PM\ni=1 ui log ui is the\nKullback-Leibler distance between u and the uniform\ndistribution). This may explain the underlying reason of\nAdaBoost\u2019s success over hard-margin LPBoost: To limit\nthe weight distribution u leads to better generalization\nperformance. But, why and how? We will discover the\nmechanism in Section 3.3.\nWhen the regularization coef\ufb01cient, T , is suf\ufb01ciently\nlarge, the entropy term in the cost function dominates.\nIn this case, all discrete probability ui become almost\nthe same and therefore gather around the center of the\nsimplex {u \u227d0, 1\u22a4u = 1}. As T decreases, the solution\nwill gradually shift to the boundaries of the simplex to\n\ufb01nd the best mixture that best approximates the maxi-\nmum. Therefore, T can be also viewed as a homotopy\nparameter that bridges a maximum entropy problem\nwith uniform distribution ui = 1/M (i = 1, ..., M), to\na solution of the max-min problem (19).\nThis observation is also consistent with the soft-margin\nLPBoost. We know that soft-margin LPBoost often out-\nperforms hard-margin LPBoost [9], [10]. In the primal,\nit is usually explained that the hinge loss of soft-margin\nis more appropriate for classi\ufb01cation. The introduction\nof slack variables in the primal actually results in box\nconstraints on the weight distribution in the dual. In\nother words the \u2113\u221enorm of u, \u2225u\u2225\u221e, is capped. This\ncapping mechanism is harder than the entropy regular-\nization mechanism of AdaBoost\u21131. Nevertheless, both are\nbene\ufb01cial on inseparable data. In [24], it is proved that\nsoft-margin LPBoost actually maximizes the average of\n1/D smallest margins.\nNow let us take a look at the cost function of Ad-\naBoost and LPBoost in the primal. The log-sum-exp cost\nemployed by AdaBoost can be viewed as a smooth\napproximation of the maximum function because of the\nfollowing inequality:\nmax\ni\nai \u2264log\n\u0000PM\ni=1 exp ai\n\u0001\n\u2264max\ni\nai + log M.\nTherefore, LPBoost uses a hard maximum (or minimum)\nfunction while AdaBoost uses a soft approximation of\nthe maximum (minimum) function. We try to explain\nwhy AdaBoost\u2019s soft cost function is better than LP-\nBoost\u2019s3 hard cost function next.\n3.3\nAdaBoost\u21131 controls the margin variance via\nmaximizing the entropy of the weights on the training\nexamples\nIn AdaBoost training, there are two sets of weights: the\nweights of the weak classi\ufb01ers w and the weights on the\ntraining examples u. In the last section, we suppose that\nto limit u is bene\ufb01cial for classi\ufb01cation performance. By\nlooking at the Karush-Kuhn-Tucker (KKT) conditions of\nthe convex program that we have formulated, we are\nable to reveal the relationship between the two sets of\nweights. More precisely, we show how AdaBoost\u21131 (and\nAdaBoost4) controls the margin variance by optimizing\nthe entropy of weights u.\nRecall that we have to introduce new equalities zi =\n\u2212yiHiw, \u2200i in order to obtain the dual (10) (and (15)).\nObviously zi is the negative margin of sample xi. Notice\nthat the Lagrange multiplier u is associated with these\nequalities. Let (w\u22c6, z\u22c6) and (u\u22c6, q\u22c6, r\u22c6) be any primal\nand dual optimal points with zero duality gap. One of\nthe KKT conditions tells us\n\u2207zL(w\u22c6, z\u22c6, u\u22c6, q\u22c6, r\u22c6) = 0.\n(23)\nThe Lagrangian L(\u00b7) is de\ufb01ned in (12). This equation\nfollows\nu\u22c6\ni =\nexp z\u22c6\ni\nPM\ni=1 exp z\u22c6\ni\n, \u2200i = 1, \u00b7 \u00b7 \u00b7 M.\n(24)\nEqu. (24) guarantees that u\u22c6is a probability distribution.\nNote that (24) is actually the same as the update rule\nused in AdaBoost. The optimal value5 of the Lagrange\ndual problem (10), which we denote Opt\u22c6\n(10), equals to\nthe optimal value of the original problem (5) (and (11))\ndue to the strong duality, hence Opt\u22c6\n(5) = Opt\u22c6\n(10).\nFrom (24), at optimality we have\n\u2212z\u22c6\ni = \u2212log u\u22c6\ni \u2212log\n\u0000PM\ni=1 exp z\u22c6\ni\n\u0001\n= \u2212log u\u22c6\ni \u2212Opt\u22c6\n(10)\n= \u2212log u\u22c6\ni \u2212Opt\u22c6\n(5), \u2200i = 1, \u00b7 \u00b7 \u00b7 M.\n(25)\n3. Hereafter, we use LPBoost to denote hard-margin LPBoost unless\notherwise speci\ufb01ed.\n4. We believe that the only difference between AdaBoost\u21131 and\nAdaBoost is on the optimization method employed by each algorithm.\nWe conjecture that some theoretical results on AdaBoost\u21131 derived in\nthis paper may also apply to AdaBoost.\n5. Hereafter we use the symbol Opt\u22c6\n(\u00b7) to denote the optimal value\nof Problem (\u00b7).\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 32, NO. 12, DEC. 2010\n6\nThis equation suggests that, after convergence, the mar-\ngins\u2019 values are determined by the weights on the train-\ning examples u\u22c6and the cost function\u2019s value. From (25),\nthe margin\u2019s variance is entirely determined by u\u22c6:\nvar{\u2212z\u22c6} = var{log u\u22c6} + var{Opt\u22c6\n(5)} = var{log u\u22c6}.\n(26)\nWe now understand the reason why capping u as\nLPBoost does, or uniforming u as AdaBoost does can\nimprove the classi\ufb01cation performance. These two equa-\ntions reveal the important role that the weight distribu-\ntion u plays in AdaBoost. All that we knew previously\nis that the weights on the training examples measure\nhow dif\ufb01cult an individual example can be correctly\nclassi\ufb01ed. In fact, besides that, the weight distribution on\nthe training examples is also a proxy for minimizing the\nmargin\u2019s distribution divergence. From the viewpoint of\noptimization, this is an interesting \ufb01nding. In AdaBoost,\none of the main purposes is to control the divergence\nof the margin distribution, which may not be easy to\noptimize directly because a margin can take a value\nout of the range [0, 1], where entropy is not applicable.\nAdaBoost\u2019s cost function allows one to do so implicitly\nin the primal but explicitly in the dual. A future research\ntopic is to apply this idea to other machine learning\nproblems.\nThe connection between the dual variable u and mar-\ngins tells us that AdaBoost often seems to optimize the\nminimum margin (or average margin? We will answer\nthis question in the next section.) but also it considers\nanother quantity related to the variance of the margins.\nIn the dual problem (15), minimizing the maximum\nedge on the weak classi\ufb01ers contributes to maximizing\nthe margin. At the same time, minimizing the negative\nentropy of weights on training examples contributes to\ncontrolling the margin\u2019s variance. We make this useful\nobservation by examining the dual problem as well as\nthe KKT optimality conditions. But it remains unclear\nabout the exact statistics measures that AdaBoost opti-\nmizes. Next section presents a complete answer to this\nquestion through analyzing AdaBoost\u2019s primal optimiza-\ntion problem.\nWe know that Arc-Gv chooses w in a different way\nfrom AdaBoost. Therefore Arc-Gv optimizes a different\ncost function and does not minimize the negative en-\ntropy of u any more. We expect that AdaBoost will have\na more uniform distribution of u. We run AdaBoost and\nArc-Gv with decision stumps on two datasets breast-\ncancer and australian (all datasets used in this paper are\navailable at [25] unless otherwise speci\ufb01ed). Fig. 1 dis-\nplays the results. AdaBoost indeed has a small negative\nentropy of u in both experiments, which agrees with our\nprediction.\nIt is evident now that AdaBoost\u21131 controls the vari-\nance of margins by regularizing the Shannon entropy of\nthe corresponding dual variable u. For on-line learning\nalgorithms there are two main families of regularization\nstrategies: entropy regularization and regularization us-\n200\n400\n600\n800\n1000\n0.68\n0.7\n0.72\n0.74\n0.76\n0.78\nnumber of iterations\nnegative entropy of u\n200\n400\n600\n800\n1000\n0.9\n0.95\n1\n1.05\nnumber of iterations\nnegative entropy of u\nFig. 1: Negative entropy of u produced by the standard\nAdaBoost and Arc-Gv at each iteration on datasets breast-cancer\nand australian respectively. The negative entropy produced by\nAdaBoost (black) is consistently lower than the one by Arc-Gv\n(blue).\ning squared Euclidean distance. A question that natu-\nrally arises here is: What happens if we use squared\nEuclidean distance to replace the entropy in the dual of\nAdaBoost\u21131 (15)? In other words, can we directly minimize\nthe variance of the dual variable u to achieve the purpose\nof controlling the variance of margins? We answer this\nquestion by having a look at the convex loss functions\nfor classi\ufb01cation.\nFig. 2 plots four popular convex loss functions. It is\nshown in [26] that as the data size increases, practically\nall popular convex loss functions are Bayes-consistent,\nalthough convergence rates and other measures of con-\nsistency may vary. In the context of boosting, AdaBoost,\nLogitBoost and soft-margin LPBoost use exponential\nloss, logistic loss and hinge loss respectively. Here we\nare interested in the squared hinge loss. LogitBoost\nwill be discussed in the next section. As mentioned,\nin theory, there is no particular reason to prefer hinge\nloss to squared hinge loss. Now if squared hinge loss is\nadopted, the cost function of soft-margin LPBoost (21)\nbecomes\nmax\nw,\u033a,\u03be \u033a \u2212D PM\ni=1 \u03be2\ni ,\nand the constraints remain the same as in (21). Its dual\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 32, NO. 12, DEC. 2010\n7\nis easily derived6\nmin\nr,u r +\n1\n4D\nPM\ni=1 u2\ni\ns.t. PM\ni=1uiyiHij \u2264r (\u2200j = 1, \u00b7 \u00b7 \u00b7 , N),\n(27)\nu \u227d0, 1\u22a4u = 1.\nWe can view the above optimization problem as variance\nregularized LPBoost. In short, to minimize the vari-\nance of the dual variable u for controlling the margin\u2019s\nvariance, one can simply replace soft-margin LPBoost\u2019s\nhinge loss with the squared hinge loss. Both the primal\nand dual problems are quadratic programs (QP) and\nhence can be ef\ufb01ciently solved using off-the-shelf QP\nsolvers like MOSEK [27], CPLEX [28].\nActually we can generalize the hinge loss into\n(max{0, 1 \u2212yF(x)})p .\nWhen p \u22651, the loss is convex. p = 1 is the hinge\nloss and p = 2 is the squared hinge loss. If we use a\ngeneralized hinge loss (p > 1) for boosting, we end up\nwith a regularized LPBoost which has the format:\nmin\nr,u r + D1\u2212q(p1\u2212q \u2212p\u2212q)PM\ni=1 uq\ni ,\n(28)\nsubject to the same constraints as in (27). Here p and q\nare dual to each other by 1\np + 1\nq = 1. It is interesting that\n(28) can also be seen as entropy regularized LPBoost;\nmore precisely, Tsallis entropy [29] regularized LPBoost.\nDe\ufb01nition 3.2. (Tsallis entropy) Tsallis entropy is a gen-\neralization of the Shannon entropy, de\ufb01ned as\nSq(u) = 1 \u2212P\ni uq\ni\nq \u22121\n, (u \u227d0, 1\u22a4u = 1).\n(29)\nwhere q is a real number. In the limit as q \u21921, we have\nuq\u22121\ni\n= exp((q \u22121) log ui) \u22431 + (q \u22121) log ui. So S1 =\n\u2212P\ni ui log ui, which is Shannon entropy.\nTsallis\nentropy\n[29]\ncan\nalso\nbe\nviewed\nas\na\nq-\ndeformation of Shannon entropy because Sq(u)\n=\n\u2212P\ni ui logq ui\nwhere\nlogq(u)\n=\nu1\u2212q\u22121\n1\u2212q\nis\nthe\nq-\nlogarithm. Clearly logq(u) \u2192log(u) when q \u21921.\nIn summary, we conclude that although the primal\nproblems of boosting with different loss functions seem\ndissimilar, their corresponding dual problems share the\nsame formulation. Most of them can be interpreted as\nentropy regularized LPBoost. Table 1 summarizes the\nresult. The analysis of LogitBoost will be presented in\nthe next section.\n3.4\nLagrange dual of LogitBoost\nThus far, we have discussed AdaBoost and its relation\nto LPBoost. In this section, we consider LogitBoost [6]\nfrom its dual.\n6. The primal constraint \u03be \u227d0 can be dropped because it is implicitly\nenforced.\nTheorem 3.2. The dual of LogitBoost is a binary relative\nentropy maximization problem, which writes\nmax\nr,u\nr\nT \u2212PM\ni=1 [(\u2212ui) log(\u2212ui) + (1 + ui) log(1 + ui)]\ns.t. PM\ni=1uiyiHij \u2265r (\u2200j = 1, \u00b7 \u00b7 \u00b7 , N).\n(30)\nWe can also rewrite it into an equivalent form:\nmin\nr,u r + T PM\ni=1 [ui log ui + (1 \u2212ui) log(1 \u2212ui)]\ns.t. PM\ni=1uiyiHij \u2264r (\u2200j = 1, \u00b7 \u00b7 \u00b7 , N).\n(31)\nThe proof follows the fact that the conjugate of the\nlogistic loss function logit(x) = log(1 + exp \u2212x) is\nlogit\u2217(u) =\n(\n(\u2212u) log(\u2212u) + (1 + u) log(1 + u), 0 \u2265u \u2265\u22121;\n\u221e,\notherwise,\nwith 0 log 0 = 0. logit\u2217(u) is a convex function in its\ndomain. The corresponding primal is\nmin\nw\nPM\ni=1logit(zi)\ns.t. zi = yiHiw, (\u2200i = 1, \u00b7 \u00b7 \u00b7 , M),\n(32)\nw \u227d0, 1\u22a4w = 1\nT .\nIn (31), the dual variable u has a constraint 1 \u227du \u227d\n0, which is automatically enforced by the logarithmic\nfunction. Another difference of (31) from duals of Ad-\naBoost and LPBoost etc. is that u does not need to be\nnormalized. In other words, in LogitBoost the weight\nassociated with each training sample is not necessarily a\ndistribution. As in (24) for AdaBoost, we can also relate\na dual optimal point u\u22c6and a primal optimal point w\u22c6\n( between (31) and (32) ) by\nu\u22c6\ni =\nexp \u2212z\u22c6\ni\n1 + exp \u2212z\u22c6\ni\n, \u2200i = 1, \u00b7 \u00b7 \u00b7 M.\n(33)\nSo the margin of xi is solely determined by u\u22c6\ni : z\u22c6\ni =\nlog 1\u2212u\u22c6\ni\nu\u22c6\ni , \u2200i. For a positive margin (xi is correctly classi-\n\ufb01ed), we must have u\u22c6\ni < 0.5.\nSimilarly, we can also use CG to solve LogitBoost. As\nshown in Algorithm 2 in the case of AdaBoost, the only\nmodi\ufb01cation is to solve a different dual problem (here\nwe need to solve (31)).\n3.5\nAdaBoost\u21131 approximately maximizes the aver-\nage margin and minimizes the margin variance\nBefore we present our main result, a lemma is needed.\nLemma 3.1. The margin of AdaBoost\u21131 and AdaBoost\nfollows the Gaussian distribution. In general, the larger\nthe number of weak classi\ufb01ers, the more closely does the\nmargin follow the form of Gaussian under the assump-\ntion that selected weak classi\ufb01ers are uncorrelated.\nProof: The central limit theorem [30] states that the\nsum of a set of i.i.d. random variables xi, (i = 1 \u00b7 \u00b7 \u00b7 N)\nis approximately distributed following a Gaussian dis-\ntribution if the random variables have \ufb01nite mean and\nvariance.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 32, NO. 12, DEC. 2010\n8\nTABLE 1: Dual problems of boosting algorithms are entropy regularized LPBoost.\nalgorithm\nloss in primal\nentropy regularized LPBoost in dual\nAdaBoost\nexponential loss\nShannon entropy\nLogitBoost\nlogistic loss\nbinary relative entropy\nsoft-margin \u2113p(p > 1) LPBoost\ngeneralized hinge loss\nTsallis entropy\n\u22122\n\u22121\n0\n1\n2\n0\n0.5\n1\n1.5\n2\n2.5\n3\nmargins\nconvex loss functions\nloss\n \n \nexponential\nlogistic\nhinge\nsquared hinge\nFig. 2: Various loss functions used in classi\ufb01cation. Exponential:\nexp \u2212s; logistic: log(1 + exp \u2212s); hinge: max{0, 1 \u2212s}; squared\nhinge: (max{0, 1 \u2212s})2. Here s = yF(x).\nNote that the central limit theorem applies when each\nvariable xi has an arbitrary probability distribution Qi as\nlong as the mean and variance of Qi are \ufb01nite.\nAs mentioned, the normalized margin of AdaBoost for\ni-th example is de\ufb01ned as\n\u033ai = (yi\nPN\nj=1 hj(xi)wj)/1\u22a4w = \u2212zi/1\u22a4w.\n(34)\nIn the following analysis, we ignore the normalization\nterm 1\u22a4w because it does not have any impact on the\nmargin\u2019s distribution. Hence the margin \u033a is the sum of\nN variables \u02c6wj with \u02c6wj = yihj(xi)wj. It is easy to see\nthat each \u02c6wj follows a discrete distribution with binary\nvalues either wj or \u2212wj. Therefore wj must have \ufb01nite\nmean and variance. Using the central limit theorem, we\nknow that the distribution of \u033ai is a Gaussian.\nIn the case of discrete variables (\u033ai can be discrete), the\nassumption identical distributions can be substantially\nweakened [31]. The generalized central limit theorem\nessentially states that anything that can be thought of as\nbeing made up as the sum of many small independent\nvariables is approximately normally distributed.\nA condition of the central limit theorem is that the N\nvariables must be independent. In the case of the number\nof weak hypotheses is \ufb01nite, as the margin is expressed\nin (34), each hj(\u00b7) is \ufb01xed beforehand, and assume all the\ntraining examples are randomly independently drawn,\nthe variable \u033ai would be independent too. When the\nset of weak hypotheses is in\ufb01nite, it is well known that\nusually AdaBoost selects independent weak classi\ufb01ers\nsuch that each weak classi\ufb01er makes different errors\non the training dataset [15]. In this sense, wj might be\nviewed as roughly independent from each other. More\ndiverse weak classi\ufb01ers will make the selected weak\nclassi\ufb01ers less dependent.7\nHere we give some empirical evidence for approx-\nimate Gaussianity. The normal (Gaussian) probability\nplot is used to visually assess whether the data follow a\nGaussian distribution. If the data are Gaussian the plot\nforms a straight line. Other distribution types introduce\ncurvature in the plot. We run AdaBoost with decision\nstumps on the dataset australian. Fig. 3 shows two\nplots of the margins with 50 and 1100 weak classi\ufb01ers,\nrespectively. We see that with 50 weak classi\ufb01ers, the\nmargin distribution can be reasonably approximated by\na Gaussian; with 1100 classi\ufb01ers, the distribution is very\nclose to a Gaussian. The kurtosis of a 1D data provides\na numerical evaluation of the Gaussianity. We know that\nthe kurtosis of a Gaussian distribution is zero and almost\nall the other distributions have non-zero kurtosis. In\nour experiment, the kurtosis is \u22120.056 for the case with\n50 weak classi\ufb01ers and \u22120.34 for 1100 classi\ufb01ers. Both\nare close to zero, which indicates AdaBoost\u2019s margin\ndistribution can be well approximated by Gaussian.\nTheorem 3.3. AdaBoost\u21131 approximately maximizes the\nunnormalized average margin and at the same time min-\nimizes the variance of the margin distribution under the\nassumption that the margin follows a Gaussian distribu-\ntion.\nProof: From (6) and (34), the cost function that\nAdaBoost\u21131 minimizes is\nfab(w) = log\n\u0010PM\ni=1 exp \u2212\u033ai\nT\n\u0011\n.\n(35)\nAs proved in Lemma 3.1, \u033ai follows a Gaussian\nG(\u033a; \u00af\u033a, \u03c3) =\n1\n\u221a\n2\u03c0\u03c3 exp \u2212(\u033a \u2212\u00af\u033a)2\n2\u03c32\n,\nwith mean \u00af\u033a, variance \u03c32; and PM\ni=1 \u033ai = 1. We assume\nthat the optimal value of the regularization parameter T\nis known a priori.\nThe Monte Carlo integration method can be used to\ncompute a continuous integral\nZ\ng(x)f(x)dx \u22431\nK\nK\nX\nk=1\nf(xk),\n(36)\nwhere g(x) is a probability distribution such that\nR\ng(x)dx = 1 and f(x) is an arbitrary function. xk,\n(k = 1 \u00b7 \u00b7 \u00b7 K), are randomly sampled from the distribu-\ntion g(x). The more samples are used, the more accurate\nthe approximation is.\n7. Nevertheless, this statement is not rigid.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 32, NO. 12, DEC. 2010\n9\n\u22122\n\u22121\n0\n1\n2\n3\n0.001\n0.003\n0.01 \n0.02 \n0.05 \n0.10 \n0.25 \n0.50 \n0.75 \n0.90 \n0.95 \n0.98 \n0.99 \n0.997\n0.999\nmargins\nprobability\nnormal probability plot\n0\n2\n4\n6\n8\n0.001\n0.003\n0.01 \n0.02 \n0.05 \n0.10 \n0.25 \n0.50 \n0.75 \n0.90 \n0.95 \n0.98 \n0.99 \n0.997\n0.999\nmargins\nprobability\nnormal probability plot\nFig. 3: Gaussianity test for the margin distribution with 50 and\n1100 weak classi\ufb01ers, respectively. A Gaussian distribution will\nform a straight line. The dataset used is australian.\n(35) can be viewed as a discrete Monte Carlo approx-\nimation of the following integral (we omit a constant\nterm log M, which is irrelevant to the analysis):\n\u02c6\nfab(w)\n= log\nZ \u033a2\n\u033a1\nG(\u033a; \u00af\u033a, \u03c3) exp\n\u0010\n\u2212\u033a\nT\n\u0011\nd\u033a\n= log\nZ \u033a2\n\u033a1\n1\n\u221a\n2\u03c0\u03c3 exp\n\u0012\n\u2212(\u033a \u2212\u00af\u033a)2\n2\u03c32\n\u2212\u033a\nT\n\u0013\nd\u033a\n= log\n\"\n1\n2 exp\n\u0012\n\u2212\u00af\u033a\nT + \u03c32\n2T 2\n\u0013\nerf\n\u0012\u033a \u2212\u00af\u033a\n\u221a\n2\u03c3 +\n\u03c3\n\u221a\n2T\n\u0013 \f\f\f\f\n\u033a2\n\u033a1\n#\n= \u2212log 2 \u2212\u00af\u033a\nT + \u03c32\n2T 2 + log\n\"\nerf\n\u0012\u033a \u2212\u00af\u033a\n\u221a\n2\u03c3 +\n\u03c3\n\u221a\n2T\n\u0013 \f\f\f\f\n\u033a2\n\u033a1\n#\n,\n(37)\nwhere erf(x)\n=\n2\n\u221a\u03c0\nR x\n0 exp \u2212s2ds is the Gauss error\nfunction. The integral range is [\u033a1, \u033a2]. With no explicit\nknowledge about the integration range, we may roughly\ncalculate the integral from \u2212\u221eto +\u221e. Then the last term\nin (37) is log 2 and the result is analytical and simple\n\u02c6\nfab(w) = \u2212\u00af\u033a\nT + 1\n2\n\u03c32\nT 2 .\n(38)\nThis is a reasonable approximation because Gaussian\ndistributions drop off quickly (Gaussian is not consid-\nered a heavy-tailed distribution). Also this approxima-\ntion implies that we are considering the case that the\nnumber of samples goes to +\u221e.\nConsequently, AdaBoost approximately maximizes the\ncost function\n\u2212\u02c6\nfab(w) = \u00af\u033a\nT \u22121\n2\n\u03c32\nT 2 .\n(39)\nThis cost function has a clear and simple interpretation:\nThe \ufb01rst term \u00af\u033a/T is the unnormalized average margin\nand the second term \u03c32/T 2 is the unnormalized mar-\ngin variance. So AdaBoost maximizes the unnormalized\naverage margin and also takes minimizing the unnor-\nmalized margin variance into account. This way a better\nmargin distribution can be obtained.\nNote that Theorem 3.3 depends on Lemma 3.1 that\ndoes not necessarily hold in practice.\nTheorem 3.3 is an important result in the sense that it\ntries to contribute to the open question why AdaBoost\nworks so well. Much previous work intends to believe\nthat AdaBoost maximizes the minimum margin. We\nhave theoretically shown that AdaBoost\u21131 optimizes the\nentire margin distribution by maximizing the mean and\nminimizing the variance of the margin distribution.\nWe notice that when T \u21920, Theorem 3.3 becomes\ninvalid because the Monte Carlo integration cannot\napproximate the cost function of AdaBoost (35) well.\nIn practice, T cannot approach to zero arbitrarily in\nAdaBoost.\nOne may suspect that Theorem 3.3 contradicts the\nobservation of similarity between LPBoost and AdaBoost\nas shown in Section 3.2. LPBoost maximizes the min-\nimum margin and the dual of AdaBoost is merely an\nentropy regularized LPBoost. At \ufb01rst glance, the dual\nvariable r in (15), (20), and (22) should have the same\nmeaning, i.e., maximum edge, which in turn corresponds\nto the minimum margin in the primal. Why average\nmargin? To answer this question, let us again take a look\nat the optimality conditions. Let us denote the optimal\nvalues of (15) r\u22c6and u\u22c6. At convergence, we have\n1\nT (\u2212r\u22c6+ T PM\ni=1 u\u22c6\ni log u\u22c6\ni ) = Opt\u22c6\n(10) = Opt\u22c6\n(5) = Opt\u22c6\n(6).\nHence, we have\nr\u22c6= T\nM\nX\ni=1\nu\u22c6log u\u22c6\u2212T log\n M\nX\ni=1\nexp \u2212\u033a\u22c6\ni\nT\n!\n,\nwhere \u033a\u22c6\ni is the normalized margin for xi. Clearly this is\nvery different from the optimality conditions of LPBoost,\nwhich shows that r\u22c6is the minimum margin. Only when\nT \u21920, the above relationship reduces to r\u22c6= min\ni {\u033a\u22c6\ni }\u2014\nsame as the case of LPBoost.\n3.6\nAdaBoost-QP: Direct optimization of the margin\nmean and variance using quadratic programming\nThe above analysis suggests that we can directly opti-\nmize the cost function (39). In this section we show that\n(39) is a convex programming (more precisely, quadratic\nprogramming, QP) problem in the variable w if we know\nall the base classi\ufb01ers and hence it can be ef\ufb01ciently\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 32, NO. 12, DEC. 2010\n10\nsolved. Next we formulate the QP problem in detail. We\ncall the proposed algorithm AdaBoost-QP.8\nIn kernel methods like SVMs, the original space X is\nmapped to a feature space F. The mapping function \u03a6(\u00b7)\nis not explicitly computable. It is shown in [14] that in\nboosting, one can think of the mapping function \u03a6(\u00b7)\nbeing explicitly known:\n\u03a6(x) : x 7\u2192[h1(x), \u00b7 \u00b7 \u00b7 , hN(x)]\u22a4,\n(40)\nusing the weak classi\ufb01ers. Therefore, any weak classi-\n\ufb01er set H spans a feature space F. We can design an\nalgorithm that optimizes (39):\nmin\nw\n1\n2w\u22a4Aw \u2212T b\u22a4w,\ns.t. w \u227d0, 1\u22a4w = 1,\n(41)\nwhere b =\n1\nM\nPM\ni=1 yiH\u22a4\ni\n=\n1\nM\nPM\ni=1 yi\u03a6(xi), and A =\n1\nM\nPM\ni=1(yiH\u22a4\ni \u2212b)(yiH\u22a4\ni \u2212b)\u22a4=\n1\nM\nPM\ni=1(yi\u03a6(xi) \u2212\nb)(yi\u03a6(xi)\u2212b)\u22a4.9 Clearly A must be positive semide\ufb01nite\nand this is a standard convex QP problem. The non-\nnegativeness constraint w \u227d0 introduces sparsity as\nin SVMs. Without this constraint, the above QP can\nbe analytically solved using eigenvalue decomposition\u2014\nthe largest eigenvector is the solution. Usually all entries\nof this solution would be active (non-zero values).\nIn the kernel space,\nb\u22a4w =\n1\nM\n\u0010X\nyi=1 \u03a6(xi) \u2212\nX\nyi=\u22121 \u03a6(xi)\n\u0011\u22a4\nw\ncan be viewed as the projected \u21131 norm distance be-\ntween two classes because typically this value is posi-\ntive assuming that each class has the same number of\nexamples. The matrix A approximately plays a role as the\ntotal scatter matrix in kernel linear discriminant analysis\n(LDA). Note that AdaBoost does not take the number of\nexamples in each class into consideration when it mod-\nels the problem. In contrast, LDA (kernel LDA) takes\ntraining example number into consideration. This may\nexplain why an LDA post-processing on AdaBoost gives\na better classi\ufb01cation performance on face detection [33],\nwhich is a highly imbalanced classi\ufb01cation problem. This\nobservation of similarity between AdaBoost and kernel\nLDA may inspire new algorithms. We are also interested\nin developing a CG based algorithm for iteratively gen-\nerating weak classi\ufb01ers.\n3.7\nAdaBoost-CG: Totally corrective AdaBoost us-\ning column generation\nThe number of possible weak classi\ufb01ers may be in-\n\ufb01nitely large. In this case it may be infeasible to solve\nthe optimization exactly. AdaBoost works on the primal\nproblem directly by switching between the estimating\nweak classi\ufb01ers and computing optimal weights in a\ncoordinate descent way. There is another method for\n8. In [32], the authors proposed QPreg-AdaBoost for soft-margin\nAdaBoost learning, which is inspired by SVMs. Their QPreg-AdaBoost\nis completely different from ours.\n9. To show the connection of AdaBoost-QP with kernel methods, we\nhave written \u03a6(xi) = H\u22a4\ni .\nworking out of this problem by using an optimization\ntechnique termed column generation (CG) [10], [34]. CG\nmainly works on the dual problem. The basic concept\nof the CG method is to add one constraint at a time to\nthe dual problem until an optimal solution is identi\ufb01ed.\nMore columns need to be generated and added to the\nproblem to achieve optimality. In the primal space, the\nCG method solves the problem on a subset of variables,\nwhich corresponds to a subset of constraints in the\ndual. When a column is not included in the primal, the\ncorresponding constraint does not appear in the dual.\nThat is to say, a relaxed version of the dual problem is\nsolved. If a constraint absent from the dual problem is\nviolated by the solution to the restricted problem, this\nconstraint needs to be included in the dual problem to\nfurther restrict its feasible region. In our case, instead\nof solving the optimization of AdaBoost directly, one\ncomputes the most violated constraint in (15) iteratively\nfor the current solution and adds this constraint to\nthe optimization problem. In theory, any column that\nviolates dual feasibility can be added. To do so, we need\nto solve the following subproblem:\nh\u2032(\u00b7) = argmax\nh(\u00b7)\nPM\ni=1 uiyih(xi).\n(42)\nThis strategy is exactly the same as the one that stage-\nwise AdaBoost and LPBoost use for generating the\nbest weak classi\ufb01er. That is, to \ufb01nd the weak classi-\n\ufb01er that produces minimum weighted training error.\nPutting all the above analysis together, we summarize\nour AdaBoost-CG in Algorithm 2.\nThe CG optimization (Algorithm 2) is so general that\nit can be applied to all the boosting algorithms consider\nin this paper by solving the corresponding dual. The\nconvergence follows general CG algorithms, which is\neasy to establish. When a new h\u2032(\u00b7) that violates dual\nfeasibility is added, the new optimal value of the dual\nproblem (maximization) would decrease. Accordingly,\nthe optimal value of its primal problem decreases too\nbecause they have the same optimal value due to zero\nduality gap. Moreover the primal cost function is convex,\ntherefore eventually it converges to the global minimum.\nA comment on the last step of Algorithm 2 is that\nwe can get the value of w easily. Primal-dual interior-\npoint (PD-IP) methods work on the primal and dual\nproblems simultaneously and therefore both primal and\ndual variables are available after convergence. We use\nMOSEK [27], which implements PD-IP methods. The\nprimal variable w is obtained for free when solving the\ndual problem (15).\nThe dual subproblem we need to solve has one con-\nstraint added at each iteration. Hence after many itera-\ntions solving the dual problem could become intractable\nin theory. In practice, AdaBoost-CG converges quickly\non our tested datasets. As pointed out in [35], usually\nonly a small number of the added constraints are active\nand those inactive ones may be removed. This strategy\nprevents the dual problem from growing too large.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 32, NO. 12, DEC. 2010\n11\nAdaBoost-CG is totally-corrective in the sense that the\ncoef\ufb01cients of all weak classi\ufb01ers are updated at each\niteration. In [36], an additional correction procedure is\ninserted to AdaBoost\u2019s weak classi\ufb01er selection cycle\nfor achieving totally-correction. The inserted correction\nprocedure aggressively reduces the upper bound of the\ntraining error. Like AdaBoost, it works in the primal.\nIn contrast, our algorithm optimizes the regularized loss\nfunction directly and mainly works in the dual space. In\n[37], a totally-corrective boosting is proposed by optimiz-\ning the entropy, which is inspired by [18]. As discussed,\nno explicit primal-dual connection is established. That is\nwhy an LPBoost procedure is needed over the obtained\nweak classi\ufb01ers in order to calculate the primal variable\nw. In this sense, [37] is also similar to the work of [32].\nThe following diagram summarizes the relationships\nthat we have derived on the boosting algorithms that we\nhave considered.\nAdaBoost\u21131 primal\nAdaBoost-CG\n\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192\nLagrange duality\nAdaBoost\u21131 dual\nTheorem 3.3\n\uf8e6\uf8e6y\nentropy\nx\uf8e6\uf8e6regularization\nAdaBoost-QP\nLPBoost dual\n4\nEXPERIMENTS\nIn this section we provide experimental results to verify\nthe presented theory. We have mainly used decision\nstumps as weak classi\ufb01ers due to its simplicity and\nwell-controlled complexity. In some cases, we have also\nused one of the simplest linear classi\ufb01ers, LDA, as weak\nclassi\ufb01ers. To avoid the singularity problem when solv-\ning LDA, we add a scaled identity matrix 10\u22124I to the\nwithin-class matrix. For the CG optimization framework,\nwe have con\ufb01ned ourself to AdaBoost-CG although the\ntechnique is general and applicable for optimizing other\nboosting algorithms.\n4.1\nAdaBoost-QP\nWe compare AdaBoost-QP against AdaBoost. We have\nused 14 benchmark datasets [25]. Except mushrooms,\nsvmguide1, svmguide3 and w1a, all the other datasets have\nbeen scaled to [\u22121, 1]. We randomly split each dataset\ninto training, cross-validation and test sets at a ratio of\n70 : 15 : 15.\nThe stopping criterion of AdaBoost is determined by\ncross-validation on {600, 800, 1000, 1200, 1500} rounds of\nboosting. For AdaBoost-QP, the best value for the pa-\nrameter T is chosen from { 1\n10, 1\n20, 1\n30, 1\n40, 1\n50,\n1\n100,\n1\n200,\n1\n500}\nby cross-validation. In this experiment, decision stumps\nare used as the weak classi\ufb01er such that the complexity\nof the base classi\ufb01ers is well controlled.\nAdaBoost-QP must access all weak classi\ufb01ers a priori.\nHere we run AdaBoost-QP on the 1500 weak classi\ufb01ers\ngenerated by AdaBoost. Clearly this number of hypothe-\nses may not be optimal. Theoretically the larger the size\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.2\n0.4\n0.6\n0.8\n1\nmargins\ncumulative frequency\n \n \nAdaBoost\nAdaBoost\u2212QP\nArc\u2212Gv\nFig. 4: Cumulative margins for AdaBoost, AdaBoost-QP and\nArc-Gv for the breast cancer dataset using decision stumps.\nOverall, the margin distribution of AdaBoost-QP is the best\nand it has a smallest test error. AdaBoost and Arc-Gv run 600\nrounds of boosting. Test error for AdaBoost, AdaBoost-QP and\nArc-Gv is 0.029, 0.027, 0.058 respectively.\nof the weak classi\ufb01er pool is, the better results AdaBoost-\nQP may produce. Table 2 reports the results. The exper-\niments show that among these 14 datasets, AdaBoost-\nQP outperforms AdaBoost on 9 datasets in terms of\ngeneralization error. On mushrooms, both perform very\nwell. On the other 4 datasets, AdaBoost is better.\nWe have also computed the normalized version of the\ncost function value of (39). In most cases AdaBoost-QP\nhas a larger value. This is not surprising since AdaBoost-\nQP directly maximizes (39) while AdaBoost approxi-\nmately maximizes it. Furthermore, the normalized loss\nfunction value is close to the normalized average margin\nbecause the margin variances for most datasets are very\nsmall compared with their means.\nWe also compute the largest minimum margin and\naverage margin on each dataset. On all the datasets\nAdaBoost has a larger minimum margin than AdaBoost-\nQP, This con\ufb01rms that the minimum margin is not\ncrucial for the generalization error. On the other hand,\nthe average margin produced by AdaBoost-QP, which is\nthe \ufb01rst term of the cost function (39), is consistently\nlarger than the one obtained by AdaBoost. Indirectly,\nwe have shown that a better overall margin distribution\nis more important than the largest minimum margin.\nIn Fig. 4 we plot cumulative margins for AdaBoost-\nQP and AdaBoost on the breast-cancer dataset with\ndecision stumps. We can see that while Arc-Gv has a\nlargest minimum margin, it has a worst margins dis-\ntribution overall. If we examine the average margins,\nAdaBoost-QP is the largest; AdaBoost seconds and Arc-\nGv is least. Clearly a better overall distribution does\nlead to a smaller generalization error. When Arc-Gv and\nAdaBoost run for more rounds, their margin distribu-\ntions seem to converge. That is what we see in Fig. 4.\nThese results agree well with our theoretical analysis\n(Theorem 3.3). Another observation from this experiment\nis that, to achieve the same performance, AdaBoost-QP\ntends to use fewer weak classi\ufb01ers than AdaBoost does.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 32, NO. 12, DEC. 2010\n12\nAlgorithm 2 AdaBoost-CG.\nInput: Training set (xi, yi), i = 1 \u00b7 \u00b7 \u00b7 M; termination threshold \u03b5 > 0; regularization parameter T ; (optional)\nmaximum iteration Nmax.\nInitialization:\n1\n1) N = 0 (no weak classi\ufb01ers selected);\n2) w = 0 (all primal coef\ufb01cients are zeros);\n3) ui =\n1\nM , i = 1 \u00b7 \u00b7 \u00b7 M (uniform dual weights).\nwhile true do\n2\n1) Find a new base h\u2032(\u00b7) by solving Problem (42);\n2) Check for optimal solution:\nif PM\ni=1 uiyih\u2032(xi) < r + \u03b5, then break (problem solved);\n3) Add h\u2032(\u00b7) to the restricted master problem, which corresponds to a new constraint in the dual;\n4) Solve the dual to obtain updated r and ui (i = 1, \u00b7 \u00b7 \u00b7 , M): for AdaBoost, the dual is (15);\n5) N = N + 1 (weak classi\ufb01er count);\n6) (optional) if N \u2265Nmax, then break (maximum iteration reached).\nOutput:\n1) Calculate the primal variable w from the optimality conditions and the last solved dual problem;\n2) The learned classi\ufb01er F(x) = PN\nj=1 wjhj(x).\nWe have also tested AdaBoost-QP on full sets of\nweak classi\ufb01ers because the number of possible decision\nstumps is \ufb01nite (less than (number of features \u22121) \u00d7\n(number of examples)). Table 3 reports the test error of\nAdaBoost-QP on some small datasets. As expected, in\nmost cases, the test error is slightly better than the results\nusing 1500 decision stumps in Table 2; and no signi\ufb01cant\ndifference is observed. This veri\ufb01es the capability of\nAdaBoost-QP for selecting and combining relevant weak\nclassi\ufb01ers.\n4.2\nAdaBoost-CG\nWe run AdaBoost and AdaBoost-CG with decision\nstumps on the datasets of [25]. 70% of examples are used\nfor training; 15% are used for test and the other 15% are\nnot used because we do not do cross-validation here. The\nconvergence threshold for AdaBoost-CG (\u03b5 in Algorithm\n2) is set to 10\u22125. Another important parameter to tune is\nthe regularization parameter T . For the \ufb01rst experiment,\nwe have set it to 1/1\u22a4w where w is obtained by running\nAdaBoost on the same data for 1000 iterations. Also for\nfair comparison, we have deliberately forced AdaBoost-\nCG to run 1000 iterations even if the stopping criterion\nis met. Both test and training results for AdaBoost and\nAdaBoost-CG are reported in Table 4 for a maximum\nnumber of iterations of 100, 500 and 1000.\nAs expected, in terms of test error, no algorithm sta-\ntistically outperforms the other one, since they optimize\nthe same cost function. As we can see, AdaBoost does\nslightly better on 6 datasets. AdaBoost-CG outperforms\nAdaBoost on 7 datasets and on svmguide1, both algori-\nthms perform almost identically. Therefore, empirically\nwe conclude that in therms of generalization capability,\nAdaBoost-CG is the same as the standard AdaBoost.\nHowever, in terms of training error and convergence\nspeed of the training procedure, there is signi\ufb01cant dif-\nference between these two algorithms. Looking at the\nright part of Table 4, we see that the training error of\nAdaBoost-CG is consistently better or no worse than\nAdaBoost on all tested datasets. We have the following\nconclusions.\n\u2022 The convergence speed of AdaBoost-CG is faster\nthan AdaBoost and in many cases, better training\nerror can be achieved. This is because AdaBoost\u2019s\ncoordinate descent nature is slow while AdaBoost-\nCG is totally corrective10. This also means that with\nAdaBoost-CG, we can use fewer weak classi\ufb01ers\nto build a good strong classi\ufb01er. This is desirable\nfor real-time applications like face detection [38], in\nwhich the testing speed is critical.\n\u2022 Our experiments con\ufb01rm that a smaller training\nerror does not necessarily lead to a smaller test\nerror. This has been studied extensively in statis-\ntical learning theory. It is observed that AdaBoost\nsometimes suffers from over\ufb01tting and minimizing\nthe exponential cost function of the margins does\nnot solely determine test error.\nIn the second experiment, we run cross-validation to\nselect the best value for the regularization parameter T ,\nsame as in Section 4.1. Table 5 reports the test errors\non a subset of the datasets. Slightly better results are\nobtained compared with the results in Table 4, which\nuses T determined by AdaBoost.\nWe also use LDA as weak classi\ufb01ers to compare the\nclassi\ufb01cation performance of AdaBoost and AdaBoost-\nCG. The parameter T of AdaBoost-CG is determined\nby cross-validation from { 1\n2,\n1\n5,\n1\n8,\n1\n10,\n1\n12,\n1\n15,\n1\n20,\n1\n30,\n1\n40,\n1\n50,\n1\n70,\n1\n90,\n1\n100,\n1\n120,\n1\n150}. For AdaBoost the smallest\n10. Like LPBoost, at each iteration AdaBoost-CG updates the previ-\nous weak classi\ufb01er weights w.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 32, NO. 12, DEC. 2010\n13\nTABLE 2: Test results of AdaBoost (AB) and AdaBoost-QP (QP). All tests are run 10 times. The mean and standard deviation\nare reported. AdaBoost-QP outperforms AdaBoost on 9 datasets.\ndataset\nalgorithm\ntest error\nminimum margin\naverage margin\naustralian\nAB\n0.153 \u00b1 0.034\n\u22120.012 \u00b1 0.005\n0.082 \u00b1 0.006\nQP\n0.13 \u00b1 0.038\n\u22120.227 \u00b1 0.081\n0.18 \u00b1 0.052\nb-cancer\nAB\n0.041 \u00b1 0.013\n0.048 \u00b1 0.009\n0.209 \u00b1 0.02\nQP\n0.03 \u00b1 0.012\n\u22120.424 \u00b1 0.250\n0.523 \u00b1 0.237\ndiabetes\nAB\n0.270 \u00b1 0.043\n\u22120.038 \u00b1 0.007\n0.055 \u00b1 0.005\nQP\n0.262 \u00b1 0.047\n\u22120.107 \u00b1 0.060\n0.075 \u00b1 0.031\nfourclass\nAB\n0.088 \u00b1 0.032\n\u22120.045 \u00b1 0.012\n0.084 \u00b1 0.009\nQP\n0.095 \u00b1 0.028\n\u22120.211 \u00b1 0.059\n0.128 \u00b1 0.027\ng-numer\nAB\n0.283 \u00b1 0.033\n\u22120.079 \u00b1 0.017\n0.042 \u00b1 0.006\nQP\n0.249 \u00b1 0.033\n\u22120.151 \u00b1 0.058\n0.061 \u00b1 0.020\nheart\nAB\n0.210 \u00b1 0.032\n0.02 \u00b1 0.008\n0.104 \u00b1 0.013\nQP\n0.190 \u00b1 0.058\n\u22120.117 \u00b1 0.066\n0.146 \u00b1 0.059\nionosphere\nAB\n0.121 \u00b1 0.044\n0.101 \u00b1 0.010\n0.165 \u00b1 0.012\nQP\n0.139 \u00b1 0.055\n\u22120.035 \u00b1 0.112\n0.184 \u00b1 0.063\nliver\nAB\n0.321 \u00b1 0.040\n\u22120.012 \u00b1 0.007\n0.055 \u00b1 0.005\nQP\n0.314 \u00b1 0.060\n\u22120.107 \u00b1 0.044\n0.079 \u00b1 0.021\nmushrooms\nAB\n0 \u00b1 0\n0.102 \u00b1 0.001\n0.181 \u00b1 0.001\nQP\n0.005 \u00b1 0.002\n\u22120.134 \u00b1 0.086\n0.221 \u00b1 0.084\nsonar\nAB\n0.145 \u00b1 0.046\n0.156 \u00b1 0.008\n0.202 \u00b1 0.013\nQP\n0.171 \u00b1 0.048\n0.056 \u00b1 0.066\n0.220 \u00b1 0.045\nsplice\nAB\n0.129 \u00b1 0.025\n\u22120.009 \u00b1 0.008\n0.117 \u00b1 0.009\nQP\n0.106 \u00b1 0.029\n\u22120.21 \u00b1 0.037\n0.189 \u00b1 0.02\nsvmguide1\nAB\n0.035 \u00b1 0.009\n\u22120.010 \u00b1 0.008\n0.157 \u00b1 0.016\nQP\n0.040 \u00b1 0.009\n\u22120.439 \u00b1 0.183\n0.445 \u00b1 0.155\nsvmguide3\nAB\n0.172 \u00b1 0.023\n\u22120.011 \u00b1 0.009\n0.052 \u00b1 0.005\nQP\n0.167 \u00b1 0.022\n\u22120.113 \u00b1 0.084\n0.085 \u00b1 0.038\nw1a\nAB\n0.041 \u00b1 0.014\n\u22120.048 \u00b1 0.010\n0.084 \u00b1 0.005\nQP\n0.029 \u00b1 0.009\n\u22120.624 \u00b1 0.38\n0.577 \u00b1 0.363\nTABLE 3: Test results of AdaBoost-QP on full sets of decision stumps. All tests are run 10 times.\ndataset\naustralian\nb-cancer\nfourclass\ng-numer\nheart\nliver\nmushroom\nsplice\ntest error\n0.131 \u00b1 0.041\n0.03 \u00b1 0.011\n0.091 \u00b1 0.02\n0.243 \u00b1 0.026\n0.188 \u00b1 0.058\n0.319 \u00b1 0.05\n0.003 \u00b1 0.001\n0.097 \u00b1 0.02\n10\n0\n10\n1\n10\n2\n10\n3\n0\n0.05\n0.1\n0.15\n0.2\nnumber of iterations\nerror\n \n \nCG train\nAB train\nCG test\nAB test\n10\n0\n10\n1\n10\n2\n10\n3\n\u22120.01\n0\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\nnumber of iterations\nerror\n \n \nCG train\nAB train\nCG test\nAB test\n10\n0\n10\n1\n10\n2\n10\n3\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\nnumber of iterations\nerror\n \n \nCG train\nAB train\nCG test\nAB test\n10\n0\n10\n1\n10\n2\n10\n3\n0\n0.05\n0.1\n0.15\n0.2\n0.25\nnumber of iterations\nerror\n \n \nCG train\nAB train\nCG test\nAB test\n10\n0\n10\n1\n10\n2\n10\n3\n0\n0.05\n0.1\n0.15\n0.2\nnumber of iterations\nerror\n \n \nCG train\nAB train\nCG test\nAB test\n10\n0\n10\n1\n10\n2\n10\n3\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\nnumber of iterations\nerror\n \n \nCG train\nAB train\nCG test\nAB test\nFig. 5: Test error and training error of AdaBoost, AdaBoost-CG for australian, breast-cancer, diabetes, heart, spline and svmguide3\ndatasets. These convergence curves correspond to the results in Table 4. The x-axis is on a logarithmic scale for easier comparison.\ntest error from 100, 500 and 1000 runs is reported. We\nshow the results in Table 6. As we can see, the test\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 32, NO. 12, DEC. 2010\n14\nTABLE 4: Test and training errors of AdaBoost (AB) and AdaBoost-CG (CG). All tests are run 5 times. The mean and standard\ndeviation are reported. Weak classi\ufb01ers are decision stumps.\ndataset\nalgorithm\ntest error 100\ntest error 500\ntest error 1000\ntrain error 100\ntrain error 500\ntrain error 1000\naustralian\nAB\n0.146 \u00b1 0.028\n0.165 \u00b1 0.018\n0.163 \u00b1 0.021\n0.091 \u00b1 0.013\n0.039 \u00b1 0.011\n0.013 \u00b1 0.009\nCG\n0.177 \u00b1 0.025\n0.167 \u00b1 0.023\n0.167 \u00b1 0.023\n0.013 \u00b1 0.008\n0.011 \u00b1 0.007\n0.011 \u00b1 0.007\nb-cancer\nAB\n0.041 \u00b1 0.026\n0.045 \u00b1 0.030\n0.047 \u00b1 0.032\n0.008 \u00b1 0.006\n0 \u00b1 0\n0 \u00b1 0\nCG\n0.049 \u00b1 0.033\n0.049 \u00b1 0.033\n0.049 \u00b1 0.033\n0 \u00b1 0\n0 \u00b1 0\n0 \u00b1 0\ndiabetes\nAB\n0.254 \u00b1 0.024\n0.263 \u00b1 0.028\n0.257 \u00b1 0.041\n0.171 \u00b1 0.012\n0.120 \u00b1 0.007\n0.082 \u00b1 0.006\nCG\n0.270 \u00b1 0.047\n0.254 \u00b1 0.026\n0.254 \u00b1 0.026\n0.083 \u00b1 0.008\n0.070 \u00b1 0.007\n0.070 \u00b1 0.007\nfourclass\nAB\n0.106 \u00b1 0.047\n0.097 \u00b1 0.034\n0.091 \u00b1 0.031\n0.072 \u00b1 0.023\n0.053 \u00b1 0.017\n0.046 \u00b1 0.017\nCG\n0.082 \u00b1 0.031\n0.082 \u00b1 0.031\n0.082 \u00b1 0.031\n0.042 \u00b1 0.015\n0.042 \u00b1 0.015\n0.042 \u00b1 0.015\ng-numer\nAB\n0.279 \u00b1 0.043\n0.288 \u00b1 0.048\n0.297 \u00b1 0.051\n0.206 \u00b1 0.047\n0.167 \u00b1 0.072\n0.155 \u00b1 0.082\nCG\n0.269 \u00b1 0.040\n0.262 \u00b1 0.045\n0.262 \u00b1 0.045\n0.142 \u00b1 0.077\n0.142 \u00b1 0.077\n0.142 \u00b1 0.077\nheart\nAB\n0.175 \u00b1 0.073\n0.175 \u00b1 0.088\n0.165 \u00b1 0.076\n0.049 \u00b1 0.022\n0 \u00b1 0\n0 \u00b1 0\nCG\n0.165 \u00b1 0.072\n0.165 \u00b1 0.072\n0.165 \u00b1 0.072\n0 \u00b1 0\n0 \u00b1 0\n0 \u00b1 0\nionosphere\nAB\n0.092 \u00b1 0.016\n0.104 \u00b1 0.017\n0.100 \u00b1 0.016\n0 \u00b1 0\n0 \u00b1 0\n0 \u00b1 0\nCG\n0.131 \u00b1 0.034\n0.131 \u00b1 0.034\n0.131 \u00b1 0.034\n0 \u00b1 0\n0 \u00b1 0\n0 \u00b1 0\nliver\nAB\n0.288 \u00b1 0.101\n0.265 \u00b1 0.081\n0.281 \u00b1 0.062\n0.144 \u00b1 0.018\n0.063 \u00b1 0.015\n0.020 \u00b1 0.015\nCG\n0.288 \u00b1 0.084\n0.288 \u00b1 0.084\n0.288 \u00b1 0.084\n0.017 \u00b1 0.012\n0.017 \u00b1 0.011\n0.017 \u00b1 0.011\nmushrooms\nAB\n0 \u00b1 0.001\n0 \u00b1 0.001\n0 \u00b1 0\n0 \u00b1 0\n0 \u00b1 0\n0 \u00b1 0\nCG\n0 \u00b1 0\n0 \u00b1 0\n0 \u00b1 0\n0 \u00b1 0\n0 \u00b1 0\n0 \u00b1 0\nsonar\nAB\n0.206 \u00b1 0.087\n0.213 \u00b1 0.071\n0.206 \u00b1 0.059\n0 \u00b1 0\n0 \u00b1 0\n0 \u00b1 0\nCG\n0.232 \u00b1 0.053\n0.245 \u00b1 0.078\n0.245 \u00b1 0.078\n0 \u00b1 0\n0 \u00b1 0\n0 \u00b1 0\nsplice\nAB\n0.129 \u00b1 0.011\n0.143 \u00b1 0.026\n0.143 \u00b1 0.020\n0.053 \u00b1 0.003\n0.008 \u00b1 0.006\n0.001 \u00b1 0.001\nCG\n0.161 \u00b1 0.033\n0.151 \u00b1 0.023\n0.151 \u00b1 0.023\n0.002 \u00b1 0.002\n0.001 \u00b1 0.002\n0.001 \u00b1 0.002\nsvmguide1\nAB\n0.036 \u00b1 0.012\n0.034 \u00b1 0.008\n0.037 \u00b1 0.007\n0.022 \u00b1 0.002\n0.009 \u00b1 0.002\n0.002 \u00b1 0.001\nCG\n0.037 \u00b1 0.007\n0.037 \u00b1 0.007\n0.037 \u00b1 0.007\n0.001 \u00b1 0.001\n0 \u00b1 0.001\n0 \u00b1 0.001\nsvmguide3\nAB\n0.184 \u00b1 0.037\n0.183 \u00b1 0.044\n0.182 \u00b1 0.031\n0.112 \u00b1 0.009\n0.037 \u00b1 0.004\n0.009 \u00b1 0.003\nCG\n0.184 \u00b1 0.026\n0.171 \u00b1 0.023\n0.171 \u00b1 0.023\n0.033 \u00b1 0.012\n0.023 \u00b1 0.016\n0.023 \u00b1 0.016\nw1a\nAB\n0.051 \u00b1 0.009\n0.038 \u00b1 0.005\n0.036 \u00b1 0.004\n0.045 \u00b1 0.008\n0.028 \u00b1 0.005\n0.025 \u00b1 0.005\nCG\n0.018 \u00b1 0.001\n0.018 \u00b1 0.001\n0.018 \u00b1 0.001\n0.010 \u00b1 0.004\n0.010 \u00b1 0.004\n0.010 \u00b1 0.004\nTABLE 5: Test error of AdaBoost-CG with decision stumps, using cross-validation to select the optimal T . All tests are run 5\ntimes.\ndataset\naustralian\nb-cancer\ndiabetes\nfourclass\nheart\nionosphere\nsonar\nsplice\ntest error\n0.146 \u00b1 0.027\n0.033 \u00b1 0.033\n0.266 \u00b1 0.036\n0.086 \u00b1 0.027\n0.17 \u00b1 0.082\n0.115 \u00b1 0.024\n0.2 \u00b1 0.035\n0.135 \u00b1 0.015\nerror is slightly better than with decision stumps for\nboth AdaBoost and AdaBoost-CG. Again, AdaBoost and\nAdaBoost-CG\u2019s performances are very similar.\nIn order to show that statistically there are no dif-\nference between AdaBoost-CG and AdaBoost, the Mc-\nNemar test [39] with the signi\ufb01cance level of 0.05 is\nconducted. McNemar\u2019s test is based on a \u03c72 test [39].\nIf the quantity of the \u03c72 test is not greater than \u03c72\n1,0.95 =\n3.841459, we can think of that the two tested classi\ufb01ers\nhave no statistical difference in terms of classi\ufb01cation\ncapability. On the 8 datasets with decision stumps and\nLDA (Tables 5 and 6), in all cases (5 runs per dataset), the\nresults of \u03c72 test are smaller than \u03c72\n1,0.95. Consequently,\nwe can conclude that indeed AdaBoost-CG performs\nvery similarly to AdaBoost for classi\ufb01cation.\nTo examine the effect of parameter T , we run more\nexperiments with various T on the banana dataset (2D\narti\ufb01cial data) that was used in [32]. We still use deci-\nsion stumps. The maximum iteration is set to 400. All\nruns stop earlier than 100 iterations. Table 7 reports the\nresults. Indeed, the training error depends on T . T also\nhas in\ufb02uence on the convergence speed. But, in a wide\nrange of T , the test error does not change signi\ufb01cantly.\nWe do not have a sophisticated technique to tune T . As\nmentioned, the sum of w from a run of AdaBoost can\nserve as a heuristic.\nNow let us take a close look at the convergence behav-\nior of AdaBoost-CG. Fig. 5 shows the test and training\nerror of AdaBoost and AdaBoost-CG for 6 datasets.\nWe see that AdaBoost-CG converges much faster than\nAdaBoost in terms of number of iterations. On most\ntested datasets, AdaBoost-CG is around 10 times faster\nthan AdaBoost. The test error for these two methods\nare very close upon convergence. In some datasets such\nas australian and breast-cancer we observe over-\ufb01tting for\nAdaBoost.\n4.3\nLogitBoost-CG\nWe have also run LogitBoost-CG on the same datasets.\nAll the settings are the same as in the case of AdaBoost-\nCG. The weak classi\ufb01ers are decision stumps. Table 8\nreports the experiment results. Compared to Table 5,\nvery similar results have been observed. No one achieves\nbetter results over the other one on all the datasets.\n5\nDISCUSSION AND CONCLUSION\nIn this paper, we have shown that the Lagrange dual\nproblems of AdaBoost, LogitBoost and soft-margin LP-\nBoost with generalized hinge loss are all entropy regu-\nlarized LPBoost. We both theoretically and empirically\ndemonstrate that the success of AdaBoost relies on\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 32, NO. 12, DEC. 2010\n15\nTABLE 6: Test error of AdaBoost (AB) and AdaBoost-CG (CG) with LDA as weak classi\ufb01ers, using cross-validation to select the\noptimal T . All tests are run 5 times.\ndataset\naustralian\nb-cancer\ndiabetes\nfourclass\nheart\nionosphere\nsonar\nsplice\nAB\n0.150 \u00b1 0.044\n0.029 \u00b1 0.014\n0.259 \u00b1 0.021\n0.003 \u00b1 0.004\n0.16 \u00b1 0.055\n0.108 \u00b1 0.060\n0.297 \u00b1 0.080\n0.215 \u00b1 0.027\nCG\n0.151 \u00b1 0.053\n0.035 \u00b1 0.016\n0.249 \u00b1 0.038\n0.022 \u00b1 0.015\n0.185 \u00b1 0.038\n0.104 \u00b1 0.062\n0.258 \u00b1 0.085\n0.235 \u00b1 0.035\nTABLE 7: AdaBoost-CG on banana dataset with decision\nstumps and LDA as weak classi\ufb01ers. Experiments are run 50\ntimes.\n1\nT\ntest (stumps)\ntrain (stumps)\ntest (LDA)\ntrain (LDA)\n20\n0.298 \u00b1 0.018\n0.150 \u00b1 0.019\n0.134 \u00b1 0.012\n0.032 \u00b1 0.007\n40\n0.309 \u00b1 0.019\n0.101 \u00b1 0.015\n0.135 \u00b1 0.008\n0.001 \u00b1 0.002\n80\n0.313 \u00b1 0.019\n0.033 \u00b1 0.011\n0.136 \u00b1 0.007\n0 \u00b1 0\nmaintaining a better margin distribution. Based on the\ndual formulation, a general column generation based\noptimization framework is proposed. This optimization\nframework can be applied to solve all the boosting\nalgorithms with various loss functions mentioned in this\npaper. Experiments with exponential loss show that the\nclassi\ufb01cation performance of AdaBoost-CG is statistically\nalmost identical to the standard stage-wise AdaBoost on\nreal datasets. In fact, since both algorithms optimize the\nsame cost function, we would be surprised to see a sig-\nni\ufb01cant different in their generalization error. The main\nadvantage of the proposed algorithms is signi\ufb01cantly\nfaster convergence speed.\nCompared with the conventional AdaBoost, a draw-\nback of AdaBoost-CG is the introduction of a parameter,\nsame as in LPBoost. While one can argue that AdaBoost\nimplicitly determines this same parameter by select-\ning how many iterations to run, the stopping criterion\nis nested and thus ef\ufb01cient to learn. In the case of\nAdaBoost-CG, it is not clear how to ef\ufb01ciently learn\nthis parameter. Currently, one has to run the training\nprocedure multiple times for cross validation.\nWith the optimization framework established here,\nsome issues on boosting that are previously unclear may\nbecome obvious now. For example, for designing cost-\nsensitive boosting or boosting on uneven datasets, one\ncan simply modify the primal cost function (5) to have\na weighted cost function [40]. The training procedure\nfollows AdaBoost-CG.\nTo summarize, the convex duality of boosting algo-\nrithms presented in this work generalizes the convex\nduality in LPBoost. We have shown some interesting\nproperties that the derived dual formation possesses.\nThe duality also leads to new ef\ufb01cient learning algori-\nthms. The duality provides useful insights on boosting\nthat may lack in existing interpretations [2], [6].\nIn the future, we want to extend our work to boosting\nwith non-convex loss functions such as BrownBoost [41].\nAlso it should be straightforward to optimize boosting\nfor regression using column generation. We are currently\nexploring the application of AdaBoost-CG to ef\ufb01cient\nobject detection due to its faster convergence, which is\nmore promising for feature selection [38].\nAPPENDIX A\nDESCRIPTION OF DATASETS\nTable 9 provides a description of the datasets we have\nused in the experiments.\nACKNOWLEDGMENTS\nNICTA is funded by the Australian Government as\nrepresented by the Department of Broadband, Commu-\nnications and the Digital Economy and the Australian\nResearch Council through the ICT Center of Excellence\nprogram.\nThe authors thank Sebastian Nowozin for helpful\ndiscussions.\nREFERENCES\n[1]\nY. Freund and R. E. Schapire, \u201cA decision-theoretic generalization\nof on-line learning and an application to boosting,\u201d J. Comp. &\nSyst. Sci., vol. 55, no. 1, pp. 119\u2013139, 1997.\n[2]\nR. E. Schapire, Y. Freund, P. Bartlett, and W. S. Lee, \u201cBoosting\nthe margin: A new explanation for the effectiveness of voting\nmethods,\u201d Ann. Statist., vol. 26, no. 5, pp. 1651\u20131686, 1998.\n[3]\nC. Rudin, I. Daubechies, and R. E. Schapire, \u201cThe dynamics of\nAdaBoost: Cyclic behavior and convergence of margins,\u201d J. Mach.\nLearn. Res., vol. 5, pp. 1557\u20131595, 2004.\n[4]\nC. Rudin, R. E. Schapire, and I. Daubechies, \u201cAnalysis of boosting\nalgorithms using the smooth margin function,\u201d Ann. Statist., vol.\n35, no. 6, pp. 2723\u20132768, 2007.\n[5]\nD. Mease and A. Wyner, \u201cEvidence contrary to the statistical view\nof boosting,\u201d J. Mach. Learn. Res., vol. 9, pp. 131\u2013156, 2008.\n[6]\nJ. Friedman, T. Hastie, and R. Tibshirani,\n\u201cAdditive logistic\nregression: a statistical view of boosting (with discussion and a\nrejoinder by the authors),\u201d Ann. Statist., vol. 28, no. 2, pp. 337\u2013407,\n2000.\n[7]\nC. Domingo and O. Watanabe, \u201cMadaBoost: A modi\ufb01cation of\nAdaBoost,\u201d in Proc. Annual Conf. Learn. Theory. 2000, pp. 180\u2013189,\nMorgan Kaufmann.\n[8]\nG. R\u00a8atsch and M. K. Warmuth, \u201cEf\ufb01cient margin maximizing with\nboosting,\u201d J. Mach. Learn. Res., vol. 6, pp. 2131\u20132152, 2005.\n[9]\nA. J. Grove and D. Schuurmans, \u201cBoosting in the limit: maxi-\nmizing the margin of learned ensembles,\u201d in Proc. National Conf.\nArti\ufb01cial Intell., Madison, Wisconsin, USA, 1998, pp. 692\u2013699.\n[10] A. Demiriz, K.P. Bennett, and J. Shawe-Taylor, \u201cLinear program-\nming boosting via column generation,\u201d Mach. Learn., vol. 46, no.\n1-3, pp. 225\u2013254, 2002.\n[11] L. Breiman, \u201cPrediction games and arcing algorithms,\u201d Neural\nComp., vol. 11, no. 7, pp. 1493\u20131517, 1999.\n[12] L. Reyzin and R. E. Schapire,\n\u201cHow boosting the margin can\nalso boost classi\ufb01er complexity,\u201d in Proc. Int. Conf. Mach. Learn.,\nPittsburgh, Pennsylvania, USA, 2006.\n[13] V. Koltchinskii and D. Panchenko, \u201cEmpirical margin distribu-\ntions and bounding the generalization error of combined classi-\n\ufb01ers,\u201d Ann. Statist., vol. 30, no. 1, pp. 1\u201350, 2002.\n[14] G. R\u00a8atsch, S. Mika, B. Sch\u00a8olkopf, and K.-R. M\u00a8uller, \u201cConstructing\nboosting algorithms from SVMs: An application to one-class\nclassi\ufb01cation,\u201d IEEE Trans. Pattern Anal. Mach. Intell., vol. 24, no.\n9, pp. 1184\u20131199, 2002.\n[15] R. Meir and G. R\u00a8atsch, An introduction to boosting and leveraging,\npp. 118\u2013183, Advanced lectures on machine learning. Springer-\nVerlag, New York, NY, USA, 2003.\n[16] R. E. Schapire,\nThe boosting approach to machine learning: An\noverview, pp. 149\u2013172, Nonlinear Estimation and Classi\ufb01cation.\nSpringer, 2003.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 32, NO. 12, DEC. 2010\n16\nTABLE 8: Test error of LogitBoost-CG with decision stumps, using cross-validation to select the optimal T . All tests are run 5\ntimes.\ndataset\naustralian\nb-cancer\ndiabetes\nfourclass\nheart\nionosphere\nsonar\nsplice\ntest error\n0.13 \u00b1 0.043\n0.039 \u00b1 0.012\n0.238 \u00b1 0.057\n0.071 \u00b1 0.034\n0.14 \u00b1 0.095\n0.2 \u00b1 0.069\n0.169 \u00b1 0.05\n0.104 \u00b1 0.021\nTABLE 9: Description of the datasets. Except mushrooms, svmguide1, svmguide3 and w1a, all the other datasets have been scaled\nto [\u22121, 1].\ndataset\n# examples\n# features\ndataset\n# examples\n# features\naustralian\n690\n14\nliver-disorders\n345\n6\nbreast-cancer\n683\n10\nmushrooms\n8124\n112\ndiabetes\n768\n8\nsonar\n208\n60\nfourclass\n862\n2\nsplice\n1000\n60\ngerman-numer\n1000\n24\nsvmguide1\n3089\n4\nheart\n270\n13\nsvmguide3\n1243\n22\nionosphere\n351\n34\nw1a\n2477\n300\n[17] M. Collins, R. E. Schapire, and Y. Singer,\n\u201cLogistic regression,\nAdaBoost and Bregman distances,\u201d Mach. Learn., vol. 48, no. 1-3,\npp. 253\u2013285, 2002.\n[18] J. Kivinen and M. K. Warmuth, \u201cBoosting as entropy projection,\u201d\nin Proc. Annual Conf. Learn. Theory, Santa Cruz, California, US,\n1999, pp. 134\u2013144, ACM.\n[19] G. Lebanon and J. Lafferty, \u201cBoosting and maximum likelihood\nfor exponential models,\u201d in Proc. Adv. Neural Inf. Process. Syst.\n2001, pp. 447\u2013454, MIT Press.\n[20] L. Mason, J. Baxter, P. Bartlett, and M. Frean, Functional gradient\ntechniques for combining hypotheses, chapter 12, pp. 221\u2013247, Ad-\nvances in Large Margin Classi\ufb01ers. MIT press, 1999.\n[21] S. Boyd and L. Vandenberghe, Convex Optimization, Cambridge\nUniversity Press, 2004.\n[22] Y. Freund and R. E. Schapire,\n\u201cAdaptive game playing using\nmultiplicative weights,\u201d Games & Economic Behavior, vol. 29, pp.\n79\u2013103, 1999.\n[23] R. M. Rifkin and R. A. Lippert, \u201cValue regularization and Fenchel\nduality,\u201d J. Mach. Learn. Res., vol. 8, pp. 441\u2013479, 2007.\n[24] S. Shalev-Shwartz and Y. Singer, \u201cOn the equivalence of weak\nlearnability and linear separability: New relaxations and ef\ufb01cient\nboosting algorithms,\u201d in Proc. Annual Conf. Learn. Theory, Helsinki,\nFinland, 2008.\n[25] C.-C.\nChang\nand\nC.-J.\nLin,\n\u201cLIBSVM:\na\nli-\nbrary\nfor\nsupport\nvector\nmachines,\u201d\n2001,\nhttp://www.csie.ntu.edu.tw/cjlin/libsvmtools/datasets/.\n[26] P. Bartlett, M. Jordan, and J. McAuliffe, \u201cConvexity, classi\ufb01cation,\nand risk bounds,\u201d J. Amer. Stat. Assoc., vol. 101, no. 473, pp. 138\u2013\n156, 2004.\n[27] MOSEK ApS,\n\u201cThe MOSEK optimization toolbox for matlab\nmanual, version 5.0, revision 93,\u201d 2008, http://www.mosek.com/.\n[28] ILOG,\nInc.,\n\u201cCPLEX\n11.1,\u201d\n2008,\nhttp://www.ilog.com/products/cplex/.\n[29] C. Tsallis, \u201cPossible generalization of Boltzmann-Gibbs statistics,\u201d\nJ. Stat. Physics, vol. 52, pp. 479\u2013487, 1988.\n[30] O. Kallenberg, Foundations of Modern Probability, Springer-Verlag,\n1997.\n[31] W. Feller, Introduction to Probability Theory and its Applications, 3rd\nEd., vol. 1, John Wiley & Sons, 1968.\n[32] G.\nR\u00a8atsch,\nT.\nOnoda,\nand\nK.-R.\nM\u00a8uller,\n\u201cSoft\nmargins\nfor\nAdaBoost,\u201d\nMach.\nLearn.,\nvol.\n42,\nno.\n3,\npp.\n287\u2013320,\n2001,\ndata\nsets\nare\navailable\nat\nhttp://theoval.cmp.uea.ac.uk/\u223cgcc/matlab/index.shtml.\n[33] J. Wu, M. D. Mullin, and J. M. Rehg, \u201cLinear asymmetric classi\ufb01er\nfor cascade detectors,\u201d\nin Proc. Int. Conf. Mach. Learn., Bonn,\nGermany, 2005, pp. 988\u2013995.\n[34] M. E. L\u00a8ubbecke and J. Desrosiers,\n\u201cSelected topics in column\ngeneration,\u201d Operation Res., vol. 53, no. 6, pp. 1007\u20131023, 2005.\n[35] S. Sonnenburg, G. R\u00a8atsch, C. Sch\u00a8afer, and B. Sch\u00a8olkopf, \u201cLarge\nscale multiple kernel learning,\u201d J. Mach. Learn. Res., vol. 7, pp.\n1531\u20131565, 2006.\n[36] J. \u02d8Sochman and J. Malas,\n\u201cAdaBoost with totally corrective\nupdates for fast face detection,\u201d in Proc. IEEE Int. Conf. Automatic\nFace & Gesture Recogn., Seoul, Korea, 2004, pp. 445\u2013450.\n[37] M. K. Warmuth, J. Liao, and G. R\u00a8atsch,\n\u201cTotally corrective\nboosting algorithms that maximize the margin,\u201d in Proc. Int. Conf.\nMach. Learn., Pittsburgh, Pennsylvania, 2006, pp. 1001\u20131008.\n[38] P. Viola and M. J. Jones, \u201cRobust real-time face detection,\u201d Int. J.\nComp. Vis., vol. 57, no. 2, pp. 137\u2013154, 2004.\n[39] T. G. Dietterich,\n\u201cApproximate statistical tests for comparing\nsupervised classi\ufb01cation learning algorithms,\u201d Neural Comp., vol.\n10, no. 7, pp. 1895\u20131923, 1998.\n[40] J. Leskovec, \u201cLinear programming boosting for uneven datasets,\u201d\nin Proc. Int. Conf. Mach. Learn., 2003, pp. 456\u2013463.\n[41] Y. Freund,\n\u201cAn adaptive version of the boost by majority\nalgorithm,\u201d Mach. Learn., vol. 43, no. 3, pp. 293\u2013318, 2001.\n",
        "sentence": " Inspired by the general boosting framework of [7], they implemented multi-class boosting with the column generation technique. Since our StructBoost builds upon the fully corrective boosting of Shen and Li [7], it inherits the desirable properties of column generation based boosting, such as a fast convergence rate and a clear explanation from the primal-dual convex optimization perspective. It therefore builds upon the work of column generation boosting [7] and the direct formulation for multi-class boosting [11]. Second, MMPBoost is based on the idea of gradient descent boosting [18], and our StructBoost is built upon fully corrective boosting of Shen and Li [7]. For the time being, let us put aside the difficulty of the large number of constraints, and focus on how to iteratively solve for w using column generation as in [7], [9]. With the primal-dual pair of (3) and (5) and following the general framework of column generation based boosting [7], [9], we can obtain our StructBoost as follows: Iterate the following three steps until converge:",
        "context": "4) Furthermore, based on the duals we derive, we\ndesign column generation based optimization tech-\nniques for boosting learning. We show that the new\nalgorithms have almost identical results to that of\nstandard stage-wise additive boosting algorithms\ndual formulation, a general column generation based\noptimization framework is proposed. This optimization\nframework can be applied to solve all the boosting\nalgorithms with various loss functions mentioned in this\n[10] A. Demiriz, K.P. Bennett, and J. Shawe-Taylor, \u201cLinear program-\nming boosting via column generation,\u201d Mach. Learn., vol. 46, no.\n1-3, pp. 225\u2013254, 2002.\n[11] L. Breiman, \u201cPrediction games and arcing algorithms,\u201d Neural"
    },
    {
        "title": "Training linear SVMs in linear time",
        "author": [
            "T. Joachims"
        ],
        "venue": "Proc. ACM SIGKDD Int. Conf. Knowledge discovery & data mining, 2006, pp. 217\u2013226.",
        "citeRegEx": "8",
        "shortCiteRegEx": null,
        "year": 2006,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " \u2022 To implement StructBoost, we adapt the efficient cutting-plane method\u2014originally designed for efficient linear SVM training [8]\u2014for our purpose. SSVM achieves so based on the joint feature maps over the input-output pairs, where features can be represented equivalently as in CRF [8]. Inspired by the cutting-plane method for fast training of linear SVM [8], we can equivalently rewrite the above problem into a \u201c1-slack\u201d form so that an efficient cuttingplane method can be employed to solve the optimization Proof: The proof adapts the proof in [8]. As demonstrated in [8], cutting-plane methods can be used to solve the 1-slack primal problem (7) efficiently. 2 Cutting-plane optimization for solving the 1-slack primal Despite the extra nonnegative-ness constraint w \u2265 0 in our case, it is easy to modify the cutting-plane method in [8] for solving our problem (7). For the analysis of the cuttingplane method for optimizing the 1-slack primal, readers may refer to [8] for details.",
        "context": null
    },
    {
        "title": "Linear programming boosting via column generation",
        "author": [
            "A. Demiriz",
            "K.P. Bennett",
            "J. Shawe-Taylor"
        ],
        "venue": "Mach. Learn., vol. 46, no. 1-3, pp. 225\u2013254, 2002.",
        "citeRegEx": "9",
        "shortCiteRegEx": null,
        "year": 2002,
        "abstract": "",
        "full_text": "",
        "sentence": " We demonstrate that even conventional LPBoost [9] can benefit from this reformulation to gain significant speedup in training. For the time being, let us put aside the difficulty of the large number of constraints, and focus on how to iteratively solve for w using column generation as in [7], [9]. With the primal-dual pair of (3) and (5) and following the general framework of column generation based boosting [7], [9], we can obtain our StructBoost as follows: Iterate the following three steps until converge: We show in the experiments that at each iteration of LPBoost, solving (12) is much faster than solving the m-slack primal or dual as shown in [9]. We run experiments on some UCI machine learning datasets to compare our StructBoost formulation of binary boosting against the standard LPBoost [9]. We compare the 1-slack StructBoost formulation of binary boosting agaisnt standard LPBoost [9] (i.",
        "context": null
    },
    {
        "title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
        "author": [
            "J. Lafferty",
            "A. McCallum",
            "F. Pereira"
        ],
        "venue": "Proc. Int. Conf. Mach. Learn., 2001, pp. 282\u2013289.",
        "citeRegEx": "10",
        "shortCiteRegEx": null,
        "year": 2001,
        "abstract": "",
        "full_text": "",
        "sentence": " 2 Related work The two state-of-the-art structured learning methods are CRF [10] and SSVM [4], which captures the interdependency among output variables. For example, the linear chain CRF widely used in natural language processing estimates sequences of labels for sequences of input samples due to the fact that CRF can take context into account [10], [12]. 5 CONCLUSION We have presented a boosting method for structural learning, as an alternative to SSVM [4] and CRF [10].",
        "context": null
    },
    {
        "title": "A direct formulation for totally-corrective multi-class boosting",
        "author": [
            "C. Shen",
            "Z. Hao"
        ],
        "venue": "Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2011.",
        "citeRegEx": "11",
        "shortCiteRegEx": null,
        "year": 2011,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " It therefore builds upon the work of column generation boosting [7] and the direct formulation for multi-class boosting [11]. Indeed, we show the multiclass boosting of [11] is a special case of the general framework presented here. The direct formulation for multi-class boosting in [11] can be seen as a specific instance of this approach, which is in general very slow. 2 Multi-class boosting We first show the MultiBoost algorithm in Shen and Hao [11] can be implemented by the StructBoost framework as follows. As in [11], wy is the model parameter associated with the y-th class. The multi-class discriminant function in [11] writes F (x, y;w) = wyh \u2032(x). Instead of learning k model parameter (one wr for each class) as in Shen and Hao [11], we learn a single parameter w. The main difference between (16) and MultiBoost in [11] is that here w \u2208 R, while w \u2208 Rn\u00d7k for MultiBoost, with n being the number of weak learners.",
        "context": null
    },
    {
        "title": "An introduction to conditional random fields",
        "author": [
            "C. Sutton",
            "A. McCallum"
        ],
        "venue": "Foundations and Trends in Machine Learning, 2012. [Online]. Available: http://arxiv.org/abs/1011.4088",
        "citeRegEx": "12",
        "shortCiteRegEx": null,
        "year": 2012,
        "abstract": "Often we wish to predict a large number of variables that depend on each\nother as well as on other observed variables. Structured prediction methods are\nessentially a combination of classification and graphical modeling, combining\nthe ability of graphical models to compactly model multivariate data with the\nability of classification methods to perform prediction using large sets of\ninput features. This tutorial describes conditional random fields, a popular\nprobabilistic method for structured prediction. CRFs have seen wide application\nin natural language processing, computer vision, and bioinformatics. We\ndescribe methods for inference and parameter estimation for CRFs, including\npractical issues for implementing large scale CRFs. We do not assume previous\nknowledge of graphical modeling, so this tutorial is intended to be useful to\npractitioners in a wide variety of fields.",
        "full_text": "An Introduction to Conditional Random Fields\nCharles Sutton\nUniversity of Edinburgh\ncsutton@inf.ed.ac.uk\nAndrew McCallum\nUniversity of Massachusetts Amherst\nmccallum@cs.umass.edu\n17 November 2010\nAbstract\nOften we wish to predict a large number of variables that depend on\neach other as well as on other observed variables. Structured predic-\ntion methods are essentially a combination of classi\ufb01cation and graph-\nical modeling, combining the ability of graphical models to compactly\nmodel multivariate data with the ability of classi\ufb01cation methods to\nperform prediction using large sets of input features. This tutorial de-\nscribes conditional random \ufb01elds, a popular probabilistic method for\nstructured prediction. CRFs have seen wide application in natural lan-\nguage processing, computer vision, and bioinformatics. We describe\nmethods for inference and parameter estimation for CRFs, including\npractical issues for implementing large scale CRFs. We do not assume\nprevious knowledge of graphical modeling, so this tutorial is intended\nto be useful to practitioners in a wide variety of \ufb01elds.\narXiv:1011.4088v1  [stat.ML]  17 Nov 2010\nContents\n1\nIntroduction\n1\n2\nModeling\n5\n2.1\nGraphical Modeling\n6\n2.2\nGenerative versus Discriminative Models\n10\n2.3\nLinear-chain CRFs\n18\n2.4\nGeneral CRFs\n21\n2.5\nApplications of CRFs\n23\n2.6\nFeature Engineering\n24\n2.7\nNotes on Terminology\n26\n3\nInference\n27\n3.1\nLinear-Chain CRFs\n28\n3.2\nInference in Graphical Models\n32\n3.3\nImplementation Concerns\n40\n4\nParameter Estimation\n43\ni\n4.1\nMaximum Likelihood\n44\n4.2\nStochastic Gradient Methods\n52\n4.3\nParallelism\n54\n4.4\nApproximate Training\n54\n4.5\nImplementation Concerns\n61\n5\nRelated Work and Future Directions\n63\n5.1\nRelated Work\n63\n5.2\nFrontier Areas\n70\n1\nIntroduction\nFundamental to many applications is the ability to predict multiple\nvariables that depend on each other. Such applications are as diverse\nas classifying regions of an image [60], estimating the score in a game\nof Go [111], segmenting genes in a strand of DNA [5], and extracting\nsyntax from natural-language text [123]. In such applications, we wish\nto predict a vector y = {y0, y1, . . . , yT } of random variables given an\nobserved feature vector x. A relatively simple example from natural-\nlanguage processing is part-of-speech tagging, in which each variable\nys is the part-of-speech tag of the word at position s, and the input x\nis divided into feature vectors {x0, x1 . . . xT }. Each xs contains various\ninformation about the word at position s, such as its identity, ortho-\ngraphic features such as pre\ufb01xes and su\ufb03xes, membership in domain-\nspeci\ufb01c lexicons, and information in semantic databases such as Word-\nNet.\nOne approach to this multivariate prediction problem, especially\nif our goal is to maximize the number of labels ys that are correctly\nclassi\ufb01ed, is to learn an independent per-position classi\ufb01er that maps\nx 7\u2192ys for each s. The di\ufb03culty, however, is that the output variables\nhave complex dependencies. For example, neighboring words in a doc-\n1\nument or neighboring regions in a image tend to have similar labels.\nOr the output variables may represent a complex structure such as a\nparse tree, in which a choice of what grammar rule to use near the top\nof the tree can have a large e\ufb00ect on the rest of the tree.\nA natural way to represent the manner in which output variables\ndepend on each other is provided by graphical models. Graphical\nmodels\u2014which include such diverse model families as Bayesian net-\nworks, neural networks, factor graphs, Markov random \ufb01elds, Ising\nmodels, and others\u2014represent a complex distribution over many vari-\nables as a product of local factors on smaller subsets of variables. It\nis then possible to describe how a given factorization of the proba-\nbility density corresponds to a particular set of conditional indepen-\ndence relationships satis\ufb01ed by the distribution. This correspondence\nmakes modeling much more convenient, because often our knowledge of\nthe domain suggests reasonable conditional independence assumptions,\nwhich then determine our choice of factors.\nMuch work in learning with graphical models, especially in statisti-\ncal natural-language processing, has focused on generative models that\nexplicitly attempt to model a joint probability distribution p(y, x) over\ninputs and outputs. Although there are advantages to this approach, it\nalso has important limitations. Not only can the dimensionality of x be\nvery large, but the features have complex dependencies, so constructing\na probability distribution over them is di\ufb03cult. Modelling the depen-\ndencies among inputs can lead to intractable models, but ignoring them\ncan lead to reduced performance.\nA solution to this problem is to model the conditional distribution\np(y|x) directly, which is all that is needed for classi\ufb01cation. This is a\nconditional random \ufb01eld (CRF). CRFs are essentially a way of combin-\ning the advantages of classi\ufb01cation and graphical modeling, combining\nthe ability to compactly model multivariate data with the ability to\nleverage a large number of input features for prediction. The advantage\nto a conditional model is that dependencies that involve only variables\nin x play no role in the conditional model, so that an accurate con-\nditional model can have much simpler structure than a joint model.\nThe di\ufb00erence between generative models and CRFs is thus exactly\nanalogous to the di\ufb00erence between the naive Bayes and logistic re-\ngression classi\ufb01ers. Indeed, the multinomial logistic regression model\ncan be seen as the simplest kind of CRF, in which there is only one\noutput variable.\nThere has been a large amount of applied interest in CRFs. Suc-\ncessful applications have included text processing [89, 107, 108], bioin-\nformatics [106, 65], and computer vision [43, 53]. Although early appli-\ncations of CRFs used linear chains, recent applications of CRFs have\nalso used more general graphical structures. General graphical struc-\ntures are useful for predicting complex structures, such as graphs and\ntrees, and for relaxing the iid assumption among entities, as in rela-\ntional learning [121].\nThis tutorial describes modeling, inference, and parameter estima-\ntion using conditional random \ufb01elds. We do not assume previous knowl-\nedge of graphical modeling, so this tutorial is intended to be useful to\npractitioners in a wide variety of \ufb01elds. We begin by describing mod-\nelling issues in CRFs (Chapter 2), including linear-chain CRFs, CRFs\nwith general graphical structure, and hidden CRFs that include latent\nvariables. We describe how CRFs can be viewed both as a generaliza-\ntion of the well-known logistic regression procedure, and as a discrimi-\nnative analogue of the hidden Markov model.\nIn the next two chapters, we describe inference (Chapter 3) and\nlearning (Chapter 4) in CRFs. The two procedures are closely coupled,\nbecause learning usually calls inference as a subroutine. Although the\ninference algorithms that we discuss are standard algorithms for graph-\nical models, the fact that inference is embedded within an outer param-\neter estimation procedure raises additional issues. Finally, we discuss\nrelationships between CRFs and other families of models, including\nother structured prediction methods, neural networks, and maximum\nentropy Markov models (Chapter 5).\nImplementation Details\nThroughout this monograph, we try to point out implementation de-\ntails that are sometimes elided in the research literature. For example,\nwe discuss issues relating to feature engineering (Section 2.6), avoiding\nnumerical over\ufb02ow during inference (Section 3.3), and the scalability\nof CRF training on some benchmark problems (Section 4.5).\nSince this is the \ufb01rst of our sections on implementation details, it\nseems appropriate to mention some of the available implementations of\nCRFs. At the time of writing, a few popular implementations are:\nCRF++\nhttp://crfpp.sourceforge.net/\nMALLET\nhttp://mallet.cs.umass.edu/\nGRMM\nhttp://mallet.cs.umass.edu/grmm/\nCRFSuite\nhttp://www.chokkan.org/software/crfsuite/\nFACTORIE\nhttp://www.factorie.cc\nAlso, software for Markov Logic networks (such as Alchemy: http:\n//alchemy.cs.washington.edu/) can be used to build CRF models.\nAlchemy, GRMM, and FACTORIE are the only toolkits of which we\nare aware that handle arbitrary graphical structure.\n2\nModeling\nIn this chapter, we describe conditional random \ufb01elds from a model-\ning perspective, explaining how a CRF represents distributions over\nstructured outputs as a function of a high-dimensional input vector.\nCRFs can be understood both as an extension of the logistic regression\nclassi\ufb01er to arbitrary graphical structures, or as a discriminative ana-\nlog of generative models of structured data, an such as hidden Markov\nmodels.\nWe begin with a brief introduction to graphical modeling (Sec-\ntion 2.1) and a description of generative and discriminative models\nin NLP (Section 2.2). Then we will be able to present the formal de\ufb01-\nnition of conditional random \ufb01eld, both for the commonly-used case of\nlinear chains (Section 2.3), and for general graphical structures (Sec-\ntion 2.4). Finally, we present some examples of how di\ufb00erent structures\nare used in applications (Section 2.5), and some implementation details\nconcerning feature engineering (Section 2.6).\n5\n2.1\nGraphical Modeling\nGraphical modeling is a powerful framework for representation and\ninference in multivariate probability distributions. It has proven useful\nin diverse areas of stochastic modeling, including coding theory [77],\ncomputer vision [34], knowledge representation [88], Bayesian statistics\n[33], and natural-language processing [54, 9].\nDistributions over many variables can be expensive to represent\nna\u00a8\u0131vely. For example, a table of joint probabilities of n binary vari-\nables requires storing O(2n) \ufb02oating-point numbers. The insight of the\ngraphical modeling perspective is that a distribution over very many\nvariables can often be represented as a product of local functions that\neach depend on a much smaller subset of variables. This factorization\nturns out to have a close connection to certain conditional indepen-\ndence relationships among the variables\u2014both types of information\nbeing easily summarized by a graph. Indeed, this relationship between\nfactorization, conditional independence, and graph structure comprises\nmuch of the power of the graphical modeling framework: the condi-\ntional independence viewpoint is most useful for designing models, and\nthe factorization viewpoint is most useful for designing inference algo-\nrithms.\nIn the rest of this section, we introduce graphical models from both\nthe factorization and conditional independence viewpoints, focusing on\nthose models which are based on undirected graphs. A more detailed\nmodern perspective on graphical modelling and approximate inference\nis available in a textbook by Koller and Friedman [49].\n2.1.1\nUndirected Models\nWe consider probability distributions over sets of random variables V =\nX \u222aY , where X is a set of input variables that we assume are observed,\nand Y is a set of output variables that we wish to predict. Every variable\ns \u2208V takes outcomes from a set V, which can be either continuous or\ndiscrete, although we consider only the discrete case in this tutorial. An\narbitrary assignment to X is denoted by a vector x. Given a variable\ns \u2208X, the notation xs denotes the value assigned to s by x, and\nsimilarly for an assignment to a subset a \u2282X by xa. The notation\n1{x=x\u2032} denotes an indicator function of x which takes the value 1 when\nx = x\u2032 and 0 otherwise. We also require notation for marginalization.\nFor a \ufb01xed variable assignment ys, we use the summation P\ny\\ys to\nindicate a summation over all possible assignments y whose value for\nvariable s is equal to ys.\nSuppose that we believe that a probability distribution p of interest\ncan be represented by a product of factors of the form \u03a8a(xa, ya),\nwhere each factor has scope a \u2286V . This factorization can allow us\nto represent p much more e\ufb03ciently, because the sets a may be much\nsmaller than the full variable set V . We assume that without loss of\ngenerality that each distinct set a has at most one factor \u03a8a.\nAn undirected graphical model is a family of probability distribu-\ntions that factorize according to given collection of scopes. Formally,\ngiven a collection of subsets F = a \u2282V , an undirected graphical model\nis de\ufb01ned as the set of all distributions that can be written in the form\np(x, y) = 1\nZ\nY\na\u2208F\n\u03a8a(xa, ya),\n(2.1)\nfor any choice of local function F = {\u03a8a}, where \u03a8a : V|a| \u2192\u211c+.\n(These functions are also called factors or compatibility functions.) We\nwill occasionally use the term random \ufb01eld to refer to a particular\ndistribution among those de\ufb01ned by an undirected model. The reason\nfor the term graphical model will become apparent shortly, when we\ndiscuss how the factorization of (2.1) can be represented as a graph.\nThe constant Z is a normalization factor that ensures the distribu-\ntion p sums to 1. It is de\ufb01ned as\nZ =\nX\nx,y\nY\na\u2208F\n\u03a8a(xa, ya).\n(2.2)\nThe quantity Z, considered as a function of the set F of factors, is\nsometime called the partition function. Notice that the summation in\n(2.2) is over the exponentially many possible assignments to x and y.\nFor this reason, computing Z is intractable in general, but much work\nexists on how to approximate it.\nWe will generally assume further that each local function has the\nform\n\u03a8a(xa, ya) = exp\n(X\nk\n\u03b8akfak(xa, ya)\n)\n,\n(2.3)\nfor some real-valued parameter vector \u03b8a, and for some set of feature\nfunctions or su\ufb03cient statistics {fak}. If x and y are discrete, then this\nassumption is without loss of generality, because we can have features\nhave indicator functions for every possible value, that is, if we include\none feature function fak(xa, ya) = 1{xa=x\u2217a}1{ya=y\u2217a} for every possible\nvalue x\u2217\na and y\u2217\na.\nAlso, a consequence of this parameterization is that the family of\ndistributions over V parameterized by \u03b8 is an exponential family. In-\ndeed, much of the discussion in this tutorial about parameter estimation\nfor CRFs applies to exponential families in general.\nAs we have mentioned, there is a close connection between the\nfactorization of a graphical model and the conditional independencies\namong the variables in its domain. This connection can be understood\nby means of an undirected graph known as a Markov network, which\ndirectly represents conditional independence relationships in a multi-\nvariate distribution. Let G be an undirected graph with variables V ,\nthat is, G has one node for every random variable of interest. For a\nvariable s \u2208V , let N(s) denote the neighbors of s. Then we say that a\ndistribution p is Markov with respect to G if it meets the local Markov\nproperty: for any two variables s, t \u2208V , the variable s is independent\nof t conditioned on its neighbors N(s). Intuitively, this means that the\nneighbors of s contain all of the information necessary to predict its\nvalue.\nGiven a factorization of a distribution p as in (2.1), an equivalent\nMarkov network can be constructed by connecting all pairs of variables\nthat share a local function. It is straightforward to show that p is\nMarkov with respect to this graph, because the conditional distribution\np(xs|xN(s)) that follows from (2.1) is a function only of variables that\nappear in the Markov blanket. In other words, if p factorizes according\nto G, then p is Markov with respect to G.\nThe converse direction also holds, as long as p is strictly positive.\nThis is stated in the following classical result [42, 7]:\nFig. 2.1 A Markov network with an ambiguous factorization. Both of the factor graphs at\nright factorize according to the Markov network at left.\nTheorem 2.1 (Hammersley-Cli\ufb00ord). Suppose p is a strictly posi-\ntive distribution, and G is an undirected graph that indexes the domain\nof p. Then p is Markov with respect to G if and only if p factorizes ac-\ncording to G.\nA Markov network has an undesirable ambiguity from the factor-\nization perspective, however. Consider the three-node Markov network\nin Figure 2.1 (left). Any distribution that factorizes as p(x1, x2, x3) \u221d\nf(x1, x2, x3) for some positive function f is Markov with respect to\nthis graph. However, we may wish to use a more restricted parameter-\nization, where p(x1, x2, x3) \u221df(x1, x2)g(x2, x3)h(x1, x3). This second\nmodel family is smaller, and therefore may be more amenable to param-\neter estimation. But the Markov network formalism cannot distinguish\nbetween these two parameterizations. In order to state models more\nprecisely, the factorization (2.1) can be represented directly by means\nof a factor graph [50]. A factor graph is a bipartite graph G = (V, F, E)\nin which a variable node vs \u2208V is connected to a factor node \u03a8a \u2208F\nif vs is an argument to \u03a8a. An example of a factor graph is shown\ngraphically in Figure 2.2 (right). In that \ufb01gure, the circles are vari-\nable nodes, and the shaded boxes are factor nodes. Notice that, unlike\nthe undirected graph, the factor graph depicts the factorization of the\nmodel unambiguously.\n2.1.2\nDirected Models\nWhereas the local functions in an undirected model need not have a\ndirect probabilistic interpretation, a directed graphical model describes\nhow a distribution factorizes into local conditional probability distri-\nbutions. Let G = (V, E) be a directed acyclic graph, in which \u03c0(v)\nare the parents of v in G. A directed graphical model is a family of\ndistributions that factorize as:\np(y, x) =\nY\nv\u2208V\np(yv|y\u03c0(v)).\n(2.4)\nIt can be shown by structural induction on G that p is properly normal-\nized. Directed models can be thought of as a kind of factor graph, in\nwhich the individual factors are locally normalized in a special fashion\nso that globally Z = 1. Directed models are often used as generative\nmodels, as we explain in Section 2.2.3. An example of a directed model\nis the naive Bayes model (2.5), which is depicted graphically in Fig-\nure 2.2 (left).\n2.2\nGenerative versus Discriminative Models\nIn this section we discuss several examples applications of simple graph-\nical models to natural language processing. Although these examples\nare well-known, they serve both to clarify the de\ufb01nitions in the pre-\nvious section, and to illustrate some ideas that will arise again in our\ndiscussion of conditional random \ufb01elds. We devote special attention to\nthe hidden Markov model (HMM), because it is closely related to the\nlinear-chain CRF.\n2.2.1\nClassi\ufb01cation\nFirst we discuss the problem of classi\ufb01cation, that is, predicting a single\ndiscrete class variable y given a vector of features x = (x1, x2, . . . , xK).\nOne simple way to accomplish this is to assume that once the class\nlabel is known, all the features are independent. The resulting classi\ufb01er\nis called the naive Bayes classi\ufb01er. It is based on a joint probability\nx\ny\nx\ny\nFig. 2.2 The naive Bayes classi\ufb01er, as a directed model (left), and as a factor graph (right).\nmodel of the form:\np(y, x) = p(y)\nK\nY\nk=1\np(xk|y).\n(2.5)\nThis model can be described by the directed model shown in Figure 2.2\n(left). We can also write this model as a factor graph, by de\ufb01ning a\nfactor \u03a8(y) = p(y), and a factor \u03a8k(y, xk) = p(xk|y) for each feature\nxk. This factor graph is shown in Figure 2.2 (right).\nAnother well-known classi\ufb01er that is naturally represented as a\ngraphical model is logistic regression (sometimes known as the maxi-\nmum entropy classi\ufb01er in the NLP community). In statistics, this clas-\nsi\ufb01er is motivated by the assumption that the log probability, log p(y|x),\nof each class is a linear function of x, plus a normalization constant.\nThis leads to the conditional distribution:\np(y|x) =\n1\nZ(x) exp\n\uf8f1\n\uf8f2\n\uf8f3\u03b8y +\nK\nX\nj=1\n\u03b8y,jxj\n\uf8fc\n\uf8fd\n\uf8fe,\n(2.6)\nwhere Z(x) = P\ny exp{\u03b8y +PK\nj=1 \u03b8y,jxj} is a normalizing constant, and\n\u03b8y is a bias weight that acts like log p(y) in naive Bayes. Rather than\nusing one weight vector per class, as in (2.6), we can use a di\ufb00erent\nnotation in which a single set of weights is shared across all the classes.\nThe trick is to de\ufb01ne a set of feature functions that are nonzero only\nfor a single class. To do this, the feature functions can be de\ufb01ned as\nfy\u2032,j(y, x) = 1{y\u2032=y}xj for the feature weights and fy\u2032(y, x) = 1{y\u2032=y} for\nthe bias weights. Now we can use fk to index each feature function fy\u2032,j,\nand \u03b8k to index its corresponding weight \u03b8y\u2032,j. Using this notational\ntrick, the logistic regression model becomes:\np(y|x) =\n1\nZ(x) exp\n( K\nX\nk=1\n\u03b8kfk(y, x)\n)\n.\n(2.7)\nWe introduce this notation because it mirrors the notation for condi-\ntional random \ufb01elds that we will present later.\n2.2.2\nSequence Models\nClassi\ufb01ers predict only a single class variable, but the true power of\ngraphical models lies in their ability to model many variables that\nare interdependent. In this section, we discuss perhaps the simplest\nform of dependency, in which the output variables are arranged in a\nsequence. To motivate this kind of model, we discuss an application\nfrom natural language processing, the task of named-entity recognition\n(NER). NER is the problem of identifying and classifying proper names\nin text, including locations, such as China; people, such as George\nBush; and organizations, such as the United Nations. The named-entity\nrecognition task is, given a sentence, to segment which words are part\nof entities, and to classify each entity by type (person, organization,\nlocation, and so on). The challenge of this problem is that many named\nentities are too rare to appear even in a large training set, and therefore\nthe system must identify them based only on context.\nOne approach to NER is to classify each word independently as one\nof either Person, Location, Organization, or Other (meaning\nnot an entity). The problem with this approach is that it assumes\nthat given the input, all of the named-entity labels are independent.\nIn fact, the named-entity labels of neighboring words are dependent;\nfor example, while New York is a location, New York Times is an\norganization. One way to relax this independence assumption is to\narrange the output variables in a linear chain. This is the approach\ntaken by the hidden Markov model (HMM) [96]. An HMM models a\nsequence of observations X = {xt}T\nt=1 by assuming that there is an\nunderlying sequence of states Y = {yt}T\nt=1 drawn from a \ufb01nite state\nset S. In the named-entity example, each observation xt is the identity\nof the word at position t, and each state yt is the named-entity label,\nthat is, one of the entity types Person, Location, Organization,\nand Other.\nTo model the joint distribution p(y, x) tractably, an HMM makes\ntwo independence assumptions. First, it assumes that each state de-\npends only on its immediate predecessor, that is, each state yt is in-\ndependent of all its ancestors y1, y2, . . . , yt\u22122 given the preceding state\nyt\u22121. Second, it also assumes that each observation variable xt depends\nonly on the current state yt. With these assumptions, we can specify an\nHMM using three probability distributions: \ufb01rst, the distribution p(y1)\nover initial states; second, the transition distribution p(yt|yt\u22121); and\n\ufb01nally, the observation distribution p(xt|yt). That is, the joint proba-\nbility of a state sequence y and an observation sequence x factorizes\nas\np(y, x) =\nT\nY\nt=1\np(yt|yt\u22121)p(xt|yt),\n(2.8)\nwhere, to simplify notation, we write the initial state distribution p(y1)\nas p(y1|y0). In natural language processing, HMMs have been used for\nsequence labeling tasks such as part-of-speech tagging, named-entity\nrecognition, and information extraction.\n2.2.3\nComparison\nOf the models described in this section, two are generative (the naive\nBayes and hidden Markov models) and one is discriminative (the lo-\ngistic regression model). In a general, generative models are models\nof the joint distribution p(y, x), and like naive Bayes have the form\np(y)p(x|y). In other words, they describe how the output is probabilis-\ntically generated as a function of the input. Discriminative models, on\nthe other hand, focus solely on the conditional distribution p(y|x). In\nthis section, we discuss the di\ufb00erences between generative and discrim-\ninative modeling, and the potential advantages of discriminative mod-\neling. For concreteness, we focus on the examples of naive Bayes and\nlogistic regression, but the discussion in this section applies equally as\nwell to the di\ufb00erences between arbitrarily structured generative models\nand conditional random \ufb01elds.\nThe main di\ufb00erence is that a conditional distribution p(y|x) does\nnot include a model of p(x), which is not needed for classi\ufb01cation any-\nway. The di\ufb03culty in modeling p(x) is that it often contains many\nhighly dependent features that are di\ufb03cult to model. For example,\nin named-entity recognition, an HMM relies on only one feature, the\nword\u2019s identity. But many words, especially proper names, will not have\noccurred in the training set, so the word-identity feature is uninforma-\ntive. To label unseen words, we would like to exploit other features of a\nword, such as its capitalization, its neighboring words, its pre\ufb01xes and\nsu\ufb03xes, its membership in predetermined lists of people and locations,\nand so on.\nThe principal advantage of discriminative modeling is that it is bet-\nter suited to including rich, overlapping features. To understand this,\nconsider the family of naive Bayes distributions (2.5). This is a family\nof joint distributions whose conditionals all take the \u201clogistic regression\nform\u201d (2.7). But there are many other joint models, some with com-\nplex dependencies among x, whose conditional distributions also have\nthe form (2.7). By modeling the conditional distribution directly, we\ncan remain agnostic about the form of p(x). CRFs make independence\nassumptions among y, and assumptions about how the y can depend\non x, but not among x. This point can also be understood graphi-\ncally: Suppose that we have a factor graph representation for the joint\ndistribution p(y, x). If we then construct a graph for the conditional\ndistribution p(y|x), any factors that depend only on x vanish from the\ngraphical structure for the conditional distribution. They are irrelevant\nto the conditional because they are constant with respect to y.\nTo include interdependent features in a generative model, we have\ntwo choices: enhance the model to represent dependencies among the in-\nputs, or make simplifying independence assumptions, such as the naive\nBayes assumption. The \ufb01rst approach, enhancing the model, is often\ndi\ufb03cult to do while retaining tractability. For example, it is hard to\nimagine how to model the dependence between the capitalization of a\nword and its su\ufb03xes, nor do we particularly wish to do so, since we\nalways observe the test sentences anyway. The second approach\u2014to in-\nclude a large number of dependent features in a generative model, but\nto include independence assumptions among them\u2014is possible, and in\nsome domains can work well. But it can also be problematic because\nthe independence assumptions can hurt performance. For example, al-\nthough the naive Bayes classi\ufb01er performs well in document classi\ufb01ca-\ntion, it performs worse on average across a range of applications than\nlogistic regression [16].\nFurthermore, naive Bayes can produce poor probability esti-\nmates. As an illustrative example, imagine training naive Bayes on\na data set in which all the features are repeated, that is, x =\n(x1, x1, x2, x2, . . . , xK, xK). This will increase the con\ufb01dence of the\nnaive Bayes probability estimates, even though no new information\nhas been added to the data. Assumptions like naive Bayes can be espe-\ncially problematic when we generalize to sequence models, because in-\nference essentially combines evidence from di\ufb00erent parts of the model.\nIf probability estimates of the label at each sequence position are over-\ncon\ufb01dent, it might be di\ufb03cult to combine them sensibly.\nThe di\ufb00erence between naive Bayes and logistic regression is due\nonly to the fact that the \ufb01rst is generative and the second discrimi-\nnative; the two classi\ufb01ers are, for discrete input, identical in all other\nrespects. Naive Bayes and logistic regression consider the same hy-\npothesis space, in the sense that any logistic regression classi\ufb01er can be\nconverted into a naive Bayes classi\ufb01er with the same decision boundary,\nand vice versa. Another way of saying this is that the naive Bayes model\n(2.5) de\ufb01nes the same family of distributions as the logistic regression\nmodel (2.7), if we interpret it generatively as\np(y, x) =\nexp {P\nk \u03b8kfk(y, x)}\nP\n\u02dcy,\u02dcx exp {P\nk \u03b8kfk(\u02dcy, \u02dcx)}.\n(2.9)\nThis means that if the naive Bayes model (2.5) is trained to maximize\nthe conditional likelihood, we recover the same classi\ufb01er as from logis-\ntic regression. Conversely, if the logistic regression model is interpreted\ngeneratively, as in (2.9), and is trained to maximize the joint likelihood\np(y, x), then we recover the same classi\ufb01er as from naive Bayes. In the\nterminology of Ng and Jordan [85], naive Bayes and logistic regression\nform a generative-discriminative pair. For a recent theoretical perspec-\ntive on generative and discriminative models, see Liang and Jordan\n[61].\nLogistic Regression\nHMMs\nLinear-chain CRFs\nNaive Bayes\nSEQUENCE\nSEQUENCE\nCONDITIONAL\nCONDITIONAL\nGenerative directed models\nGeneral CRFs\nCONDITIONAL\nGeneral\nGRAPHS\nGeneral\nGRAPHS\nFig. 2.3 Diagram of the relationship between naive Bayes, logistic regression, HMMs, linear-\nchain CRFs, generative models, and general CRFs.\nOne perspective for gaining insight into the di\ufb00erence between gen-\nerative and discriminative modeling is due to Minka [80]. Suppose we\nhave a generative model pg with parameters \u03b8. By de\ufb01nition, this takes\nthe form\npg(y, x; \u03b8) = pg(y; \u03b8)pg(x|y; \u03b8).\n(2.10)\nBut we could also rewrite pg using Bayes rule as\npg(y, x; \u03b8) = pg(x; \u03b8)pg(y|x; \u03b8),\n(2.11)\nwhere pg(x; \u03b8) and pg(y|x; \u03b8) are computed by inference, i.e., pg(x; \u03b8) =\nP\ny pg(y, x; \u03b8) and pg(y|x; \u03b8) = pg(y, x; \u03b8)/pg(x; \u03b8).\nNow, compare this generative model to a discriminative model over\nthe same family of joint distributions. To do this, we de\ufb01ne a prior\np(x) over inputs, such that p(x) could have arisen from pg with some\nparameter setting. That is, p(x) = pc(x; \u03b8\u2032) = P\ny pg(y, x|\u03b8\u2032). We com-\nbine this with a conditional distribution pc(y|x; \u03b8) that could also have\narisen from pg, that is, pc(y|x; \u03b8) = pg(y, x; \u03b8)/pg(x; \u03b8). Then the re-\nsulting distribution is\npc(y, x) = pc(x; \u03b8\u2032)pc(y|x; \u03b8).\n(2.12)\nBy comparing (2.11) with (2.12), it can be seen that the conditional\napproach has more freedom to \ufb01t the data, because it does not require\nthat \u03b8 = \u03b8\u2032. Intuitively, because the parameters \u03b8 in (2.11) are used\nin both the input distribution and the conditional, a good set of pa-\nrameters must represent both well, potentially at the cost of trading\no\ufb00accuracy on p(y|x), the distribution we care about, for accuracy\non p(x), which we care less about. On the other hand, this added free-\ndom brings about an increased risk of over\ufb01tting the training data, and\ngeneralizing worse on unseen data.\nTo be fair, however, generative models have several advantages of\ntheir own. First, generative models can be more natural for handling la-\ntent variables, partially-labeled data, and unlabelled data. In the most\nextreme case, when the data is entirely unlabeled, generative models\ncan be applied in an unsupervised fashion, whereas unsupervised learn-\ning in discriminative models is less natural and is still an active area\nof research.\nSecond, on some data a generative model can perform better than\na discriminative model, intuitively because the input model p(x) may\nhave a smoothing e\ufb00ect on the conditional. Ng and Jordan [85] argue\nthat this e\ufb00ect is especially pronounced when the data set is small. For\nany particular data set, it is impossible to predict in advance whether\na generative or a discriminative model will perform better. Finally,\nsometimes either the problem suggests a natural generative model, or\nthe application requires the ability to predict both future inputs and\nfuture outputs, making a generative model preferable.\nBecause a generative model takes the form p(y, x) = p(y)p(x|y),\nit is often natural to represent a generative model by a directed graph\nin which in outputs y topologically precede the inputs. Similarly, we\nwill see that it is often natural to represent a discriminative model by\na undirected graph, although this need not always be the case.\nThe relationship between naive Bayes and logistic regression mirrors\nthe relationship between HMMs and linear-chain CRFs. Just as naive\nBayes and logistic regression are a generative-discriminative pair, there\nis a discriminative analogue to the hidden Markov model, and this\nanalogue is a particular special case of conditional random \ufb01eld, as we\nexplain in the next section. This analogy between naive Bayes, logistic\nregression, generative models, and conditional random \ufb01elds is depicted\n. . .\n. . .\ny\nx\nFig. 2.4 Graphical model of an HMM-like linear-chain CRF.\n. . .\n. . .\ny\nx\nFig. 2.5 Graphical model of a linear-chain CRF in which the transition score depends on\nthe current observation.\nin Figure 2.3.\n2.3\nLinear-chain CRFs\nTo motivate our introduction of linear-chain conditional random \ufb01elds,\nwe begin by considering the conditional distribution p(y|x) that follows\nfrom the joint distribution p(y, x) of an HMM. The key point is that\nthis conditional distribution is in fact a conditional random \ufb01eld with\na particular choice of feature functions.\nFirst, we rewrite the HMM joint (2.8) in a form that is more\namenable to generalization. This is\np(y, x) = 1\nZ\nT\nY\nt=1\nexp\n\uf8f1\n\uf8f2\n\uf8f3\nX\ni,j\u2208S\n\u03b8ij1{yt=i}1{yt\u22121=j} +\nX\ni\u2208S\nX\no\u2208O\n\u00b5oi1{yt=i}1{xt=o}\n\uf8fc\n\uf8fd\n\uf8fe,\n(2.13)\nwhere \u03b8 = {\u03b8ij, \u00b5oi} are the real-valued parameters of the distribution\nand Z is a normalization constant chosen so the distribution sums to\none.1 It can be seen that (2.13) describes exactly the class of HMMs.\n1 Not all choices of \u03b8 are valid, because the summation de\ufb01ning Z, that is, Z\n=\nP\ny\nP\nx\nQT\nt=1 exp\nnP\ni,j\u2208S \u03b8ij1{yt=i}1{yt\u22121=j} + P\ni\u2208S\nP\no\u2208O \u00b5oi1{yt=i}1{xt=o}\no\n,\nmight not converge. An example of this is a model with one state where \u03b800 > 0. This\nissue is typically not an issue for CRFs, because in a CRF the summation within Z is\nEvery HMM can be written in this form by setting \u03b8ij = log p(y\u2032 =\ni|y = j) and \u00b5oi = log p(x = o|y = i). The converse direction is more\ncomplicated, and not relevant for our purposes here. The main point\nis that despite this added \ufb02exibility in the parameterization (2.13), we\nhave not added any distributions to the family.\nWe can write (2.13) more compactly by introducing the concept of\nfeature functions, just as we did for logistic regression in (2.7). Each fea-\nture function has the form fk(yt, yt\u22121, xt). In order to duplicate (2.13),\nthere needs to be one feature fij(y, y\u2032, x) = 1{y=i}1{y\u2032=j} for each tran-\nsition (i, j) and one feature fio(y, y\u2032, x) = 1{y=i}1{x=o} for each state-\nobservation pair (i, o). We refer to a feature function generically as fk,\nwhere fk ranges over both all of the fij and all of the fio. Then we can\nwrite an HMM as:\np(y, x) = 1\nZ\nT\nY\nt=1\nexp\n( K\nX\nk=1\n\u03b8kfk(yt, yt\u22121, xt)\n)\n.\n(2.14)\nAgain, equation (2.14) de\ufb01nes exactly the same family of distributions\nas (2.13), and therefore as the original HMM equation (2.8).\nThe last step is to write the conditional distribution p(y|x) that\nresults from the HMM (2.14). This is\np(y|x) =\np(y, x)\nP\ny\u2032 p(y\u2032, x) =\nQT\nt=1 exp\nnPK\nk=1 \u03b8kfk(yt, yt\u22121, xt)\no\nP\ny\u2032\nQT\nt=1 exp\nnPK\nk=1 \u03b8kfk(y\u2032\nt, y\u2032\nt\u22121, xt)\no.\n(2.15)\nThis conditional distribution (2.15) is a particular kind of linear-chain\nCRF, namely, one that includes features only for the current word\u2019s\nidentity. But many other linear-chain CRFs use richer features of the\ninput, such as pre\ufb01xes and su\ufb03xes of the current word, the identity of\nsurrounding words, and so on. Fortunately, this extension requires little\nchange to our existing notation. We simply allow the feature functions\nto be more general than indicator functions of the word\u2019s identity. This\nleads to the general de\ufb01nition of linear-chain CRFs:\nusually over a \ufb01nite set.\nDe\ufb01nition 2.1. Let Y, X be random vectors, \u03b8 = {\u03b8k} \u2208\u211cK be a\nparameter vector, and {fk(y, y\u2032, xt)}K\nk=1 be a set of real-valued feature\nfunctions. Then a linear-chain conditional random \ufb01eld is a distribution\np(y|x) that takes the form\np(y|x) =\n1\nZ(x)\nT\nY\nt=1\nexp\n( K\nX\nk=1\n\u03b8kfk(yt, yt\u22121, xt)\n)\n,\n(2.16)\nwhere Z(x) is an instance-speci\ufb01c normalization function\nZ(x) =\nX\ny\nT\nY\nt=1\nexp\n( K\nX\nk=1\n\u03b8kfk(yt, yt\u22121, xt)\n)\n.\n(2.17)\nWe have just seen that if the joint p(y, x) factorizes as an HMM,\nthen the associated conditional distribution p(y|x) is a linear-chain\nCRF. This HMM-like CRF is pictured in Figure 2.4. Other types of\nlinear-chain CRFs are also useful, however. For example, typically in\nan HMM, a transition from state i to state j receives the same score,\nlog p(yt = j|yt\u22121 = i), regardless of the input. In a CRF, we can allow\nthe score of the transition (i, j) to depend on the current observation\nvector, simply by adding a feature 1{yt=j}1{yt\u22121=1}1{xt=o}. A CRF with\nthis kind of transition feature, which is commonly used in text appli-\ncations, is pictured in Figure 2.5.\nTo indicate in the de\ufb01nition of linear-chain CRF that each feature\nfunction can depend on observations from any time step, we have writ-\nten the observation argument to fk as a vector xt, which should be\nunderstood as containing all the components of the global observations\nx that are needed for computing features at time t. For example, if the\nCRF uses the next word xt+1 as a feature, then the feature vector xt\nis assumed to include the identity of word xt+1.\nFinally, note that the normalization constant Z(x) sums over all\npossible state sequences, an exponentially large number of terms. Nev-\nertheless, it can be computed e\ufb03ciently by forward-backward, as we\nexplain in Section 3.1.\n2.4\nGeneral CRFs\nNow we present the general de\ufb01nition of a conditional random \ufb01eld,\nas it was originally introduced [54]. The generalization from linear-\nchain CRFs to general CRFs is fairly straightforward. We simply move\nfrom using a linear-chain factor graph to a more general factor graph,\nand from forward-backward to more general (perhaps approximate)\ninference algorithms.\nDe\ufb01nition 2.2. Let G be a factor graph over Y . Then p(y|x) is a\nconditional random \ufb01eld if for any \ufb01xed x, the distribution p(y|x) fac-\ntorizes according to G.\nThus, every conditional distribution p(y|x) is a CRF for some, per-\nhaps trivial, factor graph. If F = {\u03a8a} is the set of factors in G, and\neach factor takes the exponential family form (2.3), then the conditional\ndistribution can be written as\np(y|x) =\n1\nZ(x)\nY\n\u03a8A\u2208G\nexp\n\uf8f1\n\uf8f2\n\uf8f3\nK(A)\nX\nk=1\n\u03b8akfak(ya, xa)\n\uf8fc\n\uf8fd\n\uf8fe.\n(2.18)\nIn addition, practical models rely extensively on parameter tying. For\nexample, in the linear-chain case, often the same weights are used for\nthe factors \u03a8t(yt, yt\u22121, xt) at each time step. To denote this, we parti-\ntion the factors of G into C = {C1, C2, . . . CP }, where each Cp is a clique\ntemplate whose parameters are tied. This notion of clique template gen-\neralizes that in Taskar et al. [121], Sutton et al. [119], Richardson and\nDomingos [98], and McCallum et al. [76]. Each clique template Cp is\na set of factors which has a corresponding set of su\ufb03cient statistics\n{fpk(xp, yp)} and parameters \u03b8p \u2208\u211cK(p). Then the CRF can be writ-\nten as\np(y|x) =\n1\nZ(x)\nY\nCp\u2208C\nY\n\u03a8c\u2208Cp\n\u03a8c(xc, yc; \u03b8p),\n(2.19)\nwhere each factor is parameterized as\n\u03a8c(xc, yc; \u03b8p) = exp\n\uf8f1\n\uf8f2\n\uf8f3\nK(p)\nX\nk=1\n\u03b8pkfpk(xc, yc)\n\uf8fc\n\uf8fd\n\uf8fe,\n(2.20)\nand the normalization function is\nZ(x) =\nX\ny\nY\nCp\u2208C\nY\n\u03a8c\u2208Cp\n\u03a8c(xc, yc; \u03b8p).\n(2.21)\nThis notion of clique template speci\ufb01es both repeated structure and\nparameter tying in the model. For example, in a linear-chain conditional\nrandom \ufb01eld, typically one clique template C0 = {\u03a8t(yt, yt\u22121, xt)}T\nt=1 is\nused for the entire network, so C = {C0} is a singleton set. If instead we\nwant each factor \u03a8t to have a separate set of parameters, this would\nbe accomplished using T templates, by taking C = {Ct}T\nt=1, where\nCt = {\u03a8t(yt, yt\u22121, xt)}. Both the set of clique templates and the number\nof outputs can depend on the input x; for example, to model images,\nwe may use di\ufb00erent clique templates at di\ufb00erent scales depending on\nthe results of an algorithm for \ufb01nding points of interest.\nOne of the most important considerations in de\ufb01ning a general CRF\nlies in specifying the repeated structure and parameter tying. A number\nof formalisms have been proposed to specify the clique templates. For\nexample, dynamic conditional random \ufb01elds [119] are sequence models\nwhich allow multiple labels at each time step, rather than single label,\nin a manner analogous to dynamic Bayesian networks. Second, rela-\ntional Markov networks [121] are a type of general CRF in which the\ngraphical structure and parameter tying are determined by an SQL-like\nsyntax. Markov logic networks [98, 110] use logical formulae to specify\nthe scopes of local functions in an undirected model. Essentially, there\nis a set of parameters for each \ufb01rst-order rule in a knowledge base. The\nlogic portion of an MLN can be viewed as essentially a programming\nconvention for specifying the repeated structure and parameter tying\nof an undirected model. Imperatively de\ufb01ned factor graphs [76] use the\nfull expressivity of Turing-complete functions to de\ufb01ne the clique tem-\nplates, specifying both the structure of the model and the su\ufb03cient\nstatistics fpk. These functions have the \ufb02exibility to employ advanced\nprogramming ideas including recursion, arbitrary search, lazy evalua-\ntion, and memoization.\n2.5\nApplications of CRFs\nCRFs have been applied to a variety of domains, including text pro-\ncessing, computer vision, and bioinformatics. One of the \ufb01rst large-scale\napplications of CRFs was by Sha and Pereira [108], who matched state-\nof-the-art performance on segmenting noun phrases in text. Since then,\nlinear-chain CRFs have been applied to many problems in natural lan-\nguage processing, including named-entity recognition [72], feature in-\nduction for NER [71], shallow parsing [108, 120], identifying protein\nnames in biology abstracts [107], segmenting addresses in Web pages\n[26], information integration [134], \ufb01nding semantic roles in text [103],\nprediction of pitch accents [40], phone classi\ufb01cation in speech processing\n[41], identifying the sources of opinions [17], word alignment in machine\ntranslation [10], citation extraction from research papers [89], extrac-\ntion of information from tables in text documents [91], Chinese word\nsegmentation [90], Japanese morphological analysis [51], and many oth-\ners.\nIn bioinformatics, CRFs have been applied to RNA structural align-\nment [106] and protein structure prediction [65]. Semi-Markov CRFs\n[105] add somewhat more \ufb02exibility in choosing features, by allowing\nfeatures functions to depend on larger segments of the input that de-\npend on the output labelling. This can be useful for certain tasks in\ninformation extraction and especially bioinformatics.\nGeneral CRFs have also been applied to several tasks in NLP. One\npromising application is to performing multiple labeling tasks simulta-\nneously. For example, Sutton et al. [119] show that a two-level dynamic\nCRF for part-of-speech tagging and noun-phrase chunking performs\nbetter than solving the tasks one at a time. Another application is\nto multi-label classi\ufb01cation, in which each instance can have multiple\nclass labels. Rather than learning an independent classi\ufb01er for each\ncategory, Ghamrawi and McCallum [35] present a CRF that learns de-\npendencies between the categories, resulting in improved classi\ufb01cation\nperformance. Finally, the skip-chain CRF [114] is a general CRF that\nrepresents long-distance dependencies in information extraction.\nAn interesting graphical CRF structure has been applied to the\nproblem of proper-noun coreference, that is, of determining which men-\ntions in a document, such as Mr. President and he, refer to the same\nunderlying entity. McCallum and Wellner [73] learn a distance metric\nbetween mentions using a fully-connected conditional random \ufb01eld in\nwhich inference corresponds to graph partitioning. A similar model has\nbeen used to segment handwritten characters and diagrams [22, 93].\nIn computer vision, several authors have used grid-shaped CRFs [43,\n53] for labeling and segmenting images. Also, for recognizing objects,\nQuattoni et al. [95] use a tree-shaped CRF in which latent variables\nare designed to recognize characteristic parts of an object.\nIn some applications of CRFs, e\ufb03cient dynamic programs exist even\nthough the graphical model is di\ufb03cult to specify. For example, McCal-\nlum et al. [75] learn the parameters of a string-edit model in order to\ndiscriminate between matching and nonmatching pairs of strings. Also,\nthere is work on using CRFs to learn distributions over the derivations\nof a grammar [99, 19, 127, 31].\n2.6\nFeature Engineering\nIn this section we describe some \u201ctricks of the trade\u201d that involve fea-\nture engineering. Although these apply especially to language applica-\ntions, they are also useful more generally.\nFirst, when the predicted variables are discrete, the features fpk of\na clique template Cp are ordinarily chosen to have a particular form:\nfpk(yc, xc) = 1{yc=\u02dcyc}qpk(xc).\n(2.22)\nIn other words, each feature is nonzero only for a single output con\ufb01g-\nuration \u02dcyc, but as long as that constraint is met, then the feature value\ndepends only on the input observation. Essentially, this means that we\ncan think of our features as depending only on the input xc, but that\nwe have a separate set of weights for each output con\ufb01guration. This\nfeature representation is also computationally e\ufb03cient, because com-\nputing each qpk may involve nontrivial text or image processing, and\nit need be evaluated only once for every feature that uses it. To avoid\nconfusion, we refer to the functions qpk(xc) as observation functions\nrather than as features. Examples of observation functions are \u201cword\nxt is capitalized\u201d and \u201cword xt ends in ing\u201d.\nThis representation can lead to a large number of features, which\ncan have signi\ufb01cant memory and time requirements. For example, to\nmatch state-of-the-art results on a standard natural language task, Sha\nand Pereira [108] use 3.8 million features. Many of these features always\nzero in the training data. In particular, some observation functions qpk\nare nonzero for certain output con\ufb01gurations and zero for others. This\npoint can be confusing: One might think that such features can have\nno e\ufb00ect on the likelihood, but actually putting a negative weight on\nthem causes an assignment that does not appear in the training data\nto become less likely, which improves the likelihood. For this reason,\nincluding unsupported features typically results in better accuracy. In\norder to save memory, however, sometimes these unsupported features,\nthat is, those which never occur in the training data, are removed from\nthe model.\nAs a simple heuristic for getting some of the bene\ufb01ts of unsupported\nfeatures with less memory, we have had success with an ad hoc tech-\nnique for selecting a small set of unsupported features. The idea is to\nadd unsupported features only for likely paths, as follows: \ufb01rst train a\nCRF without any unsupported features, stopping after a few iterations;\nthen add unsupported features fpk(yc, xc) for cases where xc occurs in\nthe training data for some instance x(i), and p(yc|x(i)) > \u03f5.\nMcCallum [71] presents a more principled method of feature induc-\ntion for CRFs, in which the model begins with a number of base fea-\ntures, and the training procedure adds conjunctions of those features.\nAlternatively, one can use feature selection. A modern method for fea-\nture selection is L1 regularization, which we discuss in Section 4.1.1.\nLavergne et al. [56] \ufb01nd that in the most favorable cases L1 \ufb01nds models\nin which only 1% of the full feature set is non-zero, but with compa-\nrable performance to a dense feature setting. They also \ufb01nd it useful,\nafter optimizing the L1-regularized likelihood to \ufb01nd a set of nonzero\nfeatures, to \ufb01ne-tune the weights of the nonzero features only using an\nL2-regularized objective.\nSecond, if the observations are categorical rather than ordinal, that\nis, if they are discrete but have no intrinsic order, it is important to\nconvert them to binary features. For example, it makes sense to learn\na linear weight on fk(y, xt) when fk is 1 if xt is the word dog and\n0 otherwise, but not when fk is the integer index of word xt in the\ntext\u2019s vocabulary. Thus, in text applications, CRF features are typically\nbinary; in other application areas, such as vision and speech, they are\nmore commonly real-valued. For real-valued features, it can help to\napply standard tricks such as normalizing the features to have mean\n0 and standard deviation 1 or to bin the features to convert them to\ncategorical values.\nThird, in language applications, it is sometimes helpful to include\nredundant factors in the model. For example, in a linear-chain CRF,\none may choose to include both edge factors \u03a8t(yt, yt\u22121, xt) and vari-\nable factors \u03a8t(yt, xt). Although one could de\ufb01ne the same family of\ndistributions using only edge factors, the redundant node factors pro-\nvide a kind of backo\ufb00, which is useful when the amount of data is\nsmall compared to the number of features. (When there are hundreds\nof thousands of features, many data sets are small!) It is important to\nuse regularization (Section 4.1.1) when using redundant features be-\ncause it is the penalty on large weights that encourages the weight to\nbe spread across the overlapping features.\n2.7\nNotes on Terminology\nDi\ufb00erent parts of the theory of graphical models have been developed\nindependently in many di\ufb00erent areas, so many of the concepts in this\nchapter have di\ufb00erent names in di\ufb00erent areas. For example, undirected\nmodels are commonly also referred to Markov random \ufb01elds, Markov\nnetworks, and Gibbs distributions. As mentioned, we reserve the term\n\u201cgraphical model\u201d for a family of distributions de\ufb01ned by a graph struc-\nture; \u201crandom \ufb01eld\u201d or \u201cdistribution\u201d for a single probability distribu-\ntion; and \u201cnetwork\u201d as a term for the graph structure itself. This choice\nof terminology is not always consistent in the literature, partly because\nit is not ordinarily necessary to be precise in separating these concepts.\nSimilarly, directed graphical models are commonly known as\nBayesian networks, but we have avoided this term because of its con-\nfusion with the area of Bayesian statistics. The term generative model\nis an important one that is commonly used in the literature, but is not\nusually given a precise de\ufb01nition.\n3\nInference\nE\ufb03cient inference is critical for CRFs, both during training and for pre-\ndicting the labels on new inputs. The are two inference problems that\narise. First, after we have trained the model, we often predict the labels\nof a new input x using the most likely labeling y\u2217= arg maxy p(y|x).\nSecond, as will be seen in Chapter 4, estimation of the parameters typ-\nically requires that we compute the marginal distribution for each edge\np(yt, yt\u22121|x), and also the normalizing function Z(x).\nThese two inference problems can be seen as fundamentally the\nsame operation on two di\ufb00erent semirings [1], that is, to change the\nmarginalization problem to the maximization problem, we simply sub-\nstitute max for plus. Although for discrete variables the marginals can\nbe computed by brute-force summation, the time required to do this\nis exponential in the size of Y . Indeed, both inference problems are\nintractable for general graphs, because any propositional satis\ufb01ability\nproblem can be easily represented as a factor graph.\nIn the case of linear-chain CRFs, both inference tasks can be per-\nformed e\ufb03ciently and exactly by variants of the standard dynamic-\nprogramming algorithms for HMMs. We begin by presenting these\nalgorithms\u2014the forward-backward algorithm for computing marginal\n27\ndistributions and Viterbi algorithm for computing the most probable\nassignment\u2014in Section 3.1. These algorithms are a special case of the\nmore general belief propagation algorithm for tree-structured graphical\nmodels (Section 3.2.2). For more complex models, approximate infer-\nence is necessary. In principle, we could run any approximate inference\nalgorithm we want, and substitute the resulting approximate marginals\nfor the exact marginals within the gradient (4.9). This can cause issues,\nhowever, because for many optimization procedures, such as BFGS, we\nrequire an approximation to the likelihood function as well. We discuss\nthis issue in Section 4.4.\nIn one sense, the inference problem for a CRF is no di\ufb00erent than\nthat for any graphical model, so any inference algorithm for graphical\nmodels can be used, as described in several textbooks [67, 49]. How-\never, there are two additional issues that need to be kept in mind in\nthe context of CRFs. The \ufb01rst issue is that the inference subroutine is\ncalled repeatedly during parameter estimation (Section 4.1.1 explains\nwhy), which can be computationally expensive, so we may wish to trade\no\ufb00inference accuracy for computational e\ufb03ciency. The second issue is\nthat when approximate inference is used, there can be complex inter-\nactions between the inference procedure and the parameter estimation\nprocedure. We postpone discussion of these issues to Chapter 4, when\nwe discuss parameter estimation, but it is worth mentioning them here\nbecause they strongly in\ufb02uence the choice of inference algorithm.\n3.1\nLinear-Chain CRFs\nIn this section, we brie\ufb02y review the inference algorithms for HMMs,\nthe forward-backward and Viterbi algorithms, and describe how they\ncan be applied to linear-chain CRFs. These standard inference algo-\nrithms are described in more detail by Rabiner [96]. Both of these al-\ngorithms are special cases of the belief propagation algorithm described\nin Section 3.2.2, but we discuss the special case of linear chains in detail\nboth because it may help to make the earlier discussion more concrete,\nand because it is useful in practice.\nFirst, we introduce notation which will simplify the forward-\nbackward recursions. An HMM can be viewed as a factor graph\np(y, x) = Q\nt \u03a8t(yt, yt\u22121, xt) where Z = 1, and the factors are de\ufb01ned\nas:\n\u03a8t(j, i, x)\ndef= p(yt = j|yt\u22121 = i)p(xt = x|yt = j).\n(3.1)\nIf the HMM is viewed as a weighted \ufb01nite state machine, then \u03a8t(j, i, x)\nis the weight on the transition from state i to state j when the current\nobservation is x.\nNow, we review the HMM forward algorithm, which is used to com-\npute the probability p(x) of the observations. The idea behind forward-\nbackward is to \ufb01rst rewrite the naive summation p(x) = P\ny p(x, y)\nusing the distributive law:\np(x) =\nX\ny\nT\nY\nt=1\n\u03a8t(yt, yt\u22121, xt)\n(3.2)\n=\nX\nyT\nX\nyT\u22121\n\u03a8T(yT, yT\u22121, xT)\nX\nyT\u22122\n\u03a8T\u22121(yT\u22121, yT\u22122, xT\u22121)\nX\nyT\u22123\n\u00b7 \u00b7 \u00b7\n(3.3)\nNow we observe that each of the intermediate sums is reused many\ntimes during the computation of the outer sum, and so we can save an\nexponential amount of work by caching the inner sums.\nThis leads to de\ufb01ning a set of forward variables \u03b1t, each of which\nis a vector of size M (where M is the number of states) which stores\none of the intermediate sums. These are de\ufb01ned as:\n\u03b1t(j)\ndef= p(x\u27e81...t\u27e9, yt = j)\n(3.4)\n=\nX\ny\u27e81...t\u22121\u27e9\n\u03a8t(j, yt\u22121, xt)\nt\u22121\nY\nt\u2032=1\n\u03a8t\u2032(yt\u2032, yt\u2032\u22121, xt\u2032),\n(3.5)\nwhere the summation over y\u27e81...t\u22121\u27e9ranges over all assignments to the\nsequence of random variables y1, y2, . . . , yt\u22121. The alpha values can be\ncomputed by the recursion\n\u03b1t(j) =\nX\ni\u2208S\n\u03a8t(j, i, xt)\u03b1t\u22121(i),\n(3.6)\nwith initialization \u03b11(j) = \u03a81(j, y0, x1). (Recall that y0 is the \ufb01xed\ninitial state of the HMM.) It is easy to see that p(x) = P\nyT \u03b1T(yT)\nby repeatedly substituting the recursion (3.6) to obtain (3.3). A formal\nproof would use induction.\nThe backward recursion is exactly the same, except that in (3.3), we\npush in the summations in reverse order. This results in the de\ufb01nition\n\u03b2t(i)\ndef= p(x\u27e8t+1...T\u27e9|yt = i)\n(3.7)\n=\nX\ny\u27e8t+1...T\u27e9\nT\nY\nt\u2032=t+1\n\u03a8t\u2032(yt\u2032, yt\u2032\u22121, xt\u2032),\n(3.8)\nand the recursion\n\u03b2t(i) =\nX\nj\u2208S\n\u03a8t+1(j, i, xt+1)\u03b2t+1(j),\n(3.9)\nwhich is initialized \u03b2T(i) = 1. Analogously to the forward case, we\ncan compute p(x) using the backward variables as p(x) = \u03b20(y0)\ndef=\nP\ny1 \u03a81(y1, y0, x1)\u03b21(y1).\nBy combining results from the forward and backward recursions,\nwe can compute the marginal distributions p(yt\u22121, yt|x) needed for the\ngradient (4.6). This can be seen from either the probabilistic or the\nfactorization perspectives. First, taking a probabilistic viewpoint we\ncan write\np(yt\u22121, yt|x) = p(x|yt\u22121, yt)p(yt, yt\u22121)\np(x)\n(3.10)\n= p(x\u27e81...t\u22121\u27e9, yt\u22121)p(yt|yt\u22121)p(xt|yt)p(x\u27e8t+1...T\u27e9|yt)\np(x)\n(3.11)\n\u221d\u03b1t\u22121(yt\u22121)\u03a8t(yt, yt\u22121, xt)\u03b2t(yt),\n(3.12)\nwhere in the second line we have used the fact that x\u27e81...t\u22121\u27e9is indepen-\ndent from x\u27e8t+1...T\u27e9and from xt given yt\u22121, yt. Equivalently, from the\nfactorization perspective, we can apply the distributive law to obtain\nwe see that\np(yt\u22121, yt, x) = \u03a8t(yt, yt\u22121, xt)\n\uf8eb\n\uf8ed\nX\ny\u27e81...t\u22122\u27e9\nt\u22121\nY\nt\u2032=1\n\u03a8t\u2032(yt\u2032, yt\u2032\u22121, xt\u2032)\n\uf8f6\n\uf8f8\n\uf8eb\n\uf8ed\nX\ny\u27e8t+1...T\u27e9\nT\nY\nt\u2032=t+1\n\u03a8t\u2032(yt\u2032, yt\u2032\u22121, xt\u2032)\n\uf8f6\n\uf8f8,\n(3.13)\nwhich can be computed from the forward and backward recursions as\np(yt\u22121, yt, x) = \u03b1t\u22121(yt\u22121)\u03a8t(yt, yt\u22121, xt)\u03b2t(yt).\n(3.14)\nOnce we have p(yt\u22121, yt, x), we can renormalize over yt, yt\u22121 to obtain\nthe desired marginal p(yt\u22121, yt|x).\nFinally, to compute the globally most probable assignment y\u2217=\narg maxy p(y|x), we observe that the trick in (3.3) still works if all\nthe summations are replaced by maximization. This yields the Viterbi\nrecursion:\n\u03b4t(j) = max\ni\u2208S \u03a8t(j, i, xt)\u03b4t\u22121(i)\n(3.15)\nNow that we have described the forward-backward and Viterbi\nalgorithms for HMMs, the generalization to linear-chain CRFs is\nfairly straightforward. The forward-backward algorithm for linear-chain\nCRFs is identical to the HMM version, except that the transition\nweights \u03a8t(j, i, xt) are de\ufb01ned di\ufb00erently. We observe that the CRF\nmodel (2.16) can be rewritten as:\np(y|x) =\n1\nZ(x)\nT\nY\nt=1\n\u03a8t(yt, yt\u22121, xt),\n(3.16)\nwhere we de\ufb01ne\n\u03a8t(yt, yt\u22121, xt) = exp\n(X\nk\n\u03b8kfk(yt, yt\u22121, xt)\n)\n.\n(3.17)\nWith that de\ufb01nition, the forward recursion (3.6), the backward re-\ncursion (3.9), and the Viterbi recursion (3.15) can be used unchanged\nfor linear-chain CRFs. Instead of computing p(x) as in an HMM, in a\nCRF the forward and backward recursions compute Z(x).\nWe mention three more specialised inference tasks that can also be\nsolved using direct analogues of the HMM algorithms. First, assign-\nments to y can be sampled from the joint posterior p(y|x) using the\nforward algorithm combined with a backward sampling place, in exactly\nthe same way as an HMM. Second, if instead of \ufb01nding the single best\nassignment arg maxy p(y|x), we wish to \ufb01nd the k assignments with\nhighest probability, we can do this also using the standard algorithms\nfrom HMMs. Finally, sometimes it is useful to compute a marginal prob-\nability p(yt, yt+1, . . . yt+k|x) over a possibly non-contiguous range of\nnodes. For example, this is useful for measuring the model\u2019s con\ufb01dence\nin its predicted labeling over a segment of input. This marginal proba-\nbility can be computed e\ufb03ciently using constrained forward-backward,\nas described by Culotta and McCallum [25].\n3.2\nInference in Graphical Models\nExact inference algorithms for general graphs exist. Although these al-\ngorithms require exponential time in the worst case, they can still be\ne\ufb03cient for graphs that occur in practice. The most popular exact algo-\nrithm, the junction tree algorithm, successively clusters variables until\nthe graph becomes a tree. Once an equivalent tree has been constructed,\nits marginals can be computed using exact inference algorithms that\nare speci\ufb01c to trees. However, for certain complex graphs, the junction\ntree algorithm is forced to make clusters which are very large, which\nis why the procedure still requires exponential time in the worst case.\nFor more details on exact inference, see Koller and Friedman [49].\nFor this reason, an enormous amount of e\ufb00ort has been devoted to\napproximate inference algorithms. Two classes of approximate inference\nalgorithms have received the most attention: Monte Carlo algorithms\nand variational algorithms. Monte Carlo algorithms are stochastic al-\ngorithms that attempt to approximately produce a sample from the\ndistribution of interest. Variational algorithms are algorithms that con-\nvert the inference problem into an optimization problem, by attempting\nto \ufb01nd a simple distribution that most closely matches the intractable\ndistribution of interest. Generally, Monte Carlo algorithms are unbiased\nin the sense that they guaranteed to sample from the distribution of\ninterest given enough computation time, although it is usually impos-\nsible in practice to know when that point has been reached. Variational\nalgorithms, on the other hand, can be much faster, but they tend to\nbe biased, by which we mean that they tend to have a source of error\nthat is inherent to the approximation, and cannot be easily lessened\nby giving them more computation time. Despite this, variational algo-\nrithms can be useful for CRFs, because parameter estimation requires\nperforming inference many times, and so a fast inference procedure is\nvital to e\ufb03cient training.\nIn the remainder of this section, we outline two examples of ap-\nproximate inference algorithms, one from each of these two categories.\nToo much work has been done on approximate inference for us to at-\ntempt to summarize it here. Rather, our aim is to highlight the general\nissues that arise when using approximate inference algorithms within\nCRF training. In this chapter, we focus on describing the inference al-\ngorithms themselves, whereas in Chapter 4 we discuss their application\nto CRFs.\n3.2.1\nMarkov Chain Monte Carlo\nCurrently the most popular type of Monte Carlo method for complex\nmodels is Markov Chain Monte Carlo (MCMC) [101]. Rather than\nattempting to approximate a marginal distribution p(ys|x) directly,\nMCMC methods generate approximate samples from the joint distri-\nbution p(y|x). MCMC methods work by constructing a Markov chain,\nwhose state space is the same as that of Y , in careful way so that when\nthe chain is simulated for a long time, the distribution over states of\nthe chain is approximately p(ys|x). Suppose that we want to approxi-\nmate the expectation of some function f(x, y) that depends on. Given\na sample y1, y2, . . . , yM from a Markov chain in an MCMC method,\nwe can approximate this expectation as:\nX\ny\np(y|x)f(x, y) \u22481\nM\nM\nX\nj=1\nf(x, yj)\n(3.18)\nFor example, in the context of CRFs, these approximate expectations\ncan then be used to approximate the quantities required for learning,\nspeci\ufb01cally the gradient (4.6).\nA simple example of an MCMC method is Gibbs sampling. In each\niteration of the Gibbs sampling algorithm, each variable is resampled\nindividually, keeping all of the other variables \ufb01xed. Suppose that we\nalready have a sample yj from iteration j. Then to generate the next\nsample yj+1,\n(1) Set yj+1 \u2190yj.\n(2) For each s \u2208V , resample component s. Sample yj+1\ns\nfrom\nthe distribution p(ys|y\\s, x).\n(3) Return the resulting value of yj+1.\nThis procedure de\ufb01nes a Markov chain that can be used to approx-\nimation expectations as in (3.18). In the case of general CRFs, then\nusing the notation from Section 2.4, this conditional probability can be\ncomputed as\np(ys|y\\s, x) = \u03ba\nY\nCp\u2208C\nY\n\u03a8c\u2208Cp\n\u03a8c(xc, yc; \u03b8p),\n(3.19)\nwhere \u03ba is a normalizing constant. This is much easier to compute than\nthe joint probability p(y|x), because computing \u03ba requires a summation\nonly over all possible values of ys rather than assignments to the full\nvector y.\nA major advantage of Gibbs sampling is that it is simple to imple-\nment. Indeed, software packages such as BUGS can take a graphical\nmodel as input and automatically compile an appropriate Gibbs sam-\npler [66]. The main disadvantage of Gibbs sampling is that it can work\npoorly if p(y|x) has strong dependencies, which is often the case in\nsequential data. By \u201cworks poorly\u201d we mean that it may take many\niterations before the distribution over samples from the Markov chain\nis close to the desired distribution p(y|x).\nThere is an enormous literature on MCMC algorithms. The text-\nbook by Robert and Casella [101] provides an overview. However,\nMCMC algorithms are not commonly applied in the context of con-\nditional random \ufb01elds. Perhaps the main reason for this is that as we\nhave mentioned earlier, parameter estimation by maximum likelihood\nrequires calculating marginals many times. In the most straightforward\napproach, one MCMC chain would be run for each training example\nfor each parameter setting that is visited in the course of a gradient de-\nscent algorithm. Since MCMC chains can take thousands of iterations\nto converge, this can be computationally prohibitive. One can imagine\nways of addressing this, such as not running the chain all the way to\nconvergence (see Section 4.4.3).\n3.2.2\nBelief Propagation\nAn important variational inference algorithm is belief propagation\n(BP), which we explain in this section. In addition, it is a direct gen-\neralization of the exact inference algorithms for linear-chain CRFs.\nSuppose that G is a tree, and we wish to compute the marginal\ndistribution of a variable s. The intuition behind BP is that each of\nthe neighboring factors of s makes a multiplicative contribution to the\nmarginal of s, called a message, and each of these messages can be\ncomputed separately because the graph is a tree. More formally, for\nevery factor a \u2208N(s), call Va the set of variables that are \u201cupstream\u201d\nof a, that is, the set of variables v for which a is between s and v.\nIn a similar fashion, call Fa the set of factors that are upstream of a,\nincluding a itself. But now because G is a tree, the sets {Va} \u222a{s}\nform a partition of the variables in G. This means that we can split up\nthe summation required for the marginal into a product of independent\nsubproblems as:\np(ys) \u221d\nX\ny\\ys\nY\na\n\u03a8a(ya)\n(3.20)\n=\nY\na\u2208N(s)\nX\nyVa\nY\n\u03a8b\u2208Fa\n\u03a8b(yb)\n(3.21)\nDenote each factor in the above equation by mas, that is,\nmas(xs) =\nX\nyVa\nY\n\u03a8b\u2208Fa\n\u03a8b(yb),\n(3.22)\ncan be thought of as a message from the factor a to the variable s that\nsummarizes the impact of the network upstream of a on the belief in s.\nIn a similar fashion, we can de\ufb01ne messages from variables to factors\nas\nmsA(xs) =\nX\nyVs\nY\n\u03a8b\u2208Fs\n\u03a8b(yb).\n(3.23)\nThen, from (3.21), we have that the marginal p(ys) is proportional to\nthe product of all the incoming messages to variable s. Similarly, factor\nmarginals can be computed as\np(ya) \u221d\u03a8a(ya)\nY\ns\u2208a\nmsa(ya).\n(3.24)\nHere we treat a as a set a variables denoting the scope of factor \u03a8a,\nas we will throughout. In addition, we will sometimes use the reverse\nnotation c \u220bs to mean the set of all factors c that contain the variable\ns.\nNaively computing the messages according to (3.22) is impractical,\nbecause the messages as we have de\ufb01ned them require summation over\npossibly many variables in the graph. Fortunately, the messages can\nalso be written using a recursion that requires only local summation.\nThe recursion is\nmas(xs) =\nX\nya\\ys\n\u03a8a(ya)\nY\nt\u2208a\\s\nmta(xt)\nmsa(xs) =\nY\nb\u2208N(s)\\a\nmbs(xs)\n(3.25)\nThat this recursion matches the explicit de\ufb01nition of m can be seen by\nrepeated substitution, and proven by induction. In a tree, it is possible\nto schedule these recursions such that the antecedent messages are\nalways sent before their dependents, by \ufb01rst sending messages from\nthe root, and so on. This is the algorithm known as belief propagation\n[88].\nIn addition to computing single-variable marginals, we will also wish\nto compute factor marginals p(ya) and joint probabilites p(y) for a\ngiven assignment y. (Recall that the latter problem is di\ufb03cult because\nit requires computing the partition function log Z.) First, to compute\nmarginals over factors\u2014or over any connected set of variables, in fact\u2014\nwe can use the same decomposition of the marginal as for the single-\ny1\ny2\ny3\n\u03a8A\n\u03a8B\nmA2\nmB2\nmC2\n\u03a8C\nFig. 3.1 Illustration of the correspondence between forward backward and belief propaga-\ntion in linear chain graphs\nvariable case, and get\np(ya) = \u03ba\u03a8a(ya)\nY\ns\u2208a\nmsa(ys),\n(3.26)\nwhere \u03ba is a normalization constant. In fact, a similar idea works for\nany connected set of variables\u2014not just a set that happens to be the\ndomain of some factor\u2014although if the set is too large, then computing\n\u03ba is impractical.\nBP can also be used to compute the normalizing constant Z(x). This\ncan be done directly from the propagation algorithm, in an analogous\nway to the forward-backward algorithm in Section 3.1. Alternatively,\nthere is another way to compute Z(x) from only the beliefs at the end\nof the algorithm. In a tree structured distribution, it is always true that\np(y) =\nY\ns\u2208V\np(ys)\nY\na\np(ya)\nQ\nt\u2208a p(yt)\n(3.27)\nFor example, in a linear chain this amounts to\np(y) =\nT\nY\nt=1\np(yt)\nT\nY\nt=1\np(yt, yt\u22121)\np(yt)p(yt\u22121),\n(3.28)\nwhich, after cancelling and rearranging terms, is just another way to\nwrite the familiar equation p(y) = Q\nt p(yt|yt\u22121). More generally, (3.27)\ncan be derived using the junction tree theorem, by considering a junc-\ntion tree with one cluster for each factor. Using this identity, we can\ncompute p(y) (or log Z) from the per-variable and per-factor marginals.\nIf G is a tree, belief propagation computes the marginal distribu-\ntions exactly. Indeed, if G is a linear chain, then BP reduces to the\nforward-backward algorithm (Section 3.1). To see this, refer to Fig-\nure 3.1. The \ufb01gure shows a three node linear chain along with the BP\nmessages as we have described them in this section. To see the corre-\nspondence to forward backward, the forward message that we denoted\n\u03b12 in Section 3.1 corresponds to the product of the two messages mA2\nand mC2 (the thick, dark blue arrows in the \ufb01gure). The backward\nmessage \u03b22 corresponds to the message mB2 (the thick, light orange\narrow in the \ufb01gure).\nIf G is not a tree, the message updates (3.25) are no longer guar-\nanteed to return the exact marginals, nor are they guaranteed even to\nconverge, but we can still iterate them in an attempt to \ufb01nd a \ufb01xed\npoint. This procedure is called loopy belief propagation. To emphasize\nthe approximate nature of this procedure, we refer to the approximate\nmarginals that result from loopy BP as beliefs rather than as marginals,\nand denote them by q(ys).\nSurprisingly, loopy BP can be seen as a variational method for in-\nference, meaning that there actually exists an objective function over\nbeliefs that is approximately minimized by the iterative BP procedure.\nSeveral introductory papers [137, 131] describe this in more detail.\nThe general idea behind a variational algorithm is:\n(1) De\ufb01ne a family of tractable distributions Q and an objective\nfunction O(q). The function O should be designed to measure\nhow well a tractable distribution q \u2208Q approximates the\ndistribution p of interest.\n(2) Find the \u201cclosest\u201d tractable distribution q\u2217= minq\u2208Q O(q).\n(3) Use the marginals of q\u2217to approximate those of p.\nFor example, suppose that we take Q be the set of all possible distri-\nbutions over y, and we choose the objective function\nO(q) = KL(q\u2225p) \u2212log Z\n(3.29)\n= \u2212H(q) \u2212\nX\na\nq(ya) log \u03a8a(ya).\n(3.30)\nThen the solution to this variational problem is q\u2217= p with optimal\nvalue O(q\u2217) = log Z. Solving this particular variational formulation is\nthus equivalent to performing exact inference. Approximate inference\ntechniques can be devised by changing the set Q\u2014for example, by\nrequiring q to be fully factorized\u2014or by using a di\ufb00erent objective O.\nFor example, the mean \ufb01eld method arises by requiring q to be fully\nfactorized, i.e., q(y) = Q\ns qs(ys) for some choice for qs, and \ufb01nding the\nfactorized q that most closely matches p.\nWith that background on variational methods, let us see how belief\npropagation can be understood in this framework. We make two ap-\nproximations. First, we approximate the entropy term H(q) of (3.30),\nwhich as it stands is di\ufb03cult to compute. If q were a tree-structured\ndistribution, then its entropy could be written exactly as\nHBethe(q) =\nX\na\nq(ya) log q(ya) +\nX\ni\n(1 \u2212di)q(yi) log q(yi).\n(3.31)\nThis follows from substituting the junction-tree formulation (3.27) of\nthe joint into the de\ufb01nition of entropy. If q is not a tree, then we can still\ntake HBethe as an approximation to H to compute the exact variational\nobjective O. This yields the Bethe free energy:\nOBethe(q) = HBethe(q) \u2212\nX\na\nq(ya) log \u03a8a(ya)\n(3.32)\nThe objective OBethe depends on q only through its marginals, so rather\nthan optimizing it over all probability distributions q, we can optimize\nover the space of all marginal vectors. Speci\ufb01cally, every distribution q\nhas an associated belief vector q, with elements qa;ya for each factor a\nand assignment ya, and elements qi;yi for each variable i and assignment\nyi. The space of all possible belief vectors has been called the marginal\npolytope [130]. However, for intractable models, the marginal polytope\ncan have extremely complex structure.\nThis leads us to the second variational approximation made by loopy\nBP, namely that the objective OBethe is optimized instead over a relax-\nation of the marginal polytope. The relaxation is to require that the\nbeliefs be only locally consistent, that is, that\nX\nya\\yi\nqa(ya) = qi(yi)\n\u2200a, i \u2208a\n(3.33)\nUnder these constraints, Yedidia et al. [136] show that constrained\nstationary points of OBethe \ufb01xed points of loopy BP. So we can view\nthe Bethe energy OBethe as an objective function that the loopy BP\n\ufb01xed-point operations attempt to optimize.\nThis variational perspective provides new insight into the method\nthat would not be available if we thought of it solely from the mes-\nsage passing perspective. One of the most important insights is that\nit shows how to use loopy BP to approximate log Z. Because we in-\ntroduced minq OBethe(q) as an approximation to minq O(q), and we\nknow that minq O(q) = log Z, then it seems reasonable to de\ufb01ne\nlog ZBethe = minq OBethe(q) as an approximation to log Z. This will be\nimportant when we discuss CRF parameter estimation using BP in\nSection 4.4.2.\n3.3\nImplementation Concerns\nIn this section, we mention a few implementation techniques that are\nimportant to practical inference in CRFs: sparsity and preventing nu-\nmerical under\ufb02ow.\nFirst, it is often possible to exploit sparsity in the model to make\ninference more e\ufb03cient. Two di\ufb00erent types of sparsity are relevant:\nsparsity in the factor values, and sparsity in the features. First, about\nthe factor values, recall that in the linear-chain case, each of the for-\nward updates (3.6) and backward updates (3.9) requires O(M2) time,\nthat is, quadratic time in the number of labels. Analogously, in general\nCRFs, an update of loopy BP in a model with pairwise factors requires\nO(M2) time. In some models, however, it is possible to implement in-\nference more e\ufb03ciently, because it is known a priori not all factor values\n(yt, yt\u22121) are feasible, that is, the factor \u03a8t(yt, yt+1, xt) is 0 for many\nvalues yt, yt+1. In such cases, the computational cost of sending a mes-\nsage can be reduced by implementing the message-passing iterations\nusing sparse matrix operations.\nThe second kind of sparsity that is useful is sparsity in the feature\nvectors. Recall from (2.20) that computing the factors \u03a8c(xc, yc) re-\nquires computing a dot product between the parameter vector \u03b8p and\nand the vector of features Fc = {fpk(yc, xc)}. Often, many elements\nof the vectors Fc are zero. For example, natural language applications\noften involve binary indicator variables on word identity. In this case,\nthe time required to compute the factors \u03a8c can be greatly improved\nusing a sparse vector representation. In a similar fashion, we can use\nsparsity improve the time required to compute the likelihood gradient,\nas we discuss in Chapter 4.\nA related trick, that will also speed up forward backward, is to tie\nthe parameters for certain subsets of transitions [20]. This has the e\ufb00ect\nof reducing the e\ufb00ective size of the model\u2019s transition matrix, lessening\nthe e\ufb00ect of the quadratic dependence of the size of the label set.\nA second implementation concern that arises in inference is avoiding\nnumerical under\ufb02ow. The probabilities involved in forward-backward\nand belief propagation are often too small to be represented within\nnumerical precision (for example, in an HMM they decay toward 0\nexponentially fast in T). There are two standard approaches to this\ncommon problem. One approach is to scale each of the vectors \u03b1t and\n\u03b2t to sum to 1, thereby magnifying small values. This scaling does\nnot a\ufb00ect our ability to compute Z(x) because it can be computed as\nZ(x) = p(y\u2032|x)\u22121 Q\nt(\u03a8t(y\u2032\nt, y\u2032\nt+1, xt)) for an arbitrary assignment y\u2032,\nwhere p(y\u2032|x)\u22121 is computed from the marginals using (3.27). But in\nfact, there is actually a more e\ufb03cient method described by Rabiner [96]\nthat involves saving each of the local scaling factors. In any case, the\nscaling trick can be used in forward-backward or loopy BP; in either\ncase, it does not a\ufb00ect the \ufb01nal values of the beliefs.\nA second approach to preventing under\ufb02ow is to perform compu-\ntations in the logarithmic domain, e.g., the forward recursion (3.6)\nbecomes\nlog \u03b1t(j) =\nM\ni\u2208S\n\u0000log \u03a8t(j, i, xt) + log \u03b1t\u22121(i)\n\u0001\n,\n(3.34)\nwhere \u2295is the operator a \u2295b = log(ea + eb). At \ufb01rst, this does not\nseem much of an improvement, since numerical precision is lost when\ncomputing ea and eb. But \u2295can be computed as\na \u2295b = a + log(1 + eb\u2212a) = b + log(1 + ea\u2212b),\n(3.35)\nwhich can be much more numerically stable, particularly if we pick the\nversion of the identity with the smaller exponent.\nAt \ufb01rst, it would seem that the normalization approach is prefer-\nable to the logarithmic approach, because the logarithmic approach\nrequires O(TM2) calls to the special functions log and exp, which can\nbe computationally expensive. This observation is correct for HMMs,\nbut not for CRFs. In a CRF, even when the normalization approach is\nused, it is still necessary to call the exp function in order to compute\n\u03a8t(yt, yt+1, xt), de\ufb01ned in (3.17). So in CRFs, special functions can-\nnot be avoided. In the worst case, there are TM2 of these \u03a8t values, so\nthe normalization approach needs TM2 calls to special functions just as\nthe logarithmic domain approach does. However, there are some special\ncases in which the normalization approach can yield a speedup, such\nas when the transition features do not depend on the observations, so\nthat there are only M2 distinct \u03a8t values.\n4\nParameter Estimation\nIn this chapter we discuss how to estimate the parameters \u03b8 = {\u03b8k}\nof a conditional random \ufb01eld. In the simplest and typical case, we are\nprovided with fully labeled independent data, but there has also been\nwork in CRFs with latent variables and CRFs for relational learning.\nCRFs are trained by maximum likelihood, that is, the parameters\nare chosen such that the training data has highest probability under\nthe model. In principle, this can be done in a manner exactly analogous\nto logistic regression, which should not be surprising given the close re-\nlationship between these models that was described in Chapter 2. The\nmain di\ufb00erence is computational: CRFs tend to have more parame-\nters and more complex structure than a simple classi\ufb01er, so training is\ncorrespondingly more expensive.\nIn tree structured CRFs, the maximum likelihood parameters can\nbe found by a numerical optimization procedure that calls the infer-\nence algorithms of Section 3.1 as a subroutine. Crucially, the likelihood\nis a convex function of the parameters, which means that powerful\noptimization procedures are available that provably converge to the\noptimal solution. For general CRFs, on the other hand, maximum like-\nlihood training is intractable. One way to deal with this problem is\n43\nto use approximate inference methods, as discussed in Chapter 3, but\nanother way is to choose a di\ufb00erent training criterion than maximum\nlikelihood.\nWe begin by describing maximum likelihood training, both in the\nlinear chain case (Section 4.1.1) and in the case of general graphical\nstructures (Section 4.1.2), including the case of latent variables. Then\nwe discuss training in general graphical structures, in which approxima-\ntions are necessary. We also describe two general methods for speed-\ning up parameter estimation that exploit iid structure in the data:\nstochastic gradient descent (Section 4.2) and multithreaded training\n(Section 4.3). In CRFs with general structure, typically approximate\ninference procedures must be used. The approximate training proce-\ndures build on the approximate algorithms for inference described in\nChapter 3, but there can be complications in the interaction between\napproximate inference and learning. This is described in Section 4.4.\n4.1\nMaximum Likelihood\n4.1.1\nLinear-chain CRFs\nIn a linear-chain CRF, the maximum likelihood parameters can be\ndetermined using numerical optimization methods. We are given iid\ntraining data D = {x(i), y(i)}N\ni=1, where each x(i) = {x(i)\n1 , x(i)\n2 , . . . x(i)\nT }\nis a sequence of inputs, and each y(i) = {y(i)\n1 , y(i)\n2 , . . . y(i)\nT } is a sequence\nof the desired predictions.\nParameter estimation is typically performed by penalized maximum\nlikelihood. Because we are modeling the conditional distribution, the\nfollowing log likelihood, sometimes called the conditional log likelihood,\nis appropriate:\n\u2113(\u03b8) =\nN\nX\ni=1\nlog p(y(i)|x(i)).\n(4.1)\nOne way to understand the conditional likelihood p(y|x; \u03b8) is to imagine\ncombining it with some arbitrary prior p(x; \u03b8\u2032) to form a joint p(y, x).\nThen when we optimize the joint log likelihood\nlog p(y, x) = log p(y|x; \u03b8) + log p(x; \u03b8\u2032),\n(4.2)\nthe two terms on the right-hand side are decoupled, that is, the value of\n\u03b8\u2032 does not a\ufb00ect the optimization over \u03b8. If we do not need to estimate\np(x), then we can simply drop the second term, which leaves (4.1).\nAfter substituting in the CRF model (2.16) into the likelihood (4.1),\nwe get the following expression:\n\u2113(\u03b8) =\nN\nX\ni=1\nT\nX\nt=1\nK\nX\nk=1\n\u03b8kfk(y(i)\nt , y(i)\nt\u22121, x(i)\nt ) \u2212\nN\nX\ni=1\nlog Z(x(i)),\n(4.3)\nIt is often the case that we have a large number of parameters, e.g.,\nseveral hundred thousand. As a measure to avoid over\ufb01tting, we use\nregularization, which is a penalty on weight vectors whose norm is too\nlarge. A common choice of penalty is based on the Euclidean norm of \u03b8\nand on a regularization parameter 1/2\u03c32 that determines the strength\nof the penalty. Then the regularized log likelihood is\n\u2113(\u03b8) =\nN\nX\ni=1\nT\nX\nt=1\nK\nX\nk=1\n\u03b8kfk(y(i)\nt , y(i)\nt\u22121, x(i)\nt )\u2212\nN\nX\ni=1\nlog Z(x(i))\u2212\nK\nX\nk=1\n\u03b82\nk\n2\u03c32 . (4.4)\nThe parameter \u03c32 is a free parameter which determines how much to\npenalize large weights. Intuitively, the idea is to reduce the potential\nfor a small number of features to dominate the prediction. The nota-\ntion for the regularizer is intended to suggest that regularization can\nalso be viewed as performing maximum a posteriori (MAP) estima-\ntion of \u03b8, if \u03b8 is assigned a Gaussian prior with mean 0 and covari-\nance \u03c32I. Determining the best regularization parameter can require a\ncomputationally-intensive parameter sweep. Fortunately, often the ac-\ncuracy of the \ufb01nal model is not sensitive to small changes in \u03c32 (e.g.,\nup to a factor of 10). The best value of \u03c32 depends on the size of the\ntraining set; for medium-sized training sets, \u03c32 = 10 is typical.\nAn alternative choice of regularization is to use the L1 norm instead\nof the Euclidean norm, which corresponds to an exponential prior on\nparameters [37]. This results in the following penalized likelihood:\n\u2113\u2032(\u03b8) =\nN\nX\ni=1\nT\nX\nt=1\nK\nX\nk=1\n\u03b8kfk(y(i)\nt , y(i)\nt\u22121, x(i)\nt ) \u2212\nN\nX\ni=1\nlog Z(x(i)) \u2212\u03b1\nK\nX\nk=1\n|\u03b8k|.\n(4.5)\nThis regularizer tends to encourage sparsity in the learned parameters,\nmeaning that most of the \u03b8k are 0. This can be useful for performing\nfeature selection, and also has theoretical advantages [84]. In practice,\nmodels trained with the L1 regularizer tend to be sparser but have\nroughly the same accuracy as models training using the L2 regularizer\n[56]. A disadvantage of the L1 regularizer is that it is not di\ufb00erentiable\nat 0, which complicates numerical parameter estimation somewhat [37,\n3, 138].\nIn general, the function \u2113(\u03b8) cannot be maximized in closed form,\nso numerical optimization is used. The partial derivatives of (4.4) are\n\u2202\u2113\n\u2202\u03b8k\n=\nN\nX\ni=1\nT\nX\nt=1\nfk(y(i)\nt , y(i)\nt\u22121, x(i)\nt )\u2212\nN\nX\ni=1\nT\nX\nt=1\nX\ny,y\u2032\nfk(y, y\u2032, x(i)\nt )p(y, y\u2032|x(i))\u2212\u03b8k\n\u03c32 .\n(4.6)\nThe \ufb01rst term is the expected value of fk under the empirical distribu-\ntion:\n\u02dcp(y, x) = 1\nN\nN\nX\ni=1\n1{y=y(i)}1{x=x(i)}.\n(4.7)\nThe second term, which arises from the derivative of log Z(x), is the\nexpectation of fk under the model distribution p(y|x; \u03b8)\u02dcp(x). Therefore,\nat the unregularized maximum likelihood solution, when the gradient\nis zero, these two expectations are equal. This pleasing interpretation is\na standard result about maximum likelihood estimation in exponential\nfamilies.\nTo compute the likelihood \u2113(\u03b8) and its derivative requires techniques\nfrom inference in graphical models. In the likelihood, inference is needed\nto compute the partition function Z(x(i)), which is a sum over all pos-\nsible labellings. In the derivatives, inference is required to compute the\nmarginal distributions p(y, y\u2032|x(i)). Because both of these quantities\ndepend on x(i), we will need to run inference once for each training\ninstance every time the likelihood is computed. This is the key compu-\ntational di\ufb00erence between CRFs and generative Markov random \ufb01elds.\nIn linear-chain models, inference can be performed e\ufb03ciently using the\nalgorithms described in Section 3.1.\nNow we discuss how to optimize \u2113(\u03b8). The function \u2113(\u03b8) is con-\ncave, which follows from the convexity of functions of the form g(x) =\nlog P\ni exp xi. Convexity is extremely helpful for parameter estimation,\nbecause it means that every local optimum is also a global optimum.\nAdding regularization ensures that \u2113is strictly concave, which implies\nthat it has exactly one global optimum.\nPerhaps the simplest approach to optimize \u2113is steepest ascent along\nthe gradient (4.6), but this requires too many iterations to be practical.\nNewton\u2019s method converges much faster because it takes into account\nthe curvature of the likelihood, but it requires computing the Hessian,\nthe matrix of all second derivatives. The size of the Hessian is quadratic\nin the number of parameters. Since practical applications often use tens\nof thousands or even millions of parameters, simply storing the full\nHessian is not practical.\nInstead, current techniques for optimizing (4.4) make approximate\nuse of second-order information. Particularly successful have been\nquasi-Newton methods such as BFGS [6], which compute an approx-\nimation to the Hessian from only the \ufb01rst derivative of the objective\nfunction. A full K \u00d7 K approximation to the Hessian still requires\nquadratic size, however, so a limited-memory version of BFGS is used,\ndue to Byrd et al. [14]. Conjugate gradient is another optimization tech-\nnique that also makes approximate use of second-order information and\nhas been used successfully with CRFs. For a good introduction to both\nlimited-memory BFGS and conjugate gradient, see Nocedal and Wright\n[87]. Either can be thought of as a black-box optimization routine that\nis a drop-in replacement for vanilla gradient ascent. When such second-\norder methods are used, gradient-based optimization is much faster\nthan the original approaches based on iterative scaling in La\ufb00erty et al.\n[54], as shown experimentally by several authors [108, 132, 68, 79]. Fi-\nnally, trust region methods have recently been shown to perform well\non multinomial logistic regression [63], and may work well for CRFs as\nwell.\nFinally, we discuss the computational cost of training linear chain\nmodels. As we will see in Section 3.1, the likelihood and gradient for\na single training instance can be computed by forward-backward in\ntime O(TM2), where M is the number of labels and T the length of\nthe training instance. Because we need to run forward-backward for\neach training instance, each computation of the likelihood and gra-\ndient requires O(TM2N) time, so that the total cost of training is\nO(TM2NG), where G the number of gradient computations required\nby the optimization procedure. Unfortunately, G depends on the data\nset and is di\ufb03cult to predict in advance. For batch L-BFGS on linear-\nchain CRFs, it is often but not always under 100. For many data sets,\nthis cost is reasonable, but if the number of states M is large, or the\nnumber of training sequences N is very large, then this can become\nexpensive. Depending on the number of labels, training CRFs can take\nanywhere from a few minutes to a few days; see Section 4.5 for exam-\nples.\n4.1.2\nGeneral CRFs\nParameter estimation for general CRFs is essentially the same as for\nlinear-chains, except that computing the model expectations requires\nmore general inference algorithms. First, we discuss the fully-observed\ncase, in which the training and testing data are independent, and the\ntraining data is fully observed. In this case the conditional log likeli-\nhood, using the notation of Section 2.4, is\n\u2113(\u03b8) =\nX\nCp\u2208C\nX\n\u03a8c\u2208Cp\nK(p)\nX\nk=1\n\u03b8pkfpk(xc, yc) \u2212log Z(x).\n(4.8)\nThe equations in this section do not explicitly sum over training in-\nstances, because if a particular application happens to have iid training\ninstances, they can be represented by disconnected components in the\ngraph G.\nThe partial derivative of the log likelihood with respect to a param-\neter \u03b8pk associated with a clique template Cp is\n\u2202\u2113\n\u2202\u03b8pk\n=\nX\n\u03a8c\u2208Cp\nfpk(xc, yc) \u2212\nX\n\u03a8c\u2208Cp\nX\ny\u2032c\nfpk(xc, y\u2032\nc)p(y\u2032\nc|x).\n(4.9)\nThe function \u2113(\u03b8) has many of the same properties as in the linear-chain\ncase. First, the zero-gradient conditions can be interpreted as requiring\nthat the su\ufb03cient statistics Fpk(x, y) = P\n\u03a8c fpk(xc, yc) have the same\nexpectations under the empirical distribution and under the model dis-\ntribution. Second, the function \u2113(\u03b8) is concave, and can be e\ufb03ciently\nmaximized by second-order techniques such as conjugate gradient and\nL-BFGS. Finally, regularization is used just as in the linear-chain case.\nAll of the discussion so far has assumed that the training data con-\ntains the true values of all the label variables in the model. In the latent\nvariable case, on the other hand, the model contains variables that are\nobserved at neither training nor test time. This situation is called a\nhidden-state CRF (HCRF) by Quattoni et al. [95] which was one of\nthe \ufb01rst examples of latent variable CRFs. Quattoni et al. [94] present\na more detailed description. For other early applications of HCRFs,\nsee [120, 75]. It is more di\ufb03cult to train CRFs with latent variables\nbecause the latent variables need to be marginalized out to compute\nthe likelihood. Because of this di\ufb03cultly, the original work on CRFs\nfocused on fully-observed training data, but recently there has been\nincreasing interest in HCRFs.\nSuppose we have a conditional random \ufb01eld with inputs x in which\nthe output variables y are observed in the training data, but we have\nadditional variables w that are latent, so that the CRF has the form\np(y, w|x) =\n1\nZ(x)\nY\nCp\u2208C\nY\n\u03a8c\u2208Cp\n\u03a8c(xc, wc, yc; \u03b8p).\n(4.10)\nA natural objective function to maximize during training is the\nmarginal likelihood\n\u2113(\u03b8) = log p(y|x) = log\nX\nw\np(y, w|x).\n(4.11)\nThe \ufb01rst question is how even to compute the marginal likelihood \u2113(\u03b8),\nbecause if there are many variables w, the sum cannot be computed di-\nrectly. The key is to realize that we need to compute log P\nw p(y, w|x)\nnot for any possible assignment y, but only for the particular assign-\nment that occurs in the training data. This motivates taking the origi-\nnal CRF (4.10), and clamping the variables Y to their observed values\nin the training data, yielding a distribution over w:\np(w|y, x) =\n1\nZ(y, x)\nY\nCp\u2208C\nY\n\u03a8c\u2208Cp\n\u03a8c(xc, wc, yc; \u03b8p),\n(4.12)\nwhere the normalization factor is\nZ(y, x) =\nX\nw\nY\nCp\u2208C\nY\n\u03a8c\u2208Cp\n\u03a8c(xc, wc, yc; \u03b8p).\n(4.13)\nThis new normalization constant Z(y, x) can be computed by the same\ninference algorithm that we use to compute Z(x). In fact, Z(y, x) is\neasier to compute, because it sums only over w, while Z(x) sums over\nboth w and y. Graphically, this amounts to saying that clamping the\nvariables y in the graph G can simplify the structure among w.\nOnce we have Z(y, x), the marginal likelihood can be computed as\np(y|x) =\n1\nZ(x)\nX\nw\nY\nCp\u2208C\nY\n\u03a8c\u2208Cp\n\u03a8c(xc, wc, yc; \u03b8p) = Z(y, x)\nZ(x) .\n(4.14)\nNow that we have a way to compute \u2113, we discuss how to maximize it\nwith respect to \u03b8. Maximizing \u2113(\u03b8) can be di\ufb03cult because \u2113is no longer\nconvex in general (log-sum-exp is convex, but the di\ufb00erence of two\nlog-sum-exp functions might not be), so optimization procedures are\ntypically guaranteed to \ufb01nd only local maxima. Whatever optimization\ntechnique is used, the model parameters must be carefully initialized\nin order to reach a good local maximum.\nWe discuss two di\ufb00erent ways to maximize \u2113: directly using the\ngradient, as in Quattoni et al. [95]; and using EM, as in McCallum et al.\n[75]. (In addition, it is also natural to use stochastic gradient descent\nhere; see Section 4.2.) To maximize \u2113directly, we need to calculate its\ngradient. The simplest way to do this is to use the following fact. For\nany function f(\u03b8), we have\ndf\nd\u03b8 = f(\u03b8)d log f\nd\u03b8\n,\n(4.15)\nwhich can be seen by applying the chain rule to log f and rearranging.\nApplying this to the marginal likelihood \u2113(\u03b8) = log P\nw p(y, w|x) yields\n\u2202\u2113\n\u2202\u03b8pk\n=\n1\nP\nw p(y, w|x)\nX\nw\n\u2202\n\u2202\u03b8pk\n\u0002\np(y, w|x)\n\u0003\n(4.16)\n=\nX\nw\np(w|y, x) \u2202\n\u2202\u03b8pk\n\u0002\nlog p(y, w|x)\n\u0003\n.\n(4.17)\nThis is the expectation of the fully-observed gradient, where the expec-\ntation is taken over w. This expression simpli\ufb01es to\n\u2202\u2113\n\u2202\u03b8pk\n=\nX\n\u03a8c\u2208Cp\nX\nw\u2032c\np(w\u2032\nc|y, x)fk(yc, xc, w\u2032\nc)\n\u2212\nX\n\u03a8c\u2208Cp\nX\nw\u2032c,y\u2032c\np(w\u2032\nc, y\u2032\nc|xc)fk(y\u2032\nc, xc, w\u2032\nc).\n(4.18)\nThis gradient requires computing two di\ufb00erent kinds of marginal proba-\nbilities. The \ufb01rst term contains a marginal probability p(w\u2032\nc|y, x), which\nis exactly a marginal distribution of the clamped CRF (4.12). The sec-\nond term contains a di\ufb00erent marginal p(w\u2032\nc, y\u2032\nc|xc), which is the same\nmarginal probability required in a fully-observed CRF. Once we have\ncomputed the gradient, \u2113can be maximized by standard techniques\nsuch as conjugate gradient. For BFGS, it has been our experience that\nthe memory-based approximation to the Hessian can become confused\nby violations of convexity, such as occur in latent-variable CRFs. One\npractical trick in this situation is to reset the Hessian approximation\nwhen that happens.\nAlternatively, \u2113can be optimized using expectation maximization\n(EM). At each iteration j in the EM algorithm, the current parame-\nter vector \u03b8(j) is updated as follows. First, in the E-step, an auxiliary\nfunction q(w) is computed as q(w) = p(w|y, x; \u03b8(j)). Second, in the\nM-step, a new parameter vector \u03b8(j+1) is chosen as\n\u03b8(j+1) = arg max\n\u03b8\u2032\nX\nw\u2032\nq(w\u2032) log p(y, w\u2032|x; \u03b8\u2032).\n(4.19)\nThe direct maximization algorithm and the EM algorithm are strikingly\nsimilar. This can be seen by substituting the de\ufb01nition of q into (4.19)\nand taking derivatives. The gradient is almost identical to the direct\ngradient (4.18). The only di\ufb00erence is that in EM, the distribution\np(w|y, x) is obtained from a previous, \ufb01xed parameter setting rather\nthan from the argument of the maximization. We are unaware of any\nempirical comparison of EM to direct optimization for latent-variable\nCRFs.\n4.2\nStochastic Gradient Methods\nSo far, all of the methods that we have discussed for optimizing the\nlikelihood work in a batch setting, meaning that they do not make any\nchange to the model parameters until they have scanned the entire\ntraining set. If the training data consist of a large number of iid sam-\nples, then this may seem wasteful. We may suspect that many di\ufb00erent\nitems in the training data provide similar information about the model\nparameters, so that it should be possible to update the parameters after\nseeing only a few examples, rather than sweeping through all of them.\nStochastic gradient descent (SGD) is a simple optimization method\nthat is designed to exploit this insight. The basic idea is at every itera-\ntion, to pick a training instance at random, and take a small step in the\ndirection given by the gradient for that instance only. In the batch set-\nting, gradient descent is generally a poor optimization method, because\nthe direction of steepest descent locally (that is, the negative gradient)\ncan point in a very di\ufb00erent direction than the optimum. So stochastic\ngradient methods involve an interesting tradeo\ufb00: the directions of the\nindividual steps may be much better in L-BFGS than in SGD, but the\nSGD directions can be computed much faster.\nIn order to keep the notation simple, we present SGD only for the\ncase of linear-chain CRFs, but it can be easily used with any graphical\nstructure, as long as the training data are iid. The gradient of the\nlikelihood for a single training instance (x(i), y(i)) is\n\u2202\u2113i\n\u2202\u03b8k\n=\nT\nX\nt=1\nfk(y(i)\nt , y(i)\nt\u22121, x(i)\nt ) \u2212\nT\nX\nt=1\nX\ny,y\u2032\nfk(y, y\u2032, x(i)\nt )p(y, y\u2032|x(i)) \u2212\n\u03b8k\nN\u03c32 .\n(4.20)\nThis is exactly the same as the full gradient (4.6), with two changes: the\nsum over training instances has been removed, and the regularization\ncontains an additional factor of 1/N. These ensure that the batch gra-\ndient equals the sum of the per-instance gradients, i.e., \u2207\u2113= PN\ni=1 \u2207\u2113i,\nwhere we use \u2207\u2113i to denote the gradient for instance i.\nAt each iteration m of SGD, we randomly select a training instance\n(x(i), y(i)). Then compute the new parameter vector \u03b8(m) from the old\nvector \u03b8(m) by\n\u03b8(m) = \u03b8(m\u22121) \u2212\u03b1m\u2207\u2113i(\u03b8(m\u22121)),\n(4.21)\nwhere \u03b1m > 0 is a step size parameter that controls how far the pa-\nrameters move in the direction of the gradient. If the step size is too\nlarge, then the parameters will swing too far in the direction of what-\never training instance is sampled at each iteration. If \u03b1m is too small,\nthen training will proceed very slowly, to the extent that in extreme\ncases, the parameters may appear to have converged numerically when\nin fact they are far from the minimum.\nWe want \u03b1m to decrease as m increases, so that the optimization\nalgorithm converges to a single answer. The most common way to\ndo this is to select a step size schedule of a form like \u03b1m \u223c1/m or\n\u03b1m \u223c1/\u221am. These choices are motivated by the classic convergence\nresults for stochastic approximation procedures [100, 47]. However, sim-\nply taking \u03b1m = 1/m is usually bad, because then the \ufb01rst few step\nsizes are too large. Instead, a common trick is to use a schedule like\n\u03b1m =\n1\n\u03c32(m0 + m),\n(4.22)\nwhere m0 is a free parameter that needs to be set. A suggestion for\nsetting this parameter, due to Leon Bottou [11], is to sample a small\nsubset of the training data and run one pass of SGD over the subset\nwith various \ufb01xed step sizes \u03b1. Pick the \u03b1\u2217such that the resulting\nlikelihood on the subset after one pass is highest, and choose m0 such\nthat \u03b10 = \u03b1\u2217.\nStochastic gradient descent has also gone by the name of backprop-\nagation in the neural network literature, and many tricks for tuning\nthe method have been developed over the years [57]. Recently, there\nhas been renewed interest in advanced online optimization methods\n[128, 24, 109, 36], which also update parameters in an online fashion,\nbut in a more sophisticated way than simple SGD. Vishwanathan et al.\n[128] was the \ufb01rst application of stochastic gradient methods to CRFs.\nThe main disadvantage of stochastic gradient methods is that they\ndo require tuning, unlike o\ufb00-the-shelf solvers such as conjugate gradient\nand L-BFGS. Stochastic gradient methods are also not useful in rela-\ntional settings in which the training data are not iid, or on small data\nsets. On appropriate data sets, however, stochastic gradient methods\ncan o\ufb00er considerable speedups.\n4.3\nParallelism\nStochastic gradient descent speeds up the gradient computation by\ncomputing it over fewer instances. An alternative way to speed up\nthe gradient computation is to compute the gradient over multiple in-\nstances in parallel. Because the gradient (4.6) is a sum over training\ninstances, it is easy to divide the computation into multiple threads,\nwhere each thread computes the gradient on a subset of training in-\nstances. If the CRF implementation is run on a multicore machine,\nthen the threads will run in parallel, greatly speeding up the gradient\ncomputation. This is a characteristic shared by many common machine\nlearning algorithms, as pointed out by Chu et al. [18].\nIn principle, one could also distribute the gradient computation\nacross multiple machines, rather than multiple cores of the same ma-\nchine, but the overhead involved in transferring large parameter vec-\ntors across the network can be an issue. A potentially promising way to\navoid this is to update the parameter vectors asynchronously. An ex-\nample of this idea is recent work on incorporating parallel computation\ninto stochastic gradient methods [55].\n4.4\nApproximate Training\nAll of the training methods that we have described so far, includ-\ning the stochastic and parallel gradient methods, assume that the\ngraphical structure of the CRF is tractable, that is, that we can ef-\n\ufb01ciently compute the partition function Z(x) and the marginal dis-\ntributions p(yc|x). This is the case, for example, in linear chain and\ntree-structured CRFs. Early work on CRFs focused on these cases,\nboth because of the tractability of inference, and because this choice is\nvery natural for certain tasks such as sequence labeling tasks in NLP.\nBut more complex graphs are important in domains such as com-\nputer vision, where grid-structured graphs are natural, and for more\nglobal models of natural language [114, 30, 13]. When the graphical\nstructure is more complex, then the marginal distributions and the\npartition function cannot be computed tractably, and we must resort\nto approximations. As described in Chapter 3, there is a large literature\non approximate inference algorithms. In the context of CRFs, however,\nthere is a crucial additional consideration, which is that the approx-\nimate inference procedure is embedded within a larger optimization\nprocedure for selecting the parameters.\nThere are two general ways to think about approximate training\nin CRFs [118]: One can either modify the likelihood, or approximate\nthe marginal distributions directly. Modifying the likelihood typically\nmeans \ufb01nding some substitute for \u2113(\u03b8) (such as the BP approxima-\ntion (4.27)), which we will call a surrogate likelihood that is easier to\ncompute but is still expected to favor good parameter setting. Then the\nsurrogate likelihood can be optimized using a gradient-based method,\nin a similar way to the exact likelihood. Approximating the marginal\ndistributions means using a generic inference algorithm to compute an\napproximation to the marginals p(yc|x), substituting the approximate\nmarginals for the exact marginals in the gradient (4.9), and performing\nsome kind of gradient descent procedure using the resulting approxi-\nmate gradients.\nAlthough surrogate likelihood and approximate marginal methods\nare obviously closely related, they are distinct. Usually an surrogate\nlikelihood method directly yields an approximate marginals method,\nbecause just as the derivatives of log Z(x) give the true marginal distri-\nbutions, the derivatives of an approximation to log Z(x) can be viewed\nas an approximation to the marginal distributions. These approximate\nmarginals are sometimes termed pseudomarginals [129]. However, the\nreverse direction does not always hold: for example, there are certain\napproximate marginal procedures that provably do not correspond to\nthe derivative of any likelihood function [118, 112].\nThe main advantage of a surrogate likelihood method is that having\nan objective function can make it easier to understand the properties of\nthe method, both to human analysts and to the optimization procedure.\nAdvanced optimization engines such as conjugate gradient and BFGS\nrequire an objective function in order to operate. The advantage to the\napproximate marginals viewpoint, on the other hand, is that it is more\n\ufb02exible. It is easy to incorporate arbitrary inference algorithms, includ-\ning tricks such as early stopping of BP and MCMC. Also, approximate\nmarginal methods \ufb01t well within a stochastic gradient framework.\nThere are aspects of the interaction between approximate inference\nand parameter estimation that are not completely understood. For ex-\nample, Kulesza and Pereira [52] present an example of a situation in\nwhich the perceptron algorithm interacts in a pathological fashion with\nmax-product belief propagation. Surrogate likelihood methods, by con-\ntrast, do not seem to display this sort of pathology, as Wainwright [129]\npoint out for the case of convex surrogate likelihoods.\nTo make this discussion more concrete, in the rest of this section, we\nwill discuss several examples of surrogate likelihood and approximate\nmarginal methods. We discuss surrogate likelihood methods based on\npseudolikelihood (Section 4.4.1) and belief propagation (Section 4.4.2)\nand approximate gradient methods based on belief propagation (Sec-\ntion 4.4.2) and MCMC (Section 4.4.3).\n4.4.1\nPseudolikelihood\nOne of the earliest surrogate likelihoods is the pseudolikelihood [8]. The\nidea in pseudolikelihood is for the training objective to depend only on\nconditional distributions over single variables. Because the normalizing\nconstants for these distributions depend only on single variables, they\ncan be computed e\ufb03ciently. In the context of CRFs, the pseudolikeli-\nhood is\n\u2113pl(\u03b8) =\nX\ns\u2208V\nlog p(ys|yN(s), x; \u03b8)\n(4.23)\nHere the summation over s ranges over all output nodes in the graph,\nand yN(s) are the values of the variables N(s) that are neighbors of s.\n(As in (4.8), we do not include the sum over training instances explic-\nitly.)\nIntuitively, one way to understand pseudolikelihood is that it at-\ntempts to match the local conditional distributions p(ys|yN(s), x; \u03b8) ac-\ncording to the model to those of the training data, and because of the\nconditional independence assumptions of the model, the local condi-\ntional distributions are su\ufb03cient to specify the joint. (This is similar\nto the motivation behind a Gibbs sampler.)\nThe parameters are estimated by maximizing the pseudolikelihood,\ni.e., the estimates are \u02c6\u03b8pl = max\u03b8 \u2113pl(\u03b8). Typically, the maximization is\ncarried out by a second order method such as limited-memory BFGS,\nbut in principle parallel computation or stochastic gradient can be ap-\nplied to the pseudolikelihood exactly in the same way as the full like-\nlihood. Also, regularization can be used just as with maximum likeli-\nhood.\nThe motivation behind pseudolikelihood is computational e\ufb03ciency.\nThe pseudolikelihood can be computed and optimized without needing\nto compute Z(x) or the marginal distributions. Although pseudolikeli-\nhood has sometimes proved e\ufb00ective in NLP [126], more commonly the\nperformance of pseudolikelihood is poor [115], in an intuitively analo-\ngous way that a Gibbs sampler can mix slowly in sequential models.\nOne can obtain better performance by performing a \u201cblockwise\u201d ver-\nsion of pseudolikelihood in which the local terms involve conditional\nprobabilities of larger regions in the model. For example, in a linear-\nchain CRF, one could consider a per-edge pseudolikelihood:\n\u2113epl(\u03b8) =\nT\u22121\nX\nt=1\nlog p(yt, yt+1|yt\u22121, yt+2, \u03b8)\n(4.24)\n(Here we assume that the sequence is padded with dummy labels y0\nand yT+1 so that the edge cases are correct.) This blockwise version of\npseudolikelihood is a special case of composite likelihood [64, 29], for\nwhich there are general theoretical results concerning asymptotic con-\nsistency and normality. Typically larger blocks lead to better parameter\nestimates, both in theory and in practice.\n4.4.2\nBelief Propagation\nThe loopy belief propagation algorithm (Section 3.2.2) can be used\nwithin approximate CRF training. This can be done within either the\nsurrogate likelihood or the approximate gradient perspectives.\nIn the approximate gradient algorithm, at every iteration of train-\ning, we run loopy BP on the training input x, yielding a set of approx-\nimate marginals q(yc) for each clique in the model. Then we approxi-\nmate the true gradient (4.9) by substituting in the BP marginals. This\nresults in approximate partial derivatives\n\u2202\u02dc\u2113\n\u2202\u03b8pk\n=\nX\n\u03a8c\u2208Cp\nfpk(xc, yc) \u2212\nX\n\u03a8c\u2208Cp\nX\ny\u2032c\nfpk(xc, y\u2032\nc)q(y\u2032\nc).\n(4.25)\nThese can be used to update the current parameter setting as\n\u03b8(t+1)\npk\n= \u03b8(t)\npk + \u03b1 \u2202\u02dc\u2113\n\u2202\u03b8pk\n(4.26)\nwhere \u03b1 > 0 is a step size parameter. The advantages of this setup\nare that it is extremely simple, and is especially useful within an outer\nstochastic gradient approximation.\nMore interestingly, however, it is also possible to use loopy BP\nwithin a surrogate likelihood setup. To do this, we need to develop\nsome surrogate function for the true likelihood (4.8) which has the\nproperty that the gradient of the surrogate likelihood are exactly the\napproximate BP gradients (4.26). This may seem like a tall order, but\nfortunately it is possible using the Bethe free energy described in Sec-\ntion 3.2.2.\nRemember from that section that loopy belief propagation can be\nviewed as an optimization algorithm, namely, one that minimizes the\nobjective function OBethe(q) (3.32) over the set of all locally consistent\nbelief vectors, and that the minimizing value minq OBethe(q) can be used\nas an approximation to the partition function. Substituting in that\napproximation to the true likelihood (4.8) gives us, for a \ufb01xed belief\nvector q, the approximate likelihood\n\u2113Bethe(\u03b8, q) =\nX\nCp\u2208C\nX\n\u03a8c\u2208Cp\nlog \u03a8c(xc, yc)\u2212\nX\nCp\u2208C\nX\n\u03a8c\u2208Cp\nq(yc) log\nq(yc)\n\u03a8c(xc, yc)\n+\nX\ns\u2208Y\n(1 \u2212di)q(ys) log q(ys).\n(4.27)\nThen approximate training can be viewed as the optimization prob-\nlem max\u03b8 minq \u2113Bethe(\u03b8, q). This is a saddlepoint problem, in which we\nare maximizing with respect to one variable (to \ufb01nd the best parame-\nters) and minimizing with respect to another (to solve the approximate\ninference problem). One approach to solve saddlepoint problems is co-\nordinate ascent, that is, to alternately minimize \u2113Bethe with respect to\nq for \ufb01xed \u03b8 and take a gradient step to partially maximize \u2113Bethe with\nrespect to \u03b8 for \ufb01xed b. The \ufb01rst step (minimizing with respect to q)\nis just running the loopy BP algorithm. The key point is that for the\nsecond step (maximizing with respect to \u03b8), the partial derivatives of\n(4.27) with respect to a weight \u03b8k is exactly (4.26), as desired.\nAlternatively, there is a di\ufb00erent surrogate likelihood that can also\nbe used. This is\n\u02c6\u2113(\u03b8; q) = log\n\"Q\nCp\u2208C\nQ\n\u03a8c\u2208Cp q(yc)\nQ\ns\u2208Y q(ys)ds\u22121\n#\n,\n(4.28)\nIn other words, instead of the true joint likelihood, we use the product\nover each clique\u2019s approximate belief, dividing by the node beliefs to\navoid overcounting. The nice thing about this is that it is a direct\ngeneralisation of the true likelihood for tree-structured models, as can\nbe seen by comparing (4.28) with (3.27). This surrogate likelihood can\nbe justi\ufb01ed using a dual version of Bethe energy that we have presented\nhere [78, 81]. When BP has converged, for the resulting belief vector\nq, it can be shown that \u2113Bethe(\u03b8, q) = \u02c6\u2113(\u03b8, q). This equivalence does not\nhold in general for arbitrary values of q, e.g., if BP has not converged.\nAnother surrogate likelihood method that is related to BP is the\npiecewise estimator [117], in which the factors of the model are par-\ntitioned into tractable subgraphs that are trained independently. This\nidea can work surprisingly well (better than pseudolikelihood) if the lo-\ncal features are su\ufb03ciently informative. Sutton and Minka [118] discuss\nthe close relationship between piecewise training and early stopping of\nbelief propagation.\n4.4.3\nMarkov Chain Monte Carlo\nMarkov Chain Monte Carlo (MCMC) inference methods (Section 3.2.1)\ncan be used within CRF training by setting up a Markov chain whose\nstationary distribution is p(y|x; \u03b8), running the chain for a number of\niterations, and using the resulting approximate marginals \u02c6p(y|x; \u03b8) to\napproximate the true marginals in the gradient (4.9).\nIn practice, however, MCMC methods are not commonly used in the\ncontext of CRFs. There are two main reasons for this. First, MCMC\nmethods typically require many iterations to reach convergence, and\nas we have emphasized, inference needs to be run for many di\ufb00erent\nparameter settings over the course of training. Second, many MCMC\nmethods, such as Metropolis-Hastings, require computing a ratio of\nnormalising constants Z\u03b81(x)/Z\u03b82(x) for two di\ufb00erent parameters set-\ntings \u03b81 and \u03b82. This presents a severe di\ufb03culty for models in which\ncomputing Z\u03b8(x) is intractable.\nOne possibility to overcome these di\ufb03culties is contrastive diver-\ngence (CD) [44], in which the true marginals p(yc|x) in (4.9) are ap-\nproximated by running an MCMC method for only a few iterations,\nwhere the initial state of the Markov chain (which is just an assign-\nment to y) is set to be the value of y in the training data. CD has been\nmostly applied to latent variable models such as restricted Boltzmann\nmachines, it can also be applied to CRFs. We are unaware of much\nwork in this direction.\nAnother possibility is a more recent method called SampleRank\n[135], whose objective is that the learned parameters score pairs of\nys such that their sorted ranking obeys a given supervised ranking\n(which is often speci\ufb01ed in terms of a \ufb01xed scoring function on y that\ncompares to true target values of y). Approximate gradients may be\ncalculated from pairs of successive states of the MCMC sampler. Like\nCD, SampleRank learns very quickly because it performs useful pa-\nrameter updates on many individual MCMC steps. Experiments have\nshown the structured classi\ufb01cation accuracy from SampleRank to be\nsubstantially higher than CD [135].\nThe discussion above concerns MCMC methods within an approx-\nimate gradient framework. In contrast, it is very di\ufb03cult to use an\nMCMC inference method within an surrogate likelihood framework,\nbecause it is notoriously di\ufb03cult to obtain a good approximation to\nlog Z(x) given samples from an MCMC method.\nTask\nParameters\nPredicates\n# Sequences\n# Positions\nLabels\nTime (s)\nNP chunking\n248471\n116731\n8936\n211727\n3\n958s\nNER\n187540\n119265\n946\n204567\n9\n4866s\nPOS tagging\n509951\n127764\n38219\n912344\n45\n325500s\nTable 4.1 Scale of typical CRF applications in natural language processing\n4.5\nImplementation Concerns\nTo make the discussion of e\ufb03cient training methods more concrete, here\nwe give some examples of data sets from NLP in which CRFs have been\nsuccessful. The idea is to give a sense of the scales of problem to which\nCRFs have been applied, and of typical values of the number of the\nnumbers of features and of training times.\nWe describe three example tasks to which CRFs have been applied.\nThe \ufb01rst example task is noun-phrase (NP) chunking [104], in which\nthe problem is to \ufb01nd base noun phrases in text, such as the phrases\n\u201cHe\u201d and \u201cthe current account de\ufb01cit\u201d in the sentence He reckons the\ncurrent account de\ufb01cit will narrow. The second task is named identity\nrecognition (NER) [125], The \ufb01nal task is part-of-speech tagging (POS),\nthat is, labelling each word in a sentence with its part of speech. The NP\nchunking and POS data sets are derived from the WSJ Penn Treebank\n[70], while the NER data set consists of newswire articles from Reuters.\nWe will not go into detail about the features that we use, but they\ninclude the identity of the current and previous word, pre\ufb01xes and\nsu\ufb03xes, and (for the named-entity and chunking tasks) automatically\ngenerated part of speech tags and lists of common places and person\nnames. We do not claim that the feature sets that we have used are\noptimal for these tasks, but still they should be useful for getting a\nsense of scale.\nFor each of these data sets, Table 4.1 shows (a) the number of\nparameters in the trained CRF model, (b) the size of the training set,\nin terms of the total number of sequences and number of words, (c)\nthe number of possible labels for each sequence position, and (d) the\ntraining time. The training times range from minutes in the best case to\ndays in the worst case. As can be expected from our previous discussion,\nthe factor that seems to most in\ufb02uence training time is the number of\nlabels.\nObviously the exact training time will depend heavily on details\nof the implementation and hardware. For the examples in Table 4.1,\nwe use the MALLET toolkit on machines with a 2.4 GHz Intel Xeon\nCPU, optimizing the likelihood using batch L-BFGS without using mul-\ntithreaded or stochastic gradient training.\n5\nRelated Work and Future Directions\nIn this section, we brie\ufb02y place CRFs in the context of related lines of\nresearch, especially that of structured prediction, a general research area\nwhich is concerned with extending classi\ufb01cation methods to complex\nobjects. We also describe relationships both to neural networks and\nto a simpler sequence model called maximum entropy Markov models\n(MEMMs). Finally, we outline a few open areas for future work.\n5.1\nRelated Work\n5.1.1\nStructured Prediction\nConditional random \ufb01elds provide one method for extending the ideas\nbehind classi\ufb01cation to the prediction of more complex objects such as\nsequences and trees. This general area of research is called structured\nprediction. Essentially, logistic regression is to a CRF as classi\ufb01cation is\nto structured prediction. Examples of the types of structured outputs\nthat are considered include parse trees of natural language sentences\n[123, 31], alignments between sentences in di\ufb00erent languages [124], and\nroute plans in mobile robotics [97]. Detailed information about struc-\ntured prediction methods is available in a recent collection of research\n63\npapers [4].\nStructured prediction methods are essentially a combination of clas-\nsi\ufb01cation and graphical modeling, combining the ability to compactly\nmodel multivariate data with the ability to perform prediction using\nlarge sets of input features. The idea is, for an input x, to de\ufb01ne a\ndiscriminant function Fx(y), and predict y\u2217= arg maxy Fx(y). This\nfunction factorizes according to a set of local factors, just as in graph-\nical models. But as in classi\ufb01cation, each local factor is modeled a lin-\near function of x, although perhaps in some induced high-dimensional\nspace. To understand the bene\ufb01ts of this approach, consider a hidden\nMarkov model (Section 2.2.2) and a set of per-position classi\ufb01ers, both\nwith \ufb01xed parameters. In principle, the per-position classi\ufb01ers predict\nan output ys given all of x0 . . . xT .1 In the HMM, on the other hand,\nto predict ys it is statistically su\ufb03cient to know only the local input\nxs, the previous forward message p(ys\u22121, x0 . . . xs\u22121), and the backward\nmessage p(xs+1 . . . xT |ys). So the forward and backward messages serve\nas a summary of the rest of the input, a summary that is generally non-\nlinear in the observed features.\nIn principle, the same e\ufb00ect could be achieved using a per-position\nclassi\ufb01er if it were possible to de\ufb01ne an extremely \ufb02exible set of nonlin-\near features that depend on the entire input sequence. But as we have\nseen the size of the input vector is extremely large. For example, in\npart-of-speech tagging, each vector xs may have tens of thousands of\ncomponents, so a classi\ufb01er based on all of x would have many param-\neters. But using only xs to predict ys is also bad, because information\nfrom neighboring feature vectors is also useful in making predictions.\nEssentially the e\ufb00ect of a structured prediction method is that a con\ufb01-\ndent prediction about one variable is able to in\ufb02uence nearby, possibly\nless con\ufb01dent predictions.\nSeveral types of structured prediction algorithms have been stud-\nied. All such algorithms assume that the discriminant function Fx(y)\nover labels can be written as a sum of local functions Fx(y) =\nP\na fa(ya, x, \u03b8). The task is to estimate the real-valued parameter vec-\n1 To be fair, in practice the classi\ufb01er for ys would probably depend only on a sliding window\naround xs, rather than all of x.\ntor \u03b8 given a training set D = {x(i), y(i)}N\ni=1. The methods di\ufb00er in how\nthe parameters are selected.\nAlternative structured prediction methods are based on maximizing\nover assignments rather than marginalizing. Perhaps the most popu-\nlar of these methods has been maximum-margin methods that are so\nsuccessful for univariate classi\ufb01cation. Maximum margin methods have\nbeen generalized to the structured case [2, 122]. Both batch and online\nalgorithms have been developed to maximize this objective function.\nThe perceptron update can also be generalized to structured models\n[21]. The resulting algorithm is particularly appealing because it is lit-\ntle more di\ufb03cult to implement than the algorithm for selecting y\u2217. The\nonline perceptron update can also be made margin-aware, yielding the\nMIRA algorithm [23], which may perform better than the perceptron\nupdate.\nAnother class of methods are search-based methods [27, 28] in which\na heuristic search procedure over outputs is assumed, and learns a clas-\nsi\ufb01er that predicts the next step in the search. This has the advantage\nof \ufb01tting in nicely to many problems that are complex enough to re-\nquire performing search. It is also able to incorporate arbitrary loss\nfunctions over predictions.\nA general advantage of all of these maximization-based methods\nis that they do not require summation over all con\ufb01gurations for the\npartition function or for marginal distributions. There are certain com-\nbinatorial problems, such as matching and network \ufb02ow problems, in\nwhich \ufb01nding an optimal con\ufb01guration is tractable, but summing over\ncon\ufb01gurations is not (for an example of applying max-margin methods\nin such situations, see Taskar et al. [124]). For more complex problems,\nneither summation nor maximization is tractable, so this advantage\nis perhaps not as signi\ufb01cant. Another advantage of these methods is\nthat kernels can be naturally incorporated, in an analogous way as in\nsupport vector machines.\nFinally, LeCun et al. [59] generalizes many prediction methods, in-\ncluding the ones listed above, under the rubric of energy-based methods,\nand presents interesting historical information about their use. They\nadvocate changing the loss function to avoid probabilities altogether.\nPerhaps the main advantage of probabilistic methods is that they\ncan incorporate latent variables in a natural way, by marginalization.\nThis can be useful, for example, in collective classi\ufb01cation methods\n[121]. For examples of structured models with latent variables, see\nQuattoni et al. [95] and McCallum et al. [75]. A particularly powerful\nexample of this is provided by Bayesian methods, in which the model\nparameters themselves are integrated out (Section 5.2.1).\nThe di\ufb00erences between the various structured prediction methods\nare not well understood. To date, there has been little careful compar-\nison of these, especially CRFs and max-margin approaches, across dif-\nferent structures and domains, although see Keerthi and Sundararajan\n[46] for some experiments in this regard.2 We take the view that the\nsimilarities between various structured prediction methods are more\nimportant than the di\ufb00erences. Careful selection of features has more\ne\ufb00ect on performance than the choice of structured prediction algo-\nrithm.\n5.1.2\nNeural Networks\nThere are close relationships between neural networks and conditional\nrandom \ufb01elds, in that both can be viewed as discriminatively trained\nprobabilistic models. Neural networks are perhaps best known for their\nuse in classi\ufb01cation, but they can also be used to predict multiple out-\nputs, for example, by using a shared latent representation [15], or by\nmodelling dependencies between outputs directly [58]. Although neural\nnetworks are typically trained using stochastic gradient descent (Sec-\ntion 4.2), in principle they can be trained using any of the other meth-\nods used for CRFs. The main di\ufb00erence between them is that neural\nnetworks represent the dependence between output variables using a\nshared latent representation, while structured methods learn these de-\npendences as direct functions of the output variables.\nBecause of this, it is easy to make the mistake of thinking that CRFs\nare convex and neural networks are not. This is incorrect. A neural\nnetwork without a hidden layer is a linear classi\ufb01er that can be trained\n2 An earlier study [86] appears to have been \ufb02awed. See Keerthi and Sundararajan [46] for\ndiscussion.\ny\nx\nFig. 5.1 Graphical model of a maximum entropy Markov model [74].\ne\ufb03ciently in a number of ways, while a CRF with latent variables\nhas a complex non-convex likelihood (Section 2.4). The correct way\nof thinking is: In fully observed models, the likelihood is convex; in\nlatent variable models it is not.\nSo the main new insight of structured prediction models compared\nto neural networks is: If you add connections among the nodes in the\noutput layer, and if you have a good set of features, then sometimes you\ndon\u2019t need a hidden layer to get good performance. If you can a\ufb00ord\nto leave out the hidden, then in practice you always want to do so,\nbecause this avoids all of the problems with local minima. For harder\nproblems, however, one might expect that even after modeling output\nstructure, incorporating hidden state will still yield additional bene\ufb01t.\nOnce hidden state is introduced into the model, whether it be a neural\nnetwork or a structured model, it seems to be inevitable (at least given\nour current understanding of machine learning) that convexity will be\nlost.\n5.1.3\nMEMMs, Directed Models, and Label Bias\nLinear-chain CRFs were originally introduced as an improvement to the\nmaximum-entropy Markov model (MEMM) [74], which is essentially a\nMarkov model in which the transition probabilities are given by logistic\nregression. Formally, an MEMM is\npMEMM(y|x) =\nT\nY\nt=1\np(yt|yt\u22121, x)\n(5.1)\np(yt|yt\u22121, x) =\n1\nZt(yt\u22121, x) exp\n( K\nX\nk=1\n\u03b8kfk(yt, yt\u22121, xt)\n)\n(5.2)\nZt(yt\u22121, x) =\nX\ny\u2032\nexp\n( K\nX\nk=1\n\u03b8kfk(y\u2032, yt\u22121, xt)\n)\n(5.3)\nA similar idea can be extended to general directed graphs, in which the\ndistribution p(y|x) is expressed by a Bayesian network in which each\nCPT is a logistic regression models with input x [102].\nIn the linear-chain case, notice that the MEMM works out to have\nthe same form as the linear-chain CRF (4.3) with the exception that in a\nCRF Z(x) is a sum over sequences, whereas in a MEMM the analogous\nterm is QT\nt=1 Zt(yt\u22121, x). This di\ufb00erence has important consequences.\nUnlike in a CRFs, maximum likelihood training of MEMMs does not\nrequire performing inference, because Zt is just a simple sum over the\nlabels at a single position, rather than a sum over labels of an entire\nsequence. This is an example of the general phenomenon that training\nof directed models is less computationally demanding than undirected\nmodels.\nThere are theoretical di\ufb03culties with the MEMM model, however.\nMEMMs can exhibit the problems of label bias [54] and observation\nbias [48]. Originally, the label bias problem was described from an al-\ngorithmic perspective. Consider the backward recursion (3.9). In the\ncase of an MEMM, this amounts to\n\u03b2t(i) =\nX\nj\u2208S\np(yt+1 = j|yt = i, xt+1)\u03b2t+1(j).\n(5.4)\nUnfortunately, this sum is always 1, regardless of the value of the\ncurrent label i. To see this, assume for the sake of induction that\n\u03b2t+1(j) = 1 for all j. Then it is clear that the sum over j in (5.4)\ncollapses, and \u03b2t(i) = 1. What this means is that the future observa-\ntions provide no information about the current state, which seems to\nlose a major advantage of sequence modelling.\nPerhaps a more intuitive way to understand label bias is from the\nperspective of graphical models. Consider the graphical model of an\nMEMM, shown in Figure 5.1. By looking at the v-structures in the\ngraph, we can read o\ufb00the following independence assumptions: at\nall time steps t, the label yt is marginally independent of the future\nobservations xt+1, xt+2, etc. This independence assumption is usually\nstrongly violated in sequence modeling, which explains why CRFs can\nhave better performance than MEMMs. Also, this independence rela-\ntion explains why \u03b2t(i) should always be 1. (In general, this correspon-\ndence between graph structure and inference algorithms is one of main\nconceptual advantages of graphical modelling.) To summarize this dis-\ncussion, label bias is simply a consequence of explaining away.\nThere is a caveat here: We can always copy information from previ-\nous and future time steps into the feature vector xt, and this is common\nin practice. (The only constraint is that if we have too many features,\nthen over\ufb01tting we become an issue.) This has the e\ufb00ect of adding\narcs between (for example) xt+1. This explains why the performance\ngap between MEMMs and CRFs is not always as large as might be\nexpected.\nFinally, one might try a di\ufb00erent way to combine the advantages of\nconditional training and directed models. One can imagine de\ufb01ning a\ndirected model p(y, x), perhaps a generative model, and then training\nit by optimizing the resulting conditional likelihood p(y|x). In fact,\nthis procedure has long been done in the speech community, where it\nis called maximum mutual information training. However, this does\nnot have strong computational bene\ufb01ts over CRFs. The reason is that\ncomputing the conditional likelihood p(y|x) requires computing the\nmarginal probability p(x), which plays the same role as Z(x) in the\nCRF likelihood. In fact, training is more complex in a directed model,\nbecause the model parameters are constrained to be probabilities\u2014\nconstraints which can actually make the optimization problem more\ndi\ufb03cult.\n5.2\nFrontier Areas\nFinally, we describe a few open research areas that related to CRFs.\nIn all of the cases below, the research question is a special case of\na larger question for general graphical models, but there are special\nadditional considerations in conditional models that make the problem\nmore di\ufb03cult.\n5.2.1\nBayesian CRFs\nBecause of the large number of parameters in typical applications of\nCRFs, the models can be prone to over\ufb01tting. The standard way to\ncontrol this is using regularization, as described in Section 4.1.1. One\nway that we motivated this procedure is as an approximation to a\nfully Bayesian procedure. That is, instead of predicting the labels of a\ntesting instance x as y\u2217= maxy p(y|x; \u02c6\u03b8), where \u02c6\u03b8 is a single parameter\nestimate, in a Bayesian method we would use the predictive distribution\ny\u2217= maxy\nR\np(y|x; \u03b8)p(\u03b8) QN\ni=1 p(y(i)|x(i), \u03b8)d\u03b8. This integral over \u03b8\nneeds to be approximated, for example, by MCMC.\nIn general, it is di\ufb03cult to formulate e\ufb03cient Bayesian methods for\nundirected models; see [83, 82] for some of the few examples in this\nregard. A few papers have specially considered approximate inference\nalgorithms for Bayesian CRFs [92, 133], but while these methods are\ninteresting, they do not seem to be useful at the scale of current CRF\napplications (e.g., those in Table 4.1). Even for linear chain models,\nBayesian methods are not commonly in use for CRFs, primarily due\nto the computational demands. If all we want is the bene\ufb01ts of model\naveraging, one may question whether simpler ensemble learning tech-\nniques, such as bagging, would give the same bene\ufb01t. However, the\nBayesian perspective does have other potential bene\ufb01ts, particularly\nwhen more complex, hierarchical priors are considered.\n5.2.2\nSemi-supervised CRFs\nOne practical di\ufb03culty in applying CRFs is that training requires ob-\ntaining true labels for potentially many sequences. This can be expen-\nsive because it is more time consuming for a human labeller to provide\nlabels for sequence labelling than for simple classi\ufb01cation. For this rea-\nson, it would be very useful to have techniques that can obtain good\naccuracy given only a small amount of labeled data.\nOne strategy for achieving this goal is semi-supervised learning,\nin which in addition to some fully-labelled data {(x(i), y(i))}N\ni=1, the\ndata set is assumed to contain a large number of unlabelled instances\n{x(j)}M\nj=1, for which we observe only the inputs. However, unlike in gen-\nerative models, it is less obvious how to incorporate unlabelled data\ninto a conditional criterion, because the unlabelled data is a sample\nfrom the distribution p(x), which in principle need have no relation-\nship to the CRF p(y|x). In order to deal with this, several di\ufb00erent\ntypes of regularization terms have been proposed that take the un-\nlabelled data into account, including entropy regularization [39, 45],\ngeneralized expectation criteria [69], posterior regularization [32, 38],\nand measurement-based learning [62].\n5.2.3\nStructure Learning in CRFs\nAll of the methods described in this tutorial assume that the structure\nof the model has been decided in advance. It is natural to ask if we\ncan learn the structure of the model as well. As in graphical models\nmore generally, this is a di\ufb03cult problem. In fact, Bradley and Guestrin\n[12] point out an interesting complication that is speci\ufb01c to conditional\nmodels. Typically, maximum likelihood structure learning can be per-\nformed e\ufb03ciently if the model is restricted to be tree-structured, using\nthe well-known Chow-Liu algorithm. The analogous algorithm in the\nconditional case is more di\ufb03cult, however, because it requires estimat-\ning marginal distributions of the form p(yu, yv|x1:N), that is, we need\nto estimate the e\ufb00ects of the entire input on every pair of variables. It\nis di\ufb03cult to estimate these distributions e\ufb03ciently without knowing\nthe structure of the model to begin with.\nAcknowledgments\nWe thank Francine Chen, Benson Limketkai, Gregory Druck, Kedar\nBellare, and Ray Mooney for useful comments on earlier versions of this\ntutorial. A previous version of this tutorial has appeared in Sutton and\nMcCallum [116], and as part of Charles Sutton\u2019s doctoral dissertation\n[113].\nReferences\n[1] Srinivas M. Aji and Robert J. McEliece. The generalized dis-\ntributive law. IEEE Transactions on Information Theory, 46(2):\n325\u2013343, 2000.\n[2] Yasemin Altun, Ioannis Tsochantaridis, and Thomas Hofmann.\nHidden Markov support vector machines. In International Con-\nference on Machine Learning (ICML), 2003.\n[3] Galen Andrew and Jianfeng Gao.\nScalable training of l1-\nregularized log-linear models.\nIn International Conference on\nMachine Learning (ICML), 2007.\n[4] G\u00a8okhan H. Bakir, Thomas Hofmann, Bernhard Sch\u00a8olkopf,\nAlexander J. Smola, Ben Taskar, and S. V. N. Vishwanathan,\neditors. Predicting Structured Data. MIT Press, 2007.\n[5] Axel Bernal, Koby Crammer, Artemis Hatzigeorgiou, and Fer-\nnando Pereira. Global discriminative learning for higher-accuracy\ncomputational gene prediction. PLoS Computational Biology, 3\n(3), 2007.\n[6] Dimitri P. Bertsekas. Nonlinear Programming. Athena Scienti\ufb01c,\n2nd edition, 1999.\n[7] Julian Besag. Spatial interaction and the statistical analysis of\n73\nlattice systems. Journal of the Royal Statistical Society. Series\nB, 36(2):192\u2013236, 1974.\n[8] Julian Besag. Statistical analysis of non-lattice data. The Statis-\ntician, 24(3):179\u2013195, 1975.\n[9] David M. Blei, Andrew Y. Ng, and Michael I. Jordan. Latent\ndirichlet allocation. Journal of Machine Learning Research, 3:\n993, 2003.\n[10] Phil Blunsom and Trevor Cohn. Discriminative word alignment\nwith conditional random \ufb01elds. In International Conference on\nComputational Linguistics and 44th Annual Meeting of the As-\nsociation for Computational Linguistics (COLING-ACL), pages\n65\u201372, 2006.\n[11] L\u00b4eon Bottou. Stochastic gradient descent examples on toy prob-\nlems. 2010. URL http://leon.bottou.org/projects/sgd.\n[12] Joseph K. Bradley and Carlos Guestrin. Learning tree conditional\nrandom \ufb01elds. In International Conference on Machine Learning\n(ICML 2010), 2010.\n[13] Razvan Bunescu and Raymond J. Mooney. Collective information\nextraction with relational Markov networks. In Proceedings of\nthe 42nd Annual Meeting of the Association for Computational\nLinguistics, 2004.\n[14] Richard H. Byrd, Jorge Nocedal, and Robert B. Schnabel. Rep-\nresentations of quasi-Newton matrices and their use in limited\nmemory methods. Math. Program., 63(2):129\u2013156, 1994. ISSN\n0025-5610.\n[15] Rich Caruana.\nMultitask learning.\nMachine Learning, 28(1):\n41\u201375, 1997. ISSN 0885-6125. doi: http://dx.doi.org/10.1023/A:\n1007379606734.\n[16] Rich Caruana and Alexandru Niculescu-Mizil. An empirical com-\nparison of supervised learning algorithms using di\ufb00erent perfor-\nmance metrics.\nTechnical Report TR2005-1973, Cornell Uni-\nversity, 2005. http://www.niculescu-mizil.org/paper.php?\np=comparison.tr.pdf.\n[17] Yejin Choi, Claire Cardie, Ellen Rilo\ufb00, and Siddharth Patward-\nhan.\nIdentifying sources of opinions with conditional random\n\ufb01elds and extraction patterns. In Proceedings of the Human Lan-\nguage Technology Conference/Conference on Empirical Methods\nin Natural Language Processing (HLT-EMNLP), 2005.\n[18] C.T. Chu, S.K. Kim, Y.A. Lin, Y.Y. Yu, G. Bradski, A.Y. Ng,\nand K. Olukotun. Map-reduce for machine learning on multicore.\nIn Advances in Neural Information Processing Systems 19, pages\n281\u2013288. MIT Press, 2007.\n[19] Stephen Clark and James R. Curran.\nParsing the WSJ using\nCCG and log-linear models. In Proceedings of the 42nd Meeting\nof the Association for Computational Linguistics (ACL), pages\n103\u2013110, 2004.\n[20] Trevor Cohn.\nE\ufb03cient inference in large conditional random\n\ufb01elds. In European Conference on Machine Learning (ECML),\npages 606\u2013613, Berlin, Germany, September 2006.\n[21] Michael Collins.\nDiscriminative training methods for hidden\nMarkov models: Theory and experiments with perceptron algo-\nrithms. In Conference on Empirical Methods in Natural Language\nProcessing (EMNLP), 2002.\n[22] Philip J. Cowans and Martin Szummer. A graphical model for si-\nmultaneous partitioning and labeling. In Conference on Arti\ufb01cial\nIntelligence and Statistics (AISTATS), 2005.\n[23] Koby Crammer and Yoram Singer. Ultraconservative online al-\ngorithms for multiclass problems. Journal of Machine Learning\nResearch, 3:951\u2013991, Jan 2003.\n[24] Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz,\nand Yoram Singer. Online passive-aggressive algorithms. Journal\nof Machine Learning Research, 2006.\n[25] Aron Culotta and Andrew McCallum. Con\ufb01dence estimation for\ninformation extraction. In Human Language Technology Confer-\nence (HLT), 2004.\n[26] Aron Culotta, Ron Bekkerman, and Andrew McCallum.\nEx-\ntracting social networks and contact information from email and\nthe web. In First Conference on Email and Anti-Spam (CEAS),\nMountain View, CA, 2004.\n[27] Hal Daum\u00b4e III and Daniel Marcu.\nLearning as search op-\ntimization: Approximate large margin methods for structured\nprediction.\nIn International Conference on Machine Learning\n(ICML), Bonn, Germany, 2005. URL http://pub.hal3.name/\n#daume05laso.\n[28] Hal Daum\u00b4e III, John Langford, and Daniel Marcu. Search-based\nstructured prediction. Machine Learning Journal, 2009.\n[29] Joshua Dillon and Guy Lebanon. Statistical and computational\ntradeo\ufb00s in stochastic composite likelihood. arXiv:1003.0691v1,\n2010.\n[30] Jenny Finkel, Trond Grenager, and Christopher D. Manning.\nIncorporating non-local information into information extraction\nsystems by Gibbs sampling. In Proceedings of the 43rd Annual\nMeeting of the Association for Computational Linguistics (ACL),\n2005.\n[31] Jenny Rose Finkel, Alex Kleeman, and Christopher D. Manning.\nE\ufb03cient, feature-based, conditional random \ufb01eld parsing. In An-\nnual Meeting of the Association for Computational Linguistics\n(ACL/HLT), pages 959\u2013967, 2008.\n[32] Kuzman Ganchev, Joao Graca, Jennifer Gillenwater, and Ben\nTaskar.\nPosterior regularization for structured latent variable\nmodels. Technical Report MS-CIS-09-16, University of Pennsyl-\nvania Department of Computer and Information Science, 2009.\n[33] Alan E. Gelfand and Adrian F. M. Smith. Sampling-based ap-\nproaches to calculating marginal densities. Journal of the Amer-\nican Statistical Association, 85:398\u2013409, 1990.\n[34] Stuart Geman and Donald Geman. Stochastic relaxation, Gibbs\ndistributions, and the Bayesian restoration of images.\nIEEE\nTransactions on Pattern Analysis and Machine Intelligence, 6:\n721\u2013741, 1984.\n[35] Nadia Ghamrawi and Andrew McCallum. Collective multi-label\nclassi\ufb01cation.\nIn Conference on Information and Knowledge\nManagement (CIKM), 2005.\n[36] Amir Globerson, Terry Koo, Xavier Carreras, and Michael\nCollins. Exponentiated gradient algorithms for log-linear struc-\ntured prediction. In International Conference on Machine Learn-\ning (ICML), 2007.\n[37] Joshua Goodman. Exponential priors for maximum entropy mod-\nels. In Proceedings of the Human Language Technology Confer-\nence/North American Chapter of the Association for Computa-\ntional Linguistics (HLT/NAACL), 2004.\n[38] Joao Graca, Kuzman Ganchev, Ben Taskar, and Fernando\nPereira. Posterior vs parameter sparsity in latent variable models.\nIn Y. Bengio, D. Schuurmans, J. La\ufb00erty, C. K. I. Williams, and\nA. Culotta, editors, Advances in Neural Information Processing\nSystems 22, pages 664\u2013672. 2009.\n[39] Yves Grandvalet and Yoshua Bengio. Semi-supervised learning\nby entropy minimization.\nIn Advances in Neural Information\nProcessing Systems, 2004.\n[40] Michelle L. Gregory and Yasemin Altun. Using conditional ran-\ndom \ufb01elds to predict pitch accents in conversational speech. In\nAnnual Meeting of the Association for Computational Linguis-\ntics (ACL), pages 677\u2013683, 2004. doi: http://dx.doi.org/10.3115/\n1218955.1219041.\n[41] Asela Gunawardana, Milind Mahajan Alex Acero, and John C.\nPlatt. Hidden conditional random \ufb01elds for phone classi\ufb01cation.\nIn International Conference on Speech Communication and Tech-\nnology, 2005.\n[42] John M. Hammersley and Peter Cli\ufb00ord. Markov \ufb01elds on \ufb01nite\ngraphs and lattices. 1971.\n[43] Xuming He, Richard S. Zemel, and Miguel \u00b4A. Carreira-Perpi\u02dcni\u00b4an.\nMultiscale conditional random \ufb01elds for image labelling. In IEEE\nComputer Society Conference on Computer Vision and Pattern\nRecognition, 2004.\n[44] Geo\ufb00rey E. Hinton. Training products of experts by minimizing\ncontrastive divergence. Neural Computation, 14:1771\u20131800, 2002.\n[45] F. Jiao, S. Wang, C. Lee, R. Greiner, and D Schuurmans. Semi-\nsupervised conditional random \ufb01elds for improved sequence seg-\nmentation and labeling. In Joint Conference of the International\nCommittee on Computational Linguistics and the Association for\nComputational Linguistics (COLING/ACL), 2006.\n[46] S.\nSathiya\nKeerthi\nand\nS.\nSundararajan.\nCRF\nversus\nSVM-struct for sequence labeling.\nTechnical report, Ya-\nhoo! Research, 2007.\nURL http://www.keerthis.com/crf_\ncomparison_keerthi_07.pdf.\n[47] J. Kiefer and J. Wolfowitz. Stochastic estimation of the maximum\nof a regression function. Annals of Mathematical Statistics, 23:\n462\u2013466, 1952.\n[48] Dan Klein and Christopher D. Manning. Conditional structure\nversus conditional estimation in NLP models. In Conference on\nEmpirical Methods in Natural Language Processing (EMNLP),\n2002.\n[49] Daphne Koller and Nir Friedman. Probabilistic Graphical Models:\nPrinciples and Techniques. MIT Press, 2009.\n[50] Frank R. Kschischang, Brendan J. Frey, and Hans-Andrea\nLoeliger. Factor graphs and the sum-product algorithm. IEEE\nTransactions on Information Theory, 47(2):498\u2013519, 2001.\n[51] Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto. Applying\nconditional random \ufb01elds to Japanese morphological analysis. In\nProceedings of the Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), 2004.\n[52] Alex Kulesza and Fernando Pereira.\nStructured learning with\napproximate inference. In Advances in Neural Information Pro-\ncessing Systems, 2008.\n[53] Sanjiv Kumar and Martial Hebert. Discriminative \ufb01elds for mod-\neling spatial dependencies in natural images. In Sebastian Thrun,\nLawrence Saul, and Bernhard Sch\u00a8olkopf, editors, Advances in\nNeural Information Processing Systems (NIPS)16. MIT Press,\nCambridge, MA, 2003.\n[54] John La\ufb00erty, Andrew McCallum, and Fernando Pereira. Con-\nditional random \ufb01elds: Probabilistic models for segmenting and\nlabeling sequence data.\nInternational Conference on Machine\nLearning (ICML), 2001.\n[55] John Langford, Alex Smola, and Martin Zinkevich. Slow learn-\ners are fast. In Y. Bengio, D. Schuurmans, J. La\ufb00erty, C. K. I.\nWilliams, and A. Culotta, editors, Advances in Neural Informa-\ntion Processing Systems 22, pages 2331\u20132339, 2009.\n[56] T. Lavergne, O. Capp\u00b4e, and F. Yvon. Practical very large scale\ncrfs.\nIn Proc. 48th Annual Meeting Association for Computa-\ntional Linguistics (ACL), pages 504\u2013513, 2010.\n[57] Yann Le Cun, L\u00b4eon Bottou, Genevieve B. Orr, and Klaus-\nRobert M\u00a8uller. E\ufb03cient backprop. In Neural Networks, Tricks\nof the Trade, Lecture Notes in Computer Science LNCS 1524.\nSpringer Verlag, 1998. URL http://leon.bottou.org/papers/\nlecun-98x.\n[58] Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Ha\ufb00ner.\nGradient-based learning applied to document recognition. Pro-\nceedings of the IEEE, 86(11):2278\u20132324, November 1998.\n[59] Yann\nLeCun,\nSumit\nChopra,\nRaia\nHadsell,\nRanzato\nMarc\u2019Aurelio, and Fu-Jie Huang.\nA tutorial on energy-\nbased learning. In G. Bakir, T. Hofman, B. Sch\u00a8olkopf, A. Smola,\nand B. Taskar, editors, Predicting Structured Data. MIT Press,\n2007.\n[60] Stan Z. Li. Markov Random Field Modeling in Image Analysis.\nSpringer-Verlag, 2001.\n[61] P. Liang and M.I. Jordan. An asymptotic analysis of generative,\ndiscriminative, and pseudolikelihood estimators. In International\nConference on Machine Learning (ICML), pages 584\u2013591. ACM,\n2008.\n[62] P. Liang, M. I. Jordan, and D. Klein. Learning from measure-\nments in exponential families. In International Conference on\nMachine Learning (ICML), 2009.\n[63] Chih-Jen Lin, Ruby Chiu-Hsing Weng, and Sathiya Keerthi.\nTrust region newton methods for large-scale logistic regression.\nIn Interational Conference on Machine Learning (ICML), 2007.\n[64] Bruce G. Lindsay. Composite likelihood methods. Contemporary\nMathematics, pages 221\u2013239, 1988.\n[65] Yan Liu, Jaime Carbonell, Peter Weigele, and Vanathi Gopalakr-\nishnan. Protein fold recognition using segmentation conditional\nrandom \ufb01elds (SCRFs). Journal of Computational Biology, 13\n(2):394\u2013406, 2006.\n[66] David J. Lunn, Andrew Thomas, Nicky Best, and David Spiegel-\nhalter. WinBUGS\u2014a Bayesian modelling framework: Concepts,\nstructure, and extensibility. Statistics and Computing, 10(4):325\u2013\n337, 2000.\n[67] David J. C. MacKay. Information Theory, Inference, and Learn-\ning Algorithms. Cambridge University Press, 2003.\n[68] Robert Malouf.\nA comparison of algorithms for maximum\nentropy parameter estimation.\nIn Dan Roth and Antal\nvan den Bosch, editors, Conference on Natural Language Learn-\ning (CoNLL), pages 49\u201355, 2002.\n[69] Gideon Mann and Andrew McCallum. Generalized expectation\ncriteria for semi-supervised learning of conditional random \ufb01elds.\nIn Proceedings of Association of Computational Linguistics, 2008.\n[70] Mitchell\nP.\nMarcus,\nBeatrice\nSantorini,\nand\nMary\nAnn\nMarcinkiewicz.\nBuilding a large annotated corpus of English:\nThe Penn Treebank. Computational Linguistics, 19(2):313\u2013330,\n1993.\n[71] Andrew McCallum. E\ufb03ciently inducing features of conditional\nrandom \ufb01elds. In Conference on Uncertainty in AI (UAI), 2003.\n[72] Andrew McCallum and Wei Li. Early results for named entity\nrecognition with conditional random \ufb01elds, feature induction and\nweb-enhanced lexicons. In Seventh Conference on Natural Lan-\nguage Learning (CoNLL), 2003.\n[73] Andrew McCallum and Ben Wellner.\nConditional models of\nidentity uncertainty with application to noun coreference.\nIn\nLawrence K. Saul, Yair Weiss, and L\u00b4eon Bottou, editors, Ad-\nvances in Neural Information Processing Systems 17, pages 905\u2013\n912. MIT Press, Cambridge, MA, 2005.\n[74] Andrew McCallum, Dayne Freitag, and Fernando Pereira. Maxi-\nmum entropy Markov models for information extraction and seg-\nmentation.\nIn International Conference on Machine Learning\n(ICML), pages 591\u2013598. Morgan Kaufmann, San Francisco, CA,\n2000.\n[75] Andrew McCallum, Kedar Bellare, and Fernando Pereira.\nA\nconditional random \ufb01eld for discriminatively-trained \ufb01nite-state\nstring edit distance. In Conference on Uncertainty in AI (UAI),\n2005.\n[76] Andrew McCallum, Karl Schultz, and Sameer Singh.\nFacto-\nrie: Probabilistic programming via imperatively de\ufb01ned factor\ngraphs. In Advances on Neural Information Processing Systems\n(NIPS), 2009.\n[77] Robert J. McEliece, David J. C. MacKay, and Jung-Fu Cheng.\nTurbo decoding as an instance of Pearl\u2019s \u201cbelief propagation\u201d\nalgorithm. IEEE Journal on Selected Areas in Communications,\n16(2):140\u2013152, 1998.\n[78] Thomas P. Minka. The EP energy function and minimization\nschemes. Technical report, 2001. http://research.microsoft.\ncom/~minka/papers/ep/minka-ep-energy.pdf.\n[79] Thomas P. Minka.\nA comparsion of numerical optimizers for\nlogistic regression. Technical report, 2003. http://research.\nmicrosoft.com/~minka/papers/logreg/.\n[80] Tom Minka.\nDiscriminative models, not discriminative train-\ning.\nTechnical Report MSR-TR-2005-144, Microsoft Research,\nOctober 2005.\nftp://ftp.research.microsoft.com/pub/tr/\nTR-2005-144.pdf.\n[81] Tom Minka. Divergence measures and message passing. Technical\nReport MSR-TR-2005-173, Microsoft Research, 2005.\n[82] Iain Murray. Advances in Markov chain Monte Carlo methods.\nPhD thesis, Gatsby computational neuroscience unit, University\nCollege London, 2007.\n[83] Iain Murray, Zoubin Ghahramani, and David J. C. MacKay.\nMCMC for doubly-intractable distributions. In Uncertainty in\nArti\ufb01cial Intelligence (UAI), pages 359\u2013366. AUAI Press, 2006.\n[84] Andrew Y. Ng. Feature selection, l1 vs. l2 regularization, and\nrotational invariance. In International Conference on Machine\nLearning (ICML), 2004.\n[85] Andrew Y. Ng and Michael I. Jordan. On discriminative vs. gen-\nerative classi\ufb01ers: A comparison of logistic regression and naive\nbayes. In Thomas G. Dietterich, Suzanna Becker, and Zoubin\nGhahramani, editors, Advances in Neural Information Processing\nSystems 14, pages 841\u2013848, Cambridge, MA, 2002. MIT Press.\n[86] N. Nguyen and Y. Guo. Comparisons of sequence labeling algo-\nrithms and extensions. In International Conference on Machine\nLearning (ICML), 2007.\n[87] Jorge Nocedal and Stephen J. Wright. Numerical Optimization.\nSpringer-Verlag, New York, 1999. ISBN 0-387-98793-2.\n[88] Judea Pearl. Probabilistic Reasoning in Intelligent Systems: Net-\nworks of Plausible Inference. Morgan Kaufmann, 1988.\n[89] Fuchun Peng and Andrew McCallum. Accurate information ex-\ntraction from research papers using conditional random \ufb01elds.\nIn Proceedings of Human Language Technology Conference and\nNorth American Chapter of the Association for Computational\nLinguistics (HLT-NAACL\u201904), 2004.\n[90] Fuchun Peng, Fangfang Feng, and Andrew McCallum. Chinese\nsegmentation and new word detection using conditional random\n\ufb01elds. In Proceedings of The 20th International Conference on\nComputational Linguistics (COLING), pages 562\u2013568, 2004.\n[91] David Pinto, Andrew McCallum, Xing Wei, and W. Bruce Croft.\nTable extraction using conditional random \ufb01elds. In Proceedings\nof the ACM SIGIR, 2003.\n[92] Yuan Qi, Martin Szummer, and Thomas P. Minka.\nBayesian\nconditional random \ufb01elds. In Conference on Arti\ufb01cial Intelligence\nand Statistics (AISTATS), 2005.\n[93] Yuan Qi, Martin Szummer, and Thomas P. Minka.\nDiagram\nstructure recognition by Bayesian conditional random \ufb01elds.\nIn International Conference on Computer Vision and Pattern\nRecognition, 2005.\n[94] A. Quattoni, S. Wang, L.P. Morency, M. Collins, and T. Darrell.\nHidden-state conditional random \ufb01elds. IEEE PAMI, 2007.\n[95] Ariadna Quattoni, Michael Collins, and Trevor Darrell. Condi-\ntional random \ufb01elds for object recognition. In Lawrence K. Saul,\nYair Weiss, and L\u00b4eon Bottou, editors, Advances in Neural In-\nformation Processing Systems 17, pages 1097\u20131104. MIT Press,\nCambridge, MA, 2005.\n[96] Lawrence R. Rabiner. A tutorial on hidden Markov models and\nselected applications in speech recognition.\nProceedings of the\nIEEE, 77(2):257 \u2013 286, 1989.\n[97] Nathan Ratli\ufb00, J. Andrew Bagnell, and Martin Zinkevich. Max-\nimum margin planning. In International Conference on Machine\nLearning, July 2006.\n[98] Matthew Richardson and Pedro Domingos. Markov logic net-\nworks. Machine Learning, 62(1\u20132):107\u2013136, 2006.\n[99] Stefan Riezler, Tracy King, Ronald Kaplan, Richard Crouch,\nJohn T. Maxwell III, and Mark Johnson. Parsing the Wall Street\nJournal using a lexical-functional grammar and discriminative es-\ntimation techniques. In Proceedings of the 40th Annual Meeting\nof the Association for Computational Linguistics, 2002.\n[100] H. Robbins and S. Monro. A stochastic approximation method.\nAnnals of Mathematical Statistics, 22:400\u2013407, 1951.\n[101] Christian Robert and George Casella. Monte Carlo Statistical\nMethods. Springer, 2004.\n[102] David Rosenberg, Dan Klein, and Ben Taskar. Mixture-of-parents\nmaximum entropy Markov models. In Conference on Uncertainty\nin Arti\ufb01cial Intelligence (UAI), 2007.\n[103] Dan Roth and Wen-tau Yih. Integer linear programming infer-\nence for conditional random \ufb01elds. In International Conference\non Machine Learning (ICML), pages 737\u2013744, 2005.\n[104] Erik F. Tjong Kim Sang and Sabine Buchholz.\nIntroduction\nto the CoNLL-2000 shared task: Chunking.\nIn Proceedings of\nCoNLL-2000 and LLL-2000, 2000. See http://lcg-www.uia.\nac.be/~erikt/research/np-chunking.html.\n[105] Sunita Sarawagi and William W. Cohen.\nSemi-Markov condi-\ntional random \ufb01elds for information extraction. In Lawrence K.\nSaul, Yair Weiss, and L\u00b4eon Bottou, editors, Advances in Neural\nInformation Processing Systems 17, pages 1185\u20131192. MIT Press,\nCambridge, MA, 2005.\n[106] Kengo\nSato\nand\nYasubumi\nSakakibara.\nRNA\nsec-\nondary\nstructural\nalignment\nwith\nconditional\nran-\ndom\n\ufb01elds.\nBioinformatics,\n21:ii237\u2013242,\n2005.\nURL\nhttp://bioinformatics.oxfordjournals.org/cgi/content/\nabstract/21/suppl_2/ii237.\n[107] Burr Settles. Abner: an open source tool for automatically tag-\nging genes, proteins, and other entity names in text. Bioinfor-\nmatics, 21(14):3191\u20133192, 2005.\n[108] Fei Sha and Fernando Pereira. Shallow parsing with conditional\nrandom \ufb01elds. In Conference on Human Language Technology\nand North American Association for Computational Linguistics\n(HLT-NAACL), pages 213\u2013220, 2003.\n[109] Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro. Pegasos:\nPrimal estimated sub-gradient solver for SVM. In International\nConference on Machine Learning (ICML), 2007.\n[110] P. Singla and P. Domingos. Discriminative training of Markov\nlogic networks. In Proceedings of the Twentieth National Con-\nference on Arti\ufb01cial Intelligence, pages 868\u2013873, Pittsburgh, PA,\n2005. AAAI Press.\n[111] David H. Stern, Thore Graepel, and David J. C. MacKay. Mod-\nelling uncertainty in the game of go. In Lawrence K. Saul, Yair\nWeiss, and L\u00b4eon Bottou, editors, Advances in Neural Information\nProcessing Systems 17, pages 1353\u20131360. MIT Press, Cambridge,\nMA, 2005.\n[112] Ilya Sutskever and Tijmen Tieleman. On the convergence prop-\nerties of contrastive divergence. In Conference on Arti\ufb01cial In-\ntelligence and Statistics (AISTATS), 2010.\n[113] Charles Sutton. E\ufb03cient Training Methods for Conditional Ran-\ndom Fields. PhD thesis, University of Massachusetts, 2008.\n[114] Charles Sutton and Andrew McCallum.\nCollective segmenta-\ntion and labeling of distant entities in information extraction. In\nICML Workshop on Statistical Relational Learning and Its Con-\nnections to Other Fields, 2004.\n[115] Charles Sutton and Andrew McCallum.\nPiecewise training of\nundirected models. In Conference on Uncertainty in Arti\ufb01cial\nIntelligence (UAI), 2005.\n[116] Charles Sutton and Andrew McCallum. An introduction to con-\nditional random \ufb01elds for relational learning. In Lise Getoor and\nBen Taskar, editors, Introduction to Statistical Relational Learn-\ning. MIT Press, 2007.\n[117] Charles Sutton and Andrew McCallum. Piecewise training for\nstructured prediction. Machine Learning, 77(2\u20133):165\u2013194, 2009.\n[118] Charles Sutton and Tom Minka. Local training and belief propa-\ngation. Technical Report TR-2006-121, Microsoft Research, 2006.\n[119] Charles Sutton, Khashayar Rohanimanesh, and Andrew McCal-\nlum. Dynamic conditional random \ufb01elds: Factorized probabilistic\nmodels for labeling and segmenting sequence data. In Interna-\ntional Conference on Machine Learning (ICML), 2004.\n[120] Charles Sutton, Andrew McCallum, and Khashayar Rohani-\nmanesh. Dynamic conditional random \ufb01elds: Factorized proba-\nbilistic models for labeling and segmenting sequence data. Jour-\nnal of Machine Learning Research, 8:693\u2013723, March 2007. URL\npublications/jmlr-sutton07a.pdf.\n[121] Ben Taskar, Pieter Abbeel, and Daphne Koller. Discriminative\nprobabilistic models for relational data. In Conference on Uncer-\ntainty in Arti\ufb01cial Intelligence (UAI), 2002.\n[122] Ben Taskar, Carlos Guestrin, and Daphne Koller. Max-margin\nMarkov networks. In Sebastian Thrun, Lawrence Saul, and Bern-\nhard Sch\u00a8olkopf, editors, Advances in Neural Information Process-\ning Systems 16. MIT Press, Cambridge, MA, 2004.\n[123] Ben Taskar, Dan Klein, Michael Collins, Daphne Koller, and\nChristopher Manning. Max-margin parsing. In Empirical Meth-\nods in Natural Language Processing (EMNLP04), 2004.\n[124] Ben Taskar, Simon Lacoste-Julien, and Dan Klein. A discrimi-\nnative matching approach to word alignment. In Conference on\nHuman Language Technology and Empirical Methods in Natural\nLanguage Processing (HLT-EMNLP), pages 73\u201380, 2005.\n[125] Erik F. Tjong Kim Sang and Fien De Meulder. Introduction to\nthe conll-2003 shared task: Language-independent named entity\nrecognition. In Walter Daelemans and Miles Osborne, editors,\nProceedings of CoNLL-2003, pages 142\u2013147. Edmonton, Canada,\n2003.\n[126] Kristina Toutanova, Dan Klein, Christopher D. Manning, and\nYoram Singer. Feature-rich part-of-speech tagging with a cyclic\ndependency network. In HLT-NAACL, 2003.\n[127] Paul Viola and Mukund Narasimhan. Learning to extract infor-\nmation from semi-structured text using a discriminative context\nfree grammar. In Proceedings of the ACM SIGIR, 2005.\n[128] S.V.N. Vishwanathan, Nicol N. Schraudolph, Mark W. Schmidt,\nand Kevin Murphy. Accelerated training of conditional random\n\ufb01elds with stochastic meta-descent. In International Conference\non Machine Learning (ICML), pages 969\u2013976, 2006.\n[129] Martin J. Wainwright. Estimating the wrong Markov random\n\ufb01eld: Bene\ufb01ts in the computation-limited setting. In Y. Weiss,\nB. Sch\u00a8olkopf, and J. Platt, editors, Advances in Neural Informa-\ntion Processing Systems 18. MIT Press, Cambridge, MA, 2006.\n[130] Martin J. Wainwright and Michael I. Jordan. Graphical models,\nexponential families, and variational inference. Technical Report\nTechnical Report 649, UC Berkeley, Dept. of Statistics, Septem-\nber 2003.\n[131] M.J. Wainwright and M.I. Jordan. Graphical models, exponential\nfamilies, and variational inference. Foundations and Trends in\nMachine Learning, 1(1-2):1\u2013305, 2008.\n[132] Hanna Wallach. E\ufb03cient training of conditional random \ufb01elds.\nM.Sc. thesis, University of Edinburgh, 2002.\n[133] Max Welling and Sridevi Parise. Bayesian random \ufb01elds: The\nBethe-Laplace approximation. In Uncertainty in Arti\ufb01cial Intel-\nligence (UAI), 2006.\n[134] Michael Wick, Khashayar Rohanimanesh, Andrew McCallum,\nand AnHai Doan. A discriminative approach to ontology align-\nment. In International Workshop on New Trends in Information\nIntegration (NTII), 2008.\n[135] Michael Wick, Khashayar Rohanimanesh, Aron Culotta, and An-\ndrew McCallum. Samplerank: Learning preferences from atomic\ngradients.\nIn Neural Information Processing Systems (NIPS)\nWorkshop on Advances in Ranking, 2009.\n[136] Jonathan S. Yedidia, William T. Freeman, and Yair Weiss. Con-\nstructing free energy approximations and generalized belief prop-\nagation algorithms.\nTechnical Report TR2004-040, Mitsubishi\nElectric Research Laboratories, 2004.\n[137] Jonathan S. Yedidia, William T. Freeman, and Yair Weiss. Con-\nstructing free-energy approximations and generalized belief prop-\nagation algorithms. IEEE Transactions on Information Theory,\n51(7):2282\u20132312, July 2005.\n[138] Jin Yu, S.V.N. Vishwanathan, Simon G\u00a8uunter, and Nicol N.\nSchraudolph.\nA quasi-Newton approach to nonsmooth convex\noptimization problems in machine learning. Journal of Machine\nLearning Research, 11:1145\u20131200, Mar 2010.\n",
        "sentence": " For example, the linear chain CRF widely used in natural language processing estimates sequences of labels for sequences of input samples due to the fact that CRF can take context into account [10], [12].",
        "context": "cessful applications have included text processing [89, 107, 108], bioin-\nformatics [106, 65], and computer vision [43, 53]. Although early appli-\ncations of CRFs used linear chains, recent applications of CRFs have\nof-the-art performance on segmenting noun phrases in text. Since then,\nlinear-chain CRFs have been applied to many problems in natural lan-\nguage processing, including named-entity recognition [72], feature in-\nas it was originally introduced [54]. The generalization from linear-\nchain CRFs to general CRFs is fairly straightforward. We simply move\nfrom using a linear-chain factor graph to a more general factor graph,"
    },
    {
        "title": "Multi-class image segmentation using conditional random fields and global classification",
        "author": [
            "N. Plath",
            "M. Toussaint",
            "S. Nakajima"
        ],
        "venue": "Proc. Int. Conf. Mach. Learn., 2009.",
        "citeRegEx": "13",
        "shortCiteRegEx": null,
        "year": 2009,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " CRF is particularly of interest in computer vision for its success in semantic image segmentation [13].",
        "context": null
    },
    {
        "title": "Kernelized structural SVM learning for supervised object segmentation",
        "author": [
            "L. Bertelli",
            "T. Yu",
            "D. Vu",
            "B. Gokturk"
        ],
        "venue": "Proc. IEEE Conf. Comp. Vis. Patt. Recogn. IEEE, 2011, pp. 2153\u20132160.",
        "citeRegEx": "14",
        "shortCiteRegEx": null,
        "year": 2011,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " SSVM can also be used for similar purposes as demonstrated in [14]. Recently, structured SVM [14], [16] and a tree-based graph learning method [23] have been proposed to learn these parameters in a principled way. presented an image segmentation approach that uses nonlinear kernel for the unary energy term in the CRF model [14].",
        "context": null
    },
    {
        "title": "Discriminative models for multi-class object layout",
        "author": [
            "C. Desai",
            "D. Ramanan",
            "C.C. Fowlkes"
        ],
        "venue": "Int. J. Comp. Vis., vol. 95, no. 1, pp. 1\u201312, 2011.",
        "citeRegEx": "15",
        "shortCiteRegEx": null,
        "year": 2011,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " SSVM has also been used to learn statistics that capture the spatial arrangements of various object classes in images [15].",
        "context": null
    },
    {
        "title": "Learning CRFs using graph cuts",
        "author": [
            "M. Szummer",
            "P. Kohli",
            "D. Hoiem"
        ],
        "venue": "Proc. Eur. Conf. Comp. Vis., 2008, pp. 582\u2013595.",
        "citeRegEx": "16",
        "shortCiteRegEx": null,
        "year": 2008,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " [16] learned optimal parameters of a CRF, avoiding tedious cross validation. Recently, structured SVM [14], [16] and a tree-based graph learning method [23] have been proposed to learn these parameters in a principled way. Note that our setting (21) differs most CRF learning settings such as [16]. These traditional CRF methods often use a linear model [16]. which can be solved efficiently by graph cuts [16], [24]. Similar to [16], the minimization (30) still can be solved efficiently by graph cuts.",
        "context": null
    },
    {
        "title": "Boosting structured prediction for imitation learning",
        "author": [
            "N. Ratliff",
            "D. Bradley",
            "J.A. Bagnell",
            "J. Chestnutt"
        ],
        "venue": "Proc. Adv. Neural Inf. Process. Syst., 2007.",
        "citeRegEx": "17",
        "shortCiteRegEx": null,
        "year": 2007,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " [17] proposed boosting for imitation learning based on structured prediction called maximum margin planning (MMP). In the MMPBoost of [17], a demonstrated policy is provided as example behavior for training and the purpose is to learn a function over features of the environment that produce policies with similar behavior.",
        "context": null
    },
    {
        "title": "Boosting algorithms as gradient descent",
        "author": [
            "L. Mason",
            "J. Baxter",
            "P.L. Bartlett",
            "M.R. Frean"
        ],
        "venue": "Proc. Adv. Neural Inf. Process. Syst., 1999, pp. 512\u2013518.",
        "citeRegEx": "18",
        "shortCiteRegEx": null,
        "year": 1999,
        "abstract": "",
        "full_text": "",
        "sentence": " Second, MMPBoost is based on the idea of gradient descent boosting [18], and our StructBoost is built upon fully corrective boosting of Shen and Li [7].",
        "context": null
    },
    {
        "title": "Structured gradient boosting",
        "author": [
            "C. Parker"
        ],
        "venue": "2007, PhD thesis, Oregon State University. [Online]. Available: http://hdl.handle. net/1957/6490",
        "citeRegEx": "19",
        "shortCiteRegEx": null,
        "year": 2007,
        "abstract": "",
        "full_text": "",
        "sentence": " Parker [19] developed a margin-based structured perceptron update and showed that it can incorporate general notions of misclassification cost as well as kernels. Therefore the method in [19] is essentially an online version of SSVM.",
        "context": null
    },
    {
        "title": "Simple training of dependency parsers via structured boosting",
        "author": [
            "Q. Wang",
            "D. Lin",
            "D. Schuurmans"
        ],
        "venue": "Proc. Int. Joint Conf. Artificial Intell., 2007, pp. 1756\u20131762.",
        "citeRegEx": "20",
        "shortCiteRegEx": null,
        "year": 2007,
        "abstract": "",
        "full_text": "",
        "sentence": " [20] learned a local predictor using standard methods, e.",
        "context": null
    },
    {
        "title": "Optimized cutting plane algorithm for support vector machines",
        "author": [
            "V. Franc",
            "S. Sonnenburg"
        ],
        "venue": "Proc. Int. Conf. Mach. Learn., New York, NY, USA, 2008, pp. 320\u2013327. [Online]. Available: http://doi.acm.org/10.1145/1390156.1390197",
        "citeRegEx": "21",
        "shortCiteRegEx": null,
        "year": 2008,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " In theory, improved cutting-plane methods such as [21] can also be adapted for solving our optimization problem at each column generation.",
        "context": null
    },
    {
        "title": "SUN database: Large-scale scene recognition from abbey to zoo",
        "author": [
            "J. Xiao",
            "J. Hays",
            "K. Ehinger",
            "A. Oliva",
            "A. Torralba"
        ],
        "venue": "Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2010.",
        "citeRegEx": "22",
        "shortCiteRegEx": null,
        "year": 2010,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " 1: The hierarchical structures of two selected subsets of the SUN dataset [22] used in our experiments for hierarchical image classification. We have constructed two hierarchical image datasets from the SUN dataset [22]. We have used the HOG features as described in [22].",
        "context": null
    },
    {
        "title": "On parameter learning in CRF-based approaches to object class image segmentation",
        "author": [
            "S. Nowozin",
            "P.V. Gehler",
            "C.H. Lampert"
        ],
        "venue": "Proc. Eur. Conf. Comp. Vis., 2010, pp. 98\u2013111.",
        "citeRegEx": "23",
        "shortCiteRegEx": null,
        "year": 2010,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Recently, structured SVM [14], [16] and a tree-based graph learning method [23] have been proposed to learn these parameters in a principled way.",
        "context": null
    },
    {
        "title": "Class segmentation and object localization with superpixel neighborhoods",
        "author": [
            "B. Fulkerson",
            "A. Vedaldi",
            "S. Soatto"
        ],
        "venue": "Proc. Int. Conf. Comp. Vis., 2009.",
        "citeRegEx": "24",
        "shortCiteRegEx": null,
        "year": 2009,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " which can be solved efficiently by graph cuts [16], [24]. The precision=recall point [24] and intersection-union score are used to evaluation our method. 6 CRF parameter learning for image segmentation In this experiment, we extend the super-pixels based segmentation method [24] with CRF parameter learning. We generate super-pixels and features same as in [24]: the neighborhood size is set to 2; histogram of visual words features are generated for each superpixel; code book size is 200. Two unary potentials: U1, U2 are constructed using two AdaBoost classifiers; one is trained on the visual word histogram features [24], and the other is trained on color histogram together with the thumbnail feature [38]. at/\u223cpinz/ [24], which is able to discourage small isolated segments. \u2016x \u2212 x\u20162 calculates the `2 norm of the color difference between two super-pixels in the LUV color-space; `(x,x) is the shared boundary length between two super-pixels, as in [24]. As [24], we use the precision = recall point and intersection-union score to evaluation our method.",
        "context": null
    },
    {
        "title": "An efficient boosting algorithm for combining preferences",
        "author": [
            "Y. Freund",
            "R. Iyer",
            "R.E. Schapire",
            "Y. Singer"
        ],
        "venue": "J. Mach. Learn. Res., vol. 4, pp. 933\u2013969, 2003.",
        "citeRegEx": "25",
        "shortCiteRegEx": null,
        "year": 2003,
        "abstract": "",
        "full_text": "",
        "sentence": " Note that RankBoost may also be applied to this problem [25].",
        "context": null
    },
    {
        "title": "Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories",
        "author": [
            "S. Lazebnik",
            "C. Schmid",
            "J. Ponce"
        ],
        "venue": "Proc. IEEE Conf. Comp. Vis. Patt. Recogn., vol. 2, 2006, pp. 2169 \u2013 2178.",
        "citeRegEx": "26",
        "shortCiteRegEx": null,
        "year": 2006,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " We run our multi-class boosting on two image datasets: MNIST2 and Scene15 [26].",
        "context": null
    },
    {
        "title": "Fast and accurate digit classification",
        "author": [
            "S. Maji",
            "J. Malik"
        ],
        "venue": "EECS Department, University of California, Berkeley, Tech. Rep. UCB/EECS-2009-159, Nov 2009. [Online]. Available: http://www. eecs.berkeley.edu/Pubs/TechRpts/2009/EECS-2009-159.html",
        "citeRegEx": "27",
        "shortCiteRegEx": null,
        "year": 2009,
        "abstract": "",
        "full_text": "",
        "sentence": " Spatial pyramid HOG features [27] are used here.",
        "context": null
    },
    {
        "title": "Scene classification using a hybrid generative/discriminative approach",
        "author": [
            "A. Bosch",
            "A. Zisserman",
            "X. Munoz"
        ],
        "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 30, no. 4, pp. 712 \u2013 727, 2008.",
        "citeRegEx": "28",
        "shortCiteRegEx": null,
        "year": 2008,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " An image is divided into 31 sub-windows in a spatial hierarchy manner [28].",
        "context": null
    },
    {
        "title": "CENTRIST: A visual descriptor for scene categorization",
        "author": [
            "J. Wu",
            "J.M. Rehg"
        ],
        "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 33, no. 8, pp. 1489\u20131501, 2011.",
        "citeRegEx": "29",
        "shortCiteRegEx": null,
        "year": 2011,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " CENTRIST [29] is used as the feature descriptor.",
        "context": null
    },
    {
        "title": "Multiclass learning, boosting, and error-correcting codes",
        "author": [
            "V. Guruswami",
            "A. Sahai"
        ],
        "venue": "Proc. Annual Conf. Computational Learning Theory. ACM, 1999, pp. 145\u2013155.",
        "citeRegEx": "30",
        "shortCiteRegEx": null,
        "year": 1999,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " ECC [30] and AdaBoost. ECC [30] and AdaBoost.",
        "context": null
    },
    {
        "title": "Improved boosting algorithms using confidence-rated predictions",
        "author": [
            "R.E. Schapire",
            "Y. Singer"
        ],
        "venue": "Mach. Learn., 1999, pp. 80\u201391.",
        "citeRegEx": "31",
        "shortCiteRegEx": null,
        "year": 1999,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " MH [31]. MH [31] on two image multi-class classification datasets: MNIST and Scene15.",
        "context": null
    },
    {
        "title": "Object detection with discriminatively trained part-based models",
        "author": [
            "P.F. Felzenszwalb",
            "R.B. Girshick",
            "D. McAllester",
            "D. Ramanan"
        ],
        "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 32, no. 9, pp. 1627\u2013 1645, September 2010.",
        "citeRegEx": "32",
        "shortCiteRegEx": null,
        "year": 2010,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " For HOG feature, we use the code in [32].",
        "context": null
    },
    {
        "title": "Visual tracking with online multiple instance learning",
        "author": [
            "B. Babenko",
            "M.-H. Yang",
            "S. Belongie"
        ],
        "venue": "Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2009.",
        "citeRegEx": "33",
        "shortCiteRegEx": null,
        "year": 2009,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " of 50), multi-instance tracking (MIL) [33], fragment tracking (Frag) [34], online AdaBoost tracking (OAB) [35], and visual tracking decomposition (VTD) [36]. See [33].",
        "context": null
    },
    {
        "title": "Robust fragments-based tracking using the integral histogram",
        "author": [
            "A. Adam",
            "E. Rivlin",
            "I. Shimshoni"
        ],
        "venue": "Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2006, pp. 798\u2013805.",
        "citeRegEx": "34",
        "shortCiteRegEx": null,
        "year": 2006,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " of 50), multi-instance tracking (MIL) [33], fragment tracking (Frag) [34], online AdaBoost tracking (OAB) [35], and visual tracking decomposition (VTD) [36].",
        "context": null
    },
    {
        "title": "Real-time tracking via on-line boosting",
        "author": [
            "H. Grabner",
            "M. Grabner",
            "H. Bischof"
        ],
        "venue": "Proc. British Mach. Vis. Conf., 2006, pp. 47\u201356.",
        "citeRegEx": "35",
        "shortCiteRegEx": null,
        "year": 2006,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " of 50), multi-instance tracking (MIL) [33], fragment tracking (Frag) [34], online AdaBoost tracking (OAB) [35], and visual tracking decomposition (VTD) [36].",
        "context": null
    },
    {
        "title": "Visual tracking decomposition",
        "author": [
            "J. Kwon",
            "K.M. Lee"
        ],
        "venue": "Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2010, pp. 1269\u20131276.",
        "citeRegEx": "36",
        "shortCiteRegEx": null,
        "year": 2010,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " of 50), multi-instance tracking (MIL) [33], fragment tracking (Frag) [34], online AdaBoost tracking (OAB) [35], and visual tracking decomposition (VTD) [36]. The sequences \u201cshaking, singer\u201d are obtained from [36], and the rest sequences are from [37].",
        "context": null
    },
    {
        "title": "Superpixel tracking",
        "author": [
            "S. Wang",
            "H. Lu",
            "F. Yang",
            "M.-H. Yang"
        ],
        "venue": "Proc. Int. Conf. Comp. Vis., pp. 1323\u20131330, 2011.",
        "citeRegEx": "37",
        "shortCiteRegEx": null,
        "year": 2011,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " The sequences \u201cshaking, singer\u201d are obtained from [36], and the rest sequences are from [37].",
        "context": null
    },
    {
        "title": "Superparsing: Scalable nonparametric image parsing with superpixels",
        "author": [
            "J. Tighe",
            "S. Lazebnik"
        ],
        "venue": "Proc. Eur. Conf. Comp. Vis., 2010, pp. 352\u2013365.",
        "citeRegEx": "38",
        "shortCiteRegEx": null,
        "year": 2010,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Two unary potentials: U1, U2 are constructed using two AdaBoost classifiers; one is trained on the visual word histogram features [24], and the other is trained on color histogram together with the thumbnail feature [38].",
        "context": null
    }
]