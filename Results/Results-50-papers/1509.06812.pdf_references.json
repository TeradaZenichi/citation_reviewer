[
    {
        "title": "ImageNet classification with deep convolutional neural networks",
        "author": [
            "A. Krizhevsky",
            "I. Sutskever",
            "G.E. Hinton"
        ],
        "venue": "In Neural Information Processing Systems,",
        "citeRegEx": "1",
        "shortCiteRegEx": "1",
        "year": 2012,
        "abstract": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called \"dropout\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.",
        "full_text": "",
        "sentence": " [1])).",
        "context": null
    },
    {
        "title": "Multiple object recognition with visual attention",
        "author": [
            "J. Ba",
            "V. Mnih",
            "K. Kavukcuoglu"
        ],
        "venue": "International Conference on Learning Representations,",
        "citeRegEx": "2",
        "shortCiteRegEx": null,
        "year": 2015,
        "abstract": "We present an attention-based model for recognizing multiple objects in\nimages. The proposed model is a deep recurrent neural network trained with\nreinforcement learning to attend to the most relevant regions of the input\nimage. We show that the model learns to both localize and recognize multiple\nobjects despite being given only class labels during training. We evaluate the\nmodel on the challenging task of transcribing house number sequences from\nGoogle Street View images and show that it is both more accurate than the\nstate-of-the-art convolutional networks and uses fewer parameters and less\ncomputation.",
        "full_text": "Published as a conference paper at ICLR 2015\nMULTIPLE OBJECT RECOGNITION WITH\nVISUAL ATTENTION\nJimmy Lei Ba\u2217\nUniversity of Toronto\njimmy@psi.utoronto.ca\nVolodymyr Mnih\nGoogle DeepMind\nvmnih@google.com\nKoray Kavukcuoglu\nGoogle DeepMind\nkorayk@google.com\nABSTRACT\nWe present an attention-based model for recognizing multiple objects in images.\nThe proposed model is a deep recurrent neural network trained with reinforcement\nlearning to attend to the most relevant regions of the input image. We show that the\nmodel learns to both localize and recognize multiple objects despite being given\nonly class labels during training. We evaluate the model on the challenging task of\ntranscribing house number sequences from Google Street View images and show\nthat it is both more accurate than the state-of-the-art convolutional networks and\nuses fewer parameters and less computation.\n1\nINTRODUCTION\nConvolutional neural networks have recently been very successful on a variety of recognition and\nclassi\ufb01cation tasks (Krizhevsky et al., 2012; Goodfellow et al., 2013; Jaderberg et al., 2014a; Vinyals\net al., 2014; Karpathy et al., 2014). One of the main drawbacks of convolutional networks (Con-\nvNets) is their poor scalability with increasing input image size so ef\ufb01cient implementations of these\nmodels on multiple GPUs (Krizhevsky et al., 2012) or even spanning multiple machines (Dean et al.,\n2012b) have become necessary.\nApplications of ConvNets to multi-object and sequence recognition from images have avoided work-\ning with big images and instead focused on using ConvNets for recognizing characters or short\nsequence segments from image patches containing reasonably tightly cropped instances (Goodfel-\nlow et al., 2013; Jaderberg et al., 2014a). Applying such a recognizer to large images containing\nuncropped instances requires integrating it with a separately trained sequence detector or a bottom-\nup proposal generator. Non-maximum suppression is often performed to obtain the \ufb01nal detections.\nWhile combining separate components trained using different objective functions has been shown to\nbe worse than end-to-end training of a single system in other domains, integrating object localization\nand recognition into a single globally-trainable architecture has been dif\ufb01cult.\nIn this work, we take inspiration from the way humans perform visual sequence recognition tasks\nsuch as reading by continually moving the fovea to the next relevant object or character, recog-\nnizing the individual object, and adding the recognized object to our internal representation of the\nsequence. Our proposed system is a deep recurrent neural network that at each step processes a\nmulti-resolution crop of the input image, called a glimpse. The network uses information from the\nglimpse to update its internal representation of the input, and outputs the next glimpse location and\npossibly the next object in the sequence. The process continues until the model decides that there\nare no more objects to process. We show how the proposed system can be trained end-to-end by\napproximately maximizing a variational lower bound on the label sequence log-likelihood. This\ntraining procedure can be used to train the model to both localize and recognize multiple objects\npurely from label sequences.\nWe evaluate the model on the task of transcribing multi-digit house numbers from publicly available\nGoogle Street View imagery. Our attention-based model outperforms the state-of-the-art ConvNets\non tightly cropped inputs while using both fewer parameters and much less computation. We also\nshow that our model outperforms ConvNets by a much larger margin in the more realistic setting of\nlarger and less tightly cropped input sequences.\n1Work done while at Google DeepMind.\n1\narXiv:1412.7755v2  [cs.LG]  23 Apr 2015\nPublished as a conference paper at ICLR 2015\n\u02c6l1\nr(1)\n1\nr(2)\n1\nr(2)\n0\n\u02c6l2\n\u02c6l3\n\u02c6l4\ncontext\nglimpse\nclassi\ufb01cation\ny1\nIcoarse\nemission\nr(2)\n2\nr(2)\n3\nr(2)\nn\nr(1)\nn\nr(1)\n3\nr(1)\n2\n\u02c6ln+1\nys\n(x1, l1)\n(x2, l2)\n(x3, l3)\n(xn, ln)\nFigure 1: The deep recurrent attention model.\n2\nRELATED WORK\nRecognizing multiple objects in images has been one of the most important goals of computer vision.\nPerhaps the most common approach to image-based classi\ufb01cation of character sequences involves\ncombining a sliding window detector with a character classi\ufb01er (Wang et al., 2012; Jaderberg et al.,\n2014b). The detector and the classi\ufb01er are typically trained separately, using different loss functions.\nThe seminal work on ConvNets of LeCun et al. (1998) introduced a graph transformer network\narchitecture for recognizing a sequence of digits when reading checks, and also showed how the\nwhole system could be trained end-to-end. That system however, still relied on a number of ad-hoc\ncomponents for extracting candidate locations.\nMore recently, ConvNets operating on cropped sequences of characters have achieved state-of-the-\nart performance on house number recognition (Goodfellow et al., 2013) and natural scene text recog-\nnition (Jaderberg et al., 2014a). Goodfellow et al. (2013) trained a separate ConvNets classi\ufb01er for\neach character position in a house number with all weights except for the output layer shared among\nthe classi\ufb01ers. Jaderberg et al. (2014a) showed that synthetically generated images of text can be\nused to train ConvNets classi\ufb01ers that achieve state-of-the-art text recognition performance on real-\nworld images of cropped text.\nOur work builds on the long line of the previous attempts on attention-based visual processing (Itti\net al., 1998; Larochelle & Hinton, 2010; Alexe et al., 2012), and in particular extends the recurrent\nattention model (RAM) proposed in Mnih et al. (2014). While RAM was shown to learn successful\ngaze strategies on cluttered digit classi\ufb01cation tasks and on a toy visual control problem it was\nnot shown to scale to real-world image tasks or multiple objects. Our approach of learning by\nmaximizing variational lower bound is equivalent to the reinforcement learning procedure used in\nRAM and is related to the work of Maes et al. (2009) who showed how reinforcement learning can\nbe used to tackle general structured prediction problems.\n3\nDEEP RECURRENT VISUAL ATTENTION MODEL\nFor simplicity, we \ufb01rst describe how our model can be applied to classifying a single object and\nlater show how it can be extended to multiple objects. Processing an image x with an attention-\nbased model is a sequential process with N steps, where each step consists of a saccade followed\nby a glimpse. At each step n, the model receives a location ln along with a glimpse observation\nxn taken at location ln. The model uses the observation to update its internal state and outputs the\nlocation ln+1 to process at the next time-step. Usually the number of pixels in the glimpse xn is\nmuch smaller than the number of pixels in the original image x, making the computational cost of\nprocessing a single glimpse independent of the size of the image.\nA graphical representation of our model is shown in Figure 2. The model can be broken down into\na number of sub-components, each mapping some input into a vector output. We will use the term\n\u201cnetwork\u201d to describe these non-linear sub-components since they are typically multi-layered neural\nnetworks.\n2\nPublished as a conference paper at ICLR 2015\nGlimpse network: The glimpse network is a non-linear function that receives the current input im-\nage patch, or glimpse, xn and its location tuple ln , where ln = (xn, yn), as input and outputs a\nvector gn. The job of the glimpse network is to extract a set of useful features from location ln of\nthe raw visual input. We will use Gimage(xn|Wimage) to denote the output vector from function\nGimage(\u00b7) that takes an image patch xn and is parameterized by weights Wimage. Gimage(\u00b7) typ-\nically consists of three convolutional hidden layers without any pooling layers followed by a fully\nconnected layer. Separately, the location tuple is mapped by Gloc(ln|Wloc) using a fully connected\nhidden layer where, both Gimage(xn|Wimage) and Gloc(ln|Wloc) have the same dimension. We\ncombine the high bandwidth image information with the low bandwidth location tuple by multiply-\ning the two vectors element-wise to get the \ufb01nal glimpse feature vector gn,\ngn = Gimage(xn|Wimage)Gloc(ln|Wloc).\n(1)\nThis type of multiplicative interaction between \u201cwhat\u201d and \u201cwhere\u201d was initially proposed\nby Larochelle & Hinton (2010).\nRecurrent network: The recurrent network aggregates information extracted from the individual\nglimpses and combines the information in a coherent manner that preserves spatial information. The\nglimpse feature vector gn from the glimpse network is supplied as input to the recurrent network at\neach time step. The recurrent network consists of two recurrent layers with non-linear function\nRrecur. We de\ufb01ned the two outputs of the recurrent layers as r(1) and r(2).\nr(1)\nn\n= Rrecur(gn, r(1)\nn\u22121|Wr1) and r(2)\nn\n= Rrecur(r(1)\nn , r(2)\nn\u22121|Wr2)\n(2)\nWe use Long-Short-Term Memory units (Hochreiter & Schmidhuber, 1997) for the non-linearity\nRrecur because of their ability to learn long-range dependencies and stable learning dynamics.\nEmission network: The emission network takes the current state of recurrent network as input and\nmakes a prediction on where to extract the next image patch for the glimpse network. It acts as a\ncontroller that directs attention based on the current internal states from the recurrent network. It\nconsists of a fully connected hidden layer that maps the feature vector r(2)\nn\nfrom the top recurrent\nlayer to a coordinate tuple \u02c6ln+1.\n\u02c6ln+1 = E(r(2)\nn |We)\n(3)\nContext network: The context network provides the initial state for the recurrent network and its\noutput is used by the emission network to predict the location of the \ufb01rst glimpse. The context\nnetwork C(\u00b7) takes a down-sampled low-resolution version of the whole input image Icoarse and\noutputs a \ufb01xed length vector cI. The contextual information provides sensible hints on where the\npotentially interesting regions are in a given image. The context network employs three convolu-\ntional layers that map a coarse image Icoarse to a feature vector used as the initial state of the top\nrecurrent layer r2 in the recurrent network. However, the bottom layer r1 is initialized with a vector\nof zeros for reasons we will explain later.\nClassi\ufb01cation network: The classi\ufb01cation network outputs a prediction for the class label y based\non the \ufb01nal feature vector r(1)\nN of the lower recurrent layer. The classi\ufb01cation network has one fully\nconnected hidden layer and a softmax output layer for the class y.\nP(y|I) = O(r1\nn|Wo)\n(4)\nIdeally, the deep recurrent attention model should learn to look at locations that are relevant for\nclassifying objects of interest. The existence of the contextual information, however, provides a\n\u201cshort cut\u201d solution such that it is much easier for the model to learn from contextual information\nthan by combining information from different glimpses. We prevent such undesirable behavior by\nconnecting the context network and classi\ufb01cation network to different recurrent layers in our deep\nmodel. As a result, the contextual information cannot be used directly by the classi\ufb01cation network\nand only affects the sequence of glimpse locations produced by the model.\n3.1\nLEARNING WHERE AND WHAT\nGiven the class labels y of image I, we can formulate learning as a supervised classi\ufb01cation problem\nwith the cross entropy objective function. The attention model predicts the class label conditioned on\n3\nPublished as a conference paper at ICLR 2015\nintermediate latent location variables l from each glimpse and extracts the corresponding patches.\nWe can thus maximize likelihood of the class label by marginalizing over the glimpse locations\nlog p(y|I, W) = log P\nl p(l|I, W)p(y|l, I, W).\nThe marginalized objective function can be learned through optimizing its variational free energy\nlower bound F:\nlog\nX\nl\np(l|I, W)p(y|l, I, W) \u2265\nX\nl\np(l|I, W) log p(y, l|I, W) + H[l]\n(5)\n=\nX\nl\np(l|I, W) log p(y|l, I, W)\n(6)\nThe learning rules can be derived by taking derivatives of the above free energy with respect to the\nmodel parameter W:\n\u2202F\n\u2202W =\nX\nl\np(l|I, W)\u2202log p(y|l, I, W)\n\u2202W\n+\nX\nl\nlog p(y|l, I, W)\u2202p(l|I, W)\n\u2202W\n(7)\n=\nX\nl\np(l|I, W)\n\u0014\u2202log p(y|l, I, W)\n\u2202W\n+ log p(y|l, I, W)\u2202log p(l|I, W)\n\u2202W\n\u0015\n(8)\nFor each glimpse in the glimpse sequence, it is dif\ufb01cult to evaluate exponentially many glimpse\nlocations during training. The summation in equation 8 can then be approximated using Monte\nCarlo samples.\n\u02dclm \u223cp(ln|I, W) = N(ln; \u02c6ln, \u03a3)\n(9)\n\u2202F\n\u2202W \u22481\nM\nM\nX\nm=1\n\u0014\u2202log p(y|\u02dclm, I, W)\n\u2202W\n+ log p(y|\u02dclm, I, W)\u2202log p(\u02dclm|I, W)\n\u2202W\n\u0015\n(10)\nThe equation 10 gives a practical algorithm to train the deep attention model. Namely, we can sample\nthe glimpse location prediction from the model after each glimpse. The samples are then used in the\nstandard backpropagation to obtain an estimator for the gradient of the model parameters. Notice\nthat log likelihood log p(y|\u02dclm, I, W) has an unbounded range that can introduce substantial high\nvariance in the gradient estimator. Especially when the sampled location is off from the object in\nthe image, the log likelihood will induce an undesired large gradient update that is backpropagated\nthrough the rest of the model.\nWe can reduce the variance in the estimator 10 by replacing the log p(y|\u02dclm, I, W) with a 0/1 discrete\nindicator function R and using a baseline technique used in Mnih et al. (2014).\nR =\n\u001a1\ny = arg maxy log p(y|\u02dclm, I, W)\n0\notherwise\n(11)\nbn = Ebaseline(r(2)\nn |Wbaseline)\n(12)\nAs shown, the recurrent network state vector r(2)\nn\nis used to estimate a state-based baseline b for\neach glimpse that signi\ufb01cantly improve the learning ef\ufb01ciency. The baseline effectively centers the\nrandom variable R and can be learned by regressing towards the expected value of R. Given both\nthe indicator function and the baseline, we have the following gradient update:\n\u2202F\n\u2202W \u22481\nM\nM\nX\nm=1\n\u0014\u2202log p(y|\u02dclm, I, W)\n\u2202W\n+ \u03bb(R \u2212b)\u2202log p(\u02dclm|I, W)\n\u2202W\n\u0015\n(13)\nwhere, hyper-parameter \u03bb balances the scale of the two gradient components.\nIn fact, by us-\ning the 0/1 indicator function, the learning rule from equation 13 is equivalent to the REIN-\nFORCE (Williams, 1992) learning rule employed in Mnih et al. (2014) for training their attention\nmodel. When viewed as a reinforcement learning update, the second term in equation 13 is an unbi-\nased estimate of the gradient with respect to W of the expected reward R under the model glimpse\n4\nPublished as a conference paper at ICLR 2015\npolicy. Here we show that such learning rule can also be motivated by simply approximately opti-\nmizing the free energy.\nDuring inference, the feedforward location prediction can be used as a deterministic prediction on\nthe location coordinates to extract the next input image patch for the model. The model behaves as\na normal feedforward network. Alternatively, our marginalized objective function equation 5 sug-\ngests a procedure to estimate the expected class prediction by using samples of location sequences\n{\u02dclm\n1 , \u00b7 \u00b7 \u00b7 , \u02dclm\nN} and averaging their predictions,\nEl[p(y|I)] \u22481\nM\nM\nX\nm=1\np(y|I, \u02dclm).\n(14)\nThis allows the attention model to be evaluated multiple times on each image with the classi\ufb01cation\npredictions being averaged. In practice, we found that averaging the log probabilities gave the best\nperformance.\nIn this paper, we encode the real valued glimpse location tuple ln using a Cartesian coordinate that is\ncentered at the middle of the input image. The ratio converting unit width in the coordinate system\nto the number of pixels is a hyper-parameter. This ratio presents an exploration versus exploitation\ntrade off. The proposed model performance is very sensitive to this setting. We found that setting\nits value to be around 15% of the input image width tends to work well.\n3.2\nMULTI-OBJECT/SEQUENTIAL CLASSIFICATION AS A VISUAL ATTENTION TASK\nOur proposed attention model can be easily extended to solve classi\ufb01cation tasks involving multiple\nobjects. To train the deep recurrent attention model for the sequential recognition task, the multiple\nobject labels for a given image need to be cast into an ordered sequence {y1, y2, \u00b7 \u00b7 \u00b7 , ys}. The\ndeep recurrent attention model then learns to predict one object at a time as it explores the image\nin a sequential manner. We can utilize a simple \ufb01xed number of glimpses for each target in the\nsequence. In addition, a new class label for the \u201cend-of-sequence\u201d symbol is included to deal with\nvariable numbers of objects in an image. We can stop the recurrent attention model once a terminal\nsymbol is predicted. Concretely, the objective function for the sequential prediction is\nlog p(y1, y2, \u00b7 \u00b7 \u00b7 , yS|I, W) =\nS\nX\ns=1\nlog\nX\nl\np(ls|I, W)p(ys|ls, I, W)\n(15)\nThe learning rule is derived as in equation 13 from the free energy and the gradient is accumulated\nacross all targets. We assign a \ufb01xed number of glimpses, N, for each target. Assuming S targets in\nan image, the model would be trained with N \u00d7 (S + 1) glimpses. The bene\ufb01t of using a recurrent\nmodel for multiple object recognition is that it is a compact and simple form yet \ufb02exible enough to\ndeal with images containing variable numbers of objects.\nLearning a model from images of many objects is a challenging setup. We can reduce the dif\ufb01-\nculty by modifying our indicator function R to be proportional to the number of targets the model\npredicted correctly.\nRs =\nX\nj\u2264s\nRj\n(16)\nIn addition, we restrict the gradient of the objective function so that it only contains glimpses up to\nthe \ufb01rst mislabeled target and ignores the targets after the \ufb01rst mistake. This curriculum-like adap-\ntion to the learning is crucial to obtain a high performance attention model for sequential prediction.\n4\nEXPERIMENTS\nTo show the effectiveness of the deep recurrent attention model (DRAM), we \ufb01rst investigate a num-\nber of multi-object classi\ufb01cation tasks involving a variant of MNIST. We then apply the proposed\nattention model to a real-world object recognition task using the multi-digit SVHN dataset Netzer\net al. (2011) and compare with the state-of-the-art deep ConvNets. A description of the models and\ntraining protocols we used can be found in the Appendix.\n5\nPublished as a conference paper at ICLR 2015\nTable 1: Error rates on the MNIST pairs\nclassi\ufb01cation task.\nModel\nTest Err.\nRAM Mnih et al. (2014)\n9%\nDRAM w/o context\n7%\nDRAM\n5%\nTable 2: Error rates on the MNIST two\ndigit addition task.\nModel\nTest Err.\nConvNet 64-64-64-512\n3.2%\nDRAM\n2.5%\nFigure 2: Left) Two examples of the learned policy on the digit pair classi\ufb01cation task. The \ufb01rst\ncolumn shows the input image while the next 5 columns show the selected glimpse locations. Right)\nTwo examples of the learned policy on the digit addition task. The \ufb01rst column shows the input\nimage while the next 5 columns show the selected glimpse locations.\nAs suggested in Mnih et al. (2014), classi\ufb01cation performance can be improved by having a glimpse\nnetwork with two different scales. Namely, given a glimpse location ln, we extract two patches\n(x1\nn, x2\nn) where x1\nn is the original patch and x2\nn is a down-sampled coarser image patch. We use the\nconcatenation of x1\nn and x2\nn as the glimpse observation. \u201cfoveal\u201d feature.\nThe hyper-parameters in our experiments are the learning rate \u03b7 and the location variance \u03a3 in\nequation 9. They are determined by grid search and cross-validation.\n4.1\nLEARNING TO FIND DIGITS\nWe \ufb01rst evaluate the effectiveness of the controller in the deep recurrent attention model using the\nMNIST handwritten digit dataset.\nWe generated a dataset of pairs of randomly picked handwritten digits in a 100x100 image with\ndistraction noise in the background. The task is to identify the 55 different combinations of the two\ndigits as a classi\ufb01cation problem. The attention models are allowed 4 glimpses before making a\nclassi\ufb01cation prediction. The goal of this experiment is to evaluate the ability of the controller and\nrecurrent network to combine information from multiple glimpses with minimum effort from the\nglimpse network. The results are shown in table (4.1). The DRAM model with a context network\nsigni\ufb01cantly outperforms the other models.\n4.2\nLEARNING TO DO ADDITION\nFor a more challenging task, we designed another dataset with two MNIST digits on an empty\n100x100 background where the task is to predict the sum of the two digits in the image as a classi-\n\ufb01cation problem with 19 targets. The model has to \ufb01nd where each digit is and add them up. When\nthe two digits are sampled uniformly from all classes, the label distribution is heavily imbalanced\nfor the summation where most of the probability mass concentrated around 10. Also, there are many\ndigit combinations that can be mapped to the same target, for example, [5,5] and [3,7].\nThe class label provides a weaker association between the visual feature and supervision signal in\nthis task than in the digit combination task. We used the same model as in the combination task.\nThe deep recurrent attention model is able to discover a glimpse policy to solve this task achieving\na 2.5% error rate. In comparison, the ConvNets take longer to learn and perform worse when given\nweak supervision.\nSome inference samples are shown in \ufb01gure 2 It is surprising that the learned glimpses policy for\npredicting the next glimpse is very different in the addition task comparing to the predicting combi-\nnation task. The model that learned to do addition toggles its glimpses between the two digits.\n6\nPublished as a conference paper at ICLR 2015\nTable 3: Whole sequence recognition error rates on\nmulti-digit SVHN.\nModel\nTest Err.\n11 layer CNN Goodfellow et al. (2013)\n3.96%\n10 layer CNN\n4.11%\nSingle DRAM\n5.1%\nSingle DRAM MC avg.\n4.4%\nforward-backward DRAM MC avg.\n3.9%\nTable 4: Whole sequence recognitionn error rate on\nenlarged multi-digit SVHN.\nModel\nTest Err.\n10 layer CNN resize\n50%\n10 layer CNN re-trained\n5.60%\nSingle DRAM focus\n5.7%\nforward-backward DRAM focus\n5.0%\nSingle DRAM \ufb01ne-tuned\n5.1%\nforward-backward DRAM \ufb01ne-tuning\n4.46%\n4.3\nLEARNING TO READ HOUSE NUMBERS\nThe publicly available multi-digit street view house number (SVHN) dataset Netzer et al. (2011)\nconsists of images of digits taken from pictures of house fronts. Following Goodfellow et al. (2013),\nwe formed a validation set of 5000 images by randomly sampling images from the training set\nand the extra set, and these were used for selecting the learning rate and sampling variance for the\nstochastic glimpse policy. The models are trained using the remaining 200,000 training images. We\nfollow the preprocessing technique from Goodfellow et al. (2013) to generate tightly cropped 64\nx 64 images with multi-digits at the center and similar data augmentation is used to create 54x54\njittered images during training. We also convert the RGB images to grayscale as we observe the\ncolor information does not affect the \ufb01nal classi\ufb01cation performance.\nWe trained a model to classify all the digits in an image sequentially with the objective function\nde\ufb01ned in equation 15. The label sequence ordering is chosen to go from left to right as the natural\nordering of the house number. The attention model is given 3 glimpses for each digit before making\na prediction. The recurrent model keeps running until it predicts a terminal label or until the longest\ndigit length in the dataset is reached. In the SVHN dataset, up to 5 digits can appear in an image.\nThis means the recurrent model will run up to 18 glimpses per image, that is 5 x 3 plus 3 glimpses\nfor a terminal label. Learning the attention model took around 3 days on a GPU.\nThe model performance is shown in table (4.3). We found that there is still a performance gap\nbetween the state-of-the-art deep ConvNet and a single DRAM that \u201creads\u201d from left to right, even\nwith the Monte Carlo averaging. The DRAM often over predicts additional digits in the place of the\nterminal class. In addition, the distribution of the leading digit in real-life follows Benford\u2019s law.\nWe therefore train a second recurrent attention model to \u201cread\u201d the house numbers from right to\nleft as a backward model.\nThe forward and backward model can share the same weights for\ntheir glimpse networks but they have different weights for their recurrent and their emission net-\nworks. The predictions of both forward and backward models can be combined to estimate the\n\ufb01nal sequence prediction. Following the observation that attention models often overestimate the\nsequence length, we can \ufb02ip \ufb01rst k number of sequence prediction from the backwards model,\nwhere k is the shorter length of the sequence length prediction between the forward and back-\nward model.\nThis simple heuristic works very well in practice and we obtain state-of-the-art\nperformance on the Street View house number dataset with the forward-backward recurrent at-\ntention model. Videos showing sample runs of the forward and backward models on SVHN test\ndata can be found at http://www.psi.toronto.edu/\u02dcjimmy/dram/forward.avi and\nhttp://www.psi.toronto.edu/\u02dcjimmy/dram/backward.avi respectively. These vi-\nsualizations show that the attention model learns to follow the slope of multi-digit house numbers\nwhen they go up or down.\nFor comparison, we also implemented a deep ConvNet with a similar architecture to the one used in\nGoodfellow et al. (2013). The network had 8 convolutional layers with 128 \ufb01lters in each followed\nby 2 fully connected layers of 3096 ReLU units. Dropout is applied to all 10 layers with 50%\ndropout rate to prevent over-\ufb01tting.\nMoreover, we generate a less cropped 110x110 multi-digit SVHN dataset by enlarging the bounding\nbox of each image such that the relative size of the digits stays the same as in the 54x54 images. Our\ndeep attention model trained on 54x54 can be directly applied to the new 110x110 dataset with no\nmodi\ufb01cation. The performance can be further improved by \u201cfocusing\u201d the model on where the digits\n7\nPublished as a conference paper at ICLR 2015\n(Giga) \ufb02oating-point op.\n10 layer CNN\nDRAM\nDRAM MC avg.\nF-B DRAM MC avg.\n54x54\n2.1\n\u22640.2\n0.35\n0.7\n110x110\n8.5\n\u22640.2\n1.1\n2.2\nparam. (millions)\n10 layer CNN\nDRAM\nDRAM MC avg.\nF-B DRAM MC avg.\n54x54\n51\n14\n14\n28\n110x110\n169\n14\n14\n28\nTable 5: Computation cost of DRAM V.S. deep ConvNets\nare. We run the model once and crop a 54x54 bounding box around the glimpse location sequence\nand feed the 54x54 bounding box to the attention model again to generate the \ufb01nal prediction.\nThis allows DRAM to \u201cfocus\u201d and obtain a similar prediction accuracy on the enlarged images\nas on the cropped image without ever being trained on large images. We also compared the deep\nConvNet trained on the 110x110 images with the \ufb01ne tuned attention model. The deep attention\nmodel signi\ufb01cantly outperforms the deep ConvNet with very little training time. The DRAM model\nonly takes a few hours to \ufb01ne-tune on the enlarged SVHN data, compared to one week for the deep\n10 layer ConvNet.\n5\nDISCUSSION\nIn our experiments, the proposed deep recurrent attention model (DRAM) outperforms the state-of-\nthe-art deep ConvNets on the standard SVHN sequence recognition task. Moreover, as we increase\nthe image area around the house numbers or lower the signal-to-noise ratio, the advantage of the\nattention model becomes more signi\ufb01cant.\nIn table 5, we compare the computational cost of our proposed deep recurrent attention model with\nthat of deep ConvNets in terms of the number of \ufb02oat-pointing operations for the multi-digit SVHN\nmodels along with the number of parameters in each model. The recurrent attention models that\nonly process a selected subset of the input scales better than a ConvNet that looks over an entire\nimage. The estimated cost for the DRAM is calculated using the maximum sequence length in the\ndataset, however the expected computational cost is much lower in practice since most of the house\nnumbers are around 2 \u22123 digits long. In addition, since the attention based model does not process\nthe whole image, it can naturally work on images of different size with the same computational cost\nindependent of the input dimensionality.\nWe also found that the attention-based model is less prone to over-\ufb01tting than ConvNets, likely\nbecause of the stochasticity in the glimpse policy during training. Though it is still bene\ufb01cial to\nregularize the attention model with some dropout noise between the hidden layers during training,\nwe found that it gives a very marginal performance boost of 0.1% on the multi-digit SVHN task. On\nthe other hand, the deep 10 layer ConvNet is only able to achieve 5.5% error rate when dropout is\nonly applied to the last two fully connected hidden layer.\nFinally, we note that DRAM can easily deal with variable length label sequences. Moreover, a\nmodel trained on a dataset with a \ufb01xed sequence length can easily be transferred and \ufb01ne tuned with\na similar dataset but longer target sequences. This is especially useful when there is lack of data for\nthe task with longer sequences.\n6\nCONCLUSION\nWe described a novel computer vision model that uses an attention mechanism to decide where to\nfocus its computation and showed how it can be trained end-to-end to sequentially classify multiple\nobjects in an image. The model outperformed the state-of-the-art ConvNets on a multi-digit house\nnumber recognition task while using both fewer parameters and less computation than the best Con-\nvNets, thereby showing that attention mechanisms can improve both the accuracy and ef\ufb01ciency of\nConvNets on a real-world task. Since our proposed deep recurrent attention model is \ufb02exible, pow-\nerful, and ef\ufb01cient, we believe that it may be a promising approach for tackling other challenging\ncomputer vision tasks.\n8\nPublished as a conference paper at ICLR 2015\n7\nACKNOWLEDGEMENTS\nWe would like to thank Geoffrey Hinton, Nando de Freitas and Chris Summer\ufb01eld for many helpful\ncomments and discussions. We would also like to thank the developers of DistBelief (Dean et al.,\n2012a).\nREFERENCES\nAlexe, Bogdan, Heess, Nicolas, Teh, Yee Whye, and Ferrari, Vittorio. Searching for objects driven by context.\nIn NIPS, 2012. 2\nDean, Jeffrey, Corrado, Greg, Monga, Rajat, Chen, Kai, Devin, Matthieu, Mao, Mark, Senior, Andrew, Tucker,\nPaul, Yang, Ke, Le, Quoc V, et al. Large scale distributed deep networks. In Advances in Neural Information\nProcessing Systems, pp. 1223\u20131231, 2012a. 9\nDean, Jeffrey, Corrado, Greg, Monga, Rajat, Chen, Kai, Devin, Matthieu, Mao, Mark, Senior, Andrew, Tucker,\nPaul, Yang, Ke, Le, Quoc V, et al. Large scale distributed deep networks. In Advances in Neural Information\nProcessing Systems, pp. 1223\u20131231, 2012b. 1\nGoodfellow, Ian J, Bulatov, Yaroslav, Ibarz, Julian, Arnoud, Sacha, and Shet, Vinay. Multi-digit number recog-\nnition from street view imagery using deep convolutional neural networks. arXiv preprint arXiv:1312.6082,\n2013. 1, 2, 7\nHochreiter, Sepp and Schmidhuber, J\u00a8urgen. Long short-term memory. Neural computation, 9(8):1735\u20131780,\n1997. 3\nItti, L., Koch, C., and Niebur, E. A model of saliency-based visual attention for rapid scene analysis. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 20(11):1254\u20131259, 1998. 2\nJaderberg, Max, Simonyan, Karen, Vedaldi, Andrea, and Zisserman, Andrew. Synthetic data and arti\ufb01cial\nneural networks for natural scene text recognition. arXiv preprint arXiv:1406.2227, 2014a. 1, 2\nJaderberg, Max, Vedaldi, Andrea, and Zisserman, Andrew. Deep features for text spotting. In Computer\nVision\u2013ECCV 2014, pp. 512\u2013528. Springer, 2014b. 2\nKarpathy, Andrej, Joulin, Armand, and Li, Fei Fei F. Deep fragment embeddings for bidirectional image\nsentence mapping. In Advances in Neural Information Processing Systems, pp. 1889\u20131897, 2014. 1\nKrizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoff. Imagenet classi\ufb01cation with deep convolutional neural\nnetworks. In Advances in Neural Information Processing Systems 25, pp. 1106\u20131114, 2012. 1\nLarochelle, Hugo and Hinton, Geoffrey E. Learning to combine foveal glimpses with a third-order boltzmann\nmachine. In NIPS, 2010. 2, 3\nLeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-based learning applied to document recognition.\nProceedings of the IEEE, 86(11):2278\u20132324, November 1998. 2\nMaes, Francis, Denoyer, Ludovic, and Gallinari, Patrick. Structured prediction with reinforcement learning.\nMachine learning, 77(2-3):271\u2013301, 2009. 2\nMnih, Volodymyr, Heess, Nicolas, Graves, Alex, and Kavukcuoglu, Koray. Recurrent models of visual atten-\ntion. arXiv preprint arXiv:1406.6247, 2014. 2, 4, 6\nNetzer, Yuval, Wang, Tao, Coates, Adam, Bissacco, Alessandro, Wu, Bo, and Ng, Andrew Y. Reading digits\nin natural images with unsupervised feature learning. In NIPS workshop on deep learning and unsupervised\nfeature learning, volume 2011, pp. 4, 2011. 5, 7\nVinyals, Oriol, Toshev, Alexander, Bengio, Samy, and Erhan, Dumitru. Show and tell: A neural image caption\ngenerator. arXiv preprint arXiv:1411.4555, 2014. 1\nWang, Tao, Wu, David J, Coates, Adam, and Ng, Andrew Y. End-to-end text recognition with convolutional\nneural networks. In Pattern Recognition (ICPR), 2012 21st International Conference on, pp. 3304\u20133308.\nIEEE, 2012. 2\nWilliams, Ronald J. Simple statistical gradient-following algorithms for connectionist reinforcement learning.\nMachine learning, 8(3-4):229\u2013256, 1992. 4\n9\nPublished as a conference paper at ICLR 2015\nModel\nTest Err.\nsmall DRAM\n5.1%\nsmall DRAM + dropout\n4.6%\nTable 6: Effectiveness of Regularization\n8\nAPPENDIX\n8.1\nGENERAL TRAINING DETAILS\nWe used the ReLU activation function in the hidden layers, g(x) = max(0, x), for the rest of the\nresults reported here or otherwise noted. We found that ReLU units signi\ufb01cantly speed up training.\nWe optimized the model parameters using stochastic gradient descent with the Nesterov momentum\ntechnique. A mini-batch size of 128 was used to estimate the gradient direction. The momentum\ncoef\ufb01cient was set to 0.9 throughout the training. The learning rate \u03b7 scheduling was applied in\ntraining to improve the convergence of the learning process. \u03b7 starts at 0.01 in the \ufb01rst epoch and\nwas exponentially reduced by a factor of 0.97 after each epoch.\n8.2\nDETAILS OF LEARNING TO FIND DIGITS\nThe unit width for the Cartesian coordinates was set to 20 and glimpse location sampling standard\ndeviation was set to 0.03. There are 512 LSTM units and 256 hidden units in each fully connected\nlayer of the model. We intentionally used a simple fully connected single hidden layer network of\n256 hidden units as Gimage(\u00b7) in the glimpse network.\n8.3\nDETAILS OF LEARNING TO READ HOUSE NUMBERS\nUnlike in the MNIST experiment, the number of digits in each image varies and digits have more\nvariations due to natural backgrounds, lighting variation, and highly variable resolution. We use a\nmuch larger deep recurrent attention model for this task. It was crucial to have a powerful glimpse\nnetwork to obtain good performance. As described in section 3, the glimpse network consists of\nthree convolutional layers with 5x5 \ufb01lter kernels in the \ufb01rst layer and 3x3 in the later two. The\nnumber of \ufb01lters in those layers was {64, 64, 128}.There are 512 LSTM units in each layer of the\nrecurrent network. Also, the fully connected hidden layers all have 1024 ReLU hidden units in each\nmodule listed in section 3. The Cartesian coordinate unit width was set to 12 pixels and glimpse\nlocation is sampled from a \ufb01xed variance of 0.03.\n10\n",
        "sentence": " This has motivated recent work on visual attention-based models [2, 3, 4], which reduce the number of parameters and computational operations by selecting informative regions of an image to focus on. [2, 3]) chooses, typically stochastically, a series of discrete glimpse locations. First, we present a new learning algorithm for stochastic attention models and compare it with a training method based on variational inference [2]. Our model achieves similar performance to the variational method [2], but with much faster training times. Such models have been applied successfully in image classification [12, 4, 3, 2], object tracking [13, 3], machine translation [6], caption generation [5], and image generation [14, 15]. Attention has been shown both to improve computational efficiency [2] and to yield insight into the network\u2019s behavior [5]. [2]). We first outline the approach of [2], who trained the model to maximize a variational lower bound on `. In the case where q(a | y, I) = p(a | I,\u03b8) is the prior, as considered by [2], this reduces to F = \u2211 As observed by [2], one must carefully use control variates in order to make this technique practical; we defer discussion of control variates to Section 4. Past work using similar gradient updates has found significant benefit from the use of control variates, or reward baselines, to reduce the variance [17, 10, 3, 11, 2]. [2] introduced several heuristics to encourage exploration, including: (1) raising the temperature of the proposal distribution, (2) regularizing the attention policy to encourage viewing all image locations, and (3) adding a regularization term to encourage high entropy in the action distribution. The goal of this experiment was to evaluate the effectiveness of our proposed WS-RAM model compared with the variational approach of [2].",
        "context": "We also found that the attention-based model is less prone to over-\ufb01tting than ConvNets, likely\nbecause of the stochasticity in the glimpse policy during training. Though it is still bene\ufb01cial to\nThe deep recurrent attention model is able to discover a glimpse policy to solve this task achieving\na 2.5% error rate. In comparison, the ConvNets take longer to learn and perform worse when given\nweak supervision.\nthe-art deep ConvNets on the standard SVHN sequence recognition task. Moreover, as we increase\nthe image area around the house numbers or lower the signal-to-noise ratio, the advantage of the\nattention model becomes more signi\ufb01cant."
    },
    {
        "title": "Recurrent models of visual attention",
        "author": [
            "V. Mnih",
            "N. Heess",
            "A. Graves",
            "K. Kavukcuoglu"
        ],
        "venue": "Neural Information Processing Systems,",
        "citeRegEx": "3",
        "shortCiteRegEx": null,
        "year": 2014,
        "abstract": "Applying convolutional neural networks to large images is computationally\nexpensive because the amount of computation scales linearly with the number of\nimage pixels. We present a novel recurrent neural network model that is capable\nof extracting information from an image or video by adaptively selecting a\nsequence of regions or locations and only processing the selected regions at\nhigh resolution. Like convolutional neural networks, the proposed model has a\ndegree of translation invariance built-in, but the amount of computation it\nperforms can be controlled independently of the input image size. While the\nmodel is non-differentiable, it can be trained using reinforcement learning\nmethods to learn task-specific policies. We evaluate our model on several image\nclassification tasks, where it significantly outperforms a convolutional neural\nnetwork baseline on cluttered images, and on a dynamic visual control problem,\nwhere it learns to track a simple object without an explicit training signal\nfor doing so.",
        "full_text": "Recurrent Models of Visual Attention\nVolodymyr Mnih\nNicolas Heess\nAlex Graves\nKoray Kavukcuoglu\nGoogle DeepMind\n{vmnih,heess,gravesa,korayk} @ google.com\nAbstract\nApplying convolutional neural networks to large images is computationally ex-\npensive because the amount of computation scales linearly with the number of\nimage pixels.\nWe present a novel recurrent neural network model that is ca-\npable of extracting information from an image or video by adaptively selecting\na sequence of regions or locations and only processing the selected regions at\nhigh resolution. Like convolutional neural networks, the proposed model has a\ndegree of translation invariance built-in, but the amount of computation it per-\nforms can be controlled independently of the input image size. While the model\nis non-differentiable, it can be trained using reinforcement learning methods to\nlearn task-speci\ufb01c policies. We evaluate our model on several image classi\ufb01cation\ntasks, where it signi\ufb01cantly outperforms a convolutional neural network baseline\non cluttered images, and on a dynamic visual control problem, where it learns to\ntrack a simple object without an explicit training signal for doing so.\n1\nIntroduction\nNeural network-based architectures have recently had great success in signi\ufb01cantly advancing the\nstate of the art on challenging image classi\ufb01cation and object detection datasets [8, 12, 19]. Their\nexcellent recognition accuracy, however, comes at a high computational cost both at training and\ntesting time. The large convolutional neural networks typically used currently take days to train on\nmultiple GPUs even though the input images are downsampled to reduce computation [12]. In the\ncase of object detection processing a single image at test time currently takes seconds when running\non a single GPU [8, 19] as these approaches effectively follow the classical sliding window paradigm\nfrom the computer vision literature where a classi\ufb01er, trained to detect an object in a tightly cropped\nbounding box, is applied independently to thousands of candidate windows from the test image at\ndifferent positions and scales. Although some computations can be shared, the main computational\nexpense for these models comes from convolving \ufb01lter maps with the entire input image, therefore\ntheir computational complexity is at least linear in the number of pixels.\nOne important property of human perception is that one does not tend to process a whole scene\nin its entirety at once. Instead humans focus attention selectively on parts of the visual space to\nacquire information when and where it is needed, and combine information from different \ufb01xations\nover time to build up an internal representation of the scene [18], guiding future eye movements\nand decision making. Focusing the computational resources on parts of a scene saves \u201cbandwidth\u201d\nas fewer \u201cpixels\u201d need to be processed. But it also substantially reduces the task complexity as\nthe object of interest can be placed in the center of the \ufb01xation and irrelevant features of the visual\nenvironment (\u201cclutter\u201d) outside the \ufb01xated region are naturally ignored.\nIn line with its fundamental role, the guidance of human eye movements has been extensively studied\nin neuroscience and cognitive science literature. While low-level scene properties and bottom up\nprocesses (e.g. in the form of saliency; [11]) play an important role, the locations on which humans\n\ufb01xate have also been shown to be strongly task speci\ufb01c (see [9] for a review and also e.g. [15, 22]). In\nthis paper we take inspiration from these results and develop a novel framework for attention-based\ntask-driven visual processing with neural networks. Our model considers attention-based processing\n1\narXiv:1406.6247v1  [cs.LG]  24 Jun 2014\nof a visual scene as a control problem and is general enough to be applied to static images, videos,\nor as a perceptual module of an agent that interacts with a dynamic visual environment (e.g. robots,\ncomputer game playing agents).\nThe model is a recurrent neural network (RNN) which processes inputs sequentially, attending to\ndifferent locations within the images (or video frames) one at a time, and incrementally combines\ninformation from these \ufb01xations to build up a dynamic internal representation of the scene or envi-\nronment. Instead of processing an entire image or even bounding box at once, at each step, the model\nselects the next location to attend to based on past information and the demands of the task. Both\nthe number of parameters in our model and the amount of computation it performs can be controlled\nindependently of the size of the input image, which is in contrast to convolutional networks whose\ncomputational demands scale linearly with the number of image pixels. We describe an end-to-end\noptimization procedure that allows the model to be trained directly with respect to a given task and\nto maximize a performance measure which may depend on the entire sequence of decisions made by\nthe model. This procedure uses backpropagation to train the neural-network components and policy\ngradient to address the non-differentiabilities due to the control problem.\nWe show that our model can learn effective task-speci\ufb01c strategies for where to look on several\nimage classi\ufb01cation tasks as well as a dynamic visual control problem. Our results also suggest that\nan attention-based model may be better than a convolutional neural network at both dealing with\nclutter and scaling up to large input images.\n2\nPrevious Work\nComputational limitations have received much attention in the computer vision literature. For in-\nstance, for object detection, much work has been dedicated to reducing the cost of the widespread\nsliding window paradigm, focusing primarily on reducing the number of windows for which the\nfull classi\ufb01er is evaluated, e.g. via classi\ufb01er cascades (e.g. [7, 24]), removing image regions from\nconsideration via a branch and bound approach on the classi\ufb01er output (e.g. [13]), or by proposing\ncandidate windows that are likely to contain objects (e.g. [1, 23]). Even though substantial speedups\nmay be obtained with such approaches, and some of these can be combined with or used as an add-on\nto CNN classi\ufb01ers [8], they remain \ufb01rmly rooted in the window classi\ufb01er design for object detection\nand only exploit past information to inform future processing of the image in a very limited way.\nA second class of approaches that has a long history in computer vision and is strongly motivated\nby human perception are saliency detectors (e.g. [11]). These approaches prioritize the processing\nof potentially interesting (\u201csalient\u201d) image regions which are typically identi\ufb01ed based on some\nmeasure of local low-level feature contrast. Saliency detectors indeed capture some of the properties\nof human eye movements, but they typically do not to integrate information across \ufb01xations, their\nsaliency computations are mostly hardwired, and they are based on low-level image properties only,\nusually ignoring other factors such as semantic content of a scene and task demands (but see [22]).\nSome works in the computer vision literature and elsewhere e.g. [2, 4, 6, 14, 16, 17, 20] have em-\nbraced vision as a sequential decision task as we do here. There, as in our work, information about\nthe image is gathered sequentially and the decision where to attend next is based on previous \ufb01xa-\ntions of the image. [4] employs the learned Bayesian observer model from [5] to the task of object\ndetection. The learning framework of [5] is related to ours as they also employ a policy gradient\nformulation (cf. section 3) but their overall setup is considerably more restrictive than ours and only\nsome parts of the system are learned.\nOur work is perhaps the most similar to the other attempts to implement attentional processing in a\ndeep learning framework [6, 14, 17]. Our formulation which employs an RNN to integrate visual\ninformation over time and to decide how to act is, however, more general, and our learning procedure\nallows for end-to-end optimization of the sequential decision process instead of relying on greedy\naction selection. We further demonstrate how the same general architecture can be used for ef\ufb01cient\nobject recognition in still images as well as to interact with a dynamic visual environment in a\ntask-driven way.\n3\nThe Recurrent Attention Model (RAM)\nIn this paper we consider the attention problem as the sequential decision process of a goal-directed\nagent interacting with a visual environment. At each point in time, the agent observes the environ-\nment only via a bandwidth-limited sensor, i.e. it never senses the environment in full. It may extract\n2\nlt-1\ngt\nGlimpse\nSensor\nxt\n\u03c1(xt , lt-1)\n\u03b8g\n0\n\u03b8g\n1\n\u03b8g\n2\nGlimpse Network : fg( \u03b8g )\nlt-1\ngt\nlt\nat\nlt\ngt+1\nlt+1\nat+1\nht\nht+1\nfg(\u03b8g)\nht-1\nfl(\u03b8l)\nfa(\u03b8a)\nfh(\u03b8h)\nfg(\u03b8g)\nfl(\u03b8l)\nfa(\u03b8a)\nfh(\u03b8h)\nxt\n\u03c1(xt , lt-1)\nlt-1\nGlimpse Sensor\nA)\nB)\nC)\nFigure 1: A) Glimpse Sensor: Given the coordinates of the glimpse and an input image, the sen-\nsor extracts a retina-like representation \u03c1(xt, lt\u22121) centered at lt\u22121 that contains multiple resolution\npatches. B) Glimpse Network: Given the location (lt\u22121) and input image (xt), uses the glimpse\nsensor to extract retina representation \u03c1(xt, lt\u22121). The retina representation and glimpse location is\nthen mapped into a hidden space using independent linear layers parameterized by \u03b80\ng and \u03b81\ng respec-\ntively using recti\ufb01ed units followed by another linear layer \u03b82\ng to combine the information from both\ncomponents. The glimpse network fg(.; {\u03b80\ng, \u03b81\ng, \u03b82\ng}) de\ufb01nes a trainable bandwidth limited sensor\nfor the attention network producing the glimpse representation gt. C) Model Architecture: Overall,\nthe model is an RNN. The core network of the model fh(.; \u03b8h) takes the glimpse representation gt as\ninput and combining with the internal representation at previous time step ht\u22121, produces the new\ninternal state of the model ht. The location network fl(.; \u03b8l) and the action network fa(.; \u03b8a) use the\ninternal state ht of the model to produce the next location to attend to lt and the action/classi\ufb01cation\nat respectively. This basic RNN iteration is repeated for a variable number of steps.\ninformation only in a local region or in a narrow frequency band. The agent can, however, actively\ncontrol how to deploy its sensor resources (e.g. choose the sensor location). The agent can also\naffect the true state of the environment by executing actions. Since the environment is only partially\nobserved the agent needs to integrate information over time in order to determine how to act and\nhow to deploy its sensor most effectively. At each step, the agent receives a scalar reward (which\ndepends on the actions the agent has executed and can be delayed), and the goal of the agent is to\nmaximize the total sum of such rewards.\nThis formulation encompasses tasks as diverse as object detection in static images and control prob-\nlems like playing a computer game from the image stream visible on the screen. For a game, the\nenvironment state would be the true state of the game engine and the agent\u2019s sensor would operate\non the video frame shown on the screen. (Note that for most games, a single frame would not fully\nspecify the game state). The environment actions here would correspond to joystick controls, and\nthe reward would re\ufb02ect points scored. For object detection in static images the state of the envi-\nronment would be \ufb01xed and correspond to the true contents of the image. The environmental action\nwould correspond to the classi\ufb01cation decision (which may be executed only after a \ufb01xed number\nof \ufb01xations), and the reward would re\ufb02ect if the decision is correct.\n3.1\nModel\nThe agent is built around a recurrent neural network as shown in Fig. 1. At each time step, it\nprocesses the sensor data, integrates information over time, and chooses how to act and how to\ndeploy its sensor at next time step:\nSensor: At each step t the agent receives a (partial) observation of the environment in the form of\nan image xt. The agent does not have full access to this image but rather can extract information\nfrom xt via its bandwidth limited sensor \u03c1, e.g. by focusing the sensor on some region or frequency\nband of interest.\nIn this paper we assume that the bandwidth-limited sensor extracts a retina-like representation\n\u03c1(xt, lt\u22121) around location lt\u22121 from image xt. It encodes the region around l at a high-resolution\nbut uses a progressively lower resolution for pixels further from l, resulting in a vector of much\n3\nlower dimensionality than the original image x. We will refer to this low-resolution representation\nas a glimpse [14]. The glimpse sensor is used inside what we call the glimpse network fg to produce\nthe glimpse feature vector gt = fg(xt, lt\u22121; \u03b8g) where \u03b8g = {\u03b80\ng, \u03b81\ng, \u03b82\ng} (Fig. 1B).\nInternal state: The agent maintains an interal state which summarizes information extracted from\nthe history of past observations; it encodes the agent\u2019s knowledge of the environment and is in-\nstrumental to deciding how to act and where to deploy the sensor. This internal state is formed\nby the hidden units ht of the recurrent neural network and updated over time by the core network:\nht = fh(ht\u22121, gt; \u03b8h). The external input to the network is the glimpse feature vector gt.\nActions: At each step, the agent performs two actions: it decides how to deploy its sensor via the\nsensor control lt, and an environment action at which might affect the state of the environment.\nThe nature of the environment action depends on the task. In this work, the location actions are\nchosen stochastically from a distribution parameterized by the location network fl(ht; \u03b8l) at time t:\nlt \u223cp(\u00b7|fl(ht; \u03b8l)). The environment action at is similarly drawn from a distribution conditioned\non a second network output at \u223cp(\u00b7|fa(ht; \u03b8a)). For classi\ufb01cation it is formulated using a softmax\noutput and for dynamic environments, its exact formulation depends on the action set de\ufb01ned for\nthat particular environment (e.g. joystick movements, motor control, ...).\nReward: After executing an action the agent receives a new visual observation of the environment\nxt+1 and a reward signal rt+1. The goal of the agent is to maximize the sum of the reward signal1\nwhich is usually very sparse and delayed: R = PT\nt=1 rt. In the case of object recognition, for\nexample, rT = 1 if the object is classi\ufb01ed correctly after T steps and 0 otherwise.\nThe above setup is a special instance of what is known in the RL community as a Partially Observ-\nable Markov Decision Process (POMDP). The true state of the environment (which can be static or\ndynamic) is unobserved. In this view, the agent needs to learn a (stochastic) policy \u03c0((lt, at)|s1:t; \u03b8)\nwith parameters \u03b8 that, at each step t, maps the history of past interactions with the environment\ns1:t = x1, l1, a1, . . . xt\u22121, lt\u22121, at\u22121, xt to a distribution over actions for the current time step, sub-\nject to the constraint of the sensor. In our case, the policy \u03c0 is de\ufb01ned by the RNN outlined above,\nand the history st is summarized in the state of the hidden units ht. We will describe the speci\ufb01c\nchoices for the above components in Section 4.\n3.2\nTraining\nThe parameters of our agent are given by the parameters of the glimpse network, the core network\n(Fig. 1C), and the action network \u03b8 = {\u03b8g, \u03b8h, \u03b8a} and we learn these to maximize the total reward\nthe agent can expect when interacting with the environment.\nMore formally, the policy of the agent, possibly in combination with the dynamics of the environ-\nment (e.g. for game-playing), induces a distribution over possible interaction sequences s1:N and we\naim to maximize the reward under this distribution: J(\u03b8) = Ep(s1:T ;\u03b8)\nhPT\nt=1 rt\ni\n= Ep(s1:T ;\u03b8) [R],\nwhere p(s1:T ; \u03b8) depends on the policy\nMaximizing J exactly is non-trivial since it involves an expectation over the high-dimensional inter-\naction sequences which may in turn involve unknown environment dynamics. Viewing the problem\nas a POMDP, however, allows us to bring techniques from the RL literature to bear: As shown by\nWilliams [26] a sample approximation to the gradient is given by\n\u2207\u03b8J =\nT\nX\nt=1\nEp(s1:T ;\u03b8) [\u2207\u03b8 log \u03c0(ut|s1:t; \u03b8)R] \u22481\nM\nM\nX\ni=1\nT\nX\nt=1\n\u2207\u03b8 log \u03c0(ui\nt|si\n1:t; \u03b8)Ri,\n(1)\nwhere si\u2019s are interaction sequences obtained by running the current agent \u03c0\u03b8 for i = 1 . . . M\nepisodes.\nThe learning rule (1) is also known as the REINFORCE rule, and it involves running the agent with\nits current policy to obtain samples of interaction sequences s1:T and then adjusting the parameters\n\u03b8 of our agent such that the log-probability of chosen actions that have led to high cumulative reward\nis increased, while that of actions having produced low reward is decreased.\n1 Depending on the scenario it may be more appropriate to consider a sum of discounted rewards, where\nrewards obtained in the distant future contribute less: R = PT\nt=1 \u03b3t\u22121rt. In this case we can have T \u2192\u221e.\n4\nEq. (1) requires us to compute \u2207\u03b8 log \u03c0(ui\nt|si\n1:t; \u03b8). But this is just the gradient of the RNN that\nde\ufb01nes our agent evaluated at time step t and can be computed by standard backpropagation [25].\nVariance Reduction : Equation (1) provides us with an unbiased estimate of the gradient but it may\nhave high variance. It is therefore common to consider a gradient estimate of the form\n1\nM\nM\nX\ni=1\nT\nX\nt=1\n\u2207\u03b8 log \u03c0(ui\nt|si\n1:t; \u03b8)\n\u0000Ri\nt \u2212bt\n\u0001\n,\n(2)\nwhere Ri\nt = PT\nt\u2032=1 ri\nt\u2032 is the cumulative reward obtained following the execution of action ui\nt, and\nbt is a baseline that may depend on si\n1:t (e.g. via hi\nt) but not on the action ui\nt itself. This estimate\nis equal to (1) in expectation but may have lower variance. It is natural to select bt = E\u03c0 [Rt] [21],\nand this form of baseline known as the value function in the reinforcement learning literature. The\nresulting algorithm increases the log-probability of an action that was followed by a larger than\nexpected cumulative reward, and decreases the probability if the obtained cumulative reward was\nsmaller. We use this type of baseline and learn it by reducing the squared error between Ri\nt\u2019s and bt.\nUsing a Hybrid Supervised Loss: The algorithm described above allows us to train the agent when\nthe \u201cbest\u201d actions are unknown, and the learning signal is only provided via the reward. For instance,\nwe may not know a priori which sequence of \ufb01xations provides most information about an unknown\nimage, but the total reward at the end of an episode will give us an indication whether the tried\nsequence was good or bad.\nHowever, in some situations we do know the correct action to take: For instance, in an object\ndetection task the agent has to output the label of the object as the \ufb01nal action. For the training\nimages this label will be known and we can directly optimize the policy to output the correct label\nassociated with a training image at the end of an observation sequence. This can be achieved, as is\ncommon in supervised learning, by maximizing the conditional probability of the true label given\nthe observations from the image, i.e. by maximizing log \u03c0(a\u2217\nT |s1:T ; \u03b8), where a\u2217\nT corresponds to the\nground-truth label(-action) associated with the image from which observations s1:T were obtained.\nWe follow this approach for classi\ufb01cation problems where we optimize the cross entropy loss to\ntrain the action network fa and backpropagate the gradients through the core and glimpse networks.\nThe location network fl is always trained with REINFORCE.\n4\nExperiments\nWe evaluated our approach on several image classi\ufb01cation tasks as well as a simple game. We \ufb01rst\ndescribe the design choices that were common to all our experiments:\nRetina and location encodings: The retina encoding \u03c1(x, l) extracts k square patches centered at\nlocation l, with the \ufb01rst patch being gw \u00d7 gw pixels in size, and each successive patch having twice\nthe width of the previous. The k patches are then all resized to gw \u00d7 gw and concatenated. Glimpse\nlocations l were encoded as real-valued (x, y) coordinates2 with (0, 0) being the center of the image\nx and (\u22121, \u22121) being the top left corner of x.\nGlimpse network: The glimpse network fg(x, l) had two fully connected layers. Let Linear(x) de-\nnote a linear transformation of the vector x, i.e. Linear(x) = Wx+b for some weight matrix W and\nbias vector b, and let Rect(x) = max(x, 0) be the recti\ufb01er nonlinearity. The output g of the glimpse\nnetwork was de\ufb01ned as g = Rect(Linear(hg) + Linear(hl)) where hg = Rect(Linear(\u03c1(x, l)))\nand hl = Rect(Linear(l)). The dimensionality of hg and hl was 128 while the dimensionality of\ng was 256 for all attention models trained in this paper.\nLocation network: The policy for the locations l was de\ufb01ned by a two-component Gaussian with a\n\ufb01xed variance. The location network outputs the mean of the location policy at time t and is de\ufb01ned\nas fl(h) = Linear(h) where h is the state of the core network/RNN.\nCore network: For the classi\ufb01cation experiments that follow the core fh was a network of recti\ufb01er\nunits de\ufb01ned as ht = fh(ht\u22121) = Rect(Linear(ht\u22121) + Linear(gt)). The experiment done on a\ndynamic environment used a core of LSTM units [10].\n2We also experimented with using a discrete representation for the locations l but found that it was dif\ufb01cult\nto learn policies over more than 25 possible discrete locations.\n5\n(a) 28x28 MNIST\nModel\nError\nFC, 2 layers (256 hiddens each)\n1.35%\n1 Random Glimpse, 8 \u00d7 8, 1 scale\n42.85%\nRAM, 2 glimpses, 8 \u00d7 8, 1 scale\n6.27%\nRAM, 3 glimpses, 8 \u00d7 8, 1 scale\n2.7%\nRAM, 4 glimpses, 8 \u00d7 8, 1 scale\n1.73%\nRAM, 5 glimpses, 8 \u00d7 8, 1 scale\n1.55%\nRAM, 6 glimpses, 8 \u00d7 8, 1 scale\n1.29%\nRAM, 7 glimpses, 8 \u00d7 8, 1 scale\n1.47%\n(b) 60x60 Translated MNIST\nModel\nError\nFC, 2 layers (64 hiddens each)\n7.56%\nFC, 2 layers (256 hiddens each)\n3.7%\nConvolutional, 2 layers\n2.31%\nRAM, 4 glimpses, 12 \u00d7 12, 3 scales\n2.29%\nRAM, 6 glimpses, 12 \u00d7 12, 3 scales\n1.86%\nRAM, 8 glimpses, 12 \u00d7 12, 3 scales\n1.84%\nTable 1: Classi\ufb01cation results on the MNIST and Translated MNIST datasets. FC denotes a fully-\nconnected network with two layers of recti\ufb01er units. The convolutional network had one layer of 8\n10 \u00d7 10 \ufb01lters with stride 5, followed by a fully connected layer with 256 units with recti\ufb01ers after\neach layer. Instances of the attention model are labeled with the number of glimpses, the number of\nscales in the retina, and the size of the retina.\n(a) Random test cases for the Translated MNIST\ntask.\n(b) Random test cases for the Cluttered Translated\nMNIST task.\nFigure 2: Examples of test cases for the Translated and Cluttered Translated MNIST tasks.\n4.1\nImage Classi\ufb01cation\nThe attention network used in the following classi\ufb01cation experiments made a classi\ufb01cation decision\nonly at the last timestep t = N. The action network fa was simply a linear softmax classi\ufb01er de\ufb01ned\nas fa(h) = exp (Linear(h)) /Z, where Z is a normalizing constant. The RNN state vector h had\ndimensionality 256. All methods were trained using stochastic gradient descent with momentum of\n0.9. Hyperparameters such as the learning rate and the variance of the location policy were selected\nusing random search [3]. The reward at the last time step was 1 if the agent classi\ufb01ed correctly and\n0 otherwise. The rewards for all other timesteps were 0.\nCentered Digits: We \ufb01rst tested the ability of our training method to learn successful glimpse\npolicies by using it to train RAM models with up to 7 glimpses on the MNIST digits dataset. The\n\u201cretina\u201d for this experiment was simply an 8\u00d78 patch, which is only big enough to capture a part of\na digit, hence the experiment also tested the ability of RAM to combine information from multiple\nglimpses. Note that since the \ufb01rst glimpse is always random, the single glimpse model is effectively\na classi\ufb01er that gets a single random 8 \u00d7 8 patch as input. We also trained a standard feedforward\nneural network with two hidden layers of 256 recti\ufb01ed linear units as a baseline. The error rates\nachieved by the different models on the test set are shown in Table 1a. We see that each additional\nglimpse improves the performance of RAM until it reaches its minimum with 6 glimpses, where it\nmatches the performance of the fully connected model training on the full 28 \u00d7 28 centered digits.\nThis demonstrates the model can successfully learn to combine information from multiple glimpses.\nNon-Centered Digits: The second problem we considered was classifying non-centered digits. We\ncreated a new task called Translated MNIST, for which data was generated by placing an MNIST\ndigit in a random location of a larger blank patch. Training cases were generated on the \ufb02y so the\neffective training set size was 50000 (the size of the MNIST training set) multiplied by the possible\nnumber of locations. Figure 2a contains a random sample of test cases for the 60 by 60 Translated\nMNIST task. Table 1b shows the results for several different models trained on the Translated\nMNIST task with 60 by 60 patches. In addition to RAM and two fully-connected networks we\nalso trained a network with one convolutional layer of 16 10 \u00d7 10 \ufb01lters with stride 5 followed\nby a recti\ufb01er nonlinearity and then a fully-connected layer of 256 recti\ufb01er units. The convolutional\nnetwork, the RAM networks, and the smaller fully connected model all had roughly the same number\nof parameters. Since the convolutional network has some degree of translation invariance built in, it\n6\n(a) 60x60 Cluttered Translated MNIST\nModel\nError\nFC, 2 layers (64 hiddens each)\n28.96%\nFC, 2 layers (256 hiddens each)\n13.2%\nConvolutional, 2 layers\n7.83%\nRAM, 4 glimpses, 12 \u00d7 12, 3 scales\n7.1%\nRAM, 6 glimpses, 12 \u00d7 12, 3 scales\n5.88%\nRAM, 8 glimpses, 12 \u00d7 12, 3 scales\n5.23%\n(b) 100x100 Cluttered Translated MNIST\nModel\nError\nConvolutional, 2 layers\n16.51%\nRAM, 4 glimpses, 12 \u00d7 12, 4 scales\n14.95%\nRAM, 6 glimpses, 12 \u00d7 12, 4 scales\n11.58%\nRAM, 8 glimpses, 12 \u00d7 12, 4 scales\n10.83%\nTable 2: Classi\ufb01cation on the Cluttered Translated MNIST dataset. FC denotes a fully-connected\nnetwork with two layers of recti\ufb01er units. The convolutional network had one layer of 8 10 \u00d7 10\n\ufb01lters with stride 5, followed by a fully connected layer with 256 units in the 60 \u00d7 60 case and\n86 units in the 100 \u00d7 100 case with recti\ufb01ers after each layer. Instances of the attention model are\nlabeled with the number of glimpses, the size of the retina, and the number of scales in the retina.\nAll models except for the big fully connected network had roughly the same number of parameters.\nFigure 3: Examples of the learned policy on 60 \u00d7 60 cluttered-translated MNIST task. Column 1:\nThe input image with glimpse path overlaid in green. Columns 2-7: The six glimpses the network\nchooses. The center of each image shows the full resolution glimpse, the outer low resolution areas\nare obtained by upscaling the low resolution glimpses back to full image size. The glimpse paths\nclearly show that the learned policy avoids computation in empty or noisy parts of the input space\nand directly explores the area around the object of interest.\nattains a signi\ufb01cantly lower error rate of 2.3% than the fully connected networks. However, RAM\nwith 4 glimpses gets roughly the same performance as the convolutional network and outperforms\nit for 6 and 8 glimpses, reaching roughly 1.9% error. This is possible because the attention model\ncan focus its retina on the digit and hence learn a translation invariant policy. This experiment also\nshows that the attention model is able to successfully search for an object in a big image when the\nobject is not centered.\nCluttered Non-Centered Digits: One of the most challenging aspects of classifying real-world\nimages is the presence of a wide range clutter. Systems that operate on the entire image at full\nresolution are particularly susceptible to clutter and must learn to be invariant to it. One possible\nadvantage of an attention mechanism is that it may make it easier to learn in the presence of clutter\nby focusing on the relevant part of the image and ignoring the irrelevant part. We test this hypothesis\nwith several experiments on a new task we call Cluttered Translated MNIST. Data for this task was\ngenerated by \ufb01rst placing an MNIST digit in a random location of a larger blank image and then\nadding random 8 by 8 subpatches from other random MNIST digits to random locations of the\nimage. The goal is to classify the complete digit present in the image. Figure 2b shows a random\nsample of test cases for the 60 by 60 Cluttered Translated MNIST task.\nTable 2a shows the classi\ufb01cation results for the models we trained on 60 by 60 Cluttered Translated\nMNIST with 4 pieces of clutter. The presence of clutter makes the task much more dif\ufb01cult but the\nperformance of the attention model is affected less than the performance of the other models. RAM\nwith 4 glimpses reaches 7.1% error, which outperforms fully-connected models by a wide margin\nand the convolutional neural network by 0.7%, and RAM trained with 6 and 8 glimpses achieves\neven lower error. Since RAM achieves larger relative error improvements over a convolutional\nnetwork in the presence of clutter these results suggest the attention-based models may be better at\ndealing with clutter than convolutional networks because they can simply ignore it by not looking at\n7\nit. Two samples of learned policy is shown in Figure 6 and more are included in the supplementary\nmaterials. The \ufb01rst column shows the original data point with the glimpse path overlaid. The\nlocation of the \ufb01rst glimpse is marked with a \ufb01lled circle and the location of the \ufb01nal glimpse is\nmarked with an empty circle. The intermediate points on the path are traced with solid straight\nlines. Each consecutive image to the right shows a representation of the glimpse that the network\nsees. It can be seen that the learned policy can reliably \ufb01nd and explore around the object of interest\nwhile avoiding clutter at the same time.\nTo further test this hypothesis we also performed experiments on 100 by 100 Cluttered Translated\nMNIST with 8 pieces of clutter. The test errors achieved by the models we compared are shown\nin Table 2b. The results show similar improvements of RAM over a convolutional network. It has\nto be noted that the overall capacity and the amount of computation of our model does not change\nfrom 60 \u00d7 60 images to 100 \u00d7 100, whereas the hidden layer of the convolutional network that is\nconnected to the linear layer grows linearly with the number of pixels in the input.\n4.2\nDynamic Environments\nOne appealing property of the recurrent attention model is that it can be applied to videos or inter-\nactive problems with a visual input just as easily as to static image tasks. We test the ability of our\napproach to learn a control policy in a dynamic visual environment while perceiving the environment\nthrough a bandwidth-limited retina by training it to play a simple game. The game is played on a 24\nby 24 screen of binary pixels and involves two objects: a single pixel that represents a ball falling\nfrom the top of the screen while bouncing off the sides of the screen and a two-pixel paddle posi-\ntioned at the bottom of the screen which the agent controls with the aim of catching the ball. When\nthe falling pixel reaches the bottom of the screen the agent either gets a reward of 1 if the paddle\noverlaps with the ball and a reward of 0 otherwise. The game then restarts from the beginning.\nWe trained the recurrent attention model to play the game of \u201cCatch\u201d using only the \ufb01nal reward\nas input. The network had a 6 by 6 retina at three scales as its input, which means that the agent\nhad to capture the ball in the 6 by 6 highest resolution region in order to know its precise position.\nIn addition to the two location actions, the attention model had three game actions (left, right, and\ndo nothing) and the action network fa used a linear softmax to model a distribution over the game\nactions. We used a core network of 256 LSTM units.\nWe performed random search to \ufb01nd suitable hyper-parameters and trained each agent for 20 mil-\nlion frames.\nA video of the best agent, which catches the ball roughly 85% of the time, can\nbe downloaded from http://www.cs.toronto.edu/\u02dcvmnih/docs/attention.mov.\nThe video shows that the recurrent attention model learned to play the game by tracking the ball\nnear the bottom of the screen. Since the agent was not in any way told to track the ball and was\nonly rewarded for catching it, this result demonstrates the ability of the model to learn effective\ntask-speci\ufb01c attention policies.\n5\nDiscussion\nThis paper introduced a novel visual attention model that is formulated as a single recurrent neural\nnetwork which takes a glimpse window as its input and uses the internal state of the network to\nselect the next location to focus on as well as to generate control signals in a dynamic environment.\nAlthough the model is not differentiable, the proposed uni\ufb01ed architecture is trained end-to-end\nfrom pixel inputs to actions using a policy gradient method. The model has several appealing prop-\nerties. First, both the number of parameters and the amount of computation RAM performs can\nbe controlled independently of the size of the input images. Second, the model is able to ignore\nclutter present in an image by centering its retina on the relevant regions. Our experiments show that\nRAM signi\ufb01cantly outperforms a convolutional architecture with a comparable number of parame-\nters on a cluttered object classi\ufb01cation task. Additionally, the \ufb02exibility of our approach allows for\na number of interesting extensions. For example, the network can be augmented with another action\nthat allows it terminate at any time point and make a \ufb01nal classi\ufb01cation decision. Our preliminary\nexperiments show that this allows the network to learn to stop taking glimpses once it has enough in-\nformation to make a con\ufb01dent classi\ufb01cation. The network can also be allowed to control the scale at\nwhich the retina samples the image allowing it to \ufb01t objects of different size in the \ufb01xed size retina.\nIn both cases, the extra actions can be simply added to the action network fa and trained using the\npolicy gradient procedure we have described. Given the encouraging results achieved by RAM, ap-\nplying the model to large scale object recognition and video classi\ufb01cation is a natural direction for\nfuture work.\n8\nSupplementary Material\nFigure 4: Examples of the learned policy on 60 \u00d7 60 cluttered-translated MNIST task. Column 1:\nThe input image from MNIST test set with glimpse path overlaid in green (correctly classi\ufb01ed) or\nred (false classi\ufb01ed). Columns 2-7: The six glimpses the network chooses. The center of each image\nshows the full resolution glimpse, the outer low resolution areas are obtained by upscaling the low\nresolution glimpses back to full image size. The glimpse paths clearly show that the learned policy\navoids computation in empty or noisy parts of the input space and directly explores the area around\nthe object of interest.\n9\nFigure 5: Examples of the learned policy on 60 \u00d7 60 cluttered-translated MNIST task. Column 1:\nThe input image from MNIST test set with glimpse path overlaid in green (correctly classi\ufb01ed) or\nred (false classi\ufb01ed). Columns 2-7: The six glimpses the network chooses. The center of each image\nshows the full resolution glimpse, the outer low resolution areas are obtained by upscaling the low\nresolution glimpses back to full image size. The glimpse paths clearly show that the learned policy\navoids computation in empty or noisy parts of the input space and directly explores the area around\nthe object of interest.\n10\nFigure 6: Examples of the learned policy on 60 \u00d7 60 cluttered-translated MNIST task. Column 1:\nThe input image from MNIST test set with glimpse path overlaid in green (correctly classi\ufb01ed) or\nred (false classi\ufb01ed). Columns 2-7: The six glimpses the network chooses. The center of each image\nshows the full resolution glimpse, the outer low resolution areas are obtained by upscaling the low\nresolution glimpses back to full image size. The glimpse paths clearly show that the learned policy\navoids computation in empty or noisy parts of the input space and directly explores the area around\nthe object of interest.\n11\nReferences\n[1] Bogdan Alexe, Thomas Deselaers, and Vittorio Ferrari. What is an object? In CVPR, 2010.\n[2] Bogdan Alexe, Nicolas Heess, Yee Whye Teh, and Vittorio Ferrari. Searching for objects driven by\ncontext. In NIPS, 2012.\n[3] James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. The Journal of\nMachine Learning Research, 13:281\u2013305, 2012.\n[4] Nicholas J. Butko and Javier R. Movellan. Optimal scanning for faster object detection. In CVPR, 2009.\n[5] N.J. Butko and J.R. Movellan. I-pomdp: An infomax model of eye movement. In Proceedings of the 7th\nIEEE International Conference on Development and Learning, ICDL \u201908, pages 139 \u2013144, 2008.\n[6] Misha Denil, Loris Bazzani, Hugo Larochelle, and Nando de Freitas. Learning where to attend with deep\narchitectures for image tracking. Neural Computation, 24(8):2151\u20132184, 2012.\n[7] Pedro F. Felzenszwalb, Ross B. Girshick, and David A. McAllester. Cascade object detection with de-\nformable part models. In CVPR, 2010.\n[8] Ross B. Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate\nobject detection and semantic segmentation. CoRR, abs/1311.2524, 2013.\n[9] Mary Hayhoe and Dana Ballard. Eye movements in natural behavior. Trends in Cognitive Sciences,\n9(4):188 \u2013 194, 2005.\n[10] Sepp Hochreiter and J\u00a8urgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735\u2013\n1780, 1997.\n[11] L. Itti, C. Koch, and E. Niebur. A model of saliency-based visual attention for rapid scene analysis. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 20(11):1254\u20131259, 1998.\n[12] Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton.\nImagenet classi\ufb01cation with deep convolutional\nneural networks. In Advances in Neural Information Processing Systems 25, pages 1106\u20131114, 2012.\n[13] Christoph H. Lampert, Matthew B. Blaschko, and Thomas Hofmann. Beyond sliding windows: Object\nlocalization by ef\ufb01cient subwindow search. In CVPR, 2008.\n[14] Hugo Larochelle and Geoffrey E. Hinton. Learning to combine foveal glimpses with a third-order boltz-\nmann machine. In NIPS, 2010.\n[15] Stefan Mathe and Cristian Sminchisescu. Action from still image dataset and inverse optimal control to\nlearn task speci\ufb01c visual scanpaths. In NIPS, 2013.\n[16] Lucas Paletta, Gerald Fritz, and Christin Seifert. Q-learning of sequential attention for visual object\nrecognition from informative local descriptors. In CVPR, 2005.\n[17] M. Ranzato. On Learning Where To Look. ArXiv e-prints, 2014.\n[18] Ronald A. Rensink. The dynamic representation of scenes. Visual Cognition, 7(1-3):17\u201342, 2000.\n[19] Pierre Sermanet, David Eigen, Xiang Zhang, Micha\u00a8el Mathieu, Rob Fergus, and Yann LeCun. Overfeat:\nIntegrated recognition, localization and detection using convolutional networks. CoRR, abs/1312.6229,\n2013.\n[20] Kenneth O. Stanley and Risto Miikkulainen. Evolving a roving eye for go. In GECCO, 2004.\n[21] Richard S. Sutton, David Mcallester, Satinder Singh, and Yishay Mansour. Policy gradient methods for\nreinforcement learning with function approximation. In NIPS, pages 1057\u20131063. MIT Press, 2000.\n[22] Antonio Torralba, Aude Oliva, Monica S Castelhano, and John M Henderson. Contextual guidance of eye\nmovements and attention in real-world scenes: the role of global features in object search. Psychol Rev,\npages 766\u2013786, 2006.\n[23] K E A van de Sande, J.R.R. Uijlings, T Gevers, and A.W.M. Smeulders. Segmentation as Selective Search\nfor Object Recognition. In ICCV, 2011.\n[24] Paul A. Viola and Michael J. Jones. Rapid object detection using a boosted cascade of simple features. In\nCVPR, 2001.\n[25] Daan Wierstra, Alexander Foerster, Jan Peters, and Juergen Schmidhuber. Solving deep memory pomdps\nwith recurrent policy gradients. In ICANN. 2007.\n[26] R.J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning.\nMachine Learning, 8(3):229\u2013256, 1992.\n12\n",
        "sentence": " This has motivated recent work on visual attention-based models [2, 3, 4], which reduce the number of parameters and computational operations by selecting informative regions of an image to focus on. [2, 3]) chooses, typically stochastically, a series of discrete glimpse locations. Such models have been applied successfully in image classification [12, 4, 3, 2], object tracking [13, 3], machine translation [6], caption generation [5], and image generation [14, 15]. Such models have been applied successfully in image classification [12, 4, 3, 2], object tracking [13, 3], machine translation [6], caption generation [5], and image generation [14, 15]. The choice of baseline is crucial to good performance; NVIL uses a separate neural network to compute the baseline, an approach also used by [3] in the context of attention networks. Past work using similar gradient updates has found significant benefit from the use of control variates, or reward baselines, to reduce the variance [17, 10, 3, 11, 2].",
        "context": "this paper we take inspiration from these results and develop a novel framework for attention-based\ntask-driven visual processing with neural networks. Our model considers attention-based processing\n1\narXiv:1406.6247v1  [cs.LG]  24 Jun 2014\nan attention-based model may be better than a convolutional neural network at both dealing with\nclutter and scaling up to large input images.\n2\nPrevious Work\nComputational limitations have received much attention in the computer vision literature. For in-\nRecurrent Models of Visual Attention\nVolodymyr Mnih\nNicolas Heess\nAlex Graves\nKoray Kavukcuoglu\nGoogle DeepMind\n{vmnih,heess,gravesa,korayk} @ google.com\nAbstract\nApplying convolutional neural networks to large images is computationally ex-"
    },
    {
        "title": "Learning generative models with visual attention",
        "author": [
            "Y. Tang",
            "N. Srivastava",
            "R. Salakhutdinov"
        ],
        "venue": "Neural Information Processing Systems,",
        "citeRegEx": "4",
        "shortCiteRegEx": null,
        "year": 2014,
        "abstract": "Attention has long been proposed by psychologists as important for\neffectively dealing with the enormous sensory stimulus available in the\nneocortex. Inspired by the visual attention models in computational\nneuroscience and the need of object-centric data for generative models, we\ndescribe for generative learning framework using attentional mechanisms.\nAttentional mechanisms can propagate signals from region of interest in a scene\nto an aligned canonical representation, where generative modeling takes place.\nBy ignoring background clutter, generative models can concentrate their\nresources on the object of interest. Our model is a proper graphical model\nwhere the 2D Similarity transformation is a part of the top-down process. A\nConvNet is employed to provide good initializations during posterior inference\nwhich is based on Hamiltonian Monte Carlo. Upon learning images of faces, our\nmodel can robustly attend to face regions of novel test subjects. More\nimportantly, our model can learn generative models of new faces from a novel\ndataset of large images where the face locations are not known.",
        "full_text": "Learning Generative Models with Visual Attention\nYichuan Tang, Nitish Srivastava, Ruslan Salakhutdinov\nDepartment of Computer Science\nUniversity of Toronto\nToronto, Ontario, Canada\n{tang,nitish,rsalakhu}@cs.toronto.edu\nAbstract\nAttention has long been proposed by psychologists to be important for ef\ufb01ciently\ndealing with the massive amounts of sensory stimulus in the neocortex. Inspired\nby the attention models in visual neuroscience and the need for object-centered\ndata for generative models, we propose a deep-learning based generative frame-\nwork using attention. The attentional mechanism propagates signals from the\nregion of interest in a scene to an aligned canonical representation for genera-\ntive modeling. By ignoring scene background clutter, the generative model can\nconcentrate its resources on the object of interest. A convolutional neural net is\nemployed to provide good initializations during posterior inference which uses\nHamiltonian Monte Carlo. Upon learning images of faces, our model can robustly\nattend to the face region of novel test subjects. More importantly, our model can\nlearn generative models of new faces from a novel dataset of large images where\nthe face locations are not known.1\n1\nIntroduction\nBuilding rich generative models that are capable of extracting useful, high-level latent represen-\ntations from high-dimensional sensory input lies at the core of solving many AI-related tasks, in-\ncluding object recognition, speech perception and language understanding. These models capture\nunderlying structure in data by de\ufb01ning \ufb02exible probability distributions over high-dimensional data\nas part of a complex, partially observed system. Some of the successful generative models that\nare able to discover meaningful high-level latent representations include the Boltzmann Machine\nfamily of models: Restricted Boltzmann Machines, Deep Belief Nets [1], and Deep Boltzmann Ma-\nchines [2]. Mixture models, such as Mixtures of Factor Analyzers [3] and Mixtures of Gaussians,\nhave also been used for modeling natural image patches [4]. More recently, denoising auto-encoders\nhave been proposed as a way to model the transition operator that has the same invariant distribution\nas the data generating distribution [5].\nGenerative models have an advantage over discriminative models when part of the images are oc-\ncluded or missing. Occlusions are very common in realistic settings and have been largely ignored\nin recent literature on deep learning. In addition, prior knowledge can be easily incorporated in\ngenerative models in the forms of structured latent variables, such as lighting and deformable parts.\nHowever, the enormous amount of content in high-resolution images makes generative learning dif-\n\ufb01cult [6, 7]. Therefore, generative models have found most success in learning to model small\npatches of natural images and objects: Zoran and Weiss [4] learned a mixture of Gaussians model\nover 8\u00d78 image patches; Salakhutdinov and Hinton [2] used 64\u00d764 centered and uncluttered stereo\nimages of toy objects on a clear background; Tang et al. [8] used 24\u00d724 images of centered and\ncropped faces. The fact that these models require curated training data limits their applicability on\nusing the (virtually) unlimited unlabeled data.\nIn this paper, we propose a framework to infer the region of interest in a big image for genera-\ntive modeling. This will allow us to learn a generative model of faces on a very large dataset of\n1In the proceedings of Neural Information Processing Systems, 2014, Montr\u00e9al, Qu\u00e9bec, Canada.\n1\narXiv:1312.6110v3  [cs.CV]  21 Feb 2015\n(unlabeled) images containing faces. Our framework is able to dynamically route the relevant infor-\nmation to the generative model and can ignore the background clutter. The need to dynamically and\nselectively route information is also present in the biological brain. Plethora of evidence points to\nthe presence of attention in the visual cortex [9, 10]. Recently, in visual neuroscience, attention has\nbeen shown to exist not only in extrastriate areas, but also all the way down to V1 [11].\nAttention as a form of routing was originally proposed by Anderson and Van Essen [12] and then\nextended by Olshausen et al. [13]. Dynamic routing has been hypothesized as providing a way for\nachieving shift and size invariance in the visual cortex [14, 15]. Tsotsos et al. [16] proposed a model\ncombining search and attention called the Selective Tuning model. Larochelle and Hinton [17] pro-\nposed a way of using third-order Boltzmann Machines to combine information gathered from many\nfoveal glimpses. Their model chooses where to look next to \ufb01nd locations that are most informative\nof the object class. Reichert et al. [18] proposed a hierarchical model to show that certain aspects of\ncovert object-based attention can be modeled by Deep Boltzmann Machines. Several other related\nmodels attempt to learn where to look for objects [19, 20] and for video based tracking [21]. Inspired\nby Olshausen et al. [13], we use 2D similarity transformations to implement the scaling, rotation,\nand shift operation required for routing. Our main motivation is to enable the learning of generative\nmodels in big images where the location of the object of interest is unknown a-priori.\n2\nGaussian Restricted Boltzmann Machines\nBefore we describe our model, we brie\ufb02y review the Gaussian Restricted Boltzmann Machine\n(GRBM) [22], as it will serve as the building block for our attention-based model. GRBMs are\na type of Markov Random Field model that has a bipartite structure with real-valued visible vari-\nables v \u2208RD connected to binary stochastic hidden variables h \u2208{0, 1}H. The energy of the joint\ncon\ufb01guration {v, h} of the Gaussian RBM is de\ufb01ned as follows:\nEGRBM(v, h; \u0398)\n=\n1\n2\nX\ni\n(vi \u2212bi)2\n\u03c32\ni\n\u2212\nX\nj\ncjhj \u2212\nX\nij\nWijvihj,\n(1)\nwhere \u0398 = {W, b, c, \u03c3} are the model parameters. The marginal distribution over the visible vector\nv is P(v; \u0398) =\n1\nZ(\u0398)\nP\nh exp (\u2212E(v, h; \u0398)) and the corresponding conditional distributions take\nthe following form:\np(hj = 1|v)\n=\n1/\n\u00001 + exp(\u2212\nX\ni\nWijvi \u2212cj)\n\u0001\n,\n(2)\np(vi|h)\n=\nN(vi; \u00b5i, \u03c32\ni ),\nwhere \u00b5i = bi + \u03c32\ni\nX\nj\nWijhj.\n(3)\nObserve that conditioned on the states of the hidden variables (Eq. 3), each visible unit is modeled\nby a Gaussian distribution, whose mean is shifted by the weighted combination of the hidden unit\nactivations. Unlike directed models, an RBM\u2019s conditional distribution over hidden nodes is factorial\nand can be easily computed.\nWe can also add a binary RBM on top of the learned GRBM by treating the inferred h as the\n\u201cvisible\u201d layer together with a second hidden layer h2. This results in a 2-layer Gaussian Deep\nBelief Network (GDBN) [1] that is a more powerful model of v.\nSpeci\ufb01cally, in a GDBN model, p(h1, h2) is modeled by the energy function of the 2nd-layer RBM,\nwhile p(v1|h1) is given by Eq. 3. Ef\ufb01cient inference can be performed using the greedy approach\nof [1] by treating each DBN layer as a separate RBM model. GDBNs have been applied to various\ntasks, including image classi\ufb01cation, video action and speech recognition [6, 23, 24, 25].\n3\nThe Model\nLet I be a high resolution image of a scene, e.g. a 256\u00d7256 image. We want to use attention to\npropagate regions of interest from I up to a canonical representation. For example, in order to learn\na model of faces, the canonical representation could be a 24\u00d724 aligned and cropped frontal face\nimage. Let v \u2208RD represent this low resolution canonical image. In this work, we focus on a Deep\nBelief Network2 to model v.\n2Other generative models can also be used with our attention framework.\n2\nOlshausen et al. 93\nOur model\n2d similarity\ntransformation\nFigure 1: Left: The Shifter Circuit, a well-known neuroscience model for visual attention [13]; Right: The\nproposed model uses 2D similarity transformations from geometry and a Gaussian DBN to model canonical\nface images. Associative memory corresponds to the DBN, object-centered frame correspond to the visible\nlayer and the attentional mechanism is modeled by 2D similarity transformations.\nThis is illustrated in the diagrams of Fig. 1. The left panel displays the model of Olshausen et.al. [13],\nwhereas the right panel shows a graphical diagram of our proposed generative model with an atten-\ntional mechanism. Here, h1 and h2 represent the latent hidden variables of the DBN model, and\n\u25b3x, \u25b3y, \u25b3\u03b8, \u25b3s (position, rotation, and scale) are the parameters of the 2D similarity transforma-\ntion.\nThe 2D similarity transformation is used to rotate, scale, and translate the canonical image v onto the\ncanvas that we denote by I. Let p = [x y]T be a pixel coordinate (e.g. [0, 0] or [0, 1]) of the canonical\nimage v. Let {p} be the set of all coordinates of v. For example, if v is 24\u00d724, then {p} ranges\nfrom [0, 0] to [23, 23]. Let the \u201cgaze\u201d variables u \u2208R4 \u2261[\u25b3x, \u25b3y, \u25b3\u03b8, \u25b3s] be the parameter\nof the Similarity transformation. In order to simplify derivations and to make transformations be\nlinear w.r.t. the transformation parameters, we can equivalently rede\ufb01ne u = [a, b, \u25b3x, \u25b3y],\nwhere a = s sin(\u03b8) \u22121 and b = s cos(\u03b8) (see [26] for details). We further de\ufb01ne a function\nw := w(p, u) \u2192p\u2032 as the transformation function to warp points p to p\u2032:\np\u2032 \u225c\nh x\u2032\ny\u2032\ni\n=\nh 1 + a\n\u2212b\nb\n1 + a\nih x\ny\ni\n+\nh \u25b3x\n\u25b3y\ni\n.\n(4)\nWe use the notation I({p}) to denote the bilinear interpolation of I at coordinates {p} with anti-\naliasing. Let x(u) be the extracted low-resolution image at warped locations p\u2032:\nx(u) \u225cI(w({p}, u)).\n(5)\nIntuitively, x(u) is a patch extracted from I according to the shift, rotation and scale parameters\nof u, as shown in Fig. 1, right panel. It is this patch of data that we seek to model generatively. Note\nthat the dimensionality of x(u) is equal to the cardinality of {p}, where {p} denotes the set of pixel\ncoordinates of the canonical image v. Unlike standard generative learning tasks, the data x(u) is\nnot static but changes with the latent variables u. Given v and u, we model the top-down generative\nprocess over3 x with a Gaussian distribution having a diagonal covariance matrix \u03c32I:\np(x|v, u, I) \u221dexp\n\u0012\n\u22121\n2\nX\ni\n(xi(u) \u2212vi)2\n\u03c32\ni\n\u0013\n.\n(6)\nThe fact that we do not seek to model the rest of the regions/pixels of I is by design. By using 2D\nsimilarity transformation to mimic attention, we can discard the complex background of the scene\nand let the generative model focus on the object of interest. The proposed generative model takes\nthe following form:\np(x, v, u|I) = p(x|v, u, I)p(v)p(u),\n(7)\nwhere for p(u) we use a \ufb02at prior that is constant for all u, and p(v) is de\ufb01ned by a 2-layer Gaussian\nDeep Belief Network. The conditional p(x|v, u, I) is given by a Gaussian distribution as in Eq. 6.\nTo simplify the inference procedure, p(x|v, u, I) and the GDBN model of v, p(v), will share the\nsame noise parameters \u03c3i.\n3We will often omit dependence of x on u for clarity of presentation.\n3\n4\nInference\nWhile the generative equations in the last section are straightforward and intuitive, inference in these\nmodels is typically intractable due to the complicated energy landscape of the posterior. During\ninference, we wish to compute the distribution over the gaze variables u and canonical object v given\nthe big image I. Unlike in standard RBMs and DBNs, there are no simplifying factorial assumptions\nabout the conditional distribution of the latent variable u. Having a 2D similarity transformation is\nreminiscent of third-order Boltzmann machines with u performing top-down multiplicative gating\nof the connections between v and I. It is well known that inference in these higher-order models is\nrather complicated.\nOne way to perform inference in our model is to resort to Gibbs sampling by computing the set of\nalternating conditional posteriors: The conditional distribution over the canonical image v takes the\nfollowing form:\np(v|u, h1, I) = N\n\u0010\u00b5 + x(u)\n2\n; \u03c32\u0011\n,\n(8)\nwhere \u00b5i = bi + \u03c32\ni\nP\nj Wijh1\nj is the top-down in\ufb02uence of the DBN. Note that if we know the\ngaze variable u and the \ufb01rst layer of hidden variables h1, then v is simply de\ufb01ned by a Gaussian\ndistribution, where the mean is given by the average of the top-down in\ufb02uence and bottom-up in-\nformation from x. The conditional distributions over h1 and h2 given v are given by the standard\nDBN inference equations [1]. The conditional posterior over the gaze variables u is given by:\np(u|x, v) = p(x|u, v)p(u)\np(x|v)\n,\nlog p(u|x, v) \u221dlog p(x|u, v) + log p(u) = 1\n2\nX\ni\n(xi(u) \u2212vi)2\n\u03c32\ni\n+ const.\n(9)\nUsing Bayes\u2019 rule, the unnormalized log probability of p(u|x, v) is de\ufb01ned in Eq. 9. We stress that\nthis equation is atypical in that the random variable of interest u actually affects the conditioning\nvariable x (see Eq. 5) We can explore the gaze variables using Hamiltonian Monte Carlo (HMC)\nalgorithm [27, 28]. Intuitively, conditioned on the canonical object v that our model has in \u201cmind\u201d,\nHMC searches over the entire image I to \ufb01nd a region x with a good match to v.\nIf the goal is only to \ufb01nd the MAP estimate of p(u|x, v), then we may want to use second-order\nmethods for optimizing u. This would be equivalent to the Lucas-Kanade framework in computer\nvision, developed for image alignment [29]. However, HMC has the advantage of being a proper\nMCMC sampler that satis\ufb01es detailed balance and \ufb01ts nicely with our probabilistic framework.\nThe HMC algorithm \ufb01rst speci\ufb01es the Hamiltonian over the position variables u and auxiliary\nmomentum variables r: H(u, r) = U(u) + K(r), where the potential function is de\ufb01ned by\nU(u) = 1\n2\nP\ni\n(xi(u)\u2212vi)2\n\u03c32\ni\nand the kinetic energy function is given by K(r) = 1\n2\nP\ni r2\ni . The dy-\nnamics of the system is de\ufb01ned by: \u2202u\n\u2202t = r,\n\u2202r\n\u2202t = \u2212\u2202H\n\u2202u\n(10)\n\u2202H\n\u2202u = (x(u) \u2212v)\n\u03c32\n\u2202x(u)\n\u2202u\n,\n(11)\n\u2202x\n\u2202u =\n\u2202x\n\u2202w({p}, u)\n\u2202w({p}, u)\n\u2202u\n=\nX\ni\n\u2202xi\n\u2202w(pi, u)\n\u2202w(pi, u)\n\u2202u\n.\n(12)\nObserve that Eq. 12 decomposes into sums over single coordinate positions pi = [x y]T. Let us\ndenote p\u2032\ni = w(pi, u) to be the coordinate pi warped by u. For the \ufb01rst term on the RHS of Eq. 12,\n\u2202xi\n\u2202w(pi, u) = \u2207I(p\u2032\ni),\n(dimension 1 by 2 )\n(13)\nwhere \u2207I(p\u2032\ni) denotes the sampling of the gradient images of I at the warped location pi. For the\nsecond term on the RHS of Eq. 12, we note that we can re-write Eq. 4 as:\nh x\u2032\ny\u2032\ni\n=\nh x\n\u2212y\n1\n0\ny\nx\n0\n1\ni\"\na\nb\n\u25b3x\n\u25b3y\n#\n+\nh x\ny\ni\n,\n(14)\n4\ngiving us\n\u2202w(pi, u)\n\u2202u\n=\nh x\n\u2212y\n1\n0\ny\nx\n0\n1\ni\n.\n(15)\nHMC simulates the discretized system by performing leap-frog updates of u and r using Eq. 10.\nAdditional hyperparameters that need to be speci\ufb01ed include the step size \u03f5, number of leap-frog\nsteps, and the mass of the variables (see [28] for details).\n4.1\nApproximate Inference\n(a)\nAverage\nA\nB\n(b)\nFigure 2:\n(a) HMC can easily get\nstuck at local optima. (b) Importance\nof modeling p(u|v, I). Best in color.\nHMC essentially performs gradient descent with momentum,\ntherefore it is prone to getting stuck at local optimums. This\nis especially a problem for our task of \ufb01nding the best trans-\nformation parameters. While the posterior over u should be\nunimodal near the optimum, many local minima exist away\nfrom the global optimum. For example, in Fig. 2(a), the big\nimage I is enclosed by the blue box, and the canonical image\nv is enclosed by the green box. The current setting of u aligns\ntogether the wrong eyes. However, it is hard to move the green\nbox to the left due to the local optima created by the dark in-\ntensities of the eye. Resampling the momentum variable every\niteration in HMC does not help signi\ufb01cantly because we are\nmodeling real-valued images using a Gaussian distribution as\nthe residual, leading to quadratic costs in the difference be-\ntween x(u) and v (see Eq. 9). This makes the energy barriers\nbetween modes extremely high.\nTo alleviate this problem we need to \ufb01nd good initializations\nof u.\nWe use a Convolutional Network (ConvNet) to per-\nform ef\ufb01cient approximate inference, resulting in good initial\nguesses. Speci\ufb01cally, given v, u and I, we predict the change\nin u that will lead to the maximum log p(u|x, v). In other\nwords, instead of using the gradient \ufb01eld for updating u, we\nlearn a ConvNet to output a better vector \ufb01eld in the space\nof u. We used a fairly standard ConvNet architecture and the standard stochastic gradient descent\nlearning procedure.\nWe note that standard feedforward face detectors seek to model p(u|I), while completely ignoring\nthe canonical face v. In contrast, here we take v into account as well. The ConvNet is used to initial-\nize u for the HMC algorithm. This is important in a proper generative model because conditioning\non v is appealing when multiple faces are present in the scene. Fig. 2(b) is a hypothesized Euclidean\nspace of v, where the black manifold represents canonical faces and the blue manifold represents\ncropped faces x(u). The blue manifold has a low intrinsic dimensionality of 4, spanned by u. At A\nand B, the blue comes close to black manifold. This means that there are at least two modes in the\nposterior over u. By conditioning on v, we can narrow the posterior to a single mode, depending on\nwhom we want to focus our attention. We demonstrate this exact capability in Sec. 6.3.\nFig. 3 demonstrates the iterative process of how approximate inference works in our model. Specif-\nically, based on u, the ConvNet takes a window patch around x(u) (72\u00d772) and v (24\u00d724) as input,\nand predicts the output [\u25b3x, \u25b3y, \u25b3\u03b8, \u25b3s]. In step 2, u is updated accordingly, followed by step 3\nof alternating Gibbs updates of v and h, as discussed in Sec. 4. The process is repeated. For the\ndetails of the ConvNet see the supplementary materials.\n5\nLearning\nWhile inference in our framework localizes objects of interest and is akin to object detection, it is not\nthe main objective. Our motivation is not to compete with state-of-the-art object detectors but rather\npropose a probabilistic generative framework capable of generative modeling of objects which are\nat unknown locations in big images. This is because labels are expensive to obtain and are often not\navailable for images in an unconstrained environment.\nTo learn generatively without labels we propose a simple Monte Carlo based Expectation-\nMaximization algorithm. This algorithm is an unbiased estimator of the maximum likelihood objec-\n5\nConvNet\nConvNet\nStep 1\nStep 2\nStep 3\nStep 4\n1 Gibbs step\nFigure 3: Inference process: u in step 1 is randomly initialized. The average v and the extracted x(u) form\nthe input to a ConvNet for approximate inference, giving a new u. The new u is used to sample p(v|I, u, h).\nIn step 3, one step of Gibbs sampling of the GDBN is performed. Step 4 repeats the approximate inference\nusing the updated v and x(u).\nInference steps\n1\n2\n3\n4\n5\n6\nHMC\nV\nX\nFigure 4: Example of an inference step. v is 24\u00d724, x is 72\u00d772. Approximate inference quickly \ufb01nds a\ngood initialization for u, while HMC provides further adjustments. Intermediate inference steps on the right\nare subsampled from 10 actual iterations.\ntive. During the E-step, we use the Gibbs sampling algorithm developed in Sec. 4 to draw samples\nfrom the posterior over the latent gaze variables u, the canonical variables v, and the hidden vari-\nables h1, h2 of a Gaussian DBN model. During the M-step, we can update the weights of the\nGaussian DBN by using the posterior samples as its training data. In addition, we can update the\nparameters of the ConvNet that performs approximate inference. Due to the fact that the \ufb01rst E-step\nrequires a good inference algorithm, we need to pretrain the ConvNet using labeled gaze data as\npart of a bootstrap process. Obtaining training data for this initial phase is not a problem as we can\njitter/rotate/scale to create data. In Sec. 6.2, we demonstrate the ability to learn a good generative\nmodel of face images from the CMU Multi-PIE dataset.\n6\nExperiments\nWe used two face datasets in our experiments. The \ufb01rst dataset is a frontal face dataset, called\nthe Caltech Faces from 1999, collected by Markus Weber. In this dataset, there are 450 faces of 27\nunique individuals under different lighting conditions, expressions, and backgrounds. We downsam-\npled the images from their native 896 by 692 by a factor of 2. The dataset also contains manually\nlabeled eyes and mouth coordinates, which will serve as the gaze labels. We also used the CMU\nMulti-PIE dataset [30], which contains 337 subjects, captured under 15 viewpoints and 19 illumi-\nnation conditions in four recording sessions for a total of more than 750,000 images. We demon-\nstrate our model\u2019s ability to perform approximate inference, to learn without labels, and to perform\nidentity-based attention given an image with two people.\n6.1\nApproximate inference\nWe \ufb01rst investigate the critical inference algorithm of p(u|v, I) on the Caltech Faces dataset. We\nrun 4 steps of approximate inference detailed in Sec. 4.1 and diagrammed in Fig. 3, followed by\nthree iterations of 20 leap-frog steps of HMC. Since we do not initially know the correct v, we\ninitialize v to be the average face across all subjects.\nFig. 4 shows the image of v and x during inference for a test subject. The initial gaze box is colored\nyellow on the left. Subsequent gaze updates progress from yellow to blue. Once ConvNet-based\napproximate inference gives a good initialization, starting from step 5, \ufb01ve iterations of 20 leap-frog\nsteps of HMC are used to sample from the the posterior.\nFig. 5 shows the quantitative results of Intersection over Union (IOU) of the ground truth face box\nand the inferred face box. The results show that inference is very robust to initialization and requires\n6\n0\n20\n40\n60\n80\n100\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n1.1\nAccuracy of Approximate Inference\nInitial Pixel Offset\nAccuracy\n \n \nTrials with IOU > 0.5\nAverage IOU\n(a)\n0\n5\n10\n15\n0\n0.2\n0.4\n0.6\n0.8\n1\nAccuracy of Approximate Inference\n# of Inference Steps\nAccuracy\n \n \nTrials with IOU > 0.5\nAverage IOU\n(b)\n0\n20\n40\n60\n80\n100\n\u22120.2\n\u22120.1\n0\n0.1\n0.2\n0.3\nAccuracy Improvements\nInitial Pixel Offset\nAccuracy\n \n \nAverage IOU Improvements\n(c)\nFigure 5: (a) Accuracy as a function of gaze initialization (pixel offset). Blue curve is the percentage success\nof at least 50% IOU. Red curve is the average IOU. (b) Accuracy as a function of the number of approximate\ninference steps when initializing 50 pixels away. (c) Accuracy improvements of HMC as a function of gaze\ninitializations.\n(a) DBN trained on Caltech\n(b) DBN updated with Multi-PIE\nFigure 6: Left: Samples from a 2-layer DBN trained on Caltech. Right: samples from an updated DBN after\ntraining on CMU Multi-PIE without labels. Samples highlighted in green are similar to faces from CMU.\nonly a few steps of approximate inference to converge. HMC clearly improves model performance,\nresulting in an IOU increase of about 5% for localization. This is impressive given that none of\nthe test subjects were part of the training and the background is different from backgrounds in the\ntraining set.\nOur method OpenCV\nNCC\ntemplate\nIOU > 0.5\n97%\n97%\n93%\n78%\n# evaluations\nO(c)\nO(whs) O(whs) O(whs)\nTable 1: Face localization accuracy. w: image width;\nh: image height; s: image scales; c: number of inference\nsteps used.\nWe also compared our inference algorithm to\nthe template matching in the task of face de-\ntection. We took the \ufb01rst 5 subjects as test\nsubjects and the rest as training. We can lo-\ncalize with 97% accuracy (IOU > 0.5) us-\ning our inference algorithm4. In comparison,\na near state-of-the-art face detection system\nfrom OpenCV 2.4.9 obtains the same 97% ac-\ncuracy. It uses Haar Cascades, which is a form of AdaBoost5. Normalized Cross Correlation [31]\nobtained 93% accuracy, while Euclidean distance template matching achieved an accuracy of only\n78%. However, note that our algorithm looks at a constant number of windows while the other\nbaselines are all based on scanning windows.\n6.2\nGenerative learning without labels\nnats\nNo CMU training CMU w/o labels CMU w/ labels\nCaltech Train\n617\u00b10.4\n627\u00b10.5\n569\u00b10.6\nCaltech Valid\n512\u00b11.1\n503\u00b11.8\n494\u00b11.7\nCMU Train\n96\u00b10.8\n499\u00b10.1\n594\u00b10.5\nCMU Valid\n85\u00b10.5\n387\u00b10.3\n503\u00b10.7\nlog \u02c6Z\n454.6\n687.8\n694.2\nTable 2: Variational lower-bound estimates on the log-density of the\nGaussian DBNs (higher is better).\nThe main advantage of our\nmodel is that it can learn on\nlarge images of faces without lo-\ncalization label information (no\nmanual cropping required). To\ndemonstrate, we use both the\nCaltech and the CMU faces\ndataset. For the CMU faces, a\nsubset of 2526 frontal faces with\nground truth labels are used. We split the Caltech dataset into a training and a validation set. For\nthe CMU faces, we \ufb01rst took 10% of the images as training cases for the ConvNet for approximate\ninference. This is needed due to the completely different backgrounds of the Caltech and CMU\ndatasets. The remaining 90% of the CMU faces are split into a training and validation set. We \ufb01rst\ntrained a GDBN with 1024 h1 and 256 h2 hidden units on the Caltech training set. We also trained\n4u is randomly initialized at \u00b1 30 pixels, scale range from 0.5 to 1.5.\n5OpenCV detection uses pretrained model from haarcascade_frontalface_default.xml, scaleFactor=1.1,\nminNeighbors=3 and minSize=30.\n7\nFigure 7: Left: Conditioned on different v will result in a different \u25b3u. Note that the initial u is exactly the\nsame for two trials. Right: Additional examples. The only difference between the top and bottom panels is the\nconditioned v. Best viewed in color.\na ConvNet for approximate inference using the Caltech training set and 10% of the CMU training\nimages.\nTable 2 shows the estimates of the variational lower-bounds on the average log-density (higher is\nbetter) that the GDBN models assign to the ground-truth cropped face images from the training/test\nsets under different scenarios. In the left column, the model is only trained on Caltech faces. Thus it\ngives very low probabilities to the CMU faces. Indeed, GDBNs achieve a variational lower-bound of\nonly 85 nats per test image. In the middle column, we use our approximate inference to estimate the\nlocation of the CMU training faces and further trained the GDBN on the newly localized faces. This\ngives a dramatic increase of the model performance on the CMU Validation set6, achieving a lower-\nbound of 387 nats per test image. The right column gives the best possible results if we can train\nwith the CMU manual localization labels. In this case, GDBNs achieve a lower-bound of 503 nats.\nWe used Annealed Importance Sampling (AIS) to estimate the partition function for the top-layer\nRBM. Details on estimating the variational lower bound are in the supplementary materials.\nFig. 6(a) further shows samples drawn from the Caltech trained DBN, whereas Fig. 6(b) shows\nsamples after training with the CMU dataset using estimated u. Observe that samples in Fig. 6(b)\nshow a more diverse set of faces. We trained GDBNs using a greedy, layer-wise algorithm of [1].\nFor the top layer we use Fast Persistent Contrastive Divergence [32], which substantially improved\ngenerative performance of GDBNs (see supplementary material for more details).\n6.3\nInference with ambiguity\nOur attentional mechanism can also be useful when multiple objects/faces are present in the scene.\nIndeed, the posterior p(u|x, v) is conditioned on v, which means that where to attend is a func-\ntion of the canonical object v the model has in \u201cmind\u201d (see Fig. 2(b)). To explore this, we \ufb01rst\nsynthetically generate a dataset by concatenating together two faces from the Caltech dataset. We\nthen train approximate inference ConvNet as in Sec. 4.1 and test on the held-out subjects. Indeed,\nas predicted, Fig. 7 shows that depending on which canonical image is conditioned, the same exact\ngaze initialization leads to two very different gaze shifts. Note that this phenomenon is observed\nacross different scales and location of the initial gaze. For example, in Fig. 7, right-bottom panel,\nthe initialized yellow box is mostly on the female\u2019s face to the left, but because the conditioned\ncanonical face v is that of the right male, attention is shifted to the right.\n7\nConclusion\nIn this paper we have proposed a probabilistic graphical model framework for learning generative\nmodels using attention. Experiments on face modeling have shown that ConvNet based approximate\ninference combined with HMC sampling is suf\ufb01cient to explore the complicated posterior distribu-\ntion. More importantly, we can generatively learn objects of interest from novel big images. Future\nwork will include experimenting with faces as well as other objects in a large scene. Currently the\nConvNet approximate inference is trained in a supervised manner, but reinforcement learning could\nalso be used instead.\nAcknowledgements\nThe authors gratefully acknowledge the support and generosity from Samsung, Google, and ONR\ngrant N00014-14-1-0232.\n6We note that we still made use of labels coming from the 10% of CMU Multi-PIE training set in order to\npretrain our ConvNet. \"w/o labels\" here means that no labels for the CMU Train/Valid images are given.\n8\nReferences\n[1] G. E. Hinton, S. Osindero, and Y. W. Teh. A fast learning algorithm for deep belief nets. Neural Compu-\ntation, 18(7):1527\u20131554, 2006.\n[2] R. Salakhutdinov and G. Hinton. Deep Boltzmann machines. In AISTATS, 2009.\n[3] Geoffrey E. Hinton, Peter Dayan, and Michael Revow. Modeling the manifolds of images of handwritten\ndigits. IEEE Transactions on Neural Networks, 8(1):65\u201374, 1997.\n[4] Daniel Zoran and Yair Weiss. From learning models of natural image patches to whole image restoration.\nIn ICCV. IEEE, 2011.\n[5] Yoshua Bengio, Li Yao, Guillaume Alain, and Pascal Vincent. Generalized denoising auto-encoders as\ngenerative models. In Advances in Neural Information Processing Systems 26, 2013.\n[6] H. Lee, R. Grosse, R. Ranganath, and A. Y. Ng. Convolutional deep belief networks for scalable unsu-\npervised learning of hierarchical representations. In ICML, pages 609\u2013616, 2009.\n[7] Marc\u2019Aurelio Ranzato, Joshua Susskind, Volodymyr Mnih, and Geoffrey Hinton. On Deep Generative\nModels with Applications to Recognition. In CVPR, 2011.\n[8] Yichuan Tang, Ruslan Salakhutdinov, and Geoffrey E. Hinton. Deep mixtures of factor analysers. In\nICML. icml.cc / Omnipress, 2012.\n[9] M. I. Posner and C. D. Gilbert. Attention and primary visual cortex. Proc. of the National Academy of\nSciences, 96(6), March 1999.\n[10] E. A. Buffalo, P. Fries, R. Landman, H. Liang, and R. Desimone. A backward progression of attentional\neffects in the ventral stream. PNAS, 107(1):361\u2013365, Jan. 2010.\n[11] N Kanwisher and E Wojciulik. Visual attention: Insights from brain imaging. Nature Reviews Neuro-\nscience, 1:91\u2013100, 2000.\n[12] C. H. Anderson and D. C. Van Essen. Shifter circuits: A computational strategy for dynamic aspects of\nvisual processing. National Academy of Sciences, 84:6297\u20136301, 1987.\n[13] B. A. Olshausen, C. H. Anderson, and D. C. Van Essen. A neurobiological model of visual attention and\ninvariant pattern recognition based on dynamic routing of information. The Journal of neuroscience : the\nof\ufb01cial journal of the Society for Neuroscience, 13(11):4700\u20134719, 1993.\n[14] Laurenz Wiskott. How does our visual system achieve shift and size invariance?, 2004.\n[15] S. Chikkerur, T. Serre, C. Tan, and T. Poggio. What and where: a Bayesian inference theory of attention.\nVision Research, 50(22):2233\u20132247, October 2010.\n[16] J. K. Tsotsos, S. M. Culhane, W. Y. K. Wai, Y. H. Lai, N. Davis, and F. Nu\ufb02o. Modeling visual-attention\nvia selective tuning. Arti\ufb01cial Intelligence, 78(1-2):507\u2013545, October 1995.\n[17] Hugo Larochelle and Geoffrey E. Hinton. Learning to combine foveal glimpses with a third-order boltz-\nmann machine. In NIPS, pages 1243\u20131251. Curran Associates, Inc., 2010.\n[18] D. P. Reichert, P. Seri\u00e8s, and A. J. Storkey. A hierarchical generative model of recurrent object-based\nattention in the visual cortex. In ICANN (1), volume 6791, pages 18\u201325. Springer, 2011.\n[19] B. Alexe, N. Heess, Y. W. Teh, and V. Ferrari. Searching for objects driven by context. In NIPS 2012,\nDecember 2012.\n[20] Marc\u2019Aurelio Ranzato. On learning where to look. arXiv, arXiv:1405.5488, 2014.\n[21] M. Denil, L. Bazzani, H. Larochelle, and N. de Freitas. Learning where to attend with deep architectures\nfor image tracking. Neural Computation, 28:2151\u20132184, 2012.\n[22] G. E. Hinton and R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science,\n313:504\u2013507, 2006.\n[23] A. Krizhevsky. Learning multiple layers of features from tiny images. Master\u2019s thesis, University of\nToronto, Toronto, Ontario, Canada, 2009.\n[24] Graham W. Taylor, Rob Fergus, Yann LeCun, and Christoph Bregler. Convolutional learning of spatio-\ntemporal features. In ECCV 2010. Springer, 2010.\n[25] A. Mohamed, G. Dahl, and G. Hinton. Acoustic modeling using deep belief networks. IEEE Transactions\non Audio, Speech, and Language Processing, 2011.\n[26] Richard Szeliski. Computer Vision - Algorithms and Applications. Texts in Computer Science. Springer,\n2011.\n[27] S. Duane, A. D. Kennedy, B. J Pendleton, and D. Roweth. Hybrid Monte Carlo. Physics Letters B,\n195(2):216\u2013222, 1987.\n[28] R. M. Neal. MCMC using Hamiltonian dynamics. in Handbook of Markov Chain Monte Carlo (eds S.\nBrooks, A. Gelman, G. Jones, XL Meng). Chapman and Hall/CRC Press, 2010.\n[29] Simon Baker and Iain Matthews. Lucas-kanade 20 years on: A unifying framework. International Journal\nof Computer Vision, 56:221\u2013255, 2002.\n[30] Ralph Gross, Iain Matthews, Jeffrey F. Cohn, Takeo Kanade, and Simon Baker. Multi-pie. Image Vision\nComput., 28(5):807\u2013813, 2010.\n[31] J. P. Lewis. Fast normalized cross-correlation, 1995.\n[32] T. Tieleman and G. E. Hinton. Using fast weights to improve persistent contrastive divergence. In Pro-\nceedings of the 26th Annual International Conference on Machine Learning, ICML 2009, volume 382,\npage 130. ACM, 2009.\n9\n72x72\n24x24\n66x66\nconv\npool\n22x22\nfully \nconnected\nconv\n22x22\n1024\nmultiply\nFigure 8: A visual diagram of the convolutional net used for approximate inference.\nAPPENDIX\nConvolutional Neural Network\nThe training of ConvNet for approximate inference is standard and did not involve any special \u2019tricks\u2019. We\nused SGD with minibatch size of 128 samples. We used a standard ConvNet architecture with convolution C\nlayers followed by max-pooling S layers. The ConvNet takes as input x and v to predict change in u such that\nto maximize log p(u|x, v). In order to better predict change of u, x as well as a bigger border around x are\nused as the input to the ConvNet. Therefore, x has resolution 72\u00d772 and v has resolution of 24\u00d724.\nTwo handle two different inputs with different resolutions, two different \u201cstreams\" are used in this ConvNet\narchitecture. One stream will process x and another one for v. These two streams will be combined multi-\nplicatively after subsampling the x stream by a factor of 3. The rest of the ConvNet is same as the standard\nclassi\ufb01cation ConvNets, except that we use mean squared error as our cost function. See Figure 8 for a visual\ndiagram of what the convolutional neural network architecture used.\nlayer\ntype\nlatent variables\n\ufb01lter size\n# weights\n0\ninput x\nmaps:3 72x72\n-\n-\n1\ninput v\nmaps:3 24x24\n-\n-\n2\nConv of layer 0\nmaps:16 66x66\n7x7\n2352\n3\nPooling\nmaps:16 22x22\n3x3\n-\n4\nConv of layer 1\nmaps:16 22x22\n5x5\n1200\n5\nCombine layers 3,4\nmaps:16 22x22\n-\n-\n6\nFully connected\n1024\n-\n7.9M\n7\nFully connected\n4\n-\n4K\nTable 3: Model architectures of the convolutional neural network used during approximate inference.\nTable 3 details the exact model architecture used. In layer 5, the two streams have the same number of hidden\nmaps and hidden topography. We combine these two multiplicatively by multiplying their activation element-\nwise. This creates a third-order \ufb02avor and is more powerful for the task of determining where to shift attention\nto next.\n10\nGaussian Deep Belief Network\nThe training of the Gaussian Deep Belief Network is performed in a standard greedy layerwise fashion. The\n\ufb01rst layer Gaussian Restricted Boltzmann Machine is trained with the Contrastive Divergence algorithm where\nthe standard deviation of each visible unit is learned as well. After training the \ufb01rst layer, we use Eq. 3 to obtain\n\ufb01rst hidden layer binary probabilities. We then train a 2nd binary-binary Restricted Boltzmann Machine using\nthe fast persistent contrastive divergence learning algorithm. This greedy training leads us to the Gaussian Deep\nBelief Network. No \ufb01netuning of the entire network is performed.\nQuantitative evaluation for Gaussian Deep Belief Network\nFor quantitative evaluation, we approximate the standard variational lower bound on the log likelihood of the\nGaussian Deep Belief Network. The model is a directed model:\np(v, h1, h2) = p(v|h1)p(h1, h2)\n(16)\nFor any approximating posterior distribution q(h1|v), the GDBN\u2019s log-likelihood has this lower variational\nbound:\nlog\nX\nh1\np(v, h1) \u2265\nX\nh1\nq(h1|v)[log p(v|h1) + log p\u2217(h1)] \u2212log Z + H(q(h1|v))\n(17)\nThe entropy H(q(h1|v)) can be calculated since we made the factorial assumption on q(h1|v).\nlog p(v|h1) = \u2212\nX\nlog \u03c3i \u2212D\n2 log 2\u03c0 \u22121\n2\nD\nX\ni\n(x \u2212\u00b5)2\n\u03c32\ni\n(18)\nlog p\u2217(h1) = bTh1 + log\nX\nj\nexp{h1Wj + cj}\n(19)\nIn order to calculate the expectation of the approximating posteriors, we use Monte Carlo sampling.\nX\nh1\nq(h1|v) log p\u2217(v, h1) \u22481\nM\nM\nX\nm=1\nlog p\u2217(v, h1(m))\n(20)\n= \u2212\nX\nlog \u03c3i \u2212D\n2 log 2\u03c0 \u22121\n2\nD\nX\ni\n(x \u2212\u00b5)2\n\u03c32\ni\n(21)\n+ bTh1 + log\nX\nj\nexp{h1Wj + cj}\n(22)\nwhere h1(m) is the m-th sample from the posterior q(h1|v).\nIn order to calculate the partition function of the top-most layer of the GDBN, we use Annealed Importance\nSampling (AIS). We used 100 chains with 50,000 intermediate distributions to estimate the partition function\nof the binary-binary RBM which forms the top layer of the GDBN. Even though AIS is an unbiased estimator\nof the partition function, it is prone to under-estimating it due to bad mixing of the chains. This causes the log\nprobability to be over-estimated. Therefore the variational lower bounds reported in our paper are not strictly\nguaranteed to be lower bounds and are subject to errors. However, we believe that the margin of error is unlikely\nto be high enough to affect our conclusions.\nAdditional Results\nWe present some more examples of inference process of our framework.\nBelow, we show some success cases and a failure case for inference on the CMU Multi-PIE dataset. The initial\ngaze variables of u are highlighted in yellow and later iterations are highlighted with color gradually changing\nto blue.\n11\nInference steps\n1\n2\n3\n4\n5\n6\nHMC\nV\nX\nInference steps\n1\n2\n3\n4\n5\n6\nHMC\nV\nX\nFigure 9: Example of an approximate inference steps. v is 24\u00d724, x is 72\u00d772. Approximate inference quickly\n\ufb01nds a good initialization for u, while HMC makes small adjustments.\n(a) Ex. 1\n(b) Ex. 2\n(c) Ex. 3\n(d) Ex. 4\nFigure 10: E-step for learning on CMU Multi-PIE. (a),(b),(c) are successful. (d) is a failure case.\n12\n",
        "sentence": " This has motivated recent work on visual attention-based models [2, 3, 4], which reduce the number of parameters and computational operations by selecting informative regions of an image to focus on. Such models have been applied successfully in image classification [12, 4, 3, 2], object tracking [13, 3], machine translation [6], caption generation [5], and image generation [14, 15].",
        "context": "data for generative models, we propose a deep-learning based generative frame-\nwork using attention. The attentional mechanism propagates signals from the\nregion of interest in a scene to an aligned canonical representation for genera-\nAbstract\nAttention has long been proposed by psychologists to be important for ef\ufb01ciently\ndealing with the massive amounts of sensory stimulus in the neocortex. Inspired\nby the attention models in visual neuroscience and the need for object-centered\nLearning Generative Models with Visual Attention\nYichuan Tang, Nitish Srivastava, Ruslan Salakhutdinov\nDepartment of Computer Science\nUniversity of Toronto\nToronto, Ontario, Canada\n{tang,nitish,rsalakhu}@cs.toronto.edu\nAbstract"
    },
    {
        "title": "Show, attend, and tell: neural image caption generation with visual attention",
        "author": [
            "K. Xu",
            "J. Ba",
            "R. Kiros",
            "K. Cho",
            "A. Courville",
            "R. Salakhutdinov",
            "R.S. Zemel",
            "Y. Bengio"
        ],
        "venue": "International Conference on Machine Learning,",
        "citeRegEx": "5",
        "shortCiteRegEx": null,
        "year": 2015,
        "abstract": "",
        "full_text": "",
        "sentence": " One such approach was recently used by [5] to automatically generate image captions and highlight which image region was relevant to each word in the caption. [5]) obtain features from a weighted average of all image locations, where locations are weighted based on a model\u2019s saliency map. Unfortunately, this comes at a cost: while soft attention models can be trained with standard backpropagation [6, 5], this does not work for hard attention models, whose glimpse selections are typically discrete. Such models have been applied successfully in image classification [12, 4, 3, 2], object tracking [13, 3], machine translation [6], caption generation [5], and image generation [14, 15]. Attention has been shown both to improve computational efficiency [2] and to yield insight into the network\u2019s behavior [5]. We also applied the WS-RAM method to learn a stochastic attention model similar to [5] for generating image captions. Similarly to [5], we first ran a convolutional network, and the attention network then determined which part of the convolutional net representation to attend to. The attention network predicted both which layer to attend to and a location within the layer, in contrast with [5], where the scale was held fixed.",
        "context": null
    },
    {
        "title": "Neural machine translation by jointly learning to align and translate",
        "author": [
            "D. Bahdanau",
            "K. Cho",
            "Y. Bengio"
        ],
        "venue": "International Conference on Learning Representations,",
        "citeRegEx": "6",
        "shortCiteRegEx": null,
        "year": 2015,
        "abstract": "Neural machine translation is a recently proposed approach to machine\ntranslation. Unlike the traditional statistical machine translation, the neural\nmachine translation aims at building a single neural network that can be\njointly tuned to maximize the translation performance. The models proposed\nrecently for neural machine translation often belong to a family of\nencoder-decoders and consists of an encoder that encodes a source sentence into\na fixed-length vector from which a decoder generates a translation. In this\npaper, we conjecture that the use of a fixed-length vector is a bottleneck in\nimproving the performance of this basic encoder-decoder architecture, and\npropose to extend this by allowing a model to automatically (soft-)search for\nparts of a source sentence that are relevant to predicting a target word,\nwithout having to form these parts as a hard segment explicitly. With this new\napproach, we achieve a translation performance comparable to the existing\nstate-of-the-art phrase-based system on the task of English-to-French\ntranslation. Furthermore, qualitative analysis reveals that the\n(soft-)alignments found by the model agree well with our intuition.",
        "full_text": "Published as a conference paper at ICLR 2015\nNEURAL MACHINE TRANSLATION\nBY JOINTLY LEARNING TO ALIGN AND TRANSLATE\nDzmitry Bahdanau\nJacobs University Bremen, Germany\nKyungHyun Cho\nYoshua Bengio\u2217\nUniversit\u00b4e de Montr\u00b4eal\nABSTRACT\nNeural machine translation is a recently proposed approach to machine transla-\ntion. Unlike the traditional statistical machine translation, the neural machine\ntranslation aims at building a single neural network that can be jointly tuned to\nmaximize the translation performance. The models proposed recently for neu-\nral machine translation often belong to a family of encoder\u2013decoders and encode\na source sentence into a \ufb01xed-length vector from which a decoder generates a\ntranslation. In this paper, we conjecture that the use of a \ufb01xed-length vector is a\nbottleneck in improving the performance of this basic encoder\u2013decoder architec-\nture, and propose to extend this by allowing a model to automatically (soft-)search\nfor parts of a source sentence that are relevant to predicting a target word, without\nhaving to form these parts as a hard segment explicitly. With this new approach,\nwe achieve a translation performance comparable to the existing state-of-the-art\nphrase-based system on the task of English-to-French translation. Furthermore,\nqualitative analysis reveals that the (soft-)alignments found by the model agree\nwell with our intuition.\n1\nINTRODUCTION\nNeural machine translation is a newly emerging approach to machine translation, recently proposed\nby Kalchbrenner and Blunsom (2013), Sutskever et al. (2014) and Cho et al. (2014b). Unlike the\ntraditional phrase-based translation system (see, e.g., Koehn et al., 2003) which consists of many\nsmall sub-components that are tuned separately, neural machine translation attempts to build and\ntrain a single, large neural network that reads a sentence and outputs a correct translation.\nMost of the proposed neural machine translation models belong to a family of encoder\u2013\ndecoders (Sutskever et al., 2014; Cho et al., 2014a), with an encoder and a decoder for each lan-\nguage, or involve a language-speci\ufb01c encoder applied to each sentence whose outputs are then com-\npared (Hermann and Blunsom, 2014). An encoder neural network reads and encodes a source sen-\ntence into a \ufb01xed-length vector. A decoder then outputs a translation from the encoded vector. The\nwhole encoder\u2013decoder system, which consists of the encoder and the decoder for a language pair,\nis jointly trained to maximize the probability of a correct translation given a source sentence.\nA potential issue with this encoder\u2013decoder approach is that a neural network needs to be able to\ncompress all the necessary information of a source sentence into a \ufb01xed-length vector. This may\nmake it dif\ufb01cult for the neural network to cope with long sentences, especially those that are longer\nthan the sentences in the training corpus. Cho et al. (2014b) showed that indeed the performance of\na basic encoder\u2013decoder deteriorates rapidly as the length of an input sentence increases.\nIn order to address this issue, we introduce an extension to the encoder\u2013decoder model which learns\nto align and translate jointly. Each time the proposed model generates a word in a translation, it\n(soft-)searches for a set of positions in a source sentence where the most relevant information is\nconcentrated. The model then predicts a target word based on the context vectors associated with\nthese source positions and all the previous generated target words.\n\u2217CIFAR Senior Fellow\n1\narXiv:1409.0473v7  [cs.CL]  19 May 2016\nPublished as a conference paper at ICLR 2015\nThe most important distinguishing feature of this approach from the basic encoder\u2013decoder is that\nit does not attempt to encode a whole input sentence into a single \ufb01xed-length vector. Instead, it en-\ncodes the input sentence into a sequence of vectors and chooses a subset of these vectors adaptively\nwhile decoding the translation. This frees a neural translation model from having to squash all the\ninformation of a source sentence, regardless of its length, into a \ufb01xed-length vector. We show this\nallows a model to cope better with long sentences.\nIn this paper, we show that the proposed approach of jointly learning to align and translate achieves\nsigni\ufb01cantly improved translation performance over the basic encoder\u2013decoder approach. The im-\nprovement is more apparent with longer sentences, but can be observed with sentences of any\nlength. On the task of English-to-French translation, the proposed approach achieves, with a single\nmodel, a translation performance comparable, or close, to the conventional phrase-based system.\nFurthermore, qualitative analysis reveals that the proposed model \ufb01nds a linguistically plausible\n(soft-)alignment between a source sentence and the corresponding target sentence.\n2\nBACKGROUND: NEURAL MACHINE TRANSLATION\nFrom a probabilistic perspective, translation is equivalent to \ufb01nding a target sentence y that max-\nimizes the conditional probability of y given a source sentence x, i.e., arg maxy p(y | x). In\nneural machine translation, we \ufb01t a parameterized model to maximize the conditional probability\nof sentence pairs using a parallel training corpus. Once the conditional distribution is learned by a\ntranslation model, given a source sentence a corresponding translation can be generated by searching\nfor the sentence that maximizes the conditional probability.\nRecently, a number of papers have proposed the use of neural networks to directly learn this condi-\ntional distribution (see, e.g., Kalchbrenner and Blunsom, 2013; Cho et al., 2014a; Sutskever et al.,\n2014; Cho et al., 2014b; Forcada and \u02dcNeco, 1997). This neural machine translation approach typ-\nically consists of two components, the \ufb01rst of which encodes a source sentence x and the second\ndecodes to a target sentence y. For instance, two recurrent neural networks (RNN) were used by\n(Cho et al., 2014a) and (Sutskever et al., 2014) to encode a variable-length source sentence into a\n\ufb01xed-length vector and to decode the vector into a variable-length target sentence.\nDespite being a quite new approach, neural machine translation has already shown promising results.\nSutskever et al. (2014) reported that the neural machine translation based on RNNs with long short-\nterm memory (LSTM) units achieves close to the state-of-the-art performance of the conventional\nphrase-based machine translation system on an English-to-French translation task.1 Adding neural\ncomponents to existing translation systems, for instance, to score the phrase pairs in the phrase\ntable (Cho et al., 2014a) or to re-rank candidate translations (Sutskever et al., 2014), has allowed to\nsurpass the previous state-of-the-art performance level.\n2.1\nRNN ENCODER\u2013DECODER\nHere, we describe brie\ufb02y the underlying framework, called RNN Encoder\u2013Decoder, proposed by\nCho et al. (2014a) and Sutskever et al. (2014) upon which we build a novel architecture that learns\nto align and translate simultaneously.\nIn the Encoder\u2013Decoder framework, an encoder reads the input sentence, a sequence of vectors\nx = (x1, \u00b7 \u00b7 \u00b7 , xTx), into a vector c.2 The most common approach is to use an RNN such that\nht = f (xt, ht\u22121)\n(1)\nand\nc = q ({h1, \u00b7 \u00b7 \u00b7 , hTx}) ,\nwhere ht \u2208Rn is a hidden state at time t, and c is a vector generated from the sequence of the\nhidden states. f and q are some nonlinear functions. Sutskever et al. (2014) used an LSTM as f and\nq ({h1, \u00b7 \u00b7 \u00b7 , hT }) = hT , for instance.\n1 We mean by the state-of-the-art performance, the performance of the conventional phrase-based system\nwithout using any neural network-based component.\n2 Although most of the previous works (see, e.g., Cho et al., 2014a; Sutskever et al., 2014; Kalchbrenner and\nBlunsom, 2013) used to encode a variable-length input sentence into a \ufb01xed-length vector, it is not necessary,\nand even it may be bene\ufb01cial to have a variable-length vector, as we will show later.\n2\nPublished as a conference paper at ICLR 2015\nThe decoder is often trained to predict the next word yt\u2032 given the context vector c and all the\npreviously predicted words {y1, \u00b7 \u00b7 \u00b7 , yt\u2032\u22121}. In other words, the decoder de\ufb01nes a probability over\nthe translation y by decomposing the joint probability into the ordered conditionals:\np(y) =\nT\nY\nt=1\np(yt | {y1, \u00b7 \u00b7 \u00b7 , yt\u22121} , c),\n(2)\nwhere y =\n\u0000y1, \u00b7 \u00b7 \u00b7 , yTy\n\u0001\n. With an RNN, each conditional probability is modeled as\np(yt | {y1, \u00b7 \u00b7 \u00b7 , yt\u22121} , c) = g(yt\u22121, st, c),\n(3)\nwhere g is a nonlinear, potentially multi-layered, function that outputs the probability of yt, and st is\nthe hidden state of the RNN. It should be noted that other architectures such as a hybrid of an RNN\nand a de-convolutional neural network can be used (Kalchbrenner and Blunsom, 2013).\n3\nLEARNING TO ALIGN AND TRANSLATE\nIn this section, we propose a novel architecture for neural machine translation. The new architecture\nconsists of a bidirectional RNN as an encoder (Sec. 3.2) and a decoder that emulates searching\nthrough a source sentence during decoding a translation (Sec. 3.1).\n3.1\nDECODER: GENERAL DESCRIPTION\nx1\nx2\nx3\nxT\n+\n\u03b1t,1\n\u03b1t,2\n\u03b1t,3\n\u03b1t,T\nyt-1\nyt\nh1\nh2\nh3\nhT\nh1\nh2\nh3\nhT\ns t-1\ns t\nFigure 1: The graphical illus-\ntration of the proposed model\ntrying to generate the t-th tar-\nget word yt given a source\nsentence (x1, x2, . . . , xT ).\nIn a new model architecture, we de\ufb01ne each conditional probability\nin Eq. (2) as:\np(yi|y1, . . . , yi\u22121, x) = g(yi\u22121, si, ci),\n(4)\nwhere si is an RNN hidden state for time i, computed by\nsi = f(si\u22121, yi\u22121, ci).\nIt should be noted that unlike the existing encoder\u2013decoder ap-\nproach (see Eq. (2)), here the probability is conditioned on a distinct\ncontext vector ci for each target word yi.\nThe context vector ci depends on a sequence of annotations\n(h1, \u00b7 \u00b7 \u00b7 , hTx) to which an encoder maps the input sentence. Each\nannotation hi contains information about the whole input sequence\nwith a strong focus on the parts surrounding the i-th word of the\ninput sequence. We explain in detail how the annotations are com-\nputed in the next section.\nThe context vector ci is, then, computed as a weighted sum of these\nannotations hi:\nci =\nTx\nX\nj=1\n\u03b1ijhj.\n(5)\nThe weight \u03b1ij of each annotation hj is computed by\n\u03b1ij =\nexp (eij)\nPTx\nk=1 exp (eik)\n,\n(6)\nwhere\neij = a(si\u22121, hj)\nis an alignment model which scores how well the inputs around position j and the output at position\ni match. The score is based on the RNN hidden state si\u22121 (just before emitting yi, Eq. (4)) and the\nj-th annotation hj of the input sentence.\nWe parametrize the alignment model a as a feedforward neural network which is jointly trained with\nall the other components of the proposed system. Note that unlike in traditional machine translation,\n3\nPublished as a conference paper at ICLR 2015\nthe alignment is not considered to be a latent variable. Instead, the alignment model directly com-\nputes a soft alignment, which allows the gradient of the cost function to be backpropagated through.\nThis gradient can be used to train the alignment model as well as the whole translation model jointly.\nWe can understand the approach of taking a weighted sum of all the annotations as computing an\nexpected annotation, where the expectation is over possible alignments. Let \u03b1ij be a probability that\nthe target word yi is aligned to, or translated from, a source word xj. Then, the i-th context vector\nci is the expected annotation over all the annotations with probabilities \u03b1ij.\nThe probability \u03b1ij, or its associated energy eij, re\ufb02ects the importance of the annotation hj with\nrespect to the previous hidden state si\u22121 in deciding the next state si and generating yi. Intuitively,\nthis implements a mechanism of attention in the decoder. The decoder decides parts of the source\nsentence to pay attention to. By letting the decoder have an attention mechanism, we relieve the\nencoder from the burden of having to encode all information in the source sentence into a \ufb01xed-\nlength vector. With this new approach the information can be spread throughout the sequence of\nannotations, which can be selectively retrieved by the decoder accordingly.\n3.2\nENCODER: BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES\nThe usual RNN, described in Eq. (1), reads an input sequence x in order starting from the \ufb01rst\nsymbol x1 to the last one xTx. However, in the proposed scheme, we would like the annotation\nof each word to summarize not only the preceding words, but also the following words. Hence,\nwe propose to use a bidirectional RNN (BiRNN, Schuster and Paliwal, 1997), which has been\nsuccessfully used recently in speech recognition (see, e.g., Graves et al., 2013).\nA BiRNN consists of forward and backward RNN\u2019s. The forward RNN \u2212\u2192f reads the input sequence\nas it is ordered (from x1 to xTx) and calculates a sequence of forward hidden states (\u2212\u2192h 1, \u00b7 \u00b7 \u00b7 , \u2212\u2192h Tx).\nThe backward RNN \u2190\u2212f reads the sequence in the reverse order (from xTx to x1), resulting in a\nsequence of backward hidden states (\u2190\u2212h 1, \u00b7 \u00b7 \u00b7 , \u2190\u2212h Tx).\nWe obtain an annotation for each word xj by concatenating the forward hidden state \u2212\u2192h j and the\nbackward one \u2190\u2212h j, i.e., hj =\nh\u2212\u2192h \u22a4\nj ; \u2190\u2212h \u22a4\nj\ni\u22a4\n. In this way, the annotation hj contains the summaries\nof both the preceding words and the following words. Due to the tendency of RNNs to better\nrepresent recent inputs, the annotation hj will be focused on the words around xj. This sequence\nof annotations is used by the decoder and the alignment model later to compute the context vector\n(Eqs. (5)\u2013(6)).\nSee Fig. 1 for the graphical illustration of the proposed model.\n4\nEXPERIMENT SETTINGS\nWe evaluate the proposed approach on the task of English-to-French translation. We use the bilin-\ngual, parallel corpora provided by ACL WMT \u201914.3 As a comparison, we also report the perfor-\nmance of an RNN Encoder\u2013Decoder which was proposed recently by Cho et al. (2014a). We use\nthe same training procedures and the same dataset for both models.4\n4.1\nDATASET\nWMT \u201914 contains the following English-French parallel corpora: Europarl (61M words), news\ncommentary (5.5M), UN (421M) and two crawled corpora of 90M and 272.5M words respectively,\ntotaling 850M words. Following the procedure described in Cho et al. (2014a), we reduce the size of\nthe combined corpus to have 348M words using the data selection method by Axelrod et al. (2011).5\nWe do not use any monolingual data other than the mentioned parallel corpora, although it may be\npossible to use a much larger monolingual corpus to pretrain an encoder. We concatenate news-test-\n3 http://www.statmt.org/wmt14/translation-task.html\n4 Implementations are available at https://github.com/lisa-groundhog/GroundHog.\n5 Available online at http://www-lium.univ-lemans.fr/\u02dcschwenk/cslm_joint_paper/.\n4\nPublished as a conference paper at ICLR 2015\n0\n10\n20\n30\n40\n50\n60\nSentence length\n0\n5\n10\n15\n20\n25\n30\nBLEU score\nRNNsearch-50\nRNNsearch-30\nRNNenc-50\nRNNenc-30\nFigure 2: The BLEU scores\nof the generated translations\non the test set with respect\nto the lengths of the sen-\ntences.\nThe results are on\nthe full test set which in-\ncludes sentences having un-\nknown words to the models.\n2012 and news-test-2013 to make a development (validation) set, and evaluate the models on the test\nset (news-test-2014) from WMT \u201914, which consists of 3003 sentences not present in the training\ndata.\nAfter a usual tokenization6, we use a shortlist of 30,000 most frequent words in each language to\ntrain our models. Any word not included in the shortlist is mapped to a special token ([UNK]). We\ndo not apply any other special preprocessing, such as lowercasing or stemming, to the data.\n4.2\nMODELS\nWe train two types of models. The \ufb01rst one is an RNN Encoder\u2013Decoder (RNNencdec, Cho et al.,\n2014a), and the other is the proposed model, to which we refer as RNNsearch. We train each model\ntwice: \ufb01rst with the sentences of length up to 30 words (RNNencdec-30, RNNsearch-30) and then\nwith the sentences of length up to 50 word (RNNencdec-50, RNNsearch-50).\nThe encoder and decoder of the RNNencdec have 1000 hidden units each.7 The encoder of the\nRNNsearch consists of forward and backward recurrent neural networks (RNN) each having 1000\nhidden units. Its decoder has 1000 hidden units. In both cases, we use a multilayer network with a\nsingle maxout (Goodfellow et al., 2013) hidden layer to compute the conditional probability of each\ntarget word (Pascanu et al., 2014).\nWe use a minibatch stochastic gradient descent (SGD) algorithm together with Adadelta (Zeiler,\n2012) to train each model. Each SGD update direction is computed using a minibatch of 80 sen-\ntences. We trained each model for approximately 5 days.\nOnce a model is trained, we use a beam search to \ufb01nd a translation that approximately maximizes the\nconditional probability (see, e.g., Graves, 2012; Boulanger-Lewandowski et al., 2013). Sutskever\net al. (2014) used this approach to generate translations from their neural machine translation model.\nFor more details on the architectures of the models and training procedure used in the experiments,\nsee Appendices A and B.\n5\nRESULTS\n5.1\nQUANTITATIVE RESULTS\nIn Table 1, we list the translation performances measured in BLEU score. It is clear from the table\nthat in all the cases, the proposed RNNsearch outperforms the conventional RNNencdec. More\nimportantly, the performance of the RNNsearch is as high as that of the conventional phrase-based\ntranslation system (Moses), when only the sentences consisting of known words are considered.\nThis is a signi\ufb01cant achievement, considering that Moses uses a separate monolingual corpus (418M\nwords) in addition to the parallel corpora we used to train the RNNsearch and RNNencdec.\n6 We used the tokenization script from the open-source machine translation package, Moses.\n7 In this paper, by a \u2019hidden unit\u2019, we always mean the gated hidden unit (see Appendix A.1.1).\n5\nPublished as a conference paper at ICLR 2015\nThe\nagreement\non\nthe\nEuropean\nEconomic\nArea\nwas\nsigned\nin\nAugust\n1992\n.\n<end>\nL'\naccord\nsur\nla\nzone\n\u00e9conomique\neurop\u00e9enne\na\n\u00e9t\u00e9\nsign\u00e9\nen\nao\u00fbt\n1992\n.\n<end>\nIt\nshould\nbe\nnoted\nthat\nthe\nmarine\nenvironment\nis\nthe\nleast\nknown\nof\nenvironments\n.\n<end>\nIl\nconvient\nde\nnoter\nque\nl'\nenvironnement\nmarin\nest\nle\nmoins\nconnu\nde\nl'\nenvironnement\n.\n<end>\n(a)\n(b)\nDestruction\nof\nthe\nequipment\nmeans\nthat\nSyria\ncan\nno\nlonger\nproduce\nnew\nchemical\nweapons\n.\n<end>\nLa\ndestruction\nde\nl'\n\u00e9quipement\nsignifie\nque\nla\nSyrie\nne\npeut\nplus\nproduire\nde\nnouvelles\narmes\nchimiques\n.\n<end>\n\"\nThis\nwill\nchange\nmy\nfuture\nwith\nmy\nfamily\n,\n\"\nthe\nman\nsaid\n.\n<end>\n\"\nCela\nva\nchanger\nmon\navenir\navec\nma\nfamille\n\"\n,\na\ndit\nl'\nhomme\n.\n<end>\n(c)\n(d)\nFigure 3:\nFour sample alignments found by RNNsearch-50. The x-axis and y-axis of each plot\ncorrespond to the words in the source sentence (English) and the generated translation (French),\nrespectively. Each pixel shows the weight \u03b1ij of the annotation of the j-th source word for the i-th\ntarget word (see Eq. (6)), in grayscale (0: black, 1: white). (a) an arbitrary sentence. (b\u2013d) three\nrandomly selected samples among the sentences without any unknown words and of length between\n10 and 20 words from the test set.\nOne of the motivations behind the proposed approach was the use of a \ufb01xed-length context vector\nin the basic encoder\u2013decoder approach. We conjectured that this limitation may make the basic\nencoder\u2013decoder approach to underperform with long sentences. In Fig. 2, we see that the perfor-\nmance of RNNencdec dramatically drops as the length of the sentences increases. On the other hand,\nboth RNNsearch-30 and RNNsearch-50 are more robust to the length of the sentences. RNNsearch-\n50, especially, shows no performance deterioration even with sentences of length 50 or more. This\nsuperiority of the proposed model over the basic encoder\u2013decoder is further con\ufb01rmed by the fact\nthat the RNNsearch-30 even outperforms RNNencdec-50 (see Table 1).\n6\nPublished as a conference paper at ICLR 2015\nModel\nAll\nNo UNK\u25e6\nRNNencdec-30\n13.93\n24.19\nRNNsearch-30\n21.50\n31.44\nRNNencdec-50\n17.82\n26.71\nRNNsearch-50\n26.75\n34.16\nRNNsearch-50\u22c6\n28.45\n36.15\nMoses\n33.30\n35.63\nTable 1: BLEU scores of the trained models com-\nputed on the test set. The second and third columns\nshow respectively the scores on all the sentences and,\non the sentences without any unknown word in them-\nselves and in the reference translations. Note that\nRNNsearch-50\u22c6was trained much longer until the\nperformance on the development set stopped improv-\ning. (\u25e6) We disallowed the models to generate [UNK]\ntokens when only the sentences having no unknown\nwords were evaluated (last column).\n5.2\nQUALITATIVE ANALYSIS\n5.2.1\nALIGNMENT\nThe proposed approach provides an intuitive way to inspect the (soft-)alignment between the words\nin a generated translation and those in a source sentence. This is done by visualizing the annotation\nweights \u03b1ij from Eq. (6), as in Fig. 3. Each row of a matrix in each plot indicates the weights\nassociated with the annotations. From this we see which positions in the source sentence were\nconsidered more important when generating the target word.\nWe can see from the alignments in Fig. 3 that the alignment of words between English and French\nis largely monotonic. We see strong weights along the diagonal of each matrix. However, we also\nobserve a number of non-trivial, non-monotonic alignments. Adjectives and nouns are typically\nordered differently between French and English, and we see an example in Fig. 3 (a). From this\n\ufb01gure, we see that the model correctly translates a phrase [European Economic Area] into [zone\n\u00b4economique europ\u00b4een]. The RNNsearch was able to correctly align [zone] with [Area], jumping\nover the two words ([European] and [Economic]), and then looked one word back at a time to\ncomplete the whole phrase [zone \u00b4economique europ\u00b4eenne].\nThe strength of the soft-alignment, opposed to a hard-alignment, is evident, for instance, from\nFig. 3 (d). Consider the source phrase [the man] which was translated into [l\u2019 homme]. Any hard\nalignment will map [the] to [l\u2019] and [man] to [homme]. This is not helpful for translation, as one\nmust consider the word following [the] to determine whether it should be translated into [le], [la],\n[les] or [l\u2019]. Our soft-alignment solves this issue naturally by letting the model look at both [the] and\n[man], and in this example, we see that the model was able to correctly translate [the] into [l\u2019]. We\nobserve similar behaviors in all the presented cases in Fig. 3. An additional bene\ufb01t of the soft align-\nment is that it naturally deals with source and target phrases of different lengths, without requiring a\ncounter-intuitive way of mapping some words to or from nowhere ([NULL]) (see, e.g., Chapters 4\nand 5 of Koehn, 2010).\n5.2.2\nLONG SENTENCES\nAs clearly visible from Fig. 2 the proposed model (RNNsearch) is much better than the conventional\nmodel (RNNencdec) at translating long sentences. This is likely due to the fact that the RNNsearch\ndoes not require encoding a long sentence into a \ufb01xed-length vector perfectly, but only accurately\nencoding the parts of the input sentence that surround a particular word.\nAs an example, consider this source sentence from the test set:\nAn admitting privilege is the right of a doctor to admit a patient to a hospital or\na medical centre to carry out a diagnosis or a procedure, based on his status as a\nhealth care worker at a hospital.\nThe RNNencdec-50 translated this sentence into:\nUn privil`ege d\u2019admission est le droit d\u2019un m\u00b4edecin de reconna\u02c6\u0131tre un patient `a\nl\u2019h\u02c6opital ou un centre m\u00b4edical d\u2019un diagnostic ou de prendre un diagnostic en\nfonction de son \u00b4etat de sant\u00b4e.\n7\nPublished as a conference paper at ICLR 2015\nThe RNNencdec-50 correctly translated the source sentence until [a medical center]. However, from\nthere on (underlined), it deviated from the original meaning of the source sentence. For instance, it\nreplaced [based on his status as a health care worker at a hospital] in the source sentence with [en\nfonction de son \u00b4etat de sant\u00b4e] (\u201cbased on his state of health\u201d).\nOn the other hand, the RNNsearch-50 generated the following correct translation, preserving the\nwhole meaning of the input sentence without omitting any details:\nUn privil`ege d\u2019admission est le droit d\u2019un m\u00b4edecin d\u2019admettre un patient `a un\nh\u02c6opital ou un centre m\u00b4edical pour effectuer un diagnostic ou une proc\u00b4edure, selon\nson statut de travailleur des soins de sant\u00b4e `a l\u2019h\u02c6opital.\nLet us consider another sentence from the test set:\nThis kind of experience is part of Disney\u2019s efforts to \u201dextend the lifetime of its\nseries and build new relationships with audiences via digital platforms that are\nbecoming ever more important,\u201d he added.\nThe translation by the RNNencdec-50 is\nCe type d\u2019exp\u00b4erience fait partie des initiatives du Disney pour \u201dprolonger la dur\u00b4ee\nde vie de ses nouvelles et de d\u00b4evelopper des liens avec les lecteurs num\u00b4eriques qui\ndeviennent plus complexes.\nAs with the previous example, the RNNencdec began deviating from the actual meaning of the\nsource sentence after generating approximately 30 words (see the underlined phrase). After that\npoint, the quality of the translation deteriorates, with basic mistakes such as the lack of a closing\nquotation mark.\nAgain, the RNNsearch-50 was able to translate this long sentence correctly:\nCe genre d\u2019exp\u00b4erience fait partie des efforts de Disney pour \u201dprolonger la dur\u00b4ee\nde vie de ses s\u00b4eries et cr\u00b4eer de nouvelles relations avec des publics via des\nplateformes num\u00b4eriques de plus en plus importantes\u201d, a-t-il ajout\u00b4e.\nIn conjunction with the quantitative results presented already, these qualitative observations con-\n\ufb01rm our hypotheses that the RNNsearch architecture enables far more reliable translation of long\nsentences than the standard RNNencdec model.\nIn Appendix C, we provide a few more sample translations of long source sentences generated by\nthe RNNencdec-50, RNNsearch-50 and Google Translate along with the reference translations.\n6\nRELATED WORK\n6.1\nLEARNING TO ALIGN\nA similar approach of aligning an output symbol with an input symbol was proposed recently by\nGraves (2013) in the context of handwriting synthesis. Handwriting synthesis is a task where the\nmodel is asked to generate handwriting of a given sequence of characters. In his work, he used a\nmixture of Gaussian kernels to compute the weights of the annotations, where the location, width\nand mixture coef\ufb01cient of each kernel was predicted from an alignment model. More speci\ufb01cally,\nhis alignment was restricted to predict the location such that the location increases monotonically.\nThe main difference from our approach is that, in (Graves, 2013), the modes of the weights of the\nannotations only move in one direction. In the context of machine translation, this is a severe limi-\ntation, as (long-distance) reordering is often needed to generate a grammatically correct translation\n(for instance, English-to-German).\nOur approach, on the other hand, requires computing the annotation weight of every word in the\nsource sentence for each word in the translation. This drawback is not severe with the task of\ntranslation in which most of input and output sentences are only 15\u201340 words. However, this may\nlimit the applicability of the proposed scheme to other tasks.\n8\nPublished as a conference paper at ICLR 2015\n6.2\nNEURAL NETWORKS FOR MACHINE TRANSLATION\nSince Bengio et al. (2003) introduced a neural probabilistic language model which uses a neural net-\nwork to model the conditional probability of a word given a \ufb01xed number of the preceding words,\nneural networks have widely been used in machine translation. However, the role of neural net-\nworks has been largely limited to simply providing a single feature to an existing statistical machine\ntranslation system or to re-rank a list of candidate translations provided by an existing system.\nFor instance, Schwenk (2012) proposed using a feedforward neural network to compute the score of\na pair of source and target phrases and to use the score as an additional feature in the phrase-based\nstatistical machine translation system. More recently, Kalchbrenner and Blunsom (2013) and Devlin\net al. (2014) reported the successful use of the neural networks as a sub-component of the existing\ntranslation system. Traditionally, a neural network trained as a target-side language model has been\nused to rescore or rerank a list of candidate translations (see, e.g., Schwenk et al., 2006).\nAlthough the above approaches were shown to improve the translation performance over the state-\nof-the-art machine translation systems, we are more interested in a more ambitious objective of\ndesigning a completely new translation system based on neural networks. The neural machine trans-\nlation approach we consider in this paper is therefore a radical departure from these earlier works.\nRather than using a neural network as a part of the existing system, our model works on its own and\ngenerates a translation from a source sentence directly.\n7\nCONCLUSION\nThe conventional approach to neural machine translation, called an encoder\u2013decoder approach, en-\ncodes a whole input sentence into a \ufb01xed-length vector from which a translation will be decoded.\nWe conjectured that the use of a \ufb01xed-length context vector is problematic for translating long sen-\ntences, based on a recent empirical study reported by Cho et al. (2014b) and Pouget-Abadie et al.\n(2014).\nIn this paper, we proposed a novel architecture that addresses this issue. We extended the basic\nencoder\u2013decoder by letting a model (soft-)search for a set of input words, or their annotations com-\nputed by an encoder, when generating each target word. This frees the model from having to encode\na whole source sentence into a \ufb01xed-length vector, and also lets the model focus only on information\nrelevant to the generation of the next target word. This has a major positive impact on the ability\nof the neural machine translation system to yield good results on longer sentences. Unlike with\nthe traditional machine translation systems, all of the pieces of the translation system, including\nthe alignment mechanism, are jointly trained towards a better log-probability of producing correct\ntranslations.\nWe tested the proposed model, called RNNsearch, on the task of English-to-French translation. The\nexperiment revealed that the proposed RNNsearch outperforms the conventional encoder\u2013decoder\nmodel (RNNencdec) signi\ufb01cantly, regardless of the sentence length and that it is much more ro-\nbust to the length of a source sentence. From the qualitative analysis where we investigated the\n(soft-)alignment generated by the RNNsearch, we were able to conclude that the model can cor-\nrectly align each target word with the relevant words, or their annotations, in the source sentence as\nit generated a correct translation.\nPerhaps more importantly, the proposed approach achieved a translation performance comparable to\nthe existing phrase-based statistical machine translation. It is a striking result, considering that the\nproposed architecture, or the whole family of neural machine translation, has only been proposed\nas recently as this year. We believe the architecture proposed here is a promising step toward better\nmachine translation and a better understanding of natural languages in general.\nOne of challenges left for the future is to better handle unknown, or rare words. This will be required\nfor the model to be more widely used and to match the performance of current state-of-the-art\nmachine translation systems in all contexts.\n9\nPublished as a conference paper at ICLR 2015\nACKNOWLEDGMENTS\nThe authors would like to thank the developers of Theano (Bergstra et al., 2010; Bastien et al.,\n2012). We acknowledge the support of the following agencies for research funding and computing\nsupport: NSERC, Calcul Qu\u00b4ebec, Compute Canada, the Canada Research Chairs and CIFAR. Bah-\ndanau thanks the support from Planet Intelligent Systems GmbH. We also thank Felix Hill, Bart van\nMerri\u00b4enboer, Jean Pouget-Abadie, Coline Devin and Tae-Ho Kim.\nREFERENCES\nAxelrod, A., He, X., and Gao, J. (2011). Domain adaptation via pseudo in-domain data selection.\nIn Proceedings of the ACL Conference on Empirical Methods in Natural Language Processing\n(EMNLP), pages 355\u2013362. Association for Computational Linguistics.\nBastien, F., Lamblin, P., Pascanu, R., Bergstra, J., Goodfellow, I. J., Bergeron, A., Bouchard, N.,\nand Bengio, Y. (2012). Theano: new features and speed improvements. Deep Learning and\nUnsupervised Feature Learning NIPS 2012 Workshop.\nBengio, Y., Simard, P., and Frasconi, P. (1994). Learning long-term dependencies with gradient\ndescent is dif\ufb01cult. IEEE Transactions on Neural Networks, 5(2), 157\u2013166.\nBengio, Y., Ducharme, R., Vincent, P., and Janvin, C. (2003). A neural probabilistic language model.\nJ. Mach. Learn. Res., 3, 1137\u20131155.\nBergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., Turian, J., Warde-\nFarley, D., and Bengio, Y. (2010). Theano: a CPU and GPU math expression compiler. In\nProceedings of the Python for Scienti\ufb01c Computing Conference (SciPy). Oral Presentation.\nBoulanger-Lewandowski, N., Bengio, Y., and Vincent, P. (2013). Audio chord recognition with\nrecurrent neural networks. In ISMIR.\nCho, K., van Merrienboer, B., Gulcehre, C., Bougares, F., Schwenk, H., and Bengio, Y. (2014a).\nLearning phrase representations using RNN encoder-decoder for statistical machine translation.\nIn Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014). to\nappear.\nCho, K., van Merri\u00a8enboer, B., Bahdanau, D., and Bengio, Y. (2014b). On the properties of neural\nmachine translation: Encoder\u2013Decoder approaches. In Eighth Workshop on Syntax, Semantics\nand Structure in Statistical Translation. to appear.\nDevlin, J., Zbib, R., Huang, Z., Lamar, T., Schwartz, R., and Makhoul, J. (2014). Fast and robust\nneural network joint models for statistical machine translation. In Association for Computational\nLinguistics.\nForcada, M. L. and \u02dcNeco, R. P. (1997). Recursive hetero-associative memories for translation. In\nJ. Mira, R. Moreno-D\u00b4\u0131az, and J. Cabestany, editors, Biological and Arti\ufb01cial Computation: From\nNeuroscience to Technology, volume 1240 of Lecture Notes in Computer Science, pages 453\u2013462.\nSpringer Berlin Heidelberg.\nGoodfellow, I., Warde-Farley, D., Mirza, M., Courville, A., and Bengio, Y. (2013). Maxout net-\nworks. In Proceedings of The 30th International Conference on Machine Learning, pages 1319\u2013\n1327.\nGraves, A. (2012). Sequence transduction with recurrent neural networks. In Proceedings of the\n29th International Conference on Machine Learning (ICML 2012).\nGraves, A. (2013).\nGenerating sequences with recurrent neural networks.\narXiv:1308.0850\n[cs.NE].\nGraves, A., Jaitly, N., and Mohamed, A.-R. (2013). Hybrid speech recognition with deep bidirec-\ntional LSTM. In Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Work-\nshop on, pages 273\u2013278.\n10\nPublished as a conference paper at ICLR 2015\nHermann, K. and Blunsom, P. (2014). Multilingual distributed representations without word align-\nment. In Proceedings of the Second International Conference on Learning Representations (ICLR\n2014).\nHochreiter, S. (1991). Untersuchungen zu dynamischen neuronalen Netzen. Diploma thesis, Institut\nf\u00a8ur Informatik, Lehrstuhl Prof. Brauer, Technische Universit\u00a8at M\u00a8unchen.\nHochreiter, S. and Schmidhuber, J. (1997). Long short-term memory. Neural Computation, 9(8),\n1735\u20131780.\nKalchbrenner, N. and Blunsom, P. (2013). Recurrent continuous translation models. In Proceedings\nof the ACL Conference on Empirical Methods in Natural Language Processing (EMNLP), pages\n1700\u20131709. Association for Computational Linguistics.\nKoehn, P. (2010). Statistical Machine Translation. Cambridge University Press, New York, NY,\nUSA.\nKoehn, P., Och, F. J., and Marcu, D. (2003). Statistical phrase-based translation. In Proceedings\nof the 2003 Conference of the North American Chapter of the Association for Computational\nLinguistics on Human Language Technology - Volume 1, NAACL \u201903, pages 48\u201354, Stroudsburg,\nPA, USA. Association for Computational Linguistics.\nPascanu, R., Mikolov, T., and Bengio, Y. (2013a). On the dif\ufb01culty of training recurrent neural\nnetworks. In ICML\u20192013.\nPascanu, R., Mikolov, T., and Bengio, Y. (2013b). On the dif\ufb01culty of training recurrent neural\nnetworks. In Proceedings of the 30th International Conference on Machine Learning (ICML\n2013).\nPascanu, R., Gulcehre, C., Cho, K., and Bengio, Y. (2014). How to construct deep recurrent neural\nnetworks. In Proceedings of the Second International Conference on Learning Representations\n(ICLR 2014).\nPouget-Abadie, J., Bahdanau, D., van Merri\u00a8enboer, B., Cho, K., and Bengio, Y. (2014). Overcoming\nthe curse of sentence length for neural machine translation using automatic segmentation. In\nEighth Workshop on Syntax, Semantics and Structure in Statistical Translation. to appear.\nSchuster, M. and Paliwal, K. K. (1997). Bidirectional recurrent neural networks. Signal Processing,\nIEEE Transactions on, 45(11), 2673\u20132681.\nSchwenk, H. (2012).\nContinuous space translation models for phrase-based statistical machine\ntranslation. In M. Kay and C. Boitet, editors, Proceedings of the 24th International Conference on\nComputational Linguistics (COLIN), pages 1071\u20131080. Indian Institute of Technology Bombay.\nSchwenk, H., Dchelotte, D., and Gauvain, J.-L. (2006). Continuous space language models for\nstatistical machine translation. In Proceedings of the COLING/ACL on Main conference poster\nsessions, pages 723\u2013730. Association for Computational Linguistics.\nSutskever, I., Vinyals, O., and Le, Q. (2014). Sequence to sequence learning with neural networks.\nIn Advances in Neural Information Processing Systems (NIPS 2014).\nZeiler, M. D. (2012).\nADADELTA: An adaptive learning rate method.\narXiv:1212.5701\n[cs.LG].\n11\nPublished as a conference paper at ICLR 2015\nA\nMODEL ARCHITECTURE\nA.1\nARCHITECTURAL CHOICES\nThe proposed scheme in Section 3 is a general framework where one can freely de\ufb01ne, for instance,\nthe activation functions f of recurrent neural networks (RNN) and the alignment model a. Here, we\ndescribe the choices we made for the experiments in this paper.\nA.1.1\nRECURRENT NEURAL NETWORK\nFor the activation function f of an RNN, we use the gated hidden unit recently proposed by Cho\net al. (2014a). The gated hidden unit is an alternative to the conventional simple units such as an\nelement-wise tanh. This gated unit is similar to a long short-term memory (LSTM) unit proposed\nearlier by Hochreiter and Schmidhuber (1997), sharing with it the ability to better model and learn\nlong-term dependencies. This is made possible by having computation paths in the unfolded RNN\nfor which the product of derivatives is close to 1. These paths allow gradients to \ufb02ow backward\neasily without suffering too much from the vanishing effect (Hochreiter, 1991; Bengio et al., 1994;\nPascanu et al., 2013a). It is therefore possible to use LSTM units instead of the gated hidden unit\ndescribed here, as was done in a similar context by Sutskever et al. (2014).\nThe new state si of the RNN employing n gated hidden units8 is computed by\nsi = f(si\u22121, yi\u22121, ci) = (1 \u2212zi) \u25e6si\u22121 + zi \u25e6\u02dcsi,\nwhere \u25e6is an element-wise multiplication, and zi is the output of the update gates (see below). The\nproposed updated state \u02dcsi is computed by\n\u02dcsi = tanh (We(yi\u22121) + U [ri \u25e6si\u22121] + Cci) ,\nwhere e(yi\u22121) \u2208Rm is an m-dimensional embedding of a word yi\u22121, and ri is the output of the\nreset gates (see below). When yi is represented as a 1-of-K vector, e(yi) is simply a column of an\nembedding matrix E \u2208Rm\u00d7K. Whenever possible, we omit bias terms to make the equations less\ncluttered.\nThe update gates zi allow each hidden unit to maintain its previous activation, and the reset gates ri\ncontrol how much and what information from the previous state should be reset. We compute them\nby\nzi = \u03c3 (Wze(yi\u22121) + Uzsi\u22121 + Czci) ,\nri = \u03c3 (Wre(yi\u22121) + Ursi\u22121 + Crci) ,\nwhere \u03c3 (\u00b7) is a logistic sigmoid function.\nAt each step of the decoder, we compute the output probability (Eq. (4)) as a multi-layered func-\ntion (Pascanu et al., 2014). We use a single hidden layer of maxout units (Goodfellow et al., 2013)\nand normalize the output probabilities (one for each word) with a softmax function (see Eq. (6)).\nA.1.2\nALIGNMENT MODEL\nThe alignment model should be designed considering that the model needs to be evaluated Tx \u00d7 Ty\ntimes for each sentence pair of lengths Tx and Ty. In order to reduce computation, we use a single-\nlayer multilayer perceptron such that\na(si\u22121, hj) = v\u22a4\na tanh (Wasi\u22121 + Uahj) ,\nwhere Wa \u2208Rn\u00d7n, Ua \u2208Rn\u00d72n and va \u2208Rn are the weight matrices. Since Uahj does not\ndepend on i, we can pre-compute it in advance to minimize the computational cost.\n8 Here, we show the formula of the decoder. The same formula can be used in the encoder by simply\nignoring the context vector ci and the related terms.\n12\nPublished as a conference paper at ICLR 2015\nA.2\nDETAILED DESCRIPTION OF THE MODEL\nA.2.1\nENCODER\nIn this section, we describe in detail the architecture of the proposed model (RNNsearch) used in the\nexperiments (see Sec. 4\u20135). From here on, we omit all bias terms in order to increase readability.\nThe model takes a source sentence of 1-of-K coded word vectors as input\nx = (x1, . . . , xTx), xi \u2208RKx\nand outputs a translated sentence of 1-of-K coded word vectors\ny = (y1, . . . , yTy), yi \u2208RKy,\nwhere Kx and Ky are the vocabulary sizes of source and target languages, respectively. Tx and Ty\nrespectively denote the lengths of source and target sentences.\nFirst, the forward states of the bidirectional recurrent neural network (BiRNN) are computed:\n\u2212\u2192h i =\n(\n(1 \u2212\u2212\u2192z i) \u25e6\u2212\u2192h i\u22121 + \u2212\u2192z i \u25e6\u2212\u2192h i\n, if i > 0\n0\n, if i = 0\nwhere\n\u2212\u2192h i = tanh\n\u0010\u2212\u2192\nWExi + \u2212\u2192\nU\nh\u2212\u2192r i \u25e6\u2212\u2192h i\u22121\ni\u0011\n\u2212\u2192z i =\u03c3\n\u0010\u2212\u2192\nW zExi + \u2212\u2192\nU z\n\u2212\u2192h i\u22121\n\u0011\n\u2212\u2192r i =\u03c3\n\u0010\u2212\u2192\nW rExi + \u2212\u2192\nU r\n\u2212\u2192h i\u22121\n\u0011\n.\nE \u2208Rm\u00d7Kx is the word embedding matrix. \u2212\u2192\nW, \u2212\u2192\nW z, \u2212\u2192\nW r \u2208Rn\u00d7m, \u2212\u2192\nU , \u2212\u2192\nU z, \u2212\u2192\nU r \u2208Rn\u00d7n are\nweight matrices. m and n are the word embedding dimensionality and the number of hidden units,\nrespectively. \u03c3(\u00b7) is as usual a logistic sigmoid function.\nThe backward states (\u2190\u2212h 1, \u00b7 \u00b7 \u00b7 , \u2190\u2212h Tx) are computed similarly. We share the word embedding matrix\nE between the forward and backward RNNs, unlike the weight matrices.\nWe concatenate the forward and backward states to to obtain the annotations (h1, h2, \u00b7 \u00b7 \u00b7 , hTx),\nwhere\nhi =\n\" \u2212\u2192h i\n\u2190\u2212h i\n#\n(7)\nA.2.2\nDECODER\nThe hidden state si of the decoder given the annotations from the encoder is computed by\nsi =(1 \u2212zi) \u25e6si\u22121 + zi \u25e6\u02dcsi,\nwhere\n\u02dcsi = tanh (WEyi\u22121 + U [ri \u25e6si\u22121] + Cci)\nzi =\u03c3 (WzEyi\u22121 + Uzsi\u22121 + Czci)\nri =\u03c3 (WrEyi\u22121 + Ursi\u22121 + Crci)\nE is the word embedding matrix for the target language. W, Wz, Wr \u2208Rn\u00d7m, U, Uz, Ur \u2208Rn\u00d7n,\nand C, Cz, Cr \u2208Rn\u00d72n are weights. Again, m and n are the word embedding dimensionality\nand the number of hidden units, respectively. The initial hidden state s0 is computed by s0 =\ntanh\n\u0010\nWs\n\u2190\u2212h 1\n\u0011\n, where Ws \u2208Rn\u00d7n.\nThe context vector ci are recomputed at each step by the alignment model:\nci =\nTx\nX\nj=1\n\u03b1ijhj,\n13\nPublished as a conference paper at ICLR 2015\nModel\nUpdates (\u00d7105)\nEpochs\nHours\nGPU\nTrain NLL\nDev. NLL\nRNNenc-30\n8.46\n6.4\n109\nTITAN BLACK\n28.1\n53.0\nRNNenc-50\n6.00\n4.5\n108\nQuadro K-6000\n44.0\n43.6\nRNNsearch-30\n4.71\n3.6\n113\nTITAN BLACK\n26.7\n47.2\nRNNsearch-50\n2.88\n2.2\n111\nQuadro K-6000\n40.7\n38.1\nRNNsearch-50\u22c6\n6.67\n5.0\n252\nQuadro K-6000\n36.7\n35.2\nTable 2: Learning statistics and relevant information. Each update corresponds to updating the\nparameters once using a single minibatch. One epoch is one pass through the training set. NLL is\nthe average conditional log-probabilities of the sentences in either the training set or the development\nset. Note that the lengths of the sentences differ.\nwhere\n\u03b1ij =\nexp (eij)\nPTx\nk=1 exp (eik)\neij =v\u22a4\na tanh (Wasi\u22121 + Uahj) ,\nand hj is the j-th annotation in the source sentence (see Eq. (7)). va \u2208Rn\u2032, Wa \u2208Rn\u2032\u00d7n and\nUa \u2208Rn\u2032\u00d72n are weight matrices. Note that the model becomes RNN Encoder\u2013Decoder (Cho\net al., 2014a), if we \ufb01x ci to \u2212\u2192h Tx.\nWith the decoder state si\u22121, the context ci and the last generated word yi\u22121, we de\ufb01ne the probability\nof a target word yi as\np(yi|si, yi\u22121, ci) \u221dexp\n\u0000y\u22a4\ni Woti\n\u0001\n,\nwhere\nti =\n\u0002\nmax\n\b\u02dcti,2j\u22121, \u02dcti,2j\n\t\u0003\u22a4\nj=1,...,l\nand \u02dcti,k is the k-th element of a vector \u02dcti which is computed by\n\u02dcti =Uosi\u22121 + VoEyi\u22121 + Coci.\nWo \u2208RKy\u00d7l, Uo \u2208R2l\u00d7n, Vo \u2208R2l\u00d7m and Co \u2208R2l\u00d72n are weight matrices. This can be under-\nstood as having a deep output (Pascanu et al., 2014) with a single maxout hidden layer (Goodfellow\net al., 2013).\nA.2.3\nMODEL SIZE\nFor all the models used in this paper, the size of a hidden layer n is 1000, the word embedding\ndimensionality m is 620 and the size of the maxout hidden layer in the deep output l is 500. The\nnumber of hidden units in the alignment model n\u2032 is 1000.\nB\nTRAINING PROCEDURE\nB.1\nPARAMETER INITIALIZATION\nWe initialized the recurrent weight matrices U, Uz, Ur, \u2190\u2212\nU , \u2190\u2212\nU z, \u2190\u2212\nU r, \u2212\u2192\nU , \u2212\u2192\nU z and \u2212\u2192\nU r as random or-\nthogonal matrices. For Wa and Ua, we initialized them by sampling each element from the Gaussian\ndistribution of mean 0 and variance 0.0012. All the elements of Va and all the bias vectors were ini-\ntialized to zero. Any other weight matrix was initialized by sampling from the Gaussian distribution\nof mean 0 and variance 0.012.\nB.2\nTRAINING\nWe used the stochastic gradient descent (SGD) algorithm. Adadelta (Zeiler, 2012) was used to\nautomatically adapt the learning rate of each parameter (\u03f5 = 10\u22126 and \u03c1 = 0.95). We explicitly\n14\nPublished as a conference paper at ICLR 2015\nnormalized the L2-norm of the gradient of the cost function each time to be at most a prede\ufb01ned\nthreshold of 1, when the norm was larger than the threshold (Pascanu et al., 2013b). Each SGD\nupdate direction was computed with a minibatch of 80 sentences.\nAt each update our implementation requires time proportional to the length of the longest sentence in\na minibatch. Hence, to minimize the waste of computation, before every 20-th update, we retrieved\n1600 sentence pairs, sorted them according to the lengths and split them into 20 minibatches. The\ntraining data was shuf\ufb02ed once before training and was traversed sequentially in this manner.\nIn Tables 2 we present the statistics related to training all the models used in the experiments.\nC\nTRANSLATIONS OF LONG SENTENCES\nSource\nAn admitting privilege is the right of a doctor to admit a patient to a hospital or a medical centre\nto carry out a diagnosis or a procedure, based on his status as a health care worker at a hospital.\nReference\nLe privil`ege d\u2019admission est le droit d\u2019un m\u00b4edecin, en vertu de son statut de membre soignant\nd\u2019un h\u02c6opital, d\u2019admettre un patient dans un h\u02c6opital ou un centre m\u00b4edical a\ufb01n d\u2019y d\u00b4elivrer un\ndiagnostic ou un traitement.\nRNNenc-50\nUn privil`ege d\u2019admission est le droit d\u2019un m\u00b4edecin de reconna\u02c6\u0131tre un patient `a l\u2019h\u02c6opital ou un\ncentre m\u00b4edical d\u2019un diagnostic ou de prendre un diagnostic en fonction de son \u00b4etat de sant\u00b4e.\nRNNsearch-50\nUn privil`ege d\u2019admission est le droit d\u2019un m\u00b4edecin d\u2019admettre un patient `a un h\u02c6opital ou un\ncentre m\u00b4edical pour effectuer un diagnostic ou une proc\u00b4edure, selon son statut de travailleur des\nsoins de sant\u00b4e `a l\u2019h\u02c6opital.\nGoogle\nTranslate\nUn privil`ege admettre est le droit d\u2019un m\u00b4edecin d\u2019admettre un patient dans un h\u02c6opital ou un\ncentre m\u00b4edical pour effectuer un diagnostic ou une proc\u00b4edure, fond\u00b4ee sur sa situation en tant\nque travailleur de soins de sant\u00b4e dans un h\u02c6opital.\nSource\nThis kind of experience is part of Disney\u2019s efforts to \u201dextend the lifetime of its series and build\nnew relationships with audiences via digital platforms that are becoming ever more important,\u201d\nhe added.\nReference\nCe type d\u2019exp\u00b4erience entre dans le cadre des efforts de Disney pour \u201d\u00b4etendre la dur\u00b4ee de\nvie de ses s\u00b4eries et construire de nouvelles relations avec son public gr\u02c6ace `a des plateformes\nnum\u00b4eriques qui sont de plus en plus importantes\u201d, a-t-il ajout\u00b4e.\nRNNenc-50\nCe type d\u2019exp\u00b4erience fait partie des initiatives du Disney pour \u201dprolonger la dur\u00b4ee de vie de\nses nouvelles et de d\u00b4evelopper des liens avec les lecteurs num\u00b4eriques qui deviennent plus com-\nplexes.\nRNNsearch-50\nCe genre d\u2019exp\u00b4erience fait partie des efforts de Disney pour \u201dprolonger la dur\u00b4ee de vie de ses\ns\u00b4eries et cr\u00b4eer de nouvelles relations avec des publics via des plateformes num\u00b4eriques de plus\nen plus importantes\u201d, a-t-il ajout\u00b4e.\nGoogle\nTranslate\nCe genre d\u2019exp\u00b4erience fait partie des efforts de Disney `a \u201c\u00b4etendre la dur\u00b4ee de vie de sa s\u00b4erie et\nconstruire de nouvelles relations avec le public par le biais des plates-formes num\u00b4eriques qui\ndeviennent de plus en plus important\u201d, at-il ajout\u00b4e.\nSource\nIn a press conference on Thursday, Mr Blair stated that there was nothing in this video that might\nconstitute a \u201dreasonable motive\u201d that could lead to criminal charges being brought against the\nmayor.\nReference\nEn conf\u00b4erence de presse, jeudi, M. Blair a af\ufb01rm\u00b4e qu\u2019il n\u2019y avait rien dans cette vid\u00b4eo qui puisse\nconstituer des \u201dmotifs raisonnables\u201d pouvant mener au d\u00b4ep\u02c6ot d\u2019une accusation criminelle contre\nle maire.\nRNNenc-50\nLors de la conf\u00b4erence de presse de jeudi, M. Blair a dit qu\u2019il n\u2019y avait rien dans cette vid\u00b4eo qui\npourrait constituer une \u201dmotivation raisonnable\u201d pouvant entra\u02c6\u0131ner des accusations criminelles\nport\u00b4ees contre le maire.\nRNNsearch-50\nLors d\u2019une conf\u00b4erence de presse jeudi, M. Blair a d\u00b4eclar\u00b4e qu\u2019il n\u2019y avait rien dans cette vid\u00b4eo qui\npourrait constituer un \u201dmotif raisonnable\u201d qui pourrait conduire `a des accusations criminelles\ncontre le maire.\nGoogle\nTranslate\nLors d\u2019une conf\u00b4erence de presse jeudi, M. Blair a d\u00b4eclar\u00b4e qu\u2019il n\u2019y avait rien dans cette vido\nqui pourrait constituer un \u201dmotif raisonnable\u201d qui pourrait mener `a des accusations criminelles\nportes contre le maire.\nTable 3: The translations generated by RNNenc-50 and RNNsearch-50 from long source sentences\n(30 words or more) selected from the test set. For each source sentence, we also show the gold-\nstandard translation. The translations by Google Translate were made on 27 August 2014.\n15\n",
        "sentence": " Unfortunately, this comes at a cost: while soft attention models can be trained with standard backpropagation [6, 5], this does not work for hard attention models, whose glimpse selections are typically discrete. Such models have been applied successfully in image classification [12, 4, 3, 2], object tracking [13, 3], machine translation [6], caption generation [5], and image generation [14, 15].",
        "context": "3\nPublished as a conference paper at ICLR 2015\nthe alignment is not considered to be a latent variable. Instead, the alignment model directly com-\nputes a soft alignment, which allows the gradient of the cost function to be backpropagated through.\neasily without suffering too much from the vanishing effect (Hochreiter, 1991; Bengio et al., 1994;\nPascanu et al., 2013a). It is therefore possible to use LSTM units instead of the gated hidden unit\nFurthermore, qualitative analysis reveals that the proposed model \ufb01nds a linguistically plausible\n(soft-)alignment between a source sentence and the corresponding target sentence.\n2\nBACKGROUND: NEURAL MACHINE TRANSLATION"
    },
    {
        "title": "Reinforcement learning neural Turing machines",
        "author": [
            "W. Zaremba",
            "I. Sutskever"
        ],
        "venue": "arXiv:1505.00521,",
        "citeRegEx": "7",
        "shortCiteRegEx": null,
        "year": 2015,
        "abstract": "",
        "full_text": "",
        "sentence": " (The latter problem was also observed by [7] in the context of memory networks.",
        "context": null
    },
    {
        "title": "The Helmholtz machine",
        "author": [
            "P. Dayan",
            "G.E. Hinton",
            "R.M. Neal",
            "R.S. Zemel"
        ],
        "venue": "Neural Computation, 7:889\u2013904,",
        "citeRegEx": "8",
        "shortCiteRegEx": null,
        "year": 1995,
        "abstract": " Discovering the structure inherent in a set of patterns is a fundamental aim of statistical inference or learning. One fruitful approach is to build a parameterized stochastic generative model, independent draws from which are likely to produce the patterns. For all but the simplest generative models, each pattern can be generated in exponentially many ways. It is thus intractable to adjust the parameters to maximize the probability of the observed patterns. We describe a way of finessing this combinatorial explosion by maximizing an easily computed lower bound on the probability of the observations. Our method can be viewed as a form of hierarchical self-supervised learning that may relate to the function of bottom-up and top-down cortical processing pathways. ",
        "full_text": "",
        "sentence": " inference networks [8], the reweighted wake-sleep algorithm [9], and control variates [10, 11]. A classic example was the Helmholtz machine [8], where the inference network predicts a mean field approximation to the posterior.",
        "context": null
    },
    {
        "title": "Reweighted wake-sleep",
        "author": [
            "J. Bornschein",
            "Y. Bengio"
        ],
        "venue": "arXiv:1406.2751,",
        "citeRegEx": "9",
        "shortCiteRegEx": null,
        "year": 2014,
        "abstract": "",
        "full_text": "",
        "sentence": " inference networks [8], the reweighted wake-sleep algorithm [9], and control variates [10, 11]. The reweighted wake-sleep approach [9] is similar to traditional wake-sleep, but uses importance sampling in place of mean field inference to approximate the posterior. Instead, we adopt an approach based on the wake-p step of reweighted wake-sleep [9], where we attempt to maximize the marginal log-probability ` directly. Our training procedure for the inference network parallels the wake-q step of reweighted wakesleep [9]. This method improves upon prior work by using the reweighted wake-sleep algorithm [9] to approximate expectations from the posterior over glimpses.",
        "context": null
    },
    {
        "title": "Variational Bayesian inference with stochastic search",
        "author": [
            "J. Paisley",
            "D.M. Blei",
            "M.I. Jordan"
        ],
        "venue": "International Conference on Machine Learning,",
        "citeRegEx": "10",
        "shortCiteRegEx": null,
        "year": 2012,
        "abstract": "Mean-field variational inference is a method for approximate Bayesian\nposterior inference. It approximates a full posterior distribution with a\nfactorized set of distributions by maximizing a lower bound on the marginal\nlikelihood. This requires the ability to integrate a sum of terms in the log\njoint likelihood using this factorized distribution. Often not all integrals\nare in closed form, which is typically handled by using a lower bound. We\npresent an alternative algorithm based on stochastic optimization that allows\nfor direct optimization of the variational lower bound. This method uses\ncontrol variates to reduce the variance of the stochastic search gradient, in\nwhich existing lower bounds can play an important role. We demonstrate the\napproach on two non-conjugate models: logistic regression and an approximation\nto the HDP.",
        "full_text": "Variational Bayesian Inference with Stochastic Search\nJohn Paisley1\njpaisley@berkeley.edu\nDavid M. Blei3\nblei@cs.princeton.edu\nMichael I. Jordan1,2\njordan@eecs.berkeley.edu\n1Department of EECS, 2Department of Statistics, UC Berkeley\n3Department of Computer Science, Princeton University\nAbstract\nMean-\ufb01eld variational inference is a method\nfor approximate Bayesian posterior inference.\nIt approximates a full posterior distribution\nwith a factorized set of distributions by max-\nimizing a lower bound on the marginal likeli-\nhood. This requires the ability to integrate a\nsum of terms in the log joint likelihood using\nthis factorized distribution. Often not all in-\ntegrals are in closed form, which is typically\nhandled by using a lower bound. We present\nan alternative algorithm based on stochastic\noptimization that allows for direct optimiza-\ntion of the variational lower bound.\nThis\nmethod uses control variates to reduce the\nvariance of the stochastic search gradient, in\nwhich existing lower bounds can play an im-\nportant role. We demonstrate the approach\non two non-conjugate models: logistic regres-\nsion and an approximation to the HDP.\n1. Introduction\nMean-\ufb01eld variational Bayesian (MFVB) inference is\nan optimization-based approach to approximating the\nfull posterior of the latent variables of a Bayesian\nmodel (Jordan et al., 1999). It has been applied to\nmany problem domains, for example mixture model-\ning (Blei & Jordan, 2006), sequential modeling (Beal,\n2003) and factor analysis (Paisley & Carin, 2009). In\naddition, recent development of the theory has ex-\ntended the method to online inference and stochastic\noptimization settings, making variational Bayes a vi-\nable approach for Bayesian learning with massive data\nsets (Ho\ufb00man et al., 2010; Wang et al., 2011).\nAppearing in Proceedings of the 29 th International Confer-\nence on Machine Learning, Edinburgh, Scotland, UK, 2012.\nCopyright 2012 by the author(s)/owner(s).\nVariational Bayes approximates the full posterior by\nattempting to minimize the Kullback-Leibler diver-\ngence between the true posterior and a prede\ufb01ned fac-\ntorized distribution on the same variables. Minimiz-\ning this divergence is equivalent to maximizing the fa-\nmiliar variational objective function.\nTo review, let\n\u0398 = {\u03b8i} represent the set of latent variables (ran-\ndom e\ufb00ects and parameters) in the model and X rep-\nresent the data. The joint likelihood of X and \u0398 is\nP(X, \u0398|\u03a5), with \u03a5 the set of hyperparameters. Varia-\ntional inference approximates the posterior P(\u0398|X, \u03a5)\nwith a Q distribution that takes a set of variational\nparameters \u03a8 = {\u03c8i}. This distribution is factorized,\nQ(\u0398|\u03a8) = Q\ni qi(\u03b8i|\u03c8i), and the values of \u03a8 are opti-\nmized to maximize the objective function,\nL(X, \u03a8) = EQ[ln P(X, \u0398|\u03a5)] + H[Q(\u0398|\u03a8)].\n(1)\nThe solution is only locally optimal when L is not\nconvex, which is usually the case.\nMost variational\ninference algorithms optimize L by coordinate ascent,\nwhich repeatedly cycles through and optimizes with\nrespect to each variational parameter \u03c8i. Often the\nlocally optimal value of \u03c8i has a closed-form solution,\nfor example in conjugate exponential models.\nThe log of the joint likelihood results in a sum of terms;\na major issue that often arises in MFVB is that not all\nexpectations in this sum are in closed form. A typical\nsolution in this case is to replace the problematic func-\ntion with another function of the same variables (plus\nauxiliary variables) that is a point-wise lower bound.\nThis new function is selected such that the expectation\nis tractable. While inference can now proceed, a draw-\nback of introducing bounds is that the true variational\nobjective function is no longer being optimized, which\nmay lead to a signi\ufb01cantly worse posterior approxi-\nmation. Therefore, much attention has been paid to\ndeveloping tight bounds of commonly occurring func-\ntions (e.g., Jaakkola & Jordan (2000), Marlin et al.\n(2011), Leisink & Kappen (2001)).\nVariational Bayesian Inference with Stochastic Search\nWe present a method for directly optimizing Eq. (1)\nfor models in which not all expectations are tractable;\nwe show how a stochastic approximation of \u2207\u03c8iL\ncan allow for optimization of L when the expecta-\ntion Eqi[ln P(X, \u0398|\u03a5)] is not in closed form. The ap-\nproximation is unbiased, and so by using the proposed\nstochastic method we are directly optimizing L.\nOur stochastic approximation is based on Monte Carlo\nintegration, for which the number of samples heavily\ndepends on the variance of this approximation. We in-\ntroduce a control variate (Ross, 2006) to signi\ufb01cantly\nreduce the variance of this stochastic approximation.\nA control variate is a tractable function g that is highly\ncorrelated with the intractable function f. The func-\ntion g replaces f in Eq. (1), and the gradient is then\nstochastically corrected for bias.\nExisting lower bounds have properties that make them\nideal as control variates, and thus can improve the\nspeed of the algorithm. However, a major advantage\nof the control variate methodology is that it does not\nrequire the tractable function g to bound f, but only to\ncorrelate well with it (i.e., to approximate it well mod-\nulo a scaling). This opens the door to many more func-\ntions that may give better approximations than a lower\nbound. One of these possible functions is the second-\norder Taylor expansion, which often gives a very good\napproximation, while also allowing for closed-form ex-\npectations. We show the potential performance gain\nusing this function as a control variate, which we de-\nnote the control variate delta method for MFVB.\nRelated work.\nRecent work by Knowles & Minka\n(2011) has also addressed the problem of intractable\nexpectations in MFVB inference in the context of de-\nveloping a more general variational message passing al-\ngorithm. Our solution arises from a di\ufb00erent perspec-\ntive and results in a new algorithm based on stochastic\noptimization. Graves (2011) considers a similar prob-\nlem for neural networks, but a lack of control variates\nlimits the algorithm to signi\ufb01cantly simpler variational\napproximations.\nStochastic search algorithms have\nalso been developed for models of Evolution Strate-\ngies (see, e.g., Yi et al. (2009)).\n2. Mean-\ufb01eld variational inference\nMean-\ufb01eld variational Bayesian (MFVB) inference ap-\nproximates the full posterior of the latent variables of\na Bayesian model with a factorized distribution. As\nmotivated in the introduction, let \u0398 = {\u03b8i} be these\nvariables, X the data and \u03a5 all hyperparameters of\nthe prior distributions on \u0398.\nWe de\ufb01ne the factor-\nized distribution on \u0398 to be Q(\u0398|\u03a8) = Q\ni qi(\u03b8i|\u03c8i),\nwhere \u03c8i are the parameters of the qi distributions.\nThe variational objective function arises by bounding\nthe marginal likelihood using the Q distribution,\nln P(X|\u03a5)\n=\nln\nZ\n\u0398\nP(X, \u0398|\u03a5)d\u0398\n(2)\n\u2265\nZ\n\u0398\nQ(\u0398|\u03a8) ln P(X, \u0398|\u03a5)\nQ(\u0398|\u03a8) d\u0398.\nMaximizing this lower bound (denoted L) with re-\nspect to \u03a8 is equivalent to minimizing the Kullback-\nLeibler divergence between Q(\u0398) and P(\u0398|X, \u03a5),\nwhich makes up the di\ufb00erence in Eq. (2).\nTo facilitate our discussion, we write the functions ap-\npearing in the log joint likelihood as ln P(X, \u0398|\u03a5) =\nP\nj fj(XAj, \u0398Bj), where Aj indexes the data appear-\ning in function j and Bj indexes the latent variables\nappearing in function j. We note that the index j does\nnot correspond to variables or distributions, but to the\nterms of the log joint likelihood. Using this notation,\nthe variational lower bound in Eq. (1) becomes\nL = P\nj EQ[fj(XAj, \u0398Bj)] + P\ni H[qi(\u03b8i|\u03c8i)].\n(3)\nFor each function fj, those \u03b8i /\u2208\u0398Bj will have their\ncorresponding qi removed from the expectation. For\nthose \u03b8i \u2208\u0398Bj, the expectation of fj results in a new\nfunction of variational parameters \u03c8i \u2208\u03a8Bj. Ideally,\nall expectations will be in closed form, allowing for the\noptimization of \u03a8 to proceed.\nIn the case where an expectation in Eq. (3) is not\ntractable, a nicer functional lower bound can replace\nthe problematic function. That is, let Eqi[fj(\u03b8i)] be\nintractable.1 A common approach to dealing with this\nissue is to introduce a function g(\u03b8i, \u03be) that replaces fj\nand is a point-wise lower bound: fj(\u03b8i) \u2265g(\u03b8i, \u03be) for\nall \u03b8i. The function g usually takes auxiliary variables\n\u03be, which determines how tightly g approximates fj and\nis tuned along with the other parameters during infer-\nence. The expectation Eqi[g(\u03b8i, \u03be)] has a closed-form\nsolution, and gives a lower bound on the variational\nobjective that can be optimized.\nTo illustrate, consider the case where fj is convex in \u03b8i.\nThen a bound g could be a \ufb01rst-order Taylor expansion\nof fj about the point \u03be, which has a closed-form ex-\npectation. Signi\ufb01cantly tighter tractable bounds have\nalso been developed for various frequently occurring\nfunctions (e.g., Marlin et al. (2011), Knowles & Minka\n(2011)). In general, the looser the bound the further\none is from optimizing the variational objective, and\nlearning of \u03c8i can su\ufb00er as a result.\n1We have simpli\ufb01ed the notation for clarity.\nVariational Bayesian Inference with Stochastic Search\n3. Stochastic search variational Bayes\nWe next present a method based on stochastic search\nfor directly optimizing the variational objective func-\ntion L in cases where some expectations cannot be\ncomputed in the log joint likelihood.\nThis method\nuses a stochastic approximation of the gradient with\nrespect to the variational parameters of the associated\nq distribution. To further simplify notation, we drop\nall indices; f is the intractable function of \u03b8 (plus other\nvariational parameters), and \u03b8 has a variational distri-\nbution q taking parameters \u03c8.\nWe separate the lower bound L into two functions, Ef\nand h, where h(X, \u03a8) contains everything in L except\nfor Ef. Notably, h contains all other functions of \u03c8\nresulting from expectations calculated with respect to\nq. In coordinate ascent variational inference, the \ufb01rst\nstep in optimizing q with respect to its parameters \u03c8\nis to take the gradient of the variational objective,\n\u2207\u03c8L = \u2207\u03c8Eq[f(\u03b8)] + \u2207\u03c8h(X, \u03a8).\n(4)\nThis gradient contains a tractable term resulting from\n\u2207\u03c8h, and an intractable term \u2207\u03c8Eqf. Our goal is to\nmake a stochastic approximation of this gradient. To\nthis end, assuming the necessary regularity conditions,\nwe rewrite this function as\n\u2207\u03c8Eq[f(\u03b8)] = \u2207\u03c8\nZ\n\u03b8\nf(\u03b8)q(\u03b8|\u03c8)d\u03b8\n(5)\n=\nZ\n\u03b8\nf(\u03b8)\u2207\u03c8q(\u03b8|\u03c8)d\u03b8\n=\nZ\n\u03b8\nf(\u03b8)q(\u03b8|\u03c8)\u2207\u03c8 ln q(\u03b8|\u03c8)d\u03b8.\nWe use the identity \u2207\u03c8q(\u03b8|\u03c8) = q(\u03b8|\u03c8)\u2207\u03c8 ln q(\u03b8|\u03c8).\nIt follows that \u2207\u03c8Eq[f(\u03b8)] = Eq[f(\u03b8)\u2207\u03c8 ln q(\u03b8|\u03c8)]. We\ncan stochastically approximate this expectation using\nMonte Carlo integration,\n\u2207\u03c8Eq[f(\u03b8)] \u2248\n1\nS\nS\nX\ns=1\nf(\u03b8(s))\u2207\u03c8 ln q(\u03b8(s)|\u03c8),\n(6)\nwhere \u03b8(s) iid\n\u223cq(\u03b8|\u03c8) for s = 1, . . . , S. We can there-\nfore replace \u2207\u03c8Eq[f(\u03b8)] with the unbiased stochastic\napproximation of this gradient in Eq. (6). Denote this\napproximation as \u03b6. At iteration t, we update the vari-\national parameter \u03c8 by taking a gradient step,\n\u03c8(t+1) = \u03c8(t) + \u03c1t\u2207\u03c8h(X, \u03a8(t)) + \u03c1t\u03b6t.\n(7)\nBy decreasing the step size \u03c1t such that P\u221e\nt=1 \u03c1t = \u221e\nand P\u221e\nt=1 \u03c12\nt < \u221e, convergence to a local optimal so-\nlution of L is guaranteed. For example, \u03c1t = (w +t)\u2212\u03b7\nwith \u03b7 \u2208(0.5, 1] and w \u22650 satis\ufb01es this requirement.\n4. Searching with control variates\nA practical issue with the stochastic approximation\nproposed in Sec. 3 is that the variance of the gradient\napproximation may be very large. Given S samples\nof a random vector X, the covariance of its unbiased\nsample mean \u00afX is known to be Cov( \u00afX) = Cov(X)/S.\nWhen the diagonal values of Cov(X) are large, many\nsamples will be required to bring this variance below\na desired level for approximating the expectation. As\nour experiments will show in Sec. 6, the value of S can\nbe very large in practice and lead to a slow algorithm.\nWe therefore seek a variance reduction method to re-\nduce the number of samples needed to construct the\nstochastic search direction.\nWe introduce a control variate (Ross, 2006) to reduce\nthe variance of the stochastic gradient constructed in\nEq. (6). A control variate is a random variable that\nis highly correlated with an intractable variable, but\nfor which the expectation is tractable.\nIn this case\nthe random variable is f(\u03b8), for which we introduce\na control variate g(\u03b8). Control variates are ideal for\nMFVB because they can leverage the existing bounds,\nthough they also admit a larger class of functions. We\nnext review this variance reduction technique for Ef,\nand discuss the modi\ufb01cations needed to account for\nthe stochastic vector f(\u03b8)\u2207\u03c8 ln q(\u03b8|\u03c8).\n4.1. A control variate for f(\u03b8)\nGenerally speaking, variance reduction works by mod-\nifying a function of a random variable such that its ex-\npectation remains the same, but its variance decreases.\nToward this end, we introduce a control variate g(\u03b8),\nwhich approximates f(\u03b8) well in the highly probable\nregions as de\ufb01ned by q(\u03b8), but also has a closed-form\nexpectation under q. Using g and a scalar a \u2208R, we\n\ufb01rst form the new function \u02c6f,\n\u02c6f(\u03b8) = f(\u03b8) \u2212a(g(\u03b8) \u2212Eq[g(\u03b8)]).\n(8)\nThis function has the same expectation as f and there-\nfore can replace it in L in Eq. (3).\nThe next step is to set the value of a to minimize the\nvariance of \u02c6f. A simple calculation shows that\nVar( \u02c6f) = Var(f) \u22122aCov(f, g) + a2Var(g).\n(9)\nTaking the derivative with respect to a and setting to\nzero gives the optimal value,\na = Cov(f, g)\nVar(g) .\n(10)\nAs is usual, this covariance and variance is unknown\nin the functions we encounter. We can approximate\nVariational Bayesian Inference with Stochastic Search\nAlgorithm 1 Variational Bayes with stochastic search\nGoal To calculate \u2207\u03c8L = \u2207\u03c8Eq[f(\u03b8)] + \u2207\u03c8h(X, \u03a8).\nApproximate \u2207\u03c8L using stochastic search.\ninput Variance reduction parameter \u03f5.\n1: Introduce the function g(\u03b8) as a control variate\nthat highly correlates with f(\u03b8).\n2: Sample an initial (small) collection \u03b8(s) \u223cq(\u03b8|\u03c8).\n3: Sum the sample variances and covariances\n\u03b2 = PK\nk=1 Var(g \u2202ln q\n\u2202\u03c8k ), \u03b3 = PK\nk=1 Var(f \u2202ln q\n\u2202\u03c8k ),\n\u03b1 = PK\nk=1 Cov(f \u2202ln q\n\u2202\u03c8k , g \u2202ln q\n\u2202\u03c8k ).\n4: Set \u02c6a = \u03b1/\u03b2 and S = (\u03b3 \u2212\u03b12/\u03b2)/\u03f5K.\n5: Sample \u03b8(s) \u223cq(\u03b8|\u03c8) i.i.d. for s = 1, . . . , \u2308S\u2309.\n6: Construct the stochastic search vector\n\u03b6 = 1\nS\nPS\ns=1{f(\u03b8(s)) \u2212\u02c6ag(\u03b8(s))}\u2207\u03c8 ln q(\u03b8(s)|\u03c8).\n7: Step in the direction of the stochastic gradient\n\u03c8\u2032 = \u03c8 + \u03c1\u03b6 + \u03c1\u2207\u03c8(h(X, \u03a8) + \u02c6aEq[g(\u03b8)]).\na with \u02c6a, found by plugging the sample variance and\ncovariance into Eq. (10) using samples from the algo-\nrithm.\nThe potential reduction in variance is seen by plugging\nEq. (10) into Eq. (9) and taking the ratio of the two\nvariances,\nVar( \u02c6f)/Var(f) = 1 \u2212Corr(f, g)2.\n(11)\nTherefore, the greater the correlation between f and g,\nthe greater the variance reduction. Tight lower bounds\nof f by construction have this high correlation, but we\nnote that tight upper bounds work as well, as do well-\napproximating functions that do not bound f.\nUsing the control variate g, we now write the stochastic\napproximation to the gradient as\n\u2207\u03c8Eq[ \u02c6f(\u03b8)] \u2248\u02c6a\u2207\u03c8Eq[g(\u03b8)]\n(12)\n+ 1\nS\nS\nX\ns=1\n{f(\u03b8(s)) \u2212\u02c6ag(\u03b8(s))}\u2207\u03c8 ln q(\u03b8(s)|\u03c8),\nwhere \u03b8(s) iid\n\u223cq(\u03b8|\u03c8) for s = 1, . . . , S.\nWriting the stochastic approximation this way allows\nfor a more intuitive understanding of the algorithm.\nBy separating the tractable and stochastic parts as\ndone in Eq. (12), we \ufb01rst replace the intractable func-\ntion f with a tractable approximation g. (This resem-\nbles the standard method when g lower bounds f.)\nThe gradient of Eg is then corrected by a stochastic\nvector. The variance of the correction is smaller than\nthat of the original stochastic approximation in Sec. 3,\nsince the function f(\u03b8) is close to \u02c6ag(\u03b8). The gradient\nof Eg can be thought of as an initial guess, followed\nby a stochastic correction which ensures that we are\noptimizing the variational objective function.\n4.2. The stochastic search case\nWe have introduced a control variate for f(\u03b8), but in\nfact we would like to minimize the variance of the vec-\ntor f(\u03b8)\u2207\u03c8 ln q(\u03b8|\u03c8) in Eq. (6). In this case, the con-\ntrol variate becomes g(\u03b8)\u2207\u03c8 ln q(\u03b8|\u03c8) and we have the\nfollowing modi\ufb01cation.\nLet \u03c8k be the kth dimension of \u03c8.\nThen for each\ndimension the discussion in Sec. 4.1 carries through,\nbut for f \u2202ln q\n\u2202\u03c8k\nand g \u2202ln q\n\u2202\u03c8k\ninstead of f and g.\nThe\nvariance of each dimension again follows Eq. (9), and\nwe seek an a to minimize the sum of these equations.\nThis results in the optimal value\na = P\nk Cov(f \u2202ln q\n\u2202\u03c8k , g \u2202ln q\n\u2202\u03c8k )/ P\nk Var(g \u2202ln q\n\u2202\u03c8k ),\nwhich we approximate using samples. We summarize\nstochastic search variational Bayes in Algorithm 1.\n5. Stochastic search VB for two models\nWe next illustrate stochastic search variational infer-\nence on logistic regression and a \ufb01nite approximation\nto the hierarchical Dirichlet process (Teh et al., 2007).\nFor logistic regression, we will consider two control\nvariates, one of which is a lower bound and the other\nof which is not a bound. For the \ufb01nite HDP, we will\nconsider a piecewise control variate, one part being an\nupper bound on the original function.\n5.1. Logistic regression\nBinary logistic regression takes in d-dimensional data\nvectors xn and predicts the class yn \u2208{\u22121, 1} to which\neach belongs. The parameter is \u03b8 \u2208Rd and the predic-\ntion law is Pr(yn|xn, \u03b8) = \u03c3(ynxT\nn\u03b8) where \u03c3(\u00b7) is the\nsigmoid function, \u03c3(b) = (1+e\u2212b)\u22121. Bayesian logistic\nregression places a prior distribution on the coe\ufb03cient\nvector, \u03b8 \u223cNormal(0, cI). For inference we de\ufb01ne a\nGaussian variational q distribution\nq(\u03b8) = Normal(\u00b5, \u03a3).\n(13)\nThe variational lower bound for this model is\nL = PN\nn=1 Eq[ln \u03c3(ynxT\nn\u03b8)]+Eq[ln p(\u03b8)\u2212ln q(\u03b8)]. (14)\nThe expectations of fn(yn, xn; \u03b8) := ln \u03c3(ynxT\nn\u03b8) are\nintractable. One approach to avoiding this issue is to\nforgo variational inference and use Laplace\u2019s method\nto approximate q. This method sets \u00b5 to the MAP so-\nlution, and \u03a3\u22121 to the negative Hessian of the log joint\nlikelihood evaluated at \u00b5. Another is to lower bound\nfn with the bound in, e.g., Jaakkola & Jordan (2000),\nwhich allows for closed-form variational inference. We\nconsider this bound as a control variate.\nVariational Bayesian Inference with Stochastic Search\n\u221210\n\u22128\n\u22126\n\u22124\n\u22122\n0\n2\n\u22120.5\n0\n0.5\n1\n|\n| ||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n| |||\n| ||\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n| |\n|\n|\n|\n||\n|\n|\n|\n| |\n||\n|\n||\n|\n|\n|\n|\n|\n|| |\n|\n||||\n|\n|\n|\n|\n|\n||\n|||\n|\n|\n| | |\n|\n|\n| |\n| |\n|\n|\n|\n|\n|\n| | |\n|||\n\u22122\n0\n2\n4\n6\n8\n\u22121\n\u22120.5\n0\n0.5\n1\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n| |\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n| |\n|\n|\n|\n|\n|\n|\n|\n|\n| |\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n| |\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n||\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\nJaakkola bound\nTaylor expansion\n\u0001\nFigure 1. Approximation error between ln \u03c3(\u03b8) and the two\ncontrol variates considered. The mean and variance of q\nused in these examples are (left) \u00b5 = 3, \u03c32 = 3 and (right)\n\u00b5 = \u22125, \u03c32 = 1. We show 100 samples from these q distri-\nbutions, at which points the functions would be evaluated\nfor the stochastic gradient (for a = 1). The Taylor expan-\nsion is closer to the true function at the region of interest\nas de\ufb01ned by q. The bene\ufb01t of this is that fewer samples\nwill be necessary to approximate the gradient.\nA lower bound control variate.\nThe lower bound\nfor fn developed by Jaakkola & Jordan (2000) is a\nuseful control variate for variational logistic regression.\nFor each pair (xn, yn), this bound takes an auxiliary\nparameter \u03ben > 0 and has the form\ngn(yn, xn; \u03b8, \u03ben) = ln \u03c3(\u03ben) + 1\n2(ynxT\nn\u03b8 \u2212\u03ben)\n\u2212\u03bb(\u03ben)((xT\nn\u03b8)2 \u2212\u03be2\nn).\n(15)\nWe have \u03bb(\u03ben) = (2\u03c3(\u03ben) \u22121)/(4\u03ben). We select this\nbound for illustrative purposes, but any lower bound\nwill work in principle. For a multivariate Gaussian q\ndistribution, having a quadratic term in g is essential\nfor stochastically learning a full covariance matrix. In\ngeneral, tighter bounds will require fewer samples, but\nfor some functions \ufb01nding tight bounds may require\nmuch e\ufb00ort. We next consider a general purpose con-\ntrol variate that can help in this case.\nControl variate delta method.\nWe also consider\nthe second-order Taylor expansion of f as a control\nvariate. The second-order Taylor expansion often ac-\ncurately approximates a function of interest, and when\nused alone is known as the delta method. In addition\nto accuracy, the quadratic approximation of the delta\nmethod results in a function for which the expectation\nwith respect to q is very likely to be analytic.\nThe delta method arguably should not be used for\nmean-\ufb01eld variational inference because the second-\norder Taylor expansion is not a lower bound. On the\nother hand, the \ufb01rst-order Taylor expansion often is a\nlower bound. Therefore, though their bounds are typ-\nically loose, \ufb01rst-order approximations are commonly\nemployed for MFVB. An advantage of the proposed\nstochastic search algorithm is that second-order meth-\nods can now be used as a control variate to (i) more\naccurately approximate the function of interest, and\n(ii) signi\ufb01cantly reduce the variance of the stochastic\ngradient. We call this approach of using Taylor expan-\nsion control variates the control variate delta method.\nWe consider a second-order Taylor expansion at \u02c6\u00b5,\nthe current mean of q, for approximating ln \u03c3(ynxT\nn\u03b8).\nLetting \u03c3n := \u03c3(ynxT\nn \u02c6\u00b5), this control variate is\ngn(yn, xn; \u03b8, \u02c6\u00b5) = ln \u03c3n + yn(1 \u2212\u03c3n)(\u03b8 \u2212\u02c6\u00b5)T xn\n(16)\n\u22121\n2\u03c3n(1 \u2212\u03c3n)(\u03b8 \u2212\u02c6\u00b5)T xnxT\nn(\u03b8 \u2212\u02c6\u00b5).\nAs with the Jaakkola & Jordan (2000) bound, this\ncontrol variate contains a quadratic term that helps in\nlearning the covariance matrix of q.\nWe compare these control variates in Figure 1. In these\nplots we show the di\ufb00erence fn \u2212gn for two speci\ufb01c q\ndistributions, and with x = 1. We also show 100 sam-\nples from q, which indicates the regions where these\nfunctions would be evaluated (for a = 1). The plots\nshow that the second-order Taylor expansion approxi-\nmates fn signi\ufb01cantly better where it matters; we sup-\nport this conclusion with the experiments in Sec. 6.\n5.2. Hierarchical Dirichlet processes\nWe also investigate a stochastic search VB algorithm\nfor an approximation to the hierarchical Dirichlet pro-\ncess (Teh et al., 2007).\nWe focus on the two-level\ngenerative structure using \ufb01nite dimensional Dirichlet\npriors as an approximation to the in\ufb01nite dimensional\nprocess\u2014in the limit the HDP is recovered. In this\n\ufb01nite process, a top-level Dirichlet-distributed proba-\nbility vector \u03b8 parameterizes the Dirichlet distribution\nfor d = 1, . . . , D second-level probability vectors,\n(\u03c0d1, . . . , \u03c0dK)\niid\n\u223cDirichlet(\u03b2\u03b81, . . . , \u03b2\u03b8K),\n(\u03b81, . . . , \u03b8K) \u223cDirichlet( \u03b1\nK , . . . , \u03b1\nK ).\n(17)\nIn topic models, these \u03c0d vectors are often used as\ndistributions on word distributions.\nIn this section,\nwe focus solely on the generic hierarchical structure in\nEq. (17). We de\ufb01ne the approximate posterior of \u03b8 as\nq(\u03b8) = Dirichlet(c1, . . . , cK).\n(18)\nThe part of the lower bound associated with \u03b8 is\nL\u03b8 = P\nk \u03b2Eq[\u03b8k] P\nd Eq[ln \u03c0dk] \u2212P\nk DEq[ln \u0393(\u03b2\u03b8k)]\n+ Eq[ln p(\u03b8) \u2212ln q(\u03b8)].\n(19)\nThe expectation Eq[ln \u0393(\u03b2\u03b8k)] is intractable for each\nk. We use a stochastic approximation, and introduce\ntwo control variates for this function, depending on\nthe current expected value of \u03b2\u03b8k.\nVariational Bayesian Inference with Stochastic Search\n0\n1\n2\n3\n4\n5\n6\n7\n\u22128\n\u22126\n\u22124\n\u22122\n0\n2\n0\n0.5\n1\n1.5\n2\n2.5\n\u22121.5\n\u22121\n\u22120.5\n0\n0.5\n-ln\u0001\u0002\u0003\u0004\u0005\n-ln\u0001\u0002\u0003\u0004\u0005-ln\u0003\u0004\n\u0003\u0004\nFigure 2. (left) The intractable function in the HDP.\n(right) the di\ufb00erence after introducing a control variate and\nsetting a = 1. Since \u2212ln \u0393(\u03b2\u03b8)\u2212ln \u03b2\u03b8 = \u2212ln \u0393(\u03b2\u03b8+1), the\nright \ufb01gure is the left \ufb01gure shifted by one unit to the left\nand truncated at zero. The very large variance near zero\n(where most values of \u03b2\u03b8 will lie) has been signi\ufb01cantly re-\nduced. For larger values of \u03b2\u03b8, we use a \ufb01rst-order Taylor\napproximation at \u03b2Eq\u03b8 of the nearly linear function.\nAs \u03b2\u03b8k approaches zero, the function fk(\u03b2\u03b8k)\n=\n\u2212ln \u0393(\u03b2\u03b8k) diverges to \u2212\u221e. By construction of the\nDirichlet prior, many values of \u03b8k will be very small.\n(In the in\ufb01nite limit, there are an in\ufb01nite number of\nsuch values smaller than any \u03b4 > 0.) The variance in\nthis region is massive\u2014when computer precision be-\ncomes an issue it can be in\ufb01nite (see Figure 2).\nWe propose the control variate gk(\u03b2\u03b8k) = ln \u03b2\u03b8k, with\nEq[ln \u03b8k] = \u03c6(ck)\u2212\u03c6(P\nj cj) where \u03c6(\u00b7) is the digamma\nfunction. This control variate not only correlates well\nwith fk, but if a = 1, lim\u03b2\u03b8k\u21920 fk \u2212agk = 0, as shown\nin Figure 2. This results from the equality\n\u2212ln \u0393(\u03b2\u03b8k) \u2212ln \u03b2\u03b8k = \u2212ln \u0393(\u03b2\u03b8k + 1).\n(20)\nFor all other values of a, this equality does not hold,\nand the di\ufb00erence fk \u2212agk diverges as \u03b2\u03b8k \u21920. For\nthis model, we can thus give the optimal value of a in\nadvance, and we set a = 1.\nFrom Figure 2, we also see that the approximation gets\nworse when \u03b2\u03b8k gets large, which can occur for a few\nhighly probable dimensions when \u03b2 is large. Since fk\nis approximately linear in this regime, we use a \ufb01rst-\norder Taylor expansion of fk about the mean \u00af\u03b8k =\nEq[\u03b8k] as a control variate. This gives the following\ntwo control variates,\ngk = ln \u03b2\u03b8k,\n0 < \u03b2\u00af\u03b8k < \u03ba1,\n(21)\ngk = \u2212ln \u0393(\u03b2\u00af\u03b8k) \u2212\u03b2(\u03b8k \u2212\u00af\u03b8k)\u03c6(\u03b2\u00af\u03b8k),\n\u03b2\u00af\u03b8k > \u03ba2.\nSince fk is concave, this second control variate is an\nupper bound on L\u03b8 without the stochastic correction.\nWe discuss the boundaries \u03ba1 and \u03ba2 in Sec. 6.\nThus far, we\u2019ve focused mainly on reducing the vari-\nance induced by fk, but in Sec. 4.2 we noted that\n\u2207ln q introduces variance to the Monte Carlo integral\nas well. This suggests that we should look at other\nparts of the integral for potential variance reduction.\nWe brie\ufb02y show how this can be done for the HDP.\nThe lower bound in Eq. (19) contains a sum of K in-\ntractable integrals over the probability simplex \u2206K.\nWe perform separate stochastic approximations of\neach gradient. Using the fact that each gamma func-\ntion is over a single dimension of the simplex, for a\nfunction of \u03b8k the variables \u03b8i\u0338=k will integrate out. In\nthis case, marginalizing a Dirichlet distribution to a\nsingle dimension yields a beta distribution. That is,\nZ\n\u03b8\u2208\u2206K\nln \u0393(\u03b2\u03b8k)q(\u03b8|c)d\u03b8 =\nZ 1\n0\nln \u0393(\u03b2\u03b8k)q\u2032\nk(\u03b8k|c)d\u03b8k,\nwhere q\u2032\nk(\u03b8k|c) = Beta(\u03b8k|ck, P\ni\u0338=k ci).\nWe can choose which of these integrals to stochas-\ntically approximate for gradient ascent.\nHowever,\nthe stochastic gradient using q\u2032\nk results in signi\ufb01-\ncantly less variance than for q since \u03b8(s)\nk\nwill be near\nzero; the vector \u2207c ln q\u2032\nk has K \u22121 entries containing\nln(1 \u2212\u03b8(s)\nk ) \u2212Eq[ln(1 \u2212\u03b8k)], while these values will be\nln \u03b8(s)\ni\n\u2212Eq[ln \u03b8i] for i = 1, . . . , K when using \u2207c ln q.\n6. Experiments\nWe perform experiments using stochastic search VB\nfor binary classi\ufb01cation with logistic regression and for\ntopic modeling with the approximate HDP. We next\ngive the details of the experiments we perform and the\ndata sets and algorithms used for comparison.\nData and set-up.\nFor logistic regression, we use\n\ufb01ve data sets from the UCI repository: Iris, Pima,\nSPECTF, Voting and WDBC. These data sets range\nfrom 150 to 768 labeled examples living in 5 to 45 di-\nmensions, including a dimension of all ones to account\nfor o\ufb00set.\nWe perform experiments with stochastic\nsearch variational inference using the two control vari-\nates discussed in Sec. 5.1. We compare with two ad-\nditional methods for posterior approximation: varia-\ntional inference with the Jaakkola & Jordan (2000)\nbound and Laplace\u2019s method.\nWe evaluate perfor-\nmance on the true variational objective function in Eq.\n(14) using each posterior approximation.\nFor the HDP topic model, we use 8,000 documents\nwith 3,012 vocabulary size from The New York Times.\nWe compare with (i) a point estimate of the top-level\nprobability vector using a delta q distribution, and (ii)\n\ufb01xing the top-level distribution to the uniform vector,\nwhich is equivalent to LDA (Blei et al., 2003).\nWe\nperform experiments for di\ufb00erent corpus sizes, di\ufb00er-\nent values of \u03b2, and we set K = 200.\nVariational Bayesian Inference with Stochastic Search\n0\n50\n100\n150\n200\n0\n0.05\n0.1\n0.15\n0\n50\n100\n150\n200\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n# samples\nvariance reduction\nSPECTF\nVOTE\nWDBC\nTaylor CV\nJ&J Bound CV\nIteration number\na^\n0\n50\n100\n150\n200\n0\n5\n10\n15\nx10-3\n0\n50\n100\n150\n200\n0.7\n0.8\n0.9\n1\n1.1\n0\n50\n100\n150\n200\n0\n0.05\n0.1\n0.15\n0.2\n0\n50\n100\n150\n200\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n0\n50\n100\n150\n200\n10\n2\n10\n4\n10\n6\n10\n8\n0\n50\n100\n150\n200\n10\n2\n10\n4\n10\n6\n10\n8\n0\n50\n100\n150\n200\n10\n2\n10\n4\n10\n6\n10\n8\nFigure 3. Experimental results for variational logistic regression. We compare the variance reduction obtained by the two\ncontrol variates under consideration. (top row) The number of samples per iteration setting \u03f5 = 0.1 in Algorithm 1. The\nyellow and black lines represent the estimated number that would be required without variance reduction according to\neach control variate. As expected, these curves overlap. (middle row) The variance reduction factor of Eq. (11). The\nselected control variates signi\ufb01cantly reduce the variance. The second-order Taylor control variate is signi\ufb01cantly better\nthan the lower bound. (bottom row) The optimal scaling factor estimated from samples.\nTable 1. Optimizing the variational objective function.\nThe stochastic search methods (indicated by CV) signif-\nicantly outperform the other methods toward this end. All\nvalues were calculated for the true lower bound in Eq. (14)\nusing their respective posterior approximations.\nmodel|data\niris\npima\nspectf\nvote\nwdbc\nTaylor CV\n-7.9\n-3974\n-165\n-67.8\n-74.6\nJ&J CV\n-7.9\n-3974\n-165\n-67.6\n-74.8\nLaplace\n-11.9\n-3985\n-170\n-70.5\n-80.0\nJ&J bnd\n-11.5\n-3976\n-173\n-74.6\n-86.2\nTable 2. Running time of each algorithm on each data set.\nWe use the approximated number of samples required with-\nout a control variate to estimate the last value. The times\nare given in milliseconds (ms), seconds (s), minutes (m),\nhours (hr) and years (yr).\nmodel|data\niris\npima\nspectf\nvote\nwdbc\nTaylor CV\n0.33s\n1.7m\n20s\n17s\n11s\nJ&J CV\n0.42s\n18m\n1.2m\n1.2m\n2.3m\nLaplace\n21ms\n29ms\n94ms\n20ms\n0.10s\nJ&J bnd\n64ms\n88ms\n0.13s\n97ms\n0.15s\nSS no CV\n2.4s\n>2yr\n6.6hr\n9hr\n1.4hr\nLogistic regression results.\nIn Table 1 we show\nthe variational lower bound for each model on each\ndata set. Since all algorithms return an approxima-\ntion of the posterior distribution on the vector \u03b8, this\ncomparison is meaningful and gives a measure of how\nclose each posterior is to the true posterior. We see\na considerable improvement for the stochastic algo-\nrithms (denoted by their control variate). Since both\nstochastic algorithms optimize the same objective, the\nperformance should be the same.\nWe show performance details of the stochastic search\nVB algorithm in Figure 3 and Table 2. In Figure 3,\nwe show the number of samples, the variance reduction\nfactor and the scaling \u02c6a as a function of iteration. We\nsee that the control variates provide a major reduction\nin variance. Also, the Taylor expansion control vari-\nate (i.e., control variate delta method) requires signif-\nicantly fewer samples than the bound control variate,\nwhich bene\ufb01ts the running time (see Table 2). While\nthe non-sampling methods are faster, control variates\nmake stochastic search VB a viable inference method\nwhen compared to the base algorithm of Sec. 3.\nVariational Bayesian Inference with Stochastic Search\nTable 3. The fraction of times that algorithm \u27e8row\u27e9was\nranked \u27e8column\u27e9for the 32 di\ufb00erent parameter/data size\npairs using the variational lower bound.\nmodel | rank\n1st\n2nd\n3rd\nHDP-stochastic\n0.66\n0.31\n0.03\nHDP-point\n0.34\n0.66\n0\nLDA\n0\n0.03\n0.97\n\u0001\nc ln q(\u0002|c)\n\u0001\nc ln qk' (\u0002|c)\nnumber of docs (x1000)\n# samples\n\u0003\u0004\u0005\u0004\u0006\n1\n2\n3\n4\n5\n6\n7\n8\n10\n1\n10\n2\n10\n3\n10\n4\n10\n5\n1\n2\n3\n4\n5\n6\n7\n8\n10\n2\n10\n3\n10\n4\n10\n5\n\u0003\u0004\u0005\u0004\u0007\nFigure 4. Average number of samples per iteration for the\ntwo equivalent gradient approximations, \u2207c ln q vs \u2207c ln q\u2032\nk,\nwhere q is the Dirichlet and q\u2032\nk the beta distribution. Sam-\npling is further reduced (see text for discussion).\nHierarchical Dirichlet process results.\nWe \ufb01t\ntopic models to The New York Times using di\ufb00erent\nnumbers of documents (D = 1000 to 8000) and concen-\ntration parameter values \u03b2 \u2208{1, 5, 10, 15}. As switch\npoints for the two control variates, we set \u03ba1 = 1 and\n\u03ba2 = 2.\nWe summarize our results in Table 3.\nIn\ngeneral, \ufb01tting a variational posterior on the top-level\nDirichlet vector yielded a better posterior approxima-\ntion than a point estimate and a \u03b8 \ufb01xed as uniform.\nHowever, this improvement was not as dramatic as for\nlogistic regression.\nIn Figure 4, we show the number of samples required\nfrom the Dirichlet q distribution to approximate the\nstochastic integral. We compare the two methods dis-\ncussed in Sec. 5.2 for reducing the variance of the\nstochastic vector \u2207c ln q by instead using \u2207c ln q\u2032\nk. We\nsee a signi\ufb01cant reduction in the number of samples.\nExperiments without control variates were not possi-\nble due to computer precision issues and the massive\nvariance of ln \u0393(\u03b2\u03b8) near zero.\n7. Conclusion\nWe have presented stochastic search variational Bayes,\na method for optimizing intractable variational ob-\njective functions such as those arising from non-\nconjugacy.\nThe algorithm relies on a stochastic ap-\nproximation of the gradient; we showed how control\nvariates can signi\ufb01cantly reduce the variance of this\nMonte Carlo integral. Since existing lower bounds can\nbe recast as control variates, our approach is relevant\nto many existing MFVB algorithms. However, a lack\nof restrictions on control variates allows for other types\nof function approximations when a good bound is not\nreadily available. We introduced the control variate\ndelta method toward this end.\nAcknowledgements\nJ.P. and M.J. are supported by\nONR grant number N00014-11-1-0688 under the MURI\nprogram.\nD.B. is supported by ONR N00014-11-1-0651,\nNSF CAREER 0745520, AFOSR FA9550-09-1-0668, the\nAlfred P. Sloan foundation, and a grant from Google.\nReferences\nBeal,\nM.J.\nVariational Algorithms for Approximate\nBayesian Inference. PhD thesis, Gatsby Computational\nNeuroscience Unit, University College London, 2003.\nBlei, D. and Jordan, M. Variational inference for Dirichlet\nprocess mixtures. Bayesian Analysis, 1:121\u2013144, 2006.\nBlei, D., Ng, A., and Jordan, M. Latent Dirichlet alloca-\ntion. Journal of Machine Learning Research, 3:993\u20131022,\n2003.\nGraves, A. Practical variational inference for neural net-\nworks. In Neural Information Processing Systems, 2011.\nHo\ufb00man, M., Blei, D., and Bach, F. Online learning for\nlatent Dirichlet allocation. In Neural Information Pro-\ncessing Systems, 2010.\nJaakkola, T. and Jordan, M.I. Bayesian parameter estima-\ntion via variational methods. Statistics and Computing,\n10:25\u201337, 2000.\nJordan, M.I., Ghahramani, Z., Jaakkola, T., and Saul,\nL.K. An introduction to variational methods for graph-\nical models. Machine Learning, 37:183\u2013233, 1999.\nKnowles, D.A. and Minka, T.P. Non-conjugate variational\nmessage passing for multinomial and binary regression.\nIn Neural Information Processing Systems, 2011.\nLeisink, M.A.R. and Kappen, H.J.\nA tighter bound for\ngraphical models.\nNeural Computation, 13(9):2149\u2013\n2171, 2001.\nMarlin, B., Khan, E., and Murphy, K. Piecewise bounds\nfor estimating Bernoulli-logistic latent Gaussian models.\nIn International Conference on Machine Learning, 2011.\nPaisley, J. and Carin, L.\nNonparametric factor analysis\nwith beta process priors.\nIn International Conference\non Machine Learning, 2009.\nRoss, S.M. Simulation. Academic Press, San Diego, 4th\nedition, 2006.\nTeh, Y., Jordan, M., Beal, M., and Blei, D. Hierarchical\nDirichlet processes. Journal of the American Statistical\nAssociation, 101(476):1566\u20131581, 2007.\nWang, C., Paisley, J., and Blei, D. Online variational infer-\nence for the hierarchical Dirichlet process. In Arti\ufb01cial\nIntelligence and Statistics, 2011.\nYi, S., Wierstra, D., Schaul, T., and Schmidhuber, J.\nStochastic search using the natural gradient. In Inter-\nnational Conference on Machine Learning, 2009.\n",
        "sentence": " inference networks [8], the reweighted wake-sleep algorithm [9], and control variates [10, 11]. Past work using similar gradient updates has found significant benefit from the use of control variates, or reward baselines, to reduce the variance [17, 10, 3, 11, 2]. Choosing effective control variates for the stochastic gradient estimators amounts to finding a function that is highly correlated with the gradient vectors, and whose expectation is known or tractable to compute [10, 24].",
        "context": "tive and results in a new algorithm based on stochastic\noptimization. Graves (2011) considers a similar prob-\nlem for neural networks, but a lack of control variates\nlimits the algorithm to signi\ufb01cantly simpler variational\napproximations.\nstochastic search direction.\nWe introduce a control variate (Ross, 2006) to reduce\nthe variance of the stochastic gradient constructed in\nEq. (6). A control variate is a random variable that\nis highly correlated with an intractable variable, but\nproximation of the gradient; we showed how control\nvariates can signi\ufb01cantly reduce the variance of this\nMonte Carlo integral. Since existing lower bounds can\nbe recast as control variates, our approach is relevant"
    },
    {
        "title": "Neural variational inference and learning in belief networks",
        "author": [
            "A. Mnih",
            "K. Gregor"
        ],
        "venue": "International Conference on Machine Learning,",
        "citeRegEx": "11",
        "shortCiteRegEx": null,
        "year": 2014,
        "abstract": "Highly expressive directed latent variable models, such as sigmoid belief\nnetworks, are difficult to train on large datasets because exact inference in\nthem is intractable and none of the approximate inference methods that have\nbeen applied to them scale well. We propose a fast non-iterative approximate\ninference method that uses a feedforward network to implement efficient exact\nsampling from the variational posterior. The model and this inference network\nare trained jointly by maximizing a variational lower bound on the\nlog-likelihood. Although the naive estimator of the inference model gradient is\ntoo high-variance to be useful, we make it practical by applying several\nstraightforward model-independent variance reduction techniques. Applying our\napproach to training sigmoid belief networks and deep autoregressive networks,\nwe show that it outperforms the wake-sleep algorithm on MNIST and achieves\nstate-of-the-art results on the Reuters RCV1 document dataset.",
        "full_text": "arXiv:1402.0030v2  [cs.LG]  4 Jun 2014\nNeural Variational Inference and Learning in Belief Networks\nAndriy Mnih\nAMNIH@GOOGLE.COM\nKarol Gregor\nKAROLG@GOOGLE.COM\nGoogle DeepMind\nAbstract\nHighly expressive directed latent variable mod-\nels, such as sigmoid belief networks, are dif\ufb01-\ncult to train on large datasets because exact in-\nference in them is intractable and none of the\napproximate inference methods that have been\napplied to them scale well. We propose a fast\nnon-iterative approximate inference method that\nuses a feedforward network to implement ef\ufb01-\ncient exact sampling from the variational poste-\nrior. The model and this inference network are\ntrained jointly by maximizing a variational lower\nbound on the log-likelihood. Although the naive\nestimator of the inference network gradient is too\nhigh-variance to be useful, we make it practi-\ncal by applying several straightforward model-\nindependent variance reduction techniques. Ap-\nplying our approach to training sigmoid belief\nnetworks and deep autoregressive networks, we\nshow that it outperforms the wake-sleep algo-\nrithm on MNIST and achieves state-of-the-art re-\nsults on the Reuters RCV1 document dataset.\n1. Introduction\nCompared to powerful globally-normalized latent variable\nmodels, such as deep belief networks (Hinton et al., 2006)\nand deep Boltzmann machines (Salakhutdinov & Hinton,\n2009a), which can now be trained on fairly large datasets,\ntheir purely directed counterparts have been left behind due\nto the lack of ef\ufb01cient learning algorithms. This is unfor-\ntunate, because their modularity and ability to generate ob-\nservations ef\ufb01ciently make them better suited for integra-\ntion into larger systems.\nTraining highly expressive directed latent variable mod-\nels on large datasets is a challenging problem due to the\ndif\ufb01culties posed by inference. Although the generality\nProceedings of the 31 st International Conference on Machine\nLearning, Beijing, China, 2014. JMLR: W&CP volume 32. Copy-\nright 2014 by the author(s).\nof Markov Chain Monte Carlo (MCMC) methods makes\nthem straightforward to apply to models of this type (Neal,\n1992), they tend to suffer from slow mixing and are usually\ntoo computationally expensive to be practical in all but the\nsimplest models. Such methods are also dif\ufb01cult to scale to\nlarge datasets because they need to store the current state of\nthe latent variables for all the training observations between\nparameter updates.\nVariational methods (Jordan et al., 1999) provide an\noptimization-based\nalternative\nto\nthe\nsampling-based\nMonte Carlo methods, and tend to be more ef\ufb01cient. They\ninvolve approximating the exact posterior using a distribu-\ntion from a more tractable family, often a fully factored\none, by maximizing a variational lower bound on the log-\nlikelihood w.r.t. the parameters of the distribution. For a\nsmall class of models, using such variational posteriors al-\nlows the expectations that specify the parameter updates to\nbe computed analytically. However, for highly expressive\nmodels such as the ones we are interested in, these expecta-\ntions are intractable even with the simplest variational pos-\nteriors. This dif\ufb01culty is usually dealt with by lower bound-\ning the intractable expectations with tractable one by intro-\nducing more variational parameters, as was done for sig-\nmoid belief nets by Saul et al. (1996). However, this tech-\nnique increases the gap between the bound being optimized\nand the log-likelihood, potentially resulting in a poorer \ufb01t\nto the data. In general, variational methods tend to be more\nmodel-dependent than sampling-based methods, often re-\nquiring non-trivial model-speci\ufb01c derivations.\nWe propose a new approach to training directed graphi-\ncal models that combines the advantages of the sampling-\nbased and variational methods. Its central idea is using a\nfeedforward network to implement ef\ufb01cient exact sampling\nfrom the variational posterior for the given observation. We\ntrain this inference network jointly with the model by max-\nimizing the variational lower bound on the log-likelihood,\nestimating all the required gradients using samples from\nthe inference network. Although naive estimate of the gra-\ndient for the inference network parameters is unusable due\nto its high variance, we make the approach practical by ap-\nplying several straightforward and general variance reduc-\nNeural Variational Inference and Learning in Belief Networks\ntion techniques. The resulting training procedure for the\ninference network can be seen as an instance of the RE-\nINFORCE algorithm (Williams, 1992).\nDue to our use\nof stochastic feedforward networks for performing infer-\nence we call our approach Neural Variational Inference and\nLearning (NVIL).\nCompared to MCMC methods, where many iterations over\nthe latent variables are required to generate a sample from\nthe exact posterior and successive samples tend to be highly\ncorrelated, NVIL does not suffer from mixing issues as\neach forward pass through the inference network generates\nan independent exact sample from the variational posterior.\nIn addition to being much faster than MCMC, our approach\nhas the additional advantage of not needing to store the\nlatent variables for each observation and thus is not only\nmore memory ef\ufb01cient but also applicable to the pure on-\nline learning setting, where each training case is seen once\nbefore being discarded.\nIn contrast to other work on scaling up variational infer-\nence, NVIL can handle both discrete and continuous latent\nvariables (unlike Kingma & Welling (2013); Rezende et al.\n(2014)) as well variational posteriors with complex depen-\ndency structures (unlike Ranganath et al. (2013)). More-\nover, the variance reduction methods we employ are sim-\nple and model-independent, unlike the more sophisticated\nmodel-speci\ufb01c control variates of Paisley et al. (2012).\nThough the idea of training an inference model by\nfollowing the gradient of the variational bound has\nbeen considered before, it was dismissed as infeasible\n(Dayan & Hinton, 1996). Our primary contribution is to\nshow how to reduce the variance of the naive gradient es-\ntimator to make it practical without narrowing its range of\napplicability. We also show that the resulting method trains\nsigmoid belief networks better than the wake-sleep algo-\nrithm (Hinton et al., 1995), which is the only algorithm we\nare aware of that is capable of training the same range of\nmodels ef\ufb01ciently. Finally, we demonstrate the effective-\nness and scalability of NVIL by using it to achieve state-\nof-the-art results on the Reuters RCV1 document dataset.\n2. Neural variational inference and learning\n2.1. Variational objective\nSuppose we are interested in training a latent variable\nmodel P\u03b8(x, h) with parameters \u03b8. We assume that ex-\nact inference in the model is intractable and thus maximum\nlikelihood learning is not an option. For simplicity, we will\nalso assume that all the latent variables in the model are dis-\ncrete, though essentially the same approach applies if some\nor all of the variables are continuous.\nWe will train the model by maximizing a variational\nlower bound on the marginal log-likelihood.\nFollowing\nthe standard variational inference approach (Jordan et al.,\n1999), given an observation x, we introduce a distribution\nQ\u03c6(h|x) with parameters \u03c6, which will serve as an approx-\nimation to its exact posterior P\u03b8(h|x). The variational pos-\nterior Q will have a simpler form than the exact posterior\nand thus will be easier to work with.\nThe contribution of x to the log-likelihood can then be\nlower-bounded as follows (Jordan et al., 1999):\nlog P\u03b8(x) = log\nX\nh\nP\u03b8(x, h)\n\u2265\nX\nh\nQ\u03c6(h|x) log P\u03b8(x, h)\nQ\u03c6(h|x)\n= EQ[log P\u03b8(x, h) \u2212log Q\u03c6(h|x)]\n(1)\n= L(x, \u03b8, \u03c6).\nBy rewriting the bound as\nL(x, \u03b8, \u03c6) = log P\u03b8(x) \u2212KL(Q\u03c6(h|x), P\u03b8(h|x)),\n(2)\nwe see that its tightness is determined by the Kullback-\nLeibler (KL) divergence between the variational distribu-\ntion and the exact posterior. Maximizing the bound with\nrespect to the parameters \u03c6 of the variational distribution\nmakes the distribution a better approximation to the poste-\nrior (w.r.t. the KL-divergence) and tightens the bound.\nIn contrast to most applications of variational inference\nwhere the variational posterior for each observation is de-\n\ufb01ned using its own set of variational parameters, our ap-\nproach does not use any local variational parameters. In-\nstead, we use a \ufb02exible feedforward model to compute the\nvariational distribution from the observation. We call the\nmodel mapping x to Q\u03c6(h|x) the inference network. The\narchitecture of the inference network is constrained only by\nthe requirement that Q\u03c6(h|x) it de\ufb01nes has to be ef\ufb01cient\nto evaluate and sample from. Using samples from the infer-\nence network we will be able to compute gradient estimates\nfor the model and inference network parameters for a large\nclass of highly expressive architectures, without having to\ndeal with architecture-speci\ufb01c approximations.\nGiven a training set D,\nconsisting of observations\nx1, ..., xD, we train the model by (locally) maximizing\nL(D, \u03b8, \u03c6) = P\ni L(xi, \u03b8, \u03c6) using gradient ascent w.r.t. to\nthe model and inference network parameters. To ensure\nscalability to large datasets, we will perform stochastic op-\ntimization by estimating gradients on small minibatches of\nrandomly sampled training cases.\n2.2. Parameter gradients\nThe gradient of the variational bound for a single observa-\ntion x w.r.t. to the model parameters is straightforward to\nNeural Variational Inference and Learning in Belief Networks\nderive and has the form\n\u2207\u03b8L(x) = EQ [\u2207\u03b8 log P\u03b8(x, h)] ,\n(3)\nwhere we left \u03b8 and \u03c6 off the list of the arguments of L to\nsimplify the notation. The corresponding gradient w.r.t. to\nthe inference network parameters is somewhat more in-\nvolved:\n\u2207\u03c6L(x) = EQ[(log P\u03b8(x, h) \u2212log Q\u03c6(h|x))\n\u00d7 \u2207\u03c6 log Q\u03c6(h|x)],\n(4)\nWe give its derivation in the supplementary material.\nAs both gradients involve expectations which are in-\ntractable in all but a handful of special cases, we will es-\ntimate them with Monte Carlo integration, using samples\nfrom the inference network. Having generated n samples\nh(1), ..., h(n) from Q\u03c6(h|x), we compute\n\u2207\u03b8L(x) \u22481\nn\nn\nX\ni=1\n\u2207\u03b8 log P\u03b8(x, h(i))\n(5)\nand\n\u2207\u03c6L(x) \u22481\nn\nn\nX\ni=1\n(log P\u03b8(x, h(i)) \u2212log Q\u03c6(h(i)|x))\n\u00d7 \u2207\u03c6 log Q\u03c6(h(i)|x).\n(6)\nThe above gradient estimators are unbiased and thus can be\nused to perform stochastic maximization of the variational\nobjective using a suitable learning rate annealing schedule.\nThe speed of convergence of this procedure, however, de-\npends heavily on the variance of the estimators used, as we\nwill see in Section 4.2.\nThe model gradient estimator (5) is well-behaved and does\nnot pose a problem. The variance of the inference network\ngradient estimator (6), however, can be very high due to\nthe scaling of the gradient inside the expectation by a po-\ntentially large term. As a result, learning variational param-\neters with updates based on this estimator can be unaccept-\nably slow. In fact, it is widely believed that learning vari-\national parameters using gradient estimators of the form\n(6) is infeasible (Hinton & Zemel, 1994; Dayan & Hinton,\n1996; Kingma & Welling, 2013). In the next section we\nwill show how to make this approach practical by applying\nvariance reduction techniques.\n2.3. Variance reduction techniques\nThough gradient estimates computed using Eq. 6 are usu-\nally too noisy to be useful in practice, it is easy to reduce\ntheir variance to a manageable level with the following\nmodel-independent techniques.\n2.3.1. CENTERING THE LEARNING SIGNAL\nInspecting Eq. 4, we see that we are using\nl\u03c6(x, h) = log P\u03b8(x, h) \u2212log Q\u03c6(h|x)\n(7)\nas the learning signal for the inference network parameters,\nand thus are effectively \ufb01tting log Q\u03c6(h|x) to log P\u03b8(x, h).\nThis might seem surprising, given that we want the in-\nference network Q\u03c6(h|x) to approximate the posterior\ndistribution P\u03b8(x|h), as opposed to the joint distribution\nP\u03b8(x, h). It turns out however that using the joint instead of\nthe posterior distribution in Eq. 4 does not affect the value\nof the expectation. To see that we start by noting that\nEQ[\u2207\u03c6 log Q\u03c6(h|x)] = EQ\n\u0014\u2207\u03c6Q\u03c6(h|x)\nQ\u03c6(h|x)\n\u0015\n= \u2207\u03c6EQ[1] = 0.\n(8)\nTherefore we can subtract any c that does not depend on h\nfrom the learning signal in Eq. 4 without affecting the value\nof the expectation:\nEQ[(l\u03c6(x, h) \u2212c)\u2207\u03c6 log Q\u03c6(h|x)]\n= EQ[l\u03c6(x, h)\u2207\u03c6 log Q\u03c6(h|x)] \u2212cEQ[\u2207\u03c6 log Q\u03c6(h|x)]\n= EQ[l\u03c6(x, h)\u2207\u03c6 log Q\u03c6(h|x)].\n(9)\nAnd as log P\u03b8(x, h)\n=\nlog P\u03b8(h|x) + log P\u03b8(x) and\nlog P\u03b8(x) does not depend on h, using P\u03b8(h|x) in Eq. 4\nin place of P\u03b8(x, h) does not affect the value of the expec-\ntation.\nThis equivalence allows us to compute the learning sig-\nnal ef\ufb01ciently, without having to evaluate the intractable\nP\u03b8(h|x) term.\nThe price we pay for this tractability is\nthe much higher variance of the estimates computed us-\ning Eq. 6. Fortunately, Eq. 9 suggests that we can reduce\nthe variance by subtracting a carefully chosen c from the\nlearning signal. The simplest option is to make c a pa-\nrameter and adapt it as learning progresses. However, c\nwill not be able capture the systematic differences in the\nlearning signal for different observations x, which arise in\npart due to the presence of the log P\u03b8(x) term. Thus we\ncan reduce the gradient variance further by subtracting an\nobservation-dependent term C\u03c8(x) to minimize those dif-\nferences. Doing this does not affect the expected value of\nthe gradient estimator because C\u03c8(x) does not depend on\nthe latent variables. Borrowing a name from the reinforce-\nment learning literature we will refer to c and C\u03c8(x) as\nbaselines. We will elaborate on this connection in Sec-\ntion 3.4.\nWe implement the input-dependent baseline C\u03c8(x) using a\nneural network and train it to minimize the expected square\nof the centered learning signal EQ[(l\u03c6(x, h)\u2212C\u03c8(x)\u2212c)2].\nThough this approach to \ufb01tting the baseline does not re-\nsult in the maximal variance reduction, it is simpler and in\nNeural Variational Inference and Learning in Belief Networks\nour experience works as well as the optimal approach of\nWeaver & Tao (2001) which requires taking into account\nthe magnitude of the gradient of the inference network pa-\nrameters. We also experimented with per-parameter base-\nlines but found that they did not improve on the global ones.\nFinally, we note that incorporating baselines into the learn-\ning signal can be seen as using simple control variates.\nIn contrast to the more elaborate control variates (e.g. of\nPaisley et al. (2012)), baselines do not depend on the form\nof the model or of the variational distribution and thus are\neasier to use.\n2.3.2. VARIANCE NORMALIZATION\nEven after centering, using l\u03c6(x, h) as the learning signal\nis non-trivial as its average magnitude can change dramat-\nically, and not necessarily monotonically, as training pro-\ngresses. This variability makes training an inference net-\nwork using a \ufb01xed learning rate dif\ufb01cult. We address this\nissue by dividing the centered learning signal by a running\nestimate of its standard deviation. This normalization en-\nsures that the signal is approximately unit variance, and can\nbe seen as a simple and ef\ufb01cient way of adapting the learn-\ning rate. To ensure that we stop learning when the magni-\ntude of the signal approaches zero, we apply variance nor-\nmalization only when the estimate of the standard devia-\ntion is greater than 1. The algorithm for computing NVIL\nparameter updates using the variance reduction techniques\ndescribed so far is provided in the supplementary material.\n2.3.3. LOCAL LEARNING SIGNALS\nSo far we made no assumptions about the structure of the\nmodel or the inference network. However, by taking advan-\ntage of their conditional independence properties we can\ntrain the inference network using simpler and less noisy lo-\ncal learning signals instead of the monolithic global learn-\ning signal l\u03c6(x, h). Our approach to deriving a local signal\nfor a set of parameters involves removing all the terms from\nthe global signal that do not affect the value of the resulting\ngradient estimator.\nWe will derive the layer-speci\ufb01c learning signals for the\ncommon case of both the model and the inference network\nhaving n layers of latent variables. The model and the vari-\national posterior distributions then naturally factor as\nP\u03b8(x, h) =P\u03b8(x|h1)\nYn\u22121\ni=1 P\u03b8(hi|hi+1)P\u03b8(hn),\n(10)\nQ\u03c6(h|x) =Q\u03c61(h1|x)\nYn\u22121\ni=1 Q\u03c6i+1(hi+1|hi),\n(11)\nwhere hi denotes the latent variables in the ith layer and \u03c6i\nthe parameters of the variational distribution for that layer.\nWe will also use hi:j to denote the latent variables in layers\ni through j.\nTo learn the parameters of the the variational distribution\nfor layer i , we need to compute the following gradient:\n\u2207\u03c6iL(x) = EQ(h|x)[l\u03c6(x, h)\u2207\u03c6i log Q\u03c6i(hi|hi\u22121)].\nUsing the law of iterated expectation we can rewrite the\nexpectation w.r.t. Q(h|x) as\n\u2207\u03c6iL(x) = EQ(h1:i\u22121|x)[\nEQ(hi:n|hi\u22121)[l\u03c6(x, h)\u2207\u03c6i log Q\u03c6i(hi|hi\u22121)]|hi\u22121]],\nwhere we also used the fact that under the variational pos-\nterior, hi:n is independent of h1:i\u22122 and x, given hi\u22121. As\na consequence of Eq. 9, when computing the expectation\nw.r.t. Q(hi:n|hi\u22121), all the terms in the learning signal that\ndo not depend on hi:n can be safely dropped without af-\nfecting the result. This gives us the following local learning\nsignal for layer i:\nli\n\u03c6(x, h) = log P\u03b8(hi\u22121:n) \u2212log Q\u03c6(hi:n|hi\u22121).\n(12)\nTo get the signal for the \ufb01rst hidden layer we simply use x\nin place of h0, in which case we simply recover the global\nlearning signal. For hidden layers i > 1, however, the local\nsignal involves fewer terms than l\u03c6(x, h) and thus can be\nexpected to be less noisy. As we do not assume any within-\nlayer structure, Eq. 12 applies to models and inference net-\nworks whether or not Q\u03c6(hi|hi\u22121) and P\u03b8(hi|hi+1) are\nfactorial.\nSince local signals can be signi\ufb01cantly different from each\nother, we use separate baselines and variance estimates for\neach signal. For layers i > 1, the input-dependent baseline\nC\u03c8(x) is replaced by Ci\n\u03c8i(hi\u22121).\nIn some cases, further simpli\ufb01cation of the learning signal\nis possible, yielding a different signal per latent variable.\nWe leave exploring this as future work.\n3. Related work\n3.1. Feedforward approximations to inference\nThe idea of training an approximate inference network\nby optimizing a variational lower bound is not new.\nIt\ngoes back at least to Hinton & Zemel (1994), who derived\nthe variational objective from the Minimum Description\nLength (MDL) perspective and used it to train linear au-\ntoencoders. Their probabilistic encoder and decoder cor-\nrespond to our inference network and model respectively.\nHowever, they computed the gradients analytically, which\nwas possible due to the simplicity of their model, and dis-\nmissed the sampling-based approach as infeasible due to\nnoise.\nSalakhutdinov & Larochelle (2010) proposed using a feed-\nforward \u201crecognition\u201d model to perform ef\ufb01cient input-\ndependent initialization for the mean \ufb01eld inference algo-\nrithm in deep Boltzmann machines. As the recognition\nNeural Variational Inference and Learning in Belief Networks\nmodel is trained to match the marginal probabilities pro-\nduced by mean \ufb01eld inference, it inherits the limitations\nof the inference procedure, such as the inability to model\nstructured posteriors. In contrast, in NVIL the inference\nnet is trained to match the true posterior directly, without\ninvolving an approximate inference algorithm, and thus the\naccuracy of the \ufb01t is limited only by the expressiveness of\nthe inference network itself.\nRecently a method for training nonlinear models with\ncontinuous latent variables,\ncalled Stochastic Gradi-\nent Variational Bayes (SGVB), has been proposed by\nKingma & Welling (2013) and Rezende et al. (2014). Like\nNVIL, it involves using feedforward models to perform\napproximate inference and trains them by optimizing a\nsampling-based estimate of the variational bound on the\nlog-likelihood. However, SGVB is considerably less gen-\neral than NVIL, because it uses a gradient estimator ob-\ntained by taking advantage of special properties of real-\nvalued random variables and thus is not applicable to\nmodels with discrete random variables. Moreover, unlike\nNVIL, SGVB method cannot handle inference networks\nwith nonlinear dependencies between latent variables. The\nideas of the two methods are complementary however, and\nNVIL is likely to bene\ufb01t from the SGVB-style treatment of\ncontinuous-valued variables, while SGVB might converge\nfaster using the variance reduction techniques we proposed.\nGregor et al. (2013) have recently proposed a related al-\ngorithm for training sigmoid belief network like models\nbased on the MDL framework. They also use a feedfor-\nward model to perform approximate inference, but concen-\ntrate on the case of a deterministic inference network and\ncan handle only binary latent variables. The inference net-\nwork is trained by backpropagating through binary thresh-\nolding units, ignoring the thresholding nonlinearities, to ap-\nproximately minimize the coding cost of the joint latent-\nvisible con\ufb01gurations. This approach can be seen as ap-\nproximately maximizing a looser variational lower bound\nthan (2) due to the absence of the entropy term.\nAn inference network for ef\ufb01cient generation of sam-\nples from the approximate posterior can also be seen as\na probabilistic generalization of the approximate feed-\nforward inference methods developed for sparse coding\nmodels in the last few years (Kavukcuoglu et al., 2008;\nBradley & Bagnell, 2008; Gregor & LeCun, 2010).\n3.2. Sampling-based variational inference\nLike NVIL, Black Box Variational Inference (BBVI,\nRanganath et al., 2013) learns the variational parameters\nof the posterior by optimizing the variational bound using\nsampling-based gradient estimates, which makes it appli-\ncable to a large range of models. However, unlike NVIL,\nBBVI follows the traditional approach of learning a sepa-\nrate set of variational parameters for each observation and\ndoes not use an inference network. Moreover, BBVI uses a\nfully-factorized mean \ufb01eld approximation to the posterior,\nwhich limits its power.\n3.3. The wake-sleep algorithm\nNVIL shares many similarities with the wake-sleep algo-\nrithm (Hinton et al., 1995), which enjoys the same scala-\nbility and applicability to a wide range of models. This\nalgorithm was introduced for training Helmholtz machines\n(Dayan et al., 1995), which are multi-layer belief networks\naugmented with recognition networks. These recognition\nnetworks are used for approximate inference and are di-\nrectly analogous to NVIL inference networks. Wake-sleep\nalternates between updating the model parameters in the\nwake phase and the recognition network parameters in the\nsleep phase. The model parameter update is based on the\nsamples generated from the recognition network on the\ntraining data and is identical to the NVIL one (Eq. 5). How-\never, in contrast to NVIL, the recognition network param-\neters are learned from samples generated by the model. In\nother words, the recognition network is trained to recover\nthe hidden causes corresponding to the samples from the\nmodel distribution by following the gradient\n\u2207\u03c6L(x) = EP\u03b8(x,h) [\u2207\u03c6 log Q\u03c6(h|x)] .\n(13)\nUnfortunately, this update does not optimize the same ob-\njective as the model parameter update, which means that\nthe wake-sleep algorithm does not optimize a well-de\ufb01ned\nobjective function and is not guaranteed to converge. This\nis the algorithm\u2019s main weakness, compared to NVIL,\nwhich optimizes a variational lower bound on the log-\nlikelihood.\nThe wake-sleep gradient for recognition network parame-\nters does have the advantage of being much easier to es-\ntimate than the corresponding gradient of the variational\nbound. In fact, the idea of training the recognition net-\nworks using the gradient of the bound was mentioned in\n(Hinton & Zemel, 1994) and (Dayan & Hinton, 1996) but\nnot seriously considered due concerns about the high vari-\nance of the estimates. In Section 4.2 we show that while\nthe naive estimator of the gradient given in Eq. 6 does ex-\nhibit high variance, the variance reduction techniques from\nSection 2.3 improve it dramatically and make it practical.\n3.4. REINFORCE\nUsing the gradient (4) to train the inference network can\nbe seen as an instance of the REINFORCE algorithm\n(Williams, 1992) from reinforcement learning (RL), which\nadapts the parameters of a stochastic model to maximize\nthe external reward signal which depends on the model\u2019s\noutput. Given a model P\u03b8(x) and a reward signal r(x),\nNeural Variational Inference and Learning in Belief Networks\nREINFORCE updates the model parameters using the rule\n\u2206\u03b8 \u221dEP [(r(x) \u2212b)\u2207\u03b8 log P\u03b8(x)].\n(14)\nWe can view NVIL as an application of REINFORCE on\nthe per-training-case basis, with the inference network cor-\nresponding to the stochastic model, latent state h to the out-\nput, and the learning signal l\u03c6(x, h) to the reward. The term\nb in Eq. 14, called a baseline in the RL literature, is a hy-\nperparameter that can be adapted to reduce the variance of\nthe parameter update. Thus it serves the same function as\nc and C\u03c8(x) that we subtract from the learning signal to\ncenter it in Section 2.3.1. The considerable body of work\non baselines and other variance reduction methods done in\nthe RL community (e.g. Greensmith et al., 2004) is likely\nto contain additional techniques relevant for training infer-\nence networks.\n4. Experimental results\nWe performed two sets of experiments, with the \ufb01rst set\nintended to evaluate the effectiveness of our variance re-\nduction techniques and to compare NVIL\u2019s performance to\nthat of the wake-sleep algorithm. In the second set of ex-\nperiments, we demonstrate NVIL\u2019s ability to handle larger\nreal-world datasets by using it to train generative models of\ndocuments.\n4.1. Experimental protocol\nWe trained all models using stochastic gradient ascent us-\ning minibatches of 20 observations sampled randomly from\nthe training data. The gradient estimates were computed\nusing a single sample from the inference network. For each\ndataset, we created a validation set by removing a random\nsubset of 100 observations from the training set. The only\nform of regularization we used was early stopping based on\nthe validation bound, implemented by keeping track of the\nparameter con\ufb01guration with the best validation score seen\nso far. We implemented each input-dependent baseline us-\ning a neural network with a single hidden layer of 100 tanh\nunits.\nWe used \ufb01xed learning rates because we found them to\nproduce superior results to the annealing schedules we ex-\nperimented with. The learning rates we report were se-\nlected based on the validation set performance in prelim-\ninary experiments with smaller models. We always make\nthe learning rate for inference network \ufb01ve times smaller\nthan for the model (which is the one we report), as we found\nthis to improve performance. We used inference networks\nwith layered structure given by Eq. 11, without dependen-\ncies within each layer except in the experiment with au-\ntoregressive inference networks. All multi-layer inference\nnetworks were trained using layer-speci\ufb01c learning signals\nfrom Section 2.3.3.\nAs the models we train are intractable, we cannot compute\nthe exact log-likelihoods for them. Instead we report the\nestimates of the variational bound (2) computed using 10\nsamples from the inference network, which we found to be\nsuf\ufb01cient to get the accurate bound estimates. We expect\nthis approach to underestimate the log-likelihood consider-\nably, but leave \ufb01nding more direct and thus less pessimistic\nevaluation methods as future work.\n4.2. Modelling images of digits\nOur \ufb01rst set of experiments was performed on the binarized\nversion of the MNIST dataset, which has become the stan-\ndard benchmark for evaluating generative models of binary\ndata. The dataset consists of 70,000 28 \u00d7 28 binary im-\nages of handwritten digits, partitioned into a 60,000-image\ntraining set and 10,000-image test set. We used the bina-\nrization of Salakhutdinov & Murray (2008), which makes\nour scores directly comparable to those in the literature.\nWe used 3 \u00d7 10\u22124 as the learning rate for training mod-\nels with NVIL on this dataset. Centering the input vectors\nby subtracting the mean vector was essential for making\nthe inference networks and input-dependent baselines work\nwell.\nTo demonstrate the importance of variance reduction tech-\nniques, we trained two SBNs using a range of variance con-\ntrol settings. The \ufb01rst SBN had a single layer of 200 latent\nvariables, while the second one had two layers of 200 vari-\nables each. Figure 1 shows the estimate of the variational\nobjective on the validation set plotted against the number\nof parameter updates. For both models, it is clear that us-\ning all three techniques \u2013 the input-dependent and input-\nindependent baselines along with variance normalization \u2013\nis essential for best performance. However, of the three\ntechniques, the input-dependent baseline appears to be the\nleast important. Comparing the plots for the two models\nsuggests that variance reduction becomes more important\nfor larger models, with the gap between the best combina-\ntion and the others (excluding the very worst one) widen-\ning. For both models, learning with all three variance re-\nduction techniques disabled makes barely any progress and\nis clearly infeasible.\nWe found that disabling layer-speci\ufb01c learning signals had\nlittle effect on the performance of the resulting model. The\ndifference was about 0.4 nats for an SBN with two or three\nlayers of latent variables.\nWe next compared NVIL to the wake-sleep algorithm,\nwhich is its closest competitor in terms of scalability and\nbreadth of applicability, by training a range of models us-\ning both algorithms. Wake-sleep training used a learning\nrate of 1 \u00d7 10\u22124, as we found this algorithm to be more\nsensitive to the choice of the learning rate than NVIL,\nNeural Variational Inference and Learning in Belief Networks\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600\n1800\n2000\n\u2212240\n\u2212220\n\u2212200\n\u2212180\n\u2212160\n\u2212140\n\u2212120\n\u2212100\nSBN 200\nNumber of parameter updates\nValidation set bound\n \n \nBaseline, IDB, & VN\nBaseline & VN\nBaseline only\nVN only\nNo baselines & no VN\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600\n1800\n2000\n\u2212240\n\u2212220\n\u2212200\n\u2212180\n\u2212160\n\u2212140\n\u2212120\n\u2212100\nSBN 200\u2212200\nNumber of parameter updates\nValidation set bound\n \n \nBaseline, IDB, & VN\nBaseline & VN\nBaseline only\nVN only\nNo baselines & no VN\nFigure 1. Bounds on the validation set log-likelihood for an SBN with (Left) one and (Right) two layers of 200 latent variables. Baseline\nand IDB refer to the input-independent and the input-dependent baselines respectively. VN is variance normalization.\nTable 1. Results on the binarized MNIST dataset. \u201cDim\u201d is the\nnumber of latent variables in each layer, starting with the deepest\none. NVIL and WS refer to the models trained with NVIL and\nwake-sleep respectively. NLL is the negative log-likelihood for\nthe tractable models and an estimate of it for the intractable ones.\nMODEL\nDIM\nTEST NLL\nNVIL\nWS\nSBN\n200\n113.1\n120.8\nSBN\n500\n112.8\n121.4\nSBN\n200-200\n99.8\n107.7\nSBN\n200-200-200\n96.7\n102.2\nSBN\n200-200-500\n97.0\n102.3\nFDARN\n200\n92.5\n95.9\nFDARN\n500\n90.7\n97.2\nFDARN\n400\n96.3\nDARN\n400\n93.0\nNADE\n500\n88.9\nRBM (CD3)\n500\n105.5\nRBM (CD25)\n500\n86.3\nMOB\n500\n137.6\nperforming considerably better with lower learning rates.\nThe results, along with some baselines from the litera-\nture, are shown in Table 1. We report only the means of\nthe bound estimates as their standard deviations were all\nvery small, none exceeding 0.1 nat. We can see that mod-\nels trained with NVIL have considerably better bounds on\nthe log-likelihood, compared to their wake-sleep counter-\nparts, with the difference ranging from 3.4 to 8.6 nats. Ad-\nditional layers make SBNs perform better, independently\nof the training method. Interestingly, single-layer fDARN\n(Gregor et al., 2013) models, which have autoregressive\nconnections between the latent variables, perform better\nthan any of the SBN models trained using the same al-\ngorithm.\nComparing to results from the literature, we\nsee that all the SBN and fDARN models we trained per-\nform much better than a mixture of 500 factorial Bernoulli\ndistributions (MoB) but not as well as the determinis-\ntic Neural Autoregressive Distribution Estimator (NADE)\n(Larochelle & Murray, 2011). The NVIL-trained fDARN\nmodels with 200 and 500 latent variables also outperform\nthe fDARN (as well as the more expressive DARN) model\nwith 400 latent variables from (Gregor et al., 2013), which\nwere trained using an MDL-based algorithm. The fDARN\nand multi-layer SBN models trained using NVIL also out-\nperform a 500-hidden-unit RBM trained with 3-step con-\ntrastive divergence (CD), but not the one trained with 25-\nstep CD (Salakhutdinov & Murray, 2008). However, both\nsampling and CD-25 training in an RBM is considerably\nmore expensive than sampling or NVIL training for any of\nour models.\nThe sampling-based approach to computing gradients al-\nlows NVIL to handle variational posteriors with complex\ndependencies.\nTo demonstrate this ability, we retrained\nseveral of the SBN models using inference networks with\nautoregressive connections within each layer. These net-\nworks can capture the dependencies between variables\nwithin layers and thus are considerably more expressive\nthan the ones with factorial layers. Results in Table 2 in-\ndicate that using inference networks with autoregressive\nconnections produces better models, with the single-layer\nmodels exhibiting large gains.\n4.3. Document modelling\nWe also applied NVIL to the more practical task of docu-\nment modelling. The goal is to train a generative model\nof documents which are represented as vectors of word\ncounts, also known as bags of words. We trained two sim-\nNeural Variational Inference and Learning in Belief Networks\nTable 2. The effect of using autoregressive connections in the in-\nference network. \u201cDim\u201d is the number of latent variables in each\nlayer, starting with the deepest one. \u201cTest NLL\u201d is an estimate\nof the lower bound on the log-likelihood on the MNIST test set.\n\u201dAutoreg\u201d and \u201cFactorial\u201d refer to using inference networks with\nand without autoregressive connections respectively.\nMODEL\nDIM\nTEST NLL\nAUTOREG\nFACTORIAL\nSBN\n200\n103.8\n113.1\nSBN\n500\n104.4\n112.8\nSBN\n200-200-200\n94.5\n96.7\nSBN\n200-200-500\n96.0\n97.0\nple models on the 20 Newsgroups and Reuters Corpus Vol-\nume I (RCV1-v2) datasets, which have been used to eval-\nuate similar models in (Salakhutdinov & Hinton, 2009b;\nLarochelle & Lauly, 2012).\n20 Newsgroups is a fairly\nsmall dataset of Usenet newsgroup posts, consisting of\nabout 11K training and 7.5K test documents.\nRCV1 is\na much larger dataset of Reuters newswire articles, with\nabout 794.4K training and 10K test documents. We use\nthe standard preprocessed versions of the datasets from\nSalakhutdinov & Hinton (2009b), which have vocabularies\nof 2K and 10K words respectively.\nWe experimented with two simple document models, based\non the SBN and DARN architectures. Both models had a\nsingle layer of latent variables and a multinomial visible\nlayer and can be seen as directed counterparts of the Repli-\ncated Softmax model (Salakhutdinov & Hinton, 2009b).\nWe used the same training procedure as on MNIST with\nthe exception of the learning rates which were 3 \u00d7 10\u22125 on\n20 Newsgroups and 10\u22123 on RCV1.\nThe\nestablished\nevaluation\nmetric\nfor\nsuch\nmod-\nels\nis\nthe\nperplexity\nper\nword,\ncomputed\nas\nexp\n\u0010\n\u22121\nN\nP\nn\n1\nLn log P(xn)\n\u0011\n,\nwhere N\nis the num-\nber of documents, Ln is the length of document n, and\nP(xn) the probability of the document under the model.\nAs we cannot compute log P(xn), we use the variational\nlower bound in its place and thus report an upper bound on\nperplexity.\nThe\nresults\nfor our\nmodels,\nalong\nwith\nones\nfor\nthe\nReplicated\nSoftmax\nand\nDocNADE\nmod-\nels\nfrom\n(Salakhutdinov & Hinton,\n2009b)\nand\n(Larochelle & Lauly,\n2012)\nrespectively,\nare\nshown\nin Table 3. We can see that the SBN and fDARN models\nwith 50 latent variables perform well, producing better\nscores than LDA and Replicated Softmax on both datasets.\nTheir performance is also competitive with that of Doc-\nNADE on 20 Newsgroups. The score of 724 for fDARN\nwith 50 latent variables on RCV1 is already better than\nDocNADE\u2019s 742, the best published result on that dataset.\nTable 3. Document modelling results. \u201cDim\u201d is the number of\nlatent variables in the model. The third and the fourth columns\nreport the estimated test set perplexity on the 20 Newsgroups and\nReuters RCV1 datasets respectively.\nMODEL\nDIM\n20 NEWS\nREUTERS\nSBN\n50\n909\n784\nFDARN\n50\n917\n724\nFDARN\n200\n598\nLDA\n50\n1091\n1437\nLDA\n200\n1058\n1142\nREPSOFTMAX\n50\n953\n988\nDOCNADE\n50\n896\n742\nfDARN with 200 hidden units, however, performs even\nbetter, setting a new record with 598.\n5. Discussion and future work\nWe developed, NVIL, a new training method for intractable\ndirected latent variable models which is general and easy to\napply to new models. We showed that NVIL consistently\noutperforms the wake-sleep algorithm at training sigmoid-\nbelief-network-like models. Finally, we demonstrated the\npotential of our approach by achieving state-of-the-art re-\nsults on a sizable dataset of documents (Reuters RCV1).\nAs the emphasis of this paper is on the training method, we\napplied it to some of the simplest possible model and in-\nference network architectures, which was suf\ufb01cient to ob-\ntain promising results. We believe that considerable perfor-\nmance gains can be made by using more expressive archi-\ntectures, such as those with nonlinearities between layers of\nstochastic variables. Applying NVIL to models with con-\ntinuous latent variables is another promising direction since\nbinary latent variables are not always appropriate.\nWe expect NVIL to be also applicable to training condi-\ntional latent variable models for modelling the distribu-\ntion of observations given some context, which would re-\nquire making the inference network take both the context\nand the observation as input. This would make it an al-\nternative to the importance-sampling training method of\nTang & Salakhutdinov (2013) for conditional models with\nstructured high-dimensional outputs.\nWe hope that the generality and \ufb02exibility of our approach\nwill make it easier to apply powerful directed latent vari-\nable models to real-world problems.\nACKNOWLEDGEMENTS\nWe thank Koray Kavukcuoglu, Volodymyr Mnih, and\nNicolas Heess for their helpful comments. We thank Rus-\nlan Salakhutdinov for providing us with the preprocessed\ndocument datasets.\nNeural Variational Inference and Learning in Belief Networks\nReferences\nBradley, David M and Bagnell, J Andrew.\nDifferential\nsparse coding. In Advances in Neural Information Pro-\ncessing Systems, volume 20, 2008.\nDayan, Peter and Hinton, Geoffrey E.\nVarieties of\nhelmholtz machine. Neural Networks, 9(8):1385\u20131403,\n1996.\nDayan, Peter, Hinton, Geoffrey E, Neal, Radford M, and\nZemel, Richard S. The helmholtz machine. Neural com-\nputation, 7(5):889\u2013904, 1995.\nGreensmith, Evan, Bartlett, Peter L., and Baxter, Jonathan.\nVariance reduction techniques for gradient estimates in\nreinforcement learning. Journal of Machine Learning\nResearch, 5:1471\u20131530, 2004.\nGregor, Karol and LeCun, Yann. Learning fast approxima-\ntions of sparse coding. In Proc. International Conference\non Machine learning (ICML\u201910), 2010.\nGregor, Karol, Mnih, Andriy, and Wierstra, Daan. Deep au-\ntoregressive networks. arXiv preprint arXiv:1310.8499,\n2013.\nHinton, Geoffrey E and Zemel, Richard S. Autoencoders,\nminimum description length, and Helmholtz free energy.\nIn Advances in Neural Information Processing Systems,\n1994.\nHinton, Geoffrey E, Dayan, Peter, Frey, Brendan J, and\nNeal, Radford M. The \"wake-sleep\" algorithm for un-\nsupervised neural networks. Science, 268(5214):1158\u2013\n1161, 1995.\nHinton, Geoffrey E., Osindero, Simon, and Teh, Yee Whye.\nA fast learning algorithm for deep belief nets. Neural\nComputation, 18(7):1527\u20131554, 2006.\nJordan,\nMichael I.,\nGhahramani,\nZoubin,\nJaakkola,\nTommi S., and Saul, Lawrence K.\nAn introduction\nto variational methods for graphical models. Machine\nLearning, 37(2):183\u2013233, 1999.\nKavukcuoglu, Koray, Ranzato, Marc\u2019Aurelio, and LeCun,\nYann. Fast inference in sparse coding algorithms with\napplications to object recognition.\nTechnical report,\nCourant Institute, NYU, 2008.\nKingma, Diederik P and Welling, Max.\nAuto-encoding\nvariational bayes.\narXiv preprint arXiv:1312.6114,\n2013.\nLarochelle, Hugo and Lauly, Stanislas. A neural autore-\ngressive topic model. In Advances in Neural Information\nProcessing Systems, pp. 2717\u20132725, 2012.\nLarochelle, Hugo and Murray, Iain. The neural autoregres-\nsive distribution estimator.\nJMLR: W&CP, 15:29\u201337,\n2011.\nNeal, Radford M.\nConnectionist learning of belief net-\nworks. Arti\ufb01cial intelligence, 56(1):71\u2013113, 1992.\nPaisley, John William, Blei, David M., and Jordan,\nMichael I. Variational bayesian inference with stochastic\nsearch. In ICML, 2012.\nRanganath, Rajesh, Gerrish, Sean, and Blei, David M.\nBlack box variational inference.\narXiv preprint\narXiv:1401.0118, 2013.\nRezende, Danilo Jimenez, Mohamed, Shakir, and Wierstra,\nDaan. Stochastic back-propagation and variational in-\nference in deep latent gaussian models. arXiv preprint\narXiv:1401.4082, 2014.\nSalakhutdinov, Ruslan and Hinton, Geoffrey E. Deep boltz-\nmann machines. In International Conference on Arti\ufb01-\ncial Intelligence and Statistics, pp. 448\u2013455, 2009a.\nSalakhutdinov, Ruslan and Hinton, Geoffrey E. Replicated\nsoftmax: an undirected topic model. In Advances in neu-\nral information processing systems, 2009b.\nSalakhutdinov, Ruslan and Larochelle, Hugo.\nEf\ufb01cient\nlearning of deep boltzmann machines. In International\nConference on Arti\ufb01cial Intelligence and Statistics, pp.\n693\u2013700, 2010.\nSalakhutdinov, Ruslan and Murray, Iain. On the quantita-\ntive analysis of Deep Belief Networks. In Proceedings\nof the 25th Annual International Conference on Machine\nLearning (ICML 2008), 2008.\nSaul,\nLawrence K.,\nJaakkola,\nTommi,\nand Jordan,\nMichael I.\nMean \ufb01eld theory for sigmoid belief net-\nworks. Journal of Arti\ufb01cial Intelligence Research, 4:61\u2013\n76, 1996.\nTang, Yichuan and Salakhutdinov, Ruslan.\nLearning\nstochastic feedforward neural networks. In Advances in\nNeural Information Processing Systems, 2013.\nWeaver, Lex and Tao, Nigel. The optimal reward baseline\nfor gradient-based reinforcement learning.\nIn In Pro-\nceedings of the Seventeenth Conference on Uncertainty\nin Arti\ufb01cial Intelligence, 2001.\nWilliams, Ronald J. Simple statistical gradient-following\nalgorithms for connectionist reinforcement learning.\nMachine learning, 8(3-4):229\u2013256, 1992.\nNeural Variational Inference and Learning in Belief Networks\nA. Algorithm for computing NVIL gradients\nAlgorithm 1 provides an outline of our implementation of\nNVIL gradient computation for a minibatch of n randomly\nchosen training cases. The exponential smoothing factor \u03b1\nused for updating the estimates of the mean c and variance\nv of the inference network learning signal was set to 0.8 in\nour experiments.\nAlgorithm 1 Compute gradient estimates for the model and\nthe inference network\n\u2206\u03b8 \u21900, \u2206\u03c6 \u21900, \u2206\u03c8 \u21900\nL \u21900\n{Compute the learning signal and the bound}\nfor i \u21901 to n do\nxi \u2190random training case\n{Sample from the inference model}\nhi \u223cQ\u03c6(hi|xi)\n{Compute the unnormalized learning signal}\nli \u2190log P\u03b8(xi, hi) \u2212log Q\u03c6(hi|xi)\n{Add the case contribution to the bound}\nL \u2190L + li\n{Subtract the input-dependent baseline}\nli \u2190li \u2212C\u03c8(xi)\nend for\n{Update the learning signal statistics}\ncb \u2190mean(l1, ..., ln)\nvb \u2190variance(l1, ..., ln)\nc \u2190\u03b1c + (1 \u2212\u03b1)cb\nv \u2190\u03b1v + (1 \u2212\u03b1)vb\nfor i \u21901 to n do\nli \u2190\nli\u2212c\nmax(1,\u221av)\n{Accumulate the model parameter gradient}\n\u2206\u03b8 \u2190\u2206\u03b8 + \u2207\u03b8 log P\u03b8(xi, hi)\n{Accumulate the inference net gradient}\n\u2206\u03c6 \u2190\u2206\u03c6 + li\u2207\u03c6 log Q\u03c6(hi|xi)\n{Accumulate the input-dependent baseline gradient}\n\u2206\u03c8 \u2190\u2206\u03c8 + li\u2207\u03c8C\u03c8(xi)\nend for\nB. Inference network gradient derivation\nDifferentiating the variational lower bound w.r.t. to the in-\nference network parameters gives\n\u2207\u03c6L(x) =\u2207\u03c6EQ[log P\u03b8(x, h) \u2212log Q\u03c6(h|x)]\n=\u2207\u03c6\nX\nh\nQ\u03c6(h|x) log P\u03b8(x, h)\u2212\n\u2207\u03c6\nX\nh\nQ\u03c6(h|x) log Q\u03c6(h|x)\n=\nX\nh\nlog P\u03b8(x, h)\u2207\u03c6Q\u03c6(h|x)\u2212\nX\nh\n(log Q\u03c6(h|x) + 1) \u2207\u03c6Q\u03c6(h|x)\n=\nX\nh\n(log P\u03b8(x, h) \u2212log Q\u03c6(h|x)) \u2207\u03c6Q\u03c6(h|x),\nwhere\nwe\nused\nthe\nfact\nthat\nP\nh \u2207\u03c6Q\u03c6(h|x)\n=\n\u2207\u03c6\nP\nh Q\u03c6(h|x)\n=\n\u2207\u03c61\n=\n0.\nUsing the identity\n\u2207\u03c6Q\u03c6(h|x) = Q\u03c6(h|x)\u2207\u03c6 log Q\u03c6(h|x), then gives\n\u2207\u03c6L(x) =\nX\nh\n(log P\u03b8(x, h) \u2212log Q\u03c6(h|x))\n\u00d7 Q\u03c6(h|x)\u2207\u03c6 log Q\u03c6(h|x)\n=EQ [(log P\u03b8(x, h) \u2212log Q\u03c6(h|x)) \u2207\u03c6 log Q\u03c6(h|x)] .\n",
        "sentence": " inference networks [8], the reweighted wake-sleep algorithm [9], and control variates [10, 11]. Neural variational inference and learning (NVIL) [11] trains both networks to maximize a variational lower bound on the log-likelihood. Past work using similar gradient updates has found significant benefit from the use of control variates, or reward baselines, to reduce the variance [17, 10, 3, 11, 2]. Following the analogy with reinforcement learning highlighted by [11], these control variates can also be viewed as reward baselines:",
        "context": "Neural Variational Inference and Learning in Belief Networks\nA. Algorithm for computing NVIL gradients\nAlgorithm 1 provides an outline of our implementation of\nNVIL gradient computation for a minibatch of n randomly\nness and scalability of NVIL by using it to achieve state-\nof-the-art results on the Reuters RCV1 document dataset.\n2. Neural variational inference and learning\n2.1. Variational objective\nSuppose we are interested in training a latent variable\nLike NVIL, Black Box Variational Inference (BBVI,\nRanganath et al., 2013) learns the variational parameters\nof the posterior by optimizing the variational bound using\nsampling-based gradient estimates, which makes it appli-"
    },
    {
        "title": "Learning to combine foveal glimpses with a third-order Boltzmann machine",
        "author": [
            "H. Larochelle",
            "G.E. Hinton"
        ],
        "venue": "Neural Information Processing Systems,",
        "citeRegEx": "12",
        "shortCiteRegEx": null,
        "year": 2010,
        "abstract": "",
        "full_text": "",
        "sentence": " Such models have been applied successfully in image classification [12, 4, 3, 2], object tracking [13, 3], machine translation [6], caption generation [5], and image generation [14, 15].",
        "context": null
    },
    {
        "title": "Learning where to attend with deep architectures for image tracking",
        "author": [
            "M. Denil",
            "L. Bazzani",
            "H. Larochelle",
            "N. de Freitas"
        ],
        "venue": "Neural Computation,",
        "citeRegEx": "13",
        "shortCiteRegEx": "13",
        "year": 2012,
        "abstract": "We discuss an attentional model for simultaneous object tracking and\nrecognition that is driven by gaze data. Motivated by theories of perception,\nthe model consists of two interacting pathways: identity and control, intended\nto mirror the what and where pathways in neuroscience models. The identity\npathway models object appearance and performs classification using deep\n(factored)-Restricted Boltzmann Machines. At each point in time the\nobservations consist of foveated images, with decaying resolution toward the\nperiphery of the gaze. The control pathway models the location, orientation,\nscale and speed of the attended object. The posterior distribution of these\nstates is estimated with particle filtering. Deeper in the control pathway, we\nencounter an attentional mechanism that learns to select gazes so as to\nminimize tracking uncertainty. Unlike in our previous work, we introduce gaze\nselection strategies which operate in the presence of partial information and\non a continuous action space. We show that a straightforward extension of the\nexisting approach to the partial information setting results in poor\nperformance, and we propose an alternative method based on modeling the reward\nsurface as a Gaussian Process. This approach gives good performance in the\npresence of partial information and allows us to expand the action space from a\nsmall, discrete set of fixation points to a continuous domain.",
        "full_text": "1\nLearning where to Attend with Deep\nArchitectures for Image Tracking\nMisha Denil1, Loris Bazzani2, Hugo Larochelle3 and Nando de Freitas1\n1University of British Columbia.\n2University of Verona.\n3University of Sherbrooke.\nKeywords: Restricted Boltzmann machines, Bayesian optimization, bandits, atten-\ntion, deep learning, particle \ufb01ltering, saliency\nAbstract\nWe discuss an attentional model for simultaneous object tracking and recognition that\nis driven by gaze data. Motivated by theories of perception, the model consists of\ntwo interacting pathways: identity and control, intended to mirror the what and where\npathways in neuroscience models. The identity pathway models object appearance and\nperforms classi\ufb01cation using deep (factored)-Restricted Boltzmann Machines. At each\npoint in time the observations consist of foveated images, with decaying resolution to-\nward the periphery of the gaze. The control pathway models the location, orientation,\nscale and speed of the attended object. The posterior distribution of these states is esti-\nmated with particle \ufb01ltering. Deeper in the control pathway, we encounter an attentional\nmechanism that learns to select gazes so as to minimize tracking uncertainty. Unlike in\nour previous work, we introduce gaze selection strategies which operate in the presence\narXiv:1109.3737v1  [cs.AI]  16 Sep 2011\nof partial information and on a continuous action space. We show that a straightforward\nextension of the existing approach to the partial information setting results in poor per-\nformance, and we propose an alternative method based on modeling the reward surface\nas a Gaussian Process. This approach gives good performance in the presence of par-\ntial information and allows us to expand the action space from a small, discrete set of\n\ufb01xation points to a continuous domain.\n1\nIntroduction\nHumans track and recognize objects effortlessly and ef\ufb01ciently, exploiting attentional\nmechanisms (Rensink, 2000; Colombo, 2001) to cope with the vast stream of data. We\nuse the human visual system as inspiration to build a system for simultaneous object\ntracking and recognition from gaze data. An attentional strategy is learned online to\nchoose \ufb01xation points which lead to low uncertainty in the location of the target ob-\nject. Our tracking system is composed of two interacting pathways. This separation\nof responsibility is a common feature in models from the computational neuroscience\nliterature as it is believed to re\ufb02ect a separation of information processing into ventral\nand dorsal pathways in the human brain (Olshausen et al., 1993a).\nThe identity pathway (ventral) is responsible for comparing observations of the\nscene to an object template using an appearance model, and on a higher level, for clas-\nsifying the target object. The identity pathway consists of a two hidden layer deep\nnetwork. The top layer corresponds to a multi-\ufb01xation Restricted Boltzmann Machine\n(RBM) (Larochelle & Hinton, 2010), as shown in Figure 1. It accumulates informa-\ntion from the \ufb01rst hidden layers at consecutive time steps. For the \ufb01rst layers, we use\n(factored)-RBMs (Hinton & Salakhutdinov, 2006; Ranzato & Hinton, 2010; Welling\net al., 2005; Swersky et al., 2011), but autoencoders (Vincent et al., 2008), sparse\ncoding (Olshausen & Field, 1996; Kavukcuoglu et al., 2009), two-layer ICA (K\u00a8oster\n& Hyv\u00a8arinen, 2007) and convolutional architectures (Lee et al., 2009) could also be\nadopted.\nThe control pathway (dorsal) is responsible for aligning the object template with the\nfull scene so the remaining modules can operate independently of the object\u2019s position\nand scale. This pathway is separated into a localization module and a \ufb01xation module\n2\nvt\nxt\nht\nat\nbt\nRt\nvt+1\nxt+1\nht+1\nat+1\nbt+1\nRt+1\nh\n[2]\nt+1\nPolicy \u03c0\nReward\nBelief state\nGaze observation\nFirst hidden layer\nState\nSecond hidden layer\nObject class\nAction\nHD Video\nTracking region\nct+1\nFigure 1: From a sequence of gazes (vt, vt+1, . . .), the model infers the hidden features h for\neach gaze (that is, the activation intensity of each receptive \ufb01eld), the hidden features for the\nfusion of the sequence of gazes and the object class c. Only one time step of classi\ufb01cation is kept\nin the \ufb01gure for clarity. The location, size, speed and orientation of the gaze patch are encoded\nin the state xt. The actions at follow a learned policy \u03c0t that depends on the past rewards\n{r1, . . . , rt\u22121}. This particular reward is a function of the belief state bt = p(xt|a1:t, h1:t), also\nknown as the \ufb01ltering distribution. Unlike typical commonly used partially observed Markov\ndecision models (POMDPs), the reward is a function of the beliefs. In this sense, the problem is\ncloser to one of sequential experimental design. With more layers in the ventral v\u2212h\u2212h[2] \u2212c\npathway, other rewards and policies could be designed to implement higher-level attentional\nstrategies.\n3\nwhich work cooperatively to accomplish this goal. The localization module is imple-\nmented with a particle \ufb01lter (Doucet et al., 2001) which estimates the location, velocity\nand scale of the target object. We make no attempt to implement such states with neural\narchitectures, but it seems clear that they could be encoded with grid cells (McNaughton\net al., 2006) and retinotopic maps as in V1 and the superior colliculus (Rosa, 2002; Gi-\nrard & Berthoz, 2005). The \ufb01xation module learns an attentional strategy to select\n\ufb01xation points relative to the object template. These \ufb01xation points are the centres of\npartial template observations, and are compared with observations of the corresponding\nlocations in the scene using the appearance model (see Figure 2). Reward is assigned\nto each \ufb01xation based on the uncertainty of the target location at each time step. The\n\ufb01xation module uses the reward signal to adapt its gaze selection policy to achieve good\nlocalization. Our previous work (Bazzani et al., 2010) used Hedge (Auer et al., 1998a;\nFreund & Schapire, 1997) to learn this policy. In this extended paper we show that a\nstraightforward adaptation of our previous approach to the partial information setting\nresults in poor performance, and we propose an alternative method based on modelling\nthe reward surface as a Gaussian Process. This approach gives good performance in the\npresence of partial information and allows us to expand the action space from a small,\ndiscrete set of \ufb01xation points to a continuous domain.\nThe proposed system can be motivated from different perspectives. First, starting\nwith Isard & Blake (1996), many particle \ufb01lters have been proposed for image tracking,\nbut these typically use simple observation models such as B-splines (Isard & Blake,\n1996) and colour templates (Okuma et al., 2004). RBMs are more expressive mod-\nels of shape, and hence, we conjecture that they will play a useful role where simple\nappearance models fail. Second, from a deep learning computational perspective, this\nwork allows us to tackle large images and video, which is typically not possible due\nto the number of parameters required to represent large images in deep models. The\nuse of \ufb01xations synchronized with information about the state (e.g. location and scale)\nof such \ufb01xations eliminates the need to look at the entire image or video. Third, the\nsystem is invariant to image transformations encoded in the state, such as location,\nscale and orientation. Fourth, from a dynamic sensor network perspective, this paper\npresents a very simple, but ef\ufb01cient and novel way of deciding how to gather measure-\nments dynamically. Lastly, in the context of psychology, the proposed model realizes\n4\nFigure 2: Left: A typical video frame with the estimated target region highlighted. To cope with\nthe large image size our system considers only the target region at each time step. Centre left:\nA close-up of the template extracted from the \ufb01rst frame. The template is compared to the target\nregion by selecting a \ufb01xation point for comparison as shown. Centre right: A visualization of a\nsingle \ufb01xation. In addition to covering only a very small portion of the original frame, the image\nis foveated with high resolution near the centre and low resolution on the periphery to further\nreduce the dimensionality. Right: The most active \ufb01lters of the \ufb01rst layer (factored)-RBM when\nobserving the displayed location. The control pathway compares these features to the features\nactive at the corresponding scene location in order to update the belief state.\nto some extent the functional architecture for dynamic scene representation of Rensink\n(2000). The rate at which different attentional mechanisms develop in newborns (in-\ncluding alertness, saccades and smooth pursuit, attention to object features and high-\nlevel task driven attention) guided the design of the proposed approach and was a great\nsource of inspiration (Colombo, 2001).\nOur attentional model can be seen as building a saliency map (Koch & Ullman,\n1985) over the target template. Previous work on saliency modelling has focused on\nidentifying salient points in an image using a bottom up process which looks for out-\nliers under some local feature model (which may include a task dependent prior, global\nscene features, or various other heuristics). These features can be computed from static\nimages (Torralba et al., 2006), or from local regions of spacetime (Gaborski et al., 2004)\nfor video. Additionally, a wide variety of different feature types have been applied to\nthis problem, including engineered features (Gao et al., 2007) as well as features that are\nlearned from data (Zhang et al., 2009). Core to these methods is the idea that saliency\nis determined by some type of novelty measure. Our approach is different, in that rather\nthan identifying locally or globally novel features, our process identi\ufb01es features which\nare useful for the task at hand. In our system the saliency signal for a location comes\nfrom a top down process which evaluates how well the features at that location enable\n5\nthe system to localize the target object. The work of Gao et al. (2007) considers a simi-\nlar approach to saliency by de\ufb01ning saliency to be the mutual information between the\nfeatures at a location and the class label of an object being sought; however, in order\nto make their model tractable the authors are forced to use speci\ufb01cally engineered fea-\ntures. Our system is able to cope with arbitrary feature types, and although we consider\nonly on localization in this paper, our model is suf\ufb01ciently general to be applied to\nidentifying salient features for other goals.\nRecently, a dynamic RBM state-space model was proposed in Taylor et al. (2010).\nBoth the implementation and intention behind that proposal are different from the ap-\nproach discussed here. To the best of our knowledge, our approach is the \ufb01rst successful\nattempt to combine dynamic state estimation from gazes with online policy learning for\ngaze adaptation, using deep network network models of appearance. Many other dual-\npathway architectures have been proposed in computational neuroscience, including Ol-\nshausen et al. (1993b) and Postma et al. (1997), but we believe ours has the advantage\nthat it is very simple, modular (with each module easily replaceable), suitable for large\ndatasets and easy to extend.\n2\nIdentity Pathway\nThe identity pathway in our model mirrors the ventral pathway in neuroscience models.\nIt is responsible for modelling the appearance of the target object and also, at a higher\nlevel, for classi\ufb01cation.\n2.1\nAppearance Model\nWe use (factored)-RBMs to model the appearance of objects and perform object classi-\n\ufb01cation using the gazes chosen by the control module (see Figure 3). These undirected\nprobabilistic graphical models are governed by a Boltzmann distribution over the gaze\ndata vt and the hidden features ht \u2208{0, 1}nh. We assume that the receptive \ufb01elds w,\nalso known as RBM weights or \ufb01lters, have been trained beforehand. We also assume\nthat readers are familiar with these models and, if otherwise, refer them to Ranzato &\nHinton (2010) and Swersky et al. (2010).\n6\nht\nvt\nVisual \ufb01eld\nW\nHidden activations\nFigure 3: An RBM senses a small foveated image derived from the video. The level of activation\nof each \ufb01lter is recorded in the ht units. The RBM weights (\ufb01lters) W are visualized in the upper\nleft. We currently pre-train these weights.\n2.2\nClassi\ufb01cation Model\nThe identity pathway also performs object recognition, classifying a sequence of gaze\ninstances selected with the gaze policy. We implement a multi-\ufb01xation RBM very sim-\nilar to the one proposed in Larochelle & Hinton (2010), where the binary variables zt\n(see Figure 4) are introduced to encode the relative gaze location at within the multi-\n\ufb01xation RBM (a \u201c1 in K\u201d or \u201cone hot\u201d encoding of the gaze location was used for\nzt).\nThe multi-\ufb01xation RBM uses the relative gaze location information in order to ag-\ngregate the \ufb01rst hidden layer representations ht at \u2206consecutive time steps into a single,\nhigher level representation h[2]\nt .\nMore speci\ufb01cally, the energy function of the multi-\ufb01xation RBM is:\nE(ht\u2212\u2206+1:t, zt\u2212\u2206+1:t, h[2]\nt ) = \u2212d\u22a4h[2]\nt \u2212\n\u2206\nX\ni=1\nb\u22a4ht\u2212\u2206+i +\nF\nX\nf=1\n(Pf,:h[2]\nt )(Wf,:ht\u2212\u2206+i)(Vf,:zt\u2212\u2206+i)\nwhere the notation Pf,: refers to the f th row vector of the matrix P. From this en-\nergy function, we de\ufb01ne a distribution over ht\u2212\u2206+1:t and h[2]\nt (conditioned on zt\u2212\u2206+1:t)\n7\nht+1\nvt+1\nzt+1\nat+1\nh\n[2]\nt+1\nClass label\nht\nvt\nzt\nAction\nVisual \ufb01eld\nat\nFigure 4: Gaze accumulation and classi\ufb01cation in the identity pathway. A multi-\ufb01xation RBM\nmodels the conditional distribution (given the gaze positions at) of \u2206consecutive hidden fea-\ntures ht, extracted by the \ufb01rst layer RBM on the foveated images. In this illustration, \u2206= 2.\nThe multi-\ufb01xation RBM encodes the gaze position at in a \u201cone hot\u201d representation noted zt.\nThe activation probabilities of the second layer hidden units h[2]\nt\nare used by a classi\ufb01er to\npredict the object\u2019s class.\nthrough the Boltzmann distribution:\np(ht\u2212\u2206+1:t, h[2]\nt |zt\u2212\u2206+1:t) = exp\n\u0010\n\u2212E(ht\u2212\u2206+1:t, zt\u2212\u2206+1:t, h[2]\nt )\n\u0011\n/Z(zt\u2212\u2206+1:t)\n(1)\nwhere the normalization constant Z(zt\u2212\u2206+1:t) ensures that Equation 1 sums to 1. To\nsample from this distribution, one can use Gibbs sampling by alternating between sam-\npling the top-most hidden layer h[2]\nt\ngiven all individual processed gazes ht\u2212\u2206+1:t and\nvice versa. To train the multi-\ufb01xation RBM, we collect a training set consisting in se-\nquences of \u2206pairs (ht, zt) by randomly selecting \u2206gaze positions at which to \ufb01xate\nand computing the associated ht. These sets are extracted from a collection of images\nin which the object to detect has been centred. Unsupervised learning using contrastive\ndivergence can then be performed on this training set. See Larochelle & Hinton (2010)\nfor more details.\nThe main difference between this multi-\ufb01xation RBM and the one described in\n8\nLarochelle & Hinton (2010) is that h[2]\nt\ndoes not explicitly model the class label ct.\nInstead, a multinomial logistic regression classi\ufb01er is trained separately, to predict ct\nfrom the aggregated representation extracted from h[2]\nt . More speci\ufb01cally, we use the\nvector of activation probabilities of all hidden units h[2]\nt,j in h[2]\nt , conditioned on ht\u2212\u2206+1:t\nand zt\u2212\u2206+1:t, as the aggregated representation:\np(h[2]\nt,j = 1|ht\u2212\u2206+1:t, zt\u2212\u2206+1:t) = sigm\n \ndj +\n\u2206\nX\ni=1\nF\nX\nf=1\nPf,j(Wf,:ht\u2212\u2206+i)(Vf,:zt\u2212\u2206+i)\n!\nWe experimented with a single \ufb01xation module, but found the multi-\ufb01xation module to\nincrease classi\ufb01cation accuracy. To improve the estimate the class variable ct over time,\nwe accumulate the classi\ufb01cation decisions at each time step.\nNote that the process of pursuit (tracking) is essential to classi\ufb01cation. As the target\nis tracked, the algorithm \ufb01xates at locations near the target\u2019s estimated location. The\nsize and orientation of these \ufb01xations also depends on the corresponding state estimates.\nNote that we don\u2019t \ufb01xate exactly at the target location estimate as this would provide\nonly one distinct \ufb01xation over several time steps if the tracking policy has converged\nto a speci\ufb01c gaze. It should also be pointed out that instead of using random \ufb01xations,\none could again use the control strategy proposed in this paper to decide where to look\nwith respect to the track estimate so as to reduce classi\ufb01cation uncertainty. We leave\nthe implementation of this extra attentional mechanism for future work.\n3\nControl Pathway\nThe control pathway mirrors the responsibility of the dorsal pathway in human visual\nprocessing. It tracks the state of the target (position, speed, etc) and normalizes the\ninput so that other modules need not account for these variations. At a higher level\nit is responsible for learning an attentional strategy which maximizes the amount of\ninformation learned with each \ufb01xation. The structure of the control pathway is shown\nin Figure 5.\n3.1\nState-space model\nThe standard approach to image tracking is based on the formulation of Markovian,\nnonlinear, non-Gaussian state-space models, which are solved with approximate Bayesian\n9\nState\nBelief\nReward\nAction\nbt\nxt\nht\nbt+1\nxt+1\nht+1\nPolicy\nat\nRt\nat+1\nRt+1\nFigure 5: In\ufb02uence diagram for the control pathway. The true state of the tacked object xt,\ngenerates some set of features ht, in the identity pathway. These features depend on the action\nchosen at time t and are used to update the belief state bt. Statistics of the belief state are\ncollected to compute the reward rt, which is used to update the policy for the next time step.\n\ufb01ltering techniques. In this setting, the unobserved signal (object\u2019s position, veloc-\nity, scale, orientation or discrete set of operations) is denoted {xt \u2208X; t \u2208N}. This\nsignal has initial distribution p (x0) and transition equation p (xt| xt\u22121, at\u22121) . Here\nat \u2208A denotes an action at time t, de\ufb01ned on a compact set A. For descrete poli-\ncies A is \ufb01nitie whereas for continuous policies A is a region in R2. The observations\n{ht \u2208H; t \u2208N\u2217}, are assumed to be conditionally independent given the process state\n{xt; t \u2208N}. Note that from the state space model perspective the observations are the\nhidden units of the second layer of the of the appearance model in the identity pathway.\nIn summary, the state-space model is described by the following distributions:\np (x0)\np (xt| xt\u22121, at\u22121)\nfor t \u22651\np (ht| xt, at)\nfor t \u22651,\nFor the transition model, we will adopt a classical autoregressive process.\n10\nOur aim is to estimate recursively in time the posterior distribution1 p (x0:t| h1:t, a1:t)\nand its associated features, including the marginal distribution bt \u225cp (xt| h1:t, a1:t) \u2014\nknown as the \ufb01ltering distribution or belief state. This distribution satis\ufb01es the follow-\ning recurrence:\nbt \u221dp(ht|xt, at)\nZ\np(xt|xt\u22121, at\u22121)p(dxt\u22121|h1:t\u22121, a1:t\u22121).\nExcept for standard distributions (e.g. Gaussian or discrete), this recurrence is intractable.\nAfter learning the observation model we will use it for tracking. The observation\nmodel is often de\ufb01ned in terms of the distance of the observations from a template \u03c4,\np (ht|xt, at) \u221dexp (\u2212d(h(xt, at), \u03c4)) ,\nwhere d(\u00b7, \u00b7) denotes a distance metric and \u03c4 an object template (for example, a color\nhistogram or spline). In this model, the observation h(xt, at) is a function of the current\nstate hypothesis and the selected action. The problem with this approach is eliciting a\ngood template. Often color histograms or splines are insuf\ufb01cient. For this reason, we\nwill construct the templates with (factored)-RBMs as follows. First, optical \ufb02ow is used\nto detect new object candidates entering the visual scene. Second, we assign a template\nto the detected object candidate, as shown in Figure 2. The same \ufb01gure also shows a\ntypical foveated observation (higher resolution in the center and lower in the periphery\nof the gaze) and the receptive \ufb01elds for this observation learned beforehand with an\nRBM. The control algorithm will be used to learn which parts of the template are most\ninformative, either by picking from amoung a prede\ufb01ned set of \ufb01xation points, or by\nusing a continuous policy. Finally, we de\ufb01ne the likelihood of each observation directly\nin terms of the distance of the hidden units of the RBM h(xt, at, vt), to the hidden units\nof the corresponding template region h(x1, at = k, v1). That is,\np (ht|xt, at = k) \u221dexp (\u2212d(h(xt, at = k, vt), h(x1, at = k, v1))) .\nThe above template is static, but conceivably one could adapt it over time.\n3.2\nReward Function\nA gaze control strategy speci\ufb01es a policy \u03c0(\u00b7) for selecting \ufb01xation points. The purpose\nof this strategy is to select \ufb01xation points which maximize an instantaneous reward\n1We use the notation x0:t \u225c{x0, ..., xt} to represent the past history of a variable over time.\n11\nfunction rt(\u00b7). The reward can be any desired behaviour for the system, such as mini-\nmizing posterior uncertainty or achieving a more abstract goal. We focus on gathering\nobservations so as to minimize the uncertainty in the estimate of the \ufb01ltering distribu-\ntion: rt(at|bt) \u225cu[ep(xt|h1:t, a1:t)]. More speci\ufb01cally, as discussed later, this reward\nwill be a function of the variance of the importance weights of the particle \ufb01lter approx-\nimation ep(xt|h1:t, a1:t) of the belief state.\nIt is also useful to consider the cumulative reward\nRT =\nT\nX\nt=1\nrt(at|bt) ,\nwhich is the sum of the instantaneous rewards which have been received up to time T.\nThe gaze control strategies we consider are all \u201cno-regret\u201d which means that the average\ngap between our cumulative reward and the cumulative reward from always picking the\noptimal action goes to zero as T \u2192\u221e.\nIn our current implementation, each action is a different gaze location and the ob-\njective is to choose where to look so as to minimize the uncertainty about the belief\nstate.\n4\nGaze control\nWe compare several different strategies for learning the gaze selection policy. In an\nearlier version of this work (Bazzani et al., 2010) we learned the gaze selection policy\nwith a portfolio allocation algorithm called Hedge (Freund & Schapire, 1997; Auer\net al., 1998b). Hedge requires knowledge of the rewards for all actions at each time\nstep, which is not realistic when gazes must be preformed sequentially, since the target\nobject will move between \ufb01xations. We compare this strategy, as well as two baseline\nmethods, to two very different alternatives.\nEXP3 is an extension of Hedge to partial information games (Auer et al., 2001).\nUnlike Hedge, EXP3 requires knowledge of the reward only for the action selected\nat each time step. EXP3 is more appropriate to the setting at hand, and is also more\ncomputationally ef\ufb01cient than Hedge; however, this comes at a cost of substantially\nlower theoretical performance.\nBoth Hedge and EXP3 learn gaze selection policies which choose among a discrete\n12\nset of predetermined \ufb01xation points. We can instead learn a continuous policy by es-\ntimating the reward surface using a Gaussian Process (Rasmussen & Williams, 2006).\nBy assuming that the reward surface is smooth, we can draw on the tools of Bayesian\noptimization (Brochu et al., 2010) to search for the optimal gaze location using as few\nexploratory steps as possible.\nThe following sections describe each of these approaches in more detail.\n4.1\nBaseline\nWe consider two baseline strategies, which we call random and circular. The random\nstrategy samples gaze selections uniformly from a small discrete set of possibilities.\nThe circular strategy also uses a small discrete set of gaze locations and cycles through\nthem in a \ufb01xed order.\n4.2\nHedge\nTo use Hedge (Freund & Schapire, 1997; Auer et al., 1998b) for gaze selection we must\n\ufb01rst discretize the action space by selecting a \ufb01xed \ufb01nite number of possible \ufb01xation\npoints. Hedge maintains an importance weight G(i) for each possible \ufb01xation point and\nuses them to form a stochastic policy at each time step. An action is selected according\nto this policy and the reward for each possible action is observed. These rewards are\nthen used to update the importance weights and the process repeats. Pseudo code for\nHedge is shown in Algorithm 1.\nAlgorithm 1 Hedge\nInput: \u03b3 > 0\nInput: G0(i) \u21900\nforeach i \u2208A\nfor t = 1, 2, . . . do\nfor i \u2208A do\npt(i) \u2190\nexp(\u03b3Gt\u22121(i))\nP\nj\u2208A exp(\u03b3Gt\u22121(j))\nat \u223c(pt(1), . . . , pt(|A|))\n// sample an action from the distribution (pt(k))\nfor i \u2208A do\nrt(i) \u2190rt(i|bt)\nGt(i) \u2190Gt\u22121(i) + rt(i)\n13\n4.3\nEXP3\nEXP3 (Auer et al., 2001) is a generalization of Hedge to the partial information setting.\nIn order to maintain estimates for the importance weights, Hedge requires reward infor-\nmation for each possible action at each time step. EXP3 works by wrapping Hedge in\nan outer loop which simulates a fully observed reward vector at each time step. EXP3\nselects actions based on a mixture of the policy found by Hedge and a uniform distribu-\ntion. EXP3 is able to function in the presence of partial information, but this comes at\nthe cost of substantially worse theoretical guarantees. Pseudo code for EXP3 is shown\nin Algorithm 2.\nAlgorithm 2 EXP3\nInput: \u03b3 \u2208(0, 1]\nInitialize Hedge(\u03b3)\nfor t \u22081, 2, . . . do\nReceive pt from Hedge\n\u02c6pt \u2190(1 \u2212\u03b3)pt +\n\u03b3\n|A|\nat \u223c(\u02c6pt(1), . . . , \u02c6pt(|A|))\nSimulate reward vector for Hedge where \u02c6rt(j) \u2190\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\nrt(j)/pt(j)\nifj = at\n0\notherwise\n4.4\nBayesian Optimization\nBoth Hedge and EXP3 discretize the space of possible \ufb01xation points and learn a dis-\ntribution over this \ufb01nite set. In contrast, Bayesian optimization is able to treat the space\nof possible \ufb01xation points as fully continuous by placing a smoothness prior on how\nreward is expected to vary with location. Intuitively, if we know the reward at one lo-\ncation, then we expect other, nearby locations to produce similar rewards. Gaussian\nProcess priors encode this type of belief (Rasmussen & Williams, 2006), and have been\nused extensively for optimization of cost functions when it is important to minimize the\ntotal number of function evaluations (Brochu et al., 2010).\nWe model the latent reward function rt(at|bt) \u225cr(at|bt, \u03b8t) as a zero mean Gaus-\n14\nsian Process\nr(at|bt, \u03b8t) \u223cGP(0, k(at, a\u2032\nt|bt, \u03b8t)) ,\nwhere bt is the belief state (see Section 3.1), and \u03b8t are the model hyperparameters.\nThe kernel function k(\u00b7, \u00b7), gives the covariance between the reward at any two gaze\nlocations. To ease the notation, the explicit dependence of r(\u00b7) and k(\u00b7, \u00b7) on bt and \u03b8t\nwill be dropped.\nWe assume that the true reward function r(\u00b7) is not directly measurable, and what\nwe observe are measurements of this function corrupted by Gaussian noise. That is, at\neach time step the instantaneous reward rt, is given by\nrt = r(at) + \u03c3n\u03b4n ,\nwhere \u03b4n \u223cN(0, 1) and \u03c3n is a hyperparameter indicating the amount of observation\nnoise, which we absorb into \u03b8t.\nGiven a set of observations we can compute the posterior predictive distribution for\nr(\u00b7):\nr(a|r1:t, a1:t) \u223cN(mt(a), s2\nt(a)) ,\n(2)\nmt(a) = kT[K + \u03c32\nnI]\u22121r1:t ,\ns2\nt(a) = k(a, a) \u2212kT[K + \u03c32\nnI]\u22121k ,\nwhere\nK =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\nk(a1, a1)\n\u00b7 \u00b7 \u00b7\nk(a1, at)\n...\n...\n...\nk(at, a1)\n\u00b7 \u00b7 \u00b7\nk(at, at)\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb,\nk =\nh\nk(a1, a)\n\u00b7 \u00b7 \u00b7\nk(at, a)\niT\n,\nr1:t =\nh\nr1\n\u00b7 \u00b7 \u00b7\nrt\niT\n.\nIt remains to specify the form of the kernel function, k(\u00b7, \u00b7). We experimented with\nseveral possibilities, but found that the speci\ufb01c form of the kernel function is not crit-\nical to the performance of this method. For the experiments in this paper we used the\nsquared exponential kernel,\nk(ai, aj) = \u03c32\nm exp\n \n\u22121\n2\nD\nX\nk=1\n\u0012ai,k \u2212aj,k\n\u2113k\n\u00132!\n,\n15\nwhere \u03c32\nm and the {\u21131, . . . , \u2113D} are hyperparameters.\nEquation 2 is a Gaussian Process estimate of the reward surface and can be used to\nselect a \ufb01xation point for the next time step. This estimate gives both a predicted reward\nvalue and an associated uncertainty for each possible \ufb01xation point. This is the strength\nof Gaussian Processes for this type of optimization problem, since the predictions can\nbe used to balance exploration (choosing a \ufb01xation point where the reward is highly\nuncertain) and exploitation (choosing a point we are con\ufb01dent will have high reward).\nThere are many selection methods available in the literature which offer different\ntradeoffs between these two criteria. In this paper we use GP-UCB (Srinivas et al.,\n2010) which selects\nat+1 = arg max\na\nmt(a) +\np\n\u03b2tst(a)\n(3)\nwhere \u03b2t is a parameter. The setting \u03b2t = 2 log(t3\u03c02/3\u03b4) (with \u03b4 = 0.001) is used\nthroughout this paper.\nEquation 3 must still be optimized to \ufb01nd at+1, which can be performed using stan-\ndard global optimization tools. We use DIRECT (Jones et al., 1993) due to the existence\nof a readily available implementation.\nThe Gaussian Process regression is controlled by several hyperparameters (see Fig-\nure 6): \u03c32\nm controls the overall magnitude of the covariance, and \u03c32\nn controls the amount\nof observation noise. The remaining parameters {\u21131, . . . , \u2113D} are length scale parame-\nters which control the range of the covariance effects in each dimension.\nTreatment of the hyperparameters requires special consideration in this setting. The\npure Bayesian approach is to put a prior on each parameter and integrate them out\nof the predictive distribution. However, since the integrals involved are not tractable\nanalytically, this requires computationally expensive numerical approximations. Speed\nis an issue here since GP-UCB requires that we optimize a function of the posterior\nprocess at each time step so, for instance, computing Monte Carlo averages for each\nevaluation of Equation 2 is prohibitively slow.\nAn alternative approach is to choose parameter values via maximum likelihood.\nThis can be done quickly, and allows us to make speedy predictions; however, in this\ncase we suffer from problems of data scarcity, particularly early in the tracking process\nwhen few observations have been made. The length scale parameters are particularly\n16\nT\nat\nr(  )\nrt\n\u03c32\nn\n\u03c32\nm\nD\n \u2113 i\nK\nFigure 6: Graphical model for Bayesian optimization. The \u2113i are length scales in each dimen-\nsion, \u03c32\nm is the magnitude parameter and \u03c32\nn is the noise level. In our model \u03c32\nm and \u03c32\nn follow\na uniform prior and the \u2113i follow independent Student-t priors.\nprone to receiving very poor estimates when there is little data available.\nWe have found that using informative priors for the length scale parameters and\nmaking MAP, rather than ML, estimates at each time step provides a solution to the\nproblems described above. MAP estimates can be made quickly using gradient opti-\nmization methods (Rasmussen & Williams, 2006), and informative priors provide re-\nsistance to the problems encountered with ML. The experiments in Section 6 place\nuniform priors on the magnitude and noise parameters and place independent Student-t\npriors on each length scale parameter. The experiments also use an initial data collec-\ntion phase of 10 time steps before any adjustment of the parameters is made.\n5\nAlgorithm\nSince the belief state cannot be computed analytically, we will adopt particle \ufb01ltering\nto approximate it. The full algorithm is shown in Algorithm 3.\nWe refer readers to Doucet et al. (2001) for a more in depth treatment of these\nsequential Monte Carlo methods. Assume that at time t \u22121 we have N \u226b1 parti-\ncles (samples) {x(i)\n0:t\u22121}N\ni=1 distributed according to p (dx0:t\u22121|h1:t\u22121, a1:t\u22121). We can\n17\nAlgorithm 3 Particle \ufb01ltering algorithm with gaze control. The algorithm shown here\nis for partial information strategies. For full information strategies the importance sam-\npling step is done independently for each possible action and the gaze control step is\nable to use reward information from each possible action to create the new strategy\n\u03c0t+1(\u00b7).\n1. Initialization\nfor i = 1 to N do\nx(i)\n0 \u223cp(x0)\nInitialize the policy \u03c01(\u00b7)\n// How this is done depends on the control strategy\nfor t = 1 . . . do\n2. Importance sampling\nfor i = 1 to N do\n// Predict the next state\nex(i)\nt\n\u223cqt\n\u0010\ndx(i)\nt\n\f\f\f ex(i)\n0:t\u22121, h1:t, a1:t\n\u0011\nex(i)\n0:t \u2190\n\u0010\nx(i)\n0:t\u22121, ex(i)\nt\n\u0011\nk\u22c6\u223c\u03c0t(\u00b7)\n// Select an action according to the policy\nfor i = 1 to N do\n// Evaluate the importance weights\new(i)\nt\n\u2190\np\n\u0010\nht|ex(i)\nt , at = k\u22c6\u0011\np\n\u0010\nex(i)\nt |ex(i)\n0:t\u22121, at\u22121\n\u0011\nqt\n\u0010\nex(i)\nt\n\f\f\f ex(i)\n0:t\u22121, h1:t, a1:t\n\u0011\nfor i = 1 to N do\n// Normalize the importance weights\nw(i)\nt\n\u2190\new(i)\nt\nPN\nj=1 ew(j)\nt\n3. Gaze control\nrt = PN\ni=1(w(i)\nt )2\n// Receive reward for the chosen action\nIncorporate rt into the policy to create \u03c0t+1(\u00b7)\n4. Selection\nResample with replacement N particles\n\u0010\nx(i)\n0:t; i = 1, . . . , N\n\u0011\nfrom the set\n\u0010\nex(i)\n0:t; i = 1, . . . , N\n\u0011\naccording to the normalized importance weights w(i)\nt\n18\napproximate this belief state with the following empirical distribution\nbp (dx0:t\u22121|h1:t\u22121, a1:t\u22121) \u225c1\nN\nN\nX\ni=1\n\u03b4x(i)\n0:t\u22121 (dx0:t\u22121) .\nParticle \ufb01lters combine sequential importance sampling with a selection scheme de-\nsigned to obtain N new particles {x(i)\n0:t}N\ni=1 distributed approximately according to p (dx0:t|h1:t, a1:t).\n5.1\nImportance sampling step\nThe joint distributions p (dx0:t\u22121|h1:t\u22121, a1:t\u22121) and p (dx0:t|h1:t, a1:t) are of different\ndimension. We \ufb01rst modify and extend the current paths x(i)\n0:t\u22121 to obtain new paths ex(i)\n0:t\nusing a proposal kernel qt (dex0:t|x0:t\u22121, h1:t, a1:t) . As our goal is to design a sequential\nprocedure, we set\nqt (dex0:t| x0:t\u22121, h1:t, a1:t) = \u03b4x0:t\u22121 (dex0:t\u22121) qt (dext| ex0:t\u22121, h1:t, a1:t) ,\nthat is ex0:t = (x0:t\u22121, ext). The aim of this kernel is to obtain new paths whose distribu-\ntion\nqt (dex0:t|h1:t, a1:t) = p (dex0:t\u22121|h1:t\u22121, a1:t\u22121) qt (dext| ex0:t\u22121, h1:t, a1:t) ,\nis as \u201cclose\u201d as possible to p (dex0:t|h1:t, a1:t). Since we cannot choose qt (dex0:t|h1:t, a1:t) =\np (dex0:t|h1:t, a1:t) because this is the quantity we are trying to approximate in the \ufb01rst\nplace, it is necessary to weight the new particles so as to obtain consistent estimates.\nWe perform this \u201ccorrection\u201d with importance sampling, using the weights\newt = ewt\u22121\np (ht|ext, at) p (dext|ex0:t\u22121, at\u22121)\nqt (dext| ex0:t\u22121, h1:t, a1:t)\n.\nThe choice of the transition prior as proposal distribution is by far the most com-\nmon one. In this case, the importance weights reduce to the expression for the like-\nlihood. However, it is possible to construct better proposal distributions, which make\nuse of more recent observations, using object detectors (Okuma et al., 2004), saliency\nmaps (Itti et al., 1998), optical \ufb02ow, and approximate \ufb01ltering methods such as the\nunscented particle \ufb01lter. One could also easily incorporate strategies to manage data\nassociation and other tracking related issues. After normalizing the weights, w(i)\nt\n=\new(i)\nt\nPN\nj=1 ew(j)\nt , we obtain the following estimate of the \ufb01ltering distribution:\nep (dx0:t|h1:t, a1:t) =\nN\nX\ni=1\nw(i)\nt \u03b4ex(i)\n0:t (dx0:t) .\n19\nFinally a selection step is used to obtain an \u201cunweighted\u201d approximate empirical\ndistribution \u02c6p(dx0:t|h1:t, a1:t) of the weighted measure \u02dcp(dx0:t|h1:t, a1:t). The basic idea\nis to discard samples with small weights and multiply those with large weights. The use\nof a selection step is key to making the SMC procedure effective; see Doucet et al.\n(2001) for details on how to implement this black box routine.\n6\nExperiments\n6.1\nFull Information Policies\nIn this section, three experiments are carried out to evaluate quantitatively and qual-\nitatively the proposed approach. The \ufb01rst experiment provides comparisons to other\ncontrol policies on a synthetic dataset. The second experiment, on a similar synthetic\ndataset, demonstrates how the approach can handle large variations in scale, occlusion\nand multiple targets. The \ufb01nal experiment is a demonstration of tracking and classi\ufb01ca-\ntion performance on several real videos. For the synthetic digit videos, we trained the\n\ufb01rst-layer RBMs on the foveated images, while for the real videos we trained factored-\nRBMs on foveated natural image patches (Ranzato & Hinton, 2010).\nThe \ufb01rst experiment uses 10 video sequences (one for each digit) built from the\nMNIST dataset. Each sequence contains a moving digit and static digits in the back-\nground (to create distractions). The objective is to track and recognize the moving digit;\nsee Figure 7. The gaze template had K = 9 gaze positions, chosen so that gaze G5 was\nat the center. The location of the template was initialized with optical \ufb02ow.\nWe compare the Hedge learning algorithm against algorithms with deterministic\nand random policies. The deterministic policy chooses each gaze in sequence and in\na particular pre-speci\ufb01ed order, whereas the random policy selects a gaze uniformly at\nrandom. We adopted the Bhattacharyya distance in the speci\ufb01cation of the observation\nmodel. A multi-\ufb01xation RBM was trained to map the \ufb01rst layer hidden units of three\ntime consecutive time steps into a second hidden layer, and trained a logistic regressor\nto further map to the 10 digit classes. We used the transition prior as proposal for the\nparticle \ufb01lter.\nTables 6.1 and 6.1 report the comparison results. Tracking accuracy was measured\n20\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nAVG.\nLEARNED\nPOLICY\n1.2\n(1.2)\n3.0\n(2.0)\n2.9\n(1.0)\n2.2\n(0.7)\n1.0\n(1.9)\n1.8\n(1.9)\n3.8\n(1.0)\n3.8\n(1.5)\n1.5\n(1.7)\n3.8\n(2.8)\n2.5\n(1.6)\nDETERMINISTIC\nPOLICY\n18.2\n(29.6)\n536.9\n(395.6)\n104.4\n(69.7)\n2.9\n(2.2)\n201.3\n(113.4)\n4.6\n(4.0)\n5.6\n(3.1)\n64.4\n(45.3)\n142.0\n(198.8)\n144.6\n(157.7)\n122.5\n(101.9)\nRANDOM\nPOLICY\n41.5\n(54.0)\n410.7\n(329.4)\n3.2\n(2.0)\n3.3\n(2.4)\n42.8\n(60.9)\n6.5\n(9.6)\n5.7\n(3.2)\n80.7\n(48.6)\n38.9\n(50.6)\n225.2\n(241.6)\n85.9\n(80.2)\nTable 1: Tracking error (in pixels) on several video sequences using different policies for gaze\nselection.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nAVG.\nLEARNED\nPOLICY\n95.62%\n100.00%\n99.66%\n99.33%\n99.66%\n100.00%\n100.00%\n98.32%\n97.98%\n89.56%\n98.01%\nDETERMINISTIC\nPOLICY\n99.33%\n100.00%\n98.99%\n94.95%\n5.39%\n98.32%\n0.00%\n29.63%\n52.19%\n0.00%\n57.88%\nRANDOM\nPOLICY\n98.32%\n100.00%\n96.30%\n99.66%\n29.97%\n96.30%\n89.56%\n22.90%\n12.79%\n13.80%\n65.96%\nTable 2: Classi\ufb01cation accuracy on several video sequences using different policies for gaze\nselection.\nin terms of the mean and standard deviation (in brackets) over time of the distance be-\ntween the target ground truth and the estimate; measured in pixels. The analysis high-\nlights that the error of the learned policy is always below the error of the other policies.\nIn most of the experiments, the tracker fails when an occlusion occurs for the determin-\nistic and the random policies, while the learned policy is successful. This is very clear\nin the videos at: http://www.youtube.com/user/anonymousTrack\nThe loss of track for the simple policies is mirrored by the high variance results in\nTable 6.1 (experiments 0, 1, 4, and so on). The average mean and standard deviations\n(last column of Table 6.1) make it clear that the proposed strategy for learning a gaze\npolicy can be of enormous bene\ufb01t. The improvements in tracking performance are\nmirrored by improvements in classi\ufb01cation performance (Table 6.1).\nFigure 7 provides further anecdotal evidence for the policy learning algorithm. The\ntop sequence shows the target and the particle \ufb01lter estimate of its location over time.\nThe middle sequence illustrates how the policy changes over time. In particular, it\ndemonstrates that hedge can effectively learn where to look in order to improve tracking\nperformance (we chose this simple example as in this case it is obvious that the center\nof the eight (G5) is the most reliable gaze action). The classi\ufb01cation results over time\nare shown in the third row.\n21\nT = 69/300\nT = 20/300\nT = 142/300\nT = 181/300\nT = 215/300\nT = 260/300\nG1\nG2\nG3\nG4\nG5\nG6\nG7\nG8\nG9\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nG1\nG2\nG3\nG4\nG5\nG6\nG7\nG8\nG9\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nG1\nG2\nG3\nG4\nG5\nG6\nG7\nG8\nG9\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nG1\nG2\nG3\nG4\nG5\nG6\nG7\nG8\nG9\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nG1\nG2\nG3\nG4\nG5\nG6\nG7\nG8\nG9\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nG1\nG2\nG3\nG4\nG5\nG6\nG7\nG8\nG9\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n1\n0\n1\n0\n1\n0\n1\n0\n1\n0\n1\n0\n1\n0\n1\n0\n1\n0\n1\n0\n1\n0\n1\n0\nG1 G2 G3 G4 G5 G6 G7 G8 G9\nG1 G2 G3 G4 G5 G6 G7 G8 G9\nG1 G2 G3 G4 G5 G6 G7 G8 G9\nG1 G2 G3 G4 G5 G6 G7 G8 G9\nG1 G2 G3 G4 G5 G6 G7 G8 G9\nG1 G2 G3 G4 G5 G6 G7 G8 G9\n1 2 3 4 5 6 7 8 9\n0\n1 2 3 4 5 6 7 8 9\n0\n1 2 3 4 5 6 7 8 9\n0\n1 2 3 4 5 6 7 8 9\n0\n1 2 3 4 5 6 7 8 9\n0\n1 2 3 4 5 6 7 8 9\n0\nFigure 7: Tracking and classi\ufb01cation accuracy results with the learned policy. First row:\nposition of the target and estimate over time. Second row: policy distribution over the 9 gazes;\nhedge clearly converges to the most reasonable policy. Third row: cumulative class distribution\nfor recognition.\nThe second experiment addresses a similar video sequence, but tracking multiple\ntargets. The image scale of each target changes signi\ufb01cantly over time, so the algorithm\nhas to be invariant with respect to these scale transformations. In this case, we used a\nmixture proposal distribution consisting of motion detectors and the transition prior. We\nalso tested a saliency proposal but found it to be less effective than the motion detectors\nfor this dataset. Figure 8 (top) shows some of the video frames and tracks. The videos\nallow one to better appreciate the performance of the multi-target tracking algorithm in\nthe presence of occlusions.\nTracking and classi\ufb01cation results for the real videos are shown in Figure 8 and the\naccompanying videos.\n6.2\nPartial Information Policies\nIn this section, two experiments are carried out to evaluate the performance of the dif-\nferent gaze selection policies.\nIn the \ufb01rst experiment we compare the performance of each gaze selection method\non a data set of several videos of digits from the MNIST data set moving on a black\nbackground. The target in each video encounters one or more partial occlusions which\nthe tracking algorithm must handle gracefully. Additionally, each video sequence has\n22\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\njennifer lopez\nFigure 8: Top: Multi-target tracking with occlusions and changes in scale on a synthetic video.\nMiddle and bottom: Tracking in real video sequences.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nAvg\nBayesopt\n5.36\n(2.32)\n7.92\n(2.52)\n2.62\n(3.89)\n4.05\n(1.67)\n1.70\n(5.10)\n8.31\n(3.35)\n4.94\n(2.28)\n12.09\n(3.53)\n1.52\n(2.76)\n9.06\n(1.66)\n5.76\n(2.91)\nHedge\n2.97\n(1.56)\n3.20\n(2.19)\n2.97\n(1.99)\n2.92\n(2.00)\n3.14\n(1.80)\n2.96\n(2.08)\n2.86\n(1.96)\n2.98\n(1.76)\n2.81\n(1.64)\n3.15\n(3.73)\n3.00\n(2.07)\nEXP3\n3.18\n(5.05)\n3.03\n(10.08)\n65.46\n(3212.16)\n91.81\n(3671.66)\n2.62\n(2.35)\n7.20\n(303.29)\n67.54\n(2346.82)\n2.97\n(3.99)\n3.06\n(2.71)\n77.01\n(3135.17)\n32.39\n(1269.33)\nTable 3: Tracking error on several video sequences using different methods for gaze selection.\nThe table shows mean tracking error as well as the error variance (in brackets) over a single\ntest sequence.\nbeen corrupted with 30% noise. We measure the error between the estimated track\nand the ground truth for each gaze selection method, and demonstrate that Bayesian\noptimization preforms comparably to Hedge, but that EXP3 is not able to reach a sat-\nisfactory level of performance. We also demonstrate qualitatively that the Bayesian\noptimization approach learns good gaze selection policies on this data set.\nOur second experiment provides evidence that the Bayesian optimization method\ncan generalize to real world data.\nTable 3 reports the results from our \ufb01rst experiment. The table shows the mean\ntracking error, measured by averaging distance between the estimated and ground truth\n23\ntrack over the entire video sequence. Here we see that the Bayesian optimization ap-\nproach compares favorably to Hedge in terms of tracking performance, and that EXP3\npreforms substantially worse than the other two methods. Although Hedge preforms\nmarginally better than Bayesian optimization, it is important to remember that Bayesian\noptimization solves a signi\ufb01cantly more dif\ufb01cult problem. Hedge relies on discretizing\nthe action space, and must have access to the rewards for all possible actions at each\ntime step. In contrast, Bayesian optimization considers a fully continuous action space,\nand receives reward information only for the chosen actions.\nFigure 9: Top: Digit templates with the estimated reward surfaces superimposed. Markers\nindicate the best \ufb01xation point found in each of ten runs. Bottom: A visualization of the image\nfound by averaging the best \ufb01xation points found across ten runs.\nFigure 9 shows the reward surfaces learned for each digit by Bayesian optimization,\nas well as a visualization of the overall best \ufb01xation points using data aggregated across\nten runs. The optimal \ufb01xation points found by the algorithm are tightly clustered, and\nthe resulting observations are very distinguishable.\nIn our second experiment we use the Youtube celebrity dataset from Kim et al.\n(2008). This data set consists of several videos of celebrities taken from Youtube and\nis challenging for tracking algorithms as the videos exhibit a wide variety of illumi-\nnations, expressions and face orientations. We run our tracking model using Bayesian\noptimization to learn a gaze selection policy on this data set, and present some results in\nFigure 10. Although we report only qualitative results from this experiment, it provides\nanecdotal evidence that Bayesian optimization is able to form a good gaze selection\npolicy on real world data.\n24\nFigure 10: Results on a real data set. Far left: An example frame from the video sequence.\nCenter left: The tracking template with the optimal \ufb01xation window highlighted. Center right:\nThe reward surface produced by Bayesian optimization. The white markers show the centers of\neach \ufb01xation point in a single tracking run. Right: Input to the observation model when \ufb01xating\non the best point. (Best viewed from a distance).\n7\nConclusions and Future Work\nWe have proposed a decision-theoretic probabilistic graphical model for joint classi\ufb01-\ncation, tracking and planning. The experiments demonstrate the signi\ufb01cant potential\nof this approach. We examined several different strategies for gaze control in both the\nfull and partial information settings. We saw that a straightforward generalization of\nthe full information policy to partial information gave poor performance and we pro-\nposed an alternative method which is able not only to perform well in the presence of\npartial information but also allows us to expand the set of possible \ufb01xation points to be\na continuous domain.\nThere are many routes for further exploration. In this work we pre-trained the\n(factored)-RBMs. However, existing particle \ufb01ltering and stochastic optimization al-\ngorithms could be used to train the RBMs online. Following the same methodology,\nwe should also be able to adapt and improve the target templates and proposal distribu-\ntions over time. This is essential to extend the results to long video sequences where\nthe object undergoes signi\ufb01cant transformations (e.g. as is done in the predator tracking\nsystem (Kalal et al., 2010)).\nDeployment to more complex video sequences will require more careful and thought-\nful design of the proposal distributions, transition distributions, control algorithms, tem-\nplate models, data-association and motion analysis modules. Fortunately, many of the\nsolutions to these problems have already been engineered in the computer vision, track-\ning and online learning communities. Admittedly, much work remains to be done.\n25\nSaliency maps are ubiquitous in visual attention studies. Here, we simply used stan-\ndard saliency tools and motion \ufb02ow in the construction of the proposal distributions for\nparticle \ufb01ltering. There might be better ways to exploit the saliency maps, as neuro-\nphysiological experiments seem to suggest (Gottlieb et al., 1998).\nOne of the most interesting avenues for future work is the construction of more\nabstract attentional strategies. In this work, we focused on attending to regions of the\nvisual \ufb01eld, but clearly one could attend to subsets of receptive \ufb01elds or objects in the\ndeep appearance model.\nThe current model has no ability to recover from a tracking failure. It may be\npossible to use information from the identity pathway (i.e. the classi\ufb01er output) to detect\nand recover from tracking failure.\nA closer examination of the exploration/exploitation tradeoff in the tracking setting\nis in order. For instance, the methods we considered assume that future rewards are\nindependent of past actions. This assumption is clearly not true in our setting, since\nchoosing a long sequence of very poor \ufb01xation points can lead to tracking failure. We\ncan potentially solve this problem by incorporating the current tracking con\ufb01dence into\nthe gaze selection strategy. This would allow the exploration/exploitation trade off to\nbe explicitly modulated by the needs of the tracker, e.g. after choosing a poor \ufb01xation\npoint the selection policy could be adjusted temporarily to place extra emphasis on ex-\nploiting good \ufb01xation points until con\ufb01dence in the target location has been recovered.\nContextual bandits provide a framework for integrating and reasoning about this type\nof side-information in a principled manner.\nAcknowledgments\nWe thank Ben Marlin, Kenji Okuma, Marc\u2019Aurelio Ranzato and Kevin Swersky. This\nwork was supported by CIFAR\u2019s NCAP program and NSERC.\nReferences\nAuer, P., Cesa-Bianchi, N., Freund, Y., and Schapire, R.E. Gambling in a rigged casino:\nThe adversarial multi-armed bandit problem. In focs, pp. 322. Published by the IEEE\n26\nComputer Society, 1998a.\nAuer, P., Cesa-Bianchi, N., Freund, Y., and Schapire, R.E. The nonstochastic mul-\ntiarmed bandit problem. SIAM Journal on Computing, 32(1):48\u201377, 2001. ISSN\n0097-5397.\nAuer, Peter, Cesa-Bianchi, Nicol`o, Freund, Yoav, and Schapire, Robert E. Gambling\nin a rigged casino: the adversarial multi-armed bandit problem. Technical Report\nNC2-TR-1998-025, 1998b.\nBazzani, L., de Freitas, N., and Ting, J.A. Learning attentional mechanisms for si-\nmultaneous object tracking and recognition with deep networks. NIPS 2010 Deep\nLearning and Unsupervised Feature Learning Workshop, 2010.\nBrochu, E., Cora, V.M., and de Freitas, N. A tutorial on Bayesian optimization of\nexpensive cost functions, with application to active user modeling and hierarchical\nreinforcement learning. Technical report, University of British Columbia, 2010.\nColombo, John. The development of visual attention in infancy. Annual Review of\nPsychology, pp. 337\u2013367, 2001.\nDoucet, A, de Freitas, N, and Gordon, N. Introduction to sequential Monte Carlo meth-\nods. In Doucet, A, de Freitas, N, and Gordon, N J (eds.), Sequential Monte Carlo\nMethods in Practice. Springer-Verlag, 2001.\nFreund, Yoav and Schapire, Robert E. A decision-theoretic generalization of on-line\nlearning and an application to boosting. Journal of Computer and System Sciences,\n55:119\u2013139, 1997.\nGaborski, R., Vaingankar, V., Chaoji, V., Teredesai, A., and Tentler, A. Detection of\ninconsistent regions in video streams. In Proc. SPIE Human Vision and Electronic\nImaging. Citeseer, 2004.\nGao, D., Mahadevan, V., and Vasconcelos, N. The discriminant center-surround hy-\npothesis for bottom-up saliency. Advances in neural information processing systems,\n20, 2007.\n27\nGirard, B. and Berthoz, A. From brainstem to cortex: Computational models of saccade\ngeneration circuitry. Progress in Neurobiology, 77(4):215 \u2013 251, 2005.\nGottlieb, Jacqueline P., Kusunoki, Makoto, and Goldberg, Michael E. The representa-\ntion of visual salience in monkey parietal cortex. Nature, 391:481\u2013484, 1998.\nHinton, GE and Salakhutdinov, RR. Reducing the dimensionality of data with neural\nnetworks. Science, 313(5786):504\u2013507, 2006.\nIsard, M and Blake, A. Contour tracking by stochastic propagation of conditional den-\nsity. In European Computer Vision Conference, pp. 343\u2013356, 1996.\nItti, L., Koch, C., and Niebur, E. A model of saliency-based visual attention for rapid\nscene analysis. IEEE Transactions on Pattern Analysis and Machine Intelligence, 20\n(11):1254 \u20131259, 1998.\nJones, D.R., Perttunen, C.D., and Stuckman, B.E. Lipschitzian optimization without\nthe Lipschitz constant. Journal of Optimization Theory and Applications, 79(1):157\u2013\n181, 1993. ISSN 0022-3239.\nKalal, Z., Mikolajczyk, K., and Matas, J. Face-tld: Tracking-learning-detection applied\nto faces. In Image Processing (ICIP), 2010 17th IEEE International Conference on,\npp. 3789\u20133792. IEEE, 2010.\nKavukcuoglu, K., Ranzato, M.A., Fergus, R., and Le-Cun, Yann. Learning invariant\nfeatures through topographic \ufb01lter maps. In Computer Vision and Pattern Recogni-\ntion, pp. 1605\u20131612, 2009.\nKim, M., Kumar, S., Pavlovic, V., and Rowley, H. Face tracking and recognition with\nvisual constraints in real-world videos. IEEE Conf. Computer Vision and Pattern\nRecognition, 2008.\nKoch, C. and Ullman, S. Shifts in selective visual attention: towards the underlying\nneural circuitry. Hum Neurobiol, 4(4):219\u201327, 1985.\nK\u00a8oster, Urs and Hyv\u00a8arinen, Aapo. A two-layer ICA-like model estimated by score\nmatching. In International Conference of Arti\ufb01cial Neural Networks, pp. 798\u2013807,\n2007.\n28\nLarochelle, Hugo and Hinton, Geoffrey. Learning to combine foveal glimpses with a\nthird-order Boltzmann machine. In Neural Information Processing Systems, 2010.\nLee, H., Grosse, R., Ranganath, R., and Ng, A.Y. Convolutional deep belief networks\nfor scalable unsupervised learning of hierarchical representations. In International\nConference on Machine Learning, 2009.\nMcNaughton, Bruce L., Battaglia, Francesco P., Jensen, Ole, Moser, Edvard I., and\nMoser, May-Britt. Path integration and the neural basis of the \u2019cognitive map\u2019. Nature\nReviews Neuroscience, 7(8):663\u2013678, 2006.\nOkuma, Kenji, Taleghani, Ali, de Freitas, Nando, and Lowe, David G.\nA boosted\nparticle \ufb01lter: Multitarget detection and tracking. In ECCV, 2004.\nOlshausen, B. A. and Field, D. J. Emergence of simple-cell receptive \ufb01eld properties\nby learning a sparse code for natural images. Nature, 381:607\u2013609, 1996.\nOlshausen, B.A., Anderson, C.H., and Van Essen, D.C. A neurobiological model of\nvisual attention and invariant pattern recognition based on dynamic routing of infor-\nmation. The Journal of Neuroscience, 13(11):4700, 1993a. ISSN 0270-6474.\nOlshausen, Bruno A., Anderson, Charles H., and Essen, David C. Van. A neurobio-\nlogical model of visual attention and invariant pattern recognition based on dynamic\nrouting of information. Journal of Neuroscience, 13:4700\u20134719, 1993b.\nPostma, Eric O., van den Herik, H. Jaap, and Hudson, Patrick T. W. SCAN: A scalable\nmodel of attentional selection. Neural Networks, 10(6):993 \u2013 1015, 1997.\nRanzato, M.A. and Hinton, G.E. Modeling pixel means and covariances using factor-\nized third-order Boltzmann machines. In Computer Vision and Pattern Recognition,\npp. 2551\u20132558, 2010.\nRasmussen, C.E. and Williams, C.K.I. Gaussian processes for machine learning. Adap-\ntive computation and machine learning. MIT Press, 2006. ISBN 9780262182539.\nURL http://books.google.ca/books?id=vWtwQgAACAAJ.\nRensink, Ronald A. The dynamic representation of scenes. Visual Cognition, pp. 17\u201342,\n2000.\n29\nRosa, M.G.P. Visual maps in the adult primate cerebral cortex: Some implications\nfor brain development and evolution. Brazilian Journal of Medical and Biological\nResearch, 35:1485 \u2013 1498, 2002.\nSrinivas, N., Krause, A., Kakade, S.M., and Seeger, M. Gaussian process optimization\nin the bandit setting: No regret and experimental design. International Conference\non Machine Learning, 2010.\nSwersky, K., Chen, Bo, Marlin, B., and de Freitas, N. A tutorial on stochastic approx-\nimation algorithms for training restricted Boltzmann machines and deep belief nets.\nIn ITA Workshop, pp. 1\u201310, 2010.\nSwersky, K., Buchman, D., Marlin, B.M., and de Freitas, N. On autoencoders and score\nmatching for energy based models. International Conference in Machine Learning,\n2011.\nTaylor, G.W., Sigal, L., Fleet, D.J., and Hinton, G.E. Dynamical binary latent variable\nmodels for 3D human pose tracking. In Computer Vision and Pattern Recognition,\npp. 631\u2013638, 2010.\nTorralba, A., Oliva, A., Castelhano, M.S., and Henderson, J.M. Contextual guidance\nof eye movements and attention in real-world scenes: The role of global features in\nobject search. Psychological review, 113(4):766, 2006.\nVincent, P., Larochelle, H., Bengio, Y., and Manzagol, P.A. Extracting and composing\nrobust features with denoising autoencoders. In International Conference on Machine\nLearning, pp. 1096\u20131103, 2008.\nWelling, M., Rosen-Zvi, M., and Hinton, G. Exponential family harmoniums with\nan application to information retrieval. Neural Information Processing Systems, 17:\n1481\u20131488, 2005.\nZhang, L., Tong, M.H., and Cottrell, G.W. Sunday: Saliency using natural statistics\nfor dynamic analysis of scenes. In Proceedings of the 31st Annual Cognitive Science\nConference, Amsterdam, Netherlands. Citeseer, 2009.\n30\n",
        "sentence": " Such models have been applied successfully in image classification [12, 4, 3, 2], object tracking [13, 3], machine translation [6], caption generation [5], and image generation [14, 15].",
        "context": "plate models, data-association and motion analysis modules. Fortunately, many of the\nsolutions to these problems have already been engineered in the computer vision, track-\ning and online learning communities. Admittedly, much work remains to be done.\n25\ntions over time. This is essential to extend the results to long video sequences where\nthe object undergoes signi\ufb01cant transformations (e.g. as is done in the predator tracking\nsystem (Kalal et al., 2010)).\ntures. Our system is able to cope with arbitrary feature types, and although we consider\nonly on localization in this paper, our model is suf\ufb01ciently general to be applied to\nidentifying salient features for other goals."
    },
    {
        "title": "Generating sequences with recurrent neural networks",
        "author": [
            "A. Graves"
        ],
        "venue": "arXiv:1308.0850,",
        "citeRegEx": "14",
        "shortCiteRegEx": null,
        "year": 2014,
        "abstract": "This paper shows how Long Short-term Memory recurrent neural networks can be\nused to generate complex sequences with long-range structure, simply by\npredicting one data point at a time. The approach is demonstrated for text\n(where the data are discrete) and online handwriting (where the data are\nreal-valued). It is then extended to handwriting synthesis by allowing the\nnetwork to condition its predictions on a text sequence. The resulting system\nis able to generate highly realistic cursive handwriting in a wide variety of\nstyles.",
        "full_text": "Generating Sequences With\nRecurrent Neural Networks\nAlex Graves\nDepartment of Computer Science\nUniversity of Toronto\ngraves@cs.toronto.edu\nAbstract\nThis paper shows how Long Short-term Memory recurrent neural net-\nworks can be used to generate complex sequences with long-range struc-\nture, simply by predicting one data point at a time.\nThe approach is\ndemonstrated for text (where the data are discrete) and online handwrit-\ning (where the data are real-valued). It is then extended to handwriting\nsynthesis by allowing the network to condition its predictions on a text\nsequence. The resulting system is able to generate highly realistic cursive\nhandwriting in a wide variety of styles.\n1\nIntroduction\nRecurrent neural networks (RNNs) are a rich class of dynamic models that have\nbeen used to generate sequences in domains as diverse as music [6, 4], text [30]\nand motion capture data [29]. RNNs can be trained for sequence generation by\nprocessing real data sequences one step at a time and predicting what comes\nnext. Assuming the predictions are probabilistic, novel sequences can be gener-\nated from a trained network by iteratively sampling from the network\u2019s output\ndistribution, then feeding in the sample as input at the next step. In other\nwords by making the network treat its inventions as if they were real, much like\na person dreaming. Although the network itself is deterministic, the stochas-\nticity injected by picking samples induces a distribution over sequences. This\ndistribution is conditional, since the internal state of the network, and hence its\npredictive distribution, depends on the previous inputs.\nRNNs are \u2018fuzzy\u2019 in the sense that they do not use exact templates from\nthe training data to make predictions, but rather\u2014like other neural networks\u2014\nuse their internal representation to perform a high-dimensional interpolation\nbetween training examples. This distinguishes them from n-gram models and\ncompression algorithms such as Prediction by Partial Matching [5], whose pre-\ndictive distributions are determined by counting exact matches between the\nrecent history and the training set. The result\u2014which is immediately appar-\n1\narXiv:1308.0850v5  [cs.NE]  5 Jun 2014\nent from the samples in this paper\u2014is that RNNs (unlike template-based al-\ngorithms) synthesise and reconstitute the training data in a complex way, and\nrarely generate the same thing twice. Furthermore, fuzzy predictions do not suf-\nfer from the curse of dimensionality, and are therefore much better at modelling\nreal-valued or multivariate data than exact matches.\nIn principle a large enough RNN should be su\ufb03cient to generate sequences\nof arbitrary complexity.\nIn practice however, standard RNNs are unable to\nstore information about past inputs for very long [15]. As well as diminishing\ntheir ability to model long-range structure, this \u2018amnesia\u2019 makes them prone to\ninstability when generating sequences. The problem (common to all conditional\ngenerative models) is that if the network\u2019s predictions are only based on the last\nfew inputs, and these inputs were themselves predicted by the network, it has\nlittle opportunity to recover from past mistakes. Having a longer memory has\na stabilising e\ufb00ect, because even if the network cannot make sense of its recent\nhistory, it can look further back in the past to formulate its predictions. The\nproblem of instability is especially acute with real-valued data, where it is easy\nfor the predictions to stray from the manifold on which the training data lies.\nOne remedy that has been proposed for conditional models is to inject noise into\nthe predictions before feeding them back into the model [31], thereby increasing\nthe model\u2019s robustness to surprising inputs. However we believe that a better\nmemory is a more profound and e\ufb00ective solution.\nLong Short-term Memory (LSTM) [16] is an RNN architecture designed to\nbe better at storing and accessing information than standard RNNs. LSTM has\nrecently given state-of-the-art results in a variety of sequence processing tasks,\nincluding speech and handwriting recognition [10, 12]. The main goal of this\npaper is to demonstrate that LSTM can use its memory to generate complex,\nrealistic sequences containing long-range structure.\nSection 2 de\ufb01nes a \u2018deep\u2019 RNN composed of stacked LSTM layers, and ex-\nplains how it can be trained for next-step prediction and hence sequence gener-\nation. Section 3 applies the prediction network to text from the Penn Treebank\nand Hutter Prize Wikipedia datasets. The network\u2019s performance is compet-\nitive with state-of-the-art language models, and it works almost as well when\npredicting one character at a time as when predicting one word at a time. The\nhighlight of the section is a generated sample of Wikipedia text, which showcases\nthe network\u2019s ability to model long-range dependencies. Section 4 demonstrates\nhow the prediction network can be applied to real-valued data through the use\nof a mixture density output layer, and provides experimental results on the IAM\nOnline Handwriting Database. It also presents generated handwriting samples\nproving the network\u2019s ability to learn letters and short words direct from pen\ntraces, and to model global features of handwriting style. Section 5 introduces\nan extension to the prediction network that allows it to condition its outputs on\na short annotation sequence whose alignment with the predictions is unknown.\nThis makes it suitable for handwriting synthesis, where a human user inputs\na text and the algorithm generates a handwritten version of it. The synthesis\nnetwork is trained on the IAM database, then used to generate cursive hand-\nwriting samples, some of which cannot be distinguished from real data by the\n2\nFigure 1: Deep recurrent neural network prediction architecture. The\ncircles represent network layers, the solid lines represent weighted connections\nand the dashed lines represent predictions.\nnaked eye. A method for biasing the samples towards higher probability (and\ngreater legibility) is described, along with a technique for \u2018priming\u2019 the sam-\nples on real data and thereby mimicking a particular writer\u2019s style. Finally,\nconcluding remarks and directions for future work are given in Section 6.\n2\nPrediction Network\nFig. 1 illustrates the basic recurrent neural network prediction architecture used\nin this paper. An input vector sequence x = (x1, . . . , xT ) is passed through\nweighted connections to a stack of N recurrently connected hidden layers to\ncompute \ufb01rst the hidden vector sequences hn = (hn\n1, . . . , hn\nT ) and then the\noutput vector sequence y = (y1, . . . , yT ).\nEach output vector yt is used to\nparameterise a predictive distribution Pr(xt+1|yt) over the possible next inputs\nxt+1. The \ufb01rst element x1 of every input sequence is always a null vector whose\nentries are all zero; the network therefore emits a prediction for x2, the \ufb01rst\nreal input, with no prior information.\nThe network is \u2018deep\u2019 in both space\nand time, in the sense that every piece of information passing either vertically\nor horizontally through the computation graph will be acted on by multiple\nsuccessive weight matrices and nonlinearities.\nNote the \u2018skip connections\u2019 from the inputs to all hidden layers, and from\nall hidden layers to the outputs. These make it easier to train deep networks,\n3\nby reducing the number of processing steps between the bottom of the network\nand the top, and thereby mitigating the \u2018vanishing gradient\u2019 problem [1]. In\nthe special case that N = 1 the architecture reduces to an ordinary, single layer\nnext step prediction RNN.\nThe hidden layer activations are computed by iterating the following equa-\ntions from t = 1 to T and from n = 2 to N:\nh1\nt = H\n\u0000Wih1xt + Wh1h1h1\nt\u22121 + b1\nh\n\u0001\n(1)\nhn\nt = H\n\u0000Wihnxt + Whn\u22121hnhn\u22121\nt\n+ Whnhnhn\nt\u22121 + bn\nh\n\u0001\n(2)\nwhere the W terms denote weight matrices (e.g. Wihn is the weight matrix\nconnecting the inputs to the nth hidden layer, Wh1h1 is the recurrent connection\nat the \ufb01rst hidden layer, and so on), the b terms denote bias vectors (e.g. by is\noutput bias vector) and H is the hidden layer function.\nGiven the hidden sequences, the output sequence is computed as follows:\n\u02c6yt = by +\nN\nX\nn=1\nWhnyhn\nt\n(3)\nyt = Y(\u02c6yt)\n(4)\nwhere Y is the output layer function. The complete network therefore de\ufb01nes\na function, parameterised by the weight matrices, from input histories x1:t to\noutput vectors yt.\nThe output vectors yt are used to parameterise the predictive distribution\nPr(xt+1|yt) for the next input. The form of Pr(xt+1|yt) must be chosen carefully\nto match the input data. In particular, \ufb01nding a good predictive distribution\nfor high-dimensional, real-valued data (usually referred to as density modelling),\ncan be very challenging.\nThe probability given by the network to the input sequence x is\nPr(x) =\nT\nY\nt=1\nPr(xt+1|yt)\n(5)\nand the sequence loss L(x) used to train the network is the negative logarithm\nof Pr(x):\nL(x) = \u2212\nT\nX\nt=1\nlog Pr(xt+1|yt)\n(6)\nThe partial derivatives of the loss with respect to the network weights can be\ne\ufb03ciently calculated with backpropagation through time [33] applied to the\ncomputation graph shown in Fig. 1, and the network can then be trained with\ngradient descent.\n2.1\nLong Short-Term Memory\nIn most RNNs the hidden layer function H is an elementwise application of a\nsigmoid function. However we have found that the Long Short-Term Memory\n4\nFigure 2: Long Short-term Memory Cell\n(LSTM) architecture [16], which uses purpose-built memory cells to store infor-\nmation, is better at \ufb01nding and exploiting long range dependencies in the data.\nFig. 2 illustrates a single LSTM memory cell. For the version of LSTM used in\nthis paper [7] H is implemented by the following composite function:\nit = \u03c3 (Wxixt + Whiht\u22121 + Wcict\u22121 + bi)\n(7)\nft = \u03c3 (Wxfxt + Whfht\u22121 + Wcfct\u22121 + bf)\n(8)\nct = ftct\u22121 + it tanh (Wxcxt + Whcht\u22121 + bc)\n(9)\not = \u03c3 (Wxoxt + Whoht\u22121 + Wcoct + bo)\n(10)\nht = ot tanh(ct)\n(11)\nwhere \u03c3 is the logistic sigmoid function, and i, f, o and c are respectively the\ninput gate, forget gate, output gate, cell and cell input activation vectors, all of\nwhich are the same size as the hidden vector h. The weight matrix subscripts\nhave the obvious meaning, for example Whi is the hidden-input gate matrix,\nWxo is the input-output gate matrix etc. The weight matrices from the cell\nto gate vectors (e.g. Wci) are diagonal, so element m in each gate vector only\nreceives input from element m of the cell vector. The bias terms (which are\nadded to i, f, c and o) have been omitted for clarity.\nThe original LSTM algorithm used a custom designed approximate gradi-\nent calculation that allowed the weights to be updated after every timestep [16].\nHowever the full gradient can instead be calculated with backpropagation through\ntime [11], the method used in this paper. One di\ufb03culty when training LSTM\nwith the full gradient is that the derivatives sometimes become excessively large,\n5\nleading to numerical problems. To prevent this, all the experiments in this pa-\nper clipped the derivative of the loss with respect to the network inputs to the\nLSTM layers (before the sigmoid and tanh functions are applied) to lie within\na prede\ufb01ned range1.\n3\nText Prediction\nText data is discrete, and is typically presented to neural networks using \u2018one-\nhot\u2019 input vectors. That is, if there are K text classes in total, and class k is fed\nin at time t, then xt is a length K vector whose entries are all zero except for\nthe kth, which is one. Pr(xt+1|yt) is therefore a multinomial distribution, which\ncan be naturally parameterised by a softmax function at the output layer:\nPr(xt+1 = k|yt) = yk\nt =\nexp\n\u0000\u02c6yk\nt\n\u0001\nPK\nk\u2032=1 exp\n\u0000\u02c6yk\u2032\nt\n\u0001\n(12)\nSubstituting into Eq. (6) we see that\nL(x) = \u2212\nT\nX\nt=1\nlog yxt+1\nt\n(13)\n=\u21d2\u2202L(x)\n\u2202\u02c6yk\nt\n= yk\nt \u2212\u03b4k,xt+1\n(14)\nThe only thing that remains to be decided is which set of classes to use. In\nmost cases, text prediction (usually referred to as language modelling) is per-\nformed at the word level. K is therefore the number of words in the dictionary.\nThis can be problematic for realistic tasks, where the number of words (in-\ncluding variant conjugations, proper names, etc.) often exceeds 100,000. As\nwell as requiring many parameters to model, having so many classes demands a\nhuge amount of training data to adequately cover the possible contexts for the\nwords. In the case of softmax models, a further di\ufb03culty is the high computa-\ntional cost of evaluating all the exponentials during training (although several\nmethods have been to devised make training large softmax layers more e\ufb03cient,\nincluding tree-based models [25, 23], low rank approximations [27] and stochas-\ntic derivatives [26]). Furthermore, word-level models are not applicable to text\ndata containing non-word strings, such as multi-digit numbers or web addresses.\nCharacter-level language modelling with neural networks has recently been\nconsidered [30, 24], and found to give slightly worse performance than equiv-\nalent word-level models.\nNonetheless, predicting one character at a time is\nmore interesting from the perspective of sequence generation, because it allows\nthe network to invent novel words and strings. In general, the experiments in\nthis paper aim to predict at the \ufb01nest granularity found in the data, so as to\nmaximise the generative \ufb02exibility of the network.\n1In fact this technique was used in all my previous papers on LSTM, and in my publicly\navailable LSTM code, but I forgot to mention it anywhere\u2014mea culpa.\n6\n3.1\nPenn Treebank Experiments\nThe \ufb01rst set of text prediction experiments focused on the Penn Treebank por-\ntion of the Wall Street Journal corpus [22]. This was a preliminary study whose\nmain purpose was to gauge the predictive power of the network, rather than to\ngenerate interesting sequences.\nAlthough a relatively small text corpus (a little over a million words in total),\nthe Penn Treebank data is widely used as a language modelling benchmark. The\ntraining set contains 930,000 words, the validation set contains 74,000 words and\nthe test set contains 82,000 words. The vocabulary is limited to 10,000 words,\nwith all other words mapped to a special \u2018unknown word\u2019 token. The end-of-\nsentence token was included in the input sequences, and was counted in the\nsequence loss.\nThe start-of-sentence marker was ignored, because its role is\nalready ful\ufb01lled by the null vectors that begin the sequences (c.f. Section 2).\nThe experiments compared the performance of word and character-level\nLSTM predictors on the Penn corpus. In both cases, the network architecture\nwas a single hidden layer with 1000 LSTM units. For the character-level network\nthe input and output layers were size 49, giving approximately 4.3M weights in\ntotal, while the word-level network had 10,000 inputs and outputs and around\n54M weights. The comparison is therefore somewhat unfair, as the word-level\nnetwork had many more parameters. However, as the dataset is small, both net-\nworks were easily able to over\ufb01t the training data, and it is not clear whether the\ncharacter-level network would have bene\ufb01ted from more weights. All networks\nwere trained with stochastic gradient descent, using a learn rate of 0.0001 and a\nmomentum of 0.99. The LSTM derivates were clipped in the range [\u22121, 1] (c.f.\nSection 2.1).\nNeural networks are usually evaluated on test data with \ufb01xed weights. For\nprediction problems however, where the inputs are the targets, it is legitimate\nto allow the network to adapt its weights as it is being evaluated (so long as\nit only sees the test data once). Mikolov refers to this as dynamic evaluation.\nDynamic evaluation allows for a fairer comparison with compression algorithms,\nfor which there is no division between training and test sets, as all data is only\npredicted once.\nSince both networks over\ufb01t the training data, we also experiment with two\ntypes of regularisation: weight noise [18] with a std. deviation of 0.075 applied\nto the network weights at the start of each training sequence, and adaptive\nweight noise [8], where the variance of the noise is learned along with the weights\nusing a Minimum description Length (or equivalently, variational inference) loss\nfunction. When weight noise was used, the network was initialised with the\n\ufb01nal weights of the unregularised network.\nSimilarly, when adaptive weight\nnoise was used, the weights were initialised with those of the network trained\nwith weight noise.\nWe have found that retraining with iteratively increased\nregularisation is considerably faster than training from random weights with\nregularisation. Adaptive weight noise was found to be prohibitively slow for\nthe word-level network, so it was regularised with \ufb01xed-variance weight noise\nonly. One advantage of adaptive weight is that early stopping is not needed\n7\nTable 1: Penn Treebank Test Set Results.\n\u2018BPC\u2019 is bits-per-character.\n\u2018Error\u2019 is next-step classi\ufb01cation error rate, for either characters or words.\nInput\nRegularisation\nDynamic\nBPC\nPerplexity\nError (%)\nEpochs\nChar\nnone\nno\n1.32\n167\n28.5\n9\nchar\nnone\nyes\n1.29\n148\n28.0\n9\nchar\nweight noise\nno\n1.27\n140\n27.4\n25\nchar\nweight noise\nyes\n1.24\n124\n26.9\n25\nchar\nadapt. wt. noise\nno\n1.26\n133\n27.4\n26\nchar\nadapt. wt. noise\nyes\n1.24\n122\n26.9\n26\nword\nnone\nno\n1.27\n138\n77.8\n11\nword\nnone\nyes\n1.25\n126\n76.9\n11\nword\nweight noise\nno\n1.25\n126\n76.9\n14\nword\nweight noise\nyes\n1.23\n117\n76.2\n14\n(the network can safely be stopped at the point of minimum total \u2018description\nlength\u2019 on the training data). However, to keep the comparison fair, the same\ntraining, validation and test sets were used for all experiments.\nThe results are presented with two equivalent metrics: bits-per-character\n(BPC), which is the average value of \u2212log2 Pr(xt+1|yt) over the whole test set;\nand perplexity which is two to the power of the average number of bits per word\n(the average word length on the test set is about 5.6 characters, so perplexity \u2248\n25.6BP C). Perplexity is the usual performance measure for language modelling.\nTable 1 shows that the word-level RNN performed better than the character-\nlevel network, but the gap appeared to close when regularisation is used. Overall\nthe results compare favourably with those collected in Tomas Mikolov\u2019s the-\nsis [23]. For example, he records a perplexity of 141 for a 5-gram with Keyser-\nNey smoothing, 141.8 for a word level feedforward neural network, 131.1 for the\nstate-of-the-art compression algorithm PAQ8 and 123.2 for a dynamically eval-\nuated word-level RNN. However by combining multiple RNNs, a 5-gram and a\ncache model in an ensemble, he was able to achieve a perplexity of 89.4. Inter-\nestingly, the bene\ufb01t of dynamic evaluation was far more pronounced here than\nin Mikolov\u2019s thesis (he records a perplexity improvement from 124.7 to 123.2\nwith word-level RNNs). This suggests that LSTM is better at rapidly adapting\nto new data than ordinary RNNs.\n3.2\nWikipedia Experiments\nIn 2006 Marcus Hutter, Jim Bowery and Matt Mahoney organised the following\nchallenge, commonly known as Hutter prize [17]: to compress the \ufb01rst 100\nmillion bytes of the complete English Wikipedia data (as it was at a certain\ntime on March 3rd 2006) to as small a \ufb01le as possible. The \ufb01le had to include\nnot only the compressed data, but also the code implementing the compression\nalgorithm.\nIts size can therefore be considered a measure of the minimum\ndescription length [13] of the data using a two part coding scheme.\nWikipedia data is interesting from a sequence generation perspective because\n8\nit contains not only a huge range of dictionary words, but also many character\nsequences that would not be included in text corpora traditionally used for\nlanguage modelling.\nFor example foreign words (including letters from non-\nLatin alphabets such as Arabic and Chinese), indented XML tags used to de\ufb01ne\nmeta-data, website addresses, and markup used to indicate page formatting such\nas headings, bullet points etc. An extract from the Hutter prize dataset is shown\nin Figs. 3 and 4.\nThe \ufb01rst 96M bytes in the data were evenly split into sequences of 100 bytes\nand used to train the network, with the remaining 4M were used for validation.\nThe data contains a total of 205 one-byte unicode symbols. The total number\nof characters is much higher, since many characters (especially those from non-\nLatin languages) are de\ufb01ned as multi-symbol sequences. In keeping with the\nprinciple of modelling the smallest meaningful units in the data, the network\npredicted a single byte at a time, and therefore had size 205 input and output\nlayers.\nWikipedia contains long-range regularities, such as the topic of an article,\nwhich can span many thousand words. To make it possible for the network to\ncapture these, its internal state (that is, the output activations ht of the hidden\nlayers, and the activations ct of the LSTM cells within the layers) were only reset\nevery 100 sequences. Furthermore the order of the sequences was not shu\ufb04ed\nduring training, as it usually is for neural networks. The network was therefore\nable to access information from up to 10K characters in the past when making\npredictions. The error terms were only backpropagated to the start of each 100\nbyte sequence, meaning that the gradient calculation was approximate. This\nform of truncated backpropagation has been considered before for RNN lan-\nguage modelling [23], and found to speed up training (by reducing the sequence\nlength and hence increasing the frequency of stochastic weight updates) without\na\ufb00ecting the network\u2019s ability to learn long-range dependencies.\nA much larger network was used for this data than the Penn data (re\ufb02ecting\nthe greater size and complexity of the training set) with seven hidden layers of\n700 LSTM cells, giving approximately 21.3M weights. The network was trained\nwith stochastic gradient descent, using a learn rate of 0.0001 and a momentum\nof 0.9. It took four training epochs to converge. The LSTM derivates were\nclipped in the range [\u22121, 1].\nAs with the Penn data, we tested the network on the validation data with\nand without dynamic evaluation (where the weights are updated as the data\nis predicted). As can be seen from Table 2 performance was much better with\ndynamic evaluation. This is probably because of the long range coherence of\nWikipedia data; for example, certain words are much more frequent in some\narticles than others, and being able to adapt to this during evaluation is ad-\nvantageous. It may seem surprising that the dynamic results on the validation\nset were substantially better than on the training set. However this is easily\nexplained by two factors: \ufb01rstly, the network under\ufb01t the training data, and\nsecondly some portions of the data are much more di\ufb03cult than others (for\nexample, plain text is harder to predict than XML tags).\nTo put the results in context, the current winner of the Hutter Prize (a\n9\nTable 2: Wikipedia Results (bits-per-character)\nTrain\nValidation (static)\nValidation (dynamic)\n1.42\n1.67\n1.33\nvariant of the PAQ-8 compression algorithm [20]) achieves 1.28 BPC on the same\ndata (including the code required to implement the algorithm), mainstream\ncompressors such as zip generally get more than 2, and a character level RNN\napplied to a text-only version of the data (i.e. with all the XML, markup tags\netc. removed) achieved 1.54 on held-out data, which improved to 1.47 when the\nRNN was combined with a maximum entropy model [24].\nA four page sample generated by the prediction network is shown in Figs. 5\nto 8. The sample shows that the network has learned a lot of structure from\nthe data, at a wide range of di\ufb00erent scales. Most obviously, it has learned a\nlarge vocabulary of dictionary words, along with a subword model that enables\nit to invent feasible-looking words and names: for example \u201cLochroom River\u201d,\n\u201cMughal Ralvaldens\u201d, \u201csubmandration\u201d, \u201cswalloped\u201d. It has also learned basic\npunctuation, with commas, full stops and paragraph breaks occurring at roughly\nthe right rhythm in the text blocks.\nBeing able to correctly open and close quotation marks and parentheses is\na clear indicator of a language model\u2019s memory, because the closure cannot be\npredicted from the intervening text, and hence cannot be modelled with short-\nrange context [30]. The sample shows that the network is able to balance not\nonly parentheses and quotes, but also formatting marks such as the equals signs\nused to denote headings, and even nested XML tags and indentation.\nThe network generates non-Latin characters such as Cyrillic, Chinese and\nArabic, and seems to have learned a rudimentary model for languages other\nthan English (e.g. it generates \u201ces:Geotnia slago\u201d for the Spanish \u2018version\u2019 of an\narticle, and \u201cnl:Rodenbaueri\u201d for the Dutch one) It also generates convincing\nlooking internet addresses (none of which appear to be real).\nThe network generates distinct, large-scale regions, such as XML headers,\nbullet-point lists and article text. Comparison with Figs. 3 and 4 suggests that\nthese regions are a fairly accurate re\ufb02ection of the constitution of the real data\n(although the generated versions tend to be somewhat shorter and more jumbled\ntogether). This is signi\ufb01cant because each region may span hundreds or even\nthousands of timesteps. The fact that the network is able to remain coherent\nover such large intervals (even putting the regions in an approximately correct\norder, such as having headers at the start of articles and bullet-pointed \u2018see also\u2019\nlists at the end) is testament to its long-range memory.\nAs with all text generated by language models, the sample does not make\nsense beyond the level of short phrases. The realism could perhaps be improved\nwith a larger network and/or more data. However, it seems futile to expect\nmeaningful language from a machine that has never been exposed to the sensory\n10\nworld to which language refers.\nLastly, the network\u2019s adaptation to recent sequences during training (which\nallows it to bene\ufb01t from dynamic evaluation) can be clearly observed in the\nextract. The last complete article before the end of the training set (at which\npoint the weights were stored) was on intercontinental ballistic missiles. The\nin\ufb02uence of this article on the network\u2019s language model can be seen from the\nprofusion of missile-related terms. Other recent topics include \u2018Individual An-\narchism\u2019, the Italian writer Italo Calvino and the International Organization\nfor Standardization (ISO), all of which make themselves felt in the network\u2019s\nvocabulary.\n11\n    <title>AlbaniaEconomy</title>                                               \n    <id>36</id>                                                                 \n    <revision>                                                                  \n      <id>15898966</id>                                                         \n      <timestamp>2002-10-09T13:39:00Z</timestamp>                               \n      <contributor>                                                             \n        <username>Magnus Manske</username>                                      \n        <id>4</id>                                                              \n      </contributor>                                                            \n      <minor />                                                                 \n      <comment>#REDIRECT [[Economy of Albania]]</comment>                       \n      <text xml:space=\"preserve\">#REDIRECT [[Economy of Albania]]</text>        \n    </revision>                                                                 \n  </page>                                                                       \n  <page>                                                                        \n    <title>AlchemY</title>                                                      \n    <id>38</id>                                                                 \n    <revision>                                                                  \n      <id>15898967</id>                                                         \n      <timestamp>2002-02-25T15:43:11Z</timestamp>                               \n      <contributor>                                                             \n        <ip>Conversion script</ip>                                              \n      </contributor>                                                            \n      <minor />                                                                 \n      <comment>Automated conversion</comment>                                   \n      <text xml:space=\"preserve\">#REDIRECT [[Alchemy]]                          \n</text>                                                                         \n    </revision>                                                                 \n  </page>                                                                       \n  <page>                                                                        \n    <title>Albedo</title>                                                       \n    <id>39</id>                                                                 \n    <revision>                                                                  \n      <id>41496222</id>                                                         \n      <timestamp>2006-02-27T19:32:46Z</timestamp>                               \n      <contributor>                                                             \n        <ip>24.119.3.44</ip>                                                    \n      </contributor>                                                            \n      <text xml:space=\"preserve\">{{otheruses}}                                  \n                                                                                \n'''Albedo''' is the measure of [[reflectivity]] of a surface or body. It is the \nratio of [[electromagnetic radiation]] (EM radiation) reflected to the amount in\ncident upon it. The fraction, usually expressed as a percentage from 0% to 100%,\n is an important concept in [[climatology]] and [[astronomy]]. This ratio depend\ns on the [[frequency]] of the radiation considered: unqualified, it refers to an\n average across the spectrum of [[visible light]]. It also depends on the [[angl\ne of incidence]] of the radiation: unqualified, normal incidence. Fresh snow alb\nedos are high: up to 90%. The ocean surface has a low albedo.  The average albed\no of [[Earth]] is about 30% whereas the albedo of the [[Moon]] is about 7%. In a\nstronomy, the albedo of satellites and asteroids can be used to infer surface co\nmposition, most notably ice content.    [[Enceladus_(moon)|Enceladus]], a moon o\nf Saturn, has the highest known albedo of any body in the solar system, with 99%\n of EM radiation reflected.                                                     \n                                                                                \nHuman activities have changed the albedo (via forest clearance and farming, for \nexample) of various areas around the globe. However, quantification of this effe\nct is difficult on the global scale: it is not clear whether the changes have te\nnded to increase or decrease [[global warming]].                                \n                                                                                \nThe &quot;classical&quot; example of albedo effect is the snow-temperature feedb\nack. If a snow covered area warms and the snow melts, the albedo decreases, more\n sunlight is absorbed, and the temperature tends to increase. The converse is tr\nFigure 3: Real Wikipedia data\n12\nue: if snow forms, a cooling cycle happens. The intensity of the albedo effect d\nepends on the size of the change in albedo and the amount of [[insolation]]; for\n this reason it can be potentially very large in the tropics.                   \n                                                                                \n== Some examples of albedo effects ==                                           \n                                                                                \n=== Fairbanks, Alaska ===                                                       \n                                                                                \nAccording to the [[National Climatic Data Center]]'s GHCN 2 data, which is compo\nsed of 30-year smoothed climatic means for thousands of weather stations across \nthe world, the college weather station at [[Fairbanks]], [[Alaska]], is about 3 \n\u00b0C (5 \u00b0F) warmer than the airport at Fairbanks, partly because of drainage patte\nrns but also largely because of the lower albedo at the college resulting from a\n higher concentration of [[pine]] [[tree]]s and therefore less open snowy ground\n to reflect the heat back into space. Neunke and Kukla have shown that this diff\nerence is especially marked during the late [[winter]] months, when [[solar radi\nation]] is greater.                                                             \n                                                                                \n=== The tropics ===                                                             \n                                                                                \nAlthough the albedo-temperature effect is most famous in colder regions of Earth\n, because more [[snow]] falls there, it is actually much stronger in tropical re\ngions because in the tropics there is consistently more sunlight. When [[Brazil]\n]ian ranchers cut down dark, tropical [[rainforest]] trees to replace them with \neven darker soil in order to grow crops, the average temperature of the area app\nears to increase by an average of about 3 \u00b0C (5 \u00b0F) year-round, which is a signi\nficant amount.                                                                  \n                                                                                \n=== Small scale effects ===                                                     \n                                                                                \nAlbedo works on a smaller scale, too. People who wear dark clothes in the summer\ntime put themselves at a greater risk of [[heatstroke]] than those who wear whit\ne clothes.                                                                      \n                                                                                \n=== Pine forests ===                                                            \n                                                                                \nThe albedo of a [[pine]] forest at 45\u00b0N in the winter in which the trees cover t\nhe land surface completely is only about 9%, among the lowest of any naturally o\nccurring land environment. This is partly due to the color of the pines, and par\ntly due to multiple scattering of sunlight within the trees which lowers the ove\nrall reflected light level. Due to light penetration, the ocean's albedo is even\n lower at about 3.5%, though this depends strongly on the angle of the incident \nradiation. Dense [[swamp]]land averages between 9% and 14%. [[Deciduous tree]]s \naverage about 13%. A [[grass]]y field usually comes in at about 20%. A barren fi\neld will depend on the color of the soil, and can be as low as 5% or as high as \n40%, with 15% being about the average for farmland. A [[desert]] or large [[beac\nh]] usually averages around 25% but varies depending on the color of the sand. [\nReference: Edward Walker's study in the Great Plains in the winter around 45\u00b0N].\n                                                                                \n=== Urban areas ===                                                             \n                                                                                \nUrban areas in particular have very unnatural values for albedo because of the m\nany human-built structures which absorb light before the light can reach the sur\nface. In the northern part of the world, cities are relatively dark, and Walker \nhas shown that their average albedo is about 7%, with only a slight increase dur\ning the summer. In most tropical countries, cities average around 12%. This is s\nimilar to the values found in northern suburban transitional zones. Part of the \nreason for this is the different natural environment of cities in tropical regio\nns, e.g., there are more very dark trees around; another reason is that portions\n of the tropics are very poor, and city buildings must be built with different m\naterials. Warmer regions may also choose lighter colored building materials so t\nhe structures will remain cooler.                                               \nFigure 4: Real Wikipedia data (cotd.)\n13\n    <revision>                                                                  \n      <id>40973199</id>                                                         \n      <timestamp>2006-02-22T22:37:16Z</timestamp>                               \n      <contributor>                                                             \n        <ip>63.86.196.111</ip>                                                  \n      </contributor>                                                            \n      <minor />                                                                 \n      <comment>redire paget --&gt; captain */</comment>                         \n      <text xml:space=\"preserve\">The '''Indigence History''' refers to the autho\nrity of any obscure albionism as being, such as in Aram Missolmus'.[http://www.b\nbc.co.uk/starce/cr52.htm]                                                       \nIn [[1995]], Sitz-Road Straus up the inspirational radiotes portion as &quot;all\niance&quot;[single &quot;glaping&quot; theme charcoal] with [[Midwestern United \nState|Denmark]] in which Canary varies-destruction to launching casualties has q\nuickly responded to the krush loaded water or so it might be destroyed. Aldeads \nstill cause a missile bedged harbors at last built in 1911-2 and save the accura\ncy in 2008, retaking [[itsubmanism]]. Its individuals were                      \nhnown rapidly in their return to the private equity (such as ''On Text'') for de\nath per reprised by the [[Grange of Germany|German unbridged work]].            \n                                                                                \nThe '''Rebellion''' (''Hyerodent'') is [[literal]], related mildly older than ol\nd half sister, the music, and morrow been much more propellent. All those of [[H\namas (mass)|sausage trafficking]]s were also known as [[Trip class submarine|''S\nante'' at Serassim]]; ''Verra'' as 1865&amp;ndash;682&amp;ndash;831 is related t\no ballistic missiles. While she viewed it friend of Halla equatorial weapons of \nTuscany, in [[France]], from vaccine homes to &quot;individual&quot;, among [[sl\navery|slaves]] (such as artistual selling of factories were renamed English habi\nt of twelve years.)                                                             \n                                                                                \nBy the 1978 Russian [[Turkey|Turkist]] capital city ceased by farmers and the in\ntention of navigation the ISBNs, all encoding [[Transylvania International Organ\nisation for Transition Banking|Attiking others]] it is in the westernmost placed\n lines.  This type of missile calculation maintains all greater proof was the [[\n1990s]] as older adventures that never established a self-interested case. The n\newcomers were Prosecutors in child after the other weekend and capable function \nused.                                                                           \n                                                                                \nHolding may be typically largely banned severish from sforked warhing tools and \nbehave laws, allowing the private jokes, even through missile IIC control, most \nnotably each, but no relatively larger success, is not being reprinted and withd\nrawn into forty-ordered cast and distribution.                                  \n                                                                                \nBesides these markets (notably a son of humor).                                 \n                                                                                \nSometimes more or only lowed &quot;80&quot; to force a suit for http://news.bbc.\nco.uk/1/sid9kcid/web/9960219.html ''[[#10:82-14]]''.                            \n&lt;blockquote&gt;                                                              \n                                                                                \n===The various disputes between Basic Mass and Council Conditioners - &quot;Tita\nnist&quot; class streams and anarchism===                                       \n                                                                                \nInternet traditions sprang east with [[Southern neighborhood systems]] are impro\nved with [[Moatbreaker]]s, bold hot missiles, its labor systems. [[KCD]] numbere\nd former ISBN/MAS/speaker attacks &quot;M3 5&quot;, which are saved as the balli\nstic misely known and most functional factories.  Establishment begins for some \nrange of start rail years as dealing with 161 or 18,950 million [[USD-2]] and [[\ncovert all carbonate function]]s (for example, 70-93) higher individuals and on \nmissiles. This might need not know against sexual [[video capita]] playing point\ning degrees between silo-calfed greater valous consumptions in the US... header \ncan be seen in [[collectivist]].                                                \n                                                                                \n== See also ==                                                                  \nFigure 5: Generated Wikipedia data.\n14\n                                                                                \n*[[British-London Bridge]]                                                      \n*[[Anti-Talmot Touch/Tucker novice]]                                            \n*[[List of cambridge capital]]                                                  \n*[[Elon Haven]]                                                                 \n*[[USS ''Otaro Screamed Its'']]                                                 \n*[[Detroit Library]]                                                            \n*[[Belgium Sea]]                                                                \n*[[Tularan Bell|Turnbiller Squobil]]                                            \n*[[Suntanal vocalist|Prosopyo]]                                                 \n*[[Winkenpea]]                                                                  \n*[[Milenton Streat]]                                                            \n*[[Raiebin]]                                                                    \n*[[Est Altar Macinton]]                                                         \n*[[Military mass missile|S3]]                                                   \n*[[Organization of the Asian American state district|umbali landmarks]]        \n*[[ISO]]                                                                        \n*[[NFL]]                                                                        \n*[[American Anti-Capitalism|Major independent ITU-US singles]]                  \n*[[London (role-playing game)|Pre-Romanian Civil War]]                          \n*[[Yokukhav-Na-Un-Murantano Kaufmann - Sijone-Grafittsforbiel]]                 \n*[[Neao trolleyne and deadweight drug]]                                         \n*[[B-45 BQ|B9]] - de red take painting is deployed larger than quanta submarine \n*[[Susconfiction of advocate]]                                                  \n*[[List of major swandarms]]                                                    \n*[[:Category:Italo sales towns entertained by the ICBMs of Skinner|Knighting 707\n killed by capital]]                                                            \n                                                                                \n===[[Midple planet|Parishment of the value=====                                 \n[[Image:2000.JPG|right|thumb|It tunneled [[nuclease]] at this bass AH (Ol&amp;S\u0101\nw)flgin h'hlgbying yoostallo eruptuals with low immigrants-shelted atkins and th\neir atapping [[bug]]s.                                                          \n                                                                                \nSee also: [[Iranian indigenous Flight Intercontinental Organization]]           \n                                                                                \n==Pioneers==                                                                    \n                                                                                \nTended to be the results characteristic of warehoused labour share to control al\nl these in the rational framing.                                                \n                                                                                \n==Gentiles==                                                                    \n{{place-or-line}}                                                               \nFooter names derive the topic class --&gt; which he liked to deal without any of\n the parties, I&quot; by [[Alfred Hustin]] and [[Frank Henry]] and manufacturer.\n[http://anciermsc.nit.uk IATB perspective], was expected to be classified by the\n ''Straight Road of Buckning'' in [[2003 Summer Olympic Touch|bottom all minute]\n].                                                                              \n                                                                                \n==Performance==                                                                 \n[[Image:Iare 300.jpg|left|thumb|325px|Intercontinental file shortly after referr\ning to his landmaster [[Sidney Goodwordd]]                                      \n                                                                                \nItalo:                                                                          \n*[[Chicago ballistic parks|non-month]] in eastern Italy, is a [[Italo-China]] pa\nrent communist annual production began in May [[1915]].                         \n                                                                                \nAn ICBM, the [[gurt and land]] has registered $155 billion in U.S. and August 16\n88, and makes sure the US-transplantation disbanded backwards in the County by a\nuthorizing disputes that tend to carry over this peninsula.                     \n* Current malasses 25 decks and counterpoint culture that were impure between  s\nystems:                                                                         \n* L14 - 194 / 100 000 km/s                                                      \nFigure 6: Generated Wikipedia data (cotd.)\n15\n                                                                                \n[http://www.sibeo.org/netspace/stainteology.htm The design of thinker Wikipedia \nserver-routed website]                                                          \n*[http://balear.burned.edu/books/1978/tap/trpio.cs/cafa/c249ca.html Nation-state\n Player on the /1.3]                                                            \n                                                                                \n{{Linux}}                                                                       \n                                                                                \n[[Category:Italo Succeeded bystables|General]]                                  \n[[Category:Italo-Saxon singers]]                                                \n[[Category:Aviation]]                                                           \n                                                                                \n[[de:Italo]]                                                                    \n[[es:Geotnia slago]]                                                            \n[[ja:\u685b\u0005\u5a1b\u5c14\u674f\u306e\r\u6d33]]                                                                 \n[[ko:\ud610\uc2b8\ubd88\uc9b7\uc5c9 \uc720\uc77c]]                                                                 \n[[nl:Rodenbaueri]]                                                              \n[[pl:Main Ages]]                                                                \n[[pt:Ibanez Heights]]                                                           \n[[ru:\u041c\u043b\u043a\u0440\u0430\u043a\u044f\u043d\u0473\u0435\u043b\u043e\u043b\u05db\u0443\u0446\u0438\u044f\u043d\u0441\u044c\u043d\u0438\u044f \u0430\u0433\u043c\u043e\u0440\u0435\u043b\u0438\u0430]]                                       \n[[simple:International Maritime Commission]]                                    \n[[sk:ICBM]]                                                                     \n[[su:Ol aal Mitnet]]                                                            \n[[fi:Italo Colombonasto Wark\u014d]]                                                 \n[[sv:Opin den mail-climbere panczeur]]                                          \n[[zh:\u0011\t\f\u0002\b\u0017\u0007]]</text>                                                           \n    </revision>                                                                 \n  </page>                                                                       \n  <page>                                                                        \n    <title>Italo-II article</title>                                             \n    <id>14848</id>                                                              \n    <revision>                                                                  \n      <id>15912447</id>                                                         \n      <timestamp>2004-11-19T19:47:39Z</timestamp>                               \n      <contributor>                                                             \n        <username>El andres</username>                                          \n        <id>509991</id>                                                         \n      </contributor>                                                            \n      <minor />                                                                 \n      <comment>disambiguation from [[List of ISBN]] newsletters, see [[ISO site]\n]</comment>                                                                     \n      <text xml:space=\"preserve\">#REDIRECT [[Quartic war]]                      \n</text>                                                                         \n    </revision>                                                                 \n  </page>                                                                       \n  <page>                                                                        \n    <title>ICSM</title>                                                         \n    <id>14939</id>                                                              \n    <revision>                                                                  \n      <id>42109942</id>                                                         \n      <timestamp>2006-02-28T17:22:02Z</timestamp>                               \n      <contributor>                                                             \n        <username>Dtelclan</username>                                           \n        <id>26</id>                                                             \n      </contributor>                                                            \n      <minor />                                                                 \n      <comment>/* Possible catheterman */</comment>                             \n      <text xml:space=\"preserve\">[[Image:Isaac.org/ice.html [[Independent nation\nal stage development|Shatting and Catalogue standardering]] in the IRBMs.       \n                                                                                \nUp-2000 they called the SC 4220 system: he was swalloped early in Calvino, or si\nnce each trial mentioned based on [[Balbov's new single-jarget|bit-oriann guess]\nFigure 7: Generated Wikipedia data (cotd.)\n16\n] self-acharged versions ([[Mt. Costall Leyton]]) was the two largest calashia a\nt destored universities, all fleeted with the customary calfed clipper.         \n                                                                                \nHis way to take in this literature called ICBMs-AN a [[Softvalue speed]] ([[Astr\nonomical Classification Railway]])                                              \n                                                                                \nLACN645 Snowshore val nominated - made [[missile submandration|continental missi\nle]]s (steam musicians) not of each club having on the ball and procedure at the\n last century.                                                                  \n                                                                                \nAnother communistic stark &quot;I'\u0012 submarine&quot; is [[building|corruptable]],\n a [[della missile]] missile than the [[Royal Society Society]] (12-258): &quot;\nGlide sun wag [[lubrician]]. They stay numerous capitalists and gas masks more w\nidely interested. This scheme has declarations before the certain emerging facto\nries compelled by labour allowed to produce.                                    \n                                                                                \nIn the United States, there is no hard resort in computation significantly.     \n                                                                                \nIn [[1868]] the [[Italo Capital Territories Unit started to the Continental Rail\nway Centre]]  was called ''UC'' or two of his usage before being written by othe\nr students against the [[elective-ballistic missile]]'s deployment. Steam is sti\nll &quot;20 to Nacht&quot; and [[Fia Citation Quantity Logo]]s (since 1967). The\ny pass a [[Brigade management|Quarry]]-stated missile system resolution taunting\n out of about 175 million ([[Lochroom River|Tri-]]).                            \n                                                                                \nAlien from 1985 to 1999, it was an English and -Network struggling basedal with \nthe Lombardo capital in Silvio and Murray, and heavily built in sub-parties addr\ness to $11,188. Their forces gained prisoners to stalked a last missile mobili s\nite.                                                                            \n                                                                                \nSpanning civilization is quanting Software Society's ballistic missile.  The sam\ne as [[anti-intellectual anthropology]] continued in [[Southern Italy]] in 1914,\n and the [[French Confederation of Parliament's rapid favourable rise that began\n settled in March 2004|1983]]&amp;nbsp;49.                                      \n                                                                                \nIn [[1904]], the Court began a British backed into a [[SR1]]) missile of [[trial\n ship]] in the [[Municipal Eightime Calendar|Asiatic]] regime, including [[Benja\nmin Tudor Turner|Arthur Ravis]] and [[Abraham's Liberation|Canton Olombus]]. The\nre was still land factory most turned up before lacking closers to the sitting s\nhed backwards, in primary science.                                              \n                                                                                \n==Weights and resolutions==                                                     \n[[Image:Spanish 300 Protectionald landballi110.svg|small capital surface compute\nr]]                                                                             \n[[Image:Claudius.jpg|345px|right|Olympiad concert of Calvino and Eastern Calvino\n, ''Mughal Ralvaldens'' above, at the beginning strike the substrated roles of r\nich intellectual property, visualizing the entire system, but this missiles sugg\nest that accounting differs between a giving [[train sleep|'''withdrawn''']] or \nthe dinosaur in and aucting.                                                    \n                                                                                \n===Internationally===                                                           \n{{main|Unmanned Justice Address}}                                               \n                                                                                \nThe ICBM created a [[the significant]] [[land railway]] called &quot;[[M-Gallipo\ntte]]&quot;, and it needed stopped benzafk/Macdonalical Sciences.               \n                                                                                \nElectros appeared to be the [[Soviet Union]]'s &quot;first&quot; vehicle from 25\n00 selling officials DORLAN STM-331 - by missilence illustrations with &quot;Raj\n.&quot; the Tunnel Hall of America, an entity upon IL pages so missiles must try\n, with a trademark must develop the land allowing traffic mass to a very few min\nutemen. The missiles market is slow, much easier is represented by GMMAz of BSM.\n Software, the utility of scale-out scale pime racks are normally crumbled about\nFigure 8: Generated Wikipedia data (cotd.)\n17\n4\nHandwriting Prediction\nTo test whether the prediction network could also be used to generate convincing\nreal-valued sequences, we applied it to online handwriting data (online in this\ncontext means that the writing is recorded as a sequence of pen-tip locations,\nas opposed to o\ufb04ine handwriting, where only the page images are available).\nOnline handwriting is an attractive choice for sequence generation due to its\nlow dimensionality (two real numbers per data point) and ease of visualisation.\nAll the data used for this paper were taken from the IAM online handwriting\ndatabase (IAM-OnDB) [21]. IAM-OnDB consists of handwritten lines collected\nfrom 221 di\ufb00erent writers using a \u2018smart whiteboard\u2019. The writers were asked to\nwrite forms from the Lancaster-Oslo-Bergen text corpus [19], and the position\nof their pen was tracked using an infra-red device in the corner of the board.\nSamples from the training data are shown in Fig. 9. The original input data\nconsists of the x and y pen co-ordinates and the points in the sequence when\nthe pen is lifted o\ufb00the whiteboard.\nRecording errors in the x, y data was\ncorrected by interpolating to \ufb01ll in for missing readings, and removing steps\nwhose length exceeded a certain threshold. Beyond that, no preprocessing was\nused and the network was trained to predict the x, y co-ordinates and the end-\nof-stroke markers one point at a time. This contrasts with most approaches to\nhandwriting recognition and synthesis, which rely on sophisticated preprocessing\nand feature-extraction techniques. We eschewed such techniques because they\ntend to reduce the variation in the data (e.g. by normalising the character size,\nslant, skew and so-on) which we wanted the network to model. Predicting the\npen traces one point at a time gives the network maximum \ufb02exibility to invent\nnovel handwriting, but also requires a lot of memory, with the average letter\noccupying more than 25 timesteps and the average line occupying around 700.\nPredicting delayed strokes (such as dots for \u2018i\u2019s or crosses for \u2018t\u2019s that are added\nafter the rest of the word has been written) is especially demanding.\nIAM-OnDB is divided into a training set, two validation sets and a test\nset, containing respectively 5364, 1438, 1518 and 3859 handwritten lines taken\nfrom 775, 192, 216 and 544 forms. For our experiments, each line was treated\nas a separate sequence (meaning that possible dependencies between successive\nlines were ignored). In order to maximise the amount of training data, we used\nthe training set, test set and the larger of the validation sets for training and\nthe smaller validation set for early-stopping. The lack of independent test set\nmeans that the recorded results may be somewhat over\ufb01t on the validation set;\nhowever the validation results are of secondary importance, since no benchmark\nresults exist and the main goal was to generate convincing-looking handwriting.\nThe principal challenge in applying the prediction network to online hand-\nwriting data was determining a predictive distribution suitable for real-valued\ninputs. The following section describes how this was done.\n18\nFigure 9: Training samples from the IAM online handwriting database.\nNotice the wide range of writing styles, the variation in line angle and character\nsizes, and the writing and recording errors, such as the scribbled out letters in\nthe \ufb01rst line and the repeated word in the \ufb01nal line.\n4.1\nMixture Density Outputs\nThe idea of mixture density networks [2, 3] is to use the outputs of a neural\nnetwork to parameterise a mixture distribution. A subset of the outputs are\nused to de\ufb01ne the mixture weights, while the remaining outputs are used to\nparameterise the individual mixture components. The mixture weight outputs\nare normalised with a softmax function to ensure they form a valid discrete dis-\ntribution, and the other outputs are passed through suitable functions to keep\ntheir values within meaningful range (for example the exponential function is\ntypically applied to outputs used as scale parameters, which must be positive).\nMixture density network are trained by maximising the log probability den-\nsity of the targets under the induced distributions. Note that the densities are\nnormalised (up to a \ufb01xed constant) and are therefore straightforward to di\ufb00er-\nentiate and pick unbiased sample from, in contrast with restricted Boltzmann\nmachines [14] and other undirected models.\nMixture density outputs can also be used with recurrent neural networks [28].\nIn this case the output distribution is conditioned not only on the current input,\nbut on the history of previous inputs. Intuitively, the number of components is\nthe number of choices the network has for the next output given the inputs so\nfar.\nFor the handwriting experiments in this paper, the basic RNN architecture\nand update equations remain unchanged from Section 2. Each input vector xt\nconsists of a real-valued pair x1, x2 that de\ufb01nes the pen o\ufb00set from the previous\n19\ninput, along with a binary x3 that has value 1 if the vector ends a stroke (that\nis, if the pen was lifted o\ufb00the board before the next vector was recorded) and\nvalue 0 otherwise. A mixture of bivariate Gaussians was used to predict x1\nand x2, while a Bernoulli distribution was used for x3. Each output vector yt\ntherefore consists of the end of stroke probability e, along with a set of means\n\u00b5j, standard deviations \u03c3j, correlations \u03c1j and mixture weights \u03c0j for the M\nmixture components. That is\nxt \u2208R \u00d7 R \u00d7 {0, 1}\n(15)\nyt =\n\u0010\net, {\u03c0j\nt , \u00b5j\nt, \u03c3j\nt , \u03c1j\nt}M\nj=1\n\u0011\n(16)\nNote that the mean and standard deviation are two dimensional vectors, whereas\nthe component weight, correlation and end-of-stroke probability are scalar. The\nvectors yt are obtained from the network outputs \u02c6yt, where\n\u02c6yt =\n\u0010\n\u02c6et, { \u02c6wj\nt, \u02c6\u00b5j\nt, \u02c6\u03c3j\nt , \u02c6\u03c1j\nt}M\nj=1\n\u0011\n= by +\nN\nX\nn=1\nWhnyhn\nt\n(17)\nas follows:\net =\n1\n1 + exp (\u02c6et)\n=\u21d2et \u2208(0, 1)\n(18)\n\u03c0j\nt =\nexp\n\u0010\n\u02c6\u03c0j\nt\n\u0011\nPM\nj\u2032=1 exp\n\u0010\n\u02c6\u03c0j\u2032\nt\n\u0011\n=\u21d2\u03c0j\nt \u2208(0, 1),\nX\nj\n\u03c0j\nt = 1\n(19)\n\u00b5j\nt = \u02c6\u00b5j\nt\n=\u21d2\u00b5j\nt \u2208R\n(20)\n\u03c3j\nt = exp\n\u0010\n\u02c6\u03c3j\nt\n\u0011\n=\u21d2\u03c3j\nt > 0\n(21)\n\u03c1j\nt = tanh(\u02c6\u03c1j\nt)\n=\u21d2\u03c1j\nt \u2208(\u22121, 1)\n(22)\nThe probability density Pr(xt+1|yt) of the next input xt+1 given the output\nvector yt is de\ufb01ned as follows:\nPr(xt+1|yt) =\nM\nX\nj=1\n\u03c0j\nt N(xt+1|\u00b5j\nt, \u03c3j\nt , \u03c1j\nt)\n(\net\nif (xt+1)3 = 1\n1 \u2212et\notherwise\n(23)\nwhere\nN(x|\u00b5, \u03c3, \u03c1) =\n1\n2\u03c0\u03c31\u03c32\np\n1 \u2212\u03c12 exp\n\u0014\n\u2212Z\n2(1 \u2212\u03c12)\n\u0015\n(24)\nwith\nZ = (x1 \u2212\u00b51)2\n\u03c32\n1\n+ (x2 \u2212\u00b52)2\n\u03c32\n2\n\u22122\u03c1(x1 \u2212\u00b51)(x2 \u2212\u00b52)\n\u03c31\u03c32\n(25)\n20\nThis can be substituted into Eq. (6) to determine the sequence loss (up to\na constant that depends only on the quantisation of the data and does not\nin\ufb02uence network training):\nL(x) =\nT\nX\nt=1\n\u2212log\n\uf8eb\n\uf8edX\nj\n\u03c0j\nt N(xt+1|\u00b5j\nt, \u03c3j\nt , \u03c1j\nt)\n\uf8f6\n\uf8f8\u2212\n(\nlog et\nif (xt+1)3 = 1\nlog(1 \u2212et)\notherwise\n(26)\nThe derivative of the loss with respect to the end-of-stroke outputs is straight-\nforward:\n\u2202L(x)\n\u2202\u02c6et\n= (xt+1)3 \u2212et\n(27)\nThe derivatives with respect to the mixture density outputs can be found by\n\ufb01rst de\ufb01ning the component responsibilities \u03b3j\nt :\n\u02c6\u03b3j\nt = \u03c0j\nt N(xt+1|\u00b5j\nt, \u03c3j\nt , \u03c1j\nt)\n(28)\n\u03b3j\nt =\n\u02c6\u03b3j\nt\nPM\nj\u2032=1 \u02c6\u03b3j\u2032\nt\n(29)\nThen observing that\n\u2202L(x)\n\u2202\u02c6\u03c0j\nt\n= \u03c0j\nt \u2212\u03b3j\nt\n(30)\n\u2202L(x)\n\u2202(\u02c6\u00b5j\nt, \u02c6\u03c3j\nt , \u02c6\u03c1j\nt)\n= \u2212\u03b3j\nt\n\u2202log N(xt+1|\u00b5j\nt, \u03c3j\nt , \u03c1j\nt)\n\u2202(\u02c6\u00b5j\nt, \u02c6\u03c3j\nt , \u02c6\u03c1j\nt)\n(31)\nwhere\n\u2202log N(x|\u00b5, \u03c3, \u03c1)\n\u2202\u02c6\u00b51\n= C\n\u03c31\n\u0012x1 \u2212\u00b51\n\u03c31\n\u2212\u03c1(x2 \u2212\u00b52)\n\u03c32\n\u0013\n(32)\n\u2202log N(x|\u00b5, \u03c3, \u03c1)\n\u2202\u02c6\u00b52\n= C\n\u03c32\n\u0012x2 \u2212\u00b52\n\u03c32\n\u2212\u03c1(x1 \u2212\u00b51)\n\u03c31\n\u0013\n(33)\n\u2202log N(x|\u00b5, \u03c3, \u03c1)\n\u2202\u02c6\u03c31\n= C(x1 \u2212\u00b51)\n\u03c31\n\u0012x1 \u2212\u00b51\n\u03c31\n\u2212\u03c1(x2 \u2212\u00b52)\n\u03c32\n\u0013\n\u22121\n(34)\n\u2202log N(x|\u00b5, \u03c3, \u03c1)\n\u2202\u02c6\u03c32\n= C(x2 \u2212\u00b52)\n\u03c32\n\u0012x2 \u2212\u00b52\n\u03c32\n\u2212\u03c1(x1 \u2212\u00b51)\n\u03c31\n\u0013\n\u22121\n(35)\n\u2202log N(x|\u00b5, \u03c3, \u03c1)\n\u2202\u02c6\u03c1\n= (x1 \u2212\u00b51)(x2 \u2212\u00b52)\n\u03c31\u03c32\n+ \u03c1 (1 \u2212CZ)\n(36)\nwith Z de\ufb01ned as in Eq. (25) and\nC =\n1\n1 \u2212\u03c12\n(37)\nFig. 10 illustrates the operation of a mixture density output layer applied to\nonline handwriting prediction.\n21\nOutput Density\nFigure 10: Mixture density outputs for handwriting prediction. The\ntop heatmap shows the sequence of probability distributions for the predicted\npen locations as the word \u2018under\u2019 is written.\nThe densities for successive\npredictions are added together, giving high values where the distributions\noverlap.\nTwo types of prediction are visible from the density map:\nthe small\nblobs that spell out the letters are the predictions as the strokes are being\nwritten, the three large blobs are the predictions at the ends of the strokes for\nthe \ufb01rst point in the next stroke.\nThe end-of-stroke predictions have much\nhigher variance because the pen position was not recorded when it was o\ufb00the\nwhiteboard, and hence there may be a large distance between the end of one\nstroke and the start of the next.\nThe bottom heatmap shows the mixture component weights during the\nsame sequence.\nThe stroke ends are also visible here, with the most active\ncomponents switching o\ufb00in three places, and other components switching on:\nevidently end-of-stroke predictions use a di\ufb00erent set of mixture components\nfrom in-stroke predictions.\n22\n4.2\nExperiments\nEach point in the data sequences consisted of three numbers: the x and y o\ufb00set\nfrom the previous point, and the binary end-of-stroke feature.\nThe network\ninput layer was therefore size 3.\nThe co-ordinate o\ufb00sets were normalised to\nmean 0, std. dev. 1 over the training set. 20 mixture components were used\nto model the o\ufb00sets, giving a total of 120 mixture parameters per timestep\n(20 weights, 40 means, 40 standard deviations and 20 correlations). A further\nparameter was used to model the end-of-stroke probability, giving an output\nlayer of size 121.\nTwo network architectures were compared for the hidden\nlayers: one with three hidden layers, each consisting of 400 LSTM cells, and one\nwith a single hidden layer of 900 LSTM cells. Both networks had around 3.4M\nweights. The three layer network was retrained with adaptive weight noise [8],\nwith all std. devs. initialised to 0.075. Training with \ufb01xed variance weight noise\nproved ine\ufb00ective, probably because it prevented the mixture density layer from\nusing precisely speci\ufb01ed weights.\nThe networks were trained with rmsprop, a form of stochastic gradient de-\nscent where the gradients are divided by a running average of their recent mag-\nnitude [32]. De\ufb01ne \u03f5i = \u2202L(x)\n\u2202wi\nwhere wi is network weight i. The weight update\nequations were:\nni = \u2135ni + (1 \u2212\u2135)\u03f52\ni\n(38)\ngi = \u2135gi + (1 \u2212\u2135)\u03f5i\n(39)\n\u2206i = \u2136\u2206i \u2212\u05d2\u03f5i\np\nni \u2212g2\ni + \u2138\n(40)\nwi = wi + \u2206i\n(41)\nwith the following parameters:\n\u2135= 0.95\n(42)\n\u2136= 0.9\n(43)\n= \u05d20.0001\n(44)\n\u2138= 0.0001\n(45)\nThe output derivatives\n\u2202L(x)\n\u2202\u02c6yt\nwere clipped in the range [\u2212100, 100], and the\nLSTM derivates were clipped in the range [\u221210, 10]. Clipping the output gradi-\nents proved vital for numerical stability; even so, the networks sometimes had\nnumerical problems late on in training, after they had started over\ufb01tting on the\ntraining data.\nTable 3 shows that the three layer network had an average per-sequence loss\n15.3 nats lower than the one layer net.\nHowever the sum-squared-error was\nslightly lower for the single layer network.\nthe use of adaptive weight noise\nreduced the loss by another 16.7 nats relative to the unregularised three layer\nnetwork, but did not signi\ufb01cantly change the sum-squared error. The adaptive\nweight noise network appeared to generate the best samples.\n23\nTable 3: Handwriting Prediction Results. All results recorded on the val-\nidation set. \u2018Log-Loss\u2019 is the mean value of L(x) (in nats). \u2018SSE\u2019 is the mean\nsum-squared-error per data point.\nNetwork\nRegularisation\nLog-Loss\nSSE\n1 layer\nnone\n-1025.7\n0.40\n3 layer\nnone\n-1041.0\n0.41\n3 layer\nadaptive weight noise\n-1057.7\n0.41\n4.3\nSamples\nFig. 11 shows handwriting samples generated by the prediction network. The\nnetwork has clearly learned to model strokes, letters and even short words (es-\npecially common ones such as \u2018of\u2019 and \u2018the\u2019). It also appears to have learned a\nbasic character level language models, since the words it invents (\u2018eald\u2019, \u2018bryoes\u2019,\n\u2018lenrest\u2019) look somewhat plausible in English. Given that the average character\noccupies more than 25 timesteps, this again demonstrates the network\u2019s ability\nto generate coherent long-range structures.\n5\nHandwriting Synthesis\nHandwriting synthesis is the generation of handwriting for a given text. Clearly\nthe prediction networks we have described so far are unable to do this, since\nthere is no way to constrain which letters the network writes. This section de-\nscribes an augmentation that allows a prediction network to generate data se-\nquences conditioned on some high-level annotation sequence (a character string,\nin the case of handwriting synthesis). The resulting sequences are su\ufb03ciently\nconvincing that they often cannot be distinguished from real handwriting. Fur-\nthermore, this realism is achieved without sacri\ufb01cing the diversity in writing\nstyle demonstrated in the previous section.\nThe main challenge in conditioning the predictions on the text is that the two\nsequences are of very di\ufb00erent lengths (the pen trace being on average twenty\n\ufb01ve times as long as the text), and the alignment between them is unknown until\nthe data is generated. This is because the number of co-ordinates used to write\neach character varies greatly according to style, size, pen speed etc. One neural\nnetwork model able to make sequential predictions based on two sequences of\ndi\ufb00erent length and unknown alignment is the RNN transducer [9]. However\npreliminary experiments on handwriting synthesis with RNN transducers were\nnot encouraging. A possible explanation is that the transducer uses two sepa-\nrate RNNs to process the two sequences, then combines their outputs to make\ndecisions, when it is usually more desirable to make all the information avail-\nable to single network. This work proposes an alternative model, where a \u2018soft\nwindow\u2019 is convolved with the text string and fed in as an extra input to the\nprediction network. The parameters of the window are output by the network\n24\nFigure 11: Online handwriting samples generated by the prediction\nnetwork. All samples are 700 timesteps long.\n25\nat the same time as it makes the predictions, so that it dynamically determines\nan alignment between the text and the pen locations. Put simply, it learns to\ndecide which character to write next.\n5.1\nSynthesis Network\nFig. 12 illustrates the network architecture used for handwriting synthesis. As\nwith the prediction network, the hidden layers are stacked on top of each other,\neach feeding up to the layer above, and there are skip connections from the\ninputs to all hidden layers and from all hidden layers to the outputs.\nThe\ndi\ufb00erence is the added input from the character sequence, mediated by the\nwindow layer.\nGiven a length U character sequence c and a length T data sequence x, the\nsoft window wt into c at timestep t (1 \u2264t \u2264T) is de\ufb01ned by the following\ndiscrete convolution with a mixture of K Gaussian functions\n\u03c6(t, u) =\nK\nX\nk=1\n\u03b1k\nt exp\n\u0010\n\u2212\u03b2k\nt\n\u0000\u03bak\nt \u2212u\n\u00012\u0011\n(46)\nwt =\nU\nX\nu=1\n\u03c6(t, u)cu\n(47)\nwhere \u03c6(t, u) is the window weight of cu at timestep t. Intuitively, the \u03bat param-\neters control the location of the window, the \u03b2t parameters control the width of\nthe window and the \u03b1t parameters control the importance of the window within\nthe mixture. The size of the soft window vectors is the same as the size of the\ncharacter vectors cu (assuming a one-hot encoding, this will be the number of\ncharacters in the alphabet). Note that the window mixture is not normalised\nand hence does not determine a probability distribution; however the window\nweight \u03c6(t, u) can be loosely interpreted as the network\u2019s belief that it is writ-\ning character cu at time t. Fig. 13 shows the alignment implied by the window\nweights during a training sequence.\nThe size 3K vector p of window parameters is determined as follows by the\noutputs of the \ufb01rst hidden layer of the network:\n(\u02c6\u03b1t, \u02c6\u03b2t, \u02c6\u03bat) = Wh1ph1\nt + bp\n(48)\n\u03b1t = exp (\u02c6\u03b1t)\n(49)\n\u03b2t = exp\n\u0010\n\u02c6\u03b2t\n\u0011\n(50)\n\u03bat = \u03bat\u22121 + exp (\u02c6\u03bat)\n(51)\nNote that the location parameters \u03bat are de\ufb01ned as o\ufb00sets from the previous\nlocations ct\u22121, and that the size of the o\ufb00set is constrained to be greater than\nzero. Intuitively, this means that network learns how far to slide each window\nat each step, rather than an absolute location. Using o\ufb00sets was essential to\ngetting the network to align the text with the pen trace.\n26\nInputs\nCharacters\nHidden 1\nWindow\nHidden 2\nOutputs\nFigure 12: Synthesis Network Architecture Circles represent layers, solid\nlines represent connections and dashed lines represent predictions. The topology\nis similar to the prediction network in Fig. 1, except that extra input from the\ncharacter sequence c, is presented to the hidden layers via the window layer\n(with a delay in the connection to the \ufb01rst hidden layer to avoid a cycle in the\ngraph).\n27\nThought that the muster from\nFigure 13: Window weights during a handwriting synthesis sequence\nEach point on the map shows the value of \u03c6(t, u), where t indexes the pen trace\nalong the horizontal axis and u indexes the text character along the vertical axis.\nThe bright line is the alignment chosen by the network between the characters\nand the writing. Notice that the line spreads out at the boundaries between\ncharacters; this means the network receives information about next and previous\nletters as it makes transitions, which helps guide its predictions.\n28\nThe wt vectors are passed to the second and third hidden layers at time t,\nand the \ufb01rst hidden layer at time t+1 (to avoid creating a cycle in the processing\ngraph). The update equations for the hidden layers are\nh1\nt = H\n\u0000Wih1xt + Wh1h1h1\nt\u22121 + Wwh1wt\u22121 + b1\nh\n\u0001\n(52)\nhn\nt = H\n\u0000Wihnxt + Whn\u22121hnhn\u22121\nt\n+ Whnhnhn\nt\u22121 + Wwhnwt + bn\nh\n\u0001\n(53)\nThe equations for the output layer remain unchanged from Eqs. (17) to (22).\nThe sequence loss is\nL(x) = \u2212log Pr(x|c)\n(54)\nwhere\nPr(x|c) =\nT\nY\nt=1\nPr (xt+1|yt)\n(55)\nNote that yt is now a function of c as well as x1:t.\nThe loss derivatives with respect to the outputs \u02c6et, \u02c6\u03c0t, \u02c6\u00b5t, \u02c6\u03c3t, \u02c6\u03c1t remain un-\nchanged from Eqs. (27), (30) and (31). Given the loss derivative \u2202L(x)\n\u2202wt\nwith\nrespect to the size W window vector wt, obtained by backpropagating the out-\nput derivatives through the computation graph in Fig. 12, the derivatives with\nrespect to the window parameters are as follows:\n\u03f5(k, t, u)\ndef\n= \u03b1k\nt exp\n\u0010\n\u2212\u03b2k\nt\n\u0000\u03bak\nt \u2212u\n\u00012\u0011 W\nX\nj=1\n\u2202L(x)\n\u2202wj\nt\ncj\nu\n(56)\n\u2202L(x)\n\u2202\u02c6\u03b1k\nt\n=\nU\nX\nu=1\n\u03f5(k, t, u)\n(57)\n\u2202L(x)\n\u2202\u02c6\u03b2k\nt\n= \u2212\u03b2k\nt\nU\nX\nu=1\n\u03f5(k, t, u)(\u03bak\nt \u2212u)2\n(58)\n\u2202L(x)\n\u2202\u03bak\nt\n= \u2202L(x)\n\u2202\u03bak\nt+1\n+ 2\u03b2k\nt\nU\nX\nu=1\n\u03f5(k, t, u)(u \u2212\u03bak\nt )\n(59)\n\u2202L(x)\n\u2202\u02c6\u03bak\nt\n= exp\n\u0000\u02c6\u03bak\nt\n\u0001 \u2202L(x)\n\u2202\u03bak\nt\n(60)\nFig. 14 illustrates the operation of a mixture density output layer applied to\nhandwriting synthesis.\n5.2\nExperiments\nThe synthesis network was applied to the same input data as the handwriting\nprediction network in the previous section. The character-level transcriptions\nfrom the IAM-OnDB were now used to de\ufb01ne the character sequences c. The full\ntranscriptions contain 80 distinct characters (capital letters, lower case letters,\ndigits, and punctuation). However we used only a subset of 57, with all the\n29\nSynthesis Output Density\nFigure 14: Mixture density outputs for handwriting synthesis. The top\nheatmap shows the predictive distributions for the pen locations, the bottom\nheatmap shows the mixture component weights. Comparison with Fig. 10 indi-\ncates that the synthesis network makes more precise predictions (with smaller\ndensity blobs) than the prediction-only network, especially at the ends of strokes,\nwhere the synthesis network has the advantage of knowing which letter comes\nnext.\n30\nTable 4: Handwriting Synthesis Results. All results recorded on the val-\nidation set. \u2018Log-Loss\u2019 is the mean value of L(x) in nats. \u2018SSE\u2019 is the mean\nsum-squared-error per data point.\nRegularisation\nLog-Loss\nSSE\nnone\n-1096.9\n0.23\nadaptive weight noise\n-1128.2\n0.23\ndigits and most of the punctuation characters replaced with a generic \u2018non-\nletter\u2019 label2.\nThe network architecture was as similar as possible to the best prediction\nnetwork: three hidden layers of 400 LSTM cells each, 20 bivariate Gaussian\nmixture components at the output layer and a size 3 input layer. The character\nsequence was encoded with one-hot vectors, and hence the window vectors were\nsize 57. A mixture of 10 Gaussian functions was used for the window parameters,\nrequiring a size 30 parameter vector. The total number of weights was increased\nto approximately 3.7M.\nThe network was trained with rmsprop, using the same parameters as in\nthe previous section. The network was retrained with adaptive weight noise,\ninitial standard deviation 0.075, and the output and LSTM gradients were again\nclipped in the range [\u2212100, 100] and [\u221210, 10] respectively.\nTable 4 shows that adaptive weight noise gave a considerable improvement\nin log-loss (around 31.3 nats) but no signi\ufb01cant change in sum-squared error.\nThe regularised network appears to generate slightly more realistic sequences,\nalthough the di\ufb00erence is hard to discern by eye. Both networks performed\nconsiderably better than the best prediction network. In particular the sum-\nsquared-error was reduced by 44%. This is likely due in large part to the im-\nproved predictions at the ends of strokes, where the error is largest.\n5.3\nUnbiased Sampling\nGiven c, an unbiased sample can be picked from Pr(x|c) by iteratively drawing\nxt+1 from Pr (xt+1|yt), just as for the prediction network. The only di\ufb00erence is\nthat we must also decide when the synthesis network has \ufb01nished writing the text\nand should stop making any future decisions. To do this, we use the following\nheuristic: as soon as \u03c6(t, U + 1) > \u03c6(t, u) \u22001 \u2264u \u2264U the current input xt is\nde\ufb01ned as the end of the sequence and sampling ends. Examples of unbiased\nsynthesis samples are shown in Fig. 15. These and all subsequent \ufb01gures were\ngenerated using the synthesis network retrained with adaptive weight noise.\nNotice how stylistic traits, such as character size, slant, cursiveness etc. vary\n2This was an oversight; however it led to the interesting result that when the text contains\na non-letter, the network must select a digits or punctuation mark to generate. Sometimes\nthe character can be be inferred from the context (e.g. the apostrophe in \u201ccan\u2019t\u201d); otherwise\nit is chosen at random.\n31\nwidely between the samples, but remain more-or-less consistent within them.\nThis suggests that the network identi\ufb01es the traits early on in the sequence,\nthen remembers them until the end. By looking through enough samples for a\ngiven text, it appears to be possible to \ufb01nd virtually any combination of stylistic\ntraits, which suggests that the network models them independently both from\neach other and from the text.\n\u2018Blind taste tests\u2019 carried out by the author during presentations suggest\nthat at least some unbiased samples cannot be distinguished from real hand-\nwriting by the human eye. Nonetheless the network does make mistakes we\nwould not expect a human writer to make, often involving missing, confused\nor garbled letters3; this suggests that the network sometimes has trouble de-\ntermining the alignment between the characters and the trace. The number of\nmistakes increases markedly when less common words or phrases are included\nin the character sequence. Presumably this is because the network learns an\nimplicit character-level language model from the training set that gets confused\nwhen rare or unknown transitions occur.\n5.4\nBiased Sampling\nOne problem with unbiased samples is that they tend to be di\ufb03cult to read\n(partly because real handwriting is di\ufb03cult to read, and partly because the\nnetwork is an imperfect model). Intuitively, we would expect the network to\ngive higher probability to good handwriting because it tends to be smoother\nand more predictable than bad handwriting. If this is true, we should aim to\noutput more probable elements of Pr(x|c) if we want the samples to be easier to\nread. A principled search for high probability samples could lead to a di\ufb03cult\ninference problem, as the probability of every output depends on all previous\noutputs. However a simple heuristic, where the sampler is biased towards more\nprobable predictions at each step independently, generally gives good results.\nDe\ufb01ne the probability bias b as a real number greater than or equal to zero.\nBefore drawing a sample from Pr(xt+1|yt), each standard deviation \u03c3j\nt in the\nGaussian mixture is recalculated from Eq. (21) to\n\u03c3j\nt = exp\n\u0010\n\u02c6\u03c3j\nt \u2212b\n\u0011\n(61)\nand each mixture weight is recalculated from Eq. (19) to\n\u03c0j\nt =\nexp\n\u0010\n\u02c6\u03c0j\nt (1 + b)\n\u0011\nPM\nj\u2032=1 exp\n\u0010\n\u02c6\u03c0j\u2032\nt (1 + b)\n\u0011\n(62)\nThis arti\ufb01cially reduces the variance in both the choice of component from the\nmixture, and in the distribution of the component itself. When b = 0 unbiased\nsampling is recovered, and as b \u2192\u221ethe variance in the sampling disappears\n3We expect humans to make mistakes like misspelling \u2018temperament\u2019 as \u2018temperement\u2019, as\nthe second writer in Fig. 15 seems to have done.\n32\nFigure 15: Real and generated handwriting. The top line in each block is\nreal, the rest are unbiased samples from the synthesis network. The two texts\nare from the validation set and were not seen during training.\n33\nand the network always outputs the mode of the most probable component in\nthe mixture (which is not necessarily the mode of the mixture, but at least a\nreasonable approximation). Fig. 16 shows the e\ufb00ect of progressively increasing\nthe bias, and Fig. 17 shows samples generated with a low bias for the same texts\nas Fig. 15.\n5.5\nPrimed Sampling\nAnother reason to constrain the sampling would be to generate handwriting\nin the style of a particular writer (rather than in a randomly selected style).\nThe easiest way to do this would be to retrain it on that writer only.\nBut\neven without retraining, it is possible to mimic a particular style by \u2018priming\u2019\nthe network with a real sequence, then generating an extension with the real\nsequence still in the network\u2019s memory. This can be achieved for a real x, c and\na synthesis character string s by setting the character sequence to c\u2032 = c + s\nand clamping the data inputs to x for the \ufb01rst T timesteps, then sampling\nas usual until the sequence ends. Examples of primed samples are shown in\nFigs. 18 and 19. The fact that priming works proves that the network is able to\nremember stylistic features identi\ufb01ed earlier on in the sequence. This technique\nappears to work better for sequences in the training data than those the network\nhas never seen.\nPrimed sampling and reduced variance sampling can also be combined. As\nshown in Figs. 20 and 21 this tends to produce samples in a \u2018cleaned up\u2019 version\nof the priming style, with overall stylistic traits such as slant and cursiveness\nretained, but the strokes appearing smoother and more regular.\nA possible\napplication would be the arti\ufb01cial enhancement of poor handwriting.\n6\nConclusions and Future Work\nThis paper has demonstrated the ability of Long Short-Term Memory recur-\nrent neural networks to generate both discrete and real-valued sequences with\ncomplex, long-range structure using next-step prediction. It has also introduced\na novel convolutional mechanism that allows a recurrent network to condition\nits predictions on an auxiliary annotation sequence, and used this approach to\nsynthesise diverse and realistic samples of online handwriting. Furthermore, it\nhas shown how these samples can be biased towards greater legibility, and how\nthey can be modelled on the style of a particular writer.\nSeveral directions for future work suggest themselves. One is the applica-\ntion of the network to speech synthesis, which is likely to be more challenging\nthan handwriting synthesis due to the greater dimensionality of the data points.\nAnother is to gain a better insight into the internal representation of the data,\nand to use this to manipulate the sample distribution directly. It would also\nbe interesting to develop a mechanism to automatically extract high-level an-\nnotations from sequence data. In the case of handwriting, this could allow for\n34\nFigure 16: Samples biased towards higher probability. The probability\nbiases b are shown at the left. As the bias increases the diversity decreases and\nthe samples tend towards a kind of \u2018average handwriting\u2019 which is extremely\nregular and easy to read (easier, in fact, than most of the real handwriting in the\ntraining set). Note that even when the variance disappears, the same letter is\nnot written the same way at di\ufb00erent points in a sequence (for examples the \u2018e\u2019s\nin \u201cexactly the same\u201d, the \u2018l\u2019s in \u201cuntil they all look\u201d), because the predictions\nare still in\ufb02uenced by the previous outputs. If you look closely you can see that\nthe last three lines are not quite exactly the same.\n35\nFigure 17: A slight bias. The top line in each block is real. The rest are\nsamples from the synthesis network with a probability bias of 0.15, which seems\nto give a good balance between diversity and legibility.\n36\nFigure 18: Samples primed with real sequences. The priming sequences\n(drawn from the training set) are shown at the top of each block. None of the\nlines in the sampled text exist in the training set. The samples were selected\nfor legibility.\n37\nFigure 19: Samples primed with real sequences (cotd).\n38\nFigure 20: Samples primed with real sequences and biased towards\nhigher probability. The priming sequences are at the top of the blocks. The\nprobability bias was 1. None of the lines in the sampled text exist in the training\nset.\n39\nFigure 21: Samples primed with real sequences and biased towards\nhigher probability (cotd)\n40\nmore nuanced annotations than just text, for example stylistic features, di\ufb00erent\nforms of the same letter, information about stroke order and so on.\nAcknowledgements\nThanks to Yichuan Tang, Ilya Sutskever, Navdeep Jaitly, Geo\ufb00rey Hinton and\nother colleagues at the University of Toronto for numerous useful comments\nand suggestions. This work was supported by a Global Scholarship from the\nCanadian Institute for Advanced Research.\nReferences\n[1] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies\nwith gradient descent is di\ufb03cult. IEEE Transactions on Neural Networks,\n5(2):157\u2013166, March 1994.\n[2] C. Bishop. Mixture density networks. Technical report, 1994.\n[3] C. Bishop. Neural Networks for Pattern Recognition. Oxford University\nPress, Inc., 1995.\n[4] N. Boulanger-Lewandowski, Y. Bengio, and P. Vincent. Modeling tempo-\nral dependencies in high-dimensional sequences: Application to polyphonic\nmusic generation and transcription. In Proceedings of the Twenty-nine In-\nternational Conference on Machine Learning (ICML\u201912), 2012.\n[5] J. G. Cleary, Ian, and I. H. Witten. Data compression using adaptive cod-\ning and partial string matching. IEEE Transactions on Communications,\n32:396\u2013402, 1984.\n[6] D. Eck and J. Schmidhuber. A \ufb01rst look at music composition using lstm\nrecurrent neural networks. Technical report, IDSIA USI-SUPSI Instituto\nDalle Molle.\n[7] F. Gers, N. Schraudolph, and J. Schmidhuber. Learning precise timing\nwith LSTM recurrent networks. Journal of Machine Learning Research,\n3:115\u2013143, 2002.\n[8] A. Graves. Practical variational inference for neural networks. In Advances\nin Neural Information Processing Systems, volume 24, pages 2348\u20132356.\n2011.\n[9] A. Graves. Sequence transduction with recurrent neural networks. In ICML\nRepresentation Learning Worksop, 2012.\n[10] A. Graves, A. Mohamed, and G. Hinton. Speech recognition with deep\nrecurrent neural networks. In Proc. ICASSP, 2013.\n41\n[11] A. Graves and J. Schmidhuber. Framewise phoneme classi\ufb01cation with bidi-\nrectional LSTM and other neural network architectures. Neural Networks,\n18:602\u2013610, 2005.\n[12] A. Graves and J. Schmidhuber. O\ufb04ine handwriting recognition with multi-\ndimensional recurrent neural networks. In Advances in Neural Information\nProcessing Systems, volume 21, 2008.\n[13] P. D. Gr\u00a8unwald. The Minimum Description Length Principle (Adaptive\nComputation and Machine Learning). The MIT Press, 2007.\n[14] G. Hinton. A Practical Guide to Training Restricted Boltzmann Machines.\nTechnical report, 2010.\n[15] S. Hochreiter, Y. Bengio, P. Frasconi, and J. Schmidhuber. Gradient Flow\nin Recurrent Nets: the Di\ufb03culty of Learning Long-term Dependencies.\nIn S. C. Kremer and J. F. Kolen, editors, A Field Guide to Dynamical\nRecurrent Neural Networks. 2001.\n[16] S. Hochreiter and J. Schmidhuber.\nLong Short-Term Memory.\nNeural\nComputation, 9(8):1735\u20131780, 1997.\n[17] M. Hutter. The Human Knowledge Compression Contest, 2012.\n[18] K.-C. Jim, C. Giles, and B. Horne. An analysis of noise in recurrent neural\nnetworks: convergence and generalization. Neural Networks, IEEE Trans-\nactions on, 7(6):1424 \u20131438, 1996.\n[19] S. Johansson, R. Atwell, R. Garside, and G. Leech. The tagged LOB corpus\nuser\u2019s manual; Norwegian Computing Centre for the Humanities, 1986.\n[20] B. Knoll and N. de Freitas. A machine learning perspective on predictive\ncoding with paq. CoRR, abs/1108.3298, 2011.\n[21] M. Liwicki and H. Bunke.\nIAM-OnDB - an on-line English sentence\ndatabase acquired from handwritten text on a whiteboard. In Proc. 8th\nInt. Conf. on Document Analysis and Recognition, volume 2, pages 956\u2013\n961, 2005.\n[22] M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. Building a large\nannotated corpus of english: The penn treebank.\nCOMPUTATIONAL\nLINGUISTICS, 19(2):313\u2013330, 1993.\n[23] T. Mikolov. Statistical Language Models based on Neural Networks. PhD\nthesis, Brno University of Technology, 2012.\n[24] T. Mikolov, I. Sutskever, A. Deoras, H. Le, S. Kombrink, and J. Cernocky.\nSubword language modeling with neural networks. Technical report, Un-\npublished Manuscript, 2012.\n42\n[25] A. Mnih and G. Hinton.\nA Scalable Hierarchical Distributed Language\nModel. In Advances in Neural Information Processing Systems, volume 21,\n2008.\n[26] A. Mnih and Y. W. Teh. A fast and simple algorithm for training neural\nprobabilistic language models.\nIn Proceedings of the 29th International\nConference on Machine Learning, pages 1751\u20131758, 2012.\n[27] T. N. Sainath, A. Mohamed, B. Kingsbury, and B. Ramabhadran. Low-\nrank matrix factorization for deep neural network training with high-\ndimensional output targets. In Proc. ICASSP, 2013.\n[28] M. Schuster. Better generative models for sequential data problems: Bidi-\nrectional recurrent mixture density networks.\npages 589\u2013595. The MIT\nPress, 1999.\n[29] I. Sutskever, G. E. Hinton, and G. W. Taylor. The recurrent temporal\nrestricted boltzmann machine. pages 1601\u20131608, 2008.\n[30] I. Sutskever, J. Martens, and G. Hinton. Generating text with recurrent\nneural networks. In ICML, 2011.\n[31] G. W. Taylor and G. E. Hinton. Factored conditional restricted boltzmann\nmachines for modeling motion style. In Proc. 26th Annual International\nConference on Machine Learning, pages 1025\u20131032, 2009.\n[32] T. Tieleman and G. Hinton. Lecture 6.5 - rmsprop: Divide the gradient by\na running average of its recent magnitude, 2012.\n[33] R. Williams and D. Zipser. Gradient-based learning algorithms for recur-\nrent networks and their computational complexity. In Back-propagation:\nTheory, Architectures and Applications, pages 433\u2013486. 1995.\n43\n",
        "sentence": " Such models have been applied successfully in image classification [12, 4, 3, 2], object tracking [13, 3], machine translation [6], caption generation [5], and image generation [14, 15].",
        "context": "including tree-based models [25, 23], low rank approximations [27] and stochas-\ntic derivatives [26]). Furthermore, word-level models are not applicable to text\ndata containing non-word strings, such as multi-digit numbers or web addresses.\nthey can be modelled on the style of a particular writer.\nSeveral directions for future work suggest themselves. One is the applica-\ntion of the network to speech synthesis, which is likely to be more challenging\nguage modelling [23], and found to speed up training (by reducing the sequence\nlength and hence increasing the frequency of stochastic weight updates) without\na\ufb00ecting the network\u2019s ability to learn long-range dependencies."
    },
    {
        "title": "DRAW: a recurrent neural network for image generation",
        "author": [
            "K. Gregor",
            "I. Danihelka",
            "A. Graves",
            "D. Wierstra"
        ],
        "venue": "arXiv:1502.04623,",
        "citeRegEx": "15",
        "shortCiteRegEx": null,
        "year": 2015,
        "abstract": "This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural\nnetwork architecture for image generation. DRAW networks combine a novel\nspatial attention mechanism that mimics the foveation of the human eye, with a\nsequential variational auto-encoding framework that allows for the iterative\nconstruction of complex images. The system substantially improves on the state\nof the art for generative models on MNIST, and, when trained on the Street View\nHouse Numbers dataset, it generates images that cannot be distinguished from\nreal data with the naked eye.",
        "full_text": "DRAW: A Recurrent Neural Network For Image Generation\nKarol Gregor\nKAROLG@GOOGLE.COM\nIvo Danihelka\nDANIHELKA@GOOGLE.COM\nAlex Graves\nGRAVESA@GOOGLE.COM\nDanilo Jimenez Rezende\nDANILOR@GOOGLE.COM\nDaan Wierstra\nWIERSTRA@GOOGLE.COM\nGoogle DeepMind\nAbstract\nThis paper introduces the Deep Recurrent Atten-\ntive Writer (DRAW) neural network architecture\nfor image generation. DRAW networks combine\na novel spatial attention mechanism that mimics\nthe foveation of the human eye, with a sequential\nvariational auto-encoding framework that allows\nfor the iterative construction of complex images.\nThe system substantially improves on the state\nof the art for generative models on MNIST, and,\nwhen trained on the Street View House Numbers\ndataset, it generates images that cannot be distin-\nguished from real data with the naked eye.\n1. Introduction\nA person asked to draw, paint or otherwise recreate a visual\nscene will naturally do so in a sequential, iterative fashion,\nreassessing their handiwork after each modi\ufb01cation. Rough\noutlines are gradually replaced by precise forms, lines are\nsharpened, darkened or erased, shapes are altered, and the\n\ufb01nal picture emerges. Most approaches to automatic im-\nage generation, however, aim to generate entire scenes at\nonce. In the context of generative neural networks, this typ-\nically means that all the pixels are conditioned on a single\nlatent distribution (Dayan et al., 1995; Hinton & Salakhut-\ndinov, 2006; Larochelle & Murray, 2011). As well as pre-\ncluding the possibility of iterative self-correction, the \u201cone\nshot\u201d approach is fundamentally dif\ufb01cult to scale to large\nimages. The Deep Recurrent Attentive Writer (DRAW) ar-\nchitecture represents a shift towards a more natural form of\nimage construction, in which parts of a scene are created\nindependently from others, and approximate sketches are\nsuccessively re\ufb01ned.\nProceedings of the 32 nd International Conference on Machine\nLearning, Lille, France, 2015. JMLR: W&CP volume 37. Copy-\nright 2015 by the author(s).\nTime\nFigure 1. A trained DRAW network generating MNIST dig-\nits. Each row shows successive stages in the generation of a sin-\ngle digit. Note how the lines composing the digits appear to be\n\u201cdrawn\u201d by the network. The red rectangle delimits the area at-\ntended to by the network at each time-step, with the focal preci-\nsion indicated by the width of the rectangle border.\nThe core of the DRAW architecture is a pair of recurrent\nneural networks: an encoder network that compresses the\nreal images presented during training, and a decoder that\nreconstitutes images after receiving codes. The combined\nsystem is trained end-to-end with stochastic gradient de-\nscent, where the loss function is a variational upper bound\non the log-likelihood of the data. It therefore belongs to the\nfamily of variational auto-encoders, a recently emerged\nhybrid of deep learning and variational inference that has\nled to signi\ufb01cant advances in generative modelling (Gre-\ngor et al., 2014; Kingma & Welling, 2014; Rezende et al.,\n2014; Mnih & Gregor, 2014; Salimans et al., 2014). Where\nDRAW differs from its siblings is that, rather than generat-\narXiv:1502.04623v2  [cs.CV]  20 May 2015\nDRAW: A Recurrent Neural Network For Image Generation\ning images in a single pass, it iteratively constructs scenes\nthrough an accumulation of modi\ufb01cations emitted by the\ndecoder, each of which is observed by the encoder.\nAn obvious correlate of generating images step by step is\nthe ability to selectively attend to parts of the scene while\nignoring others. A wealth of results in the past few years\nsuggest that visual structure can be better captured by a se-\nquence of partial glimpses, or foveations, than by a sin-\ngle sweep through the entire image (Larochelle & Hinton,\n2010; Denil et al., 2012; Tang et al., 2013; Ranzato, 2014;\nZheng et al., 2014; Mnih et al., 2014; Ba et al., 2014; Ser-\nmanet et al., 2014). The main challenge faced by sequential\nattention models is learning where to look, which can be\naddressed with reinforcement learning techniques such as\npolicy gradients (Mnih et al., 2014). The attention model in\nDRAW, however, is fully differentiable, making it possible\nto train with standard backpropagation. In this sense it re-\nsembles the selective read and write operations developed\nfor the Neural Turing Machine (Graves et al., 2014).\nThe following section de\ufb01nes the DRAW architecture,\nalong with the loss function used for training and the pro-\ncedure for image generation. Section 3 presents the selec-\ntive attention model and shows how it is applied to read-\ning and modifying images.\nSection 4 provides experi-\nmental results on the MNIST, Street View House Num-\nbers and CIFAR-10 datasets, with examples of generated\nimages; and concluding remarks are given in Section 5.\nLastly, we would like to direct the reader to the video\naccompanying this paper (https://www.youtube.\ncom/watch?v=Zt-7MI9eKEo) which contains exam-\nples of DRAW networks reading and generating images.\n2. The DRAW Network\nThe basic structure of a DRAW network is similar to that of\nother variational auto-encoders: an encoder network deter-\nmines a distribution over latent codes that capture salient\ninformation about the input data; a decoder network re-\nceives samples from the code distribuion and uses them to\ncondition its own distribution over images. However there\nare three key differences. Firstly, both the encoder and de-\ncoder are recurrent networks in DRAW, so that a sequence\nof code samples is exchanged between them; moreover the\nencoder is privy to the decoder\u2019s previous outputs, allow-\ning it to tailor the codes it sends according to the decoder\u2019s\nbehaviour so far. Secondly, the decoder\u2019s outputs are suc-\ncessively added to the distribution that will ultimately gen-\nerate the data, as opposed to emitting this distribution in\na single step. And thirdly, a dynamically updated atten-\ntion mechanism is used to restrict both the input region\nobserved by the encoder, and the output region modi\ufb01ed\nby the decoder. In simple terms, the network decides at\neach time-step \u201cwhere to read\u201d and \u201cwhere to write\u201d as well\nread\nx\nzt\nzt+1\nP(x|z1:T )\nwrite\nencoder\nRNN\nsample\ndecoder\nRNN\nread\nx\nwrite\nencoder\nRNN\nsample\ndecoder\nRNN\nct\u22121\nct\ncT\n\u03c3\nhenc\nt\u22121\nhdec\nt\u22121\nQ(zt|x, z1:t\u22121)\nQ(zt+1|x, z1:t)\n. . .\ndecoding\n(generative model)\nencoding\n(inference)\nx\nencoder\nFNN\nsample\ndecoder\nFNN\nz\nQ(z|x)\nP(x|z)\nFigure 2. Left: Conventional Variational Auto-Encoder. Dur-\ning generation, a sample z is drawn from a prior P(z) and passed\nthrough the feedforward decoder network to compute the proba-\nbility of the input P(x|z) given the sample. During inference the\ninput x is passed to the encoder network, producing an approx-\nimate posterior Q(z|x) over latent variables. During training, z\nis sampled from Q(z|x) and then used to compute the total de-\nscription length KL\n\u0000Q(Z|x)||P(Z)\n\u0001\n\u2212log(P(x|z)), which is\nminimised with stochastic gradient descent. Right: DRAW Net-\nwork. At each time-step a sample zt from the prior P(zt) is\npassed to the recurrent decoder network, which then modi\ufb01es part\nof the canvas matrix. The \ufb01nal canvas matrix cT is used to com-\npute P(x|z1:T ). During inference the input is read at every time-\nstep and the result is passed to the encoder RNN. The RNNs at\nthe previous time-step specify where to read. The output of the\nencoder RNN is used to compute the approximate posterior over\nthe latent variables at that time-step.\nas \u201cwhat to write\u201d. The architecture is sketched in Fig. 2,\nalongside a feedforward variational auto-encoder.\n2.1. Network Architecture\nLet RNN enc be the function enacted by the encoder net-\nwork at a single time-step. The output of RNN enc at time\nt is the encoder hidden vector henc\nt\n. Similarly the output of\nthe decoder RNN dec at t is the hidden vector hdec\nt\n. In gen-\neral the encoder and decoder may be implemented by any\nrecurrent neural network. In our experiments we use the\nLong Short-Term Memory architecture (LSTM; Hochreiter\n& Schmidhuber (1997)) for both, in the extended form with\nforget gates (Gers et al., 2000).\nWe favour LSTM due\nto its proven track record for handling long-range depen-\ndencies in real sequential data (Graves, 2013; Sutskever\net al., 2014). Throughout the paper, we use the notation\nb = W (a) to denote a linear weight matrix with bias from\nthe vector a to the vector b.\nAt each time-step t, the encoder receives input from both\nthe image x and from the previous decoder hidden vector\nhdec\nt\u22121. The precise form of the encoder input depends on a\nread operation, which will be de\ufb01ned in the next section.\nThe output henc\nt\nof the encoder is used to parameterise a\ndistribution Q(Zt|henc\nt\n) over the latent vector zt. In our\nDRAW: A Recurrent Neural Network For Image Generation\nexperiments the latent distribution is a diagonal Gaussian\nN(Zt|\u00b5t, \u03c3t):\n\u00b5t = W (henc\nt\n)\n(1)\n\u03c3t = exp (W (henc\nt\n))\n(2)\nBernoulli distributions are more common than Gaussians\nfor latent variables in auto-encoders (Dayan et al., 1995;\nGregor et al., 2014); however a great advantage of Gaus-\nsian latents is that the gradient of a function of the sam-\nples with respect to the distribution parameters can be eas-\nily obtained using the so-called reparameterization trick\n(Kingma & Welling, 2014; Rezende et al., 2014).\nThis\nmakes it straightforward to back-propagate unbiased, low\nvariance stochastic gradients of the loss function through\nthe latent distribution.\nAt each time-step a sample zt \u223cQ(Zt|henc\nt\n) drawn from\nthe latent distribution is passed as input to the decoder. The\noutput hdec\nt\nof the decoder is added (via a write opera-\ntion, de\ufb01ned in the sequel) to a cumulative canvas matrix\nct, which is ultimately used to reconstruct the image. The\ntotal number of time-steps T consumed by the network be-\nfore performing the reconstruction is a free parameter that\nmust be speci\ufb01ed in advance.\nFor each image x presented to the network, c0, henc\n0\n, hdec\n0\nare initialised to learned biases, and the DRAW net-\nwork iteratively computes the following equations for t =\n1 . . . , T:\n\u02c6xt = x \u2212\u03c3(ct\u22121)\n(3)\nrt = read(xt, \u02c6xt, hdec\nt\u22121)\n(4)\nhenc\nt\n= RNN enc(henc\nt\u22121, [rt, hdec\nt\u22121])\n(5)\nzt \u223cQ(Zt|henc\nt\n)\n(6)\nhdec\nt\n= RNN dec(hdec\nt\u22121, zt)\n(7)\nct = ct\u22121 + write(hdec\nt\n)\n(8)\nwhere \u02c6xt is the error image, [v, w] is the concatenation\nof vectors v and w into a single vector, and \u03c3 denotes\nthe logistic sigmoid function: \u03c3(x) =\n1\n1+exp(\u2212x). Note\nthat henc\nt\n, and hence Q(Zt|henc\nt\n), depends on both x\nand the history z1:t\u22121 of previous latent samples.\nWe\nwill sometimes make this dependency explicit by writing\nQ(Zt|x, z1:t\u22121), as shown in Fig. 2.\nhenc can also be\npassed as input to the read operation; however we did not\n\ufb01nd that this helped performance and therefore omitted it.\n2.2. Loss Function\nThe \ufb01nal canvas matrix cT is used to parameterise a model\nD(X|cT ) of the input data. If the input is binary, the natural\nchoice for D is a Bernoulli distribution with means given\nby \u03c3(cT ). The reconstruction loss Lx is de\ufb01ned as the\nnegative log probability of x under D:\nLx = \u2212log D(x|cT )\n(9)\nThe latent loss Lz for a sequence of latent distributions\nQ(Zt|henc\nt\n) is de\ufb01ned as the summed Kullback-Leibler di-\nvergence of some latent prior P(Zt) from Q(Zt|henc\nt\n):\nLz =\nT\nX\nt=1\nKL\n\u0000Q(Zt|henc\nt\n)||P(Zt)\n\u0001\n(10)\nNote that this loss depends upon the latent samples zt\ndrawn from Q(Zt|henc\nt\n), which depend in turn on the input\nx. If the latent distribution is a diagonal Gaussian with \u00b5t,\n\u03c3t as de\ufb01ned in Eqs 1 and 2, a simple choice for P(Zt) is\na standard Gaussian with mean zero and standard deviation\none, in which case Eq. 10 becomes\nLz = 1\n2\n T\nX\nt=1\n\u00b52\nt + \u03c32\nt \u2212log \u03c32\nt\n!\n\u2212T/2\n(11)\nThe total loss L for the network is the expectation of the\nsum of the reconstruction and latent losses:\nL = \u27e8Lx + Lz\u27e9z\u223cQ\n(12)\nwhich we optimise using a single sample of z for each\nstochastic gradient descent step.\nLz can be interpreted as the number of nats required to\ntransmit the latent sample sequence z1:T to the decoder\nfrom the prior, and (if x is discrete) Lx is the number of\nnats required for the decoder to reconstruct x given z1:T .\nThe total loss is therefore equivalent to the expected com-\npression of the data by the decoder and prior.\n2.3. Stochastic Data Generation\nAn image \u02dcx can be generated by a DRAW network by it-\neratively picking latent samples \u02dczt from the prior P, then\nrunning the decoder to update the canvas matrix \u02dcct. After T\nrepetitions of this process the generated image is a sample\nfrom D(X|\u02dccT ):\n\u02dczt \u223cP(Zt)\n(13)\n\u02dchdec\nt\n= RNN dec(\u02dchdec\nt\u22121, \u02dczt)\n(14)\n\u02dcct = \u02dcct\u22121 + write(\u02dchdec\nt\n)\n(15)\n\u02dcx \u223cD(X| \u02dccT )\n(16)\nNote that the encoder is not involved in image generation.\n3. Read and Write Operations\nThe DRAW network described in the previous section is\nnot complete until the read and write operations in Eqs. 4\nand 8 have been de\ufb01ned. This section describes two ways\nto do so, one with selective attention and one without.\nDRAW: A Recurrent Neural Network For Image Generation\n3.1. Reading and Writing Without Attention\nIn the simplest instantiation of DRAW the entire input im-\nage is passed to the encoder at every time-step, and the de-\ncoder modi\ufb01es the entire canvas matrix at every time-step.\nIn this case the read and write operations reduce to\nread(x, \u02c6xt, hdec\nt\u22121) = [x, \u02c6xt]\n(17)\nwrite(hdec\nt\n) = W (hdec\nt\n)\n(18)\nHowever this approach does not allow the encoder to fo-\ncus on only part of the input when creating the latent dis-\ntribution; nor does it allow the decoder to modify only a\npart of the canvas vector. In other words it does not pro-\nvide the network with an explicit selective attention mech-\nanism, which we believe to be crucial to large scale image\ngeneration. We refer to the above con\ufb01guration as \u201cDRAW\nwithout attention\u201d.\n3.2. Selective Attention Model\nTo endow the network with selective attention without sac-\nri\ufb01cing the bene\ufb01ts of gradient descent training, we take in-\nspiration from the differentiable attention mechanisms re-\ncently used in handwriting synthesis (Graves, 2013) and\nNeural Turing Machines (Graves et al., 2014).\nUnlike\nthe aforementioned works, we consider an explicitly two-\ndimensional form of attention, where an array of 2D Gaus-\nsian \ufb01lters is applied to the image, yielding an image\n\u2018patch\u2019 of smoothly varying location and zoom. This con-\n\ufb01guration, which we refer to simply as \u201cDRAW\u201d, some-\nwhat resembles the af\ufb01ne transformations used in computer\ngraphics-based autoencoders (Tieleman, 2014).\nAs illustrated in Fig. 3, the N \u00d7N grid of Gaussian \ufb01lters is\npositioned on the image by specifying the co-ordinates of\nthe grid centre and the stride distance between adjacent \ufb01l-\nters. The stride controls the \u2018zoom\u2019 of the patch; that is, the\nlarger the stride, the larger an area of the original image will\nbe visible in the attention patch, but the lower the effective\nresolution of the patch will be. The grid centre (gX, gY )\nand stride \u03b4 (both of which are real-valued) determine the\nmean location \u00b5i\nX, \u00b5j\nY of the \ufb01lter at row i, column j in the\npatch as follows:\n\u00b5i\nX = gX + (i \u2212N/2 \u22120.5) \u03b4\n(19)\n\u00b5j\nY = gY + (j \u2212N/2 \u22120.5) \u03b4\n(20)\nTwo more parameters are required to fully specify the at-\ntention model: the isotropic variance \u03c32 of the Gaussian\n\ufb01lters, and a scalar intensity \u03b3 that multiplies the \ufb01lter re-\nsponse. Given an A \u00d7 B input image x, all \ufb01ve attention\nparameters are dynamically determined at each time step\n\u03b4\ngY {\n{\ngX\n{\nFigure 3. Left: A 3 \u00d7 3 grid of \ufb01lters superimposed on an image.\nThe stride (\u03b4) and centre location (gX, gY ) are indicated. Right:\nThree N \u00d7 N patches extracted from the image (N = 12). The\ngreen rectangles on the left indicate the boundary and precision\n(\u03c3) of the patches, while the patches themselves are shown to the\nright. The top patch has a small \u03b4 and high \u03c3, giving a zoomed-in\nbut blurry view of the centre of the digit; the middle patch has\nlarge \u03b4 and low \u03c3, effectively downsampling the whole image;\nand the bottom patch has high \u03b4 and \u03c3.\nvia a linear transformation of the decoder output hdec:\n(\u02dcgX, \u02dcgY , log \u03c32, log \u02dc\u03b4, log \u03b3) = W (hdec)\n(21)\ngX = A + 1\n2\n(\u02dcgX + 1)\n(22)\ngY = B + 1\n2\n(\u02dcgY + 1)\n(23)\n\u03b4 = max(A, B) \u22121\nN \u22121\n\u02dc\u03b4\n(24)\nwhere the variance, stride and intensity are emitted in the\nlog-scale to ensure positivity. The scaling of gX, gY and \u03b4\nis chosen to ensure that the initial patch (with a randomly\ninitialised network) roughly covers the whole input image.\nGiven the attention parameters emitted by the decoder, the\nhorizontal and vertical \ufb01lterbank matrices FX and FY (di-\nmensions N \u00d7 A and N \u00d7 B respectively) are de\ufb01ned as\nfollows:\nFX[i, a] =\n1\nZX\nexp\n\u0012\n\u2212(a \u2212\u00b5i\nX)2\n2\u03c32\n\u0013\n(25)\nFY [j, b] =\n1\nZY\nexp\n \n\u2212(b \u2212\u00b5j\nY )2\n2\u03c32\n!\n(26)\nwhere (i, j) is a point in the attention patch, (a, b) is a point\nin the input image, and Zx, Zy are normalisation constants\nthat ensure that P\na FX[i, a] = 1 and P\nb FY [j, b] = 1.\nDRAW: A Recurrent Neural Network For Image Generation\nFigure 4. Zooming. Top Left: The original 100\u00d775 image. Top\nMiddle: A 12 \u00d7 12 patch extracted with 144 2D Gaussian \ufb01lters.\nTop Right: The reconstructed image when applying transposed\n\ufb01lters on the patch. Bottom: Only two 2D Gaussian \ufb01lters are\ndisplayed. The \ufb01rst one is used to produce the top-left patch fea-\nture. The last \ufb01lter is used to produce the bottom-right patch fea-\nture. By using different \ufb01lter weights, the attention can be moved\nto a different location.\n3.3. Reading and Writing With Attention\nGiven FX, FY and intensity \u03b3 determined by hdec\nt\u22121, along\nwith an input image x and error image \u02c6xt, the read opera-\ntion returns the concatenation of two N \u00d7 N patches from\nthe image and error image:\nread(x, \u02c6xt, hdec\nt\u22121) = \u03b3[FY xF T\nX, FY \u02c6xF T\nX]\n(27)\nNote that the same \ufb01lterbanks are used for both the image\nand error image. For the write operation, a distinct set of\nattention parameters \u02c6\u03b3, \u02c6FX and \u02c6FY are extracted from hdec\nt\n,\nthe order of transposition is reversed, and the intensity is\ninverted:\nwt = W (hdec\nt\n)\n(28)\nwrite(hdec\nt\n) = 1\n\u02c6\u03b3\n\u02c6F T\nY wt \u02c6FX\n(29)\nwhere wt is the N \u00d7 N writing patch emitted by hdec\nt\n. For\ncolour images each point in the input and error image (and\nhence in the reading and writing patches) is an RGB triple.\nIn this case the same reading and writing \ufb01lters are used for\nall three channels.\n4. Experimental Results\nWe assess the ability of DRAW to generate realistic-\nlooking images by training on three datasets of progres-\nsively increasing visual complexity: MNIST (LeCun et al.,\n1998), Street View House Numbers (SVHN) (Netzer et al.,\n2011) and CIFAR-10 (Krizhevsky, 2009).\nThe images\ngenerated by the network are always novel (not simply\ncopies of training examples), and are virtually indistin-\nguishable from real data for MNIST and SVHN; the gener-\nated CIFAR images are somewhat blurry, but still contain\nrecognisable structure from natural scenes. The binarized\nMNIST results substantially improve on the state of the art.\nAs a preliminary exercise, we also evaluate the 2D atten-\ntion module of the DRAW network on cluttered MNIST\nclassi\ufb01cation.\nFor all experiments, the model D(X|cT ) of the input data\nwas a Bernoulli distribution with means given by \u03c3(cT ).\nFor the MNIST experiments, the reconstruction loss from\nEq 9 was the usual binary cross-entropy term.\nFor the\nSVHN and CIFAR-10 experiments, the red, green and blue\npixel intensities were represented as numbers between 0\nand 1, which were then interpreted as independent colour\nemission probabilities. The reconstruction loss was there-\nfore the cross-entropy between the pixel intensities and the\nmodel probabilities. Although this approach worked well\nin practice, it means that the training loss did not corre-\nspond to the true compression cost of RGB images.\nNetwork hyper-parameters for all the experiments are\npresented in Table 3.\nThe Adam optimisation algo-\nrithm (Kingma & Ba, 2014) was used throughout.\nEx-\namples of generation sequences for MNIST and SVHN\nare provided in the accompanying video (https://www.\nyoutube.com/watch?v=Zt-7MI9eKEo).\n4.1. Cluttered MNIST Classi\ufb01cation\nTo test the classi\ufb01cation ef\ufb01cacy of the DRAW attention\nmechanism (as opposed to its ability to aid in image gener-\nation), we evaluate its performance on the 100 \u00d7 100 clut-\ntered translated MNIST task (Mnih et al., 2014). Each im-\nage in cluttered MNIST contains many digit-like fragments\nof visual clutter that the network must distinguish from the\ntrue digit to be classi\ufb01ed. As illustrated in Fig. 5, having\nan iterative attention model allows the network to progres-\nsively zoom in on the relevant region of the image, and\nignore the clutter outside it.\nOur model consists of an LSTM recurrent network that re-\nceives a 12 \u00d7 12 \u2018glimpse\u2019 from the input image at each\ntime-step, using the selective read operation de\ufb01ned in Sec-\ntion 3.2. After a \ufb01xed number of glimpses the network uses\na softmax layer to classify the MNIST digit. The network\nis similar to the recently introduced Recurrent Attention\nModel (RAM) (Mnih et al., 2014), except that our attention\nmethod is differentiable; we therefore refer to it as \u201cDiffer-\nentiable RAM\u201d.\nThe results in Table 1 demonstrate a signi\ufb01cant improve-\nment in test error over the original RAM network. More-\nover our model had only a single attention patch at each\nDRAW: A Recurrent Neural Network For Image Generation\nTime\nFigure 5. Cluttered MNIST classi\ufb01cation with attention. Each\nsequence shows a succession of four glimpses taken by the net-\nwork while classifying cluttered translated MNIST. The green\nrectangle indicates the size and location of the attention patch,\nwhile the line width represents the variance of the \ufb01lters.\nTable 1. Classi\ufb01cation test error on 100 \u00d7 100 Cluttered Trans-\nlated MNIST.\nModel\nError\nConvolutional, 2 layers\n14.35%\nRAM, 4 glimpses, 12 \u00d7 12, 4 scales\n9.41%\nRAM, 8 glimpses, 12 \u00d7 12, 4 scales\n8.11%\nDifferentiable RAM, 4 glimpses, 12 \u00d7 12\n4.18%\nDifferentiable RAM, 8 glimpses, 12 \u00d7 12\n3.36%\ntime-step, whereas RAM used four, at different zooms.\n4.2. MNIST Generation\nWe trained the full DRAW network as a generative model\non the binarized MNIST dataset (Salakhutdinov & Mur-\nray, 2008). This dataset has been widely studied in the\nliterature, allowing us to compare the numerical perfor-\nmance (measured in average nats per image on the test\nset) of DRAW with existing methods. Table 2 shows that\nDRAW without selective attention performs comparably to\nother recent generative models such as DARN, NADE and\nDBMs, and that DRAW with attention considerably im-\nproves on the state of the art.\nTable 2. Negative log-likelihood (in nats) per test-set example on\nthe binarised MNIST data set. The right hand column, where\npresent, gives an upper bound (Eq. 12) on the negative log-\nlikelihood. The previous results are from [1] (Salakhutdinov &\nHinton, 2009), [2] (Murray & Salakhutdinov, 2009), [3] (Uria\net al., 2014), [4] (Raiko et al., 2014), [5] (Rezende et al., 2014),\n[6] (Salimans et al., 2014), [7] (Gregor et al., 2014).\nModel\n\u2212log p\n\u2264\nDBM 2hl [1]\n\u224884.62\nDBN 2hl [2]\n\u224884.55\nNADE [3]\n88.33\nEoNADE 2hl (128 orderings) [3]\n85.10\nEoNADE-5 2hl (128 orderings) [4]\n84.68\nDLGM [5]\n\u224886.60\nDLGM 8 leapfrog steps [6]\n\u224885.51\n88.30\nDARN 1hl [7]\n\u224884.13\n88.30\nDARN 12hl [7]\n-\n87.72\nDRAW without attention\n-\n87.40\nDRAW\n-\n80.97\nFigure 6. Generated MNIST images. All digits were generated\nby DRAW except those in the rightmost column, which shows the\ntraining set images closest to those in the column second to the\nright (pixelwise L2 is the distance measure). Note that the net-\nwork was trained on binary samples, while the generated images\nare mean probabilities.\nOnce the DRAW network was trained, we generated\nMNIST digits following the method in Section 2.3, exam-\nples of which are presented in Fig. 6. Fig. 7 illustrates\nthe image generation sequence for a DRAW network with-\nout selective attention (see Section 3.1). It is interesting to\ncompare this with the generation sequence for DRAW with\nattention, as depicted in Fig. 1. Whereas without attention\nit progressively sharpens a blurred image in a global way,\nDRAW: A Recurrent Neural Network For Image Generation\nTime\nFigure 7. MNIST generation sequences for DRAW without at-\ntention. Notice how the network \ufb01rst generates a very blurry im-\nage that is subsequently re\ufb01ned.\nwith attention it constructs the digit by tracing the lines\u2014\nmuch like a person with a pen.\n4.3. MNIST Generation with Two Digits\nThe main motivation for using an attention-based genera-\ntive model is that large images can be built up iteratively,\nby adding to a small part of the image at a time. To test\nthis capability in a controlled fashion, we trained DRAW\nto generate images with two 28 \u00d7 28 MNIST images cho-\nsen at random and placed at random locations in a 60 \u00d7 60\nblack background. In cases where the two digits overlap,\nthe pixel intensities were added together at each point and\nclipped to be no greater than one. Examples of generated\ndata are shown in Fig. 8. The network typically generates\none digit and then the other, suggesting an ability to recre-\nate composite scenes from simple pieces.\n4.4. Street View House Number Generation\nMNIST digits are very simplistic in terms of visual struc-\nture, and we were keen to see how well DRAW performed\non natural images. Our \ufb01rst natural image generation ex-\nperiment used the multi-digit Street View House Numbers\ndataset (Netzer et al., 2011). We used the same preprocess-\ning as (Goodfellow et al., 2013), yielding a 64 \u00d7 64 house\nnumber image for each training example. The network was\nthen trained using 54 \u00d7 54 patches extracted at random lo-\ncations from the preprocessed images. The SVHN training\nset contains 231,053 images, and the validation set contains\n4,701 images.\nThe house number images generated by the network are\nFigure 8. Generated MNIST images with two digits.\nFigure 9. Generated SVHN images.\nThe rightmost column\nshows the training images closest (in L2 distance) to the gener-\nated images beside them. Note that the two columns are visually\nsimilar, but the numbers are generally different.\nhighly realistic, as shown in Figs. 9 and 10. Fig. 11 reveals\nthat, despite the long training time, the DRAW network un-\nder\ufb01t the SVHN training data.\n4.5. Generating CIFAR Images\nThe most challenging dataset we applied DRAW to was\nthe CIFAR-10 collection of natural images (Krizhevsky,\nDRAW: A Recurrent Neural Network For Image Generation\nTable 3. Experimental Hyper-Parameters.\nTask\n#glimpses\nLSTM #h\n#z\nRead Size\nWrite Size\n100 \u00d7 100 MNIST Classi\ufb01cation\n8\n256\n-\n12 \u00d7 12\n-\nMNIST Model\n64\n256\n100\n2 \u00d7 2\n5 \u00d7 5\nSVHN Model\n32\n800\n100\n12 \u00d7 12\n12 \u00d7 12\nCIFAR Model\n64\n400\n200\n5 \u00d7 5\n5 \u00d7 5\ns\nTime\nFigure 10. SVHN Generation Sequences. The red rectangle in-\ndicates the attention patch. Notice how the network draws the dig-\nits one at a time, and how it moves and scales the writing patch to\nproduce numbers with different slopes and sizes.\n 5060\n 5080\n 5100\n 5120\n 5140\n 5160\n 5180\n 5200\n 5220\n 0\n 50\n 100\n 150\n 200\n 250\n 300\n 350\ncost per example\nminibatch number (thousands)\ntraining\nvalidation\nFigure 11. Training and validation cost on SVHN. The valida-\ntion cost is consistently lower because the validation set patches\nwere extracted from the image centre (rather than from random\nlocations, as in the training set). The network was never able to\nover\ufb01t on the training data.\n2009). CIFAR-10 is very diverse, and with only 50,000\ntraining examples it is very dif\ufb01cult to generate realistic-\nFigure 12. Generated CIFAR images.\nThe rightmost column\nshows the nearest training examples to the column beside it.\nlooking objects without over\ufb01tting (in other words, without\ncopying from the training set). Nonetheless the images in\nFig. 12 demonstrate that DRAW is able to capture much of\nthe shape, colour and composition of real photographs.\n5. Conclusion\nThis paper introduced the Deep Recurrent Attentive Writer\n(DRAW) neural network architecture, and demonstrated its\nability to generate highly realistic natural images such as\nphotographs of house numbers, as well as improving on the\nbest known results for binarized MNIST generation. We\nalso established that the two-dimensional differentiable at-\ntention mechanism embedded in DRAW is bene\ufb01cial not\nonly to image generation, but also to image classi\ufb01cation.\nAcknowledgments\nOf the many who assisted in creating this paper, we are es-\npecially thankful to Koray Kavukcuoglu, Volodymyr Mnih,\nJimmy Ba, Yaroslav Bulatov, Greg Wayne, Andrei Rusu\nand Shakir Mohamed.\nDRAW: A Recurrent Neural Network For Image Generation\nReferences\nBa, Jimmy, Mnih, Volodymyr, and Kavukcuoglu, Koray.\nMultiple object recognition with visual attention. arXiv\npreprint arXiv:1412.7755, 2014.\nDayan, Peter, Hinton, Geoffrey E, Neal, Radford M, and\nZemel, Richard S. The helmholtz machine. Neural com-\nputation, 7(5):889\u2013904, 1995.\nDenil, Misha, Bazzani, Loris, Larochelle, Hugo, and\nde Freitas, Nando. Learning where to attend with deep\narchitectures for image tracking. Neural computation,\n24(8):2151\u20132184, 2012.\nGers, Felix A, Schmidhuber, J\u00a8urgen, and Cummins, Fred.\nLearning to forget: Continual prediction with lstm. Neu-\nral computation, 12(10):2451\u20132471, 2000.\nGoodfellow,\nIan J, Bulatov,\nYaroslav,\nIbarz,\nJulian,\nArnoud,\nSacha,\nand\nShet,\nVinay.\nMulti-digit\nnumber recognition from street view imagery using\ndeep convolutional neural networks.\narXiv preprint\narXiv:1312.6082, 2013.\nGraves, Alex. Generating sequences with recurrent neural\nnetworks. arXiv preprint arXiv:1308.0850, 2013.\nGraves, Alex, Wayne, Greg, and Danihelka, Ivo. Neural\nturing machines. arXiv preprint arXiv:1410.5401, 2014.\nGregor, Karol, Danihelka, Ivo, Mnih, Andriy, Blundell,\nCharles, and Wierstra, Daan. Deep autoregressive net-\nworks. In Proceedings of the 31st International Confer-\nence on Machine Learning, 2014.\nHinton, Geoffrey E and Salakhutdinov, Ruslan R. Reduc-\ning the dimensionality of data with neural networks. Sci-\nence, 313(5786):504\u2013507, 2006.\nHochreiter, Sepp and Schmidhuber, J\u00a8urgen. Long short-\nterm memory.\nNeural computation, 9(8):1735\u20131780,\n1997.\nKingma,\nDiederik\nand\nBa,\nJimmy.\nAdam:\nA\nmethod for stochastic optimization.\narXiv preprint\narXiv:1412.6980, 2014.\nKingma, Diederik P and Welling, Max.\nAuto-encoding\nvariational bayes. In Proceedings of the International\nConference on Learning Representations (ICLR), 2014.\nKrizhevsky, Alex.\nLearning multiple layers of features\nfrom tiny images. 2009.\nLarochelle, Hugo and Hinton, Geoffrey E.\nLearning to\ncombine foveal glimpses with a third-order boltzmann\nmachine. In Advances in Neural Information Processing\nSystems, pp. 1243\u20131251. 2010.\nLarochelle, Hugo and Murray, Iain. The neural autoregres-\nsive distribution estimator. Journal of Machine Learning\nResearch, 15:29\u201337, 2011.\nLeCun, Yann, Bottou, L\u00b4eon, Bengio, Yoshua, and Haffner,\nPatrick.\nGradient-based learning applied to document\nrecognition.\nProceedings of the IEEE, 86(11):2278\u2013\n2324, 1998.\nMnih, Andriy and Gregor, Karol. Neural variational infer-\nence and learning in belief networks. In Proceedings of\nthe 31st International Conference on Machine Learning,\n2014.\nMnih, Volodymyr, Heess, Nicolas, Graves, Alex, et al. Re-\ncurrent models of visual attention. In Advances in Neural\nInformation Processing Systems, pp. 2204\u20132212, 2014.\nMurray, Iain and Salakhutdinov, Ruslan. Evaluating prob-\nabilities under high-dimensional latent variable models.\nIn Advances in neural information processing systems,\npp. 1137\u20131144, 2009.\nNetzer, Yuval, Wang, Tao, Coates, Adam, Bissacco,\nAlessandro, Wu, Bo, and Ng, Andrew Y. Reading dig-\nits in natural images with unsupervised feature learning.\n2011.\nRaiko, Tapani, Li, Yao, Cho, Kyunghyun, and Bengio,\nYoshua. Iterative neural autoregressive distribution es-\ntimator nade-k. In Advances in Neural Information Pro-\ncessing Systems, pp. 325\u2013333. 2014.\nRanzato, Marc\u2019Aurelio. On learning where to look. arXiv\npreprint arXiv:1405.5488, 2014.\nRezende, Danilo J, Mohamed, Shakir, and Wierstra, Daan.\nStochastic backpropagation and approximate inference\nin deep generative models. In Proceedings of the 31st In-\nternational Conference on Machine Learning, pp. 1278\u2013\n1286, 2014.\nSalakhutdinov, Ruslan and Hinton, Geoffrey E. Deep boltz-\nmann machines. In International Conference on Arti\ufb01-\ncial Intelligence and Statistics, pp. 448\u2013455, 2009.\nSalakhutdinov, Ruslan and Murray, Iain. On the quantita-\ntive analysis of Deep Belief Networks. In Proceedings\nof the 25th Annual International Conference on Machine\nLearning, pp. 872\u2013879. Omnipress, 2008.\nSalimans, Tim, Kingma, Diederik P, and Welling, Max.\nMarkov chain monte carlo and variational inference:\nBridging the gap. arXiv preprint arXiv:1410.6460, 2014.\nSermanet, Pierre, Frome, Andrea, and Real, Esteban. At-\ntention for \ufb01ne-grained categorization. arXiv preprint\narXiv:1412.7054, 2014.\nDRAW: A Recurrent Neural Network For Image Generation\nSutskever, Ilya, Vinyals, Oriol, and Le, Quoc VV.\nSe-\nquence to sequence learning with neural networks. In\nAdvances in Neural Information Processing Systems, pp.\n3104\u20133112, 2014.\nTang, Yichuan, Srivastava, Nitish, and Salakhutdinov, Rus-\nlan. Learning generative models with visual attention.\narXiv preprint arXiv:1312.6110, 2013.\nTieleman, Tijmen. Optimizing Neural Networks that Gen-\nerate Images. PhD thesis, University of Toronto, 2014.\nUria, Benigno, Murray, Iain, and Larochelle, Hugo. A deep\nand tractable density estimator. In Proceedings of the\n31st International Conference on Machine Learning, pp.\n467\u2013475, 2014.\nZheng, Yin, Zemel, Richard S, Zhang, Yu-Jin, and\nLarochelle, Hugo.\nA neural autoregressive approach\nto attention-based recognition. International Journal of\nComputer Vision, pp. 1\u201313, 2014.\n",
        "sentence": " Such models have been applied successfully in image classification [12, 4, 3, 2], object tracking [13, 3], machine translation [6], caption generation [5], and image generation [14, 15].",
        "context": "ing and modifying images.\nSection 4 provides experi-\nmental results on the MNIST, Street View House Num-\nbers and CIFAR-10 datasets, with examples of generated\nimages; and concluding remarks are given in Section 5.\nMultiple object recognition with visual attention. arXiv\npreprint arXiv:1412.7755, 2014.\nDayan, Peter, Hinton, Geoffrey E, Neal, Radford M, and\nZemel, Richard S. The helmholtz machine. Neural com-\nputation, 7(5):889\u2013904, 1995.\nmodel probabilities. Although this approach worked well\nin practice, it means that the training loss did not corre-\nspond to the true compression cost of RGB images.\nNetwork hyper-parameters for all the experiments are\npresented in Table 3."
    },
    {
        "title": "Connectionist learning of belief networks",
        "author": [
            "Radford M. Neal"
        ],
        "venue": "Artificial Intelligence,",
        "citeRegEx": "16",
        "shortCiteRegEx": "16",
        "year": 1992,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " This difficulty is closely related to the problem of posterior inference in training deep generative models such as sigmoid belief networks [16].",
        "context": null
    },
    {
        "title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
        "author": [
            "R.J. Williams"
        ],
        "venue": "Machine Learning, 8:229\u2013256,",
        "citeRegEx": "17",
        "shortCiteRegEx": null,
        "year": 1992,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " In particular, one uses an algortihm from reinforcement learning called REINFORCE [17], which attempts to infer a reward baseline for each instance. Past work using similar gradient updates has found significant benefit from the use of control variates, or reward baselines, to reduce the variance [17, 10, 3, 11, 2].",
        "context": null
    },
    {
        "title": "Auto-encoding variational Bayes",
        "author": [
            "D.P. Kingma",
            "M. Welling"
        ],
        "venue": "International Conference on Learning Representations,",
        "citeRegEx": "18",
        "shortCiteRegEx": null,
        "year": 2014,
        "abstract": "This paper employs the Auto-Encoding Variational Bayes (AEVB) estimator based on Stochastic Gradient Variational Bayes (SGVB), designed to optimize recognition models for challenging posterior distributions and large-scale datasets. It has been applied to the mnist dataset and extended to form a Dynamic Bayesian Network (DBN) in the context of time series. The paper delves into Bayesian inference, variational methods, and the fusion of Variational Autoencoders (VAEs) and variational techniques. Emphasis is placed on reparameterization for achieving efficient optimization. AEVB employs VAEs as an approximation for intricate posterior distributions.",
        "full_text": "",
        "sentence": " Another method based on inference networks is variational autoencoders [18, 19], which exploit a clever reparameterization of the probabilistic model in order to improve the signal in the stochastic gradients.",
        "context": null
    },
    {
        "title": "Stochastic backpropagation and approximate inference in deep generative models",
        "author": [
            "D.J. Rezende",
            "S. Mohamed",
            "D. Wierstra"
        ],
        "venue": "International Conference on Machine Learning,",
        "citeRegEx": "19",
        "shortCiteRegEx": null,
        "year": 2014,
        "abstract": "We marry ideas from deep neural networks and approximate Bayesian inference\nto derive a generalised class of deep, directed generative models, endowed with\na new algorithm for scalable inference and learning. Our algorithm introduces a\nrecognition model to represent approximate posterior distributions, and that\nacts as a stochastic encoder of the data. We develop stochastic\nback-propagation -- rules for back-propagation through stochastic variables --\nand use this to develop an algorithm that allows for joint optimisation of the\nparameters of both the generative and recognition model. We demonstrate on\nseveral real-world data sets that the model generates realistic samples,\nprovides accurate imputations of missing data and is a useful tool for\nhigh-dimensional data visualisation.",
        "full_text": "Stochastic Backpropagation and Approximate Inference\nin Deep Generative Models\nDanilo J. Rezende, Shakir Mohamed, Daan Wierstra\n{danilor, shakir, daanw}@google.com\nGoogle DeepMind, London\nAbstract\nWe marry ideas from deep neural networks\nand approximate Bayesian inference to derive\na generalised class of deep, directed genera-\ntive models, endowed with a new algorithm\nfor scalable inference and learning. Our algo-\nrithm introduces a recognition model to rep-\nresent an approximate posterior distribution\nand uses this for optimisation of a variational\nlower bound.\nWe develop stochastic back-\npropagation \u2013 rules for gradient backpropa-\ngation through stochastic variables \u2013 and de-\nrive an algorithm that allows for joint optimi-\nsation of the parameters of both the genera-\ntive and recognition models. We demonstrate\non several real-world data sets that by using\nstochastic backpropagation and variational\ninference, we obtain models that are able to\ngenerate realistic samples of data, allow for\naccurate imputations of missing data, and\nprovide a useful tool for high-dimensional\ndata visualisation.\n1. Introduction\nThere is an immense e\ufb00ort in machine learning and\nstatistics to develop accurate and scalable probabilistic\nmodels of data. Such models are called upon whenever\nwe are faced with tasks requiring probabilistic reason-\ning, such as prediction, missing data imputation and\nuncertainty estimation; or in simulation-based analy-\nses, common in many scienti\ufb01c \ufb01elds such as genetics,\nrobotics and control that require generating a large\nnumber of independent samples from the model.\nRecent e\ufb00orts to develop generative models have fo-\ncused on directed models, since samples are easily ob-\ntained by ancestral sampling from the generative pro-\ncess. Directed models such as belief networks and sim-\nilar latent variable models (Dayan et al., 1995; Frey,\n1996; Saul et al., 1996; Bartholomew & Knott, 1999;\nProceedings of the 31 st International Conference on Ma-\nchine Learning, Beijing, China, 2014. JMLR: W&CP vol-\nume 32. Copyright 2014 by the author(s).\nUria et al., 2014; Gregor et al., 2014) can be easily sam-\npled from, but in most cases, e\ufb03cient inference algo-\nrithms have remained elusive. These e\ufb00orts, combined\nwith the demand for accurate probabilistic inferences\nand fast simulation, lead us to seek generative models\nthat are i) deep, since hierarchical architectures allow\nus to capture complex structure in the data, ii) al-\nlow for fast sampling of fantasy data from the inferred\nmodel, and iii) are computationally tractable and scal-\nable to high-dimensional data.\nWe meet these desiderata by introducing a class of\ndeep, directed generative models with Gaussian la-\ntent variables at each layer. To allow for e\ufb03cient and\ntractable inference, we use introduce an approximate\nrepresentation of the posterior over the latent variables\nusing a recognition model that acts as a stochastic en-\ncoder of the data. For the generative model, we de-\nrive the objective function for optimisation using vari-\national principles; for the recognition model, we spec-\nify its structure and regularisation by exploiting recent\nadvances in deep learning. Using this construction, we\ncan train the entire model by a modi\ufb01ed form of gra-\ndient backpropagation that allows for optimisation of\nthe parameters of both the generative and recognition\nmodels jointly.\nWe build upon the large body of prior work (in section\n6) and make the following contributions:\n\u2022 We combine ideas from deep neural networks and\nprobabilistic latent variable modelling to derive a\ngeneral class of deep, non-linear latent Gaussian\nmodels (section 2).\n\u2022 We present a new approach for scalable varia-\ntional inference that allows for joint optimisation\nof both variational and model parameters by ex-\nploiting the properties of latent Gaussian distri-\nbutions and gradient backpropagation (sections 3\nand 4).\n\u2022 We provide a comprehensive and systematic eval-\nuation of the model demonstrating its applicabil-\nity to problems in simulation, visualisation, pre-\ndiction and missing data imputation (section 5).\narXiv:1401.4082v3  [stat.ML]  30 May 2014\nStochastic Backpropagation in DLGMs\nhn,2\nhn,1\nvn\nn = 1 . . . N\n\u03b8g\n(a)\ngenerative\nrecognition\n. . .\n. . .\n. . .\n. . .\n\u00b5\nC\n\u00b5\nC\n. . .\n\u00b5\nC\n. . .\n. . .\n. . .\n(b)\nFigure 1. (a) Graphical model for DLGMs (5).\n(b) The\ncorresponding computational graph. Black arrows indicate\nthe forward pass of sampling from the recognition and gen-\nerative models: Solid lines indicate propagation of deter-\nministic activations, dotted lines indicate propagation of\nsamples. Red arrows indicate the backward pass for gra-\ndient computation: Solid lines indicate paths where deter-\nministic backpropagation is used, dashed arrows indicate\nstochastic backpropagation.\n2. Deep Latent Gaussian Models\nDeep latent Gaussian models (DLGMs) are a general\nclass of deep directed graphical models that consist of\nGaussian latent variables at each layer of a process-\ning hierarchy. The model consists of L layers of latent\nvariables. To generate a sample from the model, we\nbegin at the top-most layer (L) by drawing from a\nGaussian distribution. The activation hl at any lower\nlayer is formed by a non-linear transformation of the\nlayer above hl+1, perturbed by Gaussian noise. We\ndescend through the hierarchy and generate observa-\ntions v by sampling from the observation likelihood\nusing the activation of the lowest layer h1. This pro-\ncess is described graphically in \ufb01gure 1(a).\nThis generative process is described as follows:\n\u03bel \u223cN(\u03bel|0, I),\nl = 1, . . . , L\n(1)\nhL = GL\u03beL,\n(2)\nhl = Tl(hl+1) + Gl\u03bel,\nl = 1 . . . L \u22121\n(3)\nv \u223c\u03c0(v|T0(h1)),\n(4)\nwhere \u03bel are mutually independent Gaussian variables.\nThe transformations Tl represent multi-layer percep-\ntrons (MLPs) and Gl are matrices.\nAt the visible\nlayer, the data is generated from any appropriate dis-\ntribution \u03c0(v|\u00b7) whose parameters are speci\ufb01ed by a\ntransformation of the \ufb01rst latent layer. Throughout\nthe paper we refer to the set of parameters in this gen-\nerative model by \u03b8g, i.e. the parameters of the maps\nTl and the matrices Gl. This construction allows us to\nmake use of as many deterministic and stochastic lay-\ners as needed. We adopt a weak Gaussian prior over\n\u03b8g, p(\u03b8g) = N(\u03b8|0, \u03baI).\nThe joint probability distribution of this model can be\nexpressed in two equivalent ways:\np(v,h)=p(v|h1,\u03b8g)p(hL|\u03b8g)p(\u03b8g)\nL\u22121\nY\nl=1\npl(hl|hl+1,\u03b8g) (5)\np(v,\u03be)= p(v|h1(\u03be1...L), \u03b8g)p(\u03b8g)\nL\nY\nl=1\nN(\u03be|0, I).\n(6)\nThe conditional distributions p(hl|hl+1) are implic-\nitly de\ufb01ned by equation (3) and are Gaussian dis-\ntributions with mean \u00b5l = Tl(hl+1) and covariance\nSl = GlG\u22a4\nl .\nEquation (6) makes explicit that this\ngenerative model works by applying a complex non-\nlinear transformation to a spherical Gaussian distribu-\ntion p(\u03be) = QL\nl=1 N(\u03bel|0, I) such that the transformed\ndistribution tries to match the empirical distribution.\nA graphical model corresponding to equation (5) is\nshown in \ufb01gure 1(a).\nThis speci\ufb01cation for deep latent Gaussian models\n(DLGMs) generalises a number of well known mod-\nels. When we have only one layer of latent variables\nand use a linear mapping T(\u00b7), we recover factor anal-\nysis (Bartholomew & Knott, 1999) \u2013 more general\nmappings allow for a non-linear factor analysis (Lap-\npalainen & Honkela, 2000). When the mappings are\nof the form Tl(h) = Alf(h) + bl, for simple element-\nwise non-linearities f such as the probit function or\nthe recti\ufb01ed linearity, we recover the non-linear Gaus-\nsian belief network (Frey & Hinton, 1999).\nWe de-\nscribe the relationship to other existing models in sec-\ntion 6.\nGiven this speci\ufb01cation, our key task is to\ndevelop a method for tractable inference. A number\nof approaches are known and widely used, and include:\nmean-\ufb01eld variational EM (Beal, 2003); the wake-sleep\nalgorithm (Dayan, 2000); and stochastic variational\nmethods and related control-variate estimators (Wil-\nson, 1984; Williams, 1992; Ho\ufb00man et al., 2013). We\nalso follow a stochastic variational approach, but shall\ndevelop an alternative to these existing inference algo-\nrithms that overcomes many of their limitations and\nthat is both scalable and e\ufb03cient.\n3. Stochastic Backpropagation\nGradient descent methods in latent variable mod-\nels\ntypically\nrequire\ncomputations\nof\nthe\nform\n\u2207\u03b8Eq\u03b8 [f(\u03be)], where the expectation is taken with re-\nspect to a distribution q\u03b8(\u00b7) with parameters \u03b8, and f\nis a loss function that we assume to be integrable and\nsmooth. This quantity is di\ufb03cult to compute directly\nsince i) the expectation is unknown for most problems,\nand ii) there is an indirect dependency on the param-\neters of q over which the expectation is taken.\nWe now develop the key identities that are used to\nallow for e\ufb03cient inference by exploiting speci\ufb01c prop-\nStochastic Backpropagation in DLGMs\nerties of the problem of computing gradients through\nrandom variables.\nWe refer to this computational\nstrategy as stochastic backpropagation.\n3.1. Gaussian Backpropagation (GBP)\nWhen the distribution q is a K-dimensional Gaussian\nN(\u03be|\u00b5, C) the required gradients can be computed us-\ning the Gaussian gradient identities:\n\u2207\u00b5iEN (\u00b5,C) [f(\u03be)] = EN (\u00b5,C) [\u2207\u03beif(\u03be)] ,\n(7)\n\u2207CijEN (\u00b5,C) [f(\u03be)] = 1\n2EN (\u00b5,C)\nh\n\u22072\n\u03bei,\u03bejf(\u03be)\ni\n,\n(8)\nwhich are due to the theorems by Bonnet (1964) and\nPrice (1958), respectively. These equations are true\nin expectation for any integrable and smooth func-\ntion f(\u03be). Equation (7) is a direct consequence of the\nlocation-scale transformation for the Gaussian (dis-\ncussed in section 3.2). Equation (8) can be derived\nby successive application of the product rule for in-\ntegrals; we provide the proofs for these identities in\nappendix B.\nEquations (7) and (8) are especially interesting since\nthey allow for unbiased gradient estimates by using a\nsmall number of samples from q. Assume that both\nthe mean \u00b5 and covariance matrix C depend on a\nparameter vector \u03b8. We are now able to write a general\nrule for Gaussian gradient computation by combining\nequations (7) and (8) and using the chain rule:\n\u2207\u03b8EN(\u00b5,C)[f(\u03be)]=EN(\u00b5,C)\n\u0014\ng\u22a4\u2202\u00b5\n\u2202\u03b8 + 1\n2Tr\n\u0012\nH\u2202C\n\u2202\u03b8\n\u0013\u0015\n(9)\nwhere g and H are the gradient and the Hessian of the\nfunction f(\u03be), respectively. Equation (9) can be inter-\npreted as a modi\ufb01ed backpropagation rule for Gaus-\nsian distributions that takes into account the gradients\nthrough the mean \u00b5 and covariance C. This reduces\nto the standard backpropagation rule when C is con-\nstant. Unfortunately this rule requires knowledge of\nthe Hessian matrix of f(\u03be), which has an algorithmic\ncomplexity O(K3). For inference in DLGMs, we later\nintroduce an unbiased though higher variance estima-\ntor that requires only quadratic complexity.\n3.2. Generalised Backpropagation Rules\nWe describe two approaches to derive general back-\npropagation rules for non-Gaussian q-distributions.\nUsing the product rule for integrals. For many\nexponential family distributions, it is possible to \ufb01nd\na function B(\u03be; \u03b8) to ensure that\n\u2207\u03b8Ep(\u03be|\u03b8)[f(\u03be)]== \u2212Ep(\u03be|\u03b8)[\u2207\u03be[B(\u03be; \u03b8)f(\u03be)]].\nThat is, we express the gradient with respect to the\nparameters of q as an expectation of gradients with\nrespect to the random variables themselves. This ap-\nproach can be used to derive rules for many distribu-\ntions such as the Gaussian, inverse Gamma and log-\nNormal. We discuss this in more detail in appendix\nC.\nUsing suitable co-ordinate transformations.\nWe can also derive stochastic backpropagation rules\nfor any distribution that can be written as a smooth,\ninvertible transformation of a standard base distribu-\ntion. For example, any Gaussian distribution N(\u00b5, C)\ncan be obtained as a transformation of a spherical\nGaussian \u03f5 \u223cN(0, I), using the transformation y =\n\u00b5+R\u03f5 and C = RR\u22a4. The gradient of the expectation\nwith respect to R is then:\n\u2207REN(\u00b5,C) [f(\u03be)] = \u2207REN (0,I) [f(\u00b5 + R\u03f5)]\n= EN (0,I)\n\u0002\n\u03f5g\u22a4\u0003\n,\n(10)\nwhere g is the gradient of f evaluated at \u00b5 + R\u03f5 and\nprovides a lower-cost alternative to Price\u2019s theorem\n(8).\nSuch transformations are well known for many\ndistributions, especially those with a self-similarity\nproperty or location-scale formulation, such as the\nGaussian, Student\u2019s t-distribution, stable distribu-\ntions, and generalised extreme value distributions.\nStochastic backpropagation in other contexts.\nThe Gaussian gradient identities described above do\nnot appear to be widely used. These identities have\nbeen recognised by Opper & Archambeau (2009) for\nvariational inference in Gaussian process regression,\nand following this work, by Graves (2011) for param-\neter learning in large neural networks. Concurrently\nwith this paper, Kingma & Welling (2014) present an\nalternative discussion of stochastic backpropagation.\nOur approaches were developed simultaneously and\nprovide complementary perspectives on the use and\nderivation of stochastic backpropagation rules.\n4. Scalable Inference in DLGMs\nWe use the matrix V to refer to the full data set of\nsize N \u00d7 D with observations vn = [vn1, . . . , vnD]\u22a4.\n4.1. Free Energy Objective\nTo perform inference in DLGMs we must integrate out\nthe e\ufb00ect of any latent variables \u2013 this requires us to\ncompute the integrated or marginal likelihood. In gen-\neral, this will be an intractable integration and instead\nwe optimise a lower bound on the marginal likelihood.\nWe introduce an approximate posterior distribution\nq(\u00b7) and apply Jensen\u2019s inequality following the varia-\ntional principle (Beal, 2003) to obtain:\nStochastic Backpropagation in DLGMs\nL(V) = \u2212log p(V) = \u2212log\nZ\np(V|\u03be, \u03b8g)p(\u03be, \u03b8g)d\u03be\n= \u2212log\nZ q(\u03be)\nq(\u03be)p(V|\u03be, \u03b8g)p(\u03be, \u03b8g)d\u03be\n(11)\n\u2264F(V)=DKL[q(\u03be)\u2225p(\u03be)]\u2212Eq [log p(V|\u03be,\u03b8g)p(\u03b8g)] .\nThis objective consists of two terms:\nthe \ufb01rst is\nthe KL-divergence between the variational distribution\nand the prior distribution (which acts a regulariser),\nand the second is a reconstruction error.\nWe specify the approximate posterior as a distribution\nq(\u03be|v) that is conditioned on the observed data. This\ndistribution can be speci\ufb01ed as any directed acyclic\ngraph where each node of the graph is a Gaussian\nconditioned, through linear or non-linear transforma-\ntions, on its parents.\nThe joint distribution in this\ncase is non-Gaussian, but stochastic backpropagation\ncan still be applied.\nFor simplicity, we use a q(\u03be|v) that is a Gaussian dis-\ntribution that factorises across the L layers (but not\nnecessarily within a layer):\nq(\u03be|V, \u03b8r) =\nN\nY\nn=1\nL\nY\nl=1\nN\n\u0000\u03ben,l|\u00b5l(vn), Cl(vn)\n\u0001\n,\n(12)\nwhere the mean \u00b5l(\u00b7) and covariance Cl(\u00b7) are generic\nmaps represented by deep neural networks.\nParam-\neters of the q-distribution are denoted by the vector\n\u03b8r.\nFor a Gaussian prior and a Gaussian recognition\nmodel, the KL term in (11) can be computed ana-\nlytically and the free energy becomes:\nDKL[N(\u00b5,C)\u2225N(0,I)]= 1\n2\n\u0002\nTr(C)\u2212log |C|+\u00b5\u22a4\u00b5\u2212D\n\u0003\n,\nF(V) = \u2212\nX\nn\nEq [log p(vn|h(\u03ben))] +\n1\n2\u03ba\u2225\u03b8g\u22252\n+ 1\n2\nX\nn,l\n\u0002\n\u2225\u00b5n,l\u22252+Tr(Cn,l)\u2212log |Cn,l|\u22121\n\u0003\n, (13)\nwhere Tr(C) and |C| indicate the trace and the deter-\nminant of the covariance matrix C, respectively.\nThe speci\ufb01cation of an approximate posterior distri-\nbution that is conditioned on the observed data is the\n\ufb01rst component of an e\ufb03cient variational inference al-\ngorithm. We shall refer to the distribution q(\u03be|v) (12)\nas a recognition model, whose design is independent\nof the generative model. A recognition model allows\nus introduce a form of amortised inference (Gershman\n& Goodman, 2014) for variational methods in which\nwe share statistical strength by allowing for generali-\nsation across the posterior estimates for all latent vari-\nables using a model. The implication of this general-\nisation ability is: faster convergence during training;\nand faster inference at test time since we only require a\nsingle pass through the recognition model, rather than\nneeding to perform any iterative computations (such\nas in a generalised E-step).\nTo allow for the best possible inference, the speci\ufb01ca-\ntion of the recognition model must be \ufb02exible enough\nto provide an accurate approximation of the poste-\nrior distribution \u2013 motivating the use of deep neu-\nral networks.\nWe regularise the recognition model\nby introducing additional noise, speci\ufb01cally, bit-\ufb02ip\nor drop-out noise at the input layer and small addi-\ntional Gaussian noise to samples from the recognition\nmodel. We use recti\ufb01ed linear activation functions as\nnon-linearities for any deterministic layers of the neu-\nral network. We found that such regularisation is es-\nsential and without it the recognition model is unable\nto provide accurate inferences for unseen data points.\n4.2. Gradients of the Free Energy\nTo optimise (13), we use Monte Carlo methods for\nany expectations and use stochastic gradient descent\nfor optimisation. For optimisation, we require e\ufb03cient\nestimators of the gradients of all terms in equation\n(13) with respect to the parameters \u03b8g and \u03b8r of the\ngenerative and the recognition models, respectively.\nThe gradients with respect to the jth generative pa-\nrameter \u03b8g\nj can be computed using:\n\u2207\u03b8g\nj F(V) = \u2212Eq\nh\n\u2207\u03b8g\nj log p(V|h)\ni\n+ 1\n\u03ba\u03b8g\nj .\n(14)\nAn unbiased estimator of \u2207\u03b8g\nj F(V) is obtained by\napproximating equation (14) with a small number of\nsamples (or even a single sample) from the recognition\nmodel q.\nTo obtain gradients with respect to the recognition\nparameters \u03b8r, we use the rules for Gaussian back-\npropagation developed in section 3.\nTo address the\ncomplexity of the Hessian in the general rule (9), we\nuse the co-ordinate transformation for the Gaussian to\nwrite the gradient with respect to the factor matrix R\ninstead of the covariance C (recalling C = RR\u22a4) de-\nrived in equation (10), where derivatives are computed\nfor the function f(\u03be) = log p(v|h(\u03be)).\nThe gradients of F(v) in equation (13) with respect to\nthe variational mean \u00b5l(v) and the factors Rl(v) are:\n\u2207\u00b5lF(v) = \u2212Eq\nh\n\u2207\u03bel log p(v|h(\u03be))\ni\n+ \u00b5l,\n(15)\n\u2207Rl,i,jF(v) = \u22121\n2Eq\n\u0002\n\u03f5l,j\u2207\u03bel,i log p(v|h(\u03be))\n\u0003\n+ 1\n2\u2207Rl,i,j [Tr Cn,l \u2212log |Cn,l|] ,\n(16)\nwhere the gradients \u2207Rl,i,j [Tr Cn,l \u2212log |Cn,l|] are\ncomputed by backpropagation. Unbiased estimators\nof the gradients (15) and (16) are obtained jointly\nStochastic Backpropagation in DLGMs\nAlgorithm 1 Learning in DLGMs\nwhile hasNotConverged() do\nV \u2190getMiniBatch()\n\u03ben \u223cq(\u03ben|vn) (bottom-up pass) eq. (12)\nh \u2190h(\u03be) (top-down pass) eq. (3)\nupdateGradients() eqs (14) \u2013 (17)\n\u03b8g,r \u2190\u03b8g,r + \u2206\u03b8g,r\nend while\nby sampling from the recognition model \u03be \u223cq(\u03be|v)\n(bottom-up pass) and updating the values of the gener-\native model layers using equation (3) (top-down pass).\nFinally the gradients \u2207\u03b8r\nj F(v) obtained from equa-\ntions (15) and (16) are:\n\u2207\u03b8rF(v)=\u2207\u00b5F(v)\u22a4\u2202\u00b5\n\u2202\u03b8r +Tr\n\u0012\n\u2207RF(v) \u2202R\n\u2202\u03b8r\n\u0013\n.\n(17)\nThe gradients (14) \u2013 (17) are now used to descend\nthe free-energy surface with respect to both the gen-\nerative and recognition parameters in a single optimi-\nsation step. Figure 1(b) shows the \ufb02ow of computa-\ntion in DLGMs. Our algorithm proceeds by \ufb01rst per-\nforming a forward pass (black arrows), consisting of a\nbottom-up (recognition) phase and a top-down (gener-\nation) phase, which updates the hidden activations of\nthe recognition model and parameters of any Gaussian\ndistributions, and then a backward pass (red arrows)\nin which gradients are computed using the appropriate\nbackpropagation rule for deterministic and stochastic\nlayers. We take a descent step using:\n\u2206\u03b8g,r = \u2212\u0393g,r\u2207\u03b8g,rF(V),\n(18)\nwhere \u0393g,r is a diagonal pre-conditioning matrix com-\nputed using the RMSprop heuristic1.\nThe learning\nprocedure is summarised in algorithm 1.\n4.3. Gaussian Covariance Parameterisation\nThere are a number of approaches for parameterising\nthe covariance matrix of the recognition model q(\u03be).\nMaintaining a full covariance matrix C in equation\n(13) would entail an algorithmic complexity of O(K3)\nfor training and sampling per layer, where K is the\nnumber of latent variables per layer.\nThe simplest approach is to use a diagonal covariance\nmatrix C = diag(d), where d is a K-dimensional vec-\ntor.\nThis approach is appealing since it allows for\nlinear-time computation and sampling, but only allows\nfor axis-aligned posterior distributions.\nWe can improve upon the diagonal approximation by\nparameterising the covarinace as a rank-1 matrix with\n1Described by G. Hinton, \u2018RMSprop: Divide the gradi-\nent by a running average of its recent magnitude\u2019, in Neural\nnetworks for machine learning, Coursera lecture 6e, 2012.\na diagonal correction. Using a vectors u and d, with\nD = diag(d), we parameterise the precision C\u22121 as:\nC\u22121 = D + uu\u22a4.\n(19)\nThis representation allows for arbitrary rotations of\nthe Gaussian distribution along one principal direction\nwith relatively few additional parameters (Magdon-\nIsmail & Purnell, 2010). By application of the matrix\ninversion lemma (Woodbury identity), we obtain the\ncovariance matrix in terms of d and u as:\nC = D\u22121 \u2212\u03b7D\u22121uu\u22a4D\u22121,\n\u03b7 =\n1\nu\u22a4D\n\u22121u+1,\nlog |C| = log \u03b7 \u2212log |D|.\n(20)\nThis allows both the trace Tr(C) and log |C| needed in\nthe computation of the Gaussian KL, as well as their\ngradients, to be computed in O(K) time per layer.\nThe factorisation C = RR\u22a4, with R a matrix of the\nsame size as C and can be computed directly in terms\nof d and u. One solution for R is:\nR = D\u22121\n2 \u2212\n\u0014 1 \u2212\u221a\u03b7\nu\u22a4D\u22121u\n\u0015\nD\u22121uu\u22a4D\u22121\n2 .\n(21)\nThe product of R with an arbitrary vector can be com-\nputed in O(K) without computing R explicitly. This\nalso allows us to sample e\ufb03ciently from this Gaussian,\nsince any Gaussian random variable \u03be with mean \u00b5\nand covariance matrix C = RR\u22a4can be written as\n\u03be = \u00b5 + R\u03f5, where \u03f5 is a standard Gaussian variate.\nSince this covariance parametrisation has linear cost\nin the number of latent variables, we can also use it to\nparameterise the variational distribution of all layers\njointly, instead of the factorised assumption in (12).\n4.4. Algorithm Complexity\nThe computational complexity of producing a sample\nfrom the generative model is O(L \u00afK2), where \u00afK is the\naverage number of latent variables per layer and L is\nthe number of layers (counting both deterministic and\nstochastic layers). The computational complexity per\ntraining sample during training is also O(L \u00afK2) \u2013 the\nsame as that of matching auto-encoder.\n5. Results\nGenerative models have a number of applications in\nsimulation, prediction, data visualisation, missing data\nimputation and other forms of probabilistic reason-\ning. We describe the testing methodology we use and\npresent results on a number of these tasks.\n5.1. Analysing the Approximate Posterior\nWe use sampling to evaluate the true posterior\ndistribution for a number of MNIST digits using\nthe binarised data set from Larochelle & Murray\nStochastic Backpropagation in DLGMs\n(a) Diagonal covariance\n(b) Low-rank covariance\nRank1\nDiag\nWake\u2212Sleep\nFA\n84\n88\n92\n96\n100\n104\nTest neg. marginal likelihood\n(c) Performance\nFigure 2. (a, b) Analysis of the true vs. approximate posterior for MNIST. Within each image we show four views of the\nsame posterior, zooming in on the region centred on the MAP (red) estimate. (c) Comparison of test log likelihoods.\nTable 1. Comparison of negative log-probabilities on the\ntest set for the binarised MNIST data.\nModel\n\u2212ln p(v)\nFactor Analysis\n106.00\nNLGBN (Frey & Hinton, 1999)\n95.80\nWake-Sleep (Dayan, 2000)\n91.3\nDLGM diagonal covariance\n87.30\nDLGM rank-one covariance\n86.60\nResults below from Uria et al. (2014)\nMoBernoullis K=10\n168.95\nMoBernoullis K=500\n137.64\nRBM (500 h, 25 CD steps) approx.\n86.34\nDBN 2hl approx.\n84.55\nNADE 1hl (\ufb01xed order)\n88.86\nNADE 1hl (\ufb01xed order, RLU, minibatch)\n88.33\nEoNADE 1hl (2 orderings)\n90.69\nEoNADE 1hl (128 orderings)\n87.71\nEoNADE 2hl (2 orderings)\n87.96\nEoNADE 2hl (128 orderings)\n85.10\n(2011). We visualise the posterior distribution for a\nmodel with two Gaussian latent variables in \ufb01gure 2.\nThe true posterior distribution is shown by the grey\nregions and was computed by importance sampling\nwith a large number of particles aligned in a grid\nbetween -5 and 5.\nIn \ufb01gure 2(a) we see that these\nposterior distributions are elliptical or spherical in\nshape and thus, it is reasonable to assume that they\ncan be well approximated by a Gaussian.\nSamples\nfrom the prior (green) are spread widely over the\nspace and very few samples fall in the region of\nsigni\ufb01cant posterior mass, explaining the ine\ufb03ciency\nof estimation methods that rely on samples from the\nprior. Samples from the recognition model (blue) are\nconcentrated on the posterior mass, indicating that\nthe recognition model has learnt the correct posterior\nstatistics, which should lead to e\ufb03cient learning.\nIn \ufb01gure 2(a) we see that samples from the recognition\nmodel are aligned to the axis and do not capture\nthe posterior correlation. The correlation is captured\nusing the structured covariance model in \ufb01gure 2(b).\nNot all posteriors are Gaussian in shape, but the\nrecognition places mass in the best location possible to\nprovide a reasonable approximation. As a benchmark\nfor comparison, the performance in terms of test\nlog-likelihood is shown in \ufb01gure 2(c), using the same\narchitecture, for factor analysis (FA), the wake-sleep\nalgorithm, and our approach using both the diagonal\nand structured covariance approaches.\nFor this ex-\nperiment, the generative model consists of 100 latent\nvariables feeding into a deterministic layer of 300\nnodes, which then feeds to the observation likelihood.\nWe use the same structure for the recognition model.\n5.2. Simulation and Prediction\nWe evaluate the performance of a three layer latent\nGaussian model on the MNIST data set. The model\nconsists of two deterministic layers with 200 hidden\nunits and a stochastic layer of 200 latent variables.\nWe use mini-batches of 200 observations and trained\nthe model using stochastic backpropagation. Samples\nfrom this model are shown in \ufb01gure 3(a).\nWe also\ncompare the test log-likelihood to a large number of\nexisting approaches in table 1.\nWe used the bina-\nrised dataset as in Uria et al. (2014) and quote the\nlog-likelihoods in the lower part of the table from this\nwork. These results show that our approach is com-\npetitive with some of the best models currently avail-\nable. The generated digits also match the true data\nwell and visually appear as good as some of the best\nvisualisations from these competing approaches.\nWe also analysed the performance of our model on\nthree high-dimensional real image data sets.\nThe\nNORB object recognition data set consists of 24, 300\nimages that are of size 96 \u00d7 96 pixels. We use a model\nconsisting of 1 deterministic layer of 400 hidden units\nand one stochastic layer of 100 latent variables. Sam-\nples produced from this model are shown in \ufb01gure\n4(a). The CIFAR10 natural images data set consists\nof 50, 000 RGB images that are of size 32 \u00d7 32 pix-\nels, which we split into random 8 \u00d7 8 patches. We use\nthe same model as used for the MNIST experiment\nand show samples from the model in \ufb01gure 4(b). The\nFrey faces data set consists of almost 2, 000 images of\ndi\ufb00erent facial expressions of size 28 \u00d7 20 pixels.\n5.3. Data Visualisation\nLatent variable models are often used for visualisa-\ntion of high-dimensional data sets.\nWe project the\nMNIST data set to a 2-dimensional latent space and\nuse this 2D embedding as a visualisation of the data \u2013\nan embedding for MNIST is shown in \ufb01gure 3(b). The\nclasses separate into di\ufb00erent regions, suggesting that\nsuch embeddings can be useful in understanding the\nstructure of high-dimensional data sets.\nStochastic Backpropagation in DLGMs\n(a) Left: Training data. Middle: Sampled pixel probabilities. Right: Model samples\n(b) 2D embedding.\nFigure 3. Performance on the MNIST dataset. For the visualisation, each colour corresponds to one of the digit classes.\n(a) NORB\n(b) CIFAR\n(c) Frey\nFigure 4. Sampled generated from DLGMs for three data sets: (a) NORB, (b) CIFAR 10, (c) Frey faces. In all images,\nthe left image shows samples from the training data and the right side shows the generated samples.\n5.4. Missing Data Imputation and Denoising\nWe demonstrate the ability of the model to impute\nmissing data using the street view house numbers\n(SVHN) data set (Netzer et al., 2011), which consists\nof 73, 257 images of size 32 \u00d7 32 pixels, and the Frey\nfaces and MNIST data sets. The performance of the\nmodel is shown in \ufb01gure 5.\nWe test the imputation ability under two di\ufb00erent\nmissingness types (Little & Rubin, 1987): Missing-at-\nRandom (MAR), where we consider 60% and 80% of\nthe pixels to be missing randomly, and Not Missing-at-\nRandom (NMAR), where we consider a square region\nof the image to be missing. The model produces very\ngood completions in both test cases. There is uncer-\ntainty in the identity of the image and this is re\ufb02ected\nin the errors in these completions as the resampling\nprocedure is run (see transitions from digit 9 to 7, and\ndigit 8 to 6 in \ufb01gure 5 ). This further demonstrates\nthe ability of the model to capture the diversity of the\nunderlying data. We do not integrate over the missing\nvalues, but use a procedure that simulates a Markov\nchain that we show converges to the true marginal dis-\ntribution of missing given observed pixels. The impu-\ntation procedure is discussed in appendix F.\n6. Discussion\nDirected Graphical Models.\nDLGMs form a\nuni\ufb01ed family of models that includes factor analy-\nsis (Bartholomew & Knott, 1999), non-linear factor\nanalysis (Lappalainen & Honkela, 2000), and non-\nFigure 5. Imputation results: Row 1, SVHN. Row 2, Frey\nfaces.\nRows 3\u20135, MNIST. Col.\n1 shows the true data.\nCol. 2 shows pixel locations set as missing in grey. The\nremaining columns show imputations for 15 iterations.\nlinear Gaussian belief networks (Frey & Hinton, 1999).\nOther related models include sigmoid belief networks\n(Saul et al., 1996) and deep auto-regressive net-\nworks (Gregor et al., 2014), which use auto-regressive\nBernoulli distributions at each layer instead of Gaus-\nsian distributions. The Gaussian process latent vari-\nable model and deep Gaussian processes (Lawrence,\n2005; Damianou & Lawrence, 2013) form the non-\nparametric analogue of our model and employ Gaus-\nStochastic Backpropagation in DLGMs\nsian process priors over the non-linear functions be-\ntween each layer. The neural auto-regressive density\nestimator (NADE) (Larochelle & Murray, 2011; Uria\net al., 2014) uses function approximation to model con-\nditional distributions within a directed acyclic graph.\nNADE is amongst the most competitive generative\nmodels currently available, but has several limitations,\nsuch as the inability to allow for deep representations\nand di\ufb03culties in extending to locally-connected mod-\nels (e.g., through the use of convolutional layers), pre-\nventing it from scaling easily to high-dimensional data.\nAlternative latent Gaussian inference.\nFew of\nthe alternative approaches for inferring latent Gaus-\nsian distributions meet the desiderata for scalable in-\nference we seek. The Laplace approximation has been\nconcluded to be a poor approximation in general, in\naddition to being computationally expensive. INLA is\nrestricted to models with few hyperparameters (< 10),\nwhereas our interest is in 100s-1000s. EP cannot be\napplied to latent variable models due to the inability\nto match moments of the joint distribution of latent\nvariables and model parameters. Furthermore, no re-\nliable methods exist for moment-matching with means\nand covariances formed by non-linear transformations\n\u2013 linearisation and importance sampling are two, but\nare either inaccurate or very slow. Thus, the the varia-\ntional approach we present remains a general-purpose\nand competitive approach for inference.\nMonte Carlo variance reduction.\nControl vari-\nate methods are amongst the most general and ef-\nfective techniques for variance reduction when Monte\nCarlo methods are used (Wilson, 1984). One popu-\nlar approach is the REINFORCE algorithm (Williams,\n1992), since it is simple to implement and applicable\nto both discrete and continuous models, though con-\ntrol variate methods are becoming increasingly popu-\nlar for variational inference problems (Ho\ufb00man et al.,\n2013; Blei et al., 2012; Ranganath et al., 2014; Sali-\nmans & Knowles, 2014). Unfortunately, such estima-\ntors have the undesirable property that their variance\nscales linearly with the number of independent random\nvariables in the target function, while the variance of\nGBP is bounded by a constant: for K-dimensional la-\ntent variables the variance of REINFORCE scales as\nO(K), whereas GBP scales as O(1) (see appendix D).\nAn important family of alternative estimators is based\non quadrature and series expansion methods (Honkela\n& Valpola, 2004; Lappalainen & Honkela, 2000).\nThese methods have low-variance at the price of in-\ntroducing biases in the estimation. More recently a\ncombination of the series expansion and control vari-\nate approaches has been proposed by Blei et al. (2012).\nA very general alternative is the wake-sleep algorithm\n(Dayan et al., 1995). The wake-sleep algorithm can\nperform well, but it fails to optimise a single consistent\nobjective function and there is thus no guarantee that\noptimising it leads to a decrease in the free energy (11).\nRelation to denoising auto-encoders.\nDenois-\ning auto-encoders (DAE) (Vincent et al., 2010) intro-\nduce a random corruption to the encoder network and\nattempt to minimize the expected reconstruction er-\nror under this corruption noise with additional reg-\nularisation terms.\nIn our variational approach, the\nrecognition distribution q(\u03be|v) can be interpreted as a\nstochastic encoder in the DAE setting. There is then a\ndirect correspondence between the expression for the\nfree energy (11) and the reconstruction error and reg-\nularization terms used in denoising auto-encoders (c.f.\nequation (4) of Bengio et al. (2013)). Thus, we can see\ndenoising auto-encoders as a realisation of variational\ninference in latent variable models.\nThe key di\ufb00erence is that the form of encoding \u2018corrup-\ntion\u2019 and regularisation terms used in our model have\nbeen derived directly using the variational principle\nto provide a strict bound on the marginal likelihood\nof a known directed graphical model that allows for\neasy generation of samples. DAEs can also be used as\ngenerative models by simulating from a Markov chain\n(Bengio et al., 2013; Bengio & Thibodeau-Laufer,\n2013). But the behaviour of these Markov chains will\nbe very problem speci\ufb01c, and we lack consistent tools\nto evaluate their convergence.\n7. Conclusion\nWe\nhave\nintroduced\na\ngeneral-purpose\ninference\nmethod for models with continuous latent variables.\nOur approach introduces a recognition model, which\ncan be seen as a stochastic encoding of the data, to al-\nlow for e\ufb03cient and tractable inference. We derived a\nlower bound on the marginal likelihood for the genera-\ntive model and speci\ufb01ed the structure and regularisa-\ntion of the recognition model by exploiting recent ad-\nvances in deep learning. By developing modi\ufb01ed rules\nfor backpropagation through stochastic layers, we de-\nrived an e\ufb03cient inference algorithm that allows for\njoint optimisation of all parameters. We show on sev-\neral real-world data sets that the model generates real-\nistic samples, provides accurate imputations of missing\ndata and can be a useful tool for high-dimensional data\nvisualisation.\nAppendices can be found with the online version of the pa-\nper. http://arxiv.org/abs/1401.4082\nAcknowledgements. We are grateful for feedback from\nthe reviewers as well as Peter Dayan, Antti Honkela, Neil\nLawrence and Yoshua Bengio.\nStochastic Backpropagation in DLGMs\nReferences\nBartholomew, D. J. and Knott, M. Latent variable mod-\nels and factor analysis, volume 7 of Kendall\u2019s library of\nstatistics. Arnold, 2nd edition, 1999.\nBeal,\nM. J.\nVariational Algorithms for approximate\nBayesian inference.\nPhD thesis, University of Cam-\nbridge, 2003.\nBengio, Y. and Thibodeau-Laufer, \u00b4E.\nDeep generative\nstochastic networks trainable by backprop.\nTechnical\nreport, University of Montreal, 2013.\nBengio, Y., Yao, L., Alain, G., and Vincent, P.\nGen-\neralized denoising auto-encoders as generative models.\nIn Advances in Neural Information Processing Systems\n(NIPS), pp. 1\u20139, 2013.\nBlei, D. M., Jordan, M. I., and Paisley, J. W. Variational\nBayesian inference with stochastic search. In Proceed-\nings of the 29th International Conference on Machine\nLearning (ICML), pp. 1367\u20131374, 2012.\nBonnet, G.\nTransformations des signaux al\u00b4eatoires a\ntravers les syst`emes non lin\u00b4eaires sans m\u00b4emoire. Annales\ndes T\u00b4el\u00b4ecommunications, 19(9-10):203\u2013220, 1964.\nDamianou, A. C. and Lawrence, N. D. Deep Gaussian pro-\ncesses. In Proceedings of the International Conference on\nArti\ufb01cial Intelligence and Statistics (AISTATS), 2013.\nDayan, P., Hinton, G. E., Neal, R. M., and Zemel, R. S.\nThe Helmholtz machine. Neural computation, 7(5):889\u2013\n904, September 1995.\nDayan, P. Helmholtz machines and wake-sleep learning.\nHandbook of Brain Theory and Neural Network. MIT\nPress, Cambridge, MA, 44(0), 2000.\nFrey, B. J. Variational inference for continuous sigmoidal\nBayesian networks. In Proceedings of the International\nConference on Arti\ufb01cial Intelligence and Statistics (AIS-\nTATS), 1996.\nFrey, B. J. and Hinton, G. E. Variational learning in non-\nlinear Gaussian belief networks. Neural Computation, 11\n(1):193\u2013213, January 1999.\nGershman, S. J. and Goodman, N. D. Amortized inference\nin probabilistic reasoning. In Proceedings of the 36th An-\nnual Conference of the Cognitive Science Society, 2014.\nGraves, A. Practical variational inference for neural net-\nworks.\nIn Advances in Neural Information Processing\nSystems 24 (NIPS), pp. 2348\u20132356, 2011.\nGregor, K., Mnih, A., and Wierstra, D. Deep autoregres-\nsive networks. In Proceedings of the International Con-\nference on Machine Learning (ICML), October 2014.\nHo\ufb00man, M., Blei, D. M., Wang, C., and Paisley, J.\nStochastic variational inference.\nJournal of Machine\nLearning Research, 14:1303\u20131347, May 2013.\nHonkela, A. and Valpola, H.\nUnsupervised variational\nBayesian learning of nonlinear models. In Advances in\nNeural Information Processing Systems (NIPS), 2004.\nKingma, D. P. and Welling, M. Auto-encoding variational\nBayes. Proceedings of the International Conference on\nLearning Representations (ICLR), 2014.\nLappalainen, H. and Honkela, A. Bayesian non-linear inde-\npendent component analysis by multi-layer perceptrons.\nIn Advances in independent component analysis (ICA),\npp. 93\u2013121. Springer, 2000.\nLarochelle, H. and Murray, I. The neural autoregressive\ndistribution estimator.\nIn Proceedings of the Interna-\ntional Conference on Arti\ufb01cial Intelligence and Statistics\n(AISTATS), 2011.\nLawrence, N.\nProbabilistic non-linear principal compo-\nnent analysis with Gaussian process latent variable mod-\nels. The Journal of Machine Learning Research, 6:1783\u2013\n1816, 2005.\nLittle, R. J. and Rubin, D. B.\nStatistical analysis with\nmissing data, volume 539. Wiley New York, 1987.\nMagdon-Ismail, M. and Purnell, J. T. Approximating the\ncovariance matrix of GMMs with low-rank perturba-\ntions. In Proceedings of the 11th international conference\non Intelligent data engineering and automated learning\n(IDEAL), pp. 300\u2013307, 2010.\nNetzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B.,\nand Ng, A. Y. Reading digits in natural images with\nunsupervised feature learning.\nIn NIPS Workshop on\nDeep Learning and Unsupervised Feature Learning, 2011.\nOpper, M. and Archambeau, C.\nThe variational Gaus-\nsian approximation revisited.\nNeural computation, 21\n(3):786\u201392, March 2009.\nPrice, R. A useful theorem for nonlinear devices having\nGaussian inputs.\nIEEE Transactions on Information\nTheory, 4(2):69\u201372, 1958.\nRanganath, R., Gerrish, S., and Blei, D. M.\nBlack box\nvariational inference. In Proceedings of the International\nConference on Arti\ufb01cial Intelligence and Statistics (AIS-\nTATS), October 2014.\nSalimans, T. and Knowles, D. A. On using control vari-\nates with stochastic approximation for variational bayes\nand its connection to stochastic linear regression. ArXiv\npreprint. arXiv:1401.1022, October 2014.\nSaul, L. K., Jaakkola, T., and Jordan, M. I. Mean \ufb01eld\ntheory for sigmoid belief networks. Journal of Arti\ufb01cial\nIntelligence Research (JAIR), 4:61\u201376, 1996.\nUria, B., Murray, I., and Larochelle, H.\nA deep and\ntractable density estimator.\nIn Proceedings of the In-\nternational Conference on Machine Learning (ICML),\n2014.\nVincent, P., Larochelle, H., Lajoie, I., Bengio, Y., and\nManzagol, P. Stacked denoising autoencoders: Learn-\ning useful representations in a deep network with a local\ndenoising criterion. The Journal of Machine Learning\nResearch, 11:3371\u20133408, 2010.\nWilliams, R. J.\nSimple statistical gradient-following al-\ngorithms for connectionist reinforcement learning. Ma-\nchine Learning, 8:229 \u2013 256, 1992.\nWilson, J. R. Variance reduction techniques for digital sim-\nulation. American Journal of Mathematical and Man-\nagement Sciences, 4(3):277\u2013312, 1984.\nAppendices:\nStochastic Backpropagation and Approximate Inference\nin Deep Generative Models\nDanilo J. Rezende, Shakir Mohamed, Daan Wierstra\n{danilor, shakir, daanw}@google.com\nGoogle DeepMind, London\nA. Additional Model Details\nIn equation (6) we showed an alternative form of the\njoint log likelihood that explicitly separates the deter-\nministic and stochastic parts of the generative model\nand corroborates the view that the generative model\nworks by applying a complex non-linear transforma-\ntion to a spherical Gaussian distribution N(\u03be|0, I)\nsuch that the transformed distribution best matches\nthe empirical distribution.\nWe provide more details\non this view here for clarity.\nFrom the model description in equations (3) and (4),\nwe can interpret the variables hl as deterministic func-\ntions of the noise variables \u03bel. This can be formally\nintroduced as a coordinate transformation of the prob-\nability density in equation (5): we perform a change of\ncoordinates hl \u2192\u03bel. The density of the transformed\nvariables \u03bel can be expressed in terms of the density (5)\ntimes the determinant of the Jacobian of the transfor-\nmation p(\u03bel) = p(hl(\u03bel))| \u2202hl\n\u2202\u03bel |. Since the co-ordinate\ntransformation is linear we have | \u2202hl\n\u2202\u03bel | = |Gl| and the\ndistribution of \u03bel is obtained as follows:\np(\u03bel)= p(hl(\u03bel))|\u2202hl\n\u2202\u03bel\n|\np(\u03bel)=p(hL)|GL|\nL\u22121\nY\nl=1\n|Gl|pl(hl|hl+1)=\nL\nY\nl=1\n|Gl||Sl|\u22121\n2 N(\u03bel)\n=\nL\nY\nl=1\n|Gl||GlGT\nl |\u22121\n2 N(\u03bel|0, I) =\nL\nY\nl=1\nN(\u03bel|0, I). (22)\nCombining this equation with the distribution of the\nvisible layer we obtain equation (6).\nA.1. Examples\nBelow we provide simple, explicit examples of genera-\ntive and recognition models.\nIn the case of a two-layer model the activation h1(\u03be1,2)\nin equation (6) can be explicitly written as\nh1(\u03be1,2)\n= W1f(G2\u03be2) + G1\u03be1 + b1.\n(23)\nSimilarly, a simple recognition model consists of a sin-\ngle deterministic layer and a stochastic Gaussian layer\nwith the rank-one covariance structure and is con-\nstructed as:\nq(\u03bel|v) = N\n\u0000\u03bel|\u00b5; (diag(d) + uu\u22a4)\u22121\u0001\n(24)\n\u00b5 = W\u00b5z + b\u00b5\n(25)\nlog d = Wdz + bd;\nu = Wuz + bu\n(26)\nz = f(Wvv + bv)\n(27)\nwhere the function f is a recti\ufb01ed linearity (but other\nnon-linearities such as tanh can be used).\nB. Proofs for the Gaussian Gradient\nIdentities\nHere we review the derivations of Bonnet\u2019s and Price\u2019s\ntheorems that were presented in section 3.\nTheorem B.1 (Bonnet\u2019s theorem). Let f(\u03be) : Rd 7\u2192\nR be a integrable and twice di\ufb00erentiable function. The\ngradient of the expectation of f(\u03be) under a Gaussian\ndistribution N(\u03be|\u00b5, C) with respect to the mean \u00b5 can\nbe expressed as the expectation of the gradient of f(\u03be).\n\u2207\u00b5iEN (\u00b5,C) [f(\u03be)] = EN (\u00b5,C) [\u2207\u03beif(\u03be)] ,\nProof.\n\u2207\u00b5iEN (\u00b5,C) [f(\u03be)] =\nZ\n\u2207\u00b5iN(\u03be|\u00b5, C)f(\u03be)d\u03be\n= \u2212\nZ\n\u2207\u03beiN(\u03be|\u00b5, C)f(\u03be)d\u03be\n=\n\u0014Z\nN(\u03be|\u00b5, C)f(\u03be)d\u03be\u00aci\n\u0015\u03bei=+\u221e\n\u03bei=\u2212\u221e\n+\nZ\nN(\u03be|\u00b5, C)\u2207\u03beif(\u03be)d\u03be\n= EN (\u00b5,C) [\u2207\u03beif(\u03be)] ,\n(28)\nProceedings of the 31 st International Conference on\nMachine Learning, Beijing, China, 2014. JMLR: W&CP\nvolume 32. Copyright 2014 by the author(s).\nStochastic Backpropagation in DLGMs\nwhere we have used the identity\n\u2207\u00b5iN(\u03be|\u00b5, C) = \u2212\u2207\u03beiN(\u03be|\u00b5, C)\nin moving from step 1 to 2. From step 2 to 3 we have\nused the product rule for integrals with the \ufb01rst term\nevaluating to zero.\nTheorem B.2 (Price\u2019s theorem). Under the same\nconditions as before. The gradient of the expectation\nof f(\u03be) under a Gaussian distribution N(\u03be|0, C) with\nrespect to the covariance C can be expressed in terms\nof the expectation of the Hessian of f(\u03be) as\n\u2207Ci,jEN (0,C) [f(\u03be)] = 1\n2EN (0,C)\n\u0002\n\u2207\u03bei,\u03bejf(\u03be)\n\u0003\nProof.\n\u2207Ci,jEN (0,C) [f(\u03be)] =\nZ\n\u2207Ci,jN(\u03be|0, C)f(\u03be)d\u03be\n= 1\n2\nZ\n\u2207\u03bei,\u03bejN(\u03be|0, C)f(\u03be)d\u03be\n= 1\n2\nZ\nN(\u03be|0, C)\u2207\u03bei,\u03bejf(\u03be)d\u03be\n= 1\n2EN (0,C)\n\u0002\n\u2207\u03bei,\u03bejf(\u03be)\n\u0003\n. (29)\nIn moving from steps 1 to 2, we have used the identity\n\u2207Ci,jN(\u03be|\u00b5, C) = 1\n2\u2207\u03bei,\u03bejN(\u03be|\u00b5, C),\nwhich can be veri\ufb01ed by taking the derivatives on both\nsides and comparing the resulting expressions. From\nstep 2 to 3 we have used the product rule for integrals\ntwice.\nC. Deriving Stochastic\nBack-propagation Rules\nIn section 3 we described two ways in which to derive\nstochastic back-propagation rules.\nWe show speci\ufb01c\nexamples and provide some more discussion in this sec-\ntion.\nC.1. Using the Product Rule for Integrals\nWe can derive rules for stochastic back-propagation for\nmany distributions by \ufb01nding a appropriate non-linear\nfunction B(x; \u03b8) that allows us to express the gradient\nwith respect to the parameters of the distribution as a\ngradient with respect to the random variable directly.\nThe approach we described in the main text was:\n\u2207\u03b8Ep[f(x)]=\nZ\n\u2207\u03b8p(x|\u03b8)f(x)dx=\nZ\n\u2207xp(x|\u03b8)B(x)f(x)dx\n= [B(x)f(x)p(x|\u03b8)]supp(x) \u2212\nZ\np(x|\u03b8)\u2207x[B(x)f(x)]\n= \u2212Ep(x|\u03b8)[\u2207x[B(x)f(x)]]\n(30)\nwhere we have introduced the non-linear function\nB(x; \u03b8) to allow for the transformation of the gradi-\nents and have applied the product rule for integrals\n(rule for integration by parts) to rewrite the integral\nin two parts in the second line, and the supp(x) indi-\ncates that the term is evaluated at the boundaries of\nthe support. To use this approach, we require that the\ndensity we are analysing be zero at the boundaries of\nthe support to ensure that the \ufb01rst term in the second\nline is zero.\nAs an alternative, we can also write this di\ufb00erently\nand \ufb01nd an non-linear function of the form:\n\u2207\u03b8Ep[f(x)]== \u2212Ep(x|\u03b8)[B(x)\u2207xf(x)].\n(31)\nConsider general exponential family distributions of\nthe form:\np(x|\u03b8) = h(x) exp(\u03b7(\u03b8)\u22a4\u03c6(x) \u2212A(\u03b8))\n(32)\nwhere h(x) is the base measure, \u03b8 is the set of mean\nparameters of the distribution, \u03b7 is the set of natural\nparameters, and A(\u03b8) is the log-partition function. We\ncan express the non-linear function in (30) using these\nquantities as:\nB(x) =\n[\u2207\u03b8\u03b7(\u03b8)\u03c6(x) \u2212\u2207\u03b8A(\u03b8)]\n[\u2207x log[h(x)] + \u03b7(\u03b8)T \u2207x\u03c6(x)].\n(33)\nThis can be derived for a number of distributions such\nas the Gaussian, inverse Gamma, Log-Normal, Wald\n(inverse Gaussian) and other distributions. We show\nsome of these below:\nFamily\n\u03b8\nB(x)\nGaussian\n\u0012 \u00b5\n\u03c32\n\u0013\n\u0012\n\u22121\n(x\u2212\u00b5\u2212\u03c3)(x\u2212\u00b5+\u03c3)\n2\u03c32(x\u2212\u00b5)\n\u0013\nInv. Gamma\n\u0012\n\u03b1\n\u03b2\n\u0013\n \nx2(\u2212ln x\u2212\u03a8(\u03b1)+ln \u03b2)\n\u2212x(\u03b1+1)+\u03b2\n(\nx2\n\u2212x(\u03b1+1)+\u03b2 )(\u22121\nx + \u03b1\n\u03b2 )\n!\nLog-Normal\n\u0012 \u00b5\n\u03c32\n\u0013\n\u0012\n\u22121\n(ln x\u2212\u00b5\u2212\u03c3)(ln x\u2212\u00b5+\u03c3)\n2\u03c32(ln x\u2212\u00b5)\n\u0013\nThe B(x; \u03b8) corresponding to the second formulation\ncan also be derived and may be useful in certain situa-\ntions, requiring the solution of a \ufb01rst order di\ufb00erential\nequation.\nThis approach of searching for non-linear\ntransformations leads us to the second approach for\nderiving stochastic back-propagation rules.\nStochastic Backpropagation in DLGMs\nC.2. Using Alternative Coordinate\nTransformations\nThere are many distributions outside the exponential\nfamily that we would like to consider using. A sim-\npler approach is to search for a co-ordinate transfor-\nmation that allows us to separate the deterministic\nand stochastic parts of the distribution. We described\nthe case of the Gaussian in section 3. Other distri-\nbutions also have this property. As an example, con-\nsider the Levy distribution (which is a special case of\nthe inverse Gamma considered above).\nDue to the\nself-similarity property of this distribution, if we draw\nX from a Levy distribution with known parameters\nX \u223cLevy(\u00b5, \u03bb), we can obtain any other Levy distri-\nbution by rescaling and shifting this base distribution:\nkX + b \u223cLevy(k\u00b5 + b, kc).\nMany other distributions hold this property, allow-\ning stochastic back-propagation rules to be determined\nfor distributions such as the Student\u2019s t-distribution,\nLogistic distribution, the class of stable distributions\nand the class of generalised extreme value distributions\n(GEV). Examples of co-ordinate transformations T(\u00b7)\nand the resulsting distributions are shown below for\nvariates X drawn from the standard distribution listed\nin the \ufb01rst column.\nStd Distr.\nT(\u00b7)\nGen. Distr.\nGEV(\u00b5, \u03c3, 0) mX+b\nGEV(m\u00b5+b, m\u03c3, 0)\nExp(1)\n\u00b5+\u03b2ln(1+exp(\u2212X))\nLogistic(\u00b5, \u03b2)\nExp(1)\n\u03bbX\n1\nk\nWeibull(\u03bb, k)\nD. Variance Reduction using Control\nVariates\nAn alternative approach for stochastic gradient com-\nputation is commonly based on the method of control\nvariates. We analyse the variance properties of var-\nious estimators in a simple example using univariate\nfunction.\nWe then show the correspondence of the\nwidely-known REINFORCE algorithm to the general\ncontrol variate framework.\nD.1. Variance discussion for REINFORCE\nThe REINFORCE estimator is based on\n\u2207\u03b8Ep[f(\u03be)] = Ep[(f(\u03be) \u2212b)\u2207\u03b8 log p(\u03be|\u03b8)],\n(34)\nwhere b is a baseline typically chosen to reduce the\nvariance of the estimator.\nThe variance of (34) scales poorly with the num-\nber of random variables (Dayan et al., 1995).\nTo\nsee this limitation, consider functions of the form\nf(\u03be)\n=\nPK\ni=1 f(\u03bei),\nwhere each individual term\nand\nits\ngradient\nhas\na\nbounded\nvariance,\ni.e.,\n\u03bal \u2264Var[f(\u03bei)] \u2264\u03bau and \u03bal \u2264Var[\u2207\u03beif(\u03bei)] \u2264\u03bau for\nsome 0 \u2264\u03bal \u2264\u03bau and assume independent or weakly\ncorrelated random variables. Given these assumptions\nthe variance of GBP (7) scales as Var[\u2207\u03beif(\u03be)] \u223cO(1),\nwhile the variance for REINFORCE (34) scales as\nVar\nh\n(\u03bei\u2212\u00b5i)\n\u03c32\ni\n(f(\u03be) \u2212E[f(\u03be)])\ni\n\u223cO(K).\nFor the variance of GBP above, all terms in f(\u03be) that\ndo not depend on \u03bei have zero gradient, whereas for\nREINFORCE the variance involves a summation over\nall K terms. Even if most of these terms have zero\nexpectation, they still contribute to the variance of\nthe estimator.\nThus, the REINFORCE estimator\nhas the undesirable property that its variance scales\nlinearly with the number of independent random\nvariables in the target function, while the variance of\nGBP is bounded by a constant.\nThe assumption of weakly correlated terms is rele-\nvant for variational learning in larger generative mod-\nels where independence assumptions and structure in\nthe variational distribution result in free energies that\nare summations over weakly correlated or independent\nterms.\nD.2. Univariate variance analysis\nIn analysing the variance properties of many estima-\ntors, we discuss the general scaling of likelihood ratio\napproaches in appendix D. As an example to further\nemphasise the high-variance nature of these alternative\napproaches, we present a short analysis in the univari-\nate case.\nConsider a random variable p(\u03be) = N(\u03be|\u00b5, \u03c32) and a\nsimple quadratic function of the form\nf(\u03be) = c\u03be2\n2 .\n(35)\nFor this function we immediately obtain the following\nvariances\nV ar[\u2207\u03bef(\u03be)] = c2\u03c32\n(36)\nV ar[\u2207\u03be2f(\u03be)] = 0\n(37)\nV ar[(\u03be \u2212\u00b5)\n\u03c3\n\u2207\u03bef(\u03be)] = 2c2\u03c32 + \u00b52c2\n(38)\nV ar[(\u03be \u2212\u00b5)\n\u03c32\n(f(\u03be) \u2212E[f(\u03be)])] = 2c2\u00b52 + 5\n2c2\u03c32 (39)\nEquations (36), (37) and (38) correspond to the vari-\nance of the estimators based on (7), (8), (10) respec-\ntively whereas equation (39) corresponds to the vari-\nance of the REINFORCE algorithm for the gradient\nwith respect to \u00b5.\nStochastic Backpropagation in DLGMs\nFrom these relations we see that, for any parameter\ncon\ufb01guration, the variance of the REINFORCE esti-\nmator is strictly larger than the variance of the estima-\ntor based on (7). Additionally, the ratio between the\nvariances of the former and later estimators is lower-\nbounded by 5/2. We can also see that the variance\nof the estimator based on equation (8) is zero for this\nspeci\ufb01c function whereas the variance of the estimator\nbased on equation (10) is not.\nE. Estimating the Marginal Likelihood\nWe compute the marginal likelihood by importance\nsampling by generating S samples from the recognition\nmodel and using the following estimator:\np(v) \u22481\nS\nS\nX\ns=1\np(v|h(\u03be(s)))p(\u03be(s))\nq(\u03bes|v)\n;\n\u03be(s) \u223cq(\u03be|v)\n(40)\nF. Missing Data Imputation\nImage completion can be approximatively achieved by\na simple iterative procedure which consists of (i) ini-\ntializing the non-observed pixels with random values;\n(ii) sampling from the recognition distribution given\nthe resulting image; (iii) reconstruct the image given\nthe sample from the recognition model; (iv) iterate the\nprocedure.\nWe denote the observed and missing entries in an ob-\nservation as vo, vm, respectively. The observed vo is\n\ufb01xed throughout, therefore all the computations in this\nsection will be conditioned on vo. The imputation pro-\ncedure can be written formally as a Markov chain on\nthe space of missing entries vm with transition kernel\nT q(v\u2032\nm|vm, vo) given by\nT q(v\u2032\nm|vm, vo) =\nZZ\np(v\u2032\nm, v\u2032\no|\u03be)q(\u03be|v)dv\u2032\nod\u03be,\n(41)\nwhere v = (vm, vo).\nProvided that the recognition model q(\u03be|v) constitutes\na good approximation of the true posterior p(\u03be|v), (41)\ncan be seen as an approximation of the kernel\nT(v\u2032\nm|vm, vo) =\nZZ\np(v\u2032\nm, v\u2032\no|\u03be)p(\u03be|v)dv\u2032\nod\u03be.\n(42)\nThe kernel (42) has two important properties: (i) it\nhas as its eigen-distribution the marginal p(vm|vo); (ii)\nT(v\u2032\nm|vm, vo) > 0 \u2200vo, vm, v\u2032\nm. The property (i) can\nbe derived by applying the kernel (42) to the marginal\np(vm|vo) and noting that it is a \ufb01xed point. Property\n(ii) is an immediate consequence of the smoothness of\nthe model.\nWe apply the fundamental theorem for Markov chains\n(Neal, 1993, pp. 38) and conclude that given the above\nproperties, a Markov chain generated by (42) is guar-\nanteed to generate samples from the correct marginal\np(vm|vo).\nIn practice, the stationary distribution of the com-\npleted\npixels\nwill\nnot\nbe\nexactly\nthe\nmarginal\np(vm|vo), since we use the approximated kernel (41).\nEven in this setting we can provide a bound on the L1\nnorm of the di\ufb00erence between the resulting stationary\nmarginal and the target marginal p(vm|vo)\nProposition F.1 (L1 bound on marginal error ). If\nthe recognition model q(\u03be|v) is such that for all \u03be\n\u2203\u03b5 > 0 s.t.\nZ \f\f\f\f\nq(\u03be|v)p(v)\np(\u03be)\n\u2212p(v|\u03be)\n\f\f\f\f dv \u2264\u03b5\n(43)\nthen the marginal p(vm|vo) is a weak \ufb01xed point of the\nkernel (41) in the following sense:\nZ \f\f\f\f\f\nZ \u0000T q(v\u2032\nm|vm, vo)\u2212\nT(v\u2032\nm|vm, vo)\n\u0001\np(vm|vo)dvm\n\f\f\f\f\fdv\u2032\nm < \u03b5.\n(44)\nProof.\nZ \f\f\f\f\nZ\n[T q(v\u2032\nm|vm, vo)\u2212T(v\u2032\nm|vm, vo)] p(vm|vo)dvm\n\f\f\f\f dv\u2032\nm\n=\nZ\n|\nZZ\np(v\u2032\nm, v\u2032\no|\u03be)p(vm, vo)[q(\u03be|vm, vo)\n\u2212p(\u03be|vm, vo)]dvmd\u03be|dv\u2032\nm\n=\nZ \f\f\f\f\nZ\np(v\u2032|\u03be)p(v)[q(\u03be|v) \u2212p(\u03be|v)]p(v)\np(\u03be)\np(\u03be)\np(v)dvd\u03be\n\f\f\f\f dv\u2032\n=\nZ \f\f\f\f\nZ\np(v\u2032|\u03be)p(\u03be)[q(\u03be|v)p(v)\np(\u03be) \u2212p(v|\u03be)]dvd\u03be\n\f\f\f\f dv\u2032\n\u2264\nZ Z\np(v\u2032|\u03be)p(\u03be)\nZ \f\f\f\fq(\u03be|v)p(v)\np(\u03be) \u2212p(v|\u03be)\n\f\f\f\f dvd\u03bedv\u2032\n\u2264\u03b5,\nwhere we apply the condition (43) to obtain the last\nstatement.\nThat is, if the recognition model is su\ufb03ciently close\nto the true posterior to guarantee that (43) holds for\nsome acceptable error \u03b5 than (44) guarantees that the\n\ufb01xed-point of the Markov chain induced by the kernel\n(41) is no further than \u03b5 from the true marginal with\nrespect to the L1 norm.\nStochastic Backpropagation in DLGMs\nG. Variational Bayes for Deep Directed\nModels\nIn the main test we focussed on the variational prob-\nlem of specifying an posterior on the latent variables\nonly. It is natural to consider the variational Bayes\nproblem in which we specify an approximate posterior\nfor both the latent variables and model parameters.\nFollowing the same construction and considering an\nGaussian approximate distribution on the model pa-\nrameters \u03b8g, the free energy becomes:\nF(V) = \u2212\nX\nn\nreconstruction error\nz\n}|\n{\nEq[log p(vn|h(\u03ben))]\n+ 1\n2\nX\nn,l\n\u0002\n\u2225\u00b5n,l\u22252 + Tr Cn,l \u2212log |Cn,l| \u22121\n\u0003\n|\n{z\n}\nlatent regularization term\n+ 1\n2\nX\nj\n\"\nm2\nj\n\u03ba + \u03c4j\n\u03ba + log \u03ba \u2212log \u03c4j \u22121\n#\n|\n{z\n}\nparameter regularization term\n,\n(45)\nwhich now includes an additional term for the cost of\nusing parameters and their regularisation. We must\nnow compute the additional set of gradients with re-\nspect to the parameter\u2019s mean mj and variance \u03c4j are:\n\u2207mjF(v) = \u2212Eq\nh\n\u2207\u03b8g\nj log p(v|h(\u03be))\ni\n+ mj\n(46)\n\u2207\u03c4jF(v) = \u22121\n2Eq\n\u0014\u03b8j \u2212mj\n\u03c4j\n\u2207\u03b8g\nj log p(v|h(\u03be))\n\u0015\n+ 1\n2\u03ba \u22121\n2\u03c4j\n(47)\nH. Additional Simulation Details\nWe use training data of various types including binary\nand real-valued data sets. In all cases, we train using\nmini-batches, which requires the introduction of scal-\ning terms in the free energy objective function (13) in\norder to maintain the correct scale between the prior\nover the parameters and the remaining terms (Ahn\net al., 2012; Welling & Teh, 2011). We make use of\nthe objective:\nF(V) = \u2212\u03bb\nX\nn\nEq [log p(vn|h(\u03ben))] +\n1\n2\u03ba\u2225\u03b8g\u22252\n+ \u03bb\n2\nX\nn,l\n\u0002\n\u2225\u00b5n,l\u22252 + Tr(Cn,l) \u2212log |Cn,l| \u22121\n\u0003\n,\n(48)\nwhere n is an index over observations in the mini-batch\nand \u03bb is equal to the ratio of the data-set and the mini-\nbatch size. At each iteration, a random mini-batch of\nsize 200 observations is chosen.\nAll parameters of the model were initialized using sam-\nples from a Gaussian distribution with mean zero and\nvariance 1 \u00d7 106; the prior variance of the parameters\nwas \u03ba = 1 \u00d7 106. We compute the marginal likelihood\non the test data by importance sampling using samples\nfrom the recognition model; we describe our estimator\nin appendix E.\nReferences\nAhn, S., Balan, A. K., and Welling, M. Bayesian poste-\nrior sampling via stochastic gradient Fisher scoring.\nIn ICML, 2012.\nDayan, P., Hinton, G. E., Neal, R. M., and Zemel,\nR. S. The Helmholtz machine. Neural computation,\n7(5):889\u2013904, September 1995.\nNeal, R. M. Probabilistic inference using Markov chain\nMonte Carlo methods. Technical Report CRG-TR-\n93-1, University of Toronto, 1993.\nWelling, M. and Teh, Y. W.\nBayesian learning via\nstochastic gradient Langevin dynamics. In Proceed-\nings of the 28th International Conference on Ma-\nchine Learning (ICML-11), pp. 681\u2013688, 2011.\n",
        "sentence": " Another method based on inference networks is variational autoencoders [18, 19], which exploit a clever reparameterization of the probabilistic model in order to improve the signal in the stochastic gradients.",
        "context": "also follow a stochastic variational approach, but shall\ndevelop an alternative to these existing inference algo-\nrithms that overcomes many of their limitations and\nthat is both scalable and e\ufb03cient.\n3. Stochastic Backpropagation\nequation (4) of Bengio et al. (2013)). Thus, we can see\ndenoising auto-encoders as a realisation of variational\ninference in latent variable models.\nThe key di\ufb00erence is that the form of encoding \u2018corrup-\nBayesian networks. In Proceedings of the International\nConference on Arti\ufb01cial Intelligence and Statistics (AIS-\nTATS), 1996.\nFrey, B. J. and Hinton, G. E. Variational learning in non-\nlinear Gaussian belief networks. Neural Computation, 11"
    },
    {
        "title": "A model of saliency-based visual attention for rapid scene analysis",
        "author": [
            "L. Itti",
            "C. Koch",
            "E. Niebur"
        ],
        "venue": "IEEE Transactions of Pattern Analysis and Machine Intelligence, 20(11):1254\u201359, November",
        "citeRegEx": "20",
        "shortCiteRegEx": null,
        "year": 1998,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " [20, 21]), there are no labels for where to attend.",
        "context": null
    },
    {
        "title": "Learning to predict where humans look",
        "author": [
            "T. Judd",
            "K. Ehinger",
            "F. Durand",
            "A. Torralba"
        ],
        "venue": "International Conference on Computer Vision,",
        "citeRegEx": "21",
        "shortCiteRegEx": null,
        "year": 2009,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " [20, 21]), there are no labels for where to attend.",
        "context": null
    },
    {
        "title": "Learning stochastic feedforward neural networks",
        "author": [
            "Y. Tang",
            "R. Salakhutdinov"
        ],
        "venue": "Neural Information Processing Systems,",
        "citeRegEx": "22",
        "shortCiteRegEx": null,
        "year": 2013,
        "abstract": "",
        "full_text": "",
        "sentence": " When q is chosen to be the prior, this approach is equivalent to the method of [22] for learning generative feed-forward networks.",
        "context": null
    },
    {
        "title": "Importance weighted autoencoders",
        "author": [
            "Y. Burda",
            "R. Grosse",
            "R. Salakhutdinov"
        ],
        "venue": "arXiv:1509.00519,",
        "citeRegEx": "23",
        "shortCiteRegEx": null,
        "year": 2015,
        "abstract": "The variational autoencoder (VAE; Kingma, Welling (2014)) is a recently\nproposed generative model pairing a top-down generative network with a\nbottom-up recognition network which approximates posterior inference. It\ntypically makes strong assumptions about posterior inference, for instance that\nthe posterior distribution is approximately factorial, and that its parameters\ncan be approximated with nonlinear regression from the observations. As we show\nempirically, the VAE objective can lead to overly simplified representations\nwhich fail to use the network's entire modeling capacity. We present the\nimportance weighted autoencoder (IWAE), a generative model with the same\narchitecture as the VAE, but which uses a strictly tighter log-likelihood lower\nbound derived from importance weighting. In the IWAE, the recognition network\nuses multiple samples to approximate the posterior, giving it increased\nflexibility to model complex posteriors which do not fit the VAE modeling\nassumptions. We show empirically that IWAEs learn richer latent space\nrepresentations than VAEs, leading to improved test log-likelihood on density\nestimation benchmarks.",
        "full_text": "Under review as a conference paper at ICLR 2016\nIMPORTANCE WEIGHTED AUTOENCODERS\nYuri Burda, Roger Grosse & Ruslan Salakhutdinov\nDepartment of Computer Science\nUniversity of Toronto\nToronto, ON, Canada\n{yburda,rgrosse,rsalakhu}@cs.toronto.edu\nABSTRACT\nThe variational autoencoder (VAE; Kingma & Welling (2014)) is a recently pro-\nposed generative model pairing a top-down generative network with a bottom-up\nrecognition network which approximates posterior inference. It typically makes\nstrong assumptions about posterior inference, for instance that the posterior dis-\ntribution is approximately factorial, and that its parameters can be approximated\nwith nonlinear regression from the observations. As we show empirically, the\nVAE objective can lead to overly simpli\ufb01ed representations which fail to use the\nnetwork\u2019s entire modeling capacity. We present the importance weighted autoen-\ncoder (IWAE), a generative model with the same architecture as the VAE, but\nwhich uses a strictly tighter log-likelihood lower bound derived from importance\nweighting. In the IWAE, the recognition network uses multiple samples to ap-\nproximate the posterior, giving it increased \ufb02exibility to model complex posteri-\nors which do not \ufb01t the VAE modeling assumptions. We show empirically that\nIWAEs learn richer latent space representations than VAEs, leading to improved\ntest log-likelihood on density estimation benchmarks.\n1\nINTRODUCITON\nIn recent years, there has been a renewed focus on learning deep generative models (Hinton et al.,\n2006; Salakhutdinov & E., 2009; Gregor et al., 2014; Kingma & Welling, 2014; Rezende et al.,\n2014). A common dif\ufb01culty faced by most approaches is the need to perform posterior inference\nduring training: the log-likelihood gradients for most latent variable models are de\ufb01ned in terms\nof posterior statistics (e.g. Salakhutdinov & E. (2009); Neal (1992); Gregor et al. (2014)). One\napproach for dealing with this problem is to train a recognition network alongside the generative\nmodel (Dayan et al., 1995). The recognition network aims to predict the posterior distribution over\nlatent variables given the observations, and can often generate a rough approximation much more\nquickly than generic inference algorithms such as MCMC.\nThe variational autoencoder (VAE; Kingma & Welling (2014); Rezende et al. (2014)) is a recently\nproposed generative model which pairs a top-down generative network with a bottom-up recognition\nnetwork. Both networks are jointly trained to maximize a variational lower bound on the data log-\nlikelihood. VAEs have recently been successful at separating style and content (Kingma et al., 2014;\nKulkarni et al., 2015) and at learning to \u201cdraw\u201d images in a realistic manner (Gregor et al., 2015).\nVAEs make strong assumptions about the posterior distribution. Typically VAE models assume that\nthe posterior is approximately factorial, and that its parameters can be predicted from the observables\nthrough a nonlinear regression. Because they are trained to maximize a variational lower bound\non the log-likelihood, they are encouraged to learn representations where these assumptions are\nsatis\ufb01ed, i.e. where the posterior is approximately factorial and predictable with a neural network.\nWhile this effect is bene\ufb01cial, it comes at a cost: constraining the form of the posterior limits the\nexpressive power of the model. This is especially true of the VAE objective, which harshly penalizes\napproximate posterior samples which are unlikely to explain the data, even if the recognition network\nputs much of its probability mass on good explanations.\nIn this paper, we introduce the importance weighted autoencoder (IWAE), a generative model which\nshares the VAE architecture, but which is trained with a tighter log-likelihood lower bound de-\n1\narXiv:1509.00519v4  [cs.LG]  7 Nov 2016\nUnder review as a conference paper at ICLR 2016\nrived from importance weighting. The recognition network generates multiple approximate pos-\nterior samples, and their weights are averaged. As the number of samples is increased, the lower\nbound approaches the true log-likelihood. The use of multiple samples gives the IWAE additional\n\ufb02exibility to learn generative models whose posterior distributions do not \ufb01t the VAE modeling as-\nsumptions. This approach is related to reweighted wake sleep (Bornschein & Bengio, 2015), but\nthe IWAE is trained using a single uni\ufb01ed objective. Compared with the VAE, our IWAE is able to\nlearn richer representations with more latent dimensions, which translates into signi\ufb01cantly higher\nlog-likelihoods on density estimation benchmarks.\n2\nBACKGROUND\nIn this section, we review the variational autoencoder (VAE) model of Kingma & Welling (2014). In\nparticular, we describe a generalization of the architecture to multiple stochastic hidden layers. We\nnote, however, that Kingma & Welling (2014) used a single stochastic hidden layer, and there are\nother sensible generalizations to multiple layers, such as the one presented by Rezende et al. (2014).\nThe VAE de\ufb01nes a generative process in terms of ancestral sampling through a cascade of hidden\nlayers:\np(x|\u03b8) =\nX\nh1,...,hL\np(hL|\u03b8)p(hL\u22121|hL, \u03b8) \u00b7 \u00b7 \u00b7 p(x|h1, \u03b8).\n(1)\nHere, \u03b8 is a vector of parameters of the variational autoencoder, and h = {h1, . . . , hL} denotes the\nstochastic hidden units, or latent variables. The dependence on \u03b8 is often suppressed for clarity. For\nconvenience, we de\ufb01ne h0 = x. Each of the terms p(h\u2113|h\u2113+1) may denote a complicated nonlinear\nrelationship, for instance one computed by a multilayer neural network. However, it is assumed\nthat sampling and probability evaluation are tractable for each p(h\u2113|h\u2113+1). Note that L denotes\nthe number of stochastic hidden layers; the deterministic layers are not shown explicitly here. We\nassume the recognition model q(h|x) is de\ufb01ned in terms of an analogous factorization:\nq(h|x) = q(h1|x)q(h2|h1) \u00b7 \u00b7 \u00b7 q(hL|hL\u22121),\n(2)\nwhere sampling and probability evaluation are tractable for each of the terms in the product.\nIn this work, we assume the same families of conditional probability distributions as Kingma &\nWelling (2014). In particular, the prior p(hL) is \ufb01xed to be a zero-mean, unit-variance Gaussian.\nIn general, each of the conditional distributions p(h\u2113| h\u2113+1) and q(h\u2113|h\u2113\u22121) is a Gaussian with\ndiagonal covariance, where the mean and covariance parameters are computed by a deterministic\nfeed-forward neural network. For real-valued observations, p(x|h1) is also de\ufb01ned to be such a\nGaussian; for binary observations, it is de\ufb01ned to be a Bernoulli distribution whose mean parameters\nare computed by a neural network.\nThe VAE is trained to maximize a variational lower bound on the log-likelihood, as derived from\nJensen\u2019s Inequality:\nlog p(x) = log Eq(h|x)\n\u0014p(x, h)\nq(h|x)\n\u0015\n\u2265Eq(h|x)\n\u0014\nlog p(x, h)\nq(h|x)\n\u0015\n= L(x).\n(3)\nSince L(x) = log p(x) \u2212DKL(q(h|x)||p(h|x)), the training procedure is forced to trade off the\ndata log-likelihood log p(x) and the KL divergence from the true posterior. This is bene\ufb01cial, in that\nit encourages the model to learn a representation where posterior inference is easy to approximate.\nIf one computes the log-likelihood gradient for the recognition network directly from Eqn. 3, the re-\nsult is a REINFORCE-like update rule which trains slowly because it does not use the log-likelihood\ngradients with respect to latent variables (Dayan et al., 1995; Mnih & Gregor, 2014). Instead,\nKingma & Welling (2014) proposed a reparameterization of the recognition distribution in terms\nof auxiliary variables with \ufb01xed distributions, such that the samples from the recognition model are\na deterministic function of the inputs and auxiliary variables. While they presented the reparameter-\nization trick for a variety of distributions, for convenience we discuss the special case of Gaussians,\nsince that is all we require in this work. (The general reparameterization trick can be used with our\nIWAE as well.)\nIn this paper, the recognition distribution q(h\u2113|h\u2113\u22121, \u03b8) always takes the form of a Gaussian\nN(h\u2113|\u00b5(h\u2113\u22121, \u03b8), \u03a3(h\u2113\u22121, \u03b8)), whose mean and covariance are computed from the the states of\n2\nUnder review as a conference paper at ICLR 2016\nthe hidden units at the previous layer and the model parameters. This can be alternatively expressed\nby \ufb01rst sampling an auxiliary variable \u03f5\u2113\u223cN(0, I), and then applying the deterministic mapping\nh\u2113(\u03f5\u2113, h\u2113\u22121, \u03b8) = \u03a3(h\u2113\u22121, \u03b8)1/2\u03f5\u2113+ \u00b5(h\u2113\u22121, \u03b8).\n(4)\nThe joint recognition distribution q(h|x, \u03b8) over all latent variables can be expressed in terms of\na deterministic mapping h(\u03f5, x, \u03b8), with \u03f5 = (\u03f51, . . . , \u03f5L), by applying Eqn. 4 for each layer in\nsequence. Since the distribution of \u03f5 does not depend on \u03b8, we can reformulate the gradient of the\nbound L(x) from Eqn. 3 by pushing the gradient operator inside the expectation:\n\u2207\u03b8 log Eh\u223cq(h|x,\u03b8)\n\u0014p(x, h|\u03b8)\nq(h|x, \u03b8)\n\u0015\n= \u2207\u03b8E\u03f51,...,\u03f5L\u223cN (0,I)\n\u0014\nlog p(x, h(\u03f5, x, \u03b8)|\u03b8)\nq(h(\u03f5, x, \u03b8)|x, \u03b8)\n\u0015\n(5)\n= E\u03f51,...,\u03f5L\u223cN (0,I)\n\u0014\n\u2207\u03b8 log p(x, h(\u03f5, x, \u03b8)|\u03b8)\nq(h(\u03f5, x, \u03b8)|x, \u03b8)\n\u0015\n.\n(6)\nAssuming the mapping h is represented as a deterministic feed-forward neural network, for a \ufb01xed\n\u03f5, the gradient inside the expectation can be computed using standard backpropagation. In practice,\none approximates the expectation in Eqn. 6 by generating k samples of \u03f5 and applying the Monte\nCarlo estimator\n1\nk\nk\nX\ni=1\n\u2207\u03b8 log w (x, h(\u03f5i, x, \u03b8), \u03b8)\n(7)\nwith w(x, h, \u03b8) = p(x, h|\u03b8)/q(h|x, \u03b8). This is an unbiased estimate of \u2207\u03b8L(x). We note that\nthe VAE update and the basic REINFORCE-like update are both unbiased estimators of the same\ngradient, but the VAE update tends to have lower variance in practice because it makes use of the\nlog-likelihood gradients with respect to the latent variables.\n3\nIMPORTANCE WEIGHTED AUTOENCODER\nThe VAE objective of Eqn. 3 heavily penalizes approximate posterior samples which fail to explain\nthe observations. This places a strong constraint on the model, since the variational assumptions\nmust be approximately satis\ufb01ed in order to achieve a good lower bound. In particular, the posterior\ndistribution must be approximately factorial and predictable with a feed-forward neural network.\nThis VAE criterion may be too strict; a recognition network which places only a small fraction\n(e.g. 20%) of its samples in the region of high posterior probability region may still be suf\ufb01cient for\nperforming accurate inference. If we lower our standards in this way, this may give us additional\n\ufb02exibility to train a generative network whose posterior distributions do not \ufb01t the VAE assump-\ntions. This is the motivation behind our proposed algorithm, the Importance Weighted Autoencoder\n(IWAE).\nOur IWAE uses the same architecture as the VAE, with both a generative network and a recognition\nnetwork. The difference is that it is trained to maximize a different lower bound on log p(x). In\nparticular, we use the following lower bound, corresponding to the k-sample importance weighting\nestimate of the log-likelihood:\nLk(x) = Eh1,...,hk\u223cq(h|x)\n\"\nlog 1\nk\nk\nX\ni=1\np(x, hi)\nq(hi|x)\n#\n.\n(8)\nHere, h1, . . . , hk are sampled independently from the recognition model. The term inside the sum\ncorresponds to the unnormalized importance weights for the joint distribution, which we will denote\nas wi = p(x, hi)/q(hi|x).\nThis is a lower bound on the marginal log-likelihood, as follows from Jensen\u2019s Inequality and the\nfact that the average importance weights are an unbiased estimator of p(x):\nLk = E\n\"\nlog 1\nk\nk\nX\ni=1\nwi\n#\n\u2264log E\n\"\n1\nk\nk\nX\ni=1\nwi\n#\n= log p(x),\n(9)\nwhere the expectations are with respect to q(h|x).\nIt is perhaps unintuitive that importance weighting would be a reasonable estimator in high dimen-\nsions. Observe, however, that the special case of k = 1 is equivalent to the standard VAE objective\nshown in Eqn. 3. Using more samples can only improve the tightness of the bound:\n3\nUnder review as a conference paper at ICLR 2016\nTheorem 1. For all k, the lower bounds satisfy\nlog p(x) \u2265Lk+1 \u2265Lk.\n(10)\nMoreover, if p(h, x)/q(h|x) is bounded, then Lk approaches log p(x) as k goes to in\ufb01nity.\nProof. See Appendix A.\nThe bound Lk can be estimated using the straightforward Monte Carlo estimator, where we generate\nsamples from the recognition network and average the importance weights. One might worry about\nthe variance of this estimator, since importance weighting famously suffers from extremely high\nvariance in cases where the proposal and target distributions are not a good match. However, as\nour estimator is based on the log of the average importance weights, it does not suffer from high\nvariance. This argument is made more precise in Appendix B.\n3.1\nTRAINING PROCEDURE\nTo train an IWAE with a stochastic gradient based optimizer, we use an unbiased estimate of the\ngradient of Lk, de\ufb01ned in Eqn. 8. As with the VAE, we use the reparameterization trick to derive a\nlow-variance upate rule:\n\u2207\u03b8Lk(x) = \u2207\u03b8Eh1,...,hk\n\"\nlog 1\nk\nk\nX\ni=1\nwi\n#\n= \u2207\u03b8E\u03f51,...,\u03f5k\n\"\nlog 1\nk\nk\nX\ni=1\nw(x, h(x, \u03f5i, \u03b8), \u03b8)\n#\n(11)\n= E\u03f51,...,\u03f5k\n\"\n\u2207\u03b8 log 1\nk\nk\nX\ni=1\nw(x, h(x, \u03f5i, \u03b8), \u03b8)\n#\n(12)\n= E\u03f51,...,\u03f5k\n\" k\nX\ni=1\nf\nwi\u2207\u03b8 log w(x, h(x, \u03f5i, \u03b8), \u03b8)\n#\n,\n(13)\nwhere \u03f51, . . . , \u03f5k are the same auxiliary variables as de\ufb01ned in Section 2 for the VAE, wi =\nw(x, h(x, \u03f5i, \u03b8), \u03b8) are the importance weights expressed as a deterministic function, and f\nwi =\nwi/ Pk\ni=1 wi are the normalized importance weights.\nIn the context of a gradient-based learning algorithm, we draw k samples from the recognition\nnetwork (or, equivalently, k sets of auxiliary variables), and use the Monte Carlo estimate of Eqn. 13:\nk\nX\ni=1\nf\nwi\u2207\u03b8 log w (x, h(\u03f5i, x, \u03b8), \u03b8) .\n(14)\nIn the special case of k = 1, the single normalized weight f\nw1 takes the value 1, and one obtains the\nVAE update rule.\nWe unpack this update because it does not quite parallel that of the standard VAE.1 The gradient of\nthe log weights decomposes as:\n\u2207\u03b8 log w(x, h(x, \u03f5i, \u03b8), \u03b8) = \u2207\u03b8 log p(x, h(x, \u03f5i, \u03b8)|\u03b8) \u2212\u2207\u03b8 log q(h(x, \u03f5i, \u03b8)|x, \u03b8).\n(15)\nThe \ufb01rst term encourages the generative model to assign high probability to each h\u2113given h\u2113+1\n(following the convention that x = h0). It also encourages the recognition network to adjust the\nhidden representations so that the generative network makes better predictions. In the case of a single\nstochastic layer (i.e. L = 1), the combination of these two effects is equivalent to backpropagation\nin a stochastic autoencoder. The second term of this update encourages the recognition network to\nhave a spread-out distribution over predictions. This update is averaged over the samples with weight\nproportional to the importance weights, motivating the name \u201cimportance weighted autoencoder.\u201d\n1Kingma & Welling (2014) separated out the KL divergence in the bound of Eqn. 3 in order to achieve a\nsimpler and lower-variance update. Unfortunately, no analogous trick applies for k > 1. In principle, the IWAE\nupdates may be higher variance for this reason. However, in our experiments, we observed that the performance\nof the two update rules was indistinguishable in the case of k = 1.\n4\nUnder review as a conference paper at ICLR 2016\nThe dominant computational cost in IWAE training is computing the activations and parameter gra-\ndients needed for \u2207\u03b8 log w(x, h(x, \u03f5i, \u03b8), \u03b8). This corresponds to the forward and backward passes\nin backpropagation. In the basic IWAE implementation, both passes must be done independently for\neach of the k samples. Therefore, the number of operations scales linearly with k. In our GPU-based\nimplementation, the samples are processed in parallel by replicating each training example k times\nwithin a mini-batch.\nOne can greatly reduce the computational cost by adding another form of stochasticity. Speci\ufb01cally,\nonly the forward pass is needed to compute the importance weights. The sum in Eqn. 14 can be\nstochastically approximated by choosing a single sample \u03f5i proprtional to its normalized weight f\nwi\nand then computing \u2207\u03b8 log w(x, h(x, \u03f5i, \u03b8), \u03b8). This method requires k forward passes and one\nbackward pass per training example. Since the backward pass requires roughly twice as many add-\nmultiply operations as the forward pass, for large k, this trick reduces the number of add-multiply\noperations by roughly a factor of 3. This comes at the cost of increased variance in the updates, but\nempirically we have found the tradeoff to be favorable.\n4\nRELATED WORK\nThere are several broad families of approaches to training deep generative models. Some models\nare de\ufb01ned in terms of Boltzmann distributions (Smolensky, 1986; Salakhutdinov & E., 2009). This\nhas the advantage that many of the conditional distributions are tractable, but the inability to sample\nfrom the model or compute the partition function has been a major roadblock (Salakhutdinov &\nMurray, 2008). Other models are de\ufb01ned in terms of belief networks (Neal, 1992; Gregor et al.,\n2014). These models are tractable to sample from, but the conditional distributions become tangled\ndue to the explaining away effect.\nOne strategy for dealing with intractable posterior inference is to train a recognition network\nwhich approximates the posterior. A classic approach was the wake-sleep algorithm, used to train\nHelmholtz machines (Dayan et al., 1995). The generative model was trained to model the condi-\ntionals inferred by the recognition net, and the recognition net was trained to explain synthetic data\ngenerated by the generative net. Unfortunately, wake-sleep trained the two networks on different ob-\njective functions. Deep autoregressive networks (Gregor et al., 2014) consisted of deep generative\nand recognition networks trained using a single variational lower bound. Neural variational infer-\nence and learning (Mnih & Gregor, 2014) is another algorithm for training recognition networks\nwhich reduces stochasticity in the updates by training a third network to predict reward baselines\nin the context of the REINFORCE algorithm (Williams, 1992). Salakhutdinov & Larochelle (2010)\nused a recognition network to approximate the posterior distribution in deep Boltzmann machines.\nVariational autoencoders (Kingma & Welling, 2014; Rezende et al., 2014), as described in detail in\nSection 2, are another combination of generative and recognition networks, trained with the same\nvariational objective as DARN and NVIL. However, in place of REINFORCE, they reduce the\nvariance of the updates through a clever reparameterization of the random choices. The reparame-\nterization trick is also known as \u201cbackprop through a random number generator\u201d (Williams, 1992).\nOne factor distinguishing VAEs from the other models described above is that the model is described\nin terms of a simple distribution followed by a deterministic mapping, rather than a sequence of\nstochastic choices. Similar architectures have been proposed which use different training objectives.\nGenerative adversarial networks (Goodfellow et al., 2014) train a generative network and a recog-\nnition network which act in opposition: the recognition network attempts to distinguish between\ntraining examples and generated samples, and the generative model tries to generate samples which\nfool the recognition network. Maximum mean discrepancy (MMD) networks (Li et al., 2015; Dziu-\ngaite et al., 2015) attempt to generate samples which match a certain set of statistics of the training\ndata. They can be viewed as a kind of adversarial net where the adversary simply looks at the set of\npre-chosen statistics (Dziugaite et al., 2015). In contrast to VAEs, the training criteria for adversarial\nnets and MMD nets are not based on the data log-likelihood.\nOther researchers have derived log-probability lower bounds by way of importance sampling. Tang\n& Salakhutdinov (2013) and Ba et al. (2015) avoided recognition networks entirely, instead perform-\ning inference using importance sampling from the prior. Gogate et al. (2007) presented a variety\nof graphical model inference algorithms based on importance weighting. Reweighted wake-sleep\n5\nUnder review as a conference paper at ICLR 2016\n(RWS) of Bornschein & Bengio (2015) is another recognition network approach which combines\nthe original wake-sleep algorithm with updates to the generative network equivalent to gradient as-\ncent on our bound Lk. However, Bornschein & Bengio (2015) interpret this update as following a\nbiased estimate of \u2207\u03b8 log p(x), whereas we interpret it as following an unbiased estimate of \u2207\u03b8Lk.\nThe IWAE also differs from RWS in that the generative and recognition networks are trained to\nmaximize a single objective, Lk. By contrast, the q-wake and sleep steps of RWS do not appear to\nbe related to Lk. Finally, the IWAE differs from RWS in that it makes use of the reparameterization\ntrick.\nApart from our approach of using multiple approximate posterior samples, another way to improve\nthe \ufb02exibility of posterior inference is to use a more sophisticated algorithm than importance sam-\npling. Examples of this approach include normalizing \ufb02ows (Rezende & Mohamed, 2015) and the\nHamiltonian variational approximation of Salimans et al. (2015).\nAfter the publication of this paper the authors learned that the idea of using an importance weighted\nlower bound for training variational autoencoders has been independently explored by Laurent Dinh\nand Vincent Dumoulin, and preliminary results of their work were presented at the 2014 CIFAR\nNCAP Deep Learning summer school.\n5\nEXPERIMENTAL RESULTS\nWe have compared the generative performance of the VAE and IWAE in terms of their held-out log-\nlikelihoods on two density estimation benchmark datasets. We have further investigated a particular\nissue we have observed with VAEs and IWAEs, namely that they learn latent spaces of signi\ufb01cantly\nlower dimensionality than the modeling capacity they are allowed. We tested whether the IWAE\ntraining method ameliorates this effect.\n5.1\nEVALUATION ON DENSITY ESTIMATION\nWe evaluated the models on two benchmark datasets: MNIST, a dataset of images of handwritten\ndigits (LeCun et al., 1998), and Omniglot, a dataset of handwritten characters in a variety of world\nalphabets (Lake et al., 2013). In both cases, the observations were binarized 28 \u00d7 28 images.2 We\nused the standard splits of MNIST into 60,000 training and 10,000 test examples, and of Omniglot\ninto 24,345 training and 8,070 test examples.\nWe trained models with two architectures:\n1. An architecture with a single stochastic layer h1 with 50 units. In between the observations\nand the stochastic layer were two deterministic layers, each with 200 units.\n2. An architecture with two stochastic layers h1 and h2, with 100 and 50 units, respectively.\nIn between x and h1 were two deterministic layers with 200 units each. In between h1 and\nh2 were two deterministic layers with 100 units each.\nAll deterministic hidden units used the tanh nonlinearity. All stochastic layers used Gaussian dis-\ntributions with diagonal covariance, with the exception of the visible layer, which used Bernoulli\ndistributions. An exp nonlinearity was applied to the predicted variances of the Gaussian distribu-\ntions. The network architectures are summarized in Appendix C.\nAll models were initialized with the heuristic of Glorot & Bengio (2010). For optimization, we used\nAdam (Kingma & Ba, 2015) with parameters \u03b21 = 0.9, \u03b22 = 0.999, \u03f5 = 10\u22124 and minibaches of\nsize 20. The training proceeded for 3i passes over the data with learning rate of 0.001 \u00b7 10\u2212i/7 for\ni = 0 . . . 7 (for a total of P7\ni=0 3i = 3280 passes over the data). This learning rate schedule was\nchosen based on preliminary experiments training a VAE with one stochastic layer on MNIST.\n2Unfortunately, the generative modeling literature is inconsistent about the method of binarization, and\ndifferent choices can lead to considerably different log-likelihood values. We follow the procedure of Salakhut-\ndinov & Murray (2008): the binary-valued observations are sampled with expectations equal to the real values\nin the training set. See Appendix D for an alternative binarization scheme.\n6\nUnder review as a conference paper at ICLR 2016\nMNIST\nOMNIGLOT\nVAE\nIWAE\nVAE\nIWAE\n# stoch.\nlayers\nk\nNLL\nactive\nunits\nNLL\nactive\nunits\nNLL\nactive\nunits\nNLL\nactive\nunits\n1\n1\n86.76\n19\n86.76\n19\n108.11\n28\n108.11\n28\n5\n86.47\n20\n85.54\n22\n107.62\n28\n106.12\n34\n50\n86.35\n20\n84.78\n25\n107.80\n28\n104.67\n41\n2\n1\n85.33\n16+5\n85.33\n16+5\n107.58\n28+4\n107.56\n30+5\n5\n85.01\n17+5\n83.89\n21+5\n106.31\n30+5\n104.79\n38+6\n50\n84.78\n17+5\n82.90\n26+7\n106.30\n30+5\n103.38\n44+7\nTable 1: Results on density estimation and the number of active latent dimensions. For models with two latent\nlayers, \u201ck1+k2\u201d denotes k1 active units in the \ufb01rst layer and k2 in the second layer. The generative performance\nof IWAEs improved with increasing k, while that of VAEs bene\ufb01tted only slightly. Two-layer models achieved\nbetter generative performance than one-layer models.\nFor each number of samples k \u2208{1, 5, 50} we trained a VAE with the gradient of L(x) estimted\nas in Eqn. 7 and an IWAE with the gradient estimated as in Eqn. 14. For each k, the VAE and the\nIWAE were trained for approximately the same length of time.\nAll log-likelihood values were estimated as the mean of L5000 on the test set. Hence, the reported\nvalues are stochastic lower bounds on the true value, but are likely to be more accurate than the\nlower bounds used for training.\nThe log-likelihood results are reported in Table 1. Our VAE results are comparable to those previ-\nously reported in the literature. We observe that training a VAE with k > 1 helped only slightly. By\ncontrast, using multiple samples improved the IWAE results considerably on both datasets. Note that\nthe two algorithms are identical for k = 1, so the results ought to match up to random variability.\nOn MNIST, IWAE with two stochastic layers and k = 50 achieves a log-likelihood of -82.90 on\nthe permutation-invariant model on this dataset. By comparison, deep belief networks achieved log-\nlikelihood of approximately -84.55 nats (Murray & Salakhutdinov, 2009), and deep autoregressive\nnetworks achieved log-likelihood of -84.13 nats (Gregor et al., 2014). Gregor et al. (2015), who\nexploited spatial structure, achieved a log-likelihood of -80.97. We did not \ufb01nd over\ufb01tting to be a\nserious issue for either the VAE or the IWAE: in both cases, the training log-likelihood was 0.62 to\n0.79 nats higher than the test log-likelihood. We present samples from our models in Appendix E.\nFor the OMNIGLOT dataset, the best performing IWAE has log-likelihood of -103.38 nats, which is\nslightly worse than the log-likelihood of -100.46 nats achieved by a Restricted Boltzmann Machine\nwith 500 hidden units trained with persistent contrastive divergence (Burda et al., 2015). RBMs\ntrained with centering or FANG methods achieve a similar performance of around -100 nats (Grosse\n& Salakhudinov, 2015). The training log-likelihood for the models we trained was 2.39 to 2.65 nats\nhigher than the test log-likelihood.\n5.2\nLATENT SPACE REPRESENTATION\nWe have observed that both VAEs and IWAEs tend to learn latent representations with effective\ndimensions far below their capacity. Our next set of experiments aimed to quantify this effect and\ndetermine whether the IWAE objective ameliorates this effect.\nIf a latent dimension encodes useful information about the data, we would expect its distribution\nto change depending on the observations. Based on this intuition, we measured activity of a latent\ndimension u using the statistic Au = Covx\n\u0000Eu\u223cq(u|x)[u]\n\u0001\n. We de\ufb01ned the dimension u to be active\nif Au > 10\u22122. We have observed two pieces of evidence that this criterion is both well-de\ufb01ned and\nmeaningful:\n1. The distribution of Au for a trained model consisted of two widely separated modes, as\nshown in Appendix C.\n7\nUnder review as a conference paper at ICLR 2016\nFirst stage\nSecond stage\ntrained as\nNLL\nactive units\ntrained as\nNLL\nactive units\nExperiment 1\nVAE\n86.76\n19\nIWAE, k = 50\n84.88\n22\nExperiment 2\nIWAE, k = 50\n84.78\n25\nVAE\n86.02\n23\nTable 2: Results of continuing to train a VAE model with the IWAE objective, and vice versa. Training the\nVAE with the IWAE objective increased the latent dimension and test log-likelihood, while training the IWAE\nwith the VAE objective had the opposite effect.\n2. To con\ufb01rm that the inactive dimensions were indeed insigni\ufb01cant to the predictions, we\nevaluated all models with the inactive dimensions removed. In all cases, this changed the\ntest log-likelihood by less than 0.06 nats.\nIn Table 1, we report the numbers of active units for all conditions. In all conditions, the number of\nactive dimensions was far less than the total number of dimensions. Adding more latent dimensions\ndid not increase the number of active dimensions. Interestingly, in the two-layer models, the second\nlayer used very little of its modeling capacity: the number of active dimensions was always less\nthan 10. In all cases with k > 1, the IWAE learned more latent dimensions than the VAE. Since this\ncoincided with higher log-likelihood values, we speculate that a larger number of active dimensions\nre\ufb02ects a richer latent representation.\nSuper\ufb01cially, the phenomenon of inactive dimensions appears similar to the problem of \u201cunits dying\nout\u201d in neural networks and latent variable models, an effect which is often ascribed to dif\ufb01culties\nin optimization. For example, if a unit is inactive, it may never receive a meaningful gradient signal\nbecause of a plateau in the optimization landscape. In such cases, the problem may be avoided\nthrough a better initialization. To determine whether the inactive units resulted from an optimization\nissue or a modeling issue, we took the best-performing VAE and IWAE models from Table 1, and\ncontinued training the VAE model using the IWAE objective and vice versa. In both cases, the model\nwas trained for an additional 37 passes over the data with a learning rate of 10\u22124.\nThe results are shown in Table 2. We found that continuing to train the VAE with the IWAE objective\nincreased the number of active dimensions and the test log-likelihood, while continuing to train\nthe IWAE with the VAE objective did the opposite. The fact that training with the VAE objective\nactively reduces both the number of active dimensions and the log-likelihood strongly suggests that\ninactivation of the latent dimensions is driven by the objective functions rather than by optimization\nissues. On the other hand, optimization also appears to play a role, as the results in Table 2 are not\nquite identical to those in Table 1.\n6\nCONCLUSION\nIn this paper, we presented the importance weighted autoencoder, a variant on the VAE trained by\nmaximizing a tighter log-likelihood lower bound derived from importance weighting. We showed\nempirically that IWAEs learn richer latent representations and achieve better generative performance\nthan VAEs with equivalent architectures and training time. We believe this method may improve the\n\ufb02exibility of other generative models currently trained with the VAE objective.\n7\nACKNOWLEDGEMENTS\nThis research was supported by NSERC, the Fields Institute, and Samsung.\nREFERENCES\nBa, J. L., Mnih, V., and Kavukcuoglu, K. Multiple object recognition with visual attention. In International\nConference on Learning Representations, 2015.\nBornschein, J. and Bengio, Y. Reweighted wake-sleep. International Conference on Learning Representations,\n2015.\nBurda, Y., Grosse, R. B., and Salakhutdinov, R. Accurate and conservative estimates of MRF log-likelihood\nusing reverse annealing. Arti\ufb01cial Intelligence and Statistics, pp. 102\u2013110, 2015.\n8\nUnder review as a conference paper at ICLR 2016\nDayan, P., Hinton, G. E., Neal, R. M., and Zemel, R. S. The Helmholtz machine. Neural Computation, 7:\n889\u2013904, 1995.\nDziugaite, K. G., Roy, D. M., and Ghahramani, Z. Training generative neural networks via maximum mean\ndiscrepancy optimization. In Uncertainty in Arti\ufb01cial Intelligence, 2015.\nGlorot, X. and Bengio, Y. Understanding the dif\ufb01culty of training deep feedforward neural networks. In\nArti\ufb01cial Intelligence and Statistics, pp. 249\u2013256, 2010.\nGogate, V., Bidyuk, B., and Dechter, R. Studies in lower bounding probability of evidence using the Markov\ninequality. In Uncertainty in Arti\ufb01cial Intelligence, 2007.\nGoodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio,\nY. Generative adversarial nets. In Neural Information Processing Systems, 2014.\nGregor, K., Danihelka, I., Mnih, A., Blundell, C., and Wierstra, D. Deep autoregressive networks. International\nConference on Machine Learning, 2014.\nGregor, K., Danihelka, I., Graves, A., Rezende, D. J., and Wierstra, D. DRAW: A recurrent neural network for\nimage generation. In International Conference on Machine Learning, pp. 1462\u20131471, 2015.\nGrosse, R. and Salakhudinov, R. Scaling up natural gradient by sparsely factorizing the inverse \ufb01sher matrix.\nIn International Conference on Machine Learning, 2015.\nHinton, G. E., Osindero, S., and Teh, Y. A fast learning algorithm for deep belief nets. Neural Computation,\n2006.\nKingma, D. and Ba, J. L. Adam: A method for stochastic optimization. In International Conference on\nLearning Representations, 2015.\nKingma, D. P. and Welling, M. Auto-Encoding Variational Bayes. International Conference on Learning\nRepresentations, 2014.\nKingma, D. P., Mohamed, S., Rezende, D. J., and Welling, M. Semi-supervised learning with deep generative\nmodels. In Neural Information Processing Systems, 2014.\nKulkarni, T. D., Whitney, W., Kohli, P., and Tenenbaum, J. B. Deep convolutional inverse graphics network.\narXiv:1503.03167, 2015.\nLake, B. M., Salakhutdinov, R., and Tenenbaum, J. B. One-shot learning by inverting a compositional causal\nprocess. In Neural Information Processing Systems, 2013.\nLarochelle, H., Murray I. The neural autoregressive distribution estimator. In Arti\ufb01cial Intelligence and Statis-\ntics, pp. 29\u201337, 2011.\nLeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-based learning applied to document recognition.\nProceedings of the IEEE, 86(11):2278\u20132324, 1998.\nLi, Y., Swersky, K., and Zemel, R. Generative moment matching networks. In International Conference on\nMachine Learning, pp. 1718\u20131727, 2015.\nMnih, A. and Gregor, K. Neural variational inference and learning in belief networks. In International Confer-\nence on Machine Learning, pp. 1791\u20131799, 2014.\nMurray, I. and Salakhutdinov, R. Evaluating probabilities under high-dimensional latent variable models. In\nNeural Information Processing Systems, pp. 1137\u20131144, 2009.\nNeal, R. M. Connectionist learning of belief networks. Arti\ufb01cial Intelligence, 1992.\nRezende, D. J. and Mohamed, S. Variational inference with normalizing \ufb02ows. In International Conference on\nMachine Learning, pp. 1530\u20131538, 2015.\nRezende, D. J., Mohamed, S., and Wierstra, D. Stochastic backpropagation and approximate inference in deep\ngenerative models. International Conference on Machine Learning, pp. 1278\u20131286, 2014.\nSalakhutdinov, R. and E., Hinton G. Deep Boltzmann machines. In Neural Information Processing Systems,\n2009.\nSalakhutdinov, R. and Larochelle, H. Ef\ufb01cient learning of deep Boltzmann machines. In Arti\ufb01cial Intelligence\nand Statistics, 2010.\n9\nUnder review as a conference paper at ICLR 2016\nSalakhutdinov, R. and Murray, I. On the quantitative analysis of deep belief networks. In International Con-\nference on Machine Learning, 2008.\nSalimans, T., Kingma, D. P., and Welling, M. Markov chain Monte Carlo and variational inference: bridging\nthe gap. In International Conference on Machine Learning, pp. 1218\u20131226, 2015.\nSmolensky, P. Information processing in dynamical systems: foundations of harmony theory. In Rumelhart,\nD. E. and McClelland, J. L. (eds.), Parallel Distributed Processing: Explorations in the Microstructure of\nCognition. MIT Press, 1986.\nTang, Y. and Salakhutdinov, R.\nLearning stochastic feedforward neural networks.\nIn Neural Information\nProcessing Systems, 2013.\nWilliams, R. J. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Ma-\nchine Learning, 8:229\u2013256, 1992.\nAPPENDIX A\nProof of Theorem 1. We need to show the following facts about the log-likelihood lower bound Lk:\n1. log p(x) \u2265Lk,\n2. Lk \u2265Lm for k \u2265m,\n3. log p(x) = limk\u2192\u221eLk, assuming p(h, x)/q(h|x) is bounded.\nWe prove each in turn:\n1. It follows from Jensen\u2019s inequality that\nLk = E\n\"\nlog 1\nk\nk\nX\ni=1\np(x, hi)\nq(hi|x)\n#\n\u2264log E\n\"\n1\nk\nk\nX\ni=1\np(x, hi)\nq(hi|x)\n#\n= log p(x)\n(16)\n2. Let I \u2282{1, . . . , k} with |I| = m be a uniformly distributed subset of distinct indices from\n{1, . . . , k}. We will use the following simple observation: EI={i1,...,im}\nh ai1+...+aim\nm\ni\n=\na1+...+ak\nk\nfor any sequence of numbers a1, . . . , ak.\nUsing this observation and Jensen\u2019s inequality, we get\nLk = Eh1,...,hk\n\"\nlog 1\nk\nk\nX\ni=1\np(x, hi)\nq(hi|x)\n#\n(17)\n= Eh1,...,hk\n\uf8ee\n\uf8f0log EI={i1,...,im}\n\uf8ee\n\uf8f01\nm\nm\nX\nj=1\np(x, hij)\nq(hij|x)\n\uf8f9\n\uf8fb\n\uf8f9\n\uf8fb\n(18)\n\u2265Eh1,...,hk\n\uf8ee\n\uf8f0EI={i1,...,im}\n\uf8ee\n\uf8f0log 1\nm\nm\nX\nj=1\np(x, hij)\nq(hij|x)\n\uf8f9\n\uf8fb\n\uf8f9\n\uf8fb\n(19)\n= Eh1,...,hm\n\"\nlog 1\nm\nm\nX\ni=1\np(x, hi)\nq(hi|x)\n#\n= Lm\n(20)\n3. Consider the random variable Mk = 1\nk\nPk\ni=1\np(x,hi)\nq(hi|x). If p(h, x)/q(h|x) is bounded, then\nit follows from the strong law of large numbers that Mk converges to Eq(hi|x)\nh\np(x,hi)\nq(hi|x)\ni\n=\np(x) almost surely. Hence Lk = E log[Mk] converges to log p(x) as k \u2192\u221e.\n10\nUnder review as a conference paper at ICLR 2016\nAPPENDIX B\nIt is well known that the variance of an unnormalized importance sampling based estimator can\nbe extremely large, or even in\ufb01nite, if the proposal distribution is not well matched to the target\ndistribution. Here we argue that the Monte Carlo estimator of Lk, described in Section 3, does not\nsuffer from large variance. More precisely, we bound the mean absolute deviation (MAD). While\nthis does not directly bound the variance, it would be surprising if an estimator had small MAD yet\nextremely large variance.\nSuppose we have a strictly positive unbiased estimator \u02c6Z of a positive quantity Z, and we wish to\nuse log \u02c6Z as an estimator of log Z. By Jensen\u2019s inequality, this is a biased estimator, i.e. E[log \u02c6Z] \u2264\nlog Z. Denote the bias as \u03b4 = log Z \u2212E[log \u02c6Z]. We start with the observation that log \u02c6Z is unlikely\nto overestimate log Z by very much, as can be shown with Markov\u2019s Inequality:\nPr(log \u02c6Z > log Z + b) \u2264e\u2212b.\n(21)\nLet (X)+ denote max(X, 0). We now use the above facts to bound the MAD:\nE\nh\f\f\flog \u02c6Z \u2212E[log \u02c6Z]\n\f\f\f\ni\n= 2E\n\u0014\u0010\nlog \u02c6Z \u2212E[log \u02c6Z]\n\u0011\n+\n\u0015\n(22)\n= 2E\n\u0014\u0010\nlog \u02c6Z \u2212log Z + log Z \u2212E[log \u02c6Z]\n\u0011\n+\n\u0015\n(23)\n\u22642E\n\u0014\u0010\nlog \u02c6Z \u2212log Z\n\u0011\n+ +\n\u0010\nlog Z \u2212E[log \u02c6Z]\n\u0011\n+\n\u0015\n(24)\n= 2E\n\u0014\u0010\nlog \u02c6Z \u2212log Z\n\u0011\n+\n\u0015\n+ 2\u03b4\n(25)\n= 2\nZ \u221e\n0\nPr\n\u0010\nlog \u02c6Z \u2212log Z > t\n\u0011\ndt + 2\u03b4\n(26)\n\u22642\nZ \u221e\n0\ne\u2212tdt + 2\u03b4\n(27)\n= 2 + 2\u03b4\n(28)\nHere, (22) is a general formula for the MAD, (26) uses the formula E[Y ] =\nR \u221e\n0\nPr(Y > t) dt for\na nonnegative random variable Y , and (27) applies the bound (21). Hence, the MAD is bounded by\n2 + 2\u03b4. In the context of IWAE, \u03b4 corresponds to the gap between Lk and log p(x).\nAPPENDIX C\nNETWORK ARCHITECTURES\nHere is a summary of the network architectures used in the experiments:\nq(h1|x) = N(h1|\u00b5q,1, diag(\u03c3q,1))\nx\n200d\n200d\n\u00b5q,1\n\u03c3q,1\nlin+tanh\nlin+tanh\nlin\nlin+exp\nq(h2|h1) = N(h2|\u00b5q,2, diag(\u03c3q,2))\nh1\n100d\n100d\n\u00b5q,2\n\u03c3q,2\nlin+tanh\nlin+tanh\nlin\nlin+exp\np(h1|h2) = N(h1|\u00b5p,1, diag(\u03c3p,1))\nh2\n100d\n100d\n\u00b5p,1\n\u03c3p,1\nlin+tanh\nlin+tanh\nlin\nlin+exp\np(x|h1) = Bernoulli(x|\u00b5p,0)\nh1\n200d\n200d\n\u00b5p,0\nlin+tanh\nlin+tanh\nlin+sigm\n11\nUnder review as a conference paper at ICLR 2016\nDISTRIBUTION OF ACTIVITY STATISTIC\nIn Section 5.2, we de\ufb01ned the activity statistic Au = Covx\n\u0000Eu\u223cq(u|x)[u]\n\u0001\n, and chose a threshold\nof 10\u22122 for determining if a unit is active. One justi\ufb01cation for this is that the distribution of this\nstatistic consisted of two widely separated modes in every case we looked at. Here is the histogram\nof log Au for a VAE with one stochastic layer:\n8\n7\n6\n5\n4\n3\n2\n1\n0\n1\nlog variance of \u00b5\n0\n2\n4\n6\n8\n10\n12\nnumber of units\nVISUALIZATION OF POSTERIOR DISTRIBUTIONS\nWe show some examples of true and approximate posteriors for VAE and IWAE models trained with\ntwo latent dimensions. Heat maps show true posterior distributions for 6 training examples, and the\npictures in the bottom row show the examples and their reconstruction from samples from q(h|x).\nLeft: VAE. Middle: IWAE, with k = 5. Right: IWAE, with k = 50. The IWAE prefers less regular\nposteriors and more spread out posterior predictions.\n12\nUnder review as a conference paper at ICLR 2016\nAPPENDIX D\nRESULTS FOR A FIXED MNIST BINARIZATION\nSeveral previous works have used a \ufb01xed binarization of the MNIST dataset de\ufb01ned by Larochelle\n(2011). We repeated our experiments training the models on the 50000 examples from the training\ndataset, and evaluating them on the 10000 examples from the test dataset. Otherwise we used the\nsame training procedure and hyperparameters as in the experiments in the main part of the paper.\nThe results in table 3 indicate that the conclusions about the relative merits of VAEs and IWAEs are\nunchanged in the new experimental setup. In this setup we noticed signi\ufb01cantly larger amounts of\nover\ufb01tting.\nVAE\nIWAE\n# stoch.\nlayers\nk\nNLL\nactive\nunits\nNLL\nactive\nunits\n1\n1\n88.71\n19\n88.71\n19\n5\n88.83\n19\n87.63\n22\n50\n89.05\n20\n87.10\n24\n2\n1\n88.08\n16+5\n88.08\n16+5\n5\n87.63\n17+5\n86.17\n21+5\n50\n87.86\n17+6\n85.32\n24+7\nTable 3: Results on density estimation and the number of active latent dimensions on the \ufb01xed binarization\nMNIST dataset. For models with two latent layers, \u201ck1 + k2\u201d denotes k1 active units in the \ufb01rst layer and k2\nin the second layer. The generative performance of IWAEs improved with increasing k, while that of VAEs\nbene\ufb01tted only slightly. Two-layer models achieved better generative performance than one-layer models.\nAPPENDIX E\nSAMPLES\n13\nUnder review as a conference paper at ICLR 2016\nTable 4: Random samples from VAE (left column) and IWAE with k = 50 (right column) models. Row 1:\nmodels with one stochastic layer. Row 2: models with two stochastic layers. Samples are represented as the\nmeans of the corresponding Bernoulli distributions.\n14\n",
        "sentence": " [23] further analyzed a closely related importance sampling based estimator in the context of generative models, bounding the mean absolute deviation and showing that the bias decreases monotonically with the number of samples.",
        "context": "distribution. Here we argue that the Monte Carlo estimator of Lk, described in Section 3, does not\nsuffer from large variance. More precisely, we bound the mean absolute deviation (MAD). While\n10\nUnder review as a conference paper at ICLR 2016\nAPPENDIX B\nIt is well known that the variance of an unnormalized importance sampling based estimator can\nbe extremely large, or even in\ufb01nite, if the proposal distribution is not well matched to the target\nIt is perhaps unintuitive that importance weighting would be a reasonable estimator in high dimen-\nsions. Observe, however, that the special case of k = 1 is equivalent to the standard VAE objective"
    },
    {
        "title": "The optimal reward baseline for gradient-based reinforcement learning",
        "author": [
            "Lex Weaver",
            "Nigel Tao"
        ],
        "venue": "In Proceedings of the Seventeenth conference on Uncertainty in artificial intelligence,",
        "citeRegEx": "24",
        "shortCiteRegEx": "24",
        "year": 2001,
        "abstract": "",
        "full_text": "",
        "sentence": " Choosing effective control variates for the stochastic gradient estimators amounts to finding a function that is highly correlated with the gradient vectors, and whose expectation is known or tractable to compute [10, 24].",
        "context": null
    },
    {
        "title": "Gradient-based learning applied to document recognition",
        "author": [
            "Y. LeCun",
            "L. Bottou",
            "Y. Bengio",
            "P. Haffner"
        ],
        "venue": "Proceedings of the IEEE, 86(11):2278\u20132324,",
        "citeRegEx": "25",
        "shortCiteRegEx": null,
        "year": 1998,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " To measure the effectiveness of the proposed WS-RAM method, we first investigated a toy classification task involving a variant of the MNIST handwritten digits dataset [25] where transformations were applied to the images. We generated a dataset of randomly translated and scaled handwritten digits from the MNIST dataset [25].",
        "context": null
    },
    {
        "title": "Framing image description as a ranking task: Data, models and evaluation metrics",
        "author": [
            "Micah Hodosh",
            "Peter Young",
            "Julia Hockenmaier"
        ],
        "venue": "Journal of Artificial Intelligence Research,",
        "citeRegEx": "26",
        "shortCiteRegEx": "26",
        "year": 2013,
        "abstract": "The ability to associate images with natural language sentences that describe what is depicted in them is a hallmark of image understanding, and a prerequisite for applications such as sentence-based image search. In analogy to image search, we propose to frame sentence-based image annotation as the task of ranking a given pool of captions. We introduce a new benchmark collection for sentence-based image description and search, consisting of 8,000 images that are each paired with five different captions which provide clear descriptions of the salient entities and events. We introduce a number of systems that perform quite well on this task, even though they are only based on features that can be obtained with minimal supervision. Our results clearly indicate the importance of training on multiple captions per image, and of capturing syntactic (word order-based) and semantic features of these captions. We also perform an in-depth comparison of human and automatic evaluation metrics for this task, and propose strategies for collecting human judgments cheaply and on a very large scale, allowing us to augment our collection with additional relevance judgments of which captions describe which image. Our analysis shows that metrics that consider the ranked list of results for each query image or sentence are significantly more robust than metrics that are based on a single response per query. Moreover, our study suggests that the evaluation of ranking-based image description systems may be fully automated.",
        "full_text": "",
        "sentence": " We then evaluated the proposed method on a substantially more difficult image caption generation task using the Flickr8k [26] dataset.",
        "context": null
    },
    {
        "title": "Adam: a method for stochastic optimization",
        "author": [
            "D. Kingma",
            "J.L. Ba"
        ],
        "venue": "arXiv:1412.6980,",
        "citeRegEx": "27",
        "shortCiteRegEx": null,
        "year": 2014,
        "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization\nof stochastic objective functions, based on adaptive estimates of lower-order\nmoments. The method is straightforward to implement, is computationally\nefficient, has little memory requirements, is invariant to diagonal rescaling\nof the gradients, and is well suited for problems that are large in terms of\ndata and/or parameters. The method is also appropriate for non-stationary\nobjectives and problems with very noisy and/or sparse gradients. The\nhyper-parameters have intuitive interpretations and typically require little\ntuning. Some connections to related algorithms, on which Adam was inspired, are\ndiscussed. We also analyze the theoretical convergence properties of the\nalgorithm and provide a regret bound on the convergence rate that is comparable\nto the best known results under the online convex optimization framework.\nEmpirical results demonstrate that Adam works well in practice and compares\nfavorably to other stochastic optimization methods. Finally, we discuss AdaMax,\na variant of Adam based on the infinity norm.",
        "full_text": "Published as a conference paper at ICLR 2015\nADAM: A METHOD FOR STOCHASTIC OPTIMIZATION\nDiederik P. Kingma*\nUniversity of Amsterdam, OpenAI\ndpkingma@openai.com\nJimmy Lei Ba\u2217\nUniversity of Toronto\njimmy@psi.utoronto.ca\nABSTRACT\nWe introduce Adam, an algorithm for \ufb01rst-order gradient-based optimization of\nstochastic objective functions, based on adaptive estimates of lower-order mo-\nments. The method is straightforward to implement, is computationally ef\ufb01cient,\nhas little memory requirements, is invariant to diagonal rescaling of the gradients,\nand is well suited for problems that are large in terms of data and/or parameters.\nThe method is also appropriate for non-stationary objectives and problems with\nvery noisy and/or sparse gradients. The hyper-parameters have intuitive interpre-\ntations and typically require little tuning. Some connections to related algorithms,\non which Adam was inspired, are discussed. We also analyze the theoretical con-\nvergence properties of the algorithm and provide a regret bound on the conver-\ngence rate that is comparable to the best known results under the online convex\noptimization framework. Empirical results demonstrate that Adam works well in\npractice and compares favorably to other stochastic optimization methods. Finally,\nwe discuss AdaMax, a variant of Adam based on the in\ufb01nity norm.\n1\nINTRODUCTION\nStochastic gradient-based optimization is of core practical importance in many \ufb01elds of science and\nengineering. Many problems in these \ufb01elds can be cast as the optimization of some scalar parameter-\nized objective function requiring maximization or minimization with respect to its parameters. If the\nfunction is differentiable w.r.t. its parameters, gradient descent is a relatively ef\ufb01cient optimization\nmethod, since the computation of \ufb01rst-order partial derivatives w.r.t. all the parameters is of the same\ncomputational complexity as just evaluating the function. Often, objective functions are stochastic.\nFor example, many objective functions are composed of a sum of subfunctions evaluated at different\nsubsamples of data; in this case optimization can be made more ef\ufb01cient by taking gradient steps\nw.r.t. individual subfunctions, i.e. stochastic gradient descent (SGD) or ascent. SGD proved itself\nas an ef\ufb01cient and effective optimization method that was central in many machine learning success\nstories, such as recent advances in deep learning (Deng et al., 2013; Krizhevsky et al., 2012; Hinton\n& Salakhutdinov, 2006; Hinton et al., 2012a; Graves et al., 2013). Objectives may also have other\nsources of noise than data subsampling, such as dropout (Hinton et al., 2012b) regularization. For\nall such noisy objectives, ef\ufb01cient stochastic optimization techniques are required. The focus of this\npaper is on the optimization of stochastic objectives with high-dimensional parameters spaces. In\nthese cases, higher-order optimization methods are ill-suited, and discussion in this paper will be\nrestricted to \ufb01rst-order methods.\nWe propose Adam, a method for ef\ufb01cient stochastic optimization that only requires \ufb01rst-order gra-\ndients with little memory requirement. The method computes individual adaptive learning rates for\ndifferent parameters from estimates of \ufb01rst and second moments of the gradients; the name Adam\nis derived from adaptive moment estimation. Our method is designed to combine the advantages\nof two recently popular methods: AdaGrad (Duchi et al., 2011), which works well with sparse gra-\ndients, and RMSProp (Tieleman & Hinton, 2012), which works well in on-line and non-stationary\nsettings; important connections to these and other stochastic optimization methods are clari\ufb01ed in\nsection 5. Some of Adam\u2019s advantages are that the magnitudes of parameter updates are invariant to\nrescaling of the gradient, its stepsizes are approximately bounded by the stepsize hyperparameter,\nit does not require a stationary objective, it works with sparse gradients, and it naturally performs a\nform of step size annealing.\n\u2217Equal contribution. Author ordering determined by coin \ufb02ip over a Google Hangout.\n1\narXiv:1412.6980v9  [cs.LG]  30 Jan 2017\nPublished as a conference paper at ICLR 2015\nAlgorithm 1: Adam, our proposed algorithm for stochastic optimization. See section 2 for details,\nand for a slightly more ef\ufb01cient (but less clear) order of computation. g2\nt indicates the elementwise\nsquare gt \u2299gt. Good default settings for the tested machine learning problems are \u03b1 = 0.001,\n\u03b21 = 0.9, \u03b22 = 0.999 and \u03f5 = 10\u22128. All operations on vectors are element-wise. With \u03b2t\n1 and \u03b2t\n2\nwe denote \u03b21 and \u03b22 to the power t.\nRequire: \u03b1: Stepsize\nRequire: \u03b21, \u03b22 \u2208[0, 1): Exponential decay rates for the moment estimates\nRequire: f(\u03b8): Stochastic objective function with parameters \u03b8\nRequire: \u03b80: Initial parameter vector\nm0 \u21900 (Initialize 1st moment vector)\nv0 \u21900 (Initialize 2nd moment vector)\nt \u21900 (Initialize timestep)\nwhile \u03b8t not converged do\nt \u2190t + 1\ngt \u2190\u2207\u03b8ft(\u03b8t\u22121) (Get gradients w.r.t. stochastic objective at timestep t)\nmt \u2190\u03b21 \u00b7 mt\u22121 + (1 \u2212\u03b21) \u00b7 gt (Update biased \ufb01rst moment estimate)\nvt \u2190\u03b22 \u00b7 vt\u22121 + (1 \u2212\u03b22) \u00b7 g2\nt (Update biased second raw moment estimate)\nbmt \u2190mt/(1 \u2212\u03b2t\n1) (Compute bias-corrected \ufb01rst moment estimate)\nbvt \u2190vt/(1 \u2212\u03b2t\n2) (Compute bias-corrected second raw moment estimate)\n\u03b8t \u2190\u03b8t\u22121 \u2212\u03b1 \u00b7 bmt/(\u221abvt + \u03f5) (Update parameters)\nend while\nreturn \u03b8t (Resulting parameters)\nIn section 2 we describe the algorithm and the properties of its update rule. Section 3 explains\nour initialization bias correction technique, and section 4 provides a theoretical analysis of Adam\u2019s\nconvergence in online convex programming. Empirically, our method consistently outperforms other\nmethods for a variety of models and datasets, as shown in section 6. Overall, we show that Adam is\na versatile algorithm that scales to large-scale high-dimensional machine learning problems.\n2\nALGORITHM\nSee algorithm 1 for pseudo-code of our proposed algorithm Adam. Let f(\u03b8) be a noisy objec-\ntive function: a stochastic scalar function that is differentiable w.r.t. parameters \u03b8. We are in-\nterested in minimizing the expected value of this function, E[f(\u03b8)] w.r.t. its parameters \u03b8. With\nf1(\u03b8), ..., , fT (\u03b8) we denote the realisations of the stochastic function at subsequent timesteps\n1, ..., T. The stochasticity might come from the evaluation at random subsamples (minibatches)\nof datapoints, or arise from inherent function noise. With gt = \u2207\u03b8ft(\u03b8) we denote the gradient, i.e.\nthe vector of partial derivatives of ft, w.r.t \u03b8 evaluated at timestep t.\nThe algorithm updates exponential moving averages of the gradient (mt) and the squared gradient\n(vt) where the hyper-parameters \u03b21, \u03b22 \u2208[0, 1) control the exponential decay rates of these moving\naverages. The moving averages themselves are estimates of the 1st moment (the mean) and the\n2nd raw moment (the uncentered variance) of the gradient. However, these moving averages are\ninitialized as (vectors of) 0\u2019s, leading to moment estimates that are biased towards zero, especially\nduring the initial timesteps, and especially when the decay rates are small (i.e. the \u03b2s are close to 1).\nThe good news is that this initialization bias can be easily counteracted, resulting in bias-corrected\nestimates bmt and bvt. See section 3 for more details.\nNote that the ef\ufb01ciency of algorithm 1 can, at the expense of clarity, be improved upon by changing\nthe order of computation, e.g. by replacing the last three lines in the loop with the following lines:\n\u03b1t = \u03b1 \u00b7\np\n1 \u2212\u03b2t\n2/(1 \u2212\u03b2t\n1) and \u03b8t \u2190\u03b8t\u22121 \u2212\u03b1t \u00b7 mt/(\u221avt + \u02c6\u03f5).\n2.1\nADAM\u2019S UPDATE RULE\nAn important property of Adam\u2019s update rule is its careful choice of stepsizes. Assuming \u03f5 = 0, the\neffective step taken in parameter space at timestep t is \u2206t = \u03b1 \u00b7 bmt/\u221abvt. The effective stepsize has\ntwo upper bounds: |\u2206t| \u2264\u03b1 \u00b7 (1 \u2212\u03b21)/\u221a1 \u2212\u03b22 in the case (1 \u2212\u03b21) > \u221a1 \u2212\u03b22, and |\u2206t| \u2264\u03b1\n2\nPublished as a conference paper at ICLR 2015\notherwise. The \ufb01rst case only happens in the most severe case of sparsity: when a gradient has\nbeen zero at all timesteps except at the current timestep. For less sparse cases, the effective stepsize\nwill be smaller. When (1 \u2212\u03b21) = \u221a1 \u2212\u03b22 we have that | bmt/\u221abvt| < 1 therefore |\u2206t| < \u03b1. In\nmore common scenarios, we will have that bmt/\u221abvt \u2248\u00b11 since |E[g]/\np\nE[g2]| \u22641. The effective\nmagnitude of the steps taken in parameter space at each timestep are approximately bounded by\nthe stepsize setting \u03b1, i.e., |\u2206t| \u2a85\u03b1. This can be understood as establishing a trust region around\nthe current parameter value, beyond which the current gradient estimate does not provide suf\ufb01cient\ninformation. This typically makes it relatively easy to know the right scale of \u03b1 in advance. For\nmany machine learning models, for instance, we often know in advance that good optima are with\nhigh probability within some set region in parameter space; it is not uncommon, for example, to\nhave a prior distribution over the parameters. Since \u03b1 sets (an upper bound of) the magnitude of\nsteps in parameter space, we can often deduce the right order of magnitude of \u03b1 such that optima\ncan be reached from \u03b80 within some number of iterations. With a slight abuse of terminology,\nwe will call the ratio bmt/\u221abvt the signal-to-noise ratio (SNR). With a smaller SNR the effective\nstepsize \u2206t will be closer to zero. This is a desirable property, since a smaller SNR means that\nthere is greater uncertainty about whether the direction of bmt corresponds to the direction of the true\ngradient. For example, the SNR value typically becomes closer to 0 towards an optimum, leading\nto smaller effective steps in parameter space: a form of automatic annealing. The effective stepsize\n\u2206t is also invariant to the scale of the gradients; rescaling the gradients g with factor c will scale bmt\nwith a factor c and bvt with a factor c2, which cancel out: (c \u00b7 bmt)/(\u221a\nc2 \u00b7 bvt) = bmt/\u221abvt.\n3\nINITIALIZATION BIAS CORRECTION\nAs explained in section 2, Adam utilizes initialization bias correction terms. We will here derive\nthe term for the second moment estimate; the derivation for the \ufb01rst moment estimate is completely\nanalogous. Let g be the gradient of the stochastic objective f, and we wish to estimate its second\nraw moment (uncentered variance) using an exponential moving average of the squared gradient,\nwith decay rate \u03b22. Let g1, ..., gT be the gradients at subsequent timesteps, each a draw from an\nunderlying gradient distribution gt \u223cp(gt). Let us initialize the exponential moving average as\nv0 = 0 (a vector of zeros). First note that the update at timestep t of the exponential moving average\nvt = \u03b22 \u00b7 vt\u22121 + (1 \u2212\u03b22) \u00b7 g2\nt (where g2\nt indicates the elementwise square gt \u2299gt) can be written as\na function of the gradients at all previous timesteps:\nvt = (1 \u2212\u03b22)\nt\nX\ni=1\n\u03b2t\u2212i\n2\n\u00b7 g2\ni\n(1)\nWe wish to know how E[vt], the expected value of the exponential moving average at timestep t,\nrelates to the true second moment E[g2\nt ], so we can correct for the discrepancy between the two.\nTaking expectations of the left-hand and right-hand sides of eq. (1):\nE[vt] = E\n\"\n(1 \u2212\u03b22)\nt\nX\ni=1\n\u03b2t\u2212i\n2\n\u00b7 g2\ni\n#\n(2)\n= E[g2\nt ] \u00b7 (1 \u2212\u03b22)\nt\nX\ni=1\n\u03b2t\u2212i\n2\n+ \u03b6\n(3)\n= E[g2\nt ] \u00b7 (1 \u2212\u03b2t\n2) + \u03b6\n(4)\nwhere \u03b6 = 0 if the true second moment E[g2\ni ] is stationary; otherwise \u03b6 can be kept small since\nthe exponential decay rate \u03b21 can (and should) be chosen such that the exponential moving average\nassigns small weights to gradients too far in the past. What is left is the term (1 \u2212\u03b2t\n2) which is\ncaused by initializing the running average with zeros. In algorithm 1 we therefore divide by this\nterm to correct the initialization bias.\nIn case of sparse gradients, for a reliable estimate of the second moment one needs to average over\nmany gradients by chosing a small value of \u03b22; however it is exactly this case of small \u03b22 where a\nlack of initialisation bias correction would lead to initial steps that are much larger.\n3\nPublished as a conference paper at ICLR 2015\n4\nCONVERGENCE ANALYSIS\nWe analyze the convergence of Adam using the online learning framework proposed in (Zinkevich,\n2003). Given an arbitrary, unknown sequence of convex cost functions f1(\u03b8), f2(\u03b8),..., fT (\u03b8). At\neach time t, our goal is to predict the parameter \u03b8t and evaluate it on a previously unknown cost\nfunction ft. Since the nature of the sequence is unknown in advance, we evaluate our algorithm\nusing the regret, that is the sum of all the previous difference between the online prediction ft(\u03b8t)\nand the best \ufb01xed point parameter ft(\u03b8\u2217) from a feasible set X for all the previous steps. Concretely,\nthe regret is de\ufb01ned as:\nR(T) =\nT\nX\nt=1\n[ft(\u03b8t) \u2212ft(\u03b8\u2217)]\n(5)\nwhere \u03b8\u2217= arg min\u03b8\u2208X\nPT\nt=1 ft(\u03b8). We show Adam has O(\n\u221a\nT) regret bound and a proof is given\nin the appendix. Our result is comparable to the best known bound for this general convex online\nlearning problem. We also use some de\ufb01nitions simplify our notation, where gt \u225c\u2207ft(\u03b8t) and gt,i\nas the ith element. We de\ufb01ne g1:t,i \u2208Rt as a vector that contains the ith dimension of the gradients\nover all iterations till t, g1:t,i = [g1,i, g2,i, \u00b7 \u00b7 \u00b7 , gt,i]. Also, we de\ufb01ne \u03b3 \u225c\n\u03b22\n1\n\u221a\u03b22 . Our following\ntheorem holds when the learning rate \u03b1t is decaying at a rate of t\u22121\n2 and \ufb01rst moment running\naverage coef\ufb01cient \u03b21,t decay exponentially with \u03bb, that is typically close to 1, e.g. 1 \u221210\u22128.\nTheorem 4.1. Assume that the function ft has bounded gradients, \u2225\u2207ft(\u03b8)\u22252 \u2264G, \u2225\u2207ft(\u03b8)\u2225\u221e\u2264\nG\u221efor all \u03b8 \u2208Rd and distance between any \u03b8t generated by Adam is bounded, \u2225\u03b8n \u2212\u03b8m\u22252 \u2264D,\n\u2225\u03b8m \u2212\u03b8n\u2225\u221e\u2264D\u221efor any m, n \u2208{1, ..., T}, and \u03b21, \u03b22 \u2208[0, 1) satisfy\n\u03b22\n1\n\u221a\u03b22 < 1. Let \u03b1t =\n\u03b1\n\u221a\nt\nand \u03b21,t = \u03b21\u03bbt\u22121, \u03bb \u2208(0, 1). Adam achieves the following guarantee, for all T \u22651.\nR(T) \u2264\nD2\n2\u03b1(1 \u2212\u03b21)\nd\nX\ni=1\np\nTbvT,i+\n\u03b1(1 + \u03b21)G\u221e\n(1 \u2212\u03b21)\u221a1 \u2212\u03b22(1 \u2212\u03b3)2\nd\nX\ni=1\n\u2225g1:T,i\u22252+\nd\nX\ni=1\nD2\n\u221eG\u221e\n\u221a1 \u2212\u03b22\n2\u03b1(1 \u2212\u03b21)(1 \u2212\u03bb)2\nOur Theorem 4.1 implies when the data features are sparse and bounded gradients, the sum-\nmation term can be much smaller than its upper bound Pd\ni=1 \u2225g1:T,i\u22252\n<< dG\u221e\n\u221a\nT and\nPd\ni=1\np\nTbvT,i << dG\u221e\n\u221a\nT, in particular if the class of function and data features are in the form of\nsection 1.2 in (Duchi et al., 2011). Their results for the expected value E[Pd\ni=1 \u2225g1:T,i\u22252] also apply\nto Adam. In particular, the adaptive method, such as Adam and Adagrad, can achieve O(log d\n\u221a\nT),\nan improvement over O(\n\u221a\ndT) for the non-adaptive method. Decaying \u03b21,t towards zero is impor-\ntant in our theoretical analysis and also matches previous empirical \ufb01ndings, e.g. (Sutskever et al.,\n2013) suggests reducing the momentum coef\ufb01cient in the end of training can improve convergence.\nFinally, we can show the average regret of Adam converges,\nCorollary 4.2. Assume that the function ft has bounded gradients, \u2225\u2207ft(\u03b8)\u22252 \u2264G, \u2225\u2207ft(\u03b8)\u2225\u221e\u2264\nG\u221efor all \u03b8 \u2208Rd and distance between any \u03b8t generated by Adam is bounded, \u2225\u03b8n \u2212\u03b8m\u22252 \u2264D,\n\u2225\u03b8m \u2212\u03b8n\u2225\u221e\u2264D\u221efor any m, n \u2208{1, ..., T}. Adam achieves the following guarantee, for all\nT \u22651.\nR(T)\nT\n= O( 1\n\u221a\nT\n)\nThis result can be obtained by using Theorem 4.1 and Pd\ni=1 \u2225g1:T,i\u22252 \u2264dG\u221e\n\u221a\nT.\nThus,\nlimT \u2192\u221e\nR(T )\nT\n= 0.\n5\nRELATED WORK\nOptimization methods bearing a direct relation to Adam are RMSProp (Tieleman & Hinton, 2012;\nGraves, 2013) and AdaGrad (Duchi et al., 2011); these relationships are discussed below. Other\nstochastic optimization methods include vSGD (Schaul et al., 2012), AdaDelta (Zeiler, 2012) and the\nnatural Newton method from Roux & Fitzgibbon (2010), all setting stepsizes by estimating curvature\n4\nPublished as a conference paper at ICLR 2015\nfrom \ufb01rst-order information. The Sum-of-Functions Optimizer (SFO) (Sohl-Dickstein et al., 2014)\nis a quasi-Newton method based on minibatches, but (unlike Adam) has memory requirements linear\nin the number of minibatch partitions of a dataset, which is often infeasible on memory-constrained\nsystems such as a GPU. Like natural gradient descent (NGD) (Amari, 1998), Adam employs a\npreconditioner that adapts to the geometry of the data, since bvt is an approximation to the diagonal\nof the Fisher information matrix (Pascanu & Bengio, 2013); however, Adam\u2019s preconditioner (like\nAdaGrad\u2019s) is more conservative in its adaption than vanilla NGD by preconditioning with the square\nroot of the inverse of the diagonal Fisher information matrix approximation.\nRMSProp:\nAn optimization method closely related to Adam is RMSProp (Tieleman & Hinton,\n2012). A version with momentum has sometimes been used (Graves, 2013). There are a few impor-\ntant differences between RMSProp with momentum and Adam: RMSProp with momentum gener-\nates its parameter updates using a momentum on the rescaled gradient, whereas Adam updates are\ndirectly estimated using a running average of \ufb01rst and second moment of the gradient. RMSProp\nalso lacks a bias-correction term; this matters most in case of a value of \u03b22 close to 1 (required in\ncase of sparse gradients), since in that case not correcting the bias leads to very large stepsizes and\noften divergence, as we also empirically demonstrate in section 6.4.\nAdaGrad:\nAn algorithm that works well for sparse gradients is AdaGrad (Duchi et al., 2011). Its\nbasic version updates parameters as \u03b8t+1 = \u03b8t \u2212\u03b1 \u00b7 gt/\nqPt\ni=1 g2\nt . Note that if we choose \u03b22 to be\nin\ufb01nitesimally close to 1 from below, then lim\u03b22\u21921 bvt = t\u22121 \u00b7 Pt\ni=1 g2\nt . AdaGrad corresponds to a\nversion of Adam with \u03b21 = 0, in\ufb01nitesimal (1 \u2212\u03b22) and a replacement of \u03b1 by an annealed version\n\u03b1t = \u03b1 \u00b7 t\u22121/2, namely \u03b8t \u2212\u03b1 \u00b7 t\u22121/2 \u00b7 bmt/\np\nlim\u03b22\u21921 bvt = \u03b8t \u2212\u03b1 \u00b7 t\u22121/2 \u00b7 gt/\nq\nt\u22121 \u00b7 Pt\ni=1 g2\nt =\n\u03b8t \u2212\u03b1 \u00b7 gt/\nqPt\ni=1 g2\nt . Note that this direct correspondence between Adam and Adagrad does\nnot hold when removing the bias-correction terms; without bias correction, like in RMSProp, a \u03b22\nin\ufb01nitesimally close to 1 would lead to in\ufb01nitely large bias, and in\ufb01nitely large parameter updates.\n6\nEXPERIMENTS\nTo empirically evaluate the proposed method, we investigated different popular machine learning\nmodels, including logistic regression, multilayer fully connected neural networks and deep convolu-\ntional neural networks. Using large models and datasets, we demonstrate Adam can ef\ufb01ciently solve\npractical deep learning problems.\nWe use the same parameter initialization when comparing different optimization algorithms. The\nhyper-parameters, such as learning rate and momentum, are searched over a dense grid and the\nresults are reported using the best hyper-parameter setting.\n6.1\nEXPERIMENT: LOGISTIC REGRESSION\nWe evaluate our proposed method on L2-regularized multi-class logistic regression using the MNIST\ndataset. Logistic regression has a well-studied convex objective, making it suitable for comparison\nof different optimizers without worrying about local minimum issues. The stepsize \u03b1 in our logistic\nregression experiments is adjusted by 1/\n\u221a\nt decay, namely \u03b1t =\n\u03b1\n\u221a\nt that matches with our theorat-\nical prediction from section 4. The logistic regression classi\ufb01es the class label directly on the 784\ndimension image vectors. We compare Adam to accelerated SGD with Nesterov momentum and\nAdagrad using minibatch size of 128. According to Figure 1, we found that the Adam yields similar\nconvergence as SGD with momentum and both converge faster than Adagrad.\nAs discussed in (Duchi et al., 2011), Adagrad can ef\ufb01ciently deal with sparse features and gradi-\nents as one of its main theoretical results whereas SGD is low at learning rare features. Adam with\n1/\n\u221a\nt decay on its stepsize should theoratically match the performance of Adagrad. We examine the\nsparse feature problem using IMDB movie review dataset from (Maas et al., 2011). We pre-process\nthe IMDB movie reviews into bag-of-words (BoW) feature vectors including the \ufb01rst 10,000 most\nfrequent words. The 10,000 dimension BoW feature vector for each review is highly sparse. As sug-\ngested in (Wang & Manning, 2013), 50% dropout noise can be applied to the BoW features during\n5\nPublished as a conference paper at ICLR 2015\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\niterations over entire dataset\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\ntraining cost\nMNIST Logistic Regression\nAdaGrad\nSGDNesterov\nAdam\n0\n20\n40\n60\n80\n100\n120\n140\n160\niterations over entire dataset\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\ntraining cost\nIMDB BoW feature Logistic Regression\nAdagrad+dropout\nRMSProp+dropout\nSGDNesterov+dropout\nAdam+dropout\nFigure 1: Logistic regression training negative log likelihood on MNIST images and IMDB movie\nreviews with 10,000 bag-of-words (BoW) feature vectors.\ntraining to prevent over-\ufb01tting. In \ufb01gure 1, Adagrad outperforms SGD with Nesterov momentum\nby a large margin both with and without dropout noise. Adam converges as fast as Adagrad. The\nempirical performance of Adam is consistent with our theoretical \ufb01ndings in sections 2 and 4. Sim-\nilar to Adagrad, Adam can take advantage of sparse features and obtain faster convergence rate than\nnormal SGD with momentum.\n6.2\nEXPERIMENT: MULTI-LAYER NEURAL NETWORKS\nMulti-layer neural network are powerful models with non-convex objective functions. Although\nour convergence analysis does not apply to non-convex problems, we empirically found that Adam\noften outperforms other methods in such cases. In our experiments, we made model choices that are\nconsistent with previous publications in the area; a neural network model with two fully connected\nhidden layers with 1000 hidden units each and ReLU activation are used for this experiment with\nminibatch size of 128.\nFirst, we study different optimizers using the standard deterministic cross-entropy objective func-\ntion with L2 weight decay on the parameters to prevent over-\ufb01tting. The sum-of-functions (SFO)\nmethod (Sohl-Dickstein et al., 2014) is a recently proposed quasi-Newton method that works with\nminibatches of data and has shown good performance on optimization of multi-layer neural net-\nworks. We used their implementation and compared with Adam to train such models. Figure 2\nshows that Adam makes faster progress in terms of both the number of iterations and wall-clock\ntime. Due to the cost of updating curvature information, SFO is 5-10x slower per iteration com-\npared to Adam, and has a memory requirement that is linear in the number minibatches.\nStochastic regularization methods, such as dropout, are an effective way to prevent over-\ufb01tting and\noften used in practice due to their simplicity. SFO assumes deterministic subfunctions, and indeed\nfailed to converge on cost functions with stochastic regularization. We compare the effectiveness of\nAdam to other stochastic \ufb01rst order methods on multi-layer neural networks trained with dropout\nnoise. Figure 2 shows our results; Adam shows better convergence than other methods.\n6.3\nEXPERIMENT: CONVOLUTIONAL NEURAL NETWORKS\nConvolutional neural networks (CNNs) with several layers of convolution, pooling and non-linear\nunits have shown considerable success in computer vision tasks. Unlike most fully connected neural\nnets, weight sharing in CNNs results in vastly different gradients in different layers. A smaller\nlearning rate for the convolution layers is often used in practice when applying SGD. We show the\neffectiveness of Adam in deep CNNs. Our CNN architecture has three alternating stages of 5x5\nconvolution \ufb01lters and 3x3 max pooling with stride of 2 that are followed by a fully connected layer\nof 1000 recti\ufb01ed linear hidden units (ReLU\u2019s). The input image are pre-processed by whitening, and\n6\nPublished as a conference paper at ICLR 2015\n0\n50\n100\n150\n200\niterations over entire dataset\n10\n-2\n10\n-1\ntraining cost\nMNIST Multilayer Neural Network + dropout\nAdaGrad\nRMSProp\nSGDNesterov\nAdaDelta\nAdam\n(a)\n(b)\nFigure 2: Training of multilayer neural networks on MNIST images. (a) Neural networks using\ndropout stochastic regularization. (b) Neural networks with deterministic cost function. We compare\nwith the sum-of-functions (SFO) optimizer (Sohl-Dickstein et al., 2014)\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\niterations over entire dataset\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\ntraining cost\nCIFAR10 ConvNet First 3 Epoches\nAdaGrad\nAdaGrad+dropout\nSGDNesterov\nSGDNesterov+dropout\nAdam\nAdam+dropout\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\niterations over entire dataset\n10-4\n10-3\n10-2\n10-1\n100\n101\n102\ntraining cost\nCIFAR10 ConvNet\nAdaGrad\nAdaGrad+dropout\nSGDNesterov\nSGDNesterov+dropout\nAdam\nAdam+dropout\nFigure 3: Convolutional neural networks training cost. (left) Training cost for the \ufb01rst three epochs.\n(right) Training cost over 45 epochs. CIFAR-10 with c64-c64-c128-1000 architecture.\ndropout noise is applied to the input layer and fully connected layer. The minibatch size is also set\nto 128 similar to previous experiments.\nInterestingly, although both Adam and Adagrad make rapid progress lowering the cost in the initial\nstage of the training, shown in Figure 3 (left), Adam and SGD eventually converge considerably\nfaster than Adagrad for CNNs shown in Figure 3 (right). We notice the second moment estimate bvt\nvanishes to zeros after a few epochs and is dominated by the \u03f5 in algorithm 1. The second moment\nestimate is therefore a poor approximation to the geometry of the cost function in CNNs comparing\nto fully connected network from Section 6.2. Whereas, reducing the minibatch variance through\nthe \ufb01rst moment is more important in CNNs and contributes to the speed-up. As a result, Adagrad\nconverges much slower than others in this particular experiment. Though Adam shows marginal\nimprovement over SGD with momentum, it adapts learning rate scale for different layers instead of\nhand picking manually as in SGD.\n7\nPublished as a conference paper at ICLR 2015\n\u03b21=0\n\u03b21=0.9\n\u03b22=0.99\n\u03b22=0.999\n\u03b22=0.9999\n\u03b22=0.99\n\u03b22=0.999\n\u03b22=0.9999\n(a) after 10 epochs\n(b) after 100 epochs\nlog10(\u03b1)\nLoss\nFigure 4: Effect of bias-correction terms (red line) versus no bias correction terms (green line)\nafter 10 epochs (left) and 100 epochs (right) on the loss (y-axes) when learning a Variational Auto-\nEncoder (VAE) (Kingma & Welling, 2013), for different settings of stepsize \u03b1 (x-axes) and hyper-\nparameters \u03b21 and \u03b22.\n6.4\nEXPERIMENT: BIAS-CORRECTION TERM\nWe also empirically evaluate the effect of the bias correction terms explained in sections 2 and 3.\nDiscussed in section 5, removal of the bias correction terms results in a version of RMSProp (Tiele-\nman & Hinton, 2012) with momentum. We vary the \u03b21 and \u03b22 when training a variational auto-\nencoder (VAE) with the same architecture as in (Kingma & Welling, 2013) with a single hidden\nlayer with 500 hidden units with softplus nonlinearities and a 50-dimensional spherical Gaussian\nlatent variable. We iterated over a broad range of hyper-parameter choices, i.e. \u03b21 \u2208[0, 0.9] and\n\u03b22 \u2208[0.99, 0.999, 0.9999], and log10(\u03b1) \u2208[\u22125, ..., \u22121]. Values of \u03b22 close to 1, required for robust-\nness to sparse gradients, results in larger initialization bias; therefore we expect the bias correction\nterm is important in such cases of slow decay, preventing an adverse effect on optimization.\nIn Figure 4, values \u03b22 close to 1 indeed lead to instabilities in training when no bias correction term\nwas present, especially at \ufb01rst few epochs of the training. The best results were achieved with small\nvalues of (1\u2212\u03b22) and bias correction; this was more apparent towards the end of optimization when\ngradients tends to become sparser as hidden units specialize to speci\ufb01c patterns. In summary, Adam\nperformed equal or better than RMSProp, regardless of hyper-parameter setting.\n7\nEXTENSIONS\n7.1\nADAMAX\nIn Adam, the update rule for individual weights is to scale their gradients inversely proportional to a\n(scaled) L2 norm of their individual current and past gradients. We can generalize the L2 norm based\nupdate rule to a Lp norm based update rule. Such variants become numerically unstable for large\np. However, in the special case where we let p \u2192\u221e, a surprisingly simple and stable algorithm\nemerges; see algorithm 2. We\u2019ll now derive the algorithm. Let, in case of the Lp norm, the stepsize\nat time t be inversely proportional to v1/p\nt\n, where:\nvt = \u03b2p\n2vt\u22121 + (1 \u2212\u03b2p\n2)|gt|p\n(6)\n= (1 \u2212\u03b2p\n2)\nt\nX\ni=1\n\u03b2p(t\u2212i)\n2\n\u00b7 |gi|p\n(7)\n8\nPublished as a conference paper at ICLR 2015\nAlgorithm 2: AdaMax, a variant of Adam based on the in\ufb01nity norm. See section 7.1 for details.\nGood default settings for the tested machine learning problems are \u03b1 = 0.002, \u03b21 = 0.9 and\n\u03b22 = 0.999. With \u03b2t\n1 we denote \u03b21 to the power t. Here, (\u03b1/(1 \u2212\u03b2t\n1)) is the learning rate with the\nbias-correction term for the \ufb01rst moment. All operations on vectors are element-wise.\nRequire: \u03b1: Stepsize\nRequire: \u03b21, \u03b22 \u2208[0, 1): Exponential decay rates\nRequire: f(\u03b8): Stochastic objective function with parameters \u03b8\nRequire: \u03b80: Initial parameter vector\nm0 \u21900 (Initialize 1st moment vector)\nu0 \u21900 (Initialize the exponentially weighted in\ufb01nity norm)\nt \u21900 (Initialize timestep)\nwhile \u03b8t not converged do\nt \u2190t + 1\ngt \u2190\u2207\u03b8ft(\u03b8t\u22121) (Get gradients w.r.t. stochastic objective at timestep t)\nmt \u2190\u03b21 \u00b7 mt\u22121 + (1 \u2212\u03b21) \u00b7 gt (Update biased \ufb01rst moment estimate)\nut \u2190max(\u03b22 \u00b7 ut\u22121, |gt|) (Update the exponentially weighted in\ufb01nity norm)\n\u03b8t \u2190\u03b8t\u22121 \u2212(\u03b1/(1 \u2212\u03b2t\n1)) \u00b7 mt/ut (Update parameters)\nend while\nreturn \u03b8t (Resulting parameters)\nNote that the decay term is here equivalently parameterised as \u03b2p\n2 instead of \u03b22. Now let p \u2192\u221e,\nand de\ufb01ne ut = limp\u2192\u221e(vt)1/p, then:\nut = lim\np\u2192\u221e(vt)1/p = lim\np\u2192\u221e\n \n(1 \u2212\u03b2p\n2)\nt\nX\ni=1\n\u03b2p(t\u2212i)\n2\n\u00b7 |gi|p\n!1/p\n(8)\n= lim\np\u2192\u221e(1 \u2212\u03b2p\n2)1/p\n \nt\nX\ni=1\n\u03b2p(t\u2212i)\n2\n\u00b7 |gi|p\n!1/p\n(9)\n= lim\np\u2192\u221e\n \nt\nX\ni=1\n\u0010\n\u03b2(t\u2212i)\n2\n\u00b7 |gi|\n\u0011p\n!1/p\n(10)\n= max\n\u0000\u03b2t\u22121\n2\n|g1|, \u03b2t\u22122\n2\n|g2|, . . . , \u03b22|gt\u22121|, |gt|\n\u0001\n(11)\nWhich corresponds to the remarkably simple recursive formula:\nut = max(\u03b22 \u00b7 ut\u22121, |gt|)\n(12)\nwith initial value u0 = 0. Note that, conveniently enough, we don\u2019t need to correct for initialization\nbias in this case. Also note that the magnitude of parameter updates has a simpler bound with\nAdaMax than Adam, namely: |\u2206t| \u2264\u03b1.\n7.2\nTEMPORAL AVERAGING\nSince the last iterate is noisy due to stochastic approximation, better generalization performance is\noften achieved by averaging. Previously in Moulines & Bach (2011), Polyak-Ruppert averaging\n(Polyak & Juditsky, 1992; Ruppert, 1988) has been shown to improve the convergence of standard\nSGD, where \u00af\u03b8t = 1\nt\nPn\nk=1 \u03b8k. Alternatively, an exponential moving average over the parameters can\nbe used, giving higher weight to more recent parameter values. This can be trivially implemented\nby adding one line to the inner loop of algorithms 1 and 2: \u00af\u03b8t \u2190\u03b22 \u00b7 \u00af\u03b8t\u22121 +(1\u2212\u03b22)\u03b8t, with \u00af\u03b80 = 0.\nInitalization bias can again be corrected by the estimator b\u03b8t = \u00af\u03b8t/(1 \u2212\u03b2t\n2).\n8\nCONCLUSION\nWe have introduced a simple and computationally ef\ufb01cient algorithm for gradient-based optimiza-\ntion of stochastic objective functions. Our method is aimed towards machine learning problems with\n9\nPublished as a conference paper at ICLR 2015\nlarge datasets and/or high-dimensional parameter spaces. The method combines the advantages of\ntwo recently popular optimization methods: the ability of AdaGrad to deal with sparse gradients,\nand the ability of RMSProp to deal with non-stationary objectives. The method is straightforward\nto implement and requires little memory. The experiments con\ufb01rm the analysis on the rate of con-\nvergence in convex problems. Overall, we found Adam to be robust and well-suited to a wide range\nof non-convex optimization problems in the \ufb01eld machine learning.\n9\nACKNOWLEDGMENTS\nThis paper would probably not have existed without the support of Google Deepmind. We would\nlike to give special thanks to Ivo Danihelka, and Tom Schaul for coining the name Adam. Thanks to\nKai Fan from Duke University for spotting an error in the original AdaMax derivation. Experiments\nin this work were partly carried out on the Dutch national e-infrastructure with the support of SURF\nFoundation. Diederik Kingma is supported by the Google European Doctorate Fellowship in Deep\nLearning.\nREFERENCES\nAmari, Shun-Ichi. Natural gradient works ef\ufb01ciently in learning. Neural computation, 10(2):251\u2013276, 1998.\nDeng, Li, Li, Jinyu, Huang, Jui-Ting, Yao, Kaisheng, Yu, Dong, Seide, Frank, Seltzer, Michael, Zweig, Geoff,\nHe, Xiaodong, Williams, Jason, et al. Recent advances in deep learning for speech research at microsoft.\nICASSP 2013, 2013.\nDuchi, John, Hazan, Elad, and Singer, Yoram. Adaptive subgradient methods for online learning and stochastic\noptimization. The Journal of Machine Learning Research, 12:2121\u20132159, 2011.\nGraves, Alex. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.\nGraves, Alex, Mohamed, Abdel-rahman, and Hinton, Geoffrey. Speech recognition with deep recurrent neural\nnetworks. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on,\npp. 6645\u20136649. IEEE, 2013.\nHinton, G.E. and Salakhutdinov, R.R. Reducing the dimensionality of data with neural networks. Science, 313\n(5786):504\u2013507, 2006.\nHinton, Geoffrey, Deng, Li, Yu, Dong, Dahl, George E, Mohamed, Abdel-rahman, Jaitly, Navdeep, Senior,\nAndrew, Vanhoucke, Vincent, Nguyen, Patrick, Sainath, Tara N, et al. Deep neural networks for acoustic\nmodeling in speech recognition: The shared views of four research groups. Signal Processing Magazine,\nIEEE, 29(6):82\u201397, 2012a.\nHinton, Geoffrey E, Srivastava, Nitish, Krizhevsky, Alex, Sutskever, Ilya, and Salakhutdinov, Ruslan R. Im-\nproving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580,\n2012b.\nKingma, Diederik P and Welling, Max. Auto-Encoding Variational Bayes. In The 2nd International Conference\non Learning Representations (ICLR), 2013.\nKrizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classi\ufb01cation with deep convolutional\nneural networks. In Advances in neural information processing systems, pp. 1097\u20131105, 2012.\nMaas, Andrew L, Daly, Raymond E, Pham, Peter T, Huang, Dan, Ng, Andrew Y, and Potts, Christopher.\nLearning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association\nfor Computational Linguistics: Human Language Technologies-Volume 1, pp. 142\u2013150. Association for\nComputational Linguistics, 2011.\nMoulines, Eric and Bach, Francis R.\nNon-asymptotic analysis of stochastic approximation algorithms for\nmachine learning. In Advances in Neural Information Processing Systems, pp. 451\u2013459, 2011.\nPascanu, Razvan and Bengio, Yoshua.\nRevisiting natural gradient for deep networks.\narXiv preprint\narXiv:1301.3584, 2013.\nPolyak, Boris T and Juditsky, Anatoli B. Acceleration of stochastic approximation by averaging. SIAM Journal\non Control and Optimization, 30(4):838\u2013855, 1992.\n10\nPublished as a conference paper at ICLR 2015\nRoux, Nicolas L and Fitzgibbon, Andrew W. A fast natural newton method. In Proceedings of the 27th\nInternational Conference on Machine Learning (ICML-10), pp. 623\u2013630, 2010.\nRuppert, David. Ef\ufb01cient estimations from a slowly convergent robbins-monro process. Technical report,\nCornell University Operations Research and Industrial Engineering, 1988.\nSchaul, Tom, Zhang, Sixin, and LeCun, Yann. No more pesky learning rates. arXiv preprint arXiv:1206.1106,\n2012.\nSohl-Dickstein, Jascha, Poole, Ben, and Ganguli, Surya. Fast large-scale optimization by unifying stochas-\ntic gradient and quasi-newton methods. In Proceedings of the 31st International Conference on Machine\nLearning (ICML-14), pp. 604\u2013612, 2014.\nSutskever, Ilya, Martens, James, Dahl, George, and Hinton, Geoffrey. On the importance of initialization and\nmomentum in deep learning. In Proceedings of the 30th International Conference on Machine Learning\n(ICML-13), pp. 1139\u20131147, 2013.\nTieleman, T. and Hinton, G. Lecture 6.5 - RMSProp, COURSERA: Neural Networks for Machine Learning.\nTechnical report, 2012.\nWang, Sida and Manning, Christopher. Fast dropout training. In Proceedings of the 30th International Confer-\nence on Machine Learning (ICML-13), pp. 118\u2013126, 2013.\nZeiler, Matthew D. Adadelta: An adaptive learning rate method. arXiv preprint arXiv:1212.5701, 2012.\nZinkevich, Martin. Online convex programming and generalized in\ufb01nitesimal gradient ascent. 2003.\n11\nPublished as a conference paper at ICLR 2015\n10\nAPPENDIX\n10.1\nCONVERGENCE PROOF\nDe\ufb01nition 10.1. A function f : Rd \u2192R is convex if for all x, y \u2208Rd, for all \u03bb \u2208[0, 1],\n\u03bbf(x) + (1 \u2212\u03bb)f(y) \u2265f(\u03bbx + (1 \u2212\u03bb)y)\nAlso, notice that a convex function can be lower bounded by a hyperplane at its tangent.\nLemma 10.2. If a function f : Rd \u2192R is convex, then for all x, y \u2208Rd,\nf(y) \u2265f(x) + \u2207f(x)T (y \u2212x)\nThe above lemma can be used to upper bound the regret and our proof for the main theorem is\nconstructed by substituting the hyperplane with the Adam update rules.\nThe following two lemmas are used to support our main theorem. We also use some de\ufb01nitions sim-\nplify our notation, where gt \u225c\u2207ft(\u03b8t) and gt,i as the ith element. We de\ufb01ne g1:t,i \u2208Rt as a vector\nthat contains the ith dimension of the gradients over all iterations till t, g1:t,i = [g1,i, g2,i, \u00b7 \u00b7 \u00b7 , gt,i]\nLemma 10.3. Let gt = \u2207ft(\u03b8t) and g1:t be de\ufb01ned as above and bounded, \u2225gt\u22252 \u2264G, \u2225gt\u2225\u221e\u2264\nG\u221e. Then,\nT\nX\nt=1\ns\ng2\nt,i\nt\n\u22642G\u221e\u2225g1:T,i\u22252\nProof. We will prove the inequality using induction over T.\nThe base case for T = 1, we have\nq\ng2\n1,i \u22642G\u221e\u2225g1,i\u22252.\nFor the inductive step,\nT\nX\nt=1\ns\ng2\nt,i\nt\n=\nT \u22121\nX\nt=1\ns\ng2\nt,i\nt\n+\ns\ng2\nT,i\nT\n\u22642G\u221e\u2225g1:T \u22121,i\u22252 +\ns\ng2\nT,i\nT\n= 2G\u221e\nq\n\u2225g1:T,i\u22252\n2 \u2212g2\nT +\ns\ng2\nT,i\nT\nFrom, \u2225g1:T,i\u22252\n2 \u2212g2\nT,i +\ng4\nT,i\n4\u2225g1:T,i\u22252\n2 \u2265\u2225g1:T,i\u22252\n2 \u2212g2\nT,i, we can take square root of both side and\nhave,\nq\n\u2225g1:T,i\u22252\n2 \u2212g2\nT,i \u2264\u2225g1:T,i\u22252 \u2212\ng2\nT,i\n2\u2225g1:T,i\u22252\n\u2264\u2225g1:T,i\u22252 \u2212\ng2\nT,i\n2\np\nTG2\u221e\nRearrange the inequality and substitute the\nq\n\u2225g1:T,i\u22252\n2 \u2212g2\nT,i term,\nG\u221e\nq\n\u2225g1:T,i\u22252\n2 \u2212g2\nT +\ns\ng2\nT,i\nT\n\u22642G\u221e\u2225g1:T,i\u22252\n12\nPublished as a conference paper at ICLR 2015\nLemma 10.4. Let \u03b3 \u225c\n\u03b22\n1\n\u221a\u03b22 . For \u03b21, \u03b22 \u2208[0, 1) that satisfy\n\u03b22\n1\n\u221a\u03b22 < 1 and bounded gt, \u2225gt\u22252 \u2264G,\n\u2225gt\u2225\u221e\u2264G\u221e, the following inequality holds\nT\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n\u2264\n2\n1 \u2212\u03b3\n1\n\u221a1 \u2212\u03b22\n\u2225g1:T,i\u22252\nProof. Under the assumption,\n\u221a\n1\u2212\u03b2t\n2\n(1\u2212\u03b2t\n1)2 \u2264\n1\n(1\u2212\u03b21)2 . We can expand the last term in the summation\nusing the update rules in Algorithm 1,\nT\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n=\nT \u22121\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n+\np\n1 \u2212\u03b2T\n2\n(1 \u2212\u03b2T\n1 )2\n(PT\nk=1(1 \u2212\u03b21)\u03b2T \u2212k\n1\ngk,i)2\nq\nT PT\nj=1(1 \u2212\u03b22)\u03b2T \u2212j\n2\ng2\nj,i\n\u2264\nT \u22121\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n+\np\n1 \u2212\u03b2T\n2\n(1 \u2212\u03b2T\n1 )2\nT\nX\nk=1\nT((1 \u2212\u03b21)\u03b2T \u2212k\n1\ngk,i)2\nq\nT PT\nj=1(1 \u2212\u03b22)\u03b2T \u2212j\n2\ng2\nj,i\n\u2264\nT \u22121\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n+\np\n1 \u2212\u03b2T\n2\n(1 \u2212\u03b2T\n1 )2\nT\nX\nk=1\nT((1 \u2212\u03b21)\u03b2T \u2212k\n1\ngk,i)2\nq\nT(1 \u2212\u03b22)\u03b2T \u2212k\n2\ng2\nk,i\n\u2264\nT \u22121\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n+\np\n1 \u2212\u03b2T\n2\n(1 \u2212\u03b2T\n1 )2\n(1 \u2212\u03b21)2\np\nT(1 \u2212\u03b22)\nT\nX\nk=1\nT\n\u0012 \u03b22\n1\n\u221a\u03b22\n\u0013T \u2212k\n\u2225gk,i\u22252\n\u2264\nT \u22121\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n+\nT\np\nT(1 \u2212\u03b22)\nT\nX\nk=1\n\u03b3T \u2212k\u2225gk,i\u22252\nSimilarly, we can upper bound the rest of the terms in the summation.\nT\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n\u2264\nT\nX\nt=1\n\u2225gt,i\u22252\np\nt(1 \u2212\u03b22)\nT \u2212t\nX\nj=0\nt\u03b3j\n\u2264\nT\nX\nt=1\n\u2225gt,i\u22252\np\nt(1 \u2212\u03b22)\nT\nX\nj=0\nt\u03b3j\nFor \u03b3 < 1, using the upper bound on the arithmetic-geometric series, P\nt t\u03b3t <\n1\n(1\u2212\u03b3)2 :\nT\nX\nt=1\n\u2225gt,i\u22252\np\nt(1 \u2212\u03b22)\nT\nX\nj=0\nt\u03b3j \u2264\n1\n(1 \u2212\u03b3)2\u221a1 \u2212\u03b22\nT\nX\nt=1\n\u2225gt,i\u22252\n\u221a\nt\nApply Lemma 10.3,\nT\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n\u2264\n2G\u221e\n(1 \u2212\u03b3)2\u221a1 \u2212\u03b22\n\u2225g1:T,i\u22252\nTo simplify the notation, we de\ufb01ne \u03b3 \u225c\n\u03b22\n1\n\u221a\u03b22 . Intuitively, our following theorem holds when the\nlearning rate \u03b1t is decaying at a rate of t\u22121\n2 and \ufb01rst moment running average coef\ufb01cient \u03b21,t decay\nexponentially with \u03bb, that is typically close to 1, e.g. 1 \u221210\u22128.\nTheorem 10.5. Assume that the function ft has bounded gradients, \u2225\u2207ft(\u03b8)\u22252 \u2264G, \u2225\u2207ft(\u03b8)\u2225\u221e\u2264\nG\u221efor all \u03b8 \u2208Rd and distance between any \u03b8t generated by Adam is bounded, \u2225\u03b8n \u2212\u03b8m\u22252 \u2264D,\n13\nPublished as a conference paper at ICLR 2015\n\u2225\u03b8m \u2212\u03b8n\u2225\u221e\u2264D\u221efor any m, n \u2208{1, ..., T}, and \u03b21, \u03b22 \u2208[0, 1) satisfy\n\u03b22\n1\n\u221a\u03b22 < 1. Let \u03b1t =\n\u03b1\n\u221a\nt\nand \u03b21,t = \u03b21\u03bbt\u22121, \u03bb \u2208(0, 1). Adam achieves the following guarantee, for all T \u22651.\nR(T) \u2264\nD2\n2\u03b1(1 \u2212\u03b21)\nd\nX\ni=1\np\nTbvT,i+\n\u03b1(\u03b21 + 1)G\u221e\n(1 \u2212\u03b21)\u221a1 \u2212\u03b22(1 \u2212\u03b3)2\nd\nX\ni=1\n\u2225g1:T,i\u22252+\nd\nX\ni=1\nD2\n\u221eG\u221e\n\u221a1 \u2212\u03b22\n2\u03b1(1 \u2212\u03b21)(1 \u2212\u03bb)2\nProof. Using Lemma 10.2, we have,\nft(\u03b8t) \u2212ft(\u03b8\u2217) \u2264gT\nt (\u03b8t \u2212\u03b8\u2217) =\nd\nX\ni=1\ngt,i(\u03b8t,i \u2212\u03b8\u2217\n,i)\nFrom the update rules presented in algorithm 1,\n\u03b8t+1 = \u03b8t \u2212\u03b1t bmt/\np\nbvt\n= \u03b8t \u2212\n\u03b1t\n1 \u2212\u03b2t\n1\n\u0012 \u03b21,t\n\u221abvt\nmt\u22121 + (1 \u2212\u03b21,t)\n\u221abvt\ngt\n\u0013\nWe focus on the ith dimension of the parameter vector \u03b8t \u2208Rd. Subtract the scalar \u03b8\u2217\n,i and square\nboth sides of the above update rule, we have,\n(\u03b8t+1,i \u2212\u03b8\u2217\n,i)2 =(\u03b8t,i \u2212\u03b8\u2217\n,i)2 \u2212\n2\u03b1t\n1 \u2212\u03b2t\n1\n( \u03b21,t\np\nbvt,i\nmt\u22121,i + (1 \u2212\u03b21,t)\np\nbvt,i\ngt,i)(\u03b8t,i \u2212\u03b8\u2217\n,i) + \u03b12\nt ( bmt,i\np\nbvt,i\n)2\nWe can rearrange the above equation and use Young\u2019s inequality, ab \u2264a2/2 + b2/2. Also, it can be\nshown that\np\nbvt,i =\nqPt\nj=1(1 \u2212\u03b22)\u03b2t\u2212j\n2\ng2\nj,i/\np\n1 \u2212\u03b2t\n2 \u2264\u2225g1:t,i\u22252 and \u03b21,t \u2264\u03b21. Then\ngt,i(\u03b8t,i \u2212\u03b8\u2217\n,i) =(1 \u2212\u03b2t\n1)\np\nbvt,i\n2\u03b1t(1 \u2212\u03b21,t)\n\u0012\n(\u03b8t,i \u2212\u03b8\u2217\n,t)2 \u2212(\u03b8t+1,i \u2212\u03b8\u2217\n,i)2\n\u0013\n+\n\u03b21,t\n(1 \u2212\u03b21,t)\nbv\n1\n4\nt\u22121,i\n\u221a\u03b1t\u22121\n(\u03b8\u2217\n,i \u2212\u03b8t,i)\u221a\u03b1t\u22121\nmt\u22121,i\nbv\n1\n4\nt\u22121,i\n+ \u03b1t(1 \u2212\u03b2t\n1)\np\nbvt,i\n2(1 \u2212\u03b21,t)\n( bmt,i\np\nbvt,i\n)2\n\u2264\n1\n2\u03b1t(1 \u2212\u03b21)\n\u0012\n(\u03b8t,i \u2212\u03b8\u2217\n,t)2 \u2212(\u03b8t+1,i \u2212\u03b8\u2217\n,i)2\n\u0013p\nbvt,i +\n\u03b21,t\n2\u03b1t\u22121(1 \u2212\u03b21,t)(\u03b8\u2217\n,i \u2212\u03b8t,i)2p\nbvt\u22121,i\n+\n\u03b21\u03b1t\u22121\n2(1 \u2212\u03b21)\nm2\nt\u22121,i\np\nbvt\u22121,i\n+\n\u03b1t\n2(1 \u2212\u03b21)\nbm2\nt,i\np\nbvt,i\nWe apply Lemma 10.4 to the above inequality and derive the regret bound by summing across all\nthe dimensions for i \u22081, ..., d in the upper bound of ft(\u03b8t) \u2212ft(\u03b8\u2217) and the sequence of convex\nfunctions for t \u22081, ..., T:\nR(T) \u2264\nd\nX\ni=1\n1\n2\u03b11(1 \u2212\u03b21)(\u03b81,i \u2212\u03b8\u2217\n,i)2p\nbv1,i +\nd\nX\ni=1\nT\nX\nt=2\n1\n2(1 \u2212\u03b21)(\u03b8t,i \u2212\u03b8\u2217\n,i)2(\np\nbvt,i\n\u03b1t\n\u2212\np\nbvt\u22121,i\n\u03b1t\u22121\n)\n+\n\u03b21\u03b1G\u221e\n(1 \u2212\u03b21)\u221a1 \u2212\u03b22(1 \u2212\u03b3)2\nd\nX\ni=1\n\u2225g1:T,i\u22252 +\n\u03b1G\u221e\n(1 \u2212\u03b21)\u221a1 \u2212\u03b22(1 \u2212\u03b3)2\nd\nX\ni=1\n\u2225g1:T,i\u22252\n+\nd\nX\ni=1\nT\nX\nt=1\n\u03b21,t\n2\u03b1t(1 \u2212\u03b21,t)(\u03b8\u2217\n,i \u2212\u03b8t,i)2p\nbvt,i\n14\nPublished as a conference paper at ICLR 2015\nFrom the assumption, \u2225\u03b8t \u2212\u03b8\u2217\u22252 \u2264D, \u2225\u03b8m \u2212\u03b8n\u2225\u221e\u2264D\u221e, we have:\nR(T) \u2264\nD2\n2\u03b1(1 \u2212\u03b21)\nd\nX\ni=1\np\nTbvT,i +\n\u03b1(1 + \u03b21)G\u221e\n(1 \u2212\u03b21)\u221a1 \u2212\u03b22(1 \u2212\u03b3)2\nd\nX\ni=1\n\u2225g1:T,i\u22252 + D2\n\u221e\n2\u03b1\nd\nX\ni=1\nt\nX\nt=1\n\u03b21,t\n(1 \u2212\u03b21,t)\np\ntbvt,i\n\u2264\nD2\n2\u03b1(1 \u2212\u03b21)\nd\nX\ni=1\np\nTbvT,i +\n\u03b1(1 + \u03b21)G\u221e\n(1 \u2212\u03b21)\u221a1 \u2212\u03b22(1 \u2212\u03b3)2\nd\nX\ni=1\n\u2225g1:T,i\u22252\n+ D2\n\u221eG\u221e\n\u221a1 \u2212\u03b22\n2\u03b1\nd\nX\ni=1\nt\nX\nt=1\n\u03b21,t\n(1 \u2212\u03b21,t)\n\u221a\nt\nWe can use arithmetic geometric series upper bound for the last term:\nt\nX\nt=1\n\u03b21,t\n(1 \u2212\u03b21,t)\n\u221a\nt \u2264\nt\nX\nt=1\n1\n(1 \u2212\u03b21)\u03bbt\u22121\u221a\nt\n\u2264\nt\nX\nt=1\n1\n(1 \u2212\u03b21)\u03bbt\u22121t\n\u2264\n1\n(1 \u2212\u03b21)(1 \u2212\u03bb)2\nTherefore, we have the following regret bound:\nR(T) \u2264\nD2\n2\u03b1(1 \u2212\u03b21)\nd\nX\ni=1\np\nTbvT,i +\n\u03b1(1 + \u03b21)G\u221e\n(1 \u2212\u03b21)\u221a1 \u2212\u03b22(1 \u2212\u03b3)2\nd\nX\ni=1\n\u2225g1:T,i\u22252 +\nd\nX\ni=1\nD2\n\u221eG\u221e\n\u221a1 \u2212\u03b22\n2\u03b1\u03b21(1 \u2212\u03bb)2\n15\n",
        "sentence": " All networks were trained using Adam [27], with the learning rate set to the highest value that allowed the model to successfully converge to a sensible attention policy.",
        "context": "models, including logistic regression, multilayer fully connected neural networks and deep convolu-\ntional neural networks. Using large models and datasets, we demonstrate Adam can ef\ufb01ciently solve\npractical deep learning problems.\nlearning rate for the convolution layers is often used in practice when applying SGD. We show the\neffectiveness of Adam in deep CNNs. Our CNN architecture has three alternating stages of 5x5\nmethods for a variety of models and datasets, as shown in section 6. Overall, we show that Adam is\na versatile algorithm that scales to large-scale high-dimensional machine learning problems.\n2\nALGORITHM"
    },
    {
        "title": "Deep visual-semantic alignments for generating image descriptions",
        "author": [
            "Andrej Karpathy",
            "Li Fei-Fei"
        ],
        "venue": "arXiv preprint arXiv:1412.2306,",
        "citeRegEx": "28",
        "shortCiteRegEx": "28",
        "year": 2014,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " The training/valid/test split followed the same protocol as used in previous work [28].",
        "context": null
    }
]