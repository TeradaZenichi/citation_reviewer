[
    {
        "title": "Joint language and translation modeling with recurrent neural networks",
        "author": [
            "Michael Auli",
            "Michel Galley",
            "Chris Quirk",
            "Geoffrey Zweig"
        ],
        "venue": "In EMNLP,",
        "citeRegEx": "2",
        "shortCiteRegEx": "2",
        "year": 2013,
        "abstract": "",
        "full_text": "",
        "sentence": " Their RNN model uses the word embeddings of Mikolov [2] and a lattice representation of the decoder output to facilitate search over the space of possible translations.",
        "context": null
    },
    {
        "title": "The principled design of large-scale recursive neural network architectures\u2013DAG-RNNs and the protein structure prediction problem",
        "author": [
            "Pierre Baldi",
            "Gianluca Pollastri"
        ],
        "venue": "The Journal of Machine Learning Research,",
        "citeRegEx": "3",
        "shortCiteRegEx": "3",
        "year": 2003,
        "abstract": "",
        "full_text": "",
        "sentence": " They have been used successfully on non-temporal sequence data, including genetic data [3].",
        "context": null
    },
    {
        "title": "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization",
        "author": [
            "Satanjeev Banerjee",
            "Alon Lavie"
        ],
        "venue": null,
        "citeRegEx": "4",
        "shortCiteRegEx": "4",
        "year": 2005,
        "abstract": "",
        "full_text": "",
        "sentence": " METEOR, introduced in 2005 by Banerjee and Lavie is an alternative metric intended to overcome these weaknesses of the BLEU score [4].",
        "context": null
    },
    {
        "title": "Evolving memory cell structures for sequence learning",
        "author": [
            "Justin Bayer",
            "Daan Wierstra",
            "Julian Togelius",
            "J\u00fcrgen Schmidhuber"
        ],
        "venue": "In Artificial Neural Networks\u2013ICANN",
        "citeRegEx": "5",
        "shortCiteRegEx": "5",
        "year": 2009,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Several recent papers explore genetic algorithms for neural networks, especially as means of learning the architecture of neural networks, a problem not addressed by backpropagation ([5], [28]).",
        "context": null
    },
    {
        "title": "Evolving networks: Using the genetic algorithm with connectionist learning",
        "author": [
            "Richard K Belew",
            "John McInerney",
            "Nicol N Schraudolph"
        ],
        "venue": "In In. Citeseer,",
        "citeRegEx": "6",
        "shortCiteRegEx": "6",
        "year": 1990,
        "abstract": "",
        "full_text": "",
        "sentence": " A number of papers from the 1990s ([6], [27]) championed the idea of learning neural networks with genetic algorithms with some even claiming that achieving success on real-world problems by applying many small changes to a network\u2019s weights was impossible.",
        "context": null
    },
    {
        "title": "Advances in optimizing recurrent networks",
        "author": [
            "Yoshua Bengio",
            "Nicolas Boulanger-Lewandowski",
            "Razvan Pascanu"
        ],
        "venue": "In Acoustics, Speech and Signal Processing (ICASSP),",
        "citeRegEx": "7",
        "shortCiteRegEx": "7",
        "year": 2013,
        "abstract": "After a more than decade-long period of relatively little research activity\nin the area of recurrent neural networks, several new developments will be\nreviewed here that have allowed substantial progress both in understanding and\nin technical solutions towards more efficient training of recurrent networks.\nThese advances have been motivated by and related to the optimization issues\nsurrounding deep learning. Although recurrent networks are extremely powerful\nin what they can in principle represent in terms of modelling sequences,their\ntraining is plagued by two aspects of the same issue regarding the learning of\nlong-term dependencies. Experiments reported here evaluate the use of clipping\ngradients, spanning longer time ranges with leaky integration, advanced\nmomentum techniques, using more powerful output probability models, and\nencouraging sparser gradients to help symmetry breaking and credit assignment.\nThe experiments are performed on text and music data and show off the combined\neffects of these techniques in generally improving both training and test\nerror.",
        "full_text": "arXiv:1212.0901v2  [cs.LG]  14 Dec 2012\nADVANCES IN OPTIMIZING RECURRENT NETWORKS\nYoshua Bengio, Nicolas Boulanger-Lewandowski and Razvan Pascanu\nU. Montreal\nABSTRACT\nAfter a more than decade-long period of relatively little research ac-\ntivity in the area of recurrent neural networks, several new develop-\nments will be reviewed here that have allowed substantial progress\nboth in understanding and in technical solutions towards more ef\ufb01-\ncient training of recurrent networks. These advances have been mo-\ntivated by and related to the optimization issues surrounding deep\nlearning. Although recurrent networks are extremely powerful in\nwhat they can in principle represent in terms of modeling sequences,\ntheir training is plagued by two aspects of the same issue regarding\nthe learning of long-term dependencies. Experiments reported here\nevaluate the use of clipping gradients, spanning longer time ranges\nwith leaky integration, advanced momentum techniques, using more\npowerful output probability models, and encouraging sparser gra-\ndients to help symmetry breaking and credit assignment. The ex-\nperiments are performed on text and music data and show off the\ncombined effects of these techniques in generally improving both\ntraining and test error.\nIndex Terms\u2014 Recurrent networks, deep learning, representa-\ntion learning, long-term dependencies\n1. INTRODUCTION\nMachine learning algorithms for capturing statistical structure in se-\nquential data face a fundamental problem [1, 2], called the dif\ufb01culty\nof learning long-term dependencies.\nIf the operations performed\nwhen forming a \ufb01xed-size summary of relevant past observations\n(for the purpose of predicting some future observations) are linear,\nthis summary must exponentially forget past events that are further\naway, to maintain stability. On the other hand, if they are non-linear,\nthen this non-linearity is composed many times, yielding a highly\nnon-linear relationship between past events and future events. Learn-\ning such non-linear relationships turns out to be dif\ufb01cult, for reasons\nthat are discussed here, along with recent proposals for reducing this\ndif\ufb01culty.\nRecurrent neural networks [3] can represent such non-linear\nmaps (F, below) that iteratively build a relevant summary of past\nobservations.\nIn their simplest form, recurrent neural networks\n(RNNs) form a deterministic state variable ht as a function of the\npresent input observation xt and the past value(s) of the state vari-\nable, e.g., ht = F\u03b8(ht\u22121, xt), where \u03b8 are tunable parameters that\ncontrol what will be remembered about the past sequence and what\nwill be discarded. Depending on the type of problem at hand, a loss\nfunction L(ht, yt) is de\ufb01ned, with yt an observed random variable\nat time t and Ct = L(ht, yt) the cost at time t. The generalization\nobjective is to minimize the expected future cost, and the training\nobjective involves the average of Ct over observed sequences. In\nprinciple, RNNs can be trained by gradient-based optimization pro-\ncedures (using the back-propagation algorithm [3] to compute a\ngradient), but it was observed early on [1, 2] that capturing depen-\ndencies that span a long interval was dif\ufb01cult, making the task of\noptimizing \u03b8 to minimize the average of Ct\u2019s almost impossible for\nsome tasks when the span of the dependencies of interest increases\nsuf\ufb01ciently. More precisely, using a local numerical optimization\nsuch as stochastic gradient descent or second order methods (which\ngradually improve the solution), the proportion of trials (differing\nonly from their random initialization) falling into the basin of at-\ntraction of a good enough solution quickly becomes very small as\nthe temporal span of dependencies is increased (beyond tens or\nhundreds of steps, depending of the task).\nThese dif\ufb01culties are probably responsible for the major reduc-\ntion in research efforts in the area of RNNs in the 90\u2019s and 2000\u2019s.\nHowever, a revival of interest in these learning algorithms is taking\nplace, in particular thanks to [4] and [5]. This paper studies the is-\nsues giving rise to these dif\ufb01culties and discusses, reviews, and com-\nbines several techniques that have been proposed in order to improve\ntraining of RNNs, following up on a recent thesis devoted to the sub-\nject [6]. We \ufb01nd that these techniques generally help generalization\nperformance as well as training performance, which suggest they\nhelp to improve the optimization of the training criterion. We also\n\ufb01nd that although these techniques can be applied in the online set-\nting, i.e., as add-ons to stochastic gradient descent (SGD), they allow\nto compete with batch (or large minibatch) second-order methods\nsuch as Hessian-Free optimization, recently found to greatly help\ntraining of RNNs [4].\n2. LEARNING LONG-TERM DEPENDENCIES AND THE\nOPTIMIZATION DIFFICULTY WITH DEEP LEARNING\nThere has been several breakthroughs in recent years in the algo-\nrithms and results obtained with so-called deep learning algorithms\n(see [7] and [8] for reviews). Deep learning algorithms discover\nmultiple levels of representation, typically as deep neural networks\nor graphical models organized with many levels of representation-\ncarrying latent variables.\nVery little work on deep architectures\noccurred before the major advances of 2006 [9, 10, 11], probably\nbecause of optimization dif\ufb01culties due to the high level of non-\nlinearity in deeper networks (whose output is the composition of\nthe non-linearity at each layer). Some experiments [12] showed the\npresence of an extremely large number of apparent local minima of\nthe training criterion, with no two different initializations going to\nthe same function (i.e. eliminating the effect of permutations and\nother symmetries of parametrization giving rise to the same func-\ntion). Furthermore, qualitatively different initialization (e.g., using\nunsupervised learning) could yield models in completely different\nregions of function space. An unresolved question is whether these\ndif\ufb01culties are actually due to local minima or to ill-conditioning\n(which makes gradient descent converge so slowly as to appear\nstuck in a local minimum). Some ill-conditioning has clearly been\nshown to be involved, especially for the dif\ufb01cult problem of training\ndeep auto-encoders, through comparisons [13] of stochastic gradient\ndescent and Hessian-free optimization (a second order optimiza-\ntion method).\nThese optimization questions become particularly\nimportant when trying to train very large networks on very large\ndatasets [14], where one realizes that a major challenge for deep\nlearning is the under\ufb01tting issue. Of course one can trivially over\ufb01t\nby increasing capacity in the wrong places (e.g. in the output layer),\nbut what we are trying to achieve is learning of more powerful\nrepresentations in order to also get good generalization.\nThe same questions can be asked for RNNs. When the compu-\ntations performed by a RNN are unfolded through time, one clearly\nsees a deep neural network with shared weights (across the \u2019layers\u2019,\neach corresponding to a different time step), and with a cost function\nthat may depends on the output of intermediate layers. Hessian-free\noptimization has been successfully used to considerably extend the\nspan of temporal dependencies that a RNN can learn [4], suggest-\ning that ill-conditioning effects are also at play in the dif\ufb01culties of\ntraining RNN.\nAn important aspect of these dif\ufb01culties is that the gradient can\nbe decomposed [2, 15] into terms that involve products of Jacobians\n\u2202ht\n\u2202ht\u22121 over subsequences linking an event at time t1 and one at time\nt2:\n\u2202ht2\n\u2202ht1 = Qt2\n\u03c4=t1+1\n\u2202h\u03c4\n\u2202h\u03c4\u22121 . As t2 \u2212t1 increases, the products of\nt2 \u2212t1 of these Jacobian matrices tend to either vanish (when the\nleading eigenvalues of\n\u2202ht\n\u2202ht\u22121 are less than 1) or explode (when the\nleading eigenvalues of\n\u2202ht\n\u2202ht\u22121 are greater than 11). This is problem-\natic because the total gradient due to a loss Ct2 at time t2 is a sum\nwhose terms correspond to the effects at different time spans, which\nare weighted by\n\u2202ht2\n\u2202ht1 for different t1\u2019s:\n\u2202Ct2\n\u2202\u03b8\n=\nX\nt1\u2264t2\n\u2202Ct2\n\u2202ht2\n\u2202ht2\n\u2202ht1\n\u2202ht1\n\u2202\u03b8(t1)\nwhere\n\u2202ht1\n\u2202\u03b8(t1) is the derivative of ht1 with respect to the instantia-\ntion of the parameters \u03b8 at step t1, i.e., that directly come into the\ncomputation of ht1 in F. When the\n\u2202ht2\n\u2202ht1 tend to vanish for increas-\ning t2 \u2212t1, the long-term term effects become exponentially smaller\nin magnitude than the shorter-term ones, making it very dif\ufb01cult to\ncapture them. On the other hand, when\n\u2202ht2\n\u2202ht1 \u201cexplode\u201d (becomes\nlarge), gradient descent updates can be destructive (move to poor\ncon\ufb01guration of parameters). It is not that the gradient is wrong, it is\nthat gradient descent makes small but \ufb01nite steps \u2206\u03b8 yielding a \u2206C,\nwhereas the gradient measures the effect of \u2206C when \u2206\u03b8 \u21920. A\nmuch deeper discussion of this issue can be found in [15], along with\na point of view inspired by dynamical systems theory and by the ge-\nometrical aspect of the problem, having to do with the shape of the\ntraining criterion as a function of \u03b8 near those regions of exploding\ngradient. In particular, it is argued that the strong non-linearity oc-\ncurring where gradients explode is shaped like a cliff where not just\nthe \ufb01rst but also the second derivative becomes large in the direc-\ntion orthogonal to the cliff. Similarly, \ufb02atness of the cost function\noccurs simultaneously on the \ufb01rst and second derivatives. Hence di-\nviding the gradient by the second derivative in each direction (i.e.,\npre-multiplying by the inverse of some proxy for the Hessian ma-\ntrix) could in principle reduce the exploding and vanishing gradient\neffects, as argued in [4].\n1 Note that this is not a suf\ufb01cient condition, but a necessary one. Further\nmore one usually wants to operate in the regime where the leading eigenvalue\nis larger than 1 but the gradients do not explode.\n3. ADVANCES IN TRAINING RECURRENT NETWORKS\n3.1. Clipped Gradient\nTo address the exploding gradient effect, [16, 15] recently proposed\nto clip gradients above a given threshold. Under the hypothesis that\nthe explosion occurs in very small regions (the cliffs in cost func-\ntion mentioned above), most of the time this will have no effect, but\nit will avoid aberrant parameter changes in those cliff regions, while\nguaranteeing that the resulting updates are still in a descent direction.\nThe speci\ufb01c form of clipping used here was proposed in [15] and is\ndiscussed there at much greater length: when the norm of the gradi-\nent vector g for a given sequence is above a threshold, the update\nis done in the direction threshold\ng\n||g||. As argued in [15], this very\nsimple method implements a very simple form of second order opti-\nmization in the sense that the second derivative is also proportionally\nlarge in those exploding gradient regions.\n3.2. Spanning Longer Time Ranges with Leaky Integration\nAn old idea to reduce the effect of vanishing gradients is to intro-\nduce shorter paths between t1 and t2, either via connections with\nlonger time delays [17] or inertia (slow-changing units) in some of\nthe hidden units [18, 19], or both [20]. Long-Short-Term Mem-\nory (LSTM) networks [21], which were shown to be able to han-\ndle much longer range dependencies, also bene\ufb01t from a linearly\nself-connected memory unit with a near 1 self-weight which allows\nsignals (and gradients) to propagate over long time spans.\nA different interpretation to this slow-changing units is that they\nbehave like low-pass \ufb01lter and hence they can be used to focus cer-\ntain units on different frequency regions of the data. The analogy\ncan be brought one step further by introducing band-pass \ufb01lter units\n[22] or by using domain speci\ufb01c knowledge to decide on what fre-\nquency bands different units should focus. [23] shows that adding\nlow frequency information as an additional input to a recurrent net-\nwork helps improving the performance of the model.\nIn the experiments performed here, a subset of the units were\nforced to change slowly by using the following \u201cleaky integration\u201d\nstate-to-state map: ht,i = \u03b1iht\u22121,i + (1 \u2212\u03b1i)Fi(ht\u22121, xt). The\nstandard RNN corresponds to \u03b1i = 0, while here different values\nof \u03b1i were randomly sampled from (0.02, 0.2), allowing some units\nto react quickly while others are forced to change slowly, but also\npropagate signals and gradients further in time. Note that because\n\u03b1 < 1, the vanishing effect is still present (and gradients can still\nexplode via F), but the time-scale of the vanishing effect can be\nexpanded.\n3.3. Combining Recurrent Nets with a Powerful Output Proba-\nbility Model\nOne way to reduce the under\ufb01tting of RNNs is to introduce multi-\nplicative interactions in the parametrization of F, as was done suc-\ncessfully in [4]. When the output predictions are multivariate, an-\nother approach is to capture the high-order dependencies between\nthe output variables using a powerful output probability model such\nas a Restricted Boltzmann Machine (RBM) [24, 25] or a determinis-\ntic variant of it called NADE [26, 25]. In the experiments performed\nhere, we have experimented with a NADE output model for the mu-\nsic data.\n3.4. Sparser Gradients via Sparse Output Regularization and\nRecti\ufb01ed Outputs\n[7] hypothesized that one reason for the dif\ufb01culty in optimizing\ndeep networks is that in ordinary neural networks gradients diffuse\nthrough the layers, diffusing credit and blame through many units,\nmaybe making it dif\ufb01cult for hidden units to specialize.\nWhen\nthe gradient on hidden units is more sparse, one could imagine\nthat symmetries would be broken more easily and credit or blame\nassigned less uniformly. This is what was advocated in [27], ex-\nploiting the idea of recti\ufb01er non-linearities introduced earlier in\n[28], i.e., the neuron non-linearity is out = max(0, in) instead\nof out = tanh(in) or out = sigmoid(in). This approach was\nvery successful in recent work on deep learning for object recog-\nnition [29], beating by far the state-of-the-art on ImageNet (1000\nclasses). Here, we apply this deep learning idea to RNNs, using\nan L1 penalty on outputs of hidden units to promote sparsity of\nactivations. The underlying hypothesis is that if the gradient is con-\ncentrated in a few paths (in the unfolded computation graph of the\nRNN), it will reduce the vanishing gradients effect.\n3.5. Simpli\ufb01ed Nesterov Momentum\nNesterov accelerated gradient (NAG) [30] is a \ufb01rst-order optimiza-\ntion method to improve stability and convergence of regular gradient\ndescent. Recently, [6] showed that NAG could be computed by the\nfollowing update rules:\nvt = \u00b5t\u22121vt\u22121 \u2212\u01ebt\u22121\u2207f(\u03b8t\u22121 + \u00b5t\u22121vt\u22121)\n(1)\n\u03b8t = \u03b8t\u22121 + vt\n(2)\nwhere \u03b8t are the model parameters, vt the velocity, \u00b5t \u2208[0, 1] the\nmomentum (decay) coef\ufb01cient and \u01ebt > 0 the learning rate at it-\neration t, f(\u03b8) is the objective function and \u2207f(\u03b8\u2032) is a shorthand\nnotation for the gradient \u2202f(\u03b8)\n\u2202\u03b8 |\u03b8=\u03b8\u2032. These equations have a form\nsimilar to standard momentum updates:\nvt = \u00b5t\u22121vt\u22121 \u2212\u01ebt\u22121\u2207f(\u03b8t\u22121)\n(3)\n\u03b8t = \u03b8t\u22121 + vt\n(4)\n= \u03b8t\u22121 + \u00b5t\u22121vt\u22121 \u2212\u01ebt\u22121\u2207f(\u03b8t\u22121)\n(5)\nand differ only in the evaluation point of the gradient at each itera-\ntion. This important difference, thought to counterbalance too high\nvelocities by \u201cpeeking ahead\u201d actual objective values in the candi-\ndate search direction, results in signi\ufb01cantly improved RNN perfor-\nmance on a number of tasks.\nIn this section, we derive a new formulation of Nesterov mo-\nmentum differing from (3) and (5) only in the linear combination\ncoef\ufb01cients of the velocity and gradient contributions at each itera-\ntion, and we offer an alternative interpretation of the method. The\nkey departure from (1) and (2) resides in committing to the \u201cpeeked-\nahead\u201d parameters \u0398t\u22121 \u2261\u03b8t\u22121 + \u00b5t\u22121vt\u22121 and backtracking by\nthe same amount before each update. Our new parameters \u0398t up-\ndates become:\nvt = \u00b5t\u22121vt\u22121 \u2212\u01ebt\u22121\u2207f(\u0398t\u22121)\n(6)\n\u0398t = \u0398t\u22121 \u2212\u00b5t\u22121vt\u22121 + \u00b5tvt + vt\n= \u0398t\u22121 + \u00b5t\u00b5t\u22121vt\u22121 \u2212(1 + \u00b5t)\u01ebt\u22121\u2207f(\u0398t\u22121)\n(7)\nAssuming a zero initial velocity v1 = 0 and velocity at convergence\nof optimization vT \u22430, the parameters \u0398 are a completely equiva-\nlent replacement of \u03b8.\nNote that equation (7) is identical to regular momentum (5)\nwith different linear combination coef\ufb01cients. More precisely, for an\nequivalent velocity update (6), the velocity contribution to the new\nparameters \u00b5t\u00b5t\u22121 < \u00b5t is reduced relatively to the gradient con-\ntribution (1 + \u00b5t)\u01ebt\u22121 > \u01ebt\u22121. This allows storing past velocities\nfor a longer time with a higher \u00b5, while actually using those veloci-\nties more conservatively during the updates. We suspect this mecha-\nnism is a crucial ingredient for good empirical performance. While\nthe \u201cpeeking ahead\u201d point of view suggests that a similar strategy\ncould be adapted for regular gradient descent (misleadingly, because\nit would amount to a reduced learning rate \u01ebt), our derivation shows\nwhy it is important to choose search directions aligned with the cur-\nrent velocity to yield substantial improvement. The general case is\nalso simpler to implement.\n4. EXPERIMENTS\nIn the experimental section we compare vanilla SGD versus SGD\nplus some of the enhancements discussed above. Speci\ufb01cally we\nuse the letter \u2018C\u2018 to indicate that gradient clipping is used, \u2018L\u2018 for\nleaky-integration units, \u2018R\u2018 if we use recti\ufb01er units with L1 penalty\nand \u2018M\u2018 for Nesterov momentum.\n4.1. Music Data\nWe evaluate our models on the four polyphonic music datasets of\nvarying complexity used in [25]: classical piano music (Piano-\nmidi.de), folk tunes with chords instantiated from ABC nota-\ntion (Nottingham), orchestral music (MuseData) and the four-part\nchorales by J.S. Bach (JSB chorales). The symbolic sequences con-\ntain high-level pitch and timing information in the form of a binary\nmatrix, or piano-roll, specifying precisely which notes occur at each\ntime-step. They form interesting benchmarks for RNNs because of\ntheir high dimensionality and the complex temporal dependencies\ninvolved at different time scales. Each dataset contains at least 7\nhours of polyphonic music with an average polyphony (number of\nsimultaneous notes) of 3.9.\nPiano-rolls were prepared by aligning each time-step (88 pitch\nlabels that cover the whole range of piano) on an integer fraction\nof the beat (quarter note) and transposing each sequence in a com-\nmon tonality (C major/minor) to facilitate learning. Source \ufb01les and\npreprocessed piano-rolls split in train, validation and test sets are\navailable on the authors\u2019 website2.\n4.1.1. Setup and Results\nWe select hyperparameters, such as the number of hidden units nh,\nregularization coef\ufb01cients \u03bbL1, the choice of non-linearity function,\nor the momentum schedule \u00b5t, learning rate \u01ebt, number of leaky\nunits nleaky or leaky factors \u03b1 according to log-likelihood on a val-\nidation set and we report the \ufb01nal performance on the test set for the\nbest choice in each category. We do so by using random search [31]\non the following intervals:\nnh \u2208[100, 400]\n\u01ebt \u2208[10\u22124, 10\u22121]\n\u00b5t \u2208[10\u22123, 0.95]\n\u03bbL1 \u2208[10\u22126, 10\u22123]\nnleaky \u2208{0%, 25%, 50%}\n\u03b1 \u2208[0.02, 2]\nThe cutoff threshold for gradient clipping is set based on the\naverage norm of the gradient over one pass on the data, and we used\n15 in this case for all music datasets. The data is split into sequences\n2www-etud.iro.umontreal.ca/\u02dcboulanni/icml2012\nTable 1. Log-likelihood and expected accuracy for various RNN models in the symbolic music prediction task. The double line separates\nsigmoid recognition layers (above) to structured output probability models (below).\nModel\nPiano-midi.de\nNottingham\nMuseData\nJSB chorales\nLL\nLL\nACC %\nLL\nLL\nACC %\nLL\nLL\nACC %\nLL\nLL\nACC %\n(train)\n(test)\n(test)\n(train)\n(test)\n(test)\n(train)\n(test)\n(test)\n(train)\n(test)\n(test)\nRNN (SGD)\n-7.10\n-7.86\n22.84\n-3.49\n-3.75\n66.90\n-6.93\n-7.20\n27.97\n-7.88\n-8.65\n29.97\nRNN (SGD+C)\n-7.15\n-7.59\n22.98\n-3.40\n-3.67\n67.47\n-6.79\n-7.04\n30.53\n-7.81\n-8.65\n29.98\nRNN (SGD+CL)\n-7.04\n-7.57\n22.97\n-3.31\n-3.57\n67.97\n-6.47\n-6.99\n31.53\n-7.78\n-8.63\n29.98\nRNN (SGD+CLR)\n-6.40\n-7.80\n24.22\n-2.99\n-3.55\n70.20\n-6.70\n-7.34\n29.06\n-7.67\n-9.47\n29.98\nRNN (SGD+CRM)\n-6.92\n-7.73\n23.71\n-3.20\n-3.43\n68.47\n-7.01\n-7.24\n29.13\n-8.08\n-8.81\n29.52\nRNN (HF)\n-7.00\n-7.58\n22.93\n-3.47\n-3.76\n66.71\n-6.76\n-7.12\n29.77\n-8.11\n-8.58\n29.41\nRNN-RBM\nN/A\n-7.09\n28.92\nN/A\n-2.39\n75.40\nN/A\n-6.01\n34.02\nN/A\n-6.27\n33.12\nRNN-NADE (SGD)\n-7.23\n-7.48\n20.69\n-2.85\n-2.91\n64.95\n-6.86\n-6.74\n24.91\n-5.46\n-5.83\n32.11\nRNN-NADE (SGD+CR)\n-6.70\n-7.34\n21.22\n-2.14\n-2.51\n69.80\n-6.27\n-6.37\n26.60\n-4.44\n-5.33\n34.52\nRNN-NADE (SGD+CRM)\n-6.61\n-7.34\n22.12\n-2.11\n-2.49\n69.54\n-5.99\n-6.19\n29.62\n-4.26\n-5.19\n35.08\nRNN-NADE (HF)\n-6.32\n-7.05\n23.42\n-1.81\n-2.31\n71.50\n-5.20\n-5.60\n32.60\n-4.91\n-5.56\n32.50\nTable 2. Entropy (bits per character) and perplexity for various RNN models on next character and next word prediction task.\nModel\nPenn Treebank Corpus\nPenn Treebank Corpus\nword level\ncharacter level\nperplexity\nperplexity\nentropy\nentropy\n(train)\n(test)\n(train)\n(test)\nRNN (SGD)\n112.11\n145.16\n1.78\n1.76\nRNN (SGD+C)\n78.71\n136.63\n1.40\n1.44\nRNN (SGD+CL)\n76.70\n129.83\n1.56\n1.56\nRNN (SGD+CLR)\n75.45\n128.35\n1.45\n1.49\nof 100 steps over which we compute the gradient. The hidden state\nis carried over from one sequence to another if they belong to the\nsame song, otherwise is set to 0.\nTable 1 presents log-likelihood (LL) and expected frame-level\naccuracy for various RNNs in the symbolic music prediction task.\nResults clearly show that these enhancements allow to improve\non regular SGD in almost all cases; they also make SGD competitive\nwith HF for the sigmoid recognition layers RNNs.\n4.2. Text Data\nWe use the Penn Treebank Corpus to explore both word and char-\nacter prediction tasks. The data is split by using sections 0-20 as\ntraining data (5017k characters), sections 21-22 as validation (393k\ncharacters) and sections 23-24 as test data (442k characters).\nFor the word level prediction, we \ufb01x the dictionary to 10000\nwords, which we divide into 30 classes according to their frequency\nin text (each class holding approximately 3.3% of the total number\nof tokens in the training set). Such a factorization allows for faster\nimplementation, as we are not required to evaluate the whole output\nlayer (10000 units) which is the computational bottleneck, but only\nthe output of the corresponding class [32].\n4.2.1. Setup and Results\nIn the case of next word prediction, we compute gradients over se-\nquences of 40 steps, where we carry the hidden state from one se-\nquence to another. We use a small grid-search around the parameters\nused to get state of the art results for this number of classes [32], i.e.,\nwith a network of 200 hidden units yielding a perplexity of 134. We\nexplore learning rate of 0.1, 0.01, 0.001, recti\ufb01er units versus sig-\nmoid units, cutoff threshold for the gradients of 30, 50 or none, and\nno leaky units versus 50 of the units being sampled from 0.2 and\n0.02.\nFor the character level model we compute gradients over se-\nquences of 150 steps, as we assume that longer dependencies are\nmore crucial in this case. We use 500 hidden units and explore learn-\ning rates of 0.5, 0.1 and 0.01.\nIn table 2 we have entropy (bits per character) or perplexity for\nvarous RNNs on the word and character prediction tasks. Again, we\nobserve substantial improvements in both training and test perplex-\nity, suggesting that these techniques make optimization easier.\n5. CONCLUSIONS\nThrough our experiments we provide evidence that part of the issue\nof training RNN is due to the rough error surface which can not be\neasily handled by SGD. We follow an incremental set of improve-\nments to SGD, and show that in most cases they improve both the\ntraining and test error, and allow this enhanced SGD to compete or\neven improve on a second-order method which was found to work\nparticularly well for RNNs, i.e., Hessian-Free optimization.\n6. REFERENCES\n[1] S. Hochreiter, \u201c Untersuchungen zu dynamischen neuronalen\nNetzen. Diploma thesis, T.U. M\u00a8unich,\u201d 1991.\n[2] Y. Bengio, P. Simard, and P. Frasconi, \u201cLearning long-term de-\npendencies with gradient descent is dif\ufb01cult,\u201d IEEE T. Neural\nNets, 1994.\n[3] D.E. Rumelhart, G.E. Hinton, and R.J. Williams, \u201cLearning\nrepresentations by back-propagating errors,\u201d Nature, vol. 323,\npp. 533\u2013536, 1986.\n[4] J. Martens and I. Sutskever, \u201cLearning recurrent neural net-\nworks with Hessian-free optimization,\u201d in ICML\u20192011, 2011.\n[5] T. Mikolov, S. Kombrink, L. Burget, J. Cernocky, and S. Khu-\ndanpur,\n\u201cExtensions of recurrent neural network language\nmodel,\u201d in ICASSP 2011, 2011.\n[6] I. Sutskever, Training Recurrent Neural Networks, Ph.D. the-\nsis, CS Dept., U. Toronto, 2012.\n[7] Yoshua Bengio, Learning deep architectures for AI, Now Pub-\nlishers, 2009.\n[8] Y. Bengio, A. Courville, and P. Vincent, \u201cUnsupervised feature\nlearning and deep learning: A review and new perspectives,\u201d\nTech. Rep., arXiv:1206.5538, 2012.\n[9] G. E. Hinton, S. Osindero, and Y.-W. Teh, \u201cA fast learning\nalgorithm for deep belief nets,\u201d Neural Computation, vol. 18,\npp. 1527\u20131554, 2006.\n[10] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle,\n\u201cGreedy layer-wise training of deep networks,\u201d in NIPS\u20192006,\n2007.\n[11] M. Ranzato, C. Poultney, S. Chopra, and Y. LeCun,\n\u201cEf\ufb01-\ncient learning of sparse representations with an energy-based\nmodel,\u201d in NIPS\u20192006, 2007.\n[12] D. Erhan, Y. Bengio, A. Courville, P. Manzagol, P. Vincent,\nand S. Bengio, \u201cWhy does unsupervised pre-training help deep\nlearning?,\u201d J. Machine Learning Res., (11) 2010.\n[13] J. Martens, \u201cDeep learning via Hessian-free optimization,\u201d in\nICML\u20192010, 2010, pp. 735\u2013742.\n[14] Q. Le, M. Ranzato, R. Monga, M. Devin, G. Corrado, K. Chen,\nJ. Dean, and A. Ng, \u201cBuilding high-level features using large\nscale unsupervised learning,\u201d in ICML\u20192012, 2012.\n[15] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio, \u201cUn-\nderstanding the exploding gradient problem,\u201d Tech. Rep., Uni-\nversit\u00b4e De Montr\u00b4eal, 2012, arXiv:arXiv:1211.5063.\n[16] Tomas Mikolov, Statistical Language Models based on Neural\nNetworks, Ph.D. thesis, Brno University of Technology, 2012.\n[17] T. Lin, B. G. Horne, P. Tino, and C. L. Giles, \u201cLearning long-\nterm dependencies is not as dif\ufb01cult with NARX recurrent neu-\nral networks,\u201d Tech. Rep. UMICAS-TR-95-78, U. Mariland,\n1995.\n[18] S. ElHihi and Y. Bengio, \u201cHierarchical recurrent neural net-\nworks for long-term dependencies,\u201d in NIPS\u20191995, 1996.\n[19] Herbert Jaeger, Mantas Lukosevicius, Dan Popovici, and Udo\nSiewert, \u201cOptimization and applications of echo state networks\nwith leaky- integrator neurons,\u201d Neural Networks, vol. 20, no.\n3, pp. 335\u2013352, 2007.\n[20] I. Sutskever and G. Hinton, \u201cTemporal kernel recurrent neural\nnetworks,\u201d Neural Networks, vol. 23, no. 2, (23) 2, 2010.\n[21] S. Hochreiter and J. Schmidhuber, \u201cLong short-term memory,\u201d\nNeural Computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.\n[22] Udo Siewert and Welf Wustlich, \u201cEcho-state networks with\nband-pass neurons: Towards generic time-scale-independent\nreservoir structures,\u201d Preliminary Report, October 2007.\n[23] Tomas Mikolov and Geoffrey Zweig, \u201cContext dependent reu-\ncrrent neural network language model,\u201d Workshop on Spoken\nLanguage Technology, 2012.\n[24] I. Sutskever, G. Hinton, and G. Taylor, \u201cThe recurrent temporal\nrestricted Boltzmann machine,\u201d in NIPS\u20192008. 2009.\n[25] N. Boulanger-Lewandowski, Y. Bengio, and P. Vincent, \u201cMod-\neling temporal dependencies in high-dimensional sequences:\nApplication to polyphonic music generation and transcription,\u201d\nin ICML\u20192012, 2012.\n[26] H. Larochelle and I. Murray, \u201cThe Neural Autoregressive Dis-\ntribution Estimator,\u201d in AISTATS\u20192011, 2011.\n[27] X. Glorot, A. Bordes, and Y. Bengio, \u201cDeep sparse recti\ufb01er\nneural networks,\u201d in AISTATS\u20192011, 2011.\n[28] V. Nair and G.E. Hinton, \u201cRecti\ufb01ed linear units improve re-\nstricted Boltzmann machines,\u201d in ICML\u20192010, 2010.\n[29] A. Krizhevsky, I. Sutskever, and G. Hinton,\n\u201cImageNet\nclassi\ufb01cation with deep convolutional neural networks,\u201d\nin\nNIPS\u20192012. 2012.\n[30] Yu Nesterov, \u201cA method for unconstrained convex minimiza-\ntion problem with the rate of convergence o(1/k2),\u201d Doklady\nAN SSSR (translated as Soviet. Math. Docl.), vol. 269, pp. 543\u2013\n547, 1983.\n[31] James Bergstra and Yoshua Bengio,\n\u201cRandom search for\nhyper-parameter optimization,\u201d J. Machine Learning Res., vol.\n13, pp. 281\u2013305, 2012.\n[32] Tomas Mikolov, Stefan Kombrink, Lukas Burget, Jan Cer-\nnocky, and Sanjeev Khudanpur, \u201cExtensions of recurrent neu-\nral network language model,\u201d\nin Proc. 2011 IEEE interna-\ntional conference on acoustics, speech and signal processing\n(ICASSP 2011), 2011.\n",
        "sentence": " These units have been demonstrated to improve the performance of many deep neural networks ([43], [47], [73]) on tasks as varied as speech processing and object recognition, and have been used in recurrent neural networks by [7].",
        "context": "very successful in recent work on deep learning for object recog-\nnition [29], beating by far the state-of-the-art on ImageNet (1000\nclasses). Here, we apply this deep learning idea to RNNs, using\ncient training of recurrent networks. These advances have been mo-\ntivated by and related to the optimization issues surrounding deep\nlearning. Although recurrent networks are extremely powerful in\n(see [7] and [8] for reviews). Deep learning algorithms discover\nmultiple levels of representation, typically as deep neural networks\nor graphical models organized with many levels of representation-\ncarrying latent variables."
    },
    {
        "title": "A neural probabilistic language model",
        "author": [
            "Yoshua Bengio",
            "R\u00e9jean Ducharme",
            "Pascal Vincent",
            "Christian Janvin"
        ],
        "venue": "The Journal of Machine Learning Research,",
        "citeRegEx": "8",
        "shortCiteRegEx": "8",
        "year": 2003,
        "abstract": "",
        "full_text": "",
        "sentence": " in 2003 [8], and more recently brought to wider attention in the deep learning community by Socher, Manning and Ng in a number of papers describing Recursive Autoencoder (RAE) networks ([59], [56], [60], [58]).",
        "context": null
    },
    {
        "title": "Learning long-term dependencies with gradient descent is difficult",
        "author": [
            "Yoshua Bengio",
            "Patrice Simard",
            "Paolo Frasconi"
        ],
        "venue": "Neural Networks, IEEE Transactions on,",
        "citeRegEx": "9",
        "shortCiteRegEx": "9",
        "year": 1994,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " But learning on recurrent networks can be especially hard due to the difficulty of learning long-range dependencies as described by Bengio et al in 1994 [9] and expanded upon in [31].",
        "context": null
    },
    {
        "title": "Theano: a CPU and GPU math expression compiler",
        "author": [
            "James Bergstra",
            "Olivier Breuleux",
            "Fr\u00e9d\u00e9ric Bastien",
            "Pascal Lamblin",
            "Razvan Pascanu",
            "Guillaume Desjardins",
            "Joseph Turian",
            "David Warde-Farley",
            "Yoshua Bengio"
        ],
        "venue": "In Proceedings of the Python for scientific computing conference (SciPy),",
        "citeRegEx": "10",
        "shortCiteRegEx": "10",
        "year": 2010,
        "abstract": "",
        "full_text": "",
        "sentence": " backward propagation using GPUs, such as Theano ( [10]) and Torch ([13]), have made it easy to implement fast training algorithms.",
        "context": null
    },
    {
        "title": "Training a 3-node neural network is NP-complete",
        "author": [
            "Avrim L Blum",
            "Ronald L Rivest"
        ],
        "venue": "In Machine learning: From theory to applications,",
        "citeRegEx": "11",
        "shortCiteRegEx": "11",
        "year": 1993,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " It has been known since at least 1993 that optimizing even a 3-layer neural network is NPComplete [11].",
        "context": null
    },
    {
        "title": "Lazy sparse stochastic gradient descent for regularized multinomial logistic regression",
        "author": [
            "Bob Carpenter"
        ],
        "venue": "Alias-i, Inc., Tech. Rep, pages",
        "citeRegEx": "12",
        "shortCiteRegEx": "12",
        "year": 2008,
        "abstract": "",
        "full_text": "",
        "sentence": " A growing body of recent work ([12], [37], [55], [40]), thus read shows that given sparse inputs to a linear model with any standard regularizer, sparsity can be fully exploited even if the gradient is not sparse (owing to regularization).",
        "context": null
    },
    {
        "title": "Torch7: A matlab-like environment for machine learning",
        "author": [
            "Ronan Collobert",
            "Koray Kavukcuoglu",
            "Cl\u00e9ment Farabet"
        ],
        "venue": "In BigLearn, NIPS Workshop, number EPFL-CONF-192376,",
        "citeRegEx": "13",
        "shortCiteRegEx": "13",
        "year": 2011,
        "abstract": "",
        "full_text": "",
        "sentence": " backward propagation using GPUs, such as Theano ( [10]) and Torch ([13]), have made it easy to implement fast training algorithms.",
        "context": null
    },
    {
        "title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization",
        "author": [
            "Yann N Dauphin",
            "Razvan Pascanu",
            "Caglar Gulcehre",
            "Kyunghyun Cho",
            "Surya Ganguli",
            "Yoshua Bengio"
        ],
        "venue": "In Advances in Neural Information Processing Systems,",
        "citeRegEx": "14",
        "shortCiteRegEx": "14",
        "year": 2014,
        "abstract": "",
        "full_text": "",
        "sentence": " [14] shows that while many critical points exist on the error surfaces of large neural networks, the ratio of saddle points to true local minima increases exponentially with the size of the network Fast implementations and improved gradient following heuristics have rendered RNN training feasible. In the paper that described the abundance of saddle points on the error surfaces of neural networks ([14]), the authors present a saddle-free version of Newton\u2019s method.",
        "context": null
    },
    {
        "title": "A survey on the application of recurrent neural networks to statistical language modeling",
        "author": [
            "Wim De Mulder",
            "Steven Bethard",
            "Marie-Francine Moens"
        ],
        "venue": "Computer Speech & Language,",
        "citeRegEx": "15",
        "shortCiteRegEx": "15",
        "year": 2015,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " More recently, [15] covers recurrent neural nets for language modeling.",
        "context": null
    },
    {
        "title": "Adaptive subgradient methods for online learning and stochastic optimization",
        "author": [
            "John Duchi",
            "Elad Hazan",
            "Yoram Singer"
        ],
        "venue": "The Journal of Machine Learning Research,",
        "citeRegEx": "16",
        "shortCiteRegEx": "16",
        "year": 2011,
        "abstract": "",
        "full_text": "",
        "sentence": " Some popular heuristics, such as AdaGrad [16], AdaDelta [72], and RMSprop [1], adaptively tune the learning rate for each feature.",
        "context": null
    },
    {
        "title": "Finding structure in time",
        "author": [
            "Jeffrey L Elman"
        ],
        "venue": "Cognitive science,",
        "citeRegEx": "18",
        "shortCiteRegEx": "18",
        "year": 1990,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " In the foundational papers, generally published in cognitive science and computational neuroscience journals ([33], [34], [18]), biologically plausible mechanisms are emphasized. Figure 6: An Elman network as described in Finding Structure in Time (1990) [18]. Elman networks, introduced in [18], simplify the structure in the Jordan network. Such an encoding is discussed in [18] among others. In many experiments with recurrent neural networks ([18], [63], [71]), input is fed in one character at a time (and output generated one character at a time).",
        "context": null
    },
    {
        "title": "Long short-term memory in recurrent neural networks",
        "author": [
            "Felix Gers"
        ],
        "venue": "Unpublished PhD dissertation, E\u0301cole Polytechnique Fe\u0301de\u0301rale de Lausanne, Lausanne, Switzerland,",
        "citeRegEx": "19",
        "shortCiteRegEx": "19",
        "year": 2001,
        "abstract": "",
        "full_text": "",
        "sentence": " Among the most useful resources are Alex Graves\u2019 2012 book on supervised sequence labelling with recurrent neural networks [24] and Felix Gers\u2019 doctoral thesis [19]. Following the convention adopted in several foundational recurrent net papers ([32], [19], [21], [63]), we index neurons with j and j\u2032, and by wjj\u2032 , we denote the weight corresponding to the directed edge from node j\u2032 to node j.",
        "context": null
    },
    {
        "title": "Recurrent nets that time and count",
        "author": [
            "Felix A Gers",
            "J\u00fcrgen Schmidhuber"
        ],
        "venue": "In Neural Networks,",
        "citeRegEx": "20",
        "shortCiteRegEx": "20",
        "year": 2000,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Also in 2000, Gers and Schmidhuber proposed peephole connections [20], which pass from the carousel directly to the input and output gates of that same node without first having to pass through the output gate.",
        "context": null
    },
    {
        "title": "Learning to forget: Continual prediction with LSTM",
        "author": [
            "Felix A Gers",
            "J\u00fcrgen Schmidhuber",
            "Fred Cummins"
        ],
        "venue": "Neural computation,",
        "citeRegEx": "21",
        "shortCiteRegEx": "21",
        "year": 2000,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Following the convention adopted in several foundational recurrent net papers ([32], [19], [21], [63]), we index neurons with j and j\u2032, and by wjj\u2032 , we denote the weight corresponding to the directed edge from node j\u2032 to node j. in [21]. Forget gates, proposed in 2000 by Gers and Schmidhuber [21], add a gate similar to input and output gates to allow the network to flush information from the constant error carousel.",
        "context": null
    },
    {
        "title": "Deep sparse rectifier networks",
        "author": [
            "Xavier Glorot",
            "Antoine Bordes",
            "Yoshua Bengio"
        ],
        "venue": "In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics. JMLR W&CP Volume,",
        "citeRegEx": "22",
        "shortCiteRegEx": "22",
        "year": 2011,
        "abstract": "",
        "full_text": "",
        "sentence": " However, a rectified linear units (ReLUs) introduce sparsity to hidden layers [22].",
        "context": null
    },
    {
        "title": "word2vec explained: deriving mikolov et al.\u2019s negative-sampling word-embedding method",
        "author": [
            "Yoav Goldberg",
            "Omer Levy"
        ],
        "venue": "arXiv preprint arXiv:1402.3722,",
        "citeRegEx": "23",
        "shortCiteRegEx": "23",
        "year": 2014,
        "abstract": "",
        "full_text": "",
        "sentence": " Freely available code to produce word vectors from co-occurrence stats include Glove from Pennington Socher and Manning [51], and word2vec [23], which implements a word embedding algorithm from Mikolov et al.",
        "context": null
    },
    {
        "title": "Supervised sequence labelling with recurrent neural networks, volume 385",
        "author": [
            "Alex Graves"
        ],
        "venue": null,
        "citeRegEx": "24",
        "shortCiteRegEx": "24",
        "year": 2012,
        "abstract": "",
        "full_text": "",
        "sentence": " Among the most useful resources are Alex Graves\u2019 2012 book on supervised sequence labelling with recurrent neural networks [24] and Felix Gers\u2019 doctoral thesis [19]. An illustration like this appears in [24]",
        "context": null
    },
    {
        "title": "A novel connectionist system for unconstrained handwriting recognition",
        "author": [
            "Alex Graves",
            "Marcus Liwicki",
            "Santiago Fern\u00e1ndez",
            "Roman Bertolami",
            "Horst Bunke",
            "J\u00fcrgen Schmidhuber"
        ],
        "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,",
        "citeRegEx": "25",
        "shortCiteRegEx": "25",
        "year": 2009,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " for phoneme classification [26] and handwriting recognition [25]. to achieve state of the art results on handwriting recognition and phoneme classification [25] [26]. ( [42], [25]), data is collected from a whiteboard using an eBeam interface.",
        "context": null
    },
    {
        "title": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures",
        "author": [
            "Alex Graves",
            "J\u00fcrgen Schmidhuber"
        ],
        "venue": "Neural Networks,",
        "citeRegEx": "26",
        "shortCiteRegEx": "26",
        "year": 2005,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " for phoneme classification [26] and handwriting recognition [25]. to achieve state of the art results on handwriting recognition and phoneme classification [25] [26].",
        "context": null
    },
    {
        "title": "Neural network synthesis using cellular encoding and the genetic algorithm",
        "author": [
            "Frederic Gruau"
        ],
        "venue": null,
        "citeRegEx": "27",
        "shortCiteRegEx": "27",
        "year": 1994,
        "abstract": "",
        "full_text": "",
        "sentence": " A number of papers from the 1990s ([6], [27]) championed the idea of learning neural networks with genetic algorithms with some even claiming that achieving success on real-world problems by applying many small changes to a network\u2019s weights was impossible.",
        "context": null
    },
    {
        "title": "Optimizing neural networks with genetic algorithms",
        "author": [
            "Steven A Harp",
            "Tariq Samad"
        ],
        "venue": "In Proceedings of the 54th American Power Conference, Chicago,",
        "citeRegEx": "28",
        "shortCiteRegEx": "28",
        "year": 2013,
        "abstract": "",
        "full_text": "",
        "sentence": " Several recent papers explore genetic algorithms for neural networks, especially as means of learning the architecture of neural networks, a problem not addressed by backpropagation ([5], [28]).",
        "context": null
    },
    {
        "title": "Long short-term memory",
        "author": [
            "Sepp Hochreiter",
            "J\u00fcrgen Schmidhuber"
        ],
        "venue": "Neural computation,",
        "citeRegEx": "32",
        "shortCiteRegEx": "32",
        "year": 1997,
        "abstract": "",
        "full_text": "",
        "sentence": " Following the convention adopted in several foundational recurrent net papers ([32], [19], [21], [63]), we index neurons with j and j\u2032, and by wjj\u2032 , we denote the weight corresponding to the directed edge from node j\u2032 to node j. Some of Elman\u2019s ideas, including fixedweight edges and the importance of self-connected recurrent edges in hidden nodes survive in Hochreiter and Schmidhuber\u2019s subsequent work on Long ShortTerm Memory [32]. Figure 9: LSTM memory cell as initially described in Hochreiter [32]. However, in the original LSTM paper [32], the activation function for g is the sigmoid \u03c3.",
        "context": null
    },
    {
        "title": "Neural networks and physical systems with emergent collective computational abilities",
        "author": [
            "John J Hopfield"
        ],
        "venue": "Proceedings of the national academy of sciences,",
        "citeRegEx": "33",
        "shortCiteRegEx": "33",
        "year": 1982,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " In the foundational papers, generally published in cognitive science and computational neuroscience journals ([33], [34], [18]), biologically plausible mechanisms are emphasized. Early work, including Hopfield nets [33], learned via a Hebbian principle but did not produce networks useful for discriminative tasks. In 1982, Hopfield introduced a family of recurrent neural networks [33].",
        "context": null
    },
    {
        "title": "Serial order: A parallel distributed processing approach",
        "author": [
            "Michael I Jordan"
        ],
        "venue": "Advances in psychology,",
        "citeRegEx": "34",
        "shortCiteRegEx": "34",
        "year": 1997,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " In the foundational papers, generally published in cognitive science and computational neuroscience journals ([33], [34], [18]), biologically plausible mechanisms are emphasized. Jordan networks (Figure 5), introduced by Michael Jordan in 1986, present an early architecture for supervised learning on sequences ([34]).",
        "context": null
    },
    {
        "title": "Deep visual-semantic alignments for generating image descriptions",
        "author": [
            "Andrej Karpathy",
            "Li Fei-Fei"
        ],
        "venue": "arXiv preprint arXiv:1412.2306,",
        "citeRegEx": "35",
        "shortCiteRegEx": "35",
        "year": 2014,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " In other papers ([53], [57], [35]), biological inspiration is downplayed in favor of achieving empirical results on important tasks and datasets. use such a network for generating captions for images [35]. Recently, recurrent neural networks have been used successfully for image captioning, [35, 44, 67]. This work clearly precedes [35] as it is cited throughout, but [44] claim it as an independent innovation. similarly use a convolutional neural network to encode images together with a bidirectional neural network to decode translations, using word2vec embeddings as word representations [35].",
        "context": null
    },
    {
        "title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing",
        "author": [
            "Alex Krizhevsky",
            "Ilya Sutskever",
            "Geoffrey E Hinton"
        ],
        "venue": null,
        "citeRegEx": "36",
        "shortCiteRegEx": "36",
        "year": 2012,
        "abstract": "",
        "full_text": "",
        "sentence": " Convolutional neural networks, developed by Yann LeCun, [38] are a variant of feedforward neural network that, since 2012, hold records in many computer vision tasks, such as object detection [36]. Large scale feedforward neural networks trained via backpropagation have set many large-scale machine learning records, most notably on the computer vision task of object detection ([39], [36]).",
        "context": null
    },
    {
        "title": "Sparse online learning via truncated gradient",
        "author": [
            "John Langford",
            "Lihong Li",
            "Tong Zhang"
        ],
        "venue": "In Advances in neural information processing systems,",
        "citeRegEx": "37",
        "shortCiteRegEx": "37",
        "year": 2009,
        "abstract": "We propose a general method called truncated gradient to induce sparsity in\nthe weights of online learning algorithms with convex loss functions. This\nmethod has several essential properties: The degree of sparsity is continuous\n-- a parameter controls the rate of sparsification from no sparsification to\ntotal sparsification. The approach is theoretically motivated, and an instance\nof it can be regarded as an online counterpart of the popular\n$L_1$-regularization method in the batch setting. We prove that small rates of\nsparsification result in only small additional regret with respect to typical\nonline learning guarantees. The approach works well empirically. We apply the\napproach to several datasets and find that for datasets with large numbers of\nfeatures, substantial sparsity is discoverable.",
        "full_text": "Sparse Online Learning via Truncated Gradient\nJohn Langford\nYahoo Research\njl@yahoo-inc.com\nLihong Li\nRutgers Computer Science Department\nlihong@cs.rutgers.edu\nTong Zhang\nRutgers Statistics Department\ntongz@rci.rutgers.edu\nAbstract\nWe propose a general method called truncated gradient to induce sparsity in the weights\nof online learning algorithms with convex loss functions.\nThis method has several essential\nproperties:\n1. The degree of sparsity is continuous\u0016a parameter controls the rate of sparsi\u001ccation from\nno sparsi\u001ccation to total sparsi\u001ccation.\n2. The approach is theoretically motivated, and an instance of it can be regarded as an online\ncounterpart of the popular L1-regularization method in the batch setting. We prove that\nsmall rates of sparsi\u001ccation result in only small additional regret with respect to typical\nonline learning guarantees.\n3. The approach works well empirically.\nWe apply the approach to several datasets and \u001cnd that for datasets with large numbers of\nfeatures, substantial sparsity is discoverable.\n1\nIntroduction\nWe are concerned with machine learning over large datasets. As an example, the largest dataset\nwe use here has over 107 sparse examples and 109 features using about 1011 bytes. In this setting,\nmany common approaches fail, simply because they cannot load the dataset into memory or they\nare not su\u001eciently e\u001ecient. There are roughly two approaches which can work:\n1. Parallelize a batch learning algorithm over many machines (e.g., [3]).\n2. Stream the examples to an online learning algorithm (e.g., [9], [10], [2], and [6]).\nThis paper focuses on the second approach.\nTypical online learning algorithms have at least one weight for every feature, which is too much\nin some applications for a couple reasons:\n1. Space constraints. If the state of the online learning algorithm over\u001dows RAM it can not\ne\u001eciently run. A similar problem occurs if the state over\u001dows the L2 cache.\n1\narXiv:0806.4686v2  [cs.LG]  4 Jul 2008\n2. Test time constraints on computation. Substantially reducing the number of features can yield\nsubstantial improvements in the computational time required to evaluate a new sample.\nThis paper addresses the problem of inducing sparsity in learned weights while using an online\nlearning algorithm. There are several ways to do this wrong for our problem. For example:\n1. Simply adding L1 regularization to the gradient of an online weight update doesn't work\nbecause gradients don't induce sparsity. The essential di\u001eculty is that a gradient update has\nthe form a+b where a and b are two \u001doats. Very few \u001doat pairs add to 0 (or any other default\nvalue) so there is little reason to expect a gradient update to accidentally produce sparsity.\n2. Simply rounding weights to 0 is problematic because a weight may be small due to being\nuseless or small because it has been updated only once (either at the beginning of training or\nbecause the set of features appearing is also sparse). Rounding techniques can also play havoc\nwith standard online learning guarantees.\n3. Black-box wrapper approaches which eliminate features and test the impact of the elimination\nare not e\u001ecient enough. These approaches typically run an algorithm many times which is\nparticularly undesirable with large datasets.\n1.1\nWhat Others Do\nThe Lasso algorithm [13] is commonly used to achieve L1 regularization for linear regression. This\nalgorithm does not work automatically in an online fashion.\nThere are two formulations of L1\nregularization. Consider a loss function L(w, zi) which is convex in w, where zi = (xi, yi) is an\ninput/output pair. One is the convex constraint formulation\n\u02c6w = arg min\nw\nn\nX\ni=1\nL(w, zi)\nsubject to \u2225w\u22251 \u2264s,\n(1)\nwhere s is a tunable parameter. The other is soft-regularization, where\n\u02c6w = arg min\nw\nn\nX\ni=1\nL(w, zi) + \u03bb\u2225w\u22251.\n(2)\nWith appropriately chosen \u03bb, the two formulations are equivalent. The convex constraint formu-\nlation has a simple online version using the projection idea in [15]. It requires the projection of\nweight w into an L1 ball at every online step. This operation is di\u001ecult to implement e\u001eciently for\nlarge-scale data where we have examples with sparse features and a large number of features. In\nsuch situation, we require that the number of operations per online step to be linear with respect\nto the number of nonzero features, and independent of the total number of features. Our method,\nwhich works with the soft-regularization formulation (2), satis\u001ces the requirement. Additional de-\ntails can be found in Section 5. In addition to L1 regularization formulation (2), the family of online\nalgorithms we consider in this paper also include some nonconvex sparsi\u001ccation techniques.\nThe Forgetron algorithm [4] is an online learning algorithm that manages memory use.\nIt\noperates by decaying the weights on previous examples and then rounding these weights to zero\nwhen they become small. The Forgetron is stated for kernelized online algorithms, while we are\nconcerned with the simple linear setting. When applied to a linear kernel, the Forgetron is not\ncomputationally or space competitive with approaches operating directly on feature weights.\n2\n1.2\nWhat We Do\nAt a high level, the approach we take is weight decay to a default value. This simple approach\nenjoys a strong performance guarantee, as discussed in section 3.\nFor instance, the algorithm\nnever performs much worse than a standard online learning algorithm, and the additional loss due\nto sparsi\u001ccation is controlled continuously with a single real-valued parameter. The theory gives\na family of algorithms with convex loss functions for inducing sparsity\u0016one per online learning\nalgorithm. We instantiate this for square loss and show how to deal with sparse examples e\u001eciently\nin section 4.\nAs mentioned in the introduction, we are mainly interested in sparse online methods for large\nscale problems with sparse features. For such problems, our algorithm should satisfy the following\nrequirements:\n\u2022 The algorithm should be computationally e\u001ecient: the number of operations per online step\nshould be linear in the number of nonzero features, and independent of the total number of\nfeatures.\n\u2022 The algorithm should be memory e\u001ecient: it needs to maintain a list of active features, and can\ninsert (when the corresponding weight becomes nonzero) and delete (when the corresponding\nweight becomes zero) features dynamically.\nThe implementation details, showing that our methods satisfy the above requirements, are provided\nin section 5.\nTheoretical results stating how much sparsity is achieved using this method generally require\nadditional assumptions which may or may not be met in practice. Consequently, we rely on ex-\nperiments in section 6 to show that our method achieves good sparsity practice. We compare our\napproach to a few others, including L1 regularization on small data, as well as online rounding of\ncoe\u001ecients to zero.\n2\nOnline Learning with GD\nIn the setting of standard online learning, we are interested in sequential prediction problems where\nrepeatedly from i = 1, 2, . . .:\n1. An unlabeled example xi arrives.\n2. We make a prediction based on existing weights wi \u2208Rd.\n3. We observe yi, let zi = (xi, yi), and incur some known loss L(wi, zi) convex in parameter wi.\n4. We update weights according to some rule: wi+1 \u2190f(wi).\nWe want to come up with an update rule f, which allows us to bound the sum of losses\nt\nX\ni=1\nL(wi, zi)\nas well as achieving sparsity.\nFor this purpose, we start with the standard stochastic gradient\ndescent rule, which is of the form:\nf(wi) = wi \u2212\u03b7\u22071L(wi, zi),\n(3)\n3\nwhere \u22071L(a, b) is a sub-gradient of L(a, b) with respect to the \u001crst variable a. The parameter\n\u03b7 > 0 is often referred to as the learning rate. In our analysis, we only consider constant learning\nrate with \u001cxed \u03b7 > 0 for simplicity. In theory, it might be desirable to have a decaying learning\nrate \u03b7i which becomes smaller when i increases to get the so called no-regret bound without knowing\nT in advance. However, if T is known in advance, one can select a constant \u03b7 accordingly so that\nthe regret vanishes as T \u2192\u221e. Since our focus is on sparsity, not how to choose learning rate, for\nclarity, we use a constant learning rate in the analysis because it leads to simpler bounds.\nThe above method has been widely used in online learning such as [10] and [2]. Moreover, it is\nargued to be e\u001ecient even for solving batch problems where we repeatedly run the online algorithm\nover training data multiple times. For example, the idea has been successfully applied to solve\nlarge-scale standard SVM formulations [11, 14]. In the scenario outlined in the introduction, online\nlearning methods are more suitable than some traditional batch learning methods.\nHowever, a main drawback of (3) is that it does not achieve sparsity, which we address in this\npaper. Note that in the literature, this particular update rule is often referred to as gradient descent\n(GD) or stochastic gradient descent (SGD). There are other variants, such as exponentiated gradient\ndescent (EG). Since our focus in this paper is sparsity, not GD versus EG, we shall only consider\nmodi\u001ccations of (3) for simplicity.\n3\nSparse Online Learning\nIn this section, we examine several methods for achieving sparsity in online learning. The \u001crst idea\nis simple coe\u001ecient rounding, which is the most natural method. We will then consider its full\nonline implementation, and another method which is the online counterpart of L1 regularization in\nbatch learning. As we shall see, all these ideas are closely related.\n3.1\nSimple Coe\u001ecient Rounding\nIn order to achieve sparsity, the most natural method is to round small coe\u001ecients (that are no\nlarger than a threshold \u03b8 > 0) to zero after every K online steps. That is, if i/K is not an integer,\nwe use the standard GD rule in (3); if i/K is an integer, we modify the rule as:\nf(wi) = T0(wi \u2212\u03b7\u22071L(wi, zi), \u03b8),\n(4)\nwhere for a vector v = [v1, . . . , vd] \u2208Rd, and a scalar \u03b8 \u22650, T0(v, \u03b8) = [T0(v1, \u03b8), . . . , T0(vd, \u03b8)],\nwith\nT0(vj, \u03b8) =\n(\n0\nif |vj| \u2264\u03b8\nvj\notherwise .\nThat is, we \u001crst perform a standard stochastic gradient descent rule, and then round the updated\ncoe\u001ecients toward zero. The e\u001bect is to remove nonzero and small components in the weight vector.\nIn general, we should not take K = 1, especially when \u03b7 is small, since each step modi\u001ces wi\nby only a small amount. If a coe\u001ecient is zero, it remains small after one online update, and the\nrounding operation pulls it back to zero. Consequently, rounding can be done only after every K\nsteps (with a reasonably large K); in this case, nonzero coe\u001ecients have su\u001ecient time to go above\nthe threshold \u03b8. However, if K is too large, then in the training stage, we will need to keep many\nmore nonzero features in the intermediate steps before they are rounded to zero. In the extreme\n4\ncase, we may simply round the coe\u001ecients in the end, which does not solve the storage problem in\nthe training phase. The sensitivity in choosing appropriate K is a main drawback of this method;\nanother drawback is the lack of theoretical guarantee for its online performance.\nThe above mentioned issues motivate us to consider more principled sparse online learning\nmethods. In section 3.3, we derive an online version of rounding using an idea called truncated\ngradient for which regret bounds hold.\n3.2\nA Sub-gradient Algorithm for L1 Regularization\nIn our experiments, we combine rounding-in-the-end-of-training with a simple online sub-gradient\nmethod for L1 regularization with a regularization parameter g > 0:\nf(wi) = wi \u2212\u03b7\u22071L(wi, zi) \u2212\u03b7g sgn(wi),\n(5)\nwhere for a vector v = [v1, . . . , vd], sgn(v) = [sgn(v1), . . . , sgn(vd)], and sgn(vj) = 1 when vj > 0,\nsgn(vj) = \u22121 when vj < 0, and sgn(vj) = 0 when vj = 0. In the experiments, the online method\n(5) plus rounding in the end is used as a simple baseline. One should note that this method does\nnot produce sparse weights online. Therefore it does not handle large-scale problems for which we\ncannot keep all features in memory.\n3.3\nTruncated Gradient\nIn order to obtain an online version of the simple rounding rule in (4), we observe that the direct\nrounding to zero is too aggressive. A less aggressive version is to shrink the coe\u001ecient to zero by a\nsmaller amount. We call this idea truncated gradient.\nThe amount of shrinkage is measured by a gravity parameter gi > 0:\nf(wi) = T1(wi \u2212\u03b7\u22071L(wi, zi), \u03b7gi, \u03b8),\n(6)\nwhere for a vector v = [v1, . . . , vd] \u2208Rd, and a scalar g \u22650, T1(v, \u03b1, \u03b8) = [T1(v1, \u03b1, \u03b8), . . . , T1(vd, \u03b1, \u03b8)],\nwith\nT1(vj, \u03b1, \u03b8) =\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\nmax(0, vj \u2212\u03b1)\nif vj \u2208[0, \u03b8]\nmin(0, vj + \u03b1)\nif vj \u2208[\u2212\u03b8, 0]\nvj\notherwise\n.\nAgain, the truncation can be performed every K online steps. That is, if i/K is not an integer, we\nlet gi = 0; if i/K is an integer, we let gi = Kg for a gravity parameter g > 0. This particular choice\nis equivalent to (4) when we set g such that \u03b7Kg \u2265\u03b8. This requires a large g when \u03b7 is small. In\npractice, one should set a small, \u001cxed g, as implied by our regret bound developed later.\nIn general, the larger the parameters g and \u03b8 are, the more sparsity is incurred. Due to the\nextra truncation T1, this method can lead to sparse solutions, which is con\u001crmed in our experiments\ndescribed later. In those experiments, the degree of sparsity discovered varies with the problem.\nA special case, which we use in the experiment, is to let g = \u03b8 in (6). In this case, we can\nuse only one parameter g to control sparsity. Since \u03b7Kg \u226a\u03b8 when \u03b7K is small, the truncation\noperation is less aggressive than the rounding in (4). At \u001crst sight, the procedure appears to be an\nad-hoc way to \u001cx (4). However, we can establish a regret bound for this method, showing that it is\ntheoretically sound.\n5\nAnother important special case of (6) is setting \u03b8 = \u221e. This leads to the following update rule\nfor every K-th online step\nf(wi) = T(wi \u2212\u03b7\u22071L(wi, zi), gi\u03b7),\n(7)\nwhere for a vector v = [v1, . . . , vd] \u2208Rd, and a scalar g \u22650, T(v, \u03b1) = [T(v1, \u03b1), . . . , T(vd, \u03b1)], with\nT(vj, \u03b1) =\n(\nmax(0, vj \u2212\u03b1)\nif vj > 0\nmin(0, vj + \u03b1)\notherwise .\nThe method is a modi\u001ccation of the standard sub-gradient online method for L1 regularization in\n(5). The parameter gi \u22650 controls the sparsity that can be achieved with the algorithm. Note\nthat when gi = 0, the update rule is identical to the standard stochastic gradient descent rule. In\ngeneral, we may perform a truncation every K steps. That is, if i/K is not an integer, we let gi = 0;\nif i/K is an integer, we let gi = Kg for a gravity parameter g > 0. The reason for doing so (instead\nof a constant g) is that we can perform a more aggressive truncation with gravity parameter Kg\nafter each K steps. This can potentially lead to better sparsity.\nThe procedure in (7) can be regarded as an online counterpart of L1 regularization in the sense\nthat it approximately solves an L1 regularization problem in the limit of \u03b7 \u21920. Truncated gradient\ndescent for L1 regularization is di\u001berent from the naive application of stochastic gradient descent\nrule (3) with an added L1 regularization term. As pointed out in the introduction, the latter fails\nbecause it rarely leads to sparsity. Our theory shows that even with sparsi\u001ccation, the prediction\nperformance is still comparable to that of the standard online learning algorithm. In the following,\nwe develop a general regret bound for this general method, which also shows how the regret may\ndepend on the sparsi\u001ccation parameter g.\n3.4\nRegret Analysis\nThroughout the paper, we use \u2225\u00b7 \u22251 for 1-norm, and \u2225\u00b7 \u2225for 2-norm. For reference, we make the\nfollowing assumption regarding the loss function:\nAssumption 3.1 We assume that L(w, z) is convex in w, and there exist non-negative constants\nA and B such that (\u22071L(w, z))2 \u2264AL(w, z) + B for all w \u2208Rd and z \u2208Rd+1.\nFor linear prediction problems, we have a general loss function of the form L(w, z) = \u03c6(wT x, y).\nThe following are some common loss functions \u03c6(\u00b7, \u00b7) with corresponding choices of parameters A\nand B (which are not unique), under the assumption that supx \u2225x\u2225\u2264C.\n\u2022 Logistic: \u03c6(p, y) = ln(1 + exp(\u2212py)); A = 0 and B = C2. This loss is for binary classi\u001ccation\nproblems with y \u2208{\u00b11}.\n\u2022 SVM (hinge loss): \u03c6(p, y) = max(0, 1 \u2212py); A = 0 and B = C2. This loss is for binary\nclassi\u001ccation problems with y \u2208{\u00b11}.\n\u2022 Least squares (square loss): \u03c6(p, y) = (p\u2212y)2; A = 4C2 and B = 0. This loss is for regression\nproblems.\nOur main result is Theorem 3.1 that is parameterized by A and B. The proof is left to the\nappendix. Specializing it to particular losses yields several corollaries. The one that can be applied\nto the least squares loss will be given later in Corollary 4.1.\n6\nTheorem 3.1 (Sparse Online Regret) Consider sparse online update rule (7) with w1 = 0 and\n\u03b7 > 0. If Assumption 3.1 holds, then for all \u00afw \u2208Rd we have\n1 \u22120.5A\u03b7\nT\nT\nX\ni=1\n\u0014\nL(wi, zi) +\ngi\n1 \u22120.5A\u03b7\u2225wi+1 \u00b7 I(wi+1 \u2264\u03b8)\u22251\n\u0015\n\u2264\u03b7\n2B + \u2225\u00afw\u22252\n2\u03b7T + 1\nT\nT\nX\ni=1\n[L( \u00afw, zi) + gi\u2225\u00afw \u00b7 I(wi+1 \u2264\u03b8)\u22251],\nwhere for vectors v = [v1, . . . , vd] and v\u2032 = [v\u2032\n1, . . . , v\u2032\nd], we let\n\u2225v \u00b7 I(|v\u2032| \u2264\u03b8)\u22251 =\nd\nX\nj=1\n|vj|I(|v\u2032\nj| \u2264\u03b8),\nwhere I(\u00b7) is the set indicator function.\nWe state the theorem with a constant learning rate \u03b7. As mentioned earlier, it is possible to\nobtain a result with variable learning rate where \u03b7 = \u03b7i decays as i increases. Although this may\nlead to a no-regret bound without knowing T in advance, it introduces extra complexity to the\npresentation of the main idea. Since our focus is on sparsity rather than optimizing learning rate,\nwe do not include such a result for clarity. If T is known in advance, then in the above bound, one\ncan simply take \u03b7 = O(1/\n\u221a\nT) and the regret is of order O(1/\n\u221a\nT).\nIn the above theorem, the right-hand side involves a term gi\u2225\u00afw \u00b7 I(wi+1 \u2264\u03b8)\u22251 that depends on\nwi+1 which is not easily estimated. To remove this dependency, a trivial upper bound of \u03b8 = \u221e\ncan be used, leading to L1 penalty gi\u2225\u00afw\u22251. In the general case of \u03b8 < \u221e, we cannot remove the\nwi+1 dependency because the e\u001bective regularization condition (as shown on the left-hand side) is\nthe non-convex penalty gi\u2225w \u00b7 I(|w| \u2264\u03b8)\u22251. Solving such a non-convex formulation is hard both in\nthe online and batch settings. In general, we only know how to e\u001eciently discover a local minimum\nwhich is di\u001ecult to characterize. Without a good characterization of the local minimum, it is not\npossible for us to replace gi\u2225\u00afw \u00b7 I(wi+1 \u2264\u03b8)\u22251 on the right-hand side by gi\u2225\u00afw \u00b7 I( \u00afw \u2264\u03b8)\u22251 because\nsuch a formulation would have implied that we could e\u001eciently solve a non-convex problem with a\nsimple online update rule. Still, when \u03b8 < \u221e, one naturally expects that the right-hand side penalty\ngi\u2225\u00afw \u00b7 I(wi+1 \u2264\u03b8)\u22251 is much smaller than the corresponding L1 penalty gi\u2225\u00afw\u22251, especially when wj\nhas many components that are close to 0. Therefore the situation with \u03b8 < \u221ecan potentially yield\nbetter performance on some data. This is con\u001crmed in our experiments.\nTheorem 3.1 also implies a trade-o\u001b between sparsity and regret performance. We may simply\nconsider the case where gi = g is a constant. When g is small, we have less sparsity but the regret\nterm g\u2225\u00afw \u00b7I(wi+1 \u2264\u03b8)\u22251 \u2264g\u2225\u00afw\u22251 on the right-hand side is also small. When g is large, we are able\nto achieve more sparsity but the regret g\u2225\u00afw \u00b7 I(wi+1 \u2264\u03b8)\u22251 on the right-hand side also becomes\nlarge. Such a trade-o\u001b (sparsity versus prediction accuracy) is empirically studied in Section 6. Our\nobservation suggests that we can gain signi\u001ccant sparsity with only a small decrease of accuracy\n(that is, using a small g).\nNow consider the case \u03b8 = \u221eand gi = g. When T \u2192\u221e, if we let \u03b7 \u21920 and \u03b7T \u2192\u221e, then\nTheorem 3.1 implies that\n1\nT\nT\nX\ni=1\n[L(wi, zi) + g\u2225wi\u22251] \u2264inf\n\u00afw\u2208Rd\n\"\n1\nT\nT\nX\ni=1\nL( \u00afw, zi) + 2g\u2225\u00afw\u22251\n#\n+ o(1).\n7\nIn other words, if we let L\u2032(w, z) = L(w, z)+g\u2225w\u22251 be the L1 regularized loss, then the L1 regularized\nregret is small when \u03b7 \u21920 and T \u2192\u221e. This implies that our procedure can be regarded as the\nonline counterpart of L1-regularization methods. In the stochastic setting where the examples are\ndrawn iid from some underlying distribution, the sparse online gradient method proposed in this\npaper solves the L1 regularization problem.\n3.5\nStochastic Setting\nStochastic-gradient-based online learning methods can be used to solve large-scale batch optimiza-\ntion problems, often quite successfully [11, 14]. In this setting, we can go through training examples\none-by-one in an online fashion, and repeat multiple times over the training data. In this section,\nwe analyze the performance of such a procedure using Theorem 3.1.\nTo simplify the analysis, instead of assuming that we go through the data one by one, we assume\nthat each additional data point is drawn from the training data randomly with equal probability.\nThis corresponds to the standard stochastic optimization setting, in which observed samples are iid\nfrom some underlying distributions. The following result is a simple consequence of Theorem 3.1.\nFor simplicity, we only consider the case with \u03b8 = \u221eand constant gravity gi = g.\nTheorem 3.2 Consider a set of training data zi = (xi, yi) for i = 1, . . . , n, and let\nR(w, g) = 1\nn\nn\nX\ni=1\nL(w, zi) + g\u2225w\u22251\nbe the L1 regularized loss over training data. Let \u02c6w1 = w1 = 0, and de\u001cne recursively for t = 1, 2, . . .\nwt+1 = T(wt \u2212\u03b7\u22071(wt, zit), g\u03b7),\n\u02c6wt+1 = \u02c6wt + (wt+1 \u2212\u02c6wt)/(t + 1),\nwhere each it is drawn from {1, . . . , n} uniformly at random. If Assumption 3.1 holds, then at any\ntime T, the following inequalities are valid for all \u00afw \u2208Rd:\nEi1,...,iT\n\u0014\n(1 \u22120.5A\u03b7)R\n\u0012\n\u02c6wT ,\ng\n1 \u22120.5A\u03b7\n\u0013\u0015\n\u2264Ei1,...,iT\n\"\n1 \u22120.5A\u03b7\nT\nT\nX\ni=1\nR\n\u0012\nwi,\ng\n1 \u22120.5A\u03b7\n\u0013#\n\u2264\u03b7\n2B + \u2225\u00afw\u22252\n2\u03b7T + R( \u00afw, g).\nProof.\nNote that the recursion of \u02c6wt implies that\n\u02c6wT = 1\nT\nT\nX\nt=1\nwt\nfrom telescoping the update rule.\nBecause R(w, g) is convex in w, the \u001crst inequality follows directly from Jensen's inequality.\nIn the following we only need to prove the second inequality. Theorem 3.1 implies the following:\n1 \u22120.5A\u03b7\nT\nT\nX\nt=1\n\u0014\nL(wt, zit) +\ng\n1 \u22120.5A\u03b7\u2225wt\u22251\n\u0015\n\u2264g\u2225\u00afw\u22251 + \u03b7\n2B + \u2225\u00afw\u22252\n2\u03b7T + 1\nT\nT\nX\nt=1\nL( \u00afw, zit).\n(8)\n8\nObserve that\nEit\n\u0014\nL(wt, zit) +\ng\n1 \u22120.5A\u03b7\u2225wt\u22251\n\u0015\n= R\n\u0012\nwt,\ng\n1 \u22120.5A\u03b7\n\u0013\nand\ng\u2225\u00afw\u22251 + Ei1,...,iT\n\"\n1\nT\nT\nX\nt=1\nL( \u00afw, zit)\n#\n= R( \u00afw, g).\nThe second inequality is obtained by taking the expectation with respect to Ei1,...,iT in (8).\n\u25a1\nIf we let \u03b7 \u21920 and \u03b7T \u2192\u221e, the bound in Theorem 3.2 becomes\nE [R( \u02c6wT , g)] \u2264E\n\"\n1\nT\nT\nX\nt=1\nR(wt, g)\n#\n\u2264inf\n\u00afw R( \u00afw, g) + o(1).\nThat is, on average \u02c6wT approximately solves the L1 regularization problem\ninf\nw\n\"\n1\nn\nn\nX\ni=1\nL(w, zi) + g\u2225w\u22251\n#\n.\nIf we choose a random stopping time T, then the above inequalities says that on average R(wT )\nalso solves this L1 regularization problem approximately. Therefore in our experiment, we use the\nlast solution wT instead of the aggregated solution \u02c6wT .\nSince 1-norm regularization is frequently used to achieve sparsity in the batch learning setting,\nthe connection to 1-norm regularization can be regarded as an alternative justi\u001ccation for the\nsparse-online algorithm developed in this paper.\n4\nTruncated Gradient Algorithm for Least Squares\nThe method in Section 3 can be directly applied to least squares regression. This leads to Algorithm\n1 which implements sparsi\u001ccation for square loss according to equation (7). In the description, we\nuse superscripted symbol wj to denote the j-th component of vector w (in order to di\u001berentiate\nfrom wi, which we have used to denote the i-th weight vector). For clarity, we also drop the index\ni from wi. Although we keep the choice of gravity parameters gi open in the algorithm description,\nin practice, we only consider the following choice:\ngi =\n(\nKg\nif i/K is an integer\n0\notherwise\n.\nThis may give a more aggressive truncation (thus sparsity) after every K-th iteration. Since we do\nnot have a theorem formalizing how much more sparsity one can gain from this idea, its e\u001bect will\nonly be examined through experiments.\nIn many online learning situations (such as web applications), only a small subset of the features\nhave nonzero values for any example x.\nIt is thus desirable to deal with sparsity only in this\nsmall subset rather than all features, while simultaneously inducing sparsity on all feature weights.\nMoreover, it is important to store only features with non-zero coe\u001ecients (if the number of features\nis so large that it cannot be stored in memory, this approach allows us to use a hashtable to track\n9\nAlgorithm 1 Truncated Gradient\nInputs:\n\u2022 threshold \u03b8 \u22650\n\u2022 gravity sequence gi \u22650\n\u2022 learning rate \u03b7 \u2208(0, 1)\n\u2022 example oracle O\ninitialize weights wj \u21900 (j = 1, . . . , d)\nfor trial i = 1, 2, . . .\n1. Acquire an unlabeled example x = [x1, x2, . . . , xd] from oracle O\n2. forall weights wj (j = 1, . . . , d)\n(a) if wj > 0 and wj \u2264\u03b8 then wj \u2190max{wj \u2212gi\u03b7, 0}\n(b) elseif wj < 0 and wj \u2265\u2212\u03b8 then wj \u2190min{wj + gi\u03b7, 0}\n3. Compute prediction: \u02c6y = P\nj wjxj\n4. Acquire the label y from oracle O\n5. Update weights for all features j: wj \u2190wj + 2\u03b7(y \u2212\u02c6y)xj\nonly the nonzero coe\u001ecients). We describe how this can be implemented e\u001eciently in the next\nsection.\nFor reference, we present a specialization of Theorem 3.1 in the following corollary that is directly\napplicable to Algorithm 1.\nCorollary 4.1 (Sparse Online Square Loss Regret) If there exists C > 0 such that for all x, \u2225x\u2225\u2264\nC, then for all \u00afw \u2208Rd, we have\n1 \u22122C2\u03b7\nT\nT\nX\ni=1\n\u0014\n(wT\ni xi \u2212yi)2 +\ngi\n1 \u22122C2\u03b7\u2225wi \u00b7 I(|wi| \u2264\u03b8)\u22251\n\u0015\n\u2264\u2225\u00afw\u22252\n2\u03b7T + 1\nT\nT\nX\ni=1\n\u0002\n( \u00afwT xi \u2212yi)2 + gi+1\u2225\u00afw \u00b7 I(|wi+1| \u2264\u03b8)\u22251\n\u0003\n,\nwhere wi = [w1, . . . , wd] \u2208Rd is the weight vector used for prediction at the i-th step of Algorithm 1;\n(xi, yi) is the data point observed at the i-step.\nThis corollary explicitly states that the average square loss incurred by the learner (left term) is\nbounded by the average square loss of the best weight vector \u00afw, plus a term related to the size of\n\u00afw which decays as 1/T and an additive o\u001bset controlled by the sparsity threshold \u03b8 and the gravity\nparameter gi.\n10\n5\nE\u001ecient Implementation\nWe altered a standard gradient-descent implementation (Vowpal Wabbit [7]) according to algorithm\n1. Vowpal Wabbit optimizes square loss on a linear representation w \u00b7x via gradient descent (3) with\na couple caveats:\n1. The prediction is normalized by the square root of the number of nonzero entries in a sparse\nvector, w \u00b7 x/|x|0.5\n0 .\nThis alteration is just a constant rescaling on dense vectors which is\ne\u001bectively removable by an appropriate rescaling of the learning rate.\n2. The prediction is clipped to the interval [0, 1], implying that the loss function is not square\nloss for unclipped predictions outside of this dynamic range. Instead the update is a constant\nvalue, equivalent to the gradient of a linear loss function.\nThe learning rate in Vowpal Wabbit is controllable, supporting 1/i decay as well as a constant\nlearning rate (and rates in-between). The program operates in an entirely online fashion, so the\nmemory footprint is essentially just the weight vector, even when the amount of data is very large.\nAs mentioned earlier, we would like the algorithm's computational complexity to depend linearly\non the number of nonzero features of an example, rather than the total number of features. The\napproach we took was to store a time-stamp \u03c4j for each feature j. The time-stamp was initialized\nto the index of the example where feature j was nonzero for the \u001crst time. During online learning,\nwe simply went through all nonzero features j of example i, and could \u0010simulate\u0011 the shrinkage of\nwj after \u03c4j in a batch mode. These weights are then updated, and their time stamps are reset to\ni. This lazy-update idea of delaying the shrinkage calculation until needed is the key to e\u001ecient\nimplementation of truncated gradient. Speci\u001ccally, instead of using update rule (6) for weight wj,\nwe shrunk the weights of all nonzero feature j di\u001berently by the following:\nf(wj) = T1\n\u0012\nwj + 2\u03b7(y \u2212\u02c6y)xj,\n\u0016i \u2212\u03c4j\nK\n\u0017\nK\u03b7g, \u03b8\n\u0013\n,\nand \u03c4j is updated by\n\u03c4j \u2190\u03c4j +\n\u0016i \u2212\u03c4j\nK\n\u0017\nK.\nWe note that such a lazy-update trick by maintaining the time-stamp information can be applied\nto the other two algorithms given in section 3. In the coe\u001ecient rounding algorithm (4), for instance,\nfor each nonzero feature j of example i, we can \u001crst perform a regular gradient descent on the square\nloss, and then do the following: if |wj| is below the threshold \u03b8 and i \u2265\u03c4j + K, we round wj to 0\nand set \u03c4j to i.\nThis implementation shows that the truncated gradient method satis\u001ces the following require-\nments needed for solving large scale problems with sparse features.\n\u2022 The algorithm is computationally e\u001ecient: the number of operations per online step is linear\nin the number of nonzero features, and independent of the total number of features.\n\u2022 The algorithm is memory e\u001ecient: it maintains a list of active features, and a feature can be\ninserted when observed, and deleted when the corresponding weight becomes zero.\n11\nIf we apply the online projection idea in [15] to solve (1), then in the update rule (7), one\nhas to pick the smallest gi \u22650 such that \u2225wi+1\u22251 \u2264s.\nWe do not know an e\u001ecient method\nto \u001cnd this speci\u001cc gi using operations independent of the total number of features. A standard\nimplementation relies on sorting all weights, which requires O(d ln d) operations, where d is the total\nnumber of (nonzero) features. This complexity is unacceptable for our purpose. However, we shall\npoint out that in an important recent work [5], the authors proposed an e\u001ecient online \u21131-projection\nmethod. The idea is to use a balanced tree to keep track of weights, which allows e\u001ecient threshold\n\u001cnding and tree updates in O(k ln d) operations on average (here, k denotes the number of nonzero\ncoe\u001ecients in the current training example). Although the algorithm still has weak dependency on\nd, it is applicable to large scale practical applications. The theoretical analysis presented in this\npaper shows that we can obtain a meaningful regret bound by picking an arbitrary gi. This is useful\nbecause the resulting method is much simpler to implement and is computationally more e\u001ecient\nper online step. Moreover, our method allows non-convex updates that are closely related to the\nsimple coe\u001ecient rounding idea. Due to the complexity of implementing the balanced tree strategy\nin [5], we shall not compare to it in this paper.\n6\nEmpirical Results\nWe applied Vowpal Wabbit with the e\u001eciently implemented sparsify option, as described in the\nprevious section, to a selection of datasets, including eleven datasets from the UCI repository [1],\nthe much larger dataset rcv1 [8], and a private large-scale dataset Big_Ads related to ad interest\nprediction. While UCI datasets are useful for benchmark purposes, rcv1 and Big_Ads are more\ninteresting since they embody real-world datasets with large numbers of features, many of which\nare less informative for making predictions than others. The datasets are summarized in Table 1.\nThe UCI datasets we used do not have many features, and it is expected that a large fraction\nof these features are useful for making predictions. For comparison purposes as well as to better\ndemonstrate the behavior of our algorithm, we also added 1000 random binary features to those\ndatasets. Each feature has value 1 with probability 0.05 and 0 otherwise.\n6.1\nFeature Sparsi\u001ccation of Truncated Gradient Descent\nIn the \u001crst set of experiments, we are interested in how much reduction in the number of features\nis possible without a\u001becting learning performance signi\u001ccantly; speci\u001ccally, we require the accuracy\nbe reduced by no more than 1% for classi\u001ccation tasks, and the total square loss be increased by\nno more than 1% for regression tasks. As common practice, we allowed the algorithm to run on the\ntraining data set for multiple passes with decaying learning rate. For each dataset, we performed\n10-fold cross validation over the training set to identify the best set of parameters, including the\nlearning rate \u03b7, the sparsi\u001ccation rate g, number of passes of the training set, and the decay of\nlearning rate across these passes. This set of parameters was then used to train Vowpal Wabbit on\nthe whole training set. Finally, the learned classi\u001cer/regressor is evaluated on the test set. We \u001cxed\nK = 1 and \u03b8 = \u221ein these experiments, and will study the e\u001bects of K and \u03b8 in later subsections.\nFigure 1 shows the fraction of reduced features after sparsi\u001ccation is applied to each dataset.\nFor UCI datasets with randomly added features, Vowpal Wabbit is able to reduce the number of\nfeatures by a fraction of more than 90%, except for the ad dataset in which only 71% reduction\nis observed. This less satisfying result might be improved by a more extensive parameter search\n12\nTable 1: Dataset Summary.\nDataset\n#features\n#train data\n#test data\ntask\nad\n1411\n2455\n824\nclassi\u001ccation\ncrx\n47\n526\n164\nclassi\u001ccation\nhousing\n14\n381\n125\nregression\nkrvskp\n74\n2413\n783\nclassi\u001ccation\nmagic04\n11\n14226\n4794\nclassi\u001ccation\nmushroom\n117\n6079\n2045\nclassi\u001ccation\nspambase\n58\n3445\n1156\nclassi\u001ccation\nwbc\n10\n520\n179\nclassi\u001ccation\nwdbc\n31\n421\n148\nclassi\u001ccation\nwpbc\n33\n153\n45\nclassi\u001ccation\nzoo\n17\n77\n24\nregression\nrcv1\n38853\n781265\n23149\nclassi\u001ccation\nBig_Ads\n3 \u00d7 109\n26 \u00d7 106\n2.7 \u00d7 106\nclassi\u001ccation\nin cross validation. However, if we can tolerate 1.3% decrease in accuracy (instead of 1% as for\nother datasets) during cross validation, Vowpal Wabbit is able to achieve 91.4% reduction, indicating\nthat a large reduction is still possible at the tiny additional cost of 0.3% accuracy loss. With this\nslightly more aggressive sparsi\u001ccation, the test-set accuracy drops from 95.9% (when only 1% loss\nin accuracy is allowed in cross validation) to 95.4%, while the accuracy without sparsi\u001ccation is\n96.5%.\nEven for the original UCI datasets without arti\u001ccially added features, Vowpal Wabbit manages\nto \u001clter out some of the less useful features while maintaining the same level of performance. For\nexample, for the ad dataset, a reduction of 83.4% is achieved. Compared to the results above, it\nseems the most e\u001bective feature reductions occur on datasets with a large number of less useful\nfeatures, exactly where sparsi\u001ccation is needed.\nFor rcv1, more than 75% of features are removed after the sparsi\u001ccation process, indicating the\ne\u001bectiveness of our algorithm in real-life problems. We were not able to try many parameters in\ncross validation because of the size of rcv1. It is expected that more reduction is possible when a\nmore thorough parameter search is performed.\nThe previous results do not exercise the full power of the approach presented here because\nthey are applied to datasets where standard Lasso regularization [13] is or may be computationally\nviable. We have also applied this approach to a large non-public dataset Big_Ads where the goal is\npredicting which of two ads was clicked on given context information (the content of ads and query\ninformation). Here, accepting a 0.009 increase in classi\u001ccation error allows us to reduce the number\nof features from about 3 \u00d7 109 to about 24 \u00d7 106, a factor of 125 decrease in the number of features.\nFor classi\u001ccation tasks, we also study how our sparsi\u001ccation solution a\u001bects AUC (Area Under\nthe ROC Curve), which is a standard metric for classi\u001ccation.1 Using the same sets of parameters\nfrom 10-fold cross validation described above, we \u001cnd that the criterion is not a\u001bected signi\u001ccantly\nby sparsi\u001ccation and in some cases, they are actually slightly improved. The reason may be that our\n1We use AUC here and in later subsections because it is insensitive to threshold, which is unlike accuracy.\n13\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\nBig_Ads\nrcv1\nzoo\nwpbc\nwdbc\nwbc\nspam\nshroom\nmagic04\nkrvskp\nhousing\ncrx\nad\nFraction Left\nDataset\nFraction of Features Left\nBase data\n1000 extra\nFigure 1:\nA plot showing the amount of features left after sparsi\u001ccation for each dataset. The\n\u001crst result is the fraction of features left when the performance is changed by at most 1% due to\nsparsi\u001ccation. The second result is the % sparsi\u001ccation when 1000 random features are added to\neach example. For rcv1 and Big_Ads there is no second column, since the experiment is not useful.\nsparsi\u001ccation method remove some of the features that could have confused Vowpal Wabbit . The\nratios of the AUC with and without sparsi\u001ccation for all classi\u001ccation tasks are plotted in Figures 2.\nIt is often the case that these ratios are above 98%.\n6.2\nThe E\u001bects of K\nAs we argued before, using a K value larger than 1 may be desired in truncated gradient and the\nrounding algorithms. This advantage is empirically demonstrated here. In particular, we try K = 1,\nK = 10, and K = 20 in both algorithms. As before, cross validation is used to select parameters\nin the rounding algorithm, including learning rate \u03b7, number of passes of data during training, and\nlearning rate decay over training passes.\nFigures 3 and 4 give the AUC vs. number-of-feature plots, where each data point is generated\nby running respective algorithm using a di\u001berent value of g (for truncated gradient) and \u03b8 (for the\nrounding algorithm). We used \u03b8 = \u221ein truncated gradient.\nFor truncated gradient, the performances with K = 10 or 20 are at least as good as those with\nK = 1, and for the spambase dataset further feature reduction is achieved at the same level of\nperformance, reducing the number of features from 76 (when K = 1) to 25 (when K = 10 or 20)\nwith of an AUC of about 0.89.\nSuch an e\u001bect is even more remarkable in the rounding algorithm. For instance, in the ad dataset\nthe algorithm using K = 1 achieves an AUC of 0.94 with 322 features, while 13 and 7 features are\nneeded using K = 10 and K = 20, respectively.\n6.3\nThe E\u001bects of \u03b8 in Truncated Gradient\nIn this subsection, we empirically study the e\u001bect of \u03b8 in truncated gradient. The rounding algorithm\nis also included for comparison due to its similarity to truncated gradient when \u03b8 = g. As before, we\n14\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\n 1.2\nrcv1\nwpbc\nwdbc\nwbc\nspam\nshroom\nmagic04\nkrvskp\ncrx\nad\nRatio\nDataset\nRatio of AUC\nBase data\n1000 extra\nFigure 2:\nA plot showing the ratio of the AUC when sparsi\u001ccation is used over the AUC when\nno sparsi\u001ccation is used. The same process as in Figure 1 is used to determine empirically good\nparameters. The \u001crst result is for the original dataset, while the second result is for the modi\u001ced\ndataset where 1000 random features are added to each example.\nused cross validation to choose parameters for each \u03b8 value tried, and focused on the AUC metric\nin the eight UCI classi\u001ccation tasks, except the degenerate one of wpbc. We \u001cxed K = 10 in both\nalgorithm.\nFigure 5 gives the AUC vs. number-of-feature plots, where each data point is generated by\nrunning respective algorithms using a di\u001berent value of g (for truncated gradient) and \u03b8 (for the\nrounding algorithm). A few observations are in place. First, the results verify the observation that\nthe behavior of truncated gradient with \u03b8 = g is similar to the rounding algorithm. Second, these\nresults suggest that, in practice, it may be desired to use \u03b8 = \u221ein truncated gradient because it\navoids the local minimum problem.\n6.4\nComparison to Other Algorithms\nThe next set of experiments compares truncated gradient descent to other algorithms regarding their\nabilities to tradeo\u001b feature sparsi\u001ccation and performance. Again, we focus on the AUC metric in\nUCI classi\u001ccation tasks except wpdc. The algorithms for comparison include:\n\u2022 The truncated gradient algorithm: We \u001cxed K = 10 and \u03b8 = \u221e, used crossed-validated\nparameters, and altered the gravity parameter g.\n\u2022 The rounding algorithm described in section 3.1: We \u001cxed K = 10, used cross-validated\nparameters, and altered the rounding threshold \u03b8.\n\u2022 The subgradient algorithm described in section 3.2: We \u001cxed K = 10, used cross-validated\nparameters, and altered the regularization parameter g.\n\u2022 The Lasso [13] for batch L1 regularization: We used a publicly available implementation [12].\n15\nNote that we do not attempt to compare these algorithms on rcv1 and Big_Ads simply because\ntheir sizes are too large for the Lasso and subgradient descent (c.f., section 5).\nFigure 6 gives the results. First, it is observed that truncated gradient is consistently competitive\nwith the other two online algorithms and signi\u001ccantly outperformed them in some problems. This\nsuggests the e\u001bectiveness of truncated gradient.\nSecond, it is interesting to observe that the qualitative behavior of truncated gradient is often\nsimilar to that of LASSO, especially when very sparse weight vectors are allowed (the left sides in\nthe graphs). This is consistent with theorem 3.2 showing the relation between these two algorithms.\nHowever, LASSO usually has worse performance when the allowed number of nonzero weights is set\ntoo large (the right side of the graphs). In this case, LASSO seems to over\u001ct. In contrast, truncated\ngradient is more robust to over\u001ctting. The robustness of online learning is often attributed to early\nstopping, which has been extensively discussed in the literature (e.g., in [14]).\nFinally, it is worth emphasizing that the experiments in this subsection try to shed some light\non the relative strengths of these algorithms in terms of feature sparsi\u001ccation. For large datasets\nsuch as Big_Ads only truncated gradient, coe\u001ecient rounding, and the sub-gradient algorithms\nare applicable to large-scale problems with sparse features. As we have shown and argued, the\nrounding algorithm is quite ad hoc and may not work robustly in some problems, and the sub-\ngradient algorithm does not lead to sparsity in general during training.\n7\nConclusion\nThis paper covers the \u001crst sparsi\u001ccation technique for large-scale online learning with strong the-\noretical guarantees.\nThe algorithm, truncated gradient, is the natural extension of Lasso-style\nregression to the online-learning setting. Theorem 3.1 proves that the technique is sound: it never\nharms performance much compared to standard stochastic gradient descent in adversarial situations.\nFurthermore, we show that the asymptotic solution of one instance of the algorithm is essentially\nequivalent to Lasso regression, and thus justifying the algorithm's ability to produce sparse weight\nvectors when the number of features is intractably large.\nThe theorem is veri\u001ced experimentally in a number of problems. In some cases, especially for\nproblems with many irrelevant features, this approach achieves a one or two order of magnitude\nreduction in the number of features.\nReferences\n[1] Arthur Asuncion and David J. Newman.\nUCI machine learning repository,\n2007.\nUniversity\nof\nCalifornia,\nIrvine,\nSchool\nof\nInformation\nand\nComputer\nSciences,\nhttp://www.ics.uci.edu/\u223cmlearn/MLRepository.html.\n[2] Nicol\u00f2 Cesa-Bianchi, Philip M. Long, and Manfred Warmuth. Worst-case quadratic loss bounds\nfor prediction using linear functions and gradient descent. IEEE Transactions on Neural Net-\nworks, 7(3):604\u0015619, 1996.\n[3] Cheng-Tao Chu, Sang Kyun Kim, Yi-An Lin, YuanYuan Yu, Gary Bradski, Andrew Y. Ng,\nand Kunle Olukotun. Map-reduce for machine learning on multicore. In Advances in Neural\nInformation Processing Systems 20 (NIPS-07), 2008.\n16\n[4] Ofer Dekel, Shai Shalev-Schwartz, and Yoram Singer. The Forgetron: A kernel-based percep-\ntron on a \u001cxed budget. In Advances in Neural Information Processing Systems 18 (NIPS-05),\npages 259\u0015266, 2006.\n[5] John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra. E\u001ecient projections\nonto the \u21131-ball for learning in high dimensions. In ICML'08, 2008.\n[6] Jyrki Kivinen and Manfred K. Warmuth. Exponentiated gradient versus gradient descent for\nlinear predictors. Information and Computation, 132(1):1\u001563, 1997.\n[7] John Langford, Lihong Li, and Alexander L. Strehl. Vowpal Wabbit (fast online learning),\n2007. http://hunch.net/\u223cvw/.\n[8] David D. Lewis, Yiming Yang, Tony G. Rose, and Fan Li. RCV1: A new benchmark collection\nfor text categorization research. Journal of Machine Learning Research, 5:361\u0015397, 2004.\n[9] Nick Littlestone. Learning quickly when irrelevant attributes abound: A new linear-threshold\nalgorithms. Machine Learning, 2(4):285\u0015318, 1988.\n[10] Nick Littlestone, Philip M. Long, and Manfred K. Warmuth. On-line learning of linear func-\ntions. Computational Complexity, 5(2):1\u001523, 1995.\n[11] Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro.\nPegasos: Primal Estimated sub-\nGrAdient SOlver for SVM. In Proceedings of the Twenty-Fourth International Conference on\nMachine Learning (ICML-07), 2007.\n[12] Karl Sj\u00f6strand. Matlab implementation of LASSO, LARS, the elastic net and SPCA, June\n2005. Version 2.0, http://www2.imm.dtu.dk/pubdb/p.php?3897.\n[13] Robert Tibshirani.\nRegression shrinkage and selection via the lasso.\nJournal of the Royal\nStatistical Society, B., 58(1):267\u0015288, 1996.\n[14] Tong Zhang. Solving large scale linear prediction problems using stochastic gradient descent\nalgorithms. In Proceedings of the Twenty-First International Conference on Machine Learning\n(ICML-04), pages 919\u0015926, 2004.\n[15] Martin Zinkevich. Online convex programming and generalized in\u001cnitesimal gradient ascent.\nIn Proceedings of the Twentieth International Conference on Machine Learning (ICML-03),\npages 928\u0015936, 2003.\nA\nProof of Theorem 3.1\nThe following lemma is the essential step in our analysis.\nLemma A.1 For update rule (6) applied to weight vector w on example z = (x, y) with gravity\nparameter gi = g, resulting in a weight vector w\u2032. If Assumption 3.1 holds, then for all \u00afw \u2208Rd, we\nhave\n(1 \u22120.5A\u03b7)L(w, z) + g\u2225w\u2032 \u00b7 I(|w\u2032| \u2264\u03b8)\u22251\n\u2264L( \u00afw, z) + g\u2225\u00afw \u00b7 I(|w\u2032| \u2264\u03b8)\u22251 + \u03b7\n2B + \u2225\u00afw \u2212w\u22252 \u2212\u2225\u00afw \u2212w\u2032\u22252\n2\u03b7\n.\n17\nProof.\nConsider any target vector \u00afw \u2208Rd and let \u02dcw = w \u2212\u03b7\u22071L(w, z).\nWe have w\u2032 =\nT1( \u02dcw, g\u03b7, \u03b8). Let\nu( \u00afw, w\u2032) = g\u2225\u00afw \u00b7 I(|w\u2032| \u2264\u03b8)\u22251 \u2212g\u2225w\u2032 \u00b7 I(|w\u2032| \u2264\u03b8)\u22251.\nThen the update equation implies the following:\n\u2225\u00afw \u2212w\u2032\u22252\n\u2264\u2225\u00afw \u2212w\u2032\u22252 + \u2225w\u2032 \u2212\u02dcw\u22252\n=\u2225\u00afw \u2212\u02dcw\u22252 \u22122( \u00afw \u2212w\u2032)T (w\u2032 \u2212\u02dcw)\n\u2264\u2225\u00afw \u2212\u02dcw\u22252 + 2\u03b7u( \u00afw, w\u2032)\n=\u2225\u00afw \u2212w\u22252 + \u2225w \u2212\u02dcw\u22252 + 2( \u00afw \u2212w)T (w \u2212\u02dcw) + 2\u03b7u( \u00afw, w\u2032)\n=\u2225\u00afw \u2212w\u22252 + \u03b72\u2225\u22071L(w, z)\u22252 + 2\u03b7( \u00afw \u2212w)T \u22071L(w, z) + 2\u03b7u( \u00afw, w\u2032)\n\u2264\u2225\u00afw \u2212w\u22252 + \u03b72\u2225\u22071L(w, z)\u22252 + 2\u03b7(L( \u00afw, z) \u2212L(w, z)) + 2\u03b7u( \u00afw, w\u2032)\n\u2264\u2225\u00afw \u2212w\u22252 + \u03b72(AL(w, z) + B) + 2\u03b7(L( \u00afw, z) \u2212L(w, z)) + 2\u03b7u( \u00afw, w\u2032).\nHere, the \u001crst and second equalities follow from algebra, and the third from the de\u001cnition of \u02dcw.\nThe \u001crst inequality follows because a square is always non-negative. The second inequality follows\nbecause w\u2032 = T1( \u02dcw, g\u03b7, \u03b8), which implies that (w\u2032 \u2212\u02dcw)T w\u2032 = \u2212g\u03b7\u2225w\u2032 \u00b7I(|w\u2032| \u2264\u03b8)\u22251 and |w\u2032\nj \u2212\u02dcwj| \u2264\ng\u03b7I(|w\u2032\nj| \u2264\u03b8). Therefore\n\u2212( \u00afw \u2212w\u2032)T (w\u2032 \u2212\u02dcw) = \u2212\u00afwT (w\u2032 \u2212\u02dcw) + w\u2032T (w\u2032 \u2212\u02dcw)\n\u2264\nd\nX\nj=1\n| \u00afwj||w\u2032\nj \u2212\u02dcwj| + (w\u2032 \u2212\u02dcw)T w\u2032\n\u2264g\u03b7\nd\nX\nj=1\n| \u00afwj|I(|w\u2032\nj| \u2264\u03b8) + (w\u2032 \u2212\u02dcw)T w\u2032 = \u03b7u( \u00afw, w\u2032).\nThe third inequality follows from the de\u001cnition of sub-gradient of a convex function, which implies\nthat\n( \u00afw \u2212w)T \u22071L(w, z) \u2264L( \u00afw, z) \u2212L(w, z)\nfor all w and \u00afw.\nThe fourth inequality follows from Assumption 3.1.\nRearranging the above\ninequality leads to the desired bound.\n\u25a1\nProof.\n(of theorem 3.1) Apply Lemma A.1 to the update on trial i, we have\n(1 \u22120.5A\u03b7)L(wi, zi) + gi\u2225wi+1 \u00b7 I(|wi+1| \u2264\u03b8)\u22251\n\u2264L( \u00afw, zi) + \u2225\u00afw \u2212wi\u22252 \u2212\u2225\u00afw \u2212wi+1\u22252\n2\u03b7\n+ gi\u2225\u00afw \u00b7 I(|wi+1| \u2264\u03b8)\u22251 + \u03b7\n2B.\n18\nNow summing over i = 1, 2, . . . , T, we obtain\nT\nX\ni=1\n[(1 \u22120.5A\u03b7)L(wi, zi) + gi\u2225wi+1 \u00b7 I(|wi+1| \u2264\u03b8)\u22251]\n\u2264\nT\nX\ni=1\n\u0014\u2225\u00afw \u2212wi\u22252 \u2212\u2225\u00afw \u2212wi+1\u22252\n2\u03b7\n+ L( \u00afw, zi) + gi\u2225\u00afw \u00b7 I(|wi+1| \u2264\u03b8)\u22251 + \u03b7\n2B\n\u0015\n=\n\u2225\u00afw \u2212w1\u22252 \u2212\u2225\u00afw \u2212wT \u22252\n2\u03b7\n+ \u03b7\n2TB +\nT\nX\ni=1\n[L( \u00afw, zi) + gi\u2225\u00afw \u00b7 I(|wi+1| \u2264\u03b8)\u22251]\n\u2264\n\u2225\u00afw\u22252\n2\u03b7\n+ \u03b7\n2TB +\nT\nX\ni=1\n[L( \u00afw, zi) + gi\u2225\u00afw \u00b7 I(|wi+1| \u2264\u03b8)\u22251].\nThe \u001crst equality follows from the telescoping sum and the second inequality follows from the initial\ncondition (all weights are zero) and dropping negative quantities. The theorem follows by dividing\nwith respect to T and rearranging terms.\n\u25a1\n19\n10\n0\n10\n1\n10\n2\n10\n3\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nad\nNumber of Features\nAUC\n \n \nK=1\nK=10\nK=20\n10\n0\n10\n1\n10\n2\n10\n3\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\ncrx\nNumber of Features\nAUC\n \n \nK=1\nK=10\nK=20\n10\n0\n10\n1\n10\n2\n10\n3\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nkrvskp\nNumber of Features\nAUC\n \n \nK=1\nK=10\nK=20\n10\n0\n10\n1\n10\n2\n10\n3\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nmagic04\nNumber of Features\nAUC\n \n \nK=1\nK=10\nK=20\n10\n0\n10\n1\n10\n2\n10\n3\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nmushroom\nNumber of Features\nAUC\n \n \nK=1\nK=10\nK=20\n10\n0\n10\n1\n10\n2\n10\n3\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nspambase\nNumber of Features\nAUC\n \n \nK=1\nK=10\nK=20\n10\n0\n10\n1\n10\n2\n10\n3\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nwbc\nNumber of Features\nAUC\n \n \nK=1\nK=10\nK=20\n10\n0\n10\n1\n10\n2\n10\n3\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nwdbc\nNumber of Features\nAUC\n \n \nK=1\nK=10\nK=20\nFigure 3: E\u001bect of K on AUC in truncated gradient.\n20\n10\n0\n10\n1\n10\n2\n10\n3\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nad\nNumber of Features\nAUC\n \n \nK=1\nK=10\nK=20\n10\n0\n10\n1\n10\n2\n10\n3\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\ncrx\nNumber of Features\nAUC\n \n \nK=1\nK=10\nK=20\n10\n0\n10\n1\n10\n2\n10\n3\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nkrvskp\nNumber of Features\nAUC\n \n \nK=1\nK=10\nK=20\n10\n0\n10\n1\n10\n2\n10\n3\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nmagic04\nNumber of Features\nAUC\n \n \nK=1\nK=10\nK=20\n10\n0\n10\n1\n10\n2\n10\n3\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nmushroom\nNumber of Features\nAUC\n \n \nK=1\nK=10\nK=20\n10\n0\n10\n1\n10\n2\n10\n3\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nspambase\nNumber of Features\nAUC\n \n \nK=1\nK=10\nK=20\n10\n0\n10\n1\n10\n2\n10\n3\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nwbc\nNumber of Features\nAUC\n \n \nK=1\nK=10\nK=20\n10\n0\n10\n1\n10\n2\n10\n3\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nwdbc\nNumber of Features\nAUC\n \n \nK=1\nK=10\nK=20\nFigure 4: E\u001bect of K on AUC in the rounding algorithm.\n21\n10\n0\n10\n1\n10\n2\n10\n3\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nad\nNumber of Features\nAUC\n \n \nRounding Algorithm\nTrunc. Grad. (\u03b8=1g)\nTrunc. Grad. (\u03b8=\u221e)\n10\n0\n10\n1\n10\n2\n10\n3\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\ncrx\nNumber of Features\nAUC\n \n \nRounding Algorithm\nTrunc. Grad. (\u03b8=1g)\nTrunc. Grad. (\u03b8=\u221e)\n10\n0\n10\n1\n10\n2\n10\n3\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nkrvskp\nNumber of Features\nAUC\n \n \nRounding Algorithm\nTrunc. Grad. (\u03b8=1g)\nTrunc. Grad. (\u03b8=\u221e)\n10\n0\n10\n1\n10\n2\n10\n3\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nmagic04\nNumber of Features\nAUC\n \n \nRounding Algorithm\nTrunc. Grad. (\u03b8=1g)\nTrunc. Grad. (\u03b8=\u221e)\n10\n0\n10\n1\n10\n2\n10\n3\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nmushroom\nNumber of Features\nAUC\n \n \nRounding Algorithm\nTrunc. Grad. (\u03b8=1g)\nTrunc. Grad. (\u03b8=\u221e)\n10\n0\n10\n1\n10\n2\n10\n3\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nspambase\nNumber of Features\nAUC\n \n \nRounding Algorithm\nTrunc. Grad. (\u03b8=1g)\nTrunc. Grad. (\u03b8=\u221e)\n10\n0\n10\n1\n10\n2\n10\n3\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nwbc\nNumber of Features\nAUC\n \n \nRounding Algorithm\nTrunc. Grad. (\u03b8=1g)\nTrunc. Grad. (\u03b8=\u221e)\n10\n0\n10\n1\n10\n2\n10\n3\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nwdbc\nNumber of Features\nAUC\n \n \nRounding Algorithm\nTrunc. Grad. (\u03b8=1g)\nTrunc. Grad. (\u03b8=\u221e)\nFigure 5: E\u001bect of \u03b8 on AUC in truncated gradient.\n22\n10\n0\n10\n1\n10\n2\n10\n3\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nad\nNumber of Features\nAUC\n \n \nTrunc. Grad.\nRounding\nSub\u2212gradient\nLasso\n10\n0\n10\n1\n10\n2\n10\n3\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\ncrx\nNumber of Features\nAUC\n \n \nTrunc. Grad.\nRounding\nSub\u2212gradient\nLasso\n10\n0\n10\n1\n10\n2\n10\n3\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nkrvskp\nNumber of Features\nAUC\n \n \nTrunc. Grad.\nRounding\nSub\u2212gradient\nLasso\n10\n0\n10\n1\n10\n2\n10\n3\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nmagic04\nNumber of Features\nAUC\n \n \nTrunc. Grad.\nRounding\nSub\u2212gradient\nLasso\n10\n0\n10\n1\n10\n2\n10\n3\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nmushroom\nNumber of Features\nAUC\n \n \nTrunc. Grad.\nRounding\nSub\u2212gradient\nLasso\n10\n0\n10\n1\n10\n2\n10\n3\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nspambase\nNumber of Features\nAUC\n \n \nTrunc. Grad.\nRounding\nSub\u2212gradient\nLasso\n10\n0\n10\n1\n10\n2\n10\n3\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nwbc\nNumber of Features\nAUC\n \n \nTrunc. Grad.\nRounding\nSub\u2212gradient\nLasso\n10\n0\n10\n1\n10\n2\n10\n3\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nwdbc\nNumber of Features\nAUC\n \n \nTrunc. Grad.\nRounding\nSub\u2212gradient\nLasso\nFigure 6: Comparison of four algorithms.\n23\n",
        "sentence": " A growing body of recent work ([12], [37], [55], [40]), thus read shows that given sparse inputs to a linear model with any standard regularizer, sparsity can be fully exploited even if the gradient is not sparse (owing to regularization).",
        "context": "counterpart of the popular L1-regularization method in the batch setting. We prove that\nsmall rates of sparsi\u001ccation result in only small additional regret with respect to typical\nonline learning guarantees.\n3. The approach works well empirically.\nonline counterpart of L1-regularization methods. In the stochastic setting where the examples are\ndrawn iid from some underlying distribution, the sparse online gradient method proposed in this\npaper solves the L1 regularization problem.\n3.5\n(GD) or stochastic gradient descent (SGD). There are other variants, such as exponentiated gradient\ndescent (EG). Since our focus in this paper is sparsity, not GD versus EG, we shall only consider\nmodi\u001ccations of (3) for simplicity.\n3"
    },
    {
        "title": "Handwritten digit recognition with a backpropagation network. In Advances in neural information processing systems",
        "author": [
            "B Boser Le Cun",
            "John S Denker",
            "D Henderson",
            "Richard E Howard",
            "W Hubbard",
            "Lawrence D Jackel"
        ],
        "venue": null,
        "citeRegEx": "38",
        "shortCiteRegEx": "38",
        "year": 1990,
        "abstract": "",
        "full_text": "",
        "sentence": " Convolutional neural networks, developed by Yann LeCun, [38] are a variant of feedforward neural network that, since 2012, hold records in many computer vision tasks, such as object detection [36].",
        "context": null
    },
    {
        "title": "Efficient elastic net regularization for sparse linear models",
        "author": [
            "Zachary C Lipton",
            "Charles Elkan"
        ],
        "venue": null,
        "citeRegEx": "40",
        "shortCiteRegEx": "40",
        "year": 2015,
        "abstract": "This paper presents an algorithm for efficient training of sparse linear\nmodels with elastic net regularization. Extending previous work on delayed\nupdates, the new algorithm applies stochastic gradient updates to non-zero\nfeatures only, bringing weights current as needed with closed-form updates.\nClosed-form delayed updates for the $\\ell_1$, $\\ell_{\\infty}$, and rarely used\n$\\ell_2$ regularizers have been described previously. This paper provides\nclosed-form updates for the popular squared norm $\\ell^2_2$ and elastic net\nregularizers.\n  We provide dynamic programming algorithms that perform each delayed update in\nconstant time. The new $\\ell^2_2$ and elastic net methods handle both fixed and\nvarying learning rates, and both standard {stochastic gradient descent} (SGD)\nand {forward backward splitting (FoBoS)}. Experimental results show that on a\nbag-of-words dataset with $260,941$ features, but only $88$ nonzero features on\naverage per training example, the dynamic programming method trains a logistic\nregression classifier with elastic net regularization over $2000$ times faster\nthan otherwise.",
        "full_text": "arXiv:1505.06449v3  [cs.LG]  2 Jul 2015\nEf\ufb01cient Elastic Net Regularization\nfor Sparse Linear Models\nZachary C. Lipton\nCharles Elkan\nUniversity of California, San Diego\n{zlipton, elkan}@cs.ucsd.edu\nMay 27th, 2015\nAbstract\nThis paper presents an algorithm for ef\ufb01cient training of sparse linear mod-\nels with elastic net regularization. Extending previous work on delayed updates,\nthe new algorithm applies stochastic gradient updates to non-zero features only,\nbringing weights current as needed with closed-form updates. Closed-form de-\nlayed updates for the \u21131, \u2113\u221e, and rarely used \u21132 regularizers have been described\npreviously. This paper provides closed-form updates for the popular squared norm\n\u21132\n2 and elastic net regularizers. We provide dynamic programming algorithms that\nperform each delayed update in constant time. The new \u21132\n2 and elastic net methods\nhandle both \ufb01xed and varying learning rates, and both standard stochastic gradi-\nent descent (SGD) and forward backward splitting (FoBoS). Experimental results\nshow that on a bag-of-words dataset with 260, 941 features, but only 88 nonzero\nfeatures on average per training example, the dynamic programming method trains\na logistic regression classi\ufb01er with elastic net regularization over 2000 times faster\nthan otherwise.\n1\nIntroduction\nFor many applications of linear classi\ufb01cation or linear regression, training and test\nexamples are sparse, and with appropriate regularization, a \ufb01nal trained model can be\nsparse and still achieve high accuracy. It is therefore desirable to be able to train linear\nmodels using algorithms that require time that scales only with the number of non-zero\nfeature values.\nIncremental training algorithms such as stochastic gradient descent (SGD) are widely\nused to learn high-dimensional models from large-scale data. These methods process\neach example one at a time or in small batches, updating the model on the \ufb02y. When a\ndataset is sparse and the loss function is not regularized, this sparsity can be exploited\nby updating only the weights corresponding to non-zero feature values for each ex-\nample. However, to prevent over\ufb01tting to high-dimensional data, it is often useful to\n1\napply a regularization penalty, and speci\ufb01cally to impose a prior belief that the true\nmodel parameters are sparse and small in magnitude. Unfortunately, widely used reg-\nularizers such as \u21131 (lasso), \u21132\n2 (ridge), and elastic net (\u21131 + \u21132\n2) destroy the sparsity of\nthe stochastic gradient for each example, so they seemingly require most weights to be\nupdated for every example.\nThis paper builds upon methods for delayed updating of weights \ufb01rst described\nby [2], [11], and [6]. As each training example is processed, the algorithm updates only\nthose weights corresponding to non-zero feature values in the example. The model is\nbrought current as needed by \ufb01rst applying closed-form constant-time delayed updates\nfor each of these weights. For sparse data sets, the algorithm runs in time independent\nof the nominal dimensionality, scaling linearly with the number of non-zero feature\nvalues per example.\nTo date, constant-time delayed update formulas have been derived only for the \u21131\nand \u2113\u221eregularizers, and for the rarely used \u21132 regularizer. We extend previous work\nby showing the proper closed form updates for the popular \u21132\n2 squared norm regularizer\nand for elastic net regularization. When the learning rate varies (typically decreasing as\na function of time), we show that the elastic net update can be computed with a dynamic\nprogramming algorithm that requires only constant-time computation per update.1\nA straightforward experimental implementation of the proposed methods shows\nthat on a representative dataset containing the abstracts of a million articles from\nbiomedical literature, we can train a logistic regression classi\ufb01er with elastic net regu-\nlarization over 2000 times faster than using an otherwise identical implementation that\ndoes not take advantage of sparsity. Even if the standard implementation exploits spar-\nsity when making predictions during training, additionally exploiting sparsity when\ndoing updates, via dynamic programming, still makes learning 1400 times faster.\n2\nBackground and De\ufb01nitions\nWe consider a data matrix X \u2208\nR\nn\u00d7 d where each row xi is one of n examples and\neach column, indexed by j, corresponds to one of d features. We desire a linear model\nparametrized by a weight vector w \u2208\nR\nd that minimizes a convex objective function\nF(w) expressible as Pn\ni=1 Fi(w), where F is the loss with respect to the entire dataset\nX and Fi is the loss due to example xi.\nIn many datasets, the vast majority of feature values xij are zero. The-bag-of-words\nrepresentation of text is one such case. We say that such datasets are sparse. When\nfeatures correspond to counts or to binary values, as in bag-of-words, we sometimes\nsay say that a zero-valued entry xij is absent. We use p to refer to the average number of\nnonzero features per example. Naturally, when a dataset is sparse, we prefer algorithms\nthat take time O(p) per example to those that require time O(d).\n1The dynamic programming algorithms below for delayed updates with varying learning rates use time\nO(1) per update, but have space complexity O(T) where T is the total number of stochastic gradient updates.\nIf this space complexity is too great, that problem can be solved by allotting a \ufb01xed space budget and bringing\nall weights current whenever the budget is exhausted. As the cost of bringing weights current is amortized\nacross many updates, it adds negligibly to the total running time.\n2\n2.1\nRegularization\nTo prevent over\ufb01tting, regularization restricts the freedom of a model\u2019s parameters,\npenalizing their distance from some prior belief. Widely used regularizers penalize\nlarge weights with an objective function of the form\nF(w) = L(w) + R(w).\n(1)\nMany commonly used regularizers R(w) are of the form \u03bb||w|| where \u03bb determines\nthe strength of regularization and the \u21130, \u21131, \u21132\n2, or \u2113\u221enorms are common choices for\nthe penalty function. The \u21131 regularizer is popular owing to its tendency to produce\nsparse models. In this paper, we focus on elastic net, a linear combination of \u21131 and \u21132\n2\nregularization that has been shown to produce comparably sparse models to \u21131 while\noften achieving superior accuracy [13].\n2.2\nStochastic Gradient Descent\nGradient descent is a common strategy to learn optimal parameter values w. To mini-\nmize F(w), a number of steps T , indexed by t, are taken in the direction of the negative\ngradient:\nw(t+1) := w(t) \u2212\u03b7\nn\nX\ni=1\n\u2207Fi(w)\nwhere the learning rate \u03b7 may be a function of time t. An appropriately decreasing \u03b7\nensures that the algorithm will converge to a vector w within distance \u01eb of the optimal\nvector for any small value \u01eb [2].\nTraditional (or \u201cbatch\u201d) gradient descent requires a pass through the entire dataset\nfor each update. Stochastic gradient descent (SGD) circumvents this problem by up-\ndating the model once after visiting each example. With SGD, examples are randomly\nselected one at a time or in small so-called mini-batches. For simplicity of notation,\nwithout loss of generality we will assume that examples are selected one at a time. At\ntime t + 1 the gradient \u2207Fi(w(t)) is calculated with respect to the selected example\nxi, and then the model is updated according to the rule\nw(t+1) := w(t) \u2212\u03b7\u2207Fi(w(t)).\nBecause the examples are chosen randomly, the expected value of this noisy gradient\nis identical to the true value of the gradient taken with respect to the entire corpus.\nGiven a continuously differentiable convex objective function F(w), stochastic\ngradient descent is known to converge for learning rates \u03b7 that satisfy P\nt \u03b7t = \u221eand\nP\nt \u03b72\nt < \u221e[1]. Learning rates \u03b7i \u221d1/t and \u03b7i \u221d1/\n\u221a\nt both satisfy these properties.2\nFor many objective functions, such as those of linear or logistic regression without\nregularization, the noisy gradient \u2207Fi(w) is sparse when the input is sparse. In these\ncases, one needs only to update the weights corresponding to non-zero features in the\n2 Some common objective functions, such as those involving \u21131 regularization, are not differentiable\nwhen weights are equal to zero. However, forward backward splitting (FoBoS) offers a principled approach\nto this problem [11].\n3\ncurrent example xi. These updates require time O(p), where p \u226ad is the average\nnumber of nonzero features in an example.\nRegularization, however, can ruin the sparsity of the gradient. Consider an objec-\ntive function as in Equation (1), where R(w) = ||w|| for some norm || \u00b7 ||. In these\ncases, even when the feature value xij = 0, the partial derivative (\u2202/\u2202wj)Fi is nonzero\nowing to the regularization penalty if wj is nonzero. A simple optimization is to up-\ndate a weight only when either the weight or feature is nonzero. Given feature sparsity\nand persistent model sparsity throughout training, not updating wj when wj = 0 and\nxij = 0 provides a substantial bene\ufb01t. But such an approach still scales with the size\nof the model, which may be far larger than p. In contrast, the algorithms below scale\nin time complexity O(p).\n2.3\nForward Backward Splitting\nProximal algorithms are an approach to optimization in which each update consists\nof solving a convex optimization problem [9]. Forward Backward Splitting (FoBoS)\n[11] is a proximal algorithm that provides a principled approach to online optimization\nwith non-smooth regularizers. We \ufb01rst step in the direction of the negative gradient of\nthe differentiable unregularized loss function. We then update the weights by solving\na convex optimization problem that simultaneously penalizes distance from the new\nparameters and minimizes the regularization term.\nIn FoBoS, \ufb01rst a standard unregularized stochastic gradient step is applied:\nw(t+ 1\n2 ) = w(t) \u2212\u03b7(t)\u2207Li(w(t)).\nNote that if (\u2202/\u2202wj)Li = 0 then w\n(t+ 1\n2 )\nj\n= w(t)\nj . Then a convex optimization is\nsolved, applying the regularization penalty. For elastic net the problem to be solved is\nw(t+1) = argminw\n\u00121\n2||w \u2212w(t+ 1\n2 )||2\n2 + \u03b7t\u03bb1||w||1 + 1\n2\u03b7t\u03bb2||w||2\n2\n\u0013\n.\n(2)\nThe problems corresponding to \u21131 or \u21132\n2 separately can be derived by setting the corre-\nsponding \u03bb to 0.\n3\nLazy Updates\nThe idea of lazy updating was introduced in [2], [6], and [11]. This paper extends the\nidea for the cases of \u21132\n2 and elastic net regularization. The essence of the approach is\ngiven in Algorithm (1). We maintain an array \u03c8 \u2208\nRd in which each \u03c8j stores the\nindex of the last iteration at which the value of weight j was current. When process-\ning example xi at time k, we iterate through its nonzero features xij. For each such\nnonzero feature, we lazily apply the k \u2212\u03c8j delayed updates collectively in constant\ntime, bringing its weight wj current. Using the updated weights, we compute the pre-\ndiction \u02c6y(k) with the fully updated relevant parameters from w(k). We then compute\nthe gradient and update these parameters.\n4\nAlgorithm 1 Lazy Updates\nRequire: \u03c8 \u2208\nRd\nfor t \u22081, ..., T do\nSample xi randomly from the training set\nfor j s.t. xij \u0338= 0 do\nwj \u2190Lazy(wj, t, \u03c8j)\n\u03c8j \u2190t\nend for\nw \u2190w \u2212\u2207Fi(w)\nend for\nWhen training is complete, we pass once over all nonzero weights to apply the\ndelayed updates to bring the model current. Provided that we can apply any number\nk \u2212\u03c8j of delayed updates in O(1) time, the algorithm processes each example in O(p)\ntime regardless of the dimension d.\nTo use the approach with a chosen regularizer, it remains only to demonstrate the\nexistence of constant time updates. In the following subsections, we derive constant-\ntime updates for \u21131, \u21132\n2 and elastic net regularization, starting with the simple case\nwhere the learning rate \u03b7 is \ufb01xed during each epoch, and extending to the more com-\nplicated case when the learning rate is decreased every iteration as a function of time.3\n4\nPrior Work\nOver the last several years, a large body of work has advanced the \ufb01eld of online\nlearning. Notable contributions include ways of adaptively decreasing the learning rate\nseparately for each parameter such as AdaGrad [3] and AdaDelta [12], using small\nbatches to reduce the variance of the noisy gradient [7], and other variance reduction\nmethods such as Stochastic Average Gradient (SAG) [10] and Stochastic Variance Re-\nduced Gradient (SVRG) [5].\nIn 2008, Carpenter described an idea for performing lazy updates for stochastic\ngradient descent [2]. With that method, we maintain a vector \u03c8 \u2208\nNd, where each \u03c8i\nstores the index of the last epoch in which each weight was last regularized. We then\nperform periodic batch updates. However, as the paper acknowledges, the approach\ndescribed results in updates that do not produce the same result as applying an update\nafter each time step.\nLangford et al. concurrently developed an approach for lazily updating \u21131 regular-\nized linear models [6]. They restrict attention to \u21131 models. Additionally, they describe\nthe closed form update only when the learning rate \u03b7 is constant, although they suggest\nthat an update can be derived when \u03b7t decays as t grows large. We derive constant-time\nupdates for \u21132\n2 and elastic net regularization. Our algorithms are applicable with both\n\ufb01xed and varying learning rates.\n3 The results hold for schedules of weight decrease that depend on time, but cannot be directly applied to\nAdaGrad [3] or RMSprop, methods where each weight has its own learning rate which is decreased with the\ninverse of the accumulated sum (or moving average) of squared gradients with respect to that weight.\n5\nIn 2008 also, as mentioned above, Duchi and Singer described the FoBoS method\n[4]. They share the insight of applying updates lazily when training on sparse high-\ndimensional data. Their lazy updates hold for norms \u2113q for q \u2208{1, 2, \u221e}, However\nthey do not hold for the commonly used \u21132\n2 squared norm. Consequently they also\ndo not hold for mixed regularizers involving \u21132\n2 such as the widely used elastic net\n(\u21131 + \u21132\n2).\n5\nConstant-Time Lazy Updating for SGD\nIn this section, we derive constant-time stochastic gradient updates for use when pro-\ncessing examples from sparse datasets. Using these, the lazy update algorithm can\ntrain linear models with time complexity O(p) per example. For brevity, we describe\nthe more general case where the learning rate is varied. When the learning rate is\nconstant the algorithm can be easily modi\ufb01ed to have O(1) space complexity.\n5.1\nLazy \u21131 Regularization with Decreasing Learning Rate\nThe closed-form update for \u21131 regularized models is [11]\nw(k)\nj\n= sgn(w(\u03c8j)\nj\n)\nh\n|w(\u03c8j)\nj\n| \u2212\u03bb1 (S(k \u22121) \u2212S(\u03c8j \u22121))\ni\n+\nwhere S(t) is a function that returns the partial sum Pt\n\u03c4=0 \u03b7(\u03c4). The sum Pt+n\u22121\n\u03c4=t\n\u03b7(\u03c4)\ncan be computed in constant time using a caching approach. On each iteration t, we\ncompute S(t) in constant time given its predecessor as S(t) = \u03b7(t) + S(t \u22121). The\nbase case for this recursion is S(0) = \u03b7(0). We then cache this value in an array for\nsubsequent constant time lookup.\nWhen the learning rate decays with 1/t, the terms \u03b7(\u03c4) follow the harmonic series,.\nEach partial sum of the harmonic series is a harmonic number H(t) = Pt\ni=1 1/t.\nClearly\nt+n\u22121\nX\n\u03c4=t\n\u03b7(\u03c4) = \u03b7(0) (H(t + n) \u2212H(t))\nwhere H\u03c4 is the \u03c4th harmonic number. While there is no closed-form expression to\ncalculate the \u03c4th harmonic number, there exist good approximations.\nThe O(T ) space complexity of this algorithm may seem problematic. However,\nthis problem is easily dealt with by bringing all weights current after each epoch. The\ncost to do so is amortized across all iterations and is thus negligible.\n5.2\nLazy \u21132\n2 Regularization with Decreasing Learning Rate\nFor a given example xi, if the feature value xij = 0 and the learning rate is varying,\nthen the stochastic gradient update rule for an \u21132\n2 regularized objective is\nw(t+1)\nj\n= w(t)\nj\n\u2212\u03b7(t)\u03bb2w(t)\nj .\n6\nThe decreasing learning rate prevents collecting successive updates as terms in a geo-\nmetric series, as we could if the learning rate were \ufb01xed. However, we can employ a\ndynamic programming strategy.\nLemma 1. For SGD with \u21132\n2 regularization, the constant-time lazy update to bring a\nweight current from iteration \u03c8j to k is\nw(k)\nj\n= w(\u03c8j)\nj\nP(k \u22121)\nP(\u03c8j \u22121)\nwhere P(t) is the partial product Qt\n\u03c4=0(1 \u2212\u03b7(\u03c4)\u03bb2).\nProof. Rewriting the multiple update expression yields\nw(t+1)\nj\n= w(t)\nj (1 \u2212\u03b7(t)\u03bb2)\nw(t+n)\nj\n= w(t)\nj (1 \u2212\u03b7(t)\u03bb2)(1 \u2212\u03b7(t+1)\u03bb2) \u00b7 ... \u00b7 (1 \u2212\u03b7(t+n\u22121)\u03bb2).\nThe products P(t) = Qt\n\u03c4=0(1 \u2212\u03b7(\u03c4)\u03bb2) can be cached on each iteration in constant\ntime using the recursive relation\nP(t) = (1 \u2212\u03b7(t)\u03bb2)P(t \u22121).\nThe base case is P(0) = a0 = (1 \u2212\u03b70\u03bb2). Given cached values P(0), ..., P(t + n), it\nis then easy to calculate the exact lazy update in constant time:\nw(t+n)\nj\n= w(t)\nj\nP(t + n \u22121)\nP(t \u22121)\n.\nThe claim follows.\nAs in the case of \u21132\n2 regularization with \ufb01xed learning rate, we need not worry that\nthe regularization update will \ufb02ip the sign of the weight wj, because P(t) > 0 for all\nt \u22650.\n5.3\nLazy Elastic Net Regularization with Decreasing Learning Rate\nNext, we derive the constant time lazy update for SGD with elastic net regularization.\nRecall that a model regularized by elastic net has an objective function of the form\nF(w) = L(w) + \u03bb1||w||1 + \u03bb2\n2 ||w||2\n2.\nWhen a feature xj = 0, the SGD update rule is\nw(t+1)\nj\n= sgn(w(t)\nj )\nh\n|w(t)\nj | \u2212\u03b7(t)\u03bb1 \u2212\u03b7(t)\u03bb2|w(t)\nj |\ni\n+\n= sgn(w(t)\nj )\nh\n(1 \u2212\u03b7(t)\u03bb2)|w(t)\nj | \u2212\u03b7(t)\u03bb1\ni\n+\n(3)\n7\nTheorem 1. To bring the weight wj current from time \u03c8j to time k using repeated\nEquation (3) updates, the constant time update is\nw(k)\nj\n= sgn(w(\u03c8j)\nj\n)\n\u0014\n|w(\u03c8j)\nj\n| P(k \u22121)\nP(\u03c8j \u22121) \u2212P(k \u22121) \u00b7 (B(k \u22121) \u2212B(\u03c8j \u22121))\n\u0015\n+\nwhere P(t) = (1 \u2212\u03b7(t)\u03bb2) \u00b7 P(t \u22121) with base case P(\u22121) = 1 and B(t) =\nPt\n\u03c4=0 \u03b7(\u03c4)/P(\u03c4 \u22121) with base case B(\u22121) = 0.\nProof. The time-varying learning rate prevents us from working out a simple expan-\nsion. Instead, we can write the following inductive expression for consecutive terms in\nthe sequence:\nw(t+1)\nj\n= sgn(w(t)\nj )\nh\n(1 \u2212\u03b7(t)\u03bb2)|w(t)\nj | \u2212\u03b7(t)\u03bb1\ni\n+\nWriting a\u03c4 = (1 \u2212\u03b7(\u03c4)\u03bb2) and b\u03c4 = \u2212\u03b7(\u03c4)\u03bb1 gives\nw(t+1) = sgn(w(t)\nj )\nh\nat|w(t)| + bt\ni\n+\n...\nw(t+n) = sgn(w(t)\nj )\n\u0002\na(t+n\u22121)(...a(t+1)\n\u0000atwt \u2212bt\n\u0001\n\u2212b(t+1)...) \u2212b(t+n\u22121)\n\u0003\n+\n= sgn(w(t)\nj )\n\"\n|w(t)\nj |\nt+n\u22121\nY\n\u03c4=t\na\u03c4 +\nt+n\u22122\nX\n\u03c4=t\nbi\n t+n\u22122\nY\nq=\u03c4\naq\n!\n+ b(t+n\u22121)\n#\n+\nThe leftmost term Qt+n\u22121\n\u03c4=t\na\u03c4 can be calculated in constant time as P(t+n\u22121)/P(t\u2212\n1) using cached values from the dynamic programming scheme discussed in the pre-\nvious section. To cache the remaining terms, we group the center and rightmost terms\nand apply the simpli\ufb01cation\nt+n\u22122\nX\n\u03c4=t\nbi\n t+n\u22122\nY\nq=\u03c4\naq\n!\n+ bt+n\u22121\n= bt\nP(t + n \u22122)\nP(t \u22121)\n+ bt+1\nP(t + n \u22122)\nP(t)\n+ ... + bt+n\u22121\nP(t + n \u22122)\nP(t + n \u22122)\n= \u2212\u03bb1P(t + n \u22122)\n\u0012\n\u03b7(t)\nP(t \u22121) + \u03b7(t+1)\nP(t) + ... +\n\u03b7(t+n\u22121)\nP(t + n \u22122)\n\u0013\n.\nWe now add a new layer to the dynamic programming formulation. In addition to\nprecalculating all values P(t) as we go, we de\ufb01ne a partial sum over inverses of partial\nproducts\nB(t) =\nt\nX\n\u03c4=0\n\u03b7(\u03c4)\nP(\u03c4 \u22121).\nGiven that P(t\u22121) can be accessed in constant time at time t, B(t) can now be cached\nin constant time. With the base case B(\u22121) = 0, the dynamic programming here\n8\ndepends upon the recurrence relation\nB(t) = B(t \u22121) +\n\u03b7(t)\nP(t \u22121).\nThen, for SGD elastic net with decreasing learning rate, the update rule to apply\nany number n of consecutive regularization updates in constant time to weight wj is\nw(t+n) = sgn(w(t)\nj )\n\u0014\n|w(t)\nj |P(t + n \u22121)\nP(t \u22121)\n\u2212\u03bb1P(t + n \u22121) (B(t + n \u22121) \u2212B(t \u22121))\n\u0015\n+\n6\nLazy Updates for Forward Backward Splitting\nHere we turn our attention to FoBoS updates for \u21132\n2 and elastic net regularization. For\n\u21132\n2 regularization, to apply the regularization update we solve the problem from Equa-\ntion (2) with \u03bb1 set to 0. Solving for w\u2217gives the update\nw(t+1)\nj\n=\nw(t)\nj\n1 + \u03b7(t)\u03bb2\nwhen xij = 0. Note that this differs from the standard stochastic gradient descent step.\nWe can store the values \u03a6(t) = Qt\n\u03c4=0\n1\n1+\u03b7t\u03bb2 . Then, the constant time lazy update for\nFoBoS with \u21132\n2 regularization to bring a weight current at time k from time \u03c8j is\nw(k)\nj\n= w(\u03c8j)\nj\n\u03a6(k \u22121)\n\u03a6(\u03c8j \u22121)\nwhere \u03a6(t) = (1 + \u03b7(t)\u03bb2)\u22121 \u00b7 \u03a6(t \u22121) with base case \u03a6(0) =\n1\n1+\u03b70\u03bb2 .\nFinally, in the case of elastic net regularization via forward backward splitting, we\nsolve the convex optimization problem from Equation (2). This objective also comes\napart and can be optimized for each wj separately. Setting the derivative with respect\nto wj to zero yields the solution\nw(t+1)\nj\n= sgn(w(t)\nj )\n\"\n|w(t)\nj | \u2212\u03b7(t)\u03bb1\n\u03b7t\u03bb2 + 1\n#\n+\nTheorem 2. A constant-time lazy update for FoBoS with elastic net regularization and\ndecreasing learning rate to bring a weight current at time k from time \u03c8j is\nw(k)\nj\n= sgn(w(\u03c8j)\nj\n)\n\u0014\n|w(\u03c8j)\nj\n| \u03a6(k \u22121)\n\u03a6(\u03c8j \u22121) \u2212\u03a6(k \u22121) \u00b7 \u03bb1 (\u03b2(k \u22121) \u2212\u03b2(\u03c8j \u22121))\n\u0015\n+\nwhere \u03a6(t) = \u03a6(t\u22121)\u00b7\n1\n1+\u03b7t\u03bb2 with base case \u03a6(\u22121) = 1 and \u03b2(t) = \u03b2(t\u22121)+\n\u03b7(t)\n\u03a6(t\u22121)\nwith base case \u03b2(\u22121) = 0.\n9\nProof. Write at = (\u03b7(t)\u03bb2 + 1)\u22121 and bt = \u2212\u03b7(t)\u03bb1. Note that neither at nor bt\ndepends upon wj. Consider successive updates:\nw(t+1)\nj\n= sgn(w(t)\nj )\nh\nat(|w(t)\nj | + bt)\ni\n+\nw(t+n)\nj\n= sgn(w(t)\nj )\n\uf8ee\n\uf8f0|w(t)\nj |\nt+n\u22121\nY\n\u03b2=t\na\u03b2 +\nt+n\u22121\nX\n\u03c4=t\n \nb\u03c4\nt+n\u22121\nY\n\u03b1=\u03c4\na\u03b1\n!\uf8f9\n\uf8fb\n+\n.\nInside the square brackets, \u03a6(t+n\u22121)\n\u03a6(t\u22121)\ncan be substituted for Qt+n\u22121\n\u03b2=t\na\u03b2 and the second\nterm can be expanded as\nt+n\u22121\nX\n\u03c4=t\n \nb\u03c4\nt+n\u22121\nY\n\u03b1=\u03c4\na\u03b1\n!\n=\nt+n\u22121\nX\n\u03c4=t\n\u0012\nb\u03c4\n\u03a6(t + n \u22121)\n\u03a6(\u03c4 \u22121)\n\u0013\n= \u2212\u03a6(t + n \u22121) \u00b7 \u03bb1\nt+n\u22121\nX\n\u03c4=t\n\u0012\n\u03b7(\u03c4)\n\u03a6(\u03c4 \u22121)\n\u0013\nUsing the dynamic programming approach, for each time t, we calculate\n\u03b2(t) = \u03b2(t \u22121) +\n\u03b7(t)\n\u03a6(t \u22121)\nwith the base cases \u03b2(0) = \u03b7(0) and \u03b2(\u22121) = 0. Then\nw(t+n)\nj\n= sgn(w(t)\nj )\n\u0014\n|w(t)\nj |\u03a6(t + n \u22121)\n\u03a6(t \u22121)\n\u2212\u03a6(t + n \u22121) \u00b7 \u03bb1 (\u03b2(t + n \u22121) \u2212\u03b2(t \u22121))\n\u0015\n+\n7\nExperiments\nThe usefulness of logistic regression with elastic net regularization is well-known. To\ncon\ufb01rm the correctness and speed of the dynamic programming algorithm just pre-\nsented, we implemented it and tested it on a bag-of-words representation of abstracts\nfrom biomedical articles indexed in Medline as described in [8]. The dataset contains\nexactly 106 examples, 260, 941 features and an average of 88.54 nonzero features per\ndocument.\nWe implemented algorithms in Python. Datasets are represented by standard sparse\nSciPy matrices. We implemented both standard and lazy FoBoS for logistic regression\nregularized with elastic net. We con\ufb01rmed on a synthetic dataset that the standard\nFoBoS updates and lazy updates output essentially identical weights. To make a fair\ncomparison, we also report results where the non-lazy algorithm exploits sparsity when\ncalculating predictions. Even when both methods exploit sparsity to calculate \u02c6y, lazy\nupdates lead to training over 1400 times faster. Note that sparse data structures must\n10\nLazy Updates\nDense Updates\nDense with Sparse Predictions\nSGD\n.0102\n21.377\n14.381\nFoBoS\n.0120\n22.511\n16.785\nTable 1: Average time in seconds for each algorithm to process one example.\nbe used even with dense updates, because a dense matrix to represent the input dataset\nwould use an unreasonable amount of memory.\nLogistic regression with lazy elastic net regularization runs approximately 2000\ntimes faster than with dense regularization updates for both SGD and FoBoS. In the\nabsence of overhead, exploiting sparsity should yield a 2947\u00d7 speedup. Clearly the\nadditional dynamic programming calculations do not erode the bene\ufb01ts of exploiting\nsparsity. While the dynamic programming strategy consumes space linear in the num-\nber of iterations, it does not present a major time penalty. Concerning space, storing\ntwo \ufb02oating point numbers for each time step t is a modest use of space compared\nto storing the data itself. Further, if space ever were a problem, all weights could pe-\nriodically be brought current. The cost of this update would be amortized across all\niterations and thus would be negligible.\n8\nDiscussion\nMany interesting datasets are high-dimensional, and many high-dimensional datasets\nare sparse. To be useful, learning algorithms should have time complexity that scales\nwith the number of non-zero feature values per example, as opposed to with the nom-\ninal dimensionality. This paper provides algorithms for fast training of linear models\nwith \u21132\n2 or with elastic net regularization. Experiments con\ufb01rm the correctness and\nempirical bene\ufb01t of the method. In future work we hope to use similar ideas to take\nadvantage of sparsity in nonlinear models, such as the sparsity provided by recti\ufb01ed\nlinear activation units in modern neural networks.\nAcknowledgments\nThis research was conducted with generous support from the Division of Biomedical\nInformatics at the University of California, San Diego, which has funded the \ufb01rst au-\nthor via a training grant from the National Library of Medicine. Galen Andrew began\nevaluating lazy updates for multilabel classi\ufb01cation with Charles Elkan in the sum-\nmer of 2014. His notes provided an insightful starting point for this research. Sharad\nVikram provided invaluable help in checking the derivations of closed form updates.\nReferences\n[1] L\u00b4eon Bottou. Stochastic gradient descent tricks. In Neural Networks: Tricks of\nthe Trade, pages 421\u2013436. Springer, 2012.\n11\n[2] Bob Carpenter. Lazy sparse stochastic gradient descent for regularized multino-\nmial logistic regression. Alias-i, Inc., Tech. Rep, pages 1\u201320, 2008.\n[3] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for\nonline learning and stochastic optimization. The Journal of Machine Learning\nResearch, 12:2121\u20132159, 2011.\n[4] John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra. Ef\ufb01cient\nprojections onto the l 1-ball for learning in high dimensions. In Proceedings of\nthe 25th international conference on Machine learning, pages 272\u2013279. ACM,\n2008.\n[5] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using\npredictive variance reduction. In Advances in Neural Information Processing\nSystems, pages 315\u2013323, 2013.\n[6] John Langford, Lihong Li, and Tong Zhang. Sparse online learning via truncated\ngradient. In Advances in neural information processing systems, pages 905\u2013912,\n2009.\n[7] Mu Li, Tong Zhang, Yuqiang Chen, and Alexander J Smola. Ef\ufb01cient mini-batch\ntraining for stochastic optimization. In Proceedings of the 20th ACM SIGKDD\ninternational conference on Knowledge discovery and data mining, pages 661\u2013\n670. ACM, 2014.\n[8] Zachary C Lipton, Charles Elkan, and Balakrishnan Naryanaswamy. Optimal\nthresholding of classi\ufb01ers to maximize f1 measure. In Machine Learning and\nKnowledge Discovery in Databases, pages 225\u2013239. Springer, 2014.\n[9] Neal Parikh and Stephen Boyd. Proximal algorithms. Foundations and Trends in\noptimization, 1(3):123\u2013231, 2013.\n[10] Mark Schmidt, Nicolas Le Roux, and Francis Bach. Minimizing \ufb01nite sums with\nthe stochastic average gradient. arXiv preprint arXiv:1309.2388, 2013.\n[11] Yoram Singer and John C Duchi. Ef\ufb01cient learning using forward-backward split-\nting. In Advances in Neural Information Processing Systems, pages 495\u2013503,\n2009.\n[12] Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint\narXiv:1212.5701, 2012.\n[13] Hui Zou and Trevor Hastie. Regularization and variable selection via the elastic\nnet. Journal of the Royal Statistical Society: Series B (Statistical Methodology),\n67(2):301\u2013320, 2005.\n12\n",
        "sentence": " A growing body of recent work ([12], [37], [55], [40]), thus read shows that given sparse inputs to a linear model with any standard regularizer, sparsity can be fully exploited even if the gradient is not sparse (owing to regularization).",
        "context": "sparse models. In this paper, we focus on elastic net, a linear combination of \u21131 and \u21132\n2\nregularization that has been shown to produce comparably sparse models to \u21131 while\noften achieving superior accuracy [13].\n2.2\nStochastic Gradient Descent\n11\n[2] Bob Carpenter. Lazy sparse stochastic gradient descent for regularized multino-\nmial logistic regression. Alias-i, Inc., Tech. Rep, pages 1\u201320, 2008.\n[3] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for\nempirical bene\ufb01t of the method. In future work we hope to use similar ideas to take\nadvantage of sparsity in nonlinear models, such as the sparsity provided by recti\ufb01ed\nlinear activation units in modern neural networks.\nAcknowledgments"
    },
    {
        "title": "Optimal thresholding of classifiers to maximize F1 measure",
        "author": [
            "Zachary C Lipton",
            "Charles Elkan",
            "Balakrishnan Naryanaswamy"
        ],
        "venue": "In Machine Learning and Knowledge Discovery in Databases,",
        "citeRegEx": "41",
        "shortCiteRegEx": "41",
        "year": 2014,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Even in the straightforward case of binary classification, without time dependencies, commonly used performance metrics like F1 give rise to complicated thresholding strategies which may not accord with any reasonable intuition of what should constitute good performance [41].",
        "context": null
    },
    {
        "title": "A novel approach to on-line handwriting recognition based on bidirectional long short-term memory networks",
        "author": [
            "Marcus Liwicki",
            "Alex Graves",
            "Horst Bunke",
            "J\u00fcrgen Schmidhuber"
        ],
        "venue": "In Proc. 9th Int. Conf. on Document Analysis and Recognition,",
        "citeRegEx": "42",
        "shortCiteRegEx": "42",
        "year": 2007,
        "abstract": "",
        "full_text": "",
        "sentence": " ( [42], [25]), data is collected from a whiteboard using an eBeam interface.",
        "context": null
    },
    {
        "title": "Recurrent neural networks for noise reduction in robust ASR",
        "author": [
            "Andrew L Maas",
            "Quoc V Le",
            "Tyler M O\u2019Neil",
            "Oriol Vinyals",
            "Patrick Nguyen",
            "Andrew Y Ng"
        ],
        "venue": "In INTERSPEECH. Citeseer,",
        "citeRegEx": "43",
        "shortCiteRegEx": "43",
        "year": 2012,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " This approach has been used with deep belief nets for speech modeling in [43]. These units have been demonstrated to improve the performance of many deep neural networks ([43], [47], [73]) on tasks as varied as speech processing and object recognition, and have been used in recurrent neural networks by [7].",
        "context": null
    },
    {
        "title": "Deep captioning with multimodal recurrent neural networks (m-RNN)",
        "author": [
            "Junhua Mao",
            "Wei Xu",
            "Yi Yang",
            "Jiang Wang",
            "Alan Yuille"
        ],
        "venue": "arXiv preprint arXiv:1412.6632,",
        "citeRegEx": "44",
        "shortCiteRegEx": "44",
        "year": 2014,
        "abstract": "In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model\nfor generating novel image captions. It directly models the probability\ndistribution of generating a word given previous words and an image. Image\ncaptions are generated by sampling from this distribution. The model consists\nof two sub-networks: a deep recurrent neural network for sentences and a deep\nconvolutional network for images. These two sub-networks interact with each\nother in a multimodal layer to form the whole m-RNN model. The effectiveness of\nour model is validated on four benchmark datasets (IAPR TC-12, Flickr 8K,\nFlickr 30K and MS COCO). Our model outperforms the state-of-the-art methods. In\naddition, we apply the m-RNN model to retrieval tasks for retrieving images or\nsentences, and achieves significant performance improvement over the\nstate-of-the-art methods which directly optimize the ranking objective function\nfor retrieval. The project page of this work is:\nwww.stat.ucla.edu/~junhua.mao/m-RNN.html .",
        "full_text": "Published as a conference paper at ICLR 2015\nDEEP CAPTIONING WITH MULTIMODAL RECURRENT\nNEURAL NETWORKS (M-RNN)\nJunhua Mao\nUniversity of California, Los Angeles; Baidu Research\nmjhustc@ucla.edu\nWei Xu & Yi Yang & Jiang Wang & Zhiheng Huang\nBaidu Research\n{wei.xu,yangyi05,wangjiang03,huangzhiheng}@baidu.com\nAlan Yuille\nUniversity of California, Los Angeles\nyuille@stat.ucla.edu\nABSTRACT\nIn this paper, we present a multimodal Recurrent Neural Network (m-RNN) model\nfor generating novel image captions. It directly models the probability distribution\nof generating a word given previous words and an image. Image captions are gen-\nerated according to this distribution. The model consists of two sub-networks: a\ndeep recurrent neural network for sentences and a deep convolutional network for\nimages. These two sub-networks interact with each other in a multimodal layer\nto form the whole m-RNN model. The effectiveness of our model is validated on\nfour benchmark datasets (IAPR TC-12, Flickr 8K, Flickr 30K and MS COCO).\nOur model outperforms the state-of-the-art methods. In addition, we apply the\nm-RNN model to retrieval tasks for retrieving images or sentences, and achieves\nsigni\ufb01cant performance improvement over the state-of-the-art methods which di-\nrectly optimize the ranking objective function for retrieval. The project page of\nthis work is: www.stat.ucla.edu/\u02dcjunhua.mao/m-RNN.html. 1\n1\nINTRODUCTION\nObtaining sentence level descriptions for images is becoming an important task and it has many ap-\nplications, such as early childhood education, image retrieval, and navigation for the blind. Thanks\nto the rapid development of computer vision and natural language processing technologies, recent\nwork has made signi\ufb01cant progress on this task (see a brief review in Section 2). Many previous\nmethods treat it as a retrieval task. They learn a joint embedding to map the features of both sen-\ntences and images to the same semantic space. These methods generate image captions by retrieving\nthem from a sentence database. Thus, they lack the ability of generating novel sentences or describ-\ning images that contain novel combinations of objects and scenes.\nIn this work, we propose a multimodal Recurrent Neural Networks (m-RNN) model 2 to address\nboth the task of generating novel sentences descriptions for images, and the task of image and\nsentence retrieval. The whole m-RNN model contains a language model part, a vision part and a\nmultimodal part. The language model part learns a dense feature embedding for each word in the\n1Most recently, we adopt a simple strategy to boost the performance of image captioning task signi\ufb01cantly.\nMore details are shown in Section 8. The code and related data (e.g. re\ufb01ned image features and hypotheses\nsentences generated by the m-RNN model) are available at https://github.com/mjhucla/mRNN-CR.\n2A previous version of this work appears in the NIPS 2014 Deep Learning Workshop with the title \u201cExplain\nImages with Multimodal Recurrent Neural Networks\u201d http://arxiv.org/abs/1410.1090 (Mao et al.\n(2014)). We observed subsequent arXiv papers which also use recurrent neural networks in this topic and cite\nour work. We gratefully acknowledge them.\n1\narXiv:1412.6632v5  [cs.CV]  11 Jun 2015\nPublished as a conference paper at ICLR 2015\nRetr.\nGen.\n1. Tourists are sitting at a long table with \nbeer bottles on it in a rather dark restaurant \nand are raising their bierglaeser; \n2. Tourists are sitting at a long table with a \nwhite table-cloth in a somewhat dark \nrestaurant;\nTourists are sitting at a long table \nwith a white table cloth and are \neating;\n1. Top view of the lights of a city at \nnight, with a well-illuminated square \nin front of a church in the foreground;\n2. People on the stairs in front of an \nilluminated cathedral with two towers \nat night;\nA square with burning street lamps \nand a street in the foreground;\n1. A dry landscape with light brown \ngrass and green shrubs and trees in the \nforeground and large reddish-brown \nrocks and a blue sky in the background;\n2. A few bushes at the bottom and a \nclear sky in the background; \nA dry landscape with green trees and \nbushes and light brown grass in the \nforeground and reddish-brown round rock \ndomes and a blue sky in the background;\n1. Group picture of nine tourists and \none local on a grey rock with a lake \nin the background;\n2. Five people are standing and four \nare squatting on a brown rock in the \nforeground;\nA blue sky in the background;\nFigure 1: Examples of the generated and two top-ranked retrieved sentences given the query image\nfrom IAPR TC-12 dataset. The sentences can well describe the content of the images. We show a\nfailure case in the fourth image, where the model mistakenly treats the lake as the sky and misses\nall the people. More examples from the MS COCO dataset can be found on the project page:\nwww.stat.ucla.edu/\u02dcjunhua.mao/m-RNN.html.\ndictionary and stores the semantic temporal context in recurrent layers. The vision part contains a\ndeep Convolutional Neural Network (CNN) which generates the image representation. The multi-\nmodal part connects the language model and the deep CNN together by a one-layer representation.\nOur m-RNN model is learned using a log-likelihood cost function (see details in Section 4). The\nerrors can be backpropagated to the three parts of the m-RNN model to update the model parameters\nsimultaneously.\nIn the experiments, we validate our model on four benchmark datasets: IAPR TC-12 (Grubinger\net al. (2006)), Flickr 8K (Rashtchian et al. (2010)), Flickr 30K (Young et al. (2014)) and MS COCO\n(Lin et al. (2014)). We show that our method achieves state-of-the-art performance, signi\ufb01cantly\noutperforming all the other methods for the three tasks: generating novel sentences, retrieving im-\nages given a sentence and retrieving sentences given an image. Our framework is general and can\nbe further improved by incorporating more powerful deep representations for images and sentences.\n2\nRELATED WORK\nDeep model for computer vision and natural language. The methods based on the deep neural\nnetwork developed rapidly in recent years in both the \ufb01eld of computer vision and natural lan-\nguage. For computer vision, Krizhevsky et al. (2012) propose a deep Convolutional Neural Net-\nworks (CNN) with 8 layers (denoted as AlexNet) and outperform previous methods by a large\nmargin in the image classi\ufb01cation task of ImageNet challenge (Russakovsky et al. (2014)). This\nnetwork structure is widely used in computer vision, e.g. Girshick et al. (2014) design a object de-\ntection framework (RCNN) based on this work. Recently, Simonyan & Zisserman (2014) propose a\nCNN with over 16 layers (denoted as VggNet) and performs substantially better than the AlexNet.\nFor natural language, the Recurrent Neural Network (RNN) shows the state-of-the-art performance\nin many tasks, such as speech recognition and word embedding learning (Mikolov et al. (2010; 2011;\n2013)). Recently, RNNs have been successfully applied to machine translation to extract semantic\ninformation from the source sentence and generate target sentences (e.g. Kalchbrenner & Blunsom\n(2013), Cho et al. (2014) and Sutskever et al. (2014)).\nImage-sentence retrieval. Many previous methods treat the task of describing images as a retrieval\ntask and formulate the problem as a ranking or embedding learning problem (Hodosh et al. (2013);\nFrome et al. (2013); Socher et al. (2014)). They \ufb01rst extract the word and sentence features (e.g.\nSocher et al. (2014) uses dependency tree Recursive Neural Network to extract sentence features)\nas well as the image features. Then they optimize a ranking cost to learn an embedding model that\nmaps both the sentence feature and the image feature to a common semantic feature space. In this\nway, they can directly calculate the distance between images and sentences. Recently, Karpathy\net al. (2014) show that object level image features based on object detection results can generate\nbetter results than image features extracted at the global level.\n2\nPublished as a conference paper at ICLR 2015\n(b). The m-RNN model\nEmbedding I\nEmbedding II\nRecurrent\nMultimodal\nSoftMax\nwstart\nImage\nCNN\nw1\nImage\nCNN\nwL\nImage\nCNN\n...\nPredict\nw1\nPredict\nw2\nPredict\nwend\n(a). The simple RNN model\nw(t)\nr(t-1)\nr(t)\ny(t)\nw(t-1)\n...\ny(t-1)\nInput Word \nLayer w\nRecurrent\nLayer r\nOutput\nLayer y\nunfold\nThe m-RNN model for one time frame\n128\n256\n256\n512\nFigure 2: Illustration of the simple Recurrent Neural Network (RNN) and our multimodal Recurrent\nNeural Network (m-RNN) architecture. (a). The simple RNN. (b). Our m-RNN model. The inputs\nof our model are an image and its corresponding sentence descriptions. w1, w2, ..., wL represents the\nwords in a sentence. We add a start sign wstart and an end sign wend to all the training sentences. The\nmodel estimates the probability distribution of the next word given previous words and the image.\nIt consists of \ufb01ve layers (i.e. two word embedding layers, a recurrent layer, a multimodal layer and\na softmax layer) and a deep CNN in each time frame. The number above each layer indicates the\ndimension of the layer. The weights are shared among all the time frames. (Best viewed in color)\nGenerating novel sentence descriptions for images. There are generally three categories of meth-\nods for this task. The \ufb01rst category assumes a speci\ufb01c rule of the language grammar. They parse\nthe sentence and divide it into several parts (Mitchell et al. (2012); Gupta & Mannem (2012)). Each\npart is associated with an object or an attribute in the image (e.g. Kulkarni et al. (2011) uses a Con-\nditional Random Field model and Farhadi et al. (2010) uses a Markov Random Field model). This\nkind of method generates sentences that are syntactically correct. The second category retrieves\nsimilar captioned images, and generates new descriptions by generalizing and re-composing the re-\ntrieved captions (Kuznetsova et al. (2014)). The third category of methods, which is more related\nto our method, learns a probability density over the space of multimodal inputs (i.e. sentences and\nimages), using for example, Deep Boltzmann Machines (Srivastava & Salakhutdinov (2012)), and\ntopic models (Barnard et al. (2003); Jia et al. (2011)). They generate sentences with richer and more\n\ufb02exible structure than the \ufb01rst group. The probability of generating sentences using the model can\nserve as the af\ufb01nity metric for retrieval. Our method falls into this category. More closely related\nto our tasks and method is the work of Kiros et al. (2014b), which is built on a Log-BiLinear model\n(Mnih & Hinton (2007)) and use AlexNet to extract visual features. It needs a \ufb01xed length of context\n(i.e. \ufb01ve words), whereas in our model, the temporal context is stored in a recurrent architecture,\nwhich allows arbitrary context length.\nShortly after Mao et al. (2014), several papers appear with record breaking results (e.g. Kiros et al.\n(2014a); Karpathy & Fei-Fei (2014); Vinyals et al. (2014); Donahue et al. (2014); Fang et al. (2014);\nChen & Zitnick (2014)). Many of them are built on recurrent neural networks. It demonstrates the\neffectiveness of storing context information in a recurrent layer. Our work has two major difference\nfrom these methods. Firstly, we incorporate a two-layer word embedding system in the m-RNN\nnetwork structure which learns the word representation more ef\ufb01ciently than the single-layer word\nembedding. Secondly, we do not use the recurrent layer to store the visual information. The image\nrepresentation is inputted to the m-RNN model along with every word in the sentence description.\nIt utilizes of the capacity of the recurrent layer more ef\ufb01ciently, and allows us to achieve state-of-\nthe-art performance using a relatively small dimensional recurrent layer. In the experiments, we\nshow that these two strategies lead to better performance. Our method is still the best-performing\napproach for almost all the evaluation metrics.\n3\nMODEL ARCHITECTURE\n3.1\nSIMPLE RECURRENT NEURAL NETWORK\nWe brie\ufb02y introduce the simple Recurrent Neural Network (RNN) or Elman network (Elman\n(1990)). Its architecture is shown in Figure 2(a). It has three types of layers in each time frame:\n3\nPublished as a conference paper at ICLR 2015\nthe input word layer w, the recurrent layer r and the output layer y. The activation of input, re-\ncurrent and output layers at time t is denoted as w(t), r(t), and y(t) respectively. w(t) denotes\nthe current word vector, which can be a simple 1-of-N coding representation h(t) (i.e. the one-hot\nrepresentation, which is binary and has the same dimension as the vocabulary size with only one\nnon-zero element) Mikolov et al. (2010). y(t) can be calculated as follows:\nx(t) = [w(t) r(t \u22121)];\nr(t) = f1(U \u00b7 x(t));\ny(t) = g1(V \u00b7 r(t));\n(1)\nwhere x(t) is a vector that concatenates w(t) and r(t\u22121), f1(.) and g1(.) are element-wise sigmoid\nand softmax function respectively, and U, V are weights which will be learned.\nThe size of the RNN is adaptive to the length of the input sequence. The recurrent layers connect\nthe sub-networks in different time frames. Accordingly, when we do backpropagation, we need to\npropagate the error through recurrent connections back in time (Rumelhart et al. (1988)).\n3.2\nOUR M-RNN MODEL\nThe structure of our multimodal Recurrent Neural Network (m-RNN) is shown in Figure 2(b). It\nhas \ufb01ve layers in each time frame: two word embedding layers, the recurrent layer, the multimodal\nlayer, and the softmax layer).\nThe two word embedding layers embed the one-hot input into a dense word representation. It en-\ncodes both the syntactic and semantic meaning of the words. The semantically relevant words can be\nfound by calculating the Euclidean distance between two dense word vectors in embedding layers.\nMost of the sentence-image multimodal models (Karpathy et al. (2014); Frome et al. (2013); Socher\net al. (2014); Kiros et al. (2014b)) use pre-computed word embedding vectors as the initialization of\ntheir model. In contrast, we randomly initialize our word embedding layers and learn them from the\ntraining data. We show that this random initialization is suf\ufb01cient for our architecture to generate\nthe state-of-the-art result. We treat the activation of the word embedding layer II (see Figure 2(b))\nas the \ufb01nal word representation, which is one of the three direct inputs of the multimodal layer.\nAfter the two word embedding layers, we have a recurrent layer with 256 dimensions. The calcula-\ntion of the recurrent layer is slightly different from the calculation for the simple RNN. Instead of\nconcatenating the word representation at time t (denoted as w(t)) and the recurrent layer activation\nat time t \u22121 (denoted as r(t \u22121)), we \ufb01rst map r(t \u22121) into the same vector space as w(t) and add\nthem together:\nr(t) = f2(Ur \u00b7 r(t \u22121) + w(t));\n(2)\nwhere \u201c+\u201d represents element-wise addition. We set f2(.) to be the Recti\ufb01ed Linear Unit (ReLU),\ninspired by its the recent success when training very deep structure in computer vision \ufb01eld (Nair\n& Hinton (2010); Krizhevsky et al. (2012)). This differs from the simple RNN where the sigmoid\nfunction is adopted (see Section 3.1). ReLU is faster, and harder to saturate or over\ufb01t the data than\nnon-linear functions like the sigmoid. When the backpropagation through time (BPTT) is conducted\nfor the RNN with sigmoid function, the vanishing or exploding gradient problem appears since even\nthe simplest RNN model can have a large temporal depth 3. Previous work (Mikolov et al. (2010;\n2011)) use heuristics, such as the truncated BPTT, to avoid this problem. The truncated BPTT\nstops the BPTT after k time steps, where k is a hand-de\ufb01ned hyperparameter. Because of the good\nproperties of ReLU, we do not need to stop the BPTT at an early stage, which leads to better and\nmore ef\ufb01cient utilization of the data than the truncated BPTT.\nAfter the recurrent layer, we set up a 512 dimensional multimodal layer that connects the language\nmodel part and the vision part of the m-RNN model (see Figure 2(b)). This layer has three inputs:\nthe word-embedding layer II, the recurrent layer and the image representation. For the image rep-\nresentation, here we use the activation of the 7th layer of AlexNet (Krizhevsky et al. (2012)) or 15th\nlayer of VggNet (Simonyan & Zisserman (2014)), though our framework can use any image fea-\ntures. We map the activation of the three layers to the same multimodal feature space and add them\ntogether to obtain the activation of the multimodal layer:\nm(t) = g2(Vw \u00b7 w(t) + Vr \u00b7 r(t) + VI \u00b7 I);\n(3)\n3We tried Sigmoid and Scaled Hyperbolic Tangent function as the non-linear functions for RNN in the\nexperiments but they lead to the gradient explosion problem easily.\n4\nPublished as a conference paper at ICLR 2015\nwhere \u201c+\u201d denotes element-wise addition, m denotes the multimodal layer feature vector, I denotes\nthe image feature. g2(.) is the element-wise scaled hyperbolic tangent function (LeCun et al. (2012)):\ng2(x) = 1.7159 \u00b7 tanh(2\n3x)\n(4)\nThis function forces the gradients into the most non-linear value range and leads to a faster training\nprocess than the basic hyperbolic tangent function.\nBoth the simple RNN and m-RNN models have a softmax layer that generates the probability dis-\ntribution of the next word. The dimension of this layer is the vocabulary size M, which is different\nfor different datasets.\n4\nTRAINING THE M-RNN\nTo train our m-RNN model we adopt a log-likelihood cost function. It is related to the Perplexity of\nthe sentences in the training set given their corresponding images. Perplexity is a standard measure\nfor evaluating language model. The perplexity for one word sequence (i.e. a sentence) w1:L is\ncalculated as follows:\nlog2 PPL(w1:L|I) = \u22121\nL\nL\nX\nn=1\nlog2 P(wn|w1:n\u22121, I)\n(5)\nwhere L is the length of the word sequence, PPL(w1:L|I) denotes the perplexity of the sentence\nw1:L given the image I. P(wn|w1:n\u22121, I) is the probability of generating the word wn given I and\nprevious words w1:n\u22121. It corresponds to the activation of the SoftMax layer of our model.\nThe cost function of our model is the average log-likelihood of the words given their context words\nand corresponding images in the training sentences plus a regularization term. It can be calculated\nby the perplexity:\nC = 1\nN\nNs\nX\ni=1\nLi \u00b7 log2 PPL(w(i)\n1:Li|I(i)) + \u03bb\u03b8 \u00b7 \u2225\u03b8\u22252\n2\n(6)\nwhere Ns and N denotes the number of sentences and the number of words in the training set\nreceptively, Li denotes the length of ith sentences, and \u03b8 represents the model parameters.\nOur training objective is to minimize this cost function, which is equivalent to maximize the proba-\nbility of generating the sentences in the training set using the model. The cost function is differen-\ntiable and we use backpropagation to learn the model parameters.\n5\nSENTENCE GENERATION, IMAGE RETRIEVAL AND SENTENCE RETRIEVAL\nWe use the trained m-RNN model for three tasks: 1) Sentences generation, 2) Image retrieval (re-\ntrieving most relevant images to the given sentence), 3) Sentence retrieval (retrieving most relevant\nsentences to the given image).\nThe sentence generation process is straightforward. Starting from the start sign wstart or arbitrary\nnumber of reference words (e.g. we can input the \ufb01rst K words in the reference sentence to the\nmodel and then start to generate new words), our model can calculate the probability distribution\nof the next word: P(wn|w1:n\u22121, I). Then we can sample from this probability distribution to pick\nthe next word. In practice, we \ufb01nd that selecting the word with the maximum probability performs\nslightly better than sampling. After that, we input the picked word to the model and continue the\nprocess until the model outputs the end sign wend.\nFor the retrieval tasks, we use our model to calculate the probability of generating a sentence w1:L\ngiven an image I: P(w1:L|I) = Q\nn P(wn|w1:n\u22121, I). The probability can be treated as an af\ufb01nity\nmeasurement between sentences and images.\nFor the image retrieval task, given the query sentence wQ\n1:L, we rank the dataset images ID accord-\ning to the probability P(wQ\n1:L|ID) and retrieved the top ranked images. This is equivalent to the\nperplexity-based image retrieval in Kiros et al. (2014b).\n5\nPublished as a conference paper at ICLR 2015\nThe sentence retrieval task is trickier because there might be some sentences that have high proba-\nbility or perplexity for any image query (e.g. sentences consist of many frequently appeared words).\nTo solve this problem, Kiros et al. (2014b) uses the perplexity of a sentence conditioned on the\naveraged image feature across the training set as the reference perplexity to normalize the original\nperplexity. Different from them, we use the normalized probability where the normalization factor\nis the marginal probability of wD\n1:L:\nP(wD\n1:L|IQ)/P(wD\n1:L);\nP(wD\n1:L) = P\nI\u2032 P(wD\n1:L|I\n\u2032) \u00b7 P(I\n\u2032)\n(7)\nwhere wD\n1:L denotes the sentence in the dataset, IQ denotes the query image, and I\n\u2032 are images\nsampled from the training set. We approximate P(I\n\u2032) by a constant and ignore this term. This\nstrategy leads to a much better performance than that in Kiros et al. (2014b) in the experiments.\nThe normalized probability is equivalent to the probability P(IQ|wD\n1:L), which is symmetric to the\nprobability P(wQ\n1:L|ID) used in the image retrieval task.\n6\nLEARNING OF SENTENCE AND IMAGE FEATURES\nThe architecture of our model allows the gradients from the loss function to be backpropagated to\nboth the language modeling part (i.e. the word embedding layers and the recurrent layer) and the\nvision part (e.g. the AlexNet or VggNet).\nFor the language part, as mentioned above, we randomly initialize the language modeling layers and\nlearn their parameters. For the vision part, we use the pre-trained AlexNet (Krizhevsky et al. (2012))\nor the VggNet (Simonyan & Zisserman (2014)) on ImageNet dataset (Russakovsky et al. (2014)).\nRecently, Karpathy et al. (2014) show that using the RCNN object detection results (Girshick et al.\n(2014)) combined with the AlexNet features performs better than simply treating the image as a\nwhole frame. In the experiments, we show that our method performs much better than Karpathy\net al. (2014) when the same image features are used, and is better than or comparable to their results\neven when they use more sophisticated features based on object detection.\nWe can update the CNN in the vision part of our model according to the gradient backpropagated\nfrom the multimodal layer. In this paper, we \ufb01x the image features and the deep CNN network in the\ntraining stage due to a shortage of data. In future work, we will apply our method on large datasets\n(e.g. the complete MS COCO dataset, which has not yet been released) and \ufb01netune the parameters\nof the deep CNN network in the training stage.\nThe m-RNN model is trained using Baidu\u2019s internal deep learning platform PADDLE, which allows\nus to explore many different model architectures in a short period. The hyperparameters, such as\nlayer dimensions and the choice of the non-linear activation functions, are tuned via cross-validation\non Flickr8K dataset and are then \ufb01xed across all the experiments. It takes 25 ms on average to\ngenerate a sentence (excluding image feature extraction stage) on a single core CPU.\n7\nEXPERIMENTS\n7.1\nDATASETS\nWe test our method on four benchmark datasets with sentence level annotations: IAPR TC-12 (Grub-\ninger et al. (2006)), Flickr 8K (Rashtchian et al. (2010)), Flickr 30K (Young et al. (2014)) and MS\nCOCO (Lin et al. (2014)).\nIAPR TC-12. This dataset consists of around 20,000 images taken from different locations around\nthe world. It contains images of different sports and actions, people, animals, cities, landscapes,\netc. For each image, it provides at least one sentence annotation. On average, there are about 1.7\nsentence annotations for one image. We adopt the standard separation of training and testing set as\nprevious works (Guillaumin et al. (2010); Kiros et al. (2014b)) with 17,665 images for training and\n1962 images for testing.\nFlickr8K. This dataset consists of 8,000 images extracted from Flickr. For each image, it provides\n\ufb01ve sentence annotations. We adopt the standard separation of training, validation and testing set\nprovided by the dataset. There are 6,000 images for training, 1,000 images for validation and 1,000\nimages for testing.\n6\nPublished as a conference paper at ICLR 2015\nFlickr30K. This dataset is a recent extension of Flickr8K. For each image, it also provides \ufb01ve\nsentences annotations. It consists of 158,915 crowd-sourced captions describing 31,783 images.\nThe grammar and style for the annotations of this dataset is similar to Flickr8K. We follow the\nprevious work (Karpathy et al. (2014)) which used 1,000 images for testing. This dataset, as well as\nthe Flick8K dataset, were originally used for the image-sentence retrieval tasks.\nMS COCO. The current release of this recently proposed dataset contains 82,783 training images\nand 40,504 validation images. For each image, it provides \ufb01ve sentences annotations. We randomly\nsampled 4,000 images for validation and 1,000 images for testing from their currently released\nvalidation set. The dataset partition of MS COCO and Flickr30K is available in the project page 4.\n7.2\nEVALUATION METRICS\nSentence Generation. Following previous works, we use the sentence perplexity (see Equ. 5) and\nBLEU scores (i.e. B-1, B-2, B-3, and B-4) (Papineni et al. (2002)) as the evaluation metrics. BLEU\nscores were originally designed for automatic machine translation where they rate the quality of a\ntranslated sentences given several reference sentences. Similarly, we can treat the sentence gener-\nation task as the \u201ctranslation\u201d of the content of images to sentences. BLEU remains the standard\nevaluation metric for sentence generation methods for images, though it has drawbacks. For some\nimages, the reference sentences might not contain all the possible descriptions in the image and\nBLEU might penalize some correctly generated sentences. Please see more details of the calcula-\ntion of BLEU scores for this task in the supplementary material section 10.3 5.\nSentence Retrieval and Image Retrieval. We adopt the same evaluation metrics as previous works\n(Socher et al. (2014); Frome et al. (2013); Karpathy et al. (2014)) for both the tasks of sentences\nretrieval and image retrieval. We use R@K (K = 1, 5, 10) as the measurement. R@K is the recall\nrate of a correctly retrieved groundtruth given top K candidates. Higher R@K usually means better\nretrieval performance. Since we care most about the top-ranked retrieved results, the R@K scores\nwith smaller K are more important.\nThe Med r is another metric we use, which is the median rank of the \ufb01rst retrieved groundtruth\nsentence or image. Lower Med r usually means better performance. For IAPR TC-12 datasets,\nwe use additional evaluation metrics to conduct a fair comparison with previous work (Kiros et al.\n(2014b)). Please see the details in the supplementary material section 10.3.\n7.3\nRESULTS ON IAPR TC-12\nThe results of the sentence generation task6 are shown in Table 1. Ours-RNN-Base serves as a\nbaseline method for our m-RNN model. It has the same architecture as m-RNN except that it does\nnot have the image representation input.\nTo conduct a fair comparison, we follow the same experimental settings of Kiros et al. (2014b)\nto calculate the BLEU scores and perplexity. These two evaluation metrics are not necessarily\ncorrelated to each other for the following reasons. As mentioned in Section 4, perplexity is calculated\naccording to the conditional probability of the word in a sentence given all of its previous reference\nwords. Therefore, a strong language model that successfully captures the distributions of words in\nsentences can have a low perplexity without the image content. But the content of the generated\nsentences might be uncorrelated to images. From Table 1, we can see that although our baseline\nmethod of RNN generates a low perplexity, its BLEU score is low, indicating that it fails to generate\nsentences that are consistent with the content of images.\nTable 1 shows that our m-RNN model performs much better than our baseline RNN model and the\nstate-of-the-art methods both in terms of the perplexity and BLEU score.\n4www.stat.ucla.edu/\u02dcjunhua.mao/m-RNN.html\n5The BLEU outputted by our implementation is slightly lower than the recently released MS COCO caption\nevaluation toolbox (Chen et al. (2015)) because of different tokenization methods of the sentences. We re-\nevaluate our method using the toolbox in the current version of the paper.\n6Kiros et al. (2014b) further improved their results after the publication. We compare our results with their\nupdated ones here.\n7\nPublished as a conference paper at ICLR 2015\nPPL\nB-1\nB-2\nB-3\nB-4\nLBL, Mnih & Hinton (2007)\n9.29\n0.321\n0.145\n0.064\n-\nMLBLB-AlexNet, Kiros et al. (2014b)\n9.86\n0.393\n0.211\n0.112\n-\nMLBLF-AlexNet, Kiros et al. (2014b)\n9.90\n0.387\n0.209\n0.115\n-\nGupta et al. (2012)\n-\n0.15\n0.06\n0.01\n-\nGupta & Mannem (2012)\n-\n0.33\n0.18\n0.07\n-\nOurs-RNN-Base\n7.77\n0.307\n0.177\n0.096\n0.043\nOurs-m-RNN-AlexNet\n6.92\n0.482\n0.357\n0.269\n0.208\nTable 1: Results of the sentence generation task on the IAPR TC-12 dataset. \u201cB\u201d is short for BLEU.\nSentence Retrival (Image to Text)\nImage Retrival (Text to Image)\nR@1\nR@5\nR@10\nMed r\nR@1\nR@5\nR@10\nMed r\nOurs-m-RNN\n20.9\n43.8\n54.4\n8\n13.2\n31.2\n40.8\n21\nTable 2: R@K and median rank (Med r) for IAPR TC-12 dataset.\nSentence Retrival (Image to Text)\nImage Retrival (Text to Image)\nR@1\nR@5\nR@10\nMed r\nR@1\nR@5\nR@10\nMed r\nRandom\n0.1\n0.5\n1.0\n631\n0.1\n0.5\n1.0\n500\nSDT-RNN-AlexNet\n4.5\n18.0\n28.6\n32\n6.1\n18.5\n29.0\n29\nSocher-avg-RCNN\n6.0\n22.7\n34.0\n23\n6.6\n21.6\n31.7\n25\nDeViSE-avg-RCNN\n4.8\n16.5\n27.3\n28\n5.9\n20.1\n29.6\n29\nDeepFE-AlexNet\n5.9\n19.2\n27.3\n34\n5.2\n17.6\n26.5\n32\nDeepFE-RCNN\n12.6\n32.9\n44.0\n14\n9.7\n29.6\n42.5\n15\nOurs-m-RNN-AlexNet\n14.5\n37.2\n48.5\n11\n11.5\n31.0\n42.4\n15\nTable 3: Results of R@K and median rank (Med r) for Flickr8K dataset. \u201c-AlexNet\u201d denotes the\nimage representation based on AlexNet extracted from the whole image frame. \u201c-RCNN\u201d denotes\nthe image representation extracted from possible objects detected by the RCNN algorithm.\nFor the retrieval tasks, since there are no publicly available results of R@K and Med r in this dataset,\nwe report R@K scores of our method in Table 2 for future comparisons. The result shows that\n20.9% top-ranked retrieved sentences and 13.2% top-ranked retrieved images are groundtruth. We\nalso adopt additional evaluation metrics to compare our method with Kiros et al. (2014b), see sup-\nplementary material Section 10.2.\n7.4\nRESULTS ON FLICKR8K\nThis dataset was widely used as a benchmark dataset for image and sentence retrieval. The R@K\nand Med r of different methods are shown in Table 3. We compare our model with several state-of-\nthe-art methods: SDT-RNN (Socher et al. (2014)), DeViSE (Frome et al. (2013)), DeepFE (Karpathy\net al. (2014)) with various image representations. Our model outperforms these methods by a large\nmargin when using the same image representation (e.g. AlexNet). We also list the performance of\nmethods using more sophisticated features in Table 3. \u201c-avg-RCNN\u201d denotes methods with features\nof the average CNN activation of all objects above a detection con\ufb01dence threshold. DeepFE-RCNN\nKarpathy et al. (2014) uses a fragment mapping strategy to better exploit the object detection results.\nThe results show that using these features improves the performance. Even without the help from\nthe object detection methods, however, our method performs better than these methods in almost all\nthe evaluation metrics. We will develop our framework using better image features based on object\ndetection in the future work.\nThe PPL, B-1, B-2, B-3 and B-4 of the generated sentences using our m-RNN-AlexNet model in\nthis dataset are 24.39, 0.565, 0.386, 0.256, and 0.170 respectively.\n8\nPublished as a conference paper at ICLR 2015\nSentence Retrival (Image to Text)\nImage Retrival (Text to Image)\nR@1\nR@5\nR@10\nMed r\nR@1\nR@5\nR@10\nMed r\nFlickr30K\nRandom\n0.1\n0.6\n1.1\n631\n0.1\n0.5\n1.0\n500\nDeViSE-avg-RCNN\n4.8\n16.5\n27.3\n28\n5.9\n20.1\n29.6\n29\nDeepFE-RCNN\n16.4\n40.2\n54.7\n8\n10.3\n31.4\n44.5\n13\nRVR\n12.1\n27.8\n47.8\n11\n12.7\n33.1\n44.9\n12.5\nMNLM-AlexNet\n14.8\n39.2\n50.9\n10\n11.8\n34.0\n46.3\n13\nMNLM-VggNet\n23.0\n50.7\n62.9\n5\n16.8\n42.0\n56.5\n8\nNIC\n17.0\n56.0\n-\n7\n17.0\n57.0\n-\n7\nLRCN\n14.0\n34.9\n47.0\n11\n-\n-\n-\n-\nDeepVS\n22.2\n48.2\n61.4\n4.8\n15.2\n37.7\n50.5\n9.2\nOurs-m-RNN-AlexNet\n18.4\n40.2\n50.9\n10\n12.6\n31.2\n41.5\n16\nOurs-m-RNN-VggNet\n35.4\n63.8\n73.7\n3\n22.8\n50.7\n63.1\n5\nMS COCO\nRandom\n0.1\n0.6\n1.1\n631\n0.1\n0.5\n1.0\n500\nDeepVS-RCNN\n29.4\n62.0\n75.9\n2.5\n20.9\n52.8\n69.2\n4\nOurs-m-RNN-VggNet\n41.0\n73.0\n83.5\n2\n29.0\n42.2\n77.0\n3\nTable 4: Results of R@K and median rank (Med r) for Flickr30K dataset and MS COCO dataset.\nFlickr30K\nMS COCO\nPPL\nB-1\nB-2\nB-3\nB-4\nPPL\nB-1\nB-2\nB-3\nB-4\nRVR\n-\n-\n-\n-\n0.13\n-\n-\n-\n-\n0.19\nDeepVS-AlexNet\n-\n0.47\n0.21\n0.09\n-\n-\n0.53\n0.28\n0.15\n-\nDeepVS-VggNet\n21.20\n0.50\n0.30\n0.15\n-\n19.64\n0.57\n0.37\n0.19\n-\nNIC\n-\n0.66\n-\n-\n-\n-\n0.67\n-\n-\n-\nLRCN\n-\n0.59\n0.39\n0.25\n0.16\n-\n0.63\n0.44\n0.31\n0.21\nDMSM\n-\n-\n-\n-\n-\n-\n-\n-\n-\n0.21\nOurs-m-RNN-AlexNet\n35.11\n0.54\n0.36\n0.23\n0.15\n-\n-\n-\n-\n-\nOurs-m-RNN-VggNet\n20.72\n0.60\n0.41\n0.28\n0.19\n13.60\n0.67\n0.49\n0.35\n0.25\nTable 5: Results of generated sentences on the Flickr 30K dataset and MS COCO dataset.\nOur m-RNN\nMNLM\nNIC\nLRCN\nRVR\nDeepVS\nRNN Dim.\n256\n300\n512\n1000 (\u00d74)\n100\n300-600\nLSTM\nNo\nYes\nYes\nYes\nNo\nNo\nTable 6: Properties of the recurrent layers for the \ufb01ve very recent methods. LRCN has a stack of\nfour 1000 dimensional LSTM layers. We achieves state-of-the-art performance using a relatively\nsmall dimensional recurrent layer. LSTM (Hochreiter & Schmidhuber (1997)) can be treated as a\nsophisticated version of the RNN.\n7.5\nRESULTS ON FLICKR30K AND MS COCO\nWe compare our method with several state-of-the-art methods in these two recently released dataset\n(Note that the last six methods appear very recently, we use the results reported in their papers):\nDeViSE (Frome et al. (2013)), DeepFE (Karpathy et al. (2014)), MNLM (Kiros et al. (2014a)),\nDMSM (Fang et al. (2014)), NIC (Vinyals et al. (2014)), LRCN (Donahue et al. (2014)), RVR\n(Chen & Zitnick (2014)), and DeepVS (Karpathy & Fei-Fei (2014)). The results of the retrieval\ntasks and the sentence generation task 7 are shown in Table 4 and Table 5 respectively. We also\nsummarize some of the properties of the recurrent layers adopted in the \ufb01ve very recent methods in\nTable 6.\n7We only select the word with maximum probability each time in the sentence generation process in Table\n5 while many comparing methods (e.g. DMSM, NIC, LRCN) uses a beam search scheme that keeps the best K\ncandidates. The beam search scheme will lead to better performance in practice using the same model.\n9\nPublished as a conference paper at ICLR 2015\nB1\nB2\nB3\nB4\nCIDEr\nROUGE L\nMETEOR\nm-RNN-greedy-c5\n0.668\n0.488\n0.342\n0.239\n0.729\n0.489\n0.221\nm-RNN-greedy-c40\n0.845\n0.730\n0.598\n0.473\n0.740\n0.616\n0.291\nm-RNN-beam-c5\n0.680\n0.506\n0.369\n0.272\n0.791\n0.499\n0.225\nm-RNN-beam-c40\n0.865\n0.760\n0.641\n0.529\n0.789\n0.640\n0.304\nTable 7: Results of the MS COCO test set evaluated by MS COCO evaluation server\nOur method with VggNet image representation (Simonyan & Zisserman (2014)) outperforms the\nstate-of-the-art methods, including the very recently released methods, in almost all the evaluation\nmetrics. Note that the dimension of the recurrent layer of our model is relatively small compared\nto the competing methods. It shows the advantage and ef\ufb01ciency of our method that directly inputs\nthe visual information to the multimodal layer instead of storing it in the recurrent layer. The m-\nRNN model with VggNet performs better than that with AlexNet, which indicates the importance\nof strong image representations in this task. 71% of the generated sentences for MS COCO datasets\nare novel (i.e. different from training sentences).\nWe also validate our method on the test set of MS COCO by their evaluation server (Chen et al.\n(2015)). The results are shown in Table 7. We evaluate our model with greedy inference (select\nthe word with the maximum probability each time) as well as with the beam search inference. \u201c-\nc5\u201d represents results using 5 reference sentences and \u201c-c40\u201d represents results using 40 reference\nsentences.\nTo further validate the importance of different components of the m-RNN model, we train sev-\neral variants of the original m-RNN model and compare their performance. In particular, we show\nthat the two-layer word embedding system outperforms the single-layer version and the strategy of\ndirectly inputting the visual information to the multimodal layer substantially improves the perfor-\nmance (about 5% for B-1). Due to the limited space, we put the details of these experiments in\nSection 10.1 in the supplementary material after the main paper.\n8\nNEAREST NEIGHBOR AS REFERENCE\nRecently, Devlin et al. (2015b) proposed a nearest neighbor approach that retrieves the captions\nof the k nearest images in the training set, ranks these captions according to the consensus of the\ncaption w.r.t. to the rest of the captions, and output the top ranked one.\nInspired by this method, we \ufb01rst adopt the m-RNN model with the transposed weight sharing strat-\negy (Mao et al. (2015), denoted as m-RNN-shared) to generate n hypotheses using a beam search\nscheme. Speci\ufb01cally, we keep the n best candidates in the sentence generation process until the\nmodel generates the end sign wend. These n best candidates are approximately the n most probable\nsentences generated by the model, and can be treated as the n hypotheses. In our experiments, we set\nn = 10 since it gives us a diversi\ufb01ed set of hypotheses without too much outliers on our validation\nset. 8\nAfter generating the hypotheses of a target image, we retrieve its nearest neighbors in the image\nfeature space on the training set (see details in Section 8.1). Then we calculate the \u201cconsensus\u201d\nscores (Devlin et al. (2015a)) of the hypotheses w.r.t. to the groundtruth captions of the nearest\nneighbor images, and rerank the hypotheses according to these scores (see details in Section 8.2).\n8.1\nIMAGE FEATURES FOR THE NEAREST NEIGHBOR IMAGE SEARCH\nWe try two types of image features for the nearest neighbor image search 9. The \ufb01rst one is the\noriginal image features extracted by the VggNet (Simonyan & Zisserman (2014)). We \ufb01rst resize\nthe image so that its short side is 256 pixels. Then we extract features on ten 224 \u00d7 224 windows\n8If we directly output the top hypotheses generated by the model, then n = 5 gives us the best performance.\nBut if we want to rerank the hypotheses, then n = 10 gives us a better result on the validation set.\n9We release both types of the features on MS COCO 2014 train, val and test sets. Please refer to the readme\n\ufb01le at https://github.com/mjhucla/mRNN-CR to see how to download and use them.\n10\nPublished as a conference paper at ICLR 2015\nTarget Image\nNearest Five Neighbors In Terms of m-RNN Refined Feature\nNearest Five Neighbors In Terms of Original VGG Feature\nFigure 3: The sample images and their nearest neighbors retrieved by two types of features. Com-\npared to the original VggNet features, the features re\ufb01ned by the m-RNN model are better for cap-\nturing richer and more accurate visual information.\n(the four corners, the center and their mirrored versions) on the resized image. Finally, we average\npool the ten features to make it a 4,096 dimensional feature.\nThe second type is the feature re\ufb01ned by our m-RNN model. It can be calculated as: Ir = g2(VI \u00b7I),\nwhere VI is the weight matrix between the image representation and the multimodal layer (see\nEquation 3), and g2(.) is the scaled hyperbolic tangent function.\nWe show the sample images and their nearest neighbors in Figure 3. We \ufb01nd that compared to the\noriginal VggNet features, the features re\ufb01ned by the m-RNN model capture richer and more accurate\nvisual information. E.g., the target image in the second row contains an old woman with a bunch of\nbananas. The original VggNet features do not retrieve images with bananas in them.\n8.2\nCONSENSUS RERANKING\nSuppose we have get the k nearest neighbor images in the training set as the reference. We follow\nDevlin et al. (2015a) to calculate the consensus score of a hypotheses. The difference is that Devlin\net al. (2015a) treat the captions of the k nearest neighbor images as the hypotheses while our hy-\npotheses are generated by the m-RNN model. More speci\ufb01cally, for each hypothesis, we calculate\nthe mean similarity between this hypothesis and all the captions of the k nearest neighbor images.\nMS COCO val for consensus reranking\nB1\nB2\nB3\nB4\nCIDEr ROUGE L METEOR\nm-RNN-shared\n0.686 0.511 0.375 0.280 0.842\n0.500\n0.228\nm-RNN-shared-NNref-BLEU\n0.718 0.550 0.409 0.305 0.909\n0.519\n0.235\nm-RNN-shared-NNref-CIDEr\n0.714 0.543 0.406 0.304 0.938\n0.519\n0.239\nm-RNN-shared-NNref-BLEU-Orcale 0.792 0.663 0.543 0.443 1.235\n0.602\n0.287\nm-RNN-shared-NNref-CIDEr-Oracle 0.784 0.648 0.529 0.430 1.272\n0.593\n0.287\nMS COCO 2014 test server\nB1\nB2\nB3\nB4\nCIDEr ROUGE L METEOR\nm-RNN-shared\n0.685 0.512 0.376 0.279 0.819\n0.504\n0.229\nm-RNN-shared-NNref-BLEU\n0.720 0.553 0.410 0.302 0.886\n0.524\n0.238\nm-RNN-shared-NNref-CIDEr\n0.716 0.545 0.404 0.299 0.917\n0.521\n0.242\nTable 8: Results of m-RNN-shared model after applying consensus reranking using nearest neigh-\nbors as references (m-RNN-shared-NNref), compared with those of the original m-RNN model on\nour validation set and MS COCO test server.\n11\nPublished as a conference paper at ICLR 2015\nThe consensus score of this hypothesis is the mean similarity score of the m nearest captions. The\nsimilarity between a hypothesis and one of its nearest neighbor reference captions is de\ufb01ned by\na sentence-level BLEU score (Papineni et al. (2002)) or a sentence-level CIDEr (Vedantam et al.\n(2014)). We cross-validate the hyperparamters k and m. For the BLEU-based similarity, the opti-\nmal k and m are 60 and 175 respectively. For the CIDEr-based similarity, the optimal k and m are\n60 and 125 respectively.\n8.3\nEXPERIMENTS\nWe show the results of our model on our validation set and the MS COCO testing server in Table\n8. For BLEU-based consensus reranking, we get an improvement of 3.5 points on our validation\nset and 3.3 points on the MS COCO test 2014 set in terms of BLEU4 score. For the CIDEr-based\nconsensus reranking, we get an improvement of 9.4 points on our validation set and 9.8 points on\nthe MS COCO test 2014 set in terms of CIDEr.\n8.4\nDISCUSSION\nWe show the rank of the ten hypotheses before and after reranking in Figure 4. Although the hy-\npotheses are similar to each other, there are some variances among them (E.g., some of them capture\nmore details of the images. Some of them might be partially wrong). The reranking process is able\nto improve the rank of good captions.\nWe also show the oracle performance of the ten hypotheses, which is the upper bound of the con-\nsensus reranking. More speci\ufb01cally, for each image in our validation set, we rerank the hypotheses\naccording to the scores (BLEU or CIDEr) w.r.t to the groundtruth captions. The results of this oracle\nreranking are shown in Table 8 (see rows with \u201c-oracle\u201d). The oracle performance is surprisingly\nhigh, indicating that there is still room for improvement, both for the m-RNN model itself and the\nreranking strategy.\n9\nCONCLUSION\nWe propose a multimodal Recurrent Neural Network (m-RNN) framework that performs at the\nstate-of-the-art in three tasks: sentence generation, sentence retrieval given query image and image\nOriginal \nAfter Reranking (C(Der) \n1. a piece of cake on a plate on a table \n2. a piece of cake on a white plate \n3. a piece of cake sitting on top of a white \nplate \n4. a piece of cake sitting on top of a plate \n5. a piece of cake on a plate with a fork \n6. a close up of a piece of cake on a plate \n7. a piece of chocolate cake on a plate \n8. a piece of cake sitting on a plate \n9. a slice of cake on a white plate \n10. a slice of cake on a plate with a fork \n1. a piece of cake on a plate with a fork \n2. a slice of cake on a plate with a fork \n3. a close up of a piece of cake on a plate \n4. a piece of cake on a plate on a table \n5. a piece of cake on a white plate \n6. a piece of cake sitting on top of a plate \n7. a piece of cake sitting on top of a white \nplate \n8. a piece of chocolate cake on a plate \n9. a piece of cake sitting on a plate \n10. a slice of cake on a white plate \n1. a black and white photo of a black bear \n2. a black and white photo of a bear \n3. a black bear laying on top of a rock \n4. a black bear sitting on top of a wooden bench \n5. a black bear sitting on top of a rock \n6. a black bear laying on top of a wooden bench \n7. a black and white photo of a dog \n8. a black bear laying on top of a wooden floor \n9. a close up of a black and white dog \n10. a close up of a black and white cat \n1. a black bear sitting on top of a rock \n2. a black bear laying on top of a rock \n3. a black bear sitting on top of a wooden bench \n4. a black bear laying on top of a wooden bench \n5. a black bear laying on top of a wooden floor \n6. a black and white photo of a black bear \n7. a black and white photo of a bear \n8. a close up of a black and white dog \n9. a black and white photo of a dog \n10. a close up of a black and white cat \n1. a group of people standing next to each other \n2. a group of people standing around a train \n3. a group of people standing in a room \n4. a group of people in a room with luggage \n5. a group of people that are standing in a room \n6. a group of people standing next to a train \n7. a group of people standing in front of a train \n8. a group of people sitting on a bench \n9. a group of people standing in a room with luggage \n10. a group of people standing next to each other on a \ntrain \n1. a group of people standing in a room with luggage \n2. a group of people in a room with luggage \n3. a group of people standing next to a train \n4. a group of people standing in front of a train \n5. a group of people standing around a train \n6. a group of people standing next to each other on a \ntrain \n7. a group of people standing in a room \n8. a group of people standing next to each other \n9. a group of people that are standing in a room \n10. a group of people sitting on a bench \nFigure 4: The original rank of the hypotheses and the rank after consensus reranking (CIDEr).\n12\nPublished as a conference paper at ICLR 2015\nretrieval given query sentence. The model consists of a deep RNN, a deep CNN and these two\nsub-networks interact with each other in a multimodal layer. Our m-RNN is powerful of connecting\nimages and sentences and is \ufb02exible to incorporate more complex image representations and more\nsophisticated language models.\nACKNOWLEDGMENTS\nWe thank Andrew Ng, Kai Yu, Chang Huang, Duohao Qin, Haoyuan Gao, Jason Eisner for useful\ndiscussions and technical support. We also thank the comments and suggestions of the anonymous\nreviewers from ICLR 2015 and NIPS 2014 Deep Learning Workshop. We acknowledge the Center\nfor Minds, Brains and Machines (CBMM), partially funded by NSF STC award CCF-1231216, and\nARO 62250-CS.\nREFERENCES\nBarnard, Kobus, Duygulu, Pinar, Forsyth, David, De Freitas, Nando, Blei, David M, and Jordan,\nMichael I. Matching words and pictures. JMLR, 3:1107\u20131135, 2003.\nChen, X., Fang, H., Lin, TY, Vedantam, R., Gupta, S., Dollr, P., and Zitnick, C. L. Microsoft coco\ncaptions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015.\nChen, Xinlei and Zitnick, C Lawrence. Learning a recurrent visual representation for image caption\ngeneration. arXiv preprint arXiv:1411.5654, 2014.\nCho, Kyunghyun, van Merrienboer, Bart, Gulcehre, Caglar, Bougares, Fethi, Schwenk, Holger,\nand Bengio, Yoshua. Learning phrase representations using rnn encoder-decoder for statistical\nmachine translation. arXiv preprint arXiv:1406.1078, 2014.\nDevlin, Jacob, Cheng, Hao, Fang, Hao, Gupta, Saurabh, Deng, Li, He, Xiaodong, Zweig, Geoffrey,\nand Mitchell, Margaret. Language models for image captioning: The quirks and what works.\narXiv preprint arXiv:1505.01809, 2015a.\nDevlin, Jacob, Gupta, Saurabh, Girshick, Ross, Mitchell, Margaret, and Zitnick, C Lawrence. Ex-\nploring nearest neighbor approaches for image captioning. arXiv preprint arXiv:1505.04467,\n2015b.\nDonahue, Jeff, Hendricks, Lisa Anne, Guadarrama, Sergio, Rohrbach, Marcus, Venugopalan, Sub-\nhashini, Saenko, Kate, and Darrell, Trevor. Long-term recurrent convolutional networks for visual\nrecognition and description. arXiv preprint arXiv:1411.4389, 2014.\nElman, Jeffrey L. Finding structure in time. Cognitive science, 14(2):179\u2013211, 1990.\nFang, Hao, Gupta, Saurabh, Iandola, Forrest, Srivastava, Rupesh, Deng, Li, Doll\u00b4ar, Piotr, Gao,\nJianfeng, He, Xiaodong, Mitchell, Margaret, Platt, John, et al. From captions to visual concepts\nand back. arXiv preprint arXiv:1411.4952, 2014.\nFarhadi, Ali, Hejrati, Mohsen, Sadeghi, Mohammad Amin, Young, Peter, Rashtchian, Cyrus, Hock-\nenmaier, Julia, and Forsyth, David. Every picture tells a story: Generating sentences from images.\nIn ECCV, pp. 15\u201329. 2010.\nFrome, Andrea, Corrado, Greg S, Shlens, Jon, Bengio, Samy, Dean, Jeff, Mikolov, Tomas, et al.\nDevise: A deep visual-semantic embedding model. In NIPS, pp. 2121\u20132129, 2013.\nGirshick, R., Donahue, J., Darrell, T., and Malik, J. Rich feature hierarchies for accurate object\ndetection and semantic segmentation. In CVPR, 2014.\nGrubinger, Michael, Clough, Paul, M\u00a8uller, Henning, and Deselaers, Thomas. The iapr tc-12 bench-\nmark: A new evaluation resource for visual information systems. In International Workshop\nOntoImage, pp. 13\u201323, 2006.\nGuillaumin, Matthieu, Verbeek, Jakob, and Schmid, Cordelia. Multiple instance metric learning\nfrom automatically labeled bags of faces. In ECCV, pp. 634\u2013647, 2010.\n13\nPublished as a conference paper at ICLR 2015\nGupta, Ankush and Mannem, Prashanth. From image annotation to image description. In ICONIP,\n2012.\nGupta, Ankush, Verma, Yashaswi, and Jawahar, CV. Choosing linguistics over vision to describe\nimages. In AAAI, 2012.\nHochreiter, Sepp and Schmidhuber, J\u00a8urgen. Long short-term memory. Neural computation, 9(8):\n1735\u20131780, 1997.\nHodosh, Micah, Young, Peter, and Hockenmaier, Julia. Framing image description as a ranking\ntask: Data, models and evaluation metrics. JAIR, 47:853\u2013899, 2013.\nJia, Yangqing, Salzmann, Mathieu, and Darrell, Trevor.\nLearning cross-modality similarity for\nmultinomial data. In ICCV, pp. 2407\u20132414, 2011.\nKalchbrenner, Nal and Blunsom, Phil. Recurrent continuous translation models. In EMNLP, pp.\n1700\u20131709, 2013.\nKarpathy, Andrej and Fei-Fei, Li. Deep visual-semantic alignments for generating image descrip-\ntions. arXiv preprint arXiv:1412.2306, 2014.\nKarpathy, Andrej, Joulin, Armand, and Fei-Fei, Li. Deep fragment embeddings for bidirectional\nimage sentence mapping. In arXiv:1406.5679, 2014.\nKiros, Ryan, Salakhutdinov, Ruslan, and Zemel, Richard S. Unifying visual-semantic embeddings\nwith multimodal neural language models. arXiv preprint arXiv:1411.2539, 2014a.\nKiros, Ryan, Zemel, R, and Salakhutdinov, Ruslan. Multimodal neural language models. In ICML,\n2014b.\nKrizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classi\ufb01cation with deep con-\nvolutional neural networks. In NIPS, pp. 1097\u20131105, 2012.\nKulkarni, Girish, Premraj, Visruth, Dhar, Sagnik, Li, Siming, Choi, Yejin, Berg, Alexander C, and\nBerg, Tamara L. Baby talk: Understanding and generating image descriptions. In CVPR, 2011.\nKuznetsova, Polina, Ordonez, Vicente, Berg, Tamara L, and Choi, Yejin. Treetalk: Composition and\ncompression of trees for image descriptions. Transactions of the Association for Computational\nLinguistics, 2(10):351\u2013362, 2014.\nLeCun, Yann A, Bottou, L\u00b4eon, Orr, Genevieve B, and M\u00a8uller, Klaus-Robert. Ef\ufb01cient backprop. In\nNeural networks: Tricks of the trade, pp. 9\u201348. Springer, 2012.\nLin, Tsung-Yi, Maire, Michael, Belongie, Serge, Hays, James, Perona, Pietro, Ramanan, Deva,\nDoll\u00b4ar, Piotr, and Zitnick, C Lawrence. Microsoft coco: Common objects in context. arXiv\npreprint arXiv:1405.0312, 2014.\nMao, Junhua, Xu, Wei, Yang, Yi, Wang, Jiang, and Yuille, Alan L. Explain images with multimodal\nrecurrent neural networks. NIPS DeepLearning Workshop, 2014.\nMao, Junhua, Xu, Wei, Yang, Yi, Wang, Jiang, Huang, Zhiheng, and Yuille, Alan. Learning like a\nchild: Fast novel visual concept learning from sentence descriptions of images. arXiv preprint\narXiv:1504.06692, 2015.\nMikolov, Tomas, Kara\ufb01\u00b4at, Martin, Burget, Lukas, Cernock`y, Jan, and Khudanpur, Sanjeev. Recur-\nrent neural network based language model. In INTERSPEECH, pp. 1045\u20131048, 2010.\nMikolov, Tomas, Kombrink, Stefan, Burget, Lukas, Cernocky, JH, and Khudanpur, Sanjeev. Exten-\nsions of recurrent neural network language model. In ICASSP, pp. 5528\u20135531, 2011.\nMikolov, Tomas, Sutskever, Ilya, Chen, Kai, Corrado, Greg S, and Dean, Jeff. Distributed represen-\ntations of words and phrases and their compositionality. In NIPS, pp. 3111\u20133119, 2013.\n14\nPublished as a conference paper at ICLR 2015\nMitchell, Margaret, Han, Xufeng, Dodge, Jesse, Mensch, Alyssa, Goyal, Amit, Berg, Alex, Ya-\nmaguchi, Kota, Berg, Tamara, Stratos, Karl, and Daum\u00b4e III, Hal. Midge: Generating image\ndescriptions from computer vision detections. In EACL, 2012.\nMnih, Andriy and Hinton, Geoffrey. Three new graphical models for statistical language modelling.\nIn ICML, pp. 641\u2013648. ACM, 2007.\nNair, Vinod and Hinton, Geoffrey E. Recti\ufb01ed linear units improve restricted boltzmann machines.\nIn ICML, pp. 807\u2013814, 2010.\nPapineni, Kishore, Roukos, Salim, Ward, Todd, and Zhu, Wei-Jing. Bleu: a method for automatic\nevaluation of machine translation. In ACL, pp. 311\u2013318, 2002.\nRashtchian, Cyrus, Young, Peter, Hodosh, Micah, and Hockenmaier, Julia. Collecting image anno-\ntations using amazon\u2019s mechanical turk. In NAACL-HLT workshop 2010, pp. 139\u2013147, 2010.\nRumelhart, David E, Hinton, Geoffrey E, and Williams, Ronald J. Learning representations by\nback-propagating errors. Cognitive modeling, 1988.\nRussakovsky, Olga, Deng, Jia, Su, Hao, Krause, Jonathan, Satheesh, Sanjeev, Ma, Sean, Huang,\nZhiheng, Karpathy, Andrej, Khosla, Aditya, Bernstein, Michael, Berg, Alexander C., and Fei-Fei,\nLi. ImageNet Large Scale Visual Recognition Challenge, 2014.\nSimonyan, Karen and Zisserman, Andrew. Very deep convolutional networks for large-scale image\nrecognition. arXiv preprint arXiv:1409.1556, 2014.\nSocher, Richard, Le, Q, Manning, C, and Ng, A. Grounded compositional semantics for \ufb01nding and\ndescribing images with sentences. In TACL, 2014.\nSrivastava, Nitish and Salakhutdinov, Ruslan. Multimodal learning with deep boltzmann machines.\nIn NIPS, pp. 2222\u20132230, 2012.\nSutskever, Ilya, Vinyals, Oriol, and Le, Quoc VV. Sequence to sequence learning with neural net-\nworks. In NIPS, pp. 3104\u20133112, 2014.\nVedantam, Ramakrishna, Zitnick, C Lawrence, and Parikh, Devi. Cider: Consensus-based image\ndescription evaluation. arXiv preprint arXiv:1411.5726, 2014.\nVinyals, Oriol, Toshev, Alexander, Bengio, Samy, and Erhan, Dumitru. Show and tell: A neural\nimage caption generator. arXiv preprint arXiv:1411.4555, 2014.\nYoung, Peter, Lai, Alice, Hodosh, Micah, and Hockenmaier, Julia. From image descriptions to visual\ndenotations: New similarity metrics for semantic inference over event descriptions. In ACL, pp.\n479\u2013488, 2014.\n10\nSUPPLEMENTARY MATERIAL\n10.1\nEFFECTIVENESS OF THE DIFFERENT COMPONENTS OF THE M-RNN MODEL\nB-1\nB-2\nB-3\nB-4\nm-RNN\n0.600\n0.412\n0.278\n0.187\nm-RNN-NoEmbInput\n0.592\n0.408\n0.277\n0.188\nm-RNN-OneLayerEmb\n0.594\n0.406\n0.274\n0.184\nm-RNN-EmbOneInput\n0.590\n0.406\n0.274\n0.185\nm-RNN-visInRnn\n0.466\n0.267\n0.157\n0.101\nm-RNN-visInRnn-both\n0.546\n0.333\n0.191\n0.120\nm-RNN-visInRnn-both-shared\n0.478\n0.279\n0.171\n0.110\nTable 9: Performance comparison of different versions of m-RNN models on the Flickr30K dataset.\nAll the models adopt VggNet as the image representation. See Figure 5 for details of the models.\n15\nPublished as a conference paper at ICLR 2015\nEmbedding I\nEmbedding II\nRecurrent\nMultimodal\nSoftMax\nwstart\nImage\nCNN\nPredict\nw1\nThe Original m-RNN \nmodel for one time frame\n128\n256\n256\n512\nwstart\nImage\nCNN\nPredict\nw1\n128\n256\n256\n512\nm-RNN-NoEmbInput\nwstart\nImage\nCNN\nPredict\nw1\n256\n256\n512\nm-RNN-OneLayerEmb\nwstart\nImage\nCNN\nPredict\nw1\n128\n256\n256\n512\nm-RNN-EmbOneInput\nwstart\nImage\nCNN\nPredict\nw1\n128\n256\n256\n512\nwstart\nImage\nCNN\nPredict\nw1\n128\n256\n256\n512\nVI\n(1)\nVI\n(2)\nwstart\nImage\nCNN\nPredict\nw1\n128\n256\n256\n512\nVI\nVI\nm-RNN-VisualInRnn\nm-RNN-VisualInRnn-\nBoth\nm-RNN-VisualInRnn-\nBoth-Shared\nFigure 5: Illustration of the seven variants of the m-RNN models.\nIn this section, we compare different variants of our m-RNN model to show the effectiveness of the\ntwo-layer word embedding and the strategy to input the visual information to the multimodal layer.\nThe word embedding system. Intuitively, the two word embedding layers capture high-level se-\nmantic meanings of words more ef\ufb01ciently than the single layer word embedding. As an input to\nthe multimodal layer, it offers useful information for predicting the next word distribution.\nTo validate its ef\ufb01ciency, we train three different m-RNN networks: m-RNN-NoEmbInput, m-RNN-\nOneLayerEmb, m-RNN-EmbOneInput. They are illustrated in Figure 5. \u201cm-RNN-NoEmbInput\u201d\ndenotes the m-RNN model whose connection between the word embedding layer II and the mul-\ntimodal layer is cut off. Thus the multimodal layer has only two inputs: the recurrent layer and\nthe image representation. \u201cm-RNN-OneLayerEmb\u201d denotes the m-RNN model whose two word\nembedding layers are replaced by a single 256 dimensional word-embedding layer. There are much\nmore parameters of the word-embedding layers in the m-RNN-OneLayerEmb than those in the\noriginal m-RNN (256 \u00b7 M v.s. 128 \u00b7 M + 128 \u00b7 256) if the dictionary size M is large. \u201cm-RNN-\nEmbOneInput\u201d denotes the m-RNN model whose connection between the word embedding layer II\nand the multimodal layer is replaced by the connection between the word embedding layer I and the\nmultimodal layer. The performance comparisons are shown in Table 9.\nTable 9 shows that the original m-RNN model with the two word embedding layers and the con-\nnection between word embedding layer II and multimodal layer performs the best. It veri\ufb01es the\neffectiveness of the two word embedding layers.\nHow to connect the vision and the language part of the model. We train three variants of m-RNN\nmodels where the image representation is inputted into the recurrent layer: m-RNN-VisualInRNN,\nm-RNN-VisualInRNN-both, and m-RNN-VisualInRNN-Both-Shared. For m-RNN-VisualInRNN,\nwe only input the image representation to the word embedding layer II while for the later two mod-\nels, we input the image representation to both the multimodal layer and word embedding layer II.\n16\nPublished as a conference paper at ICLR 2015\nThe weights of the two connections V (1)\nI\n, V (2)\nI\nare shared for m-RNN-VisualInRNN-Both-Shared.\nPlease see details of these models in Figure 5. Table 9 shows that the original m-RNN model\nperforms much better than these models, indicating that it is effective to directly input the visual\ninformation to the multimodal layer.\nIn practice, we \ufb01nd that it is harder to train these variants than to train the original m-RNN model\nand we have to keep the learning rate very small to avoid the exploding gradient problem. Increasing\nthe dimension of the recurrent layer or replacing RNN with LSTM (a sophisticated version of RNN\nHochreiter & Schmidhuber (1997)) might solve the problem. We will explore this issue in future\nwork.\n10.2\nADDITIONAL RETRIEVAL PERFORMANCE COMPARISONS ON IAPR TC-12\nFor the retrieval results in this dataset, in addition to the R@K and Med r, we also adopt exactly\nthe same evaluation metrics as Kiros et al. (2014b) and plot the mean number of matches of the\nretrieved groundtruth sentences or images with respect to the percentage of the retrieved sentences\nor images for the testing set. For the sentence retrieval task, Kiros et al. (2014b) uses a shortlist of\n100 images which are the nearest neighbors of the query image in the feature space. This shortlist\nstrategy makes the task harder because similar images might have similar descriptions and it is often\nharder to \ufb01nd subtle differences among the sentences and pick the most suitable one.\nThe recall accuracy curves with respect to the percentage of retrieved images (sentence retrieval\ntask) or sentences (sentence retrieval task) are shown in Figure 6. The \ufb01rst method, bowdecaf, is a\nstrong image based bag-of-words baseline (Kiros et al. (2014b)). The second and the third models\n(Kiros et al. (2014b)) are all multimodal deep models. Our m-RNN model signi\ufb01cantly outperforms\nthese three methods in this task.\n10.3\nTHE CALCULATION OF BLEU SCORE\nThe BLEU score was proposed by Papineni et al. (2002) and was originally used as a evaluation\nmetric for machine translation. To calculate BLEU-N (i.e. B-N in the paper where N=1,2,3,4) score,\nwe \ufb01rst compute the modi\ufb01ed n-gram precision (Papineni et al. (2002)), pn. Then we compute the\ngeometric mean of pn up to length N and multiply it by a brevity penalty BP:\nBP = min(1, e1\u2212r\nc )\n(8)\nB-N = BP \u00b7 e\n1\nN\nPN\nn=1 log pn\n(9)\nwhere r is the length of the reference sentence and c is the length of the generated sentence. We\nuse the same strategy as Fang et al. (2014) where pn, r, and c are computed over the whole testing\ncorpus. When there are multiple reference sentences, the length of the reference that is closest\n(longer or shorter) to the length of the candidate is used to compute r.\n0.01\n0.02\n0.05\n0.1 \n0.25\n0.5 \n1   \n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n \n \nOurs\u2212mRNN\nbow\u2212decaf\nMLBL\u2212F\u2212decaf\nMLBL\u2212B\u2212decaf\n(a) Image to Text Curve\n0.0005 0.001 0.002 0.005 0.01  0.02  0.05  0.1   \n0.25  0.5   \n1     \n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n \n \nOurs\u2212mRNN\nbow\u2212decaf\nMLBL\u2212F\u2212decaf\nMLBL\u2212B\u2212decaf\n(b) Text to Image Curve\nFigure 6: Retrieval recall curve for (a). Sentence retrieval task (b). Image retrieval task on IAPR\nTC-12 dataset. The behavior on the far left (i.e. top few retrievals) is most important.\n17\n",
        "sentence": " Recently, recurrent neural networks have been used successfully for image captioning, [35, 44, 67]. This work clearly precedes [35] as it is cited throughout, but [44] claim it as an independent innovation.",
        "context": "Images with Multimodal Recurrent Neural Networks\u201d http://arxiv.org/abs/1410.1090 (Mao et al.\n(2014)). We observed subsequent arXiv papers which also use recurrent neural networks in this topic and cite\nour work. We gratefully acknowledge them.\n1\nRNN model with VggNet performs better than that with AlexNet, which indicates the importance\nof strong image representations in this task. 71% of the generated sentences for MS COCO datasets\nare novel (i.e. different from training sentences).\nPublished as a conference paper at ICLR 2015\nDEEP CAPTIONING WITH MULTIMODAL RECURRENT\nNEURAL NETWORKS (M-RNN)\nJunhua Mao\nUniversity of California, Los Angeles; Baidu Research\nmjhustc@ucla.edu\nWei Xu & Yi Yang & Jiang Wang & Zhiheng Huang\nBaidu Research"
    },
    {
        "title": "Learning recurrent neural networks with hessian-free optimization",
        "author": [
            "James Martens",
            "Ilya Sutskever"
        ],
        "venue": "In Proceedings of the 28th International Conference on Machine Learning",
        "citeRegEx": "45",
        "shortCiteRegEx": "45",
        "year": 2011,
        "abstract": "",
        "full_text": "",
        "sentence": " , truncated Newton approach [45] and applied it to a network which learns to generate text one character at a time in [63].",
        "context": null
    },
    {
        "title": "Efficient estimation of word representations in vector space",
        "author": [
            "Tomas Mikolov",
            "Kai Chen",
            "Greg Corrado",
            "Jeffrey Dean"
        ],
        "venue": "arXiv preprint arXiv:1301.3781,",
        "citeRegEx": "46",
        "shortCiteRegEx": "46",
        "year": 2013,
        "abstract": "We propose two novel model architectures for computing continuous vector\nrepresentations of words from very large data sets. The quality of these\nrepresentations is measured in a word similarity task, and the results are\ncompared to the previously best performing techniques based on different types\nof neural networks. We observe large improvements in accuracy at much lower\ncomputational cost, i.e. it takes less than a day to learn high quality word\nvectors from a 1.6 billion words data set. Furthermore, we show that these\nvectors provide state-of-the-art performance on our test set for measuring\nsyntactic and semantic word similarities.",
        "full_text": "Ef\ufb01cient Estimation of Word Representations in\nVector Space\nTomas Mikolov\nGoogle Inc., Mountain View, CA\ntmikolov@google.com\nKai Chen\nGoogle Inc., Mountain View, CA\nkaichen@google.com\nGreg Corrado\nGoogle Inc., Mountain View, CA\ngcorrado@google.com\nJeffrey Dean\nGoogle Inc., Mountain View, CA\njeff@google.com\nAbstract\nWe propose two novel model architectures for computing continuous vector repre-\nsentations of words from very large data sets. The quality of these representations\nis measured in a word similarity task, and the results are compared to the previ-\nously best performing techniques based on different types of neural networks. We\nobserve large improvements in accuracy at much lower computational cost, i.e. it\ntakes less than a day to learn high quality word vectors from a 1.6 billion words\ndata set. Furthermore, we show that these vectors provide state-of-the-art perfor-\nmance on our test set for measuring syntactic and semantic word similarities.\n1\nIntroduction\nMany current NLP systems and techniques treat words as atomic units - there is no notion of similar-\nity between words, as these are represented as indices in a vocabulary. This choice has several good\nreasons - simplicity, robustness and the observation that simple models trained on huge amounts of\ndata outperform complex systems trained on less data. An example is the popular N-gram model\nused for statistical language modeling - today, it is possible to train N-grams on virtually all available\ndata (trillions of words [3]).\nHowever, the simple techniques are at their limits in many tasks. For example, the amount of\nrelevant in-domain data for automatic speech recognition is limited - the performance is usually\ndominated by the size of high quality transcribed speech data (often just millions of words). In\nmachine translation, the existing corpora for many languages contain only a few billions of words\nor less. Thus, there are situations where simple scaling up of the basic techniques will not result in\nany signi\ufb01cant progress, and we have to focus on more advanced techniques.\nWith progress of machine learning techniques in recent years, it has become possible to train more\ncomplex models on much larger data set, and they typically outperform the simple models. Probably\nthe most successful concept is to use distributed representations of words [10]. For example, neural\nnetwork based language models signi\ufb01cantly outperform N-gram models [1, 27, 17].\n1.1\nGoals of the Paper\nThe main goal of this paper is to introduce techniques that can be used for learning high-quality word\nvectors from huge data sets with billions of words, and with millions of words in the vocabulary. As\nfar as we know, none of the previously proposed architectures has been successfully trained on more\n1\narXiv:1301.3781v3  [cs.CL]  7 Sep 2013\nthan a few hundred of millions of words, with a modest dimensionality of the word vectors between\n50 - 100.\nWe use recently proposed techniques for measuring the quality of the resulting vector representa-\ntions, with the expectation that not only will similar words tend to be close to each other, but that\nwords can have multiple degrees of similarity [20]. This has been observed earlier in the context\nof in\ufb02ectional languages - for example, nouns can have multiple word endings, and if we search for\nsimilar words in a subspace of the original vector space, it is possible to \ufb01nd words that have similar\nendings [13, 14].\nSomewhat surprisingly, it was found that similarity of word representations goes beyond simple\nsyntactic regularities. Using a word offset technique where simple algebraic operations are per-\nformed on the word vectors, it was shown for example that vector(\u201dKing\u201d) - vector(\u201dMan\u201d) + vec-\ntor(\u201dWoman\u201d) results in a vector that is closest to the vector representation of the word Queen [20].\nIn this paper, we try to maximize accuracy of these vector operations by developing new model\narchitectures that preserve the linear regularities among words. We design a new comprehensive test\nset for measuring both syntactic and semantic regularities1, and show that many such regularities\ncan be learned with high accuracy. Moreover, we discuss how training time and accuracy depends\non the dimensionality of the word vectors and on the amount of the training data.\n1.2\nPrevious Work\nRepresentation of words as continuous vectors has a long history [10, 26, 8]. A very popular model\narchitecture for estimating neural network language model (NNLM) was proposed in [1], where a\nfeedforward neural network with a linear projection layer and a non-linear hidden layer was used to\nlearn jointly the word vector representation and a statistical language model. This work has been\nfollowed by many others.\nAnother interesting architecture of NNLM was presented in [13, 14], where the word vectors are\n\ufb01rst learned using neural network with a single hidden layer. The word vectors are then used to train\nthe NNLM. Thus, the word vectors are learned even without constructing the full NNLM. In this\nwork, we directly extend this architecture, and focus just on the \ufb01rst step where the word vectors are\nlearned using a simple model.\nIt was later shown that the word vectors can be used to signi\ufb01cantly improve and simplify many\nNLP applications [4, 5, 29]. Estimation of the word vectors itself was performed using different\nmodel architectures and trained on various corpora [4, 29, 23, 19, 9], and some of the resulting word\nvectors were made available for future research and comparison2. However, as far as we know, these\narchitectures were signi\ufb01cantly more computationally expensive for training than the one proposed\nin [13], with the exception of certain version of log-bilinear model where diagonal weight matrices\nare used [23].\n2\nModel Architectures\nMany different types of models were proposed for estimating continuous representations of words,\nincluding the well-known Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA).\nIn this paper, we focus on distributed representations of words learned by neural networks, as it was\npreviously shown that they perform signi\ufb01cantly better than LSA for preserving linear regularities\namong words [20, 31]; LDA moreover becomes computationally very expensive on large data sets.\nSimilar to [18], to compare different model architectures we de\ufb01ne \ufb01rst the computational complex-\nity of a model as the number of parameters that need to be accessed to fully train the model. Next,\nwe will try to maximize the accuracy, while minimizing the computational complexity.\n1The test set is available at www.fit.vutbr.cz/\u02dcimikolov/rnnlm/word-test.v1.txt\n2http://ronan.collobert.com/senna/\nhttp://metaoptimize.com/projects/wordreprs/\nhttp://www.fit.vutbr.cz/\u02dcimikolov/rnnlm/\nhttp://ai.stanford.edu/\u02dcehhuang/\n2\nFor all the following models, the training complexity is proportional to\nO = E \u00d7 T \u00d7 Q,\n(1)\nwhere E is number of the training epochs, T is the number of the words in the training set and Q is\nde\ufb01ned further for each model architecture. Common choice is E = 3 \u221250 and T up to one billion.\nAll models are trained using stochastic gradient descent and backpropagation [26].\n2.1\nFeedforward Neural Net Language Model (NNLM)\nThe probabilistic feedforward neural network language model has been proposed in [1]. It consists\nof input, projection, hidden and output layers. At the input layer, N previous words are encoded\nusing 1-of-V coding, where V is size of the vocabulary. The input layer is then projected to a\nprojection layer P that has dimensionality N \u00d7 D, using a shared projection matrix. As only N\ninputs are active at any given time, composition of the projection layer is a relatively cheap operation.\nThe NNLM architecture becomes complex for computation between the projection and the hidden\nlayer, as values in the projection layer are dense. For a common choice of N = 10, the size of the\nprojection layer (P) might be 500 to 2000, while the hidden layer size H is typically 500 to 1000\nunits. Moreover, the hidden layer is used to compute probability distribution over all the words in the\nvocabulary, resulting in an output layer with dimensionality V . Thus, the computational complexity\nper each training example is\nQ = N \u00d7 D + N \u00d7 D \u00d7 H + H \u00d7 V,\n(2)\nwhere the dominating term is H \u00d7 V . However, several practical solutions were proposed for\navoiding it; either using hierarchical versions of the softmax [25, 23, 18], or avoiding normalized\nmodels completely by using models that are not normalized during training [4, 9]. With binary tree\nrepresentations of the vocabulary, the number of output units that need to be evaluated can go down\nto around log2(V ). Thus, most of the complexity is caused by the term N \u00d7 D \u00d7 H.\nIn our models, we use hierarchical softmax where the vocabulary is represented as a Huffman binary\ntree. This follows previous observations that the frequency of words works well for obtaining classes\nin neural net language models [16]. Huffman trees assign short binary codes to frequent words, and\nthis further reduces the number of output units that need to be evaluated: while balanced binary tree\nwould require log2(V ) outputs to be evaluated, the Huffman tree based hierarchical softmax requires\nonly about log2(Unigram perplexity(V )). For example when the vocabulary size is one million\nwords, this results in about two times speedup in evaluation. While this is not crucial speedup for\nneural network LMs as the computational bottleneck is in the N \u00d7D\u00d7H term, we will later propose\narchitectures that do not have hidden layers and thus depend heavily on the ef\ufb01ciency of the softmax\nnormalization.\n2.2\nRecurrent Neural Net Language Model (RNNLM)\nRecurrent neural network based language model has been proposed to overcome certain limitations\nof the feedforward NNLM, such as the need to specify the context length (the order of the model N),\nand because theoretically RNNs can ef\ufb01ciently represent more complex patterns than the shallow\nneural networks [15, 2]. The RNN model does not have a projection layer; only input, hidden and\noutput layer. What is special for this type of model is the recurrent matrix that connects hidden\nlayer to itself, using time-delayed connections. This allows the recurrent model to form some kind\nof short term memory, as information from the past can be represented by the hidden layer state that\ngets updated based on the current input and the state of the hidden layer in the previous time step.\nThe complexity per training example of the RNN model is\nQ = H \u00d7 H + H \u00d7 V,\n(3)\nwhere the word representations D have the same dimensionality as the hidden layer H. Again, the\nterm H \u00d7 V can be ef\ufb01ciently reduced to H \u00d7 log2(V ) by using hierarchical softmax. Most of the\ncomplexity then comes from H \u00d7 H.\n3\n2.3\nParallel Training of Neural Networks\nTo train models on huge data sets, we have implemented several models on top of a large-scale\ndistributed framework called DistBelief [6], including the feedforward NNLM and the new models\nproposed in this paper. The framework allows us to run multiple replicas of the same model in\nparallel, and each replica synchronizes its gradient updates through a centralized server that keeps\nall the parameters. For this parallel training, we use mini-batch asynchronous gradient descent with\nan adaptive learning rate procedure called Adagrad [7]. Under this framework, it is common to use\none hundred or more model replicas, each using many CPU cores at different machines in a data\ncenter.\n3\nNew Log-linear Models\nIn this section, we propose two new model architectures for learning distributed representations\nof words that try to minimize computational complexity. The main observation from the previous\nsection was that most of the complexity is caused by the non-linear hidden layer in the model. While\nthis is what makes neural networks so attractive, we decided to explore simpler models that might\nnot be able to represent the data as precisely as neural networks, but can possibly be trained on much\nmore data ef\ufb01ciently.\nThe new architectures directly follow those proposed in our earlier work [13, 14], where it was\nfound that neural network language model can be successfully trained in two steps: \ufb01rst, continuous\nword vectors are learned using simple model, and then the N-gram NNLM is trained on top of these\ndistributed representations of words. While there has been later substantial amount of work that\nfocuses on learning word vectors, we consider the approach proposed in [13] to be the simplest one.\nNote that related models have been proposed also much earlier [26, 8].\n3.1\nContinuous Bag-of-Words Model\nThe \ufb01rst proposed architecture is similar to the feedforward NNLM, where the non-linear hidden\nlayer is removed and the projection layer is shared for all words (not just the projection matrix);\nthus, all words get projected into the same position (their vectors are averaged). We call this archi-\ntecture a bag-of-words model as the order of words in the history does not in\ufb02uence the projection.\nFurthermore, we also use words from the future; we have obtained the best performance on the task\nintroduced in the next section by building a log-linear classi\ufb01er with four future and four history\nwords at the input, where the training criterion is to correctly classify the current (middle) word.\nTraining complexity is then\nQ = N \u00d7 D + D \u00d7 log2(V ).\n(4)\nWe denote this model further as CBOW, as unlike standard bag-of-words model, it uses continuous\ndistributed representation of the context. The model architecture is shown at Figure 1. Note that the\nweight matrix between the input and the projection layer is shared for all word positions in the same\nway as in the NNLM.\n3.2\nContinuous Skip-gram Model\nThe second architecture is similar to CBOW, but instead of predicting the current word based on the\ncontext, it tries to maximize classi\ufb01cation of a word based on another word in the same sentence.\nMore precisely, we use each current word as an input to a log-linear classi\ufb01er with continuous\nprojection layer, and predict words within a certain range before and after the current word. We\nfound that increasing the range improves quality of the resulting word vectors, but it also increases\nthe computational complexity. Since the more distant words are usually less related to the current\nword than those close to it, we give less weight to the distant words by sampling less from those\nwords in our training examples.\nThe training complexity of this architecture is proportional to\nQ = C \u00d7 (D + D \u00d7 log2(V )),\n(5)\nwhere C is the maximum distance of the words. Thus, if we choose C = 5, for each training word\nwe will select randomly a number R in range < 1; C >, and then use R words from history and\n4\nw(t-2)\nw(t+1)\nw(t-1)\nw(t+2)\nw(t)\nSUM\n       INPUT         PROJECTION         OUTPUT\nw(t)\n          INPUT         PROJECTION      OUTPUT\nw(t-2)\nw(t-1)\nw(t+1)\nw(t+2)\n                   CBOW                                                   Skip-gram\nFigure 1: New model architectures. The CBOW architecture predicts the current word based on the\ncontext, and the Skip-gram predicts surrounding words given the current word.\nR words from the future of the current word as correct labels. This will require us to do R \u00d7 2\nword classi\ufb01cations, with the current word as input, and each of the R + R words as output. In the\nfollowing experiments, we use C = 10.\n4\nResults\nTo compare the quality of different versions of word vectors, previous papers typically use a table\nshowing example words and their most similar words, and understand them intuitively. Although\nit is easy to show that word France is similar to Italy and perhaps some other countries, it is much\nmore challenging when subjecting those vectors in a more complex similarity task, as follows. We\nfollow previous observation that there can be many different types of similarities between words, for\nexample, word big is similar to bigger in the same sense that small is similar to smaller. Example\nof another type of relationship can be word pairs big - biggest and small - smallest [20]. We further\ndenote two pairs of words with the same relationship as a question, as we can ask: \u201dWhat is the\nword that is similar to small in the same sense as biggest is similar to big?\u201d\nSomewhat surprisingly, these questions can be answered by performing simple algebraic operations\nwith the vector representation of words. To \ufb01nd a word that is similar to small in the same sense as\nbiggest is similar to big, we can simply compute vector X = vector(\u201dbiggest\u201d)\u2212vector(\u201dbig\u201d)+\nvector(\u201dsmall\u201d). Then, we search in the vector space for the word closest to X measured by cosine\ndistance, and use it as the answer to the question (we discard the input question words during this\nsearch). When the word vectors are well trained, it is possible to \ufb01nd the correct answer (word\nsmallest) using this method.\nFinally, we found that when we train high dimensional word vectors on a large amount of data, the\nresulting vectors can be used to answer very subtle semantic relationships between words, such as\na city and the country it belongs to, e.g. France is to Paris as Germany is to Berlin. Word vectors\nwith such semantic relationships could be used to improve many existing NLP applications, such\nas machine translation, information retrieval and question answering systems, and may enable other\nfuture applications yet to be invented.\n5\nTable 1: Examples of \ufb01ve types of semantic and nine types of syntactic questions in the Semantic-\nSyntactic Word Relationship test set.\nType of relationship\nWord Pair 1\nWord Pair 2\nCommon capital city\nAthens\nGreece\nOslo\nNorway\nAll capital cities\nAstana\nKazakhstan\nHarare\nZimbabwe\nCurrency\nAngola\nkwanza\nIran\nrial\nCity-in-state\nChicago\nIllinois\nStockton\nCalifornia\nMan-Woman\nbrother\nsister\ngrandson\ngranddaughter\nAdjective to adverb\napparent\napparently\nrapid\nrapidly\nOpposite\npossibly\nimpossibly\nethical\nunethical\nComparative\ngreat\ngreater\ntough\ntougher\nSuperlative\neasy\neasiest\nlucky\nluckiest\nPresent Participle\nthink\nthinking\nread\nreading\nNationality adjective\nSwitzerland\nSwiss\nCambodia\nCambodian\nPast tense\nwalking\nwalked\nswimming\nswam\nPlural nouns\nmouse\nmice\ndollar\ndollars\nPlural verbs\nwork\nworks\nspeak\nspeaks\n4.1\nTask Description\nTo measure quality of the word vectors, we de\ufb01ne a comprehensive test set that contains \ufb01ve types\nof semantic questions, and nine types of syntactic questions. Two examples from each category are\nshown in Table 1. Overall, there are 8869 semantic and 10675 syntactic questions. The questions\nin each category were created in two steps: \ufb01rst, a list of similar word pairs was created manually.\nThen, a large list of questions is formed by connecting two word pairs. For example, we made a\nlist of 68 large American cities and the states they belong to, and formed about 2.5K questions by\npicking two word pairs at random. We have included in our test set only single token words, thus\nmulti-word entities are not present (such as New York).\nWe evaluate the overall accuracy for all question types, and for each question type separately (se-\nmantic, syntactic). Question is assumed to be correctly answered only if the closest word to the\nvector computed using the above method is exactly the same as the correct word in the question;\nsynonyms are thus counted as mistakes. This also means that reaching 100% accuracy is likely\nto be impossible, as the current models do not have any input information about word morphology.\nHowever, we believe that usefulness of the word vectors for certain applications should be positively\ncorrelated with this accuracy metric. Further progress can be achieved by incorporating information\nabout structure of words, especially for the syntactic questions.\n4.2\nMaximization of Accuracy\nWe have used a Google News corpus for training the word vectors. This corpus contains about\n6B tokens. We have restricted the vocabulary size to 1 million most frequent words. Clearly, we\nare facing time constrained optimization problem, as it can be expected that both using more data\nand higher dimensional word vectors will improve the accuracy. To estimate the best choice of\nmodel architecture for obtaining as good as possible results quickly, we have \ufb01rst evaluated models\ntrained on subsets of the training data, with vocabulary restricted to the most frequent 30k words.\nThe results using the CBOW architecture with different choice of word vector dimensionality and\nincreasing amount of the training data are shown in Table 2.\nIt can be seen that after some point, adding more dimensions or adding more training data provides\ndiminishing improvements. So, we have to increase both vector dimensionality and the amount\nof the training data together. While this observation might seem trivial, it must be noted that it is\ncurrently popular to train word vectors on relatively large amounts of data, but with insuf\ufb01cient size\n6\nTable 2:\nAccuracy on subset of the Semantic-Syntactic Word Relationship test set, using word\nvectors from the CBOW architecture with limited vocabulary. Only questions containing words from\nthe most frequent 30k words are used.\nDimensionality / Training words\n24M\n49M\n98M\n196M\n391M\n783M\n50\n13.4\n15.7\n18.6\n19.1\n22.5\n23.2\n100\n19.4\n23.1\n27.8\n28.7\n33.4\n32.2\n300\n23.2\n29.2\n35.3\n38.6\n43.7\n45.9\n600\n24.0\n30.1\n36.5\n40.8\n46.6\n50.4\nTable 3: Comparison of architectures using models trained on the same data, with 640-dimensional\nword vectors. The accuracies are reported on our Semantic-Syntactic Word Relationship test set,\nand on the syntactic relationship test set of [20]\nModel\nSemantic-Syntactic Word Relationship test set\nMSR Word Relatedness\nArchitecture\nSemantic Accuracy [%]\nSyntactic Accuracy [%]\nTest Set [20]\nRNNLM\n9\n36\n35\nNNLM\n23\n53\n47\nCBOW\n24\n64\n61\nSkip-gram\n55\n59\n56\n(such as 50 - 100). Given Equation 4, increasing amount of training data twice results in about the\nsame increase of computational complexity as increasing vector size twice.\nFor the experiments reported in Tables 2 and 4, we used three training epochs with stochastic gradi-\nent descent and backpropagation. We chose starting learning rate 0.025 and decreased it linearly, so\nthat it approaches zero at the end of the last training epoch.\n4.3\nComparison of Model Architectures\nFirst we compare different model architectures for deriving the word vectors using the same training\ndata and using the same dimensionality of 640 of the word vectors. In the further experiments, we\nuse full set of questions in the new Semantic-Syntactic Word Relationship test set, i.e. unrestricted to\nthe 30k vocabulary. We also include results on a test set introduced in [20] that focuses on syntactic\nsimilarity between words3.\nThe training data consists of several LDC corpora and is described in detail in [18] (320M words,\n82K vocabulary). We used these data to provide a comparison to a previously trained recurrent\nneural network language model that took about 8 weeks to train on a single CPU. We trained a feed-\nforward NNLM with the same number of 640 hidden units using the DistBelief parallel training [6],\nusing a history of 8 previous words (thus, the NNLM has more parameters than the RNNLM, as the\nprojection layer has size 640 \u00d7 8).\nIn Table 3, it can be seen that the word vectors from the RNN (as used in [20]) perform well mostly\non the syntactic questions. The NNLM vectors perform signi\ufb01cantly better than the RNN - this is\nnot surprising, as the word vectors in the RNNLM are directly connected to a non-linear hidden\nlayer. The CBOW architecture works better than the NNLM on the syntactic tasks, and about the\nsame on the semantic one. Finally, the Skip-gram architecture works slightly worse on the syntactic\ntask than the CBOW model (but still better than the NNLM), and much better on the semantic part\nof the test than all the other models.\nNext, we evaluated our models trained using one CPU only and compared the results against publicly\navailable word vectors. The comparison is given in Table 4. The CBOW model was trained on subset\n3We thank Geoff Zweig for providing us the test set.\n7\nTable 4: Comparison of publicly available word vectors on the Semantic-Syntactic Word Relation-\nship test set, and word vectors from our models. Full vocabularies are used.\nModel\nVector\nTraining\nAccuracy [%]\nDimensionality\nwords\nSemantic\nSyntactic\nTotal\nCollobert-Weston NNLM\n50\n660M\n9.3\n12.3\n11.0\nTurian NNLM\n50\n37M\n1.4\n2.6\n2.1\nTurian NNLM\n200\n37M\n1.4\n2.2\n1.8\nMnih NNLM\n50\n37M\n1.8\n9.1\n5.8\nMnih NNLM\n100\n37M\n3.3\n13.2\n8.8\nMikolov RNNLM\n80\n320M\n4.9\n18.4\n12.7\nMikolov RNNLM\n640\n320M\n8.6\n36.5\n24.6\nHuang NNLM\n50\n990M\n13.3\n11.6\n12.3\nOur NNLM\n20\n6B\n12.9\n26.4\n20.3\nOur NNLM\n50\n6B\n27.9\n55.8\n43.2\nOur NNLM\n100\n6B\n34.2\n64.5\n50.8\nCBOW\n300\n783M\n15.5\n53.1\n36.1\nSkip-gram\n300\n783M\n50.0\n55.9\n53.3\nTable 5: Comparison of models trained for three epochs on the same data and models trained for\none epoch. Accuracy is reported on the full Semantic-Syntactic data set.\nModel\nVector\nTraining\nAccuracy [%]\nTraining time\nDimensionality\nwords\n[days]\nSemantic\nSyntactic\nTotal\n3 epoch CBOW\n300\n783M\n15.5\n53.1\n36.1\n1\n3 epoch Skip-gram\n300\n783M\n50.0\n55.9\n53.3\n3\n1 epoch CBOW\n300\n783M\n13.8\n49.9\n33.6\n0.3\n1 epoch CBOW\n300\n1.6B\n16.1\n52.6\n36.1\n0.6\n1 epoch CBOW\n600\n783M\n15.4\n53.3\n36.2\n0.7\n1 epoch Skip-gram\n300\n783M\n45.6\n52.2\n49.2\n1\n1 epoch Skip-gram\n300\n1.6B\n52.2\n55.1\n53.8\n2\n1 epoch Skip-gram\n600\n783M\n56.7\n54.5\n55.5\n2.5\nof the Google News data in about a day, while training time for the Skip-gram model was about three\ndays.\nFor experiments reported further, we used just one training epoch (again, we decrease the learning\nrate linearly so that it approaches zero at the end of training). Training a model on twice as much\ndata using one epoch gives comparable or better results than iterating over the same data for three\nepochs, as is shown in Table 5, and provides additional small speedup.\n4.4\nLarge Scale Parallel Training of Models\nAs mentioned earlier, we have implemented various models in a distributed framework called Dis-\ntBelief. Below we report the results of several models trained on the Google News 6B data set,\nwith mini-batch asynchronous gradient descent and the adaptive learning rate procedure called Ada-\ngrad [7]. We used 50 to 100 model replicas during the training. The number of CPU cores is an\n8\nTable 6:\nComparison of models trained using the DistBelief distributed framework. Note that\ntraining of NNLM with 1000-dimensional vectors would take too long to complete.\nModel\nVector\nTraining\nAccuracy [%]\nTraining time\nDimensionality\nwords\n[days x CPU cores]\nSemantic\nSyntactic\nTotal\nNNLM\n100\n6B\n34.2\n64.5\n50.8\n14 x 180\nCBOW\n1000\n6B\n57.3\n68.9\n63.7\n2 x 140\nSkip-gram\n1000\n6B\n66.1\n65.1\n65.6\n2.5 x 125\nTable 7: Comparison and combination of models on the Microsoft Sentence Completion Challenge.\nArchitecture\nAccuracy [%]\n4-gram [32]\n39\nAverage LSA similarity [32]\n49\nLog-bilinear model [24]\n54.8\nRNNLMs [19]\n55.4\nSkip-gram\n48.0\nSkip-gram + RNNLMs\n58.9\nestimate since the data center machines are shared with other production tasks, and the usage can\n\ufb02uctuate quite a bit. Note that due to the overhead of the distributed framework, the CPU usage of\nthe CBOW model and the Skip-gram model are much closer to each other than their single-machine\nimplementations. The result are reported in Table 6.\n4.5\nMicrosoft Research Sentence Completion Challenge\nThe Microsoft Sentence Completion Challenge has been recently introduced as a task for advancing\nlanguage modeling and other NLP techniques [32]. This task consists of 1040 sentences, where one\nword is missing in each sentence and the goal is to select word that is the most coherent with the\nrest of the sentence, given a list of \ufb01ve reasonable choices. Performance of several techniques has\nbeen already reported on this set, including N-gram models, LSA-based model [32], log-bilinear\nmodel [24] and a combination of recurrent neural networks that currently holds the state of the art\nperformance of 55.4% accuracy on this benchmark [19].\nWe have explored the performance of Skip-gram architecture on this task. First, we train the 640-\ndimensional model on 50M words provided in [32]. Then, we compute score of each sentence in\nthe test set by using the unknown word at the input, and predict all surrounding words in a sentence.\nThe \ufb01nal sentence score is then the sum of these individual predictions. Using the sentence scores,\nwe choose the most likely sentence.\nA short summary of some previous results together with the new results is presented in Table 7.\nWhile the Skip-gram model itself does not perform on this task better than LSA similarity, the scores\nfrom this model are complementary to scores obtained with RNNLMs, and a weighted combination\nleads to a new state of the art result 58.9% accuracy (59.2% on the development part of the set and\n58.7% on the test part of the set).\n5\nExamples of the Learned Relationships\nTable 8 shows words that follow various relationships. We follow the approach described above: the\nrelationship is de\ufb01ned by subtracting two word vectors, and the result is added to another word. Thus\nfor example, Paris - France + Italy = Rome. As it can be seen, accuracy is quite good, although\nthere is clearly a lot of room for further improvements (note that using our accuracy metric that\n9\nTable 8: Examples of the word pair relationships, using the best word vectors from Table 4 (Skip-\ngram model trained on 783M words with 300 dimensionality).\nRelationship\nExample 1\nExample 2\nExample 3\nFrance - Paris\nItaly: Rome\nJapan: Tokyo\nFlorida: Tallahassee\nbig - bigger\nsmall: larger\ncold: colder\nquick: quicker\nMiami - Florida\nBaltimore: Maryland\nDallas: Texas\nKona: Hawaii\nEinstein - scientist\nMessi: mid\ufb01elder\nMozart: violinist\nPicasso: painter\nSarkozy - France\nBerlusconi: Italy\nMerkel: Germany\nKoizumi: Japan\ncopper - Cu\nzinc: Zn\ngold: Au\nuranium: plutonium\nBerlusconi - Silvio\nSarkozy: Nicolas\nPutin: Medvedev\nObama: Barack\nMicrosoft - Windows\nGoogle: Android\nIBM: Linux\nApple: iPhone\nMicrosoft - Ballmer\nGoogle: Yahoo\nIBM: McNealy\nApple: Jobs\nJapan - sushi\nGermany: bratwurst\nFrance: tapas\nUSA: pizza\nassumes exact match, the results in Table 8 would score only about 60%). We believe that word\nvectors trained on even larger data sets with larger dimensionality will perform signi\ufb01cantly better,\nand will enable the development of new innovative applications. Another way to improve accuracy is\nto provide more than one example of the relationship. By using ten examples instead of one to form\nthe relationship vector (we average the individual vectors together), we have observed improvement\nof accuracy of our best models by about 10% absolutely on the semantic-syntactic test.\nIt is also possible to apply the vector operations to solve different tasks. For example, we have\nobserved good accuracy for selecting out-of-the-list words, by computing average vector for a list of\nwords, and \ufb01nding the most distant word vector. This is a popular type of problems in certain human\nintelligence tests. Clearly, there is still a lot of discoveries to be made using these techniques.\n6\nConclusion\nIn this paper we studied the quality of vector representations of words derived by various models on\na collection of syntactic and semantic language tasks. We observed that it is possible to train high\nquality word vectors using very simple model architectures, compared to the popular neural network\nmodels (both feedforward and recurrent). Because of the much lower computational complexity, it\nis possible to compute very accurate high dimensional word vectors from a much larger data set.\nUsing the DistBelief distributed framework, it should be possible to train the CBOW and Skip-gram\nmodels even on corpora with one trillion words, for basically unlimited size of the vocabulary. That\nis several orders of magnitude larger than the best previously published results for similar models.\nAn interesting task where the word vectors have recently been shown to signi\ufb01cantly outperform the\nprevious state of the art is the SemEval-2012 Task 2 [11]. The publicly available RNN vectors were\nused together with other techniques to achieve over 50% increase in Spearman\u2019s rank correlation\nover the previous best result [31]. The neural network based word vectors were previously applied\nto many other NLP tasks, for example sentiment analysis [12] and paraphrase detection [28]. It can\nbe expected that these applications can bene\ufb01t from the model architectures described in this paper.\nOur ongoing work shows that the word vectors can be successfully applied to automatic extension\nof facts in Knowledge Bases, and also for veri\ufb01cation of correctness of existing facts. Results\nfrom machine translation experiments also look very promising. In the future, it would be also\ninteresting to compare our techniques to Latent Relational Analysis [30] and others. We believe that\nour comprehensive test set will help the research community to improve the existing techniques for\nestimating the word vectors. We also expect that high quality word vectors will become an important\nbuilding block for future NLP applications.\n10\n7\nFollow-Up Work\nAfter the initial version of this paper was written, we published single-machine multi-threaded C++\ncode for computing the word vectors, using both the continuous bag-of-words and skip-gram archi-\ntectures4. The training speed is signi\ufb01cantly higher than reported earlier in this paper, i.e. it is in the\norder of billions of words per hour for typical hyperparameter choices. We also published more than\n1.4 million vectors that represent named entities, trained on more than 100 billion words. Some of\nour follow-up work will be published in an upcoming NIPS 2013 paper [21].\nReferences\n[1] Y. Bengio, R. Ducharme, P. Vincent. A neural probabilistic language model. Journal of Ma-\nchine Learning Research, 3:1137-1155, 2003.\n[2] Y. Bengio, Y. LeCun. Scaling learning algorithms towards AI. In: Large-Scale Kernel Ma-\nchines, MIT Press, 2007.\n[3] T. Brants, A. C. Popat, P. Xu, F. J. Och, and J. Dean. Large language models in machine\ntranslation. In Proceedings of the Joint Conference on Empirical Methods in Natural Language\nProcessing and Computational Language Learning, 2007.\n[4] R. Collobert and J. Weston. A Uni\ufb01ed Architecture for Natural Language Processing: Deep\nNeural Networks with Multitask Learning. In International Conference on Machine Learning,\nICML, 2008.\n[5] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu and P. Kuksa. Natural Lan-\nguage Processing (Almost) from Scratch. Journal of Machine Learning Research, 12:2493-\n2537, 2011.\n[6] J. Dean, G.S. Corrado, R. Monga, K. Chen, M. Devin, Q.V. Le, M.Z. Mao, M.A. Ranzato, A.\nSenior, P. Tucker, K. Yang, A. Y. Ng., Large Scale Distributed Deep Networks, NIPS, 2012.\n[7] J.C. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and\nstochastic optimization. Journal of Machine Learning Research, 2011.\n[8] J. Elman. Finding Structure in Time. Cognitive Science, 14, 179-211, 1990.\n[9] Eric H. Huang, R. Socher, C. D. Manning and Andrew Y. Ng. Improving Word Representations\nvia Global Context and Multiple Word Prototypes. In: Proc. Association for Computational\nLinguistics, 2012.\n[10] G.E. Hinton, J.L. McClelland, D.E. Rumelhart. Distributed representations. In: Parallel dis-\ntributed processing: Explorations in the microstructure of cognition. Volume 1: Foundations,\nMIT Press, 1986.\n[11] D.A. Jurgens, S.M. Mohammad, P.D. Turney, K.J. Holyoak. Semeval-2012 task 2: Measuring\ndegrees of relational similarity. In: Proceedings of the 6th International Workshop on Semantic\nEvaluation (SemEval 2012), 2012.\n[12] A.L. Maas, R.E. Daly, P.T. Pham, D. Huang, A.Y. Ng, and C. Potts. Learning word vectors for\nsentiment analysis. In Proceedings of ACL, 2011.\n[13] T. Mikolov. Language Modeling for Speech Recognition in Czech, Masters thesis, Brno Uni-\nversity of Technology, 2007.\n[14] T. Mikolov, J. Kopeck\u00b4y, L. Burget, O. Glembek and J. \u02c7Cernock\u00b4y. Neural network based lan-\nguage models for higly in\ufb02ective languages, In: Proc. ICASSP 2009.\n[15] T. Mikolov, M. Kara\ufb01\u00b4at, L. Burget, J. \u02c7Cernock\u00b4y, S. Khudanpur. Recurrent neural network\nbased language model, In: Proceedings of Interspeech, 2010.\n[16] T. Mikolov, S. Kombrink, L. Burget, J. \u02c7Cernock\u00b4y, S. Khudanpur. Extensions of recurrent neural\nnetwork language model, In: Proceedings of ICASSP 2011.\n[17] T. Mikolov, A. Deoras, S. Kombrink, L. Burget, J. \u02c7Cernock\u00b4y. Empirical Evaluation and Com-\nbination of Advanced Language Modeling Techniques, In: Proceedings of Interspeech, 2011.\n4The code is available at https://code.google.com/p/word2vec/\n11\n[18] T. Mikolov, A. Deoras, D. Povey, L. Burget, J. \u02c7Cernock\u00b4y. Strategies for Training Large Scale\nNeural Network Language Models, In: Proc. Automatic Speech Recognition and Understand-\ning, 2011.\n[19] T. Mikolov. Statistical Language Models based on Neural Networks. PhD thesis, Brno Univer-\nsity of Technology, 2012.\n[20] T. Mikolov, W.T. Yih, G. Zweig. Linguistic Regularities in Continuous Space Word Represen-\ntations. NAACL HLT 2013.\n[21] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean. Distributed Representations of\nWords and Phrases and their Compositionality. Accepted to NIPS 2013.\n[22] A. Mnih, G. Hinton. Three new graphical models for statistical language modelling. ICML,\n2007.\n[23] A. Mnih, G. Hinton. A Scalable Hierarchical Distributed Language Model. Advances in Neural\nInformation Processing Systems 21, MIT Press, 2009.\n[24] A. Mnih, Y.W. Teh. A fast and simple algorithm for training neural probabilistic language\nmodels. ICML, 2012.\n[25] F. Morin, Y. Bengio. Hierarchical Probabilistic Neural Network Language Model. AISTATS,\n2005.\n[26] D. E. Rumelhart, G. E. Hinton, R. J. Williams. Learning internal representations by back-\npropagating errors. Nature, 323:533.536, 1986.\n[27] H. Schwenk. Continuous space language models. Computer Speech and Language, vol. 21,\n2007.\n[28] R. Socher, E.H. Huang, J. Pennington, A.Y. Ng, and C.D. Manning. Dynamic Pooling and\nUnfolding Recursive Autoencoders for Paraphrase Detection. In NIPS, 2011.\n[29] J. Turian, L. Ratinov, Y. Bengio. Word Representations: A Simple and General Method for\nSemi-Supervised Learning. In: Proc. Association for Computational Linguistics, 2010.\n[30] P. D. Turney. Measuring Semantic Similarity by Latent Relational Analysis. In: Proc. Interna-\ntional Joint Conference on Arti\ufb01cial Intelligence, 2005.\n[31] A. Zhila, W.T. Yih, C. Meek, G. Zweig, T. Mikolov. Combining Heterogeneous Models for\nMeasuring Relational Similarity. NAACL HLT 2013.\n[32] G. Zweig, C.J.C. Burges. The Microsoft Research Sentence Completion Challenge, Microsoft\nResearch Technical Report MSR-TR-2011-129, 2011.\n12\n",
        "sentence": " [46].",
        "context": "in [13], with the exception of certain version of log-bilinear model where diagonal weight matrices\nare used [23].\n2\nModel Architectures\nMany different types of models were proposed for estimating continuous representations of words,\nversity of Technology, 2007.\n[14] T. Mikolov, J. Kopeck\u00b4y, L. Burget, O. Glembek and J. \u02c7Cernock\u00b4y. Neural network based lan-\nguage models for higly in\ufb02ective languages, In: Proc. ICASSP 2009.\nbeen already reported on this set, including N-gram models, LSA-based model [32], log-bilinear\nmodel [24] and a combination of recurrent neural networks that currently holds the state of the art\nperformance of 55.4% accuracy on this benchmark [19]."
    },
    {
        "title": "Rectified linear units improve restricted boltzmann machines",
        "author": [
            "Vinod Nair",
            "Geoffrey E Hinton"
        ],
        "venue": "In Proceedings of the 27th International Conference on Machine Learning",
        "citeRegEx": "47",
        "shortCiteRegEx": "47",
        "year": 2010,
        "abstract": "",
        "full_text": "",
        "sentence": " These units have been demonstrated to improve the performance of many deep neural networks ([43], [47], [73]) on tasks as varied as speech processing and object recognition, and have been used in recurrent neural networks by [7].",
        "context": null
    },
    {
        "title": "BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pages 311\u2013318",
        "author": [
            "Kishore Papineni",
            "Salim Roukos",
            "Todd Ward",
            "Wei-Jing Zhu"
        ],
        "venue": "Association for Computational Linguistics,",
        "citeRegEx": "48",
        "shortCiteRegEx": "48",
        "year": 2002,
        "abstract": "",
        "full_text": "",
        "sentence": " Developed in 2002, BLEU score, a common evaluation metric for machine translation, is related to modified unigram precision [48].",
        "context": null
    },
    {
        "title": "On the difficulty of training recurrent neural networks",
        "author": [
            "Razvan Pascanu",
            "Tomas Mikolov",
            "Yoshua Bengio"
        ],
        "venue": "arXiv preprint arXiv:1211.5063,",
        "citeRegEx": "49",
        "shortCiteRegEx": "49",
        "year": 2012,
        "abstract": "There are two widely known issues with properly training Recurrent Neural\nNetworks, the vanishing and the exploding gradient problems detailed in Bengio\net al. (1994). In this paper we attempt to improve the understanding of the\nunderlying issues by exploring these problems from an analytical, a geometric\nand a dynamical systems perspective. Our analysis is used to justify a simple\nyet effective solution. We propose a gradient norm clipping strategy to deal\nwith exploding gradients and a soft constraint for the vanishing gradients\nproblem. We validate empirically our hypothesis and proposed solutions in the\nexperimental section.",
        "full_text": "On the di\ufb03culty of training Recurrent Neural Networks\nRazvan Pascanu\npascanur@iro.umontreal.ca\nUniversite de Montreal\nTomas Mikolov\nt.mikolov@gmail.com\nBrno University\nYoshua Bengio\nyoshua.bengio@umontreal.ca\nUniversite de Montreal\nAbstract\nThere are two widely known issues with prop-\nerly training Recurrent Neural Networks, the\nvanishing and the exploding gradient prob-\nlems detailed in Bengio et al. (1994).\nIn\nthis paper we attempt to improve the under-\nstanding of the underlying issues by explor-\ning these problems from an analytical, a geo-\nmetric and a dynamical systems perspective.\nOur analysis is used to justify a simple yet ef-\nfective solution. We propose a gradient norm\nclipping strategy to deal with exploding gra-\ndients and a soft constraint for the vanishing\ngradients problem. We validate empirically\nour hypothesis and proposed solutions in the\nexperimental section.\n1. Introduction\nA recurrent neural network (RNN), e.g. Fig. 1, is a\nneural network model proposed in the 80\u2019s (Rumelhart\net al., 1986; Elman, 1990; Werbos, 1988) for modeling\ntime series. The structure of the network is similar to\nthat of a standard multilayer perceptron, with the dis-\ntinction that we allow connections among hidden units\nassociated with a time delay. Through these connec-\ntions the model can retain information about the past\ninputs, enabling it to discover temporal correlations\nbetween events that are possibly far away from each\nother in the data (a crucial property for proper learn-\ning of time series).\nWhile in principle the recurrent network is a simple\nand powerful model, in practice, it is unfortunately\nhard to train properly. Among the main reasons why\nthis model is so unwieldy are the vanishing gradient\nut\nxt\nEt\nFigure 1. Schematic of a recurrent neural network.\nThe\nrecurrent connections in the hidden layer allow information\nto persist from one input to another.\nand exploding gradient problems described in Bengio\net al. (1994).\n1.1. Training recurrent networks\nA generic recurrent neural network, with input ut and\nstate xt for time step t, is given by equation (1). In\nthe theoretical section of this paper we will sometimes\nmake use of the speci\ufb01c parametrization given by equa-\ntion (11) 1 in order to provide more precise conditions\nand intuitions about the everyday use-case.\nxt = F(xt\u22121, ut, \u03b8)\n(1)\nxt = Wrec\u03c3(xt\u22121) + Winut + b\n(2)\nThe parameters of the model are given by the recurrent\nweight matrix Wrec, the biases b and input weight\nmatrix Win, collected in \u03b8 for the general case. x0 is\nprovided by the user, set to zero or learned, and \u03c3 is an\nelement-wise function (usually the tanh or sigmoid).\nA cost E measures the performance of the network\non some given task and it can be broken apart into\nindividual costs for each step E = P\n1\u2264t\u2264T Et, where\nEt = L(xt).\nOne approach that can be used to compute the nec-\nessary gradients is Backpropagation Through Time\n(BPTT), where the recurrent model is represented as\n1 This formulation is equivalent to the more widely\nknown equation xt = \u03c3(Wrecxt\u22121 + Winut + b), and it\nwas chosen for convenience.\narXiv:1211.5063v2  [cs.LG]  16 Feb 2013\nOn the di\ufb03culty of training Recurrent Neural Networks\na deep multi-layer one (with an unbounded number of\nlayers) and backpropagation is applied on the unrolled\nmodel (see Fig. 2).\n\u2202Et+1\n\u2202xt+1\nEt+1\nEt\nEt\u22121\nxt+1\nxt\nxt\u22121\nut\u22121\nut\nut+1\n\u2202Et\n\u2202xt\n\u2202Et\u22121\n\u2202xt\u22121\n\u2202xt+2\n\u2202xt+1\n\u2202xt+1\n\u2202xt\n\u2202xt\n\u2202xt\u22121\n\u2202xt\u22121\n\u2202xt\u22122\nFigure 2. Unrolling recurrent neural networks in time by\ncreating a copy of the model for each time step. We denote\nby xt the hidden state of the network at time t, by ut the\ninput of the network at time t and by Et the error obtained\nfrom the output at time t.\nWe will diverge from the classical BPTT equations at\nthis point and re-write the gradients (see equations (3),\n(4) and (5)) in order to better highlight the exploding\ngradients problem. These equations were obtained by\nwriting the gradients in a sum-of-products form.\n\u2202E\n\u2202\u03b8 =\nX\n1\u2264t\u2264T\n\u2202Et\n\u2202\u03b8\n(3)\n\u2202Et\n\u2202\u03b8 =\nX\n1\u2264k\u2264t\n\u0012 \u2202Et\n\u2202xt\n\u2202xt\n\u2202xk\n\u2202+xk\n\u2202\u03b8\n\u0013\n(4)\n\u2202xt\n\u2202xk\n=\nY\nt\u2265i>k\n\u2202xi\n\u2202xi\u22121\n=\nY\nt\u2265i>k\nWT\nrecdiag(\u03c3\u2032(xi\u22121)) (5)\n\u2202+xk\n\u2202\u03b8\nrefers to the \u201cimmediate\u201d partial derivative of\nthe state xk with respect to \u03b8, i.e., where xk\u22121 is\ntaken as a constant with respect to \u03b8.\nSpeci\ufb01cally,\nconsidering equation 2, the value of any row i of the\nmatrix ( \u2202+xk\n\u2202Wrec ) is just \u03c3(xk\u22121).\nEquation (5) also\nprovides the form of Jacobian matrix\n\u2202xi\n\u2202xi\u22121 for the\nspeci\ufb01c parametrization given in equation (11), where\ndiag converts a vector into a diagonal matrix, and \u03c3\u2032\ncomputes the derivative of \u03c3 in an element-wise fash-\nion.\nNote that each term\n\u2202Et\n\u2202\u03b8 from equation (3) has the\nsame form and the behaviour of these individual terms\ndetermine the behaviour of the sum. Henceforth we\nwill focus on one such generic term, calling it simply\nthe gradient when there is no confusion.\nAny gradient component \u2202Et\n\u2202\u03b8 is also a sum (see equa-\ntion (4)), whose terms we refer to as temporal contribu-\ntions or temporal components. One can see that each\nsuch temporal contribution \u2202Et\n\u2202xt\n\u2202xt\n\u2202xk\n\u2202+xk\n\u2202\u03b8\nmeasures how\n\u03b8 at step k a\ufb00ects the cost at step t > k. The factors\n\u2202xt\n\u2202xk (equation (5)) transport the error \u201cin time\u201c from\nstep t back to step k. We would further loosely distin-\nguish between long term and short term contributions,\nwhere long term refers to components for which k \u226at\nand short term to everything else.\n2. Exploding and Vanishing Gradients\nAs introduced in Bengio et al. (1994), the exploding\ngradients problem refers to the large increase in the\nnorm of the gradient during training. Such events are\ncaused by the explosion of the long term components,\nwhich can grow exponentially more then short term\nones. The vanishing gradients problem refers to the\nopposite behaviour, when long term components go\nexponentially fast to norm 0, making it impossible for\nthe model to learn correlation between temporally dis-\ntant events.\n2.1. The mechanics\nTo understand this phenomenon we need to look at the\nform of each temporal component, and in particular at\nthe matrix factors \u2202xt\n\u2202xk (see equation (5)) that take the\nform of a product of t \u2212k Jacobian matrices. In the\nsame way a product of t \u2212k real numbers can shrink\nto zero or explode to in\ufb01nity, so does this product of\nmatrices (along some direction v).\nIn what follows we will try to formalize these intu-\nitions (extending a similar derivation done in Bengio\net al. (1994) where only a single hidden unit case was\nconsidered).\nIf we consider a linear version of the model (i.e. set \u03c3 to\nthe identity function in equation (11)) we can use the\npower iteration method to formally analyze this prod-\nuct of Jacobian matrices and obtain tight conditions\nfor when the gradients explode or vanish (see the sup-\nplementary materials for a detailed derivation of these\nconditions). It is su\ufb03cient for the largest eigenvalue\n\u03bb1 of the recurrent weight matrix to be smaller than\n1 for long term components to vanish (as t \u2192\u221e) and\nnecessary for it to be larger than 1 for gradients to\nexplode.\nWe can generalize these results for nonlinear functions\n\u03c3 where the absolute values of \u03c3\u2032(x) is bounded (say\nby a value \u03b3 \u2208R) and therefore \u2225diag(\u03c3\u2032(xk))\u2225\u2264\u03b3.\nWe \ufb01rst prove that it is su\ufb03cient for \u03bb1 < 1\n\u03b3 , where\n\u03bb1 is the absolute value of the largest eigenvalue of\nthe recurrent weight matrix Wrec, for the vanishing\ngradient problem to occur. Note that we assume the\nparametrization given by equation (11). The Jacobian\nmatrix\n\u2202xk+1\n\u2202xk\nis given by WT\nrecdiag(\u03c3\u2032(xk)). The 2-\nnorm of this Jacobian is bounded by the product of\nOn the di\ufb03culty of training Recurrent Neural Networks\nthe norms of the two matrices (see equation (6)). Due\nto our assumption, this implies that it is smaller than\n1.\n\u2200k,\n\r\r\r\r\n\u2202xk+1\n\u2202xk\n\r\r\r\r \u2264\n\r\rWT\nrec\n\r\r \u2225diag(\u03c3\u2032(xk))\u2225< 1\n\u03b3 \u03b3 < 1\n(6)\nLet \u03b7 \u2208R be such that \u2200k,\n\r\r\r \u2202xk+1\n\u2202xk\n\r\r\r \u2264\u03b7 < 1. The\nexistence of \u03b7 is given by equation (6). By induction\nover i, we can show that\n\u2202Et\n\u2202xt\n t\u22121\nY\ni=k\n\u2202xi+1\n\u2202xi\n!\n\u2264\u03b7t\u2212k \u2202Et\n\u2202xt\n(7)\nAs \u03b7 < 1, it follows that, according to equation (7),\nlong term contributions (for which t\u2212k is large) go to\n0 exponentially fast with t \u2212k.\n\u25a1\nBy inverting this proof we get the necessary condition\nfor exploding gradients, namely that the largest eigen-\nvalue \u03bb1 is larger than 1\n\u03b3 (otherwise the long term com-\nponents would vanish instead of exploding). For tanh\nwe have \u03b3 = 1 while for sigmoid we have \u03b3 = 1/4.\n2.2. Drawing similarities with Dynamical\nSystems\nWe can improve our understanding of the exploding\ngradients and vanishing gradients problems by employ-\ning a dynamical systems perspective, as it was done\nbefore in Doya (1993); Bengio et al. (1993).\nWe recommend reading Strogatz (1994) for a formal\nand detailed treatment of dynamical systems theory.\nFor any parameter assignment \u03b8, depending on the ini-\ntial state x0, the state xt of an autonomous dynamical\nsystem converges, under the repeated application of\nthe map F, to one of several possible di\ufb00erent attrac-\ntor states (e.g. point attractors, though other type of\nattractors exist). The model could also \ufb01nd itself in\na chaotic regime, case in which some of the following\nobservations may not hold, but that is not treated in\ndepth here.\nAttractors describe the asymptotic be-\nhaviour of the model. The state space is divided into\nbasins of attraction, one for each attractor.\nIf the\nmodel is started in one basin of attraction, the model\nwill converge to the corresponding attractor as t grows.\nDynamical systems theory tells us that as \u03b8 changes\nslowly, the asymptotic behaviour changes smoothly\nalmost everywhere except for certain crucial points\nwhere drastic changes occur (the new asymptotic be-\nhaviour ceases to be topologically equivalent to the old\none). These points are called bifurcation boundaries\nand are caused by attractors that appear, disappear\nor change shape.\n(Doya, 1993) hypothesizes that such bifurcation cross-\nings could cause the gradients to explode. We would\nlike to extend this observation into a su\ufb03cient condi-\ntion for gradients to explode, and for that reason we\nwill re-use the one-hidden unit model (and plot) from\n(Doya, 1993) (see Fig. 3).\nThe x-axis covers the parameter b and the y-axis the\nasymptotic state x\u221e. The bold line follows the move-\nment of the \ufb01nal point attractor, x\u221e, as b changes. At\nb1 we have a bifurcation boundary where a new attrac-\ntor emerges (when b decreases from \u221e), while at b2 we\nhave another that results in the disappearance of one\nof the two attractors. In the interval (b1, b2) we are in\na rich regime, where there are two attractors and the\nchange in position of boundary between them, as we\nchange b, is traced out by a dashed line. The vector\n\ufb01eld (gray dashed arrows) describe the evolution of the\nstate x if the network is initialized in that region.\nFigure 3. Bifurcation diagram of a single hidden unit RNN\n(with \ufb01xed recurrent weight of 5.0 and adjustable bias b;\nexample introduced in Doya (1993)). See text.\nWe show that there are two types of events that could\nlead to a large change in xt, with t \u2192\u221e. One is cross-\ning a boundary between basins of attraction (depicted\nwith a un\ufb01lled circles), while the other is crossing a bi-\nfurcation boundary (\ufb01lled circles). For large t, the \u2206xt\nresulting from a change in b will be large even for very\nsmall changes in b (as the system is attracted towards\ndi\ufb00erent attractors) which leads to a large gradient.\nIt is however neither necessary nor su\ufb03cient to cross\na bifurcation for the gradients to explode, as bifurca-\ntions are global events that could have no e\ufb00ect lo-\ncally. Learning traces out a path in the parameter-\nstate space. If we are at a bifurcation boundary, but\nthe state of the model is such that it is in the basin of\nattraction of one attractor (from many possible attrac-\ntors) that does not change shape or disappear when\nthe bifurcation is crossed, then this bifurcation will\nnot a\ufb00ect learning.\nOn the di\ufb03culty of training Recurrent Neural Networks\nCrossing boundaries between basins of attraction is a\nlocal event, and it is su\ufb03cient for the gradients to ex-\nplode. If we assume that crossing into an emerging\nattractor or from a disappearing one (due to a bifur-\ncation) quali\ufb01es as crossing some boundary between\nattractors, that we can formulate a su\ufb03cient condition\nfor gradients to explode which encapsulates the obser-\nvations made in Doya (1993), extending them to also\nnormal crossing of boundaries between di\ufb00erent basins\nof attractions. Note how in the \ufb01gure, there are only\ntwo values of b with a bifurcation, but a whole range\nof values for which there can be a boundary crossing.\nAnother limitation of previous analysis is that they\nonly consider autonomous systems and assume the\nobservations hold for input-driven models. In (Ben-\ngio et al., 1994) input is dealt with by assuming it\nis bounded noise. The downside of this approach is\nthat it limits how one can reason about the input. In\npractice, the input is supposed to drive the dynamical\nsystem, being able to leave the model in some attrac-\ntor state, or kick it out of the basin of attraction when\ncertain triggering patterns present themselves.\nWe propose to extend our analysis to input driven\nmodels by folding the input into the map. We consider\nthe family of maps Ft, where we apply a di\ufb00erent Ft at\neach step. Intuitively, for the gradients to explode we\nrequire the same behaviour as before, where (at least\nin some direction) the maps F1, .., Ft agree and change\ndirection. Fig. 4 describes this behaviour.\nF1\nF2\nF3\nF1\nF2\nF3\n\u2206x0\n\u2206xt\nxt\nxt\nFigure 4. This diagram illustrates how the change in xt,\n\u2206xt, can be large for a small \u2206x0.\nThe blue vs red\n(left vs right) trajectories are generated by the same maps\nF1, F2, . . . for two di\ufb00erent initial states.\nFor the speci\ufb01c parametrization provided by equa-\ntion (11) we can take the analogy one step further\nby decomposing the maps Ft into a \ufb01xed map\n\u02dcF\nand a time-varying one Ut.\nF(x) = Wrec\u03c3(x) + b\ncorresponds to an input-less recurrent network, while\nUt(x) = x + Winut describes the e\ufb00ect of the input.\nThis is depicted in in Fig. 5. Since Ut changes with\ntime, it can not be analyzed using standard dynami-\ncal systems tools, but \u02dcF can. This means that when a\nboundary between basins of attractions is crossed for\n\u02dcF, the state will move towards a di\ufb00erent attractor,\nwhich for large t could lead (unless the input maps Ut\nare opposing this) to a large discrepancy in xt. There-\nfore studying the asymptotic behaviour of \u02dcF can pro-\nvide useful information about where such events are\nlikely to happen.\nF1\nF2\n\u02dcF\n\u02dcF\nU1\nU2\n\u02dcF\n\u02dcF\nF1\nF2\nU1\nU2\n\u2206x0\n\u2206xt\nxt\nxt\nFigure 5.\nIllustrates how one can break apart the maps\nF1, ..Ft into a constant map \u02dcF and the maps U1, .., Ut. The\ndotted vertical line represents the boundary between basins\nof attraction, and the straight dashed arrow the direction\nof the map \u02dcF on each side of the boundary. This diagram\nis an extension of Fig. 4.\nOne interesting observation from the dynamical sys-\ntems perspective with respect to vanishing gradients\nis the following. If the factors \u2202xt\n\u2202xk go to zero (for t \u2212k\nlarge), it means that xt does not depend on xk (if\nwe change xk by some \u2206, xt stays the same). This\ntranslates into the model at xt being close to conver-\ngence towards some attractor (which it would reach\nfrom anywhere in the neighbourhood of xk).\n2.3. The geometrical interpretation\nLet us consider a simple one hidden unit model (equa-\ntion (8)) where we provide an initial state x0 and train\nthe model to have a speci\ufb01c target value after 50 steps.\nNote that for simplicity we assume no input.\nxt = w\u03c3(xt\u22121) + b\n(8)\nFig. 6 shows the error surface E50 = (\u03c3(x50) \u22120.7)2,\nwhere x0 = .5 and \u03c3 to be the sigmoid function.\nWe can more easily analyze the behavior of this model\nby further simplifying it to be linear (\u03c3 then being the\nidentity function), with b = 0. xt = x0wt from which it\nfollows that \u2202xt\n\u2202w = tx0wt\u22121 and \u22022xt\n\u2202w2 = t(t\u22121)x0wt\u22122,\nimplying that when the \ufb01rst derivative explodes, so\ndoes the second derivative.\nIn the general case, when the gradients explode they do\nso along some directions v. This says that there exists,\nin such situations, a vector v such that \u2202Et\n\u2202\u03b8 v \u2265C\u03b1t,\nwhere C, \u03b1 \u2208R and \u03b1 > 1. For the linear case (\u03c3 is the\nidentity function), v is the eigenvector corresponding\nto the largest eigenvalue of Wrec.\nIf this bound is\ntight, we hypothesize that in general when gradients\nOn the di\ufb03culty of training Recurrent Neural Networks\nFigure 6. We plot the error surface of a single hidden unit\nrecurrent network, highlighting the existence of high cur-\nvature walls. The solid lines depicts standard trajectories\nthat gradient descent might follow.\nUsing dashed arrow\nthe diagram shows what would happen if the gradients is\nrescaled to a \ufb01xed size when its norm is above a threshold.\nexplode so does the curvature along v, leading to a\nwall in the error surface, like the one seen in Fig. 6.\nIf this holds, then it gives us a simple solution to the\nexploding gradients problem depicted in Fig. 6.\nIf both the gradient and the leading eigenvector of the\ncurvature are aligned with the exploding direction v, it\nfollows that the error surface has a steep wall perpen-\ndicular to v (and consequently to the gradient). This\nmeans that when stochastic gradient descent (SGD)\nreaches the wall and does a gradient descent step, it\nwill be forced to jump across the valley moving perpen-\ndicular to the steep walls, possibly leaving the valley\nand disrupting the learning process.\nThe dashed arrows in Fig. 6 correspond to ignoring\nthe norm of this large step, ensuring that the model\nstays close to the wall. The key insight is that all the\nsteps taken when the gradient explodes are aligned\nwith v and ignore other descent direction (i.e.\nthe\nmodel moves perpendicular to the wall). At the wall, a\nsmall-norm step in the direction of the gradient there-\nfore merely pushes us back inside the smoother low-\ncurvature region besides the wall, whereas a regular\ngradient step would bring us very far, thus slowing or\npreventing further training. Instead, with a bounded\nstep, we get back in that smooth region near the wall\nwhere SGD is free to explore other descent directions.\nThe important addition in this scenario to the classical\nhigh curvature valley, is that we assume that the val-\nley is wide, as we have a large region around the wall\nwhere if we land we can rely on \ufb01rst order methods\nto move towards the local minima. This is why just\nclipping the gradient might be su\ufb03cient, not requiring\nthe use a second order method. Note that this algo-\nrithm should work even when the rate of growth of the\ngradient is not the same as the one of the curvature\n(a case for which a second order method would fail\nas the ratio between the gradient and curvature could\nstill explode).\nOur hypothesis could also help to understand the re-\ncent success of the Hessian-Free approach compared\nto other second order methods. There are two key dif-\nferences between Hessian-Free and most other second-\norder algorithms. First, it uses the full Hessian matrix\nand hence can deal with exploding directions that are\nnot necessarily axis-aligned.\nSecond, it computes a\nnew estimate of the Hessian matrix before each up-\ndate step and can take into account abrupt changes in\ncurvature (such as the ones suggested by our hypothe-\nsis) while most other approaches use a smoothness as-\nsumption, i.e., averaging 2nd order signals over many\nsteps.\n3. Dealing with the exploding and\nvanishing gradient\n3.1. Previous solutions\nUsing an L1 or L2 penalty on the recurrent weights can\nhelp with exploding gradients. Given that the parame-\nters initialized with small values, the spectral radius of\nWrec is probably smaller than 1, from which it follows\nthat the gradient can not explode (see necessary condi-\ntion found in section 2.1). The regularization term can\nensure that during training the spectral radius never\nexceeds 1. This approach limits the model to a sim-\nple regime (with a single point attractor at the origin),\nwhere any information inserted in the model has to die\nout exponentially fast in time. In such a regime we can\nnot train a generator network, nor can we exhibit long\nterm memory traces.\nDoya (1993) proposes to pre-program the model (to\ninitialize the model in the right regime) or to use\nteacher forcing.\nThe \ufb01rst proposal assumes that if\nthe model exhibits from the beginning the same kind\nof asymptotic behaviour as the one required by the\ntarget, then there is no need to cross a bifurcation\nboundary. The downside is that one can not always\nknow the required asymptotic behaviour, and, even if\nsuch information is known, it is not trivial to initial-\nize a model in this speci\ufb01c regime.\nWe should also\nnote that such initialization does not prevent cross-\ning the boundary between basins of attraction, which,\nas shown, could happen even though no bifurcation\nboundary is crossed.\nTeacher forcing is a more interesting, yet a not very\nwell understood solution. It can be seen as a way of\ninitializing the model in the right regime and the right\nOn the di\ufb03culty of training Recurrent Neural Networks\nregion of space. It has been shown that in practice\nit can reduce the chance that gradients explode, and\neven allow training generator models or models that\nwork with unbounded amounts of memory(Pascanu\nand Jaeger, 2011; Doya and Yoshizawa, 1991). One\nimportant downside is that it requires a target to be\nde\ufb01ned at every time step.\nIn Hochreiter and Schmidhuber (1997); Graves et al.\n(2009) a solution is proposed for the vanishing gra-\ndients problem, where the structure of the model is\nchanged.\nSpeci\ufb01cally it introduces a special set of\nunits called LSTM units which are linear and have a\nrecurrent connection to itself which is \ufb01xed to 1. The\n\ufb02ow of information into the unit and from the unit is\nguarded by an input and output gates (their behaviour\nis learned). There are several variations of this basic\nstructure. This solution does not address explicitly the\nexploding gradients problem.\nSutskever et al. (2011) use the Hessian-Free opti-\nmizer in conjunction with structural damping, a spe-\nci\ufb01c damping strategy of the Hessian. This approach\nseems to deal very well with the vanishing gradient,\nthough more detailed analysis is still missing.\nPre-\nsumably this method works because in high dimen-\nsional spaces there is a high probability for long term\ncomponents to be orthogonal to short term ones. This\nwould allow the Hessian to rescale these components\nindependently. In practice, one can not guarantee that\nthis property holds. As discussed in section 2.3, this\nmethod is able to deal with the exploding gradient\nas well. Structural damping is an enhancement that\nforces the change in the state to be small, when the pa-\nrameter changes by some small value \u2206\u03b8. This asks for\nthe Jacobian matrices \u2202xt\n\u2202\u03b8 to have small norm, hence\nfurther helping with the exploding gradients problem.\nThe fact that it helps when training recurrent neural\nmodels on long sequences suggests that while the cur-\nvature might explode at the same time with the gradi-\nent, it might not grow at the same rate and hence not\nbe su\ufb03cient to deal with the exploding gradient.\nEcho State Networks (Luko\u02c7sevi\u02c7cius and Jaeger, 2009)\navoid the exploding and vanishing gradients problem\nby not learning the recurrent and input weights. They\nare sampled from hand crafted distributions. Because\nusually the largest eigenvalue of the recurrent weight\nis, by construction, smaller than 1, information fed in\nto the model has to die out exponentially fast. This\nmeans that these models can not easily deal with long\nterm dependencies, even though the reason is slightly\ndi\ufb00erent from the vanishing gradients problem.\nAn\nextension to the classical model is represented by leaky\nintegration units (Jaeger et al., 2007), where\nxk = \u03b1xk\u22121 + (1 \u2212\u03b1)\u03c3(Wrecxk\u22121 + Winuk + b).\nWhile these units can be used to solve the standard\nbenchmark proposed by Hochreiter and Schmidhu-\nber (1997) for learning long term dependencies (see\n(Jaeger, 2012)), they are more suitable to deal with\nlow frequency information as they act as a low pass\n\ufb01lter. Because most of the weights are randomly sam-\npled, is not clear what size of models one would need\nto solve complex real world tasks.\nWe would make a \ufb01nal note about the approach pro-\nposed by Tomas Mikolov in his PhD thesis (Mikolov,\n2012)(and implicitly used in the state of the art re-\nsults on language modelling (Mikolov et al., 2011)).\nIt involves clipping the gradient\u2019s temporal compo-\nnents element-wise (clipping an entry when it exceeds\nin absolute value a \ufb01xed threshold). Clipping has been\nshown to do well in practice and it forms the backbone\nof our approach.\n3.2. Scaling down the gradients\nAs suggested in section 2.3, one simple mechanism to\ndeal with a sudden increase in the norm of the gradi-\nents is to rescale them whenever they go over a thresh-\nold (see algorithm 1).\nAlgorithm 1 Pseudo-code for norm clipping the gra-\ndients whenever they explode\n\u02c6g \u2190\u2202E\n\u2202\u03b8\nif \u2225\u02c6g\u2225\u2265threshold then\n\u02c6g \u2190threshold\n\u2225\u02c6g\u2225\n\u02c6g\nend if\nThis algorithm is very similar to the one proposed by\nTomas Mikolov and we only diverged from the original\nproposal in an attempt to provide a better theoretical\nfoundation (ensuring that we always move in a de-\nscent direction with respect to the current mini-batch),\nthough in practice both variants behave similarly.\nThe proposed clipping is simple to implement and\ncomputationally e\ufb03cient, but it does however in-\ntroduce an additional hyper-parameter, namely the\nthreshold. One good heuristic for setting this thresh-\nold is to look at statistics on the average norm over\na su\ufb03ciently large number of updates.\nIn our ex-\nperiments we have noticed that for a given task and\nmodel size, training is not very sensitive to this hyper-\nparameter and the algorithm behaves well even for\nrather small thresholds.\nThe algorithm can also be thought of as adapting\nthe learning rate based on the norm of the gradient.\nCompared to other learning rate adaptation strate-\ngies, which focus on improving convergence by col-\nlecting statistics on the gradient (as for example in\nOn the di\ufb03culty of training Recurrent Neural Networks\nDuchi et al. (2011), or Moreira and Fiesler (1995) for\nan overview), we rely on the instantaneous gradient.\nThis means that we can handle very abrupt changes\nin norm, while the other methods would not be able\nto do so.\n3.3. Vanishing gradient regularization\nWe opt to address the vanishing gradients problem us-\ning a regularization term that represents a preference\nfor parameter values such that back-propagated gra-\ndients neither increase or decrease too much in mag-\nnitude. Our intuition is that increasing the norm of\n\u2202xt\n\u2202xk means the error at time t is more sensitive to all\ninputs ut, .., uk ( \u2202xt\n\u2202xk is a factor in\n\u2202Et\n\u2202uk ). In practice\nsome of these inputs will be irrelevant for the predic-\ntion at time t and will behave like noise that the net-\nwork needs to learn to ignore. The network can not\nlearn to ignore these irrelevant inputs unless there is\nan error signal. These two issues can not be solved in\nparallel, and it seems natural to expect that we need\nto force the network to increase the norm of \u2202xt\n\u2202xk at the\nexpense of larger errors (caused by the irrelevant input\nentries) and then wait for it to learn to ignore these\nirrelevant input entries. This suggest that moving to-\nwards increasing the norm of\n\u2202xt\n\u2202xk can not be always\ndone while following a descent direction of the error E\n(which is, for e.g., what a second order method would\ntry to do), and therefore we need to enforce it via a\nregularization term.\nThe regularizer we propose below prefers solutions for\nwhich the error signal preserves norm as it travels back\nin time:\n\u2126=\nX\nk\n\u2126k =\nX\nk\n\uf8eb\n\uf8ed\n\r\r\r\n\u2202E\n\u2202xk+1\n\u2202xk+1\n\u2202xk\n\r\r\r\n\r\r\r\n\u2202E\n\u2202xk+1\n\r\r\r\n\u22121\n\uf8f6\n\uf8f8\n2\n(9)\nIn order to be computationally e\ufb03cient, we only use\nthe \u201cimmediate\u201d partial derivative of \u2126with respect to\nWrec (we consider that xk and\n\u2202E\n\u2202xk+1 as being constant\nwith respect to Wrec when computing the derivative\nof \u2126k), as depicted in equation (10). Note we use the\nparametrization of equation (11). This can be done ef-\n\ufb01ciently because we get the values of\n\u2202E\n\u2202xk from BPTT.\nWe use Theano to compute these gradients (Bergstra\net al., 2010; Bastien et al., 2012).\n\u2202+\u2126\n\u2202Wrec\n=\nP\nk\n\u2202+\u2126k\n\u2202Wrec\n=\nP\nk\n\u2202+\n\uf8eb\n\uf8ed\n\r\r\r\r\n\u2202E\n\u2202xk+1\nWT\nrecdiag(\u03c3\u2032(xk))\n\r\r\r\r\n\r\r\r\r\n\u2202E\n\u2202xk+1\n\r\r\r\r\n\u22121\n\uf8f6\n\uf8f8\n2\n\u2202Wrec\n(10)\nNote that our regularization term only forces the Ja-\ncobian matrices \u2202xk+1\n\u2202xk\nto preserve norm in the relevant\ndirection of the error\n\u2202E\n\u2202xk+1 , not for any direction (i.e.\nwe do not enforce that all eigenvalues are close to 1).\nThe second observation is that we are using a soft con-\nstraint, therefore we are not ensured the norm of the\nerror signal is preserved. If it happens that these Jaco-\nbian matrices are such that the norm explodes (as t\u2212k\nincreases), then this could lead to the exploding gradi-\nents problem and we need to deal with it for example\nas described in section 3.2.\nThis can be seen from\nthe dynamical systems perspective as well: preventing\nvanishing gradients implies that we are pushing the\nmodel such that it is further away from the attrac-\ntor (such that it does not converge to it, case in which\nthe gradients vanish) and closer to boundaries between\nbasins of attractions, making it more probable for the\ngradients to explode.\n4. Experiments and Results\n4.1. Pathological synthetic problems\nAs done in Martens and Sutskever (2011), we address\nthe pathological problems proposed by Hochreiter and\nSchmidhuber (1997) that require learning long term\ncorrelations. We refer the reader to this original pa-\nper for a detailed description of the tasks and to the\nsupplementary materials for the complete description\nof the experimental setup.\n4.1.1. The Temporal Order problem\nWe consider the temporal order problem as the pro-\ntotypical pathological problem, extending our results\nto the other proposed tasks afterwards. The input is\na long stream of discrete symbols. At two points in\ntime (in the beginning and middle of the sequence) a\nsymbol within {A, B} is emitted. The task consists in\nclassifying the order (either AA, AB, BA, BB) at the\nend of the sequence.\nFig. 7 shows the success rate of standard SGD, SGD-C\n(SGD enhanced with out clipping strategy) and SGD-\nCR (SGD with the clipping strategy and the regular-\nization term). Note that for sequences longer than 20,\nthe vanishing gradients problem ensures that neither\nSGD nor SGD-C algorithms can solve the task. The\nx-axis is on log scale.\nThis task provides empirical evidence that exploding\ngradients are linked with tasks that require long mem-\nory traces.\nWe know that initially the model oper-\nates in the one-attractor regime (i.e.\n\u03bb1\n<\n1), in\nwhich the amount of memory is controlled by \u03bb1. More\nmemory means larger spectral radius, and, when this\nvalue crosses a certain threshold the model enters rich\nregimes where gradients are likely to explode. We see\nin Fig. 7 that as long as the vanishing gradient prob-\nOn the di\ufb03culty of training Recurrent Neural Networks\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\nLog of sequence length\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRate of success\nSGD-C\nSGD\nSGD-CR\nFigure 7. Rate of success for solving the temporal order\nproblem versus log of sequence length. See text.\nlem does not become an issue, addressing the explod-\ning gradients problem ensures a better success rate.\nWhen combining clipping as well as the regularization\nterm proposed in section 3.3, we call this algorithm\nSGD-CR. SGD-CR solved the task with a success rate\nof 100% for sequences up to 200 steps (the maximal\nlength used in Martens and Sutskever (2011)). Fur-\nthermore, we can train a single model to deal with\nany sequence of length 50 up to 200 (by providing se-\nquences of di\ufb00erent lengths for di\ufb00erent SGD steps).\nInterestingly enough, the trained model can gen-\neralize to new sequences that can be twice as\nlong as the ones seen during training.\n4.1.2. Other pathological tasks\nSGD-CR was also able to solve (100% success on the\nlengths listed below, for all but one task) other patho-\nlogical problems proposed in Hochreiter and Schmid-\nhuber (1997), namely the addition problem, the mul-\ntiplication problem, the 3-bit temporal order prob-\nlem, the random permutation problem and the noise-\nless memorization problem in two variants (when the\npattern needed to be memorized is 5 bits in length\nand when it contains over 20 bits of information; see\nMartens and Sutskever (2011)). For the \ufb01rst 4 prob-\nlems we used a single model for lengths up to 200,\nwhile for the noiseless memorization we used a dif-\nferent model for each sequence length (50, 100, 150\nand 200). The hardest problems for which only one\ntrail out of 8 succeeded was the random permutation\nproblem. In all cases, we observe successful generaliza-\ntion to sequences longer than the training sequences.\nIn most cases, these results outperforms Martens and\nSutskever (2011) in terms of success rate, they deal\nwith longer sequences than in Hochreiter and Schmid-\nhuber (1997) and compared to (Jaeger, 2012) they gen-\neralize to longer sequences.\nTable 1. Results on polyphonic music prediction in nega-\ntive log likelihood per time step. Lower is better.\nData set\nData\nfold\nSGD\nSGD+C\nSGD+CR\nPiano-\ntrain\n6.87\n6.81\n7.01\nmidi.de\ntest\n7.56\n7.53\n7.46\nNottingham\ntrain\n3.67\n3.21\n3.24\ntest\n3.80\n3.48\n3.46\nMuseData\ntrain\n8.25\n6.54\n6.51\ntest\n7.11\n7.00\n6.99\nTable 2. Results on the next character prediction task in\nentropy (bits/character)\nData set\nData\nfold\nSGD\nSGD+C\nSGD+CR\n1 step\ntrain\n1.46\n1.34\n1.36\ntest\n1.50\n1.42\n1.41\n5 steps\ntrain\nN/A\n3.76\n3.70\ntest\nN/A\n3.89\n3.74\n4.2. Natural problems\nWe address the task of polyphonic music prediction,\nusing the datasets Piano-midi.de, Nottingham and\nMuseData described in Boulanger-Lewandowski et al.\n(2012) and language modelling at the character level\non the Penn Treebank dataset (Mikolov et al., 2012).\nWe also explore a modi\ufb01ed version of the task, where\nwe ask the model to predict the 5th character in the\nfuture (instead of the next). Our assumption is that\nto solve this modi\ufb01ed task long term correlations are\nmore important than short term ones, and hence our\nregularization term should be more helpful.\nThe training and test scores reported in Table 1 are\naverage negative log likelihood per time step. We \ufb01xed\nhyper-parameters across the three runs, except for\nthe regularization factor and clipping cuto\ufb00threshold.\nSGD-CR provides a statistically signi\ufb01cant im-\nprovement on the state-of-the-art for RNNs on\nall the polyphonic music prediction tasks except\nfor MuseData on which we get exactly the same per-\nformance as the state-of-the-art (Bengio et al., 2012),\nwhich uses a di\ufb00erent architecture. Table 2 contains\nthe results on language modelling (in bits per letter).\nThese results suggest that clipping the gradients solves\nan optimization issue and does not act as a regular-\nizer, as both the training and test error improve in\ngeneral. Results on Penn Treebank reach the state of\nthe art achieved by Mikolov et al. (2012), who used a\ndi\ufb00erent clipping algorithm similar to ours, thus pro-\nviding evidence that both behave similarly. The reg-\nularized model performs as well as the Hessian-Free\ntrained model.\nBy employing the proposed regularization term we are\nable to improve test error even on tasks that are not\nOn the di\ufb03culty of training Recurrent Neural Networks\ndominated by long term contributions.\n5. Summary and Conclusions\nWe provided di\ufb00erent perspectives through which one\ncan gain more insight into the exploding and vanishing\ngradients issue. To deal with the exploding gradients\nproblem, we propose a solution that involves clipping\nthe norm of the exploded gradients when it is too large.\nThe algorithm is motivated by the assumption that\nwhen gradients explode, the curvature and higher or-\nder derivatives explode as well, and we are faced with\na speci\ufb01c pattern in the error surface, namely a val-\nley with a single steep wall.\nIn order to deal with\nthe vanishing gradient problem we use a regulariza-\ntion term that forces the error signal not to vanish as\nit travels back in time. This regularization term forces\nthe Jacobian matrices\n\u2202xi\n\u2202xi\u22121 to preserve norm only in\nrelevant directions. In practice we show that these so-\nlutions improve performance on both the pathological\nsynthetic datasets considered as well as on polyphonic\nmusic prediction and language modelling.\nAcknowledgements\nWe would like to thank the Theano development team\nas well (particularly to Frederic Bastien, Pascal Lam-\nblin and James Bergstra) for their help.\nWe acknowledge NSERC, FQRNT, CIFAR, RQCHP\nand Compute Canada for the resources they provided.\nReferences\nBastien, F., Lamblin, P., Pascanu, R., Bergstra, J.,\nGoodfellow, I., Bergeron, A., Bouchard, N., and\nBengio, Y. (2012). Theano: new features and speed\nimprovements. Submited to Deep Learning and Un-\nsupervised Feature Learning NIPS 2012 Workshop.\nBengio, Y., Frasconi, P., and Simard, P. (1993). The\nproblem of learning long-term dependencies in re-\ncurrent networks. pages 1183\u20131195, San Francisco.\nIEEE Press. (invited paper).\nBengio, Y., Simard, P., and Frasconi, P. (1994). Learn-\ning long-term dependencies with gradient descent is\ndi\ufb03cult. IEEE Transactions on Neural Networks,\n5(2), 157\u2013166.\nBengio, Y., Boulanger-Lewandowski, N., and Pascanu,\nR. (2012).\nAdvances in optimizing recurrent net-\nworks. Technical Report arXiv:1212.0901, U. Mon-\ntreal.\nBergstra, J., Breuleux, O., Bastien, F., Lamblin, P.,\nPascanu, R., Desjardins, G., Turian, J., Warde-\nFarley, D., and Bengio, Y. (2010). Theano: a CPU\nand GPU math expression compiler. In Proceedings\nof the Python for Scienti\ufb01c Computing Conference\n(SciPy). Oral Presentation.\nBoulanger-Lewandowski, N., Bengio, Y., and Vincent,\nP. (2012). Modeling temporal dependencies in high-\ndimensional sequences: Application to polyphonic\nmusic generation and transcription.\nIn Proceed-\nings of the Twenty-nine International Conference on\nMachine Learning (ICML\u201912). ACM.\nDoya, K. (1993). Bifurcations of recurrent neural net-\nworks in gradient descent learning. IEEE Transac-\ntions on Neural Networks, 1, 75\u201380.\nDoya, K. and Yoshizawa, S. (1991). Adaptive synchro-\nnization of neural and physical oscillators. In J. E.\nMoody, S. J. Hanson, and R. Lippmann, editors,\nNIPS, pages 109\u2013116. Morgan Kaufmann.\nDuchi, J. C., Hazan, E., and Singer, Y. (2011). Adap-\ntive subgradient methods for online learning and\nstochastic optimization. Journal of Machine Learn-\ning Research, 12, 2121\u20132159.\nElman, J. (1990). Finding structure in time. Cognitive\nScience, 14(2), 179\u2013211.\nGraves, A., Liwicki, M., Fernandez, S., Bertolami, R.,\nBunke, H., and Schmidhuber, J. (2009). A Novel\nConnectionist System for Unconstrained Handwrit-\ning Recognition.\nIEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 31(5), 855\u2013868.\nHochreiter, S. and Schmidhuber, J. (1997).\nLong\nshort-term memory.\nNeural Computation, 9(8),\n1735\u20131780.\nJaeger, H. (2012). Long short-term memory in echo\nstate networks: Details of a simulation study. Tech-\nnical report, Jacobs University Bremen.\nJaeger, H., Lukosevicius, M., Popovici, D., and Siew-\nert, U. (2007).\nOptimization and applications of\necho state networks with leaky- integrator neurons.\nNeural Networks, 20(3), 335\u2013352.\nLuko\u02c7sevi\u02c7cius, M. and Jaeger, H. (2009).\nReservoir\ncomputing approaches to recurrent neural network\ntraining. Computer Science Review, 3(3), 127\u2013149.\nMartens, J. and Sutskever, I. (2011). Learning recur-\nrent neural networks with Hessian-free optimization.\nIn Proc. ICML\u20192011. ACM.\nMikolov, T. (2012). Statistical Language Models based\non Neural Networks. Ph.D. thesis, Brno University\nof Technology.\nOn the di\ufb03culty of training Recurrent Neural Networks\nMikolov, T., Deoras, A., Kombrink, S., Burget, L.,\nand Cernocky, J. (2011). Empirical evaluation and\ncombination of advanced language modeling tech-\nniques. In Proc. 12th annual conference of the in-\nternational speech communication association (IN-\nTERSPEECH 2011).\nMikolov, T., Sutskever, I., Deoras, A., Le, H.-S.,\nKombrink, S., and Cernocky, J. (2012).\nSubword\nlanguage modeling with neural networks. preprint\n(http://www.\ufb01t.vutbr.cz/ imikolov/rnnlm/char.pdf).\nMoreira, M. and Fiesler, E. (1995).\nNeural net-\nworks with adaptive learning rate and momentum\nterms.\nIdiap-RR Idiap-RR-04-1995, IDIAP, Mar-\ntigny, Switzerland.\nPascanu, R. and Jaeger, H. (2011). A neurodynamical\nmodel for working memory. Neural Netw., 24, 199\u2013\n207.\nRumelhart, D. E., Hinton, G. E., and Williams,\nR. J. (1986).\nLearning representations by back-\npropagating errors. Nature, 323(6088), 533\u2013536.\nStrogatz, S. (1994). Nonlinear Dynamics And Chaos:\nWith Applications To Physics, Biology, Chemistry,\nAnd Engineering (Studies in Nonlinearity). Studies\nin nonlinearity. Perseus Books Group, 1 edition.\nSutskever, I., Martens, J., and Hinton, G. (2011).\nGenerating text with recurrent neural networks. In\nL. Getoor and T. Sche\ufb00er, editors, Proceedings of the\n28th International Conference on Machine Learning\n(ICML-11), ICML \u201911, pages 1017\u20131024, New York,\nNY, USA. ACM.\nWerbos, P. J. (1988).\nGeneralization of backpropa-\ngation with application to a recurrent gas market\nmodel. Neural Networks, 1(4), 339\u2013356.\nAnalytical analysis of the exploding\nand vanishing gradients problem\nxt = Wrec\u03c3(xt\u22121) + Winut + b\n(11)\nLet us consider the term gT\nk =\n\u2202Et\n\u2202xt\n\u2202xt\n\u2202xk\n\u2202+xk\n\u2202\u03b8\nfor the\nlinear version of the parametrization in equation (11)\n(i.e. set \u03c3 to the identity function) and assume t goes\nto in\ufb01nity and l = t \u2212k. We have that:\n\u2202xt\n\u2202xk\n=\n\u0000WT\nrec\n\u0001l\n(12)\nBy employing a generic power iteration method based\nproof we can show that, given certain conditions,\n\u2202Et\n\u2202xt\n\u0000WT\nrec\n\u0001l grows exponentially.\nProof Let Wrec have the eigenvalues \u03bb1, .., \u03bbn with\n|\u03bb1| > |\u03bb2| > .. > |\u03bbn| and the corresponding eigen-\nvectors q1, q2, .., qn which form a vector basis. We can\nnow write the row vector \u2202Et\n\u2202xt into this basis:\n\u2202Et\n\u2202xt = PN\ni=1 ciqT\ni\nIf j is such that cj \u0338= 0 and any j\u2032 < j, cj\u2032 = 0, using\nthe fact that qT\ni\n\u0000WT\nrec\n\u0001l = \u03bbl\niqT\ni we have that\n\u2202Et\n\u2202xt\n\u2202xt\n\u2202xk\n= cj\u03bbl\njqT\nj + \u03bbl\nj\nn\nX\ni=j+1\nci\n\u03bbl\ni\n\u03bbl\nj\nqT\ni \u2248cj\u03bbl\njqT\nj\n(13)\nWe used the fact that\n\f\f\u03bbi/\u03bbj\n\f\f < 1 for i > j, which\nmeans that liml\u2192\u221e\n\f\f\u03bbi/\u03bbj\n\f\fl = 0. If |\u03bbj| > 1, it follows\nthat \u2202xt\n\u2202xk grows exponentially fast with l, and it does\nso along the direction qj.\n\u25a1\nThe proof assumes Wrec is diagonalizable for simplic-\nity, though using the Jordan normal form of Wrec one\ncan extend this proof by considering not just the eigen-\nvector of largest eigenvalue but the whole subspace\nspanned by the eigenvectors sharing the same (largest)\neigenvalue.\nThis result provides a necessary condition for gradients\nto grow, namely that the spectral radius (the absolute\nvalue of the largest eigenvalue) of Wrec must be larger\nthan 1.\nIf qj is not in the null space of \u2202+xk\n\u2202\u03b8\nthe entire temporal\ncomponent grows exponentially with l. This approach\nextends easily to the entire gradient. If we re-write it\nin terms of the eigen-decomposition of W, we get:\n\u2202Et\n\u2202\u03b8 =\nn\nX\nj=1\n \nt\nX\ni=k\ncj\u03bbt\u2212k\nj\nqT\nj\n\u2202+xk\n\u2202\u03b8\n!\n(14)\nOn the di\ufb03culty of training Recurrent Neural Networks\nWe can now pick j and k such that cjqT\nj\n\u2202+xk\n\u2202\u03b8\ndoes not\nhave 0 norm, while maximizing |\u03bbj|. If for the chosen\nj it holds that |\u03bbj| > 1 then \u03bbt\u2212k\nj\ncjqT\nj\n\u2202+xk\n\u2202\u03b8\nwill dom-\ninate the sum and because this term grows exponen-\ntially fast to in\ufb01nity with t, the same will happen to\nthe sum.\nExperimental setup\nNote that all hyper-parameters where selected based\non their performance on a validation set using a grid\nsearch.\nThe pathological synthetic tasks\nSimilar success criteria is used in all of the tasks be-\nlow (borrowed from Martens and Sutskever (2011)),\nnamely that the model should make no more than 1%\nerror on a batch of 10000 test samples. In all cases,\ndiscrete symbols are depicted by a one-hot encoding,\nand in case of regression a prediction for a given se-\nquence is considered as a success if the error is less\nthan 0.04.\nAddition problem\nThe input consists of a sequence of random numbers,\nwhere two random positions (one in the beginning\nand one in the middle of the sequence) are marked.\nThe model needs to predict the sum of the two ran-\ndom numbers after the entire sequence was seen. For\neach generated sequence we sample the length T \u2032 from\n[T, 11\n10T], though for clarity we refer to T as the length\nof the sequence in the paper. The \ufb01rst position is sam-\npled from [1, T \u2032\n10], while the second position is sampled\nfrom [ T \u2032\n10, T \u2032\n2 ]. These positions i, j are marked in a dif-\nferent input channel that is 0 everywhere except for the\ntwo sampled positions when it is 1. The model needs\nto predict the sum of the random numbers found at\nthe sampled positions i, j divided by 2.\nTo address this problem we use a 50 hidden units\nmodel, with a tanh activation function.\nThe learn-\ning rate is set to .01 and the factor \u03b1 in front of the\nregularization term is 0.5. We use clipping with a cut-\no\ufb00threshold of 6 on the norm of the gradients. The\nweights are initialized from a normal distribution with\nmean 0 and standard derivation .1.\nThe model is trained on sequences of varying length\nT between 50 and 200. We manage to get a success\nrate of 100% at solving this task, which outperforms\nthe results presented in Martens and Sutskever (2011)\n(using Hessian Free), where we see a decline in success\nrate as the length of the sequence gets closer to 200\nsteps. Hochreiter and Schmidhuber (1997) only con-\nsiders sequences up to 100 steps. Jaeger (2012) also\naddresses this task with 100% success rate, though the\nsolution does not seem to generalize well as it relies on\nvery large output weights, which for ESNs are usually\na sign of instability. We use a single model to deal\nwith all lengths of sequences (50, 100, 150 200), and\nthe trained model generalizes to new sequences that\ncan be up 400 steps (while the error is still under 1%).\nMultiplication problem\nThis task is similar to the problem above, just that\nthe predicted value is the product of the random num-\nbers instead of the sum.\nWe used the same hyper-\nparameters as for the previous case, and obtained very\nsimilar results.\nTemporal order problem\nFor the temporal order the length of the sequence is\n\ufb01xed to T, We have a \ufb01xed set of two symbols {A, B}\nand 4 distractor symbols {c, d, e, f}. The sequence en-\ntries are uniformly sampled from the distractor sym-\nbols everywhere except at two random positions, the\n\ufb01rst position sampled from [ T\n10, 2T\n10 ], while the second\nfrom [ 4T\n10 , 5T\n10 ]. The task is to predict the order in which\nthe non-distractor symbols were provided, i.e. either\n{AA, AB, BA, BB}.\nWe use a 50 hidden units model, with a learning rate of\n.001 and \u03b1, the regularization coe\ufb03cient, set to 2. The\ncut-o\ufb00threshold for clipping the norm of the gradient\nis left to 6. As for the other two task we have a 100%\nsuccess rate at training a single model to deal with\nsequences between 50 to 200 steps. This outperforms\nthe previous state of the art because of the success\nrate, but also the single model generalizes to longer\nsequences (up to 400 steps).\n3-bit temporal order problem\nSimilar to the previous one, except that we have 3\nrandom positions, \ufb01rst sampled from [ T\n10, 2T\n10 ], second\nfrom [ 3T\n10 , 4T\n10 ] and last from [ 6T\n10 , 7T\n10 ].\nWe use similar hyper-parameters as above, but that\nwe increase the hidden layer size to 100 hidden units.\nAs before we outperform the state of the art while\ntraining a single model that is able to generalize to\nnew sequence lengths.\nRandom permutation problem\nIn this case we have a dictionary of 100 symbols. Ex-\ncept the \ufb01rst and last position which have the same\nvalue sampled from {1, 2} the other entries are ran-\nOn the di\ufb03culty of training Recurrent Neural Networks\ndomly picked from [3, 100].\nThe task is to do next\nsymbol prediction, though the only predictable sym-\nbol is the last one.\nWe use a 100 hidden units with a learning rate of .001\nand \u03b1, the regularization coe\ufb03cient, set to 1.\nThe\ncuto\ufb00threshold is left to 6. This task turns out to be\nmore di\ufb03cult to learn, and only 1 out of 8 experiments\nsucceeded. As before we use a single model to deal\nwith multiple values for T (from 50 to 200 units).\nNoiseless memorization problem\nFor the noiseless memorization we are presented with\na binary pattern of length 5, followed by T steps of\nconstant value. After these T steps the model needs\nto generate the pattern seen initially.\nWe also con-\nsider the extension of this problem from Martens and\nSutskever (2011), where the pattern has length 10, and\nthe symbol set has cardinality 5 instead of 2.\nWe manage a 100% success rate on these tasks, though\nwe train a di\ufb00erent model for the 5 sequence lengths\nconsidered (50, 100, 150, 200).\nNatural Tasks\nPolyphonic music prediction\nWe train our model, a sigmoid units RNN, on se-\nquences of 200 steps. The cut-o\ufb00coe\ufb03cient thresh-\nold is the same in all cases, namely 8 (note that one\nhas to take the mean over the sequence length when\ncomputing the gradients).\nIn case of the Piano-midi.de dataset we use 300 hid-\nden units and an initial learning rate of 1.0 (whir the\nlearning rate halved every time the error over an epoch\nincreased instead of decreasing). For the regularized\nmodel we used a initial value for regularization coef-\n\ufb01cient \u03b1 of 0.5, where \u03b1 follows a 1/t schedule, i.e.\n\u03b1t = 1\n2t (where t measures the number of epochs).\nFor the Nottingham dataset we used the exact same\nsetup. For MuseData we increased the hidden layer to\n400 hidden units. The learning rate was also decreased\nto 0.5. For the regularized model, the initial value for\n\u03b1 was 0.1, and \u03b1t = 1\nt .\nWe make the observation that for natural tasks it\nseems useful to use a schedule that decreases the reg-\nularization term. We assume that the regularization\nterm forces the model to focus on long term correla-\ntions at the expense of short term ones, so it may be\nuseful to have this decaying factor in order to allow\nthe model to make better use of the short term infor-\nmation.\nLanguage modelling\nFor the language modelling task we used a 500 sig-\nmoidal hidden units model with no biases (Mikolov\net al., 2012). The model is trained over sequences of\n200 steps, where the hidden state is carried over from\none step to the next one.\nWe use a cut-o\ufb00threshold of 45 (though we take the\nsum of the cost over the sequence length) for all ex-\nperiments.\nFor next character prediction we have a\nlearning rate of 0.01 when using clipping with no reg-\nularization term, 0.05 when we add the regularization\nterm and 0.001 when we do not use clipping. When\npredicting the 5th character in the future we use a\nlearning rate of 0.05 with the regularization term and\n0.1 without it.\nThe regularization factor \u03b1 for next character predic-\ntion was set to .01 and kept constant, while for the\nmodi\ufb01ed task we used an initial value of 0.05 with a 1\nt\nschedule.\n",
        "sentence": " In [49], Pascanu et al.",
        "context": "tigny, Switzerland.\nPascanu, R. and Jaeger, H. (2011). A neurodynamical\nmodel for working memory. Neural Netw., 24, 199\u2013\n207.\nRumelhart, D. E., Hinton, G. E., and Williams,\nR. J. (1986).\nLearning representations by back-\nIEEE Press. (invited paper).\nBengio, Y., Simard, P., and Frasconi, P. (1994). Learn-\ning long-term dependencies with gradient descent is\ndi\ufb03cult. IEEE Transactions on Neural Networks,\n5(2), 157\u2013166.\nBengio, Y., Boulanger-Lewandowski, N., and Pascanu,\nOn the di\ufb03culty of training Recurrent Neural Networks\nRazvan Pascanu\npascanur@iro.umontreal.ca\nUniversite de Montreal\nTomas Mikolov\nt.mikolov@gmail.com\nBrno University\nYoshua Bengio\nyoshua.bengio@umontreal.ca\nUniversite de Montreal\nAbstract"
    },
    {
        "title": "Gradient calculations for dynamic recurrent neural networks: A survey",
        "author": [
            "Barak A Pearlmutter"
        ],
        "venue": "Neural Networks, IEEE Transactions on,",
        "citeRegEx": "50",
        "shortCiteRegEx": "50",
        "year": 1995,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Other resources focus on a specific technical aspect such as [50], which surveys gradient calculations in recurrent neural networks.",
        "context": null
    },
    {
        "title": "Glove: Global vectors for word representation",
        "author": [
            "Jeffrey Pennington",
            "Richard Socher",
            "Christopher D Manning"
        ],
        "venue": "Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014),",
        "citeRegEx": "51",
        "shortCiteRegEx": "51",
        "year": 2014,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Freely available code to produce word vectors from co-occurrence stats include Glove from Pennington Socher and Manning [51], and word2vec [23], which implements a word embedding algorithm from Mikolov et al.",
        "context": null
    },
    {
        "title": "Learning internal representations by error propagation",
        "author": [
            "David E Rumelhart",
            "Geoffrey E Hinton",
            "Ronald J Williams"
        ],
        "venue": "Technical report, DTIC Document,",
        "citeRegEx": "52",
        "shortCiteRegEx": "52",
        "year": 1985,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Backpropagation, an algorithm introduced to neural networks in [52], in 1985 [52].",
        "context": null
    },
    {
        "title": "Bidirectional recurrent neural networks",
        "author": [
            "Mike Schuster",
            "Kuldip K Paliwal"
        ],
        "venue": "Signal Processing, IEEE Transactions on,",
        "citeRegEx": "53",
        "shortCiteRegEx": "53",
        "year": 1997,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " In other papers ([53], [57], [35]), biological inspiration is downplayed in favor of achieving empirical results on important tasks and datasets. 1 Bidirectional recurrent neural networks (BRNNs) [53] extend RNNs to model dependence on both past states and future states. Along with the LSTM, one of the most used RNN setups is the bidirectional recurrent neural network (BRNN) (Figure 11) first described in [53]. Figure 11: Structure of a bidirectional recurrent neural network as described by Schuster and Paliwal in [53].",
        "context": null
    },
    {
        "title": "Turing computability with neural nets",
        "author": [
            "Hava T Siegelmann",
            "Eduardo D Sontag"
        ],
        "venue": "Applied Mathematics Letters,",
        "citeRegEx": "54",
        "shortCiteRegEx": "54",
        "year": 1991,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " A wellknown result by Siegelman and Sontag from 1991 demonstrated that a finite sized recurrent neural network with sigmoidal activation functions can simulate a universal Turing machine [54].",
        "context": null
    },
    {
        "title": "Efficient learning using forward-backward splitting",
        "author": [
            "Yoram Singer",
            "John C Duchi"
        ],
        "venue": "In Advances in Neural Information Processing Systems,",
        "citeRegEx": "55",
        "shortCiteRegEx": "55",
        "year": 2009,
        "abstract": "",
        "full_text": "",
        "sentence": " A growing body of recent work ([12], [37], [55], [40]), thus read shows that given sparse inputs to a linear model with any standard regularizer, sparsity can be fully exploited even if the gradient is not sparse (owing to regularization).",
        "context": null
    },
    {
        "title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection",
        "author": [
            "Richard Socher",
            "Eric H Huang",
            "Jeffrey Pennin",
            "Christopher D Manning",
            "Andrew Y Ng"
        ],
        "venue": "In Advances in Neural Information Processing Systems,",
        "citeRegEx": "56",
        "shortCiteRegEx": "56",
        "year": 2011,
        "abstract": "",
        "full_text": "",
        "sentence": " in 2003 [8], and more recently brought to wider attention in the deep learning community by Socher, Manning and Ng in a number of papers describing Recursive Autoencoder (RAE) networks ([59], [56], [60], [58]).",
        "context": null
    },
    {
        "title": "Grounded compositional semantics for finding and describing images with sentences",
        "author": [
            "Richard Socher",
            "Andrej Karpathy",
            "Quoc V Le",
            "Christopher D Manning",
            "Andrew Y Ng"
        ],
        "venue": "Transactions of the Association for Computational Linguistics,",
        "citeRegEx": "57",
        "shortCiteRegEx": "57",
        "year": 2014,
        "abstract": "",
        "full_text": "",
        "sentence": " In other papers ([53], [57], [35]), biological inspiration is downplayed in favor of achieving empirical results on important tasks and datasets.",
        "context": null
    },
    {
        "title": "Parsing natural scenes and natural language with recursive neural networks",
        "author": [
            "Richard Socher",
            "Cliff C Lin",
            "Chris Manning",
            "Andrew Y Ng"
        ],
        "venue": "In Proceedings of the 28th international conference on machine learning",
        "citeRegEx": "58",
        "shortCiteRegEx": "58",
        "year": 2011,
        "abstract": "",
        "full_text": "",
        "sentence": " in 2003 [8], and more recently brought to wider attention in the deep learning community by Socher, Manning and Ng in a number of papers describing Recursive Autoencoder (RAE) networks ([59], [56], [60], [58]).",
        "context": null
    },
    {
        "title": "Learning continuous phrase representations and syntactic parsing with recursive neural networks",
        "author": [
            "Richard Socher",
            "Christopher D Manning",
            "Andrew Y Ng"
        ],
        "venue": "In Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop,",
        "citeRegEx": "59",
        "shortCiteRegEx": "59",
        "year": 2010,
        "abstract": "",
        "full_text": "",
        "sentence": " in 2003 [8], and more recently brought to wider attention in the deep learning community by Socher, Manning and Ng in a number of papers describing Recursive Autoencoder (RAE) networks ([59], [56], [60], [58]).",
        "context": null
    },
    {
        "title": "Semi-supervised recursive autoencoders for predicting sentiment distributions",
        "author": [
            "Richard Socher",
            "Jeffrey Pennington",
            "Eric H Huang",
            "Andrew Y Ng",
            "Christopher D Manning"
        ],
        "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,",
        "citeRegEx": "60",
        "shortCiteRegEx": "60",
        "year": 2011,
        "abstract": "",
        "full_text": "",
        "sentence": " in 2003 [8], and more recently brought to wider attention in the deep learning community by Socher, Manning and Ng in a number of papers describing Recursive Autoencoder (RAE) networks ([59], [56], [60], [58]).",
        "context": null
    },
    {
        "title": "Unsupervised learning of video representations using LSTMs",
        "author": [
            "Nitish Srivastava",
            "Elman Mansimov",
            "Ruslan Salakhutdinov"
        ],
        "venue": "arXiv preprint arXiv:1502.04681,",
        "citeRegEx": "61",
        "shortCiteRegEx": "61",
        "year": 2015,
        "abstract": "We use multilayer Long Short Term Memory (LSTM) networks to learn\nrepresentations of video sequences. Our model uses an encoder LSTM to map an\ninput sequence into a fixed length representation. This representation is\ndecoded using single or multiple decoder LSTMs to perform different tasks, such\nas reconstructing the input sequence, or predicting the future sequence. We\nexperiment with two kinds of input sequences - patches of image pixels and\nhigh-level representations (\"percepts\") of video frames extracted using a\npretrained convolutional net. We explore different design choices such as\nwhether the decoder LSTMs should condition on the generated output. We analyze\nthe outputs of the model qualitatively to see how well the model can\nextrapolate the learned video representation into the future and into the past.\nWe try to visualize and interpret the learned features. We stress test the\nmodel by running it on longer time scales and on out-of-domain data. We further\nevaluate the representations by finetuning them for a supervised learning\nproblem - human action recognition on the UCF-101 and HMDB-51 datasets. We show\nthat the representations help improve classification accuracy, especially when\nthere are only a few training examples. Even models pretrained on unrelated\ndatasets (300 hours of YouTube videos) can help action recognition performance.",
        "full_text": "Unsupervised Learning of Video Representations using LSTMs\nNitish Srivastava\nNITISH@CS.TORONTO.EDU\nElman Mansimov\nEMANSIM@CS.TORONTO.EDU\nRuslan Salakhutdinov\nRSALAKHU@CS.TORONTO.EDU\nUniversity of Toronto, 6 Kings College Road, Toronto, ON M5S 3G4 CANADA\nAbstract\nWe use multilayer Long Short Term Memory\n(LSTM) networks to learn representations of\nvideo sequences.\nOur model uses an encoder\nLSTM to map an input sequence into a \ufb01xed\nlength representation. This representation is de-\ncoded using single or multiple decoder LSTMs\nto perform different tasks, such as reconstruct-\ning the input sequence, or predicting the future\nsequence.\nWe experiment with two kinds of\ninput sequences \u2013 patches of image pixels and\nhigh-level representations (\u201cpercepts\u201d) of video\nframes extracted using a pretrained convolutional\nnet. We explore different design choices such\nas whether the decoder LSTMs should condi-\ntion on the generated output.\nWe analyze the\noutputs of the model qualitatively to see how\nwell the model can extrapolate the learned video\nrepresentation into the future and into the past.\nWe try to visualize and interpret the learned fea-\ntures. We stress test the model by running it on\nlonger time scales and on out-of-domain data.\nWe further evaluate the representations by \ufb01ne-\ntuning them for a supervised learning problem \u2013\nhuman action recognition on the UCF-101 and\nHMDB-51 datasets. We show that the represen-\ntations help improve classi\ufb01cation accuracy, es-\npecially when there are only a few training ex-\namples.\nEven models pretrained on unrelated\ndatasets (300 hours of YouTube videos) can help\naction recognition performance.\n1. Introduction\nUnderstanding temporal sequences is important for solv-\ning many problems in the AI-set. Recently, recurrent neu-\nral networks using the Long Short Term Memory (LSTM)\narchitecture (Hochreiter & Schmidhuber, 1997) have been\nused successfully to perform various supervised sequence\nlearning tasks, such as speech recognition (Graves & Jaitly,\n2014), machine translation (Sutskever et al., 2014; Cho\net al., 2014), and caption generation for images (Vinyals\net al., 2014). They have also been applied on videos for\nrecognizing actions and generating natural language de-\nscriptions (Donahue et al., 2014). A general sequence to\nsequence learning framework was described by Sutskever\net al. (2014) in which a recurrent network is used to encode\na sequence into a \ufb01xed length representation, and then an-\nother recurrent network is used to decode a sequence out of\nthat representation. In this work, we apply and extend this\nframework to learn representations of sequences of images.\nWe choose to work in the unsupervised setting where we\nonly have access to a dataset of unlabelled videos.\nVideos are an abundant and rich source of visual infor-\nmation and can be seen as a window into the physics of\nthe world we live in, showing us examples of what con-\nstitutes objects, how objects move against backgrounds,\nwhat happens when cameras move and how things get oc-\ncluded. Being able to learn a representation that disen-\ntangles these factors would help in making intelligent ma-\nchines that can understand and act in their environment.\nAdditionally, learning good video representations is essen-\ntial for a number of useful tasks, such as recognizing ac-\ntions and gestures.\n1.1. Why Unsupervised Learning?\nSupervised learning has been extremely successful in learn-\ning good visual representations that not only produce good\nresults at the task they are trained for, but also transfer well\nto other tasks and datasets. Therefore, it is natural to ex-\ntend the same approach to learning video representations.\nThis has led to research in 3D convolutional nets (Ji et al.,\n2013; Tran et al., 2014), different temporal fusion strategies\n(Karpathy et al., 2014) and exploring different ways of pre-\nsenting visual information to convolutional nets (Simonyan\n& Zisserman, 2014a). However, videos are much higher di-\nmensional entities compared to single images. Therefore, it\nbecomes increasingly dif\ufb01cult to do credit assignment and\nlearn long range structure, unless we collect much more\nlabelled data or do a lot of feature engineering (for exam-\nple computing the right kinds of \ufb02ow features) to keep the\narXiv:1502.04681v3  [cs.LG]  4 Jan 2016\nUnsupervised Learning with LSTMs\ndimensionality low. The costly work of collecting more\nlabelled data and the tedious work of doing more clever en-\ngineering can go a long way in solving particular problems,\nbut this is ultimately unsatisfying as a machine learning\nsolution. This highlights the need for using unsupervised\nlearning to \ufb01nd and represent structure in videos. More-\nover, videos have a lot of structure in them (spatial and\ntemporal regularities) which makes them particularly well\nsuited as a domain for building unsupervised learning mod-\nels.\n1.2. Our Approach\nWhen designing any unsupervised learning model, it is cru-\ncial to have the right inductive biases and choose the right\nobjective function so that the learning signal points the\nmodel towards learning useful features. In this paper, we\nuse the LSTM Encoder-Decoder framework to learn video\nrepresentations.\nThe key inductive bias here is that the\nsame operation must be applied at each time step to prop-\nagate information to the next step. This enforces the fact\nthat the physics of the world remains the same, irrespec-\ntive of input. The same physics acting on any state, at any\ntime, must produce the next state. Our model works as\nfollows. The Encoder LSTM runs through a sequence of\nframes to come up with a representation. This representa-\ntion is then decoded through another LSTM to produce a\ntarget sequence. We consider different choices of the tar-\nget sequence. One choice is to predict the same sequence\nas the input. The motivation is similar to that of autoen-\ncoders \u2013 we wish to capture all that is needed to reproduce\nthe input but at the same time go through the inductive bi-\nases imposed by the model. Another option is to predict the\nfuture frames. Here the motivation is to learn a representa-\ntion that extracts all that is needed to extrapolate the motion\nand appearance beyond what has been observed. These two\nnatural choices can also be combined. In this case, there are\ntwo decoder LSTMs \u2013 one that decodes the representation\ninto the input sequence and another that decodes the same\nrepresentation to predict the future.\nThe inputs to the model can, in principle, be any represen-\ntation of individual video frames. However, for the pur-\nposes of this work, we limit our attention to two kinds of\ninputs. The \ufb01rst is image patches. For this we use natural\nimage patches as well as a dataset of moving MNIST digits.\nThe second is high-level \u201cpercepts\u201d extracted by applying a\nconvolutional net trained on ImageNet. These percepts are\nthe states of last (and/or second-to-last) layers of recti\ufb01ed\nlinear hidden states from a convolutional neural net model.\nIn order to evaluate the learned representations we quali-\ntatively analyze the reconstructions and predictions made\nby the model. For a more quantitative evaluation, we use\nthese LSTMs as initializations for the supervised task of ac-\ntion recognition. If the unsupervised learning model comes\nup with useful representations then the classi\ufb01er should be\nable to perform better, especially when there are only a few\nlabelled examples. We \ufb01nd that this is indeed the case.\n1.3. Related Work\nThe \ufb01rst approaches to learning representations of videos\nin an unsupervised way were based on ICA (van Hateren\n& Ruderman, 1998; Hurri & Hyv\u00a8arinen, 2003). Le et al.\n(2011) approached this problem using multiple layers of\nIndependent Subspace Analysis modules. Generative mod-\nels for understanding transformations between pairs of con-\nsecutive images are also well studied (Memisevic, 2013;\nMemisevic & Hinton, 2010; Susskind et al., 2011). This\nwork was extended recently by Michalski et al. (2014) to\nmodel longer sequences.\nRecently, Ranzato et al. (2014) proposed a generative\nmodel for videos.\nThe model uses a recurrent neural\nnetwork to predict the next frame or interpolate between\nframes. In this work, the authors highlight the importance\nof choosing the right loss function. It is argued that squared\nloss in input space is not the right objective because it does\nnot respond well to small distortions in input space. The\nproposed solution is to quantize image patches into a large\ndictionary and train the model to predict the identity of\nthe target patch. This does solve some of the problems of\nsquared loss but it introduces an arbitrary dictionary size\ninto the picture and altogether removes the idea of patches\nbeing similar or dissimilar to one other. Designing an ap-\npropriate loss function that respects our notion of visual\nsimilarity is a very hard problem (in a sense, almost as hard\nas the modeling problem we want to solve in the \ufb01rst place).\nTherefore, in this paper, we use the simple squared loss ob-\njective function as a starting point and focus on designing\nan encoder-decoder RNN architecture that can be used with\nany loss function.\n2. Model Description\nIn this section, we describe several variants of our LSTM\nEncoder-Decoder model. The basic unit of our network\nis the LSTM cell block. Our implementation of LSTMs\nfollows closely the one discussed by Graves (2013).\n2.1. Long Short Term Memory\nIn this section we brie\ufb02y describe the LSTM unit which is\nthe basic building block of our model. The unit is shown in\nFig. 1 (reproduced from Graves (2013)).\nEach LSTM unit has a cell which has a state ct at time t.\nThis cell can be thought of as a memory unit. Access to\nthis memory unit for reading or modifying it is controlled\nthrough sigmoidal gates \u2013 input gate it, forget gate ft and\nUnsupervised Learning with LSTMs\nFigure 2: Long Short-term Memory Cell\n(LSTM) architecture [16], which uses purpose-built memory cells to store infor-\nmation, is better at \ufb01nding and exploiting long range dependencies in the data.\nFig. 2 illustrates a single LSTM memory cell. For the version of LSTM used in\nthis paper [7] H is implemented by the following composite function:\nit = \u03c3 (Wxixt + Whiht\u22121 + Wcict\u22121 + bi)\n(7)\nft = \u03c3 (Wxfxt + Whfht\u22121 + Wcfct\u22121 + bf)\n(8)\nct = ftct\u22121 + it tanh (Wxcxt + Whcht\u22121 + bc)\n(9)\not = \u03c3 (Wxoxt + Whoht\u22121 + Wcoct + bo)\n(10)\nht = ot tanh(ct)\n(11)\nwhere \u03c3 is the logistic sigmoid function, and i, f, o and c are respectively the\ninput gate, forget gate, output gate, cell and cell input activation vectors, all of\nwhich are the same size as the hidden vector h. The weight matrix subscripts\nhave the obvious meaning, for example Whi is the hidden-input gate matrix,\nWxo is the input-output gate matrix etc. The weight matrices from the cell\nto gate vectors (e.g. Wci) are diagonal, so element m in each gate vector only\nreceives input from element m of the cell vector. The bias terms (which are\nadded to i, f, c and o) have been omitted for clarity.\nThe original LSTM algorithm used a custom designed approximate gradi-\nent calculation that allowed the weights to be updated after every timestep [16].\nHowever the full gradient can instead be calculated with backpropagation through\ntime [11], the method used in this paper. One di\ufb03culty when training LSTM\nwith the full gradient is that the derivatives sometimes become excessively large,\n5\nFigure 1. LSTM unit\noutput gate ot. The LSTM unit operates as follows. At\neach time step it receives inputs from two external sources\nat each of the four terminals (the three gates and the input).\nThe \ufb01rst source is the current frame xt. The second source\nis the previous hidden states of all LSTM units in the same\nlayer ht\u22121. Additionally, each gate has an internal source,\nthe cell state ct\u22121 of its cell block. The links between a\ncell and its own gates are called peephole connections. The\ninputs coming from different sources get added up, along\nwith a bias. The gates are activated by passing their to-\ntal input through the logistic function. The total input at\nthe input terminal is passed through the tanh non-linearity.\nThe resulting activation is multiplied by the activation of\nthe input gate. This is then added to the cell state after mul-\ntiplying the cell state by the forget gate\u2019s activation ft. The\n\ufb01nal output from the LSTM unit ht is computed by multi-\nplying the output gate\u2019s activation ot with the updated cell\nstate passed through a tanh non-linearity. These updates\nare summarized for a layer of LSTM units as follows\nit\n=\n\u03c3 (Wxixt + Whiht\u22121 + Wcict\u22121 + bi) ,\nft\n=\n\u03c3 (Wxfxt + Whfht\u22121 + Wcfct\u22121 + bf) ,\nct\n=\nftct\u22121 + it tanh (Wxcxt + Whcht\u22121 + bc) ,\not\n=\n\u03c3 (Wxoxt + Whoht\u22121 + Wcoct + bo) ,\nht\n=\not tanh(ct).\nNote that all Wc\u2022 matrices are diagonal, whereas the rest\nare dense. The key advantage of using an LSTM unit over\na traditional neuron in an RNN is that the cell state in an\nLSTM unit sums activities over time. Since derivatives dis-\ntribute over sums, the error derivatives don\u2019t vanish quickly\nas they get sent back into time. This makes it easy to do\ncredit assignment over long sequences and discover long-\nrange features.\n2.2. LSTM Autoencoder Model\nIn this section, we describe a model that uses Recurrent\nNeural Nets (RNNs) made of LSTM units to do unsuper-\nv1\nv2\nv3\nv3\nv2\nv3\nv2\n\u02c6v3\n\u02c6v2\n\u02c6v1\nLearned\nRepresentation\nW1\nW1\ncopy\nW2\nW2\nFigure 2. LSTM Autoencoder Model\nvised learning. The model consists of two RNNs \u2013 the en-\ncoder LSTM and the decoder LSTM as shown in Fig. 2.\nThe input to the model is a sequence of vectors (image\npatches or features). The encoder LSTM reads in this se-\nquence. After the last input has been read, the decoder\nLSTM takes over and outputs a prediction for the target se-\nquence. The target sequence is same as the input sequence,\nbut in reverse order. Reversing the target sequence makes\nthe optimization easier because the model can get off the\nground by looking at low range correlations. This is also\ninspired by how lists are represented in LISP. The encoder\ncan be seen as creating a list by applying the cons func-\ntion on the previously constructed list and the new input.\nThe decoder essentially unrolls this list, with the hidden to\noutput weights extracting the element at the top of the list\n(car function) and the hidden to hidden weights extract-\ning the rest of the list (cdr function). Therefore, the \ufb01rst\nelement out is the last element in.\nThe decoder can be of two kinds \u2013 conditional or uncondi-\ntioned. A conditional decoder receives the last generated\noutput frame as input, i.e., the dotted input in Fig. 2 is\npresent. An unconditioned decoder does not receive that\ninput. This is discussed in more detail in Sec. 2.4. Fig. 2\nshows a single layer LSTM Autoencoder. The architecture\ncan be extend to multiple layers by stacking LSTMs on top\nof each other.\nWhy should this learn good features?\nThe state of the encoder LSTM after the last input has been\nread is the representation of the input video. The decoder\nLSTM is being asked to reconstruct back the input se-\nquence from this representation. In order to do so, the rep-\nresentation must retain information about the appearance\nof the objects and the background as well as the motion\ncontained in the video. However, an important question for\nany autoencoder-style model is what prevents it from learn-\ning an identity mapping and effectively copying the input\nto the output. In that case all the information about the in-\nput would still be present but the representation will be no\nbetter than the input. There are two factors that control this\nbehaviour. First, the fact that there are only a \ufb01xed num-\nber of hidden units makes it unlikely that the model can\nUnsupervised Learning with LSTMs\nv1\nv2\nv3\nv4\nv5\n\u02c6v4\n\u02c6v5\n\u02c6v6\nLearned\nRepresentation\nW1\nW1\ncopy\nW2\nW2\nFigure 3. LSTM Future Predictor Model\nlearn trivial mappings for arbitrary length input sequences.\nSecond, the same LSTM operation is used to decode the\nrepresentation recursively. This means that the same dy-\nnamics must be applied on the representation at any stage\nof decoding. This further prevents the model from learning\nan identity mapping.\n2.3. LSTM Future Predictor Model\nAnother natural unsupervised learning task for sequences\nis predicting the future. This is the approach used in lan-\nguage models for modeling sequences of words. The de-\nsign of the Future Predictor Model is same as that of the\nAutoencoder Model, except that the decoder LSTM in this\ncase predicts frames of the video that come after the in-\nput sequence (Fig. 3). Ranzato et al. (2014) use a similar\nmodel but predict only the next frame at each time step.\nThis model, on the other hand, predicts a long sequence\ninto the future. Here again we can consider two variants of\nthe decoder \u2013 conditional and unconditioned.\nWhy should this learn good features?\nIn order to predict the next few frames correctly, the model\nneeds information about which objects and background are\npresent and how they are moving so that the motion can\nbe extrapolated. The hidden state coming out from the en-\ncoder will try to capture this information. Therefore, this\nstate can be seen as a representation of the input sequence.\n2.4. Conditional Decoder\nFor each of these two models, we can consider two possi-\nbilities - one in which the decoder LSTM is conditioned on\nthe last generated frame and the other in which it is not. In\nthe experimental section, we explore these choices quanti-\ntatively. Here we brie\ufb02y discuss arguments for and against\na conditional decoder. A strong argument in favour of using\na conditional decoder is that it allows the decoder to model\nmultiple modes in the target sequence distribution. With-\nout that, we would end up averaging the multiple modes in\nthe low-level input space. However, this is an issue only if\nwe expect multiple modes in the target sequence distribu-\ntion. For the LSTM Autoencoder, there is only one correct\nv1\nv2\nv3\nv3\nv2\nv4\nv5\n\u02c6v3\n\u02c6v2\n\u02c6v1\n\u02c6v4\n\u02c6v5\n\u02c6v6\nSequence of Input Frames\nFuture Prediction\nInput Reconstruction\nLearned\nRepresentation\nW1\nW1\ncopy\ncopy\nW2\nW2\nW3\nW3\nFigure 4. The Composite Model: The LSTM predicts the future\nas well as the input sequence.\ntarget and hence a unimodal target distribution. But for the\nLSTM Future Predictor there is a possibility of multiple\ntargets given an input because even if we assume a deter-\nministic universe, everything needed to predict the future\nwill not necessarily be observed in the input.\nThere is also an argument against using a conditional\ndecoder from the optimization point-of-view.\nThere are\nstrong short-range correlations in video data, for example,\nmost of the content of a frame is same as the previous one.\nIf the decoder was given access to the last few frames while\ngenerating a particular frame at training time, it would \ufb01nd\nit easy to pick up on these correlations. There would only\nbe a very small gradient that tries to \ufb01x up the extremely\nsubtle errors that require long term knowledge about the\ninput sequence. In an unconditioned decoder, this input is\nremoved and the model is forced to look for information\ndeep inside the encoder.\n2.5. A Composite Model\nThe two tasks \u2013 reconstructing the input and predicting the\nfuture can be combined to create a composite model as\nshown in Fig. 4. Here the encoder LSTM is asked to come\nup with a state from which we can both predict the next few\nframes as well as reconstruct the input.\nThis composite model tries to overcome the shortcomings\nthat each model suffers on its own. A high-capacity au-\ntoencoder would suffer from the tendency to learn trivial\nrepresentations that just memorize the inputs. However,\nthis memorization is not useful at all for predicting the fu-\nture. Therefore, the composite model cannot just memo-\nUnsupervised Learning with LSTMs\nrize information. On the other hand, the future predictor\nsuffers form the tendency to store information only about\nthe last few frames since those are most important for pre-\ndicting the future, i.e., in order to predict vt, the frames\n{vt\u22121, . . . , vt\u2212k} are much more important than v0, for\nsome small value of k. Therefore the representation at the\nend of the encoder will have forgotten about a large part of\nthe input. But if we ask the model to also predict all of the\ninput sequence, then it cannot just pay attention to the last\nfew frames.\n3. Experiments\nWe design experiments to accomplish the following objec-\ntives:\n\u2022 Get a qualitative understanding of what the LSTM\nlearns to do.\n\u2022 Measure the bene\ufb01t of initializing networks for super-\nvised learning tasks with the weights found by unsu-\npervised learning, especially with very few training\nexamples.\n\u2022 Compare the different proposed models - Autoen-\ncoder, Future Predictor and Composite models and\ntheir conditional variants.\n\u2022 Compare with state-of-the-art action recognition\nbenchmarks.\n3.1. Datasets\nWe use the UCF-101 and HMDB-51 datasets for super-\nvised tasks. The UCF-101 dataset (Soomro et al., 2012)\ncontains 13,320 videos with an average length of 6.2 sec-\nonds belonging to 101 different action categories.\nThe\ndataset has 3 standard train/test splits with the training set\ncontaining around 9,500 videos in each split (the rest are\ntest). The HMDB-51 dataset (Kuehne et al., 2011) contains\n5100 videos belonging to 51 different action categories.\nMean length of the videos is 3.2 seconds. This also has\n3 train/test splits with 3570 videos in the training set and\nrest in test.\nTo train the unsupervised models, we used a subset of the\nSports-1M dataset (Karpathy et al., 2014), that contains\n1 million YouTube clips. Even though this dataset is la-\nbelled for actions, we did not do any supervised experi-\nments on it because of logistical constraints with working\nwith such a huge dataset. We instead collected 300 hours\nof video by randomly sampling 10 second clips from the\ndataset. It is possible to collect better samples if instead of\nchoosing randomly, we extracted videos where a lot of mo-\ntion is happening and where there are no shot boundaries.\nHowever, we did not do so in the spirit of unsupervised\nlearning, and because we did not want to introduce any un-\nnatural bias in the samples. We also used the supervised\ndatasets (UCF-101 and HMDB-51) for unsupervised train-\ning. However, we found that using them did not give any\nsigni\ufb01cant advantage over just using the YouTube videos.\nWe extracted percepts using the convolutional neural net\nmodel of Simonyan & Zisserman (2014b).\nThe videos\nhave a resolution of 240 \u00d7 320 and were sampled at al-\nmost 30 frames per second. We took the central 224 \u00d7 224\npatch from each frame and ran it through the convnet. This\ngave us the RGB percepts. Additionally, for UCF-101, we\ncomputed \ufb02ow percepts by extracting \ufb02ows using the Brox\nmethod and training the temporal stream convolutional net-\nwork as described by Simonyan & Zisserman (2014a). We\nfound that the fc6 features worked better than fc7 for sin-\ngle frame classi\ufb01cation using both RGB and \ufb02ow percepts.\nTherefore, we used the 4096-dimensional fc6 layer as the\ninput representation of our data. Besides these percepts,\nwe also trained the proposed models on 32 \u00d7 32 patches of\npixels.\nAll models were trained using backprop on a single\nNVIDIA Titan GPU. A two layer 2048 unit Composite\nmodel that predicts 13 frames and reconstructs 16 frames\ntook 18-20 hours to converge on 300 hours of percepts. We\ninitialized weights by sampling from a uniform distribu-\ntion whose scale was set to 1/sqrt(fan-in). Biases at all\nthe gates were initialized to zero. Peep-hole connections\nwere initialized to zero. The supervised classi\ufb01ers trained\non 16 frames took 5-15 minutes to converge. The code\ncan be found at https://github.com/emansim/\nunsupervised-videos.\n3.2. Visualization and Qualitative Analysis\nThe aim of this set of experiments to visualize the proper-\nties of the proposed models.\nExperiments on MNIST\nWe \ufb01rst trained our models on a dataset of moving MNIST\ndigits. In this dataset, each video was 20 frames long and\nconsisted of two digits moving inside a 64 \u00d7 64 patch.\nThe digits were chosen randomly from the training set and\nplaced initially at random locations inside the patch. Each\ndigit was assigned a velocity whose direction was chosen\nuniformly randomly on a unit circle and whose magnitude\nwas also chosen uniformly at random over a \ufb01xed range.\nThe digits bounced-off the edges of the 64 \u00d7 64 frame and\noverlapped if they were at the same location. The reason\nfor working with this dataset is that it is in\ufb01nite in size and\ncan be generated quickly on the \ufb02y. This makes it possi-\nble to explore the model without expensive disk accesses\nor over\ufb01tting issues. It also has interesting behaviours due\nto occlusions and the dynamics of bouncing off the walls.\nUnsupervised Learning with LSTMs\nGround Truth Future\n-\n\u001b\nInput Sequence\n\u001b\n-\nFuture Prediction\n-\n\u001b\nInput Reconstruction\n\u001b\n-\nOne Layer Composite Model\nTwo Layer Composite Model\nTwo Layer Composite Model with a Conditional Future Predictor\nFigure 5. Reconstruction and future prediction obtained from the Composite Model on a dataset of moving MNIST digits.\nWe \ufb01rst trained a single layer Composite Model.\nEach\nLSTM had 2048 units. The encoder took 10 frames as in-\nput. The decoder tried to reconstruct these 10 frames and\nthe future predictor attempted to predict the next 10 frames.\nWe used logistic output units with a cross entropy loss func-\ntion. Fig. 5 shows two examples of running this model.\nThe true sequences are shown in the \ufb01rst two rows. The\nnext two rows show the reconstruction and future predic-\ntion from the one layer Composite Model. It is interesting\nto note that the model \ufb01gures out how to separate superim-\nposed digits and can model them even as they pass through\neach other. This shows some evidence of disentangling the\ntwo independent factors of variation in this sequence. The\nmodel can also correctly predict the motion after bounc-\ning off the walls. In order to see if adding depth helps,\nwe trained a two layer Composite Model, with each layer\nhaving 2048 units. We can see that adding depth helps the\nmodel make better predictions. Next, we changed the fu-\nture predictor by making it conditional. We can see that\nthis model makes sharper predictions.\nExperiments on Natural Image Patches\nNext, we tried to see if our models can also work with nat-\nural image patches. For this, we trained the models on se-\nquences of 32 \u00d7 32 natural image patches extracted from\nthe UCF-101 dataset. In this case, we used linear output\nunits and the squared error loss function. The input was\n16 frames and the model was asked to reconstruct the 16\nframes and predict the future 13 frames. Fig. 6 shows the\nresults obtained from a two layer Composite model with\n2048 units. We found that the reconstructions and the pre-\ndictions are both very blurry.\nWe then trained a bigger\nmodel with 4096 units. The outputs from this model are\nalso shown in Fig. 6. We can see that the reconstructions\nget much sharper.\nGeneralization over time scales\nIn the next experiment, we test if the model can work\nat time scales that are different than what it was trained\non. We take a one hidden layer unconditioned Compos-\nite Model trained on moving MNIST digits. The model\nhas 2048 LSTM units and looks at a 64 \u00d7 64 input. It\nwas trained on input sequences of 10 frames to reconstruct\nthose 10 frames as well as predict 10 frames into the fu-\nture. In order to test if the future predictor is able to gen-\neralize beyond 10 frames, we let the model run for 100\nsteps into the future. Fig. 7(a) shows the pattern of ac-\ntivity in the LSTM units of the future predictor pathway\nfor a randomly chosen test input.\nIt shows the activity\nat each of the three sigmoidal gates (input, forget, out-\nput), the input (after the tanh non-linearity, before being\nmultiplied by the input gate), the cell state and the \ufb01nal\noutput (after being multiplied by the output gate). Even\nthough the units are ordered randomly along the vertical\naxis, we can see that the dynamics has a periodic quality\nto it. The model is able to generate persistent motion for\nlong periods of time. In terms of reconstruction, the model\nonly outputs blobs after the \ufb01rst 15 frames, but the motion\nis relatively well preserved. More results, including long\nrange future predictions over hundreds of time steps can see\nUnsupervised Learning with LSTMs\nGround Truth Future\n-\n\u001b\nInput Sequence\n\u001b\n-\nFuture Prediction\n-\n\u001b\nInput Reconstruction\n\u001b\n-\nTwo Layer Composite Model with 2048 LSTM units\nTwo Layer Composite Model with 4096 LSTM units\nFigure 6. Reconstruction and future prediction obtained from the Composite Model on a dataset of natural image patches. The \ufb01rst two\nrows show ground truth sequences. The model takes 16 frames as inputs. Only the last 10 frames of the input sequence are shown here.\nThe next 13 frames are the ground truth future. In the rows that follow, we show the reconstructed and predicted frames for two instances\nof the model.\nbeen at http://www.cs.toronto.edu/\u02dcnitish/\nunsupervised_video. To show that setting up a pe-\nriodic behaviour is not trivial, Fig. 7(b) shows the activ-\nity from a randomly initialized future predictor. Here, the\nLSTM state quickly converges and the outputs blur com-\npletely.\nOut-of-domain Inputs\nNext, we test this model\u2019s ability to deal with out-of-\ndomain inputs. For this, we test the model on sequences\nof one and three moving digits. The model was trained on\nsequences of two moving digits, so it has never seen in-\nputs with just one digit or three digits. Fig. 8 shows the\nreconstruction and future prediction results. For one mov-\ning digit, we can see that the model can do a good job but\nit really tries to hallucinate a second digit overlapping with\nthe \ufb01rst one. The second digit shows up towards the end\nof the future reconstruction. For three digits, the model\nmerges digits into blobs. However, it does well at getting\nthe overall motion right. This highlights a key drawback of\nmodeling entire frames of input in a single pass. In order to\nmodel videos with variable number of objects, we perhaps\nneed models that not only have an attention mechanism in\nplace, but can also learn to execute themselves a variable\nnumber of times and do variable amounts of computation.\nVisualizing Features\nNext, we visualize the features learned by this model.\nFig. 9 shows the weights that connect each input frame to\nthe encoder LSTM. There are four sets of weights. One\nset of weights connects the frame to the input units. There\nare three other sets, one corresponding to each of the three\ngates (input, forget and output). Each weight has a size of\n64 \u00d7 64. A lot of features look like thin strips. Others\nlook like higher frequency strips. It is conceivable that the\nhigh frequency features help in encoding the direction and\nvelocity of motion.\nFig. 10 shows the output features from the two LSTM de-\ncoders of a Composite Model. These correspond to the\nweights connecting the LSTM output units to the output\nlayer. They appear to be somewhat qualitatively different\nfrom the input features shown in Fig. 9. There are many\nmore output features that are local blobs, whereas those are\nrare in the input features. In the output features, the ones\nthat do look like strips are much shorter than those in the\ninput features. One way to interpret this is the following.\nThe model needs to know about motion (which direction\nand how fast things are moving) from the input. This re-\nquires precise information about location (thin strips) and\nvelocity (high frequency strips). But when it is generating\nthe output, the model wants to hedge its bets so that it does\nnot suffer a huge loss for predicting things sharply at the\nwrong place. This could explain why the output features\nhave somewhat bigger blobs. The relative shortness of the\nstrips in the output features can be explained by the fact that\nin the inputs, it does not hurt to have a longer feature than\nwhat is needed to detect a location because information is\ncoarse-coded through multiple features. But in the output,\nthe model may not want to put down a feature that is bigger\nthan any digit because other units will have to conspire to\ncorrect for it.\n3.3. Action Recognition on UCF-101/HMDB-51\nThe aim of this set of experiments is to see if the features\nlearned by unsupervised learning can help improve perfor-\nUnsupervised Learning with LSTMs\n(a) Trained Future Predictor\n0\n20\n40\n60\n80\n0\n50\n100\n150\nInput Gates\n0\n20\n40\n60\n80\n0\n50\n100\n150\nForget Gates\n0\n20\n40\n60\n80\n0\n50\n100\n150\nInput\n0\n20\n40\n60\n80\n0\n50\n100\n150\nOutput Gates\n0\n20\n40\n60\n80\n0\n50\n100\n150\nCell States\n0\n20\n40\n60\n80\n0\n50\n100\n150\nOutput\n(b) Randomly Initialized Future Predictor\nFigure 7. Pattern of activity in 200 randomly chosen LSTM units in the Future Predictor of a 1 layer (unconditioned) Composite Model\ntrained on moving MNIST digits. The vertical axis corresponds to different LSTM units. The horizontal axis is time. The model was\nonly trained to predict the next 10 frames, but here we let it run to predict the next 100 frames. Top: The dynamics has a periodic quality\nwhich does not die out. Bottom : The pattern of activity, if the trained weights in the future predictor are replaced by random weights.\nThe dynamics quickly dies out.\nmance on supervised tasks.\nWe trained a two layer Composite Model with 2048 hid-\nden units with no conditioning on either decoders. The\nmodel was trained on percepts extracted from 300 hours\nof YouTube data. The model was trained to autoencode\n16 frames and predict the next 13 frames. We initialize an\nLSTM classi\ufb01er with the weights learned by the encoder\nLSTM from this model. The classi\ufb01er is shown in Fig. 11.\nThe output from each LSTM in the second layer goes into a\nsoftmax classi\ufb01er that makes a prediction about the action\nbeing performed at each time step. Since only one action is\nbeing performed in each video in the datasets we consider,\nthe target is the same at each time step. At test time, the\npredictions made at each time step are averaged. To get a\nprediction for the entire video, we average the predictions\nfrom all 16 frame blocks in the video with a stride of 8\nframes. Using a smaller stride did not improve results.\nThe baseline for comparing these models is an identical\nLSTM classi\ufb01er but with randomly initialized weights. All\nclassi\ufb01ers used dropout regularization, where we dropped\nactivations as they were communicated across layers but\nnot through time within the same LSTM as proposed in\nZaremba et al. (2014). We emphasize that this is a very\nstrong baseline and does signi\ufb01cantly better than just using\nsingle frames. Using dropout was crucial in order to train\ngood baseline models especially with very few training ex-\namples.\nv1\nv2\n. . .\nvT\n. . .\n. . .\ny1\ny2\n. . .\nyT\nW (1)\nW (1)\nW (1)\nW (2)\nW (2)\nW (2)\nFigure 11. LSTM Classi\ufb01er.\nUnsupervised Learning with LSTMs\nGround Truth Future\n-\n\u001b\nInput Sequence\n\u001b\n-\nFuture Prediction\n-\n\u001b\nInput Reconstruction\n\u001b\n-\nFigure 8. Out-of-domain runs. Reconstruction and Future prediction for test sequences of one and three moving digits. The model was\ntrained on sequences of two moving digits.\nInput\n(a) Inputs\nInput Gates\n(b) Input Gates\nForget Gates\n(c) Forget Gates\nOutput Gates\n(d) Output Gates\nFigure 9. Input features from a Composite Model trained on moving MNIST digits. In an LSTM, each input frame is connected to four\nsets of units - the input, the input gate, forget gate and output gate. These \ufb01gures show the top-200 features ordered by L2 norm of the\ninput features. The features in corresponding locations belong to the same LSTM unit.\n(a) Input Reconstruction\n(b) Future Prediction\nFigure 10. Output features from the two decoder LSTMs of a Composite Model trained on moving MNIST digits. These \ufb01gures show\nthe top-200 features ordered by L2 norm.\nUnsupervised Learning with LSTMs\nModel\nUCF-101\nRGB\nUCF-101\n1- frame \ufb02ow\nHMDB-51\nRGB\nSingle Frame\n72.2\n72.2\n40.1\nLSTM classi\ufb01er\n74.5\n74.3\n42.8\nComposite LSTM\nModel + Finetuning\n75.8\n74.9\n44.1\nTable 1. Summary of Results on Action Recognition.\nFig. 12 compares three models - single frame classi\ufb01er\n(logistic regression), baseline LSTM classi\ufb01er and the\nLSTM classi\ufb01er initialized with weights from the Com-\nposite Model as the number of labelled videos per class is\nvaried. Note that having one labelled video means having\nmany labelled 16 frame blocks. We can see that for the case\nof very few training examples, unsupervised learning gives\na substantial improvement. For example, for UCF-101, the\nperformance improves from 29.6% to 34.3% when train-\ning on only one labelled video. As the size of the labelled\ndataset grows, the improvement becomes smaller. Even for\nthe full UCF-101 dataset we still get a considerable im-\nprovement from 74.5% to 75.8%. On HMDB-51, the im-\nprovement is from 42.8% to 44.0% for the full dataset (70\nvideos per class) and 14.4% to 19.1% for one video per\nclass. Although, the improvement in classi\ufb01cation by us-\ning unsupervised learning was not as big as we expected,\nwe still managed to yield an additional improvement over\na strong baseline. We discuss some avenues for improve-\nments later.\nWe further ran similar experiments on the optical \ufb02ow per-\ncepts extracted from the UCF-101 dataset.\nA temporal\nstream convolutional net, similar to the one proposed by Si-\nmonyan & Zisserman (2014b), was trained on single frame\noptical \ufb02ows as well as on stacks of 10 optical \ufb02ows. This\ngave an accuracy of 72.2% and 77.5% respectively. Here\nagain, our models took 16 frames as input, reconstructed\nthem and predicted 13 frames into the future. LSTMs with\n128 hidden units improved the accuracy by 2.1% to 74.3%\nfor the single frame case. Bigger LSTMs did not improve\nresults. By pretraining the LSTM, we were able to further\nimprove the classi\ufb01cation to 74.9% (\u00b10.1). For stacks of\n10 frames we improved very slightly to 77.7%. These re-\nsults are summarized in Table 1.\n3.4. Comparison of Different Model Variants\nThe aim of this set of experiments is to compare the dif-\nferent variants of the model proposed in this paper. Since\nit is always possible to get lower reconstruction error by\ncopying the inputs, we cannot use input reconstruction er-\nror as a measure of how good a model is doing. However,\nwe can use the error in predicting the future as a reasonable\nmeasure of how good the model is doing. Besides, we can\nuse the performance on supervised tasks as a proxy for how\ngood the unsupervised model is doing. In this section, we\nModel\nCross Entropy\non MNIST\nSquared loss\non image\npatches\nFuture Predictor\n350.2\n225.2\nComposite Model\n344.9\n210.7\nConditional Future Predictor\n343.5\n221.3\nComposite Model with\nConditional Future Predictor\n341.2\n208.1\nTable 2. Future prediction results on MNIST and image patches.\nAll models use 2 layers of LSTMs.\npresent results from these two analyses.\nFuture prediction results are summarized in Table 2. For\nMNIST we compute the cross entropy of the predictions\nwith respect to the ground truth, both of which are 64 \u00d7\n64 patches. For natural image patches, we compute the\nsquared loss. We see that the Composite Model always\ndoes a better job of predicting the future compared to the\nFuture Predictor. This indicates that having the autoen-\ncoder along with the future predictor to force the model\nto remember more about the inputs actually helps predict\nthe future better. Next, we can compare each model with\nits conditional variant. Here, we \ufb01nd that the conditional\nmodels perform better, as was also noted in Fig. 5.\nNext, we compare the models using performance on a su-\npervised task. Table 3 shows the performance on action\nrecognition achieved by \ufb01netuning different unsupervised\nlearning models. Besides running the experiments on the\nfull UCF-101 and HMDB-51 datasets, we also ran the ex-\nperiments on small subsets of these to better highlight the\ncase where we have very few training examples. We \ufb01nd\nthat all unsupervised models improve over the baseline\nLSTM which is itself well-regularized by using dropout.\nThe Autoencoder model seems to perform consistently bet-\nter than the Future Predictor. The Composite model which\ncombines the two does better than either one alone. Con-\nditioning on the generated inputs does not seem to give a\nclear advantage over not doing so. The Composite Model\nwith a conditional future predictor works the best, although\nits performance is almost same as that of the Composite\nModel.\n3.5. Comparison with Other Action Recognition\nBenchmarks\nFinally, we compare our models to the state-of-the-art ac-\ntion recognition results. The performance is summarized in\nTable 4. The table is divided into three sets. The \ufb01rst set\ncompares models that use only RGB data (single or mul-\ntiple frames). The second set compares models that use\nexplicitly computed \ufb02ow features only. Models in the third\nset use both.\nOn RGB data, our model performs at par with the best deep\nUnsupervised Learning with LSTMs\n1\n2\n4\n10\n20\n50\n100\nTraining Examples per class\n20\n30\n40\n50\n60\n70\n80\nClassi\ufb01cation Accuracy\nSingle Frame\nLSTM\nLSTM + Pretraining\n(a) UCF-101 RGB\n1\n2\n4\n8\n16\n32\n64\nTraining Examples per class\n5\n10\n15\n20\n25\n30\n35\n40\n45\n50\nClassi\ufb01cation Accuracy\nSingle Frame\nLSTM\nLSTM + Pretraining\n(b) HMDB-51 RGB\nFigure 12. Effect of pretraining on action recognition with change in the size of the labelled training set. The error bars are over 10\ndifferent samples of training sets.\nMethod\nUCF-101 small\nUCF-101\nHMDB-51 small\nHMDB-51\nBaseline LSTM\n63.7\n74.5\n25.3\n42.8\nAutoencoder\n66.2\n75.1\n28.6\n44.0\nFuture Predictor\n64.9\n74.9\n27.3\n43.1\nConditional Autoencoder\n65.8\n74.8\n27.9\n43.1\nConditional Future Predictor\n65.1\n74.9\n27.4\n43.4\nComposite Model\n67.0\n75.8\n29.1\n44.1\nComposite Model with Conditional Future Predictor\n67.1\n75.8\n29.2\n44.0\nTable 3. Comparison of different unsupervised pretraining methods. UCF-101 small is a subset containing 10 videos per class. HMDB-\n51 small contains 4 videos per class.\nmodels. It performs 3% better than the LRCN model that\nalso used LSTMs on top of convnet features1. Our model\nperforms better than C3D features that use a 3D convolu-\ntional net. However, when the C3D features are concate-\nnated with fc6 percepts, they do slightly better than our\nmodel.\nThe improvement for \ufb02ow features over using a randomly\ninitialized LSTM network is quite small. We believe this is\natleast partly due to the fact that the \ufb02ow percepts already\ncapture a lot of the motion information that the LSTM\nwould otherwise discover.\nWhen we combine predictions from the RGB and \ufb02ow\nmodels, we obtain 84.3 accuracy on UCF-101. We believe\nfurther improvements can be made by running the model\nover different patch locations and mirroring the patches.\nAlso, our model can be applied deeper inside the convnet\ninstead of just at the top-level. That can potentially lead to\nfurther improvements. In this paper, we focus on showing\nthat unsupervised training helps consistently across both\ndatasets and across different sized training sets.\n1However, the improvement is only partially from unsuper-\nvised learning, since we used a better convnet model.\nMethod\nUCF-101 HMDB-\n51\nSpatial Convolutional Net (Simonyan &\nZisserman, 2014a)\n73.0\n40.5\nC3D (Tran et al., 2014)\n72.3\n-\nC3D + fc6 (Tran et al., 2014)\n76.4\n-\nLRCN (Donahue et al., 2014)\n71.1\n-\nComposite LSTM Model\n75.8\n44.0\nTemporal Convolutional Net (Simonyan &\nZisserman, 2014a)\n83.7\n54.6\nLRCN (Donahue et al., 2014)\n77.0\n-\nComposite LSTM Model\n77.7\n-\nLRCN (Donahue et al., 2014)\n82.9\n-\nTwo-stream Convolutional Net (Simonyan &\nZisserman, 2014a)\n88.0\n59.4\nMulti-skip feature stacking (Lan et al., 2014)\n89.1\n65.1\nComposite LSTM Model\n84.3\n-\nTable 4. Comparison with state-of-the-art action recognition\nmodels.\n4. Conclusions\nWe proposed models based on LSTMs that can learn good\nvideo representations. We compared them and analyzed\ntheir properties through visualizations. Moreover, we man-\naged to get an improvement on supervised tasks. The best\nperforming model was the Composite Model that combined\nan autoencoder and a future predictor.\nConditioning on\ngenerated outputs did not have a signi\ufb01cant impact on the\nUnsupervised Learning with LSTMs\nperformance for supervised tasks, however it made the fu-\nture predictions look slightly better. The model was able to\npersistently generate motion well beyond the time scales it\nwas trained for. However, it lost the precise object features\nrapidly after the training time scale. The features at the in-\nput and output layers were found to have some interesting\nproperties.\nTo further get improvements for supervised tasks, we be-\nlieve that the model can be extended by applying it convo-\nlutionally across patches of the video and stacking multiple\nlayers of such models. Applying this model in the lower\nlayers of a convolutional net could help extract motion in-\nformation that would otherwise be lost across max-pooling\nlayers. In our future work, we plan to build models based\non these autoencoders from the bottom up instead of apply-\ning them only to percepts.\nAcknowledgments\nWe acknowledge the support of Samsung, Raytheon BBN\nTechnologies, and NVIDIA Corporation for the donation\nof a GPU used for this research. The authors would like to\nthank Geoffrey Hinton and Ilya Sutskever for helpful dis-\ncussions and comments.\nReferences\nCho, Kyunghyun, van Merrienboer, Bart, G\u00a8ulc\u00b8ehre, C\u00b8 aglar, Bah-\ndanau, Dzmitry, Bougares, Fethi, Schwenk, Holger, and Ben-\ngio, Yoshua.\nLearning phrase representations using RNN\nencoder-decoder for statistical machine translation.\nIn Pro-\nceedings of the 2014 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2014, pp. 1724\u20131734,\n2014.\nDonahue, Jeff, Hendricks, Lisa Anne, Guadarrama, Sergio,\nRohrbach, Marcus, Venugopalan, Subhashini, Saenko, Kate,\nand Darrell, Trevor.\nLong-term recurrent convolutional\nnetworks for visual recognition and description.\nCoRR,\nabs/1411.4389, 2014.\nGraves, Alex. Generating sequences with recurrent neural net-\nworks. CoRR, abs/1308.0850, 2013.\nGraves, Alex and Jaitly, Navdeep. Towards end-to-end speech\nrecognition with recurrent neural networks.\nIn Proceedings\nof the 31st International Conference on Machine Learning\n(ICML-14), pp. 1764\u20131772, 2014.\nHochreiter, Sepp and Schmidhuber, J\u00a8urgen.\nLong short-term\nmemory. Neural Computation, 9(8):1735\u20131780, 1997.\nHurri, Jarmo and Hyv\u00a8arinen, Aapo.\nSimple-cell-like receptive\n\ufb01elds maximize temporal coherence in natural video. Neural\nComputation, 15(3):663\u2013691, 2003.\nJi, Shuiwang, Xu, Wei, Yang, Ming, and Yu, Kai. 3d convolu-\ntional neural networks for human action recognition. Pattern\nAnalysis and Machine Intelligence, IEEE Transactions on, 35\n(1):221\u2013231, Jan 2013.\nKarpathy, Andrej, Toderici, George, Shetty, Sanketh, Leung,\nThomas, Sukthankar, Rahul, and Fei-Fei, Li. Large-scale video\nclassi\ufb01cation with convolutional neural networks. In CVPR,\n2014.\nKuehne, H., Jhuang, H., Garrote, E., Poggio, T., and Serre, T.\nHMDB: a large video database for human motion recognition.\nIn Proceedings of the International Conference on Computer\nVision (ICCV), 2011.\nLan, Zhen-Zhong, Lin, Ming, Li, Xuanchong, Hauptmann,\nAlexander G., and Raj, Bhiksha. Beyond gaussian pyramid:\nMulti-skip feature stacking for action recognition.\nCoRR,\nabs/1411.6660, 2014.\nLe, Q. V., Zou, W., Yeung, S. Y., and Ng, A. Y. Learning hi-\nerarchical spatio-temporal features for action recognition with\nindependent subspace analysis. In CVPR, 2011.\nMemisevic, Roland.\nLearning to relate images.\nIEEE Trans-\nactions on Pattern Analysis and Machine Intelligence, 35(8):\n1829\u20131846, 2013.\nMemisevic, Roland and Hinton, Geoffrey E. Learning to represent\nspatial transformations with factored higher-order boltzmann\nmachines. Neural Computation, 22(6):1473\u20131492, June 2010.\nMichalski, Vincent, Memisevic, Roland, and Konda, Kishore.\nModeling deep temporal dependencies with recurrent grammar\ncells. In Advances in Neural Information Processing Systems\n27, pp. 1925\u20131933. Curran Associates, Inc., 2014.\nRanzato, Marc\u2019Aurelio, Szlam, Arthur, Bruna, Joan, Mathieu,\nMicha\u00a8el, Collobert, Ronan, and Chopra, Sumit. Video (lan-\nguage) modeling: a baseline for generative models of natural\nvideos. CoRR, abs/1412.6604, 2014.\nSimonyan, K. and Zisserman, A. Two-stream convolutional net-\nworks for action recognition in videos. In Advances in Neural\nInformation Processing Systems, 2014a.\nSimonyan,\nK. and Zisserman,\nA.\nVery deep convolu-\ntional networks for large-scale image recognition.\nCoRR,\nabs/1409.1556, 2014b.\nSoomro, k., Roshan Zamir, A., and Shah, M. UCF101: A dataset\nof 101 human actions classes from videos in the wild.\nIn\nCRCV-TR-12-01, 2012.\nSusskind, J., Memisevic, R., Hinton, G., and Pollefeys, M. Mod-\neling the joint density of two images under a variety of trans-\nformations. In Proceedings of IEEE Conference on Computer\nVision and Pattern Recognition, 2011.\nSutskever, Ilya, Vinyals, Oriol, and Le, Quoc V. V. Sequence to\nsequence learning with neural networks. In Advances in Neural\nInformation Processing Systems 27, pp. 3104\u20133112. 2014.\nTran, Du, Bourdev, Lubomir D., Fergus, Rob, Torresani, Lorenzo,\nand Paluri, Manohar. C3D: generic features for video analysis.\nCoRR, abs/1412.0767, 2014.\nvan Hateren, J. H. and Ruderman, D. L. Independent component\nanalysis of natural image sequences yields spatio-temporal \ufb01l-\nters similar to simple cells in primary visual cortex. Proceed-\nings. Biological sciences / The Royal Society, 265(1412):2315\u2013\n2320, 1998.\nVinyals, Oriol, Toshev, Alexander, Bengio, Samy, and Erhan, Du-\nmitru. Show and tell: A neural image caption generator. CoRR,\nabs/1411.4555, 2014.\nZaremba, Wojciech, Sutskever, Ilya, and Vinyals, Oriol.\nRe-\ncurrent neural network regularization. CoRR, abs/1409.2329,\n2014.\n",
        "sentence": " Among the most interesting of these applications are unsupervised video encoding [61], video captioning [66] and program execution [71].",
        "context": "Additionally, learning good video representations is essen-\ntial for a number of useful tasks, such as recognizing ac-\ntions and gestures.\n1.1. Why Unsupervised Learning?\nSupervised learning has been extremely successful in learn-\nvaried. Note that having one labelled video means having\nmany labelled 16 frame blocks. We can see that for the case\nof very few training examples, unsupervised learning gives\na substantial improvement. For example, for UCF-101, the\nlabelled examples. We \ufb01nd that this is indeed the case.\n1.3. Related Work\nThe \ufb01rst approaches to learning representations of videos\nin an unsupervised way were based on ICA (van Hateren\n& Ruderman, 1998; Hurri & Hyv\u00a8arinen, 2003). Le et al."
    },
    {
        "title": "On the importance of initialization and momentum in deep learning",
        "author": [
            "Ilya Sutskever",
            "James Martens",
            "George Dahl",
            "Geoffrey Hinton"
        ],
        "venue": "In Proceedings of the 30th International Conference on Machine Learning",
        "citeRegEx": "62",
        "shortCiteRegEx": "62",
        "year": 2013,
        "abstract": "",
        "full_text": "",
        "sentence": " When the momentum parameter is well-tuned and the network is initialized well, momentum methods can train deep nets and recurrent nets competitively with more computationally expensive methods like the Hessian Free optimizer [62].",
        "context": null
    },
    {
        "title": "Generating text with recurrent neural networks",
        "author": [
            "Ilya Sutskever",
            "James Martens",
            "Geoffrey E Hinton"
        ],
        "venue": "In Proceedings of the 28th International Conference on Machine Learning",
        "citeRegEx": "63",
        "shortCiteRegEx": "63",
        "year": 2011,
        "abstract": "",
        "full_text": "",
        "sentence": " Following the convention adopted in several foundational recurrent net papers ([32], [19], [21], [63]), we index neurons with j and j\u2032, and by wjj\u2032 , we denote the weight corresponding to the directed edge from node j\u2032 to node j. Common choices for the activation function include the sigmoid \u03c3(z) = 1/(1 + e\u2212z) and the tanh function \u03c6(z) = (e \u2212 e\u2212z)/(ez + e\u2212z) which has become common in feedforward neural nets and was applied to recurrent nets in [63]. , truncated Newton approach [45] and applied it to a network which learns to generate text one character at a time in [63]. In many experiments with recurrent neural networks ([18], [63], [71]), input is fed in one character at a time (and output generated one character at a time).",
        "context": null
    },
    {
        "title": "Sequence to sequence learning with neural networks",
        "author": [
            "Ilya Sutskever",
            "Oriol Vinyals",
            "Quoc VV Le"
        ],
        "venue": "In Advances in Neural Information Processing Systems,",
        "citeRegEx": "64",
        "shortCiteRegEx": "64",
        "year": 2014,
        "abstract": "Deep Neural Networks (DNNs) are powerful models that have achieved excellent\nperformance on difficult learning tasks. Although DNNs work well whenever large\nlabeled training sets are available, they cannot be used to map sequences to\nsequences. In this paper, we present a general end-to-end approach to sequence\nlearning that makes minimal assumptions on the sequence structure. Our method\nuses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to\na vector of a fixed dimensionality, and then another deep LSTM to decode the\ntarget sequence from the vector. Our main result is that on an English to\nFrench translation task from the WMT'14 dataset, the translations produced by\nthe LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's\nBLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did\nnot have difficulty on long sentences. For comparison, a phrase-based SMT\nsystem achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM\nto rerank the 1000 hypotheses produced by the aforementioned SMT system, its\nBLEU score increases to 36.5, which is close to the previous best result on\nthis task. The LSTM also learned sensible phrase and sentence representations\nthat are sensitive to word order and are relatively invariant to the active and\nthe passive voice. Finally, we found that reversing the order of the words in\nall source sentences (but not target sentences) improved the LSTM's performance\nmarkedly, because doing so introduced many short term dependencies between the\nsource and the target sentence which made the optimization problem easier.",
        "full_text": "arXiv:1409.3215v3  [cs.CL]  14 Dec 2014\nSequence to Sequence Learning\nwith Neural Networks\nIlya Sutskever\nGoogle\nilyasu@google.com\nOriol Vinyals\nGoogle\nvinyals@google.com\nQuoc V. Le\nGoogle\nqvl@google.com\nAbstract\nDeep Neural Networks (DNNs) are powerful models that have achieved excel-\nlent performance on dif\ufb01cult learning tasks. Although DNNs work well whenever\nlarge labeled training sets are available, they cannot be used to map sequences to\nsequences. In this paper, we present a general end-to-end approach to sequence\nlearning that makes minimal assumptions on the sequence structure. Our method\nuses a multilayered Long Short-Term Memory (LSTM) to map the input sequence\nto a vector of a \ufb01xed dimensionality, and then another deep LSTM to decode the\ntarget sequence from the vector. Our main result is that on an English to French\ntranslation task from the WMT\u201914 dataset, the translations produced by the LSTM\nachieve a BLEU score of 34.8 on the entire test set, where the LSTM\u2019s BLEU\nscore was penalized on out-of-vocabulary words. Additionally, the LSTM did not\nhave dif\ufb01culty on long sentences. For comparison, a phrase-based SMT system\nachieves a BLEU score of 33.3 on the same dataset. When we used the LSTM\nto rerank the 1000 hypotheses produced by the aforementioned SMT system, its\nBLEU score increases to 36.5, which is close to the previous best result on this\ntask. The LSTM also learned sensible phrase and sentence representations that\nare sensitive to word order and are relatively invariant to the active and the pas-\nsive voice. Finally, we found that reversing the order of the words in all source\nsentences (but not target sentences) improved the LSTM\u2019s performance markedly,\nbecause doing so introduced many short term dependencies between the source\nand the target sentence which made the optimization problem easier.\n1\nIntroduction\nDeep Neural Networks (DNNs) are extremely powerful machine learning models that achieve ex-\ncellent performance on dif\ufb01cult problems such as speech recognition [13, 7] and visual object recog-\nnition [19, 6, 21, 20]. DNNs are powerful because they can perform arbitrary parallel computation\nfor a modest number of steps. A surprising example of the power of DNNs is their ability to sort\nN N-bit numbers using only 2 hidden layers of quadratic size [27]. So, while neural networks are\nrelated to conventional statistical models, they learn an intricate computation. Furthermore, large\nDNNs can be trained with supervised backpropagation whenever the labeled training set has enough\ninformation to specify the network\u2019s parameters. Thus, if there exists a parameter setting of a large\nDNN that achieves good results (for example, because humans can solve the task very rapidly),\nsupervised backpropagation will \ufb01nd these parameters and solve the problem.\nDespite their \ufb02exibility and power, DNNs can only be applied to problems whose inputs and targets\ncan be sensibly encoded with vectors of \ufb01xed dimensionality. It is a signi\ufb01cant limitation, since\nmany important problems are best expressed with sequences whose lengths are not known a-priori.\nFor example, speech recognition and machine translation are sequential problems. Likewise, ques-\ntion answering can also be seen as mapping a sequence of words representing the question to a\n1\nsequence of words representing the answer. It is therefore clear that a domain-independent method\nthat learns to map sequences to sequences would be useful.\nSequences pose a challenge for DNNs because they require that the dimensionality of the inputs and\noutputs is known and \ufb01xed. In this paper, we show that a straightforward application of the Long\nShort-Term Memory (LSTM) architecture [16] can solve general sequence to sequence problems.\nThe idea is to use one LSTM to read the input sequence, one timestep at a time, to obtain large \ufb01xed-\ndimensional vector representation, and then to use another LSTM to extract the output sequence\nfrom that vector (\ufb01g. 1). The second LSTM is essentially a recurrent neural network language model\n[28, 23, 30] except that it is conditioned on the input sequence. The LSTM\u2019s ability to successfully\nlearn on data with long range temporal dependencies makes it a natural choice for this application\ndue to the considerable time lag between the inputs and their corresponding outputs (\ufb01g. 1).\nThere have been a number of related attempts to address the general sequence to sequence learning\nproblem with neural networks. Our approach is closely related to Kalchbrenner and Blunsom [18]\nwho were the \ufb01rst to map the entire input sentence to vector, and is related to Cho et al. [5] although\nthe latter was used only for rescoring hypotheses produced by a phrase-based system. Graves [10]\nintroduced a novel differentiable attention mechanism that allows neural networks to focus on dif-\nferent parts of their input, and an elegant variant of this idea was successfully applied to machine\ntranslation by Bahdanau et al. [2]. The Connectionist Sequence Classi\ufb01cation is another popular\ntechnique for mapping sequences to sequences with neural networks, but it assumes a monotonic\nalignment between the inputs and the outputs [11].\nFigure 1: Our model reads an input sentence \u201cABC\u201d and produces \u201cWXYZ\u201d as the output sentence. The\nmodel stops making predictions after outputting the end-of-sentence token. Note that the LSTM reads the\ninput sentence in reverse, because doing so introduces many short term dependencies in the data that make the\noptimization problem much easier.\nThe main result of this work is the following. On the WMT\u201914 English to French translation task,\nwe obtained a BLEU score of 34.81 by directly extracting translations from an ensemble of 5 deep\nLSTMs (with 384M parameters and 8,000 dimensional state each) using a simple left-to-right beam-\nsearch decoder. This is by far the best result achieved by direct translation with large neural net-\nworks. For comparison, the BLEU score of an SMT baseline on this dataset is 33.30 [29]. The 34.81\nBLEU score was achieved by an LSTM with a vocabulary of 80k words, so the score was penalized\nwhenever the reference translation contained a word not covered by these 80k. This result shows\nthat a relatively unoptimized small-vocabulary neural network architecture which has much room\nfor improvement outperforms a phrase-based SMT system.\nFinally, we used the LSTM to rescore the publicly available 1000-best lists of the SMT baseline on\nthe same task [29]. By doing so, we obtained a BLEU score of 36.5, which improves the baseline by\n3.2 BLEU points and is close to the previous best published result on this task (which is 37.0 [9]).\nSurprisingly, the LSTM did not suffer on very long sentences, despite the recent experience of other\nresearchers with related architectures [26]. We were able to do well on long sentences because we\nreversed the order of words in the source sentence but not the target sentences in the training and test\nset. By doing so, we introduced many short term dependencies that made the optimization problem\nmuch simpler (see sec. 2 and 3.3). As a result, SGD could learn LSTMs that had no trouble with\nlong sentences. The simple trick of reversing the words in the source sentence is one of the key\ntechnical contributions of this work.\nA useful property of the LSTM is that it learns to map an input sentence of variable length into\na \ufb01xed-dimensional vector representation. Given that translations tend to be paraphrases of the\nsource sentences, the translation objective encourages the LSTM to \ufb01nd sentence representations\nthat capture their meaning, as sentences with similar meanings are close to each other while different\n2\nsentences meanings will be far. A qualitative evaluation supports this claim, showing that our model\nis aware of word order and is fairly invariant to the active and passive voice.\n2\nThe model\nThe Recurrent Neural Network (RNN) [31, 28] is a natural generalization of feedforward neural\nnetworks to sequences. Given a sequence of inputs (x1, . . . , xT ), a standard RNN computes a\nsequence of outputs (y1, . . . , yT ) by iterating the following equation:\nht\n=\nsigm\n\u0000W hxxt + W hhht\u22121\n\u0001\nyt\n=\nW yhht\nThe RNN can easily map sequences to sequences whenever the alignment between the inputs the\noutputs is known ahead of time. However, it is not clear how to apply an RNN to problems whose\ninput and the output sequences have different lengths with complicated and non-monotonic relation-\nships.\nThe simplest strategy for general sequence learning is to map the input sequence to a \ufb01xed-sized\nvector using one RNN, and then to map the vector to the target sequence with another RNN (this\napproach has also been taken by Cho et al. [5]). While it could work in principle since the RNN is\nprovided with all the relevant information, it would be dif\ufb01cult to train the RNNs due to the resulting\nlong term dependencies (\ufb01gure 1) [14, 4, 16, 15]. However, the Long Short-Term Memory (LSTM)\n[16] is known to learn problems with long range temporal dependencies, so an LSTM may succeed\nin this setting.\nThe goal of the LSTM is to estimate the conditional probability p(y1, . . . , yT \u2032|x1, . . . , xT ) where\n(x1, . . . , xT ) is an input sequence and y1, . . . , yT \u2032 is its corresponding output sequence whose length\nT \u2032 may differ from T . The LSTM computes this conditional probability by \ufb01rst obtaining the \ufb01xed-\ndimensional representation v of the input sequence (x1, . . . , xT ) given by the last hidden state of the\nLSTM, and then computing the probability of y1, . . . , yT \u2032 with a standard LSTM-LM formulation\nwhose initial hidden state is set to the representation v of x1, . . . , xT :\np(y1, . . . , yT \u2032|x1, . . . , xT ) =\nT \u2032\nY\nt=1\np(yt|v, y1, . . . , yt\u22121)\n(1)\nIn this equation, each p(yt|v, y1, . . . , yt\u22121) distribution is represented with a softmax over all the\nwords in the vocabulary. We use the LSTM formulation from Graves [10]. Note that we require that\neach sentence ends with a special end-of-sentence symbol \u201c<EOS>\u201d, which enables the model to\nde\ufb01ne a distribution over sequences of all possible lengths. The overall scheme is outlined in \ufb01gure\n1, where the shown LSTM computes the representation of \u201cA\u201d, \u201cB\u201d, \u201cC\u201d, \u201c<EOS>\u201d and then uses\nthis representation to compute the probability of \u201cW\u201d, \u201cX\u201d, \u201cY\u201d, \u201cZ\u201d, \u201c<EOS>\u201d.\nOur actual models differ from the above description in three important ways. First, we used two\ndifferent LSTMs: one for the input sequence and another for the output sequence, because doing\nso increases the number model parameters at negligible computational cost and makes it natural to\ntrain the LSTM on multiple language pairs simultaneously [18]. Second, we found that deep LSTMs\nsigni\ufb01cantly outperformed shallow LSTMs, so we chose an LSTM with four layers. Third, we found\nit extremely valuable to reverse the order of the words of the input sentence. So for example, instead\nof mapping the sentence a, b, c to the sentence \u03b1, \u03b2, \u03b3, the LSTM is asked to map c, b, a to \u03b1, \u03b2, \u03b3,\nwhere \u03b1, \u03b2, \u03b3 is the translation of a, b, c. This way, a is in close proximity to \u03b1, b is fairly close to \u03b2,\nand so on, a fact that makes it easy for SGD to \u201cestablish communication\u201d between the input and the\noutput. We found this simple data transformation to greatly improve the performance of the LSTM.\n3\nExperiments\nWe applied our method to the WMT\u201914 English to French MT task in two ways. We used it to\ndirectly translate the input sentence without using a reference SMT system and we it to rescore the\nn-best lists of an SMT baseline. We report the accuracy of these translation methods, present sample\ntranslations, and visualize the resulting sentence representation.\n3\n3.1\nDataset details\nWe used the WMT\u201914 English to French dataset. We trained our models on a subset of 12M sen-\ntences consisting of 348M French words and 304M English words, which is a clean \u201cselected\u201d\nsubset from [29]. We chose this translation task and this speci\ufb01c training set subset because of the\npublic availability of a tokenized training and test set together with 1000-best lists from the baseline\nSMT [29].\nAs typical neural language models rely on a vector representation for each word, we used a \ufb01xed\nvocabulary for both languages. We used 160,000 of the most frequent words for the source language\nand 80,000 of the most frequent words for the target language. Every out-of-vocabulary word was\nreplaced with a special \u201cUNK\u201d token.\n3.2\nDecoding and Rescoring\nThe core of our experiments involved training a large deep LSTM on many sentence pairs. We\ntrained it by maximizing the log probability of a correct translation T given the source sentence S,\nso the training objective is\n1/|S|\nX\n(T,S)\u2208S\nlog p(T |S)\nwhere S is the training set. Once training is complete, we produce translations by \ufb01nding the most\nlikely translation according to the LSTM:\n\u02c6T = arg max\nT\np(T |S)\n(2)\nWe search for the most likely translation using a simple left-to-right beam search decoder which\nmaintains a small number B of partial hypotheses, where a partial hypothesis is a pre\ufb01x of some\ntranslation. At each timestep we extend each partial hypothesis in the beam with every possible\nword in the vocabulary. This greatly increases the number of the hypotheses so we discard all but\nthe B most likely hypotheses according to the model\u2019s log probability. As soon as the \u201c<EOS>\u201d\nsymbol is appended to a hypothesis, it is removed from the beam and is added to the set of complete\nhypotheses. While this decoder is approximate, it is simple to implement. Interestingly, our system\nperforms well even with a beam size of 1, and a beam of size 2 provides most of the bene\ufb01ts of beam\nsearch (Table 1).\nWe also used the LSTM to rescore the 1000-best lists produced by the baseline system [29]. To\nrescore an n-best list, we computed the log probability of every hypothesis with our LSTM and took\nan even average with their score and the LSTM\u2019s score.\n3.3\nReversing the Source Sentences\nWhile the LSTM is capable of solving problems with long term dependencies, we discovered that\nthe LSTM learns much better when the source sentences are reversed (the target sentences are not\nreversed). By doing so, the LSTM\u2019s test perplexity dropped from 5.8 to 4.7, and the test BLEU\nscores of its decoded translations increased from 25.9 to 30.6.\nWhile we do not have a complete explanation to this phenomenon, we believe that it is caused by\nthe introduction of many short term dependencies to the dataset. Normally, when we concatenate a\nsource sentence with a target sentence, each word in the source sentence is far from its corresponding\nword in the target sentence. As a result, the problem has a large \u201cminimal time lag\u201d [17]. By\nreversing the words in the source sentence, the average distance between corresponding words in\nthe source and target language is unchanged. However, the \ufb01rst few words in the source language\nare now very close to the \ufb01rst few words in the target language, so the problem\u2019s minimal time lag is\ngreatly reduced. Thus, backpropagation has an easier time \u201cestablishing communication\u201d between\nthe source sentence and the target sentence, which in turn results in substantially improved overall\nperformance.\nInitially, we believed that reversing the input sentences would only lead to more con\ufb01dent predic-\ntions in the early parts of the target sentence and to less con\ufb01dent predictions in the later parts. How-\never, LSTMs trained on reversed source sentences did much better on long sentences than LSTMs\n4\ntrained on the raw source sentences (see sec. 3.7), which suggests that reversing the input sentences\nresults in LSTMs with better memory utilization.\n3.4\nTraining details\nWe found that the LSTM models are fairly easy to train. We used deep LSTMs with 4 layers,\nwith 1000 cells at each layer and 1000 dimensional word embeddings, with an input vocabulary\nof 160,000 and an output vocabulary of 80,000. Thus the deep LSTM uses 8000 real numbers to\nrepresent a sentence. We found deep LSTMs to signi\ufb01cantly outperform shallow LSTMs, where\neach additional layer reduced perplexity by nearly 10%, possibly due to their much larger hidden\nstate. We used a naive softmax over 80,000 words at each output. The resulting LSTM has 384M\nparameters of which 64M are pure recurrent connections (32M for the \u201cencoder\u201d LSTM and 32M\nfor the \u201cdecoder\u201d LSTM). The complete training details are given below:\n\u2022 We initialized all of the LSTM\u2019s parameters with the uniform distribution between -0.08\nand 0.08\n\u2022 We used stochastic gradient descent without momentum, with a \ufb01xed learning rate of 0.7.\nAfter 5 epochs, we begun halving the learning rate every half epoch. We trained our models\nfor a total of 7.5 epochs.\n\u2022 We used batches of 128 sequences for the gradient and divided it the size of the batch\n(namely, 128).\n\u2022 Although LSTMs tend to not suffer from the vanishing gradient problem, they can have\nexploding gradients. Thus we enforced a hard constraint on the norm of the gradient [10,\n25] by scaling it when its norm exceeded a threshold. For each training batch, we compute\ns = \u2225g\u22252, where g is the gradient divided by 128. If s > 5, we set g = 5g\ns .\n\u2022 Different sentences have different lengths. Most sentences are short (e.g., length 20-30)\nbut some sentences are long (e.g., length > 100), so a minibatch of 128 randomly chosen\ntraining sentences will have many short sentences and few long sentences, and as a result,\nmuch of the computation in the minibatch is wasted. To address this problem, we made sure\nthat all sentences in a minibatch are roughly of the same length, yielding a 2x speedup.\n3.5\nParallelization\nA C++ implementation of deep LSTM with the con\ufb01guration from the previous section on a sin-\ngle GPU processes a speed of approximately 1,700 words per second. This was too slow for our\npurposes, so we parallelized our model using an 8-GPU machine. Each layer of the LSTM was\nexecuted on a different GPU and communicated its activations to the next GPU / layer as soon as\nthey were computed. Our models have 4 layers of LSTMs, each of which resides on a separate\nGPU. The remaining 4 GPUs were used to parallelize the softmax, so each GPU was responsible\nfor multiplying by a 1000 \u00d7 20000 matrix. The resulting implementation achieved a speed of 6,300\n(both English and French) words per second with a minibatch size of 128. Training took about a ten\ndays with this implementation.\n3.6\nExperimental Results\nWe used the cased BLEU score [24] to evaluate the quality of our translations. We computed our\nBLEU scores using multi-bleu.pl1 on the tokenized predictions and ground truth. This way\nof evaluating the BELU score is consistent with [5] and [2], and reproduces the 33.3 score of [29].\nHowever, if we evaluate the best WMT\u201914 system [9] (whose predictions can be downloaded from\nstatmt.org\\matrix) in this manner, we get 37.0, which is greater than the 35.8 reported by\nstatmt.org\\matrix.\nThe results are presented in tables 1 and 2. Our best results are obtained with an ensemble of LSTMs\nthat differ in their random initializations and in the random order of minibatches. While the decoded\ntranslations of the LSTM ensemble do not outperform the best WMT\u201914 system, it is the \ufb01rst time\nthat a pure neural translation system outperforms a phrase-based SMT baseline on a large scale MT\n1There several variants of the BLEU score, and each variant is de\ufb01ned with a perl script.\n5\nMethod\ntest BLEU score (ntst14)\nBahdanau et al. [2]\n28.45\nBaseline System [29]\n33.30\nSingle forward LSTM, beam size 12\n26.17\nSingle reversed LSTM, beam size 12\n30.59\nEnsemble of 5 reversed LSTMs, beam size 1\n33.00\nEnsemble of 2 reversed LSTMs, beam size 12\n33.27\nEnsemble of 5 reversed LSTMs, beam size 2\n34.50\nEnsemble of 5 reversed LSTMs, beam size 12\n34.81\nTable 1: The performance of the LSTM on WMT\u201914 English to French test set (ntst14). Note that\nan ensemble of 5 LSTMs with a beam of size 2 is cheaper than of a single LSTM with a beam of\nsize 12.\nMethod\ntest BLEU score (ntst14)\nBaseline System [29]\n33.30\nCho et al. [5]\n34.54\nBest WMT\u201914 result [9]\n37.0\nRescoring the baseline 1000-best with a single forward LSTM\n35.61\nRescoring the baseline 1000-best with a single reversed LSTM\n35.85\nRescoring the baseline 1000-best with an ensemble of 5 reversed LSTMs\n36.5\nOracle Rescoring of the Baseline 1000-best lists\n\u223c45\nTable 2: Methods that use neural networks together with an SMT system on the WMT\u201914 English\nto French test set (ntst14).\ntask by a sizeable margin, despite its inability to handle out-of-vocabulary words. The LSTM is\nwithin 0.5 BLEU points of the best WMT\u201914 result if it is used to rescore the 1000-best list of the\nbaseline system.\n3.7\nPerformance on long sentences\nWe were surprised to discover that the LSTM did well on long sentences, which is shown quantita-\ntively in \ufb01gure 3. Table 3 presents several examples of long sentences and their translations.\n3.8\nModel Analysis\n\u22128\n\u22126\n\u22124\n\u22122\n0\n2\n4\n6\n8\n10\n\u22126\n\u22125\n\u22124\n\u22123\n\u22122\n\u22121\n0\n1\n2\n3\n4\nJohn respects Mary\nMary respects John\nJohn admires Mary\nMary admires John\nMary is in love with John\nJohn is in love with Mary\n\u221215\n\u221210\n\u22125\n0\n5\n10\n15\n20\n\u221220\n\u221215\n\u221210\n\u22125\n0\n5\n10\n15\nI gave her a card in the garden\nIn the garden , I gave her a card\nShe was given a card by me in the garden\nShe gave me a card in the garden\nIn the garden , she gave me a card\nI was given a card by her in the garden\nFigure 2: The \ufb01gure shows a 2-dimensional PCA projection of the LSTM hidden states that are obtained\nafter processing the phrases in the \ufb01gures. The phrases are clustered by meaning, which in these examples is\nprimarily a function of word order, which would be dif\ufb01cult to capture with a bag-of-words model. Notice that\nboth clusters have similar internal structure.\nOne of the attractive features of our model is its ability to turn a sequence of words into a vector\nof \ufb01xed dimensionality. Figure 2 visualizes some of the learned representations. The \ufb01gure clearly\nshows that the representations are sensitive to the order of words, while being fairly insensitive to the\n6\nType\nSentence\nOur model\nUlrich UNK , membre du conseil d\u2019 administration du constructeur automobile Audi ,\naf\ufb01rme qu\u2019 il s\u2019 agit d\u2019 une pratique courante depuis des ann\u00b4ees pour que les t\u00b4el\u00b4ephones\nportables puissent \u02c6etre collect\u00b4es avant les r\u00b4eunions du conseil d\u2019 administration a\ufb01n qu\u2019 ils\nne soient pas utilis\u00b4es comme appareils d\u2019 \u00b4ecoute `a distance .\nTruth\nUlrich Hackenberg , membre du conseil d\u2019 administration du constructeur automobile Audi ,\nd\u00b4eclare que la collecte des t\u00b4el\u00b4ephones portables avant les r\u00b4eunions du conseil , a\ufb01n qu\u2019 ils\nne puissent pas \u02c6etre utilis\u00b4es comme appareils d\u2019 \u00b4ecoute `a distance , est une pratique courante\ndepuis des ann\u00b4ees .\nOur model\n\u201c Les t\u00b4el\u00b4ephones cellulaires , qui sont vraiment une question , non seulement parce qu\u2019 ils\npourraient potentiellement causer des interf\u00b4erences avec les appareils de navigation , mais\nnous savons , selon la FCC , qu\u2019 ils pourraient interf\u00b4erer avec les tours de t\u00b4el\u00b4ephone cellulaire\nlorsqu\u2019 ils sont dans l\u2019 air \u201d , dit UNK .\nTruth\n\u201c Les t\u00b4el\u00b4ephones portables sont v\u00b4eritablement un probl`eme , non seulement parce qu\u2019 ils\npourraient \u00b4eventuellement cr\u00b4eer des interf\u00b4erences avec les instruments de navigation , mais\nparce que nous savons , d\u2019 apr`es la FCC , qu\u2019 ils pourraient perturber les antennes-relais de\nt\u00b4el\u00b4ephonie mobile s\u2019 ils sont utilis\u00b4es `a bord \u201d , a d\u00b4eclar\u00b4e Rosenker .\nOur model\nAvec la cr\u00b4emation , il y a un \u201c sentiment de violence contre le corps d\u2019 un \u02c6etre cher \u201d ,\nqui sera \u201c r\u00b4eduit `a une pile de cendres \u201d en tr`es peu de temps au lieu d\u2019 un processus de\nd\u00b4ecomposition \u201c qui accompagnera les \u00b4etapes du deuil \u201d .\nTruth\nIl y a , avec la cr\u00b4emation , \u201c une violence faite au corps aim\u00b4e \u201d ,\nqui va \u02c6etre \u201c r\u00b4eduit `a un tas de cendres \u201d en tr`es peu de temps , et non apr`es un processus de\nd\u00b4ecomposition , qui \u201c accompagnerait les phases du deuil \u201d .\nTable 3: A few examples of long translations produced by the LSTM alongside the ground truth\ntranslations. The reader can verify that the translations are sensible using Google translate.\n4 7 8\n12\n17\n22\n28\n35\n79\ntest sentences sorted by their length\n20\n25\n30\n35\n40\nBLEU score\nLSTM  (34.8)\nbaseline (33.3)\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\ntest sentences sorted by average word frequency rank\n20\n25\n30\n35\n40\nBLEU score\nLSTM  (34.8)\nbaseline (33.3)\nFigure 3: The left plot shows the performance of our system as a function of sentence length, where the\nx-axis corresponds to the test sentences sorted by their length and is marked by the actual sequence lengths.\nThere is no degradation on sentences with less than 35 words, there is only a minor degradation on the longest\nsentences. The right plot shows the LSTM\u2019s performance on sentences with progressively more rare words,\nwhere the x-axis corresponds to the test sentences sorted by their \u201caverage word frequency rank\u201d.\nreplacement of an active voice with a passive voice. The two-dimensional projections are obtained\nusing PCA.\n4\nRelated work\nThere is a large body of work on applications of neural networks to machine translation. So far,\nthe simplest and most effective way of applying an RNN-Language Model (RNNLM) [23] or a\n7\nFeedforward Neural Network Language Model (NNLM) [3] to an MT task is by rescoring the n-\nbest lists of a strong MT baseline [22], which reliably improves translation quality.\nMore recently, researchers have begun to look into ways of including information about the source\nlanguage into the NNLM. Examples of this work include Auli et al. [1], who combine an NNLM\nwith a topic model of the input sentence, which improves rescoring performance. Devlin et al. [8]\nfollowed a similar approach, but they incorporated their NNLM into the decoder of an MT system\nand used the decoder\u2019s alignment information to provide the NNLM with the most useful words in\nthe input sentence. Their approach was highly successful and it achieved large improvements over\ntheir baseline.\nOur work is closely related to Kalchbrenner and Blunsom [18], who were the \ufb01rst to map the input\nsentence into a vector and then back to a sentence, although they map sentences to vectors using\nconvolutional neural networks, which lose the ordering of the words. Similarly to this work, Cho et\nal. [5] used an LSTM-like RNN architecture to map sentences into vectors and back, although their\nprimary focus was on integrating their neural network into an SMT system. Bahdanau et al. [2] also\nattempted direct translations with a neural network that used an attention mechanism to overcome\nthe poor performance on long sentences experienced by Cho et al. [5] and achieved encouraging\nresults. Likewise, Pouget-Abadie et al. [26] attempted to address the memory problem of Cho et\nal. [5] by translating pieces of the source sentence in way that produces smooth translations, which\nis similar to a phrase-based approach. We suspect that they could achieve similar improvements by\nsimply training their networks on reversed source sentences.\nEnd-to-end training is also the focus of Hermann et al. [12], whose model represents the inputs and\noutputs by feedforward networks, and map them to similar points in space. However, their approach\ncannot generate translations directly: to get a translation, they need to do a look up for closest vector\nin the pre-computed database of sentences, or to rescore a sentence.\n5\nConclusion\nIn this work, we showed that a large deep LSTM, that has a limited vocabulary and that makes\nalmost no assumption about problem structure can outperform a standard SMT-based system whose\nvocabulary is unlimited on a large-scale MT task. The success of our simple LSTM-based approach\non MT suggests that it should do well on many other sequence learning problems, provided they\nhave enough training data.\nWe were surprised by the extent of the improvement obtained by reversing the words in the source\nsentences. We conclude that it is important to \ufb01nd a problem encoding that has the greatest number\nof short term dependencies, as they make the learning problem much simpler. In particular, while\nwe were unable to train a standard RNN on the non-reversed translation problem (shown in \ufb01g. 1),\nwe believe that a standard RNN should be easily trainable when the source sentences are reversed\n(although we did not verify it experimentally).\nWe were also surprised by the ability of the LSTM to correctly translate very long sentences. We\nwere initially convinced that the LSTM would fail on long sentences due to its limited memory,\nand other researchers reported poor performance on long sentences with a model similar to ours\n[5, 2, 26]. And yet, LSTMs trained on the reversed dataset had little dif\ufb01culty translating long\nsentences.\nMost importantly, we demonstrated that a simple, straightforward and a relatively unoptimized ap-\nproach can outperform an SMT system, so further work will likely lead to even greater translation\naccuracies. These results suggest that our approach will likely do well on other challenging sequence\nto sequence problems.\n6\nAcknowledgments\nWe thank Samy Bengio, Jeff Dean, Matthieu Devin, Geoffrey Hinton, Nal Kalchbrenner, Thang Luong, Wolf-\ngang Macherey, Rajat Monga, Vincent Vanhoucke, Peng Xu, Wojciech Zaremba, and the Google Brain team\nfor useful comments and discussions.\n8\nReferences\n[1] M. Auli, M. Galley, C. Quirk, and G. Zweig. Joint language and translation modeling with recurrent\nneural networks. In EMNLP, 2013.\n[2] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate.\narXiv preprint arXiv:1409.0473, 2014.\n[3] Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. A neural probabilistic language model. In Journal of\nMachine Learning Research, pages 1137\u20131155, 2003.\n[4] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is dif\ufb01cult.\nIEEE Transactions on Neural Networks, 5(2):157\u2013166, 1994.\n[5] K. Cho, B. Merrienboer, C. Gulcehre, F. Bougares, H. Schwenk, and Y. Bengio. Learning phrase represen-\ntations using RNN encoder-decoder for statistical machine translation. In Arxiv preprint arXiv:1406.1078,\n2014.\n[6] D. Ciresan, U. Meier, and J. Schmidhuber. Multi-column deep neural networks for image classi\ufb01cation.\nIn CVPR, 2012.\n[7] G. E. Dahl, D. Yu, L. Deng, and A. Acero. Context-dependent pre-trained deep neural networks for large\nvocabulary speech recognition. IEEE Transactions on Audio, Speech, and Language Processing - Special\nIssue on Deep Learning for Speech and Language Processing, 2012.\n[8] J. Devlin, R. Zbib, Z. Huang, T. Lamar, R. Schwartz, and J. Makhoul. Fast and robust neural network\njoint models for statistical machine translation. In ACL, 2014.\n[9] Nadir Durrani, Barry Haddow, Philipp Koehn, and Kenneth Hea\ufb01eld. Edinburgh\u2019s phrase-based machine\ntranslation systems for wmt-14. In WMT, 2014.\n[10] A. Graves. Generating sequences with recurrent neural networks. In Arxiv preprint arXiv:1308.0850,\n2013.\n[11] A. Graves, S. Fern\u00b4andez, F. Gomez, and J. Schmidhuber. Connectionist temporal classi\ufb01cation: labelling\nunsegmented sequence data with recurrent neural networks. In ICML, 2006.\n[12] K. M. Hermann and P. Blunsom. Multilingual distributed representations without word alignment. In\nICLR, 2014.\n[13] G. Hinton, L. Deng, D. Yu, G. Dahl, A. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen,\nT. Sainath, and B. Kingsbury. Deep neural networks for acoustic modeling in speech recognition. IEEE\nSignal Processing Magazine, 2012.\n[14] S. Hochreiter. Untersuchungen zu dynamischen neuronalen netzen. Master\u2019s thesis, Institut fur Infor-\nmatik, Technische Universitat, Munchen, 1991.\n[15] S. Hochreiter, Y. Bengio, P. Frasconi, and J. Schmidhuber. Gradient \ufb02ow in recurrent nets: the dif\ufb01culty\nof learning long-term dependencies, 2001.\n[16] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 1997.\n[17] S. Hochreiter and J. Schmidhuber. LSTM can solve hard long time lag problems. 1997.\n[18] N. Kalchbrenner and P. Blunsom. Recurrent continuous translation models. In EMNLP, 2013.\n[19] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet classi\ufb01cation with deep convolutional neural\nnetworks. In NIPS, 2012.\n[20] Q.V. Le, M.A. Ranzato, R. Monga, M. Devin, K. Chen, G.S. Corrado, J. Dean, and A.Y. Ng. Building\nhigh-level features using large scale unsupervised learning. In ICML, 2012.\n[21] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition.\nProceedings of the IEEE, 1998.\n[22] T. Mikolov. Statistical Language Models based on Neural Networks. PhD thesis, Brno University of\nTechnology, 2012.\n[23] T. Mikolov, M. Kara\ufb01\u00b4at, L. Burget, J. Cernock`y, and S. Khudanpur. Recurrent neural network based\nlanguage model. In INTERSPEECH, pages 1045\u20131048, 2010.\n[24] K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. BLEU: a method for automatic evaluation of machine\ntranslation. In ACL, 2002.\n[25] R. Pascanu, T. Mikolov, and Y. Bengio. On the dif\ufb01culty of training recurrent neural networks. arXiv\npreprint arXiv:1211.5063, 2012.\n[26] J. Pouget-Abadie, D. Bahdanau, B. van Merrienboer, K. Cho, and Y. Bengio.\nOvercoming the\ncurse of sentence length for neural machine translation using automatic segmentation. arXiv preprint\narXiv:1409.1257, 2014.\n[27] A. Razborov. On small depth threshold circuits. In Proc. 3rd Scandinavian Workshop on Algorithm\nTheory, 1992.\n[28] D. Rumelhart, G. E. Hinton, and R. J. Williams. Learning representations by back-propagating errors.\nNature, 323(6088):533\u2013536, 1986.\n[29] H. Schwenk. University le mans. http://www-lium.univ-lemans.fr/\u02dcschwenk/cslm_\njoint_paper/, 2014. [Online; accessed 03-September-2014].\n[30] M. Sundermeyer, R. Schluter, and H. Ney. LSTM neural networks for language modeling. In INTER-\nSPEECH, 2010.\n[31] P. Werbos. Backpropagation through time: what it does and how to do it. Proceedings of IEEE, 1990.\n9\n",
        "sentence": " \u2019s model for sequence to sequence learning [64], compute the output at each time step and pass a representation of this information as the input at the following time step. [64] translates sentences between natural languages. Seeking comprehensibility, we break with this convention and use i, f , and o to refer to input, forget and output gates respectively as in [64]. [64].",
        "context": "dimensional vector representation, and then to use another LSTM to extract the output sequence\nfrom that vector (\ufb01g. 1). The second LSTM is essentially a recurrent neural network language model\nuses a multilayered Long Short-Term Memory (LSTM) to map the input sequence\nto a vector of a \ufb01xed dimensionality, and then another deep LSTM to decode the\ntarget sequence from the vector. Our main result is that on an English to French\ndue to the considerable time lag between the inputs and their corresponding outputs (\ufb01g. 1).\nThere have been a number of related attempts to address the general sequence to sequence learning"
    },
    {
        "title": "Computing machinery and intelligence",
        "author": [
            "Alan M Turing"
        ],
        "venue": "Mind, pages 433\u2013",
        "citeRegEx": "65",
        "shortCiteRegEx": "65",
        "year": 1950,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " In Alan Turing\u2019s groundbreaking paper Computing Machinery and Intelligence, he proposes an \u201cImitation Game\u201d which judges a machine\u2019s intelligence by its ability to convincingly engage in dialogue [65].",
        "context": null
    },
    {
        "title": "Sequence to sequence\u2013video to text",
        "author": [
            "Subhashini Venugopalan",
            "Marcus Rohrbach",
            "Jeff Donahue",
            "Raymond Mooney",
            "Trevor Darrell",
            "Kate Saenko"
        ],
        "venue": "arXiv preprint arXiv:1505.00487,",
        "citeRegEx": "66",
        "shortCiteRegEx": "66",
        "year": 2015,
        "abstract": "",
        "full_text": "",
        "sentence": " Among the most interesting of these applications are unsupervised video encoding [61], video captioning [66] and program execution [71]. In [66], Venugopalan et al.",
        "context": null
    },
    {
        "title": "Show and tell: A neural image caption generator",
        "author": [
            "Oriol Vinyals",
            "Alexander Toshev",
            "Samy Bengio",
            "Dumitru Erhan"
        ],
        "venue": "arXiv preprint arXiv:1411.4555,",
        "citeRegEx": "67",
        "shortCiteRegEx": "67",
        "year": 2014,
        "abstract": "Automatically describing the content of an image is a fundamental problem in\nartificial intelligence that connects computer vision and natural language\nprocessing. In this paper, we present a generative model based on a deep\nrecurrent architecture that combines recent advances in computer vision and\nmachine translation and that can be used to generate natural sentences\ndescribing an image. The model is trained to maximize the likelihood of the\ntarget description sentence given the training image. Experiments on several\ndatasets show the accuracy of the model and the fluency of the language it\nlearns solely from image descriptions. Our model is often quite accurate, which\nwe verify both qualitatively and quantitatively. For instance, while the\ncurrent state-of-the-art BLEU-1 score (the higher the better) on the Pascal\ndataset is 25, our approach yields 59, to be compared to human performance\naround 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66,\nand on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we\nachieve a BLEU-4 of 27.7, which is the current state-of-the-art.",
        "full_text": "Show and Tell: A Neural Image Caption Generator\nOriol Vinyals\nGoogle\nvinyals@google.com\nAlexander Toshev\nGoogle\ntoshev@google.com\nSamy Bengio\nGoogle\nbengio@google.com\nDumitru Erhan\nGoogle\ndumitru@google.com\nAbstract\nAutomatically describing the content of an image is a\nfundamental problem in arti\ufb01cial intelligence that connects\ncomputer vision and natural language processing. In this\npaper, we present a generative model based on a deep re-\ncurrent architecture that combines recent advances in com-\nputer vision and machine translation and that can be used\nto generate natural sentences describing an image.\nThe\nmodel is trained to maximize the likelihood of the target de-\nscription sentence given the training image. Experiments\non several datasets show the accuracy of the model and the\n\ufb02uency of the language it learns solely from image descrip-\ntions. Our model is often quite accurate, which we verify\nboth qualitatively and quantitatively. For instance, while\nthe current state-of-the-art BLEU-1 score (the higher the\nbetter) on the Pascal dataset is 25, our approach yields 59,\nto be compared to human performance around 69. We also\nshow BLEU-1 score improvements on Flickr30k, from 56 to\n66, and on SBU, from 19 to 28. Lastly, on the newly released\nCOCO dataset, we achieve a BLEU-4 of 27.7, which is the\ncurrent state-of-the-art.\n1. Introduction\nBeing able to automatically describe the content of an\nimage using properly formed English sentences is a very\nchallenging task, but it could have great impact, for instance\nby helping visually impaired people better understand the\ncontent of images on the web. This task is signi\ufb01cantly\nharder, for example, than the well-studied image classi\ufb01-\ncation or object recognition tasks, which have been a main\nfocus in the computer vision community [27]. Indeed, a\ndescription must capture not only the objects contained in\nan image, but it also must express how these objects relate\nto each other as well as their attributes and the activities\nthey are involved in. Moreover, the above semantic knowl-\nedge has to be expressed in a natural language like English,\nwhich means that a language model is needed in addition to\nvisual understanding.\nMost previous attempts have proposed to stitch together\nA group of people \nshopping at an \noutdoor market. \n!\nThere are many \nvegetables at the \nfruit stand.\nVision!\nDeep CNN\nLanguage !\nGenerating!\nRNN\nFigure 1.\nNIC, our model, is based end-to-end on a neural net-\nwork consisting of a vision CNN followed by a language gener-\nating RNN. It generates complete sentences in natural language\nfrom an input image, as shown on the example above.\nexisting solutions of the above sub-problems, in order to go\nfrom an image to its description [6, 16]. In contrast, we\nwould like to present in this work a single joint model that\ntakes an image I as input, and is trained to maximize the\nlikelihood p(S|I) of producing a target sequence of words\nS = {S1, S2, . . .} where each word St comes from a given\ndictionary, that describes the image adequately.\nThe main inspiration of our work comes from recent ad-\nvances in machine translation, where the task is to transform\na sentence S written in a source language, into its transla-\ntion T in the target language, by maximizing p(T|S). For\nmany years, machine translation was also achieved by a se-\nries of separate tasks (translating words individually, align-\ning words, reordering, etc), but recent work has shown that\ntranslation can be done in a much simpler way using Re-\ncurrent Neural Networks (RNNs) [3, 2, 30] and still reach\nstate-of-the-art performance. An \u201cencoder\u201d RNN reads the\nsource sentence and transforms it into a rich \ufb01xed-length\nvector representation, which in turn in used as the initial\nhidden state of a \u201cdecoder\u201d RNN that generates the target\nsentence.\nHere, we propose to follow this elegant recipe, replac-\ning the encoder RNN by a deep convolution neural network\n(CNN). Over the last few years it has been convincingly\nshown that CNNs can produce a rich representation of the\ninput image by embedding it to a \ufb01xed-length vector, such\nthat this representation can be used for a variety of vision\n1\narXiv:1411.4555v2  [cs.CV]  20 Apr 2015\ntasks [28]. Hence, it is natural to use a CNN as an image\n\u201cencoder\u201d, by \ufb01rst pre-training it for an image classi\ufb01cation\ntask and using the last hidden layer as an input to the RNN\ndecoder that generates sentences (see Fig. 1). We call this\nmodel the Neural Image Caption, or NIC.\nOur contributions are as follows. First, we present an\nend-to-end system for the problem. It is a neural net which\nis fully trainable using stochastic gradient descent. Second,\nour model combines state-of-art sub-networks for vision\nand language models. These can be pre-trained on larger\ncorpora and thus can take advantage of additional data. Fi-\nnally, it yields signi\ufb01cantly better performance compared\nto state-of-the-art approaches; for instance, on the Pascal\ndataset, NIC yielded a BLEU score of 59, to be compared to\nthe current state-of-the-art of 25, while human performance\nreaches 69. On Flickr30k, we improve from 56 to 66, and\non SBU, from 19 to 28.\n2. Related Work\nThe problem of generating natural language descriptions\nfrom visual data has long been studied in computer vision,\nbut mainly for video [7, 32]. This has led to complex sys-\ntems composed of visual primitive recognizers combined\nwith a structured formal language, e.g. And-Or Graphs or\nlogic systems, which are further converted to natural lan-\nguage via rule-based systems.\nSuch systems are heav-\nily hand-designed, relatively brittle and have been demon-\nstrated only on limited domains, e.g. traf\ufb01c scenes or sports.\nThe problem of still image description with natural text\nhas gained interest more recently. Leveraging recent ad-\nvances in recognition of objects, their attributes and loca-\ntions, allows us to drive natural language generation sys-\ntems, though these are limited in their expressivity. Farhadi\net al. [6] use detections to infer a triplet of scene elements\nwhich is converted to text using templates. Similarly, Li\net al. [19] start off with detections and piece together a \ufb01-\nnal description using phrases containing detected objects\nand relationships.\nA more complex graph of detections\nbeyond triplets is used by Kulkani et al. [16], but with\ntemplate-based text generation. More powerful language\nmodels based on language parsing have been used as well\n[23, 1, 17, 18, 5]. The above approaches have been able to\ndescribe images \u201cin the wild\u201d, but they are heavily hand-\ndesigned and rigid when it comes to text generation.\nA large body of work has addressed the problem of rank-\ning descriptions for a given image [11, 8, 24]. Such ap-\nproaches are based on the idea of co-embedding of images\nand text in the same vector space. For an image query, de-\nscriptions are retrieved which lie close to the image in the\nembedding space. Most closely, neural networks are used to\nco-embed images and sentences together [29] or even image\ncrops and subsentences [13] but do not attempt to generate\nnovel descriptions. In general, the above approaches cannot\ndescribe previously unseen compositions of objects, even\nthough the individual objects might have been observed in\nthe training data. Moreover, they avoid addressing the prob-\nlem of evaluating how good a generated description is.\nIn this work we combine deep convolutional nets for im-\nage classi\ufb01cation [12] with recurrent networks for sequence\nmodeling [10], to create a single network that generates de-\nscriptions of images. The RNN is trained in the context of\nthis single \u201cend-to-end\u201d network. The model is inspired by\nrecent successes of sequence generation in machine trans-\nlation [3, 2, 30], with the difference that instead of starting\nwith a sentence, we provide an image processed by a con-\nvolutional net. The closest works are by Kiros et al. [15]\nwho use a neural net, but a feedforward one, to predict the\nnext word given the image and previous words. A recent\nwork by Mao et al. [21] uses a recurrent NN for the same\nprediction task. This is very similar to the present proposal\nbut there are a number of important differences: we use a\nmore powerful RNN model, and provide the visual input to\nthe RNN model directly, which makes it possible for the\nRNN to keep track of the objects that have been explained\nby the text. As a result of these seemingly insigni\ufb01cant dif-\nferences, our system achieves substantially better results on\nthe established benchmarks. Lastly, Kiros et al. [14] pro-\npose to construct a joint multimodal embedding space by\nusing a powerful computer vision model and an LSTM that\nencodes text. In contrast to our approach, they use two sepa-\nrate pathways (one for images, one for text) to de\ufb01ne a joint\nembedding, and, even though they can generate text, their\napproach is highly tuned for ranking.\n3. Model\nIn this paper, we propose a neural and probabilistic\nframework to generate descriptions from images. Recent\nadvances in statistical machine translation have shown that,\ngiven a powerful sequence model, it is possible to achieve\nstate-of-the-art results by directly maximizing the proba-\nbility of the correct translation given an input sentence in\nan \u201cend-to-end\u201d fashion \u2013 both for training and inference.\nThese models make use of a recurrent neural network which\nencodes the variable length input into a \ufb01xed dimensional\nvector, and uses this representation to \u201cdecode\u201d it to the de-\nsired output sentence. Thus, it is natural to use the same ap-\nproach where, given an image (instead of an input sentence\nin the source language), one applies the same principle of\n\u201ctranslating\u201d it into its description.\nThus, we propose to directly maximize the probability of\nthe correct description given the image by using the follow-\ning formulation:\n\u03b8\u22c6= arg max\n\u03b8\nX\n(I,S)\nlog p(S|I; \u03b8)\n(1)\nwhere \u03b8 are the parameters of our model, I is an image, and\nS its correct transcription. Since S represents any sentence,\nits length is unbounded. Thus, it is common to apply the\nchain rule to model the joint probability over S0, . . . , SN,\nwhere N is the length of this particular example as\nlog p(S|I) =\nN\nX\nt=0\nlog p(St|I, S0, . . . , St\u22121)\n(2)\nwhere we dropped the dependency on \u03b8 for convenience.\nAt training time, (S, I) is a training example pair, and we\noptimize the sum of the log probabilities as described in (2)\nover the whole training set using stochastic gradient descent\n(further training details are given in Section 4).\nIt is natural to model p(St|I, S0, . . . , St\u22121) with a Re-\ncurrent Neural Network (RNN), where the variable number\nof words we condition upon up to t \u22121 is expressed by a\n\ufb01xed length hidden state or memory ht. This memory is\nupdated after seeing a new input xt by using a non-linear\nfunction f:\nht+1 = f(ht, xt) .\n(3)\nTo make the above RNN more concrete two crucial design\nchoices are to be made: what is the exact form of f and\nhow are the images and words fed as inputs xt. For f we\nuse a Long-Short Term Memory (LSTM) net, which has\nshown state-of-the art performance on sequence tasks such\nas translation. This model is outlined in the next section.\nFor the representation of images, we use a Convolutional\nNeural Network (CNN). They have been widely used and\nstudied for image tasks, and are currently state-of-the art\nfor object recognition and detection. Our particular choice\nof CNN uses a novel approach to batch normalization and\nyields the current best performance on the ILSVRC 2014\nclassi\ufb01cation competition [12].\nFurthermore, they have\nbeen shown to generalize to other tasks such as scene clas-\nsi\ufb01cation by means of transfer learning [4]. The words are\nrepresented with an embedding model.\n3.1. LSTM-based Sentence Generator\nThe choice of f in (3) is governed by its ability to deal\nwith vanishing and exploding gradients [10], the most com-\nmon challenge in designing and training RNNs. To address\nthis challenge, a particular form of recurrent nets, called\nLSTM, was introduced [10] and applied with great success\nto translation [3, 30] and sequence generation [9].\nThe core of the LSTM model is a memory cell c encod-\ning knowledge at every time step of what inputs have been\nobserved up to this step (see Figure 2) . The behavior of the\ncell is controlled by \u201cgates\u201d \u2013 layers which are applied mul-\ntiplicatively and thus can either keep a value from the gated\nlayer if the gate is 1 or zero this value if the gate is 0. In\nparticular, three gates are being used which control whether\nto forget the current cell value (forget gate f), if it should\nh\n\u03c3\n\u03c3\n\u03c3\nc\ninput\nLSTM\nmemory block\nword prediction\nsoftmax\ninput\ngate i\noutput\ngate f\nforget\ngate f\nupdating\nterm\nct-1\nct\nmt\nx\nFigure 2.\nLSTM: the memory block contains a cell c which is\ncontrolled by three gates. In blue we show the recurrent connec-\ntions \u2013 the output m at time t \u22121 is fed back to the memory at\ntime t via the three gates; the cell value is fed back via the forget\ngate; the predicted word at time t \u22121 is fed back in addition to the\nmemory output m at time t into the Softmax for word prediction.\nread its input (input gate i) and whether to output the new\ncell value (output gate o). The de\ufb01nition of the gates and\ncell update and output are as follows:\nit\n=\n\u03c3(Wixxt + Wimmt\u22121)\n(4)\nft\n=\n\u03c3(Wfxxt + Wfmmt\u22121)\n(5)\not\n=\n\u03c3(Woxxt + Wommt\u22121)\n(6)\nct\n=\nft \u2299ct\u22121 + it \u2299h(Wcxxt + Wcmmt\u22121)(7)\nmt\n=\not \u2299ct\n(8)\npt+1\n=\nSoftmax(mt)\n(9)\nwhere \u2299represents the product with a gate value, and the\nvarious W matrices are trained parameters. Such multi-\nplicative gates make it possible to train the LSTM robustly\nas these gates deal well with exploding and vanishing gra-\ndients [10]. The nonlinearities are sigmoid \u03c3(\u00b7) and hyper-\nbolic tangent h(\u00b7). The last equation mt is what is used to\nfeed to a Softmax, which will produce a probability distri-\nbution pt over all words.\nTraining\nThe LSTM model is trained to predict each\nword of the sentence after it has seen the image as well\nas all preceding words as de\ufb01ned by p(St|I, S0, . . . , St\u22121).\nFor this purpose, it is instructive to think of the LSTM in un-\nrolled form \u2013 a copy of the LSTM memory is created for the\nLSTM\nLSTM\nLSTM\nWeS1\nWeSN-1\np1\npN\np2\nlog p1(S1) \nlog p2(S2) \nlog pN(SN) \n...\nLSTM\nWeS0\nS1\nSN-1\nS0\nimage\nFigure 3. LSTM model combined with a CNN image embedder\n(as de\ufb01ned in [12]) and word embeddings. The unrolled connec-\ntions between the LSTM memories are in blue and they corre-\nspond to the recurrent connections in Figure 2. All LSTMs share\nthe same parameters.\nimage and each sentence word such that all LSTMs share\nthe same parameters and the output mt\u22121 of the LSTM at\ntime t \u22121 is fed to the LSTM at time t (see Figure 3). All\nrecurrent connections are transformed to feed-forward con-\nnections in the unrolled version. In more detail, if we denote\nby I the input image and by S = (S0, . . . , SN) a true sen-\ntence describing this image, the unrolling procedure reads:\nx\u22121\n=\nCNN(I)\n(10)\nxt\n=\nWeSt,\nt \u2208{0 . . . N \u22121}\n(11)\npt+1\n=\nLSTM(xt),\nt \u2208{0 . . . N \u22121}\n(12)\nwhere we represent each word as a one-hot vector St of\ndimension equal to the size of the dictionary. Note that we\ndenote by S0 a special start word and by SN a special stop\nword which designates the start and end of the sentence. In\nparticular by emitting the stop word the LSTM signals that a\ncomplete sentence has been generated. Both the image and\nthe words are mapped to the same space, the image by using\na vision CNN, the words by using word embedding We.\nThe image I is only input once, at t = \u22121, to inform the\nLSTM about the image contents. We empirically veri\ufb01ed\nthat feeding the image at each time step as an extra input\nyields inferior results, as the network can explicitly exploit\nnoise in the image and over\ufb01ts more easily.\nOur loss is the sum of the negative log likelihood of the\ncorrect word at each step as follows:\nL(I, S) = \u2212\nN\nX\nt=1\nlog pt(St) .\n(13)\nThe above loss is minimized w.r.t. all the parameters of the\nLSTM, the top layer of the image embedder CNN and word\nembeddings We.\nInference\nThere are multiple approaches that can be used\nto generate a sentence given an image, with NIC. The \ufb01rst\none is Sampling where we just sample the \ufb01rst word ac-\ncording to p1, then provide the corresponding embedding\nas input and sample p2, continuing like this until we sample\nthe special end-of-sentence token or some maximum length.\nThe second one is BeamSearch: iteratively consider the set\nof the k best sentences up to time t as candidates to generate\nsentences of size t + 1, and keep only the resulting best k\nof them. This better approximates S = arg maxS\u2032 p(S\u2032|I).\nWe used the BeamSearch approach in the following experi-\nments, with a beam of size 20. Using a beam size of 1 (i.e.,\ngreedy search) did degrade our results by 2 BLEU points on\naverage.\n4. Experiments\nWe performed an extensive set of experiments to assess\nthe effectiveness of our model using several metrics, data\nsources, and model architectures, in order to compare to\nprior art.\n4.1. Evaluation Metrics\nAlthough it is sometimes not clear whether a description\nshould be deemed successful or not given an image, prior\nart has proposed several evaluation metrics. The most re-\nliable (but time consuming) is to ask for raters to give a\nsubjective score on the usefulness of each description given\nthe image. In this paper, we used this to reinforce that some\nof the automatic metrics indeed correlate with this subjec-\ntive score, following the guidelines proposed in [11], which\nasks the graders to evaluate each generated sentence with a\nscale from 1 to 41.\nFor this metric, we set up an Amazon Mechanical Turk\nexperiment. Each image was rated by 2 workers. The typ-\nical level of agreement between workers is 65%. In case\nof disagreement we simply average the scores and record\nthe average as the score. For variance analysis, we perform\nbootstrapping (re-sampling the results with replacement and\ncomputing means/standard deviation over the resampled re-\nsults). Like [11] we report the fraction of scores which are\nlarger or equal than a set of prede\ufb01ned thresholds.\nThe rest of the metrics can be computed automatically\nassuming one has access to groundtruth, i.e. human gen-\nerated descriptions. The most commonly used metric so\nfar in the image description literature has been the BLEU\nscore [25], which is a form of precision of word n-grams\nbetween generated and reference sentences 2. Even though\n1 The raters are asked whether the image is described without any er-\nrors, described with minor errors, with a somewhat related description, or\nwith an unrelated description, with a score of 4 being the best and 1 being\nthe worst.\n2In this literature, most previous work report BLEU-1, i.e., they only\ncompute precision at the unigram level, whereas BLEU-n is a geometric\naverage of precision over 1- to n-grams.\nthis metric has some obvious drawbacks, it has been shown\nto correlate well with human evaluations.\nIn this work,\nwe corroborate this as well, as we show in Section 4.3.\nAn extensive evaluation protocol, as well as the generated\noutputs of our system, can be found at http://nic.\ndroppages.com/.\nBesides BLEU, one can use the perplexity of the model\nfor a given transcription (which is closely related to our\nobjective function in (1)). The perplexity is the geometric\nmean of the inverse probability for each predicted word. We\nused this metric to perform choices regarding model selec-\ntion and hyperparameter tuning in our held-out set, but we\ndo not report it since BLEU is always preferred 3. A much\nmore detailed discussion regarding metrics can be found in\n[31], and research groups working on this topic have been\nreporting other metrics which are deemed more appropriate\nfor evaluating caption. We report two such metrics - ME-\nTEOR and Cider - hoping for much more discussion and\nresearch to arise regarding the choice of metric.\nLastly, the current literature on image description has\nalso been using the proxy task of ranking a set of avail-\nable descriptions with respect to a given image (see for in-\nstance [14]). Doing so has the advantage that one can use\nknown ranking metrics like recall@k. On the other hand,\ntransforming the description generation task into a ranking\ntask is unsatisfactory: as the complexity of images to de-\nscribe grows, together with its dictionary, the number of\npossible sentences grows exponentially with the size of the\ndictionary, and the likelihood that a prede\ufb01ned sentence will\n\ufb01t a new image will go down unless the number of such\nsentences also grows exponentially, which is not realistic;\nnot to mention the underlying computational complexity\nof evaluating ef\ufb01ciently such a large corpus of stored sen-\ntences for each image. The same argument has been used in\nspeech recognition, where one has to produce the sentence\ncorresponding to a given acoustic sequence; while early at-\ntempts concentrated on classi\ufb01cation of isolated phonemes\nor words, state-of-the-art approaches for this task are now\ngenerative and can produce sentences from a large dictio-\nnary.\nNow that our models can generate descriptions of rea-\nsonable quality, and despite the ambiguities of evaluating\nan image description (where there could be multiple valid\ndescriptions not in the groundtruth) we believe we should\nconcentrate on evaluation metrics for the generation task\nrather than for ranking.\n4.2. Datasets\nFor evaluation we use a number of datasets which consist\nof images and sentences in English describing these images.\n3Even though it would be more desirable, optimizing for BLEU score\nyields a discrete optimization problem. In general, perplexity and BLEU\nscores are fairly correlated.\nThe statistics of the datasets are as follows:\nDataset name\nsize\ntrain\nvalid.\ntest\nPascal VOC 2008 [6]\n-\n-\n1000\nFlickr8k [26]\n6000\n1000\n1000\nFlickr30k [33]\n28000\n1000\n1000\nMSCOCO [20]\n82783\n40504\n40775\nSBU [24]\n1M\n-\n-\nWith the exception of SBU, each image has been annotated\nby labelers with 5 sentences that are relatively visual and\nunbiased.\nSBU consists of descriptions given by image\nowners when they uploaded them to Flickr. As such they\nare not guaranteed to be visual or unbiased and thus this\ndataset has more noise.\nThe Pascal dataset is customary used for testing only af-\nter a system has been trained on different data such as any of\nthe other four dataset. In the case of SBU, we hold out 1000\nimages for testing and train on the rest as used by [18]. Sim-\nilarly, we reserve 4K random images from the MSCOCO\nvalidation set as test, called COCO-4k, and use it to report\nresults in the following section.\n4.3. Results\nSince our model is data driven and trained end-to-end,\nand given the abundance of datasets, we wanted to an-\nswer questions such as \u201chow dataset size affects general-\nization\u201d, \u201cwhat kinds of transfer learning it would be able\nto achieve\u201d, and \u201chow it would deal with weakly labeled\nexamples\u201d. As a result, we performed experiments on \ufb01ve\ndifferent datasets, explained in Section 4.2, which enabled\nus to understand our model in depth.\n4.3.1\nTraining Details\nMany of the challenges that we faced when training our\nmodels had to do with over\ufb01tting. Indeed, purely supervised\napproaches require large amounts of data, but the datasets\nthat are of high quality have less than 100000 images. The\ntask of assigning a description is strictly harder than object\nclassi\ufb01cation and data driven approaches have only recently\nbecome dominant thanks to datasets as large as ImageNet\n(with ten times more data than the datasets we described\nin this paper, with the exception of SBU). As a result, we\nbelieve that, even with the results we obtained which are\nquite good, the advantage of our method versus most cur-\nrent human-engineered approaches will only increase in the\nnext few years as training set sizes will grow.\nNonetheless, we explored several techniques to deal with\nover\ufb01tting. The most obvious way to not over\ufb01t is to ini-\ntialize the weights of the CNN component of our system\nto a pretrained model (e.g., on ImageNet). We did this in\nall the experiments (similar to [8]), and it did help quite a\nlot in terms of generalization. Another set of weights that\ncould be sensibly initialized are We, the word embeddings.\nWe tried initializing them from a large news corpus [22],\nbut no signi\ufb01cant gains were observed, and we decided to\njust leave them uninitialized for simplicity. Lastly, we did\nsome model level over\ufb01tting-avoiding techniques. We tried\ndropout [34] and ensembling models, as well as exploring\nthe size (i.e., capacity) of the model by trading off number\nof hidden units versus depth. Dropout and ensembling gave\na few BLEU points improvement, and that is what we report\nthroughout the paper.\nWe trained all sets of weights using stochastic gradi-\nent descent with \ufb01xed learning rate and no momentum.\nAll weights were randomly initialized except for the CNN\nweights, which we left unchanged because changing them\nhad a negative impact. We used 512 dimensions for the em-\nbeddings and the size of the LSTM memory.\nDescriptions were preprocessed with basic tokenization,\nkeeping all words that appeared at least 5 times in the train-\ning set.\n4.3.2\nGeneration Results\nWe report our main results on all the relevant datasets in Ta-\nbles 1 and 2. Since PASCAL does not have a training set,\nwe used the system trained using MSCOCO (arguably the\nlargest and highest quality dataset for this task). The state-\nof-the-art results for PASCAL and SBU did not use image\nfeatures based on deep learning, so arguably a big improve-\nment on those scores comes from that change alone. The\nFlickr datasets have been used recently [11, 21, 14], but\nmostly evaluated in a retrieval framework. A notable ex-\nception is [21], where they did both retrieval and genera-\ntion, and which yields the best performance on the Flickr\ndatasets up to now.\nHuman scores in Table 2 were computed by comparing\none of the human captions against the other four. We do this\nfor each of the \ufb01ve raters, and average their BLEU scores.\nSince this gives a slight advantage to our system, given the\nBLEU score is computed against \ufb01ve reference sentences\nand not four, we add back to the human scores the average\ndifference of having \ufb01ve references instead of four.\nGiven that the \ufb01eld has seen signi\ufb01cant advances in the\nlast years, we do think it is more meaningful to report\nBLEU-4, which is the standard in machine translation mov-\ning forward. Additionally, we report metrics shown to cor-\nrelate better with human evaluations in Table 14. Despite\nrecent efforts on better evaluation metrics [31], our model\nfares strongly versus human raters. However, when evalu-\nating our captions using human raters (see Section 4.3.6),\nour model fares much more poorly, suggesting more work\n4We used the implementation of these metrics kindly provided in\nhttp://www.mscoco.org.\nMetric\nBLEU-4\nMETEOR\nCIDER\nNIC\n27.7\n23.7\n85.5\nRandom\n4.6\n9.0\n5.1\nNearest Neighbor\n9.9\n15.7\n36.5\nHuman\n21.7\n25.2\n85.4\nTable 1. Scores on the MSCOCO development set.\nApproach\nPASCAL\nFlickr\nFlickr\nSBU\n(xfer)\n30k\n8k\nIm2Text [24]\n11\nTreeTalk [18]\n19\nBabyTalk [16]\n25\nTri5Sem [11]\n48\nm-RNN [21]\n55\n58\nMNLM [14]5\n56\n51\nSOTA\n25\n56\n58\n19\nNIC\n59\n66\n63\n28\nHuman\n69\n68\n70\nTable 2. BLEU-1 scores. We only report previous work results\nwhen available. SOTA stands for the current state-of-the-art.\nis needed towards better metrics. On the of\ufb01cial test set for\nwhich labels are only available through the of\ufb01cial website,\nour model had a 27.2 BLEU-4.\n4.3.3\nTransfer Learning, Data Size and Label Quality\nSince we have trained many models and we have several\ntesting sets, we wanted to study whether we could transfer\na model to a different dataset, and how much the mismatch\nin domain would be compensated with e.g. higher quality\nlabels or more training data.\nThe most obvious case for transfer learning and data size\nis between Flickr30k and Flickr8k. The two datasets are\nsimilarly labeled as they were created by the same group.\nIndeed, when training on Flickr30k (with about 4 times\nmore training data), the results obtained are 4 BLEU points\nbetter. It is clear that in this case, we see gains by adding\nmore training data since the whole process is data-driven\nand over\ufb01tting prone. MSCOCO is even bigger (5 times\nmore training data than Flickr30k), but since the collection\nprocess was done differently, there are likely more differ-\nences in vocabulary and a larger mismatch. Indeed, all the\nBLEU scores degrade by 10 points. Nonetheless, the de-\nscriptions are still reasonable.\nSince PASCAL has no of\ufb01cial training set and was\ncollected independently of Flickr and MSCOCO, we re-\nport transfer learning from MSCOCO (in Table 2). Doing\ntransfer learning from Flickr30k yielded worse results with\nBLEU-1 at 53 (cf. 59).\nLastly, even though SBU has weak labeling (i.e., the la-\nbels were captions and not human generated descriptions),\n5We computed these BLEU scores with the outputs that the authors of\n[14] kindly provided for their OxfordNet system.\nthe task is much harder with a much larger and noisier vo-\ncabulary. However, much more data is available for train-\ning. When running the MSCOCO model on SBU, our per-\nformance degrades from 28 down to 16.\n4.3.4\nGeneration Diversity Discussion\nHaving trained a generative model that gives p(S|I), an ob-\nvious question is whether the model generates novel cap-\ntions, and whether the generated captions are both diverse\nand high quality. Table 3 shows some samples when re-\nturning the N-best list from our beam search decoder in-\nstead of the best hypothesis. Notice how the samples are di-\nverse and may show different aspects from the same image.\nThe agreement in BLEU score between the top 15 generated\nsentences is 58, which is similar to that of humans among\nthem. This indicates the amount of diversity our model gen-\nerates. In bold are the sentences that are not present in the\ntraining set. If we take the best candidate, the sentence is\npresent in the training set 80% of the times. This is not\ntoo surprising given that the amount of training data is quite\nsmall, so it is relatively easy for the model to pick \u201cexem-\nplar\u201d sentences and use them to generate descriptions. If\nwe instead analyze the top 15 generated sentences, about\nhalf of the times we see a completely novel description, but\nstill with a similar BLEU score, indicating that they are of\nenough quality, yet they provide a healthy diversity.\nA man throwing a frisbee in a park.\nA man holding a frisbee in his hand.\nA man standing in the grass with a frisbee.\nA close up of a sandwich on a plate.\nA close up of a plate of food with french fries.\nA white plate topped with a cut in half sandwich.\nA display case \ufb01lled with lots of donuts.\nA display case \ufb01lled with lots of cakes.\nA bakery display case \ufb01lled with lots of donuts.\nTable 3. N-best examples from the MSCOCO test set. Bold lines\nindicate a novel sentence not present in the training set.\n4.3.5\nRanking Results\nWhile we think ranking is an unsatisfactory way to evalu-\nate description generation from images, many papers report\nranking scores, using the set of testing captions as candi-\ndates to rank given a test image. The approach that works\nbest on these metrics (MNLM), speci\ufb01cally implemented a\nranking-aware loss. Nevertheless, NIC is doing surprisingly\nwell on both ranking tasks (ranking descriptions given im-\nages, and ranking images given descriptions), as can be seen\nin Tables 4 and 5. Note that for the Image Annotation task,\nwe normalized our scores similar to what [21] used.\nApproach\nImage Annotation\nImage Search\nR@1 R@10 Med r R@1 R@10 Med r\nDeFrag [13]\n13\n44\n14\n10\n43\n15\nm-RNN [21]\n15\n49\n11\n12\n42\n15\nMNLM [14]\n18\n55\n8\n13\n52\n10\nNIC\n20\n61\n6\n19\n64\n5\nTable 4. Recall@k and median rank on Flickr8k.\nApproach\nImage Annotation\nImage Search\nR@1 R@10 Med r R@1 R@10 Med r\nDeFrag [13]\n16\n55\n8\n10\n45\n13\nm-RNN [21]\n18\n51\n10\n13\n42\n16\nMNLM [14]\n23\n63\n5\n17\n57\n8\nNIC\n17\n56\n7\n17\n57\n7\nTable 5. Recall@k and median rank on Flickr30k.\nFigure 4.\nFlickr-8k: NIC: predictions produced by NIC on the\nFlickr8k test set (average score: 2.37); Pascal: NIC: (average\nscore: 2.45); COCO-1k: NIC: A subset of 1000 images from the\nMSCOCO test set with descriptions produced by NIC (average\nscore: 2.72); Flickr-8k: ref: these are results from [11] on Flickr8k\nrated using the same protocol, as a baseline (average score: 2.08);\nFlickr-8k: GT: we rated the groundtruth labels from Flickr8k us-\ning the same protocol. This provides us with a \u201ccalibration\u201d of the\nscores (average score: 3.89)\n4.3.6\nHuman Evaluation\nFigure 4 shows the result of the human evaluations of the\ndescriptions provided by NIC, as well as a reference system\nand groundtruth on various datasets. We can see that NIC\nis better than the reference system, but clearly worse than\nthe groundtruth, as expected. This shows that BLEU is not\na perfect metric, as it does not capture well the difference\nbetween NIC and human descriptions assessed by raters.\nExamples of rated images can be seen in Figure 5. It is\ninteresting to see, for instance in the second image of the\n\ufb01rst column, how the model was able to notice the frisbee\ngiven its size.\nFigure 5. A selection of evaluation results, grouped by human rating.\n4.3.7\nAnalysis of Embeddings\nIn order to represent the previous word St\u22121 as input to\nthe decoding LSTM producing St, we use word embedding\nvectors [22], which have the advantage of being indepen-\ndent of the size of the dictionary (contrary to a simpler one-\nhot-encoding approach). Furthermore, these word embed-\ndings can be jointly trained with the rest of the model. It\nis remarkable to see how the learned representations have\ncaptured some semantic from the statistics of the language.\nTable 4.3.7 shows, for a few example words, the nearest\nother words found in the learned embedding space.\nNote how some of the relationships learned by the model\nwill help the vision component. Indeed, having \u201chorse\u201d,\n\u201cpony\u201d, and \u201cdonkey\u201d close to each other will encourage the\nCNN to extract features that are relevant to horse-looking\nanimals. We hypothesize that, in the extreme case where\nwe see very few examples of a class (e.g., \u201cunicorn\u201d), its\nproximity to other word embeddings (e.g., \u201chorse\u201d) should\nprovide a lot more information that would be completely\nlost with more traditional bag-of-words based approaches.\n5. Conclusion\nWe have presented NIC, an end-to-end neural network\nsystem that can automatically view an image and generate\nWord\nNeighbors\ncar\nvan, cab, suv, vehicule, jeep\nboy\ntoddler, gentleman, daughter, son\nstreet\nroad, streets, highway, freeway\nhorse\npony, donkey, pig, goat, mule\ncomputer\ncomputers, pc, crt, chip, compute\nTable 6. Nearest neighbors of a few example words\na reasonable description in plain English. NIC is based on\na convolution neural network that encodes an image into a\ncompact representation, followed by a recurrent neural net-\nwork that generates a corresponding sentence. The model is\ntrained to maximize the likelihood of the sentence given the\nimage. Experiments on several datasets show the robust-\nness of NIC in terms of qualitative results (the generated\nsentences are very reasonable) and quantitative evaluations,\nusing either ranking metrics or BLEU, a metric used in ma-\nchine translation to evaluate the quality of generated sen-\ntences. It is clear from these experiments that, as the size\nof the available datasets for image description increases, so\nwill the performance of approaches like NIC. Furthermore,\nit will be interesting to see how one can use unsupervised\ndata, both from images alone and text alone, to improve im-\nage description approaches.\nAcknowledgement\nWe would like to thank Geoffrey Hinton, Ilya Sutskever,\nQuoc Le, Vincent Vanhoucke, and Jeff Dean for useful dis-\ncussions on the ideas behind the paper, and the write up.\nReferences\n[1] A. Aker and R. Gaizauskas. Generating image descriptions\nusing dependency relational patterns. In ACL, 2010.\n[2] D. Bahdanau, K. Cho, and Y. Bengio.\nNeural ma-\nchine translation by jointly learning to align and translate.\narXiv:1409.0473, 2014.\n[3] K. Cho, B. van Merrienboer, C. Gulcehre, F. Bougares,\nH. Schwenk, and Y. Bengio. Learning phrase representations\nusing RNN encoder-decoder for statistical machine transla-\ntion. In EMNLP, 2014.\n[4] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang,\nE. Tzeng, and T. Darrell. Decaf: A deep convolutional acti-\nvation feature for generic visual recognition. In ICML, 2014.\n[5] D. Elliott and F. Keller. Image description using visual de-\npendency representations. In EMNLP, 2013.\n[6] A. Farhadi,\nM. Hejrati,\nM. A. Sadeghi,\nP. Young,\nC. Rashtchian, J. Hockenmaier, and D. Forsyth. Every pic-\nture tells a story: Generating sentences from images.\nIn\nECCV, 2010.\n[7] R. Gerber and H.-H. Nagel. Knowledge representation for\nthe generation of quanti\ufb01ed natural language descriptions of\nvehicle traf\ufb01c in image sequences. In ICIP. IEEE, 1996.\n[8] Y. Gong, L. Wang, M. Hodosh, J. Hockenmaier, and\nS. Lazebnik. Improving image-sentence embeddings using\nlarge weakly annotated photo collections. In ECCV, 2014.\n[9] A. Graves. Generating sequences with recurrent neural net-\nworks. arXiv:1308.0850, 2013.\n[10] S. Hochreiter and J. Schmidhuber. Long short-term memory.\nNeural Computation, 9(8), 1997.\n[11] M. Hodosh, P. Young, and J. Hockenmaier. Framing image\ndescription as a ranking task: Data, models and evaluation\nmetrics. JAIR, 47, 2013.\n[12] S. Ioffe and C. Szegedy. Batch normalization: Accelerating\ndeep network training by reducing internal covariate shift. In\narXiv:1502.03167, 2015.\n[13] A. Karpathy, A. Joulin, and L. Fei-Fei. Deep fragment em-\nbeddings for bidirectional image sentence mapping. NIPS,\n2014.\n[14] R. Kiros, R. Salakhutdinov, and R. S. Zemel.\nUnifying\nvisual-semantic embeddings with multimodal neural lan-\nguage models. In arXiv:1411.2539, 2014.\n[15] R. Kiros and R. Z. R. Salakhutdinov. Multimodal neural lan-\nguage models. In NIPS Deep Learning Workshop, 2013.\n[16] G. Kulkarni, V. Premraj, S. Dhar, S. Li, Y. Choi, A. C. Berg,\nand T. L. Berg. Baby talk: Understanding and generating\nsimple image descriptions. In CVPR, 2011.\n[17] P. Kuznetsova, V. Ordonez, A. C. Berg, T. L. Berg, and\nY. Choi. Collective generation of natural image descriptions.\nIn ACL, 2012.\n[18] P. Kuznetsova, V. Ordonez, T. Berg, and Y. Choi. Treetalk:\nComposition and compression of trees for image descrip-\ntions. ACL, 2(10), 2014.\n[19] S. Li, G. Kulkarni, T. L. Berg, A. C. Berg, and Y. Choi. Com-\nposing simple image descriptions using web-scale n-grams.\nIn Conference on Computational Natural Language Learn-\ning, 2011.\n[20] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-\nmanan, P. Doll\u00b4ar, and C. L. Zitnick. Microsoft coco: Com-\nmon objects in context. arXiv:1405.0312, 2014.\n[21] J. Mao, W. Xu, Y. Yang, J. Wang, and A. Yuille.\nEx-\nplain images with multimodal recurrent neural networks. In\narXiv:1410.1090, 2014.\n[22] T. Mikolov, K. Chen, G. Corrado, and J. Dean.\nEf\ufb01cient\nestimation of word representations in vector space. In ICLR,\n2013.\n[23] M. Mitchell, X. Han, J. Dodge, A. Mensch, A. Goyal, A. C.\nBerg, K. Yamaguchi, T. L. Berg, K. Stratos, and H. D. III.\nMidge: Generating image descriptions from computer vision\ndetections. In EACL, 2012.\n[24] V. Ordonez, G. Kulkarni, and T. L. Berg. Im2text: Describ-\ning images using 1 million captioned photographs. In NIPS,\n2011.\n[25] K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. BLEU: A\nmethod for automatic evaluation of machine translation. In\nACL, 2002.\n[26] C. Rashtchian, P. Young, M. Hodosh, and J. Hockenmaier.\nCollecting image annotations using amazon\u2019s mechanical\nturk.\nIn NAACL HLT Workshop on Creating Speech and\nLanguage Data with Amazon\u2019s Mechanical Turk, pages 139\u2013\n147, 2010.\n[27] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,\nS. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,\nA. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual\nRecognition Challenge, 2014.\n[28] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,\nand Y. LeCun. Overfeat: Integrated recognition, localization\nand detection using convolutional networks. arXiv preprint\narXiv:1312.6229, 2013.\n[29] R. Socher, A. Karpathy, Q. V. Le, C. Manning, and A. Y. Ng.\nGrounded compositional semantics for \ufb01nding and describ-\ning images with sentences. In ACL, 2014.\n[30] I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence\nlearning with neural networks. In NIPS, 2014.\n[31] R. Vedantam, C. L. Zitnick, and D. Parikh.\nCIDEr:\nConsensus-based\nimage\ndescription\nevaluation.\nIn\narXiv:1411.5726, 2015.\n[32] B. Z. Yao, X. Yang, L. Lin, M. W. Lee, and S.-C. Zhu. I2t:\nImage parsing to text description. Proceedings of the IEEE,\n98(8), 2010.\n[33] P. Young, A. Lai, M. Hodosh, and J. Hockenmaier. From im-\nage descriptions to visual denotations: New similarity met-\nrics for semantic inference over event descriptions. In ACL,\n2014.\n[34] W. Zaremba, I. Sutskever, and O. Vinyals. Recurrent neural\nnetwork regularization. In arXiv:1409.2329, 2014.\n",
        "sentence": " Recently, recurrent neural networks have been used successfully for image captioning, [35, 44, 67].",
        "context": "In this work we combine deep convolutional nets for im-\nage classi\ufb01cation [12] with recurrent networks for sequence\nmodeling [10], to create a single network that generates de-\nscriptions of images. The RNN is trained in the context of\n[8] Y. Gong, L. Wang, M. Hodosh, J. Hockenmaier, and\nS. Lazebnik. Improving image-sentence embeddings using\nlarge weakly annotated photo collections. In ECCV, 2014.\n[9] A. Graves. Generating sequences with recurrent neural net-\nshown state-of-the art performance on sequence tasks such\nas translation. This model is outlined in the next section.\nFor the representation of images, we use a Convolutional\nNeural Network (CNN). They have been widely used and"
    },
    {
        "title": "Backpropagation through time: what it does and how to do it",
        "author": [
            "Paul J Werbos"
        ],
        "venue": "Proceedings of the IEEE,",
        "citeRegEx": "68",
        "shortCiteRegEx": "68",
        "year": 1990,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " This algorithm is called backpropagation through time (BPTT), and was introduced in 1990 [68].",
        "context": null
    },
    {
        "title": "A learning algorithm for continually running fully recurrent neural networks",
        "author": [
            "Ronald J Williams",
            "David Zipser"
        ],
        "venue": "Neural computation,",
        "citeRegEx": "70",
        "shortCiteRegEx": "70",
        "year": 1989,
        "abstract": " The exact form of a gradient-following learning algorithm for completely recurrent networks running in continually sampled time is derived and used as the basis for practical algorithms for temporal supervised learning tasks. These algorithms have (1) the advantage that they do not require a precisely defined training interval, operating while the network runs; and (2) the disadvantage that they require nonlocal communication in the network being trained and are computationally expensive. These algorithms allow networks having recurrent connections to learn complex tasks that require the retention of information over time periods having either fixed or indefinite length. ",
        "full_text": "",
        "sentence": " Truncated backpropagation through time (TBPTT) is one solution to this problem for continuously running networks [70].",
        "context": null
    },
    {
        "title": "Learning to execute",
        "author": [
            "Wojciech Zaremba",
            "Ilya Sutskever"
        ],
        "venue": "arXiv preprint arXiv:1410.4615,",
        "citeRegEx": "71",
        "shortCiteRegEx": "71",
        "year": 2014,
        "abstract": "Recurrent Neural Networks (RNNs) with Long Short-Term Memory units (LSTM) are\nwidely used because they are expressive and are easy to train. Our interest\nlies in empirically evaluating the expressiveness and the learnability of LSTMs\nin the sequence-to-sequence regime by training them to evaluate short computer\nprograms, a domain that has traditionally been seen as too complex for neural\nnetworks. We consider a simple class of programs that can be evaluated with a\nsingle left-to-right pass using constant memory. Our main result is that LSTMs\ncan learn to map the character-level representations of such programs to their\ncorrect outputs. Notably, it was necessary to use curriculum learning, and\nwhile conventional curriculum learning proved ineffective, we developed a new\nvariant of curriculum learning that improved our networks' performance in all\nexperimental conditions. The improved curriculum had a dramatic impact on an\naddition problem, making it possible to train an LSTM to add two 9-digit\nnumbers with 99% accuracy.",
        "full_text": "Under review as a conference paper at ICLR 2015\nLEARNING TO EXECUTE\nWojciech Zaremba\u2217\nNew York University\nwoj.zaremba@gmail.com\nIlya Sutskever\nGoogle\nilyasu@google.com\nABSTRACT\nRecurrent Neural Networks (RNNs) with Long Short-Term Memory units\n(LSTM) are widely used because they are expressive and are easy to train. Our\ninterest lies in empirically evaluating the expressiveness and the learnability of\nLSTMs in the sequence-to-sequence regime by training them to evaluate short\ncomputer programs, a domain that has traditionally been seen as too complex for\nneural networks. We consider a simple class of programs that can be evaluated\nwith a single left-to-right pass using constant memory. Our main result is that\nLSTMs can learn to map the character-level representations of such programs to\ntheir correct outputs. Notably, it was necessary to use curriculum learning, and\nwhile conventional curriculum learning proved ineffective, we developed a new\nvariant of curriculum learning that improved our networks\u2019 performance in all\nexperimental conditions. The improved curriculum had a dramatic impact on an\naddition problem, making it possible to train an LSTM to add two 9-digit numbers\nwith 99% accuracy.\n\u2217\n1\nINTRODUCTION\nExecution of computer programs requires dealing with a number of nontrivial concepts. To execute\na program, a system has to understand numerical operations, if-statements, variable assignments,\nthe compositionality of operations, and many more.\nWe show that Recurrent Neural Networks (RNN) with Long Short-Term Memory (LSTM) units\ncan accurately evaluate short simple programs in the sequence-to-sequence framework of Sutskever\net al. (2014). The LSTM reads the program character-by-character and computes the program\u2019s\noutput. We consider a constrained set of computer programs that can be evaluated in linear time\nand constant memory, because the LSTM reads the program only once and its memory capacity is\nlimited (Section 3).\nWe found it dif\ufb01cult to train LSTMs to execute computer programs, so we used curriculum learn-\ning to simplify the learning problem. We design a curriculum procedure which outperforms both\nconventional training that uses no curriculum learning (baseline) as well as the naive curriculum\nlearning of strategy of Bengio et al. (2009) (Section 4). We provide a plausible explanation for the\neffectiveness of our procedure relative to naive curriculum learning (Section 7).\nFinally, in addition to curriculum learning strategies, we examine two simple input transformations\nthat further simplify the sequence-to-sequence learning problem. We show that, in many cases,\nreversing the input sequence (Sutskever et al., 2014) and replicating the input sequence improves\nthe LSTM\u2019s performance on a memorization task (Section 3.2).\nThe code for replicating most of the experiments in this work can be found in https://github.\ncom/wojciechz/learning_to_execute.\n\u2217Work done while the author was in Google Brain.\n1\narXiv:1410.4615v3  [cs.NE]  19 Feb 2015\nUnder review as a conference paper at ICLR 2015\nInput:\nj=8584\nfor x in range(8):\nj+=920\nb=(1500+j)\nprint((b+7567))\nTarget: 25011.\nInput:\ni=8827\nc=(i-5347)\nprint((c+8704) if 2641<8500 else 5308)\nTarget: 12184.\nFigure 1: Example programs on which we train the LSTM. The output of each program is a single\ninteger. A \u201cdot\u201d symbol indicates the end of the integer, which has to be predicted by the LSTM.\n2\nRELATED WORK\nThere has been related research that used Tree Neural Networks (also known as Recursive Neu-\nral Networks) to evaluate symbolic mathematical expressions and logical formulas (Zaremba et al.,\n2014a; Bowman et al., 2014; Bowman, 2013), which is close in spirit to our work. Computer pro-\ngrams are more complex than mathematical or logical expressions because it is possible to simulate\neither with an appropriate computer program.\nFrom a methodological perspective, we formulate the program evaluation task as a sequence-\nto-sequence learning problem with a recurrent neural network (Sutskever et al., 2014) (see also\n(Mikolov, 2012; Sutskever, 2013; Pascanu et al., 2013)). Other interesting applications of recurrent\nneural networks include speech recognition (Robinson et al., 1996; Graves et al., 2013), machine\ntranslation (Cho et al., 2014; Sutskever et al., 2014), handwriting recognition (Pham et al., 2013;\nZaremba et al., 2014b), and many more.\nMaddison & Tarlow (2014) trained a language model of program text, and Mou et al. (2014) used a\nneural network to determine whether two programs are equivalent. Both of these approaches require\nthe parse trees of programs, while the input to our model is a string of character representing our\nprogram.\nPredicting program output requires that the model deals with long term dependencies that arise\nfrom variable assignment. For this reason, we chose to use the Long Short-Term Memory model\n(Hochreiter & Schmidhuber, 1997), although there are many other RNN variants that perform well\non tasks with long term dependencies (Cho et al., 2014; Jaeger et al., 2007; Koutn\u00b4\u0131k et al., 2014;\nMartens, 2010; Bengio et al., 2013).\nInitially, we found it dif\ufb01cult to train LSTMs to accurately evaluate programs. The compositional\nnature of computer programs suggests that the LSTM would learn faster if we \ufb01rst taught it about the\nindividual operators and how to combine them. This approach can be implemented with curriculum\nlearning (Bengio et al., 2009; Kumar et al., 2010; Lee & Grauman, 2011), which prescribes to grad-\nually increase the \u201cdif\ufb01culty level\u201d of the examples presented to the LSTM. It is partially motivated\nby fact that humans and animals learn much faster when they are given hard but manageable tasks.\nUnfortunately, we found the naive curriculum learning strategy of Bengio et al. (2009) to sometimes\nbe harmful. One of our key contributions is the formulation of a new curriculum learning strategy\nthat substantially improves the speed and the quality of training in every experimental setting that\nwe considered.\n3\nPROGRAM SUBCLASS\nWe train RNNs on the class of short programs that can be evaluated in O (n) time and constant\nmemory. This restriction is dictated by the computational structure of the RNN itself, as it can only\n2\nUnder review as a conference paper at ICLR 2015\nInput:\nvqppkn\nsqdvfljmnc\ny2vxdddsepnimcbvubkomhrpliibtwztbljipcc\nTarget: hkhpg\nFigure 2: A sample program with its outputs when the characters are scrambled. It helps illustrate\nthe dif\ufb01culty faced by our neural network.\nperform a single pass over the program and its memory is limited. Our programs use the Python\nsyntax and are constructed from a small number of operations and their compositions (nesting).\nWe allow the following operations: addition, subtraction, multiplication, variable assignments, if-\nstatements, and for-loops, but we forbid double loops. Every program ends with a single \u201cprint\u201d\nstatement whose output is an integer. Two example programs are shown in Figure 1.\nWe select our programs from a family of distributions parametrized by their length and nesting. The\nlength parameter is the number of digits in the integers that appear in the programs (so the integers\nare chosen uniformly from [1, 10length]). The appendix presents the pseudocode 1 of the algorithm\nused to generate our programs. For example, two programs that are generated with length = 4 and\nnesting = 3 are shown in Figure 1.\nWe impose restrictions on the operands of multiplication and on the ranges of for-loop, since they\npose a greater dif\ufb01culty to our model. We constrain one of the arguments of multiplication and the\nrange of for-loops to be chosen uniformly from the much smaller range [1, 4\u00b7 length]. We do so since\nour models are able to perform linear-time computation while generic integer multiplication requires\nsuperlinear time. Similar considerations apply to for-loops, since nested for-loops can implement\ninteger multiplication.\nThe nesting parameter is the number of times we are allowed to combine the operations with each\nother. Higher values of nesting yield programs with deeper parse trees. Nesting makes the task much\nharder for the LSTMs, because they do not have a natural way of dealing with compositionality,\nunlike Tree Neural Networks. It is surprising that the LSTMs can handle nested expressions at all.\nThe programs also do not receive an external input.\nIt is important to emphasize that the LSTM reads the entire input one character at a time and pro-\nduces the output one character at a time. The characters are initially meaningless from the model\u2019s\nperspective; for instance, the model does not know that \u201c+\u201d means addition or that 6 is followed\nby 7. In fact, scrambling the input characters (e.g., replacing \u201ca\u201d with \u201cq\u201d, \u201cb\u201d with \u201cw\u201d, etc.,) has\nno effect on the model\u2019s ability to solve this problem. We demonstrate the dif\ufb01culty of the task by\npresenting an input-output example with scrambled characters in Figure 2.\nFinally, we wanted to verify that our program are not trivial to evaluate, by ensuring that the bias\ncoming from Benford\u2019s law (Hill, 1995) is not too strong. Our setup has 12 possible output char-\nacters, that is 10 digits, the end of sequence character, and minus. Their output distribution is not\nuniform, which can be seen by noticing that the minus sign and the dot do not occur with the same\nfrequency as the other digits. If we assume that the output characters are independent, the probabil-\nity of guessing the correct character is \u223c8.3%. The most common character is 1 which occurs with\nprobability 12.7% over the entire output.\nHowever, there is a bias in the distribution of the \ufb01rst character. There are 11 possible choices, which\ncan be randomly guessed with a probability of 9%. The most common character is 1, and it occurs\nwith a probability 20.3% in its \ufb01rst position, indicating a strong bias. Still, this value is far below\nour model prediction accuracy. Moreover, the most probable second character in the \ufb01rst position of\nthe output occurs with probability 12.6%, which is indistinguishable from probability distribution\nof digits in the other positions. The last character is always the end of sequence. The most common\ndigit prior to the last character is 4, and it occures with probability 10.3%. These statistics are\ncomputed with 10000 randomly generated programs with length = 4 and nesting = 1. The\nabsence of a strong bias for this con\ufb01guration suggests that there will be even less bias in with\ngreater nesting and longer digits, which we have also con\ufb01rmed numerically.\n3\nUnder review as a conference paper at ICLR 2015\nInput:\nprint(398345+425098)\nTarget: 823443\nFigure 3: A typical data sample for the addition task.\n3.1\nADDITION TASK\nIt is dif\ufb01cult to intuitively assess the accuracy of an LSTM on a program evaluation task. For\nexample, it is not clear whether an accuracy of 50% is impressive. Thus, we also evaluate our models\non a more familiar addition task, where the dif\ufb01culty is measured by the length of the inputs. We\nconsider the addition of only two numbers of the same length (Figure 3) that are chosen uniformly\nfrom [1, 10length]. Adding two number of the same length is simpler than adding variable length\nnumbers. Model doesn\u2019t need to align them.\n3.2\nMEMORIZATION TASK\nIn addition to program evaluation and addition, we also investigate the task of memorizing a random\nsequence of numbers. Given an example input 123456789, the LSTM reads it one character at a\ntime, stores it in memory, and then outputs 123456789 one character at a time. We present and\nexplore two simple performance enhancing techniques: input reversing Sutskever et al. (2014) and\ninput doubling.\nThe idea of input reversing is to reverse the order of the input (987654321) while keeping the de-\nsired output unchanged (123456789). It may appear to be a neutral operation because the average\ndistance between each input and its corresponding target does not change. However, input reversing\nintroduces many short term dependencies that make it easier for the LSTM to learn to make correct\npredictions. This strategy was \ufb01rst introduced by Sutskever et al. (2014).\nThe second performance enhancing technique is input doubling, where we present the input se-\nquence twice (so the example input becomes 123456789; 123456789), while the output remains\nunchanged (123456789). This method is meaningless from a probabilistic perspective as RNNs ap-\nproximate the conditional distribution p(y|x), yet here we attempt to learn p(y|x, x). Still, it gives\nnoticeable performance improvements. By processing the input several times before producing the\noutput, the LSTM is given the opportunity to correct any mistakes or omissions it made before.\n4\nCURRICULUM LEARNING\nOur program generation procedure is parametrized by length and nesting. These two parameters\nallow us control the complexity of the program. When length and nesting are large enough, the\nlearning problem becomes nearly intractable. This indicates that in order to learn to evaluate pro-\ngrams of a given length = a and nesting = b, it may help to \ufb01rst learn to evaluate programs with\nlength \u226aa and nesting \u226ab. We evaluate the following curriculum learning strategies:\nNo curriculum learning (baseline) The baseline approach does not use curriculum learning. This\nmeans that we generate all the training samples with length = a and nesting = b. This strategy is the\nmost \u201csound\u201d from statistical perspective, since it is generally recommended to make the training\ndistribution identical to test distribution.\nNaive curriculum strategy (naive) We begin with length = 1 and nesting = 1. Once learning\nstops making progress on the validation set, we increase length by 1. We repeat this process until\nits length reaches a, in which case we increase nesting by one and reset length to 1. We can also\nchoose to \ufb01rst increase nesting and then length. However, it does not make a noticeable difference in\nperformance. We skip this option in the rest of paper, and increase length \ufb01rst in all our experiments.\nThis strategy is has been examined in previous work on curriculum learning (Bengio et al., 2009).\nHowever, we show that sometimes it gives even worse performance than baseline.\nMixed strategy (mix) To generate a random sample, we \ufb01rst pick a random length from [1, a] and\na random nesting from [1, b] independently for every sample. The Mixed strategy uses a balanced\n4\nUnder review as a conference paper at ICLR 2015\nmixture of easy and dif\ufb01cult examples, so at every point during training, a sizable fraction of the\ntraining samples will have the appropriate dif\ufb01culty for the LSTM.\nCombining the mixed strategy with naive curriculum strategy (combined) This strategy com-\nbines the mix strategy with the naive strategy. In this approach, every training case is obtained either\nby the naive strategy or by the mix strategy. As a result, the combined strategy always exposes the\nnetwork at least to some dif\ufb01cult examples, which is the key way in which it differs from the naive\ncurriculum strategy. We noticed that it always outperformed the naive strategy and would generally\n(but not always) outperform the mix strategy. We explain why our new curriculum learning strategies\noutperform the naive curriculum strategy in Section 7.\nWe evaluate these four strategies on the program evaluation task (Section 6.1) and on the memoriza-\ntion task (Section 6.3).\n5\nLSTM\nIn this section we brie\ufb02y describe the deep LSTM (Section 5). All vectors are n-dimensional unless\nexplicitly stated otherwise. Let hl\nt \u2208Rn be a hidden state in layer l in timestep t. Let Tn,m : Rn \u2192\nRm be a biased linear mapping (x \u2192Wx + b for some W and b). We let \u2299be element-wise\nmultiplication and let h0\nt be the input to the deep LSTM at timestep t. We use the activations at the\ntop layer L (namely hL\nt ) to predict yt where L is the depth of our LSTM.\nThe structure of the LSTM allows it to train on problems with long term dependencies relatively\neasily. The \u201clong term\u201d memory is stored in a vector of memory cells cl\nt \u2208Rn. Although many\nLSTM architectures differ slightly in their connectivity structure and activation functions, all LSTM\narchitectures have additive memory cells that make it easy to learn to store information for long\nperiods of time. We used an LSTM described by the following equations (from Graves et al. (2013)):\nLSTM : hl\u22121\nt\n, hl\nt\u22121, cl\nt\u22121 \u2192hl\nt, cl\nt\n\uf8eb\n\uf8ec\n\uf8ed\ni\nf\no\ng\n\uf8f6\n\uf8f7\n\uf8f8=\n\uf8eb\n\uf8ec\n\uf8ed\nsigm\nsigm\nsigm\ntanh\n\uf8f6\n\uf8f7\n\uf8f8T2n,4n\n\u0012\nhl\u22121\nt\nhl\nt\u22121\n\u0013\ncl\nt = f \u2299cl\nt\u22121 + i \u2299g\nhl\nt = o \u2299tanh(cl\nt)\n6\nEXPERIMENTS\nIn this section, we report the results of our curriculum learning strategies on the program evaluation\nand memorization tasks. In both experiments, we used the same LSTM architecture.\nOur LSTM has two layers and is unrolled for 50 steps in both experiments. It has 400 cells per layer\nand its parameters are initialized uniformly in [\u22120.08, 0.08]. This gives total \u223c2.5M parameters.\nWe initialize the hidden states to zero. We then use the \ufb01nal hidden states of the current minibatch\nas the initial hidden state of the subsequent minibatch. Thus it is possible that a program and its\noutput could be separated across different minibatches. The size of minibatch is 100. We constrain\nthe norm of the gradients (normalized by minibatch size) to be no greater than 5 (Mikolov et al.,\n2010). We keep the learning rate equal to 0.5 until we reach the target length and nesting (we only\nvary the length, i.e., the number of digits, in the memorization task).\nAfter reaching the target accuracy (95%) we decrease the learning rate by 0.8. We keep the learning\nrate on the same level until there is no improvement on the training set. We decrease it again, when\nthere is no improvement on training set. The only difference between experiments is the termination\ncriteria. For the program output prediction, we stop when learning rate becomes smaller than 0.001.\nFor copying task, we stop training after 20 epochs, where each epoch has 0.5M samples.\nWe begin training with length = 1 and nesting = 1 (or length=1 for the memorization task). We\nensure that the training, validation, and test sets are disjoint. It is achieved computing the hash value\nof each sample and taking it modulo 3.\nImportant note on error rates: We use teacher forcing when we compute the accuracy of our\nLSTMs. That is, when predicting the i-th digit of the target, the LSTM is provided with the correct\n5\nUnder review as a conference paper at ICLR 2015\n\ufb01rst i \u22121 digits of the target. This is different from using the LSTM to generate the entire output\non its own, as done by Sutskever et al. (2014), which would almost surely result in lower numerical\naccuracies. To help make intuitive sense of our results, we present a large number of test cases and\nthe outputs computed by the LSTM, albeit with teacher forcing.\n6.1\nRESULTS ON PROGRAM EVALUATION\nWe train our LSTMs using the four strategies described in Section 4:\n\u2022 No curriculum learning (baseline),\n\u2022 Naive curriculum strategy (naive)\n\u2022 Mixed strategy (mix), and\n\u2022 Combined strategy (combined).\nFigure 4 shows the absolute performance of the baseline strategy (training on the original target\ndistribution), and of the best performing strategy, combined. Moreover, Figure 5 shows the perfor-\nmance of the three curriculum strategies relative to baseline. Finally, we provide several example\npredictions on test data in the supplementary materials. The accuracy of a random predictor would\nbe \u223c8.3%, since there are 12 possible output symbols.\nFigure 4: Absolute prediction accuracy of the baseline strategy and of the combined strategy (see\nSection 4) on the program evaluation task. Deeper nesting and longer integers make the task more\ndif\ufb01cult. Overall, the combined strategy outperformed the baseline strategy in every setting.\nFigure 5: Relative prediction accuracy of the different strategies with respect to the baseline strategy.\nThe Naive curriculum strategy was found to sometime perform worse than baseline. A possible\nexplanation is provided in Section 7. The combined strategy outperforms all other strategies in\nevery con\ufb01guration on program evaluation.\n6.2\nRESULTS ON THE ADDITION TASK\nFigure 6 presents the accuracy achieved by the LSTM with the various curriculum strategies on\nthe addition task. Remarkably, the combined curriculum strategy resulted in 99% accuracy on the\naddition of 9-digit long numbers, which is a massive improvement over the naive curriculum.\n6.3\nRESULTS ON THE MEMORIZATION TASK\nRecall that the goal of the memorization task is to read a sequence of digits into the hidden state and\nthen to reconstruct it from the hidden state. Namely, given an input such as 123456789, the goal is\n6\nUnder review as a conference paper at ICLR 2015\nFigure 6: The effect of curriculum strategies on the addition task.\nFigure 7: Prediction accuracy on the memorization task for the four curriculum strategies. The input\nlength ranges from 5 to 65 digits. Every strategy is evaluated with the following 4 input modi\ufb01cation\nschemes: no modi\ufb01cation; input inversion; input doubling; and input doubling and inversion. The\ntraining time was not limited; the network was trained till convergence.\nto produce the output 123456789. The model processes the input one input character at the time and\nhas to reconstruct the output only after loading the entire input into its memory. This task provides\ninsight into the LSTM\u2019s ability to learn to remember. We have evaluated our model on sequences\nof lengths ranging from 5 to 65. We use the four curriculum strategies of Section 4. In addition, we\ninvestigate two strategies to modify the input which increase performance:\n\u2022 Inverting input (Sutskever et al., 2014)\n\u2022 Doubling Input\nBoth strategies are described in Section 3.2. Figure 7 shows the absolute performance of the baseline\nstrategy and of the combined strategy. This Figure shows the performance at convergence. We\nfurther present in Supplementary material (Section 9) results after 20 epochs (Figure 8).\nFor this task, the combined strategy no longer outperforms the mixed strategy in every experimental\nsetting, although both strategies are always better than using no curriculum and the naive curriculum\nstrategy. Each graph contains 4 settings, which correspond to the possible combinations of input in-\nversion and input doubling. The result clearly shows that the simultaneously doubling and reversing\nthe input achieves the best results. Random guessing would achieve an accuracy of \u223c9%, since\nthere are 11 possible output symbols.\n7\nHIDDEN STATE ALLOCATION HYPOTHESIS\nOur experimental results suggest that a proper curriculum learning strategy is critical for achieving\ngood performance on very hard problems where conventional stochastic gradient descent (SGD)\n7\nUnder review as a conference paper at ICLR 2015\nperforms poorly. The results on both of our problems (Sections 6.3 and 6.1) show that the combined\nstrategy is better than all other curriculum strategies, including both naive curriculum learning, and\ntraining on the target distribution. We have a plausible explanation for why this is the case.\nIt seems natural to train models with examples of increasing dif\ufb01culty. This way the models have\na chance to learn the correct intermediate concepts, and then utilize them for the more dif\ufb01cult\nproblem instances. Otherwise, learning the full task might be just too dif\ufb01cult for SGD from a\nrandom initialization. This explanation has been proposed in previous work on curriculum learning\nBengio et al. (2009). However, based the on empirical results, the naive strategy of curriculum\nlearning can sometimes be worse than learning with the target distribution.\nIn our tasks, the neural network has to perform a lot of memorization. The easier examples usually\nrequire less memorization than the hard examples. For instance, in order to add two 5-digit numbers,\none has to remember at least 5 digits before producing any output. The best way to accurately\nmemorize 5 numbers could be to spread them over the entire hidden state / memory cell (i.e., use\na distributed representation). Indeed, the network has no incentive to utilize only a fraction of\nits state, and it is always better to make use of its entire memory capacity. This implies that the\nharder examples would require a restructuring of its memory patterns. It would need to contract its\nrepresentations of 5 digit numbers in order to free space for the 6-th number. This process of memory\npattern restructuring might be dif\ufb01cult to implement, so it could be the reason for the sometimes poor\nperformance of the naive curriculum learning strategy relative to baseline.\nThe combined strategy reduces the need to restructure the memory patterns. The combined strategy\nis a combination of the naive curriculum strategy and of the mix strategy, which is a mixture of ex-\namples of all dif\ufb01culties. The examples produced by the naive curriculum strategy help to learn the\nintermediate input-output mapping, which is useful for solving the target task, while the extra sam-\nples from the mix strategy prevent the network from utilizing all the memory on the easy examples,\nthus eliminating the need to restructure its memory patterns.\n8\nCRITIQUE\nPerfect prediction of program output requires a complete understanding of all operands and con-\ncepts, and of the precise way in which they are combined. However, imperfect prediction might be\nachieved in a multitude of ways, and could heavily rely on memorization, without a genuine un-\nderstanding of the underlying concepts. For instance, perfect addition is relatively intricate, as the\nLSTM needs to know the order of numbers and to correctly compute the carry.\nThere are many alternatives to the addition algorithm if perfect output is not required. For instance,\none can perform element-wise addition, and as long as there is no carry then the output would be\nperfectly correct. Another alternative, which requires more memory, but is also more simpler, is to\nmemorize all results of addition for 2 digit numbers. Then multi-digit addition can be broken down\nto multiple 2-digits additions element-wise. Once again, such an algorithm would have a reasonably\nhigh prediction accuracy, although it would be far from correct.\nWe do not know how heavily our model relies on memorization and how far the learned algorithm\nis from the actual, correct algorithm. This could be tested by creating a big discrepancy between the\ntraining and test data, but in this work, the training and the test distributions are the same. We plan\nto examine how well our models would generalize on very different new examples in future work.\n9\nDISCUSSION\nWe have shown that it is possible to learn to evaluate programs with limited prior knowledge. This\nwork demonstrate the power and expressiveness of sequence-to-sequence LSTMs. We also showed\nthat correct curriculum learning is crucial for achieving good results on very dif\ufb01cult tasks that\ncannot be optimized with standard SGD. We also found that the general method of doubling the\ninput reliably improves the performance of sequence-to-sequence LSTMs.\nOur results are encouraging but they leave many questions open. For example, we are not able to\nevaluate arbitrary programs (e.g., ones that run in more than O (n) time). This cannot be achieved\nwith conventional RNNs or LSTMs due to their runtime restrictions. We also do not know the\n8\nUnder review as a conference paper at ICLR 2015\noptimal curriculum learning strategy. To understand it, it may be necessary to identify the training\nsamples that are most bene\ufb01cial to the model.\n10\nACKNOWLEDGMENTS\nWe wish to thank Oriol Vinyals for useful discussions, and to Koray Kavukcuoglu for help during code develop-\nment. Moreover, we wish to acknowledge Marc\u2019Aurelio Ranzato for useful comments on the \ufb01rst version of the\npaper. Some chunks of our code origin from Google Deepmind repository. We thank to unknown developers\nof LSTM function, and auxiliary functions.\nREFERENCES\nBengio, Yoshua, Louradour, J\u00b4er\u02c6ome, Collobert, Ronan, and Weston, Jason. Curriculum learning. In Proceed-\nings of the 26th annual international conference on machine learning, pp. 41\u201348. ACM, 2009.\nBengio, Yoshua, Boulanger-Lewandowski, Nicolas, and Pascanu, Razvan. Advances in optimizing recurrent\nnetworks. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on,\npp. 8624\u20138628. IEEE, 2013.\nBowman, Samuel R.\nCan recursive neural tensor networks learn logical reasoning?\narXiv preprint\narXiv:1312.6192, 2013.\nBowman, Samuel R, Potts, Christopher, and Manning, Christopher D. Recursive neural networks for learning\nlogical semantics. arXiv preprint arXiv:1406.1827, 2014.\nCho, Kyunghyun, van Merrienboer, Bart, Gulcehre, Caglar, Bougares, Fethi, Schwenk, Holger, and Bengio,\nYoshua. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv\npreprint arXiv:1406.1078, 2014.\nGraves, Alex, Mohamed, Abdel-rahman, and Hinton, Geoffrey. Speech recognition with deep recurrent neural\nnetworks. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on,\npp. 6645\u20136649. IEEE, 2013.\nHill, Theodore P. A statistical derivation of the signi\ufb01cant-digit law. Statistical Science, pp. 354\u2013363, 1995.\nHochreiter, Sepp and Schmidhuber, J\u00a8urgen. Long short-term memory. Neural computation, 9(8):1735\u20131780,\n1997.\nJaeger, Herbert, Luko\u02c7sevi\u02c7cius, Mantas, Popovici, Dan, and Siewert, Udo. Optimization and applications of\necho state networks with leaky-integrator neurons. Neural Networks, 20(3):335\u2013352, 2007.\nKoutn\u00b4\u0131k, Jan, Greff, Klaus, Gomez, Faustino, and Schmidhuber, J\u00a8urgen. A clockwork rnn. arXiv preprint\narXiv:1402.3511, 2014.\nKumar, M Pawan, Packer, Benjamin, and Koller, Daphne. Self-paced learning for latent variable models. In\nAdvances in Neural Information Processing Systems, pp. 1189\u20131197, 2010.\nLee, Yong Jae and Grauman, Kristen. Learning the easy things \ufb01rst: Self-paced visual category discovery. In\nComputer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pp. 1721\u20131728. IEEE, 2011.\nMaddison, Chris J and Tarlow, Daniel. Structured generative models of natural source code. arXiv preprint\narXiv:1401.0514, 2014.\nMartens, James. Deep learning via hessian-free optimization. In Proceedings of the 27th International Confer-\nence on Machine Learning (ICML-10), pp. 735\u2013742, 2010.\nMikolov, Tom\u00b4a\u02c7s. Statistical language models based on neural networks. PhD thesis, Ph. D. thesis, Brno\nUniversity of Technology, 2012.\nMikolov, Tomas, Kara\ufb01\u00b4at, Martin, Burget, Lukas, Cernock`y, Jan, and Khudanpur, Sanjeev. Recurrent neural\nnetwork based language model. In INTERSPEECH, pp. 1045\u20131048, 2010.\nMou, Lili, Li, Ge, Liu, Yuxuan, Peng, Hao, Jin, Zhi, Xu, Yan, and Zhang, Lu. Building program vector\nrepresentations for deep learning. arXiv preprint arXiv:1409.3358, 2014.\nPascanu, Razvan, Gulcehre, Caglar, Cho, Kyunghyun, and Bengio, Yoshua. How to construct deep recurrent\nneural networks. arXiv preprint arXiv:1312.6026, 2013.\nPham, Vu, Kermorvant, Christopher, and Louradour, J\u00b4er\u02c6ome. Dropout improves recurrent neural networks for\nhandwriting recognition. arXiv preprint arXiv:1312.4569, 2013.\nRobinson, Tony, Hochberg, Mike, and Renals, Steve. The use of recurrent neural networks in continuous speech\nrecognition. In Automatic speech and speaker recognition, pp. 233\u2013258. Springer, 1996.\nSutskever, Ilya. Training Recurrent Neural Networks. PhD thesis, University of Toronto, 2013.\nSutskever, Ilya, Vinyals, Oriol, and Le, Quoc V. Sequence to sequence learning with neural networks. arXiv\npreprint arXiv:1409.3215, 2014.\nZaremba, Wojciech, Kurach, Karol, and Fergus, Rob. Learning to discover ef\ufb01cient mathematical identities.\narXiv preprint arXiv:1406.1584, 2014a.\nZaremba, Wojciech, Sutskever, Ilya, and Vinyals, Oriol.\nRecurrent neural network regularization.\narXiv\npreprint arXiv:1409.2329, 2014b.\n9\nUnder review as a conference paper at ICLR 2015\nSUPPLEMENTARY MATERIAL\nInput:\nlength, nesting\nstack = EmptyStack()\nOperations = Addition, Subtraction, Multiplication, If-Statement,\nFor-Loop, Variable Assignment\nfor i = 1 to nesting do\nOperation = a random operation from Operations\nValues = List\nCode = List\nfor params in Operation.params do\nif not empty stack and Uniform(1) > 0.5 then\nvalue, code = stack.pop()\nelse\nvalue = random.int(10length)\ncode = toString(value)\nend if\nvalues.append(value)\ncode.append(code)\nend for\nnew value= Operation.evaluate(values)\nnew code = Operation.generate code(codes)\nstack.push((new value, new code))\nend for\nfinal value, final code = stack.pop()\ndatasets = training, validation, testing\nidx = hash(final code) modulo 3\ndatasets[idx].add((final value, final code))\nAlgorithm 1: Pseudocode of the algorithm used to generate the distribution over the python pro-\ngram. Programs produced by this algorithm are guaranteed to never have dead code. The type of the\nsample (train, test, or validation) is determined by its hash modulo 3.\n11\nADDITIONAL RESULTS ON THE MEMORIZATION PROBLEM\nWe present the algorithm for generating the training cases, and present an extensive qualitative evaluation of\nthe samples and the kinds of predictions made by the trained LSTMs.\nWe emphasize that these predictions rely on teacher forcing. That is, even if the LSTM made an incorrect\nprediction in the i-th output digit, the LSTM will be provided as input the correct i-th output digit for predicting\nthe i + 1-th digit. While teacher forcing has no effect whenever the LSTM makes no errors at all, a sample that\nmakes an early error and gets the remainder of the digits correctly needs to be interpreted with care.\n12\nQUALITATIVE EVALUATION OF THE CURRICULUM STRATEGIES\n12.1\nEXAMPLES OF PROGRAM EVALUATION PREDICTION. LENGTH = 4, NESTING = 1\nInput:\nprint(6652).\nTarget:\n6652.\n\u201dBaseline\u201d prediction:\n6652.\n\u201dNaive\u201d prediction:\n6652.\n\u201dMix\u201d prediction:\n6652.\n\u201dCombined\u201d prediction:\n6652.\nInput:\n10\nUnder review as a conference paper at ICLR 2015\nFigure 8: Prediction accuracy on the memorization task for the four curriculum strategies. The input\nlength ranges from 5 to 65 digits. Every strategy is evaluated with the following 4 input modi\ufb01cation\nschemes: no modi\ufb01cation; input inversion; input doubling; and input doubling and inversion. The\ntraining time is limited to 20 epochs.\nprint((5997-738)).\nTarget:\n5259.\n\u201dBaseline\u201d prediction:\n5101.\n\u201dNaive\u201d prediction:\n5101.\n\u201dMix\u201d prediction:\n5249.\n\u201dCombined\u201d prediction:\n5229.\nInput:\nprint((16*3071)).\nTarget:\n49136.\n\u201dBaseline\u201d prediction:\n49336.\n\u201dNaive\u201d prediction:\n48676.\n\u201dMix\u201d prediction:\n57026.\n\u201dCombined\u201d prediction:\n49626.\nInput:\nc=2060;\nprint((c-4387)).\nTarget:\n-2327.\n\u201dBaseline\u201d prediction:\n-2320.\n\u201dNaive\u201d prediction:\n-2201.\n\u201dMix\u201d prediction:\n-2377.\n\u201dCombined\u201d prediction:\n-2317.\nInput:\nprint((2*5172)).\n11\nUnder review as a conference paper at ICLR 2015\nTarget:\n10344.\n\u201dBaseline\u201d prediction:\n10344.\n\u201dNaive\u201d prediction:\n10324.\n\u201dMix\u201d prediction:\n10344.\n\u201dCombined\u201d prediction:\n10344.\nInput:\nprint((9891-4715)).\nTarget:\n5176.\n\u201dBaseline\u201d prediction:\n5196.\n\u201dNaive\u201d prediction:\n5104.\n\u201dMix\u201d prediction:\n4246.\n\u201dCombined\u201d prediction:\n5196.\nInput:\nprint(4849).\nTarget:\n4849.\n\u201dBaseline\u201d prediction:\n4849.\n\u201dNaive\u201d prediction:\n4849.\n\u201dMix\u201d prediction:\n4849.\n\u201dCombined\u201d prediction:\n4849.\nInput:\nprint((4*7054)).\nTarget:\n28216.\n\u201dBaseline\u201d prediction:\n28216.\n\u201dNaive\u201d prediction:\n28116.\n\u201dMix\u201d prediction:\n28216.\n\u201dCombined\u201d prediction:\n28216.\nInput:\nprint((4635-5257)).\nTarget:\n-622.\n\u201dBaseline\u201d prediction:\n-688.\n\u201dNaive\u201d prediction:\n-628.\n\u201dMix\u201d prediction:\n-692.\n\u201dCombined\u201d prediction:\n-632.\nInput:\ne=1079\nfor x in range(10):e+=4729\nprint(e).\nTarget:\n48369.\n\u201dBaseline\u201d prediction:\n48017.\n\u201dNaive\u201d prediction:\n48011.\n\u201dMix\u201d prediction:\n48101.\n\u201dCombined\u201d prediction:\n48009.\n12.2\nEXAMPLES OF PROGRAM EVALUATION PREDICTION. LENGTH = 4, NESTING = 2\nInput:\n12\nUnder review as a conference paper at ICLR 2015\ne=6653\nfor x in range(14):e+=6311\nprint(e).\nTarget:\n95007.\n\u201dBaseline\u201d prediction:\n94093.\n\u201dNaive\u201d prediction:\n90013.\n\u201dMix\u201d prediction:\n95015.\n\u201dCombined\u201d prediction:\n94103.\nInput:\ni=6404;\nprint((i+8074)).\nTarget:\n14478.\n\u201dBaseline\u201d prediction:\n14498.\n\u201dNaive\u201d prediction:\n14444.\n\u201dMix\u201d prediction:\n14482.\n\u201dCombined\u201d prediction:\n14478.\nInput:\nprint((8*(5051-648))).\nTarget:\n35224.\n\u201dBaseline\u201d prediction:\n34044.\n\u201dNaive\u201d prediction:\n32180.\n\u201dMix\u201d prediction:\n33284.\n\u201dCombined\u201d prediction:\n33004.\nInput:\nh=(3681 if 9279<3033 else 6191)\nfor x in range(7):h-=9910\nprint(h).\nTarget:\n-63179.\n\u201dBaseline\u201d prediction:\n-62049.\n\u201dNaive\u201d prediction:\n-63117.\n\u201dMix\u201d prediction:\n-62013.\n\u201dCombined\u201d prediction:\n-62009.\nInput:\nprint(((3210+2472)+1477)).\nTarget:\n7159.\n\u201dBaseline\u201d prediction:\n7009.\n\u201dNaive\u201d prediction:\n7019.\n\u201dMix\u201d prediction:\n7995.\n\u201dCombined\u201d prediction:\n7079.\nInput:\nb=8494\nfor x in range(2):b+=7484\nprint((b*14)).\nTarget:\n328468.\n\u201dBaseline\u201d prediction:\n318004.\n\u201dNaive\u201d prediction:\n338088.\n\u201dMix\u201d prediction:\n329220.\n\u201dCombined\u201d prediction:\n338080.\n13\nUnder review as a conference paper at ICLR 2015\nInput:\nj=6447;\nprint((12*(j-4689))).\nTarget:\n21096.\n\u201dBaseline\u201d prediction:\n21266.\n\u201dNaive\u201d prediction:\n10046.\n\u201dMix\u201d prediction:\n10606.\n\u201dCombined\u201d prediction:\n20402.\nInput:\nprint((13*9201)).\nTarget:\n119613.\n\u201dBaseline\u201d prediction:\n118313.\n\u201dNaive\u201d prediction:\n118011.\n\u201dMix\u201d prediction:\n117669.\n\u201dCombined\u201d prediction:\n119533.\nInput:\ng=1054;\nprint((6028+(g-1953))).\nTarget:\n5129.\n\u201dBaseline\u201d prediction:\n4013.\n\u201dNaive\u201d prediction:\n5035.\n\u201dMix\u201d prediction:\n4015.\n\u201dCombined\u201d prediction:\n4009.\nInput:\nd=6817\nfor x in range(7):d-=(4581-2186)\nprint(d).\nTarget:\n-9948.\n\u201dBaseline\u201d prediction:\n-1996.\n\u201dNaive\u201d prediction:\n-1610.\n\u201dMix\u201d prediction:\n-1882.\n\u201dCombined\u201d prediction:\n-1980.\n12.3\nEXAMPLES OF PROGRAM EVALUATION PREDICTION. LENGTH = 4, NESTING = 3\nInput:\nf=4692\nfor x in range(4):f-=1664\nj=1443\nfor x in range(8):j+=f\nd=j\nfor x in range(11):d-=4699\nprint(d).\nTarget:\n-65958.\n\u201dBaseline\u201d prediction:\n-13262.\n\u201dNaive\u201d prediction:\n-73194.\n\u201dMix\u201d prediction:\n-40188.\n\u201dCombined\u201d prediction:\n-12004.\n14\nUnder review as a conference paper at ICLR 2015\nInput:\nb=9930\nfor x in range(11):b-=4369\ng=b;\nprint(((g-8043)+9955)).\nTarget:\n-36217.\n\u201dBaseline\u201d prediction:\n-37515.\n\u201dNaive\u201d prediction:\n-38609.\n\u201dMix\u201d prediction:\n-35893.\n\u201dCombined\u201d prediction:\n-35055.\nInput:\nd=5446\nfor x in range(8):d+=(2678 if 4803<2829 else 9848)\nprint((d if 5935<4845 else 3043)).\nTarget:\n3043.\n\u201dBaseline\u201d prediction:\n3043.\n\u201dNaive\u201d prediction:\n3043.\n\u201dMix\u201d prediction:\n3043.\n\u201dCombined\u201d prediction:\n3043.\nInput:\nprint((((2578 if 7750<1768 else 8639)-2590)+342)).\nTarget:\n6391.\n\u201dBaseline\u201d prediction:\n-555.\n\u201dNaive\u201d prediction:\n6329.\n\u201dMix\u201d prediction:\n6461.\n\u201dCombined\u201d prediction:\n6105.\nInput:\nprint((((841 if 2076<7326 else 1869)*10) if 7827<317 else 7192)).\nTarget:\n7192.\n\u201dBaseline\u201d prediction:\n7192.\n\u201dNaive\u201d prediction:\n7192.\n\u201dMix\u201d prediction:\n7192.\n\u201dCombined\u201d prediction:\n7192.\nInput:\nd=8640;\nprint((7135 if 6710>((d+7080)*14) else 7200)).\nTarget:\n7200.\n\u201dBaseline\u201d prediction:\n7200.\n\u201dNaive\u201d prediction:\n7200.\n\u201dMix\u201d prediction:\n7200.\n\u201dCombined\u201d prediction:\n7200.\nInput:\nb=6968\nfor x in range(10):b-=(299 if 3389<9977 else 203)\nprint((12*b)).\n15\nUnder review as a conference paper at ICLR 2015\nTarget:\n47736.\n\u201dBaseline\u201d prediction:\n-0666.\n\u201dNaive\u201d prediction:\n11262.\n\u201dMix\u201d prediction:\n48666.\n\u201dCombined\u201d prediction:\n48766.\nInput:\nj=(1*5057);\nprint(((j+1215)+6931)).\nTarget:\n13203.\n\u201dBaseline\u201d prediction:\n13015.\n\u201dNaive\u201d prediction:\n12007.\n\u201dMix\u201d prediction:\n13379.\n\u201dCombined\u201d prediction:\n13205.\nInput:\nprint(((1090-3305)+9466)).\nTarget:\n7251.\n\u201dBaseline\u201d prediction:\n7111.\n\u201dNaive\u201d prediction:\n7099.\n\u201dMix\u201d prediction:\n7595.\n\u201dCombined\u201d prediction:\n7699.\nInput:\na=8331;\nprint((a-(15*7082))).\nTarget:\n-97899.\n\u201dBaseline\u201d prediction:\n-96991.\n\u201dNaive\u201d prediction:\n-19959.\n\u201dMix\u201d prediction:\n-95551.\n\u201dCombined\u201d prediction:\n-96397.\n12.4\nEXAMPLES OF PROGRAM EVALUATION PREDICTION. LENGTH = 6, NESTING = 1\nInput:\nprint((71647-548966)).\nTarget:\n-477319.\n\u201dBaseline\u201d prediction:\n-472122.\n\u201dNaive\u201d prediction:\n-477591.\n\u201dMix\u201d prediction:\n-479705.\n\u201dCombined\u201d prediction:\n-475009.\nInput:\nprint(1508).\nTarget:\n1508.\n\u201dBaseline\u201d prediction:\n1508.\n\u201dNaive\u201d prediction:\n1508.\n\u201dMix\u201d prediction:\n1508.\n\u201dCombined\u201d prediction:\n1508.\nInput:\n16\nUnder review as a conference paper at ICLR 2015\nj=611989;\nprint((j+763864)).\nTarget:\n1375853.\n\u201dBaseline\u201d prediction:\n1379920.\n\u201dNaive\u201d prediction:\n1378991.\n\u201dMix\u201d prediction:\n1375119.\n\u201dCombined\u201d prediction:\n1375173.\nInput:\nprint((151108 if 289653>33296 else 564130)).\nTarget:\n151108.\n\u201dBaseline\u201d prediction:\n154973.\n\u201dNaive\u201d prediction:\n151108.\n\u201dMix\u201d prediction:\n151108.\n\u201dCombined\u201d prediction:\n151108.\nInput:\nc=142012\nfor x in range(12):c-=166776\nprint(c).\nTarget:\n-1859300.\n\u201dBaseline\u201d prediction:\n-1840831.\n\u201dNaive\u201d prediction:\n-1840000.\n\u201dMix\u201d prediction:\n-1979720.\n\u201dCombined\u201d prediction:\n-1820700.\nInput:\nprint((678740+203140)).\nTarget:\n881880.\n\u201dBaseline\u201d prediction:\n880475.\n\u201dNaive\u201d prediction:\n881666.\n\u201dMix\u201d prediction:\n880190.\n\u201dCombined\u201d prediction:\n885920.\nInput:\nprint((929067-75246)).\nTarget:\n853821.\n\u201dBaseline\u201d prediction:\n851233.\n\u201dNaive\u201d prediction:\n867113.\n\u201dMix\u201d prediction:\n855615.\n\u201dCombined\u201d prediction:\n853009.\nInput:\nd=960350\nfor x in range(24):d-=187946\nprint(d).\nTarget:\n-3550354.\n\u201dBaseline\u201d prediction:\n-3571998.\n\u201dNaive\u201d prediction:\n-3699993.\n\u201dMix\u201d prediction:\n-3899220.\n\u201dCombined\u201d prediction:\n-3507790.\n17\nUnder review as a conference paper at ICLR 2015\nInput:\nprint((8*786463)).\nTarget:\n6291704.\n\u201dBaseline\u201d prediction:\n6270804.\n\u201dNaive\u201d prediction:\n6271904.\n\u201dMix\u201d prediction:\n6297644.\n\u201dCombined\u201d prediction:\n6270004.\nInput:\nprint((498592-570324)).\nTarget:\n-71732.\n\u201dBaseline\u201d prediction:\n-61086.\n\u201dNaive\u201d prediction:\n-73582.\n\u201dMix\u201d prediction:\n-19000.\n\u201dCombined\u201d prediction:\n-72842.\n12.5\nEXAMPLES OF PROGRAM EVALUATION PREDICTION. LENGTH = 6, NESTING = 2\nInput:\nprint((39007+416968)).\nTarget:\n455975.\n\u201dBaseline\u201d prediction:\n559917.\n\u201dNaive\u201d prediction:\n438887.\n\u201dMix\u201d prediction:\n458993.\n\u201dCombined\u201d prediction:\n450031.\nInput:\nprint((586051+664462)).\nTarget:\n1250513.\n\u201dBaseline\u201d prediction:\n1250939.\n\u201dNaive\u201d prediction:\n1240719.\n\u201dMix\u201d prediction:\n1230881.\n\u201dCombined\u201d prediction:\n1240551.\nInput:\nprint(948950).\nTarget:\n948950.\n\u201dBaseline\u201d prediction:\n948950.\n\u201dNaive\u201d prediction:\n948950.\n\u201dMix\u201d prediction:\n948950.\n\u201dCombined\u201d prediction:\n948950.\nInput:\ni=849846\nfor x in range(15):i-=557574\nprint((362961 if 881013<597832 else i)).\n18\nUnder review as a conference paper at ICLR 2015\nTarget:\n-7513764.\n\u201dBaseline\u201d prediction:\n-7422756.\n\u201dNaive\u201d prediction:\n-7011048.\n\u201dMix\u201d prediction:\n-2617777.\n\u201dCombined\u201d prediction:\n-7101146.\nInput:\ng=977055;\nprint((g-(592222+268807))).\nTarget:\n116026.\n\u201dBaseline\u201d prediction:\n132440.\n\u201dNaive\u201d prediction:\n101488.\n\u201dMix\u201d prediction:\n114988.\n\u201dCombined\u201d prediction:\n125682.\nInput:\nprint(((17*711621) if 224989>711768 else 267900)).\nTarget:\n267900.\n\u201dBaseline\u201d prediction:\n267900.\n\u201dNaive\u201d prediction:\n267900.\n\u201dMix\u201d prediction:\n267900.\n\u201dCombined\u201d prediction:\n267900.\nInput:\nj=114940;\nprint((j+482118)).\nTarget:\n597058.\n\u201dBaseline\u201d prediction:\n590006.\n\u201dNaive\u201d prediction:\n690004.\n\u201dMix\u201d prediction:\n599816.\n\u201dCombined\u201d prediction:\n599990.\nInput:\nprint((171932*19)).\nTarget:\n3266708.\n\u201dBaseline\u201d prediction:\n3249998.\n\u201dNaive\u201d prediction:\n3131798.\n\u201dMix\u201d prediction:\n3390158.\n\u201dCombined\u201d prediction:\n3100388.\nInput:\nh=411671;\nprint((242648 if (h+31605)>679390 else 449699)).\nTarget:\n449699.\n\u201dBaseline\u201d prediction:\n449699.\n\u201dNaive\u201d prediction:\n449699.\n\u201dMix\u201d prediction:\n449699.\n\u201dCombined\u201d prediction:\n449699.\nInput:\nprint(11332).\n19\nUnder review as a conference paper at ICLR 2015\nTarget:\n11332.\n\u201dBaseline\u201d prediction:\n11332.\n\u201dNaive\u201d prediction:\n11332.\n\u201dMix\u201d prediction:\n11332.\n\u201dCombined\u201d prediction:\n11332.\n12.6\nEXAMPLES OF PROGRAM EVALUATION PREDICTION. LENGTH = 6, NESTING = 3\nInput:\nc=335973;\nb=(c+756088);\nprint((6*(b+66858))).\nTarget:\n6953514.\n\u201dBaseline\u201d prediction:\n1099522.\n\u201dNaive\u201d prediction:\n7773362.\n\u201dMix\u201d prediction:\n6993124.\n\u201dCombined\u201d prediction:\n1044444.\nInput:\nc=935280;\nprint((765618 if 409621<(c-(329375 if 806201<240281 else 81797)) else\n805944)).\nTarget:\n765618.\n\u201dBaseline\u201d prediction:\n800988.\n\u201dNaive\u201d prediction:\n765644.\n\u201dMix\u201d prediction:\n765616.\n\u201dCombined\u201d prediction:\n865618.\nInput:\nprint(((670421 if 144271>805597 else 364643)*20)).\nTarget:\n7292860.\n\u201dBaseline\u201d prediction:\n1774640.\n\u201dNaive\u201d prediction:\n7134660.\n\u201dMix\u201d prediction:\n7292860.\n\u201dCombined\u201d prediction:\n7292860.\nInput:\nprint((108196 if 714126>847153 else (888873-(381812*13)))).\nTarget:\n-4074683.\n\u201dBaseline\u201d prediction:\n13205544.\n\u201dNaive\u201d prediction:\n-4011899.\n\u201dMix\u201d prediction:\n-4422909.\n\u201dCombined\u201d prediction:\n-4048381.\nInput:\nj=(181489 if 467875>46774 else (127738 if 866523<633391 else 592486))\n;\nprint((j-627483)).\n20\nUnder review as a conference paper at ICLR 2015\nTarget:\n-445994.\n\u201dBaseline\u201d prediction:\n-333153.\n\u201dNaive\u201d prediction:\n-488724.\n\u201dMix\u201d prediction:\n-440880.\n\u201dCombined\u201d prediction:\n-447944.\nInput:\nf=483654\nfor x in range(9):f-=913681\na=f\nfor x in range(12):a-=926785\nprint((124798 if a>326533 else 576599)).\nTarget:\n576599.\n\u201dBaseline\u201d prediction:\n176599.\n\u201dNaive\u201d prediction:\n576599.\n\u201dMix\u201d prediction:\n576599.\n\u201dCombined\u201d prediction:\n576599.\nInput:\nf=136315;\nh=(f+37592);\ng=418652;\nprint((g-(h+234728))).\nTarget:\n10017.\n\u201dBaseline\u201d prediction:\n12115.\n\u201dNaive\u201d prediction:\n-1123.\n\u201dMix\u201d prediction:\n-000..\n\u201dCombined\u201d prediction:\n-0033.\nInput:\na=768606\nfor x in range(11):a+=454841\nf=a\nfor x in range(3):f-=696226\nprint((340434 if f<287035 else 523084)).\nTarget:\n523084.\n\u201dBaseline\u201d prediction:\n523084.\n\u201dNaive\u201d prediction:\n523084.\n\u201dMix\u201d prediction:\n523084.\n\u201dCombined\u201d prediction:\n523084.\nInput:\nb=468503;\nprint((b-(326264+406077))).\nTarget:\n-263838.\n\u201dBaseline\u201d prediction:\n-278797.\n\u201dNaive\u201d prediction:\n-241144.\n\u201dMix\u201d prediction:\n-252080.\n\u201dCombined\u201d prediction:\n-277882.\nInput:\ng=801925;\nprint((58095+(g+(824920 if 842317>176260 else 570318)))).\n21\nUnder review as a conference paper at ICLR 2015\nTarget:\n1684940.\n\u201dBaseline\u201d prediction:\n1602221.\n\u201dNaive\u201d prediction:\n1799892.\n\u201dMix\u201d prediction:\n1677788.\n\u201dCombined\u201d prediction:\n1611888.\n12.7\nEXAMPLES OF PREDICTING RESULT OF ADDITION.\nLENGTH = 6\nInput:\nprint(284993+281178).\nTarget:\n566171.\n\u201dBaseline\u201d prediction:\n566199.\n\u201dNaive\u201d prediction:\n566151.\n\u201dMix\u201d prediction:\n566171.\n\u201dCombined\u201d prediction:\n566171.\nInput:\nprint(616216+423489).\nTarget:\n1039705.\n\u201dBaseline\u201d prediction:\n1039712.\n\u201dNaive\u201d prediction:\n1039605.\n\u201dMix\u201d prediction:\n1039605.\n\u201dCombined\u201d prediction:\n1039705.\nInput:\nprint(559794+837898).\nTarget:\n1397692.\n\u201dBaseline\u201d prediction:\n1397694.\n\u201dNaive\u201d prediction:\n1397662.\n\u201dMix\u201d prediction:\n1397792.\n\u201dCombined\u201d prediction:\n1397692.\nInput:\nprint(830194+551314).\nTarget:\n1381508.\n\u201dBaseline\u201d prediction:\n1381401.\n\u201dNaive\u201d prediction:\n1381518.\n\u201dMix\u201d prediction:\n1381508.\n\u201dCombined\u201d prediction:\n1381508.\nInput:\nprint(252849+873177).\nTarget:\n1126026.\n\u201dBaseline\u201d prediction:\n1126020.\n\u201dNaive\u201d prediction:\n1126006.\n\u201dMix\u201d prediction:\n1125026.\n\u201dCombined\u201d prediction:\n1126026.\nInput:\nprint(17513+163744).\n22\nUnder review as a conference paper at ICLR 2015\nTarget:\n181257.\n\u201dBaseline\u201d prediction:\n181398.\n\u201dNaive\u201d prediction:\n181287.\n\u201dMix\u201d prediction:\n181257.\n\u201dCombined\u201d prediction:\n181257.\nInput:\nprint(530590+569236).\nTarget:\n1099826.\n\u201dBaseline\u201d prediction:\n1099708.\n\u201dNaive\u201d prediction:\n1099826.\n\u201dMix\u201d prediction:\n1099826.\n\u201dCombined\u201d prediction:\n1099826.\nInput:\nprint(856484+436077).\nTarget:\n1292561.\n\u201dBaseline\u201d prediction:\n1292589.\n\u201dNaive\u201d prediction:\n1292571.\n\u201dMix\u201d prediction:\n1292561.\n\u201dCombined\u201d prediction:\n1292561.\nInput:\nprint(731632+833163).\nTarget:\n1564795.\n\u201dBaseline\u201d prediction:\n1564769.\n\u201dNaive\u201d prediction:\n1564775.\n\u201dMix\u201d prediction:\n1564795.\n\u201dCombined\u201d prediction:\n1564795.\nInput:\nprint(738532+444531).\nTarget:\n1183063.\n\u201dBaseline\u201d prediction:\n1183000.\n\u201dNaive\u201d prediction:\n1183063.\n\u201dMix\u201d prediction:\n1183063.\n\u201dCombined\u201d prediction:\n1183063.\n12.8\nEXAMPLES OF PREDICTING RESULT OF ADDITION.\nLENGTH = 8\nInput:\nprint(32847917+95908452).\nTarget:\n128756369.\n\u201dBaseline\u201d prediction:\n128899997.\n\u201dNaive\u201d prediction:\n128756669.\n\u201dMix\u201d prediction:\n128756369.\n\u201dCombined\u201d prediction:\n128756369.\nInput:\nprint(49173072+46963478).\n23\nUnder review as a conference paper at ICLR 2015\nTarget:\n96136550.\n\u201dBaseline\u201d prediction:\n96129999.\n\u201dNaive\u201d prediction:\n96136050.\n\u201dMix\u201d prediction:\n96136550.\n\u201dCombined\u201d prediction:\n96136550.\nInput:\nprint(79385668+60159139).\nTarget:\n139544807.\n\u201dBaseline\u201d prediction:\n139679090.\n\u201dNaive\u201d prediction:\n139544707.\n\u201dMix\u201d prediction:\n139544807.\n\u201dCombined\u201d prediction:\n139544807.\nInput:\nprint(16183468+42542767).\nTarget:\n58726235.\n\u201dBaseline\u201d prediction:\n58798523.\n\u201dNaive\u201d prediction:\n58726035.\n\u201dMix\u201d prediction:\n58726235.\n\u201dCombined\u201d prediction:\n58726235.\nInput:\nprint(15982788+54043908).\nTarget:\n70026696.\n\u201dBaseline\u201d prediction:\n60014022.\n\u201dNaive\u201d prediction:\n70026496.\n\u201dMix\u201d prediction:\n60026696.\n\u201dCombined\u201d prediction:\n70026696.\nInput:\nprint(45356253+31242293).\nTarget:\n76598546.\n\u201dBaseline\u201d prediction:\n76699777.\n\u201dNaive\u201d prediction:\n76598246.\n\u201dMix\u201d prediction:\n76598546.\n\u201dCombined\u201d prediction:\n76598546.\nInput:\nprint(93230501+12607891).\nTarget:\n105838392.\n\u201dBaseline\u201d prediction:\n105999882.\n\u201dNaive\u201d prediction:\n105838292.\n\u201dMix\u201d prediction:\n105838392.\n\u201dCombined\u201d prediction:\n105838392.\nInput:\nprint(2487336+40625181).\n24\nUnder review as a conference paper at ICLR 2015\nTarget:\n43112517.\n\u201dBaseline\u201d prediction:\n43178441.\n\u201dNaive\u201d prediction:\n43112917.\n\u201dMix\u201d prediction:\n43112517.\n\u201dCombined\u201d prediction:\n43112517.\nInput:\nprint(61854571+75028157).\nTarget:\n136882728.\n\u201dBaseline\u201d prediction:\n136860087.\n\u201dNaive\u201d prediction:\n136883928.\n\u201dMix\u201d prediction:\n136882728.\n\u201dCombined\u201d prediction:\n136882728.\nInput:\nprint(13828700+10188872).\nTarget:\n24017572.\n\u201dBaseline\u201d prediction:\n24000349.\n\u201dNaive\u201d prediction:\n24018872.\n\u201dMix\u201d prediction:\n23017572.\n\u201dCombined\u201d prediction:\n24017572.\n25\n",
        "sentence": " We use the tanh function \u03c6 for the input node g following the latest state of the art setup of Zaremba and Sutskever in [71]. In many experiments with recurrent neural networks ([18], [63], [71]), input is fed in one character at a time (and output generated one character at a time). Among the most interesting of these applications are unsupervised video encoding [61], video captioning [66] and program execution [71]. In Learning to Execute ([71]), Zaremba and Sutskever experiment with networks which read computer programs one character at a time and predict their output.",
        "context": "\u0013\ncl\nt = f \u2299cl\nt\u22121 + i \u2299g\nhl\nt = o \u2299tanh(cl\nt)\n6\nEXPERIMENTS\nIn this section, we report the results of our curriculum learning strategies on the program evaluation\nand memorization tasks. In both experiments, we used the same LSTM architecture.\nperiods of time. We used an LSTM described by the following equations (from Graves et al. (2013)):\nLSTM : hl\u22121\nt\n, hl\nt\u22121, cl\nt\u22121 \u2192hl\nt, cl\nt\n\uf8eb\n\uf8ec\n\uf8ed\ni\nf\no\ng\n\uf8f6\n\uf8f7\n\uf8f8=\n\uf8eb\n\uf8ec\n\uf8ed\nsigm\nsigm\nsigm\ntanh\n\uf8f6\n\uf8f7\n\uf8f8T2n,4n\n\u0012\nhl\u22121\nt\nhl\nt\u22121\n\u0013\ncl\nt = f \u2299cl\nt\u22121 + i \u2299g\nhl\nthe input achieves the best results. Random guessing would achieve an accuracy of \u223c9%, since\nthere are 11 possible output symbols.\n7\nHIDDEN STATE ALLOCATION HYPOTHESIS"
    },
    {
        "title": "Adadelta: an adaptive learning rate method",
        "author": [
            "Matthew D Zeiler"
        ],
        "venue": "arXiv preprint arXiv:1212.5701,",
        "citeRegEx": "72",
        "shortCiteRegEx": "72",
        "year": 2012,
        "abstract": "We present a novel per-dimension learning rate method for gradient descent\ncalled ADADELTA. The method dynamically adapts over time using only first order\ninformation and has minimal computational overhead beyond vanilla stochastic\ngradient descent. The method requires no manual tuning of a learning rate and\nappears robust to noisy gradient information, different model architecture\nchoices, various data modalities and selection of hyperparameters. We show\npromising results compared to other methods on the MNIST digit classification\ntask using a single machine and on a large scale voice dataset in a distributed\ncluster environment.",
        "full_text": "ADADELTA: AN ADAPTIVE LEARNING RATE METHOD\nMatthew D. Zeiler1,2\u2217\n1Google Inc., USA\n2New York University, USA\nABSTRACT\nWe present a novel per-dimension learning rate method for\ngradient descent called ADADELTA. The method dynami-\ncally adapts over time using only \ufb01rst order information and\nhas minimal computational overhead beyond vanilla stochas-\ntic gradient descent. The method requires no manual tuning of\na learning rate and appears robust to noisy gradient informa-\ntion, different model architecture choices, various data modal-\nities and selection of hyperparameters. We show promising\nresults compared to other methods on the MNIST digit clas-\nsi\ufb01cation task using a single machine and on a large scale\nvoice dataset in a distributed cluster environment.\nIndex Terms\u2014 Adaptive Learning Rates, Machine Learn-\ning, Neural Networks, Gradient Descent\n1. INTRODUCTION\nThe aim of many machine learning methods is to update a\nset of parameters x in order to optimize an objective function\nf(x). This often involves some iterative procedure which ap-\nplies changes to the parameters, \u2206x at each iteration of the\nalgorithm. Denoting the parameters at the t-th iteration as xt,\nthis simple update rule becomes:\nxt+1 = xt + \u2206xt\n(1)\nIn this paper we consider gradient descent algorithms which\nattempt to optimize the objective function by following the\nsteepest descent direction given by the negative of the gradi-\nent gt. This general approach can be applied to update any\nparameters for which a derivative can be obtained:\n\u2206xt = \u2212\u03b7gt\n(2)\nwhere gt is the gradient of the parameters at the t-th iteration\n\u2202f(xt)\n\u2202xt\nand \u03b7 is a learning rate which controls how large of\na step to take in the direction of the negative gradient. Fol-\nlowing this negative gradient for each new sample or batch\nof samples chosen from the dataset gives a local estimate\nof which direction minimizes the cost and is referred to as\nstochastic gradient descent (SGD) [1]. While often simple to\nderive the gradients for each parameter analytically, the gradi-\nent descent algorithm requires the learning rate hyperparam-\neter to be chosen.\n\u2217This work was done while Matthew D. Zeiler was an intern at Google.\nSetting the learning rate typically involves a tuning pro-\ncedure in which the highest possible learning rate is chosen\nby hand. Choosing higher than this rate can cause the system\nto diverge in terms of the objective function, and choosing\nthis rate too low results in slow learning. Determining a good\nlearning rate becomes more of an art than science for many\nproblems.\nThis work attempts to alleviate the task of choosing a\nlearning rate by introducing a new dynamic learning rate that\nis computed on a per-dimension basis using only \ufb01rst order\ninformation. This requires a trivial amount of extra compu-\ntation per iteration over gradient descent. Additionally, while\nthere are some hyper parameters in this method, we has found\ntheir selection to not drastically alter the results. The bene\ufb01ts\nof this approach are as follows:\n\u2022 no manual setting of a learning rate.\n\u2022 insensitive to hyperparameters.\n\u2022 separate dynamic learning rate per-dimension.\n\u2022 minimal computation over gradient descent.\n\u2022 robust to large gradients, noise and architecture choice.\n\u2022 applicable in both local or distributed environments.\n2. RELATED WORK\nThere are many modi\ufb01cations to the gradient descent algo-\nrithm.\nThe most powerful such modi\ufb01cation is Newton\u2019s\nmethod which requires second order derivatives of the cost\nfunction:\n\u2206xt = H\u22121\nt\ngt\n(3)\nwhere H\u22121\nt\nis the inverse of the Hessian matrix of second\nderivatives computed at iteration t. This determines the op-\ntimal step size to take for quadratic problems, but unfortu-\nnately is prohibitive to compute in practice for large models.\nTherefore, many additional approaches have been proposed\nto either improve the use of \ufb01rst order information or to ap-\nproximate the second order information.\n2.1. Learning Rate Annealing\nThere have been several attempts to use heuristics for estimat-\ning a good learning rate at each iteration of gradient descent.\nThese either attempt to speed up learning when suitable or to\nslow down learning near a local minima. Here we consider\nthe latter.\narXiv:1212.5701v1  [cs.LG]  22 Dec 2012\nWhen gradient descent nears a minima in the cost sur-\nface, the parameter values can oscillate back and forth around\nthe minima. One method to prevent this is to slow down the\nparameter updates by decreasing the learning rate. This can\nbe done manually when the validation accuracy appears to\nplateau. Alternatively, learning rate schedules have been pro-\nposed [1] to automatically anneal the learning rate based on\nhow many epochs through the data have been done. These ap-\nproaches typically add additional hyperparameters to control\nhow quickly the learning rate decays.\n2.2. Per-Dimension First Order Methods\nThe heuristic annealing procedure discussed above modi\ufb01es\na single global learning rate that applies to all dimensions of\nthe parameters. Since each dimension of the parameter vector\ncan relate to the overall cost in completely different ways,\na per-dimension learning rate that can compensate for these\ndifferences is often advantageous.\n2.2.1. Momentum\nOne method of speeding up training per-dimension is the mo-\nmentum method [2]. This is perhaps the simplest extension to\nSGD that has been successfully used for decades. The main\nidea behind momentum is to accelerate progress along dimen-\nsions in which gradient consistently point in the same direc-\ntion and to slow progress along dimensions where the sign\nof the gradient continues to change. This is done by keeping\ntrack of past parameter updates with an exponential decay:\n\u2206xt = \u03c1\u2206xt\u22121 \u2212\u03b7gt\n(4)\nwhere \u03c1 is a constant controlling the decay of the previous\nparameter updates. This gives a nice intuitive improvement\nover SGD when optimizing dif\ufb01cult cost surfaces such as a\nlong narrow valley. The gradients along the valley, despite\nbeing much smaller than the gradients across the valley, are\ntypically in the same direction and thus the momentum term\naccumulates to speed up progress. In SGD the progress along\nthe valley would be slow since the gradient magnitude is small\nand the \ufb01xed global learning rate shared by all dimensions\ncannot speed up progress. Choosing a higher learning rate\nfor SGD may help but the dimension across the valley would\nthen also make larger parameter updates which could lead\nto oscillations back as forth across the valley. These oscil-\nlations are mitigated when using momentum because the sign\nof the gradient changes and thus the momentum term damps\ndown these updates to slow progress across the valley. Again,\nthis occurs per-dimension and therefore the progress along the\nvalley is unaffected.\n2.2.2. ADAGRAD\nA recent \ufb01rst order method called ADAGRAD [3] has shown\nremarkably good results on large scale learning tasks in a dis-\ntributed environment [4]. This method relies on only \ufb01rst\norder information but has some properties of second order\nmethods and annealing. The update rule for ADAGRAD is\nas follows:\n\u2206xt = \u2212\n\u03b7\nqPt\n\u03c4=1 g2\u03c4\ngt\n(5)\nHere the denominator computes the \u21132 norm of all previous\ngradients on a per-dimension basis and \u03b7 is a global learning\nrate shared by all dimensions.\nWhile there is the hand tuned global learning rate, each\ndimension has its own dynamic rate. Since this dynamic rate\ngrows with the inverse of the gradient magnitudes, large gra-\ndients have smaller learning rates and small gradients have\nlarge learning rates. This has the nice property, as in second\norder methods, that the progress along each dimension evens\nout over time. This is very bene\ufb01cial for training deep neu-\nral networks since the scale of the gradients in each layer is\noften different by several orders of magnitude, so the optimal\nlearning rate should take that into account. Additionally, this\naccumulation of gradient in the denominator has the same ef-\nfects as annealing, reducing the learning rate over time.\nSince the magnitudes of gradients are factored out in\nADAGRAD, this method can be sensitive to initial conditions\nof the parameters and the corresponding gradients. If the ini-\ntial gradients are large, the learning rates will be low for the\nremainder of training. This can be combatted by increasing\nthe global learning rate, making the ADAGRAD method sen-\nsitive to the choice of learning rate. Also, due to the continual\naccumulation of squared gradients in the denominator, the\nlearning rate will continue to decrease throughout training,\neventually decreasing to zero and stopping training com-\npletely. We created our ADADELTA method to overcome the\nsensitivity to the hyperparameter selection as well as to avoid\nthe continual decay of the learning rates.\n2.3. Methods Using Second Order Information\nWhereas the above methods only utilized gradient and func-\ntion evaluations in order to optimize the objective, second\norder methods such as Newton\u2019s method or quasi-Newtons\nmethods make use of the Hessian matrix or approximations\nto it. While this provides additional curvature information\nuseful for optimization, computing accurate second order in-\nformation is often expensive.\nSince computing the entire Hessian matrix of second\nderivatives is too computationally expensive for large models,\nBecker and LecCun [5] proposed a diagonal approximation to\nthe Hessian. This diagonal approximation can be computed\nwith one additional forward and back-propagation through\nthe model, effectively doubling the computation over SGD.\nOnce the diagonal of the Hessian is computed, diag(H), the\nupdate rule becomes:\n\u2206xt = \u2212\n1\n|diag(Ht)| + \u00b5 gt\n(6)\nwhere the absolute value of this diagonal Hessian is used to\nensure the negative gradient direction is always followed and\n\u00b5 is a small constant to improve the conditioning of the Hes-\nsian for regions of small curvature.\nA recent method by Schaul et al. [6] incorporating the\ndiagonal Hessian with ADAGRAD-like terms has been intro-\nduced to alleviate the need for hand speci\ufb01ed learning rates.\nThis method uses the following update rule:\n\u2206xt = \u2212\n1\n|diag(Ht)|\nE[gt\u2212w:t]2\nE[g2\nt\u2212w:t] gt\n(7)\nwhere E[gt\u2212w:t] is the expected value of the previous w gra-\ndients and E[g2\nt\u2212w:t] is the expected value of squared gradi-\nents over the same window w. Schaul et al. also introduce a\nheuristic for this window size w (see [6] for more details).\n3. ADADELTA METHOD\nThe idea presented in this paper was derived from ADA-\nGRAD [3] in order to improve upon the two main draw-\nbacks of the method: 1) the continual decay of learning rates\nthroughout training, and 2) the need for a manually selected\nglobal learning rate. After deriving our method we noticed\nseveral similarities to Schaul et al. [6], which will be com-\npared to below.\nIn the ADAGRAD method the denominator accumulates\nthe squared gradients from each iteration starting at the be-\nginning of training. Since each term is positive, this accumu-\nlated sum continues to grow throughout training, effectively\nshrinking the learning rate on each dimension. After many it-\nerations, this learning rate will become in\ufb01nitesimally small.\n3.1. Idea 1: Accumulate Over Window\nInstead of accumulating the sum of squared gradients over all\ntime, we restricted the window of past gradients that are ac-\ncumulated to be some \ufb01xed size w (instead of size t where\nt is the current iteration as in ADAGRAD). With this win-\ndowed accumulation the denominator of ADAGRAD cannot\naccumulate to in\ufb01nity and instead becomes a local estimate\nusing recent gradients. This ensures that learning continues\nto make progress even after many iterations of updates have\nbeen done.\nSince storing w previous squared gradients is inef\ufb01cient,\nour methods implements this accumulation as an exponen-\ntially decaying average of the squared gradients. Assume at\ntime t this running average is E[g2]t then we compute:\nE[g2]t = \u03c1 E[g2]t\u22121 + (1 \u2212\u03c1) g2\nt\n(8)\nwhere \u03c1 is a decay constant similar to that used in the momen-\ntum method. Since we require the square root of this quantity\nin the parameter updates, this effectively becomes the RMS\nof previous squared gradients up to time t:\nRMS[g]t =\np\nE[g2]t + \u03f5\n(9)\nwhere a constant \u03f5 is added to better condition the denomina-\ntor as in [5]. The resulting parameter update is then:\n\u2206xt = \u2212\n\u03b7\nRMS[g]t\ngt\n(10)\nAlgorithm 1 Computing ADADELTA update at time t\nRequire: Decay rate \u03c1, Constant \u03f5\nRequire: Initial parameter x1\n1: Initialize accumulation variables E[g2]0 = 0, E[\u2206x2]0 = 0\n2: for t = 1 : T do %% Loop over # of updates\n3:\nCompute Gradient: gt\n4:\nAccumulate Gradient: E[g2]t = \u03c1E[g2]t\u22121 + (1 \u2212\u03c1)g2\nt\n5:\nCompute Update: \u2206xt = \u2212\nRMS[\u2206x]t\u22121\nRMS[g]t\ngt\n6:\nAccumulate Updates: E[\u2206x2]t = \u03c1E[\u2206x2]t\u22121+(1\u2212\u03c1)\u2206x2\nt\n7:\nApply Update: xt+1 = xt + \u2206xt\n8: end for\n3.2. Idea 2: Correct Units with Hessian Approximation\nWhen considering the parameter updates, \u2206x, being applied\nto x, the units should match. That is, if the parameter had\nsome hypothetical units, the changes to the parameter should\nbe changes in those units as well. When considering SGD,\nMomentum, or ADAGRAD, we can see that this is not the\ncase. The units in SGD and Momentum relate to the gradient,\nnot the parameter:\nunits of \u2206x \u221dunits of g \u221d\u2202f\n\u2202x \u221d\n1\nunits of x\n(11)\nassuming the cost function, f, is unitless. ADAGRAD also\ndoes not have correct units since the update involves ratios of\ngradient quantities, hence the update is unitless.\nIn contrast, second order methods such as Newton\u2019s\nmethod that use Hessian information or an approximation\nto the Hessian do have the correct units for the parameter\nupdates:\n\u2206x \u221dH\u22121g \u221d\n\u2202f\n\u2202x\n\u22022f\n\u2202x2\n\u221dunits of x\n(12)\nNoticing this mismatch of units we considered terms to\nadd to Eqn. 10 in order for the units of the update to match\nthe units of the parameters. Since second order methods are\ncorrect, we rearrange Newton\u2019s method (assuming a diagonal\nHessian) for the inverse of the second derivative to determine\nthe quantities involved:\n\u2206x =\n\u2202f\n\u2202x\n\u22022f\n\u2202x2\n\u21d2\n1\n\u22022f\n\u2202x2\n= \u2206x\n\u2202f\n\u2202x\n(13)\nSince the RMS of the previous gradients is already repre-\nsented in the denominator in Eqn. 10 we considered a mea-\nsure of the \u2206x quantity in the numerator. \u2206xt for the current\ntime step is not known, so we assume the curvature is locally\nsmooth and approximate \u2206xt by compute the exponentially\ndecaying RMS over a window of size w of previous \u2206x to\ngive the ADADELTA method:\n\u2206xt = \u2212RMS[\u2206x]t\u22121\nRMS[g]t\ngt\n(14)\nwhere the same constant \u03f5 is added to the numerator RMS as\nwell. This constant serves the purpose both to start off the \ufb01rst\niteration where \u2206x0 = 0 and to ensure progress continues to\nbe made even if previous updates become small.\nThis derivation made the assumption of diagonal curva-\nture so that the second derivatives could easily be rearranged.\nFurthermore, this is an approximation to the diagonal Hessian\nusing only RMS measures of g and \u2206x. This approximation\nis always positive as in Becker and LeCun [5], ensuring the\nupdate direction follows the negative gradient at each step.\nIn Eqn. 14 the RMS[\u2206x]t\u22121 quantity lags behind the de-\nnominator by 1 time step, due to the recurrence relationship\nfor \u2206xt. An interesting side effect of this is that the system is\nrobust to large sudden gradients which act to increase the de-\nnominator, reducing the effective learning rate at the current\ntime step, before the numerator can react.\nThe method in Eqn. 14 uses only \ufb01rst order information\nand has some properties from each of the discussed meth-\nods. The negative gradient direction for the current iteration\n\u2212gt is always followed as in SGD. The numerator acts as\nan acceleration term, accumulating previous gradients over a\nwindow of time as in momentum. The denominator is re-\nlated to ADAGRAD in that the squared gradient information\nper-dimension helps to even out the progress made in each di-\nmension, but is computed over a window to ensure progress\nis made later in training. Finally, the method relates to Schaul\net al. \u2019s in that some approximation to the Hessian is made,\nbut instead costs only one gradient computation per iteration\nby leveraging information from past updates. For the com-\nplete algorithm details see Algorithm 1.\n4. EXPERIMENTS\nWe evaluate our method on two tasks using several different\nneural network architectures. We train the neural networks\nusing SGD, Momentum, ADAGRAD, and ADADELTA in a\nsupervised fashion to minimize the cross entropy objective\nbetween the network output and ground truth labels. Compar-\nisons are done both on a local computer and in a distributed\ncompute cluster.\n4.1. Handwritten Digit Classi\ufb01cation\nIn our \ufb01rst set of experiments we train a neural network on the\nMNIST handwritten digit classi\ufb01cation task. For comparison\nwith Schaul et al. \u2019s method we trained with tanh nonlinear-\nities and 500 hidden units in the \ufb01rst layer followed by 300\nhidden units in the second layer, with the \ufb01nal softmax out-\nput layer on top. Our method was trained on mini-batches of\n100 images per batch for 6 epochs through the training set.\nSetting the hyperparameters to \u03f5 = 1e \u22126 and \u03c1 = 0.95 we\nachieve 2.00% test set error compared to the 2.10% of Schaul\net al. While this is nowhere near convergence it gives a sense\nof how quickly the algorithms can optimize the classi\ufb01cation\nobjective.\n0\n10\n20\n30\n40\n50\n  1\n1.5\n  2\n2.5\n  3\n3.5\n  4\n4.5\n  5\n5.5\n  6\nEpoch\nTest Error %\n \n \nSGD\nMOMENTUM\nADAGRAD\nADADELTA\nFig. 1. Comparison of learning rate methods on MNIST digit\nclassi\ufb01cation for 50 epochs.\nTo further analyze various methods to convergence, we\ntrain the same neural network with 500 hidden units in the \ufb01rst\nlayer, 300 hidden units in the second layer and recti\ufb01ed linear\nactivation functions in both layers for 50 epochs. We notice\nthat recti\ufb01ed linear units work better in practice than tanh, and\ntheir non-saturating nature further tests each of the methods\nat coping with large variations of activations and gradients.\nIn Fig. 1 we compare SGD, Momentum, ADAGRAD,\nand ADADELTA in optimizing the test set errors. The unal-\ntered SGD method does the worst in this case, whereas adding\nthe momentum term to it signi\ufb01cantly improves performance.\nADAGRAD performs well for the \ufb01rst 10 epochs of training,\nafter which it slows down due to the accumulations in the de-\nnominator which continually increase. ADADELTA matches\nthe fast initial convergence of ADAGRAD while continuing\nto reduce the test error, converging near the best performance\nwhich occurs with momentum.\nSGD\nMOMENTUM\nADAGRAD\n\u03f5 = 1e0\n2.26%\n89.68%\n43.76%\n\u03f5 = 1e\u22121\n2.51%\n2.03%\n2.82%\n\u03f5 = 1e\u22122\n7.02%\n2.68%\n1.79%\n\u03f5 = 1e\u22123\n17.01%\n6.98%\n5.21%\n\u03f5 = 1e\u22124\n58.10%\n16.98%\n12.59%\nTable 1. MNIST test error rates after 6 epochs of training for\nvarious hyperparameter settings using SGD, MOMENTUM,\nand ADAGRAD.\n\u03c1 = 0.9\n\u03c1 = 0.95\n\u03c1 = 0.99\n\u03f5 = 1e\u22122\n2.59%\n2.58%\n2.32%\n\u03f5 = 1e\u22124\n2.05%\n1.99%\n2.28%\n\u03f5 = 1e\u22126\n1.90%\n1.83%\n2.05%\n\u03f5 = 1e\u22128\n2.29%\n2.13%\n2.00%\nTable 2. MNIST test error rate after 6 epochs for various\nhyperparameter settings using ADADELTA.\n4.2. Sensitivity to Hyperparameters\nWhile momentum converged to a better \ufb01nal solution than\nADADELTA after many epochs of training, it was very sen-\nsitive to the learning rate selection, as was SGD and ADA-\nGRAD. In Table 1 we vary the learning rates for each method\nand show the test set errors after 6 epochs of training using\nrecti\ufb01ed linear units as the activation function. The optimal\nsettings from each column were used to generate Fig. 1. With\nSGD, Momentum, or ADAGRAD the learning rate needs to\nbe set to the correct order of magnitude, above which the so-\nlutions typically diverge and below which the optimization\nproceeds slowly. We can see that these results are highly vari-\nable for each method, compared to ADADELTA in Table 2\nin which the two hyperparameters do not signi\ufb01cantly alter\nperformance.\n4.3. Effective Learning Rates\nTo investigate some of the properties of ADADELTA we plot\nin Fig. 2 the step sizes and parameter updates of 10 randomly\nselected dimensions in each of the 3 weight matrices through-\nout training. There are several interesting things evident in\nthis \ufb01gure. First, the step sizes, or effective learning rates (all\nterms except gt from Eqn. 14) shown in the left portion of the\n\ufb01gure are larger for the lower layers of the network and much\nsmaller for the top layer at the beginning of training. This\nproperty of ADADELTA helps to balance the fact that lower\nlayers have smaller gradients due to the diminishing gradi-\n0\n100\n200\n0\n0.5\n1\nd x1\n0\n100\n200\n0\n0.5\n1\nd x2\n0\n100\n200\n0\n0.5\n1\nd x3\n0\n100\n200\n\u22120.01\n0\n0.01\n6 x1\n0\n100\n200\n\u22120.01\n0\n0.01\n6 x2\n0\n100\n200\n\u22120.01\n0\n0.01\n6 x3\nFig. 2.\nStep sizes and parameter updates shown every 60\nbatches during training the MNIST network with tanh non-\nlinearities for 25 epochs. Left: Step sizes for 10 randomly\nselected dimensions of each of the 3 weight matrices of the\nnetwork. Right: Parameters changes for the same 10 dimen-\nsions for each of the 3 weight matrices. Note the large step\nsizes in lower layers that help compensate for vanishing gra-\ndients that occur with backpropagation.\nent problem in neural networks and thus should have larger\nlearning rates.\nSecondly, near the end of training these step sizes con-\nverge to 1. This is typically a high learning rate that would\nlead to divergence in most methods, however this conver-\ngence towards 1 only occurs near the end of training when the\ngradients and parameter updates are small. In this scenario,\nthe \u03f5 constants in the numerator and denominator dominate\nthe past gradients and parameter updates, converging to the\nlearning rate of 1.\nThis leads to the last interesting property of ADADELTA\nwhich is that when the step sizes become 1, the parameter\nupdates (shown on the right of Fig. 2) tend towards zero. This\noccurs smoothly for each of the weight matrices effectively\noperating as if an annealing schedule was present.\nHowever, having no explicit annealing schedule imposed\non the learning rate could be why momentum with the proper\nhyperparameters outperforms ADADELTA later in training as\nseen in Fig. 1. With momentum, oscillations that can occur\nnear a minima are smoothed out, whereas with ADADELTA\nthese can accumulate in the numerator. An annealing sched-\nule could possibly be added to the ADADELTA method to\ncounteract this in future work.\n4.4. Speech Data\nIn the next set of experiments we trained a large-scale neu-\nral network with 4 hidden layers on several hundred hours\nof US English data collected using Voice Search, Voice IME,\nand read data. The network was trained using the distributed\nsystem of [4] in which a centralized parameter server accu-\nmulates the gradient information reported back from several\nreplicas of the neural network. In our experiments we used ei-\nther 100 or 200 such replica networks to test the performance\nof ADADELTA in a highly distributed environment.\nThe neural network is setup as in [7] where the inputs\nare 26 frames of audio, each consisting of 40 log-energy \ufb01l-\nter bank outputs.\nThe outputs of the network were 8,000\nsenone labels produced from a GMM-HMM system using\nforced alignment with the input frames. Each hidden layer\nof the neural network had 2560 hidden units and was trained\nwith either logistic or recti\ufb01ed linear nonlinearities.\nFig. 3 shows the performance of the ADADELTA method\nwhen using 100 network replicas. Notice our method ini-\ntially converges faster and outperforms ADAGRAD through-\nout training in terms of frame classi\ufb01cation accuracy on the\ntest set. The same settings of \u03f5 = 1e\u22126 and \u03c1 = 0.95 from\nthe MNIST experiments were used for this setup.\nWhen training with recti\ufb01ed linear units and using 200\nmodel replicas we also used the same settings of hyperpa-\nrameters (see Fig. 4). Despite having 200 replicates which\ninherently introduces signi\ufb01cants amount of noise to the gra-\ndient accumulations, the ADADELTA method performs well,\nquickly converging to the same frame accuracy as the other\nmethods.\n0\n20\n40\n60\n80\n100\n18\n20\n22\n24\n26\n28\n30\n32\n34\n36\n38\nTime (hours)\nFame Acc %uracy\n \n \nADAGRAD log\nADADELTA log\nFig. 3. Comparison of ADAGRAD and ADADELTA on the\nSpeech Dataset with 100 replicas using logistic nonlinearities.\n0\n20\n40\n60\n80\n100\n120\n140\n160\n15\n20\n25\n30\n35\nTime (hours)\nFrame Accuracy %\n \n \nADAGRAD relu\nADADELTA relu\nMOMENTUM relu\nFig. 4.\nComparison of ADAGRAD, Momentum, and\nADADELTA on the Speech Dataset with 200 replicas using\nrecti\ufb01ed linear nonlinearities.\n5. CONCLUSION\nIn this tech report we introduced a new learning rate method\nbased on only \ufb01rst order information which shows promis-\ning result on MNIST and a large scale Speech recognition\ndataset. This method has trivial computational overhead com-\npared to SGD while providing a per-dimension learning rate.\nDespite the wide variation of input data types, number of hid-\nden units, nonlinearities and number of distributed replicas,\nthe hyperparameters did not need to be tuned, showing that\nADADELTA is a robust learning rate method that can be ap-\nplied in a variety of situations.\nAcknowledgements We thank Geoff Hinton, Yoram\nSinger, Ke Yang, Marc\u2019Aurelio Ranzato and Jeff Dean for\nthe helpful comments and discussions regarding this work.\n6. REFERENCES\n[1] H. Robinds and S. Monro, \u201cA stochastic approximation\nmethod,\u201d Annals of Mathematical Statistics, vol. 22, pp.\n400\u2013407, 1951.\n[2] D.E. Rumelhart, G.E. Hinton, and R.J. Williams, \u201cLearn-\ning representations by back-propagating errors,\u201d Nature,\nvol. 323, pp. 533\u2013536, 1986.\n[3] J. Duchi, E. Hazan, and Y. Singer, \u201cAdaptive subgradient\nmethods for online leaning and stochastic optimization,\u201d\nin COLT, 2010.\n[4] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin,\nQ. Le, M. Mao, M. Ranzato, A. Senior, P. Tucker,\nK. Yang, and A. Ng, \u201cLarge scale distributed deep net-\nworks,\u201d in NIPS, 2012.\n[5] S. Becker and Y. LeCun, \u201cImproving the convergence of\nback-propagation learning with second order methods,\u201d\nTech. Rep., Department of Computer Science, University\nof Toronto, Toronto, ON, Canada, 1988.\n[6] T. Schaul, S. Zhang, and Y. LeCun,\n\u201cNo more pesky\nlearning rates,\u201d arXiv:1206.1106, 2012.\n[7] N. Jaitly, P. Nguyen, A. Senior, and V. Vanhoucke, \u201cAp-\nplication of pretrained deep neural networks to large vo-\ncabulary speech recognition,\u201d in Interspeech, 2012.\n",
        "sentence": " Some popular heuristics, such as AdaGrad [16], AdaDelta [72], and RMSprop [1], adaptively tune the learning rate for each feature.",
        "context": "2.1. Learning Rate Annealing\nThere have been several attempts to use heuristics for estimat-\ning a good learning rate at each iteration of gradient descent.\nThese either attempt to speed up learning when suitable or to\nWhile momentum converged to a better \ufb01nal solution than\nADADELTA after many epochs of training, it was very sen-\nsitive to the learning rate selection, as was SGD and ADA-\nGRAD. In Table 1 we vary the learning rates for each method\nden units, nonlinearities and number of distributed replicas,\nthe hyperparameters did not need to be tuned, showing that\nADADELTA is a robust learning rate method that can be ap-\nplied in a variety of situations."
    },
    {
        "title": "On rectified linear units for speech processing",
        "author": [
            "Matthew D Zeiler",
            "M Ranzato",
            "Rajat Monga",
            "M Mao",
            "K Yang",
            "Quoc Viet Le",
            "Patrick Nguyen",
            "A Senior",
            "Vincent Vanhoucke",
            "Jeffrey Dean"
        ],
        "venue": "In Acoustics, Speech and Signal Processing (ICASSP),",
        "citeRegEx": "73",
        "shortCiteRegEx": "73",
        "year": 2013,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " These units have been demonstrated to improve the performance of many deep neural networks ([43], [47], [73]) on tasks as varied as speech processing and object recognition, and have been used in recurrent neural networks by [7].",
        "context": null
    }
]