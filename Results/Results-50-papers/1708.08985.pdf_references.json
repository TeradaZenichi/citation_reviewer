[
    {
        "title": "Real-time small obstacle detection on highways using compressive rbm road reconstruction",
        "author": [
            "C. Creusot",
            "A. Munawar"
        ],
        "venue": "In IEEE Intelligent Vehicles Symposium, Seoul,",
        "citeRegEx": "1",
        "shortCiteRegEx": "1",
        "year": 2015,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Generative models have previously been applied to anomaly detection [11, 1] and noise reduction [14]. For example, in case of anomaly detection on the road [1], any non-road object (e. Munawar [1].",
        "context": null
    },
    {
        "title": "Describing objects by their attributes",
        "author": [
            "A. Farhadi",
            "I. Endres",
            "D. Hoiem",
            "D. Forsyth"
        ],
        "venue": "In Computer Vision and Pattern Recognition,",
        "citeRegEx": "2",
        "shortCiteRegEx": "2",
        "year": 2009,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " The attributes [2] are handcrafted and mainly based on the appearance of the input data, i.",
        "context": null
    },
    {
        "title": "Night vision animal detection",
        "author": [
            "D. Forslund",
            "J. Bjrkefur"
        ],
        "venue": "IEEE Intelligent Vehicles Symposium Proceedings,",
        "citeRegEx": "3",
        "shortCiteRegEx": "3",
        "year": 2014,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Anomaly detection is a key and usually the first requirement in many signal-processing applications pipeline [3, 10].",
        "context": null
    },
    {
        "title": "Medical image denoising using convolutional denoising autoencoders",
        "author": [
            "L. Gondara"
        ],
        "venue": "IEEE 16th International Conference on Data Mining Workshops (ICDMW),",
        "citeRegEx": "4",
        "shortCiteRegEx": "4",
        "year": 2016,
        "abstract": "Image denoising is an important pre-processing step in medical image\nanalysis. Different algorithms have been proposed in past three decades with\nvarying denoising performances. More recently, having outperformed all\nconventional methods, deep learning based models have shown a great promise.\nThese methods are however limited for requirement of large training sample size\nand high computational costs. In this paper we show that using small sample\nsize, denoising autoencoders constructed using convolutional layers can be used\nfor efficient denoising of medical images. Heterogeneous images can be combined\nto boost sample size for increased denoising performance. Simplest of networks\ncan reconstruct images with corruption levels so high that noise and signal are\nnot differentiable to human eye.",
        "full_text": "Medical image denoising using convolutional\ndenoising autoencoders\nLovedeep Gondara\nDepartment of Computer Science\nSimon Fraser University\nlgondara@sfu.ca\nAbstract\u2014Image denoising is an important pre-processing step\nin medical image analysis. Different algorithms have been pro-\nposed in past three decades with varying denoising performances.\nMore recently, having outperformed all conventional methods,\ndeep learning based models have shown a great promise. These\nmethods are however limited for requirement of large training\nsample size and high computational costs. In this paper we show\nthat using small sample size, denoising autoencoders constructed\nusing convolutional layers can be used for ef\ufb01cient denoising\nof medical images. Heterogeneous images can be combined to\nboost sample size for increased denoising performance. Simplest\nof networks can reconstruct images with corruption levels so high\nthat noise and signal are not differentiable to human eye.\nKeywords\u2014Image denoising, denoising autoencoder, convolu-\ntional autoencoder\nI.\nINTRODUCTION\nMedical imaging including X-rays, Magnetic Resonance\nImaging (MRI), Computer Tomography (CT), ultrasound etc.\nare susceptible to noise [21]. Reasons vary from use of\ndifferent image acquisition techniques to attempts at decreasing\npatients exposure to radiation. As the amount of radiation is\ndecreased, noise increases [1]. Denoising is often required for\nproper image analysis, both by humans and machines.\nImage denoising, being a classical problem in computer\nvision has been studied in detail. Various methods exist, rang-\ning from models based on partial differential equations (PDEs)\n[18], [20], [22], domain transformations such as wavelets [6],\nDCT [29], BLS-GSM [19] etc., non local techniques including\nNL-means [30], [3], combination of non local means and\ndomain transformations such as BM3D [7] and a family of\nmodels exploiting sparse coding techniques [17], [9], [15]. All\nmethods share a common goal, expressed as\nz = x + y\n(1)\nWhere z is the noisy image produced as a sum of original\nimage x and some noise y. Most methods try to approximate\nx using z as close as possible. IN most cases, y is assumed to\nbe generated from a well de\ufb01ned process.\nWith recent developments in deep learning [14], [11], [23],\n[2], [10], results from models based on deep architectures\nhave been promising. Autoencoders have been used for im-\nage denoising [24], [25], [28], [5]. They easily outperform\nconventional denoising methods and are less restrictive for\nspeci\ufb01cation of noise generative processes. Denoising au-\ntoencoders constructed using convolutional layers have better\nimage denoising performance for their ability to exploit strong\nspatial correlations.\nIn this paper we present empirical evidence that stacked\ndenoising autoencoders built using convolutional layers work\nwell for small sample sizes, typical of medical image\ndatabases. Which is in contrary to the belief that for optimal\nperformance, very large training datasets are needed for models\nbased on deep architectures. We also show that these methods\ncan recover signal even when noise levels are very high, at the\npoint where most other denoising methods would fail.\nRest of this paper is organized as following, next section\ndiscusses related work in image denoising using deep architec-\ntures. Section III introduces autoencoders and their variants.\nSection IV explains our experimental set-up and details our\nempirical evaluation and section V presents our conclusions\nand directions for future work.\nII.\nRELATED WORK\nAlthough BM3D [7] is considered state-of-the-art in image\ndenoising and is a very well engineered method, Burger et\nal. [4] showed that a plain multi layer perceptron (MLP) can\nachieve similar denoising performance.\nDenoising autoencoders are a recent addition to image de-\nnoising literature. Used as a building block for deep networks,\nthey were introduced by Vincent et al. [24] as an extension to\nclassic autoencoders. It was shown that denoising autoencoders\ncan be stacked [25] to form a deep network by feeding the\noutput of one denoising autoencoder to the one below it.\nJain et al. [12] proposed image denoising using convolu-\ntional neural networks. It was observed that using a small sam-\nple of training images, performance at par or better than state-\nof-the-art based on wavelets and Markov random \ufb01elds can be\nachieved. Xie et al. [28] used stacked sparse autoencoders for\nimage denoising and inpainting, it performed at par with K-\nSVD. Agostenelli et al. [1] experimented with adaptive multi\ncolumn deep neural networks for image denoising, built using\ncombination of stacked sparse autoencoders. This system was\nshown to be robust for different noise types.\nIII.\nPRELIMINARIES\nA. Autoencoders\nAn autoencoder is a type of neural network that tries\nto learn an approximation to identity function using back-\npropagation, i.e. given a set of unlabeled training inputs\nx(1), x(2), ..., x(n), it uses\narXiv:1608.04667v2  [cs.CV]  18 Sep 2016\nz(i) = x(i)\n(2)\nAn autoencoder \ufb01rst takes an input x \u2208[0, 1]d and\nmaps(encode) it to a hidden representation y \u2208[0, 1]d\u2032 using\ndeterministic mapping, such as\ny = s(Wx + b)\n(3)\nwhere s can be any non linear function. Latent represen-\ntation y is then mapped back(decode) into a reconstruction z,\nwhich is of same shape as x using similar mapping.\nz = s(W \u2032y + b\u2032)\n(4)\nIn (4), prime symbol is not a matrix transpose. Model\nparameters (W, W \u2032, b, b\u2032) are optimized to minimize recon-\nstruction error, which can be assessed using different loss\nfunctions such as squared error or cross-entropy.\nBasic architecture of an autoencoder is shown in Fig. 1\n[32]\nFig. 1.\nA basic autoencoder\nHere layer L1 is input layer which is encoded in layer L2\nusing latent representation and input is reconstructed at L3.\nUsing number of hidden units lower than inputs forces\nautoencoder to learn a compressed approximation. Mostly an\nautoencoder learns low dimensional representation very similar\nto Principal Component Analysis (PCA). Having hidden units\nlarger than number of inputs can still discover useful insights\nby imposing certain sparsity constraints.\n1) Denoising Autoencoders: Denoising autoencoder is a\nstochastic extension to classic autoencoder [24], that is we\nforce the model to learn reconstruction of input given its noisy\nversion. A stochastic corruption process randomly sets some\nof the inputs to zero, forcing denoising autoencoder to predict\nmissing(corrupted) values for randomly selected subsets of\nmissing patterns.\nBasic architecture of a denoising autoencoder is shown in\nFig. 2\nFig. 2.\nDenoising autoencoder, some inputs are set to missing\nDenoising autoencoders can be stacked to create a deep\nnetwork (stacked denoising autoencoder) [25] shown in Fig. 3\n[33].\nFig. 3.\nA stacked denoising autoencoder\nOutput from the layer below is fed to the current layer and\ntraining is done layer wise.\n2) Convolutional\nautoencoder:\nConvolutional\nautoen-\ncoders [16] are based on standard autoencoder architecture\nwith convolutional encoding and decoding layers. Compared\nto classic autoencoders, convolutional autoencoders are better\nsuited for image processing as they utilize full capability of\nconvolutional neural networks to exploit image structure.\nIn convolutional autoencoders, weights are shared among\nall input locations which helps preserve local spatiality. Rep-\nresentation of ith feature map is given as\nhi = s(x \u2217W i + bi).\n(5)\nwhere bias is broadcasted to whole map, \u2217denotes convo-\nlution (2D) and s is an activation. Single bias per latent map\nis used and reconstruction is obtained as\ny = s(\nX\ni\u2208H\nhi \u2217\u02dcW i + c)\n(6)\nwhere c is bias per input channel, H is group of latent\nfeature maps, \u02dcW is \ufb02ip operation over both weight dimensions.\nBackpropogation is used for computation of gradient of the\nerror function with respect to the parameters.\nIV.\nEVALUATION\nA. Data\nWe used two datasets, mini-MIAS database of mammo-\ngrams(MMM) [13] and a dental radiography database(DX)\n[26]. MMM has 322 images of 1024 \u00d7 1024 resolution and\nDX has 400 cephalometric X-ray images collected from 400\npatients with a resolution of 1935 \u00d7 2400. Random images\nfrom both datasets are shown in Fig. 4.\nFig. 4. Random sample of medical images from datasets MMM and DX, rows\n1 and 2 show X-ray images from DX, whereas row 3 shows mammograms\nfrom MMM\nB. Experimental setup\nAll images were processed prior to modelling. Pre-\nprocessing consisted of resizing all images to 64 \u00d7 64 for\ncomputational resource reasons. Different parameters detailed\nin Table I were used for corruption.\nTABLE I.\nDATASET PERTURBATIONS\nNoise type\ncorruption parameters\nGaussian\np=0.1, \u00b5 = 0, \u03c3 = 1\nGaussian\np=0.5, \u00b5 = 0, \u03c3 = 1\nGaussian\np=0.2, \u00b5 = 0, \u03c3 = 2\nGaussian\np=0.2, \u00b5 = 0, \u03c3 = 5\nPoisson\np=0.2, \u03bb = 1\nPoisson\np=0.2, \u03bb = 5\np is proportion of noise introduced, \u03c3 and \u00b5 are standard deviation and mean of\nnormal distribution and \u03bb is mean of Poisson distribution\nInstead of corrupting a single image at a time, \ufb02attened\ndataset with each row representing an image was corrupted,\nhence simultaneously perturbing all images. Corrupted datasets\nwere then used for modelling. Relatively simple architec-\nture was used for convolutional denoising autoencoder (CNN\nDAE), shown in Fig. 5.\nFig. 5.\nArchitecture of CNN DAE used\nKeras [31] was used for implementing this model on an\nAcer Aspire M5 notebook (Intel Core i5-4200U, 10 GB RAM,\nno GPU). Images were compared using structural similarity\nindex measure(SSIM) instead of peak signal to noise ratio\n(PSNR) for its consistency and accuracy [27]. A composite\nindex of three measures, SSIM estimates the visual effects of\nshifts in image luminance, contrast and other remaining errors,\ncollectively called structural changes. For original and coded\nsignals x and y, SSIM is given as\nSSIM(x, y) = [l(x, y)]\u03b1[c(x, y)]\u03b2[s(x, y)]\u03b3\n(7)\nwhere \u03b1, \u03b2 and \u03b3 > 0 control the relative signi\ufb01cance of\neach of three terms in SSIM and l, c and s are luminance,\ncontrast and structural components calculated as\nl(x, y) = 2\u00b5x\u00b5y + C1\n\u00b52x + \u00b52y + C1\n(8)\nc(x, y) = 2\u03c3x\u03c3y + C2\n\u03c32x + \u03c32y + C2\n(9)\ns(x, y) = 2\u03c3xy + C3\n\u03c3x\u03c3y + C3\n(10)\nwhere \u00b5x and \u00b5y represents the mean of original and\ncoded image, \u03c3x and \u03c3y are standard deviation and \u03c3xy is\nthe covariance of two images.\nBasic settings were kept constant with 100 epochs and\na batch size of 10. No \ufb01ne-tuning was performed to get\ncomparison results on a basic architecture, that should be easy\nto implement even by a naive user. Mean of SSIM scores over\nthe set of test images is reported for comparison.\nC. Empirical evaluation\nFor baseline comparison, images corrupted with lowest\nnoise level (\u00b5 = 0, \u03c3 = 1, p = 0.1) were used. To keep similar\nsample size for training, we used 300 images from each of the\ndatasets, leaving us with 22 for testing in MMM and 100 in\nDX.\nUsing a batch size of 10 and 100 epochs, denoising results\nare presented in Fig. 6 and Table II.\nFig. 6.\nDenoising results on both datasets, top row shows real images with\nsecond row showing the noisier version (\u00b5 = 0, \u03c3 = 1, p = 0.1), third row\nshows images denoised using CNN DAE and fourth row shows results of\napplying a median \ufb01lter\nTABLE II.\nMEAN SSIM SCORES FOR TEST IMAGES FROM MMM AND\nDX DATASETS\nImage type\nMMM\nDX\nNoisy\n0.45\n0.62\nCNN DAE\n0.81\n0.88\nMedian \ufb01lter\n0.73\n0.86\nResults show an increased denoising performance using\nthis simple architecture on small datasets over the use of\nmedian \ufb01lter, which is most often used for this type of noise.\nModel converged nicely for the given noise levels and\nsample size, shown in Fig. 7. It can bee seen that even using\n50 epochs, reducing training time in half, we would have got\nsimilar results.\nTo test if increased sample size by combining hetero-\ngeneous data sources would have an impact on denoising\nperformance, we combined both datasets with 721 images for\ntraining and 100 for testing.\nFig. 7.\nTraining and validation loss from 100 epochs using a batchsize of 10\nFig. 8.\nDenoising performance of CNN DAE on combined dataset, top row\nshows real images, second row is noisier version with minimal noise, third\nrow is denoising result of NL means, fourth rows shows results of median\n\ufb01lter, \ufb01fth row is results of using smaller dataset (300 training samples) with\nCNN DAE, sixth row is the results of CNN DAE on larger combined dataset.\nDenoising results on three randomly chosen test images\nfrom combined dataset are shown in Fig. 8 and Table III.\nTable III shows that CNN DAE performs better than NL\nmeans and median \ufb01lter. Increasing sample size marginally\nenhanced the denoising performance.\nTo test the limits of CNN DAEs denoising performance, we\nTABLE III.\nCOMPARING MEAN SSIM SCORES USING DIFFERENT\nDENOISING FILTERS\nImage type\nSSIM\nNoisy\n0.63\nNL means\n0.62\nMedian \ufb01lter\n0.80\nCNN DAE(a)\n0.89\nCNN DAE(b)\n0.90\nCNN DAE(a) is denoising performance using smaller dataset and CNN DAE(b) is\ndenoising performance on same images using the combined dataset.\nused rest of the noisy datasets with varying noise generative\npatterns and noise levels. Images with high corruption levels\nare barely visible to human eye, so denoising performance\non those is of interest. Denoising results along with noisy and\nnoiseless images on varying levels of Gaussian noise are shown\nin Fig. 9.\nFig. 9.\nDenoising performance of CNN DAE on different Gaussian noise\npatterns. Top row shows original images, second row is noisy images with\nnoise levels of \u00b5 = 0, \u03c3 = 1, p = 0.5, third row shows denoising results,\nfourth row shows corruption with p = 0.2, \u03c3 = 5, \ufb01fth row is denoised\nimages using CNN DAE, sixth and seventh rows shows noisy and denoised\nimages corrupted with p = 0.2, \u03c3 = 10.\nIt can be seen that as noise level increases, this simple\nnetwork has trouble reconstructing original signal. However,\neven when the image is not visible to human eye, this network\nis successful in partial generation of real images. Using a more\ncomplex deeper model, or by increasing number of training\nsamples and number of epochs might help.\nPerformance of CNN DAE was tested on images corrupted\nusing Poisson noise with p = 0.2, \u03bb = 1 and \u03bb = 5. Denoising\nresults are shown in Fig. 10.\nFig. 10.\nCNN DAE performance on Poisson corrupted images. Top row\nshows images corrupted with p = 0.2, \u03bb = 1 with second row showing\ndenoised results using CNN DAE. Third and fourth rows show noisy and\ndenoised images corrupted with p = 0.2, \u03bb = 5.\nTable IV shows comparison of CNN DAE with median\n\ufb01lter and NL means for denoising performance on varying\nnoise levels and types. It is clear that CNN DAE outperforms\nboth denoising methods by a wide margin, which increases as\nnoise level increases.\nTABLE IV.\nCOMPARISON USING MEAN SSIM FOR DIFFERENT NOISE\nPATTERNS AND LEVELS\nImage type\np = 0.5\nsd = 5\nsd = 10\nP oisson, \u03bb = 5\nNoisy\n0.10\n0.03\n0.01\n0.33\nNL means\n0.25\n0.03\n0.01\n0.15\nMedian \ufb01lter\n0.28\n0.11\n0.03\n0.17\nCNN DAE\n0.70\n0.55\n0.39\n0.85\np = 0.5 represents 50% corrupted images with \u00b5 = 0, \u03c3 = 1, sd = 5 are images\ncorrupted with p = 0.2, \u00b5 = 0, \u03c3 = 5, sd = 10 are corrupted with\np = 0.2, \u00b5 = 0, \u03c3 = 10 and P oisson, \u03bb = 5 are corrupted with a Poisson noise\nusing \u03bb = 5\nAlso, as the noise level is increased the network has trouble\nconverging. Fig. 11 shows the loss curves for Gaussian noise\nwith \u00b5 = 0, p = 0.2, \u03c3 = 10. Even using 100 epochs, model\nhas not converged.\nFig. 11.\nModel having trouble converging at higher noise levels, no decrease\nin validation errors can be seen with increasing number of epochs.\nV.\nCONCLUSION\nWe have shown that denoising autoencoder constructed\nusing convolutional layers can be used for ef\ufb01cient denoising\nof medical images. In contrary to the belief, we have shown\nthat good denoising performance can be achieved using small\ntraining datasets, training samples as few as 300 are enough\nfor good performance.\nOur future work would focus on \ufb01nding an optimal ar-\nchitecture for small sample denoising. We would like to\ninvestigate similar architectures on high resolution images and\nthe use of other image denoising methods such as singular\nvalue decomposition (SVD) and median \ufb01lters for image pre-\nprocessing before using CNN DAE, in hope of boosting\ndenoising performance. It would also be of interest, if given\nonly a few images can we combine them with other readily\navailable images from datasets such as ImageNet [8] for better\ndenoising performance by increasing training sample size.\nREFERENCES\n[1]\nAgostinelli, Forest, Michael R. Anderson, and Honglak Lee. \u201dAdaptive\nmulti-column deep neural networks with application to robust image\ndenoising.\u201d Advances in Neural Information Processing Systems. 2013.\n[2]\nBengio, Yoshua, et al. \u201dGreedy layer-wise training of deep networks.\u201d\nAdvances in neural information processing systems 19 (2007): 153.\n[3]\nBuades, Antoni, Bartomeu Coll, and Jean-Michel Morel. \u201dA review of\nimage denoising algorithms, with a new one.\u201d Multiscale Modeling and\nSimulation 4.2 (2005): 490-530.\n[4]\nBurger, Harold C., Christian J. Schuler, and Stefan Harmeling. \u201dImage\ndenoising: Can plain neural networks compete with BM3D?.\u201d Computer\nVision and Pattern Recognition (CVPR), 2012 IEEE Conference on.\nIEEE, 2012.\n[5]\nCho, Kyunghyun. \u201dBoltzmann machines and denoising autoencoders for\nimage denoising.\u201d arXiv preprint arXiv:1301.3468 (2013).\n[6]\nCoifman, Ronald R., and David L. Donoho. Translation-invariant de-\nnoising. Springer New York, 1995.\n[7]\nDabov, Kostadin, et al. \u201dImage denoising by sparse 3-D transform-\ndomain collaborative \ufb01ltering.\u201d IEEE Transactions on image processing\n16.8 (2007): 2080-2095.\n[8]\nDeng, Jia, et al. \u201dImagenet: A large-scale hierarchical image database.\u201d\nComputer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE\nConference on. IEEE, 2009.\n[9]\nElad, Michael, and Michal Aharon. \u201dImage denoising via sparse and\nredundant representations over learned dictionaries.\u201d IEEE Transactions\non Image processing 15.12 (2006): 3736-3745.\n[10]\nGlorot, Xavier, Antoine Bordes, and Yoshua Bengio. \u201dDeep Sparse\nRecti\ufb01er Neural Networks.\u201d Aistats. Vol. 15. No. 106. 2011.\n[11]\nHinton, Geoffrey, et al. \u201dDeep neural networks for acoustic modeling\nin speech recognition: The shared views of four research groups.\u201d IEEE\nSignal Processing Magazine 29.6 (2012): 82-97.\n[12]\nJain, Viren, and Sebastian Seung. \u201dNatural image denoising with\nconvolutional networks.\u201d Advances in Neural Information Processing\nSystems. 2009.\n[13]\nJ Suckling et al (1994): The Mammographic Image Analysis Society\nDigital Mammogram Database Exerpta Medica. International Congress\nSeries 1069 pp375-378.\n[14]\nKrizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. \u201dImagenet\nclassi\ufb01cation with deep convolutional neural networks.\u201d Advances in\nneural information processing systems. 2012.\n[15]\nMairal, Julien, et al. \u201dOnline dictionary learning for sparse coding.\u201d\nProceedings of the 26th annual international conference on machine\nlearning. ACM, 2009.\n[16]\nMasci, Jonathan, et al. \u201dStacked convolutional auto-encoders for hierar-\nchical feature extraction.\u201d International Conference on Arti\ufb01cial Neural\nNetworks. Springer Berlin Heidelberg, 2011.\n[17]\nOlshausen, Bruno A., and David J. Field. \u201dSparse coding with an\novercomplete basis set: A strategy employed by V1?.\u201d Vision research\n37.23 (1997): 3311-3325.\n[18]\nPerona, Pietro, and Jitendra Malik. \u201dScale-space and edge detection\nusing anisotropic diffusion.\u201d IEEE Transactions on pattern analysis and\nmachine intelligence 12.7 (1990): 629-639.\n[19]\nPortilla, Javier, et al. \u201dImage denoising using scale mixtures of Gaus-\nsians in the wavelet domain.\u201d IEEE Transactions on Image processing\n12.11 (2003): 1338-1351.\n[20]\nRudin, Leonid I., and Stanley Osher. \u201dTotal variation based image\nrestoration with free local constraints.\u201d Image Processing, 1994. Pro-\nceedings. ICIP-94., IEEE International Conference. Vol. 1. IEEE, 1994.\n[21]\nSanches, Joo M., Jacinto C. Nascimento, and Jorge S. Marques.\n\u201dMedical image noise reduction using the SylvesterLyapunov equation.\u201d\nIEEE transactions on image processing 17.9 (2008): 1522-1539.\n[22]\nSubakan, Ozlem, et al. \u201dFeature preserving image smoothing using a\ncontinuous mixture of tensors.\u201d 2007 IEEE 11th International Conference\non Computer Vision. IEEE, 2007.\n[23]\nSutskever, Ilya, Oriol Vinyals, and Quoc V. Le. \u201dSequence to sequence\nlearning with neural networks.\u201d Advances in neural information process-\ning systems. 2014.\n[24]\nVincent, Pascal, et al. \u201dExtracting and composing robust features\nwith denoising autoencoders.\u201d Proceedings of the 25th international\nconference on Machine learning. ACM, 2008.\n[25]\nVincent, Pascal, et al. \u201dStacked denoising autoencoders: Learning useful\nrepresentations in a deep network with a local denoising criterion.\u201d\nJournal of Machine Learning Research 11.Dec (2010): 3371-3408.\n[26]\nWang, Ching-Wei, et al. \u201dA benchmark for comparison of dental\nradiography analysis algorithms.\u201d Medical image analysis 31 (2016): 63-\n76.\n[27]\nWang, Zhou, et al. \u201dImage quality assessment: from error visibility\nto structural similarity.\u201d IEEE transactions on image processing 13.4\n(2004): 600-612.\n[28]\nXie, Junyuan, Linli Xu, and Enhong Chen. \u201dImage denoising and\ninpainting with deep neural networks.\u201d Advances in Neural Information\nProcessing Systems. 2012.\n[29]\nYaroslavsky, Leonid P., Karen O. Egiazarian, and Jaakko T. Astola.\n\u201dTransform domain image restoration methods: review, comparison, and\ninterpretation.\u201d Photonics West 2001-Electronic Imaging. International\nSociety for Optics and Photonics, 2001.\n[30]\nZhang, Dapeng, and Zhou Wang. \u201dImage information restoration based\non long-range correlation.\u201d IEEE Transactions on Circuits and Systems\nfor Video Technology 12.5 (2002): 331-341.\n[31]\nFranois\nChollet,\nkeras,\n(2015),\nGitHub\nrepository,\nhttps://github.com/fchollet/keras\n[32]\nDeep learning tutorial, Stanford University.Autoencoders. Available:\nhttp://u\ufb02dl.stanford.edu/tutorial/unsupervised/Autoencoders/\n[33]\nIntroduction Auto-Encoder, wikidocs.Stacked Denoising Auto-Encoder\n(SdA). Available: https://wikidocs.net/3413\n",
        "sentence": " Gondara [4] presents an application of such denoising system to remove noise from medical images.",
        "context": "Medical image denoising using convolutional\ndenoising autoencoders\nLovedeep Gondara\nDepartment of Computer Science\nSimon Fraser University\nlgondara@sfu.ca\nAbstract\u2014Image denoising is an important pre-processing step\nKeywords\u2014Image denoising, denoising autoencoder, convolu-\ntional autoencoder\nI.\nINTRODUCTION\nMedical imaging including X-rays, Magnetic Resonance\nImaging (MRI), Computer Tomography (CT), ultrasound etc.\nAdvances in neural information processing systems 19 (2007): 153.\n[3]\nBuades, Antoni, Bartomeu Coll, and Jean-Michel Morel. \u201dA review of\nimage denoising algorithms, with a new one.\u201d Multiscale Modeling and\nSimulation 4.2 (2005): 490-530.\n[4]"
    },
    {
        "title": "A Practical Guide to Training Restricted Boltzmann Machines, pages 599\u2013619",
        "author": [
            "G.E. Hinton"
        ],
        "venue": null,
        "citeRegEx": "5",
        "shortCiteRegEx": "5",
        "year": 2012,
        "abstract": "",
        "full_text": "",
        "sentence": " The network was trained by using single-step contrastive divergence (CD-1) [5].",
        "context": null
    },
    {
        "title": "Adam: A method for stochastic optimization",
        "author": [
            "D.P. Kingma",
            "J. Ba"
        ],
        "venue": "CoRR, abs/1412.6980,",
        "citeRegEx": "6",
        "shortCiteRegEx": "6",
        "year": 2014,
        "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization\nof stochastic objective functions, based on adaptive estimates of lower-order\nmoments. The method is straightforward to implement, is computationally\nefficient, has little memory requirements, is invariant to diagonal rescaling\nof the gradients, and is well suited for problems that are large in terms of\ndata and/or parameters. The method is also appropriate for non-stationary\nobjectives and problems with very noisy and/or sparse gradients. The\nhyper-parameters have intuitive interpretations and typically require little\ntuning. Some connections to related algorithms, on which Adam was inspired, are\ndiscussed. We also analyze the theoretical convergence properties of the\nalgorithm and provide a regret bound on the convergence rate that is comparable\nto the best known results under the online convex optimization framework.\nEmpirical results demonstrate that Adam works well in practice and compares\nfavorably to other stochastic optimization methods. Finally, we discuss AdaMax,\na variant of Adam based on the infinity norm.",
        "full_text": "Published as a conference paper at ICLR 2015\nADAM: A METHOD FOR STOCHASTIC OPTIMIZATION\nDiederik P. Kingma*\nUniversity of Amsterdam, OpenAI\ndpkingma@openai.com\nJimmy Lei Ba\u2217\nUniversity of Toronto\njimmy@psi.utoronto.ca\nABSTRACT\nWe introduce Adam, an algorithm for \ufb01rst-order gradient-based optimization of\nstochastic objective functions, based on adaptive estimates of lower-order mo-\nments. The method is straightforward to implement, is computationally ef\ufb01cient,\nhas little memory requirements, is invariant to diagonal rescaling of the gradients,\nand is well suited for problems that are large in terms of data and/or parameters.\nThe method is also appropriate for non-stationary objectives and problems with\nvery noisy and/or sparse gradients. The hyper-parameters have intuitive interpre-\ntations and typically require little tuning. Some connections to related algorithms,\non which Adam was inspired, are discussed. We also analyze the theoretical con-\nvergence properties of the algorithm and provide a regret bound on the conver-\ngence rate that is comparable to the best known results under the online convex\noptimization framework. Empirical results demonstrate that Adam works well in\npractice and compares favorably to other stochastic optimization methods. Finally,\nwe discuss AdaMax, a variant of Adam based on the in\ufb01nity norm.\n1\nINTRODUCTION\nStochastic gradient-based optimization is of core practical importance in many \ufb01elds of science and\nengineering. Many problems in these \ufb01elds can be cast as the optimization of some scalar parameter-\nized objective function requiring maximization or minimization with respect to its parameters. If the\nfunction is differentiable w.r.t. its parameters, gradient descent is a relatively ef\ufb01cient optimization\nmethod, since the computation of \ufb01rst-order partial derivatives w.r.t. all the parameters is of the same\ncomputational complexity as just evaluating the function. Often, objective functions are stochastic.\nFor example, many objective functions are composed of a sum of subfunctions evaluated at different\nsubsamples of data; in this case optimization can be made more ef\ufb01cient by taking gradient steps\nw.r.t. individual subfunctions, i.e. stochastic gradient descent (SGD) or ascent. SGD proved itself\nas an ef\ufb01cient and effective optimization method that was central in many machine learning success\nstories, such as recent advances in deep learning (Deng et al., 2013; Krizhevsky et al., 2012; Hinton\n& Salakhutdinov, 2006; Hinton et al., 2012a; Graves et al., 2013). Objectives may also have other\nsources of noise than data subsampling, such as dropout (Hinton et al., 2012b) regularization. For\nall such noisy objectives, ef\ufb01cient stochastic optimization techniques are required. The focus of this\npaper is on the optimization of stochastic objectives with high-dimensional parameters spaces. In\nthese cases, higher-order optimization methods are ill-suited, and discussion in this paper will be\nrestricted to \ufb01rst-order methods.\nWe propose Adam, a method for ef\ufb01cient stochastic optimization that only requires \ufb01rst-order gra-\ndients with little memory requirement. The method computes individual adaptive learning rates for\ndifferent parameters from estimates of \ufb01rst and second moments of the gradients; the name Adam\nis derived from adaptive moment estimation. Our method is designed to combine the advantages\nof two recently popular methods: AdaGrad (Duchi et al., 2011), which works well with sparse gra-\ndients, and RMSProp (Tieleman & Hinton, 2012), which works well in on-line and non-stationary\nsettings; important connections to these and other stochastic optimization methods are clari\ufb01ed in\nsection 5. Some of Adam\u2019s advantages are that the magnitudes of parameter updates are invariant to\nrescaling of the gradient, its stepsizes are approximately bounded by the stepsize hyperparameter,\nit does not require a stationary objective, it works with sparse gradients, and it naturally performs a\nform of step size annealing.\n\u2217Equal contribution. Author ordering determined by coin \ufb02ip over a Google Hangout.\n1\narXiv:1412.6980v9  [cs.LG]  30 Jan 2017\nPublished as a conference paper at ICLR 2015\nAlgorithm 1: Adam, our proposed algorithm for stochastic optimization. See section 2 for details,\nand for a slightly more ef\ufb01cient (but less clear) order of computation. g2\nt indicates the elementwise\nsquare gt \u2299gt. Good default settings for the tested machine learning problems are \u03b1 = 0.001,\n\u03b21 = 0.9, \u03b22 = 0.999 and \u03f5 = 10\u22128. All operations on vectors are element-wise. With \u03b2t\n1 and \u03b2t\n2\nwe denote \u03b21 and \u03b22 to the power t.\nRequire: \u03b1: Stepsize\nRequire: \u03b21, \u03b22 \u2208[0, 1): Exponential decay rates for the moment estimates\nRequire: f(\u03b8): Stochastic objective function with parameters \u03b8\nRequire: \u03b80: Initial parameter vector\nm0 \u21900 (Initialize 1st moment vector)\nv0 \u21900 (Initialize 2nd moment vector)\nt \u21900 (Initialize timestep)\nwhile \u03b8t not converged do\nt \u2190t + 1\ngt \u2190\u2207\u03b8ft(\u03b8t\u22121) (Get gradients w.r.t. stochastic objective at timestep t)\nmt \u2190\u03b21 \u00b7 mt\u22121 + (1 \u2212\u03b21) \u00b7 gt (Update biased \ufb01rst moment estimate)\nvt \u2190\u03b22 \u00b7 vt\u22121 + (1 \u2212\u03b22) \u00b7 g2\nt (Update biased second raw moment estimate)\nbmt \u2190mt/(1 \u2212\u03b2t\n1) (Compute bias-corrected \ufb01rst moment estimate)\nbvt \u2190vt/(1 \u2212\u03b2t\n2) (Compute bias-corrected second raw moment estimate)\n\u03b8t \u2190\u03b8t\u22121 \u2212\u03b1 \u00b7 bmt/(\u221abvt + \u03f5) (Update parameters)\nend while\nreturn \u03b8t (Resulting parameters)\nIn section 2 we describe the algorithm and the properties of its update rule. Section 3 explains\nour initialization bias correction technique, and section 4 provides a theoretical analysis of Adam\u2019s\nconvergence in online convex programming. Empirically, our method consistently outperforms other\nmethods for a variety of models and datasets, as shown in section 6. Overall, we show that Adam is\na versatile algorithm that scales to large-scale high-dimensional machine learning problems.\n2\nALGORITHM\nSee algorithm 1 for pseudo-code of our proposed algorithm Adam. Let f(\u03b8) be a noisy objec-\ntive function: a stochastic scalar function that is differentiable w.r.t. parameters \u03b8. We are in-\nterested in minimizing the expected value of this function, E[f(\u03b8)] w.r.t. its parameters \u03b8. With\nf1(\u03b8), ..., , fT (\u03b8) we denote the realisations of the stochastic function at subsequent timesteps\n1, ..., T. The stochasticity might come from the evaluation at random subsamples (minibatches)\nof datapoints, or arise from inherent function noise. With gt = \u2207\u03b8ft(\u03b8) we denote the gradient, i.e.\nthe vector of partial derivatives of ft, w.r.t \u03b8 evaluated at timestep t.\nThe algorithm updates exponential moving averages of the gradient (mt) and the squared gradient\n(vt) where the hyper-parameters \u03b21, \u03b22 \u2208[0, 1) control the exponential decay rates of these moving\naverages. The moving averages themselves are estimates of the 1st moment (the mean) and the\n2nd raw moment (the uncentered variance) of the gradient. However, these moving averages are\ninitialized as (vectors of) 0\u2019s, leading to moment estimates that are biased towards zero, especially\nduring the initial timesteps, and especially when the decay rates are small (i.e. the \u03b2s are close to 1).\nThe good news is that this initialization bias can be easily counteracted, resulting in bias-corrected\nestimates bmt and bvt. See section 3 for more details.\nNote that the ef\ufb01ciency of algorithm 1 can, at the expense of clarity, be improved upon by changing\nthe order of computation, e.g. by replacing the last three lines in the loop with the following lines:\n\u03b1t = \u03b1 \u00b7\np\n1 \u2212\u03b2t\n2/(1 \u2212\u03b2t\n1) and \u03b8t \u2190\u03b8t\u22121 \u2212\u03b1t \u00b7 mt/(\u221avt + \u02c6\u03f5).\n2.1\nADAM\u2019S UPDATE RULE\nAn important property of Adam\u2019s update rule is its careful choice of stepsizes. Assuming \u03f5 = 0, the\neffective step taken in parameter space at timestep t is \u2206t = \u03b1 \u00b7 bmt/\u221abvt. The effective stepsize has\ntwo upper bounds: |\u2206t| \u2264\u03b1 \u00b7 (1 \u2212\u03b21)/\u221a1 \u2212\u03b22 in the case (1 \u2212\u03b21) > \u221a1 \u2212\u03b22, and |\u2206t| \u2264\u03b1\n2\nPublished as a conference paper at ICLR 2015\notherwise. The \ufb01rst case only happens in the most severe case of sparsity: when a gradient has\nbeen zero at all timesteps except at the current timestep. For less sparse cases, the effective stepsize\nwill be smaller. When (1 \u2212\u03b21) = \u221a1 \u2212\u03b22 we have that | bmt/\u221abvt| < 1 therefore |\u2206t| < \u03b1. In\nmore common scenarios, we will have that bmt/\u221abvt \u2248\u00b11 since |E[g]/\np\nE[g2]| \u22641. The effective\nmagnitude of the steps taken in parameter space at each timestep are approximately bounded by\nthe stepsize setting \u03b1, i.e., |\u2206t| \u2a85\u03b1. This can be understood as establishing a trust region around\nthe current parameter value, beyond which the current gradient estimate does not provide suf\ufb01cient\ninformation. This typically makes it relatively easy to know the right scale of \u03b1 in advance. For\nmany machine learning models, for instance, we often know in advance that good optima are with\nhigh probability within some set region in parameter space; it is not uncommon, for example, to\nhave a prior distribution over the parameters. Since \u03b1 sets (an upper bound of) the magnitude of\nsteps in parameter space, we can often deduce the right order of magnitude of \u03b1 such that optima\ncan be reached from \u03b80 within some number of iterations. With a slight abuse of terminology,\nwe will call the ratio bmt/\u221abvt the signal-to-noise ratio (SNR). With a smaller SNR the effective\nstepsize \u2206t will be closer to zero. This is a desirable property, since a smaller SNR means that\nthere is greater uncertainty about whether the direction of bmt corresponds to the direction of the true\ngradient. For example, the SNR value typically becomes closer to 0 towards an optimum, leading\nto smaller effective steps in parameter space: a form of automatic annealing. The effective stepsize\n\u2206t is also invariant to the scale of the gradients; rescaling the gradients g with factor c will scale bmt\nwith a factor c and bvt with a factor c2, which cancel out: (c \u00b7 bmt)/(\u221a\nc2 \u00b7 bvt) = bmt/\u221abvt.\n3\nINITIALIZATION BIAS CORRECTION\nAs explained in section 2, Adam utilizes initialization bias correction terms. We will here derive\nthe term for the second moment estimate; the derivation for the \ufb01rst moment estimate is completely\nanalogous. Let g be the gradient of the stochastic objective f, and we wish to estimate its second\nraw moment (uncentered variance) using an exponential moving average of the squared gradient,\nwith decay rate \u03b22. Let g1, ..., gT be the gradients at subsequent timesteps, each a draw from an\nunderlying gradient distribution gt \u223cp(gt). Let us initialize the exponential moving average as\nv0 = 0 (a vector of zeros). First note that the update at timestep t of the exponential moving average\nvt = \u03b22 \u00b7 vt\u22121 + (1 \u2212\u03b22) \u00b7 g2\nt (where g2\nt indicates the elementwise square gt \u2299gt) can be written as\na function of the gradients at all previous timesteps:\nvt = (1 \u2212\u03b22)\nt\nX\ni=1\n\u03b2t\u2212i\n2\n\u00b7 g2\ni\n(1)\nWe wish to know how E[vt], the expected value of the exponential moving average at timestep t,\nrelates to the true second moment E[g2\nt ], so we can correct for the discrepancy between the two.\nTaking expectations of the left-hand and right-hand sides of eq. (1):\nE[vt] = E\n\"\n(1 \u2212\u03b22)\nt\nX\ni=1\n\u03b2t\u2212i\n2\n\u00b7 g2\ni\n#\n(2)\n= E[g2\nt ] \u00b7 (1 \u2212\u03b22)\nt\nX\ni=1\n\u03b2t\u2212i\n2\n+ \u03b6\n(3)\n= E[g2\nt ] \u00b7 (1 \u2212\u03b2t\n2) + \u03b6\n(4)\nwhere \u03b6 = 0 if the true second moment E[g2\ni ] is stationary; otherwise \u03b6 can be kept small since\nthe exponential decay rate \u03b21 can (and should) be chosen such that the exponential moving average\nassigns small weights to gradients too far in the past. What is left is the term (1 \u2212\u03b2t\n2) which is\ncaused by initializing the running average with zeros. In algorithm 1 we therefore divide by this\nterm to correct the initialization bias.\nIn case of sparse gradients, for a reliable estimate of the second moment one needs to average over\nmany gradients by chosing a small value of \u03b22; however it is exactly this case of small \u03b22 where a\nlack of initialisation bias correction would lead to initial steps that are much larger.\n3\nPublished as a conference paper at ICLR 2015\n4\nCONVERGENCE ANALYSIS\nWe analyze the convergence of Adam using the online learning framework proposed in (Zinkevich,\n2003). Given an arbitrary, unknown sequence of convex cost functions f1(\u03b8), f2(\u03b8),..., fT (\u03b8). At\neach time t, our goal is to predict the parameter \u03b8t and evaluate it on a previously unknown cost\nfunction ft. Since the nature of the sequence is unknown in advance, we evaluate our algorithm\nusing the regret, that is the sum of all the previous difference between the online prediction ft(\u03b8t)\nand the best \ufb01xed point parameter ft(\u03b8\u2217) from a feasible set X for all the previous steps. Concretely,\nthe regret is de\ufb01ned as:\nR(T) =\nT\nX\nt=1\n[ft(\u03b8t) \u2212ft(\u03b8\u2217)]\n(5)\nwhere \u03b8\u2217= arg min\u03b8\u2208X\nPT\nt=1 ft(\u03b8). We show Adam has O(\n\u221a\nT) regret bound and a proof is given\nin the appendix. Our result is comparable to the best known bound for this general convex online\nlearning problem. We also use some de\ufb01nitions simplify our notation, where gt \u225c\u2207ft(\u03b8t) and gt,i\nas the ith element. We de\ufb01ne g1:t,i \u2208Rt as a vector that contains the ith dimension of the gradients\nover all iterations till t, g1:t,i = [g1,i, g2,i, \u00b7 \u00b7 \u00b7 , gt,i]. Also, we de\ufb01ne \u03b3 \u225c\n\u03b22\n1\n\u221a\u03b22 . Our following\ntheorem holds when the learning rate \u03b1t is decaying at a rate of t\u22121\n2 and \ufb01rst moment running\naverage coef\ufb01cient \u03b21,t decay exponentially with \u03bb, that is typically close to 1, e.g. 1 \u221210\u22128.\nTheorem 4.1. Assume that the function ft has bounded gradients, \u2225\u2207ft(\u03b8)\u22252 \u2264G, \u2225\u2207ft(\u03b8)\u2225\u221e\u2264\nG\u221efor all \u03b8 \u2208Rd and distance between any \u03b8t generated by Adam is bounded, \u2225\u03b8n \u2212\u03b8m\u22252 \u2264D,\n\u2225\u03b8m \u2212\u03b8n\u2225\u221e\u2264D\u221efor any m, n \u2208{1, ..., T}, and \u03b21, \u03b22 \u2208[0, 1) satisfy\n\u03b22\n1\n\u221a\u03b22 < 1. Let \u03b1t =\n\u03b1\n\u221a\nt\nand \u03b21,t = \u03b21\u03bbt\u22121, \u03bb \u2208(0, 1). Adam achieves the following guarantee, for all T \u22651.\nR(T) \u2264\nD2\n2\u03b1(1 \u2212\u03b21)\nd\nX\ni=1\np\nTbvT,i+\n\u03b1(1 + \u03b21)G\u221e\n(1 \u2212\u03b21)\u221a1 \u2212\u03b22(1 \u2212\u03b3)2\nd\nX\ni=1\n\u2225g1:T,i\u22252+\nd\nX\ni=1\nD2\n\u221eG\u221e\n\u221a1 \u2212\u03b22\n2\u03b1(1 \u2212\u03b21)(1 \u2212\u03bb)2\nOur Theorem 4.1 implies when the data features are sparse and bounded gradients, the sum-\nmation term can be much smaller than its upper bound Pd\ni=1 \u2225g1:T,i\u22252\n<< dG\u221e\n\u221a\nT and\nPd\ni=1\np\nTbvT,i << dG\u221e\n\u221a\nT, in particular if the class of function and data features are in the form of\nsection 1.2 in (Duchi et al., 2011). Their results for the expected value E[Pd\ni=1 \u2225g1:T,i\u22252] also apply\nto Adam. In particular, the adaptive method, such as Adam and Adagrad, can achieve O(log d\n\u221a\nT),\nan improvement over O(\n\u221a\ndT) for the non-adaptive method. Decaying \u03b21,t towards zero is impor-\ntant in our theoretical analysis and also matches previous empirical \ufb01ndings, e.g. (Sutskever et al.,\n2013) suggests reducing the momentum coef\ufb01cient in the end of training can improve convergence.\nFinally, we can show the average regret of Adam converges,\nCorollary 4.2. Assume that the function ft has bounded gradients, \u2225\u2207ft(\u03b8)\u22252 \u2264G, \u2225\u2207ft(\u03b8)\u2225\u221e\u2264\nG\u221efor all \u03b8 \u2208Rd and distance between any \u03b8t generated by Adam is bounded, \u2225\u03b8n \u2212\u03b8m\u22252 \u2264D,\n\u2225\u03b8m \u2212\u03b8n\u2225\u221e\u2264D\u221efor any m, n \u2208{1, ..., T}. Adam achieves the following guarantee, for all\nT \u22651.\nR(T)\nT\n= O( 1\n\u221a\nT\n)\nThis result can be obtained by using Theorem 4.1 and Pd\ni=1 \u2225g1:T,i\u22252 \u2264dG\u221e\n\u221a\nT.\nThus,\nlimT \u2192\u221e\nR(T )\nT\n= 0.\n5\nRELATED WORK\nOptimization methods bearing a direct relation to Adam are RMSProp (Tieleman & Hinton, 2012;\nGraves, 2013) and AdaGrad (Duchi et al., 2011); these relationships are discussed below. Other\nstochastic optimization methods include vSGD (Schaul et al., 2012), AdaDelta (Zeiler, 2012) and the\nnatural Newton method from Roux & Fitzgibbon (2010), all setting stepsizes by estimating curvature\n4\nPublished as a conference paper at ICLR 2015\nfrom \ufb01rst-order information. The Sum-of-Functions Optimizer (SFO) (Sohl-Dickstein et al., 2014)\nis a quasi-Newton method based on minibatches, but (unlike Adam) has memory requirements linear\nin the number of minibatch partitions of a dataset, which is often infeasible on memory-constrained\nsystems such as a GPU. Like natural gradient descent (NGD) (Amari, 1998), Adam employs a\npreconditioner that adapts to the geometry of the data, since bvt is an approximation to the diagonal\nof the Fisher information matrix (Pascanu & Bengio, 2013); however, Adam\u2019s preconditioner (like\nAdaGrad\u2019s) is more conservative in its adaption than vanilla NGD by preconditioning with the square\nroot of the inverse of the diagonal Fisher information matrix approximation.\nRMSProp:\nAn optimization method closely related to Adam is RMSProp (Tieleman & Hinton,\n2012). A version with momentum has sometimes been used (Graves, 2013). There are a few impor-\ntant differences between RMSProp with momentum and Adam: RMSProp with momentum gener-\nates its parameter updates using a momentum on the rescaled gradient, whereas Adam updates are\ndirectly estimated using a running average of \ufb01rst and second moment of the gradient. RMSProp\nalso lacks a bias-correction term; this matters most in case of a value of \u03b22 close to 1 (required in\ncase of sparse gradients), since in that case not correcting the bias leads to very large stepsizes and\noften divergence, as we also empirically demonstrate in section 6.4.\nAdaGrad:\nAn algorithm that works well for sparse gradients is AdaGrad (Duchi et al., 2011). Its\nbasic version updates parameters as \u03b8t+1 = \u03b8t \u2212\u03b1 \u00b7 gt/\nqPt\ni=1 g2\nt . Note that if we choose \u03b22 to be\nin\ufb01nitesimally close to 1 from below, then lim\u03b22\u21921 bvt = t\u22121 \u00b7 Pt\ni=1 g2\nt . AdaGrad corresponds to a\nversion of Adam with \u03b21 = 0, in\ufb01nitesimal (1 \u2212\u03b22) and a replacement of \u03b1 by an annealed version\n\u03b1t = \u03b1 \u00b7 t\u22121/2, namely \u03b8t \u2212\u03b1 \u00b7 t\u22121/2 \u00b7 bmt/\np\nlim\u03b22\u21921 bvt = \u03b8t \u2212\u03b1 \u00b7 t\u22121/2 \u00b7 gt/\nq\nt\u22121 \u00b7 Pt\ni=1 g2\nt =\n\u03b8t \u2212\u03b1 \u00b7 gt/\nqPt\ni=1 g2\nt . Note that this direct correspondence between Adam and Adagrad does\nnot hold when removing the bias-correction terms; without bias correction, like in RMSProp, a \u03b22\nin\ufb01nitesimally close to 1 would lead to in\ufb01nitely large bias, and in\ufb01nitely large parameter updates.\n6\nEXPERIMENTS\nTo empirically evaluate the proposed method, we investigated different popular machine learning\nmodels, including logistic regression, multilayer fully connected neural networks and deep convolu-\ntional neural networks. Using large models and datasets, we demonstrate Adam can ef\ufb01ciently solve\npractical deep learning problems.\nWe use the same parameter initialization when comparing different optimization algorithms. The\nhyper-parameters, such as learning rate and momentum, are searched over a dense grid and the\nresults are reported using the best hyper-parameter setting.\n6.1\nEXPERIMENT: LOGISTIC REGRESSION\nWe evaluate our proposed method on L2-regularized multi-class logistic regression using the MNIST\ndataset. Logistic regression has a well-studied convex objective, making it suitable for comparison\nof different optimizers without worrying about local minimum issues. The stepsize \u03b1 in our logistic\nregression experiments is adjusted by 1/\n\u221a\nt decay, namely \u03b1t =\n\u03b1\n\u221a\nt that matches with our theorat-\nical prediction from section 4. The logistic regression classi\ufb01es the class label directly on the 784\ndimension image vectors. We compare Adam to accelerated SGD with Nesterov momentum and\nAdagrad using minibatch size of 128. According to Figure 1, we found that the Adam yields similar\nconvergence as SGD with momentum and both converge faster than Adagrad.\nAs discussed in (Duchi et al., 2011), Adagrad can ef\ufb01ciently deal with sparse features and gradi-\nents as one of its main theoretical results whereas SGD is low at learning rare features. Adam with\n1/\n\u221a\nt decay on its stepsize should theoratically match the performance of Adagrad. We examine the\nsparse feature problem using IMDB movie review dataset from (Maas et al., 2011). We pre-process\nthe IMDB movie reviews into bag-of-words (BoW) feature vectors including the \ufb01rst 10,000 most\nfrequent words. The 10,000 dimension BoW feature vector for each review is highly sparse. As sug-\ngested in (Wang & Manning, 2013), 50% dropout noise can be applied to the BoW features during\n5\nPublished as a conference paper at ICLR 2015\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\niterations over entire dataset\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\ntraining cost\nMNIST Logistic Regression\nAdaGrad\nSGDNesterov\nAdam\n0\n20\n40\n60\n80\n100\n120\n140\n160\niterations over entire dataset\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\ntraining cost\nIMDB BoW feature Logistic Regression\nAdagrad+dropout\nRMSProp+dropout\nSGDNesterov+dropout\nAdam+dropout\nFigure 1: Logistic regression training negative log likelihood on MNIST images and IMDB movie\nreviews with 10,000 bag-of-words (BoW) feature vectors.\ntraining to prevent over-\ufb01tting. In \ufb01gure 1, Adagrad outperforms SGD with Nesterov momentum\nby a large margin both with and without dropout noise. Adam converges as fast as Adagrad. The\nempirical performance of Adam is consistent with our theoretical \ufb01ndings in sections 2 and 4. Sim-\nilar to Adagrad, Adam can take advantage of sparse features and obtain faster convergence rate than\nnormal SGD with momentum.\n6.2\nEXPERIMENT: MULTI-LAYER NEURAL NETWORKS\nMulti-layer neural network are powerful models with non-convex objective functions. Although\nour convergence analysis does not apply to non-convex problems, we empirically found that Adam\noften outperforms other methods in such cases. In our experiments, we made model choices that are\nconsistent with previous publications in the area; a neural network model with two fully connected\nhidden layers with 1000 hidden units each and ReLU activation are used for this experiment with\nminibatch size of 128.\nFirst, we study different optimizers using the standard deterministic cross-entropy objective func-\ntion with L2 weight decay on the parameters to prevent over-\ufb01tting. The sum-of-functions (SFO)\nmethod (Sohl-Dickstein et al., 2014) is a recently proposed quasi-Newton method that works with\nminibatches of data and has shown good performance on optimization of multi-layer neural net-\nworks. We used their implementation and compared with Adam to train such models. Figure 2\nshows that Adam makes faster progress in terms of both the number of iterations and wall-clock\ntime. Due to the cost of updating curvature information, SFO is 5-10x slower per iteration com-\npared to Adam, and has a memory requirement that is linear in the number minibatches.\nStochastic regularization methods, such as dropout, are an effective way to prevent over-\ufb01tting and\noften used in practice due to their simplicity. SFO assumes deterministic subfunctions, and indeed\nfailed to converge on cost functions with stochastic regularization. We compare the effectiveness of\nAdam to other stochastic \ufb01rst order methods on multi-layer neural networks trained with dropout\nnoise. Figure 2 shows our results; Adam shows better convergence than other methods.\n6.3\nEXPERIMENT: CONVOLUTIONAL NEURAL NETWORKS\nConvolutional neural networks (CNNs) with several layers of convolution, pooling and non-linear\nunits have shown considerable success in computer vision tasks. Unlike most fully connected neural\nnets, weight sharing in CNNs results in vastly different gradients in different layers. A smaller\nlearning rate for the convolution layers is often used in practice when applying SGD. We show the\neffectiveness of Adam in deep CNNs. Our CNN architecture has three alternating stages of 5x5\nconvolution \ufb01lters and 3x3 max pooling with stride of 2 that are followed by a fully connected layer\nof 1000 recti\ufb01ed linear hidden units (ReLU\u2019s). The input image are pre-processed by whitening, and\n6\nPublished as a conference paper at ICLR 2015\n0\n50\n100\n150\n200\niterations over entire dataset\n10\n-2\n10\n-1\ntraining cost\nMNIST Multilayer Neural Network + dropout\nAdaGrad\nRMSProp\nSGDNesterov\nAdaDelta\nAdam\n(a)\n(b)\nFigure 2: Training of multilayer neural networks on MNIST images. (a) Neural networks using\ndropout stochastic regularization. (b) Neural networks with deterministic cost function. We compare\nwith the sum-of-functions (SFO) optimizer (Sohl-Dickstein et al., 2014)\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\niterations over entire dataset\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\ntraining cost\nCIFAR10 ConvNet First 3 Epoches\nAdaGrad\nAdaGrad+dropout\nSGDNesterov\nSGDNesterov+dropout\nAdam\nAdam+dropout\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\niterations over entire dataset\n10-4\n10-3\n10-2\n10-1\n100\n101\n102\ntraining cost\nCIFAR10 ConvNet\nAdaGrad\nAdaGrad+dropout\nSGDNesterov\nSGDNesterov+dropout\nAdam\nAdam+dropout\nFigure 3: Convolutional neural networks training cost. (left) Training cost for the \ufb01rst three epochs.\n(right) Training cost over 45 epochs. CIFAR-10 with c64-c64-c128-1000 architecture.\ndropout noise is applied to the input layer and fully connected layer. The minibatch size is also set\nto 128 similar to previous experiments.\nInterestingly, although both Adam and Adagrad make rapid progress lowering the cost in the initial\nstage of the training, shown in Figure 3 (left), Adam and SGD eventually converge considerably\nfaster than Adagrad for CNNs shown in Figure 3 (right). We notice the second moment estimate bvt\nvanishes to zeros after a few epochs and is dominated by the \u03f5 in algorithm 1. The second moment\nestimate is therefore a poor approximation to the geometry of the cost function in CNNs comparing\nto fully connected network from Section 6.2. Whereas, reducing the minibatch variance through\nthe \ufb01rst moment is more important in CNNs and contributes to the speed-up. As a result, Adagrad\nconverges much slower than others in this particular experiment. Though Adam shows marginal\nimprovement over SGD with momentum, it adapts learning rate scale for different layers instead of\nhand picking manually as in SGD.\n7\nPublished as a conference paper at ICLR 2015\n\u03b21=0\n\u03b21=0.9\n\u03b22=0.99\n\u03b22=0.999\n\u03b22=0.9999\n\u03b22=0.99\n\u03b22=0.999\n\u03b22=0.9999\n(a) after 10 epochs\n(b) after 100 epochs\nlog10(\u03b1)\nLoss\nFigure 4: Effect of bias-correction terms (red line) versus no bias correction terms (green line)\nafter 10 epochs (left) and 100 epochs (right) on the loss (y-axes) when learning a Variational Auto-\nEncoder (VAE) (Kingma & Welling, 2013), for different settings of stepsize \u03b1 (x-axes) and hyper-\nparameters \u03b21 and \u03b22.\n6.4\nEXPERIMENT: BIAS-CORRECTION TERM\nWe also empirically evaluate the effect of the bias correction terms explained in sections 2 and 3.\nDiscussed in section 5, removal of the bias correction terms results in a version of RMSProp (Tiele-\nman & Hinton, 2012) with momentum. We vary the \u03b21 and \u03b22 when training a variational auto-\nencoder (VAE) with the same architecture as in (Kingma & Welling, 2013) with a single hidden\nlayer with 500 hidden units with softplus nonlinearities and a 50-dimensional spherical Gaussian\nlatent variable. We iterated over a broad range of hyper-parameter choices, i.e. \u03b21 \u2208[0, 0.9] and\n\u03b22 \u2208[0.99, 0.999, 0.9999], and log10(\u03b1) \u2208[\u22125, ..., \u22121]. Values of \u03b22 close to 1, required for robust-\nness to sparse gradients, results in larger initialization bias; therefore we expect the bias correction\nterm is important in such cases of slow decay, preventing an adverse effect on optimization.\nIn Figure 4, values \u03b22 close to 1 indeed lead to instabilities in training when no bias correction term\nwas present, especially at \ufb01rst few epochs of the training. The best results were achieved with small\nvalues of (1\u2212\u03b22) and bias correction; this was more apparent towards the end of optimization when\ngradients tends to become sparser as hidden units specialize to speci\ufb01c patterns. In summary, Adam\nperformed equal or better than RMSProp, regardless of hyper-parameter setting.\n7\nEXTENSIONS\n7.1\nADAMAX\nIn Adam, the update rule for individual weights is to scale their gradients inversely proportional to a\n(scaled) L2 norm of their individual current and past gradients. We can generalize the L2 norm based\nupdate rule to a Lp norm based update rule. Such variants become numerically unstable for large\np. However, in the special case where we let p \u2192\u221e, a surprisingly simple and stable algorithm\nemerges; see algorithm 2. We\u2019ll now derive the algorithm. Let, in case of the Lp norm, the stepsize\nat time t be inversely proportional to v1/p\nt\n, where:\nvt = \u03b2p\n2vt\u22121 + (1 \u2212\u03b2p\n2)|gt|p\n(6)\n= (1 \u2212\u03b2p\n2)\nt\nX\ni=1\n\u03b2p(t\u2212i)\n2\n\u00b7 |gi|p\n(7)\n8\nPublished as a conference paper at ICLR 2015\nAlgorithm 2: AdaMax, a variant of Adam based on the in\ufb01nity norm. See section 7.1 for details.\nGood default settings for the tested machine learning problems are \u03b1 = 0.002, \u03b21 = 0.9 and\n\u03b22 = 0.999. With \u03b2t\n1 we denote \u03b21 to the power t. Here, (\u03b1/(1 \u2212\u03b2t\n1)) is the learning rate with the\nbias-correction term for the \ufb01rst moment. All operations on vectors are element-wise.\nRequire: \u03b1: Stepsize\nRequire: \u03b21, \u03b22 \u2208[0, 1): Exponential decay rates\nRequire: f(\u03b8): Stochastic objective function with parameters \u03b8\nRequire: \u03b80: Initial parameter vector\nm0 \u21900 (Initialize 1st moment vector)\nu0 \u21900 (Initialize the exponentially weighted in\ufb01nity norm)\nt \u21900 (Initialize timestep)\nwhile \u03b8t not converged do\nt \u2190t + 1\ngt \u2190\u2207\u03b8ft(\u03b8t\u22121) (Get gradients w.r.t. stochastic objective at timestep t)\nmt \u2190\u03b21 \u00b7 mt\u22121 + (1 \u2212\u03b21) \u00b7 gt (Update biased \ufb01rst moment estimate)\nut \u2190max(\u03b22 \u00b7 ut\u22121, |gt|) (Update the exponentially weighted in\ufb01nity norm)\n\u03b8t \u2190\u03b8t\u22121 \u2212(\u03b1/(1 \u2212\u03b2t\n1)) \u00b7 mt/ut (Update parameters)\nend while\nreturn \u03b8t (Resulting parameters)\nNote that the decay term is here equivalently parameterised as \u03b2p\n2 instead of \u03b22. Now let p \u2192\u221e,\nand de\ufb01ne ut = limp\u2192\u221e(vt)1/p, then:\nut = lim\np\u2192\u221e(vt)1/p = lim\np\u2192\u221e\n \n(1 \u2212\u03b2p\n2)\nt\nX\ni=1\n\u03b2p(t\u2212i)\n2\n\u00b7 |gi|p\n!1/p\n(8)\n= lim\np\u2192\u221e(1 \u2212\u03b2p\n2)1/p\n \nt\nX\ni=1\n\u03b2p(t\u2212i)\n2\n\u00b7 |gi|p\n!1/p\n(9)\n= lim\np\u2192\u221e\n \nt\nX\ni=1\n\u0010\n\u03b2(t\u2212i)\n2\n\u00b7 |gi|\n\u0011p\n!1/p\n(10)\n= max\n\u0000\u03b2t\u22121\n2\n|g1|, \u03b2t\u22122\n2\n|g2|, . . . , \u03b22|gt\u22121|, |gt|\n\u0001\n(11)\nWhich corresponds to the remarkably simple recursive formula:\nut = max(\u03b22 \u00b7 ut\u22121, |gt|)\n(12)\nwith initial value u0 = 0. Note that, conveniently enough, we don\u2019t need to correct for initialization\nbias in this case. Also note that the magnitude of parameter updates has a simpler bound with\nAdaMax than Adam, namely: |\u2206t| \u2264\u03b1.\n7.2\nTEMPORAL AVERAGING\nSince the last iterate is noisy due to stochastic approximation, better generalization performance is\noften achieved by averaging. Previously in Moulines & Bach (2011), Polyak-Ruppert averaging\n(Polyak & Juditsky, 1992; Ruppert, 1988) has been shown to improve the convergence of standard\nSGD, where \u00af\u03b8t = 1\nt\nPn\nk=1 \u03b8k. Alternatively, an exponential moving average over the parameters can\nbe used, giving higher weight to more recent parameter values. This can be trivially implemented\nby adding one line to the inner loop of algorithms 1 and 2: \u00af\u03b8t \u2190\u03b22 \u00b7 \u00af\u03b8t\u22121 +(1\u2212\u03b22)\u03b8t, with \u00af\u03b80 = 0.\nInitalization bias can again be corrected by the estimator b\u03b8t = \u00af\u03b8t/(1 \u2212\u03b2t\n2).\n8\nCONCLUSION\nWe have introduced a simple and computationally ef\ufb01cient algorithm for gradient-based optimiza-\ntion of stochastic objective functions. Our method is aimed towards machine learning problems with\n9\nPublished as a conference paper at ICLR 2015\nlarge datasets and/or high-dimensional parameter spaces. The method combines the advantages of\ntwo recently popular optimization methods: the ability of AdaGrad to deal with sparse gradients,\nand the ability of RMSProp to deal with non-stationary objectives. The method is straightforward\nto implement and requires little memory. The experiments con\ufb01rm the analysis on the rate of con-\nvergence in convex problems. Overall, we found Adam to be robust and well-suited to a wide range\nof non-convex optimization problems in the \ufb01eld machine learning.\n9\nACKNOWLEDGMENTS\nThis paper would probably not have existed without the support of Google Deepmind. We would\nlike to give special thanks to Ivo Danihelka, and Tom Schaul for coining the name Adam. Thanks to\nKai Fan from Duke University for spotting an error in the original AdaMax derivation. Experiments\nin this work were partly carried out on the Dutch national e-infrastructure with the support of SURF\nFoundation. Diederik Kingma is supported by the Google European Doctorate Fellowship in Deep\nLearning.\nREFERENCES\nAmari, Shun-Ichi. Natural gradient works ef\ufb01ciently in learning. Neural computation, 10(2):251\u2013276, 1998.\nDeng, Li, Li, Jinyu, Huang, Jui-Ting, Yao, Kaisheng, Yu, Dong, Seide, Frank, Seltzer, Michael, Zweig, Geoff,\nHe, Xiaodong, Williams, Jason, et al. Recent advances in deep learning for speech research at microsoft.\nICASSP 2013, 2013.\nDuchi, John, Hazan, Elad, and Singer, Yoram. Adaptive subgradient methods for online learning and stochastic\noptimization. The Journal of Machine Learning Research, 12:2121\u20132159, 2011.\nGraves, Alex. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.\nGraves, Alex, Mohamed, Abdel-rahman, and Hinton, Geoffrey. Speech recognition with deep recurrent neural\nnetworks. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on,\npp. 6645\u20136649. IEEE, 2013.\nHinton, G.E. and Salakhutdinov, R.R. Reducing the dimensionality of data with neural networks. Science, 313\n(5786):504\u2013507, 2006.\nHinton, Geoffrey, Deng, Li, Yu, Dong, Dahl, George E, Mohamed, Abdel-rahman, Jaitly, Navdeep, Senior,\nAndrew, Vanhoucke, Vincent, Nguyen, Patrick, Sainath, Tara N, et al. Deep neural networks for acoustic\nmodeling in speech recognition: The shared views of four research groups. Signal Processing Magazine,\nIEEE, 29(6):82\u201397, 2012a.\nHinton, Geoffrey E, Srivastava, Nitish, Krizhevsky, Alex, Sutskever, Ilya, and Salakhutdinov, Ruslan R. Im-\nproving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580,\n2012b.\nKingma, Diederik P and Welling, Max. Auto-Encoding Variational Bayes. In The 2nd International Conference\non Learning Representations (ICLR), 2013.\nKrizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classi\ufb01cation with deep convolutional\nneural networks. In Advances in neural information processing systems, pp. 1097\u20131105, 2012.\nMaas, Andrew L, Daly, Raymond E, Pham, Peter T, Huang, Dan, Ng, Andrew Y, and Potts, Christopher.\nLearning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association\nfor Computational Linguistics: Human Language Technologies-Volume 1, pp. 142\u2013150. Association for\nComputational Linguistics, 2011.\nMoulines, Eric and Bach, Francis R.\nNon-asymptotic analysis of stochastic approximation algorithms for\nmachine learning. In Advances in Neural Information Processing Systems, pp. 451\u2013459, 2011.\nPascanu, Razvan and Bengio, Yoshua.\nRevisiting natural gradient for deep networks.\narXiv preprint\narXiv:1301.3584, 2013.\nPolyak, Boris T and Juditsky, Anatoli B. Acceleration of stochastic approximation by averaging. SIAM Journal\non Control and Optimization, 30(4):838\u2013855, 1992.\n10\nPublished as a conference paper at ICLR 2015\nRoux, Nicolas L and Fitzgibbon, Andrew W. A fast natural newton method. In Proceedings of the 27th\nInternational Conference on Machine Learning (ICML-10), pp. 623\u2013630, 2010.\nRuppert, David. Ef\ufb01cient estimations from a slowly convergent robbins-monro process. Technical report,\nCornell University Operations Research and Industrial Engineering, 1988.\nSchaul, Tom, Zhang, Sixin, and LeCun, Yann. No more pesky learning rates. arXiv preprint arXiv:1206.1106,\n2012.\nSohl-Dickstein, Jascha, Poole, Ben, and Ganguli, Surya. Fast large-scale optimization by unifying stochas-\ntic gradient and quasi-newton methods. In Proceedings of the 31st International Conference on Machine\nLearning (ICML-14), pp. 604\u2013612, 2014.\nSutskever, Ilya, Martens, James, Dahl, George, and Hinton, Geoffrey. On the importance of initialization and\nmomentum in deep learning. In Proceedings of the 30th International Conference on Machine Learning\n(ICML-13), pp. 1139\u20131147, 2013.\nTieleman, T. and Hinton, G. Lecture 6.5 - RMSProp, COURSERA: Neural Networks for Machine Learning.\nTechnical report, 2012.\nWang, Sida and Manning, Christopher. Fast dropout training. In Proceedings of the 30th International Confer-\nence on Machine Learning (ICML-13), pp. 118\u2013126, 2013.\nZeiler, Matthew D. Adadelta: An adaptive learning rate method. arXiv preprint arXiv:1212.5701, 2012.\nZinkevich, Martin. Online convex programming and generalized in\ufb01nitesimal gradient ascent. 2003.\n11\nPublished as a conference paper at ICLR 2015\n10\nAPPENDIX\n10.1\nCONVERGENCE PROOF\nDe\ufb01nition 10.1. A function f : Rd \u2192R is convex if for all x, y \u2208Rd, for all \u03bb \u2208[0, 1],\n\u03bbf(x) + (1 \u2212\u03bb)f(y) \u2265f(\u03bbx + (1 \u2212\u03bb)y)\nAlso, notice that a convex function can be lower bounded by a hyperplane at its tangent.\nLemma 10.2. If a function f : Rd \u2192R is convex, then for all x, y \u2208Rd,\nf(y) \u2265f(x) + \u2207f(x)T (y \u2212x)\nThe above lemma can be used to upper bound the regret and our proof for the main theorem is\nconstructed by substituting the hyperplane with the Adam update rules.\nThe following two lemmas are used to support our main theorem. We also use some de\ufb01nitions sim-\nplify our notation, where gt \u225c\u2207ft(\u03b8t) and gt,i as the ith element. We de\ufb01ne g1:t,i \u2208Rt as a vector\nthat contains the ith dimension of the gradients over all iterations till t, g1:t,i = [g1,i, g2,i, \u00b7 \u00b7 \u00b7 , gt,i]\nLemma 10.3. Let gt = \u2207ft(\u03b8t) and g1:t be de\ufb01ned as above and bounded, \u2225gt\u22252 \u2264G, \u2225gt\u2225\u221e\u2264\nG\u221e. Then,\nT\nX\nt=1\ns\ng2\nt,i\nt\n\u22642G\u221e\u2225g1:T,i\u22252\nProof. We will prove the inequality using induction over T.\nThe base case for T = 1, we have\nq\ng2\n1,i \u22642G\u221e\u2225g1,i\u22252.\nFor the inductive step,\nT\nX\nt=1\ns\ng2\nt,i\nt\n=\nT \u22121\nX\nt=1\ns\ng2\nt,i\nt\n+\ns\ng2\nT,i\nT\n\u22642G\u221e\u2225g1:T \u22121,i\u22252 +\ns\ng2\nT,i\nT\n= 2G\u221e\nq\n\u2225g1:T,i\u22252\n2 \u2212g2\nT +\ns\ng2\nT,i\nT\nFrom, \u2225g1:T,i\u22252\n2 \u2212g2\nT,i +\ng4\nT,i\n4\u2225g1:T,i\u22252\n2 \u2265\u2225g1:T,i\u22252\n2 \u2212g2\nT,i, we can take square root of both side and\nhave,\nq\n\u2225g1:T,i\u22252\n2 \u2212g2\nT,i \u2264\u2225g1:T,i\u22252 \u2212\ng2\nT,i\n2\u2225g1:T,i\u22252\n\u2264\u2225g1:T,i\u22252 \u2212\ng2\nT,i\n2\np\nTG2\u221e\nRearrange the inequality and substitute the\nq\n\u2225g1:T,i\u22252\n2 \u2212g2\nT,i term,\nG\u221e\nq\n\u2225g1:T,i\u22252\n2 \u2212g2\nT +\ns\ng2\nT,i\nT\n\u22642G\u221e\u2225g1:T,i\u22252\n12\nPublished as a conference paper at ICLR 2015\nLemma 10.4. Let \u03b3 \u225c\n\u03b22\n1\n\u221a\u03b22 . For \u03b21, \u03b22 \u2208[0, 1) that satisfy\n\u03b22\n1\n\u221a\u03b22 < 1 and bounded gt, \u2225gt\u22252 \u2264G,\n\u2225gt\u2225\u221e\u2264G\u221e, the following inequality holds\nT\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n\u2264\n2\n1 \u2212\u03b3\n1\n\u221a1 \u2212\u03b22\n\u2225g1:T,i\u22252\nProof. Under the assumption,\n\u221a\n1\u2212\u03b2t\n2\n(1\u2212\u03b2t\n1)2 \u2264\n1\n(1\u2212\u03b21)2 . We can expand the last term in the summation\nusing the update rules in Algorithm 1,\nT\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n=\nT \u22121\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n+\np\n1 \u2212\u03b2T\n2\n(1 \u2212\u03b2T\n1 )2\n(PT\nk=1(1 \u2212\u03b21)\u03b2T \u2212k\n1\ngk,i)2\nq\nT PT\nj=1(1 \u2212\u03b22)\u03b2T \u2212j\n2\ng2\nj,i\n\u2264\nT \u22121\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n+\np\n1 \u2212\u03b2T\n2\n(1 \u2212\u03b2T\n1 )2\nT\nX\nk=1\nT((1 \u2212\u03b21)\u03b2T \u2212k\n1\ngk,i)2\nq\nT PT\nj=1(1 \u2212\u03b22)\u03b2T \u2212j\n2\ng2\nj,i\n\u2264\nT \u22121\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n+\np\n1 \u2212\u03b2T\n2\n(1 \u2212\u03b2T\n1 )2\nT\nX\nk=1\nT((1 \u2212\u03b21)\u03b2T \u2212k\n1\ngk,i)2\nq\nT(1 \u2212\u03b22)\u03b2T \u2212k\n2\ng2\nk,i\n\u2264\nT \u22121\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n+\np\n1 \u2212\u03b2T\n2\n(1 \u2212\u03b2T\n1 )2\n(1 \u2212\u03b21)2\np\nT(1 \u2212\u03b22)\nT\nX\nk=1\nT\n\u0012 \u03b22\n1\n\u221a\u03b22\n\u0013T \u2212k\n\u2225gk,i\u22252\n\u2264\nT \u22121\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n+\nT\np\nT(1 \u2212\u03b22)\nT\nX\nk=1\n\u03b3T \u2212k\u2225gk,i\u22252\nSimilarly, we can upper bound the rest of the terms in the summation.\nT\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n\u2264\nT\nX\nt=1\n\u2225gt,i\u22252\np\nt(1 \u2212\u03b22)\nT \u2212t\nX\nj=0\nt\u03b3j\n\u2264\nT\nX\nt=1\n\u2225gt,i\u22252\np\nt(1 \u2212\u03b22)\nT\nX\nj=0\nt\u03b3j\nFor \u03b3 < 1, using the upper bound on the arithmetic-geometric series, P\nt t\u03b3t <\n1\n(1\u2212\u03b3)2 :\nT\nX\nt=1\n\u2225gt,i\u22252\np\nt(1 \u2212\u03b22)\nT\nX\nj=0\nt\u03b3j \u2264\n1\n(1 \u2212\u03b3)2\u221a1 \u2212\u03b22\nT\nX\nt=1\n\u2225gt,i\u22252\n\u221a\nt\nApply Lemma 10.3,\nT\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n\u2264\n2G\u221e\n(1 \u2212\u03b3)2\u221a1 \u2212\u03b22\n\u2225g1:T,i\u22252\nTo simplify the notation, we de\ufb01ne \u03b3 \u225c\n\u03b22\n1\n\u221a\u03b22 . Intuitively, our following theorem holds when the\nlearning rate \u03b1t is decaying at a rate of t\u22121\n2 and \ufb01rst moment running average coef\ufb01cient \u03b21,t decay\nexponentially with \u03bb, that is typically close to 1, e.g. 1 \u221210\u22128.\nTheorem 10.5. Assume that the function ft has bounded gradients, \u2225\u2207ft(\u03b8)\u22252 \u2264G, \u2225\u2207ft(\u03b8)\u2225\u221e\u2264\nG\u221efor all \u03b8 \u2208Rd and distance between any \u03b8t generated by Adam is bounded, \u2225\u03b8n \u2212\u03b8m\u22252 \u2264D,\n13\nPublished as a conference paper at ICLR 2015\n\u2225\u03b8m \u2212\u03b8n\u2225\u221e\u2264D\u221efor any m, n \u2208{1, ..., T}, and \u03b21, \u03b22 \u2208[0, 1) satisfy\n\u03b22\n1\n\u221a\u03b22 < 1. Let \u03b1t =\n\u03b1\n\u221a\nt\nand \u03b21,t = \u03b21\u03bbt\u22121, \u03bb \u2208(0, 1). Adam achieves the following guarantee, for all T \u22651.\nR(T) \u2264\nD2\n2\u03b1(1 \u2212\u03b21)\nd\nX\ni=1\np\nTbvT,i+\n\u03b1(\u03b21 + 1)G\u221e\n(1 \u2212\u03b21)\u221a1 \u2212\u03b22(1 \u2212\u03b3)2\nd\nX\ni=1\n\u2225g1:T,i\u22252+\nd\nX\ni=1\nD2\n\u221eG\u221e\n\u221a1 \u2212\u03b22\n2\u03b1(1 \u2212\u03b21)(1 \u2212\u03bb)2\nProof. Using Lemma 10.2, we have,\nft(\u03b8t) \u2212ft(\u03b8\u2217) \u2264gT\nt (\u03b8t \u2212\u03b8\u2217) =\nd\nX\ni=1\ngt,i(\u03b8t,i \u2212\u03b8\u2217\n,i)\nFrom the update rules presented in algorithm 1,\n\u03b8t+1 = \u03b8t \u2212\u03b1t bmt/\np\nbvt\n= \u03b8t \u2212\n\u03b1t\n1 \u2212\u03b2t\n1\n\u0012 \u03b21,t\n\u221abvt\nmt\u22121 + (1 \u2212\u03b21,t)\n\u221abvt\ngt\n\u0013\nWe focus on the ith dimension of the parameter vector \u03b8t \u2208Rd. Subtract the scalar \u03b8\u2217\n,i and square\nboth sides of the above update rule, we have,\n(\u03b8t+1,i \u2212\u03b8\u2217\n,i)2 =(\u03b8t,i \u2212\u03b8\u2217\n,i)2 \u2212\n2\u03b1t\n1 \u2212\u03b2t\n1\n( \u03b21,t\np\nbvt,i\nmt\u22121,i + (1 \u2212\u03b21,t)\np\nbvt,i\ngt,i)(\u03b8t,i \u2212\u03b8\u2217\n,i) + \u03b12\nt ( bmt,i\np\nbvt,i\n)2\nWe can rearrange the above equation and use Young\u2019s inequality, ab \u2264a2/2 + b2/2. Also, it can be\nshown that\np\nbvt,i =\nqPt\nj=1(1 \u2212\u03b22)\u03b2t\u2212j\n2\ng2\nj,i/\np\n1 \u2212\u03b2t\n2 \u2264\u2225g1:t,i\u22252 and \u03b21,t \u2264\u03b21. Then\ngt,i(\u03b8t,i \u2212\u03b8\u2217\n,i) =(1 \u2212\u03b2t\n1)\np\nbvt,i\n2\u03b1t(1 \u2212\u03b21,t)\n\u0012\n(\u03b8t,i \u2212\u03b8\u2217\n,t)2 \u2212(\u03b8t+1,i \u2212\u03b8\u2217\n,i)2\n\u0013\n+\n\u03b21,t\n(1 \u2212\u03b21,t)\nbv\n1\n4\nt\u22121,i\n\u221a\u03b1t\u22121\n(\u03b8\u2217\n,i \u2212\u03b8t,i)\u221a\u03b1t\u22121\nmt\u22121,i\nbv\n1\n4\nt\u22121,i\n+ \u03b1t(1 \u2212\u03b2t\n1)\np\nbvt,i\n2(1 \u2212\u03b21,t)\n( bmt,i\np\nbvt,i\n)2\n\u2264\n1\n2\u03b1t(1 \u2212\u03b21)\n\u0012\n(\u03b8t,i \u2212\u03b8\u2217\n,t)2 \u2212(\u03b8t+1,i \u2212\u03b8\u2217\n,i)2\n\u0013p\nbvt,i +\n\u03b21,t\n2\u03b1t\u22121(1 \u2212\u03b21,t)(\u03b8\u2217\n,i \u2212\u03b8t,i)2p\nbvt\u22121,i\n+\n\u03b21\u03b1t\u22121\n2(1 \u2212\u03b21)\nm2\nt\u22121,i\np\nbvt\u22121,i\n+\n\u03b1t\n2(1 \u2212\u03b21)\nbm2\nt,i\np\nbvt,i\nWe apply Lemma 10.4 to the above inequality and derive the regret bound by summing across all\nthe dimensions for i \u22081, ..., d in the upper bound of ft(\u03b8t) \u2212ft(\u03b8\u2217) and the sequence of convex\nfunctions for t \u22081, ..., T:\nR(T) \u2264\nd\nX\ni=1\n1\n2\u03b11(1 \u2212\u03b21)(\u03b81,i \u2212\u03b8\u2217\n,i)2p\nbv1,i +\nd\nX\ni=1\nT\nX\nt=2\n1\n2(1 \u2212\u03b21)(\u03b8t,i \u2212\u03b8\u2217\n,i)2(\np\nbvt,i\n\u03b1t\n\u2212\np\nbvt\u22121,i\n\u03b1t\u22121\n)\n+\n\u03b21\u03b1G\u221e\n(1 \u2212\u03b21)\u221a1 \u2212\u03b22(1 \u2212\u03b3)2\nd\nX\ni=1\n\u2225g1:T,i\u22252 +\n\u03b1G\u221e\n(1 \u2212\u03b21)\u221a1 \u2212\u03b22(1 \u2212\u03b3)2\nd\nX\ni=1\n\u2225g1:T,i\u22252\n+\nd\nX\ni=1\nT\nX\nt=1\n\u03b21,t\n2\u03b1t(1 \u2212\u03b21,t)(\u03b8\u2217\n,i \u2212\u03b8t,i)2p\nbvt,i\n14\nPublished as a conference paper at ICLR 2015\nFrom the assumption, \u2225\u03b8t \u2212\u03b8\u2217\u22252 \u2264D, \u2225\u03b8m \u2212\u03b8n\u2225\u221e\u2264D\u221e, we have:\nR(T) \u2264\nD2\n2\u03b1(1 \u2212\u03b21)\nd\nX\ni=1\np\nTbvT,i +\n\u03b1(1 + \u03b21)G\u221e\n(1 \u2212\u03b21)\u221a1 \u2212\u03b22(1 \u2212\u03b3)2\nd\nX\ni=1\n\u2225g1:T,i\u22252 + D2\n\u221e\n2\u03b1\nd\nX\ni=1\nt\nX\nt=1\n\u03b21,t\n(1 \u2212\u03b21,t)\np\ntbvt,i\n\u2264\nD2\n2\u03b1(1 \u2212\u03b21)\nd\nX\ni=1\np\nTbvT,i +\n\u03b1(1 + \u03b21)G\u221e\n(1 \u2212\u03b21)\u221a1 \u2212\u03b22(1 \u2212\u03b3)2\nd\nX\ni=1\n\u2225g1:T,i\u22252\n+ D2\n\u221eG\u221e\n\u221a1 \u2212\u03b22\n2\u03b1\nd\nX\ni=1\nt\nX\nt=1\n\u03b21,t\n(1 \u2212\u03b21,t)\n\u221a\nt\nWe can use arithmetic geometric series upper bound for the last term:\nt\nX\nt=1\n\u03b21,t\n(1 \u2212\u03b21,t)\n\u221a\nt \u2264\nt\nX\nt=1\n1\n(1 \u2212\u03b21)\u03bbt\u22121\u221a\nt\n\u2264\nt\nX\nt=1\n1\n(1 \u2212\u03b21)\u03bbt\u22121t\n\u2264\n1\n(1 \u2212\u03b21)(1 \u2212\u03bb)2\nTherefore, we have the following regret bound:\nR(T) \u2264\nD2\n2\u03b1(1 \u2212\u03b21)\nd\nX\ni=1\np\nTbvT,i +\n\u03b1(1 + \u03b21)G\u221e\n(1 \u2212\u03b21)\u221a1 \u2212\u03b22(1 \u2212\u03b3)2\nd\nX\ni=1\n\u2225g1:T,i\u22252 +\nd\nX\ni=1\nD2\n\u221eG\u221e\n\u221a1 \u2212\u03b22\n2\u03b1\u03b21(1 \u2212\u03bb)2\n15\n",
        "sentence": " Adam [6] was used as the optimizer to verify the validity of the proposed technique for different learning methods.",
        "context": "We propose Adam, a method for ef\ufb01cient stochastic optimization that only requires \ufb01rst-order gra-\ndients with little memory requirement. The method computes individual adaptive learning rates for\nmethods for a variety of models and datasets, as shown in section 6. Overall, we show that Adam is\na versatile algorithm that scales to large-scale high-dimensional machine learning problems.\n2\nALGORITHM\noptimization framework. Empirical results demonstrate that Adam works well in\npractice and compares favorably to other stochastic optimization methods. Finally,\nwe discuss AdaMax, a variant of Adam based on the in\ufb01nity norm.\n1\nINTRODUCTION"
    },
    {
        "title": "Learning multiple layers of features from tiny images",
        "author": [
            "A. Krizhevsky",
            "G. Hinton"
        ],
        "venue": null,
        "citeRegEx": "7",
        "shortCiteRegEx": "7",
        "year": 2009,
        "abstract": "",
        "full_text": "",
        "sentence": " In this experiment we used just 500 gray-scale CIFAR-10 images [7] as anomaly data (as shown in Figure 4).",
        "context": null
    },
    {
        "title": "Imagenet classification with deep convolutional neural networks",
        "author": [
            "A. Krizhevsky",
            "I. Sutskever",
            "G.E. Hinton"
        ],
        "venue": "Advances in Neural Information Processing Systems",
        "citeRegEx": "8",
        "shortCiteRegEx": "8",
        "year": 2012,
        "abstract": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called \"dropout\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.",
        "full_text": "",
        "sentence": " [9] uses deep visual features obtained from AlexNet [8] to represent objects and associated them with a scene to define type of objects that can be found in the certain environment.",
        "context": null
    },
    {
        "title": "Detecting anomalous objects on mobile platforms",
        "author": [
            "W. Lawson",
            "L. Hiatt",
            "K. Sullivan"
        ],
        "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops,",
        "citeRegEx": "9",
        "shortCiteRegEx": "9",
        "year": 2016,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " [9] uses deep visual features obtained from AlexNet [8] to represent objects and associated them with a scene to define type of objects that can be found in the certain environment.",
        "context": null
    },
    {
        "title": "Spatio-temporal anomaly detection for industrial robots through prediction in unsupervised feature space",
        "author": [
            "A. Munawar",
            "P. Vinayavekhin",
            "G.D. Magistris"
        ],
        "venue": "In IEEE Winter Conference on Applications of Computer Vision (WACV), Santa Rosa,",
        "citeRegEx": "10",
        "shortCiteRegEx": "10",
        "year": 2017,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Anomaly detection is a key and usually the first requirement in many signal-processing applications pipeline [3, 10].",
        "context": null
    },
    {
        "title": "Anomaly detection using autoencoders with nonlinear dimensionality reduction",
        "author": [
            "M. Sakurada",
            "T. Yairi"
        ],
        "venue": "In Proceedings of the MLSDA 2014 2Nd Workshop on Machine Learning for Sensory Data Analysis,",
        "citeRegEx": "11",
        "shortCiteRegEx": "11",
        "year": 2014,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Generative models have previously been applied to anomaly detection [11, 1] and noise reduction [14].",
        "context": null
    },
    {
        "title": "Object-centric anomaly detection by attribute-based reasoning",
        "author": [
            "B. Saleh",
            "A. Farhadi",
            "A. Elgammal"
        ],
        "venue": "In Proceedings of the 2013 IEEE Conference on Computer Vision and Pattern Recognition,",
        "citeRegEx": "12",
        "shortCiteRegEx": "12",
        "year": 2013,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " [12] proposed a method to model a normality of a particular class of object using visual attributes.",
        "context": null
    },
    {
        "title": "Wind noise reduction using non-negative sparse coding",
        "author": [
            "M.N. Schmidt",
            "J. Larsen",
            "F.T. Hsiao"
        ],
        "venue": "IEEE Workshop on Machine Learning for Signal Processing,",
        "citeRegEx": "13",
        "shortCiteRegEx": "13",
        "year": 2007,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " [13] uses non-negative sparse coding to reduce the wind noise in speech data.",
        "context": null
    },
    {
        "title": "Extracting and composing robust features with denoising autoencoders",
        "author": [
            "P. Vincent",
            "H. Larochelle",
            "Y. Bengio",
            "P.-A. Manzagol"
        ],
        "venue": "Proceedings of the Twentyfifth International Conference on Machine Learning",
        "citeRegEx": "14",
        "shortCiteRegEx": "14",
        "year": 2008,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Generative models have previously been applied to anomaly detection [11, 1] and noise reduction [14].",
        "context": null
    },
    {
        "title": "Stacked denoising autoencoders: Learning  useful representations in a deep network with a local denoising criterion",
        "author": [
            "P. Vincent",
            "H. Larochelle",
            "I. Lajoie",
            "Y. Bengio",
            "P.-A. Manzagol"
        ],
        "venue": "J. Mach. Learn. Res.,",
        "citeRegEx": "15",
        "shortCiteRegEx": "15",
        "year": 2010,
        "abstract": "",
        "full_text": "",
        "sentence": " [15] is also very important in this area.",
        "context": null
    },
    {
        "title": "Learning deep representations of appearance and motion for anomalous event detection",
        "author": [
            "D. Xu",
            "E. Ricci",
            "Y. Yan",
            "J. Song",
            "N. Sebe"
        ],
        "venue": "Proceedings of the British Machine Vision Conference",
        "citeRegEx": "17",
        "shortCiteRegEx": "17",
        "year": 2015,
        "abstract": "We present a novel unsupervised deep learning framework for anomalous event\ndetection in complex video scenes. While most existing works merely use\nhand-crafted appearance and motion features, we propose Appearance and Motion\nDeepNet (AMDN) which utilizes deep neural networks to automatically learn\nfeature representations. To exploit the complementary information of both\nappearance and motion patterns, we introduce a novel double fusion framework,\ncombining both the benefits of traditional early fusion and late fusion\nstrategies. Specifically, stacked denoising autoencoders are proposed to\nseparately learn both appearance and motion features as well as a joint\nrepresentation (early fusion). Based on the learned representations, multiple\none-class SVM models are used to predict the anomaly scores of each input,\nwhich are then integrated with a late fusion strategy for final anomaly\ndetection. We evaluate the proposed method on two publicly available video\nsurveillance datasets, showing competitive performance with respect to state of\nthe art approaches.",
        "full_text": "XU ET AL.: LEARNING DEEP REPRESENTATIONS OF APPEARANCE AND MOTION...\n1\nLearning Deep Representations of\nAppearance and Motion for Anomalous\nEvent Detection\nDan Xu1\ndanxuhk@gmail.com\n1DISI, University of Trento,\nTrento, Italy\nElisa Ricci2,3\neliricci@fbk.eu\n2Fondazione Bruno Kessler (FBK),\nTrento, Italy\nYan Yan1,4, Jingkuan Song1\nyan@disi.unitn.it,jingkuan.song@unitn.it\n3University of Perugia,\nPerugia, Italy\nNicu Sebe1\nsebe@disi.unitn.it\n4ADSC, UIUC Singapore,\nSingapore\nAbstract\nWe present a novel unsupervised deep learning framework for anomalous event de-\ntection in complex video scenes. While most existing works merely use hand-crafted\nappearance and motion features, we propose Appearance and Motion DeepNet (AMDN)\nwhich utilizes deep neural networks to automatically learn feature representations. To\nexploit the complementary information of both appearance and motion patterns, we in-\ntroduce a novel double fusion framework, combining both the bene\ufb01ts of traditional early\nfusion and late fusion strategies. Speci\ufb01cally, stacked denoising autoencoders are pro-\nposed to separately learn both appearance and motion features as well as a joint repre-\nsentation (early fusion). Based on the learned representations, multiple one-class SVM\nmodels are used to predict the anomaly scores of each input, which are then integrated\nwith a late fusion strategy for \ufb01nal anomaly detection. We evaluate the proposed method\non two publicly available video surveillance datasets, showing competitive performance\nwith respect to state of the art approaches.\n1\nIntroduction\nA fundamental challenge in intelligent video surveillance is to automatically detect abnormal\nevents in long video streams. This problem has attracted considerable attentions from both\nacademia and industry in recent years [4, 16, 17, 24]. Video anomaly detection is also\nimportant as it is related to other interesting topics in computer vision, such as dominant\nbehavior detection [23], visual saliency [37] and interestingness prediction [7]. A typical\napproach to tackle the anomaly detection task is to learn a model which describes normal\nactivities in the video scene and then discovers unusual events by examining patterns which\ndistinctly diverge from the model. However, the complexity of scenes and the deceptive\nnature of abnormal behaviours make anomaly detection still a very challenging task.\nc\u20dd2015. The copyright of this document resides with its authors.\nIt may be distributed unchanged freely in print or electronic forms.\narXiv:1510.01553v1  [cs.CV]  6 Oct 2015\n2\nXU ET AL.: LEARNING DEEP REPRESENTATIONS OF APPEARANCE AND MOTION...\nImage Sequences\nOptical Flow Maps\nForeground\nExtract multi-scale \nimage patches and warp\nExtract \ufb01xed-sized\noptical \ufb02ow patches\nOne Class \nSVM\nOne Class \nSVM\nOne Class \nSVM\nLate Fusion\nAnomaly Detection\nAppearance Representations\nMotion Representations\nJoint  Representations\nwa\nha\nwm\nhm\n\u0001\n\u0001\n\u0001\nDecoder\nEncoder\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\nDecoder\nEncoder\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\nDecoder\nEncoder\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\nPixel-level\nEarly \nFusion\nFigure 1: Overview of the proposed AMDN method for anomalous event detection.\nAmong previous works, several anomaly detection approaches are based on analyzing\nindividual moving objects in the scene. Tracking is usually an initial step for this class of\nmethods. By using accurate tracking algorithms, trajectory extraction can be carried out to\nfurther perform trajectory clustering analysis [5, 20] or design representative features [28] to\nmodel typical activities and subsequently discover anomalies. In [33], trajectories which are\nspatially close and have similar motion patterns are identi\ufb01ed and used for detecting unusual\nevents. Vaswani et al. [29] propose a \u201cshape activity\u201d model to describe moving objects and\ndetect anomalies. However, as tracking performance signi\ufb01cantly degrades in the presence\nof several occluded targets, tracking-based methods are not suitable for analyzing complex\nand crowded scenes.\nTo overcome the aforementioned limitations, researchers address the problem by learning\nspatio/temporal activity patterns in a local or a global context from either 2D image cells or\n3D video volumes [2, 4, 8, 18, 21, 22, 24]. This category of methods builds the models based\non hand-crafted features extracted from low-level appearance and motion cues, such as color,\ntexture and optical \ufb02ow. Commonly used low-level features include histogram of oriented\ngradients (HOG), 3D spatio-temporal gradient, histogram of optical \ufb02ow (HOF), among oth-\ners. Cong et al. [4] employ multi-scale histograms of optical \ufb02ow and a sparse coding model\nand use the reconstruction error as a metric for outlier detection. Mehran et al. [18] propose\na \u201csocial force\u201d model based on optical \ufb02ow features to represent crowd activity patterns\nand identify anomalous activities. In [2] co-occurrence statistics of spatio-temporal events\nare employed in combination with Markov Random Fields (MRFs) to discover unusual ac-\ntivities. Kratz et al. [12] introduce a HMMs-based approach for detecting abnormal events\nby analyzing the motion variation of local space-time volumes. Kim et al. [11] propose a\nmethod based on local optical \ufb02ow features and MRFs to spot unusual events. Mahadevan\net al. [17] introduce a mixtures of dynamic textures model to jointly employ appearance\nand motion features. However, the adoption of hand-crafted features is a clear limitation of\nprevious methods, as it implies enforcing some some a priori knowledge which, in case of\ncomplex video surveillance scene, is very dif\ufb01cult to de\ufb01ne.\nRecently, deep learning architectures have been successfully used to tackle various com-\nputer vision tasks, such as image classi\ufb01cation [13], object detection [6] and activity recog-\nnition [26]. However, these works are mainly based on Convolutional Neural Networks and\nconsider a supervised learning scenario. Unsupervised deep learning approaches based on\nautoencoder networks [30] have also been investigated to address important tasks such as\nobject tracking [32] and face alignment [38]. The key of the success is that, using deep\narchitectures, rich and discriminative features can be learned via multi-layer nonlinear trans-\nXU ET AL.: LEARNING DEEP REPRESENTATIONS OF APPEARANCE AND MOTION...\n3\nformations. Therefore, it is reasonable to expect that detecting unusual events in videos can\nalso bene\ufb01t from deep learning models.\nFollowing this intuition, in this paper we propose a novel Appearance and Motion Deep-\nNet (AMDN) framework for discovering anomalous activities in complex video surveillance\nscenes. Opposite to previous works [8, 21, 24], instead of using hand-crafted features to\nmodel activity patterns, we propose to learn discriminative feature representations of both\nappearance and motion patterns in a fully unsupervised manner. A novel approach based on\nstacked denoising autoencoders (SDAE) [31] is introduced to achieve this goal. An overview\nof the proposed AMDN is shown in Fig. 1. Low-level visual information including still im-\nage patches and dynamic motion \ufb01elds represented with optical \ufb02ow is used as input of two\nseparate networks, to \ufb01rst learn appearance and motion features, respectively. To further\ninvestigate the correlations between appearance and motion, early fusion is performed by\ncombining image pixels with their corresponding optical \ufb02ow to learn a joint representa-\ntion. Finally, for abnormal event prediction, a late fusion strategy is introduced to combine\nthe anomaly scores predicted by multiple one-class SVM classi\ufb01ers, each corresponding to\none of the three learned feature representations. The bene\ufb01ts of the proposed double fusion\nframework (i.e. combining both early fusion and late fusion strategies) are con\ufb01rmed by our\nextensive experimental evaluation, conducted on two publicly available datasets.\nIn summary, the main contributions of this paper are: i) As far as we know, we are the\n\ufb01rst to introduce an unsupervised deep learning framework to automatically construct dis-\ncriminative representations for video anomaly detection. ii) We propose a new approach\nto learn appearance and motion features as well as their correlations. Deep learning meth-\nods for combining multiple modalities have been investigated in previous works [19, 27].\nHowever, to our knowledge, this is the \ufb01rst work where multimodal deep learning is ap-\nplied to anomalous event detection. iii) A double fusion scheme is proposed to combine\nappearance and motion features for discovering unusual activities. It is worth noting that the\nadvantages of combining early and late fusion approaches have been investigated in previ-\nous works [14]. However, Lan et al.[14] do not consider a deep learning framework. iv) The\nproposed method is validated on challenging anomaly detection datasets and we obtain very\ncompetitive performance compared with the state-of-the-art.\n2\nAMDN for Abnormal Event Detection\nThe proposed AMDN framework for detecting anomalous activities is based on two main\nbuilding blocks (Fig.1). First, SDAE are used to learn appearance and motion representations\nof visual data, as well as a joint representation capturing the correlation between appearance\nand motion features (Sec. 2.1). In the second phase (Sec. 2.2), to detect anomalous events,\nwe propose to train three separate one-class SVMs [25] based on the three different types\nof learned feature representations. Once the one-class SVM models are learned, given a test\nsample corresponding to an image patch, three anomaly scores are computed and combined.\nThe combination of the one-class SVM scores is obtained with a novel late fusion scheme.\nIn the following we describe the proposed approach in details.\n2.1\nLearning Deep Appearance and Motion Representations\nIn this subsection we present the proposed AMDN for learning deep representations of ap-\npearance and motion. In the following we \ufb01rst introduce denoising autoencoders and then\n4\nXU ET AL.: LEARNING DEEP REPRESENTATIONS OF APPEARANCE AND MOTION...\ndescribe the details of the structure and the learning approach of the proposed AMDN.\n2.1.1\nDenoising Autoencoders\nA Denoising Auto-Encoder (DAE) is a one-hidden-layer neural network which is trained to\nreconstruct a data point xi from its (partially) corrupted version \u02dcxi [30]. Typical corrupted\ninputs are obtained by drawing samples from a conditional distribution p(x|\u02dcx) (e.g. common\nchoices for corrupting samples are additive Gaussian white noise or salt-pepper noise). A\nDAE neural network can be divided into two parts: encoder and decoder, with a single shared\nhidden layer. These two parts actually attempt to learn two mapping functions, denoted as\nfe(W,b) and fd(W\u2032,b\u2032), where W,b denote the weights and the bias term of the encoder\npart, and W\u2032,b\u2032 refer to the corresponding parameters of the decoder. For a corrupted input\n\u02dcxi, a compressed hidden layer representation hi can be obtained through hi = fe(\u02dcxi | W,b) =\n\u03c3(W\u02dcxi + b). Then, the decoder tries to recover the original input xi from hi computing\n\u02c6xi = fd(hi | W\u2032,b\u2032) = s(W\u2032hi+b\u2032), The function \u03c3(\u00b7) and s(\u00b7) are activation functions, which\nare typically nonlinear transformations such as the sigmoid. Using this encoder/decoder\nstructure, the network can learn a more stable and robust feature representations of the input.\nGiven a training set T = {xi}N\ni=1, a DAE learns its parameters (W,W\u2032,b,b\u2032) by solving\nthe following regularized least square optimization problem:\nmin\nW,W\u2032,b,b\u2032\nN\n\u2211\ni=1\n\u2225xi \u2212\u02c6xi\u22252\n2 +\u03bb(\u2225W\u22252\nF +\u2225W\u2032\u22252\nF)\n(1)\nwhere \u2225\u00b7 \u2225F denotes the Frobenius norm. The \ufb01rst term represents the average reconstruc-\ntion error, while the weight penalty term is introduced for regularization. The parameter \u03bb\nbalances the importance of the two terms. Typically, sparsity constraints are imposed on\nthe output of the hidden units to discover meaningful representations from the data [32]. If\nwe let \u00b5j be the target sparsity level and \u02c6\u00b5 j = 1\nN \u2211N\ni=1 hj\ni be the average activation values\nall over all training samples for the j-th unit, an extra penalty term based on cross-entropy,\n\u03d5(\u00b5\u00b5\u00b5|| \u02c6\u00b5\u00b5\u00b5) = \u2212\u2211H\nj=1[\u00b5 j log( \u02c6\u00b5j) + (1 \u2212\u00b5j)log(1 \u2212\u02c6\u00b5j)], can be added to (1) to learn a sparse\nrepresentation. Here, H is the number of hidden units. The optimization problem (1) has a\nnon-convex objective function and gradient descent can be used to compute a local optima.\n2.1.2\nAMDN Structure\nThe proposed AMDN structure consists of three SDAE pipelines (Fig.1) corresponding to\ndifferent types of low-level inputs. The three SDAE networks learn appearance and motion\nfeatures as well as a joint representation of them. We show the basic structures of the pro-\nposed SDAE networks in Fig. 2 (a) and (b). Each SDAE consists of two parts: encoder and\ndecoder. For the encoder part, we use an over-complete set of \ufb01lters in the \ufb01rst layer to\ncapture a representative information from the data. Then, the number of neurons is reduced\nby half in the next layer until reaching the \u201cbottleneck\u201d hidden layer. The decoder part has\na symmetric structure with respect to the encoder part. We now describe the proposed three\nfeature learning pipelines in details.\nAppearance representation. This SDAE aims at learning mid-level appearance repre-\nsentations from the original image pixels. To capture rich appearance attributes, a multi-scale\nsliding-window approach with a stride d is used to extract dense image patches, which are\nthen warped into equal size wa \u00d7 ha \u00d7 ca, where wa,ha are the width and height of each\nXU ET AL.: LEARNING DEEP REPRESENTATIONS OF APPEARANCE AND MOTION...\n5\n(c) \n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\n(a) \n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\n(b) \n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\nBottleneck\nhidden layer\nW\u2032A\nl+1, b\u2032A\nl+1\nW\u2032A\n2l, b\u2032A\n2l\nWA\n1 , bA\n1\nWA\nl , bA\nl\n!\n!\nDecoder\nEncoder\nW\u2032J\n2l, b\u2032J\n2l\nW\u2032J\nl+1, b\u2032J\nl+1\nWJ\nl , bJ\nl\nWJ\n1 , bJ\n1\n\u02c6xA\ni (\u02c6xM\ni )\nxA\ni (xM\ni )\n\u02c6xJ\ni\nxM\ni\nxA\ni\nFigure 2: (a) The structure of the appearance and motion representation learning pipelines.\n(b) The structure of the joint representation learning pipeline. (c) Some examples of the\nweight \ufb01lters extracted from the \ufb01rst layer of the joint representation learning pipeline.\npatch and ca is the number of the channels (ca = 1 for gray images). The warped patches\nxA\ni \u2208IRwa\u00d7ha\u00d7ca are used for training. All the patches are linearly normalized into a range\n[0, 1]. We stack 4 encoding layers with \u03bda \u00d7 wa \u00d7 ha \u00d7 ca neurons in the \ufb01rst layer, where\n\u03bda > 1 is an ampli\ufb01cation factor for constructing an over-complete set of \ufb01lters.\nMotion representation. The motion information is computed with optical \ufb02ow. We use\na sliding window approach with windows of \ufb01xed size wm \u00d7hm \u00d7cm (cm = 2 for optical \ufb02ow\nmagnitude along x and y axes), to generate dense optical \ufb02ow patches xm\ni \u2208IRwm\u00d7hm\u00d7cm for\nmotion representation learning. Similar to the appearance feature pipeline, the patches are\nnormalized into [0,1] within each channel and 4 encoding layers are used. The number of\nneurons of the \ufb01rst layer is set to \u03bdm \u00d7wm \u00d7hm \u00d7cm.\nJoint appearance and motion representation. The above-mentioned SDAE learn ap-\npearance and motion features separately. Taking into account the correlations between mo-\ntion and appearance, we propose to couple these two pipelines to learn a joint representation.\nThe network training data xJ\ni \u2208IRw j\u00d7h j\u00d7(ca+cm) are obtained through a pixel-level early fusion\nof the gray image patches and the corresponding optical \ufb02ow patches.\n2.1.3\nAMDN Training\nWe train the AMDN with two steps: pretraining and \ufb01ne-tuning. The layer-wise pretrain-\ning learns one single denoising auto-encoder at a time using (1) with sparsity constraints\n(Sec.2.1.1). The input is corrupted to learn the mapping function fe(\u00b7), which is then used\nto produce the representation for the next layer with uncorrupted inputs. By using a greedy\nlayer-wise pretraining, the denoising autoencoders can be stacked to build a multi-layer feed-\nforward deep neural network, i.e. a stacked denoising autoencoder. The network parameters\nare initialized through pretraining all layers, and then \ufb01ne-tuning is used to adjust parameters\nover the whole network.\nFine-tuning treats all the layers of an SDAE as a single model. Given a training set\nT k = {xk\ni }Nk\ni=1 with Nk training samples (k \u2208{A,M,J} corresponds to appearance, motion\nand joint representation, respectively), the backpropagation algorithm can be used to \ufb01ne-\ntune the network. The following objective function is used for \ufb01ne-tuning the SDAE with\n2L+1 layers:\nJ(T k) =\nNk\n\u2211\ni\n\u2225xk\ni \u2212\u02c6xk\ni \u22252\n2 +\u03bbF\nL\n\u2211\ni=1\n(\u2225Wk\ni \u22252\nF +\u2225W\u2032k\ni \u22252\nF),\n(2)\n6\nXU ET AL.: LEARNING DEEP REPRESENTATIONS OF APPEARANCE AND MOTION...\nwhere \u03bbF is a user de\ufb01ned parameter. To speed up the convergence during training, stochastic\ngradient descent (SGD) is employed and the training set is divided into mini-batches with\nsize Nk\nb. Fig. 2 (c) shows some of the learned \ufb01lters in the \ufb01rst layer after \ufb01ne-tuning for the\njoint representation learning pipeline.\nAfter \ufb01ne-tuning the whole network, the learned features representation can be computed\nto perform video anomaly detection. Theoretically, the output of each layer in an SDAE can\nbe used as a novel learned feature representation. In this work, we choose the output of the\n\u201cbottleneck\" hidden layer to obtain a more compact representation. Let xk\ni be the i-th input\ndata sample, and \u03c3k\nl (Wk\nl ,bk\nl ) be the mapping function of the l-th hidden layer of the k-th\nSDAE pipeline. The learned features, sk\ni , can be extracted through a forward pass computing\nsk\ni = \u03c3L(\u03c3L\u22121(\u00b7\u00b7\u00b7\u03c31(Wk\n1xk\ni + bk\n1))), where the L-th hidden layer is the \u201cbottleneck\" hidden\nlayer.\n2.2\nAbnormal Event Detection with Deep Representations\nWe formulate the video anomaly detection problem as a patch-based binary categorization\nproblem, i.e. given a test frame we obtain MI \u00d7NI patches via sliding window with a stride\nd and classify each patch as corresponding to a normal or abnormal region. Speci\ufb01cally,\ngiven each test patch t we compute three anomaly scores Ak(sk\nt ), k \u2208{A,M,J}, using one-\nclass SVM models and the computed features representations sk\nt . The three scores are then\nlinearly combined to obtain the \ufb01nal anomaly score A(sk\nt ) = \u2211k\u2208{A,M,J} \u03b1kAk(sk\nt ). A pre-\nprocessing of dynamic background subtraction can be carried out to improve the computing\nef\ufb01ciency during the test phase as the anomalies exist in the foreground region.\n2.2.1\nOne-class SVM Modeling\nOne-class SVM is a widely used algorithm for outlier detection, where the main idea is\nto learn a hypersphere in the feature space and map most of the training data into it. The\noutliers of the data distribution correspond to point lying outside the hypersphere. Formally,\ngiven a set of training samples S = {sk\ni }Nk\ni=1, the underlying problem of one-class SVM can\nbe formulated as the following quadratic program:\nmin\nw,\u03c1\n1\n2\u2225w\u22252 +\n1\n\u03bdNk\nNk\n\u2211\ni=1\n\u03bei \u2212\u03c1\ns.t.\nwT\u03a6(sk\ni ) \u2265\u03c1 \u2212\u03bei, \u03bei \u22650.\n(3)\nwhere w is the learned weight vector, \u03c1 is the offset, \u03a6(\u00b7) is a feature projection function\nwhich maps feature vector sk\ni into a higher dimensional feature space. The user de\ufb01ned\nparameter \u03bd \u2208(0,1] regulates the expected fraction of outliers distributed outside the hy-\npersphere. Introducing a nonlinear mapping, the projection function \u03a6(\u00b7) can be de\ufb01ned\nimplicitely by introducing an associated kernel function k(sk\ni ,sk\nj) = \u03a6(sk\ni )T\u03a6(sk\nj) and (3)\ncan be solved in the corresponding dual form [25]. In our experiments we consider a rbf\nkernel, k(sk\ni ,sk\nj) = e\n\u2212\u2225sk\ni \u2212sk\nj\u22252\n2\u03c32\n. Given the optimal w and \u03c1 obtained by solving (3), an out-\nlier score for a test sample sk\nt of the k-th SDAE pipeline can be estimated by computing\nAk(sk\nt ) = \u03c1 \u2212wT\u03a6(sk\nt ).\nXU ET AL.: LEARNING DEEP REPRESENTATIONS OF APPEARANCE AND MOTION...\n7\nFigure 3: Examples of anomaly detection results on Ped1 (top) and Ped2 (bottom) sequences.\nOur approach can detect anomalies such as bikes, vehicles and skaters and provides accurate\nlocalization.\n2.2.2\nLate Fusion for Anomaly Detection\nAn unsupervised late fusion scheme is designed to automatically learn the weight vector\n\u03b1\u03b1\u03b1 = [\u03b1A,\u03b1M,\u03b1J]. The weight learning approach is based on the following optimization:\nmin\nWsk,\u03b1k\n\u2211\nk\n\u03b1ktr\n\u0010\nWskSk\u0000WskSk\u0001T\u0011\n+\u03bbs \u2225\u03b1\u03b1\u03b1\u22252\n2\ns.t.\n\u03b1k > 0,\n\u2211\nk\n\u03b1k = 1\n(4)\nwhere Sk = [sk\n1,...,sk\nNk] is the matrix of training set samples, Wsk maps the k-th feature sk\ni\ninto a new subspace and WskSk \u0000WskSk\u0001T represents the covariance of k-th feature type in\nthe new subspace. The term \u2225\u03b1\u03b1\u03b1\u22252\n2 is introduced to avoid over\ufb01tting and \u03bbs is a user de\ufb01ned\nparameter. To solve (4), we \ufb01rst get Wsk as the d eigenvectors of SkSkT corresponding to the\nd-largest eigenvalues. Then, \u03b1\u03b1\u03b1 can be obtained by solving the simplex problem [39]:\nmin\n\u03b1k>0,\u2211\nk\n\u03b1k=1\n1\n2\u2225\u03b1\u03b1\u03b1 \u2212c\u22252\n2\n(5)\nwith c = [cA,cM,cJ], ck = \u22121\n2\u03bbs tr\n\u0010\nWskSk\u0000WskSk\u0001T\u0011\n. Then for each patch t, we identify if\nit corresponds to an abnormal activity by computing the associated anomaly score A(sk\nt ) and\ncomparing it with a threshold \u03b7, i.e. A(sk\nt )\nnormal\n\u2276\nabnormal\n\u03b7.\n3\nExperimental Results\nDatasets and Experimental Setup.\nThe proposed method is mainly implemented in Mat-\nlab and C++ based on Caffe framework [9]. The code for optical \ufb02ow calculation is written in\nC++ and wrapped with Matlab mex for computational ef\ufb01ciency [15]. For one-class SVMs,\nwe use the LIBSVM library (version 3.2) [3]. The experiments are carried out on a PC with\na middle-level graphics card (NVIDIA Quadro K4000) and a multi-core 2.1 GHz CPU with\n32 GB memory. Two publicly available datasets, the UCSD (Ped1 and Ped2) dataset [17]\nand the Train dataset [36], are used to evaluate the performance of the proposed approach.\n8\nXU ET AL.: LEARNING DEEP REPRESENTATIONS OF APPEARANCE AND MOTION...\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.2\n0.4\n0.6\n0.8\n1\nFalse Positive Rate (FPR)\nTrue Positive Rate (TPR)\n \n \nMDT[16]\nMPPCA[10]\nSocial Force + MPPCA[16]\nSocial Force[17]\nSparse reconstruction[4]\nLocal statistical[22]\nDetection at 150FPS[15]\nAMDN\n(a) Frame-level ROC curve of PED1 Dataset\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.2\n0.4\n0.6\n0.8\n1\nFalse Positive Rate (FPR)\nTrue Positive Rate (TPR)\n \n \nMDT[16]\nMPPCA[10]\nSocial Force + MPPCA[16]\nSocial Force[17]\nSparse Reconstruction[4]\nDetection at 150FPS[15]\nAMDN\n(b) Pixel-level ROC curve of PED1 Dataset\nFigure 4: UCSD dataset (Ped1 sequence): comparison of frame-level and pixel-level\nanomaly detection results with state of the art methods.\nAlgorithm\nPed1(frame)\nPed1(pixel)\nPed2\nEER\nAUC\nEER\nAUC\nEER\nAUC\nMPPCA [11]\n40%\n59.0%\n81%\n20.5%\n30%\n69.3%\nSocial force [18]\n31%\n67.5%\n79%\n19.7%\n42%\n55.6%\nSocial force+MPPCA [17]\n32%\n66.8%\n71%\n21.3%\n36%\n61.3%\nSparse reconstruction [4]\n19%\n\u2013\n54%\n45.3%\n\u2013\n\u2013\nMixture dynamic texture [17]\n25%\n81.8%\n58%\n44.1%\n25%\n82.9%\nLocal Statistical Aggregates [24]\n16%\n92.7%\n\u2013\n\u2013\n\u2013\n\u2013\nDetection at 150 FPS [16]\n15%\n91.8%\n43%\n63.8%\n\u2013\n\u2013\nJoint representation (early fusion)\n22%\n84.9%\n47.1%\n57.8%\n24%\n81.5%\nFusion of appearance and motion pipelines (late fusion)\n18%\n89.1%\n43.6%\n62.1%\n19%\n87.3%\nAMDN (double fusion)\n16%\n92.1%\n40.1%\n67.2%\n17%\n90.8%\nTable 1: UCSD dataset: comparison in terms of EER (Equal Error Rate) and AUC (Area\nUnder ROC) with the state of the art methods on Ped1 and Ped2.\nThe UCSD pedestrian dataset [17] is a challenging anomaly detection dataset including\ntwo subsets: Ped1 and Ped2. The video sequences depict different crowded scenes and\nanomalies include bicycles, vehicles, skateboarders and wheelchairs. In some frames the\nanomalies occur at multiple locations. Ped1 has 34 training and 16 test image sequences\nwith about 3,400 anomalous and 5,500 normal frames, and the image resolution is 238\u00d7158\npixels. Ped2 has 16 training and 12 test image sequences with about 1,652 anomalous and\n346 normal frames. The image resolution is 360\u00d7240 pixels.\nThe Train dataset [36] depicts moving people inside a train. This is also a challenging\nabnormal event detection dataset due to dynamic illumination changes and camera shake\nproblems. The dataset consists of 19218 frames, and the anomalous events are mainly due\nto unusual movements of people on the train.\nQuantitative evaluation.\nIn the \ufb01rst series of experiments we evaluate the performance of\nthe proposed method on the UCSD dataset. For appearance learning, patches are extracted\nusing a sliding window approach at three different scales, i.e. 15\u00d715, 18\u00d718 and 20\u00d720\npixels. This generates more than 50 million image patches, 10 million of which are randomly\nsampled and warped into the same size (wa \u00d7ha = 15\u00d715 pixels) for training. For learning\nthe motion representation, the patch size is \ufb01xed to wm \u00d7hm = 15\u00d715 pixels, and 6 million\ntraining patches are randomly sampled. In the test phase, we use sliding widow with a size of\n15\u00d715 and a stride d = 15. The number of neurons of the \ufb01rst layer of the appearance and\nmotion network is both set to 1024, while for the joint pipeline is 2048. Then the encoder\nXU ET AL.: LEARNING DEEP REPRESENTATIONS OF APPEARANCE AND MOTION...\n9\n\u0001a\u0002\n\u0001b\u0002\n\u0001c\u0002\n\u0003\n\u0003\u0002\u0004\n\u0003\u0002\u0005\n\u0003\u0002\u0006\n\u0003\u0002\u0007\n\u0003\u0002\b\n\u0003\u0002\t\n\u0003\u0002\n\u0003\u0002\u000b\n\u0003\u0002\f\n\u0004\n\u0003\n\u0003\u0002\u0005\n\u0003\u0002\u0007\n\u0003\u0002\t\n\u0003\u0002\u000b\n\u0004\n\u0004\u0002\u0005\n\u0017\u001e\u001c\u001b##\n\u0016(\u001e\u001c\")\"&%\n\u0001\n\u0001\n\u000f&$\"%\u001b%*\u0001\u000e\u001e!\u001b,\"&(\u0001\u0012\u001e\u001b(%\"% \u0019\u0005\u0004\u001a\n\u0018'\u001b*\"&\u0001*\u001e$'&(\u001b#\u0015(\"\u001e%*\u001e\u001d\u0001\u0010%\u001e( /\u0019\u0006\u0005\u001a\n\u0012&\u001c\u001b#\u0001&'*\"\u001c\u001b#\u0001\u001f#&-\u0019\u0004\u001a\n\u000e\u001e!\u001b,\"&(\u0001*\u001e$'#\u001b*\u001e\u0019\f\u001a\n\u0013\".*+(\u001e\u0001&\u001f\u0001\u0011\u001b+))\"\u001b%\n\r\u0013\u000f\u0014\nFigure 5: Train dataset: (a) a frame depicting typical activities, (b) an example of a detected\nanomaly. (c) precision/recall curve.\npart can be simply de\ufb01ned as: 1024(2048) \u2192512(1024) \u2192256(512) \u2192128(256), and the\ndecoder part is a symmetric structure. For the pre-training of the DAE, the corrupted inputs\nare produced by adding a Gaussian noise with variance 0.0003. The network training is\nbased on the SGD with a momentum parameter set to 0.9. We use a \ufb01xed learning rates\n\u03bb = 0.01, \u03bbF = 0.0001 and a mini-batch size Nb = 256. For one-class SVMs, the parameter\n\u03bd is tuned with cross validation. The learned late fusion weights [\u03b1A,\u03b1M,\u03b1J] are obtained\nwith \u03bbs = 0.1 and for Ped1 and Ped2 are [0.2,0.5,0.3] and [0.2,0.4,0.4], respectively.\nSome examples of anomaly detection results on the UCSD dataset are shown in Fig. 3.\nTo perform a quantitative evaluation, we use both a frame-level ground truth and a pixel-level\nground truth. The frame-level ground truth represents whether one or more anomalies occur\nin a test frame. The pixel-level ground truth is used to evaluate the anomaly localization\nperformance. If the detected anomaly region is more than 40% overlapping with the anno-\ntated region, it is a true detection. We carry out a frame-level evaluation on both Ped1 and\nPed2. Ped1 provides 10 test image sequences with pixel-level ground truth. The pixel-level\nevaluation is performed on these test sequences.\nFig. 4 (a) and (b) show the frame-level and pixel-level detection results on Ped1. The\nROC curve is produced by varying the threshold parameter \u03b7. Table 1 shows a quantitative\ncomparison in terms of Area Under Curve (AUC) and Equal Error Rate (EER) of our method\nwith several state-of-the-art approaches. From the frame-level evaluation, it is evident that\nour method outperforms most previous methods and that its performance are competitive\nwith the best two baselines [24] and [16]. Moreover, considering pixel-level evaluation,\ni.e. accuracy in anomaly localization, our method outperforms all the competing approaches\nboth in terms of EER and AUC. Table 1 also demonstrate the advantages of the proposed\ndouble fusion strategy. Our AMDN guarantees better performance than early fusion and\nlate fusion approaches. Speci\ufb01cally, for early fusion we only consider the learned joint\nappearance/motion representation and a single one-class SVM. For late fusion we use the two\nseparate appearance and motion pipelines and the proposed fusion scheme but we discard\nthe joint representation pipeline. Interestingly, in this application the late fusion strategy\noutperforms early fusion.\nIn the second series of experiments we consider the Train dataset. For parameters, we\nuse the same experimental setting of the UCSD experiments, except from the parameters\n\u03bbF and Nb which are set to 0.00001 and 100, respectively. The learned late fusion weights\nare [\u03b1A,\u03b1M,\u03b1J] = [0.3,0.4,0.3]. We compare the proposed approach with several methods,\n10\nXU ET AL.: LEARNING DEEP REPRESENTATIONS OF APPEARANCE AND MOTION...\nincluding dominant behavior learning [23], spatio-temporal oriented energies [36], local op-\ntical \ufb02ow [1], behavior templates [10] and mixture of Gaussian. From the precision/recall\ncurve shown in Fig. 5 (c), it is clear that our method outperforms all the baselines.\n4\nConclusions\nWe presented a novel unsupervised learning approach for video anomaly detection based on\ndeep representations. The proposed method is based on multiple stacked autoencoder net-\nworks for learning both appearance and motion representations of scene activities. A double\nfusion scheme is designed to combine the learned feature representations. Extensive exper-\niments on two challenging datasets demonstrate the effectiveness of the proposed approach\nand show competitive performance with respect to existing methods. Future works include\ninvestigating other network architectures, alternative approaches for fusing multimodal data\nin the context of SDAE, and extending our framework using multi-task learning [34, 35] for\ndetecting anomalies in heterogeneous scenes.\nAcknowledgments\nThis work was partially supported by the MIUR Cluster project Active Ageing at Home, the\nEC H2020 project ACANTO and by A*STAR Singapore under the Human-Centered Cyber-\nphysical Systems (HCCS) grant. The authors also would like to thank NVIDIA for GPU\ndonation.\nReferences\n[1] A. Adam, E. Rivlin, I. Shimshoni, and D. Reinitz. Robust real-time unusual event\ndetection using multiple \ufb01xed-location monitors. IEEE Transactions on PAMI, 30(3):\n555\u2013560, 2008.\n[2] Y. Benezeth, P. Jodoin, V. Saligrama, and C. Rosenberger. Abnormal events detection\nbased on spatio-temporal co-occurences. In CVPR, 2009.\n[3] C. Chang and C. Lin. Libsvm: a library for support vector machines. ACM Transactions\non Intelligent Systems and Technology, 2(3):27, 2011.\n[4] Y. Cong, J. Yuan, and J. Liu. Sparse reconstruction cost for abnormal event detection.\nIn CVPR, 2011.\n[5] Z. Fu, W. Hu, and T. Tan. Similarity based vehicle trajectory clustering and anomaly\ndetection. In ICIP, 2005.\n[6] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate\nobject detection and semantic segmentation. In CVPR, 2014.\n[7] H. Grabner, F. Nater, M. Druey, and L. Van Gool. Visual interestingness in image\nsequences. In ACM MM, 2013.\n[8] T. Hospedales, S. Gong, and T. Xiang. Video behaviour mining using a dynamic topic\nmodel. IJCV, 98(3):303\u2013323, 2012.\nXU ET AL.: LEARNING DEEP REPRESENTATIONS OF APPEARANCE AND MOTION...\n11\n[9] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and\nT. Darrell. Caffe: Convolutional architecture for fast feature embedding. In ACM MM,\n2014.\n[10] P Jodoin, J. Konrad, and V. Saligrama. Modeling background activity for behavior\nsubtraction. In ICDSC, 2008.\n[11] J. Kim and K. Grauman. Observe locally, infer globally: a space-time mrf for detecting\nabnormal activities with incremental updates. In CVPR, 2009.\n[12] L. Kratz and K. Nishino. Anomaly detection in extremely crowded scenes using spatio-\ntemporal motion pattern models. In CVPR, 2009.\n[13] A. Krizhevsky, I. Sutskever, and G.E. Hinton. Imagenet classi\ufb01cation with deep con-\nvolutional neural networks. In NIPS, 2012.\n[14] Z. Lan, L. Bao, S. Yu, W. Liu, and A. G. Hauptmann. Double fusion for multimedia\nevent detection. In Advances in Multimedia Modeling, 2012.\n[15] C. Liu. Beyond pixels: exploring new representations and applications for motion\nanalysis. PhD thesis, Massachusetts Institute of Technology, 2009.\n[16] C. Lu, J. Shi, and J. Jia. Abnormal event detection at 150 fps in matlab. In ICCV, 2013.\n[17] V. Mahadevan, W. Li, V. Bhalodia, and N. Vasconcelos. Anomaly detection in crowded\nscenes. In CVPR, 2010.\n[18] R. Mehran, A. Oyama, and M. Shah. Abnormal crowd behavior detection using social\nforce model. In CVPR, 2009.\n[19] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y. Ng. Multimodal deep learning.\nIn ICML, 2011.\n[20] C. Piciarelli, C. Micheloni, and G. Foresti. Trajectory-based anomalous event detection.\nIEEE Transactions on Circuits and Systems for Video Technology, 18(11):1544\u20131554,\n2008.\n[21] V. Reddy, C. Sanderson, and B.C. Lovell. Improved anomaly detection in crowded\nscenes via cell-based analysis of foreground speed, size and texture. In CVPRW, 2011.\n[22] E. Ricci, G. Zen, N. Sebe, and S. Messelodi. A prototype learning framework using\nemd: Application to complex scenes analysis. IEEE Transactions on PAMI, 35(3):\n513\u2013526, 2013.\n[23] M. Roshtkhari and M. Levine. Online dominant and anomalous behavior detection in\nvideos. In CVPR, 2013.\n[24] V. Saligrama and Z. Chen. Video anomaly detection based on local statistical aggre-\ngates. In CVPR, 2012.\n[25] B. Sch\u00f6lkopf, J. Platt, J. Shawe-Taylor, A. Smola, and R. Williamson. Estimating the\nsupport of a high-dimensional distribution. Neural Computation, 13(7):1443\u20131471,\n2001.\n12\nXU ET AL.: LEARNING DEEP REPRESENTATIONS OF APPEARANCE AND MOTION...\n[26] K. Simonyan and A. Zisserman. Two-stream convolutional networks for action recog-\nnition in videos. In NIPS, 2014.\n[27] N. Srivastava and R. R. Salakhutdinov.\nMultimodal learning with deep boltzmann\nmachines. In NIPS, 2012.\n[28] C. Stauffer and W.E.L. Grimson. Learning patterns of activity using real-time tracking.\nIEEE Transactions on PAMI, 22(8):747\u2013757, 2000.\n[29] N. Vaswani, A. Roy-Chowdhury, and R. Chellappa. \" shape activity\": a continuous-\nstate hmm for moving/deforming shapes with application to abnormal activity detec-\ntion. IEEE Transactions on Image Processing, 14(10):1603\u20131616, 2005.\n[30] P. Vincent, H. Larochelle, Y. Bengio, and P. Manzagol. Extracting and composing\nrobust features with denoising autoencoders. In ICML, 2008.\n[31] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P. Manzagol. Stacked denoising\nautoencoders: Learning useful representations in a deep network with a local denoising\ncriterion. JMLR, 11:3371\u20133408, 2010.\n[32] N. Wang and D. Yeung. Learning a deep compact image representation for visual\ntracking. In NIPS, 2013.\n[33] X. Wang, K. Tieu, and E. Grimson. Learning semantic scene models by trajectory\nanalysis. In ECCV, 2006.\n[34] Y. Yan, E. Ricci, R. Subramanian, O. Lanz, and N. Sebe. No matter where you are:\nFlexible graph-guided multi-task learning for multi-view head pose classi\ufb01cation under\ntarget motion. In ICCV, 2013.\n[35] Y. Yan, E. Ricci, R. Subramanian, G. Liu, and N. Sebe. Multi-task linear discriminant\nanalysis for multi-view action recognition. IEEE Transactions on Image Processing,\n23(12):5599\u20135611, 2014.\n[36] R. Zaharescu, A.and Wildes. Anomalous behaviour detection using spatiotemporal\noriented energies, subset inclusion histogram comparison and event-driven processing.\nIn ECCV, 2010.\n[37] Y. Zhai and M. Shah. Visual attention detection in video sequences using spatiotempo-\nral cues. In ACM MM, 2006.\n[38] J. Zhang, S. Shan, M. Kan, and X. Chen. Coarse-to-\ufb01ne auto-encoder networks (cfan)\nfor real-time face alignment. In ECCV, 2014.\n[39] G. Zoutendijk. Methods of feasible directions: a study in linear and non-linear pro-\ngramming. 1960.\n",
        "sentence": " [17] used stacked denoising autoencoders to learn the deep features in an unsupervised fashion and use them to represent both appearance and motion of the scene.",
        "context": "[30] P. Vincent, H. Larochelle, Y. Bengio, and P. Manzagol. Extracting and composing\nrobust features with denoising autoencoders. In ICML, 2008.\n[31] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P. Manzagol. Stacked denoising\nautoencoders: Learning useful representations in a deep network with a local denoising\ncriterion. JMLR, 11:3371\u20133408, 2010.\n[32] N. Wang and D. Yeung. Learning a deep compact image representation for visual\ntracking. In NIPS, 2013.\nto learn appearance and motion features as well as their correlations. Deep learning meth-\nods for combining multiple modalities have been investigated in previous works [19, 27]."
    }
]