[
    {
        "title": "Freebase: A shared database of structured general human knowledge",
        "author": [
            "Robert Cook",
            "Patrick Tufts"
        ],
        "venue": "In Proceedings of the national conference on Artificial Intelligence,",
        "citeRegEx": "Bollacker et al\\.,? \\Q2007\\E",
        "shortCiteRegEx": "Bollacker et al\\.",
        "year": 2007,
        "abstract": "",
        "full_text": "",
        "sentence": " (2009) adopted Freebase (Bollacker et al., 2008; Bollacker et al., 2007), a large-scale crowdsourcing knowledge base online which contains billions of relation instances and thousands of relation names, to distantly supervise Wikipedia corpus. (2009) adopted Freebase (Bollacker et al., 2008; Bollacker et al., 2007), a large-scale crowdsourcing knowledge base online which contains billions of relation instances and thousands of relation names, to distantly supervise Wikipedia corpus. The basic alignment assumption of this work is that if a pair of entities participate in a relation, all sentences that mention these entities are labeled by that relation name. Then we can extract a variety of textual features and learn a multi-class logistic regression classifier. Inspired by multi-instance learning (Maron and Lozano-P\u00e9rez, 1998), Riedel et al. (2010) relaxed the strong assumption and replaced all sentences with at least one sentence. (2009) adopted Freebase (Bollacker et al., 2008; Bollacker et al., 2007), a large-scale crowdsourcing knowledge base online which contains billions of relation instances and thousands of relation names, to distantly supervise Wikipedia corpus. The basic alignment assumption of this work is that if a pair of entities participate in a relation, all sentences that mention these entities are labeled by that relation name. Then we can extract a variety of textual features and learn a multi-class logistic regression classifier. Inspired by multi-instance learning (Maron and Lozano-P\u00e9rez, 1998), Riedel et al. (2010) relaxed the strong assumption and replaced all sentences with at least one sentence. Hoffmann et al. (2011) pointed out that many entity",
        "context": null
    },
    {
        "title": "Freebase: a collaboratively created graph database for structuring human knowledge",
        "author": [
            "Colin Evans",
            "Praveen Paritosh",
            "Tim Sturge",
            "Jamie Taylor"
        ],
        "venue": "In Proceedings of the 2008 ACM SIGMOD international",
        "citeRegEx": "Bollacker et al\\.,? \\Q2008\\E",
        "shortCiteRegEx": "Bollacker et al\\.",
        "year": 2008,
        "abstract": "",
        "full_text": "",
        "sentence": " (2009) adopted Freebase (Bollacker et al., 2008; Bollacker et al., 2007), a large-scale crowdsourcing knowledge base online which contains billions of relation instances and thousands of relation names, to distantly supervise Wikipedia corpus.",
        "context": null
    },
    {
        "title": "Matrix completion for multi-label image classification",
        "author": [
            "Fernando Torre",
            "Jo\u00e3o P Costeira",
            "Alexandre Bernardino"
        ],
        "venue": "In Advances in Neural Information Processing Systems,",
        "citeRegEx": "Cabral et al\\.,? \\Q2011\\E",
        "shortCiteRegEx": "Cabral et al\\.",
        "year": 2011,
        "abstract": "",
        "full_text": "",
        "sentence": " This promising theory has been successfully applied on many active research areas, such as computer vision (Cabral et al., 2011), recommender system (Rennie and Srebro, 2005) and system controlling (Fazel et al. This promising theory has been successfully applied on many active research areas, such as computer vision (Cabral et al., 2011), recommender system (Rennie and Srebro, 2005) and system controlling (Fazel et al., 2001). Our models for relation extraction are based on the theoretic framework proposed by Goldberg et al. (2010), which formulated the multi-label transductive learning as a matrix completion problem.",
        "context": null
    },
    {
        "title": "Exact matrix completion via convex optimization",
        "author": [
            "Cand\u00e8s",
            "Recht2009] Emmanuel J Cand\u00e8s",
            "Benjamin Recht"
        ],
        "venue": "Foundations of Computational mathematics,",
        "citeRegEx": "Cand\u00e8s et al\\.,? \\Q2009\\E",
        "shortCiteRegEx": "Cand\u00e8s et al\\.",
        "year": 2009,
        "abstract": "We consider a problem of considerable practical interest: the recovery of a\ndata matrix from a sampling of its entries. Suppose that we observe m entries\nselected uniformly at random from a matrix M. Can we complete the matrix and\nrecover the entries that we have not seen?\n  We show that one can perfectly recover most low-rank matrices from what\nappears to be an incomplete set of entries. We prove that if the number m of\nsampled entries obeys m >= C n^{1.2} r log n for some positive numerical\nconstant C, then with very high probability, most n by n matrices of rank r can\nbe perfectly recovered by solving a simple convex optimization program. This\nprogram finds the matrix with minimum nuclear norm that fits the data. The\ncondition above assumes that the rank is not too large. However, if one\nreplaces the 1.2 exponent with 1.25, then the result holds for all values of\nthe rank. Similar results hold for arbitrary rectangular matrices as well. Our\nresults are connected with the recent literature on compressed sensing, and\nshow that objects other than signals and images can be perfectly reconstructed\nfrom very limited information.",
        "full_text": "Exact Matrix Completion via Convex Optimization\nEmmanuel J. Cand`es\u2020 and Benjamin Recht\u266f\n\u2020 Applied and Computational Mathematics, Caltech, Pasadena, CA 91125\n\u266fCenter for the Mathematics of Information, Caltech, Pasadena, CA 91125\nMay 2008\nAbstract\nWe consider a problem of considerable practical interest: the recovery of a data matrix from\na sampling of its entries. Suppose that we observe m entries selected uniformly at random from\na matrix M. Can we complete the matrix and recover the entries that we have not seen?\nWe show that one can perfectly recover most low-rank matrices from what appears to be an\nincomplete set of entries. We prove that if the number m of sampled entries obeys\nm \u2265C n1.2r log n\nfor some positive numerical constant C, then with very high probability, most n \u00d7 n matrices\nof rank r can be perfectly recovered by solving a simple convex optimization program. This\nprogram \ufb01nds the matrix with minimum nuclear norm that \ufb01ts the data. The condition above\nassumes that the rank is not too large. However, if one replaces the 1.2 exponent with 1.25,\nthen the result holds for all values of the rank. Similar results hold for arbitrary rectangular\nmatrices as well. Our results are connected with the recent literature on compressed sensing,\nand show that objects other than signals and images can be perfectly reconstructed from very\nlimited information.\nKeywords. Matrix completion, low-rank matrices, convex optimization, duality in optimiza-\ntion, nuclear norm minimization, random matrices, noncommutative Khintchine inequality, decou-\npling, compressed sensing.\n1\nIntroduction\nIn many practical problems of interest, one would like to recover a matrix from a sampling of its\nentries. As a motivating example, consider the task of inferring answers in a partially \ufb01lled out\nsurvey. That is, suppose that questions are being asked to a collection of individuals. Then we\ncan form a matrix where the rows index each individual and the columns index the questions.\nWe collect data to \ufb01ll out this table but unfortunately, many questions are left unanswered. Is it\npossible to make an educated guess about what the missing answers should be? How can one make\nsuch a guess? Formally, we may view this problem as follows. We are interested in recovering a\ndata matrix M with n1 rows and n2 columns but only get to observe a number m of its entries\nwhich is comparably much smaller than n1n2, the total number of entries. Can one recover the\nmatrix M from m of its entries? In general, everyone would agree that this is impossible without\nsome additional information.\n1\narXiv:0805.4471v1  [cs.IT]  29 May 2008\nIn many instances, however, the matrix we wish to recover is known to be structured in the\nsense that it is low-rank or approximately low-rank. (We recall for completeness that a matrix with\nn1 rows and n2 columns has rank r if its rows or columns span an r-dimensional space.) Below are\ntwo examples of practical scenarios where one would like to be able to recover a low-rank matrix\nfrom a sampling of its entries.\n\u2022 The Net\ufb02ix problem. In the area of recommender systems, users submit ratings on a subset\nof entries in a database, and the vendor provides recommendations based on the user\u2019s pref-\nerences [28,32]. Because users only rate a few items, one would like to infer their preference\nfor unrated items.\nA special instance of this problem is the now famous Net\ufb02ix problem [2]. Users (rows of the\ndata matrix) are given the opportunity to rate movies (columns of the data matrix) but users\ntypically rate only very few movies so that there are very few scattered observed entries of\nthis data matrix. Yet one would like to complete this matrix so that the vendor (here Net\ufb02ix)\nmight recommend titles that any particular user is likely to be willing to order. In this case,\nthe data matrix of all user-ratings may be approximately low-rank because it is commonly\nbelieved that only a few factors contribute to an individual\u2019s tastes or preferences.\n\u2022 Triangulation from incomplete data. Suppose we are given partial information about the dis-\ntances between objects and would like to reconstruct the low-dimensional geometry describing\ntheir locations. For example, we may have a network of low-power wirelessly networked sen-\nsors scattered randomly across a region. Suppose each sensor only has the ability to construct\ndistance estimates based on signal strength readings from its nearest fellow sensors. From\nthese noisy distance estimates, we can form a partially observed distance matrix. We can\nthen estimate the true distance matrix whose rank will be equal to two if the sensors are\nlocated in a plane or three if they are located in three dimensional space [24,31]. In this case,\nwe only need to observe a few distances per node to have enough information to reconstruct\nthe positions of the objects.\nThese examples are of course far from exhaustive and there are many other problems which fall in\nthis general category. For instance, we may have some very limited information about a covariance\nmatrix of interest. Yet, this covariance matrix may be low-rank or approximately low-rank because\nthe variables only depend upon a comparably smaller number of factors.\n1.1\nImpediments and solutions\nSuppose for simplicity that we wish to recover a square n \u00d7 n matrix M of rank r.1\nSuch a\nmatrix M can be represented by n2 numbers, but it only has (2n \u2212r)r degrees of freedom. This\nfact can be revealed by counting parameters in the singular value decomposition (the number of\ndegrees of freedom associated with the description of the singular values and of the left and right\nsingular vectors). When the rank is small, this is considerably smaller than n2. For instance, when\nM encodes a 10-dimensional phenomenon, then the number of degrees of freedom is about 20 n\no\ufb00ering a reduction in dimensionality by a factor about equal to n/20. When n is large (e.g. in the\nthousands or millions), the data matrix carries much less information than its ambient dimension\n1We emphasize that there is nothing special about M being square and all of our discussion would apply to\narbitrary rectangular matrices as well. The advantage of focusing on square matrices is a simpli\ufb01ed exposition and\nreduction in the number of parameters of which we need to keep track.\n2\nsuggests. The problem is now whether it is possible to recover this matrix from a sampling of its\nentries without having to probe all the n2 entries, or more generally collect n2 or more measurements\nabout M.\n1.1.1\nWhich matrices?\nIn general, one cannot hope to be able to recover a low-rank matrix from a sample of its entries.\nConsider the rank-1 matrix M equal to\nM = e1e\u2217\nn =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\n0\n0\n\u00b7 \u00b7 \u00b7\n0\n1\n0\n0\n\u00b7 \u00b7 \u00b7\n0\n0\n...\n...\n...\n...\n...\n0\n0\n\u00b7 \u00b7 \u00b7\n0\n0\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb,\n(1.1)\nwhere here and throughout, ei is the ith canonical basis vector in Euclidean space (the vector with\nall entries equal to 0 but the ith equal to 1). This matrix has a 1 in the top-right corner and all the\nother entries are 0. Clearly this matrix cannot be recovered from a sampling of its entries unless\nwe pretty much see all the entries. The reason is that for most sampling sets, we would only get to\nsee zeros so that we would have no way of guessing that the matrix is not zero. For instance, if we\nwere to see 90% of the entries selected at random, then 10% of the time we would only get to see\nzeroes.\nIt is therefore impossible to recover all low-rank matrices from a set of sampled entries but\ncan one recover most of them? To investigate this issue, we introduce a simple model of low-rank\nmatrices. Consider the singular value decomposition (SVD) of a matrix M\nM =\nr\nX\nk=1\n\u03c3kukv\u2217\nk,\n(1.2)\nwhere the uk\u2019s and vk\u2019s are the left and right singular vectors, and the \u03c3k\u2019s are the singular values\n(the roots of the eigenvalues of M \u2217M). Then we could think of a generic low-rank matrix as follows:\nthe family {uk}1\u2264k\u2264r is selected uniformly at random among all families of r orthonormal vectors,\nand similarly for the the family {vk}1\u2264k\u2264r. The two families may or may not be independent of\neach other. We make no assumptions about the singular values \u03c3k. In the sequel, we will refer to\nthis model as the random orthogonal model. This model is convenient in the sense that it is both\nvery concrete and simple, and useful in the sense that it will help us \ufb01x the main ideas. In the\nsequel, however, we will consider far more general models. The question for now is whether or not\none can recover such a generic matrix from a sampling of its entries.\n1.1.2\nWhich sampling sets?\nClearly, one cannot hope to reconstruct any low-rank matrix M\u2014even of rank 1\u2014if the sampling\nset avoids any column or row of M. Suppose that M is of rank 1 and of the form xy\u2217, x, y \u2208Rn\nso that the (i, j)th entry is given by\nMij = xiyj.\nThen if we do not have samples from the \ufb01rst row for example, one could never guess the value of\nthe \ufb01rst component x1, by any method whatsoever; no information about x1 is observed. There is\n3\nof course nothing special about the \ufb01rst row and this argument extends to any row or column. To\nhave any hope of recovering an unknown matrix, one needs at least one observation per row and\none observation per column.\nWe have just seen that if the sampling is adversarial, e.g. one observes all of the entries of M\nbut those in the \ufb01rst row, then one would not even be able to recover matrices of rank 1. But what\nhappens for most sampling sets? Can one recover a low-rank matrix from almost all sampling sets of\ncardinality m? Formally, suppose that the set \u2126of locations corresponding to the observed entries\n((i, j) \u2208\u2126if Mij is observed) is a set of cardinality m sampled uniformly at random. Then can\none recover a generic low-rank matrix M, perhaps with very large probability, from the knowledge\nof the value of its entries in the set \u2126?\n1.1.3\nWhich algorithm?\nIf the number of measurements is su\ufb03ciently large, and if the entries are su\ufb03ciently uniformly\ndistributed as above, one might hope that there is only one low-rank matrix with these entries. If\nthis were true, one would want to recover the data matrix by solving the optimization problem\nminimize\nrank(X)\nsubject to\nXij = Mij\n(i, j) \u2208\u2126,\n(1.3)\nwhere X is the decision variable and rank(X) is equal to the rank of the matrix X. The program\n(1.3) is a common sense approach which simply seeks the simplest explanation \ufb01tting the observed\ndata.\nIf there were only one low-rank object \ufb01tting the data, this would recover M.\nThis is\nunfortunately of little practical use because this optimization problem is not only NP-hard, but all\nknown algorithms which provide exact solutions require time doubly exponential in the dimension\nn of the matrix in both theory and practice [14].\nIf a matrix has rank r, then it has exactly r nonzero singular values so that the rank function\nin (1.3) is simply the number of nonvanishing singular values.\nIn this paper, we consider an\nalternative which minimizes the sum of the singular values over the constraint set. This sum is\ncalled the nuclear norm,\n\u2225X\u2225\u2217=\nn\nX\nk=1\n\u03c3k(X)\n(1.4)\nwhere, here and below, \u03c3k(X) denotes the kth largest singular value of X. The heuristic optimiza-\ntion is then given by\nminimize\n\u2225X\u2225\u2217\nsubject to\nXij = Mij\n(i, j) \u2208\u2126.\n(1.5)\nWhereas the rank function counts the number of nonvanishing singular values, the nuclear norm\nsums their amplitude and in some sense, is to the rank functional what the convex \u21131 norm is to\nthe counting \u21130 norm in the area of sparse signal recovery. The main point here is that the nuclear\nnorm is a convex function and, as we will discuss in Section 1.4 can be optimized e\ufb03ciently via\nsemide\ufb01nite programming.\n1.1.4\nA \ufb01rst typical result\nOur \ufb01rst result shows that, perhaps unexpectedly, this heuristic optimization recovers a generic M\nwhen the number of randomly sampled entries is large enough. We will prove the following:\n4\nTheorem 1.1 Let M be an n1 \u00d7 n2 matrix of rank r sampled from the random orthogonal model,\nand put n = max(n1, n2). Suppose we observe m entries of M with locations sampled uniformly at\nrandom. Then there are numerical constants C and c such that if\nm \u2265C n5/4r log n ,\n(1.6)\nthe minimizer to the problem (1.5) is unique and equal to M with probability at least 1\u2212cn\u22123; that\nis to say, the semide\ufb01nite program (1.5) recovers all the entries of M with no error. In addition,\nif r \u2264n1/5, then the recovery is exact with probability at least 1 \u2212cn\u22123 provided that\nm \u2265C n6/5r log n .\n(1.7)\nThe theorem states that a surprisingly small number of entries are su\ufb03cient to complete a generic\nlow-rank matrix. For small values of the rank, e.g. when r = O(1) or r = O(log n), one only needs\nto see on the order of n6/5 entries (ignoring logarithmic factors) which is considerably smaller than\nn2\u2014the total number of entries of a squared matrix. The real feat, however, is that the recovery\nalgorithm is tractable and very concrete. Hence the contribution is twofold:\n\u2022 Under the hypotheses of Theorem 1.1, there is a unique low-rank matrix which is consistent\nwith the observed entries.\n\u2022 Further, this matrix can be recovered by the convex optimization (1.5). In other words, for\nmost problems, the nuclear norm relaxation is formally equivalent to the combinatorially hard\nrank minimization problem (1.3).\nTheorem 1.1 is in fact a special instance of a far more general theorem that covers a much larger\nset of matrices M. We describe this general class of matrices and precise recovery conditions in\nthe next section.\n1.2\nMain results\nAs seen in our \ufb01rst example (1.1), it is impossible to recover a matrix which is equal to zero in\nnearly all of its entries unless we see all the entries of the matrix. To recover a low-rank matrix,\nthis matrix cannot be in the null space of the sampling operator giving the values of a subset of the\nentries. Now it is easy to see that if the singular vectors of a matrix M are highly concentrated,\nthen M could very well be in the null-space of the sampling operator. For instance consider the\nrank-2 symmetric matrix M given by\nM =\n2\nX\nk=1\n\u03c3kuku\u2217\nk,\nu1\n= (e1 + e2)/\n\u221a\n2,\nu2\n= (e1 \u2212e2)/\n\u221a\n2,\nwhere the singular values are arbitrary. Then this matrix vanishes everywhere except in the top-left\n2 \u00d7 2 corner and one would basically need to see all the entries of M to be able to recover this\nmatrix exactly by any method whatsoever. There is an endless list of examples of this sort. Hence,\nwe arrive at the notion that, somehow, the singular vectors need to be su\ufb03ciently spread\u2014that is,\nuncorrelated with the standard basis\u2014in order to minimize the number of observations needed to\nrecover a low-rank matrix.2 This motivates the following de\ufb01nition.\n2Both the left and right singular vectors need to be uncorrelated with the standard basis. Indeed, the matrix e1v\u2217\nhas its \ufb01rst row equal to v and all the others equal to zero. Clearly, this rank-1 matrix cannot be recovered unless\nwe basically see all of its entries.\n5\nDe\ufb01nition 1.2 Let U be a subspace of Rn of dimension r and PU be the orthogonal projection\nonto U. Then the coherence of U (vis-`a-vis the standard basis (ei)) is de\ufb01ned to be\n\u00b5(U) \u2261n\nr max\n1\u2264i\u2264n \u2225PUei\u22252.\n(1.8)\nNote that for any subspace, the smallest \u00b5(U) can be is 1, achieved, for example, if U is spanned\nby vectors whose entries all have magnitude 1/\u221an. The largest possible value for \u00b5(U) is n/r\nwhich would correspond to any subspace that contains a standard basis element.\nWe shall be\nprimarily interested in subspace with low coherence as matrices whose column and row spaces have\nlow coherence cannot really be in the null space of the sampling operator. For instance, we will see\nthat the random subspaces discussed above have nearly minimal coherence.\nTo state our main result, we introduce two assumptions about an n1 \u00d7 n2 matrix M whose\nSVD is given by M = P\n1\u2264k\u2264r \u03c3kukv\u2217\nk and with column and row spaces denoted by U and V\nrespectively.\nA0 The coherences obey max(\u00b5(U), \u00b5(V )) \u2264\u00b50 for some positive \u00b50.\nA1 The n1 \u00d7 n2 matrix P\n1\u2264k\u2264r ukv\u2217\nk has a maximum entry bounded by \u00b51\np\nr/(n1n2) in absolute\nvalue for some positive \u00b51.\nThe \u00b5\u2019s above may depend on r and n1, n2. Moreover, note that A1 always holds with \u00b51 = \u00b50\n\u221ar\nsince the (i, j)th entry of the matrix P\n1\u2264k\u2264r ukv\u2217\nk is given by P\n1\u2264k\u2264r uikvjk and by the Cauchy-\nSchwarz inequality,\n\f\f\f\f\f\f\nX\n1\u2264k\u2264r\nuikvjk\n\f\f\f\f\f\f\n\u2264\ns X\n1\u2264k\u2264r\n|uik|2\ns X\n1\u2264k\u2264r\n|vjk|2 \u2264\n\u00b50r\n\u221an1n2\n.\nHence, for su\ufb03ciently small ranks, \u00b51 is comparable to \u00b50. As we will see in Section 2, for larger\nranks, both subspaces selected from the uniform distribution and spaces constructed as the span\nof singular vectors with bounded entries are not only incoherent with the standard basis, but also\nobey A1 with high probability for values of \u00b51 at most logarithmic in n1 and/or n2. Below we will\nassume that \u00b51 is greater than or equal to 1.\nWe are in the position to state our main result: if a matrix has row and column spaces that are\nincoherent with the standard basis, then nuclear norm minimization can recover this matrix from\na random sampling of a small number of entries.\nTheorem 1.3 Let M be an n1\u00d7n2 matrix of rank r obeying A0 and A1 and put n = max(n1, n2).\nSuppose we observe m entries of M with locations sampled uniformly at random. Then there exist\nconstants C, c such that if\nm \u2265C max(\u00b52\n1, \u00b51/2\n0\n\u00b51, \u00b50n1/4) nr(\u03b2 log n)\n(1.9)\nfor some \u03b2 > 2, then the minimizer to the problem (1.5) is unique and equal to M with probability\nat least 1 \u2212cn\u2212\u03b2. For r \u2264\u00b5\u22121\n0 n1/5 this estimate can be improved to\nm \u2265C \u00b50 n6/5r(\u03b2 log n)\n(1.10)\nwith the same probability of success.\n6\nTheorem 1.3 asserts that if the coherence is low, few samples are required to recover M. For\nexample, if \u00b50 = O(1) and the rank is not too large, then the recovery is exact with large probability\nprovided that\nm \u2265C n6/5r log n .\n(1.11)\nWe give two illustrative examples of matrices with incoherent column and row spaces. This list is\nby no means exhaustive.\n1. The \ufb01rst example is the random orthogonal model. For values of the rank r greater than\nlog n, \u00b5(U) and \u00b5(V ) are O(1), \u00b51 = O(log n) both with very large probability. Hence, the\nrecovery is exact provided that m obeys (1.6) or (1.7). Specializing Theorem 1.3 to these\nvalues of the parameters gives Theorem 1.1. Hence, Theorem 1.1 is a special case of our\ngeneral recovery result.\n2. The second example is more general and, in a nutshell, simply requires that the components\nof the singular vectors of M are small. Assume that the uj and vj\u2019s obey\nmax\nij\n|\u27e8ei, uj\u27e9|2 \u2264\u00b5B/n,\nmax\nij\n|\u27e8ei, vj\u27e9|2 \u2264\u00b5B/n,\n(1.12)\nfor some value of \u00b5B = O(1). Then the maximum coherence is at most \u00b5B since \u00b5(U) \u2264\u00b5B\nand \u00b5(V ) \u2264\u00b5B. Further, we will see in Section 2 that A1 holds most of the time with\n\u00b51 = O(\u221alog n). Thus, for matrices with singular vectors obeying (1.12), the recovery is\nexact provided that m obeys (1.11) for values of the rank not exceeding \u00b5\u22121\nB n1/5.\n1.3\nExtensions\nOur main result (Theorem 1.3) extends to a variety of other low-rank matrix completion problems\nbeyond the sampling of entries. Indeed, suppose we have two orthonormal bases f1, . . . , fn and\ng1, . . . , gn of Rn, and that we are interested in solving the rank minimization problem\nminimize\nrank(X)\nsubject to\nf \u2217\ni Xgj = f \u2217\ni Mgj,\n(i, j) \u2208\u2126, .\n(1.13)\nThis comes up in a number of applications. As a motivating example, there has been a great deal of\ninterest in the machine learning community in developing specialized algorithms for the multiclass\nand multitask learning problems (see, e.g., [1, 3, 5]). In multiclass learning, the goal is to build\nmultiple classi\ufb01ers with the same training data to distinguish between more than two categories.\nFor example, in face recognition, one might want to classify whether an image patch corresponds\nto an eye, nose, or mouth. In multitask learning, we have a large set of data, but have a variety of\ndi\ufb00erent classi\ufb01cation tasks, and, for each task, only partial subsets of the data are relevant. For\ninstance, in activity recognition, we may have acquired sets of observations of multiple subjects\nand want to determine if each observed person is walking or running. However, a di\ufb00erent classi\ufb01er\nis to be learned for each individual, and it is not clear how having access to the full collection\nof observations can improve classi\ufb01cation performance. Multitask learning aims precisely to take\nadvantage of the access to the full database to improve performance on the individual tasks.\nIn the abstract formulation of this problem for linear classi\ufb01ers, we have K classes to distin-\nguish and are given training examples f1, . . . , fn. For each example, we are given partial labeling\ninformation about which classes it belongs or does not belong to. That is, for each example fj\n7\nand class k, we may either be told that fj belongs to class k, be told fj does not belong to class\nk, or provided no information about the membership of fj to class k. For each class 1 \u2264k \u2264K,\nwe would like to produce a linear function wk such that w\u2217\nkfi > 0 if fi belongs to class k and\nw\u2217\nkfi < 0 otherwise. Formally, we can search for the vector wk that satis\ufb01es the equality con-\nstraints w\u2217\nkfi = yik where yik = 1 if we are told that fi belongs to class k, yik = \u22121 if we are\ntold that fi does not belong to class k, and yik unconstrained if we are not provided information.\nA common hypothesis in the multitask setting is that the wk corresponding to each of the classes\ntogether span a very low dimensional subspace with dimension signi\ufb01cantly smaller than K [1,3,5].\nThat is, the basic assumption is that\nW = [w1, . . . , wK]\nis low-rank. Hence, the multiclass learning problem can be cast as (1.13) with observations of the\nform f \u2217\ni W ej.\nTo see that our theorem provides conditions under which (1.13) can be solved via nuclear norm\nminimization, note that there exist unitary transformations F and G such that ej = F fj and\nej = Ggj for each j = 1, . . . , n. Hence,\nf \u2217\ni Xgj = e\u2217\ni (F XG\u2217)ej.\nThen if the conditions of Theorem 1.3 hold for the matrix F XG\u2217, it is immediate that nuclear\nnorm minimization \ufb01nds the unique optimal solution of (1.13) when we are provided a large enough\nrandom collection of the inner products f \u2217\ni Mgj. In other words, all that is needed is that the\ncolumn and row spaces of M be respectively incoherent with the basis (fi) and (gi).\nFrom this perspective, we additionally remark that our results likely extend to the case where\none observes a small number of arbitrary linear functionals of a hidden matrix M. Set N = n2\nand A1, . . . , AN be an orthonormal basis for the linear space of n \u00d7 n matrices with the usual\ninner product \u27e8X, Y \u27e9= trace(X\u2217Y ). Then we expect our results should also apply to the rank\nminimization problem\nminimize\nrank(X)\nsubject to\n\u27e8Ak, X\u27e9= \u27e8Ak, M\u27e9\nk \u2208\u2126,\n(1.14)\nwhere \u2126\u2282{1, . . . , N} is selected uniformly at random. In fact, (1.14) is (1.3) when the orthobasis\nis the canonical basis (eie\u2217\nj)1\u2264i,j\u2264n. Here, those low-rank matrices which have small inner product\nwith all the basis elements Ak may be recoverable by nuclear norm minimization. To avoid unnec-\nessary confusion and notational clutter, we leave this general low-rank recovery problem for future\nwork.\n1.4\nConnections, alternatives and prior art\nNuclear norm minimization is a recent heuristic introduced by Fazel in [18], and is an extension of\nthe trace heuristic often used by the control community, see e.g. [6,26]. Indeed, when the matrix\nvariable is symmetric and positive semide\ufb01nite, the nuclear norm of X is the sum of the (nonneg-\native) eigenvalues and thus equal to the trace of X. Hence, for positive semide\ufb01nite unknowns,\n(1.5) would simply minimize the trace over the constraint set:\nminimize\ntrace(X)\nsubject to\nXij = Mij\n(i, j) \u2208\u2126\nX \u2ab00\n.\n8\nThis is a semide\ufb01nite program. Even for the general matrix M which may not be positive de\ufb01nite or\neven symmetric, the nuclear norm heuristic can be formulated in terms of semide\ufb01nite programming\nas, for instance, the program (1.5) is equivalent to\nminimize\ntrace(W1) + trace(W2)\nsubject to\nXij = Mij\n(i, j) \u2208\u2126\n\u0014W1\nX\nX\u2217\nW2\n\u0015\n\u2ab00\nwith optimization variables X, W1 and W2, (see, e.g., [18,35]). There are many e\ufb03cient algorithms\nand high-quality software available for solving these types of problems.\nOur work is inspired by results in the emerging \ufb01eld of compressive sampling or compressed\nsensing, a new paradigm for acquiring information about objects of interest from what appears to\nbe a highly incomplete set of measurements [11, 13, 17]. In practice, this means for example that\nhigh-resolution imaging is possible with fewer sensors, or that one can speed up signal acquisition\ntime in biomedical applications by orders of magnitude, simply by taking far fewer specially coded\nsamples. Mathematically speaking, we wish to reconstruct a signal x \u2208Rn from a small number\nmeasurements y = \u03a6x, y \u2208Rm, and m is much smaller than n; i.e. we have far fewer equations\nthan unknowns. In general, one cannot hope to reconstruct x but assume now that the object we\nwish to recover is known to be structured in the sense that it is sparse (or approximately sparse).\nThis means that the unknown object depends upon a smaller number of unknown parameters.\nThen it has been shown that \u21131 minimization allows recovery of sparse signals from remarkably few\nmeasurements: supposing \u03a6 is chosen randomly from a suitable distribution, then with very high\nprobability, all sparse signals with about k nonzero entries can be recovered from on the order of\nk log n measurements. For instance, if x is k-sparse in the Fourier domain, i.e. x is a superposition\nof k sinusoids, then it can be perfectly recovered with high probability\u2014by \u21131 minimization\u2014from\nthe knowledge of about k log n of its entries sampled uniformly at random [11].\nFrom this viewpoint, the results in this paper greatly extend the theory of compressed sensing\nby showing that other types of interesting objects or structures, beyond sparse signals and images,\ncan be recovered from a limited set of measurements. Moreover, the techniques for proving our\nmain results build upon ideas from the compressed sensing literature together with probabilistic\ntools such as the powerful techniques of Bourgain and of Rudelson for bounding norms of operators\nbetween Banach spaces.\nOur notion of incoherence generalizes the concept of the same name in compressive sampling.\nNotably, in [10], the authors introduce the notion of the incoherence of a unitary transformation.\nLetting U be an n \u00d7 n unitary matrix, the coherence of U is given by\n\u00b5(U) = n max\nj,k |Ujk|2.\nThis quantity ranges in values from 1 for a unitary transformation whose entries all have the same\nmagnitude to n for the identity matrix. Using this notion, [10] showed that with high probability,\na k-sparse signal could be recovered via linear programming from the observation of the inner\nproduct of the signal with m = \u2126(\u00b5(U)k log n) randomly selected columns of the matrix U.\nThis result provided a generalization of the celebrated results about partial Fourier observations\ndescribed in [11], a special case where \u00b5(U) = 1. This paper generalizes the notion of incoherence\nto problems beyond the setting of sparse signal recovery.\n9\nIn [27], the authors studied the nuclear norm heuristic applied to a related problem where\npartial information about a matrix M is available from m equations of the form\n\u27e8A(k), M\u27e9=\nX\nij\nA(k)\nij Mij = bk,\nk = 1, . . . , m,\n(1.15)\nwhere for each k, {A(k)\nij }ij is an i.i.d. sequence of Gaussian or Bernoulli random variables and\nthe sequences {A(k)} are also independent from each other (the sequences {A(k)} and {bk} are\navailable to the analyst).\nBuilding on the concept of restricted isometry introduced in [12] in\nthe context of sparse signal recovery, [27] establishes the \ufb01rst su\ufb03cient conditions for which the\nnuclear norm heuristic returns the minimum rank element in the constraint set. They prove that\nthe heuristic succeeds with large probability whenever the number m of available measurements is\ngreater than a constant times 2nr log n for n \u00d7 n matrices. Although this is an interesting result, a\nserious impediment to this approach is that one needs to essentially measure random projections of\nthe unknown data matrix\u2014a situation which unfortunately does not commonly arise in practice.\nFurther, the measurements in (1.15) give some information about all the entries of M whereas\nin our problem, information about most of the entries is simply not available. In particular, the\nresults and techniques introduced in [27] do not begin to address the matrix completion problem of\ninterest to us in this paper. As a consequence, our methods are completely di\ufb00erent; for example,\nthey do not rely on any notions of restricted isometry. Instead, as we discuss below, we prove\nthe existence of a Lagrange multiplier for the optimization (1.5) that certi\ufb01es the unique optimal\nsolution is precisely the matrix that we wish to recover.\nFinally, we would like to brie\ufb02y discuss the possibility of other recovery algorithms when the\nsampling happens to be chosen in a very special fashion. For example, suppose that M is generic\nand that we precisely observe every entry in the \ufb01rst r rows and columns of the matrix. Write M\nin block form as\nM =\n\u0014 M11\nM12\nM21\nM22\n\u0015\nwith M11 an r \u00d7 r matrix. In the special case that M11 is invertible and M has rank r, then it is\neasy to verify that M22 = M21M \u22121\n11 M12. One can prove this identity by forming the SVD of M,\nfor example. That is, if M is generic, and the upper r \u00d7 r block is invertible, and we observe every\nentry in the \ufb01rst r rows and columns, we can recover M. This result immediately generalizes to the\ncase where one observes precisely r rows and r columns and the r \u00d7 r matrix at the intersection of\nthe observed rows and columns is invertible. However, this scheme has many practical drawbacks\nthat stand in the way of a generalization to a completion algorithm from a general set of entries.\nFirst, if we miss any entry in these rows or columns, we cannot recover M, nor can we leverage\nany information provided by entries of M22. Second, if the matrix has rank less than r, and we\nobserve r rows and columns, a combinatorial search to \ufb01nd the collection that has an invertible\nsquare sub-block is required. Moreover, because of the matrix inversion, the algorithm is rather\nfragile to noise in the entries.\n1.5\nNotations and organization of the paper\nThe paper is organized as follows. We \ufb01rst argue in Section 2 that the random orthogonal model\nand, more generally, matrices with incoherent column and row spaces obey the assumptions of the\ngeneral Theorem 1.3. To prove Theorem 1.3, we \ufb01rst establish su\ufb03cient conditions which guarantee\n10\nthat the true low-rank matrix M is the unique solution to (1.5) in Section 3. One of these conditions\nis the existence of a dual vector obeying two crucial properties. Section 4 constructs such a dual\nvector and provides the overall architecture of the proof which shows that, indeed, this vector obeys\nthe desired properties provided that the number of measurements is su\ufb03ciently large. Surprisingly,\nas explored in Section 5, the existence of a dual vector certifying that M is unique is related to\nsome problems in random graph theory including \u201cthe coupon collector\u2019s problem.\u201d Following this\ndiscussion, we prove our main result via several intermediate results which are all proven in Section\n6. Section 7 introduces numerical experiments showing that matrix completion based on nuclear\nnorm minimization works well in practice. Section 8 closes the paper with a short summary of\nour \ufb01ndings, a discussion of important extensions and improvements. In particular, we will discuss\npossible ways of improving the 1.2 exponent in (1.10) so that it gets closer to 1.\nFinally, the\nAppendix provides proofs of auxiliary lemmas supporting our main argument.\nBefore continuing, we provide here a brief summary of the notations used throughout the\npaper. Matrices are bold capital, vectors are bold lowercase and scalars or entries are not bold. For\ninstance, X is a matrix and Xij its (i, j)th entry. Likewise x is a vector and xi its ith component.\nWhen we have a collection of vectors uk \u2208Rn for 1 \u2264k \u2264d, we will denote by uik the ith\ncomponent of the vector uk and [u1, . . . , ud] will denote the n \u00d7 d matrix whose kth column is uk.\nA variety of norms on matrices will be discussed. The spectral norm of a matrix is denoted\nby \u2225X\u2225. The Euclidean inner product between two matrices is \u27e8X, Y \u27e9= trace(X\u2217Y ), and the\ncorresponding Euclidean norm, called the Frobenius or Hilbert-Schmidt norm, is denoted \u2225X\u2225F .\nThat is, \u2225X\u2225F = \u27e8X, X\u27e91/2. The nuclear norm of a matrix X is \u2225X\u2225\u2217. The maximum entry of\nX (in absolute value) is denoted by \u2225X\u2225\u221e\u2261maxij |Xij|. For vectors, we will only consider the\nusual Euclidean \u21132 norm which we simply write as \u2225x\u2225.\nFurther, we will also manipulate linear transformation which acts on matrices and will use\ncaligraphic letters for these operators as in A(X).\nIn particular, the identity operator will be\ndenoted by I. The only norm we will consider for these operators is their spectral norm (the top\nsingular value) denoted by \u2225A\u2225= supX:\u2225X\u2225F \u22641 \u2225A(X)\u2225F .\nFinally, we adopt the convention that C denotes a numerical constant independent of the matrix\ndimensions, rank, and number of measurements, whose value may change from line to line. Certain\nspecial constants with precise numerical values will be ornamented with subscripts (e.g., CR). Any\nexceptions to this notational scheme will be noted in the text.\n2\nWhich matrices are incoherent?\nIn this section we restrict our attention to square n \u00d7 n matrices, but the extension to rectangular\nn1 \u00d7 n2 matrices immediately follows by setting n = max(n1, n2).\n2.1\nIncoherent bases span incoherent subspaces\nAlmost all n \u00d7 n matrices M with singular vectors {uk}1\u2264k\u2264r and {vk}1\u2264k\u2264r obeying the size\nproperty (1.12) also satisfy the assumptions A0 and A1 with \u00b50 = \u00b5B, \u00b51 = C\u00b5B\n\u221alog n for some\npositive constant C. As mentioned above, A0 holds automatically, but, observe that A1 would not\nhold with a small value of \u00b51 if two rows of the matrices [u1, . . . , ur] and [v1, . . . , vr] are identical\n11\nwith all entries of magnitude\np\n\u00b5B/n since it is not hard to see that in this case\n\u2225\nX\nk\nukv\u2217\nk\u2225\u221e= \u00b5B r/n.\nCertainly, this example is constructed in a very special way, and should occur infrequently. We\nnow show that it is generically unlikely.\nConsider the matrix\nr\nX\nk=1\n\u03f5kukv\u2217\nk,\n(2.1)\nwhere {\u03f5k}1\u2264k\u2264r is an arbitrary sign sequence. For almost all choices of sign sequences, A1 is\nsatis\ufb01ed with \u00b51 = O(\u00b5B\n\u221alog n). Indeed, if one selects the signs uniformly at random, then for\neach \u03b2 > 0,\nP(\u2225\nr\nX\nk=1\n\u03f5kukvk\u2225\u221e\u2265\u00b5B\np\n8\u03b2r log n/n) \u2264(2n2) n\u2212\u03b2.\n(2.2)\nThis is of interest because suppose the low-rank matrix we wish to recover is of the form\nM =\nr\nX\nk=1\n\u03bbkukv\u2217\nk\n(2.3)\nwith scalars \u03bbk. Since the vectors {uk} and {vk} are orthogonal, the singular values of M are\ngiven by |\u03bbk| and the singular vectors are given by sgn(\u03bbk)uk and vk for k = 1, . . . , r. Hence, in\nthis model A1 concerns the maximum entry of the matrix given by (2.1) with \u03f5k = sgn(\u03bbk). That\nis to say, for most sign patterns, the matrix of interest obeys an appropriate size condition. We\nemphasize here that the only thing that we assumed about the uk\u2019s and vk\u2019s was that they had\nsmall entries. In particular, they could be equal to each other as would be the case for a symmetric\nmatrix.\nThe claim (2.2) is a simple application of Hoe\ufb00ding\u2019s inequality. The (i, j)th entry of (2.1) is\ngiven by\nZij =\nX\n1\u2264k\u2264r\n\u03f5kuikvjk,\nand is a sum of r zero-mean independent random variables, each bounded by \u00b5B/n. Therefore,\nP(|Zij| \u2265\u03bb\u00b5B\n\u221ar/n) \u22642e\u2212\u03bb2/8.\nSetting \u03bb proportional to \u221alog n and applying the union bound gives the claim.\nTo summarize, we say that M is sampled from the incoherent basis model if it is of the form\nM =\nr\nX\nk=1\n\u03f5k\u03c3kukv\u2217\nk;\n(2.4)\n{\u03f5k}1\u2264k\u2264r is a random sign sequence, and {uk}1\u2264k\u2264r and {vk}1\u2264k\u2264r have maximum entries of size\nat most\np\n\u00b5B/n.\nLemma 2.1 There exist numerical constants c and C such that for any \u03b2 > 0, matrices from the\nincoherent basis model obey the assumption A1 with \u00b51 \u2264C\u00b5B\np\n(\u03b2 + 2) log n with probability at\nleast 1 \u2212cn\u2212\u03b2.\n12\n2.2\nRandom subspaces span incoherent subspaces\nIn this section, we prove that the random orthogonal model obeys the two assumptions A0 and\nA1 (with appropriate values for the \u00b5\u2019s) with large probability.\nLemma 2.2 Set \u00afr = max(r, log n).\nThen there exist constants C and c such that the random\northogonal model obeys:3\n1. maxi \u2225PUei\u22252 \u2264C \u00afr/n,\n2. \u2225P\n1\u2264k\u2264r ukv\u2217\nk\u2225\u221e\u2264C log n \u221a\u00afr/n.\nwith probability 1 \u2212cn\u22123 log n.\nWe note that an argument similar to the following proof would give that if C of the form K\u03b2 where\nK is a \ufb01xed numerical constant, we can achieve a probability at least 1 \u2212cn\u2212\u03b2 provided that n is\nsu\ufb03ciently large. To establish these facts, we make use of the standard result below [21].\nLemma 2.3 Let Yd be distributed as a chi-squared random variable with d degrees of freedom. Then\nfor each t > 0\nP(Yd \u2212d \u2265t\n\u221a\n2d + t2) \u2264e\u2212t2/2\nand\nP(Yd \u2212d \u2264\u2212t\n\u221a\n2d) \u2264e\u2212t2/2.\n(2.5)\nWe will use (2.5) as follows: for each \u03f5 \u2208(0, 1) we have\nP(Yd \u2265d (1 \u2212\u03f5)\u22121) \u2264e\u2212\u03f52d/4\nand\nP(Yd \u2264d (1 \u2212\u03f5)) \u2264e\u2212\u03f52d/4.\n(2.6)\nWe begin with the second assertion of Lemma 2.2 since it will imply the \ufb01rst as well. Observe\nthat it follows from\n\u2225PUei\u22252 =\nX\n1\u2264k\u2264r\nu2\nik,\n(2.7)\nthat Zr \u2261\u2225PUei\u22252 (a is \ufb01xed) is the squared Euclidean length of the \ufb01rst r components of a unit\nvector uniformly distributed on the unit sphere in n dimensions. Now suppose that x1, x2, . . . , xn\nare i.i.d. N(0, 1). Then the distribution of a unit vector uniformly distributed on the sphere is\nthat of x/\u2225x\u2225and, therefore, the law of Zr is that of Yr/Yn, where Yr = P\nk\u2264r x2\nk. Fix \u03f5 > 0 and\nconsider the event An,\u03f5 = {Yn/n \u22651 \u2212\u03f5}. For each \u03bb > 0, it follows from (2.6) that\nP(Zr \u2212r/n \u2265\u03bb\n\u221a\n2r/n) = P(Yr \u2265[r + \u03bb\n\u221a\n2r]Yn/n)\n\u2264P(Yr \u2265[r + \u03bb\n\u221a\n2r]Yn/n and An,\u03f5) + P(Ac\nn,\u03f5)\n\u2264P(Yr \u2265[r + \u03bb\n\u221a\n2r][1 \u2212\u03f5]) + e\u2212\u03f52n/4\n= P(Yr \u2212r \u2265\u03bb\n\u221a\n2r[1 \u2212\u03f5 \u2212\u03f5\np\nr/2\u03bb2]) + e\u2212\u03f52n/4.\nNow pick \u03f5 = 4(n\u22121 log n)1/2, \u03bb = 8\u221a2 log n and assume that n is su\ufb03ciently large so that\n\u03f5(1 +\np\nr/2\u03bb2) \u22641/2.\n3When r \u2265C\u2032(log n)3 for some positive constant C\u2032, a better estimate is possible, namely, \u2225P\nP\nP\n1\u2264k\u2264r ukv\u2217\nk\u2225\u221e\u2264\nC \u221ar log n/n.\n13\nThen\nP(Zr \u2212r/n \u2265\u03bb\n\u221a\n2r/n) \u2264P(Yr \u2212r \u2265(\u03bb/2)\n\u221a\n2r) + n\u22124.\nAssume now that r \u22654 log n (which means that \u03bb \u22644\n\u221a\n2r). Then it follows from (2.5) that\nP(Yr \u2212r \u2265(\u03bb/2)\n\u221a\n2r) \u2264P(Yr \u2212r \u2265(\u03bb/4)\n\u221a\n2r + (\u03bb/4)2) \u2264e\u2212\u03bb2/32 = n\u22124.\nHence\nP(Zr \u2212r/n \u226516\np\nr log n/n) \u22642n\u22124\nand, therefore,\nP(max\ni\n\u2225PUei\u22252 \u2212r/n \u226516\np\nr log n/n) \u22642n\u22123\n(2.8)\nby the union bound. Note that (2.8) establishes the \ufb01rst claim of the lemma (even for r < 4 log n\nsince in this case Zr \u2264Z\u23084 log n\u2309).\nIt remains to establish the second claim. Notice that by symmetry, E = P\n1\u2264k\u2264r ukv\u2217\nk has the\nsame distribution as\nF =\nr\nX\nk=1\n\u03f5kukv\u2217\nk,\nwhere {\u03f5k} is an independent Rademacher sequence. It then follows from Hoe\ufb00ding\u2019s inequality\nthat conditional on {uk} and {vk} we have\nP(|Fij| > t) \u22642e\u2212t2/2\u03c32\nij,\n\u03c32\nij =\nX\n1\u2264k\u2264r\nu2\nikv2\nik.\nOur previous results indicate that maxij |vij|2 \u2264(10 log n)/n with large probability and thus\n\u03c32\nij \u226410 log n\nn\n\u2225PUei\u22252.\nSet \u00afr = max(r, log n). Since \u2225PUei\u22252 \u2264C\u00afr/n with large probability, we have\n\u03c32\nij \u2264C(log n) \u00afr/n2\nwith large probability. Hence the marginal distribution of Fij obeys\nP(|Fij| > \u03bb\n\u221a\n\u00afr/n) \u22642e\u2212\u03b3\u03bb2/ log n + P(\u03c32\nij \u2265C(log n)\u00afr/n2).\nfor some numerical constant \u03b3.\nPicking \u03bb = \u03b3\u2032 log n where \u03b3\u2032 is a su\ufb03ciently large numerical\nconstant gives\n\u2225F \u2225\u221e\u2264C (log n)\n\u221a\n\u00afr/n\nwith large probability. Since E and F have the same distribution, the second claim follows.\nThe claim about the size of maxij |vij|2 is straightforward since our techniques show that for\neach \u03bb > 0\nP(Z1 \u2265\u03bb(log n)/n) \u2264P(Y1 \u2265\u03bb(1 \u2212\u03f5) log n) + e\u2212\u03f52n/4.\nMoreover,\nP(Y1 \u2265\u03bb(1 \u2212\u03f5) log n) = P(|x1| \u2265\np\n\u03bb(1 \u2212\u03f5) log n) \u22642e\u22121\n2 \u03bb(1\u2212\u03f5) log n.\nIf n is su\ufb03ciently large so that \u03f5 \u22641/5, this gives P(Z1 \u226510(log n)/n) \u22643n\u22124 and, therefore,\nP(max\nij\n|vij|2 \u226510(log n)/n) \u226412n\u22123 log n\nsince the maximum is taken over at most 4n log n pairs.\n14\n3\nDuality\nLet R\u2126: Rn1\u00d7n2 \u2192R|\u2126| be the sampling operator which extracts the observed entries, R\u2126(X) =\n(Xij)ij\u2208\u2126, so that the constraint in (1.5) becomes R\u2126(X) = R\u2126(M). Standard convex optimization\ntheory asserts that X is solution to (1.5) if there exists a dual vector (or Lagrange multiplier)\n\u03bb \u2208R|\u2126| such that R\u2217\n\u2126\u03bb is a subgradient of the nuclear norm at the point X, which we denote by\nR\u2217\n\u2126\u03bb \u2208\u2202\u2225X\u2225\u2217\n(3.1)\n(see, e.g. [7]). Recall the de\ufb01nition of a subgradient of a convex function f : Rn1\u00d7n2 \u2192R. We say\nthat Y is a subgradient of f at X0, denoted Y \u2208\u2202f(X0), if\nf(X) \u2265f(X0) + \u27e8Y , X \u2212X0\u27e9\n(3.2)\nfor all X.\nSuppose X0 \u2208Rn1\u00d7n2 has rank r with a singular value decomposition given by\nX0 =\nX\n1\u2264k\u2264r\n\u03c3k ukv\u2217\nk,\n(3.3)\nWith these notations, Y is a subgradient of the nuclear norm at X0 if and only if it is of the form\nY =\nX\n1\u2264k\u2264r\nukv\u2217\nk + W ,\n(3.4)\nwhere W obeys the following two properties:\n(i) the column space of W is orthogonal to U \u2261span (u1, . . . , ur), and the row space of W is\northogonal to V \u2261span (v1, . . . , vr);\n(ii) the spectral norm of W is less than or equal to 1.\n(see, e.g., [23,36]). To express these properties concisely, it is convenient to introduce the orthogonal\ndecomposition Rn1\u00d7n2 = T \u2295T \u22a5where T is the linear space spanned by elements of the form ukx\u2217\nand yv\u2217\nk, 1 \u2264k \u2264r, where x and y are arbitrary, and T \u22a5is its orthogonal complement. Note that\ndim(T) = r(n1 + n2 \u2212r), precisely the number of degrees of freedom in the set of n1 \u00d7 n2 matrices\nof rank r. T \u22a5is the subspace of matrices spanned by the family (xy\u2217), where x (respectively y) is\nany vector orthogonal to U (respectively V ).\nThe orthogonal projection PT onto T is given by\nPT (X) = PUX + XPV \u2212PUXPV ,\n(3.5)\nwhere PU and PV are the orthogonal projections onto U and V . Note here that while PU and PV\nare matrices, PT is a linear operator mapping matrices to matrices. We also have\nPT \u22a5(X) = (I \u2212PT )(X) = (In1 \u2212PU)X(In2 \u2212PV )\nwhere Id denotes the d \u00d7 d identity matrix. With these notations, Y \u2208\u2202\u2225X0\u2225\u2217if\n(i\u2019) PT (Y ) = P\n1\u2264k\u2264r ukv\u2217\nk,\n15\n(ii\u2019) and \u2225PT \u22a5Y \u2225\u22641.\nNow that we have characterized the subgradient of the nuclear norm, the lemma below gives\nsu\ufb03cient conditions for the uniqueness of the minimizer to (1.5).\nLemma 3.1 Consider a matrix X0 = Pr\nk=1 \u03c3k ukv\u2217\nk of rank r which is feasible for the problem\n(1.5), and suppose that the following two conditions hold:\n1. there exists a dual point \u03bb such that Y = R\u2217\n\u2126\u03bb obeys\nPT (Y ) =\nr\nX\nk=1\nukv\u2217\nk,\n\u2225PT \u22a5(Y )\u2225< 1;\n(3.6)\n2. the sampling operator R\u2126restricted to elements in T is injective.\nThen X0 is the unique minimizer.\nBefore proving this result, we would like to emphasize that this lemma provides a clear strategy\nfor proving our main result, namely, Theorem 1.3. Letting M = Pr\nk=1 \u03c3k ukv\u2217\nk, M is the unique\nsolution to (1.5) if the injectivity condition holds and if one can \ufb01nd a dual point \u03bb such that\nY = R\u2217\n\u2126\u03bb obeys (3.6).\nThe proof of Lemma 3.1 uses a standard fact which states that the nuclear norm and the spectral\nnorm are dual to one another.\nLemma 3.2 For each pair W and H, we have\n\u27e8W , H\u27e9\u2264\u2225W \u2225\u2225H\u2225\u2217.\nIn addition, for each H, there is a W obeying \u2225W \u2225= 1 which achieves the equality.\nA variety of proofs are available for this Lemma, and an elementary argument is sketched in [27].\nWe now turn to the proof of Lemma 3.1.\nProof [of Lemma 3.1] Consider any perturbation X0 + H where R\u2126(H) = 0. Then for any W 0\nobeying (i)\u2013(ii), Pr\nk=1 ukv\u2217\nk + W 0 is a subgradient of the nuclear norm at X0 and, therefore,\n\u2225X0 + H\u2225\u2217\u2265\u2225X0\u2225\u2217+ \u27e8\nr\nX\nk=1\nukv\u2217\nk + W 0, H\u27e9.\nLetting W = PT \u22a5(Y ), we may write Pr\nk=1 ukv\u2217\nk = R\u2217\n\u2126\u03bb \u2212W . Since \u2225W \u2225< 1 and R\u2126(H) = 0,\nit then follows that\n\u2225X0 + H\u2225\u2217\u2265\u2225X0\u2225\u2217+ \u27e8W 0 \u2212W , H\u27e9.\nNow by construction\n\u27e8W 0 \u2212W , H\u27e9= \u27e8PT \u22a5(W 0 \u2212W ), H\u27e9= \u27e8W 0 \u2212W , PT \u22a5(H)\u27e9.\nWe use Lemma 3.2 and set W 0 = PT \u22a5(Z) where Z is any matrix obeying \u2225Z\u2225\u22641 and\n\u27e8Z, PT \u22a5(H)\u27e9= \u2225PT \u22a5(H)\u2225\u2217. Then W 0 \u2208T \u22a5, \u2225W 0\u2225\u22641, and\n\u27e8W 0 \u2212W , H\u27e9\u2265(1 \u2212\u2225W \u2225) \u2225PT \u22a5(H)\u2225\u2217,\nwhich by assumption is strictly positive unless PT \u22a5(H) = 0. In other words, \u2225X0 + H\u2225\u2217> \u2225X0\u2225\u2217\nunless PT \u22a5(H) = 0. Assume then that PT \u22a5(H) = 0 or equivalently that H \u2208T. Then R\u2126(H) = 0\nimplies that H = 0 by the injectivity assumption. In conclusion, \u2225X0+H\u2225\u2217> \u2225X\u2225\u2217unless H = 0.\n16\n4\nArchitecture of the proof\nOur strategy to prove that M = P\n1\u2264k\u2264r \u03c3kukv\u2217\nk is the unique minimizer to (1.5) is to construct a\nmatrix Y which vanishes on \u2126c and obeys the conditions of Lemma 3.1 (and show the injectivity\nof the sampling operator restricted to matrices in T along the way). Set P\u2126to be the orthogonal\nprojector onto the indices in \u2126so that the (i, j)th component of P\u2126(X) is equal to Xij if (i, j) \u2208\u2126\nand zero otherwise. Our candidate Y will be the solution to\nminimize\n\u2225X\u2225F\nsubject to\n(PT P\u2126)(X) = Pr\nk=1 ukv\u2217\nk.\n(4.1)\nThe matrix Y vanishes on \u2126c as otherwise it would not be an optimal solution since P\u2126(Y )\nwould obey the constraint and have a smaller Frobenius norm. Hence Y = P\u2126(Y ) and PT (Y ) =\nPr\nk=1 ukv\u2217\nk. Since the Pythagoras formula gives\n\u2225Y \u22252\nF = \u2225PT (Y )\u22252\nF + \u2225PT \u22a5(Y )\u22252\nF = \u2225\nr\nX\nk=1\nukv\u2217\nk\u22252\nF + \u2225PT \u22a5(Y )\u22252\nF\n= r + \u2225PT \u22a5(Y )\u22252\nF ,\nminimizing the Frobenius norm of X amounts to minimizing the Frobenius norm of PT \u22a5(X) under\nthe constraint PT (X) = Pr\nk=1 ukv\u2217\nk. Our motivation is twofold. First, the solution to the least-\nsquares problem (4.1) has a closed form that is amenable to analysis. Second, by forcing PT \u22a5(Y )\nto be small in the Frobenius norm, we hope that it will be small in the spectral norm as well, and\nestablishing that \u2225PT \u22a5(Y )\u2225< 1 would prove that M is the unique solution to (1.5).\nTo compute the solution to (4.1), we introduce the operator A\u2126T de\ufb01ned by\nA\u2126T (M) = P\u2126PT (M).\nThen, if A\u2217\n\u2126T A\u2126T = PT P\u2126PT has full rank when restricted to T, the minimizer to (4.1) is given by\nY = A\u2126T (A\u2217\n\u2126T A\u2126T )\u22121(E),\nE \u2261\nr\nX\nk=1\nukv\u2217\nk.\n(4.2)\nWe clarify the meaning of (4.2) to avoid any confusion.\n(A\u2217\n\u2126T A\u2126T )\u22121(E) is meant to be that\nelement F in T obeying (A\u2217\n\u2126T A\u2126T )(F ) = E.\nTo summarize the aims of our proof strategy,\n\u2022 We must \ufb01rst show that A\u2217\n\u2126T A\u2126T = PT P\u2126PT is a one-to-one linear mapping from T onto\nitself. In this case, A\u2126T = P\u2126PT \u2014as a mapping from T to Rn1\u00d7n2\u2014is injective. This is\nthe second su\ufb03cient condition of Lemma 3.1. Moreover, our ansatz for Y given by (4.2) is\nwell-de\ufb01ned.\n\u2022 Having established that Y is well-de\ufb01ned, we will show that\n\u2225PT \u22a5(Y )\u2225< 1,\nthus proving the \ufb01rst su\ufb03cient condition.\n17\n4.1\nThe Bernoulli model\nInstead of showing that the theorem holds when \u2126is a set of size m sampled uniformly at random,\nwe prove the theorem for a subset \u2126\u2032 sampled according to the Bernoulli model. Here and be-\nlow, {\u03b4ij}1\u2264i\u2264n1,1\u2264j\u2264n2 is a sequence of independent identically distributed 0/1 Bernoulli random\nvariables with\nP(\u03b4ij = 1) = p \u2261\nm\nn1n2\n,\n(4.3)\nand de\ufb01ne\n\u2126\u2032 = {(i, j) : \u03b4ij = 1}.\n(4.4)\nNote that E |\u2126\u2032| = m, so that the average cardinality of \u2126\u2032 is that of \u2126. Then following the same\nreasoning as the argument developed in Section II.C of [11] shows that the probability of \u2018failure\u2019\nunder the uniform model is bounded by 2 times the probability of failure under the Bernoulli model;\nthe failure event is the event on which the solution to (1.5) is not exact. Hence, we can restrict our\nattention to the Bernoulli model and from now on, we will assume that \u2126is given by (4.4). This is\nadvantageous because the Bernoulli model admits a simpler analysis than uniform sampling thanks\nto the independence between the \u03b4ij\u2019s.\n4.2\nThe injectivity property\nWe study the injectivity of A\u2126T , which also shows that Y is well-de\ufb01ned. To prove this, we will\nshow that the linear operator p\u22121PT (P\u2126\u2212pI)PT has small operator norm, which we recall is\nsup\u2225X\u2225F \u22641 p\u22121\u2225PT (P\u2126\u2212pI)PT (X)\u2225F .\nTheorem 4.1 Suppose \u2126is sampled according to the Bernoulli model (4.3)\u2013(4.4) and put n =\nmax(n1, n2). Suppose that the coherences obey max(\u00b5(U), \u00b5(V )) \u2264\u00b50. Then, there is a numerical\nconstants CR such that for all \u03b2 > 1,\np\u22121 \u2225PT P\u2126PT \u2212pPT \u2225\u2264CR\nr\n\u00b50 nr(\u03b2 log n)\nm\n(4.5)\nwith probability at least 1 \u22123n\u2212\u03b2 provided that CR\nq\n\u00b50 nr(\u03b2 log n)\nm\n< 1.\nProof Decompose any matrix X as X = P\nab\u27e8X, eae\u2217\nb\u27e9eae\u2217\nb so that\nPT (X) =\nX\nab\n\u27e8PT (X), eae\u2217\nb\u27e9eae\u2217\nb =\nX\nab\n\u27e8X, PT (eae\u2217\nb)\u27e9eae\u2217\nb.\nHence, P\u2126PT (X) = P\nab \u03b4ab \u27e8X, PT (eae\u2217\nb)\u27e9eae\u2217\nb which gives\n(PT P\u2126PT )(X) =\nX\nab\n\u03b4ab \u27e8X, PT (eae\u2217\nb)\u27e9PT (eae\u2217\nb).\nIn other words,\nPT P\u2126PT =\nX\nab\n\u03b4ab PT (eae\u2217\nb) \u2297PT (eae\u2217\nb).\nIt follows from the de\ufb01nition (3.5) of PT that\nPT (eae\u2217\nb) = (PUea)e\u2217\nb + ea(PV eb)\u2217\u2212(PUea)(PV eb)\u2217.\n(4.6)\n18\nThis gives\n\u2225PT (eae\u2217\nb)\u22252\nF = \u27e8PT (eae\u2217\nb), eae\u2217\nb\u27e9= \u2225PUea\u22252 + \u2225PV eb\u22252 \u2212\u2225PUea\u22252 \u2225PV eb\u22252\n(4.7)\nand since \u2225PUea\u22252 \u2264\u00b5(U)r/n1 and \u2225PV eb\u22252 \u2264\u00b5(U)r/n2,\n\u2225PT (eae\u2217\nb)\u22252\nF \u22642\u00b50r/ min(n1, n2).\n(4.8)\nNow the fact that the operator PT P\u2126PT does not deviate from its expected value\nE(PT P\u2126PT ) = PT (E P\u2126)PT = PT (pI)PT = pPT\nin the spectral norm is related to Rudelson\u2019s selection theorem [29]. The \ufb01rst part of the theorem\nbelow may be found in [10] for example, see also [30] for a very similar statement.\nTheorem 4.2\n[10] Let {\u03b4ab} be independent 0/1 Bernoulli variables with P(\u03b4ab = 1) = p =\nm\nn1n2\nand put n = max(n1, n2). Suppose that \u2225PT (eae\u2217\nb)\u22252\nF \u22642\u00b50r/n. Set\nZ \u2261p\u22121\u2225\nX\nab\n(\u03b4ab \u2212p) PT (eae\u2217\nb) \u2297PT (eae\u2217\nb)\u2225= p\u22121\u2225PT P\u2126PT \u2212pPT \u2225.\n1. There exists a constant C\u2032\nR such that\nE Z \u2264C\u2032\nR\nr\n\u00b50 nr log n\nm\n(4.9)\nprovided that the right-hand side is smaller than 1.\n2. Suppose E Z \u22641. Then for each \u03bb > 0, we have\nP\n \n|Z \u2212E Z| > \u03bb\nr\n\u00b50 nr log n\nm\n!\n\u22643 exp\n \n\u2212\u03b3\u2032\n0 min\n(\n\u03bb2 log n, \u03bb\ns\nm log n\n\u00b50 nr\n)!\n(4.10)\nfor some positive constant \u03b3\u2032\n0.\nAs mentioned above, the \ufb01rst part, namely, (4.9) is an application of an established result which\nstates that if {yi} is a family of vectors in Rd and {\u03b4i} is a 0/1 Bernoulli sequence with P(\u03b4i = 1) = p,\nthen\np\u22121\u2225\nX\ni\n(\u03b4i \u2212p)yi \u2297yi\u2225\u2264C\ns\nlog d\np\nmax\ni\n\u2225yi\u2225\nfor some C > 0 provided that the right-hand side is less than 1. The proof may be found in the\ncited literature, e.g. in [10]. Hence, the \ufb01rst part follows from applying this result to vectors of\nthe form PT (eae\u2217\nb) and using the available bound on \u2225PT (eae\u2217\nb)\u2225F . The second part follows from\nTalagrand\u2019s concentration inequality and may be found in the Appendix.\nSet \u03bb =\np\n\u03b2/\u03b3\u2032\n0 and assume that m > (\u03b2/\u03b3\u2032\n0)\u00b50 nr log n. Then the left-hand side of (4.10) is\nbounded by 3n\u2212\u03b2 and thus, we established that\nZ \u2264C\u2032\nR\nr\n\u00b50 nr log n\nm\n+\n1\np\n\u03b3\u2032\n0\nr\n\u00b50 nr \u03b2 log n\nm\n19\nwith probability at least 1 \u22123n\u2212\u03b2. Setting CR = C\u2032\nR + 1/\np\n\u03b3\u2032\n0 \ufb01nishes the proof.\nTake m large enough so that CR\np\n\u00b50 (nr/m) log n \u22641/2. Then it follows from (4.5) that\np\n2\u2225PT (X)\u2225F \u2264\u2225(PT P\u2126PT )(X)\u2225F \u22643p\n2 \u2225PT (X)\u2225F\n(4.11)\nfor all X with large probability. In particular, the operator A\u2217\n\u2126T A\u2126T = PT P\u2126PT mapping T onto\nitself is well-conditioned and hence invertible. An immediate consequence is the following:\nCorollary 4.3 Assume that CR\np\n\u00b50nr(log n)/m \u22641/2. With the same probability as in Theorem\n4.1, we have\n\u2225P\u2126PT (X)\u2225F \u2264\np\n3p/2\u2225PT (X)\u2225F .\n(4.12)\nProof We have \u2225P\u2126PT (X)\u22252\nF = \u27e8X, (P\u2126PT )\u2217(P\u2126PT )X\u27e9= \u27e8X, (PT P\u2126PT )X\u27e9and thus\n\u2225P\u2126PT (X)\u22252\nF = \u27e8PT X, (PT P\u2126PT )X\u27e9\u2264\u2225PT (X)\u2225F \u2225(PT P\u2126PT )(X)\u2225F ,\nwhere the inequality is due to Cauchy-Schwarz. The conclusion (4.12) follows from (4.11).\n4.3\nThe size property\nIn this section, we explain how we will show that \u2225PT \u22a5(Y )\u2225< 1. This result will follow from \ufb01ve\nlemmas that we will prove in Section 6. Introduce\nH \u2261PT \u2212p\u22121PT P\u2126PT ,\nwhich obeys \u2225H(X)\u2225F \u2264CR\np\n\u00b50(nr/m) \u03b2 log n\u2225PT (X)\u2225F with large probability because of The-\norem 4.1. For any matrix X \u2208T, (PT P\u2126PT )\u22121(X) can be expressed in terms of the power series\n(PT P\u2126PT )\u22121(X) = p\u22121(X + H(X) + H2(X) + . . .)\nfor H is a contraction when m is su\ufb03ciently large. Since Y = P\u2126PT (PT P\u2126PT )\u22121(P\n1\u2264k\u2264r ukv\u2217\nk),\nPT \u22a5(Y ) may be decomposed as\nPT \u22a5(Y ) = p\u22121(PT \u22a5P\u2126PT )(E + H(E) + H2(E) + . . .),\nE =\nX\n1\u2264k\u2264r\nukv\u2217\nk.\n(4.13)\nTo bound the norm of the left-hand side, it is of course su\ufb03cient to bound the norm of the summands\nin the right-hand side. Taking the following \ufb01ve lemmas together establishes Theorem 1.3.\nLemma 4.4 Fix \u03b2 \u22652 and \u03bb \u22651. There is a numerical constant C0 such that if m \u2265\u03bb \u00b52\n1 nr\u03b2 log n,\nthen\np\u22121 \u2225(PT \u22a5P\u2126PT )E\u2225\u2264C0 \u03bb\u22121/2.\n(4.14)\nwith probability at least 1 \u2212n\u2212\u03b2.\nLemma 4.5 Fix \u03b2 \u22652 and \u03bb \u22651. There are numerical constants C1 and c1 such that if m \u2265\n\u03bb \u00b51 max(\u221a\u00b50, \u00b51) nr\u03b2 log n, then\np\u22121 \u2225(PT \u22a5P\u2126PT )H(E)\u2225\u2264C1 \u03bb\u22121\n(4.15)\nwith probability at least 1 \u2212c1n\u2212\u03b2.\n20\nLemma 4.6 Fix \u03b2 \u22652 and \u03bb \u22651. There are numerical constants C2 and c2 such that if m \u2265\n\u03bb \u00b54/3\n0\nnr4/3\u03b2 log n, then\np\u22121 \u2225(PT \u22a5P\u2126PT )H2(E)\u2225\u2264C2 \u03bb\u22123/2\n(4.16)\nwith probability at least 1 \u2212c2n\u2212\u03b2.\nLemma 4.7 Fix \u03b2 \u22652 and \u03bb \u22651. There are numerical constants C3 and c3 such that if m \u2265\n\u03bb\u00b52\n0 nr2\u03b2 log n, then\np\u22121 \u2225(PT \u22a5P\u2126PT )H3(E)\u2225\u2264C3 \u03bb\u22121/2\n(4.17)\nwith probability at least 1 \u2212c3n\u2212\u03b2.\nLemma 4.8 Under the assumptions of Theorem 4.1, there is a numerical constant Ck0 such that\nif m \u2265(2CR)2\u00b50nr\u03b2 log n, then\np\u22121 \u2225(PT \u22a5P\u2126PT )\nX\nk\u2265k0\nHk(E)\u2225\u2264Ck0\n\u0012n2r\nm\n\u00131/2 \u0012\u00b50nr\u03b2 log n\nm\n\u0013k0/2\n(4.18)\nwith probability at least 1 \u2212n\u2212\u03b2.\nLet us now show how we may combine these lemmas to prove our main results. Under all of the\nassumptions of Theorem 1.3, consider the four Lemmas 4.4, 4.5, 4.6 and 4.8, the latter applied with\nk0 = 3. Together they imply that there are numerical constants c and C such that \u2225PT \u22a5(Y )\u2225< 1\nwith probability at least 1 \u2212cn\u2212\u03b2 provided that the number of samples obeys\nm \u2265C max(\u00b52\n1, \u00b51/2\n0\n\u00b51, \u00b54/3\n0\nr1/3, \u00b50n1/4) nr\u03b2 log n\n(4.19)\nfor some constant C. The four expressions in the maximum come from Lemmas 4.4, 4.5, 4.6 and\n4.8 in this order. Now the bound (4.19) is only interesting in the range when \u00b50n1/4r is smaller\nthan a constant times n as otherwise the right-hand side is greater than n2 (this would say that one\nwould see all the entries in which case our claim is trivial). When \u00b50r \u2264n3/4, (\u00b50r)4/3 \u2264\u00b50n5/4r\nand thus the recovery is exact provided that m obeys (1.9).\nFor the case concerning small values of the rank, we consider all \ufb01ve lemmas and apply Lemma\n4.8, the latter applied with k0 = 4. Together they imply that \u2225PT \u22a5(Y )\u2225< 1 with probability at\nleast 1 \u2212cn\u2212\u03b2 provided that the number of samples obeys\nm \u2265C max(\u00b52\n0r, \u00b50n1/5) nr\u03b2 log n\n(4.20)\nfor some constant C. The two expressions in the maximum come from Lemmas 4.7 and 4.8 in this\norder. The reason for this simpli\ufb01ed formulation is that the terms \u00b52\n1, \u00b51/2\n0\n\u00b51 and \u00b54/3\n0\nr1/3 which\ncome from Lemmas 4.4, 4.5 and 4.6 are bounded above by \u00b52\n0r since \u00b51 \u2264\u00b50\n\u221ar. When \u00b50r \u2264n1/5,\nthe recovery is exact provided that m obeys (1.10).\n5\nConnections with Random Graph Theory\n5.1\nThe injectivity property and the coupon collector\u2019s problem\nWe argued in the Introduction that to have any hope of recovering an unknown matrix of rank 1\nby any method whatsoever, one needs at least one observation per row and one observation per\n21\ncolumn. Sample m entries uniformly at random. Viewing the row indices as bins, assign the kth\nsampled entry to the bin corresponding to its row index. Then to have any hope of recovering our\nmatrix, all the bins need to be occupied. Quantifying how many samples are required to \ufb01ll all of\nthe bins is the famous coupon collector\u2019s problem.\nCoupon collection is also connected to the injectivity of the sampling operator P\u2126restricted to\nelements in T. Suppose we sample the entries of a rank 1 matrix equal to xy\u2217with left and right\nsingular vectors u = x/\u2225x\u2225and v = y/\u2225y\u2225respectively and have not seen anything in the ith\nrow. Then we claim that P\u2126(restricted to T) has a nontrivial null space and thus PT P\u2126PT is not\ninvertible. Indeed, consider the matrix eiv\u2217. This matrix is in T and\nP\u2126(eiv\u2217) = 0\nsince eiv\u2217vanishes outside of the ith row. The same applies to the columns as well. If we have not\nseen anything in column j, then the rank-1 matrix ue\u2217\nj \u2208T and P\u2126(ue\u2217\nj) = 0. In conclusion, the\ninvertibility of PT P\u2126PT implies a complete collection.\nWhen the entries are sampled uniformly at random, it is well known that one needs on the\norder of n log n samples to sample all the rows. What is interesting is that Theorem 4.1 implies\nthat PT P\u2126PT is invertible\u2014a stronger property\u2014when the number of samples is also on the order\nof n log n. A particular implication of this discussion is that the logarithmic factors in Theorem 4.1\nare unavoidable.\n5.2\nThe injectivity property and the connectivity problem\nTo recover a matrix of rank 1, one needs much more than at least one observation per row and\ncolumn. Let R be the set of row indices, 1 \u2264i \u2264n, and C be the set of column indices, 1 \u2264j \u2264n,\nand consider the bipartite graph connecting vertices i \u2208R to vertices j \u2208C if and only if (i, j) \u2208\u2126,\ni.e. the (i, j)th entry is observed. We claim that if this graph is not fully connected, then one cannot\nhope to recover a matrix of rank 1.\nTo see this, we let I be the set of row indices and J be the set of column indices in any\nconnected component. We will assume that I and J are nonempty as otherwise, one is in the\npreviously discussed situation where some rows or columns are not sampled. Consider a rank 1\nmatrix equal to xy\u2217as before with singular vectors u = x/\u2225x\u2225and v = y/\u2225y\u2225. Then all the\ninformation about the values of the xi\u2019s with i \u2208I and of the yj\u2019s with j \u2208J are given by the\nsampled entries connecting I to J since all the other observed entries connect vertices in Ic to those\nin Jc. Now even if one observes all the entries xiyj with i \u2208I and j \u2208J, then at least the signs of\nxi, i \u2208I, and of yj, j \u2208J, would remain undetermined. Indeed, if the values (xi)i\u2208I, (yj)j\u2208J are\nconsistent with the observed entries, so are the values (\u2212xi)i\u2208I, (\u2212yj)j\u2208J. However, since the same\nanalysis holds for the sets Ic and Jc, there are at least two matrices consistent with the observed\nentries and exact matrix completion is impossible.\nThe connectivity of the graph is also related to the injectivity of the sampling operator P\u2126\nrestricted to elements in T. If the graph is not fully connected, then we claim that P\u2126(restricted\nto T) has a nontrivial null space and thus PT P\u2126PT is not invertible. Indeed, consider the matrix\nM = av\u2217+ ub\u2217,\nwhere ai = \u2212ui if i \u2208I and ai = ui otherwise, and bj = vj if j \u2208J and bj = \u2212vj otherwise. Then\nthis matrix is in T and obeys\nMij = 0\n22\nif (i, j) \u2208I \u00d7J or (i, j) \u2208Ic \u00d7Jc. Note that on the complement, i.e. (i, j) \u2208I \u00d7Jc or (i, j) \u2208Ic \u00d7J,\none has Mij = 2uivj and one can show that M \u0338= 0 unless uv\u2217= 0. Since \u2126is included in the\nunion of I \u00d7 J and Ic \u00d7 Jc, we have that P\u2126(M) = 0. In conclusion, the invertibility of PT P\u2126PT\nimplies a fully connected graph.\nWhen the entries are sampled uniformly at random, it is well known that one needs on the order\nof n log n samples to obtain a fully connected graph with large probability (see, e.g., [8]). Remark-\nably, Theorem 4.1 implies that PT P\u2126PT is invertible\u2014a stronger property\u2014when the number of\nsamples is also on the order of n log n.\n6\nProofs of the Critical Lemmas\nIn this section, we prove the \ufb01ve lemmas of Section 4.3. Before we begin, however, we develop a\nsimple estimate which we will use throughout. For each pair (a, b) and (a\u2032, b\u2032), it follows from the\nexpression of PT (eae\u2217\nb) (4.6) that\n\u27e8PT (ea\u2032e\u2217\nb\u2032), eae\u2217\nb\u27e9= \u27e8ea, PUea\u2032\u27e91{b=b\u2032} + \u27e8eb, PV eb\u2032\u27e91{a=a\u2032} \u2212\u27e8ea, PUea\u2032\u27e9\u27e8eb, PV eb\u2032\u27e9.\n(6.1)\nFix \u00b50 obeying \u00b5(U) \u2264\u00b50 and \u00b5(V ) \u2264\u00b50 and note that\n|\u27e8ea, PUea\u2032\u27e9| = |\u27e8PUea, PUea\u2032\u27e9| \u2264\u2225PUea\u2225\u2225PUea\u2032\u2225\u2264\u00b50r/n1\nand similarly for \u27e8eb, PV eb\u2032\u27e9. Suppose that b = b\u2032 and a \u0338= a\u2032, then\n|\u27e8PT (ea\u2032e\u2217\nb\u2032), eae\u2217\nb\u27e9| = |\u27e8ea, PUea\u2032\u27e9|(1 \u2212\u2225PV eb\u22252) \u2264\u00b50r/n1.\nWe have a similar bound when a = a\u2032 and b \u0338= b\u2032 whereas when a \u0338= a\u2032 and b \u0338= b\u2032,\n|\u27e8PT (ea\u2032e\u2217\nb\u2032), eae\u2217\nb\u27e9| \u2264(\u00b50r)2/(n1n2).\nIn short, it follows from this analysis (and from (4.8) for the case where (a, b) = (a\u2032, b\u2032)) that\nmax\nab,a\u2032b\u2032 |\u27e8PT (ea\u2032e\u2217\nb\u2032), eae\u2217\nb\u27e9| \u22642\u00b50r/ min(n1, n2).\n(6.2)\nA consequence of (4.8) is the estimate:\nX\na\u2032b\u2032\n|\u27e8PT (ea\u2032e\u2217\nb\u2032), eae\u2217\nb\u27e9|2 =\nX\na\u2032b\u2032\n|\u27e8PT (eae\u2217\nb), ea\u2032e\u2217\nb\u2032\u27e9|2\n= \u2225PT (eae\u2217\nb)\u22252\nF \u22642\u00b50r/ min(n1, n2),\n(6.3)\nwhich we will apply several times. A related estimate is this:\nmax\na\nX\nb\n|Eab|2 \u2264\u00b50r/ min(n1, n2),\n(6.4)\nand the same is true by exchanging the role of a and b. To see this, write\nX\nb\n|Eab|2 = \u2225e\u2217\naE\u22252 = \u2225\nX\nj\u2264r\nvj\u27e8uj, ea\u27e9\u22252 =\nX\nj\u2264r\n|\u27e8uj, ea\u27e9|2 = \u2225PUea\u22252,\n23\nand the conclusion follows from the coherence property.\nWe will prove the lemmas in the case where n1 = n2 = n for simplicity, i.e. in the case of\nsquare matrices of dimension n. The general case is treated in exactly the same way. In fact, the\nargument only makes use of the bounds (6.2), (6.3) (and sometimes (6.4)), and the general case is\nobtained by replacing n with min(n1, n2).\nEach of the following subsections computes the operator norm of some random variable. In\neach section, we denote S as the quantity whose norm we wish to analyze. We will also frequently\nuse the notation H for some auxiliary matrix variable whose norm we will need to bound. Hence,\nwe will reuse the same notation many times rather than introducing a dozens new names\u2014just like\nin computer programming where one uses the same variable name in distinct routines.\n6.1\nProof of Lemma 4.4\nIn this section, we develop a bound on\np\u22121\u2225PT \u22a5P\u2126PT (E)\u2225= p\u22121\u2225PT \u22a5(P\u2126\u2212pI)PT (E)\u2225\n\u2264p\u22121\u2225(P\u2126\u2212pI)(E)\u2225,\nwhere the equality follows from PT \u22a5PT = 0, and the inequality from PT (E) = E together with\n\u2225PT \u22a5(X)\u2225\u2264\u2225X\u2225which is valid for any matrix X. Set\nS \u2261p\u22121(P\u2126\u2212pI)(E) = p\u22121 X\nab\n(\u03b4ab \u2212p)Eabeae\u2217\nb.\n(6.5)\nWe think of S as a random variable since it depends on the random \u03b4ab\u2019s, and note that E S = 0.\nThe proof of Lemma 4.4 operates by developing an estimate on the size of (E \u2225S\u2225q)1/q for some\nq \u22651 and by applying Markov inequality to bound the tail of the random variable \u2225S\u2225. To do this,\nwe shall use a symmetrization argument and the noncommutative Khintchine inequality. Since the\nfunction f(S) = \u2225S\u2225q is convex, Jensen\u2019s inequality gives that\nE \u2225S\u2225q \u2264E \u2225S \u2212S\u2032\u2225q,\nwhere S\u2032 = p\u22121 P\nab(\u03b4\u2032\nab \u2212p)Eabeae\u2217\nb is an independent copy of S. Since (\u03b4ab \u2212\u03b4\u2032\nab) is symmetric,\nS \u2212S\u2032 has the same distribution as\np\u22121 X\nab\n\u03f5ab(\u03b4ab \u2212\u03b4\u2032\nab)Eabeae\u2217\nb \u2261S\u03f5 \u2212S\u2032\n\u03f5,\nwhere {\u03f5ab} is an independent Rademacher sequence and S\u03f5 = p\u22121 P\nab \u03f5ab\u03b4abEabeae\u2217\nb. Further, the\ntriangle inequality gives\n(E \u2225S\u03f5 \u2212S\u2032\n\u03f5\u2225q)1/q \u2264(E \u2225S\u03f5\u2225q)1/q + (E \u2225S\u2032\n\u03f5\u2225q)1/q = 2(E \u2225S\u03f5\u2225q)1/q\nsince S\u03f5 and S\u2032\n\u03f5 have the same distribution and, therefore,\n(E \u2225S\u2225q)1/q \u22642p\u22121\n \nE\u03b4 E\u03f5 \u2225\nX\nab\n\u03f5ab\u03b4ab Eabeae\u2217\nb\u2225q\n!1/q\n.\n24\nWe are now in position to apply the noncommutative Khintchine inequality which bounds the\nSchatten norm of a Rademacher series. For q \u22651, the Schatten q-norm of a matrix is denoted by\n\u2225X\u2225Sq =\n n\nX\ni=1\n\u03c3i(X)q\n!1/q\n.\nNote that the nuclear norm is equal to the Schatten 1-norm and the Frobenius norm is equal to\nthe Schatten 2-norm. The following theorem was originally proven by Lust-Picquard [25], and was\nlater sharpened by Buchholz [9].\nLemma 6.1 (Noncommutative Khintchine inequality) Let (Xi)1\u2264i\u2264r be a \ufb01nite sequence of\nmatrices of the same dimension and let {\u03f5i} be a Rademacher sequence. For each q \u22652\n\uf8ee\n\uf8f0E\u03f5\n\r\r\r\r\r\nX\ni\n\u03f5iXi\n\r\r\r\r\r\nq\nSq\n\uf8f9\n\uf8fb\n1/q\n\u2264CK\n\u221aq max\n\uf8ee\n\uf8ef\uf8f0\n\r\r\r\r\r\r\n X\ni\nX\u2217\ni Xi\n!1/2\r\r\r\r\r\r\nSq\n,\n\r\r\r\r\r\r\n X\ni\nXiX\u2217\ni\n!1/2\r\r\r\r\r\r\nSq\n\uf8f9\n\uf8fa\uf8fb,\nwhere CK = 2\u22121/4p\n\u03c0/e.\nFor reference, if X is an n \u00d7 n matrix and q \u2265log n, we have\n\u2225X\u2225\u2264\u2225X\u2225Sq \u2264e\u2225X\u2225,\nso that the Schatten q-norm is within a multiplicative constant from the operator norm. Observe\nnow that with q\u2032 \u2265q\n(E\u03b4 E\u03f5 \u2225S\u03f5\u2225q)1/q \u2264\n\u0010\nE\u03b4 E\u03f5 \u2225S\u03f5\u2225q\nSq\u2032\n\u00111/q\n\u2264\n\u0010\nE\u03b4 E\u03f5 \u2225S\u03f5\u2225q\u2032\nSq\u2032\n\u00111/q\u2032\n.\nWe apply the noncommutative Khintchine inequality with q\u2032 \u2265log n, and after a little algebra,\nobtain\n\u0010\nE\u03b4 E\u03f5 \u2225S\u03f5\u2225q\u2032\nSq\u2032\n\u00111/q\u2032\n\u2264CK\ne \u221aq\u2032\np\n \nE\u03b4 max\n\"\n\u2225\nX\nab\n\u03b4abE2\nabeae\u2217\na\u2225q\u2032/2, \u2225\nX\nab\n\u03b4abE2\nabebe\u2217\nb\u2225q\u2032/2\n#!1/q\u2032\n.\nThe two terms in the right-hand side are essentially the same and if we can bound any one of them,\nthe same technique will apply to the other. We consider the \ufb01rst and since P\nab \u03b4abE2\nabeae\u2217\na is a\ndiagonal matrix,\n\u2225\nX\nab\n\u03b4abE2\nabeae\u2217\na\u2225= max\na\nX\nb\n\u03b4abE2\nab.\nThe following lemma bounds the qth moment of this quantity.\nLemma 6.2 Suppose that q is an integer obeying 1 \u2264q \u2264np and assume np \u22652 log n. Then\nE\u03b4\n \nmax\na\nX\nb\n\u03b4abE2\nab\n!q\n\u22642\n\u00002np \u2225E\u22252\n\u221e\n\u0001q .\n(6.6)\n25\nThe proof of this lemma is in the Appendix. The same estimate applies to E\n\u0000maxb\nP\na \u03b4abE2\nab\n\u0001q\nand thus for each q \u22651\nE\u03b4 max\n\"\n\u2225\nX\nab\n\u03b4abE2\nabeae\u2217\na\u2225q, \u2225\nX\nab\n\u03b4abE2\nabebe\u2217\nb\u2225q\n#\n\u22644\n\u00002np \u2225E\u22252\n\u221e\n\u0001q .\n(In the rectangular case, the same estimate holds with n = max(n1, n2).)\nTake q = \u03b2 log n for some \u03b2 \u22651, and set q\u2032 = q. Then since \u2225E\u2225\u221e\u2264\u00b51\n\u221ar/n, we established\nthat\n(E \u2225S\u2225q)1/q \u2264C 1\np\np\n\u03b2 log n \u221anp \u2225E\u2225\u221e= C \u00b51\nr\nnr \u03b2 log n\nm\n\u2261K0.\nThen by Markov\u2019s inequality, for each t > 0,\nP(\u2225S\u2225> tK0) \u2264t\u2212q,\nand for t = e, we conclude that\nP\n \n\u2225S\u2225> Ce \u00b51\nr\nnr \u03b2 log n\nm\n!\n\u2264n\u2212\u03b2\nwith the proviso that m \u2265max(\u03b2, 2) n log n so that Lemma 6.2 holds.\nWe have not made any assumption in this section about the matrix E (except that we have a\nbound on the maximum entry) and, therefore, have proved the theorem below, which shall be used\nmany times in the sequel.\nTheorem 6.3 Let X be a \ufb01xed n \u00d7 n matrix. There is a constant C0 such that for each \u03b2 > 2\np\u22121\u2225(P\u2126\u2212pI)(X)\u2225\u2264C0\n\u0012\u03b2n log n\np\n\u00131/2\n\u2225X\u2225\u221e\n(6.7)\nwith probability at least 1 \u2212n\u2212\u03b2 provided that np \u2265\u03b2 log n.\nNote that this is the same C0 described in Lemma 4.4.\n6.2\nProof of Lemma 4.5\nWe now need to bound the spectral norm of PT \u22a5P\u2126PT H(E) and will use some of the ideas\ndeveloped in the previous section. Just as before,\np\u22121\u2225PT \u22a5P\u2126PT H(E)\u2225\u2264p\u22121\u2225(P\u2126\u2212pI) H(E)\u2225,\nand put\nS \u2261p\u22121(P\u2126\u2212pI) H(E) = p\u22122 X\nab,a\u2032b\u2032\n\u03beab\u03bea\u2032b\u2032 Ea\u2032b\u2032\u27e8PT ea\u2032e\u2217\nb\u2032, eae\u2217\nb\u27e9eae\u2217\nb,\nwhere here and below, \u03beab \u2261\u03b4ab \u2212p. Decompose S as\nS\n= p\u22122\nX\n(a,b)=(a\u2032,b\u2032)\n+ p\u22122\nX\n(a,b) \u0338= (a\u2032,b\u2032)\n\u2261S0 + S1.\n(6.8)\n26\nWe bound the spectral norm of the diagonal and o\ufb00-diagonal contributions separately.\nWe begin with S0 and decompose (\u03beab)2 as\n\u03be2\nab = (\u03b4ab \u2212p)2 = (1 \u22122p)(\u03b4ab \u2212p) + p(1 \u2212p) = (1 \u22122p)\u03beab + p(1 \u2212p),\nwhich allows us to express S0 as\nS0 = 1 \u22122p\np\nX\nab\n\u03beab Habeae\u2217\nb + (1 \u2212p)\nX\nab\nHabeae\u2217\nb,\nHab \u2261p\u22121 Eab\u27e8PT eae\u2217\nb, eae\u2217\nb\u27e9.\n(6.9)\nTheorem 6.3 bounds the spectral norm of the \ufb01rst term of the right-hand side and we have\np\u22121\u2225\nX\nab\n\u03beab Habeae\u2217\nb\u2225\u2264C0\nr\nn3\u03b2 log n\nm\n\u2225H\u2225\u221e\nwith probability at least 1 \u2212n\u2212\u03b2. Now since \u2225E\u2225\u221e\u2264\u00b51\n\u221ar/n and |\u27e8PT eae\u2217\nb, eae\u2217\nb\u27e9| \u22642\u00b50r/n by\n(6.2), \u2225H\u2225\u221e\u2264\u00b50\u00b51(2r/np) \u221ar/n, and\np\u22121\u2225\nX\nab\n\u03beab Habeae\u2217\nb\u2225\u2264C\u00b50\u00b51\nnr\nm\nr\nnr\u03b2 log n\nm\nwith the same probability. The second term of the right-hand side in (6.9) is deterministic and we\ndevelop an argument that we will reuse several times. We record a useful lemma.\nLemma 6.4 Let X be a \ufb01xed matrix and set Z \u2261P\nab Xab\u27e8PT (eae\u2217\nb), eae\u2217\nb\u27e9eae\u2217\nb. Then\n\u2225Z\u2225\u22642\u00b50r\nn\n\u2225X\u2225.\nProof Let \u039bU and \u039bV be the diagonal matrices with entries \u2225PUea\u22252 and \u2225PV eb\u22252 respectively,\n\u039bU = diag(\u2225PUea\u22252),\n\u039bV = diag(\u2225PV eb\u22252).\n(6.10)\nTo bound the spectral norm of Z, observe that it follows from (4.7) that\nZ = \u039bUX + X\u039bV \u2212\u039bUX\u039bV = \u039bUX(I \u2212\u039bV ) + X\u039bV .\n(6.11)\nHence, since \u2225\u039bU\u2225and \u2225\u039bV \u2225are bounded by min(\u00b50r/n, 1) and \u2225I \u2212\u039bV \u2225\u22641, we have\n\u2225Z\u2225\u2264\u2225\u039bU\u2225\u2225X\u2225\u2225I \u2212\u039bV \u2225+ \u2225X\u2225\u2225\u039bV \u2225\u2264(2\u00b50r/n)\u2225X\u2225.\nClearly, this lemma and \u2225E\u2225= 1 give that H de\ufb01ned in (6.9) obeys \u2225H\u2225\u22642\u00b50r/np. In summary,\n\u2225S0\u2225\u2264C nr\nm\n \n\u00b50\u00b51\nr\n\u03b2nr log n\nm\n+ \u00b50\n!\nfor some C > 0 with the same probability as in Lemma 4.4.\nIt remains to bound the o\ufb00-diagonal term. To this end, we use a useful decoupling lemma:\n27\nLemma 6.5\n[16] Let {\u03b7i}1\u2264i\u2264n be a sequence of independent random variables, and {xij}i\u0338=j be\nelements taken from a Banach space. Then\nP(\u2225\nX\ni\u0338=j\n\u03b7i\u03b7jxij\u2225\u2265t) \u2264CD P(\u2225\nX\ni\u0338=j\n\u03b7i\u03b7\u2032\njxij\u2225> t/CD),\n(6.12)\nwhere {\u03b7\u2032\ni} is an independent copy of {\u03b7i}.\nThis lemma asserts that it is su\ufb03cient to estimate P(\u2225S\u2032\n1\u2225\u2265t) where S\u2032\n1 is given by\nS\u2032\n1 \u2261p\u22122 X\nab\u0338=a\u2032b\u2032\n\u03beab\u03be\u2032\na\u2032b\u2032 Ea\u2032b\u2032\u27e8PT ea\u2032e\u2217\nb\u2032, eae\u2217\nb\u27e9eae\u2217\nb\n(6.13)\nin which {\u03be\u2032\nab} is an independent copy of {\u03beab}. We write S\u2032\n1 as\nS\u2032\n1 = p\u22121 X\nab\n\u03beab Habeae\u2217\nb,\nHab \u2261p\u22121\nX\na\u2032b\u2032:(a\u2032,b\u2032)\u0338=(a,b)\n\u03be\u2032\na\u2032b\u2032 Ea\u2032b\u2032\u27e8PT ea\u2032e\u2217\nb\u2032, eae\u2217\nb\u27e9.\n(6.14)\nTo bound the tail of \u2225S\u2032\n1\u2225, observe that\nP(\u2225S\u2032\n1\u2225\u2265t) \u2264P(\u2225S\u2032\n1\u2225\u2265t | \u2225H\u2225\u221e\u2264K) + P(\u2225H\u2225\u221e> K).\nBy independence, the \ufb01rst term of the right-hand side is bounded by Theorem 6.3. On the event\n{\u2225H\u2225\u221e\u2264K}, we have\np\u22121\u2225\nX\nab\n\u03beab Habeae\u2217\nb\u2225\u2264C\nr\nn3\u03b2 log n\nm\nK.\nwith probability at least 1 \u2212n\u2212\u03b2. To bound \u2225H\u2225\u221e, we use Bernstein\u2019s inequality.\nLemma 6.6 Let X be a \ufb01xed matrix and de\ufb01ne Q(X) as the matrix whose (a, b)th entry is\n[Q(X)]ab = p\u22121\nX\na\u2032b\u2032:(a\u2032,b\u2032)\u0338=(a,b)\n(\u03b4a\u2032b\u2032 \u2212p) Xa\u2032b\u2032\u27e8PT ea\u2032e\u2217\nb\u2032, eae\u2217\nb\u27e9,\nwhere {\u03b4ab} is an independent Bernoulli sequence obeying P(\u03b4ab = 1) = p. Then\nP\n\u0012\n\u2225Q(X)\u2225\u221e> \u03bb\nr\u00b50r\nnp \u2225X\u2225\u221e\n\u0013\n\u22642n2 exp\n\uf8eb\n\uf8ed\u2212\n\u03bb2\n2 + 2\n3\nq\n\u00b50r\nnp \u03bb\n\uf8f6\n\uf8f8.\n(6.15)\nWith \u03bb = \u221a3\u03b2 log n, the right-hand side is bounded by 2n2\u2212\u03b2 provided that np \u22654\u03b2\n3 \u00b50r log n. In\nparticular, for \u03bb = \u221a6\u03b2 log n with \u03b2 > 2, the bound is less than 2n\u2212\u03b2 provided that np \u22658\u03b2\n3 \u00b50r log n.\nProof\nThe inequality (6.15) is an application of Bernstein\u2019s inequality, which states that for a\nsum of uniformly bounded independent zero-mean random variables obeying |Yk| \u2264c,\nP\n \n|\nn\nX\nk=1\nYk| > t\n!\n\u22642e\u2212t2/(2\u03c32+2ct/3),\n(6.16)\n28\nwhere \u03c32 is the sum of the variances, \u03c32 \u2261Pn\nk=1 Var(Yk). We have\nVar([Q(X)]ab) = 1 \u2212p\np\nX\na\u2032b\u2032:(a\u2032,b\u2032)\u0338=(a,b)\n|Xa\u2032b\u2032|2|\u27e8PT ea\u2032e\u2217\nb\u2032, eae\u2217\nb\u27e9|2\n\u22641 \u2212p\np\n\u2225X\u22252\n\u221e\nX\na\u2032b\u2032:(a\u2032,b\u2032)\u0338=(a,b)\n|\u27e8PT eae\u2217\nb, ea\u2032e\u2217\nb\u2032\u27e9|2 \u22641 \u2212p\np\n\u2225X\u22252\n\u221e2\u00b50r/n\nby (6.3). Also,\np\u22121 |(\u03b4a\u2032b\u2032 \u2212p)Xa\u2032b\u2032\u27e8PT ea\u2032e\u2217\nb\u2032, eae\u2217\nb\u27e9| \u2264p\u22121 \u2225X\u2225\u221e2\u00b50r/n\nand hence, for each t > 0, (6.16) gives\nP(|[Q(X)]ab| > t) \u22642 exp\n \n\u2212\nt2\n2\u00b50r\nnp \u2225X\u22252\u221e+ 2\n3\n\u00b50r\nnp \u2225X\u2225\u221et\n!\n.\n(6.17)\nPutting t = \u03bb\np\n\u00b50r/np\u2225X\u2225\u221efor some \u03bb > 0 and applying the union bound gives (6.15).\nSince \u2225E\u2225\u221e\u2264\u00b51\n\u221ar/n it follows that H = Q(E) introduced in (6.14) obeys\n\u2225H\u2225\u221e\u2264C \u00b51\n\u221ar\nn\nr\n\u00b50nr\u03b2 log n\nm\nwith probability at least 1 \u22122n\u2212\u03b2 for each \u03b2 > 2 and, therefore,\n\u2225S\u2032\n1\u2225\u2264C \u221a\u00b50\u00b51\nnr\u03b2 log n\nm\nwith probability at least 1 \u22123n\u2212\u03b2. In conclusion, we have\np\u22121\u2225(P\u2126\u2212pI) H(E)\u2225\u2264C nr\nm\n \n\u221a\u00b50\u00b51\n r\n\u00b50nr\u03b2 log n\nm\n+ \u03b2 log n\n!\n+ \u00b50\n!\n(6.18)\nwith probability at least 1 \u2212(1 + 3CD)n\u2212\u03b2. A simple algebraic manipulation concludes the proof\nof Lemma 4.5. Note that we have not made any assumption about the matrix E and, therefore,\nestablished the following:\nLemma 6.7 Let X be a \ufb01xed n \u00d7 n matrix. There is a constant C\u2032\n0 such that\np\u22122\u2225\nX\n(a,b)\u0338=(a\u2032,b\u2032)\n\u03beab\u03bea\u2032b\u2032Xab\u27e8PT (ea\u2032e\u2217\nb\u2032), eae\u2217\nb\u27e9eae\u2217\nb\u2225\u2264C\u2032\n0\n\u221a\u00b50r \u03b2 log n\np\n\u2225X\u2225\u221e\n(6.19)\nwith probability at least 1 \u2212O(n\u2212\u03b2) for all \u03b2 > 2 provided that np \u22653\u00b50r\u03b2 log n.\n29\n6.3\nProof of Lemma 4.6\nTo prove Lemma 4.6, we need to bound the spectral norm of p\u22121 (P\u2126\u2212pI) H2(E), a matrix given\nby\np\u22123\nX\na1b1,a2b2,a3b3\n\u03bea1b1\u03bea2b2\u03bea3b3Ea3b3\u27e8PT ea3e\u2217\nb3, ea2e\u2217\nb2\u27e9\u27e8PT ea2e\u2217\nb2, ea1e\u2217\nb1\u27e9ea1e\u2217\nb1,\nwhere \u03beab = \u03b4ab \u2212p as before. It is convenient to introduce notations to compress this expression.\nSet \u03c9 = (a, b) (and \u03c9i = (ai, bi) for i = 1, 2, 3), F\u03c9 = eae\u2217\nb, and P\u03c9\u2032\u03c9 = \u27e8PT ea\u2032e\u2217\nb\u2032, eae\u2217\nb\u27e9so that\np\u22121 (P\u2126\u2212pI) H2(E) = p\u22123\nX\n\u03c91,\u03c92,\u03c93\n\u03be\u03c91\u03be\u03c92\u03be\u03c93 E\u03c93P\u03c93\u03c92P\u03c92\u03c91F\u03c91.\nPartition the sum depending on whether some of the \u03c9i\u2019s are the same or not\n1\np(P\u2126\u2212pI)H2(E) = 1\np3\n\uf8ee\n\uf8f0\nX\n\u03c91=\u03c92=\u03c93\n+\nX\n\u03c91\u0338=\u03c92=\u03c93\n+\nX\n\u03c91=\u03c93\u0338=\u03c92\n+\nX\n\u03c91=\u03c92\u0338=\u03c93\n+\nX\n\u03c91\u0338=\u03c92\u0338=\u03c93\n\uf8f9\n\uf8fb.\n(6.20)\nThe meaning should be clear; for instance, the sum P\n\u03c91\u0338=\u03c92=\u03c93 is the sum over the \u03c9\u2019s such that\n\u03c92 = \u03c93 and \u03c91 \u0338= \u03c92. Similarly, P\n\u03c91\u0338=\u03c92\u0338=\u03c93 is the sum over the \u03c9\u2019s such that they are all distinct.\nThe idea is now to use a decoupling argument to bound each sum in the right-hand side of (6.20)\n(except for the \ufb01rst which does not need to be decoupled) and show that all terms are appropriately\nsmall in the spectral norm.\nWe begin with the \ufb01rst term which is equal to\n1\np3\nX\n\u03c9\n(\u03be\u03c9)3 E\u03c9P 2\n\u03c9\u03c9F\u03c9 = 1 \u22123p + 3p2\np3\nX\n\u03c9\n\u03be\u03c9 E\u03c9P 2\n\u03c9\u03c9F\u03c9 + 1 \u22123p + 2p2\np2\nX\n\u03c9\nE\u03c9P 2\n\u03c9\u03c9F\u03c9,\n(6.21)\nwhere we have used the identity\n(\u03be\u03c9)3 = (1 \u22123p + 3p2)\u03be\u03c9 + p(1 \u22123p + 2p2).\nSet H\u03c9 = E\u03c9(p\u22121P\u03c9\u03c9)2. For the \ufb01rst term in the right-hand side of (6.21), we need to control\n\u2225P\n\u03c9 \u03be\u03c9 H\u03c9F\u03c9\u2225. This is easily bounded by Theorem 6.3. Indeed, it follows from\n|H\u03c9| \u2264\n\u00122\u00b50r\nnp\n\u00132\n\u2225E\u2225\u221e\nthat for each \u03b2 > 0,\np\u22121\u2225\nX\n\u03c9\n\u03be\u03c9 H\u03c9 F\u03c9\u2225\u2264C\n\u0010\u00b50nr\nm\n\u00112\n\u00b51\nr\nnr\u03b2 log n\nm\n= C \u00b52\n0\u00b51\np\n\u03b2 log n\n\u0010nr\nm\n\u00115/2\nwith probably at least 1 \u2212n\u2212\u03b2. For the second term in the right-hand side of (6.21), we apply\nLemma 6.4 which gives\n\u2225\nX\n\u03c9\nE\u03c9P 2\n\u03c9\u03c9F\u03c9\u2225\u2264(2\u00b50r/n)2\n30\nso that \u2225H\u2225\u2264(2\u00b50r/np)2. In conclusion, the \ufb01rst term in (6.20) has a spectral norm which is\nbounded by\nC\n\u0010nr\nm\n\u00112\n \n\u00b52\n0\u00b51\n\u0012nr\u03b2 log n\nm\n\u00131/2\n+ \u00b52\n0\n!\nwith probability at least 1 \u2212n\u2212\u03b2.\nWe now turn our attention to the second term which can be written as\np\u22123 X\n\u03c91\u0338=\u03c92\n\u03be\u03c91(\u03be\u03c92)2 E\u03c92P\u03c92\u03c92P\u03c92\u03c91F\u03c91 = 1 \u22122p\np3\nX\n\u03c91\u0338=\u03c92\n\u03be\u03c91\u03be\u03c92 E\u03c92P\u03c92\u03c92P\u03c92\u03c91F\u03c91\n+ 1 \u2212p\np2\nX\n\u03c91\u0338=\u03c92\n\u03be\u03c91 E\u03c92P\u03c92\u03c92P\u03c92\u03c91F\u03c91.\nPut S1 for the \ufb01rst term; bounding \u2225S1\u2225is a simple application of Lemma 6.7 with X\u03c9 = p\u22121E\u03c9P\u03c9\u03c9,\nwhich gives\n\u2225S1\u2225\u2264C \u00b53/2\n0\n\u00b51 (\u03b2 log n)\n\u0010nr\nm\n\u00112\nsince \u2225E\u2225\u221e\u2264\u00b51\n\u221ar/n. For the second term, we need to bound the spectral norm of S2 where\nS2 \u2261p\u22121 X\n\u03c91\n\u03be\u03c91H\u03c91F\u03c91,\nH\u03c91 = p\u22121\nX\n\u03c92:\u03c92\u0338=\u03c91\nE\u03c92P\u03c92\u03c92P\u03c92\u03c91.\nNote that H is deterministic. The lemma below provides an estimate about \u2225H\u2225\u221e.\nLemma 6.8 The matrix H obeys\n\u2225H\u2225\u221e\u2264\u00b50r\nnp\n\u0010\n3\u2225E\u2225\u221e+ 2\u00b50r\nn\n\u0011\n.\n(6.22)\nProof We begin by rewriting H as\npH\u03c9 =\nX\n\u03c9\u2032\nE\u03c9\u2032P\u03c9\u2032\u03c9\u2032P\u03c9\u2032\u03c9 \u2212E\u03c9P 2\n\u03c9\u03c9.\nClearly, |E\u03c9P 2\n\u03c9\u03c9| \u2264(\u00b50r/n)2\u2225E\u2225\u221eso that it su\ufb03ces to bound the \ufb01rst term, which is the \u03c9th entry\nof the matrix\nX\n\u03c9,\u03c9\u2032\nE\u03c9\u2032P\u03c9\u2032\u03c9\u2032P\u03c9\u2032\u03c9F\u03c9 = PT (\u039bUE + E\u039bV \u2212\u039bUE\u039bV ).\nNow it is immediate to see that \u039bUE \u2208T and likewise for E\u039bV . Hence,\n\u2225PT (\u039bUE + E\u039bV \u2212\u039bUE\u039bV )\u2225\u221e\u2264\u2225\u039bUE\u2225\u221e+ \u2225E\u039bV \u2225\u221e+ \u2225PT (\u039bUE\u039bV )\u2225\u221e\n\u22642\u2225E\u2225\u221e\u00b50r/n + \u2225PT (\u039bUE\u039bV )\u2225\u221e.\nWe \ufb01nally use the crude estimate\n\u2225PT (\u039bUE\u039bV )\u2225\u221e\u2264\u2225PT (\u039bUE\u039bV )\u2225\u22642\u2225\u039bUE\u039bV \u2225\u22642(\u00b50r/n)2\nto complete the proof of the lemma.\n31\nAs a consequence of this lemma, Theorem 6.3 gives\n\u2225S2\u2225\u2264C\np\n\u03b2 log n\n\u0010nr\nm\n\u00113/2\n(\u00b50\u00b51 + \u00b52\n0\n\u221ar)\nwith probability at least 1 \u2212n\u2212\u03b2.\nIn conclusion, the second term in (6.20) has spectral norm\nbounded by\nC\np\n\u03b2 log n\n\u0010nr\nm\n\u00113/2\n \n\u00b50\u00b51\nr\n\u00b50nr\u03b2 log n\nm\n+ \u00b50\u00b51 + \u00b52\n0\n\u221ar\n!\nwith probability at least 1 \u2212O(n\u2212\u03b2).\nWe now examine the third term which can be written as\np\u22123 X\n\u03c91\u0338=\u03c92\n(\u03be\u03c91)2\u03be\u03c92 E\u03c91P\u03c91\u03c92P\u03c92\u03c91F\u03c91 = 1 \u22122p\np3\nX\n\u03c91\u0338=\u03c92\n\u03be\u03c91\u03be\u03c92 E\u03c91P 2\n\u03c92\u03c91F\u03c91\n+ 1 \u2212p\np2\nX\n\u03c91\u0338=\u03c92\n\u03be\u03c92 E\u03c91P 2\n\u03c92\u03c91F\u03c91.\nWe use the decoupling argument once more so that for the \ufb01rst term of the right-hand side, it\nsu\ufb03ces to estimate the tail of the norm of\nS1 \u2261p\u22121 X\n\u03c91\n\u03be(1)\n\u03c91 E\u03c91H\u03c91F\u03c91,\nH\u03c91 \u2261p\u22122\nX\n\u03c92:\u03c92\u0338=\u03c91\n\u03be(2)\n\u03c92 P 2\n\u03c92\u03c91,\nwhere {\u03be(1)\n\u03c9 } and {\u03be(2)\n\u03c9 } are independent copies of {\u03be\u03c9}. It follows from Bernstein\u2019s inequality and\nthe estimates\n|P\u03c92\u03c91| \u22642\u00b50r/n\nand\nX\n\u03c92:\u03c92\u0338=\u03c91\n|P\u03c92\u03c91|4 \u2264\nmax\n\u03c92:\u03c92\u0338=\u03c91 |P\u03c92\u03c91|2\nX\n\u03c92:\u03c92\u0338=\u03c91\n|P\u03c92\u03c91|2 \u2264\n\u00122\u00b50r\nn\n\u00132 2\u00b50r\nn\nthat for each \u03bb > 0,4\nP\n \n|H\u03c91| > \u03bb\n\u00122\u00b50r\nnp\n\u00133/2!\n\u22642 exp\n\uf8eb\n\uf8ec\n\uf8ed\u2212\n\u03bb2\n2 + 2\n3\u03bb\n\u0010\n2\u00b50r\nnp\n\u00111/2\n\uf8f6\n\uf8f7\n\uf8f8.\nIt is now not hard to see that this inequality implies that\nP\n \n\u2225H\u2225\u221e>\np\n8\u03b2 log n\n\u00122\u00b50nr\nm\n\u00133/2!\n\u22642 n\u22122\u03b2+2\nprovided that m \u226516\n9 \u00b50nr \u03b2 log n. As a consequence, for each \u03b2 > 2, Theorem 6.3 gives\n\u2225S1\u2225\u2264C \u00b53/2\n0\n\u00b51 \u03b2 log n\n\u0010nr\nm\n\u00112\n4We would like to remark that one can often get better estimates; when \u03c91 \u0338= \u03c92, the bound |P\u03c92\u03c91| \u22642\u00b50r/n\nmay be rather crude. Indeed, one can derive better estimates for the random orthogonal model, for example.\n32\nwith probability at least 1 \u22123n\u2212\u03b2. The other term is equal to (1 \u2212p) times P\n\u03c91 E\u03c91H\u03c91F\u03c91, and\n\u2225\nX\n\u03c91\nE\u03c91H\u03c91F\u03c91\u2225\u2264\u2225\nX\n\u03c91\nE\u03c91H\u03c91F\u03c91\u2225F\n\u2264\u2225H\u2225\u221e\u2225E\u2225F \u2264C\np\n\u03b2 log n\n\u0010\u00b50nr\nm\n\u00113/2 \u221ar.\nIn conclusion, the third term in (6.20) has spectral norm bounded by\nC \u00b50\np\n\u03b2 log n\n\u0010nr\nm\n\u00113/2\n \n\u00b51\nr\n\u00b50nr\u03b2 log n\nm\n+ \u221a\u00b50r\n!\nwith probability at least 1 \u2212O(n\u2212\u03b2).\nWe proceed to the fourth term which can be written as\np\u22123 X\n\u03c91\u0338=\u03c93\n(\u03be\u03c91)2\u03be\u03c93 E\u03c93P\u03c93\u03c91P\u03c91\u03c91F\u03c91 = 1 \u22122p\np3\nX\n\u03c91\u0338=\u03c93\n\u03be\u03c91\u03be\u03c93 E\u03c93P\u03c93\u03c91P\u03c91\u03c91F\u03c91\n+ 1 \u2212p\np2\nX\n\u03c91\u0338=\u03c93\n\u03be\u03c93 E\u03c93P\u03c93\u03c91P\u03c91\u03c91F\u03c91.\nLet S1 be the \ufb01rst term and set H\u03c91 = p\u22122 P\n\u03c91\u0338=\u03c93 \u03be\u03c91\u03be\u03c93 E\u03c93P\u03c93\u03c91F\u03c91. Then Lemma 6.4 gives\n\u2225S1\u2225\u22642\u00b50r\nnp \u2225H\u2225\u2264C \u00b53/2\n0\n\u00b51 (\u03b2 log n)\n\u0010nr\nm\n\u00112\nwhere the last inequality is given by Lemma 6.7.\nFor the other term\u2014call it S2\u2014set H\u03c91 =\np\u22121 P\n\u03c93:\u03c93\u0338=\u03c91 \u03be\u03c93 E\u03c93P\u03c93\u03c91. Then Lemma 6.4 gives\n\u2225S2\u2225\u22642\u00b50r\nnp \u2225H\u2225.\nNotice that H\u03c91 = p\u22121 P\n\u03c93 \u03be\u03c93 E\u03c93P\u03c93\u03c91 \u2212p\u22121\u03be\u03c91E\u03c91P\u03c91\u03c91 so that with G\u03c91 = E\u03c91P\u03c91\u03c91\nH = p\u22121[PT (P\u2126\u2212pI)(E) \u2212(P\u2126\u2212pI)(G)].\nNow for any matrix X, \u2225PT (X)\u2225= \u2225X \u2212PT \u22a5(X)\u2225\u22642\u2225X\u2225and, therefore,\n\u2225H\u2225\u22642p\u22121\u2225(P\u2126\u2212pI)(E)\u2225+ p\u22121\u2225(P\u2126\u2212pI)(G)\u2225.\nAs a consequence and since \u2225G\u2225\u221e\u2264\u2225E\u2225\u221e, Theorem 6.3 gives for each \u03b2 > 2,\n\u2225H\u2225\u2264C\u00b51\nr\nnr\u03b2 log n\nm\nwith probability at least 1 \u2212n\u2212\u03b2.\nIn conclusion, the fourth term in (6.20) has spectral norm\nbounded by\nC \u00b50\u00b51\np\n\u03b2 log n\n\u0010nr\nm\n\u00113/2\n r\n\u00b50nr\u03b2 log n\nm\n+ 1\n!\n33\nwith probability at least 1 \u2212O(n\u2212\u03b2).\nWe \ufb01nally examine the last term\np\u22123\nX\n\u03c91\u0338=\u03c92\u0338=\u03c93\n\u03be\u03c91\u03be\u03c92\u03be\u03c93 E\u03c93P\u03c93\u03c92P\u03c92\u03c91F\u03c91.\nNow just as one has a decoupling inequality for pairs of variables, we have a decoupling inequality\nfor triples as well and we thus simply need to bound the tail of\nS1 \u2261p\u22123\nX\n\u03c91\u0338=\u03c92\u0338=\u03c93\n\u03be(1)\n\u03c91 \u03be(2)\n\u03c92 \u03be(3)\n\u03c93 E\u03c93P\u03c93\u03c92P\u03c92\u03c91F\u03c91\nin which the sequences {\u03be(1)\n\u03c9 }, {\u03be(2)\n\u03c9 } and {\u03be(3)\n\u03c9 } are independent copies of {\u03be\u03c9}. We refer to [16]\nfor details. We now argue as in Section 6.2 and write S1 as\nS1 = p\u22121 X\n\u03c91\n\u03be(1)\n\u03c91 H\u03c91F\u03c91,\nwhere\nH\u03c91 \u2261p\u22121\nX\n\u03c92:\u03c92\u0338=\u03c91\n\u03be(2)\n\u03c92 G\u03c92 P\u03c92\u03c91,\nG\u03c92 \u2261p\u22121\nX\n\u03c93:\u03c93\u0338=\u03c91,\u03c93\u0338=\u03c92\n\u03be(3)\n\u03c93 E\u03c93 P\u03c93\u03c92.\n(6.23)\nBy Lemma 6.6, we have for each \u03b2 > 2\n\u2225G\u2225\u221e\u2264C\nr\n\u00b50nr\u03b2 log n\nm\n\u2225E\u2225\u221e\nwith large probability and the same argument then gives\n\u2225H\u2225\u221e\u2264C\nr\n\u00b50nr\u03b2 log n\nm\n\u2225G\u2225\u221e\u2264C \u00b50nr\u03b2 log n\nm\n\u2225E\u2225\u221e\nwith probability at least 1 \u22124n\u2212\u03b2. As a consequence, Theorem 6.3 gives\n\u2225S\u2225\u2264C \u00b50\u00b51\n\u0012nr\u03b2 log n\nm\n\u00133/2\nwith probability at least 1 \u2212O(n\u2212\u03b2).\nTo summarize the calculations of this section and using the fact that \u00b50 \u22651 and \u00b51 \u2264\u00b50\n\u221ar,\nwe have established that if m \u2265\u00b50 nr(\u03b2 log n),\np\u22121\u2225(P\u2126\u2212pI) H2(E)\u2225\u2264C\n\u0010nr\nm\n\u00112\n \n\u00b52\n0\u00b51\nr\nnr\u03b2 log n\nm\n+ \u00b52\n0\n!\n+ C\np\n\u03b2 log n\n\u0010nr\nm\n\u00113/2\n\u00b52\n0\n\u221ar + C\n\u0012nr\u03b2 log n\nm\n\u00133/2\n\u00b50\u00b51\nwith probability at least 1\u2212O(n\u2212\u03b2). One can check that if m = \u03bb \u00b54/3\n0\nnr4/3\u03b2 log n for a \ufb01xed \u03b2 \u22652\nand \u03bb \u22651, then there is a constant C such that\n\u2225p\u22121 (P\u2126\u2212pI) H2(E)\u2225\u2264C\u03bb\u22123/2\nwith probability at least 1 \u2212O(n\u2212\u03b2). This is the content of Lemma 4.6.\n34\n6.4\nProof of Lemma 4.7\nClearly, one could continue on the same path and estimate the spectral norm of p\u22121(P\u2126\u2212pI) H3(E)\nby the same technique as in the previous sections. That is to say, we would write\np\u22121(P\u2126\u2212pI) H3(E) = p\u22124\nX\n\u03c91,\u03c92,\u03c93,\u03c94\n\" 4\nY\ni=1\n\u03be\u03c9i\n#\nE\u03c94\n\" 3\nY\ni=1\nP\u03c9i+1\u03c9i\n#\nF\u03c91\nwith the same notations as before, and partition the sum depending on whether some of the \u03c9i\u2019s\nare the same or not. Then we would use the decoupling argument to bound each term in the sum.\nAlthough this is a clear possibility, one would need to consider 18 cases and the calculations would\nbecome a little laborious. In this section, we propose to bound the term p\u22121(P\u2126\u2212pI) H3(E) with\na di\ufb00erent argument which has two main advantages: \ufb01rst, it is much shorter and second, it uses\nmuch of what we have already established. The downside is that it is not as sharp.\nThe starting point is to note that\np\u22121(P\u2126\u2212pI) H3(E) = p\u22121(\u039e \u25e6H3(E)),\nwhere \u039e is the matrix with i.i.d. entries equal to \u03beab = \u03b4ab \u2212p and \u25e6denotes the Hadamard product\n(componentwise multiplication). To bound the spectral norm of this Hadamard product, we apply\nan inequality due to Ando, Horn, and Johnson [4]. An elementary proof can be found in \u00a75.6 of\n[19].\nLemma 6.9\n[19] Let A and B be two n1 \u00d7 n2 matrices. Then\n\u2225A \u25e6B\u2225\u2264\u2225A\u2225\u03bd(B),\n(6.24)\nwhere \u03bd is the function\n\u03bd(B) = inf{c(X)c(Y ) : XY \u2217= B},\nand c(X) is the maximum Euclidean norm of the rows\nc(X)2 = max\n1\u2264i\u2264n\nX\nj\nX2\nij.\nTo apply (6.24), we \ufb01rst notice that one can estimate the norm of \u039e via Theorem 6.3. Indeed,\nlet Z = 11\u2217be the matrix with all entries equal to one. Then p\u22121\u039e = p\u22121(P\u2126\u2212pI)(Z) and thus\np\u22121\u2225\u039e\u2225\u2264C\n\u0012n3\u03b2 log n\nm\n\u00131/2\n(6.25)\nwith probability at least 1 \u2212n\u2212\u03b2. One could obtain a similar result by appealing to the recent\nliterature on random matrix theory and on concentration of measure. Potentially this could allow\nto derive an upper bound without the logarithmic term but we will not consider these re\ufb01nements\nhere. (It is interesting to note in passing, however, that the two page proof of Theorem 6.3 gives a\nlarge deviation result about the largest singular value of a matrix with i.i.d. entries which is sharp\nup to a multiplicative factor proportional to at most \u221alog n.)\nSecond, we bound the second factor in (6.24) via the following estimate:\n35\nLemma 6.10 There are numerical constants C and c so that for each \u03b2 > 2, H3(E) obeys\n\u03bd(H3(E)) \u2264C\u00b50r/n\n(6.26)\nwith probability at least 1 \u2212O(n\u2212\u03b2) provided that m \u2265c \u00b54/3\n0\nnr5/3(\u03b2 log n).\nThe two inequalities (6.25) and (6.26) give\np\u22121\u2225\u039e \u25e6H3(E)\u2225\u2264C\nr\n\u00b52\n0 nr2 \u03b2 log n\nm\n,\nwith large probability. Hence, when m is substantially larger than a constant times \u00b52\n0nr2(\u03b2 log n),\nwe have that the spectral norm of p\u22121(P\u2126\u2212pI) H3(E) is much less than 1. This is the content of\nLemma 4.7.\nThe remainder of this section proves Lemma 6.10. Set S \u2261H3(E) for short. Because S is in\nT, S = PT (S) = PUS + SPV \u2212PUSPV . Writing PU = Pr\nj=1 uju\u2217\nj and similarly for PV gives\nS =\nr\nX\nj=1\nuj(u\u2217\njS) +\nr\nX\nj=1\n((I \u2212PU)Svj)v\u2217\nj .\nFor each 1 \u2264j \u2264r, let \u03b1j \u2261Svj and \u03b2\u2217\nj \u2261u\u2217\njS. Then the decomposition\nS =\nr\nX\nj=1\nuj\u03b2\u2217\nj +\nr\nX\nj=1\n(PU\u22a5\u03b1j)v\u2217\nj ,\nwhere PU\u22a5= I \u2212PU, provides a factorization of the form\nS = XY \u2217,\n(\nX = [u1, . . . , ur, PU\u22a5\u03b11, . . . , PU\u22a5\u03b1r],\nY = [v1, . . . , vr, \u03b21, . . . , \u03b2r].\nIt follows from our assumption that\nc2([u1, . . . , ur]) = max\n1\u2264i\u2264n\nX\n1\u2264j\u2264r\nu2\nij = max\n1\u2264i\u2264n \u2225PUei\u22252 \u2264\u00b50r/n,\nand similarly for [v1, . . . , vr]. Hence, to prove Lemma 6.10, it su\ufb03ces to prove that the maximum\nrow norm obeys c([\u03b21, . . . , \u03b2r]) \u2264C\np\n\u00b50r/n for some constant C > 0, and similarly for the matrix\n[PU\u22a5\u03b11, . . . , PU\u22a5\u03b1r].\nLemma 6.11 There is a numerical constant C such that for each \u03b2 > 2,\nc([\u03b11, . . . , \u03b1r]) \u2264C\np\n\u00b50r/n\n(6.27)\nwith probability at least 1 \u2212O(n\u2212\u03b2) provided that m obeys the condition of Lemma 6.10.\nA similar estimate for [\u03b21, . . . , \u03b2r] is obtained in the same way by exchanging the roles of u and v.\nMoreover, a minor modi\ufb01cation of the argument gives\nc([PU\u22a5\u03b11, . . . , PU\u22a5\u03b1r]) \u2264C\np\n\u00b50r/n\n(6.28)\n36\nas well, and we will omit the details. In short, the estimate (6.27) implies Lemma 6.10.\nProof [of Lemma 6.11] To prove (6.27), we use the notations of the previous section and write\n\u03b1j = p\u22123\nX\na1b1,a2b2,a3b3\n\u03bea1b1\u03bea2b2\u03bea3b3Ea3b3\u27e8PT ea3e\u2217\nb3, ea2e\u2217\nb2\u27e9\u27e8PT ea2e\u2217\nb2, ea1e\u2217\nb1\u27e9PT (ea1e\u2217\nb1)vj\n= p\u22123\nX\n\u03c91,\u03c92,\u03c93\n\u03be\u03c91\u03be\u03c92\u03be\u03c93 E\u03c93P\u03c93\u03c92P\u03c92\u03c91PT (F\u03c91)vj\n= p\u22123\nX\n\u03c91,\u03c92,\u03c93\n\u03be\u03c91\u03be\u03c92\u03be\u03c93 E\u03c93P\u03c93\u03c92P\u03c92\u03c91(F\u03c91vj)\nsince for any matrix X, PT (X)vj = Xvj for each 1 \u2264j \u2264r. We then follow the same steps as in\nSection 6.3 and partition the sum depending on whether some of the \u03c9i\u2019s are the same or not\n\u03b1j = p\u22123\n\uf8ee\n\uf8f0\nX\n\u03c91=\u03c92=\u03c93\n+\nX\n\u03c91\u0338=\u03c92=\u03c93\n+\nX\n\u03c91=\u03c93\u0338=\u03c92\n+\nX\n\u03c91=\u03c92\u0338=\u03c93\n+\nX\n\u03c91\u0338=\u03c92\u0338=\u03c93\n\uf8f9\n\uf8fb.\n(6.29)\nThe idea is this: to establish (6.27), it is su\ufb03cient to show that if \u03b3j is any of the \ufb01ve terms above,\nit obeys\ns X\n1\u2264j\u2264r\n|\u03b3ij|2 \u2264C\np\n\u00b50r/n\n(6.30)\n(\u03b3ij is the ith component of \u03b3j as usual) with large probability. The strategy for getting such\nestimates is to use decoupling whenever applicable.\nJust as Theorem 6.3 proved useful to bound the norm of p\u22121(P\u2126\u2212pI)H2(E) in Section 6.3,\nthe lemma below will help bounding the magnitudes of the components of \u03b1j.\nLemma 6.12 De\ufb01ne S \u2261p\u22121 P\nij\nP\n\u03c9 \u03be\u03c9H\u03c9\u27e8ei, F\u03c9vj\u27e9eie\u2217\nj. Then for each \u03bb > 0\nP(\u2225S\u2225\u221e\u2265\np\n\u00b50/n) \u22642n2 exp\n \n\u2212\n1\n2n\n\u00b50p\u2225H\u22252\u221e+ 2\n3p\n\u221ar\u2225H\u2225\u221e\n!\n.\n(6.31)\nProof The proof is an application of Bernstein\u2019s inequality (6.16). Note that \u27e8ei, F\u03c9vj\u27e9= 1{a=i}vbj\nand hence\nVar(Sij) \u2264p\u22121\u2225H\u22252\n\u221e\nX\n\u03c9\n|\u27e8ei, F\u03c9vj\u27e9|2 = p\u22121\u2225H\u22252\n\u221e\nsince P\n\u03c9 |\u27e8ei, F\u03c9vj\u27e9|2 = 1, and |p\u22121H\u03c9\u27e8ei, F\u03c9vj\u27e9| \u2264p\u22121 \u2225H\u2225\u221e\np\n\u00b50r/n since |\u27e8ei, F\u03c9vj\u27e9| \u2264|vbj|\nand\n|vbj| \u2264\u2225PV eb\u2225\u2264\np\n\u00b50r/n.\nEach term in (6.29) is given by the corresponding term in (6.20) after formally substituting F\u03c9\nwith F\u03c9vj. We begin with the \ufb01rst term whose ith component is equal to\n\u03b3ij \u2261p\u22123(1 \u22123p + 3p2)\nX\n\u03c9\n\u03be\u03c9 E\u03c9P 2\n\u03c9\u03c9\u27e8ei, F\u03c9vj\u27e9+ p\u22122(1 \u22123p + 2p2)\nX\n\u03c9\nE\u03c9P 2\n\u03c9\u03c9\u27e8ei, F\u03c9vj\u27e9.\n(6.32)\n37\nIgnoring the constant factor (1 \u22123p + 3p2) which is bounded by 1, we write the \ufb01rst of these two\nterms as\n(S0)ij \u2261p\u22121 X\n\u03c9\n\u03be\u03c9 H\u03c9\u27e8ei, F\u03c9vj\u27e9,\nH\u03c9 = E\u03c9 (p\u22121P\u03c9\u03c9)2.\nSince \u2225H\u2225\u221e\u2264(\u00b50nr/m)2 \u00b51\n\u221ar/n, it follows from Lemma (6.12) that\nP\n\u0010\n\u2225S0\u2225\u221e\u2265\np\n\u00b50/n\n\u0011\n\u22642n2 e\u22121/D,\nD \u2264C\n\u0012\n\u00b53\n0\u00b52\n1\n\u0010nr\nm\n\u00115\n+ \u00b52\n0\u00b51\n\u0010nr\nm\n\u00113\u0013\nfor some numerical C > 0. Since \u00b51 \u2264\u00b50\n\u221ar, we have that when m \u2265\u03bb\u00b50 nr6/5(\u03b2 log n) for some\nnumerical constant \u03bb > 0, \u2225S0\u2225\u221e\u2265\np\n\u00b50/n with probability at most 2n2e\u2212(\u03b2 log n)3; this probability\nis inversely proportional to a superpolynomial in n. For the second term, the matrix with entries\nE\u03c9P 2\n\u03c9\u03c9 is given by\n\u039b2\nUE + E\u039b2\nV + 2\u039bUE\u039bV + \u039b2\nUE\u039b2\nV \u22122\u039b2\nUE\u039bV \u22122\u039bUE\u039b2\nV\nand thus\nX\n\u03c9\nE\u03c9P 2\n\u03c9\u03c9\u27e8ei, F\u03c9vj\u27e9= \u27e8ei, (\u039b2\nUE + E\u039b2\nV + 2\u039bUE\u039bV + \u039b2\nUE\u039b2\nV \u22122\u039b2\nUE\u039bV \u22122\u039bUE\u039b2\nV )vj\u27e9.\nThis is a sum of six terms and we will show how to bound the \ufb01rst three; the last three are dealt\nin exactly the same way and obey better estimates. For the \ufb01rst, we have\n\u27e8ei, \u039b2\nUEvj\u27e9= \u27e8\u039b2\nUei, Evj\u27e9= \u2225PUei\u22254\u27e8ei, uj\u27e9\nHence\np\u22122\ns X\n1\u2264j\u2264r\n|\u27e8ei, \u039b2\nUEvj\u27e9|2 = p\u22122\u2225PUei\u22254\ns X\n1\u2264j\u2264r\n|\u27e8ei, uj\u27e9|2 = p\u22122\u2225PUei\u22255 \u2264\n\u0012\u00b50r\nnp\n\u00132 r\u00b50r\nn .\nIn other words, when m \u2265\u00b50nr, the right hand-side is bounded by\np\n\u00b50r/n as desired. For the\nsecond term, we have\n\u27e8ei, E\u039b2\nV vj\u27e9=\nX\nb\n\u2225PV eb\u22254vbj\u27e8ei, Eeb\u27e9=\nX\nb\n\u2225PV eb\u22254vbjEib.\nHence it follows from the Cauchy-Schwarz inequality and (6.4) that\np\u22122|\u27e8ei, E\u039b2\nV vj\u27e9| \u2264\n\u0012\u00b50r\nnp\n\u00132 r\u00b50r\nn .\nIn other words, when m \u2265\u00b50nr5/4,\np\u22122\ns X\n1\u2264j\u2264r\n|\u27e8ei, E\u039b2\nV vj\u27e9|2 \u2264\nr\u00b50r\nn\n(6.33)\nas desired. For the third term, we have\n\u27e8ei, \u039bUE\u039bV vj\u27e9= \u2225PUei\u22252 X\nb\n\u2225PV eb\u22252vbjEib.\n38\nThe Cauchy-Schwarz inequality gives\n2p\u22122|\u27e8ei, \u039bUE\u039bV vj\u27e9| \u22642\n\u0012\u00b50r\nnp\n\u00132 r\u00b50r\nn\njust as before. In other words, when m \u2265\u00b50nr5/4, 2p\u22122qP\n1\u2264j\u2264r |\u27e8ei, \u039bUE\u039bV vj\u27e9|2 is bounded by\n2\np\n\u00b50r/n. The other terms obey (6.33) as well when m \u2265\u00b50nr5/4. In conclusion, the \ufb01rst term\n(6.32) in (6.29) obeys (6.30) with probability at least 1\u2212O(n\u2212\u03b2) provided that m \u2265\u00b50nr5/4(\u03b2 log n).\nWe now turn our attention to the second term which can be written as\n\u03b3ij \u2261p\u22123(1 \u22122p)\nX\n\u03c91\u0338=\u03c92\n\u03be\u03c91\u03be\u03c92 E\u03c92P\u03c92\u03c92P\u03c92\u03c91\u27e8ei, F\u03c91vj\u27e9\n+ p\u22122(1 \u2212p)\nX\n\u03c91\u0338=\u03c92\n\u03be\u03c91 E\u03c92P\u03c92\u03c92P\u03c92\u03c91\u27e8ei, F\u03c91vj\u27e9.\nWe decouple the \ufb01rst term so that it su\ufb03ces to bound\n(S0)ij \u2261p\u22121 X\n\u03c91\n\u03be(1)\n\u03c91 H\u03c91\u27e8ei, F\u03c91vj\u27e9,\nH\u03c91 \u2261p\u22122\nX\n\u03c92:\u03c92\u0338=\u03c91\n\u03be(2)\n\u03c92 E\u03c92P\u03c92\u03c92P\u03c92\u03c91,\nwhere the sequences {\u03be(1)\n\u03c9 } and {\u03be(2)\n\u03c9 } are independent. The method from Section 6.2 shows that\n\u2225H\u2225\u221e\u2264C\nr\n\u00b50nr\u03b2 log n\nm\nsup\n\u03c9 |E\u03c9(p\u22121P\u03c9\u03c9)| \u2264C\np\n\u03b2 log n\n\u0010\u00b50nr\nm\n\u00113/2\n\u2225E\u2225\u221e\nwith probability at least 1 \u22122n\u2212\u03b2 for each \u03b2 > 2. Therefore, Lemma 6.12 gives\nP\n\u0010\n\u2225S0\u2225\u221e\u2265\np\n\u00b50/n\n\u0011\n\u22642n2e\u22121/D,\n(6.34)\nwhere D obeys\nD \u2264C\n\u0012\n\u00b52\n0\u00b52\n1(\u03b2 log n)\n\u0010nr\nm\n\u00114\n+ \u00b53/2\n0\n\u00b51\np\n\u03b2 log n\n\u0010nr\nm\n\u00115/2\u0013\n.\n(6.35)\nfor some positive constant C.\nHence, when m \u2265\u03bb\u00b50 nr5/4(\u03b2 log n) for some su\ufb03ciently large\nnumerical constant \u03bb > 0, we have that \u2225S0\u2225\u221e\u2265\np\n\u00b50/n with probability at most 2n2e\u2212(\u03b2 log n)2.\nThis is inversely proportional to a superpolynomial in n. We write the second term as\n(S1)ij \u2261p\u22121 X\n\u03c91\u0338=\u03c92\n\u03be\u03c91H\u03c91\u27e8ei, F\u03c91vj\u27e9,\nH\u03c91 = p\u22121\nX\n\u03c92:\u03c92\u0338=\u03c91\nE\u03c92P\u03c92\u03c92P\u03c92\u03c91.\nWe know from Section 6.3 that H obeys \u2225H\u2225\u221e\u2264C \u00b52\n0 r2/m since \u00b51 \u2264\u00b50\n\u221ar so that Lemma 6.12\ngives\nP\n\u0010\n\u2225S1\u2225\u221e\u2265\np\n\u00b50/n\n\u0011\n\u22642n2e\u22121/D,\nD \u2264C\n \n\u00b53\n0\nn3r4\nm3 + \u00b52\n0\nn2r5/2\nm2\n!\nfor some C > 0. Hence, when m \u2265\u03bb\u00b50 nr4/3(\u03b2 log n) for some numerical constant \u03bb > 0, we have\nthat \u2225S1\u2225\u221e\u2265\np\n\u00b50/n with probability at most 2n2e\u2212(\u03b2 log n)2. This is inversely proportional to a\nsuperpolynomial in n. In conclusion and taking into account the decoupling constants in (6.12),\n39\nthe second term in (6.29) obeys (6.30) with probability at least 1 \u2212O(n\u2212\u03b2) provided that m is\nsu\ufb03ciently large as above.\nWe now examine the third term which can be written as\np\u22123(1 \u22122p)\nX\n\u03c91\u0338=\u03c92\n\u03be\u03c91\u03be\u03c92 E\u03c91P 2\n\u03c92\u03c91\u27e8ei, F\u03c91vj\u27e9+ p\u22122(1 \u2212p)\nX\n\u03c91\u0338=\u03c92\n\u03be\u03c92 E\u03c91P 2\n\u03c92\u03c91\u27e8ei, F\u03c91vj\u27e9.\nFor the \ufb01rst term of the right-hand side, it su\ufb03ces to estimate the tail of\n(S0)ij \u2261p\u22121 X\n\u03c91\n\u03be(1)\n\u03c91 E\u03c91H\u03c91\u27e8ei, F\u03c91vj\u27e9,\nH\u03c91 \u2261p\u22122\nX\n\u03c92:\u03c92\u0338=\u03c91\n\u03be(2)\n\u03c92 P 2\n\u03c92\u03c91,\nwhere {\u03be(1)\n\u03c9 } and {\u03be(2)\n\u03c9 } are independent. We know from Section 6.3 that \u2225H\u2225\u221eobeys \u2225H\u2225\u221e\u2264\nC \u221a\u03b2 log n (\u00b50nr/m)3/2 with probability at least 1 \u22122n\u2212\u03b2 for each \u03b2 > 2. Thus, Lemma (6.12)\nshows that S0 obeys (6.34)\u2013(6.35) just as before.\nThe other term is equal to (1 \u2212p) times\nP\n\u03c91 E\u03c91H\u03c91\u27e8ei, F\u03c91vj\u27e9, and by the Cauchy-Schwarz inequality and (6.4)\n\f\f\f\f\f\nX\n\u03c91\nE\u03c91H\u03c91\u27e8ei, F\u03c91vj\u27e9\n\f\f\f\f\f \u2264\u2225H\u2225\u221e\u2225e\u2217\ni E\u2225\n X\nb\nv2\nbj\n!1/2\n\u2264C\nr\u00b50\nn\np\n\u03b2 log n\n \n\u00b50nr4/3\nm\n!3/2\non the event where \u2225H\u2225\u221e\u2264C \u221a\u03b2 log n (\u00b50nr/m)3/2. Hence, when m \u2265\u03bb\u00b50 nr4/3 (\u03b2 log n) for\nsome numerical constant \u03bb > 0, we have that | P\n\u03c91 E\u03c91H\u03c91\u27e8ei, F\u03c91vj\u27e9| \u2264\np\n\u00b50/n on this event. In\nconclusion, the third term in (6.29) obeys (6.30) with probability at least 1\u2212O(n\u2212\u03b2) provided that\nm is su\ufb03ciently large as above.\nWe proceed to the fourth term which can be written as\np\u22123(1 \u22122p)\nX\n\u03c91\u0338=\u03c93\n\u03be\u03c91\u03be\u03c93 E\u03c93P\u03c93\u03c91P\u03c91\u03c91\u27e8ei, F\u03c91vj\u27e9+ p\u22122(1 \u2212p)\nX\n\u03c91\u0338=\u03c93\n\u03be\u03c93 E\u03c93P\u03c93\u03c91P\u03c91\u03c91\u27e8ei, F\u03c91vj\u27e9.\nWe use the decoupling trick for the \ufb01rst term and bound the tail of\n(S0)ij \u2261p\u22121 X\n\u03c91\n\u03be(1)\n\u03c91 H\u03c91(p\u22121P\u03c91\u03c91) \u27e8ei, F\u03c91vj\u27e9,\nH\u03c91 \u2261p\u22121\nX\n\u03c93:\u03c93\u0338=\u03c91\n\u03be(3)\n\u03c93 E\u03c93P\u03c93\u03c91,\nwhere {\u03be(1)\n\u03c9 } and {\u03be(3)\n\u03c9 } are independent. We know from Section 6.2 that\n\u2225H\u2225\u221e\u2264C\nr\n\u00b50nr\u03b2 log n\nm\n\u2225E\u2225\u221e\nwith probability at least 1 \u22122n\u2212\u03b2 for each \u03b2 > 2. Therefore, Lemma 6.12 shows that S0 obeys\n(6.34)\u2013(6.35) just as before. The other term is equal to (1\u2212p) times P\n\u03c91 H\u03c91(p\u22121P\u03c91\u03c91) \u27e8ei, F\u03c91vj\u27e9,\nand the Cauchy-Schwarz inequality gives\n\f\f\f\f\f\nX\n\u03c91\nH\u03c91(p\u22121P\u03c91\u03c91) \u27e8ei, F\u03c91vj\u27e9\n\f\f\f\f\f \u2264\u221an \u2225H\u2225\u221e\n\u00b50nr\nm\n\u2264C \u00b51\n\u221ar\u03b2 log n\n\u221an\n\u0010\u00b50nr\nm\n\u00113/2\non the event \u2225H\u2225\u221e\u2264C\np\n\u00b50nr(\u03b2 log n)/m \u2225E\u2225\u221e. Because \u00b51 \u2264\u00b50\n\u221ar, we have that whenever\nm \u2265\u03bb \u00b54/3\n0\nnr5/3 (\u03b2 log n) for some numerical constant \u03bb > 0, p\u22121| P\n\u03c91 H\u03c91P\u03c91\u03c91\u27e8ei, F\u03c91vj\u27e9| \u2264\n40\np\n\u00b50/n just as before. In conclusion, the fourth term in (6.29) obeys (6.30) with probability at\nleast 1 \u2212O(n\u2212\u03b2) provided that m is su\ufb03ciently large as above.\nWe \ufb01nally examine the last term\np\u22123\nX\n\u03c91\u0338=\u03c92\u0338=\u03c93\n\u03be\u03c91\u03be\u03c92\u03be\u03c93 E\u03c93P\u03c93\u03c92P\u03c92\u03c91\u27e8ei, F\u03c91vj\u27e9.\nJust as before, we need to bound the tail of\n(S0)ij \u2261p\u22121\nX\n\u03c91,\u03c92,\u03c93\n\u03be(1)\n\u03c91 H\u03c91\u27e8ei, F\u03c91vj\u27e9,\nwhere H is given by (6.23). We know from Section 6.3 that H obeys\n\u2225H\u2225\u221e\u2264C (\u03b2 log n) \u00b50nr\nm\n\u00b51\n\u221ar\nn\nwith probability at least 1 \u22124n\u2212\u03b2 for each \u03b2 > 2. Therefore, Lemma 6.12 gives\nP\n\u0012\n\u2225S0\u2225\u221e\u22651\n5\np\n\u00b50/n\n\u0013\n\u22642n2e\u22121/D,\nD \u2264C\n\u0012\n\u00b50\u00b52\n1(\u03b2 log n)2 \u0010nr\nm\n\u00113\n+ \u00b50\u00b51(\u03b2 log n)\n\u0010nr\nm\n\u00112\u0013\nfor some C > 0. Hence, when m \u2265\u03bb\u00b50 nr4/3(\u03b2 log n) for some numerical constant \u03bb > 0, we have\nthat \u2225S0\u2225\u221e\u22651\n5\np\n\u00b50/n with probability at most 2n2e\u2212(\u03b2 log n). In conclusion, the \ufb01fth term in\n(6.29) obeys (6.30) with probability at least 1 \u2212O(n\u2212\u03b2) provided that m is su\ufb03ciently large as\nabove.\nTo summarize the calculations of this section, if m = \u03bb \u00b54/3\n0\nnr5/3 (\u03b2 log n) where \u03b2 \u22652 is \ufb01xed\nand \u03bb is some su\ufb03ciently large numerical constant, then\nX\n1\u2264j\u2264r\n|\u03b1ij|2 \u2264\u00b50r/n\nwith probability at least 1 \u2212O(n\u2212\u03b2). This concludes the proof.\n6.5\nProof of Lemma 4.8\nIt remains to study the spectral norm of p\u22121(PT \u22a5P\u2126PT ) P\nk\u2265k0 Hk(E) for some positive integer\nk0, which we bound by the Frobenius norm\np\u22121\u2225(PT \u22a5P\u2126PT )\nX\nk\u2265k0\nHk(E)\u2225\u2264p\u22121\u2225(P\u2126PT )\nX\nk\u2265k0\nHk(E)\u2225F\n\u2264\np\n3/2p \u2225\nX\nk\u2265k0\nHk(E)\u2225F ,\nwhere the inequality follows from Corollary 4.3. To bound the Frobenius of the series, write\n\u2225\nX\nk\u2265k0\nHk(E)\u2225F \u2264\u2225H\u2225k0\u2225E\u2225F + \u2225H\u2225k0+1\u2225E\u2225F + . . .\n\u2264\n\u2225H\u2225k0\n1 \u2212\u2225H\u2225\u2225E\u2225F .\n41\nTheorem 4.1 gives an upper bound on \u2225H\u2225since \u2225H\u2225\u2264CR\np\n\u00b50nr\u03b2 log n/m < 1/2 on an event\nwith probability at least 1 \u22123n\u2212\u03b2. Since \u2225E\u2225F = \u221ar, we conclude that\np\u22121\u2225(P\u2126PT )\nX\nk\u2265k0\nHk(E)\u2225F \u2264C 1\n\u221ap\n\u0012\u00b50nr\u03b2 log n\nm\n\u0013k0/2 \u221ar = C\n\u0012n2r\nm\n\u00131/2 \u0012\u00b50nr\u03b2 log n\nm\n\u0013k0/2\nwith large probability. This is the content of Lemma 4.8.\n7\nNumerical Experiments\nTo demonstrate the practical applicability of the nuclear norm heuristic for recovering low-rank\nmatrices from their entries, we conducted a series of numerical experiments for a variety of the\nmatrix sizes n, ranks r, and numbers of entries m.\nFor each (n, m, r) triple, we repeated the\nfollowing procedure 50 times. We generated M, an n \u00d7 n matrix of rank r, by sampling two n \u00d7 r\nfactors ML and MR with i.i.d. Gaussian entries and setting M = MLM \u2217\nR. We sampled a subset\n\u2126of m entries uniformly at random. Then the nuclear norm minimization\nminimize\n\u2225X\u2225\u2217\nsubject to\nXij = Mij,\n(i, j) \u2208\u2126\nwas solved using the SDP solver SDPT3 [34].\nWe declared M to be recovered if the solution\nreturned by the SDP, Xopt, satis\ufb01ed \u2225Xopt \u2212M\u2225F /\u2225M\u2225F < 10\u22123. Figure 1 shows the results\nof these experiments for n = 40 and 50. The x-axis corresponds to the fraction of the entries of\nthe matrix that are revealed to the SDP solver. The y-axis corresponds to the ratio between the\ndimension of the set of rank r matrices, dr = r(2n \u2212r), and the number of measurements m. Note\nthat both of these axes range from zero to one as a value greater than one on the x-axis corresponds\nto an overdetermined linear system where the semide\ufb01nite program always succeeds, and a value of\ngreater than one on the y-axis corresponds to a situation where there is always an in\ufb01nite number\nof matrices with rank r with the given entries. The color of each cell in the \ufb01gures re\ufb02ects the\nempirical recovery rate of the 50 runs (scaled between 0 and 1). White denotes perfect recovery in\nall experiments, and black denotes failure for all experiments. Interestingly, the experiments reveal\nvery similar plots for di\ufb00erent n, suggesting that our asymptotic conditions for recovery may be\nrather conservative.\nFor a second experiment, we generated random positive semide\ufb01nite matrices and tried to\nrecover them from their entries using the nuclear norm heuristic. As above, we repeated the same\nprocedure 50 times for each (n, m, r) triple. We generated M, an n\u00d7n positive semide\ufb01nite matrix\nof rank r, by sampling an n \u00d7 r factor MF with i.i.d. Gaussian entries and setting M = MF M \u2217\nF .\nWe sampled a subset \u2126of m entries uniformly at random.\nThen we solved the nuclear norm\nminimization problem\nminimize\ntrace(X)\nsubject to\nXij = Mij,\n(i, j) \u2208\u2126\nX \u2ab00\n.\nAs above, we declared M to be recovered if \u2225Xopt \u2212M\u2225F /\u2225M\u2225F < 10\u22123. Figure 2 shows the\nresults of these experiments for n = 40 and 50.\nThe x-axis again corresponds to the fraction\nof the entries of the matrix that are revealed to the SDP solver, but, in this case, the number of\n42\nm/n2\ndr/m\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n1\n0.9\n0.8\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\nm/n2\ndr/m\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n1\n0.9\n0.8\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\n(a)\n(b)\nFigure 1: Recovery of full matrices from their entries. For each (n, m, r) triple, we\nrepeated the following procedure 50 times. A matrix M of rank r and a subset of m entries were\nselected at random. Then we solved the nuclear norm minimization for X subject to Xij = Mij\non the selected entries. We declared M to be recovered if \u2225Xopt \u2212M\u2225F /\u2225M\u2225F < 10\u22123. The\nresults are shown for (a) n = 40 and (b) n = 50. The color of each cell re\ufb02ects the empirical\nrecovery rate (scaled between 0 and 1). White denotes perfect recovery in all experiments, and\nblack denotes failure for all experiments.\nmeasurements is divided by Dn = n(n+1)/2, the number of unique entries in a positive-semide\ufb01nite\nmatrix and the dimension of the rank r matrices is dr = nr \u2212r(r \u22121)/2. The color of each cell\nis chosen in the same fashion as in the experiment with full matrices. Interestingly, the recovery\nregion is much larger for positive semide\ufb01nite matrices, and future work is needed to investigate if\nthe theoretical scaling is also more favorable in this scenario of low-rank matrix completion.\nFinally, in Figure 3, we plot the performance of the nuclear norm heuristic when recovering\nlow-rank matrices from Gaussian projections of these matrices. In these cases, M was generated\nin the same fashion as above, but, in place of sampling entries, we generated m random Gaussian\nprojections of the data (see the discussion in Section 1.4). Then we solved the optimization\nminimize\n\u2225X\u2225\u2217\nsubject to\nA(X) = A(M) .\nwith the additional constraint that X \u2ab00 in the positive semide\ufb01nite case. Here A(X) denotes a\nlinear map of the form (1.15) where the entries are sampled i.i.d. from a zero-mean unit variance\nGaussian distribution. In these experiments, the recovery regime is far larger than in the case\nof that of sampling entries, but this is not particularly surprising as each Gaussian observation\nmeasures a contribution from every entry in the matrix M. These Gaussian models were studied\nextensively in [27].\n43\nm/Dn\ndr/m\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n1\n0.9\n0.8\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\nm/Dn\ndr/m\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n1\n0.9\n0.8\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\n(a)\n(b)\nFigure 2: Recovery of positive semide\ufb01nite matrices from their entries. For each\n(n, m, r) triple, we repeated the following procedure 50 times. A positive semide\ufb01nite matrix\nM of rank r and a set of m entries were selected at random. Then we solved the nuclear norm\nminimization subject to Xij = Mij on the selected entries with the constraint that X \u2ab00.\nThe color scheme for each cell denotes empirical recovery probability and is the same as in\nFigure 1. The results are shown for (a) n = 40 and (b) n = 50.\nm/n2\ndr/m\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n1\n0.9\n0.8\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\nm/Dn\ndr/m\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n1\n0.9\n0.8\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\n(a)\n(b)\nFigure 3: Recovery of matrices from Gaussian observations. For each (n, m, r) triple,\nwe repeated the following procedure 10 times. In (a), a matrix of rank r was generated as in\nFigures 1. In (b) a positive semide\ufb01nite matrix of rank r was generated as in Figures 2. In\nboth plots, we select a matrix A from the Gaussian ensemble with m rows and n2 (in (a)) or\nDn = n(n + 1)/2 (in (b)) columns. Then we solve the nuclear norm minimization subject to\nA(X) = A(M). The color scheme for each cell denotes empirical recovery probability and is\nthe same as in Figures 1 and 2.\n44\n8\nDiscussion\n8.1\nImprovements\nIn this paper, we have shown that under suitable conditions, one can reconstruct an n \u00d7 n matrix\nof rank r from a small number of its sampled entries provided that this number is on the order\nof n1.2r log n, at least for moderate values of the rank. One would like to know whether better\nresults hold in the sense that exact matrix recovery would be guaranteed with a reduced number\nof measurements. In particular, recall that an n \u00d7 n matrix of rank r depends on (2n \u2212r)r degrees\nof freedom; is it true then that it is possible to recover most low-rank matrices from on the order\nof nr\u2014up to logarithmic multiplicative factors\u2014randomly selected entries? Can the sample size\nbe merely proportional to the true complexity of the low-rank object we wish to recover?\nIn this direction, we would like to emphasize that there is nothing in our approach that appar-\nently prevents us from getting stronger results. Indeed, we developed a bound on the spectral norm\nof each of the \ufb01rst four terms (PT \u22a5P\u2126PT )Hk(E) in the series (4.13) (corresponding to values of k\nequal to 0, 1, 2, 3) and used a general argument to bound the remainder of the series. Presumably,\none could bound higher order terms by the same techniques. Getting an appropriate bound on\n\u2225(PT \u22a5P\u2126PT )H4(E)\u2225would lower the exponent of n from 6/5 to 7/6. The appropriate bound on\n\u2225(PT \u22a5P\u2126PT )H5(E)\u2225would further lower the exponent to 8/7, and so on. To obtain an optimal\nresult, one would need to reach k of size about log n. In doing so, however, one would have to\npay special attention to the size of the decoupling constants (the constant CD for two variables in\nLemma 6.5) which depend on k\u2014the number of decoupled variables. These constants grow with k\nand upper bounds are known [15,16].\n8.2\nFurther directions\nIt would be of interest to extend our results to the case where the unknown matrix is approximately\nlow-rank. Suppose we write the SVD of a matrix M as\nM =\nX\n1\u2264k\u2264n\n\u03c3kukv\u2217\nk,\nwhere \u03c31 \u2265\u03c32 \u2265. . . \u2265\u03c3n \u22650 and assume for simplicity that none of the \u03c3k\u2019s vanish. In general, it\nis impossible to complete such a matrix exactly from a partial subset of its entries. However, one\nmight hope to be able to recover a good approximation if, for example, most of the singular values\nare small or negligible. For instance, consider the truncated SVD of the matrix M,\nMr =\nX\n1\u2264k\u2264r\n\u03c3kukv\u2217\nk,\nwhere the sum extends over the r largest singular values and let M\u22c6be the solution to (1.5). Then\none would not expect to have M\u22c6= M but it would be of great interest to determine whether the\nsize of M\u22c6\u2212M is comparable to that of M \u2212Mr provided that the number of sampled entries\nis su\ufb03ciently large. For example, one would like to know whether it is reasonable to expect that\n\u2225M\u22c6\u2212M\u2225\u2217is on the same order as \u2225M \u2212Mr\u2225\u2217(one could ask for a similar comparison with a\ndi\ufb00erent norm). If the answer is positive, then this would say that approximately low-rank matrices\ncan be accurately recovered from a small set of sampled entries.\n45\nAnother important direction is to determine whether the reconstruction is robust to noise as in\nsome applications, one would presumably observe\nYij = Mij + zij,\n(i, j) \u2208\u2126,\nwhere z is a deterministic or stochastic perturbation. In this setup, one would perhaps want to\nminimize the nuclear norm subject to \u2225P\u2126(X \u2212Y )\u2225F \u2264\u03f5 where \u03f5 is an upper bound on the\nnoise level instead of enforcing the equality constraint P\u2126(X) = P\u2126(Y ). Can one expect that this\nalgorithm or a variation thereof provides accurate answers? That is, can one expect that the error\nbetween the recovered and the true data matrix be proportional to the noise level?\n9\nAppendix\n9.1\nProof of Theorem 4.2\nThe proof of (4.10) follows that in [10] but we shall use slightly more precise estimates.\nLet Y1, . . . , Yn be a sequence of independent random variables taking values in a Banach space\nand let Y\u22c6be the supremum de\ufb01ned as\nY\u22c6= sup\nf\u2208F\nn\nX\ni=1\nf(Yi),\n(9.1)\nwhere F is a countable family of real-valued functions such that if f \u2208F, then \u2212f \u2208F. Talagrand\n[33] proved a concentration inequality about Y\u22c6, see also [22, Corollary 7.8].\nTheorem 9.1 Assume that |f| \u2264B and E f(Yi) = 0 for every f in F and i = 1, . . . , n. Then for\nall t \u22650,\nP(|Y\u22c6\u2212E Y\u22c6| > t) \u22643 exp\n\u0012\n\u2212t\nKB log\n\u0012\n1 +\nBt\n\u03c32 + B E Y\u22c6\n\u0013\u0013\n,\n(9.2)\nwhere \u03c32 = supf\u2208F\nPn\ni=1 E f2(Yi), and K is a numerical constant.\nWe note that very precise values of the numerical constant K are known and are small, see [20].\nWe will apply this theorem to the random variable Z de\ufb01ned in the statement of Theorem 4.2.\nPut Yab = p\u22121(\u03b4ab \u2212p) PT (eae\u2217\nb) \u2297PT (eae\u2217\nb) and Y = P\nab Yab. By de\ufb01nition,\nZ = sup \u27e8X1, Y(X2)\u27e9= sup\nX\nab\n\u27e8X1, Yab(X2)\u27e9\n= sup p\u22121 X\nab\n(\u03b4ab \u2212p)\u27e8X1, PT (eae\u2217\nb)\u27e9\u27e8PT (eae\u2217\nb), X2\u27e9,\nwhere the supremum is over a countable collection of matrices X1 and X2 obeying \u2225X1\u2225F \u22641 and\n\u2225X2\u2225F \u22641. Note that it follows from (4.8)\n|\u27e8X1, Yab(X2)\u27e9| = p\u22121 |\u03b4ab \u2212p| |\u27e8X1, PT (eae\u2217\nb)\u27e9| |\u27e8PT (eae\u2217\nb), X2\u27e9|\n\u2264p\u22121 \u2225PT (eae\u2217\nb)\u22252\nF \u22642\u00b50r/(min(n1, n2)p) = 2\u00b50 nr/m\n46\n(recall that n = max(n1, n2)). Hence, we can apply Theorem 9.1 with B = 2\u00b50(nr/m). Also\nE |\u27e8X1, Yab(X2)\u27e9|2 = p\u22121(1 \u2212p) |\u27e8X1, PT (eae\u2217\nb)\u27e9|2 |\u27e8X2, PT (eae\u2217\nb)\u27e9|2\n\u2264p\u22121 \u2225PT (eae\u2217\nb)\u22252\nF |\u27e8PT (X2), eae\u2217\nb\u27e9|2\nso that\nX\nab\nE |\u27e8X1, Yab(X2)\u27e9|2 \u2264(2\u00b50 nr/m)\nX\nab\n|\u27e8PT (X2), eae\u2217\nb\u27e9|2\n= (2\u00b50 nr/m) \u2225PT (X2)\u22252\nF \u22642\u00b50nr/m.\nSince E Z \u22641, Theorem 9.1 gives\nP(|Z \u2212E Z| > t) \u22643 exp\n\u0012\n\u2212t\nKB log(1 + t/2)\n\u0013\n\u22643 exp\n\u0012\n\u2212t log 2\nKB min(1, t/2)\n\u0013\n,\nwhere we have used the fact that log(1+u) \u2265(log 2) min(1, u) for u \u22650. Plugging t = \u03bb\nq\n\u00b50 nr log n\nm\nand B = 2\u00b50 nr/m establishes the claim.\n9.2\nProof of Lemma 6.2\nWe shall make use of the following lemma which is an application of well-known deviation bounds\nabout binomial variables.\nLemma 9.2 Let {\u03b4i}1\u2264i\u2264n be a sequence of i.i.d. Bernoulli variables with P(\u03b4i = 1) = p and\nY = Pn\ni=1 \u03b4i. Then for each \u03bb > 0,\nP(Y > \u03bb E Y ) \u2264exp\n\u0012\n\u2212\n\u03bb2\n2 + 2\u03bb/3 E Y\n\u0013\n.\n(9.3)\nThe random variable P\nb \u03b4abE2\nab is bounded by \u2225E\u22252\n\u221e\nP\nb \u03b4ab and it thus su\ufb03ces to estimate the\nqth moment of Y\u2217= max Ya where Ya = P\nb \u03b4ab. The inequality (9.3) implies that\nP(Y\u2217> \u03bbnp) \u2264n exp\n\u0012\n\u2212\n\u03bb2\n2 + 2\u03bb/3 np\n\u0013\n,\nand for \u03bb \u22652, this gives P(Y\u2217> \u03bbnp) \u2264n e\u2212\u03bbnp/2. Hence\nE Y q\n\u2217=\nZ \u221e\n0\nP(Y\u2217> t) qtq\u22121 dt \u2264(2np)q +\nZ \u221e\n2np\nn e\u2212t/2 qtq\u22121 dt.\nBy integrating by parts, one can check that when q \u2264np, we have\nZ \u221e\n2np\nn e\u2212t/2 qtq\u22121 dt \u2264nq (2np)q e\u2212np.\nUnder the assumptions of the lemma, we have nq e\u2212np \u22641 and, therefore,\nE Y q\n\u2217\u22642 (2np)q.\nThe conclusion follows.\n47\nAcknowledgments\nE. C. was partially supported by a National Science Foundation grant CCF-515362, by the 2006\nWaterman Award (NSF) and by an ONR grant. The authors would like to thank Ali Jadbabaie,\nPablo Parrilo, Ali Rahimi, Terence Tao, and Joel Tropp for fruitful discussions about parts of this\npaper. E. C. would like to thank Arnaud Durand for his careful proof-reading and comments.\nReferences\n[1] J. Abernethy, F. Bach, T. Evgeniou, and J.-P. Vert. Low-rank matrix factorization with attributes.\nTechnical Report N24/06/MM, Ecole des Mines de Paris, 2006.\n[2] ACM SIGKDD and Net\ufb02ix. Proceedings of KDD Cup and Workshop, 2007. Proceedings available online\nat http://www.cs.uic.edu/\u223cliub/KDD-cup-2007/proceedings.html.\n[3] Y. Amit, M. Fink, N. Srebro, and S. Ullman. Uncovering shared structures in multiclass classi\ufb01cation.\nIn Proceedings of the Twenty-fourth International Conference on Machine Learning, 2007.\n[4] T. Ando, R. A. Horn, and C. R. Johnson.\nThe singular values of a Hadamard product: A basic\ninequality. Linear and Multilinear Algebra, 21:345\u2013365, 1987.\n[5] A. Argyriou, T. Evgeniou, and M. Pontil. Multi-task feature learning. In Neural Information Processing\nSystems, 2007.\n[6] C. Beck and R. D\u2019Andrea. Computational study and comparisons of LFT reducibility methods. In\nProceedings of the American Control Conference, 1998.\n[7] D. P. Bertsekas, A. Nedic, and A. E. Ozdaglar. Convex Analysis and Optimization. Athena Scienti\ufb01c,\nBelmont, MA, 2003.\n[8] B. Bollob\u00b4as. Random Graphs. Cambridge University Press, Cambridge, 2nd edition, 2001.\n[9] A. Buchholz. Operator Khintchine inequality in non-commutative probability. Math. Annalen, 319:1\u201316,\n2001.\n[10] E. J. Cand`es and J. Romberg. Sparsity and incoherence in compressive sampling. Inverse Problems,\n23(3):969\u2013985, 2007.\n[11] E. J. Cand`es, J. Romberg, and T. Tao. Robust uncertainty principles: exact signal reconstruction from\nhighly incomplete frequency information. IEEE Trans. Inform. Theory, 52(2):489\u2013509, 2006.\n[12] E. J. Cand`es and T. Tao. Decoding by linear programming. IEEE Transactions on Information Theory,\n51(12):4203\u20134215, 2005.\n[13] E. J. Cand`es and T. Tao. Near optimal signal recovery from random projections: Universal encoding\nstrategies? IEEE Trans. Inform. Theory, 52(12):5406\u20135425, December 2006.\n[14] A. L. Chistov and D. Yu. Grigoriev. Complexity of quanti\ufb01er elimination in the theory of algebraically\nclosed \ufb01elds. In Proceedings of the 11th Symposium on Mathematical Foundations of Computer Science,\nvolume 176 of Lecture Notes in Computer Science, pages 17\u201331. Springer Verlag, 1984.\n[15] V. H. de la Pe\u02dcna. Decoupling and Khintchine\u2019s inequalities for U-statistics. Ann. Probab., 20(4):1877\u2013\n1892, 1992.\n[16] V. H. de la Pe\u02dcna and S. J. Montgomery-Smith. Decoupling inequalities for the tail probabilities of\nmultivariate U-statistics. Ann. Probab., 23(2):806\u2013816, 1995.\n[17] D. L. Donoho. Compressed sensing. IEEE Trans. Inform. Theory, 52(4):1289\u20131306, 2006.\n[18] M. Fazel. Matrix Rank Minimization with Applications. PhD thesis, Stanford University, 2002.\n48\n[19] R. A. Horn and C. R. Johnson. Topics in matrix analysis. Cambridge University Press, Cambridge,\n1994. Corrected reprint of the 1991 original.\n[20] T. Klein and E. Rio. Concentration around the mean for maxima of empirical processes. Ann. Probab.,\n33(3):1060\u20131077, 2005.\n[21] B. Laurent and P. Massart. Adaptive estimation of a quadratic functional by model selection. Ann.\nStatist., 28(5):1302\u20131338, 2000.\n[22] M. Ledoux. The Concentration of Measure Phenomenon. American Mathematical Society, 2001.\n[23] A. S. Lewis. The mathematics of eigenvalue optimization. Mathematical Programming, 97(1\u20132):155\u2013176,\n2003.\n[24] N. Linial, E. London, and Y. Rabinovich. The geometry of graphs and some of its algorithmic applica-\ntions. Combinatorica, 15:215\u2013245, 1995.\n[25] F. Lust-Picquard. In\u00b4egalit\u00b4es de Khintchine dans Cp (1 < p < \u221e). Comptes Rendus Acad. Sci. Paris,\nS\u00b4erie I, 303(7):289\u2013292, 1986.\n[26] M. Mesbahi and G. P. Papavassilopoulos. On the rank minimization problem over a positive semide\ufb01nite\nlinear matrix inequality. IEEE Transactions on Automatic Control, 42(2):239\u2013243, 1997.\n[27] B. Recht, M. Fazel, and P. Parrilo. Guaranteed minimum rank solutions of matrix equations via nuclear\nnorm minimization. 2007. Submitted to SIAM Review.\n[28] J. D. M. Rennie and N. Srebro. Fast maximum margin matrix factorization for collaborative prediction.\nIn Proceedings of the International Conference of Machine Learning, 2005.\n[29] M. Rudelson. Random vectors in the isotropic position. J. Funct. Anal., 164(1):60\u201372, 1999.\n[30] M. Rudelson and R. Vershynin. Sampling from large matrices: an approach through geometric functional\nanalysis. J. ACM, 54(4):Art. 21, 19 pp. (electronic), 2007.\n[31] A. M.-C. So and Y. Ye. Theory of semide\ufb01nite programming for sensor network localization. Mathe-\nmatical Programming, Series B, 109, 2007.\n[32] N. Srebro. Learning with Matrix Factorizations. PhD thesis, Massachusetts Institute of Technology,\n2004.\n[33] M. Talagrand. New concentration inequalities in product spaces. Invent. Math., 126(3):505\u2013563, 1996.\n[34] K. C. Toh, M.J. Todd, and R. H. T\u00a8ut\u00a8unc\u00a8u. SDPT3 - a MATLAB software package for semide\ufb01nite-\nquadratic-linear programming. Available from http://www.math.nus.edu.sg/~mattohkc/sdpt3.html.\n[35] L. Vandenberghe and S. P. Boyd. Semide\ufb01nite programming. SIAM Review, 38(1):49\u201395, 1996.\n[36] G. A. Watson.\nCharacterization of the subdi\ufb00erential of some matrix norms.\nLinear Algebra and\nApplications, 170:1039\u20131053, 1992.\n49\n",
        "sentence": "",
        "context": "Pablo Parrilo, Ali Rahimi, Terence Tao, and Joel Tropp for fruitful discussions about parts of this\npaper. E. C. would like to thank Arnaud Durand for his careful proof-reading and comments.\nReferences\nThe conclusion follows.\n47\nAcknowledgments\nE. C. was partially supported by a National Science Foundation grant CCF-515362, by the 2006\nWaterman Award (NSF) and by an ONR grant. The authors would like to thank Ali Jadbabaie,\nclosed \ufb01elds. In Proceedings of the 11th Symposium on Mathematical Foundations of Computer Science,\nvolume 176 of Lecture Notes in Computer Science, pages 17\u201331. Springer Verlag, 1984."
    },
    {
        "title": "Foundations of Large-Scale Multimedia Information Management and Retrieval",
        "author": [
            "Edward Y Chang"
        ],
        "venue": null,
        "citeRegEx": "Chang.,? \\Q2011\\E",
        "shortCiteRegEx": "Chang.",
        "year": 2011,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " For the future work, we plan to improve our models so that they will be capable of incremental learning on large-scale datasets (Chang, 2011).",
        "context": null
    },
    {
        "title": "A generalization of principal components analysis to the exponential family",
        "author": [
            "Sanjoy Dasgupta",
            "Robert E Schapire"
        ],
        "venue": "In Advances in neural information processing systems,",
        "citeRegEx": "Collins et al\\.,? \\Q2001\\E",
        "shortCiteRegEx": "Collins et al\\.",
        "year": 2001,
        "abstract": "",
        "full_text": "",
        "sentence": " Their approach is composed of several models, such as PCA (Collins et al., 2001) and collaborative filtering (Koren, 2008).",
        "context": null
    },
    {
        "title": "Constructing biological knowledge bases by extracting information from text sources",
        "author": [
            "Craven",
            "Kumlien1999] Mark Craven",
            "Johan Kumlien"
        ],
        "venue": "In ISMB,",
        "citeRegEx": "Craven et al\\.,? \\Q1999\\E",
        "shortCiteRegEx": "Craven et al\\.",
        "year": 1999,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "A rank minimization heuristic with application to minimum order system",
        "author": [
            "Fazel et al.2001] Maryam Fazel",
            "Haitham Hindi",
            "Stephen P Boyd"
        ],
        "venue": null,
        "citeRegEx": "Fazel et al\\.,? \\Q2001\\E",
        "shortCiteRegEx": "Fazel et al\\.",
        "year": 2001,
        "abstract": "",
        "full_text": "",
        "sentence": " , 2011), recommender system (Rennie and Srebro, 2005) and system controlling (Fazel et al., 2001).",
        "context": null
    },
    {
        "title": "Transduction with matrix completion: Three birds with one stone",
        "author": [
            "Ben Recht",
            "Junming Xu",
            "Robert Nowak",
            "Xiaojin Zhu"
        ],
        "venue": "In Advances in neural information processing systems,",
        "citeRegEx": "Goldberg et al\\.,? \\Q2010\\E",
        "shortCiteRegEx": "Goldberg et al\\.",
        "year": 2010,
        "abstract": "",
        "full_text": "",
        "sentence": " edu/riedel/data-univSchema/ We follow the suggestion in (Goldberg et al., 2010) that \u03bc starts at \u03c31\u03b7\u03bc, and \u03c31 is the largest singular value of the matrix Z. edu/riedel/data-univSchema/ We follow the suggestion in (Goldberg et al., 2010) that \u03bc starts at \u03c31\u03b7\u03bc, and \u03c31 is the largest singular value of the matrix Z. We set \u03b7\u03bc = 0.01. The final value of \u03bc, namely \u03bcF , is equal to 0.01. Ma et al. (2011) revealed that as long as the nonnegative step sizes satisfy \u03c4z < min( 4|\u03a9Y | \u03bb , |\u03a9X |) and \u03c4b < 4|\u03a9Y | \u03bb(n+m) , the FPC algorithm will guarantee to converge to a global optimum.",
        "context": null
    },
    {
        "title": "Convergence of fixed-point continuation algorithms for matrix rank minimization",
        "author": [
            "Goldfarb",
            "Ma2011] Donald Goldfarb",
            "Shiqian Ma"
        ],
        "venue": "Foundations of Computational Mathematics,",
        "citeRegEx": "Goldfarb et al\\.,? \\Q2011\\E",
        "shortCiteRegEx": "Goldfarb et al\\.",
        "year": 2011,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Calculating the singular values and pseudo-inverse of a matrix",
        "author": [
            "Golub",
            "Kahan1965] Gene Golub",
            "William Kahan"
        ],
        "venue": "Journal of the Society for Industrial & Applied Mathematics, Series B: Numerical Analysis,",
        "citeRegEx": "Golub et al\\.,? \\Q1965\\E",
        "shortCiteRegEx": "Golub et al\\.",
        "year": 1965,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Knowledge-based weak supervision for information extraction of overlapping relations",
        "author": [
            "Congle Zhang",
            "Xiao Ling",
            "Luke Zettlemoyer",
            "Daniel S. Weld"
        ],
        "venue": "In Proceedings of the 49th Annual Meeting",
        "citeRegEx": "Hoffmann et al\\.,? \\Q2011\\E",
        "shortCiteRegEx": "Hoffmann et al\\.",
        "year": 2011,
        "abstract": "",
        "full_text": "",
        "sentence": " , 2009) or weak (Hoffmann et al., 2011) supervision paradigm attractive, and we improve the effectiveness of the paradigm in this paper. In order to conduct reliable experiments, we adjust and estimate the parameters for our approaches, DRMC-b and DRMC-1, and compare them with other four kinds of landmark methods (Mintz et al., 2009; Hoffmann et al., 2011; Surdeanu et al., 2012; Riedel et al., 2013) on two public datasets. (2010), and also used by Hoffmann et al. (2011) and Surdeanu et al. (2010), and also used by Hoffmann et al. (2011) and Surdeanu et al. (2012). Three kinds of features, namely, lexical, syntactic and named entity tag features, were extracted from relation mentions. (2010), and also used by Hoffmann et al. (2011) and Surdeanu et al. (2012). Three kinds of features, namely, lexical, syntactic and named entity tag features, were extracted from relation mentions. The second dataset13, NYT\u201913, was also released by Riedel et al. (2013), in which they only regarded the lexicalized dependency path between two entities as features. , 2009), MultiR-11 (Hoffmann et al., 2011), MIML-12 and MIML-at-least-one-12 (Surdeanu et al. , 2009), MultiR-11 (Hoffmann et al., 2011), MIML-12 and MIML-at-least-one-12 (Surdeanu et al., 2012) on NYT\u201910 dataset. Surdeanu et al. (2012) released the open source code15 to reproduce the experimental results on those previous methods.",
        "context": null
    },
    {
        "title": "Why the logistic function? a tutorial discussion on probabilities and neural networks. Computational Cognitive Science Technical Report",
        "author": [
            "Michael Jordan"
        ],
        "venue": null,
        "citeRegEx": "Jordan.,? \\Q1995\\E",
        "shortCiteRegEx": "Jordan.",
        "year": 1995,
        "abstract": "",
        "full_text": "",
        "sentence": " We assume that the actual entry u belonging to the underlying matrix Z\u2217 is randomly generated via a sigmoid function (Jordan, 1995): Pr(u|v) = 1/(1 + e\u2212uv), given the observed binary entry v from the observed sparse matrix Z.",
        "context": null
    },
    {
        "title": "Factorization meets the neighborhood: a multifaceted collaborative filtering model",
        "author": [
            "Yehuda Koren"
        ],
        "venue": "In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining,",
        "citeRegEx": "Koren.,? \\Q2008\\E",
        "shortCiteRegEx": "Koren.",
        "year": 2008,
        "abstract": "",
        "full_text": "",
        "sentence": " , 2001) and collaborative filtering (Koren, 2008).",
        "context": null
    },
    {
        "title": "Fixed point and bregman iterative methods for matrix rank minimization",
        "author": [
            "Ma et al.2011] Shiqian Ma",
            "Donald Goldfarb",
            "Lifeng Chen"
        ],
        "venue": "Mathematical Programming,",
        "citeRegEx": "Ma et al\\.,? \\Q2011\\E",
        "shortCiteRegEx": "Ma et al\\.",
        "year": 2011,
        "abstract": "The linearly constrained matrix rank minimization problem is widely\napplicable in many fields such as control, signal processing and system\nidentification. The tightest convex relaxation of this problem is the linearly\nconstrained nuclear norm minimization. Although the latter can be cast as a\nsemidefinite programming problem, such an approach is computationally expensive\nto solve when the matrices are large. In this paper, we propose fixed point and\nBregman iterative algorithms for solving the nuclear norm minimization problem\nand prove convergence of the first of these algorithms. By using a homotopy\napproach together with an approximate singular value decomposition procedure,\nwe get a very fast, robust and powerful algorithm, which we call FPCA (Fixed\nPoint Continuation with Approximate SVD), that can solve very large matrix rank\nminimization problems. Our numerical results on randomly generated and real\nmatrix completion problems demonstrate that this algorithm is much faster and\nprovides much better recoverability than semidefinite programming solvers such\nas SDPT3. For example, our algorithm can recover 1000 x 1000 matrices of rank\n50 with a relative error of 1e-5 in about 3 minutes by sampling only 20 percent\nof the elements. We know of no other method that achieves as good\nrecoverability. Numerical experiments on online recommendation, DNA microarray\ndata set and image inpainting problems demonstrate the effectiveness of our\nalgorithms.",
        "full_text": "arXiv:0905.1643v2  [math.OC]  12 May 2009\nMath. Program., Ser. A manuscript No.\n(will be inserted by the editor)\nShiqian Ma \u00b7 Donald Goldfarb \u00b7 Lifeng Chen\nFixed point and Bregman iterative methods for matrix rank\nminimization\nSubmitted October 27, 2008, Revised May 7, 2009\nAbstract The linearly constrained matrix rank minimization problem is widely applicable in many \ufb01elds such as\ncontrol, signal processing and system identi\ufb01cation. The tightest convex relaxation of this problem is the linearly\nconstrained nuclear norm minimization. Although the latter can be cast as a semide\ufb01nite programming problem,\nsuch an approach is computationally expensive to solve when the matrices are large. In this paper, we propose\n\ufb01xed point and Bregman iterative algorithms for solving the nuclear norm minimization problem and prove conver-\ngence of the \ufb01rst of these algorithms. By using a homotopy approach together with an approximate singular value\ndecomposition procedure, we get a very fast, robust and powerful algorithm, which we call FPCA (Fixed Point Con-\ntinuation with Approximate SVD), that can solve very large matrix rank minimization problems 1. Our numerical\nresults on randomly generated and real matrix completion problems demonstrate that this algorithm is much faster\nand provides much better recoverability than semide\ufb01nite programming solvers such as SDPT3. For example, our\nalgorithm can recover 1000 \u00d7 1000 matrices of rank 50 with a relative error of 10\u22125 in about 3 minutes by sam-\npling only 20 percent of the elements. We know of no other method that achieves as good recoverability. Numerical\nexperiments on online recommendation, DNA microarray data set and image inpainting problems demonstrate the\neffectiveness of our algorithms.\nKeywords Matrix Rank Minimization \u00b7 Matrix Completion Problem \u00b7 Nuclear Norm Minimization \u00b7 Fixed Point\nIterative Method \u00b7 Bregman Distances \u00b7 Singular Value Decomposition\nAMS subject classi\ufb01cation. 65K05, 90C25, 90C06, 93C41, 68Q32\n1 Introduction\nThe matrix rank minimization problem can be written as\nmin rank(X)\ns.t. X \u2208C ,\nDepartment of Industrial Engineering and Operations Research, Columbia University, New York, NY 10027. Email: {sm2756, goldfarb,\nlc2161}@columbia.edu\nResearch supported in part by NSF Grant DMS 06-06712, ONR Grants N00014-03-0514 and N00014-08-1-1118, and DOE Grants\nDE-FG01-92ER-25126 and DE-FG02-08ER-58562.\n1 The code can be downloaded from http://www.columbia.edu/\u223csm2756/FPCA.htm for non-commercial use.\n2\nShiqian Ma et al.\nwhere X \u2208Rm\u00d7n and C is a convex set. This model has many applications such as determining a low-order con-\ntroller for a plant [21] and a minimum order linear system realization [19], and solving low-dimensional Euclidean\nembedding problems [28].\nIn this paper, we are interested in methods for solving the af\ufb01nely constrained matrix rank minimization problem\nmin rank(X)\ns.t. A (X) = b,\n(1.1)\nwhere X \u2208Rm\u00d7n is the decision variable, and the linear map A : Rm\u00d7n \u2192Rp and vector b \u2208Rp are given.\nThe matrix completion problem\nmin rank(X)\ns.t. Xij = Mij,(i, j) \u2208\u2126\n(1.2)\nis a special case of (1.1), where X and M are both m \u00d7 n matrices and \u2126is a subset of index pairs (i, j). The so\ncalled collaborative \ufb01ltering problem [33; 36] can be cast as a matrix completion problem. Suppose users in an\nonline survey provide ratings of some movies. This yields a matrix M with users as rows and movies as columns\nwhose (i, j)-th entry Mij is the rating given by the i-th user to the j-th movie. Since most users rate only a small\nportion of the movies, we typically only know a small subset {Mij|(i, j) \u2208\u2126} of the entries. Based on the known\nratings of a user, we want to predict the user\u2019s ratings of the movies that the user did not rate; i.e., we want to \ufb01ll in\nthe missing entries of the matrix. It is commonly believed that only a few factors contribute to an individual\u2019s tastes\nor preferences for movies. Thus the rating matrix M is likely to be of numerical low rank in the sense that relatively\nfew of the top singular values account for most of the sum of all of the singular values. Finding such a low-rank\nmatrix M corresponds to solving the matrix completion problem (1.2).\n1.1 Connections to compressed sensing\nWhen the matrix X is diagonal, problem (1.1) reduces to the cardinality minimization problem\nmin \u2225x\u22250\ns.t. Ax = b,\n(1.3)\nwhere x \u2208Rn,A \u2208Rm\u00d7n,b \u2208Rm and \u2225x\u22250 denotes the number of nonzeros in the vector x. This problem \ufb01nds\nthe sparsest solution to an underdetermined system of equations and has a wide range of applications in signal\nprocessing. This problem is NP-hard [30]. To get a more computationally tractable problem, we can replace \u2225x\u22250\nby its convex envelope.\nDe\ufb01nition 1 The convex envelope of a function f : C \u2192R is de\ufb01ned as the largest convex function g such that\ng(x) \u2264f(x) for all x \u2208C (see e.g., [25]).\nIt is well known that the convex envelope of \u2225x\u22250 is \u2225x\u22251, the \u21131 norm of x, which is the sum of the absolute\nvalues of all components of x. Replacing the objective function \u2225x\u22250 in (1.3) by \u2225x\u22251 yields the so-called basis\npursuit problem\nmin \u2225x\u22251\ns.t. Ax = b.\n(1.4)\nThe basis pursuit problem has received an increasing amount of attention since the emergence of the \ufb01eld of com-\npressed sensing (CS) [11; 14]. Compressed sensing theories connect the NP-hard problem (1.3) to the convex and\nFixed point and Bregman iterative methods for matrix rank minimization\n3\ncomputationally tractable problem (1.4) and provide guarantees for when an optimal solution to (1.4) gives an op-\ntimal solution to (1.3). In the cardinality minimization and basis pursuit problems (1.3) and (1.4), b is a vector of\nmeasurements of the signal x obtained using the sampling matrix A. The main result of compressed sensing is that\nwhen the signal x is sparse, i.e., k := \u2225x\u22250 \u226an, we can recover the signal by solving (1.4) with a very limited num-\nber of measurements, i.e., m \u226an, when A is a Gaussian random matrix or when it corresponds to a partial Fourier\ntransformation. Note that if b is contaminated by noise, the constraint Ax = b in (1.4) must be relaxed, resulting in\neither the problem\nmin \u2225x\u22251\ns.t. \u2225Ax\u2212b\u22252 \u2264\u03b8\n(1.5)\nor its Lagrangian version\nmin\u00b5\u2225x\u22251 + 1\n2\u2225Ax\u2212b\u22252\n2,\n(1.6)\nwhere \u03b8 and \u00b5 are parameters and \u2225x\u22252 denotes the Euclidean norm of a vector x.. Algorithms for solving (1.4)\nand its variants (1.5) and (1.6) have been widely investigated and many algorithms have been suggested including\nconvex optimization methods ([2; 10; 20; 24; 27]) and heuristic methods ([13; 15; 16; 39; 40]).\n1.2 Nuclear norm minimization\nThe rank of a matrix is the number of its positive singular values. The matrix rank minimization (1.1) is NP-hard\nin general due to the combinational nature of the function rank(\u00b7). Similar to the cardinality function \u2225x\u22250, we can\nreplace rank(X) by its convex envelope to get a convex and more computationally tractable approximation to (1.1).\nIt turns out that the convex envelope of rank(X) on the set {X \u2208Rm\u00d7n : \u2225X\u22252 \u22641} is the nuclear norm \u2225X\u2225\u2217[18],\ni.e., the nuclear norm is the best convex approximation of the rank function over the unit ball of matrices with norm\nless than one, where \u2225X\u22252 is the operator norm of X. The nuclear norm and operator norm are de\ufb01ned as follows.\nDe\ufb01nition 2 Nuclear norm and Operator norm. Assume that the matrix X has r positive singular values of \u03c31 \u2265\n\u03c32 \u2265... \u2265\u03c3r > 0. The nuclear norm of X is de\ufb01ned as the sum of its singular values, i.e.,\n\u2225X\u2225\u2217:=\nr\n\u2211\ni=1\n\u03c3i(X).\nThe operator norm of matrix X is de\ufb01ned as the largest singular value of X, i.e.,\n\u2225X\u22252 := \u03c31(X).\nThe nuclear norm is also known as Schatten 1-norm or Ky Fan norm. Using it as an approximation to rank(X)\nin (1.1) yields the nuclear norm minimization problem\nmin \u2225X\u2225\u2217\ns.t. A (X) = b.\n(1.7)\nAs in the basis pursuit problem, if b is contaminated by noise, the constraint A (X) = b must be relaxed, resulting\nin either the problem\nmin \u2225X\u2225\u2217\ns.t. \u2225A (X)\u2212b\u22252 \u2264\u03b8\n4\nShiqian Ma et al.\nor its Lagrangian version\nmin\u00b5\u2225X\u2225\u2217+ 1\n2\u2225A (X)\u2212b\u22252\n2,\n(1.8)\nwhere \u03b8 and \u00b5 are parameters.\nNote that if we write X in vector form by stacking the columns of X in a single vector vec(X) \u2208Rmn, then we\nget the following equivalent formation of (1.7):\nmin \u2225X\u2225\u2217\ns.t. A vec(X) = b,\n(1.9)\nwhere A \u2208Rp\u00d7mn is the matrix corresponding to the linear map A . An important question is: when will an optimal\nsolution to the nuclear norm minimization problem (1.7) give an optimal solution to matrix rank minimization\nproblem (1.1). In response to this question, Recht et al. [32] proved that if the entries of A are suitably random,\ne.g., i.i.d. Gaussian, then with very high probability, most m \u00d7 n matrices of rank r can be recovered by solving\nthe nuclear norm minimization (1.7) or equivalently, (1.9), whenever p \u2265Cr(m+n)log(mn), where C is a positive\nconstant.\nFor the matrix completion problem (1.2), the corresponding nuclear norm minimization problem is\nmin \u2225X\u2225\u2217\ns.t. Xij = Mij,(i, j) \u2208\u2126.\n(1.10)\nCand`es et al. [9] proved the following result.\nTheorem 1 Let M be an n1 \u00d7n2 matrix of rank r with SVD\nM =\nr\n\u2211\nk=1\n\u03c3kukv\u22a4\nk ,\nwhere the family {uk}1\u2264k\u2264r is selected uniformly at random among all families of r orthonormal vectors, and\nsimilarly for the family {vk}1\u2264k\u2264r. Let n = max(n1,n2). Suppose we observe m entries of M with locations sampled\nuniformly at random. Then there are constants C and c such that if\nm \u2265Cn5/4rlogn,\nthe minimizer to the problem (1.10) is unique and equal to M with probability at least 1 \u2212cn\u22123. In addition, if\nr \u2264n1/5, then the recovery is exact with probability at least 1\u2212cn\u22123 provided that\nm \u2265Cn6/5rlogn.\nThis theorem states that a surprisingly small number of entries are suf\ufb01cient to complete a low-rank matrix with\nhigh probability.\nRecently, this result was strengthened by Cand`es and Tao in [12], where it is proved that under certain incoher-\nence conditions, the number of samples m that are required is only O(nrlogn).\nThe dual problem corresponding to the nuclear norm minimization problem (1.7) is\nmax b\u22a4z\ns.t. \u2225A \u2217(z)\u22252 \u22641,\n(1.11)\nFixed point and Bregman iterative methods for matrix rank minimization\n5\nwhere A \u2217is the adjoint operator of A . Both (1.7) and (1.11) can be rewritten as equivalent semide\ufb01nite program-\nming (SDP) problems. The SDP formulation of (1.7) is:\nmin\nX,W1,W2\n1\n2(Tr(W1)+Tr(W2))\ns.t.\n\"\nW1 X\nX\u22a4W2\n#\n\u2ab00\nA (X) = b,\n(1.12)\nwhere Tr(X) denotes the trace of the square matrix X. The SDP formulation of (1.11) is:\nmax\nz\nb\u22a4z\ns.t.\n\"\nIm\nA \u2217(z)\nA \u2217(z)\u22a4\nIn\n#\n\u2ab00.\n(1.13)\nThus to solve (1.12) and (1.13), we can use SDP solvers such as SeDuMi [38] and SDPT3 [42] to solve (1.12)\nand (1.13). Note that the number of variables in (1.12) is 1\n2(m+n)(m+n+1). SDP solvers cannot usually solve a\nproblem when m and n are both much larger than 100.\nRecently, Liu and Vandenberghe [29] proposed an interior-point method for another nuclear norm approximation\nproblem\nmin\u2225A (x)\u2212B\u2225\u2217,\n(1.14)\nwhere B \u2208Rm\u00d7n and\nA (x) = x1A1 +x2A2 +\u00b7\u00b7\u00b7+xpAp\nis a linear mapping from Rp to Rm\u00d7n. The equivalent SDP formulation of (1.14) is\nmin\nx,W1,W2\n1\n2(Tr(W1)+Tr(W2))\ns.t.\n\"\nW1\n(A (x)\u2212B)\u22a4\nA (x)\u2212B\nW2\n#\n\u2ab00.\n(1.15)\nLiu and Vandenberghe [29] proposed a customized method for computing the scaling direction in an interior point\nmethod for solving the SDP (1.15). The complexity of each iteration in their method was reduced from O(p6) to\nO(p4) when m = O(p) and n = O(p); thus they were able to solve problems up to dimension m = n = 350.\nAnother algorithm for solving (1.7) is due to Burer and Monteiro [6; 7], (see also Rennie and Srebro [33; 36]).\nThis algorithm uses the low-rank factorization X = LR\u22a4of the matrix X \u2208Rm\u00d7n, where L \u2208Rm\u00d7r,R \u2208Rn\u00d7r,r \u2264\nmin{m,n}, and solves the optimization problem\nmin\nL,R\n1\n2(\u2225L\u22252\nF +\u2225R\u22252\nF)\ns.t. A (LR\u22a4) = b,\n(1.16)\nwhere \u2225X\u2225F denotes the Frobenius norm of the matrix X:\n\u2225X\u2225F := (\nr\n\u2211\ni=1\n\u03c32\ni )1/2 = (\u2211\ni,j\nX2\nij)1/2 = (Tr(XX\u22a4))1/2.\nIt is known that as long as r is chosen to be suf\ufb01ciently larger than the rank of the optimal solution matrix of\nthe nuclear norm problem (1.7), this low-rank factorization problem is equivalent to the nuclear norm problem (1.7)\n6\nShiqian Ma et al.\n(see e.g., [32]). The advantage of this low-rank factorization formulation is that both the objective function and the\nconstraints are differentiable. Thus gradient-based optimization algorithms such as conjugate gradient algorithms\nand augmented Lagrangian algorithms can be used to solve this problem. However, the constraints in this problem\nare nonconvex, so one can only be assured of obtaining a local minimizer. Also, how to choose r is still an open\nquestion.\nOne very interesting algorithm is the so called singular value thresholding algorithm (SVT) [8] which appeared\nalmost simultaneously with our work. SVT is inspired by the linearized Bregman algorithms for compressed sensing\nand \u21131-regularized problems. In [8] it is shown that SVT is ef\ufb01cient for large matrix completion problems. However,\nSVT only works well for very low rank matrix completion problems. For problems where the matrices are not of\nvery low rank, SVT is slow and not robust therefore often fails.\nOur algorithms have some similarity with the SVT algorithm in that they make use of matrix shrinkage (see\nSection 2). However, other than that, they are greatly different. All of our methods are based on a \ufb01xed point\ncontinuation (FPC) algorithm which uses an operator splitting technique for solving (1.8). By adopting a Monte\nCarlo approximate SVD in the FPC, we get an algorithm, which we call FPCA (Fixed Point Continuation with\nApproximate SVD), that usually gets the optimal solution to (1.1) even if the condition of Theorem 1, or those for\nthe af\ufb01ne constrained case, are violated. Moreover, our algorithm is much faster than state-of-the-art SDP solvers\nsuch as SDPT3 applied to (1.12). Also, FPCA can recover matrices of moderate rank that cannot be recovered by\nSDPT3, SVT, etc. with the same amount of samples. For example, for matrices of size 1000 \u00d7 1000 and rank 50,\nFPCA can recover them with a relative error of 10\u22125 in about 3 minutes by sampling only 20 percent of the matrix\nelements. As far as we know, there is no other method that has as good a recoverability property.\n1.3 Outline and Notation\nOutline. The rest of this paper is organized as follows. In Section 2 we review the \ufb01xed-point continuation\nalgorithm for \u21131-regularized problems. In Section 3 we give an analogous \ufb01xed-point iterative algorithm for the\nnuclear norm minimization problem and prove that it converges to an optimal solution. In Section 4 we discuss\na continuation technique for accelerating the convergence of our algorithm. In Section 5 we propose a Bregman\niterative algorithm for nuclear norm minimization extending the approach in [44] for compressed sensing to the\nrank minimization problem. In Section 6 we incorporate a Monte-Carlo approximate SVD procedure into our \ufb01xed-\npoint continuation algorithm to speed it up and improve its ability to recover low-rank matrices. Numerical results\nfor both synthesized matrices and real problems are given in Section 7. We give conclusions in Section 8.\nNotation. Throughout this paper, we always assume that the singular values are arranged in nonincreasing\norder, i.e., \u03c31 \u2265\u03c32 \u2265... \u2265\u03c3r > 0 = \u03c3r+1 = ... = \u03c3min{m,n}. \u2202f denotes the subdifferential of the function f and\ngk = g(Xk) = A \u2217(A (Xk) \u2212b)) is the gradient of function 1\n2\u2225A (X) \u2212b\u22252\n2 at the point Xk. Diag(s) denotes the\ndiagonal matrix whose diagonal elements are the elements of the vector s. sgn(t) is the signum function of t \u2208R,\ni.e.,\nsgn(t) :=\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\n+1 if t > 0,\n0\nif t = 0,\n\u22121 if t < 0,\nwhile the signum multifunction of t \u2208R is\nSGN(t) := \u2202|t| =\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\n{+1}\nif t > 0,\n[\u22121,1] if t = 0,\n{\u22121}\nif t < 0.\nFixed point and Bregman iterative methods for matrix rank minimization\n7\nWe use a\u2299b to denote the elementwise multiplication of two vectors a and b. We use X(k : l) to denote the submatrix\nof X consisting of the k-th to l-th column of X. We use Rn\n+ to denote the nonnegative orthant in Rn.\n2 Fixed point iterative algorithm\nOur \ufb01xed point iterative algorithm for solving (1.8) is the following simple two-line algorithm:\n(\nY k = Xk \u2212\u03c4g(Xk)\nXk+1 = S\u03c4\u00b5(Y k),\n(2.1)\nwhere S\u03bd(\u00b7) is the matrix shrinkage operator which will be de\ufb01ned later.\nOur algorithm (2.1) is inspired by the \ufb01xed point iterative algorithm proposed in [24] for the \u21131-regularized\nproblem (1.6). The idea behind this algorithm is an operator splitting technique. Note that x\u2217is an optimal solution\nto (1.6) if and only if\n0 \u2208\u00b5SGN(x\u2217)+g\u2217,\n(2.2)\nwhere g\u2217= A\u22a4(Ax\u2217\u2212b). For any \u03c4 > 0, (2.2) is equivalent to\n0 \u2208\u03c4\u00b5SGN(x\u2217)+\u03c4g(x\u2217).\n(2.3)\nNote that the operator T(\u00b7) := \u03c4\u00b5SGN(\u00b7)+\u03c4g(\u00b7) on the right hand side of (2.3) can be split into two parts: T(\u00b7) =\nT1(\u00b7)\u2212T2(\u00b7), where T1(\u00b7) = \u03c4\u00b5SGN(\u00b7)+I(\u00b7) and T2(\u00b7) = I(\u00b7)\u2212\u03c4g(\u00b7).\nLetting y = T2(x\u2217) = x\u2217\u2212\u03c4A\u22a4(Ax\u2217\u2212b), (2.3) is equivalent to\n0 \u2208T1(x\u2217)\u2212y = \u03c4\u00b5SGN(x\u2217)+x\u2217\u2212y.\n(2.4)\nNote that (2.4) is actually the optimality conditions for the following convex problem\nmin\nx\u2217\u03c4\u00b5\u2225x\u2217\u22251 + 1\n2\u2225x\u2217\u2212y\u22252\n2.\n(2.5)\nThis problem has a closed form optimal solution given by the so called shrinkage operator:\nx\u2217= \u02dcs\u03bd(y),\nwhere \u03bd = \u03c4\u00b5, and shrinkage operator \u02dcs\u03bd(\u00b7) is given by\n\u02dcs\u03bd(\u00b7) = sgn(\u00b7)\u2299max{|\u00b7|\u2212\u03bd,0}.\n(2.6)\nThus, the \ufb01xed point iterative algorithm is given by\nxk+1 = \u02dcs\u03c4\u00b5(xk \u2212\u03c4gk).\n(2.7)\nHale et al. [24] proved global and \ufb01nite convergence of this algorithm to the optimal solution of the \u21131-regularized\nproblem (1.6).\nMotivated by this work, we develop a \ufb01xed point iterative algorithm for (1.8). Since the objective function in\n(1.8) is convex, X\u2217is the optimal solution to (1.8) if and only if\n0 \u2208\u00b5\u2202\u2225X\u2217\u2225\u2217+g(X\u2217),\n(2.8)\n8\nShiqian Ma et al.\nwhere g(X\u2217) = A \u2217(A (X\u2217)\u2212b). Note that if the Singular Value Decomposition (SVD) of X is X = U\u03a3V \u22a4, where\nU \u2208Rm\u00d7r,\u03a3 = Diag(\u03c3) \u2208Rr\u00d7r,V \u2208Rn\u00d7r, then (see e.g., [1; 4])\n\u2202\u2225X\u2225\u2217= {UV \u22a4+W : U\u22a4W = 0,WV = 0,\u2225W\u2225\u22641}.\nHence, we get the following optimality conditions for (1.8):\nTheorem 2 The matrix X \u2208Rm\u00d7n with singular value decomposition X = U\u03a3V \u22a4, U \u2208Rm\u00d7r,\u03a3 = Diag(\u03c3) \u2208\nRr\u00d7r,V \u2208Rn\u00d7r, is optimal for the problem (1.8) if and only if there exists a matrix W \u2208Rm\u00d7n such that\n\u00b5(UV\u22a4+W)+g(X) = 0,\n(2.9a)\nU\u22a4W = 0,WV = 0,\u2225W\u22252 \u22641.\n(2.9b)\nNow based on the optimality conditions (2.8), we can develop a \ufb01xed point iterative scheme for solving (1.8) by\nadopting the operator splitting technique described at the beginning of this section. Note that (2.8) is equivalent to\n0 \u2208\u03c4\u00b5\u2202\u2225X\u2217\u2225\u2217+X\u2217\u2212(X\u2217\u2212\u03c4g(X\u2217))\n(2.10)\nfor any \u03c4 > 0. If we let\nY \u2217= X\u2217\u2212\u03c4g(X\u2217),\nthen (2.10) is reduced to\n0 \u2208\u03c4\u00b5\u2202\u2225X\u2217\u2225\u2217+X\u2217\u2212Y \u2217,\n(2.11)\ni.e., X\u2217is the optimal solution to\nmin\nX\u2208Rm\u00d7n \u03c4\u00b5\u2225X\u2225\u2217+ 1\n2\u2225X \u2212Y \u2217\u22252\nF\n(2.12)\nIn the following we will prove that the matrix shrinkage operator applied to Y \u2217gives the optimal solution to\n(2.12). First, we need the following de\ufb01nitions.\nDe\ufb01nition 3 (Nonnegative Vector Shrinkage Operator) Assume x \u2208Rn\n+. For any \u03bd > 0, the nonnegative vector\nshrinkage operator s\u03bd(\u00b7) is de\ufb01ned as\ns\u03bd(x) := \u00afx, with \u00afxi =\n(\nxi \u2212\u03bd, if xi \u2212\u03bd > 0\n0,\no.w.\nDe\ufb01nition 4 (Matrix Shrinkage Operator) Assume X \u2208Rm\u00d7n and the SVD of X is given by X = UDiag(\u03c3)V \u22a4,\nU \u2208Rm\u00d7r,\u03c3 \u2208Rr\n+,V \u2208Rn\u00d7r. For any \u03bd > 0, the matrix shrinkage operator S\u03bd(\u00b7) is de\ufb01ned as\nS\u03bd(X) := UDiag( \u00af\u03c3)V \u22a4,\nwith \u00af\u03c3 = s\u03bd(\u03c3).\nTheorem 3 Given a matrix Y \u2208Rm\u00d7n with rank(Y) = t, let its Singular Value Decomposition (SVD) be Y =\nUYDiag(\u03b3)V \u22a4\nY , where UY \u2208Rm\u00d7t,\u03b3 \u2208Rt\n+,VY \u2208Rn\u00d7t, and a scalar \u03bd > 0. Then\nX := S\u03bd(Y) = UYDiag(s\u03bd(\u03b3))V\u22a4\nY\n(2.13)\nFixed point and Bregman iterative methods for matrix rank minimization\n9\nis an optimal solution of the problem\nmin\nX\u2208Rm\u00d7n f(X) := \u03bd\u2225X\u2225\u2217+ 1\n2\u2225X \u2212Y\u22252\nF.\n(2.14)\nProof Without loss of generality, we assume m \u2264n. Suppose that the solution X \u2208Rm\u00d7n to problem (2.14) has the\nSVD X = UDiag(\u03c3)V \u22a4, where U \u2208Rm\u00d7r,\u03c3 \u2208Rr\n+,V \u2208Rn\u00d7r. Hence, X must satisfy the optimality conditions for\n(2.14) which are\n0 \u2208\u03bd\u2202\u2225X\u2225\u2217+X \u2212Y;\ni.e., there exists a matrix\nW = \u00afU\nh\nDiag( \u00af\u03c3) 0\ni\n\u00afV \u22a4,\nwhere \u00afU \u2208Rm\u00d7(m\u2212r), \u00afV \u2208Rn\u00d7(n\u2212r), \u00af\u03c3 \u2208Rm\u2212r\n+\n, \u2225\u00af\u03c3\u2225\u221e\u22641 and both \u02c6U = [U, \u00afU] and \u02c6V = [V, \u00afV] are orthogonal matri-\nces, such that\n0 = \u03bd(UV \u22a4+W)+X \u2212Y.\n(2.15)\nHence,\n\u02c6U\n\"\n\u03bdI +Diag(\u03c3)\n0\n0\n0\n\u03bdDiag( \u00af\u03c3) 0\n#\n\u02c6V \u22a4\u2212UYDiag(\u03b3)V \u22a4\nY = 0.\n(2.16)\nTo verify that (2.13) satis\ufb01es (2.16), consider the following two cases:\nCase 1: \u03b31 \u2265\u03b32 \u2265... \u2265\u03b3t > \u03bd. In this case, choosing X as above, with r = t,U = UY,V = VY and \u03c3 = s\u03bd(\u03b3) =\n\u03b3 \u2212\u03bde, where e is a vector of r ones, and choosing \u00af\u03c3 = 0 (i.e., W = 0) satis\ufb01es (2.16).\nCase 2: \u03b31 \u2265\u03b32 \u2265... \u2265\u03b3k > \u03bd \u2265\u03b3k+1 \u2265... \u2265\u03b3t. In this case, by choosing r = k, \u02c6U(1 : t) = UY, \u02c6V(1 : t) =\nVY,\u03c3 = s\u03bd((\u03b31,...,\u03b3k)) and \u00af\u03c31 = \u03b3k+1/\u03bd,..., \u00af\u03c3t\u2212k = \u03b3t/\u03bd, \u00af\u03c3t\u2212k+1 = ... = \u00af\u03c3m\u2212r = 0, X and W satisfy (2.16).\nNote that in both cases, X can be written as the form in (2.13) based on the way we construct X.\n\u2293\u2294\nBased on the above we obtain the \ufb01xed point iterative scheme (2.1) stated at the beginning of this section for\nsolving problem (1.8).\nMoreover, from the discussion following Theorem 2 we have\nCorollary 1 X\u2217is an optimal solution to problem (1.8) if and only if X\u2217= S\u03c4\u00b5(h(X\u2217)), where h(\u00b7) = I(\u00b7)\u2212\u03c4g(\u00b7).\n3 Convergence results\nIn this section, we analyze the convergence properties of the \ufb01xed point iterative scheme (2.1). Before we prove the\nmain convergence result, we need some lemmas.\nLemma 1 The shrinkage operator S\u03bd is non-expansive, i.e., for any Y1 and Y2 \u2208Rm\u00d7n,\n\u2225S\u03bd(Y1)\u2212S\u03bd(Y2)\u2225F \u2264\u2225Y1 \u2212Y2\u2225F.\n(3.1)\nMoreover,\n\u2225Y1 \u2212Y2\u2225F = \u2225S\u03bd(Y1)\u2212S\u03bd(Y2)\u2225F \u21d0\u21d2Y1 \u2212Y2 = S\u03bd(Y1)\u2212S\u03bd(Y2).\n(3.2)\n10\nShiqian Ma et al.\nProof Without loss of generality, we assume m \u2264n. Assume SVDs of Y1 and Y2 are Y1 = U1\u03a3V \u22a4\n1 and Y2 = U2\u0393V \u22a4\n2 ,\nrespectively, where\n\u03a3 =\n \nDiag(\u03c3) 0\n0\n0\n!\n\u2208Rm\u00d7n,\u0393 =\n \nDiag(\u03b3) 0\n0\n0\n!\n\u2208Rm\u00d7n,\n\u03c3 = (\u03c31,...,\u03c3s),\u03c31 \u2265... \u2265\u03c3s > 0 and \u03b3 = (\u03b31,...,\u03b3t),\u03b31 \u2265... \u2265\u03b3t > 0. Note that here U1,V1,U2 and V2 are (full)\northogonal matrices; \u03a3,\u0393 \u2208Rm\u00d7n. Suppose that \u03c31 \u2265... \u2265\u03c3k \u2265\u03bd > \u03c3k+1 \u2265... \u2265\u03c3s and \u03b31 \u2265... \u2265\u03b3l \u2265\u03bd >\n\u03b3l+1 \u2265... \u2265\u03b3t, then\n\u00afY1 := S\u03bd(Y1) = U1 \u00af\u03a3V \u22a4\n1 , \u00afY2 := S\u03bd(Y2) = U2 \u00af\u0393V \u22a4\n2 ,\nwhere\n\u00af\u03a3 =\n \nDiag( \u00af\u03c3) 0\n0\n0\n!\n\u2208Rm\u00d7n, \u00af\u0393 =\n \nDiag( \u00af\u03b3) 0\n0\n0\n!\n\u2208Rm\u00d7n,\n\u00af\u03c3 = (\u03c31 \u2212\u03bd,...,\u03c3k \u2212\u03bd) and \u00af\u03b3 = (\u03b31 \u2212\u03bd,...,\u03b3l \u2212\u03bd). Thus,\n\u2225Y1 \u2212Y2\u22252\nF \u2212\u2225\u00afY1 \u2212\u00afY2\u22252\nF = Tr((Y1 \u2212Y2)\u22a4(Y1 \u2212Y2))\u2212Tr(( \u00afY1 \u2212\u00afY2)\u22a4( \u00afY1 \u2212\u00afY2))\n= Tr(Y \u22a4\n1 Y1 \u2212\u00afY \u22a4\n1 \u00afY1 +Y \u22a4\n2 Y2 \u2212\u00afY \u22a4\n2 \u00afY2)\u22122Tr(Y \u22a4\n1 Y2 \u2212\u00afY \u22a4\n1 \u00afY2)\n=\ns\n\u2211\ni=1\n\u03c32\ni \u2212\nk\n\u2211\ni=1\n(\u03c3i \u2212\u03bd)2 +\nt\n\u2211\ni=1\n\u03b32\ni \u2212\nl\n\u2211\ni=1\n(\u03b3i \u2212\u03bd)2 \u22122Tr(Y \u22a4\n1 Y2 \u2212\u00afY \u22a4\n1 \u00afY2)\nWe note that\nTr(Y \u22a4\n1 Y2 \u2212\u00afY \u22a4\n1 \u00afY2) = Tr((Y1 \u2212\u00afY1)\u22a4(Y2 \u2212\u00afY2)+(Y1 \u2212\u00afY1)\u22a4\u00afY2 + \u00afY1\n\u22a4(Y2 \u2212\u00afY2))\n= Tr(V1(\u03a3 \u2212\u00af\u03a3)\u22a4U\u22a4\n1 U2(\u0393 \u2212\u00af\u0393 )V \u22a4\n2 +V1(\u03a3 \u2212\u00af\u03a3)\u22a4U\u22a4\n1 U2 \u00af\u0393V \u22a4\n2 +V1 \u00af\u03a3\u22a4U\u22a4\n1 U2(\u0393 \u2212\u00af\u0393 )V \u22a4\n2\n= Tr((\u03a3 \u2212\u00af\u03a3)\u22a4U(\u0393 \u2212\u00af\u0393 )V \u22a4+(\u03a3 \u2212\u00af\u03a3)\u22a4U \u00af\u0393V \u22a4+ \u00af\u03a3\u22a4U(\u0393 \u2212\u00af\u0393 )V \u22a4),\nwhere U = U\u22a4\n1 U2,V = V \u22a4\n1 V2 are clearly orthogonal matrices. Now let us derive an upper bound for Tr(Y \u22a4\n1 Y2 \u2212\n\u00afY \u22a4\n1 \u00afY2). It is known that an orthogonal matrix U is a maximizing matrix for the problem\nmax{Tr(AU) : U is orthogonal}\nif and only if AU is positive semide\ufb01nite matrix (see 7.4.9 in [26]). It is also known that when AB is positive\nsemide\ufb01nite,\nTr(AB) =\u2211\ni\n\u03c3i(AB) \u2264\u2211\ni\n\u03c3i(A)\u03c3i(B).\n(3.3)\nThus, Tr((\u03a3 \u2212\u00af\u03a3)\u22a4U(\u0393 \u2212\u00af\u0393 )V \u22a4), Tr((\u03a3 \u2212\u00af\u03a3)\u22a4U \u00af\u0393V \u22a4) and Tr( \u00af\u03a3U(\u0393 \u2212\u00af\u0393 )V \u22a4) achieve their maximum, if and only\nif (\u03a3 \u2212\u00af\u03a3)\u22a4U(\u0393 \u2212\u00af\u0393 )V \u22a4, (\u03a3 \u2212\u00af\u03a3)\u22a4U \u00af\u0393V \u22a4and \u00af\u03a3U(\u0393 \u2212\u00af\u0393 )V \u22a4are all positive semide\ufb01nite. Applying (3.3) to these\nthree terms, we get Tr((\u03a3 \u2212\u00af\u03a3)\u22a4U(\u0393 \u2212\u00af\u0393 )V \u22a4) \u2264\u2211i \u03c3i(\u03a3 \u2212\u00af\u03a3)\u03c3i(\u0393 \u2212\u00af\u0393 ), Tr((\u03a3 \u2212\u00af\u03a3)\u22a4U \u00af\u0393V \u22a4) \u2264\u2211i \u03c3i(\u03a3 \u2212\u00af\u03a3)\u03c3i( \u00af\u0393 )\nand Tr( \u00af\u03a3U(\u0393 \u2212\u00af\u0393 )V \u22a4) \u2264\u2211i \u03c3i( \u00af\u03a3)\u03c3i(\u0393 \u2212\u00af\u0393 ). Thus, without loss of generality, assuming k \u2264l \u2264s \u2264t, we have,\n\u2225Y1 \u2212Y2\u22252\nF \u2212\u2225S\u03bd(Y1)\u2212S\u03bd(Y2)\u22252\nF\n\u2265\ns\n\u2211\ni=1\n\u03c32\ni \u2212\nk\n\u2211\ni=1\n(\u03c3i \u2212\u03bd)2 +\nt\n\u2211\ni=1\n\u03b32\ni \u2212\nl\n\u2211\ni=1\n(\u03b3i \u2212\u03bd)2 \u22122(\nl\n\u2211\ni=1\n\u03c3i\u03bd +\ns\n\u2211\ni=l+1\n\u03c3i\u03b3i +\nk\n\u2211\ni=1\n(\u03b3i \u2212\u03bd)\u03bd +\nl\n\u2211\ni=k+1\n\u03c3i(\u03b3i \u2212\u03bd))\n=\nl\n\u2211\ni=k+1\n(2\u03b3i\u03bd \u2212\u03bd2 +\u03c32\ni \u22122\u03c3i\u03b3i)+(\ns\n\u2211\ni=l+1\n\u03c32\ni +\nt\n\u2211\ni=l+1\n\u03b32\ni \u2212\ns\n\u2211\ni=l+1\n2\u03c3i\u03b3i).\nFixed point and Bregman iterative methods for matrix rank minimization\n11\nNow\ns\n\u2211\ni=l+1\n\u03c32\ni +\nt\n\u2211\ni=l+1\n\u03b32\ni \u2212\ns\n\u2211\ni=l+1\n2\u03c3i\u03b3i \u22650\nsince t \u2265s and \u03c32\ni +\u03b32\ni \u22122\u03c3i\u03b3i \u22650. Also, since the function f(x) := 2\u03b3ix\u2212x2 is monotonely increasing in (\u2212\u221e,\u03b3i]\nand \u03c3i < \u03bd \u2264\u03b3i,i = k +1,...,l,\n2\u03b3i\u03bd \u2212\u03bd2 +\u03c32\ni \u22122\u03c3i\u03b3i > 0,i = k +1,...,l.\nThus we get\nD(Y1,Y2) := \u2225Y1 \u2212Y2\u22252\nF \u2212\u2225S\u03bd(Y1)\u2212S\u03bd(Y2)\u22252\nF \u22650;\ni.e., (3.1) holds.\nAlso, D(Y1,Y2) achieves its minimum value if and only if Tr((\u03a3 \u2212\u00af\u03a3)\u22a4U(\u0393 \u2212\u00af\u0393 )V \u22a4), Tr((\u03a3 \u2212\u00af\u03a3)\u22a4U \u00af\u0393V \u22a4) and\nTr( \u00af\u03a3U(\u0393 \u2212\u00af\u0393 )V \u22a4) achieve their maximum values simultaneously.\nFurthermore, if equality in (3.1) holds, i.e., D(Y1,Y2) achieves its minimum, and its minimum is zero, then k = l,\ns = t, and \u03c3i = \u03b3i,i = k+1,...,s, which further implies \u03a3 \u2212\u00af\u03a3 = \u0393 \u2212\u00af\u0393 and Tr((\u03a3 \u2212\u00af\u03a3)\u22a4U(\u0393 \u2212\u00af\u0393 )V \u22a4) achieves its\nmaximum. By applying the result 7.4.13 in [26], we get\n\u03a3 \u2212\u00af\u03a3 = U(\u0393 \u2212\u00af\u0393 )V \u22a4,\nwhich further implies that\nY1 \u2212Y2 = S\u03bd(Y1)\u2212S\u03bd(Y2).\n(3.4)\nTo conclude, clearly \u2225S\u03bd(Y1)\u2212S\u03bd(Y2)\u2225F = \u2225Y1 \u2212Y2\u2225F if (3.4) holds.\n\u2293\u2294\nThe following two lemmas and theorem and their proofs are analogous to results and their proofs in Hale et al.\n[24].\nLemma 2 Let A X = Avec(X) and assume that \u03c4 \u2208(0,2/\u03bbmax(A\u22a4A)). Then the operator h(\u00b7) = I(\u00b7)\u2212\u03c4g(\u00b7) is non-\nexpansive, i.e., \u2225h(X)\u2212h(X\u2032)\u2225F \u2264\u2225X \u2212X\u2032\u2225F. Moreover, h(X)\u2212h(X\u2032) = X \u2212X\u2032 if and only if \u2225h(X)\u2212h(X\u2032)\u2225F =\n\u2225X \u2212X\u2032\u2225F.\nProof First, we note that since \u03c4 \u2208(0,2/\u03bbmax(A\u22a4A)), \u22121 < \u03bbi(I \u2212\u03c4A\u22a4A) \u22641,\u2200i, where \u03bbi(I \u2212\u03c4A\u22a4A) is the i-th\neigenvalue of I \u2212\u03c4A\u22a4A. Hence,\n\u2225h(X)\u2212h(X\u2032)\u2225F = \u2225(I \u2212\u03c4A\u22a4A)(vec(X)\u2212vec(X\u2032))\u22252 \u2264\u2225I \u2212\u03c4A\u22a4A\u22252\u2225vec(X)\u2212vec(X\u2032)\u22252\n\u2264\u2225vec(X)\u2212vec(X\u2032)\u22252 = \u2225X \u2212X\u2032\u2225F.\nMoreover, \u2225h(X)\u2212h(X\u2032)\u2225F = \u2225X \u2212X\u2032\u2225F if and only if the inequalities above are equalities, which happens if and\nonly if\n(I \u2212\u03c4A\u22a4A)(vec(X)\u2212vec(X\u2032)) = vec(X)\u2212vec(X\u2032),\ni.e., if and only if h(X)\u2212h(X\u2032) = X \u2212X\u2032.\n\u2293\u2294\nLemma 3 Let X\u2217be an optimal solution to problem (1.8), \u03c4 \u2208(0,2/\u03bbmax(A\u22a4A)) and \u03bd = \u03c4\u00b5. Then X is also an\noptimal solution to problem (1.8) if and only if\n\u2225S\u03bd(h(X))\u2212S\u03bd(h(X\u2217))\u2225F \u2261\u2225S\u03bd(h(X))\u2212X\u2217\u2225F = \u2225X \u2212X\u2217\u2225F.\n(3.5)\n12\nShiqian Ma et al.\nProof The \u201conly if\u201d part is an immediate consequence of Corollary 1. For the \u201cif\u201d part, from Lemmas 1 and 2,\n\u2225X \u2212X\u2217\u2225F = \u2225S\u03bd(h(X))\u2212S\u03bd(h(X\u2217))\u2225F \u2264\u2225h(X)\u2212h(X\u2217)\u2225F \u2264\u2225X \u2212X\u2217\u2225F.\nHence, both inequalities hold with equality. Therefore, \ufb01rst using Lemma 1 and then Lemma 2 we obtain\nS\u03bd(h(X))\u2212S\u03bd(h(X\u2217)) = h(X)\u2212h(X\u2217) = X \u2212X\u2217,\nwhich implies S\u03bd(h(X)) = X since S\u03bd(h(X\u2217)) = X\u2217. It then follows from Corollary 1 that X is an optimal solution\nto problem (1.8).\n\u2293\u2294\nWe now claim that the \ufb01xed-point iterations (2.1) converge to an optimal solution of problem (1.8).\nTheorem 4 The sequence {Xk} generated by the \ufb01xed point iterations with \u03c4 \u2208(0,2/\u03bbmax(A\u22a4A)) converges to\nsome X\u2217\u2208X \u2217, where X \u2217is the set of optimal solutions of problem (1.8).\nProof Since both S\u03bd(\u00b7) and h(\u00b7) are non-expansive, S\u03bd(h(\u00b7)) is also non-expansive. Therefore, {Xk} lies in a compact\nset and must have a limit point, say \u00afX = limj\u2192\u221eXk j. Also, for any X\u2217\u2208X \u2217,\n\u2225Xk+1 \u2212X\u2217\u2225F = \u2225S\u03bd(h(Xk))\u2212S\u03bd(h(X\u2217))\u2225F \u2264\u2225h(Xk)\u2212h(X\u2217)\u2225F \u2264\u2225Xk \u2212X\u2217\u2225F,\nwhich means that the sequence {\u2225Xk \u2212X\u2217\u2225F} is monotonically non-increasing. Therefore,\nlim\nk\u2192\u221e\u2225Xk \u2212X\u2217\u2225F = \u2225\u00afX \u2212X\u2217\u2225F,\n(3.6)\nwhere \u00afX can be any limit point of {Xk}. By the continuity of S\u03bd(h(\u00b7)), the image of \u00afX,\nS\u03bd(h( \u00afX)) = lim\nj\u2192\u221eS\u03bd(h(Xk j)) = lim\nj\u2192\u221eXk j+1,\nis also a limit point of {Xk}. Therefore, we have\n\u2225S\u03bd(h( \u00afX))\u2212S\u03bd(h(X\u2217))\u2225F = \u2225S\u03bd(h( \u00afX))\u2212X\u2217\u2225F = \u2225\u00afX \u2212X\u2217\u2225F,\nwhich allows us to apply Lemma 3 to get that \u00afX is an optimal solution to problem (1.8).\nFinally, by setting X\u2217= \u00afX \u2208X \u2217in (3.6), we get that\nlim\nk\u2192\u221e\u2225Xk \u2212\u00afX\u2225F = lim\nj\u2192\u221e\u2225Xk j \u2212\u00afX\u2225F = 0,\ni.e., {Xk} converges to its unique limit point \u00afX.\n\u2293\u2294\n4 Fixed point continuation\nIn this section, we discuss a continuation technique (i.e., homotopy approach) for accelerating the convergence of\nthe \ufb01xed point iterative algorithm (2.1).\n4.1 Continuation\nInspired by the work of Hale et al. [24], we \ufb01rst describe a continuation technique to accelerate the convergence\nof the \ufb01xed point iteration (2.1). Our \ufb01xed point continuation (FPC) iterative scheme for solving (1.8) is outlined\nbelow. The parameter \u03b7\u00b5 determines the rate of reduction of the consecutive \u00b5k, i.e.,\nFixed point and Bregman iterative methods for matrix rank minimization\n13\nFixed Point Continuation (FPC)\n\u2013 Initialize: Given X0, \u00af\u00b5 > 0. Select \u00b51 > \u00b52 > \u00b7\u00b7\u00b7 > \u00b5L = \u00af\u00b5 > 0. Set X = X0.\n\u2013 for \u00b5 = \u00b51,\u00b52,...,\u00b5L, do\n\u2013 while NOT converged, do\n\u2022 select \u03c4 > 0\n\u2022 compute Y = X \u2212\u03c4A \u2217(A (X)\u2212b), and SVD of Y, Y = UDiag(\u03c3)V \u22a4\n\u2022 compute X = UDiag(s\u03c4\u00b5(\u03c3))V \u22a4\n\u2013 end while\nend for\n\u00b5k+1 = max{\u00b5k\u03b7\u00b5, \u00af\u00b5},\nk = 1,...,L\u22121\n4.2 Stopping criteria for inner iterations\nNote that in the \ufb01xed point continuation algorithm, in the k-th inner iteration we solve problem (1.8) for a \ufb01xed\n\u00b5 = \u00b5k. There are several ways to determine when to stop this inner iteration, decrease \u00b5 and go to the next inner\niteration. The optimality conditions for (1.8) is given by (2.9a) and (2.9b). Thus we can use the following condition\nas a stopping criterion:\n\u2225UkV \u22a4\nk +gk/\u00b5\u22252 \u22121 < gtol,\n(4.1)\nwhere gtol is a small positive parameter. However, the expense of computing the largest singular value of a large\nmatrix greatly decreases the speed of the algorithm. Hence, we do not use this criterion as a stopping rule for large\nmatrices. Instead, we use the criterion\n\u2225Xk+1 \u2212Xk\u2225F\nmax{1,\u2225Xk\u2225F} < xtol,\n(4.2)\nwhere xtol is a small positive number, since when Xk gets close to an optimal solution X\u2217, the distance between Xk\nand Xk+1 should become very small.\n4.3 Debiasing\nDebiasing is another technique that can improve the performance of FPC. Debiasing has been used in compressed\nsensing algorithms for solving (1.4) and its variants, where debiasing is performed after a support set I has been\ntentatively identi\ufb01ed. Debiasing is the process of solving a least squares problem restricted to the support set I ,\ni.e., we solve\nmin \u2225AI xI \u2212b\u22252,\n(4.3)\nwhere AI is a submatrix of A whose columns correspond to the support index set I , and xI is a subvector of x\ncorresponding to I .\nOur debiasing procedure for the matrix completion problem differs from the procedure used in compressed\nsensing since the concept of a support set is not applicable. When we do debiasing, we \ufb01x the matrices Uk and V k\nin the singular value decomposition of Xk and then solve a least squares problem to determine the correct singular\nvalues \u03c3 \u2208Rr\n+; i.e., we solve\nmin\n\u03c3\u22650 \u2225A (UkDiag(\u03c3)V k\u22a4)\u2212b\u22252,\n(4.4)\n14\nShiqian Ma et al.\nwhere r is the rank of current matrix Xk. Because debiasing can be costly, we use a rule proposed in [43] to decide\nwhen to do it. In the continuation framework, we know that in each subproblem with a \ufb01xed \u00b5, \u2225Xk+1 \u2212Xk\u2225F\nconverges to zero, and \u2225g\u22252 converges to \u00b5 when Xk converges to the optimal solution of the subproblem. We\ntherefore choose to do debiasing when \u2225g\u22252/\u2225Xk+1 \u2212Xk\u2225F becomes large because this indicates that the change\nbetween two consecutive iterates is relatively small. Speci\ufb01cally, we call for debiasing in the solver FPC3 (see\nSection 7) when \u2225g\u22252/\u2225Xk+1 \u2212Xk\u2225F > 10.\n5 Bregman iterative algorithm\nAlgorithm FPC is designed to solve (1.8), an optimal solution of which approaches an optimal solution of the\nnuclear norm minimization problem (1.7) as \u00b5 goes to zero. However, by incorporating FPC into a Bregman iterative\ntechnique, we can solve (1.7) by solving a limited number of instances of (1.8), each corresponding to a different b.\nGiven a convex function J(\u00b7), the Bregman distance [5] of the point u from the point v is de\ufb01ned as\nDp\nJ(u,v) := J(u)\u2212J(v)\u2212< p,u\u2212v >,\n(5.1)\nwhere p \u2208\u2202J(v) is some subgradient in the subdifferential of J at the point v.\nBregman iterative regularization was introduced by Osher et al. in the context of image processing [31]. Specif-\nically, in [31], the Rudin-Osher-Fatemi [34] model\nu = argminu\u00b5\nZ\n|\u2207u|+ 1\n2\u2225u\u2212b\u22252\n2\n(5.2)\nwas extended to an iterative regularization model by replacing the total variation functional\nJ(u) = \u00b5TV(u) = \u00b5\nZ\n|\u2207u|,\nby the Bregman distance with respect to J(u). This Bregman iterative regularization procedure recursively solves\nuk+1 \u2190min\nu Dpk\nJ (u,uk)+ 1\n2\u2225u\u2212b\u22252\n2\n(5.3)\nfor k = 0,1,... starting with u0 = 0 and p0 = 0. Since (5.3) is a convex programming problem, the optimality\nconditions are given by 0 \u2208\u2202J(uk+1)\u2212pk +uk+1 \u2212b, from which we get the update formula for pk+1 :\npk+1 := pk +b\u2212uk+1.\n(5.4)\nTherefore, the Bregman iterative scheme is given by\n\uf8f1\n\uf8f2\n\uf8f3\nuk+1 \u2190minu Dpk\nJ (u,uk)+ 1\n2\u2225u\u2212b\u22252\n2\npk+1 = pk +b\u2212uk+1.\n(5.5)\nInterestingly, this turns out to be equivalent to the iterative process\n\uf8f1\n\uf8f2\n\uf8f3\nbk+1 = b+(bk \u2212uk)\nuk+1 \u2190minu J(u)+ 1\n2\u2225u\u2212bk+1\u22252\n2,\n(5.6)\nwhich can be easily implemented using existing algorithms for (5.2) with different inputs b.\nFixed point and Bregman iterative methods for matrix rank minimization\n15\nSubsequently, Yin et al. [44] proposed solving the basis pursuit problem (1.4) by applying the Bregman iterative\nregularization algorithm to\nmin\nx J(x)+ 1\n2\u2225Ax\u2212b\u22252\n2\n(5.7)\nfor J(x) = \u00b5\u2225x\u22251, and obtained the following two equivalent iterative schemes analogous to (5.5) and (5.6), respec-\ntively:\n\u2013 Version 1:\n\u2013 x0 \u21900, p0 \u21900,\n\u2013 for k = 0,1,... do\n\u2013 xk+1 \u2190argminxDpk\nJ (x,xk)+ 1\n2\u2225Ax\u2212b\u22252\n2\n\u2013 pk+1 \u2190pk \u2212A\u22a4(Axk+1 \u2212b)\n\u2013 Version 2:\n\u2013 b0 \u21900,x0 \u21900,\n\u2013 for k = 0,1,... do\n\u2013 bk+1 \u2190b+(bk \u2212Axk)\n\u2013 xk+1 \u2190argminxJ(x)+ 1\n2\u2225Ax\u2212bk+1\u22252\n2.\nOne can also use the Bregman iterative regularization algorithm applied to the unconstrained problem (1.8) to\nsolve the nuclear norm minimization problem (1.7). That is, one iteratively solves (1.8) by\nXk+1 \u2190min\nX Dpk\nJ (X,Xk)+ 1\n2\u2225A (X)\u2212b\u22252\n2,\n(5.8)\nand updates the subgradient pk+1 by\npk+1 := pk \u2212A \u2217(A(Xk+1)\u2212b),\n(5.9)\nwhere J(X) = \u00b5\u2225X\u2225\u2217.\nEquivalently, one can also use the following iterative scheme:\n\uf8f1\n\uf8f2\n\uf8f3\nbk+1 \u2190b+(bk \u2212A (Xk))\nXk+1 \u2190argminX \u00b5\u2225X\u2225\u2217+ 1\n2\u2225A (X)\u2212bk+1\u22252\n2.\n(5.10)\nThus, our Bregman iterative algorithm for nuclear norm minimization (1.7) can be outlined as follows. The last\nBregman Iterative Algorithm\n\u2013 b0 \u21900,X0 \u21900,\n\u2013 for k = 0,1,... do\n\u2013 bk+1 \u2190b+(bk \u2212A (Xk)),\n\u2013 Xk+1 \u2190argminX \u00b5\u2225X\u2225\u2217+ 1\n2\u2225A (X)\u2212bk+1\u22252\n2.\nstep can be solved by Algorithm FPC.\n16\nShiqian Ma et al.\n6 An approximate SVD based FPC algorithm: FPCA\nComputing singular value decompositions is the main computational cost in Algorithm FPC. Consequently, instead\nof computing the full SVD of the matrix Y in each iteration, we implemented a variant of algorithm FPC in which\nwe compute only a rank-r approximation to Y, where r is a predetermined parameter. We call this approximate SVD\nbased FPC algorithm (FPCA). This approach greatly reduces the computational effort required by the algorithm.\nSpeci\ufb01cally, we compute an approximate SVD by a fast Monte Carlo algorithm: the Linear Time SVD algorithm\ndeveloped by Drineas et al. [17]. For a given matrix A \u2208Rm\u00d7n, and parameters cs,ks \u2208Z+ with 1 \u2264ks \u2264cs \u2264\nn and {pi}n\ni=1, pi \u22650,\u2211n\ni=1 pi = 1, this algorithm returns an approximation to the largest ks singular values and\ncorresponding left singular vectors of the matrix A in linear O(m + n) time. The Linear Time SVD Algorithm is\noutlined below.\nLinear Time Approximate SVD Algorithm[17]\n\u2013 Input: A \u2208Rm\u00d7n, cs,ks \u2208Z+ s.t.1 \u2264ks \u2264cs \u2264n, {pi}n\ni=1 s.t.pi \u22650,\u2211n\ni=1 pi = 1.\n\u2013 Output: Hk \u2208Rm\u00d7ks and \u03c3t(C),t = 1,...,ks.\n\u2013 For t = 1 to cs,\n\u2022 Pick it \u22081,...,n with Pr[it = \u03b1] = p\u03b1,\u03b1 = 1,...,n.\n\u2022 Set C(t) = A(it)/\u221acspit.\n\u2013 Compute C\u22a4C and its SVD; say C\u22a4C = \u2211cs\nt=1 \u03c32\nt (C)ytyt\u22a4.\n\u2013 Compute ht = Cyt/\u03c3t(C) for t = 1,...,ks.\n\u2013 Return Hks, where H(t)\nks = ht, and \u03c3t(C),t = 1,...,ks.\nThe outputs \u03c3t(C),t = 1,...,ks are approximations to the largest ks singular values and H(t)\nks ,t = 1,...,k are\napproximations to the corresponding left singular vectors of the matrix A. Thus, the SVD of A is approximated by\nA \u2248Aks := HksDiag(\u03c3(C))(A\u22a4HksDiag(1/\u03c3(C))\u22a4.\nDrineas et al. [17] prove that with high probability, the following estimate holds for both \u03be = 2 and \u03be = F:\n\u2225A\u2212Aks\u22252\n\u03be \u2264\nmin\nD:rank(D)\u2264ks\n\u2225A\u2212D\u22252\n\u03be + poly(ks,1/cs)\u2225A\u22252\nF,\n(6.1)\nwhere poly(ks,1/cs) is a polynomial in ks and 1/cs. Thus, Aks is a approximation to the best rank-ks approximation\nto A. (For any matrix M \u2208Rm\u00d7n with SVD M = \u2211r\ni=1 \u03c3iuiv\u22a4\ni , where \u03c31 \u2265... \u2265\u03c3r > 0,ui \u2208Rm,vi \u2208Rn, the best\nrank-k approximation to M is given by \u00afM = \u2211k\ni=1 \u03c3iuiv\u22a4\ni ).\nNote that in this algorithm, we compute an exact SVD of a smaller matrix C\u22a4C \u2208Rcs\u00d7cs. Thus, cs determines the\nspeed of this algorithm. If we choose a large cs, we need more time to compute the SVD of C\u22a4C. However, the larger\ncs is, the more likely are the \u03c3t(C),t = 1,...,ks to be close to the largest ks singular values of the matrix A since the\nsecond term in the right hand side of (6.1) is smaller. In our numerical experiments, we found that we could choose\na relatively small cs so that the computational time was reduced without signi\ufb01cantly degrading the accuracy. In our\ntests, we obtained very good results by choosing cs = 2rm \u22122, where rm = \u230a(m+n\u2212\np\n(m+n)2 \u22124p)/2\u230bis, for a\ngiven number of entries sampled, the largest rank of m\u00d7n matrices for which the matrix completion problem has a\nunique solution.\nThere are many ways to choose the probabilities pi. In our numerical experiments in Section 7, we used the\nsimplest one, i.e., we set all pi equal to 1/n. For other choices of pi, see [17] and the references therein.\nIn our numerical experiments, we set ks using the following procedure. In the k-th iteration, when computing\nthe approximate SVD of Y k = Xk \u2212\u03c4gk, we set ks equal to the number of components in \u00afsk\u22121 that are no less than\nFixed point and Bregman iterative methods for matrix rank minimization\n17\n\u03b5ks max{\u00afsk\u22121}, where \u03b5ks is a small positive number and max{\u00afsk\u22121} is the largest component in the vector \u00afsk\u22121 used\nto form Xk = Uk\u22121Diag(\u00afsk\u22121)V k\u22121\u22a4. Note that ks is non-increasing in this procedure. However, if ks is too small at\nsome iteration, the non-expansive property (3.1) of the shrinkage operator S\u03bd may be violated since the approximate\nSVD is not a valid approximation when ks is too small. Thus, in algorithm FPCA, if (3.1) is violated 10 times, we\nincrease ks by 1. Our numerical experience indicates that this technique makes our algorithm very robust.\nOur numerical results in Section 7 show that this approximate SVD based FPC algorithm: FPCA, is very fast,\nrobust, and signi\ufb01cantly outperforms other solvers (such as SDPT3) in recovering low-rank matrices. This result is\nnot surprising. One reason for this is that in the approximate SVD algorithm, we compute a low-rank approximation\nto the original matrix. Hence, the iterative matrices produced by our algorithm are more likely to be of low-rank\nthan an exact solution to the nuclear norm minimization problem (1.10), or equivalently, to the SDP (1.12), which\nis exactly what we want. Some convergence/recoverability properties of a variant of FPCA, which uses a truncated\nSVD rather than a randomized SVD at each step, are discussed in [23].\n7 Numerical results\nIn this section, we report on the application of our FPC, FPCA and Bregman iterative algorithms to a series of matrix\ncompletion problems of the form (1.2) to demonstrate the ability of these algorithms to ef\ufb01ciently recover low-rank\nmatrices.\nTo illustrate the performance of our algorithmic approach combined with exact and approximate SVD algo-\nrithms, different stopping rules, and with or without debiasing, we tested the following solvers.\n\u2013 FPC1. Exact SVD, no debiasing, stopping rule: (4.2).\n\u2013 FPC2. Exact SVD, no debiasing, stopping rule: (4.1) and (4.2).\n\u2013 FPC3. Exact SVD with debiasing, stopping rule: (4.2).\n\u2013 FPCA. Approximate SVD, no debiasing, stopping rule: (4.2).\n\u2013 Bregman. Bregman iterative method using FPC2 to solve the subproblems.\n7.1 FPC and Bregman iterative algorithms for random matrices\nIn our \ufb01rst series of tests, we created random matrices M \u2208Rm\u00d7n with rank r by the following procedure: we \ufb01rst\ngenerated random matrices ML \u2208Rm\u00d7r and MR \u2208Rn\u00d7r with i.i.d. Gaussian entries and then set M = MLM\u22a4\nR . We\nthen sampled a subset \u2126of p entries uniformly at random. For each problem with m \u00d7 n matrix M, measurement\nnumber p and rank r, we solved 50 randomly created matrix completion problems. We use SR = p/(mn), i.e., the\nnumber of measurements divided by the number of entries of the matrix, to denote the sampling ratio. We also list\nFR = r(m+n\u2212r)/p, i.e. the dimension of the set of rank r matrices divided by the number of measurements, in the\ntables. Note that if FR > 1, then there is always an in\ufb01nite number of matrices with rank r with the given entries, so\nwe cannot hope to recover the matrix in this situation. We use rm to denote the largest rank such that FR \u22641, i.e.,\nrm = \u230a(m+n\u2212\np\n(m+n)2 \u22124p)/2\u230b. We use NS to denote the number of matrices that are recovered successfully.\nWe use AT to denote the average time (seconds) for the examples that are successfully solved.\nWe used the relative error\nrel.err. := \u2225Xopt \u2212M\u2225F\n\u2225M\u2225F\nto estimate the closeness of Xopt to M, where Xopt is the \u201coptimal\u201d solution to (1.10) produced by our algorithms.\nWe declared M to be recovered if the relative error was less than 10\u22123, which is the criterion used in [32] and [9].\n18\nShiqian Ma et al.\nWe use RA,RU,RL to denote the average, largest and smallest relative error of the successfully recovered matrices,\nrespectively.\nWe summarize the parameter settings used by the algorithms in Table 1. We use Im to denote the maximum\nnumber of iterations allowed for solving each subproblem in FPC, i.e., if the stopping rules (4.2) (and (4.1)) are not\nsatis\ufb01ed after Im iterations, we terminate the subproblem and decrease \u00b5 to start the next subproblem.\nTable 1 Parameters in Algorithm FPC\nFPC\n\u00af\u00b5 = 10\u22128,\u03b7\u00b5 = 1/4,\u00b51 = \u03b7\u00b5\u2225A \u2217b\u22252,\u03c4 = 1,xtol = 10\u221210,gtol = 10\u22124,Im = 500,X0 = 0\nApprox SVD\ncs = 2rm \u22122,\u03b5ks = 10\u22122, pi = 1/n,\u2200i\nAll numerical experiments were run in MATLAB 7.3.0 on a Dell Precision 670 workstation with an Intel\nXeon(TM) 3.4GHZ CPU and 6GB of RAM.\nThe comparisons between FPC1, FPC2, FPC3 and SDPT3 for small matrix completion problems are presented\nin Table 2. From Table 2 we can see that FPC1 and FPC2 achieve almost the same recoverability and relative error,\nwhich means that as long as we set xtol to be very small (like 10\u221210 ), we only need to use (4.2) as the stopping\nrule for the inner iterations. That is, use of stopping rule (4.1) does not affect the performance of the algorithm. Of\ncourse FPC2 costs more time than FPC1 since more iterations are sometimes needed to satisfy the stopping rules in\nFPC2. While FPC3 can improve the recoverability, it costs more time for performing debiasing. SDPT3 seems to\nobtain more accurate solutions than FPC1, FPC2 or FPC3.\nTable 2 Comparisons of FPC1, FPC2, FPC3 and SDPT3 for randomly created small matrix completion problems (m=n=40, p=800,\nSR=0.5)\nr\nFR\nSolver\nNS\nAT\nRA\nRU\nRL\n1\n0.0988\nFPC1\n50\n1.81\n1.67e-9\n1.22e-8\n6.06e-10\nFPC2\n50\n3.61\n1.32e-9\n1.20e-8\n2.55e-10\nFPC3\n50\n16.81\n1.06e-9\n2.22e-9\n5.68e-10\nSDPT3\n50\n1.81\n6.30e-10\n3.46e-9\n8.72e-11\n2\n0.1950\nFPC1\n42\n3.05\n1.01e-6\n4.23e-5\n8.36e-10\nFPC2\n42\n17.97\n1.01e-6\n4.23e-5\n2.78e-10\nFPC3\n49\n16.86\n1.26e-5\n3.53e-4\n7.62e-10\nSDPT3\n44\n1.90\n1.50e-9\n7.18e-9\n1.82e-10\n3\n0.2888\nFPC1\n35\n5.50\n9.72e-9\n2.85e-8\n1.93e-9\nFPC2\n35\n20.33\n2.17e-9\n1.41e-8\n3.88e-10\nFPC3\n42\n16.87\n3.58e-5\n7.40e-4\n1.34e-9\nSDPT3\n37\n1.95\n2.66e-9\n1.58e-8\n3.08e-10\n4\n0.3800\nFPC1\n22\n9.08\n7.91e-5\n5.46e-4\n3.57e-9\nFPC2\n22\n18.43\n7.91e-5\n5.46e-4\n4.87e-10\nFPC3\n29\n16.95\n3.83e-5\n6.18e-4\n2.57e-9\nSDPT3\n29\n2.09\n1.18e-8\n7.03e-8\n7.97e-10\n5\n0.4688\nFPC1\n1\n10.41\n2.10e-8\n2.10e-8\n2.10e-8\nFPC2\n1\n17.88\n2.70e-9\n2.70e-9\n2.70e-9\nFPC3\n5\n16.70\n1.78e-4\n6.73e-4\n6.33e-9\nSDPT3\n8\n2.26\n1.83e-7\n8.12e-7\n2.56e-9\n6\n0.5550\nFPC1\n0\n\u2014\n\u2014\n\u2014\n\u2014\nFPC2\n0\n\u2014\n\u2014\n\u2014\n\u2014\nFPC3\n0\n\u2014\n\u2014\n\u2014\n\u2014\nSDPT3\n1\n2.87\n6.58e-7\n6.58e-7\n6.58e-7\nFixed point and Bregman iterative methods for matrix rank minimization\n19\nTo illustrate the performance of our Bregman iterative algorithm, we compare the results of using it versus using\nFPC2 in Table 3. From our numerical experience, for those problems for which the Bregman iterative algorithm\ngreatly improves the recoverability, the Bregman iterative algorithm usually takes 2 to 3 iterations. Thus, in our\nnumerical tests, we \ufb01xed the number of subproblems solved by our Bregman algorithm to 3. Since our Bregman\nalgorithm achieves as good a relative error as the FPC algorithm, we only report how many of the examples that\nare successfully recovered by FPC, are improved greatly by using our Bregman iterative algorithm. In Table 3,\nNIM is the number of examples that the Bregman iterative algorithm outperformed FPC2 greatly (the relative errors\nobtained from FPC2 were at least 104 times larger than those obtained by the Bregman algorithm). From Table 3 we\ncan see that for more than half of the examples successfully recovered by FPC2, the Bregman iterative algorithm\nimproved the relative errors greatly (from [10\u221210, 10\u22129] to [10\u221216, 10\u221215]). Of course the run times for the Bregman\niterative algorithm were about three times that for algorithm FPC2, since the former calls the latter three times to\nsolve the subproblems.\nTable 3 Numerical results for the Bregman iterative method for small matrix completion problems (m=n=40, p=800, SR=0.5)\nProblem\nFPC2\nBregman\nr\nFR\nNIM (NS)\nRU\nRL\nRU\nRL\n1\n0.0988\n32 (50)\n2.22e-9\n2.55e-10\n1.87e-15\n3.35e-16\n2\n0.1950\n29 (42)\n5.01e-9\n2.80e-10\n2.96e-15\n6.83e-16\n3\n0.2888\n24 (35)\n2.77e-9\n3.88e-10\n2.93e-15\n1.00e-15\n4\n0.3800\n10 (22)\n5.51e-9\n4.87e-10\n3.11e-15\n1.30e-15\nIn the following, we discuss the numerical results obtained by our approximate SVD based FPC algorithm\n(FPCA). We will see from these numerical results that FPCA achieves much better recoverability and is much faster\nthan any of the solvers FPC1, FPC2, FPC3 or SDPT3.\nWe present the numerical results of FPCA for small (m=n=40) and medium (m=n=100) problems in Tables 4,\nand 5 respectively. Since we found that xtol = 10\u22126 is small enough to guarantee very good recoverability, we set\nxtol = 10\u22126 in algorithm FPCA and used only (4.2) as stopping rule for the inner iterations. From these tables,\nwe can see that our FPCA algorithm is much more powerful than SDPT3 for randomly created matrix completion\nproblems. When m = n = 40 and p = 800, and the rank r was less than or equal to 8, FPCA recovered the matrices\nin all 50 examples. When rank r = 9, it failed on only one example. Even for rank r = 10, which is almost the\nlargest rank that satis\ufb01es FR \u22641, FPCA still recovered the solution in more than 60% of the examples. However,\nSDPT3 started to fail to recover the matrices when the rank r = 2. When r = 6, there was only one example out of 50\nwhere the correct solution matrix was recovered. When r \u22657, none of the 50 examples could be recovered. For the\nmedium sized matrices (m = n = 100) we used p = 2000, which is only a 20% measurement rate, FPCA recovered\nthe matrices in all 50 examples when r \u22646. For r = 7, FPCA recovered the matrices in most of the examples (49\nout of 50). When r = 8, more than 60% of the matrices were recovered successfully by FPCA. Even when r = 9,\nFPCA still recovered 1 matrices. However, SDPT3 could not recover all of the matrices even when the rank r = 1\nand none of the matrices were recovered when r \u22654. When we increased the number of measurements to 3000,\nwe recovered the matrices in all 50 examples up to rank r = 12. When r = 13,14, we still recovered most of them.\nHowever, SDPT3 started to fail for some matrices when r = 3. When r \u22658, SDPT3 failed to recover any of the\nmatrices. We can also see that for the medium sized problems, FPCA was much faster than SDPT3.\n20\nShiqian Ma et al.\nTable 4 Numerical results for FPCA and SDPT3 for randomly created small matrix completion problems (m=n=40, p=800, SR=0.5)\nProblems\nFPCA\nSDPT3\nr\nFR\nNS\nAT\nRA\nRU\nRL\nNS\nAT\nRA\nRU\nRL\n1\n0.0988\n50\n3.49\n3.92e-7\n1.43e-6\n2.72e-7\n50\n1.84\n6.30e-10\n3.46e-9\n8.70e-11\n2\n0.1950\n50\n3.60\n1.44e-6\n7.16e-6\n4.41e-7\n44\n1.93\n1.50e-9\n7.18e-9\n1.82e-10\n3\n0.2888\n50\n3.97\n1.91e-6\n4.07e-6\n9.28e-7\n37\n1.99\n2.66e-9\n1.58e-8\n3.10e-10\n4\n0.3800\n50\n4.03\n2.64e-6\n8.14e-6\n1.54e-6\n29\n2.12\n1.18e-8\n7.03e-8\n8.00e-10\n5\n0.4688\n50\n4.16\n3.40e-6\n7.62e-6\n1.52e-6\n8\n2.30\n1.83e-7\n8.12e-7\n2.60e-9\n6\n0.5550\n50\n4.45\n4.08e-6\n7.62e-6\n2.26e-6\n1\n2.89\n6.58e-7\n6.58e-7\n6.58e-7\n7\n0.6388\n50\n4.78\n6.04e-6\n1.57e-5\n2.52e-6\n0\n\u2014\n\u2014\n\u2014\n\u2014\n8\n0.7200\n50\n4.99\n8.48e-6\n5.72e-5\n3.72e-6\n0\n\u2014\n\u2014\n\u2014\n\u2014\n9\n0.7987\n49\n5.73\n2.58e-5\n5.94e-4\n5.94e-6\n0\n\u2014\n\u2014\n\u2014\n\u2014\n10\n0.8750\n30\n7.20\n8.64e-5\n6.04e-4\n8.48e-6\n0\n\u2014\n\u2014\n\u2014\n\u2014\n11\n0.9487\n0\n\u2014\n\u2014\n\u2014\n\u2014\n0\n\u2014\n\u2014\n\u2014\n\u2014\nTable 5 Numerical results for FPCA and SDPT3 for randomly created medium matrix completion problems (m=n=100)\nProblems\nFPCA\nSDPT3\np\nr\nSR\nFR\nNS\nAT\nRA\nRU\nRL\nNS\nAT\nRA\nRU\nRL\n2000\n1\n0.2\n0.0995\n50\n4.93\n5.80e-6\n1.53e-5\n2.86e-6\n47\n15.10\n1.55e-9\n1.83e-8\n1.40e-10\n2000\n2\n0.2\n0.1980\n50\n5.26\n6.10e-6\n9.36e-6\n4.06e-6\n31\n16.02\n7.95e-9\n8.69e-8\n5.20e-10\n2000\n3\n0.2\n0.2955\n50\n5.80\n7.48e-6\n1.70e-5\n4.75e-6\n13\n19.23\n1.05e-4\n9.70e-4\n9.08e-10\n2000\n4\n0.2\n0.3920\n50\n9.33\n1.09e-5\n5.14e-5\n6.79e-6\n0\n\u2014\n\u2014\n\u2014\n\u2014\n2000\n5\n0.2\n0.4875\n50\n5.42\n1.61e-5\n8.95e-5\n8.12e-6\n0\n\u2014\n\u2014\n\u2014\n\u2014\n2000\n6\n0.2\n0.5820\n50\n7.02\n2.62e-5\n7.07e-5\n8.72e-6\n0\n\u2014\n\u2014\n\u2014\n\u2014\n2000\n7\n0.2\n0.6755\n49\n8.69\n7.69e-5\n5.53e-4\n1.11e-5\n0\n\u2014\n\u2014\n\u2014\n\u2014\n2000\n8\n0.2\n0.7680\n32\n10.94\n1.97e-4\n8.15e-4\n2.29e-5\n0\n\u2014\n\u2014\n\u2014\n\u2014\n2000\n9\n0.2\n0.8595\n1\n11.75\n4.38e-4\n4.38e-4\n4.38e-4\n0\n\u2014\n\u2014\n\u2014\n\u2014\n2000\n10\n0.2\n0.9500\n0\n\u2014\n\u2014\n\u2014\n\u2014\n0\n\u2014\n\u2014\n\u2014\n\u2014\n3000\n1\n0.3\n0.0663\n50\n7.73\n1.97e-6\n3.15e-6\n1.22e-6\n50\n36.68\n2.01e-10\n9.64e-10\n7.52e-11\n3000\n2\n0.3\n0.1320\n50\n7.85\n2.68e-6\n8.41e-6\n1.44e-6\n50\n36.50\n1.13e-9\n2.97e-9\n1.77e-10\n3000\n3\n0.3\n0.1970\n50\n8.10\n2.82e-6\n4.38e-6\n1.83e-6\n46\n38.50\n1.28e-5\n5.89e-4\n2.10e-10\n3000\n4\n0.3\n0.2613\n50\n8.94\n3.57e-6\n5.62e-6\n2.64e-6\n42\n41.28\n4.60e-6\n1.21e-4\n4.53e-10\n3000\n5\n0.3\n0.3250\n50\n9.12\n4.06e-6\n8.41e-6\n2.78e-6\n32\n43.92\n7.82e-8\n1.50e-6\n1.23e-9\n3000\n6\n0.3\n0.3880\n50\n9.24\n4.84e-6\n9.14e-6\n3.71e-6\n17\n49.60\n3.44e-7\n4.29e-6\n3.68e-9\n3000\n7\n0.3\n0.4503\n50\n9.41\n5.72e-6\n1.09e-5\n3.96e-6\n3\n59.18\n1.43e-4\n4.28e-4\n1.57e-7\n3000\n8\n0.3\n0.5120\n50\n9.62\n6.37e-6\n1.90e-5\n4.43e-6\n0\n\u2014\n\u2014\n\u2014\n\u2014\n3000\n9\n0.3\n0.5730\n50\n10.35\n6.32e-6\n1.60e-5\n4.56e-6\n0\n\u2014\n\u2014\n\u2014\n\u2014\n3000\n10\n0.3\n0.6333\n50\n10.93\n8.45e-6\n3.79e-5\n5.59e-6\n0\n\u2014\n\u2014\n\u2014\n\u2014\n3000\n11\n0.3\n0.6930\n50\n11.58\n1.41e-5\n6.84e-5\n6.99e-6\n0\n\u2014\n\u2014\n\u2014\n\u2014\n3000\n12\n0.3\n0.7520\n50\n12.17\n1.84e-5\n1.46e-4\n8.84e-6\n0\n\u2014\n\u2014\n\u2014\n\u2014\n3000\n13\n0.3\n0.8103\n48\n15.24\n5.12e-5\n6.91e-4\n1.25e-5\n0\n\u2014\n\u2014\n\u2014\n\u2014\n3000\n14\n0.3\n0.8680\n39\n18.85\n2.35e-4\n9.92e-4\n2.05e-5\n0\n\u2014\n\u2014\n\u2014\n\u2014\n3000\n15\n0.3\n0.9250\n0\n\u2014\n\u2014\n\u2014\n\u2014\n0\n\u2014\n\u2014\n\u2014\n\u2014\n3000\n16\n0.3\n0.9813\n0\n\u2014\n\u2014\n\u2014\n\u2014\n0\n\u2014\n\u2014\n\u2014\n\u2014\n7.2 Comparison of FPCA and SVT\nIn this subsection we compare our FPCA algorithm against the SVT algorithm proposed in [8]. The SVT code\nis downloaded from http://svt.caltech.edu. We constructed two sets of test problems. One set contained \u201ceasy\u201d\nproblems. These problems are \u201ceasy\u201d because the matrices are of very low-rank compared to the matrix size and the\nnumber of samples, and hence they are easy to recover. For all problems in this set, FR was less than 0.34. The other\nFixed point and Bregman iterative methods for matrix rank minimization\n21\nset contained \u201chard\u201d problems, i.e., problems that are very challenging. These problems involved matrices that are\nnot of very low rank and for which sampled a very limited number of entries. For this set of problems, FR ranged\nfrom 0.40 to 0.87. The maximum iteration number in SVT was set to be 1000. All other parameters were set to\ntheir default values in SVT. The parameters of FPCA were set somewhat loosely for easy problems. Speci\ufb01cally, we\nset \u00af\u00b5 = 10\u22124,xtol = 10\u22124,\u03c4 = 2,Im = 10 and all other parameters were set to the values given in Table 1. Relative\nerrors and times were averaged over 5 runs. In this subsection, all test matrices were square, i.e., m = n.\nTable 6 Comparison of FPCA and SVT on easy problems\nProblems\nFPCA\nSVT\nn\nr\np\nSR\nFR\nrel.err.\ntime\nrel.err.\ntime\n100\n10\n5666\n0.57\n0.34\n4.27e-5\n0.39\n1.64e-3\n30.40\n200\n10\n15665\n0.39\n0.25\n6.40e-5\n1.38\n1.90e-4\n9.33\n500\n10\n49471\n0.20\n0.20\n2.48e-4\n8.01\n1.88e-4\n23.77\n1000\n10\n119406\n0.12\n0.17\n5.04e-4\n18.49\n1.68e-4\n41.81\n1000\n50\n389852\n0.39\n0.25\n3.13e-5\n120.64\n1.63e-4\n228.79\n1000\n100\n569900\n0.57\n0.33\n2.26e-5\n177.17\n1.71e-4\n635.15\n5000\n10\n597973\n0.02\n0.17\n1.58e-3\n1037.12\n1.73e-4\n121.39\n5000\n50\n2486747\n0.10\n0.20\n5.39e-4\n1252.70\n1.59e-4\n1375.33\n5000\n100\n3957533\n0.16\n0.25\n2.90e-4\n2347.41\n1.74e-4\n5369.76\nFrom Table 6, we can see that for the easy problems except for one problem which is exceptionally sparse as\nwell as having low rank, FPCA was much faster and usually provided more accurate solutions than SVT.\nFor hard problems, all parameters of FPCA were set to the values given in Table 1, except that we set xtol = 10\u22126\nsince this value is small enough to guarantee very good recoverability. Also, for small problems ( i.e., max{m,n} <\n1000 ), we set Im = 500; and for large problems ( i.e., max{m,n} \u22651000 ), we set Im = 20. We use \u201c\u2014\u201d to indicate\nthat the algorithm either diverges or does not terminate in one hour. Relative errors and times were averaged over 5\nruns.\nTable 7 Comparison of FPCA and SVT on hard problems\nProblems\nFPCA\nSVT\nn\nr\nSR\nFR\nrel.err.\ntime\nrel.err.\ntime\n40\n9\n0.5\n0.80\n1.21e-5\n5.72\n5.01e-1\n3.05\n100\n14\n0.3\n0.87\n1.32e-4\n19.00\n8.31e-1\n316.90\n1000\n20\n0.1\n0.40\n2.46e-5\n116.15\n\u2014\n\u2014\n1000\n30\n0.1\n0.59\n2.00e-3\n128.30\n\u2014\n\u2014\n1000\n50\n0.2\n0.49\n1.04e-5\n183.67\n\u2014\n\u2014\nFrom Table 7, we can see that for the hard problems, SVT either diverged or did not solve the problems in less\nthan one hour, or it yielded a very inaccurate solution. In contrast, FPCA always provided a very good solution\nef\ufb01ciently.\nWe can also see that FPCA was able to ef\ufb01ciently solve large problems (m = n = 1000) that could not be solved\nby SDPT3 due to the large size of the matrices and the large number of constraints.\n22\nShiqian Ma et al.\n7.3 Results for real data matrices\nIn this section, we consider matrix completion problems based on two real data sets: the Jester joke data set [22]\nand the DNA data set [35]. The Jester joke data set contains 4.1 million ratings for 100 jokes from 73,421 users\nand is available on the website http://www.ieor.berkeley.edu/\u02dcEgoldberg/jester-data/. Since the number of jokes is\nonly 100, but the number of users is quite large, we randomly selected nu users to get a modestly sized matrix for\ntesting purpose. As in [37], we randomly held out two ratings for each user. Since some entries in the matrix are\nmissing, we cannot compute the relative error as we did for the randomly created matrices. Instead, we computed\nthe Normalized Mean Absolute Error (NMAE) as in [22] and [37]. The Mean Absolute Error (MAE) is de\ufb01ned as\nMAE = 1\n2N\nN\n\u2211\ni=1\n|\u02c6ri\ni1 \u2212ri\ni1|+|\u02c6ri\ni2 \u2212ri\ni2|,\n(7.1)\nwhere ri\nj and \u02c6ri\nj are the withheld and predicted ratings of movie j by user i, respectively, for j = i1,i2. NMAE is\nde\ufb01ned as\nNMAE =\nMAE\nrmax \u2212rmin\n,\n(7.2)\nwhere rmin and rmax are lower and upper bounds for the ratings. Since all ratings are scaled to the range [\u221210,+10],\nwe have rmin = \u221210 and rmax = 10.\nThe numerical results for the Jester data set using FPC1 and FPCA are presented in Tables 8 and 9, respectively.\nIn these two tables, \u03c3max and \u03c3min are the largest and smallest positive singular values of the recovered matrices,\nand rank is the rank of the recovered matrices. The distributions of the singular values of the recovered matrices are\nshown in Figures 1 and 2. From Tables 8 and 9 we can see that by using FPC1 and FPCA to recover these matrices,\nwe can get relatively low NMAEs, which are comparable to the results shown in [37] and [22].\nTable 8 Numerical results for FPC1 for the Jester joke data set\nnum.user\nnum.samp\nsamp.ratio\nrank\n\u03c3max\n\u03c3min\nNMAE\nTime\n100\n7172\n0.7172\n79\n285.65\n3.49e-4\n0.1727\n34.30\n1000\n71152\n0.7115\n100\n786.37\n38.43\n0.1667\n304.81\n2000\n140691\n0.7035\n100\n1.1242e+3\n65.06\n0.1582\n661.66\nTable 9 Numerical results for FPCA for the Jester joke data set (cs is the number of rows we picked for the approximate SVD)\nnum.user\nnum.samp\nsamp.ratio\n\u03b5ks\ncs\nrank\n\u03c3max\n\u03c3min\nNMAE\nTime\n100\n7172\n0.7172\n10\u22122\n25\n20\n295.14\n32.68\n0.1627\n26.73\n1000\n71152\n0.7115\n10\u22122\n100\n85\n859.27\n48.04\n0.2008\n808.52\n1000\n71152\n0.7115\n10\u22124\n100\n90\n859.46\n44.62\n0.2101\n778.56\n2000\n140691\n0.7035\n10\u22124\n200\n100\n1.1518e+3\n63.52\n0.1564\n1.1345e+3\nWe also used two data sets of DNA microarrays from [35]. These data sets are available on the website http://cellcycle-www.stanfor\nThe \ufb01rst microarray data set is a matrix that represents the expression of 6178 genes in 14 experiments based on the\nElutriation data set in [35]. The second microarray data set is based on the Cdc15 data set in [35], and represents\nthe expression of 6178 genes in 24 experiments. However, some entries in these two matrices are missing. For eval-\nuating our algorithms, we created complete matrices by deleted all rows containing missing values. This is similar\nFixed point and Bregman iterative methods for matrix rank minimization\n23\n0\n20\n40\n60\n80\n100\n0\n50\n100\n150\n200\n250\n300\n0\n20\n40\n60\n80\n100\n0\n100\n200\n300\n400\n500\n600\n700\n800\n0\n20\n40\n60\n80\n100\n0\n200\n400\n600\n800\n1000\n1200\nFig. 1 Distribution of the singular values of the recovered matrices for the Jester data set using FPC1. Left:100 users, Middle: 1000\nusers, Right: 2000 users\n0\n20\n40\n60\n80\n100\n0\n50\n100\n150\n200\n250\n300\n0\n20\n40\n60\n80\n100\n0\n100\n200\n300\n400\n500\n600\n700\n800\n900\n0\n20\n40\n60\n80\n100\n0\n100\n200\n300\n400\n500\n600\n700\n800\n900\n0\n20\n40\n60\n80\n100\n0\n200\n400\n600\n800\n1000\n1200\nFig. 2 Distribution of the singular values of the recovered matrices for the Jester data set using FPCA. Upper Left: 100 users, \u03b5ks =\n10\u22122,cs = 25; Upper Right: 1000 users, \u03b5ks = 10\u22122,cs = 100; Bottom Left: 1000 users, \u03b5ks = 10\u22124,cs = 100; Bottom Right: 2000 users,\n\u03b5ks = 10\u22124,cs = 200\nto how the DNA microarray data set was preprocessed in [41]. The resulting complete matrix for the Elutriation\ndata set was 5766\u00d7 14. The complete matrix for the Cdc15 data set was 4381\u00d7 24. We must point out that these\nDNA microarray matrices are neither low-rank nor even approximately low-rank although such claims have been\nmade in some papers. The distributions of the singular values of these two matrices are shown in Figure 3. From\nthis \ufb01gure we can see that in each microarray matrix, only one singular value is close to zero, while the others are\nfar away from zero. Thus there is no way to claim that the rank of the Elutriation matrix is less than 13, or the\nrank of the Cdc15 matrix is less than 23. Since these matrices are not low-rank, we cannot expect our algorithms to\nrecover these matrices by sampling only a small portion of their entries. Thus we needed to further modify the data\n24\nShiqian Ma et al.\n0\n2\n4\n6\n8\n10\n12\n14\n0\n10\n20\n30\n40\n50\n60\n70\n0\n5\n10\n15\n20\n25\n0\n10\n20\n30\n40\n50\n60\n70\n80\nFig. 3 Distribution of the singular values of the matrices in the original DNA microarray data sets. Left: Elutriation matrix; Right:\nCdc15 matrix.\nsets to yield low-rank matrices. Speci\ufb01cally, we used the best rank-2 approximation to the Elutriation matrix as the\nnew complete Elutriation matrix and the best rank-5 approximation to the Cdc15 matrix as the new complete Cdc15\nmatrix. The numerical results for FPCA for recovering these two matrices are presented in Table 10. In the FPCA\nalgorithm, we set \u03b5ks = 10\u22122 and xtol = 10\u22126. For the Elutriation matrix, we set cs = 115 and for the Cdc15 matrix,\nwe set cs = 88. The observed entries were randomly sampled. From Table 10 we can see that by taking 60% of the\nentries of the matrices, our FPCA algorithm can recover these matrices very well, yielding relative errors as low as\n10\u22125 and 10\u22126, which is promising for practical use.\nTable 10 Numerical results of FPCA for DNA microarray data sets\nMatrix\nm\nn\np\nrank\nSR\nFR\nrel.err\nTime\nElutriation\n5766\n14\n48434\n2\n0.6\n0.2386\n1.83e-5\n218.01\nCdc15\n4381\n24\n63086\n5\n0.6\n0.3487\n7.95e-6\n189.32\nTo graphically illustrate the effectiveness of FPCA, we applied it to image inpainting [3]. Grayscale images and\ncolor images can be expressed as matrices and tensors, respectively. In grayscale image inpainting, the grayscale\nvalue of some of the pixels of the image are missing, and we want to \ufb01ll in these missing values. If the image is\nof low-rank, or of numerical low-rank, we can solve the image inpainting problem as a matrix completion problem\n(1.2). In our test we applied SVD to the 512 \u00d7 512 image in Figure 4(a), and truncated this decomposition to get\nthe rank-40 image which is shown in Figure 4(b). Figure 4(c) is a masked version of the image in Figure 4(a),\nwhere one half of the pixels in Figure 4(a) were masked uniformly at random. Figure 4(d) is the image obtained\nfrom Figure 4(c) by applying FPCA. Figure 4(d) is a low-rank approximation to Figure 4(a) with a relative error of\n8.41e \u22122. Figure 4(e) is a masked version of the image in Figure 4(b), where one half of the pixels in Figure 4(b)\nwere masked uniformly at random. Figure 4(f) is the image obtained from Figure 4(e) by applying FPCA. Figure\n4(f) is an approximation to Figure 4(b) with a relative error of 3.61e \u22122. Figure 4(g) is another masked image\nobtained from Figure 4(b), where 4 percent of the pixels were masked in a non-random fashion. Figure 4(h) is the\nimage obtained from Figure 4(g) by applying FPCA. Figure 4(g) is an approximation to Figure 4(b) with a relative\nerror of 1.70e\u22122.\nFixed point and Bregman iterative methods for matrix rank minimization\n25\n50\n100\n150\n200\n250\n300\n350\n400\n450\n500\n50\n100\n150\n200\n250\n300\n350\n400\n450\n500\n50\n100\n150\n200\n250\n300\n350\n400\n450\n500\n50\n100\n150\n200\n250\n300\n350\n400\n450\n500\n(a)\n(b)\n50\n100\n150\n200\n250\n300\n350\n400\n450\n500\n50\n100\n150\n200\n250\n300\n350\n400\n450\n500\n50\n100\n150\n200\n250\n300\n350\n400\n450\n500\n50\n100\n150\n200\n250\n300\n350\n400\n450\n500\n(c)\n(d)\n50\n100\n150\n200\n250\n300\n350\n400\n450\n500\n50\n100\n150\n200\n250\n300\n350\n400\n450\n500\n50\n100\n150\n200\n250\n300\n350\n400\n450\n500\n50\n100\n150\n200\n250\n300\n350\n400\n450\n500\n(e)\n(f)\n50\n100\n150\n200\n250\n300\n350\n400\n450\n500\n50\n100\n150\n200\n250\n300\n350\n400\n450\n500\n50\n100\n150\n200\n250\n300\n350\n400\n450\n500\n50\n100\n150\n200\n250\n300\n350\n400\n450\n500\n(g)\n(h)\nFig. 4 (a): Original 512\u00d7512 image with full rank; (b): Original image truncated to be of rank 40; (c): 50% randomly masked original\nimage; (d): Recovered image from 50% randomly masked original image (rel.err = 8.41e \u22122); (e): 50% randomly masked rank 40\nimage; (f): Recovered image from 50% randomly masked rank 40 image (rel.err = 3.61e \u22122); (g): Deterministically masked rank 40\nimage (SR = 0.96); (h): Recovered image from deterministically masked rank 40 image (rel.err = 1.70e\u22122).\n26\nShiqian Ma et al.\n8 Conclusions and discussions\nIn this paper, we derived a \ufb01xed point continuation algorithm and a Bregman iterative algorithm for solving the\nlinearly constrained nuclear norm minimization problem, which is a convex relaxation of the NP-hard linearly\nconstrained matrix rank minimization problem. The convergence of the \ufb01xed point iterative scheme was established.\nBy adopting an approximate SVD technique, we obtained a very powerful algorithm (FPCA) for the matrix rank\nminimization problem. On matrix completion problems, FPCA greatly outperforms SDP solvers such as SDPT3 in\nboth speed and recoverability of low-rank matrices. Further study is needed to prove the convergence of algorithm\nFPCA.\nAcknowledgements We would like to thank two anonymous referees for their helpful comments. The \ufb01rst author thanks Junzhou\nHuang from Rutgers University for fruitful discussions on image inpainting.\nReferences\n1. Bach, F.R.: Consistency of trace norm minimization. Journal of Machine Learning Research 9(Jun), 1019\u20131048 (2008)\n2. van den Berg, E., Friedlander, M.P.: Probing the Pareto frontier for basis pursuit solutions. Preprint available at Optimization\nOnline: 2008.01.1889 (2008)\n3. Bertalm\u00b4\u0131o, M., Sapiro, G., Caselles, V., Ballester, C.: Image inpainting. Proceedings of SIGGRAPH 2000, New Orleans, USA\n(2000)\n4. Borwein, J.M., Lewis, A.S.: Convex Analysis and Nonlinear Optimization. Springer-Verlag (2003)\n5. Bregman, L.: The relaxation method of \ufb01nding the common points of convex sets and its application to the solution of problems in\nconvex programming. USSR Computational Mathematics and Mathematical Physics 7, 200\u2013217 (1967)\n6. Burer, S., Monteiro, R.D.C.: A nonlinear programming algorithm for solving semide\ufb01nite programs via low-rank factorization.\nMathematical Programming (Series B) 95, 329\u2013357 (2003)\n7. Burer, S., Monteiro, R.D.C.: Local mimima and convergence in low-rank semide\ufb01nite programming. Mathematical Programming\n103(3), 427\u2013444 (2005)\n8. Cai, J., Cand`es, E.J., Shen, Z.: A singular value thresholding algorithm for matrix completion. submitted for publication (2008)\n9. Cand`es, E.J., Recht, B.: Exact matrix completion via convex optimization. Submitted (2008)\n10. Cand`es, E.J., Romberg, J.: \u21131-MAGIC: Recovery of sparse signals via convex programming. Tech. rep., Caltech (2005)\n11. Cand`es, E.J., Romberg, J., Tao, T.: Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency\ninformation. IEEE Transactions on Information Theory 52, 489\u2013509 (2006)\n12. Cand`es, E.J., Tao, T.: The power of convex relaxation: near-optimal matrix completion. preprint (2009)\n13. Dai, W., Milenkovic, O.: Subspace pursuit for compressive sensing: closing the gap between performance and complexity. Preprint\navailable at arXiv: 0803.0811 (2008)\n14. Donoho, D.: Compressed sensing. IEEE Transactions on Information Theory 52, 1289\u20131306 (2006)\n15. Donoho, D., Tsaig, Y., Drori, I., Starck, J.C.: Sparse solution of underdetermined linear equations by stagewise orthogonal matching\npursuit. Submitted to IEEE Trransactions on Information Theory (2006)\n16. Donoho, D.L., Tsaig, Y.: Fast solution of \u21131-norm minimization problems when the solution may be sparse. Tech. rep., Department\nof Statistics, Stanford University (2006)\n17. Drineas, P., Kannan, R., Mahoney, M.W.: Fast Monte Carlo algorithms for matrices ii: Computing low-rank approximations to a\nmatrix. SIAM J. Computing 36, 132\u2013157 (2006)\n18. Fazel, M.: Matrix rank minimization with applications. Ph.D. thesis, Stanford University (2002)\n19. Fazel, M., Hindi, H., Boyd, S.: A rank minimization heuristic with application to minimum order system approximation.\nIn:\nProceedings of the American Control Conference (2001)\n20. Figueiredo, M.A.T., Nowak, R.D., Wright, S.J.: Gradient projection for sparse reconstruction: Application to compressed sensing\nand other inverse problems. IEEE Journal on Selected Topics in Signal Processing 1(4) (2007)\n21. Ghaoui, L.E., Gahinet, P.: Rank minimization under LMI constraints: A framework for output feedback problems. In: Proceedings\nof the European Control Conference (1993)\n22. Goldberg, K., Roeder, T., Gupta, D., Perkins, C.: Eigentaste: A constant time collaborative \ufb01ltering algorithm. Information Retrieval\n4(2), 133\u2013151 (2001)\n23. Goldfarb, D., Ma, S.: Convergence of \ufb01xed point continuation algorithms for matrix rank minimization. Tech. rep., Department of\nIEOR, Columbia University (2009)\nFixed point and Bregman iterative methods for matrix rank minimization\n27\n24. Hale, E.T., Yin, W., Zhang, Y.: A \ufb01xed-point continuation method for \u21131-regularized minimization with applications to compressed\nsensing. Tech. rep., CAAM TR07-07 (2007)\n25. Hiriart-Urruty, J.B., Lemar\u00b4echal, C.: Convex Analysis and Minimization Algorithms II: Advanced Theory and Bundle Methods.\nSpringer-Verlag, New York (1993)\n26. Horn, R.A., Johnson, C.R.: Matrix Analysis. Cambridge University Press (1985)\n27. Kim, S.J., Koh, K., Lustig, M., Boyd, S., Gorinevsky, D.: A method for large-scale \u21131-regularized least-squares. IEEE Journal on\nSelected Topics in Signal Processing 4(1), 606\u2013617 (2007)\n28. Linial, N., London, E., Rabinovich, Y.: The geometry of graphs and some of its algorithmic applications. Combinatorica 15,\n215\u2013245 (1995)\n29. Liu, Z., Vandenberghe, L.: Interior-point method for nuclear norm approximation with application to system identi\ufb01cation. Sub-\nmitted to Mathematical Programming Series B (2008)\n30. Natarajan, B.K.: Sparse approximation solutions to linear systems. SIAM J. Computing 24(2), 227\u2013234 (1995)\n31. Osher, S., Burger, M., Goldfarb, D., Xu, J., Yin, W.: An iterative regularization method for total varitaion-based image restoration.\nSIAM MMS 4(2), 460\u2013489 (2005)\n32. Recht, B., Fazel, M., Parrilo, P.: Guaranteed minimum rank solutions of matrix equations via nuclear norm minimization. Submitted\nto SIAM Review (2007)\n33. Rennie, J.D.M., Srebro, N.: Fast maximum margin matrix factorization for collaborative prediction. In: Proceedings of the Interna-\ntional Conference of Machine Learning (2005)\n34. Rudin, L., Osher, S., Fatemi, E.: Nonlinear total variation based noise removal algorithms. Physica D 60, 259\u2013268 (1992)\n35. Spellman, P.T., Sherlock, G., Zhang, M.Q., Iyer, V.R., Anders, K., Eisen, M.B., Brown, P.O., Botstein, D., Futcher, B.: Compre-\nhensive identi\ufb01cation of cell cycle-regulated genes of the yeast saccharomyces cerevisiae by microarray hybridization. Molecular\nBiology of the Cell 9, 3273\u20133297 (1998)\n36. Srebro, N.: Learning with matrix factorizations. Ph.D. thesis, Massachusetts Institute of Technology (2004)\n37. Srebro, N., Jaakkola, T.: Weighted low-rank approximations. In: Proceedings of the Twentieth International Conference on Machine\nLearning (ICML-2003) (2003)\n38. Sturm, J.F.: Using SeDuMi 1.02, a Matlab toolbox for optimization over symmetric cones. Optimization Methods and Software\n11-12, 625\u2013653 (1999)\n39. Tibshirani, R.: Regression shrinkage and selection via the lasso. Journal Royal Statistical Society B 58, 267\u2013288 (1996)\n40. Tropp, J.: Just relax: Convex programming methods for identifying sparse signals. IEEE Transactions on Information Theory 51,\n1030\u20131051 (2006)\n41. Troyanskaya, O., Cantor, M., Sherlock, G., Brown, P., Hastie, T., Tibshirani, R., Botstein, D., Altman, R.B.: Missing value estima-\ntion methods for DNA microarrays. Bioinformatics 17(6), 520\u2013525 (2001)\n42. T\u00a8ut\u00a8unc\u00a8u, R.H., Toh, K.C., Todd, M.J.: Solving semide\ufb01nite-quadratic-linear programs using SDPT3. Mathematical Programming\nSeries B 95, 189\u2013217 (2003)\n43. Wen, Z., Yin, W., Goldfarb, D., Zhang, Y.: A fast algorithm for sparse reconstruction based on shrinkage, subspace optimization\nand continuation. Tech. rep., Department of IEOR, Columbia University (2009)\n44. Yin, W., Osher, S., Goldfarb, D., Darbon, J.: Bregman iterative algorithms for \u21131-minimization with applications to compressed\nsensing. SIAM J. Imaging Sci 1(1), 143\u2013168 (2008)\n",
        "sentence": " We also modify the fixed point continuation (FPC) algorithm (Ma et al., 2011) to find the global optimum. Then, Ma et al. (2011) proposed the fixed point continuation (FPC) algorithm which is fast and robust. Then, Ma et al. (2011) proposed the fixed point continuation (FPC) algorithm which is fast and robust. Moreover, Goldfrab and Ma (2011) proved the convergence of the FPC algorithm for solving the nuclear norm minimization problem.",
        "context": "5 Bregman iterative algorithm\nAlgorithm FPC is designed to solve (1.8), an optimal solution of which approaches an optimal solution of the\nnuclear norm minimization problem (1.7) as \u00b5 goes to zero. However, by incorporating FPC into a Bregman iterative\ncontinuation (FPC) algorithm which uses an operator splitting technique for solving (1.8). By adopting a Monte\nCarlo approximate SVD in the FPC, we get an algorithm, which we call FPCA (Fixed Point Continuation with\nFixed point and Bregman iterative methods for matrix rank minimization\n13\nFixed Point Continuation (FPC)\n\u2013 Initialize: Given X0, \u00af\u00b5 > 0. Select \u00b51 > \u00b52 > \u00b7\u00b7\u00b7 > \u00b5L = \u00af\u00b5 > 0. Set X = X0.\n\u2013 for \u00b5 = \u00b51,\u00b52,...,\u00b5L, do\n\u2013 while NOT converged, do\n\u2022 select \u03c4 > 0"
    },
    {
        "title": "A framework for multiple-instance learning",
        "author": [
            "Maron",
            "Lozano-P\u00e9rez1998] Oded Maron",
            "Tom\u00e1s Lozano-P\u00e9rez"
        ],
        "venue": "Advances in neural information processing systems,",
        "citeRegEx": "Maron et al\\.,? \\Q1998\\E",
        "shortCiteRegEx": "Maron et al\\.",
        "year": 1998,
        "abstract": "",
        "full_text": "",
        "sentence": " Snow et al. (2004) used WordNet as the knowledge base to discover more hpyernym/hyponym relations between entities from news articles. Snow et al. (2004) used WordNet as the knowledge base to discover more hpyernym/hyponym relations between entities from news articles. However, either bioinformatic database or WordNet is maintained by a few experts, thus hardly kept up-to-date. As we are stepping into the big data era, the explosion of unstructured Web texts simulates us to build more powerful models that can automatically extract relation instances from large-scale online natural language corpora without handlabeled annotation. Mintz et al. (2009) adopted Freebase (Bollacker et al.",
        "context": null
    },
    {
        "title": "Distant supervision for relation extraction with an incomplete knowledge base",
        "author": [
            "Min et al.2013] Bonan Min",
            "Ralph Grishman",
            "Li Wan",
            "Chang Wang",
            "David Gondek"
        ],
        "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Asso-",
        "citeRegEx": "Min et al\\.,? \\Q2013\\E",
        "shortCiteRegEx": "Min et al\\.",
        "year": 2013,
        "abstract": "",
        "full_text": "",
        "sentence": " Other literatures (Takamatsu et al., 2012; Min et al., 2013; Zhang et al., 2013; Xu et al., 2013) addressed more specific issues, like how to construct the negative class in learning or how to adopt more information, such as name entity tags, to improve the performance. Our work is more relevant to Riedel et al.\u2019s (2013) which considered the task as a matrix factorization problem.",
        "context": null
    },
    {
        "title": "Fast maximum margin matrix factorization for collaborative prediction",
        "author": [
            "Rennie",
            "Srebro2005] Jasson DM Rennie",
            "Nathan Srebro"
        ],
        "venue": "In Proceedings of the 22nd international conference on Machine learning,",
        "citeRegEx": "Rennie et al\\.,? \\Q2005\\E",
        "shortCiteRegEx": "Rennie et al\\.",
        "year": 2005,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " They extended the multi-instance learning framework (Riedel et al., 2010) to the multi-label circumstance. Surdeanu et al. (2012) proposed a novel approach to multi-instance multi-label learning for relation extraction, which jointly modeled all the sentences in texts and all labels in knowledge bases for a given entity pair. The first dataset12, NYT\u201910, was developed by Riedel et al. (2010), and also used by Hoffmann et al.",
        "context": null
    },
    {
        "title": "Modeling relations and their mentions without labeled text",
        "author": [
            "Limin Yao",
            "Andrew McCallum"
        ],
        "venue": "In Machine Learning and Knowledge Discovery in Databases,",
        "citeRegEx": "Riedel et al\\.,? \\Q2010\\E",
        "shortCiteRegEx": "Riedel et al\\.",
        "year": 2010,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " They extended the multi-instance learning framework (Riedel et al., 2010) to the multi-label circumstance.",
        "context": null
    },
    {
        "title": "Relation extraction with matrix factorization and universal schemas",
        "author": [
            "Limin Yao",
            "Andrew McCallum",
            "Benjamin M. Marlin"
        ],
        "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Asso-",
        "citeRegEx": "Riedel et al\\.,? \\Q2013\\E",
        "shortCiteRegEx": "Riedel et al\\.",
        "year": 2013,
        "abstract": "",
        "full_text": "",
        "sentence": " In order to conduct reliable experiments, we adjust and estimate the parameters for our approaches, DRMC-b and DRMC-1, and compare them with other four kinds of landmark methods (Mintz et al., 2009; Hoffmann et al., 2011; Surdeanu et al., 2012; Riedel et al., 2013) on two public datasets. We also perform the experiments to compare our approaches with the state-of-the-art NFE-1316 (Riedel et al., 2013) and its sub-methods (N-13, F-13 and NF-13) on NYT\u201913 dataset. Top-500s for DRMC-1, DRMC-b and the state-ofthe-art method NFE-13 (Riedel et al., 2013).",
        "context": null
    },
    {
        "title": "Learning syntactic patterns for automatic hypernym discovery",
        "author": [
            "Snow et al.2004] Rion Snow",
            "Daniel Jurafsky",
            "Andrew Y Ng"
        ],
        "venue": "Advances in Neural Information Processing Systems",
        "citeRegEx": "Snow et al\\.,? \\Q2004\\E",
        "shortCiteRegEx": "Snow et al\\.",
        "year": 2004,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Multi-instance multi-label learning for relation extraction",
        "author": [
            "Julie Tibshirani",
            "Ramesh Nallapati",
            "Christopher D Manning"
        ],
        "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Lan-",
        "citeRegEx": "Surdeanu et al\\.,? \\Q2012\\E",
        "shortCiteRegEx": "Surdeanu et al\\.",
        "year": 2012,
        "abstract": "",
        "full_text": "",
        "sentence": " In order to conduct reliable experiments, we adjust and estimate the parameters for our approaches, DRMC-b and DRMC-1, and compare them with other four kinds of landmark methods (Mintz et al., 2009; Hoffmann et al., 2011; Surdeanu et al., 2012; Riedel et al., 2013) on two public datasets. , 2011), MIML-12 and MIML-at-least-one-12 (Surdeanu et al., 2012) on NYT\u201910 dataset. We relax the feature filtering threshold (\u03b8 = 4, 3, 2) in Surdeanu et al.\u2019s (2012) open source program to generate more sparse features from NYT\u201910 dataset.",
        "context": null
    },
    {
        "title": "Reducing wrong labels in distant supervision for relation extraction",
        "author": [
            "Issei Sato",
            "Hiroshi Nakagawa"
        ],
        "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long",
        "citeRegEx": "Takamatsu et al\\.,? \\Q2012\\E",
        "shortCiteRegEx": "Takamatsu et al\\.",
        "year": 2012,
        "abstract": "",
        "full_text": "",
        "sentence": " Other literatures (Takamatsu et al., 2012; Min et al., 2013; Zhang et al., 2013; Xu et al., 2013) addressed more specific issues, like how to construct the negative class in learning or how to adopt more information, such as name entity tags, to improve the performance.",
        "context": null
    },
    {
        "title": "Filling knowledge base gaps for distant supervision of relation extraction",
        "author": [
            "Xu et al.2013] Wei Xu",
            "Raphael Hoffmann",
            "Le Zhao",
            "Ralph Grishman"
        ],
        "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume",
        "citeRegEx": "Xu et al\\.,? \\Q2013\\E",
        "shortCiteRegEx": "Xu et al\\.",
        "year": 2013,
        "abstract": "",
        "full_text": "",
        "sentence": " Other literatures (Takamatsu et al., 2012; Min et al., 2013; Zhang et al., 2013; Xu et al., 2013) addressed more specific issues, like how to construct the negative class in learning or how to adopt more information, such as name entity tags, to improve the performance.",
        "context": null
    },
    {
        "title": "Towards accurate distant supervision for relational facts extraction",
        "author": [
            "Zhang et al.2013] Xingxing Zhang",
            "Jianwen Zhang",
            "Junyu Zeng",
            "Jun Yan",
            "Zheng Chen",
            "Zhifang Sui"
        ],
        "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computa-",
        "citeRegEx": "Zhang et al\\.,? \\Q2013\\E",
        "shortCiteRegEx": "Zhang et al\\.",
        "year": 2013,
        "abstract": "",
        "full_text": "",
        "sentence": " Other literatures (Takamatsu et al., 2012; Min et al., 2013; Zhang et al., 2013; Xu et al., 2013) addressed more specific issues, like how to construct the negative class in learning or how to adopt more information, such as name entity tags, to improve the performance.",
        "context": null
    },
    {
        "title": "Exploring various knowledge in relation extraction",
        "author": [
            "Zhou et al.2005] Guodong Zhou",
            "Jian Su",
            "Jie Zhang",
            "Min Zhang"
        ],
        "venue": "In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,",
        "citeRegEx": "Zhou et al\\.,? \\Q2005\\E",
        "shortCiteRegEx": "Zhou et al\\.",
        "year": 2005,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Traditional supervised methods (Zhou et al., 2005; Bach and Badaskar, 2007) on small hand-labeled corpora, such as MUC1 and ACE2, can achieve high precision and recall.",
        "context": null
    }
]