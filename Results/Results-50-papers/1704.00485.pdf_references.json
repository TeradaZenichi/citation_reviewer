[
    {
        "title": "editors",
        "author": [
            "S. Abiteboul",
            "R. Hull",
            "V. Vianu"
        ],
        "venue": "Foundations of Databases: The Logical Level. Addison-Wesley Longman Publishing Co., Inc., Boston, MA, USA, 1st edition",
        "citeRegEx": "2",
        "shortCiteRegEx": null,
        "year": 1995,
        "abstract": "",
        "full_text": "",
        "sentence": " Embedded multi-valued dependencies (EMVDs) are database dependencies that are more general than functional dependencies [2].",
        "context": null
    },
    {
        "title": "Efficient Algorithms for Identifying Relevant Features",
        "author": [
            "H. Almuallim",
            "T.G. Dietterich"
        ],
        "venue": "Technical report",
        "citeRegEx": "3",
        "shortCiteRegEx": null,
        "year": 1992,
        "abstract": "",
        "full_text": "",
        "sentence": " [40] infers approximate FDs using the dataset instance and exploits them during feature selection, FOCUS [3] is an approach to bias the input and reduce the number of features by performing some computations over those features, while [6] proposes a measure called consistency to aid in feature subset search.",
        "context": null
    },
    {
        "title": "Brainwash: A Data System for Feature Engineering",
        "author": [
            "M. Anderson",
            "D. Antenucci",
            "V. Bittorf",
            "M. Burgess",
            "M.J. Cafarella",
            "A. Kumar",
            "F. Niu",
            "Y. Park",
            "C. R\u00e9",
            "C. Zhang"
        ],
        "venue": "CIDR",
        "citeRegEx": "4",
        "shortCiteRegEx": null,
        "year": 2013,
        "abstract": "",
        "full_text": "",
        "sentence": " However, little work has tackled the pains of sourcing data for ML tasks in the first place, especially, how fundamental data properties affect end-to-end data workflows for ML tasks [4].",
        "context": null
    },
    {
        "title": "SystemML: Declarative Machine Learning on Spark",
        "author": [
            "M. Boehm",
            "M.W. Dusenberry",
            "D. Eriksson",
            "A.V. Evfimievski",
            "F.M. Manshadi",
            "N. Pansare",
            "B. Reinwald",
            "F.R. Reiss",
            "P. Sen",
            "A.C. Surve",
            "S. Tatikonda"
        ],
        "venue": "VLDB",
        "citeRegEx": "5",
        "shortCiteRegEx": null,
        "year": 2016,
        "abstract": "",
        "full_text": "",
        "sentence": " , [5, 28]), and how to use database ideas to improve ML tasks (e.",
        "context": null
    },
    {
        "title": "Consistency based feature selection",
        "author": [
            "M. Dash",
            "H. Liu",
            "H. Motoda"
        ],
        "venue": "Proceedings of the 4th Pacific-Asia Conference on Knowledge Discovery and Data Mining, Current Issues and New Applications, PAKDK, pages 98\u2013109, London, UK, UK",
        "citeRegEx": "6",
        "shortCiteRegEx": null,
        "year": 2000,
        "abstract": "",
        "full_text": "",
        "sentence": " [40] infers approximate FDs using the dataset instance and exploits them during feature selection, FOCUS [3] is an approach to bias the input and reduce the number of features by performing some computations over those features, while [6] proposes a measure called consistency to aid in feature subset search.",
        "context": null
    },
    {
        "title": "Bias of importance measures for multi-valued attributes and solutions",
        "author": [
            "H. Deng",
            "G. Runger",
            "E. Tuv"
        ],
        "venue": "Proceedings of the 21st International Conference on Artificial Neural Networks - Volume Part II, ICANN\u201911, pages 293\u2013300, Berlin, Heidelberg",
        "citeRegEx": "7",
        "shortCiteRegEx": null,
        "year": 2011,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " In contrast to prior work on handling regular large-domain features [7], foreign key features are distinct in that they have coarsergrained side information available in the form of foreign features. Scores such as Gini and information gain are known to be biased towards large-domain features in decision tree learning [7] and different approaches have explored alternatives to solve that issue [17].",
        "context": null
    },
    {
        "title": "Principles of Data Integration",
        "author": [
            "A. Doan",
            "A. Halevy",
            "Z. Ives"
        ],
        "venue": "Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 1st edition",
        "citeRegEx": "8",
        "shortCiteRegEx": null,
        "year": 2012,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Integrating data and features from different sources for ML and data mining algorithms often requires applying and adapting techniques from the data integration literature [27, 8].",
        "context": null
    },
    {
        "title": "A Unified Bias-Variance Decomposition and its Applications",
        "author": [
            "P. Domingos"
        ],
        "venue": "Proceedings of 17th International Conference on Machine Learning",
        "citeRegEx": "9",
        "shortCiteRegEx": null,
        "year": 2000,
        "abstract": "",
        "full_text": "",
        "sentence": " We generate 100 different training datasets and measure the average test error and average net variance (as defined in [9]) based on the different models obtained from these 100 runs.",
        "context": null
    },
    {
        "title": "Big data integration",
        "author": [
            "X.L. Dong",
            "D. Srivastava"
        ],
        "venue": "Proceedings of the VLDB Endowment,",
        "citeRegEx": "10",
        "shortCiteRegEx": "10",
        "year": 2013,
        "abstract": "The Big Data era is upon us: data is being generated, collected and analyzed at an unprecedented scale, and data-driven decision making is sweeping through society. Since the value of data explodes when it can be linked and fused with other data, addressing the big data integration (BDI) challenge is critical to realizing the promise of Big Data.\n          BDI differs from traditional data integration in many dimensions: (i) the number of data sources, even for a single domain, has grown to be in the tens of thousands, (ii) many of the data sources are very dynamic, as a huge amount of newly collected data are continuously made available, (iii) the data sources are extremely heterogeneous in their structure, with considerable variety even for substantially similar entities, and (iv) the data sources are of widely differing qualities, with significant differences in the coverage, accuracy and timeliness of data provided. This tutorial explores the progress that has been made by the data integration community on the topics of schema mapping, record linkage and data fusion in addressing these novel challenges faced by big data integration, and identifies a range of open problems for the community.",
        "full_text": "",
        "sentence": " These include integrating features from different data types in recommendation systems [18], sensor fusion [19], dimensionality reduction during feature fusion [12], and techniques to control data quality during data fusion [10].",
        "context": null
    },
    {
        "title": "Towards a Unified Architecture for in-RDBMS Analytics",
        "author": [
            "X. Feng",
            "A. Kumar",
            "B. Recht",
            "C. R\u00e9"
        ],
        "venue": "Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data, SIGMOD \u201912",
        "citeRegEx": "11",
        "shortCiteRegEx": null,
        "year": 2012,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " , [16, 11, 46]), how to scale ML (e.",
        "context": null
    },
    {
        "title": "Multiple feature fusion by subspace learning",
        "author": [
            "Y. Fu",
            "L. Cao",
            "G. Guo",
            "T.S. Huang"
        ],
        "venue": "Proceedings of the 2008 International Conference on Content-based Image and Video Retrieval, CIVR \u201908, pages 127\u2013134, New York, NY, USA",
        "citeRegEx": "12",
        "shortCiteRegEx": null,
        "year": 2008,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " These include integrating features from different data types in recommendation systems [18], sensor fusion [19], dimensionality reduction during feature fusion [12], and techniques to control data quality during data fusion [10].",
        "context": null
    },
    {
        "title": "Introduction to Statistical Relational Learning)",
        "author": [
            "L. Getoor",
            "B. Taskar"
        ],
        "venue": "The MIT Press",
        "citeRegEx": "13",
        "shortCiteRegEx": null,
        "year": 2007,
        "abstract": "",
        "full_text": "",
        "sentence": " Note that our setting is different from the statistical relational learning (SRL) setting, which deals with joins that violate the IID assumption and duplicate labeled examples from S [13]. There is a large body of work on statistical relational learning (SRL) to handle joins that cause duplicates in the fact table [13].",
        "context": null
    },
    {
        "title": "Feature Extraction: Foundations and Applications",
        "author": [
            "I. Guyon",
            "S. Gunn",
            "M. Nikravesh",
            "L.A. Zadeh"
        ],
        "venue": "New York: Springer-Verlag",
        "citeRegEx": "14",
        "shortCiteRegEx": null,
        "year": 2001,
        "abstract": "",
        "full_text": "",
        "sentence": " The data mining and ML communities have long worked on feature selection methods to improve ML accuracy [14, 15]. The trade-off between feature redundancy and relevancy is well-studied [14, 44, 21]. The conventional wisdom is that even a feature that is redundant might be highly relevant and hence, unavoidable in the mix [14].",
        "context": null
    },
    {
        "title": "The Elements of Statistical Learning: Data mining",
        "author": [
            "T. Hastie",
            "R. Tibshirani",
            "J. Friedman"
        ],
        "venue": "Inference, and Prediction. Springer-Verlag",
        "citeRegEx": "15",
        "shortCiteRegEx": null,
        "year": 2001,
        "abstract": "",
        "full_text": "",
        "sentence": " The data mining and ML communities have long worked on feature selection methods to improve ML accuracy [14, 15]. Unsupervised dimensionality reduction methods such as random hashing or PCA are also popular [15, 29].",
        "context": null
    },
    {
        "title": "The MADlib Analytics Library or MAD Skills",
        "author": [
            "J.M. Hellerstein",
            "C. R\u00e9",
            "F. Schoppmann",
            "D.Z. Wang",
            "E. Fratkin",
            "A. Gorajek",
            "K.S. Ng",
            "C. Welton",
            "X. Feng",
            "K. Li",
            "A. Kumar"
        ],
        "venue": "the SQL. In VLDB",
        "citeRegEx": "16",
        "shortCiteRegEx": null,
        "year": 2012,
        "abstract": "",
        "full_text": "",
        "sentence": " , [16, 11, 46]), how to scale ML (e.",
        "context": null
    },
    {
        "title": "Unbiased recursive partitioning: A conditional inference framework",
        "author": [
            "T. Hothorn",
            "K. Hornik",
            "A. Zeileis"
        ],
        "venue": "JOURNAL OF COMPUTATIONAL AND GRAPHICAL STATISTICS, 15(3):651\u2013674",
        "citeRegEx": "17",
        "shortCiteRegEx": null,
        "year": 2006,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Note that the absolute generalization error is often high, which is expected for decision trees [17]. Scores such as Gini and information gain are known to be biased towards large-domain features in decision tree learning [7] and different approaches have explored alternatives to solve that issue [17].",
        "context": null
    },
    {
        "title": "Visual search at pinterest",
        "author": [
            "Y. Jing",
            "D. Liu",
            "D. Kislyuk",
            "A. Zhai",
            "J. Xu",
            "J. Donahue",
            "S. Tavel"
        ],
        "venue": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201915, pages 1889\u20131898, New York, NY, USA",
        "citeRegEx": "18",
        "shortCiteRegEx": null,
        "year": 2015,
        "abstract": "We demonstrate that, with the availability of distributed computation\nplatforms such as Amazon Web Services and open-source tools, it is possible for\na small engineering team to build, launch and maintain a cost-effective,\nlarge-scale visual search system with widely available tools. We also\ndemonstrate, through a comprehensive set of live experiments at Pinterest, that\ncontent recommendation powered by visual search improve user engagement. By\nsharing our implementation details and the experiences learned from launching a\ncommercial visual search engines from scratch, we hope visual search are more\nwidely incorporated into today's commercial applications.",
        "full_text": "Visual Search at Pinterest\nYushi Jing\n1, David Liu\n1, Dmitry Kislyuk\n1, Andrew Zhai\n1, Jiajing Xu\n1, Jeff Donahue\n1,2,\nSarah Tavel\n1\n1Visual Discovery, Pinterest\n2University of California, Berkeley\n{jing, dliu, dkislyuk, andrew, jiajing, jdonahue, sarah}@pinterest.com\nABSTRACT\nWe demonstrate that, with the availability of distributed\ncomputation platforms such as Amazon Web Services and\nopen-source tools, it is possible for a small engineering team\nto build, launch and maintain a cost-e\ufb00ective, large-scale\nvisual search system with widely available tools. We also\ndemonstrate, through a comprehensive set of live experi-\nments at Pinterest, that content recommendation powered\nby visual search improve user engagement. By sharing our\nimplementation details and the experiences learned from\nlaunching a commercial visual search engines from scratch,\nwe hope visual search are more widely incorporated into to-\nday\u2019s commercial applications.\nPlease see an updated version of the paper, Visual\nDiscovery at Pinterest, presented at World Wide\nWeb (WWW) 2017.\nCategories and Subject Descriptors\nH.3.3 [Information Systems Applications]: Search Pro-\ncess; I.4.9 [Image Processing and Computer Vision]:\nApplication\nGeneral Terms\ninformation retrieval, computer vision, deep learning, dis-\ntributed systems\nKeywords\nvisual search, visual shopping, open source\n1.\nINTRODUCTION\nVisual search, or content-based image retrieval [5], is an\nactive research area driven in part by the explosive growth of\nonline photos and the popularity of search engines. Google\nGoggles, Google Similar Images and Amazon Flow are sev-\neral examples of commercial visual search systems. Although\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor pro\ufb01t or commercial advantage and that copies bear this notice and the full citation\non the \ufb01rst page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior speci\ufb01c permission\nand/or a fee. Request permissions from Permissions@acm.org.\nKDD\u201915, August 10-13, 2015, Sydney, NSW, Australia.\nCopyright is held by the owner/author(s). Publication rights licensed to ACM.\nACM 978-1-4503-3664-2/15/08 ...$15.00.\nDOI: http://dx.doi.org/10.1145/2783258.2788621 .\nFigure 1: Similar Looks: We apply object detection\nto localize products such as bags and shoes. In this\nprototype, users click on objects of interest to view\nsimilar-looking products.\nsigni\ufb01cant progress has been made in building Web-scale vi-\nsual search systems, there are few publications describing\nend-to-end architectures deployed on commercial applica-\ntions. This is in part due to the complexity of real-world\nvisual search systems, and in part due to business consider-\nations to keep core search technology proprietary.\nWe faced two main challenges in deploying a commercial\nvisual search system at Pinterest.\nFirst, as a startup we\nneeded to control the development cost in the form of both\nhuman and computational resources. For example, feature\ncomputation can become expensive with a large and con-\ntinuously growing image collection, and with engineers con-\nstantly experimenting with new features to deploy, it is vital\nfor our system to be both scalable and cost e\ufb00ective. Sec-\nond, the success of a commercial application is measured by\nthe bene\ufb01t it brings to the users (e.g. improved user engage-\nment) relative to the cost of development and maintenance.\nAs a result, our development progress needs to be frequently\nvalidated through A/B experiments with live user tra\ufb03c.\narXiv:1505.07647v3  [cs.CV]  8 Mar 2017\nFigure 2: Related Pins: Pins are selected based on\nthe curation graph.\nIn this paper, we describe our approach to deploy a com-\nmercial visual search system with those two challenges in\nmind. We makes two main contributions.\nOur \ufb01rst contribution is to present our scalable and cost\ne\ufb00ective visual search implementation using widely available\ntools, feasible for a small engineering team to implement.\nSection 2.1 describes our simple and pragmatic approach to\nspeeding up and improving the accuracy of object detection\nand localization that exploits the rich metadata available\nat Pinterest. By decoupling the di\ufb03cult (and computation-\nally expensive) task of multi-class object detection into cat-\negory classi\ufb01cation followed by per-category object detec-\ntion, we only need to run (expensive) object detectors on\nimages with high probability of containing the object. Sec-\ntion 2.2 presents our distributed pipeline to incrementally\nadd or update image features using Amazon Web Services,\nwhich avoids wasteful re-computation of unchanged image\nfeatures. Section 2.3 presents our distributed indexing and\nsearch infrastructure built on top of widely available tools.\nOur second contribution is to share results of deploying\nour visual search infrastructure in two product applications:\nRelated Pins (Section 3) and Similar Looks (Section 4). For\neach application, we use application-speci\ufb01c data sets to\nevaluate the e\ufb00ectiveness of each visual search component\n(object detection, feature representations for similarity) in\nisolation.\nAfter deploying the end-to-end system, we use\nA/B tests to measure user engagement on live tra\ufb03c.\nRelated Pins (Figure 2) is a feature that recommends Pins\nbased on the Pin the user is currently viewing. These rec-\nommendations are primarily generated from the \u201ccuration\ngraph\u201d of users, boards, and Pins. However, there is a long\ntail of less popular Pins without recommendations. Using\nvisual search, we generate recommendations for almost all\nPins on Pinterest. Our second application, Similar Looks\n(Figure 1 ) is a discovery experience we tested speci\ufb01cally\nfor fashion Pins. It allowed users to select a visual query\nfrom regions of interest (e.g. a bag or a pair of shoes) and\nidenti\ufb01ed visually similar Pins for users to explore or pur-\nchase. Instead of using the whole image, visually similarity\nis computed between the localized objects in the query and\ndatabase images. To our knowledge, this is the \ufb01rst pub-\nlished work on object detection/localization in a commer-\ncially deployed visual search system.\nOur experiments demonstrate that 1) one can achieve very\nlow false positive rate (less than 1%) with good detection\nrate by combining the object detection/localization meth-\nods with metadata, and 2) using feature representations\nfrom the VGG [21] [3] model signi\ufb01cantly improves visual\nsearch accuracy on our Pinterest benchmark datasets, and\n3) we observe signi\ufb01cant gains in user engagement when vi-\nsual search is used to power Related Pins and Similar Looks\napplications.\n2.\nVISUAL SEARCH ARCHITECTURE AT\nPINTEREST\nPinterest is a visual bookmarking tool that helps users dis-\ncover and save creative ideas. Users pin images to boards,\nwhich are curated collections around particular themes or\ntopics.\nThis human-curated user-board-image graph con-\ntains a rich set of information about the images and their\nsemantic relations to each other. For example, when an im-\nage is pinned to a board, it is implies a \u201ccuratorial link\u201d\nbetween the new board and all other boards the image ap-\npears in. Metadata, such as image annotations, can then be\npropagated through these links to form a rich description of\nthe image, the image board and the users.\nSince the image is the focus of each pin, visual features\nplay a large role in \ufb01nding interesting, inspiring and relevant\ncontent for users. In this section we describe the end-to-end\nimplementation of a visual search system that indexes bil-\nlions of images on Pinterest. We address the challenges of\ndeveloping a real-world visual search system that balances\ncost constraints with the need for fast prototyping. We de-\nscribe 1) the features that we extract from images, 2) our\ninfrastructure for distributed and incremental feature ex-\ntraction, and 3) our real-time visual search service.\n2.1\nImage Representation and Features\nWe extract a variety of features from images, including\nlocal features and \u201cdeep features\u201d extracted from the activa-\ntion of intermediate layers of a deep convolutional network.\nThe deep features come from convolutional neural networks\n(CNNs) based on the AlexNet [14] and VGG [21] architec-\ntures.\nWe used the feature representations from fc6 and\nfc8 layers. These features are binarized for representation\ne\ufb03ciency and compared using Hamming distance. We use\nFigure 3:\nInstead of running all object detectors\non all images, we \ufb01rst predict the image categories\nusing textual metadata, and then apply object de-\ntection modules speci\ufb01c to the predicted category.\nopen-source Ca\ufb00e [11] to perform training and inference of\nour CNNs on multi-GPU machines.\nThe system also extracts salient color signatures from im-\nages. Salient colors are computed by \ufb01rst detecting salient\nregions [24, 4] of the images and then applying k-means clus-\ntering to the Lab pixel values of the salient pixels. Cluster\ncentroids and weights are stored as the color signature of the\nimage.\nTwo-step Object Detection and Localization\nOne feature that is particularly relevant to Pinterest is the\npresence of certain object classes, such as bags, shoes, watches,\ndresses, and sunglasses. We adopted a two-step detection\napproach that leverages the abundance of weak text labels\non Pinterest images. Since images are pinned many times\nonto many boards, aggregated pin descriptions and board\ntitles provide a great deal of information about the image.\nA text processing pipeline within Pinterest extracts relevant\nannotations for images from the raw text, producing short\nphrases associated with each image.\nWe use these annotations to determine which object detec-\ntors to run. In Figure 1, we \ufb01rst determined that the image\nwas likely to contain bags and shoes, and then proceeded\nto apply visual object detectors for those object classes. By\n\ufb01rst performing category classi\ufb01cation, we only need to run\nthe object detectors on images with a high prior likelihood\nof matching, reducing computational cost as well as false\npositives.\nOur initial approach for object detection was a heavily op-\ntimized implementation of cascading deformable part-based\nmodels [7]. This detector outputs a bounding box for each\ndetected object, from which we extract visual descriptors for\nthe object. Our recent e\ufb00orts have focused on investigat-\ning the feasibility and performance of deep learning based\nobject detectors [8, 9, 6] as a part of our two-step detec-\ntion/localization pipeline.\nOur experiment results in Section 4 show that our sys-\ntem achieved a very low false positive rate (less than 1%),\nwhich was vital for our application. This two-step approach\nalso enables us to incorporate other signals into the category\nclassi\ufb01cation. The use of both text and visual signals for ob-\nject detection and localization is widely used [2] [1] [12] for\nWeb image retrieval and categorization.\nClick Prediction\nWhen users browse on Pinterest, they can interact with a\npin by clicking to view it full screen (\u201cclose-up\u201d) and subse-\nquently clicking through to the o\ufb00-site source of the content\n(a click-through). For each image, we predict close-up rate\nFigure 4: ROC curves for CUR prediction (left) and\nCTR prediction (right).\n(CUR) and click-through rate (CTR) based on its visual fea-\ntures. We trained a CNN to learn a mapping from images\nto the probability of a user bringing up the close-up view or\nclicking through to the content. Both CUR and CTR are\nhelpful for applications like search ranking, recommendation\nsystems and ads targeting since we often need to know which\nimages are more likely to get attention from users based on\ntheir visual content.\nCNNs have recently become the dominant approach to\nmany semantic prediction tasks involving visual inputs, in-\ncluding classi\ufb01cation [15, 14, 22, 3, 20, 13], detection [8,\n9, 6], and segmentation [17]. Training a full CNN to learn\ngood representation can be time-consuming and requires a\nvery large corpus of data.\nWe apply transfer learning to\nour model by retaining the low-level visual representations\nfrom models trained for other computer vision tasks. The\ntop-level layers of the network are \ufb01ne-tuned for our speci\ufb01c\ntask. This saves substantial training time and leverages the\nvisual features learned from a much larger corpus than that\nof the target task. We use Ca\ufb00e to perform this transfer\nlearning.\nFigure 4 depicts receiver operating characteristic (ROC)\ncurves for our CNN-based method, compared with a base-\nline based on a \u201ctraditional\u201d computer vision pipeline: a\nSVM trained with binary labels on a pyramid histogram\nof words (PHOW), which performs well on object recogni-\ntion datasets such as Caltech-101. Our CNN-based approach\noutperforms the PHOW-SVM baseline, and \ufb01ne-tuning the\nCNN from end-to-end yields a signi\ufb01cant performance boost\nas well. A similar approach was also applied to the task of\ndetecting pornographic images uploaded to Pinterest with\ngood results 1.\n2.2\nIncremental Fingerprinting Service\nMost of our vision applications depend on having a com-\nplete collection of image features, stored in a format amenable\nto bulk processing. Keeping this data up-to-date is challeng-\ning; because our collection comprises over a billion unique\nimages, it is critical to update the feature set incrementally\nand avoid unnecessary re-computation whenever possible.\n1By \ufb01ne-tuning a network for three-class classi\ufb01cation of\nignore, softcore, and porn images, we are able to achieve a\nvalidation accuracy of 83.2%. When formulated as a binary\nclassi\ufb01cation between ignore and softcore/porn categories,\nthe classi\ufb01er achieved an AUC score of 93.56%.\n1. Find New Image \nSignatures\nPinterest pin \ndatabase\n2.  Enqueuer\n3. Feature Computation Queue\n(20 - 600 compute nodes)\n4. Sorted Merge\nfor each epoch\n5. VisualJoiner\nSignature Lists\n(initial epoch)\n(delta date epoch)\nIndividual Feature Files\n(initial epoch)\n(delta date epoch)\nFingerprint Files (all features)\n(initial epoch)\n(delta date epoch)\nVisualJoin\n(Random Access Format)\nvisualjoin/00000\n\u2026\nvisualjoin/01800\nsig/dt=2014-xx-xx/{000..999}\ncolor/dt=2014-xx-xx/{000..999}\ndeep/dt=2014-xx-xx/{000..999}\n\u2026\nmerged/dt=2014-xx-xx/{000..999}\nsig/dt=2015-01-06/{000..004}\nmerged/dt=2015-01-06/{000.004}\ncolor/dt=2015-01-06/{000..004}\ndeep/dt=2015-01-06/{000..004}\n\u2026\nwork chunks\nwork chunks recombined\nOther Visual Data Sources\n(visual annotations,\ndeduplicated signature, \u2026)\nFigure 5: Examples of outputs generated by incre-\nmental \ufb01ngerprint update pipeline. The initial run\nis shown as 2014-xx-xx which includes all the images\ncreated before that run.\nWe built a system called the Incremental Fingerprinting\nService, which computes image features for all Pinterest im-\nages using a cluster of workers on Amazon EC2. It incre-\nmentally updates the collection of features under two main\nchange scenarios: new images uploaded to Pinterest, and\nfeature evolution (features added/modi\ufb01ed by engineers).\nOur approach is to split the image collection into epochs\ngrouped by upload date, and to maintain a separate feature\nstore for each version of each feature type (global, local,\ndeep features). Features are stored in bulk on Amazon S3,\norganized by feature type, version, and date.\nWhen the\ndata is fully up-to-date, each feature store contains all the\nepochs. On each run, the system detects missing epochs for\neach feature and enqueues jobs into a distributed queue to\npopulate those epochs.\nThis storage scheme enables incremental updates as fol-\nlows. Every day, a new epoch is added to our collection with\nthat day\u2019s unique uploads, and we generate the missing fea-\ntures for that date. Since old images do not change, their\nfeatures are not recomputed. If the algorithm or parameters\nfor generating a feature are modi\ufb01ed, or if a new feature is\nadded, a new feature store is started and all of the epochs\nare computed for that feature. Unchanged features are not\na\ufb00ected.\nWe copy these features into various forms for more con-\nvenient access by other jobs: features are merged to form a\n\ufb01ngerprint containing all available features of an image, and\n\ufb01ngerprints are copied into sharded, sorted \ufb01les for random\naccess by image signature (MD5 hash). These joined \ufb01nger-\nprint \ufb01les are regularly re-materialized, but the expensive\nfeature computation needs only be done once per image.\nA \ufb02ow chart of the incremental \ufb01ngerprint update pro-\ncess is shown in Figure 5. It consists of \ufb01ve main jobs: job\n(1) compiles a list of newly uploaded image signatures and\ngroups them by date into epochs. We randomly divide each\nepoch into sorted shards of approximately 200,000 images\nto limit the size of the \ufb01nal \ufb01ngerprint \ufb01les. Job (2) identi-\n\ufb01es missing epochs in each feature store and enqueues jobs\ninto PinLater (a distributed queue service similar to Ama-\nzon SQS). The jobs subdivide the shards into\u201cwork chunks\u201d,\ntuned such that each chunk takes approximate 30 minutes to\ncompute. Job (3) runs on an automatically-launched cluster\nof EC2 instances, scaled depending on the size of the update.\nSpot instances can be used; if an instance is terminated, its\njob is rescheduled on another worker. The output of each\nwork chunk is saved onto S3, and eventually recombined into\nfeature \ufb01les corresponding to the original shards.\nJob (4) merges the individual feature shards into a uni\ufb01ed\n\ufb01ngerprint containing all of the available features for each\nimage. Job (5) merges all the epochs into a sorted, sharded\nHFile format allowing for random access.\nThe initial computation of all available features on all im-\nages, takes a little over a day using a cluster of several hun-\ndred 32-core machines, and produces roughly 5 TB of feature\ndata. The steady-state requirement to process new images\nincrementally is only about 5 machines.\n2.3\nSearch Infrastructure\nAt Pinterest, there are several use cases for a distributed\nvisual search system. One use case is to explore similar look-\ning products (Pinterest Similar Looks), and others include\nnear-duplicate detection and content recommendation. In\nall these applications, visually similar results are computed\nfrom distributed indices built on top of the visualjoins gen-\nerated in the previous section. Since each use case has a\ndi\ufb00erent set of performance and cost requirements, our in-\nfrastructure is designed to be \ufb02exible and re-con\ufb01gurable. A\n\ufb02ow chart of the search infrastructure is shown in Figure 6.\nAs the \ufb01rst step we create distributed image indices from\nvisualjoins using Hadoop. Sharded with doc-ID, each ma-\nchine contains indexes (and features) associated with a sub-\nset of the entire image collections. Two types of indexes are\nused: the \ufb01rst is disk stored (and partially memory cached)\ntoken indices with vector-quantized features (e.g. visual vo-\ncabulary) as key, and image doc-id hashes as posting lists.\nThis is analogous to text based image retrieval system ex-\ncept text is replaced by visual tokens. The second is memory\ncached features including both visual and meta-data such as\nimage annotations and \u201ctopic vectors\u201d computed from the\nuser-board-image graph. The \ufb01rst part is used for fast (but\nVisualJoin\nFullM=VisualJoin/part-0000\nFullM=VisualJoin/part-0001\n...\nIndex Visual Features\nToken Index\n\u2026\nShard 1\nLeaf Ranker 1\n( Shard by ImageSignature )\nFeatures Tree\nToken Index\nShard 2\nFeatures Tree\nToken Index\nShard N\nFeatures Tree\nLeaf Ranker 2\nLeaf Ranker N\nMerger\nFigure 6:\nA \ufb02ow chart of the distributed visual\nsearch pipeline.\nimprecise) lookup, and the second part is used for more ac-\ncurate (but slower) ranking re\ufb01nement.\nEach machine runs a leaf ranker, which \ufb01rst computes K-\nnearest-neighbor from the indices and then re-rank the top\ncandidates by computing a score between the query image\nand each of the top candidate images based on additional\nmetadata such as annotations. In some cases the leaf ranker\nskips the token index and directly retrieve the K-nearest-\nneighbor images from the feature tree index using variations\nof approximate KNN such as [18]. A root ranker hosted on\nanother machine will retrieve K top results from each of the\nleaf rankers, and them merge the results and return them\nto the users. To handle new \ufb01ngerprints generated with our\nreal-time feature extractor, we have an online version of the\nvisual search pipeline where a very similar process occurs.\nWith the online version however, the given \ufb01ngerprint is\nqueried on pre-generated indices.\n3.\nAPPLICATION 1: RELATED PINS\nOne of the \ufb01rst applications of Pinterest\u2019s visual search\npipeline was within a recommendations product called Re-\nlated Pins, which recommends other images a user may be\ninterested in when viewing a Pin.\nTraditionally, we have\nused a combination of user-curated image-to-board relation-\nships and content-based signals to generate these recommen-\ndations.\nA problem with this approach, however, is that\ncomputing these recommendations is an o\ufb04ine process, and\nthe image-to-board relationship must already have been cu-\nrated, which may not be the case for our less popular Pins or\nnewly created Pins. As a result, 6% of images at Pinterest\nhave very few or no recommendations. For these images, we\nused the visual search pipeline described previously to gen-\nerate Visual Related Pins based on visual signals as shown\nin Figure 7.\nBefore\nAfter\nFigure 7: Before and after incorporating Visual Re-\nlated Pins\nThe \ufb01rst step of the Visual Related Pins product is to\nuse the local token index built from all existing Pinterest\nimages to detect if we have near duplicates to the query im-\nage. Speci\ufb01cally, given a query image, the system returns\na set of images that are variations of the same image but\naltered through transformation such as resizing, cropping,\nrotation, translation, adding, deleting and altering minor\nparts of the visual contents. Since the resulting images look\nvisually identical to the query image, their recommenda-\ntions are most likely relevant to the query image. In most\ncases, however, we found that there are either no near du-\nplicates detected or the near duplicates do not have enough\nrecommendations. Thus, we focused most of our attention\non retrieving visual search results generated from an index\nbased on deep features.\nStatic Evaluation of Search Relevance\nOur initial Visual Related Pins experiment utilized features\nfrom the original and \ufb01ne-tuned versions of the AlexNet\nmodel in its search infrastructure. However, recent successes\nwith deeper CNN architectures for classi\ufb01cation led us to in-\nvestigate the performance of feature sets from a variety of\nCNN models.\nTo conduct evaluation for visual search, we used the im-\nage annotations associated with the images as proxy for rele-\nvancy. This approach is commonly used for o\ufb04ine evaluation\nof visual search systems [19] in addition to human evalua-\ntion. In this work, we used top text-queries associated each\nimage as testing annotations. We retrieve 3,000 images per\nquery for 1000 queries using Pinterest Search, which yields\na dataset with about 1.6 million unique images. We label\neach image with the query that produced it. A visual search\nresult is assumed to be relevant to a query image if the two\nimages share a label.\nUsing this evaluation dataset, we computed the preci-\nsion@k measure for several feature sets: the original AlexNet\n6th layer fully-connected features (fc6), the fc6 features of\na \ufb01ne-tuned AlexNet model trained with Pinterest product\ndata, GoogLeNet (\u201closs3\u201dlayer output), and the fc6 features\nof the VGG 16-layer network [3]. We also examined com-\nTable 1: Relevance of visual search.\nModel\np@5\np@10\nlatency\nAlexNet FC6\n0.051\n0.040\n193ms\nPinterest FC6\n0.234\n0.210\n234ms\nGoogLeNet\n0.223\n0.202\n1207ms\nVGG 16-layer\n0.302\n0.269\n642ms\nFigure 8:\nVisual Related Pins increases total Re-\nlated Pins repins on Pinterest by 2%.\nbining the score from the aforementioned low-level features\nwith the score from the output vector of the classi\ufb01er layer\n(the semantic features). Table 1 shows p@5 and p@10 per-\nformance of these models using low level features for nearest\nneighbor search, along with the average latency of our vi-\nsual search service (which includes feature extraction for the\nquery image as well as retrieval). We observed a substan-\ntial gain in precision against our evaluation dataset when\nusing the FC6 features of the VGG 16-layer model, with an\nacceptable latency for our applications.\nLive Experiments\nFor our experiment, we set up a system to detect new Pins\nwith few recommendations, query our visual search system,\nand store their results in HBase to serve during Pin close-up.\nOne improvement we built on top of the visual search\nsystem for this experiment was adding a results metadata\nconformity threshold to allow greater precision at the ex-\npense of lower recall. This was important as we feared that\ndelivering poor recommendations to a user would have last-\ning e\ufb00ects on that user\u2019s engagement with Pinterest. This\nwas particularly concerning as our visual recommendations\nare served when viewing newly created Pins, a behavior that\noccurs often in newly joined users. As such we chose to lower\nthe recall if it meant improving relevancy.\nWe launched the experiment initially to 10% of Pinter-\nest eligible live tra\ufb03c. We considered a user to be eligible\nwhen they viewed a Pin close-up that did not have enough\nrecommendations, and triggered a user into either a treat-\nment group where we replaced the Related Pins section with\nvisual search results, or a control group where we did not\nalter the experience. In this experiment, what we measured\nwas the change in total repins in the Related Pins section\nwhere repinning is the action of a user adding an image to\ntheir collections. We chose to measure repins as it is one\nof our top line metrics and a standard metric for measuring\nengagement.\nAfter running the experiment for three months, Visual Re-\nlated Pins increased total repins in the Related Pins product\nby 2% as shown in Figure 8.\n4.\nAPPLICATION 2: SIMILAR LOOKS\nOne of the most popular categories on Pinterest is women\u2019s\nfashion. However, a large percentage of pins in this category\ndo not direct users to a shopping experience, and therefore\naren\u2019t actionable. There are two challenges towards making\nthese Pins actionable: 1) Many pins feature editorial shots\nsuch as \u201cstreet style\u201d out\ufb01ts, which often link to a website\nwith little additional information on the items featured in\nthe image; 2) Pin images often contain multiple objects (e.g.\na woman walking down the street, with a leopard-print bag,\nblack boots, sunglasses, torn jeans, etc.) A user looking at\nthe Pin might be interested in learning more about the bag,\nwhile another user might want to buy their sunglasses.\nUser research revealed this to be a common user frustra-\ntion, and our data indicated that users are much less likely\nto clickthrough to the external Website on women\u2019s fashion\nPins, relative to other categories.\nTo address this problem, we built a product called\u201cSimilar\nLooks\u201d, which localized and classi\ufb01ed fashion objects (Fig-\nure 9). We use object recognition to detect products such as\nbags, shoes, pants, and watches in Pin images. From these\nobjects, we extract visual and semantic features to generate\nproduct recommendations (\u201cSimilar Looks\u201d). A user would\ndiscover the recommendations if there was a red dot on the\nobject in the Pin (see Figure 1). Clicking on the red dot\nloads a feed of Pins featuring visually similar objects (e.g.\nother visually similar blue dresses).\nRelated Work\nApplying visual search to \u201csoft goods\u201d has been explored\nboth within academia and industry. Like.com, Google Shop-\nping and Zappos (owned by Amazon) are a few well-known\napplications of computer vision to fashion recommendations.\nBaidu and Alibaba also launched visual search systems re-\ncently solving similar problems.\nThere is also a growing\namount of research on vision-based fashion recommenda-\ntions [23, 16, 10]. Our approach demonstrates the feasibility\nof an object-based visual search system on tens of millions of\nPinterest users and exposes an interactive search experience\naround these detected objects.\nStatic Evaluation of Object Localization\nThe \ufb01rst step of evaluating our Similar Looks product was\nto investigate our object localization and detection capabil-\nities. We chose to focus on fashion objects because of the\naforementioned business need and because \u201csoft goods\u201d tend\nto have distinctive visual shapes (e.g. shorts, bags, glasses).\nWe collected our evaluation dataset by randomly sampling\na set of images from Pinterest\u2019s women\u2019s fashion category,\nand manually labeling 2,399 fashion objects in 9 categories\n(shoes, dress, glasses, bag, watch, pants, shorts, bikini, earn-\nings) on the images by drawing a rectangular crop over the\nobjects. We observed that shoes, bags, dresses and pants\nwere the four largest categories in our evaluation dataset.\nShown in Table 2 is the distribution of fashion objects as\nwell as the detection accuracies from the text-based \ufb01lter,\nimage-based detection, and the combined approach (where\ntext \ufb01lters are applied prior to object detection).\nFigure 9: Once a user clicks on the red dot, the sys-\ntem shows products that have a similar appearance\nto the query object.\nTable 2:\nObject detection/classi\ufb01cation accuracy\n(%)\nText\nImg\nBoth\nObjects\n#\nTP\nFP\nTP\nFP\nTP\nFP\nshoe\n873\n79.8\n6.0\n41.8\n3.1\n34.4\n1.0\ndress\n383\n75.5\n6.2\n58.8\n12.3\n47.0\n2.0\nglasses\n238\n75.2\n18.8\n63.0\n0.4\n50.0\n0.2\nbag\n468\n66.2\n5.3\n59.8\n2.9\n43.6\n0.5\nwatch\n36\n55.6\n6.0\n66.7\n0.5\n41.7\n0.0\npants\n253\n75.9\n2.0\n60.9\n2.2\n48.2\n0.1\nshorts\n89\n73.0\n10.1\n44.9\n1.2\n31.5\n0.2\nbikini\n32\n71.9\n1.0\n31.3\n0.2\n28.1\n0.0\nearrings\n27\n81.5\n4.7\n18.5\n0.0\n18.5\n0.0\nAverage\n72.7\n6.7\n49.5\n2.5\n38.1\n0.5\nAs previously described, the text-based approach applies\nmanually crafted rules (e.g. regular expressions) to the Pin-\nterest meta-data associated with images (which we treat as\nweak labels). For example, an image annotated with \u201cspring\nfashion, tote with \ufb02owers\u201d will be classi\ufb01ed as \u201cbag,\u201d and is\nconsidered as a positive sample if the image contains a \u201cbag\u201d\nobject box label. For image-based evaluation, we compute\nthe intersection between the predicted object bounding box\nand the labeled object bounding box of the same type, and\ncount an intersection to union ratio of 0.3 or greater as a\npositive match.\nTable 2 demonstrates that neither text annotation \ufb01lters\nnor object localization alone were su\ufb03cient for our detection\ntask due to their relatively high false positive rates at 6.7%\nand 2.5% respectively. Not surprisingly, combining two ap-\nproaches signi\ufb01cantly decreased our false positive rate to less\nthan 1%.\nSpeci\ufb01cally, we saw that for classes like \u201cglasses\u201d text an-\nnotations were insu\ufb03cient and image-based classi\ufb01cation ex-\ncelled (due to a distinctive visual shape of glasses). For other\nclasses, such as \u201cdress\u201d, this situation was reversed (the false\npositive rate for our dress detector was high, 12.3%, due\nto occlusion and high variance in style for that class, and\nadding a text-\ufb01lter dramatically improved results). Aside\nfrom reducing the number of images we needed to \ufb01nger-\nprint with our object classi\ufb01ers, for several object classes\n(shoe, bag, pants), we observed that text-pre\ufb01ltering was\ncrucial to achieve an acceptable false positive rate (1% or\nless).\nLive Experiments\nOur system identi\ufb01ed over 80 million\u201cclickable\u201dobjects from\na subset of Pinterest images. A clickable red dot is placed\nupon the detected object. Once the user clicks on the dot,\nour visual search system retrieves a collection of Pins most\nvisually similar to the object. We launched the system to a\nsmall percentage of Pinterest live tra\ufb03c and collected user\nengagement metrics such as CTR for a period of one month.\nSpeci\ufb01cally, we looked at the clickthrough rate of the dot,\nthe clickthrough rate on our visual search results, and also\ncompared engagement on Similar Look results with the ex-\nisting Related Pin recommendations.\nAs shown in Figure 10, an average of 12% of users who\nviewed a pin with a dot clicked on a dot in a given day.\nThose users went on to click on an average 0.55 Similar\nLook results.\nAlthough this data was encouraging, when\nwe compared engagement with all related content on the\npin close-up (summing both engagement with Related Pins\nand Similar Look results for the treatment group, and just\nrelated pin engagement for the control), Similar Looks ac-\ntually hurt overall engagement on the pin close-up by 4%.\nAfter the novelty e\ufb00ort wore o\ufb00, we saw gradual decrease in\nCTR on the red dots which stabilizes at around 10%.\nTo test the relevance of our Similar Looks results inde-\npendently of the bias resulting from the introduction of a\nnew user behavior (learning to click on the \u201cobject dots\u201d),\nwe designed an experiment to blend Similar Looks results\ndirectly into the existing Related Pins product (for Pins\ncontaining detected objects). This gave us a way to directly\nmeasure if users found our visually similar recommendations\nrelevant, compared to our non-visual recommendations. On\npins where we detected an object, this experiment increased\noverall engagement (repins and close-ups) in Related Pins by\n5%. Although we set an initial static blending ratio for this\nexperiment (one visually similar result to three production\nresults), this ratio adjusts in response to user click data.\n5.\nCONCLUSION AND FUTURE WORK\nWe demonstrate that, with the availability of distributed\ncomputational platforms such as Amazon Web Services and\nopen-source tools, it is possible for a handful of engineers or\nan academic lab to build a large-scale visual search system\nusing a combination of non-proprietary tools. This paper\npresented our end-to-end visual search pipeline, including\nincremental feature updating and two-step object detection\nand localization method that improves search accuracy and\nreduces development and deployment costs. Our live prod-\nuct experiments demonstrate that visual search features can\nincrease user engagement.\nWe plan to further improve our system in the following ar-\neas. First, we are interested in investigating the performance\nand e\ufb03ciency of CNN based object detection methods in the\ncontext of live visual search systems. Second, we are inter-\nested in leveraging Pinterest \u201ccuration graph\u201d to enhance\nFigure 10: Engagement rates for Similar Looks ex-\nperiment\nvisual search relevance. Lastly, we want to experiment with\nalternative interactive interfaces for visual search.\n6.\nREFERENCES\n[1] S. Bengio, J. Dean, D. Erhan, E. Ie, Q. V. Le,\nA. Rabinovich, J. Shlens, and Y. Singer. Using web\nco-occurrence statistics for improving image\ncategorization. CoRR, abs/1312.5697, 2013.\n[2] T. L. Berg, A. C. Berg, J. Edwards, M. Maire,\nR. White, Y.-W. Teh, E. Learned-Miller, and D. A.\nForsyth. Names and faces in the news. In Proceedings\nof the Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 848\u2013854, 2004.\n[3] K. Chat\ufb01eld, K. Simonyan, A. Vedaldi, and\nA. Zisserman. Return of the devil in the details:\nDelving deep into convolutional nets. In British\nMachine Vision Conference, 2014.\n[4] M. Cheng, N. Mitra, X. Huang, P. H. S. Torr, and\nS. Hu. Global contrast based salient region detection.\nTransactions on Pattern Analysis and Machine\nIntelligence (T-PAMI), 2014.\n[5] R. Datta, D. Joshi, J. Li, and J. Wang. Image\nretrieval: Ideas, in\ufb02uences, and trends of the new age.\nACM Computing Survey, 40(2):5:1\u20135:60, May 2008.\n[6] D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov.\nScalable object detection using deep neural networks.\nIn 2014 IEEE Conference on Computer Vision and\nPattern Recognition, CVPR 2014, Columbus, OH,\nUSA, June 23-28, 2014, pages 2155\u20132162, 2014.\n[7] P. F. Felzenszwalb, R. B. Girshick, and D. A.\nMcAllester. Cascade object detection with deformable\npart models. In The IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), pages\n2241\u20132248, 2010.\n[8] R. Girshick, J. Donahue, T. Darrell, and J. Malik.\nRich feature hierarchies for accurate object detection\nand semantic segmentation. arXiv preprint\narXiv:1311.2524, 2013.\n[9] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid\npooling in deep convolutional networks for visual\nrecognition. In Transactions on Pattern Analysis and\nFigure 11:\nExamples of object search results for\nshoes. Boundaries of detected objects are automati-\ncally highlighted. The top image is the query image.\nFigure 12: Samples of object detection and localization results for bags. [Green: ground truth, blue: detected\nobjects.]\nFigure 13: Samples of object detection and localization results for shoes.\nMachine Intelligence (T-PAMI), pages 346\u2013361.\nSpringer, 2014.\n[10] V. Jagadeesh, R. Piramuthu, A. Bhardwaj, W. Di, and\nN. Sundaresan. Large scale visual recommendations\nfrom street fashion images. In Proceedings of the\nInternational Conference on Knowledge Discovery and\nData Mining (SIGKDD), 14, pages 1925\u20131934, 2014.\n[11] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev,\nJ. Long, R. Girshick, S. Guadarrama, and T. Darrell.\nCa\ufb00e: Convolutional architecture for fast feature\nembedding. arXiv preprint arXiv:1408.5093, 2014.\n[12] Y. Jing and S. Baluja. Visualrank: Applying pagerank\nto large-scale image search. IEEE Transactions on\nPattern Analysis and Machine Intelligence (T-PAMI),\n30(11):1877\u20131890, 2008.\n[13] A. Karpathy, G. Toderici, S. Shetty, T. Leung,\nR. Sukthankar, and L. Fei-Fei. Large-scale video\nclassi\ufb01cation with convolutional neural networks. In\n2014 IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), pages 1725\u20131732, 2014.\n[14] A. Krizhevsky, S. Ilya, and G. E. Hinton. Imagenet\nclassi\ufb01cation with deep convolutional neural networks.\nIn Advances in Neural Information Processing Systems\n(NIPS), pages 1097\u20131105. 2012.\n[15] Y. LeCun, B. Boser, J. S. Denker, D. Henderson,\nR. E. Howard, W. Hubbard, and L. D. Jackel.\nBackpropagation applied to handwritten zip code\nrecognition. Neural Comput., 1(4):541\u2013551, Dec. 1989.\n[16] S. Liu, Z. Song, M. Wang, C. Xu, H. Lu, and S. Yan.\nStreet-to-shop: Cross-scenario clothing retrieval via\nparts alignment and auxiliary set. In Proceedings of\nthe IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2012.\n[17] J. Long, E. Shelhamer, and T. Darrell. Fully\nconvolutional networks for semantic segmentation.\narXiv preprint arXiv:1411.4038, 2014.\n[18] M. Muja and D. G. Lowe. Fast matching of binary\nfeatures. In Proceedings of the Conference on\nComputer and Robot Vision (CRV), 12, pages\n404\u2013410, Washington, DC, USA, 2012. IEEE\nComputer Society.\n[19] H. M\u00a8uller, W. M\u00a8uller, D. M. Squire,\nS. Marchand-Maillet, and T. Pun. Performance\nevaluation in content-based image retrieval: Overview\nand proposals. Pattern Recognition Letter,\n22(5):593\u2013601, 2001.\n[20] O. Russakovsky, J. Deng, H. Su, J. Krause,\nS. Satheesh, S. Ma, Z. Huang, A. Karpathy,\nA. Khosla, M. Bernstein, et al. ImageNet large scale\nvisual recognition challenge. arXiv preprint\narXiv:1409.0575, 2014.\n[21] K. Simonyan and A. Zisserman. Very deep\nconvolutional networks for large-scale image\nrecognition. CoRR, abs/1409.1556, 2014.\n[22] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,\nD. Anguelov, D. Erhan, V. Vanhoucke, and\nA. Rabinovich. Going deeper with convolutions. arXiv\npreprint arXiv:1409.4842, 2014.\n[23] K. Yamaguchi, M. H. Kiapour, L. Ortiz, and T. Berg.\nRetrieving similar styles to parse clothing.\nTransactions on Pattern Analysis and Machine\nIntelligence (T-PAMI), 2014.\n[24] Q. Yan, L. Xu, J. Shi, and J. Jia. Hierarchical saliency\ndetection. In Proceedings of the IEEE Computer\nSociety Conference on Computer Vision and Pattern\nRecognition (CVPR), 13, pages 1155\u20131162,\nWashington, DC, USA, 2013.\n",
        "sentence": " These include integrating features from different data types in recommendation systems [18], sensor fusion [19], dimensionality reduction during feature fusion [12], and techniques to control data quality during data fusion [10].",
        "context": "retrieval: Ideas, in\ufb02uences, and trends of the new age.\nACM Computing Survey, 40(2):5:1\u20135:60, May 2008.\n[6] D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov.\nScalable object detection using deep neural networks.\nnear-duplicate detection and content recommendation. In\nall these applications, visually similar results are computed\nfrom distributed indices built on top of the visualjoins gen-\nerated in the previous section. Since each use case has a\ntraction, and 3) our real-time visual search service.\n2.1\nImage Representation and Features\nWe extract a variety of features from images, including\nlocal features and \u201cdeep features\u201d extracted from the activa-"
    },
    {
        "title": "Multisensor data fusion: A review of the state-of-the-art",
        "author": [
            "B. Khaleghi",
            "A. Khamis",
            "F.O. Karray",
            "S.N. Razavi"
        ],
        "venue": "Information Fusion,",
        "citeRegEx": "19",
        "shortCiteRegEx": "19",
        "year": 2013,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " These include integrating features from different data types in recommendation systems [18], sensor fusion [19], dimensionality reduction during feature fusion [12], and techniques to control data quality during data fusion [10].",
        "context": null
    },
    {
        "title": "Adam: A Method for Stochastic Optimization",
        "author": [
            "D.P. Kingma",
            "J. Ba"
        ],
        "venue": "3rd International Conference for Learning Representations (ICLR)",
        "citeRegEx": "20",
        "shortCiteRegEx": null,
        "year": 2015,
        "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization\nof stochastic objective functions, based on adaptive estimates of lower-order\nmoments. The method is straightforward to implement, is computationally\nefficient, has little memory requirements, is invariant to diagonal rescaling\nof the gradients, and is well suited for problems that are large in terms of\ndata and/or parameters. The method is also appropriate for non-stationary\nobjectives and problems with very noisy and/or sparse gradients. The\nhyper-parameters have intuitive interpretations and typically require little\ntuning. Some connections to related algorithms, on which Adam was inspired, are\ndiscussed. We also analyze the theoretical convergence properties of the\nalgorithm and provide a regret bound on the convergence rate that is comparable\nto the best known results under the online convex optimization framework.\nEmpirical results demonstrate that Adam works well in practice and compares\nfavorably to other stochastic optimization methods. Finally, we discuss AdaMax,\na variant of Adam based on the infinity norm.",
        "full_text": "Published as a conference paper at ICLR 2015\nADAM: A METHOD FOR STOCHASTIC OPTIMIZATION\nDiederik P. Kingma*\nUniversity of Amsterdam, OpenAI\ndpkingma@openai.com\nJimmy Lei Ba\u2217\nUniversity of Toronto\njimmy@psi.utoronto.ca\nABSTRACT\nWe introduce Adam, an algorithm for \ufb01rst-order gradient-based optimization of\nstochastic objective functions, based on adaptive estimates of lower-order mo-\nments. The method is straightforward to implement, is computationally ef\ufb01cient,\nhas little memory requirements, is invariant to diagonal rescaling of the gradients,\nand is well suited for problems that are large in terms of data and/or parameters.\nThe method is also appropriate for non-stationary objectives and problems with\nvery noisy and/or sparse gradients. The hyper-parameters have intuitive interpre-\ntations and typically require little tuning. Some connections to related algorithms,\non which Adam was inspired, are discussed. We also analyze the theoretical con-\nvergence properties of the algorithm and provide a regret bound on the conver-\ngence rate that is comparable to the best known results under the online convex\noptimization framework. Empirical results demonstrate that Adam works well in\npractice and compares favorably to other stochastic optimization methods. Finally,\nwe discuss AdaMax, a variant of Adam based on the in\ufb01nity norm.\n1\nINTRODUCTION\nStochastic gradient-based optimization is of core practical importance in many \ufb01elds of science and\nengineering. Many problems in these \ufb01elds can be cast as the optimization of some scalar parameter-\nized objective function requiring maximization or minimization with respect to its parameters. If the\nfunction is differentiable w.r.t. its parameters, gradient descent is a relatively ef\ufb01cient optimization\nmethod, since the computation of \ufb01rst-order partial derivatives w.r.t. all the parameters is of the same\ncomputational complexity as just evaluating the function. Often, objective functions are stochastic.\nFor example, many objective functions are composed of a sum of subfunctions evaluated at different\nsubsamples of data; in this case optimization can be made more ef\ufb01cient by taking gradient steps\nw.r.t. individual subfunctions, i.e. stochastic gradient descent (SGD) or ascent. SGD proved itself\nas an ef\ufb01cient and effective optimization method that was central in many machine learning success\nstories, such as recent advances in deep learning (Deng et al., 2013; Krizhevsky et al., 2012; Hinton\n& Salakhutdinov, 2006; Hinton et al., 2012a; Graves et al., 2013). Objectives may also have other\nsources of noise than data subsampling, such as dropout (Hinton et al., 2012b) regularization. For\nall such noisy objectives, ef\ufb01cient stochastic optimization techniques are required. The focus of this\npaper is on the optimization of stochastic objectives with high-dimensional parameters spaces. In\nthese cases, higher-order optimization methods are ill-suited, and discussion in this paper will be\nrestricted to \ufb01rst-order methods.\nWe propose Adam, a method for ef\ufb01cient stochastic optimization that only requires \ufb01rst-order gra-\ndients with little memory requirement. The method computes individual adaptive learning rates for\ndifferent parameters from estimates of \ufb01rst and second moments of the gradients; the name Adam\nis derived from adaptive moment estimation. Our method is designed to combine the advantages\nof two recently popular methods: AdaGrad (Duchi et al., 2011), which works well with sparse gra-\ndients, and RMSProp (Tieleman & Hinton, 2012), which works well in on-line and non-stationary\nsettings; important connections to these and other stochastic optimization methods are clari\ufb01ed in\nsection 5. Some of Adam\u2019s advantages are that the magnitudes of parameter updates are invariant to\nrescaling of the gradient, its stepsizes are approximately bounded by the stepsize hyperparameter,\nit does not require a stationary objective, it works with sparse gradients, and it naturally performs a\nform of step size annealing.\n\u2217Equal contribution. Author ordering determined by coin \ufb02ip over a Google Hangout.\n1\narXiv:1412.6980v9  [cs.LG]  30 Jan 2017\nPublished as a conference paper at ICLR 2015\nAlgorithm 1: Adam, our proposed algorithm for stochastic optimization. See section 2 for details,\nand for a slightly more ef\ufb01cient (but less clear) order of computation. g2\nt indicates the elementwise\nsquare gt \u2299gt. Good default settings for the tested machine learning problems are \u03b1 = 0.001,\n\u03b21 = 0.9, \u03b22 = 0.999 and \u03f5 = 10\u22128. All operations on vectors are element-wise. With \u03b2t\n1 and \u03b2t\n2\nwe denote \u03b21 and \u03b22 to the power t.\nRequire: \u03b1: Stepsize\nRequire: \u03b21, \u03b22 \u2208[0, 1): Exponential decay rates for the moment estimates\nRequire: f(\u03b8): Stochastic objective function with parameters \u03b8\nRequire: \u03b80: Initial parameter vector\nm0 \u21900 (Initialize 1st moment vector)\nv0 \u21900 (Initialize 2nd moment vector)\nt \u21900 (Initialize timestep)\nwhile \u03b8t not converged do\nt \u2190t + 1\ngt \u2190\u2207\u03b8ft(\u03b8t\u22121) (Get gradients w.r.t. stochastic objective at timestep t)\nmt \u2190\u03b21 \u00b7 mt\u22121 + (1 \u2212\u03b21) \u00b7 gt (Update biased \ufb01rst moment estimate)\nvt \u2190\u03b22 \u00b7 vt\u22121 + (1 \u2212\u03b22) \u00b7 g2\nt (Update biased second raw moment estimate)\nbmt \u2190mt/(1 \u2212\u03b2t\n1) (Compute bias-corrected \ufb01rst moment estimate)\nbvt \u2190vt/(1 \u2212\u03b2t\n2) (Compute bias-corrected second raw moment estimate)\n\u03b8t \u2190\u03b8t\u22121 \u2212\u03b1 \u00b7 bmt/(\u221abvt + \u03f5) (Update parameters)\nend while\nreturn \u03b8t (Resulting parameters)\nIn section 2 we describe the algorithm and the properties of its update rule. Section 3 explains\nour initialization bias correction technique, and section 4 provides a theoretical analysis of Adam\u2019s\nconvergence in online convex programming. Empirically, our method consistently outperforms other\nmethods for a variety of models and datasets, as shown in section 6. Overall, we show that Adam is\na versatile algorithm that scales to large-scale high-dimensional machine learning problems.\n2\nALGORITHM\nSee algorithm 1 for pseudo-code of our proposed algorithm Adam. Let f(\u03b8) be a noisy objec-\ntive function: a stochastic scalar function that is differentiable w.r.t. parameters \u03b8. We are in-\nterested in minimizing the expected value of this function, E[f(\u03b8)] w.r.t. its parameters \u03b8. With\nf1(\u03b8), ..., , fT (\u03b8) we denote the realisations of the stochastic function at subsequent timesteps\n1, ..., T. The stochasticity might come from the evaluation at random subsamples (minibatches)\nof datapoints, or arise from inherent function noise. With gt = \u2207\u03b8ft(\u03b8) we denote the gradient, i.e.\nthe vector of partial derivatives of ft, w.r.t \u03b8 evaluated at timestep t.\nThe algorithm updates exponential moving averages of the gradient (mt) and the squared gradient\n(vt) where the hyper-parameters \u03b21, \u03b22 \u2208[0, 1) control the exponential decay rates of these moving\naverages. The moving averages themselves are estimates of the 1st moment (the mean) and the\n2nd raw moment (the uncentered variance) of the gradient. However, these moving averages are\ninitialized as (vectors of) 0\u2019s, leading to moment estimates that are biased towards zero, especially\nduring the initial timesteps, and especially when the decay rates are small (i.e. the \u03b2s are close to 1).\nThe good news is that this initialization bias can be easily counteracted, resulting in bias-corrected\nestimates bmt and bvt. See section 3 for more details.\nNote that the ef\ufb01ciency of algorithm 1 can, at the expense of clarity, be improved upon by changing\nthe order of computation, e.g. by replacing the last three lines in the loop with the following lines:\n\u03b1t = \u03b1 \u00b7\np\n1 \u2212\u03b2t\n2/(1 \u2212\u03b2t\n1) and \u03b8t \u2190\u03b8t\u22121 \u2212\u03b1t \u00b7 mt/(\u221avt + \u02c6\u03f5).\n2.1\nADAM\u2019S UPDATE RULE\nAn important property of Adam\u2019s update rule is its careful choice of stepsizes. Assuming \u03f5 = 0, the\neffective step taken in parameter space at timestep t is \u2206t = \u03b1 \u00b7 bmt/\u221abvt. The effective stepsize has\ntwo upper bounds: |\u2206t| \u2264\u03b1 \u00b7 (1 \u2212\u03b21)/\u221a1 \u2212\u03b22 in the case (1 \u2212\u03b21) > \u221a1 \u2212\u03b22, and |\u2206t| \u2264\u03b1\n2\nPublished as a conference paper at ICLR 2015\notherwise. The \ufb01rst case only happens in the most severe case of sparsity: when a gradient has\nbeen zero at all timesteps except at the current timestep. For less sparse cases, the effective stepsize\nwill be smaller. When (1 \u2212\u03b21) = \u221a1 \u2212\u03b22 we have that | bmt/\u221abvt| < 1 therefore |\u2206t| < \u03b1. In\nmore common scenarios, we will have that bmt/\u221abvt \u2248\u00b11 since |E[g]/\np\nE[g2]| \u22641. The effective\nmagnitude of the steps taken in parameter space at each timestep are approximately bounded by\nthe stepsize setting \u03b1, i.e., |\u2206t| \u2a85\u03b1. This can be understood as establishing a trust region around\nthe current parameter value, beyond which the current gradient estimate does not provide suf\ufb01cient\ninformation. This typically makes it relatively easy to know the right scale of \u03b1 in advance. For\nmany machine learning models, for instance, we often know in advance that good optima are with\nhigh probability within some set region in parameter space; it is not uncommon, for example, to\nhave a prior distribution over the parameters. Since \u03b1 sets (an upper bound of) the magnitude of\nsteps in parameter space, we can often deduce the right order of magnitude of \u03b1 such that optima\ncan be reached from \u03b80 within some number of iterations. With a slight abuse of terminology,\nwe will call the ratio bmt/\u221abvt the signal-to-noise ratio (SNR). With a smaller SNR the effective\nstepsize \u2206t will be closer to zero. This is a desirable property, since a smaller SNR means that\nthere is greater uncertainty about whether the direction of bmt corresponds to the direction of the true\ngradient. For example, the SNR value typically becomes closer to 0 towards an optimum, leading\nto smaller effective steps in parameter space: a form of automatic annealing. The effective stepsize\n\u2206t is also invariant to the scale of the gradients; rescaling the gradients g with factor c will scale bmt\nwith a factor c and bvt with a factor c2, which cancel out: (c \u00b7 bmt)/(\u221a\nc2 \u00b7 bvt) = bmt/\u221abvt.\n3\nINITIALIZATION BIAS CORRECTION\nAs explained in section 2, Adam utilizes initialization bias correction terms. We will here derive\nthe term for the second moment estimate; the derivation for the \ufb01rst moment estimate is completely\nanalogous. Let g be the gradient of the stochastic objective f, and we wish to estimate its second\nraw moment (uncentered variance) using an exponential moving average of the squared gradient,\nwith decay rate \u03b22. Let g1, ..., gT be the gradients at subsequent timesteps, each a draw from an\nunderlying gradient distribution gt \u223cp(gt). Let us initialize the exponential moving average as\nv0 = 0 (a vector of zeros). First note that the update at timestep t of the exponential moving average\nvt = \u03b22 \u00b7 vt\u22121 + (1 \u2212\u03b22) \u00b7 g2\nt (where g2\nt indicates the elementwise square gt \u2299gt) can be written as\na function of the gradients at all previous timesteps:\nvt = (1 \u2212\u03b22)\nt\nX\ni=1\n\u03b2t\u2212i\n2\n\u00b7 g2\ni\n(1)\nWe wish to know how E[vt], the expected value of the exponential moving average at timestep t,\nrelates to the true second moment E[g2\nt ], so we can correct for the discrepancy between the two.\nTaking expectations of the left-hand and right-hand sides of eq. (1):\nE[vt] = E\n\"\n(1 \u2212\u03b22)\nt\nX\ni=1\n\u03b2t\u2212i\n2\n\u00b7 g2\ni\n#\n(2)\n= E[g2\nt ] \u00b7 (1 \u2212\u03b22)\nt\nX\ni=1\n\u03b2t\u2212i\n2\n+ \u03b6\n(3)\n= E[g2\nt ] \u00b7 (1 \u2212\u03b2t\n2) + \u03b6\n(4)\nwhere \u03b6 = 0 if the true second moment E[g2\ni ] is stationary; otherwise \u03b6 can be kept small since\nthe exponential decay rate \u03b21 can (and should) be chosen such that the exponential moving average\nassigns small weights to gradients too far in the past. What is left is the term (1 \u2212\u03b2t\n2) which is\ncaused by initializing the running average with zeros. In algorithm 1 we therefore divide by this\nterm to correct the initialization bias.\nIn case of sparse gradients, for a reliable estimate of the second moment one needs to average over\nmany gradients by chosing a small value of \u03b22; however it is exactly this case of small \u03b22 where a\nlack of initialisation bias correction would lead to initial steps that are much larger.\n3\nPublished as a conference paper at ICLR 2015\n4\nCONVERGENCE ANALYSIS\nWe analyze the convergence of Adam using the online learning framework proposed in (Zinkevich,\n2003). Given an arbitrary, unknown sequence of convex cost functions f1(\u03b8), f2(\u03b8),..., fT (\u03b8). At\neach time t, our goal is to predict the parameter \u03b8t and evaluate it on a previously unknown cost\nfunction ft. Since the nature of the sequence is unknown in advance, we evaluate our algorithm\nusing the regret, that is the sum of all the previous difference between the online prediction ft(\u03b8t)\nand the best \ufb01xed point parameter ft(\u03b8\u2217) from a feasible set X for all the previous steps. Concretely,\nthe regret is de\ufb01ned as:\nR(T) =\nT\nX\nt=1\n[ft(\u03b8t) \u2212ft(\u03b8\u2217)]\n(5)\nwhere \u03b8\u2217= arg min\u03b8\u2208X\nPT\nt=1 ft(\u03b8). We show Adam has O(\n\u221a\nT) regret bound and a proof is given\nin the appendix. Our result is comparable to the best known bound for this general convex online\nlearning problem. We also use some de\ufb01nitions simplify our notation, where gt \u225c\u2207ft(\u03b8t) and gt,i\nas the ith element. We de\ufb01ne g1:t,i \u2208Rt as a vector that contains the ith dimension of the gradients\nover all iterations till t, g1:t,i = [g1,i, g2,i, \u00b7 \u00b7 \u00b7 , gt,i]. Also, we de\ufb01ne \u03b3 \u225c\n\u03b22\n1\n\u221a\u03b22 . Our following\ntheorem holds when the learning rate \u03b1t is decaying at a rate of t\u22121\n2 and \ufb01rst moment running\naverage coef\ufb01cient \u03b21,t decay exponentially with \u03bb, that is typically close to 1, e.g. 1 \u221210\u22128.\nTheorem 4.1. Assume that the function ft has bounded gradients, \u2225\u2207ft(\u03b8)\u22252 \u2264G, \u2225\u2207ft(\u03b8)\u2225\u221e\u2264\nG\u221efor all \u03b8 \u2208Rd and distance between any \u03b8t generated by Adam is bounded, \u2225\u03b8n \u2212\u03b8m\u22252 \u2264D,\n\u2225\u03b8m \u2212\u03b8n\u2225\u221e\u2264D\u221efor any m, n \u2208{1, ..., T}, and \u03b21, \u03b22 \u2208[0, 1) satisfy\n\u03b22\n1\n\u221a\u03b22 < 1. Let \u03b1t =\n\u03b1\n\u221a\nt\nand \u03b21,t = \u03b21\u03bbt\u22121, \u03bb \u2208(0, 1). Adam achieves the following guarantee, for all T \u22651.\nR(T) \u2264\nD2\n2\u03b1(1 \u2212\u03b21)\nd\nX\ni=1\np\nTbvT,i+\n\u03b1(1 + \u03b21)G\u221e\n(1 \u2212\u03b21)\u221a1 \u2212\u03b22(1 \u2212\u03b3)2\nd\nX\ni=1\n\u2225g1:T,i\u22252+\nd\nX\ni=1\nD2\n\u221eG\u221e\n\u221a1 \u2212\u03b22\n2\u03b1(1 \u2212\u03b21)(1 \u2212\u03bb)2\nOur Theorem 4.1 implies when the data features are sparse and bounded gradients, the sum-\nmation term can be much smaller than its upper bound Pd\ni=1 \u2225g1:T,i\u22252\n<< dG\u221e\n\u221a\nT and\nPd\ni=1\np\nTbvT,i << dG\u221e\n\u221a\nT, in particular if the class of function and data features are in the form of\nsection 1.2 in (Duchi et al., 2011). Their results for the expected value E[Pd\ni=1 \u2225g1:T,i\u22252] also apply\nto Adam. In particular, the adaptive method, such as Adam and Adagrad, can achieve O(log d\n\u221a\nT),\nan improvement over O(\n\u221a\ndT) for the non-adaptive method. Decaying \u03b21,t towards zero is impor-\ntant in our theoretical analysis and also matches previous empirical \ufb01ndings, e.g. (Sutskever et al.,\n2013) suggests reducing the momentum coef\ufb01cient in the end of training can improve convergence.\nFinally, we can show the average regret of Adam converges,\nCorollary 4.2. Assume that the function ft has bounded gradients, \u2225\u2207ft(\u03b8)\u22252 \u2264G, \u2225\u2207ft(\u03b8)\u2225\u221e\u2264\nG\u221efor all \u03b8 \u2208Rd and distance between any \u03b8t generated by Adam is bounded, \u2225\u03b8n \u2212\u03b8m\u22252 \u2264D,\n\u2225\u03b8m \u2212\u03b8n\u2225\u221e\u2264D\u221efor any m, n \u2208{1, ..., T}. Adam achieves the following guarantee, for all\nT \u22651.\nR(T)\nT\n= O( 1\n\u221a\nT\n)\nThis result can be obtained by using Theorem 4.1 and Pd\ni=1 \u2225g1:T,i\u22252 \u2264dG\u221e\n\u221a\nT.\nThus,\nlimT \u2192\u221e\nR(T )\nT\n= 0.\n5\nRELATED WORK\nOptimization methods bearing a direct relation to Adam are RMSProp (Tieleman & Hinton, 2012;\nGraves, 2013) and AdaGrad (Duchi et al., 2011); these relationships are discussed below. Other\nstochastic optimization methods include vSGD (Schaul et al., 2012), AdaDelta (Zeiler, 2012) and the\nnatural Newton method from Roux & Fitzgibbon (2010), all setting stepsizes by estimating curvature\n4\nPublished as a conference paper at ICLR 2015\nfrom \ufb01rst-order information. The Sum-of-Functions Optimizer (SFO) (Sohl-Dickstein et al., 2014)\nis a quasi-Newton method based on minibatches, but (unlike Adam) has memory requirements linear\nin the number of minibatch partitions of a dataset, which is often infeasible on memory-constrained\nsystems such as a GPU. Like natural gradient descent (NGD) (Amari, 1998), Adam employs a\npreconditioner that adapts to the geometry of the data, since bvt is an approximation to the diagonal\nof the Fisher information matrix (Pascanu & Bengio, 2013); however, Adam\u2019s preconditioner (like\nAdaGrad\u2019s) is more conservative in its adaption than vanilla NGD by preconditioning with the square\nroot of the inverse of the diagonal Fisher information matrix approximation.\nRMSProp:\nAn optimization method closely related to Adam is RMSProp (Tieleman & Hinton,\n2012). A version with momentum has sometimes been used (Graves, 2013). There are a few impor-\ntant differences between RMSProp with momentum and Adam: RMSProp with momentum gener-\nates its parameter updates using a momentum on the rescaled gradient, whereas Adam updates are\ndirectly estimated using a running average of \ufb01rst and second moment of the gradient. RMSProp\nalso lacks a bias-correction term; this matters most in case of a value of \u03b22 close to 1 (required in\ncase of sparse gradients), since in that case not correcting the bias leads to very large stepsizes and\noften divergence, as we also empirically demonstrate in section 6.4.\nAdaGrad:\nAn algorithm that works well for sparse gradients is AdaGrad (Duchi et al., 2011). Its\nbasic version updates parameters as \u03b8t+1 = \u03b8t \u2212\u03b1 \u00b7 gt/\nqPt\ni=1 g2\nt . Note that if we choose \u03b22 to be\nin\ufb01nitesimally close to 1 from below, then lim\u03b22\u21921 bvt = t\u22121 \u00b7 Pt\ni=1 g2\nt . AdaGrad corresponds to a\nversion of Adam with \u03b21 = 0, in\ufb01nitesimal (1 \u2212\u03b22) and a replacement of \u03b1 by an annealed version\n\u03b1t = \u03b1 \u00b7 t\u22121/2, namely \u03b8t \u2212\u03b1 \u00b7 t\u22121/2 \u00b7 bmt/\np\nlim\u03b22\u21921 bvt = \u03b8t \u2212\u03b1 \u00b7 t\u22121/2 \u00b7 gt/\nq\nt\u22121 \u00b7 Pt\ni=1 g2\nt =\n\u03b8t \u2212\u03b1 \u00b7 gt/\nqPt\ni=1 g2\nt . Note that this direct correspondence between Adam and Adagrad does\nnot hold when removing the bias-correction terms; without bias correction, like in RMSProp, a \u03b22\nin\ufb01nitesimally close to 1 would lead to in\ufb01nitely large bias, and in\ufb01nitely large parameter updates.\n6\nEXPERIMENTS\nTo empirically evaluate the proposed method, we investigated different popular machine learning\nmodels, including logistic regression, multilayer fully connected neural networks and deep convolu-\ntional neural networks. Using large models and datasets, we demonstrate Adam can ef\ufb01ciently solve\npractical deep learning problems.\nWe use the same parameter initialization when comparing different optimization algorithms. The\nhyper-parameters, such as learning rate and momentum, are searched over a dense grid and the\nresults are reported using the best hyper-parameter setting.\n6.1\nEXPERIMENT: LOGISTIC REGRESSION\nWe evaluate our proposed method on L2-regularized multi-class logistic regression using the MNIST\ndataset. Logistic regression has a well-studied convex objective, making it suitable for comparison\nof different optimizers without worrying about local minimum issues. The stepsize \u03b1 in our logistic\nregression experiments is adjusted by 1/\n\u221a\nt decay, namely \u03b1t =\n\u03b1\n\u221a\nt that matches with our theorat-\nical prediction from section 4. The logistic regression classi\ufb01es the class label directly on the 784\ndimension image vectors. We compare Adam to accelerated SGD with Nesterov momentum and\nAdagrad using minibatch size of 128. According to Figure 1, we found that the Adam yields similar\nconvergence as SGD with momentum and both converge faster than Adagrad.\nAs discussed in (Duchi et al., 2011), Adagrad can ef\ufb01ciently deal with sparse features and gradi-\nents as one of its main theoretical results whereas SGD is low at learning rare features. Adam with\n1/\n\u221a\nt decay on its stepsize should theoratically match the performance of Adagrad. We examine the\nsparse feature problem using IMDB movie review dataset from (Maas et al., 2011). We pre-process\nthe IMDB movie reviews into bag-of-words (BoW) feature vectors including the \ufb01rst 10,000 most\nfrequent words. The 10,000 dimension BoW feature vector for each review is highly sparse. As sug-\ngested in (Wang & Manning, 2013), 50% dropout noise can be applied to the BoW features during\n5\nPublished as a conference paper at ICLR 2015\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\niterations over entire dataset\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\ntraining cost\nMNIST Logistic Regression\nAdaGrad\nSGDNesterov\nAdam\n0\n20\n40\n60\n80\n100\n120\n140\n160\niterations over entire dataset\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\ntraining cost\nIMDB BoW feature Logistic Regression\nAdagrad+dropout\nRMSProp+dropout\nSGDNesterov+dropout\nAdam+dropout\nFigure 1: Logistic regression training negative log likelihood on MNIST images and IMDB movie\nreviews with 10,000 bag-of-words (BoW) feature vectors.\ntraining to prevent over-\ufb01tting. In \ufb01gure 1, Adagrad outperforms SGD with Nesterov momentum\nby a large margin both with and without dropout noise. Adam converges as fast as Adagrad. The\nempirical performance of Adam is consistent with our theoretical \ufb01ndings in sections 2 and 4. Sim-\nilar to Adagrad, Adam can take advantage of sparse features and obtain faster convergence rate than\nnormal SGD with momentum.\n6.2\nEXPERIMENT: MULTI-LAYER NEURAL NETWORKS\nMulti-layer neural network are powerful models with non-convex objective functions. Although\nour convergence analysis does not apply to non-convex problems, we empirically found that Adam\noften outperforms other methods in such cases. In our experiments, we made model choices that are\nconsistent with previous publications in the area; a neural network model with two fully connected\nhidden layers with 1000 hidden units each and ReLU activation are used for this experiment with\nminibatch size of 128.\nFirst, we study different optimizers using the standard deterministic cross-entropy objective func-\ntion with L2 weight decay on the parameters to prevent over-\ufb01tting. The sum-of-functions (SFO)\nmethod (Sohl-Dickstein et al., 2014) is a recently proposed quasi-Newton method that works with\nminibatches of data and has shown good performance on optimization of multi-layer neural net-\nworks. We used their implementation and compared with Adam to train such models. Figure 2\nshows that Adam makes faster progress in terms of both the number of iterations and wall-clock\ntime. Due to the cost of updating curvature information, SFO is 5-10x slower per iteration com-\npared to Adam, and has a memory requirement that is linear in the number minibatches.\nStochastic regularization methods, such as dropout, are an effective way to prevent over-\ufb01tting and\noften used in practice due to their simplicity. SFO assumes deterministic subfunctions, and indeed\nfailed to converge on cost functions with stochastic regularization. We compare the effectiveness of\nAdam to other stochastic \ufb01rst order methods on multi-layer neural networks trained with dropout\nnoise. Figure 2 shows our results; Adam shows better convergence than other methods.\n6.3\nEXPERIMENT: CONVOLUTIONAL NEURAL NETWORKS\nConvolutional neural networks (CNNs) with several layers of convolution, pooling and non-linear\nunits have shown considerable success in computer vision tasks. Unlike most fully connected neural\nnets, weight sharing in CNNs results in vastly different gradients in different layers. A smaller\nlearning rate for the convolution layers is often used in practice when applying SGD. We show the\neffectiveness of Adam in deep CNNs. Our CNN architecture has three alternating stages of 5x5\nconvolution \ufb01lters and 3x3 max pooling with stride of 2 that are followed by a fully connected layer\nof 1000 recti\ufb01ed linear hidden units (ReLU\u2019s). The input image are pre-processed by whitening, and\n6\nPublished as a conference paper at ICLR 2015\n0\n50\n100\n150\n200\niterations over entire dataset\n10\n-2\n10\n-1\ntraining cost\nMNIST Multilayer Neural Network + dropout\nAdaGrad\nRMSProp\nSGDNesterov\nAdaDelta\nAdam\n(a)\n(b)\nFigure 2: Training of multilayer neural networks on MNIST images. (a) Neural networks using\ndropout stochastic regularization. (b) Neural networks with deterministic cost function. We compare\nwith the sum-of-functions (SFO) optimizer (Sohl-Dickstein et al., 2014)\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\niterations over entire dataset\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\ntraining cost\nCIFAR10 ConvNet First 3 Epoches\nAdaGrad\nAdaGrad+dropout\nSGDNesterov\nSGDNesterov+dropout\nAdam\nAdam+dropout\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\niterations over entire dataset\n10-4\n10-3\n10-2\n10-1\n100\n101\n102\ntraining cost\nCIFAR10 ConvNet\nAdaGrad\nAdaGrad+dropout\nSGDNesterov\nSGDNesterov+dropout\nAdam\nAdam+dropout\nFigure 3: Convolutional neural networks training cost. (left) Training cost for the \ufb01rst three epochs.\n(right) Training cost over 45 epochs. CIFAR-10 with c64-c64-c128-1000 architecture.\ndropout noise is applied to the input layer and fully connected layer. The minibatch size is also set\nto 128 similar to previous experiments.\nInterestingly, although both Adam and Adagrad make rapid progress lowering the cost in the initial\nstage of the training, shown in Figure 3 (left), Adam and SGD eventually converge considerably\nfaster than Adagrad for CNNs shown in Figure 3 (right). We notice the second moment estimate bvt\nvanishes to zeros after a few epochs and is dominated by the \u03f5 in algorithm 1. The second moment\nestimate is therefore a poor approximation to the geometry of the cost function in CNNs comparing\nto fully connected network from Section 6.2. Whereas, reducing the minibatch variance through\nthe \ufb01rst moment is more important in CNNs and contributes to the speed-up. As a result, Adagrad\nconverges much slower than others in this particular experiment. Though Adam shows marginal\nimprovement over SGD with momentum, it adapts learning rate scale for different layers instead of\nhand picking manually as in SGD.\n7\nPublished as a conference paper at ICLR 2015\n\u03b21=0\n\u03b21=0.9\n\u03b22=0.99\n\u03b22=0.999\n\u03b22=0.9999\n\u03b22=0.99\n\u03b22=0.999\n\u03b22=0.9999\n(a) after 10 epochs\n(b) after 100 epochs\nlog10(\u03b1)\nLoss\nFigure 4: Effect of bias-correction terms (red line) versus no bias correction terms (green line)\nafter 10 epochs (left) and 100 epochs (right) on the loss (y-axes) when learning a Variational Auto-\nEncoder (VAE) (Kingma & Welling, 2013), for different settings of stepsize \u03b1 (x-axes) and hyper-\nparameters \u03b21 and \u03b22.\n6.4\nEXPERIMENT: BIAS-CORRECTION TERM\nWe also empirically evaluate the effect of the bias correction terms explained in sections 2 and 3.\nDiscussed in section 5, removal of the bias correction terms results in a version of RMSProp (Tiele-\nman & Hinton, 2012) with momentum. We vary the \u03b21 and \u03b22 when training a variational auto-\nencoder (VAE) with the same architecture as in (Kingma & Welling, 2013) with a single hidden\nlayer with 500 hidden units with softplus nonlinearities and a 50-dimensional spherical Gaussian\nlatent variable. We iterated over a broad range of hyper-parameter choices, i.e. \u03b21 \u2208[0, 0.9] and\n\u03b22 \u2208[0.99, 0.999, 0.9999], and log10(\u03b1) \u2208[\u22125, ..., \u22121]. Values of \u03b22 close to 1, required for robust-\nness to sparse gradients, results in larger initialization bias; therefore we expect the bias correction\nterm is important in such cases of slow decay, preventing an adverse effect on optimization.\nIn Figure 4, values \u03b22 close to 1 indeed lead to instabilities in training when no bias correction term\nwas present, especially at \ufb01rst few epochs of the training. The best results were achieved with small\nvalues of (1\u2212\u03b22) and bias correction; this was more apparent towards the end of optimization when\ngradients tends to become sparser as hidden units specialize to speci\ufb01c patterns. In summary, Adam\nperformed equal or better than RMSProp, regardless of hyper-parameter setting.\n7\nEXTENSIONS\n7.1\nADAMAX\nIn Adam, the update rule for individual weights is to scale their gradients inversely proportional to a\n(scaled) L2 norm of their individual current and past gradients. We can generalize the L2 norm based\nupdate rule to a Lp norm based update rule. Such variants become numerically unstable for large\np. However, in the special case where we let p \u2192\u221e, a surprisingly simple and stable algorithm\nemerges; see algorithm 2. We\u2019ll now derive the algorithm. Let, in case of the Lp norm, the stepsize\nat time t be inversely proportional to v1/p\nt\n, where:\nvt = \u03b2p\n2vt\u22121 + (1 \u2212\u03b2p\n2)|gt|p\n(6)\n= (1 \u2212\u03b2p\n2)\nt\nX\ni=1\n\u03b2p(t\u2212i)\n2\n\u00b7 |gi|p\n(7)\n8\nPublished as a conference paper at ICLR 2015\nAlgorithm 2: AdaMax, a variant of Adam based on the in\ufb01nity norm. See section 7.1 for details.\nGood default settings for the tested machine learning problems are \u03b1 = 0.002, \u03b21 = 0.9 and\n\u03b22 = 0.999. With \u03b2t\n1 we denote \u03b21 to the power t. Here, (\u03b1/(1 \u2212\u03b2t\n1)) is the learning rate with the\nbias-correction term for the \ufb01rst moment. All operations on vectors are element-wise.\nRequire: \u03b1: Stepsize\nRequire: \u03b21, \u03b22 \u2208[0, 1): Exponential decay rates\nRequire: f(\u03b8): Stochastic objective function with parameters \u03b8\nRequire: \u03b80: Initial parameter vector\nm0 \u21900 (Initialize 1st moment vector)\nu0 \u21900 (Initialize the exponentially weighted in\ufb01nity norm)\nt \u21900 (Initialize timestep)\nwhile \u03b8t not converged do\nt \u2190t + 1\ngt \u2190\u2207\u03b8ft(\u03b8t\u22121) (Get gradients w.r.t. stochastic objective at timestep t)\nmt \u2190\u03b21 \u00b7 mt\u22121 + (1 \u2212\u03b21) \u00b7 gt (Update biased \ufb01rst moment estimate)\nut \u2190max(\u03b22 \u00b7 ut\u22121, |gt|) (Update the exponentially weighted in\ufb01nity norm)\n\u03b8t \u2190\u03b8t\u22121 \u2212(\u03b1/(1 \u2212\u03b2t\n1)) \u00b7 mt/ut (Update parameters)\nend while\nreturn \u03b8t (Resulting parameters)\nNote that the decay term is here equivalently parameterised as \u03b2p\n2 instead of \u03b22. Now let p \u2192\u221e,\nand de\ufb01ne ut = limp\u2192\u221e(vt)1/p, then:\nut = lim\np\u2192\u221e(vt)1/p = lim\np\u2192\u221e\n \n(1 \u2212\u03b2p\n2)\nt\nX\ni=1\n\u03b2p(t\u2212i)\n2\n\u00b7 |gi|p\n!1/p\n(8)\n= lim\np\u2192\u221e(1 \u2212\u03b2p\n2)1/p\n \nt\nX\ni=1\n\u03b2p(t\u2212i)\n2\n\u00b7 |gi|p\n!1/p\n(9)\n= lim\np\u2192\u221e\n \nt\nX\ni=1\n\u0010\n\u03b2(t\u2212i)\n2\n\u00b7 |gi|\n\u0011p\n!1/p\n(10)\n= max\n\u0000\u03b2t\u22121\n2\n|g1|, \u03b2t\u22122\n2\n|g2|, . . . , \u03b22|gt\u22121|, |gt|\n\u0001\n(11)\nWhich corresponds to the remarkably simple recursive formula:\nut = max(\u03b22 \u00b7 ut\u22121, |gt|)\n(12)\nwith initial value u0 = 0. Note that, conveniently enough, we don\u2019t need to correct for initialization\nbias in this case. Also note that the magnitude of parameter updates has a simpler bound with\nAdaMax than Adam, namely: |\u2206t| \u2264\u03b1.\n7.2\nTEMPORAL AVERAGING\nSince the last iterate is noisy due to stochastic approximation, better generalization performance is\noften achieved by averaging. Previously in Moulines & Bach (2011), Polyak-Ruppert averaging\n(Polyak & Juditsky, 1992; Ruppert, 1988) has been shown to improve the convergence of standard\nSGD, where \u00af\u03b8t = 1\nt\nPn\nk=1 \u03b8k. Alternatively, an exponential moving average over the parameters can\nbe used, giving higher weight to more recent parameter values. This can be trivially implemented\nby adding one line to the inner loop of algorithms 1 and 2: \u00af\u03b8t \u2190\u03b22 \u00b7 \u00af\u03b8t\u22121 +(1\u2212\u03b22)\u03b8t, with \u00af\u03b80 = 0.\nInitalization bias can again be corrected by the estimator b\u03b8t = \u00af\u03b8t/(1 \u2212\u03b2t\n2).\n8\nCONCLUSION\nWe have introduced a simple and computationally ef\ufb01cient algorithm for gradient-based optimiza-\ntion of stochastic objective functions. Our method is aimed towards machine learning problems with\n9\nPublished as a conference paper at ICLR 2015\nlarge datasets and/or high-dimensional parameter spaces. The method combines the advantages of\ntwo recently popular optimization methods: the ability of AdaGrad to deal with sparse gradients,\nand the ability of RMSProp to deal with non-stationary objectives. The method is straightforward\nto implement and requires little memory. The experiments con\ufb01rm the analysis on the rate of con-\nvergence in convex problems. Overall, we found Adam to be robust and well-suited to a wide range\nof non-convex optimization problems in the \ufb01eld machine learning.\n9\nACKNOWLEDGMENTS\nThis paper would probably not have existed without the support of Google Deepmind. We would\nlike to give special thanks to Ivo Danihelka, and Tom Schaul for coining the name Adam. Thanks to\nKai Fan from Duke University for spotting an error in the original AdaMax derivation. Experiments\nin this work were partly carried out on the Dutch national e-infrastructure with the support of SURF\nFoundation. Diederik Kingma is supported by the Google European Doctorate Fellowship in Deep\nLearning.\nREFERENCES\nAmari, Shun-Ichi. Natural gradient works ef\ufb01ciently in learning. Neural computation, 10(2):251\u2013276, 1998.\nDeng, Li, Li, Jinyu, Huang, Jui-Ting, Yao, Kaisheng, Yu, Dong, Seide, Frank, Seltzer, Michael, Zweig, Geoff,\nHe, Xiaodong, Williams, Jason, et al. Recent advances in deep learning for speech research at microsoft.\nICASSP 2013, 2013.\nDuchi, John, Hazan, Elad, and Singer, Yoram. Adaptive subgradient methods for online learning and stochastic\noptimization. The Journal of Machine Learning Research, 12:2121\u20132159, 2011.\nGraves, Alex. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.\nGraves, Alex, Mohamed, Abdel-rahman, and Hinton, Geoffrey. Speech recognition with deep recurrent neural\nnetworks. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on,\npp. 6645\u20136649. IEEE, 2013.\nHinton, G.E. and Salakhutdinov, R.R. Reducing the dimensionality of data with neural networks. Science, 313\n(5786):504\u2013507, 2006.\nHinton, Geoffrey, Deng, Li, Yu, Dong, Dahl, George E, Mohamed, Abdel-rahman, Jaitly, Navdeep, Senior,\nAndrew, Vanhoucke, Vincent, Nguyen, Patrick, Sainath, Tara N, et al. Deep neural networks for acoustic\nmodeling in speech recognition: The shared views of four research groups. Signal Processing Magazine,\nIEEE, 29(6):82\u201397, 2012a.\nHinton, Geoffrey E, Srivastava, Nitish, Krizhevsky, Alex, Sutskever, Ilya, and Salakhutdinov, Ruslan R. Im-\nproving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580,\n2012b.\nKingma, Diederik P and Welling, Max. Auto-Encoding Variational Bayes. In The 2nd International Conference\non Learning Representations (ICLR), 2013.\nKrizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classi\ufb01cation with deep convolutional\nneural networks. In Advances in neural information processing systems, pp. 1097\u20131105, 2012.\nMaas, Andrew L, Daly, Raymond E, Pham, Peter T, Huang, Dan, Ng, Andrew Y, and Potts, Christopher.\nLearning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association\nfor Computational Linguistics: Human Language Technologies-Volume 1, pp. 142\u2013150. Association for\nComputational Linguistics, 2011.\nMoulines, Eric and Bach, Francis R.\nNon-asymptotic analysis of stochastic approximation algorithms for\nmachine learning. In Advances in Neural Information Processing Systems, pp. 451\u2013459, 2011.\nPascanu, Razvan and Bengio, Yoshua.\nRevisiting natural gradient for deep networks.\narXiv preprint\narXiv:1301.3584, 2013.\nPolyak, Boris T and Juditsky, Anatoli B. Acceleration of stochastic approximation by averaging. SIAM Journal\non Control and Optimization, 30(4):838\u2013855, 1992.\n10\nPublished as a conference paper at ICLR 2015\nRoux, Nicolas L and Fitzgibbon, Andrew W. A fast natural newton method. In Proceedings of the 27th\nInternational Conference on Machine Learning (ICML-10), pp. 623\u2013630, 2010.\nRuppert, David. Ef\ufb01cient estimations from a slowly convergent robbins-monro process. Technical report,\nCornell University Operations Research and Industrial Engineering, 1988.\nSchaul, Tom, Zhang, Sixin, and LeCun, Yann. No more pesky learning rates. arXiv preprint arXiv:1206.1106,\n2012.\nSohl-Dickstein, Jascha, Poole, Ben, and Ganguli, Surya. Fast large-scale optimization by unifying stochas-\ntic gradient and quasi-newton methods. In Proceedings of the 31st International Conference on Machine\nLearning (ICML-14), pp. 604\u2013612, 2014.\nSutskever, Ilya, Martens, James, Dahl, George, and Hinton, Geoffrey. On the importance of initialization and\nmomentum in deep learning. In Proceedings of the 30th International Conference on Machine Learning\n(ICML-13), pp. 1139\u20131147, 2013.\nTieleman, T. and Hinton, G. Lecture 6.5 - RMSProp, COURSERA: Neural Networks for Machine Learning.\nTechnical report, 2012.\nWang, Sida and Manning, Christopher. Fast dropout training. In Proceedings of the 30th International Confer-\nence on Machine Learning (ICML-13), pp. 118\u2013126, 2013.\nZeiler, Matthew D. Adadelta: An adaptive learning rate method. arXiv preprint arXiv:1212.5701, 2012.\nZinkevich, Martin. Online convex programming and generalized in\ufb01nitesimal gradient ascent. 2003.\n11\nPublished as a conference paper at ICLR 2015\n10\nAPPENDIX\n10.1\nCONVERGENCE PROOF\nDe\ufb01nition 10.1. A function f : Rd \u2192R is convex if for all x, y \u2208Rd, for all \u03bb \u2208[0, 1],\n\u03bbf(x) + (1 \u2212\u03bb)f(y) \u2265f(\u03bbx + (1 \u2212\u03bb)y)\nAlso, notice that a convex function can be lower bounded by a hyperplane at its tangent.\nLemma 10.2. If a function f : Rd \u2192R is convex, then for all x, y \u2208Rd,\nf(y) \u2265f(x) + \u2207f(x)T (y \u2212x)\nThe above lemma can be used to upper bound the regret and our proof for the main theorem is\nconstructed by substituting the hyperplane with the Adam update rules.\nThe following two lemmas are used to support our main theorem. We also use some de\ufb01nitions sim-\nplify our notation, where gt \u225c\u2207ft(\u03b8t) and gt,i as the ith element. We de\ufb01ne g1:t,i \u2208Rt as a vector\nthat contains the ith dimension of the gradients over all iterations till t, g1:t,i = [g1,i, g2,i, \u00b7 \u00b7 \u00b7 , gt,i]\nLemma 10.3. Let gt = \u2207ft(\u03b8t) and g1:t be de\ufb01ned as above and bounded, \u2225gt\u22252 \u2264G, \u2225gt\u2225\u221e\u2264\nG\u221e. Then,\nT\nX\nt=1\ns\ng2\nt,i\nt\n\u22642G\u221e\u2225g1:T,i\u22252\nProof. We will prove the inequality using induction over T.\nThe base case for T = 1, we have\nq\ng2\n1,i \u22642G\u221e\u2225g1,i\u22252.\nFor the inductive step,\nT\nX\nt=1\ns\ng2\nt,i\nt\n=\nT \u22121\nX\nt=1\ns\ng2\nt,i\nt\n+\ns\ng2\nT,i\nT\n\u22642G\u221e\u2225g1:T \u22121,i\u22252 +\ns\ng2\nT,i\nT\n= 2G\u221e\nq\n\u2225g1:T,i\u22252\n2 \u2212g2\nT +\ns\ng2\nT,i\nT\nFrom, \u2225g1:T,i\u22252\n2 \u2212g2\nT,i +\ng4\nT,i\n4\u2225g1:T,i\u22252\n2 \u2265\u2225g1:T,i\u22252\n2 \u2212g2\nT,i, we can take square root of both side and\nhave,\nq\n\u2225g1:T,i\u22252\n2 \u2212g2\nT,i \u2264\u2225g1:T,i\u22252 \u2212\ng2\nT,i\n2\u2225g1:T,i\u22252\n\u2264\u2225g1:T,i\u22252 \u2212\ng2\nT,i\n2\np\nTG2\u221e\nRearrange the inequality and substitute the\nq\n\u2225g1:T,i\u22252\n2 \u2212g2\nT,i term,\nG\u221e\nq\n\u2225g1:T,i\u22252\n2 \u2212g2\nT +\ns\ng2\nT,i\nT\n\u22642G\u221e\u2225g1:T,i\u22252\n12\nPublished as a conference paper at ICLR 2015\nLemma 10.4. Let \u03b3 \u225c\n\u03b22\n1\n\u221a\u03b22 . For \u03b21, \u03b22 \u2208[0, 1) that satisfy\n\u03b22\n1\n\u221a\u03b22 < 1 and bounded gt, \u2225gt\u22252 \u2264G,\n\u2225gt\u2225\u221e\u2264G\u221e, the following inequality holds\nT\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n\u2264\n2\n1 \u2212\u03b3\n1\n\u221a1 \u2212\u03b22\n\u2225g1:T,i\u22252\nProof. Under the assumption,\n\u221a\n1\u2212\u03b2t\n2\n(1\u2212\u03b2t\n1)2 \u2264\n1\n(1\u2212\u03b21)2 . We can expand the last term in the summation\nusing the update rules in Algorithm 1,\nT\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n=\nT \u22121\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n+\np\n1 \u2212\u03b2T\n2\n(1 \u2212\u03b2T\n1 )2\n(PT\nk=1(1 \u2212\u03b21)\u03b2T \u2212k\n1\ngk,i)2\nq\nT PT\nj=1(1 \u2212\u03b22)\u03b2T \u2212j\n2\ng2\nj,i\n\u2264\nT \u22121\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n+\np\n1 \u2212\u03b2T\n2\n(1 \u2212\u03b2T\n1 )2\nT\nX\nk=1\nT((1 \u2212\u03b21)\u03b2T \u2212k\n1\ngk,i)2\nq\nT PT\nj=1(1 \u2212\u03b22)\u03b2T \u2212j\n2\ng2\nj,i\n\u2264\nT \u22121\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n+\np\n1 \u2212\u03b2T\n2\n(1 \u2212\u03b2T\n1 )2\nT\nX\nk=1\nT((1 \u2212\u03b21)\u03b2T \u2212k\n1\ngk,i)2\nq\nT(1 \u2212\u03b22)\u03b2T \u2212k\n2\ng2\nk,i\n\u2264\nT \u22121\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n+\np\n1 \u2212\u03b2T\n2\n(1 \u2212\u03b2T\n1 )2\n(1 \u2212\u03b21)2\np\nT(1 \u2212\u03b22)\nT\nX\nk=1\nT\n\u0012 \u03b22\n1\n\u221a\u03b22\n\u0013T \u2212k\n\u2225gk,i\u22252\n\u2264\nT \u22121\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n+\nT\np\nT(1 \u2212\u03b22)\nT\nX\nk=1\n\u03b3T \u2212k\u2225gk,i\u22252\nSimilarly, we can upper bound the rest of the terms in the summation.\nT\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n\u2264\nT\nX\nt=1\n\u2225gt,i\u22252\np\nt(1 \u2212\u03b22)\nT \u2212t\nX\nj=0\nt\u03b3j\n\u2264\nT\nX\nt=1\n\u2225gt,i\u22252\np\nt(1 \u2212\u03b22)\nT\nX\nj=0\nt\u03b3j\nFor \u03b3 < 1, using the upper bound on the arithmetic-geometric series, P\nt t\u03b3t <\n1\n(1\u2212\u03b3)2 :\nT\nX\nt=1\n\u2225gt,i\u22252\np\nt(1 \u2212\u03b22)\nT\nX\nj=0\nt\u03b3j \u2264\n1\n(1 \u2212\u03b3)2\u221a1 \u2212\u03b22\nT\nX\nt=1\n\u2225gt,i\u22252\n\u221a\nt\nApply Lemma 10.3,\nT\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n\u2264\n2G\u221e\n(1 \u2212\u03b3)2\u221a1 \u2212\u03b22\n\u2225g1:T,i\u22252\nTo simplify the notation, we de\ufb01ne \u03b3 \u225c\n\u03b22\n1\n\u221a\u03b22 . Intuitively, our following theorem holds when the\nlearning rate \u03b1t is decaying at a rate of t\u22121\n2 and \ufb01rst moment running average coef\ufb01cient \u03b21,t decay\nexponentially with \u03bb, that is typically close to 1, e.g. 1 \u221210\u22128.\nTheorem 10.5. Assume that the function ft has bounded gradients, \u2225\u2207ft(\u03b8)\u22252 \u2264G, \u2225\u2207ft(\u03b8)\u2225\u221e\u2264\nG\u221efor all \u03b8 \u2208Rd and distance between any \u03b8t generated by Adam is bounded, \u2225\u03b8n \u2212\u03b8m\u22252 \u2264D,\n13\nPublished as a conference paper at ICLR 2015\n\u2225\u03b8m \u2212\u03b8n\u2225\u221e\u2264D\u221efor any m, n \u2208{1, ..., T}, and \u03b21, \u03b22 \u2208[0, 1) satisfy\n\u03b22\n1\n\u221a\u03b22 < 1. Let \u03b1t =\n\u03b1\n\u221a\nt\nand \u03b21,t = \u03b21\u03bbt\u22121, \u03bb \u2208(0, 1). Adam achieves the following guarantee, for all T \u22651.\nR(T) \u2264\nD2\n2\u03b1(1 \u2212\u03b21)\nd\nX\ni=1\np\nTbvT,i+\n\u03b1(\u03b21 + 1)G\u221e\n(1 \u2212\u03b21)\u221a1 \u2212\u03b22(1 \u2212\u03b3)2\nd\nX\ni=1\n\u2225g1:T,i\u22252+\nd\nX\ni=1\nD2\n\u221eG\u221e\n\u221a1 \u2212\u03b22\n2\u03b1(1 \u2212\u03b21)(1 \u2212\u03bb)2\nProof. Using Lemma 10.2, we have,\nft(\u03b8t) \u2212ft(\u03b8\u2217) \u2264gT\nt (\u03b8t \u2212\u03b8\u2217) =\nd\nX\ni=1\ngt,i(\u03b8t,i \u2212\u03b8\u2217\n,i)\nFrom the update rules presented in algorithm 1,\n\u03b8t+1 = \u03b8t \u2212\u03b1t bmt/\np\nbvt\n= \u03b8t \u2212\n\u03b1t\n1 \u2212\u03b2t\n1\n\u0012 \u03b21,t\n\u221abvt\nmt\u22121 + (1 \u2212\u03b21,t)\n\u221abvt\ngt\n\u0013\nWe focus on the ith dimension of the parameter vector \u03b8t \u2208Rd. Subtract the scalar \u03b8\u2217\n,i and square\nboth sides of the above update rule, we have,\n(\u03b8t+1,i \u2212\u03b8\u2217\n,i)2 =(\u03b8t,i \u2212\u03b8\u2217\n,i)2 \u2212\n2\u03b1t\n1 \u2212\u03b2t\n1\n( \u03b21,t\np\nbvt,i\nmt\u22121,i + (1 \u2212\u03b21,t)\np\nbvt,i\ngt,i)(\u03b8t,i \u2212\u03b8\u2217\n,i) + \u03b12\nt ( bmt,i\np\nbvt,i\n)2\nWe can rearrange the above equation and use Young\u2019s inequality, ab \u2264a2/2 + b2/2. Also, it can be\nshown that\np\nbvt,i =\nqPt\nj=1(1 \u2212\u03b22)\u03b2t\u2212j\n2\ng2\nj,i/\np\n1 \u2212\u03b2t\n2 \u2264\u2225g1:t,i\u22252 and \u03b21,t \u2264\u03b21. Then\ngt,i(\u03b8t,i \u2212\u03b8\u2217\n,i) =(1 \u2212\u03b2t\n1)\np\nbvt,i\n2\u03b1t(1 \u2212\u03b21,t)\n\u0012\n(\u03b8t,i \u2212\u03b8\u2217\n,t)2 \u2212(\u03b8t+1,i \u2212\u03b8\u2217\n,i)2\n\u0013\n+\n\u03b21,t\n(1 \u2212\u03b21,t)\nbv\n1\n4\nt\u22121,i\n\u221a\u03b1t\u22121\n(\u03b8\u2217\n,i \u2212\u03b8t,i)\u221a\u03b1t\u22121\nmt\u22121,i\nbv\n1\n4\nt\u22121,i\n+ \u03b1t(1 \u2212\u03b2t\n1)\np\nbvt,i\n2(1 \u2212\u03b21,t)\n( bmt,i\np\nbvt,i\n)2\n\u2264\n1\n2\u03b1t(1 \u2212\u03b21)\n\u0012\n(\u03b8t,i \u2212\u03b8\u2217\n,t)2 \u2212(\u03b8t+1,i \u2212\u03b8\u2217\n,i)2\n\u0013p\nbvt,i +\n\u03b21,t\n2\u03b1t\u22121(1 \u2212\u03b21,t)(\u03b8\u2217\n,i \u2212\u03b8t,i)2p\nbvt\u22121,i\n+\n\u03b21\u03b1t\u22121\n2(1 \u2212\u03b21)\nm2\nt\u22121,i\np\nbvt\u22121,i\n+\n\u03b1t\n2(1 \u2212\u03b21)\nbm2\nt,i\np\nbvt,i\nWe apply Lemma 10.4 to the above inequality and derive the regret bound by summing across all\nthe dimensions for i \u22081, ..., d in the upper bound of ft(\u03b8t) \u2212ft(\u03b8\u2217) and the sequence of convex\nfunctions for t \u22081, ..., T:\nR(T) \u2264\nd\nX\ni=1\n1\n2\u03b11(1 \u2212\u03b21)(\u03b81,i \u2212\u03b8\u2217\n,i)2p\nbv1,i +\nd\nX\ni=1\nT\nX\nt=2\n1\n2(1 \u2212\u03b21)(\u03b8t,i \u2212\u03b8\u2217\n,i)2(\np\nbvt,i\n\u03b1t\n\u2212\np\nbvt\u22121,i\n\u03b1t\u22121\n)\n+\n\u03b21\u03b1G\u221e\n(1 \u2212\u03b21)\u221a1 \u2212\u03b22(1 \u2212\u03b3)2\nd\nX\ni=1\n\u2225g1:T,i\u22252 +\n\u03b1G\u221e\n(1 \u2212\u03b21)\u221a1 \u2212\u03b22(1 \u2212\u03b3)2\nd\nX\ni=1\n\u2225g1:T,i\u22252\n+\nd\nX\ni=1\nT\nX\nt=1\n\u03b21,t\n2\u03b1t(1 \u2212\u03b21,t)(\u03b8\u2217\n,i \u2212\u03b8t,i)2p\nbvt,i\n14\nPublished as a conference paper at ICLR 2015\nFrom the assumption, \u2225\u03b8t \u2212\u03b8\u2217\u22252 \u2264D, \u2225\u03b8m \u2212\u03b8n\u2225\u221e\u2264D\u221e, we have:\nR(T) \u2264\nD2\n2\u03b1(1 \u2212\u03b21)\nd\nX\ni=1\np\nTbvT,i +\n\u03b1(1 + \u03b21)G\u221e\n(1 \u2212\u03b21)\u221a1 \u2212\u03b22(1 \u2212\u03b3)2\nd\nX\ni=1\n\u2225g1:T,i\u22252 + D2\n\u221e\n2\u03b1\nd\nX\ni=1\nt\nX\nt=1\n\u03b21,t\n(1 \u2212\u03b21,t)\np\ntbvt,i\n\u2264\nD2\n2\u03b1(1 \u2212\u03b21)\nd\nX\ni=1\np\nTbvT,i +\n\u03b1(1 + \u03b21)G\u221e\n(1 \u2212\u03b21)\u221a1 \u2212\u03b22(1 \u2212\u03b3)2\nd\nX\ni=1\n\u2225g1:T,i\u22252\n+ D2\n\u221eG\u221e\n\u221a1 \u2212\u03b22\n2\u03b1\nd\nX\ni=1\nt\nX\nt=1\n\u03b21,t\n(1 \u2212\u03b21,t)\n\u221a\nt\nWe can use arithmetic geometric series upper bound for the last term:\nt\nX\nt=1\n\u03b21,t\n(1 \u2212\u03b21,t)\n\u221a\nt \u2264\nt\nX\nt=1\n1\n(1 \u2212\u03b21)\u03bbt\u22121\u221a\nt\n\u2264\nt\nX\nt=1\n1\n(1 \u2212\u03b21)\u03bbt\u22121t\n\u2264\n1\n(1 \u2212\u03b21)(1 \u2212\u03bb)2\nTherefore, we have the following regret bound:\nR(T) \u2264\nD2\n2\u03b1(1 \u2212\u03b21)\nd\nX\ni=1\np\nTbvT,i +\n\u03b1(1 + \u03b21)G\u221e\n(1 \u2212\u03b21)\u221a1 \u2212\u03b22(1 \u2212\u03b3)2\nd\nX\ni=1\n\u2225g1:T,i\u22252 +\nd\nX\ni=1\nD2\n\u221eG\u221e\n\u221a1 \u2212\u03b22\n2\u03b1\u03b21(1 \u2212\u03bb)2\n15\n",
        "sentence": " We choose the popular Adam stochastic gradient optimization algorithm [20] with the learning rate tuned using the following grid axis: {10\u22123, 10\u22122, 10\u22121}.",
        "context": "We propose Adam, a method for ef\ufb01cient stochastic optimization that only requires \ufb01rst-order gra-\ndients with little memory requirement. The method computes individual adaptive learning rates for\n2).\n8\nCONCLUSION\nWe have introduced a simple and computationally ef\ufb01cient algorithm for gradient-based optimiza-\ntion of stochastic objective functions. Our method is aimed towards machine learning problems with\n9\njimmy@psi.utoronto.ca\nABSTRACT\nWe introduce Adam, an algorithm for \ufb01rst-order gradient-based optimization of\nstochastic objective functions, based on adaptive estimates of lower-order mo-"
    },
    {
        "title": "Toward Optimal Feature Selection",
        "author": [
            "D. Koller",
            "M. Sahami"
        ],
        "venue": "ICML",
        "citeRegEx": "21",
        "shortCiteRegEx": null,
        "year": 1995,
        "abstract": "",
        "full_text": "",
        "sentence": " The trade-off between feature redundancy and relevancy is well-studied [14, 44, 21].",
        "context": null
    },
    {
        "title": "Feature Selection in Enterprise Analytics: A Demonstration using an R-based Data Analytics System",
        "author": [
            "P. Konda",
            "A. Kumar",
            "C. R\u00e9",
            "V. Sashikanth"
        ],
        "venue": "VLDB",
        "citeRegEx": "22",
        "shortCiteRegEx": null,
        "year": 2013,
        "abstract": "",
        "full_text": "",
        "sentence": " , [22, 23]).",
        "context": null
    },
    {
        "title": "MLbase: A Distributed Machine-learning System",
        "author": [
            "T. Kraska",
            "A. Talwalkar",
            "J.C. Duchi",
            "R. Griffith",
            "M.J. Franklin",
            "M.I. Jordan"
        ],
        "venue": "CIDR",
        "citeRegEx": "23",
        "shortCiteRegEx": null,
        "year": 2013,
        "abstract": "",
        "full_text": "",
        "sentence": " , [22, 23]).",
        "context": null
    },
    {
        "title": "Demonstration of Santoku: Optimizing Machine Learning over Normalized Data",
        "author": [
            "A. Kumar",
            "M. Jalal",
            "B. Yan",
            "J. Naughton",
            "J.M. Patel"
        ],
        "venue": "VLDB",
        "citeRegEx": "24",
        "shortCiteRegEx": null,
        "year": 2015,
        "abstract": "",
        "full_text": "",
        "sentence": " The scenario of learning over joins of multiple tables without materializing the output of the join was studied in [25, 37, 34, 24], but their goal was primarily to reduce runtimes of some ML techniques without affecting accuracy.",
        "context": null
    },
    {
        "title": "Learning Generalized Linear Models Over Normalized Data",
        "author": [
            "A. Kumar",
            "J. Naughton",
            "J.M. Patel"
        ],
        "venue": "Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data, SIGMOD \u201915",
        "citeRegEx": "25",
        "shortCiteRegEx": null,
        "year": 2015,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Thus, given an ML task, data scientists almost always join multiple tables because they like to obtain more features for ML models [25]. Star schemas are ubiquitous in many applications, including retail, insurance, Web security, and recommendation systems [33, 26, 25]. The scenario of learning over joins of multiple tables without materializing the output of the join was studied in [25, 37, 34, 24], but their goal was primarily to reduce runtimes of some ML techniques without affecting accuracy.",
        "context": null
    },
    {
        "title": "To Join or Not to Join? Thinking Twice about Joins before Feature Selection",
        "author": [
            "A. Kumar",
            "J. Naughton",
            "J.M. Patel",
            "X. Zhu"
        ],
        "venue": "Proceedings of the 2016 International Conference on Management of Data, SIGMOD \u201916",
        "citeRegEx": "26",
        "shortCiteRegEx": null,
        "year": 2016,
        "abstract": "",
        "full_text": "",
        "sentence": " [26] showed that one can often omit entire tables by exploiting KFKDs in the database schema. Example (based on [26]). The analysis in [26] revealed a dichotomy in how safe it is to avoid a join from an accuracy standpoint: in terms of the bias-variance trade-off, avoiding a join is unlikely to increase bias but it might significantly increase variance, since foreign key features often have larger domains than foreign features. In [26], the tuple ratio quantifies this behavior; in our example, it is the ratio of the number of labeled customers to the number of employers. While KFKDs are not the same as FDs [39], assuming features have \u201cclosed\u201d domains, they behave essentially as FDs in the output of the join [26]. However, the results in [26] had a major caveat\u2013they applied only to linear classifiers. Surprisingly, our results show that their behavior is the exact opposite! We start by rerunning the experiments on the real-world datasets with KFK joins from [26] for these models. In other words, our work refutes an intuition from the VC dimension-based analysis of [26] and shows that these popular high-capacity classifiers are counter-intuitively comparably or more robust to avoiding joins than linear classifiers, not less. , the holdout test errors blow up) [26]. Star schemas are ubiquitous in many applications, including retail, insurance, Web security, and recommendation systems [33, 26, 25]. For the sake of tractability, in this paper, we adopt some assumptions from [26]. We take the seven real datasets from [26]; these are originally from Kaggle, GroupLens, openflights. More details about their schemas, including the list of all features are already in the public domain (listed in [26]). For Naive Bayes, we used the code from [26], while for logistic regression with L1 regularization, we used the popular R package \u201cglmnet. Interestingly, on the Yelp dataset, in which both joins are known to be not safe to avoid with the linear classifiers [26], NoJoin correctly sees a large reduction in accuracy from JoinAll\u2013about 0. This reaffirms the importance of foreign key features; as such, it is known that dropping foreign key features could cause the bias to shoot up with linear classifiers [26]. These results are surprising given the more conservative behavior predicted even for the linear classifiers in [26]. A key benefit of avoiding KFK joins safely is that ML runtimes (including feature selection) could be significantly lowered for the linear classifiers [26]. Thus, these results corroborate the orders of magnitude speedup reported in [26] for Naive Bayes with backward selection. Note that our simulation methodology is not tied to decision trees; it is generic enough to be applicable to classifier because we only use standard generic notions of error and net variance as defined in [26]. These scenarios represent opposite extremes for how likely the (test) error is likely to shoot up when XR is discarded and FK is used as a representative [26]. In contrast to these results, [26] reported that for linear models, the errors of NoJoin shot up compared to JoinAll (a gap of nearly 0. This is akin to the extra overfitting reported in [26] using the plots of the net variance. For the linear model case, [26] reported that as the skew parameters increased, the gap widened. Since FK already encodes all information that XR provides [26], the tree almost always uses FK in its partitioning, often multiple times. We first demonstrated the feasibility of avoiding joins safely in [26] for linear models. In this work, we revisit that idea for higher capacity models and find that they are counter-intuitively more robust than linear models to avoiding joins, not less as the VC dimension-based analysis in [26] suggested.",
        "context": null
    },
    {
        "title": "Data Integration in Machine Learning",
        "author": [
            "Y. Li",
            "A. Ngom"
        ],
        "venue": "IEEE International Conference on Bioinformatics and Biomedicine (BTBM)",
        "citeRegEx": "27",
        "shortCiteRegEx": null,
        "year": 2015,
        "abstract": "",
        "full_text": "",
        "sentence": " Integrating data and features from different sources for ML and data mining algorithms often requires applying and adapting techniques from the data integration literature [27, 8].",
        "context": null
    },
    {
        "title": "GraphLab: A New Framework For Parallel Machine Learning",
        "author": [
            "Y. Low",
            "J.E. Gonzalez",
            "A. Kyrola",
            "D. Bickson",
            "C.E. Guestrin",
            "J. Hellerstein"
        ],
        "venue": "UAI",
        "citeRegEx": "28",
        "shortCiteRegEx": null,
        "year": 2010,
        "abstract": "Designing and implementing efficient, provably correct parallel machine\nlearning (ML) algorithms is challenging. Existing high-level parallel\nabstractions like MapReduce are insufficiently expressive while low-level tools\nlike MPI and Pthreads leave ML experts repeatedly solving the same design\nchallenges. By targeting common patterns in ML, we developed GraphLab, which\nimproves upon abstractions like MapReduce by compactly expressing asynchronous\niterative algorithms with sparse computational dependencies while ensuring data\nconsistency and achieving a high degree of parallel performance. We demonstrate\nthe expressiveness of the GraphLab framework by designing and implementing\nparallel versions of belief propagation, Gibbs sampling, Co-EM, Lasso and\nCompressed Sensing. We show that using GraphLab we can achieve excellent\nparallel performance on large scale real-world problems.",
        "full_text": "GraphLab: A New Framework For Parallel Machine Learning\nYucheng Low\nCarnegie Mellon University\nylow@cs.cmu.edu\nJoseph Gonzalez\nCarnegie Mellon University\njegonzal@cs.cmu.edu\nAapo Kyrola\nCarnegie Mellon University\nakyrola@cs.cmu.edu\nDanny Bickson\nCarnegie Mellon University\nbickson@cs.cmu.edu\nCarlos Guestrin\nCarnegie Mellon University\nguestrin@cs.cmu.edu\nJoseph M. Hellerstein\nUC Berkeley\nhellerstein@cs.berkeley.edu\nAbstract\nDesigning and implementing ef\ufb01cient, provably\ncorrect parallel machine learning (ML) algo-\nrithms is challenging. Existing high-level par-\nallel abstractions like MapReduce are insuf-\n\ufb01ciently expressive while low-level tools like\nMPI and Pthreads leave ML experts repeatedly\nsolving the same design challenges.\nBy tar-\ngeting common patterns in ML, we developed\nGraphLab, which improves upon abstractions\nlike MapReduce by compactly expressing asyn-\nchronous iterative algorithms with sparse com-\nputational dependencies while ensuring data con-\nsistency and achieving a high degree of parallel\nperformance. We demonstrate the expressiveness\nof the GraphLab framework by designing and\nimplementing parallel versions of belief propaga-\ntion, Gibbs sampling, Co-EM, Lasso and Com-\npressed Sensing. We show that using GraphLab\nwe can achieve excellent parallel performance on\nlarge scale real-world problems.\n1\nINTRODUCTION\nExponential gains in hardware technology have enabled so-\nphisticated machine learning (ML) techniques to be applied\nto increasingly challenging real-world problems. However,\nrecent developments in computer architecture have shifted\nthe focus away from frequency scaling and towards paral-\nlel scaling, threatening the future of sequential ML algo-\nrithms. In order to bene\ufb01t from future trends in processor\ntechnology and to be able to apply rich structured models\nto rapidly scaling real-world problems, the ML community\nmust directly confront the challenges of parallelism.\nHowever, designing and implementing ef\ufb01cient and prov-\nably correct parallel algorithms is extremely challenging.\nWhile low level abstractions like MPI and Pthreads pro-\nvide powerful, expressive primitives, they force the user\nto address hardware issues and the challenges of parallel\ndata representation. Consequently, many ML experts have\nturned to high-level abstractions, which dramatically sim-\nplify the design and implementation of a restricted class of\nparallel algorithms. For example, the MapReduce abstrac-\ntion [Dean and Ghemawat, 2004] has been successfully ap-\nplied to a broad range of ML applications [Chu et al., 2006,\nWolfe et al., 2008, Panda et al., 2009, Ye et al., 2009].\nHowever, by restricting our focus to ML algorithms that\nare naturally expressed in MapReduce, we are often forced\nto make overly simplifying assumptions. Alternatively, by\ncoercing ef\ufb01cient sequential ML algorithms to satisfy the\nrestrictions imposed by MapReduce, we often produce in-\nef\ufb01cient parallel algorithms that require many processors\nto be competitive with comparable sequential methods.\nIn this paper we propose GraphLab, a new parallel frame-\nwork for ML which exploits the sparse structure and com-\nmon computational patterns of ML algorithms. GraphLab\nenables ML experts to easily design and implement ef\ufb01-\ncient scalable parallel algorithms by composing problem\nspeci\ufb01c computation, data-dependencies, and scheduling.\nWe provide an ef\ufb01cient shared-memory implementation1\nof GraphLab and use it to build parallel versions of four\npopular ML algorithms. We focus on the shared-memory\nmultiprocessor setting because it is both ubiquitous and has\nfew effective high-level abstractions. We evaluate the algo-\nrithms on a 16-processor system and demonstrate state-of-\nthe-art performance. Our main contributions include:\n\u2022 A graph-based data model which simultaneously rep-\nresents data and computational dependencies.\n\u2022 A set of concurrent access models which provide a\nrange of sequential-consistency guarantees.\n\u2022 A sophisticated modular scheduling mechanism.\n\u2022 An aggregation framework to manage global state.\n\u2022 GraphLab implementations and experimental evalua-\ntions of parameter learning and inference in graphi-\ncal models, Gibbs sampling, CoEM, Lasso and com-\npressed sensing on real-world problems.\n1The C++ reference implementation of the GraphLab is avail-\nable at http://select.cs.cmu.edu/code.\narXiv:1006.4990v1  [cs.LG]  25 Jun 2010\n2\nEXISTING FRAMEWORKS\nThere are several existing frameworks for designing and\nimplementing parallel ML algorithms. Because GraphLab\ngeneralizes these ideas and addresses several of their criti-\ncal limitations we brie\ufb02y review these frameworks.\n2.1\nMAP-REDUCE ABSTRACTION\nA program implemented in the MapReduce framework\nconsists of a Map operation and a Reduce operation. The\nMap operation is a function which is applied independently\nand in parallel to each datum (e.g., webpage) in a large data\nset (e.g., computing the word-count). The Reduce oper-\nation is an aggregation function which combines the Map\noutputs (e.g., computing the total word count). MapReduce\nperforms optimally only when the algorithm is embarrass-\ningly parallel and can be decomposed into a large num-\nber of independent computations. The MapReduce frame-\nwork expresses the class of ML algorithms which \ufb01t the\nStatistical-Query model [Chu et al., 2006] as well as prob-\nlems where feature extraction dominates the run-time.\nThe MapReduce abstraction fails when there are computa-\ntional dependencies in the data. For example, MapReduce\ncan be used to extract features from a massive collection of\nimages but cannot represent computation that depends on\nsmall overlapping subsets of images. This critical limita-\ntion makes it dif\ufb01cult to represent algorithms that operate\non structured models. As a consequence, when confronted\nwith large scale problems, we often abandon rich struc-\ntured models in favor of overly simplistic methods that are\namenable to the MapReduce abstraction.\nMany ML algorithms iteratively transform parameters dur-\ning both learning and inference. For example, algorithms\nlike Belief Propagation (BP), EM, gradient descent, and\neven Gibbs sampling, iteratively re\ufb01ne a set of parameters\nuntil some termination condition is achieved. While the\nMapReduce abstraction can be invoked iteratively, it does\nnot provide a mechanism to directly encode iterative com-\nputation. As a consequence, it is not possible to express\nsophisticated scheduling, automatically assess termination,\nor even leverage basic data persistence.\nThe popular implementations of the MapReduce abstrac-\ntion are targeted at large data-center applications and there-\nfore optimized to address node-failure and disk-centric par-\nallelism. The overhead associated with the fault-tolerant,\ndisk-centric approach is unnecessarily costly when applied\nto the typical cluster and multi-core settings encountered\nin ML research. Nonetheless, MapReduce is used in small\nclusters and even multi-core settings [Chu et al., 2006]. The\nGraphLab implementation2 described in this paper does not\naddress fault-tolerance or parallel disk access and instead\n2The GraphLab abstraction is intended for both the multicore\nand cluster settings and a distributed, fault-tolerant implementa-\ntion is ongoing research.\nassumes that processors do not fail and all data is stored in\nshared-memory. As a consequence, GraphLab does not in-\ncur the unnecessary disk overhead associated with MapRe-\nduce in the multi-core setting.\n2.2\nDAG ABSTRACTION\nIn the DAG abstraction, parallel computation is represented\nas a directed acyclic graph with data \ufb02owing along edges\nbetween vertices. Vertices correspond to functions which\nreceive information on inbound edges and output results\nto outbound edges. Implementations of this abstraction in-\nclude Dryad [Isard et al., 2007] and Pig Latin [Olston et al.,\n2008].\nWhile the DAG abstraction permits rich computational de-\npendencies it does not naturally express iterative algo-\nrithms since the structure of the data\ufb02ow graph depends on\nthe number of iterations (which must therefore be known\nprior to running the program). The DAG abstraction also\ncannot express dynamically prioritized computation.\n2.3\nSYSTOLIC ABSTRACTION\nThe Systolic abstraction [Kung and Leiserson, 1980] (and\nthe closely related Data\ufb02ow abstraction) extends the DAG\nframework to the iterative setting. Just as in the DAG Ab-\nstraction, the Systolic abstraction forces the computation to\nbe decomposed into small atomic components with limited\ncommunication between the components. The Systolic ab-\nstraction uses a directed graph G = (V, E) which is not\nnecessarily acyclic) where each vertex represents a proces-\nsor, and each edge represents a communication link. In\na single iteration, each processor reads all incoming mes-\nsages from the in-edges, performs some computation, and\nwrites messages to the out-edges. A barrier synchroniza-\ntion is performed between each iteration, ensuring all pro-\ncessors compute and communicate in lockstep.\nWhile the Systolic framework can express iterative com-\nputation, it is unable to express the wide variety of update\nschedules used in ML algorithms. For example, while gra-\ndient descent may be run within the Systolic abstraction us-\ning a Jacobi schedule it is not possible to implement coor-\ndinate descent which requires the more sequential Gauss-\nSeidel schedule.\nThe Systolic abstraction also cannot\nexpress the dynamic and specialized structured schedules\nwhich were shown by Gonzalez et al. [2009a,b] to dramat-\nically improve the performance of algorithms like BP.\n3\nTHE GRAPHLAB ABSTRACTION\nBy targeting common patterns in ML, like sparse data\ndependencies and asynchronous iterative computation,\nGraphLab achieves a balance between low-level and\nhigh-level abstractions.\nUnlike many low-level abstrac-\ntions (e.g., MPI, PThreads), GraphLab insulates users\nfrom the complexities of synchronization, data races and\ndeadlocks by providing a high level data representation\nthrough the data graph and automatically maintained data-\nconsistency guarantees through con\ufb01gurable consistency\nmodels. Unlike many high-level abstractions (i.e., MapRe-\nduce), GraphLab can express complex computational de-\npendencies using the data graph and provides sophisti-\ncated scheduling primitives which can express iterative\nparallel algorithms with dynamic scheduling.\nTo aid in the presentation of the GraphLab framework we\nuse Loopy Belief Propagation (BP) [Pearl, 1988] on pair-\nwise Markov Random Fields (MRF) as a running example.\nA pairwise MRF is an undirected graph over random vari-\nables where edges represent interactions between variables.\nLoopy BP is an approximate inference algorithm which es-\ntimates the marginal distributions by iteratively recomput-\ning parameters (messages) associated with each edge until\nsome convergence condition is achieved.\n3.1\nDATA MODEL\nThe GraphLab data model consists of two parts: a directed\ndata graph and a shared data table.\nThe data graph\nG = (V, E) encodes both the problem speci\ufb01c sparse com-\nputational structure and directly modi\ufb01able program state.\nThe user can associate arbitrary blocks of data (or param-\neters) with each vertex and directed edge in G. We denote\nthe data associated with vertex v by Dv, and the data asso-\nciated with edge (u \u2192v) by Du\u2192v. In addition, we use\n(u \u2192\u2217) to represent the set of all outbound edges from\nu and (\u2217\u2192v) for inbound edges at v. To support glob-\nally shared state, GraphLab provides a shared data table\n(SDT) which is an associative map, T [Key] \u2192Value, be-\ntween keys and arbitrary blocks of data.\nIn the Loopy BP, the data graph is the pairwise MRF, with\nthe vertex data Dv to storing the node potentials and the\ndirected edge data Du\u2192v storing the BP messages.\nIf\nthe MRF is sparse then the data graph is also sparse and\nGraphLab will achieve a high degree of parallelism. The\nSDT can be used to store shared hyper-parameters and the\nglobal convergence progress.\n3.2\nUSER DEFINED COMPUTATION\nComputation in GraphLab can be performed either through\nan update function which de\ufb01nes the local computation,\nor through the sync mechanism which de\ufb01nes global ag-\ngregation. The Update Function is analogous to the Map in\nMapReduce, but unlike in MapReduce, update unctions are\npermitted to access and modify overlapping contexts in the\ngraph. The sync mechanism is analogous to the Reduce\noperation, but unlike in MapReduce, the sync mechanism\nruns concurrently with the update functions.\n3.2.1\nUpdate Functions\nA GraphLab update function is a stateless user-de\ufb01ned\nfunction which operates on the data associated with small\nneighborhoods in the graph and represents the core element\nAlgorithm 1: Sync Algorithm on k\nt \u2190r(0)\nk\nforeach v \u2208V do\nt \u2190Foldk(Dv, t)\nT [k] \u2190Applyk(t)\nof computation. For every vertex v, we de\ufb01ne Sv as the\nneighborhood of v which consists of v, its adjacent edges\n(both inbound and outbound) and its neighboring vertices\nas shown in Fig. 1(a).\nWe de\ufb01ne DSv as the data cor-\nresponding to the neighborhood Sv. In addition to DSv,\nupdate functions also have read-only access, to the shared\ndata table T. We de\ufb01ne the application of the update func-\ntion f to the vertex v as the state mutating computation:\nDSv \u2190f(DSv, T).\nWe refer to the neighborhood Sv as the scope of v because\nSv de\ufb01nes the extent of the graph that can be accessed by\nf when applied to v. For notational simplicity, we denote\nf(DSv, T) as f(v). A GraphLab program may consist of\nmultiple update functions and it is up to the scheduling\nmodel (see Sec. 3.4) to determine which update functions\nare applied to which vertices and in which parallel order.\n3.2.2\nSync Mechanism\nThe sync mechanism aggregates data across all vertices in\nthe graph in a manner analogous to the Fold and Reduce\noperations in functional programming. The result of the\nsync operation is associated with a particular entry in the\nShared Data Table (SDT). The user provides a key k, a fold\nfunction (Eq. (3.1)), an apply function (Eq. (3.3)) as well\nas an initial value r(0)\nk\nto the SDT and an optional merge\nfunction used to construct parallel tree reductions.\nr(i+1)\nk\n\u2190\nFoldk\n\u0010\nDv, r(i)\nk\n\u0011\n(3.1)\nrl\nk\n\u2190\nMergek\n\u0010\nri\nk, rj\nk\n\u0011\n(3.2)\nT [k]\n\u2190\nApplyk(r(|V |)\nk\n)\n(3.3)\nWhen the sync mechanism is invoked, the algorithm in\nAlg. 1 uses the Foldk function to sequentially aggregate\ndata across all vertices. The Foldk function obeys the same\nconsistency rules (described in Sec. 3.3) as update func-\ntions and is therefore able to modify the vertex data. If\nthe Mergek function is provided a parallel tree reduction is\nused to combine the results of multiple parallel folds. The\nApplyk then \ufb01nalizes the resulting value (e.g., rescaling)\nbefore it is written back to the SDT with key k.\nThe sync mechanism can be set to run periodically in the\nbackground while the GraphLab engine is actively apply-\ning update functions or on demand triggered by update\nfunctions or user code. If the sync mechanism is executed\nScopev\nv\nData\nData\nData\nData\nData\nData\nData\nData\nData\nData\nData\nData\nData\nData\nData\nData\nData\nData\nData\nData\nData\nData\nData\nData\nData\nData\n(a) Scope\nData\nData\nData\nData\nData\nData\nData\nData\nData\nData\nData\nData\nData\nData\nData\nData\nData\nData\nData\nData\nData\nData\nData\nData\nData\nData\n(b) Consistency Models\nFigure 1: (a) The scope, Sv, of vertex v consists of all the data at\nthe vertex v, its inbound and outbound edges, and its neighboring\nvertices. The update function f when applied to the vertex v can\nread and modify any data within Sv. (b). We illustrate the 3\ndata consistency models by drawing their exclusion sets as a ring\nwhere no two update functions may be executed simultaneously\nif their exclusions sets (rings) overlap.\nin the background, the resulting aggregated value may not\nbe globally consistent. Nonetheless, many ML applications\nare robust to approximate global statistics.\nIn the context of the Loopy BP example, the update func-\ntion is the BP message update in which each vertex recom-\nputes its outbound messages by integrating the inbound\nmessages.\nThe sync mechanism is used to monitor the\nglobal convergence criterion (for instance, average change\nor residual in the beliefs). The Foldk function accumulates\nthe residual at the vertex, and the Applyk function divides\nthe \ufb01nal answer by the number of vertices. To monitor\nprogress, we let GraphLab run the sync mechanism as a\nperiodic background process.\n3.3\nDATA CONSISTENCY\nSince scopes may overlap, the simultaneous execution of\ntwo update functions can lead to race-conditions resulting\nin data inconsistency and even corruption. For example,\ntwo function applications to neighboring vertices could si-\nmultaneously try to modify data on a shared edge resulting\nin a corrupted value. Alternatively, a function trying to nor-\nmalize the parameters on a set of edges may compute the\nsum only to \ufb01nd that the edge values have changed.\nGraphLab provides a choice of three data consistency mod-\nels which enable the user to balance performance and data\nconsistency. The choice of data consistency model deter-\nmines the extent to which overlapping scopes can be exe-\ncuted simultaneously. We illustrate each of these models\nin Fig. 1(b) by drawing their corresponding exclusion sets.\nGraphLab guarantees that update functions never simulta-\nneously share overlapping exclusion sets. Therefore larger\nexclusion sets lead to reduced parallelism by delaying the\nexecution of update functions on nearby vertices.\nThe full consistency model ensures that during the exe-\ncution of f(v) no other function will read or modify data\nwithin Sv. Therefore, parallel execution may only occur on\nvertices that do not share a common neighbor. The slightly\nweaker edge consistency model ensures that during the ex-\necution of f(v) no other function will read or modify any\nof the data on v or any of the edges adjacent to v. Under\nthe edge consistency model, parallel execution may only\noccur on non-adjacent vertices. Finally, the weakest vertex\nconsistency model only ensures that during the execution\nof f(v) no other function will be applied to v. The vertex\nconsistency model is therefore prone to race conditions and\nshould only be used when reads and writes to adjacent data\ncan be done safely (In particular repeated reads may return\ndifferent results). However, by permitting update functions\nto be applied simultaneously to neighboring vertices, the\nvertex consistency model permits maximum parallelism.\nChoosing the right consistency model has direct implica-\ntions to program correctness. One method to prove correct-\nness of a parallel algorithm is to show that it is equivalent\nto a correct sequential algorithm. To capture the relation\nbetween sequential and parallel execution of a program we\nintroduce the concept of sequential consistency:\nDe\ufb01nition 3.1 (Sequential Consistency). A GraphLab pro-\ngram is sequentially consistent if for every parallel execu-\ntion, there exists a sequential execution of update functions\nthat produces an equivalent result.\nThe sequential consistency property is typically a suf\ufb01cient\ncondition to extend algorithmic correctness from the se-\nquential setting to the parallel setting. In particular, if the\nalgorithm is correct under any sequential execution of up-\ndate functions, then the parallel algorithm is also correct if\nsequential consistency is satis\ufb01ed.\nProposition 3.1. GraphLab guarantees sequential consis-\ntency under the following three conditions:\n1. The full consistency model is used\n2. The edge consistency model is used and update func-\ntions do not modify data in adjacent vertices.\n3. The vertex consistency model is used and update func-\ntions only access local vertex data.\nIn the Loopy BP example the update function only needs to\nread and modify data on the adjacent edges. Therefore the\nedge consistency model ensures sequential consistency.\n3.4\nSCHEDULING\nThe GraphLab update schedule describes the order in\nwhich update functions are applied to vertices and is rep-\nresented by a parallel data-structure called the scheduler.\nThe scheduler abstractly represents a dynamic list of tasks\n(vertex-function pairs) which are to be executed by the\nGraphLab engine.\nBecause constructing a scheduler requires reasoning\nabout the complexities of parallel algorithm design, the\nGraphLab framework provides a collection of base sched-\nules.\nTo represent Jacobi style algorithms (e.g., gradi-\nent descent) GraphLab provides a synchronous sched-\nuler which ensures that all vertices are updated simulta-\nneously. To represent Gauss-Seidel style algorithms (e.g.,\nGibbs sampling, coordinate descent), GraphLab provides a\nround-robin scheduler which updates all vertices sequen-\ntially using the most recently available data.\nMany ML algorithms (e.g., Lasso, CoEM, Residual BP) re-\nquire more control over the tasks that are created and the\norder in which they are executed. Therefore, GraphLab\nprovides a collection of task schedulers which permit up-\ndate functions to add and reorder tasks. GraphLab pro-\nvides two classes of task schedulers. The FIFO sched-\nulers only permit task creation but do not permit task re-\nordering. The prioritized schedules permit task reordering\nat the cost of increased overhead. For both types of task\nscheduler GraphLab also provide relaxed versions which\nincrease performance at the expense of reduced control:\nStrict Order\nRelaxed Order\nFIFO\nSingle Queue\nMulti Queue / Partitioned\nPrioritized\nPriority Queue\nApprox. Priority Queue\nIn addition GraphLab provides the splash scheduler based\non the loopy BP schedule proposed by Gonzalez et al.\n[2009a] which executes tasks along spanning trees.\nIn the Loopy BP example, different choices of scheduling\nleads to different BP algorithms. Using the Synchronous\nscheduler corresponds to the classical implementation of\nBP and using priority scheduler corresponds to Residual\nBP [Elidan et al., 2006].\n3.4.1\nSet Scheduler\nBecause scheduling is important to parallel algorithm de-\nsign, GraphLab provides a scheduler construction frame-\nwork called the set scheduler which enables users to safely\nand easily compose custom update schedules. To use the\nset scheduler the user speci\ufb01es a sequence of vertex set\nand update function pairs ((S1, f1), (S2, f2) \u00b7 \u00b7 \u00b7 (Sk, fk)),\nwhere Si \u2286V and fi is an update function. This sequence\nimplies the following execution semantics:\nfor i = 1 \u00b7 \u00b7 \u00b7 k do\nExecute fi on all vertices in Si in parallel.\nWait for all updates to complete\nThe amount of parallelism depends on the size of each set;\nthe procedure is highly sequential if the set sizes are small.\nExecuting the schedule in the manner described above can\nlead to the majority of the processors waiting for a few pro-\ncessors to complete the current set. However, by leveraging\nthe causal data dependencies encoded in the graph structure\nwe are able to construct an execution plan which identi\ufb01es\ntasks in future sets that can be executed early while still\nproducing an equivalent result.\nv1\nUpdate1\nv2\nv5\nv3\nUpdate2\nv4\nDesired Execution Sequence\nv1\nUpdate1\nv2\nv5\nv3\nUpdate2\nv4\nExecution Plan\nv1\nv3\nv5\nv4\nv2\nData Graph\nFigure 2: A simple example of the set scheduler planning pro-\ncess. Given the data graph, and a desired sequence of execution\nwhere v1, v2 and v5 are \ufb01rst run in parallel, then followed by v3\nand v4. If the edge consistency model is used, we observe that the\nexecution of v3 depends on the state of v1, v2 and v5, but the v4\nonly depends on the state of v5. The dependencies are encoded in\nthe execution plan on the right. The resulting plan allows v4 to be\nimmediately executed after v5 without waiting for v1 and v2.\nData\nData\nData\nData\nData\nData\nData\nData\nData\nData\nData\nData\nData\nData\nData\nData\nData\n1\nData\nData\n2\nData\nData\n3\nData\nData\n4\nData\nData\nData Dependency Graph\nShared Data Table\nCPU 1\nCPU 2\nCPU 3\nUpdate1(v1)\nUpdate2(v5)\nUpdate1(v3)\nUpdate1(v9)\n\u2026\nExecute Update\nScheduler\nFigure 3:\nA summary of the GraphLab framework. The user\nprovides a graph representing the computational data dependen-\ncies, as well as a SDT containing read only data. The user also\npicks a scheduling method or de\ufb01nes a custom schedule, which\nprovides a stream of update tasks in the form of (vertex, function)\npairs to the processors.\nThe set scheduler compiles an execution plan by rewrit-\ning the execution sequence as a Directed Acyclic Graph\n(DAG), where each vertex in the DAG represents an update\ntask in the execution sequence and edges represent execu-\ntion dependencies. Fig. 2 provides an example of this pro-\ncess. The DAG imposes a partial ordering over tasks which\ncan be compiled into a parallel execution schedule using\nthe greedy algorithm described by Graham [1966].\n3.5\nTERMINATION ASSESSMENT\nEf\ufb01cient parallel termination assessment can be challeng-\ning. The standard termination conditions used in many it-\nerative ML algorithms require reasoning about the global\nstate.\nThe GraphLab framework provides two methods\nfor termination assessment.\nThe \ufb01rst method relies on\nthe scheduler which signals termination when there are no\nremaining tasks. This method works for algorithms like\nResidual BP, which use task schedulers and stop produc-\ning new tasks when they converge. The second termination\nmethod relies on user provided termination functions which\nexamine the SDT and signal when the algorithm has con-\nverged. Algorithms, like parameter learning, which rely on\nglobal statistics use this method.\n3.6\nSUMMARY AND IMPLEMENTATION\nA GraphLab program is composed of the following parts:\n1. A data graph which represents the data and compu-\ntational dependencies.\n2. Update functions which describe local computation\n3. A Sync mechanism for aggregating global state.\n4. A data consistency model (i.e., Fully Consistent,\nEdge Consistent or Vertex Consistent), which deter-\nmines the extent to which computation can overlap.\n5. Scheduling primitives which express the order of\ncomputation and may depend dynamically on the data.\nWe implemented an optimized version of the GraphLab\nframework in C++ using PThreads.\nThe resulting\nGraphLab API is available under the LGPL license at\nhttp://select.cs.cmu.edu/code.\nThe data con-\nsistency models were implemented using race-free and\ndeadlock-free ordered locking protocols. To attain max-\nimum performance we addressed issues related to paral-\nlel memory allocation, concurrent random number gener-\nation, and cache ef\ufb01ciency. Since mutex collisions can be\ncostly, lock-free data structures and atomic operations were\nused whenever possible. To achieve the same level of per-\nformance for parallel learning system, the ML community\nwould have to repeatedly overcome many of the same time\nconsuming systems challenges needed to build GraphLab.\nThe GraphLab API has the opportunity to be an interface\nbetween the ML and systems communities. Parallel ML\nalgorithms built around the GraphLab API automatically\nbene\ufb01t from developments in parallel data structures. As\nnew locking protocols and parallel scheduling primitives\nare incorporated into the GraphLab API, they become im-\nmediately available to the ML community. Systems experts\ncan more easily port ML algorithms to new parallel hard-\nware by porting the GraphLab API.\n4\nCASE STUDIES\nTo demonstrate the expressiveness of the GraphLab ab-\nstraction and illustrate the parallel performance gains it\nprovides, we implement four popular ML algorithms and\nevaluate these algorithms on large real-world problems us-\ning a 16-core computer with 4 AMD Opteron 8384 proces-\nsors and 64GB of RAM.\n4.1\nMRF PARAMETER LEARNING\nTo demonstrate how the various components of the\nGraphLab framework can be assembled to build a com-\nplete ML \u201cpipeline,\u201d we use GraphLab to solve a novel\nthree-dimensional retinal image denoising task. In this task\nwe begin with raw three-dimensional laser density esti-\nmates, then use GraphLab to generate composite statistics,\nlearn parameters for a large three-dimensional grid pair-\nwise MRF, and then \ufb01nally compute expectations for each\nvoxel using Loopy BP. Each of these tasks requires both\nAlgorithm 2: BP update function\nBPUpdate(Dv, D\u2217\u2192v, Dv\u2192\u2217\u2208Sv) begin\nCompute the local belief b(xv) using {D\u2217\u2192vDv}\nforeach (v \u2192t) \u2208(v \u2192\u2217) do\nUpdate mv\u2192t(xt) using {D\u2217\u2192v, Dv} and \u03bbaxis(vt)\nfrom the SDT.\nresidual \u2190\n\f\f\f\fmv\u2192t(xt) \u2212mold\nv\u2192t(xt)\n\f\f\f\f\n1\nif residual > Termination Bound then\nAddTask(t, residual)\nend\nend\nend\nAlgorithm 3: Parameter Learning Sync\nFold(acc, vertex) begin\nReturn acc + image statistics on vertex\nend\nApply(acc) begin\nApply gradient step to \u03bb using acc and return \u03bb\nend\nlocal iterative computation and global aggregation as well\nas several different computation schedules.\nWe begin by using the GraphLab data-graph to build a large\n(256x64x64) three dimensional MRF in which each ver-\ntex corresponds to a voxel in the original image. We con-\nnect neighboring voxels in the 6 axis aligned directions.\nWe store the density observations and beliefs in the vertex\ndata and the BP messages in the directed edge data. As\nshared data we store three global edge parameters which\ndetermine the smoothing (accomplished using a Laplace\nsimilarity potentials) in each dimension.\nPrior to learn-\ning the model parameters, we \ufb01rst use the GraphLab sync\nmechanism to compute axis-aligned averages as a proxy\nfor \u201cground-truth\u201d smoothed images along each dimension.\nWe then performed simultaneous learning and inference\nin GraphLab by using the background sync mechanism\n(Alg. 3) to aggregate inferred model statistics and apply a\ngradient descent procedure. To the best of our knowledge,\nthis is the \ufb01rst time graphical model parameter learning and\nBP inference have been done concurrently.\nResults:\nIn Fig. 4(a) we plot the speedup of the parame-\nter learning algorithm, executing inference and learning se-\nquentially. We found that the Splash scheduler outperforms\nother scheduling techniques enabling a factor 15 speedup\non 16 cores. We then evaluated simultaneous parameter\nlearning and inference by allowing the sync mechanism to\nrun concurrently with inference (Fig. 4(b) and Fig. 4(c)).\nBy running a background sync at the right frequency, we\nfound that we can further accelerate parameter learning\nwhile only marginally affecting the learned parameters. In\nFig. 4(d) and Fig. 4(e) we plot examples of noisy and de-\nnoised cross sections respectively.\n0\n2\n4\n6\n8\n10\n12\n14\n16\n0\n2\n4\n6\n8\n10\n12\n14\n16\nNumber of Processors\nSpeedup\nPriority Schedule\nApprox. Priority Schedule\nLinear\nSplash Schedule\n(a) Speedup\n15\n30\n45\n60\n75\n90\n105\n120\n0\n500\n1000\n1500\n2000\nSync Frequency (Seconds)\nTotal Runtime (Seconds)\nTotal Learning Runtime\n(b) Bkgnd Sync. Runtime\n0\n15\n30\n45\n60\n75\n90\n105\n120\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\nSync Frequency (Seconds)\nAverage % deviation\nAverage % deviation\n(c) Bkgnd Sync. Error\n(d) Original\n(e) Denoised\nFigure 4: Retinal Scan Denoising (a) Speedup relative to the best single processor runtime of parameter learning using priority, approx\npriority, and Splash schedules. (b) The total runtime in seconds of parameter learning and (c) the average percent deviation in learned\nparameters plotted against the time between gradient steps using the Splash schedule on 16 processors. (d,e) A slice of the original noisy\nimage and the corresponding expected pixel values after parameter learning and denoising.\n4.2\nGIBBS SAMPLING\nThe Gibbs sampling algorithm is inherently sequential and\nhas frustrated efforts to build asymptotically consistent par-\nallel samplers. However, a standard result in parallel al-\ngorithms [Bertsekas and Tsitsiklis, 1989] is that for any\n\ufb01xed length Gauss-Seidel schedule there exists an equiv-\nalent parallel execution which can be derived from a col-\noring of the dependency graph. We can extract this form\nof parallelism using the GraphLab framework. We \ufb01rst use\nGraphLab to construct a greedy graph coloring on the MRF\nand then to execute an exact parallel Gibbs sampler.\nWe implement the standard greedy graph coloring algo-\nrithm in GraphLab by writing an update function which\nexamines the colors of the neighboring vertices of v, and\nsets v to the \ufb01rst unused color. We use the edge consis-\ntency model with the parallel coloring algorithm to ensure\nthat the parallel execution retains the same guarantees as\nthe sequential version. The parallel Gauss-Seidel schedule\nis then built using the GraphLab set scheduler (Sec. 3.4.1)\nand the coloring of the MRF. The resulting schedule con-\nsists of a sequence of vertex sets S1 to SC such that Si\ncontains all the vertices with color i. The vertex consis-\ntency model is suf\ufb01cient since the coloring ensures full se-\nquential consistency.\nTo evaluate the GraphLab parallel Gibbs sampler we con-\nsider the challenging task of marginal estimation on a fac-\ntor graph representing a protein-protein interaction network\nobtained from Elidan et al. [2006] by generating 10, 000\nsamples. The resulting MRF has roughly 100K edges and\n14K vertices. As a baseline for comparison we also ran\na GraphLab version of the highly optimized Splash Loopy\nBP [Gonzalez et al., 2009b] algorithm.\nResults:\nIn Fig. 5 we present the speedup and ef\ufb01ciency\nresults for Gibbs sampling and Loopy BP. Using the set\nschedule in conjunction with the planning optimization en-\nables the Gibbs sampler to achieve a factor of 10 speedup\non 16 processors. The execution plan takes 0.05 seconds\nto compute, an immaterial fraction of the 16 processor run-\nning time. Because of the structure of the MRF, a large\nnumber of colors (20) is needed and the vertex distribu-\ntion over colors is heavily skewed. Consequently there is\na strong sequential component to running the Gibbs sam-\npler on this model.\nIn contrast the Loopy BP speedup\ndemonstrates considerably better scaling with factor of 15\nspeedup on 16 processor. The larger cost per BP update\nin conjunction with the ability to run a fully asynchronous\nschedule enables Loopy BP to achieve relatively uniform\nupdate ef\ufb01ciency compared to Gibbs sampling.\n4.3\nCO-EM\nTo illustrate how GraphLab scales in settings with large\nstructured models we designed and implemented a parallel\nversion of Co-EM [Jones, Nigam and Ghani, 2000], a semi-\nsupervised learning algorithm for named entity recognition\n(NER). Given a list of noun phrases (NP) (e.g., \u201cbig ap-\nple\u201d), contexts (CT) (e.g., \u201ccitizen of \u201d), and co-occurence\ncounts for each NP-CT pair in a training corpus, CoEM\ntries to estimate the probability (belief) that each entity (NP\nor CT) belongs to a particular class (e.g., \u201ccountry\u201d or \u201cper-\nson\u201d). The CoEM update function is relatively fast, requir-\ning only a few \ufb02oating operations, and therefore stresses\nthe GraphLab implementation by requiring GraphLab to\nmanage massive amounts of \ufb01ne-grained parallelism.\nThe GraphLab graph for the CoEM algorithm is a bipar-\ntite graph with each NP and CT represented as a vertex,\nconnected by edges with weights corresponding to the co-\noccurence counts. Each vertex stores the current estimate\nof the belief for the corresponding entity. The update func-\ntion for CoEM recomputes the local belief by taking a\nweighted average of the adjacent vertex beliefs. The adja-\ncent vertices are rescheduled if the belief changes by more\nthan some prede\ufb01ned threshold (10\u22125).\nWe experimented with the following two NER datasets ob-\ntained from web-crawling data.\nName\nClasses\nVerts.\nEdges\n1 CPU Runtime\nsmall\n1\n0.2 mil.\n20 mil.\n40 min\nlarge\n135\n2 mil.\n200 mil.\n8 hours\n2\n4\n6\n8\n10\n12\n14\n16\n0\n2\n4\n6\n8\n10\n12\n14\n16\nNumber of Processors\nSpeedup\nLinear\nPlanned Set Schedule\nRound Robin Schedule\nSet Schedule\n(a) Gibbs Speedup\n0\n5\n10\n15\n20\n0\n1000\n2000\n3000\n4000\n5000\n6000\n7000\n8000\nColor\n# Vertices\n(b) Gibbs Color\n2\n4\n6\n8\n10\n12\n14\n16\n2\n4\n6\n8\n10\n12\n14\n16\nx 10\n4\nNumber of Processors\nSamples / (CPU * Second)\nPlanned Set Schedule\nRound Robin Schedule\nSet Schedule\n(c) Gibbs Eff.\n2\n4\n6\n8\n10\n12\n14\n16\n0\n2\n4\n6\n8\n10\n12\n14\n16\nNumber of Processors\nSpeedup\nLinear\nSplash Schedule\nResidual Schedule\n(d) BP Speedup\n2\n4\n6\n8\n10\n12\n14\n16\n1.9\n2\n2.1\n2.2\n2.3\n2.4\n2.5\nx 10\n4\nNumber of Processors\nUpdates / (CPU * Second)\nSplash Schedule\nResidual Schedule\n(e) BP Eff.\nFigure 5:\nMRF Inference (a) The speedup of the Gibbs sampler using three different schedules. The planned set schedule enables\nprocessors to safely execute more than one color simultaneously. The round robin schedule executes updates in a \ufb01xed order and relies\non the edge consistency model to maintain sequential consistency. The plan set scheduler does not apply optimization and therefore\nsuffers from substantial synchronization overhead. (b) The distribution of vertices over the 20 colors is strongly skewed resulting in a\nhigh sequential set schedule. (c) The sampling rate per processor plotted against the number of processor provides measure of parallel\noverhead which is substantially reduced by the plan optimization in the set scheduler. (d) The speedup for Loopy BP is improved\nsubstantially by the Splash. (e) The ef\ufb01ciency of the GraphLab framework as function of the number of processors.\nWe plot in Fig. 6(a) and Fig. 6(b) the speedup obtained by\nthe Partitioned Scheduler and the MultiQueue FIFO sched-\nuler, on both small and large datasets respectively. We ob-\nserve that both schedulers perform similarly and achieve\nnearly linear scaling. In addition, both schedulers obtain\nsimilar belief estimates suggesting that the update schedule\nmay not affect convergence in this application.\nWith 16 parallel processors, we could complete three full\nRound-robin iterations on the large dataset in less than\n30 minutes. As a comparison, a comparable Hadoop im-\nplementation took approximately 7.5 hours to complete\nthe exact same task, executing on an average of 95 cpus.\n[Personal communication with Justin Betteridge and Tom\nMitchell, Mar 12, 2010]. Our large performance gain can\nbe attributed to data persistence in the GraphLab frame-\nwork. Data persistence allows us to avoid the extensive\ndata copying and synchronization required by the Hadoop\nimplementation of MapReduce.\nUsing the \ufb02exibility of the GraphLab framework we were\nable to study the bene\ufb01ts of dynamic (Multiqueue FIFO)\nscheduling versus a regular round-robin scheduling in\nCoEM. Fig. 6(c) compares the number of updates required\nby both schedules to obtain a result of comparable quality\non the larger dataset. Here we measure quality by L1 pa-\nrameter distance to an empirical estimate of the \ufb01xed point\nx\u2217, obtained by running a large number of synchronous it-\nerations. For this application we do not \ufb01nd a substantial\nbene\ufb01t from dynamic scheduling.\nWe also investigated how GraphLab scales with problem\nsize.\nFigure 6(d) shows the maximum speedup on 16\ncpus attained with varying graph sizes, generated by sub-\nsampling a fraction of vertices from the large dataset. We\n\ufb01nd that parallel scaling improves with problem size and\nthat even on smaller problems GraphLab is still able to\nachieve a factor of 12 speedup on 16 cores.\n4.4\nLASSO\nThe Lasso [Tibshirani, 1996] is a popular feature selection\nand shrinkage method for linear regression which mini-\nmizes the objective L(w) = Pn\nj=1(wT xj \u2212yj)2+\u03bb ||w||1.\nUnfortunately, there does not exist, to the best of our\nknowledge, a parallel algorithm for \ufb01tting a Lasso model.\nIn this section we implement 2 different parallel algorithms\nfor solving the Lasso.\n4.4.1\nShooting Algorithm\nWe use GraphLab to implement the Shooting Algorithm\n[Fu, 1998], a popular Lasso solver, and demonstrate that\nGraphLab is able to automatically obtain parallelism by\nidentifying operations that can execute concurrently while\nretaining sequential consistency.\nThe shooting algorithm works by iteratively minimizing\nthe objective with respect to each dimension in w, cor-\nresponding to coordinate descent. We can formulate the\nShooting Algorithm in the GraphLab framework as a bi-\npartite graph with a vertex for each weight wi and a vertex\nfor each observation yj. An edge is created between wi\nand yj with weight Xi,j if and only if Xi,j is non-zero. We\nalso de\ufb01ne an update function (Alg. 4) which operates only\non the weight vertices, and corresponds exactly to a single\nminimization step in the shooting algorithm. A round-robin\nscheduling of Alg. 4 on all weight vertices corresponds ex-\nactly to the sequential shooting algorithm. We automati-\ncally obtain an equivalent parallel algorithm by select the\nfull consistency model. Hence, by encoding the shooting\nalgorithm in GraphLab we are able to discover a sequen-\ntially consistent automatic parallelization.\nWe evaluate the performance of the GraphLab implemen-\ntation on a \ufb01nancial data set obtained from Kogan et al.\n[2009]. The task is to use word counts of a \ufb01nancial report\nto predict stock volatility of the issuing company for the\nconsequent 12 months. Data set consists of word counts\nfor 30K reports with the related stock volatility metrics.\n2\n4\n6\n8\n10\n12\n14\n16\n0\n2\n4\n6\n8\n10\n12\n14\n16\nNumber of Processors\nSpeedup\nLinear\nMultiQueue FIFO\nPartitioned\n(a) CoEM Speedup Small\n2\n4\n6\n8\n10\n12\n14\n16\n0\n2\n4\n6\n8\n10\n12\n14\n16\nNumber of Processors\nSpeedup\nLinear\nMultiQueue FIFO\nPartitioned\n(b) CoEM Speedup Large\n0\n2\n4\n6\nx 10\n6\n\u221212\n\u221211\n\u221210\n\u22129\n\u22128\n\u22127\n\u22126\nNumber of updates\nLog(|x\u2212x*|1)\nRound robin\nMultiQueue FIFO\n(c) Convergence\n0\n20\n40\n60\n80\n100\n6\n8\n10\n12\n14\n16\nSingle Processor Runtime (Seconds)\nSpeedup with 16 cpus\n(d) Speedup with Problem Size\nFigure 6: CoEM Results (a,b) Speedup of MultiQueue FIFO and Partitioned Scheduler on both datasets. Speedup is measured relative\nto fastest running time on a single cpu. The large dataset achieves better scaling because the update function is slower. (c) Speed of\nconvergence measured in number of updates for MultiQueue FIFO and Round Robin (equivalent to synchronized Jacobi schedule), (d)\nSpeedup achieved with 16 cpus as the graph size is varied.\nAlgorithm 4: Shooting Algorithm\nShootingUpdate(Dwi, D\u2217\u2192wi, Dwi\u2192\u2217) begin\nMinimize the loss function with respect to wi\nif wi changed by > \u03f5 then\nRevise the residuals on all y\u2032s connected to wi\nSchedule all w\u2032s connected to neighboring y\u2032s\nend\nend\nTo demonstrate the scaling properties of the full consis-\ntency model, we create two datasets by deleting common\nwords.\nThe sparser dataset contains 209K features and\n1.2M non-zero entries, and the denser dataset contains\n217K features and 3.5M non-zero entries. The speedup\ncurves are plotted in Fig. 7. We observed better scaling\n(4x at 16 CPUs) on the sparser dataset than on the denser\ndataset (2x at 16 CPUs). This demonstrates that ensuring\nfull consistency on denser graphs inevitably increases con-\ntention resulting in reduced performance.\nAdditionally, we experimented with relaxing the consis-\ntency model, and we discovered that the shooting algorithm\nstill converges under the weakest vertex consistency guar-\nantees; obtaining solutions with only 0.5% higher loss on\nthe same termination criterion. The vertex consistent model\nis much more parallel and we can achieve signi\ufb01cantly bet-\nter speedup, especially on the denser dataset. It remains an\nopen question why the Shooting algorithm still functions\nunder such weak guarantees.\n4.5\nCompressed Sensing\nTo show how GraphLab can be used as a subcomponent of\na larger sequential algorithm, we implement a variation of\nthe interior point algorithm proposed by Kim et al. [2007]\nfor the purposes of compressed sensing. The aim is to use\na sparse linear combination of basis functions to represent\nthe image, while minimizing the reconstruction error. Spar-\nsity is achieved through the use of elastic net regularization.\n2\n4\n6\n8\n10\n12\n14\n16\n0\n2\n4\n6\n8\n10\n12\n14\n16\nNumber of Processors\nRelative Speedup\nLinear\nFull Consistency\nVertex Consistency\n(a) Sparser Dataset Speedup\n2\n4\n6\n8\n10\n12\n14\n16\n0\n2\n4\n6\n8\n10\n12\n14\n16\nNumber of Processors\nRelative Speedup\nFull Consistency\nVertex Consistency\nLinear\n(b) Denser Dataset Speedup\nFigure 7: Shooting Algorithm (a) Speedup on the sparser dataset\nusing vertex consistency and full consistency relative to the fastest\nsingle processor runtime. (b) Speedup on the denser dataset using\nvertex consistency and full consistency relative to the fastest single\nprocessor runtime.\nThe interior point method is a double loop algorithm where\nthe sequential outer loop (Alg. 5) implements a Newton\nmethod while the inner loop computes the Newton step by\nsolving a sparse linear system using GraphLab. We used\nGaussian BP (GaBP) as a linear solver [Bickson, 2008]\nsince it has a natural GraphLab representation. The GaBP\nGraphLab construction follows closely the BP example in\nSec. 4.1, but represents potentials and messages analyti-\ncally as Gaussian distributions. In addition, the outer loop\nuses a Sync operation on the data graph to compute the du-\nality gap and to terminate the algorithm when the gap falls\nbelow a prede\ufb01ned threshold. Because the graph structure\nis \ufb01xed across iterations, we can leverage data persistency\nin GraphLab, avoid both costly set up and tear down oper-\nations and resume from the converged state of the previous\niteration.\nWe evaluate the performance of this algorithm on a syn-\nthetic compressed sensing dataset constructed by apply-\ning a random projection matrix to a wavelet transform of\na 256 \u00d7 256 Lenna image (Fig. 8). Experimentally, we\nachieved a factor of 8 speedup using 16 processors using\nthe round-robin scheduling.\nAlgorithm 5: Compressed Sensing Outer Loop\nwhile duality gap \u2265\u03f5 do\nUpdate edge and node data of the data graph.\nUse GraphLab to run GaBP on the graph\nUse Sync to compute duality gap\nTake a newton step\nend\n2\n4\n6\n8\n10\n12\n14\n16\n0\n2\n4\n6\n8\n10\n12\n14\n16\nNumber of Processors\nSpeedup\nLinear\nL1 Interior Point\n(a) Speedup\n(b) Lenna\n(c) Lenna 50%\nFigure 8:\n(a) Speedup of the Interior Point algorithm on the\ncompressed sensing dataset, (b) Original 256x256 test image with\n65,536 pixels, (c) Output of compressed sensing algorithm using\n32,768 random projections.\n5\nCONCLUSIONS AND FUTURE WORK\nWe identi\ufb01ed several limitations in applying existing paral-\nlel abstractions like MapReduce to Machine Learning (ML)\nproblems. By targeting common patterns in ML, we devel-\noped GraphLab, a new parallel abstraction which achieves\na high level of usability, expressiveness and performance.\nUnlike existing parallel abstractions, GraphLab supports\nthe representation of structured data dependencies, iterative\ncomputation, and \ufb02exible scheduling.\nThe GraphLab abstraction uses a data graph to encode\nthe computational structure and data dependencies of the\nproblem. GraphLab represents local computation in the\nform of update functions which transform the data on the\ndata graph. Because update functions can modify over-\nlapping state, the GraphLab framework provides a set of\ndata consistency models which enable the user to specify\nthe minimal consistency requirements of their application\nwithout having to build their own complex locking proto-\ncols. To manage sharing and aggregation of global state,\nGraphLab provides a powerful sync mechanism.\nTo manage the scheduling of dynamic iterative parallel\ncomputation, the GraphLab abstraction provides a rich col-\nlection of parallel schedulers encompassing a wide range\nof ML algorithms. GraphLab also provides a scheduler\nconstruction framework built around a sequence of vertex\nsets which can be used to compose custom schedules.\nWe developed an optimized shared memory implementa-\ntion GraphLab and we demonstrated its performance and\n\ufb02exibility through a series of case studies. In each case\nstudy we designed and implemented a popular ML algo-\nrithm and applied it to a large real-world dataset achieving\nstate-of-the-art performance.\nOur ongoing research includes extending the GraphLab\nframework to the distributed setting allowing for compu-\ntation on even larger datasets. While we believe GraphLab\nnaturally extend to the distributed setting we face numer-\nous new challenges including ef\ufb01cient graph partitioning,\nload balancing, distributed locking, and fault tolerance.\nAcknowledgements\nWe thank Guy Blelloch and David O\u2019Hallaron for their\nguidance designing and implementing GraphLab.\nThis\nwork is supported by ONR Young Investigator Pro-\ngram grant N00014-08-1-0752, the ARO under MURI\nW911NF0810242, DARPA IPTO FA8750-09-1-0141, and\nthe NSF under grants IIS-0803333 and NeTS-NBD CNS-\n0721591. Joseph Gonzalez is supported by the AT&T Labs\nFellowship Program.\nReferences\nJ. Dean and S. Ghemawat. MapReduce: simpli\ufb01ed data process-\ning on large clusters. Commun. ACM, 51(1), 2004.\nC.T. Chu, S.K. Kim, Y.A. Lin, Y. Yu, G.R. Bradski, A.Y. Ng, and\nK. Olukotun. Map-reduce for machine learning on multicore.\nIn NIPS, 2006.\nJ. Wolfe, A. Haghighi, and D. Klein. Fully distributed EM for\nvery large datasets. In ICML. ACM, 2008.\nB. Panda, J.S. Herbach, S. Basu, and R.J. Bayardo.\nPlanet:\nmassively parallel learning of tree ensembles with mapreduce.\nProc. VLDB Endow., 2(2), 2009.\nJ. Ye, J. Chow, J. Chen, and Z. Zheng. Stochastic gradient boosted\ndistributed decision trees. In CIKM. ACM, 2009.\nM. Isard, M. Budiu, Y. Yu, A. Birrell, and D. Fetterly. Dryad:\ndistributed data-parallel programs from sequential building\nblocks. SIGOPS Oper. Syst. Rev., 41(3), 2007.\nC. Olston, B. Reed, U. Srivastava, R. Kumar, and A. Tomkins. Pig\nlatin: A not-so-foreign language for data processing. SIGMOD\n\u201908, pages ?\u2013?, June 2008.\nH. T. Kung and C. E. Leiserson. Algorithms for VLSI processor\narrays. Addison-Wesley, 1980.\nJ. Gonzalez, Y. Low, and C. Guestrin. Residual splash for opti-\nmally parallelizing belief propagation. In AISTATS, 2009a.\nJ. Gonzalez, Y. Low, C. Guestrin, and D. O\u2019Hallaron. Distributed\nparallel inference on large factor graphs. In UAI, July 2009b.\nJ. Pearl. Probabilistic reasoning in intelligent systems: networks\nof plausible inference. 1988.\nG. Elidan, I. Mcgraw, and D. Koller. Residual belief propagation:\nInformed scheduling for asynchronous message passing.\nIn\nUAI, 2006.\nR. L. Graham.\nBounds for certain multiprocessing anomalies.\nBell System Technical Journal (BSTJ), 45:1563\u20131581, 1966.\nD. Bertsekas and J. Tsitsiklis. Parallel and Distributed Computa-\ntion: Numerical Methods. Prentice-Hall, 1989.\nR. Jones. Learning to Extract Entities from Labeled and Unla-\nbeled Text. PhD thesis, Carnegie Mellon University.\nK. Nigam and R. Ghani. Analyzing the effectiveness and applica-\nbility of co-training. In CIKM, 2000.\nR. Tibshirani. Regression shrinkage and selection via the lasso. J\nROY STAT SOC B, 58:267\u2013288, 1996.\nWenjiang J. Fu.\nPenalized regressions: The bridge versus the\nlasso. J COMPUT GRAPH STAT, pages 397\u2013416, 1998.\nS. Kogan, D. Levin, B.R. Routledge, J.S. Sagi, and N. A. Smith.\nPredicting risk from \ufb01nancial reports with regression.\nIn\nHLT/NAACL, 2009.\nSeung-Jean Kim, K. Koh, M. Lustig, S. Boyd, and D. Gorinevsky.\nAn interior-point method for large-scale \u21131-regularized least\nsquares. IEEE J SEL TOP SIGNAL PROC, 1, 2007.\nD. Bickson. Gaussian Belief Propagation: Theory and Applica-\ntion. PhD thesis, The Hebrew University of Jerusalem, 2008.\n",
        "sentence": " , [5, 28]), and how to use database ideas to improve ML tasks (e.",
        "context": "32,768 random projections.\n5\nCONCLUSIONS AND FUTURE WORK\nWe identi\ufb01ed several limitations in applying existing paral-\nlel abstractions like MapReduce to Machine Learning (ML)\nproblems. By targeting common patterns in ML, we devel-\nphisticated machine learning (ML) techniques to be applied\nto increasingly challenging real-world problems. However,\nrecent developments in computer architecture have shifted\nthe focus away from frequency scaling and towards paral-\nvide powerful, expressive primitives, they force the user\nto address hardware issues and the challenges of parallel\ndata representation. Consequently, many ML experts have\nturned to high-level abstractions, which dramatically sim-"
    },
    {
        "title": "Machine Learning",
        "author": [
            "T.M. Mitchell"
        ],
        "venue": "McGraw Hill",
        "citeRegEx": "29",
        "shortCiteRegEx": null,
        "year": 1997,
        "abstract": "",
        "full_text": "",
        "sentence": " Numeric features can be discretized using standard techniques such as binning [29]. , Laplacian smoothing for Naive Bayes by adding a pseudocount of 1 to all frequency counts [29]. Unsupervised dimensionality reduction methods such as random hashing or PCA are also popular [15, 29].",
        "context": null
    },
    {
        "title": "The Logic of Representing Dependencies by Directed Graphs",
        "author": [
            "J. Pearl",
            "T. Verma"
        ],
        "venue": "AAAI",
        "citeRegEx": "30",
        "shortCiteRegEx": null,
        "year": 1987,
        "abstract": "",
        "full_text": "",
        "sentence": " The implication of EMVDs for probabilistic conditional independence in Bayesian networks was originally described by [30] and further explored by [42].",
        "context": null
    },
    {
        "title": "Data management challenges in production machine learning",
        "author": [
            "N. Polyzotis",
            "S. Roy",
            "S.E. Whang",
            "M. Zinkevich"
        ],
        "venue": "Proceedings of the 2017 ACM International Conference on Management of Data, SIGMOD \u201917, pages 1723\u20131726, New York, NY, USA",
        "citeRegEx": "31",
        "shortCiteRegEx": null,
        "year": 2017,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Furthermore, recent reports of Google\u2019s production ML systems show that features that yield marginal benefits incur high \u201ctechnical debt\u201d that decreases code mangeability and increases costs [38, 31].",
        "context": null
    },
    {
        "title": "Tree Induction for Probability-Based Ranking",
        "author": [
            "F. Provost",
            "P. Domingos"
        ],
        "venue": "Machine Learning, 52(3):199\u2013215",
        "citeRegEx": "32",
        "shortCiteRegEx": null,
        "year": 2003,
        "abstract": "",
        "full_text": "",
        "sentence": " While similar smoothing techniques have been studied for probability estimation using decision trees [32], to the best of our knowledge, this issue has not been handled in general for classification using decision trees.",
        "context": null
    },
    {
        "title": "Database Management Systems",
        "author": [
            "R. Ramakrishnan",
            "J. Gehrke"
        ],
        "venue": "McGraw-Hill, Inc.",
        "citeRegEx": "33",
        "shortCiteRegEx": null,
        "year": 2003,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " In particular, real-world relational databases often have many tables connected by database dependencies such as key-foreign key dependencies (KFKDs) [33]. Star schemas are ubiquitous in many applications, including retail, insurance, Web security, and recommendation systems [33, 26, 25].",
        "context": null
    },
    {
        "title": "Scaling Factorization Machines to Relational Data",
        "author": [
            "S. Rendle"
        ],
        "venue": "Proceedings of the VLDB Endowment",
        "citeRegEx": "34",
        "shortCiteRegEx": null,
        "year": 2013,
        "abstract": "The most common approach in predictive modeling is to describe cases with feature vectors (aka design matrix). Many machine learning methods such as linear regression or support vector machines rely on this representation. However, when the underlying data has strong relational patterns, especially relations with high cardinality, the design matrix can get very large which can make learning and prediction slow or even infeasible.This work solves this issue by making use of repeating patterns in the design matrix which stem from the underlying relational structure of the data. It is shown how coordinate descent learning and Bayesian Markov Chain Monte Carlo inference can be scaled for linear regression and factorization machine models. Empirically, it is shown on two large scale and very competitive datasets (Netflix prize, KDDCup 2012), that (1) standard learning algorithms based on the design matrix representation cannot scale to relational predictor variables, (2) the proposed new algorithms scale and (3) the predictive quality of the proposed generic feature-based approach is as good as the best specialized models that have been tailored to the respective tasks.",
        "full_text": "",
        "sentence": " The scenario of learning over joins of multiple tables without materializing the output of the join was studied in [25, 37, 34, 24], but their goal was primarily to reduce runtimes of some ML techniques without affecting accuracy.",
        "context": null
    },
    {
        "title": "Introducing CloudLab: Scientific Infrastructure for Advancing Cloud Architectures and Applications",
        "author": [
            "R. Ricci",
            "E. Eide",
            "C. Team"
        ],
        "venue": "; login:: the magazine of USENIX & SAGE, 39(6):36\u201338",
        "citeRegEx": "35",
        "shortCiteRegEx": null,
        "year": 2014,
        "abstract": "",
        "full_text": "",
        "sentence": " All experiments (except for ANN) were run on CloudLab, which offers free access to physical compute nodes for research [35].",
        "context": null
    },
    {
        "title": "Methods and Metrics for Cold-start Recommendations",
        "author": [
            "A.I. Schein",
            "A. Popescul",
            "L.H. Ungar",
            "D.M. Pennock"
        ],
        "venue": "Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
        "citeRegEx": "36",
        "shortCiteRegEx": null,
        "year": 2002,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Finally, we also do not study the \u201ccold start\u201d issue because it is orthogonal to the focus of this paper [36].",
        "context": null
    },
    {
        "title": "Learning Linear Regression Models over Factorized Joins",
        "author": [
            "M. Schleich",
            "D. Olteanu",
            "R. Ciucanu"
        ],
        "venue": "Proceedings of the 2016 International Conference on Management of Data, SIGMOD \u201916",
        "citeRegEx": "37",
        "shortCiteRegEx": null,
        "year": 2016,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " The scenario of learning over joins of multiple tables without materializing the output of the join was studied in [25, 37, 34, 24], but their goal was primarily to reduce runtimes of some ML techniques without affecting accuracy.",
        "context": null
    },
    {
        "title": "Machine Learning: The High Interest Credit Card of Technical Debt",
        "author": [
            "D. Sculley",
            "G. Holt",
            "D. Golovin",
            "E. Davydov",
            "T. Phillips",
            "D. Ebner",
            "V. Chaudhary",
            "M. Young",
            "J.-F. Crespo",
            "D. Dennison"
        ],
        "venue": "SE4ML: Software Engineering for Machine Learning ",
        "citeRegEx": "38",
        "shortCiteRegEx": null,
        "year": 2014,
        "abstract": "",
        "full_text": "",
        "sentence": " Furthermore, recent reports of Google\u2019s production ML systems show that features that yield marginal benefits incur high \u201ctechnical debt\u201d that decreases code mangeability and increases costs [38, 31].",
        "context": null
    },
    {
        "title": "Database Systems Concepts",
        "author": [
            "A. Silberschatz",
            "H. Korth",
            "S. Sudarshan"
        ],
        "venue": "McGraw-Hill, Inc., New York, NY, USA, 5 edition",
        "citeRegEx": "39",
        "shortCiteRegEx": null,
        "year": 2006,
        "abstract": "",
        "full_text": "",
        "sentence": " While KFKDs are not the same as FDs [39], assuming features have \u201cclosed\u201d domains, they behave essentially as FDs in the output of the join [26]. From a data management perspective, there are database dependencies more general than FDs: embedded multi-valued dependencies (EMVDs) and join dependencies (JDs) [39]. How does the presence of such database dependencies among features affect the behavior of ML models? There are also conditional FDs, which satisfy FD-like constraints among subsets of the dataset [39].",
        "context": null
    },
    {
        "title": "A Novel Feature Selection Approach: Combining Feature Wrappers and Filters",
        "author": [
            "O. Uncu",
            "I. Turksen"
        ],
        "venue": " Information Sciences, 177(2)",
        "citeRegEx": "40",
        "shortCiteRegEx": null,
        "year": 2007,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " [40] infers approximate FDs using the dataset instance and exploits them during feature selection, FOCUS [3] is an approach to bias the input and reduce the number of features by performing some computations over those features, while [6] proposes a measure called consistency to aid in feature subset search.",
        "context": null
    },
    {
        "title": "Feature hashing for large scale multitask learning",
        "author": [
            "K. Weinberger",
            "A. Dasgupta",
            "J. Langford",
            "A. Smola",
            "J. Attenberg"
        ],
        "venue": "Proceedings of the 26th Annual International Conference on Machine Learning, ICML \u201909, pages 1113\u20131120, New York, NY, USA",
        "citeRegEx": "41",
        "shortCiteRegEx": null,
        "year": 2009,
        "abstract": "Empirical evidence suggests that hashing is an effective strategy for\ndimensionality reduction and practical nonparametric estimation. In this paper\nwe provide exponential tail bounds for feature hashing and show that the\ninteraction between random subspaces is negligible with high probability. We\ndemonstrate the feasibility of this approach with experimental results for a\nnew use case -- multitask learning with hundreds of thousands of tasks.",
        "full_text": "arXiv:0902.2206v5  [cs.AI]  27 Feb 2010\nFeature Hashing for Large Scale Multitask Learning\nKilian Weinberger\nKILIAN@YAHOO-INC.COM\nAnirban Dasgupta\nANIRBAN@YAHOO-INC.COM\nJosh Attenberg\nJOSH@CIS.POLY.EDU\nJohn Langford\nJL@HUNCH.NET\nAlex Smola\nALEX@SMOLA.ORG\nYahoo! Research, 2821 Mission College Blvd., Santa Clara, CA 95051 USA\nKeywords: kernels, concentration inequalities, document classi\ufb01cation, classi\ufb01er personalization, multitask learning\nAbstract\nEmpirical evidence suggests that hashing is an\neffective strategy for dimensionality reduction\nand practical nonparametric estimation. In this\npaper we provide exponential tail bounds for fea-\nture hashing and show that the interaction be-\ntween random subspaces is negligible with high\nprobability.\nWe demonstrate the feasibility of\nthis approach with experimental results for a new\nuse case \u2014 multitask learning with hundreds of\nthousands of tasks.\n1. Introduction\nKernel methods use inner products as the basic tool for\ncomparisons between objects.\nThat is, given objects\nx1, . . . , xn \u2208X for some domain X, they rely on\nk(xi, xj) := \u27e8\u03c6(xi), \u03c6(xj)\u27e9\n(1)\nto compare the features \u03c6(xi) of xi and \u03c6(xj) of xj respec-\ntively.\nEq. (1) is often famously referred to as the kernel-trick. It\nallows the use of inner products between very high dimen-\nsional feature vectors \u03c6(xi) and \u03c6(xj) implicitly through\nthe de\ufb01nition of a positive semi-de\ufb01nite kernel matrix k\nwithout ever having to compute a vector \u03c6(xi) directly.\nThis can be particularly powerful in classi\ufb01cation settings\nwhere the original input representation has a non-linear de-\ncision boundary. Often, linear separability can be achieved\nin a high dimensional feature space \u03c6(xi).\nIn practice, for example in text classi\ufb01cation, researchers\nPreliminary work. Under review by the International Conference\non Machine Learning (ICML). Do not distribute.\nfrequently encounter the opposite problem: the original in-\nput space is almost linearly separable (often because of the\nexistence of handcrafted non-linear features), yet, the train-\ning set may be prohibitively large in size and very high di-\nmensional. In such a case, there is no need to map the input\nvectors into a higher dimensional feature space. Instead,\nlimited memory makes storing a kernel matrix infeasible.\nFor this common scenario several authors have recently\nproposed an alternative, but highly complimentary vari-\nation of the kernel-trick, which we refer to as the\nhashing-trick: one hashes the high dimensional input vec-\ntors x into a lower dimensional feature space Rm with\n\u03c6 : X \u2192Rm (Langford et al., 2007; Shi et al., 2009). The\nparameter vector of a classi\ufb01er can therefore live in Rm\ninstead of in Rn with kernel matrices or Rd in the origi-\nnal input space, where m \u226an and m \u226ad. Different\nfrom random projections, the hashing-trick preserves spar-\nsity and introduces no additional overhead to store projec-\ntion matrices.\nTo our knowledge, we are the \ufb01rst to provide exponential\ntail bounds on the canonical distortion of these hashed inner\nproducts. We also show that the hashing-trick can be partic-\nularly powerful in multi-task learning scenarios where the\noriginal feature spaces are the cross-product of the data, X,\nand the set of tasks, U. We show that one can use different\nhash functions for each task \u03c61, . . . , \u03c6|U| to map the data\ninto one joint space with little interference.\nWhile many potential applications exist for the hashing-\ntrick, as a particular case study we focus on collaborative\nemail spam \ufb01ltering. In this scenario, hundreds of thou-\nsands of users collectively label emails as spam or not-\nspam, and each user expects a personalized classi\ufb01er that\nre\ufb02ects their particular preferences. Here, the set of tasks,\nU, is the number of email users (this can be very large for\nopen systems such as Yahoo MailTMor GmailTM), and the\nfeature space spans the union of vocabularies in multitudes\nFeature Hashing for Large Scale Multitask Learning\nof languages.\nThis paper makes four main contributions:\n1. In sec-\ntion 2 we introduce specialized hash functions with unbi-\nased inner-products that are directly applicable to a large\nvariety of kernel-methods. 2. In section 3 we provide ex-\nponential tail bounds that help explain why hashed fea-\nture vectors have repeatedly lead to, at times surprisingly,\nstrong empirical results. 3. Also in section 3 we show that\nthe interference between independently hashed subspaces\nis negligible with high probability, which allows large-scale\nmulti-task learning in a very compressed space. 4. In sec-\ntion 5 we introduce collaborative email-spam \ufb01ltering as a\nnovel application for hash representations and provide ex-\nperimental results on large-scale real-world spam data sets.\n2. Hash Functions\nWe introduce a variant on the hash kernel proposed by (Shi\net al., 2009). This scheme is modi\ufb01ed through the introduc-\ntion of a signed sum of hashed features whereas the original\nhash kernels use an unsigned sum. This modi\ufb01cation leads\nto an unbiased estimate, which we demonstrate and further\nutilize in the following section.\nDe\ufb01nition 1 Denote by h a hash function h : N \u2192\n{1, . . . , m}. Moreover, denote by \u03be a hash function \u03be :\nN \u2192{\u00b11}. Then for vectors x, x\u2032 \u2208\u21132 we de\ufb01ne the\nhashed feature map \u03c6 and the corresponding inner product\nas\n\u03c6(h,\u03be)\ni\n(x) =\nX\nj:h(j)=i\n\u03be(i)xi\n(2)\nand \u27e8x, x\u2032\u27e9\u03c6 :=\nD\n\u03c6(h,\u03be)(x), \u03c6(h,\u03be)(x\u2032)\nE\n.\n(3)\nAlthough the hash functions in de\ufb01nition 1 are de\ufb01ned over\nthe natural numbers N, in practice we often consider hash\nfunctions over arbitrary strings. These are equivalent, since\neach \ufb01nite-length string can be represented by a unique nat-\nural number.\nUsually, we abbreviate the notation \u03c6(h,\u03be)(\u00b7) by just \u03c6(\u00b7).\nTwo hash functions \u03c6 and \u03c6\u2032 are different when \u03c6 = \u03c6(h,\u03be)\nand \u03c6\u2032 = \u03c6(h\u2032,\u03be\u2032) such that either h\u2032 \u0338= h or \u03be \u0338= \u03be\u2032. The\npurpose of the binary hash \u03be is to remove the bias inherent\nin the hash kernel of (Shi et al., 2009).\nIn a multi-task setting, we obtain instances in combination\nwith tasks, (x, u) \u2208X \u00d7 U. We can naturally extend our\nde\ufb01nition 1 to hash pairs, and will write \u03c6u(x) = \u03c6(x, u).\n3. Analysis\nThe following section is dedicated to theoretical analysis\nof hash kernels and their applications. In this sense, the\npresent paper continues where (Shi et al., 2009) falls short:\nwe prove exponential tail bounds. These bounds hold for\ngeneral hash kernels, which we later apply to show how\nhashing enables us to do large-scale multitask learning ef-\n\ufb01ciently. We start with a simple lemma about the bias and\nvariance of the hash kernel. The proof of this lemma ap-\npears in appendix A.\nLemma 2 The\nhash\nkernel\nis\nunbiased,\nthat\nis\nE\u03c6[\u27e8x, x\u2032\u27e9\u03c6]\n=\n\u27e8x, x\u2032\u27e9.\nMoreover, the variance is\n\u03c32\nx,x\u2032\n=\n1\nm\n\u0010P\ni\u0338=j x2\ni x\u2032\nj\n2 + xix\u2032\nixjx\u2032\nj\n\u0011\n, and thus, for\n\u2225x\u22252 = \u2225x\u2032\u22252 = 1, \u03c32\nx,x\u2032 = O\n\u0000 1\nm\n\u0001\n.\nThis suggests that typical values of the hash kernel should\nbe concentrated within O(\n1\n\u221am) of the target value. We use\nChebyshev\u2019s inequality to show that half of all observations\nare within a range of\n\u221a\n2\u03c3. This, together with an indirect\napplication of Talagrand\u2019s convex distance inequality via\nthe result of (Liberty et al., 2008), enables us to construct\nexponential tail bounds.\n3.1. Concentration of Measure Bounds\nIn this subsection we show that under a hashed feature-map\nthe length of each vector is preserved with high probability.\nTalagrand\u2019s inequality (Ledoux, 2001) is a key tool for the\nproof of the following theorem (detailed in the appendix B).\nTheorem 3 Let \u01eb < 1 be a \ufb01xed constant and x be a given\ninstance such that \u2225x\u22252 = 1. If m \u226572 log(1/\u03b4)/\u01eb2 and\n\u2225x\u2225\u221e\u2264\n\u01eb\n18\u221a\nlog(1/\u03b4) log(m/\u03b4), we have that\nPr[|\u2225x\u22252\n\u03c6 \u22121| \u2265\u01eb] \u22642\u03b4.\n(4)\nNote that an analogous result would also hold for the orig-\ninal hash kernel of (Shi et al., 2009), the only modi\ufb01ca-\ntion being the associated bias terms. The above result can\nalso be utilized to show a concentration bound on the inner\nproduct between two general vectors x and x\u2032.\nCorollary 4 For two vectors x and x\u2032, let us de\ufb01ne\n\u03c3 := max(\u03c3x,x, \u03c3x\u2032,x\u2032, \u03c3x\u2212x\u2032,x\u2212x\u2032)\n\u03b7 := max\n\u0012\u2225x\u2225\u221e\n\u2225x\u22252\n, \u2225x\u2032\u2225\u221e\n\u2225x\u2032\u22252\n, \u2225x \u2212x\u2032\u2225\u221e\n\u2225x \u2212x\u2032\u22252\n\u0013\n.\nAlso let \u2206= \u2225x\u22252 + \u2225x\u2032\u22252 + \u2225x \u2212x\u2032\u22252.\nIf m \u2265\n\u2126( 1\n\u01eb2 log(1/\u03b4)) and \u03b7 = O(\n\u01eb\nlog(m/\u03b4)), then we have that\nPr\nh\n| \u27e8x, x\u2032\u27e9\u03c6\u2212\u27e8x, x\u2032\u27e9|>\u01eb\u2206/2\ni\n<\u03b4.\nThe proof for this corollary can be found in appendix C. We\ncan also extend the bound in Theorem 3 for the maximal\nFeature Hashing for Large Scale Multitask Learning\ncanonical distortion over large sets of distances between\nvectors as follows:\nCorollary 5 If\nm\n\u2265\n\u2126( 1\n\u01eb2 log(n/\u03b4))\nand\n\u03b7\n=\nO(\n\u01eb\nlog(m/\u03b4)). Denote by X = {x1, . . . , xn} a set of vectors\nwhich satisfy \u2225xi \u2212xj\u2225\u221e\u2264\u03b7 \u2225xi \u2212xj\u22252 for all pairs i, j.\nIn this case with probability 1 \u2212\u03b4 we have for all i, j\n| \u2225xi \u2212xj\u22252\n\u03c6 \u2212\u2225xi \u2212xj\u22252\n2 |\n\u2225xi \u2212xj\u22252\n2\n\u2264\u01eb.\nThis means that the number of observations n (or corre-\nspondingly the size of the un-hashed kernel matrix) only\nenters logarithmically in the analysis.\nProof We apply the bound of Theorem 3 to each distance\nindividually. Note that each vector xi \u2212xj satis\ufb01es the\nconditions of the theorem, and hence for each vector xi \u2212\nxj, we preserve the distance upto a factor of (1 \u00b1 \u01eb) with\nprobability 1 \u2212\n\u03b4\nn2 . Taking the union bound over all pairs\ngives us the result.\n3.2. Multiple Hashing\nNote that the tightness of the union bound in Corollary 5\ndepends crucially on the magnitude of \u03b7. In other words,\nfor large values of \u03b7, that is, whenever some terms in x\nare very large, even a single collision can already lead to\nsigni\ufb01cant distortions of the embedding. This issue can\nbe amended by trading off sparsity with variance. A vec-\ntor of unit length may be written as (1, 0, 0, 0, . . .), or\nas\n\u0010\n1\n\u221a\n2,\n1\n\u221a\n2, 0, . . .\n\u0011\n, or more generally as a vector with c\nnonzero terms of magnitude c\u22121\n2 . This is relevant, for in-\nstance whenever the magnitudes of x follow a known pat-\ntern, e.g. when representing documents as bags of words\nsince we may simply hash frequent words several times.\nThe following corollary gives an intuition as to how the\ncon\ufb01dence bounds scale in terms of the replications:\nLemma 6 If we let x\u2032 =\n1\n\u221ac(x, . . . , x) then:\n1. It is norm preserving: \u2225x\u22252 = \u2225x\u2032\u22252 .\n2. It reduces component magnitude by\n1\n\u221ac = \u2225x\u2032\u2225\u221e\n\u2225x\u2225\u221e.\n3. Variance increases to \u03c32\nx\u2032,x\u2032 = 1\nc\u03c32\nx,x+ c\u22121\nc 2 \u2225x\u22254\n2 .\nApplying Lemma 6 to Theorem 3, a large magnitude can\nbe decreased at the cost of an increased variance.\n3.3. Approximate Orthogonality\nFor multitask learning, we must learn a different parameter\nvector for each related task. When mapped into the same\nhash-feature space we want to ensure that there is little in-\nteraction between the different parameter vectors. Let U be\na set of different tasks, u \u2208U being a speci\ufb01c one. Let w be\na combination of the parameter vectors of tasks in U \\ {u}.\nWe show that for any observation x for task u, the inter-\naction of w with x in the hashed feature space is minimal.\nFor each x, let the image of x under the hash feature-map\nfor task u be denoted as \u03c6u(x) = \u03c6(\u03be,h)((x, u)).\nTheorem 7 Let w \u2208Rm be a parameter vector for tasks\nin U \\ {u}. In this case the value of the inner product\n\u27e8w, \u03c6u(x)\u27e9is bounded by\nPr {|\u27e8w, \u03c6u(x)\u27e9| > \u01eb} \u22642e\n\u2212\n\u01eb2/2\nm\u22121\u2225w\u22252\n2\u2225x\u22252\n2+\u01eb\u2225w\u2225\u221e\u2225x\u2225\u221e/3\nProof\nWe use Bernstein\u2019s inequality (Bernstein, 1946),\nwhich states that for independent random variables Xj,\nwith E [Xj] = 0, if C > 0 is such that |Xj| \u2264C, then\nPr\n\uf8ee\n\uf8f0\nn\nX\nj=1\nXj >t\n\uf8f9\n\uf8fb\u2264exp\n \n\u2212\nt2/2\nPn\nj=1 E\n\u0002\nX2\nj\n\u0003\n+ Ct/3\n!\n. (5)\nWe have to compute the concentration property of\n\u27e8w, \u03c6u(x)\u27e9= P\nj xj\u03be(j)wh(j). Let Xj = xj\u03be(j)wh(j).\nBy the de\ufb01nition of h and \u03be, Xj are independent. Also,\nfor each j, since w depends only on the hash-functions for\nU \\ {u}, wh(j) is independent of \u03be(j). Thus, E[Xj] =\nE(\u03be,h)\n\u0002\nxj\u03be(j)wh(j)\n\u0003\n= 0. For each j, we also have |Xj| <\n\u2225x\u2225\u221e\u2225w\u2225\u221e=: C. Finally, P\nj E[X2\nj ] is given by\nE\n\uf8ee\n\uf8f0X\nj\n(xj\u03be(j)wh(j))2\n\uf8f9\n\uf8fb= 1\nm\nX\nj,\u2113\nx2\njw2\n\u2113=\n1\nm \u2225x\u22252\n2 \u2225w\u22252\n2\nThe claim follows by plugging both terms and C into the\nBernstein inequality (5).\nTheorem 7 bounds the in\ufb02uence of unrelated tasks with any\nparticular instance. In section 5 we demonstrate the real-\nworld applicability with empirical results on a large-scale\nmulti-task learning problem.\n4. Applications\nThe advantage of feature hashing is that it allows for sig-\nni\ufb01cant storage compression for parameter vectors: storing\nw in the raw feature space naively requires O(d) numbers,\nwhen w \u2208Rd. By hashing, we are able to reduce this to\nO(m) numbers while avoiding costly matrix-vector multi-\nplications common in Locally Sensitive Hashing. In addi-\ntion, the sparsity of the resulting vector is preserved.\nFeature Hashing for Large Scale Multitask Learning\nThe bene\ufb01ts of the hashing-trick leads to applications in\nalmost all areas of machine learning and beyond. In par-\nticular, feature hashing is extremely useful whenever large\nnumbers of parameters with redundancies need to be stored\nwithin bounded memory capacity.\nPersonalization\nOne powerful application of feature\nhashing is found in multitask learning. Theorem 7 allows\nus to hash multiple classi\ufb01ers for different tasks into one\nfeature space with little interaction. To illustrate, we ex-\nplore this setting in the context of spam-classi\ufb01er personal-\nization.\nSuppose we have thousands of users U and want to per-\nform related but not identical classi\ufb01cation tasks for each\nof the them. Users provide labeled data by marking emails\nas spam or not-spam. Ideally, for each user u \u2208U, we\nwant to learn a predictor wu based on the data of that user\nsolely. However, webmail users are notoriously lazy in la-\nbeling emails and even those that do not contribute to the\ntraining data expect a working spam \ufb01lter. Therefore, we\nalso need to learn an additional global predictor w0 to allow\ndata sharing amongst all users.\nStoring all predictors wi requires O(d \u00d7 (|U| + 1)) mem-\nory. In a task like collaborative spam-\ufb01ltering, |U|, the\nnumber of users can be in the hundreds of thousands and\nthe size of the vocabulary is usually in the order of mil-\nlions.\nThe naive way of dealing with this is to elimi-\nnate all infrequent tokens. However, spammers target this\nmemory-vulnerability by maliciously misspelling words\nand thereby creating highly infrequent but spam-typical\ntokens that \u201cfall under the radar\u201d of conventional classi-\n\ufb01ers. Instead, if all words are hashed into a \ufb01nite-sized\nfeature vector, infrequent but class-indicative tokens get a\nchance to contribute to the classi\ufb01cation outcome. Further,\nlarge scale spam-\ufb01lters (e.g. Yahoo MailTMor GMailTM)\ntypically have severe memory and time constraints, since\nthey have to handle billions of emails per day. To guaran-\ntee a \ufb01nite-size memory footprint we hash all weight vec-\ntors w0, . . . , w|U| into a joint, signi\ufb01cantly smaller, feature\nspace Rm with different hash functions \u03c60, . . . , \u03c6|U|. The\nresulting hashed-weight vector wh \u2208Rm can then be writ-\nten as:\nwh = \u03c60(w0) +\nX\nu\u2208U\n\u03c6u(wu).\n(6)\nNote that in practice the weight vector wh can be learned\ndirectly in the hashed space. All un-hashed weight vectors\nnever need to be computed. Given a new document/email\nx of user u \u2208U, the prediction task now consists of calcu-\nlating \u27e8\u03c60(x) + \u03c6u(x), wh\u27e9. Due to hashing we have two\nsources of error \u2013 distortion \u01ebd of the hashed inner prod-\nucts and the interference with other hashed weight vectors\n\u01ebi. More precisely:\n\u27e8\u03c60(x) + \u03c6u(x), wh\u27e9= \u27e8x, w0 + wu\u27e9+ \u01ebd + \u01ebi.\n(7)\nThe interference error consists of all collisions between\n\u03c60(x) or \u03c6u(x) with hash functions of other users,\n\u01ebi =\nX\nv\u2208U,v\u0338=0\n\u27e8\u03c60(x), \u03c6v(wv)\u27e9+\nX\nv\u2208U,v\u0338=u\n\u27e8\u03c6u(x), \u03c6v(wv)\u27e9.\n(8)\nTo show that \u01ebi is small with high probability we can\napply Theorem 7 twice, once for each term of (8).\nWe consider each user\u2019s classi\ufb01cation to be a separate\ntask, and since P\nv\u2208U,v\u0338=0 wv is independent of the hash-\nfunction \u03c60, the conditions of Theorem 7 apply with w =\nP\nv\u0338=0 wv and we can employ it to bound the second term,\nP\nv\u2208U,v\u0338=0 \u27e8\u03c6u(x), \u03c6u(wv)\u27e9.\nThe second application is\nidentical except that all subscripts \u201c0\u201d are substituted with\n\u201cu\u201d. For lack of space we do not derive the exact bounds.\nThe distortion error occurs because each hash function that\nis utilized by user u can self-collide:\n\u01ebd =\nX\nv\u2208{u,0}\n| \u27e8\u03c6v(x), \u03c6v(wv)\u27e9\u2212\u27e8x, wv\u27e9|.\n(9)\nTo show that \u01ebd is small with high probability, we apply\nCorollary 4 once for each possible values of v.\nIn section 5 we show experimental results for this set-\nting. The empirical results are stronger than the theoretical\nbounds derived in this subsection\u2014our technique outper-\nforms a single global classi\ufb01er on hundreds thousands of\nusers. We discuss an intuitive explanation in section 5.\nMassively Multiclass Estimation\nWe can also regard\nmassively multi-class classi\ufb01cation as a multitask problem,\nand apply feature hashing in a way similar to the person-\nalization setting. Instead of using a different hash func-\ntion for each user, we use a different hash function for each\nclass.\n(Shi et al., 2009) apply feature hashing to problems with\na high number of categories. They show empirically that\njoint hashing of the feature vector \u03c6(x, y) can be ef\ufb01ciently\nachieved for problems with millions of features and thou-\nsands of classes.\nCollaborative Filtering\nAssume that we are given a very\nlarge sparse matrix M where the entry Mij indicates what\naction user i took on instance j. A common example for\nactions and instances is user-ratings of movies (Bennett &\nLanning, ). A successful method for \ufb01nding common fac-\ntors amongst users and instances for predicting unobserved\nactions is to factorize M into M = U \u22a4W. If we have\nmillions of users performing millions of actions, storing U\nFeature Hashing for Large Scale Multitask Learning\nFigure 1. The hashed personalization summarized in a schematic\nlayout. Each token is duplicated and one copy is individualized\n(e.g. by concatenating each word with a unique user identi\ufb01er).\nThen, the global hash function maps all tokens into a low dimen-\nsional feature space where the document is classi\ufb01ed.\nand W in memory quickly becomes infeasible. Instead, we\nmay choose to compress the matrices U and W using hash-\ning. For U, W \u2208Rn\u00d7d denote by u, w \u2208Rm vectors with\nui =\nX\nj,k:h(j,k)=i\n\u03be(j, k)Ujk and wi =\nX\nj,k:h\u2032(j,k)=i\n\u03be\u2032(j, k)Wjk.\nwhere (h, \u03be) and (h\u2032, \u03be\u2032) are independently chosen hash\nfunctions. This allows us to approximate matrix elements\nMij = [U \u22a4W]ij via\nM \u03c6\nij :=\nX\nk\n\u03be(k, i)\u03be\u2032(k, j)uh(k,i)wh\u2032(k,j).\nThis gives a compressed vector representation of M that\ncan be ef\ufb01ciently stored.\n5. Results\nWe evaluated our algorithm in the setting of personaliza-\ntion.\nAs data set, we used a proprietary email spam-\nclassi\ufb01cation task of n = 3.2 million emails, properly\nanonymized, collected from |U| = 433167 users. Each\nemail is labeled as spam or not-spam by one user in U. Af-\nter tokenization, the data set consists of 40 million unique\nwords.\nFor all experiments in this paper, we used the Vowpal Wab-\nbit implementation1 of stochastic gradient descent on a\nsquare-loss. In the mail-spam literature the misclassi\ufb01ca-\ntion of not-spam is considered to be much more harmful\nthan misclassi\ufb01cation of spam. We therefore follow the\nconvention to set the classi\ufb01cation threshold during test\ntime such that exactly 1% of the not \u2212spam test data is\nclassi\ufb01ed as spam Our implementation of the personalized\nhash functions is illustrated in Figure 1. To obtain a person-\nalized hash function \u03c6u for user u, we concatenate a unique\nuser-id to each word in the email and then hash the newly\ngenerated tokens with the same global hash function.\n1http://hunch.net/\u223cvw/\nFigure 2. The decrease of uncaught spam over the baseline clas-\nsi\ufb01er averaged over all users. The classi\ufb01cation threshold was\nchosen to keep the not-spam misclassi\ufb01cation \ufb01xed at 1%.\nThe hashed global classi\ufb01er (global-hashed) converges relatively\nsoon, showing that the distortion error \u01ebd vanishes. The personal-\nized classi\ufb01er results in an average improvement of up to 30%.\nThe data set was collected over a span of 14 days. We\nused the \ufb01rst 10 days for training and the remaining 4 days\nfor testing. As baseline, we chose the purely global classi-\n\ufb01er trained over all users and hashed into 226 dimensional\nspace. As 226 far exceeds the total number of unique words\nwe can regard the baseline to be representative for the clas-\nsi\ufb01cation without hashing. All results are reported as the\namount of spam that passed the \ufb01lter undetected, relative\nto this baseline (eg. a value of 0.80 indicates a 20% reduc-\ntion in spam for the user)2.\nFigure 2 displays the average amount of spam in users\u2019 in-\nboxes as a function of the number of hash keys m, relative\nto the baseline above. In addition to the baseline, we eval-\nuate two different settings.\nThe\nglobal-hashed\ncurve\nrepresents\nthe\nrelative\nspam catch-rate of the global classi\ufb01er after hashing\n\u27e8\u03c60(w0), \u03c60(x)\u27e9.\nAt m = 226 this is identical to the\nbaseline. Early convergence at m = 222 suggests that at\nthis point hash collisions have no impact on the classi\ufb01-\ncation error and the baseline is indeed equivalent to that\nobtainable without hashing.\nIn the personalized setting each user u \u2208U gets her own\nclassi\ufb01er \u03c6u(wu) as well as the global classi\ufb01er \u03c60(w0).\nWithout hashing the feature space explodes, as the cross\nproduct of u = 400K users and n = 40M tokens results\nin 16 trillion possible unique personalized features. Fig-\nure 2 shows that despite aggressive hashing, personaliza-\ntion results in a 30% spam reduction once the hash table is\nindexed by 22 bits.\n2As part of our data sharing agreement, we agreed not to in-\nclude absolute classi\ufb01cation error-rates.\nFeature Hashing for Large Scale Multitask Learning\nFigure 3. Results for users clustered by training emails. For ex-\nample, the bucket [8, 15] consists of all users with eight to \ufb01fteen\ntraining emails. Although users in buckets with large amounts of\ntraining data do bene\ufb01t more from the personalized classi\ufb01er (up-\nto 65% reduction in spam), even users that did not contribute to\nthe training corpus at all obtain almost 20% spam-reduction.\nUser clustering\nOne hypothesis for the strong results in\nFigure 2 might originate from the non-uniform distribution\nof user votes \u2014 it is possible that using personalization and\nfeature hashing we bene\ufb01t a small number of users who\nhave labeled many emails, degrading the performance of\nmost users (who have labeled few or no emails) in the pro-\ncess. In fact, in real life, a large fraction of email users do\nnot contribute at all to the training corpus and only interact\nwith the classi\ufb01er during test time. The personalized ver-\nsion of the test email \u03a6u(xu) is then hashed into buckets\nof other tokens and only adds interference noise \u01ebi to the\nclassi\ufb01cation.\nIn order to show that we improve the performance of most\nusers, it is therefore important that we not only report av-\neraged results over all emails, but explicitly examine the\neffects of the personalized classi\ufb01er for users depending\non their contribution to the training set. To this end, we\nplace users into exponentially growing buckets based on\ntheir number of training emails and compute the relative\nreduction of uncaught spam for each bucket individually.\nFigure 3 shows the results on a per-bucket basis. We do not\ncompare against a purely local approach, with no global\ncomponent, since for a large fraction of users\u2014those with-\nout training data\u2014this approach cannot outperform ran-\ndom guessing.\nIt might appear rather surprising that users in the bucket\nwith none or very little training emails (the line of bucket\n[0] is identical to bucket [1]) also bene\ufb01t from personal-\nization. After all, their personalized classi\ufb01er was never\ntrained and can only add noise at test-time. The classi\ufb01er\nimprovement of this bucket can be explained by the sub-\njective de\ufb01nition of spam and not-spam. In the personal-\nized setting the individual component of user labeling is\nabsorbed by the local classi\ufb01ers and the global classi\ufb01er\nrepresents the common de\ufb01nition of spam and not-spam.\nIn other words, the global part of the personalized classi-\n\ufb01er obtains better generalization properties, bene\ufb01ting all\nusers.\n6. Related Work\nA number of researchers have tackled related, albeit differ-\nent problems.\n(Rahimi & Recht, 2008) use Bochner\u2019s theorem and sam-\npling to obtain approximate inner products for Radial Ba-\nsis Function kernels. (Rahimi & Recht, 2009) extend this\nto sparse approximation of weighted combinations of ba-\nsis functions. This is computationally ef\ufb01cient for many\nfunction spaces. Note that the representation is dense.\n(Li et al., 2007) take a complementary approach: for sparse\nfeature vectors, \u03c6(x), they devise a scheme of reducing the\nnumber of nonzero terms even further. While this is in prin-\nciple desirable, it does not resolve the problem of \u03c6(x) be-\ning high dimensional. More succinctly, it is necessary to\nexpress the function in the dual representation rather than\nexpressing f as a linear function, where w is unlikely to be\ncompactly represented: f(x) = \u27e8\u03c6(x), w\u27e9.\n(Achlioptas, 2003) provides computationally ef\ufb01cient ran-\ndomization schemes for dimensionality reduction. Instead\nof performing a dense d\u00b7m dimensional matrix vector mul-\ntiplication to reduce the dimensionality for a vector of di-\nmensionality d to one of dimensionality m, as is required\nby the algorithm of (Gionis et al., 1999), he only requires 1\n3\nof that computation by designing a matrix consisting only\nof entries {\u22121, 0, 1}. Pioneered by (Ailon & Chazelle,\n2006), there has been a line of work (Ailon & Liberty,\n2008; Matousek, 2008) on improving the complexity of\nrandom projection by using various code-matrices in or-\nder to preprocess the input vectors. Some of our theoretical\nbounds are derivable from that of (Liberty et al., 2008).\nA related construction is the CountMin sketch of (Cor-\nmode & Muthukrishnan, 2004) which stores counts in\na number of replicates of a hash table. This leads to good\nconcentration inequalities for range and point queries.\n(Shi et al., 2009) propose a hash kernel to deal with the is-\nsue of computational ef\ufb01ciency by a very simple algorithm:\nhigh-dimensional vectors are compressed by adding up all\ncoordinates which have the same hash value \u2014 one only\nneeds to perform as many calculations as there are nonzero\nterms in the vector. This is a signi\ufb01cant computational sav-\ning over locality sensitive hashing (Achlioptas, 2003; Gio-\nnis et al., 1999).\nSeveral additional works provide motivation for the investi-\ngation of hashing representations. For example, (Ganchev\n& Dredze, 2008) provide empirical evidence that the hash-\nFeature Hashing for Large Scale Multitask Learning\ning trick can be used to effectively reduce the memory\nfootprint on many sparse learning problems by an order of\nmagnitude via removal of the dictionary. Our experimen-\ntal results validate this, and show that much more radical\ncompression levels are achievable. In addition, (Langford\net al., 2007) released the Vowpal Wabbit fast online learn-\ning software which uses a hash representation similar to\nthat discussed here.\n7. Conclusion\nIn this paper we analyze the hashing-trick for dimensional-\nity reduction theoretically and empirically. As part of our\ntheoretical analysis we introduce unbiased hash functions\nand provide exponential tail bounds for hash kernels. These\ngive further inside into hash-spaces and explain previously\nmade empirical observations. We also derive that random\nsubspaces of the hashed space are likely to not interact,\nwhich makes multitask learning with many tasks possible.\nOur empirical results validate this on a real-world applica-\ntion within the context of spam \ufb01ltering. Here we demon-\nstrate that even with a very large number of tasks and\nfeatures, all mapped into a joint lower dimensional hash-\nspace, one can obtain impressive classi\ufb01cation results with\n\ufb01nite memory guarantee.\nReferences\nAchlioptas, D. (2003). Database-friendly random projec-\ntions: Johnson-lindenstrauss with binary coins. Journal\nof Computer and System Sciences, 66, 671\u2013687.\nAilon, N., & Chazelle, B. (2006). Approximate nearest\nneighbors and the fast Johnson-Lindenstrauss transform.\nProc. 38th Annual ACM Symposium on Theory of Com-\nputing (pp. 557\u2013563).\nAilon, N., & Liberty, E. (2008). Fast dimension reduction\nusing Rademacher series on dual BCH codes. Proc. 19th\nAnnual ACM-SIAM Symposium on Discrete algorithms\n(pp. 1\u20139).\nAlon, N. (2003). Problems and results in extremal combi-\nnatorics, Part I. Discrete Math, 273, 31\u201353.\nBennett, J., & Lanning, S. The Net\ufb02ix Prize. Proceedings\nof KDD Cup and Workshop 2007.\nBernstein, S. (1946). The theory of probabilities. Moscow:\nGastehizdat Publishing House.\nCormode, G., & Muthukrishnan, M. (2004). An improved\ndata stream summary: The count-min sketch and its ap-\nplications. LATIN: Latin American Symposium on The-\noretical Informatics.\nDasgupta, A., Sarlos, T., & Kumar, R. (2010). A Sparse\nJohnson Lindenstrauss Transform. Submitted.\nGanchev, K., & Dredze, M. (2008). Small statistical mod-\nels by random feature mixing. Workshop on Mobile Lan-\nguage Processing, Annual Meeting of the Association for\nComputational Linguistics.\nGionis, A., Indyk, P., & Motwani, R. (1999). Similarity\nsearch in high dimensions via hashing. Proceedings of\nthe 25th VLDB Conference (pp. 518\u2013529). Edinburgh,\nScotland: Morgan Kaufmann.\nLangford, J., Li, L., & Strehl, A. (2007).\nVow-\npal wabbit online learning project (Technical Report).\nhttp://hunch.net/?p=309.\nLedoux, M. (2001). The concentration of measure phe-\nnomenon. Providence, RI: AMS.\nLi, P., Church, K., & Hastie, T. (2007). Conditional random\nsampling: A sketch-based sampling technique for sparse\ndata. In B. Sch\u00a8olkopf, J. Platt and T. Hoffman (Eds.),\nAdvances in neural information processing systems 19,\n873\u2013880. Cambridge, MA: MIT Press.\nLiberty, E., Ailon, N., & Singer, A. (2008). Dense fast ran-\ndom projections and lean Walsh transforms. Proc. 12th\nInternational Workshop on Randomization and Approxi-\nmation Techniques in Computer Science (pp. 512\u2013522).\nMatousek, J. (2008).\nOn variants of the Johnson\u2013\nLindenstrauss lemma.\nRandom Structures and Algo-\nrithms, 33, 142\u2013156.\nRahimi, A., & Recht, B. (2008). Random features for large-\nscale kernel machines. In J. Platt, D. Koller, Y. Singer\nand S. Roweis (Eds.), Advances in neural information\nprocessing systems 20. Cambridge, MA: MIT Press.\nRahimi, A., & Recht, B. (2009).\nRandomized kitchen\nsinks.\nIn L. Bottou, Y. Bengio, D. Schuurmans and\nD. Koller (Eds.), Advances in neural information pro-\ncessing systems 21. Cambridge, MA: MIT Press.\nShi, Q., Petterson, J., Dror, G., Langford, J., Smola, A.,\nStrehl, A., & Vishwanathan, V. (2009). Hash kernels.\nAISTATS 12.\nWeinberger, K., Dasgupta, A., Attenberg, J., Langford, J.,\n& Smola, A. (2009). Feature hashing for large scale mul-\ntitask learning. 26th International Conference on Ma-\nchine Learning (p. 140).\nFeature Hashing for Large Scale Multitask Learning\nA. Mean and Variance\nProof [Lemma 2] To compute the expectation we expand\n\u27e8x, x\u2032\u27e9\u03c6 =\nX\ni,j\n\u03be(i)\u03be(j)xix\u2032\nj\u03b4h(i),h(j).\n(10)\nSince E\u03c6[\u27e8x, x\u2032\u27e9\u03c6] = Eh[E\u03be[\u27e8x, x\u2032\u27e9\u03c6]], taking expecta-\ntions over \u03be we see that only the terms i = j have nonzero\nvalue, which shows the \ufb01rst claim. For the variance we\ncompute E\u03c6[\u27e8x, x\u2032\u27e92\n\u03c6]. Expanding this, we get:\n\u27e8x, x\u2032\u27e92\n\u03c6 =\nX\ni,j,k,l\n\u03be(i)\u03be(j)\u03be(k)\u03be(l)xix\u2032\njxkx\u2032\nl\u03b4h(i),h(j)\u03b4h(k),h(l).\nThis expression can be simpli\ufb01ed by noting that:\nE\u03be [\u03be(i)\u03be(j)\u03be(k)\u03be(l)] = \u03b4ij\u03b4kl + [1 \u2212\u03b4ijkl](\u03b4ik\u03b4jl + \u03b4il\u03b4jk).\nPassing the expectation over \u03be through the sum, this allows\nus to break down the expansion of the variance into two\nterms.\nE\u03c6[\u27e8x, x\u2032\u27e92\n\u03c6] =\nX\ni,k\nxix\u2032\nixkx\u2032\nk +\nX\ni\u0338=j\nx2\ni x\u2032\nj\n2Eh\n\u0002\n\u03b4h(i),h(j)\n\u0003\n+\nX\ni\u0338=j\nxix\u2032\nixjx\u2032\njEh\n\u0002\n\u03b4h(i),h(j)\n\u0003\n= \u27e8x, x\u2032\u27e92 + 1\nm\n\uf8eb\n\uf8edX\ni\u0338=j\nx2\ni x\u2032\nj\n2 +\nX\ni\u0338=j\nxix\u2032\nixjx\u2032\nj\n\uf8f6\n\uf8f8\nby noting that Eh\n\u0002\n\u03b4h(i),h(j)\n\u0003\n= 1\nm for i \u0338= j. Using the fact\nthat \u03c32 = E\u03c6[\u27e8x, x\u2032\u27e92\n\u03c6]\u2212E\u03c6[\u27e8x, x\u2032\u27e9\u03c6]2 proves the claim.\nB. Concentration of Measure\nWe use the concentration result derived by Liberty, Ailon\nand Singer in (Liberty et al., 2008). Liberty et al. cre-\nate a Johnson-Lindenstrauss random projection matrix by\ncombining a carefully constructed deterministic matrix A\nwith random diagonal matrices.\nFor completeness we\nrestate the relevant lemma.\nLet i range over the hash-\nbuckets. Let m = c log(1/\u03b4)/\u01eb2 for a large enough con-\nstant c. For a given vector x, de\ufb01ne the diagonal matrix\nDx as (Dx)jj = xj. For any matrix A \u2208\u211cm\u00d7d, de\ufb01ne\n\u2225x\u2225A \u2261maxy:\u2225y\u22252=1 \u2225ADxy\u22252.\nLemma 2 (Liberty et al., 2008).\nFor any column-\nnormalized matrix A, vector x with \u2225x\u22252 = 1 and an\ni.i.d. random \u00b11 diagonal matrix Ds, the following holds:\n\u2200x, if \u2225x\u2225A \u2264\n\u01eb\n6\u221a\nlog(1/\u03b4) then, Pr[|\u2225ADsx\u22252\u22121| > \u01eb] \u2264\n\u03b4.\nWe also need the following form of a weighted balls and\nbins inequality \u2013 the statement of the Lemma, as well as\nthe proof follows that of Lemma 6 (Dasgupta et al., 2010).\nWe still outline the proof because of some parameter values\nbeing different.\nLemma 8 Let m be the size of the hash function range and\nlet \u03b7 =\n1\n2\u221a\nm log(m/\u03b4). If x is such that \u2225x\u22252 = 1 and\n\u2225x\u2225\u221e\u2264\u03b7, then de\ufb01ne \u03c32\n\u2217= maxi\nPd\nj=1 x2\nj\u03b4ih(j) where i\nranges over all hash-buckets. We have that with probability\n1 \u2212\u03b4,\n\u03c32\n\u2217\u22642\nm\nProof\nWe outline the proof-steps.\nSince the buck-\nets have identical distribution, we look only at the 1st\nbucket, i.e.\nat i = 1 and bound P\nj:h(j)=1 x2\nj.\nDe-\n\ufb01ne Xj = x2\nj\n\u0000\u03b41h(j) \u22121\nm\n\u0001\n.\nThen Eh[Xj] = 0 and\nEh[X2\nj ] = x4\nj\n\u0000 1\nm \u2212\n1\nm2\n\u0001\n\u2264\nx4\nj\nm \u2264\nx2\nj\u03b72\nm\nusing \u2225x\u2225\u221e\u2264\n\u03b7. Thus, P\nj Eh[X2\nj ] \u2264\n\u03b72\nm . Also note that P\nj Xj =\nP\nj:h(j)=1 x2\nj \u22121\nm. Plugging this into the Bernstein\u2019s in-\nequality, equation 5, we have that\nPr[\nX\nj\nXj > 1\nm] \u2264exp\n\u0012\n\u2212\n1/2m2\n\u03b72/m + \u03b72/3m\n\u0013\n= exp(\u2212\n3\n8m\u03b72 ) \u2264exp(\u2212log(m/\u03b4)) \u2264\u03b4/m\nBy taking union bound over all the m buckets, we get the\nabove result.\nProof [Theorem 3] Given the function \u03c6 = (h, r), de\ufb01ne\nthe matrix A as Aij = \u03b4ih(j) and Ds as (Ds)jj = rj. Let\nx be as speci\ufb01ed, i.e. \u2225x\u22252 = 1 and \u2225x\u2225\u221e\u2264\u03b7. Note that\n\u2225x\u2225\u03c6 = \u2225ADsx\u22252. Let y \u2208\u211cd be such that \u2225y\u22252 = 1.\nThus\n\u2225ADxy\u22252\n2 =\nm\nX\ni=1\n\uf8eb\n\uf8ed\nd\nX\nj=1\nyj\u03b4ih(j)xj\n\uf8f6\n\uf8f8\n2\n\u2264\nm\nX\ni=1\n(\nd\nX\nj=1\ny2\nj \u03b4ih(j))(\nd\nX\nj=1\nx2\nj\u03b4ih(j))\n\u2264\nm\nX\ni=1\n(\nd\nX\nj=1\ny2\nj \u03b4ih(j))\u03c32\n\u2217\u2264\u03c32\n\u2217.\nby applying the Cauchy-Schwartz inequality, and using the\nde\ufb01nition of \u03c3\u2217. Thus, \u2225x\u2225A = maxy:\u2225y\u22252=1 \u2225ADxy\u22252 \u2264\n\u03c3\u2217\u2264\n\u221a\n2m\u22121/2.\nIf m \u2265\n72\n\u01eb2 log(1/\u03b4), we have that\n\u2225x\u2225A \u2264\n\u01eb\n6\u221a\nlog(1/\u03b4), which satis\ufb01es the conditions of\nLemma 2 from (Liberty et al., 2008). Thus applying the\nabove result from Lemma 2 (Liberty et al., 2008) to x, and\nFeature Hashing for Large Scale Multitask Learning\nusing Lemma 8, we have that Pr[|\u2225ADsx\u22252 \u22121| \u2265\u01eb] \u2264\u03b4\nand hence\nPr[|\u2225x\u22252\n\u03c6 \u22121| \u2265\u01eb] \u2264\u03b4\nby taking union over the two error probabilities of Lemma\n2 and Lemma 8, we have the result.\nC. Inner Product\nProof [Corollary 4] We have that 2 \u27e8x, x\u2032\u27e9\u03c6 = \u2225x\u22252\n\u03c6 +\n\u2225x\u2032\u22252\n\u03c6 \u2212\u2225x \u2212x\u2032\u22252\n\u03c6. Taking expectations, we have the stan-\ndard inner product inequality. Thus,\n|2 \u27e8x, x\u2032\u27e9\u03c6 \u22122 \u27e8x, x\u2032\u27e9| \u2264| \u2225x\u22252\n\u03c6 \u2212\u2225x\u22252 |\n+ | \u2225x\u2032\u22252\n\u03c6 \u2212\u2225x\u2032\u22252 | + | \u2225x \u2212x\u2032\u22252\n\u03c6 \u2212\u2225x \u2212x\u2032\u22252 |\nUsing union bound, with probability 1 \u22123\u03b4, each of the\nterms above is bounded using Theorem 3. Thus, putting\nthe bounds together, we have that, with probability 1 \u22123\u03b4,\n|2 \u27e8\u03c6u(x), \u03c6u(x)\u27e9\u22122 \u27e8x, x\u27e9| \u2264\u01eb(\u2225x\u22252 + \u2225x\u2032\u22252 + \u2225x \u2212x\u2032\u22252)\nD. Refutation of the Previous Incorrect Proof\nThere were a few bugs in the previous version of the pa-\nper (Weinberger et al., 2009). We now detail each of them\nand illustrate why it was an error. The current result shows\nthat the using hashing we can create a projection matrix\nthat can preserve distances to a factor of (1 \u00b1 \u01eb) for vectors\nwith a bounded \u2225x\u2225\u221e/\u2225x\u22252 ratio. The constraint on input\nvectors can be circumvented by multiple hashing, as out-\nlined in Section 3.2, but that would require hashing O( 1\n\u01eb2 )\ntimes. Recent work (Dasgupta et al., 2010) suggests that\nbetter theoretical bounds can be shown for this construc-\ntion. We thank Tamas Sarlos and Ravi Kumar for the fol-\nlowing writeup on the errors and for suggestion the new\nproof in Appendix B.\n1. The statement of the main theorem in Weinberger et\nal. (Weinberger et al., 2009, Theorem 3) is false as\nit contradicts the lower bound of Alon (Alon, 2003).\nThe \ufb02aw lies in the probability of error in (Weinberger\net al., 2009, Theorem 3), which was claimed to be\nexp(\u2212\n\u221a\u01eb\n4\u03b7 ). This error can be made arbitrarily small\nwithout increasing the embedding dimensionality m\nbut by decreasing \u03b7 =\n||x||\u221e\n||x||2 , which in turn can be\nachieved by preprocessing the input vectors x. How-\never, this contradicts Alon\u2019s lower bound on the em-\nbedding dimensionality. The details of this contra-\ndiction are best presented through (Weinberger et al.,\n2009, Corollary 5) as follows.\nSet m = 128 and \u03b4 = 1/2 and consider the ver-\ntices of the n-simplex in \u211cn+1, i.e., x1 = (1, 0, ..., 0),\nx2 = (0, 1, 0, ..., 0), .... Let P \u2208\u211c(n+1)c\u00d7(n+1)\nbe the naive, replication based preconditioner, with\nreplication parameter c = 512 log2 n as de\ufb01ned in\nSection 2 of our submission or (Weinberger et al.,\n2009, Section 3.2).\nTherefore for all pairs i \u0338= j\nwe have that ||Pxi \u2212Pxj||\u221e= 1/\u221ac and that\n||Pxi \u2212Pxj||2 =\n\u221a\n2. Hence we can apply (Wein-\nberger et al., 2009, Corollary 5) to the set of vec-\ntors Pxi with \u03b7 = 1/\n\u221a\n2c = 1/(32 log n); then the\nclaimed approximation error is\nq\n2\nm +64\u03b72 log2 n\n2\u03b4 =\n1\n8+ 1\n16 \u22641\n4. If Corollary 5 were true, then it would fol-\nlow that with probability at least 1/2, the linear trans-\nformation A = \u03c6 \u00b7 P : \u211cn+1 \u2192\u211cm distorts the pair-\nwise distances of the above n + 1 vectors by at most\na 1 \u00b1 1/4 multiplicative factor. On the other hand,\nthe lower bound of Alon shows that any such transfor-\nmation A must map to \u2126(log n) dimensions; see the\nremarks following Theorem 9.3 in (Alon, 2003) and\nset \u01eb = 1/4 there. This clearly contradicts m = 128\nabove.\n2. The proof of the Theorem 3 contained a fatal, un-\n\ufb01xable error. Recall that \u03b4ij denotes the usual Kro-\nnecker symbol, and h and h\u2032 are hash functions. Wein-\nberger et al. make the following observation after\nequation (13) of their proof on page 8 in Appendix\nB.\n\u201cFirst note that P\ni\nP\nj \u03b4h(j)i + \u03b4h\u2032(j)i is at\nmost 2t, where t = |{j : h(j) \u0338= h\u2032(j)}|.\u201d\nThe quoted observation is false. Let d denote the di-\nmension of the input. Then, P\ni\nP\nj \u03b4h(j)i + \u03b4h\u2032(j)i =\nP\nj(P\ni \u03b4h(j)i + \u03b4h\u2032(j)i) = P\nj 2 = 2d, independent\nof the choice of the hash function. Note that t played\na crucial role in the proof of (Weinberger et al., 2009)\nrelating the Euclidean approximation error of the di-\nmensionality reduction to Talagrand\u2019s convex distance\nde\ufb01ned over the set of hash functions. Albeit the error\nis elementary, we do not see how to rectify its conse-\nquences in (Weinberger et al., 2009) even if the claim\nwere of the right form.\n3. The proof of Theorem 3 in (Weinberger et al., 2009)\nalso contains a minor and \ufb01xable error. To see this,\nconsider the sentence towards the end of the proof\nTheorem 3 in (Weinberger et al., 2009) where 0 <\n\u01eb < 1 and \u03b2 = \u03b2(x) \u22651.\n\u201cNoting\nthat\ns2\n=\n(\np\n\u03b22 + \u01eb \u2212\n\u03b2)/4||x||\u221e\u2265\u221a\u01eb/4||x||\u221e, ...\u201d\nFeature Hashing for Large Scale Multitask Learning\nHere the authors wrongly assume that\np\n\u03b22 + \u01eb\u2212\u03b2 \u2265\n\u221a\u01eb holds, whereas the truth is\np\n\u03b22 + \u01eb \u2212\u03b2 \u2264\u221a\u01eb\nalways.\nObserve that this glitch is easy to \ufb01x locally, however\nthis change is minor and the modi\ufb01ed claim would\nstill be false.\nSince for all 0 \u2264y \u22641 we have\nthat \u221a1 + y \u22651 + y/3, from \u03b2 \u22651 it follows\nthat\np\n\u03b22 + \u01eb \u2212\u03b2 \u2265\u01eb/3. Plugging the latter esti-\nmate into the \u201cproof\u201d of Theorem 3 would result in a\nmodi\ufb01ed claim where the original probability of error,\nexp(\u2212\n\u221a\u01eb\n4\u03b7 ), is replaced with exp(\u2212\n\u01eb\n12\u03b7).\nUpdating\nthe numeric constants in the \ufb01rst section of this note\nwould show that the new claim still contradicts Alon\u2019s\nlower bound. To justify observe that counter example\nis based on a constant \u01eb and the modi\ufb01ed claim would\nstill lack the necessary \u2126(log n) dependency in its tar-\nget dimensionality.\n",
        "sentence": " A standard unsupervised method to construct f is the Random hashing trick [41], i.",
        "context": "compactly represented: f(x) = \u27e8\u03c6(x), w\u27e9.\n(Achlioptas, 2003) provides computationally ef\ufb01cient ran-\ndomization schemes for dimensionality reduction. Instead\nof performing a dense d\u00b7m dimensional matrix vector mul-\nthat discussed here.\n7. Conclusion\nIn this paper we analyze the hashing-trick for dimensional-\nity reduction theoretically and empirically. As part of our\ntheoretical analysis we introduce unbiased hash functions\n\u03c60(x) or \u03c6u(x) with hash functions of other users,\n\u01ebi =\nX\nv\u2208U,v\u0338=0\n\u27e8\u03c60(x), \u03c6v(wv)\u27e9+\nX\nv\u2208U,v\u0338=u\n\u27e8\u03c6u(x), \u03c6v(wv)\u27e9.\n(8)\nTo show that \u01ebi is small with high probability we can\napply Theorem 7 twice, once for each term of (8)."
    },
    {
        "title": "A Method for Implementing a Probabilistic Model as a Relational Database",
        "author": [
            "S.K.M. Wong",
            "C.J. Butz",
            "Y. Xiang"
        ],
        "venue": "In Proceedings of the Eleventh conference on Uncertainty in artificial intelligence,",
        "citeRegEx": "42",
        "shortCiteRegEx": "42",
        "year": 1995,
        "abstract": "This paper discusses a method for implementing a probabilistic inference\nsystem based on an extended relational data model. This model provides a\nunified approach for a variety of applications such as dynamic programming,\nsolving sparse linear equations, and constraint propagation. In this framework,\nthe probability model is represented as a generalized relational database.\nSubsequent probabilistic requests can be processed as standard relational\nqueries. Conventional database management systems can be easily adopted for\nimplementing such an approximate reasoning system.",
        "full_text": "556 \nA Method for Implementing a Probabilistic Model as a Relational \nDatabase \nS.K.M. Wong, C.J. Butz, andY. Xiang \nDepartment of Computer Science \nUniversity of Regina \nRegina, Saskatchewan \nCanada, S4S OA2 \n< {wong,butz,yxiang}@cs.uregina.ca> \nAbstract \nThis paper discusses a method for im\u00ad\nplementing a probabilistic inference system \nbased on an extended relational data model. \nThis model provides a unified approach for a \nvariety of applications such as dynamic pro\u00ad\ngramming, solving sparse linear equations, \nand constraint propagation. In this frame\u00ad\nwork, the probability model is represented \nas a generalized relational database. Subse\u00ad\nquent probabilistic requests can be processed \nas standard relational queries. Conventional \ndatabase management systems can be easily \nadopted for implementing such an approxi\u00ad\nmate reasoning system. \n1 \nIntroduction \nProbabilistic models [4, 9, 10] are used for making de\u00ad\ncisions under uncertainty. The input to a probabilistic \nmodel is usually a Bayesian network [10]. It may also \nconsist of a set of potentials which define a Markov \nnetwork [4]. In this paper, we assume that the proba\u00ad\nbilistic model is described by a Markov network. For \nthis model, the propagation method [5, 6, 7, 12, 13] \ncan be conveniently applied to convert the potentials \ninto marginal distributions. \nThere is another important reason to characterize a \nprobabilistic model by a Markov network, as it has \nbeen shown that such a network can be represented as \na generalized relational database (14, 15, 16]. That \nis, the probabilistic model can be transformed into \nan equivalent (extended) relational data model. More \nspecifically, the marginal corresponding to each po\u00ad\ntential can be viewed as a relation in the relational \ndatabase. Furthermore, the database scheme derived \nfrom a Markov network forms an acyclic join depen\u00ad\ndency [15], which possesses many desirable properties \n[1, 8] in database applications. \nAs the probabilistic model is now represented by a re\u00ad\nlational data model, a probability request expressed \nas a conditional probability can be equivalently trans\u00ad\nformed into a standard query to be executed by the \ndatabase management system. Naturally, all query \noptimization techniques can be directly applied to pro\u00ad\ncessing this query including data structure modifica\u00ad\ntion. Thus, these transformations allow us to take \nfull advantage of the query optimizer and other per\u00ad\nformance enhancement capabilities available in tradi\u00ad\ntional relational databases. \nThis paper, a sequel of the presentation in the IPMU \nconference [15], reports on the technical details in\u00ad\nvolved in the design of a probabilistic inference system \nby transforming a Markov network into a relational \ndatabase. \nOur paper is organized as follows. In Section 2, for \ncompleteness we review a unified relational data model \nfor both probabilistic reasoning and database manage\u00ad\nment systems. In Section 3, we show that a factored \nprobability distribution can be expressed as a general\u00ad\nized acyclic join dependency. The method for imple\u00ad\nmenting a probabilistic inference system is described in \nSection 4. First, we describe how a relational database \nis constructed for a given probabilistic model. We then \nshow that processing a request for evidential reason\u00ad\ning is equivalent to processing a standard relational \nquery. We conclude by pointing out that the extended \nrelational database system can in fact model a number \nof apparently different but closely related applications \n[12]. \n2 \nAn Extended Relational Data \nModel for Probabilistic Inference \nBefore introducing our data model, we need to define \nsome basic notions pertinent to our discussion such \nas: hypergraphs, factored distributions, and marginal\u00ad\nization. Then we show how under certain conditions a \nfactored joint probability distribution can be expressed \nA Method for Implementing a Probabilistic Model as a Relational Database \n557 \nas a generalized acyclic join dependency m the ex\u00ad\ntended relational model. \n2.1 \nBasic Notions \nHypergraphs and Hypertrees : \nLet C denote a lattice. We say that 1{ is a hyper\u00ad\ngraph, if 1{ is a finite subset of C. Consider, for ex\u00ad\nample, the power set 2x, where X= {xl, X2, ... , xn} \nis a set of variables. The power set 2x is a lattice of \nall subsets of X. Any subset of 2x is a hypergraph \non 2x. We say that an element t in a hypergraph 1{ \nis a twig if there exists another element b in 1{, dis\u00ad\ntinct from t, such that t n (U(1i- {t})) = t n b. We \ncall any such b a branch for the twig t. A hypergraph \n1{ is a hypertree (an acyclic hypergraph [1]) if its ele-\nments can be ordered, say h1, h2, ... , hn, so that h; is \na twig in {h1, h2, ... , h;}, fori = 2, ... , n. We call any \nsuch ordering a hypertree construction ordering for 1{. \nGiven a hypertree construction ordering h1, h2, .. . , hn, \nwe can choose, fori from 2 to n, an integer b(i) such \nthat 1 :S b( i) :S i - 1 and hb(i) is a branch for h; in \n{h1, h2, ... , h;}. We call the function b(i) satisfying this \ncondition a branching function for 1{ and h1, h2, ... , hn. \nFor example, let X =  {x1, x2, ... , x6} and C = 2x. \nConsider a hypergraph, 1{ = {h1 = {x1, x2,x3},h2 = \n{x1, x2, x4}, h3 \n= {x2, x3, xs}, h4 \n= {xs, x6} }, de\u00ad\npicted in Figure 1. This hypergraph is in fact a hy\u00ad\npertree; the ordering, for example, h1, h2, h3, h4, is a \nhypertree construction ordering and b(2) = 1, b(3) = 1, \nand b( 4) = 3 define its branching function. \nFigure 1: A graphical representation of the hypergraph \n1{ = {hl, h2, h3, h4}\u00b7 \nA hypertree K on C is called a hypertree cover for \na given hypergraph 1{ on C if for every element h \nof 1{, there exists an element k (h) of K such that \nh Y k(h). \nIn general, a hypergraph 1{ may have \nmany hypertree covers. For example, the hypertree \ndepicted in Figure 1 is a hypertree cover of the hy\u00ad\npergraph, { {xb x2}, {x1, x3}, {x1, x2, x4}, {x2, xs}, \n{x3, xs}, {xs, x6}}. \nFactored Probability Distributions : \nLet X= {x1, x2, ... , xn} denote a set of variables. A \nfactored probability distribution p(x1, x2, ... , Xn) can \nbe written as: \nwhere each h; is a subset of X, i.e., h; E 2x, and \u00a2h, is \na real-valued function on h;. Moreover, X= h1 U h2 U \n. .. U hn = U7=l h;. By definition, 1i = {h1, h2, ... , hn} \nis a hypergraph on the lattice 2x. Thus, a factored \nprobability distribution can be viewed as a product on \na hypergraph 1{, namely: \nLet Vx denote the discrete frame (state space) of the \nvariable x E X. We call an element of Vx a configura\u00ad\ntion of x. We define vh to be the Cartesian product of \nthe frames of the variables in a hyperedge h E 2x: \nWe call vh the frame of h, and we call its elements \nconfigurations of h. \nLet h, k E 2x, and h Y k. If c is a configuration of \nk, i.e., c E Vk, we write c.l.h for the configuration of \nh obtained by deleting the values of the variables in \nk and not in h. For example, let h = {x1, x2}, k = \n{x1, x2, x3, x4}, and c = (c1, c2, c3, c4), where c; E Vx;\u00b7 \nThen, c.l.h \n= ( c1, c2). \nIf h and k are disjoint subsets of X, ch is a configura\u00ad\ntion of h, and Ck is a configuration of k, then we write \n(Chock) for the configuration of h U k obtained by con\u00ad\ncatenating ch and Ck. In other words, ( ch o Ck) is the \nunique configuration of hUk such that ( ch ock).l.h = ch \nand ( Ch o ck ).l.k = Ck. Using the above notation, a fac\u00ad\ntored probability distribution \u00a2 on U1{ can be defined \nas follows: \n\u00a2(c) = (IT \u00a2h)( c) = IT \u00a2h( c.l.h), \nhE1i \nhE1i \nwhere c E vx is an arbitrary configuration and X = \nU?i. \nMarginalization : \n558 \nWong, Butz, and Xiang \n<l>h \nFigure 2: The function c/Jh expressed as a relation. \nConsider a function \u00a2k on a set k of variables. If h C k, \ns \n-\nthen <Pk denotes the function on h defined as follows: \n<Pth(ch) = L tPk(ch o Ck-h), \nCk-h \nwhere ch is a configuration of h, Ck-h is a configuration \nof k- h, and ch o Ck-h is a configuration of k. We \ncall <Pth the marginal of tPk on h. \nA major task in probabilistic reasoning with belief net\u00ad\nworks is to compute marginals as new evidence be\u00ad\ncomes available. \n3 \nRepresentation of a Factored \nProbability Distribution as a \nGeneralized Acyclic Join \nDependency \nLet c be a configuration of X= {x1, x2, ... , Xn}\u00b7 Con\u00ad\nsider a factored probability distribution \u00a2 on 1\u00a3: \n\u00a2(c)= II tPh(c.l.h). \nhE1i \nWe can conveniently express each function </Jh in \nthe above product as a relation cl> h. Suppose h = \n{x1,x2, .. ,xt} . \nThe function tPh can be expressed \nas a relation on the set { x1, x2, ... , Xt, f,!>h} of at\u00ad\ntributes as shown in Figure 2. A configuration c; = \n(ci!,Ci2, ... ,c;t) in the above table denotes a row ex\u00ad\ncluding the last element in the row, and s is the car-\ndinality of Vh. \n. \nBy definition, the product \u00a2h \u00b7 \u00a2k of any two function \nc/>h and </Jk is given by: \nwhere c E Vhuk. We can therefore express the product \nc/>h \u00b7cf>k equivalently as a product join of the relations cl>h \nand cl>k, written cl>h 0 cl>k, which is defined as follows: \n(i) Compute the natural join, cl>h txJ cl>k, of the two \nrelations of cl>h and cl>k. \n(ii) Add a new column with attribute !\u00a2h\u00b7\u00a2k to the \nrelation cl>h txl cl>k on h U k. Each value of l\u00a2h\u00b7\u00a2k \nis given by tPh ( c.l.h) \u00b7 tPk ( c.l.k), where c E Vhuk. \n(iii) Obtain the resultant relation cl>h 0 cl>k by project\u00ad\ning the relation obtained in Step (ii) on the set of \nattributes h U k U U\u00a2h\u00b7\u00a2k }. \nFor example, let h = {x1, x2}, k = {x2, x3}, and vh = \nVk = {0, 1}. The product join cl>h 0 cl>k is illustrated in \nFigure 3. \n(i) \n= \n(ii) \n--+ \n(iii) \n--+ \nXt \n0 \n0 \n1 \n1 \nXt \n0 \n0 \n0 \n0 \n1 \n1 \n1 \n1 \nXt \n0 \n0 \n0 \n0 \n1 \n1 \n1 \n1 \nXt \n0 \n0 \n0 \n0 \n1 \n1 \n1 \n1 \nx, \n'\"'h \n0 \nat \n1 \na, \n0 \na3 \n1 \na\u2022 \nx, \nX3 \n0 \n0 \n0 \n1 \n1 \n0 \n1 \n1 \n0 \n0 \n0 \n1 \n1 \n0 \n1 \n1 \nx, \nX3 \n0 \n0 \n0 \n1 \n1 \n0 \n1 \n1 \n0 \n0 \n0 \n1 \n1 \n0 \n1 \n1 \nx, \nX3 \n0 \nu \n0 \n1 \n1 \n0 \n1 \n1 \n0 \n0 \n0 \n1 \n1 \n0 \n1 \n1 \nx, \nXJ \nfq,_x \n0 \n0 \nbt \n0 \n1 \nb, \n1 \n0 \nb3 \n1 \n1 \nb. \nfq,h \nJq,k \nal \nbt \nat \nb, \na, \nb3 \na, \nb. \na3 \nbt \na3 \nb, \na\u2022 \nb3 \na\u2022 \nb. \n'\"' \n'\"' \n'\"' \u00b7<i>k \nal \nbt \na1 \u00b7 bt \nal \nb, \na1 \u00b7 b2 \na, \nb3 \na, \u00b7 b3 \na2 \nb. \na2 \u00b7 b4 \na3 \nbt \na3 \u00b7 bt \na3 \nb, \na3 \u00b7 b2 \na\u2022 \nb3 \na\u2022 \u00b7 b3 \na\u2022 \nb. \na4 \u00b7 b4 \n'\"' \u00b7<i>k \na1 \u00b7 bt \na1 \u00b7 b2 \na,\u00b7 b3 \na2 \u00b7 b4 \na, \u00b7 bt \na, . b, \na4 \u00b7 b3 \na4 \u00b7 b4 \nFigure 3: The join of two relations cl>h and cl>k. \nSince the operator 0 is both commutative and associa\u00ad\ntive, we can express a factored probability distribution \nas a join of relations: \n<P = II tPh = \u00ae cl>h = Q9{cl>hlh E 1\u00a3}. \nhE1i \nhE1i \nWe can also define marginalization as a relational op\u00ad\neration. \nLet c>th denote the relation obtained by \nmarginalizing the function \u00a2k on h \u008a k. We can con\u00ad\nstruct the relation c>th in two steps: \n(a) Project the relation cl>k on the set of attributes \nh U {/ \u00a2k}, without eliminating identical configu\u00ad\nrations. \n(b) For every configuration ch E vh, replace the set of \nconfigurations of hU{!\u00a2k} in the relation obtained \nfrom Step (a) by the singleton configuration Ch o \nCLck-h </Jk(ch o Ck-h)). \nA Method for Implementing a Probabilistic Model as a Relational Database \n559 \nx, \nX2 \nx, \nJq, \nu \n0 \n0 \na, \n0 \n0 \n1 \nd2 \n0 \n1 \n0 \nd, \n0 \n1 \n1 \nd, \n1 \n0 \n0 \nd. \n1 \n0 \n1 \nd. \n1 \n1 \n0 \nds \n1 \n1 \n1 \nds \nFigure 4: A relation <I>k with attributes x1, x2, x3, fq,k, \nand k = {x1, x2, x3}. \nx, \nX2 \nfq, \n0 \n0 \nd, \n0 \n0 \nd2 \n0 \n1 \nd, \n0 \n1 \nd, \n1 \n0 \nd. \n1 \n0 \nd. \n1 \n1 \ndo \n1 \n1 \nds \nFigure 5: The projection of the relation <I>k in Fig\u00ad\nure 4 onto {x1, x2}U{fq,k}. \nConsider, for example, the relation <I>k with k = \n{x1, x2, x3} as shown in Figure 4. Suppose we want \nto compute <I>th for h = {x1, x2}. From Step (a), we \nobtain the relation in Figure 5 by projecting <I>k on \nh U {fq,k }. The final result is shown in Figure 6. \nTwo important properties are satisfied by the operator \nt of marginalization. \nLemma 1 [12, 15] \n(i) If <I>k is a relation on k, and h \u008a g \u008a k, then \n( <I>tg).j.h = <I>th. \n(ii) If <I>h and <I>k are relations on h and k, respec\u00ad\ntively, then \nBefore discussing the computation of marginals of a \nfactored distribution, let us first state the notion of \ncomputational feasibility introduced by Shafer (12]. \nWe call a set of attributes feasible if it is feasible to \nx, \nX2 \nJ.,rh \nu \nu \na,+ a2 \n0 \n1 \nd, + d, \n1 \n0 \nd.+ d. \n1 \n1 \nds + ds \nFigure 6: The marginalization <I>th of the relation <I>k \nin Figure 4 onto h = {x1, x2}. \nrepresent relations on these attributes, join them, and \nmarginalize on them. We assume that any subset of \nfeasible attributes is also feasible. Furthermore, we as\u00ad\nsume that the factored distribution is represented on \na hypertree and every element in 1l is feasible. \nLemma 2 [12, 15] Let <I>= @{<I>hlh E 1l} be a fac\u00ad\ntored probability distribution on a hypertree 1l. Let t \nbe a twig in 1l and b be a branch for t. Then, \n(i) \n(@{<I>hlh E 1l}).l.uW' \n= (@{<I>hlh E 1\u00a3-t}) \u00ae<I>ftnb. \n(ii) \nIf k \u008a U1l-t, then (@{<I>hih E 1l}).l.k = \n( \u00ae{ <l>h\"t ih E 1\u00a3-t} ).l.k, where 1l-t denotes the set \nof hyperedges 1l - {t}, <I>i:t = <I>b \u00ae <t>ftnb, and \n<l>h\"t = <I>h for all other h in 1l-t. \nWe now describe a procedure for computing <J>.I.k for \nk E 1\u00a3, where <I> = @{ <I>h ih E 1l} and 1l is a hyper\u00ad\ntree. Choose a hypertree construction ordering for 1l \nthat begins with h1 = k as the root, say h1, h2, ... , hn, \nand choose a branching b( i) function for this particular \nordering. Fori= 1, 2, ... , n, let \nThis is a sequence of sub-hypertrees, each larger than \nthe last; 1\u00a31 = { h d and 1ln = 1l. The element h; \nis a twig in 1li. To compute <J>.I.k, we start with 1ln \ngoing backwards in this sequence. We use Lemma 2 \neach time to perform the reduction. At the step from \n1l; to 1li-1, we go from <J>.I.U1i' to <J>.I.urc-\u2022. We omit \nh; in 1li and change the relation on hb(i) in 1li-1 from \n<I>i \nto \nhb(i) \nand the other relations in 1li-: are not changed. The \ncollection of relations with which we begin, { <I>J: ih E \n1ln}, is simply { <I>h ih E 1l}, and the collection with \nwhich we end, {<I>\u008bIh E 1\u00a31}, consists of the single \nrelation <I>\u00ab = <J>.I.h,. \nConsider a factored probability distribution <I> = \n@{<I>hlh E 1l} on a hypertree 1l = {h1, h2, ... ,hn}. \nWe say that <I> satisfies the acyclic join dependency \n(AJD), *[h1, h2, ... , hn], if <I> decomposes losslessly onto \na hypertree construction ordering h1, h2, ... , hn, i.e., <I> \ncan be expressed as: \nwhere \u00ae' is a generalized join operator defined by: \n560 \nWong, Butz, and Xiang \nHence, \nThe relation (<I>-1-h)-1 is defined as follows. First, let \nus define the inverse function (\u00a2-1-h)-1 of\u00a2. That is, \n<I>t \u00ae ( <I>ftnb) -1 = <f>-1-t \u00ae ( <f>-1-tnb) -1. \n( \u00a2+') -1 (c) = ( \u0001 \u00a2 ( c o c')) \n-1 \n, whe<e \u0001 \u00a2 ( c o c') > 0, Thnelation <I> can thO<efme be exp\u2022e,ed \"\" \nc is a configuration of h \u008a U1l, and c' is a configuration \nof U1l - h. We call the function ( \u00a2-1-h) -1 the inverse \nmarginal of\u00a2 on h. The inverse relation ( <f>.l.h )-1 is the \nrelation constructed from the inverse function ( \u00a2-1-h) -1. \nObviously, the product (\u00a2-1-h)-1 \n\u00b7 \u00a2-1-h is a unit function \non h, and (<I>-1-h)-1 \u00ae <f>.l.h is an identity relation on h. \nTheorem 1 [15] Any factored probability distribu\u00ad\ntion <I> \n= @{<I>hlh \nE 1l} on a hypertree, 1l \n= \n{ h1, h2, ... , hn}, decomposes losslessly onto a hypertree \nconstruction ordering h1, h2, ... , hn. That is, <I> satis\u00ad\nfies the AJD, *[h1, h2, ... , hn]. \nProof\" \nSuppose t E 1l is a twig. By Lemma 2, \n((Q$){<I>hlh E 1l-t}) \u00ae<I>t)ww\u2022 \n(Q$){<I>hlh E 11-t}) \u00ae<I>ttn(u1r') \n(Q$){<I>hlh E 1{-t}) \u00ae<I>ftnb. \nNote that (<I>ftnb)-1 \u00ae<I>ttnb \u00ae<l>t \n= <I>t, as (<I>ttnb)-1 \u00ae \n<t>ftnb is an identity relation on t n b. Thus, \n<f>W1l-1 \u00ae (<I>ftnb)-1 \u00ae <I>t \n(Q$){<I>hlh E 11-t}) \u00ae <I>ftnb \u00ae (<I>ftnb)-1 \u00ae <I>t \n(Q$){<I>hih E 1l-t}) \u00ae <I>t \n<I>. \nNow we want to show that: \n(<I>ftnb)-1 \u00ae <I>t \n= (<I>-1-tnb)-1 \u00ae <f>-1-t. \nNote that by property (ii) of Lemma 1, we obtain: \n<I>t \u00ae (Q$){ <I>h lh E 1{-t} ).1-tn(u'W') \n(<I>t \u00ae (Q$){<I>hih E 1l-t}))-l-t \n<f>-1-t. \nOn the other hand, we have: \n(<I>ftnb)-1 \u00ae ((Q$){<I>hlh E 11-t})-1-tn(u?r'))-1 \n(<I>Jtnb \u00ae (Q$){<I>hlh E 11-t})-1-tn(u'W'))-1 \n((<I>t \u00ae (Q$){<I>hlh E 11-t).l.tn(u?r')).l.tnb)-1 \n(((<I>t \u00ae (Q$){<I>hlh E 1{-t}))-1-t).l.tnb)-1 \n( ( <f>-1-t )-1-tnb) -1 \n( <f>-1-tnb) -1. \n<I> \n<I>ww\u2022 \u00ae <f>-1-t \u00ae (<I>Hu'W')nt)-1 \n<f>-1-UH -t (:9' <f>-1-t. \nMoreover, \nQ$){<I>hlh E 1{-t} \u00ae <I>t.l.tnb \nQ$){<I>h\"tih E 1l-t}. \nWe can immediately apply the same procedure to \n<I>-1-uH-' for further reduction. Thus, by applying this \nalgorithm recursively, the desired result is obtained. \nD \n4 \nThe method for implementing a \nProbabilistic Inference System \nIn order to convert a probabilistic model into a re\u00ad\nlational model, first we need to be able to efficiently \ntransform the input potentials into marginals. Since \nwe assume that the hypergraph induced by the po\u00ad\ntentials is a hypertree, we can apply the propagation \nmethod [6, 12] to compute all their marginals. This \nprocess involves first moving backward along the hy\u00ad\npertree construction ordering to find the marginal of \nthe root, then moving forward from the root to the \nleaves for determining marginals of the other poten\u00ad\ntials. \nThe next task is to transform a probability request into \na standard relational query addressed to the database \nwhich is equivalent to the original probability model. \nThe relational query can be formulated by scanning \nthe probability request to determine the marginals in\u00ad\nvolved along the hypertree construction ordering, as \nwell as the specific variables (attributes) within each \nrespective marginal. Once the query is expressed in \nterms of the query language provided, it is then sub\u00ad\nmitted to and processed by the standard database \nmanagement system in the usual manner. \n4.1 \nTransformation of Potentials to \nMarginals (Relations) \nWe are given as input a set of potentials \u00a2h 's which \ndefine a factored joint probability distribution <I> = \n@{<I>hlh E 1l}, where 1l = {h1, h2, . . .  , hn} is the cor\u00ad\nresponding hypergraph. The first step in this transfor\u00ad\nmation is to check if the hypergraph 1l is a hypertree \n[1], but if so determine a branching function b( i) for \nit. If we do not have a hypertree, then some potentials \nA Method for Implementing a Probabilistic Model as a Relational Database \n561 \ncan be combined so that the resultant hypergraph is a \nhypertree [12]. \nIn the following discussion, we henceforth assume that \n1l = {h1, h2, . . .  , hn} is a hypertree. Let the branch\u00ad\ning function b(i), i = 2, . . .  , n define a hypertree con\u00ad\nstruction ordering. The procedure for computing the \nmarginal of the root h1 by moving backward along the \nhypertree construction ordering has been described in \nSection 3. \nOnce we have determined the root marginal, q,.J.h1, we \nmay move forward along the hypertree construction \nordering to compute marginals of the other potentials. \nFor this purpose, while we are moving backward we \nshould save the relation ( <l>h ).J.h;nhb(i) at each stage \n1li. Then we can determine' the other marginals by \nthe formula: \nTo see this, consider the situation where we have just \ncomputed the root marginal q,.J.h1, namely: q,.J.h1 = \n<I>h \n19. (<!>2 ).J.h1nh, Note that <!>2 - (<I>').j.h, where \n1 '61 \nh, \n. \nh, -\n, \n<I>'= @ {<I>h i h E 1l-h1 }. By Lemma 1, we obtain: \n( q, h1 0 ( <I>t ).j.h1 nh, ).j.h1 nh, \nq,.J.h1nh, 19. (1>2 ).j.h1nh2\u2022 \nh1 \n'61 \nh, \nFrom Lemmas 1 and 2, it follows: \n<I>t 0 ((<I>t).j.h1nh2)-1 0 (<I>.j.h1 ).j.h1nh2 \nq,2 \n19. q,.J.h,nh, \nh, '61 \nh, \n(<I>t 0 <I>h,).j.h, \nq,.J.h, . \nHence, by continuing moving forward, we will arrive \nat the above general formula. \nConsider, for example, a factored joint probability dis\u00ad\ntribution defined by six potentials [4] as shown in col\u00ad\numn 2 of Tables 1 to 6. We have modified the column \nnames to reflect the notation used in this paper. The \ncorresponding hypergraph, 1l = {h1 = {x1, x2}, h2 = \n{x2,x3,x4,x5},h3 \n{x2,x4,X5,x5},h4 \n{x2,x5,x7},h5 = {x2,x7,xs},h6 = {x7,xs,x9}}, is \ndepicted in Figure 7. This hypergraph is in fact a hy\u00ad\npertree. The sequence h1, h2, h3, h4, h5, h6, is a hyper\u00ad\ntree construction ordering which defines the branching \nfunction, b(2) = 1, b(3) = 2, b(4) = 3, b(5) = 4, b(6) = \n5. \nTo compute the root marginal q,.J.h1 , we may move \nbackward from the leaf hyperedge towards the root h1 \nalong the hypertree construction ordering. Thus we \nfirst transform the hypergraph 1{6 ( = 1l) to 1{5. That \nconfiguration \n4>(x1x2) \nc>+h1 \nc>+h1 \n-,,xl \"\"\"'1X2 \n0.502 \n0.391 \n0.391 \n..,.,1 \nx, \n0.261 \n0.058 \n0.058 \nX1\u2022X2 \n0.498 \n0.387 \n0.387 \nX1 x, \n0.739 \n0.164 \n0.164 \nconfiguration \n\u00a2>(x2x3x<xo) \n<I>v\" 0 ((<I>v,)+h2nh1 ) -1 \n<I>+h2 \n\u2022X2\u2022Xa\u2022X4...,X5 \n0.475 \n0.569 \n0.443 \n..,X2\u2022Xa\u2022X4 \"'\u2022 \n0.435 \n0.431 \n0.335 \n\u2022x2-,X3 X4-,X5 \n0.000 \n0.000 \n0.000 \n\u2022X:J\u2022X3 \"'\u2022 xs \n0.000 \n0.000 \n0.000 \n..,.,, X3-,,x4-,X5 \n0.000 \n0.000 \n0.000 \n..,.,, X3...,X4 xs \n0.000 \n0.000 \n0.000 \n..,.,, x, X4-,X5 \n0.000 \n0.000 \n0.000 \n..,.,, \nXJ \n\"'\u2022 \"'\u2022 \n0.000 \n0.000 \n0.000 \nX2\u2022Xa\u2022X4-,X5 \n0.000 \n0.000 \n0.000 \nX2\u2022Xa\u2022X4 \"'\u2022 \n0.000 \n0.000 \n0.000 \nX2-,X3 x4-,Xs \n0.475 \n0.144 \n0.032 \nX2\u2022X3 \"'\u2022 xs \n0.435 \n0.450 \n0.100 \nx, Xa\u2022X4-,X5 \n0.029 \n0.122 \n0.027 \n\"'2 Xa\u2022X4 xo \n0.061 \n0.212 \n0.047 \n\"'2 X3 X4-,X5 \n0.029 \n0.009 \n0.002 \nx, x, \"'\u2022 Xs \n0.739 \n0.063 \n0.014 \nis, we omit h6 in 1l6 and change the relation <l>h5 m \n1{5 to <I>t defined by: \nq,5 hs = \nand the other relations in 1{5 are not changed. Simil\u00ad\niarly we have: \n<I> h. \n<I>t \n0 ( <I>\u0080.).j.{x,,x7}' \n<I>t \n<I>h, 0 ( <I>t ).j.{x,,xs} \n> \n1>\u00812 \nq,h1 1 \nAs <I>h, = q,.J.h1, we have thus determined the root \nmarginal by moving backward. Now we start moving \nforward from the root. By applying the formula for \ncomputing other marginals, we immediately obtain: \nq,.J.h, \n<J>t 0 ((<J>\u00822).j.{x2})-1(q,.j.h1 ).j.{x2}, \nq,.J.h, \n<J>t 0 ((<J>t).j.{x2,x4,xs})-1 (<I>.j.h2).j.{x2,x4,x5}, \nq,.J.h\u2022 \n<J>t 0 ((<I>t).j.{x,,xs})-l(q,.j.h3).j.{x2,x6}, \nq,.J.h\u2022 \n<J>t 0 ((<I>t).j.{x,,x7})-l(q,.j.h4).j.{x2,x7}, \nq,.J.hs \n<I>t 0 ((<I>t).j.{x7,xs})-1(<J>.j.hs ).j.{x7,x8}. \nThe numerical results are shown in the last column of \nTables 1 to 6. These relations q,.J.h; form an acyclic join \ndependency in our extended relational data model. \n562 \nWong, Butz, and Xiang \nconfiguration \n\u00a2(x2x4x5xs) \nq,3 \nh \n\u00ae ((.Pw,)J.h3nh2)-1 \n-..x2 -..x4 \"\"\"Xs -.xs \n0.561 \n0.602 \n-..x2...,X4 -,,xiS xs \n0.371 \n0.398 \n-.,x2-..x4 xs-.xs \n0.519 \n0.676 \n-..x2-.x4 xs xs \n0.250 \n0.324 \nxx, x4 -.xs -..xs \n0.016 \n0.235 \nxx, X4\"\"'X5 xs \n0.052 \n0.765 \nxx, X4 xs-.xs \n0.058 \n0.251 \nxx, X4 xs xs \n0.173 \n0.749 \nX2 -.,x4 ...,X5 ...,X6 \n0.561 \n0.602 \nx2...,x4 ...,Xs xs \n0.371 \n0.398 \nx2-.x4 xs-.xs \n0.519 \n0.675 \nx2-..x4 xs xs \n0.250 \n0.325 \nx, x4 -.xs...,xs \n0.250 \n0.235 \nX2 X4'\"\"X5 xs \n0.052 \n0.765 \nx, \nx. xs...,xs \n0.058 \n0.251 \nx, x. xs xs \n0.173 \n0.749 \nTable 3: h3 = {z2,X4,X5,X6} \nconfiguration \n<f>(x,xsx7) \nq,\u2022 h \u00ae ((<I>\u00ac )J.h4nh3)-1 \n\u2022X2\u2022Xs\u2022X7 \n0.579 \n0.579 \n...,X2\u2022Xs X7 \n0.421 \n0.421 \nyx, Xs\u2022X7 \n0.563 \n0.563 \nzx2 \nxs X7 \n0.437 \n0.437 \nX2\u2022Xs\u2022X7 \n0.579 \n0.579 \nX2\u2022Xs X7 \n0.421 \n0.421 \nX2 Xs\u2022X7 \n0.563 \n0.563 \nx2 xs X7 \n0.437 \n0.437 \nconfiguration \n\u00a2(x,x1xs) \n<>L \u00ae((<>* \n)J.h5nh4 )-1 \n\u2022X2...,X7-,X8 \n0.697 \n0.697 \n...,x2\u2022X7 xs \n0.303 \n0.303 \n'\"'X2 X7\u2022Xs \n0.722 \n0.722 \n'\"'X2 X7 xs \n0.278 \n0.278 \nX2\u2022X7..,X8 \n0.433 \n0.433 \nX2..,X7 Xs \n0.567 \n0.567 \nX2 X7\u2022Xs \n0.261 \n0.261 \nX2 X7 xs \n0.739 \n0.739 \nconfiguration \n\u00a2(x1xsxg) \n<I>t \u00ae ((<I>t)J.h6nh5)-1 \n\u2022X7-,Xs\"\"'1Xg \n0.647 \n0.647 \n\u2022X7...,X8 Xg \n0.353 \n0.353 \n'\"'X7 Xg\u2022Xg \n0.579 \n0.579 \n'\"'X7 xs Xg \n0.421 \n0.421 \nX7-,XgoXg \n0.594 \n0.594 \nX7\u2022Xs Xg \n0.406 \n0.406 \nX7 Xg...,Xg \n0.438 \n0.438 \nX7 xs Xg \n0.562 \n0.562 \nq,J.h, \n0.267 \n0.176 \n0.226 \n0.109 \n0.000 \n0.000 \n0.000 \n0.000 \n0.016 \n0.011 \n0.015 \n0.015 \n0.008 \n0.026 \n0.029 \n0.085 \nq,J.h4 \n0.285 \n0.208 \n0.160 \n0.125 \n0.049 \n0.036 \n0.077 \n0.060 \nq,J.hs \n0.310 \n0.135 \n0.240 \n0.093 \n0.055 \n0.071 \n0.025 \n0.071 \nq,J.hs \n0.236 \n0.129 \n0.119 \n0.088 \n0.157 \n0.108 \n0.072 \n0.092 \nX a \nXd \nf.p\" \n<I>\"\n= <l>{xa, ... ,xd} = \nC 1a \nC1d \n<t>\"TciT \nC2a \nC2d \n4>\" (c,) \nCma \nCmd \n</>\"(em) \nFigure 7: The relation <I>\"'. \n4.2 \nTransformation of a Probability Request \nto a Query \nJust as we can transform a potential <I>h, to a marginal \nrelation <J>.I.h;, we can transform a probability request \nof the form p(xa, . . . , xdiXe = f, . . .  , Xg = 1) to a re\u00ad\nlational query. This query can then be processed by \nthe database management system. There are, how\u00ad\never, two ways to construct the query depending on \nwhether the product join ( 0) and generalized join ( 0') \noperators have been incorporated into the database \nmanagement system. We will show how to transform \nthe probability request to a relational query in either \nsituation. \n(i) In the first case we assume that the database \nmanagement system has been extended to include \nthe product join and generalized join operators. \nThen with respect to a particular hypertree con\u00ad\nstruction ordering, we first determine the join\u00ad\npath hr, . . .  , hs such that the union hr U . . .  U \nhs of these relation schemes (hyperedges) con\u0111 \ntains all the variables in the probability request \np(xa, ... , xdlxe = f, . . .  , Xg = 1). Then the re\u00ad\nlation <I>\"' = <I>{xa, ... ,xd}' depicted in Figure 7, is \nbeing constructed by the query: \nSELECT Xa, . . .  , Xd \nINTO <I>\"' \nFROM <J>.!.hr 0' ... 0' <J>.!.h, \nWHERE Xe = f, ... , Xg = f. \nAt this point, we have the information needed to \nanswer the probability request all in a single rela\u00ad\ntion <I>\"'. However, to compute the required condi\u00ad\ntional probability, we need to marginalize <I>\"' onto \n{xa, ... , xd} by the following query: \nSELECT Xa, . . .  , Xd, SU M(f<t>J \nINTO w\"' \nFROM <I>\"' \nGROUPBY Xa, . .. ,Xd \nSince the relation W\"' is not normalized, we have \nto define the normalization relation q,\"' which is a \nconstant relation as shown in Figure 8, where \nA Method for Implementing a Probabilistic Model as a Relational Database \n563 \nX a \nXd \n/J,, \n4-.. = \nCta \nCtd \n,f, .. (ct) = >. \nC2a \nC2d \n,J, .. (c2) = >. \nCma \nCmd \n,J, .. (cm) = >. \nFigure 8: The constant relation j,. \n.X= I: '\u00a2, (c) . \nc \nFinally, the answer to the probability request is \ngiven by the relation 1J!, \u00ae j;1. This demon\u00ad\nstrates that any probability request can be eas\u00ad\nily answered by submitting simple queries as de\u00ad\nscribed to the relational database management \nsystem. \nThe above discussion indicates that we need \nnot implement the marginalization operator .J_, \nas the standard relational query languages al\u00ad\nready provide the SUM and GROUP BY facil\u00ad\nities. These two functions are indeed equivalent \nto the marginalization operation. \n(ii) In the second case we simulate the product join \nand generalized join operators as we are interfac\u00ad\ning with a standard database management sys\u00ad\ntem. We will first discuss the simulation of the \nproduct join (\u00ae) and generalized join (\u00ae') oper\u00ad\nators , before we construct the relation to answer \nthe probability request. \nSuppose we want to compute the product join of \ntwo relations <l>h and <l>k, i.e. , <l>h \u00ae<l>k. According \nto the definition of \u00ae (see the example in Fig\u00ad\nure 3), we construct the relation <I> h tx1 <I> k by the \nquery: \nSELECT h U k, f1>h, f1>k \nINTO <l>huk \nFROM <l>h,<l>k. \nNext we create a new column labelled by the at\u00ad\ntribute f t/>h \u00b7t/>k, representing the product \u00a2Jh \u00b7 \u00a2Jk by \nthe query: \nALTER TABLE <l>huk ADD f4>h\u00b74>k FLOAT. \nBy definition, the entries in this column are: \nwhere c E Vhuk. The following query: \nUPDATE <l>huk \nSET f4>h\u00b7t/>k = f1>h * f1>k, \naccomplishes this task. The last step in simulat\u00ad\ning the product join \u00ae is to project <l>huk onto the \nset of attributes h U k U {ft/>h\u00b7t/>k} using the query: \nSELECT h U k,J\u00a2h\u00b74>k \nINTO <l>h\u00aek \nFROM <l>huk\u00b7 \nThus we have derived the relation <l>h0k = <l>h \u00ae \n<l>k . \nSince <l>h \u00ae' <l>k = <l>h \u00ae<l>k0<1>hnk -1, the simulation \nof the generalized join 0' is just a simple exten\u00ad\nsion of the product join 0. That is we need only \ncompute <l>hnk -1, the inverse relation of <l>hnk. We \nconstruct <l>hnk by the query: \nSELECT h n k, SU M(f\u00a2h) \nINTO <l>hnk \nFROM <l>h \nGROUPBY h n k. \nNote that we can use SU M(ft/>k) and <l>k in the \nabove query, since <I>ihnk = <I>thnk. It is straight\u00ad\nforward to construct the inverse relation <I> hnk -1 \nfrom <l>hnk. Now the relation <l>h\u00ae'k = <l>h 01 <l>k is \nobtained by performing the product join <l>h\u00aek \u00ae \n<l>hnk \n-1 . \nLet hr, hr+1, . .. , hs-1, hs denote the join-path. \nWe can compute the relation <I>e = ( ( . .. ( ( cf>.l-hr 0' \ncf>.l-hr+I) 0' ... ) 0' cf>.l-h,_,) 0' cf>.l-h,) by repeatedly \napplying the generalized join operation. It is un\u00ad\nderstood that the selection Xe = f, . . .  , Xg = 1 has \nbeen performed on each of the relations in the \njoin-path before <I>e is computed. \nThe relation <I>\"', depicted in Figure 7, is obtained \nby the query: \nSELECT X a, .. .  , Xd \nINTO <1>, \nFROM <I>e. \nWe construct 1J!, and j,, depicted in Figure 8, as \ndescribed in (i) of this subsection. The relation \n1J! 0 \u0089;1 is the answer to the given probability \nrequest. \n5 \nConclusion \nOnce it is acknowledged that a probabilistic model \ncan be viewed as an extended relational data model, \nit immediately follows that a probabilistic model can \nbe implemented as an everyday database application. \nThus, we are spared the arduous task of designing and \n564 \nWong, Butz, and Xiang \nimplementing our own probabilistic inference system \nand the associated costs. Even if such a system was \nsuccessfully implemented, the resulting performance \nmay not be comparable to that of existing relational \ndatabases. Our approach enables us to take advantage \nof the various performance enhancement techniques \nincluding query processing, query optimization, and \ndata structure storage and manipulation, available in \ntraditional relational database management systems. \nThus the time required for belief update and answer\u00ad\ning probability requests is shortened. \nThe proposed relational data model also provides a \nunified approach to design both database and proba\u00ad\nbilistic reasoning systems. \nIn this paper, we have defined the product join oper\u00ad\nator Q9 based on ordinary multiplication primarily be\u00ad\ncause we are dealing with probabilities. By defining Q9 \ndifferently (e.g. based on addition), our relational data \nmodel can be easily extended to solve a number of ap\u00ad\nparently different but closely related problems such as \ndynamic programming [2], solving sparse linear equa\u00ad\ntions [11], and constraint propagation [3]. \nReferences \n[1] C. Beeri, R. Fagin, D. Maier and M. Yannakakis, \n\"On the desirability of acyclic database schemes,\" \nJournal of the Association for Computing Ma\u00ad\nchinery, vol. 30, 479-513, 1983. \n[2] U. Bertele and F. Brioschi, Nonserial Dynamic \nProgramming. Academic Press, 1972. \n(3] R. Dechter, A. Dechter and J. Pearl, \"Optimiza\u00ad\ntion in constraint networks,\" in Influence Dia\u00ad\ngrams, Belief Nets, and Decision Analysis, edited \nby R.M. Oliver and J.Q. Smith, Wiley, 1990. \n(4] P. Hajek, T. Havranek and R. Jirousek, Uncertain \nInformation Processing in Expert Systems. CRC \nPress, 1992. \n[5) F.V. Jensen, \n\"Junction tree and decomposable \nhypergraphs,\" \nTechnical report, JUDEX, Aal\u00ad\nborg, Denmark, 1988. \n(6] F.V. Jensen, S.L. Lauritzen, and K.G. Olesen, \n\"Bayesian updating in causal probabilistic net\u00ad\nworks by local computations,\" \nComputational \nStatistics Quarterly, vol. 4, 269-282, 1990. \n[7) S.L. Lauritzen and D.J. Spiegelhalterr, \n\"Lo-\ncal computation with probabilities on graphical \nstructures and their application to expert sys\u00ad\ntems,\" Journal of the Royal Statistical Society, \nSeries B, vol 50, 157-244, 1988. \n[8) D. Maier, The Theory of Relational Databases. \nComputer Science Press, 1983. \n(9] R.E. Neapolitan, Probabilistic Reasoning in Ex\u00ad\npert Systems. John Wiley and Sons, 1990. \n[10] J. Pearl, Probablistic Reasoning in Intelligent Sys\u00ad\ntems: Networks of Plausible Inference. Morgan \nKaufmann, 1988. \n[11) D .J. Rose, \"Triangulated graphs and the elimina\u00ad\ntion process,\" Journal of Mathematical Analysis \nand Its Applications, 32, 597-609, 1970. \n(12] G. Shafer, \"An axiomatic study of computation \nin hypertrees,\" School of Business Working Paper \nSeries, (No. 232), University of Kansas, Lawrence, \n1991. \n(13] P. Shenoy, \"Valuation-based systems for discrete \noptimization,\" Proc. Sixth Conference on Uncer\u00ad\ntainty in Artificial Intelligence, 334-335, 1990. \n[14) S.K.M. Wong, \n\"An extended relational data \nmodel for probablistic reasoning,\" submitted to \npublication, 1994. \n[15] S.K.M. Wong, Y. Xiang and X. Nie, \n\"Rep\u00ad\nresentation of bayesian networks as relational \ndatabases,\" Proc. Fifth International Conference \nInformation Processing and Management of Un\u00ad\ncertainty in Knowledge-based systems, 159-165, \n1994. \n[16] S.K.M. Wong, Z.W. Wang, \"On axiomatization \nof probabilistic conditional independence,\" Proc. \nTenth Conference on Uncertainty in Artificial In\u00ad\ntelligence, 591-597, 1994. \n",
        "sentence": " The implication of EMVDs for probabilistic conditional independence in Bayesian networks was originally described by [30] and further explored by [42].",
        "context": "tional relational databases. \nThis paper, a sequel of the presentation in the IPMU \nconference [15], reports on the technical details in\u00ad\nvolved in the design of a probabilistic inference system \nby transforming a Markov network into a relational\nprobabilistic model by a Markov network, as it has \nbeen shown that such a network can be represented as \na generalized relational database (14, 15, 16]. That \nis, the probabilistic model can be transformed into\ncisions under uncertainty. The input to a probabilistic \nmodel is usually a Bayesian network [10]. It may also \nconsist of a set of potentials which define a Markov \nnetwork [4]. In this paper, we assume that the proba\u00ad"
    },
    {
        "title": "Crossmine: Efficient classification across multiple database relations",
        "author": [
            "X. Yin",
            "J. Han",
            "J. Yang",
            "P.S. Yu"
        ],
        "venue": "Proceedings of the 2004 European Conference on Constraint-Based Mining and Inductive Databases, pages 172\u2013195, Berlin, Heidelberg",
        "citeRegEx": "43",
        "shortCiteRegEx": null,
        "year": 2005,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " It was also studied in [43] but their focus was on devising a new ML algorithm.",
        "context": null
    },
    {
        "title": "Efficient Feature Selection via Analysis of Relevance and Redundancy",
        "author": [
            "L. Yu",
            "H. Liu"
        ],
        "venue": "Journal of Machine Learning Research,",
        "citeRegEx": "44",
        "shortCiteRegEx": "44",
        "year": 2004,
        "abstract": "",
        "full_text": "",
        "sentence": " The trade-off between feature redundancy and relevancy is well-studied [14, 44, 21].",
        "context": null
    },
    {
        "title": "Understanding Deep Learning Requires Rethinking Generalization",
        "author": [
            "C. Zhang",
            "S. Bengio",
            "M. Hardt",
            "B. Recht",
            "O. Vinyals"
        ],
        "venue": "International Conference on Learning Representations (ICLR)",
        "citeRegEx": "45",
        "shortCiteRegEx": null,
        "year": 2017,
        "abstract": "Despite their massive size, successful deep artificial neural networks can\nexhibit a remarkably small difference between training and test performance.\nConventional wisdom attributes small generalization error either to properties\nof the model family, or to the regularization techniques used during training.\n  Through extensive systematic experiments, we show how these traditional\napproaches fail to explain why large neural networks generalize well in\npractice. Specifically, our experiments establish that state-of-the-art\nconvolutional networks for image classification trained with stochastic\ngradient methods easily fit a random labeling of the training data. This\nphenomenon is qualitatively unaffected by explicit regularization, and occurs\neven if we replace the true images by completely unstructured random noise. We\ncorroborate these experimental findings with a theoretical construction showing\nthat simple depth two neural networks already have perfect finite sample\nexpressivity as soon as the number of parameters exceeds the number of data\npoints as it usually does in practice.\n  We interpret our experimental findings by comparison with traditional models.",
        "full_text": "UNDERSTANDING\nDEEP\nLEARNING\nREQUIRES\nRE-\nTHINKING GENERALIZATION\nChiyuan Zhang\u2217\nMassachusetts Institute of Technology\nchiyuan@mit.edu\nSamy Bengio\nGoogle Brain\nbengio@google.com\nMoritz Hardt\nGoogle Brain\nmrtz@google.com\nBenjamin Recht\u2020\nUniversity of California, Berkeley\nbrecht@berkeley.edu\nOriol Vinyals\nGoogle DeepMind\nvinyals@google.com\nABSTRACT\nDespite their massive size, successful deep arti\ufb01cial neural networks can exhibit a\nremarkably small difference between training and test performance. Conventional\nwisdom attributes small generalization error either to properties of the model fam-\nily, or to the regularization techniques used during training.\nThrough extensive systematic experiments, we show how these traditional ap-\nproaches fail to explain why large neural networks generalize well in practice.\nSpeci\ufb01cally, our experiments establish that state-of-the-art convolutional networks\nfor image classi\ufb01cation trained with stochastic gradient methods easily \ufb01t a ran-\ndom labeling of the training data. This phenomenon is qualitatively unaffected\nby explicit regularization, and occurs even if we replace the true images by com-\npletely unstructured random noise. We corroborate these experimental \ufb01ndings\nwith a theoretical construction showing that simple depth two neural networks al-\nready have perfect \ufb01nite sample expressivity as soon as the number of parameters\nexceeds the number of data points as it usually does in practice.\nWe interpret our experimental \ufb01ndings by comparison with traditional models.\n1\nINTRODUCTION\nDeep arti\ufb01cial neural networks often have far more trainable model parameters than the number of\nsamples they are trained on. Nonetheless, some of these models exhibit remarkably small gener-\nalization error, i.e., difference between \u201ctraining error\u201d and \u201ctest error\u201d. At the same time, it is\ncertainly easy to come up with natural model architectures that generalize poorly. What is it then\nthat distinguishes neural networks that generalize well from those that don\u2019t? A satisfying answer\nto this question would not only help to make neural networks more interpretable, but it might also\nlead to more principled and reliable model architecture design.\nTo answer such a question, statistical learning theory has proposed a number of different complexity\nmeasures that are capable of controlling generalization error. These include VC dimension (Vapnik,\n1998), Rademacher complexity (Bartlett & Mendelson, 2003), and uniform stability (Mukherjee\net al., 2002; Bousquet & Elisseeff, 2002; Poggio et al., 2004). Moreover, when the number of\nparameters is large, theory suggests that some form of regularization is needed to ensure small\ngeneralization error. Regularization may also be implicit as is the case with early stopping.\n1.1\nOUR CONTRIBUTIONS\nIn this work, we problematize the traditional view of generalization by showing that it is incapable\nof distinguishing between different neural networks that have radically different generalization per-\nformance.\n\u2217Work performed while interning at Google Brain.\n\u2020Work performed at Google Brain.\narXiv:1611.03530v2  [cs.LG]  26 Feb 2017\nRandomization tests.\nAt the heart of our methodology is a variant of the well-known randomiza-\ntion test from non-parametric statistics (Edgington & Onghena, 2007). In a \ufb01rst set of experiments,\nwe train several standard architectures on a copy of the data where the true labels were replaced by\nrandom labels. Our central \ufb01nding can be summarized as:\nDeep neural networks easily \ufb01t random labels.\nMore precisely, when trained on a completely random labeling of the true data, neural networks\nachieve 0 training error. The test error, of course, is no better than random chance as there is no\ncorrelation between the training labels and the test labels. In other words, by randomizing labels\nalone we can force the generalization error of a model to jump up considerably without changing\nthe model, its size, hyperparameters, or the optimizer. We establish this fact for several different\nstandard architectures trained on the CIFAR10 and ImageNet classi\ufb01cation benchmarks. While\nsimple to state, this observation has profound implications from a statistical learning perspective:\n1. The effective capacity of neural networks is suf\ufb01cient for memorizing the entire data set.\n2. Even optimization on random labels remains easy. In fact, training time increases only by\na small constant factor compared with training on the true labels.\n3. Randomizing labels is solely a data transformation, leaving all other properties of the learn-\ning problem unchanged.\nExtending on this \ufb01rst set of experiments, we also replace the true images by completely random\npixels (e.g., Gaussian noise) and observe that convolutional neural networks continue to \ufb01t the data\nwith zero training error. This shows that despite their structure, convolutional neural nets can \ufb01t\nrandom noise. We furthermore vary the amount of randomization, interpolating smoothly between\nthe case of no noise and complete noise. This leads to a range of intermediate learning problems\nwhere there remains some level of signal in the labels. We observe a steady deterioration of the\ngeneralization error as we increase the noise level. This shows that neural networks are able to\ncapture the remaining signal in the data, while at the same time \ufb01t the noisy part using brute-force.\nWe discuss in further detail below how these observations rule out all of VC-dimension, Rademacher\ncomplexity, and uniform stability as possible explanations for the generalization performance of\nstate-of-the-art neural networks.\nThe role of explicit regularization.\nIf the model architecture itself isn\u2019t a suf\ufb01cient regularizer, it\nremains to see how much explicit regularization helps. We show that explicit forms of regularization,\nsuch as weight decay, dropout, and data augmentation, do not adequately explain the generalization\nerror of neural networks. Put differently:\nExplicit regularization may improve generalization performance, but is neither necessary nor by\nitself suf\ufb01cient for controlling generalization error.\nIn contrast with classical convex empirical risk minimization, where explicit regularization is nec-\nessary to rule out trivial solutions, we found that regularization plays a rather different role in deep\nlearning. It appears to be more of a tuning parameter that often helps improve the \ufb01nal test error\nof a model, but the absence of all regularization does not necessarily imply poor generalization er-\nror. As reported by Krizhevsky et al. (2012), \u21132-regularization (weight decay) sometimes even helps\noptimization, illustrating its poorly understood nature in deep learning.\nFinite sample expressivity.\nWe complement our empirical observations with a theoretical con-\nstruction showing that generically large neural networks can express any labeling of the training\ndata. More formally, we exhibit a very simple two-layer ReLU network with p = 2n+d parameters\nthat can express any labeling of any sample of size n in d dimensions. A previous construction due\nto Livni et al. (2014) achieved a similar result with far more parameters, namely, O(dn). While our\ndepth 2 network inevitably has large width, we can also come up with a depth k network in which\neach layer has only O(n/k) parameters.\nWhile prior expressivity results focused on what functions neural nets can represent over the entire\ndomain, we focus instead on the expressivity of neural nets with regards to a \ufb01nite sample. In\ncontrast to existing depth separations (Delalleau & Bengio, 2011; Eldan & Shamir, 2016; Telgarsky,\n2016; Cohen & Shashua, 2016) in function space, our result shows that even depth-2 networks of\nlinear size can already represent any labeling of the training data.\nThe role of implicit regularization.\nWhile explicit regularizers like dropout and weight-decay\nmay not be essential for generalization, it is certainly the case that not all models that \ufb01t the training\ndata well generalize well. Indeed, in neural networks, we almost always choose our model as the\noutput of running stochastic gradient descent. Appealing to linear models, we analyze how SGD\nacts as an implicit regularizer. For linear models, SGD always converges to a solution with small\nnorm. Hence, the algorithm itself is implicitly regularizing the solution. Indeed, we show on small\ndata sets that even Gaussian kernel methods can generalize well with no regularization. Though this\ndoesn\u2019t explain why certain architectures generalize better than other architectures, it does suggest\nthat more investigation is needed to understand exactly what the properties are inherited by models\nthat were trained using SGD.\n1.2\nRELATED WORK\nHardt et al. (2016) give an upper bound on the generalization error of a model trained with stochastic\ngradient descent in terms of the number of steps gradient descent took. Their analysis goes through\nthe notion of uniform stability (Bousquet & Elisseeff, 2002). As we point out in this work, uniform\nstability of a learning algorithm is independent of the labeling of the training data. Hence, the\nconcept is not strong enough to distinguish between the models trained on the true labels (small\ngeneralization error) and models trained on random labels (high generalization error). This also\nhighlights why the analysis of Hardt et al. (2016) for non-convex optimization was rather pessimistic,\nallowing only a very few passes over the data. Our results show that even empirically training neural\nnetworks is not uniformly stable for many passes over the data. Consequently, a weaker stability\nnotion is necessary to make further progress along this direction.\nThere has been much work on the representational power of neural networks, starting from universal\napproximation theorems for multi-layer perceptrons (Cybenko, 1989; Mhaskar, 1993; Delalleau &\nBengio, 2011; Mhaskar & Poggio, 2016; Eldan & Shamir, 2016; Telgarsky, 2016; Cohen & Shashua,\n2016). All of these results are at the population level characterizing which mathematical functions\ncertain families of neural networks can express over the entire domain. We instead study the repre-\nsentational power of neural networks for a \ufb01nite sample of size n. This leads to a very simple proof\nthat even O(n)-sized two-layer perceptrons have universal \ufb01nite-sample expressivity.\nBartlett (1998) proved bounds on the fat shattering dimension of multilayer perceptrons with sig-\nmoid activations in terms of the \u21131-norm of the weights at each node. This important result gives a\ngeneralization bound for neural nets that is independent of the network size. However, for RELU\nnetworks the \u21131-norm is no longer informative. This leads to the question of whether there is a dif-\nferent form of capacity control that bounds generalization error for large neural nets. This question\nwas raised in a thought-provoking work by Neyshabur et al. (2014), who argued through experi-\nments that network size is not the main form of capacity control for neural networks. An analogy to\nmatrix factorization illustrated the importance of implicit regularization.\n2\nEFFECTIVE CAPACITY OF NEURAL NETWORKS\nOur goal is to understand the effective model capacity of feed-forward neural networks. Toward\nthis goal, we choose a methodology inspired by non-parametric randomization tests. Speci\ufb01cally,\nwe take a candidate architecture and train it both on the true data and on a copy of the data in\nwhich the true labels were replaced by random labels. In the second case, there is no longer any\nrelationship between the instances and the class labels. As a result, learning is impossible. Intuition\nsuggests that this impossibility should manifest itself clearly during training, e.g., by training not\nconverging or slowing down substantially. To our surprise, several properties of the training process\nfor multiple standard achitectures is largely unaffected by this transformation of the labels. This\nposes a conceptual challenge. Whatever justi\ufb01cation we had for expecting a small generalization\nerror to begin with must no longer apply to the case of random labels.\n0\n5\n10\n15\n20\n25\nthousand steps\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\naverage_loss\ntrue labels\nrandom labels\nshuffled pixels\nrandom pixels\ngaussian\n(a) learning curves\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nlabel corruption\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\ntime to overfit\nInception\nAlexNet\nMLP 1x512\n(b) convergence slowdown\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nlabel corruption\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\ntest_error\nInception\nAlexNet\nMLP 1x512\n(c) generalization error growth\nFigure 1: Fitting random labels and random pixels on CIFAR10. (a) shows the training loss of\nvarious experiment settings decaying with the training steps. (b) shows the relative convergence\ntime with different label corruption ratio. (c) shows the test error (also the generalization error since\ntraining error is 0) under different label corruptions.\nTo gain further insight into this phenomenon, we experiment with different levels of randomization\nexploring the continuum between no label noise and completely corrupted labels. We also try out\ndifferent randomizations of the inputs (rather than labels), arriving at the same general conclusion.\nThe experiments are run on two image classi\ufb01cation datasets, the CIFAR10 dataset (Krizhevsky\n& Hinton, 2009) and the ImageNet (Russakovsky et al., 2015) ILSVRC 2012 dataset. We test the\nInception V3 (Szegedy et al., 2016) architecture on ImageNet and a smaller version of Inception,\nAlexnet (Krizhevsky et al., 2012), and MLPs on CIFAR10. Please see Section A in the appendix for\nmore details of the experimental setup.\n2.1\nFITTING RANDOM LABELS AND PIXELS\nWe run our experiments with the following modi\ufb01cations of the labels and input images:\n\u2022 True labels: the original dataset without modi\ufb01cation.\n\u2022 Partially corrupted labels: independently with probability p, the label of each image is\ncorrupted as a uniform random class.\n\u2022 Random labels: all the labels are replaced with random ones.\n\u2022 Shuf\ufb02ed pixels: a random permutation of the pixels is chosen and then the same permuta-\ntion is applied to all the images in both training and test set.\n\u2022 Random pixels: a different random permutation is applied to each image independently.\n\u2022 Gaussian: A Gaussian distribution (with matching mean and variance to the original image\ndataset) is used to generate random pixels for each image.\nSurprisingly, stochastic gradient descent with unchanged hyperparameter settings can optimize the\nweights to \ufb01t to random labels perfectly, even though the random labels completely destroy the\nrelationship between images and labels. We further break the structure of the images by shuf\ufb02ing\nthe image pixels, and even completely re-sampling random pixels from a Gaussian distribution. But\nthe networks we tested are still able to \ufb01t.\nFigure 1a shows the learning curves of the Inception model on the CIFAR10 dataset under vari-\nous settings. We expect the objective function to take longer to start decreasing on random labels\nbecause initially the label assignments for every training sample is uncorrelated. Therefore, large\npredictions errors are back-propagated to make large gradients for parameter updates. However,\nsince the random labels are \ufb01xed and consistent across epochs, the network starts \ufb01tting after going\nthrough the training set multiple times. We \ufb01nd the following observations for \ufb01tting random labels\nvery interesting: a) we do not need to change the learning rate schedule; b) once the \ufb01tting starts,\nit converges quickly; c) it converges to (over)\ufb01t the training set perfectly. Also note that \u201crandom\npixels\u201d and \u201cGaussian\u201d start converging faster than \u201crandom labels\u201d. This might be because with\nrandom pixels, the inputs are more separated from each other than natural images that originally\nbelong to the same category, therefore, easier to build a network for arbitrary label assignments.\nOn the CIFAR10 dataset, Alexnet and MLPs all converge to zero loss on the training set. The shaded\nrows in Table 1 show the exact numbers and experimental setup. We also tested random labels on the\nImageNet dataset. As shown in the last three rows of Table 2 in the appendix, although it does not\nreach the perfect 100% top-1 accuracy, 95.20% accuracy is still very surprising for a million random\nlabels from 1000 categories. Note that we did not do any hyperparameter tuning when switching\nfrom the true labels to random labels. It is likely that with some modi\ufb01cation of the hyperparameters,\nperfect accuracy could be achieved on random labels. The network also manages to reach \u223c90%\ntop-1 accuracy even with explicit regularizers turned on.\nPartially corrupted labels\nWe further inspect the behavior of neural network training with a vary-\ning level of label corruptions from 0 (no corruption) to 1 (complete random labels) on the CIFAR10\ndataset. The networks \ufb01t the corrupted training set perfectly for all the cases. Figure 1b shows the\nslowdown of the convergence time with increasing level of label noises. Figure 1c depicts the test\nerrors after convergence. Since the training errors are always zero, the test errors are the same as\ngeneralization errors. As the noise level approaches 1, the generalization errors converge to 90% \u2014\nthe performance of random guessing on CIFAR10.\n2.2\nIMPLICATIONS\nIn light of our randomization experiments, we discuss how our \ufb01ndings pose a challenge for several\ntraditional approaches for reasoning about generalization.\nRademacher complexity and VC-dimension.\nRademacher complexity is commonly used and\n\ufb02exible complexity measure of a hypothesis class. The empirical Rademacher complexity of a\nhypothesis class H on a dataset {x1, . . . , xn} is de\ufb01ned as\n\u02c6Rn(H) = E\u03c3\n\"\nsup\nh\u2208H\n1\nn\nn\nX\ni=1\n\u03c3ih(xi)\n#\n(1)\nwhere \u03c31, . . . , \u03c3n \u2208{\u00b11} are i.i.d. uniform random variables. This de\ufb01nition closely resembles\nour randomization test. Speci\ufb01cally, \u02c6Rn(H) measures ability of H to \ufb01t random \u00b11 binary label\nassignments. While we consider multiclass problems, it is straightforward to consider related binary\nclassi\ufb01cation problems for which the same experimental observations hold. Since our randomization\ntests suggest that many neural networks \ufb01t the training set with random labels perfectly, we expect\nthat \u02c6Rn(H) \u22481 for the corresponding model class H. This is, of course, a trivial upper bound on\nthe Rademacher complexity that does not lead to useful generalization bounds in realistic settings.\nA similar reasoning applies to VC-dimension and its continuous analog fat-shattering dimension,\nunless we further restrict the network. While Bartlett (1998) proves a bound on the fat-shattering\ndimension in terms of \u21131 norm bounds on the weights of the network, this bound does not apply to\nthe ReLU networks that we consider here. This result was generalized to other norms by Neyshabur\net al. (2015), but even these do not seem to explain the generalization behavior that we observe.\nUniform stability.\nStepping away from complexity measures of the hypothesis class, we can in-\nstead consider properties of the algorithm used for training. This is commonly done with some\nnotion of stability, such as uniform stability (Bousquet & Elisseeff, 2002). Uniform stability of an\nalgorithm A measures how sensitive the algorithm is to the replacement of a single example. How-\never, it is solely a property of the algorithm, which does not take into account speci\ufb01cs of the data\nor the distribution of the labels. It is possible to de\ufb01ne weaker notions of stability (Mukherjee et al.,\n2002; Poggio et al., 2004; Shalev-Shwartz et al., 2010). The weakest stability measure is directly\nequivalent to bounding generalization error and does take the data into account. However, it has\nbeen dif\ufb01cult to utilize this weaker stability notion effectively.\n3\nTHE ROLE OF REGULARIZATION\nMost of our randomization tests are performed with explicit regularization turned off. Regularizers\nare the standard tool in theory and practice to mitigate over\ufb01tting in the regime when there are more\nTable 1: The training and test accuracy (in percentage) of various models on the CIFAR10 dataset.\nPerformance with and without data augmentation and weight decay are compared. The results of\n\ufb01tting random labels are also included.\nmodel\n# params\nrandom crop\nweight decay\ntrain accuracy\ntest accuracy\nInception\n1,649,402\nyes\nyes\n100.0\n89.05\nyes\nno\n100.0\n89.31\nno\nyes\n100.0\n86.03\nno\nno\n100.0\n85.75\n(\ufb01tting random labels)\nno\nno\n100.0\n9.78\nInception w/o\nBatchNorm\n1,649,402\nno\nyes\n100.0\n83.00\nno\nno\n100.0\n82.00\n(\ufb01tting random labels)\nno\nno\n100.0\n10.12\nAlexnet\n1,387,786\nyes\nyes\n99.90\n81.22\nyes\nno\n99.82\n79.66\nno\nyes\n100.0\n77.36\nno\nno\n100.0\n76.07\n(\ufb01tting random labels)\nno\nno\n99.82\n9.86\nMLP 3x512\n1,735,178\nno\nyes\n100.0\n53.35\nno\nno\n100.0\n52.39\n(\ufb01tting random labels)\nno\nno\n100.0\n10.48\nMLP 1x512\n1,209,866\nno\nyes\n99.80\n50.39\nno\nno\n100.0\n50.51\n(\ufb01tting random labels)\nno\nno\n99.34\n10.61\nparameters than data points (Vapnik, 1998). The basic idea is that although the original hypothesis\nis too large to generalize well, regularizers help con\ufb01ne learning to a subset of the hypothesis space\nwith manageable complexity. By adding an explicit regularizer, say by penalizing the norm of\nthe optimal solution, the effective Rademacher complexity of the possible solutions is dramatically\nreduced.\nAs we will see, in deep learning, explicit regularization seems to play a rather different role. As the\nbottom rows of Table 2 in the appendix show, even with dropout and weight decay, InceptionV3 is\nstill able to \ufb01t the random training set extremely well if not perfectly. Although not shown explicitly,\non CIFAR10, both Inception and MLPs still \ufb01t perfectly the random training set with weight decay\nturned on. However, AlexNet with weight decay turned on fails to converge on random labels. To\ninvestigate the role of regularization in deep learning, we explicitly compare behavior of deep nets\nlearning with and without regularizers.\nInstead of doing a full survey of all kinds of regularization techniques introduced for deep learn-\ning, we simply take several commonly used network architectures, and compare the behavior when\nturning off the equipped regularizers. The following regularizers are covered:\n\u2022 Data augmentation: augment the training set via domain-speci\ufb01c transformations. For\nimage data, commonly used transformations include random cropping, random perturba-\ntion of brightness, saturation, hue and contrast.\n\u2022 Weight decay: equivalent to a \u21132 regularizer on the weights; also equivalent to a hard\nconstrain of the weights to an Euclidean ball, with the radius decided by the amount of\nweight decay.\n\u2022 Dropout (Srivastava et al., 2014): mask out each element of a layer output randomly with\na given dropout probability. Only the Inception V3 for ImageNet uses dropout in our\nexperiments.\nTable 1 shows the results of Inception, Alexnet and MLPs on CIFAR10, toggling the use of data\naugmentation and weight decay. Both regularization techniques help to improve the generalization\n0\n2000\n4000\n6000\n8000\n10000\nthousand training steps\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\naccuracy\ntest(w/ aug, wd, dropout)\ntrain(w/ aug, wd, dropout)\ntest(w/o aug, dropout)\ntrain(w/o aug, dropout)\ntest(w/o aug, wd, dropout)\ntrain(w/o aug, wd, dropout)\n(a) Inception on ImageNet\n0\n5\n10\n15\n20\nthousand training steps\n0.6\n0.7\n0.8\n0.9\n1.0\naccuracy\ntest(Inception)\ntrain(Inception)\ntest(Inception w/o BN)\ntrain(Inception w/o BN)\n(b) Inception on CIFAR10\nFigure 2: Effects of implicit regularizers on generalization performance. aug is data augmentation,\nwd is weight decay, BN is batch normalization. The shaded areas are the cumulative best test ac-\ncuracy, as an indicator of potential performance gain of early stopping. (a) early stopping could\npotentially improve generalization when other regularizers are absent. (b) early stopping is not nec-\nessarily helpful on CIFAR10, but batch normalization stablize the training process and improves\ngeneralization.\nperformance, but even with all of the regularizers turned off, all of the models still generalize very\nwell.\nTable 2 in the appendix shows a similar experiment on the ImageNet dataset. A 18% top-1 accuracy\ndrop is observed when we turn off all the regularizers. Speci\ufb01cally, the top-1 accuracy without\nregularization is 59.80%, while random guessing only achieves 0.1% top-1 accuracy on ImageNet.\nMore strikingly, with data-augmentation on but other explicit regularizers off, Inception is able to\nachieve a top-1 accuracy of 72.95%. Indeed, it seems like the ability to augment the data using\nknown symmetries is signi\ufb01cantly more powerful than just tuning weight decay or preventing low\ntraining error.\nInception achieves 80.38% top-5 accuracy without regularization, while the reported number of\nthe winner of ILSVRC 2012 (Krizhevsky et al., 2012) achieved 83.6%. So while regularization is\nimportant, bigger gains can be achieved by simply changing the model architecture. It is dif\ufb01cult\nto say that the regularizers count as a fundamental phase change in the generalization capability of\ndeep nets.\n3.1\nIMPLICIT REGULARIZATIONS\nEarly stopping was shown to implicitly regularize on some convex learning problems (Yao et al.,\n2007; Lin et al., 2016). In Table 2 in the appendix, we show in parentheses the best test accuracy\nalong the training process. It con\ufb01rms that early stopping could potentially1 improve the general-\nization performance. Figure 2a shows the training and testing accuracy on ImageNet. The shaded\narea indicate the accumulative best test accuracy, as a reference of potential performance gain for\nearly stopping. However, on the CIFAR10 dataset, we do not observe any potential bene\ufb01t of early\nstopping.\nBatch normalization (Ioffe & Szegedy, 2015) is an operator that normalizes the layer responses\nwithin each mini-batch. It has been widely adopted in many modern neural network architectures\nsuch as Inception (Szegedy et al., 2016) and Residual Networks (He et al., 2016). Although not\nexplicitly designed for regularization, batch normalization is usually found to improve the general-\nization performance. The Inception architecture uses a lot of batch normalization layers. To test the\nimpact of batch normalization, we create a \u201cInception w/o BatchNorm\u201d architecture that is exactly\nthe same as Inception in Figure 3, except with all the batch normalization layers removed. Figure 2b\n1We say \u201cpotentially\u201d because to make this statement rigorous, we need to have another isolated test set and\ntest the performance there when we choose early stopping point on the \ufb01rst test set (acting like a validation set).\ncompares the learning curves of the two variants of Inception on CIFAR10, with all the explicit reg-\nularizers turned off. The normalization operator helps stablize the learning dynamics, but the impact\non the generalization performance is only 3\u223c4%. The exact accuracy is also listed in the section\n\u201cInception w/o BatchNorm\u201d of Table 1.\nIn summary, our observations on both explicit and implicit regularizers are consistently suggesting\nthat regularizers, when properly tuned, could help to improve the generalization performance. How-\never, it is unlikely that the regularizers are the fundamental reason for generalization, as the networks\ncontinue to perform well after all the regularizers removed.\n4\nFINITE-SAMPLE EXPRESSIVITY\nMuch effort has gone into characterizing the expressivity of neural networks, e.g, Cybenko (1989);\nMhaskar (1993); Delalleau & Bengio (2011); Mhaskar & Poggio (2016); Eldan & Shamir (2016);\nTelgarsky (2016); Cohen & Shashua (2016). Almost all of these results are at the \u201cpopulation level\u201d\nshowing what functions of the entire domain can and cannot be represented by certain classes of\nneural networks with the same number of parameters. For example, it is known that at the population\nlevel depth k is generically more powerful than depth k \u22121.\nWe argue that what is more relevant in practice is the expressive power of neural networks on a\n\ufb01nite sample of size n. It is possible to transfer population level results to \ufb01nite sample results\nusing uniform convergence theorems. However, such uniform convergence bounds would require\nthe sample size to be polynomially large in the dimension of the input and exponential in the depth\nof the network, posing a clearly unrealistic requirement in practice.\nWe instead directly analyze the \ufb01nite-sample expressivity of neural networks, noting that this dra-\nmatically simpli\ufb01es the picture. Speci\ufb01cally, as soon as the number of parameters p of a networks is\ngreater than n, even simple two-layer neural networks can represent any function of the input sam-\nple. We say that a neural network C can represent any function of a sample of size n in d dimensions\nif for every sample S \u2286Rd with |S| = n and every function f : S \u2192R, there exists a setting of the\nweights of C such that C(x) = f(x) for every x \u2208S.\nTheorem 1. There exists a two-layer neural network with ReLU activations and 2n+d weights that\ncan represent any function on a sample of size n in d dimensions.\nThe proof is given in Section C in the appendix, where we also discuss how to achieve width O(n/k)\nwith depth k. We remark that it\u2019s a simple exercise to give bounds on the weights of the coef\ufb01cient\nvectors in our construction. Lemma 1 gives a bound on the smallest eigenvalue of the matrix A.\nThis can be used to give reasonable bounds on the weight of the solution w.\n5\nIMPLICIT REGULARIZATION: AN APPEAL TO LINEAR MODELS\nAlthough deep neural nets remain mysterious for many reasons, we note in this section that it is not\nnecessarily easy to understand the source of generalization for linear models either. Indeed, it is\nuseful to appeal to the simple case of linear models to see if there are parallel insights that can help\nus better understand neural networks.\nSuppose we collect n distinct data points {(xi, yi)} where xi are d-dimensional feature vectors and\nyi are labels. Letting loss denote a nonnegative loss function with loss(y, y) = 0, consider the\nempirical risk minimization (ERM) problem\nminw\u2208Rd 1\nn\nPn\ni=1 loss(wT xi, yi)\n(2)\nIf d \u2265n, then we can \ufb01t any labeling. But is it then possible to generalize with such a rich model\nclass and no explicit regularization?\nLet X denote the n \u00d7 d data matrix whose i-th row is xT\ni . If X has rank n, then the system of\nequations Xw = y has an in\ufb01nite number of solutions regardless of the right hand side. We can \ufb01nd\na global minimum in the ERM problem (2) by simply solving this linear system.\nBut do all global minima generalize equally well? Is there a way to determine when one global\nminimum will generalize whereas another will not? One popular way to understand quality of\nminima is the curvature of the loss function at the solution. But in the linear case, the curvature of\nall optimal solutions is the same (Choromanska et al., 2015). To see this, note that in the case when\nyi is a scalar,\n\u22072 1\nn\nPn\ni=1 loss(wT xi, yi) = 1\nnXT diag(\u03b2)X,\n\u0012\n\u03b2i := \u22022 loss(z,yi)\n\u2202z2\n\f\f\f\nz=yi , \u2200i\n\u0013\nA similar formula can be found when y is vector valued. In particular, the Hessian is not a function\nof the choice of w. Moreover, the Hessian is degenerate at all global optimal solutions.\nIf curvature doesn\u2019t distinguish global minima, what does? A promising direction is to consider the\nworkhorse algorithm, stochastic gradient descent (SGD), and inspect which solution SGD converges\nto. Since the SGD update takes the form wt+1 = wt \u2212\u03b7tetxit where \u03b7t is the step size and et is\nthe prediction error loss. If w0 = 0, we must have that the solution has the form w = Pn\ni=1 \u03b1ixi\nfor some coef\ufb01cients \u03b1. Hence, if we run SGD we have that w = XT \u03b1 lies in the span of the\ndata points. If we also perfectly interpolate the labels we have Xw = y. Enforcing both of these\nidentities, this reduces to the single equation\nXXT \u03b1 = y\n(3)\nwhich has a unique solution. Note that this equation only depends on the dot-products between\nthe data points xi. We have thus derived the \u201ckernel trick\u201d (Sch\u00a8olkopf et al., 2001)\u2014albeit in a\nroundabout fashion.\nWe can therefore perfectly \ufb01t any set of labels by forming the Gram matrix (aka the kernel matrix)\non the data K = XXT and solving the linear system K\u03b1 = y for \u03b1. This is an n \u00d7 n linear system\nthat can be solved on standard workstations whenever n is less than a hundred thousand, as is the\ncase for small benchmarks like CIFAR10 and MNIST.\nQuite surprisingly, \ufb01tting the training labels exactly yields excellent performance for convex models.\nOn MNIST with no preprocessing, we are able to achieve a test error of 1.2% by simply solving (3).\nNote that this is not exactly simple as the kernel matrix requires 30GB to store in memory. Nonethe-\nless, this system can be solved in under 3 minutes in on a commodity workstation with 24 cores and\n256 GB of RAM with a conventional LAPACK call. By \ufb01rst applying a Gabor wavelet transform to\nthe data and then solving (3), the error on MNIST drops to 0.6%. Surprisingly, adding regularization\ndoes not improve either model\u2019s performance!\nSimilar results follow for CIFAR10. Simply applying a Gaussian kernel on pixels and using no\nregularization achieves 46% test error. By preprocessing with a random convolutional neural net\nwith 32,000 random \ufb01lters, this test error drops to 17% error2. Adding \u21132 regularization further\nreduces this number to 15% error. Note that this is without any data augmentation.\nNote that this kernel solution has an appealing interpretation in terms of implicit regularization.\nSimple algebra reveals that it is equivalent to the minimum \u21132-norm solution of Xw = y. That is,\nout of all models that exactly \ufb01t the data, SGD will often converge to the solution with minimum\nnorm. It is very easy to construct solutions of Xw = y that don\u2019t generalize: for example, one could\n\ufb01t a Gaussian kernel to data and place the centers at random points. Another simple example would\nbe to force the data to \ufb01t random labels on the test data. In both cases, the norm of the solution is\nsigni\ufb01cantly larger than the minimum norm solution.\nUnfortunately, this notion of minimum norm is not predictive of generalization performance. For\nexample, returning to the MNIST example, the \u21132-norm of the minimum norm solution with no\npreprocessing is approximately 220. With wavelet preprocessing, the norm jumps to 390. Yet the\ntest error drops by a factor of 2. So while this minimum-norm intuition may provide some guidance\nto new algorithm design, it is only a very small piece of the generalization story.\n6\nCONCLUSION\nIn this work we presented a simple experimental framework for de\ufb01ning and understanding a notion\nof effective capacity of machine learning models. The experiments we conducted emphasize that the\neffective capacity of several successful neural network architectures is large enough to shatter the\n2This conv-net is the Coates & Ng (2012) net, but with the \ufb01lters selected at random instead of with k-means.\ntraining data. Consequently, these models are in principle rich enough to memorize the training data.\nThis situation poses a conceptual challenge to statistical learning theory as traditional measures of\nmodel complexity struggle to explain the generalization ability of large arti\ufb01cial neural networks.\nWe argue that we have yet to discover a precise formal measure under which these enormous models\nare simple. Another insight resulting from our experiments is that optimization continues to be\nempirically easy even if the resulting model does not generalize. This shows that the reasons for\nwhy optimization is empirically easy must be different from the true cause of generalization.\nREFERENCES\nMart\u00b4\u0131n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S.\nCorrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew\nHarp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath\nKudlur, Josh Levenberg, Dan Man\u00b4e, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah,\nMike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vin-\ncent Vanhoucke, Vijay Vasudevan, Fernanda Vi\u00b4egas, Oriol Vinyals, Pete Warden, Martin Watten-\nberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning\non heterogeneous systems, 2015. URL http://tensorflow.org/. Software available from ten-\nsor\ufb02ow.org.\nPeter L Bartlett. The Sample Complexity of Pattern Classi\ufb01cation with Neural Networks - The Size\nof the Weights is More Important than the Size of the Network. IEEE Trans. Information Theory,\n1998.\nPeter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: risk bounds and\nstructural results. Journal of Machine Learning Research, 3:463\u2013482, March 2003.\nOlivier Bousquet and Andr\u00b4e Elisseeff. Stability and generalization. Journal of Machine Learning\nResearch, 2:499\u2013526, March 2002.\nAnna Choromanska, Mikael Henaff, Michael Mathieu, G\u00b4erard Ben Arous, and Yann LeCun. The\nloss surfaces of multilayer networks. In AISTATS, 2015.\nAdam Coates and Andrew Y. Ng. Learning feature representations with k-means. In Neural Net-\nworks: Tricks of the Trade, Reloaded. Springer, 2012.\nNadav Cohen and Amnon Shashua. Convolutional Recti\ufb01er Networks as Generalized Tensor De-\ncompositions. In ICML, 2016.\nG Cybenko. Approximation by superposition of sigmoidal functions. Mathematics of Control,\nSignals and Systems, 2(4):303\u2013314, 1989.\nOlivier Delalleau and Yoshua Bengio. Shallow vs. Deep Sum-Product Networks. In Advances in\nNeural Information Processing Systems, 2011.\nE. Edgington and P. Onghena. Randomization Tests. Statistics: A Series of Textbooks and Mono-\ngraphs. Taylor & Francis, 2007. ISBN 9781584885894.\nRonen Eldan and Ohad Shamir. The Power of Depth for Feedforward Neural Networks. In COLT,\n2016.\nMoritz Hardt, Benjamin Recht, and Yoram Singer.\nTrain faster, generalize better: Stability of\nstochastic gradient descent. In ICML, 2016.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image\nRecognition. In CVPR, 2016.\nSergey Ioffe and Christian Szegedy. Batch Normalization: Accelerating Deep Network Training by\nReducing Internal Covariate Shift. In ICML, 2015.\nAlex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Tech-\nnical report, Department of Computer Science, University of Toronto, 2009.\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet Classi\ufb01cation with Deep Con-\nvolutional Neural Networks. In Advances in Neural Information Processing Systems, 2012.\nJunhong Lin, Raffaello Camoriano, and Lorenzo Rosasco. Generalization Properties and Implicit\nRegularization for Multiple Passes SGM. In ICML, 2016.\nRoi Livni, Shai Shalev-Shwartz, and Ohad Shamir. On the computational ef\ufb01ciency of training\nneural networks. In Advances in Neural Information Processing Systems, 2014.\nHrushikesh Mhaskar and Tomaso A. Poggio. Deep vs. shallow networks : An approximation theory\nperspective. CoRR, abs/1608.03287, 2016. URL http://arxiv.org/abs/1608.03287.\nHrushikesh Narhar Mhaskar. Approximation properties of a multilayered feedforward arti\ufb01cial neu-\nral network. Advances in Computational Mathematics, 1(1):61\u201380, 1993.\nSayan Mukherjee, Partha Niyogi, Tomaso Poggio, and Ryan Rifkin. Statistical learning: Stability\nis suf\ufb01cient for generalization and necessary and suf\ufb01cient for consistency of empirical risk min-\nimization. Technical Report AI Memo 2002-024, Massachusetts Institute of Technology, 2002.\nBehnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the\nrole of implicit regularization in deep learning. CoRR, abs/1412.6614, 2014.\nBehnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-Based Capacity Control in Neural\nNetworks. In COLT, pp. 1376\u20131401, 2015.\nTomaso Poggio, Ryan Rifkin, Sayan Mukherjee, and Partha Niyogi. General conditions for predic-\ntivity in learning theory. Nature, 428(6981):419\u2013422, 2004.\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng\nHuang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-\nFei. Imagenet large scale visual recognition challenge. International Journal of Computer Vision,\n115(3):211\u2013252, 2015. ISSN 1573-1405. doi: 10.1007/s11263-015-0816-y.\nBernhard Sch\u00a8olkopf, Ralf Herbrich, and Alex J Smola. A generalized representer theorem. In COLT,\n2001.\nShai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Learnability, stability\nand uniform convergence. Journal of Machine Learning Research, 11:2635\u20132670, October 2010.\nNitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.\nDropout: a simple way to prevent neural networks from over\ufb01tting. Journal of Machine Learning\nResearch, 15(1):1929\u20131958, 2014.\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Re-\nthinking the inception architecture for computer vision. In CVPR, pp. 2818\u20132826, 2016. doi:\n10.1109/CVPR.2016.308.\nMatus Telgarsky. Bene\ufb01ts of depth in neural networks. In COLT, 2016.\nVladimir N. Vapnik. Statistical Learning Theory. Adaptive and learning systems for signal process-\ning, communications, and control. Wiley, 1998.\nYuan Yao, Lorenzo Rosasco, and Andrea Caponnetto. On early stopping in gradient descent learn-\ning. Constructive Approximation, 26(2):289\u2013315, 2007.\nConv Module\nC,KxK \ufb01lters\u000f\u0003\nSxS strides\nConvolution\nC, KxK \ufb01lters\nSxS strides\nBatch Norm\nActivation\nReLU\nInception Module\nCh1 + Ch3 \ufb01lters\nConv Module\nCh1,1x1 \ufb01lters\n1x1 strides\nMerge\nConcat in channels\nConv Module\nCh3,3x3 \ufb01lters\n1x1 strides\nDownsample Module\nCh3 \ufb01lters\nConv Module\nCh3,3x3 \ufb01lters\n2x2 strides\nMerge\nConcat in channels\nMax Pool\n3x3 kernel\n2x2 strides\nInception (Small)\n28x28x3 inputs\nImages\n28x28x3 inputs\nConv Module\n96,3x3 \ufb01lters\n1x1 strides\nInception Module\n32 + 32 \ufb01lters\nInception Module\n32 + 48 \ufb01lters\nDownsample Module\n80 \ufb01lters\nInception Module\n80 + 80 \ufb01lters\nInception Module\n48 + 96 \ufb01lters\nDownsample Module\n96 \ufb01lters\nInception Module\n96 + 64 \ufb01lters\nInception Module\n112 + 48 \ufb01lters\nInception Module\n176 + 160 \ufb01lters\nInception Module\n176 + 160 \ufb01lters\nMean Pooling\n7x7 kernel (global)\nFully Connected\n10-way outputs\nFigure 3: The small Inception model adapted for the CIFAR10 dataset. On the left we show the\nConv module, the Inception module and the Downsample module, which are used to construct the\nInception architecture on the right.\nA\nEXPERIMENTAL SETUP\nWe focus on two image classi\ufb01cation datasets, the CIFAR10 dataset (Krizhevsky & Hinton, 2009)\nand the ImageNet (Russakovsky et al., 2015) ILSVRC 2012 dataset.\nThe CIFAR10 dataset contains 50,000 training and 10,000 validation images, split into 10 classes.\nEach image is of size 32x32, with 3 color channels. We divide the pixel values by 255 to scale\nthem into [0, 1], crop from the center to get 28x28 inputs, and then normalize them by subtract-\ning the mean and dividing the adjusted standard deviation independently for each image with the\nper_image_whitening function in TENSORFLOW (Abadi et al., 2015).\nFor the experiment on CIFAR10, we test a simpli\ufb01ed Inception (Szegedy et al., 2016) and Alexnet\n(Krizhevsky et al., 2012) by adapting the architectures to smaller input image sizes. We also test\nstandard multi-layer perceptrons (MLPs) with various number of hidden layers.\nThe small Inception model uses a combination of 1x1 and 3x3 convolution pathways. The detailed\narchitecture is illustrated in Figure 3. The small Alexnet is constructed by two (convolution 5x5\n\u2192max-pool 3x3 \u2192local-response-normalization) modules followed by two fully connected layers\nwith 384 and 192 hidden units, respectively. Finally a 10-way linear layer is used for prediction.\nThe MLPs use fully connected layers. MLP 1x512 means one hidden layer with 512 hidden units.\nAll of the architectures use standard recti\ufb01ed linear activation functions (ReLU).\nFor all experiments on CIFAR10, we train using SGD with a momentum parameter of 0.9. An\ninitial learning rate of 0.1 (for small Inception) or 0.01 (for small Alexnet and MLPs) are used,\nwith a decay factor of 0.95 per training epoch. Unless otherwise speci\ufb01ed, for the experiments with\nrandomized labels or pixels, we train the networks without weight decay, dropout, or other forms of\nexplicit regularization. Section 3 discusses the effects of various regularizers on \ufb01tting the networks\nand generalization.\nThe ImageNet dataset contains 1,281,167 training and 50,000 validation images, split into 1000\nclasses. Each image is resized to 299x299 with 3 color channels. In the experiment on ImageNet,\nwe use the Inception V3 (Szegedy et al., 2016) architecture and reuse the data preprocessing and\nexperimental setup from the TENSORFLOW package. The data pipeline is extended to allow dis-\nabling of data augmentation and feeding random labels that are consistent across epochs. We run\nthe ImageNet experiment in a distributed asynchronized SGD system with 50 workers.\nB\nDETAILED RESULTS ON IMAGENET\nTable 2: The top-1 and top-5 accuracy (in percentage) of the Inception v3 model on the ImageNet\ndataset. We compare the training and test accuracy with various regularization turned on and off,\nfor both true labels and random labels. The original reported top-5 accuracy of the Alexnet on\nILSVRC 2012 is also listed for reference. The numbers in parentheses are the best test accuracy\nduring training, as a reference for potential performance gain of early stopping.\ndata\naug\ndropout\nweight\ndecay\ntop-1 train\ntop-5 train\ntop-1 test\ntop-5 test\nImageNet 1000 classes with the original labels\nyes\nyes\nyes\n92.18\n99.21\n77.84\n93.92\nyes\nno\nno\n92.33\n99.17\n72.95\n90.43\nno\nno\nyes\n90.60\n100.0\n67.18 (72.57)\n86.44 (91.31)\nno\nno\nno\n99.53\n100.0\n59.80 (63.16)\n80.38 (84.49)\nAlexnet (Krizhevsky et al., 2012)\n-\n-\n-\n83.6\nImageNet 1000 classes with random labels\nno\nyes\nyes\n91.18\n97.95\n0.09\n0.49\nno\nno\nyes\n87.81\n96.15\n0.12\n0.50\nno\nno\nno\n95.20\n99.14\n0.11\n0.56\nTable 2 shows the performance on Imagenet with true labels and random labels, respectively.\nC\nPROOF OF THEOREM 1\nLemma 1. For any two interleaving sequences of n real numbers b1 < x1 < b2 < x2 \u00b7 \u00b7 \u00b7 < bn <\nxn, the n \u00d7 n matrix A = [max{xi \u2212bj, 0}]ij has full rank. Its smallest eigenvalue is mini xi \u2212bi.\nProof. By its de\ufb01nition, the matrix A is lower triangular, that is, all entries with i < j vanish. A\nbasic linear algebra fact states that a lower-triangular matrix has full rank if and only if all of the\nentries on the diagional are nonzero. Since, xi > bi, we have that max{xi \u2212bi, 0} > 0. Hence, A\nis invertible. The second claim follows directly from the fact that a lower-triangular matrix has all\nits eigenvalues on the main diagonal. This in turn follows from the \ufb01rst fact, since A \u2212\u03bbI can have\nlower rank only if \u03bb equals one of the diagonal values.\nProof of Theorem 1. For weight vectors w, b \u2208Rn and a \u2208Rd, consider the function c: Rn \u2192R,\nc(x) =\nX\nj=1\nwj max{\u27e8a, x\u27e9\u2212bj, 0}\nIt is easy to see that c can be expressed by a depth 2 network with ReLU activations.\nNow, \ufb01x a sample S = {z1, . . . , zn} of size n and a target vector y \u2208Rn. To prove the theorem, we\nneed to \ufb01nd weights a, b, w so that yi = c(zi) for all i \u2208{1, . . . , n}\nFirst, choose a and b such that with xi = \u27e8a, zi\u27e9we have the interleaving property b1 < x1 < b2 <\n\u00b7 \u00b7 \u00b7 < bn < xn. This is possible since all zi\u2019s are distinct. Next, consider the set of n equations in\nthe n unknowns w,\nyi = c(zi) ,\ni \u2208{1, . . . , n} .\nWe have c(zi) = Aw, where A = [max{xi \u2212bi, 0}]ij is the matrix we encountered in Lemma 1.\nWe chose a and b so that the lemma applies and hence A has full rank. We can now solve the linear\nsystem y = Aw to \ufb01nd suitable weights w.\nWhile the construction in the previous proof has inevitably high width given that the depth is 2, it is\npossible to trade width for depth. The construction is as follows. With the notation from the proof\nand assuming w.l.o.g. that x1, . . . , xn \u2208[0, 1], partition the interval [0, 1] into b disjoint intervals\nI1, . . . , Ib so that each interval Ij contains n/b points. At layer j, apply the construction from the\nproof to all points in Ij. This requires O(n/b) nodes at level j. This construction results in a circuit\nof width O(n/b) and depth b + 1 which so far has b outputs (one from each layer). It remains to\nimplement a multiplexer which selects one of the b outputs based on which interval a given input\nx falls into. This boils down to implementing one (approximate) indicator function fj for each\ninterval Ij and outputting Pb\nj=1 fj(x)oj, where oj is the output of layer j. This results in a single\noutput circuit. Implementing a single indicator function requires constant size and depth with ReLU\nactiviations. Hence, the \ufb01nal size of the construction is O(n) and the depth is b+c for some constant\nc. Setting k = b \u2212c gives the next corollary.\nCorollary 1. For every k \u22652, there exists neural network with ReLU activations of depth k,\nwidth O(n/k) and O(n + d) weights that can represent any function on a sample of size n in d\ndimensions.\nD\nRESULTS OF IMPLICIT REGULARIZATION FOR LINEAR MODELS\nTable 3: Generalizing with kernels. The test error associated with solving the kernel equation (3) on\nsmall benchmarks. Note that changing the preprocessing can signi\ufb01cantly change the resulting test\nerror.\ndata set\npre-processing\ntest error\nMNIST\nnone\n1.2%\nMNIST\ngabor \ufb01lters\n0.6%\nCIFAR10\nnone\n46%\nCIFAR10\nrandom conv-net\n17%\nTable 3 list the experiment results of linear models described in Section 5.\nE\nFITTING RANDOM LABELS WITH EXPLICIT REGULARIZATION\nIn Section 3, we showed that it is dif\ufb01cult to say that commonly used explicit regularizers count as\na fundamental phase change in the generalization capability of deep nets. In this appendix, we add\nsome experiments to investigate how explicit regularizers affect the ability to \ufb01t random labels.\nTable 4: Results on \ufb01tting random labels on the CIFAR10 dataset with weight decay and data aug-\nmentation.\nModel\nRegularizer\nTraining Accuracy\nInception\nWeight decay\n100%\nAlexnet\nFailed to converge\nMLP 3x512\n100%\nMLP 1x512\n99.21%\nInception\nRandom Cropping1\n99.93%\nAugmentation2\n99.28%\nFrom Table 4, we can see that for weight decay using the default coef\ufb01cient for each model, except\nAlexnet, all other models are still able to \ufb01t random labels. We also tested random cropping and\ndata augmentation with the Inception architecture. By changing the default weight decay factor\nfrom 0.95 to 0.999, and running for more epochs, we observe over\ufb01tting to random labels in both\ncases. It is expected to take longer to converge because data augmentation explodes the training set\nsize (though many samples are not i.i.d. any more).\n1In random cropping and augmentation, a new randomly modi\ufb01ed image is used in each epoch, but the\n(randomly assigned) labels are kept consistent for all the epochs. The \u201ctraining accuracy\u201d means a slightly\ndifferent thing here as the training set is different in each epoch. The global average of the online accuracy at\neach mini-batch on the augmented samples is reported here.\n2Data augmentation includes random left-right \ufb02ipping and random rotation up to 25 degrees.\n",
        "sentence": " While this seems similar to how deep neural networks excel at sheer memorization but still offer good test accuracy [45], the models in our setting are not necessarily memorizing all features \u2013 only the foreign keys.",
        "context": "2This conv-net is the Coates & Ng (2012) net, but with the \ufb01lters selected at random instead of with k-means.\ntraining data. Consequently, these models are in principle rich enough to memorize the training data.\n1. The effective capacity of neural networks is suf\ufb01cient for memorizing the entire data set.\n2. Even optimization on random labels remains easy. In fact, training time increases only by\na small constant factor compared with training on the true labels.\nThis situation poses a conceptual challenge to statistical learning theory as traditional measures of\nmodel complexity struggle to explain the generalization ability of large arti\ufb01cial neural networks."
    },
    {
        "title": "I/O-Efficient Statistical Computing with RIOT",
        "author": [
            "Y. Zhang"
        ],
        "venue": "In ICDE,",
        "citeRegEx": "46",
        "shortCiteRegEx": "46",
        "year": 2010,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " , [16, 11, 46]), how to scale ML (e.",
        "context": null
    }
]