[
    {
        "title": "Floresta sint\u00e1 (c) tica\u201d: a treebank for portuguese",
        "author": [
            "Susana Afonso",
            "Eckhard Bick",
            "Renato Haber",
            "Diana Santos."
        ],
        "venue": "Proc. of the Third Intern. Conf. on Language Resources and Evaluation (LREC), pages 1698\u20131703.",
        "citeRegEx": "Afonso et al\\.,? 2002",
        "shortCiteRegEx": "Afonso et al\\.",
        "year": 2002,
        "abstract": "",
        "full_text": "",
        "sentence": " 80% Portuguese Sint(c)tica\u2020 (Afonso et al., 2002) 75.",
        "context": null
    },
    {
        "title": "Speedread: A fast named entity recognition pipeline",
        "author": [
            "Rami Al-Rfou",
            "Steven Skiena"
        ],
        "venue": null,
        "citeRegEx": "Al.Rfou. and Skiena.,? \\Q2012\\E",
        "shortCiteRegEx": "Al.Rfou. and Skiena.",
        "year": 2012,
        "abstract": "Online content analysis employs algorithmic methods to identify entities in\nunstructured text. Both machine learning and knowledge-base approaches lie at\nthe foundation of contemporary named entities extraction systems. However, the\nprogress in deploying these approaches on web-scale has been been hampered by\nthe computational cost of NLP over massive text corpora. We present SpeedRead\n(SR), a named entity recognition pipeline that runs at least 10 times faster\nthan Stanford NLP pipeline. This pipeline consists of a high performance Penn\nTreebank- compliant tokenizer, close to state-of-art part-of-speech (POS)\ntagger and knowledge-based named entity recognizer.",
        "full_text": "SpeedRead: A Fast Named Entity Recognition Pipeline\nRami Al \u2212Rf ou\u2032\nSteven Skiena\nDepartment of Computer Science\nStony Brook University\nNY 11794, USA\n{ralrfou, skiena}@cs.stonybrook.edu\nABSTRACT\nOnline content analysis employs algorithmic methods to identify entities in unstructured text.\nBoth machine learning and knowledge-base approaches lie at the foundation of contemporary\nnamed entities extraction systems. However, the progress in deploying these approaches on\nweb-scale has been been hampered by the computational cost of NLP over massive text corpora.\nWe present SpeedRead (SR), a named entity recognition pipeline that runs at least 10 times\nfaster than Stanford NLP pipeline. This pipeline consists of a high performance Penn Treebank-\ncompliant tokenizer, close to state-of-art part-of-speech (POS) tagger and knowledge-based\nnamed entity recognizer.\nKEYWORDS: Tokenization, Part Of Speech, Named Entity Recognition, NLP pipelines.\narXiv:1301.2857v1  [cs.CL]  14 Jan 2013\n1\nIntroduction\nInformation retrieval (IR) systems rely on text as a main source of data, which is processed\nusing natural language processing (NLP) techniques to extract information and relations.\nNamed entity recognition is essential in information and event-extraction tasks. Since NLP\nalgorithms require computationally expensive operations, the NLP stages of an IR system\nbecome the bottleneck with regards to scalability (Pauls and Klein, 2011). Most of the relevant\nwork, conducted by researchers, was limited to small corpora of news and blogs because of\nthe limitation of the available algorithms in terms of speed. Most of the NLP pipelines use\npreviously computed features that are generated by other NLP tasks, which adds computational\ncost to the overall NLP pipeline. For example, named entity recognition and parsing need POS\ntags; co-reference resolution requires named entities. In effect, we anticipate lower speed for\nfuture tasks.\nA conservative estimate of a sample of the web news and articles can add up to terabytes of text.\nOn such scale, speed makes a huge difference. For example, considering the task of annotating\n10 TiBs of text with POS tags and named entities using a 20 CPU cores computer cluster would\ntake at least 4 months using the fastest NLP pipeline available for researchers, our calculations\nshow. Using our proposed NLP pipeline the time is reduced to a week.\nSeveral projects have tried to improve the speed by using code optimization. Figure 1a shows\nthat Stanford POS tagger has improved throughout the years, increasing its speed by more than\n10 times between 2006 and 2012. However, the current speed is twice slower than the SENNA\nPOS tagger.\n(a) POS taggers performance.\n(b) NER taggers performance.\nFigure 1: Performance of NLP pipelines through the years over POS and NER tagging. Stanford\nPOS tagger uses L3W model, its speed in 2006 is slow to be apparent in the graph. Stanford\ntagger uses CONLL 4 classes model. SENNA pipeline was \ufb01rst released in 2008\nIn this paper, we present a new NLP pipeline, SpeedRead, where we integrate global knowledge\nextracted from large corpora with machine learning algorithms to achieve high performance.\nFigures 1a and 1b show that our pipeline is 10 times faster than Stanford pipeline in both tasks:\nPOS tagging and NER tagging. Our design is built on two principles: (1) majority of the words\nhave unique annotations and tagging them is an easy task; (2) the features extracted for the\nfrequent words should be cached for later use by the classi\ufb01er. Both principles are simple and\nthey show how to bridge the large gap in performance between current systems and what can\nbe achieved.\nOur work makes the following contributions:\nPhase\nSpeedRead\nRelative Speed\nTokenization\n11.8\nPOS\n11.1\nNER\n13.9\nTOK+POS+NER\n18.0\nTable 1: SpeedRead relative speed to Stanford pipeline.\n\u2022 Exposing the performance limitations of the current NLP systems: We show that there is an\nalgorithmic room for improving performance, rather than relying solely on optimizing\nthe code.\n\u2022 High performance NLP pipeline that supports English tokenization, POS tagging and named\nentity recognition: Novel design decisions that are not taken by most of the available tools\nto explore new area of the accuracy-performance space. SpeedRead is available under an\nopen-source license. The code\u2019s organization is simple and it is written in Python for its\nreadability bene\ufb01ts. This makes it easier for others to contribute and hack.\n\u2022 Techniques to reduce computation needed for sequence tagging tasks: We distinguish between\nambiguous and non-ambiguous words. We use the larger copora to calculate the frequent\nwords and their frequent tags. We cache the extracted features of the most frequent\nwords to avoid unnecessary calculations and boost performance.\nFigure 2 shows the design of the SpeedRead pipeline. The \ufb01rst stage is tokenization followed by\nPOS tagging that is used as an essential feature to decide the boundaries of the named entities\u2019\nphrases. Once the phrases are detected, a classi\ufb01er decides to which category these named\nentities belong to.\nThis paper is structured as follows. In Section 2, we discuss the current NLP pipelines, available\nto researchers. Section 3 discusses SpeedRead tokenizer\u2019s architecture, speed and accuracy. In\nSection 4, we discuss the status of the current state-of-art POS taggers and describe SpeedRead\nnew POS tagger. Section 5 describes the architecture SpeedRead\u2019s named entity recognition\nphase. Finally, in Section 5.2, we discuss the status of the pipeline and the future improvements.\n1.1\nExperimental Setup\nAll the experiments presented in this paper were conducted on a single machine that has i7\nintel 920 processor running on 2.67GHz, the operating system used is Ubuntu 11.10. The time\nof execution is the sum of {sys, user} periods calculated by the Linux command time. The\nspeeds that are reported are calculated by averaging the execution time of \ufb01ve runs without\nconsidering any initialization times.\n2\nRelated Work\nThere are many available natural language processing packages available for researchers under\nopen source licenses or non-commercial ones. However, this section is not meant to review\nthe literature of named entity recognition research as this is already available in (Nadeau and\nSekine, 2007). We are trying to discuss the most popular solutions and the ones we think are\ninteresting to present.\nStanford NLP pipeline (Toutanova and Manning, 2000; Toutanova et al., 2003; Klein et al.,\n2003; Finkel et al., 2005; Lee et al., 2011) is one of the most popular and used NLP packages.\nRami\u2019s cat\non the mat.\nTokenization\nRami\n\u2019s\ncat\non\nthe\nmat\n.\nPOS\nTagging\nRami\nNNP\n\u2019s\nPOS\ncat\nNN\non\nIN\nthe\nDT\nmat\nNN\n.\n.\nNER\nChunking\nRami\nI\n\u2019s\nO\ncat\nO\non\nO\nthe\nO\nmat\nO\n.\nO\nNER\nClassi\ufb01cation\nRami\nPER\n\u2019s\nO\ncat\nO\non\nO\nthe\nO\nmat\nO\n.\nO\n1\nFigure 2: SpeadRead named entity recognition pipeline. First, tokenization split the words\ninto basic units to be processed in the later phases. POS tagging identi\ufb01es to which speech\ncategories words belong to. There are 45 part of speech category, we are mainly interested\nin nouns. Chunking identi\ufb01es the borders of phrases that make up the named entities. In the\nabove sentence, the named entity, Rami, is one word phrase. The last stage classi\ufb01es each\nphrase to one of four categories; Person, Location, Organization or Miscellaneous.\nThe pipeline is rich in features, \ufb02exible for tweaking and supports many natural languages.\nDespite being written in Java, there are many other programming language bindings that are\nmaintained by the community. The pipeline offers a tokenization, POS tagging, named entity\nrecognition, parsing and co-referencing resolution. The pipeline requirements of memory and\ncomputation are non-trivial. To accommodate the various computational resources, the pipeline\noffers several models for each task that vary in speed, memory consumption and accuracy. In\ngeneral, to achieve good performance in terms of speed, the user has to increase the memory\navailable to the pipeline to 1-3 GiBs and choose the faster but less accurate models.\nMore recent efforts include SENNA pipeline. Even though it lacks a proper tokenizer, it offers\nPOS tagging, named entity recognition, chunking, semantic role labeling(Collobert and Weston,\n2008) and parsing (Collobert, 2011). The pipeline has simple interface, high speed and small\nmemory footprint (less than 190MiB).\nSENNA builds on the idea of deep learning of extracting useful features from unlabeled text.\nThis unsupervised learning phase is done using auto-encoders and neural networks language\nmodels. It allows the pipeline to map words into another space of representation that has\nlower dimensionality. SENNA maps every word available in its 130 thousand word dictionary\nto a vector of 50 \ufb02oating numbers. These vectors are then merged into a sentence structure\nusing convolutional networks. The same architecture is then trained on different tasks using\nannotated text to generate different classi\ufb01ers. The big advantage of taking this approach is the\nlesser amount of engineering that it requires to solve multiple problems.\nNLTK (Bird et al., 2009) is a set of tools and interfaces to other NLP packages. Its simple APIs\nand good documentation makes it a favorable option for students and researchers. Written in\nPython, NLTK does not offer great speed or close to state-of-art accuracy with its tools. On the\nother hand, it is well maintained and has great community support.\nWikipediaMiner (Milne and Witten, 2008) detects conceptual words and named entities; it also\ndisambiguates the word senses. This approach can be modi\ufb01ed to detect only the words that\nrepresent entities, then using the disambiguated sense, it can decide which class the entity\nbelongs to. Its use of the Wikipedia interlinking information is a good example of the power\nof using knowledge-based systems. Our basic investigation shows that the current system\nneeds large chunks of memory to load all the interlinking graph of Wikipedia and it would be\nhard to optimize for speed. TAGME (Ferragina and Scaiella, 2010) is extending the work of\nWikipediaMiner to annotate short snippets of text. They are presenting a new disambiguation\nsystem that is faster and more accurate. Their system is much simpler and takes into account\nthe sparseness of the senses and the possible lack of unambiguous senses in short texts.\nStanford and SENNA performed the best in terms of speed and quality in our early investigation.\nTherefore, we will focus on both of them from now on as good representatives of a wide range\nof NLP packages.\n3\nTokenizer\nThe \ufb01rst task that an NLP pipeline has to deal with is tokenization and sentence segmentation\n(Webster and Kit, 1992). Tokenization target is to identify tokens in the text. Tokens are the\nbasic units which need not to be processed in the subsequent stages. Part of the complexity\nof tokenization comes from the fact that the de\ufb01nition of what a token is, depends on the\napplication that is being developed. Punctuation brings another level of ambiguity; commas\nand periods can play different roles in the text. For example, we do not need to split a number\nlike 1,000.54 into more units whereas we need to split a comma-separated list of words. On\nthe other hand, tokenization is important as it reduces the size of the vocabulary and improves\nthe accuracy of the taggers by producing similar vocabulary to the one used for training.\nAs many NLP tasks\u2019 gold standards are dependent on Penn Treebank(PTB), a corpus of annotated\ntext and parsed sentences taken from Wall Street Journal (WSJ), we opted for their tokenization\nscheme.\nSearching for good tokenizers, we limited our options to the ones that support Unicode. We\nbelieve that Unicode support is essential to any applications that depends on the pipeline. Stan-\nford tokenizer and Ucto (Gompel, 2012) projects offer almost Penn Treebank (PTB) compliant\ntokenizers plus other variations that are richer in terms of features.\nTable 2 shows that there is a substantial gap in performance between basic white space tokenizer\n(words are delimited by spaces or tabs and sentences are split by new line characters) and\nmore sophisticated tokenizers as Stanford tokenizer and Ucto. We observed that the Stanford\ntokenizer is 50 times slower than the baseline (WhiteSpace tokenizer), which motivated us to\nlook at the problem again.\nThe Stanford tokenizer is implemented using JFlex, a Java alternative to Flex. The tokenizer\nmatured over the years by adding more features and modes of operation which makes it harder\nfor us to modify. Ucto uses C++ to compile a list of regular expressions that passes over the\ntext multiple times.\nSpeedRead, like the Stanford tokenizer, uses a lexical analyzer to construct the tokenizer.\nHowever, we use different generating engine than the (F)lex family. SpeedRead depends on\nQuex (Schafer, 2012), a lexical analyzer generator, to generate our tokenizer. Quex makes\ndifferent trade-off decisions than the usual lex tools when it comes to the tokenizer\u2019s generation\ntime. Quex spends more time optimizing its internal NFA to produce a faster engine. While\ngenerating a tokenizer from a normal lex \ufb01le can take few minutes, Quex takes hours for the\nsame task. However, Quex supports Unicode in multiple ways and has similar description\nlanguage to lex, but is cleaner and more powerful. The extensive multiple mode support makes\nTokenizer\nWord/Second\nRelative Speed\nUcto\n185,500\n0.8\nPTB Sed Script\n214220\n0.96\nStanford\n222,176\n1.0\nSpeedRead\n2,626,183\n11.8\nWhiteSpace\n11,130,048\n50.0\nTable 2: Speed of different tokenizers measured as word/second; Every tokenizer generates\ndifferent number of tokens. For consistency, the original words count before tokenization\nused to calculate the speed. Words count is calculated using linux command wc. Execution\ntime includes both tokenization and sentence segmentation times with the exception that the\noriginal PTB Sed Script does not do sentence segmentation. Ucto\u2019s default con\ufb01guration is used.\nStanford tokenizer runs with strict PTB \ufb02ag turned on.\nit easy to write the lexical rules in understandable and organized way. All of that results in a\nfast C implementation of a Penn Treebank compliant tokenizer as Table 2 shows.\nAs a design decision, we did not support some features which we believe will not affect the\naccuracy of the tokenizer. Table 3 shows the features which are not implemented. While some\nof the features are easy to add as supporting contractions, others, involving abbreviations\nespecially U.S., prove to be complex (Gillick, 2009).\nFeature\nText\nPTB\nSpeedRead\nReordering\nJapan. ...\nJapan ... .\nJapan . ...\nPunctuation\nU.S.\"\nU.S. . \"\nU.S. \"\naddition\nContractions\ngimme\ngim me\ngimme\nTable 3: Some features that are not implemented in SpeedRead Tokenizer. Contractions that\ninvolves apstrophes are implemented in SpeedRead. For instance, can\u2019t will be tokenized to ca\nn\u2019t.\nTable 4 shows that the accuracy of our tokenizer is Penn Treebank compliant, despite the\nmissing features. Moreover, running SpeedRead and Stanford tokenizers over Reuters RCV1\ncorpus results in approximately 214, 215 million tokens consecutively.\n3.1\nSentence Segmentation\nWhile PTB offers a set of rules for tokenization, their tokenizer assumes that the sentences are\nalready segmented, which is done manually. SpeedRead\u2019s sentence segmentation uses the same\nrules that Stanford tokenizer uses. For instance, a period is an end of a sentence unless it is part\nof an acronym or abbreviation. The list of rules to detect those acronyms and abbreviations\nare taken from the Stanford tokenizer. Any quotations or brackets, that follow the end of\nthe sentence, will be part of that sentence. Running SpeedRead\u2019s sentence segmentation on\nReuters RCV1 generated 7.8 million sentences, while Stanford tokenizer generated 8.2 million\nsentences.\nTokenizer\nAccuracy\nPTB Sed Script\n100.0%\nStanford tokenizer\n99.7%\nSpeedRead\n99.0%\nWhite Space\n0.0%\nTable 4: Accuracy of the tokenizers over the \ufb01rst 1000 sentence in the Penn Treebank. The gold\nstandard was created by getting the tokenized text from the parse trees and manually segment\nthe original text into sentences according to the parse trees. Errors in differentiating between\nstarting and ending quotations are not considered. Not supporting MXPOST convention,\nreplacing brackets with special tokens, is not considered necessary.\n4\nPart of Speech Tagger (POS)\nEarlier work to solve the POS tagging problem relied on lexical and local features using\nmaximum entropy models (Toutanova and Manning, 2000). Later, more advanced models took\nadvantage of the context words and their predicted tags (Toutanova et al., 2003) to achieve\nhigher accuracy. As POS tagging is a sequence tagging problem, modeling the sequence into a\nMaximum Entropy Markov Model (MEMM) or Conditional Random Fields (CRF) model (to\ninfer the probability of the tags\u2019 sequences) seems to be the preferred option. The probability of\neach tag is computed using log-linear model with features that include large enough context\nwords and their already-computed tags. This transforms every instance of the problem into a\nlarge vector of features that is expensive to compute. Then the sequence of vectors are fed to\ngraphical model to compute the probability of each class, using the inference rules. The size of\nfeatures\u2019 vector and the inference computation are the same regardless of the complexity of the\nproblem.\nAlthough the previous algorithms are suf\ufb01cient to achieve satisfying accuracy, their computation\nrequirements are overkill for most of the cases faced by the algorithm. For example, the has a\nunique POS tag that never changes depending on its position in the sentence. Moreover, more\nand that are frequent enough in the English text that there is a need to cache their extracted\nfeatures.\n4.1\nAlgorithm\nSpeedRead takes advantage of the previous observations and tries to distinguish between\nambiguous and certain words. To understand such in\ufb02uences, we ran a Stanford POS tagger\n(left 3 words Model (L3W); trained on Wall Street Journal(WSJ), Sections 1-18) over a 1 GiB\nof news text to calculate the following dictionaries:\n\u2022 The most frequent POS tag of each token (Uni).\n\u2022 The most frequent POS tag of each token, given the previous POS tag (Bi).\n\u2022 The most frequent POS tag of each token, given the previous and next POS tags (Tri).\nUsing the above dictionaries to calculate the POS tag of a word, leads to various preci-\nsion/recall scores. (Lee et al., 2011) shows that using sieves is the solution to combine\nseveral rules/dictionaries. In a sieve algorithm, there is a set of rules that are cascaded after\neach other. The algorithm runs the rules from the highest in precision to the lowest. The\n\ufb01rst rule, matching the problem instance, returns its computed tag immediately. SpeedRead\nimplements few sieves in the following order:\n1. Certain tokens: Given a sentence, if the percentage frequency of the most frequent tag\nof a token is more than a threshold (in our work, 95%) then return that tag.\n2. Left and Right tags (Tri): For each token with unknown tag, return the most frequent\ntag, given the left and right POS tags if they are known.\n3. Left tags (Bi): For each token with unknown tag, return the most frequent tag, given the\nleft POS tag if it is known from the previous stages.\n4. Token tag (Uni) : For each token with unknown tag up to this stage, return the most\nfrequent tag.\n5. Backoff tag: If the token is unknown, use regular expression tagger to deduce the tag;\nthe regular expression tagger relies heavily on matching suf\ufb01xes.\n4.2\nResults\nTable 5 shows the performance of different algorithms running on different sections of PTB.\nStanford and SENNA models use sections 1-18, 19-21, 22-24 for training, development and\ntesting datasets, respectively. Despite the simplicity of our algorithm, it achieves relatively high\naccuracy on the various datasets available.\nApplying more context-aware rules, SpeedRead with sieves 1-5 (SR[Tri/Bi/Uni]) implemented,\nshows improvement in accuracy by around 2.85% compared to just using unigrams, SpeedRead\nwith sieves 1,4-5 (SR[Uni]). To be sure that our algorithm is robust enough and not over\ufb01tting\nthe dataset, we calculated the dictionaries again by running SENNA POS tagger(Collobert et al.,\n2011) over Reuters RCV1 corpus and the results were similar.\n````````````\nPOS Tagger\nSections\n19-21\n22-24\n1-24\nStanford Bidirectional\n97.27\n97.32\n98.16\nStanford L3W\n96.97\n96.89\n97.90\nSENNA\n97.81\n96.99\n97.68\nSR[Tri/Bi/Uni]\n96.73\n96.39\n96.66\nSR[Bi/Uni]\n96.06\n95.82\n96.03\nSR[Uni]\n93.73\n93.56\n93.70\nTable 5: Accuracy of different taggers on different sections of Penn Treebank. The \ufb01rst column\ncorresponds to the development set and the second to the testing set.\nTables 5 and 6 show the tradeoff between accuracy and speed. Stanford pipeline offers two\nmodels with different speeds and accuracies. Since Left 3 Words model (L3W) is the preferred\ntagger to use in practice, we chose it to be our reference in terms of speed. L3W model runs 18\ntimes faster than the state-of-art Bidirectional model and is only 0.4% less accurate. SpeedRead\npushes the speed by another factor of 11 with only 0.5% drop in accuracy. Since the speed of\nsome algorithms vary with the memory used, every algorithm was given enough memory that\nadding more memory will not affect its speed. The memory footprint is reported in the fourth\ncolumn of Table 6.\nPOS Tagger\nSpeed\nRelative\nMemory\nToken/Sec\nSpeed\nin MiB\nStanford Bi\n1389\n0.04\n900\nStanford L3W\n28,646\n1.00\n450\nSENNA\n34,385\n1.20\n150\nSR [Tri/Bi/Uni]\n318,368\n11.11\n600\nSR [Bi/Uni]\n397,501\n13.87\n250\nSR [Uni]\n564,977\n19.72\n120\nTable 6: Speed of different POS taggers. The \ufb01rst two taggers are Stanford taggers. The \ufb01rst\ntagger runs the Bidirectional(Bi) model and the second runs the Left 3 Words (L3w) model.\nSpeedRead has three variations\nFigure 3: Accumulative percentage of errors made by the most frequent mistagged words. The\ntotal number of words is around 2000, the graph lists only the most frequent 1000.\n4.3\nError Analysis\nThe most common errors are functional words, such as that, more, .. which have multiple roles\nin speech. This con\ufb01rms some of the conclusions reported by (Manning, 2011). Figure 3 shows\nthat less than 10% of mistagged words are responsible for slightly more than 50% of the errors.\nRegarding unknown words, the only part of the tagger that generalizes over unseen tokens is\nthe regular expression tagger. Regular expressions are not extensive enough to achieve high\naccuracy. Therefore, we are planning to implement another backoff phase for the frequent\nunseen words where we accumulate the sentences, containing these words, after suf\ufb01cient\namount of text is processed and then run Stanford/SENNA tagger over those sentences to\ncalculate the most common tag.\nTable 7 shows the confusion matrix of the most ambiguous tags; the less ambiguous tags are\nclustered into one category, O. One of the biggest sources of confusion in tagging is between\nadjectives (JJ) and nouns (NN). Proper nouns are the second source of errors as most of the\ncapitalized words will be mistakenly tagged as proper nouns while they are either adjectives\nor nouns. Such errors are the result of the weak logic implemented in the backoff tagger in\nSpeedRead, where regular expressions are applied in sequence returning the \ufb01rst match. Other\ntypes of errors are adverbs (RB) and propositions (IN). These errors are mainly because of the\nPPPPPPP\nP\nRef\nTest\nDT\nIN\nJJ\nNN\nNNP\nNNPS\nNNS\nRB\nVBD\nDT\n11094\n62\n3\n7\n3\n0\n0\n1\n0\nIN\n15\n13329\n9\n1\n0\n0\n0\n88\n0\nJJ\n1\n11\n7461\n257\n130\n2\n10\n65\n38\nNN\n1\n5\n288\n17196\n111\n0\n18\n11\n2\nNNP\n8\n13\n118\n109\n12585\n264\n31\n8\n0\nNNPS\n0\n0\n0\n0\n70\n81\n16\n0\n0\nNNS\n0\n0\n1\n23\n20\n42\n7922\n0\n0\nRB\n17\n281\n103\n23\n8\n0\n0\n3892\n0\nVBD\n0\n0\n8\n5\n4\n0\n0\n0\n4311\nVBG\n0\n0\n25\n104\n5\n0\n0\n0\n0\nO\n26\n163\n154\n172\n47\n4\n107\n67\n174\nTable 7: Confusion Matrix of the POS tags assigned by SpeedRead over the words of sections\n22-24 of PTB. O represents all the other not mentioned tags.\nambiguity of the functional words. Functional words need deeper understanding of discourse,\nsemantic and syntactic nature of the text. Taking into consideration the contexts around the\nwords improves the accuracy of tagging. However, trigrams are still small to be considered\nsuf\ufb01cient context for resolving all the ambiguities.\n5\nNamed Entity Recognition (NER)\nNamed entity recognition is essential to understand and extract information from text. Many\nefforts and several shared tasks, aiming to improve named entity recognition and classi\ufb01cation,\nhad been made; CONLL 2000/2003 (Tjong Kim Sang and De Meulder, 2003) are some of the\nshared tasks that addressed the named entity recognition task. We use CONLL 2003\u2019s de\ufb01nition\nof named entity recognition and classi\ufb01cation task. CONLL 2003 de\ufb01nes the chunk borders of\nan entity by using IOB tags, where I-TYPE means that the word is inside an entity, B-TYPE\nmeans a beginning of a new entity if the previous token is part of an entity of the same type\nand O for anything that is not part of an entity. For classi\ufb01cation, the task de\ufb01nes four different\ntypes: Person(PER), Organization(ORG), Location(LOC) and Miscellaneous(MISC) (See Figure\n4).\nWe split the task into two phases. The \ufb01rst is to detect the borders of the entity phrase. After\nthe entity chunk is detected, the second phase will classify each entity phrase to either a Person,\nLocation, Organization or Miscellaneous.\nColumbia/ORG is an American/Misc university located in New/LOC York/LOC.\nFigure 4: Annotated text after NER.\n5.1\nChunking\nWe rely on the POS tags of the phrase words to detect the phrase that constitute an entity. A\nword is considered to be a part of an entity: (1) if it is a demonym (our compiled list contains\n320 nationalities), (2) if one of the following conjunction words {&, de, of} appearing in\nmiddle of an entity phrase or, (3) if its POS tag is NNP(S) except if it belongs to one of these\nsets:\n\u2022 Week days and months and their abbreviations.\n\u2022 Sports (our compiled list contains 182 names).\n\u2022 Job and profession titles (our compiled list contains 314 title).\n\u2022 Single Capital letters.\nThese sets are compiled using freebase.\nCONLL dataset shows a strong correlation between POS tags NNP(S) and the words that are\npart of entities\u2019 phrases; 86% of the words that appear in entities\u2019 phrases have NNP(S) POS\ntags. The remaining words are distributed among different POS tags; 6.3% are demonyms.\nAdding the demonyms and proper nouns guarantee 92.3% coverage of the entities\u2019 words that\nappear in the dataset.\nUsing POS tags as main criteria to detect the entity phrases is expected, given the importance\nof the POS tags for the NER task. 14 out of 16 submitted paper to CONLL 2003 used POS tags\nas part of their feature set.\nThe behavior of the chunking algorithm is greedy as it tries to concatenate as many consecutive\nwords as possible into one entity phrase. A technical issue appears in detecting the borders\nof phrases when multiple entities appear after each other without non-entity separator. This\nsituation can be divided into two cases. Firstly, if the two consecutive entities are of the\nsame type. In this case, the chunking tag should be B-TYPE. Looking at the dataset, such tag\nappears less than 0.2% out of all the entities\u2019 tags. For example, in the original Stanford MEMM\nimplementation, the classi\ufb01er (Klein et al., 2003) generates IOB chunking tags while in the\nlater CRF models (Finkel et al., 2005) only IO chunking tags are generated. The second case is\nwhen the phrases are of different types. In the dataset, this case appears 248 times over 34834\nentities. Since both cases are not frequent enough to harm the performance of the classi\ufb01ers,\nSpeedRead does not recognize them.\n5.1.1\nResults\nTable 8 shows F1 score of the chunking phase using different taggers to generate the POS tags.\nThis score is calculated over the chunking tags of the words. I and B tags are considered as\none class while O is left as it is. It is clear from Table 8 that using better POS taggers does\nnot necessarily produce better results. The quality of SpeedRead POS tagging is suf\ufb01cient for\nthe chunking stage. SENNA and SpeedRead POS taggers work better for the detection phase\nbecause they are more aggressive, assigning the NNP tag to any capitalized word. On the other\nhand, Stanford tagger prefers to assign the tag of the lowered case shape of the word, if it is a\ncommon word.\n````````````\nPhase\nDataset\nTrain\nDev\nTest\nSR+SR POS\n94.24\n94.49\n93.12\nSR+Stanford POS L3W\n92.98\n93.37\n92.05\nSR+CONLL POS\n90.88\n90.82\n89.43\nSR+SENNA POS\n94.73\n95.07\n93.80\nTable 8: F1 scores of the chunking phase using different POS tags. F1 score is calculated over\ntokens and not entities.\n5.1.2\nError Analysis\nTable 9 shows the error cases that appears in the chunking phase. The most common class of\nerrors in the chunking phase is titles, such as {RESULTS, DIVISION, CONFERENCE, PTS, PCT}.\nThese words seem to confuse the POS tagger. Another source of confusion for the POS tagger is\nthe words {Women, Men}; such words appear in the name of sports so they get assigned NNP\ntag. As expected, all numbers that are part of entities are not detected. Conjunction words are\nthe second important class of errors. (Pawel and Robert, 2007) shows that conjunction words\nthat appear in middle of entities\u2019 phrases are hard to detect and need special classi\ufb01cation task.\nAs most of of occurrences are part of entities and the converse is true for and, we decided to\ninclude the former and exclude the later.\nWord\nPercentage\nType of error\nTitles\n22.7%\nDetected\nTitles\n4.9%\nMissed\nof\n2.6%\nDetected\n96, 95, 1000 ...\n2.6%\nMissed\nMen\n1.3%\nDetected\nWomen\n1.3%\nDetected\nand\n1.1%\nMissed\ncentral\n1.1%\nDetected\nTable 9: Most frequent errors in the chunking stage.\n5.2\nClassi\ufb01cation\nClassi\ufb01cation is a harder problem than just detecting an entity. For example, \u201cWest Bank\"\ncan belong to two classes, location and organization. Disambiguating the sense of an entity\ndepends on the context. For instance, \u201cMr. Green\" indicates that \u201cGreen\" is a person, while\n\u201caround Green\" points to a location. To classify an entity, we used a logistic regression clas-\nsi\ufb01er, sklearn (Scikit, 2011). The features we feed to the classi\ufb01er are two factors per type:\n\u03c6i j(T ypei, phrasej) and \u03c8i j(T ypei, contex t j). Context consists of two words that precede and\nfollow an entity phrase. To calculate these factors:\n\u03c6i j(T ypei, phrase) =\nn\nY\nk\nP(T ypei|wk)\n(1)\n\u03c8i j(T ypei, contex t = {wbe f ore, wa f ter}) = P(T ypei|wbe f ore) \u00d7 P(T ypei|wa f ter)\n(2)\nThe conditional probabilities of the types, given a speci\ufb01c word, are calculated using the\ndistribution of tags frequencies over words, retrieved from the annotated Reuters RCV1 corpus.\nSENNA NER tagger has been used to annotate the corpus.\nTable 10 indicates the importance of the classi\ufb01cation phase. First row shows that, given\nchunked input, the classi\ufb01cation phase is able to achieve close scores to the state-of- art\nclassi\ufb01ers. However, given the chunks generated by SpeedRead, the scores drop around 9.5%\nin F1 scores.\nXXXXXXXXXX\nPhase\nDataset\nTraining\nDev\nTest\nSR+Gold Chunks\n90.80\n91.98\n87.87\nSpeeRead\n82.05\n83.35\n78.28\nStanford\n99.28\n92.98\n89.03\nSENNA\n96.75\n97.24\n89.58\nTable 10: F1 scores calculated using conlleval.pl script for NER taggers. The table shows that\nSpeedRead F1 score is 10% below the sate-of-art achieved by SENNA.\nTo analyze the scores of the classi\ufb01cation phase further, Table 11 shows a confusion matrix over\nthe tags generated by SpeedRead. The errors that involve O are signs of chunking errors; there\nare 1158 chunking errors which exceed the total number of classi\ufb01cation errors, 849.\nPPPPPPP\nP\nRef\nTest\nLOC\nMISC\nORG\nPER\nO\nLOC\n1737\n34\n95\n36\n23\nMISC\n36\n660\n57\n52\n113\nORG\n323\n73\n1954\n37\n109\nPER\n26\n8\n72\n2632\n35\nO\n66\n248\n412\n152\n37445\nTable 11: Confusion matrix of the SpeadRead NER tags over the CONLL test dataset tokens.\nThe chunking errors contain more false positives than false negatives. The chunking algorithm\nis aggressive in considering every NNP(S) as part of an entity. That would be \ufb01ne if we had a\nperfect POS tagger. The reality that the POS tagger has hard time classifying uppercased words\nin titles and camel cased words that appear at the beginning of the sentence.\nOnce non-entity is considered part of an entity phrase, the classi\ufb01er has higher chance of\nclassifying it as an ORG than any other tag. The names of the organizations contain a mix of\nlocations and persons\u2019 names, forcing the classi\ufb01er to consider any long or mix of words as an\norganization entity. That appears more clearly in the second most frequent category of errors.\n323 words in organizations entities\u2019 names were classi\ufb01ed as locations. This could be explained\nby the fact that many companies and banks name themselves after country names and their\nlocations. For example, \u201cBank of England\" could be classi\ufb01ed as a location because of the strong\nassociation between England and the tag location.\nTable 12 shows that Stanford pipeline has a high cost for the accuracy achieved by the classi\ufb01er.\nSENNA achieves close accuracy with twice the speed and less memory usage. SpeedRead takes\nanother approach by focusing on speed. We are able to speed up the pipeline to the factor of\n13. SpeedRead\u2019s memory footprint is half the memory consumed by the Stanford pipeline. Even\nthough SpeedRead\u2019s accuracy is not close to the state-of-art, it still achieves 18% increase over\nthe CONLL 2003 baseline. Moreover, adapting the pipeline to new domains could be easily done\nby integrating other knowledge base sources as freebase or Wikipedia. SENNA and SpeedRead\nare able to calculate POS tags at the end of the NER phase without extra computation while that\nis not true of Stanford pipeline standalone NER application. Using Stanford corenlp pipeline\ndoes not guarantee better execution time.\nNER Tagger\nToken/Sec\nRelative\nMemory\nSpeed\nMiB\nStanford\n11,612\n1.00\n1900\nSENNA\n18,579\n2.13\n150\nSpeedRead\n153,194\n13.9\n950\nTable 12: Speed of different NER taggers. SpeedRead is faster by 13.9 times using half the\nmemory consumed by Stanford.\nConclusion and Future Work\nOur success in implementing a high performance tokenizer and POS tagger shows that it is\npossible to use simple algorithms and conditional probabilities, accumulated from a large\ncorpora, to achieve good classi\ufb01cation and chunking accuracies.\nThis could lead to a general technique of approximating any sequence tagging problem using\nsuf\ufb01ciently large dictionaries of conditional probabilities of contexts and inputs. This approx-\nimation has the advantage of speeding up the calculations and opens the horizon for new\napplications where scalability matters.\nExpanding this approach to other languages depends on the availability of other high accurate\ntaggers in these languages. We are looking to infer these conditional probabilities from a global\nknowledge base as freebase or the interlinking graph of Wikipedia.\nSpeedRead is available under GPLv3 license and it is available to download from www.textmap.\norg/speedread. We anticipate that it will be useful to large spectrum of named entity\nrecognition applications.\nReferences\nBird, S., Klein, E., and Loper, E. (2009). Natural Language Processing with Python. O\u2019Reilly\nMedia.\nCollobert, R. (2011). Deep learning for ef\ufb01cient discriminative parsing. In International\nConference on Arti\ufb01cial Intelligence and Statistics.\nCollobert, R. and Weston, J. (2008). A uni\ufb01ed architecture for natural language process-\ning: deep neural networks with multitask learning. In Proceedings of the 25th international\nconference on Machine learning, ICML \u201908, pages 160\u2013167, New York, NY, USA. ACM.\nCollobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., and Kuksa, P. (2011).\nNatural language processing (almost) from scratch. Journal of Machine Learning Research,\n12:2493\u20132537.\nFerragina, P. and Scaiella, U. (2010). Tagme: on-the-\ufb02y annotation of short text fragments (by\nwikipedia entities). In Proceedings of the 19th ACM international conference on Information and\nknowledge management, CIKM \u201910, pages 1625\u20131628, New York, NY, USA. ACM.\nFinkel, J. R., Grenager, T., and Manning, C. (2005). Incorporating non-local information into\ninformation extraction systems by gibbs sampling. In In ACL, pages 363\u2013370.\nGillick, D. (2009). Sentence boundary detection and the problem with the u.s. In Proceedings\nof Human Language Technologies: The 2009 Annual Conference of the North American Chapter of\nthe Association for Computational Linguistics, Companion Volume: Short Papers, NAACL-Short\n\u201909, pages 241\u2013244, Stroudsburg, PA, USA. Association for Computational Linguistics.\nGompel, M. V. (2012). Ucto: Unicode tokenizer. http://ilk.uvt.nl/ucto.\nKlein, D., Smarr, J., Nguyen, H., and Manning, C. D. (2003). Named entity recognition with\ncharacter-level models. In Daelemans, W. and Osborne, M., editors, Proceedings of CoNLL-2003,\npages 180\u2013183. Edmonton, Canada.\nLee, H., Peirsman, Y., Chang, A., Chambers, N., Surdeanu, M., and Jurafsky, D. (2011).\nStanford\u2019s multi-pass sieve coreference resolution system at the conll-2011 shared task. In\nProceedings of the CoNLL-2011 Shared Task.\nManning, C. D. (2011). Part-of-speech tagging from 97% to 100%: Is it time for some\nlinguistics? In Gelbukh, A. F., editor, CICLing (1), volume 6608 of Lecture Notes in Computer\nScience, pages 171\u2013189. Springer.\nMilne, D. and Witten, I. H. (2008). An effective, low-cost measure of semantic relatedness\nobtained from wikipedia links. In In Proceedings of AAAI 2008.\nNadeau, D. and Sekine, S. (2007). A survey of named entity recognition and classi\ufb01cation.\nLingvisticae Investigationes, 30(1):3\u201326.\nPauls, A. and Klein, D. (2011). Faster and smaller n-gram language models. In Lin, D.,\nMatsumoto, Y., and Mihalcea, R., editors, ACL, pages 258\u2013267. The Association for Computer\nLinguistics.\nPawel, M. and Robert, D. (2007). Handling conjunctions in named entities. Lingvisticae\nInvestigationes, 30(1):49\u201368.\nSchafer, F.-R. (2012).\nQuex - fast universal lexical analyzer generator.\nhttp://quex.\nsourceforge.net.\nScikit, S.-l. D. (2011). Scikit-learn: Machine learning in python. Journal of Machine Learning\nResearch, 12:2825\u20132830.\nTjong Kim Sang, E. F. and De Meulder, F. (2003). Introduction to the conll-2003 shared task:\nLanguage-independent named entity recognition. In Daelemans, W. and Osborne, M., editors,\nProceedings of CoNLL-2003, pages 142\u2013147. Edmonton, Canada.\nToutanova, K., Klein, D., Manning, C. D., and Singer, Y. (2003). Feature-rich part-of-speech\ntagging with a cyclic dependency network. In Proceedings of the 2003 Conference of the\nNorth American Chapter of the Association for Computational Linguistics on Human Language\nTechnology - Volume 1, NAACL \u201903, pages 173\u2013180, Stroudsburg, PA, USA. Association for\nComputational Linguistics.\nToutanova, K. and Manning, C. D. (2000).\nEnriching the knowledge sources used in a\nmaximum entropy part-of-speech tagger. In Proceedings of the 2000 Joint SIGDAT conference\non Empirical methods in natural language processing and very large corpora: held in conjunction\nwith the 38th Annual Meeting of the Association for Computational Linguistics - Volume 13,\nEMNLP \u201900, pages 63\u201370, Stroudsburg, PA, USA. Association for Computational Linguistics.\nWebster, J. J. and Kit, C. (1992). Tokenization as the initial phase in nlp. In Proceedings of\nthe 14th conference on Computational linguistics - Volume 4, COLING \u201992, pages 1106\u20131110,\nStroudsburg, PA, USA. Association for Computational Linguistics.\n",
        "sentence": " In addition to pure performance, the system has a faster execution speed than comparable NLP pipelines (Al-Rfou\u2019 and Skiena, 2012).",
        "context": "In this paper, we present a new NLP pipeline, SpeedRead, where we integrate global knowledge\nextracted from large corpora with machine learning algorithms to achieve high performance.\nsystem that is faster and more accurate. Their system is much simpler and takes into account\nthe sparseness of the senses and the possible lack of unambiguous senses in short texts.\nalgorithmic room for improving performance, rather than relying solely on optimizing\nthe code.\n\u2022 High performance NLP pipeline that supports English tokenization, POS tagging and named"
    },
    {
        "title": "Prague Dependency Treebank 2.5 \u2013 a revisited version of PDT 2.0",
        "author": [
            "Eduard Bej\u010dek",
            "Jarmila Panevov\u00e1",
            "Jan Popelka",
            "Pavel Stra\u0148\u00e1k",
            "Magda \u0160ev\u010d\u0131\u0301kov\u00e1",
            "Jan \u0160t\u011bp\u00e1nek",
            "Zden\u011bk \u017dabokrtsk\u00fd"
        ],
        "venue": "In Proceedings of COLING",
        "citeRegEx": "Bej\u010dek et al\\.,? \\Q2012\\E",
        "shortCiteRegEx": "Bej\u010dek et al\\.",
        "year": 2012,
        "abstract": "",
        "full_text": "",
        "sentence": " 5 (Bej\u010dek et al., 2012) 71.",
        "context": null
    },
    {
        "title": "Adaptive importance sampling to accelerate training of a neural probabilistic language model",
        "author": [
            "Yoshua Bengio",
            "J-S Senecal."
        ],
        "venue": "Neural Networks, IEEE Transactions on, 19(4):713\u2013722.",
        "citeRegEx": "Bengio and Senecal.,? 2008",
        "shortCiteRegEx": "Bengio and Senecal.",
        "year": 2008,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " mating the error by sampling (Bengio and Senecal, 2008).",
        "context": null
    },
    {
        "title": "Neural probabilistic language models",
        "author": [
            "Y. Bengio",
            "H. Schwenk",
            "J.S. Sen\u00e9cal",
            "F. Morin",
            "J.L. Gauvain."
        ],
        "venue": "Innovations in Machine Learning, pages 137\u2013 186.",
        "citeRegEx": "Bengio et al\\.,? 2006",
        "shortCiteRegEx": "Bengio et al\\.",
        "year": 2006,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Recent developments have led to state-of-art performance in several NLP tasks such as language modeling (Bengio et al., 2006; Mikolov et al., 2010), and syntactic tasks such as sequence tagging (Collobert et al. icant amount of computational resources (Bengio et al., 2006; Dean et al., 2012).",
        "context": null
    },
    {
        "title": "Curriculum learning",
        "author": [
            "Y. Bengio",
            "J. Louradour",
            "R. Collobert",
            "J. Weston."
        ],
        "venue": "International Conference on Machine Learning, ICML.",
        "citeRegEx": "Bengio et al\\.,? 2009",
        "shortCiteRegEx": "Bengio et al\\.",
        "year": 2009,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " In our work, we start from the example construction method outlined in (Bengio et al., 2009).",
        "context": null
    },
    {
        "title": "Theano: a CPU and GPU math expression compiler",
        "author": [
            "James Bergstra",
            "Olivier Breuleux",
            "Fr\u00e9d\u00e9ric Bastien",
            "Pascal Lamblin",
            "Razvan Pascanu",
            "Guillaume Desjardins",
            "Joseph Turian",
            "David Warde-Farley",
            "Yoshua Bengio."
        ],
        "venue": "Proceedings",
        "citeRegEx": "Bergstra et al\\.,? 2010",
        "shortCiteRegEx": "Bergstra et al\\.",
        "year": 2010,
        "abstract": "",
        "full_text": "",
        "sentence": " \u2022 Efficient implementation - Training these models was made possible by our contributions to Theano (machine learning library (Bergstra et al., 2010)). For our experiments, we build a model as the one described in Section 3 using Theano (Bergstra et al., 2010).",
        "context": null
    },
    {
        "title": "Domain adaptation with structural correspondence learning",
        "author": [
            "John Blitzer",
            "Ryan McDonald",
            "Fernando Pereira."
        ],
        "venue": "Conference on Empirical Methods in Natural Language Processing, Sydney, Australia.",
        "citeRegEx": "Blitzer et al\\.,? 2006",
        "shortCiteRegEx": "Blitzer et al\\.",
        "year": 2006,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " This makes them hard to port to new languages and tasks (Blitzer et al., 2006).",
        "context": null
    },
    {
        "title": "Stochastic gradient learning in neural networks",
        "author": [
            "L\u00e9on Bottou."
        ],
        "venue": "Proceedings of Neuro-N\u0131\u0302mes 91, Nimes, France. EC2.",
        "citeRegEx": "Bottou.,? 1991",
        "shortCiteRegEx": "Bottou.",
        "year": 1991,
        "abstract": "",
        "full_text": "",
        "sentence": " scent (Bottou, 1991), and update the parameters which contributed to the error using backpropagation (Rumelhart et al.",
        "context": null
    },
    {
        "title": "The tiger treebank",
        "author": [
            "Sabine Brants",
            "Stefanie Dipper",
            "Silvia Hansen",
            "Wolfgang Lezius",
            "George Smith."
        ],
        "venue": "IN PROCEEDINGS OF THE WORKSHOP ON TREEBANKS AND LINGUISTIC THEORIES, pages 24\u201341.",
        "citeRegEx": "Brants et al\\.,? 2002",
        "shortCiteRegEx": "Brants et al\\.",
        "year": 2002,
        "abstract": "",
        "full_text": "",
        "sentence": " Language Source Test TnT Unknown Known All German Tiger\u2020 (Brants et al., 2002) 89.",
        "context": null
    },
    {
        "title": "Tnt: a statistical part-ofspeech tagger",
        "author": [
            "Thorsten Brants."
        ],
        "venue": "Proceedings of the sixth conference on Applied natural language processing, pages 224\u2013231. Association for Computational Linguistics.",
        "citeRegEx": "Brants.,? 2000",
        "shortCiteRegEx": "Brants.",
        "year": 2000,
        "abstract": "",
        "full_text": "",
        "sentence": " We compare our results to a similar experiment conducted in their work, where they trained a TnT tagger (Brants, 2000) on several treebanks.",
        "context": null
    },
    {
        "title": "Class-based n-gram models of natural language",
        "author": [
            "Peter F Brown",
            "Peter V Desouza",
            "Robert L Mercer",
            "Vincent J Della Pietra",
            "Jenifer C Lai."
        ],
        "venue": "Computational linguistics, 18(4):467\u2013479.",
        "citeRegEx": "Brown et al\\.,? 1992",
        "shortCiteRegEx": "Brown et al\\.",
        "year": 1992,
        "abstract": "",
        "full_text": "",
        "sentence": " Word clustering has been used to learn classes of words that have similar semantic features to improve language modeling (Brown et al., 1992) and knowledge transfer across languages (T\u00e4ckstr\u00f6m et al.",
        "context": null
    },
    {
        "title": "Marginalized denoising autoencoders for domain adaptation",
        "author": [
            "Minmin Chen",
            "Zhixiang Xu",
            "Kilian Weinberger",
            "Fei Sha."
        ],
        "venue": "John Langford and",
        "citeRegEx": "Chen et al\\.,? 2012",
        "shortCiteRegEx": "Chen et al\\.",
        "year": 2012,
        "abstract": "Stacked denoising autoencoders (SDAs) have been successfully used to learn\nnew representations for domain adaptation. Recently, they have attained record\naccuracy on standard benchmark tasks of sentiment analysis across different\ntext domains. SDAs learn robust data representations by reconstruction,\nrecovering original features from data that are artificially corrupted with\nnoise. In this paper, we propose marginalized SDA (mSDA) that addresses two\ncrucial limitations of SDAs: high computational cost and lack of scalability to\nhigh-dimensional features. In contrast to SDAs, our approach of mSDA\nmarginalizes noise and thus does not require stochastic gradient descent or\nother optimization algorithms to learn parameters ? in fact, they are computed\nin closed-form. Consequently, mSDA, which can be implemented in only 20 lines\nof MATLAB^{TM}, significantly speeds up SDAs by two orders of magnitude.\nFurthermore, the representations learnt by mSDA are as effective as the\ntraditional SDAs, attaining almost identical accuracies in benchmark tasks.",
        "full_text": "Marginalized Denoising Autoencoders for Domain Adaptation\nMinmin Chen\nMC15@CSE.WUSTL.EDU\nZhixiang (Eddie) Xu\nXUZX@CSE.WUSTL.EDU\nKilian Q. Weinberger\nKILIAN@WUSTL.EDU\nWashington University, St. Louis, MO 63130, USA\nFei Sha\nFEISHA@USC.EDU\nU. of Southern California, Los Angeles, CA 90089, USA\nAbstract\nStacked denoising autoencoders (SDAs) have\nbeen successfully used to learn new represen-\ntations for domain adaptation.\nRecently, they\nhave attained record accuracy on standard bench-\nmark tasks of sentiment analysis across differ-\nent text domains. SDAs learn robust data repre-\nsentations by reconstruction, recovering original\nfeatures from data that are arti\ufb01cially corrupted\nwith noise. In this paper, we propose marginal-\nized SDA (mSDA) that addresses two crucial lim-\nitations of SDAs: high computational cost and\nlack of scalability to high-dimensional features.\nIn contrast to SDAs, our approach of mSDA\nmarginalizes noise and thus does not require\nstochastic gradient descent or other optimization\nalgorithms to learn parameters \u2014 in fact, they are\ncomputed in closed-form. Consequently, mSDA,\nwhich can be implemented in only 20 lines of\nMATLABTM, signi\ufb01cantly speeds up SDAs by\ntwo orders of magnitude. Furthermore, the rep-\nresentations learnt by mSDA are as effective as\nthe traditional SDAs, attaining almost identical\naccuracies in benchmark tasks.\n1. Introduction\nDomain adaptation (Ben-David et al., 2009; Huang et al.,\n2007; Weinberger et al., 2009; Xue et al., 2008) aims to\ngeneralize a classi\ufb01er that is trained on a source domain, for\nwhich typically plenty of training data is available, to a tar-\nget domain, for which data is scarce. Cross-domain gener-\nalization is important in many application areas of machine\nlearning, where such an imbalance of training data may oc-\nAppearing in Proceedings of the 29 th International Conference\non Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright\n2012 by the author(s)/owner(s).\ncur. Examples are computational biology (Liu et al., 2008),\nnatural language processing (Daume III, 2007; McClosky\net al., 2006) and computer vision (Saenko et al., 2010).\nData in the source and the target are often distributed differ-\nently. This presents a major obstacle in adapting predictive\nmodels. Recent work has investigated several techniques\nfor alleviating the difference: instance reweighting (Huang\net al., 2007; Mansour et al., 2009), sub-sampling from both\ndomains (Chen et al., 2011b) and learning joint target and\nsource feature representations (Blitzer et al., 2006; Glorot\net al., 2011; Xue et al., 2008).\nRecently, Glorot et al. (2011) proposed a new approach that\nfalls into the third category. The authors propose to learn\nrobust feature representations with stacked denoising au-\ntoencoders (SDA) (Vincent et al., 2008). Denoising autoen-\ncoders are one-layer neural networks that are optimized to\nreconstruct input data from partial and random corruption.\nThese denoisers can be stacked into deep learning archi-\ntectures. The outputs of their intermediate layers are then\nused as input features for SVMs (Lee et al., 2009). Glorot\net al. (2011) demonstrate that using SDA-learned features\nin conjunction with linear SVM classi\ufb01ers yields record\nperformance on the benchmark tasks of sentiment analysis\nacross different product domains (Blitzer et al., 2006).\nDespite their remarkable and promising results, SDAs are\nlimited by their high computational cost. They are signif-\nicantly slower to train than competing algorithms (Blitzer\net al., 2006; Chen et al., 2011a; Xue et al., 2008), primarily\nbecause of their reliance on iterative and numerical opti-\nmization to learn model parameters. The challenge is fur-\nther compounded by the dimensionality of the input data\nand the need for computationally intensive model selection\nprocedures to tune hyperparameters. Consequently, even\na highly optimized implementation (Bergstra et al., 2010)\nmay require hours (even days) of training time.\nIn this paper, we address this challenge with a variant of\nmSDA for Domain Adaptation\nSDA. The proposed method, which we refer to as marginal-\nized Stacked Denoising Autoencoder (mSDA), adopts the\ngreedy layer-by-layer training of SDAs. However, a cru-\ncial difference is that we use linear denoisers as the basic\nbuilding blocks. The key observation is that, in this set-\nting, the random feature corruption can be marginalized\nout. Conceptually, this is equivalent to training the mod-\nels with an in\ufb01nitely large number of corrupted input data.\nFitting models on such a scale would be impossible for the\nconventional SDAs, which often rely on stochastic gradient\ndescent, and need to sweep through all the training data.\nOur contributions are summarized as follows: i) we con-\ntribute to deep learning by demonstrating that linear de-\nnoisers can be used as building blocks for learning feature\nrepresentations. ii) we show that linearity can signi\ufb01cantly\nsimplify parameter estimation \u2014 our approach results in\nclosed-form solutions for the optimal parameters. iii) we\nevaluate our approach rigorously on established domain\nadaptation benchmark data sets and compare with several\ncompeting state-of-the-art algorithms. We show that the\nclassi\ufb01cation performance of mSDA matches that of SDA\nacross our benchmark data sets, while achieving tremen-\ndous speedups during training time (reducing training from\nup to 2 days for SDA to a few minutes with mSDA).\n2. Notation and Background\nWe follow the setup of Glorot et al. (2011) and focus\non the problem of domain adaptation throughout this pa-\nper.\nWe assume that our data originates from two do-\nmains, source S and target T. From the source domain\nS, we sample data DS = {x1, . . . , xns} \u2282Rd\nwith\nknown labels LS = {y1, . . . , yns}, whereas from the tar-\nget domain we are only able to sample data without labels\nDT = {xns+1, . . . xn} \u2282Rd. We do not assume that both\ndomains use identical features and we pad all input vectors\nwith zeros to make both domains be of equal dimensional-\nity d. Our goal is to learn a classi\ufb01er h\u2208H with the help of\nthe labeled set DS and the unlabeled set DT , to accurately\npredict the labels of data from the target domain T. In prac-\ntice (and as we show in section 5) it is straightforward to\nalso extend this framework to multiple target domains.\nStacked Denoising Autoencoder. Various forms of au-\ntoencoders have been developed in the deep learning lit-\nerature (Rumelhart et al., 1986; Baldi & Hornik, 1989;\nKavukcuoglu et al., 2009; Lee et al., 2009; Vincent et al.,\n2008; Rifai et al., 2011). In its simplest form, an autoen-\ncoder has two components, an encoder h(\u00b7) maps an input\nx \u2208Rd to some hidden representation h(x) \u2208Rdh, and\na decoder g(\u00b7) maps this hidden representation back to a\nreconstructed version of x, such that g(h(x))\u2248x. The pa-\nrameters of the autoencoders are learned to minimize the\nreconstruction error, measured by some loss \u2113(x, g(h(x))).\nChoices for the loss include squared error or Kullback-\nLeibler divergence when the feature values are in [0, 1].\nDenoising Autoencoders (DAs) incorporate a slight modi-\n\ufb01cation to this setup and corrupt the inputs before mapping\nthem into the hidden representation. They are trained to\nreconstruct (or denoise) the original input x from its cor-\nrupted version \u02dcx by minimizing \u2113(x, g(h(\u02dcx))).\nTypical\nchoices of corruption include additive isotropic Gaussian\nnoise or binary masking noise. In this work, as in Vin-\ncent et al. (2008), we use the latter and set a fraction of the\nfeatures of each input to zero. This is a natural choice for\nbag-of-word representations of texts, where typical class-\nspeci\ufb01c words can be missing due to the writing style of\nthe author or differences between train and test domains.\nThe stacked denoising autoencoder (SDA) of Vincent et al.\n(2008) stacks several DAs together to create higher-level\nrepresentations, by feeding the hidden representation of the\ntth DA as input into the (t + 1)th DA. The training is per-\nformed greedily, layer by layer.\nFeature Generation. Many researchers have seen autoen-\ncoders as a powerful tool for automatic discovery and ex-\ntraction of nonlinear features.\nFor example, Lee et al.\n(2009) demonstrate that the hidden representations com-\nputed by either all or partial layers of a convolutional neural\nnetwork (CNN) make excellent features for classi\ufb01cation\nwith SVMs. The pre-processing with a CNN improves the\ngeneralization by increasing robustness against noise and\nlabel-invariant transformations.\nGlorot et al. (2011) successfully apply SDAs to extract fea-\ntures for domain adaptation in document sentiment anal-\nysis. The authors train an SDA to reconstruct the input\nvectors (ignoring the labels) on the union of the source and\ntarget data. A classi\ufb01er (e.g. a linear SVM) trained on the\nresulting feature representation h(x) transfers signi\ufb01cantly\nbetter from source to target than one trained on x directly.\nSimilar to CNNs, SDAs also combine correlated input di-\nmensions, as they reconstruct removed feature values from\nuncorrupted features. It is shown that SDAs are able to\ndisentangle hidden factors which explain the variations in\nthe input data, and automatically group features in accor-\ndance with their relatedness to these factors (Glorot et al.,\n2011). This helps transfer across domains as these generic\nconcepts are invariant to domain-speci\ufb01c vocabularies.\nAs an intuitive example, imagine that we classify product\nreviews according to their sentiments. The source data con-\nsists of book reviews, the target of kitchen appliances. A\nclassi\ufb01er trained on the original source never encounters\nthe bigram \u201cenergy ef\ufb01cient\u201d during training and therefore\nassigns zero weight to it. In the learned SDA represen-\ntation, the bigram \u201cenergy ef\ufb01cient\u201d would tend to recon-\nstruct, and be reconstructed by, co-occurring features, typ-\nmSDA for Domain Adaptation\nically of similar sentiment (e.g. \u201cgood\u201d or \u201clove\u201d). Hence,\nthe source-trained classi\ufb01er can assign weights even to fea-\ntures that never occur in its original domain representation,\nwhich are \u201cre-constructed\u201d by the SDA.\nAlthough SDAs generate excellent features for domain\nadaptation, they have several drawbacks: 1. Training with\n(stochastic) gradient descent is slow and hard to paral-\nlelize (although a dense-matrix GPU implementation ex-\nists (Bergstra et al., 2010) and an implementation based\non reconstruction sampling exists (Dauphin Y., 2011) for\nsparse inputs); 2.\nThere are several hyper-parameters\n(learning rate, number of epochs, noise ratio, mini-batch\nsize and network structure), which need to be set by cross\nvalidation \u2014 this is particularly expensive as each individ-\nual run can take several hours; 3. The optimization is in-\nherently non-convex and dependent on its initialization.\n3. SDA with Marginalized Corruption\nIn this section we introduce a modi\ufb01ed version of SDA,\nwhich preserves its strong feature learning capabilities, and\nalleviates the concerns mentioned above through speedups\nof several orders of magnitudes, fewer meta-parameters,\nfaster model-selection and layer-wise convexity.\n3.1. Single-layer Denoiser\nThe basic building block of our framework is a one-layer\ndenoising autoencoder. We take the inputs x1, . . . , xn from\nD=DS \u222aDT and corrupt them by random feature removal\n\u2014 each feature is set to 0 with probability p\u22650. Let us\ndenote the corrupted version of xi as \u02dcxi. As opposed to\nthe two-level encoder and decoder in SDA, we reconstruct\nthe corrupted inputs with a single mapping W : Rd \u2192Rd,\nthat minimizes the squared reconstruction loss\n1\n2n\nn\nX\ni=1\n\u2225xi \u2212W\u02dcxi\u22252.\n(1)\nTo simplify notation, we assume that a constant feature is\nadded to the input, xi = [xi; 1], and an appropriate bias\nis incorporated within the mapping W = [W, b]. The\nconstant feature is never corrupted.\nThe solution to (1) depends on which features of each input\nare randomly corrupted. To lower the variance, we perform\nmultiple passes over the training set, each time with dif-\nferent corruption. We solve for the W that minimizes the\noverall squared loss\nLsq(W) =\n1\n2mn\nm\nX\nj=1\nn\nX\ni=1\n\u2225xi \u2212W\u02dcxi,j\u22252,\n(2)\nwhere \u02dcxi,j represents the jth corrupted version of the orig-\ninal input xi.\nAlgorithm 1 mDA in MATLABTM.\nfunction [W,h]=mDA(X,p);\nX=[X;ones(1,size(X,2))];\nd=size(X,1);\nq=[ones(d-1,1).*(1-p); 1];\nS=X*X\u2019;\nQ=S.*(q*q\u2019);\nQ(1:d+1:end)=q.*diag(S);\nP=S.*repmat(q\u2019,d,1);\nW=P(1:end-1,:)/(Q+1e-5*eye(d));\nh=tanh(W*X);\nLet us de\ufb01ne the design matrix X = [x1, . . . , xn] \u2208Rd\u00d7n\nand its m-times repeated version as X = [X, . . . , X]. Fur-\nther, we denote the corrupted version of X as \u02dcX. With this\nnotation, the loss in eq. (1) reduces to\nLsq(W)=\n1\n2nmtr\n\u0014\u0010\nX \u2212W eX\n\u0011\u22a4\u0010\nX \u2212W eX\n\u0011\u0015\n.\n(3)\nThe solution to (3) can be expressed as the well-known\nclosed-form solution for ordinary least squares (Bishop,\n2006):\nW = PQ\u22121 with Q = eX eX\u22a4and P = X eX\u22a4.\n(4)\n(In practice this can be computed as a system of linear\nequations, without the costly matrix inversion.)\n3.2. Marginalized Denoising Autoencoder\nThe larger m is, the more corruptions we average over. Ide-\nally we would like m \u2192\u221e, effectively using in\ufb01nitely\nmany copies of noisy data to compute the denoising trans-\nformation W.\nBy the weak law of large numbers, the matrices P and Q,\nas de\ufb01ned in (3), converge to their expected values as m\nbecomes very large. If we are interested in the limit case,\nwhere m\u2192\u221e, we can derive the expectations of Q and P,\nand express the corresponding mapping W as\nW = E[P]E[Q]\u22121.\n(5)\nIn the remainder of this section, we compute the expecta-\ntions of these two matrices. For now, let us focus on\nE[Q] =\nn\nX\ni=1\nE\n\u0002\u02dcxi\u02dcx\u22a4\ni\n\u0003\n.\n(6)\nAn off-diagonal entry in the matrix \u02dcxi\u02dcx\u22a4\ni is uncorrupted if\nthe two features \u03b1 and \u03b2 both \u201csurvived\u201d the corruption,\nwhich happens with probability (1 \u2212p)2. For the diago-\nnal entries, this holds with probability 1 \u2212p. Let us de-\n\ufb01ne a vector q = [1 \u2212p, . . . , 1 \u2212p, 1]\u22a4\u2208Rd+1, where\nq\u03b1 represents the probability of a feature \u03b1 \u201csurviving\u201d the\ncorruption. As the constant feature is never corrupted, we\nmSDA for Domain Adaptation\nhave qd+1 =1. If we further de\ufb01ne the scatter matrix of the\noriginal uncorrupted input as S = XX\u22a4, we can express\nthe expectation of the matrix Q as\nE[Q]\u03b1,\u03b2 =\n\u001a\nS\u03b1\u03b2q\u03b1q\u03b2\nif\n\u03b1 \u0338= \u03b2\nS\u03b1\u03b2q\u03b1\nif\n\u03b1 = \u03b2 .\n(7)\nSimilarly, we obtain the expectation of P in closed-form as\nE[P]\u03b1\u03b2 = S\u03b1\u03b2q\u03b2.\nWith the help of these expected matrices, we can com-\npute the reconstructive mapping W directly in closed-form\nwithout ever explicitly constructing a single corrupted in-\nput \u02dcxi.\nWe refer to this algorithm as marginalized De-\nnoising Autoencoder (mDA). Algorithm 1 shows a 10-line\nMATLABTM implementation. The mDA has several ad-\nvantages over traditional denoisers: 1.\nIt requires only\na single sweep through the data to compute the matrices\nE[Q], E[P]; 2. Training is convex and a globally optimal\nsolution is guaranteed; 3. The optimization is performed in\nnon-iterative closed-form.\n3.3. Nonlinear feature generation and stacking\nArguably two of the key contributors to the success of the\nSDA are its nonlinearity and the stacking of multiple lay-\ners of denoising autoencoders to create a \u201cdeep\u201d learning\narchitecture. Our framework has the same capabilities.\nIn SDAs, the nonlinearity is injected through the nonlin-\near encoder function h(\u00b7), which is learned together with\nthe reconstruction weights W. Such an approach makes\nthe training procedure highly non-convex and requires it-\nerative procedures to learn the model parameters. To pre-\nserve the closed-form solution from the linear mapping in\nsection 3.2 we insert nonlinearity into our learned repre-\nsentation after the weights W are computed. A nonlinear\nsquashing-function is applied on the output of each mDA.\nSeveral choices are possible, including sigmoid, hyperbolic\ntangent, tanh(), or the recti\ufb01er function (Nair & Hinton,\n2010). Throughout this work, we use the tanh() function.\nInspired by the layer-wise stacking of SDA, we stack sev-\neral mDA layers by feeding the output of the (t\u22121)th mDA\n(after the squashing function) as the input into the tth mDA.\nLet us denote the output of the tth mDA as ht and the orig-\ninal input as h0 = x. The training is performed greedily\nlayer by layer: each map Wt is learned (in closed-form)\nto reconstruct the previous mDA output ht\u22121 from all pos-\nsible corruptions and the output of the tth layer becomes\nht = tanh(Wtht\u22121). In our experiments, we found that\neven without the nonlinear squashing function, stacking\nstill improves the performance. However, the nonlinearity\nimproves over the linear stacking signi\ufb01cantly. We refer to\nthe stacked denoising algorithm as marginalized Stacked\nDenoising Autoencoder (mSDA). Algorithm 2 shows a 8-\nlines MATLABTM implementation of mSDA.\nAlgorithm 2 mSDA in MATLABTM.\nfunction [Ws,hs]=mSDA(X,p,l);\n[d,n]=size(X);\nWs=zeros(d,d+1,l);\nhs=zeros(d,n,l+1);\nhs(:,:,1)=X;\nfor t=1:l\n[Ws(:,:,t), hs(:,:,t+1)]=mDA(hs(:,:,t),p);\nend;\n3.4. mSDA for Domain Adaptation\nWe apply mSDA to domain adaptation by \ufb01rst learning fea-\ntures in an unsupervised fashion on the union of the source\nand target data sets.\nOne observation reported in (Glo-\nrot et al., 2011) is that if multiple domains are available,\nsharing the unsupervised pre-training of SDA across all do-\nmains is bene\ufb01cial compared to pre-training on the source\nand target only. We observe a similar trend with our ap-\nproach. The results reported in section 5 are based on fea-\ntures learned on data from all available domains. Once a\nmSDA is trained, the output of all layers, after squashing,\ntanh(Wtht\u22121), combined with the original features h0,\nare concatenated and form the new representation. All in-\nputs are transformed into the new feature space. A linear\nSupport Vector Machine (SVM) (Chang & Lin, 2011) is\nthen trained on the transformed source inputs and tested\non the target domain. There are two meta-parameters in\nmSDA: the corruption probability p and the number of lay-\ners l. In our experiments, both are set with 5-fold cross\nvalidation on the labeled data from the source domain. As\nthe mSDA training is almost instantaneous, this grid search\nis almost entirely dominated by the SVM training time.\n4. Extension for High Dimensional Data\nMany data sets (e.g. bag-of-words text documents) are nat-\nurally high dimensional. As the dimensionality increases,\nhill-climbing approaches used in SDAs can become pro-\nhibitively expensive. In practice, a work-around is to trun-\ncate the input data to the r\u226ad most common features (Glo-\nrot et al., 2011). Unfortunately, this prevents SDAs from\nutilizing important information found in rarer features. (As\nwe show in section 5, including these rarer features leads\nto signi\ufb01cantly better results.) High dimensionality also\nposes a challenge to mSDA, as the system of linear equa-\ntions in (5) of complexity O(d3) becomes too costly. In\nthis section we describe how to approximate this calcula-\ntion with a simple division into d\nr sub-problems of O(r3).\nWe combine the concept of \u201cpivot features\u201d from Blitzer\net al. (2006) and the use of most-frequent features\nfrom Glorot et al. (2011). Instead of learning a single map-\nping W \u2208Rd\u00d7(d+1) to reconstruct all corrupted features,\nwe learn multiple mappings but only reconstruct the r \u226ad\nmSDA for Domain Adaptation\nD\u2212>B\nE\u2212>B\nK\u2212>B\nB\u2212>D\nE\u2212>D\nK\u2212>D\nB\u2212>E\nD\u2212>E\nK\u2212>E\nB\u2212>K\nD\u2212>K\nE\u2212>K\n\u22124\n\u22122\n0\n2\n4\n6\n8\n10\n12\n \n \nTransfer Loss (%)\nD\u2212>B\nE\u2212>B\nK\u2212>B\nB\u2212>D\nE\u2212>D\nK\u2212>D\nB\u2212>E\nD\u2212>E\nK\u2212>E\nB\u2212>K\nD\u2212>K\nE\u2212>K\n\u22124\n\u22122\n0\n2\n4\n6\n8\n10\n12\n \n \nBaseline\nPCA\nSCL (Blitzer et. al., 2007)\nCODA (Chen et. al., 2011)\nSDA (Glorot et. al., 2011)\nmSDA (l=5)\nFigure 1. Detailed comparison across all twelve domain adaptation task in the small Amazon benchmark data. The reviews are from\nthe domains Books, Kitchen appliances, Electronics, DVDs. With an exception of B \u2192E and D \u2192E, mSDA5 leads to the lowest\ntransfer-loss.\nmost frequent features (here, r = 5000). For an input xi\nwe denote the shortened r-dimensional vector of only the r\nmost-frequent features as zi \u2208Rr. We perform this recon-\nstruction with S random non-overlapping sub-sets of input\nfeatures. Without loss of generality, we assume that the\nfeature-dimensions in the input space are in random order\nand divide-up the input vectors as xi =\nh\nx1\ni\n\u22a4, . . . , xS\ni\n\u22a4i\u22a4\n.\nFor each one of these sub-spaces we learn an independent\nmapping Ws which minimizes\nLs(Ws) = 1\n2n\nn\nX\ni=1\nS\nX\ns=1\n\u2225zi \u2212Ws\u02dcxs\ni\u22252.\n(8)\nEach mapping Ws can be solved in closed-form as in (5),\nfollowing the method described in section 3.2. We de\ufb01ne\nthe output of the \ufb01rst layer in the resulting mSDA as the\naverage of all reconstructions,\nh1 = tanh\n \n1\nS\nS\nX\ns=1\nWsxs\n!\n.\n(9)\nOnce the \ufb01rst layer, of dimension r \u226ad, is built, we can\nstack multiple layers on top of it using the regular mSDA as\ndescribed in section 3.3 and Algorithm 2. It is worth point-\ning out that, although features might be separated in differ-\nent sub-sets within the \ufb01rst layer, they can still be combined\nin subsequent layers of the mSDA.\n5. Results\nWe evaluate mSDA on the Amazon reviews benchmark data\nsets (Blitzer et al., 2006) together with several other algo-\nrithms for representation learning and domain adaptation.\nThe dataset contains more than 340, 000 reviews from 25\ndifferent types of products from Amazon.com. For simplic-\nity (and comparability), we follow the convention of (Chen\net al., 2011b; Glorot et al., 2011) and only consider the\nbinary classi\ufb01cation problem whether a review is positive\n(higher than 3 stars) or negative (3 stars or lower).\nAs\nmSDA and SDA focus on feature learning, we use the raw\nbag-of-words (bow) unigram/bigram features as their input.\nTo be fair to other algorithms that we compare to, we also\npre-process with tf-idf (Salton & Buckley, 1988) and use\nthe transformed feature vectors as their input if that leads\nto better results. Finally, we remove \ufb01ve domains which\ncontain less than 1, 000 reviews.\nDifferent domains in the complete set vary substantially in\nterms of number of instances and class distribution. Some\ndomains (books and music) have hundreds of thousands\nof reviews, while others (food and outdoor) have only a\nfew hundred. There are a total of 380 possible transfer\ntasks (e.g. Apparel \u2192Baby). The proportion of nega-\ntive examples in different domains also differs greatly. To\ncounter the effect of class- and size-imbalance, a more con-\ntrolled smaller dataset was created by Blitzer et al. (2006),\nwhich contains reviews of four types of products: books,\nDVDs, electronics, and kitchen appliances. Here, each do-\nmain consists of 2, 000 labeled inputs and approximately\n4, 000 unlabeled ones (varying slightly between domains)\nand the two classes are exactly balanced. Almost all prior\nwork provides results only on this smaller set with its more\nmanageable twelve transfer tasks. We focus most of our\ncomparative analysis on this smaller set but also provide\nresults on the entire data for completeness.\nMethods.\nAs baseline, we train a linear SVM on the\nraw bag-of-words representation of the labeled source and\ntest it on target. We also include the results of the same\nsetup with dense features obtained by projecting the entire\ndata set (labeled and unlabeled source+target) onto a low-\ndimensional sub-space with PCA (we refer to this setting\nas PCA). Besides these two baselines, we evaluate the ef\ufb01-\ncacy of a linear SVM trained on features learned by mSDA\nand two alternative feature learning algorithms, Structural\nCorrespondence Learning (SCL) (Blitzer et al., 2006) and\nmSDA for Domain Adaptation\n10\n1\n10\n2\n10\n3\n10\n4\n10\n5\n10\n6\n1\n1.05\n1.1\n1.15\n1.2\n1.25\n1.3\n1.35\n \n \nBaseline\nSDA (Glorot et. al., 2011)\nmSDA (l=1,2,3,4,5)\n10\n1\n10\n2\n10\n3\n10\n4\n10\n5\n1\n1.1\n1.2\n1.3\n1.4\n1.5\n \n \nBaseline\nPCA\nSCL (Blitzer et. al., 2007)\nCODA (Chen et. al., 2011)\nSDA (Glorot et. al., 2011)\nmSDA (l=1,2,3,4,5)\nTraining time in seconds (log)\nTransfer Ratio\nl=1\nl=5\nAmazon Benchmark (small)\nAmazon Benchmark (complete)\nTraining time in seconds (log)\nTraining time in seconds (log)\nl=1\nl=5\nFigure 2. Transfer ratio and training times on the small (left) and full (right) Amazon Benchmark data. Results are averaged across the\ntwelve and 380 domain adaptation tasks in the respective data sets (5, 000 features). The graphs compare the results of mSDA with\nbaseline, SDA and, on the small data set, with CODA, SCL and PCA. The speedups of mSDA over SDA, with similar transfer ratio, is\n180\u00d7 on the small task and 230\u00d7 on the complete benchmark.\n1-layer1 SDA (Glorot et al., 2011). Finally, we also com-\npare against CODA (Chen et al., 2011b), a state-of-the-art\ndomain adaptation algorithm which is based on sample-\nand feature-selection, applied to tf-idf features. For CODA,\nSDA and SCL we use implementations provided by the au-\nthors. All hyper-parameters are set by 5-fold cross valida-\ntion on the source training set2.\nMetrics. Following Glorot et al. (2011), we evaluate our\nresults with the transfer error e(S, T) and the in-domain\nerror e(T, T). The transfer error e(S, T) denotes the clas-\nsi\ufb01cation error of a classi\ufb01er trained on the labeled source\ndata and tested on the unlabeled target data. The in-domain\nerror e(T, T) denotes the classi\ufb01cation error of a classi\ufb01er\nthat is trained on the labeled target data and tested on the\nunlabeled target data. Similar to Glorot et al. (2011) we\nmeasure the performance of a domain adaptation algorithm\nin terms of the transfer loss, de\ufb01ned as e(S, T)\u2212eb(T, T),\nwhere eb(T, T) de\ufb01nes the in-domain error of the baseline.\nIn other words, the transfer loss measures how much higher\nthe error of an adapted classi\ufb01er is in comparison to a lin-\near SVM that is trained on actual labeled target bow data.\nThe various domain-adaptation tasks vary substantially in\ndif\ufb01culty, which is why we do not average the trans-\nfer losses (which would be dominated by a few most\ndif\ufb01cult tasks).\nInstead, we average the transfer ratio,\ne(S, T)/eb(T, T), the ratio of the transfer error over the\nin-domain error. As with the transfer loss, a lower transfer\nratio implies better domain adaptation.\n1We were only able to obtain the 1-layer implementation from\nthe authors. Anecdotally, multiple-layer SDA only leads to small\nimprovements on this benchmark set but increases the training\ntime drastically.\n2We keep the default values of some of the parameters in SCL,\ne.g. the number of stop-words removed and stemming parameters\n\u2014 as they were already tuned for this benchmark set by the au-\nthors.\nFor timing purposes, we ignore the time of the SVM train-\ning and only report the mSDA or SDA training time. As\nboth algorithms are unsupervised, we do not re-train for\ndifferent transfer tasks within a benchmark set \u2014 instead\nwe learn one representation on the union of all domains.\nCODA (Chen et al., 2011a) does not take advantage of data\nbesides source and target and we report the average train-\ning time per transfer task.3 All experiments were conducted\non an off-the-shelf desktop with dual 6-core Intel i7 CPUs\nclocked at 2.66Ghz.\n5.1. Comparison with Related Work\nIn the \ufb01rst set of experiments, we use the setting from (Glo-\nrot et al., 2011) on the small Amazon benchmark set. The\ninput data is reduced to only the 5, 000 most frequent terms\nof unigrams and bigrams as features.\nComparison per task. Figure 1 presents a detailed com-\nparison of the transfer loss across the twelve domain adap-\ntation tasks using the various methods mentioned. A linear\nSVM trained on the features generated by SDA and mSDA\nclearly outperform all the other methods. For several tasks,\nthe transfer loss goes to negative \u2014 in other words, a SVM\ntrained on the transformed source data has higher accuracy\nthan one trained on the original target data. This is a strong\nindication that the learned new representation bridges the\ngap between domains. It is worth pointing out that in ten\nout of the twelve tasks mSDA achieves a lower transfer-loss\nthan SDA.\nTiming. Figure 2 (left) depicts the transfer ratio as a func-\ntion of training time required for different algorithms, av-\neraged over 12 tasks. The time is plotted in log scale. We\ncan make three observations: 1. SDA outperforms all other\nrelated work in terms of transfer-ratio, but is also the slow-\n3In CODA, the feature splitting and classi\ufb01er training are in-\nseparable and we necessarily include both in our timing.\nmSDA for Domain Adaptation\n10\n2\n10\n3\n10\n4\n10\n5\n0.95\n0.97\n0.99\n1.01\n1.03\n1.05\nTraining time in seconds (log)\nd = 5, 000\nd = 5, 000\nd = 10, 000\nd = 10, 000\nd = 20, 000\nd = 20, 000\nd = 30, 000\nd = 30, 000\nd = 40, 000\nd = 40, 000\nTransfer Ratio\n1\n2\n3\n4\n5\n6\n1\n1.05\n1.1\n1.15\n1.2\n \n \nmSDA\nSDA (Glorot et. al., 2011)\n1.2\n1.3\n1.4\n1.5\n1.6\n1.7\n1.8\n1.9\n2\n1.2\n1.3\n1.4\n1.5\n1.6\n1.7\n1.8\n1.9\n2\nProxy A-distance on raw input\nProxy A-distance on mSDA\nEK\nBD\nBE\nBK\nDE\nDK\nFigure 3. Left: Transfer ratio as a function of the input dimensionality (terms are picked in decreasing order of their frequency). Right:\nBesides domain adaptation, mSDA also helps in domain recognition tasks.\nest to train (more than 5 hours of training time). 2. SCL\nand PCA are relatively fast, but their features cannot com-\npete in terms of transfer performance. 3. The training time\nof mSDA is two orders of magnitude faster that of SDAs\n(180\u00d7 speedup), with comparable transfer ratio. Training\none layer of mDA on all 27, 677 documents from the small\nset requires less than 25 seconds. A 5-layer mSDA requires\nless than 2 minutes to train, and the resulting feature trans-\nformation achieves slightly better transfer ratio than SDAs.\nLarge scale results.\nTo demonstrate the capabilities of\nmSDA to scale to large data sets, we also evaluate it on the\ncomplete set with n = 340, 000 reviews from 20 domains\nand a total of 380 domain adaptation tasks (see right plot\nin Figure 2). We compare mSDA to SDA (1-layer). The\nlarge set is more heterogenous in terms of the number of\ndomains, domain size and class distribution than the small\nset and both the transfer error and transfer ratio are aver-\naged across 380 tasks. Nonetheless, a similar trend can be\nobserved. The transfer ratio reported in Figure 2 (right) cor-\nresponds to averaged transfer errors of (baseline) 13.93%,\n( one-layer SDA) 10.50%, (mSDA, l = 1) 11.50%, (mSDA,\nl = 3) 10.47%, (mSDA, l = 5) 10.33%. With only one\nlayer, mSDA performs a little worse than SDA but reduces\nthe training time from over two days to about \ufb01ve minutes\n(700\u00d7 speedup). With three layers, mSDA matches the\ntransfer-error and transfer-ratio of SDA and still only re-\nquires 14 minutes of training time (230\u00d7 speedup).\n5.2. Further Analysis\nIn addition to comparison with prior work, we also analyze\nvarious other aspects of mSDA.\nLow-frequency features. Prior work often limits the in-\nput data to the most frequent features (Glorot et al., 2011).\nWe use the modi\ufb01cation from section 4 to scale mSDA\n(5-layers) up to high dimensions and include less-frequent\nuni-grams and bi-grams in the input (small Amazon set).\nIn the case of SDA we make the \ufb01rst layer a dimension-\nality reducing transformation from d dimensions to 5000.\nThe left plot in Figure 3 shows the performance of mSDA\nand SDA as the input dimensionality increases (words are\npicked in decreasing order of their frequency). The trans-\nfer ratio is computed relative to the baseline with d=5000\nfeature. Clearly, both algorithms bene\ufb01t from having more\nfeatures up to 30, 000. mSDA matches the transfer-ratio\nof SDA consistently and, as the dimensionality increases,\ngains even higher speed-up. With 30, 000 input features,\nSDA requires over one day and mSDA only 3 minutes\n(458\u00d7 speedup).\nTransfer distance. Ben-David et al. (2007) suggest the\nProxy-A-distance (PAD) as a measure of how different two\ndomains are from each other.\nThe metric is de\ufb01ned as\n2(1 \u22122\u03f5), where \u03f5 is the generalization error of a classi-\n\ufb01er (a linear SVM in our case) trained on the binary classi-\n\ufb01cation problem to distinguish inputs between the two do-\nmains. The right plot in Figure 3 shows the PAD before and\nafter mSDA is applied. Surprisingly, the distance increases\nin the new representation \u2014 i.e. distinguishing between\ntwo domains becomes easier with the mSDA features. We\nexplain this effect through the fact that mSDA is unsuper-\nvised and learns a generally better representation for the\ninput data. This helps both tasks, distinguishing between\ndomains and sentiment analysis (e.g.\nin the electronic-\ndomain mSDA might interpolate the feature \u201cdvd player\u201d\nfrom \u201cblue ray\u201d, both are not particularly relevant for sen-\ntiment analysis but might help distinguish the review from\nthe book domain.). Glorot et al. (2011) observe a similar\neffect with the representations learned with SDA.\n5.3. General Trends\nIn summary, we observe a few general trends across all ex-\nperiments: 1. With one layer, mSDA is up to three orders\nof magnitudes faster but slightly less expressive than the\noriginal SDA. This can be attributed to the fact that mSDA\nhas no hidden layer. 2. There is a clear trend that addi-\ntional \u201cdeep\u201d layers improve the results signi\ufb01cantly (here,\nup to \ufb01ve layers). With additional layers, the mSDA fea-\nmSDA for Domain Adaptation\ntures reach (and surpass) the accuracy of 1-layer SDA and\nstill obtain a several hundred-fold speedup. 3. The mSDA\nfeatures help diverse classi\ufb01cation tasks, domain classi\ufb01-\ncation and sentiment analysis, and can be trained very ef\ufb01-\nciently on high-dimensional data.\n6. Discussion and Conclusion\nAlthough mSDA \ufb01rst and foremost marginalizes out the\ncorruption in SDA training, the two algorithms differ in\nseveral profound ways: First, the mDA layers do not have\nhidden nodes \u2014 this allows a closed-form solution with\nsubstantial speed-ups but might entail limitations that still\nneed to be investigated. Second, mSDA only has two free\nmeta-parameters, controlling the amount of noise as well\nas the number of layers to be stacked, which greatly sim-\npli\ufb01es the model selection. Finally, leveraging on the an-\nalytic tractability of linear regression, the parameters of\nan mDA are trained to optimally denoise all possible cor-\nrupted training inputs \u2014 arguably \u201cin\ufb01nitely many\u201d. This\nis practically infeasible for SDAs.\nWe hope that our work on mSDA will inspire future\nresearch on ef\ufb01cient training of SDA, beyond domain\nadaptation, and impact a variety of research problems.\nThe fast training time, the capability to scale to large\nand high-dimensional data and implementation simplicity\nmake mSDA a promising method with appeal to a large au-\ndience within and beyond machine learning.\nAcknowledgements\nKQW, MC, ZX were supported by NSF IIS-1149882 and NIH\nU01 1U01NS073457-01. FS was supported by NSF IIS-0957742,\nDARPA CSSG N10AP20019 and D11AP00278.\nReferences\nBaldi, P. and Hornik, K. Neural networks and principal compo-\nnent analysis: Learning from examples without local minima.\nNeural networks, 2(1):53\u201358, 1989.\nBen-David, S., Blitzer, J., Crammer, K., and Pereira, F. Analysis\nof representations for domain adaptation. NIPS, 19:137, 2007.\nBen-David, S., Blitzer, J., Crammer, K., Kulesza, A., Pereira, F.,\nand Wortman, Jenn. A Theory of Learning from Different Do-\nmains. Machine Learning, 2009.\nBergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R.,\nDesjardins, G., Turian, J., Warde-Farley, D., and Bengio, Y.\nTheano: a CPU and GPU Math Expression Compiler. In SciPy,\nJune 2010.\nBishop, Christopher. Pattern Recognition and Machine Learning.\nSpringer, 2006.\nBlitzer, J., McDonald, R., and Pereira, F. Domain adaptation with\nstructural correspondence learning. In Proceedings of the 2006\nConference on EMNLP, pp. 120\u2013128, 2006.\nChang, C.C. and Lin, C.J. LIBSVM: a library for support vector\nmachines. ACM Transactions on IST, 2(3):27, 2011.\nChen, M., Weinberger, K.Q., and Blitzer, J.C. Co-Training for\nDomain Adaptation. In NIPS, 2011a.\nChen, M., Weinberger, K.Q., and Chen, Y. Automatic Feature\nDecomposition for Single View Co-training. In ICML, 2011b.\nDaume III, H. Frustratingly Easy Domain Adaptation. In ACL,\n2007.\nDauphin Y., Glorot X., Bengio Y. Large-Scale Learning of Em-\nbeddings with Reconstruction Sampling. In ICML, 2011.\nGlorot, X., Bordes, A., and Bengio, Y. Domain adaptation for\nlarge-scale sentiment classi\ufb01cation: A deep learning approach.\nIn ICML, 2011.\nHuang, J., Smola, A.J., Gretton, A., Borgwardt, K. M., and\nScholkopf, B. Correcting Sample Selection Bias by Unlabeled\nData. In NIPS 19, pp. 601\u2013608. MIT Press, 2007.\nKavukcuoglu, K., Ranzato, M.A., Fergus, R., and Le-Cun, Y.\nLearning invariant features through topographic \ufb01lter maps. In\nCVPR 2009. IEEE Conference on, pp. 1605\u20131612. IEEE, 2009.\nLee, Honglak, Largman, Yan, Pham, Peter, and Ng, Andrew Y.\nUnsupervised Feature Learning for Audio Classi\ufb01cation using\nConvolutional Deep Belief Networks. In NIPS. 2009.\nLiu, Qian, Mackey, Aaron, Roos, David, and Pereira, Fernando.\nEvigan: a hidden variable model for integrating gene evidence\nfor eukaryotic gene prediction. Bioinformatics, 2008.\nMansour, T., Mohri, M., and Rostamizadeh, A. Domain Adapta-\ntion with Multiple Sources. In NIPS. 2009.\nMcClosky, D., Charniak, E., and Johnson, M. Reranking and self-\ntraining for parser adaptation. In ACL, pp. 337\u2013344, 2006.\nNair, V. and Hinton, G.E. Recti\ufb01ed linear units improve restricted\nboltzmann machines. In ICML 27, 2010.\nRifai, S., Vincent, P., Muller, X., Glorot, X., and Bengio, Y. Con-\ntractive auto-encoders: Explicit invariance during feature ex-\ntraction. In ICML, 2011.\nRumelhart, D.E., Hintont, G.E., and Williams, R.J. Learning rep-\nresentations by back-propagating errors. Nature, 323(6088):\n533\u2013536, 1986.\nSaenko, K., Kulis, B., Fritz, M., and Darrell, T. Adapting vi-\nsual category models to new domains. Computer Vision\u2013ECCV\n2010, pp. 213\u2013226, 2010.\nSalton, G. and Buckley, C. Term-weighting approaches in auto-\nmatic text retrieval. Information processing & management, 24\n(5):513\u2013523, 1988.\nVincent, P., Larochelle, H., Bengio, Y., and Manzagol, P.A. Ex-\ntracting and composing robust features with denoising autoen-\ncoders. In ICML, 2008.\nWeinberger, K.Q., Dasgupta, A., Langford, J., Smola, A., and\nAttenberg, J. Feature hashing for large scale multitask learning.\nIn ICML 26, pp. 1113\u20131120. ACM, 2009.\nXue, G., Dai, W., Yang, Q., and Yu, Y. Topic-bridged PLSA for\ncross-domain text classication. In SIGIR, 2008.\n",
        "sentence": " These embeddings are generated as a result of training \u201cdeep\u201d architectures, and it has been shown that such representations are well suited for domain adaptation tasks (Glorot et al., 2011; Chen et al., 2012).",
        "context": "of representations for domain adaptation. NIPS, 19:137, 2007.\nBen-David, S., Blitzer, J., Crammer, K., Kulesza, A., Pereira, F.,\nand Wortman, Jenn. A Theory of Learning from Different Do-\nmains. Machine Learning, 2009.\nbeddings with Reconstruction Sampling. In ICML, 2011.\nGlorot, X., Bordes, A., and Bengio, Y. Domain adaptation for\nlarge-scale sentiment classi\ufb01cation: A deep learning approach.\nIn ICML, 2011.\nHuang, J., Smola, A.J., Gretton, A., Borgwardt, K. M., and\nsource feature representations (Blitzer et al., 2006; Glorot\net al., 2011; Xue et al., 2008).\nRecently, Glorot et al. (2011) proposed a new approach that\nfalls into the third category. The authors propose to learn"
    },
    {
        "title": "The expressive power of word embeddings. CoRR, abs/1301.3226",
        "author": [
            "Yanqing Chen",
            "Bryan Perozzi",
            "Rami Al-Rfou",
            "Steven Skiena"
        ],
        "venue": null,
        "citeRegEx": "Chen et al\\.,? \\Q2013\\E",
        "shortCiteRegEx": "Chen et al\\.",
        "year": 2013,
        "abstract": "",
        "full_text": "",
        "sentence": " (Chen et al., 2013) shows that the embeddings generated by SENNA",
        "context": null
    },
    {
        "title": "A unified architecture for natural language processing: Deep neural networks with multitask learning",
        "author": [
            "R. Collobert",
            "J. Weston."
        ],
        "venue": "International Conference on Machine Learning, ICML.",
        "citeRegEx": "Collobert and Weston.,? 2008",
        "shortCiteRegEx": "Collobert and Weston.",
        "year": 2008,
        "abstract": "",
        "full_text": "",
        "sentence": " (Collobert and Weston, 2008) shows that word embeddings can almost substitute NLP common features on several tasks.",
        "context": null
    },
    {
        "title": "Natural language processing (almost) from scratch",
        "author": [
            "Ronan Collobert",
            "Jason Weston",
            "L\u00e9on Bottou",
            "Michael Karlen",
            "Koray Kavukcuoglu",
            "Pavel Kuksa."
        ],
        "venue": "J. Mach. Learn. Res., 12:2493\u20132537, November.",
        "citeRegEx": "Collobert et al\\.,? 2011",
        "shortCiteRegEx": "Collobert et al\\.",
        "year": 2011,
        "abstract": "We propose a unified neural network architecture and learning algorithm that\ncan be applied to various natural language processing tasks including:\npart-of-speech tagging, chunking, named entity recognition, and semantic role\nlabeling. This versatility is achieved by trying to avoid task-specific\nengineering and therefore disregarding a lot of prior knowledge. Instead of\nexploiting man-made input features carefully optimized for each task, our\nsystem learns internal representations on the basis of vast amounts of mostly\nunlabeled training data. This work is then used as a basis for building a\nfreely available tagging system with good performance and minimal computational\nrequirements.",
        "full_text": "arXiv\narXiv\nNatural Language Processing (almost) from Scratch\nRonan Collobert\nronan@collobert.com\nNEC Labs America, Princeton NJ.\nJason Weston\njweston@google.com\nGoogle, New York, NY.\nL\u00b4eon Bottou\nleon@bottou.org\nMichael Karlen\nmichael.karlen@gmail.com\nKoray Kavukcuoglu\u2020\nkoray@cs.nyu.edu\nPavel Kuksa\u2021\npkuksa@cs.rutgers.edu\nNEC Labs America, Princeton NJ.\nAbstract\nWe propose a uni\ufb01ed neural network architecture and learning algorithm that can be applied\nto various natural language processing tasks including: part-of-speech tagging, chunking,\nnamed entity recognition, and semantic role labeling. This versatility is achieved by trying\nto avoid task-speci\ufb01c engineering and therefore disregarding a lot of prior knowledge.\nInstead of exploiting man-made input features carefully optimized for each task, our system\nlearns internal representations on the basis of vast amounts of mostly unlabeled training\ndata. This work is then used as a basis for building a freely available tagging system with\ngood performance and minimal computational requirements.\nKeywords:\nNatural Language Processing, Neural Networks\n1. Introduction\nWill a computer program ever be able to convert a piece of English text into a data structure\nthat unambiguously and completely describes the meaning of the natural language text?\nAmong numerous problems, no consensus has emerged about the form of such a data\nstructure. Until such fundamental Arti\ufb01cial Intelligence problems are resolved, computer\nscientists must settle for reduced objectives: extracting simpler representations describing\nrestricted aspects of the textual information.\nThese simpler representations are often motivated by speci\ufb01c applications, for instance,\nbag-of-words variants for information retrieval. These representations can also be motivated\nby our belief that they capture something more general about natural language.\nThey\ncan describe syntactic information (e.g. part-of-speech tagging, chunking, and parsing) or\nsemantic information (e.g. word-sense disambiguation, semantic role labeling, named entity\nextraction, and anaphora resolution). Text corpora have been manually annotated with such\ndata structures in order to compare the performance of various systems. The availability of\nstandard benchmarks has stimulated research in Natural Language Processing (NLP) and\n\u2020. Koray Kavukcuoglu is also with New York University, New York, NY.\n\u2021. Pavel Kuksa is also with Rutgers University, New Brunswick, NJ.\nc\u20dd2009 Ronan Collobert, Jason Weston, L\u00b4eon Bottou, Michael Karlen, Koray Kavukcuoglu and Pavel Kuksa.\narXiv:1103.0398v1  [cs.LG]  2 Mar 2011\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\ne\ufb00ective systems have been designed for all these tasks. Such systems are often viewed as\nsoftware components for constructing real-world NLP solutions.\nThe overwhelming majority of these state-of-the-art systems address a benchmark\ntask by applying linear statistical models to ad-hoc features.\nIn other words, the\nresearchers themselves discover intermediate representations by engineering task-speci\ufb01c\nfeatures. These features are often derived from the output of preexisting systems, leading\nto complex runtime dependencies. This approach is e\ufb00ective because researchers leverage\na large body of linguistic knowledge. On the other hand, there is a great temptation to\noptimize the performance of a system for a speci\ufb01c benchmark. Although such performance\nimprovements can be very useful in practice, they teach us little about the means to progress\ntoward the broader goals of natural language understanding and the elusive goals of Arti\ufb01cial\nIntelligence.\nIn this contribution, we try to excel on multiple benchmarks while avoiding task-speci\ufb01c\nenginering.\nInstead we use a single learning system able to discover adequate internal\nrepresentations. In fact we view the benchmarks as indirect measurements of the relevance\nof the internal representations discovered by the learning procedure, and we posit that these\nintermediate representations are more general than any of the benchmarks. Our desire to\navoid task-speci\ufb01c engineered features led us to ignore a large body of linguistic knowledge.\nInstead we reach good performance levels in most of the tasks by transferring intermediate\nrepresentations discovered on large unlabeled datasets. We call this approach \u201calmost from\nscratch\u201d to emphasize the reduced (but still important) reliance on a priori NLP knowledge.\nThe paper is organized as follows.\nSection 2 describes the benchmark tasks of\ninterest. Section 3 describes the uni\ufb01ed model and reports benchmark results obtained with\nsupervised training. Section 4 leverages large unlabeled datasets (\u223c852 million words)\nto train the model on a language modeling task.\nPerformance improvements are then\ndemonstrated by transferring the unsupervised internal representations into the supervised\nbenchmark models. Section 5 investigates multitask supervised training. Section 6 then\nevaluates how much further improvement can be achieved by incorporating standard NLP\ntask-speci\ufb01c engineering into our systems. Drifting away from our initial goals gives us the\nopportunity to construct an all-purpose tagger that is simultaneously accurate, practical,\nand fast. We then conclude with a short discussion section.\n2. The Benchmark Tasks\nIn this section, we brie\ufb02y introduce four standard NLP tasks on which we will benchmark\nour architectures within this paper: Part-Of-Speech tagging (POS), chunking (CHUNK),\nNamed Entity Recognition (NER) and Semantic Role Labeling (SRL). For each of them,\nwe consider a standard experimental setup and give an overview of state-of-the-art systems\non this setup. The experimental setups are summarized in Table 1, while state-of-the-art\nsystems are reported in Table 2.\n2.1 Part-Of-Speech Tagging\nPOS aims at labeling each word with a unique tag that indicates its syntactic role, e.g.\nplural noun, adverb, . . . A standard benchmark setup is described in detail by Toutanova\n2\narXiv\nNatural Language Processing (almost) from Scratch\nTask\nBenchmark\nDataset\nTraining set\nTest set\n(#tokens)\n(#tokens)\n(#tags)\nPOS\nToutanova et al. (2003)\nWSJ\nsections 0\u201318\nsections 22\u201324\n( 45 )\n( 912,344 )\n( 129,654 )\nChunking\nCoNLL 2000\nWSJ\nsections 15\u201318\nsection 20\n( 42 )\n( 211,727 )\n( 47,377 )\n(IOBES)\nNER\nCoNLL 2003\nReuters\n\u201ceng.train\u201d\n\u201ceng.testb\u201d\n( 17 )\n( 203,621 )\n( 46,435 )\n(IOBES)\nSRL\nCoNLL 2005\nWSJ\nsections 2\u201321\nsection 23\n( 186 )\n( 950,028 )\n+ 3 Brown sections\n(IOBES)\n( 63,843 )\nTable 1:\nExperimental setup: for each task, we report the standard benchmark we used,\nthe dataset it relates to, as well as training and test information.\nSystem\nAccuracy\nShen et al. (2007)\n97.33%\nToutanova et al. (2003)\n97.24%\nGim\u00b4enez and M`arquez (2004)\n97.16%\n(a) POS\nSystem\nF1\nShen and Sarkar (2005)\n95.23%\nSha and Pereira (2003)\n94.29%\nKudo and Matsumoto (2001)\n93.91%\n(b) CHUNK\nSystem\nF1\nAndo and Zhang (2005)\n89.31%\nFlorian et al. (2003)\n88.76%\nKudo and Matsumoto (2001)\n88.31%\n(c) NER\nSystem\nF1\nKoomen et al. (2005)\n77.92%\nPradhan et al. (2005)\n77.30%\nHaghighi et al. (2005)\n77.04%\n(d) SRL\nTable 2: State-of-the-art systems on four NLP tasks. Performance is reported in per-word\naccuracy for POS, and F1 score for CHUNK, NER and SRL. Systems in bold will be referred\nas benchmark systems in the rest of the paper (see text).\net al. (2003). Sections 0\u201318 of Wall Street Journal (WSJ) data are used for training, while\nsections 19\u201321 are for validation and sections 22\u201324 for testing.\nThe best POS classi\ufb01ers are based on classi\ufb01ers trained on windows of text, which are\nthen fed to a bidirectional decoding algorithm during inference. Features include preceding\nand following tag context as well as multiple words (bigrams, trigrams. . . ) context, and\nhandcrafted features to deal with unknown words.\nToutanova et al. (2003), who use\nmaximum entropy classi\ufb01ers, and a bidirectional dependency network (Heckerman et al.,\n2001) at inference, reach 97.24% per-word accuracy. Gim\u00b4enez and M`arquez (2004) proposed\na SVM approach also trained on text windows, with bidirectional inference achieved with\ntwo Viterbi decoders (left-to-right and right-to-left).\nThey obtained 97.16% per-word\naccuracy.\nMore recently, Shen et al. (2007) pushed the state-of-the-art up to 97.33%,\nwith a new learning algorithm they call guided learning, also for bidirectional sequence\nclassi\ufb01cation.\n3\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\n2.2 Chunking\nAlso called shallow parsing, chunking aims at labeling segments of a sentence with syntactic\nconstituents such as noun or verb phrases (NP or VP). Each word is assigned only one unique\ntag, often encoded as a begin-chunk (e.g. B-NP) or inside-chunk tag (e.g. I-NP). Chunking\nis often evaluated using the CoNLL 2000 shared task1. Sections 15\u201318 of WSJ data are\nused for training and section 20 for testing. Validation is achieved by splitting the training\nset.\nKudoh and Matsumoto (2000) won the CoNLL 2000 challenge on chunking with a F1-\nscore of 93.48%.\nTheir system was based on Support Vector Machines (SVMs).\nEach\nSVM was trained in a pairwise classi\ufb01cation manner, and fed with a window around the\nword of interest containing POS and words as features, as well as surrounding tags. They\nperform dynamic programming at test time.\nLater, they improved their results up to\n93.91% (Kudo and Matsumoto, 2001) using an ensemble of classi\ufb01ers trained with di\ufb00erent\ntagging conventions (see Section 3.2.3).\nSince then, a certain number of systems based on second-order random \ufb01elds were\nreported (Sha and Pereira, 2003; McDonald et al., 2005; Sun et al., 2008), all reporting\naround 94.3% F1 score. These systems use features composed of words, POS tags, and\ntags.\nMore recently, Shen and Sarkar (2005) obtained 95.23% using a voting classi\ufb01er scheme,\nwhere each classi\ufb01er is trained on di\ufb00erent tag representations2 (IOB, IOE, . . . ). They use\nPOS features coming from an external tagger, as well carefully hand-crafted specialization\nfeatures which again change the data representation by concatenating some (carefully\nchosen) chunk tags or some words with their POS representation. They then build trigrams\nover these features, which are \ufb01nally passed through a Viterbi decoder a test time.\n2.3 Named Entity Recognition\nNER labels atomic elements in the sentence into categories such as \u201cPERSON\u201d or\n\u201cLOCATION\u201d. As in the chunking task, each word is assigned a tag pre\ufb01xed by an indicator\nof the beginning or the inside of an entity. The CoNLL 2003 setup3 is a NER benchmark\ndataset based on Reuters data. The contest provides training, validation and testing sets.\nFlorian et al. (2003) presented the best system at the NER CoNLL 2003 challenge, with\n88.76% F1 score. They used a combination of various machine-learning classi\ufb01ers. Features\nthey picked included words, POS tags, CHUNK tags, pre\ufb01xes and su\ufb03xes, a large gazetteer\n(not provided by the challenge), as well as the output of two other NER classi\ufb01ers trained\non richer datasets. Chieu (2003), the second best performer of CoNLL 2003 (88.31% F1),\nalso used an external gazetteer (their performance goes down to 86.84% with no gazetteer)\nand several hand-chosen features.\nLater, Ando and Zhang (2005) reached 89.31% F1 with a semi-supervised approach.\nThey trained jointly a linear model on NER with a linear model on two auxiliary\nunsupervised tasks. They also performed Viterbi decoding at test time. The unlabeled\n1. See http://www.cnts.ua.ac.be/conll2000/chunking.\n2. See Table 3 for tagging scheme details.\n3. See http://www.cnts.ua.ac.be/conll2003/ner.\n4\narXiv\nNatural Language Processing (almost) from Scratch\ncorpus was 27M words taken from Reuters. Features included words, POS tags, su\ufb03xes\nand pre\ufb01xes or CHUNK tags, but overall were less specialized than CoNLL 2003 challengers.\n2.4 Semantic Role Labeling\nSRL aims at giving a semantic role to a syntactic constituent of a sentence.\nIn the\nPropBank (Palmer et al., 2005) formalism one assigns roles ARG0-5 to words that are\narguments of a verb (or more technically, a predicate) in the sentence, e.g. the following\nsentence might be tagged \u201c[John]ARG0 [ate]REL [the apple]ARG1 \u201d, where \u201cate\u201d is the\npredicate. The precise arguments depend on a verb\u2019s frame and if there are multiple verbs\nin a sentence some words might have multiple tags.\nIn addition to the ARG0-5 tags,\nthere there are several modi\ufb01er tags such as ARGM-LOC (locational) and ARGM-TMP\n(temporal) that operate in a similar way for all verbs. We picked CoNLL 20054 as our SRL\nbenchmark. It takes sections 2\u201321 of WSJ data as training set, and section 24 as validation\nset. A test set composed of section 23 of WSJ concatenated with 3 sections from the Brown\ncorpus is also provided by the challenge.\nState-of-the-art SRL systems consist of several stages: producing a parse tree, identifying\nwhich parse tree nodes represent the arguments of a given verb, and \ufb01nally classifying these\nnodes to compute the corresponding SRL tags.\nThis entails extracting numerous base\nfeatures from the parse tree and feeding them into statistical models. Feature categories\ncommonly used by these system include (Gildea and Jurafsky, 2002; Pradhan et al., 2004):\n\u2022 the parts of speech and syntactic labels of words and nodes in the tree;\n\u2022 the node\u2019s position (left or right) in relation to the verb;\n\u2022 the syntactic path to the verb in the parse tree;\n\u2022 whether a node in the parse tree is part of a noun or verb phrase;\n\u2022 the voice of the sentence: active or passive;\n\u2022 the node\u2019s head word; and\n\u2022 the verb sub-categorization.\nPradhan et al. (2004) take these base features and de\ufb01ne additional features, notably\nthe part-of-speech tag of the head word, the predicted named entity class of the argument,\nfeatures providing word sense disambiguation for the verb (they add 25 variants of 12 new\nfeature types overall). This system is close to the state-of-the-art in performance. Pradhan\net al. (2005) obtain 77.30% F1 with a system based on SVM classi\ufb01ers and simultaneously\nusing the two parse trees provided for the SRL task. In the same spirit, Haghighi et al.\n(2005) use log-linear models on each tree node, re-ranked globally with a dynamic algorithm.\nTheir system reaches 77.04% using the \ufb01ve top Charniak parse trees.\nKoomen et al. (2005) hold the state-of-the-art with Winnow-like (Littlestone, 1988)\nclassi\ufb01ers, followed by a decoding stage based on an integer program that enforces speci\ufb01c\nconstraints on SRL tags. They reach 77.92% F1 on CoNLL 2005, thanks to the \ufb01ve top\nparse trees produced by the Charniak (2000) parser (only the \ufb01rst one was provided by the\ncontest) as well as the Collins (1999) parse tree.\n4. See http://www.lsi.upc.edu/~srlconll.\n5\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\n2.5 Evaluation\nIn our experiments, we strictly followed the standard evaluation procedure of each CoNLL\nchallenges for NER, CHUNK and SRL. All these three tasks are evaluated by computing the\nF1 scores over chunks produced by our models. The POS task is evaluated by computing\nthe per-word accuracy, as it is the case for the standard benchmark we refer to (Toutanova\net al., 2003). We picked the conlleval script5 for evaluating POS6, NER and CHUNK.\nFor SRL, we used the srl-eval.pl script included in the srlconll package7.\n2.6 Discussion\nWhen participating in an (open) challenge, it is legitimate to increase generalization by all\nmeans. It is thus not surprising to see many top CoNLL systems using external labeled data,\nlike additional NER classi\ufb01ers for the NER architecture of Florian et al. (2003) or additional\nparse trees for SRL systems (Koomen et al., 2005). Combining multiple systems or tweaking\ncarefully features is also a common approach, like in the chunking top system (Shen and\nSarkar, 2005).\nHowever, when comparing systems, we do not learn anything of the quality of each\nsystem if they were trained with di\ufb00erent labeled data. For that reason, we will refer to\nbenchmark systems, that is, top existing systems which avoid usage of external data and\nhave been well-established in the NLP \ufb01eld: (Toutanova et al., 2003) for POS and (Sha and\nPereira, 2003) for chunking. For NER we consider (Ando and Zhang, 2005) as they were\nusing additional unlabeled data only. We picked (Koomen et al., 2005) for SRL, keeping in\nmind they use 4 additional parse trees not provided by the challenge. These benchmark\nsystems will serve as baseline references in our experiments.\nWe marked them in bold\nin Table 2.\nWe note that for the four tasks we are considering in this work, it can be seen that for the\nmore complex tasks (with corresponding lower accuracies), the best systems proposed have\nmore engineered features relative to the best systems on the simpler tasks. That is, the POS\ntask is one of the simplest of our four tasks, and only has relatively few engineered features,\nwhereas SRL is the most complex, and many kinds of features have been designed for it.\nThis clearly has implications for as yet unsolved NLP tasks requiring more sophisticated\nsemantic understanding than the ones considered here.\n3. The Networks\nAll the NLP tasks above can be seen as tasks assigning labels to words. The traditional NLP\napproach is: extract from the sentence a rich set of hand-designed features which are then\nfed to a standard classi\ufb01cation algorithm, e.g. a Support Vector Machine (SVM), often with\na linear kernel. The choice of features is a completely empirical process, mainly based \ufb01rst\non linguistic intuition, and then trial and error, and the feature selection is task dependent,\nimplying additional research for each new NLP task. Complex tasks like SRL then require\na large number of possibly complex features (e.g., extracted from a parse tree) which can\n5. Available at http://www.cnts.ua.ac.be/conll2000/chunking/conlleval.txt.\n6. We used the \u201c-r\u201d option of the conlleval script to get the per-word accuracy, for POS only.\n7. Available at http://www.lsi.upc.es/~srlconll/srlconll-1.1.tgz.\n6\narXiv\nNatural Language Processing (almost) from Scratch\nInput Window\nLookup Table\nLinear\nHardTanh\nLinear\nText\ncat\nsat\non the mat\nFeature 1\nw1\n1\nw1\n2\n. . .\nw1\nN\n...\nFeature K\nwK\n1\nwK\n2\n. . .\nwK\nN\nLTW 1\n...\nLTW K\nM 1 \u00d7 \u00b7\nM 2 \u00d7 \u00b7\nword of interest\nd\nconcat\nn1\nhu\nn2\nhu = #tags\nFigure 1: Window approach network.\nimpact the computational cost which might be important for large-scale applications or\napplications requiring real-time response.\nInstead, we advocate a radically di\ufb00erent approach: as input we will try to pre-process\nour features as little as possible and then use a multilayer neural network (NN) architecture,\ntrained in an end-to-end fashion.\nThe architecture takes the input sentence and learns\nseveral layers of feature extraction that process the inputs. The features computed by the\ndeep layers of the network are automatically trained by backpropagation to be relevant to\nthe task. We describe in this section a general multilayer architecture suitable for all our\nNLP tasks, which is generalizable to other NLP tasks as well.\nOur architecture is summarized in Figure 1 and Figure 2. The \ufb01rst layer extracts features\nfor each word. The second layer extracts features from a window of words or from the whole\nsentence, treating it as a sequence with local and global structure (i.e., it is not treated like\na bag of words). The following layers are standard NN layers.\nNotations\nWe consider a neural network f\u03b8(\u00b7), with parameters \u03b8.\nAny feed-forward\nneural network with L layers, can be seen as a composition of functions fl\n\u03b8(\u00b7), corresponding\nto each layer l:\nf\u03b8(\u00b7) = fL\n\u03b8 (fL\u22121\n\u03b8\n(. . . f1\n\u03b8 (\u00b7) . . .)) .\nIn the following, we will describe each layer we use in our networks shown in Figure 1\nand Figure 2. We adopt few notations. Given a matrix A we denote [A]i, j the coe\ufb03cient\n7\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\nInput Sentence\nLookup Table\nConvolution\nMax Over Time\nLinear\nHardTanh\nLinear\nText\nThe cat\nsat\non\nthe mat\nFeature 1\nw1\n1\nw1\n2\n. . .\nw1\nN\n...\nFeature K\nwK\n1\nwK\n2\n. . .\nwK\nN\nLTW 1\n...\nLTW K\nmax(\u00b7)\nM 2 \u00d7 \u00b7\nM 3 \u00d7 \u00b7\nd\nPadding\nPadding\nn1\nhu\nM 1 \u00d7 \u00b7\nn1\nhu\nn2\nhu\nn3\nhu = #tags\nFigure 2: Sentence approach network.\nat row i and column j in the matrix.\nWe also denote \u27e8A\u27e9dwin\ni\nthe vector obtained by\nconcatenating the dwin column vectors around the ith column vector of matrix A \u2208Rd1\u00d7d2:\nh\n\u27e8A\u27e9dwin\ni\niT\n=\n\u0010\n[A]1, i\u2212dwin/2 . . . [A]d1, i\u2212dwin/2 , . . . , [A]1, i+dwin/2 . . . [A]d1, i+dwin/2\n\u0011\n.\nAs a special case, \u27e8A\u27e91\ni represents the ith column of matrix A. For a vector v, we denote\n[v]i the scalar at index i in the vector. Finally, a sequence of element {x1, x2, . . . , xT } is\nwritten [x]T\n1 . The ith element of the sequence is [x]i.\n8\narXiv\nNatural Language Processing (almost) from Scratch\n3.1 Transforming Words into Feature Vectors\nOne of the essential key points of our architecture is its ability to perform well with the\nuse of (almost8) raw words. The ability for our method to learn good word representations\nis thus crucial to our approach. For e\ufb03ciency, words are fed to our architecture as indices\ntaken from a \ufb01nite dictionary D. Obviously, a simple index does not carry much useful\ninformation about the word. However, the \ufb01rst layer of our network maps each of these\nword indices into a feature vector, by a lookup table operation. Given a task of interest, a\nrelevant representation of each word is then given by the corresponding lookup table feature\nvector, which is trained by backpropagation.\nMore formally, for each word w \u2208D, an internal dwrd-dimensional feature vector\nrepresentation is given by the lookup table layer LTW (\u00b7):\nLTW (w) = \u27e8W\u27e91\nw ,\nwhere W \u2208Rdwrd\u00d7|D| is a matrix of parameters to be learnt, \u27e8W\u27e91\nw \u2208Rdwrd is the wth\ncolumn of W and dwrd is the word vector size (a hyper-parameter to be chosen by the user).\nGiven a sentence or any sequence of T words [w]T\n1 in D, the lookup table layer applies the\nsame operation for each word in the sequence, producing the following output matrix:\nLTW ([w]T\n1 ) =\n\u0010\n\u27e8W\u27e91\n[w]1\n\u27e8W\u27e91\n[w]2\n. . .\n\u27e8W\u27e91\n[w]T\n\u0011\n.\n(1)\nThis matrix can then be fed to further neural network layers, as we will see below.\n3.1.1 Extending to Any Discrete Features\nOne might want to provide features other than words if one suspects that these features are\nhelpful for the task of interest. For example, for the NER task, one could provide a feature\nwhich says if a word is in a gazetteer or not. Another common practice is to introduce some\nbasic pre-processing, such as word-stemming or dealing with upper and lower case. In this\nlatter option, the word would be then represented by three discrete features: its lower case\nstemmed root, its lower case ending, and a capitalization feature.\nGenerally speaking, we can consider a word as represented by K discrete features w \u2208\nD1\u00d7\u00b7 \u00b7 \u00b7\u00d7DK, where Dk is the dictionary for the kth feature. We associate to each feature a\nlookup table LTW k(\u00b7), with parameters W k \u2208Rdk\nwrd\u00d7|Dk| where dk\nwrd \u2208N is a user-speci\ufb01ed\nvector size. Given a word w, a feature vector of dimension dwrd = P\nk dk\nwrd is then obtained\nby concatenating all lookup table outputs:\nLTW 1,...,W K(w) =\n\uf8eb\n\uf8ec\n\uf8ed\nLTW 1(w1)\n...\nLTW K(wK)\n\uf8f6\n\uf8f7\n\uf8f8=\n\uf8eb\n\uf8ec\n\uf8ed\n\u27e8W 1\u27e91\nw1\n...\n\u27e8W K\u27e91\nwK\n\uf8f6\n\uf8f7\n\uf8f8.\n8. We did some pre-processing, namely lowercasing and encoding capitalization as another feature. With\nenough (unlabeled) training data, presumably we could learn a model without this processing. Ideally,\nan even more raw input would be to learn from letter sequences rather than words, however we felt that\nthis was beyond the scope of this work.\n9\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\nThe matrix output of the lookup table layer for a sequence of words [w]T\n1 is then similar\nto (1), but where extra rows have been added for each discrete feature:\nLTW 1,...,W K([w]T\n1 ) =\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ed\n\u27e8W 1\u27e91\n[w1]1\n. . .\n\u27e8W 1\u27e91\n[w1]T\n...\n...\n\u27e8W K\u27e91\n[wK]1\n. . .\n\u27e8W K\u27e91\n[wK]T\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f8.\n(2)\nThese vector features in the lookup table e\ufb00ectively learn features for words in the dictionary.\nNow, we want to use these trainable features as input to further layers of trainable feature\nextractors, that can represent groups of words and then \ufb01nally sentences.\n3.2 Extracting Higher Level Features from Word Feature Vectors\nFeature vectors produced by the lookup table layer need to be combined in subsequent layers\nof the neural network to produce a tag decision for each word in the sentence. Producing\ntags for each element in variable length sequences (here, a sentence is a sequence of words)\nis a standard problem in machine-learning. We consider two common approaches which tag\none word at the time: a window approach, and a (convolutional) sentence approach.\n3.2.1 Window Approach\nA window approach assumes the tag of a word depends mainly on its neighboring words.\nGiven a word to tag, we consider a \ufb01xed size ksz (a hyper-parameter) window of words\naround this word. Each word in the window is \ufb01rst passed through the lookup table layer (1)\nor (2), producing a matrix of word features of \ufb01xed size dwrd \u00d7 ksz. This matrix can be\nviewed as a dwrd ksz-dimensional vector by concatenating each column vector, which can be\nfed to further neural network layers. More formally, the word feature window given by the\n\ufb01rst network layer can be written as:\nf1\n\u03b8 = \u27e8LTW ([w]T\n1 )\u27e9dwin\nt\n=\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\n\u27e8W\u27e91\n[w]t\u2212dwin/2\n...\n\u27e8W\u27e91\n[w]t\n...\n\u27e8W\u27e91\n[w]t+dwin/2\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\n.\n(3)\nLinear Layer\nThe \ufb01xed size vector f1\n\u03b8 can be fed to one or several standard neural\nnetwork layers which perform a\ufb03ne transformations over their inputs:\nfl\n\u03b8 = W l fl\u22121\n\u03b8\n+ bl ,\n(4)\nwhere W l \u2208Rnl\nhu\u00d7nl\u22121\nhu and bl \u2208Rnl\nhu are the parameters to be trained. The hyper-parameter\nnl\nhu is usually called the number of hidden units of the lth layer.\nHardTanh Layer\nSeveral linear layers are often stacked, interleaved with a non-linearity\nfunction, to extract highly non-linear features. If no non-linearity is introduced, our network\n10\narXiv\nNatural Language Processing (almost) from Scratch\nwould be a simple linear model. We chose a \u201chard\u201d version of the hyperbolic tangent as non-\nlinearity. It has the advantage of being slightly cheaper to compute (compared to the exact\nhyperbolic tangent), while leaving the generalization performance unchanged (Collobert,\n2004). The corresponding layer l applies a HardTanh over its input vector:\nh\nfl\n\u03b8\ni\ni = HardTanh(\nh\nfl\u22121\n\u03b8\ni\ni) ,\nwhere\nHardTanh(x) =\n\uf8f1\n\uf8f2\n\uf8f3\n\u22121\nif x < \u22121\nx\nif \u22121 <= x <= 1\n1\nif x > 1\n.\n(5)\nScoring\nFinally, the output size of the last layer L of our network is equal to the number\nof possible tags for the task of interest. Each output can be then interpreted as a score of\nthe corresponding tag (given the input of the network), thanks to a carefully chosen cost\nfunction that we will describe later in this section.\nRemark 1 (Border E\ufb00ects) The feature window (3) is not well de\ufb01ned for words near\nthe beginning or the end of a sentence. To circumvent this problem, we augment the sentence\nwith a special \u201cPADDING\u201d word replicated dwin/2 times at the beginning and the end. This\nis akin to the use of \u201cstart\u201d and \u201cstop\u201d symbols in sequence models.\n3.2.2 Sentence Approach\nWe will see in the experimental section that a window approach performs well for most\nnatural language processing tasks we are interested in. However this approach fails with\nSRL, where the tag of a word depends on a verb (or, more correctly, predicate) chosen\nbeforehand in the sentence. If the verb falls outside the window, one cannot expect this word\nto be tagged correctly. In this particular case, tagging a word requires the consideration of\nthe whole sentence. When using neural networks, the natural choice to tackle this problem\nbecomes a convolutional approach, \ufb01rst introduced by Waibel et al. (1989) and also called\nTime Delay Neural Networks (TDNNs) in the literature.\nWe describe in detail our convolutional network below. It successively takes the complete\nsentence, passes it through the lookup table layer (1), produces local features around each\nword of the sentence thanks to convolutional layers, combines these feature into a global\nfeature vector which can then be fed to standard a\ufb03ne layers (4). In the semantic role\nlabeling case, this operation is performed for each word in the sentence, and for each verb\nin the sentence. It is thus necessary to encode in the network architecture which verb we\nare considering in the sentence, and which word we want to tag. For that purpose, each\nword at position i in the sentence is augmented with two features in the way described\nin Section 3.1.1. These features encode the relative distances i \u2212posv and i \u2212posw with\nrespect to the chosen verb at position posv, and the word to tag at position posw respectively.\nConvolutional Layer\nA convolutional layer can be seen as a generalization of a window\napproach: given a sequence represented by columns in a matrix fl\u22121\n\u03b8\n(in our lookup table\nmatrix (1)), a matrix-vector operation as in (4) is applied to each window of successive\n11\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\n 0\n 10\n 20\n 30\n 40\n 50\n 60\n 70\nThe\nproposed\nchanges\nalso\nwould\nallow\nexecutives\nto\nreport\nexercises\nof\noptions\nlater\nand\nless\noften\n.\n 0\n 10\n 20\n 30\n 40\n 50\n 60\n 70\nThe\nproposed\nchanges\nalso\nwould\nallow\nexecutives\nto\nreport\nexercises\nof\noptions\nlater\nand\nless\noften\n.\nFigure 3: Number of features chosen at each word position by the Max layer. We consider\na sentence approach network (Figure 2) trained for SRL. The number of \u201clocal\u201d features\noutput by the convolution layer is 300 per word. By applying a Max over the sentence,\nwe obtain 300 features for the whole sentence. It is interesting to see that the network\ncatches features mostly around the verb of interest (here \u201creport\u201d) and word of interest\n(\u201cproposed\u201d (left) or \u201coften\u201d (right)).\nwindows in the sequence. Using previous notations, the tth output column of the lth layer\ncan be computed as:\n\u27e8fl\n\u03b8\u27e91\nt = W l \u27e8fl\u22121\n\u03b8\n\u27e9dwin\nt\n+ bl\n\u2200t ,\n(6)\nwhere the weight matrix W l is the same across all windows t in the sequence. Convolutional\nlayers extract local features around each window of the given sequence. As for standard\na\ufb03ne layers (4), convolutional layers are often stacked to extract higher level features.\nIn this case, each layer must be followed by a non-linearity (5) or the network would be\nequivalent to one convolutional layer.\nMax Layer\nThe size of the output (6) depends on the number of words in the sentence\nfed to the network. Local feature vectors extracted by the convolutional layers have to be\ncombined to obtain a global feature vector, with a \ufb01xed size independent of the sentence\nlength, in order to apply subsequent standard a\ufb03ne layers.\nTraditional convolutional\nnetworks often apply an average (possibly weighted) or a max operation over the \u201ctime\u201d t\nof the sequence (6). (Here, \u201ctime\u201d just means the position in the sentence, this term stems\nfrom the use of convolutional layers in e.g. speech data where the sequence occurs over\ntime.) The average operation does not make much sense in our case, as in general most\nwords in the sentence do not have any in\ufb02uence on the semantic role of a given word to tag.\nInstead, we used a max approach, which forces the network to capture the most useful local\nfeatures produced by the convolutional layers (see Figure 3), for the task at hand. Given a\nmatrix fl\u22121\n\u03b8\noutput by a convolutional layer l \u22121, the Max layer l outputs a vector fl\n\u03b8:\nh\nfl\n\u03b8\ni\ni = max\nt\nh\nfl\u22121\n\u03b8\ni\ni, t\n1 \u2264i \u2264nl\u22121\nhu .\n(7)\nThis \ufb01xed sized global feature vector can be then fed to standard a\ufb03ne network layers (4).\nAs in the window approach, we then \ufb01nally produce one score per possible tag for the given\ntask.\n12\narXiv\nNatural Language Processing (almost) from Scratch\nScheme\nBegin\nInside\nEnd\nSingle\nOther\nIOB\nB-X\nI-X\nI-X\nB-X\nO\nIOE\nI-X\nI-X\nE-X\nE-X\nO\nIOBES\nB-X\nI-X\nE-X\nS-X\nO\nTable 3:\nVarious tagging schemes. Each word in a segment labeled \u201cX\u201d is tagged with a\npre\ufb01xed label, depending of the word position in the segment (begin, inside, end). Single\nword segment labeling is also output. Words not in a labeled segment are labeled \u201cO\u201d.\nVariants of the IOB (and IOE) scheme exist, where the pre\ufb01x B (or E) is replaced by I for\nall segments not contiguous with another segment having the same label \u201cX\u201d.\nRemark 2 The same border e\ufb00ects arise in the convolution operation (6) as in the window\napproach (3). We again work around this problem by padding the sentences with a special\nword.\n3.2.3 Tagging Schemes\nAs explained earlier, the network output layers compute scores for all the possible tags for\nthe task of interest. In the window approach, these tags apply to the word located in the\ncenter of the window. In the (convolutional) sentence approach, these tags apply to the\nword designated by additional markers in the network input.\nThe POS task indeed consists of marking the syntactic role of each word. However, the\nremaining three tasks associate labels with segments of a sentence. This is usually achieved\nby using special tagging schemes to identify the segment boundaries, as shown in Table 3.\nSeveral such schemes have been de\ufb01ned (IOB, IOE, IOBES, . . . ) without clear conclusion\nas to which scheme is better in general. State-of-the-art performance is sometimes obtained\nby combining classi\ufb01ers trained with di\ufb00erent tagging schemes (e.g. Kudo and Matsumoto,\n2001).\nThe ground truth for the NER, CHUNK, and SRL tasks is provided using two di\ufb00erent\ntagging schemes. In order to eliminate this additional source of variations, we have decided\nto use the most expressive IOBES tagging scheme for all tasks. For instance, in the CHUNK\ntask, we describe noun phrases using four di\ufb00erent tags. Tag \u201cS-NP\u201d is used to mark a noun\nphrase containing a single word. Otherwise tags \u201cB-NP\u201d, \u201cI-NP\u201d, and \u201cE-NP\u201d are used\nto mark the \ufb01rst, intermediate and last words of the noun phrase. An additional tag \u201cO\u201d\nmarks words that are not members of a chunk. During testing, these tags are then converted\nto the original IOB tagging scheme and fed to the standard performance evaluation scripts\nmentioned in Section 2.5.\n3.3 Training\nAll our neural networks are trained by maximizing a likelihood over the training data, using\nstochastic gradient ascent. If we denote \u03b8 to be all the trainable parameters of the network,\nwhich are trained using a training set T we want to maximize the following log-likelihood\n13\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\nwith respect to \u03b8:\n\u03b8 7\u2192\nX\n(x, y)\u2208T\nlog p(y | x, \u03b8) ,\n(8)\nwhere x corresponds to either a training word window or a sentence and its associated\nfeatures, and y represents the corresponding tag. The probability p(\u00b7) is computed from the\noutputs of the neural network. We will see in this section two ways of interpreting neural\nnetwork outputs as probabilities.\n3.3.1 Word-Level Log-Likelihood\nIn this approach, each word in a sentence is considered independently.\nGiven an input\nexample x, the network with parameters \u03b8 outputs a score [f\u03b8(x)]i, for the ith tag with\nrespect to the task of interest. To simplify the notation, we drop x from now, and we write\ninstead [f\u03b8]i. This score can be interpreted as a conditional tag probability p(i | x, \u03b8) by\napplying a softmax (Bridle, 1990) operation over all the tags:\np(i | x, \u03b8) =\ne[f\u03b8]i\nP\nj e[f\u03b8]j\n.\n(9)\nDe\ufb01ning the log-add operation as\nlogadd\ni\nzi = log(\nX\ni\nezi) ,\n(10)\nwe can express the log-likelihood for one training example (x, y) as follows:\nlog p(y | x, \u03b8) = [f\u03b8]y \u2212logadd\nj\n[f\u03b8]j .\n(11)\nWhile this training criterion, often referred as cross-entropy is widely used for classi\ufb01cation\nproblems, it might not be ideal in our case, where there is often a correlation between the\ntag of a word in a sentence and its neighboring tags. We now describe another common\napproach for neural networks which enforces dependencies between the predicted tags in a\nsentence.\n3.3.2 Sentence-Level Log-Likelihood\nIn tasks like chunking, NER or SRL we know that there are dependencies between word\ntags in a sentence: not only are tags organized in chunks, but some tags cannot follow\nother tags. Training using a word-level approach discards this kind of labeling information.\nWe consider a training scheme which takes into account the sentence structure: given the\npredictions of all tags by our network for all words in a sentence, and given a score for going\nfrom one tag to another tag, we want to encourage valid paths of tags during training, while\ndiscouraging all other paths.\nWe consider the matrix of scores f\u03b8([x]T\n1 ) output by the network. As before, we drop the\ninput [x]T\n1 for notation simpli\ufb01cation. The element [f\u03b8]i, t of the matrix is the score output\nby the network with parameters \u03b8, for the sentence [x]T\n1 and for the ith tag, at the tth word.\n14\narXiv\nNatural Language Processing (almost) from Scratch\nWe introduce a transition score [A]i, j for jumping from i to j tags in successive words, and\nan initial score [A]i, 0 for starting from the ith tag. As the transition scores are going to be\ntrained (as are all network parameters \u03b8), we de\ufb01ne \u02dc\u03b8 = \u03b8 \u222a{[A]i, j \u2200i, j}. The score of\na sentence [x]T\n1 along a path of tags [i]T\n1 is then given by the sum of transition scores and\nnetwork scores:\ns([x]T\n1 , [i]T\n1 , \u02dc\u03b8) =\nT\nX\nt=1\n\u0010\n[A][i]t\u22121, [i]t + [f\u03b8][i]t, t\n\u0011\n.\n(12)\nExactly as for the word-level likelihood (11), where we were normalizing with respect to all\ntags using a softmax (9), we normalize this score over all possible tag paths [j]T\n1 using a\nsoftmax, and we interpret the resulting ratio as a conditional tag path probability. Taking\nthe log, the conditional probability of the true path [y]T\n1 is therefore given by:\nlog p([y]T\n1 | [x]T\n1 , \u02dc\u03b8) = s([x]T\n1 , [y]T\n1 , \u02dc\u03b8) \u2212logadd\n\u2200[j]T\n1\ns([x]T\n1 , [j]T\n1 , \u02dc\u03b8) .\n(13)\nWhile the number of terms in the logadd operation (11) was equal to the number of tags, it\ngrows exponentially with the length of the sentence in (13). Fortunately, one can compute\nit in linear time with the following standard recursion over t, taking advantage of the\nassociativity and distributivity on the semi-ring9 (R \u222a{\u2212\u221e}, logadd, +):\n\u03b4t(k) \u2206=\nlogadd\n{[j]t\n1 \u2229[j]t=k}\ns([x]t\n1, [j]t\n1, \u02dc\u03b8)\n= logadd\ni\nlogadd\n{[j]t\n1 \u2229[j]t\u22121=i \u2229[j]t=k}\ns([x]t\n1, [j]t\u22121\n1\n, \u02dc\u03b8) + [A][j]t\u22121, k + [f\u03b8]k, t\n= logadd\ni\n\u03b4t\u22121(i) + [A]i, k + [f\u03b8]k, t\n= [f\u03b8]k, t + logadd\ni\n\u0010\n\u03b4t\u22121(i) + [A]i, k\n\u0011\n\u2200k ,\n(14)\nfollowed by the termination\nlogadd\n\u2200[j]T\n1\ns([x]T\n1 , [j]T\n1 , \u02dc\u03b8) = logadd\ni\n\u03b4T (i) .\n(15)\nWe can now maximize in (8) the log-likelihood (13) over all the training pairs ([x]T\n1 , [y]T\n1 ).\nAt inference time, given a sentence [x]T\n1 to tag, we have to \ufb01nd the best tag path which\nminimizes the sentence score (12). In other words, we must \ufb01nd\nargmax\n[j]T\n1\ns([x]T\n1 , [j]T\n1 , \u02dc\u03b8) .\n(16)\nThe Viterbi algorithm is the natural choice for this inference. It corresponds to performing\nthe recursion (14) and (15), but where the logadd is replaced by a max, and then tracking\nback the optimal path through each max.\n9. In other words, read logadd as \u2295and + as \u2297.\n15\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\nRemark 3 (Graph Transformer Networks) Our approach is a particular case of the\ndiscriminative forward training for graph transformer networks (GTNs) (Bottou et al., 1997;\nLe Cun et al., 1998). The log-likelihood (13) can be viewed as the di\ufb00erence between the\nforward score constrained over the valid paths (in our case there is only the labeled path)\nand the unconstrained forward score (15).\nRemark 4 (Conditional Random Fields) An important feature of equation (12) is the\nabsence of normalization.\nSumming the exponentials e [f\u03b8]i, t over all possible tags does\nnot necessarily yield the unity.\nIf this was the case, the scores could be viewed as the\nlogarithms of conditional transition probabilities, and our model would be subject to the\nlabel-bias problem that motivates Conditional Random Fields (CRFs) (La\ufb00erty et al., 2001).\nThe denormalized scores should instead be likened to the potential functions of a CRF.\nIn fact, a CRF maximizes the same likelihood (13) using a linear model instead of a\nnonlinear neural network. CRFs have been widely used in the NLP world, such as for POS\ntagging (La\ufb00erty et al., 2001), chunking (Sha and Pereira, 2003), NER (McCallum and Li,\n2003) or SRL (Cohn and Blunsom, 2005). Compared to such CRFs, we take advantage of\nthe nonlinear network to learn appropriate features for each task of interest.\n3.3.3 Stochastic Gradient\nMaximizing (8) with stochastic gradient (Bottou, 1991) is achieved by iteratively selecting\na random example (x, y) and making a gradient step:\n\u03b8 \u2190\u2212\u03b8 + \u03bb \u2202log p(y | x, \u03b8)\n\u2202\u03b8\n,\n(17)\nwhere \u03bb is a chosen learning rate. Our neural networks described in Figure 1 and Figure 2\nare a succession of layers that correspond to successive composition of functions. The neural\nnetwork is \ufb01nally composed with the word-level log-likelihood (11), or successively composed\nin the recursion (14) if using the sentence-level log-likelihood (13).\nThus, an analytical\nformulation of the derivative (17) can be computed, by applying the di\ufb00erentiation chain\nrule through the network, and through the word-level log-likelihood (11) or through the\nrecurrence (14).\nRemark 5 (Di\ufb00erentiability) Our cost functions are di\ufb00erentiable almost everywhere.\nNon-di\ufb00erentiable points arise because we use a \u201chard\u201d transfer function (5) and because\nwe use a \u201cmax\u201d layer (7) in the sentence approach network.\nFortunately, stochastic\ngradient still converges to a meaningful local minimum despite such minor di\ufb00erentiability\nproblems (Bottou, 1991, 1998). Stochastic gradient iterations that hit a non-di\ufb00erentiability\nare simply skipped.\nRemark 6 (Modular Approach) The well known \u201cback-propagation\u201d algorithm (LeCun,\n1985; Rumelhart et al., 1986) computes gradients using the chain rule. The chain rule can\nalso be used in a modular implementation.10 Our modules correspond to the boxes in Figure 1\nand Figure 2. Given derivatives with respect to its outputs, each module can independently\n10. See http://torch5.sf.net.\n16\narXiv\nNatural Language Processing (almost) from Scratch\nApproach\nPOS\nChunking\nNER\nSRL\n(PWA)\n(F1)\n(F1)\n(F1)\nBenchmark Systems\n97.24\n94.29\n89.31\n77.92\nNN+WLL\n96.31\n89.13\n79.53\n55.40\nNN+SLL\n96.37\n90.33\n81.47\n70.99\nTable 4:\nComparison in generalization performance of benchmark NLP systems with a\nvanilla neural network (NN) approach, on POS, chunking, NER and SRL tasks. We report\nresults with both the word-level log-likelihood (WLL) and the sentence-level log-likelihood\n(SLL). Generalization performance is reported in per-word accuracy rate (PWA) for POS\nand F1 score for other tasks. The NN results are behind the benchmark results, in Section 4\nwe show how to improve these models using unlabeled data.\nTask\nWindow/Conv. size\nWord dim.\nCaps dim.\nHidden units\nLearning rate\nPOS\ndwin = 5\nd0 = 50\nd1 = 5\nn1\nhu = 300\n\u03bb = 0.01\nCHUNK\n\u201d\n\u201d\n\u201d\n\u201d\n\u201d\nNER\n\u201d\n\u201d\n\u201d\n\u201d\n\u201d\nSRL\n\u201d\n\u201d\n\u201d\nn1\nhu = 300\nn2\nhu = 500\n\u201d\nTable 5:\nHyper-parameters of our networks. We report for each task the window size\n(or convolution size), word feature dimension, capital feature dimension, number of hidden\nunits and learning rate.\ncompute derivatives with respect to its inputs and with respect to its trainable parameters,\nas proposed by Bottou and Gallinari (1991). This allows us to easily build variants of our\nnetworks. For details about gradient computations, see Appendix A.\nRemark 7 (Tricks) Many tricks have been reported for training neural networks (LeCun\net al., 1998). Which ones to choose is often confusing. We employed only two of them: the\ninitialization and update of the parameters of each network layer were done according to\nthe \u201cfan-in\u201d of the layer, that is the number of inputs used to compute each output of this\nlayer (Plaut and Hinton, 1987). The fan-in for the lookup table (1), the lth linear layer (4)\nand the convolution layer (6) are respectively 1, nl\u22121\nhu and dwin\u00d7nl\u22121\nhu . The initial parameters\nof the network were drawn from a centered uniform distribution, with a variance equal to\nthe inverse of the square-root of the fan-in. The learning rate in (17) was divided by the\nfan-in, but stays \ufb01xed during the training.\n3.4 Supervised Benchmark Results\nFor POS, chunking and NER tasks, we report results with the window architecture described\nin Section 3.2.1. The SRL task was trained using the sentence approach (Section 3.2.2).\nResults are reported in Table 4, in per-word accuracy (PWA) for POS, and F1 score for all\n17\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\nfrance\njesus\nxbox\nreddish\nscratched\nmegabits\n454\n1973\n6909\n11724\n29869\n87025\npersuade\nthickets\ndecadent\nwidescreen\nodd\nppa\nfaw\nsavary\ndivo\nantica\nanchieta\nuddin\nblackstock\nsympathetic\nverus\nshabby\nemigration\nbiologically\ngiorgi\njfk\noxide\nawe\nmarking\nkayak\nshaheed\nkhwarazm\nurbina\nthud\nheuer\nmclarens\nrumelia\nstationery\nepos\noccupant\nsambhaji\ngladwin\nplanum\nilias\neglinton\nrevised\nworshippers\ncentrally\ngoa\u2019uld\ngsNUMBER\nedging\nleavened\nritsuko\nindonesia\ncollation\noperator\nfrg\npandionidae\nlifeless\nmoneo\nbacha\nw.j.\nnamsos\nshirt\nmahan\nnilgiris\nTable 6: Word embeddings in the word lookup table of a SRL neural network trained from\nscratch, with a dictionary of size 100, 000. For each column the queried word is followed by\nits index in the dictionary (higher means more rare) and its 10 nearest neighbors (arbitrary\nusing the Euclidean metric).\nthe other tasks. We performed experiments both with the word-level log-likelihood (WLL)\nand with the sentence-level log-likelihood (SLL). The hyper-parameters of our networks are\nreported in Table 5. All our networks were fed with two raw text features: lower case words,\nand a capital letter feature. We chose to consider lower case words to limit the number\nof words in the dictionary.\nHowever, to keep some upper case information lost by this\ntransformation, we added a \u201ccaps\u201d feature which tells if each word was in low caps, was all\ncaps, had \ufb01rst letter capital, or had one capital. Additionally, all occurrences of sequences\nof numbers within a word are replaced with the string \u201cNUMBER\u201d, so for example both the\nwords \u201cPS1\u201d and \u201cPS2\u201d would map to the single word \u201cpsNUMBER\u201d. We used a dictionary\ncontaining the 100,000 most common words in WSJ (case insensitive). Words outside this\ndictionary were replaced by a single special \u201cRARE\u201d word.\nResults show that neural networks \u201cout-of-the-box\u201d are behind baseline benchmark\nsystems.\nLooking at all submitted systems reported on each CoNLL challenge website\nshowed us our networks performance are nevertheless in the performance ballpark of existing\napproaches. The training criterion which takes into account the sentence structure (SLL)\nseems to boost the performance for the Chunking, NER and SRL tasks, with little advantage\nfor POS. This result is in line with existing NLP studies comparing sentence-level and word-\nlevel likelihoods (Liang et al., 2008). The capacity of our network architectures lies mainly\nin the word lookup table, which contains 50\u00d7100, 000 parameters to train. In the WSJ data,\n15% of the most common words appear about 90% of the time. Many words appear only\na few times. It is thus very di\ufb03cult to train properly their corresponding 50 dimensional\nfeature vectors in the lookup table. Ideally, we would like semantically similar words to be\nclose in the embedding space represented by the word lookup table: by continuity of the\nneural network function, tags produced on semantically similar sentences would be similar.\nWe show in Table 6 that it is not the case: neighboring words in the embedding space do\nnot seem to be semantically related.\n18\narXiv\nNatural Language Processing (almost) from Scratch\n 95.5\n 96\n 96.5\n 100\n 300\n 500\n 700\n 900\n(a) POS\n 90\n 90.5\n 91\n 91.5\n 100\n 300\n 500\n 700\n 900\n(b) CHUNK\n 85\n 85.5\n 86\n 86.5\n 100\n 300\n 500\n 700\n 900\n(c) NER\n 67\n 67.5\n 68\n 68.5\n 69\n 100\n 300\n 500\n 700\n 900\n(d) SRL\nFigure 4:\nF1 score on the validation set (y-axis) versus number of hidden units (x-axis)\nfor di\ufb00erent tasks trained with the sentence-level likelihood (SLL), as in Table 4. For SRL,\nwe vary in this graph only the number of hidden units in the second layer. The scale is\nadapted for each task. We show the standard deviation (obtained over 5 runs with di\ufb00erent\nrandom initialization), for the architecture we picked (300 hidden units for POS, CHUNK\nand NER, 500 for SRL).\nWe will focus in the next section on improving these word embeddings by leveraging\nunlabeled data. We will see our approach results in a performance boost for all tasks.\nRemark 8 (Architectures) In all our experiments in this paper, we tuned the hyper-\nparameters by trying only a few di\ufb00erent architectures by validation. In practice, the choice\nof hyperparameters such as the number of hidden units, provided they are large enough, has\na limited impact on the generalization performance. In Figure 4, we report the F1 score\nfor each task on the validation set, with respect to the number of hidden units. Considering\nthe variance related to the network initialization, we chose the smallest network achieving\n\u201creasonable\u201d performance, rather than picking the network achieving the top performance\nobtained on a single run.\nRemark 9 (Training Time) Training our network is quite computationally expensive.\nChunking and NER take about one hour to train, POS takes few hours, and SRL takes\nabout three days. Training could be faster with a larger learning rate, but we prefered to\nstick to a small one which works, rather than \ufb01nding the optimal one for speed. Second\norder methods (LeCun et al., 1998) could be another speedup technique.\n4. Lots of Unlabeled Data\nWe would like to obtain word embeddings carrying more syntactic and semantic information\nthan shown in Table 6. Since most of the trainable parameters of our system are associated\nwith the word embeddings, these poor results suggest that we should use considerably\nmore training data.\nFollowing our NLP from scratch philosophy, we now describe how\nto dramatically improve these embeddings using large unlabeled datasets. We then use\nthese improved embeddings to initialize the word lookup tables of the networks described\nin Section 3.4.\n19\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\n4.1 Datasets\nOur \ufb01rst English corpus is the entire English Wikipedia.11 We have removed all paragraphs\ncontaining non-roman characters and all MediaWiki markups.\nThe resulting text was\ntokenized using the Penn Treebank tokenizer script.12 The resulting dataset contains about\n631 million words.\nAs in our previous experiments, we use a dictionary containing the\n100,000 most common words in WSJ, with the same processing of capitals and numbers.\nAgain, words outside the dictionary were replaced by the special \u201cRARE\u201d word.\nOur second English corpus is composed by adding an extra 221 million words extracted\nfrom the Reuters RCV1 (Lewis et al., 2004) dataset.13 We also extended the dictionary to\n130, 000 words by adding the 30, 000 most common words in Reuters. This is useful in order\nto determine whether improvements can be achieved by further increasing the unlabeled\ndataset size.\n4.2 Ranking Criterion versus Entropy Criterion\nWe used these unlabeled datasets to train language models that compute scores describing\nthe acceptability of a piece of text. These language models are again large neural networks\nusing the window approach described in Section 3.2.1 and in Figure 1. As in the previous\nsection, most of the trainable parameters are located in the lookup tables.\nSimilar language models were already proposed by Bengio and Ducharme (2001) and\nSchwenk and Gauvain (2002). Their goal was to estimate the probability of a word given\nthe previous words in a sentence. Estimating conditional probabilities suggests a cross-\nentropy criterion similar to those described in Section 3.3.1. Because the dictionary size is\nlarge, computing the normalization term can be extremely demanding, and sophisticated\napproximations are required. More importantly for us, neither work leads to signi\ufb01cant\nword embeddings being reported.\nShannon (1951) has estimated the entropy of the English language between 0.6 and 1.3\nbits per character by asking human subjects to guess upcoming characters. Cover and King\n(1978) give a lower bound of 1.25 bits per character using a subtle gambling approach.\nMeanwhile, using a simple word trigram model, Brown et al. (1992b) reach 1.75 bits per\ncharacter. Teahan and Cleary (1996) obtain entropies as low as 1.46 bits per character\nusing variable length character n-grams. The human subjects rely of course on all their\nknowledge of the language and of the world. Can we learn the grammatical structure of the\nEnglish language and the nature of the world by leveraging the 0.2 bits per character that\nseparate human subjects from simple n-gram models? Since such tasks certainly require\nhigh capacity models, obtaining su\ufb03ciently small con\ufb01dence intervals on the test set entropy\nmay require prohibitively large training sets.14 The entropy criterion lacks dynamical range\nbecause its numerical value is largely determined by the most frequent phrases. In order to\nlearn syntax, rare but legal phrases are no less signi\ufb01cant than common phrases.\n11. Available at http://download.wikimedia.org. We took the November 2007 version.\n12. Available at http://www.cis.upenn.edu/~treebank/tokenization.html.\n13. Now available at http://trec.nist.gov/data/reuters/reuters.html.\n14. However, Klein and Manning (2002) describe a rare example of realistic unsupervised grammar induction\nusing a cross-entropy approach on binary-branching parsing trees, that is, by forcing the system to\ngenerate a hierarchical representation.\n20\narXiv\nNatural Language Processing (almost) from Scratch\nIt is therefore desirable to de\ufb01ne alternative training criteria. We propose here to use a\npairwise ranking approach (Cohen et al., 1998). We seek a network that computes a higher\nscore when given a legal phrase than when given an incorrect phrase. Because the ranking\nliterature often deals with information retrieval applications, many authors de\ufb01ne complex\nranking criteria that give more weight to the ordering of the best ranking instances (see\nBurges et al., 2007; Cl\u00b4emen\u00b8con and Vayatis, 2007). However, in our case, we do not want\nto emphasize the most common phrase over the rare but legal phrases. Therefore we use a\nsimple pairwise criterion.\nWe consider a window approach network, as described in Section 3.2.1 and Figure 1,\nwith parameters \u03b8 which outputs a score f\u03b8(x) given a window of text x = [w]dwin\n1\n. We\nminimize the ranking criterion with respect to \u03b8:\n\u03b8 7\u2192\nX\nx\u2208X\nX\nw\u2208D\nmax\nn\n0 , 1 \u2212f\u03b8(x) + f\u03b8(x(w))\no\n,\n(18)\nwhere X is the set of all possible text windows with dwin words coming from our training\ncorpus, D is the dictionary of words, and x(w) denotes the text window obtained by replacing\nthe central word of text window [w]dwin\n1\nby the word w.\nOkanohara and Tsujii (2007) use a related approach to avoiding the entropy criteria\nusing a binary classi\ufb01cation approach (correct/incorrect phrase). Their work focuses on\nusing a kernel classi\ufb01er, and not on learning word embeddings as we do here. Smith and\nEisner (2005) also propose a contrastive criterion which estimates the likelihood of the data\nconditioned to a \u201cnegative\u201d neighborhood.\nThey consider various data neighborhoods,\nincluding sentences of length dwin drawn from Ddwin. Their goal was however to perform\nwell on some tagging task on fully unsupervised data, rather than obtaining generic word\nembeddings useful for other tasks.\n4.3 Training Language Models\nThe language model network was trained by stochastic gradient minimization of the ranking\ncriterion (18), sampling a sentence-word pair (s, w) at each iteration.\nSince training times for such large scale systems are counted in weeks, it is not feasible\nto try many combinations of hyperparameters. It also makes sense to speed up the training\ntime by initializing new networks with the embeddings computed by earlier networks. In\nparticular, we found it expedient to train a succession of networks using increasingly large\ndictionaries, each network being initialized with the embeddings of the previous network.\nSuccessive dictionary sizes and switching times are chosen arbitrarily. (Bengio et al., 2009)\nprovides a more detailed discussion of this, the (as yet, poorly understood) \u201ccurriculum\u201d\nprocess.\nFor the purposes of model selection we use the process of \u201cbreeding\u201d.\nThe idea of\nbreeding is instead of trying a full grid search of possible values (which we did not have\nenough computing power for) to search for the parameters in anology to breeding biological\ncell lines. Within each line, child networks are initialized with the embeddings of their\nparents and trained on increasingly rich datasets with sometimes di\ufb00erent parameters. That\nis, suppose we have k processors, which is much less than the possible set of parameters\none would like to try. One chooses k initial parameter choices from the large set, and trains\n21\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\nthese on the k processors. In our case, possible parameters to adjust are: the learning rate\n\u03bb, the word embedding dimensions d, number of hidden units n1\nhu and input window size\ndwin. One then trains each of these models in an online fashion for a certain amount of\ntime (i.e. a few days), and then selects the best ones using the validation set error rate.\nThat is, breeding decisions were made on the basis of the value of the ranking criterion (18)\nestimated on a validation set composed of one million words held out from the Wikipedia\ncorpus. In the next breeding iteration, one then chooses another set of k parameters from\nthe possible grid of values that permute slightly the most successful candidates from the\nprevious round. As many of these parameter choices can share weights, we can e\ufb00ectively\ncontinue online training retaining some of the learning from the previous iterations.\nVery long training times make such strategies necessary for the foreseeable future: if we\nhad been given computers ten times faster, we probably would have found uses for datasets\nten times bigger. However, we should say we believe that although we ended up with a\nparticular choice of parameters, many other choices are almost equally as good, although\nperhaps there are others that are better as we could not do a full grid search.\nIn the following subsections, we report results obtained with two trained language\nmodels.\nThe results achieved by these two models are representative of those achieved\nby networks trained on the full corpuses.\n\u2022 Language model LM1 has a window size dwin = 11 and a hidden layer with n1\nhu = 100\nunits. The embedding layers were dimensioned like those of the supervised networks\n(Table 5).\nModel LM1 was trained on our \ufb01rst English corpus (Wikipedia) using\nsuccessive dictionaries composed of the 5000, 10, 000, 30, 000, 50, 000 and \ufb01nally\n100, 000 most common WSJ words. The total training time was about four weeks.\n\u2022 Language model LM2 has the same dimensions. It was initialized with the embeddings\nof LM1, and trained for an additional three weeks on our second English corpus\n(Wikipedia+Reuters) using a dictionary size of 130,000 words.\n4.4 Embeddings\nBoth networks produce much more appealing word embeddings than in Section 3.4. Table 7\nshows the ten nearest neighbors of a few randomly chosen query words for the LM1 model.\nThe syntactic and semantic properties of the neighbors are clearly related to those of the\nquery word.\nThese results are far more satisfactory than those reported in Table 7 for\nembeddings obtained using purely supervised training of the benchmark NLP tasks.\n4.5 Semi-supervised Benchmark Results\nSemi-supervised learning has been the object of much attention during the last few years (see\nChapelle et al., 2006).\nPrevious semi-supervised approaches for NLP can be roughly\ncategorized as follows:\n\u2022 Ad-hoc approaches such as (Rosenfeld and Feldman, 2007) for relation extraction.\n\u2022 Self-training approaches, such as (Ue\ufb03ng et al., 2007) for machine translation,\nand (McClosky et al., 2006) for parsing. These methods augment the labeled training\n22\narXiv\nNatural Language Processing (almost) from Scratch\nfrance\njesus\nxbox\nreddish\nscratched\nmegabits\n454\n1973\n6909\n11724\n29869\n87025\naustria\ngod\namiga\ngreenish\nnailed\noctets\nbelgium\nsati\nplaystation\nbluish\nsmashed\nmb/s\ngermany\nchrist\nmsx\npinkish\npunched\nbit/s\nitaly\nsatan\nipod\npurplish\npopped\nbaud\ngreece\nkali\nsega\nbrownish\ncrimped\ncarats\nsweden\nindra\npsNUMBER\ngreyish\nscraped\nkbit/s\nnorway\nvishnu\nhd\ngrayish\nscrewed\nmegahertz\neurope\nananda\ndreamcast\nwhitish\nsectioned\nmegapixels\nhungary\nparvati\ngeforce\nsilvery\nslashed\ngbit/s\nswitzerland\ngrace\ncapcom\nyellowish\nripped\namperes\nTable 7: Word embeddings in the word lookup table of the language model neural network\nLM1 trained with a dictionary of size 100, 000. For each column the queried word is followed\nby its index in the dictionary (higher means more rare) and its 10 nearest neighbors (using\nthe Euclidean metric, which was chosen arbitrarily).\nset with examples from the unlabeled dataset using the labels predicted by the model\nitself. Transductive approaches, such as (Joachims, 1999) for text classi\ufb01cation can\nbe viewed as a re\ufb01ned form of self-training.\n\u2022 Parameter sharing approaches such as (Ando and Zhang, 2005; Suzuki and Isozaki,\n2008).\nAndo and Zhang propose a multi-task approach where they jointly train\nmodels sharing certain parameters. They train POS and NER models together with a\nlanguage model (trained on 15 million words) consisting of predicting words given the\nsurrounding tokens. Suzuki and Isozaki embed a generative model (Hidden Markov\nModel) inside a CRF for POS, Chunking and NER. The generative model is trained\non one billion words. These approaches should be seen as a linear counterpart of our\nwork. Using multilayer models vastly expands the parameter sharing opportunities\n(see Section 5).\nOur approach simply consists of initializing the word lookup tables of the supervised\nnetworks with the embeddings computed by the language models. Supervised training is\nthen performed as in Section 3.4.\nIn particular the supervised training stage is free to\nmodify the lookup tables. This sequential approach is computationally convenient because\nit separates the lengthy training of the language models from the relatively fast training of\nthe supervised networks. Once the language models are trained, we can perform multiple\nexperiments on the supervised networks in a relatively short time. Note that our procedure\nis clearly linked to the (semi-supervised) deep learning procedures of (Hinton et al., 2006;\nBengio et al., 2007; Weston et al., 2008).\nTable 8 clearly shows that this simple initialization signi\ufb01cantly boosts the generalization\nperformance of the supervised networks for each task. It is worth mentioning the larger\nlanguage model led to even better performance.\nThis suggests that we could still take\nadvantage of even bigger unlabeled datasets.\n23\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\nApproach\nPOS\nCHUNK\nNER\nSRL\n(PWA)\n(F1)\n(F1)\n(F1)\nBenchmark Systems\n97.24\n94.29\n89.31\n77.92\nNN+WLL\n96.31\n89.13\n79.53\n55.40\nNN+SLL\n96.37\n90.33\n81.47\n70.99\nNN+WLL+LM1\n97.05\n91.91\n85.68\n58.18\nNN+SLL+LM1\n97.10\n93.65\n87.58\n73.84\nNN+WLL+LM2\n97.14\n92.04\n86.96\n58.34\nNN+SLL+LM2\n97.20\n93.63\n88.67\n74.15\nTable 8:\nComparison in generalization performance of benchmark NLP systems with our\n(NN) approach on POS, chunking, NER and SRL tasks. We report results with both the\nword-level log-likelihood (WLL) and the sentence-level log-likelihood (SLL). We report with\n(LMn) performance of the networks trained from the language model embeddings (Table 7).\nGeneralization performance is reported in per-word accuracy (PWA) for POS and F1 score\nfor other tasks.\n4.6 Ranking and Language\nThere is a large agreement in the NLP community that syntax is a necessary prerequisite for\nsemantic role labeling (Gildea and Palmer, 2002). This is why state-of-the-art semantic role\nlabeling systems thoroughly exploit multiple parse trees. The parsers themselves (Charniak,\n2000; Collins, 1999) contain considerable prior information about syntax (one can think of\nthis as a kind of informed pre-processing).\nOur system does not use such parse trees because we attempt to learn this information\nfrom the unlabeled data set. It is therefore legitimate to question whether our ranking\ncriterion (18) has the conceptual capability to capture such a rich hierarchical information.\nAt \ufb01rst glance, the ranking task appears unrelated to the induction of probabilistic\ngrammars that underly standard parsing algorithms. The lack of hierarchical representation\nseems a fatal \ufb02aw (Chomsky, 1956).\nHowever, ranking is closely related to an alternative description of the language\nstructure: operator grammars (Harris, 1968). Instead of directly studying the structure\nof a sentence, Harris de\ufb01nes an algebraic structure on the space of all sentences. Starting\nfrom a couple of elementary sentence forms, sentences are described by the successive\napplication of sentence transformation operators.\nThe sentence structure is revealed as\na side e\ufb00ect of the successive transformations. Sentence transformations can also have a\nsemantic interpretation.\nIn the spirit of structural linguistics, Harris describes procedures to discover sentence\ntransformation operators by leveraging the statistical regularities of the language. Such\nprocedures are obviously useful for machine learning approaches. In particular, he proposes\na test to decide whether two sentences forms are semantically related by a transformation\noperator. He \ufb01rst de\ufb01nes a ranking criterion (Harris, 1968, section 4.1):\n\u201cStarting for convenience with very short sentence forms, say ABC, we\nchoose a particular word choice for all the classes, say BqCq, except one, in\n24\narXiv\nNatural Language Processing (almost) from Scratch\nthis case A; for every pair of members Ai, Aj of that word class we ask how\nthe sentence formed with one of the members, i.e. AiBqCq compares as to\nacceptability with the sentence formed with the other member, i.e. AjBqCq.\u201d\nThese gradings are then used to compare sentence forms:\n\u201cIt now turns out that, given the graded n-tuples of words for a particular\nsentence form, we can \ufb01nd other sentences forms of the same word classes in\nwhich the same n-tuples of words produce the same grading of sentences.\u201d\nThis is an indication that these two sentence forms exploit common words with the same\nsyntactic function and possibly the same meaning. This observation forms the empirical\nbasis for the construction of operator grammars that describe real-world natural languages\nsuch as English.\nTherefore there are solid reasons to believe that the ranking criterion (18) has the\nconceptual potential to capture strong syntactic and semantic information. On the other\nhand, the structure of our language models is probably too restrictive for such goals, and\nour current approach only exploits the word embeddings discovered during training.\n5. Multi-Task Learning\nIt is generally accepted that features trained for one task can be useful for related tasks. This\nidea was already exploited in the previous section when certain language model features,\nnamely the word embeddings, were used to initialize the supervised networks.\nMulti-task learning (MTL) leverages this idea in a more systematic way. Models for\nall tasks of interests are jointly trained with an additional linkage between their trainable\nparameters in the hope of improving the generalization error. This linkage can take the form\nof a regularization term in the joint cost function that biases the models towards common\nrepresentations.\nA much simpler approach consists in having the models share certain\nparameters de\ufb01ned a priori. Multi-task learning has a long history in machine learning and\nneural networks. Caruana (1997) gives a good overview of these past e\ufb00orts.\n5.1 Joint Decoding versus Joint Training\nMultitask approaches do not necessarily involve joint training. For instance, modern speech\nrecognition systems use Bayes rule to combine the outputs of an acoustic model trained on\nspeech data and a language model trained on phonetic or textual corpora (Jelinek, 1976).\nThis joint decoding approach has been successfully applied to structurally more complex\nNLP tasks.\nSutton and McCallum (2005b) obtains improved results by combining the\npredictions of independently trained CRF models using a joint decoding process at test\ntime that requires more sophisticated probabilistic inference techniques.\nOn the other\nhand, Sutton and McCallum (2005a) obtain results somewhat below the state-of-the-art\nusing joint decoding for SRL and syntactic parsing. Musillo and Merlo (2006) also describe\na negative result at the same joint task.\nJoint decoding invariably works by considering additional probabilistic dependency\npaths between the models.\nTherefore it de\ufb01nes an implicit supermodel that describes\nall the tasks in the same probabilistic framework. Separately training a submodel only\n25\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\nApproach\nPOS\nCHUNK\nNER\nSRL\n(PWA)\n(F1)\n(F1)\n(F1)\nBenchmark Systems\n97.24\n94.29\n89.31\n\u2013\nWindow Approach\nNN+SLL+LM2\n97.20\n93.63\n88.67\n\u2013\nNN+SLL+LM2+MTL\n97.22\n94.10\n88.62\n\u2013\nSentence Approach\nNN+SLL+LM2\n97.12\n93.37\n88.78\n74.15\nNN+SLL+LM2+MTL\n97.22\n93.75\n88.27\n74.29\nTable 9:\nE\ufb00ect of multi-tasking on our neural architectures. We trained POS, CHUNK\nNER in a MTL way, both for the window and sentence network approaches. SRL was only\nincluded in the sentence approach joint training. As a baseline, we show previous results\nof our window approach system, as well as additional results for our sentence approach\nsystem, when trained separately on each task. Benchmark system performance is also given\nfor comparison.\nmakes sense when the training data blocks these additional dependency paths (in the sense\nof d-separation, Pearl, 1988).\nThis implies that, without joint training, the additional\ndependency paths cannot directly involve unobserved variables. Therefore, the natural idea\nof discovering common internal representations across tasks requires joint training.\nJoint training is relatively straightforward when the training sets for the individual\ntasks contain the same patterns with di\ufb00erent labels. It is then su\ufb03cient to train a model\nthat computes multiple outputs for each pattern (Suddarth and Holden, 1991).\nUsing\nthis scheme, Sutton et al. (2007) demonstrates improvements on POS tagging and noun-\nphrase chunking using jointly trained CRFs. However the joint labeling requirement is a\nlimitation because such data is not often available. Miller et al. (2000) achieves performance\nimprovements by jointly training NER, parsing, and relation extraction in a statistical\nparsing model. The joint labeling requirement problem was weakened using a predictor to\n\ufb01ll in the missing annotations.\nAndo and Zhang (2005) propose a setup that works around the joint labeling\nrequirements. They de\ufb01ne linear models of the form fi(x) = w\u22a4\ni \u03a6(x) + v\u22a4\ni \u0398\u03a8(x) where\nfi is the classi\ufb01er for the i-th task with parameters wi and vi. Notations \u03a6(x) and \u03a8(x)\nrepresent engineered features for the pattern x. Matrix \u0398 maps the \u03a8(x) features into a low\ndimensional subspace common across all tasks. Each task is trained using its own examples\nwithout a joint labeling requirement. The learning procedure alternates the optimization\nof wi and vi for each task, and the optimization of \u0398 to minimize the average loss for all\nexamples in all tasks. The authors also consider auxiliary unsupervised tasks for predicting\nsubstructures. They report excellent results on several tasks, including POS and NER.\n26\narXiv\nNatural Language Processing (almost) from Scratch\nLookup Table\nLinear\nLookup Table\nLinear\nHardTanh\nHardTanh\nLinear\nTask 1\nLinear\nTask 2\nM 2\n(t1) \u00d7 \u00b7\nM 2\n(t2) \u00d7 \u00b7\nLTW 1\n...\nLTW K\nM 1 \u00d7 \u00b7\nn1\nhu\nn1\nhu\nn2\nhu,(t1) = #tags\nn2\nhu,(t2) = #tags\nFigure 5: Example of multitasking with NN. Task 1 and Task 2 are two tasks trained with\nthe window approach architecture presented in Figure 1. Lookup tables as well as the \ufb01rst\nhidden layer are shared. The last layer is task speci\ufb01c. The principle is the same with more\nthan two tasks.\n5.2 Multi-Task Benchmark Results\nTable 9 reports results obtained by jointly trained models for the POS, CHUNK, NER and\nSRL tasks using the same setup as Section 4.5. We trained jointly POS, CHUNK and NER\nusing the window approach network. As we mentioned earlier, SRL can be trained only\nwith the sentence approach network, due to long-range dependencies related to the verb\npredicate. We thus also trained all four tasks using the sentence approach network. In\nboth cases, all models share the lookup table parameters (2). The parameters of the \ufb01rst\nlinear layers (4) were shared in the window approach case (see Figure 5), and the \ufb01rst the\nconvolution layer parameters (6) were shared in the sentence approach networks.\nFor the window approach, best results were obtained by enlarging the \ufb01rst hidden layer\nsize to n1\nhu = 500 (chosen by validation) in order to account for its shared responsibilities.\nWe used the same architecture than SRL for the sentence approach network. The word\nembedding dimension was kept constant d0 = 50 in order to reuse the language models\nof Section 4.5.\nTraining was achieved by minimizing the loss averaged across all tasks. This is easily\nachieved with stochastic gradient by alternatively picking examples for each task and\napplying (17) to all the parameters of the corresponding model, including the shared\nparameters. Note that this gives each task equal weight. Since each task uses the training\nsets described in Table 1, it is worth noticing that examples can come from quite di\ufb00erent\n27\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\nApproach\nPOS\nCHUNK\nNER\nSRL\n(PWA)\n(F1)\n(F1)\nBenchmark Systems\n97.24\n94.29\n89.31\n77.92\nNN+SLL+LM2\n97.20\n93.63\n88.67\n74.15\nNN+SLL+LM2+Su\ufb03x2\n97.29\n\u2013\n\u2013\n\u2013\nNN+SLL+LM2+Gazetteer\n\u2013\n\u2013\n89.59\n\u2013\nNN+SLL+LM2+POS\n\u2013\n94.32\n88.67\n\u2013\nNN+SLL+LM2+CHUNK\n\u2013\n\u2013\n\u2013\n74.72\nTable 10:\nComparison in generalization performance of benchmark NLP systems with\nour neural networks (NNs) using increasing task-speci\ufb01c engineering. We report results\nobtained with a network trained without the extra task-speci\ufb01c features (Section 5) and\nwith the extra task-speci\ufb01c features described in Section 6. The POS network was trained\nwith two character word su\ufb03xes; the NER network was trained using the small CoNLL\n2003 gazetteer; the CHUNK and NER networks were trained with additional POS features;\nand \ufb01nally, the SRL network was trained with additional CHUNK features.\ndatasets. The generalization performance for each task was measured using the traditional\ntesting data speci\ufb01ed in Table 1. Fortunately, none of the training and test sets overlap\nacross tasks.\nWhile we \ufb01nd worth mentioning that MTL can produce a single uni\ufb01ed architecture that\nperforms well for all these tasks, no (or only marginal) improvements were obtained with\nthis approach compared to training separate architectures per task (which still use semi-\nsupervised learning, which is somehow the most important MTL task). The next section\nshows we can leverage known correlations between tasks in more direct manner.\n6. The Temptation\nResults so far have been obtained by staying (almost15) true to our from scratch philosophy.\nWe have so far avoided specializing our architecture for any task, disregarding a lot of useful\na priori NLP knowledge. We have shown that, thanks to large unlabeled datasets, our\ngeneric neural networks can still achieve close to state-of-the-art performance by discovering\nuseful features. This section explores what happens when we increase the level of task-\nspeci\ufb01c engineering in our systems by incorporating some common techniques from the\nNLP literature. We often obtain further improvements. These \ufb01gures are useful to quantify\nhow far we went by leveraging large datasets instead of relying on a priori knowledge.\n6.1 Su\ufb03x Features\nWord su\ufb03xes in many western languages are strong predictors of the syntactic function\nof the word and therefore can bene\ufb01t the POS system. For instance, Ratnaparkhi (1996)\n15. We did some basic preprocessing of the raw input words as described in Section 3.4, hence the \u201calmost\u201d\nin the title of this article. A completely from scratch approach would presumably not know anything\nabout words at all and would work from letters only (or, taken to a further extreme, from speech or\noptical character recognition, as humans do).\n28\narXiv\nNatural Language Processing (almost) from Scratch\nuses inputs representing word su\ufb03xes and pre\ufb01xes up to four characters. We achieve this\nin the POS task by adding discrete word features (Section 3.1.1) representing the last two\ncharacters of every word. The size of the su\ufb03x dictionary was 455. This led to a small\nimprovement of the POS performance (Table 10, row NN+SLL+LM2+Su\ufb03x2). We also tried\nsu\ufb03xes obtained with the Porter (1980) stemmer and obtained the same performance as\nwhen using two character su\ufb03xes.\n6.2 Gazetteers\nState-of-the-art NER systems often use a large dictionary containing well known named\nentities (e.g. Florian et al., 2003).\nWe restricted ourselves to the gazetteer provided\nby the CoNLL challenge, containing 8, 000 locations, person names, organizations, and\nmiscellaneous entities. We trained a NER network with 4 additional word features indicating\n(feature \u201con\u201d or \u201co\ufb00\u201d) whether the word is found in the gazetteer under one of these four\ncategories. The gazetteer includes not only words, but also chunks of words. If a sentence\nchunk is found in the gazetteer, then all words in the chunk have their corresponding\ngazetteer feature turned to \u201con\u201d.\nThe resulting system displays a clear performance\nimprovement (Table 10, row NN+SLL+LM2+Gazetteer), slightly outperforming the baseline.\nA plausible explanation of this large boost over the network using only the language model\nis that gazeetters include word chunks, while we use only the word representation of our\nlanguage model. For example, \u201cunited\u201d and \u201cbicycle\u201d seen separately are likely to be non-\nentities, while \u201cunited bicycle\u201d might be an entity, but catching it would require higher\nlevel representations of our language model.\n6.3 Cascading\nWhen one considers related tasks, it is reasonable to assume that tags obtained for one task\ncan be useful for taking decisions in the other tasks. Conventional NLP systems often use\nfeatures obtained from the output of other preexisting NLP systems. For instance, Shen\nand Sarkar (2005) describe a chunking system that uses POS tags as input; Florian et al.\n(2003) describes a NER system whose inputs include POS and CHUNK tags, as well as\nthe output of two other NER classi\ufb01ers. State-of-the-art SRL systems exploit parse trees\n(Gildea and Palmer, 2002; Punyakanok et al., 2005), related to CHUNK tags, and built\nusing POS tags (Charniak, 2000; Collins, 1999).\nTable 10 reports results obtained for the CHUNK and NER tasks by adding discrete\nword features (Section 3.1.1) representing the POS tags. In order to facilitate comparisons,\ninstead of using the more accurate tags from our POS network, we use for each task the\nPOS tags provided by the corresponding CoNLL challenge. We also report results obtained\nfor the SRL task by adding word features representing the CHUNK tags (also provided by\nthe CoNLL challenge). We consistently obtain moderate improvements.\n6.4 Ensembles\nConstructing ensembles of classi\ufb01ers is a proven way to trade computational e\ufb03ciency for\ngeneralization performance (Bell et al., 2007). Therefore it is not surprising that many\nNLP systems achieve state-of-the-art performance by combining the outputs of multiple\n29\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\nApproach\nPOS\nCHUNK\nNER\n(PWA)\n(F1)\n(F1)\nBenchmark Systems\n97.24\n94.29\n89.31\nNN+SLL+LM2+POS\nworst\n97.29\n93.99\n89.35\nNN+SLL+LM2+POS\nmean\n97.31\n94.17\n89.65\nNN+SLL+LM2+POS\nbest\n97.35\n94.32\n89.86\nNN+SLL+LM2+POS\nvoting ensemble\n97.37\n94.34\n89.70\nNN+SLL+LM2+POS\njoined ensemble\n97.30\n94.35\n89.67\nTable 11: Comparison in generalization performance for POS, CHUNK and NER tasks of\nthe networks obtained using by combining ten training runs with di\ufb00erent initialization.\nclassi\ufb01ers. For instance, Kudo and Matsumoto (2001) use an ensemble of classi\ufb01ers trained\nwith di\ufb00erent tagging conventions (see Section 3.2.3). Winning a challenge is of course a\nlegitimate objective. Yet it is often di\ufb03cult to \ufb01gure out which ideas are most responsible\nfor the state-of-the-art performance of a large ensemble.\nBecause neural networks are nonconvex, training runs with di\ufb00erent initial parameters\nusually give di\ufb00erent solutions.\nTable 11 reports results obtained for the CHUNK and\nNER task after ten training runs with random initial parameters. Voting the ten network\noutputs on a per tag basis (\u201cvoting ensemble\u201d) leads to a small improvement over the average\nnetwork performance. We have also tried a more sophisticated ensemble approach: the ten\nnetwork output scores (before sentence-level likelihood) were combined with an additional\nlinear layer (4) and then fed to a new sentence-level likelihood (13). The parameters of\nthe combining layers were then trained on the existing training set, while keeping the ten\nnetworks \ufb01xed (\u201cjoined ensemble\u201d). This approach did not improve on simple voting.\nThese ensembles come of course at the expense of a ten fold increase of the running\ntime. On the other hand, multiple training times could be improved using smart sampling\nstrategies (Neal, 1996).\nWe can also observe that the performance variability among the ten networks is not very\nlarge. The local minima found by the training algorithm are usually good local minima,\nthanks to the oversized parameter space and to the noise induced by the stochastic gradient\nprocedure (LeCun et al., 1998). In order to reduce the variance in our experimental results,\nwe always use the same initial parameters for networks trained on the same task (except of\ncourse for the results reported in Table 11.)\n6.5 Parsing\nGildea and Palmer (2002) o\ufb00er several arguments suggesting that syntactic parsing is a\nnecessary prerequisite for the SRL task. The CoNLL 2005 SRL benchmark task provides\nparse trees computed using both the Charniak (2000) and Collins (1999) parsers. State-of-\nthe-art systems often exploit additional parse trees such as the k top ranking parse trees\n(Koomen et al., 2005; Haghighi et al., 2005).\nIn contrast our SRL networks so far do not use parse trees at all. They rely instead\non internal representations transferred from a language model trained with an objective\n30\narXiv\nNatural Language Processing (almost) from Scratch\nlevel 0\nS\nNP\nThe luxury auto maker\nb-np\ni-np\ni-np\ne-np\nNP\nlast year\nb-np e-np\nVP\nsold\ns-vp\nNP\n1,214 cars\nb-np e-np\nPP\nin\ns-pp\nNP\nthe U.S.\nb-np e-np\nlevel 1\nS\nThe luxury auto maker last year\no\no\no\no\no\no\nVP\nsold 1,214 cars\nb-vp i-vp e-vp\nPP\nin\nthe U.S.\nb-pp i-pp e-pp\nlevel 2\nS\nThe luxury auto maker last year\no\no\no\no\no\no\nVP\nsold 1,214 cars in\nthe U.S.\nb-vp i-vp i-vp i-vp i-vp e-vp\nFigure 6: Charniak parse tree for the sentence \u201cThe luxury auto maker last year sold 1,214\ncars in the U.S.\u201d. Level 0 is the original tree. Levels 1 to 4 are obtained by successively\ncollapsing terminal tree branches. For each level, words receive tags describing the segment\nassociated with the corresponding leaf. All words receive tag \u201cO\u201d at level 3 in this example.\nfunction that captures a lot of syntactic information (see Section 4.6).\nIt is therefore\nlegitimate to question whether this approach is an acceptable lightweight replacement for\nparse trees.\nWe answer this question by providing parse tree information as additional input features\nto our system. We have limited ourselves to the Charniak parse tree provided with the\nCoNLL 2005 data.\nConsidering that a node in a syntactic parse tree assigns a label\nto a segment of the parsed sentence, we propose a way to feed (partially) this labeled\nsegmentation to our network, through additional lookup tables.\nEach of these lookup\ntables encode labeled segments of each parse tree level (up to a certain depth). The labeled\nsegments are fed to the network following a IOBES tagging scheme (see Sections 3.2.3\nand 3.1.1). As there are 40 di\ufb00erent phrase labels in WSJ, each additional tree-related\nlookup tables has 161 entries (40 \u00d7 4 + 1) corresponding to the IBES segment tags, plus the\nextra O tag.\nWe call level 0 the information associated with the leaves of the original Charniak parse\ntree. The lookup table for level 0 encodes the corresponding IOBES phrase tags for each\nwords. We obtain levels 1 to 4 by repeatedly trimming the leaves as shown in Figure 6. We\nlabeled \u201cO\u201d words belonging to the root node \u201cS\u201d, or all words of the sentence if the root\nitself has been trimmed.\nExperiments were performed using the LM2 language model using the same network\narchitectures (see Table 5) and using additional lookup tables of dimension 5 for each\n31\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\nApproach\nSRL\n(valid)\n(test)\nBenchmark System (six parse trees)\n77.35\n77.92\nBenchmark System (top Charniak parse tree only)\n74.76\n\u2013\nNN+SLL+LM2\n72.29\n74.15\nNN+SLL+LM2+Charniak (level 0 only)\n74.44\n75.65\nNN+SLL+LM2+Charniak (levels 0 & 1)\n74.50\n75.81\nNN+SLL+LM2+Charniak (levels 0 to 2)\n75.09\n76.05\nNN+SLL+LM2+Charniak (levels 0 to 3)\n75.12\n75.89\nNN+SLL+LM2+Charniak (levels 0 to 4)\n75.42\n76.06\nNN+SLL+LM2+CHUNK\n\u2013\n74.72\nNN+SLL+LM2+PT0\n\u2013\n75.49\nTable 12:\nGeneralization performance on the SRL task of our NN architecture compared\nwith the benchmark system. We show performance of our system fed with di\ufb00erent levels\nof depth of the Charniak parse tree. We report previous results of our architecture with no\nparse tree as a baseline. Koomen et al. (2005) report test and validation performance using\nsix parse trees, as well as validation performance using only the top Charniak parse tree.\nFor comparison purposes, we hence also report validation performance. Finally, we report\nour performance with the CHUNK feature, and compare it against a level 0 feature PT0\nobtained by our network.\nparse tree level. Table 12 reports the performance improvements obtained by providing\nincreasing levels of parse tree information. Level 0 alone increases the F1 score by almost\n1.5%. Additional levels yield diminishing returns. The top performance reaches 76.06% F1\nscore. This is not too far from the state-of-the-art system which we note uses six parse\ntrees instead of one. Koomen et al. (2005) also report a 74.76% F1 score on the validation\nset using only the Charniak parse tree. Using the \ufb01rst three parse tree levels, we reach this\nperformance on the validation set.\nWe also reported in Table 12 our previous performance obtained with the CHUNK\nfeature (see Table 10). It is surprising to observe that adding chunking features into the\nsemantic role labeling network performs signi\ufb01cantly worse than adding features describing\nthe level 0 of the Charniak parse tree (Table 12). Indeed, if we ignore the label pre\ufb01xes\n\u201cBIES\u201d de\ufb01ning the segmentation, the parse tree leaves (at level 0) and the chunking\nhave identical labeling. However, the parse trees identify leaf sentence segments that are\noften smaller than those identi\ufb01ed by the chunking tags, as shown by Hollingshead et al.\n(2005).16\nInstead of relying on Charniak parser, we chose to train a second chunking\nnetwork to identify the segments delimited by the leaves of the Penn Treebank parse trees\n(level 0). Our network achieved 92.25% F1 score on this task (we call it PT0), while we\n16. As in (Hollingshead et al., 2005), consider the sentence and chunk labels \u201c(NP They) (VP are starting\nto buy) (NP growth stocks)\u201d. The parse tree can be written as \u201c(S (NP They) (VP are (VP starting (S\n(VP to (VP buy (NP growth stocks)))))))\u201d. The tree leaves segmentation is thus given by \u201c(NP They)\n(VP are) (VP starting) (VP to) (VP buy) (NP growth stocks)\u201d.\n32\narXiv\nNatural Language Processing (almost) from Scratch\nevaluated Charniak performance as 91.94% on the same task. As shown in Table 12, feeding\nour own PT0 prediction into the SRL system gives similar performance to using Charniak\npredictions, and is consistently better than the CHUNK feature.\n6.6 Word Representations\nIn Section 4, we adapted our neural network architecture for training a language model task.\nBy leveraging a large amount of unlabeled text data, we induced word embeddings which\nwere shown to boost generalization performance on all tasks. While we chose to stick with\none single architecture, other ways to induce word representations exist. Mnih and Hinton\n(2007) proposed a related language model approach inspired from Restricted Boltzmann\nMachines. However, word representations are perhaps more commonly infered from n-gram\nlanguage modelling rather than smoothed language models. One popular approach is the\nBrown clustering algorithm (Brown et al., 1992a), which builds hierachical word clusters\nby maximizing the bigram\u2019s mutual information.\nThe induced word representation has\nbeen used with success in a wide variety of NLP tasks, including POS (Sch\u00a8utze, 1995),\nNER (Miller et al., 2004; Ratinov and Roth, 2009), or parsing (Koo et al., 2008). Other\nrelated approaches exist, like phrase clustering (Lin and Wu, 2009) which has been shown\nto work well for NER. Finally, Huang and Yates (2009) have recently proposed a smoothed\nlanguage modelling approach based on a Hidden Markov Model, with success on POS and\nChunking tasks.\nWhile a comparison of all these word representations is beyond the scope of this paper,\nit is rather fair to question the quality of our word embeddings compared to a popular NLP\napproach. In this section, we report a comparison of our word embeddings against Brown\nclusters, when used as features into our neural network architecture. We report as baseline\nprevious results where our word embeddings are \ufb01ne-tuned for each task. We also report\nperformance when our embeddings are kept \ufb01xed during task-speci\ufb01c training. Since convex\nmachine learning algorithms are common practice in NLP, we \ufb01nally report performances\nfor the convex version of our architecture.\nFor the convex experiments, we considered the linear version of our neural networks\n(instead of having several linear layers interleaved with a non-linearity). While we always\npicked the sentence approach for SRL, we had to consider the window approach in this\nparticular convex setup, as the sentence approach network (see Figure 2) includes a Max\nlayer.\nHaving only one linear layer in our neural network is not enough to make our\narchitecture convex: all lookup-tables (for each discrete feature) must also be \ufb01xed. The\nword-lookup table is simply \ufb01xed to the embeddings obtained from our language model\nLM2. All other discrete feature lookup-tables (caps, POS, Brown Clusters...) are \ufb01xed to a\nstandard sparse representation. Using the notation introduced in Section 3.1.1, if LTW k is\nthe lookup-table of the kth discrete feature, we have W k \u2208R|Dk|\u00d7|Dk| and the representation\nof the discrete input w is obtained with:\nLTW k(w) = \u27e8W k\u27e91\nw =\n\u0012\n0, \u00b7 \u00b7 \u00b7 0,\n1\nat index w, 0, \u00b7 \u00b7 \u00b7 0\n\u0013T\n.\n(19)\nTraining our architecture in this convex setup with the sentence-level likelihood (13)\ncorresponds to training a CRF. In that respect, these convex experiments show the\nperformance of our word embeddings in a classical NLP framework.\n33\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\nApproach\nPOS\nCHUNK\nNER\nSRL\n(PWA)\n(F1)\n(F1)\n(F1)\nNon-convex Approach\nLM2 (non-linear NN)\n97.29\n94.32\n89.59\n76.06\nLM2 (non-linear NN, \ufb01xed embeddings)\n97.10\n94.45\n88.79\n72.24\nBrown Clusters (non-linear NN, 130K words)\n96.92\n94.35\n87.15\n72.09\nBrown Clusters (non-linear NN, all words)\n96.81\n94.21\n86.68\n71.44\nConvex Approach\nLM2 (linear NN, \ufb01xed embeddings)\n96.69\n93.51\n86.64\n59.11\nBrown Clusters (linear NN, 130K words)\n96.56\n94.20\n86.46\n51.54\nBrown Clusters (linear NN, all words)\n96.28\n94.22\n86.63\n56.42\nTable 13:\nGeneralization performance of our neural network architecture trained with\nour language model (LM2) word embeddings, and with the word representations derived\nfrom the Brown Clusters. As before, all networks are fed with a capitalization feature.\nAdditionally, POS is using a word su\ufb03x of size 2 feature, CHUNK is fed with POS, NER\nuses the CoNLL 2003 gazetteer, and SRL is fed with levels 1\u20135 of the Charniak parse tree,\nas well as a verb position feature. We report performance with both convex and non-convex\narchitectures (300 hidden units for all tasks, with an additional 500 hidden units layer for\nSRL). We also provide results for Brown Clusters induced with a 130K word dictionary, as\nwell as Brown Clusters induced with all words of the given tasks.\nFollowing the Ratinov and Roth (2009) and Koo et al. (2008) setups, we generated 1, 000\nBrown clusters using the implementation17 from Liang (2005). To make the comparison\nfair, the clusters were \ufb01rst induced on the concatenation of Wikipedia and Reuters datasets,\nas we did in Section 4 for training our largest language model LM2, using a 130K word\ndictionary. This dictionary covers about 99% of the words in the training set of each task.\nTo cover the last 1%, we augmented the dictionary with the missing words (reaching about\n140K words) and induced Brown Clusters using the concatenation of WSJ, Wikipedia, and\nReuters.\nThe Brown clustering approach is hierarchical and generates a binary tree of clusters.\nEach word in the vocabulary is assigned to a node in the tree. Features are extracted from\nthis tree by considering the path from the root to the node containing the word of interest.\nFollowing Ratinov & Roth, we picked as features the path pre\ufb01xes of size 4, 6, 10 and 20. In\nthe non-convex experiments, we fed these four Brown Cluster features to our architecture\nusing four di\ufb00erent lookup tables, replacing our word lookup table. The size of the lookup\ntables was chosen to be 12 by validation. In the convex case, we used the classical sparse\nrepresentation (19), as for any other discrete feature.\nWe \ufb01rst report in Table 13 generalization performance of our best non-convex networks\ntrained with our LM2 language model and with Brown Cluster features. Our embeddings\nperform at least as well as Brown Clusters. Results are more mitigated in a convex setup.\nFor most task, going non-convex is better for both word representation types. In general,\n17. Available at http://www.eecs.berkeley.edu/~pliang/software.\n34\narXiv\nNatural Language Processing (almost) from Scratch\nTask\nFeatures\nPOS\nSu\ufb03x of size 2\nCHUNK\nPOS\nNER\nCoNLL 2003 gazetteer\nPT0\nPOS\nSRL\nPT0, verb position\nTable 14:\nFeatures used by SENNA implementation, for each task. In addition, all tasks\nuse \u201clow caps word\u201d and \u201ccaps\u201d features.\n\u201c\ufb01ne-tuning\u201d our embeddings for each task also gives an extra boost. Finally, using a better\nword coverage with Brown Clusters (\u201call words\u201d instead of \u201c130K words\u201d in Table 13) did\nnot help.\nMore complex features could be possibly combined instead of using a non-linear\nmodel. For instance, Turian et al. (2010) performed a comparison of Brown Clusters and\nembeddings trained in the same spirit as ours18, with additional features combining labels\nand tokens. We believe this type of comparison should be taken with care, as combining\na given feature with di\ufb00erent word representations might not have the same e\ufb00ect on each\nword representation.\n6.7 Engineering a Sweet Spot\nWe implemented a standalone version of our architecture, written in the C language.\nWe gave the name \u201cSENNA\u201d (Semantic/syntactic Extraction using a Neural Network\nArchitecture) to the resulting system. The parameters of each architecture are the ones\ndescribed in Table 5.\nAll the networks were trained separately on each task using the\nsentence-level likelihood (SLL). The word embeddings were initialized to LM2 embeddings,\nand then \ufb01ne-tuned for each task. We summarize features used by our implementation\nin Table 14, and we report performance achieved on each task in Table 15.\nThe runtime\nversion19 contains about 2500 lines of C code, runs in less than 150MB of memory, and needs\nless than a millisecond per word to compute all the tags. Table 16 compares the tagging\nspeeds for our system and for the few available state-of-the-art systems: the Toutanova et al.\n(2003) POS tagger20, the Shen et al. (2007) POS tagger21 and the Koomen et al. (2005) SRL\nsystem.22 All programs were run on a single 3GHz Intel core. The POS taggers were run\nwith Sun Java 1.6 with a large enough memory allocation to reach their top tagging speed.\n18. However they did not reach our embedding performance.\nThere are several di\ufb00erences in how they\ntrained their models that might explain this. Firstly, they may have experienced di\ufb03culties because\nthey train 50-dimensional embeddings for 269K distinct words using a comparatively small training set\n(RCV1, 37M words), unlikely to contain enough instances of the rare words. Secondly, they predict the\ncorrectness of the \ufb01nal word of each window instead of the center word (Turian et al., 2010), e\ufb00ectively\nrestricting the model to unidirectional prediction. Finally, they do not \ufb01ne tune their embeddings after\nunsupervised training.\n19. Available at http://ml.nec-labs.com/senna.\n20. Available at http://nlp.stanford.edu/software/tagger.shtml. We picked the 3.0 version (May 2010).\n21. Available at http://www.cis.upenn.edu/~xtag/spinal.\n22. Available at http://l2r.cs.uiuc.edu/~cogcomp/asoftware.php?skey=SRL.\n35\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\nTask\nBenchmark\nSENNA\nPart of Speech (POS)\n(Accuracy)\n97.24 %\n97.29 %\nChunking (CHUNK)\n(F1)\n94.29 %\n94.32 %\nNamed Entity Recognition (NER)\n(F1)\n89.31 %\n89.59 %\nParse Tree level 0 (PT0)\n(F1)\n91.94 %\n92.25 %\nSemantic Role Labeling (SRL)\n(F1)\n77.92 %\n75.49 %\nTable 15: Performance of the engineered sweet spot (SENNA) on various tagging tasks. The\nPT0 task replicates the sentence segmentation of the parse tree leaves. The corresponding\nbenchmark score measures the quality of the Charniak parse tree leaves relative to the Penn\nTreebank gold parse trees.\nPOS System\nRAM (MB)\nTime (s)\nToutanova et al. (2003)\n800\n64\nShen et al. (2007)\n2200\n833\nSENNA\n32\n4\nSRL System\nRAM (MB)\nTime (s)\nKoomen et al. (2005)\n3400\n6253\nSENNA\n124\n51\nTable 16:\nRuntime speed and memory consumption comparison between state-of-the-art\nsystems and our approach (SENNA). We give the runtime in seconds for running both\nthe POS and SRL taggers on their respective testing sets. Memory usage is reported in\nmegabytes.\nThe beam size of the Shen tagger was set to 3 as recommended in the paper. Regardless\nof implementation di\ufb00erences, it is clear that our neural networks run considerably faster.\nThey also require much less memory. Our POS and SRL tagger runs in 32MB and 120MB\nof RAM respectively. The Shen and Toutanova taggers slow down signi\ufb01cantly when the\nJava machine is given less than 2.2GB and 800MB of RAM respectively, while the Koomen\ntagger requires at least 3GB of RAM.\nWe believe that a number of reasons explain the speed advantage of our system. First,\nour system only uses rather simple input features and therefore avoids the nonnegligible\ncomputation time associated with complex handcrafted features. Secondly, most network\ncomputations are dense matrix-vector operations. In contrast, systems that rely on a great\nnumber of sparse features experience memory latencies when traversing the sparse data\nstructures. Finally, our compact implementation is self-contained. Since it does not rely on\nthe outputs of disparate NLP system, it does not su\ufb00er from communication latency issues.\n7. Critical Discussion\nAlthough we believe that this contribution represents a step towards the \u201cNLP from scratch\u201d\nobjective, we are keenly aware that both our goal and our means can be criticized.\n36\narXiv\nNatural Language Processing (almost) from Scratch\nThe main criticism of our goal can be summarized as follows. Over the years, the NLP\ncommunity has developed a considerable expertise in engineering e\ufb00ective NLP features.\nWhy should they forget this painfully acquired expertise and instead painfully acquire\nthe skills required to train large neural networks? As mentioned in our introduction, we\nobserve that no single NLP task really covers the goals of NLP. Therefore we believe that\ntask-speci\ufb01c engineering (i.e. that does not generalize to other tasks) is not desirable. But\nwe also recognize how much our neural networks owe to previous NLP task-speci\ufb01c research.\nThe main criticism of our means is easier to address. Why did we choose to rely on a\ntwenty year old technology, namely multilayer neural networks? We were simply attracted\nby their ability to discover hidden representations using a stochastic learning algorithm\nthat scales linearly with the number of examples. Most of the neural network technology\nnecessary for our work has been described ten years ago (e.g. Le Cun et al., 1998). However,\nif we had decided ten years ago to train the language model network LM2 using a vintage\ncomputer, training would only be nearing completion today. Training algorithms that scale\nlinearly are most able to bene\ufb01t from such tremendous progress in computer hardware.\n8. Conclusion\nWe have presented a multilayer neural network architecture that can handle a number of\nNLP tasks with both speed and accuracy. The design of this system was determined by\nour desire to avoid task-speci\ufb01c engineering as much as possible. Instead we rely on large\nunlabeled datasets and let the training algorithm discover internal representations that\nprove useful for all the tasks of interest. Using this strong basis, we have engineered a fast\nand e\ufb03cient \u201call purpose\u201d NLP tagger that we hope will prove useful to the community.\nAcknowledgments\nWe acknowledge the persistent support of NEC for this research e\ufb00ort. We thank Yoshua\nBengio, Samy Bengio, Eric Cosatto, Vincent Etter, Hans-Peter Graf, Ralph Grishman, and\nVladimir Vapnik for their useful feedback and comments.\n37\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\nAppendix A. Neural Network Gradients\nWe consider a neural network f\u03b8(\u00b7), with parameters \u03b8. We maximize the likelihood (8), or\nminimize ranking criterion (18), with respect to the parameters \u03b8, using stochastic gradient.\nBy negating the likelihood, we now assume it all corresponds to minimize a cost C(f\u03b8(\u00b7)),\nwith respect to \u03b8.\nFollowing the classical \u201cback-propagation\u201d derivations (LeCun, 1985; Rumelhart et al.,\n1986) and the modular approach shown in (Bottou, 1991), any feed-forward neural network\nwith L layers, like the ones shown in Figure 1 and Figure 2, can be seen as a composition\nof functions fl\n\u03b8(\u00b7), corresponding to each layer l:\nf\u03b8(\u00b7) = fL\n\u03b8 (fL\u22121\n\u03b8\n(. . . f1\n\u03b8 (\u00b7) . . .))\nPartionning the parameters of the network with respect to each layers 1 \u2264l \u2264L, we write:\n\u03b8 = (\u03b81, . . . , \u03b8l, . . . , \u03b8L) .\nWe are now interested in computing the gradients of the cost with respect to each \u03b8l.\nApplying the chain rule (generalized to vectors) we obtain the classical backpropagation\nrecursion:\n\u2202C\n\u2202\u03b8l\n=\n\u2202fl\n\u03b8\n\u2202\u03b8l\n\u2202C\n\u2202fl\n\u03b8\n(20)\n\u2202C\n\u2202fl\u22121\n\u03b8\n=\n\u2202fl\n\u03b8\n\u2202fl\u22121\n\u03b8\n\u2202C\n\u2202fl\n\u03b8\n.\n(21)\nIn other words, we \ufb01rst initialize the recursion by computing the gradient of the cost with\nrespect to the last layer output \u2202C/\u2202fL\n\u03b8 . Then each layer l computes the gradient respect\nto its own parameters with (20), given the gradient coming from its output \u2202C/\u2202fl\n\u03b8. To\nperform the backpropagation, it also computes the gradient with respect to its own inputs,\nas shown in (21). We now derive the gradients for each layer we used in this paper.\nLookup Table Layer\nGiven a matrix of parameters \u03b81 = W 1 and word (or discrete\nfeature) indices [w]T\n1 , the layer outputs the matrix:\nfl\n\u03b8([w]T\nl ) =\n\u0010\n\u27e8W\u27e91\n[w]1\n\u27e8W\u27e91\n[w]2\n. . .\n\u27e8W\u27e91\n[w]T\n\u0011\n.\nThe gradients of the weights \u27e8W\u27e91\ni are given by:\n\u2202C\n\u2202\u27e8W\u27e91\ni\n=\nX\n{1\u2264t\u2264T / [w]t=i}\n\u27e8\u2202C\n\u2202fl\n\u03b8\n\u27e91\ni\nThis sum equals zero if the index i in the lookup table does not corresponds to a word in\nthe sequence. In this case, the ith column of W does not need to be updated. As a Lookup\nTable Layer is always the \ufb01rst layer, we do not need to compute its gradients with respect\nto the inputs.\n38\narXiv\nNatural Language Processing (almost) from Scratch\nLinear Layer\nGiven parameters \u03b8l = (W l, bl), and an input vector fl\u22121\n\u03b8\nthe output is\ngiven by:\nfl\n\u03b8 = W lfl\u22121\n\u03b8\n+ bl .\n(22)\nThe gradients with respect to the parameters are then obtained with:\n\u2202C\n\u2202W l =\n\u0014 \u2202C\n\u2202fl\n\u03b8\n\u0015 h\nfl\u22121\n\u03b8\niT\nand \u2202C\n\u2202bl = \u2202C\n\u2202fl\n\u03b8\n,\n(23)\nand the gradients with respect to the inputs are computed with:\n\u2202C\n\u2202fl\u22121\n\u03b8\n=\nh\nW liT \u2202C\n\u2202fl\n\u03b8\n.\n(24)\nConvolution Layer\nGiven a input matrix fl\u22121\n\u03b8\n, a Convolution Layer fl\n\u03b8(\u00b7) applies a\nLinear Layer operation (22) successively on each window \u27e8fl\u22121\n\u03b8\n\u27e9dwin\nt\n(1 \u2264t \u2264T) of size\ndwin.\nUsing (23), the gradients of the parameters are thus given by summing over all\nwindows:\n\u2202C\n\u2202W l =\nT\nX\nt=1\n\u0014\n\u27e8\u2202C\n\u2202fl\n\u03b8\n\u27e91\nt\n\u0015 h\n\u27e8fl\u22121\n\u03b8\n\u27e9dwin\nt\niT\nand \u2202C\n\u2202bl =\nT\nX\nt=1\n\u27e8\u2202C\n\u2202fl\n\u03b8\n\u27e91\nt .\nAfter initializing the input gradients \u2202C/\u2202fl\u22121\n\u03b8\nto zero, we iterate (24) over all windows for\n1 \u2264t \u2264T, leading the accumulation23:\n\u27e8\u2202C\n\u2202fl\u22121\n\u03b8\n\u27e9dwin\nt\n+=\nh\nW liT\n\u27e8\u2202C\n\u2202fl\n\u03b8\n\u27e91\nt .\nMax Layer\nGiven a matrix fl\u22121\n\u03b8\n, the Max Layer computes\nh\nfl\n\u03b8\ni\ni = max\nt\nh\n\u27e8fl\u22121\n\u03b8\n\u27e91\nt\ni\ni and ai = argmax\nt\nh\n\u27e8fl\u22121\n\u03b8\n\u27e91\nt\ni\ni \u2200i ,\nwhere ai stores the index of the largest value. We only need to compute the gradient with\nrespect to the inputs, as this layer has no parameters. The gradient is given by\n\"\n\u27e8\u2202C\n\u2202fl\u22121\n\u03b8\n\u27e91\nt\n#\ni\n=\n( h\n\u27e8\u2202C\n\u2202fl\n\u03b8 \u27e91\nt\ni\ni\nif t = ai\n0\notherwise\n.\nHardTanh Layer\nGiven a vector fl\u22121\n\u03b8\n, and the de\ufb01nition of the HardTanh (5) we get\n\"\n\u2202C\n\u2202fl\u22121\n\u03b8\n#\ni\n=\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n0\nif\nh\nfl\u22121\n\u03b8\ni\ni < \u22121\nh\n\u2202C\n\u2202fl\n\u03b8\ni\ni\nif \u22121 <=\nh\nfl\u22121\n\u03b8\ni\ni <= 1\n0\nif\nh\nfl\u22121\n\u03b8\ni\ni > 1\n,\nif we ignore non-di\ufb00erentiability points.\n23. We denote \u201c+=\u201d any accumulation operation.\n39\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\nWord-Level Log-Likelihood\nThe network outputs a score [f\u03b8]i for each tag indexed by\ni. Following (11), if y is the true tag for a given example, the stochastic score to minimize\ncan be written as\nC(f\u03b8) = logadd\nj\n[f\u03b8]j \u2212[f\u03b8]y\nConsidering the de\ufb01nition of the logadd (10), the gradient with respect to f\u03b8 is given by\n\u2202C\n\u2202[f\u03b8]i\n=\ne[f\u03b8]i\nP\nk e[f\u03b8]k \u22121i=y\n\u2200i.\nSentence-Level Log-Likelihood\nThe network outputs a matrix where each element\n[f\u03b8]i, t gives a score for tag i at word t. Given a tag sequence [y]T\n1 and a input sequence [x]T\n1 ,\nwe maximize the likelihood (13), which corresponds to minimizing the score\nC(f\u03b8, A) = logadd\n\u2200[j]T\n1\ns([x]T\n1 , [j]T\n1 , \u02dc\u03b8)\n|\n{z\n}\nClogadd\n\u2212s([x]T\n1 , [y]T\n1 , \u02dc\u03b8) ,\nwith\ns([x]T\n1 , [y]T\n1 , \u02dc\u03b8) =\nT\nX\nt=1\n\u0010\n[A][y]t\u22121, [y]t + [f\u03b8][y]t, t\n\u0011\n.\nWe \ufb01rst initialize all gradients to zero\n\u2202C\n\u2202\n\u0002\nf\u03b8\n\u0003\ni, t\n= 0 \u2200i, t and\n\u2202C\n\u2202[A]i, j\n= 0\n\u2200i, j .\nWe then accumulate gradients over the second part of the cost \u2212s([x]T\n1 , [y]T\n1 , \u02dc\u03b8), which\ngives:\n\u2202C\n\u2202\n\u0002\nf\u03b8\n\u0003\n[y]t, t\n+= 1\n\u2202C\n\u2202[A][y]t\u22121, [y]t\n+= 1\n\u2200t .\nWe now need to accumulate the gradients over the \ufb01rst part of the cost, that is Clogadd.\nWe di\ufb00erentiate Clogadd by applying the chain rule through the recursion (14). First we\ninitialize our recursion with\n\u2202Clogadd\n\u2202\u03b4T (i) =\ne\u03b4T (i)\nP\nk e\u03b4T (k)\n\u2200i .\nWe then compute iteratively:\n\u2202Clogadd\n\u2202\u03b4t\u22121(i) =\nX\nj\n\u2202Clogadd\n\u2202\u03b4t(j)\ne\u03b4t\u22121(i)+[A]i, j\nP\nk e\u03b4t\u22121(k)+[A]k, j ,\n(25)\n40\narXiv\nNatural Language Processing (almost) from Scratch\nwhere at each step t of the recursion we accumulate of the gradients with respect to the\ninputs f\u03b8, and the transition scores [A]i, j:\n\u2202C\n\u2202\n\u0002\nf\u03b8\n\u0003\ni, t\n+=\u2202Clogadd\n\u2202\u03b4t(i)\n\u2202\u03b4t(i)\n\u2202\n\u0002\nf\u03b8\n\u0003\ni, t\n= \u2202Clogadd\n\u2202\u03b4t(i)\n\u2202C\n\u2202[A]i, j\n+=\u2202Clogadd\n\u2202\u03b4t(j)\n\u2202\u03b4t(j)\n\u2202[A]i, j\n= \u2202Clogadd\n\u2202\u03b4t(j)\ne\u03b4t\u22121(i)+[A]i, j\nP\nk e\u03b4t\u22121(k)+[A]k, j .\nRanking Criterion\nWe use the ranking criterion (18) for training our language model.\nIn this case, given a \u201cpositive\u201d example x and a \u201cnegative\u201d example x(w), we want to\nminimize:\nC(f\u03b8(x), f\u03b8(xw)) = max\nn\n0 , 1 \u2212f\u03b8(x) + f\u03b8(x(w))\no\n.\n(26)\nIgnoring the non-di\ufb00erentiability of max(0, \u00b7) in zero, the gradient is simply given by:\n \n\u2202C\n\u2202f\u03b8(x)\n\u2202C\n\u2202f\u03b8(xw)\n!\n=\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f3\n\u0012\n\u22121\n1\n\u0013\nif 1 \u2212f\u03b8(x) + f\u03b8(x(w)) > 0\n\u0012\n0\n0\n\u0013\notherwise\n.\nReferences\nR. K. Ando and T. Zhang. A framework for learning predictive structures from multiple\ntasks and unlabeled data. JMLR, 6:1817\u20131953, 11 2005.\nR. M. Bell, Y. Koren, and C. Volinsky. The BellKor solution to the Net\ufb02ix Prize. Technical\nreport, AT&T Labs, 2007. http://www.research.att.com/~volinsky/netflix.\nY. Bengio and R. Ducharme. A neural probabilistic language model. In NIPS 13, 2001.\nY. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. Greedy layer-wise training of deep\nnetworks. In Advances in Neural Information Processing Systems, NIPS 19, 2007.\nY. Bengio, J. Louradour, R. Collobert, and J. Weston. Curriculum learning. In International\nConference on Machine Learning, ICML, 2009.\nL. Bottou. Stochastic gradient learning in neural networks. In Proceedings of Neuro-N\u02c6\u0131mes\n91, Nimes, France, 1991. EC2.\nL. Bottou. Online algorithms and stochastic approximations. In David Saad, editor, Online\nLearning and Neural Networks. Cambridge University Press, Cambridge, UK, 1998.\nL. Bottou and P. Gallinari. A framework for the cooperation of learning algorithms. In\nD. Touretzky and R. Lippmann, editors, Advances in Neural Information Processing\nSystems, volume 3. Morgan Kaufmann, Denver, 1991.\nL. Bottou, Y. LeCun, and Yoshua Bengio. Global training of document processing systems\nusing graph transformer networks. In Proc. of Computer Vision and Pattern Recognition,\npages 489\u2013493, Puerto-Rico, 1997. IEEE.\n41\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\nJ. S. Bridle. Probabilistic interpretation of feedforward classi\ufb01cation network outputs, with\nrelationships to statistical pattern recognition. In F. Fogelman Souli\u00b4e and J. H\u00b4erault,\neditors, Neurocomputing: Algorithms, Architectures and Applications, pages 227\u2013236.\nNATO ASI Series, 1990.\nP. F. Brown, P. V. deSouza, R. L. Mercer, V. J. D. Pietra, and J C. Lai. Class-based n-gram\nmodels of natural language. Computational Linguistics, 18(4):467\u2013479, 1992a.\nP. F. Brown, V. J. Della Pietra, R. L. Mercer, S. A. Della Pietra, and J. C. Lai. An estimate\nof an upper bound for the entropy of english. Computational Linguistics, 18(1):31\u201341,\n1992b.\nC. J. C. Burges, R. Ragno, and Quoc Viet Le. Learning to rank with nonsmooth cost\nfunctions.\nIn B. Sch\u00a8olkopf, J. Platt, and T. Ho\ufb00man, editors, Advances in Neural\nInformation Processing Systems 19, pages 193\u2013200. MIT Press, Cambridge, MA, 2007.\nR. Caruana. Multitask Learning. Machine Learning, 28(1):41\u201375, 1997.\nO. Chapelle, B. Schlkopf, and A. Zien. Semi-Supervised Learning. Adaptive computation\nand machine learning. MIT Press, Cambridge, Mass., USA, 09 2006.\nE. Charniak. A maximum-entropy-inspired parser. Proceedings of the \ufb01rst conference on\nNorth American chapter of the Association for Computational Linguistics, pages 132\u2013139,\n2000.\nH. L. Chieu. Named entity recognition with a maximum entropy approach. In In Proceedings\nof the Seventh Conference on Natural Language Learning (CoNLL-2003, pages 160\u2013163,\n2003.\nN. Chomsky.\nThree models for the description of language.\nIRE Transactions on\nInformation Theory, 2(3):113\u2013124, September 1956.\nS. Cl\u00b4emen\u00b8con and N. Vayatis. Ranking the best instances. Journal of Machine Learning\nResearch, 8:2671\u20132699, December 2007.\nW. W. Cohen, R. E. Schapire, and Y. Singer. Learning to order things. Journal of Arti\ufb01cial\nIntelligence Research, 10:243\u2013270, 1998.\nT. Cohn and P. Blunsom. Semantic role labelling with tree conditional random \ufb01elds. In\nNinth Conference on Computational Natural Language (CoNLL), 2005.\nM. Collins.\nHead-Driven Statistical Models for Natural Language Parsing.\nPhD thesis,\nUniversity of Pennsylvania, 1999.\nR. Collobert. Large Scale Machine Learning. PhD thesis, Universit\u00b4e Paris VI, 2004.\nT. Cover and R. King. A convergent gambling estimate of the entropy of english. IEEE\nTransactions on Information Theory, 24(4):413\u2013421, July 1978.\n42\narXiv\nNatural Language Processing (almost) from Scratch\nR. Florian, A. Ittycheriah, H. Jing, and T. Zhang.\nNamed entity recognition through\nclassi\ufb01er combination.\nIn Proceedings of the seventh conference on Natural language\nlearning at HLT-NAACL 2003, pages 168\u2013171. Association for Computational Linguistics,\n2003.\nD. Gildea and D. Jurafsky. Automatic labeling of semantic roles. Computational Linguistics,\n28(3):245\u2013288, 2002.\nD. Gildea and M. Palmer. The necessity of parsing for predicate argument recognition.\nProceedings of the 40th Annual Meeting of the ACL, pages 239\u2013246, 2002.\nJ. Gim\u00b4enez and L. M`arquez.\nSVMTool:\nA general POS tagger generator based on\nsupport vector machines. In Proceedings of the 4th International Conference on Language\nResources and Evaluation (LREC\u201904), 2004.\nA. Haghighi, K. Toutanova, and C. D. Manning. A joint model for semantic role labeling.\nIn Proceedings of the Ninth Conference on Computational Natural Language Learning\n(CoNLL-2005). Association for Computational Linguistics, June 2005.\nZ. S. Harris. Mathematical Structures of Language. John Wiley & Sons Inc., 1968.\nD. Heckerman, D. M. Chickering, C. Meek, R. Rounthwaite, and C. Kadie. Dependency\nnetworks for inference, collaborative \ufb01ltering, and data visualization. Journal of Machine\nLearning Research, 1:49\u201375, 2001. ISSN 1532-4435.\nG. E. Hinton, S. Osindero, and Y.-W. Teh. A fast learning algorithm for deep belief nets.\nNeural Comp., 18(7):1527\u20131554, July 2006.\nK. Hollingshead, S. Fisher, and B. Roark.\nComparing and combining \ufb01nite-state and\ncontext-free parsers. In HLT \u201905: Proceedings of the conference on Human Language\nTechnology and Empirical Methods in Natural Language Processing, pages 787\u2013794.\nAssociation for Computational Linguistics, 2005.\nF. Huang and A. Yates. Distributional representations for handling sparsity in supervised\nsequence-labeling.\nIn Proceedings of the Association for Computational Linguistics\n(ACL), pages 495\u2013503. Association for Computational Linguistics, 2009.\nF. Jelinek. Continuous speech recognition by statistical methods. Proceedings of the IEEE,\n64(4):532\u2013556, 1976.\nT. Joachims. Transductive inference for text classi\ufb01cation using support vector machines.\nIn ICML, 1999.\nD. Klein and C. D. Manning. Natural language grammar induction using a constituent-\ncontext model.\nIn Thomas G. Dietterich, Suzanna Becker, and Zoubin Ghahramani,\neditors, Advances in Neural Information Processing Systems 14, pages 35\u201342. MIT Press,\nCambridge, MA, 2002.\nT. Koo, X. Carreras, and M. Collins.\nSimple semi-supervised dependency parsing.\nIn\nProceedings of the Association for Computational Linguistics (ACL), pages 595\u2013603, 2008.\n43\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\nP. Koomen, V. Punyakanok, D. Roth, and W. Yih. Generalized inference with multiple\nsemantic role labeling systems (shared task paper). In Ido Dagan and Dan Gildea, editors,\nProc. of the Annual Conference on Computational Natural Language Learning (CoNLL),\npages 181\u2013184, 2005.\nT. Kudo and Y. Matsumoto. Chunking with support vector machines. In In Proceedings\nof the 2nd Meeting of the North American Association for Computational Linguistics:\nNAACL 2001, pages 1\u20138. Association for Computational Linguistics, 2001.\nT. Kudoh and Y. Matsumoto. Use of support vector learning for chunk identi\ufb01cation. In\nProceedings of CoNLL-2000 and LLL-2000, pages 142\u2013144, 2000.\nJ. La\ufb00erty, A. McCallum, and F. Pereira. Conditional random \ufb01elds: Probabilistic models\nfor segmenting and labeling sequence data. In Eighteenth International Conference on\nMachine Learning, ICML, 2001.\nY. Le Cun, L. Bottou, Y. Bengio, and P. Ha\ufb00ner.\nGradient based learning applied to\ndocument recognition. Proceedings of IEEE, 86(11):2278\u20132324, 1998.\nY. LeCun.\nA learning scheme for asymmetric threshold networks.\nIn Proceedings of\nCognitiva 85, pages 599\u2013604, Paris, France, 1985.\nY. LeCun, L. Bottou, G. B. Orr, and K.-R. M\u00a8uller. E\ufb03cient backprop. In G.B. Orr and\nK.-R. M\u00a8uller, editors, Neural Networks: Tricks of the Trade, pages 9\u201350. Springer, 1998.\nD. D. Lewis, Y. Yang, T. G. Rose, and F. Li. Rcv1: A new benchmark collection for text\ncategorization research. Journal of Machine Learning Research, 5:361\u2013397, 2004.\nP. Liang. Semi-supervised learning for natural language. Master\u2019s thesis, Massachusetts\nInstitute of Technology, 2005.\nP. Liang, H. Daum\u00b4e, III, and D. Klein. Structure compilation: trading structure for features.\nIn International conference on Machine learning (ICML), pages 592\u2013599. ACM, 2008.\nD. Lin and X. Wu.\nPhrase clustering for discriminative learning.\nIn Proceedings of\nthe Association for Computational Linguistics (ACL), pages 1030\u20131038. Association for\nComputational Linguistics, 2009.\nN. Littlestone. Learning quickly when irrelevant attributes abound: A new linear-threshold\nalgorithm. In Machine Learning, pages 285\u2013318, 1988.\nA. McCallum and Wei Li.\nEarly results for named entity recognition with conditional\nrandom \ufb01elds, feature induction and web-enhanced lexicons.\nIn Proceedings of the\nseventh conference on Natural language learning at HLT-NAACL 2003, pages 188\u2013191.\nAssociation for Computational Linguistics, 2003.\nD. McClosky, E. Charniak, and M. Johnson. E\ufb00ective self-training for parsing. Proceedings\nof HLT-NAACL 2006, 2006.\n44\narXiv\nNatural Language Processing (almost) from Scratch\nR. McDonald, K. Crammer, and F. Pereira. Flexible text segmentation with structured\nmultilabel classi\ufb01cation. In HLT \u201905: Proceedings of the conference on Human Language\nTechnology and Empirical Methods in Natural Language Processing, pages 987\u2013994.\nAssociation for Computational Linguistics, 2005.\nS. Miller, H. Fox, L. Ramshaw, and R. Weischedel. A novel use of statistical parsing to\nextract information from text.\n6th Applied Natural Language Processing Conference,\n2000.\nS. Miller, J. Guinness, and A. Zamanian.\nName tagging with word clusters and\ndiscriminative training. In Proceedings of HLT-NAACL, pages 337\u2013342, 2004.\nA Mnih and G. E. Hinton. Three new graphical models for statistical language modelling.\nIn International Conference on Machine Learning, ICML, pages 641\u2013648, 2007.\nG. Musillo and P. Merlo. Robust Parsing of the Proposition Bank. ROMAND 2006: Robust\nMethods in Analysis of Natural language Data, 2006.\nR. M. Neal. Bayesian Learning for Neural Networks. Number 118 in Lecture Notes in\nStatistics. Springer-Verlag, New York, 1996.\nD. Okanohara and J. Tsujii. A discriminative language model with pseudo-negative samples.\nProceedings of the 45th Annual Meeting of the ACL, pages 73\u201380, 2007.\nM. Palmer, D. Gildea, and P. Kingsbury. The proposition bank: An annotated corpus of\nsemantic roles. Comput. Linguist., 31(1):71\u2013106, 2005. ISSN 0891-2017.\nJ. Pearl. Probabilistic Reasoning in Intelligent Systems. Morgan Kaufman, San Mateo,\n1988.\nD. C. Plaut and G. E. Hinton. Learning sets of \ufb01lters using back-propagation. Computer\nSpeech and Language, 2:35\u201361, 1987.\nM. F. Porter. An algorithm for su\ufb03x stripping. Program, 14(3):130\u2013137, 1980.\nS. Pradhan, W. Ward, K. Hacioglu, J. Martin, and D. Jurafsky. Shallow semantic parsing\nusing support vector machines. Proceedings of HLT/NAACL-2004, 2004.\nS. Pradhan, K. Hacioglu, W. Ward, J. H. Martin, and D. Jurafsky. Semantic role chunking\ncombining complementary syntactic views. In Proceedings of the Ninth Conference on\nComputational Natural Language Learning (CoNLL-2005), pages 217\u2013220. Association\nfor Computational Linguistics, June 2005.\nV. Punyakanok, D. Roth, and W. Yih. The necessity of syntactic parsing for semantic role\nlabeling. In IJCAI, pages 1117\u20131123, 2005.\nL. Ratinov and D. Roth. Design challenges and misconceptions in named entity recognition.\nIn Proceedings of the Thirteenth Conference on Computational Natural Language Learning\n(CoNLL), pages 147\u2013155. Association for Computational Linguistics, 2009.\n45\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\nA. Ratnaparkhi. A maximum entropy model for part-of-speech tagging. In Eric Brill and\nKenneth Church, editors, Proceedings of the Conference on Empirical Methods in Natural\nLanguage Processing, pages 133\u2013142. Association for Computational Linguistics, 1996.\nB. Rosenfeld and R. Feldman.\nUsing Corpus Statistics on Entities to Improve Semi-\nsupervised Relation Extraction from the Web. Proceedings of the 45th Annual Meeting\nof the ACL, pages 600\u2013607, 2007.\nD. E. Rumelhart, G. E. Hinton, and R. J. Williams.\nLearning internal representations\nby back-propagating errors. In D.E. Rumelhart and J. L. McClelland, editors, Parallel\nDistributed Processing: Explorations in the Microstructure of Cognition, volume 1, pages\n318\u2013362. MIT Press, 1986.\nH. Sch\u00a8utze. Distributional part-of-speech tagging. In Proceedings of the Association for\nComputational Linguistics (ACL), pages 141\u2013148. Morgan Kaufmann Publishers Inc.,\n1995.\nH. Schwenk and J. L. Gauvain.\nConnectionist language modeling for large vocabulary\ncontinuous speech recognition. In IEEE International Conference on Acoustics, Speech,\nand Signal Processing, pages 765\u2013768, 2002.\nF. Sha and F. Pereira. Shallow parsing with conditional random \ufb01elds. In NAACL \u201903:\nProceedings of the 2003 Conference of the North American Chapter of the Association for\nComputational Linguistics on Human Language Technology, pages 134\u2013141. Association\nfor Computational Linguistics, 2003.\nC. E. Shannon. Prediction and entropy of printed english. Bell Systems Technical Journal,\n30:50\u201364, 1951.\nH. Shen and A. Sarkar. Voting between multiple data representations for text chunking.\nAdvances in Arti\ufb01cial Intelligence, pages 389\u2013400, 2005.\nL. Shen, G. Satta, and A. K. Joshi. Guided learning for bidirectional sequence classi\ufb01cation.\nIn Proceedings of the 45th Annual Meeting of the Association for Computational\nLinguistics (ACL), 2007.\nN. A. Smith and J. Eisner. Contrastive estimation: Training log-linear models on unlabeled\ndata. In Proceedings of the 43rd Annual Meeting of the Association for Computational\nLinguistics (ACL), pages 354\u2013362. Association for Computational Linguistics, 2005.\nS. C. Suddarth and A. D. C. Holden. Symbolic-neural systems and the use of hints for\ndeveloping complex systems. International Journal of Man-Machine Studies, 35(3):291\u2013\n311, 1991.\nX. Sun, L.-P. Morency, D. Okanohara, and J. Tsujii. Modeling latent-dynamic in shallow\nparsing: a latent conditional model with improved inference. In COLING \u201908: Proceedings\nof the 22nd International Conference on Computational Linguistics, pages 841\u2013848.\nAssociation for Computational Linguistics, 2008.\n46\narXiv\nNatural Language Processing (almost) from Scratch\nC. Sutton and A. McCallum. Joint parsing and semantic role labeling. In Proceedings of\nCoNLL-2005, pages 225\u2013228, 2005a.\nC. Sutton and A. McCallum. Composition of conditional random \ufb01elds for transfer learning.\nProceedings of the conference on Human Language Technology and Empirical Methods in\nNatural Language Processing, pages 748\u2013754, 2005b.\nC. Sutton, A. McCallum, and K. Rohanimanesh. Dynamic Conditional Random Fields:\nFactorized Probabilistic Models for Labeling and Segmenting Sequence Data. JMLR, 8:\n693\u2013723, 2007.\nJ. Suzuki and H. Isozaki. Semi-supervised sequential labeling and segmentation using giga-\nword scale unlabeled data. In Proceedings of ACL-08: HLT, pages 665\u2013673, Columbus,\nOhio, June 2008. Association for Computational Linguistics.\nW. J. Teahan and J. G. Cleary. The entropy of english using ppm-based models. In In Data\nCompression Conference (DCC\u201996), pages 53\u201362. IEEE Computer Society Press, 1996.\nK. Toutanova, D. Klein, C. D. Manning, and Y. Singer. Feature-rich part-of-speech tagging\nwith a cyclic dependency network. In HLT-NAACL, 2003.\nJ. Turian, L. Ratinov, and Y. Bengio.\nWord representations:\nA simple and general\nmethod for semi-supervised learning. In Proceedings of the Association for Computational\nLinguistics (ACL), pages 384\u2013392. Association for Computational Linguistics, 2010.\nN. Ue\ufb03ng, G. Ha\ufb00ari, and A. Sarkar.\nTransductive learning for statistical machine\ntranslation. Proceedings of the 45th Annual Meeting of the ACL, pages 25\u201332, 2007.\nA. Waibel, T. Hanazawa, G. Hinton, K. Shikano, and K.J. Lang. Phoneme recognition\nusing time-delay neural networks. IEEE Transactions on Acoustics, Speech, and Signal\nProcessing, 37(3):328\u2013339, 1989.\nJ. Weston, F. Ratle, and R. Collobert. Deep learning via semi-supervised embedding. In\nProceedings of the 25th international conference on Machine learning, pages 1168\u20131175.\nACM, 2008.\n47\n",
        "sentence": " Following, (Collobert et al., 2011)\u2019s advice, we divide each layer by the fan in of that layer, and we consider the embeddings layer to have a fan in of 1. One disadvantage of the approach used by (Collobert et al., 2011) is that there is no clear stopping criteria for the model training process. cantly improve results on NLP tasks (Turian et al., 2010; Collobert et al., 2011).",
        "context": "the \u201cfan-in\u201d of the layer, that is the number of inputs used to compute each output of this\nlayer (Plaut and Hinton, 1987). The fan-in for the lookup table (1), the lth linear layer (4)\nand the convolution layer (6) are respectively 1, nl\u22121\nThere are several di\ufb00erences in how they\ntrained their models that might explain this. Firstly, they may have experienced di\ufb03culties because\nthey train 50-dimensional embeddings for 269K distinct words using a comparatively small training set\nperformance when our embeddings are kept \ufb01xed during task-speci\ufb01c training. Since convex\nmachine learning algorithms are common practice in NLP, we \ufb01nally report performances\nfor the convex version of our architecture."
    },
    {
        "title": "Deep learning for efficient discriminative parsing",
        "author": [
            "Ronan Collobert."
        ],
        "venue": "AISTATS.",
        "citeRegEx": "Collobert.,? 2011",
        "shortCiteRegEx": "Collobert.",
        "year": 2011,
        "abstract": "",
        "full_text": "",
        "sentence": " The system they built, SENNA, offers part of speech tagging, chunking, named entity recognition, semantic role labeling and dependency parsing (Collobert, 2011).",
        "context": null
    },
    {
        "title": "Large scale distributed deep networks",
        "author": [
            "Jeffrey Dean",
            "Greg Corrado",
            "Rajat Monga",
            "Kai Chen",
            "Matthieu Devin",
            "Quoc Le",
            "Mark Mao",
            "Marc\u2019Aurelio Ranzato",
            "Andrew Senior",
            "Paul Tucker",
            "Ke Yang",
            "Andrew Ng"
        ],
        "venue": null,
        "citeRegEx": "Dean et al\\.,? \\Q2012\\E",
        "shortCiteRegEx": "Dean et al\\.",
        "year": 2012,
        "abstract": "",
        "full_text": "",
        "sentence": " icant amount of computational resources (Bengio et al., 2006; Dean et al., 2012).",
        "context": null
    },
    {
        "title": "Towards a slovene dependency treebank",
        "author": [
            "Sa\u0161o D\u017eeroski",
            "Toma\u017e Erjavec",
            "Nina Ledinek",
            "Petr Pajas",
            "Zdenek \u017dabokrtsky",
            "Andreja \u017dele."
        ],
        "venue": "Proc. of the Fifth Intern. Conf. on Language Resources and Evaluation (LREC).",
        "citeRegEx": "D\u017eeroski et al\\.,? 2006",
        "shortCiteRegEx": "D\u017eeroski et al\\.",
        "year": 2006,
        "abstract": "",
        "full_text": "",
        "sentence": " 80% Slovene SDT\u2020 (D\u017eeroski et al., 2006) 68.",
        "context": null
    },
    {
        "title": "Domain adaptation for large-scale sentiment classification: A deep learning approach",
        "author": [
            "Xavier Glorot",
            "Antoine Bordes",
            "Yoshua Bengio."
        ],
        "venue": "Proceedings of the Twenty-eight International Conference on Machine Learning (ICML\u201911), volume 27,",
        "citeRegEx": "Glorot et al\\.,? 2011",
        "shortCiteRegEx": "Glorot et al\\.",
        "year": 2011,
        "abstract": "",
        "full_text": "",
        "sentence": " These embeddings are generated as a result of training \u201cdeep\u201d architectures, and it has been shown that such representations are well suited for domain adaptation tasks (Glorot et al., 2011; Chen et al., 2012).",
        "context": null
    },
    {
        "title": "Inducing crosslingual distributed representations of words",
        "author": [
            "Alexandre Klementiev",
            "Ivan Titov",
            "Binod Bhattarai."
        ],
        "venue": "Proceedings of COLING 2012, pages 1459\u20131474, Mumbai, India, December. The COLING 2012 Organizing Committee.",
        "citeRegEx": "Klementiev et al\\.,? 2012",
        "shortCiteRegEx": "Klementiev et al\\.",
        "year": 2012,
        "abstract": "",
        "full_text": "",
        "sentence": " (Klementiev et al., 2012) induce distributed representations for a pair of languages jointly, where a learner can be trained on annotations present in one language and applied to test data in another.",
        "context": null
    },
    {
        "title": "Simple semi-supervised dependency parsing",
        "author": [
            "Terry Koo",
            "Xavier Carreras",
            "Michael Collins."
        ],
        "venue": "In Proc. ACL/HLT.",
        "citeRegEx": "Koo et al\\.,? 2008",
        "shortCiteRegEx": "Koo et al\\.",
        "year": 2008,
        "abstract": "",
        "full_text": "",
        "sentence": " Dependency parsing and other NLP tasks have been shown to benefit from such a large unannotated corpus (Koo et al., 2008), and a variety of unsupervised feature learning methods have been shown to unilaterally improve the performance of supervised learning tasks (Turian et al.",
        "context": null
    },
    {
        "title": "The danish dependency treebank and the dtag treebank tool",
        "author": [
            "Matthias Trautner Kromann."
        ],
        "venue": "Proceedings of the Second Workshop on Treebanks and Linguistic Theories (TLT), page 217.",
        "citeRegEx": "Kromann.,? 2003",
        "shortCiteRegEx": "Kromann.",
        "year": 2003,
        "abstract": "",
        "full_text": "",
        "sentence": " 10% Danish DDT\u2020 (Kromann, 2003) 73.",
        "context": null
    },
    {
        "title": "Building a large annotated corpus of english: The penn treebank",
        "author": [
            "Mitchell P Marcus",
            "Mary Ann Marcinkiewicz",
            "Beatrice Santorini."
        ],
        "venue": "Computational linguistics, 19(2):313\u2013330.",
        "citeRegEx": "Marcus et al\\.,? 1993",
        "shortCiteRegEx": "Marcus et al\\.",
        "year": 1993,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " 00% English PennTreebank (Marcus et al., 1993) 75.",
        "context": null
    },
    {
        "title": "Recurrent neural network based language model",
        "author": [
            "T. Mikolov",
            "M. Karafi\u00e1t",
            "L. Burget",
            "J. Cernocky",
            "S. Khudanpur."
        ],
        "venue": "Proceedings of Interspeech.",
        "citeRegEx": "Mikolov et al\\.,? 2010",
        "shortCiteRegEx": "Mikolov et al\\.",
        "year": 2010,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Recent developments have led to state-of-art performance in several NLP tasks such as language modeling (Bengio et al., 2006; Mikolov et al., 2010), and syntactic tasks such as sequence tagging (Collobert et al.",
        "context": null
    },
    {
        "title": "A scalable hierarchical distributed language model",
        "author": [
            "Andriy Mnih",
            "Geoffrey E Hinton."
        ],
        "venue": "Advances in neural information processing systems, 21:1081\u2013 1088.",
        "citeRegEx": "Mnih and Hinton.,? 2009",
        "shortCiteRegEx": "Mnih and Hinton.",
        "year": 2009,
        "abstract": "",
        "full_text": "",
        "sentence": " Several suggestions have been proposed to speed up the training procedure, either by changing the model architecture to exploit an algorithmic speedup (Mnih and Hinton, 2009; Morin and Bengio, 2005) or by esti-",
        "context": null
    },
    {
        "title": "Hierarchical probabilistic neural network language model",
        "author": [
            "Frederic Morin",
            "Yoshua Bengio."
        ],
        "venue": "Proceedings of the international workshop on artificial intelligence and statistics, pages 246\u2013252.",
        "citeRegEx": "Morin and Bengio.,? 2005",
        "shortCiteRegEx": "Morin and Bengio.",
        "year": 2005,
        "abstract": "",
        "full_text": "",
        "sentence": " Several suggestions have been proposed to speed up the training procedure, either by changing the model architecture to exploit an algorithmic speedup (Mnih and Hinton, 2009; Morin and Bengio, 2005) or by esti-",
        "context": null
    },
    {
        "title": "Babelnet: Building a very large multilingual semantic network",
        "author": [
            "Roberto Navigli",
            "Simone Paolo Ponzetto."
        ],
        "venue": "Proceedings of the 48th annual meeting of the association for computational linguistics, pages 216\u2013225. Association for Computational Lin-",
        "citeRegEx": "Navigli and Ponzetto.,? 2010",
        "shortCiteRegEx": "Navigli and Ponzetto.",
        "year": 2010,
        "abstract": "",
        "full_text": "",
        "sentence": " Particularly, Wikipedia is well suited for multilingual applications (Navigli and Ponzetto, 2010).",
        "context": null
    },
    {
        "title": "Talbanken05: A swedish treebank with phrase structure and dependency annotation",
        "author": [
            "Joakim Nivre",
            "Jens Nilsson",
            "Johan Hall."
        ],
        "venue": "Proceedings of the fifth International Conference on Language Resources and Evaluation (LREC), pages 1392\u20131395.",
        "citeRegEx": "Nivre et al\\.,? 2006",
        "shortCiteRegEx": "Nivre et al\\.",
        "year": 2006,
        "abstract": "",
        "full_text": "",
        "sentence": " 60% Swedish Talbanken05\u2020 (Nivre et al., 2006) 83.",
        "context": null
    },
    {
        "title": "The CoNLL 2007 shared task on dependency parsing",
        "author": [
            "Joakim Nivre",
            "Johan Hall",
            "Sandra K\u00fcbler",
            "Ryan McDonald",
            "Jens Nilsson",
            "Sebastian Riedel",
            "Deniz Yuret."
        ],
        "venue": "Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pages",
        "citeRegEx": "Nivre et al\\.,? 2007",
        "shortCiteRegEx": "Nivre et al\\.",
        "year": 2007,
        "abstract": "",
        "full_text": "",
        "sentence": " Despite recent momentum towards developing multilingual tools (Nivre et al., 2007; Haji\u010d et al., 2009; Pradhan et al., 2012), most of NLP",
        "context": null
    },
    {
        "title": "A universal part-of-speech tagset",
        "author": [
            "Slav Petrov",
            "Dipanjan Das",
            "Ryan McDonald."
        ],
        "venue": "Nicoletta Calzolari (Conference Chair), Khalid Choukri, Thierry Declerck, Mehmet U\u011fur Do\u011fan, Bente Maegaard, Joseph Mariani, Jan Odijk, and Stelios Piperidis, ed-",
        "citeRegEx": "Petrov et al\\.,? 2012",
        "shortCiteRegEx": "Petrov et al\\.",
        "year": 2012,
        "abstract": "",
        "full_text": "",
        "sentence": " The results are compared to the TnT tagger results reported by (Petrov et al., 2012). We train and test our models on the universal tagset proposed by (Petrov et al., 2012).",
        "context": null
    },
    {
        "title": "CoNLL2012 shared task: Modeling multilingual unrestricted coreference in OntoNotes",
        "author": [
            "Sameer Pradhan",
            "Alessandro Moschitti",
            "Nianwen Xue",
            "Olga Uryupina",
            "Yuchen Zhang."
        ],
        "venue": "Proceedings of the Sixteenth Conference on Computational Natu-",
        "citeRegEx": "Pradhan et al\\.,? 2012",
        "shortCiteRegEx": "Pradhan et al\\.",
        "year": 2012,
        "abstract": "",
        "full_text": "",
        "sentence": " Despite recent momentum towards developing multilingual tools (Nivre et al., 2007; Haji\u010d et al., 2009; Pradhan et al., 2012), most of NLP",
        "context": null
    },
    {
        "title": "Learning representations by backpropagating errors",
        "author": [
            "David E Rumelhart",
            "Geoffrey E Hinton",
            "Ronald J Williams."
        ],
        "venue": "Cognitive modeling, 1:213.",
        "citeRegEx": "Rumelhart et al\\.,? 2002",
        "shortCiteRegEx": "Rumelhart et al\\.",
        "year": 2002,
        "abstract": "",
        "full_text": "",
        "sentence": " scent (Bottou, 1991), and update the parameters which contributed to the error using backpropagation (Rumelhart et al., 2002).",
        "context": null
    },
    {
        "title": "The Contemporary Chinese Dic",
        "author": [
            "Lu Shuxiang"
        ],
        "venue": null,
        "citeRegEx": "Shuxiang.,? \\Q2004\\E",
        "shortCiteRegEx": "Shuxiang.",
        "year": 2004,
        "abstract": "",
        "full_text": "",
        "sentence": " This is expected given the limited size vocabulary of the language - the number of entries in the Contemporary Chinese Dictionary are estimated to be 65 thousand words (Shuxiang, 2004).",
        "context": null
    }
]