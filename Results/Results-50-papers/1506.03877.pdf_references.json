[
    {
        "title": "A fast learning algorithm for deep belief nets",
        "author": [
            "Geoffrey E. Hinton",
            "Simon Osindero",
            "Yee Whye Teh"
        ],
        "venue": "Neural Computation,",
        "citeRegEx": "1",
        "shortCiteRegEx": "1",
        "year": 2006,
        "abstract": " We show how to use \u201ccomplementary priors\u201d to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind. ",
        "full_text": "",
        "sentence": " This is especially true for models with multiple layers of deterministic or stochastic variables, which is unfortunate because it has been argued previously [1, 2] that deeper generative models have the potential to capture higher-level abstractions and thus generalize better.",
        "context": null
    },
    {
        "title": "Learning deep architectures for AI",
        "author": [
            "Yoshua Bengio"
        ],
        "venue": "Now Publishers,",
        "citeRegEx": "2",
        "shortCiteRegEx": "2",
        "year": 2009,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " This is especially true for models with multiple layers of deterministic or stochastic variables, which is unfortunate because it has been argued previously [1, 2] that deeper generative models have the potential to capture higher-level abstractions and thus generalize better.",
        "context": null
    },
    {
        "title": "The wake-sleep algorithm for unsupervised neural networks",
        "author": [
            "Geoffrey E. Hinton",
            "Peter Dayan",
            "Brendan J. Frey",
            "Radford M. Neal"
        ],
        "venue": "Science, 268:1558\u20131161,",
        "citeRegEx": "3",
        "shortCiteRegEx": "3",
        "year": 1995,
        "abstract": "",
        "full_text": "",
        "sentence": " With the Helmholtz machine [3, 4], a concept was introduced that proposed to not only fit a powerful but intractable generative model p(x,h) to the training data, but also to jointly train an approximate inference model q(h|x). This basic idea has been applied and enhanced many times; initially with the wake-sleep algorithm (WS, [3, 5]) and more recently with the variational autoencoder (VAE, [6]), stochastic backpropagation and approximate inference in deep generative models [7], neural variational inference and learning (NVIL, [8]) and reweighted wake-sleep (RWS, [9]).",
        "context": null
    },
    {
        "title": "The Helmholtz machine",
        "author": [
            "Peter Dayan",
            "Geoffrey E Hinton",
            "Radford M Neal",
            "Richard S Zemel"
        ],
        "venue": "Neural computation,",
        "citeRegEx": "4",
        "shortCiteRegEx": "4",
        "year": 1995,
        "abstract": " Discovering the structure inherent in a set of patterns is a fundamental aim of statistical inference or learning. One fruitful approach is to build a parameterized stochastic generative model, independent draws from which are likely to produce the patterns. For all but the simplest generative models, each pattern can be generated in exponentially many ways. It is thus intractable to adjust the parameters to maximize the probability of the observed patterns. We describe a way of finessing this combinatorial explosion by maximizing an easily computed lower bound on the probability of the observations. Our method can be viewed as a form of hierarchical self-supervised learning that may relate to the function of bottom-up and top-down cortical processing pathways. ",
        "full_text": "",
        "sentence": " With the Helmholtz machine [3, 4], a concept was introduced that proposed to not only fit a powerful but intractable generative model p(x,h) to the training data, but also to jointly train an approximate inference model q(h|x).",
        "context": null
    },
    {
        "title": "Varieties of helmholtz machine",
        "author": [
            "Peter Dayan",
            "Geoffrey E Hinton"
        ],
        "venue": "Neural Networks,",
        "citeRegEx": "5",
        "shortCiteRegEx": "5",
        "year": 1996,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " This basic idea has been applied and enhanced many times; initially with the wake-sleep algorithm (WS, [3, 5]) and more recently with the variational autoencoder (VAE, [6]), stochastic backpropagation and approximate inference in deep generative models [7], neural variational inference and learning (NVIL, [8]) and reweighted wake-sleep (RWS, [9]).",
        "context": null
    },
    {
        "title": "Auto-encoding variational bayes",
        "author": [
            "Durk P. Kingma",
            "Max Welling"
        ],
        "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),",
        "citeRegEx": "6",
        "shortCiteRegEx": "6",
        "year": 2014,
        "abstract": "This paper employs the Auto-Encoding Variational Bayes (AEVB) estimator based on Stochastic Gradient Variational Bayes (SGVB), designed to optimize recognition models for challenging posterior distributions and large-scale datasets. It has been applied to the mnist dataset and extended to form a Dynamic Bayesian Network (DBN) in the context of time series. The paper delves into Bayesian inference, variational methods, and the fusion of Variational Autoencoders (VAEs) and variational techniques. Emphasis is placed on reparameterization for achieving efficient optimization. AEVB employs VAEs as an approximation for intricate posterior distributions.",
        "full_text": "",
        "sentence": " This basic idea has been applied and enhanced many times; initially with the wake-sleep algorithm (WS, [3, 5]) and more recently with the variational autoencoder (VAE, [6]), stochastic backpropagation and approximate inference in deep generative models [7], neural variational inference and learning (NVIL, [8]) and reweighted wake-sleep (RWS, [9]).",
        "context": null
    },
    {
        "title": "Stochastic backpropagation and approximate inference in deep generative models",
        "author": [
            "Danilo J. Rezende",
            "Shakir Mohamed",
            "Daan Wierstra"
        ],
        "venue": "In ICML\u20192014,",
        "citeRegEx": "7",
        "shortCiteRegEx": "7",
        "year": 2014,
        "abstract": "We marry ideas from deep neural networks and approximate Bayesian inference\nto derive a generalised class of deep, directed generative models, endowed with\na new algorithm for scalable inference and learning. Our algorithm introduces a\nrecognition model to represent approximate posterior distributions, and that\nacts as a stochastic encoder of the data. We develop stochastic\nback-propagation -- rules for back-propagation through stochastic variables --\nand use this to develop an algorithm that allows for joint optimisation of the\nparameters of both the generative and recognition model. We demonstrate on\nseveral real-world data sets that the model generates realistic samples,\nprovides accurate imputations of missing data and is a useful tool for\nhigh-dimensional data visualisation.",
        "full_text": "Stochastic Backpropagation and Approximate Inference\nin Deep Generative Models\nDanilo J. Rezende, Shakir Mohamed, Daan Wierstra\n{danilor, shakir, daanw}@google.com\nGoogle DeepMind, London\nAbstract\nWe marry ideas from deep neural networks\nand approximate Bayesian inference to derive\na generalised class of deep, directed genera-\ntive models, endowed with a new algorithm\nfor scalable inference and learning. Our algo-\nrithm introduces a recognition model to rep-\nresent an approximate posterior distribution\nand uses this for optimisation of a variational\nlower bound.\nWe develop stochastic back-\npropagation \u2013 rules for gradient backpropa-\ngation through stochastic variables \u2013 and de-\nrive an algorithm that allows for joint optimi-\nsation of the parameters of both the genera-\ntive and recognition models. We demonstrate\non several real-world data sets that by using\nstochastic backpropagation and variational\ninference, we obtain models that are able to\ngenerate realistic samples of data, allow for\naccurate imputations of missing data, and\nprovide a useful tool for high-dimensional\ndata visualisation.\n1. Introduction\nThere is an immense e\ufb00ort in machine learning and\nstatistics to develop accurate and scalable probabilistic\nmodels of data. Such models are called upon whenever\nwe are faced with tasks requiring probabilistic reason-\ning, such as prediction, missing data imputation and\nuncertainty estimation; or in simulation-based analy-\nses, common in many scienti\ufb01c \ufb01elds such as genetics,\nrobotics and control that require generating a large\nnumber of independent samples from the model.\nRecent e\ufb00orts to develop generative models have fo-\ncused on directed models, since samples are easily ob-\ntained by ancestral sampling from the generative pro-\ncess. Directed models such as belief networks and sim-\nilar latent variable models (Dayan et al., 1995; Frey,\n1996; Saul et al., 1996; Bartholomew & Knott, 1999;\nProceedings of the 31 st International Conference on Ma-\nchine Learning, Beijing, China, 2014. JMLR: W&CP vol-\nume 32. Copyright 2014 by the author(s).\nUria et al., 2014; Gregor et al., 2014) can be easily sam-\npled from, but in most cases, e\ufb03cient inference algo-\nrithms have remained elusive. These e\ufb00orts, combined\nwith the demand for accurate probabilistic inferences\nand fast simulation, lead us to seek generative models\nthat are i) deep, since hierarchical architectures allow\nus to capture complex structure in the data, ii) al-\nlow for fast sampling of fantasy data from the inferred\nmodel, and iii) are computationally tractable and scal-\nable to high-dimensional data.\nWe meet these desiderata by introducing a class of\ndeep, directed generative models with Gaussian la-\ntent variables at each layer. To allow for e\ufb03cient and\ntractable inference, we use introduce an approximate\nrepresentation of the posterior over the latent variables\nusing a recognition model that acts as a stochastic en-\ncoder of the data. For the generative model, we de-\nrive the objective function for optimisation using vari-\national principles; for the recognition model, we spec-\nify its structure and regularisation by exploiting recent\nadvances in deep learning. Using this construction, we\ncan train the entire model by a modi\ufb01ed form of gra-\ndient backpropagation that allows for optimisation of\nthe parameters of both the generative and recognition\nmodels jointly.\nWe build upon the large body of prior work (in section\n6) and make the following contributions:\n\u2022 We combine ideas from deep neural networks and\nprobabilistic latent variable modelling to derive a\ngeneral class of deep, non-linear latent Gaussian\nmodels (section 2).\n\u2022 We present a new approach for scalable varia-\ntional inference that allows for joint optimisation\nof both variational and model parameters by ex-\nploiting the properties of latent Gaussian distri-\nbutions and gradient backpropagation (sections 3\nand 4).\n\u2022 We provide a comprehensive and systematic eval-\nuation of the model demonstrating its applicabil-\nity to problems in simulation, visualisation, pre-\ndiction and missing data imputation (section 5).\narXiv:1401.4082v3  [stat.ML]  30 May 2014\nStochastic Backpropagation in DLGMs\nhn,2\nhn,1\nvn\nn = 1 . . . N\n\u03b8g\n(a)\ngenerative\nrecognition\n. . .\n. . .\n. . .\n. . .\n\u00b5\nC\n\u00b5\nC\n. . .\n\u00b5\nC\n. . .\n. . .\n. . .\n(b)\nFigure 1. (a) Graphical model for DLGMs (5).\n(b) The\ncorresponding computational graph. Black arrows indicate\nthe forward pass of sampling from the recognition and gen-\nerative models: Solid lines indicate propagation of deter-\nministic activations, dotted lines indicate propagation of\nsamples. Red arrows indicate the backward pass for gra-\ndient computation: Solid lines indicate paths where deter-\nministic backpropagation is used, dashed arrows indicate\nstochastic backpropagation.\n2. Deep Latent Gaussian Models\nDeep latent Gaussian models (DLGMs) are a general\nclass of deep directed graphical models that consist of\nGaussian latent variables at each layer of a process-\ning hierarchy. The model consists of L layers of latent\nvariables. To generate a sample from the model, we\nbegin at the top-most layer (L) by drawing from a\nGaussian distribution. The activation hl at any lower\nlayer is formed by a non-linear transformation of the\nlayer above hl+1, perturbed by Gaussian noise. We\ndescend through the hierarchy and generate observa-\ntions v by sampling from the observation likelihood\nusing the activation of the lowest layer h1. This pro-\ncess is described graphically in \ufb01gure 1(a).\nThis generative process is described as follows:\n\u03bel \u223cN(\u03bel|0, I),\nl = 1, . . . , L\n(1)\nhL = GL\u03beL,\n(2)\nhl = Tl(hl+1) + Gl\u03bel,\nl = 1 . . . L \u22121\n(3)\nv \u223c\u03c0(v|T0(h1)),\n(4)\nwhere \u03bel are mutually independent Gaussian variables.\nThe transformations Tl represent multi-layer percep-\ntrons (MLPs) and Gl are matrices.\nAt the visible\nlayer, the data is generated from any appropriate dis-\ntribution \u03c0(v|\u00b7) whose parameters are speci\ufb01ed by a\ntransformation of the \ufb01rst latent layer. Throughout\nthe paper we refer to the set of parameters in this gen-\nerative model by \u03b8g, i.e. the parameters of the maps\nTl and the matrices Gl. This construction allows us to\nmake use of as many deterministic and stochastic lay-\ners as needed. We adopt a weak Gaussian prior over\n\u03b8g, p(\u03b8g) = N(\u03b8|0, \u03baI).\nThe joint probability distribution of this model can be\nexpressed in two equivalent ways:\np(v,h)=p(v|h1,\u03b8g)p(hL|\u03b8g)p(\u03b8g)\nL\u22121\nY\nl=1\npl(hl|hl+1,\u03b8g) (5)\np(v,\u03be)= p(v|h1(\u03be1...L), \u03b8g)p(\u03b8g)\nL\nY\nl=1\nN(\u03be|0, I).\n(6)\nThe conditional distributions p(hl|hl+1) are implic-\nitly de\ufb01ned by equation (3) and are Gaussian dis-\ntributions with mean \u00b5l = Tl(hl+1) and covariance\nSl = GlG\u22a4\nl .\nEquation (6) makes explicit that this\ngenerative model works by applying a complex non-\nlinear transformation to a spherical Gaussian distribu-\ntion p(\u03be) = QL\nl=1 N(\u03bel|0, I) such that the transformed\ndistribution tries to match the empirical distribution.\nA graphical model corresponding to equation (5) is\nshown in \ufb01gure 1(a).\nThis speci\ufb01cation for deep latent Gaussian models\n(DLGMs) generalises a number of well known mod-\nels. When we have only one layer of latent variables\nand use a linear mapping T(\u00b7), we recover factor anal-\nysis (Bartholomew & Knott, 1999) \u2013 more general\nmappings allow for a non-linear factor analysis (Lap-\npalainen & Honkela, 2000). When the mappings are\nof the form Tl(h) = Alf(h) + bl, for simple element-\nwise non-linearities f such as the probit function or\nthe recti\ufb01ed linearity, we recover the non-linear Gaus-\nsian belief network (Frey & Hinton, 1999).\nWe de-\nscribe the relationship to other existing models in sec-\ntion 6.\nGiven this speci\ufb01cation, our key task is to\ndevelop a method for tractable inference. A number\nof approaches are known and widely used, and include:\nmean-\ufb01eld variational EM (Beal, 2003); the wake-sleep\nalgorithm (Dayan, 2000); and stochastic variational\nmethods and related control-variate estimators (Wil-\nson, 1984; Williams, 1992; Ho\ufb00man et al., 2013). We\nalso follow a stochastic variational approach, but shall\ndevelop an alternative to these existing inference algo-\nrithms that overcomes many of their limitations and\nthat is both scalable and e\ufb03cient.\n3. Stochastic Backpropagation\nGradient descent methods in latent variable mod-\nels\ntypically\nrequire\ncomputations\nof\nthe\nform\n\u2207\u03b8Eq\u03b8 [f(\u03be)], where the expectation is taken with re-\nspect to a distribution q\u03b8(\u00b7) with parameters \u03b8, and f\nis a loss function that we assume to be integrable and\nsmooth. This quantity is di\ufb03cult to compute directly\nsince i) the expectation is unknown for most problems,\nand ii) there is an indirect dependency on the param-\neters of q over which the expectation is taken.\nWe now develop the key identities that are used to\nallow for e\ufb03cient inference by exploiting speci\ufb01c prop-\nStochastic Backpropagation in DLGMs\nerties of the problem of computing gradients through\nrandom variables.\nWe refer to this computational\nstrategy as stochastic backpropagation.\n3.1. Gaussian Backpropagation (GBP)\nWhen the distribution q is a K-dimensional Gaussian\nN(\u03be|\u00b5, C) the required gradients can be computed us-\ning the Gaussian gradient identities:\n\u2207\u00b5iEN (\u00b5,C) [f(\u03be)] = EN (\u00b5,C) [\u2207\u03beif(\u03be)] ,\n(7)\n\u2207CijEN (\u00b5,C) [f(\u03be)] = 1\n2EN (\u00b5,C)\nh\n\u22072\n\u03bei,\u03bejf(\u03be)\ni\n,\n(8)\nwhich are due to the theorems by Bonnet (1964) and\nPrice (1958), respectively. These equations are true\nin expectation for any integrable and smooth func-\ntion f(\u03be). Equation (7) is a direct consequence of the\nlocation-scale transformation for the Gaussian (dis-\ncussed in section 3.2). Equation (8) can be derived\nby successive application of the product rule for in-\ntegrals; we provide the proofs for these identities in\nappendix B.\nEquations (7) and (8) are especially interesting since\nthey allow for unbiased gradient estimates by using a\nsmall number of samples from q. Assume that both\nthe mean \u00b5 and covariance matrix C depend on a\nparameter vector \u03b8. We are now able to write a general\nrule for Gaussian gradient computation by combining\nequations (7) and (8) and using the chain rule:\n\u2207\u03b8EN(\u00b5,C)[f(\u03be)]=EN(\u00b5,C)\n\u0014\ng\u22a4\u2202\u00b5\n\u2202\u03b8 + 1\n2Tr\n\u0012\nH\u2202C\n\u2202\u03b8\n\u0013\u0015\n(9)\nwhere g and H are the gradient and the Hessian of the\nfunction f(\u03be), respectively. Equation (9) can be inter-\npreted as a modi\ufb01ed backpropagation rule for Gaus-\nsian distributions that takes into account the gradients\nthrough the mean \u00b5 and covariance C. This reduces\nto the standard backpropagation rule when C is con-\nstant. Unfortunately this rule requires knowledge of\nthe Hessian matrix of f(\u03be), which has an algorithmic\ncomplexity O(K3). For inference in DLGMs, we later\nintroduce an unbiased though higher variance estima-\ntor that requires only quadratic complexity.\n3.2. Generalised Backpropagation Rules\nWe describe two approaches to derive general back-\npropagation rules for non-Gaussian q-distributions.\nUsing the product rule for integrals. For many\nexponential family distributions, it is possible to \ufb01nd\na function B(\u03be; \u03b8) to ensure that\n\u2207\u03b8Ep(\u03be|\u03b8)[f(\u03be)]== \u2212Ep(\u03be|\u03b8)[\u2207\u03be[B(\u03be; \u03b8)f(\u03be)]].\nThat is, we express the gradient with respect to the\nparameters of q as an expectation of gradients with\nrespect to the random variables themselves. This ap-\nproach can be used to derive rules for many distribu-\ntions such as the Gaussian, inverse Gamma and log-\nNormal. We discuss this in more detail in appendix\nC.\nUsing suitable co-ordinate transformations.\nWe can also derive stochastic backpropagation rules\nfor any distribution that can be written as a smooth,\ninvertible transformation of a standard base distribu-\ntion. For example, any Gaussian distribution N(\u00b5, C)\ncan be obtained as a transformation of a spherical\nGaussian \u03f5 \u223cN(0, I), using the transformation y =\n\u00b5+R\u03f5 and C = RR\u22a4. The gradient of the expectation\nwith respect to R is then:\n\u2207REN(\u00b5,C) [f(\u03be)] = \u2207REN (0,I) [f(\u00b5 + R\u03f5)]\n= EN (0,I)\n\u0002\n\u03f5g\u22a4\u0003\n,\n(10)\nwhere g is the gradient of f evaluated at \u00b5 + R\u03f5 and\nprovides a lower-cost alternative to Price\u2019s theorem\n(8).\nSuch transformations are well known for many\ndistributions, especially those with a self-similarity\nproperty or location-scale formulation, such as the\nGaussian, Student\u2019s t-distribution, stable distribu-\ntions, and generalised extreme value distributions.\nStochastic backpropagation in other contexts.\nThe Gaussian gradient identities described above do\nnot appear to be widely used. These identities have\nbeen recognised by Opper & Archambeau (2009) for\nvariational inference in Gaussian process regression,\nand following this work, by Graves (2011) for param-\neter learning in large neural networks. Concurrently\nwith this paper, Kingma & Welling (2014) present an\nalternative discussion of stochastic backpropagation.\nOur approaches were developed simultaneously and\nprovide complementary perspectives on the use and\nderivation of stochastic backpropagation rules.\n4. Scalable Inference in DLGMs\nWe use the matrix V to refer to the full data set of\nsize N \u00d7 D with observations vn = [vn1, . . . , vnD]\u22a4.\n4.1. Free Energy Objective\nTo perform inference in DLGMs we must integrate out\nthe e\ufb00ect of any latent variables \u2013 this requires us to\ncompute the integrated or marginal likelihood. In gen-\neral, this will be an intractable integration and instead\nwe optimise a lower bound on the marginal likelihood.\nWe introduce an approximate posterior distribution\nq(\u00b7) and apply Jensen\u2019s inequality following the varia-\ntional principle (Beal, 2003) to obtain:\nStochastic Backpropagation in DLGMs\nL(V) = \u2212log p(V) = \u2212log\nZ\np(V|\u03be, \u03b8g)p(\u03be, \u03b8g)d\u03be\n= \u2212log\nZ q(\u03be)\nq(\u03be)p(V|\u03be, \u03b8g)p(\u03be, \u03b8g)d\u03be\n(11)\n\u2264F(V)=DKL[q(\u03be)\u2225p(\u03be)]\u2212Eq [log p(V|\u03be,\u03b8g)p(\u03b8g)] .\nThis objective consists of two terms:\nthe \ufb01rst is\nthe KL-divergence between the variational distribution\nand the prior distribution (which acts a regulariser),\nand the second is a reconstruction error.\nWe specify the approximate posterior as a distribution\nq(\u03be|v) that is conditioned on the observed data. This\ndistribution can be speci\ufb01ed as any directed acyclic\ngraph where each node of the graph is a Gaussian\nconditioned, through linear or non-linear transforma-\ntions, on its parents.\nThe joint distribution in this\ncase is non-Gaussian, but stochastic backpropagation\ncan still be applied.\nFor simplicity, we use a q(\u03be|v) that is a Gaussian dis-\ntribution that factorises across the L layers (but not\nnecessarily within a layer):\nq(\u03be|V, \u03b8r) =\nN\nY\nn=1\nL\nY\nl=1\nN\n\u0000\u03ben,l|\u00b5l(vn), Cl(vn)\n\u0001\n,\n(12)\nwhere the mean \u00b5l(\u00b7) and covariance Cl(\u00b7) are generic\nmaps represented by deep neural networks.\nParam-\neters of the q-distribution are denoted by the vector\n\u03b8r.\nFor a Gaussian prior and a Gaussian recognition\nmodel, the KL term in (11) can be computed ana-\nlytically and the free energy becomes:\nDKL[N(\u00b5,C)\u2225N(0,I)]= 1\n2\n\u0002\nTr(C)\u2212log |C|+\u00b5\u22a4\u00b5\u2212D\n\u0003\n,\nF(V) = \u2212\nX\nn\nEq [log p(vn|h(\u03ben))] +\n1\n2\u03ba\u2225\u03b8g\u22252\n+ 1\n2\nX\nn,l\n\u0002\n\u2225\u00b5n,l\u22252+Tr(Cn,l)\u2212log |Cn,l|\u22121\n\u0003\n, (13)\nwhere Tr(C) and |C| indicate the trace and the deter-\nminant of the covariance matrix C, respectively.\nThe speci\ufb01cation of an approximate posterior distri-\nbution that is conditioned on the observed data is the\n\ufb01rst component of an e\ufb03cient variational inference al-\ngorithm. We shall refer to the distribution q(\u03be|v) (12)\nas a recognition model, whose design is independent\nof the generative model. A recognition model allows\nus introduce a form of amortised inference (Gershman\n& Goodman, 2014) for variational methods in which\nwe share statistical strength by allowing for generali-\nsation across the posterior estimates for all latent vari-\nables using a model. The implication of this general-\nisation ability is: faster convergence during training;\nand faster inference at test time since we only require a\nsingle pass through the recognition model, rather than\nneeding to perform any iterative computations (such\nas in a generalised E-step).\nTo allow for the best possible inference, the speci\ufb01ca-\ntion of the recognition model must be \ufb02exible enough\nto provide an accurate approximation of the poste-\nrior distribution \u2013 motivating the use of deep neu-\nral networks.\nWe regularise the recognition model\nby introducing additional noise, speci\ufb01cally, bit-\ufb02ip\nor drop-out noise at the input layer and small addi-\ntional Gaussian noise to samples from the recognition\nmodel. We use recti\ufb01ed linear activation functions as\nnon-linearities for any deterministic layers of the neu-\nral network. We found that such regularisation is es-\nsential and without it the recognition model is unable\nto provide accurate inferences for unseen data points.\n4.2. Gradients of the Free Energy\nTo optimise (13), we use Monte Carlo methods for\nany expectations and use stochastic gradient descent\nfor optimisation. For optimisation, we require e\ufb03cient\nestimators of the gradients of all terms in equation\n(13) with respect to the parameters \u03b8g and \u03b8r of the\ngenerative and the recognition models, respectively.\nThe gradients with respect to the jth generative pa-\nrameter \u03b8g\nj can be computed using:\n\u2207\u03b8g\nj F(V) = \u2212Eq\nh\n\u2207\u03b8g\nj log p(V|h)\ni\n+ 1\n\u03ba\u03b8g\nj .\n(14)\nAn unbiased estimator of \u2207\u03b8g\nj F(V) is obtained by\napproximating equation (14) with a small number of\nsamples (or even a single sample) from the recognition\nmodel q.\nTo obtain gradients with respect to the recognition\nparameters \u03b8r, we use the rules for Gaussian back-\npropagation developed in section 3.\nTo address the\ncomplexity of the Hessian in the general rule (9), we\nuse the co-ordinate transformation for the Gaussian to\nwrite the gradient with respect to the factor matrix R\ninstead of the covariance C (recalling C = RR\u22a4) de-\nrived in equation (10), where derivatives are computed\nfor the function f(\u03be) = log p(v|h(\u03be)).\nThe gradients of F(v) in equation (13) with respect to\nthe variational mean \u00b5l(v) and the factors Rl(v) are:\n\u2207\u00b5lF(v) = \u2212Eq\nh\n\u2207\u03bel log p(v|h(\u03be))\ni\n+ \u00b5l,\n(15)\n\u2207Rl,i,jF(v) = \u22121\n2Eq\n\u0002\n\u03f5l,j\u2207\u03bel,i log p(v|h(\u03be))\n\u0003\n+ 1\n2\u2207Rl,i,j [Tr Cn,l \u2212log |Cn,l|] ,\n(16)\nwhere the gradients \u2207Rl,i,j [Tr Cn,l \u2212log |Cn,l|] are\ncomputed by backpropagation. Unbiased estimators\nof the gradients (15) and (16) are obtained jointly\nStochastic Backpropagation in DLGMs\nAlgorithm 1 Learning in DLGMs\nwhile hasNotConverged() do\nV \u2190getMiniBatch()\n\u03ben \u223cq(\u03ben|vn) (bottom-up pass) eq. (12)\nh \u2190h(\u03be) (top-down pass) eq. (3)\nupdateGradients() eqs (14) \u2013 (17)\n\u03b8g,r \u2190\u03b8g,r + \u2206\u03b8g,r\nend while\nby sampling from the recognition model \u03be \u223cq(\u03be|v)\n(bottom-up pass) and updating the values of the gener-\native model layers using equation (3) (top-down pass).\nFinally the gradients \u2207\u03b8r\nj F(v) obtained from equa-\ntions (15) and (16) are:\n\u2207\u03b8rF(v)=\u2207\u00b5F(v)\u22a4\u2202\u00b5\n\u2202\u03b8r +Tr\n\u0012\n\u2207RF(v) \u2202R\n\u2202\u03b8r\n\u0013\n.\n(17)\nThe gradients (14) \u2013 (17) are now used to descend\nthe free-energy surface with respect to both the gen-\nerative and recognition parameters in a single optimi-\nsation step. Figure 1(b) shows the \ufb02ow of computa-\ntion in DLGMs. Our algorithm proceeds by \ufb01rst per-\nforming a forward pass (black arrows), consisting of a\nbottom-up (recognition) phase and a top-down (gener-\nation) phase, which updates the hidden activations of\nthe recognition model and parameters of any Gaussian\ndistributions, and then a backward pass (red arrows)\nin which gradients are computed using the appropriate\nbackpropagation rule for deterministic and stochastic\nlayers. We take a descent step using:\n\u2206\u03b8g,r = \u2212\u0393g,r\u2207\u03b8g,rF(V),\n(18)\nwhere \u0393g,r is a diagonal pre-conditioning matrix com-\nputed using the RMSprop heuristic1.\nThe learning\nprocedure is summarised in algorithm 1.\n4.3. Gaussian Covariance Parameterisation\nThere are a number of approaches for parameterising\nthe covariance matrix of the recognition model q(\u03be).\nMaintaining a full covariance matrix C in equation\n(13) would entail an algorithmic complexity of O(K3)\nfor training and sampling per layer, where K is the\nnumber of latent variables per layer.\nThe simplest approach is to use a diagonal covariance\nmatrix C = diag(d), where d is a K-dimensional vec-\ntor.\nThis approach is appealing since it allows for\nlinear-time computation and sampling, but only allows\nfor axis-aligned posterior distributions.\nWe can improve upon the diagonal approximation by\nparameterising the covarinace as a rank-1 matrix with\n1Described by G. Hinton, \u2018RMSprop: Divide the gradi-\nent by a running average of its recent magnitude\u2019, in Neural\nnetworks for machine learning, Coursera lecture 6e, 2012.\na diagonal correction. Using a vectors u and d, with\nD = diag(d), we parameterise the precision C\u22121 as:\nC\u22121 = D + uu\u22a4.\n(19)\nThis representation allows for arbitrary rotations of\nthe Gaussian distribution along one principal direction\nwith relatively few additional parameters (Magdon-\nIsmail & Purnell, 2010). By application of the matrix\ninversion lemma (Woodbury identity), we obtain the\ncovariance matrix in terms of d and u as:\nC = D\u22121 \u2212\u03b7D\u22121uu\u22a4D\u22121,\n\u03b7 =\n1\nu\u22a4D\n\u22121u+1,\nlog |C| = log \u03b7 \u2212log |D|.\n(20)\nThis allows both the trace Tr(C) and log |C| needed in\nthe computation of the Gaussian KL, as well as their\ngradients, to be computed in O(K) time per layer.\nThe factorisation C = RR\u22a4, with R a matrix of the\nsame size as C and can be computed directly in terms\nof d and u. One solution for R is:\nR = D\u22121\n2 \u2212\n\u0014 1 \u2212\u221a\u03b7\nu\u22a4D\u22121u\n\u0015\nD\u22121uu\u22a4D\u22121\n2 .\n(21)\nThe product of R with an arbitrary vector can be com-\nputed in O(K) without computing R explicitly. This\nalso allows us to sample e\ufb03ciently from this Gaussian,\nsince any Gaussian random variable \u03be with mean \u00b5\nand covariance matrix C = RR\u22a4can be written as\n\u03be = \u00b5 + R\u03f5, where \u03f5 is a standard Gaussian variate.\nSince this covariance parametrisation has linear cost\nin the number of latent variables, we can also use it to\nparameterise the variational distribution of all layers\njointly, instead of the factorised assumption in (12).\n4.4. Algorithm Complexity\nThe computational complexity of producing a sample\nfrom the generative model is O(L \u00afK2), where \u00afK is the\naverage number of latent variables per layer and L is\nthe number of layers (counting both deterministic and\nstochastic layers). The computational complexity per\ntraining sample during training is also O(L \u00afK2) \u2013 the\nsame as that of matching auto-encoder.\n5. Results\nGenerative models have a number of applications in\nsimulation, prediction, data visualisation, missing data\nimputation and other forms of probabilistic reason-\ning. We describe the testing methodology we use and\npresent results on a number of these tasks.\n5.1. Analysing the Approximate Posterior\nWe use sampling to evaluate the true posterior\ndistribution for a number of MNIST digits using\nthe binarised data set from Larochelle & Murray\nStochastic Backpropagation in DLGMs\n(a) Diagonal covariance\n(b) Low-rank covariance\nRank1\nDiag\nWake\u2212Sleep\nFA\n84\n88\n92\n96\n100\n104\nTest neg. marginal likelihood\n(c) Performance\nFigure 2. (a, b) Analysis of the true vs. approximate posterior for MNIST. Within each image we show four views of the\nsame posterior, zooming in on the region centred on the MAP (red) estimate. (c) Comparison of test log likelihoods.\nTable 1. Comparison of negative log-probabilities on the\ntest set for the binarised MNIST data.\nModel\n\u2212ln p(v)\nFactor Analysis\n106.00\nNLGBN (Frey & Hinton, 1999)\n95.80\nWake-Sleep (Dayan, 2000)\n91.3\nDLGM diagonal covariance\n87.30\nDLGM rank-one covariance\n86.60\nResults below from Uria et al. (2014)\nMoBernoullis K=10\n168.95\nMoBernoullis K=500\n137.64\nRBM (500 h, 25 CD steps) approx.\n86.34\nDBN 2hl approx.\n84.55\nNADE 1hl (\ufb01xed order)\n88.86\nNADE 1hl (\ufb01xed order, RLU, minibatch)\n88.33\nEoNADE 1hl (2 orderings)\n90.69\nEoNADE 1hl (128 orderings)\n87.71\nEoNADE 2hl (2 orderings)\n87.96\nEoNADE 2hl (128 orderings)\n85.10\n(2011). We visualise the posterior distribution for a\nmodel with two Gaussian latent variables in \ufb01gure 2.\nThe true posterior distribution is shown by the grey\nregions and was computed by importance sampling\nwith a large number of particles aligned in a grid\nbetween -5 and 5.\nIn \ufb01gure 2(a) we see that these\nposterior distributions are elliptical or spherical in\nshape and thus, it is reasonable to assume that they\ncan be well approximated by a Gaussian.\nSamples\nfrom the prior (green) are spread widely over the\nspace and very few samples fall in the region of\nsigni\ufb01cant posterior mass, explaining the ine\ufb03ciency\nof estimation methods that rely on samples from the\nprior. Samples from the recognition model (blue) are\nconcentrated on the posterior mass, indicating that\nthe recognition model has learnt the correct posterior\nstatistics, which should lead to e\ufb03cient learning.\nIn \ufb01gure 2(a) we see that samples from the recognition\nmodel are aligned to the axis and do not capture\nthe posterior correlation. The correlation is captured\nusing the structured covariance model in \ufb01gure 2(b).\nNot all posteriors are Gaussian in shape, but the\nrecognition places mass in the best location possible to\nprovide a reasonable approximation. As a benchmark\nfor comparison, the performance in terms of test\nlog-likelihood is shown in \ufb01gure 2(c), using the same\narchitecture, for factor analysis (FA), the wake-sleep\nalgorithm, and our approach using both the diagonal\nand structured covariance approaches.\nFor this ex-\nperiment, the generative model consists of 100 latent\nvariables feeding into a deterministic layer of 300\nnodes, which then feeds to the observation likelihood.\nWe use the same structure for the recognition model.\n5.2. Simulation and Prediction\nWe evaluate the performance of a three layer latent\nGaussian model on the MNIST data set. The model\nconsists of two deterministic layers with 200 hidden\nunits and a stochastic layer of 200 latent variables.\nWe use mini-batches of 200 observations and trained\nthe model using stochastic backpropagation. Samples\nfrom this model are shown in \ufb01gure 3(a).\nWe also\ncompare the test log-likelihood to a large number of\nexisting approaches in table 1.\nWe used the bina-\nrised dataset as in Uria et al. (2014) and quote the\nlog-likelihoods in the lower part of the table from this\nwork. These results show that our approach is com-\npetitive with some of the best models currently avail-\nable. The generated digits also match the true data\nwell and visually appear as good as some of the best\nvisualisations from these competing approaches.\nWe also analysed the performance of our model on\nthree high-dimensional real image data sets.\nThe\nNORB object recognition data set consists of 24, 300\nimages that are of size 96 \u00d7 96 pixels. We use a model\nconsisting of 1 deterministic layer of 400 hidden units\nand one stochastic layer of 100 latent variables. Sam-\nples produced from this model are shown in \ufb01gure\n4(a). The CIFAR10 natural images data set consists\nof 50, 000 RGB images that are of size 32 \u00d7 32 pix-\nels, which we split into random 8 \u00d7 8 patches. We use\nthe same model as used for the MNIST experiment\nand show samples from the model in \ufb01gure 4(b). The\nFrey faces data set consists of almost 2, 000 images of\ndi\ufb00erent facial expressions of size 28 \u00d7 20 pixels.\n5.3. Data Visualisation\nLatent variable models are often used for visualisa-\ntion of high-dimensional data sets.\nWe project the\nMNIST data set to a 2-dimensional latent space and\nuse this 2D embedding as a visualisation of the data \u2013\nan embedding for MNIST is shown in \ufb01gure 3(b). The\nclasses separate into di\ufb00erent regions, suggesting that\nsuch embeddings can be useful in understanding the\nstructure of high-dimensional data sets.\nStochastic Backpropagation in DLGMs\n(a) Left: Training data. Middle: Sampled pixel probabilities. Right: Model samples\n(b) 2D embedding.\nFigure 3. Performance on the MNIST dataset. For the visualisation, each colour corresponds to one of the digit classes.\n(a) NORB\n(b) CIFAR\n(c) Frey\nFigure 4. Sampled generated from DLGMs for three data sets: (a) NORB, (b) CIFAR 10, (c) Frey faces. In all images,\nthe left image shows samples from the training data and the right side shows the generated samples.\n5.4. Missing Data Imputation and Denoising\nWe demonstrate the ability of the model to impute\nmissing data using the street view house numbers\n(SVHN) data set (Netzer et al., 2011), which consists\nof 73, 257 images of size 32 \u00d7 32 pixels, and the Frey\nfaces and MNIST data sets. The performance of the\nmodel is shown in \ufb01gure 5.\nWe test the imputation ability under two di\ufb00erent\nmissingness types (Little & Rubin, 1987): Missing-at-\nRandom (MAR), where we consider 60% and 80% of\nthe pixels to be missing randomly, and Not Missing-at-\nRandom (NMAR), where we consider a square region\nof the image to be missing. The model produces very\ngood completions in both test cases. There is uncer-\ntainty in the identity of the image and this is re\ufb02ected\nin the errors in these completions as the resampling\nprocedure is run (see transitions from digit 9 to 7, and\ndigit 8 to 6 in \ufb01gure 5 ). This further demonstrates\nthe ability of the model to capture the diversity of the\nunderlying data. We do not integrate over the missing\nvalues, but use a procedure that simulates a Markov\nchain that we show converges to the true marginal dis-\ntribution of missing given observed pixels. The impu-\ntation procedure is discussed in appendix F.\n6. Discussion\nDirected Graphical Models.\nDLGMs form a\nuni\ufb01ed family of models that includes factor analy-\nsis (Bartholomew & Knott, 1999), non-linear factor\nanalysis (Lappalainen & Honkela, 2000), and non-\nFigure 5. Imputation results: Row 1, SVHN. Row 2, Frey\nfaces.\nRows 3\u20135, MNIST. Col.\n1 shows the true data.\nCol. 2 shows pixel locations set as missing in grey. The\nremaining columns show imputations for 15 iterations.\nlinear Gaussian belief networks (Frey & Hinton, 1999).\nOther related models include sigmoid belief networks\n(Saul et al., 1996) and deep auto-regressive net-\nworks (Gregor et al., 2014), which use auto-regressive\nBernoulli distributions at each layer instead of Gaus-\nsian distributions. The Gaussian process latent vari-\nable model and deep Gaussian processes (Lawrence,\n2005; Damianou & Lawrence, 2013) form the non-\nparametric analogue of our model and employ Gaus-\nStochastic Backpropagation in DLGMs\nsian process priors over the non-linear functions be-\ntween each layer. The neural auto-regressive density\nestimator (NADE) (Larochelle & Murray, 2011; Uria\net al., 2014) uses function approximation to model con-\nditional distributions within a directed acyclic graph.\nNADE is amongst the most competitive generative\nmodels currently available, but has several limitations,\nsuch as the inability to allow for deep representations\nand di\ufb03culties in extending to locally-connected mod-\nels (e.g., through the use of convolutional layers), pre-\nventing it from scaling easily to high-dimensional data.\nAlternative latent Gaussian inference.\nFew of\nthe alternative approaches for inferring latent Gaus-\nsian distributions meet the desiderata for scalable in-\nference we seek. The Laplace approximation has been\nconcluded to be a poor approximation in general, in\naddition to being computationally expensive. INLA is\nrestricted to models with few hyperparameters (< 10),\nwhereas our interest is in 100s-1000s. EP cannot be\napplied to latent variable models due to the inability\nto match moments of the joint distribution of latent\nvariables and model parameters. Furthermore, no re-\nliable methods exist for moment-matching with means\nand covariances formed by non-linear transformations\n\u2013 linearisation and importance sampling are two, but\nare either inaccurate or very slow. Thus, the the varia-\ntional approach we present remains a general-purpose\nand competitive approach for inference.\nMonte Carlo variance reduction.\nControl vari-\nate methods are amongst the most general and ef-\nfective techniques for variance reduction when Monte\nCarlo methods are used (Wilson, 1984). One popu-\nlar approach is the REINFORCE algorithm (Williams,\n1992), since it is simple to implement and applicable\nto both discrete and continuous models, though con-\ntrol variate methods are becoming increasingly popu-\nlar for variational inference problems (Ho\ufb00man et al.,\n2013; Blei et al., 2012; Ranganath et al., 2014; Sali-\nmans & Knowles, 2014). Unfortunately, such estima-\ntors have the undesirable property that their variance\nscales linearly with the number of independent random\nvariables in the target function, while the variance of\nGBP is bounded by a constant: for K-dimensional la-\ntent variables the variance of REINFORCE scales as\nO(K), whereas GBP scales as O(1) (see appendix D).\nAn important family of alternative estimators is based\non quadrature and series expansion methods (Honkela\n& Valpola, 2004; Lappalainen & Honkela, 2000).\nThese methods have low-variance at the price of in-\ntroducing biases in the estimation. More recently a\ncombination of the series expansion and control vari-\nate approaches has been proposed by Blei et al. (2012).\nA very general alternative is the wake-sleep algorithm\n(Dayan et al., 1995). The wake-sleep algorithm can\nperform well, but it fails to optimise a single consistent\nobjective function and there is thus no guarantee that\noptimising it leads to a decrease in the free energy (11).\nRelation to denoising auto-encoders.\nDenois-\ning auto-encoders (DAE) (Vincent et al., 2010) intro-\nduce a random corruption to the encoder network and\nattempt to minimize the expected reconstruction er-\nror under this corruption noise with additional reg-\nularisation terms.\nIn our variational approach, the\nrecognition distribution q(\u03be|v) can be interpreted as a\nstochastic encoder in the DAE setting. There is then a\ndirect correspondence between the expression for the\nfree energy (11) and the reconstruction error and reg-\nularization terms used in denoising auto-encoders (c.f.\nequation (4) of Bengio et al. (2013)). Thus, we can see\ndenoising auto-encoders as a realisation of variational\ninference in latent variable models.\nThe key di\ufb00erence is that the form of encoding \u2018corrup-\ntion\u2019 and regularisation terms used in our model have\nbeen derived directly using the variational principle\nto provide a strict bound on the marginal likelihood\nof a known directed graphical model that allows for\neasy generation of samples. DAEs can also be used as\ngenerative models by simulating from a Markov chain\n(Bengio et al., 2013; Bengio & Thibodeau-Laufer,\n2013). But the behaviour of these Markov chains will\nbe very problem speci\ufb01c, and we lack consistent tools\nto evaluate their convergence.\n7. Conclusion\nWe\nhave\nintroduced\na\ngeneral-purpose\ninference\nmethod for models with continuous latent variables.\nOur approach introduces a recognition model, which\ncan be seen as a stochastic encoding of the data, to al-\nlow for e\ufb03cient and tractable inference. We derived a\nlower bound on the marginal likelihood for the genera-\ntive model and speci\ufb01ed the structure and regularisa-\ntion of the recognition model by exploiting recent ad-\nvances in deep learning. By developing modi\ufb01ed rules\nfor backpropagation through stochastic layers, we de-\nrived an e\ufb03cient inference algorithm that allows for\njoint optimisation of all parameters. We show on sev-\neral real-world data sets that the model generates real-\nistic samples, provides accurate imputations of missing\ndata and can be a useful tool for high-dimensional data\nvisualisation.\nAppendices can be found with the online version of the pa-\nper. http://arxiv.org/abs/1401.4082\nAcknowledgements. We are grateful for feedback from\nthe reviewers as well as Peter Dayan, Antti Honkela, Neil\nLawrence and Yoshua Bengio.\nStochastic Backpropagation in DLGMs\nReferences\nBartholomew, D. J. and Knott, M. Latent variable mod-\nels and factor analysis, volume 7 of Kendall\u2019s library of\nstatistics. Arnold, 2nd edition, 1999.\nBeal,\nM. J.\nVariational Algorithms for approximate\nBayesian inference.\nPhD thesis, University of Cam-\nbridge, 2003.\nBengio, Y. and Thibodeau-Laufer, \u00b4E.\nDeep generative\nstochastic networks trainable by backprop.\nTechnical\nreport, University of Montreal, 2013.\nBengio, Y., Yao, L., Alain, G., and Vincent, P.\nGen-\neralized denoising auto-encoders as generative models.\nIn Advances in Neural Information Processing Systems\n(NIPS), pp. 1\u20139, 2013.\nBlei, D. M., Jordan, M. I., and Paisley, J. W. Variational\nBayesian inference with stochastic search. In Proceed-\nings of the 29th International Conference on Machine\nLearning (ICML), pp. 1367\u20131374, 2012.\nBonnet, G.\nTransformations des signaux al\u00b4eatoires a\ntravers les syst`emes non lin\u00b4eaires sans m\u00b4emoire. Annales\ndes T\u00b4el\u00b4ecommunications, 19(9-10):203\u2013220, 1964.\nDamianou, A. C. and Lawrence, N. D. Deep Gaussian pro-\ncesses. In Proceedings of the International Conference on\nArti\ufb01cial Intelligence and Statistics (AISTATS), 2013.\nDayan, P., Hinton, G. E., Neal, R. M., and Zemel, R. S.\nThe Helmholtz machine. Neural computation, 7(5):889\u2013\n904, September 1995.\nDayan, P. Helmholtz machines and wake-sleep learning.\nHandbook of Brain Theory and Neural Network. MIT\nPress, Cambridge, MA, 44(0), 2000.\nFrey, B. J. Variational inference for continuous sigmoidal\nBayesian networks. In Proceedings of the International\nConference on Arti\ufb01cial Intelligence and Statistics (AIS-\nTATS), 1996.\nFrey, B. J. and Hinton, G. E. Variational learning in non-\nlinear Gaussian belief networks. Neural Computation, 11\n(1):193\u2013213, January 1999.\nGershman, S. J. and Goodman, N. D. Amortized inference\nin probabilistic reasoning. In Proceedings of the 36th An-\nnual Conference of the Cognitive Science Society, 2014.\nGraves, A. Practical variational inference for neural net-\nworks.\nIn Advances in Neural Information Processing\nSystems 24 (NIPS), pp. 2348\u20132356, 2011.\nGregor, K., Mnih, A., and Wierstra, D. Deep autoregres-\nsive networks. In Proceedings of the International Con-\nference on Machine Learning (ICML), October 2014.\nHo\ufb00man, M., Blei, D. M., Wang, C., and Paisley, J.\nStochastic variational inference.\nJournal of Machine\nLearning Research, 14:1303\u20131347, May 2013.\nHonkela, A. and Valpola, H.\nUnsupervised variational\nBayesian learning of nonlinear models. In Advances in\nNeural Information Processing Systems (NIPS), 2004.\nKingma, D. P. and Welling, M. Auto-encoding variational\nBayes. Proceedings of the International Conference on\nLearning Representations (ICLR), 2014.\nLappalainen, H. and Honkela, A. Bayesian non-linear inde-\npendent component analysis by multi-layer perceptrons.\nIn Advances in independent component analysis (ICA),\npp. 93\u2013121. Springer, 2000.\nLarochelle, H. and Murray, I. The neural autoregressive\ndistribution estimator.\nIn Proceedings of the Interna-\ntional Conference on Arti\ufb01cial Intelligence and Statistics\n(AISTATS), 2011.\nLawrence, N.\nProbabilistic non-linear principal compo-\nnent analysis with Gaussian process latent variable mod-\nels. The Journal of Machine Learning Research, 6:1783\u2013\n1816, 2005.\nLittle, R. J. and Rubin, D. B.\nStatistical analysis with\nmissing data, volume 539. Wiley New York, 1987.\nMagdon-Ismail, M. and Purnell, J. T. Approximating the\ncovariance matrix of GMMs with low-rank perturba-\ntions. In Proceedings of the 11th international conference\non Intelligent data engineering and automated learning\n(IDEAL), pp. 300\u2013307, 2010.\nNetzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B.,\nand Ng, A. Y. Reading digits in natural images with\nunsupervised feature learning.\nIn NIPS Workshop on\nDeep Learning and Unsupervised Feature Learning, 2011.\nOpper, M. and Archambeau, C.\nThe variational Gaus-\nsian approximation revisited.\nNeural computation, 21\n(3):786\u201392, March 2009.\nPrice, R. A useful theorem for nonlinear devices having\nGaussian inputs.\nIEEE Transactions on Information\nTheory, 4(2):69\u201372, 1958.\nRanganath, R., Gerrish, S., and Blei, D. M.\nBlack box\nvariational inference. In Proceedings of the International\nConference on Arti\ufb01cial Intelligence and Statistics (AIS-\nTATS), October 2014.\nSalimans, T. and Knowles, D. A. On using control vari-\nates with stochastic approximation for variational bayes\nand its connection to stochastic linear regression. ArXiv\npreprint. arXiv:1401.1022, October 2014.\nSaul, L. K., Jaakkola, T., and Jordan, M. I. Mean \ufb01eld\ntheory for sigmoid belief networks. Journal of Arti\ufb01cial\nIntelligence Research (JAIR), 4:61\u201376, 1996.\nUria, B., Murray, I., and Larochelle, H.\nA deep and\ntractable density estimator.\nIn Proceedings of the In-\nternational Conference on Machine Learning (ICML),\n2014.\nVincent, P., Larochelle, H., Lajoie, I., Bengio, Y., and\nManzagol, P. Stacked denoising autoencoders: Learn-\ning useful representations in a deep network with a local\ndenoising criterion. The Journal of Machine Learning\nResearch, 11:3371\u20133408, 2010.\nWilliams, R. J.\nSimple statistical gradient-following al-\ngorithms for connectionist reinforcement learning. Ma-\nchine Learning, 8:229 \u2013 256, 1992.\nWilson, J. R. Variance reduction techniques for digital sim-\nulation. American Journal of Mathematical and Man-\nagement Sciences, 4(3):277\u2013312, 1984.\nAppendices:\nStochastic Backpropagation and Approximate Inference\nin Deep Generative Models\nDanilo J. Rezende, Shakir Mohamed, Daan Wierstra\n{danilor, shakir, daanw}@google.com\nGoogle DeepMind, London\nA. Additional Model Details\nIn equation (6) we showed an alternative form of the\njoint log likelihood that explicitly separates the deter-\nministic and stochastic parts of the generative model\nand corroborates the view that the generative model\nworks by applying a complex non-linear transforma-\ntion to a spherical Gaussian distribution N(\u03be|0, I)\nsuch that the transformed distribution best matches\nthe empirical distribution.\nWe provide more details\non this view here for clarity.\nFrom the model description in equations (3) and (4),\nwe can interpret the variables hl as deterministic func-\ntions of the noise variables \u03bel. This can be formally\nintroduced as a coordinate transformation of the prob-\nability density in equation (5): we perform a change of\ncoordinates hl \u2192\u03bel. The density of the transformed\nvariables \u03bel can be expressed in terms of the density (5)\ntimes the determinant of the Jacobian of the transfor-\nmation p(\u03bel) = p(hl(\u03bel))| \u2202hl\n\u2202\u03bel |. Since the co-ordinate\ntransformation is linear we have | \u2202hl\n\u2202\u03bel | = |Gl| and the\ndistribution of \u03bel is obtained as follows:\np(\u03bel)= p(hl(\u03bel))|\u2202hl\n\u2202\u03bel\n|\np(\u03bel)=p(hL)|GL|\nL\u22121\nY\nl=1\n|Gl|pl(hl|hl+1)=\nL\nY\nl=1\n|Gl||Sl|\u22121\n2 N(\u03bel)\n=\nL\nY\nl=1\n|Gl||GlGT\nl |\u22121\n2 N(\u03bel|0, I) =\nL\nY\nl=1\nN(\u03bel|0, I). (22)\nCombining this equation with the distribution of the\nvisible layer we obtain equation (6).\nA.1. Examples\nBelow we provide simple, explicit examples of genera-\ntive and recognition models.\nIn the case of a two-layer model the activation h1(\u03be1,2)\nin equation (6) can be explicitly written as\nh1(\u03be1,2)\n= W1f(G2\u03be2) + G1\u03be1 + b1.\n(23)\nSimilarly, a simple recognition model consists of a sin-\ngle deterministic layer and a stochastic Gaussian layer\nwith the rank-one covariance structure and is con-\nstructed as:\nq(\u03bel|v) = N\n\u0000\u03bel|\u00b5; (diag(d) + uu\u22a4)\u22121\u0001\n(24)\n\u00b5 = W\u00b5z + b\u00b5\n(25)\nlog d = Wdz + bd;\nu = Wuz + bu\n(26)\nz = f(Wvv + bv)\n(27)\nwhere the function f is a recti\ufb01ed linearity (but other\nnon-linearities such as tanh can be used).\nB. Proofs for the Gaussian Gradient\nIdentities\nHere we review the derivations of Bonnet\u2019s and Price\u2019s\ntheorems that were presented in section 3.\nTheorem B.1 (Bonnet\u2019s theorem). Let f(\u03be) : Rd 7\u2192\nR be a integrable and twice di\ufb00erentiable function. The\ngradient of the expectation of f(\u03be) under a Gaussian\ndistribution N(\u03be|\u00b5, C) with respect to the mean \u00b5 can\nbe expressed as the expectation of the gradient of f(\u03be).\n\u2207\u00b5iEN (\u00b5,C) [f(\u03be)] = EN (\u00b5,C) [\u2207\u03beif(\u03be)] ,\nProof.\n\u2207\u00b5iEN (\u00b5,C) [f(\u03be)] =\nZ\n\u2207\u00b5iN(\u03be|\u00b5, C)f(\u03be)d\u03be\n= \u2212\nZ\n\u2207\u03beiN(\u03be|\u00b5, C)f(\u03be)d\u03be\n=\n\u0014Z\nN(\u03be|\u00b5, C)f(\u03be)d\u03be\u00aci\n\u0015\u03bei=+\u221e\n\u03bei=\u2212\u221e\n+\nZ\nN(\u03be|\u00b5, C)\u2207\u03beif(\u03be)d\u03be\n= EN (\u00b5,C) [\u2207\u03beif(\u03be)] ,\n(28)\nProceedings of the 31 st International Conference on\nMachine Learning, Beijing, China, 2014. JMLR: W&CP\nvolume 32. Copyright 2014 by the author(s).\nStochastic Backpropagation in DLGMs\nwhere we have used the identity\n\u2207\u00b5iN(\u03be|\u00b5, C) = \u2212\u2207\u03beiN(\u03be|\u00b5, C)\nin moving from step 1 to 2. From step 2 to 3 we have\nused the product rule for integrals with the \ufb01rst term\nevaluating to zero.\nTheorem B.2 (Price\u2019s theorem). Under the same\nconditions as before. The gradient of the expectation\nof f(\u03be) under a Gaussian distribution N(\u03be|0, C) with\nrespect to the covariance C can be expressed in terms\nof the expectation of the Hessian of f(\u03be) as\n\u2207Ci,jEN (0,C) [f(\u03be)] = 1\n2EN (0,C)\n\u0002\n\u2207\u03bei,\u03bejf(\u03be)\n\u0003\nProof.\n\u2207Ci,jEN (0,C) [f(\u03be)] =\nZ\n\u2207Ci,jN(\u03be|0, C)f(\u03be)d\u03be\n= 1\n2\nZ\n\u2207\u03bei,\u03bejN(\u03be|0, C)f(\u03be)d\u03be\n= 1\n2\nZ\nN(\u03be|0, C)\u2207\u03bei,\u03bejf(\u03be)d\u03be\n= 1\n2EN (0,C)\n\u0002\n\u2207\u03bei,\u03bejf(\u03be)\n\u0003\n. (29)\nIn moving from steps 1 to 2, we have used the identity\n\u2207Ci,jN(\u03be|\u00b5, C) = 1\n2\u2207\u03bei,\u03bejN(\u03be|\u00b5, C),\nwhich can be veri\ufb01ed by taking the derivatives on both\nsides and comparing the resulting expressions. From\nstep 2 to 3 we have used the product rule for integrals\ntwice.\nC. Deriving Stochastic\nBack-propagation Rules\nIn section 3 we described two ways in which to derive\nstochastic back-propagation rules.\nWe show speci\ufb01c\nexamples and provide some more discussion in this sec-\ntion.\nC.1. Using the Product Rule for Integrals\nWe can derive rules for stochastic back-propagation for\nmany distributions by \ufb01nding a appropriate non-linear\nfunction B(x; \u03b8) that allows us to express the gradient\nwith respect to the parameters of the distribution as a\ngradient with respect to the random variable directly.\nThe approach we described in the main text was:\n\u2207\u03b8Ep[f(x)]=\nZ\n\u2207\u03b8p(x|\u03b8)f(x)dx=\nZ\n\u2207xp(x|\u03b8)B(x)f(x)dx\n= [B(x)f(x)p(x|\u03b8)]supp(x) \u2212\nZ\np(x|\u03b8)\u2207x[B(x)f(x)]\n= \u2212Ep(x|\u03b8)[\u2207x[B(x)f(x)]]\n(30)\nwhere we have introduced the non-linear function\nB(x; \u03b8) to allow for the transformation of the gradi-\nents and have applied the product rule for integrals\n(rule for integration by parts) to rewrite the integral\nin two parts in the second line, and the supp(x) indi-\ncates that the term is evaluated at the boundaries of\nthe support. To use this approach, we require that the\ndensity we are analysing be zero at the boundaries of\nthe support to ensure that the \ufb01rst term in the second\nline is zero.\nAs an alternative, we can also write this di\ufb00erently\nand \ufb01nd an non-linear function of the form:\n\u2207\u03b8Ep[f(x)]== \u2212Ep(x|\u03b8)[B(x)\u2207xf(x)].\n(31)\nConsider general exponential family distributions of\nthe form:\np(x|\u03b8) = h(x) exp(\u03b7(\u03b8)\u22a4\u03c6(x) \u2212A(\u03b8))\n(32)\nwhere h(x) is the base measure, \u03b8 is the set of mean\nparameters of the distribution, \u03b7 is the set of natural\nparameters, and A(\u03b8) is the log-partition function. We\ncan express the non-linear function in (30) using these\nquantities as:\nB(x) =\n[\u2207\u03b8\u03b7(\u03b8)\u03c6(x) \u2212\u2207\u03b8A(\u03b8)]\n[\u2207x log[h(x)] + \u03b7(\u03b8)T \u2207x\u03c6(x)].\n(33)\nThis can be derived for a number of distributions such\nas the Gaussian, inverse Gamma, Log-Normal, Wald\n(inverse Gaussian) and other distributions. We show\nsome of these below:\nFamily\n\u03b8\nB(x)\nGaussian\n\u0012 \u00b5\n\u03c32\n\u0013\n\u0012\n\u22121\n(x\u2212\u00b5\u2212\u03c3)(x\u2212\u00b5+\u03c3)\n2\u03c32(x\u2212\u00b5)\n\u0013\nInv. Gamma\n\u0012\n\u03b1\n\u03b2\n\u0013\n \nx2(\u2212ln x\u2212\u03a8(\u03b1)+ln \u03b2)\n\u2212x(\u03b1+1)+\u03b2\n(\nx2\n\u2212x(\u03b1+1)+\u03b2 )(\u22121\nx + \u03b1\n\u03b2 )\n!\nLog-Normal\n\u0012 \u00b5\n\u03c32\n\u0013\n\u0012\n\u22121\n(ln x\u2212\u00b5\u2212\u03c3)(ln x\u2212\u00b5+\u03c3)\n2\u03c32(ln x\u2212\u00b5)\n\u0013\nThe B(x; \u03b8) corresponding to the second formulation\ncan also be derived and may be useful in certain situa-\ntions, requiring the solution of a \ufb01rst order di\ufb00erential\nequation.\nThis approach of searching for non-linear\ntransformations leads us to the second approach for\nderiving stochastic back-propagation rules.\nStochastic Backpropagation in DLGMs\nC.2. Using Alternative Coordinate\nTransformations\nThere are many distributions outside the exponential\nfamily that we would like to consider using. A sim-\npler approach is to search for a co-ordinate transfor-\nmation that allows us to separate the deterministic\nand stochastic parts of the distribution. We described\nthe case of the Gaussian in section 3. Other distri-\nbutions also have this property. As an example, con-\nsider the Levy distribution (which is a special case of\nthe inverse Gamma considered above).\nDue to the\nself-similarity property of this distribution, if we draw\nX from a Levy distribution with known parameters\nX \u223cLevy(\u00b5, \u03bb), we can obtain any other Levy distri-\nbution by rescaling and shifting this base distribution:\nkX + b \u223cLevy(k\u00b5 + b, kc).\nMany other distributions hold this property, allow-\ning stochastic back-propagation rules to be determined\nfor distributions such as the Student\u2019s t-distribution,\nLogistic distribution, the class of stable distributions\nand the class of generalised extreme value distributions\n(GEV). Examples of co-ordinate transformations T(\u00b7)\nand the resulsting distributions are shown below for\nvariates X drawn from the standard distribution listed\nin the \ufb01rst column.\nStd Distr.\nT(\u00b7)\nGen. Distr.\nGEV(\u00b5, \u03c3, 0) mX+b\nGEV(m\u00b5+b, m\u03c3, 0)\nExp(1)\n\u00b5+\u03b2ln(1+exp(\u2212X))\nLogistic(\u00b5, \u03b2)\nExp(1)\n\u03bbX\n1\nk\nWeibull(\u03bb, k)\nD. Variance Reduction using Control\nVariates\nAn alternative approach for stochastic gradient com-\nputation is commonly based on the method of control\nvariates. We analyse the variance properties of var-\nious estimators in a simple example using univariate\nfunction.\nWe then show the correspondence of the\nwidely-known REINFORCE algorithm to the general\ncontrol variate framework.\nD.1. Variance discussion for REINFORCE\nThe REINFORCE estimator is based on\n\u2207\u03b8Ep[f(\u03be)] = Ep[(f(\u03be) \u2212b)\u2207\u03b8 log p(\u03be|\u03b8)],\n(34)\nwhere b is a baseline typically chosen to reduce the\nvariance of the estimator.\nThe variance of (34) scales poorly with the num-\nber of random variables (Dayan et al., 1995).\nTo\nsee this limitation, consider functions of the form\nf(\u03be)\n=\nPK\ni=1 f(\u03bei),\nwhere each individual term\nand\nits\ngradient\nhas\na\nbounded\nvariance,\ni.e.,\n\u03bal \u2264Var[f(\u03bei)] \u2264\u03bau and \u03bal \u2264Var[\u2207\u03beif(\u03bei)] \u2264\u03bau for\nsome 0 \u2264\u03bal \u2264\u03bau and assume independent or weakly\ncorrelated random variables. Given these assumptions\nthe variance of GBP (7) scales as Var[\u2207\u03beif(\u03be)] \u223cO(1),\nwhile the variance for REINFORCE (34) scales as\nVar\nh\n(\u03bei\u2212\u00b5i)\n\u03c32\ni\n(f(\u03be) \u2212E[f(\u03be)])\ni\n\u223cO(K).\nFor the variance of GBP above, all terms in f(\u03be) that\ndo not depend on \u03bei have zero gradient, whereas for\nREINFORCE the variance involves a summation over\nall K terms. Even if most of these terms have zero\nexpectation, they still contribute to the variance of\nthe estimator.\nThus, the REINFORCE estimator\nhas the undesirable property that its variance scales\nlinearly with the number of independent random\nvariables in the target function, while the variance of\nGBP is bounded by a constant.\nThe assumption of weakly correlated terms is rele-\nvant for variational learning in larger generative mod-\nels where independence assumptions and structure in\nthe variational distribution result in free energies that\nare summations over weakly correlated or independent\nterms.\nD.2. Univariate variance analysis\nIn analysing the variance properties of many estima-\ntors, we discuss the general scaling of likelihood ratio\napproaches in appendix D. As an example to further\nemphasise the high-variance nature of these alternative\napproaches, we present a short analysis in the univari-\nate case.\nConsider a random variable p(\u03be) = N(\u03be|\u00b5, \u03c32) and a\nsimple quadratic function of the form\nf(\u03be) = c\u03be2\n2 .\n(35)\nFor this function we immediately obtain the following\nvariances\nV ar[\u2207\u03bef(\u03be)] = c2\u03c32\n(36)\nV ar[\u2207\u03be2f(\u03be)] = 0\n(37)\nV ar[(\u03be \u2212\u00b5)\n\u03c3\n\u2207\u03bef(\u03be)] = 2c2\u03c32 + \u00b52c2\n(38)\nV ar[(\u03be \u2212\u00b5)\n\u03c32\n(f(\u03be) \u2212E[f(\u03be)])] = 2c2\u00b52 + 5\n2c2\u03c32 (39)\nEquations (36), (37) and (38) correspond to the vari-\nance of the estimators based on (7), (8), (10) respec-\ntively whereas equation (39) corresponds to the vari-\nance of the REINFORCE algorithm for the gradient\nwith respect to \u00b5.\nStochastic Backpropagation in DLGMs\nFrom these relations we see that, for any parameter\ncon\ufb01guration, the variance of the REINFORCE esti-\nmator is strictly larger than the variance of the estima-\ntor based on (7). Additionally, the ratio between the\nvariances of the former and later estimators is lower-\nbounded by 5/2. We can also see that the variance\nof the estimator based on equation (8) is zero for this\nspeci\ufb01c function whereas the variance of the estimator\nbased on equation (10) is not.\nE. Estimating the Marginal Likelihood\nWe compute the marginal likelihood by importance\nsampling by generating S samples from the recognition\nmodel and using the following estimator:\np(v) \u22481\nS\nS\nX\ns=1\np(v|h(\u03be(s)))p(\u03be(s))\nq(\u03bes|v)\n;\n\u03be(s) \u223cq(\u03be|v)\n(40)\nF. Missing Data Imputation\nImage completion can be approximatively achieved by\na simple iterative procedure which consists of (i) ini-\ntializing the non-observed pixels with random values;\n(ii) sampling from the recognition distribution given\nthe resulting image; (iii) reconstruct the image given\nthe sample from the recognition model; (iv) iterate the\nprocedure.\nWe denote the observed and missing entries in an ob-\nservation as vo, vm, respectively. The observed vo is\n\ufb01xed throughout, therefore all the computations in this\nsection will be conditioned on vo. The imputation pro-\ncedure can be written formally as a Markov chain on\nthe space of missing entries vm with transition kernel\nT q(v\u2032\nm|vm, vo) given by\nT q(v\u2032\nm|vm, vo) =\nZZ\np(v\u2032\nm, v\u2032\no|\u03be)q(\u03be|v)dv\u2032\nod\u03be,\n(41)\nwhere v = (vm, vo).\nProvided that the recognition model q(\u03be|v) constitutes\na good approximation of the true posterior p(\u03be|v), (41)\ncan be seen as an approximation of the kernel\nT(v\u2032\nm|vm, vo) =\nZZ\np(v\u2032\nm, v\u2032\no|\u03be)p(\u03be|v)dv\u2032\nod\u03be.\n(42)\nThe kernel (42) has two important properties: (i) it\nhas as its eigen-distribution the marginal p(vm|vo); (ii)\nT(v\u2032\nm|vm, vo) > 0 \u2200vo, vm, v\u2032\nm. The property (i) can\nbe derived by applying the kernel (42) to the marginal\np(vm|vo) and noting that it is a \ufb01xed point. Property\n(ii) is an immediate consequence of the smoothness of\nthe model.\nWe apply the fundamental theorem for Markov chains\n(Neal, 1993, pp. 38) and conclude that given the above\nproperties, a Markov chain generated by (42) is guar-\nanteed to generate samples from the correct marginal\np(vm|vo).\nIn practice, the stationary distribution of the com-\npleted\npixels\nwill\nnot\nbe\nexactly\nthe\nmarginal\np(vm|vo), since we use the approximated kernel (41).\nEven in this setting we can provide a bound on the L1\nnorm of the di\ufb00erence between the resulting stationary\nmarginal and the target marginal p(vm|vo)\nProposition F.1 (L1 bound on marginal error ). If\nthe recognition model q(\u03be|v) is such that for all \u03be\n\u2203\u03b5 > 0 s.t.\nZ \f\f\f\f\nq(\u03be|v)p(v)\np(\u03be)\n\u2212p(v|\u03be)\n\f\f\f\f dv \u2264\u03b5\n(43)\nthen the marginal p(vm|vo) is a weak \ufb01xed point of the\nkernel (41) in the following sense:\nZ \f\f\f\f\f\nZ \u0000T q(v\u2032\nm|vm, vo)\u2212\nT(v\u2032\nm|vm, vo)\n\u0001\np(vm|vo)dvm\n\f\f\f\f\fdv\u2032\nm < \u03b5.\n(44)\nProof.\nZ \f\f\f\f\nZ\n[T q(v\u2032\nm|vm, vo)\u2212T(v\u2032\nm|vm, vo)] p(vm|vo)dvm\n\f\f\f\f dv\u2032\nm\n=\nZ\n|\nZZ\np(v\u2032\nm, v\u2032\no|\u03be)p(vm, vo)[q(\u03be|vm, vo)\n\u2212p(\u03be|vm, vo)]dvmd\u03be|dv\u2032\nm\n=\nZ \f\f\f\f\nZ\np(v\u2032|\u03be)p(v)[q(\u03be|v) \u2212p(\u03be|v)]p(v)\np(\u03be)\np(\u03be)\np(v)dvd\u03be\n\f\f\f\f dv\u2032\n=\nZ \f\f\f\f\nZ\np(v\u2032|\u03be)p(\u03be)[q(\u03be|v)p(v)\np(\u03be) \u2212p(v|\u03be)]dvd\u03be\n\f\f\f\f dv\u2032\n\u2264\nZ Z\np(v\u2032|\u03be)p(\u03be)\nZ \f\f\f\fq(\u03be|v)p(v)\np(\u03be) \u2212p(v|\u03be)\n\f\f\f\f dvd\u03bedv\u2032\n\u2264\u03b5,\nwhere we apply the condition (43) to obtain the last\nstatement.\nThat is, if the recognition model is su\ufb03ciently close\nto the true posterior to guarantee that (43) holds for\nsome acceptable error \u03b5 than (44) guarantees that the\n\ufb01xed-point of the Markov chain induced by the kernel\n(41) is no further than \u03b5 from the true marginal with\nrespect to the L1 norm.\nStochastic Backpropagation in DLGMs\nG. Variational Bayes for Deep Directed\nModels\nIn the main test we focussed on the variational prob-\nlem of specifying an posterior on the latent variables\nonly. It is natural to consider the variational Bayes\nproblem in which we specify an approximate posterior\nfor both the latent variables and model parameters.\nFollowing the same construction and considering an\nGaussian approximate distribution on the model pa-\nrameters \u03b8g, the free energy becomes:\nF(V) = \u2212\nX\nn\nreconstruction error\nz\n}|\n{\nEq[log p(vn|h(\u03ben))]\n+ 1\n2\nX\nn,l\n\u0002\n\u2225\u00b5n,l\u22252 + Tr Cn,l \u2212log |Cn,l| \u22121\n\u0003\n|\n{z\n}\nlatent regularization term\n+ 1\n2\nX\nj\n\"\nm2\nj\n\u03ba + \u03c4j\n\u03ba + log \u03ba \u2212log \u03c4j \u22121\n#\n|\n{z\n}\nparameter regularization term\n,\n(45)\nwhich now includes an additional term for the cost of\nusing parameters and their regularisation. We must\nnow compute the additional set of gradients with re-\nspect to the parameter\u2019s mean mj and variance \u03c4j are:\n\u2207mjF(v) = \u2212Eq\nh\n\u2207\u03b8g\nj log p(v|h(\u03be))\ni\n+ mj\n(46)\n\u2207\u03c4jF(v) = \u22121\n2Eq\n\u0014\u03b8j \u2212mj\n\u03c4j\n\u2207\u03b8g\nj log p(v|h(\u03be))\n\u0015\n+ 1\n2\u03ba \u22121\n2\u03c4j\n(47)\nH. Additional Simulation Details\nWe use training data of various types including binary\nand real-valued data sets. In all cases, we train using\nmini-batches, which requires the introduction of scal-\ning terms in the free energy objective function (13) in\norder to maintain the correct scale between the prior\nover the parameters and the remaining terms (Ahn\net al., 2012; Welling & Teh, 2011). We make use of\nthe objective:\nF(V) = \u2212\u03bb\nX\nn\nEq [log p(vn|h(\u03ben))] +\n1\n2\u03ba\u2225\u03b8g\u22252\n+ \u03bb\n2\nX\nn,l\n\u0002\n\u2225\u00b5n,l\u22252 + Tr(Cn,l) \u2212log |Cn,l| \u22121\n\u0003\n,\n(48)\nwhere n is an index over observations in the mini-batch\nand \u03bb is equal to the ratio of the data-set and the mini-\nbatch size. At each iteration, a random mini-batch of\nsize 200 observations is chosen.\nAll parameters of the model were initialized using sam-\nples from a Gaussian distribution with mean zero and\nvariance 1 \u00d7 106; the prior variance of the parameters\nwas \u03ba = 1 \u00d7 106. We compute the marginal likelihood\non the test data by importance sampling using samples\nfrom the recognition model; we describe our estimator\nin appendix E.\nReferences\nAhn, S., Balan, A. K., and Welling, M. Bayesian poste-\nrior sampling via stochastic gradient Fisher scoring.\nIn ICML, 2012.\nDayan, P., Hinton, G. E., Neal, R. M., and Zemel,\nR. S. The Helmholtz machine. Neural computation,\n7(5):889\u2013904, September 1995.\nNeal, R. M. Probabilistic inference using Markov chain\nMonte Carlo methods. Technical Report CRG-TR-\n93-1, University of Toronto, 1993.\nWelling, M. and Teh, Y. W.\nBayesian learning via\nstochastic gradient Langevin dynamics. In Proceed-\nings of the 28th International Conference on Ma-\nchine Learning (ICML-11), pp. 681\u2013688, 2011.\n",
        "sentence": " This basic idea has been applied and enhanced many times; initially with the wake-sleep algorithm (WS, [3, 5]) and more recently with the variational autoencoder (VAE, [6]), stochastic backpropagation and approximate inference in deep generative models [7], neural variational inference and learning (NVIL, [8]) and reweighted wake-sleep (RWS, [9]).",
        "context": "mean-\ufb01eld variational EM (Beal, 2003); the wake-sleep\nalgorithm (Dayan, 2000); and stochastic variational\nmethods and related control-variate estimators (Wil-\nson, 1984; Williams, 1992; Ho\ufb00man et al., 2013). We\nA very general alternative is the wake-sleep algorithm\n(Dayan et al., 1995). The wake-sleep algorithm can\nperform well, but it fails to optimise a single consistent\nobjective function and there is thus no guarantee that\n904, September 1995.\nDayan, P. Helmholtz machines and wake-sleep learning.\nHandbook of Brain Theory and Neural Network. MIT\nPress, Cambridge, MA, 44(0), 2000.\nFrey, B. J. Variational inference for continuous sigmoidal"
    },
    {
        "title": "Neural variational inference and learning in belief networks",
        "author": [
            "Andriy Mnih",
            "Karol Gregor"
        ],
        "venue": "In Proceedings of the 31st International Conference on Machine Learning (ICML",
        "citeRegEx": "8",
        "shortCiteRegEx": "8",
        "year": 2014,
        "abstract": "Highly expressive directed latent variable models, such as sigmoid belief\nnetworks, are difficult to train on large datasets because exact inference in\nthem is intractable and none of the approximate inference methods that have\nbeen applied to them scale well. We propose a fast non-iterative approximate\ninference method that uses a feedforward network to implement efficient exact\nsampling from the variational posterior. The model and this inference network\nare trained jointly by maximizing a variational lower bound on the\nlog-likelihood. Although the naive estimator of the inference model gradient is\ntoo high-variance to be useful, we make it practical by applying several\nstraightforward model-independent variance reduction techniques. Applying our\napproach to training sigmoid belief networks and deep autoregressive networks,\nwe show that it outperforms the wake-sleep algorithm on MNIST and achieves\nstate-of-the-art results on the Reuters RCV1 document dataset.",
        "full_text": "arXiv:1402.0030v2  [cs.LG]  4 Jun 2014\nNeural Variational Inference and Learning in Belief Networks\nAndriy Mnih\nAMNIH@GOOGLE.COM\nKarol Gregor\nKAROLG@GOOGLE.COM\nGoogle DeepMind\nAbstract\nHighly expressive directed latent variable mod-\nels, such as sigmoid belief networks, are dif\ufb01-\ncult to train on large datasets because exact in-\nference in them is intractable and none of the\napproximate inference methods that have been\napplied to them scale well. We propose a fast\nnon-iterative approximate inference method that\nuses a feedforward network to implement ef\ufb01-\ncient exact sampling from the variational poste-\nrior. The model and this inference network are\ntrained jointly by maximizing a variational lower\nbound on the log-likelihood. Although the naive\nestimator of the inference network gradient is too\nhigh-variance to be useful, we make it practi-\ncal by applying several straightforward model-\nindependent variance reduction techniques. Ap-\nplying our approach to training sigmoid belief\nnetworks and deep autoregressive networks, we\nshow that it outperforms the wake-sleep algo-\nrithm on MNIST and achieves state-of-the-art re-\nsults on the Reuters RCV1 document dataset.\n1. Introduction\nCompared to powerful globally-normalized latent variable\nmodels, such as deep belief networks (Hinton et al., 2006)\nand deep Boltzmann machines (Salakhutdinov & Hinton,\n2009a), which can now be trained on fairly large datasets,\ntheir purely directed counterparts have been left behind due\nto the lack of ef\ufb01cient learning algorithms. This is unfor-\ntunate, because their modularity and ability to generate ob-\nservations ef\ufb01ciently make them better suited for integra-\ntion into larger systems.\nTraining highly expressive directed latent variable mod-\nels on large datasets is a challenging problem due to the\ndif\ufb01culties posed by inference. Although the generality\nProceedings of the 31 st International Conference on Machine\nLearning, Beijing, China, 2014. JMLR: W&CP volume 32. Copy-\nright 2014 by the author(s).\nof Markov Chain Monte Carlo (MCMC) methods makes\nthem straightforward to apply to models of this type (Neal,\n1992), they tend to suffer from slow mixing and are usually\ntoo computationally expensive to be practical in all but the\nsimplest models. Such methods are also dif\ufb01cult to scale to\nlarge datasets because they need to store the current state of\nthe latent variables for all the training observations between\nparameter updates.\nVariational methods (Jordan et al., 1999) provide an\noptimization-based\nalternative\nto\nthe\nsampling-based\nMonte Carlo methods, and tend to be more ef\ufb01cient. They\ninvolve approximating the exact posterior using a distribu-\ntion from a more tractable family, often a fully factored\none, by maximizing a variational lower bound on the log-\nlikelihood w.r.t. the parameters of the distribution. For a\nsmall class of models, using such variational posteriors al-\nlows the expectations that specify the parameter updates to\nbe computed analytically. However, for highly expressive\nmodels such as the ones we are interested in, these expecta-\ntions are intractable even with the simplest variational pos-\nteriors. This dif\ufb01culty is usually dealt with by lower bound-\ning the intractable expectations with tractable one by intro-\nducing more variational parameters, as was done for sig-\nmoid belief nets by Saul et al. (1996). However, this tech-\nnique increases the gap between the bound being optimized\nand the log-likelihood, potentially resulting in a poorer \ufb01t\nto the data. In general, variational methods tend to be more\nmodel-dependent than sampling-based methods, often re-\nquiring non-trivial model-speci\ufb01c derivations.\nWe propose a new approach to training directed graphi-\ncal models that combines the advantages of the sampling-\nbased and variational methods. Its central idea is using a\nfeedforward network to implement ef\ufb01cient exact sampling\nfrom the variational posterior for the given observation. We\ntrain this inference network jointly with the model by max-\nimizing the variational lower bound on the log-likelihood,\nestimating all the required gradients using samples from\nthe inference network. Although naive estimate of the gra-\ndient for the inference network parameters is unusable due\nto its high variance, we make the approach practical by ap-\nplying several straightforward and general variance reduc-\nNeural Variational Inference and Learning in Belief Networks\ntion techniques. The resulting training procedure for the\ninference network can be seen as an instance of the RE-\nINFORCE algorithm (Williams, 1992).\nDue to our use\nof stochastic feedforward networks for performing infer-\nence we call our approach Neural Variational Inference and\nLearning (NVIL).\nCompared to MCMC methods, where many iterations over\nthe latent variables are required to generate a sample from\nthe exact posterior and successive samples tend to be highly\ncorrelated, NVIL does not suffer from mixing issues as\neach forward pass through the inference network generates\nan independent exact sample from the variational posterior.\nIn addition to being much faster than MCMC, our approach\nhas the additional advantage of not needing to store the\nlatent variables for each observation and thus is not only\nmore memory ef\ufb01cient but also applicable to the pure on-\nline learning setting, where each training case is seen once\nbefore being discarded.\nIn contrast to other work on scaling up variational infer-\nence, NVIL can handle both discrete and continuous latent\nvariables (unlike Kingma & Welling (2013); Rezende et al.\n(2014)) as well variational posteriors with complex depen-\ndency structures (unlike Ranganath et al. (2013)). More-\nover, the variance reduction methods we employ are sim-\nple and model-independent, unlike the more sophisticated\nmodel-speci\ufb01c control variates of Paisley et al. (2012).\nThough the idea of training an inference model by\nfollowing the gradient of the variational bound has\nbeen considered before, it was dismissed as infeasible\n(Dayan & Hinton, 1996). Our primary contribution is to\nshow how to reduce the variance of the naive gradient es-\ntimator to make it practical without narrowing its range of\napplicability. We also show that the resulting method trains\nsigmoid belief networks better than the wake-sleep algo-\nrithm (Hinton et al., 1995), which is the only algorithm we\nare aware of that is capable of training the same range of\nmodels ef\ufb01ciently. Finally, we demonstrate the effective-\nness and scalability of NVIL by using it to achieve state-\nof-the-art results on the Reuters RCV1 document dataset.\n2. Neural variational inference and learning\n2.1. Variational objective\nSuppose we are interested in training a latent variable\nmodel P\u03b8(x, h) with parameters \u03b8. We assume that ex-\nact inference in the model is intractable and thus maximum\nlikelihood learning is not an option. For simplicity, we will\nalso assume that all the latent variables in the model are dis-\ncrete, though essentially the same approach applies if some\nor all of the variables are continuous.\nWe will train the model by maximizing a variational\nlower bound on the marginal log-likelihood.\nFollowing\nthe standard variational inference approach (Jordan et al.,\n1999), given an observation x, we introduce a distribution\nQ\u03c6(h|x) with parameters \u03c6, which will serve as an approx-\nimation to its exact posterior P\u03b8(h|x). The variational pos-\nterior Q will have a simpler form than the exact posterior\nand thus will be easier to work with.\nThe contribution of x to the log-likelihood can then be\nlower-bounded as follows (Jordan et al., 1999):\nlog P\u03b8(x) = log\nX\nh\nP\u03b8(x, h)\n\u2265\nX\nh\nQ\u03c6(h|x) log P\u03b8(x, h)\nQ\u03c6(h|x)\n= EQ[log P\u03b8(x, h) \u2212log Q\u03c6(h|x)]\n(1)\n= L(x, \u03b8, \u03c6).\nBy rewriting the bound as\nL(x, \u03b8, \u03c6) = log P\u03b8(x) \u2212KL(Q\u03c6(h|x), P\u03b8(h|x)),\n(2)\nwe see that its tightness is determined by the Kullback-\nLeibler (KL) divergence between the variational distribu-\ntion and the exact posterior. Maximizing the bound with\nrespect to the parameters \u03c6 of the variational distribution\nmakes the distribution a better approximation to the poste-\nrior (w.r.t. the KL-divergence) and tightens the bound.\nIn contrast to most applications of variational inference\nwhere the variational posterior for each observation is de-\n\ufb01ned using its own set of variational parameters, our ap-\nproach does not use any local variational parameters. In-\nstead, we use a \ufb02exible feedforward model to compute the\nvariational distribution from the observation. We call the\nmodel mapping x to Q\u03c6(h|x) the inference network. The\narchitecture of the inference network is constrained only by\nthe requirement that Q\u03c6(h|x) it de\ufb01nes has to be ef\ufb01cient\nto evaluate and sample from. Using samples from the infer-\nence network we will be able to compute gradient estimates\nfor the model and inference network parameters for a large\nclass of highly expressive architectures, without having to\ndeal with architecture-speci\ufb01c approximations.\nGiven a training set D,\nconsisting of observations\nx1, ..., xD, we train the model by (locally) maximizing\nL(D, \u03b8, \u03c6) = P\ni L(xi, \u03b8, \u03c6) using gradient ascent w.r.t. to\nthe model and inference network parameters. To ensure\nscalability to large datasets, we will perform stochastic op-\ntimization by estimating gradients on small minibatches of\nrandomly sampled training cases.\n2.2. Parameter gradients\nThe gradient of the variational bound for a single observa-\ntion x w.r.t. to the model parameters is straightforward to\nNeural Variational Inference and Learning in Belief Networks\nderive and has the form\n\u2207\u03b8L(x) = EQ [\u2207\u03b8 log P\u03b8(x, h)] ,\n(3)\nwhere we left \u03b8 and \u03c6 off the list of the arguments of L to\nsimplify the notation. The corresponding gradient w.r.t. to\nthe inference network parameters is somewhat more in-\nvolved:\n\u2207\u03c6L(x) = EQ[(log P\u03b8(x, h) \u2212log Q\u03c6(h|x))\n\u00d7 \u2207\u03c6 log Q\u03c6(h|x)],\n(4)\nWe give its derivation in the supplementary material.\nAs both gradients involve expectations which are in-\ntractable in all but a handful of special cases, we will es-\ntimate them with Monte Carlo integration, using samples\nfrom the inference network. Having generated n samples\nh(1), ..., h(n) from Q\u03c6(h|x), we compute\n\u2207\u03b8L(x) \u22481\nn\nn\nX\ni=1\n\u2207\u03b8 log P\u03b8(x, h(i))\n(5)\nand\n\u2207\u03c6L(x) \u22481\nn\nn\nX\ni=1\n(log P\u03b8(x, h(i)) \u2212log Q\u03c6(h(i)|x))\n\u00d7 \u2207\u03c6 log Q\u03c6(h(i)|x).\n(6)\nThe above gradient estimators are unbiased and thus can be\nused to perform stochastic maximization of the variational\nobjective using a suitable learning rate annealing schedule.\nThe speed of convergence of this procedure, however, de-\npends heavily on the variance of the estimators used, as we\nwill see in Section 4.2.\nThe model gradient estimator (5) is well-behaved and does\nnot pose a problem. The variance of the inference network\ngradient estimator (6), however, can be very high due to\nthe scaling of the gradient inside the expectation by a po-\ntentially large term. As a result, learning variational param-\neters with updates based on this estimator can be unaccept-\nably slow. In fact, it is widely believed that learning vari-\national parameters using gradient estimators of the form\n(6) is infeasible (Hinton & Zemel, 1994; Dayan & Hinton,\n1996; Kingma & Welling, 2013). In the next section we\nwill show how to make this approach practical by applying\nvariance reduction techniques.\n2.3. Variance reduction techniques\nThough gradient estimates computed using Eq. 6 are usu-\nally too noisy to be useful in practice, it is easy to reduce\ntheir variance to a manageable level with the following\nmodel-independent techniques.\n2.3.1. CENTERING THE LEARNING SIGNAL\nInspecting Eq. 4, we see that we are using\nl\u03c6(x, h) = log P\u03b8(x, h) \u2212log Q\u03c6(h|x)\n(7)\nas the learning signal for the inference network parameters,\nand thus are effectively \ufb01tting log Q\u03c6(h|x) to log P\u03b8(x, h).\nThis might seem surprising, given that we want the in-\nference network Q\u03c6(h|x) to approximate the posterior\ndistribution P\u03b8(x|h), as opposed to the joint distribution\nP\u03b8(x, h). It turns out however that using the joint instead of\nthe posterior distribution in Eq. 4 does not affect the value\nof the expectation. To see that we start by noting that\nEQ[\u2207\u03c6 log Q\u03c6(h|x)] = EQ\n\u0014\u2207\u03c6Q\u03c6(h|x)\nQ\u03c6(h|x)\n\u0015\n= \u2207\u03c6EQ[1] = 0.\n(8)\nTherefore we can subtract any c that does not depend on h\nfrom the learning signal in Eq. 4 without affecting the value\nof the expectation:\nEQ[(l\u03c6(x, h) \u2212c)\u2207\u03c6 log Q\u03c6(h|x)]\n= EQ[l\u03c6(x, h)\u2207\u03c6 log Q\u03c6(h|x)] \u2212cEQ[\u2207\u03c6 log Q\u03c6(h|x)]\n= EQ[l\u03c6(x, h)\u2207\u03c6 log Q\u03c6(h|x)].\n(9)\nAnd as log P\u03b8(x, h)\n=\nlog P\u03b8(h|x) + log P\u03b8(x) and\nlog P\u03b8(x) does not depend on h, using P\u03b8(h|x) in Eq. 4\nin place of P\u03b8(x, h) does not affect the value of the expec-\ntation.\nThis equivalence allows us to compute the learning sig-\nnal ef\ufb01ciently, without having to evaluate the intractable\nP\u03b8(h|x) term.\nThe price we pay for this tractability is\nthe much higher variance of the estimates computed us-\ning Eq. 6. Fortunately, Eq. 9 suggests that we can reduce\nthe variance by subtracting a carefully chosen c from the\nlearning signal. The simplest option is to make c a pa-\nrameter and adapt it as learning progresses. However, c\nwill not be able capture the systematic differences in the\nlearning signal for different observations x, which arise in\npart due to the presence of the log P\u03b8(x) term. Thus we\ncan reduce the gradient variance further by subtracting an\nobservation-dependent term C\u03c8(x) to minimize those dif-\nferences. Doing this does not affect the expected value of\nthe gradient estimator because C\u03c8(x) does not depend on\nthe latent variables. Borrowing a name from the reinforce-\nment learning literature we will refer to c and C\u03c8(x) as\nbaselines. We will elaborate on this connection in Sec-\ntion 3.4.\nWe implement the input-dependent baseline C\u03c8(x) using a\nneural network and train it to minimize the expected square\nof the centered learning signal EQ[(l\u03c6(x, h)\u2212C\u03c8(x)\u2212c)2].\nThough this approach to \ufb01tting the baseline does not re-\nsult in the maximal variance reduction, it is simpler and in\nNeural Variational Inference and Learning in Belief Networks\nour experience works as well as the optimal approach of\nWeaver & Tao (2001) which requires taking into account\nthe magnitude of the gradient of the inference network pa-\nrameters. We also experimented with per-parameter base-\nlines but found that they did not improve on the global ones.\nFinally, we note that incorporating baselines into the learn-\ning signal can be seen as using simple control variates.\nIn contrast to the more elaborate control variates (e.g. of\nPaisley et al. (2012)), baselines do not depend on the form\nof the model or of the variational distribution and thus are\neasier to use.\n2.3.2. VARIANCE NORMALIZATION\nEven after centering, using l\u03c6(x, h) as the learning signal\nis non-trivial as its average magnitude can change dramat-\nically, and not necessarily monotonically, as training pro-\ngresses. This variability makes training an inference net-\nwork using a \ufb01xed learning rate dif\ufb01cult. We address this\nissue by dividing the centered learning signal by a running\nestimate of its standard deviation. This normalization en-\nsures that the signal is approximately unit variance, and can\nbe seen as a simple and ef\ufb01cient way of adapting the learn-\ning rate. To ensure that we stop learning when the magni-\ntude of the signal approaches zero, we apply variance nor-\nmalization only when the estimate of the standard devia-\ntion is greater than 1. The algorithm for computing NVIL\nparameter updates using the variance reduction techniques\ndescribed so far is provided in the supplementary material.\n2.3.3. LOCAL LEARNING SIGNALS\nSo far we made no assumptions about the structure of the\nmodel or the inference network. However, by taking advan-\ntage of their conditional independence properties we can\ntrain the inference network using simpler and less noisy lo-\ncal learning signals instead of the monolithic global learn-\ning signal l\u03c6(x, h). Our approach to deriving a local signal\nfor a set of parameters involves removing all the terms from\nthe global signal that do not affect the value of the resulting\ngradient estimator.\nWe will derive the layer-speci\ufb01c learning signals for the\ncommon case of both the model and the inference network\nhaving n layers of latent variables. The model and the vari-\national posterior distributions then naturally factor as\nP\u03b8(x, h) =P\u03b8(x|h1)\nYn\u22121\ni=1 P\u03b8(hi|hi+1)P\u03b8(hn),\n(10)\nQ\u03c6(h|x) =Q\u03c61(h1|x)\nYn\u22121\ni=1 Q\u03c6i+1(hi+1|hi),\n(11)\nwhere hi denotes the latent variables in the ith layer and \u03c6i\nthe parameters of the variational distribution for that layer.\nWe will also use hi:j to denote the latent variables in layers\ni through j.\nTo learn the parameters of the the variational distribution\nfor layer i , we need to compute the following gradient:\n\u2207\u03c6iL(x) = EQ(h|x)[l\u03c6(x, h)\u2207\u03c6i log Q\u03c6i(hi|hi\u22121)].\nUsing the law of iterated expectation we can rewrite the\nexpectation w.r.t. Q(h|x) as\n\u2207\u03c6iL(x) = EQ(h1:i\u22121|x)[\nEQ(hi:n|hi\u22121)[l\u03c6(x, h)\u2207\u03c6i log Q\u03c6i(hi|hi\u22121)]|hi\u22121]],\nwhere we also used the fact that under the variational pos-\nterior, hi:n is independent of h1:i\u22122 and x, given hi\u22121. As\na consequence of Eq. 9, when computing the expectation\nw.r.t. Q(hi:n|hi\u22121), all the terms in the learning signal that\ndo not depend on hi:n can be safely dropped without af-\nfecting the result. This gives us the following local learning\nsignal for layer i:\nli\n\u03c6(x, h) = log P\u03b8(hi\u22121:n) \u2212log Q\u03c6(hi:n|hi\u22121).\n(12)\nTo get the signal for the \ufb01rst hidden layer we simply use x\nin place of h0, in which case we simply recover the global\nlearning signal. For hidden layers i > 1, however, the local\nsignal involves fewer terms than l\u03c6(x, h) and thus can be\nexpected to be less noisy. As we do not assume any within-\nlayer structure, Eq. 12 applies to models and inference net-\nworks whether or not Q\u03c6(hi|hi\u22121) and P\u03b8(hi|hi+1) are\nfactorial.\nSince local signals can be signi\ufb01cantly different from each\nother, we use separate baselines and variance estimates for\neach signal. For layers i > 1, the input-dependent baseline\nC\u03c8(x) is replaced by Ci\n\u03c8i(hi\u22121).\nIn some cases, further simpli\ufb01cation of the learning signal\nis possible, yielding a different signal per latent variable.\nWe leave exploring this as future work.\n3. Related work\n3.1. Feedforward approximations to inference\nThe idea of training an approximate inference network\nby optimizing a variational lower bound is not new.\nIt\ngoes back at least to Hinton & Zemel (1994), who derived\nthe variational objective from the Minimum Description\nLength (MDL) perspective and used it to train linear au-\ntoencoders. Their probabilistic encoder and decoder cor-\nrespond to our inference network and model respectively.\nHowever, they computed the gradients analytically, which\nwas possible due to the simplicity of their model, and dis-\nmissed the sampling-based approach as infeasible due to\nnoise.\nSalakhutdinov & Larochelle (2010) proposed using a feed-\nforward \u201crecognition\u201d model to perform ef\ufb01cient input-\ndependent initialization for the mean \ufb01eld inference algo-\nrithm in deep Boltzmann machines. As the recognition\nNeural Variational Inference and Learning in Belief Networks\nmodel is trained to match the marginal probabilities pro-\nduced by mean \ufb01eld inference, it inherits the limitations\nof the inference procedure, such as the inability to model\nstructured posteriors. In contrast, in NVIL the inference\nnet is trained to match the true posterior directly, without\ninvolving an approximate inference algorithm, and thus the\naccuracy of the \ufb01t is limited only by the expressiveness of\nthe inference network itself.\nRecently a method for training nonlinear models with\ncontinuous latent variables,\ncalled Stochastic Gradi-\nent Variational Bayes (SGVB), has been proposed by\nKingma & Welling (2013) and Rezende et al. (2014). Like\nNVIL, it involves using feedforward models to perform\napproximate inference and trains them by optimizing a\nsampling-based estimate of the variational bound on the\nlog-likelihood. However, SGVB is considerably less gen-\neral than NVIL, because it uses a gradient estimator ob-\ntained by taking advantage of special properties of real-\nvalued random variables and thus is not applicable to\nmodels with discrete random variables. Moreover, unlike\nNVIL, SGVB method cannot handle inference networks\nwith nonlinear dependencies between latent variables. The\nideas of the two methods are complementary however, and\nNVIL is likely to bene\ufb01t from the SGVB-style treatment of\ncontinuous-valued variables, while SGVB might converge\nfaster using the variance reduction techniques we proposed.\nGregor et al. (2013) have recently proposed a related al-\ngorithm for training sigmoid belief network like models\nbased on the MDL framework. They also use a feedfor-\nward model to perform approximate inference, but concen-\ntrate on the case of a deterministic inference network and\ncan handle only binary latent variables. The inference net-\nwork is trained by backpropagating through binary thresh-\nolding units, ignoring the thresholding nonlinearities, to ap-\nproximately minimize the coding cost of the joint latent-\nvisible con\ufb01gurations. This approach can be seen as ap-\nproximately maximizing a looser variational lower bound\nthan (2) due to the absence of the entropy term.\nAn inference network for ef\ufb01cient generation of sam-\nples from the approximate posterior can also be seen as\na probabilistic generalization of the approximate feed-\nforward inference methods developed for sparse coding\nmodels in the last few years (Kavukcuoglu et al., 2008;\nBradley & Bagnell, 2008; Gregor & LeCun, 2010).\n3.2. Sampling-based variational inference\nLike NVIL, Black Box Variational Inference (BBVI,\nRanganath et al., 2013) learns the variational parameters\nof the posterior by optimizing the variational bound using\nsampling-based gradient estimates, which makes it appli-\ncable to a large range of models. However, unlike NVIL,\nBBVI follows the traditional approach of learning a sepa-\nrate set of variational parameters for each observation and\ndoes not use an inference network. Moreover, BBVI uses a\nfully-factorized mean \ufb01eld approximation to the posterior,\nwhich limits its power.\n3.3. The wake-sleep algorithm\nNVIL shares many similarities with the wake-sleep algo-\nrithm (Hinton et al., 1995), which enjoys the same scala-\nbility and applicability to a wide range of models. This\nalgorithm was introduced for training Helmholtz machines\n(Dayan et al., 1995), which are multi-layer belief networks\naugmented with recognition networks. These recognition\nnetworks are used for approximate inference and are di-\nrectly analogous to NVIL inference networks. Wake-sleep\nalternates between updating the model parameters in the\nwake phase and the recognition network parameters in the\nsleep phase. The model parameter update is based on the\nsamples generated from the recognition network on the\ntraining data and is identical to the NVIL one (Eq. 5). How-\never, in contrast to NVIL, the recognition network param-\neters are learned from samples generated by the model. In\nother words, the recognition network is trained to recover\nthe hidden causes corresponding to the samples from the\nmodel distribution by following the gradient\n\u2207\u03c6L(x) = EP\u03b8(x,h) [\u2207\u03c6 log Q\u03c6(h|x)] .\n(13)\nUnfortunately, this update does not optimize the same ob-\njective as the model parameter update, which means that\nthe wake-sleep algorithm does not optimize a well-de\ufb01ned\nobjective function and is not guaranteed to converge. This\nis the algorithm\u2019s main weakness, compared to NVIL,\nwhich optimizes a variational lower bound on the log-\nlikelihood.\nThe wake-sleep gradient for recognition network parame-\nters does have the advantage of being much easier to es-\ntimate than the corresponding gradient of the variational\nbound. In fact, the idea of training the recognition net-\nworks using the gradient of the bound was mentioned in\n(Hinton & Zemel, 1994) and (Dayan & Hinton, 1996) but\nnot seriously considered due concerns about the high vari-\nance of the estimates. In Section 4.2 we show that while\nthe naive estimator of the gradient given in Eq. 6 does ex-\nhibit high variance, the variance reduction techniques from\nSection 2.3 improve it dramatically and make it practical.\n3.4. REINFORCE\nUsing the gradient (4) to train the inference network can\nbe seen as an instance of the REINFORCE algorithm\n(Williams, 1992) from reinforcement learning (RL), which\nadapts the parameters of a stochastic model to maximize\nthe external reward signal which depends on the model\u2019s\noutput. Given a model P\u03b8(x) and a reward signal r(x),\nNeural Variational Inference and Learning in Belief Networks\nREINFORCE updates the model parameters using the rule\n\u2206\u03b8 \u221dEP [(r(x) \u2212b)\u2207\u03b8 log P\u03b8(x)].\n(14)\nWe can view NVIL as an application of REINFORCE on\nthe per-training-case basis, with the inference network cor-\nresponding to the stochastic model, latent state h to the out-\nput, and the learning signal l\u03c6(x, h) to the reward. The term\nb in Eq. 14, called a baseline in the RL literature, is a hy-\nperparameter that can be adapted to reduce the variance of\nthe parameter update. Thus it serves the same function as\nc and C\u03c8(x) that we subtract from the learning signal to\ncenter it in Section 2.3.1. The considerable body of work\non baselines and other variance reduction methods done in\nthe RL community (e.g. Greensmith et al., 2004) is likely\nto contain additional techniques relevant for training infer-\nence networks.\n4. Experimental results\nWe performed two sets of experiments, with the \ufb01rst set\nintended to evaluate the effectiveness of our variance re-\nduction techniques and to compare NVIL\u2019s performance to\nthat of the wake-sleep algorithm. In the second set of ex-\nperiments, we demonstrate NVIL\u2019s ability to handle larger\nreal-world datasets by using it to train generative models of\ndocuments.\n4.1. Experimental protocol\nWe trained all models using stochastic gradient ascent us-\ning minibatches of 20 observations sampled randomly from\nthe training data. The gradient estimates were computed\nusing a single sample from the inference network. For each\ndataset, we created a validation set by removing a random\nsubset of 100 observations from the training set. The only\nform of regularization we used was early stopping based on\nthe validation bound, implemented by keeping track of the\nparameter con\ufb01guration with the best validation score seen\nso far. We implemented each input-dependent baseline us-\ning a neural network with a single hidden layer of 100 tanh\nunits.\nWe used \ufb01xed learning rates because we found them to\nproduce superior results to the annealing schedules we ex-\nperimented with. The learning rates we report were se-\nlected based on the validation set performance in prelim-\ninary experiments with smaller models. We always make\nthe learning rate for inference network \ufb01ve times smaller\nthan for the model (which is the one we report), as we found\nthis to improve performance. We used inference networks\nwith layered structure given by Eq. 11, without dependen-\ncies within each layer except in the experiment with au-\ntoregressive inference networks. All multi-layer inference\nnetworks were trained using layer-speci\ufb01c learning signals\nfrom Section 2.3.3.\nAs the models we train are intractable, we cannot compute\nthe exact log-likelihoods for them. Instead we report the\nestimates of the variational bound (2) computed using 10\nsamples from the inference network, which we found to be\nsuf\ufb01cient to get the accurate bound estimates. We expect\nthis approach to underestimate the log-likelihood consider-\nably, but leave \ufb01nding more direct and thus less pessimistic\nevaluation methods as future work.\n4.2. Modelling images of digits\nOur \ufb01rst set of experiments was performed on the binarized\nversion of the MNIST dataset, which has become the stan-\ndard benchmark for evaluating generative models of binary\ndata. The dataset consists of 70,000 28 \u00d7 28 binary im-\nages of handwritten digits, partitioned into a 60,000-image\ntraining set and 10,000-image test set. We used the bina-\nrization of Salakhutdinov & Murray (2008), which makes\nour scores directly comparable to those in the literature.\nWe used 3 \u00d7 10\u22124 as the learning rate for training mod-\nels with NVIL on this dataset. Centering the input vectors\nby subtracting the mean vector was essential for making\nthe inference networks and input-dependent baselines work\nwell.\nTo demonstrate the importance of variance reduction tech-\nniques, we trained two SBNs using a range of variance con-\ntrol settings. The \ufb01rst SBN had a single layer of 200 latent\nvariables, while the second one had two layers of 200 vari-\nables each. Figure 1 shows the estimate of the variational\nobjective on the validation set plotted against the number\nof parameter updates. For both models, it is clear that us-\ning all three techniques \u2013 the input-dependent and input-\nindependent baselines along with variance normalization \u2013\nis essential for best performance. However, of the three\ntechniques, the input-dependent baseline appears to be the\nleast important. Comparing the plots for the two models\nsuggests that variance reduction becomes more important\nfor larger models, with the gap between the best combina-\ntion and the others (excluding the very worst one) widen-\ning. For both models, learning with all three variance re-\nduction techniques disabled makes barely any progress and\nis clearly infeasible.\nWe found that disabling layer-speci\ufb01c learning signals had\nlittle effect on the performance of the resulting model. The\ndifference was about 0.4 nats for an SBN with two or three\nlayers of latent variables.\nWe next compared NVIL to the wake-sleep algorithm,\nwhich is its closest competitor in terms of scalability and\nbreadth of applicability, by training a range of models us-\ning both algorithms. Wake-sleep training used a learning\nrate of 1 \u00d7 10\u22124, as we found this algorithm to be more\nsensitive to the choice of the learning rate than NVIL,\nNeural Variational Inference and Learning in Belief Networks\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600\n1800\n2000\n\u2212240\n\u2212220\n\u2212200\n\u2212180\n\u2212160\n\u2212140\n\u2212120\n\u2212100\nSBN 200\nNumber of parameter updates\nValidation set bound\n \n \nBaseline, IDB, & VN\nBaseline & VN\nBaseline only\nVN only\nNo baselines & no VN\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600\n1800\n2000\n\u2212240\n\u2212220\n\u2212200\n\u2212180\n\u2212160\n\u2212140\n\u2212120\n\u2212100\nSBN 200\u2212200\nNumber of parameter updates\nValidation set bound\n \n \nBaseline, IDB, & VN\nBaseline & VN\nBaseline only\nVN only\nNo baselines & no VN\nFigure 1. Bounds on the validation set log-likelihood for an SBN with (Left) one and (Right) two layers of 200 latent variables. Baseline\nand IDB refer to the input-independent and the input-dependent baselines respectively. VN is variance normalization.\nTable 1. Results on the binarized MNIST dataset. \u201cDim\u201d is the\nnumber of latent variables in each layer, starting with the deepest\none. NVIL and WS refer to the models trained with NVIL and\nwake-sleep respectively. NLL is the negative log-likelihood for\nthe tractable models and an estimate of it for the intractable ones.\nMODEL\nDIM\nTEST NLL\nNVIL\nWS\nSBN\n200\n113.1\n120.8\nSBN\n500\n112.8\n121.4\nSBN\n200-200\n99.8\n107.7\nSBN\n200-200-200\n96.7\n102.2\nSBN\n200-200-500\n97.0\n102.3\nFDARN\n200\n92.5\n95.9\nFDARN\n500\n90.7\n97.2\nFDARN\n400\n96.3\nDARN\n400\n93.0\nNADE\n500\n88.9\nRBM (CD3)\n500\n105.5\nRBM (CD25)\n500\n86.3\nMOB\n500\n137.6\nperforming considerably better with lower learning rates.\nThe results, along with some baselines from the litera-\nture, are shown in Table 1. We report only the means of\nthe bound estimates as their standard deviations were all\nvery small, none exceeding 0.1 nat. We can see that mod-\nels trained with NVIL have considerably better bounds on\nthe log-likelihood, compared to their wake-sleep counter-\nparts, with the difference ranging from 3.4 to 8.6 nats. Ad-\nditional layers make SBNs perform better, independently\nof the training method. Interestingly, single-layer fDARN\n(Gregor et al., 2013) models, which have autoregressive\nconnections between the latent variables, perform better\nthan any of the SBN models trained using the same al-\ngorithm.\nComparing to results from the literature, we\nsee that all the SBN and fDARN models we trained per-\nform much better than a mixture of 500 factorial Bernoulli\ndistributions (MoB) but not as well as the determinis-\ntic Neural Autoregressive Distribution Estimator (NADE)\n(Larochelle & Murray, 2011). The NVIL-trained fDARN\nmodels with 200 and 500 latent variables also outperform\nthe fDARN (as well as the more expressive DARN) model\nwith 400 latent variables from (Gregor et al., 2013), which\nwere trained using an MDL-based algorithm. The fDARN\nand multi-layer SBN models trained using NVIL also out-\nperform a 500-hidden-unit RBM trained with 3-step con-\ntrastive divergence (CD), but not the one trained with 25-\nstep CD (Salakhutdinov & Murray, 2008). However, both\nsampling and CD-25 training in an RBM is considerably\nmore expensive than sampling or NVIL training for any of\nour models.\nThe sampling-based approach to computing gradients al-\nlows NVIL to handle variational posteriors with complex\ndependencies.\nTo demonstrate this ability, we retrained\nseveral of the SBN models using inference networks with\nautoregressive connections within each layer. These net-\nworks can capture the dependencies between variables\nwithin layers and thus are considerably more expressive\nthan the ones with factorial layers. Results in Table 2 in-\ndicate that using inference networks with autoregressive\nconnections produces better models, with the single-layer\nmodels exhibiting large gains.\n4.3. Document modelling\nWe also applied NVIL to the more practical task of docu-\nment modelling. The goal is to train a generative model\nof documents which are represented as vectors of word\ncounts, also known as bags of words. We trained two sim-\nNeural Variational Inference and Learning in Belief Networks\nTable 2. The effect of using autoregressive connections in the in-\nference network. \u201cDim\u201d is the number of latent variables in each\nlayer, starting with the deepest one. \u201cTest NLL\u201d is an estimate\nof the lower bound on the log-likelihood on the MNIST test set.\n\u201dAutoreg\u201d and \u201cFactorial\u201d refer to using inference networks with\nand without autoregressive connections respectively.\nMODEL\nDIM\nTEST NLL\nAUTOREG\nFACTORIAL\nSBN\n200\n103.8\n113.1\nSBN\n500\n104.4\n112.8\nSBN\n200-200-200\n94.5\n96.7\nSBN\n200-200-500\n96.0\n97.0\nple models on the 20 Newsgroups and Reuters Corpus Vol-\nume I (RCV1-v2) datasets, which have been used to eval-\nuate similar models in (Salakhutdinov & Hinton, 2009b;\nLarochelle & Lauly, 2012).\n20 Newsgroups is a fairly\nsmall dataset of Usenet newsgroup posts, consisting of\nabout 11K training and 7.5K test documents.\nRCV1 is\na much larger dataset of Reuters newswire articles, with\nabout 794.4K training and 10K test documents. We use\nthe standard preprocessed versions of the datasets from\nSalakhutdinov & Hinton (2009b), which have vocabularies\nof 2K and 10K words respectively.\nWe experimented with two simple document models, based\non the SBN and DARN architectures. Both models had a\nsingle layer of latent variables and a multinomial visible\nlayer and can be seen as directed counterparts of the Repli-\ncated Softmax model (Salakhutdinov & Hinton, 2009b).\nWe used the same training procedure as on MNIST with\nthe exception of the learning rates which were 3 \u00d7 10\u22125 on\n20 Newsgroups and 10\u22123 on RCV1.\nThe\nestablished\nevaluation\nmetric\nfor\nsuch\nmod-\nels\nis\nthe\nperplexity\nper\nword,\ncomputed\nas\nexp\n\u0010\n\u22121\nN\nP\nn\n1\nLn log P(xn)\n\u0011\n,\nwhere N\nis the num-\nber of documents, Ln is the length of document n, and\nP(xn) the probability of the document under the model.\nAs we cannot compute log P(xn), we use the variational\nlower bound in its place and thus report an upper bound on\nperplexity.\nThe\nresults\nfor our\nmodels,\nalong\nwith\nones\nfor\nthe\nReplicated\nSoftmax\nand\nDocNADE\nmod-\nels\nfrom\n(Salakhutdinov & Hinton,\n2009b)\nand\n(Larochelle & Lauly,\n2012)\nrespectively,\nare\nshown\nin Table 3. We can see that the SBN and fDARN models\nwith 50 latent variables perform well, producing better\nscores than LDA and Replicated Softmax on both datasets.\nTheir performance is also competitive with that of Doc-\nNADE on 20 Newsgroups. The score of 724 for fDARN\nwith 50 latent variables on RCV1 is already better than\nDocNADE\u2019s 742, the best published result on that dataset.\nTable 3. Document modelling results. \u201cDim\u201d is the number of\nlatent variables in the model. The third and the fourth columns\nreport the estimated test set perplexity on the 20 Newsgroups and\nReuters RCV1 datasets respectively.\nMODEL\nDIM\n20 NEWS\nREUTERS\nSBN\n50\n909\n784\nFDARN\n50\n917\n724\nFDARN\n200\n598\nLDA\n50\n1091\n1437\nLDA\n200\n1058\n1142\nREPSOFTMAX\n50\n953\n988\nDOCNADE\n50\n896\n742\nfDARN with 200 hidden units, however, performs even\nbetter, setting a new record with 598.\n5. Discussion and future work\nWe developed, NVIL, a new training method for intractable\ndirected latent variable models which is general and easy to\napply to new models. We showed that NVIL consistently\noutperforms the wake-sleep algorithm at training sigmoid-\nbelief-network-like models. Finally, we demonstrated the\npotential of our approach by achieving state-of-the-art re-\nsults on a sizable dataset of documents (Reuters RCV1).\nAs the emphasis of this paper is on the training method, we\napplied it to some of the simplest possible model and in-\nference network architectures, which was suf\ufb01cient to ob-\ntain promising results. We believe that considerable perfor-\nmance gains can be made by using more expressive archi-\ntectures, such as those with nonlinearities between layers of\nstochastic variables. Applying NVIL to models with con-\ntinuous latent variables is another promising direction since\nbinary latent variables are not always appropriate.\nWe expect NVIL to be also applicable to training condi-\ntional latent variable models for modelling the distribu-\ntion of observations given some context, which would re-\nquire making the inference network take both the context\nand the observation as input. This would make it an al-\nternative to the importance-sampling training method of\nTang & Salakhutdinov (2013) for conditional models with\nstructured high-dimensional outputs.\nWe hope that the generality and \ufb02exibility of our approach\nwill make it easier to apply powerful directed latent vari-\nable models to real-world problems.\nACKNOWLEDGEMENTS\nWe thank Koray Kavukcuoglu, Volodymyr Mnih, and\nNicolas Heess for their helpful comments. We thank Rus-\nlan Salakhutdinov for providing us with the preprocessed\ndocument datasets.\nNeural Variational Inference and Learning in Belief Networks\nReferences\nBradley, David M and Bagnell, J Andrew.\nDifferential\nsparse coding. In Advances in Neural Information Pro-\ncessing Systems, volume 20, 2008.\nDayan, Peter and Hinton, Geoffrey E.\nVarieties of\nhelmholtz machine. Neural Networks, 9(8):1385\u20131403,\n1996.\nDayan, Peter, Hinton, Geoffrey E, Neal, Radford M, and\nZemel, Richard S. The helmholtz machine. Neural com-\nputation, 7(5):889\u2013904, 1995.\nGreensmith, Evan, Bartlett, Peter L., and Baxter, Jonathan.\nVariance reduction techniques for gradient estimates in\nreinforcement learning. Journal of Machine Learning\nResearch, 5:1471\u20131530, 2004.\nGregor, Karol and LeCun, Yann. Learning fast approxima-\ntions of sparse coding. In Proc. International Conference\non Machine learning (ICML\u201910), 2010.\nGregor, Karol, Mnih, Andriy, and Wierstra, Daan. Deep au-\ntoregressive networks. arXiv preprint arXiv:1310.8499,\n2013.\nHinton, Geoffrey E and Zemel, Richard S. Autoencoders,\nminimum description length, and Helmholtz free energy.\nIn Advances in Neural Information Processing Systems,\n1994.\nHinton, Geoffrey E, Dayan, Peter, Frey, Brendan J, and\nNeal, Radford M. The \"wake-sleep\" algorithm for un-\nsupervised neural networks. Science, 268(5214):1158\u2013\n1161, 1995.\nHinton, Geoffrey E., Osindero, Simon, and Teh, Yee Whye.\nA fast learning algorithm for deep belief nets. Neural\nComputation, 18(7):1527\u20131554, 2006.\nJordan,\nMichael I.,\nGhahramani,\nZoubin,\nJaakkola,\nTommi S., and Saul, Lawrence K.\nAn introduction\nto variational methods for graphical models. Machine\nLearning, 37(2):183\u2013233, 1999.\nKavukcuoglu, Koray, Ranzato, Marc\u2019Aurelio, and LeCun,\nYann. Fast inference in sparse coding algorithms with\napplications to object recognition.\nTechnical report,\nCourant Institute, NYU, 2008.\nKingma, Diederik P and Welling, Max.\nAuto-encoding\nvariational bayes.\narXiv preprint arXiv:1312.6114,\n2013.\nLarochelle, Hugo and Lauly, Stanislas. A neural autore-\ngressive topic model. In Advances in Neural Information\nProcessing Systems, pp. 2717\u20132725, 2012.\nLarochelle, Hugo and Murray, Iain. The neural autoregres-\nsive distribution estimator.\nJMLR: W&CP, 15:29\u201337,\n2011.\nNeal, Radford M.\nConnectionist learning of belief net-\nworks. Arti\ufb01cial intelligence, 56(1):71\u2013113, 1992.\nPaisley, John William, Blei, David M., and Jordan,\nMichael I. Variational bayesian inference with stochastic\nsearch. In ICML, 2012.\nRanganath, Rajesh, Gerrish, Sean, and Blei, David M.\nBlack box variational inference.\narXiv preprint\narXiv:1401.0118, 2013.\nRezende, Danilo Jimenez, Mohamed, Shakir, and Wierstra,\nDaan. Stochastic back-propagation and variational in-\nference in deep latent gaussian models. arXiv preprint\narXiv:1401.4082, 2014.\nSalakhutdinov, Ruslan and Hinton, Geoffrey E. Deep boltz-\nmann machines. In International Conference on Arti\ufb01-\ncial Intelligence and Statistics, pp. 448\u2013455, 2009a.\nSalakhutdinov, Ruslan and Hinton, Geoffrey E. Replicated\nsoftmax: an undirected topic model. In Advances in neu-\nral information processing systems, 2009b.\nSalakhutdinov, Ruslan and Larochelle, Hugo.\nEf\ufb01cient\nlearning of deep boltzmann machines. In International\nConference on Arti\ufb01cial Intelligence and Statistics, pp.\n693\u2013700, 2010.\nSalakhutdinov, Ruslan and Murray, Iain. On the quantita-\ntive analysis of Deep Belief Networks. In Proceedings\nof the 25th Annual International Conference on Machine\nLearning (ICML 2008), 2008.\nSaul,\nLawrence K.,\nJaakkola,\nTommi,\nand Jordan,\nMichael I.\nMean \ufb01eld theory for sigmoid belief net-\nworks. Journal of Arti\ufb01cial Intelligence Research, 4:61\u2013\n76, 1996.\nTang, Yichuan and Salakhutdinov, Ruslan.\nLearning\nstochastic feedforward neural networks. In Advances in\nNeural Information Processing Systems, 2013.\nWeaver, Lex and Tao, Nigel. The optimal reward baseline\nfor gradient-based reinforcement learning.\nIn In Pro-\nceedings of the Seventeenth Conference on Uncertainty\nin Arti\ufb01cial Intelligence, 2001.\nWilliams, Ronald J. Simple statistical gradient-following\nalgorithms for connectionist reinforcement learning.\nMachine learning, 8(3-4):229\u2013256, 1992.\nNeural Variational Inference and Learning in Belief Networks\nA. Algorithm for computing NVIL gradients\nAlgorithm 1 provides an outline of our implementation of\nNVIL gradient computation for a minibatch of n randomly\nchosen training cases. The exponential smoothing factor \u03b1\nused for updating the estimates of the mean c and variance\nv of the inference network learning signal was set to 0.8 in\nour experiments.\nAlgorithm 1 Compute gradient estimates for the model and\nthe inference network\n\u2206\u03b8 \u21900, \u2206\u03c6 \u21900, \u2206\u03c8 \u21900\nL \u21900\n{Compute the learning signal and the bound}\nfor i \u21901 to n do\nxi \u2190random training case\n{Sample from the inference model}\nhi \u223cQ\u03c6(hi|xi)\n{Compute the unnormalized learning signal}\nli \u2190log P\u03b8(xi, hi) \u2212log Q\u03c6(hi|xi)\n{Add the case contribution to the bound}\nL \u2190L + li\n{Subtract the input-dependent baseline}\nli \u2190li \u2212C\u03c8(xi)\nend for\n{Update the learning signal statistics}\ncb \u2190mean(l1, ..., ln)\nvb \u2190variance(l1, ..., ln)\nc \u2190\u03b1c + (1 \u2212\u03b1)cb\nv \u2190\u03b1v + (1 \u2212\u03b1)vb\nfor i \u21901 to n do\nli \u2190\nli\u2212c\nmax(1,\u221av)\n{Accumulate the model parameter gradient}\n\u2206\u03b8 \u2190\u2206\u03b8 + \u2207\u03b8 log P\u03b8(xi, hi)\n{Accumulate the inference net gradient}\n\u2206\u03c6 \u2190\u2206\u03c6 + li\u2207\u03c6 log Q\u03c6(hi|xi)\n{Accumulate the input-dependent baseline gradient}\n\u2206\u03c8 \u2190\u2206\u03c8 + li\u2207\u03c8C\u03c8(xi)\nend for\nB. Inference network gradient derivation\nDifferentiating the variational lower bound w.r.t. to the in-\nference network parameters gives\n\u2207\u03c6L(x) =\u2207\u03c6EQ[log P\u03b8(x, h) \u2212log Q\u03c6(h|x)]\n=\u2207\u03c6\nX\nh\nQ\u03c6(h|x) log P\u03b8(x, h)\u2212\n\u2207\u03c6\nX\nh\nQ\u03c6(h|x) log Q\u03c6(h|x)\n=\nX\nh\nlog P\u03b8(x, h)\u2207\u03c6Q\u03c6(h|x)\u2212\nX\nh\n(log Q\u03c6(h|x) + 1) \u2207\u03c6Q\u03c6(h|x)\n=\nX\nh\n(log P\u03b8(x, h) \u2212log Q\u03c6(h|x)) \u2207\u03c6Q\u03c6(h|x),\nwhere\nwe\nused\nthe\nfact\nthat\nP\nh \u2207\u03c6Q\u03c6(h|x)\n=\n\u2207\u03c6\nP\nh Q\u03c6(h|x)\n=\n\u2207\u03c61\n=\n0.\nUsing the identity\n\u2207\u03c6Q\u03c6(h|x) = Q\u03c6(h|x)\u2207\u03c6 log Q\u03c6(h|x), then gives\n\u2207\u03c6L(x) =\nX\nh\n(log P\u03b8(x, h) \u2212log Q\u03c6(h|x))\n\u00d7 Q\u03c6(h|x)\u2207\u03c6 log Q\u03c6(h|x)\n=EQ [(log P\u03b8(x, h) \u2212log Q\u03c6(h|x)) \u2207\u03c6 log Q\u03c6(h|x)] .\n",
        "sentence": " This basic idea has been applied and enhanced many times; initially with the wake-sleep algorithm (WS, [3, 5]) and more recently with the variational autoencoder (VAE, [6]), stochastic backpropagation and approximate inference in deep generative models [7], neural variational inference and learning (NVIL, [8]) and reweighted wake-sleep (RWS, [9]).",
        "context": "plying our approach to training sigmoid belief\nnetworks and deep autoregressive networks, we\nshow that it outperforms the wake-sleep algo-\nrithm on MNIST and achieves state-of-the-art re-\nsults on the Reuters RCV1 document dataset.\n1. Introduction\n1994.\nHinton, Geoffrey E, Dayan, Peter, Frey, Brendan J, and\nNeal, Radford M. The \"wake-sleep\" algorithm for un-\nsupervised neural networks. Science, 268(5214):1158\u2013\n1161, 1995.\nHinton, Geoffrey E., Osindero, Simon, and Teh, Yee Whye.\nnetworks are used for approximate inference and are di-\nrectly analogous to NVIL inference networks. Wake-sleep\nalternates between updating the model parameters in the\nwake phase and the recognition network parameters in the"
    },
    {
        "title": "Reweighted wake-sleep",
        "author": [
            "Jorg Bornschein",
            "Yoshua Bengio"
        ],
        "venue": "In International Conference on Learning Representations (ICLR\u20192015),",
        "citeRegEx": "9",
        "shortCiteRegEx": "9",
        "year": 2015,
        "abstract": "",
        "full_text": "",
        "sentence": " This basic idea has been applied and enhanced many times; initially with the wake-sleep algorithm (WS, [3, 5]) and more recently with the variational autoencoder (VAE, [6]), stochastic backpropagation and approximate inference in deep generative models [7], neural variational inference and learning (NVIL, [8]) and reweighted wake-sleep (RWS, [9]). Analogous to the parameter updates in reweighted wake-sleep (RWS, [9]) we can derive an important sampling based estimate for the parameter gradients and use them to optimize towards a These properties are basically inherited from the RWS training algorithm [9]. 83 RWS - NADE[9] 13. 38 RWS - SBN[9] 13.",
        "context": null
    },
    {
        "title": "Deep unsupervised learning using nonequilibrium thermodynamics",
        "author": [
            "Jascha Sohl-Dickstein",
            "Eric A. Weiss",
            "Niru Maheswaranathan",
            "Surya Ganguli"
        ],
        "venue": "CoRR, abs/1503.03585,",
        "citeRegEx": "10",
        "shortCiteRegEx": "10",
        "year": 2015,
        "abstract": "A central problem in machine learning involves modeling complex data-sets\nusing highly flexible families of probability distributions in which learning,\nsampling, inference, and evaluation are still analytically or computationally\ntractable. Here, we develop an approach that simultaneously achieves both\nflexibility and tractability. The essential idea, inspired by non-equilibrium\nstatistical physics, is to systematically and slowly destroy structure in a\ndata distribution through an iterative forward diffusion process. We then learn\na reverse diffusion process that restores structure in data, yielding a highly\nflexible and tractable generative model of the data. This approach allows us to\nrapidly learn, sample from, and evaluate probabilities in deep generative\nmodels with thousands of layers or time steps, as well as to compute\nconditional and posterior probabilities under the learned model. We\nadditionally release an open source reference implementation of the algorithm.",
        "full_text": "Deep Unsupervised Learning using\nNonequilibrium Thermodynamics\nJascha Sohl-Dickstein\nJASCHA@STANFORD.EDU\nStanford University\nEric A. Weiss\nEAWEISS@BERKELEY.EDU\nUniversity of California, Berkeley\nNiru Maheswaranathan\nNIRUM@STANFORD.EDU\nStanford University\nSurya Ganguli\nSGANGULI@STANFORD.EDU\nStanford University\nAbstract\nA central problem in machine learning involves\nmodeling complex data-sets using highly \ufb02exi-\nble families of probability distributions in which\nlearning, sampling, inference, and evaluation\nare still analytically or computationally tractable.\nHere, we develop an approach that simultane-\nously achieves both \ufb02exibility and tractability.\nThe essential idea, inspired by non-equilibrium\nstatistical physics, is to systematically and slowly\ndestroy structure in a data distribution through\nan iterative forward diffusion process. We then\nlearn a reverse diffusion process that restores\nstructure in data, yielding a highly \ufb02exible and\ntractable generative model of the data. This ap-\nproach allows us to rapidly learn, sample from,\nand evaluate probabilities in deep generative\nmodels with thousands of layers or time steps,\nas well as to compute conditional and posterior\nprobabilities under the learned model. We addi-\ntionally release an open source reference imple-\nmentation of the algorithm.\n1. Introduction\nHistorically, probabilistic models suffer from a tradeoff be-\ntween two con\ufb02icting objectives: tractability and \ufb02exibil-\nity. Models that are tractable can be analytically evaluated\nand easily \ufb01t to data (e.g. a Gaussian or Laplace). However,\nProceedings of the 32 nd International Conference on Machine\nLearning, Lille, France, 2015. JMLR: W&CP volume 37. Copy-\nright 2015 by the author(s).\nthese models are unable to aptly describe structure in rich\ndatasets. On the other hand, models that are \ufb02exible can be\nmolded to \ufb01t structure in arbitrary data. For example, we\ncan de\ufb01ne models in terms of any (non-negative) function\n\u03c6(x) yielding the \ufb02exible distribution p (x) = \u03c6(x)\nZ , where\nZ is a normalization constant. However, computing this\nnormalization constant is generally intractable. Evaluating,\ntraining, or drawing samples from such \ufb02exible models typ-\nically requires a very expensive Monte Carlo process.\nA variety of analytic approximations exist which amelio-\nrate, but do not remove, this tradeoff\u2013for instance mean\n\ufb01eld theory and its expansions (T, 1982; Tanaka, 1998),\nvariational Bayes (Jordan et al., 1999), contrastive diver-\ngence (Welling & Hinton, 2002; Hinton, 2002), minimum\nprobability \ufb02ow (Sohl-Dickstein et al., 2011b;a), minimum\nKL contraction (Lyu, 2011), proper scoring rules (Gneit-\ning & Raftery, 2007; Parry et al., 2012), score matching\n(Hyv\u00a8arinen, 2005), pseudolikelihood (Besag, 1975), loopy\nbelief propagation (Murphy et al., 1999), and many, many\nmore. Non-parametric methods (Gershman & Blei, 2012)\ncan also be very effective1.\n1.1. Diffusion probabilistic models\nWe present a novel way to de\ufb01ne probabilistic models that\nallows:\n1. extreme \ufb02exibility in model structure,\n2. exact sampling,\n1Non-parametric methods can be seen as transitioning\nsmoothly between tractable and \ufb02exible models. For instance,\na non-parametric Gaussian mixture model will represent a small\namount of data using a single Gaussian, but may represent in\ufb01nite\ndata as a mixture of an in\ufb01nite number of Gaussians.\narXiv:1503.03585v8  [cs.LG]  18 Nov 2015\nDeep Unsupervised Learning using Nonequilibrium Thermodynamics\n3. easy multiplication with other distributions, e.g. in or-\nder to compute a posterior, and\n4. the model log likelihood, and the probability of indi-\nvidual states, to be cheaply evaluated.\nOur method uses a Markov chain to gradually convert one\ndistribution into another, an idea used in non-equilibrium\nstatistical physics (Jarzynski, 1997) and sequential Monte\nCarlo (Neal, 2001). We build a generative Markov chain\nwhich converts a simple known distribution (e.g. a Gaus-\nsian) into a target (data) distribution using a diffusion pro-\ncess. Rather than use this Markov chain to approximately\nevaluate a model which has been otherwise de\ufb01ned, we ex-\nplicitly de\ufb01ne the probabilistic model as the endpoint of the\nMarkov chain. Since each step in the diffusion chain has an\nanalytically evaluable probability, the full chain can also be\nanalytically evaluated.\nLearning in this framework involves estimating small per-\nturbations to a diffusion process. Estimating small pertur-\nbations is more tractable than explicitly describing the full\ndistribution with a single, non-analytically-normalizable,\npotential function. Furthermore, since a diffusion process\nexists for any smooth target distribution, this method can\ncapture data distributions of arbitrary form.\nWe demonstrate the utility of these diffusion probabilistic\nmodels by training high log likelihood models for a two-\ndimensional swiss roll, binary sequence, handwritten digit\n(MNIST), and several natural image (CIFAR-10, bark, and\ndead leaves) datasets.\n1.2. Relationship to other work\nThe wake-sleep algorithm (Hinton, 1995; Dayan et al.,\n1995) introduced the idea of training inference and gen-\nerative probabilistic models against each other.\nThis\napproach remained largely unexplored for nearly two\ndecades, though with some exceptions (Sminchisescu et al.,\n2006; Kavukcuoglu et al., 2010).\nThere has been a re-\ncent explosion of work developing this idea. In (Kingma\n& Welling, 2013; Gregor et al., 2013; Rezende et al., 2014;\nOzair & Bengio, 2014) variational learning and inference\nalgorithms were developed which allow a \ufb02exible genera-\ntive model and posterior distribution over latent variables\nto be directly trained against each other.\nThe variational bound in these papers is similar to the one\nused in our training objective and in the earlier work of\n(Sminchisescu et al., 2006). However, our motivation and\nmodel form are both quite different, and the present work\nretains the following differences and advantages relative to\nthese techniques:\n1. We develop our framework using ideas from physics,\nquasi-static processes, and annealed importance sam-\npling rather than from variational Bayesian methods.\n2. We show how to easily multiply the learned distribu-\ntion with another probability distribution (eg with a\nconditional distribution in order to compute a poste-\nrior)\n3. We address the dif\ufb01culty that training the inference\nmodel can prove particularly challenging in varia-\ntional inference methods, due to the asymmetry in the\nobjective between the inference and generative mod-\nels. We restrict the forward (inference) process to a\nsimple functional form, in such a way that the re-\nverse (generative) process will have the same func-\ntional form.\n4. We train models with thousands of layers (or time\nsteps), rather than only a handful of layers.\n5. We provide upper and lower bounds on the entropy\nproduction in each layer (or time step)\nThere are a number of related techniques for training prob-\nabilistic models (summarized below) that develop highly\n\ufb02exible forms for generative models, train stochastic tra-\njectories, or learn the reversal of a Bayesian network.\nReweighted wake-sleep (Bornschein & Bengio, 2015) de-\nvelops extensions and improved learning rules for the orig-\ninal wake-sleep algorithm. Generative stochastic networks\n(Bengio & Thibodeau-Laufer, 2013; Yao et al., 2014) train\na Markov kernel to match its equilibrium distribution to\nthe data distribution.\nNeural autoregressive distribution\nestimators (Larochelle & Murray, 2011) (and their recur-\nrent (Uria et al., 2013a) and deep (Uria et al., 2013b) ex-\ntensions) decompose a joint distribution into a sequence\nof tractable conditional distributions over each dimension.\nAdversarial networks (Goodfellow et al., 2014) train a gen-\nerative model against a classi\ufb01er which attempts to dis-\ntinguish generated samples from true data. A similar ob-\njective in (Schmidhuber, 1992) learns a two-way map-\nping to a representation with marginally independent units.\nIn (Rippel & Adams, 2013; Dinh et al., 2014) bijective\ndeterministic maps are learned to a latent representation\nwith a simple factorial density function. In (Stuhlm\u00a8uller\net al., 2013) stochastic inverses are learned for Bayesian\nnetworks.\nMixtures of conditional Gaussian scale mix-\ntures (MCGSMs) (Theis et al., 2012) describe a dataset\nusing Gaussian scale mixtures, with parameters which de-\npend on a sequence of causal neighborhoods.\nThere is\nadditionally signi\ufb01cant work learning \ufb02exible generative\nmappings from simple latent distributions to data distribu-\ntions \u2013 early examples including (MacKay, 1995) where\nneural networks are introduced as generative models, and\n(Bishop et al., 1998) where a stochastic manifold mapping\nis learned from a latent space to the data space. We will\ncompare experimentally against adversarial networks and\nMCGSMs.\nRelated ideas from physics include the Jarzynski equal-\nity (Jarzynski, 1997), known in machine learning as An-\nDeep Unsupervised Learning using Nonequilibrium Thermodynamics\nt = 0\nt = T\n2\nt = T\nq\n\u0000x(0\u00b7\u00b7\u00b7T )\u0001\n2\n0\n2\n2\n0\n2\n2\n0\n2\n2\n0\n2\n2\n0\n2\n2\n0\n2\np\n\u0000x(0\u00b7\u00b7\u00b7T )\u0001\n2\n0\n2\n2\n0\n2\n2\n0\n2\n2\n0\n2\n2\n0\n2\n2\n0\n2\nf\u00b5\n\u0000x(t), t\n\u0001\n\u2212x(t)\nFigure 1. The proposed modeling framework trained on 2-d swiss roll data. The top row shows time slices from the forward trajectory\nq\n\u0010\nx(0\u00b7\u00b7\u00b7T )\u0011\n. The data distribution (left) undergoes Gaussian diffusion, which gradually transforms it into an identity-covariance Gaus-\nsian (right). The middle row shows the corresponding time slices from the trained reverse trajectory p\n\u0010\nx(0\u00b7\u00b7\u00b7T )\u0011\n. An identity-covariance\nGaussian (right) undergoes a Gaussian diffusion process with learned mean and covariance functions, and is gradually transformed back\ninto the data distribution (left). The bottom row shows the drift term, f\u00b5\n\u0010\nx(t), t\n\u0011\n\u2212x(t), for the same reverse diffusion process.\nnealed Importance Sampling (AIS) (Neal, 2001), which\nuses a Markov chain which slowly converts one distribu-\ntion into another to compute a ratio of normalizing con-\nstants. In (Burda et al., 2014) it is shown that AIS can also\nbe performed using the reverse rather than forward trajec-\ntory. Langevin dynamics (Langevin, 1908), which are the\nstochastic realization of the Fokker-Planck equation, show\nhow to de\ufb01ne a Gaussian diffusion process which has any\ntarget distribution as its equilibrium. In (Suykens & Vande-\nwalle, 1995) the Fokker-Planck equation is used to perform\nstochastic optimization. Finally, the Kolmogorov forward\nand backward equations (Feller, 1949) show that for many\nforward diffusion processes, the reverse diffusion processes\ncan be described using the same functional form.\n2. Algorithm\nOur goal is to de\ufb01ne a forward (or inference) diffusion pro-\ncess which converts any complex data distribution into a\nsimple, tractable, distribution, and then learn a \ufb01nite-time\nreversal of this diffusion process which de\ufb01nes our gener-\native model distribution (See Figure 1). We \ufb01rst describe\nthe forward, inference diffusion process. We then show\nhow the reverse, generative diffusion process can be trained\nand used to evaluate probabilities. We also derive entropy\nbounds for the reverse process, and show how the learned\ndistributions can be multiplied by any second distribution\n(e.g. as would be done to compute a posterior when in-\npainting or denoising an image).\n2.1. Forward Trajectory\nWe label the data distribution q\n\u0000x(0)\u0001\n. The data distribu-\ntion is gradually converted into a well behaved (analyti-\ncally tractable) distribution \u03c0 (y) by repeated application\nof a Markov diffusion kernel T\u03c0 (y|y\u2032; \u03b2) for \u03c0 (y), where\n\u03b2 is the diffusion rate,\n\u03c0 (y) =\nZ\ndy\u2032T\u03c0 (y|y\u2032; \u03b2) \u03c0 (y\u2032)\n(1)\nq\n\u0010\nx(t)|x(t\u22121)\u0011\n= T\u03c0\n\u0010\nx(t)|x(t\u22121); \u03b2t\n\u0011\n.\n(2)\nDeep Unsupervised Learning using Nonequilibrium Thermodynamics\nt = 0\nt = T\n2\nt = T\np\n\u0000x(0\u00b7\u00b7\u00b7T )\u0001\n0\n5\n10\n15\nBin\n0\n5\n10\n15\n20\nSample\n0\n5\n10\n15\nBin\n0\n5\n10\n15\n20\nSample\n0\n5\n10\n15\nBin\n0\n5\n10\n15\n20\nSample\nFigure 2. Binary sequence learning via binomial diffusion. A binomial diffusion model was trained on binary \u2018heartbeat\u2019 data, where a\npulse occurs every 5th bin. Generated samples (left) are identical to the training data. The sampling procedure consists of initialization\nat independent binomial noise (right), which is then transformed into the data distribution by a binomial diffusion process, with trained\nbit \ufb02ip probabilities. Each row contains an independent sample. For ease of visualization, all samples have been shifted so that a pulse\noccurs in the \ufb01rst column. In the raw sequence data, the \ufb01rst pulse is uniformly distributed over the \ufb01rst \ufb01ve bins.\n(a)\n(b)\n(c)\n(d)\nFigure 3. The proposed framework trained on the CIFAR-10 (Krizhevsky & Hinton, 2009) dataset. (a) Example holdout data (similar\nto training data). (b) Holdout data corrupted with Gaussian noise of variance 1 (SNR = 1). (c) Denoised images, generated by sampling\nfrom the posterior distribution over denoised images conditioned on the images in (b). (d) Samples generated by the diffusion model.\nThe forward trajectory, corresponding to starting at the data\ndistribution and performing T steps of diffusion, is thus\nq\n\u0010\nx(0\u00b7\u00b7\u00b7T )\u0011\n= q\n\u0010\nx(0)\u0011 T\nY\nt=1\nq\n\u0010\nx(t)|x(t\u22121)\u0011\n(3)\nFor the experiments shown below, q\n\u0000x(t)|x(t\u22121)\u0001\ncorre-\nsponds to either Gaussian diffusion into a Gaussian distri-\nbution with identity-covariance, or binomial diffusion into\nan independent binomial distribution. Table App.1 gives\nthe diffusion kernels for both Gaussian and binomial distri-\nbutions.\nDeep Unsupervised Learning using Nonequilibrium Thermodynamics\n2.2. Reverse Trajectory\nThe generative distribution will be trained to describe the\nsame trajectory, but in reverse,\np\n\u0010\nx(T )\u0011\n= \u03c0\n\u0010\nx(T )\u0011\n(4)\np\n\u0010\nx(0\u00b7\u00b7\u00b7T )\u0011\n= p\n\u0010\nx(T )\u0011 T\nY\nt=1\np\n\u0010\nx(t\u22121)|x(t)\u0011\n.\n(5)\nFor both Gaussian and binomial diffusion, for continuous\ndiffusion (limit of small step size \u03b2) the reversal of the\ndiffusion process has the identical functional form as the\nforward process (Feller, 1949). Since q\n\u0000x(t)|x(t\u22121)\u0001\nis a\nGaussian (binomial) distribution, and if \u03b2t is small, then\nq\n\u0000x(t\u22121)|x(t)\u0001\nwill also be a Gaussian (binomial) distribu-\ntion. The longer the trajectory the smaller the diffusion rate\n\u03b2 can be made.\nDuring learning only the mean and covariance for a Gaus-\nsian diffusion kernel, or the bit \ufb02ip probability for a bi-\nnomial kernel, need be estimated.\nAs shown in Table\nApp.1, f\u00b5\n\u0000x(t), t\n\u0001\nand f\u03a3\n\u0000x(t), t\n\u0001\nare functions de\ufb01ning\nthe mean and covariance of the reverse Markov transitions\nfor a Gaussian, and fb\n\u0000x(t), t\n\u0001\nis a function providing the\nbit \ufb02ip probability for a binomial distribution. The compu-\ntational cost of running this algorithm is the cost of these\nfunctions, times the number of time-steps. For all results in\nthis paper, multi-layer perceptrons are used to de\ufb01ne these\nfunctions. A wide range of regression or function \ufb01tting\ntechniques would be applicable however, including nonpa-\nrameteric methods.\n2.3. Model Probability\nThe probability the generative model assigns to the data is\np\n\u0010\nx(0)\u0011\n=\nZ\ndx(1\u00b7\u00b7\u00b7T )p\n\u0010\nx(0\u00b7\u00b7\u00b7T )\u0011\n.\n(6)\nNaively this integral is intractable \u2013 but taking a cue from\nannealed importance sampling and the Jarzynski equality,\nwe instead evaluate the relative probability of the forward\nand reverse trajectories, averaged over forward trajectories,\np\n\u0010\nx(0)\u0011\n=\nZ\ndx(1\u00b7\u00b7\u00b7T )p\n\u0010\nx(0\u00b7\u00b7\u00b7T )\u0011 q\n\u0000x(1\u00b7\u00b7\u00b7T )|x(0)\u0001\nq\n\u0000x(1\u00b7\u00b7\u00b7T )|x(0)\u0001 (7)\n=\nZ\ndx(1\u00b7\u00b7\u00b7T )q\n\u0010\nx(1\u00b7\u00b7\u00b7T )|x(0)\u0011\np\n\u0000x(0\u00b7\u00b7\u00b7T )\u0001\nq\n\u0000x(1\u00b7\u00b7\u00b7T )|x(0)\u0001\n(8)\n=\nZ\ndx(1\u00b7\u00b7\u00b7T )q\n\u0010\nx(1\u00b7\u00b7\u00b7T )|x(0)\u0011\n\u00b7\np\n\u0010\nx(T )\u0011 T\nY\nt=1\np\n\u0000x(t\u22121)|x(t)\u0001\nq\n\u0000x(t)|x(t\u22121)\u0001.\n(9)\nThis can be evaluated rapidly by averaging over samples\nfrom the forward trajectory q\n\u0000x(1\u00b7\u00b7\u00b7T )|x(0)\u0001\n. For in\ufb01nites-\nimal \u03b2 the forward and reverse distribution over trajecto-\nries can be made identical (see Section 2.2). If they are\nidentical then only a single sample from q\n\u0000x(1\u00b7\u00b7\u00b7T )|x(0)\u0001\nis required to exactly evaluate the above integral, as can\nbe seen by substitution. This corresponds to the case of a\nquasi-static process in statistical physics (Spinney & Ford,\n2013; Jarzynski, 2011).\n2.4. Training\nTraining amounts to maximizing the model log likelihood,\nL =\nZ\ndx(0)q\n\u0010\nx(0)\u0011\nlog p\n\u0010\nx(0)\u0011\n(10)\n=\nZ\ndx(0)q\n\u0010\nx(0)\u0011\n\u00b7\nlog\n\uf8ee\n\uf8f0\nR\ndx(1\u00b7\u00b7\u00b7T )q\n\u0000x(1\u00b7\u00b7\u00b7T )|x(0)\u0001\n\u00b7\np\n\u0000x(T )\u0001 QT\nt=1\np(x(t\u22121)|x(t))\nq(x(t)|x(t\u22121))\n\uf8f9\n\uf8fb, (11)\nwhich has a lower bound provided by Jensen\u2019s inequality,\nL \u2265\nZ\ndx(0\u00b7\u00b7\u00b7T )q\n\u0010\nx(0\u00b7\u00b7\u00b7T )\u0011\n\u00b7\nlog\n\"\np\n\u0010\nx(T )\u0011 T\nY\nt=1\np\n\u0000x(t\u22121)|x(t)\u0001\nq\n\u0000x(t)|x(t\u22121)\u0001\n#\n.\n(12)\nAs described in Appendix B, for our diffusion trajectories\nthis reduces to,\nL \u2265K\n(13)\nK = \u2212\nT\nX\nt=2\nZ\ndx(0)dx(t)q\n\u0010\nx(0), x(t)\u0011\n\u00b7\nDKL\n\u0010\nq\n\u0010\nx(t\u22121)|x(t), x(0)\u0011\f\f\f\n\f\f\fp\n\u0010\nx(t\u22121)|x(t)\u0011\u0011\n+ Hq\n\u0010\nX(T )|X(0)\u0011\n\u2212Hq\n\u0010\nX(1)|X(0)\u0011\n\u2212Hp\n\u0010\nX(T )\u0011\n.\n(14)\nwhere the entropies and KL divergences can be analyt-\nically computed.\nThe derivation of this bound parallels\nthe derivation of the log likelihood bound in variational\nBayesian methods.\nAs in Section 2.3 if the forward and reverse trajectories are\nidentical, corresponding to a quasi-static process, then the\ninequality in Equation 13 becomes an equality.\nTraining consists of \ufb01nding the reverse Markov transitions\nwhich maximize this lower bound on the log likelihood,\n\u02c6p\n\u0010\nx(t\u22121)|x(t)\u0011\n=\nargmax\np(x(t\u22121)|x(t))\nK.\n(15)\nDeep Unsupervised Learning using Nonequilibrium Thermodynamics\nThe speci\ufb01c targets of estimation for Gaussian and bino-\nmial diffusion are given in Table App.1.\nThus, the task of estimating a probability distribution has\nbeen reduced to the task of performing regression on the\nfunctions which set the mean and covariance of a sequence\nof Gaussians (or set the state \ufb02ip probability for a sequence\nof Bernoulli trials).\n2.4.1. SETTING THE DIFFUSION RATE \u03b2t\nThe choice of \u03b2t in the forward trajectory is important for\nthe performance of the trained model. In AIS, the right\nschedule of intermediate distributions can greatly improve\nthe accuracy of the log partition function estimate (Grosse\net al., 2013). In thermodynamics the schedule taken when\nmoving between equilibrium distributions determines how\nmuch free energy is lost (Spinney & Ford, 2013; Jarzynski,\n2011).\nIn the case of Gaussian diffusion, we learn2 the forward\ndiffusion schedule \u03b22\u00b7\u00b7\u00b7T by gradient ascent on K. The\nvariance \u03b21 of the \ufb01rst step is \ufb01xed to a small constant\nto prevent over\ufb01tting. The dependence of samples from\nq\n\u0000x(1\u00b7\u00b7\u00b7T )|x(0)\u0001\non \u03b21\u00b7\u00b7\u00b7T is made explicit by using \u2018frozen\nnoise\u2019 \u2013 as in (Kingma & Welling, 2013) the noise is treated\nas an additional auxiliary variable, and held constant while\ncomputing partial derivatives of K with respect to the pa-\nrameters.\nFor binomial diffusion, the discrete state space makes gra-\ndient ascent with frozen noise impossible.\nWe instead\nchoose the forward diffusion schedule \u03b21\u00b7\u00b7\u00b7T to erase a con-\nstant fraction\n1\nT of the original signal per diffusion step,\nyielding a diffusion rate of \u03b2t = (T \u2212t + 1)\u22121.\n2.5. Multiplying Distributions, and Computing\nPosteriors\nTasks such as computing a posterior in order to do signal\ndenoising or inference of missing values requires multipli-\ncation of the model distribution p\n\u0000x(0)\u0001\nwith a second dis-\ntribution, or bounded positive function, r\n\u0000x(0)\u0001\n, producing\na new distribution \u02dcp\n\u0000x(0)\u0001\n\u221dp\n\u0000x(0)\u0001\nr\n\u0000x(0)\u0001\n.\nMultiplying distributions is costly and dif\ufb01cult for many\ntechniques, including variational autoencoders, GSNs,\nNADEs, and most graphical models. However, under a dif-\nfusion model it is straightforward, since the second distri-\nbution can be treated either as a small perturbation to each\nstep in the diffusion process, or often exactly multiplied\ninto each diffusion step. Figures 3 and 5 demonstrate the\nuse of a diffusion model to perform denoising and inpaint-\ning of natural images. The following sections describe how\n2Recent experiments suggest that it is just as effective to in-\nstead use the same \ufb01xed \u03b2t schedule as for binomial diffusion.\nto multiply distributions in the context of diffusion proba-\nbilistic models.\n2.5.1. MODIFIED MARGINAL DISTRIBUTIONS\nFirst, in order to compute \u02dcp\n\u0000x(0)\u0001\n, we multiply each of\nthe intermediate distributions by a corresponding function\nr\n\u0000x(t)\u0001\n. We use a tilde above a distribution or Markov\ntransition to denote that it belongs to a trajectory that has\nbeen modi\ufb01ed in this way. \u02dcp\n\u0000x(0\u00b7\u00b7\u00b7T )\u0001\nis the modi\ufb01ed re-\nverse trajectory, which starts at the distribution \u02dcp\n\u0000x(T )\u0001\n=\n1\n\u02dc\nZT p\n\u0000x(T )\u0001\nr\n\u0000x(T )\u0001\nand proceeds through the sequence of\nintermediate distributions\n\u02dcp\n\u0010\nx(t)\u0011\n= 1\n\u02dcZt\np\n\u0010\nx(t)\u0011\nr\n\u0010\nx(t)\u0011\n,\n(16)\nwhere \u02dcZt is the normalizing constant for the tth intermedi-\nate distribution.\n2.5.2. MODIFIED DIFFUSION STEPS\nThe Markov kernel p\n\u0000x(t) | x(t+1)\u0001\nfor the reverse diffu-\nsion process obeys the equilibrium condition\np\n\u0010\nx(t\u0011\n=\nZ\ndx(t+1)p\n\u0010\nxt) | x(t+1)\u0011\np\n\u0010\nxt+1)\u0011\n.\n(17)\nWe wish the perturbed Markov kernel \u02dcp\n\u0000x(t) | x(t+1)\u0001\nto\ninstead obey the equilibrium condition for the perturbed\ndistribution,\n\u02dcp\n\u0010\nx(t)\u0011\n=\nZ\ndx(t+1)\u02dcp\n\u0010\nx(t) | x(t+1)\u0011\n\u02dcp\n\u0010\nxt+1)\u0011\n,\n(18)\np\n\u0000x(t)\u0001\nr\n\u0000x(t)\u0001\n\u02dcZt\n=\nZ\ndx(t+1)\u02dcp\n\u0010\nx(t) | x(t+1)\u0011\n\u00b7\np\n\u0000x(t+1)\u0001\nr\n\u0000x(t+1)\u0001\n\u02dcZt+1\n,\n(19)\np\n\u0010\nx(t)\u0011\n=\nZ\ndx(t+1)\u02dcp\n\u0010\nx(t) | x(t+1)\u0011\n\u00b7\n\u02dcZtr\n\u0000x(t+1)\u0001\n\u02dcZt+1r\n\u0000x(t)\u0001p\n\u0010\nx(t+1)\u0011\n.\n(20)\nEquation 20 will be satis\ufb01ed if\n\u02dcp\n\u0010\nx(t)|x(t+1)\u0011\n= p\n\u0010\nx(t)|x(t+1)\u0011 \u02dcZt+1r\n\u0000x(t)\u0001\n\u02dcZtr\n\u0000x(t+1)\u0001.\n(21)\nEquation 21 may not correspond to a normalized proba-\nbility distribution, so we choose \u02dcp\n\u0000x(t)|x(t+1)\u0001\nto be the\ncorresponding normalized distribution\n\u02dcp\n\u0010\nx(t)|x(t+1)\u0011\n=\n1\n\u02dcZt\n\u0000x(t+1)\u0001p\n\u0010\nx(t)|x(t+1)\u0011\nr\n\u0010\nx(t)\u0011\n,\n(22)\nDeep Unsupervised Learning using Nonequilibrium Thermodynamics\n(a)\n0\n50\n100\n150\n200\n250\n0\n50\n100\n150\n200\n250\n(b)\n0\n50\n100\n150\n200\n250\n0\n50\n100\n150\n200\n250\n(c)\n0\n50\n100\n150\n200\n250\n0\n50\n100\n150\n200\n250\nFigure 4. The proposed framework trained on dead leaf images (Jeulin, 1997; Lee et al., 2001). (a) Example training image. (b) A sample\nfrom the previous state of the art natural image model (Theis et al., 2012) trained on identical data, reproduced here with permission.\n(c) A sample generated by the diffusion model. Note that it demonstrates fairly consistent occlusion relationships, displays a multiscale\ndistribution over object sizes, and produces circle-like objects, especially at smaller scales. As shown in Table 2, the diffusion model has\nthe highest log likelihood on the test set.\nwhere \u02dcZt\n\u0000x(t+1)\u0001\nis the normalization constant.\nFor a Gaussian, each diffusion step is typically very sharply\npeaked relative to r\n\u0000x(t)\u0001\n, due to its small variance. This\nmeans that\nr(x(t))\nr(x(t+1)) can be treated as a small perturbation\nto p\n\u0000x(t)|x(t+1)\u0001\n. A small perturbation to a Gaussian ef-\nfects the mean, but not the normalization constant, so in\nthis case Equations 21 and 22 are equivalent (see Appendix\nC).\n2.5.3. APPLYING r\n\u0000x(t)\u0001\nIf r\n\u0000x(t)\u0001\nis suf\ufb01ciently smooth, then it can be treated\nas a small perturbation to the reverse diffusion kernel\np\n\u0000x(t)|x(t+1)\u0001\n. In this case \u02dcp\n\u0000x(t)|x(t+1)\u0001\nwill have an\nidentical functional form to p\n\u0000x(t)|x(t+1)\u0001\n, but with per-\nturbed mean for the Gaussian kernel, or with perturbed \ufb02ip\nrate for the binomial kernel. The perturbed diffusion ker-\nnels are given in Table App.1, and are derived for the Gaus-\nsian in Appendix C.\nIf r\n\u0000x(t)\u0001\ncan be multiplied with a Gaussian (or binomial)\ndistribution in closed form, then it can be directly multi-\nplied with the reverse diffusion kernel p\n\u0000x(t)|x(t+1)\u0001\nin\nclosed form. This applies in the case where r\n\u0000x(t)\u0001\ncon-\nsists of a delta function for some subset of coordinates, as\nin the inpainting example in Figure 5.\n2.5.4. CHOOSING r\n\u0000x(t)\u0001\nTypically, r\n\u0000x(t)\u0001\nshould be chosen to change slowly over\nthe course of the trajectory. For the experiments in this\npaper we chose it to be constant,\nr\n\u0010\nx(t)\u0011\n= r\n\u0010\nx(0)\u0011\n.\n(23)\nAnother convenient choice is r\n\u0000x(t)\u0001\n= r\n\u0000x(0)\u0001 T \u2212t\nT . Un-\nder this second choice r\n\u0000x(t)\u0001\nmakes no contribution to the\nstarting distribution for the reverse trajectory. This guaran-\ntees that drawing the initial sample from \u02dcp\n\u0000x(T )\u0001\nfor the\nreverse trajectory remains straightforward.\n2.6. Entropy of Reverse Process\nSince the forward process is known, we can derive upper\nand lower bounds on the conditional entropy of each step\nin the reverse trajectory, and thus on the log likelihood,\nHq\n\u0010\nX(t)|X(t\u22121)\u0011\n+ Hq\n\u0010\nX(t\u22121)|X(0)\u0011\n\u2212Hq\n\u0010\nX(t)|X(0)\u0011\n\u2264Hq\n\u0010\nX(t\u22121)|X(t)\u0011\n\u2264Hq\n\u0010\nX(t)|X(t\u22121)\u0011\n,\n(24)\nwhere both the upper and lower bounds depend only on\nq\n\u0000x(1\u00b7\u00b7\u00b7T )|x(0)\u0001\n, and can be analytically computed. The\nderivation is provided in Appendix A.\n3. Experiments\nWe train diffusion probabilistic models on a variety of con-\ntinuous datasets, and a binary dataset. We then demonstrate\nsampling from the trained model and inpainting of miss-\ning data, and compare model performance against other\ntechniques. In all cases the objective function and gradi-\nent were computed using Theano (Bergstra & Breuleux,\n2010). Model training was with SFO (Sohl-Dickstein et al.,\n2014), except for CIFAR-10. CIFAR-10 results used the\n3 An earlier version of this paper reported higher log likeli-\nhood bounds on CIFAR-10. These were the result of the model\nlearning the 8-bit quantization of pixel values in the CIFAR-10\ndataset. The log likelihood bounds reported here are instead for\ndata that has been pre-processed by adding uniform noise to re-\nmove pixel quantization, as recommended in (Theis et al., 2015).\nDeep Unsupervised Learning using Nonequilibrium Thermodynamics\n(a)\n0\n50\n100\n150\n200\n250\n300\n0\n50\n100\n150\n200\n250\n300\n(b)\n0\n50\n100\n150\n200\n250\n300\n0\n50\n100\n150\n200\n250\n300\n(c)\n0\n50\n100\n150\n200\n250\n300\n0\n50\n100\n150\n200\n250\n300\nFigure 5. Inpainting. (a) A bark image from (Lazebnik et al., 2005). (b) The same image with the central 100\u00d7100 pixel region replaced\nwith isotropic Gaussian noise. This is the initialization \u02dcp\n\u0010\nx(T )\u0011\nfor the reverse trajectory. (c) The central 100\u00d7100 region has been\ninpainted using a diffusion probabilistic model trained on images of bark, by sampling from the posterior distribution over the missing\nregion conditioned on the rest of the image. Note the long-range spatial structure, for instance in the crack entering on the left side of the\ninpainted region. The sample from the posterior was generated as described in Section 2.5, where r\n\u0010\nx(0)\u0011\nwas set to a delta function\nfor known data, and a constant for missing data.\nDataset\nK\nK \u2212Lnull\nSwiss Roll\n2.35 bits\n6.45 bits\nBinary Heartbeat\n-2.414 bits/seq.\n12.024 bits/seq.\nBark\n-0.55 bits/pixel\n1.5 bits/pixel\nDead Leaves\n1.489 bits/pixel\n3.536 bits/pixel\nCIFAR-103\n5.4 \u00b1 0.2 bits/pixel\n11.5 \u00b1 0.2 bits/pixel\nMNIST\nSee table 2\nTable 1. The lower bound K on the log likelihood, computed on a\nholdout set, for each of the trained models. See Equation 12. The\nright column is the improvement relative to an isotropic Gaussian\nor independent binomial distribution. Lnull is the log likelihood\nof \u03c0\n\u0010\nx(0)\u0011\n. All datasets except for Binary Heartbeat were scaled\nby a constant to give them variance 1 before computing log like-\nlihood.\nopen source implementation of the algorithm, and RM-\nSprop for optimization. The lower bound on the log like-\nlihood provided by our model is reported for all datasets\nin Table 1. A reference implementation of the algorithm\nutilizing Blocks (van Merri\u00a8enboer et al., 2015) is avail-\nable at https://github.com/Sohl-Dickstein/\nDiffusion-Probabilistic-Models.\n3.1. Toy Problems\n3.1.1. SWISS ROLL\nA diffusion probabilistic model was built of a two dimen-\nsional swiss roll distribution, using a radial basis function\nnetwork to generate f\u00b5\n\u0000x(t), t\n\u0001\nand f\u03a3\n\u0000x(t), t\n\u0001\n. As illus-\ntrated in Figure 1, the swiss roll distribution was success-\nfully learned. See Appendix Section D.1.1 for more details.\nModel\nLog Likelihood\nDead Leaves\nMCGSM\n1.244 bits/pixel\nDiffusion\n1.489 bits/pixel\nMNIST\nStacked CAE\n174 \u00b1 2.3 bits\nDBN\n199 \u00b1 2.9 bits\nDeep GSN\n309 \u00b1 1.6 bits\nDiffusion\n317 \u00b1 2.7 bits\nAdversarial net\n325 \u00b1 2.9 bits\nPerfect model\n349 \u00b1 3.3 bits\nTable 2. Log likelihood comparisons to other algorithms. Dead\nleaves images were evaluated using identical training and test data\nas in (Theis et al., 2012). MNIST log likelihoods were estimated\nusing the Parzen-window code from (Goodfellow et al., 2014),\nwith values given in bits, and show that our performance is com-\nparable to other recent techniques. The perfect model entry was\ncomputed by applying the Parzen code to samples from the train-\ning data.\n3.1.2. BINARY HEARTBEAT DISTRIBUTION\nA diffusion probabilistic model was trained on simple bi-\nnary sequences of length 20, where a 1 occurs every 5th\ntime bin, and the remainder of the bins are 0, using a multi-\nlayer perceptron to generate the Bernoulli rates fb\n\u0000x(t), t\n\u0001\nof the reverse trajectory. The log likelihood under the true\ndistribution is log2\n\u0000 1\n5\n\u0001\n= \u22122.322 bits per sequence. As\ncan be seen in Figure 2 and Table 1 learning was nearly\nperfect. See Appendix Section D.1.2 for more details.\n3.2. Images\nWe trained Gaussian diffusion probabilistic models on sev-\neral image datasets. The multi-scale convolutional archi-\nDeep Unsupervised Learning using Nonequilibrium Thermodynamics\ntecture shared by these experiments is described in Ap-\npendix Section D.2.1, and illustrated in Figure D.1.\n3.2.1. DATASETS\nMNIST\nIn order to allow a direct comparison against\nprevious work on a simple dataset, we trained on MNIST\ndigits (LeCun & Cortes, 1998). Log likelihoods relative to\n(Bengio et al., 2012; Bengio & Thibodeau-Laufer, 2013;\nGoodfellow et al., 2014) are given in Table 2. Samples\nfrom the MNIST model are given in Appendix Figure\nApp.1. Our training algorithm provides an asymptotically\nconsistent lower bound on the log likelihood.\nHowever\nmost previous reported results on continuous MNIST log\nlikelihood rely on Parzen-window based estimates com-\nputed from model samples. For this comparison we there-\nfore estimate MNIST log likelihood using the Parzen-\nwindow code released with (Goodfellow et al., 2014).\nCIFAR-10\nA probabilistic model was \ufb01t to the training\nimages for the CIFAR-10 challenge dataset (Krizhevsky &\nHinton, 2009). Samples from the trained model are pro-\nvided in Figure 3.\nDead Leaf Images\nDead leaf images (Jeulin, 1997; Lee\net al., 2001) consist of layered occluding circles, drawn\nfrom a power law distribution over scales. They have an an-\nalytically tractable structure, but capture many of the statis-\ntical complexities of natural images, and therefore provide\na compelling test case for natural image models. As illus-\ntrated in Table 2 and Figure 4, we achieve state of the art\nperformance on the dead leaves dataset.\nBark Texture Images\nA probabilistic model was trained\non bark texture images (T01-T04) from (Lazebnik et al.,\n2005). For this dataset we demonstrate that it is straightfor-\nward to evaluate or generate from a posterior distribution,\nby inpainting a large region of missing data using a sample\nfrom the model posterior in Figure 5.\n4. Conclusion\nWe have introduced a novel algorithm for modeling proba-\nbility distributions that enables exact sampling and evalua-\ntion of probabilities and demonstrated its effectiveness on a\nvariety of toy and real datasets, including challenging natu-\nral image datasets. For each of these tests we used a similar\nbasic algorithm, showing that our method can accurately\nmodel a wide variety of distributions. Most existing den-\nsity estimation techniques must sacri\ufb01ce modeling power\nin order to stay tractable and ef\ufb01cient, and sampling or\nevaluation are often extremely expensive. The core of our\nalgorithm consists of estimating the reversal of a Markov\ndiffusion chain which maps data to a noise distribution; as\nthe number of steps is made large, the reversal distribution\nof each diffusion step becomes simple and easy to estimate.\nThe result is an algorithm that can learn a \ufb01t to any data dis-\ntribution, but which remains tractable to train, exactly sam-\nple from, and evaluate, and under which it is straightfor-\nward to manipulate conditional and posterior distributions.\nAcknowledgements\nWe thank Lucas Theis, Subhaneil Lahiri, Ben Poole,\nDiederik P. Kingma, Taco Cohen, Philip Bachman, and\nA\u00a8aron van den Oord for extremely helpful discussion, and\nIan Goodfellow for Parzen-window code. We thank Khan\nAcademy and the Of\ufb01ce of Naval Research for funding\nJascha Sohl-Dickstein, and we thank the Of\ufb01ce of Naval\nResearch and the Burroughs-Wellcome, Sloan, and James\nS. McDonnell foundations for funding Surya Ganguli.\nReferences\nBarron, J. T., Biggin, M. D., Arbelaez, P., Knowles, D. W.,\nKeranen, S. V., and Malik, J. Volumetric Semantic Seg-\nmentation Using Pyramid Context Features.\nIn 2013\nIEEE International Conference on Computer Vision, pp.\n3448\u20133455. IEEE, December 2013. ISBN 978-1-4799-\n2840-8. doi: 10.1109/ICCV.2013.428.\nBengio, Y. and Thibodeau-Laufer, E.\nDeep genera-\ntive stochastic networks trainable by backprop. arXiv\npreprint arXiv:1306.1091, 2013.\nBengio, Y., Mesnil, G., Dauphin, Y., and Rifai, S. Bet-\nter Mixing via Deep Representations.\narXiv preprint\narXiv:1207.4404, July 2012.\nBergstra, J. and Breuleux, O. Theano: a CPU and GPU\nmath expression compiler. Proceedings of the Python\nfor Scienti\ufb01c Computing Conference (SciPy), 2010.\nBesag, J. Statistical Analysis of Non-Lattice Data. The\nStatistician, 24(3), 179-195, 1975.\nBishop, C., Svens\u00b4en, M., and Williams, C. GTM: The gen-\nerative topographic mapping. Neural computation, 1998.\nBornschein, J. and Bengio, Y.\nReweighted Wake-Sleep.\nInternational Conference on Learning Representations,\nJune 2015.\nBurda, Y., Grosse, R. B., and Salakhutdinov, R.\nAccu-\nrate and Conservative Estimates of MRF Log-likelihood\nusing Reverse Annealing. arXiv:1412.8566, December\n2014.\nDayan, P., Hinton, G. E., Neal, R. M., and Zemel, R. S. The\nhelmholtz machine. Neural computation, 7(5):889\u2013904,\n1995.\nDeep Unsupervised Learning using Nonequilibrium Thermodynamics\nDinh, L., Krueger, D., and Bengio, Y. NICE: Non-linear\nIndependent Components Estimation. arXiv:1410.8516,\npp. 11, October 2014.\nFeller, W. On the theory of stochastic processes, with par-\nticular reference to applications. In Proceedings of the\n[First] Berkeley Symposium on Mathematical Statistics\nand Probability. The Regents of the University of Cali-\nfornia, 1949.\nGershman, S. J. and Blei, D. M. A tutorial on Bayesian\nnonparametric models.\nJournal of Mathematical Psy-\nchology, 56(1):1\u201312, 2012.\nGneiting, T. and Raftery, A. E. Strictly proper scoring rules,\nprediction, and estimation. Journal of the American Sta-\ntistical Association, 102(477):359\u2013378, 2007.\nGoodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B.,\nWarde-Farley, D., Ozair, S., Courville, A., and Bengio,\nY.\nGenerative Adversarial Nets.\nAdvances in Neural\nInformation Processing Systems, 2014.\nGregor, K., Danihelka, I., Mnih, A., Blundell, C., and Wier-\nstra, D. Deep AutoRegressive Networks. arXiv preprint\narXiv:1310.8499, October 2013.\nGrosse, R. B., Maddison, C. J., and Salakhutdinov, R. An-\nnealing between distributions by averaging moments. In\nAdvances in Neural Information Processing Systems, pp.\n2769\u20132777, 2013.\nHinton, G. E. Training products of experts by minimiz-\ning contrastive divergence. Neural Computation, 14(8):\n1771\u20131800, 2002.\nHinton, G. E. The wake-sleep algorithm for unsupervised\nneural networks ). Science, 1995.\nHyv\u00a8arinen, A.\nEstimation of non-normalized statistical\nmodels using score matching.\nJournal of Machine\nLearning Research, 6:695\u2013709, 2005.\nJarzynski, C.\nEquilibrium free-energy differences from\nnonequilibrium measurements: A master-equation ap-\nproach. Physical Review E, January 1997.\nJarzynski, C.\nEqualities and inequalities: irreversibility\nand the second law of thermodynamics at the nanoscale.\nAnnu. Rev. Condens. Matter Phys., 2011.\nJeulin, D. Dead leaves models: from space tesselation to\nrandom functions. Proc. of the Symposium on the Ad-\nvances in the Theory and Applications of Random Sets,\n1997.\nJordan, M. I., Ghahramani, Z., Jaakkola, T. S., and Saul,\nL. K. An introduction to variational methods for graphi-\ncal models. Machine learning, 37(2):183\u2013233, 1999.\nKavukcuoglu, K., Ranzato, M., and LeCun, Y. Fast infer-\nence in sparse coding algorithms with applications to ob-\nject recognition. arXiv preprint arXiv:1010.3467, 2010.\nKingma, D. P. and Welling, M. Auto-Encoding Variational\nBayes. International Conference on Learning Represen-\ntations, December 2013.\nKrizhevsky, A. and Hinton, G. Learning multiple layers of\nfeatures from tiny images. Computer Science Depart-\nment University of Toronto Tech. Rep., 2009.\nLangevin, P. Sur la th\u00b4eorie du mouvement brownien. CR\nAcad. Sci. Paris, 146(530-533), 1908.\nLarochelle, H. and Murray, I. The neural autoregressive\ndistribution estimator. Journal of Machine Learning Re-\nsearch, 2011.\nLazebnik, S., Schmid, C., and Ponce, J. A sparse texture\nrepresentation using local af\ufb01ne regions. Pattern Analy-\nsis and Machine Intelligence, IEEE Transactions on, 27\n(8):1265\u20131278, 2005.\nLeCun, Y. and Cortes, C. The MNIST database of hand-\nwritten digits. 1998.\nLee, A., Mumford, D., and Huang, J. Occlusion models for\nnatural images: A statistical study of a scale-invariant\ndead leaves model. International Journal of Computer\nVision, 2001.\nLyu, S. Unifying Non-Maximum Likelihood Learning Ob-\njectives with Minimum KL Contraction.\nAdvances in\nNeural Information Processing Systems 24, pp. 64\u201372,\n2011.\nMacKay, D.\nBayesian neural networks and density net-\nworks. Nuclear Instruments and Methods in Physics Re-\nsearch Section A: Accelerators, Spectrometers, Detec-\ntors and Associated Equipment, 1995.\nMurphy, K. P., Weiss, Y., and Jordan, M. I.\nLoopy be-\nlief propagation for approximate inference: An empiri-\ncal study. In Proceedings of the Fifteenth conference on\nUncertainty in arti\ufb01cial intelligence, pp. 467\u2013475. Mor-\ngan Kaufmann Publishers Inc., 1999.\nNeal, R. Annealed importance sampling. Statistics and\nComputing, January 2001.\nOzair, S. and Bengio, Y. Deep Directed Generative Au-\ntoencoders. arXiv:1410.0630, October 2014.\nParry, M., Dawid, A. P., Lauritzen, S., and Others. Proper\nlocal scoring rules. The Annals of Statistics, 40(1):561\u2013\n592, 2012.\nDeep Unsupervised Learning using Nonequilibrium Thermodynamics\nRezende, D. J., Mohamed, S., and Wierstra, D. Stochas-\ntic Backpropagation and Approximate Inference in Deep\nGenerative Models.\nProceedings of the 31st Inter-\nnational Conference on Machine Learning (ICML-14),\nJanuary 2014.\nRippel,\nO. and Adams,\nR. P.\nHigh-Dimensional\nProbability Estimation with Deep Density Models.\narXiv:1410.8516, pp. 12, February 2013.\nSchmidhuber, J. Learning factorial codes by predictability\nminimization. Neural Computation, 1992.\nSminchisescu, C., Kanaujia, A., and Metaxas, D. Learning\njoint top-down and bottom-up processes for 3D visual\ninference. In Computer Vision and Pattern Recognition,\n2006 IEEE Computer Society Conference on, volume 2,\npp. 1743\u20131752. IEEE, 2006.\nSohl-Dickstein, J., Battaglino, P., and DeWeese, M. New\nMethod for Parameter Estimation in Probabilistic Mod-\nels: Minimum Probability Flow. Physical Review Let-\nters, 107(22):11\u201314, November 2011a.\nISSN 0031-\n9007. doi: 10.1103/PhysRevLett.107.220601.\nSohl-Dickstein,\nJ.,\nBattaglino,\nP. B.,\nand DeWeese,\nM. R. Minimum Probability Flow Learning. Interna-\ntional Conference on Machine Learning, 107(22):11\u2013\n14, November 2011b. ISSN 0031-9007. doi: 10.1103/\nPhysRevLett.107.220601.\nSohl-Dickstein, J., Poole, B., and Ganguli, S. Fast large-\nscale optimization by unifying stochastic gradient and\nquasi-Newton methods. In Proceedings of the 31st Inter-\nnational Conference on Machine Learning (ICML-14),\npp. 604\u2013612, 2014.\nSpinney, R. and Ford, I. Fluctuation Relations : A Peda-\ngogical Overview. arXiv preprint arXiv:1201.6381, pp.\n3\u201356, 2013.\nStuhlm\u00a8uller, A., Taylor, J., and Goodman, N.\nLearning\nstochastic inverses.\nAdvances in Neural Information\nProcessing Systems, 2013.\nSuykens, J. and Vandewalle, J. Nonconvex optimization\nusing a Fokker-Planck learning machine. In 12th Euro-\npean Conference on Circuit Theory and Design, 1995.\nT, P. Convergence condition of the TAP equation for the\nin\ufb01nite-ranged Ising spin glass model. J. Phys. A: Math.\nGen. 15 1971, 1982.\nTanaka, T. Mean-\ufb01eld theory of Boltzmann machine learn-\ning. Physical Review Letters E, January 1998.\nTheis, L., Hosseini, R., and Bethge, M. Mixtures of condi-\ntional Gaussian scale mixtures applied to multiscale im-\nage representations. PloS one, 7(7):e39857, 2012.\nTheis, L., van den Oord, A., and Bethge, M.\nA note\non the evaluation of generative models. arXiv preprint\narXiv:1511.01844, 2015.\nUria, B., Murray, I., and Larochelle, H.\nRNADE:\nThe real-valued neural autoregressive density-estimator.\nAdvances in Neural Information Processing Systems,\n2013a.\nUria, B., Murray, I., and Larochelle, H.\nA Deep and\nTractable Density Estimator. arXiv:1310.1757, pp. 9,\nOctober 2013b.\nvan Merri\u00a8enboer, B., Chorowski, J., Serdyuk, D., Bengio,\nY., Bogdanov, D., Dumoulin, V., and Warde-Farley, D.\nBlocks and Fuel.\nZenodo, May 2015.\ndoi: 10.5281/\nzenodo.17721.\nWelling, M. and Hinton, G. A new learning algorithm for\nmean \ufb01eld Boltzmann machines. Lecture Notes in Com-\nputer Science, January 2002.\nYao, L., Ozair, S., Cho, K., and Bengio, Y. On the Equiv-\nalence Between Deep NADE and Generative Stochastic\nNetworks. In Machine Learning and Knowledge Discov-\nery in Databases, pp. 322\u2013336. Springer, 2014.\nDeep Unsupervised Learning using Nonequilibrium Thermodynamics\nAppendix\nA. Conditional Entropy Bounds Derivation\nThe conditional entropy Hq\n\u0000X(t\u22121)|X(t)\u0001\nof a step in the reverse trajectory is\nHq\n\u0010\nX(t\u22121), X(t)\u0011\n= Hq\n\u0010\nX(t), X(t\u22121)\u0011\n(25)\nHq\n\u0010\nX(t\u22121)|X(t)\u0011\n+ Hq\n\u0010\nX(t)\u0011\n= Hq\n\u0010\nX(t)|X(t\u22121)\u0011\n+ Hq\n\u0010\nX(t\u22121)\u0011\n(26)\nHq\n\u0010\nX(t\u22121)|X(t)\u0011\n= Hq\n\u0010\nX(t)|X(t\u22121)\u0011\n+ Hq\n\u0010\nX(t\u22121)\u0011\n\u2212Hq\n\u0010\nX(t)\u0011\n(27)\nAn upper bound on the entropy change can be constructed by observing that \u03c0 (y) is the maximum entropy distribution.\nThis holds without quali\ufb01cation for the binomial distribution, and holds for variance 1 training data for the Gaussian case.\nFor the Gaussian case, training data must therefore be scaled to have unit norm for the following equalities to hold. It need\nnot be whitened. The upper bound is derived as follows,\nHq\n\u0010\nX(t)\u0011\n\u2265Hq\n\u0010\nX(t\u22121)\u0011\n(28)\nHq\n\u0010\nX(t\u22121)\u0011\n\u2212Hq\n\u0010\nX(t)\u0011\n\u22640\n(29)\nHq\n\u0010\nX(t\u22121)|X(t)\u0011\n\u2264Hq\n\u0010\nX(t)|X(t\u22121)\u0011\n.\n(30)\nA lower bound on the entropy difference can be established by observing that additional steps in a Markov chain do not\nincrease the information available about the initial state in the chain, and thus do not decrease the conditional entropy of\nthe initial state,\nHq\n\u0010\nX(0)|X(t)\u0011\n\u2265Hq\n\u0010\nX(0)|X(t\u22121)\u0011\n(31)\nHq\n\u0010\nX(t\u22121)\u0011\n\u2212Hq\n\u0010\nX(t)\u0011\n\u2265Hq\n\u0010\nX(0)|X(t\u22121)\u0011\n+ Hq\n\u0010\nX(t\u22121)\u0011\n\u2212Hq\n\u0010\nX(0)|X(t)\u0011\n\u2212Hq\n\u0010\nX(t)\u0011\n(32)\nHq\n\u0010\nX(t\u22121)\u0011\n\u2212Hq\n\u0010\nX(t)\u0011\n\u2265Hq\n\u0010\nX(0), X(t\u22121)\u0011\n\u2212Hq\n\u0010\nX(0), X(t)\u0011\n(33)\nHq\n\u0010\nX(t\u22121)\u0011\n\u2212Hq\n\u0010\nX(t)\u0011\n\u2265Hq\n\u0010\nX(t\u22121)|X(0)\u0011\n\u2212Hq\n\u0010\nX(t)|X(0)\u0011\n(34)\nHq\n\u0010\nX(t\u22121)|X(t)\u0011\n\u2265Hq\n\u0010\nX(t)|X(t\u22121)\u0011\n+ Hq\n\u0010\nX(t\u22121)|X(0)\u0011\n\u2212Hq\n\u0010\nX(t)|X(0)\u0011\n.\n(35)\nCombining these expressions, we bound the conditional entropy for a single step,\nHq\n\u0010\nX(t)|X(t\u22121)\u0011\n\u2265Hq\n\u0010\nX(t\u22121)|X(t)\u0011\n\u2265Hq\n\u0010\nX(t)|X(t\u22121)\u0011\n+ Hq\n\u0010\nX(t\u22121)|X(0)\u0011\n\u2212Hq\n\u0010\nX(t)|X(0)\u0011\n,\n(36)\nwhere both the upper and lower bounds depend only on the conditional forward trajectory q\n\u0000x(1\u00b7\u00b7\u00b7T )|x(0)\u0001\n, and can be\nanalytically computed.\nB. Log Likelihood Lower Bound\nThe lower bound on the log likelihood is\nL \u2265K\n(37)\nK =\nZ\ndx(0\u00b7\u00b7\u00b7T )q\n\u0010\nx(0\u00b7\u00b7\u00b7T )\u0011\nlog\n\"\np\n\u0010\nx(T )\u0011 T\nY\nt=1\np\n\u0000x(t\u22121)|x(t)\u0001\nq\n\u0000x(t)|x(t\u22121)\u0001\n#\n(38)\n(39)\nDeep Unsupervised Learning using Nonequilibrium Thermodynamics\nB.1. Entropy of p\n\u0000X(T )\u0001\nWe can peel off the contribution from p\n\u0000X(T )\u0001\n, and rewrite it as an entropy,\nK =\nZ\ndx(0\u00b7\u00b7\u00b7T )q\n\u0010\nx(0\u00b7\u00b7\u00b7T )\u0011\nT\nX\nt=1\nlog\n\"\np\n\u0000x(t\u22121)|x(t)\u0001\nq\n\u0000x(t)|x(t\u22121)\u0001\n#\n+\nZ\ndx(T )q\n\u0010\nx(T )\u0011\nlog p\n\u0010\nx(T )\u0011\n(40)\n=\nZ\ndx(0\u00b7\u00b7\u00b7T )q\n\u0010\nx(0\u00b7\u00b7\u00b7T )\u0011\nT\nX\nt=1\nlog\n\"\np\n\u0000x(t\u22121)|x(t)\u0001\nq\n\u0000x(t)|x(t\u22121)\u0001\n#\n+\nZ\ndx(T )q\n\u0010\nx(T )\u0011\nlog \u03c0\n\u0000xT \u0001\n(41)\n.\n(42)\nBy design, the cross entropy to \u03c0\n\u0000x(t)\u0001\nis constant under our diffusion kernels, and equal to the entropy of p\n\u0000x(T )\u0001\n.\nTherefore,\nK =\nT\nX\nt=1\nZ\ndx(0\u00b7\u00b7\u00b7T )q\n\u0010\nx(0\u00b7\u00b7\u00b7T )\u0011\nlog\n\"\np\n\u0000x(t\u22121)|x(t)\u0001\nq\n\u0000x(t)|x(t\u22121)\u0001\n#\n\u2212Hp\n\u0010\nX(T )\u0011\n.\n(43)\nB.2. Remove the edge effect at t = 0\nIn order to avoid edge effects, we set the \ufb01nal step of the reverse trajectory to be identical to the corresponding forward\ndiffusion step,\np\n\u0010\nx(0)|x(1)\u0011\n= q\n\u0010\nx(1)|x(0)\u0011 \u03c0\n\u0000x(0)\u0001\n\u03c0\n\u0000x(1)\u0001 = T\u03c0\n\u0010\nx(0)|x(1); \u03b21\n\u0011\n.\n(44)\nWe then use this equivalence to remove the contribution of the \ufb01rst time-step in the sum,\nK =\nT\nX\nt=2\nZ\ndx(0\u00b7\u00b7\u00b7T )q\n\u0010\nx(0\u00b7\u00b7\u00b7T )\u0011\nlog\n\"\np\n\u0000x(t\u22121)|x(t)\u0001\nq\n\u0000x(t)|x(t\u22121)\u0001\n#\n+\nZ\ndx(0)dx(1)q\n\u0010\nx(0), x(1)\u0011\nlog\n\"\nq\n\u0000x(1)|x(0)\u0001\n\u03c0\n\u0000x(0)\u0001\nq\n\u0000x(1)|x(0)\u0001\n\u03c0\n\u0000x(1)\u0001\n#\n\u2212Hp\n\u0010\nX(T )\u0011\n(45)\n=\nT\nX\nt=2\nZ\ndx(0\u00b7\u00b7\u00b7T )q\n\u0010\nx(0\u00b7\u00b7\u00b7T )\u0011\nlog\n\"\np\n\u0000x(t\u22121)|x(t)\u0001\nq\n\u0000x(t)|x(t\u22121)\u0001\n#\n\u2212Hp\n\u0010\nX(T )\u0011\n,\n(46)\nwhere we again used the fact that by design \u2212\nR\ndx(t)q\n\u0000x(t)\u0001\nlog \u03c0\n\u0000x(t)\u0001\n= Hp\n\u0000X(T )\u0001\nis a constant for all t.\nB.3. Rewrite in terms of posterior q\n\u0000x(t\u22121)|x(0)\u0001\nBecause the forward trajectory is a Markov process,\nK =\nT\nX\nt=2\nZ\ndx(0\u00b7\u00b7\u00b7T )q\n\u0010\nx(0\u00b7\u00b7\u00b7T )\u0011\nlog\n\"\np\n\u0000x(t\u22121)|x(t)\u0001\nq\n\u0000x(t)|x(t\u22121), x(0)\u0001\n#\n\u2212Hp\n\u0010\nX(T )\u0011\n.\n(47)\nUsing Bayes\u2019 rule we can rewrite this in terms of a posterior and marginals from the forward trajectory,\nK =\nT\nX\nt=2\nZ\ndx(0\u00b7\u00b7\u00b7T )q\n\u0010\nx(0\u00b7\u00b7\u00b7T )\u0011\nlog\n\"\np\n\u0000x(t\u22121)|x(t)\u0001\nq\n\u0000x(t\u22121)|x(t), x(0)\u0001 q\n\u0000x(t\u22121)|x(0)\u0001\nq\n\u0000x(t)|x(0)\u0001\n#\n\u2212Hp\n\u0010\nX(T )\u0011\n.\n(48)\nDeep Unsupervised Learning using Nonequilibrium Thermodynamics\nFigure App.1. Samples from a diffusion probabilistic model trained on MNIST digits. Note that unlike many MNIST sample \ufb01gures,\nthese are true samples rather than the mean of the Gaussian or binomial distribution from which samples would be drawn.\nB.4. Rewrite in terms of KL divergences and entropies\nWe then recognize that several terms are conditional entropies,\nK =\nT\nX\nt=2\nZ\ndx(0\u00b7\u00b7\u00b7T )q\n\u0010\nx(0\u00b7\u00b7\u00b7T )\u0011\nlog\n\"\np\n\u0000x(t\u22121)|x(t)\u0001\nq\n\u0000x(t\u22121)|x(t), x(0)\u0001\n#\n+\nT\nX\nt=2\nh\nHq\n\u0010\nX(t)|X(0)\u0011\n\u2212Hq\n\u0010\nX(t\u22121)|X(0)\u0011i\n\u2212Hp\n\u0010\nX(T )\u0011\n(49)\n=\nT\nX\nt=2\nZ\ndx(0\u00b7\u00b7\u00b7T )q\n\u0010\nx(0\u00b7\u00b7\u00b7T )\u0011\nlog\n\"\np\n\u0000x(t\u22121)|x(t)\u0001\nq\n\u0000x(t\u22121)|x(t), x(0)\u0001\n#\n+ Hq\n\u0010\nX(T )|X(0)\u0011\n\u2212Hq\n\u0010\nX(1)|X(0)\u0011\n\u2212Hp\n\u0010\nX(T )\u0011\n.\n(50)\nFinally we transform the log ratio of probability distributions into a KL divergence,\nK = \u2212\nT\nX\nt=2\nZ\ndx(0)dx(t)q\n\u0010\nx(0), x(t)\u0011\nDKL\n\u0010\nq\n\u0010\nx(t\u22121)|x(t), x(0)\u0011\f\f\f\n\f\f\fp\n\u0010\nx(t\u22121)|x(t)\u0011\u0011\n(51)\n+ Hq\n\u0010\nX(T )|X(0)\u0011\n\u2212Hq\n\u0010\nX(1)|X(0)\u0011\n\u2212Hp\n\u0010\nX(T )\u0011\n.\nNote that the entropies can be analytically computed, and the KL divergence can be analytically computed given x(0) and\nx(t).\nGaussian\nBinomial\nWell\nbehaved\n(analytically\ntractable) distribution\n\u03c0\n\u0000x(T )\u0001\n=\nN\n\u0000x(T ); 0, I\n\u0001\nB\n\u0000x(T ); 0.5\n\u0001\nForward diffusion kernel\nq\n\u0000x(t)|x(t\u22121)\u0001\n=\nN\n\u0000x(t); x(t\u22121)\u221a1 \u2212\u03b2t, I\u03b2t\n\u0001\nB\n\u0000x(t); x(t\u22121) (1 \u2212\u03b2t) + 0.5\u03b2t\n\u0001\nReverse diffusion kernel\np\n\u0000x(t\u22121)|x(t)\u0001\n=\nN\n\u0000x(t\u22121); f\u00b5\n\u0000x(t), t\n\u0001\n, f\u03a3\n\u0000x(t), t\n\u0001\u0001\nB\n\u0000x(t\u22121); fb\n\u0000x(t), t\n\u0001\u0001\nTraining targets\nf\u00b5\n\u0000x(t), t\n\u0001\n, f\u03a3\n\u0000x(t), t\n\u0001\n, \u03b21\u00b7\u00b7\u00b7T\nfb\n\u0000x(t), t\n\u0001\nForward distribution\nq\n\u0000x(0\u00b7\u00b7\u00b7T )\u0001\n=\nq\n\u0000x(0)\u0001 QT\nt=1 q\n\u0000x(t)|x(t\u22121)\u0001\nReverse distribution\np\n\u0000x(0\u00b7\u00b7\u00b7T )\u0001\n=\n\u03c0\n\u0000x(T )\u0001 QT\nt=1 p\n\u0000x(t\u22121)|x(t)\u0001\nLog likelihood\nL =\nR\ndx(0)q\n\u0000x(0)\u0001\nlog p\n\u0000x(0)\u0001\nLower bound on log likelihood\nK =\n\u2212PT\nt=2 Eq(x(0),x(t))\n\u0002\nDKL\n\u0000q\n\u0000x(t\u22121)|x(t), x(0)\u0001\f\f\f\fp\n\u0000x(t\u22121)|x(t)\u0001\u0001\u0003\n+ Hq\n\u0000X(T )|X(0)\u0001\n\u2212Hq\n\u0000X(1)|X(0)\u0001\n\u2212Hp\n\u0000X(T )\u0001\nPerturbed reverse diffusion kernel\n\u02dcp\n\u0000x(t\u22121)|x(t)\u0001\n=\nN\n \nx(t\u22121); f\u00b5\n\u0000x(t), t\n\u0001\n+ f\u03a3\n\u0000x(t), t\n\u0001 \u2202log r\n\u0010\nx(t\u22121)\u2032\u0011\n\u2202x(t\u22121)\u2032\n\f\f\f\f\nx(t\u22121)\u2032=f\u00b5(x(t),t)\n, f\u03a3\n\u0000x(t), t\n\u0001\n!\nB\n\u0010\nx(t\u22121)\ni\n;\nct\u22121\ni\ndt\u22121\ni\nxt\u22121\ni\ndt\u22121\ni\n+(1\u2212ct\u22121\ni\n)(1\u2212dt\u22121\ni\n)\n\u0011\nTable App.1. The key equations in this paper for the speci\ufb01c cases of Gaussian and binomial diffusion processes. N (u; \u00b5, \u03a3) is a Gaussian distribution with mean \u00b5 and covariance\n\u03a3. B (u; r) is the distribution for a single Bernoulli trial, with u = 1 occurring with probability r, and u = 0 occurring with probability 1 \u2212r. Finally, for the perturbed Bernoulli\ntrials bt\ni = x(t\u22121) (1 \u2212\u03b2t) + 0.5\u03b2t, ct\ni =\nh\nfb\n\u0010\nx(t+1), t\n\u0011i\ni, and dt\ni = r\n\u0010\nx(t)\ni\n= 1\n\u0011\n, and the distribution is given for a single bit i.\nDeep Unsupervised Learning using Nonequilibrium Thermodynamics\nC. Perturbed Gaussian Transition\nWe wish to compute \u02dcp\n\u0000x(t\u22121) | x(t)\u0001\n. For notational simplicity, let \u00b5 = f\u00b5\n\u0000x(t), t\n\u0001\n, \u03a3 = f\u03a3\n\u0000x(t), t\n\u0001\n, and y = x(t\u22121).\nUsing this notation,\n\u02dcp\n\u0010\ny | x(t)\u0011\n\u221dp\n\u0010\ny | x(t)\u0011\nr (y)\n(52)\n= N (y; \u00b5, \u03a3) r (y) .\n(53)\nWe can rewrite this in terms of energy functions, where Er (y) = \u2212log r (y),\n\u02dcp\n\u0010\ny | x(t)\u0011\n\u221dexp [\u2212E (y)]\n(54)\nE (y) = 1\n2 (y \u2212\u00b5)T \u03a3\u22121 (y \u2212\u00b5) + Er (y) .\n(55)\nIf Er (y) is smooth relative to 1\n2 (y \u2212\u00b5)T \u03a3\u22121 (y \u2212\u00b5), then we can approximate it using its Taylor expansion around \u00b5.\nOne suf\ufb01cient condition is that the eigenvalues of the Hessian of Er (y) are everywhere much smaller magnitude than the\neigenvalues of \u03a3\u22121. We then have\nEr (y) \u2248Er (\u00b5) + (y \u2212\u00b5) g\n(56)\nwhere g =\n\u2202Er(y\u2032)\n\u2202y\u2032\n\f\f\f\f\ny\u2032=\u00b5\n. Plugging this in to the full energy,\nE (y) \u22481\n2 (y \u2212\u00b5)T \u03a3\u22121 (y \u2212\u00b5) + (y \u2212\u00b5)T g + constant\n(57)\n= 1\n2yT \u03a3\u22121y \u22121\n2yT \u03a3\u22121\u00b5 \u22121\n2\u00b5T \u03a3\u22121y + 1\n2yT \u03a3\u22121\u03a3g + 1\n2gT \u03a3\u03a3\u22121y + constant\n(58)\n= 1\n2 (y \u2212\u00b5 + \u03a3g)T \u03a3\u22121 (y \u2212\u00b5 + \u03a3g) + constant.\n(59)\nThis corresponds to a Gaussian,\n\u02dcp\n\u0010\ny | x(t)\u0011\n\u2248N (y; \u00b5 \u2212\u03a3g, \u03a3) .\n(60)\nSubstituting back in the original formalism, this is,\n\u02dcp\n\u0010\nx(t\u22121) | x(t)\u0011\n\u2248N\n\uf8eb\n\uf8edx(t\u22121); f\u00b5\n\u0010\nx(t), t\n\u0011\n+ f\u03a3\n\u0010\nx(t), t\n\u0011 \u2202log r\n\u0010\nx(t\u22121)\u2032\u0011\n\u2202x(t\u22121)\u2032\n\f\f\f\f\f\nx(t\u22121)\u2032=f\u00b5(x(t),t)\n, f\u03a3\n\u0010\nx(t), t\n\u0011\n\uf8f6\n\uf8f8.\n(61)\nDeep Unsupervised Learning using Nonequilibrium Thermodynamics\nD. Experimental Details\nD.1. Toy Problems\nD.1.1. SWISS ROLL\nA probabilistic model was built of a two dimensional swiss\nroll distribution. The generative model p\n\u0000x(0\u00b7\u00b7\u00b7T )\u0001\ncon-\nsisted of 40 time steps of Gaussian diffusion initialized\nat an identity-covariance Gaussian distribution.\nA (nor-\nmalized) radial basis function network with a single hid-\nden layer and 16 hidden units was trained to generate the\nmean and covariance functions f\u00b5\n\u0000x(t), t\n\u0001\nand a diago-\nnal f\u03a3\n\u0000x(t), t\n\u0001\nfor the reverse trajectory. The top, read-\nout, layer for each function was learned independently for\neach time step, but for all other layers weights were shared\nacross all time steps and both functions. The top layer out-\nput of f\u03a3\n\u0000x(t), t\n\u0001\nwas passed through a sigmoid to restrict\nit between 0 and 1. As can be seen in Figure 1, the swiss\nroll distribution was successfully learned.\nD.1.2. BINARY HEARTBEAT DISTRIBUTION\nA probabilistic model was trained on simple binary se-\nquences of length 20, where a 1 occurs every 5th time\nbin, and the remainder of the bins are 0. The generative\nmodel consisted of 2000 time steps of binomial diffusion\ninitialized at an independent binomial distribution with the\nsame mean activity as the data (p\n\u0010\nx(T )\ni\n= 1\n\u0011\n= 0.2). A\nmultilayer perceptron with sigmoid nonlinearities, 20 in-\nput units and three hidden layers with 50 units each was\ntrained to generate the Bernoulli rates fb\n\u0000x(t), t\n\u0001\nof the re-\nverse trajectory. The top, readout, layer was learned inde-\npendently for each time step, but for all other layers weights\nwere shared across all time steps. The top layer output was\npassed through a sigmoid to restrict it between 0 and 1. As\ncan be seen in Figure 2, the heartbeat distribution was suc-\ncessfully learned. The log likelihood under the true gener-\nating process is log2\n\u0000 1\n5\n\u0001\n= \u22122.322 bits per sequence. As\ncan be seen in Figure 2 and Table 1 learning was nearly\nperfect.\nD.2. Images\nD.2.1. ARCHITECTURE\nReadout\nIn all cases, a convolutional network was used\nto produce a vector of outputs yi \u2208R2J for each image\npixel i. The entries in yi are divided into two equal sized\nsubsets, y\u00b5 and y\u03a3.\nTemporal Dependence\nThe convolution output y\u00b5 is\nused as per-pixel weighting coef\ufb01cients in a sum over time-\ndependent \u201cbump\u201d functions, generating an output z\u00b5\ni \u2208R\nfor each pixel i,\nz\u00b5\ni =\nJ\nX\nj=1\ny\u00b5\nijgj (t) .\n(62)\nThe bump functions consist of\ngj (t) =\nexp\n\u0010\n\u2212\n1\n2w2 (t \u2212\u03c4j)2\u0011\nPJ\nk=1 exp\n\u0010\n\u2212\n1\n2w2 (t \u2212\u03c4k)2\u0011,\n(63)\nwhere \u03c4j \u2208(0, T) is the bump center, and w is the spacing\nbetween bump centers. z\u03a3 is generated in an identical way,\nbut using y\u03a3.\nFor all image experiments a number of timesteps T = 1000\nwas used, except for the bark dataset which used T = 500.\nMean and Variance\nFinally, these outputs are combined\nto produce a diffusion mean and variance prediction for\neach pixel i,\n\u03a3ii = \u03c3\n\u0000z\u03a3\ni + \u03c3\u22121 (\u03b2t)\n\u0001\n,\n(64)\n\u00b5i = (xi \u2212z\u00b5\ni ) (1 \u2212\u03a3ii) + z\u00b5\ni .\n(65)\nwhere both \u03a3 and \u00b5 are parameterized as a perturbation\naround the forward diffusion kernel T\u03c0\n\u0000x(t)|x(t\u22121); \u03b2t\n\u0001\n,\nand z\u00b5\ni is the mean of the equilibrium distribution that\nwould result from applying p\n\u0000x(t\u22121)|x(t)\u0001\nmany times. \u03a3\nis restricted to be a diagonal matrix.\nMulti-Scale Convolution\nWe wish to accomplish goals\nthat are often achieved with pooling networks \u2013 specif-\nically, we wish to discover and make use of long-range\nand multi-scale dependencies in the training data. How-\never, since the network output is a vector of coef\ufb01cients\nfor every pixel it is important to generate a full resolution\nrather than down-sampled feature map. We therefore de\ufb01ne\nmulti-scale-convolution layers that consist of the following\nsteps:\n1. Perform mean pooling to downsample the image to\nmultiple scales. Downsampling is performed in pow-\ners of two.\n2. Performing convolution at each scale.\n3. Upsample all scales to full resolution, and sum the re-\nsulting images.\n4. Perform a pointwise nonlinear transformation, con-\nsisting of a soft relu (log [1 + exp (\u00b7)]).\nThe composition of the \ufb01rst three linear operations resem-\nbles convolution by a multiscale convolution kernel, up to\nblocking artifacts introduced by upsampling. This method\nof achieving multiscale convolution was described in (Bar-\nron et al., 2013).\nDeep Unsupervised Learning using Nonequilibrium Thermodynamics\nInput\nDense\nMulti-scale\nconvolution\nConvolution\n1x1 kernel\nTemporal\ncoefficients\nTemporal\ncoefficients\nDense\nMulti-scale\nconvolution\nMean\nimage\nCovariance\nimage\nConvolution\n1x1 kernel\nFigure D.1. Network architecture for mean function f\u00b5\n\u0010\nx(t), t\n\u0011\nand covariance function f\u03a3\n\u0010\nx(t), t\n\u0011\n, for experiments in Section\n3.2. The input image x(t) passes through several layers of multi-\nscale convolution (Section D.2.1). It then passes through several\nconvolutional layers with 1 \u00d7 1 kernels. This is equivalent to a\ndense transformation performed on each pixel. A linear transfor-\nmation generates coef\ufb01cients for readout of both mean \u00b5(t) and\ncovariance \u03a3(t) for each pixel. Finally, a time dependent readout\nfunction converts those coef\ufb01cients into mean and covariance im-\nages, as described in Section D.2.1. For CIFAR-10 a dense (or\nfully connected) pathway was used in parallel to the multi-scale\nconvolutional pathway. For MNIST, the dense pathway was used\nto the exclusion of the multi-scale convolutional pathway.\nDense Layers\nDense (acting on the full image vector)\nand kernel-width-1 convolutional (acting separately on the\nfeature vector for each pixel) layers share the same form.\nThey consist of a linear transformation, followed by a tanh\nnonlinearity.\n",
        "sentence": " related to the Helmholtz machine, but similar in spirit is the approach described in [10].",
        "context": "(Sminchisescu et al., 2006). However, our motivation and\nmodel form are both quite different, and the present work\nretains the following differences and advantages relative to\nthese techniques:\n1. We develop our framework using ideas from physics,\nAccu-\nrate and Conservative Estimates of MRF Log-likelihood\nusing Reverse Annealing. arXiv:1412.8566, December\n2014.\nDayan, P., Hinton, G. E., Neal, R. M., and Zemel, R. S. The\nhelmholtz machine. Neural computation, 7(5):889\u2013904,\n1995.\nlearning, sampling, inference, and evaluation\nare still analytically or computationally tractable.\nHere, we develop an approach that simultane-\nously achieves both \ufb02exibility and tractability.\nThe essential idea, inspired by non-equilibrium"
    },
    {
        "title": "Deep boltzmann machines",
        "author": [
            "Ruslan Salakhutdinov",
            "Geoffrey E Hinton"
        ],
        "venue": "In International Conference on Artificial Intelligence and Statistics,",
        "citeRegEx": "11",
        "shortCiteRegEx": "11",
        "year": 2009,
        "abstract": "",
        "full_text": "",
        "sentence": " Analogous to a Deep Boltzmann Machine [11] we think of these as layers in a neural network with links between x and h1 on the one side, and h1 and h2 on the other side.",
        "context": null
    },
    {
        "title": "Unit tests for stochastic optimization",
        "author": [
            "Tom Schaul",
            "Ioannis Antonoglou",
            "David Silver"
        ],
        "venue": "arXiv preprint arXiv:1312.6055,",
        "citeRegEx": "12",
        "shortCiteRegEx": "12",
        "year": 2013,
        "abstract": "Optimization by stochastic gradient descent is an important component of many\nlarge-scale machine learning algorithms. A wide variety of such optimization\nalgorithms have been devised; however, it is unclear whether these algorithms\nare robust and widely applicable across many different optimization landscapes.\nIn this paper we develop a collection of unit tests for stochastic\noptimization. Each unit test rapidly evaluates an optimization algorithm on a\nsmall-scale, isolated, and well-understood difficulty, rather than in\nreal-world scenarios where many such issues are entangled. Passing these unit\ntests is not sufficient, but absolutely necessary for any algorithms with\nclaims to generality or robustness. We give initial quantitative and\nqualitative results on numerous established algorithms. The testing framework\nis open-source, extensible, and easy to apply to new algorithms.",
        "full_text": "Unit Tests for Stochastic Optimization\nTom Schaul\nIoannis Antonoglou\nDeepMind Technologies\n130 Fenchurch Street, London, UK\n{tom,ioannis,david}@deepmind.com\nDavid Silver\nAbstract\nOptimization by stochastic gradient descent is an important component of many\nlarge-scale machine learning algorithms. A wide variety of such optimization\nalgorithms have been devised; however, it is unclear whether these algorithms are\nrobust and widely applicable across many different optimization landscapes. In\nthis paper we develop a collection of unit tests for stochastic optimization. Each\nunit test rapidly evaluates an optimization algorithm on a small-scale, isolated,\nand well-understood dif\ufb01culty, rather than in real-world scenarios where many\nsuch issues are entangled. Passing these unit tests is not suf\ufb01cient, but absolutely\nnecessary for any algorithms with claims to generality or robustness. We give\ninitial quantitative and qualitative results on numerous established algorithms. The\ntesting framework is open-source, extensible, and easy to apply to new algorithms.\n1\nIntroduction\nStochastic optimization [1] is among the most widely used components in large-scale machine learn-\ning, thanks to its linear complexity, ef\ufb01cient data usage, and often superior generalization [2, 3, 4].\nIn this context, numerous variants of stochastic gradient descent have been proposed, in order to\nimprove performance, robustness, or reduce tuning effort [5, 6, 7, 8, 9]. These algorithms may\nderive from simplifying assumptions on the optimization landscape [10], but in practice, they tend\nto be used as general-purpose tools, often outside of the space of assumptions their designers in-\ntended. The troublesome conclusion is that practitioners \ufb01nd it dif\ufb01cult to discern where potential\nweaknesses of new (or old) algorithms may lie [11], and when they are applicable \u2013 an issue that\nis separate from raw performance. This results in essentially a trial-and-error procedure for \ufb01nd-\ning the appropriate algorithm variant and hyper-parameter settings, every time that the dataset, loss\nfunction, regularization parameters, or model architecture change [12].\nThe objective of this paper is to establish a collection of benchmarks to evaluate stochastic opti-\nmization algorithms and guide algorithm design toward robust variants. Our approach is akin to\nunit testing, in that it evaluates algorithms on a very broad range of small-scale, isolated, and well-\nunderstood dif\ufb01culties, rather than in real-world scenarios where many such issues are entangled.\nPassing these unit tests is not suf\ufb01cient, but absolutely necessary for any algorithms with claims to\ngenerality or robustness. This is a similar approach to the very fruitful one taken by the black-box\noptimization community [13, 14].\nThe core assumption we make is that stochastic optimization algorithms are acting locally, that\nis, they aim for a short-term reduction in loss given the current noisy gradient information, and\npossibly some internal variables that capture local properties of the optimization landscape. These\nlocal actions include both approaching nearby optima, and navigating slopes, valleys or plateaus\nthat are far from an optimum. The locality property stems from computational ef\ufb01ciency concerns,\nbut it has the additional bene\ufb01ts of minimizing initialization bias and allowing for non-stationary\noptimization, because properties of the obervation surface observed earlier in the process (and their\n1\narXiv:1312.6055v3  [cs.LG]  25 Feb 2014\nFigure 1:\nSome one-dimensional shape prototypes. The \ufb01rst six example shapes are atomic pro-\ntotypes: a quadratic bowl, an absolute value, a cliff with a non differential point after which the\nderivative increases by a factor ten, a recti\ufb01ed linear shape followed by a bend, an inverse Gaus-\nsian, an inverse Laplacian. The next six example shapes are concatenations of atomic prototypes:\na sigmoid as a concatenation of a non convex Gaussian a line and an exponential, a quadratic bowl\nfollowed by a cliff and then by an exponential function, a quadratic bowl followed by a cliff and\nanother quadratic bowl, a sinusoid as a concatenation of quadratic bowls, a line followed by a Gaus-\nsian bowl, a quadratic bowl and a cliff and \ufb01nally, a Laplace bowl followed by a cliff and another\nLaplace bowl.\nconseuences for the algorithm state) are quickly forgotten. We therefore concentrate on building\nlocal unit tests, that investigate algorithm dynamics on a broad range of local scenarios, because\nwe expect that detecting local failure modes will \ufb02ag an algorithm as unlikely to be robust on more\ncomplex tasks \u2013 and as a \ufb01rst approximation, optimization on such a complex task can be seen as a\nsequence of many smaller optimization problems (many of which will not have local optima).\nOur divide-and-conquer approach consists of disentangling potential dif\ufb01culties and testing them in\nisolation or in simple couplings. Given that our unit tests are small and quick to evaluate, we can\nhave a much larger collection of them, testing hundreds of qualitatively different aspects in less time\nthan it would take to optimize a single traditional benchmark to convergence, thus allowing us to\nspot and address potential weaknesses early.\nOur main contribution is a testing framework, with unit tests designed to test aspects such as: dis-\ncontinuous or non-differentiable surfaces, curvature scales, various noise conditions and outliers,\nsaddle-points and plateaus, cliffs and asymmetry, and curl and bootstrapping. It also allows test\ncases to be concatenated by chaining them in a temporal series, or by combining them into multi-\ndimensional unit tests (with or without variable coupling). We give initial quantitative and qualitative\nresults on a number of established algorithms.\nWe do not expect this to replace traditional benchmark domains that are closer to the real-world, but\nto complement it in terms of breadth and robustness. We have tried to keep the framework general\nand extendable, in the hope it will further grow in diversity, and help others in doing robust algorithm\ndesign.\n2\n2\nUnit test Construction\nOur testing framework is an open-source library containing a collection of unit tests and visualization\ntools. Each unit test is de\ufb01ned by a prototype function to be optimized, a prototypical scale, a noise\nprototype, and optionally a non-stationarity prototype. A prototype function is the concatenation\nof one or more local shape prototypes. A multi-dimensional unit test is a composition of one-\ndimensional unit tests, optionally with a rotation prototype or curl prototype.\n2.1\nShape Prototypes\nShape prototypes are functions de\ufb01ned on an interval, and our collection includes linear slopes\n(zero curvature), quadratic curves (\ufb01xed curvature), convex or concave curves (varying curvature),\nand curves with exponentially increasing or decreasing slope. Further, there are a number of non-\ndifferentiable local shape prototypes (absolute value, recti\ufb01ed-linear, cliff). All of these occur in\nrealistic learning scenarios, for example in logistic regression the loss surface is part concave and\npart convex, an MSE loss is the prototypical quadratic bowl, but then regularization such as L1\nintroduces non-differentiable bends (as do recti\ufb01ed-linear or maxout units in deep learning [15, 16]).\nSteep cliffs in the loss surface are a common occurrence when training recurrent neural networks,\nas discussed in [11]. See the top rows of Figure 1 for some examples of shape prototypes.\n2.2\nOne-dimensional Concatenation\nIn our framework, we can chain together a number of shape prototypes, in such a way that the result-\ning function is continuous and differentiable at all junction points. We can thus produce many pro-\ntotype functions that closely mimic existing functions, e.g., the Laplace function, sinusoids, saddle-\npoints, step-functions, etc. See the bottom rows of Figure 1 for some examples.\nA single scale parameter determines the scaling of a concatenated function across all its shapes using\nthe junction constraints. Varying the scales is an important aspect of testing robustness because it is\nnot possible to guarantee well-scaled gradients without substantial overhead. In many learning prob-\nlems, effort is put into proper normalization [17], but that is insuf\ufb01cient to guarantee homogeneous\nscaling, for example throughout all the layers of a deep neural network.\n2.3\nNoise Prototypes\nThe distinguishing feature of stochastic gradient optimization (compared to batch methods) is that it\nrelies on sample gradients (coming from a subset of even a single element of the dataset) which are\ninherently noisy. In out unit tests, we model this by four types of stochasticity:\n\u2022 Scale-independent additive Gaussian noise on the gradients, which is equivalent to random\ntranslations of inputs in a linear model with MSE loss. Note that this type of noise \ufb02ips the\nsign of the gradient near the optimum and makes it dif\ufb01cult to approach precisely.\n\u2022 Multiplicative (scale-dependent) Gaussian noise on the gradients, which multiplies the gra-\ndients by a positive random number (signs are preserved). This corresponds to a learning\nscenario where the loss curvature is different for different samples near the current point.\n\u2022 Additive zero-median Cauchy noise, mimicking the presence of outliers in the dataset.\n\u2022 Mask-out noise, which zeros the gradient (independently for each dimension) with a certain\nprobability. This mimics both training with drop-out [18], and scenarios with recti\ufb01ed\nlinear units where a unit will be inactive for some input samples, but not for others.\nFor the \ufb01rst three, we can vary the noise scale, while for mask-out we pick a drop-out frequency.\nThis noise is not necessarily unbiased (as in the Cauchy case), breaking common assumptions made\nin algorithm design (but the modi\ufb01cations in section 2.5 are even worse). See Figure 2 for an illus-\ntration of the \ufb01rst two noise prototypes. Noise prototypes and prototype functions can be combined\nindependently into one-dimensional unit tests.\n3\nFigure 2:\nExamples of noise applied on prototype functions, green dashed are typical sample\ngradients, and the standard deviation range is the blue area. The upper two subplots depict Gaussian\nadditive noise, while the lower two show Gaussian multiplicative noise. In the left column, the\nnoise is applied to the gradients of a quadratic bowl prototype (note how the multiplicative noise\ngoes to zero around the optimum in the middle), and on the right it is applied to a concatenation of\nprototypes.\n2.4\nMulti-dimensional Composition\nA whole range of dif\ufb01culties for optimization only exist in higher dimensional parameter spaces\n(e.g., saddle points, conditoning, correlation). Therefore, we build high-dimensional unit tests by\ncomposing together one-dimensional unit tests. For example for two one-dimensional prototype\nshapes La and Lb combined with a p-norm, the composition is L(a,b)(\u03b8) = (La(\u03b81)p + Lb(\u03b82)p)\n1\np .\nNoise prototypes are composed independently of shape prototypes. While they may be composed\nof concatenated one-dimensional prototypes, higher-dimensional prototypes are not concatenated\nthemselves. Various levels of conditioning can be achieved by having dramatically different scales\nin different component dimensions.\nIn addition to the choice of prototypes to be combined, and their scale, we permit a rotation in\ninput space, which couples the dimensions together and avoids axis-alignment. These rotations are\nparticularly important for testing diagonal/element-wise optimization algorithms.\n2.5\nCurl\nIn reinforcement learning a value function (the expected discounted reward for each state) can be\nlearned using temporal-difference learning (TD), an update procedure that uses bootstrapping: i.e.\nit pulls the value of the current state towards the value of its successor state [19]. These stochastic\nupdate directions are not proper gradients of any scalar energy \ufb01eld [20], but they still form a (more\ngeneral) vector \ufb01eld with non-zero curl, where the objective for the optimization algorithm is to\nconverge to its \ufb01xed-point(s). See Figure 4 for a detailed example. We implemented this aspect by\nallowing different amounts of curl to be added on top of a multi-dimensional vector \ufb01eld in our unit\ntests, which is done by rotating the produced gradient vectors using a \ufb01xed rotation matrix. This is\nreasonably realistic; in fact, for the TD example in Figure 4, the resulting vector \ufb01eld is exactly the\ngradient \ufb01eld of a quadratic combined with a (small-angle) rotation.\n2.6\nNon-stationarity\nIn many settings it is necessary to optimize a non-stationary objective function. This may typically\noccur in a non-stationary task where the problem to be solved changes over time. However, non-\n4\nFigure 3: Examples of multivariate prototypes. The \ufb01rst subplot depicts an asymmetric quadratic\nbowl with correlated dimensions, the second a surface with a saddle point, the third a sharp valley\nsurface, the fourth a half-pipe surface where the \ufb01rst dimension is a line and the second one a\nquadratic bowl. The \ufb01fth subplot depicts a surface with an ill conditioned minimum in the point\nwhere the two canyons overlap. The surface in the last subplot is the composition of a quadratic\nbowl in the \ufb01rst dimension and of a cliff in the second.\nFigure 4: Here, we consider a very simple Markov process, with two states and stochastic transitions\nbetween them, and a reward of 0 in the \ufb01rst and of 1 in the second state. Consider the parameters\nof our optimization \u03b8 to be the two state values. Each TD update changes one of them, depending\non the stochastic transition observed. In this \ufb01gure, we plot the vector \ufb01eld of expected update\ndirections (blue arrows) as a function of \u03b8, as well as one sampled trajectory of the TD algorithm.\nNote how this vector \ufb01eld is not actually a gradient \ufb01eld, but instead has substantial curl, making it\na challenging stochastic optimization task.\nstationary optimization can even be important in large stationary tasks (with temporal structure in\nthe samples), when the algorithm chooses to track a particular dynamic aspect of the problem,\nrather than attempting to converge to a global but static solution of the problem [21]. In addition,\nreinforcement learning (RL) tasks often involve non-stationary optimization. For example, many\nRL algorithms proceed by evaluating the value function using the TD algorithm described in the\n5\nprevious section. This results in two sources of non-stationarity: the target value changes at every\nstep (resulting in the previously described curl); and also the state distribution changes as the value\nfunction improves and better actions are selected. These scenarios can be therefore be viewed as\nnon-stationary loss functions, but whose optimum moves as a function of the current parameter\nvalues.\nWe test non-stationarity in three different ways. We let the location of the optimum move smoothly,\nvia random translations of the parameter space, or we let the the scale of the shape prototype vary\nrandomly (on average by 10% in each direction), or, on noisy unit tests, we let the scale of the noise\nvary randomly. Currently, these changes happen once every 10 steps. A type of non-stationarity that\ninvolves more abrupt switching is discussed in section 4.1.\n3\nExperiments\n3.1\nSetup and Algorithms\nFor our experiments, we test the candidate algorithms on over 3000 unit tests, with up to 10 param-\neter dimensions. Each algorithm-unit test pairing is repeated 10 times, but with reusing the same 10\nrandom seeds across all algorithms and setups. For each run k, we compute the true expected loss\nat the parameter value reached after 100 update steps L(k) = E\nh\nL\n\u0010\n\u03b8(k)\n100\n\u0011i\n.\nThe algorithms evaluated are SGD with \ufb01xed learning rate \u03b70 \u2208[10\u22126, 10], SGD with annealing\nwith decay factor in [10\u22122, 1] and initial rates \u03b70, SGD with momentum (regular or Nesterov\u2019s\nvariant [22]) [0.1, 0.999] and initial rates \u03b70, SGD with parameter averaging [] with decay term in\n[10\u22124, 0.5] and exponent in { 1\n2, 3\n4, 1}, ADAGRAD [10] with initial rates \u03b70, ADADELTA [23] with\ndecay parameter (1 \u2212\u03b3) \u2208[10\u22124, 0.5] and regularizer in [10\u22126, 10\u22122, the incremental delta-bar-\ndelta algorithm (IDBD [24]), RPROP [25] with initial stepsizes \u03b70, RMSprop [26] with minimal\nlearning rates \u03b70, maximal learning rates in [10, 103] and decay parameter \u03b3, as well as conjugate\ngradients. For the hyper-parameters ranges, we always consider one value per order of magnitude,\nand exhaustively sweep all combinations.\n3.2\nReference performance\nEach unit test is associated with a reference performance Lsgd, and a corresponding reference learn-\ning rate \u03b7best that is determined by doing a parameter sweep over all \ufb01xed learning rates for SGD\n(34 values log-uniform between 10\u221210 and 10) and retaining the best-performing one.\nIn our aggregate plots, unit tests are sorted (per group) by their reference learning rate, i.e., those\nthat require small steps on the left, and those where large steps are best on the right. Algorithm\nsetups are sorted as well, on the vertical axis, by their median performance on a reference unit test\n(quadratic, additive noise).\n3.3\nQualitative Evaluation\nThe algorithm performance L(k) is converted to a normalized value L(k)\nnorm =\nL(k)\u2212Linit\nLsgd\u2212Linit where\nLinit = E[L(\u03b80)] is the expected loss value at the initial point, similar to the approach taken in [27],\nbut even more condensed. In other words, a normalized value near zero corresponds to no progress,\nnegative denotes divergence, and a value near one is equivalent to the best SGD. Based on these\nresults, we assign a qualitative color value to the performance of each algorithm setup on each unit\ntest, to able to represent it in a single pixel in the resulting \ufb01gures:\n\u2022 Red: Divergence or numerical instability in all run.\n\u2022 Violet: Divergence or numerical instability in at least one run.\n\u2022 Orange: Insuf\ufb01cient progress: median(Lnorm) < 0.1\n\u2022 Yellow: Good progress: median(Lnorm) > 0.1 and high variability: Lnorm < 0.1 for at\nleast 1\n4 of the runs.\n6\n\u2022 Green: Good progress: median(Lnorm) > 0.1 and low variability: Lnorm < 0.1 for at\nmost 1\n4 of the runs.\n\u2022 Blue: Excellent progress: median(Lnorm) > 2.\n3.4\nResults\nFigures 5 and 6 shows the qualitative results of all algorithm variants on all the unit tests. There is\na wealth of information in these visualizations. For example the relatively scarce amount of blue\nindicate that it is dif\ufb01cult to substantially beat well-tuned SGD in performance on most unit tests.\nAnother unsurprising conclusion is that hyper-parameter tuning matters much less for the adaptive\nalgorithms (ADAGRAD, ADADELTA, RPROP, RMSprop) than for the non-adaptive SGD variants.\nAlso, while some unit tests are more tricky than others on average, there is quite some diversity in\nthe sense that some algorithms may outdo SGD on a unit test where other algorithms fail (especially\non the non-differentiable functions).\n4\nRealism and Future Work\nWe do not expect to replace real-world benchmark domains, but rather to complement them with our\nsuite of unit tests. Still, it is important to have suf\ufb01cient coverage of the types of potential dif\ufb01culties\nencountered in realistic settings. To a much lesser degree, we may not want to clutter the test suite\nwith unit tests that measure issues which never occur in realistic problems.\nIt is not straightforward to map very high-dimensional real-world loss functions down to low-\ndimensional prototype shapes, but it is not impossible. For example, in Figure 8 we show some\nrandom projections in parameter space of the loss function in an MNIST classi\ufb01cation task with an\nMLP [28]. We defer a fuller investigation of this type, namely obtaining statistics on how commonly\ndifferent prototypes are occurring, to future work.\nHowever, the unit tests capture the properties of some examples that can be analyzed. One of them\nwas discussed in section 2.5, another one is the simple loss function of a one-dimensional auto-\nencoder:\nL\u03b8(x) = (x + \u03b82 \u00b7 \u03c3(x \u00b7 \u03b81))2\nwhere \u03c3 is the sigmoid function. Even in the absence of noise, this minimal scenario has a saddle-\npoint near \u03b8 = (0, 0), a plateau shape away from the axes, a cliff shape near the vertical axis, and\na correlated valley near \u03b8 = (1, 1), as illustrated in Figure 7. All of these prototypical shapes are\nincluded in our set of unit tests.\nAn alternative approach is predictive: if the performance on the unit tests is highly predictive of\nan algorithm\u2019s performance on a some real-world task, then those unit tests must be capturing the\nessential aspects of the task. Again, building such a predictor is an objective for future work.\n4.1\nAlgorithm Dynamics\nOur long-term objective is to be able to do systematic testing and a full investigation of the opti-\nmization dynamics for a given algorithm. Of course, it is not possible to test it exhaustively on all\npossible loss functions (because there are in\ufb01nitely many), but a divide-and-conquer approach may\nbe the next best thing. For this, we introduce the notion of algorithm state, which is changing during\noptimization (e.g., the current stepsize or momentum). Now, a long optimization process can be\nseen as the chaining of a number of unit tests, while preserving the algorithm state in-between them.\nOur hypothesis is that the set of all possible chains of unit tests in our collection covers most of the\nqualitatively different (stationary or non-stationary) loss functions an optimization algorithm may\nencounter.\nTo evaluate an algorithm\u2019s robustness (rather than its expected performance), we can assume that\nan adversary picks the worst-case unit tests at each step in the sequence. An algorithm is only\ntruly robust if it does not diverge under any sequence of unit tests. Besides the worst-case, we may\nalso want to study typical expected behavior, namely whether the dynamics have an attractor in the\nalgorithm\u2019s state space. If an attractor exists where the algorithm is stable, then it becomes useful\nto look at the secondary criterion for the algorithm, namely its expected (normalized) performance.\n7\nFigure 5:\nQualitative results for all algorithm variants (350) on all stationnary, one-dimensional unit tests. Each column is one unit test, grouped by shared\nproperties (see caption), for example the \ufb01rst group includes all noise-free 1D unit tests, where groups of unit tests can be partially overlapping. Each group of\nrows is one algorithm, with one set of hyper-parameters per row. The color code is: red/violet=divergence, orange=slow, yellow=variability, green=acceptable,\nblue=excellent (see main text for details).\n8\nFigure 6: Qualitative results for all algorithm variants (350) on all non-stationary one-dimensional unit tests (\ufb01rs three groups), on all two-dimensional ones (next\nthree groups), and on all ten-dimensional ones (last three groups). The color code is: red/violet=divergence, orange=slow, yellow=variability, green=acceptable,\nblue=excellent. See Figure 5 and main text for details.\n9\nFigure 7: Illustration of the loss surface of a one-dimensional auto-encoder, as de\ufb01ned in the text,\nwhere the darkest blue corresponds to the lowest loss. Left: from the zoomed-out perspective if ap-\npears to be roughly a vertical valley, leading an optimizer toward the y-axis from almost anywhere in\nthe space. Center: the zoomed-in perspective around the origin, which is looking like a prototypical\nsaddle point. Right: the shape of the valley in the lower left quadrant, the walls of which become\nsteeper the more the search progresses.\nFigure 8:\nLeft: collection of 64 random projections into two dimensions of the MNIST loss sur-\nface (based on one randomly sampled digit for each column). The projections are centered around\nthe weights learned after one epoch of training, and different projections are plotted on scales be-\ntween 0.05 (top row) and 0.5 (bottom row). Right: the same as on the left, but with axis-aligned\nprojections.\nWe conjecture that this analysis may lead to novel insights into how to design robust and adaptive\noptimization algorithms.\n5\nConclusion\nThis paper established a large collection of simple comparative benchmarks to evaluate stochastic\noptimization algorithms, on a broad range of small-scale, isolated, and well-understood dif\ufb01culties.\nThis approach helps disentangle issues that tend to be confounded in real-world scenarios, while\nretaining realistic properties. Our initial results on a dozen established algorithms (under a variety of\ndifferent hyperparameter settings) show that robustness is non-trivial, and that different algorithms\nstruggle on different unit tests. The testing framework is open-source, extensible to new function\nclasses, and easy to use for evaluating the robustness of new algorithms.\nThe full source code (see also Appendix A) is available under BSD license at:\nhttps://github.com/IoannisAntonoglou/optimBench\n10\nAcknowledgements\nWe thank the anonymous ICLR reviewers for their many constructive comments.\nReferences\n[1] H. Robbins and S. Monro.\nA stochastic approximation method.\nAnnals of Mathematical\nStatistics, 22:400\u2013407, 1951.\n[2] L\u00b4eon Bottou. Online Algorithms and Stochastic Approximations. In David Saad, editor, Online\nLearning and Neural Networks. Cambridge University Press, Cambridge, UK, 1998.\n[3] L\u00b4eon Bottou and Yann LeCun. Large Scale Online Learning. In Sebastian Thrun, Lawrence\nSaul, and Bernhard Sch\u00a8olkopf, editors, Advances in Neural Information Processing Systems\n16. MIT Press, Cambridge, MA, 2004.\n[4] L\u00b4eon Bottou and Olivier Bousquet. The Tradeoffs of Large Scale Learning. In J.C. Platt,\nD. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Sys-\ntems, volume 20, pages 161\u2013168. NIPS Foundation (http://books.nips.cc), 2008.\n[5] A. Benveniste, M. Metivier, and P. Priouret. Adaptive Algorithms and Stochastic Approxima-\ntions. Springer Verlag, Berlin, New York, 1990.\n[6] N. Le Roux, P.A. Manzagol, and Y. Bengio. Topmoumoute online natural gradient algorithm,\n2008.\n[7] Antoine Bordes, L\u00b4eon Bottou, and Patrick Gallinari.\nSGD-QN: Careful Quasi-Newton\nStochastic Gradient Descent. Journal of Machine Learning Research, 10:1737\u20131754, July\n2009.\n[8] Wei Xu. Towards Optimal One Pass Large Scale Learning with Averaged Stochastic Gradient\nDescent. ArXiv-CoRR, abs/1107.2490, 2011.\n[9] Tom Schaul, Sixin Zhang, and Yann LeCun. No More Pesky Learning Rates. In International\nConference on Machine Learning (ICML), 2013.\n[10] John C. Duchi, Elad Hazan, and Yoram Singer. Adaptive Subgradient Methods for Online\nLearning and Stochastic Optimization. 2010.\n[11] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. Understanding the exploding gradient\nproblem. arXiv preprint arXiv:1211.5063, 2012.\n[12] X. Glorot and Y. Bengio. Understanding the dif\ufb01culty of training deep feedforward neural\nnetworks. In G. Orr and Muller K., editors, Proceedings of the International Conference on\nArti\ufb01cial Intelligence and Statistics (AISTATS), pages 249\u2013256. Society for Arti\ufb01cial Intelli-\ngence and Statistics, 2010.\n[13] Nikolaus Hansen, Anne Auger, Steffen Finck, Raymond Ros, et al. Real-parameter black-box\noptimization benchmarking 2010: Experimental setup. 2010.\n[14] Nikolaus Hansen, Anne Auger, Raymond Ros, Steffen Finck, and Petr Po\u02c7s\u00b4\u0131k. Comparing\nresults of 31 algorithms from the black-box optimization benchmarking BBOB-2009. In Pro-\nceedings of the 12th annual conference companion on Genetic and evolutionary computation,\npages 1689\u20131696. ACM, 2010.\n[15] Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton. Imagenet classi\ufb01cation with deep con-\nvolutional neural networks. In Advances in Neural Information Processing Systems 25, pages\n1106\u20131114, 2012.\n[16] Ian J Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio.\nMaxout networks. arXiv preprint arXiv:1302.4389, 2013.\n[17] Y. LeCun, L. Bottou, G. Orr, and K. Muller. Ef\ufb01cient BackProp. In G. Orr and Muller K.,\neditors, Neural Networks: Tricks of the trade. Springer, 1998.\n[18] Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhut-\ndinov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv\npreprint arXiv:1207.0580, 2012.\n[19] R.S. Sutton and A.G. Barto. Reinforcement Learning: An Introduction. IEEE Transactions on\nNeural Networks, 9(5):1054\u20131054, Sep 1998.\n11\n[20] Etienne Barnard. Temporal-difference methods and Markov models. IEEE Transactions on\nSystems, Man, and Cybernetics, 23(2):357\u2013365, 1993.\n[21] Richard S. Sutton, Anna Koop, and David Silver. On the role of tracking in stationary environ-\nments. In Proceedings of the Twenty-Fourth International Conference on Machine Learning\n(ICML 2007, pages 871\u2013878. ACM Press, 2007.\n[22] Yurii Nesterov and Arkadii Semenovich Nemirovskii. Interior-point polynomial algorithms in\nconvex programming, volume 13. SIAM, 1994.\n[23] Matthew D Zeiler.\nADADELTA: An Adaptive Learning Rate Method.\narXiv preprint\narXiv:1212.5701, 2012.\n[24] Richard S Sutton. Adapting bias by gradient descent: An incremental version of delta-bar-\ndelta. In AAAI, pages 171\u2013176, 1992.\n[25] Martin Riedmiller and Heinrich Braun. A direct adaptive method for faster backpropagation\nlearning: The RPROP algorithm. In Neural Networks, 1993., IEEE International Conference\non, pages 586\u2013591. IEEE, 1993.\n[26] T Tieleman and G Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of\nits recent magnitude. COURSERA: Neural Networks for Machine Learning, 2012.\n[27] Tom Schaul and Yann LeCun. Adaptive learning rates and parallelization for stochastic, sparse,\nnon-smooth gradients. In International Conference on Learning Representations, Scottsdale,\nAZ, 2013.\n[28] Yann LeCun and Corinna Cortes.\nThe MNIST dataset of handwritten digits.\n1998.\nhttp://yann.lecun.com/exdb/mnist/.\nA\nAppendix: Framework Software\nAs part of this work a software framework was developed for the computing and managing all the\nresults obtained for all the different con\ufb01gurations of function prototypes and algorithms. The main\ncomponent of the system is a database where all the results are stored and can be easily retrieved\nby querying the database accordingly. The building blocks of this database are the individual exper-\niments, where each experiment is associated to a unit test and an algorithm with \ufb01xed parameters.\nAn instance of an experiment database can either be loaded from the disk, or it can be created on the\n\ufb02y by running the associated experiments as needed. The code below creates a database and runs all\nthe experiments for all the readily available algorithms and default unit tests, and then saves them to\ndisk:\nrequire \u2019experiment\u2019\nlocal db = experimentsDB()\ndb:runExperiments()\ndb:save(\u2019experimentsDB\u2019)\nThis database now can be loaded from the disk, and the user can query it in order to retrieve speci\ufb01c\nexperiments, using \ufb01lters. An example is shown below:\nlocal db = experimentsDB()\ndb:load(\u2019experimentsDB\u2019)\nlocal experiments = db:filter({fun={\u2019quad\u2019, \u2019line\u2019},\nalgo={\u2019sgd\u2019}, learningRate=1e-4})\nThe code above loads an experiment database from the disk and it retrieves all the experiments for\nall the quadratic and line prototype shapes, for all different types of noise and all scales, further\nselecting the subset of experiments to those optimized using SGD with learningRate equal to 1e-\n4. The user can rerun the extracted experiments or have access to the associated results, i.e., the\nexpected value of the function in different optimization steps, along with the associated parameters\nvalues. In order to qualitatively assess the results the following code can be used:\ndb:ComputeReferenceValues()\ndb:cleanDB()\n12\ndb:assessPerformance()\ndb:plotExperiments()\nThe code above computes the reference expected values for each prototype function, it removes the\nexperiments for which no reference value is available, then it qualitatively assesses the performance\nof all the available experiments and \ufb01nally it plots the results given the color con\ufb01guration described\nin section 3.3. It is really easy to add a new algorithm in the database in order to evaluate its\nrobustness. The code below illustrates a simple example:\ndb:addAlgorithm(algoname, algofun, opt)\ndb:testAlgorithm(algoname)\ndb:plotExperiments({}, {algoname})\nHere a new algorithm with name algoname, function instance algo (which should satisfy the\noptim interface), and a table of different parameter con\ufb01gurations opt is added to the database\nand it is tested under all available functions prototypes. Finally, the last line plots a graph with all\nthe results for this algorithm.\nIt is also possible to add a set of new unit tests to the database, and subsequently run a set of\nexperiments associated with them. There are different parameters to be de\ufb01ned for the creation of a\nset of unit tests (that allow wildcard speci\ufb01cation too):\n1. the concatenated shape prototypes for each dimension,\n2. the noise prototype to be applied to each dimension,\n3. the scale of each dimension of the function,\n4. in case of multivariate unit tests, a parameter speci\ufb01es which p-norm is used for the com-\nbination,\n5. a rotation parameter that induces correlation of the different parameter dimensions, and\n6. a curl parameter that changes the vector \ufb01eld of a multivariate function.\n13\n",
        "sentence": " [12] for a description) with a mini-batch size of 100.",
        "context": "3.1\nSetup and Algorithms\nFor our experiments, we test the candidate algorithms on over 3000 unit tests, with up to 10 param-\neter dimensions. Each algorithm-unit test pairing is repeated 10 times, but with reusing the same 10\n[4] L\u00b4eon Bottou and Olivier Bousquet. The Tradeoffs of Large Scale Learning. In J.C. Platt,\nD. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Sys-\ning the appropriate algorithm variant and hyper-parameter settings, every time that the dataset, loss\nfunction, regularization parameters, or model architecture change [12]."
    },
    {
        "title": "Understanding the difficulty of training deep feedforward neural networks",
        "author": [
            "Xavier Glorot",
            "Yoshua Bengio"
        ],
        "venue": "In AISTATS\u20192010,",
        "citeRegEx": "13",
        "shortCiteRegEx": "13",
        "year": 2010,
        "abstract": "",
        "full_text": "",
        "sentence": " We initialize all weights according to [13] and set all biases to -1.",
        "context": null
    },
    {
        "title": "The Neural Autoregressive Distribution Estimator",
        "author": [
            "Hugo Larochelle",
            "Ian Murray"
        ],
        "venue": "In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTATS\u20192011),",
        "citeRegEx": "14",
        "shortCiteRegEx": "14",
        "year": 2011,
        "abstract": "",
        "full_text": "",
        "sentence": " in [14]. Model ADULT CONNECT4 DNA MUSHROOMS NIPS-0-12 OCR-LETTERS RCV1 WEB auto regressive NADE[14] 13.",
        "context": null
    },
    {
        "title": "A deep and tractable density estimator",
        "author": [
            "Benigno Uria Iain Murray",
            "Hugo Larochelle"
        ],
        "venue": "In ICML\u20192014,",
        "citeRegEx": "15",
        "shortCiteRegEx": "15",
        "year": 2014,
        "abstract": "The Neural Autoregressive Distribution Estimator (NADE) and its real-valued\nversion RNADE are competitive density models of multidimensional data across a\nvariety of domains. These models use a fixed, arbitrary ordering of the data\ndimensions. One can easily condition on variables at the beginning of the\nordering, and marginalize out variables at the end of the ordering, however\nother inference tasks require approximate inference. In this work we introduce\nan efficient procedure to simultaneously train a NADE model for each possible\nordering of the variables, by sharing parameters across all these models. We\ncan thus use the most convenient model for each inference task at hand, and\nensembles of such models with different orderings are immediately available.\nMoreover, unlike the original NADE, our training procedure scales to deep\nmodels. Empirically, ensembles of Deep NADE models obtain state of the art\ndensity estimation performance.",
        "full_text": "A Deep and Tractable Density Estimator\nBenigno Uria\nB.URIA@ED.AC.UK\nIain Murray\nI.MURRAY@ED.AC.UK\nSchool of Informatics, University of Edinburgh\nHugo Larochelle\nHUGO.LAROCHELLE@USHERBROOKE.CA\nD\u00b4epartement d\u2019informatique, Universit\u00b4e de Sherbrooke\nAbstract\nThe Neural Autoregressive Distribution Estimator\n(NADE) and its real-valued version RNADE are\ncompetitive density models of multidimensional\ndata across a variety of domains. These models\nuse a \ufb01xed, arbitrary ordering of the data dimen-\nsions. One can easily condition on variables at\nthe beginning of the ordering, and marginalize\nout variables at the end of the ordering, however\nother inference tasks require approximate infer-\nence. In this work we introduce an ef\ufb01cient pro-\ncedure to simultaneously train a NADE model for\neach possible ordering of the variables, by shar-\ning parameters across all these models. We can\nthus use the most convenient model for each infer-\nence task at hand, and ensembles of such models\nwith different orderings are immediately available.\nMoreover, unlike the original NADE, our train-\ning procedure scales to deep models. Empirically,\nensembles of Deep NADE models obtain state of\nthe art density estimation performance.\n1. Introduction\nIn probabilistic approaches to machine learning, large col-\nlections of variables are described by a joint probability\ndistribution. There is considerable interest in \ufb02exible model\ndistributions that can \ufb01t and generalize from training data\nin a variety of applications. To draw inferences from these\nmodels, we often condition on a subset of observed vari-\nables, and report the probabilities of settings of another\nsubset of variables, marginalizing out any unobserved nui-\nsance variables. The solutions to these inference tasks often\ncannot be computed exactly, and require iterative approxi-\nmations such as Monte Carlo or variational methods (e.g.,\nProceedings of the 31 st International Conference on Machine\nLearning, Beijing, China, 2014. JMLR: W&CP volume 32. Copy-\nright 2014 by the author(s).\nBishop, 2006). Models for which inference is tractable\nwould be preferable.\nNADE (Larochelle & Murray, 2011), and its real-valued\nvariant RNADE (Uria et al., 2013), have been shown to be\nstate of the art joint density models for a variety of real-\nworld datasets, as measured by their predictive likelihood.\nThese models predict each variable sequentially in an arbi-\ntrary order, \ufb01xed at training time. Variables at the beginning\nof the order can be set to observed values, i.e., conditioned\non. Variables at the end of the ordering are not required\nto make predictions; marginalizing these variables requires\nsimply ignoring them. However, marginalizing over and\nconditioning on any arbitrary subsets of variables will not\nbe easy in general.\nIn this work, we present a procedure for training a factorial\nnumber of NADE models simultaneously; one for each\npossible ordering of the variables. The parameters of these\nmodels are shared, and we optimize the mean cost over\nall orderings using a stochastic gradient technique. After\n\ufb01tting the shared parameters, we can extract, in constant\ntime, the NADE model with the variable ordering that is\nmost convenient for any given inference task. While the\ndifferent NADE models might not be consistent in their\nprobability estimates, this property is actually something we\ncan leverage to our advantage, by generating ensembles of\nNADE models \u201con the \ufb02y\u201d (i.e., without explicitly training\nany such ensemble) which are even better estimators than\nany single NADE. In addition, our procedure is able to train\na deep version of NADE incurring an extra computational\nexpense only linear in the number of layers.\n2. Background: NADE and RNADE\nAutoregressive methods use the product rule to factorize\nthe probability density function of a D-dimensional vector-\nvalued random variable x as a product of one-dimensional\narXiv:1310.1757v2  [stat.ML]  11 Jan 2014\nA Deep and Tractable Density Estimator\nconditional distributions:\np(x) =\nD\nY\nd=1\np(xod | xo<d),\n(1)\nwhere o is a D-tuple in the set of permutations of (1, . . . , D)\nthat serves as an ordering of the elements in x, xod denotes\nthe element of x indexed by the d-th element in o, and xo<d\nthe elements of x indexed by the \ufb01rst d \u22121 elements in o.\nThis factorisation of the pdf assumes no conditional indepen-\ndences. The only element constraining the modelling ability\nof an autoregressive model is the family of distributions\nchosen for each of the conditionals.\nIn the case of binary data, autoregressive models based\non logistic regressors and neural networks have been pro-\nposed (Frey, 1998; Bengio & Bengio, 2000). The neu-\nral autoregressive density estimator (NADE) (Larochelle &\nMurray, 2011), inspired by a mean-\ufb01eld approximation to\nthe conditionals of Equation (1) of a restricted Boltzmann\nmachine (RBM), uses a set of one-hidden-layer neural net-\nworks with tied parameters to calculate each conditional:\np(xod = 1 | xo<d) = sigm(V \u00b7,odhd + bod)\n(2)\nhd = sigm(W \u00b7,o<dxo<d + c),\n(3)\nwhere H is the number of hidden units, and V \u2208RH\u00d7D,\nb \u2208RD, W \u2208RH\u00d7D, c \u2208RH are the parameters of the\nNADE model.\nA NADE can be trained by regularized gradient descent on\nthe negative log-likelihood given the training dataset X.\nIn NADE the activation of the hidden units in (3) can be\ncomputed recursively:\nhd = sigm(ad)\nwhere\na1 = c\n(4)\nad+1 = ad + xodW \u00b7,od.\n(5)\nThis relationship between activations allows faster training\nand evaluation of a NADE model, O(DH), than autoregres-\nsive models based on untied neural networks, O(D2H).\nNADE has recently been extended to allow density esti-\nmation of real-valued vectors (Uria et al., 2013) by using\nmixture density networks or MDNs (Bishop, 1994) for each\nof the conditionals in Equation (1). The networks\u2019 hidden\nlayers use the same parameter sharing as before, with acti-\nvations computed as in (5).\nNADE and RNADE have been shown to offer better mod-\nelling performance than mixture models and untied neural\nnetworks in a range of datasets. Compared to binary RBMs\nwith hundreds of hidden units, NADEs usually have slightly\nworse modelling performance, but they have three desirable\nproperties that the former lack: 1) an easy training proce-\ndure by gradient descent on the negative likelihood of a\ntraining dataset, 2) a tractable expression for the density of\na datapoint, 3) a direct ancestral sampling procedure, rather\nthan requiring Markov chain Monte Carlo methods.\nInference under a NADE is easy as long as the variables\nto condition on are at the beginning of its ordering, and\nthe ones to marginalise over are at the end. To infer the\ndensity of xoa...ob while conditioning on xo1...oa\u22121, and\nmarginalising over xob+1...D, we simply write\np(xoa...b | xo1...a\u22121) =\nb\nY\nd=a\np(xod | xo<d),\n(6)\nwhere each one-dimensional conditional is directly available\nfrom the model. However, as in most models, arbitrary prob-\nabilistic queries require approximate inference methods.\nA disadvantage of NADE compared to other neural network\nmodels is that an ef\ufb01cient deep formulation (e.g., Bengio,\n2009) is not available. While extending NADE\u2019s de\ufb01nition\nto multiple hidden layers is trivial (we simply introduce\nregular feed-forward layers between the computation of\nEquation 3 and of Equation 2), we lack a recursive expres-\nsion like Equations 4 and 5 for the added layers. Thus,\nwhen NADE has more than one hidden layer, each addi-\ntional hidden layer must be computed separately for each\ninput dimension, yielding a complexity cubic on the size of\nthe layers O(DH2L), where L represents the number of lay-\ners. This scaling seemingly made a deep NADE impractical,\nexcept for datasets of low dimensionality.\n3. Training a factorial number of NADEs\nLooking at the simplicity of inference in Equation (6), a\nnaive approach that could exploit this property for any infer-\nence task would be to train as many NADE models as there\nare possible orderings of the input variables. Obviously,\nthis approach, requiring O(D!) time and memory, is not\nviable. However, we show here that through some careful\nparameter tying between models, we can derive an ef\ufb01cient\nstochastic procedure for training all models, minimizing the\nmean of their negative log-likelihood objectives.\nConsider for now a parameter tying strategy that simply\nuses the same weight matrices and bias parameters across all\nNADE models (we will re\ufb01ne this proposal later). We will\nnow write p(x | \u03b8, o) as the joint distribution of the NADE\nmodel that uses ordering o and p(x(n)\nod | x(n)\no<d, \u03b8, o<d, od) as\nits associated conditionals, which are computed as speci\ufb01ed\nin Equations (2) and (3), or their straightforward extension\nin the deep network case. Thus we explicitly treat the order-\ning o as a random variable. Notice that the dth conditional\nonly depends on the \ufb01rst d elements of the ordering, and is\nthus exactly the same across NADE models sharing their\n\ufb01rst d elements in o. During training we will attempt to\nminimise the expected (over variable orderings) negative\nA Deep and Tractable Density Estimator\nlog-likelihood of the model for the training data:\nJOA(\u03b8) =\nE\no\u2208D! \u2212log p(X | \u03b8, o)\n(7)\n\u221d\nE\no\u2208D!\nE\nx(n)\u2208X\n\u2212log p(x(n) | \u03b8, o),\n(8)\nwhere D! is the set of all orderings (i.e. permutations of D\nelements). This objective does not correspond to a mixture\nmodel, in which case the expectation over orderings would\nbe inside the log operation.\nUsing NADE\u2019s autoregressive expression for the density of\na datapoint, (8) can be rewritten as:\nJOA(\u03b8) =\nE\no\u2208D!\nE\nx(n)\u2208X\nD\nX\nd=1\n\u2212log p(x(n)\nod | x(n)\no<d, \u03b8, o).\n(9)\nWhere d indexes the elements in the order, o, of the dimen-\nsions. By moving the expectation over orders inside the sum\nover the elements of the order, the order can be split in three\nparts: o<d standing for the index of the d\u22121 \ufb01rst dimensions\nin the ordering; od the index of the d-th dimension in the\nordering, and o>d standing for the indices of the remaining\ndimensions in the ordering. Therefore, the loss function can\nbe rewritten as:\nJOA(\u03b8) = E\nx(n)\u2208X\nD\nX\nd=1\nE\no<d E\nod E\no>d\n\u2212log p(x(n)\nod | x(n)\no<d, \u03b8, o<d, od)\n(10)\nthe value of each term does not depend on o>d. Therefore,\nit can be simpli\ufb01ed as:\nJOA(\u03b8) =\nE\nx(n)\u2208X\nD\nX\nd=1\nE\no<d E\nod\n\u2212log p(x(n)\nod | x(n)\no<d, \u03b8, o<d, od)\n(11)\nIn practice, this loss function (11) will have a very high\nnumber of terms and will have to be approximated by sam-\npling x(n), d, and o<d. The innermost expectation over\nvalues of od can be calculated cheaply for a NADE given\nthat the hidden unit states hd are shared for all possible od.\nTherefore, assuming all orderings are equally probable, we\nwill estimate JOA(\u03b8) by:\nd\nJOA(\u03b8) =\nD\nD \u2212d + 1\nX\nod\n\u2212log p(x(n)\nod | x(n)\no<d, \u03b8, o<d, od)\n(12)\nwhich provides an unbiased estimator of (8). Thus training\ncan be done by descent on the stochastic gradient of d\nJOA(\u03b8).\nAn implementation of this order-agnostic training procedure\ncorresponds to an arti\ufb01cial neural network with D inputs and\nD outputs (or an MDN in the real-valued case), where the\ninput values in o\u2265d have been set to zero and gradients are\nbackpropagated only from the outputs in o\u2265d, and rescaled\nby\nD\nD\u2212d+1.\nThe end result is a stochastic training update costing\nO(DH + H2L), as in regular multilayer neural networks.\nAt test time, we unfortunately cannot avoid a complexity\nof O(DH2L) and perform D passes through the neural net-\nwork to obtain all D conditionals for some given ordering.\nHowever, this is still tractable, unlike computing probabil-\nities in a restricted Boltzmann machine or a deep belief\nnetwork.\n3.1. Improved parameter sharing using input masks\nWhile the parameter tying proposed so far is simple, in\npractice it leads to poor performance. One issue is that the\nvalues of the hidden units, computed using (3), are the same\nwhen a dimension is in xo>d (a value to be predicted) and\nwhen the value of that dimension is zero and conditioned on.\nWhen training just one NADE with a \ufb01xed o, each output\nunit knows which inputs feed into it, but in the multiple\nordering case that information is lost when the input is zero.\nIn order to make this distinction possible, we augment the\nparameter sharing scheme by appending to the inputs a\nbinary mask vector mo<d \u2208{0, 1}D indicating which di-\nmensions are present in the input. That is, the i-th element\nof mo<d is 1 if i \u2208o<d and 0 otherwise. One interpretation\nof this scheme is that the bias vector c of the \ufb01rst hidden\nlayer is now dependent on the ordering o and the value of\nd, thus slightly relaxing the strength of parameter sharing\nbetween the NADE models. We\u2019ve found in practice that\nthis adjustment is crucial to obtain good estimation perfor-\nmance. Some results showing the difference in statistical\nperformance with and without training masks can be seen in\nTable 2 as part of our experimental analysis (see Section 6\nfor details).\n4. On the \ufb02y generation of NADE ensembles\nOur order-agnostic training procedure can be thought of as\nproducing a set of parameters that can be used by a factorial\nnumber of NADEs, one per ordering of the input variables.\nThese different NADEs will not, in general, agree on the\nprobability of a given datapoint. While this disagreement\nmight look unappealing at \ufb01rst, we can actually use this\nsource of variability to our advantage, and obtain better\nestimates than possible with a set of consistent models.\nA NADE with a given input ordering corresponds to a dif-\nferent hypothesis space than other NADEs with different\nordering. In other words, each NADE with a different or-\ndering is a model in its own right, with slightly different\ninductive bias, despite the parameter sharing.\nA reliable approach to improve on some given estimator is to\nA Deep and Tractable Density Estimator\ninstead construct an ensemble of multiple, strong but differ-\nent estimators, e.g. with bagging (Ormoneit & Tresp, 1996)\nor stacking (Smyth & Wolpert, 1999). Our training proce-\ndure suggest a straightforward way of generating ensembles\nof NADE models: generate a set of uniformly distributed\norderings {o(k)}K\nk=1 over the input variables and use the\naverage probability 1\nK\nPK\nk=1 p(x|\u03b8, o(k)) as our estimator.\nEnsemble averaging increases the computational cost of\ndensity estimation linearly with the size of the ensemble,\nwhile the complexity of sampling doesn\u2019t change (we pick\nan ordering o(k) at random from the ensemble and sample\nfrom the corresponding NADE). Importantly, the computa-\ntional cost of training remains the same, unlike ensemble\nmethods such as bagging. Moreover, an adequate number\nof components can be chosen after training, and can even\nbe adapted to the available computational budget on the \ufb02y.\n5. Related work\nAs mentioned previously, autoregressive density/distribution\nestimation has been explored before by others. For the bi-\nnary data case, Frey (1998) considered the use of logistic re-\ngression conditional models, while Bengio & Bengio (2000)\nproposed a single layer neural network architecture, with\na parameter sharing scheme different from the one in the\nNADE model (Larochelle & Murray, 2011). In all these\ncases however, a single (usually random) input ordering was\nchosen and maintained during training.\nGregor & LeCun (2011) proposed training a variant of the\nNADE architecture under stochastically generated random\norderings. Like us, they observed much worse performance\nthan when choosing a single variable ordering, which mo-\ntivates our proposed parameter sharing scheme relying on\ninput masks. Gregor & LeCun generated a single ordering\nfor each training update, and conditioned on contexts of all\npossible sizes to compute the log-probability of an example\nand its gradients. Our stochastic approach uses only a single\nconditioning con\ufb01guration for each update, but computes\nthe average log-probability for the next dimension under\nall possible future orderings. This change allowed us to\ngeneralize NADE to deep architectures with an acceptable\ncomputational cost.\nGoodfellow et al. (2013) introduced a procedure to train\ndeep Boltzmann machines by maximizing a variational ap-\nproximation of their generalised pseudo likelihood. This\nresults in a training procedure similar to the one presented\nin this work, where a subset of the dimension is predicted\ngiven the value of the rest.\nOur algorithm also bears similarity with denoising autoen-\ncoders (Vincent et al., 2008) trained using so-called \u201cmask-\ning noise\u201d. There are two crucial differences however. The\n\ufb01rst is that our procedure corresponds to training on the\naverage reconstruction of only the inputs that are missing\nfrom the input layer. The second is that, unlike denoising\nautoencoders, the NADE models that we train can be used\nas tractable density estimators.\n6. Experimental results\nWe performed experiments on several binary and real-valued\ndatasets to asses the performance of NADEs trained using\nour order-agnostic procedure. We report the average test\nlog-likelihood of each model, that is, the average log-density\nof datapoints in a held-out test set. In the case of NADEs\ntrained in an order-agnostic way, we need to choose an or-\ndering of the variables so that one may calculate the density\nof the test datapoints. We report the average of the aver-\nage test log-likelihoods using ten different orderings chosen\nat random. Note that this is different from an ensemble,\nwhere the probabilities are averaged before calculating its\nlogarithm. To reduce clutter, we have not reported the stan-\ndard deviation across orderings. In all cases, this standard\ndeviation has magnitude smaller than the log-likelihood\u2019s\nstandard error due to the \ufb01nite size of our test sets. These\nstandard errors are also small enough not to alter the ranking\nof the different models. In the case of ensembles of NADEs\nthe standard deviation due to different sets of orderings is,\nas expected, even smaller. Every results table is partitioned\nin two halves, the top half contains baselines and the bot-\ntom half results obtained using our training procedure. In\nevery table the log-likelihood of the best model, and the\nlog-likelihood of the best ensemble are shown in bold.\nTraining con\ufb01guration details common to all datasets (ex-\ncept where speci\ufb01ed later on) follow. We trained all order-\nagnostic NADEs and RNADEs using minibatch stochastic\ngradient descent on JOA, (11). The initial learning rate,\nwhich was chosen independently for each dataset, was re-\nduced linearly to reach zero after the last iteration. For the\npurpose of consistency, we used recti\ufb01ed linear units (Nair\n& Hinton, 2010) in all experiments. We found that this\ntype of unit allow us to use higher learning rates and made\ntraining converge faster. We used Nesterov\u2019s accelerated\ngradient (Sutskever, 2013) with momentum value 0.9. No\nweight decay was applied. To avoid over\ufb01tting, we early-\nstopped training by estimating the log-likelihood on a val-\nidation dataset after each training iteration using the d\nJOA\nestimator, (12). For models with several hidden layers, each\nhidden layer was pretrained using the same hyperparameter\nvalues but only for 20 iterations, see recursive procedure in\nAlgorithm 1.\n6.1. Binary datasets\nWe start by measuring the statistical performance of a NADE\ntrained using our order-agnostic procedure on eight binary\nUCI datasets (Bache & Lichman, 2013).\nA Deep and Tractable Density Estimator\nTable 1. Average test-set log-likelihood per datapoint (in nats) of different models on eight binary datasets from the UCI repository.\nBaseline results were taken from Larochelle & Murray (2011).\nModel\nAdult\nConnect4\nDNA\nMushrooms\nNIPS-0-12\nOcr-letters\nRCV1\nWeb\nMoBernoullis\n\u221220.44\n\u221223.41\n\u221298.19\n\u221214.46\n\u2212290.02\n\u221240.56\n\u221247.59\n\u221230.16\nRBM\n\u221216.26\n\u221222.66\n\u221296.74\n\u221215.15\n\u2212277.37\n\u221243.05\n\u221248.88\n\u221229.38\nFVSBN\n\u221213.17\n\u221212.39\n\u221283.64\n\u221210.27\n\u2212276.88\n\u221239.30\n\u221249.84\n\u221229.35\nNADE (\ufb01xed order)\n\u221213.19\n\u221211.99\n\u221284.81\n\u22129.81\n\u2212273.08\n\u221227.22\n\u221246.66\n\u221228.39\nNADE 1hl\n\u221213.51\n\u221213.04\n\u221284.28\n\u221210.06\n\u2212275.20\n\u221229.05\n\u221246.79\n\u221228.30\nNADE 2hl\n\u221213.53\n\u221212.99\n\u221284.30\n\u221210.05\n\u2212274.69\n\u221228.92\n\u221246.71\n\u221228.28\nNADE 3hl\n\u221213.54\n\u221213.08\n\u221284.37\n\u221210.10\n\u2212274.86\n\u221228.89\n\u221246.76\n\u221228.29\nEoNADE 1hl (2 ord)\n\u221213.35\n\u221212.81\n\u221283.52\n\u22129.88\n\u2212274.12\n\u221228.36\n\u221246.50\n\u221228.11\nEoNADE 1hl (16 ord) \u221213.19\n\u221212.58\n\u221282.31\n\u22129.68\n\u2212272.38\n\u221227.31\n\u221246.12\n\u221227.87\nAlgorithm 1 Pretraining of a NADE with n hidden layers\non dataset X.\n1: procedure PRETRAIN(n, X)\n2:\nif n = 1 then\n3:\nreturn RANDOM-ONE-HIDDEN-LAYER-NADE()\n4:\nelse\n5:\nnade \u2190PRETRAIN(n \u22121)\n6:\nnade \u2190REMOVE-OUTPUT-LAYER(nade)\n7:\nnade \u2190ADD-A-NEW-HIDDEN-LAYER(nade)\n8:\nnade \u2190ADD-A-NEW-OUTPUT-LAYER(nade)\n9:\nnade \u2190TRAIN-ALL(nade, X, iters = 20)\n10:\nreturn nade\n11:\nend if\n12: end procedure\nExperimental con\ufb01guration details follow.\nWe \ufb01xed\nthe number of units per hidden layer to 500, following\nLarochelle & Murray (2011).\nWe used minibatches of\nsize 100. Training was run for 100 iterations, each con-\nsisting of 1000 weight updates. The initial learning rate\nwas cross-validated for each of the datasets among values\n{0.016, 0.004, 0.001, 0.00025, 0.0000675}.\nResults are shown on Table 1.\nWe compare our\nmethod to mixtures of multivariate Bernoullis with\ntheir number of components cross-validated among\n{32, 64, 128, 256, 512, 1024}, tractable RBMs of 23 hid-\nden units, fully visible sigmoidal Bayes networks (FVSBN),\nand NADEs trained using a \ufb01xed ordering of the variables.\nAll baseline results are taken from Larochelle & Murray\n(2011) and details can be found there. NADEs trained in an\norder-agnostic manner obtain performances close to those of\nNADEs trained on a \ufb01xed ordering. The use of several hid-\nden layers offers no advantage on these datasets. However,\nensembles of NADEs obtain higher test log-likelihoods on\nall datasets.\nWe also present results on binarized-MNIST (Salakhutdi-\nnov & Murray, 2008), a binary dataset of 28 by 28 pixel\nimages of handwritten digits. Unlike classi\ufb01cation, density\nestimation on this dataset remains a challenging task.\nExperimental con\ufb01guration details follow. Training was run\nfor 200 iterations each consisting of 1000 parameter updates,\nusing minibatches of size 1000. The initial learning rate\nwas set to 0.001 and chosen manually by optimizing the\nvalidation-set log-likelihood on preliminary runs.\nResults for MNIST are shown in Table 2. We compare our\nmethod with mixtures of multivariate Bernoulli distributions\nwith 10 and 500 components, \ufb01xed-ordering NADEs, RBMs\n(500 hidden units), and two-hidden-layer DBNs (500 and\n2000 hidden units on each layer) whose performance was\nestimated by Salakhutdinov & Murray (2008); Murray &\nSalakhutdinov (2009). In order to provide a more direct\ncomparison to our results, we also report the performance\nof NADEs trained using a \ufb01xed variable-ordering, mini-\nbatch stochastic gradient descent and sigmoid or recti\ufb01ed\nlinear units. We found the type of hidden-unit did not affect\nstatistical performance, while our minibatch SGD imple-\nmentation seems to obtain slightly higher log-likelihoods\nthan previously reported.\nOne and two hidden-layer NADEs trained by minimizing\nJOA obtain marginally lower (worse) test-likelihoods than a\nNADE trained for a \ufb01xed ordering of the inputs, but still per-\nform much better than mixtures of multivariate Bernoullis\nand very close to the estimated performance of RBMs. More\nthan two hidden layers are not bene\ufb01cial on this dataset.\nEnsembles of NADEs obtained by using NADEs with dif-\nferent variable orderings but trained simultaneously with\nour order-agnostic procedure obtain better statistical perfor-\nmance than NADEs trained using a \ufb01xed ordering. These\nEoNADEs can also surpass the estimated performance of\nRBMs with the same number of hidden units, and even\napproach the estimated performance of a (larger) 2-hidden-\nlayer deep belief network. A more detailed account of the\nstatistical performance of EoNADEs can be seen in Fig-\nure 1. We also report the performance on NADE trained by\nA Deep and Tractable Density Estimator\nTable 2. Average test-set log-likelihood per datapoint of different\nmodels on 28\u00d728 binarized images of digits taken from MNIST.\nModel\nTest LogL\nMoBernoullis K=10\n\u2212168.95\nMoBernoullis K=500\n\u2212137.64\nRBM (500 h, 25 CD steps) approx.\n\u221286.34\nDBN 2hl approx.\n\u221284.55\nNADE 1hl (\ufb01xed order)\n\u221288.86\nNADE 1hl (\ufb01xed order, RLU, minibatch)\n\u221288.33\nNADE 1hl (\ufb01xed order, sigm, minibatch)\n\u221288.35\nNADE 1hl (no input masks)\n\u221299.37\nNADE 2hl (no input masks)\n\u221295.33\nNADE 1hl\n\u221292.17\nNADE 2hl\n\u221289.17\nNADE 3hl\n\u221289.38\nNADE 4hl\n\u221289.60\nEoNADE 1hl (2 orderings)\n\u221290.69\nEoNADE 1hl (128 orderings)\n\u221287.71\nEoNADE 2hl (2 orderings)\n\u221287.96\nEoNADE 2hl (128 orderings)\n\u221285.10\n1\n2\n4\n8\n16\n32\n64\n128\nModels averaged\n\u221293\n\u221292\n\u221291\n\u221290\n\u221289\n\u221288\n\u221287\n\u221286\n\u221285\n\u221284\nTest loglikelihood (nats)\n2hl-DBN\nRBM\nNADE (\ufb01xed order)\n1hl-NADE\n2hl-NADE\nFigure 1. Test-set average log-likelihood per datapoint for\nRNADEs trained with our new procedure on binarized images\nof digits.\nminimizing JOA but without input masks. Input masks are\nnecessary for obtaining competitive results.\nSamples from a 2 hidden layer (500 hidden units per layer)\nNADE trained using the order-agnostic method are shown\nin Figure 2. Most of the samples can be identi\ufb01ed as digits.\nFigure 4 shows some receptive \ufb01elds from the model\u2019s \ufb01rst\nhidden layer (i.e. columns of W ). Most of the receptive\n\ufb01elds resemble pen strokes. We also show their associated\nreceptive \ufb01elds on the input masks . These can be thought\nof as biases that activate or deactivate a hidden unit. Most of\nthem will activate the unit when the input mask contains a\nFigure 2. Top: 50 examples from binarized-MNIST ordered by\ndecreasing likelihood under a 2-hidden-layer NADE. Bottom: 50\nsamples from a 2-hidden-layer NADE, also ordered by decreasing\nlikelihood under the model.\nregion of unknown values (zeros in the input mask) \ufb02anked\nby a region of known values (ones in the input mask).\nHaving at our disposal a NADE for each possible ordering\nof the inputs makes it easy to perform any inference task.\nIn Figure 3 we show examples of marginalization and im-\nputation tasks. Arbitrarily chosen regions of digits in the\nMNIST test-set are to be marginalized or sampled from. An\nRBM or a DBN would require an exponential number of\noperations to calculate either the marginal density or the\ndensity of the complete images. A NADE trained on a \ufb01xed\nordering of the variables would be able to easily calculate\nthe densities of the complete images, but would require\napproximate inference to calculate the marginal densities.\nBoth an RBM and a \ufb01xed-order NADE require MCMC\nmethods in order to sample the hollowed regions. However,\nwith our order-agnostic training procedure we can easily\ncalculate the marginal densities and sample the hollowed\nregions in constant time just by constructing a NADE with\na convenient ordering of the pixels.\n6.2. Real-valued datasets\nWe also compared the performance of RNADEs trained\nwith our order-agnostic procedure to RNADEs trained for\na \ufb01xed ordering. We start by comparing the performance\non three low-dimensional UCI datasets (Bache & Lichman,\n2013) of heterogeneous data, namely: red wine, white wine\nand parkinsons. We dropped the other two datasets tested\nA Deep and Tractable Density Estimator\n-61.21\n-36.33\n-84.40\n-46.22\n-96.68\n-66.26\n-86.37\n-73.31\n-93.35\n-79.40\n-45.84\n-41.88\nFigure 3. Example of marginalization and sampling. First column\nshows \ufb01ve examples from the test set of the MNIST dataset. The\nsecond column shows the density of these examples when a random\n10 by 10 pixel region is marginalized. The right-most \ufb01ve columns\nshow samples for the hollowed region. Both tasks can be done\neasily with a NADE where the pixels to marginalize are at the end\nof the ordering.\nFigure 4. Top:50 receptive \ufb01elds (columns of W ) with the biggest\nL2 norm. Bottom: Associated receptive \ufb01elds to the input masks.\nby Uria et al. (2013), because some of their dimensions\nonly take a \ufb01nite number of values even if those are real-\nvalued. We report the test-log-likelihood on 10 folds of\nthe dataset, each with 90% of the data used for training\nand 10% for testing. All experiments use normalized data.\nEach dimension is normalized separately by subtracting its\ntraining-set average and dividing by its standard deviation.\nExperimental details follow. Learning rate and weight decay\nrates were chosen by per-fold cross-validation; using grid\nsearch. One ninth of the training set examples were used\nfor validation purposes. Once the hyperparameter values\nhad been chosen, a \ufb01nal experiment was run using all the\ntraining data. In order to prevent over\ufb01tting, training was\nstopped when observing a training likelihood higher than\nthe one obtained at the optimal stopping point in the corre-\nsponding validation run. All RNADEs trained had a mixture\nof 20 Gaussian components for output, and were trained by\nstochastic gradient descent on JOA. We \ufb01xed the number of\nhidden units to 50, following Uria et al. (2013). The learning\nrate was chosen among {0.02, 0.005, 0.002, 0.0005} and\nthe weight decay rate among {0.02, 0.002, 0}.\nThe results are shown in Table 3. RNADEs trained using our\nprocedure obtain results close to those of RNADEs trained\nfor a \ufb01xed ordering on the red wine and white wine datasets.\nOn the Parkinsons dataset, RNADEs trained for a \ufb01xed\nordering perform better. Ensembles of RNADEs obtained\nbetter statistical performance on the three datasets.\nWe also measured the performance of our new training pro-\ncedure on 8 by 8 patches of natural images in the BSDS300\ndataset. We compare the performance of RNADEs with\ndifferent number of hidden layers trained with our proce-\ndure against a one-hidden layer RNADE trained for a \ufb01xed\nordering (Uria et al., 2013), and with mixtures of Gaussians,\nwhich remain the state of the art in this problem (Zoran &\nWeiss, 2012).\nWe adopted the setup described by Uria et al. (2013).\nThe average intensity of each patch was subtracted from\neach pixel\u2019s value. After this, all datapoints lay on a 63-\ndimensional subspace, for this reason only 63 pixels were\nmodelled, discarding the bottom-right pixel.\nExperimental details follow. The dataset\u2019s 200 training\nimage set was partitioned into a training set and a validation\nset of 180 and 20 images respectively. Hyperparameters\nwere chosen by preliminary manual search on the model\nlikelihood for the validation dataset. We used a mixture\nof 10 Gaussian components for the output distribution of\neach pixel. All hidden layers were \ufb01xed to a size of 1000\nunits. The minibatch size was set to 1000. Training was run\nfor 2000 iterations, each consisting of 1000 weight updates.\nThe initial learning rate was set to 0.001. Pretraining of\nhidden layers was done for 50 iterations.\nThe results are shown in Table 4. RNADEs with less than\n3 hidden layers trained using our order-agnostic procedure\nobtained lower statistical performance than a \ufb01xed-ordering\nNADE and a mixture of Gaussians. However RNADEs with\nmore than 3 layers are able to beat both baselines and obtain\nwhat are, to the extent of our knowledge, the best results\nA Deep and Tractable Density Estimator\nTable 3. Average test log-likelihood for different models on three real-valued UCI datasets. Baselines are taken from (Uria et al., 2013).\nModel\nRed wine\nWhite wine\nParkinsons\nGaussian\n\u221213.18\n\u221213.20\n\u221210.85\nMFA\n\u221210.19\n\u221210.73\n\u22121.99\nRNADE (\ufb01xed)\n\u22129.36\n\u221210.23\n\u22120.90\nRNADE 1hl\n\u22129.49\n\u221210.35\n\u22122.67\nRNADE 2hl\n\u22129.63\n\u221210.23\n\u22122.19\nRNADE 3hl\n\u22129.54\n\u221210.21\n\u22122.13\nRNADE 1hl 2 ord.\n\u22129.07\n\u221210.03\n\u22121.97\nRNADE 2hl 2 ord.\n\u22129.13\n\u22129.84\n\u22121.42\nRNADE 3hl 2 ord.\n\u22128.93\n\u22129.79\n\u22121.39\nRNADE 1hl 16 ord.\n\u22128.95\n\u22129.94\n\u22121.73\nRNADE 2hl 16 ord.\n\u22128.98\n\u22129.69\n\u22121.16\nRNADE 3hl 16 ord.\n\u22128.76\n\u22129.67\n\u22121.13\nTable 4. Average test-set log-likelihood for several models trained\non 8 by 8 pixel patches of natural images taken from the BSDS300\ndataset. Note that because these are log probability densities they\nare positive, higher is better.\nModel\nTest LogL\nMoG K =200 (Zoran & Weiss, 2012)\n152.8\nRNADE 1hl (\ufb01xed order)\n152.1\nRNADE 1hl\n143.2\nRNADE 2hl\n149.2\nRNADE 3hl\n152.0\nRNADE 4hl\n153.6\nRNADE 5hl\n154.7\nRNADE 6hl\n155.2\nEoRNADE 6hl 2 ord.\n156.0\nEoRNADE 6hl 32 ord.\n157.0\never reported on this task. Ensembles of RNADEs also show\nan improvement in statistical performance compared to the\nuse of single RNADEs.\nNo signs of over\ufb01tting were observed. Even when using\n6 hidden layers, the cost on the validation dataset never\nstarted increasing steadily during training. Therefore it may\nbe possible to obtain even better results using more hidden\nlayers or more hidden units per layer. Samples from the 6\nhidden layers NADE trained in an order-agnostic manner\nare shown in Figure 5.\n7. Conclusions\nWe have introduced a new training procedure that simul-\ntaneously \ufb01ts a NADE for each possible ordering of the\ndimensions. In addition, this new training procedure is able\nto train deep versions of NADE with a linear increase in\ncomputation, and construct ensembles of NADEs on the \ufb02y\nwithout incurring any extra training computational cost.\nFigure 5. Top: 50 examples of 8 \u00d7 8 patches in the BSDS300\ndataset ordered by decreasing likelihood under a 6-hidden-layer\nNADE. Bottom: 50 samples from a 6-hidden-layer NADE.\nNADEs trained with our procedure outperform mixture mod-\nels in all datasets we have investigated. However, for most\ndatasets several hidden layers are required to surpass or\nequal the performance of NADEs trained for a \ufb01xed order-\ning of the variables. Nonetheless, our method allows fast\nand exact marginalization and sampling, unlike the rest of\nthe methods compared.\nModels trained using our order-agnostic procedure obtained\nwhat are, to the best of our knowledge, the best statistical\nperformances ever reported on the BSDS300 8\u00d78-image-\npatches datasets. The use of ensembles of NADEs, which\nwe can obtain at no extra training cost and have a mild effect\non test-time cost, improved statistical performance on most\ndatasets analyzed.\nACKNOWLEDGMENTS\nWe thank John Bridle and Steve Renals for useful discussions.\nA Deep and Tractable Density Estimator\nReferences\nBache, K. and Lichman, M. UCI machine learning reposi-\ntory, 2013. http://archive.ics.uci.edu/ml.\nBengio, Y. Learning deep architectures for AI. Foundations\nand trends in Machine Learning, 2(1):1\u2013127, 2009.\nBengio, Y. and Bengio, S. Modeling high-dimensional\ndiscrete data with multi-layer neural networks. In Ad-\nvances in Neural Information Processing Systems 12\n(NIPS 1999), pp. 400\u2013406. MIT Press, 2000.\nBishop, C. M. Mixture density networks. Technical report,\nNeural Computing Research Group, Aston University,\n1994.\nBishop, C. M. Pattern recognition and machine learning.\nSpringer\u2013Verlag, New York, 2006. ISBN 0387310738.\nFrey, B. J. Graphical models for machine learning and\ndigital communication. MIT Press, 1998.\nGoodfellow, I., Mirza, M., Courville, A., and Bengio, Y.\nMulti-prediction deep Boltzmann machines. In Advances\nin Neural Information Processing Systems 26, pp. 548\u2013\n556. 2013.\nGregor, K. and LeCun, Y. Learning representations by max-\nimizing compression. Technical report, arXiv:1108.1169,\n2011.\nLarochelle, H. and Murray, I. The Neural Autoregressive\nDistribution Estimator. In Proceedings of the 14th Inter-\nnational Conference on Arti\ufb01cial Intelligence and Statis-\ntics (AISTATS 2011), volume 15, pp. 29\u201337, Ft. Laud-\nerdale, USA, 2011. JMLR W&CP.\nMurray, I. and Salakhutdinov, R.\nEvaluating probabili-\nties under high-dimensional latent variable models. In\nAdvances in Neural Information Processing Systems 21\n(NIPS 2008), pp. 1137\u20131144, 2009.\nNair, V. and Hinton, G. E. Recti\ufb01ed linear units improve re-\nstricted Boltzmann machines. In Proceedings of the 27th\nInternational Conference on Machine Learning (ICML-\n10), pp. 807\u2013814, 2010.\nOrmoneit, D. and Tresp, V. Improved Gaussian mixture den-\nsity estimates using Bayesian penalty terms and network\naveraging. In Advances in Neural Information Processing\nSystems 8 (NIPS 1995), pp. 542\u2013548. MIT Press, 1996.\nSalakhutdinov, R. and Murray, I. On the quantitative analy-\nsis of deep belief networks. In Proceedings of the 25th\nInternational Conference on Machine Learning (ICML\n2008), volume 25, pp. 872\u2013879, 2008.\nSmyth, P. and Wolpert, D. Linearly combining density\nestimators via stacking. Machine Learning, 36(1-2):59\u2013\n83, 1999.\nSutskever, I. Training recurrent neural networks. PhD\nthesis, University of Toronto, 2013.\nUria, B., Murray, I., and Larochelle, H. RNADE: The\nreal-valued neural autoregressive density-estimator. In\nAdvances in Neural Information Processing Systems 26\n(NIPS 26), 2013. To appear. arXiv:1306.0186.\nVincent, P., Larochelle, H., Bengio, Y., and Manzagol, P. A.\nExtracting and composing robust features with denoising\nautoencoders. In Proceedings of the 25th International\nConference on Machine Learning (ICML 2008), pp. 1096\u2013\n1103, Helsinki, Finland, 2008. ACM.\nZoran, D. and Weiss, Y. Natural images, Gaussian mixtures\nand dead leaves. In Advances in Neural Information Pro-\ncessing Systems 25 (NIPS 2012), pp. 1745\u20131753, 2012.\n",
        "sentence": " 39 EoNADE[15] 13.",
        "context": "NADE 3hl\n\u221213.54\n\u221213.08\n\u221284.37\n\u221210.10\n\u2212274.86\n\u221228.89\n\u221246.76\n\u221228.29\nEoNADE 1hl (2 ord)\n\u221213.35\n\u221212.81\n\u221283.52\n\u22129.88\n\u2212274.12\n\u221228.36\n\u221246.50\n\u221228.11\nEoNADE 1hl (16 ord) \u221213.19\n\u221212.58\n\u221282.31\n\u22129.68\n\u2212272.38\n\u221227.31\n\u221246.12\n\u221227.87\n\u221213.18\n\u221213.20\n\u221210.85\nMFA\n\u221210.19\n\u221210.73\n\u22121.99\nRNADE (\ufb01xed)\n\u22129.36\n\u221210.23\n\u22120.90\nRNADE 1hl\n\u22129.49\n\u221210.35\n\u22122.67\nRNADE 2hl\n\u22129.63\n\u221210.23\n\u22122.19\nRNADE 3hl\n\u22129.54\n\u221210.21\n\u22122.13\nRNADE 1hl 2 ord.\n\u22129.07\n\u221210.03\n\u22121.97\nRNADE 2hl 2 ord.\n\u22129.13\n\u22129.84\n\u22121.42\nRNADE 3hl 2 ord.\nNADE 1hl\n\u221292.17\nNADE 2hl\n\u221289.17\nNADE 3hl\n\u221289.38\nNADE 4hl\n\u221289.60\nEoNADE 1hl (2 orderings)\n\u221290.69\nEoNADE 1hl (128 orderings)\n\u221287.71\nEoNADE 2hl (2 orderings)\n\u221287.96\nEoNADE 2hl (128 orderings)\n\u221285.10\n1\n2\n4\n8\n16\n32\n64\n128\nModels averaged\n\u221293\n\u221292\n\u221291\n\u221290\n\u221289"
    },
    {
        "title": "Deep autoregressive networks",
        "author": [
            "Karol Gregor",
            "Ivo Danihelka",
            "Andriy Mnih",
            "Charles Blundell",
            "Daan Wierstra"
        ],
        "venue": "In Proceedings of the 31st International Conference on Machine Learning,",
        "citeRegEx": "16",
        "shortCiteRegEx": "16",
        "year": 2014,
        "abstract": "We introduce a deep, generative autoencoder capable of learning hierarchies\nof distributed representations from data. Successive deep stochastic hidden\nlayers are equipped with autoregressive connections, which enable the model to\nbe sampled from quickly and exactly via ancestral sampling. We derive an\nefficient approximate parameter estimation method based on the minimum\ndescription length (MDL) principle, which can be seen as maximising a\nvariational lower bound on the log-likelihood, with a feedforward neural\nnetwork implementing approximate inference. We demonstrate state-of-the-art\ngenerative performance on a number of classic data sets: several UCI data sets,\nMNIST and Atari 2600 games.",
        "full_text": "Deep AutoRegressive Networks\nKarol Gregor\nKAROLG@GOOGLE.COM\nIvo Danihelka\nDANIHELKA@GOOGLE.COM\nAndriy Mnih\nAMNIH@GOOGLE.COM\nCharles Blundell\nCBLUNDELL@GOOGLE.COM\nDaan Wierstra\nWIERSTRA@GOOGLE.COM\nGoogle DeepMind\nAbstract\nWe introduce a deep, generative autoencoder ca-\npable of learning hierarchies of distributed rep-\nresentations from data. Successive deep stochas-\ntic hidden layers are equipped with autoregres-\nsive connections, which enable the model to be\nsampled from quickly and exactly via ancestral\nsampling.\nWe derive an ef\ufb01cient approximate\nparameter estimation method based on the mini-\nmum description length (MDL) principle, which\ncan be seen as maximising a variational lower\nbound on the log-likelihood, with a feedforward\nneural network implementing approximate infer-\nence. We demonstrate state-of-the-art generative\nperformance on a number of classic data sets:\nseveral UCI data sets, MNIST and Atari 2600\ngames.\n1. Introduction\nDirected generative models provide a fully probabilistic ac-\ncount of observed random variables and their latent rep-\nresentations. Typically either the mapping from observa-\ntion to representation or representation to observation is in-\ntractable and hard to approximate ef\ufb01ciently. In contrast,\nautoencoders provide an ef\ufb01cient two-way mapping where\nan encoder maps observations to representations and a de-\ncoder maps representations back to observations. Recently\nseveral authors (Ranzato et al., 2007; Vincent et al., 2008;\nVincent, 2011; Rifai et al., 2012; Bengio et al., 2013b)\nhave developed probabilistic versions of regularised au-\ntoencoders, along with means of generating samples from\nsuch models. These sampling procedures are often itera-\ntive, producing correlated approximate samples from pre-\nvious approximate samples, and as such explore the full\nProceedings of the 31 st International Conference on Machine\nLearning, Beijing, China, 2014. JMLR: W&CP volume 32. Copy-\nright 2014 by the author(s).\ndistribution slowly, if at all.\nIn this paper, we introduce Deep AutoRegressive Networks\n(DARNs), deep generative autoencoders that in contrast\nto the aforementioned models ef\ufb01ciently generate indepen-\ndent, exact samples via ancestral sampling. To produce a\nsample, we simply perform a top-down pass through the\ndecoding part of our model, starting at the deepest hidden\nlayer and sampling one unit at a time, layer-wise. Train-\ning a DARN proceeds by minimising the total informa-\ntion stored for reconstruction of the original input, and\nas such follows the minimum description length principle\n(MDL; Rissanen, 1978). This amounts to backpropagating\nan MDL cost through the entire joint encoder/decoder.\nThere is a long history of viewing autoencoders through\nthe lens of MDL (Hinton & Van Camp, 1993; Hinton &\nZemel, 1994), yet this has not previously been consid-\nered in the context of deep autoencoders. MDL provides\na sound theoretical basis for DARN\u2019s regularisation, whilst\nthe justi\ufb01cation of regularised autoencoders was not im-\nmediately obvious.\nLearning to encode and decode ob-\nservations according to a compression metric yields rep-\nresentations that can be both concise and irredundant from\nan information theoretic point of view. Due to the equiv-\nalence of compression and prediction, compressed repre-\nsentations are good for making predictions and hence also\ngood for generating samples. Minimising the description\nlength of our model coincides exactly with minimising the\nHelmholtz variational free energy, where our encoder plays\nthe role of the variational distribution. Unlike many other\nvariational learning algorithms, our algorithm is not an ex-\npectation maximisation algorithm, but rather a stochastic\ngradient descent method, jointly optimising all parameters\nof the autoencoder simultaneously.\nDARN and its learning algorithm easily stack, allowing\never deeper representations to be learnt, whilst at the same\ntime compressing the training data \u2014 DARN allows for\nalternating layers of stochastic hidden units and determin-\nistic non-linearities. Each stochastic layer within DARN is\narXiv:1310.8499v2  [cs.LG]  20 May 2014\nDeep AutoRegressive Networks\nFigure 1. Left: DARN\u2019s decoder as a generative model. Top-\ndown, ancestral sampling through DARN\u2019s decoder starts with\nthe deepest stochastic hidden layer H2, sampling each unit in turn\nbefore proceeding downwards to lower layers, ending by produc-\ning an observation X. Centre: DARN\u2019s encoder as inference.\nConditioned upon the observation X, and sampling left-to-right,\nbottom-up, DARN\u2019s encoder infers the representation H1, H2 of\nan observation. Right: DARN as an autoencoder. During train-\ning, the encoder infers a suitable representation H1, H2 and the\ndecoder predicts this representation and the observation. The pa-\nrameters of the encoder and decoder are then simultaneously min-\nimised with respect to the implied coding cost. This cost equals\nthe Helmholtz variational free energy.\nautoregressive: each unit receives input both from the pre-\nceding layer and the preceding units within the same layer.\nAutoregressive structure captures much of the dependence\namong units within the same layer, at very little compu-\ntational cost during both learning and generation. This is\nin marked contrast to other mechanisms for lateral connec-\ntions, such as introducing within-layer undirected edges,\nwhich often come at a prohibitively high computational\ncost at training and/or generation time.\nRecently, several authors have exploited autoregression for\ndistribution modelling (Larochelle & Murray, 2011; Gre-\ngor & LeCun, 2011; Uria et al., 2013). Unlike these mod-\nels, DARN can have stochastic hidden units, and places\nautoregressive connections among these hidden units. De-\npending upon the architecture of the network, this can yield\ngains in both statistical and computational ef\ufb01ciency.\nThe remainder of the paper is structured as follows. In Sec-\ntion 2 we describe the architecture of our model, Section 3\nreviews the minimum description length principle and its\napplication to autoencoders. Section 4 describes the ap-\nproximate parameter estimation algorithm. Section 5 has\nthe results of our model on several data sets, and we con-\nclude with a brief summary in Section 6.\n2. Model Architecture\nOur model is a deep, generative autoencoder; an example is\nshown in Figure 1 with two hidden layers. DARN has three\ncomponents: the encoder q(H|X) that picks a representa-\ntion H for a given observation X, a decoder prior p(H)\nwhich provides a prior distribution on representations H\nfor generation, and a decoder conditional p(X|H) which,\ngiven a representation H, produces an observation X. We\nshall use uppercase letters for random variables and lower-\ncase for their values. We shall \ufb01rst describe our model with\na single stochastic hidden layer and later show how this is\neasily generalised to multiple layers.\nWe begin by describing the decoder prior on the repre-\nsentation h.\nThe decoder prior on the representation h\nis an autoregressive model.\nLet h1:j denote the vector\n(h1, h2, . . . , hj) where each hi \u2208{0, 1}, then\np(h) =\nnh\nY\nj=1\np(hj|h1:j\u22121)\n(1)\nwhere h = (h1, h2, . . . , hnh) denotes the representation\nwith nh hidden stochastic units. p(hj|h1:j\u22121) is the prob-\nability mass function of hj conditioned upon the activities\nof the previous units in the representation h1:j\u22121.\nIn DARN, we parameterise the conditional probability\nmass of hj in a variety of ways, depending upon the com-\nplexity of the problem. Logistic regression is the simplest:\np(Hj = 1|h1:j\u22121) = \u03c3(W (H)\nj\n\u00b7 h1:j\u22121 + b(H)\nj\n),\n(2)\nwhere \u03c3(x) =\n1\n1+e\u2212x . The parameters are W (H)\nj\n\u2208Rj\u22121,\nwhich is the weight vector, and b(H)\nj\n\u2208R, which is a bias\nparameter.\nThe conditional distributions of the decoder p(X|H) and\nof the encoder q(H|X) have similar forms:\np(x|h) =\nnx\nY\nj=1\np(xj|x1:j\u22121, h),\n(3)\nq(h|x) =\nnx\nY\nj=1\nq(hj|h1:j\u22121, x),\n(4)\nwhere, as with the decoder prior, the conditional probabil-\nity mass functions can be parameterised as in Eq. 2 (we\nshall explore some more elaborate parameterisations later).\nConsequently,\np(Xj = 1|x1:j\u22121, h) = \u03c3(W (X|H)\nj\n\u00b7 (x1:j\u22121, h) + b(X|H)\nj\n)\n(5)\nq(Hj = 1|h1:j\u22121, x) = \u03c3(W (H|X)\nj\n\u00b7 x + b(H|X)\nj\n)\n(6)\nwhere (x1:j\u22121, h) denotes the concatenation of the vec-\ntor x1:j\u22121 with the vector h, W (X|H)\nj\n\u2208Rj\u22121+nh and\nW (H|X)\nj\n\u2208Rnx are weight vector parameters and b(H|X)\nj\nand b(X|H)\nj\nare the scalar biases. Whilst in principle, Eq. 6\ncould be made autoregressive, we shall typically choose not\nto do so, as this can have signi\ufb01cant computational advan-\ntages, as we shall see later in Section 2.3.\nDeep AutoRegressive Networks\n2.1. Deeper Architectures\nThe simple model presented so far is already a universal\ndistribution approximator \u2014 it can approximate any (rea-\nsonable) distribution given suf\ufb01cient capacity. As adding\nextra hidden layers to models such as deep belief networks\nstrictly improves their representational power (Le Roux &\nBengio, 2008), we could ask whether that is also the case\nfor DARN. Although every distribution on H may be writ-\nten as Eq. 1, not every factorisation can be parameterised as\nEq. 2. Thus we propose boosting DARN\u2019s representational\npower in three ways: by adding stochastic hidden layers, by\nadding deterministic hidden layers, and by using alternate\nkinds of autoregressivity. We now consider each approach\nin turn.\nAdditional\nstochastic\nhidden\nlayers.\nWe\ncon-\nsider\nan\nautoencoder\nwith\nhidden\nstochastic\nlayers\nH(1), . . . , H(nlayers)\neach\nwith\nn(1)\nh , . . . , n(nlayers)\nh\nunits,\nrespectively. For convenience we denote the input layer\nby H(0) = X and let H(nlayers+1) = \u2205. The decoder and\nencoder probability distributions become\np(H(l)|H(l+1)) =\nn(l)\nh\nY\nj=1\np(H(l)\nj |H(l)\n1:j\u22121, H(l+1)),\n(7)\nq(H(k)|H(k\u22121)) =\nn(k)\nh\nY\nj=1\nq(H(k)\nj\n|H(k)\n1:j\u22121, H(k\u22121))\n(8)\nfor l = 0, . . . , nlayers and k = 1, . . . , nlayers.\nAdditional deterministic hidden layers.\nThe second\nway of adding complexity is to insert more complicated\ndeterministic functions between the stochastic layers. This\napplies both to the encoder and the decoder. If we wished\nto add just one deterministic hidden layer, we could use a\nsimple multi-layer perceptron such as:\nd(l) = tanh(Uh(l+1))\n(9)\np(H(l)\nj\n= 1|h(l)\n1:j\u22121, h(l+1))\n= \u03c3(W (H)\nj\n\u00b7 (h(l)\n1:j\u22121, d(l)) + b(H)\nj\n).\n(10)\nwhere U \u2208Rnd\u00d7n(l+1)\nh\nis a weight matrix, nd is the num-\nber of deterministic hidden units, W (H)\nj\n\u2208Rj\u22121+nd is a\nweight vector, and b(H)\nj\nis a scalar bias.\nAlternate kinds of autoregressivity.\nFinally, we can in-\ncrease representational power by using more \ufb02exible au-\ntoregressive models, such as NADE (Larochelle & Mur-\nray, 2011) and EoNADE (Uria et al., 2013), instead of the\nsimple linear autoregressivity we proposed in the previous\nsection.\nThe amount of information that can be stored in the rep-\nresentation H is upper bounded by the number of stochas-\ntic hidden units. Additional deterministic hidden units do\nnot introduce any extra random variables and so cannot in-\ncrease the capacity of the representation, whilst additional\nstochastic hidden units can.\n2.2. Local connectivity\nDARN can be made to scale to high-dimensional (image)\ndata by restricting connectivity, both between adjacent lay-\ners, and autoregressively, within layers. This is particularly\nuseful for modelling larger images. Local connectivity can\nbe either fully convolutional (LeCun et al., 1998) or use\nless weight sharing. In this paper we use the periodic local\nconnectivity of Gregor & LeCun (2010) which uses less\nweight sharing than full convolutional networks.\n2.3. Sampling\nSampling in DARN is simple and ef\ufb01cient as it is just an-\ncestral sampling. We start with the top-most layer, sample\nthe \ufb01rst hidden unit h1 \u223cp(H(nlayers)\n1\n) and then for each i\nin turn, we sample hi \u223cp(H(nlayers)\ni\n|h(nlayers)\n1:i\u22121 ). We repeat\nthis procedure for each successive layer until we reach the\nobservation. Sampling from the encoder works in exactly\nthe same way but in the opposite direction, sampling each\nhidden unit from q(H(l)\ni |h(l)\n1:i\u22121, h(l\u22121)) successively.\nA DARN without a stochastic hidden layer but with an au-\ntoregressive visible layer is a fully visible sigmoid belief\nnetwork (FVSBN; Frey, 1998). Thus FVSBN sampling\nscales as O(n2\nx). In NADE (Larochelle & Murray, 2011;\nGregor & LeCun, 2011), autoregression is present in the\nvisibles, but only deterministic hidden units are used. Sam-\npling then scales as O(nxnd) where nx is the number of\nvisibles and nd is the number of deterministic hidden units.\nThe complexity of sampling from a fully autoregressive\nsingle stochastic hidden layer DARN is O((nh + nx)2). If\nwe omit the autoregressivity on the observations, we obtain\na time complexity of O(nh(nx + nh)). Furthermore, if the\nstochastic hidden layer is sparse, such that at ns units are\nactive on average, we obtain an expected time complex-\nity of O(ns(nx + nh)). We call this sparse, fast version\nfDARN. As more stochastic or deterministic hidden lay-\ners are added to DARN, the advantage of DARN becomes\ngreater as each part of the decoder need only be com-\nputed once for DARN per datum, wheras deeper NADE-\nlike models (Uria et al., 2013) require re-computation of\nlarge parts of the model for each unit.\nDeep AutoRegressive Networks\n3. Minimum Description Length and\nAutoencoders\nAutoencoders have previously been trained by an MDL\nprinciple derived from bits-back coding (Hinton &\nVan Camp, 1993).\nThis yields generative autoencoders\ntrained to minimise the Helmholtz variational free energy\n(Hinton & Zemel, 1994).\nHere we extend this work to\ndeeper, autoregressive models trained by a stochastic ap-\nproximation to backpropagation as opposed to expectation\nmaximisation.\nAccording to the MDL principle, we shall train this autoen-\ncoder by \ufb01nding parameters that try to maximally compress\nthe training data. Suppose a sender wishes to communicate\na binary sequence of nx elements, x, to a receiver. We\nshall \ufb01rst sample a representation of nh binary elements,\nh, to communicate and then send the residual of x relative\nto this representation. The idea is that the representation\nhas a more concise code than the original datum x and so\ncan be compressed effectively: for example, by arithmetic\ncoding (MacKay, 2003).\nThe description length of a random variable taking a par-\nticular value indicates how many bits must be used to com-\nmunicate that particular value. Shannon\u2019s source coding\ntheorem shows that the description length is equal to the\nnegative logarithm of the probability of the random vari-\nable taking that particular value (MacKay, 2003). Hence,\nwhen communicating a datum x, having already commu-\nnicated its representation h, the description length would\nbe\nL(x|h) = \u2212log2 p(x|h).\n(11)\nWe wish for the parameters of the autoencoder to compress\nthe data well on average, and so we shall minimise the ex-\npected description length,\nL(x) =\nX\nh\nq(h|x)(L(h) + L(x|h)),\n(12)\nwhere L(h) denotes the description length of the represen-\ntation h, and q(h|x) is the encoder probability of the repre-\nsentation h. As we are using bits-back coding, the descrip-\ntion length of the representation h is\nL(h) = \u2212log2 p(h) + log2 q(h|x).\n(13)\nSubstituting Eq. 11 and Eq. 13 into Eq. 12 we recover the\nHelmholtz variational free energy:\nL(x) = \u2212\nX\nh\nq(h|x)(log2 p(x, h) \u2212log2 q(h|x)). (14)\nPicking the parameters of q(H|X) and p(X, H) to min-\nimise the description length in Eq. 14 yields a coding\nscheme that requires the fewest expected number of bits\nto communicate a datum x and its representation h.\nAs Eq. 14 is the variational free energy, the encoder\nq(H|X) that minimises Eq. 14 is the posterior p(H|X).\nVariational learning methods sometimes refer to the neg-\native expected description length \u2212L(x) as the expected\nlower bound as it serves as a lower bound upon log2 p(x).\nNote here that we shall be interested in optimising the pa-\nrameters of q(H|X) and p(X, H) simultaneously, whereas\nvariational learning often only optimises the parameters of\nq(H|X) and p(X, H) by co-ordinate descent.\n4. Learning\nLearning in DARN amounts to jointly training weights and\nbiases \u03b8 of both the encoder and the decoder, simultane-\nously, to minimise Eq. 12. The procedure is based on gra-\ndient descent by backpropagation and is based upon a num-\nber of approximations to the gradient of Eq. 12.\nWe write the expected description length in Eq. 12 as:\nL(x) =\n1\nX\nh1=0\nq(h1|x) \u00b7 \u00b7 \u00b7\n1\nX\nhnh=0\nq(hnh|h1:nh\u22121, x)\n\u00d7 (log2 q(h|x) \u2212log2 p(x, h))\n(15)\nCalculating Eq. 15 exactly is intractable. Hence we shall\nuse a Monte carlo approximation.\nLearning proceeds as follows:\n1. Given an observation x, sample a representation h \u223c\nq(H|x) (see Section 2.3).\n2. Calculate q(h|x) (Eq. 4), p(x|h) (Eq. 3) and p(h)\n(Eq. 1) for the sampled representation h and given ob-\nservation x.\n3. Calculate the gradient of Eq. 15.\n4. Update the parameters of the autoencoder by follow-\ning the gradient \u2207\u03b8L(x).\n5. Repeat.\nWe now turn to calculating the gradient of Eq. 15 which\nrequires backpropagation of the MDL cost through the\njoint encoder/decoder. Unfortunately, this pass through the\nmodel includes stochastic units. Backpropagating gradi-\nents through stochastic binary units na\u00a8\u0131vely yields gradients\nthat are highly biased, yet often work well in practice (Hin-\nton, 2012). Whilst it is possible to derive estimators that\nare unbiased (Bengio et al., 2013a), their empirical perfor-\nmance is often unsatisfactory. In this work, we backpropa-\ngate gradients through stochastic binary units, and then re-\nDeep AutoRegressive Networks\nweight these gradients to reduce bias and variance. Details\nare given in Appendix A.\nFinally note that when the encoder is not autoregressive,\nthe entire system can be trained using standard matrix oper-\nations and point-wise nonlinearities. Hence it is easily im-\nplementable on graphical processing units. The decoder\u2019s\nautoregressive computation is expressed as a full matrix\nmultiplication with a triangular matrix.\n5. Results\nWe trained our models on binary UCI data sets, MNIST\ndigits and frames from \ufb01ve Atari 2600 games (Bellemare\net al., 2013).\nThe quantitative results reported here are in terms of the\nprobability the decoder assigns to a test datum:\np(x).\nFor small DARN models, we can evaluate the likelihood\np(x) exactly by iterating over every possible representa-\ntion: p(x) = P\nh p(x, h). As the computational cost of\nthis sum grows exponentially in the size of the representa-\ntion, for DARN models with more than 16 stochastic hid-\nden units, we use an importance sampling estimate using\nthe encoder distribution:\np(x) \u22481\nS\nS\nX\ns=1\np(x, h(s))\nq(h(s)|x) ,\nh(s) \u223cq(H|x),\n(16)\nwhere s indexes one of S samples. As this estimate can\nhave high variance, we repeat the estimation ten times and\nreport the 95 per cent con\ufb01dence interval. In our experi-\nments, the variance of the estimator was low. Where avail-\nable, we used a validation set to choose the learning rate\nand certain aspects of the model architecture, such as the\nnumber of hidden units. We used the Monte Carlo approx-\nimation to expected description length in Eq. 15 of the val-\nidation set to select these.\n5.1. Binary UCI data sets\nWe evaluated the test-set performance of DARN on eight\nbinary data sets from the UCI repository (Bache & Lich-\nman, 2013). In Table 1, we compare DARN to baseline\nmodels from Uria et al. (2013).\nWe used a DARN with two hidden layers. The \ufb01rst layer\nwas deterministic, with tanh activations. The second layer\nwas a stochastic layer with an autoregressive prior p(H).\nThe decoder conditional p(X|H) included autoregressive\nconnections.\nThe architecture and learning rate was selected by cross-\nvalidation on a validation set for each data set. The num-\nber of deterministic hidden units was selected from 100 to\n500, in steps of 100, whilst the number of stochastic hid-\nden units was selected from {8, 12, 16, 32, 64, 128, 256}.\nWe used RMSprop (Graves, 2013) with momentum 0.9 and\nlearning rates 0.00025, 0.0000675 or 10\u22125. The network\nwas trained with minibatches of size 100. The best results\nare shown in bold in Table 1. DARN achieved better test\nlog-likelihood on four of eight data sets than the baseline\nmodels reported in Uria et al. (2013). We found that regu-\nlarisation by adaptive weight noise on these small data sets\n(Graves, 2011) did not yield good results, but early stop-\nping based on the performance on the validation set worked\nwell.\n5.2. Binarised MNIST data set\nWe evaluated the sampling and test-set performance of\nDARN on the binarised MNIST data set (Salakhutdinov &\nMurray, 2008), which consists of 50, 000 training, 10, 000\nvalidation, and 10, 000 testing images of hand-written dig-\nits (Larochelle & Murray, 2011). Each image is 28 \u00d7 28\npixels.\nWe used two hidden layers, one deterministic, one stochas-\ntic. The results are in Table 2 with nh denoting the number\nof stochastic hidden units. The deterministic layer had 100\nunits for architectures with 16 or fewer stochastic units per\nlayer, and 500 units for more than 16 stochastic units. The\ndeterministic activation function was taken to be the tanh\nfunction. We used no autoregressivity for the observation\nlayer \u2014 the decoder conditional is a product of independent\nBernoulli distributions, conditioned upon the representa-\ntion. Training was done with RMSprop (Graves, 2013),\nmomentum 0.9 and minibatches of size 100. We used a\nlearning rate of 3\u00d710\u22125. Adaptive weight noise (Graves,\n2011), denoted by \u201cadaNoise\u201d in Table 2, was used to avoid\nthe need for early stopping.\nAfter training, we were able to measure the exact log-\nlikelihood for networks with 16 or fewer stochastic hid-\nden units.\nFor the network with 500 hidden units,\nwe estimated the log-likelihood by importance sampling\ngiven by the above procedure.\nFor each test example,\nwe sampled 100, 000 latent representations from the en-\ncoder distribution. The estimate was repeated ten times;\nwe report the estimated 95 per cent con\ufb01dence inter-\nvals. The obtained log-likelihoods and con\ufb01dence inter-\nvals are given in Table 2 along with those of other mod-\nels.\nDARN performs favourably compared to the other\nmodels. For example, a DARN with just 9 stochastic hid-\nden units obtains almost the same log-likelihood as a mix-\nture of Bernoullis (MoBernoullis) with 500 components:\nlog2 500 \u22489. DARN with 500 stochastic hidden units\ncompares favourably to state-of-the-art generative perfor-\nmance of deep Boltzmann machines (DBM;\nSalakhut-\ndinov & Hinton, 2009) and deep belief networks (DBN;\nSalakhutdinov & Murray, 2008; Murray & Salakhutdinov,\n2009). Notably, DARN\u2019s upper bound, the expected de-\nDeep AutoRegressive Networks\nTable 1. Log likelihood (in nats) per test-set example on the eight UCI data sets.\nModel\nAdult\nConnect4\nDNA\nMushrooms\nNIPS-0-12\nOcr-letters\nRCV1\nWeb\nMoBernoullis\n20.44\n23.41\n98.19\n14.46\n290.02\n40.56\n47.59\n30.16\nRBM\n16.26\n22.66\n96.74\n15.15\n277.37\n43.05\n48.88\n29.38\nFVSBN\n13.17\n12.39\n83.64\n10.27\n276.88\n39.30\n49.84\n29.35\nNADE (\ufb01xed order)\n13.19\n11.99\n84.81\n9.81\n273.08\n27.22\n46.66\n28.39\nEoNADE 1hl (16 ord.)\n13.19\n12.58\n82.31\n9.68\n272.38\n27.31\n46.12\n27.87\nDARN\n13.19\n11.91\n81.04\n9.55\n274.68\n28.17 \u00b1 0\n46.10 \u00b1 0\n28.83 \u00b1 0\nFigure 2. Left: Samples from DARN paired with the nearest training example from binarised MNIST. The generated samples are not\nsimple memorisation of the training examples. Right: Sample probabilities from DARN trained on pixel intensities directly.\nscription length given in the far right column, is lower than\nthe likelihood of NADE whilst DARN\u2019s estimated log like-\nlihood is lower than the log-likelihood of the best reported\nEoNADE (Uria et al., 2013) results.\nWe performed several additional experiments. First, we\ntrained fDARN with 400 hidden units and 5% sparsity, re-\nsulting in an upper bound negative log-likelihood of 96.1\n(Table 2). The estimated speed of generation was 2.4\u00d7104\nmultiplications per sample, which compares favourably to\nNADE\u2019s of 2.2 \u00d7 106, a nearly 100 fold speedup. While\nthe likelihood is worse, the samples appear reasonable by\nocular inspection.\nNext, we trained a very deep, 12 stochastic layer DARN\nwith 80 stochastic units in each layer, and 400 tanh units\nin each deterministic layer. Here we also used skip connec-\ntions where each tanh layer received input from all previous\nstochastic layers. Due to computational constraints we only\nevaluated the upper bound of this architecture as reported\nin Table 2 \u2014 where it records the best upper bound among\nall DARN models, showing the value of depth in DARN.\nFinally, we trained a network with one stochastic layer, 400\nunits, and one tanh layer (1000 units) in both encoder and\ndecoder on the pixel intensities directly, rather than bina-\nrising the data set. We show the sample probabilities of\nobservables in Figure 2(right).\n5.3. Atari 2600 game frames\nWe recorded 100, 000 frames from \ufb01ve different Atari 2600\ngames (Bellemare et al., 2013) from random play, record-\ning each frame with 1% probability. We applied an edge\ndetector to the images yielding frames of 159\u00d7209 pixels1.\nFrames of these games generated by DARN are shown in\nFigure 4.\nTo scale DARN to these larger images, we used three hid-\nden layers. The stack of layers contains a locally connected\nlayer with weight-sharing (see Section 2.2), a recti\ufb01ed lin-\near activation function, another locally connected layer fol-\nlowed by a recti\ufb01ed linear function and a fully connected\nlayer followed by 300 stochastic binary units.\nThe au-\ntoregressive prior on the representation was also fully con-\nnected. The locally connected layers had 32 \ufb01lters with a\nperiod of 8. The \ufb01rst locally connected layer used stride 4\nand kernel size 8. The second locally connected layer used\nstride 2 and kernel size 4. The autoregressive connections\n1The\nscript\nto\ngenerate\nthe\ndataset\nis\navailable\nat\nhttps://github.com/fidlej/aledataset\nDeep AutoRegressive Networks\nTable 2. Log likelihood (in nats) per test-set example on the bina-\nrised MNIST data set. The right hand column, where present, rep-\nresents the expected description length (in Eq. 15) which serves\nas an upper bound on the log-likelihood for DARN.\nModel\n\u2212log p\n\u2264\nMoBernoullis K=10\n168.95\nMoBernoullis K=500\n137.64\nRBM (500 h, 25 CD steps)\n\u224886.34\nDBM 2hl\n\u224884.62\nDBN 2hl\n\u224884.55\nNADE 1hl (\ufb01xed order)\n88.33\nEoNADE 1hl (128 orderings)\n87.71\nEoNADE 2hl (128 orderings)\n85.10\nDARN nh=9, adaNoise\n138.84\n145.36\nDARN nh=10, adaNoise\n133.70\n140.86\nDARN nh=16, adaNoise\n122.80\n130.94\nDARN nh=500\n84.71 \u00b1 0.01\n90.31\nDARN nh=500, adaNoise\n84.13 \u00b1 0.01\n88.30\nfDARN nh = 400\n-\n96.1\ndeep DARN\n-\n87.72\nFigure 3. Samples from networks of different depths.\nLeft:\nFrames generated from the network described in Figure 5. Mid-\ndle: Frames generated from the network with the same sizes but\nwithout the fully-connected top layer. Right: Frames generated\nfrom the network with the top two layers removed.\nfor the visible layer used a period of 14 and kernel size 15.\nThe games in Figure 4 are ordered from left to right, in de-\ncreasing order of log probability. While DARN captures\nmuch of the structure in these games, such as the scores,\nvarious objects, and correlations between objects, when the\ngame becomes less regular, as with River Raid (second to\nright) and Sea Quest (far right), DARN is not able to gen-\nerate reasonable frames.\nFigure 5 shows the representation that DARN learns for a\ntypical frame of the game Freeway. To show the effect of\ndeeper layers in DARN, Figure 3 shows the frames DARN\ngenerates using the representation from different depths. In\nthe game of Freeway, several cars (the white blobs) travel\nalong lanes, delimited by dashed white lines. Here we used\na DARN with three stochastic hidden layers. When DARN\nwas trained using a sparsity penalty on the activations, as\nwe described in Section 2.3, it learnt a representation where\nthe second hidden layer captures the rough outline of each\ncar, whereas the \ufb01rst hidden layer \ufb01lled in the details of\neach car. A global bias learns the background image. A\nthird hidden layer decides where to place the cars. All lay-\ners except the very top layer are locally connected. The top\nlayer has 100 units. The second hidden layer was locally\nconnected with a kernel of size 21\u00d721, whilst the \ufb01rst hid-\nden layer was locally connected with a kernel of size 7\u00d77.\n6. Conclusion\nIn this paper we introduced deep autoregressive networks\n(DARN), a new deep generative architecture with autore-\ngressive stochastic hidden units capable of capturing high-\nlevel structure in data to generate high-quality samples.\nThe method, like the ubiquitous autoencoder framework,\nis comprised of not just a decoder (the generative element)\nbut a stochastic encoder as well to allow for ef\ufb01cient and\ntractable inference. Training proceeds by backpropagating\nan MDL cost through the joint model, which approximately\nequates to minimising the Helmholtz variational free en-\nergy. This procedure necessitates backpropagation through\nstochastic units, as such yielding an approximate Monte\nCarlo method. The model samples ef\ufb01ciently, trains ef\ufb01-\nciently and is scalable to locally connected and/or convolu-\ntional architectures. Results include state-of-the-art perfor-\nmance on multiple data sets.\nA. Derivation of Gradients\nWe derive the gradient of the objective function with re-\nspect to the inputs to a stochastic binary hidden unit. Let\nq(hi) be the probability distribution of the ith hidden unit,\nf(hi) be the part of the network which takes hi as an input.\nThe expected value of f(hi) and its gradient are:\nE [f(Hi)] =\n1\nX\nhi=0\nq(hi)f(hi)\n(17)\n\u2207\u03b8E [f(Hi)] =\n1\nX\nhi=0\nq(hi)\u2207\u03b8 log q(hi)f(hi)\n(18)\nwhere f does not depend directly on the inputs \u03b8 of the\nq(hi) distribution. We use a Monte Carlo approximation\nto estimate \u2207\u03b8E [f(Hi)] where hi is sampled from q(hi)\n(Williams, 1992; Andrad\u00b4ottir, 1998). Monte Carlo approxi-\nmations of the gradient are unbiased but can have high vari-\nance. To combat the high variance, we introduce a baseline\nb, inspired by control variates (Paisley et al., 2012):\n\u2207\u03b8E [f(Hi)] \u2248E\nh\n[\n\u2207\u03b8F\ni\n(19)\n[\n\u2207\u03b8F = \u2207\u03b8 log q(hi)(f(hi) \u2212b)\n(20)\nDeep AutoRegressive Networks\nFigure 4. Samples from a locally connected DARN paired with the nearest train-\ning example. Some of the generated frames have novel combinations of objects\nand scores. Columns from left to right, along with upper bound negative log-\nlikelihood on the test set: Freeway (19.9), Pong (23.7), Space Invaders (113.0),\nRiver Raid (139.4), Sea Quest (217.9).\nFigure 5. Bottom: An input frame from the\ngame Freeway. Lower Middle: Activations in\nthe encoder from the \ufb01rst hidden layer. Upper\nMiddle: Activations in the encoder from the\nsecond hidden layer. Top: Each of 25 rows is\nan activation of the fully-connected third hidden\nlayer with 100 units.\nwhere [\n\u2207\u03b8F denotes our estimator. A good baseline should\nbe correlated with f(hi), have low variance, and also be\nsuch that the expected value of \u2207\u03b8 log q(hi)b is zero to\nget an unbiased estimate of the gradient. We chose a non-\nconstant baseline. The baseline will be a \ufb01rst-order Taylor\napproximation of f. We can get the \ufb01rst-order derivatives\nfrom backpropagation. The baseline is a Taylor approxi-\nmation of f about hi, evaluated at a point h\u2032\ni:\nb(hi) = f(hi) + df(hi)\ndhi\n(h\u2032\ni \u2212hi)\n(21)\nTo satisfy the unbiasedness requirement, we need to solve\nthe following equation for h\u2032\ni:\n0 =\n1\nX\nhi=0\nq(hi)\u2207\u03b8 log q(hi)(f(hi) + df(hi)\ndhi\n(h\u2032\ni \u2212hi))\n(22)\nThe solution depends on the shape of f. If f is a linear\nfunction, any h\u2032\ni can be used. If f is a quadratic function,\nthe solution is h\u2032\ni = 1\n2. If fi is a cubic or higher-order func-\ntion, the solution depends on the coef\ufb01cients of the polyno-\nmial. We will use h\u2032\ni = 1\n2 and our estimator will be biased\nfor non-quadratic functions.\nBy substituting the baseline into Eq. 20 we obtain the \ufb01nal\nform of our estimator of the gradient:\n[\n\u2207\u03b8F = \u2207\u03b8 log q(hi)df(hi)\ndhi\n(hi \u22121\n2)\n(23)\n= \u2207\u03b8q(Hi = 1)\n2q(hi)\ndf(hi)\ndhi\n(24)\nAn implementation can estimate the gradient with respect\nto q(Hi = 1) by backpropagating with respect to hi and\nscaling the gradient by\n1\n2q(hi), where hi is the sampled bi-\nnary value.\nReferences\nAndrad\u00b4ottir, Sigr\u00b4un.\nA review of simulation optimiza-\ntion techniques. In Simulation Conference Proceedings,\n1998. Winter, volume 1, pp. 151\u2013158. IEEE, 1998.\nBache, K. and Lichman, M. UCI machine learning repos-\nDeep AutoRegressive Networks\nitory, 2013.\nURL http://archive.ics.uci.\nedu/ml.\nBellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M.\nThe arcade learning environment: An evaluation plat-\nform for general agents. Journal of Arti\ufb01cial Intelligence\nResearch, 47:253\u2013279, 06 2013.\nBengio, Yoshua, L\u00b4eonard, Nicholas, and Courville, Aaron.\nEstimating or propagating gradients through stochastic\nneurons for conditional computation.\narXiv preprint\narXiv:1308.3432, 2013a.\nBengio, Yoshua, Yao, Li, Alain, Guillaume, and Vincent,\nPascal. Generalized denoising auto-encoders as genera-\ntive models. arXiv preprint arXiv:1305.6663, 2013b.\nFrey, Brendam J. Graphical models for machine learning\nand digital communication. The MIT press, 1998.\nGraves, Alex. Practical variational inference for neural net-\nworks. In Advances in Neural Information Processing\nSystems, pp. 2348\u20132356, 2011.\nGraves, Alex. Generating sequences with recurrent neural\nnetworks. arXiv preprint arXiv:1308.0850, 2013.\nGregor, Karol and LeCun, Yann. Emergence of complex-\nlike cells in a temporal product network with local re-\nceptive \ufb01elds. arXiv preprint arXiv:1006.0448, 2010.\nGregor, Karol and LeCun, Yann.\nLearning represen-\ntations by maximizing compression.\narXiv preprint\narXiv:1108.1169, 2011.\nHinton, Geoffrey. Neural networks for machine learning.\nCoursera video lectures, 2012.\nHinton, Geoffrey E and Van Camp, Drew.\nKeeping the\nneural networks simple by minimizing the description\nlength of the weights. In Proceedings of the sixth annual\nconference on Computational learning theory, pp. 5\u201313,\n1993.\nHinton, Geoffrey E and Zemel, Richard S. Autoencoders,\nminimum description length, and helmholtz free energy.\nAdvances in Neural Information Processsing Systems,\npp. 3\u20133, 1994.\nLarochelle, Hugo and Murray, Iain. The neural autoregres-\nsive distribution estimator. Journal of Machine Learning\nResearch, 15:29\u201337, 2011.\nLe Roux, Nicolas and Bengio, Yoshua. Representational\npower of restricted boltzmann machines and deep belief\nnetworks. Neural computation, 20(6):1631\u20131649, 2008.\nLeCun, Yann, Bottou, L\u00b4eon, Bengio, Yoshua, and Haffner,\nPatrick.\nGradient-based learning applied to document\nrecognition.\nProceedings of the IEEE, 86(11):2278\u2013\n2324, 1998.\nMacKay, David JC.\nInformation theory, inference and\nlearning algorithms, Ch 6. Cambridge university press,\n2003.\nMurray, Iain and Salakhutdinov, Ruslan. Evaluating prob-\nabilities under high-dimensional latent variable models.\nIn Advances in Neural Information Processing Systems,\npp. 1137\u20131144. 2009.\nPaisley, John, Blei, David, and Jordan, Michael. Variational\nbayesian inference with stochastic search. arXiv preprint\narXiv:1206.6430, 2012.\nRanzato, Marc\u2019Aurelio, Boureau, Y-lan, and Cun, Yann L.\nSparse feature learning for deep belief networks. In Ad-\nvances in Neural Information Processing Systems, pp.\n1185\u20131192, 2007.\nRifai, Salah, Bengio, Yoshua, Dauphin, Yann N, and Vin-\ncent, Pascal. A generative process for sampling contrac-\ntive auto-encoders. In Proceedings of the 29th Interna-\ntional Conference on Machine Learning (ICML-12), pp.\n1855\u20131862, 2012.\nRissanen, Jorma. Modeling by shortest data description.\nAutomatica, 14(5):465\u2013471, 1978.\nSalakhutdinov, Ruslan and Hinton, Geoffrey E. Deep boltz-\nmann machines. In International Conference on Arti\ufb01-\ncial Intelligence and Statistics, pp. 448\u2013455, 2009.\nSalakhutdinov, Ruslan and Murray, Iain. On the quantita-\ntive analysis of deep belief networks. In Proceedings of\nthe 25th International Conference on Machine Learning,\npp. 872\u2013879. ACM, 2008.\nUria, Benigno, Murray, Iain, and Larochelle, Hugo.\nA\ndeep and tractable density estimator.\narXiv preprint\narXiv:1310.1757, 2013.\nVincent, Pascal.\nA connection between score matching\nand denoising autoencoders. Neural computation, 23(7):\n1661\u20131674, 2011.\nVincent, Pascal, Larochelle, Hugo, Bengio, Yoshua, and\nManzagol, Pierre-Antoine.\nExtracting and composing\nrobust features with denoising autoencoders. In Proceed-\nings of the 25th International Conference on Machine\nLearning, pp. 1096\u20131103, 2008.\nWilliams, Ronald J. Simple statistical gradient-following\nalgorithms for connectionist reinforcement learning.\nMachine learning, 8(3-4):229\u2013256, 1992.\n",
        "sentence": " 87 DARN[16] 13.",
        "context": "87.71\nEoNADE 2hl (128 orderings)\n85.10\nDARN nh=9, adaNoise\n138.84\n145.36\nDARN nh=10, adaNoise\n133.70\n140.86\nDARN nh=16, adaNoise\n122.80\n130.94\nDARN nh=500\n84.71 \u00b1 0.01\n90.31\nDARN nh=500, adaNoise\n84.13 \u00b1 0.01\n88.30\nfDARN nh = 400\n-\n96.1\ndeep DARN\n-\n87.72\n9.68\n272.38\n27.31\n46.12\n27.87\nDARN\n13.19\n11.91\n81.04\n9.55\n274.68\n28.17 \u00b1 0\n46.10 \u00b1 0\n28.83 \u00b1 0\nFigure 2. Left: Samples from DARN paired with the nearest training example from binarised MNIST. The generated samples are not\nright) and Sea Quest (far right), DARN is not able to gen-\nerate reasonable frames.\nFigure 5 shows the representation that DARN learns for a\ntypical frame of the game Freeway. To show the effect of\ndeeper layers in DARN, Figure 3 shows the frames DARN"
    },
    {
        "title": "Evaluating probabilities under high-dimensional latent variable models",
        "author": [
            "Iain Murray",
            "Ruslan Salakhutdinov"
        ],
        "venue": "In NIPS\u201908,",
        "citeRegEx": "17",
        "shortCiteRegEx": "17",
        "year": 2009,
        "abstract": "",
        "full_text": "",
        "sentence": " We use the MNIST dataset that was binarized according to [17] and downloaded in binarized from [18].",
        "context": null
    },
    {
        "title": "Binarized mnist dataset. http://www.cs.toronto.edu/ \u0303larocheh/ public/datasets/binarized_mnist/binarized_mnist_[train|valid|test] .amat, 2011. URL http://www.cs.toronto.edu/ \u0303larocheh/public/datasets/ binarized_mnist/binarized_mnist_train.amat",
        "author": [
            "Hugo Larochelle"
        ],
        "venue": null,
        "citeRegEx": "18",
        "shortCiteRegEx": "18",
        "year": 2011,
        "abstract": "",
        "full_text": "",
        "sentence": " We use the MNIST dataset that was binarized according to [17] and downloaded in binarized from [18].",
        "context": null
    },
    {
        "title": "The Toronto face dataset",
        "author": [
            "Joshua Susskind",
            "Adam Anderson",
            "Geoffrey E. Hinton"
        ],
        "venue": "Technical Report UTML TR 2010-001, U. Toronto,",
        "citeRegEx": "19",
        "shortCiteRegEx": "19",
        "year": 2010,
        "abstract": "",
        "full_text": "",
        "sentence": " We also trained models on the 98058 examples from the unlabeled section of the Toronto face database (TFD, [19]).",
        "context": null
    },
    {
        "title": "Annealed importance sampling",
        "author": [
            "Radford M. Neal"
        ],
        "venue": "Statistics and Computing,",
        "citeRegEx": "20",
        "shortCiteRegEx": "20",
        "year": 2001,
        "abstract": "Simulated annealing - moving from a tractable distribution to a distribution\nof interest via a sequence of intermediate distributions - has traditionally\nbeen used as an inexact method of handling isolated modes in Markov chain\nsamplers. Here, it is shown how one can use the Markov chain transitions for\nsuch an annealing sequence to define an importance sampler. The Markov chain\naspect allows this method to perform acceptably even for high-dimensional\nproblems, where finding good importance sampling distributions would otherwise\nbe very difficult, while the use of importance weights ensures that the\nestimates found converge to the correct values as the number of annealing runs\nincreases. This annealed importance sampling procedure resembles the second\nhalf of the previously-studied tempered transitions, and can be seen as a\ngeneralization of a recently-proposed variant of sequential importance\nsampling. It is also related to thermodynamic integration methods for\nestimating ratios of normalizing constants. Annealed importance sampling is\nmost attractive when isolated modes are present, or when estimates of\nnormalizing constants are required, but it may also be more generally useful,\nsince its independent sampling allows one to bypass some of the problems of\nassessing convergence and autocorrelation in Markov chain samplers.",
        "full_text": "",
        "sentence": " timate Z could certainly be made by applying AIS [20] or RAISE [21].",
        "context": null
    },
    {
        "title": "Accurate and conservative estimates of MRF log-likelihood using reverse annealing",
        "author": [
            "Yuri Burda",
            "Roger B. Grosse",
            "Ruslan Salakhutdinov"
        ],
        "venue": "CoRR, abs/1412.8566,",
        "citeRegEx": "21",
        "shortCiteRegEx": "21",
        "year": 2014,
        "abstract": "",
        "full_text": "",
        "sentence": " timate Z could certainly be made by applying AIS [20] or RAISE [21].",
        "context": null
    },
    {
        "title": "Theano: a CPU and GPU math expression compiler",
        "author": [
            "James Bergstra",
            "Olivier Breuleux",
            "Fr\u00e9d\u00e9ric Bastien",
            "Pascal Lamblin",
            "Razvan Pascanu",
            "Guillaume Desjardins",
            "Joseph Turian",
            "David Warde-Farley",
            "Yoshua Bengio"
        ],
        "venue": "In Proc. SciPy,",
        "citeRegEx": "22",
        "shortCiteRegEx": "22",
        "year": 2010,
        "abstract": "",
        "full_text": "",
        "sentence": " We thank the developers of Theano [22] and Blocks [23] for their awesome work.",
        "context": null
    },
    {
        "title": "Blocks and Fuel: Frameworks for deep learning",
        "author": [
            "B. van Merri\u00ebnboer",
            "D. Bahdanau",
            "V. Dumoulin",
            "D. Serdyuk",
            "D. Warde-Farley",
            "J. Chorowski",
            "Y. Bengio"
        ],
        "venue": "ArXiv e-prints,",
        "citeRegEx": "23",
        "shortCiteRegEx": "23",
        "year": 2015,
        "abstract": "We introduce two Python frameworks to train neural networks on large\ndatasets: Blocks and Fuel. Blocks is based on Theano, a linear algebra compiler\nwith CUDA-support. It facilitates the training of complex neural network models\nby providing parametrized Theano operations, attaching metadata to Theano's\nsymbolic computational graph, and providing an extensive set of utilities to\nassist training the networks, e.g. training algorithms, logging, monitoring,\nvisualization, and serialization. Fuel provides a standard format for machine\nlearning datasets. It allows the user to easily iterate over large datasets,\nperforming many types of pre-processing on the fly.",
        "full_text": "arXiv:1506.00619v1  [cs.LG]  1 Jun 2015\nBlocks and Fuel\nBlocks and Fuel: Frameworks for deep learning\nBart van Merri\u00a8enboer\nbart.van.merrienboer@umontreal.ca\nMontreal Institute for Learning Algorithms, University of Montreal, Montreal, Canada\nDzmitry Bahdanau\nd.bahdanau@jacobs-university.de\nJacobs University, Bremen, Germany\nVincent Dumoulin\ndumouliv@iro.umontreal.ca\nDmitriy Serdyuk\nserdyuk@iro.umontreal.ca\nDavid Warde-Farley\nwardefar@iro.umontreal.ca\nMontreal Institute for Learning Algorithms, University of Montreal, Montreal, Canada\nJan Chorowski\njan.chorowski@ii.uni.wroc.pl\nUniversity of Wroc law, Wroc law, Poland\nYoshua Bengio\nyoshua.bengio@umontreal.ca\nMontreal Institute for Learning Algorithms, University of Montreal, Montreal, Canada\nCIFAR Senior Fellow\nAbstract\nWe introduce two Python frameworks to train neural networks on large datasets: Blocks and\nFuel. Blocks is based on Theano, a linear algebra compiler with CUDA-support (Bastien et al.,\n2012; Bergstra et al., 2010). It facilitates the training of complex neural network models by\nproviding parametrized Theano operations, attaching metadata to Theano\u2019s symbolic com-\nputational graph, and providing an extensive set of utilities to assist training the networks,\ne.g. training algorithms, logging, monitoring, visualization, and serialization. Fuel provides\na standard format for machine learning datasets. It allows the user to easily iterate over\nlarge datasets, performing many types of pre-processing on the \ufb02y.\nKeywords:\nNeural networks, GPGPU, large-scale machine learning\n1. Introduction\nBlocks and Fuel are being developed by the Montreal Institute of Learning Algorithms\n(MILA) at the University of Montreal. Their focus lies on quick prototyping of complex\nneural network models. The intended target audience is researchers who design and exper-\niment machine learning algorithms, especially deep learning algorithms.\nSeveral other libraries built on top of Theano exist, including Pylearn2 and GroundHog\n(also developed by MILA), Lasagne, and Keras. Like its MILA-developed predecessors,\nBlocks maintains a focus on research and rapid prototyping. Blocks di\ufb00erentiates itself most\nnotably from the above mentioned toolkits in its unique relationship with Theano. Instead\nof introducing new abstract objects representing \u2018models\u2019 or \u2018layers\u2019, Blocks annotates the\nTheano computational graph, maintaining the \ufb02exibility of Theano while making large\nmodels manageable.\nData processing is an integral part of training neural networks, which is not addressed\nby many of the aforementioned frameworks. Fuel aims to \ufb01ll this gap. It provides tools to\ndownload datasets and iterate/preprocess them e\ufb03ciently.\n1\nVan Merri\u00a8enboer et al.\nBoth Blocks and Fuel were developed from the very beginning with a strong focus on\nsoftware engineering best practices. The development teams strive for high test coverage,\nthorough documentation and carefully considered APIs.\n2. Blocks\nBlocks comprises several components, which can be used independently from each other.\n2.1 Bricks\nTheano is a popular choice for the implementation of neural networks (see e.g. Goodfellow et al.\n(2013b); Pascanu et al. (2013)). Blocks and many other libraries, such as Pylearn2 (Goodfellow et al.,\n2013a), build on Theano by providing reusable components that are common in neural\nnetworks, such as linear transformations followed by non-linear activations, or more com-\nplicated components such as LSTM units. In Blocks these components are referred to as\nbricks or \u201cparametrized Theano operations\u201d.\nBricks consist of a set of Theano shared variables, for example the weight matrix of a\nlinear transformation or the \ufb01lters of a convolutional layer. Bricks use these parameters to\ntransform symbolic Theano variables.\nBricks can contain other bricks within them. This introduces a hierarchy on top of the\n\ufb02at computational graph de\ufb01ned by Theano, which makes it easier to address and con\ufb01gure\ncomplex models programmatically.\nThe parameters of bricks can be initialized using a variety of schemes that are popular\nin the neural network literature, such as sparse initialization, orthogonal initialization for\nrecurrent weights, etc.\nBlocks comes with a large number of \u2018bricks\u2019. Besides standard activations and transfor-\nmations used in feedforward networks (maxout, convolutional layers, table lookups) these\nalso include a variety of more advanced recurrent neural network components like LSTM,\nGRU, and support for attention mechanisms (for an overview of di\ufb00erent kinds of net-\nwork architectures, regularization methods, and optimization algorithms see Bengio et al.\n(2015)).\n2.2 Graph management\nLarge neural networks can often result in Theano computational graphs containing hundreds\nof variables and operations. Blocks does not attempt to abstract away this complex graph,\nbut to make it manageable by annotating variables in the graph. Each input, output, and\nparameter of a brick is annotated as such. Variables can also be annotated with the role\nthey play in a model, such as weights, biases, \ufb01lters, etc.\nA series of convenience tools were written that allow users to \ufb01lter the symbolic compu-\ntational graph based on these annotations, and apply transformations to the graph. Many\nregularization methods such as weight decay, weight noise, or dropout can be implemented\nin a generic, model-agnostic way. Furthermore a complex query mechanism allows for their\n\ufb01ne-grained application such as \u201capply weight noise to all weights that belong to an LSTM\nunit whose parent is a brick with the name foo\u201d.\n2\nBlocks and Fuel\n2.3 Training algorithms\nThe gradient descent training algorithm in Blocks is composed of di\ufb00erent \u2018step rules\u2019\nthat modify the descent direction (learning rate scaling, momentum, gradient clipping,\nweight norm clipping, etc.). A variety of algorithms such as AdaGrad, ADADELTA, Adam,\nRMSProp are available as step rules.\n2.4 Training\nExperiment management is performed using a \u2018main loop\u2019, which combines a Theano graph\nwith a training algorithm and a Fuel data stream. The main loop has a \ufb02exible extension\ninterface, which is used to perform tasks such as monitoring on a validation set, serialization,\nlearning rate scheduling, plotting, printing and saving logs, etc.\n3. Fuel\nFuel\u2019s goal is to provide a common interface to a variety of data formats and published\ndatasets such as MNIST, CIFAR-10, ImageNet, etc. while making it easy for users to write\nan interface to new datasets.\nBlocks relies on Fuel for its data interface, but Fuel can easily be used by other machine\nlearning frameworks that interface with datasets.\n3.1 Iteration and preprocessing pipeline\nFuel allows for di\ufb00erent ways of iterating over these datasets, such as sequential or shuf-\n\ufb02ed minibatches, support for in-memory and out-of-core datasets, and resampling (cross\nvalidation, bootstrapping).\nIt also provides a variety of on-the-\ufb02y preprocessing methods such as random crop-\nping of images, creating n-grams from text \ufb01les, and the ability to implement many other\nmethods easily. These preprocessing steps can be chained together to form more complex\ntransformations of the input data.\nTo sidestep Python\u2019s global interpreter lock (GIL) and ensure optimal performance,\nFuel can perform all operations in a separate process, transferring the processed data to the\ntraining process using TCP sockets.\n3.2 Standardized data format\nDatasets are distributed in a wide range of formats.\nFuel simpli\ufb01es dataset storage by\nconverting all built-in datasets to annotated HDF5 \ufb01les (The HDF Group, 1997-2015). In\naddition to being an e\ufb03cient format for large datasets that don\u2019t \ufb01t into memory, HDF5\nis easy to organize and document. All of the data is stored in a single HDF5 \ufb01le, with the\nfollowing metadata attached:\n\u2022 What are the data sources available (e.g. features, targets, etc.)?\n\u2022 How are these data sources o\ufb03cially split (e.g. training, validation, and test sets)?\n\u2022 Are some data sources unavailable for some splits (e.g. test set only o\ufb00ers unlabeled\nexamples)?\n3\nVan Merri\u00a8enboer et al.\n\u2022 What are the axes semantics for a given data source (e.g. batch, feature, width, height,\nchannel, time, etc.)?\nIntegrating user data into Fuel via HDF5 is straightforward, and simply requires the\ndata to be written to an HDF5 \ufb01le with metadata according to the speci\ufb01cations. Finally,\nwhile standardizing by convention on HDF5, the Fuel dataset API is independent of it;\nusers are free to implement dataset objects employing other backends and use them with\nthe rest of Fuel\u2019s components.\n3.3 Automated data management\nFuel o\ufb00ers built-in scripts that automate the task of downloading datasets, (similar to e.g.\nskdata1) and converting them to Fuel\u2019s HDF5 speci\ufb01cation.\nThe fuel\u2212download script is used to download raw data \ufb01les. Downloading the raw\nMNIST data \ufb01les is as easy as typing fuel\u2212download mnist. The fuel\u2212convert script is\nused to convert raw data \ufb01les into HDF5-format.\nReproducibility being an important feature of both Fuel and Blocks, the fuel\u2212convert\nscript automatically tags all \ufb01les it creates with relevant module and interface versions and\nthe exact command that was used to generate these \ufb01les. Inspection of this metadata is\ndone with the fuel\u2212info script.\n4. Serialization and checkpointing\nThe training of large, deep neural networks can often take days or even weeks. Hence, regular\ncheckpointing of training progress is important. Blocks aims to make the resumption of\nexperiments entirely transparent, even across platforms, while ensuring the reproducibility\nof these experiments.\nThis goal is complicated by shortcomings in Python\u2019s Pickle serialization module,\nwhich is unable to serialize many iterators, which Fuel heavily depends on in order to\niterate over large datasets e\ufb03ciently. To circumvent this we reimplemented the itertools\nmodule from the Python standard library to be serializable2.\nAs a result, Blocks experiments are able to be interrupted in the middle of a pass over\nthe dataset, serialized, and resumed later, without a\ufb00ecting the \ufb01nal training results.\n5. Documentation and community\nBlocks and Fuel are well documented, with both API documentation and tutorials available\nonline. Two active mailing lists3 support users of the libraries. A separate repository4 is\nmaintained for users to contribute non-trivial examples of the use of Blocks. Implementa-\ntions of neural machine translation models (NMT, Bahdanau et al. (2015)) and the Deep\nRecurrent Attentive Writer (DRAW, Gregor et al. (2015)) model are publicly available ex-\namples of state-of-the-art models succesfully implemented using Blocks.\n1. https://jaberg.github.io/skdata/\n2. https://github.com/mila-udem/picklable-itertools\n3. https://groups.google.com/d/forum/blocks-usersand https://groups.google.com/d/forum/fuel-users\n4. https://github.com/mila-udem/blocks-examples\n4\nBlocks and Fuel\nAcknowledgments\nThe authors would like to acknowledge the support of the following agencies for research\nfunding and computing support: NSERC, Calcul Qu\u00b4ebec, Compute Canada, the Canada\nResearch Chairs and CIFAR. Bahdanau thanks Planet Intelligent Systems GmbH for their\n\ufb01nancial support. We would also like to thank the developers of Theano.\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by\njointly learning to align and translate. In Proceedings of the International Conference on\nLearning Representations (ICLR), 2015.\nFr\u00b4ed\u00b4eric Bastien, Pascal Lamblin, Razvan Pascanu, James Bergstra, Ian J. Goodfellow,\nArnaud Bergeron, Nicolas Bouchard, and Yoshua Bengio. Theano: new features and speed\nimprovements.\nNIPS Workshop: Deep Learning and Unsupervised Feature Learning,\n2012.\nYoshua Bengio, Ian J. Goodfellow, and Aaron Courville. Deep learning. Book in preparation\nfor MIT Press, 2015. URL http://www.iro.umontreal.ca/\u02dcbengioy/dlbook.\nJames Bergstra, Olivier Breuleux, Fr\u00b4ed\u00b4eric Bastien, Pascal Lamblin, Razvan Pascanu, Guil-\nlaume Desjardins, Joseph Turian, David Warde-Farley, and Yoshua Bengio. Theano: a\nCPU and GPU math expression compiler. In Proceedings of the Python for Scienti\ufb01c\nComputing Conference (SciPy), June 2010.\nIan J. Goodfellow, David Warde-Farley, Pascal Lamblin, Vincent Dumoulin, Mehdi Mirza,\nRazvan Pascanu, James Bergstra, Fr\u00b4ed\u00b4eric Bastien, and Yoshua Bengio.\nPylearn2:\na machine learning research library.\narXiv preprint arXiv:1308.4214, 2013a.\nURL\nhttp://arxiv.org/abs/1308.4214.\nIan J. Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio.\nMaxout networks. pages 1319\u20131327, 2013b.\nKarol Gregor, Ivo Danihelka, Alex Graves, and Daan Wierstra. Draw: A recurrent neural\nnetwork for image generation. arXiv preprint arXiv:1502.04623, 2015.\nRazvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the di\ufb03culty of training recurrent\nneural networks. 2013.\nThe\nHDF\nGroup.\nHierarchical\nData\nFormat,\nversion\n5,\n1997-2015.\nhttp://www.hdfgroup.org/HDF5/.\n5\n",
        "sentence": " We thank the developers of Theano [22] and Blocks [23] for their awesome work.",
        "context": "2.1 Bricks\nTheano is a popular choice for the implementation of neural networks (see e.g. Goodfellow et al.\n(2013b); Pascanu et al. (2013)). Blocks and many other libraries, such as Pylearn2 (Goodfellow et al.,\nFuel. Blocks is based on Theano, a linear algebra compiler with CUDA-support (Bastien et al.,\n2012; Bergstra et al., 2010). It facilitates the training of complex neural network models by\n(2015)).\n2.2 Graph management\nLarge neural networks can often result in Theano computational graphs containing hundreds\nof variables and operations. Blocks does not attempt to abstract away this complex graph,"
    }
]