[
    {
        "title": "The impact of input: language acquisition in the visually impaired",
        "author": [
            "E.S. Andersen",
            "A. Dunlea",
            "L. Kekelis"
        ],
        "venue": "First Language,",
        "citeRegEx": "Andersen et al\\.,? \\Q1993\\E",
        "shortCiteRegEx": "Andersen et al\\.",
        "year": 1993,
        "abstract": " Variation in language development between blind and sighted children may result from a diminution of experience or differences in linguistic input, or it may be a product of other factors. Researchers argue about the relative weighting of these. We examine this argument by reviewing data and findings from our studies of blind children's language and we evaluate the possible impact of input, both environmental and linguistic. We show that variation cannot be uniquely attributed to either of these, but find evidence that experiential input may influence some areas while linguistic input more strongly affects others. Moreover, there is a complex interaction between these. We also find independent adaptive strategies by the children, pointing to a plasticity in the acquisition process itself. ",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Using Dynamic Time Warping to Find Patterns in Time",
        "author": [
            "D.J. Berndt",
            "J. Clifford"
        ],
        "venue": null,
        "citeRegEx": "Berndt and Clifford,? \\Q1994\\E",
        "shortCiteRegEx": "Berndt and Clifford",
        "year": 1994,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "High-performance ocr for printed english and fraktur using lstm networks",
        "author": [
            "T. Breuel",
            "A. Ul-Hasan",
            "M. Al-Azawi",
            "F. Shafait"
        ],
        "venue": "In Document Analysis and Recognition (ICDAR),",
        "citeRegEx": "Breuel et al\\.,? \\Q2013\\E",
        "shortCiteRegEx": "Breuel et al\\.",
        "year": 2013,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Scene labeling with lstm recurrent neural networks",
        "author": [
            "W. Byeon",
            "T.M. Breuel",
            "F. Raue",
            "M. Liwicki"
        ],
        "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
        "citeRegEx": "Byeon et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "Byeon et al\\.",
        "year": 2015,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Shape and the first hundred nouns",
        "author": [
            "L. Gershkoff-Stowe",
            "L.B. Smith"
        ],
        "venue": "Child development,",
        "citeRegEx": "Gershkoff.Stowe and Smith,? \\Q2004\\E",
        "shortCiteRegEx": "Gershkoff.Stowe and Smith",
        "year": 2004,
        "abstract": "This paper reports evidence from a longitudinal study in which children's attention to shape in a laboratory task of artificial noun learning was correlated with a rate shift in noun acquisitions. Eight children were tested in the laboratory at 3\u2010week intervals beginning when they had less than 25 nouns in their productive vocabulary (M age=17 months). Children were presented with a novel word generalization task at each session. Additionally, the study examined the kinds of words the children learned early, based on parent reports, and the statistical regularities inherent in those vocabularies. The results indicate that as children learned nouns, they also learned to attend to shape in the novel word task. At the same time, children showed an acceleration in new noun production outside of the laboratory.",
        "full_text": "",
        "sentence": " Gershkoff-Stowe and Smith (2004) found the initial",
        "context": null
    },
    {
        "title": "Connectionist temporal classification",
        "author": [
            "A. Graves",
            "S. Fern\u00e1ndez",
            "F. Gomez",
            "J. Schmidhuber"
        ],
        "venue": "In Proceedings of the 23rd international conference on Machine learning - ICML",
        "citeRegEx": "Graves et al\\.,? \\Q2006\\E",
        "shortCiteRegEx": "Graves et al\\.",
        "year": 2006,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " In more detail, Graves et al. (2006) introduced Connectionist Temporal Classification (CTC). Please refer to the original paper for more information (Graves et al., 2006).",
        "context": null
    },
    {
        "title": "The symbol grounding problem",
        "author": [
            "S. Harnad"
        ],
        "venue": "Physica D: Nonlinear Phenomena,",
        "citeRegEx": "Harnad,? \\Q1990\\E",
        "shortCiteRegEx": "Harnad",
        "year": 1990,
        "abstract": "How can the semantic interpretation of a formal symbol system be made\nintrinsic to the system, rather than just parasitic on the meanings in our\nheads? How can the meanings of the meaningless symbol tokens, manipulated\nsolely on the basis of their (arbitrary) shapes, be grounded in anything but\nother meaningless symbols? The problem is analogous to trying to learn Chinese\nfrom a Chinese/Chinese dictionary alone. A candidate solution is sketched:\nSymbolic representations must be grounded bottom-up in nonsymbolic\nrepresentations of two kinds: (1) \"iconic representations,\" which are analogs\nof the proximal sensory projections of distal objects and events, and (2)\n\"categorical representations,\" which are learned and innate feature-detectors\nthat pick out the invariant features of object and event categories from their\nsensory projections. Elementary symbols are the names of these object and event\ncategories, assigned on the basis of their (nonsymbolic) categorical\nrepresentations. Higher-order (3) \"symbolic representations,\" grounded in these\nelementary symbols, consist of symbol strings describing category membership\nrelations (e.g., \"An X is a Y that is Z\").",
        "full_text": "",
        "sentence": " This scenario is known as Symbol Grounding Problem (Harnad, 1990) and is still an open problem (Steels, 2008).",
        "context": null
    },
    {
        "title": "The Vanishing Gradient Problem During Learning Recurrent Neural Nets and Problem Solutions",
        "author": [
            "S. Hochreiter"
        ],
        "venue": "International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems,",
        "citeRegEx": "Hochreiter,? \\Q1998\\E",
        "shortCiteRegEx": "Hochreiter",
        "year": 1998,
        "abstract": "",
        "full_text": "",
        "sentence": " Long Short-Term Memory (LSTM) is a recurrent neural network, which is capable to learn long sequences without the vanishing gradient problem (Hochreiter & Schmidhuber, 1997; Hochreiter, 1998).",
        "context": null
    },
    {
        "title": "Long Short-Term Memory",
        "author": [
            "S. Hochreiter",
            "J. Schmidhuber"
        ],
        "venue": "Neural Computation,",
        "citeRegEx": "Hochreiter and Schmidhuber,? \\Q1997\\E",
        "shortCiteRegEx": "Hochreiter and Schmidhuber",
        "year": 1997,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Deep fragment embeddings for bidirectional image sentence mapping",
        "author": [
            "A. Karpathy",
            "A. Joulin",
            "F.F.F. Li"
        ],
        "venue": "In Advances in Neural Information Processing Systems,",
        "citeRegEx": "Karpathy et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Karpathy et al\\.",
        "year": 2014,
        "abstract": "We introduce a model for bidirectional retrieval of images and sentences\nthrough a multi-modal embedding of visual and natural language data. Unlike\nprevious models that directly map images or sentences into a common embedding\nspace, our model works on a finer level and embeds fragments of images\n(objects) and fragments of sentences (typed dependency tree relations) into a\ncommon space. In addition to a ranking objective seen in previous work, this\nallows us to add a new fragment alignment objective that learns to directly\nassociate these fragments across modalities. Extensive experimental evaluation\nshows that reasoning on both the global level of images and sentences and the\nfiner level of their respective fragments significantly improves performance on\nimage-sentence retrieval tasks. Additionally, our model provides interpretable\npredictions since the inferred inter-modal fragment alignment is explicit.",
        "full_text": "Deep Fragment Embeddings for Bidirectional Image\nSentence Mapping\nAndrej Karpathy\nArmand Joulin\nLi Fei-Fei\nDepartment of Computer Science, Stanford University, Stanford, CA 94305, USA\n{karpathy,ajoulin,feifeili}@cs.stanford.edu\nAbstract\nWe introduce a model for bidirectional retrieval of images and sentences through\na multi-modal embedding of visual and natural language data. Unlike previous\nmodels that directly map images or sentences into a common embedding space,\nour model works on a \ufb01ner level and embeds fragments of images (objects) and\nfragments of sentences (typed dependency tree relations) into a common space.\nIn addition to a ranking objective seen in previous work, this allows us to add a\nnew fragment alignment objective that learns to directly associate these fragments\nacross modalities. Extensive experimental evaluation shows that reasoning on\nboth the global level of images and sentences and the \ufb01ner level of their respective\nfragments signi\ufb01cantly improves performance on image-sentence retrieval tasks.\nAdditionally, our model provides interpretable predictions since the inferred inter-\nmodal fragment alignment is explicit.\n1\nIntroduction\nThere is signi\ufb01cant value in the ability to associate natural language descriptions with images. De-\nscribing the contents of images is useful for automated image captioning and conversely, the ability\nto retrieve images based on natural language queries has immediate image search applications. In\nparticular, in this work we are interested in training a model on a set of images and their associated\nnatural language descriptions such that we can later rank a \ufb01xed set of withheld sentences given an\nimage query, and vice versa.\nThis task is challenging because it requires detailed understanding of the content of images, sen-\ntences and their inter-modal correspondence. Consider an example sentence query, such as \u201cA dog\nwith a tennis ball is swimming in murky water\u201d (Figure 1). In order to successfully retrieve a corre-\nsponding image, we must accurately identify all entities, attributes and relationships present in the\nsentence and ground them appropriately to a complex visual scene.\nOur primary contribution is in formulating a structured, max-margin objective for a deep neural net-\nwork that learns to embed both visual and language data into a common, multimodal space. Unlike\nprevious work that embeds images and sentences, our model breaks down and embeds fragments of\nimages (objects) and fragments of sentences (dependency tree relations [1]) in a common embedding\nspace and explicitly reasons about their latent, inter-modal correspondences. Reasoning on the level\nof fragments allows us to impose a new fragment-level loss function that complements a traditional\nsentence-image ranking loss. Extensive empirical evaluation validates our approach. In particular,\nwe report dramatic improvements over state of the art methods on image-sentence retrieval tasks on\nPascal1K [2], Flickr8K [3] and Flickr30K [4] datasets. We plan to make our code publicly available.\n2\nRelated Work\nImage Annotation and Image Search. There is a growing body of work that associates images and\nsentences. Some approaches focus on describing the contents of images, formulated either as a task\nof mapping images to a \ufb01xed set of sentences written by people [5, 6], or as a task of automatically\ngenerating novel captions [7, 8, 9, 10, 11, 12]. More closely related to our approach are methods\nthat naturally allow bi-drectional mapping between the two modalities. Socher and Fei-Fei [13] and\nHodosh et al. [3] use Kernel Canonical Correlation Analysis to align images and sentences, but their\n1\narXiv:1406.5679v1  [cs.CV]  22 Jun 2014\nFigure 1: Our model takes a dataset of\nimages and their sentence descriptions\nand learns to associate their fragments.\nIn images, fragments correspond to ob-\nject detections and scene context. In sen-\ntences, fragments consist of typed de-\npendency tree [1] relations.\nmethod is not easily scalable since it relies on computing kernels quadratic in number of images\nand sentences. Farhadi et al. [5] learn a common meaning space, but their method is limited to\nrepresenting both images and sentences with a single triplet of (object, action, scene). Zitnick et al.\n[14] use a Conditional Random Field to reason about complex relationships of cartoon scenes and\ntheir natural language descriptions.\nMultimodal Representation Learning.\nOur approach falls into a general category of learning\nfrom multi-modal data. Several probabilistic models for representing joint multimodal probability\ndistributions over images and sentences have been developed, using Deep Boltzmann Machines [15],\nlog-bilinear models [16], and topic models [17, 18]. Ngiam et al. [19] described an autoencoder\nthat learns audio-video representations through a shared bottleneck layer. More closely related to\nour task and approach is work of Frome et al. [20], who introduced a visual semantic embedding\nmodel that learns to map images and words to a common semantic embedding with a ranking cost.\nAdopting a similar approach, Socher et al. [21] described a Dependency Tree Recursive Neural\nNetwork that puts entire sentences into correspondence with visual data. However, these methods\nreason about the image only on global level, using a single, \ufb01xed-sized representation from a top\nlayer of a Convolutional Neural Network as a description for the entire image, whereas our model\nreasons explicitly about objects that make up a complex scene.\nNeural Representations for Images and Natural Language.\nOur model is a neural network\nthat is connected to image pixels on one side and raw 1-of-k word representations on the other.\nThere have been multiple approaches for learning neural representations in these data domains. In\nComputer Vision, Convolutional Neural Networks (CNNs) [22] have recently been shown to learn\npowerful image representations that support state of the art image classi\ufb01cation [23, 24, 25] and\nobject detection [26, 27]. In language domain, several neural network models have been proposed\nto learn word/n-gram representations [28, 29, 30, 31, 32, 33], sentence representations [34] and\nparagraph/document representations [35].\n3\nProposed Model\nOverview of Learning and Inference. Our task is to retrieve relevant images given a sentence\nquery, and conversely, relevant sentences given an image query. We will train our model on a train-\ning set of N images and N corresponding sentences that describe their content (Figure 2). Given this\nset of correspondences, we train the weights of a neural network to output a high score when a com-\npatible image-sentence pair is fed through the network, and low score otherwise. Once the training\nis complete, all training data is discarded and the network is evaluated on a withheld set of testing\nimages and sentences. The evaluation will score all image-sentence pairs, sort images/sentences in\norder of decreasing score and record the location of a ground truth result in the list.\nFragment Embeddings. Our core insight is that images are complex structures that are made up of\nmultiple interacting entities that the sentences make explicit references to. We capture this intuition\ndirectly in our model by breaking down both images and sentences into fragments and reason about\ntheir (latent) alignment. In particular, we propose to detect objects as image fragments and use\nsentence dependency tree relations [1] as sentence fragments (Figure 2).\nFragment-Level Objective. In previous related work [20, 21], Neural Networks embed images and\nsentences into a common space and the parameters are trained such that true image-sentence pairs\nhave an inner product (interpreted as a score) higher than false image-sentence pairs by a margin.\nIn our approach, we instead embed the image and sentence fragments, and compute the image-\nsentence score as a \ufb01xed function of the scores of their fragments. Thus, in addition to the ranking\nloss seen in previous work (we refer to this as the Global Ranking Objective), we will add a second,\nstronger Fragment Alignment Objective. We will show that these objectives provide complementary\ninformation to the network.\n2\nFigure 2: Computing the Fragment and image-sentence similarities. Left: CNN representations (green) of\ndetected objects are mapped to the fragment embedding space (blue, Section 3.2). Right: Dependency tree\nrelations in the sentence are embedded (Section 3.1). Our model interprets inner products (shown as boxes)\nbetween fragments as a similarity score. The alignment (shaded boxes) is latent and inferred by our model\n(Section 3.3.1). The image-sentence similarity is computed as a \ufb01xed function of the pairwise fragment scores.\nWe \ufb01rst describe the neural networks that compute the Image and Sentence Fragment embeddings.\nThen we discuss the objective function, which is composed of the two aforementioned objectives.\n3.1\nDependency Tree Relations as Sentence Fragments\nWe would like to extract and represent the set of visually identi\ufb01able entities described in a sentence.\nFor instance, using the example in Figure 2, we would like to identify the entities (dog, child)\nand characterise their attributes (black, young) and their pairwise interactions (chasing). Inspired\nby previous work [5, 21] we observe that a dependency tree of a sentence provides a rich set of\ntyped relationships that can serve this purpose more effectively than individual words or bigrams.\nWe discard the tree structure in favor of a simpler model and interpret each relation (edge) as an\nindividual sentence fragment (Figure 2, right shows 5 example dependency relations). Thus, we\nrepresent every word using 1-of-k encoding vector w using a dictionary of 400,000 words and map\nevery dependency triplet (R, w1, w2) into the embedding space as follows:\ns = f\n\u0012\nWR\n\u0014Wew1\nWew2\n\u0015\n+ bR\n\u0013\n.\n(1)\nHere, We is a 400, 000 \u00d7 d matrix that encodes a 1-of-k vector into a d-dimensional word vector\nrepresentation (we use d = 200). We \ufb01x We to weights obtained through an unsupervised objective\ndescribed in Huang et al. [33]. Note that every relation R has its own set of weights WR and biases\nbR. We \ufb01x the element-wise nonlinearity f(.) to be the Recti\ufb01ed Linear Unit which computes\nx \u2192max(0, x). The dimensionality of s is cross-validated.\n3.2\nObject Detections as Image Fragments\nSimilar to sentences, we wish to extract and describe the set of entities that images are composed of.\nInspired by prior work [7], as a modeling assumption we observe that the subject of most sentence\ndescriptions are attributes of objects and their context in a scene. This naturally motivates the use of\nobjects and the global context as the fragments of an image. In particular, we follow Girshick et al.\n[26] and detect objects in every image with a Region Convolutional Neural Network (RCNN). The\nCNN is pre-trained on ImageNet [36] and \ufb01netuned on the 200 classes of the ImageNet Detection\nChallenge [37]. We use the top 19 detected locations and the entire image as the image fragments\nand compute the embedding vectors based on the pixels Ib inside each bounding box as follows:\nv = Wm[CNN\u03b8c(Ib)],\n(2)\nwhere CNN(Ib) takes the image inside a given bounding box and returns the 4096-dimensional\nactivations of the fully connected layer immediately before the classi\ufb01er. It might be possible to\ninitialize some of the weights in Wm with the parameters in the CNN classi\ufb01er layer, but we choose\nto discard these weights after the initial object detection step for simplicity. The CNN architecture is\nidentical to the one described in Girhsick et al. [26]. It contains approximately 60 million parameters\n\u03b8c and closely resembles the architecture of Krizhevsky et al [24].\n3.3\nObjective Function\nWe are now ready to formulate the objective function. Recall that we are given a training set of N\nimages and corresponding sentences. In the previous sections we described parameterized functions\nthat map every sentence and image to a set of fragment vectors {s} and {v}, respectively. As\n3\nFigure 3: The two objectives for a\nbatch of 2 examples. Left: Rows rep-\nresent fragments vi, columns sj. Ev-\nery square shows an ideal scenario of\nyij = sign(vT\ni sj) in the MIL ob-\njective. Red boxes are yij = \u22121.\nYellow indicates members of posi-\ntive bags that happen to currently\nbe yij = \u22121.\nRight: The scores\nare accumulated with Equation 6 into\nimage-sentence score matrix Skl.\nshown in Figure 2, our model interprets the inner product vT\ni sj between an image fragment vi and a\nsentence fragment sj as a similarity score. The similarity score for any image-sentence pair will in\nturn be computed as a \ufb01xed function of their pairwise fragment scores. Intuitively, multiple matching\nfragments will give rise to a high image-sentence score.\nWe are motivated by two criteria in designing the objective function. First, the image-sentence\nsimilarities should be consistent with the ground truth correspondences. That is, corresponding\nimage-sentence pairs should have a higher score than all other image-sentence pairs. This will\nbe enforced by the Global Ranking Objective. Second, we introduce a Fragment Alignment\nObjective that learns the appearance of all sentence fragments (such as \u201cblack dog\u201d) in the visual\ndomain. Our full objective is the weighted sum of these two objectives and a regularization term:\nC(\u03b8) = CF (\u03b8) + \u03b2CG(\u03b8) + \u03b1||\u03b8||2\n2,\n(3)\nwhere \u03b8 is a shorthand for parameters of our neural network (\u03b8 = {We, WR, Wm, \u03b8c}) and \u03b1 and \u03b2\nare hyperparameters that we cross-validate. We now describe both objectives in more detail.\n3.3.1\nFragment Alignment Objective\nThe Fragment Alignment Objective encodes the intuition that if a sentence contains a fragment\n(e.g.\u201cblue ball\u201d, Figure 3), at least one of the boxes in the corresponding image should have a high\nscore with this fragment, while all the other boxes in all the other images that have no mention of\n\u201cblue ball\u201d should have a low score. This assumption can be violated in multiple ways: a triplet\nmay not refer to anything visually identi\ufb01able in the image. The box that the triplet refers to may\nnot be detected by the RCNN. Lastly, other images may contain the described visual concept but\nits mention may omitted in the associated sentence descriptions. Nonetheless, the assumption is\nstill satis\ufb01ed in many cases and can be used to formulate a cost function. Consider an (incomplete)\nFragment Alignment Objective that assumes a dense alignment between every corresponding image\nand sentence fragments:\nC0(\u03b8) =\nX\ni\nX\nj\n\u03baijmax(0, 1 \u2212yijvT\ni sj).\n(4)\nHere, the sum is over all pairs of image and sentence fragments in the training set. The quantity vT\ni sj\ncan be interpreted as the alignment score of visual fragment vi and sentence fragment sj. In this\nincomplete objective, we de\ufb01ne yij as +1 if fragments vi and sj occur together in a corresponding\nimage-sentence pair, and \u22121 otherwise. The constants \u03baij normalize the objective with respect to\nthe number of positive and negative yij. Intuitively, C0(\u03b8) encourages scores in red regions of Figure\n3 to be less than -1 and scores along the block diagonal (green and yellow) to be more than +1.\nMultiple Instance Learning extension. The problem with the objective C0(\u03b8) is that it assumes\ndense alignment between all pairs of fragments in every corresponding image-sentence pair. How-\never, this is hardly ever the case. For example, in Figure 3, the \u201cboy playing\u201d triplet refers to only\none of the three detections. We now describe a Multiple Instance Learning (MIL) [38] extension\nof the objective C0 that attempts to infer the latent alignment between fragments in corresponding\nimage-sentence pairs. Concretely, for every triplet we put image fragments in the associated im-\nage into a positive bag, while image fragments in every other image become negative examples.\nOur precise formulation is inspired by the mi-SVM [39], which is a simple and natural extension\nof a Support Vector Machine to a Multiple Instance Learning setting. Instead of treating the yij as\nconstants, we minimize over them by wrapping Equation 4 as follows:\n4\nCF (\u03b8) = min\nyij C0(\u03b8)\ns.t.\nX\ni\u2208pj\nyij + 1\n2\n\u22651 \u2200j\nyij = \u22121 \u2200i, j s.t. mv(i) \u0338= ms(j) and yij \u2208{\u22121, 1}\n(5)\nHere, we de\ufb01ne pj to be the set of image fragments in the positive bag for sentence fragment j.\nmv(i) and ms(j) return the index of the image and sentence (an element of {1, . . . , N}) that the\nfragments vi and sj belong to. Note that the inequality simply states that at least one of the yij\nshould be positive for every sentence fragment j (i.e. at least one green box in every column in\nFigure 3). This objective cannot be solved ef\ufb01ciently [39] but a commonly used heuristic is to set\nyij = sign(vT\ni sj). If the constraint is not satis\ufb01ed for any positive bag (i.e. all scores were below\nzero), the highest-scoring item in the positive bag is set to have a positive label.\n3.3.2\nGlobal Ranking Objective\nRecall that the Global Ranking Objective ensures that the computed image-sentence similarities are\nconsistent with the ground truth annotation. First, we de\ufb01ne the image-sentence alignmnet score to\nbe the average thresholded score of their pairwise fragment scores:\nSkl =\n1\n|gk|(|gl| + n)\nX\ni\u2208gk\nX\nj\u2208gl\nmax(0, vT\ni sj).\n(6)\nHere, gk is the set of image fragments in image k and gl is the set of sentence fragments in sentence\nl. Both k, l range from 1, . . . , N. We truncate scores at zero because in the mi-SVM objective, scores\ngreater than 0 are considered correct alignments and scores less than 0 are considered to be incorrect\nalignments (i.e. false members of a positive bag). In practice, we found that it was helpful to add\na smoothing term n, since short sentences can otherwise have an advantage (we found that n = 10\nworks well on validation experiments). The Global Ranking Objective then becomes:\nCG(\u03b8) =\nX\nk\nh X\nl\nmax(0, Skl \u2212Skk + \u2206)\n|\n{z\n}\nrank images\n+\nX\nl\nmax(0, Slk \u2212Skk + \u2206)\n|\n{z\n}\nrank sentences\ni\n.\n(7)\nHere, \u2206is a hyperparameter that we cross-validate. The objective stipulates that the score for true\nimage-sentence pairs Skk should be higher than Skl or Slk for any l \u0338= k by at least a margin of \u2206.\nThis concludes our discussion of the objective function. We note that our entire model (including\nthe CNN) and objective functions are made up exclusively of dot products and thresholding at zero.\n3.4\nOptimization\nWe use Stochastic Gradient Descent (SGD) with mini-batches of 100, momentum of 0.9 and make\n15 epochs through the training data. The learning rate is cross-validated and annealed by a fraction\nof \u00d70.1 for the last two epochs. Since both Multiple Instance Learning and CNN \ufb01netuning bene\ufb01t\nfrom a good initialization, we run the \ufb01rst 10 epochs with the fragment alignment objective C0\nand CNN weights \u03b8c \ufb01xed. After 10 epochs, we switch to the full MIL objective CF and begin\n\ufb01netuning the CNN. The word embedding matrix We is kept \ufb01xed due to over\ufb01tting concerns. Our\nimplementation runs at approximately 1 second per batch on a standard CPU workstation.\n4\nExperiments\nDatasets. We evaluate our image-sentence retrieval performance on Pascal1K [2], Flickr8K [3] and\nFlickr30K [4] datasets. The datasets contain 1,000, 8,000 and 30,000 images respectively and each\nimage is annotated using Amazon Mechanical Turk with 5 independent sentences.\nSentence Data Preprocessing. We did not explicitly \ufb01lter, spellcheck or normalize any of the\nsentences for simplicity. We use the Stanford CoreNLP parser to compute the dependency trees for\nevery sentence. Since there are many possible relations (as many as hundreds), due to over\ufb01tting\nconcerns and practical considerations we remove all relation types that occur less than 1% of the\ntime in each dataset. In practice, this reduces the number of relations from 136 to 16 in Pascal1K,\n170 to 17 in Flickr8K, and from 212 to 21 in Flickr30K. Additionally, all words that are not found\nin our dictionary of 400,000 words [33] are discarded.\nImage Data Preprocessing. We use the Caffe [40] implementation of the ImageNet Detection\nRCNN model [26] to detect objects in all images.\nOn our machine with a Tesla K40 GPU,\nthe RCNN processes one image in approximately 25 seconds.\nWe discard the predictions for\n5\nPascal1K\nImage Annotation\nImage Search\nModel\nR@1\nR@5\nR@10\nMean r\nR@1\nR@5\nR@10\nMean r\nRandom Ranking\n4.0\n9.0\n12.0\n71.0\n1.6\n5.2\n10.6\n50.0\nSocher et al. [21]\n23.0\n45.0\n63.0\n16.9\n16.4\n46.6\n65.6\n12.5\nkCCA. [21]\n21.0\n47.0\n61.0\n18.0\n16.4\n41.4\n58.0\n15.9\nDeViSE [20]\n17.0\n57.0\n68.0\n11.9\n21.6\n54.6\n72.4\n9.5\nSDT-RNN [21]\n25.0\n56.0\n70.0\n13.4\n25.4\n65.2\n84.4\n7.0\nOur model\n39.0\n68.0\n79.0\n10.5\n23.6\n65.2\n79.8\n7.6\nTable 1: Pascal1K ranking experiments. R@K is Recall@K (high is good). Mean r is the mean rank (low is\ngood). Note that the test set only consists of 100 images.\nFlickr8K\nImage Annotation\nImage Search\nModel\nR@1\nR@5\nR@10\nMed r\nR@1\nR@5\nR@10\nMed r\nRandom Ranking\n0.1\n0.6\n1.1\n631\n0.1\n0.5\n1.0\n500\nSocher et al. [21]\n4.5\n18.0\n28.6\n32\n6.1\n18.5\n29.0\n29\nDeViSE [20]\n4.8\n16.5\n27.3\n28\n5.9\n20.1\n29.6\n29\nSDT-RNN [21]\n6.0\n22.7\n34.0\n23\n6.6\n21.6\n31.7\n25\nFragment Alignment Objective\n7.2\n21.9\n31.8\n25\n5.9\n20.0\n30.3\n26\nGlobal Ranking Objective\n5.8\n21.8\n34.8\n20\n7.5\n23.4\n35.0\n21\n(\u2020) Fragment + Global\n12.5\n29.4\n43.8\n14\n8.6\n26.7\n38.7\n17\n\u2020 \u2192Images: Fullframe Only\n5.9\n19.2\n27.3\n34\n5.2\n17.6\n26.5\n32\n\u2020 \u2192Sentences: BOW\n9.1\n25.9\n40.7\n17\n6.9\n22.4\n34.0\n23\n\u2020 \u2192Sentences: Bigrams\n8.7\n28.5\n41.0\n16\n8.5\n25.2\n37.0\n20\nOur model (\u2020 + MIL)\n12.6\n32.9\n44.0\n14\n9.7\n29.6\n42.5\n15\n* Hodosh et al. [3]\n8.3\n21.6\n30.3\n34\n7.6\n20.7\n30.1\n38\n* Our model (\u2020 + MIL)\n9.3\n24.9\n37.4\n21\n8.8\n27.9\n41.3\n17\nTable 2: Flickr8K experiments. R@K is Recall@K (high is good). Med r is the median rank (low is good).\nThe starred evaluation criterion (*) in [3] is slightly different since it discards 4,000 out of 5,000 test sentences.\n200 ImageNet detection classes and only keep the 4096-D activations of the fully connect layer\nimmediately before the classi\ufb01er at all of the top 19 detected locations and from the entire image.\nEvaluation Protocol for Bidirectional Retrieval. For Pascal1K we follow Socher et al. [21] and\nuse 800 images for training, 100 for validation and 100 for testing. For Flickr datasets we use\n1,000 images for validation, 1,000 for testing and the rest for training (consistent with [3]). We\ncompute the dense image-sentence similarity Skl between every image-sentence pair in the test set\nand rank images and sentences in order of decreasing score. For both Image Annotation and Image\nSearch, we report the median rank of the closest ground truth result in the list, as well as Recall@K\nwhich computes the fraction of times the correct result was found among the top K items. When\ncomparing to Hodosh et al. [3] we closely follow their evaluation protocol and only keep a subset\nof N sentences out of total 5N (we use the \ufb01rst sentence out of every group of 5).\n4.1\nComparison Methods\nSDT-RNN. Socher et al. [21] embed a fullframe CNN representation with the sentence representa-\ntion from a Semantic Dependency Tree Recursive Neural Network (SDT-RNN). Their loss matches\nour global ranking objective. We requested the source code of Socher et al. [21] and substituted\nthe Flickr8K and Flick30K datasets. To better understand the bene\ufb01ts of using our detection CNN\nand for a more fair comparison we also train their method with our CNN features. Since we have\nmultiple objects per image, we average representations from all objects with detection con\ufb01dence\nabove a (cross-validated) threshold. We refer to the exact method of Socher et al. [21] with a single\nfullframe CNN as \u201cSocher et al\u201d, and to their method with our combined CNN features as \u201cSDT-\nRNN\u201d. We were not able to evaluate SDT-RNN on Flickr30K because training times on order of\nmultiple days prevent us from satisfyingly cross-validating their hyperparameters.\nDeViSE. The DeViSE [20] source code is not publicly available but their approach is a special case\nof our method with the following modi\ufb01cations: we use the average (L2-normalized) word vectors\nas a sentence fragment, the average CNN activation of all objects above a detection threshold (as\ndiscussed in case of SDT-RNN) as an image fragment and only use the global ranking objective.\n4.2\nQuantitative Evaluation\nThe quantitative results for Pascal1K, Flickr8K, and Flickr30K are in Tables 1, 2, and 3 respectively.\nOur model outperforms previous methods.\nOur full method consistently and signi\ufb01cantly\noutperforms previous methods on Flickr8K (Table 2) and Flickr30K (Table 3) datasets.\nOn\n6\nFlickr30K\nImage Annotation\nImage Search\nModel\nR@1\nR@5\nR@10\nMed r\nR@1\nR@5\nR@10\nMed r\nRandom Ranking\n0.1\n0.6\n1.1\n631\n0.1\n0.5\n1.0\n500\nDeViSE [20]\n4.5\n18.1\n29.2\n26\n6.7\n21.9\n32.7\n25\nFragment Alignment Objective\n11\n28.7\n39.3\n18\n7.6\n23.8\n34.5\n22\nGlobal Ranking Objective\n11.5\n33.2\n44.9\n14\n8.8\n27.6\n38.4\n17\n(\u2020) Fragment + Global\n12.0\n37.1\n50.0\n10\n9.9\n30.5\n43.2\n14\nOur model (\u2020 + MIL)\n14.2\n37.7\n51.3\n10\n10.2\n30.8\n44.2\n14\nOur model + Finetune CNN\n16.4\n40.2\n54.7\n8\n10.3\n31.4\n44.5\n13\nTable 3: Flickr30K experiments. R@K is Recall@K (high is good). Med r is the median rank (low is good).\nFigure 4: Qualitative Image Annotation results. Below each image we show the top 5 sentences (among a set\nof 5,000 test sentences) in descending con\ufb01dence. We also show the triplets for the top sentence and connect\neach to the detections with the highest compatibility score (indicated by lines). The numbers next to each triplet\nindicate the matching fragment score. We color a sentence green if it correct and red otherwise. We attach many\nmore examples in the supplementary material.\nPascal1K (Table 1) the SDT-RNN appears to be competitive on Image Search.\nFragment and Global Objectives are complementary. As seen in Tables 2 and 3, both objectives\nperform well but there is a noticeable improvement when the two are combined, suggesting that the\nobjectives bring complementary information to the cost function. Note that the Global Objective\nconsistently performs slightly better, possibly because it directly minimizes the evaluation criterion\n(ranking cost), while the Fragment Alignment Objective only does so indirectly.\nExtracting object representations is important. Using only the global scene-level CNN repre-\nsentation as a single fragment for every image leads to a consistent drop in performance, suggesting\nthat a single fullframe CNN alone is inadequate in effectively representing the images. (Table 2)\nDependency tree relations outperform BoW/bigram representations. We compare to a simpler\nBag of Words (BOW) baseline to understand the contribution of dependency relations. In BOW\nbaseline we iterate over words instead of dependency triplets when creating bags of sentence\nfragments (set w1 = w2 in Equation1). As can be seen in the Table 2, this leads to a consistent drop\nin performance. This drop could be attributed to a difference between using one word or two words\nat a time, so we also compare to a bigram baseline where the words w1, w2 in Equation 1 refer to\nconsecutive words in a sentence, not nodes that share an edge in the dependency tree. Again, we\nobserve a consistent performance drop, which suggests that the dependency relations provide useful\nstructure that the neural network takes advantage of.\nFinetuning the CNN helps on Flickr30K. Our end-to-end Neural Network approach allows us to\nbackpropagate gradients all the way down to raw data (pixels or 1-of-k word encodings). In particu-\nlar, we observed additional improvements on the Flickr30K dataset (Table 3) when we \ufb01netune the\nCNN. We were not able to improve the validation performance on Pascal1K and Flickr8K datasets\nand suspect that there is an insuf\ufb01cient amount of training data.\n4.3\nQualitative Experiments\nInterpretable Predictions.\nWe show some example sentence retrieval results in Figure 4. The\nalignment in our model is explicitly inferred on the fragment level, which allows us to interpret the\nscores between images and sentences. For instance, in the last image it is apparent that the model\nretrieved the top sentence because it erroneously associated a mention of a blue person to the blue\n\ufb02ag on the bottom right of the image.\n7\nFigure 5: We \ufb01x a triplet and retrieve the highest scoring image fragments in the test set. Note that ball, person\nand dog are ImageNet Detection classes but their visual properties (e.g. soccer, standing, snowboarding, black)\nare not. Jackets and rocky scenes are not ImageNet Detection classes. Find more in supplementary material.\nFragment Alignment Objective trains attribute detectors.\nThe detection CNN is trained to\npredict one of 200 ImageNet Detection classes, so it is not clear if the representation is powerful\nenough to support learning of more complex attributes of the objects or generalize to novel classes.\nTo see whether our model successfully learns to predict sentence triplets, we \ufb01x a triplet vector and\nsearch for the highest scoring boxes in the test set. Qualitative results shown in Figure 5 suggest\nthat the model is indeed capable of generalizing to more \ufb01ne-grained subcategories (such as \u201cblack\ndog\u201d, \u201csoccer ball\u201d) and to out of sample classes such as \u201crocky terrain\u201d and \u201cjacket\u201d.\nLimitations.\nOur method has multiple limitations and failure cases. First, from a modeling per-\nspective, sentences are only modeled as bags of relations. Therefore, relations that belong to the\nsame noun phrase can sometimes align to different objects. Additionally, people frequently use\nphrases such as \u201cthree children playing\u201d but the model is incapable of counting. Moreover, the\nnon-maximum suppression in the RCNN can sometimes detect, for example, multiple people inside\none person. Since the model does not take into account any spatial information associated with the\ndetections, it is hard for it to tell the difference between two distinct people or spurious detections\nof one person. On the language side, there are many dependency relations that don\u2019t have a natural\ngrounding in an image (for example, \u201ceach other\u201d, \u201cfour people\u201d, etc. ). Compound relations and\nattributes can also become separated. For instance \u201cblack and white dog\u201d is parsed as two rela-\ntions (CONJ, black, white) and (AMOD, white, dog). While we have shown that the relations give\nrise to more powerful representations than words or bigrams, a more careful treatment of sentence\nfragments might be necessary for further progress.\n5\nConclusions\nWe addressed the problem of bidirectional retrieval of images and sentences. Our neural network\nmodel learns a multi-modal embedding space for fragments of images and sentences and reasons\nabout their latent, inter-modal alignment. Reasoning on a \ufb01ner level of image and sentence frag-\nments allowed us to formulate a new Fragment Alignment Objective that complements a more tra-\nditional Global Ranking Objective term. We have shown that our model signi\ufb01cantly improves the\nretrieval performance on image sentence retrieval tasks compared to previous work. Our model also\nproduces interpretable predictions. In future work we hope to extend the model to support counting,\nreasoning about spatial positions of objects, and move beyond bags of fragments.\nReferences\n[1] De Marneffe, M.C., MacCartney, B., Manning, C.D., et al.: Generating typed dependency parses from\nphrase structure parses. In: Proceedings of LREC. Volume 6. (2006) 449\u2013454\n[2] Rashtchian, C., Young, P., Hodosh, M., Hockenmaier, J.: Collecting image annotations using amazon\u2019s\nmechanical turk. In: Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language\nData with Amazon\u2019s Mechanical Turk, Association for Computational Linguistics (2010) 139\u2013147\n[3] Hodosh, M., Young, P., Hockenmaier, J.: Framing image description as a ranking task: data, models and\nevaluation metrics. Journal of Arti\ufb01cial Intelligence Research (2013)\n[4] Young, P., Lai, A., Hodosh, M., Hockenmaier, J.: From image descriptions to visual denotations: New\nsimilarity metrics for semantic inference over event descriptions. TACL (2014)\n[5] Farhadi, A., Hejrati, M., Sadeghi, M.A., Young, P., Rashtchian, C., Hockenmaier, J., Forsyth, D.: Every\npicture tells a story: Generating sentences from images. In: ECCV. (2010)\n8\n[6] Ordonez, V., Kulkarni, G., Berg, T.L.: Im2text: Describing images using 1 million captioned photographs.\nIn: NIPS. (2011)\n[7] Kulkarni, G., Premraj, V., Dhar, S., Li, S., Choi, Y., Berg, A.C., Berg, T.L.: Baby talk: Understanding\nand generating simple image descriptions. In: CVPR. (2011)\n[8] Yao, B.Z., Yang, X., Lin, L., Lee, M.W., Zhu, S.C.: I2t: Image parsing to text description. Proceedings\nof the IEEE 98(8) (2010) 1485\u20131508\n[9] Yang, Y., Teo, C.L., Daum\u00b4e III, H., Aloimonos, Y.: Corpus-guided sentence generation of natural images.\nIn: EMNLP. (2011)\n[10] Li, S., Kulkarni, G., Berg, T.L., Berg, A.C., Choi, Y.:\nComposing simple image descriptions using\nweb-scale n-grams. In: CoNLL. (2011)\n[11] Mitchell, M., Han, X., Dodge, J., Mensch, A., Goyal, A., Berg, A., Yamaguchi, K., Berg, T., Stratos,\nK., Daum\u00b4e, III, H.: Midge: Generating image descriptions from computer vision detections. In: EACL.\n(2012)\n[12] Kuznetsova, P., Ordonez, V., Berg, A.C., Berg, T.L., Choi, Y.: Collective generation of natural image\ndescriptions. In: ACL. (2012)\n[13] Socher, R., Fei-Fei, L.: Connecting modalities: Semi-supervised segmentation and annotation of images\nusing unaligned text corpora. In: CVPR. (2010)\n[14] Zitnick, C.L., Parikh, D., Vanderwende, L.: Learning the visual interpretation of sentences. ICCV (2013)\n[15] Srivastava, N., Salakhutdinov, R.: Multimodal learning with deep boltzmann machines. In: NIPS. (2012)\n[16] Kiros, R., Zemel, R.S., Salakhutdinov, R.: Multimodal neural language models. ICML (2014)\n[17] Jia, Y., Salzmann, M., Darrell, T.: Learning cross-modality similarity for multinomial data. In: ICCV.\n(2011)\n[18] Barnard, K., Duygulu, P., Forsyth, D., De Freitas, N., Blei, D.M., Jordan, M.I.: Matching words and\npictures. JMLR (2003)\n[19] Ngiam, J., Khosla, A., Kim, M., Nam, J., Lee, H., Ng, A.Y.: Multimodal deep learning. In: ICML. (2011)\n[20] Frome, A., Corrado, G.S., Shlens, J., Bengio, S., Dean, J., Mikolov, T., et al.: Devise: A deep visual-\nsemantic embedding model. In: NIPS. (2013)\n[21] Socher, R., Karpathy, A., Le, Q.V., Manning, C.D., Ng, A.Y.: Grounded compositional semantics for\n\ufb01nding and describing images with sentences. TACL (2014)\n[22] LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.: Gradient-based learning applied to document recognition.\nProceedings of the IEEE 86(11) (1998) 2278\u20132324\n[23] Le, Q.V.: Building high-level features using large scale unsupervised learning. In: Acoustics, Speech and\nSignal Processing (ICASSP), 2013 IEEE International Conference on, IEEE (2013) 8595\u20138598\n[24] Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classi\ufb01cation with deep convolutional neural net-\nworks. In: NIPS. (2012)\n[25] Zeiler, M.D., Fergus, R.: Visualizing and understanding convolutional neural networks. arXiv preprint\narXiv:1311.2901 (2013)\n[26] Girshick, R., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies for accurate object detection and\nsemantic segmentation. In: CVPR. (2014)\n[27] Sermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., LeCun, Y.: Overfeat: Integrated recognition,\nlocalization and detection using convolutional networks. In: ICLR. (2014)\n[28] Bengio, Y., Schwenk, H., Sen\u00b4ecal, J.S., Morin, F., Gauvain, J.L.: Neural probabilistic language models.\nIn: Innovations in Machine Learning. Springer (2006)\n[29] Mnih, A., Hinton, G.: Three new graphical models for statistical language modelling. In: ICML. (2007)\n[30] Mikolov, T., Sutskever, I., Chen, K., Corrado, G.S., Dean, J.: Distributed representations of words and\nphrases and their compositionality. In: NIPS. (2013)\n[31] Turian, J., Ratinov, L., Bengio, Y.:\nWord representations: a simple and general method for semi-\nsupervised learning. In: ACL. (2010)\n[32] Collobert, R., Weston, J.: A uni\ufb01ed architecture for natural language processing: Deep neural networks\nwith multitask learning. In: ICML. (2008)\n[33] Huang, E.H., Socher, R., Manning, C.D., Ng, A.Y.: Improving word representations via global context\nand multiple word prototypes. In: ACL. (2012)\n[34] Socher, R., Lin, C.C., Manning, C., Ng, A.Y.: Parsing natural scenes and natural language with recursive\nneural networks. In: ICML. (2011)\n[35] Le, Q.V., Mikolov, T.: Distributed representations of sentences and documents. ICML (2014)\n[36] Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale hierarchical image\ndatabase. In: CVPR. (2009)\n[37] Russakovsky, O., Deng, J., Krause, J., Berg, A., Fei-Fei, L.: Large scale visual recognition challenge\n2013. http://image-net.org/challenges/LSVRC/2013/ (2013)\n[38] Chen, Y., Bi, J., Wang, J.Z.: Miles: Multiple-instance learning via embedded instance selection. CVPR\n28(12) (2006)\n[39] Andrews, S., Hofmann, T., Tsochantaridis, I.: Multiple instance learning with generalized support vector\nmachines. In: AAAI/IAAI. (2002) 943\u2013944\n[40] Jia,\nY.:\nCaffe:\nAn open source convolutional architecture for fast feature embedding.\nhttp://caffe.berkeleyvision.org/ (2013)\n9\n",
        "sentence": " LSTM has been succesfully applied to several scenarios, such as, image captioning (Karpathy et al., 2014), texture classification (Byeon, Breuel, Raue, & Liwicki, 2015), and machine translation (Sutskever, Vinyals, & Le, 2014).",
        "context": "[32] Collobert, R., Weston, J.: A uni\ufb01ed architecture for natural language processing: Deep neural networks\nwith multitask learning. In: ICML. (2008)\n[33] Huang, E.H., Socher, R., Manning, C.D., Ng, A.Y.: Improving word representations via global context\ngenerating novel captions [7, 8, 9, 10, 11, 12]. More closely related to our approach are methods\nthat naturally allow bi-drectional mapping between the two modalities. Socher and Fei-Fei [13] and\n[22] LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.: Gradient-based learning applied to document recognition.\nProceedings of the IEEE 86(11) (1998) 2278\u20132324"
    },
    {
        "title": "Grounding of word meanings in latent dirichlet allocation-based multimodal concepts",
        "author": [
            "T. Nakamura",
            "T. Araki",
            "T. Nagai",
            "N. Iwahashi"
        ],
        "venue": "Advanced Robotics,",
        "citeRegEx": "Nakamura et al\\.,? \\Q2011\\E",
        "shortCiteRegEx": "Nakamura et al\\.",
        "year": 2011,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Nakamura et al. (2011) introduced a multimodal categorization applied to robotics. Nakamura et al. (2011) introduced a multimodal categorization applied to robotics. Their framework exploited the relation of concepts in different modalities (visual, audio and haptic) using a Multimodal latent Dirichlet allocation. Previous approaches has focused on feature engineering, and the segmentation and the classification tasks are considered as independent modules. This paper is focusing on a model for segmentation and classification tasks in the objectword association scenario. Moreover, we are interested in multimodal sequences that represent a semantic concept sequence with the constraint that not all elements can be on both modalities. For instance, one modality sequence (text lines of digits) is represented by \u20182 4 5\u2019, and the other modality (spoken words) is represented by \u2018four six five\u2019. Also, the association problem is different from the traditional setup where the association is fixed via a pre-defined coding scheme of the classes (e.g. 1-of-K scheme) before training. We explain the difference between common approaches for multimodal machine learning and our problem setup in Section 1.1. In this work, we investigate the benefits of exploiting the alignment between elements that are common in the multimodal sequence and still agree in a similar representation via coding scheme. Note that our work is an extension of Raue et al. (2015) where both modalities represent the same semantic sequence (no missing elements). Nakamura et al. (2011) introduced a multimodal categorization applied to robotics. Their framework exploited the relation of concepts in different modalities (visual, audio and haptic) using a Multimodal latent Dirichlet allocation. Previous approaches has focused on feature engineering, and the segmentation and the classification tasks are considered as independent modules. This paper is focusing on a model for segmentation and classification tasks in the objectword association scenario. Moreover, we are interested in multimodal sequences that represent a semantic concept sequence with the constraint that not all elements can be on both modalities. For instance, one modality sequence (text lines of digits) is represented by \u20182 4 5\u2019, and the other modality (spoken words) is represented by \u2018four six five\u2019. Also, the association problem is different from the traditional setup where the association is fixed via a pre-defined coding scheme of the classes (e.g. 1-of-K scheme) before training. We explain the difference between common approaches for multimodal machine learning and our problem setup in Section 1.1. In this work, we investigate the benefits of exploiting the alignment between elements that are common in the multimodal sequence and still agree in a similar representation via coding scheme. Note that our work is an extension of Raue et al. (2015) where both modalities represent the same semantic sequence (no missing elements). Similarly to Raue et al. (2015), the model was implemented by two Long Short-Term Memories (LSTMs) that their output vectors were aligned in the time axis using Dynamic Time Warping (DTW) (Berndt & Clifford, 1994).",
        "context": null
    },
    {
        "title": "Protocols from perceptual observations",
        "author": [
            "C.J. Needham",
            "P.E. Santos",
            "D.R. Magee",
            "V. Devin",
            "D.C. Hogg",
            "A.G. Cohn"
        ],
        "venue": "Artificial Intelligence,",
        "citeRegEx": "Needham et al\\.,? \\Q2005\\E",
        "shortCiteRegEx": "Needham et al\\.",
        "year": 2005,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Columbia object image library (coil-100)",
        "author": [
            "S. Nene",
            "S. Nayar",
            "H. Murase"
        ],
        "venue": "Tech. rep.",
        "citeRegEx": "Nene et al\\.,? \\Q1996\\E",
        "shortCiteRegEx": "Nene et al\\.",
        "year": 1996,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "A tutorial on hidden markov models and selected applications in speech recognition",
        "author": [
            "L.R. Rabiner"
        ],
        "venue": "Proceedings of the IEEE,",
        "citeRegEx": "Rabiner,? \\Q1989\\E",
        "shortCiteRegEx": "Rabiner",
        "year": 1989,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " One sequence is LSTM output vectors, and the other sequence is obtained by a forward-backward propagation of probabilities similar to Hidden Markov Models (HMM) (Rabiner, 1989).",
        "context": null
    },
    {
        "title": "Symbol Grounding in Multimodal Sequences using Recurrent Neural Network",
        "author": [
            "F. Raue",
            "W. Byeon",
            "T. Breuel",
            "M. Liwicki"
        ],
        "venue": "In Workshop Cognitive Computation: Integrating Neural and Symbolic Approaches at NIPS",
        "citeRegEx": "Raue et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "Raue et al\\.",
        "year": 2015,
        "abstract": "",
        "full_text": "",
        "sentence": " In contrast, the language development can be limited by the lack of stimulus (Andersen, Dunlea, & Kekelis, 1993; Spencer, 2000), i.e., deafness, blindness. Asano et al. found two different patterns in the brain activity of infants depending on the semantic correctness between a visual and an audio stimulus. In simpler terms, the brain activity is pattern \u2018A\u2019 if the visual and audio signals represent the same semantic concept. Otherwise, the pattern is \u2018B\u2019. Related work has been proposed in different multimodal scenarios inspired by the Symbol Grounding Problem. Yu and Ballard (2004) explored a framework that learns the association between objects and their spoken names in day-to-day tasks. In both cases, our model performances better that the model proposed by Raue et al. (2015). In Section 1, we mentioned that this work is an extension of Raue et al. (2015). They have introduced a Symbolic Association scenario where their model learns to associate multimodal sequences and learns the semantic binding between Semantic Concepts (SeC ) and vectorial representation (SyF ). Figure 2: General overview of the original model proposed by Raue et al. (2015) and the contributions of this paper. In addition, the proposed extension was compared against the original model in (Raue et al., 2015). 00 Original Model (Raue et al. (2015)) 12.",
        "context": null
    },
    {
        "title": "Improved multimodal deep learning with variation of information",
        "author": [
            "K. Sohn",
            "W. Shang",
            "H. Lee"
        ],
        "venue": "In Advances in Neural Information Processing Systems,",
        "citeRegEx": "Sohn et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Sohn et al\\.",
        "year": 2014,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Looking without listening: is audition a prerequisite for normal development of visual attention during infancy",
        "author": [
            "P.E. Spencer"
        ],
        "venue": "Journal of deaf studies and deaf education,",
        "citeRegEx": "Spencer,? \\Q2000\\E",
        "shortCiteRegEx": "Spencer",
        "year": 2000,
        "abstract": "",
        "full_text": "",
        "sentence": " In contrast, the language development can be limited by the lack of stimulus (Andersen, Dunlea, & Kekelis, 1993; Spencer, 2000), i.",
        "context": null
    },
    {
        "title": "Multimodal learning with deep boltzmann machines",
        "author": [
            "N. Srivastava",
            "R.R. Salakhutdinov"
        ],
        "venue": "In Advances in neural information processing systems,",
        "citeRegEx": "Srivastava and Salakhutdinov,? \\Q2012\\E",
        "shortCiteRegEx": "Srivastava and Salakhutdinov",
        "year": 2012,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "The symbol grounding problem has been solved, so whats next ?. Symbols, Embodiment and Meaning",
        "author": [
            "L. Steels"
        ],
        "venue": null,
        "citeRegEx": "Steels,? \\Q2008\\E",
        "shortCiteRegEx": "Steels",
        "year": 2008,
        "abstract": "",
        "full_text": "",
        "sentence": " This scenario is known as Symbol Grounding Problem (Harnad, 1990) and is still an open problem (Steels, 2008). However, many questions remain still open (Needham, Santos, Magee, Devin, Hogg, & Cohn, 2005; Steels, 2008).",
        "context": null
    },
    {
        "title": "Sequence to sequence learning with neural networks. In Advances in neural information processing",
        "author": [
            "I. Sutskever",
            "O. Vinyals",
            "Q.V. Le"
        ],
        "venue": null,
        "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Sutskever et al\\.",
        "year": 2014,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Show and tell: A neural image caption generator",
        "author": [
            "O. Vinyals",
            "A. Toshev",
            "S. Bengio",
            "D. Erhan"
        ],
        "venue": "arXiv preprint arXiv:1411.4555",
        "citeRegEx": "Vinyals et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Vinyals et al\\.",
        "year": 2014,
        "abstract": "Automatically describing the content of an image is a fundamental problem in\nartificial intelligence that connects computer vision and natural language\nprocessing. In this paper, we present a generative model based on a deep\nrecurrent architecture that combines recent advances in computer vision and\nmachine translation and that can be used to generate natural sentences\ndescribing an image. The model is trained to maximize the likelihood of the\ntarget description sentence given the training image. Experiments on several\ndatasets show the accuracy of the model and the fluency of the language it\nlearns solely from image descriptions. Our model is often quite accurate, which\nwe verify both qualitatively and quantitatively. For instance, while the\ncurrent state-of-the-art BLEU-1 score (the higher the better) on the Pascal\ndataset is 25, our approach yields 59, to be compared to human performance\naround 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66,\nand on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we\nachieve a BLEU-4 of 27.7, which is the current state-of-the-art.",
        "full_text": "Show and Tell: A Neural Image Caption Generator\nOriol Vinyals\nGoogle\nvinyals@google.com\nAlexander Toshev\nGoogle\ntoshev@google.com\nSamy Bengio\nGoogle\nbengio@google.com\nDumitru Erhan\nGoogle\ndumitru@google.com\nAbstract\nAutomatically describing the content of an image is a\nfundamental problem in arti\ufb01cial intelligence that connects\ncomputer vision and natural language processing. In this\npaper, we present a generative model based on a deep re-\ncurrent architecture that combines recent advances in com-\nputer vision and machine translation and that can be used\nto generate natural sentences describing an image.\nThe\nmodel is trained to maximize the likelihood of the target de-\nscription sentence given the training image. Experiments\non several datasets show the accuracy of the model and the\n\ufb02uency of the language it learns solely from image descrip-\ntions. Our model is often quite accurate, which we verify\nboth qualitatively and quantitatively. For instance, while\nthe current state-of-the-art BLEU-1 score (the higher the\nbetter) on the Pascal dataset is 25, our approach yields 59,\nto be compared to human performance around 69. We also\nshow BLEU-1 score improvements on Flickr30k, from 56 to\n66, and on SBU, from 19 to 28. Lastly, on the newly released\nCOCO dataset, we achieve a BLEU-4 of 27.7, which is the\ncurrent state-of-the-art.\n1. Introduction\nBeing able to automatically describe the content of an\nimage using properly formed English sentences is a very\nchallenging task, but it could have great impact, for instance\nby helping visually impaired people better understand the\ncontent of images on the web. This task is signi\ufb01cantly\nharder, for example, than the well-studied image classi\ufb01-\ncation or object recognition tasks, which have been a main\nfocus in the computer vision community [27]. Indeed, a\ndescription must capture not only the objects contained in\nan image, but it also must express how these objects relate\nto each other as well as their attributes and the activities\nthey are involved in. Moreover, the above semantic knowl-\nedge has to be expressed in a natural language like English,\nwhich means that a language model is needed in addition to\nvisual understanding.\nMost previous attempts have proposed to stitch together\nA group of people \nshopping at an \noutdoor market. \n!\nThere are many \nvegetables at the \nfruit stand.\nVision!\nDeep CNN\nLanguage !\nGenerating!\nRNN\nFigure 1.\nNIC, our model, is based end-to-end on a neural net-\nwork consisting of a vision CNN followed by a language gener-\nating RNN. It generates complete sentences in natural language\nfrom an input image, as shown on the example above.\nexisting solutions of the above sub-problems, in order to go\nfrom an image to its description [6, 16]. In contrast, we\nwould like to present in this work a single joint model that\ntakes an image I as input, and is trained to maximize the\nlikelihood p(S|I) of producing a target sequence of words\nS = {S1, S2, . . .} where each word St comes from a given\ndictionary, that describes the image adequately.\nThe main inspiration of our work comes from recent ad-\nvances in machine translation, where the task is to transform\na sentence S written in a source language, into its transla-\ntion T in the target language, by maximizing p(T|S). For\nmany years, machine translation was also achieved by a se-\nries of separate tasks (translating words individually, align-\ning words, reordering, etc), but recent work has shown that\ntranslation can be done in a much simpler way using Re-\ncurrent Neural Networks (RNNs) [3, 2, 30] and still reach\nstate-of-the-art performance. An \u201cencoder\u201d RNN reads the\nsource sentence and transforms it into a rich \ufb01xed-length\nvector representation, which in turn in used as the initial\nhidden state of a \u201cdecoder\u201d RNN that generates the target\nsentence.\nHere, we propose to follow this elegant recipe, replac-\ning the encoder RNN by a deep convolution neural network\n(CNN). Over the last few years it has been convincingly\nshown that CNNs can produce a rich representation of the\ninput image by embedding it to a \ufb01xed-length vector, such\nthat this representation can be used for a variety of vision\n1\narXiv:1411.4555v2  [cs.CV]  20 Apr 2015\ntasks [28]. Hence, it is natural to use a CNN as an image\n\u201cencoder\u201d, by \ufb01rst pre-training it for an image classi\ufb01cation\ntask and using the last hidden layer as an input to the RNN\ndecoder that generates sentences (see Fig. 1). We call this\nmodel the Neural Image Caption, or NIC.\nOur contributions are as follows. First, we present an\nend-to-end system for the problem. It is a neural net which\nis fully trainable using stochastic gradient descent. Second,\nour model combines state-of-art sub-networks for vision\nand language models. These can be pre-trained on larger\ncorpora and thus can take advantage of additional data. Fi-\nnally, it yields signi\ufb01cantly better performance compared\nto state-of-the-art approaches; for instance, on the Pascal\ndataset, NIC yielded a BLEU score of 59, to be compared to\nthe current state-of-the-art of 25, while human performance\nreaches 69. On Flickr30k, we improve from 56 to 66, and\non SBU, from 19 to 28.\n2. Related Work\nThe problem of generating natural language descriptions\nfrom visual data has long been studied in computer vision,\nbut mainly for video [7, 32]. This has led to complex sys-\ntems composed of visual primitive recognizers combined\nwith a structured formal language, e.g. And-Or Graphs or\nlogic systems, which are further converted to natural lan-\nguage via rule-based systems.\nSuch systems are heav-\nily hand-designed, relatively brittle and have been demon-\nstrated only on limited domains, e.g. traf\ufb01c scenes or sports.\nThe problem of still image description with natural text\nhas gained interest more recently. Leveraging recent ad-\nvances in recognition of objects, their attributes and loca-\ntions, allows us to drive natural language generation sys-\ntems, though these are limited in their expressivity. Farhadi\net al. [6] use detections to infer a triplet of scene elements\nwhich is converted to text using templates. Similarly, Li\net al. [19] start off with detections and piece together a \ufb01-\nnal description using phrases containing detected objects\nand relationships.\nA more complex graph of detections\nbeyond triplets is used by Kulkani et al. [16], but with\ntemplate-based text generation. More powerful language\nmodels based on language parsing have been used as well\n[23, 1, 17, 18, 5]. The above approaches have been able to\ndescribe images \u201cin the wild\u201d, but they are heavily hand-\ndesigned and rigid when it comes to text generation.\nA large body of work has addressed the problem of rank-\ning descriptions for a given image [11, 8, 24]. Such ap-\nproaches are based on the idea of co-embedding of images\nand text in the same vector space. For an image query, de-\nscriptions are retrieved which lie close to the image in the\nembedding space. Most closely, neural networks are used to\nco-embed images and sentences together [29] or even image\ncrops and subsentences [13] but do not attempt to generate\nnovel descriptions. In general, the above approaches cannot\ndescribe previously unseen compositions of objects, even\nthough the individual objects might have been observed in\nthe training data. Moreover, they avoid addressing the prob-\nlem of evaluating how good a generated description is.\nIn this work we combine deep convolutional nets for im-\nage classi\ufb01cation [12] with recurrent networks for sequence\nmodeling [10], to create a single network that generates de-\nscriptions of images. The RNN is trained in the context of\nthis single \u201cend-to-end\u201d network. The model is inspired by\nrecent successes of sequence generation in machine trans-\nlation [3, 2, 30], with the difference that instead of starting\nwith a sentence, we provide an image processed by a con-\nvolutional net. The closest works are by Kiros et al. [15]\nwho use a neural net, but a feedforward one, to predict the\nnext word given the image and previous words. A recent\nwork by Mao et al. [21] uses a recurrent NN for the same\nprediction task. This is very similar to the present proposal\nbut there are a number of important differences: we use a\nmore powerful RNN model, and provide the visual input to\nthe RNN model directly, which makes it possible for the\nRNN to keep track of the objects that have been explained\nby the text. As a result of these seemingly insigni\ufb01cant dif-\nferences, our system achieves substantially better results on\nthe established benchmarks. Lastly, Kiros et al. [14] pro-\npose to construct a joint multimodal embedding space by\nusing a powerful computer vision model and an LSTM that\nencodes text. In contrast to our approach, they use two sepa-\nrate pathways (one for images, one for text) to de\ufb01ne a joint\nembedding, and, even though they can generate text, their\napproach is highly tuned for ranking.\n3. Model\nIn this paper, we propose a neural and probabilistic\nframework to generate descriptions from images. Recent\nadvances in statistical machine translation have shown that,\ngiven a powerful sequence model, it is possible to achieve\nstate-of-the-art results by directly maximizing the proba-\nbility of the correct translation given an input sentence in\nan \u201cend-to-end\u201d fashion \u2013 both for training and inference.\nThese models make use of a recurrent neural network which\nencodes the variable length input into a \ufb01xed dimensional\nvector, and uses this representation to \u201cdecode\u201d it to the de-\nsired output sentence. Thus, it is natural to use the same ap-\nproach where, given an image (instead of an input sentence\nin the source language), one applies the same principle of\n\u201ctranslating\u201d it into its description.\nThus, we propose to directly maximize the probability of\nthe correct description given the image by using the follow-\ning formulation:\n\u03b8\u22c6= arg max\n\u03b8\nX\n(I,S)\nlog p(S|I; \u03b8)\n(1)\nwhere \u03b8 are the parameters of our model, I is an image, and\nS its correct transcription. Since S represents any sentence,\nits length is unbounded. Thus, it is common to apply the\nchain rule to model the joint probability over S0, . . . , SN,\nwhere N is the length of this particular example as\nlog p(S|I) =\nN\nX\nt=0\nlog p(St|I, S0, . . . , St\u22121)\n(2)\nwhere we dropped the dependency on \u03b8 for convenience.\nAt training time, (S, I) is a training example pair, and we\noptimize the sum of the log probabilities as described in (2)\nover the whole training set using stochastic gradient descent\n(further training details are given in Section 4).\nIt is natural to model p(St|I, S0, . . . , St\u22121) with a Re-\ncurrent Neural Network (RNN), where the variable number\nof words we condition upon up to t \u22121 is expressed by a\n\ufb01xed length hidden state or memory ht. This memory is\nupdated after seeing a new input xt by using a non-linear\nfunction f:\nht+1 = f(ht, xt) .\n(3)\nTo make the above RNN more concrete two crucial design\nchoices are to be made: what is the exact form of f and\nhow are the images and words fed as inputs xt. For f we\nuse a Long-Short Term Memory (LSTM) net, which has\nshown state-of-the art performance on sequence tasks such\nas translation. This model is outlined in the next section.\nFor the representation of images, we use a Convolutional\nNeural Network (CNN). They have been widely used and\nstudied for image tasks, and are currently state-of-the art\nfor object recognition and detection. Our particular choice\nof CNN uses a novel approach to batch normalization and\nyields the current best performance on the ILSVRC 2014\nclassi\ufb01cation competition [12].\nFurthermore, they have\nbeen shown to generalize to other tasks such as scene clas-\nsi\ufb01cation by means of transfer learning [4]. The words are\nrepresented with an embedding model.\n3.1. LSTM-based Sentence Generator\nThe choice of f in (3) is governed by its ability to deal\nwith vanishing and exploding gradients [10], the most com-\nmon challenge in designing and training RNNs. To address\nthis challenge, a particular form of recurrent nets, called\nLSTM, was introduced [10] and applied with great success\nto translation [3, 30] and sequence generation [9].\nThe core of the LSTM model is a memory cell c encod-\ning knowledge at every time step of what inputs have been\nobserved up to this step (see Figure 2) . The behavior of the\ncell is controlled by \u201cgates\u201d \u2013 layers which are applied mul-\ntiplicatively and thus can either keep a value from the gated\nlayer if the gate is 1 or zero this value if the gate is 0. In\nparticular, three gates are being used which control whether\nto forget the current cell value (forget gate f), if it should\nh\n\u03c3\n\u03c3\n\u03c3\nc\ninput\nLSTM\nmemory block\nword prediction\nsoftmax\ninput\ngate i\noutput\ngate f\nforget\ngate f\nupdating\nterm\nct-1\nct\nmt\nx\nFigure 2.\nLSTM: the memory block contains a cell c which is\ncontrolled by three gates. In blue we show the recurrent connec-\ntions \u2013 the output m at time t \u22121 is fed back to the memory at\ntime t via the three gates; the cell value is fed back via the forget\ngate; the predicted word at time t \u22121 is fed back in addition to the\nmemory output m at time t into the Softmax for word prediction.\nread its input (input gate i) and whether to output the new\ncell value (output gate o). The de\ufb01nition of the gates and\ncell update and output are as follows:\nit\n=\n\u03c3(Wixxt + Wimmt\u22121)\n(4)\nft\n=\n\u03c3(Wfxxt + Wfmmt\u22121)\n(5)\not\n=\n\u03c3(Woxxt + Wommt\u22121)\n(6)\nct\n=\nft \u2299ct\u22121 + it \u2299h(Wcxxt + Wcmmt\u22121)(7)\nmt\n=\not \u2299ct\n(8)\npt+1\n=\nSoftmax(mt)\n(9)\nwhere \u2299represents the product with a gate value, and the\nvarious W matrices are trained parameters. Such multi-\nplicative gates make it possible to train the LSTM robustly\nas these gates deal well with exploding and vanishing gra-\ndients [10]. The nonlinearities are sigmoid \u03c3(\u00b7) and hyper-\nbolic tangent h(\u00b7). The last equation mt is what is used to\nfeed to a Softmax, which will produce a probability distri-\nbution pt over all words.\nTraining\nThe LSTM model is trained to predict each\nword of the sentence after it has seen the image as well\nas all preceding words as de\ufb01ned by p(St|I, S0, . . . , St\u22121).\nFor this purpose, it is instructive to think of the LSTM in un-\nrolled form \u2013 a copy of the LSTM memory is created for the\nLSTM\nLSTM\nLSTM\nWeS1\nWeSN-1\np1\npN\np2\nlog p1(S1) \nlog p2(S2) \nlog pN(SN) \n...\nLSTM\nWeS0\nS1\nSN-1\nS0\nimage\nFigure 3. LSTM model combined with a CNN image embedder\n(as de\ufb01ned in [12]) and word embeddings. The unrolled connec-\ntions between the LSTM memories are in blue and they corre-\nspond to the recurrent connections in Figure 2. All LSTMs share\nthe same parameters.\nimage and each sentence word such that all LSTMs share\nthe same parameters and the output mt\u22121 of the LSTM at\ntime t \u22121 is fed to the LSTM at time t (see Figure 3). All\nrecurrent connections are transformed to feed-forward con-\nnections in the unrolled version. In more detail, if we denote\nby I the input image and by S = (S0, . . . , SN) a true sen-\ntence describing this image, the unrolling procedure reads:\nx\u22121\n=\nCNN(I)\n(10)\nxt\n=\nWeSt,\nt \u2208{0 . . . N \u22121}\n(11)\npt+1\n=\nLSTM(xt),\nt \u2208{0 . . . N \u22121}\n(12)\nwhere we represent each word as a one-hot vector St of\ndimension equal to the size of the dictionary. Note that we\ndenote by S0 a special start word and by SN a special stop\nword which designates the start and end of the sentence. In\nparticular by emitting the stop word the LSTM signals that a\ncomplete sentence has been generated. Both the image and\nthe words are mapped to the same space, the image by using\na vision CNN, the words by using word embedding We.\nThe image I is only input once, at t = \u22121, to inform the\nLSTM about the image contents. We empirically veri\ufb01ed\nthat feeding the image at each time step as an extra input\nyields inferior results, as the network can explicitly exploit\nnoise in the image and over\ufb01ts more easily.\nOur loss is the sum of the negative log likelihood of the\ncorrect word at each step as follows:\nL(I, S) = \u2212\nN\nX\nt=1\nlog pt(St) .\n(13)\nThe above loss is minimized w.r.t. all the parameters of the\nLSTM, the top layer of the image embedder CNN and word\nembeddings We.\nInference\nThere are multiple approaches that can be used\nto generate a sentence given an image, with NIC. The \ufb01rst\none is Sampling where we just sample the \ufb01rst word ac-\ncording to p1, then provide the corresponding embedding\nas input and sample p2, continuing like this until we sample\nthe special end-of-sentence token or some maximum length.\nThe second one is BeamSearch: iteratively consider the set\nof the k best sentences up to time t as candidates to generate\nsentences of size t + 1, and keep only the resulting best k\nof them. This better approximates S = arg maxS\u2032 p(S\u2032|I).\nWe used the BeamSearch approach in the following experi-\nments, with a beam of size 20. Using a beam size of 1 (i.e.,\ngreedy search) did degrade our results by 2 BLEU points on\naverage.\n4. Experiments\nWe performed an extensive set of experiments to assess\nthe effectiveness of our model using several metrics, data\nsources, and model architectures, in order to compare to\nprior art.\n4.1. Evaluation Metrics\nAlthough it is sometimes not clear whether a description\nshould be deemed successful or not given an image, prior\nart has proposed several evaluation metrics. The most re-\nliable (but time consuming) is to ask for raters to give a\nsubjective score on the usefulness of each description given\nthe image. In this paper, we used this to reinforce that some\nof the automatic metrics indeed correlate with this subjec-\ntive score, following the guidelines proposed in [11], which\nasks the graders to evaluate each generated sentence with a\nscale from 1 to 41.\nFor this metric, we set up an Amazon Mechanical Turk\nexperiment. Each image was rated by 2 workers. The typ-\nical level of agreement between workers is 65%. In case\nof disagreement we simply average the scores and record\nthe average as the score. For variance analysis, we perform\nbootstrapping (re-sampling the results with replacement and\ncomputing means/standard deviation over the resampled re-\nsults). Like [11] we report the fraction of scores which are\nlarger or equal than a set of prede\ufb01ned thresholds.\nThe rest of the metrics can be computed automatically\nassuming one has access to groundtruth, i.e. human gen-\nerated descriptions. The most commonly used metric so\nfar in the image description literature has been the BLEU\nscore [25], which is a form of precision of word n-grams\nbetween generated and reference sentences 2. Even though\n1 The raters are asked whether the image is described without any er-\nrors, described with minor errors, with a somewhat related description, or\nwith an unrelated description, with a score of 4 being the best and 1 being\nthe worst.\n2In this literature, most previous work report BLEU-1, i.e., they only\ncompute precision at the unigram level, whereas BLEU-n is a geometric\naverage of precision over 1- to n-grams.\nthis metric has some obvious drawbacks, it has been shown\nto correlate well with human evaluations.\nIn this work,\nwe corroborate this as well, as we show in Section 4.3.\nAn extensive evaluation protocol, as well as the generated\noutputs of our system, can be found at http://nic.\ndroppages.com/.\nBesides BLEU, one can use the perplexity of the model\nfor a given transcription (which is closely related to our\nobjective function in (1)). The perplexity is the geometric\nmean of the inverse probability for each predicted word. We\nused this metric to perform choices regarding model selec-\ntion and hyperparameter tuning in our held-out set, but we\ndo not report it since BLEU is always preferred 3. A much\nmore detailed discussion regarding metrics can be found in\n[31], and research groups working on this topic have been\nreporting other metrics which are deemed more appropriate\nfor evaluating caption. We report two such metrics - ME-\nTEOR and Cider - hoping for much more discussion and\nresearch to arise regarding the choice of metric.\nLastly, the current literature on image description has\nalso been using the proxy task of ranking a set of avail-\nable descriptions with respect to a given image (see for in-\nstance [14]). Doing so has the advantage that one can use\nknown ranking metrics like recall@k. On the other hand,\ntransforming the description generation task into a ranking\ntask is unsatisfactory: as the complexity of images to de-\nscribe grows, together with its dictionary, the number of\npossible sentences grows exponentially with the size of the\ndictionary, and the likelihood that a prede\ufb01ned sentence will\n\ufb01t a new image will go down unless the number of such\nsentences also grows exponentially, which is not realistic;\nnot to mention the underlying computational complexity\nof evaluating ef\ufb01ciently such a large corpus of stored sen-\ntences for each image. The same argument has been used in\nspeech recognition, where one has to produce the sentence\ncorresponding to a given acoustic sequence; while early at-\ntempts concentrated on classi\ufb01cation of isolated phonemes\nor words, state-of-the-art approaches for this task are now\ngenerative and can produce sentences from a large dictio-\nnary.\nNow that our models can generate descriptions of rea-\nsonable quality, and despite the ambiguities of evaluating\nan image description (where there could be multiple valid\ndescriptions not in the groundtruth) we believe we should\nconcentrate on evaluation metrics for the generation task\nrather than for ranking.\n4.2. Datasets\nFor evaluation we use a number of datasets which consist\nof images and sentences in English describing these images.\n3Even though it would be more desirable, optimizing for BLEU score\nyields a discrete optimization problem. In general, perplexity and BLEU\nscores are fairly correlated.\nThe statistics of the datasets are as follows:\nDataset name\nsize\ntrain\nvalid.\ntest\nPascal VOC 2008 [6]\n-\n-\n1000\nFlickr8k [26]\n6000\n1000\n1000\nFlickr30k [33]\n28000\n1000\n1000\nMSCOCO [20]\n82783\n40504\n40775\nSBU [24]\n1M\n-\n-\nWith the exception of SBU, each image has been annotated\nby labelers with 5 sentences that are relatively visual and\nunbiased.\nSBU consists of descriptions given by image\nowners when they uploaded them to Flickr. As such they\nare not guaranteed to be visual or unbiased and thus this\ndataset has more noise.\nThe Pascal dataset is customary used for testing only af-\nter a system has been trained on different data such as any of\nthe other four dataset. In the case of SBU, we hold out 1000\nimages for testing and train on the rest as used by [18]. Sim-\nilarly, we reserve 4K random images from the MSCOCO\nvalidation set as test, called COCO-4k, and use it to report\nresults in the following section.\n4.3. Results\nSince our model is data driven and trained end-to-end,\nand given the abundance of datasets, we wanted to an-\nswer questions such as \u201chow dataset size affects general-\nization\u201d, \u201cwhat kinds of transfer learning it would be able\nto achieve\u201d, and \u201chow it would deal with weakly labeled\nexamples\u201d. As a result, we performed experiments on \ufb01ve\ndifferent datasets, explained in Section 4.2, which enabled\nus to understand our model in depth.\n4.3.1\nTraining Details\nMany of the challenges that we faced when training our\nmodels had to do with over\ufb01tting. Indeed, purely supervised\napproaches require large amounts of data, but the datasets\nthat are of high quality have less than 100000 images. The\ntask of assigning a description is strictly harder than object\nclassi\ufb01cation and data driven approaches have only recently\nbecome dominant thanks to datasets as large as ImageNet\n(with ten times more data than the datasets we described\nin this paper, with the exception of SBU). As a result, we\nbelieve that, even with the results we obtained which are\nquite good, the advantage of our method versus most cur-\nrent human-engineered approaches will only increase in the\nnext few years as training set sizes will grow.\nNonetheless, we explored several techniques to deal with\nover\ufb01tting. The most obvious way to not over\ufb01t is to ini-\ntialize the weights of the CNN component of our system\nto a pretrained model (e.g., on ImageNet). We did this in\nall the experiments (similar to [8]), and it did help quite a\nlot in terms of generalization. Another set of weights that\ncould be sensibly initialized are We, the word embeddings.\nWe tried initializing them from a large news corpus [22],\nbut no signi\ufb01cant gains were observed, and we decided to\njust leave them uninitialized for simplicity. Lastly, we did\nsome model level over\ufb01tting-avoiding techniques. We tried\ndropout [34] and ensembling models, as well as exploring\nthe size (i.e., capacity) of the model by trading off number\nof hidden units versus depth. Dropout and ensembling gave\na few BLEU points improvement, and that is what we report\nthroughout the paper.\nWe trained all sets of weights using stochastic gradi-\nent descent with \ufb01xed learning rate and no momentum.\nAll weights were randomly initialized except for the CNN\nweights, which we left unchanged because changing them\nhad a negative impact. We used 512 dimensions for the em-\nbeddings and the size of the LSTM memory.\nDescriptions were preprocessed with basic tokenization,\nkeeping all words that appeared at least 5 times in the train-\ning set.\n4.3.2\nGeneration Results\nWe report our main results on all the relevant datasets in Ta-\nbles 1 and 2. Since PASCAL does not have a training set,\nwe used the system trained using MSCOCO (arguably the\nlargest and highest quality dataset for this task). The state-\nof-the-art results for PASCAL and SBU did not use image\nfeatures based on deep learning, so arguably a big improve-\nment on those scores comes from that change alone. The\nFlickr datasets have been used recently [11, 21, 14], but\nmostly evaluated in a retrieval framework. A notable ex-\nception is [21], where they did both retrieval and genera-\ntion, and which yields the best performance on the Flickr\ndatasets up to now.\nHuman scores in Table 2 were computed by comparing\none of the human captions against the other four. We do this\nfor each of the \ufb01ve raters, and average their BLEU scores.\nSince this gives a slight advantage to our system, given the\nBLEU score is computed against \ufb01ve reference sentences\nand not four, we add back to the human scores the average\ndifference of having \ufb01ve references instead of four.\nGiven that the \ufb01eld has seen signi\ufb01cant advances in the\nlast years, we do think it is more meaningful to report\nBLEU-4, which is the standard in machine translation mov-\ning forward. Additionally, we report metrics shown to cor-\nrelate better with human evaluations in Table 14. Despite\nrecent efforts on better evaluation metrics [31], our model\nfares strongly versus human raters. However, when evalu-\nating our captions using human raters (see Section 4.3.6),\nour model fares much more poorly, suggesting more work\n4We used the implementation of these metrics kindly provided in\nhttp://www.mscoco.org.\nMetric\nBLEU-4\nMETEOR\nCIDER\nNIC\n27.7\n23.7\n85.5\nRandom\n4.6\n9.0\n5.1\nNearest Neighbor\n9.9\n15.7\n36.5\nHuman\n21.7\n25.2\n85.4\nTable 1. Scores on the MSCOCO development set.\nApproach\nPASCAL\nFlickr\nFlickr\nSBU\n(xfer)\n30k\n8k\nIm2Text [24]\n11\nTreeTalk [18]\n19\nBabyTalk [16]\n25\nTri5Sem [11]\n48\nm-RNN [21]\n55\n58\nMNLM [14]5\n56\n51\nSOTA\n25\n56\n58\n19\nNIC\n59\n66\n63\n28\nHuman\n69\n68\n70\nTable 2. BLEU-1 scores. We only report previous work results\nwhen available. SOTA stands for the current state-of-the-art.\nis needed towards better metrics. On the of\ufb01cial test set for\nwhich labels are only available through the of\ufb01cial website,\nour model had a 27.2 BLEU-4.\n4.3.3\nTransfer Learning, Data Size and Label Quality\nSince we have trained many models and we have several\ntesting sets, we wanted to study whether we could transfer\na model to a different dataset, and how much the mismatch\nin domain would be compensated with e.g. higher quality\nlabels or more training data.\nThe most obvious case for transfer learning and data size\nis between Flickr30k and Flickr8k. The two datasets are\nsimilarly labeled as they were created by the same group.\nIndeed, when training on Flickr30k (with about 4 times\nmore training data), the results obtained are 4 BLEU points\nbetter. It is clear that in this case, we see gains by adding\nmore training data since the whole process is data-driven\nand over\ufb01tting prone. MSCOCO is even bigger (5 times\nmore training data than Flickr30k), but since the collection\nprocess was done differently, there are likely more differ-\nences in vocabulary and a larger mismatch. Indeed, all the\nBLEU scores degrade by 10 points. Nonetheless, the de-\nscriptions are still reasonable.\nSince PASCAL has no of\ufb01cial training set and was\ncollected independently of Flickr and MSCOCO, we re-\nport transfer learning from MSCOCO (in Table 2). Doing\ntransfer learning from Flickr30k yielded worse results with\nBLEU-1 at 53 (cf. 59).\nLastly, even though SBU has weak labeling (i.e., the la-\nbels were captions and not human generated descriptions),\n5We computed these BLEU scores with the outputs that the authors of\n[14] kindly provided for their OxfordNet system.\nthe task is much harder with a much larger and noisier vo-\ncabulary. However, much more data is available for train-\ning. When running the MSCOCO model on SBU, our per-\nformance degrades from 28 down to 16.\n4.3.4\nGeneration Diversity Discussion\nHaving trained a generative model that gives p(S|I), an ob-\nvious question is whether the model generates novel cap-\ntions, and whether the generated captions are both diverse\nand high quality. Table 3 shows some samples when re-\nturning the N-best list from our beam search decoder in-\nstead of the best hypothesis. Notice how the samples are di-\nverse and may show different aspects from the same image.\nThe agreement in BLEU score between the top 15 generated\nsentences is 58, which is similar to that of humans among\nthem. This indicates the amount of diversity our model gen-\nerates. In bold are the sentences that are not present in the\ntraining set. If we take the best candidate, the sentence is\npresent in the training set 80% of the times. This is not\ntoo surprising given that the amount of training data is quite\nsmall, so it is relatively easy for the model to pick \u201cexem-\nplar\u201d sentences and use them to generate descriptions. If\nwe instead analyze the top 15 generated sentences, about\nhalf of the times we see a completely novel description, but\nstill with a similar BLEU score, indicating that they are of\nenough quality, yet they provide a healthy diversity.\nA man throwing a frisbee in a park.\nA man holding a frisbee in his hand.\nA man standing in the grass with a frisbee.\nA close up of a sandwich on a plate.\nA close up of a plate of food with french fries.\nA white plate topped with a cut in half sandwich.\nA display case \ufb01lled with lots of donuts.\nA display case \ufb01lled with lots of cakes.\nA bakery display case \ufb01lled with lots of donuts.\nTable 3. N-best examples from the MSCOCO test set. Bold lines\nindicate a novel sentence not present in the training set.\n4.3.5\nRanking Results\nWhile we think ranking is an unsatisfactory way to evalu-\nate description generation from images, many papers report\nranking scores, using the set of testing captions as candi-\ndates to rank given a test image. The approach that works\nbest on these metrics (MNLM), speci\ufb01cally implemented a\nranking-aware loss. Nevertheless, NIC is doing surprisingly\nwell on both ranking tasks (ranking descriptions given im-\nages, and ranking images given descriptions), as can be seen\nin Tables 4 and 5. Note that for the Image Annotation task,\nwe normalized our scores similar to what [21] used.\nApproach\nImage Annotation\nImage Search\nR@1 R@10 Med r R@1 R@10 Med r\nDeFrag [13]\n13\n44\n14\n10\n43\n15\nm-RNN [21]\n15\n49\n11\n12\n42\n15\nMNLM [14]\n18\n55\n8\n13\n52\n10\nNIC\n20\n61\n6\n19\n64\n5\nTable 4. Recall@k and median rank on Flickr8k.\nApproach\nImage Annotation\nImage Search\nR@1 R@10 Med r R@1 R@10 Med r\nDeFrag [13]\n16\n55\n8\n10\n45\n13\nm-RNN [21]\n18\n51\n10\n13\n42\n16\nMNLM [14]\n23\n63\n5\n17\n57\n8\nNIC\n17\n56\n7\n17\n57\n7\nTable 5. Recall@k and median rank on Flickr30k.\nFigure 4.\nFlickr-8k: NIC: predictions produced by NIC on the\nFlickr8k test set (average score: 2.37); Pascal: NIC: (average\nscore: 2.45); COCO-1k: NIC: A subset of 1000 images from the\nMSCOCO test set with descriptions produced by NIC (average\nscore: 2.72); Flickr-8k: ref: these are results from [11] on Flickr8k\nrated using the same protocol, as a baseline (average score: 2.08);\nFlickr-8k: GT: we rated the groundtruth labels from Flickr8k us-\ning the same protocol. This provides us with a \u201ccalibration\u201d of the\nscores (average score: 3.89)\n4.3.6\nHuman Evaluation\nFigure 4 shows the result of the human evaluations of the\ndescriptions provided by NIC, as well as a reference system\nand groundtruth on various datasets. We can see that NIC\nis better than the reference system, but clearly worse than\nthe groundtruth, as expected. This shows that BLEU is not\na perfect metric, as it does not capture well the difference\nbetween NIC and human descriptions assessed by raters.\nExamples of rated images can be seen in Figure 5. It is\ninteresting to see, for instance in the second image of the\n\ufb01rst column, how the model was able to notice the frisbee\ngiven its size.\nFigure 5. A selection of evaluation results, grouped by human rating.\n4.3.7\nAnalysis of Embeddings\nIn order to represent the previous word St\u22121 as input to\nthe decoding LSTM producing St, we use word embedding\nvectors [22], which have the advantage of being indepen-\ndent of the size of the dictionary (contrary to a simpler one-\nhot-encoding approach). Furthermore, these word embed-\ndings can be jointly trained with the rest of the model. It\nis remarkable to see how the learned representations have\ncaptured some semantic from the statistics of the language.\nTable 4.3.7 shows, for a few example words, the nearest\nother words found in the learned embedding space.\nNote how some of the relationships learned by the model\nwill help the vision component. Indeed, having \u201chorse\u201d,\n\u201cpony\u201d, and \u201cdonkey\u201d close to each other will encourage the\nCNN to extract features that are relevant to horse-looking\nanimals. We hypothesize that, in the extreme case where\nwe see very few examples of a class (e.g., \u201cunicorn\u201d), its\nproximity to other word embeddings (e.g., \u201chorse\u201d) should\nprovide a lot more information that would be completely\nlost with more traditional bag-of-words based approaches.\n5. Conclusion\nWe have presented NIC, an end-to-end neural network\nsystem that can automatically view an image and generate\nWord\nNeighbors\ncar\nvan, cab, suv, vehicule, jeep\nboy\ntoddler, gentleman, daughter, son\nstreet\nroad, streets, highway, freeway\nhorse\npony, donkey, pig, goat, mule\ncomputer\ncomputers, pc, crt, chip, compute\nTable 6. Nearest neighbors of a few example words\na reasonable description in plain English. NIC is based on\na convolution neural network that encodes an image into a\ncompact representation, followed by a recurrent neural net-\nwork that generates a corresponding sentence. The model is\ntrained to maximize the likelihood of the sentence given the\nimage. Experiments on several datasets show the robust-\nness of NIC in terms of qualitative results (the generated\nsentences are very reasonable) and quantitative evaluations,\nusing either ranking metrics or BLEU, a metric used in ma-\nchine translation to evaluate the quality of generated sen-\ntences. It is clear from these experiments that, as the size\nof the available datasets for image description increases, so\nwill the performance of approaches like NIC. Furthermore,\nit will be interesting to see how one can use unsupervised\ndata, both from images alone and text alone, to improve im-\nage description approaches.\nAcknowledgement\nWe would like to thank Geoffrey Hinton, Ilya Sutskever,\nQuoc Le, Vincent Vanhoucke, and Jeff Dean for useful dis-\ncussions on the ideas behind the paper, and the write up.\nReferences\n[1] A. Aker and R. Gaizauskas. Generating image descriptions\nusing dependency relational patterns. In ACL, 2010.\n[2] D. Bahdanau, K. Cho, and Y. Bengio.\nNeural ma-\nchine translation by jointly learning to align and translate.\narXiv:1409.0473, 2014.\n[3] K. Cho, B. van Merrienboer, C. Gulcehre, F. Bougares,\nH. Schwenk, and Y. Bengio. Learning phrase representations\nusing RNN encoder-decoder for statistical machine transla-\ntion. In EMNLP, 2014.\n[4] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang,\nE. Tzeng, and T. Darrell. Decaf: A deep convolutional acti-\nvation feature for generic visual recognition. In ICML, 2014.\n[5] D. Elliott and F. Keller. Image description using visual de-\npendency representations. In EMNLP, 2013.\n[6] A. Farhadi,\nM. Hejrati,\nM. A. Sadeghi,\nP. Young,\nC. Rashtchian, J. Hockenmaier, and D. Forsyth. Every pic-\nture tells a story: Generating sentences from images.\nIn\nECCV, 2010.\n[7] R. Gerber and H.-H. Nagel. Knowledge representation for\nthe generation of quanti\ufb01ed natural language descriptions of\nvehicle traf\ufb01c in image sequences. In ICIP. IEEE, 1996.\n[8] Y. Gong, L. Wang, M. Hodosh, J. Hockenmaier, and\nS. Lazebnik. Improving image-sentence embeddings using\nlarge weakly annotated photo collections. In ECCV, 2014.\n[9] A. Graves. Generating sequences with recurrent neural net-\nworks. arXiv:1308.0850, 2013.\n[10] S. Hochreiter and J. Schmidhuber. Long short-term memory.\nNeural Computation, 9(8), 1997.\n[11] M. Hodosh, P. Young, and J. Hockenmaier. Framing image\ndescription as a ranking task: Data, models and evaluation\nmetrics. JAIR, 47, 2013.\n[12] S. Ioffe and C. Szegedy. Batch normalization: Accelerating\ndeep network training by reducing internal covariate shift. In\narXiv:1502.03167, 2015.\n[13] A. Karpathy, A. Joulin, and L. Fei-Fei. Deep fragment em-\nbeddings for bidirectional image sentence mapping. NIPS,\n2014.\n[14] R. Kiros, R. Salakhutdinov, and R. S. Zemel.\nUnifying\nvisual-semantic embeddings with multimodal neural lan-\nguage models. In arXiv:1411.2539, 2014.\n[15] R. Kiros and R. Z. R. Salakhutdinov. Multimodal neural lan-\nguage models. In NIPS Deep Learning Workshop, 2013.\n[16] G. Kulkarni, V. Premraj, S. Dhar, S. Li, Y. Choi, A. C. Berg,\nand T. L. Berg. Baby talk: Understanding and generating\nsimple image descriptions. In CVPR, 2011.\n[17] P. Kuznetsova, V. Ordonez, A. C. Berg, T. L. Berg, and\nY. Choi. Collective generation of natural image descriptions.\nIn ACL, 2012.\n[18] P. Kuznetsova, V. Ordonez, T. Berg, and Y. Choi. Treetalk:\nComposition and compression of trees for image descrip-\ntions. ACL, 2(10), 2014.\n[19] S. Li, G. Kulkarni, T. L. Berg, A. C. Berg, and Y. Choi. Com-\nposing simple image descriptions using web-scale n-grams.\nIn Conference on Computational Natural Language Learn-\ning, 2011.\n[20] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-\nmanan, P. Doll\u00b4ar, and C. L. Zitnick. Microsoft coco: Com-\nmon objects in context. arXiv:1405.0312, 2014.\n[21] J. Mao, W. Xu, Y. Yang, J. Wang, and A. Yuille.\nEx-\nplain images with multimodal recurrent neural networks. In\narXiv:1410.1090, 2014.\n[22] T. Mikolov, K. Chen, G. Corrado, and J. Dean.\nEf\ufb01cient\nestimation of word representations in vector space. In ICLR,\n2013.\n[23] M. Mitchell, X. Han, J. Dodge, A. Mensch, A. Goyal, A. C.\nBerg, K. Yamaguchi, T. L. Berg, K. Stratos, and H. D. III.\nMidge: Generating image descriptions from computer vision\ndetections. In EACL, 2012.\n[24] V. Ordonez, G. Kulkarni, and T. L. Berg. Im2text: Describ-\ning images using 1 million captioned photographs. In NIPS,\n2011.\n[25] K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. BLEU: A\nmethod for automatic evaluation of machine translation. In\nACL, 2002.\n[26] C. Rashtchian, P. Young, M. Hodosh, and J. Hockenmaier.\nCollecting image annotations using amazon\u2019s mechanical\nturk.\nIn NAACL HLT Workshop on Creating Speech and\nLanguage Data with Amazon\u2019s Mechanical Turk, pages 139\u2013\n147, 2010.\n[27] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,\nS. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,\nA. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual\nRecognition Challenge, 2014.\n[28] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,\nand Y. LeCun. Overfeat: Integrated recognition, localization\nand detection using convolutional networks. arXiv preprint\narXiv:1312.6229, 2013.\n[29] R. Socher, A. Karpathy, Q. V. Le, C. Manning, and A. Y. Ng.\nGrounded compositional semantics for \ufb01nding and describ-\ning images with sentences. In ACL, 2014.\n[30] I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence\nlearning with neural networks. In NIPS, 2014.\n[31] R. Vedantam, C. L. Zitnick, and D. Parikh.\nCIDEr:\nConsensus-based\nimage\ndescription\nevaluation.\nIn\narXiv:1411.5726, 2015.\n[32] B. Z. Yao, X. Yang, L. Lin, M. W. Lee, and S.-C. Zhu. I2t:\nImage parsing to text description. Proceedings of the IEEE,\n98(8), 2010.\n[33] P. Young, A. Lai, M. Hodosh, and J. Hockenmaier. From im-\nage descriptions to visual denotations: New similarity met-\nrics for semantic inference over event descriptions. In ACL,\n2014.\n[34] W. Zaremba, I. Sutskever, and O. Vinyals. Recurrent neural\nnetwork regularization. In arXiv:1409.2329, 2014.\n",
        "sentence": "",
        "context": "A close up of a sandwich on a plate.\nA close up of a plate of food with french fries.\nA white plate topped with a cut in half sandwich.\nA display case \ufb01lled with lots of donuts.\nA display case \ufb01lled with lots of cakes.\nstill with a similar BLEU score, indicating that they are of\nenough quality, yet they provide a healthy diversity.\nA man throwing a frisbee in a park.\nA man holding a frisbee in his hand.\nA man standing in the grass with a frisbee.\nAcknowledgement\nWe would like to thank Geoffrey Hinton, Ilya Sutskever,\nQuoc Le, Vincent Vanhoucke, and Jeff Dean for useful dis-\ncussions on the ideas behind the paper, and the write up.\nReferences"
    },
    {
        "title": "Backpropagation through time: what it does and how to do it",
        "author": [
            "P.J. Werbos"
        ],
        "venue": "Proceedings of the IEEE,",
        "citeRegEx": "Werbos,? \\Q1990\\E",
        "shortCiteRegEx": "Werbos",
        "year": 1990,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " An LSTM is trained by Backpropagation Trough Time (BPTT) (Werbos, 1990), and the loss function is defined by",
        "context": null
    },
    {
        "title": "A multimodal learning interface for grounding spoken language in sensory perceptions",
        "author": [
            "C. Yu",
            "D.H. Ballard"
        ],
        "venue": "ACM Transactions on Applied Perception (TAP),",
        "citeRegEx": "Yu and Ballard,? \\Q2004\\E",
        "shortCiteRegEx": "Yu and Ballard",
        "year": 2004,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    }
]