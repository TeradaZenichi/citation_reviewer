[
    {
        "title": "Understanding dropout",
        "author": [
            "Baldi",
            "Pierre",
            "Sadowski",
            "Peter J"
        ],
        "venue": "In Advances in Neural Information Processing Systems,",
        "citeRegEx": "Baldi et al\\.,? \\Q2013\\E",
        "shortCiteRegEx": "Baldi et al\\.",
        "year": 2013,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Dropout training for support vector machines",
        "author": [
            "Chen",
            "Ning",
            "Zhu",
            "Jun",
            "Jianfei",
            "Zhang",
            "Bo"
        ],
        "venue": "In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence,",
        "citeRegEx": "Chen et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Chen et al\\.",
        "year": 2014,
        "abstract": "Dropout and other feature noising schemes have shown promising results in\ncontrolling over-fitting by artificially corrupting the training data. Though\nextensive theoretical and empirical studies have been performed for generalized\nlinear models, little work has been done for support vector machines (SVMs),\none of the most successful approaches for supervised learning. This paper\npresents dropout training for linear SVMs. To deal with the intractable\nexpectation of the non-smooth hinge loss under corrupting distributions, we\ndevelop an iteratively re-weighted least square (IRLS) algorithm by exploring\ndata augmentation techniques. Our algorithm iteratively minimizes the\nexpectation of a re-weighted least square problem, where the re-weights have\nclosed-form solutions. The similar ideas are applied to develop a new IRLS\nalgorithm for the expected logistic loss under corrupting distributions. Our\nalgorithms offer insights on the connection and difference between the hinge\nloss and logistic loss in dropout training. Empirical results on several real\ndatasets demonstrate the effectiveness of dropout training on significantly\nboosting the classification accuracy of linear SVMs.",
        "full_text": "Dropout Training for Support Vector Machines\nNing Chen\nJun Zhu\nJianfei Chen\nBo Zhang\nState Key Lab of Intelligent Tech. & Systems; Tsinghua National TNList Lab;\nDepartment of Computer Science and Technology, Tsinghua University, Beijing 100084, China\n{ningchen@mail, dcszj@mail, chenjf10@mails, dcszb@mail}.tsinghua.edu.cn\nAbstract\nDropout and other feature noising schemes have shown\npromising results in controlling over-\ufb01tting by arti\ufb01-\ncially corrupting the training data. Though extensive\ntheoretical and empirical studies have been performed\nfor generalized linear models, little work has been done\nfor support vector machines (SVMs), one of the most\nsuccessful approaches for supervised learning. This\npaper presents dropout training for linear SVMs. To\ndeal with the intractable expectation of the non-smooth\nhinge loss under corrupting distributions, we develop an\niteratively re-weighted least square (IRLS) algorithm by\nexploring data augmentation techniques. Our algorithm\niteratively minimizes the expectation of a re-weighted\nleast square problem, where the re-weights have closed-\nform solutions. The similar ideas are applied to de-\nvelop a new IRLS algorithm for the expected logistic\nloss under corrupting distributions. Our algorithms of-\nfer insights on the connection and difference between\nthe hinge loss and logistic loss in dropout training. Em-\npirical results on several real datasets demonstrate the\neffectiveness of dropout training on signi\ufb01cantly boost-\ning the classi\ufb01cation accuracy of linear SVMs.\nIntroduction\nArti\ufb01cial feature noising augments the \ufb01nite training data\nwith an in\ufb01nite number of corrupted versions, by corrupting\nthe given training examples with a \ufb01xed noise distribution.\nAmong the many noising schemes, dropout training (Hin-\nton et al. 2012) is an effective way to control over-\ufb01tting by\nrandomly omitting subsets of features at each iteration of a\ntraining procedure. By formulating the feature noising meth-\nods as minimizing the expectation of some loss functions\nunder the corrupting distributions, recent work has provided\ntheoretical understandings of such schemes from the per-\nspective of adaptive regularization (Wager, Wang, and Liang\n2013); and has shown promising empirical results in vari-\nous applications, including document classi\ufb01cation (van der\nMaaten et al. 2013; Wager, Wang, and Liang 2013), named\nentity recognition (Wang et al. 2013), and image classi\ufb01ca-\ntion (Wang and Manning 2013).\nRegarding the loss functions, though much work has been\ndone on the quadratic loss, logistic loss, or the log-loss\nCopyright c\u20dd2021, Association for the Advancement of Arti\ufb01cial\nIntelligence (www.aaai.org). All rights reserved.\ninduced from a generalized linear model (GLM) (van der\nMaaten et al. 2013; Wager, Wang, and Liang 2013; Wang\net al. 2013), little work has been done on the margin-based\nhinge loss underlying the very successful support vector ma-\nchines (SVMs) (Vapnik 1995). One technical challenge is\nthat the non-smoothness of the hinge loss makes it hard to\ncompute or even approximate its expectation under a given\ncorrupting distribution. Existing methods are not directly\napplicable, therefore calling for new solutions. This paper\nattempts to address this challenge and \ufb01ll up the gap by\nextending dropout training as well as other feature noising\nschemes to support vector machines.\nPrevious efforts on learning SVMs with feature noising\nhave been devoted to either explicit corruption or an ad-\nversarial worst-case analysis. For example, virtual support\nvector machines (Burges and Scholkopf 1997) explicitly\naugment the training data, which are usually support vec-\ntors from previous learning iterations for computational ef\ufb01-\nciency, with additional examples that are corrupted through\nsome invariant transformation models. A standard SVM is\nthen learned on the corrupted data. Though simple and ef-\nfective, such an approach lacks elegance and the compu-\ntational cost of processing the additional corrupted exam-\nples could be prohibitive for many applications. The other\nwork (Globerson and Roweis 2006; Dekel and Shamir 2008;\nTeo et al. 2008) adopts an adversarial worst-case analysis to\nimprove the robustness of SVMs against feature deletion in\ntesting data. Though rigorous in theory, a worst-case sce-\nnario is unlikely to be encountered in practice. Moreover,\nthe worst-case analysis usually results in solving a complex\nand computationally demanding problem.\nIn this paper, we show that it is ef\ufb01cient to train lin-\near SVM predictors on an in\ufb01nite amount of corrupted\ncopies of the training data by marginalizing out the cor-\nruption distributions, an average-case analysis. We concen-\ntrate on dropout training, but the results are directly applica-\nble to other noising models, such as Gaussian, Poisson and\nLaplace (van der Maaten et al. 2013). For all these noising\nschemes, the resulting expected hinge loss can be upper-\nbounded by a variational objective by introducing auxil-\niary variables, which follow a generalized inverse Gaus-\nsian distribution. We then develop an iteratively re-weighted\nleast square (IRLS) algorithm to minimize the variational\nbounds. At each iteration, our algorithm minimizes the ex-\narXiv:1404.4171v1  [cs.LG]  16 Apr 2014\npectation of a re-weighted quadratic loss under the given\ncorrupting distribution, where the re-weights are computed\nin a simple closed form. We further apply the similar ideas\nto develop a new IRLS algorithm for the dropout training\nof logistic regression, which extends the well-known IRLS\nalgorithm for standard logistic regression (Hastie, Tibshi-\nrani, and Friedman 2009). Our IRLS algorithms shed light\non the connection and difference between the hinge loss\nand logistic loss in the context of dropout training, com-\nplementing to the previous analysis (Rosasco et al. 2004;\nGloberson et al. 2007) in the supervised learning settings.\nFinally, empirical results on classi\ufb01cation and a challenging\n\u201cnightmare at test time\u201d scenario (Globerson and Roweis\n2006) demonstrate the effectiveness of our approaches, in\ncomparison with various strong competitors.\nPreliminaries\nWe setup the problem in question and review the learning\nwith marginalized corrupted features.\nRegularized loss minimization\nConsider the binary classi\ufb01cation, where each training ex-\nample is a pair (x, y) with x \u2208RD being an input feature\nvector and y \u2208{+1, \u22121} being a binary label. Given a set of\ntraining data D = {(xn, yn)}N\nn=1, supervised learning aims\nto \ufb01nd a function f \u2208F that maps each input to a label. To\n\ufb01nd the optimal candidate, it commonly solves a regularized\nloss minimization problem\nmin\nf\u2208F \u2126(f) + 2c \u00b7 R(D; f),\n(1)\nwhere R(D; f) is the risk of applying f to the training data;\n\u2126(f) is a regularization term to control over-\ufb01tting; and c is\na non-negative regularization parameter.\nFor linear models, the function f is simply parameterized\nas f(x; w, b) = w\u22a4x + b, where w is the weight vector and\nb is an offset. We will denote \u03b8 := {w, b} for clarity. Then,\nthe regularization can be any Euclidean norms1, e.g., the\n\u21132-norm, \u2126(w) = \u2225w\u22252\n2, or the \u21131-norm, \u2126(w) = \u2225w\u22251.\nFor the loss functions, the most relevant measure is the\ntraining error, PN\nn=1 \u03b4(f(xn; \u03b8) \u0338= yn), which however\nis not easy to optimize. A convex surrogate loss is used\ninstead, which normally upper bounds the training error.\nTwo popular examples are the hinge loss and logistic loss2:\nRh(D; \u03b8) =\nN\nX\nn=1\nmax (0, \u2113\u2212ynf(xn; \u03b8)) ,\nRl(D; \u03b8) =\nN\nX\nn=1\n(\u2212log p(yn|xn, \u03b8)) ,\nwhere \u2113(>\n0) is the cost of making a wrong predic-\ntion, and p(yn|xn, \u03b8) := 1/(1 + exp(\u2212ynf(xn; \u03b8))) is\nthe logistic likelihood. Other losses include the quadratic\nloss, PN\nn=1(f(xn; \u03b8) \u2212yn)2, and the exponential loss,\nPN\nn=1 exp(\u2212ynf(xn; \u03b8)), whose feature noising analyses\nare relatively simpler (van der Maaten et al. 2013).\n1It is a common practice to not regularize the offset.\n2The natural logarithm is not an upper bound of the training\nerror. We can simply change the base without affecting learning.\nLearning with marginalized corruption\nLet \u02dcx be the corrupted version of the input features x. Con-\nsider the commonly used independent corrupting model:\np(\u02dcx|x) =\nD\nY\nd=1\np(\u02dcxd|xd; \u03b7d),\nwhere each individual distribution is a member of the expo-\nnential family, with the natural parameter \u03b7d. Another com-\nmon assumption is that the corrupting distribution is unbi-\nased, that is, Ep[\u02dcx|x] = x, where we use Ep[\u00b7] := Ep(\u02dcx|x)[\u00b7]\nto denote the expectation taken over the corrupting distribu-\ntion p(\u02dcx|x). Such examples include the unbiased blankout\n(or dropout) noise, Gaussian noise, Laplace noise, and Pois-\nson noise (Vincent et al. 2008; van der Maaten et al. 2013).\nFor the explicit corruption in (Burges and Scholkopf\n1997), each example (xn, yn) is corrupted M times from\nthe corrupting model p(\u02dcxn|xn), resulting in the corrupted\nexamples (\u02dcxnm, yn), m \u2208[M]. This procedure generates\na new corrupted data set \u02dcD with a larger size of NM. The\ngenerated dataset can be trained by minimizing the average\nloss function over M corrupted data points:\nL( \u02dcD; \u03b8) =\nN\nX\nn=1\n1\nM\nM\nX\nm=1\nR(\u02dcxnm, yn; \u03b8),\n(2)\nwhere R(x, y; \u03b8) is the loss function of the model incurred\non the training example (x, y). As L( \u02dcD; \u03b8) scales linearly\nwith the number of corrupted observations, this approach\nmay suffer from high computational costs.\nDropout training adopts the strategy of implicit corrup-\ntion, which learns the model with marginalized corrupted\nfeatures by minimizing the expectation of a loss function\nunder the corrupting distribution\nL(D; \u03b8) =\nN\nX\nn=1\nEp[R(\u02dcxn, yn; \u03b8)].\n(3)\nThe objective can be seen as a limit case of (2) when M \u2192\n\u221e, by the law of large numbers. Such an expectation scheme\nhas been widely adopted in previous work (Wager, Wang,\nand Liang 2013; van der Maaten et al. 2013; Wang et al.\n2013; Wang and Manning 2013).\nThe choice of the loss function R in (3) can make a sig-\nni\ufb01cant difference, in terms of computation cost and pre-\ndiction accuracy. Previous work on feature noising has cov-\nered the quadratic loss, exponential loss, logistic loss, and\nthe loss induced from generalized linear models (GLM). For\nthe quadratic loss and exponential loss, the expectation in\nEq. (3) can be computed analytically, thereby leading to\nsimple gradient descent algorithms (van der Maaten et al.\n2013). However, it does not have a closed form to compute\nthe expectation for the logistic loss or the GLM loss. Pre-\nvious analysis has resorted to approximation methods, such\nas using the second-order Taylor expansion (Wager, Wang,\nand Liang 2013) or an upper bound by applying Jensen\u2019s in-\nequality (van der Maaten et al. 2013), both of which lead to\neffective algorithms in practice. In contrast, little work has\nbeen done on the hinge loss, for which the expectation un-\nder corrupting distributions cannot be analytically computed\neither, therefore calling for new algorithms.\nLearning SVMs with Corrupting Noise\nWe now present a simple iteratively re-weighted least square\n(IRLS) algorithm to learn SVMs with the expected hinge\nloss under corrupting distributions. Our method consists of\na variational upper bound of the expected loss and a sim-\nple algorithm that iteratively minimizes an expectation of a\nre-weighted quadratic loss. We also apply the similar ideas\nto develop a simple IRLS algorithm for minimizing the\nexpected logistic loss, thereby allowing for a systematical\ncomparison of the hinge loss with the logistic and quadratic\nlosses in the context of feature noising.\nA variational bound with data augmentation\nLet \u03b6n := \u2113\u2212yn(w\u22a4\u02dcxn).3 Then, the expected hinge loss\ncan be written as\nRh(D; \u03b8) =\nN\nX\nn=1\nEp[max (0, \u03b6n)],\n(4)\nSince we do not have a closed-form of the expectation of\nthe max function, minimizing the expected loss (4) is in-\ntractable. Here, we derive a variational upper bound based\non a data augmentation formulation of the expected hinge\nloss. Let \u03c6(yn|\u02dcxn, \u03b8)\n=\nexp{\u22122c max(0, \u03b6n)} be the\npseudo-likelihood of the response variable for sample n.\nThen we have\nRh(D; \u03b8) = \u22121\n2c\nX\nn\nEp[log \u03c6(yn|\u02dcxn, \u03b8)].\n(5)\nUsing the ideas of data augmentation (Polson and Scott\n2011; Zhu et al. 2014), the pseudo-likelihood can be ex-\npressed as\n\u03c6(yn|\u02dcxn, \u03b8) =\nZ \u221e\n0\n1\n\u221a2\u03c0\u03bbn\nexp\n\u001a\n\u2212(\u03bbn + c\u03b6n)2\n2\u03bbn\n\u001b\nd\u03bbn, (6)\nwhere \u03bbn, n \u2208[N], is the augmented variable. Using (6)\nand Jensen\u2019s inequality, we can derive a variational upper\nbound L of the expected hinge loss as\nL(\u03b8, q(\u03bb)) =\nN\nX\nn=1\nn\n\u2212H(\u03bbn) + 1\n2Eq[log \u03bbn]\n(7)\n+ Eq\nh 1\n2\u03bbn\nEp(\u03bbn + c\u03b6n)2io\n+ constant,\nwhere H(\u03bbn) is the entropy of the variational distribution\nq(\u03bbn); q(\u03bb) := Q\nn q(\u03bbn) is joint distribution; and we have\nde\ufb01ned Eq[\u00b7] := Eq(\u03bb)[\u00b7] to denote the expectation taken\nover a variational distribution q. Now, our variational opti-\nmization problem is\nmin\n\u03b8,q(\u03bb)\u2208P \u2225w\u22252\n2 + L(\u03b8, q(\u03bb)),\n(8)\nwhere P is the simplex space of normalized distributions.\nWe should note that when there is no feature noise (i.e., \u02dcx =\nx), the bound is tight and we are learning the standard SVM\nclassi\ufb01er. Please see Appendix A for the derivation. We will\nempirically compare with SVM in experiments.\n3We treat the offset b implicitly by augmenting xn and \u02dcxn with\none dimension of deterministic 1. More details will be given in the\nalgorithm.\nIteratively Re-weighted Least Square Algorithm\nIn the upper bound, we note that when the variational distri-\nbution q(\u03bb) is given, the term Ep[(\u03bbn+c\u03b6n)2] is an expecta-\ntion of a quadratic loss, which can be analytically computed.\nWe leverage such a nice property and develop a coordinate\ndescent algorithm to solve problem (8). Our algorithm it-\neratively solves the following two steps, analogous to the\ncommon two-step procedure of a variational EM algorithm.\nFor q(\u03bb) (i.e., E-step): infer the variational distribution\nq(\u03bb). Speci\ufb01cally, optimize L over q(\u03bb), we get:\nq(\u03bbn) \u221d\n1\n\u221a\u03bbn\nexp\n\u001a\n\u22121\n2\n\u0012\n\u03bbn + c2Ep[\u03b62\nn]\n\u03bbn\n\u0013\u001b\n\u223cGIG\n\u0012\n\u03bbn; 1\n2, 1, c2Ep[\u03b62\nn]\n\u0013\n,\n(9)\nwhere the second-order expectation is\nEp[\u03b62\nn] = w\u22a4(Ep[\u02dcxn]Ep[\u02dcxn]\u22a4+ Vp[\u02dcxn])w\n\u22122\u2113ynw\u22a4Ep[\u02dcxn] + \u21132;\n(10)\nand Vp[\u02dcxn] is a D \u00d7 D diagonal matrix with the dth diago-\nnal element being the variance of \u02dcxnd, under the corrupting\ndistribution p(\u02dcxn|xn). We have denoted GIG(x; p, a, b) \u221d\nxp\u22121 exp(\u22121\n2( b\nx + ax)) as a generalized inverse Gaussian\ndistribution. Thus, \u03bb\u22121\nn\nfollows an inverse Gaussian distri-\nbution\nq(\u03bb\u22121\nn |\u02dcxn, \u03b8) \u223cIG\n \n\u03bb\u22121\nn ;\n1\nc\np\nE[\u03b62n]\n, 1\n!\n(11)\nFor \u03b8 := w (i.e., M-step): removing irrelevant terms, this\nstep involves minimizing the following objective:\nL[\u03b8] = \u2225w\u22252\n2 +\nN\nX\nn=1\nEp\n\u0014\nc\u03b6n + c2\n2 \u03b3n\u03b62\nn\n\u0015\n,\n(12)\nwhere \u03b3n := Eq[\u03bb\u22121\nn ]. We observe that this substep is\nequivalent to minimizing the expectation of a re-weighted\nquadratic loss, as summarized in Lemma 1, whose proof is\ndeferred to Appendix B, for brevity.\nLemma 1. Given q(\u03bb), the M-step minimizes the re-\nweighted quadratic loss (with the \u21132-norm regularizer):\n\u2225w\u22252\n2 + c2\n2\nX\nn\n\u03b3nEp\n\u0002\n(w\u22a4\u02dcxn \u2212yh\nn)2\u0003\n,\n(13)\nwhere yh\nn = (\u2113+\n1\nc\u03b3n )yn is the re-weighted label, and the\nre-weights are computed in closed-form:\n\u03b3n := Eq[\u03bb\u22121\nn ] =\n1\nc\np\nEp[\u03b62n]\n.\n(14)\nFor low-dimensional data, we can solve for the closed\nform solutions by doing matrix inversion. Speci\ufb01cally, op-\ntimizing L[\u03b8] over w, we get4:\nw =\n \n2\nc2 I +\nN\nX\nn=1\n\u03b3n(Ep[\u02dcxn\u02dcx\u22a4\nn ])\n!\u22121  N\nX\nn=1\n\u03b3nyh\nnEp[\u02dcxn]\n!\n,\n4To consider offset, we simply augment x and \u02dcx with an ad-\nditional unit of 1. The variance Vp[\u02dcxn] is augmented accordingly.\nThe identity matrix I is augmented by adding one zero row and one\nzero column.\nwhere Ep[\u02dcxn\u02dcx\u22a4\nn ] = Ep[\u02dcxn]Ep[\u02dcxn]\u22a4+ Vp[\u02dcxn]. However, if\nthe data are in a high-dimensional space, e.g., text docu-\nments, the above matrix inversion will be computationally\nexpensive. In such cases, we can use numerical methods,\ne.g., the quasi-Newton method.\nTo summarize, our algorithm iteratively minimizes the ex-\npectation of a simple re-weighted quadratic loss under the\ngiven corrupting distribution, where the re-weights \u03b3n are\ncomputed in an analytic form. Therefore, it is an extension of\nthe classical iteratively re-weighted least square (IRLS) al-\ngorithm (Hastie, Tibshirani, and Friedman 2009) for dropout\ntraining. We also observe that if we \ufb01x \u03b3n at 1\nc and set \u2113= 0,\nwe are minimizing the quadratic loss under the corrupting\ndistribution, as studied in (van der Maaten et al. 2013). We\nwill empirically show that our iterative algorithm for the ex-\npected hinge-loss will consistently improve over the stan-\ndard quadratic loss by adaptively updating \u03b3n. Finally, as\nwe assume that the corrupting distribution is unbiased, i.e.,\nEp[\u02dcx|x] = x, we only need to compute the variance of the\ncorrupting distribution, which is easy for all the existing ex-\nponential family distributions. An overview of the variance\nof the commonly used corrupting distributions can be found\nin (van der Maaten et al. 2013).\nAn IRLS algorithm for the logistic Loss\nWe now extend the above ideas to develop a new IRLS algo-\nrithm for the logistic-loss, which also minimizes the expec-\ntation of a re-weighted quadratic loss under the corrupting\ndistribution and computes the re-weights analytically.\nLet \u03c9n := w\u22a4\u02dcxn. Then the expected logistic loss under a\ncorrupting distribution is\nRl(D; w) = \u2212\nN\nX\nn=1\nEp\n\u0014\nlog\n\u0012\neyn\u03c9n\n1 + eyn\u03c9n\n\u0013\u0015\n.\n(15)\nAgain since the expectation cannot be computed in closed-\nform, we derive a variational bound as a surrogate. Speci\ufb01-\ncally, let \u03c8(yn|\u02dcxn, w) = pc(yn|\u02dcxn, w) =\necyn\u03c9n\n(1+eyn\u03c9n)c be the\npseudo-likelihood of the response variable for sample n. We\nhave Rl(D; w) = \u22121\nc\nP\nn Ep[log \u03c8(yn|\u02dcxn, w)]. Using the\nrecent work of data augmentation (Polson, Scott, and Win-\ndle 2012; Chen et al. 2013), the pseudo-likelihood can be\nexpressed as\n\u03c8(yn|\u02dcxn, w) = 1\n2c e\u03ban\u03c9n\nZ \u221e\n0\ne\u2212\u03bbn(yn\u03c9n)2\n2\np(\u03bbn)d\u03bbn, (16)\nwhere \u03ban :=\nc\n2yn and \u03bbn is the augmented Polya-gamma\nvariable, p(\u03bbn) \u223cPG(\u03bbn; c, 0). Using (16), we can derive\nthe upper bound of the expected logistic loss:\nL\u2032(w, q(\u03bb)) =\nN\nX\nn=1\nn1\n2Eq[\u03bbn]Ep[\u03c92\nn] \u2212H(\u03bbn)\n(17)\n\u2212Eq[log p(\u03bbn)] \u2212c\n2ynEp[\u03c9n]\no\n+ constant,\nand get the variational optimization problem\nmin\nw,q(\u03bb)\u2208P \u2225w\u22252\n2 + L\u2032(w, q(\u03bb)),\n(18)\nTable 1: Comparison of hinge loss and logistic loss under\nthe IRLS algorithmic framework.\nParameter \u2113Parameter c Update \u03b3n Update yn\nHinge\n\u2113\nc\nEq. (14)\nyh\nn\nLogistic\n\u2013\nc\nEq. (22)\nyl\nn\nwhere q(\u03bb) is the variational distribution\nWe solve the variational problem with a coordinate de-\nscent algorithm as follows:\nFor q(\u03bb) (i.e., E-step): optimizing L\u2032 over q(\u03bb), we have:\nq(\u03bbn) \u221dexp\n\u0012\n\u22121\n2\u03bbnEp[\u03c92\nn]\n\u0013\np(\u03bbn|c, 0)\n\u223cPG\n\u0012\n\u03bbn; c,\nq\nEp[\u03c92n]\n\u0013\n(19)\na Polya-Gamma distribution (Polson, Scott, and Windle\n2012), where Ep[\u03c92\nn] = w\u22a4(Ep[\u02dcxn]Ep[\u02dcxn]\u22a4+ Vp[\u02dcxn])w.\nFor w (i.e., M-step): removing irrelevant terms, this step\nminimizes the objective\nL\u2032\n[w] = \u2225w\u22252\n2 +\nN\nX\nn=1\n1\n2Eq[\u03bbn]Ep[\u03c92\nn] \u2212c\n2ynEp[\u03c9n]. (20)\nWe then have the optimal solution5:\nw =\n \nI + 1\n2\nN\nX\nn=1\nEq[\u03bbn]Ep[\u02dcxn\u02dcx\u22a4\nn ]\n!\u22121  \nc\n4\nN\nX\nn=1\nynEp[\u02dcxn]\n!\n.\nThis is actually equivalent to minimizing the expectation\nof a re-weighted quadratic loss, as in Lemma 2. The proof\nis similar to that of Lemma 1 and the expectation of a\nPolya-Gamma distribution follows (Polson, Scott, and Win-\ndle 2012).\nLemma 2. Given q(\u03bb), the M-step minimizes the re-\nweighted quadratic loss (with the \u21132-norm regularizer)\n\u2225w\u22252\n2 + c\n2\nX\nn\n\u03b3l\nnEp[(w\u22a4\u02dcxn \u2212yl\nn)2],\n(21)\nwhere yl\nn =\nc\n2\u03b3n yn is the re-weighted label, and \u03b3l\nn = \u03b3n\nc\nwith\n\u03b3n := Eq[\u03bbn] =\nc\n2\np\nEp[\u03c92n]\n\u00d7 e\n\u221a\nEp[\u03c92n] \u22121\n1 + e\n\u221a\nEp[\u03c92\nn] .\n(22)\nIt can be observed that if we \ufb01x \u03b3n =\nc\n2, the IRLS al-\ngorithm reduces to minimizing the expected quadratic loss\nunder the corrupting distribution. This is similar as in the\ncase with SVMs, where if we set \u2113= 0 and \ufb01x \u03b3n =\n1\nc,\nthe IRLS algorithm for SVMs essentially minimizes the ex-\npected quadratic loss under the corrupting distribution. Fur-\nthermore, by sharing a similar iterative structure, our IRLS\nalgorithms shed light on the similarity and difference be-\ntween the hinge loss and the logistic loss, as summarized in\nTable 1. Speci\ufb01cally, both losses can be minimized via itera-\ntively minimizing the expectation of a re-weighted quadratic\nloss, while they differ in the update rules of the weights \u03b3n\nand the labels yn at each iteration.\n5The offset can be similarly incorporated as in the hinge loss.\nExperiments\nWe now present empirical results on both classi\ufb01cation and\nthe challenging \u201cnightmare at test time\u201d scenario (Globerson\nand Roweis 2006) to demonstrate the effectiveness of the\ndropout training algorithm for SVMs, denoted by Dropout-\nSVM, and the new IRLS algorithm for the dropout train-\ning of the logistic loss, denoted by Dropout-Logistic. We\nconsider the unbiased dropout (or blankout) noise model6,\nthat is, p(\u02dcx = 0) = q and p(\u02dcx =\n1\n1\u2212qx) = 1 \u2212q, where\nq \u2208[0, 1) is a pre-speci\ufb01ed corruption level. The variance of\nthis model for each dimension d is Vp[\u02dcxd] =\nq\n1\u2212qx2\nd.\nBinary classi\ufb01cation\nWe \ufb01rst evaluate Dropout-SVM and Dropout-Logistic on bi-\nnary classi\ufb01cation tasks. We use the public Amazon book\nreview and kitchen review datasets (Blitzer, Dredze, and\nPereira 2007), which consist of the text reviews about books\nand kitchen, respectively. In both datasets, each document is\nrepresented as a 20,000 dimensional bag-of-words feature.\nThe binary classi\ufb01cation task is to distinguish whether a re-\nview content is positive or negative. Following the previous\nsettings, we choose 2,000 documents for training and ap-\nproximately 4,000 for testing.\nWe compare our methods with the methods presented\nin (van der Maaten et al. 2013) that minimize the quadratic\nloss with marginalized corrupted features (MCF), denoted\nby MCF-Quadratic, and that minimize the expected logistic\nloss, denoted by MCF-Logistic. MCF-Logistic was shown to\nbe the state-of-the-art method for dropout training on these\ndatasets, outperforming a wide range of competitors, includ-\ning the dropout training of the exponential loss and the var-\nious loss functions with a Poisson noise model. As we have\ndiscussed, both Dropout-SVM and Dropout-Logistic itera-\ntively minimize the expectation of a re-weighted quadratic\nloss, with the re-weights updated in closed-form. We include\nMCF-Quadratic as a baseline to demonstrate the effective-\nness of our methods on adaptively tuning the re-weights to\nget improved results. We implement both Dropout-SVM and\nDropout-Logistic using C++, and solve the re-weighted least\nsquare problems using L-BFGS methods (Liu and Nocedal\n1989), which are very ef\ufb01cient by exploring the sparsity of\nbag-of-words features when computing gradients7.\nFigure 1 shows classi\ufb01cation errors, where the results of\nMCF-Logistic and MCF-Quadratic are cited from (van der\nMaaten et al. 2013). We can see that on both datasets,\nDropout-SVM and Dropout-Logistic generally outperform\nMCF-Quadratic except when the dropout level is larger than\n0.9. In the meanwhile, the proposed two models give compa-\nrable results with (a bit better than on the kitchen dataset) the\nstate-of-art MCF-Logistic which means that dropout train-\ning on SVMs is an effective strategy for binary classi\ufb01ca-\n6Other noising models (e.g., Poisson) were shown to perform\nworse than the dropout model (van der Maaten et al. 2013). We\nhave similar observations for Dropout-SVM and the new IRLS al-\ngorithm for logistic regression.\n7We don\u2019t compare time with MCF methods, whose im-\nplementation are in Matlab (http://homepage.tudelft.nl/19j49/mcf/\nMarginalized Corrupted Features.html), slower than ours.\n0\n0.2\n0.4\n0.6\n0.8\n1\n0.13\n0.135\n0.14\n0.145\n0.15\n0.155\n0.16\n0.165\n0.17\n0.175\n0.18\nDropout level\nClassification error\nAmazon\u2212Books\n \n \nDropout\u2212SVM\nDropout\u2212Logistic\nMCF\u2212Logistic\nMCF\u2212Quadratic\n(a) books\n0\n0.2\n0.4\n0.6\n0.8\n1\n0.1\n0.105\n0.11\n0.115\n0.12\n0.125\n0.13\nDropout level\nClassification error\nAmazon\u2212Kitchen\n \n \nDropout\u2212SVM\nDropout\u2212Logistic\nMCF\u2212Logistic\nMCF\u2212Quadratic\n(b) kitchen\nFigure 1: Classi\ufb01cation errors on the Amazon datasets.\ntion. Finally, by noting that Dropout-SVM reduces to the\nstandard SVM when the corruption level q is zero, we can\nsee that dropout training can signi\ufb01cantly boost the classi\ufb01-\ncation performance for the simple linear SVMs.\n1\n2\n4\n8\n16\n32\n64\n128\n256\n0.13\n0.135\n0.14\n0.145\n0.15\n0.155\n0.16\n0.165\n0.17\nNumber of corrupted copies\nClassification error\n \n \nExplicit corruption\nDropout\u2212SVM\n...... \u221e\nFigure 2: Comparison between Dropout-SVM and the ex-\nplicit corruption for SVM on the Amazon-books datasets.\nDropout-SVM vs. Explicit corruption\nFigure 2 shows the classi\ufb01cation errors on the Amazon-\nbooks dataset when a SVM classi\ufb01er is trained using the ex-\nplicit corruption strategy as in Eq. (2). We change the num-\nber of corrupted copies (i.e., M) from 1 to 256. Following\nthe previous setups (van der Maaten et al. 2013), for each\nvalue of M we choose the dropout model with q selected by\ncross-validation. The hyper-parameter of the SVM classi\ufb01er\nis also chosen via cross-validation on the training data. We\ncan observe a clear trend that the error decreases when the\ntraining set contains more corrupted versions of the origi-\nnal training data, i.e., M gets larger in Eq. (2). It also shows\nthat the best performance is obtained when M approaches\nin\ufb01nity, which is equivalent to our Dropout-SVM.\nMulti-class classi\ufb01cation\nWe also evaluate our methods on multiclass classi\ufb01ca-\ntion tasks. We choose the CIFAR-10 image categorization\ndataset8. The CIFAR-10 dataset is the subset of the 80 mil-\nlion tiny images (Torralba, Fergus, and Freeman 2008). It\nconsists of 10 classes of 32 \u00d7 32 tiny images. We follow the\n8http://www.cs.toronto.edu/\u223ckriz/cifar.html\nTable 2: Classi\ufb01cation errors on CIFAR-10 data set.\nModel\nNo Corrupt Poisson Dropout\nDropout-SVM\n0.322\n0.309\n0.294\nDropout-Logistic\n0.312\n0.302\n0.293\nMCF-Logistic\n0.325\n0.300\n0.294\nMCF-Quadratic\n0.326\n0.291\n0.323\nexperimental setup of the previous work (Krizhevsky 2009;\nvan der Maaten et al. 2013) and represent each image as\na 8,192 dimensional feature descriptor. We use the same\n50,000 images for training and 10,000 for testing. There\nare various approaches to applying the binary Dropout-SVM\nand Dropout-Logistic to multiclass classi\ufb01cation, including\n\u201cone-vs-all\u201d and \u201cone-vs-one\u201d strategies. Here we choose\n\u201cone-vs-all\u201d, which has shown effectiveness in many appli-\ncations (Rifkin and Klautau 2004). The hyper-parameters\nare selected via cross-validation on the training set.\nTable 2 presents the results, where the results of quadratic\nloss and logistic loss under the MCF learning setting9 are\ncited from (van der Maaten et al. 2013). We also report the\nresults using Poisson noise. We can see that all the meth-\nods (except for the quadratic loss) can signi\ufb01cantly boost\nthe performance by adopting dropout training; meanwhile\nboth Dropout-SVM and Dropout-Logistic are competitive,\nin fact achieving comparable performance as the state-of-\nthe-art method (i.e., MCF-Logistic) under the dropout train-\ning setting. Finally, the Poisson corruption model is slightly\nworse than the dropout noise, consistent with the previous\nobservations (van der Maaten et al. 2013).\nNightmare at test time\nFinally, we evaluate our methods under the \u201cnightmare at\ntest time\u201d (Globerson and Roweis 2006) supervised learn-\ning scenario, where some input features that were present\nwhen building the classi\ufb01ers may \u201cdie\u201d or be deleted at test-\ning time. In such a scenario, it is crucial to design algorithms\nthat do not assign too much weight to any single feature dur-\ning testing, no matter how informative it may seem at train-\ning. Previous work has conducted the worst-case analysis as\nwell as the learning with marginalized corrupted features.\nWe take this scenario to test the robustness of our dropout\ntraining algorithms for both SVM and logistic regression.\nWe follow the setup of (van der Maaten et al. 2013).\nSpeci\ufb01cally, we choose the the MNIST dataset, which con-\nsists of 60,000 training and 10,000 testing handwritten dig-\nital images from 10 categories (i.e., 0, \u00b7 \u00b7 \u00b7 , 9). The images\nare represented by 28\u00d728 pixels which results in the feature\ndimension of 784. We train the models on the full training\nset, and evaluate the performance on different versions of\ntest set in which a certain level of the features are randomly\ndropped out, i.e., set to zero. We compare the performance of\nour dropout learning algorithms with the state-of-art MCF-\npredictors that use the logistic loss and quadratic loss. These\ntwo models also show the state-of-art performance on the\nsame task to the best of our knowledge. We also compare\n9The exponential loss was shown to be worse; thus omitted.\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\nDelete ratio\nClassification error\n \n \nDropout\u2212SVM\nDropout\u2212Logistic\nMCF\u2212Logistic\nMCF\u2212Quadratic\nHinge\u2212L2\nHinge\u2212FDROP\nFigure 3: Classi\ufb01cation errors of nightmare at test time on\nMNIST dataset.\nwith FDROP (Globerson and Roweis 2006), which is a state-\nof-the-art algorithm for the \u201cnightmare at test time\u201d setting\nthat minimizes the hinge loss under an adversarial worst-\ncase analysis. During training, we choose the best models\nover different dropout levels via cross-validation. For both\nDropout-SVM and Dropout-Logistic, we adopt the \u201cone-vs-\nall\u201d strategy as above for the multiclass classi\ufb01cation task.\nFigure 3 shows the classi\ufb01cation errors of different meth-\nods as a function of the random deletion percentage of fea-\ntures at the testing time. Following previous settings, for\neach deletion percentage, we use a small validation set with\nthe same deletion level to determine the regularization pa-\nrameters and the dropout level q on the whole training data.\nFrom the results, we can see that the proposed Dropout-\nSVM is consistently more robust than all the other competi-\ntors, including the two methods to minimize the expected\nlogistic-loss, especially when the feature deletion percent-\nage is high (e.g., > 50%). Comparing with the standard\nSVM (i.e., the method Hinge-L2) and the worst-case anal-\nysis of hinge loss (i.e., Hinge-FDROP), Dropout-SVM con-\nsistently boosts the performance when the deletion ratio is\ngreater than 10%. As expected, Dropout-SVM also signif-\nicantly outperforms the MCF method with a quadratic loss\n(i.e., MCF-Quadratic), which is a special case of Dropout-\nSVM as shown in our theory. Finally, we also note that our\niterative algorithm for the logistic-loss works slightly better\nthan the previous algorithm (i.e., MCF-Logistic) when the\ndeletion ratio is larger than 50%.\nConclusions\nWe present dropout training for SVMs, with an iteratively\nre-weighted least square (IRLS) algorithm by using data\naugmentation techniques. Similar ideas are applied to de-\nvelop a new IRLS algorithm for the dropout training of\nlogistic regression. Our IRLS algorithms provide insights\non the connection and difference among various losses in\ndropout learning settings. Empirical results on various tasks\ndemonstrate the effectiveness of our approaches.\nFor future work, it is remained open whether the kernel\ntrick can be incorporated in dropout learning. We are also in-\nterested in developing more ef\ufb01cient algorithms, e.g., online\ndropout learning, to deal with even larger datasets, and in-\nvestigating whether Dropout-SVM can be incorporated into\na deep learning architecture or learning with latent struc-\ntures (Zhu et al. 2014).\nAcknowledgments\nThis work is supported by National Key Project for\nBasic Research of China (Grant Nos: 2013CB329403,\n2012CB316301), National Natural Science Foundation of\nChina (Nos: 61305066, 61322308, 61332007), Tsinghua\nSelf-innovation Project (Grant Nos: 20121088071) and\nChina Postdoctoral Science Foundation Grant (Grant Nos:\n2013T60117, 2012M520281).\nReferences\n[Blitzer, Dredze, and Pereira 2007] Blitzer, J.; Dredze, M.;\nand Pereira, F. 2007. Biographies, bollywood, boom-boxes\nand blenders: Domain adaptation for sentiment classi\ufb01ca-\ntion. In Association of Computational Linguistics.\n[Burges and Scholkopf 1997] Burges, C., and Scholkopf, B.\n1997. Improving the accuracy and speed of support vector\nmachiens. In Advances in Neural Information Processing\nSystems.\n[Chen et al. 2013] Chen, N.; Zhu, J.; Xia, F.; and Zhang, B.\n2013. Generalized relational topic models with data aug-\nmentation. In International Joint Conference on Arti\ufb01cial\nIntelligence.\n[Dekel and Shamir 2008] Dekel, O., and Shamir, O. 2008.\nLearning to classify with missing and corrpted features. In\nInternational Conference on Machine Learning.\n[Globerson and Roweis 2006] Globerson, A., and Roweis, S.\n2006. Nightmare at test time: Robust learning by feature\ndeletion. In International Conference on Machine Learning.\n[Globerson et al. 2007] Globerson, A.; Koo, T. Y.; Carreras,\nX.; and Collins, M.\n2007.\nExponentiated gradient algo-\nrithms for log-linear structured prediction. In ICML.\n[Hastie, Tibshirani, and Friedman 2009] Hastie, T.; Tibshi-\nrani, R.; and Friedman, J. 2009. The elements of statistical\nlearning: data mining, inference, and prediction. Springer.\n[Hinton et al. 2012] Hinton, G.; Srivastava, N.; Krizhevsky,\nA.; Sutskever, I.; and Salakhutdinov, R. 2012. Improving\nneural networks by preventing co-adaptation of feature de-\ntectors. arXiv:1207.0580v1, preprint.\n[Krizhevsky 2009] Krizhevsky, A. 2009. Learning multiple\nlayers of features from tiny images. Technical report, Uni-\nversity of Toronto.\n[Liu and Nocedal 1989] Liu, D. C., and Nocedal, J. 1989.\nOn the limited memory BFGS method for large scale opti-\nmization. Mathematical Programming (45):503\u2013528.\n[Polson and Scott 2011] Polson, N. G., and Scott, S. L. 2011.\nData Augmentation for Support Vector Machines. Bayesian\nAnalysis 6(1):1\u201324.\n[Polson, Scott, and Windle 2012] Polson, N. G.; Scott, J. G.;\nand\nWindle,\nJ.\n2012.\nBayesian\nInference\nfor\nLogistic Models using Polya-Gamma Latent Variables.\narXiv:1205.0310v1.\n[Rifkin and Klautau 2004] Rifkin, R., and Klautau, A. 2004.\nIn defense of one-vs-all classi\ufb01cation. Journal of Machine\nLearning Research (5):101\u2013141.\n[Rosasco et al. 2004] Rosasco, L.; Vito, E. D.; Caponnetto,\nA.; Piana, M.; and Verri, A. 2004. Are loss functions all the\nsame? Neural Computation 16(5):1063\u20131076.\n[Teo et al. 2008] Teo, C.; Globerson, A.; Roweis, S.; and\nSmola, A. 2008. Convex learning with invariances. In Ad-\nvances in Neural Information Processing Systems.\n[Torralba, Fergus, and Freeman 2008] Torralba, A.; Fergus,\nR.; and Freeman, W.\n2008.\nA large dataset for non-\nparametric object and scene recognition. IEEE Transaction\non Pattern Analysis and Machine Intelligence 30(11):1958\u2013\n1970.\n[van der Maaten et al. 2013] van der Maaten, L.; Chen, M.;\nTyree, S.; and Weinberger, K. Q.\n2013.\nLearning with\nmarginalized corrupted features. In International Confer-\nence on Machine Learning.\n[Vapnik 1995] Vapnik, V.\n1995.\nThe nature of statistical\nlearning theory. Springer-Verlag.\n[Vincent et al. 2008] Vincent, P.; Larochelle, H.; Bengio, Y.;\nand Manzagol, P. A. 2008. Extracting and composing ro-\nbust features with denoising autoencoders. In International\nConference on Machine Learning.\n[Wager, Wang, and Liang 2013] Wager, S.; Wang, S.; and\nLiang, P. 2013. Dropout training as adaptive regularization.\nIn Advances in Neural Information Processing.\n[Wang and Manning 2013] Wang, S., and Manning, C. 2013.\nFast dropout training. In International Conference on Ma-\nchine Learning.\n[Wang et al. 2013] Wang, S.; Wang, M.; Wager, S.; Liang, P.;\nand Manning, C. 2013. Feature noising for log-linear struc-\ntured prediction. In Empirical Methods in Natural Language\nProcessing.\n[Zhu et al. 2014] Zhu, J.; Chen, N.; Perkins, H.; and Zhang,\nB. 2014. Gibbs max-margin topic models with data aug-\nmentation. Journal of Machine Learning Research (JMLR)\n15:1073\u20131110.\n",
        "sentence": " Other studies focus on shallow learning with dropout noise (Wager et al., 2014; Helmbold & Long, 2014; Chen et al., 2014). Graham et al. (2015) used the same noise across a batch of examples in order to speed-up the computation.",
        "context": "[Wang and Manning 2013] Wang, S., and Manning, C. 2013.\nFast dropout training. In International Conference on Ma-\nchine Learning.\n[Wang et al. 2013] Wang, S.; Wang, M.; Wager, S.; Liang, P.;\nand Manning, C. 2013. Feature noising for log-linear struc-\ncopies of the training data by marginalizing out the cor-\nruption distributions, an average-case analysis. We concen-\ntrate on dropout training, but the results are directly applica-\nble to other noising models, such as Gaussian, Poisson and\npirical results on several real datasets demonstrate the\neffectiveness of dropout training on signi\ufb01cantly boost-\ning the classi\ufb01cation accuracy of linear SVMs.\nIntroduction\nArti\ufb01cial feature noising augments the \ufb01nite training data"
    },
    {
        "title": "Efficient batchwise dropout training using submatrices",
        "author": [
            "Graham",
            "Benjamin",
            "Reizenstein",
            "Jeremy",
            "Robinson",
            "Leigh"
        ],
        "venue": "CoRR, abs/1502.02478,",
        "citeRegEx": "Graham et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "Graham et al\\.",
        "year": 2015,
        "abstract": "Dropout is a popular technique for regularizing artificial neural networks.\nDropout networks are generally trained by minibatch gradient descent with a\ndropout mask turning off some of the units---a different pattern of dropout is\napplied to every sample in the minibatch. We explore a very simple alternative\nto the dropout mask. Instead of masking dropped out units by setting them to\nzero, we perform matrix multiplication using a submatrix of the weight\nmatrix---unneeded hidden units are never calculated. Performing dropout\nbatchwise, so that one pattern of dropout is used for each sample in a\nminibatch, we can substantially reduce training times. Batchwise dropout can be\nused with fully-connected and convolutional neural networks.",
        "full_text": "Ef\ufb01cient batchwise dropout training using\nsubmatrices\nBen Graham\nb.graham@warwick.ac.uk\nJeremy Reizenstein\nj.f.reizenstein@warwick.ac.uk\nLeigh Robinson\nleigh.robinson@warwick.ac.uk\nFebruary 10, 2015\nAbstract\nDropout is a popular technique for regularizing arti\ufb01cial neural networks. Dropout\nnetworks are generally trained by minibatch gradient descent with a dropout mask\nturning off some of the units\u2014a different pattern of dropout is applied to every\nsample in the minibatch. We explore a very simple alternative to the dropout mask.\nInstead of masking dropped out units by setting them to zero, we perform matrix\nmultiplication using a submatrix of the weight matrix\u2014unneeded hidden units are\nnever calculated. Performing dropout batchwise, so that one pattern of dropout is\nused for each sample in a minibatch, we can substantially reduce training times.\nBatchwise dropout can be used with fully-connected and convolutional neural net-\nworks.\n1\nIndependent versus batchwise dropout\nDropout is a technique to regularize arti\ufb01cial neural networks\u2014it prevents over\ufb01tting\n[8]. A fully connected network with two hidden layers of 80 units each can learn to\nclassify the MNIST training set perfectly in about 20 training epochs\u2014unfortunately\nthe test error is quite high, about 2%. Increasing the number of hidden units by a factor\nof 10 and using dropout results in a lower test error, about 1.1%. The dropout network\ntakes longer to train in two senses: each training epoch takes several times longer,\nand the number of training epochs needed increases too. We consider a technique for\nspeeding up training with dropout\u2014it can substantially reduce the time needed per\nepoch.\nConsider a very simple \u2113-layer fully connected neural network with dropout. To\ntrain it with a minibatch of b samples, the forward pass is described by the equations:\nxk+1 = [xk \u00b7 dk] \u00d7 Wk\nk = 0, . . . , \u2113\u22121.\n1\narXiv:1502.02478v1  [cs.NE]  9 Feb 2015\nHere xk is a b \u00d7 nk matrix of input/hidden/output units, dk is a b \u00d7 nk dropout-mask\nmatrix of independent Bernoulli(1 \u2212pk) random variables, pk denotes the probability\nof dropping out units in level k, and Wk is an nk \u00d7 nk+1 matrix of weights connecting\nlevel k with level k + 1. We are using \u00b7 for (Hadamard) element-wise multiplication\nand \u00d7 for matrix multiplication. We have forgotten to include non-linear functions\n(e.g. the recti\ufb01er function for the hidden units, and softmax for the output units) but for\nthe introduction we will keep the network as simple as possible.\nThe network can be trained using the backpropagation algorithm to calculate the\ngradients of a cost function (e.g. negative log-likelihood) with respect to the Wk:\n\u2202cost\n\u2202Wk\n= [xk \u00b7 dk]T \u00d7 \u2202cost\n\u2202xk+1\n\u2202cost\n\u2202xk\n=\n\u0012 \u2202cost\n\u2202xk+1\n\u00d7 W T\nk\n\u0013\n\u00b7 dk.\nWith dropout training, we are trying to minimize the cost function averaged over an\nensemble of closely related networks. However, networks typically contain thousands\nof hidden units, so the size of the ensemble is much larger than the number of training\nsamples that can possibly be \u2018seen\u2019 during training. This suggests that the indepen-\ndence of the rows of the dropout mask matrices dk might not be terribly important; the\nsuccess of dropout simply cannot depend on exploring a large fraction of the available\ndropout masks. Some machine learning libraries such as Pylearn2 allow dropout to\nbe applied batchwise instead of independently1. This is done by replacing dk with a\n1\u00d7nk row matrix of independent Bernoulli(1\u2212pk) random variables, and then copying\nit vertically b times to get the right shape.\nTo be practical, it is important that each training minibatch can be processed quickly.\nA crude way of estimating the processing time is to count the number of \ufb02oating point\nmultiplication operations needed (naively) to evaluate the \u00d7 matrix multiplications\nspeci\ufb01ed above:\n\u2113\u22121\nX\nk=0\nb \u00d7 nk \u00d7 nk+1\n|\n{z\n}\nforwards\n+ nk \u00d7 b \u00d7 nk+1\n|\n{z\n}\n\u2202cost/\u2202W\n+ b \u00d7 nk+1 \u00d7 nk\n|\n{z\n}\nbackwards\n.\nHowever, when we take into account the effect of the dropout mask, we see that many\nof these multiplications are unnecessary. The (i, j)-th element of the Wk weight matrix\neffectively \u2018drops-out\u2019 of the calculations if unit i is dropped in level k, or if unit j is\ndropped in level k + 1. Applying 50% dropout in levels k and k + 1 renders 75% of\nthe multiplications unnecessary.\nIf we apply dropout independently, then the parts of Wk that disappear are dif-\nferent for each sample. This makes it effectively impossible to take advantage of the\nredundancy\u2014it is slower to check if a multiplication is necessary than to just do the\nmultiplication. However, if we apply dropout batchwise, then it becomes easy to take\nadvantage of the redundancy. We can literally drop-out redundant parts of the calcula-\ntions.\n1Pylearn2: see function apply dropout in mlp.py\n2\nG\nG\nG\nG\nMinibatch size\nEpoch training time, seconds\n100\n300\n1000\n3000\n0.1\n0.3\n1.0\n3.0\n10.0\n30.0\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n500N\n1000N\n2000N\n4000N\n8000N\nNo dropout\nBatchwise\nG\nG\nG\nG\n30\n40\n50\n60\n70\nMinibatch size\n% Time Saving\n100\n300\n1000\n3000\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n500N\n1000N\n2000N\n4000N\n8000N\nFigure 1: Left: MNIST training time for three layer networks (log scales) on an\nNVIDIA GeForce GTX 780 graphics card. Right: Percentage reduction in training\ntimes moving from no dropout to batchwise dropout. The time saving for the 500N\nnetwork with minibatches of size 100 increases from 33% to 42% if you instead com-\npare batchwise dropout with independent dropout.\nThe binary 1 \u00d7 nk batchwise dropout matrices dk naturally de\ufb01ne submatrices of\nthe weight and hidden-unit matrices. Let xdropout\nk\n:= xk[ : , dk] denote the submatrix\nof xk consisting of the level-k hidden units that survive dropout. Let W dropout\nk\n:=\nWk[dk, dk+1] denote the submatrix of Wk consisting of weights that connect active\nunits in level k to active units in level k + 1. The network can then be trained using the\nequations:\nxdropout\nk+1\n= xdropout\nk\n\u00d7 W dropout\nk\n\u2202cost\n\u2202W dropout\nk\n= (xdropout\nk\n)T \u00d7\n\u2202cost\n\u2202xdropout\nk+1\n\u2202cost\n\u2202xdropout\nk\n=\n\u2202cost\n\u2202xdropout\nk+1\n\u00d7 (W dropout\nk\n)T\nThe redundant multiplications have been eliminated. There is an additional bene\ufb01t\nin terms of memory needed to store the hidden units: xdropout\nk\nneeds less space than\nxk. In Section 2 we look at the performance improvement that can be achieved using\nCUDA/CUBLAS code running on a GPU. Roughly speaking, processing a minibatch\nwith 50% batchwise dropout takes as long as training a 50% smaller network on the\nsame data. This explains the nearly overlapping pairs of lines in Figure 1.\nWe should emphasize that batchwise dropout only improves performance during\ntraining; during testing the full Wk matrix is used as normal, scaled by a factor of\n1\u2212pk. However, machine learning research is often constrained by long training times\nand high costs of equipment. In Section 3 we show that all other things being equal,\nbatchwise dropout is similar to independent dropout, but faster. Moreover, with the\n3\nincrease in speed, all other things do not have to be equal. With the same resources,\nbatchwise dropout can be used to\n\u2022 increase the number of training epochs,\n\u2022 increase the number of hidden units,\n\u2022 increase the number of validation runs used to optimize \u201chyper-parameters\u201d, or\n\u2022 to train a number of independent copies of the network to form a committee.\nThese possibilities will often be useful as ways of improving generalization/reducing\ntest error.\nIn Section 4 we look at batchwise dropout for convolutional networks. Dropout\nfor convolutional networks is more complicated as weights are shared across spatial\nlocations. A minibatch passing up through a convolutional network might be repre-\nsented at an intermediate hidden layer by an array of size 100 \u00d7 32 \u00d7 12 \u00d7 12: 100\nsamples, the output of 32 convolutional \ufb01lters, at each of 12 \u00d7 12 spatial locations. It\nis conventional to use a dropout mask with shape 100 \u00d7 32 \u00d7 12 \u00d7 12; we will call this\nindependent dropout. In contrast, if we want to apply batchwise dropout ef\ufb01ciently by\nadapting the submatrix trick, then we will effectively be using a dropout mask with\nshape 1 \u00d7 32 \u00d7 1 \u00d7 1. This looks like a signi\ufb01cant change: we are modifying the\nensemble over which the average cost is optimized. During training, the error rates are\nhigher. However, testing the networks gives very similar error rates.\n1.1\nFast dropout\nWe might have called batchwise dropout fast dropout but that name is already taken\n[11]. Fast dropout is very different approach to solving the problem of training large\nneural network quickly without over\ufb01tting. We discuss some of the differences of the\ntwo techniques in the appendix.\n2\nImplementation\nIn theory, for n \u00d7 n matrices, addition is an O(n2) operation, and multiplication is\nO(n2.37...) by the Coppersmith\u2013Winograd algorithm. This suggests that the bulk of\nour processing time should be spent doing matrix multiplication, and that a perfor-\nmance improvement of about 60% should be possible compared to networks using in-\ndependent dropout, or no dropout at all. In practice, SGEMM functions use Strassen\u2019s\nalgorithm or naive matrix multiplication, so performance improvement of up to 75%\nshould be possible.\nWe implemented batchwise dropout for fully-connected and convolutional neural\nnetworks using CUDA/CUBLAS2. We found that using the highly optimized cublasS-\ngemm function to do the bulk of the work, with CUDA kernels used to form the sub-\nmatrices W dropout\nk\nand to update the Wk using \u2202cost/\u2202W dropout\nk\n, worked well. Better\n2Software available at http://www2.warwick.ac.uk/fac/sci/statistics/staff/academic-research/graham/\n4\nperformance may well be obtained by writing a SGEMM-like matrix multiplication\nfunction that understands submatrices.\nFor large networks and minibatches, we found that batchwise dropout was substan-\ntially faster, see Figure 1. The approximate overlap of some of the lines on the left\nindicates that 50% batchwise dropout reduces the training time in a similar manner to\nhalving the number of hidden units.\nThe graph on the right show the time saving obtained by using submatrices to im-\nplement dropout. Note that for consistency with the left hand side, the graph compares\nbatchwise dropout with dropout-free networks, not with networks using independent\ndropout. The need to implement dropout masks for independent dropout means that\nFigure 1 slightly undersells the performance bene\ufb01ts of batchwise dropout as an alter-\nnative to independent dropout.\nFor smaller networks, the performance improvement is lower\u2014bandwidth issues\nresult in the GPU being under utilized. If you were implementing batchwise dropout\nfor CPUs, you would expect to see greater performance gains for smaller networks as\nCPUs have a lower processing-power to bandwidth ratio.\n2.1\nEf\ufb01ciency tweaks\nIf you have n = 2000 hidden units and you drop out p = 50% of them, then the\nnumber of dropped units is approximately np = 1000, but with some small variation as\nyou are really dealing with a Binomial(n, p) random variable\u2014its standard deviation is\np\nnp(1 \u2212p) = 22.4. The sizes of the submatrices W dropout\nk\nand xdropout\nk\nare therefore\nslightly random. In the interests of ef\ufb01ciency and simplicity, it is convenient to remove\nthis randomness. An alternative to dropping each unit independently with probability p\nis to drop a subset of exactly np of the hidden units, uniformly at random from the set\nof all\n\u0000 n\nnp\n\u0001\nsuch subsets. It is still the case that each unit is dropped out with probability\np. However, within a hidden layer we no longer have strict independence regarding\nwhich units are dropped out. The probability of dropping out the \ufb01rst two hidden units\nchanges very slightly, from\np2 = 0.25\nto\nnp\nn \u00b7 np \u22121\nn \u22121 = 0.24987....\nAlso, we used a modi\ufb01ed form of NAG-momentum minibatch gradient descent [9].\nAfter each minibatch, we only updated the elements of W dropout\nk\n, not all the element\nof Wk. With vk and vdropout\nk\ndenoting the momentum matrix/submatrix corresponding\nto Wk and W dropout\nk\n, our update was\nvdropout\nk\n\u2190\u00b5vdropout\nk\n\u2212\u03b5(1 \u2212\u00b5)\u2202cost/\u2202W dropout\nk\nW dropout\nk\n\u2190W dropout\nk\n+ vdropout\nk\n.\nThe momentum still functions as an autoregressive process, smoothing out the gradi-\nents, we are just reducing the rate of decay \u00b5 by a factor of (1 \u2212pk)(1 \u2212pk+1).\n5\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\nNumber of dropout patterns used\n% Errors after 1000 epochs\n1\n4\n16\n64\n256\n1024\n4096\nG\nTest\nTrain\nFigure 2: Dropout networks trained using a restricted the number of dropout patterns\n(each \u00d7 is from an independent experiment). The blue line marks the test error for a\nnetwork with half as many hidden units trained without dropout.\n3\nResults for fully-connected networks\nThe fact that batchwise dropout takes less time per training epoch would count for\nnothing if a much larger number of epochs was needed to train the network, or if a\nlarge number of validation runs were needed to optimize the training process. We\nhave carried out a number of simple experiment to compare independent and batchwise\ndropout. In many cases we could have produced better results by increasing the training\ntime, annealing the learning rate, using validation to adjust the learning process, etc.\nWe choose not to do this as the primary motivation for batchwise dropout is ef\ufb01ciency,\nand excessive use of \ufb01ne-tuning is not ef\ufb01cient.\nFor datasets, we used:\n\u2022 The MNIST3 set of 28 \u00d7 28 pixel handwritten digits.\n\u2022 The CIFAR-10 dataset of 32x32 pixel color pictures ([4]).\n\u2022 An arti\ufb01cial dataset designed to be easy to over\ufb01t.\nFollowing [8], for MNIST and CIFAR-10 we trained networks with 20% dropout in\nthe input layer, and 50% dropout in the hidden layers. For the arti\ufb01cial dataset we\nincreased the input-layer dropout to 50% as this reduced the test error. In some cases,\nwe have used relatively small networks so that we would have time to train a number\nof independent copies of the networks. This was useful in order to see if the apparent\ndifferences between batchwise and independent dropout are signi\ufb01cant or just noise.\n3http://yann.lecun.com/exdb/mnist/\n6\n3.1\nMNIST\nOur \ufb01rst experiment explores the effect of dramatically restricting the number of dropout\npatterns seen during training. Consider a network with three hidden layers of size 1000,\ntrained for 1000 epochs using minibatches of size 100. The number of distinct dropout\npatterns, 23784, is so large that we can assume that we will never generate the same\ndropout mask twice. During independent dropout training we will see 60 million dif-\nferent dropout patterns, during batchwise dropout training we will see 100 times fewer\ndropout patterns.\nFor both types of dropout, we trained 12 independent networks for 1000 epochs,\nwith batches of size 100. For batchwise dropout we got a mean test error of 1.04%\n[range (0.92%,1.1%), s.d. 0.057%] and for independent dropout we got a mean test\nerrors of 1.03% [range (0.98%,1.08%), s.d. 0.033%]. The difference in the mean test\nerrors is not statistically signi\ufb01cant.\nTo explore further the reduction in the number of dropout patterns seen, we changed\nour code for (pseudo)randomly generating batchwise dropout patterns to restrict the\nnumber of distinct dropout patterns used. We modi\ufb01ed it to have period n minibatches,\nwith n = 1, 2, 4, 8, . . . ; see Figure 2. For n = 1 this corresponds to only ever us-\ning one dropout mask, so that 50% of the network\u2019s 3000 hidden weights are never\nactually trained (and 20% of the 784 input features are ignored). During training this\ncorresponds to training a dropout-free network with half as many hidden units\u2014the test\nerror for such a network is marked by a blue line in Figure 2. The error during testing\nis higher than the blue line because the untrained weights add noise to the network.\nIf n is less than thirteen, is it likely that some of the networks 3000 hidden units\nare dropped out every time and so receive no training. If n is in the range thirteen to\n\ufb01fty, then it is likely that every hidden unit receives some training, but some pairs of\nhidden units in adjacent layers will not get the chance to interact during training, so\nthe corresponding connection weight is untrained. As the number of dropout masks\nincreases into the hundreds, we see that it is quickly a case of diminishing returns.\n3.2\nArti\ufb01cial dataset\nTo test the effect of changing network size, we created an arti\ufb01cial dataset. It has 100\nclasses, each containing 1000 training samples and 100 test samples. Each class is de-\n\ufb01ned using an independent random walk of length 1000 in the discrete cube {0, 1}1000.\nFor each class we generated the random walk, and then used it to produce the training\nand test samples by randomly picking points along the length of walk (giving binary se-\nquences of length 1000) and then randomly \ufb02ipping 40% of the bits. We trained three\nlayer networks with n \u2208{250, 500, 1000, 2000} hidden units per layer with mini-\nbatches of size 100. See Figure 3.\nLooking at the training error against training epochs, independent dropout seems to\nlearn slightly faster. However, looking at the test errors over time, there does not seem\nto be much difference between the two forms of dropout. Note that the x-axis is the\nnumber of training epochs, not the training time. The batchwise dropout networks are\nlearning much faster in terms of real time.\n7\n20\n25\n30\n35\n40\n45\n50\nEpoch\n% Train Error\n0\n100\n200\n250N\n500N\n1000N\n2000N\nIndependent\nBatchwise\n2\n4\n6\n8\n10\nEpoch\n% Test Error\n0\n100\n200\n250N\n500N\n1000N\n2000N\nIndependent\nBatchwise\nFigure 3: Arti\ufb01cial dataset. 100 classes each corresponding to noisy observations of a\none dimensional manifold in {0, 1}1000.\n3.3\nCIFAR-10 fully-connected\nLearning CIFAR-10 using a fully connected network is rather dif\ufb01cult. We trained\nthree layer networks with n \u2208{125, 250, 500, 1000, 2000} hidden units per layer with\nminibatches of size 1000. We augmented the training data with horizontal \ufb02ips. See\nFigure 4.\n4\nConvolutional networks\nDropout for convolutional networks is more complicated as weights are shared across\nspatial locations. Suppose layer k has spatial size sk \u00d7 sk with nk features per spatial\nlocation, and if the k-th operation is a convolution with f \u00d7 f \ufb01lters. For a minibatch\nof size b, the convolution involves arrays with sizes:\nlayer k : b \u00d7 nk \u00d7 sk \u00d7 sk\nweights Wk : nk+1 \u00d7 nk \u00d7 f \u00d7 f\nDropout is normally applied using dropout masks with the same size as the layers. We\nwill call this independent dropout\u2014independent decisions are mode at every spatial\nlocation. In contrast, we de\ufb01ne batchwise dropout to mean using a dropout mask with\nshape 1 \u00d7 nk \u00d7 1 \u00d7 1. Each minibatch, each convolutional \ufb01lter is either on or off\u2014\nacross all spatial locations.\nThese two forms of regularization seem to be doing quite different things. Con-\nsider a \ufb01lter that detects the color red, and a picture with a red truck in it. If dropout is\napplied independently, then by the law of averages the message \u201cred\u201d will be transmit-\nted with very high probability, but with some loss of spatial information. In contrast,\n8\n0\n10\n20\n30\n40\n50\n60\nEpoch\n% Train Error\n0\n500\n1000\n125N\n250N\n500N\n1000N\n2000N\nIndependent\nBatchwise\n35\n40\n45\n50\nEpoch\n% Test Error\n0\n500\n1000\n125N\n250N\n500N\n1000N\n2000N\nIndependent\nBatchwise\nFigure 4: Results for CIFAR-10 using fully-connected networks of different sizes.\nwith batchwise dropout there is a 50% chance we delete the entire \ufb01lter output. Exper-\nimentally, the only substantial difference we could detect was that batchwise dropout\nresulted in larger errors during training.\nTo implement batchwise dropout ef\ufb01ciently, notice that the 1 \u00d7 nk \u00d7 1 \u00d7 1 dropout\nmasks corresponds to forming subarrays W dropout\nk\nof the weight arrays Wk with size\n(1 \u2212pk+1)nk+1 \u00d7 (1 \u2212pk)nk \u00d7 f \u00d7 f.\nThe forward-pass is then simply a regular convolutional operation using W dropout\nk\n; that\nmakes it possible, for example, to take advantage of the highly optimized cudnnConvolutionForward\nfunction from the NVIDIA cuDNN package.\n4.1\nMNIST\nFor MNIST, we trained a LeNet-5 type CNN with two layers of 5 \u00d7 5 \ufb01lters, two\nlayers of 2 \u00d7 2 max-pooling, and a fully connected layer [6]. There are three places for\napplying 50% dropout:\n32C5 \u2212MP2\n50%\n\u221264C5 \u2212MP2\n50%\n\u2212512N\n50%\n\u221210N.\nThe test errors for the two dropout methods are similar, see Figure 5.\n4.2\nCIFAR-10 with varying dropout intensity\nFor a \ufb01rst experiment with CIFAR-10 we used a small convolutional network with\nsmall \ufb01lters. The network is a scaled down version of the network from [1]; there are\nfour places to apply dropout:\n128C3 \u2212MP2\np\n\u2212256C2 \u2212MP2\np\n\u2212384C2 \u2212MP2\np\n\u2212512N\np\n\u221210N.\n9\n50\n100\n150\n200\n0.4\n0.5\n0.6\n0.7\n0.8\nEpochs\n% Test errror\nIndependent\nBatchwise\nFigure 5: MNIST test errors, training repeated three times for both dropout methods.\nThe input layer is 24 \u00d7 24. We trained the network for 1000 epochs using randomly\nchosen subsets of the training images, and re\ufb02ected each image horizontally with prob-\nability one half. For testing we used the centers of the images.\nIn Figure 6 we show the effect of varying the dropout probability p. The training\nerrors are increasing with p, and the training errors are higher for batchwise dropout.\nThe test-error curves both seem to have local minima around p = 0.2. The batchwise\ntest error curve seems to be shifted slightly to the left of the independent one, sug-\ngesting that for any given value of p, batchwise dropout is a slightly stronger form of\nregularization.\n4.3\nCIFAR-10 with many convolutional layers\nWe trained a deep convolutional network on CIFAR-10 without data augmentation.\nUsing the notation of [2], our network has the form\n(64nC2 \u2212FMP\n3\u221a\n2)12 \u2212832C2 \u2212896C1 \u2212output,\ni.e. it consists of 12 2 \u00d7 2 convolutions with 64n \ufb01lters in the n-th layer, 12 layers\nmax-pooling, followed by two fully connected layers; the network has 12.6 million pa-\nrameters. We used an increasing amount of dropout per layer, rising linearly from 0%\ndropout after the third layer to 50% dropout after the 14th. Even though the amount\nof dropout used in the middle layers is small, batchwise dropout took less than half as\nlong per epoch as independent dropout; this is because applying small amounts of in-\ndependent dropout in large hidden-layers creates a bandwidth performance-bottleneck.\nAs the network\u2019s max-pooling operation is stochastic, the test errors can be reduced\nby repetition. Batchwise dropout resulted in a average test error of 7.70% (down to\n5.78% with 12-fold testing). Independent dropout resulted in an average test error of\n7.63% (reduced to 5.67% with 12-fold testing).\n10\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n0.0\n0.1\n0.2\n0.3\n0.4\n0\n5\n10\n15\np\n% error\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nIndependent\nTraining\nBatchwise\nTraining\nIndependent\nTesting\nBatchwise\nTesting\nFigure 6: CIFAR-10 results using a convolutional network with dropout probability\np \u2208(0, 0.4). Batchwise dropout produces a slightly lower minimum test error.\n5\nConclusions and future work\nWe have implemented an ef\ufb01cient form of batchwise dropout. All other things being\nequal, it seems to learn at roughly the same speed as independent dropout, but each\nepoch is faster. Given a \ufb01xed computational budget, it will often allow you to train\nbetter networks.\nThere are other potential uses for batchwise dropout that we have not explored yet:\n\u2022 Restricted Boltzmann Machines can be trained by contrastive divergence [3] with\ndropout [8]. Batchwise dropout could be used to increase the speed of training.\n\u2022 When a fully connected network sits on top of a convolutional network, train-\ning the top and bottom of the network can be separated over different computa-\ntional nodes [5]. The fully connected top-parts of the network typically contains\n95% of the parameters\u2014keeping the nodes synchronized is dif\ufb01cult due to the\nlarge size of the matrices. With batchwise dropout, nodes could communicate\n\u2202cost/\u2202W dropout\nk\ninstead of \u2202cost/\u2202Wk and so reducing the bandwidth needed.\n\u2022 Using independent dropout with recurrent neural networks can be too disruptive\nto allow effective learning; one solution is to only apply dropout to some parts\nof the network [12]. Batchwise dropout may provide a less damaging form of\ndropout, as each unit will either be on or off for the whole time period.\n\u2022 Dropout is normally only used during training. It is generally more accurate\nuse the whole network for testing purposes; this is equivalent to averaging over\nthe ensemble of dropout patterns. However, in a \u201creal-time\u201d setting, such as\n11\nanalyzing successive frames from a video camera, it may be more ef\ufb01cient to\nuse dropout during testing, and then to average the output of the network over\ntime.\n\u2022 Nested dropout [7] is a variant of regular dropout that extends some of the prop-\nerties of PCA to deep networks. Batchwise nested dropout is particularly easy\nto implement as the submatrices are regular enough to qualify as matrices in the\ncontext of the SGEMM function (using the LDA argument).\n\u2022 DropConnect is an alternative form of regularization to dropout [10]. Instead of\ndropping hidden units, individual elements of the weight matrix are dropped out.\nUsing a modi\ufb01cation similar to the one in Section 2.1, there are opportunities for\nspeeding up DropConnect training by approximately a factor of two.\nReferences\n[1] D. Ciresan, U. Meier, and J. Schmidhuber. Multi-column deep neural networks\nfor image classi\ufb01cation. In Computer Vision and Pattern Recognition (CVPR),\n2012 IEEE Conference on, pages 3642\u20133649, 2012.\n[2] Ben Graham. Fractional max-pooling, 2014. http://arxiv.org/abs/1412.6071.\n[3] Hinton and Salakhutdinov. Reducing the Dimensionality of Data with Neural\nNetworks. SCIENCE: Science, 313, 2006.\n[4] Alex Krizhevsky. Learning Multiple Layers of Features from Tiny Images. Tech-\nnical report, 2009.\n[5] Alex Krizhevsky. One weird trick for parallelizing convolutional neural networks,\n2014. http://arxiv.org/abs/1404.5997.\n[6] Y. L. Le Cun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning ap-\nplied to document recognition. Proceedings of IEEE, 86(11):2278\u20132324, Novem-\nber 1998.\n[7] Oren Rippel, Michael A. Gelbart, and Ryan P. Adams. Learning ordered repre-\nsentations with nested dropout, 2014. http://arxiv.org/abs/1402.0915.\n[8] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Rus-\nlan Salakhutdinov. Dropout: A Simple Way to Prevent Neural Networks from\nOver\ufb01tting. Journal of Machine Learning Research, 15:1929\u20131958, 2014.\n[9] Ilya Sutskever, James Martens, George E. Dahl, and Geoffrey E. Hinton. On the\nimportance of initialization and momentum in deep learning. In ICML, volume 28\nof JMLR Proceedings, pages 1139\u20131147. JMLR.org, 2013.\n[10] Li Wan, Matthew Zeiler, Sixin Zhang, Yann Lecun, and Rob Fergus. Regular-\nization of Neural Networks using DropConnect, 2013. JMLR W&CP 28 (3) :\n1058\u20131066, 2013.\n12\n[11] Sida Wang and Christopher Manning. Fast dropout training. JMLR W & CP,\n28(2):118\u2013126, 2013.\n[12] Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network\nregularization, 2014. http://arxiv.org/abs/1409.2329.\n13\nA\nFast dropout\nWe might have called batchwise dropout fast dropout but that name is already taken\n[11]. Fast dropout is an alternative form of regularization that uses a probabilistic\nmodeling technique to imitate the effect of dropout; each hidden unit is replaced with\na Gaussian probability distribution. The fast relates to reducing the number of training\nepochs needed compared to regular dropout (with reference to results in a preprint4\nof [8]). Training a network 784-800-800-10 on the MNIST dataset with 20% input\ndropout and 50% hidden-layer dropout, fast dropout converges to a test error of 1.29%\nafter 100 epochs of L-BFGS. This appears to be substantially better than the test error\nobtained in the preprint after 100 epochs of regular dropout training.\nHowever, this is a dangerous comparison to make.\nThe authors of [8] used a\nlearning-rate scheme designed to produce optimal accuracy eventually, not after just\none hundred epochs. We tried using batchwise dropout with minibatches of size 100\nand an annealed learning rate of 0.01e\u22120.01\u00d7epoch. We trained a network with two\nhidden layers of 800 recti\ufb01ed linear units each. Training for 100 epochs resulted in a\ntest error of 1.22% (s.d. 0.03%). After 200 epochs the test error has reduced further\nto 1.12% (s.d. 0.04%). Moreover, per epoch, batchwise-dropout is faster than regular\ndropout while fast-dropout is slower. Assuming we can make comparisons across dif-\nferent programs5, the 200 epochs of batchwise dropout training take less time than the\n100 epoch of fast dropout training.\n4http://arxiv.org/abs/1207.0580\n5Using our software to implement the network, each batchwise dropout training epoch take 0.67 times\nas long as independent dropout. In [11] a \ufb01gures of 1.5 is given for the ratio between fast- and independent-\ndropout when using minibatch SGD; when using L-BFGS to train fast-dropout networks the training time per\nepoch will presumably be even more than 1.5 times longer, as L-BFGS use line-searches requiring additional\nforward passes through the neural network.\n14\n",
        "sentence": "",
        "context": "Over\ufb01tting. Journal of Machine Learning Research, 15:1929\u20131958, 2014.\n[9] Ilya Sutskever, James Martens, George E. Dahl, and Geoffrey E. Hinton. On the\nimportance of initialization and momentum in deep learning. In ICML, volume 28\n6\n3.1\nMNIST\nOur \ufb01rst experiment explores the effect of dramatically restricting the number of dropout\npatterns seen during training. Consider a network with three hidden layers of size 1000,\ndropout. In many cases we could have produced better results by increasing the training\ntime, annealing the learning rate, using validation to adjust the learning process, etc."
    },
    {
        "title": "On the inductive bias of dropout",
        "author": [
            "Helmbold",
            "David P",
            "Long",
            "Philip M"
        ],
        "venue": "CoRR, abs/1412.4736,",
        "citeRegEx": "Helmbold et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Helmbold et al\\.",
        "year": 2014,
        "abstract": "Dropout is a simple but effective technique for learning in neural networks\nand other settings. A sound theoretical understanding of dropout is needed to\ndetermine when dropout should be applied and how to use it most effectively. In\nthis paper we continue the exploration of dropout as a regularizer pioneered by\nWager, et.al. We focus on linear classification where a convex proxy to the\nmisclassification loss (i.e. the logistic loss used in logistic regression) is\nminimized. We show: (a) when the dropout-regularized criterion has a unique\nminimizer, (b) when the dropout-regularization penalty goes to infinity with\nthe weights, and when it remains bounded, (c) that the dropout regularization\ncan be non-monotonic as individual weights increase from 0, and (d) that the\ndropout regularization penalty may not be convex. This last point is\nparticularly surprising because the combination of dropout regularization with\nany convex loss proxy is always a convex function.\n  In order to contrast dropout regularization with $L_2$ regularization, we\nformalize the notion of when different sources are more compatible with\ndifferent regularizers. We then exhibit distributions that are provably more\ncompatible with dropout regularization than $L_2$ regularization, and vice\nversa. These sources provide additional insight into how the inductive biases\nof dropout and $L_2$ regularization differ. We provide some similar results for\n$L_1$ regularization.",
        "full_text": "On the Inductive Bias of Dropout\nDavid P. Helmbold\nUC Santa Cruz\ndph@soe.ucsc.edu\nPhilip M. Long\nMicrosoft\nplong@microsoft.com\nFebruary 18, 2015\nAbstract\nDropout is a simple but effective technique for learning in neural networks and other settings. A\nsound theoretical understanding of dropout is needed to determine when dropout should be applied\nand how to use it most effectively. In this paper we continue the exploration of dropout as a regularizer\npioneered by Wager, et.al. We focus on linear classi\ufb01cation where a convex proxy to the misclassi\ufb01cation\nloss (i.e. the logistic loss used in logistic regression) is minimized. We show:\n\u2022 when the dropout-regularized criterion has a unique minimizer,\n\u2022 when the dropout-regularization penalty goes to in\ufb01nity with the weights, and when it remains\nbounded,\n\u2022 that the dropout regularization can be non-monotonic as individual weights increase from 0, and\n\u2022 that the dropout regularization penalty may not be convex.\nThis last point is particularly surprising because the combination of dropout regularization with any\nconvex loss proxy is always a convex function.\nIn order to contrast dropout regularization with L2 regularization, we formalize the notion of when\ndifferent sources are more compatible with different regularizers. We then exhibit distributions that are\nprovably more compatible with dropout regularization than L2 regularization, and vice versa. These\nsources provide additional insight into how the inductive biases of dropout and L2 regularization differ.\nWe provide some similar results for L1 regularization.\n1\narXiv:1412.4736v4  [cs.LG]  17 Feb 2015\n1\nIntroduction\nSince its prominent role in a win of the ImageNet Large Scale Visual Recognition Challenge [Hinton, 2012,\nHinton et al., 2012], there has been intense interest in dropout (see the work by Dahl [2012], L. Deng\n[2013], Dahl et al. [2013], Wan et al. [2013], Wager et al. [2013], Baldi and Sadowski [2013], Van Erven\net al. [2014]). This paper studies the inductive bias of dropout: when one chooses to train with dropout,\nwhat prior preference over models results? We show that dropout training shapes the learner\u2019s search space\nin a much different way than L1 or L2 regularization. Our results shed new insight into why dropout prefers\nrare features, how the dropout probability affects the strength of regularization, and how dropout restricts\nthe co-adaptation of weights.\nOur theoretical study will concern learning a linear classi\ufb01er via convex optimization. The learner\nwishes to \ufb01nd a parameter vector w so that, for a random feature-label pair (x, y) \u2208Rn \u00d7 {\u22121, 1} drawn\nfrom some joint distribution P, the probability that sign(w \u00b7 x) \u0338= y is small. It does this by using training\ndata to try to minimize E(\u2113(yw \u00b7 x)), where \u2113(z) = ln(1 + exp(\u2212z)) is the loss function associated with\nlogistic regression.\nWe have chosen to focus on this problem for several reasons. First, the inductive bias of dropout is not\nwell understood even in this simple setting. Second, linear classi\ufb01ers remain a popular choice for practical\nproblems, especially in the case of very high-dimensional data. Third, we view a thorough understanding\nof dropout in this setting as a mandatory prerequisite to understanding the inductive bias of dropout when\napplied in a deep learning architecture. This is especially true when the preference over deep learning\nmodels is decomposed into preferences at each node. In any case, the setting that we are studying faithfully\ndescribes the inductive bias of a deep learning system at its output nodes.\nWe will borrow the following clean and illuminating description of dropout as arti\ufb01cial noise due to Wa-\nger et al. [2013]. An algorithm for linear classi\ufb01cation using loss \u2113and dropout updates its parameter vector\nw online, using stochastic gradient descent. Given an example (x, y), the dropout algorithm independently\nperturbs each feature i of x: with probability q, xi is replaced with 0, and, with probability p = 1 \u2212q, xi is\nreplaced with xi/p. Equivalently, x is replaced by x + \u03bd, where\n\u03bdi =\n\u001a \u2212xi\nwith probability q\n(1/p \u22121)xi\nwith probability p = 1 \u2212q\nbefore performing the stochastic gradient update step. (Note that, while \u03bd obviously depends on x, if we\nsample the components of b \u2208{\u22121, 1/p \u22121}n independently of one another and x, by choosing bi = \u22121\nwith the dropout probability q, then we may write \u03bdi = bixi.)\nStochastic gradient descent is known to converge under a broad variety of conditions [Kushner and Yin,\n1997]. Thus, if we abstract away sampling issues as done by Breiman [2004], Zhang [2004], Bartlett et al.\n[2006], Long and Servedio [2010], we are led to consider\nw\u2217def\n= argminwE(x,y)\u223cP,\u03bd(\u2113(yw \u00b7 (x + \u03bd)))\nas dropout can be viewed as a stochastic gradient update of this global objective function. We call this objec-\ntive the dropout criterion, and it can be viewed as a risk on the dropout-induced distribution. (Abstracting\naway sampling issues is consistent with our goal of concentrating on the inductive bias of the algorithm.\nFrom the point of view of a bias-variance decomposition, we do not intend to focus on the large-sample-size\ncase, where the variance is small, but rather to focus on the contribution from the bias where P could be an\nempirical sample distribution. )\n2\nWe start with the observation of Wager et al. [2013] that the dropout criterion may be decomposed as\nE(x,y)\u223cP,\u03bd(\u2113(yw \u00b7 (x + \u03bd))) = E(x,y)\u223cP (\u2113(yw \u00b7 x)) + regD,q(w),\n(1)\nwhere regD,q is non-negative, and depends only on the marginal distribution D over the feature vectors x\n(along with the dropout probability q), and not on the labels. This leads naturally to a view of dropout as a\nregularizer.\nA popular style of learning algorithm minimizes an objective function like the RHS of (1), but where\nregD,q(w) is replaced by a norm of w. One motivation for algorithms in this family is to \ufb01rst replace the\ntraining error with a convex proxy to make optimization tractable, and then to regularize using a convex\npenalty such as a norm, so that the objective function remains convex.\nWe show that regD,q(w) formalizes a preference for classi\ufb01ers that assign a very large weight to a single\nfeature. This preference is stronger than what one gets from a penalty proportional to ||w||1. In fact, we\nshow that, despite the convexity of the dropout risk, regD,q is not convex, so that dropout provides a way to\nrealize the inductive bias arising from a non-convex penalty, while still enjoying the bene\ufb01t of convexity in\nthe overall objective function (see the plots in Figures 1, 2 and 3). Figure 1 shows the even more surprising\nresult that the dropout regularization penalty is not even monotonic in the absolute values of the individual\nweights.\nIt is not hard to see that regD,q(0) = 0. Thus, if regD,q(w) is greater than the expected loss incurred\nby 0 (which is ln 2), then it might as well be in\ufb01nity, because dropout will prefer 0 to w. However, in some\ncases, dropout never reaches this extreme \u2013 it remains willing to use a model, even if its parameter is very\nlarge, unlike methods that use a convex penalty. In particular,\nregD,q(w1, 0, 0, 0, ..., 0) < ln 2\nfor all D, no matter how large w1 gets; of course, the same is true for the other features. On the other hand,\nexcept for some special cases (which are detailed in the body of the paper),\nregD,q(cw1, cw2, 0, 0, ..., 0)\ngoes to in\ufb01nity with c. It follows that regD,q cannot be approximated to within any factor, constant or\notherwise, by a convex function of w.\nTo get a sense of which sources dropout can be successfully applied to, we compare dropout with an\nalgorithm that regularizes using L2, by minimizing the L2 criterion:\nE(x,y)\u223cP (\u2113(yw \u00b7 x)) + \u03bb\n2 ||w||2\n2.\n(2)\nWill will use \u201cL2\u201d as a shorthand to refer to an algorithm that minimizes (2). Note that q, the probability\nof dropping out an input feature, plays a role in dropout analogous to \u03bb. In particular, as q goes to zero the\nexamples remain unperturbed and the dropout regularization has no effect.\nInformally, we say that joint probability distributions P and Q separate dropout from L2 if, when the\nsame parameters \u03bb and q are used for both P and Q, then using dropout leads to a much more accurate\nhypothesis for P, and using L2 leads to a much more accurate hypothesis for Q. This enables us to illus-\ntrate the inductive biases of the algorithms through the use of contrasting sources that either align or are\nincompatible with the algorithms\u2019 inductive bias. Comparing with another regularizer helps to restrict these\nillustrative examples to \u201creasonable\u201d sources, which can be handled using another regularizer. Ensuring\nthat the same values of the regularization parameter are used for both P and Q controls for the amount of\n3\nregularization, and ensures that the difference is due to the model preferences of the respective regularizers.\nThis style of analysis is new, as far as we know, and may be a useful tool for studying the inductive biases\nof other algorithms and in other settings.\nRelated previous work. Our research builds on the work of Wager et al. [2013], who analyzed dropout\nfor random (x, y) pairs where the distribution of y given x comes from a member of the exponential family,\nand the quality of a model is evaluated using the log-loss. They pointed out that, in these cases, the dropout\ncriterion can be decomposed into the original loss and a term that does not depend on y, which therefore\ncan be viewed as a regularizer. They then proposed an approximation to this dropout regularizer, discussed\nits relationship with other regularizers and training algorithms, and evaluated it experimentally. Baldi and\nSadowski [2013] exposed properties of dropout when viewed as an ensemble method (see also Bachman\net al. [2014]). Van Erven et al. [2014] showed that applying dropout for online learning in the experts\nsetting leads to algorithms that adapt to important properties of the input without requiring doubling or other\nparameter-tuning techniques, and Abernethy et al. [2014] analyzed a class of methods including dropout by\nviewing these methods as smoothers. The impact of dropout on generalization (roughly, how much dropout\nrestricts the search space of the learner, or, from a bias-variance point of view, its impact on variance)\nwas studied by Wan et al. [2013] and Wager et al. [2014]. The latter paper considers a variant of dropout\ncompatible with a poisson source, and shows that under some assumptions this dropout variant converges\nmore quickly to its in\ufb01nite sample limit than non-dropout training, and that the Bayes-optimal predictions\nare preserved under the modi\ufb01ed dropout distribution. Our results complement theirs by focusing on the\neffect of the original dropout on the algorithm\u2019s bias.\nSection 2 de\ufb01nes our notation and characterizes when the dropout criterion has a unique minimizer.\nSection 3 presents many additional properties of the dropout regularizer. Section 4 formally de\ufb01nes when\ntwo distributions separate two algorithms or regularizers. Sections 5 and 6 give sources over R2 that separate\ndropout and L2. Section 7 provides plots demonstrating that the same distributions separated dropout from\nL1 regularization. Sections 8 and 9 give separation results from L2 with many features.\n2\nPreliminaries\nWe use w\u2217for the optimizer of the dropout criterion, q for the probability that a feature is dropped out, and\np = 1 \u2212q for the probability that a feature is kept throughout the paper. As in the introduction, if X \u2286Rn\nand P is a joint distribution over X \u00d7 {\u22121, 1}, de\ufb01ne\nw\u2217(P, q) def\n= argminwE(x,y)\u223cP,\u03bd(\u2113(yw \u00b7 (x + \u03bd)))\n(3)\nwhere \u03bdi = bixi for b1, ..., bn sampled independently at random from {\u22121, 1/p \u22121} with Pr(bi = 1/p \u2212\n1) = p = 1 \u2212q, and \u2113(z) is the logistic loss function:\n\u2113(z) = ln(1 + exp(\u2212z)).\nFor some analyses, an alternative representation of w\u2217(P, q) will be easier to work with. Let r1, ..., rn\nbe sampled randomly from {0, 1}, independently of (x, y) and one another, with Pr(ri = 1) = p. De\ufb01ning\nr \u2299x = (x1r1, ..., xnrn), we have the equivalent de\ufb01nition\nw\u2217(P, q) = p argminwE(x,y)\u223cP,r(\u2113(yw \u00b7 (r \u2299x))).\n(4)\n4\nTo see that they are equivalent, note that\nE(\u2113(yw \u00b7 (x + \u03bd))) = E\n\u0012\n\u2113\n\u0012\nyw \u00b7\n\u0012r \u2299x\np\n\u0013\u0013\u0013\n= E(\u2113(y(w/p) \u00b7 (r \u2299x))).\nAlthough this paper focuses on the logistic loss, the above de\ufb01nitions can be used for any loss function \u2113().\nSince the dropout criterion is an expectation of \u2113(), we have the following obvious consequence.\nProposition 1 If loss \u2113(\u00b7) is convex, then the dropout criterion is also a convex function of w.\nSimilarly, we use v for the optimizer of the L2 regularized criterion:\nv(P, \u03bb) def\n= argminwE(x,y)\u223cP (\u2113(yw \u00b7 x)) + \u03bb\n2 ||w||2.\n(5)\nIt is not hard to see that the \u03bb\n2||w||2 term implies that v(P, \u03bb) is always well-de\ufb01ned. On the other hand,\nw\u2217(P, q) is not always well-de\ufb01ned, as can be seen by considering any distribution concentrated on a single\nexample. This motivates the following de\ufb01nition.\nDe\ufb01nition 2 Let P be a joint distribution with support contained in Rn\u00d7{\u22121, 1}. A feature i is perfect modulo ties\nfor P if either yxi \u22650 for all x in the support of P, or yxi \u22640 for all x in the support of P.\nPut another way, i is perfect modulo ties if there is a linear classi\ufb01er that only pays attention to feature i and\nis perfect on the part of P where xi is nonzero.\nProposition 3 For all \ufb01nite domains X \u2286Rn, all distributions P with support in X, and all q \u2208(0, 1),\nwe have that E(x,y)\u223cP,r(\u2113(yw \u00b7 (r \u2299x))) has a unique minimum in Rn if and only if no feature is perfect\nmodulo ties for P.\nProof: Assume for contradiction that feature i is perfect modulo ties for P and some w\u229bis the unique\nminimizer of E(x,y)\u223cP,r(\u2113(yw \u00b7 (r \u2299x))). Assume w.l.o.g. that yxi \u22650 for all x in the support of P (the\ncase where yxi \u22640 is analogous). Increasing w\u229b\ni keeps the loss unchanged on examples where xi = 0 and\ndecreases the loss on the other examples in the support of P, contradicting the assumption that w\u229bwas a\nunique minimizer of the expected loss.\nNow, suppose then each feature i has both examples where yxi > 0 and examples where yxi < 0 in\nthe support of P. Since the support of P is \ufb01nite, there is a positive lower bound on the probability of any\nexample in the support. With probability p(1 \u2212p)n\u22121, component ri of random vector r is non-zero and the\nremaining n\u22121 components are all zero. Therefore as wi increases without bound in the positive or negative\ndirection, E(x,y)\u223cP,r(\u2113(yw\u00b7(r\u2299x))) also increases without bound. Since E(x,y)\u223cP,r(\u2113(y0\u00b7(r\u2299x))) = ln 2,\nthere is a value M depending only on distribution P and the dropout probability such that minimizing\nE(x,y)\u223cP,r(\u2113(yw \u00b7 (r \u2299x))) over w \u2208[\u2212M, M]n is equivalent to minimizing E(x,y)\u223cP,r(\u2113(yw \u00b7 (r \u2299x)))\nover Rn. Since Pr(x,y)(xi = 0) \u0338= 1 for all i, {r \u2299x : r \u2208{0, 1}n, x \u2208X} has full rank and therefore\nE(x,y)\u223cP,r(\u2113(yw \u00b7 (r \u2299x))) is strictly convex. Since a strictly convex function de\ufb01ned on a compact set has\na unique minimum, E(x,y)\u223cP,r(\u2113(yw\u00b7(r\u2299x))) has a unique minimum on [\u2212M, M]n, and therefore on Rn.\nSee Table 1 for a summary of the notation used in the paper.\n5\nx = (x1, . . . , xn)\nfeature vector in Rn\ny\nlabel in {\u22121, +1}\nw = (w1, . . . , wn)\nweight vector in Rn\n\u2113(yw \u00b7 x)\nloss function, generally the logistic loss: ln(1 + exp(\u2212yw \u00b7 x))\nP, Q\nsource distributions over (x, y) pairs, varies by section\nD\nmarginal distribution over x\nq\nfeature dropout probability in (0, 1)\np = 1 \u2212q\nprobability of keeping a feature\n\u03bb\nL2 regularization parameter\n\u03bd = (\u03bd1, . . . , \u03bdn)\nadditive dropout noise, \u03bdi \u2208{\u2212xi, xi/p \u2212xi}\nr = (r1, . . . , rn)\nmultiplicative dropout noise, ri \u2208{0, 1}\n\u2299\ncomponent-wise product: r \u2299x = (r1x1, . . . , rnxn)\nw\u2217(P, q) and w\u2217\nminimizer of dropout criterion: E(\u2113(y w \u00b7 (x + \u03bd)))\nw\u229b= w\u2217/p\nminimizer of expected loss E(\u2113(y w \u00b7 (r \u2299x)))\nv(P, \u03bb) and v\nminimizer of L2-regularized loss\nregD,q(w)\nregularization due to dropout\nJ, K\ncriteria to be optimized, varies by sub-section\ng(w), g\ngradients of the current criterion\nerP (w)\n0-1 classi\ufb01cation generalization error of sign(w \u00b7 x)\nTable 1: Summary of notation used throughout the paper.\n3\nProperties of the Dropout Regularizer\nWe start by rederiving the regularization function corresponding to dropout training previously presented in\nWager et al. [2013], specialized to our context and using our notation. The \ufb01rst step is to write \u2113(yw \u00b7 x) in\nan alternative way that exposes some symmetries:\n\u2113(yw \u00b7 x) = ln(1 + exp(\u2212yw \u00b7 x))\n= ln\n\u0012exp(y(w \u00b7 x)/2) + exp(\u2212y(w \u00b7 x)/2)\nexp(y(w \u00b7 x)/2)\n\u0013\n= ln\n\u0012exp((w \u00b7 x)/2) + exp(\u2212(w \u00b7 x)/2)\nexp(y(w \u00b7 x)/2)\n\u0013\n.\n(6)\nThis then implies\nregD,q(w) = E(\u2113(yw \u00b7 (x + \u03bd))) \u2212E(\u2113(yw \u00b7 x))\n= E\n\u0012\nln\n\u0012exp((w \u00b7 (x + \u03bd))/2) + exp(\u2212(w \u00b7 (x + \u03bd))/2)\nexp(y(w \u00b7 (x + \u03bd))/2)\n\u00d7\nexp(y(w \u00b7 x)/2)\nexp((w \u00b7 x)/2) + exp(\u2212(w \u00b7 x)/2)\n\u0013\u0013\n= E\n\u0010\nln\n\u0012exp((w \u00b7 (x+\u03bd))/2)+exp(\u2212(w \u00b7 (x+\u03bd))/2)\nexp((w \u00b7 x)/2)+exp(\u2212(w \u00b7 x)/2)\n\u0013\n\u2212y(w \u00b7 \u03bd)/2\n\u0011\n.\nSince E(\u03bd) = 0, we get the following.\nProposition 4 [Wager et al., 2013]\nregD,q(w) = E\n\u0012\nln\n\u0012exp(w \u00b7 (x + \u03bd)/2) + exp(\u2212w \u00b7 (x + \u03bd)/2)\nexp((w \u00b7 x)/2) + exp(\u2212(w \u00b7 x)/2)\n\u0013\u0013\n.\n(7)\n6\nUsing a Taylor expansion, Wager et al. [2013] arrived at the following approximation:\nq\n2(1 \u2212q)\nX\ni\nw2\ni Ex\n\u0012\nx2\ni\n(1 + exp(\u2212w\u00b7x\n2 ))(1 + exp( w\u00b7x\n2 )\n\u0013\n.\n(8)\nThis approximation suggests two properties: the strength of the regularization penalty decreases exponen-\ntially in the prediction con\ufb01dence |w \u00b7 x|, and that the regularization penalty goes to in\ufb01nity as the dropout\nprobability q goes to 1. However, w \u00b7 \u03bd can be quite large, making a second-order Taylor expansion inac-\ncurate.1 In fact, the analysis in this section suggests that the regularization penalty does not decrease with\nthe con\ufb01dence and that the regularization penalty increases linearly with q = 1 \u2212p (Figure 1, Theorem 8,\nProposition 9).\nThe following propositions show that regD,q(w) satis\ufb01es at least some of the intuitive properties of a\nregularizer.\nProposition 5 regD,q(0) = 0.\nProposition 6 [Wager et al., 2013] The contribution of each x to the regularization penalty (7) is non-\nnegative: for all x,\nE\u03bd\n\u0012\nln\n\u0012exp((w \u00b7 (x + \u03bd))/2) + exp(\u2212(w \u00b7 (x + \u03bd))/2)\nexp((w \u00b7 x)/2) + exp(\u2212(w \u00b7 x)/2)\n\u0013\u0013\n\u22650.\nProof: The proposition follows from Jensen\u2019s Inequality.\nThe w\u2217(P, q) vector learned by dropout training minimizes E(x,y)\u223cP (\u2113(yw\u00b7x))+regD,q(w). However,\nthe 0 vector has \u2113(y0 \u00b7 x) = ln(2) and regD,q(0) = 0, implying:\nProposition 7 regD,q(w\u2217) \u2264ln(2).\nThus any regularization penalty greater than ln(2) is effectively equivalent to a regularization penalty of \u221e.\nWe now present new results based on analyzing the exact regD,q(w). The next properties show that the\ndropout regularizer is emphatically not like other convex or norm-based regularization penalties in that the\ndropout regularization penalty always remains bounded when a single component of the weight vector goes\nto in\ufb01nity (see also Figure 1).\nTheorem 8 For all dropout probabilities 1 \u2212p \u2208(0, 1), all n, all marginal distributions D over n-feature\nvectors, and all indices 1 \u2264i \u2264n,\nsup\nwi\nregD,q(0, . . . , 0\n| {z }\ni\u22121\n, wi, 0, . . . , 0\n| {z }\nn\u2212i\n) \u2264PrD(xi \u0338= 0)(1 \u2212p) ln(2) < ln 2.\nProof: Fix arbitrary n, p, i, and D. We have\nregD,q(0, . . . , 0\n| {z }\ni\u22121\n, wi, 0, . . . , 0\n| {z }\nn\u2212i\n)\n= Ex,\u03bd\n\u0012\nln\n\u0012exp(\u2212wi(xi+\u03bdi)/2)+exp(wi(xi+\u03bdi)/2)\nexp(\u2212wixi/2)+exp(wixi/2)\n\u0013\u0013\n.\n1Wager et al. [2013] experimentally evaluated the accuracy of a related approximation in the case that, instead of using dropout,\n\u03bd was distributed according to a zero-mean gaussian.\n7\nFigure 1: The p = 1/2 dropout regularization for x = (1, 1) as a function of wi when the other weights are\n0 together with its approximation (8) (left) and as a function of w1 for different values of the second weight\n(right).\nFix an arbitrary x in the support of D and examine the expectation over \u03bd for that x. Recall that xi + \u03bdi is\n0 with probability 1 \u2212p and is xi/p with probability p, and we will use the substitution z = |wixi|/2.\nE\u03bd\n \nln\n \nexp( \u2212wi(xi+\u03bdi)\n2\n) + exp( wi(xi+\u03bdi)\n2\n)\nexp( \u2212wixi\n2\n) + exp( wixi\n2 )\n!!\n(9)\n= (1 \u2212p) ln(2) + p ln\n\u0012\nexp(z\np) + exp(\u2212z\np )\n\u0013\n\u2212ln (exp(z) + exp(\u2212z)) .\n(10)\nWe now consider cases based on whether or not z is 0. When z = 0 (so either wi or xi is 0) then (10) is also\n0.\nIf z \u0338= 0 then consider the derivative of (10) w.r.t. z, which is\nexp(z/p) \u2212exp(\u2212z/p)\nexp(z/p) + exp(\u2212z/p) \u2212exp(z) \u2212exp(\u2212z)\nexp(z) + exp(\u2212z).\nThis derivative is positive since z > 0 and 0 < p < 1. Therefore (10) is bounded by its limit as z \u2192\u221e,\nwhich is (1 \u2212p) ln(2), in this case.\nSince (9) is 0 when xi = 0 and is bounded by (1 \u2212p) ln(2) otherwise, the expectation over x of (9) is\nbounded PrD(xi \u0338= 0)(1 \u2212p) ln(2), completing the proof.\nSince line (10) is derived using a chain of equalities, the same proof ideas can be used to show that\nTheorem 8 is tight.\nProposition 9 Under the conditions of Theorem 8,\nlim\nwi\u2192\u221eregD,q(0, . . . , 0\n| {z }\ni\u22121\n, wi, 0, . . . , 0\n| {z }\nn\u2212i\n) = PrD(xi \u0338= 0)(1 \u2212p) ln(2).\nNote that this bound on the regularization penalty depends neither on the range nor expectation of xi. In\nparticular, it has a far different character than the approximation of Equation (8).\nIn Theorem 8 the other weights are \ufb01xed at 0 as wi goes to in\ufb01nity. An additional assumption implies that\nthe regularization penalty remains bounded even when the other components are non-zero. Let w be a weight\n8\nvector such that for all x in the support of D and dropout noise vectors \u03bd we have | P\nj\u0338=i wj(xj +\u03bdj)| \u2264M\nfor some bound M (this implies that | P\nj\u0338=i wjxj| \u2264M also). Then\nregD,q(w) = Ex,\u03bd\n  \nexp( w\u00b7(x+\u03bd)\n2\n)+exp(\u2212w\u00b7(x+\u03bd)\n2\n)\nexp( w\u00b7x\n2 )+exp(\u2212w\u00b7x\n2 )\n!!\n\u2264Exi,\u03bdi\n \nlog\n \nexp( M\u2212wi(xi+\u03bdi)\n2\n+exp( M+wi(xi+\u03bdi)\n2\n)\nexp(\u2212M\u2212wixi\n2\n+exp(\u2212M+wixi\n2\n)\n!!\n\u2264M +Exi,\u03bdi\n \nlog\n \nexp(\u2212wixi+\u03bdi\n2\n)+exp( wi(xi+\u03bdi)\n2\n)\nexp( \u2212wixi\n2\n)+exp( wixi\n2 )\n!!\n.\n(11)\nUsing (11) instead of the \ufb01rst line in Theorem 8\u2019s proof gives the following.\nProposition 10 Under the conditions of Theorem 8, if the weight vector w has the property that | P\nj\u0338=i wj(xj+\n\u03bdj)| \u2264M for each x in the support of D and all of its corresponding dropout noise vectors \u03bd then\nsup\n\u03c9 regD,q(w1, w2, . . . , wi\u22121, \u03c9, wi+1, . . . , wn) \u2264M + PrD(xi \u0338= 0)(1 \u2212p) ln(2).\nProposition 10 shows that the regularization penalty starting from a non-zero initial weight vector remains\nbounded as any one of its components goes to in\ufb01nity. On the other hand, unless M is small, the bound will\nbe larger than the dropout criterion for the zero vector. This is a natural consequence as the starting weight\nvector w could already have a large regularization penalty.\nThe derivative of (10) in the proof of Theorem 8 implies that the dropout regularization penalty is\nmonotonic in |wi| when the other weights are zero. Surprisingly, this is does not hold in general. The\ndropout regularization penalty due to a single example (as in Proposition 6) can be written as\nE\u03bd\n\u0010\nln\n\u0010\nexp( w\u00b7(x+\u03bd)\n2\n) + exp( \u2212w\u00b7(x+\u03bd)\n2\n)\n\u0011\u0011\n\u2212ln\n\u0000exp( w\u00b7x\n2 ) + exp( \u2212w\u00b7x\n2\n)\n\u0001\n.\nTherefore if increasing a weight makes the second logarithm increase faster than the expectation of the \ufb01rst,\nthen the regularization penalty decreases even as the weight increases. This happens when the wixi products\ntend to have the same sign. The regularization penalty as a function of w1 for the single example x = (1, 1),\np = 1/2, and w2 set to various values is plotted in Figure 12 . This gives us the following.\nProposition 11 Unlike p-norm regularizers, the dropout regularization penalty regD,q(w) is not always\nmonotonic in the individual weights.\nIn fact, the dropout regularization penalty can decrease as weights move up from 0.\nProposition 12 Fix p = 1/2, w2 > 0, and an arbitrary x \u2208(0, \u221e)2. Let D be the distribution concentrated\non x. Then regD,q(w1, w2) locally decreases as w1 increases from 0.\nProposition 12 is proved in Appendix A.\nWe now turn to the dropout regularization\u2019s behavior when two weights vary together. If any features\nare always zero then their weights can go to \u00b1\u221ewithout affecting either the predictions or regD,q(w). Two\nlinearly dependent features might as well be one feature. After ruling out degeneracies like these, we arrive\nat the following theorem, which is proved in Appendix B.\n2Setting x = (1, 1) is in some sense without loss of generality as the prediction and dropout regularization values for any w, x\npair are identical to the values for \u02dcw, 1 when each \u02dcwi = wixi.\n9\nTheorem 13 Fix an arbitrary distribution D with support in R2, weight vector w \u2208R2, and non-dropout\nprobability p. If there is an x with positive probability under D such that w1x1 and w2x2 are both non-zero\nand have different signs, then the regularization penalty regD,q(\u03c9w) goes to in\ufb01nity as \u03c9 goes to \u00b1\u221e.\nThe theorem can be straightforwardly generalized to the case n > 2; except in degenerate cases, sending\ntwo weights to in\ufb01nity together will lead to a regularization penalty approaching in\ufb01nity.\nTheorem 13 immediately leads to the following corollary.\nCorollary 14 For a distribution D with support in R2, if there is an x with positive probability under D\nsuch that x1 \u0338= 0 and x2 \u0338= 0, then there is a w such that for any q \u2208(0, 1), the regularization penalty\nregD,q(\u03c9w) goes to in\ufb01nity with \u03c9.\nFor any w \u2208R2 with both components nonzero, there is a distribution D over R2 with bounded support\nsuch that the regularization penalty regD,q(\u03c9w) goes to in\ufb01nity with \u03c9.\nTogether Theorems 8 and 13 demonstrate that regD,q(w) is not convex (see also Figure 1). In fact,\nregD,q(w) cannot be approximated to within any factor by a convex function, even if a dependence on n\nand p is allowed. For example, Theorem 8 shows that, for all D with bounded support, both regD,q(0, \u03c9)\nand regD,q(\u03c9, 0) remain bounded as \u03c9 goes to in\ufb01nity, whereas Theorem 13 shows that there is such a D\nsuch that regD,q(\u03c9/2, \u03c9/2) is unbounded as \u03c9 goes to in\ufb01nity.\nTheorem 13 relies on the wixi products having different signs. The following shows that regD,q(w) does\nremain bounded when multiple components of w go to in\ufb01nity if the corresponding features are compatible\nin the sense that the signs of wixi are always in alignment.\nTheorem 15 Let w be a weight vector and D be a discrete distribution such that wixi \u22650 for each index\ni and all x in the support of D. The limit of regD,q(\u03c9w) as \u03c9 goes to in\ufb01nity is bounded by ln(2)(1 \u2212\np)Px\u223cD(w \u00b7 x \u0338= 0).\nThe proof of Theorem 15 (which is Appendix C) easily generalizes to alternative conditions where\n\u03c9 \u2192\u2212\u221eand/or wixi \u22640 for each i \u2264k and x in the support of D.\nTaken together Theorems 15 and 13 give an almost complete characterization of when multiple weights\ncan go to in\ufb01nity while maintaining a \ufb01nite dropout regularization penalty.\nDiscussion\nThe bounds in the preceding theorems and propositions suggest several properties of the dropout regularizer.\nFirst, the 1 \u2212p factors indicate that the strength of regularization grows linearly with dropout probability\nq = 1 \u2212p. Second, the Px\u223cD(xi \u0338= 0) factors in several of the bounds suggest that weights for rare features\nare encouraged by being penalized less strongly than weights for frequent features. This preference for rare\nfeatures is sometimes seen in algorithms like the Second-Order Perceptron [Cesa-Bianchi et al., 2002] and\nAdaGrad [Duchi et al., 2011]. Wager et al. [2013] discussed the relationship between dropout and these\nalgorithms, based on approximation (8). Empirical results indicate that dropout performs well in domains\nlike document classi\ufb01cation where rare features can have high discriminative value [Wang and Manning,\n2013]. The theorems of this section suggest that the exact dropout regularizer minimally penalizes the use\nof rare features. Finally, Theorem 13 suggests that dropout limits co-adaptation by strongly penalizing large\nweights if the wixi products often have different signs. On the other hand, if the wixi products usually have\nthe same sign, then Proposition 12 indicates that dropout encourages increasing the smaller weights to help\nshare the prediction responsibility. This intuition is reinforced by Figure 1, where the dropout penalty for\ntwo large weights is much less then a single large weight when the features are highly correlated.\n10\n4\nA de\ufb01nition of separation\nNow we turn to illustrating the inductive bias of dropout by contrasting it with L2 regularization. For this,\nwe will use a de\ufb01nition of separation between pairs of regularizers.\nEach regularizer has a regularization parameter that governs how strongly it regularizes. If we want to\ndescribe qualitatively what is preferred by one regularizer over another, we need to control for the amount\nof regularization.\nLet erP (w) = Pr(x,y)\u223cP (sign(w \u00b7 x) \u0338= y), and recall that w\u2217and v are the minimizers of the dropout\nand L2-regularized criteria respectively.\nSay that sources P and Q C-separate L2 and dropout if there exist q and \u03bb such that both erP (w\u2217(P,q))\nerP (v(P,\u03bb)) >\nC and\nerQ(v(Q,\u03bb))\nerQ(w\u2217(Q,q)) > C. Say that indexed families P = {P\u03b1} and Q = {Q\u03b1} strongly separate L2 and\ndropout if pairs of distributions in the family C-separate them for arbitrarily large C. We provide strong\nseparations, using both n = 2 and larger n.\n5\nA source preferred by L2\nConsider the joint distribution P5 de\ufb01ned as follows:\nx1\nx2\ny\nPr(x, y)\n10\n\u22121\n1\n1/3\n1.1\n\u22121\n1\n1/3\n\u22121\n1.1\n1\n1/3\n(12)\nThis distribution has weight vectors that classify examples perfectly (the green shaded region in Figure 2).\nFor this distribution, optimizing an L2-regularized criterion leads to a perfect hypothesis, while the weight\nvectors optimizing the dropout criterion make prediction errors on one-third of the distribution.\nThe intuition behind this behavior for the distribution described in (12) is that weight vectors that are pos-\nitive multiples of (1, 1) classify all of the data correctly. However, with dropout regularization the (10, \u22121)\nand (1.1, \u22121) data points encourage the second weight to be negative when the \ufb01rst component is dropped\nout. This negative push on the second weight is strong enough to prevent the minimizer of the dropout-\nregularized criterion from correctly classifying the (\u22121, 1.1) data point. Figure 2 illustrates the loss, dropout\nregularization, and dropout and L2 criterion for this data source.\nWe \ufb01rst show that distribution P5 of (12) is compatible with mild enough L2 regularization. Recall that\nv(P5, \u03bb) is weight vector found by minimizing the L2 regularized criterion (5).\nTheorem 16 If 0 < \u03bb \u22641/50, then erP5(v(P5, \u03bb)) = 0 for the distribution P5 de\ufb01ned in (12).\nIn contrast, the w\u2217(P5, q) minimizing the dropout criterion (3) has error rate at least 1/3.\nTheorem 17 If q \u22651/3 then erP5(w\u2217(P5, q)) \u22651/3 for the distribution P5 de\ufb01ned in (12).\nThe proofs of Theorem 16 and 17 are in Appendices D and E.\n11\nFigure 2: Using data favoring L2 in (12). The expected loss is plotted in the upper-left, the dropout regu-\nlarizer in the upper-right, the L2 regularized criterion as in (5) in the lower-left and the dropout criterion as\nin (3) in the lower-right, all as functions of the weight vector. The Bayes-optimal weight vectors are in the\ngreen region, and \u201c\u00d7\u201d marks show the optimizers of the criteria.\n12\n6\nA source preferred by dropout\nIn this section, consider the joint distribution P6 de\ufb01ned by\nx1\nx2\ny\nPr(x, y)\n1\n0\n1\n3/7\n\u22121/1000\n1\n1\n3/7\n1/10\n\u22121\n1\n1/7\n(13)\nThe intuition behind this distribution is that the (1, 0) data point encourages a large weight on the \ufb01rst\nfeature. This means that the negative pressure on the second weight due to the (1/10, \u22121) data point is\nmuch smaller (especially given its lower probability) than the positive pressure on the second weight due\nto the (\u22121/1000, 1) example. The L2 regularized criterion emphasizes short vectors, and prevents the \ufb01rst\nweight from growing large enough (relative to the second weight) to correctly classify the (1/10, \u22121) data\npoint. On the other hand, the \ufb01rst feature is nearly perfect; it only has the wrong sign on the second example\nwhere it is \u2212\u03f5 = \u22121/1000. This means that, in light of Theorem 8 and Proposition 10, dropout will be\nmuch more willing to use a large weight for x1, giving it an advantage for this source over L2. The plots in\nFigure 3 illustrate this intuition.\nTheorem 18 If 1/100 \u2264\u03bb \u22641, then erP6(v(P6, \u03bb)) \u22651/7 for the distribution P6 de\ufb01ned in (13).\nIn contrast, the minimizer of the dropout criterion is able to generalize perfectly.\nTheorem 19 If q \u22641/2, then erP6(w\u2217(P6, q)) = 0. for the distribution P6 de\ufb01ned in (13).\nTheorems 18 and 19 are proved in Appendices F and G.\nThe results in this and the previous section show that the distributions de\ufb01ned in (12) and (13) strongly\nseparate dropout and L2 regularization. Theorem 19 shows that for distribution P analyzed in this section\nerP (w\u2217(P, q)) = 0 for all q \u22641/2 while Theorem 18 shows that for the same distribution erP (v(P, \u03bb) \u2265\n1/7 whenever \u03bb \u22651/100. In contrast, when Q is the distribution de\ufb01ned in the previous section, Theo-\nrem 16 shows erQ(v(Q, \u03bb)) = 0 whenever \u03bb \u22641/50. For this same distribution Q, Theorem 17 shows that\nerQ(w\u2217(Q, q)) \u22651/3 whenever q \u22651/3.\n7\nL1 regularization\nIn this section, we show that the same P5 and P6 distributions that separate dropout from L2 regularization\nalso separate dropout from L1 regularization: the algorithm the minimizes\nE(x,y)\u223cP (\u2113(yw \u00b7 x)) + \u03bb||w||1.\n(14)\nAs in Sections 5 and 6, we set \u03bb = 1/100. Figure 4 plots the L1 criterion (14) for the distributions P5\nde\ufb01ned in (12) and P6 de\ufb01ned in (13). Like L2 regularization, L1 regularization produces a Bayes-opitmal\nclassi\ufb01er on P5, but not on P6. Therefore the same argument shows that these distributions also strongly\nseparate dropout and L1 regularization.\n13\nFigure 3: For the source from (13) favoring the dropout, the expected loss is plotted in the upper-left, the\ndropout regularizer in the upper-right, the expected loss plus L2 regularization as in (5) in the lower-left and\nthe dropout criterion as in (3) in the lower-right, all as functions of the weight vector. The Bayes-optimal\nweight vectors are in the green region, and \u201c\u00d7\u201d marks show the optimizers of the criteria. Note that the\nminimizer of the dropout criterion lies outside the middle-right plot and is shown on the bottom plot (which\nhas a different range and scale than the others.)\nFigure 4: A plot of the L1 criterion with \u03bb = 0.01 for distributions P5 de\ufb01ned in Section 5 (left) and P6\nde\ufb01ned in Section 6 (right). As before, the Bayes optimal classi\ufb01ers are denoted by the region shaded in\ngreen and the minimizer of the criterion is denoted with an x.\n14\n8\nA high-dimensional source preferred by L2\nIn this section we exhibit a source where L2 regularization leads to a perfect predictor while dropout regu-\nlarization creates a predictor with a constant error rate.\nConsider the source P8 de\ufb01ned as follows. The number n of features is even. All examples are labeled\n1. A random example is drawn as follows: the \ufb01rst feature takes the value 1 with probability 9/10 and \u22121\notherwise, and a subset of exactly n/2 of the remaining n \u22121 features (chosen uniformly at random) takes\nthe value 1, and the remaining n/2 \u22121 of those \ufb01rst n \u22121 features take the value \u22121.\nA majority vote over the last n \u22121 features achieves perfect prediction accuracy. This is despite the \ufb01rst\nfeature (which does not participate in the vote) being more strongly correlated with the label than any of the\nvoters in the optimal ensemble. Dropout, with its bias for single good features and discrimination against\nmultiple disagreeing features, puts too much weight on this \ufb01rst feature. In contrast, L2 regularization leads\nto the Bayes optimal classi\ufb01er by placing less weight on the \ufb01rst feature than on any of the others.\nTheorem 20 If \u03bb \u2264\n1\n30n then the weight vector v(P8, \u03bb) optimizing the L2 criterion has perfect prediction\naccuracy: erP8(v(P8, \u03bb)) = 0.\nWhen n > 125, dropout with q = 1/2 fails to \ufb01nd the Bayes optimal hypothesis. In particular, we have\nthe following theorem.\nTheorem 21 If the dropout probability q = 1/2 and the number of features is an even n > 125 then the\nweight vector w\u2217(P8, q) optimizing the dropout criterion has prediction error rate erP8(w\u2217(P8, q)) \u22651/10.\nWe conjecture that dropout fails on P8 for all n \u22654. As evidence, we analyze the n = 4 case.\nTheorem 22 If dropout probability q = 1/2 and the number of features is n = 4 then the minimizer of the\ndropout criteria w\u2217(P8, q) has has prediction error rate erP8(w\u2217(P8, q)) \u22651/10.\nTheorems 20, 21 and 22 are proved in Appendices H, I and J.\n9\nA high-dimensional source preferred by dropout\nDe\ufb01ne the source P9, which depends on (small) positive real parameters \u03b7, \u03b1, and \u03b2, as follows. A random\nlabel y is generated \ufb01rst, with both of +1 and \u22121 equally likely. The features x1, ..., xn are conditionally\nindependent given y. The \ufb01rst feature tends to be accurate but small: x1 = \u03b1y with probability 1 \u2212\u03b7, and\nis \u2212\u03b1y with probability \u03b7. The remaining features are larger but less accurate: for 2 \u2264i \u2264n, feature xi is\ny with probability 1/2 + \u03b2, and \u2212y otherwise.\nWhen \u03b7 is small enough relative to \u03b2, the Bayes\u2019 optimal prediction is to predict with the \ufb01rst feature.\nWhen \u03b1 is small, this requires concentrating the weight on w1 to outvote the other features. Dropout is\ncapable of making this one weight large while L2 regularization is not.\nTheorem 23 If q = 1/2, n \u2265100, \u03b1 > 0, \u03b2 = 1/(10\u221an \u22121), and \u03b7 \u2264\n1\n2+exp(54\u221an), then erP9(w\u2217(P9, q)) =\n\u03b7.\nTheorem 24 If \u03b2 = 1/(10\u221an \u22121), \u03bb =\n1\n30n, \u03b1 < \u03b2\u03bb, and n is a large enough even number, then for any\n\u03b7 \u2208[0, 1], erP9(v(P9, \u03bb)) \u22653/10.\n15\nTheorems 23 and 24 are proved in Appendices K and L.\nLet \u02dcn be a large enough even number in the sense of Theorem 24. Let P\u03b7 be the distribution de\ufb01ned\nat the start of Section 9 with number of features n = \u02dcn, \u03b2 = 1/(10\u221an \u22121), \u03b1 = 1/(300n\u221an), and\n0 < \u03b7 < 1/(2 + exp(54\u221an)) is a free parameter. Theorem 23 shows that erP\u03b7(w\u2217(P\u03b7, q)) = \u03b7 when\ndropout probability q = 1/2. For this same distribution, Theorem 24 shows erP\u03b7(v(P\u03b7, \u03bb)) \u22653/10 when\n\u03bb = 1/30n. Therefore\nerP\u03b7(w\u2217(P\u03b7, 1/2))\nerP\u03b7(v(P, 1/30\u02dcn))\ngoes to 0 as \u03b7 \u21920.\nThe distribution de\ufb01ned at the start of Section 8, which we call Q here, provides contrasting behavior\nwhen n = \u02dcn. Theorem 21 shows that the error erQ(w\u2217(Q, 1/2)) \u22651/10 while Theorem 20 shows that\nerQ(v(Q, 1/30\u02dcn) = 0. Therefore the P\u03b7 and Q distributions strongly separate dropout and L2 regularization\nfor parameters q = 1/2 and \u03bb = 1/30n.\n10\nConclusions\nWe have built on the interpretation of dropout as a regularizer in Wager et al. [2013] to prove several inter-\nesting properties of the dropout regularizer. This interpretation decomposes the dropout criterion minimized\nby training into a loss term plus a regularization penalty that depends on the feature vectors in the training set\n(but not the labels). We started with a characterization of when the dropout criterion has a unique minimum,\nand then turn to properties of the dropout regularization penalty. We veri\ufb01ed that the dropout regularization\npenalty has some desirable properties of a regularizer: it is 0 at the zero vector, and the contribution of each\nfeature vector in the training set is non-negative.\nOn the other hand, the dropout regularization penalty does not behave like standard regularizers. In\nparticular, we have shown:\n1. Although the dropout \u201closs plus regularization penalty\u201d criterion is convex in the weights w, the\nregularization penalty imposed by dropout training is not convex.\n2. Starting from an arbitrary weight vector, any single weight can go to in\ufb01nity while the dropout regu-\nlarization penalty remains bounded.\n3. In some cases, multiple weights can simultaneously go to in\ufb01nity while the regularization penalty\nremains bounded.\n4. The regularization penalty can decrease as weights increase from 0 when the features are correlated.\nThese are in stark contrast to standard norm-based regularizers that always diverge as any weight goes to\nin\ufb01nity, and are non-decreasing in each individual weight.\nIn most cases the dropout regularization penalty does diverge as multiple weights go to in\ufb01nity. We\ncharacterize when sending two weights to in\ufb01nity causes the dropout regularization penalty to diverge, and\nwhen it will remain \ufb01nite. In particular, dropout is willing to put a large weights on multiple features if the\nwixi products tend to have the same sign.\nThe form of our analytical bounds suggest that the strength of the regularizer grows linearly with the\ndropout probability q, and provide additional support for the claim [Wager et al., 2013] that dropout favors\nrare features.\n16\nWe found it important to check our intuition by working through small examples. To make this more\nrigorous we needed a de\ufb01nition of when a source favored dropout regularization over a more standard reg-\nularizer like L2. Such a de\ufb01nition needs to deal with the strength of regularization, a dif\ufb01culty complicated\nby the fact that dropout regularization is parameterized by the dropout probability q \u2208[0, 1] while L2 regu-\nlarization is parameterized by \u03bb \u2208[0, \u221e]. Our solution is to consider pairs of sources P and Q. We then say\nthe pair separates the dropout and L2 if dropout with a particular parameter q performs better then L2 with\na particular parameter \u03bb on source P, while L2 (with the same \u03bb) performs better than dropout (with the\nsame q) on source Q. Our de\ufb01nition uses generalization error as the most natural interpretation of \u201cperforms\nbetter\u201d.\nSections 5 through 9 are devoted to proving that dropout and L2 are strongly separated by certain pairs\nof distributions. Section 7 shows that dropout and L1 regularization are also strongly separated. Proving\nstrong separation is non-trivial even after one \ufb01nds the right distributions. This is due to several factors:\nthe minimizers of the criteria do not have closed forms, we wish to prove separation for ranges of the\nregularization values, and the binomial distributions induced by dropout are not amenable to exact analysis.\nDespite these dif\ufb01culties, the separation results reinforce the intuition that dropout is more willing to use a\nlarge weight in order to better \ufb01t the training data than L2 regularization. However, if two features often\nhave both the same and different signs (as in Theorem 13) then dropout is less willing to put even moderate\nweight on both features.\nAs a side bene\ufb01t of these analyses, the plots in Figure 2 and Figure 3 provide a dramatic illustration of\nthe dropout regularizer\u2019s non-convexity and its preference for making only a single weight large. This is\nconsistent with the insight provided by Theorems 13 and 15.\nOur analysis is for the logistic regression case corresponding to a single output node. It would be very\ninteresting to have similar analysis for multi-layer neural networks. However, dealing with non-convex loss\nof such networks will be a major challenge. Another open problem suggested by this work is how the\nde\ufb01nition of separation can be used to gain insight about other regularizers and settings.\nReferences\nJ. Abernethy, C. Lee, A. Sinha, and A. Tewari. Online linear optimization via smoothing. COLT, pages\n807\u2013823, 2014.\nP. Bachman, O. Alsharif, and D. Precup. Learning with pseudo-ensembles. NIPS, 2014.\nP. Baldi and P. J. Sadowski. Understanding dropout. In Advances in Neural Information Processing Systems,\npages 2814\u20132822, 2013.\nP. L. Bartlett, M. I. Jordan, and J. D. McAuliffe. Convexity, classi\ufb01cation, and risk bounds. Journal of the\nAmerican Statistical Association, 101(473):138\u2013156, 2006.\nL. Breiman. Some in\ufb01nity theory for predictor ensembles. Annals of Statistics, 32(1):1\u201311, 2004.\nN. Cesa-Bianchi, A. Conconi, and C. Gentile. A second-order perceptron algorithm. COLT, 2002.\nG. Dahl. Deep learning how i did it: Merck 1st place interview, 2012. http://blog.kaggle.com.\nG. E. Dahl, T. N. Sainath, and G. E. Hinton. Improving deep neural networks for lvcsr using recti\ufb01ed linear\nunits and dropout. ICASSP, 2013.\n17\nA. DasGupta. Asymptotic theory of statistics and probability. Springer, 2008.\nJ. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic opti-\nmization. JMLR, 12:2121\u20132159, 2011.\nR. L. Graham, D. E. Knuth, and O. Patashnik. Concrete Mathematics. Addison-Wesley, 1989.\nD. P. Helmbold and P. M. Long. On the necessity of irrelevant variables. JMLR, 13:2145\u20132170, 2012.\nG. E. Hinton. Dropout: a simple and effective way to improve neural networks, 2012. videolectures.net.\nG. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov. Improving neural net-\nworks by preventing co-adaptation of feature detectors, 2012. Arxiv, arXiv:1207.0580v1.\nH. J. Kushner and G. G. Yin. Stochastic approximation algorithms and applications. Springer, 1997.\ne. a. L. Deng. Recent advances in deep learning for speech research at microsoft. ICASSP, 2013.\nP. M. Long and R. A. Servedio. Random classi\ufb01cation noise defeats all convex potential boosters. Machine\nLearning, 78(3):287\u2013304, 2010.\nE. Slud. Distribution inequalities for the binomial law. Annals of Probability, 5:404\u2013412, 1977.\nT. Van Erven, W. Kotowski, and M. K. Warmuth. Follow the leader with dropout perturbations. Annual\nACM Workshop on Computational Learning Theory, pages 949\u2013974, 2014.\nS. Wager, S. I. Wang, and P. Liang. Dropout training as adaptive regularization. NIPS, 2013.\nS. Wager, W. Fithian, S. Wang, and P. S. Liang. Altitude training: Strong bounds for single-layer dropout.\nNIPS, 2014.\nL. Wan, M. Zeiler, S. Zhang, Y. L. Cun, and R. Fergus. Regularization of neural networks using dropconnect.\nIn ICML, pages 1058\u20131066, 2013.\nS. Wang and C. Manning. Fast dropout training. In ICML, pages 118\u2013126, 2013.\nT. Zhang. Statistical behavior and consistency of classi\ufb01cation methods based on convex risk minimization.\nAnnals of Statistics, 32(1):56\u201385, 2004.\nA\nProof of Proposition 12\nProposition 12. Fix p = 1/2, w2 > 0, and an arbitrary x \u2208(0, \u221e)2. Let D be the distribution\nconcentrated on x. Then regD,q(w1, w2) locally decreases as w1 increases from 0.\nFirst, we show that assuming x = (2, 2) is without loss of generality. When D concentrates all of its\nprobability on a single x, let us denote regD,1/2 by regx,1/2. Since anyplace w1 appears in the expression\nfor regx,1/2, it is multiplied by x1, if we multiply w1 by some constant c and divide x1 by c, we do not\nchange w1x1, and therefore do not change regx,1/2. The same holds for w2. Thus\nregx,1/2(w) = reg(2,2),1/2(w1x1/2, w2x2/2).\n18\nIf we change variables and let \u02dcw1 = w1x1/2 and \u02dcw2 = w2x2/2, then since x1 and x2 are both positive, \u02dcw2\nis positive iff w2 is, and regx,1/2(w) is increasing with w1 iff reg(2,2),1/2( \u02dcw) is increasing with \u02dcw1.\nWe continue assuming x = (2, 2). It suf\ufb01ces to show \u2202regD,q(w1, w2)/\u2202w1|w1=0 < 0. This derivative\nis\n3ew2 + e\u22123w2 \u22123e\u2212w2 \u2212e3w2\n2(ew2 + e\u2212w2)(e2w2 + e\u22122w2) .\n(15)\nThe sign depends only on the numerator, which is 0 when w2 = 0. The derivative of the numerator with\nrespect to w2 is 3ew2 \u22123e\u22123w2 + 3e\u2212w2 \u22123e3w2, which is negative for w2 > 0, since ez + e\u2212z is an\nincreasing function in z. Thus the numerator in (15) is decreasing in w2. Therefore (15) is negative when\nw2 > 0, and the regularization penalty is (locally) decreasing as w1 increases from 0.\n(Note: Proposition 12 may be generalized with slight modi\ufb01cations to apply whenever x has two nonzero\ncomponents. What is needed is that x1w1 and x2w2 have the same sign. For example, if x1 is negative but\nx2w2 is positive, then moving w1 from 0 in the negative direction decreases regD,q(w).)\nB\nProof of Theorem 13\nTheorem 13. Fix an arbitrary distribution D with support in R2, weight vector w \u2208R2, and\nnon-dropout probability p. If there is an x with positive probability under D such that w1x1 and\nw2x2 are both non-zero and have different signs, then the regularization penalty regD,q(\u03c9w)\ngoes to in\ufb01nity as \u03c9 goes to \u00b1\u221e.\nFix an x satisfying the conditions of the theorem.\nregD,q(\u03c9w) \u2265D(x)E\u03bd\n \nln\n \nexp(\u2212\u03c9w\u00b7(x+\u03bd)\n2\n) + exp( \u03c9w\u00b7(x+\u03bd)\n2\n)\nexp( \u2212\u03c9w\u00b7x\n2\n) + exp( \u03c9w\u00b7x\n2\n)\n!!\n> D(x)E\u03bd\n \nln\n \nexp( |\u03c9w\u00b7(x+\u03bd)|\n2\n)\n2 exp( |\u03c9w\u00b7x|\n2\n)\n!!\n= D(x)E\u03bd\n\u0012\n\u2212ln(2) + |\u03c9w \u00b7 (x + \u03bd)|\n2\n\u2212|\u03c9w \u00b7 x|\n2\n\u0013\n.\n(16)\nWe now examine the expectation over \u03bd of the term that depends on \u03bd. We assume that |w1x1| \u2265|w2x2| so\n|w \u00b7 x| = |w1x1| \u2212|w2x2|; the other case is symmetrical.\nE\u03bd(|\u03c9w \u00b7 (x + \u03bd)|) = |\u03c9|\n\u0000p2|w \u00b7 x/p| + p(1 \u2212p)|w1x1/p| + p(1 \u2212p)|w2x2/p|\n\u0001\n= |\u03c9|\n\u0000p|w \u00b7 x| + (1 \u2212p)(|w1x1| \u2212|w2x2| + |w2x2|) + (1 \u2212p)|w2x2|\n\u0001\n= |\u03c9|(|w \u00b7 x| + 2(1 \u2212p)|w2x2|).\nPlugging this into (16) gives:\nregD,q(\u03c9w) > D(x) (\u2212ln 2 + (1 \u2212p)|\u03c9||w2x2|)\nwhich goes to in\ufb01nity as \u03c9 goes to \u00b1\u221e.\n19\nC\nProof of Theorem 15\nTheorem 15. Let w be a weight vector and D be a discrete distribution such that wixi \u22650\nfor each index i and all x in the support of D. The limit of regD,q(\u03c9w) as \u03c9 goes to in\ufb01nity is\nbounded by ln(2)(1 \u2212p)Px\u223cD(w \u00b7 x \u0338= 0).\nFirst note that If w and D are such that w \u00b7 x = 0 for all x in the support of D, then regD,q(w) =\nregD,q(\u03c9w) = 0. We now analyze the general case.\nregD,q(\u03c9w) = Ex,\u03bd\n \nln\n \nexp( \u03c9w\u00b7(x+\u03bd)\n2\n) + exp( \u2212\u03c9w\u00b7(x+\u03bd)\n2\n)\nexp( \u03c9w\u00b7x\n2\n) + exp( \u2212\u03c9w\u00b7x\n2\n)\n!!\n= Ex,\u03bd\n \nln\n \nexp( \u03c9w\u00b7(x+\u03bd)\n2\n)(1 + exp(\u2212\u03c9w \u00b7 (x + \u03bd)))\nexp( \u03c9w\u00b7x\n2\n)(1 + exp(\u2212\u03c9w \u00b7 x))\n!!\n= Ex,\u03bd\n\u0010\n(\u03c9w \u00b7 (x + \u03bd)/2) + ln (1 + exp (\u2212\u03c9w \u00b7 (x + \u03bd)))\n\u2212(\u03c9w \u00b7 x/2) \u2212ln (1 + exp (\u2212\u03c9wx))\n\u0011\n.\n(17)\nOf the four terms inside the expectation in Equation (17), the \ufb01rst and third cancel since the expectation\nof \u03bd is 0. Therefore:\nregD,q(\u03c9w) = Ex\n\u0000E\u03bd\n\u0000ln(1 + exp(\u2212\u03c9w \u00b7 (x + \u03bd))) \u2212ln(1 + exp(\u2212\u03c9wx))\n\u0001\u0001\n.\n(18)\nDe\ufb01ne nez(w, x) to be the number of indices i where wixi \u0338= 0. We now consider cases based on\nnez(w, x).\nWhenever nez(w, x) = 0 then both w \u00b7 x = 0 and w \u00b7 (x + \u03bd) = 0. Therefore the contribution of these\nx to the expectation in (18) is ln(2) \u2212ln(2) = 0.\nIf nez(w, x) > 0 then w \u00b7 x > 0 (since each wixi \u22650), and the second term of (18) goes to zero as\n\u03c9 goes to in\ufb01nity. The \ufb01rst term of (18) also goes to zero, unless all of the nez(w, x) components where\nwixi > 0 are dropped out. If they are all dropped out, then the \ufb01rst term becomes ln(2). The probability that\nall nez(w, x) non-zero components are simultaneously dropped out is (1 \u2212p)nez(w,x). With this reasoning\nwe get from (18) that:\nlim\n\u03c9\u2192\u221eregD,q(\u03c9w)\n=\nn\nX\nk=1\nPx\u223cD(nez(w, x) = k)\n\u0010\nln(2)(1 \u2212p)k\u0011\n(19)\n\u2264\nn\nX\nk=1\nPx\u223cD(nez(w, x) = k) (ln(2)(1 \u2212p))\n= ln(2)(1 \u2212p)P(w \u00b7 x \u0338= 0)\nas desired.\n(Note that Equation 19 gives a precise, but more complex expression for the limit.)\n20\nD\nProof of Theorem 16\nTheorem 16. If 0 < \u03bb \u22641/50, then erP5(v(P5, \u03bb)) = 0 for the distribution P5 de\ufb01ned in (12).\nTo keep the notation clean let us abbreviate P5 as just P throughout this proof.\nBy scaling the L2 criterion we can obtain cancellation in the expectation. Let v be weight vector found\nby minimizing the following L2 regularized criterion J:\nJ(w) = 3\n\u0000E(x,y)\u223cP (\u2113(y(w \u00b7 x))) + (\u03bb/2)||w||2\u0001\n.\n(20)\nNote the factor of 3 is to simplify the expressions and doesn\u2019t affect the minimizing v.\nWe will prove Theorem 16 with a series of lemmas.\nBut \ufb01rst, let\u2019s take some partial derivatives:\n\u2202J\n\u2202w1\n=\n\u221210\n1 + exp(10w1 \u2212w2) +\n\u22121.1\n1 + exp(1.1w1 \u2212w2) +\n1\n1 + exp(\u2212w1 + 1.1w2) + 3\u03bbw1\n(21)\n\u2202J\n\u2202w2\n=\n1\n1 + exp(10w1 \u2212w2) +\n1\n1 + exp(1.1w1 \u2212w2) +\n\u22121.1\n1 + exp(\u2212w1 + 1.1w2) + 3\u03bbw2.\n(22)\nWe will repeatedly use the following basic, well-known, lemma.\nLemma 25 For any convex, differentiable function \u03c8 de\ufb01ned on Rn with a unique minimum w\u2217, for any\nw \u2208Rn, if g(w) is the gradient of \u03c8 at w then w\u2217is contained in the closed halfspace whose separating\nhyperplane goes through w, and whose normal vector is \u2212g(w); i.e., w\u2217\u00b7 g(w) \u2264w \u00b7 g(w). Furthermore,\nif g(w) \u0338= 0 then w\u2217\u00b7 g(w) < w \u00b7 g(w).\nNow we\u2019re ready to start our analysis of P.\nLemma 26 If 0 \u2264\u03bb, the optimizing v1 is positive.\nProof: By Lemma 25, it suf\ufb01ces to show that there is a point (0, a2) where both\n\u2202J\n\u2202w1\n\f\f\n(0,a2) < 0 and\n\u2202J\n\u2202w2\n\f\f\n(0,a2) = 0.\nFrom Equation (21):\n\u2202J\n\u2202w1\n\f\f\f\f\f\n(0,a2)\n=\n\u221211.1\n1 + exp(\u2212a2) +\n1\n1 + exp(1.1a2)\nand each term is decreasing as a2 increases. Since it is negative when a2 = \u22122, we have \u2202J\n\u2202w1\n\f\f\f\n(0,a2) < 0 for\nall a2 > \u22122. So, to prove the lemma, if suf\ufb01ces to show that there is a a2 \u2208(\u22122, \u221e) such that the other\nderivative \u2202J\n\u2202w2\n\f\f\f\n(0,a2) = 0.\nFrom equation (22):\n\u2202J\n\u2202w2\n\f\f\f\n(0,a2) =\n2\n1 + exp(\u2212a2) +\n\u22121.1\n1 + exp(1.1a2) + 3\u03bba2\nand each term is continuously increasing in a2. When a2 = \u22122,\n\u2202J\n\u2202w2\n\f\f\f\n(0,a2) is negative. On the other hand,\n\u2202J\n\u2202w2\n\f\f\n(0,0) is positive. Therefore for some a2 \u2208(\u22122, 0) we have \u2202J\n\u2202w2\n\f\f\n(0,a2) = 0 as desired.\n21\nLemma 27 There is a real a > 0 such that\n\u2202J(w)\n\u2202w1\n\f\f\f\f\f\n(a,a)\n+ \u2202J(w)\n\u2202w2\n\f\f\f\f\f\n(a,a)\n= 0.\nProof: Applying (21) and (22), we get\nb def\n= \u2202J(w)\n\u2202w1\n\f\f\f\f\f\n(a,a)\n+ \u2202J(w)\n\u2202w2\n\f\f\f\f\f\n(a,a)\n=\n\u22129\n1 + exp(9a) +\n\u22120.2\n1 + exp(a/10) + 6\u03bba.\nSince b is negative when a = 0 and is a continuous function of a, and lima\u2192\u221eb > \u221e, the lemma holds.\nLemma 28 v1 \u2265v2.\nProof: Let a be the value from Lemma 27, and let g = (g1, g2) be the gradient of J at (a, a). Lemma 25\nimplies that v lies in the halfspace through (a, a) in the direction of \u2212g. Lemma 27 implies that\ng1 = \u2202J(w)\n\u2202w1\n\f\f\f\f\f\n(a,a)\n= \u2212\u2202J(w)\n\u2202w2\n\f\f\f\f\f\n(a,a)\n= \u2212g2.\nExamination of the derivatives (21) and (22) at (a, a) shows that the \ufb01rst term of (21) is negative and the \ufb01rst\nterm of (22) is positive while the last three terms match (although in a different order). Therefore g1 < 0\nand g2 = \u2212g1 is positive. Applying Lemma 25 completes the proof.\nLemma 28 implies that v correctly classi\ufb01es (10, \u22121) and (11/10, \u22121). It remains to show that v\ncorrectly classi\ufb01es (\u22121, 11/10), that is, that v1 is not too much bigger than v2.\nLemma 29 If v2 \u22650.6 and \u03bb > 0 then v1 < 11v2/10.\nProof: Combining \u2202J\n\u2202w1\n\f\f\f\nv = 0 with (21), we get\n3\u03bbv1 =\n10\n1 + exp(10v1 \u2212v2) +\n1.1\n1 + exp(1.1v1 \u2212v2) +\n\u22121\n1 + exp(\u2212v1 + 1.1v2)\nand, similarly,\n3\u03bbv2 =\n\u22121\n1 + exp(10v1 \u2212v2) +\n\u22121\n1 + exp(1.1v1 \u2212v2) +\n1.1\n1 + exp(\u2212v1 + 1.1v2).\nThus\n3\u03bb(10v1 \u221211v2) =\n111\n1 + exp(10v1 \u2212v2) +\n22\n1 + exp(1.1v1 \u2212v2) \u2212\n22.1\n1 + exp(\u2212v1 + 1.1v2).\n(23)\nAssume for contraction that v1 \u226511v2/10. Then 10v1\u2212v2 \u226510v2, 1.1v1\u2212v2 \u22650.21v2, and \u2212v1+1.1v2 \u2264\n0, so\n3\u03bb(10v1 \u221211v2) \u2264\n111\n1 + exp(10v2) +\n22\n1 + exp(0.21v2) \u221211.05.\nHowever, 10v1 \u221211v2 \u22650 and (since v2 \u22650.6) the RHS is negative, giving the desired contradiction.\n22\nLemma 30 If 0 < \u03bb \u22641/50 then v2 \u22650.6.\nProof: It suf\ufb01ces to show that there is a point (x, 0.6) where the partial w.r.t. w1 is 0 and the partial w.r.t w2\nis negative.\n\u2202J\n\u2202w1\n\f\f\f\n(x,0.6) =\n\u221210\n1 + exp(10x \u22120.6) +\n\u22121.1\n1 + exp(1.1x \u22120.6) +\n1\n1 + exp(\u2212x + 0.66) + 3\u03bbx\nand is increasing in x and \u03bb (assuming x > 0) and becomes positive as x goes to in\ufb01nity. It is negative when\nevaluated at x = 0.6 and \u03bb = 1/50, so for all \u03bb \u22641/50 there is an x > 1 such that \u2202J/\u2202w+\n\f\f\n(x,1) = 0.\n\u2202J\n\u2202w2\n\f\f\f\n(x,0.6) =\n1\n1 + exp(10x \u22120.6) +\n1\n1 + exp(1.1x \u22120.6) +\n\u22121.1\n1 + exp(\u2212x + 0.66) + 1.8\u03bb\nand is decreasing in x and increasing in \u03bb. It is negative when x = 1 and \u03bb = 1/50, so it will remain\nnegative for all x > 1 and 0 \u2264\u03bb \u22641/50, as desired.\nSo, we have shown that, if \u03bb \u22641/50, then all examples are classi\ufb01ed correctly by v, which proves\nTheorem 16.\nE\nProof of Theorem 17\nTheorem 17. If q \u22651/3 then erP5(w\u2217(P5, q)) \u22651/3 for the distribution P5 de\ufb01ned in (12).\nThroughout this proof we also abbreviate P5 as just P.\nFor this subsection, let us de\ufb01ne the scaled dropout criterion\nJ(w) = 3 E(x,y)\u223cP,r(\u2113(y(w \u00b7 (r \u2299x))))\n(24)\nwhere the components of r are independent samples from a Bernoulli distribution with parameter p =\n1 \u2212q > 0. Again, the factor of 3 is to simplify the expectation and doesn\u2019t change the minimizing w.\nLet w\u229bbe the minimizer of this J(w), so that Equation (4) implies that the optimizer w\u2217of the dropout\ncriterion is pw\u229b. Note that w\u2217classi\ufb01es an example correctly if and only if w\u229bdoes.\nNext, note that we may assume without loss of generality that both components of w\u229bare positive,\nsince, if either is negative, one of (\u22121, 1.1) or (1.1, \u22121) is misclassi\ufb01ed and we are done.\nWe will prove Theorem 17 by proving that, when q \u22651/3, w\u229bmisclassi\ufb01es (\u22121, 1.1), or, equivalently,\nthat w\u229b\n1 > (11/10)w\u229b\n2 .\nFirst, let us evaluate some partial derivatives. (Note that, if xi is dropped out, the value of wi does not\nmatter.)\n\u2202J\n\u2202w1\n= (1 \u2212q)2\n\u0012\n\u221210\n1 + exp(10w1 \u2212w2) +\n\u22121.1\n1 + exp(1.1w1 \u2212w2) +\n1\n1 + exp(\u2212w1 + 1.1w2)\n\u0013\n(25)\n+ (1 \u2212q)q\n\u0012\n\u221210\n1 + exp(10w1) +\n\u22121.1\n1 + exp(1.1w1) +\n1\n1 + exp(\u2212w1)\n\u0013\n\u2202J\n\u2202w2\n= (1 \u2212q)2\n\u0012\n1\n1 + exp(10w1 \u2212w2) +\n1\n1 + exp(1.1w1 \u2212w2) +\n\u22121.1\n1 + exp(\u2212w1 + 1.1w2)\n\u0013\n(26)\n+ q(1 \u2212q)\n\u0012\n1\n1 + exp(\u2212w2) +\n1\n1 + exp(\u2212w2) +\n\u22121.1\n1 + exp(1.1w2)\n\u0013\n.\n23\nThe following is the key lemma. As before, it is useful since, for any w, if g(w) is nonzero, then w\u229b\nlies in the open halfspace through w whose normal vector is the negative gradient.\nLemma 31 For all a > 0 and q \u22651/3,\n\u2202J\n\u2202w2\n\f\f\f\f\f\n(a,10a/11)\n> 0.\n(27)\nProof: We have\n\u2202J\n\u2202w2\n\f\f\f\f\f\n(a,10a/11)\n=(1 \u2212q)2\n\u0012\n1\n1 + exp(100a/11) +\n1\n1 + exp(21a/110) + \u22121.1\n2\n\u0013\n+ q(1 \u2212q)\n\u0012\n2\n1 + exp(\u221210a/11) +\n\u22121.1\n1 + exp(a)\n\u0013\n.\nNote that this derivative is positive if and only if\nf(q, a) =\n\u0012\n1\n1 \u2212q\n\u0013\n\u2202J\n\u2202w2\n\f\f\f\f\f\n(a,10a/11)\n=q\n\u001211\n20 +\n2\n1 + exp(\u221210a/11) +\n\u22121\n1 + exp(21a/110) +\n\u22121\n1 + exp(100a/11) +\n\u221211/10\n1 + exp(a)\n\u0013\n+\n1\n1 + exp(21a/110) +\n1\n1 + exp(100a/11) + \u221211\n20\nis positive, as 0 < q < 1. Note that the terms multiplying q are increasing in a and sum to 0 when a = 0. On\nthe other hand, the terms not multiplied by q are decreasing in a and turn negative when a is just over 1/4.\nThus both parts are positive when a \u22641/4. Note that f(q, a) can be underestimated by underestimating a\non the q-terms and overestimating a on the other terms.\nFor 1/4 \u2264a \u22642,\nf(q, a) \u2265q\n\u001211\n20 +\n2\n1 + exp(\u221210/44) +\n\u22121\n1 + exp(21/440) +\n\u22121\n1 + exp(100/44) +\n\u221211/10\n1 + exp(1/4)\n\u0013\n+\n1\n1 + exp(42/110) +\n1\n1 + exp(200/11) + \u221211\n20\n\u22650.5q \u22120.15\nand is positive whenever q \u22651/3.\nFor a \u22652,\nf(q, a) \u2265q\n\u001211\n20 +\n2\n1 + exp(\u221220/11) +\n\u22121\n1 + exp(42/110) +\n\u22121\n1 + exp(200/11) +\n\u221211/10\n1 + exp(2)\n\u0013\n+ \u221211\n20\n\u22651.7q \u221211/20\nand is also positive whenever q \u22651/3.\nProof of Theorem 17: Let g = (g1, g2) be the gradient of J at (w\u229b\n1 , 10w\u229b\n1 /11). Lemma 31 shows g is\nnot 0, so by convexity\nw\u229b\u00b7 g < (w\u229b\n1 , 10w\u229b\n1 /11) \u00b7 g\n24\nwhich implies\nw\u229b\n2 g2 < (10w\u229b\n1 /11) g2.\nSince g2 > 0 (Lemma 31), this implies\nw\u229b\n2 < (10w\u229b\n1 /11)\nand the (\u22121, 11/10) example is misclassi\ufb01ed by w\u229b, and therefore by w\u2217, completing the proof.\nF\nProof of Theorem 18\nTheorem 18. If 1/100 \u2264\u03bb \u22641, then erP6(v(P6, \u03bb)) \u22651/7 for the distribution P6 de\ufb01ned\nin (13).\nTo keep the notation clean, in this section let us abbreviate P6 simply as P.\nAs the reader might expect, we will prove Theorem 18 by proving that v fails to correctly classify\n(1/10, \u22121), that is, by proving that v1 < 10v2.\nWe may assume that v1 > 0, since, otherwise, (1, 0) is misclassi\ufb01ed.\nTo obtain cancellation in the expectation, we work with the scaled L2 criterion\nJ(w) = 7E(x,y)\u223cP (\u2113(y(w \u00b7 x))) + (7\u03bb/2)||w||2.\n(28)\nand let v(P, \u03bb) be the vector minimizing this J, which we often abbreviate as simply v, leaving it implicitly\na function of \u03bb. Note that this scaling of the criteria does not change the minimizing v.\nTaking derivatives,\n\u2202J\n\u2202w1\n=\n\u22123\n1 + exp(w1) +\n3\u03f5\n1 + exp(\u2212\u03f5w1 + w2) +\n\u22120.1\n1 + exp(w1/10 \u2212w2) + 7\u03bbw1\n(29)\n\u2202J\n\u2202w2\n=\n\u22123\n1 + exp(\u2212\u03f5w1 + w2) +\n1\n1 + exp(w1/10 \u2212w2) + 7\u03bbw2.\n(30)\nLemma 32 If either: \u03bb \u22651/100 and a \u22651/3, or \u03bb \u22651/4 and a \u22651/15 then\n\u2202J(w)\n\u2202w1\n\f\f\f\n(10a,a) > 0.\nProof: We have\n\u2202J(w)\n\u2202w1\n\f\f\f\f\f\n(10a,a)\n=\n\u22123\n(1 + exp(10a)) +\n3\u03f5\n1 + exp((1 \u221210\u03f5)a) + \u22121\n20 +70\u03bba >\n\u22123\n(1 + exp(10a)) + \u22121\n20 +70\u03bba.\nEach term of the RHS is non-decreasing in a and \u03bb, and the RHS is positive when either \u03bb = 1/100 and\na = 1/3 or \u03bb = 1/4 and a = 1/15.\nTo apply this, we want to show that v2 is large enough, which we do next.\nLemma 33 If \u03bb \u22641/4 then v2 \u22651/3 and if \u03bb \u22641 then v2 \u22651/15.\n25\nTable 2: Seven times the dropout distribution. The three probability sub-columns correspond to the original\nexamples (1,0), (-1/1000, 1), (1/10, -1), and the \ufb01nal column is the over-estimate used in Lemma 36.\nx1r1\nx2r2\ny\nseven times probability\nw\u229b\u00b7 (r \u2299x) over-estimate\n0\n0\n1\n3q\n+3q2\n+q2\n0\n1\n0\n1\n3(1 \u2212q)\n\u221e\n0\n1\n1\n3q(1 \u2212q)\nw2\n\u22121/1000\n0\n1\n3q(1 \u2212q)\n0\n\u22121/1000\n1\n1\n3(1 \u2212q)2\nw2\n0\n\u22121\n1\nq(1 \u2212q)\n\u221e\n1/10\n0\n1\nq(1 \u2212q)\n\u221e\n1/10\n\u22121\n1\n(1 \u2212q)2\n\u221e\nProof: Assume to the contrary that \u03bb \u22641/4 but v2 < 1/3. From (30), and using that v1 > 0, we have\n\u2202J\n\u2202w2\n\f\f\f\f\f\nv\n<\n\u22123\n1 + exp(v2) +\n1\n1 + exp(\u2212v2) + 7\u03bbv2,\n(31)\na bound that is increasing in v2 and \u03bb. Since\n\u2202J\n\u2202w2\n\f\f\f\nv = 0, the bound must be positive. However, when\nv2 \u22641/3 and \u03bb \u22641/4, it is negative, giving the desired contradiction.\nSince the bound (31) is also negative at v2 = 1/15 and \u03bb = 1, a similar contradiction proves the other\nhalf of the lemma.\nProof: (of Theorem 18): Lemmas 32 and 33 imply that (10v2, v2) is not the minimizing v (when\n\u03bb \u22651/100), so by convexity,\nJ(10v2, v1) +\n\u0000(v1, v2) \u2212(10v2, v2)\n\u0001\n\u00b7 \u2207J(10v2, v2) < J(v1, v2)\n(32)\n(v1 \u221210v2) \u2202J\n\u2202w2\n\f\f\f\f\f\n(10v2,v2)\n< 0.\n(33)\nIf 1/100 \u2264\u03bb \u22641/4 then Lemma 33 shows that v2 \u22651/3 and if 1/4 \u2264\u03bb \u22641 then it shows that\nv2 \u22651/15. In either case, Lemma 32 shows that that \u2202J\n\u2202w2\n\f\f\f\n(10v2,v2) > 0. Therefore,\nv1 < 10v2\nand (0.1, \u22121) is misclassi\ufb01ed by v, completing the proof.\nG\nProof of Theorem 19\nTheorem 19. If q \u22641/2, then erP6(w\u2217(P6, q)) = 0. for the distribution P6 de\ufb01ned in (13).\nIn this proof, let us abbreviate P6 with just P, and use \u03f5 to denote 1/1000.\nFor this section, let us de\ufb01ne the scaled dropout criterion\nJ(w) = 7E(x,y)\u223cP,r(\u2113(y(w \u00b7 (r \u2299x)))),\n(34)\n26\nwhere, as earlier, the components of r are independent samples from a Bernoulli distribution with parameter\np = 1\u2212q = 1/2 > 0. (Note that, similarly to before, scaling up the objective function by 7 does not change\nthe minimizer of J.) See Table 2 for a tabular representation of the distribution after dropout. Let w\u229bbe the\nminimizer of J, so that w\u2217= pw\u229b(see Equation (4)).\nFirst, let us evaluate some partial derivatives (note that 1 \u2212q = (1 \u2212q)2 + q(1 \u2212q)).\n\u2202J\n\u2202w1\n= (1 \u2212q)2\n\u0012\n\u22123\n1 + exp(w1) +\n3\u03f5\n1 + exp(\u2212\u03f5w1 + w2) +\n\u22120.1\n1 + exp(0.1w1 \u2212w2)\n\u0013\n(35)\n+ (1 \u2212q)q\n\u0012\n\u22123\n1 + exp(w1) +\n3\u03f5\n1 + exp(\u2212\u03f5w1) +\n\u22120.1\n1 + exp(0.1w1)\n\u0013\n\u2202J\n\u2202w2\n= (1 \u2212q)2\n\u0012\n\u22123\n1 + exp(\u2212\u03f5w1 + w2) +\n1\n1 + exp(0.1w1 \u2212w2)\n\u0013\n(36)\n+ q(1 \u2212q)\n\u0012\n\u22123\n1 + exp(w2) +\n1\n1 + exp(\u2212w2)\n\u0013\n.\nLet\u2019s get started by showing that w\u229bcorrectly classi\ufb01es (1, 0).\nLemma 34 w\u229b\n1 > 0.\nProof: As before, it suf\ufb01ces to show that there is a point (0, a2) where both \u2202J\n\u2202w1\n\f\f\n(0,a2) < 0 and \u2202J\n\u2202w2\n\f\f\n(0,a2) =\n0.\nFrom Equation (35):\n\u2202J\n\u2202w1\n\f\f\f\n(0,a2) = (1 \u2212q)2\n\u0012\u22123\n2 +\n3\u03f5\n1 + exp(a2) +\n\u22120.1\n1 + exp(\u2212a2)\n\u0013\n+ (1 \u2212q)q\n2\n(\u22123.1 + 3\u03f5)\nwhich is decreasing in a2, and negative even as a2 approaches \u2212\u221e(recalling \u03f5 = 1/1000), so\n\u2202J\n\u2202w1\n\f\f\f\n(0,a2) is\nalways negative.\nEquation (36) implies\n\u2202J\n\u2202w2\n\f\f\f\n(0,a2) = (1 \u2212q)2\n\u0012\n\u22123\n1 + exp(a2) +\n1\n1 + exp(\u2212a2)\n\u0013\n+ q(1 \u2212q)\n\u0012\n\u22123\n1 + exp(a2) +\n1\n1 + exp(\u2212a2)\n\u0013\n.\nThis is negative when a2 = 0, approaches 1 \u2212q as a2 goes to in\ufb01nity, and is continuous, so there is a a2\nsuch that \u2202J\n\u2202w2\n\f\f\f\n(0,a2) = 0. Since \u2202J\n\u2202w1\n\f\f\f\n(0,a2) < 0, this proves the lemma.\nNext, we\u2019ll start to work on showing that w\u229bcorrectly classi\ufb01es (\u2212\u03f5, 1).\nLemma 35 For all a > 1/10,\n\u2202J\n\u2202w1\n\f\f\f\f\f\n(a/\u03f5,a)\n> 0.\nProof: From (35), we have\n\u2202J\n\u2202w1\n\f\f\f\n(a/\u03f5,a) =(1 \u2212q)2\n\u0012\n\u22123\n1 + exp(a/\u03f5) +\n3\u03f5\n1 + exp(0) +\n\u22120.1\n1 + exp(0.1(a/\u03f5) \u2212a)\n\u0013\n+ q(1 \u2212q)\n\u0012\n\u22123\n1 + exp(a/\u03f5) +\n3\u03f5\n1 + exp(\u2212a) +\n\u22120.1\n1 + exp(a/10\u03f5)\n\u0013\nwhich is positive if a > 1/10 as the positive terms (even with the \u03f5 factors) dominate the negative ones.\n27\nLemma 36\nw\u229b\n2 > 1/4.\nProof: Assuming w1 \u22650, the estimates in Table 2 along with the facts that \u2113(z) is positive and decreasing\nshow :\nJ(w) \u22653(1 \u2212q) ln(1 + exp(\u2212w2)) + 6q ln(2) + q2 ln(2)\n(37)\nwhich is decreasing in w2. If w\u229b\n2 \u22641/4, then bound (37) and the fact that w\u229b\n1 > 0 (Lemma 34) imply that\nJ(w\u229b) \u22650.69q2 + 2.4q + 1.7.\nOn the other hand,\nJ(100, 2) \u2264\u22121.5q2 + 6q + 0.42,\nand the upper bound on J(100, 2) is less than the lower bound on J(w\u229b) when 0 \u2264q \u22641/2, giving the\ndesired contradiction.\nNow, we\u2019re ready to show that w\u229bcorrectly classi\ufb01es (\u2212\u03f5, 1).\nLemma 37 \u03f5w\u229b\n1 < w\u229b\n2 .\nProof: Let g be the gradient of J evaluated at (w\u229b\n2 /\u03f5, w\u229b\n2 ). Combining Lemmas 35 and 36, g \u0338= (0, 0), so\nw\u229b\u00b7 g < (w\u229b\n2 /\u03f5, w\u229b\n2 ) \u00b7 g.\nThis implies\nw\u229b\n1\n\u2202J\n\u2202w1\n\f\f\f\n(w\u229b\n2 /\u03f5,w\u229b\n2 ) < w\u229b\n2\n\u03f5\n\u2202J\n\u2202w1\n\f\f\f\n(w\u229b\n2 /\u03f5,w\u229b\n2 ).\nSince Lemmas 35 and 36 imply that g(w\u229b\n2 /\u03f5, w\u229b\n2 )1 > 0, this completes the proof.\nFinally, we are ready to work on showing that (1/10, \u22121) is correctly classi\ufb01ed by w\u229b, i.e. that w\u229b\n1 >\n10w\u229b\n2 .\nLemma 38 For all a \u2208R,\n\u2202J\n\u2202w1\n\f\f\f\n(10a,a) < 0.\nProof: Choose a \u2208R. From (35), we have\n\u2202J\n\u2202w1\n\f\f\f\n(10a,a) = q(1 \u2212q)\n\u0012\n\u22123\n1 + exp(10a) +\n3\u03f5\n1 + exp(\u221210\u03f5a) +\n\u22121\n10(1 + exp(a))\n\u0013\n+ (1 \u2212q)2\n\u0012\n\u22123\n1 + exp(10a) +\n3\u03f5\n1 + exp(a \u221210\u03f5a) + \u22121\n20\n\u0013\n\u2264(1 \u2212q)2\n\u0012\n6\u03f5 + \u22121\n20\n\u0013\n< 0\nusing q \u22641/2 and \u03f5 = 1/1000.\nLemma 39 w\u229b\n1 > 10w\u229b\n2 .\n28\nProof: Let g be the gradient of J evaluated at u = (10w\u229b\n2 , w\u229b\n2 ). Lemma 38 implies that g \u0338= (0, 0), i.e. that\nw\u229b\n1 \u0338= 10w\u229b\n2 . Therefore,\nw\u229b\u00b7 g < u \u00b7 g\nwhich, since u2 = w\u229b\n2 , implies\nw\u229b\n1\n\u2202J\n\u2202w1\n\f\f\f\nu < 10w\u229b\n2\n\u2202J\n\u2202w1\n\f\f\f\nu.\nSince Lemma 38 implies that \u2202J/\u2202w1\n\f\f\f\nu < 0, this in turn implies\nw\u229b\n1 > 10w\u229b\n2 ,\ncompleting the proof.\nNow we have all the pieces to prove that dropout succeeds on P.\nProof (of Theorem 19): Lemma 34 implies that (1, 0) is classi\ufb01ed correctly by w\u229b, and therefore by\nw\u2217= pw\u229b. Lemma 37 implies that (\u2212\u03f5, 1) is classi\ufb01ed correctly. Lemma 39 implies that (1/10, \u22121) is\nclassi\ufb01ed correctly, completing the proof.\nH\nProof of Theorem 20\nTheorem 20. If \u03bb \u2264\n1\n30n then the weight vector v(P8, \u03bb) optimizing the L2 criterion has perfect\nprediction accuracy: erP8(v(P8, \u03bb)) = 0.\nIn this proof, let us abbreviate P8 as just P.\nBy symmetry and convexity, the optimizing v is of the form (v1, v2, v2, . . . , v2) with the last n \u22121\ncomponents being equal. Thus for this distribution minimizing the L2 criterion is equivalent to minimizing\nthe simpler criterion K(w1, w2) de\ufb01ned by:\nK(w1, w2) = 9\n10 ln (1 + exp(\u2212w1 \u2212w2)) + 1\n10 ln (1 + exp(w1 \u2212w2)) + \u03bb\n2\n\u0000w2\n1 + (n \u22121)w2\n2\n\u0001\n.\nLet (v1, v2) be the minimizing vector of K(), retaining an implicit dependence on n and \u03bb. We will be\nmaking frequent use of the partial derivatives of K:\n\u2202K\n\u2202w1\n=\n\u22129\n10(1 + exp(w1 + w2)) +\n1\n10(1 + exp(\u2212w1 + w2)) + \u03bbw1\n(38)\n\u2202K\n\u2202w2\n=\n\u22129\n10(1 + exp(w1 + w2)) +\n\u22121\n10(1 + exp(\u2212w1 + w2)) + (n \u22121)\u03bbw2.\n(39)\nIt suf\ufb01ces to show that 0 \u2264v1 < v2 so that the \ufb01rst feature does not perturb the majority vote of the\nothers.\nTo see 0 \u2264v1, notice that \u2202K/\u2202w1\n\f\f\n(0,w2) is negative for all w2, including when w2 = v2.\nTo prove v1 < v2 we show the existence of a point (a, a) such that\n\u2202K\n\u2202w1\n\f\f\f\f\f\n(a,a)\n= \u2212\u2202K\n\u2202w2\n\f\f\f\f\f\n(a,a)\n> 0,\n(40)\n29\nso that Lemma 25 implies that the optimizing (v1, v2) lies above the w1 = w2 diagonal.\nw2\nw1\n(a, a)\n\u2212\u2207K\nWe have\n\u2202K\n\u2202w1\n\f\f\f\n(a,a) =\n\u22129\n10(1 + exp(2a)) + 1\n20 + \u03bba\nwhich is increasing in a, negative when a = 0 and goes to in\ufb01nity with a. It turns positive at some a < 1.5\n(exactly where depends on \u03bb).\nOn the other hand,\n\u2202K\n\u2202w2\n\f\f\f\n(a,a) =\n\u22129\n10(1 + exp(2a)) + \u22121\n20 + \u03bb(n \u22121)a\nand is also increasing in a and goes to in\ufb01nity. However, \u2202K/\u2202w2\n\f\f\f\n(a,a) is negative at a = 1.5 whenever\n1.5\u03bb(n \u22121) \u22641/20, which is implied by the premise of the theorem.\nBoth partial derivatives are negative when a = 0, continuously go to in\ufb01nity with a, and \u2202K/\u2202w1\n\f\f\f\n(a,a)\ncrosses zero \ufb01rst. From the point where \u2202K/\u2202w1\n\f\f\f\n(a,a) crosses zero until \u2202K/\u2202w2\n\f\f\f\n(a,a) does, the magnitude\nof \u2202K/\u2202w1\n\f\f\f\n(a,a) is increasing, starting at 0, and the magnitude of \u2202K/\u2202w2\n\f\f\f\n(a,a) is decreasing until it reaches\n0. When they meet, Equation (40) holds, completing the proof.\nI\nProof of Theorem 21\nTheorem 21. If the dropout probability q = 1/2 and the number of features is an even n > 125\nthen the weight vector w\u2217(P8, q) optimizing the dropout criterion has prediction error rate\nerP8(w\u2217(P8, q)) \u22651/10.\nIn this proof, we again abbreviate, using P for P8.\nThe complicated form of the criterion optimized by dropout makes analyzing it dif\ufb01cult. Here we make\nuse of Jensen\u2019s inequality. However, a straightforward application of it is fruitless, and a key step is to apply\nJensen\u2019s inequality on just half the distribution resulting from dropout.\nSimilarly to before, let\nJ(w) = E(x,y)\u223cP,r(\u2113(y(w \u00b7 (r \u2299x)))),\n(41)\nand let w\u229bminimize J, so that w\u2217= pw\u229b.\nAgain using symmetry and convexity, the last n \u22121 components of the optimizing w\u229bare equal, so w\u229b\nis of the form (w\u229b\n1 , w\u229b\n2 , w\u229b\n2 , . . . , w\u229b\n2 ).\nLemma 40 The minimizing w\u229b\n1 of (41) is positive.\n30\nProof: Let g\nP, r be the marginal distribution of the last n \u22121 components after dropout and \u02dcx denote these\nlast n \u22121 components of the dropped-out feature vector. Then, recalling y is always 1 in our distribution\n(and p is the probability that the \ufb01rst feature is not dropped out),\n\u2202J(w)\n\u2202w1\n= E(r2,...,rn)\n\u00129p\n10E\u02dcx\u223cf\nP,r(\u2113\u2032(w \u00b7 (1, \u02dcx))) \u2212p\n10E\u02dcx\u223cf\nP,r(\u2113\u2032(w \u00b7 (\u22121, \u02dcx)))\n\u0013\nwhich is negative whenever w1 = 0, since \u2113\u2032() is negative and the two inner expectations become identical\nwhen w1 = 0. Therefore the optimizing w\u229b\n1 is positive.\nTo show that dropout fails, we want to show that w\u229b\n1 > w\u229b\n2 , i.e. that w\u229b\n1 \u2264w\u229b\n2 leads to a contradiction,\nso we begin to explore the consequences of w\u229b\n1 \u2264w\u229b\n2 .\nLemma 41 If q = 1/2 and w\u229b\n1 \u2264w\u229b\n2 then w\u229b\n2 > 4/9.\nProof: Assume to the contrary that w\u229b\n1 \u2264w\u229b\n2 \u22644/9.\nUsing Jensen\u2019s inequality,\nJ(w\u229b) \u2265\u2113(E(x,y)\u223cP,r(y(w\u229b\u00b7 x)))\nand the inner expectation is 8w\u229b\n1 /20 + w\u229b\n2 /2 \u22649w\u229b\n2 /10 as w\u229b\n1 \u2264w\u229b\n2 . Therefore, since w\u229b\n2 \u22644/9,\nJ(w\u229b) \u2265\u2113(0.4) > 0.51.\nHowever,\nJ(2.1, 0, 0, . . . , 0) = ln(2)\n2\n+ 9 ln(1 + e\u22122.1)\n20\n+ ln(1 + e2.1)\n20\n< 0.51\ncontradicting the optimality of w\u229b.\nLemma 42 If q = 1/2 and w\u229b\n1 \u2264w\u229b\n2 then J(w\u229b) \u2265Ek\u223cB(n,1/2)\u2113(w\u229b\n2 (k \u2212(n/2) + 1)) where B(n, 1/2)\nis the binomial distribution.\nProof: Consider the modi\ufb01ed distribution P1 over (x, y) examples where y is always 1, x2, ..., xn are\nuniformly distributed over the the vectors with n/2 ones and (n/2) \u22121 negative ones (as in P), but x1 is\nalways one. Since 0 < w\u229b\n1 \u2264w\u229b\n2 and the label y = 1 under P and P1,\nJ(w\u229b) = E(x,y)\u223cP,r(\u2113(w\u229b\u00b7 x))\n> E(x,y)\u223cP1,r(\u2113(w\u229b\u00b7 x))\n= E(x,y)\u223cP1,r\n\u0000\u2113\n\u0000w\u229b\n2 (1 \u00b7 (x \u2299r))\n\u0001\u0001\n= E(x,y)\u223cP1,r\n\u0000\u2113\n\u0000w\u229b\n2 (x \u00b7 r)\n\u0001\u0001\n.\nEvery x in the support of P1 has exactly (n/2) + 1 components that are 1, and the remaining (n/2) \u22121\ncomponents are \u22121. Call a component a success if it is either \u22121 and dropped out or 1 and not dropped\nout. Now, x \u00b7 r is exactly 1 \u2212(n/2) plus the number of successes. Furthermore, the number of successes is\ndistributed according to the binomial distribution B(n, 1/2). Therefore\nE(x,y)\u223cP1,r(w\u229b\n2 (x \u00b7 r)) = Ek\u223cB(n,1/2)(\u2113(w\u229b\n2 (k \u2212(n/2) + 1)))\ngiving the desired bound.\n31\nLemma 43 For even n \u22656, Ek\u223cB(n,1/2)(\u2113(w\u229b\n2 (k \u2212(n/2) + 1))) \u22651\n3\u2113\n\u0010\nw\u229b\n2 \u2212w\u229b\n2\n\u221a\n2n\n4\n\u0011\n.\nProof: Let \u03b1 = Pn/2\u22121\ni=0\n\u0000n\ni\n\u0001\n, so \u03b1 is slightly less than 2n\u22121.\nEk\u223cB(n,1/2)(\u2113(w\u229b\n2 (k \u2212(n/2) + 1))) = 1\n2n\nX\nk\n\u0012n\nk\n\u0013\n\u2113(w\u229b\n2 (k + 1 \u2212(n/2)))\n> \u03b1\n2n\nn/2\u22121\nX\nk=0\n1\n\u03b1\n\u0012n\nk\n\u0013\n\u2113(w\u229b\n2 (1 + k \u2212(n/2)))\n> \u03b1\n2n \u2113\n\uf8eb\n\uf8ed\nn/2\u22121\nX\nk=0\n1\n\u03b1\n\u0012n\nk\n\u0013\nw\u229b\n2 (1 + k \u2212(n/2))\n\uf8f6\n\uf8f8\nwhere the last step uses Jenson\u2019s inequality. Continuing,\nEk\u223cB(n,1/2)(\u2113(w\u229b\n2 (k \u2212(n/2) + 1))) > \u03b1\n2n \u2113\n\uf8eb\n\uf8edw\u229b\n2 + w\u229b\n2\n\u03b1\nn/2\u22121\nX\nk=0\n\u0012n\nk\n\u0013\n(k \u2212(n/2))\n\uf8f6\n\uf8f8.\nEquation (5.18) of Concrete Mathematics Graham et al. [1989] and the bound\n\u0000 n\nn/2\n\u0001\n\u2265\n2n\n\u221a\n2n give\nn/2\u22121\nX\nk=0\n\u0012n\nk\n\u0013\n(k \u2212(n/2)) = \u2212n\n4\n\u0012 n\nn/2\n\u0013\n\u2264\u2212\n\u221a\n2n 2n\u22121\n4\n.\nTherefore, recalling that \u03b1 < 2n\u22121 and noting \u03b1/2n > 1/3 when n \u22656,\nEk\u223cB(n,1/2)(\u2113(w\u229b\n2 (k \u2212(n/2) + 1))) > \u03b1\n2n \u2113\n \nw\u229b\n2 \u2212w\u229b\n2\n\u03b1\n2n\u22121\u221a\n2n\n4\n!\n> 1\n3\u2113\n \nw\u229b\n2 \u2212w\u229b\n2\n\u221a\n2n\n4\n!\n.\nWe now have the necessary tools to prove Theorem 21.\nProof: (of Theorem 21) If w\u229b\n1 > w\u229b\n2 then the \ufb01rst feature will dominate the majority vote of the others\nand the optimizing w\u229bhas prediction error rate 1/10 . We now assume to the contrary that w\u229b\n1 \u2264w\u229b\n2 .\nWhen n > 125 and w\u229b\n2 \u22654/9 (from Lemma 41) we have\nw\u229b\n2 \u2212w\u229b\n2\n\u221a\n2n\n4\n\u2264\u22121.31\nand \u2113(w\u229b\n2 \u2212w\u229b\n2\n\u221a\n2n\n4\n) > 1.54.\nLemmas 42 and 43 now imply that J(w\u229b) > 0.51, but (as in Lemma 41) J(2.1, 0, 0, . . . 0) < 0.51,\ncontradicting the optimality of w\u229b.\nMany of the approximations used to prove Theorem 21 are quite loose, resulting in large values of n\nbeing needed to obtain the contradiction. For this class of distributions and q = 1/2 we conjecture that\noptimizing the dropout criterion fails to produce the Bayes optimal hypothesis for every even n \u22654.\n32\nTable 3: Probabilities of x1r1 and P4\ni=2 xiri values assuming dropout probability q = 1/2.\nx1r1\nprobability\nP4\ni=2 xiri\nprobability\n1\n9/20\n2\n1/8\n0\n1/2\n1\n3/8\n-1\n1/20\n0\n3/8\n-1\n1/8\nJ\nProof of Theorem 22\nTheorem 22. If dropout probability q = 1/2 and the number of features is n = 4 then the\nminimizer of the dropout criteria w\u2217(P8, q) has has prediction error rate erP8(w\u2217(P8, q)) \u2265\n1/10.\nIn this proof, let us also refer to P8 as just P and let w\u229bbe the minimizer of (41).\nAs before, the optimizing w\u229bhas the form (w\u229b\n1 , w\u229b\n2 , w\u229b\n2 , w\u229b\n2 ) by symmetry and convexity. Recalling\nthat the label y is always 1 under distribution P, we can use the equivalent criterion\nK(w1, w2) = E(x,y)\u223cP,r(\u2113(y(w \u00b7 x))) = E(x,y)\u223cP,r\n \n\u2113\n \nw1x1r1 + w2\n4\nX\ni=2\nxiri\n!!\n.\nThis expectation can be written with 12 terms, one for each pairing of the three possible x1r1 values with\nthe four possible P4\ni=2 xiri \u2208{\u22121, 0, 1, 2} values (see Table 3).\nTaking them in order, we have\nK(w1, w2) = 9\n160\u2113(w1 + 2w2) + 27\n160\u2113(w1 + w2) + 27\n160\u2113(w1) +\n9\n160\u2113(w1 \u2212w2)\n+ 10\n160\u2113(2w2) + 30\n160\u2113(w2) + 30\n160\u2113(0) + 10\n160\u2113(w2)\n+\n1\n160\u2113(\u2212w1 + 2w2) +\n3\n160\u2113(\u2212w1 + w2) +\n3\n160\u2113(\u2212w1) +\n1\n160\u2113(\u2212w1 \u2212w2) .\nSo, when p = q = 1/2, the derivatives are:\n\u2202K\n\u2202w1\n=\n \n\u22129\n1 + exp(w1 + 2w2) +\n\u221227\n1 + exp(w1 + w2) +\n\u221227\n1 + exp(w1) +\n\u22129\n1 + exp(w1 \u2212w2)\n+\n1\n1 + exp(\u2212w1 + 2w2) +\n3\n1 + exp(\u2212w1 + w2) +\n3\n1 + exp(\u2212w1) +\n1\n1 + exp(\u2212w1 \u2212w2)\n!.\n160,\n\u2202K\n\u2202w2\n=\n \n\u221218\n1 + exp(w1 + 2w2) +\n\u221227\n1 + exp(w1 + w2) +\n9\n1 + exp(w1 \u2212w2)\n+\n\u221220\n1 + exp(2w2) +\n\u221230\n1 + exp(w2) +\n10\n1 + exp(\u2212w2)\n+\n\u22122\n1 + exp(\u2212w1 + 2w2) +\n\u22123\n1 + exp(\u2212w1 + w2) +\n1\n1 + exp(\u2212w1 \u2212w2)\n!.\n160.\n33\nw2\nw1\n(a, a)\n\u2207K = (\u2212c, c)\n\u2212\u2207K = (c, \u2212c)\nFigure 5: If \u2207K at some (a, a) is (\u2212c, c) for some c > 0 then w\u229b\n1 > w\u229b\n2 .\nIf w\u229b\n1 > w\u229b\n2 , then dropout will have prediction error rate 1/10 as w\u229b\n1 will dominate the vote of the other\nthree components. We show that w\u229b\n1 > w\u229b\n2 by proving that there is a point (a, a) in weight space such that\nthe gradient at (a, a) is of the form (\u2212c, c) for some c > 0 (see Figure 5).\nThe derivatives when evaluated at (a, a) are:\n\u2202K\n\u2202w1\n\f\f\f\f\f\n(a,a)\n=\n \n\u22129\n1 + exp(3a) +\n\u221227\n1 + exp(2a) +\n\u221226\n1 + exp(a) \u22123 +\n3\n1 + exp(\u2212a) +\n1\n1 + exp(\u22122a)\n!.\n160\n\u2202K\n\u2202w2\n\f\f\f\f\f\n(a,a)\n=\n \n\u221218\n1 + exp(3a) +\n\u221247\n1 + exp(2a) +\n\u221232\n1 + exp(a) + 3 +\n10\n1 + exp(\u2212a) +\n1\n1 + exp(\u22122a)\n!.\n160.\nNote that both of these derivatives are increasing in a, positive for large a, and negative when a = 0. At a =\n2 ln(2), derivative \u2202K/\u2202w1\n\f\f\n(a,a) is still negative, while \u2202K/\u2202w1\n\f\f\n(a,a) has turned positive, so \u2202K/\u2202w1\n\f\f\n(a,a)\ncrosses 0 \ufb01rst. The continuity of the partial derivatives now implies the existence of an (a, a) where \u2207K\nhas the form (\u2212c, c), completing the proof.\nK\nProof of Theorem 23\nTheorem 23. If q = 1/2, n \u2265100, \u03b1 > 0, \u03b2 = 1/(10\u221an \u22121), and \u03b7 \u2264\n1\n2+exp(54\u221an), then\nerP9(w\u2217(P9, q)) = \u03b7.\nFor this subsection, let P = P9 and de\ufb01ne the scaled dropout criterion\nJ(w) = E(x,y)\u223cP,r(\u2113(yw \u00b7 (r \u2299x))),\nwhere, as earlier, the components of r are independent samples from a Bernoulli distribution with parameter\np = 1 \u2212q = 1/2 > 0. Let w\u229bbe the minimizer of J, so that w\u2217= pw\u229b.\nNote that, by symmetry, the contribution to J from the cases where y is \u22121 and 1 respectively are the\nsame, so the value of J is not affected if we clamp y at 1. Let us use this form to express J, and let D be the\nmarginal distribution of feature vector x conditioned on the label y = 1.\nLet B = {2, ..., n}. By symmetry, w\u229b\ni is identical for all i \u2208B so w\u229bis the minimum of J over\nweight vectors satisfying this constraint. Let K(w1, w2) = J(w1, w2, ..., w2); note that w\u229b\n1 , w\u229b\n2 minimizes\nK de\ufb01ned by\nK(w1, w2) = Ex\u223cD,r(\u2113(w1r1x1 + w2\nX\ni\u2208B\nrixi)).\n34\nTo prove Theorem 23, it suf\ufb01ces to show that\nw\u229b\n1 > (n \u22121)w\u229b\n2 /\u03b1 > 0,\n(42)\nsince when (42) holds, w\u229balways outputs x1.\nWe have\n\u2202K\n\u2202w1\n= 1\n2Ex\u223cD,r\n\u0012\n\u2212x1\n1 + exp(w1x1 + w2\nP\ni\u2208B rixi)\n\u0013\n(43)\n\u2202K\n\u2202w2\n= Ex\u223cD,r\n\u0012\n\u2212P\ni\u2208B rixi\n1 + exp(w1r1x1 + w2\nP\ni\u2208B rixi)\n\u0013\n.\n(44)\n(Note that, in (43), we have marginalized out r1.)\nLemma 44 w\u229b\n2 > 0.\nAs before, it suf\ufb01ces to show that there is a point (a1, 0) where both \u2202K\n\u2202w2\n\f\f\n(a1,0) < 0 and \u2202K\n\u2202w1\n\f\f\n(a1,0) = 0.\nFrom equation (44),\n\u2202K\n\u2202w2\n\f\f\n(a1,0) = Ex\u223cD,r\n\u0012\n\u2212P\ni\u2208B rixi\n1 + exp(a1r1x1)\n\u0013\n< 0\nfor all real a1.\nNow, evaluating (43), dividing into cases based on x1, we get\n\u2202K\n\u2202w1\n\f\f\n(a1,0) = (\u03b7/2)\n\u0012\n\u03b1\n1 + exp(\u2212\u03b1a1)\n\u0013\n+ ((1 \u2212\u03b7)/2)\n\u0012\n\u2212\u03b1\n1 + exp(\u03b1a1)\n\u0013\n.\nThis approaches \u2212\u03b1((1 \u2212\u03b7)/2) as a1 approaches \u2212\u221e, and it approaches \u03b1\u03b7/2 as a1 approaches \u221e. Since\nit is a continuous function of a1, there must be a value of a1 such that \u2202K\n\u2202w1\n\f\f\n(a1,0) = 0. Putting this together\nwith \u2202K\n\u2202w2\n\f\f\n(a1,0) < 0 completes the proof.\nTo show the suf\ufb01cient inequalities (42), it will be useful to prove an upper bound on w\u229b\n2 . (This upper\nbound will make it easier to show, informally, that w\u229b\n1 is needed.) In order to bound the size of w\u229b\n2 , we\nwill prove a lower bound on K in terms of w2. For this, we want to show that, if w2 is too large, then the\nalgorithm will pay too much when it makes large-margin errors. For this, we need a lower bound on the\nprobability of a large-margin error. For this, we can adapt an analysis that provided a lower bound on the\nprobability of an error from Helmbold and Long [2012].\nTo simplify the proof, we will \ufb01rst provide a lower bound on the dropout risk in terms of the risk without\ndropout. We will actually prove something somewhat more general, for possible future reference.\nLemma 45 Let r and x be independent, RN-valued random variables; let \u03c6 be convex function of a scalar\nreal variable. Then\nEr,x\n \n\u03c6\n X\ni\nxiri\n!!\n\u2265Ex\n \n\u03c6\n X\ni\nxiEr(ri)\n!!\n.\n35\nProof: Since x and r are independent,\nEr,x(\u03c6(\nX\ni\nxiri))\n= Ex(Er(\u03c6(\nX\ni\nxiri)))\n\u2265Ex(\u03c6(Er(\nX\ni\nxiri))) (by Jensen\u2019s Inequality)\n= Ex(\u03c6(\nX\ni\nxiEr(ri))),\ncompleting the proof.\nNow, it is enough to lower bound the probability of a large-margin error with respect to the original\ndistribution. Recall B = {2, . . . , n}.\nLemma 46\nPr\n \n1\nn \u22121\nX\ni\u2208B\nxi < \u22122\u03b2\n!\n\u22653\n10.\nProof: If Z is a standard normal random variable and R is a binomial (\u2113, p) random variable with p \u22641/2,\nthen for \u2113(1 \u2212p) \u2264j \u2264\u2113p, Slud\u2019s inequality Slud [1977] (see also Lemma 23 of Helmbold and Long\n[2012]) gives\nPr(R \u2265j) \u2265Pr\n \nZ \u2265\nj \u2212\u2113p\np\n\u2113p(1 \u2212p)\n!\n.\n(45)\nNow, we have\nPr\n \n1\nn \u22121\nX\ni\u2208B\nxi < \u22122\u03b2\n!\n= Pr\n X\ni\u2208B\nxi/2 < \u2212(n \u22121)\u03b2\n!\n= Pr\n X\ni\u2208B\n(xi + 1)/2 < (n \u22121)/2 \u2212(n \u22121)\u03b2\n!\n= Pr\n X\ni\u2208B\nzi < (n \u22121)(1/2 \u2212\u03b2)\n!\nwhere the zi\u2019s are independent {0, 1}-valued variables with Pr(zi = 1) = 1/2 + \u03b2. Let \u00afzi be 1 \u2212zi, so\nP\ni\u2208B \u00afzi is a Binomial (n \u22121, 1/2 \u2212\u03b2) random variable. Furthermore,\nPr\n X\ni\u2208B\nzi < (n \u22121)(1/2 \u2212\u03b2)\n!\n= Pr\n X\ni\u2208B\n\u00afzi > (n \u22121) \u2212(n \u22121)(1/2 \u2212\u03b2)\n!\n= Pr\n X\ni\u2208B\n\u00afzi > (n \u22121)(1/2 + \u03b2)\n!\n.\n36\nUsing (45) with j = (n \u22121)(1/2 + \u03b2), \u2113= (n \u22121), and p = 1/2 \u2212\u03b2 gives:\nPr\n X\ni\u2208B\n\u00afzi > (n \u22121)(1/2 + \u03b2)\n!\n\u2265Pr\n \nZ \u2265(n \u22121)(1/2 + \u03b2) \u2212(n \u22121)(1/2 \u2212\u03b2)\np\n(n \u22121)(1/4 \u2212\u03b22)\n!\n= Pr\n \nZ \u2265\n2(n \u22121)\u03b2\np\n(n \u22121)(1/4 \u2212\u03b22)\n!\n.\nSince \u03b2 = 1/(10\u221an) and n \u2265100, this implies\nPr\n \n1\nn \u22121\nX\ni\u2208B\nxi < \u22122\u03b2\n!\n\u2265Pr (Z \u22651/2) .\nSince the density of Z is always at most 1/\n\u221a\n2\u03c0, we have\nPr\n \n1\nn \u22121\nX\ni\u2208B\nxi < \u22122\u03b2\n!\n\u2265Pr(Z \u22650) \u2212Pr(Z \u2208(0, 1/2)) > 1\n2 \u2212\n1\n2\n\u221a\n2\u03c0 > 3/10,\ncompleting the proof.\nNow we are ready for the lower bound on the dropout risk in terms of w2.\nLemma 47 For all w1,\nK(w1, w2) > w2\n\u221an \u22121\n67\n.\nProof: Considering only the case in which x1 is dropped out (i.e. r1 = 0), we have\nK(w1, w2) \u22651\n2E\n \n\u2113\n \nw2\nX\ni\nrixi\n!!\n.\nApplying Lemma 45, we get\nK(w1, w2) \u22651\n2E\n \n\u2113\n \n(w2/2)\nX\ni\u2208B\nxi\n!!\n.\nSince \u2113is non-increasing and non-negative, we have\nK(w1, w2) \u22651\n2\u2113(\u2212w2\u03b2(n \u22121))Pr\n \n1\nn \u22121\nX\ni\u2208B\nxi < \u22122\u03b2\n!\n,\nand applying Lemma 46 gives\nK(w1, w2) \u22653\u2113(\u2212w2\u03b2(n \u22121))\n20\n.\nSince \u2113(z) > \u2212z, we have\nK(w1, w2) \u22653w2\u03b2(n \u22121)\n20\nand, using \u03b2 =\n1\n10\u221an\u22121, we get\nK(w1, w2) \u22653w2\n\u221an \u22121\n200\n,\ncompleting the proof.\n37\nLemma 48 w\u229b\n2 <\n27\n\u221an\u22121.\nProof: Note that\nK(w, 0) = \u2113(0)/2 + (1/2)(\u03b7\u2113(\u2212\u03b1w) + (1 \u2212\u03b7)\u2113(\u03b1w)),\nis increasing in \u03b7 so that\nK(w\u229b\n1 , w\u229b\n2 ) \u2264K(5/\u03b1, 0) < \u2113(0)/2 + 1/35\n(46)\nsince \u03b7 < 1/100.\nOn the other hand, Lemma 47 gives\nK(w\u229b\n1 , w\u229b\n2 ) > w2\n\u221an \u22121\n67\n.\nSolving for w\u229b\n2 completes the proof.\nLemma 49 For all 0 < u <\n27\n\u221an\u22121, we have\n\u2202K\n\u2202w1\n\f\f\n((n\u22121)u/\u03b1,u) < 0.\nProof: From (43), we have\n2 \u2202K\n\u2202w1\n\f\f\n(nu/\u03b1,u)\n= Ex\u223cD,r\n\u0012\n\u2212x1\n1 + exp((n \u22121)ux1/\u03b1 + u P\ni\u2208B rixi)\n\u0013\n= \u03b7Ex\u223cD,r\n\u0012\n\u03b1\n1 + exp(\u2212(n \u22121)u + u P\ni\u2208B rixi)\n\u0013\n+ (1 \u2212\u03b7)Ex\u223cD,r\n\u0012\n\u2212\u03b1\n1 + exp((n \u22121)u + u P\ni\u2208B rixi)\n\u0013\n< \u03b7\u03b1 + (1 \u2212\u03b7)Ex\u223cD,r\n\u0012\n\u2212\u03b1\n1 + exp((n \u22121)u + u P\ni\u2208B rixi)\n\u0013\n< \u03b1\n\u0012\n\u03b7 +\n\u2212(1 \u2212\u03b7)\n1 + exp(2(n \u22121)u)\n\u0013\n(since P\ni\u2208B rixi \u2264n \u22121)\n< \u03b1\n\u0012\n\u03b7 +\n\u2212(1 \u2212\u03b7)\n1 + exp(54\u221an \u22121\n\u0013\n(since u < 27/\u221an \u22121)\n< 0\nsince \u03b7 \u22641/(2 + exp(54\u221an)), completing the proof.\nRecall that, to prove Theorem 23, since we already showed w\u229b\n2 > 0, all we needed was to show that\n\u03b1w\u229b\n1 > (n \u22121)w\u229b\n2 . We do this next.\nLemma 50 \u03b1w\u229b\n1 > (n \u22121)w\u229b\n2 .\nProof: Let g be the gradient of J evaluated at u = ((n \u22121)w\u229b\n2 /\u03b1, w\u229b\n2 ). Lemmas 48 and 49 implies that\ng \u0338= (0, 0). By convexity\nw\u229b\u00b7 g < u \u00b7 g\nwhich, since u2 = w\u229b\n2 , implies\nw\u229b\n1 g1 < (n \u22121)w\u229b\n2 g1/\u03b1.\n38\nSince, by Lemmas 48 and 49, g1 < 0,\nw\u229b\n1 > (n \u22121)w\u229b\n2 /\u03b1\ncompleting the proof.\nL\nProof of Theorem 24\nTheorem 24. If \u03b2 = 1/(10\u221an \u22121), \u03bb =\n1\n30n, \u03b1 < \u03b2\u03bb, and n is a large enough even number,\nthen for any \u03b7 \u2208[0, 1], erP9(v(P9, \u03bb)) \u22653/10.\nIn this proof, let us also abbreviate P9 with P and use J to denote the L2 regularized criterion in\nEquation (5) specialized for distribution this P.\nAs before, the contribution to the L2 criteron from the cases where y is \u22121 and 1 respectively are the\nsame, so the value of the criterion is not affected if we clamp y at 1. Furthermore, we leave the dependency\non \u03bb implicit and (since the source is \ufb01xed) use the more succinct v for v(P, \u03bb).\nAlso, if, as before, we let B = {2, ..., n}, then by symmetry, vi is identical for all i \u2208B so v is the\nminimum of J over weight vectors satisfying this constraint. Let K(w1, w2) = J(w1, w2, ..., w2) so that\n(v1, v2) minimizes K. Recall that D is the marginal distribution of x under P conditioned on y = 1.\nK(w1, w2) = Ex\u223cD\n \n\u2113\n \nw1x1 + w2\nX\ni\u2208B\nxi\n!!\n+ \u03bb\n2 (w2\n1 + (n \u22121)w2\n2).\nLemma 46, together with the fact that |x1| = \u03b1, implies that,\n\u03b1v1 < 2\u03b2(n \u22121)v2\n(47)\nsuf\ufb01ces to prove Theorem 24, so we set this as our subtask.\nWe have\n\u2202K\n\u2202w1\n= Ex\u223cD\n\u0012\n\u2212x1\n1 + exp(w1x1 + w2\nP\ni\u2208B xi)\n\u0013\n+ \u03bbw1\n(48)\n\u2202K\n\u2202w2\n= Ex\u223cD\n\u0012\n\u2212P\ni\u2208B xi\n1 + exp(w1x1 + w2\nP\ni\u2208B xi)\n\u0013\n+ \u03bb(n \u22121)w2.\n(49)\nFirst, we need a rough bound on v1.\nLemma 51 |v1| \u2264\u03b1\n\u03bb < \u03b2.\nProof: The second inequality follows from the constraint on \u03b1. From (48), we get\n|v1| \u22641\n\u03bbEx\u223cD\n\u0012\f\f\f\f\nx1\n1 + exp(v1x1 + v2\nP\ni\u2208B xi)\n\f\f\f\f\n\u0013\nand the facts |x1| \u2264\u03b1 and 0 <\n1\n1+exp(v1x1+v2\nP\ni\u2208B xi) \u22641 then imply |v1| \u2264\u03b1/\u03bb.\nLemma 52 For large enough n,\nPr\n X\ni\u2208B\nxi \u2208[\u03b2(n \u22121), 3\u03b2(n \u22121)]\n!\n\u22651\n13.\n39\nProof: Let \u03a6(z) = Pr(Z \u2264z) for a standard normal random variable Z and let S = P\ni\u2208B xi. Note that\nE(xi) = 2\u03b2, var(xi) = 1 \u22124\u03b22, and the third moment E(|xi \u2212E(xi)|3) = 1 \u221216\u03b24. The Berry-Esseen\ninequality (see Theorem 11.1 of DasGupta [2008]) relates binomial distributions to the normal distribution\nusing these moments, and directly implies that\nsup\nz\n\f\f\f\f\fPr\n \nS\nn \u22121 \u22122\u03b2 \u2264\nr\n1 \u22124\u03b22\nn \u22121\n\u00d7 z\n!\n\u2212\u03a6(z)\n\f\f\f\f\f \u2264\nC(1 \u221216\u03b24)\n(1 \u22124\u03b22)3/2\u221an \u22121 <\n1\n\u221an \u22121\nwhere the last inequality follows from the facts that the Berry-Esseen global constant C \u22640.8 and \u03b2 <\n1/10.\nUsing the change of variable s =\np\n(1 \u22124\u03b22)(n \u22121) z + 2\u03b2(n \u22121) this can be restated:\nsup\ns\n\f\f\f\f\fPr (S \u2264s) \u2212\u03a6\n \ns \u22122\u03b2(n \u22121)\np\n(1 \u22124\u03b22)(n \u22121)\n!\f\f\f\f\f \u2264\n1\n\u221an \u22121,\nso\nPr(S \u2208[\u03b2(n \u22121), 3\u03b2(n \u22121)])\n\u2265Prz\u2208N(0,1)\n\u0012\nz \u2208\n\u0014\n\u2212\u03b2\nr n \u22121\n1 \u22124\u03b22 , \u03b2\nr n \u22121\n1 \u22124\u03b22\n\u0015\u0013\n\u2212\n2\n\u221an \u22121\n\u2265Prz\u2208N(0,1)\n\u0012\nz \u2208\n\u0014\u22121\n10 , 1\n10\n\u0015\u0013\n\u2212\n2\n\u221an \u22121\n\u22651\n13,\nfor large enough n.\nRecent work shows that the Berry-Esseen constant C is less then 1/2, this allows us to replace the\n2\u221an \u22121 with 1/\u221an \u22121, but it still requires n on the order of 150,000 to get the 1/13 bound. Reducing the\nbound to 1/50 would make n as small as 300 suf\ufb01cient.\nNext, we need a rough bound on v2.\nLemma 53 v2 \u2265\n1\nn\u22121.\nProof: From (49), we have\nv2 =\n1\n\u03bb(n \u22121)Ex\u223cD\n\u0012\nP\ni\u2208B xi\n1 + exp(v1x1 + v2\nP\ni\u2208B xi)\n\u0013\n.\nIf we denote P\ni\u2208B xi by S, then\nv2 =\n1\n\u03bb(n \u22121)Ex\u223cD\n\u0012\nS\n1 + exp(v1x1 + v2S)\n\u0013\n.\nSince, for all odd3 s > 0\nPr(S = s)\nPr(S = \u2212s) =\n\u00121 + 2\u03b2\n1 \u22122\u03b2\n\u0013s\n3S is the sum of an odd number of \u00b11\u2019s, and thus cannot be even.\n40\nso Pr(S = \u2212s) = Pr(S = s)\n\u0010\n1\u22122\u03b2\n1+2\u03b2\n\u0011s\n. Analyzing the contributions of s and \u2212s together we have\nv2\u03bb(n \u22121) =\nn\u22121\nX\ns=1\nPr(S = s)\n\u0010\n(1 \u2212\u03b7)\ns\n1 + exp(v1\u03b1 + v2s) + \u03b7\ns\n1 + exp(\u2212v1\u03b1 + v2s)\n+\n\u0012\n(1 \u2212\u03b7)\n\u2212s\n1 + exp(v1\u03b1 \u2212v2s) + \u03b7\n\u2212s\n1 + exp(\u2212v1\u03b1 \u2212v2s)\n\u0013 \u00121 \u22122\u03b2\n1 + 2\u03b2\n\u0013s \u0011\n.\nRecalling that |v1| \u2264\u03b1/\u03bb (Lemma 51), and using the minimizing value in this range for each term gives\nv2\u03bb(n \u22121) \u2265\nn\u22121\nX\ns=1\nPr(S = s)\n\u0012\ns\n1 + exp(\u03b12/\u03bb + v2s) +\n\u0012\n\u2212s\n1 + exp(\u2212\u03b12/\u03bb \u2212v2s)\n\u0013 \u00121 \u22122\u03b2\n1 + 2\u03b2\n\u0013s\u0013\n=\nn\u22121\nX\ns=1\nPr(S = s)s\n\uf8eb\n\uf8ed\n1 \u2212exp(\u03b12/\u03bb + v2s)\n\u0010\n1\u22122\u03b2\n1+2\u03b2\n\u0011s\n1 + exp(\u03b12/\u03bb + v2s)\n\uf8f6\n\uf8f8\n\u2265\nn\u22121\nX\ns=1\nPr(S = s)s\n\u00121 \u2212exp(\u03b12/\u03bb + v2s \u22124\u03b2s)\n1 + exp(\u03b12/\u03bb + v2s)\n\u0013\n.\nAssume for contradiction that v2 < 1/(n \u22121). Then,\nv2\u03bb(n \u22121) \u2265\nn\u22121\nX\ns=1\nPr(S = s)s\n\u00121 \u2212exp(\u03b12/\u03bb + s/(n \u22121) \u22124\u03b2s)\n1 + exp(\u03b12/\u03bb + s/(n \u22121))\n\u0013\n\u2265\nn\u22121\nX\ns=1\nPr(S = s)s\n\u00121 \u2212exp(s/(n \u22121) \u22123\u03b2s)\n1 + exp(\u03b22\u03bb + s/(n \u22121))\n\u0013\n(since \u03b1 \u2264\u03b2\u03bb)\n\u2265\nn\u22121\nX\ns=1\nPr(S = s)s\n\u0012\n1 \u2212exp(\u22122\u03b2s)\n1 + exp(\u03b22\u03bb + s/(n \u22121))\n\u0013\n(for large enough n)\n\u2265\nX\ns\u2208[\u03b2(n\u22121),3\u03b2(n\u22121)]\nPr(S = s)s\n\u0012\n1 \u2212exp(\u22122\u03b2s)\n1 + exp(\u03b22\u03bb + s/(n \u22121))\n\u0013\n,\nsince each term is positive. Taking the worst-case among [\u03b2(n \u22121), 3\u03b2(n \u22121)] for each instance of s, and\napplying Lemma 52, we get\nv2 \u2265\n1\n\u03bb(n \u22121) \u00d7 1\n13 \u00d7 \u03b2(n \u22121)\n\u00121 \u2212exp(\u22122\u03b22(n \u22121))\n1 + exp(\u03b22\u03bb + 3\u03b2)\n\u0013\n= 30\u221an \u22121\n130\n\u0012\n1 \u2212exp(\u22121/50)\n1 + exp(3/(10\u221an \u22121) + 1/(3000n(n \u22121)))\n\u0013\n.\n(50)\nThus v2 = \u2126(\u221an \u22121), which, for large enough n, contradicts our assumption that v2 < 1/(n \u22121),\ncompleting the proof.\nNot that even with the many approximations made, Inequality (50) gives the desired contradiction at\nn = 60. Even when the weaker bound of 1/50 discussed following Lemma 52 is used, n = 145 still suf\ufb01ces\nto give the desired contradiction.\n41\nNow we\u2019re ready to put everything together.\nProof (of Theorem 24): Recall that, by Lemma 46, if v1 < 2\u03b2(n \u22121)v2, then erP (v(P, \u03bb)) \u22653/10.\nLemma 51 gives v1 < \u03b2. Lemma 53 implies (n \u22121)v2 \u22651. Therefore v1 < \u03b2(n \u22121)v2, completing\nthe proof.\nUsing the 1/50 version of Lemma 52 leads to a proof of the theorem for all even n \u2265300.\n42\n",
        "sentence": "",
        "context": "e. a. L. Deng. Recent advances in deep learning for speech research at microsoft. ICASSP, 2013.\nP. M. Long and R. A. Servedio. Random classi\ufb01cation noise defeats all convex potential boosters. Machine\nLearning, 78(3):287\u2013304, 2010.\npages 2814\u20132822, 2013.\nP. L. Bartlett, M. I. Jordan, and J. D. McAuliffe. Convexity, classi\ufb01cation, and risk bounds. Journal of the\nAmerican Statistical Association, 101(473):138\u2013156, 2006.\nG. Dahl. Deep learning how i did it: Merck 1st place interview, 2012. http://blog.kaggle.com.\nG. E. Dahl, T. N. Sainath, and G. E. Hinton. Improving deep neural networks for lvcsr using recti\ufb01ed linear\nunits and dropout. ICASSP, 2013.\n17"
    },
    {
        "title": "Improving neural networks by preventing co-adaptation of feature detectors",
        "author": [
            "Hinton",
            "Geoffrey E",
            "Srivastava",
            "Nitish",
            "Krizhevsky",
            "Alex",
            "Sutskever",
            "Ilya",
            "Salakhutdinov",
            "Ruslan R"
        ],
        "venue": "arXiv preprint arXiv:1207.0580,",
        "citeRegEx": "Hinton et al\\.,? \\Q2012\\E",
        "shortCiteRegEx": "Hinton et al\\.",
        "year": 2012,
        "abstract": "",
        "full_text": "",
        "sentence": " Extensive experiments (Hinton et al., 2012) have shown that dropout can help obtain the state-of-the-art performance on a range of benchmark data sets. In standard dropout (Wager et al., 2013; Hinton et al., 2012), the entries of the noise vector are sampled independently according to Pr( j = 0) = \u03b4 and Pr( j = 1 1\u2212\u03b4 ) = 1 \u2212 \u03b4, i.",
        "context": null
    },
    {
        "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
        "author": [
            "Ioffe",
            "Sergey",
            "Szegedy",
            "Christian"
        ],
        "venue": "arXiv preprint arXiv:1502.03167,",
        "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "Ioffe et al\\.",
        "year": 2015,
        "abstract": "Training Deep Neural Networks is complicated by the fact that the\ndistribution of each layer's inputs changes during training, as the parameters\nof the previous layers change. This slows down the training by requiring lower\nlearning rates and careful parameter initialization, and makes it notoriously\nhard to train models with saturating nonlinearities. We refer to this\nphenomenon as internal covariate shift, and address the problem by normalizing\nlayer inputs. Our method draws its strength from making normalization a part of\nthe model architecture and performing the normalization for each training\nmini-batch. Batch Normalization allows us to use much higher learning rates and\nbe less careful about initialization. It also acts as a regularizer, in some\ncases eliminating the need for Dropout. Applied to a state-of-the-art image\nclassification model, Batch Normalization achieves the same accuracy with 14\ntimes fewer training steps, and beats the original model by a significant\nmargin. Using an ensemble of batch-normalized networks, we improve upon the\nbest published result on ImageNet classification: reaching 4.9% top-5\nvalidation error (and 4.8% test error), exceeding the accuracy of human raters.",
        "full_text": "arXiv:1502.03167v3  [cs.LG]  2 Mar 2015\nBatch Normalization: Accelerating Deep Network Training by\nReducing Internal Covariate Shift\nSergey Ioffe\nGoogle Inc., sioffe@google.com\nChristian Szegedy\nGoogle Inc., szegedy@google.com\nAbstract\nTraining Deep Neural Networks is complicated by the fact\nthat the distribution of each layer\u2019s inputs changes during\ntraining, as the parameters of the previous layers change.\nThis slows down the training by requiring lower learning\nrates and careful parameter initialization, and makes it no-\ntoriously hard to train models with saturating nonlineari-\nties. We refer to this phenomenon as internal covariate\nshift, and address the problem by normalizing layer in-\nputs. Our method draws its strength from making normal-\nization a part of the model architecture and performing the\nnormalization for each training mini-batch. Batch Nor-\nmalization allows us to use much higher learning rates and\nbe less careful about initialization. It also acts as a regu-\nlarizer, in some cases eliminating the need for Dropout.\nApplied to a state-of-the-art image classi\ufb01cation model,\nBatch Normalization achieves the same accuracy with 14\ntimes fewer training steps, and beats the original model\nby a signi\ufb01cant margin.\nUsing an ensemble of batch-\nnormalized networks, we improve upon the best published\nresult on ImageNet classi\ufb01cation: reaching 4.9% top-5\nvalidation error (and 4.8% test error), exceeding the ac-\ncuracy of human raters.\n1\nIntroduction\nDeep learning has dramatically advanced the state of the\nart in vision, speech, and many other areas.\nStochas-\ntic gradient descent (SGD) has proved to be an effec-\ntive way of training deep networks, and SGD variants\nsuch as momentum (Sutskever et al., 2013) and Adagrad\n(Duchi et al., 2011) have been used to achieve state of the\nart performance. SGD optimizes the parameters \u0398 of the\nnetwork, so as to minimize the loss\n\u0398 = arg min\n\u0398\n1\nN\nN\nX\ni=1\n\u2113(xi, \u0398)\nwhere x1...N is the training data set. With SGD, the train-\ning proceeds in steps, and at each step we consider a mini-\nbatch x1...m of size m. The mini-batch is used to approx-\nimate the gradient of the loss function with respect to the\nparameters, by computing\n1\nm\n\u2202\u2113(xi, \u0398)\n\u2202\u0398\n.\nUsing mini-batches of examples, as opposed to one exam-\nple at a time, is helpful in several ways. First, the gradient\nof the loss over a mini-batch is an estimate of the gradient\nover the training set, whose quality improves as the batch\nsize increases. Second, computation over a batch can be\nmuch more ef\ufb01cient than m computations for individual\nexamples, due to the parallelism afforded by the modern\ncomputing platforms.\nWhile stochastic gradient is simple and effective, it\nrequires careful tuning of the model hyper-parameters,\nspeci\ufb01cally the learning rate used in optimization, as well\nas the initial values for the model parameters. The train-\ning is complicated by the fact that the inputs to each layer\nare affected by the parameters of all preceding layers \u2013 so\nthat small changes to the network parameters amplify as\nthe network becomes deeper.\nThe change in the distributions of layers\u2019 inputs\npresents a problem because the layers need to continu-\nously adapt to the new distribution. When the input dis-\ntribution to a learning system changes, it is said to experi-\nence covariate shift (Shimodaira, 2000). This is typically\nhandled via domain adaptation (Jiang, 2008). However,\nthe notion of covariate shift can be extended beyond the\nlearning system as a whole, to apply to its parts, such as a\nsub-network or a layer. Consider a network computing\n\u2113= F2(F1(u, \u03981), \u03982)\nwhere F1 and F2 are arbitrary transformations, and the\nparameters \u03981, \u03982 are to be learned so as to minimize\nthe loss \u2113. Learning \u03982 can be viewed as if the inputs\nx = F1(u, \u03981) are fed into the sub-network\n\u2113= F2(x, \u03982).\nFor example, a gradient descent step\n\u03982 \u2190\u03982 \u2212\u03b1\nm\nm\nX\ni=1\n\u2202F2(xi, \u03982)\n\u2202\u03982\n(for batch size m and learning rate \u03b1) is exactly equivalent\nto that for a stand-alone network F2 with input x. There-\nfore, the input distribution properties that make training\nmore ef\ufb01cient \u2013 such as having the same distribution be-\ntween the training and test data \u2013 apply to training the\nsub-network as well. As such it is advantageous for the\ndistribution of x to remain \ufb01xed over time. Then, \u03982 does\n1\nnot have to readjust to compensate for the change in the\ndistribution of x.\nFixed distribution of inputs to a sub-network would\nhave positive consequences for the layers outside the sub-\nnetwork, as well. Consider a layer with a sigmoid activa-\ntion function z = g(Wu + b) where u is the layer input,\nthe weight matrix W and bias vector b are the layer pa-\nrameters to be learned, and g(x) =\n1\n1+exp(\u2212x). As |x|\nincreases, g\u2032(x) tends to zero. This means that for all di-\nmensions of x = Wu+b except those with small absolute\nvalues, the gradient \ufb02owing down to u will vanish and the\nmodel will train slowly. However, since x is affected by\nW, b and the parameters of all the layers below, changes\nto those parameters during training will likely move many\ndimensions of x into the saturated regime of the nonlin-\nearity and slow down the convergence.\nThis effect is\nampli\ufb01ed as the network depth increases.\nIn practice,\nthe saturation problem and the resulting vanishing gradi-\nents are usually addressed by using Recti\ufb01ed Linear Units\n(Nair & Hinton, 2010) ReLU(x) = max(x, 0), careful\ninitialization (Bengio & Glorot, 2010; Saxe et al., 2013),\nand small learning rates. If, however, we could ensure\nthat the distribution of nonlinearity inputs remains more\nstable as the network trains, then the optimizer would be\nless likely to get stuck in the saturated regime, and the\ntraining would accelerate.\nWe refer to the change in the distributions of internal\nnodes of a deep network, in the course of training, as In-\nternal Covariate Shift. Eliminating it offers a promise of\nfaster training. We propose a new mechanism, which we\ncall Batch Normalization, that takes a step towards re-\nducing internal covariate shift, and in doing so dramati-\ncally accelerates the training of deep neural nets. It ac-\ncomplishes this via a normalization step that \ufb01xes the\nmeans and variances of layer inputs. Batch Normalization\nalso has a bene\ufb01cial effect on the gradient \ufb02ow through\nthe network, by reducing the dependence of gradients\non the scale of the parameters or of their initial values.\nThis allows us to use much higher learning rates with-\nout the risk of divergence. Furthermore, batch normal-\nization regularizes the model and reduces the need for\nDropout (Srivastava et al., 2014). Finally, Batch Normal-\nization makes it possible to use saturating nonlinearities\nby preventing the network from getting stuck in the satu-\nrated modes.\nIn Sec. 4.2, we apply Batch Normalization to the best-\nperforming ImageNet classi\ufb01cation network, and show\nthat we can match its performance using only 7% of the\ntraining steps, and can further exceed its accuracy by a\nsubstantial margin. Using an ensemble of such networks\ntrained with Batch Normalization, we achieve the top-5\nerror rate that improves upon the best known results on\nImageNet classi\ufb01cation.\n2\nTowards\nReducing\nInternal\nCovariate Shift\nWe de\ufb01ne Internal Covariate Shift as the change in the\ndistribution of network activations due to the change in\nnetwork parameters during training. To improve the train-\ning, we seek to reduce the internal covariate shift. By\n\ufb01xing the distribution of the layer inputs x as the training\nprogresses, we expect to improve the training speed. It has\nbeen long known (LeCun et al., 1998b; Wiesler & Ney,\n2011) that the network training converges faster if its in-\nputs are whitened \u2013 i.e., linearly transformed to have zero\nmeans and unit variances, and decorrelated. As each layer\nobserves the inputs produced by the layers below, it would\nbe advantageous to achieve the same whitening of the in-\nputs of each layer. By whitening the inputs to each layer,\nwe would take a step towards achieving the \ufb01xed distri-\nbutions of inputs that would remove the ill effects of the\ninternal covariate shift.\nWe could consider whitening activations at every train-\ning step or at some interval, either by modifying the\nnetwork directly or by changing the parameters of the\noptimization algorithm to depend on the network ac-\ntivation values (Wiesler et al., 2014; Raiko et al., 2012;\nPovey et al., 2014; Desjardins & Kavukcuoglu).\nHow-\never, if these modi\ufb01cations are interspersed with the op-\ntimization steps, then the gradient descent step may at-\ntempt to update the parameters in a way that requires\nthe normalization to be updated, which reduces the ef-\nfect of the gradient step. For example, consider a layer\nwith the input u that adds the learned bias b, and normal-\nizes the result by subtracting the mean of the activation\ncomputed over the training data: bx = x \u2212E[x] where\nx = u + b, X = {x1...N} is the set of values of x over\nthe training set, and E[x] =\n1\nN\nPN\ni=1 xi. If a gradient\ndescent step ignores the dependence of E[x] on b, then it\nwill update b \u2190b + \u2206b, where \u2206b \u221d\u2212\u2202\u2113/\u2202bx. Then\nu + (b + \u2206b) \u2212E[u + (b + \u2206b)] = u + b \u2212E[u + b].\nThus, the combination of the update to b and subsequent\nchange in normalization led to no change in the output\nof the layer nor, consequently, the loss. As the training\ncontinues, b will grow inde\ufb01nitely while the loss remains\n\ufb01xed. This problem can get worse if the normalization not\nonly centers but also scales the activations. We have ob-\nserved this empirically in initial experiments, where the\nmodel blows up when the normalization parameters are\ncomputed outside the gradient descent step.\nThe issue with the above approach is that the gradient\ndescent optimization does not take into account the fact\nthat the normalization takes place. To address this issue,\nwe would like to ensure that, for any parameter values,\nthe network always produces activations with the desired\ndistribution. Doing so would allow the gradient of the\nloss with respect to the model parameters to account for\nthe normalization, and for its dependence on the model\nparameters \u0398. Let again x be a layer input, treated as a\n2\nvector, and X be the set of these inputs over the training\ndata set. The normalization can then be written as a trans-\nformation\nbx = Norm(x, X)\nwhich depends not only on the given training example x\nbut on all examples X \u2013 each of which depends on \u0398 if\nx is generated by another layer. For backpropagation, we\nwould need to compute the Jacobians\n\u2202Norm(x, X)\n\u2202x\nand \u2202Norm(x, X)\n\u2202X\n;\nignoring the latter term would lead to the explosion de-\nscribed above. Within this framework, whitening the layer\ninputs is expensive, as it requires computing the covari-\nance matrix Cov[x] = Ex\u2208X [xxT ] \u2212E[x]E[x]T and its\ninverse square root, to produce the whitened activations\nCov[x]\u22121/2(x \u2212E[x]), as well as the derivatives of these\ntransforms for backpropagation. This motivates us to seek\nan alternative that performs input normalization in a way\nthat is differentiable and does not require the analysis of\nthe entire training set after every parameter update.\nSome\nof\nthe\nprevious\napproaches\n(e.g.\n(Lyu & Simoncelli,\n2008))\nuse\nstatistics\ncomputed\nover a single training example, or, in the case of image\nnetworks, over different feature maps at a given location.\nHowever, this changes the representation ability of a\nnetwork by discarding the absolute scale of activations.\nWe want to a preserve the information in the network, by\nnormalizing the activations in a training example relative\nto the statistics of the entire training data.\n3\nNormalization\nvia\nMini-Batch\nStatistics\nSince the full whitening of each layer\u2019s inputs is costly\nand not everywhere differentiable, we make two neces-\nsary simpli\ufb01cations. The \ufb01rst is that instead of whitening\nthe features in layer inputs and outputs jointly, we will\nnormalize each scalar feature independently, by making it\nhave the mean of zero and the variance of 1. For a layer\nwith d-dimensional input x = (x(1) . . . x(d)), we will nor-\nmalize each dimension\nbx(k) = x(k) \u2212E[x(k)]\np\nVar[x(k)]\nwhere the expectation and variance are computed over the\ntraining data set. As shown in (LeCun et al., 1998b), such\nnormalization speeds up convergence, even when the fea-\ntures are not decorrelated.\nNote that simply normalizing each input of a layer may\nchange what the layer can represent. For instance, nor-\nmalizing the inputs of a sigmoid would constrain them to\nthe linear regime of the nonlinearity. To address this, we\nmake sure that the transformation inserted in the network\ncan represent the identity transform. To accomplish this,\nwe introduce, for each activation x(k), a pair of parameters\n\u03b3(k), \u03b2(k), which scale and shift the normalized value:\ny(k) = \u03b3(k)bx(k) + \u03b2(k).\nThese parameters are learned along with the original\nmodel parameters, and restore the representation power\nof the network. Indeed, by setting \u03b3(k) =\np\nVar[x(k)] and\n\u03b2(k) = E[x(k)], we could recover the original activations,\nif that were the optimal thing to do.\nIn the batch setting where each training step is based on\nthe entire training set, we would use the whole set to nor-\nmalize activations. However, this is impractical when us-\ning stochastic optimization. Therefore, we make the sec-\nond simpli\ufb01cation: since we use mini-batches in stochas-\ntic gradient training, each mini-batch produces estimates\nof the mean and variance of each activation. This way, the\nstatistics used for normalization can fully participate in\nthe gradient backpropagation. Note that the use of mini-\nbatches is enabled by computation of per-dimension vari-\nances rather than joint covariances; in the joint case, reg-\nularization would be required since the mini-batch size is\nlikely to be smaller than the number of activations being\nwhitened, resulting in singular covariance matrices.\nConsider a mini-batch B of size m. Since the normal-\nization is applied to each activation independently, let us\nfocus on a particular activation x(k) and omit k for clarity.\nWe have m values of this activation in the mini-batch,\nB = {x1...m}.\nLet the normalized values be bx1...m, and their linear trans-\nformations be y1...m. We refer to the transform\nBN\u03b3,\u03b2 : x1...m \u2192y1...m\nas the Batch Normalizing Transform. We present the BN\nTransform in Algorithm 1. In the algorithm, \u01eb is a constant\nadded to the mini-batch variance for numerical stability.\nInput: Values of x over a mini-batch: B = {x1...m};\nParameters to be learned: \u03b3, \u03b2\nOutput: {yi = BN\u03b3,\u03b2(xi)}\n\u00b5B \u21901\nm\nm\nX\ni=1\nxi\n// mini-batch mean\n\u03c32\nB \u21901\nm\nm\nX\ni=1\n(xi \u2212\u00b5B)2\n// mini-batch variance\nbxi \u2190xi \u2212\u00b5B\np\n\u03c32\nB + \u01eb\n// normalize\nyi \u2190\u03b3bxi + \u03b2 \u2261BN\u03b3,\u03b2(xi)\n// scale and shift\nAlgorithm 1: Batch Normalizing Transform, applied to\nactivation x over a mini-batch.\nThe BN transform can be added to a network to manip-\nulate any activation. In the notation y = BN\u03b3,\u03b2(x), we\n3\nindicate that the parameters \u03b3 and \u03b2 are to be learned,\nbut it should be noted that the BN transform does not\nindependently process the activation in each training ex-\nample. Rather, BN\u03b3,\u03b2(x) depends both on the training\nexample and the other examples in the mini-batch. The\nscaled and shifted values y are passed to other network\nlayers. The normalized activations bx are internal to our\ntransformation, but their presence is crucial. The distri-\nbutions of values of any bx has the expected value of 0\nand the variance of 1, as long as the elements of each\nmini-batch are sampled from the same distribution, and\nif we neglect \u01eb.\nThis can be seen by observing that\nPm\ni=1 bxi = 0 and\n1\nm\nPm\ni=1 bx2\ni = 1, and taking expec-\ntations. Each normalized activation bx(k) can be viewed as\nan input to a sub-network composed of the linear trans-\nform y(k) = \u03b3(k)bx(k) + \u03b2(k), followed by the other pro-\ncessing done by the original network. These sub-network\ninputs all have \ufb01xed means and variances, and although\nthe joint distribution of these normalized bx(k) can change\nover the course of training, we expect that the introduc-\ntion of normalized inputs accelerates the training of the\nsub-network and, consequently, the network as a whole.\nDuring training we need to backpropagate the gradi-\nent of loss \u2113through this transformation, as well as com-\npute the gradients with respect to the parameters of the\nBN transform. We use chain rule, as follows (before sim-\npli\ufb01cation):\n\u2202\u2113\n\u2202bxi =\n\u2202\u2113\n\u2202yi \u00b7 \u03b3\n\u2202\u2113\n\u2202\u03c32\nB = Pm\ni=1\n\u2202\u2113\n\u2202bxi \u00b7 (xi \u2212\u00b5B) \u00b7 \u22121\n2 (\u03c32\nB + \u01eb)\u22123/2\n\u2202\u2113\n\u2202\u00b5B =\n\u0012 Pm\ni=1\n\u2202\u2113\n\u2202bxi \u00b7\n\u22121\n\u221a\n\u03c32\nB+\u01eb\n\u0013\n+\n\u2202\u2113\n\u2202\u03c32\nB \u00b7\nPm\ni=1 \u22122(xi\u2212\u00b5B)\nm\n\u2202\u2113\n\u2202xi =\n\u2202\u2113\n\u2202bxi \u00b7\n1\n\u221a\n\u03c32\nB+\u01eb +\n\u2202\u2113\n\u2202\u03c32\nB \u00b7 2(xi\u2212\u00b5B)\nm\n+\n\u2202\u2113\n\u2202\u00b5B \u00b7 1\nm\n\u2202\u2113\n\u2202\u03b3 = Pm\ni=1\n\u2202\u2113\n\u2202yi \u00b7 bxi\n\u2202\u2113\n\u2202\u03b2 = Pm\ni=1\n\u2202\u2113\n\u2202yi\nThus, BN transform is a differentiable transformation that\nintroduces normalized activations into the network. This\nensures that as the model is training, layers can continue\nlearning on input distributions that exhibit less internal co-\nvariate shift, thus accelerating the training. Furthermore,\nthe learned af\ufb01ne transform applied to these normalized\nactivations allows the BN transform to represent the iden-\ntity transformation and preserves the network capacity.\n3.1\nTraining and Inference with Batch-\nNormalized Networks\nTo Batch-Normalize a network, we specify a subset of ac-\ntivations and insert the BN transform for each of them,\naccording to Alg. 1. Any layer that previously received\nx as the input, now receives BN(x). A model employing\nBatch Normalization can be trained using batch gradient\ndescent, or Stochastic Gradient Descent with a mini-batch\nsize m > 1, or with any of its variants such as Adagrad\n(Duchi et al., 2011). The normalization of activations that\ndepends on the mini-batch allows ef\ufb01cient training, but is\nneither necessary nor desirable during inference; we want\nthe output to depend only on the input, deterministically.\nFor this, once the network has been trained, we use the\nnormalization\nbx =\nx \u2212E[x]\np\nVar[x] + \u01eb\nusing the population, rather than mini-batch, statistics.\nNeglecting \u01eb, these normalized activations have the same\nmean 0 and variance 1 as during training. We use the un-\nbiased variance estimate Var[x] =\nm\nm\u22121 \u00b7 EB[\u03c32\nB], where\nthe expectation is over training mini-batches of size m and\n\u03c32\nB are their sample variances. Using moving averages in-\nstead, we can track the accuracy of a model as it trains.\nSince the means and variances are \ufb01xed during inference,\nthe normalization is simply a linear transform applied to\neach activation. It may further be composed with the scal-\ning by \u03b3 and shift by \u03b2, to yield a single linear transform\nthat replaces BN(x). Algorithm 2 summarizes the proce-\ndure for training batch-normalized networks.\nInput: Network N with trainable parameters \u0398;\nsubset of activations {x(k)}K\nk=1\nOutput: Batch-normalized network for inference, Ninf\nBN\n1: Ntr\nBN \u2190N\n// Training BN network\n2: for k = 1 . . . K do\n3:\nAdd transformation y(k) = BN\u03b3(k),\u03b2(k)(x(k)) to\nNtr\nBN (Alg. 1)\n4:\nModify each layer in Ntr\nBN with input x(k) to take\ny(k) instead\n5: end for\n6: Train\nNtr\nBN\nto\noptimize\nthe\nparameters\n\u0398 \u222a\n{\u03b3(k), \u03b2(k)}K\nk=1\n7: Ninf\nBN \u2190Ntr\nBN\n// Inference BN network with frozen\n// parameters\n8: for k = 1 . . . K do\n9:\n// For clarity, x \u2261x(k), \u03b3 \u2261\u03b3(k), \u00b5B \u2261\u00b5(k)\nB , etc.\n10:\nProcess multiple training mini-batches B, each of\nsize m, and average over them:\nE[x] \u2190EB[\u00b5B]\nVar[x] \u2190\nm\nm\u22121EB[\u03c32\nB]\n11:\nIn Ninf\nBN, replace the transform y = BN\u03b3,\u03b2(x) with\ny =\n\u03b3\n\u221a\nVar[x]+\u01eb \u00b7 x +\n\u0000\u03b2 \u2212\n\u03b3 E[x]\n\u221a\nVar[x]+\u01eb\n\u0001\n12: end for\nAlgorithm 2: Training a Batch-Normalized Network\n3.2\nBatch-Normalized Convolutional Net-\nworks\nBatch Normalization can be applied to any set of acti-\nvations in the network. Here, we focus on transforms\n4\nthat consist of an af\ufb01ne transformation followed by an\nelement-wise nonlinearity:\nz = g(Wu + b)\nwhere W and b are learned parameters of the model, and\ng(\u00b7) is the nonlinearity such as sigmoid or ReLU. This for-\nmulation covers both fully-connected and convolutional\nlayers. We add the BN transform immediately before the\nnonlinearity, by normalizing x = Wu+b. We could have\nalso normalized the layer inputs u, but since u is likely\nthe output of another nonlinearity, the shape of its distri-\nbution is likely to change during training, and constraining\nits \ufb01rst and second moments would not eliminate the co-\nvariate shift. In contrast, Wu + b is more likely to have\na symmetric, non-sparse distribution, that is \u201cmore Gaus-\nsian\u201d (Hyv\u00a8arinen & Oja, 2000); normalizing it is likely to\nproduce activations with a stable distribution.\nNote that, since we normalize Wu+b, the bias b can be\nignored since its effect will be canceled by the subsequent\nmean subtraction (the role of the bias is subsumed by \u03b2 in\nAlg. 1). Thus, z = g(Wu + b) is replaced with\nz = g(BN(Wu))\nwhere the BN transform is applied independently to each\ndimension of x = Wu, with a separate pair of learned\nparameters \u03b3(k), \u03b2(k) per dimension.\nFor convolutional layers, we additionally want the nor-\nmalization to obey the convolutional property \u2013 so that\ndifferent elements of the same feature map, at different\nlocations, are normalized in the same way. To achieve\nthis, we jointly normalize all the activations in a mini-\nbatch, over all locations. In Alg. 1, we let B be the set of\nall values in a feature map across both the elements of a\nmini-batch and spatial locations \u2013 so for a mini-batch of\nsize m and feature maps of size p \u00d7 q, we use the effec-\ntive mini-batch of size m\u2032 = |B| = m \u00b7 p q. We learn a\npair of parameters \u03b3(k) and \u03b2(k) per feature map, rather\nthan per activation. Alg. 2 is modi\ufb01ed similarly, so that\nduring inference the BN transform applies the same linear\ntransformation to each activation in a given feature map.\n3.3\nBatch Normalization enables higher\nlearning rates\nIn traditional deep networks, too-high learning rate may\nresult in the gradients that explode or vanish, as well as\ngetting stuck in poor local minima.\nBatch Normaliza-\ntion helps address these issues. By normalizing activa-\ntions throughout the network, it prevents small changes\nto the parameters from amplifying into larger and subop-\ntimal changes in activations in gradients; for instance, it\nprevents the training from getting stuck in the saturated\nregimes of nonlinearities.\nBatch Normalization also makes training more resilient\nto the parameter scale. Normally, large learning rates may\nincrease the scale of layer parameters, which then amplify\nthe gradient during backpropagationand lead to the model\nexplosion.\nHowever, with Batch Normalization, back-\npropagation through a layer is unaffected by the scale of\nits parameters. Indeed, for a scalar a,\nBN(Wu) = BN((aW)u)\nand we can show that\n\u2202BN((aW)u)\n\u2202u\n= \u2202BN(Wu)\n\u2202u\n\u2202BN((aW)u)\n\u2202(aW)\n= 1\na \u00b7 \u2202BN(Wu)\n\u2202W\nThe scale does not affect the layer Jacobian nor, con-\nsequently, the gradient propagation.\nMoreover, larger\nweights lead to smaller gradients, and Batch Normaliza-\ntion will stabilize the parameter growth.\nWe further conjecture that Batch Normalization may\nlead the layer Jacobians to have singular values close to 1,\nwhich is known to be bene\ufb01cial for training (Saxe et al.,\n2013). Consider two consecutive layers with normalized\ninputs, and the transformation between these normalized\nvectors: bz = F(bx). If we assume that bx and bz are Gaussian\nand uncorrelated, and that F(bx) \u2248Jbx is a linear transfor-\nmation for the given model parameters, then both bx and bz\nhave unit covariances, and I = Cov[bz] = JCov[bx]JT =\nJJT . Thus, JJT = I, and so all singular values of J\nare equal to 1, which preserves the gradient magnitudes\nduring backpropagation. In reality, the transformation is\nnot linear, and the normalized values are not guaranteed to\nbe Gaussian nor independent, but we nevertheless expect\nBatch Normalization to help make gradient propagation\nbetter behaved. The precise effect of Batch Normaliza-\ntion on gradient propagation remains an area of further\nstudy.\n3.4\nBatch Normalization regularizes the\nmodel\nWhen training with Batch Normalization, a training ex-\nample is seen in conjunction with other examples in the\nmini-batch, and the training network no longer produc-\ning deterministic values for a given training example. In\nour experiments, we found this effect to be advantageous\nto the generalization of the network. Whereas Dropout\n(Srivastava et al., 2014) is typically used to reduce over-\n\ufb01tting, in a batch-normalized network we found that it can\nbe either removed or reduced in strength.\n4\nExperiments\n4.1\nActivations over time\nTo verify the effects of internal covariate shift on train-\ning, and the ability of Batch Normalization to combat it,\nwe considered the problem of predicting the digit class on\nthe MNIST dataset (LeCun et al., 1998a). We used a very\nsimple network, with a 28x28 binary image as input, and\n5\n10K\n20K\n30K\n40K\n50K\n0.7\n0.8\n0.9\n1\n \n \nWithout BN\nWith BN\n\u22122\n0\n2\n\u22122\n0\n2\n(a)\n(b) Without BN\n(c) With BN\nFigure 1: (a) The test accuracy of the MNIST network\ntrained with and without Batch Normalization, vs. the\nnumber of training steps. Batch Normalization helps the\nnetwork train faster and achieve higher accuracy.\n(b,\nc) The evolution of input distributions to a typical sig-\nmoid, over the course of training, shown as {15, 50, 85}th\npercentiles. Batch Normalization makes the distribution\nmore stable and reduces the internal covariate shift.\n3 fully-connected hidden layers with 100 activations each.\nEach hidden layer computes y = g(Wu+b) with sigmoid\nnonlinearity, and the weights W initialized to small ran-\ndom Gaussian values. The last hidden layer is followed\nby a fully-connected layer with 10 activations (one per\nclass) and cross-entropy loss. We trained the network for\n50000 steps, with 60 examples per mini-batch. We added\nBatch Normalization to each hidden layer of the network,\nas in Sec. 3.1. We were interested in the comparison be-\ntween the baseline and batch-normalized networks, rather\nthan achieving the state of the art performance on MNIST\n(which the described architecture does not).\nFigure 1(a) shows the fraction of correct predictions\nby the two networks on held-out test data, as training\nprogresses.\nThe batch-normalized network enjoys the\nhigher test accuracy. To investigate why, we studied in-\nputs to the sigmoid, in the original network N and batch-\nnormalized network Ntr\nBN (Alg. 2) over the course of train-\ning. In Fig. 1(b,c) we show, for one typical activation from\nthe last hidden layer of each network, how its distribu-\ntion evolves. The distributions in the original network\nchange signi\ufb01cantly over time, both in their mean and\nthe variance, which complicates the training of the sub-\nsequent layers. In contrast, the distributions in the batch-\nnormalized network are much more stable as training pro-\ngresses, which aids the training.\n4.2\nImageNet classi\ufb01cation\nWe applied Batch Normalization to a new variant of the\nInception network (Szegedy et al., 2014), trained on the\nImageNet classi\ufb01cation task (Russakovsky et al., 2014).\nThe network has a large number of convolutional and\npooling layers, with a softmax layer to predict the image\nclass, out of 1000 possibilities. Convolutional layers use\nReLU as the nonlinearity. The main difference to the net-\nwork described in (Szegedy et al., 2014) is that the 5 \u00d7 5\nconvolutional layers are replaced by two consecutive lay-\ners of 3 \u00d7 3 convolutions with up to 128 \ufb01lters. The net-\nwork contains 13.6 \u00b7 106 parameters, and, other than the\ntop softmax layer, has no fully-connected layers. More\ndetails are given in the Appendix. We refer to this model\nas Inception in the rest of the text. The model was trained\nusing a version of Stochastic Gradient Descent with mo-\nmentum (Sutskever et al., 2013), using the mini-batch size\nof 32. The training was performed using a large-scale, dis-\ntributed architecture (similar to (Dean et al., 2012)). All\nnetworks are evaluated as training progresses by comput-\ning the validation accuracy @1, i.e.\nthe probability of\npredicting the correct label out of 1000 possibilities, on\na held-out set, using a single crop per image.\nIn our experiments, we evaluated several modi\ufb01cations\nof Inception with Batch Normalization. In all cases, Batch\nNormalization was applied to the input of each nonlinear-\nity, in a convolutional way, as described in section 3.2,\nwhile keeping the rest of the architecture constant.\n4.2.1\nAccelerating BN Networks\nSimply adding Batch Normalization to a network does not\ntake full advantage of our method. To do so, we further\nchanged the network and its training parameters, as fol-\nlows:\nIncrease learning rate. In a batch-normalized model,\nwe have been able to achieve a training speedup from\nhigher learning rates, with no ill side effects (Sec. 3.3).\nRemove Dropout. As described in Sec. 3.4, Batch Nor-\nmalization ful\ufb01lls some of the same goals as Dropout. Re-\nmoving Dropout from Modi\ufb01ed BN-Inception speeds up\ntraining, without increasing over\ufb01tting.\nReduce the L2 weight regularization. While in Incep-\ntion an L2 loss on the model parameters controls over\ufb01t-\nting, in Modi\ufb01ed BN-Inception the weight of this loss is\nreduced by a factor of 5. We \ufb01nd that this improves the\naccuracy on the held-out validation data.\nAccelerate the learning rate decay. In training Incep-\ntion, learning rate was decayed exponentially. Because\nour network trains faster than Inception, we lower the\nlearning rate 6 times faster.\nRemove Local Response Normalization While Incep-\ntion and other networks (Srivastava et al., 2014) bene\ufb01t\nfrom it, we found that with Batch Normalization it is not\nnecessary.\nShuf\ufb02e training examples more thoroughly. We enabled\nwithin-shard shuf\ufb02ing of the training data, which prevents\nthe same examples from always appearing in a mini-batch\ntogether. This led to about 1% improvements in the val-\nidation accuracy, which is consistent with the view of\nBatch Normalization as a regularizer (Sec. 3.4): the ran-\ndomization inherent in our method should be most bene-\n\ufb01cial when it affects an example differently each time it is\nseen.\nReduce the photometric distortions.\nBecause batch-\nnormalized networks train faster and observe each train-\ning example fewer times, we let the trainer focus on more\n\u201creal\u201d images by distorting them less.\n6\n5M\n10M\n15M\n20M\n25M\n30M\n0.4\n0.5\n0.6\n0.7\n0.8\nInception\nBN\u2212Baseline\nBN\u2212x5\nBN\u2212x30\nBN\u2212x5\u2212Sigmoid\nSteps to match Inception\nFigure 2: Single crop validation accuracy of Inception\nand its batch-normalized variants, vs.\nthe number of\ntraining steps.\nModel\nSteps to 72.2%\nMax accuracy\nInception\n31.0 \u00b7 106\n72.2%\nBN-Baseline\n13.3 \u00b7 106\n72.7%\nBN-x5\n2.1 \u00b7 106\n73.0%\nBN-x30\n2.7 \u00b7 106\n74.8%\nBN-x5-Sigmoid\n69.8%\nFigure 3: For Inception and the batch-normalized\nvariants, the number of training steps required to\nreach the maximum accuracy of Inception (72.2%),\nand the maximum accuracy achieved by the net-\nwork.\n4.2.2\nSingle-Network Classi\ufb01cation\nWe evaluated the following networks, all trained on the\nLSVRC2012 training data, and tested on the validation\ndata:\nInception: the network described at the beginning of\nSection 4.2, trained with the initial learning rate of 0.0015.\nBN-Baseline: Same as Inception with Batch Normal-\nization before each nonlinearity.\nBN-x5: Inception with Batch Normalization and the\nmodi\ufb01cations in Sec. 4.2.1. The initial learning rate was\nincreased by a factor of 5, to 0.0075. The same learning\nrate increase with original Inception caused the model pa-\nrameters to reach machine in\ufb01nity.\nBN-x30: Like BN-x5, but with the initial learning rate\n0.045 (30 times that of Inception).\nBN-x5-Sigmoid: Like BN-x5, but with sigmoid non-\nlinearity g(t) =\n1\n1+exp(\u2212x) instead of ReLU. We also at-\ntempted to train the original Inception with sigmoid, but\nthe model remained at the accuracy equivalent to chance.\nIn Figure 2, we show the validation accuracy of the\nnetworks, as a function of the number of training steps.\nInception reached the accuracy of 72.2% after 31 \u00b7 106\ntraining steps. The Figure 3 shows, for each network,\nthe number of training steps required to reach the same\n72.2% accuracy, as well as the maximum validation accu-\nracy reached by the network and the number of steps to\nreach it.\nBy only using Batch Normalization (BN-Baseline), we\nmatch the accuracy of Inception in less than half the num-\nber of training steps. By applying the modi\ufb01cations in\nSec. 4.2.1, we signi\ufb01cantly increase the training speed of\nthe network. BN-x5 needs 14 times fewer steps than In-\nception to reach the 72.2% accuracy. Interestingly, in-\ncreasing the learning rate further (BN-x30) causes the\nmodel to train somewhat slower initially, but allows it to\nreach a higher \ufb01nal accuracy. It reaches 74.8% after 6\u00b7106\nsteps, i.e. 5 times fewer steps than required by Inception\nto reach 72.2%.\nWe also veri\ufb01ed that the reduction in internal covari-\nate shift allows deep networks with Batch Normalization\nto be trained when sigmoid is used as the nonlinearity,\ndespite the well-known dif\ufb01culty of training such net-\nworks. Indeed, BN-x5-Sigmoid achieves the accuracy of\n69.8%. Without Batch Normalization, Inception with sig-\nmoid never achieves better than 1/1000 accuracy.\n4.2.3\nEnsemble Classi\ufb01cation\nThe current reported best results on the ImageNet Large\nScale Visual Recognition Competition are reached by the\nDeep Image ensemble of traditional models (Wu et al.,\n2015) and the ensemble model of (He et al., 2015). The\nlatter reports the top-5 error of 4.94%, as evaluated by the\nILSVRC server. Here we report a top-5 validation error of\n4.9%, and test error of 4.82% (according to the ILSVRC\nserver). This improves upon the previous best result, and\nexceeds the estimated accuracy of human raters according\nto (Russakovsky et al., 2014).\nFor our ensemble, we used 6 networks. Each was based\non BN-x30, modi\ufb01ed via some of the following: increased\ninitial weights in the convolutional layers; using Dropout\n(with the Dropout probability of 5% or 10%, vs. 40%\nfor the original Inception); and using non-convolutional,\nper-activation Batch Normalization with last hidden lay-\ners of the model. Each network achieved its maximum\naccuracy after about 6 \u00b7 106 training steps. The ensemble\nprediction was based on the arithmetic average of class\nprobabilities predicted by the constituent networks. The\ndetails of ensemble and multicrop inference are similar to\n(Szegedy et al., 2014).\nWe demonstrate in Fig. 4 that batch normalization al-\nlows us to set new state-of-the-art by a healthy margin on\nthe ImageNet classi\ufb01cation challenge benchmarks.\n5\nConclusion\nWe have presented a novel mechanism for dramatically\naccelerating the training of deep networks. It is based on\nthe premise that covariate shift, which is known to com-\nplicate the training of machine learning systems, also ap-\n7\nModel\nResolution\nCrops\nModels\nTop-1 error\nTop-5 error\nGoogLeNet ensemble\n224\n144\n7\n-\n6.67%\nDeep Image low-res\n256\n-\n1\n-\n7.96%\nDeep Image high-res\n512\n-\n1\n24.88\n7.42%\nDeep Image ensemble\nvariable\n-\n-\n-\n5.98%\nBN-Inception single crop\n224\n1\n1\n25.2%\n7.82%\nBN-Inception multicrop\n224\n144\n1\n21.99%\n5.82%\nBN-Inception ensemble\n224\n144\n6\n20.1%\n4.9%*\nFigure 4: Batch-Normalized Inception comparison with previous state of the art on the provided validation set com-\nprising 50000 images. *BN-Inception ensemble has reached 4.82% top-5 error on the 100000 images of the test set of\nthe ImageNet as reported by the test server.\nplies to sub-networks and layers, and removing it from\ninternal activations of the network may aid in training.\nOur proposed method draws its power from normalizing\nactivations, and from incorporating this normalization in\nthe network architecture itself. This ensures that the nor-\nmalization is appropriately handled by any optimization\nmethod that is being used to train the network. To en-\nable stochastic optimization methods commonly used in\ndeep network training, we perform the normalization for\neach mini-batch, and backpropagate the gradients through\nthe normalization parameters. Batch Normalization adds\nonly two extra parameters per activation, and in doing so\npreserves the representation ability of the network. We\npresented an algorithm for constructing, training, and per-\nforming inference with batch-normalized networks. The\nresulting networks can be trained with saturating nonlin-\nearities, are more tolerant to increased training rates, and\noften do not require Dropout for regularization.\nMerely adding Batch Normalization to a state-of-the-\nart image classi\ufb01cation model yields a substantial speedup\nin training. By further increasing the learning rates, re-\nmoving Dropout, and applying other modi\ufb01cations af-\nforded by Batch Normalization, we reach the previous\nstate of the art with only a small fraction of training steps\n\u2013 and then beat the state of the art in single-network image\nclassi\ufb01cation. Furthermore, by combining multiple mod-\nels trained with Batch Normalization, we perform better\nthan the best known system on ImageNet, by a signi\ufb01cant\nmargin.\nInterestingly, our method bears similarity to the stan-\ndardization layer of (G\u00a8ulc\u00b8ehre & Bengio, 2013), though\nthe two methods stem from very different goals, and per-\nform different tasks. The goal of Batch Normalization\nis to achieve a stable distribution of activation values\nthroughout training, and in our experiments we apply it\nbefore the nonlinearity since that is where matching the\n\ufb01rst and second moments is more likely to result in a\nstable distribution. On the contrary, (G\u00a8ulc\u00b8ehre & Bengio,\n2013) apply the standardization layer to the output of the\nnonlinearity, which results in sparser activations. In our\nlarge-scale image classi\ufb01cation experiments, we have not\nobserved the nonlinearity inputs to be sparse, neither with\nnor without Batch Normalization. Other notable differ-\nentiating characteristics of Batch Normalization include\nthe learned scale and shift that allow the BN transform\nto represent identity (the standardization layer did not re-\nquire this since it was followed by the learned linear trans-\nform that, conceptually, absorbs the necessary scale and\nshift), handling of convolutional layers, deterministic in-\nference that does not depend on the mini-batch, and batch-\nnormalizing each convolutional layer in the network.\nIn this work, we have not explored the full range of\npossibilities that Batch Normalization potentially enables.\nOur future work includes applications of our method to\nRecurrent Neural Networks (Pascanu et al., 2013), where\nthe internal covariate shift and the vanishing or exploding\ngradients may be especially severe, and which would al-\nlow us to more thoroughly test the hypothesis that normal-\nization improves gradient propagation (Sec. 3.3). We plan\nto investigate whether Batch Normalization can help with\ndomain adaptation, in its traditional sense \u2013 i.e. whether\nthe normalization performed by the network would al-\nlow it to more easily generalize to new data distribu-\ntions, perhaps with just a recomputation of the population\nmeans and variances (Alg. 2). Finally, we believe that fur-\nther theoretical analysis of the algorithm would allow still\nmore improvements and applications.\nReferences\nBengio, Yoshua and Glorot, Xavier. Understanding the\ndif\ufb01culty of training deep feedforward neural networks.\nIn Proceedings of AISTATS 2010, volume 9, pp. 249\u2013\n256, May 2010.\nDean, Jeffrey, Corrado, Greg S., Monga, Rajat, Chen, Kai,\nDevin, Matthieu, Le, Quoc V., Mao, Mark Z., Ranzato,\nMarc\u2019Aurelio, Senior, Andrew, Tucker, Paul, Yang, Ke,\nand Ng, Andrew Y. Large scale distributed deep net-\nworks. In NIPS, 2012.\nDesjardins, Guillaume and Kavukcuoglu, Koray. Natural\nneural networks. (unpublished).\nDuchi, John, Hazan, Elad, and Singer, Yoram. Adaptive\nsubgradient methods for online learning and stochastic\n8\noptimization. J. Mach. Learn. Res., 12:2121\u20132159,July\n2011. ISSN 1532-4435.\nG\u00a8ulc\u00b8ehre, C\u00b8 aglar and Bengio, Yoshua. Knowledge mat-\nters: Importance of prior information for optimization.\nCoRR, abs/1301.4083, 2013.\nHe, K., Zhang, X., Ren, S., and Sun, J. Delving Deep\ninto Recti\ufb01ers: Surpassing Human-Level Performance\non ImageNet Classi\ufb01cation. ArXiv e-prints, February\n2015.\nHyv\u00a8arinen, A. and Oja, E. Independent component anal-\nysis: Algorithms and applications. Neural Netw., 13\n(4-5):411\u2013430, May 2000.\nJiang, Jing. A literature survey on domain adaptation of\nstatistical classi\ufb01ers, 2008.\nLeCun, Y., Bottou, L., Bengio, Y., and Haffner, P.\nGradient-based learning applied to document recog-\nnition. Proceedings of the IEEE, 86(11):2278\u20132324,\nNovember 1998a.\nLeCun, Y., Bottou, L., Orr, G., and Muller, K. Ef\ufb01cient\nbackprop. In Orr, G. and K., Muller (eds.), Neural Net-\nworks: Tricks of the trade. Springer, 1998b.\nLyu, S and Simoncelli, E P. Nonlinear image representa-\ntion using divisive normalization. In Proc. Computer\nVision and Pattern Recognition, pp. 1\u20138. IEEE Com-\nputer Society, Jun 23-28 2008. doi: 10.1109/CVPR.\n2008.4587821.\nNair, Vinod and Hinton, Geoffrey E. Recti\ufb01ed linear units\nimprove restricted boltzmann machines. In ICML, pp.\n807\u2013814. Omnipress, 2010.\nPascanu, Razvan, Mikolov, Tomas, and Bengio, Yoshua.\nOn the dif\ufb01culty of training recurrent neural networks.\nIn Proceedings of the 30th International Conference on\nMachine Learning, ICML 2013, Atlanta, GA, USA, 16-\n21 June 2013, pp. 1310\u20131318, 2013.\nPovey, Daniel, Zhang, Xiaohui, and Khudanpur, San-\njeev.\nParallel training of deep neural networks with\nnatural gradient and parameter averaging.\nCoRR,\nabs/1410.7455, 2014.\nRaiko, Tapani, Valpola, Harri, and LeCun, Yann. Deep\nlearning made easier by linear transformations in per-\nceptrons. In International Conference on Arti\ufb01cial In-\ntelligence and Statistics (AISTATS), pp. 924\u2013932, 2012.\nRussakovsky, Olga, Deng, Jia, Su, Hao, Krause, Jonathan,\nSatheesh, Sanjeev, Ma, Sean, Huang, Zhiheng, Karpa-\nthy, Andrej, Khosla, Aditya, Bernstein, Michael, Berg,\nAlexander C., and Fei-Fei, Li. ImageNet Large Scale\nVisual Recognition Challenge, 2014.\nSaxe, Andrew M., McClelland, James L., and Ganguli,\nSurya.\nExact solutions to the nonlinear dynamics\nof learning in deep linear neural networks.\nCoRR,\nabs/1312.6120, 2013.\nShimodaira, Hidetoshi.\nImproving predictive inference\nunder covariate shift by weighting the log-likelihood\nfunction. Journal of Statistical Planning and Inference,\n90(2):227\u2013244, October 2000.\nSrivastava, Nitish, Hinton, Geoffrey, Krizhevsky, Alex,\nSutskever, Ilya, and Salakhutdinov, Ruslan. Dropout:\nA simple way to prevent neural networks from over\ufb01t-\nting. J. Mach. Learn. Res., 15(1):1929\u20131958, January\n2014.\nSutskever, Ilya, Martens, James, Dahl, George E., and\nHinton, Geoffrey E.\nOn the importance of initial-\nization and momentum in deep learning.\nIn ICML\n(3), volume 28 of JMLR Proceedings, pp. 1139\u20131147.\nJMLR.org, 2013.\nSzegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet,\nPierre, Reed, Scott, Anguelov, Dragomir, Erhan, Du-\nmitru, Vanhoucke, Vincent, and Rabinovich, An-\ndrew.\nGoing deeper with convolutions.\nCoRR,\nabs/1409.4842, 2014.\nWiesler, Simon and Ney, Hermann. A convergence anal-\nysis of log-linear training. In Shawe-Taylor, J., Zemel,\nR.S., Bartlett, P., Pereira, F.C.N., and Weinberger, K.Q.\n(eds.), Advances in Neural Information Processing Sys-\ntems 24, pp. 657\u2013665, Granada, Spain, December 2011.\nWiesler, Simon, Richard, Alexander, Schl\u00a8uter, Ralf, and\nNey, Hermann. Mean-normalized stochastic gradient\nfor large-scale deep learning.\nIn IEEE International\nConference on Acoustics, Speech, and Signal Process-\ning, pp. 180\u2013184, Florence, Italy, May 2014.\nWu, Ren, Yan, Shengen, Shan, Yi, Dang, Qingqing, and\nSun, Gang. Deep image: Scaling up image recognition,\n2015.\nAppendix\nVariant of the Inception Model Used\nFigure 5 documents the changes that were performed\ncompared to\nthe\narchitecture with\nrespect\nto\nthe\nGoogleNet archictecture. For the interpretation of this\ntable, please consult (Szegedy et al., 2014). The notable\narchitecture changes compared to the GoogLeNet model\ninclude:\n\u2022 The 5\u00d75 convolutional layers are replaced by two\nconsecutive 3\u00d73 convolutional layers.\nThis in-\ncreases the maximum depth of the network by 9\n9\nweight layers. Also it increases the number of pa-\nrameters by 25% and the computational cost is in-\ncreased by about 30%.\n\u2022 The number 28\u00d728 inception modules is increased\nfrom 2 to 3.\n\u2022 Inside the modules, sometimes average, sometimes\nmaximum-pooling is employed. This is indicated in\nthe entries corresponding to the pooling layers of the\ntable.\n\u2022 There are no across the board pooling layers be-\ntween any two Inception modules, but stride-2 con-\nvolution/pooling layers are employed before the \ufb01l-\nter concatenation in the modules 3c, 4e.\nOur model employed separable convolution with depth\nmultiplier 8 on the \ufb01rst convolutional layer. This reduces\nthe computational cost while increasing the memory con-\nsumption at training time.\n10\ntype\npatch size/\nstride\noutput\nsize\ndepth\n#1\u00d71\n#3\u00d73\nreduce\n#3\u00d73\ndouble #3\u00d73\nreduce\ndouble\n#3\u00d73\nPool +proj\nconvolution*\n7\u00d77/2\n112\u00d7112\u00d764\n1\nmax pool\n3\u00d73/2\n56\u00d756\u00d764\n0\nconvolution\n3\u00d73/1\n56\u00d756\u00d7192\n1\n64\n192\nmax pool\n3\u00d73/2\n28\u00d728\u00d7192\n0\ninception (3a)\n28\u00d728\u00d7256\n3\n64\n64\n64\n64\n96\navg + 32\ninception (3b)\n28\u00d728\u00d7320\n3\n64\n64\n96\n64\n96\navg + 64\ninception (3c)\nstride 2\n28\u00d728\u00d7576\n3\n0\n128\n160\n64\n96\nmax + pass through\ninception (4a)\n14\u00d714\u00d7576\n3\n224\n64\n96\n96\n128\navg + 128\ninception (4b)\n14\u00d714\u00d7576\n3\n192\n96\n128\n96\n128\navg + 128\ninception (4c)\n14\u00d714\u00d7576\n3\n160\n128\n160\n128\n160\navg + 128\ninception (4d)\n14\u00d714\u00d7576\n3\n96\n128\n192\n160\n192\navg + 128\ninception (4e)\nstride 2\n14\u00d714\u00d71024\n3\n0\n128\n192\n192\n256\nmax + pass through\ninception (5a)\n7\u00d77\u00d71024\n3\n352\n192\n320\n160\n224\navg + 128\ninception (5b)\n7\u00d77\u00d71024\n3\n352\n192\n320\n192\n224\nmax + 128\navg pool\n7\u00d77/1\n1\u00d71\u00d71024\n0\nFigure 5: Inception architecture\n11\n",
        "sentence": "",
        "context": "ceptrons. In International Conference on Arti\ufb01cial In-\ntelligence and Statistics (AISTATS), pp. 924\u2013932, 2012.\nRussakovsky, Olga, Deng, Jia, Su, Hao, Krause, Jonathan,\nSatheesh, Sanjeev, Ma, Sean, Huang, Zhiheng, Karpa-\nIn IEEE International\nConference on Acoustics, Speech, and Signal Process-\ning, pp. 180\u2013184, Florence, Italy, May 2014.\nWu, Ren, Yan, Shengen, Shan, Yi, Dang, Qingqing, and\nSun, Gang. Deep image: Scaling up image recognition,\n2015.\nAppendix\njeev.\nParallel training of deep neural networks with\nnatural gradient and parameter averaging.\nCoRR,\nabs/1410.7455, 2014.\nRaiko, Tapani, Valpola, Harri, and LeCun, Yann. Deep\nlearning made easier by linear transformations in per-"
    },
    {
        "title": "Adam: A method for stochastic optimization",
        "author": [
            "Kingma",
            "Diederik P",
            "Ba",
            "Jimmy"
        ],
        "venue": "CoRR, abs/1412.6980,",
        "citeRegEx": "Kingma et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Kingma et al\\.",
        "year": 2014,
        "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization\nof stochastic objective functions, based on adaptive estimates of lower-order\nmoments. The method is straightforward to implement, is computationally\nefficient, has little memory requirements, is invariant to diagonal rescaling\nof the gradients, and is well suited for problems that are large in terms of\ndata and/or parameters. The method is also appropriate for non-stationary\nobjectives and problems with very noisy and/or sparse gradients. The\nhyper-parameters have intuitive interpretations and typically require little\ntuning. Some connections to related algorithms, on which Adam was inspired, are\ndiscussed. We also analyze the theoretical convergence properties of the\nalgorithm and provide a regret bound on the convergence rate that is comparable\nto the best known results under the online convex optimization framework.\nEmpirical results demonstrate that Adam works well in practice and compares\nfavorably to other stochastic optimization methods. Finally, we discuss AdaMax,\na variant of Adam based on the infinity norm.",
        "full_text": "Published as a conference paper at ICLR 2015\nADAM: A METHOD FOR STOCHASTIC OPTIMIZATION\nDiederik P. Kingma*\nUniversity of Amsterdam, OpenAI\ndpkingma@openai.com\nJimmy Lei Ba\u2217\nUniversity of Toronto\njimmy@psi.utoronto.ca\nABSTRACT\nWe introduce Adam, an algorithm for \ufb01rst-order gradient-based optimization of\nstochastic objective functions, based on adaptive estimates of lower-order mo-\nments. The method is straightforward to implement, is computationally ef\ufb01cient,\nhas little memory requirements, is invariant to diagonal rescaling of the gradients,\nand is well suited for problems that are large in terms of data and/or parameters.\nThe method is also appropriate for non-stationary objectives and problems with\nvery noisy and/or sparse gradients. The hyper-parameters have intuitive interpre-\ntations and typically require little tuning. Some connections to related algorithms,\non which Adam was inspired, are discussed. We also analyze the theoretical con-\nvergence properties of the algorithm and provide a regret bound on the conver-\ngence rate that is comparable to the best known results under the online convex\noptimization framework. Empirical results demonstrate that Adam works well in\npractice and compares favorably to other stochastic optimization methods. Finally,\nwe discuss AdaMax, a variant of Adam based on the in\ufb01nity norm.\n1\nINTRODUCTION\nStochastic gradient-based optimization is of core practical importance in many \ufb01elds of science and\nengineering. Many problems in these \ufb01elds can be cast as the optimization of some scalar parameter-\nized objective function requiring maximization or minimization with respect to its parameters. If the\nfunction is differentiable w.r.t. its parameters, gradient descent is a relatively ef\ufb01cient optimization\nmethod, since the computation of \ufb01rst-order partial derivatives w.r.t. all the parameters is of the same\ncomputational complexity as just evaluating the function. Often, objective functions are stochastic.\nFor example, many objective functions are composed of a sum of subfunctions evaluated at different\nsubsamples of data; in this case optimization can be made more ef\ufb01cient by taking gradient steps\nw.r.t. individual subfunctions, i.e. stochastic gradient descent (SGD) or ascent. SGD proved itself\nas an ef\ufb01cient and effective optimization method that was central in many machine learning success\nstories, such as recent advances in deep learning (Deng et al., 2013; Krizhevsky et al., 2012; Hinton\n& Salakhutdinov, 2006; Hinton et al., 2012a; Graves et al., 2013). Objectives may also have other\nsources of noise than data subsampling, such as dropout (Hinton et al., 2012b) regularization. For\nall such noisy objectives, ef\ufb01cient stochastic optimization techniques are required. The focus of this\npaper is on the optimization of stochastic objectives with high-dimensional parameters spaces. In\nthese cases, higher-order optimization methods are ill-suited, and discussion in this paper will be\nrestricted to \ufb01rst-order methods.\nWe propose Adam, a method for ef\ufb01cient stochastic optimization that only requires \ufb01rst-order gra-\ndients with little memory requirement. The method computes individual adaptive learning rates for\ndifferent parameters from estimates of \ufb01rst and second moments of the gradients; the name Adam\nis derived from adaptive moment estimation. Our method is designed to combine the advantages\nof two recently popular methods: AdaGrad (Duchi et al., 2011), which works well with sparse gra-\ndients, and RMSProp (Tieleman & Hinton, 2012), which works well in on-line and non-stationary\nsettings; important connections to these and other stochastic optimization methods are clari\ufb01ed in\nsection 5. Some of Adam\u2019s advantages are that the magnitudes of parameter updates are invariant to\nrescaling of the gradient, its stepsizes are approximately bounded by the stepsize hyperparameter,\nit does not require a stationary objective, it works with sparse gradients, and it naturally performs a\nform of step size annealing.\n\u2217Equal contribution. Author ordering determined by coin \ufb02ip over a Google Hangout.\n1\narXiv:1412.6980v9  [cs.LG]  30 Jan 2017\nPublished as a conference paper at ICLR 2015\nAlgorithm 1: Adam, our proposed algorithm for stochastic optimization. See section 2 for details,\nand for a slightly more ef\ufb01cient (but less clear) order of computation. g2\nt indicates the elementwise\nsquare gt \u2299gt. Good default settings for the tested machine learning problems are \u03b1 = 0.001,\n\u03b21 = 0.9, \u03b22 = 0.999 and \u03f5 = 10\u22128. All operations on vectors are element-wise. With \u03b2t\n1 and \u03b2t\n2\nwe denote \u03b21 and \u03b22 to the power t.\nRequire: \u03b1: Stepsize\nRequire: \u03b21, \u03b22 \u2208[0, 1): Exponential decay rates for the moment estimates\nRequire: f(\u03b8): Stochastic objective function with parameters \u03b8\nRequire: \u03b80: Initial parameter vector\nm0 \u21900 (Initialize 1st moment vector)\nv0 \u21900 (Initialize 2nd moment vector)\nt \u21900 (Initialize timestep)\nwhile \u03b8t not converged do\nt \u2190t + 1\ngt \u2190\u2207\u03b8ft(\u03b8t\u22121) (Get gradients w.r.t. stochastic objective at timestep t)\nmt \u2190\u03b21 \u00b7 mt\u22121 + (1 \u2212\u03b21) \u00b7 gt (Update biased \ufb01rst moment estimate)\nvt \u2190\u03b22 \u00b7 vt\u22121 + (1 \u2212\u03b22) \u00b7 g2\nt (Update biased second raw moment estimate)\nbmt \u2190mt/(1 \u2212\u03b2t\n1) (Compute bias-corrected \ufb01rst moment estimate)\nbvt \u2190vt/(1 \u2212\u03b2t\n2) (Compute bias-corrected second raw moment estimate)\n\u03b8t \u2190\u03b8t\u22121 \u2212\u03b1 \u00b7 bmt/(\u221abvt + \u03f5) (Update parameters)\nend while\nreturn \u03b8t (Resulting parameters)\nIn section 2 we describe the algorithm and the properties of its update rule. Section 3 explains\nour initialization bias correction technique, and section 4 provides a theoretical analysis of Adam\u2019s\nconvergence in online convex programming. Empirically, our method consistently outperforms other\nmethods for a variety of models and datasets, as shown in section 6. Overall, we show that Adam is\na versatile algorithm that scales to large-scale high-dimensional machine learning problems.\n2\nALGORITHM\nSee algorithm 1 for pseudo-code of our proposed algorithm Adam. Let f(\u03b8) be a noisy objec-\ntive function: a stochastic scalar function that is differentiable w.r.t. parameters \u03b8. We are in-\nterested in minimizing the expected value of this function, E[f(\u03b8)] w.r.t. its parameters \u03b8. With\nf1(\u03b8), ..., , fT (\u03b8) we denote the realisations of the stochastic function at subsequent timesteps\n1, ..., T. The stochasticity might come from the evaluation at random subsamples (minibatches)\nof datapoints, or arise from inherent function noise. With gt = \u2207\u03b8ft(\u03b8) we denote the gradient, i.e.\nthe vector of partial derivatives of ft, w.r.t \u03b8 evaluated at timestep t.\nThe algorithm updates exponential moving averages of the gradient (mt) and the squared gradient\n(vt) where the hyper-parameters \u03b21, \u03b22 \u2208[0, 1) control the exponential decay rates of these moving\naverages. The moving averages themselves are estimates of the 1st moment (the mean) and the\n2nd raw moment (the uncentered variance) of the gradient. However, these moving averages are\ninitialized as (vectors of) 0\u2019s, leading to moment estimates that are biased towards zero, especially\nduring the initial timesteps, and especially when the decay rates are small (i.e. the \u03b2s are close to 1).\nThe good news is that this initialization bias can be easily counteracted, resulting in bias-corrected\nestimates bmt and bvt. See section 3 for more details.\nNote that the ef\ufb01ciency of algorithm 1 can, at the expense of clarity, be improved upon by changing\nthe order of computation, e.g. by replacing the last three lines in the loop with the following lines:\n\u03b1t = \u03b1 \u00b7\np\n1 \u2212\u03b2t\n2/(1 \u2212\u03b2t\n1) and \u03b8t \u2190\u03b8t\u22121 \u2212\u03b1t \u00b7 mt/(\u221avt + \u02c6\u03f5).\n2.1\nADAM\u2019S UPDATE RULE\nAn important property of Adam\u2019s update rule is its careful choice of stepsizes. Assuming \u03f5 = 0, the\neffective step taken in parameter space at timestep t is \u2206t = \u03b1 \u00b7 bmt/\u221abvt. The effective stepsize has\ntwo upper bounds: |\u2206t| \u2264\u03b1 \u00b7 (1 \u2212\u03b21)/\u221a1 \u2212\u03b22 in the case (1 \u2212\u03b21) > \u221a1 \u2212\u03b22, and |\u2206t| \u2264\u03b1\n2\nPublished as a conference paper at ICLR 2015\notherwise. The \ufb01rst case only happens in the most severe case of sparsity: when a gradient has\nbeen zero at all timesteps except at the current timestep. For less sparse cases, the effective stepsize\nwill be smaller. When (1 \u2212\u03b21) = \u221a1 \u2212\u03b22 we have that | bmt/\u221abvt| < 1 therefore |\u2206t| < \u03b1. In\nmore common scenarios, we will have that bmt/\u221abvt \u2248\u00b11 since |E[g]/\np\nE[g2]| \u22641. The effective\nmagnitude of the steps taken in parameter space at each timestep are approximately bounded by\nthe stepsize setting \u03b1, i.e., |\u2206t| \u2a85\u03b1. This can be understood as establishing a trust region around\nthe current parameter value, beyond which the current gradient estimate does not provide suf\ufb01cient\ninformation. This typically makes it relatively easy to know the right scale of \u03b1 in advance. For\nmany machine learning models, for instance, we often know in advance that good optima are with\nhigh probability within some set region in parameter space; it is not uncommon, for example, to\nhave a prior distribution over the parameters. Since \u03b1 sets (an upper bound of) the magnitude of\nsteps in parameter space, we can often deduce the right order of magnitude of \u03b1 such that optima\ncan be reached from \u03b80 within some number of iterations. With a slight abuse of terminology,\nwe will call the ratio bmt/\u221abvt the signal-to-noise ratio (SNR). With a smaller SNR the effective\nstepsize \u2206t will be closer to zero. This is a desirable property, since a smaller SNR means that\nthere is greater uncertainty about whether the direction of bmt corresponds to the direction of the true\ngradient. For example, the SNR value typically becomes closer to 0 towards an optimum, leading\nto smaller effective steps in parameter space: a form of automatic annealing. The effective stepsize\n\u2206t is also invariant to the scale of the gradients; rescaling the gradients g with factor c will scale bmt\nwith a factor c and bvt with a factor c2, which cancel out: (c \u00b7 bmt)/(\u221a\nc2 \u00b7 bvt) = bmt/\u221abvt.\n3\nINITIALIZATION BIAS CORRECTION\nAs explained in section 2, Adam utilizes initialization bias correction terms. We will here derive\nthe term for the second moment estimate; the derivation for the \ufb01rst moment estimate is completely\nanalogous. Let g be the gradient of the stochastic objective f, and we wish to estimate its second\nraw moment (uncentered variance) using an exponential moving average of the squared gradient,\nwith decay rate \u03b22. Let g1, ..., gT be the gradients at subsequent timesteps, each a draw from an\nunderlying gradient distribution gt \u223cp(gt). Let us initialize the exponential moving average as\nv0 = 0 (a vector of zeros). First note that the update at timestep t of the exponential moving average\nvt = \u03b22 \u00b7 vt\u22121 + (1 \u2212\u03b22) \u00b7 g2\nt (where g2\nt indicates the elementwise square gt \u2299gt) can be written as\na function of the gradients at all previous timesteps:\nvt = (1 \u2212\u03b22)\nt\nX\ni=1\n\u03b2t\u2212i\n2\n\u00b7 g2\ni\n(1)\nWe wish to know how E[vt], the expected value of the exponential moving average at timestep t,\nrelates to the true second moment E[g2\nt ], so we can correct for the discrepancy between the two.\nTaking expectations of the left-hand and right-hand sides of eq. (1):\nE[vt] = E\n\"\n(1 \u2212\u03b22)\nt\nX\ni=1\n\u03b2t\u2212i\n2\n\u00b7 g2\ni\n#\n(2)\n= E[g2\nt ] \u00b7 (1 \u2212\u03b22)\nt\nX\ni=1\n\u03b2t\u2212i\n2\n+ \u03b6\n(3)\n= E[g2\nt ] \u00b7 (1 \u2212\u03b2t\n2) + \u03b6\n(4)\nwhere \u03b6 = 0 if the true second moment E[g2\ni ] is stationary; otherwise \u03b6 can be kept small since\nthe exponential decay rate \u03b21 can (and should) be chosen such that the exponential moving average\nassigns small weights to gradients too far in the past. What is left is the term (1 \u2212\u03b2t\n2) which is\ncaused by initializing the running average with zeros. In algorithm 1 we therefore divide by this\nterm to correct the initialization bias.\nIn case of sparse gradients, for a reliable estimate of the second moment one needs to average over\nmany gradients by chosing a small value of \u03b22; however it is exactly this case of small \u03b22 where a\nlack of initialisation bias correction would lead to initial steps that are much larger.\n3\nPublished as a conference paper at ICLR 2015\n4\nCONVERGENCE ANALYSIS\nWe analyze the convergence of Adam using the online learning framework proposed in (Zinkevich,\n2003). Given an arbitrary, unknown sequence of convex cost functions f1(\u03b8), f2(\u03b8),..., fT (\u03b8). At\neach time t, our goal is to predict the parameter \u03b8t and evaluate it on a previously unknown cost\nfunction ft. Since the nature of the sequence is unknown in advance, we evaluate our algorithm\nusing the regret, that is the sum of all the previous difference between the online prediction ft(\u03b8t)\nand the best \ufb01xed point parameter ft(\u03b8\u2217) from a feasible set X for all the previous steps. Concretely,\nthe regret is de\ufb01ned as:\nR(T) =\nT\nX\nt=1\n[ft(\u03b8t) \u2212ft(\u03b8\u2217)]\n(5)\nwhere \u03b8\u2217= arg min\u03b8\u2208X\nPT\nt=1 ft(\u03b8). We show Adam has O(\n\u221a\nT) regret bound and a proof is given\nin the appendix. Our result is comparable to the best known bound for this general convex online\nlearning problem. We also use some de\ufb01nitions simplify our notation, where gt \u225c\u2207ft(\u03b8t) and gt,i\nas the ith element. We de\ufb01ne g1:t,i \u2208Rt as a vector that contains the ith dimension of the gradients\nover all iterations till t, g1:t,i = [g1,i, g2,i, \u00b7 \u00b7 \u00b7 , gt,i]. Also, we de\ufb01ne \u03b3 \u225c\n\u03b22\n1\n\u221a\u03b22 . Our following\ntheorem holds when the learning rate \u03b1t is decaying at a rate of t\u22121\n2 and \ufb01rst moment running\naverage coef\ufb01cient \u03b21,t decay exponentially with \u03bb, that is typically close to 1, e.g. 1 \u221210\u22128.\nTheorem 4.1. Assume that the function ft has bounded gradients, \u2225\u2207ft(\u03b8)\u22252 \u2264G, \u2225\u2207ft(\u03b8)\u2225\u221e\u2264\nG\u221efor all \u03b8 \u2208Rd and distance between any \u03b8t generated by Adam is bounded, \u2225\u03b8n \u2212\u03b8m\u22252 \u2264D,\n\u2225\u03b8m \u2212\u03b8n\u2225\u221e\u2264D\u221efor any m, n \u2208{1, ..., T}, and \u03b21, \u03b22 \u2208[0, 1) satisfy\n\u03b22\n1\n\u221a\u03b22 < 1. Let \u03b1t =\n\u03b1\n\u221a\nt\nand \u03b21,t = \u03b21\u03bbt\u22121, \u03bb \u2208(0, 1). Adam achieves the following guarantee, for all T \u22651.\nR(T) \u2264\nD2\n2\u03b1(1 \u2212\u03b21)\nd\nX\ni=1\np\nTbvT,i+\n\u03b1(1 + \u03b21)G\u221e\n(1 \u2212\u03b21)\u221a1 \u2212\u03b22(1 \u2212\u03b3)2\nd\nX\ni=1\n\u2225g1:T,i\u22252+\nd\nX\ni=1\nD2\n\u221eG\u221e\n\u221a1 \u2212\u03b22\n2\u03b1(1 \u2212\u03b21)(1 \u2212\u03bb)2\nOur Theorem 4.1 implies when the data features are sparse and bounded gradients, the sum-\nmation term can be much smaller than its upper bound Pd\ni=1 \u2225g1:T,i\u22252\n<< dG\u221e\n\u221a\nT and\nPd\ni=1\np\nTbvT,i << dG\u221e\n\u221a\nT, in particular if the class of function and data features are in the form of\nsection 1.2 in (Duchi et al., 2011). Their results for the expected value E[Pd\ni=1 \u2225g1:T,i\u22252] also apply\nto Adam. In particular, the adaptive method, such as Adam and Adagrad, can achieve O(log d\n\u221a\nT),\nan improvement over O(\n\u221a\ndT) for the non-adaptive method. Decaying \u03b21,t towards zero is impor-\ntant in our theoretical analysis and also matches previous empirical \ufb01ndings, e.g. (Sutskever et al.,\n2013) suggests reducing the momentum coef\ufb01cient in the end of training can improve convergence.\nFinally, we can show the average regret of Adam converges,\nCorollary 4.2. Assume that the function ft has bounded gradients, \u2225\u2207ft(\u03b8)\u22252 \u2264G, \u2225\u2207ft(\u03b8)\u2225\u221e\u2264\nG\u221efor all \u03b8 \u2208Rd and distance between any \u03b8t generated by Adam is bounded, \u2225\u03b8n \u2212\u03b8m\u22252 \u2264D,\n\u2225\u03b8m \u2212\u03b8n\u2225\u221e\u2264D\u221efor any m, n \u2208{1, ..., T}. Adam achieves the following guarantee, for all\nT \u22651.\nR(T)\nT\n= O( 1\n\u221a\nT\n)\nThis result can be obtained by using Theorem 4.1 and Pd\ni=1 \u2225g1:T,i\u22252 \u2264dG\u221e\n\u221a\nT.\nThus,\nlimT \u2192\u221e\nR(T )\nT\n= 0.\n5\nRELATED WORK\nOptimization methods bearing a direct relation to Adam are RMSProp (Tieleman & Hinton, 2012;\nGraves, 2013) and AdaGrad (Duchi et al., 2011); these relationships are discussed below. Other\nstochastic optimization methods include vSGD (Schaul et al., 2012), AdaDelta (Zeiler, 2012) and the\nnatural Newton method from Roux & Fitzgibbon (2010), all setting stepsizes by estimating curvature\n4\nPublished as a conference paper at ICLR 2015\nfrom \ufb01rst-order information. The Sum-of-Functions Optimizer (SFO) (Sohl-Dickstein et al., 2014)\nis a quasi-Newton method based on minibatches, but (unlike Adam) has memory requirements linear\nin the number of minibatch partitions of a dataset, which is often infeasible on memory-constrained\nsystems such as a GPU. Like natural gradient descent (NGD) (Amari, 1998), Adam employs a\npreconditioner that adapts to the geometry of the data, since bvt is an approximation to the diagonal\nof the Fisher information matrix (Pascanu & Bengio, 2013); however, Adam\u2019s preconditioner (like\nAdaGrad\u2019s) is more conservative in its adaption than vanilla NGD by preconditioning with the square\nroot of the inverse of the diagonal Fisher information matrix approximation.\nRMSProp:\nAn optimization method closely related to Adam is RMSProp (Tieleman & Hinton,\n2012). A version with momentum has sometimes been used (Graves, 2013). There are a few impor-\ntant differences between RMSProp with momentum and Adam: RMSProp with momentum gener-\nates its parameter updates using a momentum on the rescaled gradient, whereas Adam updates are\ndirectly estimated using a running average of \ufb01rst and second moment of the gradient. RMSProp\nalso lacks a bias-correction term; this matters most in case of a value of \u03b22 close to 1 (required in\ncase of sparse gradients), since in that case not correcting the bias leads to very large stepsizes and\noften divergence, as we also empirically demonstrate in section 6.4.\nAdaGrad:\nAn algorithm that works well for sparse gradients is AdaGrad (Duchi et al., 2011). Its\nbasic version updates parameters as \u03b8t+1 = \u03b8t \u2212\u03b1 \u00b7 gt/\nqPt\ni=1 g2\nt . Note that if we choose \u03b22 to be\nin\ufb01nitesimally close to 1 from below, then lim\u03b22\u21921 bvt = t\u22121 \u00b7 Pt\ni=1 g2\nt . AdaGrad corresponds to a\nversion of Adam with \u03b21 = 0, in\ufb01nitesimal (1 \u2212\u03b22) and a replacement of \u03b1 by an annealed version\n\u03b1t = \u03b1 \u00b7 t\u22121/2, namely \u03b8t \u2212\u03b1 \u00b7 t\u22121/2 \u00b7 bmt/\np\nlim\u03b22\u21921 bvt = \u03b8t \u2212\u03b1 \u00b7 t\u22121/2 \u00b7 gt/\nq\nt\u22121 \u00b7 Pt\ni=1 g2\nt =\n\u03b8t \u2212\u03b1 \u00b7 gt/\nqPt\ni=1 g2\nt . Note that this direct correspondence between Adam and Adagrad does\nnot hold when removing the bias-correction terms; without bias correction, like in RMSProp, a \u03b22\nin\ufb01nitesimally close to 1 would lead to in\ufb01nitely large bias, and in\ufb01nitely large parameter updates.\n6\nEXPERIMENTS\nTo empirically evaluate the proposed method, we investigated different popular machine learning\nmodels, including logistic regression, multilayer fully connected neural networks and deep convolu-\ntional neural networks. Using large models and datasets, we demonstrate Adam can ef\ufb01ciently solve\npractical deep learning problems.\nWe use the same parameter initialization when comparing different optimization algorithms. The\nhyper-parameters, such as learning rate and momentum, are searched over a dense grid and the\nresults are reported using the best hyper-parameter setting.\n6.1\nEXPERIMENT: LOGISTIC REGRESSION\nWe evaluate our proposed method on L2-regularized multi-class logistic regression using the MNIST\ndataset. Logistic regression has a well-studied convex objective, making it suitable for comparison\nof different optimizers without worrying about local minimum issues. The stepsize \u03b1 in our logistic\nregression experiments is adjusted by 1/\n\u221a\nt decay, namely \u03b1t =\n\u03b1\n\u221a\nt that matches with our theorat-\nical prediction from section 4. The logistic regression classi\ufb01es the class label directly on the 784\ndimension image vectors. We compare Adam to accelerated SGD with Nesterov momentum and\nAdagrad using minibatch size of 128. According to Figure 1, we found that the Adam yields similar\nconvergence as SGD with momentum and both converge faster than Adagrad.\nAs discussed in (Duchi et al., 2011), Adagrad can ef\ufb01ciently deal with sparse features and gradi-\nents as one of its main theoretical results whereas SGD is low at learning rare features. Adam with\n1/\n\u221a\nt decay on its stepsize should theoratically match the performance of Adagrad. We examine the\nsparse feature problem using IMDB movie review dataset from (Maas et al., 2011). We pre-process\nthe IMDB movie reviews into bag-of-words (BoW) feature vectors including the \ufb01rst 10,000 most\nfrequent words. The 10,000 dimension BoW feature vector for each review is highly sparse. As sug-\ngested in (Wang & Manning, 2013), 50% dropout noise can be applied to the BoW features during\n5\nPublished as a conference paper at ICLR 2015\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\niterations over entire dataset\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\ntraining cost\nMNIST Logistic Regression\nAdaGrad\nSGDNesterov\nAdam\n0\n20\n40\n60\n80\n100\n120\n140\n160\niterations over entire dataset\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\ntraining cost\nIMDB BoW feature Logistic Regression\nAdagrad+dropout\nRMSProp+dropout\nSGDNesterov+dropout\nAdam+dropout\nFigure 1: Logistic regression training negative log likelihood on MNIST images and IMDB movie\nreviews with 10,000 bag-of-words (BoW) feature vectors.\ntraining to prevent over-\ufb01tting. In \ufb01gure 1, Adagrad outperforms SGD with Nesterov momentum\nby a large margin both with and without dropout noise. Adam converges as fast as Adagrad. The\nempirical performance of Adam is consistent with our theoretical \ufb01ndings in sections 2 and 4. Sim-\nilar to Adagrad, Adam can take advantage of sparse features and obtain faster convergence rate than\nnormal SGD with momentum.\n6.2\nEXPERIMENT: MULTI-LAYER NEURAL NETWORKS\nMulti-layer neural network are powerful models with non-convex objective functions. Although\nour convergence analysis does not apply to non-convex problems, we empirically found that Adam\noften outperforms other methods in such cases. In our experiments, we made model choices that are\nconsistent with previous publications in the area; a neural network model with two fully connected\nhidden layers with 1000 hidden units each and ReLU activation are used for this experiment with\nminibatch size of 128.\nFirst, we study different optimizers using the standard deterministic cross-entropy objective func-\ntion with L2 weight decay on the parameters to prevent over-\ufb01tting. The sum-of-functions (SFO)\nmethod (Sohl-Dickstein et al., 2014) is a recently proposed quasi-Newton method that works with\nminibatches of data and has shown good performance on optimization of multi-layer neural net-\nworks. We used their implementation and compared with Adam to train such models. Figure 2\nshows that Adam makes faster progress in terms of both the number of iterations and wall-clock\ntime. Due to the cost of updating curvature information, SFO is 5-10x slower per iteration com-\npared to Adam, and has a memory requirement that is linear in the number minibatches.\nStochastic regularization methods, such as dropout, are an effective way to prevent over-\ufb01tting and\noften used in practice due to their simplicity. SFO assumes deterministic subfunctions, and indeed\nfailed to converge on cost functions with stochastic regularization. We compare the effectiveness of\nAdam to other stochastic \ufb01rst order methods on multi-layer neural networks trained with dropout\nnoise. Figure 2 shows our results; Adam shows better convergence than other methods.\n6.3\nEXPERIMENT: CONVOLUTIONAL NEURAL NETWORKS\nConvolutional neural networks (CNNs) with several layers of convolution, pooling and non-linear\nunits have shown considerable success in computer vision tasks. Unlike most fully connected neural\nnets, weight sharing in CNNs results in vastly different gradients in different layers. A smaller\nlearning rate for the convolution layers is often used in practice when applying SGD. We show the\neffectiveness of Adam in deep CNNs. Our CNN architecture has three alternating stages of 5x5\nconvolution \ufb01lters and 3x3 max pooling with stride of 2 that are followed by a fully connected layer\nof 1000 recti\ufb01ed linear hidden units (ReLU\u2019s). The input image are pre-processed by whitening, and\n6\nPublished as a conference paper at ICLR 2015\n0\n50\n100\n150\n200\niterations over entire dataset\n10\n-2\n10\n-1\ntraining cost\nMNIST Multilayer Neural Network + dropout\nAdaGrad\nRMSProp\nSGDNesterov\nAdaDelta\nAdam\n(a)\n(b)\nFigure 2: Training of multilayer neural networks on MNIST images. (a) Neural networks using\ndropout stochastic regularization. (b) Neural networks with deterministic cost function. We compare\nwith the sum-of-functions (SFO) optimizer (Sohl-Dickstein et al., 2014)\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\niterations over entire dataset\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\ntraining cost\nCIFAR10 ConvNet First 3 Epoches\nAdaGrad\nAdaGrad+dropout\nSGDNesterov\nSGDNesterov+dropout\nAdam\nAdam+dropout\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\niterations over entire dataset\n10-4\n10-3\n10-2\n10-1\n100\n101\n102\ntraining cost\nCIFAR10 ConvNet\nAdaGrad\nAdaGrad+dropout\nSGDNesterov\nSGDNesterov+dropout\nAdam\nAdam+dropout\nFigure 3: Convolutional neural networks training cost. (left) Training cost for the \ufb01rst three epochs.\n(right) Training cost over 45 epochs. CIFAR-10 with c64-c64-c128-1000 architecture.\ndropout noise is applied to the input layer and fully connected layer. The minibatch size is also set\nto 128 similar to previous experiments.\nInterestingly, although both Adam and Adagrad make rapid progress lowering the cost in the initial\nstage of the training, shown in Figure 3 (left), Adam and SGD eventually converge considerably\nfaster than Adagrad for CNNs shown in Figure 3 (right). We notice the second moment estimate bvt\nvanishes to zeros after a few epochs and is dominated by the \u03f5 in algorithm 1. The second moment\nestimate is therefore a poor approximation to the geometry of the cost function in CNNs comparing\nto fully connected network from Section 6.2. Whereas, reducing the minibatch variance through\nthe \ufb01rst moment is more important in CNNs and contributes to the speed-up. As a result, Adagrad\nconverges much slower than others in this particular experiment. Though Adam shows marginal\nimprovement over SGD with momentum, it adapts learning rate scale for different layers instead of\nhand picking manually as in SGD.\n7\nPublished as a conference paper at ICLR 2015\n\u03b21=0\n\u03b21=0.9\n\u03b22=0.99\n\u03b22=0.999\n\u03b22=0.9999\n\u03b22=0.99\n\u03b22=0.999\n\u03b22=0.9999\n(a) after 10 epochs\n(b) after 100 epochs\nlog10(\u03b1)\nLoss\nFigure 4: Effect of bias-correction terms (red line) versus no bias correction terms (green line)\nafter 10 epochs (left) and 100 epochs (right) on the loss (y-axes) when learning a Variational Auto-\nEncoder (VAE) (Kingma & Welling, 2013), for different settings of stepsize \u03b1 (x-axes) and hyper-\nparameters \u03b21 and \u03b22.\n6.4\nEXPERIMENT: BIAS-CORRECTION TERM\nWe also empirically evaluate the effect of the bias correction terms explained in sections 2 and 3.\nDiscussed in section 5, removal of the bias correction terms results in a version of RMSProp (Tiele-\nman & Hinton, 2012) with momentum. We vary the \u03b21 and \u03b22 when training a variational auto-\nencoder (VAE) with the same architecture as in (Kingma & Welling, 2013) with a single hidden\nlayer with 500 hidden units with softplus nonlinearities and a 50-dimensional spherical Gaussian\nlatent variable. We iterated over a broad range of hyper-parameter choices, i.e. \u03b21 \u2208[0, 0.9] and\n\u03b22 \u2208[0.99, 0.999, 0.9999], and log10(\u03b1) \u2208[\u22125, ..., \u22121]. Values of \u03b22 close to 1, required for robust-\nness to sparse gradients, results in larger initialization bias; therefore we expect the bias correction\nterm is important in such cases of slow decay, preventing an adverse effect on optimization.\nIn Figure 4, values \u03b22 close to 1 indeed lead to instabilities in training when no bias correction term\nwas present, especially at \ufb01rst few epochs of the training. The best results were achieved with small\nvalues of (1\u2212\u03b22) and bias correction; this was more apparent towards the end of optimization when\ngradients tends to become sparser as hidden units specialize to speci\ufb01c patterns. In summary, Adam\nperformed equal or better than RMSProp, regardless of hyper-parameter setting.\n7\nEXTENSIONS\n7.1\nADAMAX\nIn Adam, the update rule for individual weights is to scale their gradients inversely proportional to a\n(scaled) L2 norm of their individual current and past gradients. We can generalize the L2 norm based\nupdate rule to a Lp norm based update rule. Such variants become numerically unstable for large\np. However, in the special case where we let p \u2192\u221e, a surprisingly simple and stable algorithm\nemerges; see algorithm 2. We\u2019ll now derive the algorithm. Let, in case of the Lp norm, the stepsize\nat time t be inversely proportional to v1/p\nt\n, where:\nvt = \u03b2p\n2vt\u22121 + (1 \u2212\u03b2p\n2)|gt|p\n(6)\n= (1 \u2212\u03b2p\n2)\nt\nX\ni=1\n\u03b2p(t\u2212i)\n2\n\u00b7 |gi|p\n(7)\n8\nPublished as a conference paper at ICLR 2015\nAlgorithm 2: AdaMax, a variant of Adam based on the in\ufb01nity norm. See section 7.1 for details.\nGood default settings for the tested machine learning problems are \u03b1 = 0.002, \u03b21 = 0.9 and\n\u03b22 = 0.999. With \u03b2t\n1 we denote \u03b21 to the power t. Here, (\u03b1/(1 \u2212\u03b2t\n1)) is the learning rate with the\nbias-correction term for the \ufb01rst moment. All operations on vectors are element-wise.\nRequire: \u03b1: Stepsize\nRequire: \u03b21, \u03b22 \u2208[0, 1): Exponential decay rates\nRequire: f(\u03b8): Stochastic objective function with parameters \u03b8\nRequire: \u03b80: Initial parameter vector\nm0 \u21900 (Initialize 1st moment vector)\nu0 \u21900 (Initialize the exponentially weighted in\ufb01nity norm)\nt \u21900 (Initialize timestep)\nwhile \u03b8t not converged do\nt \u2190t + 1\ngt \u2190\u2207\u03b8ft(\u03b8t\u22121) (Get gradients w.r.t. stochastic objective at timestep t)\nmt \u2190\u03b21 \u00b7 mt\u22121 + (1 \u2212\u03b21) \u00b7 gt (Update biased \ufb01rst moment estimate)\nut \u2190max(\u03b22 \u00b7 ut\u22121, |gt|) (Update the exponentially weighted in\ufb01nity norm)\n\u03b8t \u2190\u03b8t\u22121 \u2212(\u03b1/(1 \u2212\u03b2t\n1)) \u00b7 mt/ut (Update parameters)\nend while\nreturn \u03b8t (Resulting parameters)\nNote that the decay term is here equivalently parameterised as \u03b2p\n2 instead of \u03b22. Now let p \u2192\u221e,\nand de\ufb01ne ut = limp\u2192\u221e(vt)1/p, then:\nut = lim\np\u2192\u221e(vt)1/p = lim\np\u2192\u221e\n \n(1 \u2212\u03b2p\n2)\nt\nX\ni=1\n\u03b2p(t\u2212i)\n2\n\u00b7 |gi|p\n!1/p\n(8)\n= lim\np\u2192\u221e(1 \u2212\u03b2p\n2)1/p\n \nt\nX\ni=1\n\u03b2p(t\u2212i)\n2\n\u00b7 |gi|p\n!1/p\n(9)\n= lim\np\u2192\u221e\n \nt\nX\ni=1\n\u0010\n\u03b2(t\u2212i)\n2\n\u00b7 |gi|\n\u0011p\n!1/p\n(10)\n= max\n\u0000\u03b2t\u22121\n2\n|g1|, \u03b2t\u22122\n2\n|g2|, . . . , \u03b22|gt\u22121|, |gt|\n\u0001\n(11)\nWhich corresponds to the remarkably simple recursive formula:\nut = max(\u03b22 \u00b7 ut\u22121, |gt|)\n(12)\nwith initial value u0 = 0. Note that, conveniently enough, we don\u2019t need to correct for initialization\nbias in this case. Also note that the magnitude of parameter updates has a simpler bound with\nAdaMax than Adam, namely: |\u2206t| \u2264\u03b1.\n7.2\nTEMPORAL AVERAGING\nSince the last iterate is noisy due to stochastic approximation, better generalization performance is\noften achieved by averaging. Previously in Moulines & Bach (2011), Polyak-Ruppert averaging\n(Polyak & Juditsky, 1992; Ruppert, 1988) has been shown to improve the convergence of standard\nSGD, where \u00af\u03b8t = 1\nt\nPn\nk=1 \u03b8k. Alternatively, an exponential moving average over the parameters can\nbe used, giving higher weight to more recent parameter values. This can be trivially implemented\nby adding one line to the inner loop of algorithms 1 and 2: \u00af\u03b8t \u2190\u03b22 \u00b7 \u00af\u03b8t\u22121 +(1\u2212\u03b22)\u03b8t, with \u00af\u03b80 = 0.\nInitalization bias can again be corrected by the estimator b\u03b8t = \u00af\u03b8t/(1 \u2212\u03b2t\n2).\n8\nCONCLUSION\nWe have introduced a simple and computationally ef\ufb01cient algorithm for gradient-based optimiza-\ntion of stochastic objective functions. Our method is aimed towards machine learning problems with\n9\nPublished as a conference paper at ICLR 2015\nlarge datasets and/or high-dimensional parameter spaces. The method combines the advantages of\ntwo recently popular optimization methods: the ability of AdaGrad to deal with sparse gradients,\nand the ability of RMSProp to deal with non-stationary objectives. The method is straightforward\nto implement and requires little memory. The experiments con\ufb01rm the analysis on the rate of con-\nvergence in convex problems. Overall, we found Adam to be robust and well-suited to a wide range\nof non-convex optimization problems in the \ufb01eld machine learning.\n9\nACKNOWLEDGMENTS\nThis paper would probably not have existed without the support of Google Deepmind. We would\nlike to give special thanks to Ivo Danihelka, and Tom Schaul for coining the name Adam. Thanks to\nKai Fan from Duke University for spotting an error in the original AdaMax derivation. Experiments\nin this work were partly carried out on the Dutch national e-infrastructure with the support of SURF\nFoundation. Diederik Kingma is supported by the Google European Doctorate Fellowship in Deep\nLearning.\nREFERENCES\nAmari, Shun-Ichi. Natural gradient works ef\ufb01ciently in learning. Neural computation, 10(2):251\u2013276, 1998.\nDeng, Li, Li, Jinyu, Huang, Jui-Ting, Yao, Kaisheng, Yu, Dong, Seide, Frank, Seltzer, Michael, Zweig, Geoff,\nHe, Xiaodong, Williams, Jason, et al. Recent advances in deep learning for speech research at microsoft.\nICASSP 2013, 2013.\nDuchi, John, Hazan, Elad, and Singer, Yoram. Adaptive subgradient methods for online learning and stochastic\noptimization. The Journal of Machine Learning Research, 12:2121\u20132159, 2011.\nGraves, Alex. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.\nGraves, Alex, Mohamed, Abdel-rahman, and Hinton, Geoffrey. Speech recognition with deep recurrent neural\nnetworks. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on,\npp. 6645\u20136649. IEEE, 2013.\nHinton, G.E. and Salakhutdinov, R.R. Reducing the dimensionality of data with neural networks. Science, 313\n(5786):504\u2013507, 2006.\nHinton, Geoffrey, Deng, Li, Yu, Dong, Dahl, George E, Mohamed, Abdel-rahman, Jaitly, Navdeep, Senior,\nAndrew, Vanhoucke, Vincent, Nguyen, Patrick, Sainath, Tara N, et al. Deep neural networks for acoustic\nmodeling in speech recognition: The shared views of four research groups. Signal Processing Magazine,\nIEEE, 29(6):82\u201397, 2012a.\nHinton, Geoffrey E, Srivastava, Nitish, Krizhevsky, Alex, Sutskever, Ilya, and Salakhutdinov, Ruslan R. Im-\nproving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580,\n2012b.\nKingma, Diederik P and Welling, Max. Auto-Encoding Variational Bayes. In The 2nd International Conference\non Learning Representations (ICLR), 2013.\nKrizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classi\ufb01cation with deep convolutional\nneural networks. In Advances in neural information processing systems, pp. 1097\u20131105, 2012.\nMaas, Andrew L, Daly, Raymond E, Pham, Peter T, Huang, Dan, Ng, Andrew Y, and Potts, Christopher.\nLearning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association\nfor Computational Linguistics: Human Language Technologies-Volume 1, pp. 142\u2013150. Association for\nComputational Linguistics, 2011.\nMoulines, Eric and Bach, Francis R.\nNon-asymptotic analysis of stochastic approximation algorithms for\nmachine learning. In Advances in Neural Information Processing Systems, pp. 451\u2013459, 2011.\nPascanu, Razvan and Bengio, Yoshua.\nRevisiting natural gradient for deep networks.\narXiv preprint\narXiv:1301.3584, 2013.\nPolyak, Boris T and Juditsky, Anatoli B. Acceleration of stochastic approximation by averaging. SIAM Journal\non Control and Optimization, 30(4):838\u2013855, 1992.\n10\nPublished as a conference paper at ICLR 2015\nRoux, Nicolas L and Fitzgibbon, Andrew W. A fast natural newton method. In Proceedings of the 27th\nInternational Conference on Machine Learning (ICML-10), pp. 623\u2013630, 2010.\nRuppert, David. Ef\ufb01cient estimations from a slowly convergent robbins-monro process. Technical report,\nCornell University Operations Research and Industrial Engineering, 1988.\nSchaul, Tom, Zhang, Sixin, and LeCun, Yann. No more pesky learning rates. arXiv preprint arXiv:1206.1106,\n2012.\nSohl-Dickstein, Jascha, Poole, Ben, and Ganguli, Surya. Fast large-scale optimization by unifying stochas-\ntic gradient and quasi-newton methods. In Proceedings of the 31st International Conference on Machine\nLearning (ICML-14), pp. 604\u2013612, 2014.\nSutskever, Ilya, Martens, James, Dahl, George, and Hinton, Geoffrey. On the importance of initialization and\nmomentum in deep learning. In Proceedings of the 30th International Conference on Machine Learning\n(ICML-13), pp. 1139\u20131147, 2013.\nTieleman, T. and Hinton, G. Lecture 6.5 - RMSProp, COURSERA: Neural Networks for Machine Learning.\nTechnical report, 2012.\nWang, Sida and Manning, Christopher. Fast dropout training. In Proceedings of the 30th International Confer-\nence on Machine Learning (ICML-13), pp. 118\u2013126, 2013.\nZeiler, Matthew D. Adadelta: An adaptive learning rate method. arXiv preprint arXiv:1212.5701, 2012.\nZinkevich, Martin. Online convex programming and generalized in\ufb01nitesimal gradient ascent. 2003.\n11\nPublished as a conference paper at ICLR 2015\n10\nAPPENDIX\n10.1\nCONVERGENCE PROOF\nDe\ufb01nition 10.1. A function f : Rd \u2192R is convex if for all x, y \u2208Rd, for all \u03bb \u2208[0, 1],\n\u03bbf(x) + (1 \u2212\u03bb)f(y) \u2265f(\u03bbx + (1 \u2212\u03bb)y)\nAlso, notice that a convex function can be lower bounded by a hyperplane at its tangent.\nLemma 10.2. If a function f : Rd \u2192R is convex, then for all x, y \u2208Rd,\nf(y) \u2265f(x) + \u2207f(x)T (y \u2212x)\nThe above lemma can be used to upper bound the regret and our proof for the main theorem is\nconstructed by substituting the hyperplane with the Adam update rules.\nThe following two lemmas are used to support our main theorem. We also use some de\ufb01nitions sim-\nplify our notation, where gt \u225c\u2207ft(\u03b8t) and gt,i as the ith element. We de\ufb01ne g1:t,i \u2208Rt as a vector\nthat contains the ith dimension of the gradients over all iterations till t, g1:t,i = [g1,i, g2,i, \u00b7 \u00b7 \u00b7 , gt,i]\nLemma 10.3. Let gt = \u2207ft(\u03b8t) and g1:t be de\ufb01ned as above and bounded, \u2225gt\u22252 \u2264G, \u2225gt\u2225\u221e\u2264\nG\u221e. Then,\nT\nX\nt=1\ns\ng2\nt,i\nt\n\u22642G\u221e\u2225g1:T,i\u22252\nProof. We will prove the inequality using induction over T.\nThe base case for T = 1, we have\nq\ng2\n1,i \u22642G\u221e\u2225g1,i\u22252.\nFor the inductive step,\nT\nX\nt=1\ns\ng2\nt,i\nt\n=\nT \u22121\nX\nt=1\ns\ng2\nt,i\nt\n+\ns\ng2\nT,i\nT\n\u22642G\u221e\u2225g1:T \u22121,i\u22252 +\ns\ng2\nT,i\nT\n= 2G\u221e\nq\n\u2225g1:T,i\u22252\n2 \u2212g2\nT +\ns\ng2\nT,i\nT\nFrom, \u2225g1:T,i\u22252\n2 \u2212g2\nT,i +\ng4\nT,i\n4\u2225g1:T,i\u22252\n2 \u2265\u2225g1:T,i\u22252\n2 \u2212g2\nT,i, we can take square root of both side and\nhave,\nq\n\u2225g1:T,i\u22252\n2 \u2212g2\nT,i \u2264\u2225g1:T,i\u22252 \u2212\ng2\nT,i\n2\u2225g1:T,i\u22252\n\u2264\u2225g1:T,i\u22252 \u2212\ng2\nT,i\n2\np\nTG2\u221e\nRearrange the inequality and substitute the\nq\n\u2225g1:T,i\u22252\n2 \u2212g2\nT,i term,\nG\u221e\nq\n\u2225g1:T,i\u22252\n2 \u2212g2\nT +\ns\ng2\nT,i\nT\n\u22642G\u221e\u2225g1:T,i\u22252\n12\nPublished as a conference paper at ICLR 2015\nLemma 10.4. Let \u03b3 \u225c\n\u03b22\n1\n\u221a\u03b22 . For \u03b21, \u03b22 \u2208[0, 1) that satisfy\n\u03b22\n1\n\u221a\u03b22 < 1 and bounded gt, \u2225gt\u22252 \u2264G,\n\u2225gt\u2225\u221e\u2264G\u221e, the following inequality holds\nT\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n\u2264\n2\n1 \u2212\u03b3\n1\n\u221a1 \u2212\u03b22\n\u2225g1:T,i\u22252\nProof. Under the assumption,\n\u221a\n1\u2212\u03b2t\n2\n(1\u2212\u03b2t\n1)2 \u2264\n1\n(1\u2212\u03b21)2 . We can expand the last term in the summation\nusing the update rules in Algorithm 1,\nT\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n=\nT \u22121\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n+\np\n1 \u2212\u03b2T\n2\n(1 \u2212\u03b2T\n1 )2\n(PT\nk=1(1 \u2212\u03b21)\u03b2T \u2212k\n1\ngk,i)2\nq\nT PT\nj=1(1 \u2212\u03b22)\u03b2T \u2212j\n2\ng2\nj,i\n\u2264\nT \u22121\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n+\np\n1 \u2212\u03b2T\n2\n(1 \u2212\u03b2T\n1 )2\nT\nX\nk=1\nT((1 \u2212\u03b21)\u03b2T \u2212k\n1\ngk,i)2\nq\nT PT\nj=1(1 \u2212\u03b22)\u03b2T \u2212j\n2\ng2\nj,i\n\u2264\nT \u22121\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n+\np\n1 \u2212\u03b2T\n2\n(1 \u2212\u03b2T\n1 )2\nT\nX\nk=1\nT((1 \u2212\u03b21)\u03b2T \u2212k\n1\ngk,i)2\nq\nT(1 \u2212\u03b22)\u03b2T \u2212k\n2\ng2\nk,i\n\u2264\nT \u22121\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n+\np\n1 \u2212\u03b2T\n2\n(1 \u2212\u03b2T\n1 )2\n(1 \u2212\u03b21)2\np\nT(1 \u2212\u03b22)\nT\nX\nk=1\nT\n\u0012 \u03b22\n1\n\u221a\u03b22\n\u0013T \u2212k\n\u2225gk,i\u22252\n\u2264\nT \u22121\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n+\nT\np\nT(1 \u2212\u03b22)\nT\nX\nk=1\n\u03b3T \u2212k\u2225gk,i\u22252\nSimilarly, we can upper bound the rest of the terms in the summation.\nT\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n\u2264\nT\nX\nt=1\n\u2225gt,i\u22252\np\nt(1 \u2212\u03b22)\nT \u2212t\nX\nj=0\nt\u03b3j\n\u2264\nT\nX\nt=1\n\u2225gt,i\u22252\np\nt(1 \u2212\u03b22)\nT\nX\nj=0\nt\u03b3j\nFor \u03b3 < 1, using the upper bound on the arithmetic-geometric series, P\nt t\u03b3t <\n1\n(1\u2212\u03b3)2 :\nT\nX\nt=1\n\u2225gt,i\u22252\np\nt(1 \u2212\u03b22)\nT\nX\nj=0\nt\u03b3j \u2264\n1\n(1 \u2212\u03b3)2\u221a1 \u2212\u03b22\nT\nX\nt=1\n\u2225gt,i\u22252\n\u221a\nt\nApply Lemma 10.3,\nT\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n\u2264\n2G\u221e\n(1 \u2212\u03b3)2\u221a1 \u2212\u03b22\n\u2225g1:T,i\u22252\nTo simplify the notation, we de\ufb01ne \u03b3 \u225c\n\u03b22\n1\n\u221a\u03b22 . Intuitively, our following theorem holds when the\nlearning rate \u03b1t is decaying at a rate of t\u22121\n2 and \ufb01rst moment running average coef\ufb01cient \u03b21,t decay\nexponentially with \u03bb, that is typically close to 1, e.g. 1 \u221210\u22128.\nTheorem 10.5. Assume that the function ft has bounded gradients, \u2225\u2207ft(\u03b8)\u22252 \u2264G, \u2225\u2207ft(\u03b8)\u2225\u221e\u2264\nG\u221efor all \u03b8 \u2208Rd and distance between any \u03b8t generated by Adam is bounded, \u2225\u03b8n \u2212\u03b8m\u22252 \u2264D,\n13\nPublished as a conference paper at ICLR 2015\n\u2225\u03b8m \u2212\u03b8n\u2225\u221e\u2264D\u221efor any m, n \u2208{1, ..., T}, and \u03b21, \u03b22 \u2208[0, 1) satisfy\n\u03b22\n1\n\u221a\u03b22 < 1. Let \u03b1t =\n\u03b1\n\u221a\nt\nand \u03b21,t = \u03b21\u03bbt\u22121, \u03bb \u2208(0, 1). Adam achieves the following guarantee, for all T \u22651.\nR(T) \u2264\nD2\n2\u03b1(1 \u2212\u03b21)\nd\nX\ni=1\np\nTbvT,i+\n\u03b1(\u03b21 + 1)G\u221e\n(1 \u2212\u03b21)\u221a1 \u2212\u03b22(1 \u2212\u03b3)2\nd\nX\ni=1\n\u2225g1:T,i\u22252+\nd\nX\ni=1\nD2\n\u221eG\u221e\n\u221a1 \u2212\u03b22\n2\u03b1(1 \u2212\u03b21)(1 \u2212\u03bb)2\nProof. Using Lemma 10.2, we have,\nft(\u03b8t) \u2212ft(\u03b8\u2217) \u2264gT\nt (\u03b8t \u2212\u03b8\u2217) =\nd\nX\ni=1\ngt,i(\u03b8t,i \u2212\u03b8\u2217\n,i)\nFrom the update rules presented in algorithm 1,\n\u03b8t+1 = \u03b8t \u2212\u03b1t bmt/\np\nbvt\n= \u03b8t \u2212\n\u03b1t\n1 \u2212\u03b2t\n1\n\u0012 \u03b21,t\n\u221abvt\nmt\u22121 + (1 \u2212\u03b21,t)\n\u221abvt\ngt\n\u0013\nWe focus on the ith dimension of the parameter vector \u03b8t \u2208Rd. Subtract the scalar \u03b8\u2217\n,i and square\nboth sides of the above update rule, we have,\n(\u03b8t+1,i \u2212\u03b8\u2217\n,i)2 =(\u03b8t,i \u2212\u03b8\u2217\n,i)2 \u2212\n2\u03b1t\n1 \u2212\u03b2t\n1\n( \u03b21,t\np\nbvt,i\nmt\u22121,i + (1 \u2212\u03b21,t)\np\nbvt,i\ngt,i)(\u03b8t,i \u2212\u03b8\u2217\n,i) + \u03b12\nt ( bmt,i\np\nbvt,i\n)2\nWe can rearrange the above equation and use Young\u2019s inequality, ab \u2264a2/2 + b2/2. Also, it can be\nshown that\np\nbvt,i =\nqPt\nj=1(1 \u2212\u03b22)\u03b2t\u2212j\n2\ng2\nj,i/\np\n1 \u2212\u03b2t\n2 \u2264\u2225g1:t,i\u22252 and \u03b21,t \u2264\u03b21. Then\ngt,i(\u03b8t,i \u2212\u03b8\u2217\n,i) =(1 \u2212\u03b2t\n1)\np\nbvt,i\n2\u03b1t(1 \u2212\u03b21,t)\n\u0012\n(\u03b8t,i \u2212\u03b8\u2217\n,t)2 \u2212(\u03b8t+1,i \u2212\u03b8\u2217\n,i)2\n\u0013\n+\n\u03b21,t\n(1 \u2212\u03b21,t)\nbv\n1\n4\nt\u22121,i\n\u221a\u03b1t\u22121\n(\u03b8\u2217\n,i \u2212\u03b8t,i)\u221a\u03b1t\u22121\nmt\u22121,i\nbv\n1\n4\nt\u22121,i\n+ \u03b1t(1 \u2212\u03b2t\n1)\np\nbvt,i\n2(1 \u2212\u03b21,t)\n( bmt,i\np\nbvt,i\n)2\n\u2264\n1\n2\u03b1t(1 \u2212\u03b21)\n\u0012\n(\u03b8t,i \u2212\u03b8\u2217\n,t)2 \u2212(\u03b8t+1,i \u2212\u03b8\u2217\n,i)2\n\u0013p\nbvt,i +\n\u03b21,t\n2\u03b1t\u22121(1 \u2212\u03b21,t)(\u03b8\u2217\n,i \u2212\u03b8t,i)2p\nbvt\u22121,i\n+\n\u03b21\u03b1t\u22121\n2(1 \u2212\u03b21)\nm2\nt\u22121,i\np\nbvt\u22121,i\n+\n\u03b1t\n2(1 \u2212\u03b21)\nbm2\nt,i\np\nbvt,i\nWe apply Lemma 10.4 to the above inequality and derive the regret bound by summing across all\nthe dimensions for i \u22081, ..., d in the upper bound of ft(\u03b8t) \u2212ft(\u03b8\u2217) and the sequence of convex\nfunctions for t \u22081, ..., T:\nR(T) \u2264\nd\nX\ni=1\n1\n2\u03b11(1 \u2212\u03b21)(\u03b81,i \u2212\u03b8\u2217\n,i)2p\nbv1,i +\nd\nX\ni=1\nT\nX\nt=2\n1\n2(1 \u2212\u03b21)(\u03b8t,i \u2212\u03b8\u2217\n,i)2(\np\nbvt,i\n\u03b1t\n\u2212\np\nbvt\u22121,i\n\u03b1t\u22121\n)\n+\n\u03b21\u03b1G\u221e\n(1 \u2212\u03b21)\u221a1 \u2212\u03b22(1 \u2212\u03b3)2\nd\nX\ni=1\n\u2225g1:T,i\u22252 +\n\u03b1G\u221e\n(1 \u2212\u03b21)\u221a1 \u2212\u03b22(1 \u2212\u03b3)2\nd\nX\ni=1\n\u2225g1:T,i\u22252\n+\nd\nX\ni=1\nT\nX\nt=1\n\u03b21,t\n2\u03b1t(1 \u2212\u03b21,t)(\u03b8\u2217\n,i \u2212\u03b8t,i)2p\nbvt,i\n14\nPublished as a conference paper at ICLR 2015\nFrom the assumption, \u2225\u03b8t \u2212\u03b8\u2217\u22252 \u2264D, \u2225\u03b8m \u2212\u03b8n\u2225\u221e\u2264D\u221e, we have:\nR(T) \u2264\nD2\n2\u03b1(1 \u2212\u03b21)\nd\nX\ni=1\np\nTbvT,i +\n\u03b1(1 + \u03b21)G\u221e\n(1 \u2212\u03b21)\u221a1 \u2212\u03b22(1 \u2212\u03b3)2\nd\nX\ni=1\n\u2225g1:T,i\u22252 + D2\n\u221e\n2\u03b1\nd\nX\ni=1\nt\nX\nt=1\n\u03b21,t\n(1 \u2212\u03b21,t)\np\ntbvt,i\n\u2264\nD2\n2\u03b1(1 \u2212\u03b21)\nd\nX\ni=1\np\nTbvT,i +\n\u03b1(1 + \u03b21)G\u221e\n(1 \u2212\u03b21)\u221a1 \u2212\u03b22(1 \u2212\u03b3)2\nd\nX\ni=1\n\u2225g1:T,i\u22252\n+ D2\n\u221eG\u221e\n\u221a1 \u2212\u03b22\n2\u03b1\nd\nX\ni=1\nt\nX\nt=1\n\u03b21,t\n(1 \u2212\u03b21,t)\n\u221a\nt\nWe can use arithmetic geometric series upper bound for the last term:\nt\nX\nt=1\n\u03b21,t\n(1 \u2212\u03b21,t)\n\u221a\nt \u2264\nt\nX\nt=1\n1\n(1 \u2212\u03b21)\u03bbt\u22121\u221a\nt\n\u2264\nt\nX\nt=1\n1\n(1 \u2212\u03b21)\u03bbt\u22121t\n\u2264\n1\n(1 \u2212\u03b21)(1 \u2212\u03bb)2\nTherefore, we have the following regret bound:\nR(T) \u2264\nD2\n2\u03b1(1 \u2212\u03b21)\nd\nX\ni=1\np\nTbvT,i +\n\u03b1(1 + \u03b21)G\u221e\n(1 \u2212\u03b21)\u221a1 \u2212\u03b22(1 \u2212\u03b3)2\nd\nX\ni=1\n\u2225g1:T,i\u22252 +\nd\nX\ni=1\nD2\n\u221eG\u221e\n\u221a1 \u2212\u03b22\n2\u03b1\u03b21(1 \u2212\u03bb)2\n15\n",
        "sentence": "",
        "context": "modeling in speech recognition: The shared views of four research groups. Signal Processing Magazine,\nIEEE, 29(6):82\u201397, 2012a.\nHinton, Geoffrey E, Srivastava, Nitish, Krizhevsky, Alex, Sutskever, Ilya, and Salakhutdinov, Ruslan R. Im-\nDeng, Li, Li, Jinyu, Huang, Jui-Ting, Yao, Kaisheng, Yu, Dong, Seide, Frank, Seltzer, Michael, Zweig, Geoff,\nHe, Xiaodong, Williams, Jason, et al. Recent advances in deep learning for speech research at microsoft.\nICASSP 2013, 2013.\n(5786):504\u2013507, 2006.\nHinton, Geoffrey, Deng, Li, Yu, Dong, Dahl, George E, Mohamed, Abdel-rahman, Jaitly, Navdeep, Senior,\nAndrew, Vanhoucke, Vincent, Nguyen, Patrick, Sainath, Tara N, et al. Deep neural networks for acoustic"
    },
    {
        "title": "Variational dropout and the local reparameterization trick",
        "author": [
            "Kingma",
            "Diederik P",
            "Salimans",
            "Tim",
            "Welling",
            "Max"
        ],
        "venue": "CoRR, abs/1506.02557,",
        "citeRegEx": "Kingma et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "Kingma et al\\.",
        "year": 2015,
        "abstract": "We investigate a local reparameterizaton technique for greatly reducing the\nvariance of stochastic gradients for variational Bayesian inference (SGVB) of a\nposterior over model parameters, while retaining parallelizability. This local\nreparameterization translates uncertainty about global parameters into local\nnoise that is independent across datapoints in the minibatch. Such\nparameterizations can be trivially parallelized and have variance that is\ninversely proportional to the minibatch size, generally leading to much faster\nconvergence. Additionally, we explore a connection with dropout: Gaussian\ndropout objectives correspond to SGVB with local reparameterization, a\nscale-invariant prior and proportionally fixed posterior variance. Our method\nallows inference of more flexibly parameterized posteriors; specifically, we\npropose variational dropout, a generalization of Gaussian dropout where the\ndropout rates are learned, often leading to better models. The method is\ndemonstrated through several experiments.",
        "full_text": "Variational Dropout and\nthe Local Reparameterization Trick\nDiederik P. Kingma\u2217, Tim Salimans\u00d7 and Max Welling\u2217\u2020\n\u2217Machine Learning Group, University of Amsterdam\n\u00d7 Algoritmica\n\u2020 University of California, Irvine, and the Canadian Institute for Advanced Research (CIFAR)\nD.P.Kingma@uva.nl, salimans.tim@gmail.com, M.Welling@uva.nl\nAbstract\nWe investigate a local reparameterizaton technique for greatly reducing the vari-\nance of stochastic gradients for variational Bayesian inference (SGVB) of a pos-\nterior over model parameters, while retaining parallelizability. This local repa-\nrameterization translates uncertainty about global parameters into local noise that\nis independent across datapoints in the minibatch. Such parameterizations can be\ntrivially parallelized and have variance that is inversely proportional to the mini-\nbatch size, generally leading to much faster convergence. Additionally, we explore\na connection with dropout: Gaussian dropout objectives correspond to SGVB with\nlocal reparameterization, a scale-invariant prior and proportionally \ufb01xed posterior\nvariance. Our method allows inference of more \ufb02exibly parameterized posteriors;\nspeci\ufb01cally, we propose variational dropout, a generalization of Gaussian dropout\nwhere the dropout rates are learned, often leading to better models. The method\nis demonstrated through several experiments.\n1\nIntroduction\nDeep neural networks are a \ufb02exible family of models that easily scale to millions of parameters and\ndatapoints, but are still tractable to optimize using minibatch-based stochastic gradient ascent. Due\nto their high \ufb02exibility, neural networks have the capacity to \ufb01t a wide diversity of nonlinear patterns\nin the data. This \ufb02exbility often leads to over\ufb01tting when left unchecked: spurious patterns are found\nthat happen to \ufb01t well to the training data, but are not predictive for new data. Various regularization\ntechniques for controlling this over\ufb01tting are used in practice; a currently popular and empirically\neffective technique being dropout [10]. In [22] it was shown that regular (binary) dropout has a\nGaussian approximation called Gaussian dropout with virtually identical regularization performance\nbut much faster convergence. In section 5 of [22] it is shown that Gaussian dropout optimizes a lower\nbound on the marginal likelihood of the data. In this paper we show that a relationship between\ndropout and Bayesian inference can be extended and exploited to greatly improve the ef\ufb01ciency of\nvariational Bayesian inference on the model parameters. This work has a direct interpretation as a\ngeneralization of Gaussian dropout, with the same fast convergence but now with the freedom to\nspecify more \ufb02exibly parameterized posterior distributions.\nBayesian posterior inference over the neural network parameters is a theoretically attractive method\nfor controlling over\ufb01tting; exact inference is computationally intractable, but ef\ufb01cient approximate\nschemes can be designed. Markov Chain Monte Carlo (MCMC) is a class of approximate inference\nmethods with asymptotic guarantees, pioneered by [16] for the application of regularizing neural\nnetworks. Later useful re\ufb01nements include [23] and [1].\nAn alternative to MCMC is variational inference [11] or the equivalent minimum description length\n(MDL) framework. Modern variants of stochastic variational inference have been applied to neural\n1\narXiv:1506.02557v2  [stat.ML]  20 Dec 2015\nnetworks with some succes [8], but have been limited by high variance in the gradients. Despite\ntheir theoretical attractiveness, Bayesian methods for inferring a posterior distribution over neural\nnetwork weights have not yet been shown to outperform simpler methods such as dropout. Even a\nnew crop of ef\ufb01cient variational inference algorithms based on stochastic gradients with minibatches\nof data [14, 17, 19] have not yet been shown to signi\ufb01cantly improve upon simpler dropout-based\nregularization.\nIn section 2 we explore an as yet unexploited trick for improving the ef\ufb01ciency of stochastic gradient-\nbased variational inference with minibatches of data, by translating uncertainty about global param-\neters into local noise that is independent across datapoints in the minibatch. The resulting method\nhas an optimization speed on the same level as fast dropout [22], and indeed has the original Gaus-\nsian dropout method as a special case. An advantage of our method is that it allows for full Bayesian\nanalysis of the model, and that it\u2019s signi\ufb01cantly more \ufb02exible than standard dropout. The approach\npresented here is closely related to several popular methods in the literature that regularize by adding\nrandom noise; these relationships are discussed in section 4.\n2\nEf\ufb01cient and Practical Bayesian Inference\nWe consider Bayesian analysis of a dataset D, containing a set of N i.i.d. observations of tuples\n(x, y), where the goal is to learn a model with parameters or weights w of the conditional probabil-\nity p(y|x, w) (standard classi\ufb01cation or regression)1. Bayesian inference in such a model consists\nof updating some initial belief over parameters w in the form of a prior distribution p(w), after\nobserving data D, into an updated belief over these parameters in the form of (an approximation\nto) the posterior distribution p(w|D). Computing the true posterior distribution through Bayes\u2019 rule\np(w|D) = p(w)p(D|w)/p(D) involves computationally intractable integrals, so good approxima-\ntions are necessary. In variational inference, inference is cast as an optimization problem where we\noptimize the parameters \u03c6 of some parameterized model q\u03c6(w) such that q\u03c6(w) is a close approx-\nimation to p(w|D) as measured by the Kullback-Leibler divergence DKL(q\u03c6(w)||p(w|D)). This\ndivergence of our posterior q\u03c6(w) to the true posterior is minimized in practice by maximizing the\nso-called variational lower bound L(\u03c6) of the marginal likelihood of the data:\nL(\u03c6) = \u2212DKL(q\u03c6(w)||p(w)) + LD(\u03c6)\n(1)\nwhere\nLD(\u03c6) =\nX\n(x,y)\u2208D\nEq\u03c6(w) [log p(y|x, w)]\n(2)\nWe\u2019ll call LD(\u03c6) the expected log-likelihood. The bound L(\u03c6) plus DKL(q\u03c6(w)||p(w|D)) equals\nthe (conditional) marginal log-likelihood P\n(x,y)\u2208D log p(y|x). Since this marginal log-likelihood\nis constant w.r.t. \u03c6, maximizing the bound w.r.t. \u03c6 will minimize DKL(q\u03c6(w)||p(w|D)).\n2.1\nStochastic Gradient Variational Bayes (SGVB)\nVarious algorithms for gradient-based optimization of the variational bound (eq. (1)) with differ-\nentiable q and p exist. See section 4 for an overview. A recently proposed ef\ufb01cient method for\nminibatch-based optimization with differentiable models is the stochastic gradient variational Bayes\n(SGVB) method introduced in [14] (especially appendix F) and [17]. The basic trick in SGVB is\nto parameterize the random parameters w \u223cq\u03c6(w) as: w = f(\u03f5, \u03c6) where f(.) is a differen-\ntiable function and \u03f5 \u223cp(\u03f5) is a random noise variable. In this new parameterisation, an unbiased\ndifferentiable minibatch-based Monte Carlo estimator of the expected log-likelihood can be formed:\nLD(\u03c6) \u2243LSGVB\nD\n(\u03c6) = N\nM\nM\nX\ni=1\nlog p(yi|xi, w = f(\u03f5, \u03c6)),\n(3)\nwhere (xi, yi)M\ni=1 is a minibatch of data with M random datapoints (xi, yi) \u223cD, and \u03f5 is a noise\nvector drawn from the noise distribution p(\u03f5). We\u2019ll assume that the remaining term in the varia-\ntional lower bound, DKL(q\u03c6(w)||p(w)), can be computed deterministically, but otherwise it may\nbe approximated similarly. The estimator (3) is differentiable w.r.t. \u03c6 and unbiased, so its gradient\n1Note that the described method is not limited to classi\ufb01cation or regression and is straightforward to apply\nto other modeling settings like unsupervised models and temporal models.\n2\nis also unbiased: \u2207\u03c6LD(\u03c6) \u2243\u2207\u03c6LSGVB\nD\n(\u03c6). We can proceed with variational Bayesian inference\nby randomly initializing \u03c6 and performing stochastic gradient ascent on L(\u03c6) (1).\n2.2\nVariance of the SGVB estimator\nThe theory of stochastic approximation tells us that stochastic gradient ascent using (3) will asymp-\ntotically converge to a local optimum for an appropriately declining step size and suf\ufb01cient weight\nupdates [18], but in practice the performance of stochastic gradient ascent crucially depends on\nthe variance of the gradients. If this variance is too large, stochastic gradient descent will fail\nto make much progress in any reasonable amount of time. Our objective function consists of an\nexpected log likelihood term that we approximate using Monte Carlo, and a KL divergence term\nDKL(q\u03c6(w)||p(w)) that we assume can be calculated analytically and otherwise be approximated\nwith Monte Carlo with similar reparameterization.\nAssume that we draw minibatches of datapoints with replacement; see appendix F for a similar\nanalysis for minibatches without replacement. Using Li as shorthand for log p(yi|xi, w = f(\u03f5i, \u03c6)),\nthe contribution to the likelihood for the i-th datapoint in the minibatch, the Monte Carlo estimator\n(3) may be rewritten as LSGVB\nD\n(\u03c6) = N\nM\nPM\ni=1 Li, whose variance is given by\nVar\n\u0002\nLSGVB\nD\n(\u03c6)\n\u0003\n= N 2\nM 2\n\u0010 M\nX\ni=1\nVar [Li] + 2\nM\nX\ni=1\nM\nX\nj=i+1\nCov [Li, Lj]\n\u0011\n(4)\n=N 2\u0010 1\nM Var [Li] + M \u22121\nM\nCov [Li, Lj]\n\u0011\n,\n(5)\nwhere the variances and covariances are w.r.t. both the data distribution and \u03f5 distribution, i.e.\nVar [Li] = Var\u03f5,xi,yi \u0002\nlog p(yi|xi, w = f(\u03f5, \u03c6))\n\u0003\n, with xi, yi drawn from the empirical distribu-\ntion de\ufb01ned by the training set. As can be seen from (5), the total contribution to the variance by\nVar [Li] is inversely proportional to the minibatch size M. However, the total contribution by the\ncovariances does not decrease with M. In practice, this means that the variance of LSGVB\nD\n(\u03c6) can be\ndominated by the covariances for even moderately large M.\n2.3\nLocal Reparameterization Trick\nWe therefore propose an alternative estimator for which we have Cov [Li, Lj] = 0, so that the vari-\nance of our stochastic gradients scales as 1/M. We then make this new estimator computationally\nef\ufb01cient by not sampling \u03f5 directly, but only sampling the intermediate variables f(\u03f5) through which\n\u03f5 in\ufb02uences LSGVB\nD\n(\u03c6). By doing so, the global uncertainty in the weights is translated into a form\nof local uncertainty that is independent across examples and easier to sample. We refer to such a\nreparameterization from global noise to local noise as the local reparameterization trick. Whenever\na source of global noise can be translated to local noise in the intermediate states of computation\n(\u03f5 \u2192f(\u03f5)), a local reparameterization can be applied to yield a computationally and statistically\nef\ufb01cient gradient estimator.\nSuch local reparameterization applies to a fairly large family of models, but is best explained through\na simple example: Consider a standard fully connected neural network containing a hidden layer\nconsisting of 1000 neurons. This layer receives an M \u00d7 1000 input feature matrix A from the layer\nbelow, which is multiplied by a 1000 \u00d7 1000 weight matrix W, before a nonlinearity is applied,\ni.e. B = AW. We then specify the posterior approximation on the weights to be a fully factor-\nized Gaussian, i.e. q\u03c6(wi,j) = N(\u00b5i,j, \u03c32\ni,j) \u2200wi,j \u2208W, which means the weights are sampled as\nwi,j = \u00b5i,j + \u03c3i,j\u03f5i,j, with \u03f5i,j \u223cN(0, 1). In this case we could make sure that Cov [Li, Lj] = 0\nby sampling a separate weight matrix W for each example in the minibatch, but this is not com-\nputationally ef\ufb01cient: we would need to sample M million random numbers for just a single layer\nof the neural network. Even if this could be done ef\ufb01ciently, the computation following this step\nwould become much harder: Where we originally performed a simple matrix-matrix product of the\nform B = AW, this now turns into M separate local vector-matrix products. The theoretical com-\nplexity of this computation is higher, but, more importantly, such a computation can usually not be\nperformed in parallel using fast device-optimized BLAS (Basic Linear Algebra Subprograms). This\nalso happens with other neural network architectures such as convolutional neural networks, where\noptimized libraries for convolution cannot deal with separate \ufb01lter matrices per example.\n3\nFortunately, the weights (and therefore \u03f5) only in\ufb02uence the expected log likelihood through the\nneuron activations B, which are of much lower dimension. If we can therefore sample the random\nactivations B directly, without sampling W or \u03f5, we may obtain an ef\ufb01cient Monte Carlo estimator\nat a much lower cost. For a factorized Gaussian posterior on the weights, the posterior for the\nactivations (conditional on the input A) is also factorized Gaussian:\nq\u03c6(wi,j) = N(\u00b5i,j, \u03c32\ni,j) \u2200wi,j \u2208W\n=\u21d2\nq\u03c6(bm,j|A) = N(\u03b3m,j, \u03b4m,j), with\n\u03b3m,j =\n1000\nX\ni=1\nam,i\u00b5i,j,\nand\n\u03b4m,j =\n1000\nX\ni=1\na2\nm,i\u03c32\ni,j.\n(6)\nRather than sampling the Gaussian weights and then computing the resulting activations, we may\nthus sample the activations from their implied Gaussian distribution directly, using bm,j = \u03b3m,j +\np\n\u03b4m,j\u03b6m,j, with \u03b6m,j \u223cN(0, 1). Here, \u03b6 is an M \u00d7 1000 matrix, so we only need to sample M\nthousand random variables instead of M million: a thousand fold savings.\nIn addition to yielding a gradient estimator that is more computationally ef\ufb01cient than drawing sep-\narate weight matrices for each training example, the local reparameterization trick also leads to an\nestimator that has lower variance. To see why, consider the stochastic gradient estimate with respect\nto the posterior parameter \u03c32\ni,j for a minibatch of size M = 1. Drawing random weights W, we get\n\u2202LSGVB\nD\n\u2202\u03c32\ni,j\n= \u2202LSGVB\nD\n\u2202bm,j\n\u03f5i,jam,i\n2\u03c3i,j\n.\n(7)\nIf, on the other hand, we form the same gradient using the local reparameterization trick, we get\n\u2202LSGVB\nD\n\u2202\u03c32\ni,j\n= \u2202LSGVB\nD\n\u2202bm,j\n\u03b6m,ja2\nm,i\n2\np\n\u03b4m,j\n.\n(8)\nHere, there are two stochastic terms: The \ufb01rst is the backpropagated gradient \u2202LSGVB\nD\n/\u2202bm,j, and\nthe second is the sampled random noise (\u03f5i,j or \u03b6m,j). Estimating the gradient with respect to \u03c32\ni,j\nthen basically comes down to estimating the covariance between these two terms. This is much\neasier to do for \u03b6m,j as there are much fewer of these: individually they have higher correlation\nwith the backpropagated gradient \u2202LSGVB\nD\n/\u2202bm,j, so the covariance is easier to estimate. In other\nwords, measuring the effect of \u03b6m,j on \u2202LSGVB\nD\n/\u2202bm,j is easy as \u03b6m,j is the only random variable\ndirectly in\ufb02uencing this gradient via bm,j. On the other hand, when sampling random weights,\nthere are a thousand \u03f5i,j in\ufb02uencing each gradient term, so their individual effects get lost in the\nnoise. In appendix D we make this argument more rigorous, and in section 5 we show that it holds\nexperimentally.\n3\nVariational Dropout\nDropout is a technique for regularization of neural network parameters, which works by adding\nmultiplicative noise to the input of each layer of the neural network during optimization. Using the\nnotation of section 2.3, for a fully connected neural network dropout corresponds to:\nB = (A \u25e6\u03be)\u03b8, with \u03bei,j \u223cp(\u03bei,j)\n(9)\nwhere A is the M \u00d7 K matrix of input features for the current minibatch, \u03b8 is a K \u00d7 L weight ma-\ntrix, and B is the M \u00d7 L output matrix for the current layer (before a nonlinearity is applied). The\n\u25e6symbol denotes the elementwise (Hadamard) product of the input matrix with a M \u00d7 K matrix\nof independent noise variables \u03be. By adding noise to the input during training, the weight parame-\nters \u03b8 are less likely to over\ufb01t to the training data, as shown empirically by previous publications.\nOriginally, [10] proposed drawing the elements of \u03be from a Bernoulli distribution with probability\n1 \u2212p, with p the dropout rate. Later it was shown that using a continuous distribution with the same\nrelative mean and variance, such as a Gaussian N(1, \u03b1) with \u03b1 = p/(1 \u2212p), works as well or better\n[20].\nHere, we re-interpret dropout with continuous noise as a variational method, and propose a gen-\neralization that we call variational dropout. In developing variational dropout we provide a \ufb01rm\nBayesian justi\ufb01cation for dropout training by deriving its implicit prior distribution and variational\nobjective. This new interpretation allows us to propose several useful extensions to dropout, such as\na principled way of making the normally \ufb01xed dropout rates p adaptive to the data.\n4\n3.1\nVariational dropout with independent weight noise\nIf the elements of the noise matrix \u03be are drawn independently from a Gaussian N(1, \u03b1), the marginal\ndistributions of the activations bm,j \u2208B are Gaussian as well:\nq\u03c6(bm,j|A) = N(\u03b3m,j, \u03b4m,j), with \u03b3m,j =\nK\nX\ni=1\nam,i\u03b8i,j, and \u03b4m,j = \u03b1\nK\nX\ni=1\na2\nm,i\u03b82\ni,j.\n(10)\nMaking use of this fact, [22] proposed Gaussian dropout, a regularization method where, instead\nof applying (9), the activations are directly drawn from their (approximate or exact) marginal distri-\nbutions as given by (10). [22] argued that these marginal distributions are exact for Gaussian noise\n\u03be, and for Bernoulli noise still approximately Gaussian because of the central limit theorem. This\nignores the dependencies between the different elements of B, as present using (9), but [22] report\ngood results nonetheless.\nAs noted by [22], and explained in appendix B, this Gaussian dropout noise can also be interpreted\nas arising from a Bayesian treatment of a neural network with weights W that multiply the input to\ngive B = AW, where the posterior distribution of the weights is given by a factorized Gaussian with\nq\u03c6(wi,j) = N(\u03b8i,j, \u03b1\u03b82\ni,j). From this perspective, the marginal distributions (10) then arise through\nthe application of the local reparameterization trick, as introduced in section 2.3. The variational\nobjective corresponding to this interpretation is discussed in section 3.3.\n3.2\nVariational dropout with correlated weight noise\nInstead of ignoring the dependencies of the activation noise, as in section 3.1, we may retain the\ndependencies by interpreting dropout (9) as a form of correlated weight noise:\nB = (A \u25e6\u03be)\u03b8, \u03bei,j \u223cN(1, \u03b1)\n\u21d0\u21d2\nbm = amW, with\nW = (w\u2032\n1, w\u2032\n2, . . . , w\u2032\nK)\u2032, and wi = si\u03b8i, with q\u03c6(si) = N(1, \u03b1),\n(11)\nwhere am is a row of the input matrix and bm a row of the output. The wi are the rows of the\nweight matrix, each of which is constructed by multiplying a non-stochastic parameter vector \u03b8i by\na stochastic scale variable si. The distribution on these scale variables we interpret as a Bayesian\nposterior distribution. The weight parameters \u03b8i (and the biases) are estimated using maximum\nlikelihood. The original Gaussian dropout sampling procedure (9) can then be interpreted as arising\nfrom a local reparameterization of our posterior on the weights W.\n3.3\nDropout\u2019s scale-invariant prior and variational objective\nThe posterior distributions q\u03c6(W) proposed in sections 3.1 and 3.2 have in common that they can\nbe decomposed into a parameter vector \u03b8 that captures the mean, and a multiplicative noise term\ndetermined by parameters \u03b1. Any posterior distribution on W for which the noise enters this mul-\ntiplicative way, we will call a dropout posterior. Note that many common distributions, such as\nunivariate Gaussians (with nonzero mean), can be reparameterized to meet this requirement.\nDuring dropout training, \u03b8 is adapted to maximize the expected log likelihood Eq\u03b1 [LD(\u03b8)]. For this\nto be consistent with the optimization of a variational lower bound of the form in (2), the prior on\nthe weights p(w) has to be such that DKL(q\u03c6(w)||p(w)) does not depend on \u03b8. In appendix C we\nshow that the only prior that meets this requirement is the scale invariant log-uniform prior:\np(log(|wi,j|)) \u221dc,\ni.e. a prior that is uniform on the log-scale of the weights (or the weight-scales si for section 3.2). As\nexplained in appendix A, this prior has an interesting connection with the \ufb02oating point format for\nstoring numbers: From an MDL perspective, the \ufb02oating point format is optimal for communicating\nnumbers drawn from this prior. Conversely, the KL divergence DKL(q\u03c6(w)||p(w)) with this prior\nhas a natural interpretation as regularizing the number of signi\ufb01cant digits our posterior q\u03c6 stores\nfor the weights wi,j in the \ufb02oating-point format.\nPutting the expected log likelihood and KL-divergence penalty together, we see that dropout training\nmaximizes the following variatonal lower bound w.r.t. \u03b8:\nEq\u03b1 [LD(\u03b8)] \u2212DKL(q\u03b1(w)||p(w)),\n(12)\n5\nwhere we have made the dependence on the \u03b8 and \u03b1 parameters explicit. The noise parameters \u03b1\n(e.g. the dropout rates) are commonly treated as hyperparameters that are kept \ufb01xed during training.\nFor the log-uniform prior this then corresponds to a \ufb01xed limit on the number of signi\ufb01cant digits\nwe can learn for each of the weights wi,j. In section 3.4 we discuss the possibility of making this\nlimit adaptive by also maximizing the lower bound with respect to \u03b1.\nFor the choice of a factorized Gaussian approximate posterior with q\u03c6(wi,j) = N(\u03b8i,j, \u03b1\u03b82\ni,j), as\ndiscussed in section 3.1, the lower bound (12) is analyzed in detail in appendix C. There, it is shown\nthat for this particular choice of posterior the negative KL-divergence \u2212DKL(q\u03b1(w)||p(w)) is not\nanalytically tractable, but can be approximated extremely accurately using\n\u2212DKL[q\u03c6(wi)|p(wi)] \u2248constant + 0.5 log(\u03b1) + c1\u03b1 + c2\u03b12 + c3\u03b13,\nwith\nc1 = 1.16145124,\nc2 = \u22121.50204118,\nc3 = 0.58629921.\nThe same expression may be used to calculate the corresponding term \u2212DKL(q\u03b1(s)||p(s)) for the\nposterior approximation of section 3.2.\n3.4\nAdaptive regularization through optimizing the dropout rate\nThe noise parameters \u03b1 used in dropout training (e.g. the dropout rates) are usually treated as \ufb01xed\nhyperparameters, but now that we have derived dropout\u2019s variational objective (12), making these\nparameters adaptive is trivial: simply maximize the variational lower bound with respect to \u03b1. We\ncan use this to learn a separate dropout rate per layer, per neuron, of even per separate weight. In\nsection 5 we look at the predictive performance obtained by making \u03b1 adaptive.\nWe found that very large values of \u03b1 correspond to local optima from which it is hard to escape due\nto large-variance gradients. To avoid such local optima, we found it bene\ufb01cial to set a constraint\n\u03b1 \u22641 during training, i.e. we maximize the posterior variance at the square of the posterior mean,\nwhich corresponds to a dropout rate of 0.5.\n4\nRelated Work\nPioneering work in practical variational inference for neural networks was done in [8], where a\n(biased) variational lower bound estimator was introduced with good results on recurrent neural net-\nwork models. In later work [14, 17] it was shown that even more practical estimators can be formed\nfor most types of continuous latent variables or parameters using a (non-local) reparameterization\ntrick, leading to ef\ufb01cient and unbiased stochastic gradient-based variational inference. These works\nfocused on an application to latent-variable inference; extensive empirical results on inference of\nglobal model parameters were reported in [6], including succesful application to reinforcement\nlearning. These earlier works used the relatively high-variance estimator (3), upon which we im-\nprove. Variable reparameterizations have a long history in the statistics literature, but have only\nrecently found use for ef\ufb01cient gradient-based machine learning and inference [4, 13, 19]. Related\nis also probabilistic backpropagation [9], an algorithm for inferring marginal posterior probabilities;\nhowever, it requires certain tractabilities in the network making it insuitable for the type of models\nunder consideration in this paper.\nAs we show here, regularization by dropout [20, 22] can be interpreted as variational inference.\nDropConnect [21] is similar to dropout, but with binary noise on the weights rather than hidden units.\nDropConnect thus has a similar interpretation as variational inference, with a uniform prior over the\nweights, and a mixture of two Dirac peaks as posterior. In [2], standout was introduced, a variation\nof dropout where a binary belief network is learned for producing dropout rates. Recently, [15]\nproposed another Bayesian perspective on dropout. In recent work [3], a similar reparameterization\nis described and used for variational inference; their focus is on closed-form approximations of the\nvariational bound, rather than unbiased Monte Carlo estimators.\n[15] and [7] also investigate a\nBayesian perspective on dropout, but focus on the binary variant. [7] reports various encouraging\nresults on the utility of dropout\u2019s implied prediction uncertainty.\n6\n5\nExperiments\nWe compare our method to standard binary dropout and two popular versions of Gaussian dropout,\nwhich we\u2019ll denote with type A and type B. With Gaussian dropout type A we denote the pre-linear\nGaussian dropout from [20]; type B denotes the post-linear Gaussian dropout from [22]. This way,\nthe method names correspond to the matrix names in section 2 (A or B) where noise is injected.\nModels were implemented in Theano [5], and optimization was performed using Adam [12] with\ndefault hyper-parameters and temporal averaging.\nTwo types of variational dropout were included. Type A is correlated weight noise as introduced\nin section 3.2: an adaptive version of Gaussian dropout type A. Variational dropout type B has\nindependent weight uncertainty as introduced in section 3.1, and corresponds to Gaussian dropout\ntype B.\nA de facto standard benchmark for regularization methods is the task of MNIST hand-written digit\nclassi\ufb01cation. We choose the same architecture as [20]: a fully connected neural network with 3\nhidden layers and recti\ufb01ed linear units (ReLUs). We follow the dropout hyper-parameter recom-\nmendations from these earlier publications, which is a dropout rate of p = 0.5 for the hidden layers\nand p = 0.2 for the input layer. We used early stopping with all methods, where the amount of\nepochs to run was determined based on performance on a validation set.\nVariance.\nWe start out by empirically comparing the variance of the different available stochastic\nestimators of the gradient of our variational objective. To do this we train the neural network de-\nscribed above for either 10 epochs (test error 3%) or 100 epochs (test error 1.3%), using variational\ndropout with independent weight noise. After training, we calculate the gradients for the weights of\nthe top and bottom level of our network on the full training set, and compare against the gradient\nestimates per batch of M = 1000 training examples. Appendix E contains the same analysis for the\ncase of variational dropout with correlated weight noise.\nTable 1 shows that the local reparameterization trick yields the lowest variance among all variational\ndropout estimators for all conditions, although it is still substantially higher compared to not hav-\ning any dropout regularization. The 1/M variance scaling achieved by our estimator is especially\nimportant early on in the optimization when it makes the largest difference (compare weight sample\nper minibatch and weight sample per data point). The additional variance reduction obtained by our\nestimator through drawing fewer random numbers (section 2.3) is about a factor of 2, and this re-\nmains relatively stable as training progresses (compare local reparameterization and weight sample\nper data point).\ntop layer\ntop layer\nbottom layer\nbottom layer\nstochastic gradient estimator\n10 epochs\n100 epochs\n10 epochs\n100 epochs\nlocal reparameterization (ours)\n7.8 \u00d7 103\n1.2 \u00d7 103\n1.9 \u00d7 102\n1.1 \u00d7 102\nweight sample per data point (slow)\n1.4 \u00d7 104\n2.6 \u00d7 103\n4.3 \u00d7 102\n2.5 \u00d7 102\nweight sample per minibatch (standard)\n4.9 \u00d7 104\n4.3 \u00d7 103\n8.5 \u00d7 102\n3.3 \u00d7 102\nno dropout noise (minimal var.)\n2.8 \u00d7 103\n5.9 \u00d7 101\n1.3 \u00d7 102\n9.0 \u00d7 100\nTable 1: Average empirical variance of minibatch stochastic gradient estimates (1000 examples) for\na fully connected neural network, regularized by variational dropout with independent weight noise.\nSpeed.\nWe compared the regular SGVB estimator, with separate weight samples per datapoint\nwith the ef\ufb01cient estimator based on local reparameterization, in terms of wall-clock time ef\ufb01ciency.\nWith our implementation on a modern GPU, optimization with the na\u00a8\u0131ve estimator took 1635 sec-\nonds per epoch, while the ef\ufb01cient estimator took 7.4 seconds: an over 200 fold speedup.\nClassi\ufb01cation error.\nFigure 1 shows test-set classi\ufb01cation error for the tested regularization meth-\nods, for various choices of number of hidden units. Our adaptive variational versions of Gaussian\ndropout perform equal or better than their non-adaptive counterparts and standard dropout under all\ntested conditions. The difference is especially noticable for the smaller networks. In these smaller\nnetworks, we observe that variational dropout infers dropout rates that are on average far lower than\nthe dropout rates for larger networks. This adaptivity comes at negligable computational cost.\n7\n0\n200\n400\n600\n800\n1000\n1200\n1400\n# hidden units per layer\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\n2.2\n2.4\n2.6\nError on test set (%)\nMNIST classification\nNo dropout\nBinary\nGaussian (A)\nGaussian (B)\nVariational (A)\nVariational (A2)\nVariational (B)\n(a) Classi\ufb01cation error on the MNIST dataset\n1.0\n1.5\n2.0\n2.5\n3.0\nModel size k\n22\n24\n26\n28\n30\n32\n34\n36\n38\n40\nError on valid. set (%)\nNo dropout\nBinary\nGaussian dropout\nVariational dropout\n(b) Classi\ufb01cation error on the CIFAR-10 dataset\nFigure 1: Best viewed in color. (a) Comparison of various dropout methods, when applied to fully-\nconnected neural networks for classi\ufb01cation on the MNIST dataset. Shown is the classi\ufb01cation\nerror of networks with 3 hidden layers, averaged over 5 runs. he variational versions of Gaussian\ndropout perform equal or better than their non-adaptive counterparts; the difference is especially\nlarge with smaller models, where regular dropout often results in severe under\ufb01tting. (b) Compar-\nison of dropout methods when applied to convolutional net a trained on the CIFAR-10 dataset, for\ndifferent settings of network size k. The network has two convolutional layers with each 32k and\n64k feature maps, respectively, each with stride 2 and followed by a softplus nonlinearity. This is\nfollowed by two fully connected layers with each 128k hidden units.\nWe found that slightly downscaling the KL divergence part of the variational objective can be ben-\ne\ufb01cial. Variational (A2) in \ufb01gure 1 denotes performance of type A variational dropout but with a\nKL-divergence downscaled with a factor of 3; this small modi\ufb01cation seems to prevent under\ufb01tting,\nand beats all other dropout methods in the tested models.\n6\nConclusion\nEf\ufb01ciency of posterior inference using stochastic gradient-based variational Bayes (SGVB) can often\nbe signi\ufb01cantly improved through a local reparameterization where global parameter uncertainty is\ntranslated into local uncertainty per datapoint. By injecting noise locally, instead of globally at the\nmodel parameters, we obtain an ef\ufb01cient estimator that has low computational complexity, can be\ntrivially parallelized and has low variance. We show how dropout is a special case of SGVB with\nlocal reparameterization, and suggest variational dropout, a straightforward extension of regular\ndropout where optimal dropout rates are inferred from the data, rather than \ufb01xed in advance. We\nreport encouraging empirical results.\nAcknowledgments\nWe thank the reviewers and Yarin Gal for valuable feedback. Diederik Kingma is supported by the\nGoogle European Fellowship in Deep Learning, Max Welling is supported by research grants from\nGoogle and Facebook, and the NWO project in Natural AI (NAI.14.108).\nReferences\n[1] Ahn, S., Korattikara, A., and Welling, M. (2012). Bayesian posterior sampling via stochastic gradient\nFisher scoring. arXiv preprint arXiv:1206.6380.\n[2] Ba, J. and Frey, B. (2013). Adaptive dropout for training deep neural networks. In Advances in Neural\nInformation Processing Systems, pages 3084\u20133092.\n[3] Bayer, J., Karol, M., Korhammer, D., and Van der Smagt, P. (2015). Fast adaptive weight noise. arXiv\npreprint arXiv:1507.05331.\n[4] Bengio, Y. (2013).\nEstimating or propagating gradients through stochastic neurons.\narXiv preprint\narXiv:1305.2982.\n8\n[5] Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., Turian, J., Warde-Farley,\nD., and Bengio, Y. (2010). Theano: a CPU and GPU math expression compiler. In Proceedings of the\nPython for Scienti\ufb01c Computing Conference (SciPy), volume 4.\n[6] Blundell, C., Cornebise, J., Kavukcuoglu, K., and Wierstra, D. (2015). Weight uncertainty in neural net-\nworks. arXiv preprint arXiv:1505.05424.\n[7] Gal, Y. and Ghahramani, Z. (2015). Dropout as a Bayesian approximation: Representing model uncertainty\nin deep learning. arXiv preprint arXiv:1506.02142.\n[8] Graves, A. (2011). Practical variational inference for neural networks. In Advances in Neural Information\nProcessing Systems, pages 2348\u20132356.\n[9] Hern\u00b4andez-Lobato, J. M. and Adams, R. P. (2015). Probabilistic backpropagation for scalable learning of\nBayesian neural networks. arXiv preprint arXiv:1502.05336.\n[10] Hinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. R. (2012). Improving\nneural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580.\n[11] Hinton, G. E. and Van Camp, D. (1993). Keeping the neural networks simple by minimizing the descrip-\ntion length of the weights. In Proceedings of the sixth annual conference on Computational learning theory,\npages 5\u201313. ACM.\n[12] Kingma, D. and Ba, J. (2015). Adam: A method for stochastic optimization. Proceedings of the Interna-\ntional Conference on Learning Representations 2015.\n[13] Kingma, D. P. (2013). Fast gradient-based inference with continuous latent variable models in auxiliary\nform. arXiv preprint arXiv:1306.0733.\n[14] Kingma, D. P. and Welling, M. (2014). Auto-encoding variational Bayes. Proceedings of the 2nd Inter-\nnational Conference on Learning Representations.\n[15] Maeda, S.-i. (2014). A Bayesian encourages dropout. arXiv preprint arXiv:1412.7003.\n[16] Neal, R. M. (1995). Bayesian learning for neural networks. PhD thesis, University of Toronto.\n[17] Rezende, D. J., Mohamed, S., and Wierstra, D. (2014). Stochastic backpropagation and approximate\ninference in deep generative models.\nIn Proceedings of the 31st International Conference on Machine\nLearning (ICML-14), pages 1278\u20131286.\n[18] Robbins, H. and Monro, S. (1951). A stochastic approximation method. The Annals of Mathematical\nStatistics, 22(3):400\u2013407.\n[19] Salimans, T. and Knowles, D. A. (2013). Fixed-form variational posterior approximation through stochas-\ntic linear regression. Bayesian Analysis, 8(4).\n[20] Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2014). Dropout: A simple\nway to prevent neural networks from over\ufb01tting. The Journal of Machine Learning Research, 15(1):1929\u2013\n1958.\n[21] Wan, L., Zeiler, M., Zhang, S., Cun, Y. L., and Fergus, R. (2013). Regularization of neural networks using\ndropconnect. In Proceedings of the 30th International Conference on Machine Learning (ICML-13), pages\n1058\u20131066.\n[22] Wang, S. and Manning, C. (2013).\nFast dropout training.\nIn Proceedings of the 30th International\nConference on Machine Learning (ICML-13), pages 118\u2013126.\n[23] Welling, M. and Teh, Y. W. (2011). Bayesian learning via stochastic gradient Langevin dynamics. In\nProceedings of the 28th International Conference on Machine Learning (ICML-11), pages 681\u2013688.\n9\nA\nFloating-point numbers and compression\nThe \ufb02oating-point format is by far the most common number format used for model parameters in neural\nnetwork type models. The absolute value of \ufb02oating-point numbers in base 2 equals sign \u00b7 (s/2p\u22121) \u00b7 2e where\ns is the mantissa with p bits and e is the exponent, both non-negative integer-valued. For random bit patterns for\ns and e, \ufb02oating point numbers closely follow a log-uniform distribution with \ufb01nite range. The \ufb02oating-point\nformat is thus close to optimal, from a compression point of view, for communicating parameters for which the\nreceiver has a log-uniform prior distribution, with \ufb01nite range, over the magnitude of parameters. Speci\ufb01cally,\nthis prior for each individual \ufb02oating-point parameter wj is p(log |wj|) = U(interval), where the interval is\napproximately (log(2\u22128), log(28)) for single-point precision and (log(2\u221211), log(211)) for double-precision\n\ufb02oating-point numbers in the most common IEEE speci\ufb01cation.\nThe KL-divergence between a variational posterior q\u03c6 (eq. (1)) and this log-uniform prior is relatively simple:\nit equals the entropy under q\u03c6 of the log-transformed magnitude of w plus a constant:\n\u2212DKL(q\u03c6(w)||p(w)) = H(q\u03c6(log |w|)) + constant\n(13)\nThis divergence has a natural interpretation as controlling the number of signi\ufb01cant digits under q\u03c6 of the\ncorresponding \ufb02oating-point number.\nB\nDerivation of dropout\u2019s implicit variational posterior\nIn the common version of dropout [20], linear operations bi = aiW in a model, where ai is a column vector\nthat is the result of previous operations on the input for datapoint i, are replaced by a stochastic operation:\nbi = (ai \u2299(di/(1 \u2212p))W\n(14)\ni.e.:\nbi\nk =\nX\nj\nWjkai\njdi\nj/(1 \u2212p)\n(15)\ndi\nj \u223cBernoulli(1 \u2212p)\n(16)\np = dropout rate (hyper-parameter, e.g. 0.5)\n(17)\nwhere column vector ai is the result of previous operations on the input for datapoint i. Expected values and\nvariance w.r.t. the dropout rate are:\nE\nh\ndi\nj\ni\n= (1 \u2212p)\n(18)\nE\nh\ndi\nj/(1 \u2212p)\ni\n= 1\n(19)\nVar\nh\ndi\nj/(1 \u2212p)\ni\n= Var\nh\ndi\nj\ni\n/(1 \u2212p)2 = p/(1 \u2212p)\n(20)\n(21)\nThe expected value and variance for the elements of the resulting vector bi are:\nE\nh\nbi\nk\ni\n= E\n\"X\nj\nWjkai\njdi\nj/(1 \u2212p)\n#\n(22)\n=\nX\nj\nWjkai\njE\nh\ndi\nj/(1 \u2212p)\ni\n(23)\n=\nX\nj\nWjkai\nj\n(24)\nVar\nh\nbi\nk\ni\n= Var\n\"X\nj\nWjkai\njdi\nj/(1 \u2212p)\n#\n(25)\n=\nX\nj\nVar\nh\nWjkai\njdi\nj/(1 \u2212p)\ni\n(26)\n=\nX\nj\nW 2\njk(ai\nj)2Var\nh\ndi\nj/(1 \u2212p)\ni\n(27)\n= p/(1 \u2212p)\nX\nj\nW 2\njk(ai\nj)2\n(28)\n10\nB.1\nGaussian dropout\nThus, ignoring dependencies between elements of bi, dropout can be approximated by a Gaussian [22]:\np(bi|ai, W) = N(aiW, p(1 \u2212p)\u22121(ai)2(W)2)\n(29)\ni.e.:\nbi = aiW + \u03f5i \u2299\np\np(1 \u2212p)\u22121(ai)2(W)2\n(30)\nwhere:\n\u03f5 \u223cN(0, I)\n(31)\nwhere (.)2,\np\n(.) and \u2299are again component-wise operations.\nB.2\nWeight uncertainty\nEquivalently, the Gaussian dropout relationship p(bi|ai, W) can be parameterized as weight uncertainty:\nbi = aiV\n(32)\np(Vjk|Wjk, \u03b1jk) = N(Wjk, \u03b1jkW 2\njk)\n(33)\n\u03b1jk = p/(1 \u2212p)\n(34)\n\u21d2\np(bi|ai, W) = N(aiW, p(1 \u2212p)\u22121(ai)2(W)2)\n(35)\nIn the main text, we show how we can treat the distribution over weights p(Vjk|Wjk, \u03b1jk) as a variational\nposterior q\u03c6, which we can optimize w.r.t. each \u03b1jk (the dropout rate for individual parameters) to optimize a\nbound on the marginal likelihood of the data.\nC\nNegative KL-divergence for the log-uniform prior\nThe variational lower bound (1) that we wish to optimize is given by\nL(\u03c6) = \u2212DKL(q\u03c6(w)||p(w)) + LD(\u03c6),\nwhere LD(\u03c6) is the expected log likelihood term which we approximate using Monte Carlo, as dis-\ncussed in section 2.3. Here we discuss how to deterministically compute the negative KL divergence term\n\u2212DKL(q\u03c6(w)||p(w)) for the log-uniform prior proposed in section B. For simplicity we only discuss the case\nof factorizing approximate posterior q\u03c6(w) = Qk\ni=1 q\u03c6(wi), although the extension to more involved posterior\napproximations is straight forward.\nOur log-uniform prior p(wi) (appendix A) consists of a Bernoulli distribution on the sign bit si \u2208{\u22121, +1}\nwhich captures the sign of wi, combined with a uniform prior on the log-scale of wi:\nwi\n=\nsi|wi|\np(si)\n=\nBernoulli(0.5) on {\u22121, +1}\np(log(|wi|))\n\u221d\nc\n(36)\nFor an approximate posterior q\u03c6(wi) with support on the real line, the KL divergence will thus consist of two\nparts: the KL-divergence between the posterior and prior on the sign bit si and the KL-divergence between the\nconditional posterior and prior on the log scale log(|wi|).\nThe negative KL divergence between the posterior and prior on the sign bit is given by:\n\u2212DKL[q\u03c6(si)|p(si)] = log(0.5) \u2212Q\u03c6(0) log[Q\u03c6(0)] \u2212[1 \u2212Q\u03c6(0)] log[1 \u2212Q\u03c6(0)],\n(37)\nwith Q\u03c6(0) the posterior CDF of wi evaluated at 0, i.e. the probability of drawing a negative weight from our\napproximate posterior.\nThe negative KL divergence of the second part (the log-scale) is then\n\u2212DKL[q\u03c6(log(|wi|)|si)|p(log(|wi|))] = log(c) \u2212Q\u03c6(0)Eq\u03c6(wi|wi<0) [log(q\u03c6(wi)/Q\u03c6(0)) + log(|wi|)]\n\u2212(1 \u2212Q\u03c6(0)) Eq\u03c6(wi|wi>0) [log(q\u03c6(wi)/(1 \u2212Q\u03c6(0))) + log(|wi|)] ,\nwhere q\u03c6(wi)/Q\u03c6(0) is the truncation of our approximate posterior to the negative part of the real-line, and\nthe log(|wi|) term enters through transforming the uniform prior from the log-space to the normal space.\nPutting both parts together the terms involving the CDF Q\u03c6(0) cancel, and we get:\n\u2212DKL[q\u03c6(wi)|p(wi)] = log(c) + log(0.5) + H(q\u03c6(wi)) \u2212Eq log(|wi|),\n(38)\nwhere H(q\u03c6(wi)) is the entropy of our approximate posterior.\n11\nWe now consider the special case of a dropout posterior, as proposed in section 3, which we de\ufb01ned as any\napproximate posterior distribution q\u03c6(wi) for which the weight noise is multiplicative:\nwi = \u03b8i\u03f5i, with \u03f5i \u223cq\u03b1(\u03f5i),\n(39)\nwhere we have divided the parameters \u03c6i = (\u03b8i, \u03b1) into parameters governing the mean of q\u03c6(wi) (assuming\nEq\u03f5i = 1) and the multiplicative noise. Note that certain families of distributions, such as Gaussians, can\nalways be reparameterized in this way.\nAssuming q\u03b1(\u03f5i) is a continuous distribution, this choice of posterior gives us\nH(q\u03c6(wi)) = H(q\u03b1(\u03f5i)) + log(|\u03b8i|),\n(40)\nwhere the last term can be understood as the log jacobian determinant of a linear transformation of the random\nvariable \u03f5i.\nFurthermore, we can rewrite the \u2212Eq log(|wi|) term of (38) as\n\u2212Eq log(|wi|) = \u2212Eq log(|\u03b8i\u03f5i|) = \u2212log(|\u03b8i|) \u2212Eq log(|\u03f5i|).\n(41)\nTaking (40) and (41) together, the terms depending on \u03b8 thus cancel exactly, proving that the KL divergence\nbetween posterior and prior is indeed independent of these parameters:\n\u2212DKL[q\u03c6(wi)|p(wi)] = log(c) + log(0.5) + H(q\u03c6(wi)) \u2212Eq log(|wi|)\n= log(c) + log(0.5) + H(q\u03b1(\u03f5i)) \u2212Eq\u03b1 log(|\u03f5i|).\n(42)\nIn fact, the log-uniform prior is the only prior consistent with dropout. If the prior had any additional terms\ninvolving wi, their expectation w.r.t. q\u03c6 would not cancel with the entropy term in \u2212DKL[q\u03c6(wi)|p(wi)].\nFor the speci\ufb01c case of Gaussian dropout, as proposed in section 3, we have q\u03b1(\u03f5i) = N(1, \u03b1). This then\ngives us\n\u2212DKL[q\u03c6(wi)|p(wi)] = log(c) + log(0.5) + 0.5(1 + log(2\u03c0) + log(\u03b1)) \u2212Eq\u03b1 log(|\u03f5i|).\nThe \ufb01nal term Eq\u03b1 log(|\u03f5i|) in this expression is not analytically tractable, but it can be pre-computed numer-\nically for all values of \u03b1 in the relevant range and then approximated using a 3rd degree polynomial in \u03b1. As\nshown in \ufb01gure 2, this approximation is extremely close.\nFor Gaussian dropout this then gives us:\n\u2212DKL[q\u03c6(wi)|p(wi)] \u2248constant + 0.5 log(\u03b1) + c1\u03b1 + c2\u03b12 + c3\u03b13,\nwith\nc1 = 1.16145124,\nc2 = \u22121.50204118,\nc3 = 0.58629921.\nAlternatively, we can make use of the fact that \u2212Eq\u03b1 log(|\u03f5i|) \u22650 for all values of \u03b1, and use the following\nlower bound:\n\u2212DKL[q\u03c6(wi)|p(wi)] \u2265constant + 0.5 log(\u03b1).\n\u22123\n\u22122.5\n\u22122\n\u22121.5\n\u22121\n\u22120.5\n0\n\u22121.5\n\u22121\n\u22120.5\n0\nlog alpha\nnats\n \n \nexact relative neg. KL\u2212div.\n3rd order polynomial approx.\n0.5*log(alpha) lower bound\nFigure 2: Exact and approximate negative KL\ndivergence \u2212DKL[q\u03c6(wi)|p(wi)] for the log-\nuniform prior and Gaussian dropout approximate\nposterior. Since we use an improper prior, the con-\nstant is arbitrary; we choose it so that the exact\nKL divergence is 0 at \u03b1 = 1, which is the max-\nimum value we allow during optimization of the\nlower bound. The exact and approximate results\nare indistinguishable on the domain shown, which\ncorresponds to dropout rates between p = 0.05\nand p = 0.5. For log(\u03b1) \u2192\u2212\u221ethe approxi-\nmated term vanishes, so that both our polynomial\napproximation and the lower bound become exact.\n12\nD\nVariance reduction of local parameterization compared to separate\nweight samples\nIn section 2.3 we claim that our local reparameterization not only is more computationally ef\ufb01cient than drawing\nseparate weight matrices for each training example, but that it also leads to an estimator that has lower variance.\nTo see why, consider the stochastic gradient estimate with respect to the posterior parameter \u03c32\ni,j for a minibatch\nof size M = 1. Drawing random weights W to form the estimate, we get\n\u2202LSGVB\nD\n\u2202\u03c32\ni,j\n= \u2202LSGVB\nD\n\u2202bm,j\n\u03f5i,jam,i\n2\u03c3i,j\n.\n(43)\nIf, on the other hand, we form the same gradient using the local reparameterization trick, we get\n\u2202LSGVB\nD\n\u2202\u03c32\ni,j\n= \u2202LSGVB\nD\n\u2202bm,j\n\u03b6m,ja2\nm,i\n2\np\n\u03b4m,j\n.\n(44)\nWe can decompose the variance of these estimators as\nVarq\u03c6,D\n\u0014\u2202LSGVB\nD\n\u2202\u03c32\ni,j\n\u0015\n= Varbm,j\n\u0014\nEq\u03c6,D\n\u0014\u2202LSGVB\nD\n\u2202\u03c32\ni,j\n|bm,j\n\u0015\u0015\n+ Ebm,j\n\u0014\nVarq\u03c6,D\n\u0014\u2202LSGVB\nD\n\u2202\u03c32\ni,j\n|bm,j\n\u0015\u0015\n.\nHere, the \ufb01rst term is identical for both gradient estimators, but the second term is not. When drawing separate\nweight matrices per training example, it is\nEbm,j\n\u0014\nVarq\u03c6,D\n\u0014\u2202LSGVB\nD\n\u2202\u03c32\ni,j\n|bm,j\n\u0015\u0015\n= Ebm,j\n\u0014\nVarq\u03c6,D\n\u0014\u2202LSGVB\nD\n\u2202bm,j |bm,j\n\u0015\nq\n\u0015\n+Ebm,j\n\"\nEq\u03c6,D\n\"\u0012\u2202LSGVB\nD\n\u2202bm,j\n\u00132\n|bm,j\n#\nVarq\u03c6,D\n\u0002\n\u03f52\ni,j|bm,j\n\u0003\na2\nm,i\n4\u03c32\ni,j\n#\n,\n(45)\nwith q = (bm,j \u2212\u03b3m,j)2a4\nm,i/4.\nWhen using the local reparameterization trick, we simply have\nEbm,j\n\u0014\nVarq\u03c6,D\n\u0014\u2202LSGVB\nD\n\u2202\u03c32\ni,j\n|bm,j\n\u0015\u0015\n= Ebm,j\n\u0014\nVarq\u03c6,D\n\u0014\u2202LSGVB\nD\n\u2202bm,j |bm,j\n\u0015\nq\n\u0015\n.\n(46)\nIn this case, the second term vanishes because the random variable \u03b6m,j is uniquely determined given bm,j,\nas there is only a single \u03b6m,j for each bm,j. Because there are many \u03f5i,j for each bm,j, this variable is not\nuniquely determined by conditioning on bm,j, so there is an additional (positive) contribution to the variance.\nE\nVariance of stochastic gradients for variational dropout with correlated\nweight noise\nIn section 5 we empirically compare the variance of the different available stochastic estimators of the gra-\ndient of our variational objective for a model regularized using variational dropout with independent weight\nnoise (section3.1). Here we present the same analysis for variational dropout with correlated weight noise\n(section3.2). As table 2 shows, the relative performance of the different estimators is similar to that reported in\nsection 5, although the noise level is generally much higher using this regularization method.\ntop layer\ntop layer\nbottom layer\nbottom layer\nstochastic gradient estimator\n10 epochs\n100 epochs\n10 epochs\n100 epochs\nlocal reparameterization (ours)\n2.9 \u00d7 104\n5.8 \u00d7 103\n8.8 \u00d7 102\n5.3 \u00d7 102\nweight sample per minibatch (standard)\n3.1 \u00d7 105\n2.0 \u00d7 104\n5.0 \u00d7 103\n1.1 \u00d7 103\nno dropout noise (minimal var.)\n7.1 \u00d7 103\n9.8 \u00d7 102\n4.3 \u00d7 102\n1.2 \u00d7 102\nTable 2: Average empirical variance of minibatch stochastic gradient estimates (1000 examples) for\na fully connected neural network, regularized by variational dropout with correlated weight noise.\nFor this speci\ufb01cation, the variance of the stochastic gradient estimator with separate weight samples\nfor each data point is identical to that using the local reparameterization trick: Ignoring optimization\nof dropout rates, both methods correspond to the standard dropout training procedure.\n13\nF\nVariance of SGVB estimator with minibatches of datapoints without\nreplacement\nUsing the indicator variables si \u2208S, to denote whether the i-th training example is included in our minibatch,\nand Li(\u03f5, \u03c6) as shorthand for log p(yi|xi, w = f(\u03f5, \u03c6)), the Monte Carlo estimator (3) may be rewritten as\nLSGVB\nD\n(\u03c6) = N\nM\nN\nX\ni=1\nsiLi(\u03f5, \u03c6).\n(47)\nThe variance of which is given by\nVar\u03f5,S\nh\nLSGVB\nD\n(\u03c6)\ni\n\u2264N 2\nM 2\nN\nX\ni=1\nVarS [si] LD(\u03c6)2\nN 2\n+ Var\u03f5 [Li(\u03f5, \u03c6)] ES\n\u0002\ns2\ni\n\u0003\n+ 2N 2\nM 2\nX\ni>j\nES [si] ES [sj] Cov\u03f5 [Li(\u03f5, \u03c6), Lj(\u03f5, \u03c6)]\n\u2264N\nM\n N\nX\ni=1\nLD(\u03c6)2\nN 2\n+ Var\u03f5 [Li(\u03f5, \u03c6)]\n!\n+ 2\nX\ni>j\nCov\u03f5 [Li(\u03f5, \u03c6), Lj(\u03f5, \u03c6)] ,\n(48)\nwhere the inequalities arise through ignoring the negative correlations between the si, and where we use that\nES [si] = ES\n\u0002\ns2\ni\n\u0003\n= M/N, and VarS [si] \u2264M/N, for N > 2M.\nAs can be seen, the variance contributed by the random selection of data points from D is inversely proportional\nto the minibatch size M. However, the variance contributed by our random sampling of \u03f5 does not decrease with\nM, as we are using a single random sample for the entire minibatch and Cov\u03f5 [Li(\u03f5, \u03c6), Lj(\u03f5, \u03c6)] is positive\non average2. In practice, this means that the variance of LSGVB\nD\n(\u03c6) can be dominated by \u03f5 for even moderately\nlarge M.\n2We have Exi,yi [Li(\u03f5, \u03c6)] = Exj,yj [Lj(\u03f5, \u03c6)] if the examples in the training data are i.i.d., which means\nthat Exi,yi,xj,yj [Cov\u03f5 [Li(\u03f5, \u03c6), Lj(\u03f5, \u03c6)]] = Var\u03f5\n\u0002\nExi,yi [Li(\u03f5, \u03c6)]\n\u0003\n\u22650.\n14\n",
        "sentence": " , Gaussian noise), which could lead to a better approximation of the marginalized loss (Wang & Manning, 2013; Kingma et al., 2015). Some works tried to optimize the hyper-parameters that define the noise level in a Bayesian framework (Zhuo et al., 2015; Kingma et al., 2015).",
        "context": "of independent noise variables \u03be. By adding noise to the input during training, the weight parame-\nters \u03b8 are less likely to over\ufb01t to the training data, as shown empirically by previous publications.\nlikelihood. The original Gaussian dropout sampling procedure (9) can then be interpreted as arising\nfrom a local reparameterization of our posterior on the weights W.\n3.3\nDropout\u2019s scale-invariant prior and variational objective\nweight noise\nIn section 5 we empirically compare the variance of the different available stochastic estimators of the gra-\ndient of our variational objective for a model regularized using variational dropout with independent weight"
    },
    {
        "title": "Learning multiple layers of features from tiny",
        "author": [
            "Krizhevsky",
            "Alex",
            "Hinton",
            "Geoffrey"
        ],
        "venue": null,
        "citeRegEx": "Krizhevsky et al\\.,? \\Q2009\\E",
        "shortCiteRegEx": "Krizhevsky et al\\.",
        "year": 2009,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing",
        "author": [
            "Krizhevsky",
            "Alex",
            "Sutskever",
            "Ilya",
            "Hinton",
            "Geoffrey E"
        ],
        "venue": null,
        "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E",
        "shortCiteRegEx": "Krizhevsky et al\\.",
        "year": 2012,
        "abstract": "",
        "full_text": "",
        "sentence": " Dropout has been widely used to avoid overfitting of deep neural networks with a large number of parameters (Krizhevsky et al., 2012; Srivastava et al., 2014), which usually identically and independently at random samples neurons and sets their outputs to be zeros. The training procedure is similar to (Krizhevsky et al., 2012), that is using mini-batch SGD with momentum (0. , step size) is decreased after a number of epochs similar to what was done in previous works (Krizhevsky et al., 2012).",
        "context": null
    },
    {
        "title": "Attribute efficient linear regression with distribution-dependent sampling",
        "author": [
            "Kukliansky",
            "Doron",
            "Shamir",
            "Ohad"
        ],
        "venue": "In Proceedings of the 32nd International Conference on Machine Learning (ICML),",
        "citeRegEx": "Kukliansky et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "Kukliansky et al\\.",
        "year": 2015,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Gradient-based learning applied to document recognition",
        "author": [
            "LeCun",
            "Yann",
            "Bottou",
            "L\u00e9on",
            "Bengio",
            "Yoshua",
            "Haffner",
            "Patrick"
        ],
        "venue": "Proceedings of the IEEE,",
        "citeRegEx": "LeCun et al\\.,? \\Q1998\\E",
        "shortCiteRegEx": "LeCun et al\\.",
        "year": 1998,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " In the next three subsections, we present the results on three benchmark data sets for comparing e-dropout and s-dropout: MNIST (LeCun et al., 1998), CIFAR-10 and CIFAR-100 (Krizhevsky & Hinton, 2009). The MNIST (LeCun et al., 1998) data set has 60,000 training images and 10,000 testing images.",
        "context": null
    },
    {
        "title": "Optimizing neural networks with kronecker-factored approximate curvature",
        "author": [
            "Martens",
            "James",
            "Grosse",
            "Roger"
        ],
        "venue": "arXiv preprint arXiv:1503.05671,",
        "citeRegEx": "Martens et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "Martens et al\\.",
        "year": 2015,
        "abstract": "",
        "full_text": "",
        "sentence": " Dropout is a simple yet effective technique to prevent overfitting in training deep neural networks (Srivastava et al., 2014). It has received much attention recently from researchers to study its practical and theoretical properties. Notably, Wager et al. (2013); Baldi & Sadowski (2013) have analyzed the dropout from a theoretical viewpoint and found that dropout is equivalent to a data-dependent regularizer. Dropout is a simple yet effective technique to prevent overfitting in training deep neural networks (Srivastava et al., 2014). It has received much attention recently from researchers to study its practical and theoretical properties. Notably, Wager et al. (2013); Baldi & Sadowski (2013) have analyzed the dropout from a theoretical viewpoint and found that dropout is equivalent to a data-dependent regularizer.",
        "context": null
    },
    {
        "title": "Path-sgd: Path-normalized optimization in deep neural networks",
        "author": [
            "Neyshabur",
            "Behnam",
            "Salakhutdinov",
            "Ruslan R",
            "Srebro",
            "Nati"
        ],
        "venue": "In Advances in Neural Information Processing Systems,",
        "citeRegEx": "Neyshabur et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "Neyshabur et al\\.",
        "year": 2015,
        "abstract": "",
        "full_text": "",
        "sentence": " Recently, there emerge a battery of studies trying to accelearte the optimization of deep learning (Sutskever et al., 2013; Neyshabur et al., 2015; Zhang et al., 2014; Martens & Grosse, 2015; Ioffe & Szegedy, 2015; Kingma & Ba, 2014), which tackle the problem from",
        "context": null
    },
    {
        "title": "Factored 3-way restricted boltzmann machines for modeling natural images",
        "author": [
            "Ranzato",
            "Marc\u2019Aurelio",
            "Krizhevsky",
            "Alex",
            "Hinton",
            "Geoffrey E"
        ],
        "venue": "In AISTATS,",
        "citeRegEx": "Ranzato et al\\.,? \\Q2010\\E",
        "shortCiteRegEx": "Ranzato et al\\.",
        "year": 2010,
        "abstract": "",
        "full_text": "",
        "sentence": " In this sense, our theoretical analysis also explains why Z-normalization usually speeds up the training (Ranzato et al., 2010).",
        "context": null
    },
    {
        "title": "Stochastic convex optimization",
        "author": [
            "Shalev-Shwartz",
            "Shai",
            "Shamir",
            "Ohad",
            "Srebro",
            "Nathan",
            "Sridharan",
            "Karthik"
        ],
        "venue": "In The 22nd Conference on Learning Theory (COLT),",
        "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2009\\E",
        "shortCiteRegEx": "Shalev.Shwartz et al\\.",
        "year": 2009,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Dropout: A simple way to prevent neural networks from overfitting",
        "author": [
            "Srivastava",
            "Nitish",
            "Hinton",
            "Geoffrey",
            "Krizhevsky",
            "Alex",
            "Sutskever",
            "Ilya",
            "Salakhutdinov",
            "Ruslan"
        ],
        "venue": "The Journal of Machine Learning Research,",
        "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E",
        "shortCiteRegEx": "Srivastava et al\\.",
        "year": 1929,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "On the importance of initialization and momentum in deep learning",
        "author": [
            "Sutskever",
            "Ilya",
            "Martens",
            "James",
            "Dahl",
            "George",
            "Hinton",
            "Geoffrey"
        ],
        "venue": "In Proceedings of the 30th international conference on machine learning",
        "citeRegEx": "Sutskever et al\\.,? \\Q2013\\E",
        "shortCiteRegEx": "Sutskever et al\\.",
        "year": 2013,
        "abstract": "",
        "full_text": "",
        "sentence": " Recently, there emerge a battery of studies trying to accelearte the optimization of deep learning (Sutskever et al., 2013; Neyshabur et al., 2015; Zhang et al., 2014; Martens & Grosse, 2015; Ioffe & Szegedy, 2015; Kingma & Ba, 2014), which tackle the problem from",
        "context": null
    },
    {
        "title": "Dropout training as adaptive regularization",
        "author": [
            "Wager",
            "Stefan",
            "Wang",
            "Sida",
            "Liang",
            "Percy S"
        ],
        "venue": "In Advances in Neural Information Processing Systems,",
        "citeRegEx": "Wager et al\\.,? \\Q2013\\E",
        "shortCiteRegEx": "Wager et al\\.",
        "year": 2013,
        "abstract": "Dropout and other feature noising schemes control overfitting by artificially\ncorrupting the training data. For generalized linear models, dropout performs a\nform of adaptive regularization. Using this viewpoint, we show that the dropout\nregularizer is first-order equivalent to an L2 regularizer applied after\nscaling the features by an estimate of the inverse diagonal Fisher information\nmatrix. We also establish a connection to AdaGrad, an online learning\nalgorithm, and find that a close relative of AdaGrad operates by repeatedly\nsolving linear dropout-regularized problems. By casting dropout as\nregularization, we develop a natural semi-supervised algorithm that uses\nunlabeled data to create a better adaptive regularizer. We apply this idea to\ndocument classification tasks, and show that it consistently boosts the\nperformance of dropout training, improving on state-of-the-art results on the\nIMDB reviews dataset.",
        "full_text": "Dropout Training as Adaptive Regularization\nStefan Wager\u2217, Sida Wang\u2020, and Percy Liang\u2020\nDepartments of Statistics\u2217and Computer Science\u2020\nStanford University, Stanford, CA-94305\nswager@stanford.edu, {sidaw, pliang}@cs.stanford.edu\nAbstract\nDropout and other feature noising schemes control over\ufb01tting by arti\ufb01cially cor-\nrupting the training data. For generalized linear models, dropout performs a form\nof adaptive regularization. Using this viewpoint, we show that the dropout regular-\nizer is \ufb01rst-order equivalent to an L2 regularizer applied after scaling the features\nby an estimate of the inverse diagonal Fisher information matrix. We also establish\na connection to AdaGrad, an online learning algorithm, and \ufb01nd that a close rel-\native of AdaGrad operates by repeatedly solving linear dropout-regularized prob-\nlems. By casting dropout as regularization, we develop a natural semi-supervised\nalgorithm that uses unlabeled data to create a better adaptive regularizer. We ap-\nply this idea to document classi\ufb01cation tasks, and show that it consistently boosts\nthe performance of dropout training, improving on state-of-the-art results on the\nIMDB reviews dataset.\n1\nIntroduction\nDropout training was introduced by Hinton et al. [1] as a way to control over\ufb01tting by randomly\nomitting subsets of features at each iteration of a training procedure.1 Although dropout has proved\nto be a very successful technique, the reasons for its success are not yet well understood at a theo-\nretical level.\nDropout training falls into the broader category of learning methods that arti\ufb01cially corrupt train-\ning data to stabilize predictions [2, 4, 5, 6, 7]. There is a well-known connection between arti\ufb01cial\nfeature corruption and regularization [8, 9, 10]. For example, Bishop [9] showed that the effect of\ntraining with features that have been corrupted with additive Gaussian noise is equivalent to a form\nof L2-type regularization in the low noise limit. In this paper, we take a step towards understand-\ning how dropout training works by analyzing it as a regularizer. We focus on generalized linear\nmodels (GLMs), a class of models for which feature dropout reduces to a form of adaptive model\nregularization.\nUsing this framework, we show that dropout training is \ufb01rst-order equivalent to L2-regularization af-\nter transforming the input by diag(\u02c6I)\u22121/2, where \u02c6I is an estimate of the Fisher information matrix.\nThis transformation effectively makes the level curves of the objective more spherical, and so bal-\nances out the regularization applied to different features. In the case of logistic regression, dropout\ncan be interpreted as a form of adaptive L2-regularization that favors rare but useful features.\nThe problem of learning with rare but useful features is discussed in the context of online learning\nby Duchi et al. [11], who show that their AdaGrad adaptive descent procedure achieves better regret\nbounds than regular stochastic gradient descent (SGD) in this setting. Here, we show that AdaGrad\nS.W. is supported by a B.C. and E.J. Eaves Stanford Graduate Fellowship.\n1Hinton et al. introduced dropout training in the context of neural networks speci\ufb01cally, and also advocated\nomitting random hidden layers during training. In this paper, we follow [2, 3] and study feature dropout as a\ngeneric training method that can be applied to any learning algorithm.\n1\narXiv:1307.1493v2  [stat.ML]  1 Nov 2013\nand dropout training have an intimate connection: Just as SGD progresses by repeatedly solving\nlinearized L2-regularized problems, a close relative of AdaGrad advances by solving linearized\ndropout-regularized problems.\nOur formulation of dropout training as adaptive regularization also leads to a simple semi-supervised\nlearning scheme, where we use unlabeled data to learn a better dropout regularizer. The approach\nis fully discriminative and does not require \ufb01tting a generative model. We apply this idea to several\ndocument classi\ufb01cation problems, and \ufb01nd that it consistently improves the performance of dropout\ntraining. On the benchmark IMDB reviews dataset introduced by [12], dropout logistic regression\nwith a regularizer tuned on unlabeled data outperforms previous state-of-the-art. In follow-up re-\nsearch [13], we extend the results from this paper to more complicated structured prediction, such\nas multi-class logistic regression and linear chain conditional random \ufb01elds.\n2\nArti\ufb01cial Feature Noising as Regularization\nWe begin by discussing the general connections between feature noising and regularization in gen-\neralized linear models (GLMs). We will apply the machinery developed here to dropout training in\nSection 4.\nA GLM de\ufb01nes a conditional distribution over a response y \u2208Y given an input feature vector\nx \u2208Rd:\np\u03b2(y | x)\ndef\n= h(y) exp{y x \u00b7 \u03b2 \u2212A(x \u00b7 \u03b2)},\n\u2113x,y(\u03b2)\ndef\n= \u2212log p\u03b2(y | x).\n(1)\nHere, h(y) is a quantity independent of x and \u03b2, A(\u00b7) is the log-partition function, and \u2113x,y(\u03b2) is the\nloss function (i.e., the negative log likelihood); Table 1 contains a summary of notation. Common\nexamples of GLMs include linear (Y = R), logistic (Y = {0, 1}), and Poisson (Y = {0, 1, 2, . . . })\nregression.\nGiven n training examples (xi, yi), the standard maximum likelihood estimate \u02c6\u03b2 \u2208Rd minimizes\nthe empirical loss over the training examples:\n\u02c6\u03b2\ndef\n= arg min\n\u03b2\u2208Rd\nn\nX\ni=1\n\u2113xi, yi(\u03b2).\n(2)\nWith arti\ufb01cial feature noising, we replace the observed feature vectors xi with noisy versions \u02dcxi =\n\u03bd(xi, \u03bei), where \u03bd is our noising function and \u03bei is an independent random variable. We \ufb01rst create\nmany noisy copies of the dataset, and then average out the auxiliary noise. In this paper, we will\nconsider two types of noise:\n\u2022 Additive Gaussian noise: \u03bd(xi, \u03bei) = xi + \u03bei, where \u03bei \u223cN(0, \u03c32Id\u00d7d).\n\u2022 Dropout noise: \u03bd(xi, \u03bei) = xi \u2299\u03bei, where \u2299is the elementwise product of two vec-\ntors. Each component of \u03bei \u2208{0, (1 \u2212\u03b4)\u22121}d is an independent draw from a scaled\nBernoulli(1 \u2212\u03b4) random variable. In other words, dropout noise corresponds to setting \u02dcxij\nto 0 with probability \u03b4 and to xij/(1 \u2212\u03b4) else.2\nIntegrating over the feature noise gives us a noised maximum likelihood parameter estimate:\n\u02c6\u03b2 = arg min\n\u03b2\u2208Rd\nn\nX\ni=1\nE\u03be [\u2113\u02dcxi, yi(\u03b2)] , where E\u03be [Z]\ndef\n= E [Z | {xi, yi}]\n(3)\nis the expectation taken with respect to the arti\ufb01cial feature noise \u03be = (\u03be1, . . . , \u03ben). Similar expres-\nsions have been studied by [9, 10].\nFor GLMs, the noised empirical loss takes on a simpler form:\nn\nX\ni=1\nE\u03be [\u2113\u02dcxi, yi(\u03b2)] =\nn\nX\ni=1\n\u2212(y xi \u00b7 \u03b2 \u2212E\u03be [A(\u02dcxi \u00b7 \u03b2)]) =\nn\nX\ni=1\n\u2113xi, yi(\u03b2) + R(\u03b2).\n(4)\n2Arti\ufb01cial noise of the form xi \u2299\u03bei is also called blankout noise. For GLMs, blankout noise is equivalent\nto dropout noise as de\ufb01ned by [1].\n2\nTable 1: Summary of notation.\nxi\nObserved feature vector\nR(\u03b2)\nNoising penalty (5)\n\u02dcxi\nNoised feature vector\nRq(\u03b2)\nQuadratic approximation (6)\nA(x \u00b7 \u03b2)\nLog-partition function\n\u2113(\u03b2)\nNegative log-likelihood (loss)\nThe \ufb01rst equality holds provided that E\u03be[\u02dcxi] = xi, and the second is true with the following de\ufb01ni-\ntion:\nR(\u03b2)\ndef\n=\nn\nX\ni=1\nE\u03be [A(\u02dcxi \u00b7 \u03b2)] \u2212A(xi \u00b7 \u03b2).\n(5)\nHere, R(\u03b2) acts as a regularizer that incorporates the effect of arti\ufb01cial feature noising. In GLMs, the\nlog-partition function A must always be convex, and so R is always positive by Jensen\u2019s inequality.\nThe key observation here is that the effect of arti\ufb01cial feature noising reduces to a penalty R(\u03b2)\nthat does not depend on the labels {yi}. Because of this, arti\ufb01cial feature noising penalizes the\ncomplexity of a classi\ufb01er in a way that does not depend on the accuracy of a classi\ufb01er. Thus, for\nGLMs, arti\ufb01cial feature noising is a regularization scheme on the model itself that can be compared\nwith other forms of regularization such as ridge (L2) or lasso (L1) penalization. In Section 6, we\nexploit the label-independence of the noising penalty and use unlabeled data to tune our estimate of\nR(\u03b2).\nThe fact that R does not depend on the labels has another useful consequence that relates to predic-\ntion. The natural prediction rule with arti\ufb01cially noised features is to select \u02c6y to minimize expected\nloss over the added noise: \u02c6y = argminy E\u03be[\u2113\u02dcx, y(\u02c6\u03b2)]. It is common practice, however, not to noise\nthe inputs and just to output classi\ufb01cation decisions based on the original feature vector [1, 3, 14]:\n\u02c6y = argminy \u2113x, y(\u02c6\u03b2). It is easy to verify that these expressions are in general not equivalent, but\nthey are equivalent when the effect of feature noising reduces to a label-independent penalty on the\nlikelihood. Thus, the common practice of predicting with clean features is formally justi\ufb01ed for\nGLMs.\n2.1\nA Quadratic Approximation to the Noising Penalty\nAlthough the noising penalty R yields an explicit regularizer that does not depend on the labels\n{yi}, the form of R can be dif\ufb01cult to interpret. To gain more insight, we will work with a quadratic\napproximation of the type used by [9, 10]. By taking a second-order Taylor expansion of A around\nx \u00b7 \u03b2, we get that E\u03be [A(\u02dcx \u00b7 \u03b2)] \u2212A(x \u00b7 \u03b2) \u22481\n2A\u2032\u2032(x \u00b7 \u03b2) Var\u03be [\u02dcx \u00b7 \u03b2] . Here the \ufb01rst-order term\nE\u03be [A\u2032(x \u00b7 \u03b2)(\u02dcx \u2212x)] vanishes because E\u03be[\u02dcx] = x. Applying this quadratic approximation to (5)\nyields the following quadratic noising regularizer, which will play a pivotal role in the rest of the\npaper:\nRq(\u03b2)\ndef\n= 1\n2\nn\nX\ni=1\nA\u2032\u2032(xi \u00b7 \u03b2) Var\u03be [\u02dcxi \u00b7 \u03b2] .\n(6)\nThis regularizer penalizes two types of variance over the training examples: (i) A\u2032\u2032(xi \u00b7 \u03b2), which\ncorresponds to the variance of the response yi in the GLM, and (ii) Var\u03be[\u02dcxi \u00b7 \u03b2], the variance of the\nestimated GLM parameter due to noising.3\nAccuracy of approximation\nFigure 1a compares the noising penalties R and Rq for logistic re-\ngression in the case that \u02dcx \u00b7 \u03b2 is Gaussian;4 we vary the mean parameter p\ndef\n= (1 + e\u2212x\u00b7\u03b2)\u22121 and the\nnoise level \u03c3. We see that Rq is generally very accurate, although it tends to overestimate the true\npenalty for p \u22480.5 and tends to underestimate it for very con\ufb01dent predictions. We give a graphical\nexplanation for this phenomenon in the Appendix (Figure A.1).\nThe quadratic approximation also appears to hold up on real datasets. In Figure 1b, we com-\npare the evolution during training of both R and Rq on the 20 newsgroups alt.atheism vs\n3Although Rq is not convex, we were still able (using an L-BFGS algorithm) to train logistic regression\nwith Rq as a surrogate for the dropout regularizer without running into any major issues with local optima.\n4This assumption holds a priori for additive Gaussian noise, and can be reasonable for dropout by the central\nlimit theorem.\n3\n0.0\n0.5\n1.0\n1.5\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nSigma\nNoising Penalty\np = 0.5\np = 0.73\np = 0.82\np = 0.88\np = 0.95\n(a) Comparison of noising penalties R and Rq for\nlogistic regression with Gaussian perturbations,\ni.e., (\u02dcx \u2212x) \u00b7 \u03b2 \u223cN(0, \u03c32). The solid line\nindicates the true penalty and the dashed one is\nour quadratic approximation thereof; p = (1 +\ne\u2212x\u00b7\u03b2)\u22121 is the mean parameter for the logistic\nmodel.\n0\n50\n100\n150\n10\n20\n50\n100\n200\n500\nTraining Iteration\nLoss\nDropout Penalty\nQuadratic Penalty\nNegative Log\u2212Likelihood\n(b) Comparing the evolution of the exact dropout\npenalty R and our quadratic approximation Rq\nfor logistic regression on the AthR classi\ufb01cation\ntask in [15] with 22K features and n = 1000\nexamples. The horizontal axis is the number of\nquasi-Newton steps taken while training with ex-\nact dropout.\nFigure 1: Validating the quadratic approximation.\nsoc.religion.christian classi\ufb01cation task described in [15]. We see that the quadratic ap-\nproximation is accurate most of the way through the learning procedure, only deteriorating slightly\nas the model converges to highly con\ufb01dent predictions.\nIn practice, we have found that \ufb01tting logistic regression with the quadratic surrogate Rq gives\nsimilar results to actual dropout-regularized logistic regression. We use this technique for our ex-\nperiments in Section 6.\n3\nRegularization based on Additive Noise\nHaving established the general quadratic noising regularizer Rq, we now turn to studying the ef-\nfects of Rq for various likelihoods (linear and logistic regression) and noising models (additive and\ndropout). In this section, we warm up with additive noise; in Section 4 we turn to our main target of\ninterest, namely dropout noise.\nLinear regression\nSuppose \u02dcx = x + \u03b5 is generated by by adding noise with Var[\u03b5] = \u03c32Id\u00d7d to\nthe original feature vector x. Note that Var\u03be[\u02dcx \u00b7 \u03b2] = \u03c32\u2225\u03b2\u22252\n2, and in the case of linear regression\nA(z) = 1\n2z2, so A\u2032\u2032(z) = 1. Applying these facts to (6) yields a simpli\ufb01ed form for the quadratic\nnoising penalty:\nRq(\u03b2) = 1\n2\u03c32n\u2225\u03b2\u22252\n2.\n(7)\nThus, we recover the well-known result that linear regression with additive feature noising is equiv-\nalent to ridge regression [2, 9]. Note that, with linear regression, the quadratic approximation Rq is\nexact and so the correspondence with L2-regularization is also exact.\nLogistic regression\nThe situation gets more interesting when we move beyond linear regression.\nFor logistic regression, A\u2032\u2032(xi \u00b7 \u03b2) = pi(1 \u2212pi) where pi = (1 + exp(\u2212xi \u00b7 \u03b2))\u22121 is the predicted\nprobability of yi = 1. The quadratic noising penalty is then\nRq(\u03b2) = 1\n2\u03c32\u2225\u03b2\u22252\n2\nn\nX\ni=1\npi(1 \u2212pi).\n(8)\nIn other words, the noising penalty now simultaneously encourages parsimonious modeling as be-\nfore (by encouraging \u2225\u03b2\u22252\n2 to be small) as well as con\ufb01dent predictions (by encouraging the pi\u2019s to\nmove away from 1\n2).\n4\nTable 2: Form of the different regularization schemes. These expressions assume that the design\nmatrix has been normalized, i.e., that P\ni x2\nij = 1 for all j. The pi = (1 + e\u2212xi\u00b7\u03b2)\u22121 are mean\nparameters for the logistic model.\nLinear Regression\nLogistic Regression\nGLM\nL2-penalization\n\u2225\u03b2\u22252\n2\n\u2225\u03b2\u22252\n2\n\u2225\u03b2\u22252\n2\nAdditive Noising\n\u2225\u03b2\u22252\n2\n\u2225\u03b2\u22252\n2\nP\ni pi(1 \u2212pi)\n\u2225\u03b2\u22252\n2 tr(V (\u03b2))\nDropout Training\n\u2225\u03b2\u22252\n2\nP\ni, j pi(1 \u2212pi) x2\nij \u03b22\nj\n\u03b2\u22a4diag(X\u22a4V (\u03b2)X)\u03b2\n4\nRegularization based on Dropout Noise\nRecall that dropout training corresponds to applying dropout noise to training examples, where\nthe noised features \u02dcxi are obtained by setting \u02dcxij to 0 with some \u201cdropout probability\u201d \u03b4 and to\nxij/(1 \u2212\u03b4) with probability (1 \u2212\u03b4), independently for each coordinate j of the feature vector. We\ncan check that:\nVar\u03be [\u02dcxi \u00b7 \u03b2] = 1\n2\n\u03b4\n1 \u2212\u03b4\nd\nX\nj=1\nx2\nij\u03b22\nj ,\n(9)\nand so the quadratic dropout penalty is\nRq(\u03b2) = 1\n2\n\u03b4\n1 \u2212\u03b4\nn\nX\ni=1\nA\u2032\u2032(xi \u00b7 \u03b2)\nd\nX\nj=1\nx2\nij\u03b22\nj .\n(10)\nLetting X \u2208Rn\u00d7d be the design matrix with rows xi and V (\u03b2) \u2208Rn\u00d7n be a diagonal matrix with\nentries A\u2032\u2032(xi \u00b7 \u03b2), we can re-write this penalty as\nRq(\u03b2) = 1\n2\n\u03b4\n1 \u2212\u03b4 \u03b2\u22a4diag(X\u22a4V (\u03b2)X)\u03b2.\n(11)\nLet \u03b2\u2217be the maximum likelihood estimate given in\ufb01nite data. When computed at \u03b2\u2217, the matrix\n1\nnX\u22a4V (\u03b2\u2217)X =\n1\nn\nPn\ni=1 \u22072\u2113xi, yi(\u03b2\u2217) is an estimate of the Fisher information matrix I. Thus,\ndropout can be seen as an attempt to apply an L2 penalty after normalizing the feature vector by\ndiag(I)\u22121/2. The Fisher information is linked to the shape of the level surfaces of \u2113(\u03b2) around \u03b2\u2217.\nIf I were a multiple of the identity matrix, then these level surfaces would be perfectly spherical\naround \u03b2\u2217. Dropout, by normalizing the problem by diag(I)\u22121/2, ensures that while the level\nsurfaces of \u2113(\u03b2) may not be spherical, the L2-penalty is applied in a basis where the features have\nbeen balanced out. We give a graphical illustration of this phenomenon in Figure A.2.\nLinear Regression\nFor linear regression, V is the identity matrix, so the dropout objective is\nequivalent to a form of ridge regression where each column of the design matrix is normalized\nbefore applying the L2 penalty.5 This connection has been noted previously by [3].\nLogistic Regression\nThe form of dropout penalties becomes much more intriguing once we move\nbeyond the realm of linear regression. The case of logistic regression is particularly interesting.\nHere, we can write the quadratic dropout penalty from (10) as\nRq(\u03b2) = 1\n2\n\u03b4\n1 \u2212\u03b4\nn\nX\ni=1\nd\nX\nj=1\npi(1 \u2212pi) x2\nij \u03b22\nj .\n(12)\nThus, just like additive noising, dropout generally gives an advantage to con\ufb01dent predictions and\nsmall \u03b2. However, unlike all the other methods considered so far, dropout may allow for some large\npi(1 \u2212pi) and some large \u03b22\nj , provided that the corresponding cross-term x2\nij is small.\nOur analysis shows that dropout regularization should be better than L2-regularization for learning\nweights for features that are rare (i.e., often 0) but highly discriminative, because dropout effectively\ndoes not penalize \u03b2j over observations for which xij = 0. Thus, in order for a feature to earn a large\n\u03b22\nj , it suf\ufb01ces for it to contribute to a con\ufb01dent prediction with small pi(1 \u2212pi) each time that it\nis active.6 Dropout training has been empirically found to perform well on tasks such as document\n5Normalizing the columns of the design matrix before performing penalized regression is standard practice,\nand is implemented by default in software like glmnet for R [16].\n6To be precise, dropout does not reward all rare but discriminative features. Rather, dropout rewards those\nfeatures that are rare and positively co-adapted with other features in a way that enables the model to make\ncon\ufb01dent predictions whenever the feature of interest is active.\n5\nTable 3: Accuracy of L2 and dropout regularized logistic regression on a simulated example. The\n\ufb01rst row indicates results over test examples where some of the rare useful features are active (i.e.,\nwhere there is some signal that can be exploited), while the second row indicates accuracy over the\nfull test set. These results are averaged over 100 simulation runs, with 75 training examples in each.\nAll tuning parameters were set to optimal values. The sampling error on all reported values is within\n\u00b10.01.\nAccuracy\nL2-regularization\nDropout training\nActive Instances\n0.66\n0.73\nAll Instances\n0.53\n0.55\nclassi\ufb01cation where rare but discriminative features are prevalent [3]. Our result suggests that this is\nno mere coincidence.\nWe summarize the relationship between L2-penalization, additive noising and dropout in Table 2.\nAdditive noising introduces a product-form penalty depending on both \u03b2 and A\u2032\u2032. However, the full\npotential of arti\ufb01cial feature noising only emerges with dropout, which allows the penalty terms due\nto \u03b2 and A\u2032\u2032 to interact in a non-trivial way through the design matrix X (except for linear regression,\nin which all the noising schemes we consider collapse to ridge regression).\n4.1\nA Simulation Example\nThe above discussion suggests that dropout logistic regression should perform well with rare but\nuseful features. To test this intuition empirically, we designed a simulation study where all the\nsignal is grouped in 50 rare features, each of which is active only 4% of the time. We then added\n1000 nuisance features that are always active to the design matrix, for a total of d = 1050 features.\nTo make sure that our experiment was picking up the effect of dropout training speci\ufb01cally and not\njust normalization of X, we ensured that the columns of X were normalized in expectation.\nThe dropout penalty for logistic regression can be written as a matrix product\nRq(\u03b2) = 1\n2\n\u03b4\n1 \u2212\u03b4 (\u00b7 \u00b7 \u00b7\npi(1 \u2212pi)\n\u00b7 \u00b7 \u00b7)\n\uf8eb\n\uf8ed\n\u00b7 \u00b7 \u00b7\n\u00b7 \u00b7 \u00b7\nx2\nij\n\u00b7 \u00b7 \u00b7\n\u00b7 \u00b7 \u00b7\n\uf8f6\n\uf8f8\n\uf8eb\n\uf8ed\n\u00b7 \u00b7 \u00b7\n\u03b22\nj\n\u00b7 \u00b7 \u00b7\n\uf8f6\n\uf8f8.\n(13)\nWe designed the simulation study in such a way that, at the optimal \u03b2, the dropout penalty should\nhave structure\nSmall\n(con\ufb01dent prediction)\nBig\n(weak prediction)\n \n!\n\u00b7 \u00b7 \u00b7\n\u00b7 \u00b7 \u00b7\n0\n\u00b7 \u00b7 \u00b7\n\uf8eb\n\uf8ec\n\uf8ed\n\uf8f6\n\uf8f7\n\uf8f8\nBig\n(useful feature)\nSmall\n(nuisance feature)\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\n.\n(14)\nA dropout penalty with such a structure should be small. Although there are some uncertain pre-\ndictions with large pi(1 \u2212pi) and some big weights \u03b22\nj , these terms cannot interact because the\ncorresponding terms x2\nij are all 0 (these are examples without any of the rare discriminative fea-\ntures and thus have no signal). Meanwhile, L2 penalization has no natural way of penalizing some\n\u03b2j more and others less. Our simulation results, given in Table 3, con\ufb01rm that dropout training\noutperforms L2-regularization here as expected. See Appendix A.1 for details.\n5\nDropout Regularization in Online Learning\nThere is a well-known connection between L2-regularization and stochastic gradient descent (SGD).\nIn SGD, the weight vector \u02c6\u03b2 is updated with \u02c6\u03b2t+1 = \u02c6\u03b2t \u2212\u03b7t gt, where gt = \u2207\u2113xt, yt(\u02c6\u03b2t) is the\ngradient of the loss due to the t-th training example. We can also write this update as a linear\nL2-penalized problem\n\u02c6\u03b2t+1 = argmin\u03b2\n\u001a\n\u2113xt, yt(\u02c6\u03b2t) + gt \u00b7 (\u03b2 \u2212\u02c6\u03b2t) + 1\n2\u03b7t\n\u2225\u03b2 \u2212\u02c6\u03b2t\u22252\n2\n\u001b\n,\n(15)\nwhere the \ufb01rst two terms form a linear approximation to the loss and the third term is an L2-\nregularizer. Thus, SGD progresses by repeatedly solving linearized L2-regularized problems.\n6\n0\n10000\n20000\n30000\n40000\n0.8\n0.82\n0.84\n0.86\n0.88\n0.9\nsize of unlabeled data\naccuracy\n \n \ndropout+unlabeled\ndropout\nL2\n5000\n10000\n15000\n0.8\n0.82\n0.84\n0.86\n0.88\n0.9\nsize of labeled data\naccuracy\n \n \ndropout+unlabeled\ndropout\nL2\nFigure 2: Test set accuracy on the IMDB dataset [12] with unigram features. Left: 10000 labeled\ntraining examples, and up to 40000 unlabeled examples. Right: 3000-15000 labeled training exam-\nples, and 25000 unlabeled examples. The unlabeled data is discounted by a factor \u03b1 = 0.4.\nAs discussed by Duchi et al. [11], a problem with classic SGD is that it can be slow at learning\nweights corresponding to rare but highly discriminative features. This problem can be alleviated\nby running a modi\ufb01ed form of SGD with \u02c6\u03b2t+1 = \u02c6\u03b2t \u2212\u03b7 A\u22121\nt gt, where the transformation At is\nalso learned online; this leads to the AdaGrad family of stochastic descent rules. Duchi et al. use\nAt = diag(Gt)1/2 where Gt = Pt\ni=1 gig\u22a4\ni\nand show that this choice achieves desirable regret\nbounds in the presence of rare but useful features. At least super\ufb01cially, AdaGrad and dropout seem\nto have similar goals: For logistic regression, they can both be understood as adaptive alternatives\nto methods based on L2-regularization that favor learning rare, useful features. As it turns out, they\nhave a deeper connection.\nThe natural way to incorporate dropout regularization into SGD is to replace the penalty term \u2225\u03b2 \u2212\n\u02c6\u03b2\u22252\n2/2\u03b7 in (15) with the dropout regularizer, giving us an update rule\n\u02c6\u03b2t+1 = argmin\u03b2\nn\n\u2113xt, yt(\u02c6\u03b2t) + gt \u00b7 (\u03b2 \u2212\u02c6\u03b2t) + Rq(\u03b2 \u2212\u02c6\u03b2t; \u02c6\u03b2t)\no\n(16)\nwhere, Rq(\u00b7; \u02c6\u03b2t) is the quadratic noising regularizer centered at \u02c6\u03b2t:7\nRq(\u03b2 \u2212\u02c6\u03b2t; \u02c6\u03b2t) = 1\n2(\u03b2 \u2212\u02c6\u03b2t)\u22a4diag(Ht) (\u03b2 \u2212\u02c6\u03b2t), where Ht =\nt\nX\ni=1\n\u22072\u2113xi, yi(\u02c6\u03b2t).\n(17)\nThis implies that dropout descent is \ufb01rst-order equivalent to an adaptive SGD procedure with At =\ndiag(Ht). To see the connection between AdaGrad and this dropout-based online procedure, recall\nthat for GLMs both of the expressions\nE\u03b2\u2217\u0002\n\u22072\u2113x, y(\u03b2\u2217)\n\u0003\n= E\u03b2\u2217\u0002\n\u2207\u2113x, y(\u03b2\u2217)\u2207\u2113x, y(\u03b2\u2217)\u22a4\u0003\n(18)\nare equal to the Fisher information I [17]. In other words, as \u02c6\u03b2t converges to \u03b2\u2217, Gt and Ht are both\nconsistent estimates of the Fisher information. Thus, by using dropout instead of L2-regularization\nto solve linearized problems in online learning, we end up with an AdaGrad-like algorithm.\nOf course, the connection between AdaGrad and dropout is not perfect. In particular, AdaGrad\nallows for a more aggressive learning rate by using At = diag(Gt)\u22121/2 instead of diag(Gt)\u22121.\nBut, at a high level, AdaGrad and dropout appear to both be aiming for the same goal: scaling\nthe features by the Fisher information to make the level-curves of the objective more circular. In\ncontrast, L2-regularization makes no attempt to sphere the level curves, and AROW [18]\u2014another\npopular adaptive method for online learning\u2014only attempts to normalize the effective feature matrix\nbut does not consider the sensitivity of the loss to changes in the model weights. In the case of\nlogistic regression, AROW also favors learning rare features, but unlike dropout and AdaGrad does\nnot privilege con\ufb01dent predictions.\n7This expression is equivalent to (11) except that we used \u02c6\u03b2t and not \u03b2 \u2212\u02c6\u03b2t to compute Ht.\n7\nTable 4: Performance of semi-supervised dropout training for document classi\ufb01cation.\n(a) Test accuracy with and without unlabeled data on\ndifferent datasets. Each dataset is split into 3 parts\nof equal sizes: train, unlabeled, and test. Log. Reg.:\nlogistic regression with L2 regularization; Dropout:\ndropout trained with quadratic surrogate; +Unla-\nbeled: using unlabeled data.\nDatasets Log. Reg. Dropout +Unlabeled\nSubj\n88.96\n90.85\n91.48\nRT\n73.49\n75.18\n76.56\nIMDB-2k\n80.63\n81.23\n80.33\nXGraph\n83.10\n84.64\n85.41\nBbCrypt\n97.28\n98.49\n99.24\nIMDB\n87.14\n88.70\n89.21\n(b) Test accuracy on the IMDB dataset [12]. Labeled:\nusing just labeled data from each paper/method, +Un-\nlabeled: use additional unlabeled data. Drop: dropout\nwith Rq, MNB: multionomial naive Bayes with semi-\nsupervised frequency estimate from [19],8-Uni: uni-\ngram features, -Bi: bigram features.\nMethods Labeled +Unlabeled\nMNB-Uni [19]\n83.62\n84.13\nMNB-Bi [19]\n86.63\n86.98\nVect.Sent[12]\n88.33\n88.89\nNBSVM[15]-Bi\n91.22\n\u2013\nDrop-Uni\n87.78\n89.52\nDrop-Bi\n91.31\n91.98\n6\nSemi-Supervised Dropout Training\nRecall that the regularizer R(\u03b2) in (5) is independent of the labels {yi}. As a result, we can use\nadditional unlabeled training examples to estimate it more accurately. Suppose we have an unlabeled\ndataset {zi} of size m, and let \u03b1 \u2208(0, 1] be a discount factor for the unlabeled data. Then we can\nde\ufb01ne a semi-supervised penalty estimate\nR\u2217(\u03b2)\ndef\n=\nn\nn + \u03b1m\n\u0010\nR(\u03b2) + \u03b1 RUnlabeled(\u03b2)\n\u0011\n,\n(19)\nwhere R(\u03b2) is the original penalty estimate and RUnlabeled(\u03b2) = P\ni E\u03be[A(zi \u00b7 \u03b2)] \u2212A(zi \u00b7 \u03b2) is\ncomputed using (5) over the unlabeled examples zi. We select the discount parameter by cross-\nvalidation; empirically, \u03b1 \u2208[0.1, 0.4] works well. For convenience, we optimize the quadratic\nsurrogate Rq\n\u2217instead of R\u2217. Another practical option would be to use the Gaussian approximation\nfrom [3] for estimating R\u2217(\u03b2).\nMost approaches to semi-supervised learning either rely on using a generative model [19, 20, 21, 22,\n23] or various assumptions on the relationship between the predictor and the marginal distribution\nover inputs. Our semi-supervised approach is based on a different intuition: we\u2019d like to set weights\nto make con\ufb01dent predictions on unlabeled data as well as the labeled data, an intuition shared by\nentropy regularization [24] and transductive SVMs [25].\nExperiments\nWe apply this semi-supervised technique to text classi\ufb01cation. Results on several\ndatasets described in [15] are shown in Table 4a; Figure 2 illustrates how the use of unlabeled data\nimproves the performance of our classi\ufb01er on a single dataset. Overall, we see that using unlabeled\ndata to learn a better regularizer R\u2217(\u03b2) consistently improves the performance of dropout training.\nTable 4b shows our results on the IMDB dataset of [12]. The dataset contains 50,000 unlabeled\nexamples in addition to the labeled train and test sets of size 25,000 each. Whereas the train and\ntest examples are either positive or negative, the unlabeled examples contain neutral reviews as well.\nWe train a dropout-regularized logistic regression classi\ufb01er on unigram/bigram features, and use the\nunlabeled data to tune our regularizer. Our method bene\ufb01ts from unlabeled data even in the presence\nof a large amount of labeled data, and achieves state-of-the-art accuracy on this dataset.\n7\nConclusion\nWe analyzed dropout training as a form of adaptive regularization. This framework enabled us\nto uncover close connections between dropout training, adaptively balanced L2-regularization, and\nAdaGrad; and led to a simple yet effective method for semi-supervised training. There seem to be\nmultiple opportunities for digging deeper into the connection between dropout training and adaptive\nregularization. In particular, it would be interesting to see whether the dropout regularizer takes\non a tractable and/or interpretable form in neural networks, and whether similar semi-supervised\nschemes could be used to improve on the results presented in [1].\n8Our implementation of semi-supervised MNB. MNB with EM [20] failed to give an improvement.\n8\nReferences\n[1] Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdi-\nnov.\nImproving neural networks by preventing co-adaptation of feature detectors.\narXiv preprint\narXiv:1207.0580, 2012.\n[2] Laurens van der Maaten, Minmin Chen, Stephen Tyree, and Kilian Q Weinberger.\nLearning with\nmarginalized corrupted features. In Proceedings of the International Conference on Machine Learning,\n2013.\n[3] Sida I Wang and Christopher D Manning. Fast dropout training. In Proceedings of the International\nConference on Machine Learning, 2013.\n[4] Yaser S Abu-Mostafa. Learning from hints in neural networks. Journal of Complexity, 6(2):192\u2013198,\n1990.\n[5] Chris J.C. Burges and Bernhard Schlkopf. Improving the accuracy and speed of support vector machines.\nIn Advances in Neural Information Processing Systems, pages 375\u2013381, 1997.\n[6] Patrice Y Simard, Yann A Le Cun, John S Denker, and Bernard Victorri. Transformation invariance in\npattern recognition: Tangent distance and propagation. International Journal of Imaging Systems and\nTechnology, 11(3):181\u2013197, 2000.\n[7] Salah Rifai, Yann Dauphin, Pascal Vincent, Yoshua Bengio, and Xavier Muller. The manifold tangent\nclassi\ufb01er. Advances in Neural Information Processing Systems, 24:2294\u20132302, 2011.\n[8] Kiyotoshi Matsuoka. Noise injection into inputs in back-propagation learning. Systems, Man and Cyber-\nnetics, IEEE Transactions on, 22(3):436\u2013440, 1992.\n[9] Chris M Bishop. Training with noise is equivalent to Tikhonov regularization. Neural computation,\n7(1):108\u2013116, 1995.\n[10] Salah Rifai, Xavier Glorot, Yoshua Bengio, and Pascal Vincent. Adding noise to the input of a model\ntrained with a regularized objective. arXiv preprint arXiv:1104.3250, 2011.\n[11] John Duchi, Elad Hazan, and Yoram Singer.\nAdaptive subgradient methods for online learning and\nstochastic optimization. Journal of Machine Learning Research, 12:2121\u20132159, 2010.\n[12] Andrew L Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts.\nLearning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Associa-\ntion for Computational Linguistics, pages 142\u2013150. Association for Computational Linguistics, 2011.\n[13] Sida I Wang, Mengqiu Wang, Stefan Wager, Percy Liang, and Christopher D Manning. Feature noising\nfor log-linear structured prediction. In Empirical Methods in Natural Language Processing, 2013.\n[14] Ian J Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio. Maxout\nnetworks. In Proceedings of the International Conference on Machine Learning, 2013.\n[15] Sida Wang and Christopher D Manning. Baselines and bigrams: Simple, good sentiment and topic clas-\nsi\ufb01cation. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics,\npages 90\u201394. Association for Computational Linguistics, 2012.\n[16] Jerome Friedman, Trevor Hastie, and Rob Tibshirani. Regularization paths for generalized linear models\nvia coordinate descent. Journal of Statistical Software, 33(1):1, 2010.\n[17] Erich Leo Lehmann and George Casella. Theory of Point Estimation. Springer, 1998.\n[18] Koby Crammer, Alex Kulesza, Mark Dredze, et al. Adaptive regularization of weight vectors. Advances\nin Neural Information Processing Systems, 22:414\u2013422, 2009.\n[19] Jiang Su, Jelber Sayyad Shirab, and Stan Matwin. Large scale text classi\ufb01cation using semi-supervised\nmultinomial naive Bayes. In Proceedings of the International Conference on Machine Learning, 2011.\n[20] Kamal Nigam, Andrew Kachites McCallum, Sebastian Thrun, and Tom Mitchell. Text classi\ufb01cation from\nlabeled and unlabeled documents using EM. Machine Learning, 39(2-3):103\u2013134, May 2000.\n[21] G. Bouchard and B. Triggs. The trade-off between generative and discriminative classi\ufb01ers. In Interna-\ntional Conference on Computational Statistics, pages 721\u2013728, 2004.\n[22] R. Raina, Y. Shen, A. Ng, and A. McCallum. Classi\ufb01cation with hybrid generative/discriminative models.\nIn Advances in Neural Information Processing Systems, Cambridge, MA, 2004. MIT Press.\n[23] J. Suzuki, A. Fujino, and H. Isozaki.\nSemi-supervised structured output learning based on a hybrid\ngenerative and discriminative approach.\nIn Empirical Methods in Natural Language Processing and\nComputational Natural Language Learning, 2007.\n[24] Y. Grandvalet and Y. Bengio. Entropy regularization. In Semi-Supervised Learning, United Kingdom,\n2005. Springer.\n[25] Thorsten Joachims.\nTransductive inference for text classi\ufb01cation using support vector machines.\nIn\nProceedings of the International Conference on Machine Learning, pages 200\u2013209, 1999.\n9\nA\nAppendix\nFigure A.1: Quadratic approximations to the logistic loss. We see that the red curve, namely the\nquadratic approximation taken at \u03b7 = 0, p = 1/(1 + e\u03b7) = 0.5 is always above the actual loss\ncurve. Meanwhile, quadratic approximations taken at the more extreme locations of p = 0.05 and\np = 0.95 undershoot the true loss over a large range. Note that the curvature of the loss is symmetric\nin the natural parameter \u03b7 and so the performance of the quadratic approximation is equivalent at p\nand 1 \u2212p for all p \u2208(0, 1).\nA.1\nDescription of Simulation Study\nSection 4.1 gives the motivation for and a high-level description of our simulation study. Here, we\ngive a detailed description of the study.\nGenerating features.\nOur simulation has 1050 features. The \ufb01rst 50 discriminative features form\n5 groups of 10; the last 1000 features are nuisance terms. Each xi was independently generated as\nfollows:\n1. Pick a group number g \u22081, ..., 25, and a sign sgn = \u00b11.\n2. If g \u22645, draw the entries of xi with index between 10 (g \u22121) + 1 and 10 (g \u22121) + 10\nuniformly from sgn \u00b7 Exp(C), where C is selected such that E[(xi)2\nj] = 1 for all j. Set all\nthe other discriminative features to 0. If g > 5, set all the discriminative features to 0.\n3. Draw the last 1000 entries of xi independently from N(0, 1).\nNotice that this procedure guarantees that the columns of X all have the same expected second\nmoments.\nGenerating labels.\nGiven an xi, we generate yi from the Bernoulli distribution with parameter\n\u03c3(xi \u00b7 \u03b2), where the \ufb01rst 50 coordinates of \u03b2 are 0.057 and the remaining 1000 coordinates are 0.\nThe value 0.057 was selected to make the average value of |xi \u00b7 \u03b2| in the presence of signal be 2.\nTraining.\nFor each simulation run, we generated a training set of size n = 75. For this purpose, we\ncycled over the group number g deterministically. The penalization parameters were set to roughly\noptimal values. For dropout, we used \u03b4 = 0.9 while from L2-penalization we used \u03bb = 32.\n10\nFigure A.2: Comparison of two L2 regularizers. In both cases, the black solid ellipses are level sur-\nfaces of the likelihood and the blue dashed curves are level surfaces of the regularizer; the optimum\nof the regularized objective is denoted by OPT. The left panel shows a classic spherical L2 regulizer\n\u2225\u03b2\u22252\n2, whereas the right panel has an L2 regularizer \u03b2\u22a4diag(I)\u03b2 that has been adapted to the shape\nof the likelihood (I is the Fisher information matrix). The second regularizer is still aligned with\nthe axes, but the relative importance of each axis is now scaled using the curvature of the likelihood\nfunction. As argued in (11), dropout training is comparable to the setup depicted in the right panel.\n11\n",
        "sentence": " , equivalent to a form of data-dependent regularizer) (Wager et al., 2013). In standard dropout (Wager et al., 2013; Hinton et al., 2012), the entries of the noise vector are sampled independently according to Pr( j = 0) = \u03b4 and Pr( j = 1 1\u2212\u03b4 ) = 1 \u2212 \u03b4, i. Dropout is a data-dependent regularizer Dropout as a regularizer has been studied in (Wager et al., 2013; Baldi & Sadowski, 2013) for logistic regression, which is stated in the following proposition for ease of discussion later. Using the second order Taylor expansion, (Wager et al., 2013) showed that the following approximation of RD,M(w) is easy to manipulate and understand:",
        "context": "\u2022 Dropout noise: \u03bd(xi, \u03bei) = xi \u2299\u03bei, where \u2299is the elementwise product of two vec-\ntors. Each component of \u03bei \u2208{0, (1 \u2212\u03b4)\u22121}d is an independent draw from a scaled\nBernoulli(1 \u2212\u03b4) random variable. In other words, dropout noise corresponds to setting \u02dcxij\nxij/(1 \u2212\u03b4) with probability (1 \u2212\u03b4), independently for each coordinate j of the feature vector. We\ncan check that:\nVar\u03be [\u02dcxi \u00b7 \u03b2] = 1\n2\n\u03b4\n1 \u2212\u03b4\nd\nX\nj=1\nx2\nij\u03b22\nj ,\n(9)\nand so the quadratic dropout penalty is\nRq(\u03b2) = 1\n2\n\u03b4\n1 \u2212\u03b4\nn\nX\ni=1\nA\u2032\u2032(xi \u00b7 \u03b2)\nd\nX\nj=1\nx2\nbeyond the realm of linear regression. The case of logistic regression is particularly interesting.\nHere, we can write the quadratic dropout penalty from (10) as\nRq(\u03b2) = 1\n2\n\u03b4\n1 \u2212\u03b4\nn\nX\ni=1\nd\nX\nj=1\npi(1 \u2212pi) x2\nij \u03b22\nj .\n(12)"
    },
    {
        "title": "Altitude training: Strong bounds for singlelayer dropout",
        "author": [
            "Wager",
            "Stefan",
            "Fithian",
            "William",
            "Wang",
            "Sida",
            "Liang",
            "Percy S"
        ],
        "venue": "In Advances in Neural Information Processing Systems,",
        "citeRegEx": "Wager et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Wager et al\\.",
        "year": 2014,
        "abstract": "",
        "full_text": "",
        "sentence": " Other studies focus on shallow learning with dropout noise (Wager et al., 2014; Helmbold & Long, 2014; Chen et al., 2014).",
        "context": null
    },
    {
        "title": "Regularization of neural networks using dropconnect",
        "author": [
            "Wan",
            "Li",
            "Zeiler",
            "Matthew",
            "Zhang",
            "Sixin",
            "Cun",
            "Yann L",
            "Fergus",
            "Rob"
        ],
        "venue": "In Proceedings of the 30th International Conference on Machine Learning",
        "citeRegEx": "Wan et al\\.,? \\Q2013\\E",
        "shortCiteRegEx": "Wan et al\\.",
        "year": 2013,
        "abstract": "",
        "full_text": "",
        "sentence": " We used the similar neural network structure to (Wan et al., 2013): two convolution layers, two fully connected layers, a softmax layer and a cost layer at the end. 01 the same to (Wan et al., 2013), and that for evolutional dropout is set to 0.",
        "context": null
    },
    {
        "title": "Fast dropout training",
        "author": [
            "Wang",
            "Sida",
            "Manning",
            "Christopher"
        ],
        "venue": "In Proceedings of the 30th International Conference on Machine Learning",
        "citeRegEx": "Wang et al\\.,? \\Q2013\\E",
        "shortCiteRegEx": "Wang et al\\.",
        "year": 2013,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "An explicit sampling dependent spectral error bound for column subset selection",
        "author": [
            "Yang",
            "Tianbao",
            "Zhang",
            "Lijun",
            "Jin",
            "Rong",
            "Zhu",
            "Shenghuo"
        ],
        "venue": "In Proceedings of the 32nd International Conference on Machine Learning (ICML),",
        "citeRegEx": "Yang et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "Yang et al\\.",
        "year": 2015,
        "abstract": "In this paper, we consider the problem of column subset selection. We present\na novel analysis of the spectral norm reconstruction for a simple randomized\nalgorithm and establish a new bound that depends explicitly on the sampling\nprobabilities. The sampling dependent error bound (i) allows us to better\nunderstand the tradeoff in the reconstruction error due to sampling\nprobabilities, (ii) exhibits more insights than existing error bounds that\nexploit specific probability distributions, and (iii) implies better sampling\ndistributions. In particular, we show that a sampling distribution with\nprobabilities proportional to the square root of the statistical leverage\nscores is always better than uniform sampling and is better than leverage-based\nsampling when the statistical leverage scores are very nonuniform. And by\nsolving a constrained optimization problem related to the error bound with an\nefficient bisection search we are able to achieve better performance than using\neither the leverage-based distribution or that proportional to the square root\nof the statistical leverage scores. Numerical simulations demonstrate the\nbenefits of the new sampling distributions for low-rank matrix approximation\nand least square approximation compared to state-of-the art algorithms.",
        "full_text": "arXiv:1505.00526v1  [math.NA]  4 May 2015\nAn Explicit Sampling Dependent Spectral Error Bound\nfor Column Subset Selection\nTianbao Yang\nTIANBAO-YANG@UIOWA.EDU\nDepartment of Computer Science, the University of Iowa, Iowa City, USA\nLijun Zhang\nZHANGLJ@LAMDA.NJU.EDU.CN\nNational Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China\nRong Jin\nRONGJIN@CSE.MSU.EDU\nDepartment of Computer Science and Engineering, Michigan State University, East Lansing, USA\nInstitute of Data Science and Technologies at Alibaba Group, Seattle, USA\nShenghuo Zhu\nSHENGHUO@GMAIL.COM\nInstitute of Data Science and Technologies at Alibaba Group, Seattle, USA\nAbstract\nIn this paper, we consider the problem of column\nsubset selection. We present a novel analysis of\nthe spectral norm reconstruction for a simple ran-\ndomized algorithm and establish a new bound\nthat depends explicitly on the sampling probabil-\nities. The sampling dependent error bound (i) al-\nlows us to better understand the tradeoff in the\nreconstruction error due to sampling probabili-\nties, (ii) exhibits more insights than existing er-\nror bounds that exploit speci\ufb01c probability distri-\nbutions, and (iii) implies better sampling distri-\nbutions. In particular, we show that a sampling\ndistribution with probabilities proportional to the\nsquare root of the statistical leverage scores is al-\nways better than uniform sampling and is better\nthan leverage-based sampling when the statisti-\ncal leverage scores are very nonuniform. And by\nsolving a constrained optimization problem re-\nlated to the error bound with an ef\ufb01cient bisec-\ntion search we are able to achieve better perfor-\nmance than using either the leverage-based dis-\ntribution or that proportional to the square root\nof the statistical leverage scores. Numerical sim-\nulations demonstrate the bene\ufb01ts of the new sam-\npling distributions for low-rank matrix approxi-\nmation and least square approximation compared\nto state-of-the art algorithms.\n1. Introduction\nGive a data matrix A \u2208Rm\u00d7n, column subset selection\n(CSS) is an important technique for constructing a com-\npressed representation and a low rank approximation of\nA by selecting a small number of columns.\nCompared\nwith conventional singular value decomposition (SVD),\nCSS could yield more interpretable output while main-\ntaining performance as close as SVD (Mahoney, 2011).\nRecently, CSS has been applied successfully to prob-\nlems of interest to geneticists such as genotype recon-\nstruction, identifying substructure in heterogeneous popu-\nlations, etc. (Paschou et al., 2007b;a; Drineas et al., 2010;\nJaved et al., 2011).\nLet C \u2208Rm\u00d7\u2113be the matrix formed by \u2113selected columns\nof A. The key question to CSS is how to select the columns\nto minimize the reconstruction error:\n\u2225A \u2212PCA\u2225\u03be,\nwhere PC = CC\u2020 denotes the projection onto the column\nspace of C with C\u2020 being the pseudo inverse of C and \u03be =\n2 or F denotes the spectral norm or the Frobenius norm.\nIn this paper, we are particularly interested in the spectral\nnorm reconstruction with respect to a target rank k.\nOur analysis is based on a randomized algorithm that se-\nlects \u2113> k columns from A according to sampling prob-\nabilities s = (s1, . . . , sn). Building on advanced matrix\nconcentration inequalities (e.g., matrix Chernoff bound and\nBernstein inequality), we develop a novel analysis of the\nspectral norm reconstruction and establish a sampling de-\npendent relative spectral error bound with a high probabil-\nSampling Dependent Spectral Error Bound for CSS\nity as following:\n\u2225A \u2212PCA\u22252 \u2264(1 + \u01eb(s))\u2225A \u2212Ak\u22252,\nwhere Ak is the best rank-k approximation of A based\non SVD and \u01eb(s) is a quantity dependent on the sampling\nprobabilities s besides the scalars n, k, \u2113. As revealed in\nour main theorem (Theorem 1), the quantity \u01eb(s) also de-\npends on the statistical leverage scores (SLS) inherent to\nthe data, based on which are several important randomized\nalgorithms for CSS.\nTo the best of our knowledge, this is the \ufb01rst such kind of\nerror bound for CSS. Compared with existing error bounds,\nthe sampling dependent error bound brings us several ben-\ne\ufb01ts: (i) it allows us to better understand the tradeoff in the\nspectral error of reconstruction due to sampling probabili-\nties, complementary to a recent result on the tradeoff from a\nstatistical perspective (Ma et al., 2014) for least square re-\ngression; (ii) it implies that a distribution with sampling\nprobabilities proportional to the square root of the SLS\nis always better than the uniform sampling, and is poten-\ntially better than that proportional to the SLS when they\nare skewed; (iii) it motivates an optimization approach by\nsolving a constrained optimization problem related to the\nerror bound to attain better performance. In addition to\nthe theoretical analysis, we also develop an ef\ufb01cient bisec-\ntion search algorithm to solve the constrained optimization\nproblem for \ufb01nding better sampling probabilities.\nBy combining our analysis with recent developments\nfor spectral norm reconstruction of CSS (Boutsidis et al.,\n2011), we also establish the same error bound for an exact\nrank-k approximation, i.e.,\n\r\rA \u2212\u03a02\nC,k(A)\n\r\r\n2 \u2264(1 + \u01eb(s))\u2225A \u2212Ak\u22252,\nwhere \u03a02\nC,k(A) is the best approximation to A within the\ncolumn space of C that has rank at most k.\nThe remainder of the paper is organized as follows. We\nreview some closely related work in Section 2, and present\nthe main result in Section 4 with some preliminaries in Sec-\ntion 3. We conduct some empirical studies in Section 5 and\npresent the detailed analysis in Section 6. Finally, conclu-\nsion is made.\n2. Related Work\nIn this section, we review some previous work on CSS,\nlow-rank matrix approximation, and other closely related\nwork on randomized algorithms for matrices. We focus our\ndiscussion on the spectral norm reconstruction.\nDepending on whether the columns are selected deter-\nministically or randomly, the algorithms for CSS can\nbe categorized into deterministic algorithms and random-\nized algorithms. Deterministic algorithms select \u2113\u2265k\ncolumns with some deterministic selection criteria. Rep-\nresentative algorithms in this category are rank revealing\nQR factorization and its variants from the \ufb01led of nu-\nmerical linear algebra (Gu & Eisenstat, 1996; Pan, 2000;\nPan & Tang, 1999). A recent work (Boutsidis et al., 2011)\nbased on the dual set spectral sparsi\ufb01cation also falls\ninto this category which will be discussed shortly. Ran-\ndomized algorithms usually de\ufb01ne sampling probabilities\ns \u2208Rn and then select \u2113\u2265k columns based on these\nsampling probabilities.\nRepresentative sampling proba-\nbilities include ones that depend the squared Euclidean\nnorm of columns (better for Frobenius norm reconstruc-\ntion) (Frieze et al., 2004), the squared volume of simplices\nde\ufb01ned by the selected subsets of columns (known as\nvolume sampling) (Deshpande & Rademacher, 2010), and\nthe SLS (known as leverage-based sampling or subspace\nsampling) (Drineas et al., 2008; Boutsidis et al., 2009).\nDepending on whether \u2113> k is allowed, the error bounds\nfor CSS are different. Below, we review several representa-\ntive error bounds. If exactly k columns are selected to form\nC, the best bound was achieved by the rank revealing QR\nfactorization (Gu & Eisenstat, 1996) with the error bound\ngiven by:\n\u2225A \u2212PCA\u22252 \u2264\np\n1 + k(n \u2212k)\u2225A \u2212Ak\u22252.\n(1)\nwith a running time O(mnk log n).\nThe same er-\nror bound was also achieved by using volume sam-\npling (Deshpande & Rademacher, 2010). The running time\nof volume sampling based algorithms can be made close\nto linear to the size of the target matrix. Boutsidis et al.\n(2009) proposed a two-stage algorithm for selecting ex-\nactly k columns and provided error bounds for both the\nspectral norm and the Frobenius norm, where in the \ufb01rs\nstage \u0398(k log k) columns are sampled based on a distribu-\ntion related to the SLS and more if for the spectral norm\nreconstruction and in the second stage k columns are se-\nlected based on the rank revealing QR factorization. The\nspectral error bound in this work that holds with a constant\nprobability 0.8 is following:\n\u2225A\u2212PCA\u22252 \u2264\n(2)\n\u0398\n\u0010\nk log1/2 k + n1/2k3/4 log1/4(k)\n\u0011\n\u2225A \u2212Ak\u22252\nThe time complexity of their algorithm (for the spec-\ntral norm reconstruction) is given by O(min(mn2, m2n))\nsince it requires SVD of the target matrix for computing the\nsampling probabilities.\nIf more than k columns are allowed to be selected, i.e.,\n\u2113> k, better error bounds can be achieved. In the most\nrecent work by Boutsidis et al. (2011), nearly optimal error\nbounds were shown by selecting \u2113> k columns with a de-\nterministic selection criterion based on the dual set spectral\nsparsi\ufb01cation. In particular, a deterministic polynomial-\ntime algorithm 1 was proposed that achieves the following\n1A slower deterministic algorithm with a time complexity\nSampling Dependent Spectral Error Bound for CSS\nerror bound:\n\u2225A \u2212PCA\u22252 \u2264\n \n1 + 1 +\np\nn/\u2113\n1 \u2212\np\nk/\u2113\n!\n\u2225A \u2212Ak\u22252\n(3)\nin TVk + O(n\u2113k2) time where TVk is the time needed to\ncompute the top k right singular vectors of A and O(n\u2113k2)\nis the time needed to compute the selection scores. This\nbound is close to the lower bound \u2126\n\u0010q\nn+\u03b12\n\u2113+\u03b12\n\u0011\n, \u03b1 > 0\nestablished in their work. It is worth mentioning that the\nselection scores in (Boutsidis et al., 2011) computed based\non the dual set spectral sparsi\ufb01cation is dif\ufb01cult to under-\nstand than the SLS.\nAlthough our sampling dependent error bound is not di-\nrectly comparable to these results, our analysis exhibits\nthat the derived error bound could be much better than\nthat in (2). When the SLS are nonuniform, our new sam-\npling distributions could lead to a better result than (3).\nMost importantly, the sampling probabilities in our algo-\nrithm are only related to the SLS and that can be com-\nputed more ef\ufb01ciently (e.g., exactly in O(TVk) or approx-\nimately in O(mn log n) (Drineas et al., 2012)). In simula-\ntions, we observe that the new sampling distributions could\nyield even better spectral norm reconstruction than the de-\nterministic selection criterion in (Boutsidis et al., 2011), es-\npecially when the SLS are nonuniform.\nFor low rank matrix approximation, several other random-\nized algorithms have been recently developed.\nFor ex-\nample, Halko et al. (2011) used a random Gaussian ma-\ntrix \u2126\u2208Rn\u00d7\u2113or a subsampled random Fourier trans-\nform to construct a matrix \u2126and then project A into\nthe column space of Y = A\u2126, and they established nu-\nmerous spectral error bounds.\nAmong them is a com-\nparable error bound O(\np\nn/\u2113)\u2225A \u2212Ak\u22252 to (3) using\nthe subsampled random Fourier transform.\nOther ran-\ndomized algorithm for low rank approximation include\nCUR decomposition (Drineas et al., 2008; Wang & Zhang,\n2012; 2013) and the Nystr\u00a8om based approximation for PSD\nmatrices (Drineas & Mahoney, 2005; Gittens & Mahoney,\n2013).\nBesides low rank matrix approximation and column se-\nlection, CSS has also been successfully applied to least\nsquare approximation, leading to faster and interpretable\nalgorithms for over-constrained least square regression. In\nparticular, if let \u2126\u2208R\u2113\u00d7m denote a scaled sampling ma-\ntrix corresponding to selecting \u2113< m rows from A, the\nleast square problem minx\u2208Rn \u2225Ax \u2212b\u22252\n2 can be approx-\nimately solved by minx\u2208Rn \u2225\u2126Ax \u2212\u2126b\u22252\n2 (Drineas et al.,\n2008; 2006b; 2011). At ICML 2014, Ma et al. (2014) stud-\nied CSS for least square approximation from a statistical\nperspective.\nThey showed the expectation and variance\nTSVD + O(\u2113n(k2 + (\u03c1 \u2212k)2)) was also presented with an error\nbound O(\np\n\u03c1/\u2113)\u2225A \u2212Ak\u22252, where \u03c1 is the rank of A.\nof the solution to the approximated least square with uni-\nform sampling and leverage-based sampling. They found\nthat leveraging based estimator could suffer from a large\nvariance when the SLS are very nonuniform while uniform\nsampling is less vulnerable to very small SLS. This tradeoff\nis complementary to our observation. However, our obser-\nvation follows directly from the spectral norm error bound.\nMoreover, our analysis reveals that the sampling distribu-\ntion with probabilities proportional to the square root of\nthe SLS is always better than uniform sampling, suggest-\ning that intermediate sampling probabilities between SLS\nand their square roots by solving a constrained optimiza-\ntion problem could yield better performance than the mix-\ning strategy that linearly combines the SLS and uniform\nprobabilities as suggested in (Ma et al., 2014).\nThere are much more work on studying the Frobenius\nnorm\nreconstruction\nof\nCSS\n(Drineas et al.,\n2006a;\nGuruswami & Sinop,\n2012;\nBoutsidis et al.,\n2011;\nDrineas et al., 2008; Boutsidis et al., 2009).\nFor more\nreferences, we refer the reader to the survey (Mahoney,\n2011).\nIt remains an interesting question to establish\nsampling dependent error bounds for other randomized\nmatrix algorithms.\n3. Preliminaries\nLet A \u2208Rm\u00d7n be a matrix of size m \u00d7 n and has a rank of\n\u03c1 \u2264min(m, n). Let k < \u03c1 be a target rank to approximate\nA. We write the SVD decomposition of A as\nA = U\n\u0012\n\u03a31\n0\n0\n\u03a32\n\u0013 \u0012\nV \u22a4\n1\nV \u22a4\n2\n\u0013\nwhere \u03a31 \u2208Rk\u00d7k, \u03a32 \u2208R(\u03c1\u2212k)\u00d7(\u03c1\u2212k), V1 \u2208Rn\u00d7k and\nV2 \u2208Rn\u00d7(\u03c1\u2212k).\nWe use \u03c31, \u03c32, . . . to denote the sin-\ngular values of A in the descending order, and \u03bbmax(X)\nand \u03bbmin(X) to denote the maximum and minimum eigen-\nvalues of a PSD matrix X.\nFor any orthogonal matrix\nU \u2208Rn\u00d7\u2113, let U \u22a5\u2208Rn\u00d7(n\u2212\u2113) denote an orthogonal ma-\ntrix whose columns are an orthonormal basis spanning the\nsubspace of Rn that is orthogonal to the column space of\nU.\nLet s\n=\n(s1, . . . , sn) be a set of scores such that\nPn\ni=1 si = k 2, one for each column of A.\nWe will\ndrawn \u2113independent samples with replacement from the\nset [n] = {1, . . . , n} using a multinomial distribution\nwhere the probability of choosing the ith column is pi =\nsi/ Pn\nj=1 sj. Let i1, . . . , i\u2113be the indices of \u2113> k selected\ncolumns 3, and S \u2208Rn\u00d7\u2113be the corresponding sampling\n2For the sake of discussion, we are not restricting the sum of\nthese scores to be one but to be k, which does not affect our con-\nclusions.\n3Note that some of the selected columns could be duplicate.\nSampling Dependent Spectral Error Bound for CSS\nmatrix, i.e,\nSi,j =\n\u001a 1,\nif i = ij\n0,\notherwise,\nand D \u2208R\u2113\u00d7\u2113be a diagonal rescaling matrix with Djj =\n1\n\u221asij\n. Given S, we construct the C matrix as\nC = AS = (Ai1, . . . , Ai\u2113).\n(4)\nOur interest is to bound the spectral norm error between A\nand PCA for a given sampling matrix S, i.e., \u2225A\u2212PCA\u22252,\nwhere PCA projects A onto the column space of C. For\nthe bene\ufb01t of presentation, we de\ufb01ne \u2126= SD \u2208Rn\u00d7\u2113to\ndenote the sampling-and-rescaling matrix, and\nY = A\u2126,\n\u21261 = V \u22a4\n1 \u2126,\n\u21262 = V \u22a4\n2 \u2126,\n(5)\nwhere \u21261 \u2208Rk\u00d7\u2113and \u21262 \u2208R(\u03c1\u2212k)\u00d7\u2113. Since the column\nspace of Y is the same to that of C, therefore\n\u2225A \u2212PCA\u22252 = \u2225A \u2212PY A\u22252\nand we will bound \u2225A \u2212PY A\u22252 in our analysis.\nLet\nV \u22a4\n1\n= (v1, . . . , vn) \u2208Rk\u00d7n and V \u22a4\n2\n= (u1, . . . , un) \u2208\nR(\u03c1\u2212k)\u00d7n. It is easy to verify that\n\u21261 = (vi1, . . . , vi\u2113)D,\n\u21262 = (ui1, . . . , ui\u2113)D\nFinally, we let s\u2217= (s\u2217\n1, . . . , s\u2217\nn) denote the SLS of A\nrelative to the best rank-k approximation to A (Mahoney,\n2011), i.e., s\u2217\ni = \u2225vi\u22252\n2. It is not dif\ufb01cult to show that\nPn\ni=1 s\u2217\ni = k.\n4. Main Result\nBefore presenting our main result, we \ufb01rst characterize\nscores in s by two quantities as follows:\nc(s) = max\n1\u2264i\u2264n\ns\u2217\ni\nsi\n,\nq(s) = max\n1\u2264i\u2264n\nps\u2217\ni\nsi\n(6)\nBoth quantities compare s to the SLS s\u2217. With c(s) and\nq(s), we are ready to present our main theorem regarding\nthe spectral error bound.\nTheorem 1. Let A \u2208Rm\u00d7n has rank \u03c1 and C \u2208Rm\u00d7\u2113\ncontain the selected columns according to sampling scores\nin s. With a probability 1 \u2212\u03b4 \u22122k exp(\u2212\u2113/[8kc(s)]), we\nhave\n\u2225A \u2212PCA\u22252 \u2264\u03c3k+1(1 + \u01eb(s))\nwhere \u01eb(s) is\n\u01eb(s) = 3\n\uf8ee\n\uf8f0\ns\nc(s)k(\u03c1 + 1 \u2212k) log\n\u0002 \u03c1\n\u03b4\n\u0003\n\u2113\n+ q(s)k log\n\u0002 \u03c1\n\u03b4\n\u0003\n\u2113\n\uf8f9\n\uf8fb\nwhere \u03c3k+1 = \u2225A \u2212Ak\u22252 is the (k + 1)th singular value\nof A.\nRemark: Clearly, the spectral error bound and the success-\nful probability in Theorem 1 depend on the quantities c(s)\nand q(s). In the subsection below, we study the two quan-\ntities to facilitate the understanding of the result in Theo-\nrem 1.\n4.1. More about the two quantities and their tradeoffs\nThe result in Theorem 1 implies that the smaller the quan-\ntities c(s) and q(s), the better the error bound. Therefore,\nwe \ufb01rst study when c(s) and q(s) achieve their minimum\nvalues. The key results are presented in the following two\nlemmas with their proofs deferred to the supplement.\nLemma 1. The set of scores in s that minimize q(s) is given\nby si \u221dps\u2217\ni , i.e., si =\nk\u221a\ns\u2217\ni\nPn\ni=1\n\u221a\ns\u2217\ni .\nRemark: The sampling distribution with probabilities that\nare proportional to the square root of s\u2217\ni , i \u2208[n] falls in be-\ntween the uniform sampling and the leverage-based sam-\npling.\nLemma 2. c(s) \u22651, \u2200s such that Pm\ni=1 si = k. The set of\nscores in s that minimize c(s) is given by si = s\u2217\ni , and the\nminimum value of c(s) is 1.\nNext, we discuss three special samplings with s (i) propor-\ntional to the square root of the SLS, i.e., si \u221d\np\ns\u2217\ni (re-\nferred to as square-root leverage-based sampling or sqL-\nsampling for short), (ii) equal to the SLS, i.e., si = s\u2217\ni\n(referred to as leverage-based sampling or L-sampling for\nshort), and (iii) equal to uniform scalars si = k/n (referred\nto as uniform sampling or U-sampling for short). Firstly, if\nsi \u221dps\u2217\ni , q(s) achieves its minimum value and we have\nthe two quantities written as\nqsqL = 1\nk\nn\nX\ni=1\np\ns\u2217\ni\ncsqL = max\ni\ns\u2217\ni\nP\ni\np\ns\u2217\ni\nk\np\ns\u2217\ni\n= qsqL max\ni\np\ns\u2217\ni\n(7)\nIn this case, when s\u2217is \ufb02at (all SLS are equal), then\nqsqL\n=\np n\nk and csqL\n=\n1.\nThe bound becomes\neO(\np\n(\u03c1 + 1 \u2212k)k/\u2113+\np\nnk/\u21132)\u03c3k+1 that suppresses log-\narithmic terms. To analyze qsqL and csqL for skewed SLS,\nwe consider a power-law distributed SLS, i.e., there ex-\nists a small constant a and power index p > 2, such that\ns\u2217\n[i], i = 1, . . . , n ranked in descending order satisfy\ns\u2217\n[i] \u2264a2i\u2212p,\ni = 1, . . . , n\nThen it is not dif\ufb01cult to show that\n1\nk\nn\nX\ni=1\n\u221asi \u2264a\nk\n\u0012\n1 +\n2\np \u22122\n\u0013\nwhich is independent of n. Then the error bound in The-\norem 1 becomes O\n\u0012q\n\u03c1+1\u2212k\n\u2113\n+ 1\n\u2113\n\u0013\n\u03c3k+1, which is better\nthan that in (3).\nSecondly, if si \u221ds\u2217\ni , then c(s) achieves its minimum value\nand we have the two quantities written as\nqL = max\ni\n1\np\ns\u2217\ni\n,\ncL = 1\n(8)\nIn this case, when s\u2217is \ufb02at, we have qL = p n\nk and cL = 1\nSampling Dependent Spectral Error Bound for CSS\nand the same bound eO(\np\n(\u03c1 + 1 \u2212k)k/\u2113+\np\nnk/\u21132)\u03c3k+1\nfollows. However, when s\u2217is skewed, i.e., there exist very\nsmall SLS, then qL could be very large. As a comparison,\nthe q(s) for sqL-sampling is always smaller than that for\nL-sampling due the following inequality\nqsqL = 1\nk\nn\nX\ni=1\np\ns\u2217\ni = 1\nk\nn\nX\ni=1\ns\u2217\ni\nps\u2217\ni\n< max\ni\n1\nps\u2217\ni\nPn\ni=1 s\u2217\ni\nk\n= max\ni\n1\nps\u2217\ni\n= qL\nLastly, we consider the uniform sampling si = k\nn . Then\nthe two quantities become\nqU = max\ni\nn\np\ns\u2217\ni\nk\n,\ncU = max\ni\nns\u2217\ni\nk\n(9)\nSimilarly, if s\u2217is \ufb02at, qU = p n\nk and cU = 1. Moreover,\nit is interesting to compare the two quantities for the sqL-\nsampling in (7) and for the uniform sampling in (9).\nqsqL = 1\nk\nn\nX\ni=1\np\ns\u2217\ni \u2264max\ni\nn\u221asi\nk\n= qU\ncsqL = max\ni\n1\nk\np\ns\u2217\ni\nn\nX\ni=1\np\ns\u2217\ni \u2264max\ni\nns\u2217\ni\nk\n= cU\nFrom the above discussions, we can see that when s\u2217is a\n\ufb02at vector, there is no difference between the three sam-\npling scores for s. The difference comes from when s\u2217\ntends to be skewed. In this case, si \u221dps\u2217\ni works al-\nmost for sure better than uniform distribution and could\nalso be potentially better than si \u221ds\u2217\ni according to the\nsampling dependent error bound in Theorem 1. A simi-\nlar tradeoff between the L-sampling and U-sampling but\nwith a different taste was observed in (Ma et al., 2014),\nwhere they showed that for least square approximation by\nCSS leveraging-based least square estimator could have a\nlarge variance when there exist very small SLS. Nonethe-\nless, our bound here exhibits more insights, especially on\nthe sqL-sampling. More importantly, the sampling depen-\ndent bound renders the \ufb02exibility in choosing the sampling\nscores by adjusting them according to the distribution of\nthe SLS. In next subsection, we present an optimization\napproach to \ufb01nd better sampling scores. In Figure 1, we\ngive a quick view of different sampling strategies.\n4.2. Optimizing the error bound\nAs indicated by the result in Theorem 1, in order to achieve\na good performance, we need to make a balance between\nc(s) an q(s), where c(s) affects not only the error bound but\nalso the successful probability. To address this issue, we\npropose a constrained optimization approach. More specif-\nically, to ensure that the failure probability is no more than\nmore uniform \nm \nmore skewed \nU-sampling \nL-sampling \nsqL-sampling \nmpling \nL sam\nm\ng \nmixing suggested by Ma et al., 2014   \noptimization based mixing \nFigure 1. An illustration of different sampling strategies.\nThe\nmixing strategy suggested by (Ma et al., 2014) is a convex combi-\nnation of U-sampling and L-sampling. Our optimization approach\ngives an intermediate sampling between the sqL-sampling and the\nL-sampling.\n3\u03b4, we impose the following constraint on c(s)\n\u2113\n8kc(s) \u2265log\n\u0012k\n\u03b4\n\u0013\n, i.e., max\ni\ns\u2217\ni\nsi\n\u2264\n\u2113\n8k log\n\u0000 k\n\u03b4\n\u0001 := \u03b3\n(10)\nThen we cast the problem into minimizing q(s) under the\nconstraint in (10), i.e.,\nmin\ns\u2208Rn\n+\nmax\n1\u2264i\u2264n\nps\u2217\ni\nsi\ns.t.\ns\u22a41 = k, s\u2217\ni \u2264\u03b3si, i = 1, . . . , n\n(11)\nIt is easy to verify that the optimization problem in (11)\nis convex. Next, we develop an ef\ufb01cient bisection search\nalgorithm to solve the above problem with a linear conver-\ngence rate. To this end, we introduce a slack variable t and\nrewrite the optimization problem in (11) as\nmin\ns\u2208Rn\n+,t\u22650\nt,\ns.t.\ns\u22a41 = k\nand\ns\u2217\ni\nsi\n\u2264min\n\u0010\n\u03b3, t\np\ns\u2217\ni\n\u0011\n, i = 1, . . . , n\n(12)\nWe now \ufb01nd the optimal solution by performing bisection\nsearch on t. Let tmax and tmin be the upper and lower\nbounds for t. We set t = (tmin + tmax)/2 and decide the\nfeasibility of t by simply computing the quantity\nf(t) =\nn\nX\ni=1\ns\u2217\ni\nmin\n\u0000\u03b3, tps\u2217\ni\n\u0001\nEvidently, t is a feasible solution if f(t) \u2264k and is not if\nf(t) > k. Hence, we will update tmax = t if f(t) \u2264k and\ntmin = t if f(t) > k. To run the bisection algorithm, we\nneed to decide initial tmin and tmax. We can set tmin = 0.\nTo compute tmax, we make an explicit construction of s by\ndistributing the (1\u2212\u03b3\u22121) share of the largest element of s\u2217\nto the rest of the list. More speci\ufb01cally, let j be the index\nfor the largest entry in s\u2217. We set sj = \u2225s\u2217\u2225\u221e\u03b3\u22121 and\nsi = s\u2217\ni + (1 \u2212\u03b3\u22121)\u2225s\u2217\u2225\u221e/(n \u22121) for i \u0338= j. Evidently,\nthis solution satis\ufb01es the constraints s\u2217\ni \u2264\u03b3si, i \u2208[n] for\n\u03b3 \u22651. With this construction, we can show that\nq(s) \u2264max\n \n\u03b3\np\n\u2225s\u2217\u2225\u221e\n,\nn \u22121\np\n\u2225s\u2217\u2225\u221e(1 \u2212\u03b3\u22121)\n!\nSampling Dependent Spectral Error Bound for CSS\nTherefore, we set initial tmax to the value in R.H.S of the\nabove inequality. Given the optimal value of t = t\u2217we\ncompute the optimal value of si by si =\ns\u2217\ni\nmin(\u03b3,t\u2217\u221a\ns\u2217\ni ).\nThe corresponding sampling distribution clearly lies be-\ntween L-sampling and sqL-sampling. In particular, when\n\u03b3 = 1 the resulting sampling distribution is L-sampling\ndue to Lemma 2 and when \u03b3 \u2192\u221ethe resulting sampling\ndistribution approaches sqL-sampling.\nFinally, we comment on the value of \u2113. In order to make\nthe constraint in (10) feasible, we need to ensure \u03b3 \u22651.\nTherefore, we need \u2113\u2265\u2126(k log\n\u0000 k\n\u03b4\n\u0001\n).\n4.3. Subsequent Applications\nNext, we discuss two subsequent applications of CSS, one\nfor low rank approximation and one for least square ap-\nproximation.\nRank-k approximation. If a rank-k approximation is de-\nsired, we need to do some postprocessing since PCA might\nhas rank larger than k. We can use the same algorithm as\npresented in (Boutsidis et al., 2011). In particular, given\nthe constructed C \u2208Rn\u00d7\u2113, we \ufb01rst orthonormalize the\ncolumns of C to construct a matrix Q \u2208Rm\u00d7\u2113with or-\nthonormal columns, then compute the best rank-k approx-\nimation of Q\u22a4A \u2208R\u2113\u00d7n denoted by (Q\u22a4A)k, and \ufb01nally\nconstruct the low-rank approximation as Q(Q\u22a4A)k. It was\nshown that (Lemma 2.3 in (Boutsidis et al., 2011))\n\u2225A \u2212Q(Q\u22a4A)k\u22252 \u2264\n\u221a\n2\u2225A \u2212\u03a02\nC,k(A)\u22252\nwhere \u03a02\nC,k(A) is the best approximation to A within the\ncolumn space of C that has rank at most k. The running\ntime of above procedure is O(mn\u2113+ (m + n)\u21132). Regard-\ning its error bound, the above inequality together with the\nfollowing theorem implies that its spectral error bound is\nonly ampli\ufb01ed by a factor of\n\u221a\n2 compared to that of PCA.\nTheorem 2. Let A \u2208Rm\u00d7n has rank \u03c1 and C \u2208Rm\u00d7\u2113\ncontain the selected columns according to sampling scores\nin s. With a probability 1 \u2212\u03b4 \u22122k exp(\u2212\u2113/[8kc(s)]), we\nhave\n\u2225A \u2212\u03a02\nC,k(A)\u22252 \u2264\u03c3k+1(1 + \u01eb(s))\nwhere \u01eb(s) is given in Theorem 1.\nLeast Square Approximation.\nCSS has been used in\nleast square approximation for developing faster and in-\nterpretable algorithms.\nIn these applications, an over-\nconstrained least square problem is considered, i.e., given\nA \u2208Rm\u00d7n and b \u2208Rm with m \u226bn, to solve the follow-\ning problem:\nxopt = arg min\nx\u2208Rn \u2225Ax \u2212b\u22252\n2\n(13)\nThe procedure for applying CSS to least square approxima-\ntion is (i) to sample a set of \u2113> n rows from A and form\na sampling-and-rescaling matrix denoted by \u2126\u2208R\u2113\u00d7m 4;\n(ii) to solve the following reduced least square problem:\nbxopt = arg min\nx\u2208Rn \u2225\u2126Ax \u2212\u2126b\u22252\n2\n(14)\nIt is worth pointing out that in this case the SLS s\u2217=\n(s\u2217\n1, . . . , s\u2217\nm) are computed based on the the left singular\nvectors U of A by s\u2217\ni = \u2225Ui\u2217\u22252\n2, where Ui\u2217is the i-th\nrow of U. One might be interested to see whether we can\napply our analysis to derive a sampling dependent error\nbound for the approximation error \u2225xopt \u2212bxopt\u22252 sim-\nilar to previous bounds of the form \u2225xopt \u2212bxopt\u22252 \u2264\n\u01eb\n\u03c3min(A)\u2225Axtop \u2212b\u22252. Unfortunately, naively combining\nour analysis with previous analysis is a worse case analysis,\nand consequentially yields a worse bound. The reason will\nbecome clear in our later discussions. However, the statis-\ntical analysis in (Ma et al., 2014) does indicate that bxopt by\nusing sqL-sampling could have smaller variance than that\nusing L-sampling.\n5. Numerical Experiments\nBefore delving into the detailed analysis, we present some\nexperimental results. We consider synthetic data with the\ndata matrix A generated from one of the three different\nclasses of distributions introduced below, allowing the SLS\nvary from nearly uniform to very nonuniform.\n\u2022 Nearly uniform SLS (GA). Columns of A are\ngenerated from a multivariate normal distribution\nN(1m, \u03a3), where \u03a3ij = 2 \u22170.5|i\u2212j|. This data is\nreferred to as GA data.\n\u2022 Moderately nonuniform SLS (T3). Columns of A are\ngenerated from a multivariate t-distribution with 3 de-\ngree of freedom and covariance matrix \u03a3 as before.\nThis data is referred to as T3 data.\n\u2022 Very nonuniform SLS (T1). Columns of A are gen-\nerated from a multivariate t-distribution with 1 degree\nof freedom and covariance matrix \u03a3 as before. This\ndata is referred to as T1 data.\nThese distributions have been used in (Ma et al., 2014) to\ngenerate synthetic data for empirical evaluations.\nWe \ufb01rst compare the spectral norm reconstruction error\nof the three different samplings, namely L-sampling, U-\nsampling and the sqL-sampling, and the deterministic dual\nset spectral sparsi\ufb01cation algorithm. We generate synthetic\ndata with n = m = 1000 and repeat the experiments 1000\ntimes. We note that the rank of the generated data matrix\nis 1000. The averaged results are shown in Figure 2. From\nthese results we observe that (i) when the SLS are nearly\nuniform, the three sampling strategies perform similarly\nas expected; (ii) when the SLS become nonuniform, sqL-\n4We abuse the same notation \u2126.\nSampling Dependent Spectral Error Bound for CSS\n100\n200\n300\n500\n700\n900\n\u22120.8\n\u22120.6\n\u22120.4\n\u22120.2\n0\n0.2\nsample size\nlog(relative error)\nGA, k=10\n \n \nL\u2212sampling\nU\u2212sampling\nsqL\u2212sampling\ndual\u2212spectral\n100\n200\n300\n500\n700\n900\n\u22120.8\n\u22120.6\n\u22120.4\n\u22120.2\n0\n0.2\nsample size\nlog(relative error)\nGA, k=20\n \n \nL\u2212sampling\nU\u2212sampling\nsqL\u2212sampling\ndual\u2212spectral\n100\n200\n300\n500\n700\n900\n\u22120.6\n\u22120.4\n\u22120.2\n0\n0.2\nsample size\nlog(relative error)\nGA, k=40\n \n \nL\u2212sampling\nU\u2212sampling\nsqL\u2212sampling\ndual\u2212spectral\n100\n200\n300\n500\n700\n900\n\u22120.8\n\u22120.6\n\u22120.4\n\u22120.2\n0\n0.2\n0.4\n0.6\n0.8\nsample size\nlog(relative error)\nT3, k=10\n \n \nL\u2212sampling\nU\u2212sampling\nsqL\u2212sampling\ndual\u2212spectral\n100\n200\n300\n500\n700\n900\n\u22120.5\n0\n0.5\n1\nsample size\nlog(relative error)\nT3, k=20\n \n \nL\u2212sampling\nU\u2212sampling\nsqL\u2212sampling\ndual\u2212spectral\n100\n200\n300\n500\n700\n900\n\u22120.5\n0\n0.5\n1\n1.5\nsample size\nlog(relative error)\nT3, k=40\n \n \nL\u2212sampling\nU\u2212sampling\nsqL\u2212sampling\ndual\u2212spectral\n100\n200\n300\n500\n700\n900\n\u22122\n\u22121\n0\n1\n2\n3\n4\n5\n6\n7\nsample size\nlog(relative error)\nT1, k=10\n \n \nL\u2212sampling\nU\u2212sampling\nsqL\u2212sampling\ndual\u2212spectral\n100\n200\n300\n500\n700\n900\n0\n1\n2\n3\n4\n5\nsample size\nlog(relative error)\nT1, k=20\n \n \nL\u2212sampling\nU\u2212sampling\nsqL\u2212sampling\ndual\u2212spectral\n100\n200\n300\n500\n700\n900\n0\n1\n2\n3\n4\n5\n6\nsample size\nlog(relative error)\nT1, k=40\n \n \nL\u2212sampling\nU\u2212sampling\nsqL\u2212sampling\ndual\u2212spectral\nFigure 2. Comparison of the spectral error for different data, dif-\nferent samplings, different target rank and different sample size.\n\u221210\n\u22128\n\u22126\n\u22124\n\u22122\n0\n\u22120.6\n\u22120.5\n\u22120.4\n\u22120.3\n\u22120.2\n\u22120.1\nT1, k=10, l=500\nlog(\u03b3\u22121)\nlog(relative error)\n\u221210\n\u22128\n\u22126\n\u22124\n\u22122\n0\n\u22120.5\n\u22120.4\n\u22120.3\n\u22120.2\n\u22120.1\nT1, k=20, l=500\nlog(\u03b3\u22121)\nlog(relative error)\n\u221210\n\u22128\n\u22126\n\u22124\n\u22122\n0\n\u22120.3\n\u22120.25\n\u22120.2\n\u22120.15\n\u22120.1\n\u22120.05\nT1, k=40, l=500\nlog(\u03b3\u22121)\nlog(relative error)\nFigure 3. The spectral error for the sampling probabilities found\nby the constrained optimization approach with different values of\n\u03b3 \u22651. The left most point corresponds to sqL-sampling and the\nright most point corresponds to L-sampling.\nsampling performs always better than U-sampling and bet-\nter than the L-sampling when the target rank is small (e.g.,\nk = 10) or the sample size \u2113is large; (iii) when the SLS are\nnon-uniform, the spectral norm reconstruction error of sqL-\nsampling decreases faster than L-sampling w.r.t the sample\nsize \u2113; (iv) randomized algorithms generally perform better\nthan the deterministic dual set sparsi\ufb01cation algorithm.\nSecond, we compare the sampling scores found the con-\nstrained optimization with L-sampling and sqL-sampling.\nWe vary the value of \u03b3 from 1 (corresponding to L-\nsampling) to \u221e(corresponding to sqL-sampling). A result\nwith sampling size \u2113= 500 is shown in Figure 3. It demon-\nstrate that intermediate samplings found by the proposed\nconstrained optimization can perform better than both L-\nsampling and sqL-sampling.\nFinally, we apply CSS to over-constrained least square re-\ngression. To this end, we generate a synthetic data ma-\ntrix A \u2208Rm\u00d7n with m = 50 and n = 1000 similarly\nto (Ma et al., 2014). The output is generated by y = A\u22a4\u03b2+\n\u01eb where \u01eb \u223c(0, 9In) and \u03b2 = (110, 0.1130, 110)\u22a4. We\ncompare the variance and bias of the obtained estimators\nover 1000 runs for different sampling distributions. The\n0\n200\n300\n500\n700\n900\n\u22120.2\n0\n0.2\n0.4\n0.6\n0.8\nGA\nsample size\nlog(variance)\n \n \nL\u2212sampling\nU\u2212sampling\nsqL\u2212sampling\n0\n200\n300\n500\n700\n900\n\u22120.5\n0\n0.5\n1\nT3\nsample size\nlog(variance)\n \n \nL\u2212sampling\nU\u2212sampling\nsqL\u2212sampling\n0\n200\n300\n500\n700\n900\n\u22122\n\u22121.5\n\u22121\n\u22120.5\n0\n0.5\nT1\nsample size\nlog(variance)\n \n \nL\u2212sampling\nU\u2212sampling\nsqL\u2212sampling\n0\n200\n300\n500\n700\n900\n\u22128\n\u22127.5\n\u22127\n\u22126.5\n\u22126\n\u22125.5\n\u22125\nGA\nsample size\nlog(squared bias)\n \n \nL\u2212sampling\nU\u2212sampling\nsqL\u2212sampling\n0\n200\n300\n500\n700\n900\n\u22127.5\n\u22127\n\u22126.5\n\u22126\n\u22125.5\n\u22125\nT3\nsample size\nlog(squared bias)\n \n \nL\u2212sampling\nU\u2212sampling\nsqL\u2212sampling\n0\n200\n300\n500\n700\n900\n\u221211\n\u221210\n\u22129\n\u22128\n\u22127\n\u22126\nT1\nsample size\nlog(squared bias)\n \n \nL\u2212sampling\nU\u2212sampling\nsqL\u2212sampling\nFigure 4. Comparison of variance and squared bias of the estima-\ntors for different data, different samplings and different sample\nsize.\n0\n0.2\n0.4\n0.6\n0.8\n1\n\u22121\n\u22120.5\n0\n0.5\nT1, l=150\n1/\u03b3\nlog(variance)\n \n \nLU\u2212mixing\nOpt\u2212mixing\n0\n0.2\n0.4\n0.6\n0.8\n1\n\u22121.5\n\u22121\n\u22120.5\n0\nT1, l=250\n1/\u03b3\nlog(variance)\n \n \nLU\u2212mixing\nOpt\u2212mixing\n0\n0.2\n0.4\n0.6\n0.8\n1\n\u22121.8\n\u22121.6\n\u22121.4\n\u22121.2\n\u22121\n\u22120.8\nT1, l=500\n1/\u03b3\nlog(variance)\n \n \nLU\u2212mixing\nOpt\u2212mixing\n0\n0.2\n0.4\n0.6\n0.8\n1\n\u22129.5\n\u22129\n\u22128.5\n\u22128\n\u22127.5\n\u22127\n\u22126.5\n\u22126\nT1, l=150\n1/\u03b3\nlog(squared bias)\n \n \nLU\u2212mixing\nOpt\u2212mixing\n0\n0.2\n0.4\n0.6\n0.8\n1\n\u221210.5\n\u221210\n\u22129.5\n\u22129\n\u22128.5\n\u22128\n\u22127.5\nT1, l=250\n1/\u03b3\nlog(squared bias)\n \n \nLU\u2212mixing\nOpt\u2212mixing\n0\n0.2\n0.4\n0.6\n0.8\n1\n\u221211\n\u221210.5\n\u221210\n\u22129.5\n\u22129\n\u22128.5\nT1, l=500\n1/\u03b3\nlog(squared bias)\n \n \nLU\u2212mixing\nOpt\u2212mixing\nFigure 5. Comparison of variance and squared bias of the estima-\ntors for different mixing strategies. Opt refers to our optimiza-\ntion based approach and LU refers to a convex combination of L-\nsampling and U-sampling with \u03b3\u22121 as the combination weight.\nresults shown in Figure 4 demonstrate the sqL-sampling\ngives smaller variance and better bias of the estimators than\nL-sampling and U-sampling. We also compare the pro-\nposed optimization approach with the simple mixing strat-\negy (Ma et al., 2014) that uses a convex combination of the\nL-sampling and the U-sampling. The results are shown in\nFigure 5, which again support our approach.\nMore results including relative error versus varying size n\nof the target matrix, performance on a real data set and the\nFrobenius norm reconstruction error can be found in sup-\nplement.\n6. Analysis\nIn this section, we present major analysis of Theorem 1 and\nTheorem 2 with detailed proofs included in supplement.\nThe key to our analysis is the following Theorem.\nTheorem 3. Let Y, \u21261, \u21262 be de\ufb01ned in (5). Assume that\n\u21261 has full row rank. We have\n\u2225A \u2212PY A\u22252\n\u03be \u2264\u2225\u03a32\u22252\n\u03be +\n\r\r\r\u03a32\u21262\u2126\u2020\n1\n\r\r\r\n2\n\u03be\nSampling Dependent Spectral Error Bound for CSS\nand\n\r\rA \u2212\u03a02\nY,k(A)\n\r\r2\n\u03be \u2264\u2225\u03a32\u22252\n\u03be +\n\r\r\r\u03a32\u21262\u2126\u2020\n1\n\r\r\r\n2\n\u03be\nwhere \u03be could be 2 and F.\nThe \ufb01rst inequality was proved in (Halko et al., 2011)\n(Theorem 9.1) and the second inequality is credited\nto (Boutsidis et al., 2011) (Lemma 3.2) 5. Previous work\non the spectral norm analysis also start from a similar\ninequality as above.\nThey bound the second term by\nusing \u2225\u03a32\u21262\u2126\u2020\n1\u22252 \u2264\u2225\u03a32\u21262\u22252\u2225\u2126\u2020\n1\u22252 and then bound\nthe two terms separately.\nHowever, we will \ufb01rst write\n\r\r\r\u03a32\u21262\u2126\u2020\n1\n\r\r\r\n2 = \u2225\u03a32\u21262\u2126\u22a4\n1 (\u21261\u2126\u22a4\n1 )\u22121\u22252 using the fact \u21261\nhas full row rank, and then bound \u2225(\u21261\u2126\u22a4\n1 )\u22121\u22252 and\n\u2225\u21262\u2126\u22a4\n1 \u22252 separately. To this end, we will apply the Ma-\ntrix Chernoff bound as stated in Theorem 4 to bound\n\u2225(\u21261\u2126\u22a4\n1 )\u22121\u22252 and apply the matrix Bernstein inequality\nas stated in Theorem 5 to bound \u2225\u21262\u2126\u22a4\n1 \u22252.\nTheorem 4 (Matrix Chernoff (Tropp, 2012)). Let X be a\n\ufb01nite set of PSD matrices with dimension k, and suppose\nthat maxX\u2208X \u03bbmax(X) \u2264B. Sample {X1, . . . , X\u2113} inde-\npendently from X. Compute\n\u00b5max = \u2113\u03bbmax(E[X1]),\n\u00b5min = \u2113\u03bbmin(E[X1])\nThen\nPr\n(\n\u03bbmax\n \u2113\nX\ni=1\nXi\n!\n\u2265(1 + \u03b4)\u00b5max\n)\n\u2264k\n\u0014\ne\u03b4\n(1 + \u03b4)1+\u03b4\n\u0015 \u00b5max\nB\nPr\n(\n\u03bbmin\n \u2113\nX\ni=1\nXi\n!\n\u2264(1 \u2212\u03b4)\u00b5min\n)\n\u2264k\n\u0014\ne\u2212\u03b4\n(1 \u2212\u03b4)1\u2212\u03b4\n\u0015 \u00b5min\nB\nTheorem 5 (Noncommutative Bernstein Inequality (Recht,\n2011)). Let Z1, . . . , ZL be independent zero-mean ran-\ndom matrices of dimension d1 \u00d7 d2.\nSuppose \u03c4 2\nj\n=\nmax\n\b\n\u2225E[ZjZ\u22a4\nj ]\u22252, \u2225E[Z\u22a4\nj Zj\u22252\n\t\nand \u2225Zj\u22252 \u2264M al-\nmost surely for all k. Then, for any \u01eb > 0,\nPr\n\uf8ee\n\uf8f0\n\r\r\r\r\r\r\nL\nX\nj=1\nZj\n\r\r\r\r\r\r\n2\n> \u01eb\n\uf8f9\n\uf8fb\u2264(d1+d2) exp\n\"\n\u2212\u01eb2/2\nPL\nj=1 \u03c4 2\nj + M\u01eb/3\n#\nFollowing immediately from Theorem 3, we have\n\u2225A \u2212PY A\u22252 \u2264\u03c3k+1\nq\n1 + \u2225\u21262\u2126\u22a4\n1 (\u21261\u2126\u22a4\n1 )\u22121\u22252\n2\n\u2264\u03c3k+1\nq\n1 + \u2225\u21262\u2126\u22a4\n1 \u22252\n2\u03bb\u22122\nmin(\u21261\u2126\u22a4\n1 )\n\u2264\u03c3k+1(1 + \u2225\u21262\u2126\u22a4\n1 \u22252\u03bb\u22121\nmin(\u21261\u2126\u22a4\n1 )),\nwhere the last inequality uses the fact\n\u221a\na2 + b2 \u2264a +\nb. Below we bound \u03bbmin(\u21261\u2126\u22a4\n1 ) from below and bound\n\u2225\u21262\u2126\u22a4\n1 \u22252 from above.\n5In fact, the \ufb01rst inequality is implied by the second inequality.\n6.1. Bounding \u2225(\u21261\u2126\u22a4\n1 )\u22121\u22252\nWe will utilize Theorem 4 to bound \u03bbmin(\u21261\u2126\u22a4\n1 ). De\ufb01ne\nXi = viv\u22a4\ni /si. It is easy to verify that\n\u21261\u2126\u22a4\n1 =\n\u2113\nX\nj=1\n1\nsij\nvijv\u22a4\nij =\n\u2113\nX\nj=1\nXij\nand E[Xij] =\n1\nPn\ni=1 si\nPn\ni=1 siXi =\n1\nkIk, where we use\nPn\nj=1 sj = k and V \u22a4\n1 V1 = Ik.\nTherefore we have\n\u03bbmin(E[Xij]) =\n1\nk. Then the theorem below will follow\nTheorem 4.\nTheorem 6. With a probability 1 \u2212k exp(\u2212\u03b42\u2113/[2kc(s)]),\nwe have\n\u03bbmin(\u21261\u2126\u22a4\n1 ) \u2265(1 \u2212\u03b4) \u2113\nk\nTherefore, with a probability 1\u2212k exp(\u2212\u03b42\u2113/[2kc(s)]) we\nhave \u2225(\u21261\u2126\u22a4\n1 )\u22121\u22252 \u2264\n1\n1\u2212\u03b4\nk\n\u2113.\n6.2. Bounding \u2225\u21262\u2126\u22a4\n1 \u22252\nWe will utilize Theorem 5 to bound \u2225\u21262\u2126\u22a4\n1 \u22252.\nDe\ufb01ne\nZj = uijv\u22a4\nij/sij. Then\n\u21262\u2126\u22a4\n1 =\n\u2113\nX\nj=1\n1\nsij\nuijv\u22a4\nij =\nl\nX\nj=1\nZj\nand E[Zj] = 0. In order to use the matrix Bernstein in-\nequality, we will bound maxi \u2225Zi\u22252 = maxi\n\u2225uiv\u22a4\ni \u22252\nsi\n\u2264\nq(s) and \u03c4 2\nj \u2264(\u03c1+1\u2212k)c(s)\nk\n. Then we can prove the follow-\ning theorem.\nTheorem 7. With a probability 1 \u2212\u03b4, we have\n\u2225\u21262\u2126\u22a4\n1 \u22252 \u2264\nr\n2c(s)(\u03c1 + 1 \u2212k)\u2113log( \u03c1\nk)\nk\n+ 2q(s) log( \u03c1\nk)\n3\n.\nWe can complete the proof of Theorem 1 by combining\nthe bounds for \u2225\u21262\u2126\u22a4\n1 \u22252 and \u03bb\u22121\nmin(\u21261\u2126\u22a4\n1 ) and by setting\n\u03b4 = 1/2 in Theorem 6 and using union bounds.\n7. Discussions and Open Problems\nFrom the analysis, it is clear that the matrix Bernstein in-\nequality is the key to derive the sampling dependent bound\nfor \u2225\u21262\u2126\u22a4\n1 \u22252. For bounding \u03bbmin(\u21261\u2126\u22a4\n1 ), similar analysis\nusing matrix Chernoff bound has been exploited before for\nrandomized matrix approximation (Gittens, 2011).\nSince Theorem 3 also holds for the Frobenius norm, it\nmight be interested to see whether we can derive a sam-\npling dependent Frobenius norm error bound that depends\non c(s) and q(s), which, however, still remains as an\nopen problem for us.\nNonetheless, in experiments (in-\ncluded in the supplement) we observe similar phenom-\nena about the performance of L-sampling, U-sampling and\nsqL-sampling.\nSampling Dependent Spectral Error Bound for CSS\nFinally, we brie\ufb02y comment on the analysis for least square\napproximation using CSS. Previous results (Drineas et al.,\n2008; 2006b; 2011) were built on the structural conditions\nthat are characterized by two inequalities\n\u03bbmin(\u2126UU \u22a4\u2126) \u22651/\n\u221a\n2\n\u2225U \u22a4\u2126\u22a4\u2126U \u22a5U \u22a5\u22a4b\u22252\n2 \u2264\u01eb\n2\u2225U \u22a5U \u22a5\u22a4b\u22252\n2\nThe \ufb01rst condition can be guaranteed by Theorem 6 with\na high probability. For the second condition, if we adopt a\nworse case analysis\n\u2225U \u22a4\u2126\u22a4\u2126U \u22a5U \u22a5\u22a4b\u22252\n2 \u2264\u2225U \u22a4\u2126\u22a4\u2126U \u22a5\u22252\n2\u2225U \u22a5\u22a4b\u22252\n2\nand bound the \ufb01rst term in R.H.S of the above inequality\nusing Theorem 7, we would end up with a worse bound\nthan existing ones that bound the left term as a whole.\nTherefore the naive combination can\u2019t yield a good sam-\npling dependent error bound for the approximation error of\nleast square regression.\n8. Conclusions\nIn this paper, we have presented a sampling dependent\nspectral error bound for CSS. The error bound brings a new\ndistribution with sampling probabilities proportional to the\nsquare root of the statistical leverage scores and exhibits\nmore tradeoffs and insights than existing error bounds for\nCSS. We also develop a constrained optimization algorithm\nwith an ef\ufb01cient bisection search to \ufb01nd better sampling\nprobabilities for the spectral norm reconstruction. Numer-\nical simulations demonstrate that the new sampling distri-\nbutions lead to improved performance.\nReferences\nBoutsidis, Christos, Mahoney, Michael W., and Drineas,\nPetros. An improved approximation algorithm for the\ncolumn subset selection problem. In Proceedings of the\nTwentieth Annual ACM-SIAM Symposium on Discrete\nAlgorithms, pp. 968\u2013977, 2009.\nBoutsidis, Christos, Drineas, Petros, and Magdon-Ismail,\nMalik. Near optimal column-based matrix reconstruc-\ntion. In The Annual Symposium on Foundations of Com-\nputer Science, pp. 305\u2013314, 2011.\nDeshpande, Amit and Rademacher, Luis.\nEf\ufb01cient vol-\nume sampling for row/column subset selection. CoRR,\nabs/1004.4057, 2010.\nDrineas, Petros and Mahoney, Michael W. On the nystrom\nmethod for approximating a gram matrix for improved\nkernel-based learning. Journal of Machine Learning Re-\nsearch, 6:2005, 2005.\nDrineas, Petros, Mahoney, Michael W., and Muthukrish-\nnan, S.\nSubspace sampling and relative-error matrix\napproximation: Column-based methods. In APPROX-\nRANDOM, volume 4110, pp. 316\u2013326, 2006a.\nDrineas, Petros, Mahoney, Michael W., and Muthukrish-\nnan, S. Sampling algorithms for l2 regression and ap-\nplications. In ACM-SIAM Symposium on Discrete Algo-\nrithms (SODA), pp. 1127\u20131136, 2006b.\nDrineas, Petros, Mahoney, Michael W., and Muthukr-\nishnan, S.\nRelative-error cur matrix decompositions.\nSIAM Journal Matrix Analysis Applications, 30:844\u2013\n881, 2008.\nDrineas, Petros, Lewis, Jamey, and Paschou, Peristera.\nInferring geographic coordinates of origin for Euro-\npeans using small panels of ancestry informative mark-\ners. PLoS ONE, 5(8):e11892, 2010.\nDrineas, Petros, Mahoney, Michael W., Muthukrishnan,\nS., and Sarl\u00b4os, Tam`as. Faster least squares approxima-\ntion. Numerische Mathematik, 117(2):219\u2013249, Febru-\nary 2011.\nDrineas,\nPetros,\nMagdon-Ismail,\nMalik,\nMahoney,\nMichael W., and Woodruff, David P. Fast approximation\nof matrix coherence and statistical leverage. Journal of\nMachine Learning Research, 13:3475\u20133506, 2012.\nFrieze, Alan, Kannan, Ravi, and Vempala, Santosh. Fast\nmonte-carlo algorithms for \ufb01nding low-rank approxima-\ntions. Journal of ACM, 51(6):1025\u20131041, 2004.\nGittens, Alex. The spectral norm errors of the naive nys-\ntrom extension. CoRR, abs/1110.5305, 2011.\nGittens, Alex and Mahoney, Michael W. Revisiting the nys-\ntrom method for improved large-scale machine learning.\nIn Proceedings of International Conference of Machine\nLearning, volume 28, pp. 567\u2013575, 2013.\nGu, Ming and Eisenstat, Stanley C. Ef\ufb01cient algorithms\nfor computing a strong rank-revealing qr factorization.\nSIAM Journal on Scienti\ufb01c Computing, 17(4):848\u2013869,\n1996.\nGuruswami, Venkatesan and Sinop, Ali Kemal. Optimal\ncolumn-based low-rank matrix reconstruction. In Pro-\nceedings of the Twenty-third Annual ACM-SIAM Sympo-\nsium on Discrete Algorithms, pp. 1207\u20131214, 2012.\nHalko, N., Martinsson, P. G., and Tropp, J. A.\nFinding\nstructure with randomness: Probabilistic algorithms for\nconstructing approximate matrix decompositions. SIAM\nReview, 53:217\u2013288, 2011.\nJaved, A., Drineas, P., Mahoney, M. W., and Paschou, P. Ef-\n\ufb01cient genomewide selection of PCA-correlated tSNPs\nfor genotype imputation. Annals of Human Genetics, 75\n(6):707\u2013722, Nov 2011.\nSampling Dependent Spectral Error Bound for CSS\nMa, Ping, Mahoney, Michael W., and Yu, Bin. A statistical\nperspective on algorithmic leveraging. In Proceedings of\nthe 31th International Conference on Machine Learning\n(ICML), pp. 91\u201399, 2014.\nMahoney, Michael W. Randomized algorithms for matrices\nand data. Foundations and Trends in Machine Learning,\n3(2):123\u2013224, 2011.\nPan, C.-T.\nOn the existence and computation of rank-\nrevealing lu factorizations. Linear Algebra and its Appli-\ncations, 316(13):199 \u2013 222, 2000. Special Issue: Con-\nference celebrating the 60th birthday of Robert J. Plem-\nmons.\nPan, Ching-Tsuan and Tang, PingTakPeter. Bounds on sin-\ngular values revealed by qr factorizations. BIT Numer-\nical Mathematics, 39(4):740\u2013756, 1999.\nISSN 0006-\n3835.\nPaschou, Peristera, Mahoney, Michael W., Javed, A., Kidd,\nJ. R., Pakstis, A. J., Gu, S., Kidd, K. K., and Drineas,\nPetros. Intra- and interpopulation genotype reconstruc-\ntion from tagging SNPs. Genome Research, 17(1):96\u2013\n107, Jan 2007a.\nPaschou, Peristera, Ziv, Elad, Burchard, Esteban G.,\nChoudhry, Shweta, Rodriguez-Cintron, William, Ma-\nhoney, Michael W., and Drineas, Petros. PCA-correlated\nSNPs for structure identi\ufb01cation in worldwide human\npopulations. PLoS Genetics, 3(9):e160+, 2007b.\nRecht, Benjamin. A simpler approach to matrix comple-\ntion. Journal Machine Learning Research (JMLR), pp.\n3413\u20133430, 2011.\nTropp, Joel A. User-friendly tail bounds for sums of ran-\ndom matrices. Found. Comput. Math., 12(4):389\u2013434,\nAugust 2012. ISSN 1615-3375.\nWang, Shusen and Zhang, Zhihua. A scalable cur matrix\ndecomposition algorithm: Lower time complexity and\ntighter bound. In Advances in Neural Information Pro-\ncessing Systems 25, pp. 656\u2013664. 2012.\nWang, Shusen and Zhang, Zhihua. Improving cur matrix\ndecomposition and the nystr\u00a8om approximation via adap-\ntive sampling. Journal of Machine Learning Research,\n14(1):2729\u20132769, 2013.\n",
        "sentence": " Yang et al. (2015) developed sampling-dependent approximation error bound for column subset selection problem and tried to optimize the sampling-dependent error bound to obtain optimal sampling probabilities. Yang et al. (2015) developed sampling-dependent approximation error bound for column subset selection problem and tried to optimize the sampling-dependent error bound to obtain optimal sampling probabilities. Kukliansky & Shamir (2015) proposed a similar distributiondependent sampling for attribute-efficient learning in linear regression.",
        "context": "arXiv:1505.00526v1  [math.NA]  4 May 2015\nAn Explicit Sampling Dependent Spectral Error Bound\nfor Column Subset Selection\nTianbao Yang\nTIANBAO-YANG@UIOWA.EDU\nDepartment of Computer Science, the University of Iowa, Iowa City, USA\nLijun Zhang\nReferences\nBoutsidis, Christos, Mahoney, Michael W., and Drineas,\nPetros. An improved approximation algorithm for the\ncolumn subset selection problem. In Proceedings of the\nTwentieth Annual ACM-SIAM Symposium on Discrete\nAlgorithms, pp. 968\u2013977, 2009.\nrecent work by Boutsidis et al. (2011), nearly optimal error\nbounds were shown by selecting \u2113> k columns with a de-\nterministic selection criterion based on the dual set spectral\nsparsi\ufb01cation. In particular, a deterministic polynomial-"
    },
    {
        "title": "Deep learning with elastic averaging sgd",
        "author": [
            "Zhang",
            "Sixin",
            "Choromanska",
            "Anna",
            "LeCun",
            "Yann"
        ],
        "venue": "arXiv preprint arXiv:1412.6651,",
        "citeRegEx": "Zhang et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Zhang et al\\.",
        "year": 2014,
        "abstract": "We study the problem of stochastic optimization for deep learning in the\nparallel computing environment under communication constraints. A new algorithm\nis proposed in this setting where the communication and coordination of work\namong concurrent processes (local workers), is based on an elastic force which\nlinks the parameters they compute with a center variable stored by the\nparameter server (master). The algorithm enables the local workers to perform\nmore exploration, i.e. the algorithm allows the local variables to fluctuate\nfurther from the center variable by reducing the amount of communication\nbetween local workers and the master. We empirically demonstrate that in the\ndeep learning setting, due to the existence of many local optima, allowing more\nexploration can lead to the improved performance. We propose synchronous and\nasynchronous variants of the new algorithm. We provide the stability analysis\nof the asynchronous variant in the round-robin scheme and compare it with the\nmore common parallelized method ADMM. We show that the stability of EASGD is\nguaranteed when a simple stability condition is satisfied, which is not the\ncase for ADMM. We additionally propose the momentum-based version of our\nalgorithm that can be applied in both synchronous and asynchronous settings.\nAsynchronous variant of the algorithm is applied to train convolutional neural\nnetworks for image classification on the CIFAR and ImageNet datasets.\nExperiments demonstrate that the new algorithm accelerates the training of deep\narchitectures compared to DOWNPOUR and other common baseline approaches and\nfurthermore is very communication efficient.",
        "full_text": "Deep learning with Elastic Averaging SGD\nSixin Zhang\nCourant Institute, NYU\nzsx@cims.nyu.edu\nAnna Choromanska\nCourant Institute, NYU\nachoroma@cims.nyu.edu\nYann LeCun\nCenter for Data Science, NYU & Facebook AI Research\nyann@cims.nyu.edu\nAbstract\nWe study the problem of stochastic optimization for deep learning in the paral-\nlel computing environment under communication constraints. A new algorithm\nis proposed in this setting where the communication and coordination of work\namong concurrent processes (local workers), is based on an elastic force which\nlinks the parameters they compute with a center variable stored by the parameter\nserver (master). The algorithm enables the local workers to perform more explo-\nration, i.e. the algorithm allows the local variables to \ufb02uctuate further from the\ncenter variable by reducing the amount of communication between local workers\nand the master. We empirically demonstrate that in the deep learning setting, due\nto the existence of many local optima, allowing more exploration can lead to the\nimproved performance. We propose synchronous and asynchronous variants of\nthe new algorithm. We provide the stability analysis of the asynchronous vari-\nant in the round-robin scheme and compare it with the more common parallelized\nmethod ADMM. We show that the stability of EASGD is guaranteed when a simple\nstability condition is satis\ufb01ed, which is not the case for ADMM. We additionally\npropose the momentum-based version of our algorithm that can be applied in both\nsynchronous and asynchronous settings. Asynchronous variant of the algorithm\nis applied to train convolutional neural networks for image classi\ufb01cation on the\nCIFAR and ImageNet datasets. Experiments demonstrate that the new algorithm\naccelerates the training of deep architectures compared to DOWNPOUR and other\ncommon baseline approaches and furthermore is very communication ef\ufb01cient.\n1\nIntroduction\nOne of the most challenging problems in large-scale machine learning is how to parallelize the\ntraining of large models that use a form of stochastic gradient descent (SGD) [1]. There have been\nattempts to parallelize SGD-based training for large-scale deep learning models on large number\nof CPUs, including the Google\u2019s Distbelief system [2]. But practical image recognition systems\nconsist of large-scale convolutional neural networks trained on few GPU cards sitting in a single\ncomputer [3, 4]. The main challenge is to devise parallel SGD algorithms to train large-scale deep\nlearning models that yield a signi\ufb01cant speedup when run on multiple GPU cards.\nIn this paper we introduce the Elastic Averaging SGD method (EASGD) and its variants. EASGD\nis motivated by quadratic penalty method [5], but is re-interpreted as a parallelized extension of the\naveraging SGD algorithm [6]. The basic idea is to let each worker maintain its own local parameter,\nand the communication and coordination of work among the local workers is based on an elastic\nforce which links the parameters they compute with a center variable stored by the master. The center\nvariable is updated as a moving average where the average is taken in time and also in space over\nthe parameters computed by local workers. The main contribution of this paper is a new algorithm\nthat provides fast convergent minimization while outperforming DOWNPOUR method [2] and other\n1\narXiv:1412.6651v8  [cs.LG]  25 Oct 2015\nbaseline approaches in practice. Simultaneously it reduces the communication overhead between the\nmaster and the local workers while at the same time it maintains high-quality performance measured\nby the test error. The new algorithm applies to deep learning settings such as parallelized training of\nconvolutional neural networks.\nThe article is organized as follows. Section 2 explains the problem setting, Section 3 presents\nthe synchronous EASGD algorithm and its asynchronous and momentum-based variants, Section 4\nprovides stability analysis of EASGD and ADMM in the round-robin scheme, Section 5 shows ex-\nperimental results and Section 6 concludes. The Supplement contains additional material including\nadditional theoretical analysis.\n2\nProblem setting\nConsider minimizing a function F(x) in a parallel computing environment [7] with p \u2208N workers\nand a master. In this paper we focus on the stochastic optimization problem of the following form\nmin\nx F(x) := E[f(x, \u03be)],\n(1)\nwhere x is the model parameter to be estimated and \u03be is a random variable that follows the probabil-\nity distribution P over \u2126such that F(x) =\nR\n\u2126f(x, \u03be)P(d\u03be). The optimization problem in Equation 1\ncan be reformulated as follows\nmin\nx1,...,xp,\u02dcx\np\nX\ni=1\nE[f(xi, \u03bei)] + \u03c1\n2\u2225xi \u2212\u02dcx\u22252,\n(2)\nwhere each \u03bei follows the same distribution P (thus we assume each worker can sample the entire\ndataset). In the paper we refer to xi\u2019s as local variables and we refer to \u02dcx as a center variable. The\nproblem of the equivalence of these two objectives is studied in the literature and is known as the\naugmentability or the global variable consensus problem [8, 9]. The quadratic penalty term \u03c1 in\nEquation 2 is expected to ensure that local workers will not fall into different attractors that are far\naway from the center variable. This paper focuses on the problem of reducing the parameter com-\nmunication overhead between the master and local workers [10, 2, 11, 12, 13]. The problem of data\ncommunication when the data is distributed among the workers [7, 14] is a more general problem\nand is not addressed in this work. We however emphasize that our problem setting is still highly\nnon-trivial under the communication constraints due to the existence of many local optima [15].\n3\nEASGD update rule\nThe EASGD updates captured in resp. Equation 3 and 4 are obtained by taking the gradient descent\nstep on the objective in Equation 2 with respect to resp. variable xi and \u02dcx,\nxi\nt+1\n=\nxi\nt \u2212\u03b7(gi\nt(xi\nt) + \u03c1(xi\nt \u2212\u02dcxt))\n(3)\n\u02dcxt+1\n=\n\u02dcxt + \u03b7\np\nX\ni=1\n\u03c1(xi\nt \u2212\u02dcxt),\n(4)\nwhere gi\nt(xi\nt) denotes the stochastic gradient of F with respect to xi evaluated at iteration t, xi\nt and\n\u02dcxt denote respectively the value of variables xi and \u02dcx at iteration t, and \u03b7 is the learning rate.\nThe update rule for the center variable \u02dcx takes the form of moving average where the average is\ntaken over both space and time. Denote \u03b1 = \u03b7\u03c1 and \u03b2 = p\u03b1, then Equation 3 and 4 become\nxi\nt+1\n=\nxi\nt \u2212\u03b7gi\nt(xi\nt) \u2212\u03b1(xi\nt \u2212\u02dcxt)\n(5)\n\u02dcxt+1\n=\n(1 \u2212\u03b2)\u02dcxt + \u03b2\n \n1\np\np\nX\ni=1\nxi\nt\n!\n.\n(6)\nNote that choosing \u03b2 = p\u03b1 leads to an elastic symmetry in the update rule, i.e. there exists an\nsymmetric force equal to \u03b1(xi\nt \u2212\u02dcxt) between the update of each xi and \u02dcx. It has a crucial in\ufb02u-\nence on the algorithm\u2019s stability as will be explained in Section 4. Also in order to minimize the\nstaleness [16] of the difference xi\nt \u2212\u02dcxt between the center and the local variable, the update for the\nmaster in Equation 4 involves xi\nt instead of xi\nt+1.\n2\nNote also that \u03b1 = \u03b7\u03c1, where the magnitude of \u03c1 represents the amount of exploration we allow in\nthe model. In particular, small \u03c1 allows for more exploration as it allows xi\u2019s to \ufb02uctuate further\nfrom the center \u02dcx. The distinctive idea of EASGD is to allow the local workers to perform more\nexploration (small \u03c1) and the master to perform exploitation. This approach differs from other\nsettings explored in the literature [2, 17, 18, 19, 20, 21, 22, 23], and focus on how fast the center\nvariable converges. In this paper we show the merits of our approach in the deep learning setting.\n3.1\nAsynchronous EASGD\nWe discussed the synchronous update of EASGD algorithm in the previous section. In this section\nwe propose its asynchronous variant. The local workers are still responsible for updating the local\nvariables xi\u2019s, whereas the master is updating the center variable \u02dcx. Each worker maintains its own\nclock ti, which starts from 0 and is incremented by 1 after each stochastic gradient update of xi\nas shown in Algorithm 1. The master performs an update whenever the local workers \ufb01nished \u03c4\nsteps of their gradient updates, where we refer to \u03c4 as the communication period. As can be seen\nin Algorithm 1, whenever \u03c4 divides the local clock of the ith worker, the ith worker communicates\nwith the master and requests the current value of the center variable \u02dcx. The worker then waits until\nthe master sends back the requested parameter value, and computes the elastic difference \u03b1(x \u2212\u02dcx)\n(this entire procedure is captured in step a) in Algorithm 1). The elastic difference is then sent back\nto the master (step b) in Algorithm 1) who then updates \u02dcx.\nThe communication period \u03c4 controls the frequency of the communication between every local\nworker and the master, and thus the trade-off between exploration and exploitation.\nAlgorithm 1: Asynchronous EASGD:\nProcessing by worker i and the master\nInput: learning rate \u03b7, moving rate \u03b1,\ncommunication period \u03c4 \u2208N\nInitialize: \u02dcx is initialized randomly, xi = \u02dcx,\nti = 0\nRepeat\nx \u2190xi\nif (\u03c4 divides ti) then\na) xi \u2190xi \u2212\u03b1(x \u2212\u02dcx)\nb) \u02dcx \u2190\u02dcx + \u03b1(x \u2212\u02dcx)\nend\nxi \u2190xi \u2212\u03b7gi\nti(x)\nti \u2190ti + 1\nUntil forever\nAlgorithm 2: Asynchronous EAMSGD:\nProcessing by worker i and the master\nInput: learning rate \u03b7, moving rate \u03b1,\ncommunication period \u03c4 \u2208N,\nmomentum term \u03b4\nInitialize: \u02dcx is initialized randomly, xi = \u02dcx,\nvi = 0, ti = 0\nRepeat\nx \u2190xi\nif (\u03c4 divides ti) then\na) xi \u2190xi \u2212\u03b1(x \u2212\u02dcx)\nb) \u02dcx \u2190\u02dcx + \u03b1(x \u2212\u02dcx)\nend\nvi \u2190\u03b4vi \u2212\u03b7gi\nti(x + \u03b4vi)\nxi \u2190xi + vi\nti \u2190ti + 1\nUntil forever\n3.2\nMomentum EASGD\nThe momentum EASGD (EAMSGD) is a variant of our Algorithm 1 and is captured in Algorithm 2.\nIt is based on the Nesterov\u2019s momentum scheme [24, 25, 26], where the update of the local worker\nof the form captured in Equation 3 is replaced by the following update\nvi\nt+1\n=\n\u03b4vi\nt \u2212\u03b7gi\nt(xi\nt + \u03b4vi\nt)\n(7)\nxi\nt+1\n=\nxi\nt + vi\nt+1 \u2212\u03b7\u03c1(xi\nt \u2212\u02dcxt),\nwhere \u03b4 is the momentum term. Note that when \u03b4 = 0 we recover the original EASGD algorithm.\nAs we are interested in reducing the communication overhead in the parallel computing environ-\nment where the parameter vector is very large, we will be exploring in the experimental section the\nasynchronous EASGD algorithm and its momentum-based variant in the relatively large \u03c4 regime\n(less frequent communication).\n4\nStability analysis of EASGD and ADMM in the round-robin scheme\nIn this section we study the stability of the asynchronous EASGD and ADMM methods in the round-\nrobin scheme [20]. We \ufb01rst state the updates of both algorithms in this setting, and then we study\n3\ntheir stability. We will show that in the one-dimensional quadratic case, ADMM algorithm can\nexhibit chaotic behavior, leading to exponential divergence. The analytic condition for the ADMM\nalgorithm to be stable is still unknown, while for the EASGD algorithm it is very simple1.\nThe analysis of the synchronous EASGD algorithm, including its convergence rate, and its averaging\nproperty, in the quadratic and strongly convex case, is deferred to the Supplement.\nIn our setting, the ADMM method [9, 27, 28] involves solving the following minimax problem2,\nmax\n\u03bb1,...,\u03bbp\nmin\nx1,...,xp,\u02dcx\np\nX\ni=1\nF(xi) \u2212\u03bbi(xi \u2212\u02dcx) + \u03c1\n2\u2225xi \u2212\u02dcx\u22252,\n(8)\nwhere \u03bbi\u2019s are the Lagrangian multipliers. The resulting updates of the ADMM algorithm in the\nround-robin scheme are given next. Let t \u22650 be a global clock. At each t, we linearize the function\nF(xi) with F(xi\nt) +\n\n\u2207F(xi\nt), xi \u2212xi\nt\n\u000b\n+\n1\n2\u03b7\n\r\rxi \u2212xi\nt\n\r\r2 as in [28]. The updates become\n\u03bbi\nt+1\n=\n\u001a\n\u03bbi\nt \u2212(xi\nt \u2212\u02dcxt)\nif\nmod (t, p) = i \u22121;\n\u03bbi\nt\nif\nmod (t, p) \u0338= i \u22121.\n(9)\nxi\nt+1\n=\n(\nxi\nt\u2212\u03b7\u2207F (xi\nt)+\u03b7\u03c1(\u03bbi\nt+1+\u02dcxt)\n1+\u03b7\u03c1\nif\nmod (t, p) = i \u22121;\nxi\nt\nif\nmod (t, p) \u0338= i \u22121.\n(10)\n\u02dcxt+1\n=\n1\np\np\nX\ni=1\n(xi\nt+1 \u2212\u03bbi\nt+1).\n(11)\nEach local variable xi is periodically updated (with period p). First, the Lagrangian multiplier \u03bbi is\nupdated with the dual ascent update as in Equation 9. It is followed by the gradient descent update\nof the local variable as given in Equation 10. Then the center variable \u02dcx is updated with the most\nrecent values of all the local variables and Lagrangian multipliers as in Equation 11. Note that\nsince the step size for the dual ascent update is chosen to be \u03c1 by convention [9, 27, 28], we have\nre-parametrized the Lagrangian multiplier to be \u03bbi\nt \u2190\u03bbi\nt/\u03c1 in the above updates.\nThe EASGD algorithm in the round-robin scheme is de\ufb01ned similarly and is given below\nxi\nt+1\n=\n\u001a\nxi\nt \u2212\u03b7\u2207F(xi\nt) \u2212\u03b1(xi\nt \u2212\u02dcxt)\nif\nmod (t, p) = i \u22121;\nxi\nt\nif\nmod (t, p) \u0338= i \u22121.\n(12)\n\u02dcxt+1\n=\n\u02dcxt +\nX\ni:\nmod (t,p)=i\u22121\n\u03b1(xi\nt \u2212\u02dcxt).\n(13)\nAt time t, only the i-th local worker (whose index i\u22121 equals t modulo p) is activated, and performs\nthe update in Equations 12 which is followed by the master update given in Equation 13.\nWe will now focus on the one-dimensional quadratic case without noise, i.e. F(x) = x2\n2 , x \u2208R.\nFor the ADMM algorithm, let the state of the (dynamical) system at time t be st\n=\n(\u03bb1\nt, x1\nt, . . . , \u03bbp\nt , xp\nt , \u02dcxt) \u2208R2p+1. The local worker i\u2019s updates in Equations 9, 10, and 11 are\ncomposed of three linear maps which can be written as st+1 = (F i\n3 \u25e6F i\n2 \u25e6F i\n1)(st). For simplicity,\nwe will only write them out below for the case when i = 1 and p = 2:\nF 1\n1=\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ed\n1\n\u22121\n0\n0\n1\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f8, F 1\n2=\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\n1\n0\n0\n0\n0\n\u03b7\u03c1\n1+\u03b7\u03c1\n1\u2212\u03b7\n1+\u03b7\u03c1\n0\n0\n\u03b7\u03c1\n1+\u03b7\u03c1\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8, F 1\n3=\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n\u22121\np\n1\np\n\u22121\np\n1\np\n0\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8.\nFor each of the p linear maps, it\u2019s possible to \ufb01nd a simple condition such that each map, where the\nith map has the form F i\n3 \u25e6F i\n2 \u25e6F i\n1, is stable (the absolute value of the eigenvalues of the map are\n1This condition resembles the stability condition for the synchronous EASGD algorithm (Condition 17 for\np = 1) in the analysis in the Supplement.\n2The convergence analysis in [27] is based on the assumption that \u201cAt any master iteration, updates from the\nworkers have the same probability of arriving at the master.\u201d, which is not satis\ufb01ed in the round-robin scheme.\n4\nsmaller or equal to one). However, when these non-symmetric maps are composed one after another\nas follows F = F p\n3 \u25e6F p\n2 \u25e6F p\n1 \u25e6. . .\u25e6F 1\n3 \u25e6F 1\n2 \u25e6F 1\n1 , the resulting map F can become unstable! (more\nprecisely, some eigenvalues of the map can sit outside the unit circle in the complex plane).\nWe now present the numerical conditions for which the ADMM algorithm becomes unstable in the\nround-robin scheme for p = 3 and p = 8, by computing the largest absolute eigenvalue of the map\nF. Figure 1 summarizes the obtained result.\n\u03b7 (eta)\n\u03c1 (rho)\np=3\n \n \n1\n2\n3\n4\n5\n6\n7\n8\n9\n1\n2\n3\n4\n5\n6\n7\n8\n9\nx 10\n\u22123\n0.991\n0.992\n0.993\n0.994\n0.995\n0.996\n0.997\n0.998\n0.999\n1\n1.001\n\u03b7 (eta)\n\u03c1 (rho)\np=8\n \n \n1\n2\n3\n4\n5\n6\n7\n8\n9\n1\n2\n3\n4\n5\n6\n7\n8\n9\nx 10\n\u22123\n0.992\n0.994\n0.996\n0.998\n1\n1.002\nFigure 1: The largest absolute eigenvalue of the linear map F = F p\n3 \u25e6F p\n2 \u25e6F p\n1 \u25e6. . . \u25e6F 1\n3 \u25e6F 1\n2 \u25e6F 1\n1\nas a function of \u03b7 \u2208(0, 10\u22122) and \u03c1 \u2208(0, 10) when p = 3 and p = 8. To simulate the chaotic\nbehavior of the ADMM algorithm, one may pick \u03b7 = 0.001 and \u03c1 = 2.5 and initialize the state s0\neither randomly or with \u03bbi\n0 = 0, xi\n0 = \u02dcx0 = 1000, \u2200i. Figure should be read in color.\nOn the other hand, the EASGD algorithm involves composing only symmetric linear maps due to\nthe elasticity. Let the state of the (dynamical) system at time t be st = (x1\nt, . . . , xp\nt , \u02dcxt) \u2208Rp+1.\nThe activated local worker i\u2019s update in Equation 12 and the master update in Equation 13 can be\nwritten as st+1 = F i(st). In case of p = 2, the map F 1 and F 2 are de\ufb01ned as follows\nF 1=\n 1 \u2212\u03b7 \u2212\u03b1\n0\n\u03b1\n0\n1\n0\n\u03b1\n0\n1 \u2212\u03b1\n!\n, F 2=\n 1\n0\n0\n0\n1 \u2212\u03b7 \u2212\u03b1\n\u03b1\n0\n\u03b1\n1 \u2212\u03b1\n!\nFor the composite map F p \u25e6. . . \u25e6F 1 to be stable, the condition that needs to be satis\ufb01ed is actually\nthe same for each i, and is furthermore independent of p (since each linear map F i is symmetric).\nIt essentially involves the stability of the 2 \u00d7 2 matrix\n\u0010 1 \u2212\u03b7 \u2212\u03b1\n\u03b1\n\u03b1\n1 \u2212\u03b1\n\u0011\n, whose two (real)\neigenvalues \u03bb satisfy (1 \u2212\u03b7 \u2212\u03b1 \u2212\u03bb)(1 \u2212\u03b1 \u2212\u03bb) = \u03b12. The resulting stability condition (|\u03bb| \u22641)\nis simple and given as 0 \u2264\u03b7 \u22642, 0 \u2264\u03b1 \u22644\u22122\u03b7\n4\u2212\u03b7 .\n5\nExperiments\nIn this section we compare the performance of EASGD and EAMSGD with the parallel method\nDOWNPOUR and the sequential method SGD, as well as their averaging and momentum variants.\nAll the parallel comparator methods are listed below3:\n\u2022 DOWNPOUR [2], the pseudo-code of the implementation of DOWNPOUR used in this\npaper is enclosed in the Supplement.\n\u2022 Momentum DOWNPOUR (MDOWNPOUR), where the Nesterov\u2019s momentum scheme is\napplied to the master\u2019s update (note it is unclear how to apply it to the local workers or for\nthe case when \u03c4 > 1). The pseudo-code is in the Supplement.\n\u2022 A method that we call ADOWNPOUR, where we compute the average over time of the\ncenter variable \u02dcx as follows: zt+1 = (1 \u2212\u03b1t+1)zt + \u03b1t+1\u02dcxt, and \u03b1t+1 =\n1\nt+1 is a moving\nrate, and z0 = \u02dcx0. t denotes the master clock, which is initialized to 0 and incremented\nevery time the center variable \u02dcx is updated.\n\u2022 A method that we call MVADOWNPOUR, where we compute the moving average of the\ncenter variable \u02dcx as follows: zt+1 = (1 \u2212\u03b1)zt + \u03b1\u02dcxt, and the moving rate \u03b1 was chosen\nto be constant, and z0 = \u02dcx0. t denotes the master clock and is de\ufb01ned in the same way as\nfor the ADOWNPOUR method.\n3We have compared asynchronous ADMM [27] with EASGD in our setting as well, the performance is\nnearly the same. However, ADMM\u2019s momentum variant is not as stable for large communication periods.\n5\nAll the sequential comparator methods (p = 1) are listed below:\n\u2022 SGD [1] with constant learning rate \u03b7.\n\u2022 Momentum SGD (MSGD) [26] with constant momentum \u03b4.\n\u2022 ASGD [6] with moving rate \u03b1t+1 =\n1\nt+1.\n\u2022 MVASGD [6] with moving rate \u03b1 set to a constant.\nWe perform experiments in a deep learning setting on two benchmark datasets: CIFAR-10 (we refer\nto it as CIFAR) 4 and ImageNet ILSVRC 2013 (we refer to it as ImageNet) 5. We focus on the image\nclassi\ufb01cation task with deep convolutional neural networks. We next explain the experimental setup.\nThe details of the data preprocessing and prefetching are deferred to the Supplement.\n5.1\nExperimental setup\nFor all our experiments we use a GPU-cluster interconnected with In\ufb01niBand. Each node has 4 Titan\nGPU processors where each local worker corresponds to one GPU processor. The center variable of\nthe master is stored and updated on the centralized parameter server [2]6.\nTo describe the architecture of the convolutional neural network, we will \ufb01rst introduce a nota-\ntion. Let (c, y) denotes the size of the input image to each layer, where c is the number of color\nchannels and y is both the horizontal and the vertical dimension of the input.\nLet C denotes\nthe fully-connected convolutional operator and let P denotes the max pooling operator, D de-\nnotes the linear operator with dropout rate equal to 0.5 and S denotes the linear operator with\nsoftmax output non-linearity.\nWe use the cross-entropy loss and all inner layers use recti\ufb01ed\nlinear units. For the ImageNet experiment we use the similar approach to [4] with the follow-\ning 11-layer convolutional neural network (3,221)C(96,108)P(96,36)C(256,32)P(256,16)C(384,14)\nC(384,13)C(256,12)P(256,6)D(4096,1)D(4096,1)S(1000,1).\nFor the CIFAR experiment we\nuse the similar approach to [29] with the following 7-layer convolutional neural network\n(3,28)C(64,24)P(64,12)C(128,8)P(128,4)C(64,2)D(256,1)S(10,1).\nIn our experiments all the methods we run use the same initial parameter chosen randomly, except\nthat we set all the biases to zero for CIFAR case and to 0.1 for ImageNet case. This parameter is\nused to initialize the master and all the local workers7. We add l2-regularization \u03bb\n2 \u2225x\u22252 to the loss\nfunction F(x). For ImageNet we use \u03bb = 10\u22125 and for CIFAR we use \u03bb = 10\u22124. We also compute\nthe stochastic gradient using mini-batches of sample size 128.\n5.2\nExperimental results\nFor all experiments in this section we use EASGD with \u03b2 = 0.98 , for all momentum-based methods\nwe set the momentum term \u03b4 = 0.99 and \ufb01nally for MVADOWNPOUR we set the moving rate to\n\u03b1 = 0.001. We start with the experiment on CIFAR dataset with p = 4 local workers running on\na single computing node. For all the methods, we examined the communication periods from the\nfollowing set \u03c4 = {1, 4, 16, 64}. For comparison we also report the performance of MSGD which\noutperformed SGD, ASGD and MVASGD as shown in Figure 6 in the Supplement. For each method\nwe examined a wide range of learning rates (the learning rates explored in all experiments are sum-\nmarized in Table 1, 2, 3 in the Supplement). The CIFAR experiment was run 3 times independently\nfrom the same initialization and for each method we report its best performance measured by the\nsmallest achievable test error. From the results in Figure 2, we conclude that all DOWNPOUR-\nbased methods achieve their best performance (test error) for small \u03c4 (\u03c4 \u2208{1, 4}), and become\nhighly unstable for \u03c4 \u2208{16, 64}. While EAMSGD signi\ufb01cantly outperforms comparator methods\nfor all values of \u03c4 by having faster convergence. It also \ufb01nds better-quality solution measured by the\ntest error and this advantage becomes more signi\ufb01cant for \u03c4 \u2208{16, 64}. Note that the tendency to\nachieve better test performance with larger \u03c4 is also characteristic for the EASGD algorithm.\n4Downloaded from http://www.cs.toronto.edu/\u02dckriz/cifar.html.\n5Downloaded from http://image-net.org/challenges/LSVRC/2013.\n6Our implementation is available at https://github.com/sixin-zh/mpiT.\n7On the contrary, initializing the local workers and the master with different random seeds \u2019traps\u2019 the algo-\nrithm in the symmetry breaking phase.\n8Intuitively the \u2019effective \u03b2\u2019 is \u03b2/\u03c4 = p\u03b1 = p\u03b7\u03c1 (thus \u03c1 =\n\u03b2\n\u03c4p\u03b7 ) in the asynchronous setting.\n6\n50\n100\n150\n0.5\n1\n1.5\n2\nwallclock time (min)\ntraining loss (nll)\n\u03c4=1\n \n \nMSGD\nDOWNPOUR\nADOWNPOUR\nMVADOWNPOUR\nMDOWNPOUR\nEASGD\nEAMSGD\n50\n100\n150\n1\n1.5\n2\nwallclock time (min)\ntest loss (nll)\n\u03c4=1\n50\n100\n150\n16\n18\n20\n22\n24\n26\n28\nwallclock time (min)\ntest error (%)\n\u03c4=1\n50\n100\n150\n0.5\n1\n1.5\n2\nwallclock time (min)\ntraining loss (nll)\n\u03c4=4\n50\n100\n150\n1\n1.5\n2\nwallclock time (min)\ntest loss (nll)\n\u03c4=4\n50\n100\n150\n16\n18\n20\n22\n24\n26\n28\nwallclock time (min)\ntest error (%)\n\u03c4=4\n50\n100\n150\n0.5\n1\n1.5\n2\nwallclock time (min)\ntraining loss (nll)\n\u03c4=16\n50\n100\n150\n1\n1.5\n2\nwallclock time (min)\ntest loss (nll)\n\u03c4=16\n50\n100\n150\n16\n18\n20\n22\n24\n26\n28\nwallclock time (min)\ntest error (%)\n\u03c4=16\n50\n100\n150\n0.5\n1\n1.5\n2\nwallclock time (min)\ntraining loss (nll)\n\u03c4=64\n50\n100\n150\n1\n1.5\n2\nwallclock time (min)\ntest loss (nll)\n\u03c4=64\n50\n100\n150\n16\n18\n20\n22\n24\n26\n28\nwallclock time (min)\ntest error (%)\n\u03c4=64\nFigure 2: Training and test loss and the test error for the center variable versus a wallclock time for\ndifferent communication periods \u03c4 on CIFAR dataset with the 7-layer convolutional neural network.\nWe next explore different number of local workers p from the set p = {4, 8, 16} for the CIFAR\nexperiment, and p = {4, 8} for the ImageNet experiment9. For the ImageNet experiment we report\nthe results of one run with the best setting we have found. EASGD and EAMSGD were run with\n\u03c4 = 10 whereas DOWNPOUR and MDOWNPOUR were run with \u03c4 = 1. The results are in Figure 3\nand 4. For the CIFAR experiment, it\u2019s noticeable that the lowest achievable test error by either\nEASGD or EAMSGD decreases with larger p. This can potentially be explained by the fact that\nlarger p allows for more exploration of the parameter space. In the Supplement, we discuss further\nthe trade-off between exploration and exploitation as a function of the learning rate (section 9.5) and\nthe communication period (section 9.6). Finally, the results obtained for the ImageNet experiment\nalso shows the advantage of EAMSGD over the competitor methods.\n6\nConclusion\nIn this paper we describe a new algorithm called EASGD and its variants for training deep neu-\nral networks in the stochastic setting when the computations are parallelized over multiple GPUs.\nExperiments demonstrate that this new algorithm quickly achieves improvement in test error com-\npared to more common baseline approaches such as DOWNPOUR and its variants. We show that\nour approach is very stable and plausible under communication constraints. We provide the stability\nanalysis of the asynchronous EASGD in the round-robin scheme, and show the theoretical advantage\nof the method over ADMM. The different behavior of the EASGD algorithm from its momentum-\nbased variant EAMSGD is intriguing and will be studied in future works.\n9For the ImageNet experiment, the training loss is measured on a subset of the training data of size 50,000.\n7\n50\n100\n150\n0.5\n1\n1.5\n2\nwallclock time (min)\ntraining loss (nll)\np=4\n \n \nMSGD\nDOWNPOUR\nMDOWNPOUR\nEASGD\nEAMSGD\n50\n100\n150\n1\n1.5\n2\nwallclock time (min)\ntest loss (nll)\np=4\n50\n100\n150\n16\n18\n20\n22\n24\n26\n28\nwallclock time (min)\ntest error (%)\np=4\n50\n100\n150\n0.5\n1\n1.5\n2\nwallclock time (min)\ntraining loss (nll)\np=8\n50\n100\n150\n1\n1.5\n2\nwallclock time (min)\ntest loss (nll)\np=8\n50\n100\n150\n16\n18\n20\n22\n24\n26\n28\nwallclock time (min)\ntest error (%)\np=8\n50\n100\n150\n0.5\n1\n1.5\n2\nwallclock time (min)\ntraining loss (nll)\np=16\n50\n100\n150\n1\n1.5\n2\nwallclock time (min)\ntest loss (nll)\np=16\n50\n100\n150\n16\n18\n20\n22\n24\n26\n28\nwallclock time (min)\ntest error (%)\np=16\nFigure 3: Training and test loss and the test error for the center variable versus a wallclock time\nfor different number of local workers p for parallel methods (MSGD uses p = 1) on CIFAR with\nthe 7-layer convolutional neural network. EAMSGD achieves signi\ufb01cant accelerations compared to\nother methods, e.g. the relative speed-up for p = 16 (the best comparator method is then MSGD) to\nachieve the test error 21% equals 11.1.\n0\n50\n100\n150\n1\n2\n3\n4\n5\n6\nwallclock time (hour)\ntraining loss (nll)\np=4\n \n \nMSGD\nDOWNPOUR\nEASGD\nEAMSGD\n0\n50\n100\n150\n2\n3\n4\n5\n6\nwallclock time (hour)\ntest loss (nll)\np=4\n0\n50\n100\n150\n42\n44\n46\n48\n50\n52\n54\nwallclock time (hour)\ntest error (%)\np=4\n0\n50\n100\n150\n1\n2\n3\n4\n5\n6\nwallclock time (hour)\ntraining loss (nll)\np=8\n0\n50\n100\n150\n2\n3\n4\n5\n6\nwallclock time (hour)\ntest loss (nll)\np=8\n0\n50\n100\n150\n42\n44\n46\n48\n50\n52\n54\nwallclock time (hour)\ntest error (%)\np=8\nFigure 4: Training and test loss and the test error for the center variable versus a wallclock time for\ndifferent number of local workers p (MSGD uses p = 1) on ImageNet with the 11-layer convolu-\ntional neural network. Initial learning rate is decreased twice, by a factor of 5 and then 2, when we\nobserve that the online predictive loss [30] stagnates. EAMSGD achieves signi\ufb01cant accelerations\ncompared to other methods, e.g. the relative speed-up for p = 8 (the best comparator method is then\nDOWNPOUR) to achieve the test error 49% equals 1.8, and simultaneously it reduces the commu-\nnication overhead (DOWNPOUR uses communication period \u03c4 = 1 and EAMSGD uses \u03c4 = 10).\nAcknowledgments\nThe authors thank R. Power, J. Li for implementation guidance, J. Bruna, O. Henaff, C. Farabet, A.\nSzlam, Y. Bakhtin for helpful discussion, P. L. Combettes, S. Bengio and the referees for valuable\nfeedback.\n8\nReferences\n[1] Bottou, L. Online algorithms and stochastic approximations. In Online Learning and Neural Networks.\nCambridge University Press, 1998.\n[2] Dean, J, Corrado, G, Monga, R, Chen, K, Devin, M, Le, Q, Mao, M, Ranzato, M, Senior, A, Tucker, P,\nYang, K, and Ng, A. Large scale distributed deep networks. In NIPS. 2012.\n[3] Krizhevsky, A, Sutskever, I, and Hinton, G. E. Imagenet classi\ufb01cation with deep convolutional neural\nnetworks. In Advances in Neural Information Processing Systems 25, pages 1106\u20131114, 2012.\n[4] Sermanet, P, Eigen, D, Zhang, X, Mathieu, M, Fergus, R, and LeCun, Y. OverFeat: Integrated Recogni-\ntion, Localization and Detection using Convolutional Networks. ArXiv, 2013.\n[5] Nocedal, J and Wright, S. Numerical Optimization, Second Edition. Springer New York, 2006.\n[6] Polyak, B. T and Juditsky, A. B. Acceleration of stochastic approximation by averaging. SIAM Journal\non Control and Optimization, 30(4):838\u2013855, 1992.\n[7] Bertsekas, D. P and Tsitsiklis, J. N. Parallel and Distributed Computation. Prentice Hall, 1989.\n[8] Hestenes, M. R. Optimization theory: the \ufb01nite dimensional case. Wiley, 1975.\n[9] Boyd, S, Parikh, N, Chu, E, Peleato, B, and Eckstein, J. Distributed optimization and statistical learning\nvia the alternating direction method of multipliers. Found. Trends Mach. Learn., 3(1):1\u2013122, 2011.\n[10] Shamir, O. Fundamental limits of online and distributed algorithms for statistical learning and estimation.\nIn NIPS. 2014.\n[11] Yadan, O, Adams, K, Taigman, Y, and Ranzato, M. Multi-gpu training of convnets. In Arxiv. 2013.\n[12] Paine, T, Jin, H, Yang, J, Lin, Z, and Huang, T. Gpu asynchronous stochastic gradient descent to speed\nup neural network training. In Arxiv. 2013.\n[13] Seide, F, Fu, H, Droppo, J, Li, G, and Yu, D. 1-bit stochastic gradient descent and application to data-\nparallel distributed training of speech dnns. In Interspeech 2014, September 2014.\n[14] Bekkerman, R, Bilenko, M, and Langford, J.\nScaling up machine learning: Parallel and distributed\napproaches. Camridge Universityy Press, 2011.\n[15] Choromanska, A, Henaff, M. B, Mathieu, M, Arous, G. B, and LeCun, Y. The loss surfaces of multilayer\nnetworks. In AISTATS, 2015.\n[16] Ho, Q, Cipar, J, Cui, H, Lee, S, Kim, J. K, Gibbons, P. B, Gibson, G. A, Ganger, G, and Xing, E. P. More\neffective distributed ml via a stale synchronous parallel parameter server. In NIPS. 2013.\n[17] Azadi, S and Sra, S. Towards an optimal stochastic alternating direction method of multipliers. In ICML,\n2014.\n[18] Borkar, V.\nAsynchronous stochastic approximations.\nSIAM Journal on Control and Optimization,\n36(3):840\u2013851, 1998.\n[19] Nedi\u00b4c, A, Bertsekas, D, and Borkar, V. Distributed asynchronous incremental subgradient methods. In\nInherently Parallel Algorithms in Feasibility and Optimization and their Applications, volume 8 of Studies\nin Computational Mathematics, pages 381 \u2013 407. 2001.\n[20] Langford, J, Smola, A, and Zinkevich, M. Slow learners are fast. In NIPS, 2009.\n[21] Agarwal, A and Duchi, J. Distributed delayed stochastic optimization. In NIPS. 2011.\n[22] Recht, B, Re, C, Wright, S. J, and Niu, F. Hogwild: A Lock-Free Approach to Parallelizing Stochastic\nGradient Descent. In NIPS, 2011.\n[23] Zinkevich, M, Weimer, M, Smola, A, and Li, L. Parallelized stochastic gradient descent. In NIPS, 2010.\n[24] Nesterov, Y. Smooth minimization of non-smooth functions. Math. Program., 103(1):127\u2013152, 2005.\n[25] Lan, G. An optimal method for stochastic composite optimization. Mathematical Programming, 133(1-\n2):365\u2013397, 2012.\n[26] Sutskever, I, Martens, J, Dahl, G, and Hinton, G. On the importance of initialization and momentum in\ndeep learning. In ICML, 2013.\n[27] Zhang, R and Kwok, J. Asynchronous distributed admm for consensus optimization. In ICML, 2014.\n[28] Ouyang, H, He, N, Tran, L, and Gray, A. Stochastic alternating direction method of multipliers. In\nProceedings of the 30th International Conference on Machine Learning, pages 80\u201388, 2013.\n[29] Wan, L, Zeiler, M. D, Zhang, S, LeCun, Y, and Fergus, R. Regularization of neural networks using\ndropconnect. In ICML, 2013.\n[30] Cesa-Bianchi, N, Conconi, A, and Gentile, C. On the generalization ability of on-line learning algorithms.\nIEEE Transactions on Information Theory, 50(9):2050\u20132057, 2004.\n[31] Nesterov, Y. Introductory lectures on convex optimization, volume 87. Springer Science & Business\nMedia, 2004.\n9\nDeep learning with Elastic Averaging SGD\n(Supplementary Material)\n7\nAdditional theoretical results and proofs\n7.1\nQuadratic case\nWe provide here the convergence analysis of the synchronous EASGD algorithm with constant learn-\ning rate. The analysis is focused on the convergence of the center variable to the local optimum. We\ndiscuss one-dimensional quadratic case \ufb01rst, then the generalization to multi-dimensional setting\n(Lemma 7.3) and \ufb01nally to the strongly convex case (Theorem 7.1).\nOur analysis in the quadratic case extends the analysis of ASGD in [6]. Assume each of the p local\nworkers xi\nt \u2208Rn observes a noisy gradient at time t \u22650 of the linear form given in Equation 14.\ngi\nt(xi\nt) = Axi\nt \u2212b \u2212\u03bei\nt,\ni \u2208{1, . . . , p},\n(14)\nwhere the matrix A is positive-de\ufb01nite (each eigenvalue is strictly positive) and {\u03bei\nt}\u2019s are i.i.d.\nrandom variables, with zero mean and positive-de\ufb01nite covariance \u03a3. Let x\u2217denote the optimum\nsolution, where x\u2217= A\u22121b \u2208Rn. In this section we analyze the behavior of the mean squared\nerror (MSE) of the center variable \u02dcxt, where this error is denoted as E[\u2225\u02dcxt \u2212x\u2217\u22252], as a function of\nt, p, \u03b7, \u03b1 and \u03b2, where \u03b2 = p\u03b1. Note that the MSE error can be decomposed as (squared) bias and\nvariance10: E[\u2225\u02dcxt \u2212x\u2217\u22252] = \u2225E[\u02dcxt \u2212x\u2217]\u22252 + V[\u02dcxt \u2212x\u2217]. For one-dimensional case (n = 1), we\nassume A = h > 0 and \u03a3 = \u03c32 > 0.\nLemma 7.1. Let \u02dcx0 and {xi\n0}i=1,...,p be arbitrary constants, then\nE[\u02dcxt \u2212x\u2217] = \u03b3t(\u02dcx0 \u2212x\u2217) + \u03b3t \u2212\u03c6t\n\u03b3 \u2212\u03c6 \u03b1u0,\n(15)\nV[\u02dcxt \u2212x\u2217] = p2\u03b12\u03b72\n(\u03b3 \u2212\u03c6)2\n\u0012\u03b32 \u2212\u03b32t\n1 \u2212\u03b32\n+ \u03c62 \u2212\u03c62t\n1 \u2212\u03c62\n\u22122\u03b3\u03c6 \u2212(\u03b3\u03c6)t\n1 \u2212\u03b3\u03c6\n\u0013\u03c32\np ,\n(16)\nwhere u0 = Pp\ni=1(xi\n0\u2212x\u2217\u2212\n\u03b1\n1\u2212p\u03b1\u2212\u03c6(\u02dcx0\u2212x\u2217)), a = \u03b7h+(p+1)\u03b1, c2 = \u03b7hp\u03b1, \u03b3 = 1\u2212a\u2212\n\u221a\na2\u22124c2\n2\n,\nand \u03c6 = 1 \u2212a+\n\u221a\na2\u22124c2\n2\n.\nIt follows from Lemma 7.1 that for the center variable to be stable the following has to hold\n\u22121 < \u03c6 < \u03b3 < 1.\n(17)\nIt can be veri\ufb01ed that \u03c6 and \u03b3 are the two zero-roots of the polynomial in \u03bb: \u03bb2 \u2212(2 \u2212a)\u03bb + (1 \u2212\na + c2). Recall that \u03c6 and \u03bb are the functions of \u03b7 and \u03b1. Thus (see proof in Section 7.1.2)\n\u2022 \u03b3 < 1 iff c2 > 0 (i.e. \u03b7 > 0 and \u03b1 > 0).\n\u2022 \u03c6 > \u22121 iff (2 \u2212\u03b7h)(2 \u2212p\u03b1) > 2\u03b1 and (2 \u2212\u03b7h) + (2 \u2212p\u03b1) > \u03b1.\n\u2022 \u03c6 = \u03b3 iff a2 = 4c2 (i.e. \u03b7h = \u03b1 = 0).\nThe proof the above Lemma is based on the diagonalization of the linear gradient map (this map is\nsymmetric due to the relation \u03b2 = p\u03b1). The stability analysis of the asynchronous EASGD algorithm\nin the round-robin scheme is similar due to this elastic symmetry.\nProof. Substituting the gradient from Equation 14 into the update rule used by each local worker in\nthe synchronous EASGD algorithm (Equation 5 and 6) we obtain\nxi\nt+1 = xi\nt \u2212\u03b7(Axi\nt \u2212b \u2212\u03bei\nt) \u2212\u03b1(xi\nt \u2212\u02dcxt),\n(18)\n\u02dcxt+1 = \u02dcxt +\np\nX\ni=1\n\u03b1(xi\nt \u2212\u02dcxt),\n(19)\n10In our notation, V denotes the variance.\n10\nwhere \u03b7 is the learning rate, and \u03b1 is the moving rate. Recall that \u03b1 = \u03b7\u03c1 and A = h.\nFor the ease of notation we rede\ufb01ne \u02dcxt and xi\nt as follows:\n\u02dcxt \u225c\u02dcxt \u2212x\u2217and xi\nt \u225cxi\nt \u2212x\u2217.\nWe prove the lemma by explicitly solving the linear equations 18 and 19.\nLet xt\n=\n(x1\nt, . . . , xp\nt , \u02dcxt)T . We rewrite the recursive relation captured in Equation 18 and 19 as simply\nxt+1 = Mxt + bt,\nwhere the drift matrix M is de\ufb01ned as\nM =\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n1 \u2212\u03b1 \u2212\u03b7h\n0\n...\n0\n\u03b1\n0\n1 \u2212\u03b1 \u2212\u03b7h\n0\n...\n\u03b1\n...\n0\n...\n0\n...\n0\n...\n0\n1 \u2212\u03b1 \u2212\u03b7h\n\u03b1\n\u03b1\n\u03b1\n...\n\u03b1\n1 \u2212p\u03b1\n\uf8f9\n\uf8fa\uf8fa\uf8fb,\nand the (diffusion) vector bt = (\u03b7\u03be1\nt , . . . , \u03b7\u03bep\nt , 0)T .\nNote that one of the eigenvalues of matrix M, that we call \u03c6, satis\ufb01es (1\u2212\u03b1\u2212\u03b7h\u2212\u03c6)(1\u2212p\u03b1\u2212\u03c6) =\np\u03b12. The corresponding eigenvector is (1, 1, . . . , 1, \u2212\np\u03b1\n1\u2212p\u03b1\u2212\u03c6)T . Let ut be the projection of xt onto\nthis eigenvector. Thus ut = Pp\ni=1(xi\nt \u2212\n\u03b1\n1\u2212p\u03b1\u2212\u03c6 \u02dcxt). Let furthermore \u03bet = Pp\ni=1 \u03bei\nt. Therefore we\nhave\nut+1 = \u03c6ut + \u03b7\u03bet.\n(20)\nBy combining Equation 19 and 20 as follows\n\u02dcxt+1 = \u02dcxt +\np\nX\ni=1\n\u03b1(xi\nt \u2212\u02dcxt) = (1 \u2212p\u03b1)\u02dcxt + \u03b1(ut +\np\u03b1\n1 \u2212p\u03b1 \u2212\u03c6 \u02dcxt)\n= (1 \u2212p\u03b1 +\np\u03b12\n1 \u2212p\u03b1 \u2212\u03c6)\u02dcxt + \u03b1ut = \u03b3\u02dcxt + \u03b1ut,\nwhere the last step results from the following relations:\np\u03b12\n1\u2212p\u03b1\u2212\u03c6 = 1 \u2212\u03b1 \u2212\u03b7h \u2212\u03c6 and \u03c6 + \u03b3 =\n1 \u2212\u03b1 \u2212\u03b7h + 1 \u2212p\u03b1. Thus we obtained\n\u02dcxt+1 = \u03b3\u02dcxt + \u03b1ut.\n(21)\nBased on Equation 20 and 21, we can then expand ut and \u02dcxt recursively,\nut+1 = \u03c6t+1u0 + \u03c6t(\u03b7\u03be0) + . . . + \u03c60(\u03b7\u03bet),\n(22)\n\u02dcxt+1 = \u03b3t+1\u02dcx0 + \u03b3t(\u03b1u0) + . . . + \u03b30(\u03b1ut).\n(23)\nSubstituting u0, u1, . . . , ut, each given through Equation 22, into Equation 23 we obtain\n\u02dcxt = \u03b3t\u02dcx0 + \u03b3t \u2212\u03c6t\n\u03b3 \u2212\u03c6 \u03b1u0 + \u03b1\u03b7\nt\u22121\nX\nl=1\n\u03b3t\u2212l \u2212\u03c6t\u2212l\n\u03b3 \u2212\u03c6\n\u03bel\u22121.\n(24)\nTo be more speci\ufb01c, the Equation 24 is obtained by integrating by parts,\n\u02dcxt+1 = \u03b3t+1\u02dcx0 +\nt\nX\ni=0\n\u03b3t\u2212i(\u03b1ui)\n= \u03b3t+1\u02dcx0 +\nt\nX\ni=0\n\u03b3t\u2212i(\u03b1(\u03c6iu0 +\ni\u22121\nX\nl=0\n\u03c6i\u22121\u2212l\u03b7\u03bel))\n= \u03b3t+1\u02dcx0 +\nt\nX\ni=0\n\u03b3t\u2212i\u03c6i(\u03b1u0) +\nt\u22121\nX\nl=0\nt\nX\ni=l+1\n\u03b3t\u2212i\u03c6i\u22121\u2212l(\u03b1\u03b7\u03bel)\n= \u03b3t+1\u02dcx0 + \u03b3t+1 \u2212\u03c6t+1\n\u03b3 \u2212\u03c6\n(\u03b1u0) +\nt\u22121\nX\nl=0\n\u03b3t\u2212l \u2212\u03c6t\u2212l\n\u03b3 \u2212\u03c6\n(\u03b1\u03b7\u03bel).\n11\nSince the random variables \u03bel are i.i.d, we may sum the variance term by term as follows\nt\u22121\nX\nl=0\n\u0012\u03b3t\u2212l \u2212\u03c6t\u2212l\n\u03b3 \u2212\u03c6\n\u00132\n=\nt\u22121\nX\nl=0\n\u03b32(t\u2212l) \u22122\u03b3t\u2212l\u03c6t\u2212l + \u03c62(t\u2212l)\n(\u03b3 \u2212\u03c6)2\n=\n1\n(\u03b3 \u2212\u03c6)2\n\u0012\u03b32 \u2212\u03b32(t+1)\n1 \u2212\u03b32\n\u22122\u03b3\u03c6 \u2212(\u03b3\u03c6)t+1\n1 \u2212\u03b3\u03c6\n+ \u03c62 \u2212\u03c62(t+1)\n1 \u2212\u03c62\n\u0013\n.\n(25)\nNote that E[\u03bet] = Pp\ni=1 E[\u03bei\nt] = 0 and V[\u03bet] = Pp\ni=1 V[\u03bei\nt] = p\u03c32. These two facts, the equality in\nEquation 24 and Equation 25 can then be used to compute E[\u02dcxt] and V[\u02dcxt] as given in Equation 15\nand 16 in Lemma 7.1.\n7.1.1\nVisualizing Lemma 7.1\neta\nbeta\nt=1,p=1\n0\n1\n2\n0\n1\n2\neta\nbeta\nt=1,p=10\n0\n1\n2\n0\n1\n2\neta\nbeta\nt=1,p=100\n0\n1\n2\n0\n1\n2\neta\nbeta\nt=1,p=1000\n0\n1\n2\n0\n1\n2\neta\nbeta\nt=1,p=10000\n0\n1\n2\n0\n1\n2\neta\nbeta\nt=2,p=1\n0\n1\n2\n0\n1\n2\neta\nbeta\nt=2,p=10\n0\n1\n2\n0\n1\n2\neta\nbeta\nt=2,p=100\n0\n1\n2\n0\n1\n2\neta\nbeta\nt=2,p=1000\n0\n1\n2\n0\n1\n2\neta\nbeta\nt=2,p=10000\n0\n1\n2\n0\n1\n2\neta\nbeta\nt=10,p=1\n0\n1\n2\n0\n1\n2\neta\nbeta\nt=10,p=10\n0\n1\n2\n0\n1\n2\neta\nbeta\nt=10,p=100\n0\n1\n2\n0\n1\n2\neta\nbeta\nt=10,p=1000\n0\n1\n2\n0\n1\n2\neta\nbeta\nt=10,p=10000\n0\n1\n2\n0\n1\n2\neta\nbeta\nt=100,p=1\n0\n1\n2\n0\n1\n2\neta\nbeta\nt=100,p=10\n0\n1\n2\n0\n1\n2\neta\nbeta\nt=100,p=100\n0\n1\n2\n0\n1\n2\neta\nbeta\nt=100,p=1000\n0\n1\n2\n0\n1\n2\neta\nbeta\nt=100,p=10000\n0\n1\n2\n0\n1\n2\neta\nbeta\nt=inf,p=1\n0\n1\n2\n0\n1\n2\neta\nbeta\nt=inf,p=10\n0\n1\n2\n0\n1\n2\neta\nbeta\nt=inf,p=100\n0\n1\n2\n0\n1\n2\neta\nbeta\nt=inf,p=1000\n0\n1\n2\n0\n1\n2\neta\nbeta\nt=inf,p=10000\n0\n1\n2\n0\n1\n2\nFigure 5: Theoretical mean squared error (MSE) of the center \u02dcx in the quadratic case, with various\nchoices of the learning rate \u03b7 (horizontal within each block), and the moving rate \u03b2 = p\u03b1 (vertical\nwithin each block), the number of processors p = {1, 10, 100, 1000, 10000} (vertical across blocks),\nand the time steps t = {1, 2, 10, 100, \u221e} (horizontal across blocks). The MSE is plotted in log scale,\nranging from 10\u22123 to 103 (from deep blue to red). The dark red (i.e. on the upper-right corners)\nindicates divergence.\nIn Figure 5, we illustrate the dependence of MSE on \u03b2, \u03b7 and the number of processors p over time\nt. We consider the large-noise setting where \u02dcx0 = xi\n0 = 1, h = 1 and \u03c3 = 10. The MSE error\nis color-coded such that the deep blue color corresponds to the MSE equal to 10\u22123, the green color\ncorresponds to the MSE equal to 1, the red color corresponds to MSE equal to 103 and the dark red\ncolor corresponds to the divergence of algorithm EASGD (condition in Equation 17 is then violated).\nThe plot shows that we can achieve signi\ufb01cant variance reduction by increasing the number of local\nworkers p. This effect is less sensitive to the choice of \u03b2 and \u03b7 for large p.\n12\n7.1.2\nCondition in Equation 17\nWe are going to show that\n\u2022 \u03b3 < 1 iff c2 > 0 (i.e. \u03b7 > 0 and \u03b2 > 0).\n\u2022 \u03c6 > \u22121 iff (2 \u2212\u03b7h)(2 \u2212\u03b2) > 2\u03b2/p and (2 \u2212\u03b7h) + (2 \u2212\u03b2) > \u03b2/p.\n\u2022 \u03c6 = \u03b3 iff a2 = 4c2 (i.e. \u03b7h = \u03b2 = 0).\nRecall that a = \u03b7h + (p + 1)\u03b1, c2 = \u03b7hp\u03b1, \u03b3 = 1 \u2212a\u2212\n\u221a\na2\u22124c2\n2\n, \u03c6 = 1 \u2212a+\n\u221a\na2\u22124c2\n2\n, and \u03b2 = p\u03b1.\nWe have\n\u2022 \u03b3 < 1 \u21d4a\u2212\n\u221a\na2\u22124c2\n2\n> 0 \u21d4a >\n\u221a\na2 \u22124c2 \u21d4a2 > a2 \u22124c2 \u21d4c2 > 0.\n\u2022 \u03c6 > \u22121 \u21d42 > a+\n\u221a\na2\u22124c2\n2\n\u21d44 \u2212a >\n\u221a\na2 \u22124c2 \u21d44 \u2212a > 0, (4 \u2212a)2 > a2 \u22124c2 \u21d4\n4 \u2212a > 0, 4 \u22122a + c2 > 0 \u21d44 > \u03b7h + \u03b2 + \u03b1, 4 \u22122(\u03b7h + \u03b2 + \u03b1) + \u03b7h\u03b2 > 0.\n\u2022 \u03c6 = \u03b3 \u21d4\n\u221a\na2 \u22124c2 = 0 \u21d4a2 = 4c2.\nThe next corollary is a consequence of Lemma 7.1. As the number of workers p grows, the averaging\nproperty of the EASGD can be characterized as follows\nCorollary 7.1. Let the Elastic Averaging relation \u03b2 = p\u03b1 and the condition 17 hold, then\nlim\np\u2192\u221elim\nt\u2192\u221epE[(\u02dcxt \u2212x\u2217)2] =\n\u03b2\u03b7h\n(2 \u2212\u03b2)(2 \u2212\u03b7h) \u00b7 2 \u2212\u03b2 \u2212\u03b7h + \u03b2\u03b7h\n\u03b2 + \u03b7h \u2212\u03b2\u03b7h\n\u00b7 \u03c32\nh2 .\nProof. Note that when \u03b2 is \ufb01xed, limp\u2192\u221ea = \u03b7h + \u03b2 and c2 = \u03b7h\u03b2. Then limp\u2192\u221e\u03c6 = min(1 \u2212\n\u03b2, 1 \u2212\u03b7h) and limp\u2192\u221e\u03b3 = max(1 \u2212\u03b2, 1 \u2212\u03b7h). Also note that using Lemma 7.1 we obtain\nlim\nt\u2192\u221eE[(\u02dcxt \u2212x\u2217)2] =\n\u03b22\u03b72\n(\u03b3 \u2212\u03c6)2\n\u0012\n\u03b32\n1 \u2212\u03b32 +\n\u03c62\n1 \u2212\u03c62 \u2212\n2\u03b3\u03c6\n1 \u2212\u03b3\u03c6\n\u0013\u03c32\np\n=\n\u03b22\u03b72\n(\u03b3 \u2212\u03c6)2\n\u0012\u03b32(1 \u2212\u03c62)(1 \u2212\u03c6\u03b3) + \u03c62(1 \u2212\u03b32)(1 \u2212\u03c6\u03b3) \u22122\u03b3\u03c6(1 \u2212\u03b32)(1 \u2212\u03c62)\n(1 \u2212\u03b32)(1 \u2212\u03c62)(1 \u2212\u03b3\u03c6)\n\u0013\u03c32\np\n=\n\u03b22\u03b72\n(\u03b3 \u2212\u03c6)2\n\u0012\n(\u03b3 \u2212\u03c6)2(1 + \u03b3\u03c6)\n(1 \u2212\u03b32)(1 \u2212\u03c62)(1 \u2212\u03b3\u03c6)\n\u0013\u03c32\np\n=\n\u03b22\u03b72\n(1 \u2212\u03b32)(1 \u2212\u03c62) \u00b7 1 + \u03b3\u03c6\n1 \u2212\u03b3\u03c6 \u00b7 \u03c32\np .\nCorollary 7.1 is obtained by plugining in the limiting values of \u03c6 and \u03b3.\nThe crucial point of Corollary 7.1 is that the MSE in the limit t \u2192\u221eis in the order of 1/p which\nimplies that as the number of processors p grows, the MSE will decrease for the EASGD algorithm.\nAlso note that the smaller the \u03b2 is (recall that \u03b2 = p\u03b1 = p\u03b7\u03c1), the more exploration is allowed\n(small \u03c1) and simultaneously the smaller the MSE is.\n7.2\nGeneralization to multidimensional case\nThe next lemma (Lemma 7.2) shows that EASGD algorithm achieves the highest possible rate of\nconvergence when we consider the double averaging sequence (similarly to [6]) {z1, z2, . . . } de\ufb01ned\nas below\nzt+1 =\n1\nt + 1\nt\nX\nk=0\n\u02dcxk.\n(26)\nLemma 7.2 (Weak convergence). If the condition in Equation 17 holds, then the normalized double\naveraging sequence de\ufb01ned in Equation 26 converges weakly to the normal distribution with zero\nmean and variance \u03c32/ph2,\n\u221a\nt(zt \u2212x\u2217) \u21c0N(0, \u03c32\nph2 ),\nt \u2192\u221e.\n(27)\n13\nProof. As in the proof of Lemma 7.1, for the ease of notation we rede\ufb01ne \u02dcxt and xi\nt as follows:\n\u02dcxt \u225c\u02dcxt \u2212x\u2217and xi\nt \u225cxi\nt \u2212x\u2217.\nAlso recall that {\u03bei\nt}\u2019s are i.i.d. random variables (noise) with zero mean and the same covariance\n\u03a3 \u227b0. We are interested in the asymptotic behavior of the double averaging sequence {z1, z2, . . . }\nde\ufb01ned as\nzt+1 =\n1\nt + 1\nt\nX\nk=0\n\u02dcxk.\n(28)\nRecall the Equation 24 from the proof of Lemma 7.1 (for the convenience it is provided below):\n\u02dcxk = \u03b3k\u02dcx0 + \u03b1u0\n\u03b3k \u2212\u03c6k\n\u03b3 \u2212\u03c6\n+ \u03b1\u03b7\nk\u22121\nX\nl=1\n\u03b3k\u2212l \u2212\u03c6k\u2212l\n\u03b3 \u2212\u03c6\n\u03bel\u22121,\nwhere \u03bet = Pp\ni=1 \u03bei\nt. Therefore\nt\nX\nk=0\n\u02dcxk = 1 \u2212\u03b3t+1\n1 \u2212\u03b3\n\u02dcx0 + \u03b1u0\n1\n\u03b3 \u2212\u00b5\n\u00121 \u2212\u03b3t+1\n1 \u2212\u03b3\n\u22121 \u2212\u03c6t+1\n1 \u2212\u03c6\n\u0013\n+ \u03b1\u03b7\nt\u22121\nX\nl=1\nt\nX\nk=l+1\n\u03b3k\u2212l \u2212\u03c6k\u2212l\n\u03b3 \u2212\u03c6\n\u03bel\u22121\n= O(1) + \u03b1\u03b7\nt\u22121\nX\nl=1\n1\n\u03b3 \u2212\u03c6\n\u0012\n\u03b3 1 \u2212\u03b3t\u2212l\n1 \u2212\u03b3\n\u2212\u03c61 \u2212\u03c6t\u2212l\n1 \u2212\u03c6\n\u0013\n\u03bel\u22121\nNote that the only non-vanishing term (in weak convergence) of 1/\n\u221a\nt Pt\nk=0 \u02dcxk as t \u2192\u221eis\n1\n\u221a\nt\u03b1\u03b7\nt\u22121\nX\nl=1\n1\n\u03b3 \u2212\u03c6\n\u0012\n\u03b3\n1 \u2212\u03b3 \u2212\n\u03c6\n1 \u2212\u03c6\n\u0013\n\u03bel\u22121.\n(29)\nAlso recall that V[\u03bel\u22121] = p\u03c32 and\n1\n\u03b3 \u2212\u03c6\n\u0012\n\u03b3\n1 \u2212\u03b3 \u2212\n\u03c6\n1 \u2212\u03c6\n\u0013\n=\n1\n(1 \u2212\u03b3)(1 \u2212\u03c6) =\n1\n\u03b7hp\u03b1.\nTherefore the expression in Equation 29 is asymptotically normal with zero mean and variance\n\u03c32/ph2.\nThe asymptotic variance in the Lemma 7.2 is optimal with any \ufb01xed \u03b7 and \u03b2 for which Equation 17\nholds. The next lemma (Lemma 7.3) extends the result in Lemma 7.2 to the multi-dimensional\nsetting.\nLemma 7.3 (Weak convergence). Let h denotes the largest eigenvalue of A. If (2 \u2212\u03b7h)(2 \u2212\u03b2) >\n2\u03b2/p, (2 \u2212\u03b7h) + (2 \u2212\u03b2) > \u03b2/p, \u03b7 > 0 and \u03b2 > 0, then the normalized double averaging\nsequence converges weakly to the normal distribution with zero mean and the covariance matrix\nV = A\u22121\u03a3(A\u22121)T ,\n\u221atp(zt \u2212x\u2217) \u21c0N(0, V ),\nt \u2192\u221e.\n(30)\nProof. Since A is symmetric, one can use the proof technique of Lemma 7.2 to prove Lemma 7.3\nby diagonalizing the matrix A. This diagonalization essentially generalizes Lemma 7.1 to the mul-\ntidimensional case. We will not go into the details of this proof as we will provide a simpler way\nto look at the system. As in the proof of Lemma 7.1 and Lemma 7.2, for the ease of notation we\nrede\ufb01ne \u02dcxt and xi\nt as follows:\n\u02dcxt \u225c\u02dcxt \u2212x\u2217and xi\nt \u225cxi\nt \u2212x\u2217.\nLet the spatial average of the local parameters at time t be denoted as yt where yt = 1\np\nPp\ni=1 xi\nt,\nand let the average noise be denoted as \u03bet, where \u03bet = 1\np\nPp\ni=1 \u03bei\nt. Equations 18 and 19 can then be\nreduced to the following\nyt+1\n=\nyt \u2212\u03b7(Ayt \u2212\u03bet) + \u03b1(\u02dcxt \u2212yt),\n(31)\n\u02dcxt+1\n=\n\u02dcxt + \u03b2(yt \u2212\u02dcxt).\n(32)\n14\nWe focus on the case where the learning rate \u03b7 and the moving rate \u03b1 are kept constant over time11.\nRecall \u03b2 = p\u03b1 and \u03b1 = \u03b7\u03c1.\nLet\u2019s introduce the block notation Ut = (yt, \u02dcxt), \u039et = (\u03b7\u03bet, 0), M = I \u2212\u03b7L and\nL =\n\u0012 A + \u03b1\n\u03b7 I\n\u2212\u03b1\n\u03b7 I\n\u2212\u03b2\n\u03b7 I\n\u03b2\n\u03b7 I\n\u0013\n.\nFrom Equations 31 and 32 it follows that Ut+1 = MUt + \u039et. Note that this linear system has a\ndegenerate noise \u039et which prevents us from directly applying results of [6]. Expanding this recursive\nrelation and summing by parts, we have\nt\nX\nk=0\nUk\n=\nM 0U0 +\nM 1U0 + M 0\u039e0 +\nM 2U0 + M 1\u039e0 + M 0\u039e1 +\n...\nM tU0 + M t\u22121\u039e0 + \u00b7 \u00b7 \u00b7 + M 0\u039et\u22121.\nBy Lemma 7.4, \u2225M\u22252 < 1 and thus\nM 0 + M 1 + \u00b7 \u00b7 \u00b7 + M t + \u00b7 \u00b7 \u00b7 = (I \u2212M)\u22121 = \u03b7\u22121L\u22121.\nSince A is invertible, we get\nL\u22121 =\n\u0012 A\u22121\n\u03b1\n\u03b2 A\u22121\nA\u22121\n\u03b7\n\u03b2 + \u03b1\n\u03b2 A\u22121\n\u0013\n,\nthus\n1\n\u221a\nt\nt\nX\nk=0\nUk = 1\n\u221a\ntU0 + 1\n\u221a\nt\u03b7L\u22121\nt\nX\nk=1\n\u039ek\u22121 \u22121\n\u221a\nt\nt\nX\nk=1\nM k+1\u039ek\u22121.\nNote\nthat\nthe\nonly\nnon-vanishing\nterm\n(in\nweak\nconvergence)\nof\n1\n\u221a\nt\nPt\nk=0 Uk\nis\n1\n\u221a\nt(\u03b7L)\u22121 Pt\nk=1 \u039ek\u22121 thus we have\n1\n\u221a\nt(\u03b7L)\u22121\nt\nX\nk=1\n\u039ek\u22121 \u21c0N\n\u0012 \u0012\n0\n0\n\u0013\n,\n\u0012\nV\nV\nV\nV\n\u0013 \u0013\n,\n(33)\nwhere V = A\u22121\u03a3(A\u22121)T .\nLemma 7.4. If the following conditions hold:\n(2 \u2212\u03b7h)(2 \u2212p\u03b1)\n>\n2\u03b1\n(2 \u2212\u03b7h) + (2 \u2212p\u03b1)\n>\n\u03b1\n\u03b7\n>\n0\n\u03b1\n>\n0\nthen \u2225M\u22252 < 1.\nProof. The eigenvalue \u03bb of M and the (non-zero) eigenvector (y, z) of M satisfy\nM\n\u0012\ny\nz\n\u0013\n= \u03bb\n\u0012\ny\nz\n\u0013\n.\n(34)\n11As a side note, notice that the center parameter \u02dcxt is tracking the spatial average yt of the local parameters\nwith a non-symmetric spring in Equation 31 and 32. To be more precise note that the update on yt+1 contains\n(\u02dcxt \u2212yt) scaled by \u03b1, whereas the update on \u02dcxt+1 contains \u2212(\u02dcxt \u2212yt) scaled by \u03b2. Since \u03b1 = \u03b2/p the impact\nof the center \u02dcxt+1 on the spatial local average yt+1 becomes more negligible as p grows.\n15\nRecall that\nM = I \u2212\u03b7L =\n\u0012\nI \u2212\u03b7A \u2212\u03b1I\n\u03b1I\n\u03b2I\nI \u2212\u03b2I\n\u0013\n.\n(35)\nFrom the Equations 34 and 35 we obtain\n\u001a\ny \u2212\u03b7Ay \u2212\u03b1y + \u03b1z = \u03bby\n\u03b2y + (1 \u2212\u03b2)z = \u03bbz\n.\n(36)\nSince (y, z) is assumed to be non-zero, we can write z = \u03b2y/(\u03bb + \u03b2 \u22121). Then the Equation 36\ncan be reduced to\n\u03b7Ay = (1 \u2212\u03b1 \u2212\u03bb)y +\n\u03b1\u03b2\n\u03bb + \u03b2 \u22121y.\n(37)\nThus y is the eigenvector of A. Let \u03bbA be the eigenvalue of matrix A such that Ay = \u03bbAy. Thus\nbased on Equation 37 it follows that\n\u03b7\u03bbA = (1 \u2212\u03b1 \u2212\u03bb) +\n\u03b1\u03b2\n\u03bb + \u03b2 \u22121.\n(38)\nEquation 38 is equivalent to\n\u03bb2 \u2212(2 \u2212a)\u03bb + (1 \u2212a + c2) = 0,\n(39)\nwhere a = \u03b7\u03bbA + (p + 1)\u03b1, c2 = \u03b7\u03bbAp\u03b1. It follows from the condition in Equation 17 that\n\u22121 < \u03bb < 1 iff \u03b7 > 0, \u03b2 > 0, (2 \u2212\u03b7\u03bbA)(2 \u2212\u03b2) > 2\u03b2/p and (2 \u2212\u03b7\u03bbA) + (2 \u2212\u03b2) > \u03b2/p.\nLet h denote the maximum eigenvalue of A and note that 2 \u2212\u03b7\u03bbA \u22652 \u2212\u03b7h. This implies that the\ncondition of our lemma is suf\ufb01cient.\nAs in Lemma 7.2, the asymptotic covariance in the Lemma 7.3 is optimal, i.e. meets the Fisher\ninformation lower-bound. The fact that this asymptotic covariance matrix V does not contain any\nterm involving \u03c1 is quite remarkable, since the penalty term \u03c1 does have an impact on the condition\nnumber of the Hessian in Equation 2.\n7.3\nStrongly convex case\nWe now extend the above proof ideas to analyze the strongly convex case, in which the noisy gradient\ngi\nt(x) = \u2207F(x) \u2212\u03bei\nt has the regularity that there exists some 0 < \u00b5 \u2264L, for which \u00b5 \u2225x \u2212y\u22252 \u2264\n\u27e8\u2207F(x) \u2212\u2207F(y), x \u2212y\u27e9\u2264L \u2225x \u2212y\u22252 holds uniformly for any x \u2208Rd, y \u2208Rd. The noise\n{\u03bei\nt}\u2019s is assumed to be i.i.d. with zero mean and bounded variance E[\n\r\r\u03bei\nt\n\r\r2] \u2264\u03c32.\nTheorem 7.1. Let at = E\n\r\r\r 1\np\nPp\ni=1 xi\nt \u2212x\u2217\r\r\r\n2\n, bt = 1\np\nPp\ni=1 E\n\r\rxi\nt \u2212x\u2217\r\r2, ct = E \u2225\u02dcxt \u2212x\u2217\u22252,\n\u03b31 = 2\u03b7 \u00b5L\n\u00b5+L and \u03b32 = 2\u03b7L(1 \u22122\u221a\u00b5L\n\u00b5+L ). If 0 \u2264\u03b7 \u2264\n2\n\u00b5+L(1 \u2212\u03b1), 0 \u2264\u03b1 < 1 and 0 \u2264\u03b2 \u22641 then\n at+1\nbt+1\nct+1\n!\n\u2264\n 1 \u2212\u03b31 \u2212\u03b32 \u2212\u03b1\n\u03b32\n\u03b1\n0\n1 \u2212\u03b31 \u2212\u03b1\n\u03b1\n\u03b2\n0\n1 \u2212\u03b2\n!  at\nbt\nct\n!\n+\n\uf8eb\n\uf8ed\n\u03b72 \u03c32\np\n\u03b72\u03c32\n0\n\uf8f6\n\uf8f8.\nProof. The idea of the proof is based on the point of view in Lemma 7.3, i.e. how close the center\nvariable \u02dcxt is to the spatial average of the local variables yt = 1\np\nPp\ni=1 xi\nt. To further simplify the\nnotation, let the noisy gradient be \u2207f i\nt,\u03be = gi\nt(xi\nt) = \u2207F(xi\nt) \u2212\u03bei\nt, and \u2207f i\nt = \u2207F(xi\nt) be its\ndeterministic part. Then EASGD updates can be rewritten as follows,\nxi\nt+1\n=\nxi\nt \u2212\u03b7\u2207f i\nt,\u03be \u2212\u03b1(xi\nt \u2212\u02dcxt),\n(40)\n\u02dcxt+1\n=\n\u02dcxt + \u03b2(yt \u2212\u02dcxt).\n(41)\nWe have thus the update for the spatial average,\nyt+1\n=\nyt \u2212\u03b7 1\np\np\nX\ni=1\n\u2207f i\nt,\u03be \u2212\u03b1(yt \u2212\u02dcxt).\n(42)\n16\nThe idea of the proof is to bound the distance \u2225\u02dcxt \u2212x\u2217\u22252\nthrough \u2225yt \u2212x\u2217\u22252\nand\n1\np\nPp\ni\n\r\rxi\nt \u2212x\u2217\r\r2. W start from the following estimate for the strongly convex function [31],\n\u27e8\u2207F(x) \u2212\u2207F(y), x \u2212y\u27e9\u2265\n\u00b5L\n\u00b5 + L \u2225x \u2212y\u22252 +\n1\n\u00b5 + L \u2225\u2207F(x) \u2212\u2207F(y)\u22252 .\nSince \u2207f(x\u2217) = 0, we have\n\n\u2207f i\nt, xi\nt \u2212x\u2217\u000b\n\u2265\n\u00b5L\n\u00b5 + L\n\r\rxi\nt \u2212x\u2217\r\r2 +\n1\n\u00b5 + L\n\r\r\u2207f i\nt\n\r\r2 .\n(43)\nFrom Equation 40 the following relation holds,\n\r\rxi\nt+1 \u2212x\u2217\r\r2\n=\n\r\rxi\nt \u2212x\u2217\r\r2 + \u03b72 \r\r\u2207f i\nt,\u03be\n\r\r2 + \u03b12 \r\rxi\nt \u2212\u02dcxt\n\r\r2\n\u2212\n2\u03b7\n\n\u2207f i\nt,\u03be, xi\nt \u2212x\u2217\u000b\n\u22122\u03b1\n\nxi\nt \u2212\u02dcxt, xi\nt \u2212x\u2217\u000b\n+\n2\u03b7\u03b1\n\n\u2207f i\nt,\u03be, xi\nt \u2212\u02dcxt\n\u000b\n.\n(44)\nBy the cosine rule (2 \u27e8a \u2212b, c \u2212d\u27e9= \u2225a \u2212d\u22252 \u2212\u2225a \u2212c\u22252 + \u2225c \u2212b\u22252 \u2212\u2225d \u2212b\u22252), we have\n2\n\nxi\nt \u2212\u02dcxt, xi\nt \u2212x\u2217\u000b\n=\n\r\rxi\nt \u2212x\u2217\r\r2 +\n\r\rxi\nt \u2212\u02dcxt\n\r\r2 \u2212\u2225\u02dcxt \u2212x\u2217\u22252 .\n(45)\nBy the Cauchy-Schwarz inequality, we have\n\n\u2207f i\nt, xi\nt \u2212\u02dcxt\n\u000b\n\u2264\n\r\r\u2207f i\nt\n\r\r \r\rxi\nt \u2212\u02dcxt\n\r\r .\n(46)\nCombining the above estimates in Equations 43, 44, 45, 46, we obtain\n\r\rxi\nt+1 \u2212x\u2217\r\r2\n\u2264\n\r\rxi\nt \u2212x\u2217\r\r2 + \u03b72 \r\r\u2207f i\nt \u2212\u03bei\nt\n\r\r2 + \u03b12 \r\rxi\nt \u2212\u02dcxt\n\r\r2\n\u2212\n2\u03b7\n\u0012 \u00b5L\n\u00b5 + L\n\r\rxi\nt \u2212x\u2217\r\r2 +\n1\n\u00b5 + L\n\r\r\u2207f i\nt\n\r\r2 \u0013\n+ 2\u03b7\n\n\u03bei\nt, xi\nt \u2212x\u2217\u000b\n\u2212\n\u03b1\n\u0000 \r\rxi\nt \u2212x\u2217\r\r2 +\n\r\rxi\nt \u2212\u02dcxt\n\r\r2 \u2212\u2225\u02dcxt \u2212x\u2217\u22252 \u0001\n+\n2\u03b7\u03b1\n\r\r\u2207f i\nt\n\r\r \r\rxi\nt \u2212\u02dcxt\n\r\r \u22122\u03b7\u03b1\n\n\u03bei\nt, xi\nt \u2212\u02dcxt\n\u000b\n.\n(47)\nChoosing 0 \u2264\u03b1 < 1, we can have this upper-bound for the terms \u03b12 \r\rxi\nt \u2212\u02dcxt\n\r\r2 \u2212\u03b1\n\r\rxi\nt \u2212\u02dcxt\n\r\r2 +\n2\u03b7\u03b1\n\r\r\u2207f i\nt\n\r\r \r\rxi\nt \u2212\u02dcxt\n\r\r = \u2212\u03b1(1 \u2212\u03b1)\n\r\rxi\nt \u2212\u02dcxt\n\r\r2 + 2\u03b7\u03b1\n\r\r\u2207f i\nt\n\r\r \r\rxi\nt \u2212\u02dcxt\n\r\r \u2264\n\u03b72\u03b1\n1\u2212\u03b1\n\r\r\u2207f i\nt\n\r\r2 by\napplying \u2212ax2 + bx \u2264b2\n4a with x =\n\r\rxi\nt \u2212\u02dcxt\n\r\r. Thus we can further bound Equation 47 with\n\r\rxi\nt+1 \u2212x\u2217\r\r2\n\u2264\n(1 \u22122\u03b7 \u00b5L\n\u00b5 + L \u2212\u03b1)\n\r\rxi\nt \u2212x\u2217\r\r2 + (\u03b72 + \u03b72\u03b1\n1 \u2212\u03b1 \u2212\n2\u03b7\n\u00b5 + L)\n\r\r\u2207f i\nt\n\r\r2\n\u2212\n2\u03b72 \n\u2207f i\nt, \u03bei\nt\n\u000b\n+ 2\u03b7\n\n\u03bei\nt, xi\nt \u2212x\u2217\u000b\n\u22122\u03b7\u03b1\n\n\u03bei\nt, xi\nt \u2212\u02dcxt\n\u000b\n(48)\n+\n\u03b72 \r\r\u03bei\nt\n\r\r2 + \u03b1 \u2225\u02dcxt \u2212x\u2217\u22252\n(49)\nAs in Equation 48 and 49, the noise \u03bei\nt is zero mean (E\u03bei\nt = 0) and the variance of the noise \u03bei\nt is\nbounded (E\n\r\r\u03bei\nt\n\r\r2 \u2264\u03c32), if \u03b7 is chosen small enough such that \u03b72 + \u03b72\u03b1\n1\u2212\u03b1 \u2212\n2\u03b7\n\u00b5+L \u22640, then\nE\n\r\rxi\nt+1 \u2212x\u2217\r\r2\n\u2264\n(1 \u22122\u03b7 \u00b5L\n\u00b5 + L \u2212\u03b1)E\n\r\rxi\nt \u2212x\u2217\r\r2 + \u03b72\u03c32 + \u03b1E \u2225\u02dcxt \u2212x\u2217\u22252 .\n(50)\nNow we apply similar idea to estimate \u2225yt \u2212x\u2217\u22252. From Equation 42 the following relation holds,\n\u2225yt+1 \u2212x\u2217\u22252\n=\n\u2225yt \u2212x\u2217\u22252 + \u03b72\n\r\r\r\r\r\n1\np\np\nX\ni=1\n\u2207f i\nt,\u03be\n\r\r\r\r\r\n2\n+ \u03b12 \u2225yt \u2212\u02dcxt\u22252\n\u2212\n2\u03b7\n*\n1\np\np\nX\ni=1\n\u2207f i\nt,\u03be, yt \u2212x\u2217\n+\n\u22122\u03b1 \u27e8yt \u2212\u02dcxt, yt \u2212x\u2217\u27e9\n+\n2\u03b7\u03b1\n*\n1\np\np\nX\ni=1\n\u2207f i\nt,\u03be, yt \u2212\u02dcxt\n+\n.\n(51)\n17\nBy\nD\n1\np\nPp\ni=1 ai, 1\np\nPp\nj=1 bj\nE\n= 1\np\nPp\ni=1 \u27e8ai, bi\u27e9\u22121\np2\nP\ni>j \u27e8ai \u2212aj, bi \u2212bj\u27e9, we have\n*\n1\np\np\nX\ni=1\n\u2207f i\nt, yt \u2212x\u2217\n+\n= 1\np\np\nX\ni=1\n\n\u2207f i\nt, xi\nt \u2212x\u2217\u000b\n\u22121\np2\nX\ni>j\nD\n\u2207f i\nt \u2212\u2207f j\nt , xi\nt \u2212xj\nt\nE\n.\n(52)\nBy the cosine rule, we have\n2 \u27e8yt \u2212\u02dcxt, yt \u2212x\u2217\u27e9= \u2225yt \u2212x\u2217\u22252 + \u2225yt \u2212\u02dcxt\u22252 \u2212\u2225\u02dcxt \u2212x\u2217\u22252 .\n(53)\nDenote \u03bet = 1\np\nPp\ni=1 \u03bei\nt, we can rewrite Equation 51 as\n\u2225yt+1 \u2212x\u2217\u22252\n=\n\u2225yt \u2212x\u2217\u22252 + \u03b72\n\r\r\r\r\r\n1\np\np\nX\ni=1\n\u2207f i\nt \u2212\u03bet\n\r\r\r\r\r\n2\n+ \u03b12 \u2225yt \u2212\u02dcxt\u22252\n\u2212\n2\u03b7\n*\n1\np\np\nX\ni=1\n\u2207f i\nt \u2212\u03bet, yt \u2212x\u2217\n+\n\u22122\u03b1 \u27e8yt \u2212\u02dcxt, yt \u2212x\u2217\u27e9\n+\n2\u03b7\u03b1\n*\n1\np\np\nX\ni=1\n\u2207f i\nt \u2212\u03bet, yt \u2212\u02dcxt\n+\n.\n(54)\nBy combining the above Equations 52, 53 with 54, we obtain\n\u2225yt+1 \u2212x\u2217\u22252\n=\n\u2225yt \u2212x\u2217\u22252 + \u03b72\n\r\r\r\r\r\n1\np\np\nX\ni=1\n\u2207f i\nt \u2212\u03bet\n\r\r\r\r\r\n2\n+ \u03b12 \u2225yt \u2212\u02dcxt\u22252\n\u2212\n2\u03b7\n\u00121\np\np\nX\ni=1\n\n\u2207f i\nt, xi\nt \u2212x\u2217\u000b\n\u22121\np2\nX\ni>j\nD\n\u2207f i\nt \u2212\u2207f j\nt , xi\nt \u2212xj\nt\nE \u0013\n(55)\n+\n2\u03b7 \u27e8\u03bet, yt \u2212x\u2217\u27e9\u2212\u03b1(\u2225yt \u2212x\u2217\u22252 + \u2225yt \u2212\u02dcxt\u22252 \u2212\u2225\u02dcxt \u2212x\u2217\u22252)\n+\n2\u03b7\u03b1\n*\n1\np\np\nX\ni=1\n\u2207f i\nt \u2212\u03bet, yt \u2212\u02dcxt\n+\n.\n(56)\nThus it follows from Equation 43 and 56 that\n\u2225yt+1 \u2212x\u2217\u22252\n\u2264\n\u2225yt \u2212x\u2217\u22252 + \u03b72\n\r\r\r\r\r\n1\np\np\nX\ni=1\n\u2207f i\nt \u2212\u03bet\n\r\r\r\r\r\n2\n+ \u03b12 \u2225yt \u2212\u02dcxt\u22252\n\u2212\n2\u03b7 1\np\np\nX\ni=1\n\u0012 \u00b5L\n\u00b5 + L\n\r\rxi\nt \u2212x\u2217\r\r2 +\n1\n\u00b5 + L\n\r\r\u2207f i\nt\n\r\r2 \u0013\n+\n2\u03b7 1\np2\nX\ni>j\nD\n\u2207f i\nt \u2212\u2207f j\nt , xi\nt \u2212xj\nt\nE\n+\n2\u03b7 \u27e8\u03bet, yt \u2212x\u2217\u27e9\u2212\u03b1(\u2225yt \u2212x\u2217\u22252 + \u2225yt \u2212\u02dcxt\u22252 \u2212\u2225\u02dcxt \u2212x\u2217\u22252)\n+\n2\u03b7\u03b1\n*\n1\np\np\nX\ni=1\n\u2207f i\nt \u2212\u03bet, yt \u2212\u02dcxt\n+\n.\n(57)\nRecall yt = 1\np\nPp\ni=1 xi\nt, we have the following bias-variance relation,\n1\np\np\nX\ni=1\n\r\rxi\nt \u2212x\u2217\r\r2\n=\n1\np\np\nX\ni=1\n\r\rxi\nt \u2212yt\n\r\r2 + \u2225yt \u2212x\u2217\u22252 = 1\np2\nX\ni>j\n\r\r\rxi\nt \u2212xj\nt\n\r\r\r\n2\n+ \u2225yt \u2212x\u2217\u22252 ,\n1\np\np\nX\ni=1\n\r\r\u2207f i\nt\n\r\r2\n=\n1\np2\nX\ni>j\n\r\r\r\u2207f i\nt \u2212\u2207f j\nt\n\r\r\r\n2\n+\n\r\r\r\r\r\n1\np\np\nX\ni=1\n\u2207f i\nt\n\r\r\r\r\r\n2\n.\n(58)\n18\nBy the Cauchy-Schwarz inequality, we have\n\u00b5L\n\u00b5 + L\n\r\r\rxi\nt \u2212xj\nt\n\r\r\r\n2\n+\n1\n\u00b5 + L\n\r\r\r\u2207f i\nt \u2212\u2207f j\nt\n\r\r\r\n2\n\u22652\u221a\u00b5L\n\u00b5 + L\nD\n\u2207f i\nt \u2212\u2207f j\nt , xi\nt \u2212xj\nt\nE\n.\n(59)\nCombining the above estimates in Equations 57, 58, 59, we obtain\n\u2225yt+1 \u2212x\u2217\u22252\n\u2264\n\u2225yt \u2212x\u2217\u22252 + \u03b72\n\r\r\r\r\r\n1\np\np\nX\ni=1\n\u2207f i\nt \u2212\u03bet\n\r\r\r\r\r\n2\n+ \u03b12 \u2225yt \u2212\u02dcxt\u22252\n\u2212\n2\u03b7\n\u0012 \u00b5L\n\u00b5 + L \u2225yt \u2212x\u2217\u22252 +\n1\n\u00b5 + L\n\r\r\r\r\r\n1\np\np\nX\ni=1\n\u2207f i\nt\n\r\r\r\r\r\n2 \u0013\n+\n2\u03b7\n\u0012\n1 \u22122\u221a\u00b5L\n\u00b5 + L\n\u0013 1\np2\nX\ni>j\nD\n\u2207f i\nt \u2212\u2207f j\nt , xi\nt \u2212xj\nt\nE\n+\n2\u03b7 \u27e8\u03bet, yt \u2212x\u2217\u27e9\u2212\u03b1(\u2225yt \u2212x\u2217\u22252 + \u2225yt \u2212\u02dcxt\u22252 \u2212\u2225\u02dcxt \u2212x\u2217\u22252)\n+\n2\u03b7\u03b1\n*\n1\np\np\nX\ni=1\n\u2207f i\nt \u2212\u03bet, yt \u2212\u02dcxt\n+\n.\n(60)\nSimilarly if 0 \u2264\u03b1 < 1, we can have this upper-bound for the terms \u03b12 \u2225yt \u2212\u02dcxt\u22252 \u2212\u03b1 \u2225yt \u2212\u02dcxt\u22252 +\n2\u03b7\u03b1\n\r\r\r 1\np\nPp\ni=1 \u2207f i\nt\n\r\r\r \u2225yt \u2212\u02dcxt\u2225\u2264\n\u03b72\u03b1\n1\u2212\u03b1\n\r\r\r 1\np\nPp\ni=1 \u2207f i\nt\n\r\r\r\n2\nby applying \u2212ax2 + bx \u2264\nb2\n4a with x =\n\u2225yt \u2212\u02dcxt\u2225. Thus we have the following bound for the Equation 60\n\u2225yt+1 \u2212x\u2217\u22252\n\u2264\n(1 \u22122\u03b7 \u00b5L\n\u00b5 + L \u2212\u03b1) \u2225yt \u2212x\u2217\u22252 + (\u03b72 + \u03b72\u03b1\n1 \u2212\u03b1 \u2212\n2\u03b7\n\u00b5 + L)\n\r\r\r\r\r\n1\np\np\nX\ni=1\n\u2207f i\nt\n\r\r\r\r\r\n2\n\u2212\n2\u03b72\n*\n1\np\np\nX\ni=1\n\u2207f i\nt, \u03bet\n+\n+ 2\u03b7 \u27e8\u03bet, yt \u2212x\u2217\u27e9\u22122\u03b7\u03b1 \u27e8\u03bet, yt \u2212\u02dcxt\u27e9\n+\n2\u03b7\n\u0012\n1 \u22122\u221a\u00b5L\n\u00b5 + L\n\u0013 1\np2\nX\ni>j\nD\n\u2207f i\nt \u2212\u2207f j\nt , xi\nt \u2212xj\nt\nE\n+\n\u03b72 \u2225\u03bet\u22252 + \u03b1 \u2225\u02dcxt \u2212x\u2217\u22252 .\n(61)\nSince 2\u221a\u00b5L\n\u00b5+L \u22641, we need also bound the non-linear term\nD\n\u2207f i\nt \u2212\u2207f j\nt , xi\nt \u2212xj\nt\nE\n\u2264L\n\r\r\rxi\nt \u2212xj\nt\n\r\r\r\n2\n.\nRecall the bias-variance relation\n1\np\nPp\ni=1\n\r\rxi\nt \u2212x\u2217\r\r2\n=\n1\np2\nP\ni>j\n\r\r\rxi\nt \u2212xj\nt\n\r\r\r\n2\n+ \u2225yt \u2212x\u2217\u22252.\nThe key observation is that if\n1\np\nPp\ni=1\n\r\rxi\nt \u2212x\u2217\r\r2 remains bounded, then larger variance\nP\ni>j\n\r\r\rxi\nt \u2212xj\nt\n\r\r\r\n2\nimplies smaller bias \u2225yt \u2212x\u2217\u22252. Thus this non-linear term can be compensated.\nAgain choose \u03b7 small enough such that \u03b72 + \u03b72\u03b1\n1\u2212\u03b1 \u2212\n2\u03b7\n\u00b5+L \u22640 and take expectation in Equation 61,\nE \u2225yt+1 \u2212x\u2217\u22252\n\u2264\n(1 \u22122\u03b7 \u00b5L\n\u00b5 + L \u2212\u03b1)E \u2225yt \u2212x\u2217\u22252\n+\n2\u03b7L\n\u0012\n1 \u22122\u221a\u00b5L\n\u00b5 + L\n\u0013\u00121\np\np\nX\ni=1\nE\n\r\rxi\nt \u2212x\u2217\r\r2 \u2212E \u2225yt \u2212x\u2217\u22252\n\u0013\n+\n\u03b72 \u03c32\np + \u03b1E \u2225\u02dcxt \u2212x\u2217\u22252 .\n(62)\nAs for the center variable in Equation 41, we apply simply the convexity of the norm \u2225\u00b7\u22252 to obtain\n\u2225\u02dcxt+1 \u2212x\u2217\u22252 \u2264(1 \u2212\u03b2) \u2225\u02dcxt \u2212x\u2217\u22252 + \u03b2 \u2225yt \u2212x\u2217\u22252 .\n(63)\n19\nCombing the estimates from Equations 50, 62, 63, and denote at\n=\nE \u2225yt \u2212x\u2217\u22252, bt\n=\n1\np\nPp\ni=1 E\n\r\rxi\nt \u2212x\u2217\r\r2, ct = E \u2225\u02dcxt \u2212x\u2217\u22252, \u03b31 = 2\u03b7 \u00b5L\n\u00b5+L, \u03b32 = 2\u03b7L(1 \u22122\u221a\u00b5L\n\u00b5+L ), then\n at+1\nbt+1\nct+1\n!\n\u2264\n 1 \u2212\u03b31 \u2212\u03b32 \u2212\u03b1\n\u03b32\n\u03b1\n0\n1 \u2212\u03b31 \u2212\u03b1\n\u03b1\n\u03b2\n0\n1 \u2212\u03b2\n!  at\nbt\nct\n!\n+\n\uf8eb\n\uf8ed\n\u03b72 \u03c32\np\n\u03b72\u03c32\n0\n\uf8f6\n\uf8f8,\nas long as 0 \u2264\u03b2 \u22641, 0 \u2264\u03b1 < 1 and \u03b72 + \u03b72\u03b1\n1\u2212\u03b1 \u2212\n2\u03b7\n\u00b5+L \u22640, i.e. 0 \u2264\u03b7 \u2264\n2\n\u00b5+L(1 \u2212\u03b1).\n8\nAdditional pseudo-codes of the algorithms\n8.1\nDOWNPOUR pseudo-code\nAlgorithm 3 captures the pseudo-code of the implementation of the DOWNPOUR used in this paper.\nAlgorithm 3: DOWNPOUR: Processing by worker i and the master\nInput: learning rate \u03b7, communication period \u03c4 \u2208N\nInitialize: \u02dcx is initialized randomly, xi = \u02dcx, vi = 0, ti = 0\nRepeat\nif (\u03c4 divides ti) then\n\u02dcx \u2190\u02dcx + vi\nxi \u2190\u02dcx\nvi \u21900\nend\nxi \u2190xi \u2212\u03b7gi\nti(xi)\nvi \u2190vi \u2212\u03b7gi\nti(xi)\nti \u2190ti + 1\nUntil forever\n8.2\nMDOWNPOUR pseudo-code\nAlgorithms 4 and 5 capture the pseudo-codes of the implementation of momentum DOWNPOUR\n(MDOWNPOUR) used in this paper. Algorithm 4 shows the behavior of each local worker and\nAlgorithm 5 shows the behavior of the master.\nAlgorithm 4: MDOWNPOUR: Processing by worker i\nInitialize: xi = \u02dcx\nRepeat\nReceive \u02dcx from the master: xi \u2190\u02dcx\nCompute gradient gi = gi(xi)\nSend gi to the master\nUntil forever\nAlgorithm 5: MDOWNPOUR: Processing by the master\nInput: learning rate \u03b7, momentum term \u03b4\nInitialize: \u02dcx is initialized randomly, vi = 0,\nRepeat\nReceive gi\nv \u2190\u03b4v \u2212\u03b7gi\n\u02dcx \u2190\u02dcx + \u03b4v\nUntil forever\n20\n9\nExperiments - additional material\n9.1\nData preprocessing\nFor the ImageNet experiment, we re-size each RGB image so that the smallest dimension is 256\npixels. We also re-scale each pixel value to the interval [0, 1]. We then extract random crops (and\ntheir horizontal \ufb02ips) of size 3 \u00d7 221 \u00d7 221 pixels and present these to the network in mini-batches\nof size 128.\nFor the CIFAR experiment, we use the original RGB image of size 3 \u00d7 32 \u00d7 32. As before, we\nre-scale each pixel value to the interval [0, 1]. We then extract random crops (and their horizontal\n\ufb02ips) of size 3 \u00d7 28 \u00d7 28 pixels and present these to the network in mini-batches of size 128.\nThe training and test loss and the test error are only computed from the center patch (3 \u00d7 28 \u00d7 28)\nfor the CIFAR experiment and the center patch (3 \u00d7 221 \u00d7 221) for the ImageNet experiment.\n9.2\nData prefetching (Sampling the dataset by the local workers)\nWe will now explain precisely how the dataset is sampled by each local worker as uniformly and\nef\ufb01ciently as possible. The general parallel data loading scheme on a single machine is as fol-\nlows: we use k CPUs, where k = 8, to load the data in parallel. Each data loader reads from the\nmemory-mapped (mmap) \ufb01le a chunk of c raw images (preprocessing was described in the previous\nsubsection) and their labels (for CIFAR c = 512 and for ImageNet c = 64). For the CIFAR, the\nmmap \ufb01le of each data loader contains the entire dataset whereas for ImageNet, each mmap \ufb01le of\neach data loader contains different 1/k fractions of the entire dataset. A chunk of data is always\nsent by one of the data loaders to the \ufb01rst worker who requests the data. The next worker request-\ning the data from the same data loader will get the next chunk. Each worker requests in total k\ndata chunks from k different data loaders and then process them before asking for new data chunks.\nNotice that each data loader cycles through the data in the mmap \ufb01le, sending consecutive chunks\nto the workers in order in which it receives requests from them. When the data loader reaches\nthe end of the mmap \ufb01le, it selects the address in memory uniformly at random from the interval\n[0, s], where s = (number of images in the mmap \ufb01le modulo mini-batch size), and uses this\naddress to start cycling again through the data in the mmap \ufb01le. After the local worker receives the\nk data chunks from the data loaders, it shuf\ufb02es them and divides it into mini-batches of size 128.\n9.3\nLearning rates\nIn Table 1 we summarize the learning rates \u03b7 (we used constant learning rates) explored for each\nmethod shown in Figure 2. For all values of \u03c4 the same set of learning rates was explored for each\nmethod.\nTable 1: Learning rates explored for each method shown in Figure 2 (CIFAR experiment).\n\u03b7\nEASGD\n{0.05, 0.01, 0.005}\nEAMSGD\n{0.01, 0.005, 0.001}\nDOWNPOUR\nADOWNPOUR\n{0.005, 0.001, 0.0005}\nMVADOWNPOUR\nMDOWNPOUR\n{0.00005, 0.00001, 0.000005}\nSGD, ASGD, MVASGD\n{0.05, 0.01, 0.005}\nMSGD\n{0.001, 0.0005, 0.0001}\nIn Table 2 we summarize the learning rates \u03b7 (we used constant learning rates) explored for each\nmethod shown in Figure 3. For all values of p the same set of learning rates was explored for each\nmethod.\nIn Table 3 we summarize the initial learning rates \u03b7 we use for each method shown in Figure 4. For\nall values of p the same set of learning rates was explored for each method. We also used the rule\nof the thumb to decrease the initial learning rate twice, \ufb01rst time we divided it by 5 and the second\ntime by 2, when we observed that the decrease of the online predictive (training) loss saturates.\n21\nTable 2: Learning rates explored for each method shown in Figure 3 (CIFAR experiment).\n\u03b7\nEASGD\n{0.05, 0.01, 0.005}\nEAMSGD\n{0.01, 0.005, 0.001}\nDOWNPOUR\n{0.005, 0.001, 0.0005}\nMDOWNPOUR\n{0.00005, 0.00001, 0.000005}\nSGD, ASGD, MVASGD\n{0.05, 0.01, 0.005}\nMSGD\n{0.001, 0.0005, 0.0001}\nTable 3: Learning rates explored for each method shown in Figure 4 (ImageNet experiment).\n\u03b7\nEASGD\n0.1\nEAMSGD\n0.001\nDOWNPOUR\nfor p = 4: 0.02\nfor p = 8: 0.01\nSGD, ASGD, MVASGD\n0.05\nMSGD\n0.0005\n9.4\nComparison of SGD, ASGD, MVASGD and MSGD\n50\n100\n150\n0.5\n1\n1.5\n2\nwallclock time (min)\ntraining loss (nll)\n \n \nSGD\nASGD\nMVASGD\nMSGD\n50\n100\n150\n1\n1.5\n2\nwallclock time (min)\ntest loss (nll)\n50\n100\n150\n20\n30\n40\n50\n60\n70\n80\n90\nwallclock time (min)\ntest error (%)\n50\n100\n150\n17\n18\n19\n20\n21\n22\nwallclock time (min)\ntest error (%)\nFigure 6: Convergence of the training and test loss (negative log-likelihood) and the test error (orig-\ninal and zoomed) computed for the center variable as a function of wallclock time for SGD, ASGD,\nMVASGD and MSGD (p = 1) on the CIFAR experiment.\n0\n50\n100\n150\n2\n3\n4\n5\n6\nwallclock time (hour)\ntraining loss (nll)\n \n \nSGD\nASGD\nMVASGD\nMSGD\n0\n50\n100\n150\n3\n4\n5\n6\nwallclock time (hour)\ntest loss (nll)\n0\n50\n100\n150\n50\n60\n70\n80\n90\nwallclock time (hour)\ntest error (%)\n0\n50\n100\n150\n42\n44\n46\n48\n50\n52\n54\nwallclock time (hour)\ntest error (%)\nFigure 7: Convergence of the training and test loss (negative log-likelihood) and the test error (orig-\ninal and zoomed) computed for the center variable as a function of wallclock time for SGD, ASGD,\nMVASGD and MSGD (p = 1) on the ImageNet experiment.\nFigure 6 shows the convergence of the training and test loss (negative log-likelihood) and the test\nerror computed for the center variable as a function of wallclock time for SGD, ASGD, MVASGD and\nMSGD (p = 1) on the CIFAR experiment. For all CIFAR experiments we always start the averaging\nfor the ADOWNPOUR and ASGD methods from the very beginning of each experiment. For all\nImageNet experiments we start the averaging for the ASGD at the same time when we \ufb01rst reduce\nthe initial learning rate.\nFigure 7 shows the convergence of the training and test loss (negative log-likelihood) and the test\nerror computed for the center variable as a function of wallclock time for SGD, ASGD, MVASGD\nand MSGD (p = 1) on the ImageNet experiment.\n22\n9.5\nDependence of the learning rate\nThis section discusses the dependence of the trade-off between exploration and exploitation on the\nlearning rate. We compare the performance of respectively EAMSGD and EASGD for different\nlearning rates \u03b7 when p = 16 and \u03c4 = 10 on the CIFAR experiment. We observe in Figure 8 that\nhigher learning rates \u03b7 lead to better test performance for the EAMSGD algorithm which potentially\ncan be justi\ufb01ed by the fact that they sustain higher \ufb02uctuations of the local workers. We conjecture\nthat higher \ufb02uctuations lead to more exploration and simultaneously they also impose higher reg-\nularization. This picture however seems to be opposite for the EASGD algorithm for which larger\nlearning rates hurt the performance of the method and lead to over\ufb01tting. Interestingly in this ex-\nperiment for both EASGD and EAMSGD algorithm, the learning rate for which the best training\nperformance was achieved simultaneously led to the worst test performance.\n50\n100\n150\n0.5\n1\n1.5\n2\nwallclock time (min)\ntraining loss (nll)\nEAMSGD\n \n \n0.01\n0.005\n0.001\n50\n100\n150\n16\n18\n20\n22\n24\n26\n28\nwallclock time (min)\ntest error (%)\nEAMSGD\n50\n100\n150\n0.5\n1\n1.5\n2\nwallclock time (min)\ntraining loss (nll)\nEASGD\n \n \n0.05\n0.01\n0.005\n50\n100\n150\n16\n18\n20\n22\n24\n26\n28\nwallclock time (min)\ntest error (%)\nEASGD\nFigure 8: Convergence of the training loss (negative log-likelihood, original) and the test error\n(zoomed) computed for the center variable as a function of wallclock time for EAMSGD and EASGD\nrun with different values of \u03b7 on the CIFAR experiment. p = 16, \u03c4 = 10.\n9.6\nDependence of the communication period\nThis section discusses the dependence of the trade-off between exploration and exploitation on the\ncommunication period. We have observed from the CIFAR experiment that EASGD algorithm ex-\nhibits very similar convergence behavior when \u03c4 = 1 up to even \u03c4 = 1000, whereas EAMSGD can\nget trapped at worse energy (loss) level for \u03c4 = 100. This behavior of EAMSGD is most likely due to\nthe non-convexity of the objective function. Luckily, it can be avoided by gradually decreasing the\nlearning rate, i.e. increasing the penalty term \u03c1 (recall \u03b1 = \u03b7\u03c1), as shown in Figure 9. In contrast,\nthe EASGD algorithm does not seem to get trapped at all along its trajectory. The performance of\nEASGD is less sensitive to increasing the communication period compared to EAMSGD, whereas for\nthe EAMSGD the careful choice of the learning rate for large communication periods seems crucial.\nCompared to all earlier results, the experiment in this section is re-run three times with a new ran-\ndom12 seed and with faster cuDNN13 package14. All our methods are implemented in Torch15. The\nMessage Passing Interface implementation MVAPICH216 is used for the GPU-CPU communication.\n50\n100\n150\n200\n250\n300\n0.5\n1\n1.5\n2\nwallclock time (min)\ntraining loss (nll)\nEASGD\n \n \n\u03c4=1,\u03b3=0\n\u03c4=10,\u03b3=0\n\u03c4=100,\u03b3=0\n\u03c4=1000,\u03b3=0\n50\n100\n150\n200\n250\n300\n16\n18\n20\n22\n24\n26\n28\nwallclock time (min)\ntest error (%)\nEASGD\n50\n100\n150\n0.5\n1\n1.5\n2\n2.5\nwallclock time (min)\ntraining loss (nll)\nEAMSGD\n \n \n\u03c4=10,\u03b3=0\n\u03c4=100,\u03b3=0\n\u03c4=100,\u03b3=1e\u22124\n50\n100\n150\n16\n18\n20\n22\n24\n26\n28\nwallclock time (min)\ntest error (%)\nEAMSGD\nFigure 9: Convergence of the training loss (negative log-likelihood, original) and the test error\n(zoomed) computed for the center variable as a function of wallclock time for EASGD and EAMSGD\n(p = 16, \u03b7 = 0.01, \u03b2 = 0.9, \u03b4 = 0.99) on the CIFAR experiment with various communication\nperiod \u03c4 and learning rate decay \u03b3. The learning rate is decreased gradually over time based each\nlocal worker\u2019s own clock t with \u03b7t = \u03b7/(1 + \u03b3t)0.5.\n12To clarify, the random initialization we use is by default in Torch\u2019s implementation.\n13https://developer.nvidia.com/cuDNN\n14https://github.com/soumith/cudnn.torch\n15http://torch.ch\n16http://mvapich.cse.ohio-state.edu\n23\n9.7\nBreakdown of the wallclock time\nIn addition, we report in Table 4 the breakdown of the total running time for EASGD when \u03c4 =\n10 (the time breakdown for EAMSGD is almost identical) and DOWNPOUR when \u03c4 = 1 into\ncomputation time, data loading time and parameter communication time. For the CIFAR experiment\nthe reported time corresponds to processing 400 \u00d7 128 data samples whereas for the ImageNet\nexperiment it corresponds to processing 1024 \u00d7 128 data samples. For \u03c4 = 1 and p \u2208{8, 16}\nwe observe that the communication time accounts for signi\ufb01cant portion of the total running time\nwhereas for \u03c4 = 10 the communication time becomes negligible compared to the total running time\n(recall that based on previous results EASGD and EAMSGD achieve best performance with larger \u03c4\nwhich is ideal in the setting when communication is time-consuming).\np = 1\np = 4\np = 8 p = 16\n\u03c4 = 1 12/1/0 11/2/3 11/2/5 11/2/9\n\u03c4 = 10\nNA\n11/2/1 11/2/1 12/2/1\np = 1\np = 4\np = 8\n\u03c4 = 1 1248/20/0 1323/24/173 1239/61/284\n\u03c4 = 10\nNA\n1254/58/7\n1266/84/11\nTable 4: Approximate computation time, data loading time and parameter communication time [sec]\nfor DOWNPOUR (top line for \u03c4 = 1) and EASGD (the time breakdown for EAMSGD is almost\nidentical) (bottom line for \u03c4 = 10). Left time corresponds to CIFAR experiment and right table\ncorresponds to ImageNet experiment.\n9.8\nTime speed-up\nIn Figure 10 and 11, we summarize the wall clock time needed to achieve the same level of the test\nerror for all the methods in the CIFAR and ImageNet experiment as a function of the number of local\nworkers p. For the CIFAR (Figure 10) we examined the following levels: {21%, 20%, 19%, 18%}\nand for the ImageNet (Figure 11) we examined: {49%, 47%, 45%, 43%}. If some method does not\nappear on the \ufb01gure for a given test error level, it indicates that this method never achieved this level.\nFor the CIFAR experiment we observe that from among EASGD, DOWNPOUR and MDOWNPOUR\nmethods, the EASGD method needs less time to achieve a particular level of test error. We observe\nthat with higher p each of these methods does not necessarily need less time to achieve the same\nlevel of test error. This seems counter intuitive though recall that the learning rate for the methods is\nselected based on the smallest achievable test error. For larger p smaller learning rates were selected\nthan for smaller p which explains our results. Meanwhile, the EAMSGD method achieves signi\ufb01cant\nspeed-up over other methods for all the test error levels. For the ImageNet experiment we observe\nthat all methods outperform MSGD and furthermore with p = 4 or p = 8 each of these methods\nrequires less time to achieve the same level of test error. The EAMSGD consistently needs less time\nthan any other method, in particular DOWNPOUR, to achieve any of the test error levels.\n 1\n 4\n 8\n16\n0\n50\n100\n150\np\nwallclock time (min)\nlevel 21%\n \n \nMSGD\nEAMSGD\nEASGD\nDOWNPOUR\nMDOWNPOUR\n 1\n 4\n 8\n16\n0\n50\n100\n150\nlevel 20%\np\nwallclock time (min)\n 1\n 4\n 8\n16\n0\n50\n100\n150\nlevel 19%\np\nwallclock time (min)\n 1\n 4\n 8\n16\n0\n50\n100\n150\nlevel 18%\np\nwallclock time (min)\nFigure 10: The wall clock time needed to achieve the same level of the test error thr as a\nfunction of the number of local workers p on the CIFAR dataset.\nFrom left to right: thr =\n{21%, 20%, 19%, 18%}. Missing bars denote that the method never achieved speci\ufb01ed level of\ntest error.\n.\n1\n4\n8\n0\n50\n100\n150\np\nwallclock time (hour)\nlevel 49%\n \n \nMSGD\nEAMSGD\nEASGD\nDOWNPOUR\n1\n4\n8\n0\n50\n100\n150\nlevel 47%\np\nwallclock time (hour)\n1\n4\n8\n0\n50\n100\n150\nlevel 45%\np\nwallclock time (hour)\n1\n4\n8\n0\n50\n100\n150\nlevel 43%\np\nwallclock time (hour)\nFigure 11: The wall clock time needed to achieve the same level of the test error thr as a func-\ntion of the number of local workers p on the ImageNet dataset.\nFrom left to right: thr =\n{49%, 47%, 45%, 43%}. Missing bars denote that the method never achieved speci\ufb01ed level of\ntest error.\n.\n24\n",
        "sentence": " Recently, there emerge a battery of studies trying to accelearte the optimization of deep learning (Sutskever et al., 2013; Neyshabur et al., 2015; Zhang et al., 2014; Martens & Grosse, 2015; Ioffe & Szegedy, 2015; Kingma & Ba, 2014), which tackle the problem from",
        "context": "computer [3, 4]. The main challenge is to devise parallel SGD algorithms to train large-scale deep\nlearning models that yield a signi\ufb01cant speedup when run on multiple GPU cards.\ntraining of large models that use a form of stochastic gradient descent (SGD) [1]. There have been\nattempts to parallelize SGD-based training for large-scale deep learning models on large number\n2):365\u2013397, 2012.\n[26] Sutskever, I, Martens, J, Dahl, G, and Hinton, G. On the importance of initialization and momentum in\ndeep learning. In ICML, 2013.\n[27] Zhang, R and Kwok, J. Asynchronous distributed admm for consensus optimization. In ICML, 2014."
    },
    {
        "title": "Adaptive dropout rates for learning with corrupted features",
        "author": [
            "Zhuo",
            "Jingwei",
            "Zhu",
            "Jun",
            "Zhang",
            "Bo"
        ],
        "venue": "In IJCAI, pp",
        "citeRegEx": "Zhuo et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "Zhuo et al\\.",
        "year": 2015,
        "abstract": "",
        "full_text": "",
        "sentence": " Some works tried to optimize the hyper-parameters that define the noise level in a Bayesian framework (Zhuo et al., 2015; Kingma et al., 2015).",
        "context": null
    },
    {
        "title": "Online convex programming and generalized infinitesimal gradient ascent",
        "author": [
            "Zinkevich",
            "Martin"
        ],
        "venue": "In Proceedings of the International Conference on Machine Learning (ICML),",
        "citeRegEx": "Zinkevich and Martin.,? \\Q2003\\E",
        "shortCiteRegEx": "Zinkevich and Martin.",
        "year": 2003,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    }
]